,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Proof: There is no function $f \in C^2(\mathbb{R^3})$ with gradient $\nabla f(x,y,z) = (yz, xz, xy^2)$",Proof: There is no function  with gradient,"f \in C^2(\mathbb{R^3}) \nabla f(x,y,z) = (yz, xz, xy^2)","How can one show that there is no function, which is a continuously partially derivable function $f \in C^2(\mathbb{R^3})$ with this gradient $$\nabla f(x,y,z) = (yz, xz, xy^2)$$ I thought about using the Hessian matrix since one has to calculate all second partial derivatives of $f$ there. Since only the gradient is given, can I calculate the antiderivatives first: $yz = xyz$ $xz = xyz$ $xy^2 = xy^2z$ Now I want to calculate the antiderivatives of the antiderivatives: $xyz = \frac{yzx^2}{2}$ $xyz = \frac{yzx^2}{2}$ $xy^2z = \dfrac{y^2zx^2}{2}$ I didn't calculate the antiderivatives of the partial derivatives and I don't even know if that way is correct...","How can one show that there is no function, which is a continuously partially derivable function with this gradient I thought about using the Hessian matrix since one has to calculate all second partial derivatives of there. Since only the gradient is given, can I calculate the antiderivatives first: Now I want to calculate the antiderivatives of the antiderivatives: I didn't calculate the antiderivatives of the partial derivatives and I don't even know if that way is correct...","f \in C^2(\mathbb{R^3}) \nabla f(x,y,z) = (yz, xz, xy^2) f yz = xyz xz = xyz xy^2 = xy^2z xyz = \frac{yzx^2}{2} xyz = \frac{yzx^2}{2} xy^2z = \dfrac{y^2zx^2}{2}","['analysis', 'functions', 'derivatives', 'continuity', 'partial-derivative']"
1,Find $\lim_{n\rightarrow \infty}\int_0^1 f_n(x) dx$,Find,\lim_{n\rightarrow \infty}\int_0^1 f_n(x) dx,"Let $f_n:[0, 1] \rightarrow \mathbb{R}$ be defined by $f_n(x)=\dfrac{n+x^3 \cos x}{n e^x  + x^5 \sin x}, n \geq 1$ . Find $\lim_{n\rightarrow \infty}\int_0^1 f_n(x) dx$ My answer is $1-\dfrac{1}{e}.$ Please see it, right or wrong.","Let be defined by . Find My answer is Please see it, right or wrong.","f_n:[0, 1] \rightarrow \mathbb{R} f_n(x)=\dfrac{n+x^3 \cos x}{n e^x  + x^5 \sin x}, n \geq 1 \lim_{n\rightarrow \infty}\int_0^1 f_n(x) dx 1-\dfrac{1}{e}.","['real-analysis', 'integration', 'analysis', 'proof-verification', 'sequence-of-function']"
2,"Intuitively, how is the proof of IVT with connectedness equivalent to the proof with completeness?","Intuitively, how is the proof of IVT with connectedness equivalent to the proof with completeness?",,"I just learned the proof of IVT in general topology and it seems very different than the one I learned in Analysis (actually, the Analysis IVT is a bit stronger, I guess, since it asserts that the point occurs in some interval). Does this mean that connectedness somehow encompasses the notion of completeness?","I just learned the proof of IVT in general topology and it seems very different than the one I learned in Analysis (actually, the Analysis IVT is a bit stronger, I guess, since it asserts that the point occurs in some interval). Does this mean that connectedness somehow encompasses the notion of completeness?",,"['general-topology', 'analysis']"
3,integral form of Taylor theorem remainder multivariable,integral form of Taylor theorem remainder multivariable,,"Let a function f be in $C^{k+1}(B(x_0,r)),r>0$, then  $$f(x) = P_k(x;x_0)+ (k+1)\sum_{\vert \alpha \vert= k+1}\left(\int_0^1 (1-t)^kD^\alpha f(x_0+t(x-x_0))dt\right)\frac{(x-x_0)^\alpha}{\alpha !}$$ Where $P_k(x;x_0)$ is the Taylor polynomial centered at $x_0$.and $\alpha$ here is multivariable index $\alpha = (\alpha_1, \cdot \cdot \cdot, \alpha_n )$ and usual multivariable notation is used here.  I know how to get the Taylor polynomial (just use integration by part repeatedly) but how can we get the remainder in that form?","Let a function f be in $C^{k+1}(B(x_0,r)),r>0$, then  $$f(x) = P_k(x;x_0)+ (k+1)\sum_{\vert \alpha \vert= k+1}\left(\int_0^1 (1-t)^kD^\alpha f(x_0+t(x-x_0))dt\right)\frac{(x-x_0)^\alpha}{\alpha !}$$ Where $P_k(x;x_0)$ is the Taylor polynomial centered at $x_0$.and $\alpha$ here is multivariable index $\alpha = (\alpha_1, \cdot \cdot \cdot, \alpha_n )$ and usual multivariable notation is used here.  I know how to get the Taylor polynomial (just use integration by part repeatedly) but how can we get the remainder in that form?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'taylor-expansion']"
4,"Show that $f(tx)=t^pf(x),\;\forall \;t>0,\;\&\;x\in \Bbb{R}^n $ if and only $f'(x)(x)=pf(x),\;\forall \;x\in \Bbb{R}^n$",Show that  if and only,"f(tx)=t^pf(x),\;\forall \;t>0,\;\&\;x\in \Bbb{R}^n  f'(x)(x)=pf(x),\;\forall \;x\in \Bbb{R}^n","Let $f:\Bbb{R}^n\to \Bbb{R}$ be a differentiable function such that for some $p>1,$ \begin{align}f(tx)=t^pf(x),\;\forall \;t>0,\;\&\;x\in \Bbb{R}^n \qquad (1)\end{align} $i.$ I want to show that \begin{align}f'(x)(x)=pf(x),\;\forall \;x\in \Bbb{R}^n \qquad (1)\end{align} $ii.$ Is the converse also true? I believe that we can take \begin{align}\varphi(t)=f(tx),\;\forall \;t>0\end{align} and show that \begin{align}\frac{d}{dt}\big(\frac{\varphi(t)}{t^p}\big)=0,\end{align} but I don't know how to tranform this to a proof. Any help please?","Let $f:\Bbb{R}^n\to \Bbb{R}$ be a differentiable function such that for some $p>1,$ \begin{align}f(tx)=t^pf(x),\;\forall \;t>0,\;\&\;x\in \Bbb{R}^n \qquad (1)\end{align} $i.$ I want to show that \begin{align}f'(x)(x)=pf(x),\;\forall \;x\in \Bbb{R}^n \qquad (1)\end{align} $ii.$ Is the converse also true? I believe that we can take \begin{align}\varphi(t)=f(tx),\;\forall \;t>0\end{align} and show that \begin{align}\frac{d}{dt}\big(\frac{\varphi(t)}{t^p}\big)=0,\end{align} but I don't know how to tranform this to a proof. Any help please?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
5,The Fourier transform is unbounded from $L^{p}$ to $L^{p^{\prime}}$ when $2<p\leq \infty$?,The Fourier transform is unbounded from  to  when ?,L^{p} L^{p^{\prime}} 2<p\leq \infty,I previously asked this question here Haussdorff-Young inequality optimal Lebesgue exponents range and got a comment that referred me to the answer here Fourier transform in $L^p$ but I did not find the answer to my specific question: What (where can one find) argument/counterexamples  that shows  the discontinuity of the Fourier transform from $L^{p}$ to $L^{p^{\prime}}$ when $2<p\leq \infty$ ?,I previously asked this question here Haussdorff-Young inequality optimal Lebesgue exponents range and got a comment that referred me to the answer here Fourier transform in $L^p$ but I did not find the answer to my specific question: What (where can one find) argument/counterexamples  that shows  the discontinuity of the Fourier transform from $L^{p}$ to $L^{p^{\prime}}$ when $2<p\leq \infty$ ?,,"['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis', 'dispersive-pde']"
6,Determine the arc length of the following parametric curve,Determine the arc length of the following parametric curve,,"Determine the arc length of the parametric curve given by the following parametric equation:    $φ(t)= (\sqrt{t}, t+1, t)$ $t\in[10,20]$ In order to do this I simply tried it to solve it by the formula of arc lenght. Given the formula, $ L=\int_{a}^{b} \sqrt{\left(\frac{dx}{dt}\right)^2 +\left(\frac{dy}{dt}\right)^2 +\left(\frac{dz}{dt}\right)^2 } \;dt$ I get that $L=\int_{10}^{20} \sqrt {2+\frac{1}{4t}}; dt=\int_{10}^{20}\sqrt {\frac{8t+1}{4t}} \; dt$ Then, I tried to get prettier the integral function by multiplying in the argument of the square root by $4t/4t$ (I eliminated square root in the denominator) getting this  not very satisfying result: $L=\int_{10}^{20}\sqrt {\frac{(4t)^2+t}{2t}} \; dt$ I really don't know how to solve that integral, so I wonder if someone comes out with some idea about how to do it. In the other hand, I tought that maybe there is a nicer parametrization of that particular parametrization, e.g. a reparametrization, where we could solve the integral more easily, but in that case I'm not sure about how to find it. Any ideas?","Determine the arc length of the parametric curve given by the following parametric equation:    $φ(t)= (\sqrt{t}, t+1, t)$ $t\in[10,20]$ In order to do this I simply tried it to solve it by the formula of arc lenght. Given the formula, $ L=\int_{a}^{b} \sqrt{\left(\frac{dx}{dt}\right)^2 +\left(\frac{dy}{dt}\right)^2 +\left(\frac{dz}{dt}\right)^2 } \;dt$ I get that $L=\int_{10}^{20} \sqrt {2+\frac{1}{4t}}; dt=\int_{10}^{20}\sqrt {\frac{8t+1}{4t}} \; dt$ Then, I tried to get prettier the integral function by multiplying in the argument of the square root by $4t/4t$ (I eliminated square root in the denominator) getting this  not very satisfying result: $L=\int_{10}^{20}\sqrt {\frac{(4t)^2+t}{2t}} \; dt$ I really don't know how to solve that integral, so I wonder if someone comes out with some idea about how to do it. In the other hand, I tought that maybe there is a nicer parametrization of that particular parametrization, e.g. a reparametrization, where we could solve the integral more easily, but in that case I'm not sure about how to find it. Any ideas?",,"['real-analysis', 'integration', 'analysis', 'parametrization', 'arc-length']"
7,Show that $\frac{5}{n}+e^{-n}=O(\frac{1}{n})$,Show that,\frac{5}{n}+e^{-n}=O(\frac{1}{n}),"Show that $\frac{5}{n}+e^{-n}=O(\frac{1}{n})$ I know that $e^n>n, (n\geq 0)$, with which $e^{-n}<\frac{1}{n}$ and so $\frac{5}{n}+e^{-n}<\frac{5}{n}+\frac{1}{n}=6\frac{1}{n}$, then $\frac{5}{n}+e^{-n}=O(\frac{1}{n})$. Is this argument right? Thank you very much.","Show that $\frac{5}{n}+e^{-n}=O(\frac{1}{n})$ I know that $e^n>n, (n\geq 0)$, with which $e^{-n}<\frac{1}{n}$ and so $\frac{5}{n}+e^{-n}<\frac{5}{n}+\frac{1}{n}=6\frac{1}{n}$, then $\frac{5}{n}+e^{-n}=O(\frac{1}{n})$. Is this argument right? Thank you very much.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'numerical-methods']"
8,Is an integral itself a function or a constant?,Is an integral itself a function or a constant?,,"Is an integral itself a function or a constant? I have three cases: Q1: Say I have $f:\mathbb R \rightarrow \mathbb R$ and $\int_a^bf(t)\, dt$. Should I write $$ g(t)=\int_a^bf(t)\, dt \tag 1 $$ where $g:\mathbb R\rightarrow \mathbb R$. Or should I write $$ a=\int_a^bf(t)\, dt \tag 2 $$ where $a$ is a constant, $a\in \mathbb R$. Q2: If I have $x:\mathbb R\rightarrow \mathbb R$ and an improper integral $\int_{-\infty}^{\infty}x(t)\, dt$, should I write $$ h(t)=\int_{-\infty}^{\infty}x(t)\, dt \tag 3 $$ where $h:\mathbb R\rightarrow \mathbb R$. Or with a constant $b\in \mathbb R$ $$ b=\int_{-\infty}^{\infty}x(t)\, dt \tag 4 $$ Q3: Also an improper integral of $y:\mathbb R\rightarrow \mathbb R$, so $\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt$. Is the following correct $$ p(x)=\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt \tag 5 $$ where $p:\mathbb R\rightarrow \mathbb R$. Or with a constant $c\in\mathbb R$ $$ c=\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt \tag 6 $$ Thanks!","Is an integral itself a function or a constant? I have three cases: Q1: Say I have $f:\mathbb R \rightarrow \mathbb R$ and $\int_a^bf(t)\, dt$. Should I write $$ g(t)=\int_a^bf(t)\, dt \tag 1 $$ where $g:\mathbb R\rightarrow \mathbb R$. Or should I write $$ a=\int_a^bf(t)\, dt \tag 2 $$ where $a$ is a constant, $a\in \mathbb R$. Q2: If I have $x:\mathbb R\rightarrow \mathbb R$ and an improper integral $\int_{-\infty}^{\infty}x(t)\, dt$, should I write $$ h(t)=\int_{-\infty}^{\infty}x(t)\, dt \tag 3 $$ where $h:\mathbb R\rightarrow \mathbb R$. Or with a constant $b\in \mathbb R$ $$ b=\int_{-\infty}^{\infty}x(t)\, dt \tag 4 $$ Q3: Also an improper integral of $y:\mathbb R\rightarrow \mathbb R$, so $\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt$. Is the following correct $$ p(x)=\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt \tag 5 $$ where $p:\mathbb R\rightarrow \mathbb R$. Or with a constant $c\in\mathbb R$ $$ c=\int_{-\infty}^{\infty}\lvert y(t)\rvert ^2\, dt \tag 6 $$ Thanks!",,"['real-analysis', 'analysis']"
9,"If $f \in C([a, b],\mathbb{R})$ is differentiable on $(a,b]$ and $\lim_{x\to a}f'(x)$ exists, then $f\in C^1([a, b],\mathbb{R})$?","If  is differentiable on  and  exists, then ?","f \in C([a, b],\mathbb{R}) (a,b] \lim_{x\to a}f'(x) f\in C^1([a, b],\mathbb{R})","The following is exercise IV.2.3. from Analysis I by Amann and Escher. Let $-\infty < a < b < \infty$ and $f \in C([a, b],\mathbb{R})$ be differentiable on $(a, b]$. Show that, if $\lim_{x\to a} f'(x)$ exists, then $f$ is in $C^1([a, b],\mathbb{R})$ and $f'(a) = \lim_{x\to a} f'(x)$. ( Hint : Use the mean value theorem.) I think this is false and found a counterexample: Set $b=0$ and define $f:[a,0]\to\mathbb{R}$ by $$ f(x):= \begin{cases} x^2\sin(1/x),&x\in[-a,0);\\ 0,&x=0. \end{cases} $$ The function $f$ satifies all the hypotheses but $f'$ is not continuous at $x=0$. Am I right? Edit: My point is that $\lim_{x\to a} f'(x)$ does not imply $f\in C^1([a, b],\mathbb{R})$, or, roughly speaking, the continuity of $f'$ at $x=a$ does not imply its continuity on the whole interval. However, the second conclusion, $f'(a) = \lim_{x\to a} f'(x)$, is correct.","The following is exercise IV.2.3. from Analysis I by Amann and Escher. Let $-\infty < a < b < \infty$ and $f \in C([a, b],\mathbb{R})$ be differentiable on $(a, b]$. Show that, if $\lim_{x\to a} f'(x)$ exists, then $f$ is in $C^1([a, b],\mathbb{R})$ and $f'(a) = \lim_{x\to a} f'(x)$. ( Hint : Use the mean value theorem.) I think this is false and found a counterexample: Set $b=0$ and define $f:[a,0]\to\mathbb{R}$ by $$ f(x):= \begin{cases} x^2\sin(1/x),&x\in[-a,0);\\ 0,&x=0. \end{cases} $$ The function $f$ satifies all the hypotheses but $f'$ is not continuous at $x=0$. Am I right? Edit: My point is that $\lim_{x\to a} f'(x)$ does not imply $f\in C^1([a, b],\mathbb{R})$, or, roughly speaking, the continuity of $f'$ at $x=a$ does not imply its continuity on the whole interval. However, the second conclusion, $f'(a) = \lim_{x\to a} f'(x)$, is correct.",,"['real-analysis', 'analysis']"
10,Uniqueness of tempered fundamental solution (with support in half space) for the linear KdV equation,Uniqueness of tempered fundamental solution (with support in half space) for the linear KdV equation,,"Show that $E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$ is the unique tempered fundamental solution with support in $\{ (t,x) \mid t\geq 0 \}$ of the partial differential operator $P(D) = \partial_t + \frac{1}{3}\partial_x^3$. I am puzzled about an exercise problem from an undergraduate course about distributions and Fourier transfrom. The exercise concerns about the constant-coefficients 1+1 dimensional differential operator  $$P(D) = \partial_t + \frac{1}{3}\partial_x^3.$$  The first part of the exercise is to check that  $$E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$$ is a fundamental solution of $P(D)$, where $\textbf{Ai}(x) = \mathcal{F}^{-1}_{\xi \rightarrow x}(e^{i\xi^3/3})$ is the Airy function (it turns out that $\textbf{Ai}$ is $C^\infty$). This part is straightforward and I could work out. But the second part of the exercise asks to show that $u(t,x) = E(t,x)$ is the unique solution in the sense that $$  u \in \mathcal{S}'(\mathbb{R}^2_{t,x}) \\ (\partial_t + \frac{1}{3}\partial_x^3)u = \delta_{(0,0)}  \\  \textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$$ I could not work out. I have consulted Hormander's book  $$\textit{The Analysis of Linear Partial Differential Operators II}$$  and found some topics very close to it, mostly in Chap. XII ""The Cauchy and Mixed Problems"", but it seems that there is no theorem I could directly quote. $\textbf{My Question}$ Is there any general theorem about the uniqueness of tempered fundamental solutions of linear differential operators with constant coefficients? Is there any necessary or sufficient conditions such that $P(D)u = \delta$ has a unique tempered solution $u$, maybe with some requirement of the support $\textbf{supp}~u$ (for instance $\textbf{supp}~u \subset \text{half space}$ like above.) Does the requirement $\textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$  take the most important role so that actually this exercise is very easy?","Show that $E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$ is the unique tempered fundamental solution with support in $\{ (t,x) \mid t\geq 0 \}$ of the partial differential operator $P(D) = \partial_t + \frac{1}{3}\partial_x^3$. I am puzzled about an exercise problem from an undergraduate course about distributions and Fourier transfrom. The exercise concerns about the constant-coefficients 1+1 dimensional differential operator  $$P(D) = \partial_t + \frac{1}{3}\partial_x^3.$$  The first part of the exercise is to check that  $$E(t,x) = \frac{\textbf{1}_{(0,+\infty)}(t)}{t^{1/3}}\textbf{Ai}(\frac{x}{t^{1/3}})$$ is a fundamental solution of $P(D)$, where $\textbf{Ai}(x) = \mathcal{F}^{-1}_{\xi \rightarrow x}(e^{i\xi^3/3})$ is the Airy function (it turns out that $\textbf{Ai}$ is $C^\infty$). This part is straightforward and I could work out. But the second part of the exercise asks to show that $u(t,x) = E(t,x)$ is the unique solution in the sense that $$  u \in \mathcal{S}'(\mathbb{R}^2_{t,x}) \\ (\partial_t + \frac{1}{3}\partial_x^3)u = \delta_{(0,0)}  \\  \textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$$ I could not work out. I have consulted Hormander's book  $$\textit{The Analysis of Linear Partial Differential Operators II}$$  and found some topics very close to it, mostly in Chap. XII ""The Cauchy and Mixed Problems"", but it seems that there is no theorem I could directly quote. $\textbf{My Question}$ Is there any general theorem about the uniqueness of tempered fundamental solutions of linear differential operators with constant coefficients? Is there any necessary or sufficient conditions such that $P(D)u = \delta$ has a unique tempered solution $u$, maybe with some requirement of the support $\textbf{supp}~u$ (for instance $\textbf{supp}~u \subset \text{half space}$ like above.) Does the requirement $\textbf{supp}~u \subset \mathbb{R}_{t \geq 0} \times \mathbb{R}_x$  take the most important role so that actually this exercise is very easy?",,"['analysis', 'partial-differential-equations', 'fourier-analysis', 'distribution-theory']"
11,Is epsilon-delta continuity at a point equivalent to topological continuity at a point [duplicate],Is epsilon-delta continuity at a point equivalent to topological continuity at a point [duplicate],,"This question already has an answer here : Topological definition of continuity and its application to epsilon-delta definition? [duplicate] (1 answer) Closed 2 years ago . I was trying to prove the equivalence of the $\epsilon$-$\delta$ and topological notions of continuity at a point. (Given the standard topology on a metric space) I could get one direction, but the $\epsilon$-$\delta$ notion of continuity at a point doesn't seem to imply the topological notion of continuity at a point. (I think I might have messed up my definition) The definition of topological continuity at a point I was using was that a function $f$ is continuous as point $a$ if every open set in the image of $f$ which contain $f(a)$ has an open pre-image. Basically, the $\epsilon$-$\delta$ notion of continuity at point $a$ only says things about neighborhoods of $a$. But I can always union a neighborhood of $a$ with an open set in some other part of the image to get a new open set. And the $\epsilon$-$\delta$ definition gives me no information about this potentially distant set or its pre-image. In other words, take $f$ to map some open set $A$ to some open neighborhood $f(A)$ and some closed set $B$ to some open neighborhood $f(B)$ such that $f(A) \cap f(B)=\emptyset$. Also, let it be that f is $\epsilon$-$\delta$ continuous over all of $A$. So now, let's take some $a \in A$. $f$ is epsilon-delta continuous at $a$. But is it topologically-continuous at $a$? No. Because any open neighborhood of $f(a)$, I can union with $f(B)$ to get an open set whose pre-image is not an open set. I think my problem is that I got my topological definition of continuity at a point wrong. But I can't figure out how to fix it without invoking concepts from metric spaces.","This question already has an answer here : Topological definition of continuity and its application to epsilon-delta definition? [duplicate] (1 answer) Closed 2 years ago . I was trying to prove the equivalence of the $\epsilon$-$\delta$ and topological notions of continuity at a point. (Given the standard topology on a metric space) I could get one direction, but the $\epsilon$-$\delta$ notion of continuity at a point doesn't seem to imply the topological notion of continuity at a point. (I think I might have messed up my definition) The definition of topological continuity at a point I was using was that a function $f$ is continuous as point $a$ if every open set in the image of $f$ which contain $f(a)$ has an open pre-image. Basically, the $\epsilon$-$\delta$ notion of continuity at point $a$ only says things about neighborhoods of $a$. But I can always union a neighborhood of $a$ with an open set in some other part of the image to get a new open set. And the $\epsilon$-$\delta$ definition gives me no information about this potentially distant set or its pre-image. In other words, take $f$ to map some open set $A$ to some open neighborhood $f(A)$ and some closed set $B$ to some open neighborhood $f(B)$ such that $f(A) \cap f(B)=\emptyset$. Also, let it be that f is $\epsilon$-$\delta$ continuous over all of $A$. So now, let's take some $a \in A$. $f$ is epsilon-delta continuous at $a$. But is it topologically-continuous at $a$? No. Because any open neighborhood of $f(a)$, I can union with $f(B)$ to get an open set whose pre-image is not an open set. I think my problem is that I got my topological definition of continuity at a point wrong. But I can't figure out how to fix it without invoking concepts from metric spaces.",,"['general-topology', 'analysis', 'continuity']"
12,"How can I show the function below is bounded (In particular, less or equal than $\frac{1}{2}$)?","How can I show the function below is bounded (In particular, less or equal than )?",\frac{1}{2},"$$f: \mathbb R \rightarrow \mathbb R \quad with \quad f(\theta) =\begin{cases} \frac{1}{\theta^2}log\left\{ \frac{e^{\theta} + e^{-\theta}}{2} \right\}, \theta \neq 0 \\ 1/2, \theta = 0 \end{cases}.$$ My attempts: 1rst: I tried to proove f is differentiable at $x=0,$ but the computations grew really fast. My plan was to show the derivative at zero is zero; 2nd: I differentiate $f$ for $\theta \neq 0.$ I was looking forward proving it is decreasing for $\theta > 0$ and increasing for $\theta < 0.$ But I couldn't deal with the inequalities that emerged; 3rd: I tried to use a reductio ad absurdum argument, but I couldn't conclude the absurd I was searching for; 4th: since a injective continuous function must be monotone, I came up with the idea of proving $f$ is injective in $(0, \infty)$ and $(-\infty,0).$ But the expressions wered really tough to struggle with.","$$f: \mathbb R \rightarrow \mathbb R \quad with \quad f(\theta) =\begin{cases} \frac{1}{\theta^2}log\left\{ \frac{e^{\theta} + e^{-\theta}}{2} \right\}, \theta \neq 0 \\ 1/2, \theta = 0 \end{cases}.$$ My attempts: 1rst: I tried to proove f is differentiable at $x=0,$ but the computations grew really fast. My plan was to show the derivative at zero is zero; 2nd: I differentiate $f$ for $\theta \neq 0.$ I was looking forward proving it is decreasing for $\theta > 0$ and increasing for $\theta < 0.$ But I couldn't deal with the inequalities that emerged; 3rd: I tried to use a reductio ad absurdum argument, but I couldn't conclude the absurd I was searching for; 4th: since a injective continuous function must be monotone, I came up with the idea of proving $f$ is injective in $(0, \infty)$ and $(-\infty,0).$ But the expressions wered really tough to struggle with.",,"['calculus', 'real-analysis', 'analysis', 'continuity', 'monotone-functions']"
13,Showing a twice differentiable function with positive hessian is convex.,Showing a twice differentiable function with positive hessian is convex.,,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be twice differentiable . If $f_{xx}( .)\geq 0$ for any $x\in\mathbb{R}^n$, then $f$ is convex. I think they mean $f_{xx}(.)$ is a positive matrix. As, $f$ is twice differentiable, we can see for any $x, y\in \mathbb{R}^n$, there exist $\theta_1, \theta_2\in (0,1)$, satisfying  $$f(y)=f(x)+f_x(x).(y-x)+\theta_1(y-x)^Tf_{xx}(x+\theta_1\theta_2(y-x)).(y-x).$$ Hence $f(y)-f(x)\geq f_x(x).(y-x).$ Using the fact $f_x(x).(y-x)=\lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}$, I get $$f(y)-f(x)\geq \lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}.$$ Now if I don't consider the $\lim_{\lambda\to1}$, then by some calculation, I can get $$\lambda f(x)-(1-\lambda)f(y)\geq f(\lambda x+(1-\lambda)y).$$ However, I'm not sure if this is a valid thought. Thank you.","Let $f:\mathbb{R}^n\to\mathbb{R}$ be twice differentiable . If $f_{xx}( .)\geq 0$ for any $x\in\mathbb{R}^n$, then $f$ is convex. I think they mean $f_{xx}(.)$ is a positive matrix. As, $f$ is twice differentiable, we can see for any $x, y\in \mathbb{R}^n$, there exist $\theta_1, \theta_2\in (0,1)$, satisfying  $$f(y)=f(x)+f_x(x).(y-x)+\theta_1(y-x)^Tf_{xx}(x+\theta_1\theta_2(y-x)).(y-x).$$ Hence $f(y)-f(x)\geq f_x(x).(y-x).$ Using the fact $f_x(x).(y-x)=\lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}$, I get $$f(y)-f(x)\geq \lim_{\lambda\to1}\frac{f(\lambda x+(1-\lambda)y)-f(x)}{1-\lambda}.$$ Now if I don't consider the $\lim_{\lambda\to1}$, then by some calculation, I can get $$\lambda f(x)-(1-\lambda)f(y)\geq f(\lambda x+(1-\lambda)y).$$ However, I'm not sure if this is a valid thought. Thank you.",,"['analysis', 'proof-verification', 'convex-analysis', 'hessian-matrix']"
14,For which $x \in \mathbb{R}$ does the following series converge?,For which  does the following series converge?,x \in \mathbb{R},$$\sum_{n=1}^\infty \frac{3x^n}{2+x^{4n}}$$ I think the solution is $|x| \neq 1$ But I don't really know how to prove it rigorously. Thanks in advance :),$$\sum_{n=1}^\infty \frac{3x^n}{2+x^{4n}}$$ I think the solution is $|x| \neq 1$ But I don't really know how to prove it rigorously. Thanks in advance :),,"['sequences-and-series', 'analysis', 'convergence-divergence', 'divergent-series']"
15,Subadditive Sequence Convergence,Subadditive Sequence Convergence,,"Given:  A sequence ($a_n$) is called subadditive if $a_{m+n}$ ≤ $a_m$ + $a_n$ for all m, n ∈ N. Prove that if ($a_n$) is a subadditive sequence of positive real numbers, then $(\frac{a_n}{n})$ converges. I am unaware of how to start this proof, but I know that it I need to show that $\frac{a_n}{n}$ --> inf{$\frac{a_k}{k}$: k $\geq$ 1}. Any help would be appreciated.","Given:  A sequence ($a_n$) is called subadditive if $a_{m+n}$ ≤ $a_m$ + $a_n$ for all m, n ∈ N. Prove that if ($a_n$) is a subadditive sequence of positive real numbers, then $(\frac{a_n}{n})$ converges. I am unaware of how to start this proof, but I know that it I need to show that $\frac{a_n}{n}$ --> inf{$\frac{a_k}{k}$: k $\geq$ 1}. Any help would be appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis']"
16,Clamped Cubic Spline : interpolates f?,Clamped Cubic Spline : interpolates f?,,"Given the partition $x_0=0,x_1=.05,x_2=.1 \text{ of } [0,0.1] \text{ and } f(x) = e^{2x}: \\ \text{Find the cubic spline s with clamped boundary condition that interpolates f.} $ The first thing I did, to solve this question is create a table with $x,f(x),f'(x)$ and try to find x. $$ \begin{array}{c|lcr} x & \text{f(x)} & \text{f'(x)} & \\ \hline 0 & 1& 2  \\ .05 & 1.1052 & 2.2103\\ .1 & 1.2214 & 2.4428 \end{array} $$ Then I tried to find matrix A and matrix B then solve $AX=B$ but this is where I ran into some problems. $h_0= .05-0=.05 ;h_1= .1-.05=.05$ $$A=      \begin{bmatrix}     2(.05) & .05 & 0 \\     .05 & 2(.05+.05) & .05 \\     0 & .05 & 2(.05+.05) \\     \end{bmatrix} =  \begin{bmatrix}     .1 & .05 & 0 \\     .05 & .2 & .05 \\     0 & .05 & .2 \\     \end{bmatrix} $$ $$b=  \begin{bmatrix}     \frac{3}{.05}(1.105-1)-3(2)=.312 &  \\     \frac{3}{.05}(1.12214-1)-\frac{3}{.05}(1.1052-1)=1.0164 &  \\     3(2.4428)-\frac{3}{.05}(1.2214-1.1052=-.35651 &  \\     \end{bmatrix} $$ The answer is supposed to be $$x=  \begin{bmatrix}    1.998302 &  \\     2.208498&  \\     2.4406449  \\     \end{bmatrix} $$ Yet when I do $Ax=b$ I get $$x=  \begin{bmatrix}    .748&  \\     4.7&  \\     .59\\     \end{bmatrix} $$ Does anyone know how to solve this?","Given the partition $x_0=0,x_1=.05,x_2=.1 \text{ of } [0,0.1] \text{ and } f(x) = e^{2x}: \\ \text{Find the cubic spline s with clamped boundary condition that interpolates f.} $ The first thing I did, to solve this question is create a table with $x,f(x),f'(x)$ and try to find x. $$ \begin{array}{c|lcr} x & \text{f(x)} & \text{f'(x)} & \\ \hline 0 & 1& 2  \\ .05 & 1.1052 & 2.2103\\ .1 & 1.2214 & 2.4428 \end{array} $$ Then I tried to find matrix A and matrix B then solve $AX=B$ but this is where I ran into some problems. $h_0= .05-0=.05 ;h_1= .1-.05=.05$ $$A=      \begin{bmatrix}     2(.05) & .05 & 0 \\     .05 & 2(.05+.05) & .05 \\     0 & .05 & 2(.05+.05) \\     \end{bmatrix} =  \begin{bmatrix}     .1 & .05 & 0 \\     .05 & .2 & .05 \\     0 & .05 & .2 \\     \end{bmatrix} $$ $$b=  \begin{bmatrix}     \frac{3}{.05}(1.105-1)-3(2)=.312 &  \\     \frac{3}{.05}(1.12214-1)-\frac{3}{.05}(1.1052-1)=1.0164 &  \\     3(2.4428)-\frac{3}{.05}(1.2214-1.1052=-.35651 &  \\     \end{bmatrix} $$ The answer is supposed to be $$x=  \begin{bmatrix}    1.998302 &  \\     2.208498&  \\     2.4406449  \\     \end{bmatrix} $$ Yet when I do $Ax=b$ I get $$x=  \begin{bmatrix}    .748&  \\     4.7&  \\     .59\\     \end{bmatrix} $$ Does anyone know how to solve this?",,"['linear-algebra', 'analysis', 'numerical-methods', 'algorithms']"
17,Prove Riemann Integral is Zero,Prove Riemann Integral is Zero,,"$\mathbf{Theorem}: \ $ If $S\subset\mathbb{R}^N$ is a non-empty measurable set, with Jordan measure (content) zero, and $f:S\mapsto \mathbb{R}^M$ is a bounded function, then the Riemann integral, $\int_Sf=0$ $\mathbf{Lemma \ 1}:$ A set $S\subset \mathbb{R}^N$ has Jordan measure zero, if for each $\epsilon>0$ , there are compact intervals $I_1,...,I_n\subset \mathbb{R}^N$ with, $$S\subset \bigcup_{i=1}^nI_i \ \ and \ \ \sum_{i=1}^n\mu(I_j)<\epsilon_0$$ Where $\mu(I)$ denotes the Jordan measure of the set $I$ . $\mathbf{Proof:}$ Suppose $S\subset \mathbb{R}^N$ is a non-empty measurable set, with Jordan measure zero, and $f:S\mapsto \mathbb{R}^M$ is a bounded function. By $\mathbf{Lemma \ 1}$ , a set $S\subset \mathbb{R}^N$ has Jordan measure zero, if for each $\epsilon>0$ , there exists compact intervals $\{I_i\}_{I=1}^N$ , such that $S\subset \bigcup_{I=1}^NI_i$ and $\sum_{I=1}^N\mu(I_i)<\epsilon_0$ . Then for $\epsilon_0=\frac {\epsilon}{sup(f)}$ , clearly, $$\bigg|\int_Sf\bigg|\leq\bigg|\int_{\bigcup_{I=1}^NI_i}f \bigg|\leq|sup(f)|\sum_{i=1}^N\mu(I_i)<\epsilon$$ That is, $\int_Sf=0$ . Is this a sufficient proof for the above theorem, or am I doing something wrong? Can anyone provide any feedback, or a proper proof if this isn't right? Thanks in advance!","If is a non-empty measurable set, with Jordan measure (content) zero, and is a bounded function, then the Riemann integral, A set has Jordan measure zero, if for each , there are compact intervals with, Where denotes the Jordan measure of the set . Suppose is a non-empty measurable set, with Jordan measure zero, and is a bounded function. By , a set has Jordan measure zero, if for each , there exists compact intervals , such that and . Then for , clearly, That is, . Is this a sufficient proof for the above theorem, or am I doing something wrong? Can anyone provide any feedback, or a proper proof if this isn't right? Thanks in advance!","\mathbf{Theorem}: \  S\subset\mathbb{R}^N f:S\mapsto \mathbb{R}^M \int_Sf=0 \mathbf{Lemma \ 1}: S\subset \mathbb{R}^N \epsilon>0 I_1,...,I_n\subset \mathbb{R}^N S\subset \bigcup_{i=1}^nI_i \ \ and \ \ \sum_{i=1}^n\mu(I_j)<\epsilon_0 \mu(I) I \mathbf{Proof:} S\subset \mathbb{R}^N f:S\mapsto \mathbb{R}^M \mathbf{Lemma \ 1} S\subset \mathbb{R}^N \epsilon>0 \{I_i\}_{I=1}^N S\subset \bigcup_{I=1}^NI_i \sum_{I=1}^N\mu(I_i)<\epsilon_0 \epsilon_0=\frac {\epsilon}{sup(f)} \bigg|\int_Sf\bigg|\leq\bigg|\int_{\bigcup_{I=1}^NI_i}f \bigg|\leq|sup(f)|\sum_{i=1}^N\mu(I_i)<\epsilon \int_Sf=0","['analysis', 'measure-theory', 'multivariable-calculus', 'riemann-integration']"
18,Finding limit of a continuous function,Finding limit of a continuous function,,"I have difficulty in calculating the following limit. Let $f$ be a continuous function on $[-1,1]$, it is desired to find the following limit, $\lim_{n\rightarrow \infty}n \int_{-\frac{1}{n}}^{\frac{1}{n}} f(x)(1-n|x|)dx$ Thank you very much in advance.","I have difficulty in calculating the following limit. Let $f$ be a continuous function on $[-1,1]$, it is desired to find the following limit, $\lim_{n\rightarrow \infty}n \int_{-\frac{1}{n}}^{\frac{1}{n}} f(x)(1-n|x|)dx$ Thank you very much in advance.",,"['real-analysis', 'analysis']"
19,Why do we need an upper bound for $|x+5|$ when $x$ is close to $3$?,Why do we need an upper bound for  when  is close to ?,|x+5| x 3,"Show that $\lim_{x\to 3}$ $x^2 + 2x + 6 = 21$ using the $\epsilon-\delta$ definition. The following is part of the proof to the above statement. proof . We have to show that given any $\epsilon>0$, there exists some $\delta>0$ such that $|f(x)-21|<\epsilon$ whenever $0<|x-3|<\delta$. Working backwards: $|f(x)-21|=|x^2+2x-15|=|(x+5)(x-3)|=|x+5||x-3|<|x+5|\delta$, if $|x-3|<\delta.$ We need an upper bound for $|x+5|$ when $x$ is close to $3$. If $|x-3|<1$, then $2<x<4$, so that $7<x+5<9$, so $|f(x)-21|=|x+5||x-3|<9|x-3|<9\delta$. My question is why do we need an upper bound for $|x+5|$ when $x$ is close to $3$?","Show that $\lim_{x\to 3}$ $x^2 + 2x + 6 = 21$ using the $\epsilon-\delta$ definition. The following is part of the proof to the above statement. proof . We have to show that given any $\epsilon>0$, there exists some $\delta>0$ such that $|f(x)-21|<\epsilon$ whenever $0<|x-3|<\delta$. Working backwards: $|f(x)-21|=|x^2+2x-15|=|(x+5)(x-3)|=|x+5||x-3|<|x+5|\delta$, if $|x-3|<\delta.$ We need an upper bound for $|x+5|$ when $x$ is close to $3$. If $|x-3|<1$, then $2<x<4$, so that $7<x+5<9$, so $|f(x)-21|=|x+5||x-3|<9|x-3|<9\delta$. My question is why do we need an upper bound for $|x+5|$ when $x$ is close to $3$?",,"['analysis', 'continuity']"
20,Countable discontinuities and pointwise convergence,Countable discontinuities and pointwise convergence,,"If $\{f_n\}$ converges pointwise to $f$ and each $\{f_n\}$ has a countable number of discontinuities, what is a counterexample to $f$ also having a countable number of discontinuities?","If $\{f_n\}$ converges pointwise to $f$ and each $\{f_n\}$ has a countable number of discontinuities, what is a counterexample to $f$ also having a countable number of discontinuities?",,"['real-analysis', 'analysis']"
21,What's the next logical step after studying multivariable/vector calculus?,What's the next logical step after studying multivariable/vector calculus?,,"As part of an engineering degree, I've taken courses on single variable calculus, linear algebra, multivariable/vector calculus and ordinary differential equations. I'm also about to study an advanced engineering mathematics course, which covers three topics (albeit quite superficially): Complex variable calculus, fourier series/transform and partial differential equations. Seeing how multivariable/vector calulus generalizes concepts like limits, derivatives and integrals in elegant and beautiful ways, and introduces new concepts like vector fields that feel natural and intuitive, I was wondering if there's a branch in mathematical analysis that does the same thing, extending multivariable calculus to something else, which I would be able to study by myself using the knowledge I have from the courses previously mentioned. I've heard about things like tensor calculus, calculus of variations and differential geometry, but I'm not sure if any of those would be what I'm looking for (a logical next step form multivariable calculus). Any recommendations on books for self-studying the subject/subjects you consider appropiate are greatly appreciated.","As part of an engineering degree, I've taken courses on single variable calculus, linear algebra, multivariable/vector calculus and ordinary differential equations. I'm also about to study an advanced engineering mathematics course, which covers three topics (albeit quite superficially): Complex variable calculus, fourier series/transform and partial differential equations. Seeing how multivariable/vector calulus generalizes concepts like limits, derivatives and integrals in elegant and beautiful ways, and introduces new concepts like vector fields that feel natural and intuitive, I was wondering if there's a branch in mathematical analysis that does the same thing, extending multivariable calculus to something else, which I would be able to study by myself using the knowledge I have from the courses previously mentioned. I've heard about things like tensor calculus, calculus of variations and differential geometry, but I'm not sure if any of those would be what I'm looking for (a logical next step form multivariable calculus). Any recommendations on books for self-studying the subject/subjects you consider appropiate are greatly appreciated.",,"['analysis', 'multivariable-calculus', 'self-learning']"
22,Interpretation of nabla followed by a dot (which is not meant as divergence),Interpretation of nabla followed by a dot (which is not meant as divergence),,"I am studying a paper and the authors use $\nabla \cdot u$ for a real valued function $u\colon \mathbb{R}^n \to \mathbb{R}$ and I am quite confused how to interpret that, even after long searches in books and the internet. They use this construct also for other real valued functions at other positions in the paper, therefore I think its not a mistake. Can anyone see what is meant by that(I have no background in physics, maybe there is such a notation in physics)? Since $u$ is real valued I hardly can interpret it as divergence.","I am studying a paper and the authors use $\nabla \cdot u$ for a real valued function $u\colon \mathbb{R}^n \to \mathbb{R}$ and I am quite confused how to interpret that, even after long searches in books and the internet. They use this construct also for other real valued functions at other positions in the paper, therefore I think its not a mistake. Can anyone see what is meant by that(I have no background in physics, maybe there is such a notation in physics)? Since $u$ is real valued I hardly can interpret it as divergence.",,"['calculus', 'analysis', 'physics', 'vector-analysis', 'divergence-operator']"
23,Doubt about double limit definition.,Doubt about double limit definition.,,"Let $(a_n)_{n \in \mathbb{N}}$ a sequence. We say that the $\lim_{n \rightarrow \infty} a_n = L$, if for every $\varepsilon >0$ given, there exists $n_0 (\varepsilon) \in \mathbb{N}$, such that $$n > n_{0}(\varepsilon) \Rightarrow |a_n - L| < \varepsilon $$ Consider $g: \mathbb{N}\times \mathbb{N} \rightarrow \mathbb{R}$, I would like to know how I would write $$\lim_{n \rightarrow \infty}\left( \lim_{k \rightarrow \infty} g(n,k)\right) = L $$ in terms of the $\varepsilon$-$\delta$ language. My progress First, I defined $\bar{g}(n)$ as $ \bar{g}(n) := \lim_{k\rightarrow \infty} g(n,k) $ . So fixing $n \in \mathbb{N}$, for every $\varepsilon/2 >0$ exists $k(n,\varepsilon) \in \mathbb{N}$, satisfying $$k > k(n,\varepsilon) \Rightarrow |g(n,k) - \bar{g}(n)| < \varepsilon/2.$$ On the other hand $\lim_{n \rightarrow \infty} \bar{g}(n) = L$. Consequently for every $\varepsilon/2 >0,$ exists $n_0(\varepsilon) \in \mathbb{N},$ satisfying $$n > n_0(\varepsilon) \Rightarrow |\bar{g}(n) - L| < \varepsilon/2 $$ Therefore, joining the two results above we have that the definition of the double limit would be: For every $\varepsilon>0,$ exists $n_0(\varepsilon)$ $\in$ $\mathbb{N}$, and for every $n> n_0(\varepsilon)$, exists $k(n,\varepsilon)$ $\in$ $\mathbb{N}$, such that $$n> n_0(\varepsilon),\hspace{0.1cm} k>k(n,\varepsilon) \Rightarrow |g(n,k) - L| < \varepsilon.  $$ Is this  correct? I'm not confident with my result. Thanks in advance","Let $(a_n)_{n \in \mathbb{N}}$ a sequence. We say that the $\lim_{n \rightarrow \infty} a_n = L$, if for every $\varepsilon >0$ given, there exists $n_0 (\varepsilon) \in \mathbb{N}$, such that $$n > n_{0}(\varepsilon) \Rightarrow |a_n - L| < \varepsilon $$ Consider $g: \mathbb{N}\times \mathbb{N} \rightarrow \mathbb{R}$, I would like to know how I would write $$\lim_{n \rightarrow \infty}\left( \lim_{k \rightarrow \infty} g(n,k)\right) = L $$ in terms of the $\varepsilon$-$\delta$ language. My progress First, I defined $\bar{g}(n)$ as $ \bar{g}(n) := \lim_{k\rightarrow \infty} g(n,k) $ . So fixing $n \in \mathbb{N}$, for every $\varepsilon/2 >0$ exists $k(n,\varepsilon) \in \mathbb{N}$, satisfying $$k > k(n,\varepsilon) \Rightarrow |g(n,k) - \bar{g}(n)| < \varepsilon/2.$$ On the other hand $\lim_{n \rightarrow \infty} \bar{g}(n) = L$. Consequently for every $\varepsilon/2 >0,$ exists $n_0(\varepsilon) \in \mathbb{N},$ satisfying $$n > n_0(\varepsilon) \Rightarrow |\bar{g}(n) - L| < \varepsilon/2 $$ Therefore, joining the two results above we have that the definition of the double limit would be: For every $\varepsilon>0,$ exists $n_0(\varepsilon)$ $\in$ $\mathbb{N}$, and for every $n> n_0(\varepsilon)$, exists $k(n,\varepsilon)$ $\in$ $\mathbb{N}$, such that $$n> n_0(\varepsilon),\hspace{0.1cm} k>k(n,\varepsilon) \Rightarrow |g(n,k) - L| < \varepsilon.  $$ Is this  correct? I'm not confident with my result. Thanks in advance",,"['real-analysis', 'analysis', 'proof-verification', 'definition']"
24,Existence of the limit of a certain integral,Existence of the limit of a certain integral,,"Let $f_n:[0,1]\to\mathbb{R}$ be a sequence continuous functions which uniformly converges to continuous function $f:[0,1]\to\mathbb{R}$ . Let $$F_n=\int_0^1\frac{\sin(nx)}{2+\cos(nx)}f_n(x)dx,\:\:\:n\geq 1.$$ Is $F_n$ necessarily convergent? As $f_n\to f$ uniformly, the sequence is uniformly bounded so $\frac{\sin(nx)}{2+\cos(nx)}f_n(x)$ is bounded. I'm stuck on finding the limit of $\frac{\sin(nx)}{2+\cos(nx)}f_n(x)$ , if it has a Riemann integrable limit, then by Dominated convergent theorem, I can pass the limit inside the integral, I guess! However, I'm not sure about using the dominated convergent theorem I appreciate any help.","Let be a sequence continuous functions which uniformly converges to continuous function . Let Is necessarily convergent? As uniformly, the sequence is uniformly bounded so is bounded. I'm stuck on finding the limit of , if it has a Riemann integrable limit, then by Dominated convergent theorem, I can pass the limit inside the integral, I guess! However, I'm not sure about using the dominated convergent theorem I appreciate any help.","f_n:[0,1]\to\mathbb{R} f:[0,1]\to\mathbb{R} F_n=\int_0^1\frac{\sin(nx)}{2+\cos(nx)}f_n(x)dx,\:\:\:n\geq 1. F_n f_n\to f \frac{\sin(nx)}{2+\cos(nx)}f_n(x) \frac{\sin(nx)}{2+\cos(nx)}f_n(x)","['real-analysis', 'integration', 'analysis', 'lebesgue-integral', 'riemann-integration']"
25,Prove by counterexample that a bounded sequence in a metric space need not have a convergent subsequence,Prove by counterexample that a bounded sequence in a metric space need not have a convergent subsequence,,"I am trying to prove that a bounded sequence in a metric space need not have a convergent subsequence. My counterexample is: Consider the metric space:  ($(0,1]$,standard norm on $\Bbb R$ restrict to $(0,1]$) and the sequence $1,\frac{1}{2},\frac{1}{3},...$ which is bounded. As $0\notin (0,1]$,the sequence is not convergent. I thought about proving that if the sequence has a convergent subsequence, it must converge to $0$, then as $0\notin (0,1]$, we have a contradiction. Here is my attempt: Suppose, in order to get a contradiction, that a subsequence of the sequence $x_n=\frac{1}{n}$, call it $(x_{n_j})$, is a convergent subsequence. Suppose it converges to $a\in (0,1]$, then from the definition of convergence: $(\exists a\in (0,1])(\forall \epsilon>0)(\exists N\in \Bbb N)(j\ge N\implies |x_{n_j}-a|<\epsilon)$ To obtain the contradiction, we prove that for this $a$: $(\exists \epsilon>0)(\forall N\in \Bbb N)(\exists j\ge N\land |x_{n_j}-a|\ge\epsilon)$ Proof: We know that $\frac{1}{n}$ can be less then any $a>0$ if we take $n$ large, so we have a term $x_{n_l}$ of $(x_{n_j})$ such that $x_{n_l}<a$. Set $\epsilon=a-x_{n_l}$, then for all $N\in \Bbb N$, take $j\ge l$, then we have $|a-x_{n_j}|>\epsilon$ since the sequence $\frac{1}{n}$ is decreasing. Could some please check if the argument above is correct? Thanks in advance!","I am trying to prove that a bounded sequence in a metric space need not have a convergent subsequence. My counterexample is: Consider the metric space:  ($(0,1]$,standard norm on $\Bbb R$ restrict to $(0,1]$) and the sequence $1,\frac{1}{2},\frac{1}{3},...$ which is bounded. As $0\notin (0,1]$,the sequence is not convergent. I thought about proving that if the sequence has a convergent subsequence, it must converge to $0$, then as $0\notin (0,1]$, we have a contradiction. Here is my attempt: Suppose, in order to get a contradiction, that a subsequence of the sequence $x_n=\frac{1}{n}$, call it $(x_{n_j})$, is a convergent subsequence. Suppose it converges to $a\in (0,1]$, then from the definition of convergence: $(\exists a\in (0,1])(\forall \epsilon>0)(\exists N\in \Bbb N)(j\ge N\implies |x_{n_j}-a|<\epsilon)$ To obtain the contradiction, we prove that for this $a$: $(\exists \epsilon>0)(\forall N\in \Bbb N)(\exists j\ge N\land |x_{n_j}-a|\ge\epsilon)$ Proof: We know that $\frac{1}{n}$ can be less then any $a>0$ if we take $n$ large, so we have a term $x_{n_l}$ of $(x_{n_j})$ such that $x_{n_l}<a$. Set $\epsilon=a-x_{n_l}$, then for all $N\in \Bbb N$, take $j\ge l$, then we have $|a-x_{n_j}|>\epsilon$ since the sequence $\frac{1}{n}$ is decreasing. Could some please check if the argument above is correct? Thanks in advance!",,"['real-analysis', 'sequences-and-series', 'analysis', 'metric-spaces']"
26,"Show that $\int_0^\pi g^2\,dx \leq \int_0^\pi(g')^2\,dx +(\int_0^\pi g\,dx)^2$",Show that,"\int_0^\pi g^2\,dx \leq \int_0^\pi(g')^2\,dx +(\int_0^\pi g\,dx)^2","Show that $\int_0^\pi g^2\,dx \leq \int_0^\pi (g')^2\,dx +(\int_0^\pi g\,dx)^2$, where $g \in H^1 (0,\pi)$ and $H$ means Sobolev space here. I want to apply Poincare inequality for ball here since I noticed that $\int_0^\pi[g^2 - \frac 1 \pi(\int_0^\pi g\,dx)^2]\,dx \leq \int_0^\pi (g')^2\,dx$,which looks like the Poincare inequality in  a ball, but still not quite. Any hints would be appreciated.","Show that $\int_0^\pi g^2\,dx \leq \int_0^\pi (g')^2\,dx +(\int_0^\pi g\,dx)^2$, where $g \in H^1 (0,\pi)$ and $H$ means Sobolev space here. I want to apply Poincare inequality for ball here since I noticed that $\int_0^\pi[g^2 - \frac 1 \pi(\int_0^\pi g\,dx)^2]\,dx \leq \int_0^\pi (g')^2\,dx$,which looks like the Poincare inequality in  a ball, but still not quite. Any hints would be appreciated.",,"['real-analysis', 'analysis', 'inequality', 'partial-differential-equations', 'integral-inequality']"
27,Continuity of the optimal solution when the objective function is convex and separable,Continuity of the optimal solution when the objective function is convex and separable,,"I have a very simple class of optimization problems. The objective function is separable with each part being strictly (or even strongly) convex. Furthermore, there's a solitary linear constraint: the sum of the variables is $b$. The variables must all be non-negative. Can we say that the optimal solution, denoted $x(b)$, is a continuous function of the scalar $b$? minimize $\sum f_i(x_i)$ subject to: $\sum x_i = b$ with: $x_i \ge 0.$ If necessary, assume that there exists $m>0$ such that $f_i''(t) > m$ for all $i$ and for all $t \ge 0$.","I have a very simple class of optimization problems. The objective function is separable with each part being strictly (or even strongly) convex. Furthermore, there's a solitary linear constraint: the sum of the variables is $b$. The variables must all be non-negative. Can we say that the optimal solution, denoted $x(b)$, is a continuous function of the scalar $b$? minimize $\sum f_i(x_i)$ subject to: $\sum x_i = b$ with: $x_i \ge 0.$ If necessary, assume that there exists $m>0$ such that $f_i''(t) > m$ for all $i$ and for all $t \ge 0$.",,"['analysis', 'optimization', 'convex-analysis', 'convex-optimization']"
28,Soft question: Why use optimization algorithms instead of calculus methods?,Soft question: Why use optimization algorithms instead of calculus methods?,,"I know that some functions are multivariate and therefore take a long time to compute zeros for the gradient, and then test for minima, maxima, etc. However, some optimization algorithms are not tractable either, and numerically they are never analytical and they don't give an analytic solution. So what is the purpose of not simply using the analytical methods we have from calculus eg solving gradient zeros vs possibly incorrect and at best approximate optimization algorithms?","I know that some functions are multivariate and therefore take a long time to compute zeros for the gradient, and then test for minima, maxima, etc. However, some optimization algorithms are not tractable either, and numerically they are never analytical and they don't give an analytic solution. So what is the purpose of not simply using the analytical methods we have from calculus eg solving gradient zeros vs possibly incorrect and at best approximate optimization algorithms?",,"['calculus', 'analysis', 'numerical-methods', 'algorithms', 'numerical-optimization']"
29,$\lim_{m \to \infty}{\prod_{i=0}^{tm-1}(1+\frac{r(i)}{m})} = e^{\int_{0}^{t} r(s)ds}$?,?,\lim_{m \to \infty}{\prod_{i=0}^{tm-1}(1+\frac{r(i)}{m})} = e^{\int_{0}^{t} r(s)ds},"How can I deduce the following equation ?: $$ \lim_{m \to \infty}\prod_{i = 0}^{{\large tm - 1}} \left[\,1 + \frac{r\left(\,i\,\right)}{m}\,\right] = \exp\left(\,\int_{0}^{t}r\left(\,s\,\right)\,\mathrm{d}s\,\right) $$ where $t > 0$ and $r\left(\,i\,\right)$ a real valued function. I can figure out the series expansion of $\mathrm{e}$, and it makes somehow sense as you can think of $r$ as the average ""rate of interest"" for example, but what exactly is the math behind it ?.","How can I deduce the following equation ?: $$ \lim_{m \to \infty}\prod_{i = 0}^{{\large tm - 1}} \left[\,1 + \frac{r\left(\,i\,\right)}{m}\,\right] = \exp\left(\,\int_{0}^{t}r\left(\,s\,\right)\,\mathrm{d}s\,\right) $$ where $t > 0$ and $r\left(\,i\,\right)$ a real valued function. I can figure out the series expansion of $\mathrm{e}$, and it makes somehow sense as you can think of $r$ as the average ""rate of interest"" for example, but what exactly is the math behind it ?.",,"['analysis', 'exponential-function']"
30,Rearrangement of series in a (not necessarily Banach) normed vector space.,Rearrangement of series in a (not necessarily Banach) normed vector space.,,"If $X$ is a Banach space, then the following theorem holds: Let $\sum x_n$ be a series in $X$ which converges absolutely. Then every rearrangement $\sum x_{\sigma(n)}$ converges, and they all converge to the same value. Proof: Let $(s_n')$ be the sequence of the partial sums of $\sum x_{\sigma(n)}$ . Since $\sum x_n$ is absolutely convergent, given $\epsilon>0$ there is an integer $n_0$ such that $$\sum_{k=n_0}^m ||x_k|| <\epsilon$$ for all $m\geq n_0$ . Let $$p=\max_{1\leq i< n_0}\sigma^{-1}(i).$$ If $n>p$ , we have that $\{1,2,\dotsc,n_0-1\}$ is a subset of $\{\sigma(1),\sigma(2),\dotsc,\sigma(n)\}$ . Hence all the $x_i$ for $i=1,2,\dots,n_0-1$ are cancelled in $s_n-s_n'$ . So, $$||s_n-s_n'||\leq \sum_{k=n_0}^m ||x_k|| <\epsilon.$$ We conclude that $(s_n')$ converges to the same value as $(s_n)$ . However, if $X$ is a normed vector space which is not necessarily Banach, is any of the following true? If $\sum x_n$ converges absolutely, then any rearrangement converges absolutely. or If $\sum x_n$ is a convergent series which converges absolutely, then any rearrangement converges absolutely. or If $\sum x_n$ is a convergent series which converges absolutely, then any rearrangement converges. If so, it's possible to modify my proof to handle the more general result?","If is a Banach space, then the following theorem holds: Let be a series in which converges absolutely. Then every rearrangement converges, and they all converge to the same value. Proof: Let be the sequence of the partial sums of . Since is absolutely convergent, given there is an integer such that for all . Let If , we have that is a subset of . Hence all the for are cancelled in . So, We conclude that converges to the same value as . However, if is a normed vector space which is not necessarily Banach, is any of the following true? If converges absolutely, then any rearrangement converges absolutely. or If is a convergent series which converges absolutely, then any rearrangement converges absolutely. or If is a convergent series which converges absolutely, then any rearrangement converges. If so, it's possible to modify my proof to handle the more general result?","X \sum x_n X \sum x_{\sigma(n)} (s_n') \sum x_{\sigma(n)} \sum x_n \epsilon>0 n_0 \sum_{k=n_0}^m ||x_k|| <\epsilon m\geq n_0 p=\max_{1\leq i< n_0}\sigma^{-1}(i). n>p \{1,2,\dotsc,n_0-1\} \{\sigma(1),\sigma(2),\dotsc,\sigma(n)\} x_i i=1,2,\dots,n_0-1 s_n-s_n' ||s_n-s_n'||\leq \sum_{k=n_0}^m ||x_k|| <\epsilon. (s_n') (s_n) X \sum x_n \sum x_n \sum x_n",['analysis']
31,"A series that gives inconclusive results by the root & ratio tests, but converges STRONGLY.","A series that gives inconclusive results by the root & ratio tests, but converges STRONGLY.",,"I'm trying to come up with a positive sequence $\{a_n\}_1^\infty$ such that $\lim_{n\to\infty} \left(\sqrt[\leftroot{-2}\uproot{2}n]{a_n}\right) = \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = 1$, but $\forall \alpha > 0$ the series $\sum_1^\infty a_n n^\alpha$ converges. I know $1/n^n$ goes to zero faster than $n^\alpha$ goes to infinity, but it converges by the two tests. I've tried screwing around with $1/n^n$ but had no luck. Any thoughts?","I'm trying to come up with a positive sequence $\{a_n\}_1^\infty$ such that $\lim_{n\to\infty} \left(\sqrt[\leftroot{-2}\uproot{2}n]{a_n}\right) = \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = 1$, but $\forall \alpha > 0$ the series $\sum_1^\infty a_n n^\alpha$ converges. I know $1/n^n$ goes to zero faster than $n^\alpha$ goes to infinity, but it converges by the two tests. I've tried screwing around with $1/n^n$ but had no luck. Any thoughts?",,"['real-analysis', 'analysis']"
32,Method to prove limit in $\mathbb{R}^2$,Method to prove limit in,\mathbb{R}^2,"Given is the following limit $\lim_{(x,y)\to(0,0)}\frac{x^3y}{x^4+y^2}$. Now, it holds that $0\leq\left\vert\frac{x^3y}{x^4+y^2}\right\vert\leq \frac{x^3y}{y^2}=\left\vert\frac{x^3}{y}\right\vert$. It is clear that $(x,y)\mapsto\frac{x^3}{y}$ is homogeneous on $\mathbb{R^2}\setminus\{\vec0\}$. This implies that $\lim_{(x,y)\to(0,0)}\frac{x^3}{y}=0$. The squeeze lemma would now imply that $\lim_{(x,y)\to(0,0)}\frac{x^3y}{x^4+y^2}=0$. Where does it go wrong?","Given is the following limit $\lim_{(x,y)\to(0,0)}\frac{x^3y}{x^4+y^2}$. Now, it holds that $0\leq\left\vert\frac{x^3y}{x^4+y^2}\right\vert\leq \frac{x^3y}{y^2}=\left\vert\frac{x^3}{y}\right\vert$. It is clear that $(x,y)\mapsto\frac{x^3}{y}$ is homogeneous on $\mathbb{R^2}\setminus\{\vec0\}$. This implies that $\lim_{(x,y)\to(0,0)}\frac{x^3}{y}=0$. The squeeze lemma would now imply that $\lim_{(x,y)\to(0,0)}\frac{x^3y}{x^4+y^2}=0$. Where does it go wrong?",,['calculus']
33,$ \lim_{h\to0} f(x+h)-f(x-h)=0$ does not imply $\lim_{h\to0} f(x+h)-f(x)=0$,does not imply, \lim_{h\to0} f(x+h)-f(x-h)=0 \lim_{h\to0} f(x+h)-f(x)=0,"Let $f$ be defined on an open interval (a,b) and assume $x\in(a,b)$. Consider the two statements (a) $$\lim_{h\to0} f(x+h)-f(x)=0$$ (b)$$ \lim_{h\to0} f(x+h)-f(x-h)=0$$. Prove that (a) always implies (b) and give an example in which (b) holds but (a) does not . My solution : (a) Shows that the function $f$ is continuous at $x$ . So both $$\lim_{h\to0} f(x+h)-f(x)=0$$ and $$\lim_{h\to0} f(x)-f(x-h)=0$$ exists . You add both of the limits and that proves (b) .This proves the first part of the problem . For second part: Now lets consider the function $f(x)=1$ when $x=0$ and $f(x)=0$ otherwise . For this function $\lim_{h\to 0} f(h)-f(-h)=\lim_{h\to0}0-0=0$ but $f$ is not continuous . I was also thinking about the function $f(x)=0 ,x\in\mathbb{Q}$ and $f(x)=1$ . I think this function too two satisfy the limit in (b) but not (a) as it is not a continuous . I would just like to clear out my if my thinking process about continuous function is correct or not . Thank you","Let $f$ be defined on an open interval (a,b) and assume $x\in(a,b)$. Consider the two statements (a) $$\lim_{h\to0} f(x+h)-f(x)=0$$ (b)$$ \lim_{h\to0} f(x+h)-f(x-h)=0$$. Prove that (a) always implies (b) and give an example in which (b) holds but (a) does not . My solution : (a) Shows that the function $f$ is continuous at $x$ . So both $$\lim_{h\to0} f(x+h)-f(x)=0$$ and $$\lim_{h\to0} f(x)-f(x-h)=0$$ exists . You add both of the limits and that proves (b) .This proves the first part of the problem . For second part: Now lets consider the function $f(x)=1$ when $x=0$ and $f(x)=0$ otherwise . For this function $\lim_{h\to 0} f(h)-f(-h)=\lim_{h\to0}0-0=0$ but $f$ is not continuous . I was also thinking about the function $f(x)=0 ,x\in\mathbb{Q}$ and $f(x)=1$ . I think this function too two satisfy the limit in (b) but not (a) as it is not a continuous . I would just like to clear out my if my thinking process about continuous function is correct or not . Thank you",,"['calculus', 'real-analysis', 'analysis', 'proof-verification', 'alternative-proof']"
34,Determine whether the integral $\int_{0}^{\infty}\frac{\cos(x)\sin(1/x)}{x^p}dx$ is convergent or divergent,Determine whether the integral  is convergent or divergent,\int_{0}^{\infty}\frac{\cos(x)\sin(1/x)}{x^p}dx,"Determine whether the improper integral $\int_{0}^{+\infty}\frac{\cos(x)\sin(1/x)}{x^p}dx$ is convergent or divergent for $p\in\mathrm{R}$. According to the Dirichlet test, I find it is convergent when $-1<p<2$. But how can I show its divergence? Applying Integration by parts seems tedious.","Determine whether the improper integral $\int_{0}^{+\infty}\frac{\cos(x)\sin(1/x)}{x^p}dx$ is convergent or divergent for $p\in\mathrm{R}$. According to the Dirichlet test, I find it is convergent when $-1<p<2$. But how can I show its divergence? Applying Integration by parts seems tedious.",,"['calculus', 'real-analysis', 'integration', 'analysis']"
35,Piecewise continuous vs. continuous,Piecewise continuous vs. continuous,,"I'm a little confused by the concept of ""piecewise-continuous"".  My understanding is that if a function $f$ is piecewise-continuous on some domain $[a, b]$, it is not necessarily continuous on $[a, b]$, but it can be cut into pieces which are continuous on subintervals of $[a, b]$.  Does this sound correct? Would the following function be piecewise-continuous? \begin{cases}        -12 & -1 \leq x < 0 \\       \frac{x}{1-x^3} & 0\leq x < 1 \\       5-x & 1\leq x \leq 2     \end{cases}","I'm a little confused by the concept of ""piecewise-continuous"".  My understanding is that if a function $f$ is piecewise-continuous on some domain $[a, b]$, it is not necessarily continuous on $[a, b]$, but it can be cut into pieces which are continuous on subintervals of $[a, b]$.  Does this sound correct? Would the following function be piecewise-continuous? \begin{cases}        -12 & -1 \leq x < 0 \\       \frac{x}{1-x^3} & 0\leq x < 1 \\       5-x & 1\leq x \leq 2     \end{cases}",,['analysis']
36,A function differentiable at 0 but not differentiable at any other point?,A function differentiable at 0 but not differentiable at any other point?,,"I am asked to find a function $f$ that is differentiable at $x=0$ but not differentiable at any other point. I am looking at the function $f(x)=x^2$ when $x\in \mathbb{Q}$ and $f(x)=0$ when $x\notin \mathbb{Q}$. I think it is easy to see that it is not differentiable whenever $x\neq 0$ but I do not know how to go about proving that the function is differentiable at $x=0$. I am thinking about using the idea of convergence (that we can create a sequence of rational numbers close to $x=0$ that approach $0$ or are within the $\epsilon$-neighborhood of $0$) but I am not sure if I need to worry about the irrational numbers? Specifically, how do I guarantee that this function is continuous at $0$?","I am asked to find a function $f$ that is differentiable at $x=0$ but not differentiable at any other point. I am looking at the function $f(x)=x^2$ when $x\in \mathbb{Q}$ and $f(x)=0$ when $x\notin \mathbb{Q}$. I think it is easy to see that it is not differentiable whenever $x\neq 0$ but I do not know how to go about proving that the function is differentiable at $x=0$. I am thinking about using the idea of convergence (that we can create a sequence of rational numbers close to $x=0$ that approach $0$ or are within the $\epsilon$-neighborhood of $0$) but I am not sure if I need to worry about the irrational numbers? Specifically, how do I guarantee that this function is continuous at $0$?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'continuity']"
37,Convergent of a series for arbitrary sequence,Convergent of a series for arbitrary sequence,,"Prove that for arbitrary sequence $x_{1}, x_{2},...$ of $[0,1]$ there is a $x$ in $[0,1]$ such that below series is convergent: $$\sum_{n=1}^\infty \frac{1}{n^2|x-x_{n}|}.$$","Prove that for arbitrary sequence $x_{1}, x_{2},...$ of $[0,1]$ there is a $x$ in $[0,1]$ such that below series is convergent: $$\sum_{n=1}^\infty \frac{1}{n^2|x-x_{n}|}.$$",,"['sequences-and-series', 'analysis']"
38,Polynomials with bounded derivatives,Polynomials with bounded derivatives,,"Is it true that for every $m \geq 1$ there is $C = C(m)$ such that if $p(x)$ is an $m$-degree polynomial with $p(x) \in [0,1]$ for every $x \in [0,1]$ then  $|p'(x)| \leq C$ for every $x \in [0,1]$?","Is it true that for every $m \geq 1$ there is $C = C(m)$ such that if $p(x)$ is an $m$-degree polynomial with $p(x) \in [0,1]$ for every $x \in [0,1]$ then  $|p'(x)| \leq C$ for every $x \in [0,1]$?",,"['analysis', 'polynomials']"
39,A sum of exponentials,A sum of exponentials,,How would you prove that $$\sum_{n \ \text{odd}} \text{sgn}(a + nb ) e^{- | a + n b |} = - \frac{\sinh(a)}{\sinh(b)}$$ where $|a| <  b$ and the sum runs over all odd integers between $-\infty$ and $+ \infty$?,How would you prove that $$\sum_{n \ \text{odd}} \text{sgn}(a + nb ) e^{- | a + n b |} = - \frac{\sinh(a)}{\sinh(b)}$$ where $|a| <  b$ and the sum runs over all odd integers between $-\infty$ and $+ \infty$?,,"['sequences-and-series', 'analysis', 'summation', 'hypergeometric-function', 'hyperbolic-functions']"
40,"Prob. 5, Chap. 3, in Baby Rudin, 3rd ed: $\lim\sup_{n\to\infty}\left(a_n+b_n\right)\leq\lim\sup_{n\to\infty}a_n+\lim\sup_{n\to\infty}b_n$.","Prob. 5, Chap. 3, in Baby Rudin, 3rd ed: .",\lim\sup_{n\to\infty}\left(a_n+b_n\right)\leq\lim\sup_{n\to\infty}a_n+\lim\sup_{n\to\infty}b_n,"Here's Prob. 5, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: For any two real sequences $\left\{ a_n \right\}$ , $\left\{ b_n \right\}$ , prove that $$\lim\sup_{n\to\infty}\left( a_n + b_n \right) \leq \lim\sup_{n\to\infty} a_n + \lim\sup_{n\to\infty} b_n,$$ provided the sum on the right is not of the form $\infty-\infty$ . My effort: Let us put $$ c_n \colon= a_n + b_n \tag{Definition A} $$ for all $n \in \mathbb{N}$ , and let's put $$  \begin{align}  a^* & \colon= \lim\sup_{n\to\infty} a_n, \\ b^* & \colon= \lim\sup_{n\to\infty} b_n, \\ c^* & \colon= \lim\sup_{n\to\infty} c_n.  \end{align} \tag{Definitions B} $$ We need to show that $$ c^* \leq a^* + b^*. \tag{0} $$ So let's suppose that $$ c^* \not\leq a^* + b^*. $$ Then we have $$ c^* > a^* + b^*, $$ and so $$ c^*- b^* > a^*,$$ and let's take a real number $x$ such that $$ c^*- b^* > x > a^*. \tag{0} $$ Then as $ x > a^*$ , so by Theorem 3.17 (b) in Baby Rudin, we can find a natural number $N_1$ such that $$ x \geq a_n \tag{1}$$ for all $n > N_1$ . Now from (0) above, as $$ c^*-x > b^*, $$ so we can find a real number $y$ such that $$c^*-x > y > b^*. \tag{2} $$ Then as $y > b^*$ , so again by Theorem 3.17 (b) in Baby Rudin, we can find a natural number $N_2$ such that $$ y \geq b_n \tag{3}$$ for all $n>N_2$ . Now from (2) above we can conclude that $$ c^* > x+y, \tag{4} $$ and hence from (1) and (3) we also have $$ x+y \geq a_n + b_n = c_n, $$ for all $n > \max \left\{\ N_1, N_2 \ \right\}$ , which in turn implies that $$ x+y \geq \lim\sup_{n \to \infty} c_n, $$ that is [Please Refer to (Definition A) and (Definitions B) above.], $$ x+y \geq c^*, $$ which, in view of (4) above, gives rise to a contradiction to our choice of $c^*$ as the limit superior of the sequence $\left\{c_n\right\}$ . Is this proof correct? If not, then where is it deficient?","Here's Prob. 5, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: For any two real sequences , , prove that provided the sum on the right is not of the form . My effort: Let us put for all , and let's put We need to show that So let's suppose that Then we have and so and let's take a real number such that Then as , so by Theorem 3.17 (b) in Baby Rudin, we can find a natural number such that for all . Now from (0) above, as so we can find a real number such that Then as , so again by Theorem 3.17 (b) in Baby Rudin, we can find a natural number such that for all . Now from (2) above we can conclude that and hence from (1) and (3) we also have for all , which in turn implies that that is [Please Refer to (Definition A) and (Definitions B) above.], which, in view of (4) above, gives rise to a contradiction to our choice of as the limit superior of the sequence . Is this proof correct? If not, then where is it deficient?","\left\{ a_n \right\} \left\{ b_n \right\} \lim\sup_{n\to\infty}\left( a_n + b_n \right) \leq \lim\sup_{n\to\infty} a_n + \lim\sup_{n\to\infty} b_n, \infty-\infty  c_n \colon= a_n + b_n \tag{Definition A}  n \in \mathbb{N}  
\begin{align} 
a^* & \colon= \lim\sup_{n\to\infty} a_n, \\
b^* & \colon= \lim\sup_{n\to\infty} b_n, \\
c^* & \colon= \lim\sup_{n\to\infty} c_n. 
\end{align}
\tag{Definitions B}   c^* \leq a^* + b^*. \tag{0}   c^* \not\leq a^* + b^*.   c^* > a^* + b^*,   c^*- b^* > a^*, x  c^*- b^* > x > a^*. \tag{0}   x > a^* N_1  x \geq a_n \tag{1} n > N_1  c^*-x > b^*,  y c^*-x > y > b^*. \tag{2}  y > b^* N_2  y \geq b_n \tag{3} n>N_2  c^* > x+y, \tag{4}   x+y \geq a_n + b_n = c_n,  n > \max \left\{\ N_1, N_2 \ \right\}  x+y \geq \lim\sup_{n \to \infty} c_n,   x+y \geq c^*,  c^* \left\{c_n\right\}","['real-analysis', 'sequences-and-series', 'analysis', 'limsup-and-liminf']"
41,"Nondecreasing, singular function fails a weaker condition than absolutely continuous","Nondecreasing, singular function fails a weaker condition than absolutely continuous",,"Suppose $f:[0,1]\to\mathbb{R}$ is a nondecreasing function, and $f'(x)=0$ almost everywhere on $[0,1]$. For any $\epsilon>0$, prove that there exists finitely many pairwise disjoint intervals $[a_k,b_k]$, $k=1,\dots, n$ in $[0,1]$ with $$\sum_{k=1}^n(b_k-a_k)<\epsilon$$ and $$\sum_{k=1}^n(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon.$$ My attempt: I think proving by contradiction may be the way to go here. Suppose to the contrary there exists $\epsilon>0$ such that for any finitely many pairwise disjoint $[a_k,b_k]\subseteq[0,1]$ with $\sum_{k=1}^n(b_k-a_k)<\epsilon$, we have $$0\leq\sum_{k=1}^n(f(b_k)-f(a_k))\leq f(1)-f(0)-\epsilon.$$ This looks like a weaker condition than absolutely continuous. However, I am not sure how to proceed here, other than noting that $f(1)-f(0)>0$, so that $f$ is not constant. Also, I can see that $f$ cannot be absolutely continuous, otherwise it will have a contradiction, since an absolutely continuous and singular function must be constant. Thanks for any help! Update: I have another idea, let $E$ be the set where $f'\neq 0$. Then $|E|=0$. My idea is to cover $E$ in the Vitali sense by intervals $[a_k,b_k]$ and use Vitali Covering Theorem to prove that $\sum(b_k-a_k)<\epsilon$, but $[a_k, b_k]$ covers nearly all of $E$, where all the changes occurs, so that $\sum(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon$. At the moment, I have no idea how to make the above idea rigorous though.","Suppose $f:[0,1]\to\mathbb{R}$ is a nondecreasing function, and $f'(x)=0$ almost everywhere on $[0,1]$. For any $\epsilon>0$, prove that there exists finitely many pairwise disjoint intervals $[a_k,b_k]$, $k=1,\dots, n$ in $[0,1]$ with $$\sum_{k=1}^n(b_k-a_k)<\epsilon$$ and $$\sum_{k=1}^n(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon.$$ My attempt: I think proving by contradiction may be the way to go here. Suppose to the contrary there exists $\epsilon>0$ such that for any finitely many pairwise disjoint $[a_k,b_k]\subseteq[0,1]$ with $\sum_{k=1}^n(b_k-a_k)<\epsilon$, we have $$0\leq\sum_{k=1}^n(f(b_k)-f(a_k))\leq f(1)-f(0)-\epsilon.$$ This looks like a weaker condition than absolutely continuous. However, I am not sure how to proceed here, other than noting that $f(1)-f(0)>0$, so that $f$ is not constant. Also, I can see that $f$ cannot be absolutely continuous, otherwise it will have a contradiction, since an absolutely continuous and singular function must be constant. Thanks for any help! Update: I have another idea, let $E$ be the set where $f'\neq 0$. Then $|E|=0$. My idea is to cover $E$ in the Vitali sense by intervals $[a_k,b_k]$ and use Vitali Covering Theorem to prove that $\sum(b_k-a_k)<\epsilon$, but $[a_k, b_k]$ covers nearly all of $E$, where all the changes occurs, so that $\sum(f(b_k)-f(a_k))>f(1)-f(0)-\epsilon$. At the moment, I have no idea how to make the above idea rigorous though.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
42,Taylor expansion of ln(1+x) and small O,Taylor expansion of ln(1+x) and small O,,"http://people.math.sc.edu/girardi/m142/handouts/10sTaylorPolySeries.pdf The Taylor expansion of $\ln(1+x)$ is $\sum_{n=1}^{\infty} (-1)^{n-1}\frac{x^n}{n}$. Is it true that we can think of $\ln(1+x) = x+o(x^2)$, what does this mean pricesly? I find that such thing is an usual trick throughout mathematical analysis, but I can not find any related material on Baby Rudin. Does any one have any rigorous reference on this?","http://people.math.sc.edu/girardi/m142/handouts/10sTaylorPolySeries.pdf The Taylor expansion of $\ln(1+x)$ is $\sum_{n=1}^{\infty} (-1)^{n-1}\frac{x^n}{n}$. Is it true that we can think of $\ln(1+x) = x+o(x^2)$, what does this mean pricesly? I find that such thing is an usual trick throughout mathematical analysis, but I can not find any related material on Baby Rudin. Does any one have any rigorous reference on this?",,"['calculus', 'real-analysis', 'analysis']"
43,"$f:\mathbb R^2 \to \{0,1\}$ be a function , does there exist an equilateral triangle in plane with vertices $x,y,z$ such that $f(x)=f(y)=f(z)$?","be a function , does there exist an equilateral triangle in plane with vertices  such that ?","f:\mathbb R^2 \to \{0,1\} x,y,z f(x)=f(y)=f(z)","Let $f:\mathbb R^2 \to \{0,1\}$ be a function ; then is it true that there exists an equilateral triangle in plane with vertices $x,y,z \in \mathbb R^2$  such that $f(x)=f(y)=f(z)$ ?","Let $f:\mathbb R^2 \to \{0,1\}$ be a function ; then is it true that there exists an equilateral triangle in plane with vertices $x,y,z \in \mathbb R^2$  such that $f(x)=f(y)=f(z)$ ?",,['combinatorics']
44,Is this hypothesis necessary for Abel's Theorem?,Is this hypothesis necessary for Abel's Theorem?,,I am reading up on Abel's Theorem from http://www.math.uconn.edu/~kconrad/blurbs/analysis/abelthm.pdf . Screenshot here for convenience: Q1): Is it necessary to have the hypothesis that $\sum c_nx^n$ converges for $|x|<1$? I ask this question since some sources do not have this hypothesis (e.g. https://proofwiki.org/wiki/Abel's_Theorem ) Hence I somehow suspect it can be deduced from the fact that $\sum c_n$ converges. I know that the comparison test works $\sum c_nx^n\leq\sum c_n$ works if $c_n$ are nonnegative. I do not know how to deduce it if $c_n$ are possibly negative. Thanks for any help!,I am reading up on Abel's Theorem from http://www.math.uconn.edu/~kconrad/blurbs/analysis/abelthm.pdf . Screenshot here for convenience: Q1): Is it necessary to have the hypothesis that $\sum c_nx^n$ converges for $|x|<1$? I ask this question since some sources do not have this hypothesis (e.g. https://proofwiki.org/wiki/Abel's_Theorem ) Hence I somehow suspect it can be deduced from the fact that $\sum c_n$ converges. I know that the comparison test works $\sum c_nx^n\leq\sum c_n$ works if $c_n$ are nonnegative. I do not know how to deduce it if $c_n$ are possibly negative. Thanks for any help!,,"['real-analysis', 'analysis']"
45,How would you proof that this series is convergent (no solution needed)?,How would you proof that this series is convergent (no solution needed)?,,"Analyze if the series is convergent:$$\sum_{n=0}^{\infty}\frac{2n^{3}+3n^{2}+1}{2^{n}}$$ I have used ratio test and I got $\frac{1}{2}$ as result which seems pretty good, it's also smaller than $1$... But I'm not looking for a solution here. Problem with ratio test was, it took me half of a page and too much time. I didn't even have to think how to deform while I used the ratio test, I did it really fast (anyway was slow). There aren't easier and faster ways of solving this? In the exam I only got 120 minutes and a task like that is one of many tasks there. I can imagine there is a way which doesn't require you more than 2 lines, I have needed almost 7 lines (half DIN A4)...","Analyze if the series is convergent:$$\sum_{n=0}^{\infty}\frac{2n^{3}+3n^{2}+1}{2^{n}}$$ I have used ratio test and I got $\frac{1}{2}$ as result which seems pretty good, it's also smaller than $1$... But I'm not looking for a solution here. Problem with ratio test was, it took me half of a page and too much time. I didn't even have to think how to deform while I used the ratio test, I did it really fast (anyway was slow). There aren't easier and faster ways of solving this? In the exam I only got 120 minutes and a task like that is one of many tasks there. I can imagine there is a way which doesn't require you more than 2 lines, I have needed almost 7 lines (half DIN A4)...",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
46,Find the distance between two poles based on length of sagging string between them,Find the distance between two poles based on length of sagging string between them,,"A string is drawn between two 10 meter tall poles. At its lowest point, the string is 3 meters off the ground. The total length of the string is 14 meters. What's the distance between the two poles?","A string is drawn between two 10 meter tall poles. At its lowest point, the string is 3 meters off the ground. The total length of the string is 14 meters. What's the distance between the two poles?",,['analysis']
47,Why $f(x)$ is Riemann integrable?,Why  is Riemann integrable?,f(x),"If somebody can explain to me why the following function is Riemann Integrable on $[0,1]$ and how to find $\int_0^1 f(x)~dx$ $$f(x)= \begin{cases}1 &\text{ if }  x=0\\ 0 &\text{ if } x \in [0,1] \setminus \Bbb{Q}, \\ \frac 1n &\text{ if } x=\frac mn \in [0,1] \cap \Bbb{Q} \text{ with } \gcd(n,m)=1\end{cases} $$","If somebody can explain to me why the following function is Riemann Integrable on $[0,1]$ and how to find $\int_0^1 f(x)~dx$ $$f(x)= \begin{cases}1 &\text{ if }  x=0\\ 0 &\text{ if } x \in [0,1] \setminus \Bbb{Q}, \\ \frac 1n &\text{ if } x=\frac mn \in [0,1] \cap \Bbb{Q} \text{ with } \gcd(n,m)=1\end{cases} $$",,"['calculus', 'real-analysis', 'analysis']"
48,"Math Analysis, Real Analysis and Advanced Calculus similiarity and book recommendation","Math Analysis, Real Analysis and Advanced Calculus similiarity and book recommendation",,"I'm taking Calculus 3 at the moment often I like to look at the courses I am going to be taking in the coming semester. At my school, they only list Math Analysis and there are no courses for Real Analysis or Advanced Calculus. Some professors have mentioned they are different while others say they cover the same thing, so I'm a bit confused. I looked at the books at the library and it seems that the three subject cover very similar topics. I have three questions: 1. Are they the same subjects just branded differently or are there differences such as the style with which they approach the material and even the material itself covered? 2. If so which is the best of the three to tackle first? Since I cannot take the Math Analysis course at my school for another year and half( course if taught in the spring of even number years only) I may as well self study til then. 3. What is a recommended set of best books to buy for each (if they're different) or if they are more or less the same what are some recommended books that cover the topics most concisely and give the material a ""modern treatment""? Thanks for the help in advance!","I'm taking Calculus 3 at the moment often I like to look at the courses I am going to be taking in the coming semester. At my school, they only list Math Analysis and there are no courses for Real Analysis or Advanced Calculus. Some professors have mentioned they are different while others say they cover the same thing, so I'm a bit confused. I looked at the books at the library and it seems that the three subject cover very similar topics. I have three questions: 1. Are they the same subjects just branded differently or are there differences such as the style with which they approach the material and even the material itself covered? 2. If so which is the best of the three to tackle first? Since I cannot take the Math Analysis course at my school for another year and half( course if taught in the spring of even number years only) I may as well self study til then. 3. What is a recommended set of best books to buy for each (if they're different) or if they are more or less the same what are some recommended books that cover the topics most concisely and give the material a ""modern treatment""? Thanks for the help in advance!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'book-recommendation']"
49,"Sequences of Functions: Looking for a sequence $(x_n) $ on $[0,1]$ such that the sequence $(x_{n}^n)\neq0$ as $n\to\infty$",Sequences of Functions: Looking for a sequence  on  such that the sequence  as,"(x_n)  [0,1] (x_{n}^n)\neq0 n\to\infty","So, I have this problem on my analysis homework. We've been looking at sequences of functions.  In particular, we are looking at $x^n$ and the question asks us to show that there is a sequence of values of $x$ on $[0,1]$, call it $(x_n)$, such that $\lim_{n\to\infty} (x_{n}^n) \neq 0 = lim_{n\to\infty} x^n$ on $[0,1]$. Just to be sure we're on the same page, I'm using $(x_n)$ to signify the infinite sequence rather than $\{x_n\}$ which would just be the list of values taken (which could be infinite, I suppose but need not be). Any hints? Thanks. EDIT: I took out the extraneous = 0.","So, I have this problem on my analysis homework. We've been looking at sequences of functions.  In particular, we are looking at $x^n$ and the question asks us to show that there is a sequence of values of $x$ on $[0,1]$, call it $(x_n)$, such that $\lim_{n\to\infty} (x_{n}^n) \neq 0 = lim_{n\to\infty} x^n$ on $[0,1]$. Just to be sure we're on the same page, I'm using $(x_n)$ to signify the infinite sequence rather than $\{x_n\}$ which would just be the list of values taken (which could be infinite, I suppose but need not be). Any hints? Thanks. EDIT: I took out the extraneous = 0.",,"['sequences-and-series', 'analysis', 'functions']"
50,Extending Functions in Sobolev Spaces,Extending Functions in Sobolev Spaces,,"If $U\subset W$ then every function in $L^p (U)$ can be extended to a function in $L^p (W)$, for example by setting it to be 0 outside of $U$. However, not every continuous or differentiable function on $U$ can be extended to a continuous or differentiable function on $W$.  For example, $1/x$ on $(0,1)$ cannot be extended continuously and $\sqrt{x}$ on $(0,1)$ cannot be extended to a function that's differentiable on $(-1,1)$. I am learning about Sobolev spaces and am wondering whether it is true that any $f\in W^{1,1}(U)$ can be extended to a function in $W^{1,1}(W)$?","If $U\subset W$ then every function in $L^p (U)$ can be extended to a function in $L^p (W)$, for example by setting it to be 0 outside of $U$. However, not every continuous or differentiable function on $U$ can be extended to a continuous or differentiable function on $W$.  For example, $1/x$ on $(0,1)$ cannot be extended continuously and $\sqrt{x}$ on $(0,1)$ cannot be extended to a function that's differentiable on $(-1,1)$. I am learning about Sobolev spaces and am wondering whether it is true that any $f\in W^{1,1}(U)$ can be extended to a function in $W^{1,1}(W)$?",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
51,Origin of delta,Origin of delta,,"Why does delta mean change? What is the origin of delta? I understand that upper-cased delta is used in this way and that delta is the fourth letter of the Greek alphabet. I also read that delta is the first letter of a Greek word that means ""difference."" The question remains: why does delta now symbolize change?","Why does delta mean change? What is the origin of delta? I understand that upper-cased delta is used in this way and that delta is the fourth letter of the Greek alphabet. I also read that delta is the first letter of a Greek word that means ""difference."" The question remains: why does delta now symbolize change?",,"['calculus', 'analysis', 'notation', 'mathematical-physics', 'math-history']"
52,Construct $f: X\to Y$ such that $f(p)=p$,Construct  such that,f: X\to Y f(p)=p,"Let , $X=[-1,1]\times [-1,1]$ and $Y=\{0\}\times \left[-\frac{1}{2},\frac{1}{2}\right]$. Construct an example of a continuous map $f:X\to Y$ such that $f(p)=p$ for each $p\in Y$. I construct a function as below : $$f(x,y)=\begin{cases}\left(0,\frac{1}{2}+y\right) & \text{ if }(x,y)\in[-1,1]\times\left[-1,-\frac{1}{2}\right]\\(0,y) & \text{ if }(x,y)\in [-1,1]\times\left[-\frac{1}{2},\frac{1}{2}\right]\\\left(0,\frac{1}{2}-y\right) & \text{ if }(x,y)\in[-1,1]\times \left[\frac{1}{2},1\right]\end{cases}$$ Is my construction correct ? Please check. If wrong please tell me where my mistake.","Let , $X=[-1,1]\times [-1,1]$ and $Y=\{0\}\times \left[-\frac{1}{2},\frac{1}{2}\right]$. Construct an example of a continuous map $f:X\to Y$ such that $f(p)=p$ for each $p\in Y$. I construct a function as below : $$f(x,y)=\begin{cases}\left(0,\frac{1}{2}+y\right) & \text{ if }(x,y)\in[-1,1]\times\left[-1,-\frac{1}{2}\right]\\(0,y) & \text{ if }(x,y)\in [-1,1]\times\left[-\frac{1}{2},\frac{1}{2}\right]\\\left(0,\frac{1}{2}-y\right) & \text{ if }(x,y)\in[-1,1]\times \left[\frac{1}{2},1\right]\end{cases}$$ Is my construction correct ? Please check. If wrong please tell me where my mistake.",,"['real-analysis', 'analysis', 'functions', 'proof-verification']"
53,"Series converge or diverge : $\sum^\infty_{n=1}n(1+n^2)^p$ , $p\in\mathbb{R}$?","Series converge or diverge :  , ?",\sum^\infty_{n=1}n(1+n^2)^p p\in\mathbb{R},"$\sum^\infty_{n=1}n(1+n^2)^p$ , $p\in\mathbb{R}$ I tried to compare it with other known sequencies but i couldn't find the right one. I also tried to solve it using Mathematical Induction (for p=-1) but it led me nowhere. Any suggestions about what should I do?","$\sum^\infty_{n=1}n(1+n^2)^p$ , $p\in\mathbb{R}$ I tried to compare it with other known sequencies but i couldn't find the right one. I also tried to solve it using Mathematical Induction (for p=-1) but it led me nowhere. Any suggestions about what should I do?",,"['sequences-and-series', 'analysis']"
54,$V_0^1 f \geq \frac{1}{6} \sum_{n=1}^\infty V_0^1 f_n$?,?,V_0^1 f \geq \frac{1}{6} \sum_{n=1}^\infty V_0^1 f_n,"For any $n \in \mathbb{N}^+$, $f_n:[0,1]\rightarrow \{0,\frac{1}{3^n}\}$, define $f = \sum_{n=1}^\infty f_n$. Is there any clue to prove that $V_0^1 f \geq \frac{1}{6}\sum_{n=1}^\infty V_0^1 f_n$? Here $V$ is the total variation. Now I can only argue that when any of $f_n$ is not a function of bounded variation, the inequality holds. If each $f_n$ is of bounded variation, it may be beneficial to keep it in mind that $f_n$ can only have finite noncontinuous points in this case. Moreover, from the accepted proof, we get a stronger conclusion: $$ V_0^1 f \geq \frac{1}{3} \sum_{n=1}^\infty V_0^1 f_n, $$ which is tight since we can construct an example to make the equality hold.","For any $n \in \mathbb{N}^+$, $f_n:[0,1]\rightarrow \{0,\frac{1}{3^n}\}$, define $f = \sum_{n=1}^\infty f_n$. Is there any clue to prove that $V_0^1 f \geq \frac{1}{6}\sum_{n=1}^\infty V_0^1 f_n$? Here $V$ is the total variation. Now I can only argue that when any of $f_n$ is not a function of bounded variation, the inequality holds. If each $f_n$ is of bounded variation, it may be beneficial to keep it in mind that $f_n$ can only have finite noncontinuous points in this case. Moreover, from the accepted proof, we get a stronger conclusion: $$ V_0^1 f \geq \frac{1}{3} \sum_{n=1}^\infty V_0^1 f_n, $$ which is tight since we can construct an example to make the equality hold.",,"['real-analysis', 'analysis']"
55,Convergence of the $L^p$ norm to $L^{\infty}$ norm,Convergence of the  norm to  norm,L^p L^{\infty},"Let $E \subset \mathbb{R}^n$ measurable. Prove that if there exist $p_0 \geq 1$ such that $f \in L^{p_o}(E) \cap L^{\infty}(E)$, then $f \in L^p(E)$ for all $p \geq p_0$ and $\|f\|_p \rightarrow \|f\|_{\infty}$ when $p \rightarrow \infty$. I've already proved that $f\in L^p(E)$ with this conditions. The norm in the intersection space is $\|f\|_{p_0, \infty} = \|f\|_{p_0} + \|f\|_{\infty}$. Now if i set $L = \|f\|_{\infty}$ then $$\|f\|_p \leq \left( \int_E L^p \right)^{1/p}=L \cdot m(E)^{1/p} $$ is this correct? If it is, i have problems to finish the idea because i can't say that $m(E) \rightarrow 1$ because maybe $m(E)=+\infty$","Let $E \subset \mathbb{R}^n$ measurable. Prove that if there exist $p_0 \geq 1$ such that $f \in L^{p_o}(E) \cap L^{\infty}(E)$, then $f \in L^p(E)$ for all $p \geq p_0$ and $\|f\|_p \rightarrow \|f\|_{\infty}$ when $p \rightarrow \infty$. I've already proved that $f\in L^p(E)$ with this conditions. The norm in the intersection space is $\|f\|_{p_0, \infty} = \|f\|_{p_0} + \|f\|_{\infty}$. Now if i set $L = \|f\|_{\infty}$ then $$\|f\|_p \leq \left( \int_E L^p \right)^{1/p}=L \cdot m(E)^{1/p} $$ is this correct? If it is, i have problems to finish the idea because i can't say that $m(E) \rightarrow 1$ because maybe $m(E)=+\infty$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
56,"Show that if $f$ is continuous on [0,1], then: $\int_0^\frac\pi 2 f(\sin x)dx=\int_0^\frac\pi 2 f(\cos x)dx= \frac12\int_0^\pi f(\sin x)dx$","Show that if  is continuous on [0,1], then:",f \int_0^\frac\pi 2 f(\sin x)dx=\int_0^\frac\pi 2 f(\cos x)dx= \frac12\int_0^\pi f(\sin x)dx,"Also part of the problem: Show that: $$\int_0^{n\pi} f(\cos^2 x)dx=n\int_0^{\pi} f(\cos^2 x)dx$$ It may be important to note that I have taken a few semester of calculus prior to this, and that this problem was brought up in my math analysis course. Additionally, my class has only taught as far as Riemannian integrals. My work: A) For the first part of this problem ($\int_0^\frac\pi 2 f(\sin x)dx=\int_0^\frac\pi 2 f(\cos x)= \frac12\int_0^\pi f(\sin x)$ ), I intuitively know that the areas under the curve will be equal to each other because the values at the endpoints will be the same, provided that $f(\sin x)=\sin x$ and $f(\cos x)=\cos x$. However, I have two dilemmas: 1) The fact that the situation deals with a function of $\sin x$, not $\sin x$ itself. To give an example, $f(a)$ could be equal to $a^2 + 8a^{-1} + \csc a^3$ and I feel like this completely flaws my logic (note that the above function was just an example to show how arbitrary the function is). In the grand scheme of things, I don't believe this will prove to be too much of an issue, I just don't know how I can prove that it won't be an issue 2) I am unaware of how to definitively prove that the area under each curve will be the same.  Suppose $f(a)= a$ for the sake of argument, then the value $f(x)$ at each of the endpoints will be the same, but this does not tell anything about the areas under the curve B) I believe that I understand the second part of the problem. Because $x^2$ will never be negative, the graph of $f(x)$ will either always be negative or always be positive. Therefore, the value of the integral is entirely dependent on $n$. I am primarily concerned with the two dilemmas in the first part of the problem, but any additional assistance is always welcome!","Also part of the problem: Show that: $$\int_0^{n\pi} f(\cos^2 x)dx=n\int_0^{\pi} f(\cos^2 x)dx$$ It may be important to note that I have taken a few semester of calculus prior to this, and that this problem was brought up in my math analysis course. Additionally, my class has only taught as far as Riemannian integrals. My work: A) For the first part of this problem ($\int_0^\frac\pi 2 f(\sin x)dx=\int_0^\frac\pi 2 f(\cos x)= \frac12\int_0^\pi f(\sin x)$ ), I intuitively know that the areas under the curve will be equal to each other because the values at the endpoints will be the same, provided that $f(\sin x)=\sin x$ and $f(\cos x)=\cos x$. However, I have two dilemmas: 1) The fact that the situation deals with a function of $\sin x$, not $\sin x$ itself. To give an example, $f(a)$ could be equal to $a^2 + 8a^{-1} + \csc a^3$ and I feel like this completely flaws my logic (note that the above function was just an example to show how arbitrary the function is). In the grand scheme of things, I don't believe this will prove to be too much of an issue, I just don't know how I can prove that it won't be an issue 2) I am unaware of how to definitively prove that the area under each curve will be the same.  Suppose $f(a)= a$ for the sake of argument, then the value $f(x)$ at each of the endpoints will be the same, but this does not tell anything about the areas under the curve B) I believe that I understand the second part of the problem. Because $x^2$ will never be negative, the graph of $f(x)$ will either always be negative or always be positive. Therefore, the value of the integral is entirely dependent on $n$. I am primarily concerned with the two dilemmas in the first part of the problem, but any additional assistance is always welcome!",,"['calculus', 'integration', 'analysis']"
57,Prove that such a function exists that mimics binomial theorem,Prove that such a function exists that mimics binomial theorem,,"I would like to see if someone can show to me that a function exists such that: $$ (f(x)-1)^k = \sum_{j=0}^k {k \choose j}(-1)^{k-j} g(j,k,x) $$ Basically what im trying to say is im trying to find non-trivial solutions that are interesting. This means the following solutions are trivial: \begin{align*} g(j,k,x) =  f(x)^{j} \\ g(j,k,x) = (-1)^k f(x)^{k-j} \\ g(j,k,x) = 0^{k-j} (f(x)-1)^j \\ g(j,k,x) = c(k,j)(f(x)\pm m(k,j))^{b(k,j)} \end{align*} Where $c(k,j),m(k,j),b(k,j)$ are some functions with the variables $j,k$ such that would be nonpathological.  Any solutions that are similar to those listed above are considered trivial in this instance.Also we are dealing with real numbers here. I have already come up with my own solution to this problem but I would like to see if either someone can prove to my the uniqueness of my solution, or to prove that more than one of such functions exist. Here is how I derived my solution: It is known that: $$ \left(f(x+z)-f(x)\right)^k = \sum_{n=k}^\infty B_{n,k}^f(x) \frac{z^n}{n!} $$ This is shown in the following paper . Where $B_{n,k}^f(x)$ is the partial Bell Polynomial with respect to the function $f(x)$, this can also be viewed as: \begin{align*} B_{n,k}^f(x) = \sum_j \frac{n!}{j_1!j_2!\cdots j_{n-k+1}!} \prod_{m=1}^{n-k+1} \left(\frac{f^{(m)}(x)}{m!}\right)^{j_m} \end{align*} Where the sum $j$ runs through all the partitions that satisfy the following linear system: \begin{align*} n = \sum_{m=1}^{n-k+1} mj_m \\ k = \sum_{m=1}^{n-k+1}j_m \end{align*} With that being said, it can be derived that: \begin{align*} B_{n,k}^{x^{-v}}(x) = \frac{n!}{k!} x^{-vk-n} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n} \end{align*} By implementing this into out definition we have: \begin{align*} \frac{\left((x+1)^{-v}-x^{-v}\right)}{k!} = \frac{x^{-vk}}{k!}\sum_{n=0}^\infty x^{-n-k} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n+k} \\ = \frac{x^{-vk}}{k!} \sum_{j=0}^k {k \choose j}(-1)^{k-j} \left(\sum_{n=0}^{\infty} x^{-n-k} {-vj \choose n+k}\right) \end{align*} I am not going to show you proof by hand that this converges, I will let wolfram help you with that one: \begin{align*} \sum_{n=0}^{\infty} x^{-n} {-vj \choose n} = \left(\frac{1}{x}\right)^k {-vj \choose k} {}_2F_1\left(1,k+vj;k+1,-\frac{1}{x}\right) \end{align*} Therefore: \begin{align*} \left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=0}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right) \end{align*} Notice that the monment when $j=0$, the resulting term is zero, therefore: \begin{align*} \left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=1}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right) \end{align*} This is an example of a solution that is non-trivial in the sense that is defies intuition, note that $v > 0$ and $k >0$, convergence is absolute when $\lvert \frac{1}{x}\rvert < 1$. This part is for Morgan Rodgers. Lets assume that the solution i have come up with is considered a trivial solution under the pretenses i have set, then the following is true: \begin{align*} x^{-k} {-jv \choose k} {}_2F_1\left(1,k+jv;k+1;-\frac{1}{x}\right) = c(k,j) \left(f(x)+m(j,k)\right)^{b(j,k)} \end{align*} noting that in the following case, $f(x) = \left(1+\frac{1}{x}\right)^{-v}$ therefore we have $\frac{1}{x} = f(x)^{\frac{-1}{v}}-1$ therefore we have: $$ {-jv \choose k}\left(f(x)^{-\frac{1}{v}}-1\right)^k {}_2F_1\left(1,k+jv;k+1;1-f(x)^{-\frac{1}{v}}\right) = \left(f(x)+m(j,k)\right)^{b(j,k)} $$ If we set $k=3$ we find that: $$ {-jv \choose 3}\left(f(x)^{-\frac{1}{v}}-1\right)^3 {}_2F_1\left(1,3+jv;3+1;1-f(x)^{-\frac{1}{v}}\right) = c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{{-jv \choose 3}}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}(jv+1)(jv+2)}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right) $$ When we simplify we have: $$ c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{-jv}{6\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right) = \frac{jv}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(\frac{jv}{2}f(x)^{\frac{1}{v}}[jv+1]-f(x)^{\frac{2}{v}} [jv+2] +\frac{1}{2}f(x)^{\frac{3}{v}} [jv+3-\frac{2(1+f(x)^j)}{jv}]\right) $$ Although i have not proven explicitly that such functions do not exists, the resultant of the terms do not even resemble anything like the binomial theorem especially considering that there is a lone $f(x)^j$ term in the middle of the convolution. Although this proof is not complete, it provides strong evidence that this solution is most likely either non-trivial by my definition or if such a solution is considered trivial, certainly it will pathological with respect to typical binomial theorem solutions.","I would like to see if someone can show to me that a function exists such that: $$ (f(x)-1)^k = \sum_{j=0}^k {k \choose j}(-1)^{k-j} g(j,k,x) $$ Basically what im trying to say is im trying to find non-trivial solutions that are interesting. This means the following solutions are trivial: \begin{align*} g(j,k,x) =  f(x)^{j} \\ g(j,k,x) = (-1)^k f(x)^{k-j} \\ g(j,k,x) = 0^{k-j} (f(x)-1)^j \\ g(j,k,x) = c(k,j)(f(x)\pm m(k,j))^{b(k,j)} \end{align*} Where $c(k,j),m(k,j),b(k,j)$ are some functions with the variables $j,k$ such that would be nonpathological.  Any solutions that are similar to those listed above are considered trivial in this instance.Also we are dealing with real numbers here. I have already come up with my own solution to this problem but I would like to see if either someone can prove to my the uniqueness of my solution, or to prove that more than one of such functions exist. Here is how I derived my solution: It is known that: $$ \left(f(x+z)-f(x)\right)^k = \sum_{n=k}^\infty B_{n,k}^f(x) \frac{z^n}{n!} $$ This is shown in the following paper . Where $B_{n,k}^f(x)$ is the partial Bell Polynomial with respect to the function $f(x)$, this can also be viewed as: \begin{align*} B_{n,k}^f(x) = \sum_j \frac{n!}{j_1!j_2!\cdots j_{n-k+1}!} \prod_{m=1}^{n-k+1} \left(\frac{f^{(m)}(x)}{m!}\right)^{j_m} \end{align*} Where the sum $j$ runs through all the partitions that satisfy the following linear system: \begin{align*} n = \sum_{m=1}^{n-k+1} mj_m \\ k = \sum_{m=1}^{n-k+1}j_m \end{align*} With that being said, it can be derived that: \begin{align*} B_{n,k}^{x^{-v}}(x) = \frac{n!}{k!} x^{-vk-n} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n} \end{align*} By implementing this into out definition we have: \begin{align*} \frac{\left((x+1)^{-v}-x^{-v}\right)}{k!} = \frac{x^{-vk}}{k!}\sum_{n=0}^\infty x^{-n-k} \sum_{j=0}^k {k \choose j}(-1)^{k-j} {-vj \choose n+k} \\ = \frac{x^{-vk}}{k!} \sum_{j=0}^k {k \choose j}(-1)^{k-j} \left(\sum_{n=0}^{\infty} x^{-n-k} {-vj \choose n+k}\right) \end{align*} I am not going to show you proof by hand that this converges, I will let wolfram help you with that one: \begin{align*} \sum_{n=0}^{\infty} x^{-n} {-vj \choose n} = \left(\frac{1}{x}\right)^k {-vj \choose k} {}_2F_1\left(1,k+vj;k+1,-\frac{1}{x}\right) \end{align*} Therefore: \begin{align*} \left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=0}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right) \end{align*} Notice that the monment when $j=0$, the resulting term is zero, therefore: \begin{align*} \left(\left(1+\frac{1}{x}\right)^{-v}-1\right)^k = x^{-k} \sum_{j=1}^k {k \choose j}(-1)^{j} {k+jv-1 \choose k} {}_2F_1\left(1,k+jv;k+1;\frac{-1}{x}\right) \end{align*} This is an example of a solution that is non-trivial in the sense that is defies intuition, note that $v > 0$ and $k >0$, convergence is absolute when $\lvert \frac{1}{x}\rvert < 1$. This part is for Morgan Rodgers. Lets assume that the solution i have come up with is considered a trivial solution under the pretenses i have set, then the following is true: \begin{align*} x^{-k} {-jv \choose k} {}_2F_1\left(1,k+jv;k+1;-\frac{1}{x}\right) = c(k,j) \left(f(x)+m(j,k)\right)^{b(j,k)} \end{align*} noting that in the following case, $f(x) = \left(1+\frac{1}{x}\right)^{-v}$ therefore we have $\frac{1}{x} = f(x)^{\frac{-1}{v}}-1$ therefore we have: $$ {-jv \choose k}\left(f(x)^{-\frac{1}{v}}-1\right)^k {}_2F_1\left(1,k+jv;k+1;1-f(x)^{-\frac{1}{v}}\right) = \left(f(x)+m(j,k)\right)^{b(j,k)} $$ If we set $k=3$ we find that: $$ {-jv \choose 3}\left(f(x)^{-\frac{1}{v}}-1\right)^3 {}_2F_1\left(1,3+jv;3+1;1-f(x)^{-\frac{1}{v}}\right) = c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{{-jv \choose 3}}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}(jv+1)(jv+2)}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right) $$ When we simplify we have: $$ c(j,k)\left(f(x)+m(j,3)\right)^{b(j,3)} = \frac{-jv}{6\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(6f(x)^{\frac{2}{v}} \left(\frac{(jv)^2+2jv}{jv}\right) +3f(x)^{\frac{3}{v}} \left(\frac{2+2f(x)^j-(jv)^2-3jv}{jv}\right) - 3f(x)^{\frac{1}{v}}\left(jv+1\right)\right) = \frac{jv}{\left(f(x)^{\frac{-1}{v}}-1\right)^{3}}\left(\frac{jv}{2}f(x)^{\frac{1}{v}}[jv+1]-f(x)^{\frac{2}{v}} [jv+2] +\frac{1}{2}f(x)^{\frac{3}{v}} [jv+3-\frac{2(1+f(x)^j)}{jv}]\right) $$ Although i have not proven explicitly that such functions do not exists, the resultant of the terms do not even resemble anything like the binomial theorem especially considering that there is a lone $f(x)^j$ term in the middle of the convolution. Although this proof is not complete, it provides strong evidence that this solution is most likely either non-trivial by my definition or if such a solution is considered trivial, certainly it will pathological with respect to typical binomial theorem solutions.",,"['calculus', 'sequences-and-series', 'combinatorics', 'analysis', 'binomial-theorem']"
58,Show that a subsequence of a Cauchy subsequence is also Cauchy,Show that a subsequence of a Cauchy subsequence is also Cauchy,,"My approach for solving this problem was to first prove that the sequence is convergent and then use that to prove that the subsequence is convergent and therefore Cauchy. I'm pretty sure this process seems right but I'm not sure how to exactly prove this. If anyone can help, I'd appreciate it. Thanks","My approach for solving this problem was to first prove that the sequence is convergent and then use that to prove that the subsequence is convergent and therefore Cauchy. I'm pretty sure this process seems right but I'm not sure how to exactly prove this. If anyone can help, I'd appreciate it. Thanks",,"['sequences-and-series', 'analysis', 'cauchy-sequences']"
59,Infinite Discrete Form of Jensen's Inequality,Infinite Discrete Form of Jensen's Inequality,,"I refer to Wikipedia: https://en.wikipedia.org/wiki/Jensen%27s_inequality#Alternative_finite_form ""There is also an infinite discrete form."" There is no mention of how exactly the infinite discrete form looks like, but I guess that it is something like if: $\sum_{n=1}^\infty \lambda_n=1$, then $\varphi(\sum \lambda_n x_n)\leq \sum \lambda_n \phi(x_n)$. Is that correct? How is the infinite discrete form proved? Thanks a lot.","I refer to Wikipedia: https://en.wikipedia.org/wiki/Jensen%27s_inequality#Alternative_finite_form ""There is also an infinite discrete form."" There is no mention of how exactly the infinite discrete form looks like, but I guess that it is something like if: $\sum_{n=1}^\infty \lambda_n=1$, then $\varphi(\sum \lambda_n x_n)\leq \sum \lambda_n \phi(x_n)$. Is that correct? How is the infinite discrete form proved? Thanks a lot.",,"['real-analysis', 'analysis', 'inequality']"
60,Why can any union of open intervals be written as union of disjoint open intervals?,Why can any union of open intervals be written as union of disjoint open intervals?,,"Hi I see many people asked this question,but none of them proved this by considering all possible cases which my professor did. Below is the solution that I have. Show that a union of open intervals can be written as a disjoint union of open intervals. Solution. The whole problem can be reduced to writing the union of two intervals as a union of disjoint ones (in case we have several, or uncountably many, we do induction). We are given (a, b) and (c, d) and we want to write (a, b) ∪ (c, d) as a union of disjoint intervals. There are three cases that can occur: • a < b ≤ c < d. Then the two intervals are already disjoint, so we are done. • a ≤ c < b ≤ d. Then (a, b) ∪ (c, d) = (a, d). • a ≤ c < d ≤ b. Then (a, b) ∪ (c, d) = (a, b). I am not sure why there are only three cases. Can we have (c,d) and (c,b) as well? so I think there must be 5 cases that we have to consider. Also, I do not understand how I can apply this to show that any union of open intervals can be written as union of disjoint open intervals. Can anyone explain?","Hi I see many people asked this question,but none of them proved this by considering all possible cases which my professor did. Below is the solution that I have. Show that a union of open intervals can be written as a disjoint union of open intervals. Solution. The whole problem can be reduced to writing the union of two intervals as a union of disjoint ones (in case we have several, or uncountably many, we do induction). We are given (a, b) and (c, d) and we want to write (a, b) ∪ (c, d) as a union of disjoint intervals. There are three cases that can occur: • a < b ≤ c < d. Then the two intervals are already disjoint, so we are done. • a ≤ c < b ≤ d. Then (a, b) ∪ (c, d) = (a, d). • a ≤ c < d ≤ b. Then (a, b) ∪ (c, d) = (a, b). I am not sure why there are only three cases. Can we have (c,d) and (c,b) as well? so I think there must be 5 cases that we have to consider. Also, I do not understand how I can apply this to show that any union of open intervals can be written as union of disjoint open intervals. Can anyone explain?",,"['real-analysis', 'general-topology', 'analysis', 'elementary-set-theory']"
61,Lebesgue Measurable Sets Are Translation Invariant,Lebesgue Measurable Sets Are Translation Invariant,,"I'd like a few proofs that translation of Lebesgue measurable sets are Lebesgue Measurable. As an example, and to add to the collection, I've included my own proof of the statement as an answer. Let $\mathfrak{M}(\mu)$ be the space of $\mu$-measurable sets as defined in Baby Rudin as the countable union of things in $\mathfrak{M}_F(\mu)$, finitely $\mu$-measurable sets. Any and all methods of proof appreciated. I suppose there's a more elegant way to prove this statement.","I'd like a few proofs that translation of Lebesgue measurable sets are Lebesgue Measurable. As an example, and to add to the collection, I've included my own proof of the statement as an answer. Let $\mathfrak{M}(\mu)$ be the space of $\mu$-measurable sets as defined in Baby Rudin as the countable union of things in $\mathfrak{M}_F(\mu)$, finitely $\mu$-measurable sets. Any and all methods of proof appreciated. I suppose there's a more elegant way to prove this statement.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'measurable-sets']"
62,Proving a function is a metric,Proving a function is a metric,,"Define a function $d : \mathbb{Z × Z \to R}$ by setting $d(x, x) = 0$ for all $x ∈ \mathbb{Z}$, and if $x \ne y$, setting $d(x, y) = 3^{-k}$ where $3^k$ is the largest power of $3$ dividing $x − y$. Prove that $d$ is a metric on $\mathbb{Z}$. I've proven the first three parts needed to show it's a metric, but I can't figure out how to prove the triangle inequality for the function.  I know I have to show $d(x,z) \le d(x,y) + d(y,z)$, but I'm confused about how I do that for this function.","Define a function $d : \mathbb{Z × Z \to R}$ by setting $d(x, x) = 0$ for all $x ∈ \mathbb{Z}$, and if $x \ne y$, setting $d(x, y) = 3^{-k}$ where $3^k$ is the largest power of $3$ dividing $x − y$. Prove that $d$ is a metric on $\mathbb{Z}$. I've proven the first three parts needed to show it's a metric, but I can't figure out how to prove the triangle inequality for the function.  I know I have to show $d(x,z) \le d(x,y) + d(y,z)$, but I'm confused about how I do that for this function.",,"['real-analysis', 'analysis']"
63,Squeeze Theorem with strict inequalities,Squeeze Theorem with strict inequalities,,"Suppose that $I$ is an interval, and $g,f,h$ are functions defined on $I$, except possibly at $x_0$. Furthermore, $\forall \varepsilon>0,$ $\forall x\in I,$ and for some constant $L$, $$ (L-\varepsilon)g(x) < f(x) \le Lh(x) .$$ Also, $$\lim_{x\to x_0} g(x) = \lim_{x\to x_0} h(x) = 1.$$ Then is it true that $\forall \varepsilon>0,$ $$L-\varepsilon < \lim_{x\to x_0} f(x) \le L,$$ and since $\varepsilon$ can be taken to be arbitrarily small, the middle is forced to give $$\lim_{x\to x_0} f(x)=L?$$ Note that this is essentially the Squeeze Theorem with a strict lower-bound.","Suppose that $I$ is an interval, and $g,f,h$ are functions defined on $I$, except possibly at $x_0$. Furthermore, $\forall \varepsilon>0,$ $\forall x\in I,$ and for some constant $L$, $$ (L-\varepsilon)g(x) < f(x) \le Lh(x) .$$ Also, $$\lim_{x\to x_0} g(x) = \lim_{x\to x_0} h(x) = 1.$$ Then is it true that $\forall \varepsilon>0,$ $$L-\varepsilon < \lim_{x\to x_0} f(x) \le L,$$ and since $\varepsilon$ can be taken to be arbitrarily small, the middle is forced to give $$\lim_{x\to x_0} f(x)=L?$$ Note that this is essentially the Squeeze Theorem with a strict lower-bound.",,['calculus']
64,Proving or disproving continuity of $f(x+a)=\frac{1+f(x)}{1-f(x)}$,Proving or disproving continuity of,f(x+a)=\frac{1+f(x)}{1-f(x)},"Let $f$ be defined $\forall x\in\mathbb{R}$ and let $$ f(x+a)=\frac{1+f(x)}{1-f(x)} $$ hold for some $a\in\mathbb{R}^{+}$ and $\forall x\in\mathbb{R}$. Show that $f$ is periodic with a period of $4a$ and using that, check if $f$ is continuous on $\mathbb{R}$. So I've proven the part that it's periodic, that was simple: $$ f(x+4a)=\frac{1+f(x+3a)}{1-f(x+3a)} = \frac{1+\frac{1+f(x+2a)}{1-f(x+2a)}}{1-\frac{1+f(x+2a)}{1-f(x+2a)}}=\cdots=\frac{-2f(x)}{-2}=f(x), \ \forall x\in\mathbb{R}. $$ But now I can't seem to figure out how to use the fact that $f$ is periodic to prove or disprove it's continuity. Any hints?","Let $f$ be defined $\forall x\in\mathbb{R}$ and let $$ f(x+a)=\frac{1+f(x)}{1-f(x)} $$ hold for some $a\in\mathbb{R}^{+}$ and $\forall x\in\mathbb{R}$. Show that $f$ is periodic with a period of $4a$ and using that, check if $f$ is continuous on $\mathbb{R}$. So I've proven the part that it's periodic, that was simple: $$ f(x+4a)=\frac{1+f(x+3a)}{1-f(x+3a)} = \frac{1+\frac{1+f(x+2a)}{1-f(x+2a)}}{1-\frac{1+f(x+2a)}{1-f(x+2a)}}=\cdots=\frac{-2f(x)}{-2}=f(x), \ \forall x\in\mathbb{R}. $$ But now I can't seem to figure out how to use the fact that $f$ is periodic to prove or disprove it's continuity. Any hints?",,"['real-analysis', 'analysis', 'functions']"
65,Question about function with Lipschitz derivative,Question about function with Lipschitz derivative,,"I cannot solve the following problem: Show that $$\left|\int_a^bf(u)\,du-f(a)(b-a)-\frac{1}{2}f'(a)(b-a)^2\right|\le\frac{C}{2}|b-a|^{2+\delta},$$ where $0\le a\le b \le 1$, $f$ is continuous, $f'$ is Lipschitz of order $\delta$ on $[0,1]$ and $C$ is the Lipschitz constant. The only result I've got so far is that $$\left|\int_a^bf(u)\,du-f(a)(b-a)\right|\le\frac{C}{2}|b-a|^{2},$$ and I have no idea how to prove the initial problem.","I cannot solve the following problem: Show that $$\left|\int_a^bf(u)\,du-f(a)(b-a)-\frac{1}{2}f'(a)(b-a)^2\right|\le\frac{C}{2}|b-a|^{2+\delta},$$ where $0\le a\le b \le 1$, $f$ is continuous, $f'$ is Lipschitz of order $\delta$ on $[0,1]$ and $C$ is the Lipschitz constant. The only result I've got so far is that $$\left|\int_a^bf(u)\,du-f(a)(b-a)\right|\le\frac{C}{2}|b-a|^{2},$$ and I have no idea how to prove the initial problem.",,"['analysis', 'continuity', 'lipschitz-functions']"
66,Properties of decreasing sequence of Lebesgue measurable sets.,Properties of decreasing sequence of Lebesgue measurable sets.,,"I'm trying to prove a property of Lebesgue measure sets that says: If the $A_{k}$'s are measurable and $A_{1} \supset A_{2} \supset A_{3} \supset \ldots,$ and if $\lambda (A_{1}) < \infty, $ then  $$\lambda ( \bigcap_{k=1}^{\infty}  A_{k} ) = \lim_{k \to \infty} \lambda (A_{k}).$$ And I'm trying to emulate the proof of the previous property that says: If the $A_{k}$'s are measurable and $A_{1} \subset A_{2} \subset A_{3} \subset \ldots,$ then  $$\lambda ( \bigcup_{k=1}^{\infty}  A_{k} ) = \lim_{k \to \infty} \lambda (A_{k}).$$ And they express the union $\bigcup A_{k}$ as a disjoint union like this: $$\bigcup_{k=1}^{\infty} A_{k} = A_{1} \cup \bigcup_{k=2}^{\infty} (A_{k} \sim A_{k-1})$$ So I'm trying to create a increasing sequence from my decreasing one like this: Say $(A_{k})= A_{1} \supset A_{2} \supset \ldots$ then $B_{k}=A_{1} - A_{k}$ and I'm not quite sure if $(B_{k})$ is an increasing sequence, and if so, I'm able to express this as a disjoint union like before? I don't know if I'm on good track or I'm completely lost, can you help me with this please? Also, what's the point of  $\lambda (A_{1}) < \infty $ how can I use that? Thank you guys!","I'm trying to prove a property of Lebesgue measure sets that says: If the $A_{k}$'s are measurable and $A_{1} \supset A_{2} \supset A_{3} \supset \ldots,$ and if $\lambda (A_{1}) < \infty, $ then  $$\lambda ( \bigcap_{k=1}^{\infty}  A_{k} ) = \lim_{k \to \infty} \lambda (A_{k}).$$ And I'm trying to emulate the proof of the previous property that says: If the $A_{k}$'s are measurable and $A_{1} \subset A_{2} \subset A_{3} \subset \ldots,$ then  $$\lambda ( \bigcup_{k=1}^{\infty}  A_{k} ) = \lim_{k \to \infty} \lambda (A_{k}).$$ And they express the union $\bigcup A_{k}$ as a disjoint union like this: $$\bigcup_{k=1}^{\infty} A_{k} = A_{1} \cup \bigcup_{k=2}^{\infty} (A_{k} \sim A_{k-1})$$ So I'm trying to create a increasing sequence from my decreasing one like this: Say $(A_{k})= A_{1} \supset A_{2} \supset \ldots$ then $B_{k}=A_{1} - A_{k}$ and I'm not quite sure if $(B_{k})$ is an increasing sequence, and if so, I'm able to express this as a disjoint union like before? I don't know if I'm on good track or I'm completely lost, can you help me with this please? Also, what's the point of  $\lambda (A_{1}) < \infty $ how can I use that? Thank you guys!",,"['analysis', 'measure-theory', 'lebesgue-measure']"
67,find smallest $x>0$ such that $\frac{A}{cx}e^{-cx^2}\le \varepsilon$,find smallest  such that,x>0 \frac{A}{cx}e^{-cx^2}\le \varepsilon,"I was estimating some error and I got $$\varepsilon(x)\le\frac{A}{cx}e^{-cx^2}$$ $A,c$ are known and positive, $x$ is also positive. The bigger the $x$ smaller the error.  But I need to find the smallest possible $x$ such that $\,\varepsilon(x)\le \varepsilon$, lets say $\varepsilon = 10^{-5} $. To be specific, I need $$ \inf_{x\in A}A=\left\{ \ x>0\ \ \Big\rvert \ \  \frac{A}{cx}\, e^{-cx^2}\le \varepsilon \ \right\}.$$ It does not need to be exactly infimum but try to get as close as you can.","I was estimating some error and I got $$\varepsilon(x)\le\frac{A}{cx}e^{-cx^2}$$ $A,c$ are known and positive, $x$ is also positive. The bigger the $x$ smaller the error.  But I need to find the smallest possible $x$ such that $\,\varepsilon(x)\le \varepsilon$, lets say $\varepsilon = 10^{-5} $. To be specific, I need $$ \inf_{x\in A}A=\left\{ \ x>0\ \ \Big\rvert \ \  \frac{A}{cx}\, e^{-cx^2}\le \varepsilon \ \right\}.$$ It does not need to be exactly infimum but try to get as close as you can.",,"['analysis', 'inequality']"
68,Smooth maps preserve dimension,Smooth maps preserve dimension,,"I stumbled over a useful consequence, that is apparently wrong for only continuous maps. Imagine $A \subset \mathbb{R}^{n-1}$ is a compact set and $F : \mathbb{R}^{n-1} \rightarrow S^{n}$ a smooth map, then we cannot have $F(A) = S^{n}.$ In other words: Smooth maps must preserve the dimension somehow. Does anybody know how to show this?","I stumbled over a useful consequence, that is apparently wrong for only continuous maps. Imagine $A \subset \mathbb{R}^{n-1}$ is a compact set and $F : \mathbb{R}^{n-1} \rightarrow S^{n}$ a smooth map, then we cannot have $F(A) = S^{n}.$ In other words: Smooth maps must preserve the dimension somehow. Does anybody know how to show this?",,"['calculus', 'real-analysis', 'analysis', 'differential-geometry', 'differential-topology']"
69,Prove that the following function is $C^{\infty}$ [duplicate],Prove that the following function is  [duplicate],C^{\infty},"This question already has answers here : Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order? (2 answers) Closed 8 years ago . Prove that the following function: $$r:x \mapsto \begin{cases} e^{-{1\over (1-x^2)}},  & \text{if $|x|<1$} \\ 0, & \text{if $|x| \ge 1$} \end{cases}$$  is $C^{\infty}$ I found this problem on internet and i was interested to find a proof but i did't find any otheρ similar with this exercise.It would be very nice if we can have a proof for this.","This question already has answers here : Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order? (2 answers) Closed 8 years ago . Prove that the following function: $$r:x \mapsto \begin{cases} e^{-{1\over (1-x^2)}},  & \text{if $|x|<1$} \\ 0, & \text{if $|x| \ge 1$} \end{cases}$$  is $C^{\infty}$ I found this problem on internet and i was interested to find a proof but i did't find any otheρ similar with this exercise.It would be very nice if we can have a proof for this.",,"['calculus', 'real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
70,"Metric spaces as Cauchy complete categories, nlab entry, insight into a few of the constructions.","Metric spaces as Cauchy complete categories, nlab entry, insight into a few of the constructions.",,"I'm having a bit of trouble making sense of some of the concepts in the ""Metric space"" section on nlab's entry on ""Cauchy complete category"" ( http://ncatlab.org/nlab/show/Cauchy+complete+category#metric_spaces ), I was sort of following what they were saying until I got to this: $p(x)$ should be thought of as the distance $d_{\overline{X}}(x,p)$   betwen $x$ and the ""ideal point"" $p$ in the Cauchy completion... So $p$ is a point in the completion and a distance function? I don't think I follow. Also, the distance between two points in the Cauchy completion is given by the "" usual formula formula for enriched presheaves "": $d(p,p') = \int_{x \in X} hom_{[0, \infty]}(p(x),p'(x))$ = supmax $_{x \in X}$ $\{0, p'(x)- p(x) \}$ Where did that formula come from? I stopped reading right there, any insight would be appreciated.","I'm having a bit of trouble making sense of some of the concepts in the ""Metric space"" section on nlab's entry on ""Cauchy complete category"" ( http://ncatlab.org/nlab/show/Cauchy+complete+category#metric_spaces ), I was sort of following what they were saying until I got to this: $p(x)$ should be thought of as the distance $d_{\overline{X}}(x,p)$   betwen $x$ and the ""ideal point"" $p$ in the Cauchy completion... So $p$ is a point in the completion and a distance function? I don't think I follow. Also, the distance between two points in the Cauchy completion is given by the "" usual formula formula for enriched presheaves "": $d(p,p') = \int_{x \in X} hom_{[0, \infty]}(p(x),p'(x))$ = supmax $_{x \in X}$ $\{0, p'(x)- p(x) \}$ Where did that formula come from? I stopped reading right there, any insight would be appreciated.",,"['general-topology', 'analysis', 'metric-spaces', 'category-theory']"
71,What to know about convergence of integrals,What to know about convergence of integrals,,"According to the values of p>0 examine the convergence of the integral: $$\int_0^{+\infty} \dfrac{\ln(1+2x^{3p})}{(x+x^2)^{4p}\arctan(x)^{1/2}}dx$$ I didn't find a good explanation about this kind of problems,so i will be glad if someone say a few words about it.","According to the values of p>0 examine the convergence of the integral: $$\int_0^{+\infty} \dfrac{\ln(1+2x^{3p})}{(x+x^2)^{4p}\arctan(x)^{1/2}}dx$$ I didn't find a good explanation about this kind of problems,so i will be glad if someone say a few words about it.",,"['real-analysis', 'analysis', 'improper-integrals']"
72,Equivalency of the set of real numbers to the set of all continuous real functions?,Equivalency of the set of real numbers to the set of all continuous real functions?,,"I understand that the set of real numbers is equivalent to the set of real numbers in the interval $(0,1)$ and also equivalent to the set of all points in $\mathbb{R}^2$. I have seen a claim in a book that the set of real numbers is also equivalent to the set of all continuous real functions of one or several variables. However, I do not know how to think of a proof. Can anyone help?","I understand that the set of real numbers is equivalent to the set of real numbers in the interval $(0,1)$ and also equivalent to the set of all points in $\mathbb{R}^2$. I have seen a claim in a book that the set of real numbers is also equivalent to the set of all continuous real functions of one or several variables. However, I do not know how to think of a proof. Can anyone help?",,"['real-analysis', 'analysis']"
73,Why do the dimensions not line up when I calculate this (directional) derivative using the chain rule?,Why do the dimensions not line up when I calculate this (directional) derivative using the chain rule?,,"an arbitrary, differentiable function $f : \mathbb{R}^n \to \mathbb{R}$ and the function $\gamma: \mathbb{R} \to \mathbb{R}^n$ defined as $\gamma(t) = u + tv$, where $u, v$ are fixed vectors in $\mathbb{R}^n$, I'm trying to find the derivative with respect to $t$ of $(f \circ \gamma)(t)$ at $t = 0$. From the chain rule, I know that $D(f \circ \gamma)(t) = Df(\gamma(t)) \cdot D \gamma(t)$. I'm confused, though, because I thought that $D(f \circ \gamma)(t)$ (the left-hand side) would give me a function from $\mathbb{R}$ to $\mathbb{R}$, but $Df(\gamma(t))$ gives me a function from $\mathbb{R}^n$ to $\mathbb{R}$ and $D \gamma(t) = v$, which is a constant function from $\mathbb{R}$ to $\mathbb{R}^n$, so the right-hand side gives me a function from $\mathbb{R}$ to $\mathbb{R}^n$. It's a simple directional derivative that I've done a million times, and I'm probably missing something easy because it's late, but why don't these dimensions line up? What am I missing?","an arbitrary, differentiable function $f : \mathbb{R}^n \to \mathbb{R}$ and the function $\gamma: \mathbb{R} \to \mathbb{R}^n$ defined as $\gamma(t) = u + tv$, where $u, v$ are fixed vectors in $\mathbb{R}^n$, I'm trying to find the derivative with respect to $t$ of $(f \circ \gamma)(t)$ at $t = 0$. From the chain rule, I know that $D(f \circ \gamma)(t) = Df(\gamma(t)) \cdot D \gamma(t)$. I'm confused, though, because I thought that $D(f \circ \gamma)(t)$ (the left-hand side) would give me a function from $\mathbb{R}$ to $\mathbb{R}$, but $Df(\gamma(t))$ gives me a function from $\mathbb{R}^n$ to $\mathbb{R}$ and $D \gamma(t) = v$, which is a constant function from $\mathbb{R}$ to $\mathbb{R}^n$, so the right-hand side gives me a function from $\mathbb{R}$ to $\mathbb{R}^n$. It's a simple directional derivative that I've done a million times, and I'm probably missing something easy because it's late, but why don't these dimensions line up? What am I missing?",,"['calculus', 'analysis', 'derivatives']"
74,If $\mu$ is $\sigma$ finite and $f_n \rightarrow f$ a.e then $f_n \rightarrow f$ uniformly on each $E_j$,If  is  finite and  a.e then  uniformly on each,\mu \sigma f_n \rightarrow f f_n \rightarrow f E_j,"If $\mu$ is $\sigma$ finite and $f_n \rightarrow f$ a.e, there exists $E_1,E_2, \ldots \subset X$ such that $\mu((\bigcup_{1}^{\infty}E_j)^{c})=0$ and $f_n \rightarrow f$ uniformly on each $E_j$ My attempt : $f_n \rightarrow f$ a.e thus $A:= \lbrace x \mid (f_n \nrightarrow f )  \rbrace $ then $\mu(A)=0$ Now let  $E_1,E_2, \ldots \subset X$ such that $\bigcup_{1}^{\infty}E_j=X$ from $\sigma$-finiteness of measure , $\mu(E_j) < \infty$$ \forall $ $j$ Thus now by Egoroff theorem for every $\epsilon$ $\exists$ $E_{j_k} \subset E_j$ such that $\mu(E_{j_k})<\epsilon$ and $f_n \rightarrow f$ uniformly on each $E_{j_k}^{c}$ Now I can see I need to take union of this subsequence of $E_j$ but I dont know how to do so. I am stuck here Please help.","If $\mu$ is $\sigma$ finite and $f_n \rightarrow f$ a.e, there exists $E_1,E_2, \ldots \subset X$ such that $\mu((\bigcup_{1}^{\infty}E_j)^{c})=0$ and $f_n \rightarrow f$ uniformly on each $E_j$ My attempt : $f_n \rightarrow f$ a.e thus $A:= \lbrace x \mid (f_n \nrightarrow f )  \rbrace $ then $\mu(A)=0$ Now let  $E_1,E_2, \ldots \subset X$ such that $\bigcup_{1}^{\infty}E_j=X$ from $\sigma$-finiteness of measure , $\mu(E_j) < \infty$$ \forall $ $j$ Thus now by Egoroff theorem for every $\epsilon$ $\exists$ $E_{j_k} \subset E_j$ such that $\mu(E_{j_k})<\epsilon$ and $f_n \rightarrow f$ uniformly on each $E_{j_k}^{c}$ Now I can see I need to take union of this subsequence of $E_j$ but I dont know how to do so. I am stuck here Please help.",,"['analysis', 'measure-theory', 'convergence-divergence', 'uniform-convergence']"
75,Proving Composition of Uniformly Continuous/Convergent Function Sequences,Proving Composition of Uniformly Continuous/Convergent Function Sequences,,"I'm not very good at this analysis stuff, it turns out. Introduction to Analysis went great, but this intermediate real analysis thing is kicking me in the rear. So, the problem: $f_{n}: D \rightarrow [c,d]$ converges uniformly to $F: D \rightarrow [c,d].$ The function $g$ is continuous on $[c,d].$ Prove that $g(f_{n})$ converges uniformly to $g(F)$ on $D$. Hint: Start with the continuity of $g$. I have no idea where to start with this problem. I feel like I'm missing an important fact to get started, and though there are similar problems on this site, I've not found one close enough to help :( Thanks!","I'm not very good at this analysis stuff, it turns out. Introduction to Analysis went great, but this intermediate real analysis thing is kicking me in the rear. So, the problem: $f_{n}: D \rightarrow [c,d]$ converges uniformly to $F: D \rightarrow [c,d].$ The function $g$ is continuous on $[c,d].$ Prove that $g(f_{n})$ converges uniformly to $g(F)$ on $D$. Hint: Start with the continuity of $g$. I have no idea where to start with this problem. I feel like I'm missing an important fact to get started, and though there are similar problems on this site, I've not found one close enough to help :( Thanks!",,"['real-analysis', 'analysis']"
76,approximate vanishing in Pontryagin dual,approximate vanishing in Pontryagin dual,,"Let $\{n_k\}\subseteq \mathbb{Z}$ to be any given sequence of integers, and suppose it satisfies the following property: (*) For any $\lambda\in A\subseteq \mathbb{T}$(the unit circle), $|\lambda^{n_k}-1|\to 0$ as $k\to\infty$. Question 1 : Is it true that there exists a subsequence of $\{n_k\}$, say $\{n'_{l}\}$, such that $n'_{l}=0,\forall l\geq 1$ under the following cases: Case1: $A=\mathbb{T}$ in (*). Case2: $A$ is a dense subset (or dense subgroup) of $\mathbb{T}$. Case 3: $A$ is a measurable subset of $\mathbb{T}$ with postive Lebesgue measure. RK: Clearly if $\{n_k\}$ contains a bounded subsequence, then the answer is yes. For case2, when $A$ is ""good"" enough, then the answer is no. A general version of the above question is the following: Suppose $X$ is a compact metrizable abelian group, and $\{\phi_n\}\subseteq \widehat{X}$(the Pontryagin dual of $X$). And suppose it satisfies the following property: (*) For any $x\in A\subseteq X$, $|\phi_n(x)-1|\to 0$ as $n\to \infty$. Question 2 : Can we prove that there exists a subsequence of $\phi_n$, say $\phi'_l$, such that $\phi'_l(x)=1 \forall x\in X, l\geq 1$ under the assumption that $A\subseteq X$ is a dense subset(or subgroup)?","Let $\{n_k\}\subseteq \mathbb{Z}$ to be any given sequence of integers, and suppose it satisfies the following property: (*) For any $\lambda\in A\subseteq \mathbb{T}$(the unit circle), $|\lambda^{n_k}-1|\to 0$ as $k\to\infty$. Question 1 : Is it true that there exists a subsequence of $\{n_k\}$, say $\{n'_{l}\}$, such that $n'_{l}=0,\forall l\geq 1$ under the following cases: Case1: $A=\mathbb{T}$ in (*). Case2: $A$ is a dense subset (or dense subgroup) of $\mathbb{T}$. Case 3: $A$ is a measurable subset of $\mathbb{T}$ with postive Lebesgue measure. RK: Clearly if $\{n_k\}$ contains a bounded subsequence, then the answer is yes. For case2, when $A$ is ""good"" enough, then the answer is no. A general version of the above question is the following: Suppose $X$ is a compact metrizable abelian group, and $\{\phi_n\}\subseteq \widehat{X}$(the Pontryagin dual of $X$). And suppose it satisfies the following property: (*) For any $x\in A\subseteq X$, $|\phi_n(x)-1|\to 0$ as $n\to \infty$. Question 2 : Can we prove that there exists a subsequence of $\phi_n$, say $\phi'_l$, such that $\phi'_l(x)=1 \forall x\in X, l\geq 1$ under the assumption that $A\subseteq X$ is a dense subset(or subgroup)?",,"['sequences-and-series', 'analysis', 'reference-request', 'dynamical-systems', 'topological-groups']"
77,"Definition of ""deterministic coupling"" [Villani]","Definition of ""deterministic coupling"" [Villani]",,"I'm currently reading through ""Optimal transport, old and new"" by Cédric Villani. In the first chapter, he defines a coupling of two probability spaces $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ as a probability measure $\pi$ defined on $\mathcal{X} \times \mathcal{Y}$ such that $\pi$ admits $\mu$ and $\nu$ as marginals over $\mathcal{X}$ and $\mathcal{Y}$ respectively. This is fine, I understand it. However, he then goes on to define a special kind of coupling, a deterministic coupling of $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ which satisfies: There is a measurable function $T$ such that $\mathcal{Y} = T(\mathcal{X})$ $\pi =(Id,T)_\# \mu $ (where # denotes the pushforward measure). Point number 2 is what confuses me. $\pi$ is defined over $\mathcal{X} \times \mathcal{Y}$, however for any subset $E$ of $\mathcal{X} \times \mathcal{Y}$, $(Id,T)_\# \mu (E) = \mu((Id,T)^{-1}(E))$ but $\mu$ is only defined over $\mathcal{X}$, so I'm not sure how $\mu((Id,T)^{-1}(E))$ makes sense? Also, I'm unsure how to interpret $(Id,T)^{-1}(E)$. If anyone is familiar with the above terminology I would greatly appreciate some guidance.","I'm currently reading through ""Optimal transport, old and new"" by Cédric Villani. In the first chapter, he defines a coupling of two probability spaces $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ as a probability measure $\pi$ defined on $\mathcal{X} \times \mathcal{Y}$ such that $\pi$ admits $\mu$ and $\nu$ as marginals over $\mathcal{X}$ and $\mathcal{Y}$ respectively. This is fine, I understand it. However, he then goes on to define a special kind of coupling, a deterministic coupling of $(\mathcal{X},\mu)$ and $(\mathcal{Y},\nu)$ which satisfies: There is a measurable function $T$ such that $\mathcal{Y} = T(\mathcal{X})$ $\pi =(Id,T)_\# \mu $ (where # denotes the pushforward measure). Point number 2 is what confuses me. $\pi$ is defined over $\mathcal{X} \times \mathcal{Y}$, however for any subset $E$ of $\mathcal{X} \times \mathcal{Y}$, $(Id,T)_\# \mu (E) = \mu((Id,T)^{-1}(E))$ but $\mu$ is only defined over $\mathcal{X}$, so I'm not sure how $\mu((Id,T)^{-1}(E))$ makes sense? Also, I'm unsure how to interpret $(Id,T)^{-1}(E)$. If anyone is familiar with the above terminology I would greatly appreciate some guidance.",,"['analysis', 'measure-theory', 'optimal-transport']"
78,Proof using Monotone Class Theorem,Proof using Monotone Class Theorem,,"As you know I have been grappling with this question since days ago, which I copy down here for convenience: Let $X$ be set of $\mathbb R$ , and let $\mathcal B$ be its Borel $\sigma$ -algebra, and finally let $\mu_1$ and $\mu_2$ be the two measures on $(X,\mathcal B)$ such that $\mu_1((a,b))= \mu_2((a,b)) < \infty$ whenever $−\infty < a < b < \infty$ . Show that $\mu_1(A) = \mu_2(A)$ whenever $A \in \mathcal B$ .​ As you know again that this question comes from early chapters of Richard F. Bass's introductory book , therefore any solution should involve no advanced theorems such as Dynkin's. The consensus I got so far is to use the Monotone Class Theorem: Suppose $\mathscr A_0$ is an algebra, $\mathscr A$ is the smallest $\sigma$ -algebra containing $\mathscr A_0$ , and $\mathscr M$ is the smallest monotone class containing $\mathscr A_0$ , then $\mathscr A = \mathscr M$ . Here are my three questions: (1) My proof idea (naive, perhaps) is to show that, $$\begin{align} \text{if } A \in \mathcal B &\text{ then } A \in \mathscr A \\ \text{if } (a, b) \in \mathbb R^1 &\text{ then } (a, b) \in \mathscr M. \\ \end{align}$$ Since by the Monotone Theorem $\mathscr A = \mathscr M$ , therefore I can arrive at the result. Is this idea so flawed that it is dead on arrival? (Who knows, since breakthroughs sometimes happen when someone asks a really dumb question. :-) ) (2) If my idea is valid, how do you prove that if $A \in \mathcal B \rightarrow A \in \mathscr A$ ? I mean the mechanic of going from $A \in \mathcal B$ to $ A \in \mathscr A$ ? (3) And finally, what is the mechanics of proving that if $(a, b) \in \mathbb R^1 \rightarrow (a, b) \in \mathscr M$ ? Thank you for your time. POST SCRIPT! POST SCRIPT! : I finally came up with solution without any advanced theorems, even the Monotone Theorem, adapted from a solution by @JoshKeneda, who had used Dynkin's Theorem. My solution is posted here at the answer page down below. I have submitted this work to my professor, he ok'd it except for (5) because it is true only when the $A_i$ 's are pairwise disjoint. Feel free to drop me a message if you have ideas to improve (5). Thanks to all and especially to @JoshKeneda.","As you know I have been grappling with this question since days ago, which I copy down here for convenience: Let be set of , and let be its Borel -algebra, and finally let and be the two measures on such that whenever . Show that whenever .​ As you know again that this question comes from early chapters of Richard F. Bass's introductory book , therefore any solution should involve no advanced theorems such as Dynkin's. The consensus I got so far is to use the Monotone Class Theorem: Suppose is an algebra, is the smallest -algebra containing , and is the smallest monotone class containing , then . Here are my three questions: (1) My proof idea (naive, perhaps) is to show that, Since by the Monotone Theorem , therefore I can arrive at the result. Is this idea so flawed that it is dead on arrival? (Who knows, since breakthroughs sometimes happen when someone asks a really dumb question. :-) ) (2) If my idea is valid, how do you prove that if ? I mean the mechanic of going from to ? (3) And finally, what is the mechanics of proving that if ? Thank you for your time. POST SCRIPT! POST SCRIPT! : I finally came up with solution without any advanced theorems, even the Monotone Theorem, adapted from a solution by @JoshKeneda, who had used Dynkin's Theorem. My solution is posted here at the answer page down below. I have submitted this work to my professor, he ok'd it except for (5) because it is true only when the 's are pairwise disjoint. Feel free to drop me a message if you have ideas to improve (5). Thanks to all and especially to @JoshKeneda.","X \mathbb R \mathcal B \sigma \mu_1 \mu_2 (X,\mathcal B) \mu_1((a,b))= \mu_2((a,b)) < \infty −\infty < a < b < \infty \mu_1(A) = \mu_2(A) A \in \mathcal B \mathscr A_0 \mathscr A \sigma \mathscr A_0 \mathscr M \mathscr A_0 \mathscr A = \mathscr M \begin{align}
\text{if } A \in \mathcal B &\text{ then } A \in \mathscr A \\
\text{if } (a, b) \in \mathbb R^1 &\text{ then } (a, b) \in \mathscr M. \\
\end{align} \mathscr A = \mathscr M A \in \mathcal B \rightarrow A \in \mathscr A A \in \mathcal B  A \in \mathscr A (a, b) \in \mathbb R^1 \rightarrow (a, b) \in \mathscr M A_i","['real-analysis', 'analysis', 'measure-theory', 'monotone-class-theorem']"
79,Proving a subset of R is countable,Proving a subset of R is countable,,"I have a subset $V$ of $\mathbb{R}$. I know that given any sequence $(u_n)$ of elements of $V$, $(u_n)$ doesn't converge in $\mathbb{R}$. It seems ""obvious"" that it implies that $V$ is countable. However I am not familiar with this kind of proofs, and I am not able to write explicitly why the properties of the sequence imply that $V$ is at most countable. Do someone knows which classical theorem we should apply? Or can someone write explicitly the proof ? Thank you.","I have a subset $V$ of $\mathbb{R}$. I know that given any sequence $(u_n)$ of elements of $V$, $(u_n)$ doesn't converge in $\mathbb{R}$. It seems ""obvious"" that it implies that $V$ is countable. However I am not familiar with this kind of proofs, and I am not able to write explicitly why the properties of the sequence imply that $V$ is at most countable. Do someone knows which classical theorem we should apply? Or can someone write explicitly the proof ? Thank you.",,"['real-analysis', 'general-topology', 'analysis']"
80,Clarification from old post: Union of sigma-algebras is non sigma-algebra,Clarification from old post: Union of sigma-algebras is non sigma-algebra,,"I have been working on slightly different problem from one posted back in 2013 here . I followed closely the hints given by @martini there, but nevertheless I still got stuck. I am retyping the question here not to duplicate but for your convenience: Suppose $\mathscr A_1 \subset \mathscr A_2 \subset \ldots$ are $\sigma$-algebras of subsets of set $X$ . Give example of $\bigcup_{i=1}^{\infty} \mathscr A_i$ that is non $\sigma$-algebra. (The 2013 old post did not have the phrase in bold .) And to that question @martini suggested to generate an example using natural numbers $\mathbb N$ and $F_n = \{\{1\}, \{2\}, \ldots \{n\}\}$, which I think makes sense and is relevant for my question. Here are what I have gone so far: (1) Let $F_n = \{\{1\}, \{2\}, \ldots \{n\}\}$, let $\sigma(F_n)$ be its $\sigma$-algebra, and let make this example simple by making $n=2$ only (2) If $F_1:=\{1\}$, then $\sigma(F_1)=\{\emptyset,\{1\},\{1\}^c,\mathbb N \}$ (3) If $F_2:=\{\{1\},\{2\}\}$, then $\sigma(F_2)=\{\emptyset,\{1\},\{1\}^c,\{2\},\{2\}^c  \{1,2\},\{1,2\}^c,\mathbb N\}$ (4) Here $\sigma(F_1) \cup \sigma(F_2)=\sigma(F_2)$, and $\sigma(F_2)$ is a $\sigma$-algebra. Since I am looking for non $\sigma$-algebra, therefore this is not the example I have been looking for. I thinks I have been misunderstanding some concepts from the beginning, but what are they? Thank you for your time and help. NOTE: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To find out if this posting is a duplicate, I did my due diligence by asking opinions from experienced users here , and prior to that I had tried to post the question outside but got only lukewarm response.","I have been working on slightly different problem from one posted back in 2013 here . I followed closely the hints given by @martini there, but nevertheless I still got stuck. I am retyping the question here not to duplicate but for your convenience: Suppose $\mathscr A_1 \subset \mathscr A_2 \subset \ldots$ are $\sigma$-algebras of subsets of set $X$ . Give example of $\bigcup_{i=1}^{\infty} \mathscr A_i$ that is non $\sigma$-algebra. (The 2013 old post did not have the phrase in bold .) And to that question @martini suggested to generate an example using natural numbers $\mathbb N$ and $F_n = \{\{1\}, \{2\}, \ldots \{n\}\}$, which I think makes sense and is relevant for my question. Here are what I have gone so far: (1) Let $F_n = \{\{1\}, \{2\}, \ldots \{n\}\}$, let $\sigma(F_n)$ be its $\sigma$-algebra, and let make this example simple by making $n=2$ only (2) If $F_1:=\{1\}$, then $\sigma(F_1)=\{\emptyset,\{1\},\{1\}^c,\mathbb N \}$ (3) If $F_2:=\{\{1\},\{2\}\}$, then $\sigma(F_2)=\{\emptyset,\{1\},\{1\}^c,\{2\},\{2\}^c  \{1,2\},\{1,2\}^c,\mathbb N\}$ (4) Here $\sigma(F_1) \cup \sigma(F_2)=\sigma(F_2)$, and $\sigma(F_2)$ is a $\sigma$-algebra. Since I am looking for non $\sigma$-algebra, therefore this is not the example I have been looking for. I thinks I have been misunderstanding some concepts from the beginning, but what are they? Thank you for your time and help. NOTE: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ To find out if this posting is a duplicate, I did my due diligence by asking opinions from experienced users here , and prior to that I had tried to post the question outside but got only lukewarm response.",,"['analysis', 'measure-theory']"
81,Show that the Euclidean Metric is less than or equal to the Taxi-cab metric for $\mathbb{R}^{n}$,Show that the Euclidean Metric is less than or equal to the Taxi-cab metric for,\mathbb{R}^{n},"I am trying to prove that the Euclidean metric; $(\mathbb{R}^{n},d^{2})$; defined: $$d^2(x,y) = \sqrt{\sum_{i=1}^n(x_i-y _i)^2}. $$ is less than or equal to the Taxi Cab metric; $(\mathbb{R}^{n},d^{1})$ defined: $$d^1(x,y) = \sum_{i=1}^{n}|x_i-y_i|.$$  It is obvious to me that this like comparing the length of a hypotenuse with the length of two sides of a triangle. The only thing I can think of to illustrate this inequality is that if I observe that in each case, $d: X \times X \rightarrow \mathbb{R}$ I have that for $d^2(x,y)= \sqrt{r^2}$ and $d^{1}(x,y) = |r|$, clearly $$\sqrt{r^2} \le |r|.$$ This is suspiciously too simple. For example, it seems that it is missing the scenario when the L.H.S. is less than the R.H.S., but is that necessary? Any tips are appreciated. Thanks!","I am trying to prove that the Euclidean metric; $(\mathbb{R}^{n},d^{2})$; defined: $$d^2(x,y) = \sqrt{\sum_{i=1}^n(x_i-y _i)^2}. $$ is less than or equal to the Taxi Cab metric; $(\mathbb{R}^{n},d^{1})$ defined: $$d^1(x,y) = \sum_{i=1}^{n}|x_i-y_i|.$$  It is obvious to me that this like comparing the length of a hypotenuse with the length of two sides of a triangle. The only thing I can think of to illustrate this inequality is that if I observe that in each case, $d: X \times X \rightarrow \mathbb{R}$ I have that for $d^2(x,y)= \sqrt{r^2}$ and $d^{1}(x,y) = |r|$, clearly $$\sqrt{r^2} \le |r|.$$ This is suspiciously too simple. For example, it seems that it is missing the scenario when the L.H.S. is less than the R.H.S., but is that necessary? Any tips are appreciated. Thanks!",,"['general-topology', 'analysis']"
82,$-\Delta u - \alpha u^{1/3} = 0$ implies $u \equiv 0$ if $\alpha$ is small,implies  if  is small,-\Delta u - \alpha u^{1/3} = 0 u \equiv 0 \alpha,"Let $\Omega$ be a domain in $\mathbb{R}^{d}$ with smooth boundary. Let $u(x)$ be a $H^{1}(\Omega)$ solution of the equation $-\Delta u - \alpha u^{1/3} = 0$, $u|_{\partial \Omega} = 0$. The problem I am working on is to show that $u \equiv 0$ if $\alpha > 0$ is small. The hint in this problem is to use Poincare's Inequality. Multiplying the given PDE by $u$, we have $-u\Delta u - \alpha u^{4/3} = 0$. Then $$0 = \int_{\Omega}-u\Delta u - \alpha u^{4/3}\, dx = \int_{\Omega}|\nabla u|^{2} - \alpha u^{4/3}\, dx$$ and hence $\alpha\int_{\Omega}u^{4/3}\, dx = \int_{\Omega}|\nabla u|^{2}\, dx \geq \frac{1}{C}\int_{\Omega}u^{2}\, dx$ where $C$ is the constant from Poincare's Inequality. Rewriting, this implies $$\int_{\Omega}u^{2/3}u^{4/3}\, dx \leq C\alpha\int_{\Omega}u^{4/3}\, dx$$ and then $$\int_{\Omega}u^{4/3}(C\alpha - u^{2/3})\, dx \geq 0.$$ My question is: Are there suggestions on how to finish? Am I approaching this problem correctly?","Let $\Omega$ be a domain in $\mathbb{R}^{d}$ with smooth boundary. Let $u(x)$ be a $H^{1}(\Omega)$ solution of the equation $-\Delta u - \alpha u^{1/3} = 0$, $u|_{\partial \Omega} = 0$. The problem I am working on is to show that $u \equiv 0$ if $\alpha > 0$ is small. The hint in this problem is to use Poincare's Inequality. Multiplying the given PDE by $u$, we have $-u\Delta u - \alpha u^{4/3} = 0$. Then $$0 = \int_{\Omega}-u\Delta u - \alpha u^{4/3}\, dx = \int_{\Omega}|\nabla u|^{2} - \alpha u^{4/3}\, dx$$ and hence $\alpha\int_{\Omega}u^{4/3}\, dx = \int_{\Omega}|\nabla u|^{2}\, dx \geq \frac{1}{C}\int_{\Omega}u^{2}\, dx$ where $C$ is the constant from Poincare's Inequality. Rewriting, this implies $$\int_{\Omega}u^{2/3}u^{4/3}\, dx \leq C\alpha\int_{\Omega}u^{4/3}\, dx$$ and then $$\int_{\Omega}u^{4/3}(C\alpha - u^{2/3})\, dx \geq 0.$$ My question is: Are there suggestions on how to finish? Am I approaching this problem correctly?",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
83,How to prove $( \sum_{n=1}^{\infty} |x_n|^2)^{1/2} \le \sum_{n=1}^{\infty} |x_n|$ (cauchy-product),How to prove  (cauchy-product),( \sum_{n=1}^{\infty} |x_n|^2)^{1/2} \le \sum_{n=1}^{\infty} |x_n|,"I am having this: $ (\ x_n)\ _{n \in \mathbb N} $ is sequence in $\mathbb C$, so the series  $\sum_{n=1}^{\infty} |x_n|$ converges. I've already proved that the series $\sum_{n=1}^{\infty} |x_n|^2$ converges with the cauchy-product: $\sum_{n=1}^{\infty} |x_n| * \sum_{n=1}^{\infty} |x_n| =  \sum_{n=1}^{\infty}\sum_{k=0}^{n}|x_k|*|x_{n-k}|\ge   \sum_{n=1}^{\infty}\sum_{k=0}^{n}|x_k|*|x_{n-k}|=\sum_{n=1}^{\infty} |x_n|^2$ Now I have to prove that: $( \sum_{n=1}^{\infty} |x_n|^2)^{1/2} \le \sum_{n=1}^{\infty} |x_n|$ Questions: Is my Proof correct? How can I prove the other question?","I am having this: $ (\ x_n)\ _{n \in \mathbb N} $ is sequence in $\mathbb C$, so the series  $\sum_{n=1}^{\infty} |x_n|$ converges. I've already proved that the series $\sum_{n=1}^{\infty} |x_n|^2$ converges with the cauchy-product: $\sum_{n=1}^{\infty} |x_n| * \sum_{n=1}^{\infty} |x_n| =  \sum_{n=1}^{\infty}\sum_{k=0}^{n}|x_k|*|x_{n-k}|\ge   \sum_{n=1}^{\infty}\sum_{k=0}^{n}|x_k|*|x_{n-k}|=\sum_{n=1}^{\infty} |x_n|^2$ Now I have to prove that: $( \sum_{n=1}^{\infty} |x_n|^2)^{1/2} \le \sum_{n=1}^{\infty} |x_n|$ Questions: Is my Proof correct? How can I prove the other question?",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
84,Fixed point and fractional iteration: if $F(k)=k$ then $F^{1\over n}(k)$ is another fixed point of $F$,Fixed point and fractional iteration: if  then  is another fixed point of,F(k)=k F^{1\over n}(k) F,"My knowledge of the fixed points and iteration equals zero, same for the notation and terminology but I really need to know if this deduction has trivial errors or is really as nice as it seems. I would like to prove the following: Notation 1 - Given a function $f:A\rightarrow A$ define the set $Fix(f)\subseteq A$ as the set of $f$'s fixed points $Fix(f):=\{\phi:f(\phi)=\phi\}$ Notation 2 - Given a function $f:A\rightarrow A$ and the definition of function composition $\circ$ define the function $f^n$ by recursion $i)$ $f^0:=\operatorname{id}_A$ $i)$ $f^{n+1}:=f\circ f^n$ Definition 1 - Given a function $F:X\rightarrow X$, the "" $1\over n$-iterate"" of $F$ is a function $\Psi:X\rightarrow X$ with this property $\forall x(x\in X) (\Psi^n(x)=F(x))$ I guess that we can write $\Psi=F^{1\over n}$ To Prove - If $k\in X$ is a fixed point of $F$ and exists a fucntion $\Psi$ such that $\Psi^n=F$ then $F^{1\over n}(k)$ is a fixed point of $F$ $$k\in Fix(F)\implies \forall n(n\ge1)(F^{1\over n}(k)\in  Fix(F))$$ Proof 1 - For a fixed $n\gt 1$ define $\lambda:=\Psi(k)=F^{1\over n}(k)$ $\lambda:=\Psi (k)=\Psi(\Psi^n(k))$ because $k=\Psi^n(k)$ $\lambda=\Psi^n(\Psi(k))$ because iterates commute $\lambda=\Psi^n(\lambda)$ because $\Psi(k)=\lambda$ by definition Since $\Psi^n=F$ by definiton we conclude that $\lambda=F(\lambda)$ and thus $$\lambda\in Fix(F)$$ Anyways this proof seems weird to me... I feel like there is something missing: I want to prove that for every natural number (greater than zero), if $F^{1\over n}$ exists,  $F^{1\over n}(k)$ is a fixed point so maybe I need to use induction but I really don't know how I could do it Questions $1)$ - Is this proof correct? If yes and it is a known result, can you    add some info about it? $2)$ - Is it possible to use induction for the proof? Or it is useless? $3)$ - If the proof is correct, is this a result that can be    strengthened? In fact it seems to me that the real generalized result    would be something like $$k\in Fix(F)\implies \forall q(q\in\Bbb  Q\land 0\lt q \lt 1)(F^{q}(k)\in Fix(F))$$","My knowledge of the fixed points and iteration equals zero, same for the notation and terminology but I really need to know if this deduction has trivial errors or is really as nice as it seems. I would like to prove the following: Notation 1 - Given a function $f:A\rightarrow A$ define the set $Fix(f)\subseteq A$ as the set of $f$'s fixed points $Fix(f):=\{\phi:f(\phi)=\phi\}$ Notation 2 - Given a function $f:A\rightarrow A$ and the definition of function composition $\circ$ define the function $f^n$ by recursion $i)$ $f^0:=\operatorname{id}_A$ $i)$ $f^{n+1}:=f\circ f^n$ Definition 1 - Given a function $F:X\rightarrow X$, the "" $1\over n$-iterate"" of $F$ is a function $\Psi:X\rightarrow X$ with this property $\forall x(x\in X) (\Psi^n(x)=F(x))$ I guess that we can write $\Psi=F^{1\over n}$ To Prove - If $k\in X$ is a fixed point of $F$ and exists a fucntion $\Psi$ such that $\Psi^n=F$ then $F^{1\over n}(k)$ is a fixed point of $F$ $$k\in Fix(F)\implies \forall n(n\ge1)(F^{1\over n}(k)\in  Fix(F))$$ Proof 1 - For a fixed $n\gt 1$ define $\lambda:=\Psi(k)=F^{1\over n}(k)$ $\lambda:=\Psi (k)=\Psi(\Psi^n(k))$ because $k=\Psi^n(k)$ $\lambda=\Psi^n(\Psi(k))$ because iterates commute $\lambda=\Psi^n(\lambda)$ because $\Psi(k)=\lambda$ by definition Since $\Psi^n=F$ by definiton we conclude that $\lambda=F(\lambda)$ and thus $$\lambda\in Fix(F)$$ Anyways this proof seems weird to me... I feel like there is something missing: I want to prove that for every natural number (greater than zero), if $F^{1\over n}$ exists,  $F^{1\over n}(k)$ is a fixed point so maybe I need to use induction but I really don't know how I could do it Questions $1)$ - Is this proof correct? If yes and it is a known result, can you    add some info about it? $2)$ - Is it possible to use induction for the proof? Or it is useless? $3)$ - If the proof is correct, is this a result that can be    strengthened? In fact it seems to me that the real generalized result    would be something like $$k\in Fix(F)\implies \forall q(q\in\Bbb  Q\land 0\lt q \lt 1)(F^{q}(k)\in Fix(F))$$",,"['analysis', 'functions', 'proof-verification', 'fixed-point-theorems']"
85,Continuous extension on compact set in $\mathbb{R}^n$,Continuous extension on compact set in,\mathbb{R}^n,"I'm an undergrad student reading through Deimling's Nonlinear Functional Analysis and have come across the following proposition. Let $A\subset\mathbb{R}^n$ be compact and $f:A\to\mathbb{R}^n$ be a continuous function. Then there exists a continuous extension $\tilde{f}:\mathbb{R}^n\to\mathbb{R}^n$ such that $\tilde{f}(x)=f(x)$ for all $x\in A$. The proof goes as follows: Let $S=\{a_1,a_2,\ldots\}$ be a countable dense subset of $A$ and define $$\varphi_i(x) := \max\left\{ 2-\frac{|x-a_i|}{\rho(x,A)},0 \right\}\quad\text{for $x\not\in A$}$$ where $\rho$ denotes distance. Then the function $$ \tilde{f}(x) =\begin{cases} f(x),&x\in A\\ \left(\sum_{i\geq1}2^{-i}\varphi_i(x)\right)^{-1}\left(\sum_{i\geq1}2^{-i}\varphi_i(x)f(a_i)\right),&x\not\in A \end{cases} $$ satisfies the extension. QED. My understanding: Clearly, $\tilde{f}$ is continuous on the interior of $A$ and using the Weierstrass M-test I can show that $\tilde{f}$ is continuous on $\mathbb{R}^n\backslash A$. All that remains is to show that $\tilde{f}$ is continuous on the boundary of $A$ and this is where I am having no luck. Apologies in advance if the following is not clear, but I'm not sure how to proceed. As I play around with the expression for $\tilde{f}$ at some $x\in\mathbb{R}^n\backslash A$, it almost seems like we have a weighted average of values of $f$ on $A$ near $x$. However, my intuition begins to break down when looking at any finite sum because it seems that the order of enumeration of each $a_i$ may have more of an effect in the sum than the closeness of $a_i$ to $x$, i.e. the closer values may appear too late in the sum and the factor $2^{-i}$ may make these values insignificant. Maybe the division by the first factor accommodates for this, but I cannot be sure exactly what's going on here. Can anyone clarify my intuition here or, better yet, give me some hints on how to show continuity on the boundary of $A$? Thanks","I'm an undergrad student reading through Deimling's Nonlinear Functional Analysis and have come across the following proposition. Let $A\subset\mathbb{R}^n$ be compact and $f:A\to\mathbb{R}^n$ be a continuous function. Then there exists a continuous extension $\tilde{f}:\mathbb{R}^n\to\mathbb{R}^n$ such that $\tilde{f}(x)=f(x)$ for all $x\in A$. The proof goes as follows: Let $S=\{a_1,a_2,\ldots\}$ be a countable dense subset of $A$ and define $$\varphi_i(x) := \max\left\{ 2-\frac{|x-a_i|}{\rho(x,A)},0 \right\}\quad\text{for $x\not\in A$}$$ where $\rho$ denotes distance. Then the function $$ \tilde{f}(x) =\begin{cases} f(x),&x\in A\\ \left(\sum_{i\geq1}2^{-i}\varphi_i(x)\right)^{-1}\left(\sum_{i\geq1}2^{-i}\varphi_i(x)f(a_i)\right),&x\not\in A \end{cases} $$ satisfies the extension. QED. My understanding: Clearly, $\tilde{f}$ is continuous on the interior of $A$ and using the Weierstrass M-test I can show that $\tilde{f}$ is continuous on $\mathbb{R}^n\backslash A$. All that remains is to show that $\tilde{f}$ is continuous on the boundary of $A$ and this is where I am having no luck. Apologies in advance if the following is not clear, but I'm not sure how to proceed. As I play around with the expression for $\tilde{f}$ at some $x\in\mathbb{R}^n\backslash A$, it almost seems like we have a weighted average of values of $f$ on $A$ near $x$. However, my intuition begins to break down when looking at any finite sum because it seems that the order of enumeration of each $a_i$ may have more of an effect in the sum than the closeness of $a_i$ to $x$, i.e. the closer values may appear too late in the sum and the factor $2^{-i}$ may make these values insignificant. Maybe the division by the first factor accommodates for this, but I cannot be sure exactly what's going on here. Can anyone clarify my intuition here or, better yet, give me some hints on how to show continuity on the boundary of $A$? Thanks",,"['analysis', 'continuity', 'compactness']"
86,How to prove that a bijective transformation is NOT continuous,How to prove that a bijective transformation is NOT continuous,,I am having this transformation $f: \mathbb R \to \mathbb R$ $$f(x) = \begin{cases} x  & x \in \mathbb R \setminus  \mathbb Q \\x+1 & x \in \mathbb Q  \end{cases}$$ I've already prooved that this transformation is bijective. How can I proove that the transformation is NOT continous in every point $x \in \mathbb R $,I am having this transformation $f: \mathbb R \to \mathbb R$ $$f(x) = \begin{cases} x  & x \in \mathbb R \setminus  \mathbb Q \\x+1 & x \in \mathbb Q  \end{cases}$$ I've already prooved that this transformation is bijective. How can I proove that the transformation is NOT continous in every point $x \in \mathbb R $,,"['real-analysis', 'analysis', 'continuity', 'transformation']"
87,Refinement of Lebesgue Decomposition Theorem,Refinement of Lebesgue Decomposition Theorem,,"On Wikipedia, a ""refinement"" of the Lebesgue decomposition theorem is given, and it is also given as problems in Stein and Shakarchi and Bruckner and Thomson. Can someone provide a comprehensive proof of it (I've written it below), as I'm having trouble. Suppose $F$ is an increasing function on [a,b]. Prove we can write $F = A + B + C$, where $A, B, C$, are increasing functions and: A is absolutely continuous; $B$ is continuous, but $B'(x)=0$ for almost everywhere x; and C is a jump function. Moreover, prove $A, B$, and $C$ are uniquely determined up to an additive constant.","On Wikipedia, a ""refinement"" of the Lebesgue decomposition theorem is given, and it is also given as problems in Stein and Shakarchi and Bruckner and Thomson. Can someone provide a comprehensive proof of it (I've written it below), as I'm having trouble. Suppose $F$ is an increasing function on [a,b]. Prove we can write $F = A + B + C$, where $A, B, C$, are increasing functions and: A is absolutely continuous; $B$ is continuous, but $B'(x)=0$ for almost everywhere x; and C is a jump function. Moreover, prove $A, B$, and $C$ are uniquely determined up to an additive constant.",,"['analysis', 'measure-theory', 'lebesgue-measure']"
88,"How can I prove that $\sup(\bigcup_{i \in I} A_{i}) =\sup\{\sup \,A_{i} : i \in I \}$? [closed]",How can I prove that ? [closed],"\sup(\bigcup_{i \in I} A_{i}) =\sup\{\sup \,A_{i} : i \in I \}","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $I$ be non-empty set (of ""indexes"") and for all $i \in I$ let $A_{i} \subset \mathbb{R}$ be non-empty and upper bounded set. How can I prove that  $$ \sup \left( \bigcup_{i \in I} A_{i} \right) = \sup\{\sup A_{i} \mid i \in I \} $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $I$ be non-empty set (of ""indexes"") and for all $i \in I$ let $A_{i} \subset \mathbb{R}$ be non-empty and upper bounded set. How can I prove that  $$ \sup \left( \bigcup_{i \in I} A_{i} \right) = \sup\{\sup A_{i} \mid i \in I \} $$",,"['real-analysis', 'analysis', 'supremum-and-infimum']"
89,Lp spaces are nested but then why is 1/x square summable but not summable?,Lp spaces are nested but then why is 1/x square summable but not summable?,,"If $1\leq s<r<\infty$ and $f\in L^r$ then $f\in L^s$, so then why is $\frac{1}{x}$ not in $L^1$ but is in $L^2$ for the counting measure $c:\mathbb{N}\rightarrow \mathbb{R}$?","If $1\leq s<r<\infty$ and $f\in L^r$ then $f\in L^s$, so then why is $\frac{1}{x}$ not in $L^1$ but is in $L^2$ for the counting measure $c:\mathbb{N}\rightarrow \mathbb{R}$?",,"['real-analysis', 'integration', 'analysis']"
90,How prove this Ratio Test and Its Generalizations problem?,How prove this Ratio Test and Its Generalizations problem?,,"Question: let $\alpha\in (0,1)$,and the postive sequence $\{a_{n}\}$ such   $$\lim_{n\to\infty}\inf \left(n^{\alpha}\left(\dfrac{a_{n}}{a_{n+1}}-1\right)\right) =\lambda\in (0,+\infty)$$ show that   $$\lim_{n\to\infty}n^k a_{n}=0,k>0$$ maybe  this is Ratio Test and Its Generalizations? This problem is  The Chinese Mathematics Competitions 2014  before,But I can't prove it.can you help","Question: let $\alpha\in (0,1)$,and the postive sequence $\{a_{n}\}$ such   $$\lim_{n\to\infty}\inf \left(n^{\alpha}\left(\dfrac{a_{n}}{a_{n+1}}-1\right)\right) =\lambda\in (0,+\infty)$$ show that   $$\lim_{n\to\infty}n^k a_{n}=0,k>0$$ maybe  this is Ratio Test and Its Generalizations? This problem is  The Chinese Mathematics Competitions 2014  before,But I can't prove it.can you help",,['analysis']
91,Division algorithm for the natural numbers.,Division algorithm for the natural numbers.,,"I am trying to prove the following statement from Terence Tao's Analysis 1 book. Definition of multiplication $ab{+\!+} = ab+b$ . Definition of addition $(a{+\!+})+b=(a+b){+\!+}$ . Let $n$ be a natural number, and let $q$ be a positive number. Then there exists natural numbers $m$ , $r$ such that $0 \le r \lt q$ and $n=mq+r$ . Now I attempted to prove the statement by inducting on $n$ . So for the base case $n=0$ $$0=mq+r$$ Since $q$ is positive $m$ and $r$ must be $$m=0$$ $$r=0$$ and we have $$0\le r \lt q$$ as desired. Now assume true for $P(n)$ i.e. $$n=mq+r$$ and $$0\le r \lt q.$$ Now for $n{+\!+}$ (the successor to n) case. Need to show $$n{+\!+}=mq+r.$$ Then using the induction hypothesis we have on the left hand side $$n{+\!+}=(mq+r){+\!+}=(mq{+\!+})+r=mq+q+r.$$ Then we use cancellation to arrive at $$mq+q+r=mq+r$$ $$q=0.$$ Which is not possible cause we assumed q is positive.  So clearly I have done something wrong because the division algorithm is true.","I am trying to prove the following statement from Terence Tao's Analysis 1 book. Definition of multiplication . Definition of addition . Let be a natural number, and let be a positive number. Then there exists natural numbers , such that and . Now I attempted to prove the statement by inducting on . So for the base case Since is positive and must be and we have as desired. Now assume true for i.e. and Now for (the successor to n) case. Need to show Then using the induction hypothesis we have on the left hand side Then we use cancellation to arrive at Which is not possible cause we assumed q is positive.  So clearly I have done something wrong because the division algorithm is true.",ab{+\!+} = ab+b (a{+\!+})+b=(a+b){+\!+} n q m r 0 \le r \lt q n=mq+r n n=0 0=mq+r q m r m=0 r=0 0\le r \lt q P(n) n=mq+r 0\le r \lt q. n{+\!+} n{+\!+}=mq+r. n{+\!+}=(mq+r){+\!+}=(mq{+\!+})+r=mq+q+r. mq+q+r=mq+r q=0.,"['analysis', 'number-theory', 'elementary-number-theory']"
92,Why are open sets used in definitions in differential geometry?,Why are open sets used in definitions in differential geometry?,,"I find that in most definitions in differential geometry, such as those of defining a manifold, a smooth manifold,differentiable functions, diffeomorphisms on manifolds, an atlas,etc , open sets are used. From what I can guess it seems like these usage of open sets is being followed in line with notions in topology, ie a topological space and open sets covering it. Can anyone elaborate to a beginner in differential geometry why open sets are used?","I find that in most definitions in differential geometry, such as those of defining a manifold, a smooth manifold,differentiable functions, diffeomorphisms on manifolds, an atlas,etc , open sets are used. From what I can guess it seems like these usage of open sets is being followed in line with notions in topology, ie a topological space and open sets covering it. Can anyone elaborate to a beginner in differential geometry why open sets are used?",,"['general-topology', 'analysis', 'differential-geometry', 'differential-topology']"
93,Does the set of Differentiable functions change if we change our norm?,Does the set of Differentiable functions change if we change our norm?,,"This may be a naive question. I am reading the definition of differetiablity of a function $f:\mathbb{R^n}\rightarrow \mathbb{R^m}$ in the book Calculus Manifolds. I already know that all norms on $\mathbb{R}^n$ induce the same metric topology. If we change the norms in the definition (for example we can use the manhattan norm), does the set of differentiable functions change ? I already know that all norms on $\mathbb{R}^n$ induce the same metric topology but that doesn't seem to imply a negative answer to my question. Another Question: If the set of differentiable functions changes, is there any reason why we are defining differentiablity using the Pythagorean norm ? Thank you","This may be a naive question. I am reading the definition of differetiablity of a function $f:\mathbb{R^n}\rightarrow \mathbb{R^m}$ in the book Calculus Manifolds. I already know that all norms on $\mathbb{R}^n$ induce the same metric topology. If we change the norms in the definition (for example we can use the manhattan norm), does the set of differentiable functions change ? I already know that all norms on $\mathbb{R}^n$ induce the same metric topology but that doesn't seem to imply a negative answer to my question. Another Question: If the set of differentiable functions changes, is there any reason why we are defining differentiablity using the Pythagorean norm ? Thank you",,"['analysis', 'multivariable-calculus']"
94,About Cantor Set And Measure,About Cantor Set And Measure,,"Prove that the Cantor ternary set has Jordan content 0. Additionally, prove that the Cantor ternary set has uncountably many points.","Prove that the Cantor ternary set has Jordan content 0. Additionally, prove that the Cantor ternary set has uncountably many points.",,['analysis']
95,Proof that infinitely many $f$ exist if $f(f(x))=f(x)^{2013}$,Proof that infinitely many  exist if,f f(f(x))=f(x)^{2013},"Suppose $f(x)$ is function from $\mathbb{R}$ to $\mathbb{R}$ such that $f(f(x))=f(x)^{2013}$. Show that there are infinitely many such functions, of which exactly four are polynomials. If $f$ is constant ($f(x) = c$), the value of $c$ satisfying $f(f(x)) = f(x)^{2013}$ would be $-1$, $0$ and $1$. For non-constant $f$, the polynomial $g(x)=f(x)-x^{2013}$ would have all elements of the range of $f$ ($\{f(x)\mid x \in \mathbb{R}\}$) as its roots. Since $f(x)$ is everywhere continuous, $g$ has infinitely many roots, making it the zero polynomial. Thus $f(x)=x^{2013}$ for all real values of $x$.  This proves the existence of the four polynomials. Now, how would I prove the existence of infinitely many $f$?","Suppose $f(x)$ is function from $\mathbb{R}$ to $\mathbb{R}$ such that $f(f(x))=f(x)^{2013}$. Show that there are infinitely many such functions, of which exactly four are polynomials. If $f$ is constant ($f(x) = c$), the value of $c$ satisfying $f(f(x)) = f(x)^{2013}$ would be $-1$, $0$ and $1$. For non-constant $f$, the polynomial $g(x)=f(x)-x^{2013}$ would have all elements of the range of $f$ ($\{f(x)\mid x \in \mathbb{R}\}$) as its roots. Since $f(x)$ is everywhere continuous, $g$ has infinitely many roots, making it the zero polynomial. Thus $f(x)=x^{2013}$ for all real values of $x$.  This proves the existence of the four polynomials. Now, how would I prove the existence of infinitely many $f$?",,"['analysis', 'contest-math']"
96,Problem about $G_{\delta}$-set and $F_{\sigma}$-set,Problem about -set and -set,G_{\delta} F_{\sigma},"Prove if $E$ is any measurable subset of $\mathbb{R}$, then there are a $G_{\delta}$-set $G$ and a $F_{\sigma}$-set $H$ such that $H \subseteq E \subseteq G$, and such that $m(G$\ $H)=0$. In order to prove this, for each $n \in \mathbb{Z}$ let $E_n=E \cap (n,n)$. How to show that the regularity theorem for $E_n$ can be solved entirely within the interval (n,n+1). And then re-include $\mathbb{Z}$ get to the end of the proof.","Prove if $E$ is any measurable subset of $\mathbb{R}$, then there are a $G_{\delta}$-set $G$ and a $F_{\sigma}$-set $H$ such that $H \subseteq E \subseteq G$, and such that $m(G$\ $H)=0$. In order to prove this, for each $n \in \mathbb{Z}$ let $E_n=E \cap (n,n)$. How to show that the regularity theorem for $E_n$ can be solved entirely within the interval (n,n+1). And then re-include $\mathbb{Z}$ get to the end of the proof.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
97,Integral inequality problem,Integral inequality problem,,"Let $f:[0,1]\to\mathbb R$ be a differentiable function with $f(0)=0$ and $f'(x)\in(0,1)$ for every $x\in(0,1).$  Show that $$\left(\int_0^1f(x)dx\right)^2>\int_0^1(f(x))^3dx$$ I am not even sure how to begin to solve this problem.  Any help/hints is appreciated.  Thanks.","Let $f:[0,1]\to\mathbb R$ be a differentiable function with $f(0)=0$ and $f'(x)\in(0,1)$ for every $x\in(0,1).$  Show that $$\left(\int_0^1f(x)dx\right)^2>\int_0^1(f(x))^3dx$$ I am not even sure how to begin to solve this problem.  Any help/hints is appreciated.  Thanks.",,"['integration', 'analysis']"
98,Measurable vector bundles trivial,Measurable vector bundles trivial,,"I hope you can help: If $E$ is a measurable vector bundle over a compact metric space $(X,\mu)$ then there is a subset $Y\subset X$ such that $\mu(Y)=1$ and $\pi ^{-1}(Y)$ is isomorphic to a trivial vector bundle. There is a suggestion that I do not quite understand: Since $\bigcup_{r>0}\partial B(x,r)$ is an uncountable disjoint union, there $r_x>0$ such that: 1- $\mu(\partial B(x,r_x))=0$. 2- $\pi| B(x,r_x)$ is measurably isomorphic to $B(x,r_x)\times \mathbb{R}^{n}$. Thanks for your attention.","I hope you can help: If $E$ is a measurable vector bundle over a compact metric space $(X,\mu)$ then there is a subset $Y\subset X$ such that $\mu(Y)=1$ and $\pi ^{-1}(Y)$ is isomorphic to a trivial vector bundle. There is a suggestion that I do not quite understand: Since $\bigcup_{r>0}\partial B(x,r)$ is an uncountable disjoint union, there $r_x>0$ such that: 1- $\mu(\partial B(x,r_x))=0$. 2- $\pi| B(x,r_x)$ is measurably isomorphic to $B(x,r_x)\times \mathbb{R}^{n}$. Thanks for your attention.",,"['analysis', 'measure-theory', 'vector-bundles', 'fiber-bundles']"
99,function has partial derivatives but is not differentiable,function has partial derivatives but is not differentiable,,Can you write me an example of function which has partial derivatives but is not differentiable? How could I create and prove the function like that?,Can you write me an example of function which has partial derivatives but is not differentiable? How could I create and prove the function like that?,,"['real-analysis', 'analysis', 'derivatives']"
