,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Counting Review; Discrete Structures,Counting Review; Discrete Structures,,"I have forgotten a lot of the counting portion of my discrete structures course and need some explanations how to count, maybe some general strategies on counting. Some example questions I need explaining are Consider strings consisting of n characters, each character being a, b, or c. Let Sn be the number of such strings of length n that do not contain the substring aa. Which of the following is true? (a) $S_{n+1} = S_n + S_{n−1}$ for $n ≥ 2.$ (b) $S_{n+1} = 2 · S_n + S_{n−1}$ for $n ≥ 2.$ (c) $S_{n+1} = S_n + 2 · S_{n−1}$ for $n ≥ 2.$ (d) $S_{n+1} = 2 · Sn + 2 · S_{n−1}$ for $n ≥ 2.$ Have a feeling it is C Consider 4 blue balls B1, B2, B3, B4 and 5 red balls R1, R2, R3, R4, R5. We pick 3 balls of the same color and arrange them on a horizontal line. (The order on the line matters.) How many arrangements are there? (a) 64 (b) 74 (c) 84 (d) 94 if anyone can help explain these questions to me that would be appreciated!","I have forgotten a lot of the counting portion of my discrete structures course and need some explanations how to count, maybe some general strategies on counting. Some example questions I need explaining are Consider strings consisting of n characters, each character being a, b, or c. Let Sn be the number of such strings of length n that do not contain the substring aa. Which of the following is true? (a) $S_{n+1} = S_n + S_{n−1}$ for $n ≥ 2.$ (b) $S_{n+1} = 2 · S_n + S_{n−1}$ for $n ≥ 2.$ (c) $S_{n+1} = S_n + 2 · S_{n−1}$ for $n ≥ 2.$ (d) $S_{n+1} = 2 · Sn + 2 · S_{n−1}$ for $n ≥ 2.$ Have a feeling it is C Consider 4 blue balls B1, B2, B3, B4 and 5 red balls R1, R2, R3, R4, R5. We pick 3 balls of the same color and arrange them on a horizontal line. (The order on the line matters.) How many arrangements are there? (a) 64 (b) 74 (c) 84 (d) 94 if anyone can help explain these questions to me that would be appreciated!",,"['combinatorics', 'discrete-mathematics']"
1,Convert form English to logical symbols.,Convert form English to logical symbols.,,"I have a logical argument in English which says. All Humans are Mortal. Zeus is not Mortal. therefore Zeus is not Human. And I tried to convert it from English to logic. and did this h = is Human, z = is Zeus, m = is Mortal h $ \rightarrow $ m z $ \rightarrow $ ~m $ \therefore $     z $ \rightarrow $ ~h and did it wrong answer as my truth table shows this argument is valid but the original English argument according to Venn Diagram is invalid that meant my conversion was wrong. O.K. then I tried with this h ^ m z ^ ~m $ \therefore $     z ^ ~h And this time I can't get both premise true in a single row in my truth table, I mean both premise are never true at a same time. What is the actual way to solve this problem with logical symbols. (by the way this is not my homework assignment it's just an exercise in Venn Diagram Section of my course and I thought to do it with logical symbols too but could not)","I have a logical argument in English which says. All Humans are Mortal. Zeus is not Mortal. therefore Zeus is not Human. And I tried to convert it from English to logic. and did this h = is Human, z = is Zeus, m = is Mortal h $ \rightarrow $ m z $ \rightarrow $ ~m $ \therefore $     z $ \rightarrow $ ~h and did it wrong answer as my truth table shows this argument is valid but the original English argument according to Venn Diagram is invalid that meant my conversion was wrong. O.K. then I tried with this h ^ m z ^ ~m $ \therefore $     z ^ ~h And this time I can't get both premise true in a single row in my truth table, I mean both premise are never true at a same time. What is the actual way to solve this problem with logical symbols. (by the way this is not my homework assignment it's just an exercise in Venn Diagram Section of my course and I thought to do it with logical symbols too but could not)",,['discrete-mathematics']
2,Does $\theta(n)$ = $1/x$ make any sense?,Does  =  make any sense?,\theta(n) 1/x,"So, I asked this question on a discrete structures exam today, which I apparently didn't give enough thought to: $f(x) = (5x^2 + 6x + 2)/(x^3 + 4x^2 +x)$ Find the correct theta notation for the function. So, I believe it is easy to create an upper bound around $1/x$ since we can show $f(x) \leq 2/x$ for sufficiently large $x$.  I don't know that a lower bound can exist since $C$ must be positive and would need to be zero for a proper $\Omega(n)$ lower bound. First, am I correct about the non-existence of a tight bound or am I missing something here? Second, is this the only reason this is a Mickey Mouse type question or what else am I missing here?  It seems like it could be possible for a particular algorithm to have a $1/x$ complexity.","So, I asked this question on a discrete structures exam today, which I apparently didn't give enough thought to: $f(x) = (5x^2 + 6x + 2)/(x^3 + 4x^2 +x)$ Find the correct theta notation for the function. So, I believe it is easy to create an upper bound around $1/x$ since we can show $f(x) \leq 2/x$ for sufficiently large $x$.  I don't know that a lower bound can exist since $C$ must be positive and would need to be zero for a proper $\Omega(n)$ lower bound. First, am I correct about the non-existence of a tight bound or am I missing something here? Second, is this the only reason this is a Mickey Mouse type question or what else am I missing here?  It seems like it could be possible for a particular algorithm to have a $1/x$ complexity.",,"['discrete-mathematics', 'asymptotics']"
3,"Arguing the correctness of an alternative, way to count how many bit sequences with exactly n zeroes and k+1 ones are there","Arguing the correctness of an alternative, way to count how many bit sequences with exactly n zeroes and k+1 ones are there",,"I was trying to count how many bit sequences with exactly n zeroes and k+1 ones are there. One obvious reasoning is just by doing $ \binom {k+n+1}{k+1}$, by doing choose. However, I was told that you could also do it by doing the following summation: $$\sum^{n}_{i=0}\binom {k+i}{k}$$ If that is true then: $$\sum^{n}_{i=0}\binom {k+i}{k} = \binom {k+n+1}{k+1}$$ Which after a lot of algebra that I will omit, can be verified by induction! Incredible. However, I am unsure what is the combinatorial reasoning is. Does someone know how to reason it combinatorially to establish the equality? Can you also justify the correctness of your argument? In fact, I was told that the following is the ""correct"" reasoning, though I can't make sense of why its correct: On the other hand, the number of zeroes i to the left of the rightmost   one ranges from 0 to n. For a fixed value of i, there are $\binom {k+i}{k}$ possible   choices for the sequence of bits before the rightmost   one. If we sum over all possible i, we find that the number we want is $\sum^{n}_{i=0} \binom {k+i}{k}$ My main concern is with the summation. I can't understand the interpretation of the summation. Is it summing over disjoint subsets? Or why is it summing things? The only time I have seen sums in counting is when there are disjoint subsets (or with the inclusion-exclusion principle). Even if you explain me what the interpretation means, I feel it might not be too helpful unless it has an explanation of its correctness . Please provide as much detail on the combinatoric interpretation of the summation, the part of the question that I am having trouble understanding. Thats I guess what is giving me trouble. I fail to see why that description gives the desired equality. For example, one aspect that I would liked addressed is, how come is that sum NOT double counting? As i increases, combinations from previous steps are ""reconsidered""...or not? It seems to me they are. Then if they are, why is that summation NOT double counting? BOUNTY I am not able to put a bounty yet, but the answer that justifies the correctness well enough and convinces me to accept their answer, I will gladly reward you when the times comes. Take it that if there is no accepted answer, I have not yet had my confusion/doubt clarified.","I was trying to count how many bit sequences with exactly n zeroes and k+1 ones are there. One obvious reasoning is just by doing $ \binom {k+n+1}{k+1}$, by doing choose. However, I was told that you could also do it by doing the following summation: $$\sum^{n}_{i=0}\binom {k+i}{k}$$ If that is true then: $$\sum^{n}_{i=0}\binom {k+i}{k} = \binom {k+n+1}{k+1}$$ Which after a lot of algebra that I will omit, can be verified by induction! Incredible. However, I am unsure what is the combinatorial reasoning is. Does someone know how to reason it combinatorially to establish the equality? Can you also justify the correctness of your argument? In fact, I was told that the following is the ""correct"" reasoning, though I can't make sense of why its correct: On the other hand, the number of zeroes i to the left of the rightmost   one ranges from 0 to n. For a fixed value of i, there are $\binom {k+i}{k}$ possible   choices for the sequence of bits before the rightmost   one. If we sum over all possible i, we find that the number we want is $\sum^{n}_{i=0} \binom {k+i}{k}$ My main concern is with the summation. I can't understand the interpretation of the summation. Is it summing over disjoint subsets? Or why is it summing things? The only time I have seen sums in counting is when there are disjoint subsets (or with the inclusion-exclusion principle). Even if you explain me what the interpretation means, I feel it might not be too helpful unless it has an explanation of its correctness . Please provide as much detail on the combinatoric interpretation of the summation, the part of the question that I am having trouble understanding. Thats I guess what is giving me trouble. I fail to see why that description gives the desired equality. For example, one aspect that I would liked addressed is, how come is that sum NOT double counting? As i increases, combinations from previous steps are ""reconsidered""...or not? It seems to me they are. Then if they are, why is that summation NOT double counting? BOUNTY I am not able to put a bounty yet, but the answer that justifies the correctness well enough and convinces me to accept their answer, I will gladly reward you when the times comes. Take it that if there is no accepted answer, I have not yet had my confusion/doubt clarified.",,"['combinatorics', 'discrete-mathematics']"
4,Solving equations with mod,Solving equations with mod,,"So, I'm trying to solve the following equation using regular algebra, and I don't think I'm doing it right: $3x+5 = 1\pmod {11}$ I know the result is $x = 6$ , but when I do regular algebra like the following, I do not get 6: $3x=1  - 5\pmod{11}$ $x = \dfrac{(1 \pmod{11} - 5)} 3$ So, I figured that since $1 \pmod{11} = 1$ the equation becomes $x = \dfrac{-4} 3$ Which is not 6! I am totally lost here and would appreciate any help......","So, I'm trying to solve the following equation using regular algebra, and I don't think I'm doing it right: I know the result is , but when I do regular algebra like the following, I do not get 6: So, I figured that since the equation becomes Which is not 6! I am totally lost here and would appreciate any help......",3x+5 = 1\pmod {11} x = 6 3x=1  - 5\pmod{11} x = \dfrac{(1 \pmod{11} - 5)} 3 1 \pmod{11} = 1 x = \dfrac{-4} 3,"['discrete-mathematics', 'modular-arithmetic']"
5,Anti-symmetric relation given by a matrix,Anti-symmetric relation given by a matrix,,"Relation R is given by a matrix $$\begin{bmatrix} 1& 0& 0& 0\\   1& 1& 0& 0  \\ 1& 0& 1& 0  \\ 1& 1& 1& 1 \end{bmatrix} $$ Is it anti-symmetric? I'm not quite understanding this. My notes state that for any $a,b\in A$ in the binary relation $R$ is a subset of $A\times A$, it is anti-symmetric if $(a,b)\in R$ and $(b,a)\in R$ then $a = b$. If we assume that the elements of the set are $0,1,2,3$ Wouldn't $(2,3)\in R$ and $(3,2) \in R$ but $3\neq2$. So would it not be anti-symmetric or am I completely missing the ball on this concept?","Relation R is given by a matrix $$\begin{bmatrix} 1& 0& 0& 0\\   1& 1& 0& 0  \\ 1& 0& 1& 0  \\ 1& 1& 1& 1 \end{bmatrix} $$ Is it anti-symmetric? I'm not quite understanding this. My notes state that for any $a,b\in A$ in the binary relation $R$ is a subset of $A\times A$, it is anti-symmetric if $(a,b)\in R$ and $(b,a)\in R$ then $a = b$. If we assume that the elements of the set are $0,1,2,3$ Wouldn't $(2,3)\in R$ and $(3,2) \in R$ but $3\neq2$. So would it not be anti-symmetric or am I completely missing the ball on this concept?",,"['discrete-mathematics', 'relations']"
6,Find maximum number of nodes in a regular graph of degree 4 and diameter 2,Find maximum number of nodes in a regular graph of degree 4 and diameter 2,,"In $n$ nodes directed graph, every vertex has in-degree and out-degree equal to $4$. If every vertex is reachable from every other vertex directed by a path of length at most $2$. How can we find maximum value of $n$?","In $n$ nodes directed graph, every vertex has in-degree and out-degree equal to $4$. If every vertex is reachable from every other vertex directed by a path of length at most $2$. How can we find maximum value of $n$?",,"['discrete-mathematics', 'graph-theory']"
7,Combining Two Gaussian Filters,Combining Two Gaussian Filters,,"I am taking a class related to image processing and we were taught about Gaussian Filters that are related to the following Gaussian Function: $$G(u,v) = \frac{1}{2\pi\sigma^2}e^{-\frac{u^2 + v^2}{2\sigma^2}}$$ We were told in class that if apply a convolution with standard deviation $\sigma$ twice with a Gaussian filter it is equivalent to applying a Gaussian filter with standard deviation of $\sqrt{2}\sigma$. I want to actually be able to prove this but I cannot for the life of me do this, I have found resources related to Fourier analysis but I am not familiar with Fourier analysis and we are not expected to know Fourier analysis. I tried (probably naively) just multiplying two Gaussian functions with each other and then setting $\sigma' = \sqrt{2}\sigma$ to see if I can get an equivalence and I cannot get it to work. Any help would be greatly appreciated!","I am taking a class related to image processing and we were taught about Gaussian Filters that are related to the following Gaussian Function: $$G(u,v) = \frac{1}{2\pi\sigma^2}e^{-\frac{u^2 + v^2}{2\sigma^2}}$$ We were told in class that if apply a convolution with standard deviation $\sigma$ twice with a Gaussian filter it is equivalent to applying a Gaussian filter with standard deviation of $\sqrt{2}\sigma$. I want to actually be able to prove this but I cannot for the life of me do this, I have found resources related to Fourier analysis but I am not familiar with Fourier analysis and we are not expected to know Fourier analysis. I tried (probably naively) just multiplying two Gaussian functions with each other and then setting $\sigma' = \sqrt{2}\sigma$ to see if I can get an equivalence and I cannot get it to work. Any help would be greatly appreciated!",,"['probability', 'discrete-mathematics', 'fourier-analysis', 'normal-distribution']"
8,Could someone explain DeMorgan Laws?,Could someone explain DeMorgan Laws?,,"I'm having a bit of trouble visualizing these laws we learned in class today.  He mentioned DeMorgan's Law when dealing with Quantifiers, and wrote this on the board: $$\neg \forall x P(x) \iff \exists x \neg P(x)$$ $$\neg \exists x P(x) \iff \forall x \neg P(x)$$ I can't really visualize the two in my head (at least with real world examples), would someone be able to spell it out for me? (I'm also not familiar with MathJax or any of that so I pretty much copied/pasted it from my online notes, I hope it goes through ok)","I'm having a bit of trouble visualizing these laws we learned in class today.  He mentioned DeMorgan's Law when dealing with Quantifiers, and wrote this on the board: $$\neg \forall x P(x) \iff \exists x \neg P(x)$$ $$\neg \exists x P(x) \iff \forall x \neg P(x)$$ I can't really visualize the two in my head (at least with real world examples), would someone be able to spell it out for me? (I'm also not familiar with MathJax or any of that so I pretty much copied/pasted it from my online notes, I hope it goes through ok)",,"['discrete-mathematics', 'logic']"
9,Simplify a boolean algebra expression: xy + xz' + x'yz,Simplify a boolean algebra expression: xy + xz' + x'yz,,"I need to simplify xy + xz' + x'yz into xz' + yz . I know that these expressions are equal in truth value, but I'm not sure how to simplify the first to get the second. Here are the steps I can do: 1) xy + xz' + x'yz 2) y(x + x'z) + xz' 3) y((x + x')(x + z)) + xz' 4) y(x + z) + xz' But that is where I get stuck. Any help you can give me would be great. Thanks.","I need to simplify xy + xz' + x'yz into xz' + yz . I know that these expressions are equal in truth value, but I'm not sure how to simplify the first to get the second. Here are the steps I can do: 1) xy + xz' + x'yz 2) y(x + x'z) + xz' 3) y((x + x')(x + z)) + xz' 4) y(x + z) + xz' But that is where I get stuck. Any help you can give me would be great. Thanks.",,"['discrete-mathematics', 'boolean-algebra']"
10,How to prove a duality about partitions of numbers?,How to prove a duality about partitions of numbers?,,"I found the following theorem, which I think should be correct but I do not know how to prove it: Consider the set containing sums $A=\lbrace\sum\limits_{i=-a}^a iX_i\rbrace$ where $X_i$ is a non-negative integer and the positive integer $a$ increases by 2 each time(so there are $a+1$ terms in the sum), up to the constraint that $\sum\limits_{i=-a}^a X_i=b$, where $b$ is another positive integer. Now consider another set $B=\lbrace\sum\limits_{i=-b}^b iY_i\rbrace$ up to the constraint $\sum\limits_{i=-b}^b Y_i=a$, where the same $a, b$ are used and $Y_i$ is also a non-negative integer as $X_i$. The theorem is $A=B$ for all $a,b$. My strategy is to prove it by induction. It is obvious that they have the same finite cardinality. I need to construct a map which is bijective between $A,B$, but I do not how. Can someone help?","I found the following theorem, which I think should be correct but I do not know how to prove it: Consider the set containing sums $A=\lbrace\sum\limits_{i=-a}^a iX_i\rbrace$ where $X_i$ is a non-negative integer and the positive integer $a$ increases by 2 each time(so there are $a+1$ terms in the sum), up to the constraint that $\sum\limits_{i=-a}^a X_i=b$, where $b$ is another positive integer. Now consider another set $B=\lbrace\sum\limits_{i=-b}^b iY_i\rbrace$ up to the constraint $\sum\limits_{i=-b}^b Y_i=a$, where the same $a, b$ are used and $Y_i$ is also a non-negative integer as $X_i$. The theorem is $A=B$ for all $a,b$. My strategy is to prove it by induction. It is obvious that they have the same finite cardinality. I need to construct a map which is bijective between $A,B$, but I do not how. Can someone help?",,"['combinatorics', 'number-theory', 'discrete-mathematics']"
11,Problem involving recurrence equation,Problem involving recurrence equation,,"I have a problem involving two recurrence equations and I can't find an algebraic solution for it. I can however use Excel to determine its solution by generating their terms and check when their difference goes to zero. The problem The population growth in city A is described by: $a_1=120000$ $a_{n+1}=a_n+0.02(1-\frac{a_n}{230000})\times{a_n}$ and in the city B is described by: $b_1=250000$ $b_{n+1}=b_n+0.03(1-\frac{b_n}{140000})\times{b_n}$ Is there a 'n' such that both cities have the same number of population? If I could express both series in terms of n instead of in terms of previous term then the solution would be to determine that n such as $a_{n+1}=b_{n+1}$ but both series are polynomial and the whole expression becomes ugly and complicated. With the help of Excel I could determine that for n=39 they have about the same number of population (not exactly but just about). Question : Can this problem be solved only with a pen and paper, i.e. without computation tools? Note: this is a high-school problem but I would accept any advanced solution (i.e. college/university level). PS: I've assumed that if there exists a n such that $a_{n+1}=b_{n+1}$ then perhaps for that n the $a_n=b_b$ and I've tried to determine that $a_n$ by it led nowhere.","I have a problem involving two recurrence equations and I can't find an algebraic solution for it. I can however use Excel to determine its solution by generating their terms and check when their difference goes to zero. The problem The population growth in city A is described by: $a_1=120000$ $a_{n+1}=a_n+0.02(1-\frac{a_n}{230000})\times{a_n}$ and in the city B is described by: $b_1=250000$ $b_{n+1}=b_n+0.03(1-\frac{b_n}{140000})\times{b_n}$ Is there a 'n' such that both cities have the same number of population? If I could express both series in terms of n instead of in terms of previous term then the solution would be to determine that n such as $a_{n+1}=b_{n+1}$ but both series are polynomial and the whole expression becomes ugly and complicated. With the help of Excel I could determine that for n=39 they have about the same number of population (not exactly but just about). Question : Can this problem be solved only with a pen and paper, i.e. without computation tools? Note: this is a high-school problem but I would accept any advanced solution (i.e. college/university level). PS: I've assumed that if there exists a n such that $a_{n+1}=b_{n+1}$ then perhaps for that n the $a_n=b_b$ and I've tried to determine that $a_n$ by it led nowhere.",,['discrete-mathematics']
12,Proving a relation is a total order relation,Proving a relation is a total order relation,,"Consider question #21 part a: Here is the solution: However, consider the definition of a total order relation: The solution didn't prove that the relation is a partial order relation. This is a mistake in the solution, right?","Consider question #21 part a: Here is the solution: However, consider the definition of a total order relation: The solution didn't prove that the relation is a partial order relation. This is a mistake in the solution, right?",,"['discrete-mathematics', 'proof-verification', 'relations']"
13,Please check these proofs for sets,Please check these proofs for sets,,"I would appreciate the insight again for a couple of proofs since I'm learning.  These are homework problems in so much as they are problems from the textbook.  They are not required by my professor.  I'm doing a little extra through spring break. The objective was to prove the statement or provide counter examples.  I'd like to have my work critiqued since I've got to refine my abilities for proofs. The following applies to both problems: Let $A, B, C$ be sets. Problem 1 Prove: $A \oplus B = A \oplus C \Rightarrow B = C$. My objective is to is to show that $B \subseteq C$ and $C \subseteq B$. Proof: Case 1: $$ \begin{align} \text{let}\, x \in B \mid x \in A \oplus B \\ \text{so,}\, x \in B \, \text{and} \, x \notin A \text{, by definition} \\ \text{since,}\, A \oplus B = A \oplus C, x \in C \,\square \end{align} $$ Case 2: $$ \begin{align} \text{let}\, x \in C \mid x \in A \oplus C \\ \text{so,}\, x \in C \, \text{and} \, x \notin A \text{, by definition} \\ \text{since,}\, A \oplus C = A \oplus B, x \in B \,\square \end{align} $$ Problem 2 Prove: $A \times B = A \times C \Rightarrow B = C$. A similar problem and I must show that $B \subseteq C$ and $C \subseteq B$. Proof: Case 1: $$ \begin{align} \text{let} (a,b) \in A \times B\, \text{and}\, (a,c) \in A \times C \\ \text{since}\, A \times B = A \times C \text{,}\, \forall (a,b)\text{,} (a,b) = (a,c) \\ \text{so,}\, a = a\, \text{and}\, b=c \\ \text{thus}\, \forall b \in B, b \in C\,\square \end{align} $$ Case 2: $$ \begin{align} \text{let} (a,c) \in A \times C\, \text{and}\, (a,b) \in A \times B \\ \text{since}\, A \times C = A \times B \text{,}\, \forall (a,c)\text{,} (a,c) = (a,b) \\ \text{so,}\, a = a\, \text{and}\, c=b \\ \text{thus}\, \forall c \in C, c \in B\,\square \end{align} $$ I do appreciate the critique. Thanks, Andy","I would appreciate the insight again for a couple of proofs since I'm learning.  These are homework problems in so much as they are problems from the textbook.  They are not required by my professor.  I'm doing a little extra through spring break. The objective was to prove the statement or provide counter examples.  I'd like to have my work critiqued since I've got to refine my abilities for proofs. The following applies to both problems: Let $A, B, C$ be sets. Problem 1 Prove: $A \oplus B = A \oplus C \Rightarrow B = C$. My objective is to is to show that $B \subseteq C$ and $C \subseteq B$. Proof: Case 1: $$ \begin{align} \text{let}\, x \in B \mid x \in A \oplus B \\ \text{so,}\, x \in B \, \text{and} \, x \notin A \text{, by definition} \\ \text{since,}\, A \oplus B = A \oplus C, x \in C \,\square \end{align} $$ Case 2: $$ \begin{align} \text{let}\, x \in C \mid x \in A \oplus C \\ \text{so,}\, x \in C \, \text{and} \, x \notin A \text{, by definition} \\ \text{since,}\, A \oplus C = A \oplus B, x \in B \,\square \end{align} $$ Problem 2 Prove: $A \times B = A \times C \Rightarrow B = C$. A similar problem and I must show that $B \subseteq C$ and $C \subseteq B$. Proof: Case 1: $$ \begin{align} \text{let} (a,b) \in A \times B\, \text{and}\, (a,c) \in A \times C \\ \text{since}\, A \times B = A \times C \text{,}\, \forall (a,b)\text{,} (a,b) = (a,c) \\ \text{so,}\, a = a\, \text{and}\, b=c \\ \text{thus}\, \forall b \in B, b \in C\,\square \end{align} $$ Case 2: $$ \begin{align} \text{let} (a,c) \in A \times C\, \text{and}\, (a,b) \in A \times B \\ \text{since}\, A \times C = A \times B \text{,}\, \forall (a,c)\text{,} (a,c) = (a,b) \\ \text{so,}\, a = a\, \text{and}\, c=b \\ \text{thus}\, \forall c \in C, c \in B\,\square \end{align} $$ I do appreciate the critique. Thanks, Andy",,"['elementary-set-theory', 'discrete-mathematics', 'proof-verification']"
14,How to find first five terms of sequence?,How to find first five terms of sequence?,,"I'm new to recursion so please bear with me. I have to find the first five terms of a sequence with initial conditions $u_1 = 1$ and $u_2 = 5$, and, for $n \geq 3$, $$u_n = 5u_{n−1} − 6u_{n−2}.$$ I believe that $u_3 =  19$, if I'm correct, but after that I'm stuck. Could someone please help me figure the next two terms and the closed formula for this sequence.","I'm new to recursion so please bear with me. I have to find the first five terms of a sequence with initial conditions $u_1 = 1$ and $u_2 = 5$, and, for $n \geq 3$, $$u_n = 5u_{n−1} − 6u_{n−2}.$$ I believe that $u_3 =  19$, if I'm correct, but after that I'm stuck. Could someone please help me figure the next two terms and the closed formula for this sequence.",,"['discrete-mathematics', 'recurrence-relations', 'recursion']"
15,Prove the following sets equalities,Prove the following sets equalities,,"I'm really struggling with proofes, please tell me if I'm correct and if there is a better way to prove (or disprove) the following: i) $(A \setminus B) \setminus B = A \setminus B$ My answer: $x\in((A \setminus B)\setminus B) \Leftrightarrow x\in (A \setminus B)$ and $x\notin B \Leftrightarrow x\in A$ and $ x \notin B \Leftrightarrow x\in (A \setminus B)$ ii) $A - (B - A) = A$ My answer: $x\in (A \setminus (B \setminus A)) \Leftrightarrow x\in A$ and $x\notin (B \setminus A) \Leftrightarrow x\in A$ iii) $P(A \cup B) = P(A) \cup P(B)$ My answer: not true. The case where $A=\{3\}$ and $B=\{5\}$, $P(A\cup B) = \{\emptyset,\{3\},\{5\},\{3,5\}\}$ and $P(A) \cup P(B) = \{\emptyset,\{3\},\{5\}\}$ iv) $P(A\cap B) = P(A)\cap P(B)$ My answer: $x\in P(A\cap B) \Leftrightarrow x \subseteq (A\cap B) \Leftrightarrow x \subseteq A $ and $ x\subseteq B \Leftrightarrow x \in P(A)$ and $x\in P(B) \Leftrightarrow x \in (P(A) \cap P(B))$","I'm really struggling with proofes, please tell me if I'm correct and if there is a better way to prove (or disprove) the following: i) $(A \setminus B) \setminus B = A \setminus B$ My answer: $x\in((A \setminus B)\setminus B) \Leftrightarrow x\in (A \setminus B)$ and $x\notin B \Leftrightarrow x\in A$ and $ x \notin B \Leftrightarrow x\in (A \setminus B)$ ii) $A - (B - A) = A$ My answer: $x\in (A \setminus (B \setminus A)) \Leftrightarrow x\in A$ and $x\notin (B \setminus A) \Leftrightarrow x\in A$ iii) $P(A \cup B) = P(A) \cup P(B)$ My answer: not true. The case where $A=\{3\}$ and $B=\{5\}$, $P(A\cup B) = \{\emptyset,\{3\},\{5\},\{3,5\}\}$ and $P(A) \cup P(B) = \{\emptyset,\{3\},\{5\}\}$ iv) $P(A\cap B) = P(A)\cap P(B)$ My answer: $x\in P(A\cap B) \Leftrightarrow x \subseteq (A\cap B) \Leftrightarrow x \subseteq A $ and $ x\subseteq B \Leftrightarrow x \in P(A)$ and $x\in P(B) \Leftrightarrow x \in (P(A) \cap P(B))$",,"['elementary-set-theory', 'discrete-mathematics', 'proof-verification']"
16,How many vertices does this tree have?,How many vertices does this tree have?,,"Suppose that $T$ is a tree. It has $e$ edges and $n$ vertices, and   $\overline{T}$ has $10e$ edges. What is n? I think $n = 1$ is a solution, because $T$ can have no edges then, so $0=10*0$. A tree with $n$ vertices has $n-1$ edges. So it's complimentary has $n(n-1) - (n-1) = (n-1)^2$ edges. Therefore, I think, solutions of $$10(n-1)=(n-1)^2$$ are the $n$'s that fulfill the requirements. So $n=1$ comes this way as well. And there is another possible solution, $n=11$. Is this the right solution? If so, is there a better way to come to the same conclusion?","Suppose that $T$ is a tree. It has $e$ edges and $n$ vertices, and   $\overline{T}$ has $10e$ edges. What is n? I think $n = 1$ is a solution, because $T$ can have no edges then, so $0=10*0$. A tree with $n$ vertices has $n-1$ edges. So it's complimentary has $n(n-1) - (n-1) = (n-1)^2$ edges. Therefore, I think, solutions of $$10(n-1)=(n-1)^2$$ are the $n$'s that fulfill the requirements. So $n=1$ comes this way as well. And there is another possible solution, $n=11$. Is this the right solution? If so, is there a better way to come to the same conclusion?",,"['discrete-mathematics', 'graph-theory', 'trees']"
17,"Are continuous mathematical models of discrete physical phenomena messy because of a disconnect between ""continuous"" and ""discontinuous""?","Are continuous mathematical models of discrete physical phenomena messy because of a disconnect between ""continuous"" and ""discontinuous""?",,"Examples from statistical mechanics and continuum mechanics abound: a discrete phenomenon (e.g. kinetic energy of molecules) is ""averaged"" out over the constituents of the system to which it applied (continuing the example: temperature, or pressure), and then calculations are simplified for certain special cases (e.g. ideal gases), at the cost of increasing ""messiness"" as increasingly general cases are considered (e.g. non-ideal gases). I am not familiar with combinatorics and discrete mathematics, so please be gentle: are there trains of thought being explored in modern research that wonder if discrete physical phenomena might be better modelled by discrete mathematics? If such a direction is not being seriously looked into because discrete math doesn't model discrete phenomenon in a neater way, why are discrete math models of discrete physical phenomena as messy as continuous math models of discrete physical phenomena? P.S. What resources could I use to further read about related subjects?","Examples from statistical mechanics and continuum mechanics abound: a discrete phenomenon (e.g. kinetic energy of molecules) is ""averaged"" out over the constituents of the system to which it applied (continuing the example: temperature, or pressure), and then calculations are simplified for certain special cases (e.g. ideal gases), at the cost of increasing ""messiness"" as increasingly general cases are considered (e.g. non-ideal gases). I am not familiar with combinatorics and discrete mathematics, so please be gentle: are there trains of thought being explored in modern research that wonder if discrete physical phenomena might be better modelled by discrete mathematics? If such a direction is not being seriously looked into because discrete math doesn't model discrete phenomenon in a neater way, why are discrete math models of discrete physical phenomena as messy as continuous math models of discrete physical phenomena? P.S. What resources could I use to further read about related subjects?",,"['general-topology', 'reference-request', 'discrete-mathematics', 'soft-question', 'mathematical-physics']"
18,"There's only one pair of positive integers $x,y$ such that $n=\frac{(x+y)(x+y+1)}{2}+x$.",There's only one pair of positive integers  such that .,"x,y n=\frac{(x+y)(x+y+1)}{2}+x","We have $n \in \mathbb{N}$. I need to prove that for any $n \in \mathbb{N}$, the number $n$ can be expressed as $\frac{(x+y)(x+y+1)}{2}+x=n$, where $x,y$ are two positive integers, and that this representation is unique. Can anybody give me a hint here?","We have $n \in \mathbb{N}$. I need to prove that for any $n \in \mathbb{N}$, the number $n$ can be expressed as $\frac{(x+y)(x+y+1)}{2}+x=n$, where $x,y$ are two positive integers, and that this representation is unique. Can anybody give me a hint here?",,['discrete-mathematics']
19,Strong Induction - Understanding the lateral conditions.,Strong Induction - Understanding the lateral conditions.,,"I dont want the proof of this statement unless it is necessary for my questions , I just want some clarification. If cr = 1 would cr-1 = 0? How is cj 1 or 0? I understand cj is an arbitrarily picked number within the theorem 'line' but why is it limited to a rather boolean value?","I dont want the proof of this statement unless it is necessary for my questions , I just want some clarification. If cr = 1 would cr-1 = 0? How is cj 1 or 0? I understand cj is an arbitrarily picked number within the theorem 'line' but why is it limited to a rather boolean value?",,['discrete-mathematics']
20,Probability of drawing balls without replacement,Probability of drawing balls without replacement,,"I saw this problem here , but since it's on hold I have no other choice than reposting it. Suppose there are $2$ players, $r≥1$ red balls and $(n−r)$ black balls in the bin, what is the probability that first player draw the red ball first? I've already solved the case where the balls are drawn with replacement, this one however seems more complex. Here's my attempt: Let A be the case that the first player draws the red ball, and B be the case that the second one draws the red ball. $P(A)=A+A'B'A+A'B'A'B'A+\dots$ $P(A)=\frac{r}{n}+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdot\frac{r}{n-2})+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdot\frac{n-r-2}{n-2}\cdot\frac{n-r-3}{n-3}\cdot\frac{r}{n-4})+\dots+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdots\frac{2}{r+2}\cdot\frac{1}{r+1}\cdot\frac{r}{r})$ Nevertheless I have no idea how we can evalute this sum.","I saw this problem here , but since it's on hold I have no other choice than reposting it. Suppose there are $2$ players, $r≥1$ red balls and $(n−r)$ black balls in the bin, what is the probability that first player draw the red ball first? I've already solved the case where the balls are drawn with replacement, this one however seems more complex. Here's my attempt: Let A be the case that the first player draws the red ball, and B be the case that the second one draws the red ball. $P(A)=A+A'B'A+A'B'A'B'A+\dots$ $P(A)=\frac{r}{n}+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdot\frac{r}{n-2})+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdot\frac{n-r-2}{n-2}\cdot\frac{n-r-3}{n-3}\cdot\frac{r}{n-4})+\dots+(\frac{n-r}{n}\cdot\frac{n-r-1}{n-1}\cdots\frac{2}{r+2}\cdot\frac{1}{r+1}\cdot\frac{r}{r})$ Nevertheless I have no idea how we can evalute this sum.",,"['combinatorics', 'discrete-mathematics']"
21,Is this relation an equivalence relation? Check my solution please.,Is this relation an equivalence relation? Check my solution please.,,"Define a relation R on the set {$2, 3, 4, ... $}, as follows. $(x, y)$ ∈ R if and only if $x$ and $y$ have a common factor greater than $1$. Is this relation reflexive? Is it symmetric? Is it transitive? Is this an equivalence relation? Justify your arguments. My Attempt: 1) It is reflexive, since the set begins at $2$ and $(x,x)$ will always be a common factor of itself, which is greater than $1$. 2)It is symmetric, because order doesn't matter. (I do not know how I should show this formally though) 3) Not transitive as $(2,6)$ and $(6,9)$ have GCF's $> 1$ but $(2,9)$ does not. Therefore this is not an equivalence relation.","Define a relation R on the set {$2, 3, 4, ... $}, as follows. $(x, y)$ ∈ R if and only if $x$ and $y$ have a common factor greater than $1$. Is this relation reflexive? Is it symmetric? Is it transitive? Is this an equivalence relation? Justify your arguments. My Attempt: 1) It is reflexive, since the set begins at $2$ and $(x,x)$ will always be a common factor of itself, which is greater than $1$. 2)It is symmetric, because order doesn't matter. (I do not know how I should show this formally though) 3) Not transitive as $(2,6)$ and $(6,9)$ have GCF's $> 1$ but $(2,9)$ does not. Therefore this is not an equivalence relation.",,"['discrete-mathematics', 'relations']"
22,Are universal quantified statements defined for inequalities even if the inequality is undefined?,Are universal quantified statements defined for inequalities even if the inequality is undefined?,,The following universally quantified statement has an undefined inequality when $x = 1$: $∀x∈ℝ \dfrac 1{(x−1)^2}>0$ Is such a statement false or undefined?,The following universally quantified statement has an undefined inequality when $x = 1$: $∀x∈ℝ \dfrac 1{(x−1)^2}>0$ Is such a statement false or undefined?,,"['discrete-mathematics', 'logic']"
23,Why must the subgraphs that make up the solutions to Instant Insanity be disjoint?,Why must the subgraphs that make up the solutions to Instant Insanity be disjoint?,,"I keep hearing that the subgraphs to the game Instant Insanity must be disjoint. Why is this true? What if the same two colour on the front and back of a cube are the same two colours on the sides of a cube? According to the rules of the game this could still give a solution. For example in this youtube video it's not clear to me why you need to find a subgraph with no edges in common? I guess it has to do that if there were the same colour on opposite sides twice than there would be two lines on the graph between those colours. For example if cube 1 has a red front and a green back and a red side and a green other side, then there would be two edges connecting vertices green and red in a graph (and both edges would be labeled 1) so two subgraphs that are disjoint could still be formed. Am I right? Or am I not being clear enough?","I keep hearing that the subgraphs to the game Instant Insanity must be disjoint. Why is this true? What if the same two colour on the front and back of a cube are the same two colours on the sides of a cube? According to the rules of the game this could still give a solution. For example in this youtube video it's not clear to me why you need to find a subgraph with no edges in common? I guess it has to do that if there were the same colour on opposite sides twice than there would be two lines on the graph between those colours. For example if cube 1 has a red front and a green back and a red side and a green other side, then there would be two edges connecting vertices green and red in a graph (and both edges would be labeled 1) so two subgraphs that are disjoint could still be formed. Am I right? Or am I not being clear enough?",,"['discrete-mathematics', 'graph-theory', 'puzzle']"
24,Prove B is a subset of D,Prove B is a subset of D,,"Let A,B,C and D be four sets. Prove that if $A\cup B \subseteq C\cup D$, $A\cap B = \varnothing$, and $C\subseteq A$  then $B\subseteq D$","Let A,B,C and D be four sets. Prove that if $A\cup B \subseteq C\cup D$, $A\cap B = \varnothing$, and $C\subseteq A$  then $B\subseteq D$",,"['elementary-set-theory', 'discrete-mathematics']"
25,Lego Blocks and Linearity of Expectation,Lego Blocks and Linearity of Expectation,,"You are given a Lego block of length $10$ (length is measured discretely as number of “knobs”; the width of all blocks is assumed to be one) and three more Lego blocks of length $3$, $4$, $5$, respectively. The Lego blocks are randomly stacked upon the block of length $10$ such that their boundaries are contained within the boundaries of the block of length $10$. The position of a short block is given as the position of the left-most knob of the $10$-block covered by it. Hence, the block of length $3$ could be with equal probability in position $1$, $2$, $3$, $4$, $5$, $6$, $7$, or $8$, the block of length $4$ with equal probability in positions $1$, $2$, $3$, $4$, $5$, $6$, or $7$ and the block of length $5$ with equal probability in positions $1$, $2$, $3$, $4$, $5$ or $6$. The short blocks are placed independently at random. Let $X$ be the number of knobs out of the ten of the large block, which are covered by all three shorter blocks; $X \in \{0, 1, 2, 3\}$. What is $E[X]$? Hint: Use indicator random variables. I am confused as how to approach this problem. What should the indicator variables be, 1 for covered by all three blocks and 0 for not covered by all three blocks? If so then how do I find the probability of all being covered at once?","You are given a Lego block of length $10$ (length is measured discretely as number of “knobs”; the width of all blocks is assumed to be one) and three more Lego blocks of length $3$, $4$, $5$, respectively. The Lego blocks are randomly stacked upon the block of length $10$ such that their boundaries are contained within the boundaries of the block of length $10$. The position of a short block is given as the position of the left-most knob of the $10$-block covered by it. Hence, the block of length $3$ could be with equal probability in position $1$, $2$, $3$, $4$, $5$, $6$, $7$, or $8$, the block of length $4$ with equal probability in positions $1$, $2$, $3$, $4$, $5$, $6$, or $7$ and the block of length $5$ with equal probability in positions $1$, $2$, $3$, $4$, $5$ or $6$. The short blocks are placed independently at random. Let $X$ be the number of knobs out of the ten of the large block, which are covered by all three shorter blocks; $X \in \{0, 1, 2, 3\}$. What is $E[X]$? Hint: Use indicator random variables. I am confused as how to approach this problem. What should the indicator variables be, 1 for covered by all three blocks and 0 for not covered by all three blocks? If so then how do I find the probability of all being covered at once?",,"['probability', 'discrete-mathematics']"
26,How do you find the power set within a power set?,How do you find the power set within a power set?,,"I'm trying to find P(P(A)), where A = {0, 1, 2, 4, 7, 9}. Any ideas?","I'm trying to find P(P(A)), where A = {0, 1, 2, 4, 7, 9}. Any ideas?",,['discrete-mathematics']
27,Graphs with a polynomial number of shortest paths between any pair of vertices,Graphs with a polynomial number of shortest paths between any pair of vertices,,"Let $G$ be a simple undirected graph, and let $s$ and $t$ be two arbitrary vertices of $G$. Even for some rather restricted graph classes, the number of shortest paths between $s$ and $t$ can be exponential in the size of $G$. However, for some graph classes like block graphs , the number of shortest paths between $s$ and $t$ is 1 (such graphs are known as geodetic graphs). For what graph classes $G$ is it true that between any pair of vertices $s$ and $t$ of $G$, the number of shortest paths between $s$ and $t$ is bounded from above by $\text{poly}(n,m)$? Here, $n$ is the number of vertices, and $m$ the number of edges. For example, are there graphs (other than block graphs) that would have a constant number of shortest paths between any pair of vertices?","Let $G$ be a simple undirected graph, and let $s$ and $t$ be two arbitrary vertices of $G$. Even for some rather restricted graph classes, the number of shortest paths between $s$ and $t$ can be exponential in the size of $G$. However, for some graph classes like block graphs , the number of shortest paths between $s$ and $t$ is 1 (such graphs are known as geodetic graphs). For what graph classes $G$ is it true that between any pair of vertices $s$ and $t$ of $G$, the number of shortest paths between $s$ and $t$ is bounded from above by $\text{poly}(n,m)$? Here, $n$ is the number of vertices, and $m$ the number of edges. For example, are there graphs (other than block graphs) that would have a constant number of shortest paths between any pair of vertices?",,"['reference-request', 'discrete-mathematics', 'graph-theory']"
28,"Probability, that when we send a $0$ down the network we will get back a $0$","Probability, that when we send a  down the network we will get back a",0 0,"We can send a $0$ or a $1$ over a network of $1,2...$ nodes. Unfortunately on each node with probability $p$ the message is not made different, and with probability $1-p$ the message is XOR'ed. Find recurrence relation that determines with what probability if we send a $0$ over a network of $n$ nodes we will get $0$ after $n$'th node. Base cases. Obviously $$a_1=p$$ and $$a_2=p^2 + (1-p)^2$$ Because we can either succeed two times or fail two times and be ok. But what about the rest? $$a_3=p^3+{{3}\choose{2}}(1-p)^2p$$ $$a_4=p^4+ (1-p)^4+{{4}\choose{2}}(1-p)^2p^2$$ And I fail in seeing a reccurent relation between the subsequent $a_n$'s.","We can send a $0$ or a $1$ over a network of $1,2...$ nodes. Unfortunately on each node with probability $p$ the message is not made different, and with probability $1-p$ the message is XOR'ed. Find recurrence relation that determines with what probability if we send a $0$ over a network of $n$ nodes we will get $0$ after $n$'th node. Base cases. Obviously $$a_1=p$$ and $$a_2=p^2 + (1-p)^2$$ Because we can either succeed two times or fail two times and be ok. But what about the rest? $$a_3=p^3+{{3}\choose{2}}(1-p)^2p$$ $$a_4=p^4+ (1-p)^4+{{4}\choose{2}}(1-p)^2p^2$$ And I fail in seeing a reccurent relation between the subsequent $a_n$'s.",,"['discrete-mathematics', 'recurrence-relations']"
29,Proving statements using Euclidean division,Proving statements using Euclidean division,,"I have a series of statements that are proved based on the equation for Euclidean division, this is: Given two integers $a$ and $b$, with $b ≠ 0$, there exist unique   integers $q$ and $r$ such that $a = bq + r$ and $0 ≤ r < |b|$, where   $|b|$ denotes the absolute value of $b$. And these are the statements: 1) The square value of every odd integer can be written as $8k+1$. 2) If $p ≠ 3$ is a prime number then $p²+2$ is not a prime number. 3) If $a$ is an integer number that can not be divided by $2$ and $3$, then $24$ divides $(a²-1)$. 4) The sum of the square values of two odd integer numbers can not be a square value. 5) Prove that if $p \geq q \geq 5 $ and $p$ and $q$ are prime numbers then $24|(p²-q²)$. And this are the initial values given to $b$ in the Euclidean equation for each statement in order to prove the statements: 1) $4q+r$ 2) $6q+r$ 3) $12q+r$ 4) $4q+r$ 5) $12q+r$ My question is: How is the value of $b$ assigned?","I have a series of statements that are proved based on the equation for Euclidean division, this is: Given two integers $a$ and $b$, with $b ≠ 0$, there exist unique   integers $q$ and $r$ such that $a = bq + r$ and $0 ≤ r < |b|$, where   $|b|$ denotes the absolute value of $b$. And these are the statements: 1) The square value of every odd integer can be written as $8k+1$. 2) If $p ≠ 3$ is a prime number then $p²+2$ is not a prime number. 3) If $a$ is an integer number that can not be divided by $2$ and $3$, then $24$ divides $(a²-1)$. 4) The sum of the square values of two odd integer numbers can not be a square value. 5) Prove that if $p \geq q \geq 5 $ and $p$ and $q$ are prime numbers then $24|(p²-q²)$. And this are the initial values given to $b$ in the Euclidean equation for each statement in order to prove the statements: 1) $4q+r$ 2) $6q+r$ 3) $12q+r$ 4) $4q+r$ 5) $12q+r$ My question is: How is the value of $b$ assigned?",,"['number-theory', 'elementary-number-theory', 'discrete-mathematics', 'prime-numbers']"
30,bags of chocolate problem,bags of chocolate problem,,I have $4$ different types of chocolates. How many unique bags of chocolate can I make with $10$ items per bag that has at least one type of each chocolate in each bag? I don't know if this is correct but this is what i got: $$\frac{\frac{10!}{6!}\cdot\frac{6!}{4!}\cdot\frac{4!}{2!}}{10!}$$,I have $4$ different types of chocolates. How many unique bags of chocolate can I make with $10$ items per bag that has at least one type of each chocolate in each bag? I don't know if this is correct but this is what i got: $$\frac{\frac{10!}{6!}\cdot\frac{6!}{4!}\cdot\frac{4!}{2!}}{10!}$$,,"['combinatorics', 'discrete-mathematics']"
31,Determining the properties for the relation over $P(\mathbb{N})$ where $ARB \iff A \cup B \in H$,Determining the properties for the relation over  where,P(\mathbb{N}) ARB \iff A \cup B \in H,"I had two problems with this exercise: I don't know the universe for doing $\overline{A}$ (I'll show below). I couldn't show that it was transitive, although I'm fairly sure it is. Can you assist me there? For $A \in P(\mathbb{N})$, we say that $A \in H \iff \overline{A}$ is   finite. Over $P(\mathbb{N})$ is defined the relation $R$: $$ARB \iff A \cup B \in H$$ Determine its properties. First of all, the beginning statement says that for a part X of $\mathbb{N}$: $X \in H \implies \overline{X}$ is finite $\overline{X}$ is finite $\implies X \in H$ Question : When it says $\overline{A}$, I need to know the universe we're working on, right? Is the universe $\mathbb{N}$? Or $P(\mathbb{N})$? Or $\mathbb{R}$? For this exercise I will assume that it refers to $\mathbb{N}$. Reflexive No. Have a set $A$ in $P(\mathbb{N})$ where $A = \{1,2,3\}$. Note that $\overline{A}$ would be infinite, given that the universe is $\mathbb{N}$. Since $\overline{A}$ is infinite, $A \not \in H$, according to the premise we're given. $A \cup A = A$, and we know that $A \not \in H$. Therefore the relation is not reflexive. Symmetric Yes. We want to prove that $ARB \implies BRA$ for $A,B \in P(\mathbb{N})$. Have $$ARB$$ $$A \cup B \in H$$ Since $\cup$ is commutative, it is equivalent to $$B \cup A \in H$$ Hence, the relation is symmetric. Antisymmetric No. Have sets $A,B \in P(\mathbb{N})$, where $A = \mathbb{N}$ and $B = \{1,2,3\}$. Since $\overline{A} = \emptyset$, $\overline{A}$ is finite. According to the premise, this means that $A \in H$. Therefore: $$A \cup B \in H$$ So $ARB$. And then: $$B \cup A \in H$$ So $BRA$. However: $$A \not = B$$ So the relation isn't antisymmetric. Transitive Yes. Have $A,B,C \in P(\mathbb{N})$ where $ARB \land BRC$. $$ARB \land BRC$$ $$(A \in H \lor B \in H) \land (B \in H \lor C \in H)$$ $$(B \in H) \lor (A \in H \land C \in H)$$ $$B \in H \lor A \in H \lor C \in H$$ If I could show that $B \not \in H$ I would be done. But how? Total No. Have sets $A,B \in P(\mathbb{N})$ where $A = \{1,2,3\}$ and $B = \{4,5,6\}$ From our reflexive proof, we know that $A \not \in H$. The same logic can be applied to $B$. Since neither are in $H$, we have that $$A \cup B \not \in H$$ And of course, $$B \cup A \not \in H$$ Thus, $$A\not R B \land B \not R A$$ So the relation isn't total.","I had two problems with this exercise: I don't know the universe for doing $\overline{A}$ (I'll show below). I couldn't show that it was transitive, although I'm fairly sure it is. Can you assist me there? For $A \in P(\mathbb{N})$, we say that $A \in H \iff \overline{A}$ is   finite. Over $P(\mathbb{N})$ is defined the relation $R$: $$ARB \iff A \cup B \in H$$ Determine its properties. First of all, the beginning statement says that for a part X of $\mathbb{N}$: $X \in H \implies \overline{X}$ is finite $\overline{X}$ is finite $\implies X \in H$ Question : When it says $\overline{A}$, I need to know the universe we're working on, right? Is the universe $\mathbb{N}$? Or $P(\mathbb{N})$? Or $\mathbb{R}$? For this exercise I will assume that it refers to $\mathbb{N}$. Reflexive No. Have a set $A$ in $P(\mathbb{N})$ where $A = \{1,2,3\}$. Note that $\overline{A}$ would be infinite, given that the universe is $\mathbb{N}$. Since $\overline{A}$ is infinite, $A \not \in H$, according to the premise we're given. $A \cup A = A$, and we know that $A \not \in H$. Therefore the relation is not reflexive. Symmetric Yes. We want to prove that $ARB \implies BRA$ for $A,B \in P(\mathbb{N})$. Have $$ARB$$ $$A \cup B \in H$$ Since $\cup$ is commutative, it is equivalent to $$B \cup A \in H$$ Hence, the relation is symmetric. Antisymmetric No. Have sets $A,B \in P(\mathbb{N})$, where $A = \mathbb{N}$ and $B = \{1,2,3\}$. Since $\overline{A} = \emptyset$, $\overline{A}$ is finite. According to the premise, this means that $A \in H$. Therefore: $$A \cup B \in H$$ So $ARB$. And then: $$B \cup A \in H$$ So $BRA$. However: $$A \not = B$$ So the relation isn't antisymmetric. Transitive Yes. Have $A,B,C \in P(\mathbb{N})$ where $ARB \land BRC$. $$ARB \land BRC$$ $$(A \in H \lor B \in H) \land (B \in H \lor C \in H)$$ $$(B \in H) \lor (A \in H \land C \in H)$$ $$B \in H \lor A \in H \lor C \in H$$ If I could show that $B \not \in H$ I would be done. But how? Total No. Have sets $A,B \in P(\mathbb{N})$ where $A = \{1,2,3\}$ and $B = \{4,5,6\}$ From our reflexive proof, we know that $A \not \in H$. The same logic can be applied to $B$. Since neither are in $H$, we have that $$A \cup B \not \in H$$ And of course, $$B \cup A \not \in H$$ Thus, $$A\not R B \land B \not R A$$ So the relation isn't total.",,"['elementary-set-theory', 'discrete-mathematics', 'relations']"
32,Attempting to find the equivalence class of 5.,Attempting to find the equivalence class of 5.,,"For $a,b \in \mathbb{R}$ define $a \sim b$ if $a - b \in \mathbb{Z}$ How would you find the equivalence class of 5. In other words what I'm trying to describe is the set $[5]$ = {$y : 5 \sim y$}. And $[5]$ is just the name of the set.","For $a,b \in \mathbb{R}$ define $a \sim b$ if $a - b \in \mathbb{Z}$ How would you find the equivalence class of 5. In other words what I'm trying to describe is the set $[5]$ = {$y : 5 \sim y$}. And $[5]$ is just the name of the set.",,"['discrete-mathematics', 'relations', 'equivalence-relations']"
33,Number of palindromic 6 letter sequences from 4 characters,Number of palindromic 6 letter sequences from 4 characters,,"The genetic code can be viewed as a sequence of four letters T, A, G, and C. There were two parts to the question: (a) How many 6-letter sequences are there? I just said $\binom{4}{1}^6$, or $\binom{4}{1}$ choices for each letter. b is where I am having trouble. (b) How many 6-letter sequences are palindromic, (read the same forward as backward)? I originally thought that: because for each of the first three letters you get a designated letter for the last three letters, that is if the first is A, the last is A, if the second is T, the second to last is T, etc... So we only need to concern how many options there are for the first three letters. So I thought there should be  $$\binom{4}{1}^3\text{combinations.}$$ Then I thought that we might be over counting though. I can't really explain why I think that. I just wanted to check. What do you think?","The genetic code can be viewed as a sequence of four letters T, A, G, and C. There were two parts to the question: (a) How many 6-letter sequences are there? I just said $\binom{4}{1}^6$, or $\binom{4}{1}$ choices for each letter. b is where I am having trouble. (b) How many 6-letter sequences are palindromic, (read the same forward as backward)? I originally thought that: because for each of the first three letters you get a designated letter for the last three letters, that is if the first is A, the last is A, if the second is T, the second to last is T, etc... So we only need to concern how many options there are for the first three letters. So I thought there should be  $$\binom{4}{1}^3\text{combinations.}$$ Then I thought that we might be over counting though. I can't really explain why I think that. I just wanted to check. What do you think?",,"['combinatorics', 'discrete-mathematics', 'solution-verification', 'palindrome']"
34,Prove a sum of sequence: Discrete math and weak induction,Prove a sum of sequence: Discrete math and weak induction,,"The problem is as follows: Prove that $2 - (2\cdot7) + ((2\cdot7)^2) - ... +(2(-7))^n = > \frac{(1-(-7)^{(n+1)})}{4}$ whenever $n$ is a non-negative integer. Our book is asking for a basic inductive proof to show that for any $n$, $n+1$ will also hold. So far I have the following: $\frac{1-(-7)^{(n+1)}}{4} + 2(-7)^{(n+1)} =?= \frac{1-(-7)^{(n+2)}}{4}$ I'm a little confused about why these problems are giving me so much trouble. I understand that we will assume (after having proven a base case) that the proof will work for any number k, and in proving this, we will show that any number (k+1) will also hold.  Perhaps I'm just setting it up incorrectly or am missing some small algebraic facet. In any case, if some one can get me going in the right direction, I would really appreciate it.","The problem is as follows: Prove that $2 - (2\cdot7) + ((2\cdot7)^2) - ... +(2(-7))^n = > \frac{(1-(-7)^{(n+1)})}{4}$ whenever $n$ is a non-negative integer. Our book is asking for a basic inductive proof to show that for any $n$, $n+1$ will also hold. So far I have the following: $\frac{1-(-7)^{(n+1)}}{4} + 2(-7)^{(n+1)} =?= \frac{1-(-7)^{(n+2)}}{4}$ I'm a little confused about why these problems are giving me so much trouble. I understand that we will assume (after having proven a base case) that the proof will work for any number k, and in proving this, we will show that any number (k+1) will also hold.  Perhaps I'm just setting it up incorrectly or am missing some small algebraic facet. In any case, if some one can get me going in the right direction, I would really appreciate it.",,"['discrete-mathematics', 'induction']"
35,Using DeMorgan's Laws to complement a function,Using DeMorgan's Laws to complement a function,,"Using DeMorgan's Law, write an expression for the complement of $F$ if: $F(x,y,z) = x(y' + z)$. $F=x'+(y'+x)'$ $F(x,y,z) = xy + x'z + yz'$ $F=(xy)'(x'z)'(yz')'$ $F(w,x,y,z) = xyz' (y'z + x)' + (w'yz + x' )$. $F=[(xyz')'+(y'z+x)](w'yz+x')'$ My answers are underneath the numbered questions. Is everything correct? I'm not 100% sure as to what I'm exactly supposed to do. I just took all the ANDs, negated them and made them ORs and vice-versa.","Using DeMorgan's Law, write an expression for the complement of $F$ if: $F(x,y,z) = x(y' + z)$. $F=x'+(y'+x)'$ $F(x,y,z) = xy + x'z + yz'$ $F=(xy)'(x'z)'(yz')'$ $F(w,x,y,z) = xyz' (y'z + x)' + (w'yz + x' )$. $F=[(xyz')'+(y'z+x)](w'yz+x')'$ My answers are underneath the numbered questions. Is everything correct? I'm not 100% sure as to what I'm exactly supposed to do. I just took all the ANDs, negated them and made them ORs and vice-versa.",,"['logic', 'discrete-mathematics', 'propositional-calculus']"
36,Inverting a binomial sum,Inverting a binomial sum,,"Fix $n.$ Suppose I have two sequences of positive integers $a_k$ and $b_k$ that vanish for $k>n$ and satisfy the following relation: $$ a_k = \sum_{i=k}^n (-1)^{i-k} \binom{i}{k} b_{n-i}.$$ I think it is true that $$ b_k = \sum_{i=0}^k \binom{n-i}{n-k} a_{n-i}.$$ I am not sure how to prove this though. I tried induction but it gets quite complicated. It ends up having to confirm that for $m\leq n$ we have $$ \sum_{i=0}^{m-1} \binom{n-i}{n-m} a_{n-i} = \sum_{i=n-m+1}^m \sum_{j=0}^{n-i} (-1)^{i-n+m+1} \binom{n-j}{n-m, i-n+m, n-j-i} a_{n-j}$$ where the big bracket on the right is a trinomial coefficient. Can someone provide a proof please? Thanks.","Fix $n.$ Suppose I have two sequences of positive integers $a_k$ and $b_k$ that vanish for $k>n$ and satisfy the following relation: $$ a_k = \sum_{i=k}^n (-1)^{i-k} \binom{i}{k} b_{n-i}.$$ I think it is true that $$ b_k = \sum_{i=0}^k \binom{n-i}{n-k} a_{n-i}.$$ I am not sure how to prove this though. I tried induction but it gets quite complicated. It ends up having to confirm that for $m\leq n$ we have $$ \sum_{i=0}^{m-1} \binom{n-i}{n-m} a_{n-i} = \sum_{i=n-m+1}^m \sum_{j=0}^{n-i} (-1)^{i-n+m+1} \binom{n-j}{n-m, i-n+m, n-j-i} a_{n-j}$$ where the big bracket on the right is a trinomial coefficient. Can someone provide a proof please? Thanks.",,"['combinatorics', 'discrete-mathematics', 'summation']"
37,calculating degenerancy,calculating degenerancy,,"Given a function of two positive integers $n_x^2+n_y^2$. $n_x^2+n_y^2=50$ has three combinations of $n_x$ and $n_y$ that result in $n_x^2+n_y^2=50$: $$n_x=7,n_y=1$$  $$n_x=5,n_y=5$$ $$n_x=1,n_y=7$$ I need to find the net highest integer such that there are three or more combinations of $n_x$ and $n_y$ that result in this number, but I have no clue how to go about it, other than by trial and error. Can anyone point me in the right direction? A hint would be preferable.","Given a function of two positive integers $n_x^2+n_y^2$. $n_x^2+n_y^2=50$ has three combinations of $n_x$ and $n_y$ that result in $n_x^2+n_y^2=50$: $$n_x=7,n_y=1$$  $$n_x=5,n_y=5$$ $$n_x=1,n_y=7$$ I need to find the net highest integer such that there are three or more combinations of $n_x$ and $n_y$ that result in this number, but I have no clue how to go about it, other than by trial and error. Can anyone point me in the right direction? A hint would be preferable.",,"['discrete-mathematics', 'quantum-mechanics']"
38,Translating English into First Order Logic,Translating English into First Order Logic,,"Translate the following into a formula of first-order logic. ""A language L that is regular will have the following property: there will be some number N (that depends on L) such that if s is a string in L (a string is a sequence of characters) whose length is at least N then s can be written as $xyz$ where y is not the empty string and $xy^i z$ is in the language L for every nonnegative integer i."" can anyone help me with this? This is what I came up with so far and it's definitely not right... My Guess: Universe of Discourse: Language N(x)= x is some number depending on L I(x)= x is non negative integer S(x)= x is string in L for existential quantifier ill use ""bE"" and for universal quantifier ill use ""bA"" Guess starts here: bEx(N(x) ^ (S(x) > N(x)) --> bEx bEy bEz((S(x)=xyz)^y does not equal S element empty set)) ^ bAx(I(x) This is probably completely wrong; I don't really get it. Thanks for any input/help.","Translate the following into a formula of first-order logic. ""A language L that is regular will have the following property: there will be some number N (that depends on L) such that if s is a string in L (a string is a sequence of characters) whose length is at least N then s can be written as $xyz$ where y is not the empty string and $xy^i z$ is in the language L for every nonnegative integer i."" can anyone help me with this? This is what I came up with so far and it's definitely not right... My Guess: Universe of Discourse: Language N(x)= x is some number depending on L I(x)= x is non negative integer S(x)= x is string in L for existential quantifier ill use ""bE"" and for universal quantifier ill use ""bA"" Guess starts here: bEx(N(x) ^ (S(x) > N(x)) --> bEx bEy bEz((S(x)=xyz)^y does not equal S element empty set)) ^ bAx(I(x) This is probably completely wrong; I don't really get it. Thanks for any input/help.",,"['discrete-mathematics', 'first-order-logic', 'logic-translation']"
39,How many 6 letter words can be made in English with at most 2 vowels?,How many 6 letter words can be made in English with at most 2 vowels?,,How many 6 letter words can be  made in English with at most 2 vowels?,How many 6 letter words can be  made in English with at most 2 vowels?,,['discrete-mathematics']
40,Bound on signal after passing through 2 pole lowpass filter,Bound on signal after passing through 2 pole lowpass filter,,"I am trying to implement some simple digital filters for a software synthesizer. This link seemed like a good start, and many things reference it: http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt Problem is, both the frequency response formulas (H(s) = 1 / (s^2 + s/Q + 1)) and the recursive formulas don't seem to give normalized values, and passing a signal from -1 to 1 through them sometimes results in a signal with values >4, which breaks everything else. Wikipedia has some slightly different formulas for the frequency response here: http://en.wikipedia.org/wiki/Q_factor These already make more sense to me, as this can never be over 1 for any cutoff frequency, and so scales with the cutoff frequency. At least that would be my intuition, compared to the previous one. I can't find a version of this in recursive formulas though, and I'm not even sure whether I'm diagnosing the problem right. Are those formulas from the Audio cookbook supposed to go outside the range, and if yes, how can I deal with that? Is this a (simple) issue of normalizing, or is it something more complex (Gibbs phenomenon)? Any help is much appreciated.","I am trying to implement some simple digital filters for a software synthesizer. This link seemed like a good start, and many things reference it: http://www.musicdsp.org/files/Audio-EQ-Cookbook.txt Problem is, both the frequency response formulas (H(s) = 1 / (s^2 + s/Q + 1)) and the recursive formulas don't seem to give normalized values, and passing a signal from -1 to 1 through them sometimes results in a signal with values >4, which breaks everything else. Wikipedia has some slightly different formulas for the frequency response here: http://en.wikipedia.org/wiki/Q_factor These already make more sense to me, as this can never be over 1 for any cutoff frequency, and so scales with the cutoff frequency. At least that would be my intuition, compared to the previous one. I can't find a version of this in recursive formulas though, and I'm not even sure whether I'm diagnosing the problem right. Are those formulas from the Audio cookbook supposed to go outside the range, and if yes, how can I deal with that? Is this a (simple) issue of normalizing, or is it something more complex (Gibbs phenomenon)? Any help is much appreciated.",,"['discrete-mathematics', 'fourier-analysis', 'filters']"
41,Possible combinations of two sets of three into a set of two,Possible combinations of two sets of three into a set of two,,"I'm trying to find the mathematical term for a certain phenomenon and hopefully a more general way to solve such problems. Say there are two sets, each $= \{A, B, C\}.$ I would like to know how many different possible ways I can combine these sets into a set of two $\{...,...\}.$ The combination $\{A, B\}$ is considered the same as $\{B, A\}.$ The only way I can think to do this, is: $$ 3*3 - \binom{3}{2} = 6 $$ Basically, I am counting up the total number of possible combinations (3*3) and subtracting out the combinations I overcounted. Is there a name for this kind of a calculation? Is there anyway to describe this more mathematically, as opposed to this intuitive approach? Thanks!","I'm trying to find the mathematical term for a certain phenomenon and hopefully a more general way to solve such problems. Say there are two sets, each $= \{A, B, C\}.$ I would like to know how many different possible ways I can combine these sets into a set of two $\{...,...\}.$ The combination $\{A, B\}$ is considered the same as $\{B, A\}.$ The only way I can think to do this, is: $$ 3*3 - \binom{3}{2} = 6 $$ Basically, I am counting up the total number of possible combinations (3*3) and subtracting out the combinations I overcounted. Is there a name for this kind of a calculation? Is there anyway to describe this more mathematically, as opposed to this intuitive approach? Thanks!",,"['combinatorics', 'discrete-mathematics']"
42,Basis reduction and continued fractions,Basis reduction and continued fractions,,"While reading several articles about lattice basis reduction I am left with a few questions. For one, I came across this piece of text Let $\alpha$ and $\beta \in \mathbb{R}$. Then there are two almost the same ways to compute small values for $\alpha x + \beta y$ with not too large $x,y \in \mathbb{Z}$. 1) applying the continued fraction algorithm 2) Applying the lattice basis reduction algorithm to the lattice generated by the columns of the matrix \begin{pmatrix} 1 & 0 \\ C\alpha & C\beta \end{pmatrix}   for $C$ large enough. Why are those (for me different algorithms) in the above sense the same? And also, where is the $C$ coming from? When is it large enough? It obviously depends on something... All hints, examples or explanations are very much welcome.","While reading several articles about lattice basis reduction I am left with a few questions. For one, I came across this piece of text Let $\alpha$ and $\beta \in \mathbb{R}$. Then there are two almost the same ways to compute small values for $\alpha x + \beta y$ with not too large $x,y \in \mathbb{Z}$. 1) applying the continued fraction algorithm 2) Applying the lattice basis reduction algorithm to the lattice generated by the columns of the matrix \begin{pmatrix} 1 & 0 \\ C\alpha & C\beta \end{pmatrix}   for $C$ large enough. Why are those (for me different algorithms) in the above sense the same? And also, where is the $C$ coming from? When is it large enough? It obviously depends on something... All hints, examples or explanations are very much welcome.",,"['discrete-mathematics', 'integer-lattices']"
43,Extension theorem on acyclic relations,Extension theorem on acyclic relations,,"By Sziplrajn's Theorem, we know that every partial order $\succsim$ (i.e. reflexive, transitive and antisymmetric relation) on a nonempty set $X$ can be extended to a linear order (i.e. a complete partial order) on $X$ , where an extension of $~\succsim$ is a preorder $~\trianglerighteq$ such that for all $x,y \in X$ , $x\succsim y$ implies $x\trianglerighteq y$ . I am looking for a variant of this theorem in which the extension should simply be a preorder (reflexive and transitive), and the initial relation would only be required to be acyclic (an acyclic relation is one for which there exists no list $(x_1, x_2, \dots, x_n)$ with $x_i \in X$ for all $i\in \{1,\dots,n\}$ and $x_1\succsim x_2 \succsim \dots \succsim x_n \succsim x_1$ ) So can any acyclic relation be extended to a preorder?","By Sziplrajn's Theorem, we know that every partial order (i.e. reflexive, transitive and antisymmetric relation) on a nonempty set can be extended to a linear order (i.e. a complete partial order) on , where an extension of is a preorder such that for all , implies . I am looking for a variant of this theorem in which the extension should simply be a preorder (reflexive and transitive), and the initial relation would only be required to be acyclic (an acyclic relation is one for which there exists no list with for all and ) So can any acyclic relation be extended to a preorder?","\succsim X X ~\succsim ~\trianglerighteq x,y \in X x\succsim y x\trianglerighteq y (x_1, x_2, \dots, x_n) x_i \in X i\in \{1,\dots,n\} x_1\succsim x_2 \succsim \dots \succsim x_n \succsim x_1","['discrete-mathematics', 'relations']"
44,counting another problem,counting another problem,,"I am trying to do my homework and it seems really hard. i would like to get checked here and make sure that im on the right track. can anyone help me?? Question: A group of hundred students want to create a committee of twelve which will then select a chairman for the committee. a) In how many ways can this be accomplished? b) what if they decide to have two members serve as co-chairs?? Answer a) $P(100,12)= \dfrac{100!}{12!(100-12)!}$; Answer b) $P(100,12)+ P(12,1)+ P(12,2)$. since the order matters and repetition is not allowed, i selected 12 from 100 students. did i do it right??","I am trying to do my homework and it seems really hard. i would like to get checked here and make sure that im on the right track. can anyone help me?? Question: A group of hundred students want to create a committee of twelve which will then select a chairman for the committee. a) In how many ways can this be accomplished? b) what if they decide to have two members serve as co-chairs?? Answer a) $P(100,12)= \dfrac{100!}{12!(100-12)!}$; Answer b) $P(100,12)+ P(12,1)+ P(12,2)$. since the order matters and repetition is not allowed, i selected 12 from 100 students. did i do it right??",,"['number-theory', 'discrete-mathematics']"
45,"Determining if $R=\left\{(f,g)\mid \exists k\in\Bbb Z,\forall x\in\Bbb Z, \ f(x)g(x)\lt k\right\}$ is an equivalence",Determining if  is an equivalence,"R=\left\{(f,g)\mid \exists k\in\Bbb Z,\forall x\in\Bbb Z, \ f(x)g(x)\lt k\right\}","I need help with proving whether or not the relation $R=\left\{(f,g)\mid \exists k\in\Bbb Z,\forall x\in\Bbb Z, \ f(x)g(x)\lt k\right\}$ is an equivalence relation on $\mathbb{Z}\times\mathbb{Z}$ I understand the basic key components needed in order to determine if the relation is an equivalence relation, like reflexivity, symmetry, and transitivity but i can't seem to find a way to proof reflexivity.","I need help with proving whether or not the relation $R=\left\{(f,g)\mid \exists k\in\Bbb Z,\forall x\in\Bbb Z, \ f(x)g(x)\lt k\right\}$ is an equivalence relation on $\mathbb{Z}\times\mathbb{Z}$ I understand the basic key components needed in order to determine if the relation is an equivalence relation, like reflexivity, symmetry, and transitivity but i can't seem to find a way to proof reflexivity.",,"['discrete-mathematics', 'equivalence-relations']"
46,"Is there a tutorial that uses english to form an example of a proof, or a very simple way to show how a proof works?","Is there a tutorial that uses english to form an example of a proof, or a very simple way to show how a proof works?",,"I am in a discrete math in college and would like to understand proofs. I had to prove the fundamental theorem of calculus in Calc 1, and did horribly in Linear algebra because of proofs. How does one Understand proofs? Is there an elementary level proofs tutorial I could go through? -Thank you.","I am in a discrete math in college and would like to understand proofs. I had to prove the fundamental theorem of calculus in Calc 1, and did horribly in Linear algebra because of proofs. How does one Understand proofs? Is there an elementary level proofs tutorial I could go through? -Thank you.",,['discrete-mathematics']
47,Finding a Linear Recurrence Relation,Finding a Linear Recurrence Relation,,"A model for the number of lobsters caught per year is   based on the assumption that the number of lobsters   caught in a year is the average of the number caught in   the two previous years. a) Find a recurrence relation for $L_n$, where $L_n$ is the   number of lobsters caught in year n, under the assumption   for this model. My answer: $$L_n = \frac{1}{2}L_{n-1} + \frac{1}{2}L_{n-2}$$ b) Find $L_n$ if $100,000$ lobsters were caught in year $1$ and   $300,000$ were caught in year $2$. My Answer: The characteristic equation is $$r^2 - \frac{1}{2}r - \frac{1}{2} = 0$$ and $$\frac{1}{2}(2r+1)(r-1) = 0$$ The roots are $r=-\frac{1}{2}$ and $r=1$. The general solution is $$L_n = k_1\left(-\frac{1}{2}\right)^n + k_2.$$ Considering the initial conditions, we have: $$-\frac{1}{2}k_1 + k_2 = 100000 \quad\mbox{and}\quad \frac{1}{4}k_1 + k_2 = 300000$$ Solving this system of equations, we have $$k_1 =\frac{800000}{3}$$ $$k_2 = \frac{700000}{3}$$ and $$L_n = \frac{800000}{3}\left(-\frac{1}{2}\right)^n + \frac{700000}{3}.$$ Is this right?  Thank you!","A model for the number of lobsters caught per year is   based on the assumption that the number of lobsters   caught in a year is the average of the number caught in   the two previous years. a) Find a recurrence relation for $L_n$, where $L_n$ is the   number of lobsters caught in year n, under the assumption   for this model. My answer: $$L_n = \frac{1}{2}L_{n-1} + \frac{1}{2}L_{n-2}$$ b) Find $L_n$ if $100,000$ lobsters were caught in year $1$ and   $300,000$ were caught in year $2$. My Answer: The characteristic equation is $$r^2 - \frac{1}{2}r - \frac{1}{2} = 0$$ and $$\frac{1}{2}(2r+1)(r-1) = 0$$ The roots are $r=-\frac{1}{2}$ and $r=1$. The general solution is $$L_n = k_1\left(-\frac{1}{2}\right)^n + k_2.$$ Considering the initial conditions, we have: $$-\frac{1}{2}k_1 + k_2 = 100000 \quad\mbox{and}\quad \frac{1}{4}k_1 + k_2 = 300000$$ Solving this system of equations, we have $$k_1 =\frac{800000}{3}$$ $$k_2 = \frac{700000}{3}$$ and $$L_n = \frac{800000}{3}\left(-\frac{1}{2}\right)^n + \frac{700000}{3}.$$ Is this right?  Thank you!",,"['sequences-and-series', 'discrete-mathematics', 'recurrence-relations', 'closed-form']"
48,What is the area of a pixellated surface?,What is the area of a pixellated surface?,,"Recently after playing hours of Minecraft (and generating block volumes), a question popped up in my head: Is it possible to easily (by hand in a reasonable amount of time) determine the surface area of an arbitrary function? For instance, if you were to take the function $y=x^2$, and move it onto a graph with discrete coordinate values (with non-integer function values being rounded accordingly - something like $\text{Int}(y)=x^2$ for $[\text{Int}(a),\text{Int}(b)]$ ? Extending that further, like here would it be possible to find the volume of some equation or solid of revolution and in addition the surface area? In other words, could you calculate the area of a function using the rectangle method with midpoint approximation and equal width rectangles? If these are not possible, can one find some sort of approximation of this area/volume more accurate than that of the integral it seems to represent? Thanks.","Recently after playing hours of Minecraft (and generating block volumes), a question popped up in my head: Is it possible to easily (by hand in a reasonable amount of time) determine the surface area of an arbitrary function? For instance, if you were to take the function $y=x^2$, and move it onto a graph with discrete coordinate values (with non-integer function values being rounded accordingly - something like $\text{Int}(y)=x^2$ for $[\text{Int}(a),\text{Int}(b)]$ ? Extending that further, like here would it be possible to find the volume of some equation or solid of revolution and in addition the surface area? In other words, could you calculate the area of a function using the rectangle method with midpoint approximation and equal width rectangles? If these are not possible, can one find some sort of approximation of this area/volume more accurate than that of the integral it seems to represent? Thanks.",,"['algebraic-geometry', 'discrete-mathematics']"
49,$\sum_{x=0}^{\infty}\frac{(r+x+m-1)!}{(r+m-1)!x!}(1-p)^{x}$,,\sum_{x=0}^{\infty}\frac{(r+x+m-1)!}{(r+m-1)!x!}(1-p)^{x},How to simplify given series $$\sum_{x=0}^{\infty}\frac{(r+x+m-1)!}{(r+m-1)!x!}(1-p)^{x}$$ My solution: $y=r+m-1$ $q=1-p$ $$\sum_{x=0}^{\infty}\frac{(y+x)!}{y!x!}(1-p)^{x}$$ $1+(y+1)q+\frac{(y+2)(y+1)q^2}{2}+......$ $(1-q)^{-(y+1)}$,How to simplify given series $$\sum_{x=0}^{\infty}\frac{(r+x+m-1)!}{(r+m-1)!x!}(1-p)^{x}$$ My solution: $y=r+m-1$ $q=1-p$ $$\sum_{x=0}^{\infty}\frac{(y+x)!}{y!x!}(1-p)^{x}$$ $1+(y+1)q+\frac{(y+2)(y+1)q^2}{2}+......$ $(1-q)^{-(y+1)}$,,['discrete-mathematics']
50,How can I model multiplication of two sets of integers,How can I model multiplication of two sets of integers,,"First a disclaimer, I am a physician and nowhere near a mathematician. I am struggling with a problem in which I want to model a set of equations using a discrete system. For example I want to calculate a list of totals when a prescription frequency can be $\left \{2,4,6 \right \}$ and the amount can be $\left \{5,10,15 \right \}$. This gives a set of possible totals of $\left \{10,20,30,40,60,90 \right \}$. My first, naive, solution was to model the set like:  $$\text{Factor } n = a + bx \wedge x = \left \{ x \in \mathbb{N}: x \le (c - a) \div b \wedge (a + bx) \mid d \right \} $$ Where $a$ is the minimum value in the set, $b$ is the increment of the set, $c$ is the maximum of the set and $d$ is a dividend, of which the set is a divisor. Thus, I can describe the frequency as: $\text{Factor } n = 2 + 2x \wedge x = \left \{ x \in \mathbb{N}: x \le (6 - 2) \div 2 \wedge (2 + 2x) \mid \infty \right \} = \left \{ 2,4,6 \right \}$. But what happens when I multiply such a set definition with another set? If there is no maximum, i.e. $c = \infty$, no problem, the resulting set is just a multiple of the multiplication, i.e. $b_{result} = b_1 \times b_2$. And $a_{result} = a_1 \times a_2$ as is $b_{result}$. But, obviously, this is not correct when $c \neq \infty $, or $d \neq \infty$. There are two possible solutions, either generate the set and use that in subsequent calculations (can get messy) or come up with some sort of equation that can generate the set (the prefered strategy). What I want to accomplish is that a set of equations like: $total = frequency \times quantity$ and $quantity = runtime \times infusionrate$ can be solved, while disallowing entries that have no solution. For example if $total = 9$ and frequency is a multiple of 1 than quantity cannot be set at 5, thus the equation with the above values would set the dividend of quantity to 9, limiting quantity to divisors of 9. Any, help will be greatly appreciated, I have been struggling with this for years and there are no practical existing systems that help doctors with prescription calculations like this.","First a disclaimer, I am a physician and nowhere near a mathematician. I am struggling with a problem in which I want to model a set of equations using a discrete system. For example I want to calculate a list of totals when a prescription frequency can be $\left \{2,4,6 \right \}$ and the amount can be $\left \{5,10,15 \right \}$. This gives a set of possible totals of $\left \{10,20,30,40,60,90 \right \}$. My first, naive, solution was to model the set like:  $$\text{Factor } n = a + bx \wedge x = \left \{ x \in \mathbb{N}: x \le (c - a) \div b \wedge (a + bx) \mid d \right \} $$ Where $a$ is the minimum value in the set, $b$ is the increment of the set, $c$ is the maximum of the set and $d$ is a dividend, of which the set is a divisor. Thus, I can describe the frequency as: $\text{Factor } n = 2 + 2x \wedge x = \left \{ x \in \mathbb{N}: x \le (6 - 2) \div 2 \wedge (2 + 2x) \mid \infty \right \} = \left \{ 2,4,6 \right \}$. But what happens when I multiply such a set definition with another set? If there is no maximum, i.e. $c = \infty$, no problem, the resulting set is just a multiple of the multiplication, i.e. $b_{result} = b_1 \times b_2$. And $a_{result} = a_1 \times a_2$ as is $b_{result}$. But, obviously, this is not correct when $c \neq \infty $, or $d \neq \infty$. There are two possible solutions, either generate the set and use that in subsequent calculations (can get messy) or come up with some sort of equation that can generate the set (the prefered strategy). What I want to accomplish is that a set of equations like: $total = frequency \times quantity$ and $quantity = runtime \times infusionrate$ can be solved, while disallowing entries that have no solution. For example if $total = 9$ and frequency is a multiple of 1 than quantity cannot be set at 5, thus the equation with the above values would set the dividend of quantity to 9, limiting quantity to divisors of 9. Any, help will be greatly appreciated, I have been struggling with this for years and there are no practical existing systems that help doctors with prescription calculations like this.",,"['elementary-number-theory', 'discrete-mathematics']"
51,Proving DeMorgan's Theorem,Proving DeMorgan's Theorem,,"I'm trying to prove that (without using logical equivalencies): $\overline{A\cap B} = \bar A \cup \bar B$ by proving both sides: (1) $ x \in \overline{A\cap B} \to x \in \bar A\cup\bar B$ (2) $ x \in \bar A\cup\bar B \to x \in \overline{A\cap B}$ I figured out the 2nd part, but I'm struggling with the first. The only thing I'm confident about now is: Let $x \in \overline{A\cap B}$. We prove that $x \in \bar A\cup \bar B$. By definition of complement, $x \not\in A\cap B$. I'm not sure if I should use cases, or if I should prove by contradictions. With the other variation of DeMorgan's, I could assume $x \in A$ and $x \in B$ and they would lead to contradictions with the first assumption, but I can't do that here because it's a $\cap$ instead of a $\cup$. For reference, here's the proof I was given for the other variation of DeMorgan's: Prove: $\overline{A \cup B} = \bar A \cap \bar B$ (1) if $x \in \overline{A \cup B}$ then $x \in \bar A \cap \bar B$ (2) if $x \in \bar A \cap \bar B$ then $x \in \overline{A \cup B}$ Proof: (1)Let $x \in \overline{A \cup B}$. We prove that $x \in \bar A \cap \bar B$. By definition of complement, $x \not\in A \cup B$. Suppose, for contradiction, $x \not\in \bar A$. By definition of complement, $x \in A$, and by definition of union, $x \in A\cup B$, a contradiction. Thus, $x \in \bar A$. Now, suppose for contradiction, $x \not\in \bar B$. By definition of complement, $x \in B$, and by definition of union, $x \in A \cup B$, a contradiction. So, $x \in \bar B$. Therefore, $x \in \bar A$ and $x \in \bar B$, so by definition of intersection, $x \in \bar A \cap \bar B$. (I'm leaving out the 2nd part, as I've figured out the 2nd part in my problem above) Any ideas? I'm assuming it has to be of similar complexity.","I'm trying to prove that (without using logical equivalencies): $\overline{A\cap B} = \bar A \cup \bar B$ by proving both sides: (1) $ x \in \overline{A\cap B} \to x \in \bar A\cup\bar B$ (2) $ x \in \bar A\cup\bar B \to x \in \overline{A\cap B}$ I figured out the 2nd part, but I'm struggling with the first. The only thing I'm confident about now is: Let $x \in \overline{A\cap B}$. We prove that $x \in \bar A\cup \bar B$. By definition of complement, $x \not\in A\cap B$. I'm not sure if I should use cases, or if I should prove by contradictions. With the other variation of DeMorgan's, I could assume $x \in A$ and $x \in B$ and they would lead to contradictions with the first assumption, but I can't do that here because it's a $\cap$ instead of a $\cup$. For reference, here's the proof I was given for the other variation of DeMorgan's: Prove: $\overline{A \cup B} = \bar A \cap \bar B$ (1) if $x \in \overline{A \cup B}$ then $x \in \bar A \cap \bar B$ (2) if $x \in \bar A \cap \bar B$ then $x \in \overline{A \cup B}$ Proof: (1)Let $x \in \overline{A \cup B}$. We prove that $x \in \bar A \cap \bar B$. By definition of complement, $x \not\in A \cup B$. Suppose, for contradiction, $x \not\in \bar A$. By definition of complement, $x \in A$, and by definition of union, $x \in A\cup B$, a contradiction. Thus, $x \in \bar A$. Now, suppose for contradiction, $x \not\in \bar B$. By definition of complement, $x \in B$, and by definition of union, $x \in A \cup B$, a contradiction. So, $x \in \bar B$. Therefore, $x \in \bar A$ and $x \in \bar B$, so by definition of intersection, $x \in \bar A \cap \bar B$. (I'm leaving out the 2nd part, as I've figured out the 2nd part in my problem above) Any ideas? I'm assuming it has to be of similar complexity.",,"['logic', 'discrete-mathematics', 'proof-writing']"
52,Proof by induction for a summation?,Proof by induction for a summation?,,"Looking for some help with a proof by iduction. Im looking to proove the following summation holds true: $$\frac{\langle W \vert (C)^{N} \vert V \rangle}{\langle W \vert \vert V \rangle}=\frac{\langle W \vert (D+E)^{N} \vert V \rangle}{\langle W \vert \vert V \rangle}= \sum_{p=0}^N\frac{p(2N-1-p)!}{N!(N-p)!}\frac{\beta^{-p-1}-\alpha^{-p-1}}{\beta^{-1}-\alpha^{-1}}$$ The problem is related to the following rules: $$DE= D+E=C $$ $$D\vert V \rangle= \frac{1}{\beta} \vert V \rangle $$ $$\langle W\vert E= \frac{1}{\alpha}\langle W \vert $$ Now using the base case of N=1 yields to, $\frac{1}{\alpha}+\frac{1}{\beta}$ which I have shown but struggling with the hypothesis step. Any help would be most appreciated, many thanks.","Looking for some help with a proof by iduction. Im looking to proove the following summation holds true: $$\frac{\langle W \vert (C)^{N} \vert V \rangle}{\langle W \vert \vert V \rangle}=\frac{\langle W \vert (D+E)^{N} \vert V \rangle}{\langle W \vert \vert V \rangle}= \sum_{p=0}^N\frac{p(2N-1-p)!}{N!(N-p)!}\frac{\beta^{-p-1}-\alpha^{-p-1}}{\beta^{-1}-\alpha^{-1}}$$ The problem is related to the following rules: $$DE= D+E=C $$ $$D\vert V \rangle= \frac{1}{\beta} \vert V \rangle $$ $$\langle W\vert E= \frac{1}{\alpha}\langle W \vert $$ Now using the base case of N=1 yields to, $\frac{1}{\alpha}+\frac{1}{\beta}$ which I have shown but struggling with the hypothesis step. Any help would be most appreciated, many thanks.",,"['discrete-mathematics', 'summation']"
53,Partial Ordering and Covering Relations,Partial Ordering and Covering Relations,,"I am currently reading about partial ordering and covering relations. I just want to be certain that I am understanding these concepts correctly. A partial ordered set (poset) is just a relation on a set, right? It's used to order the elements the relation is a set on? And for the covering relation, the way the author describes seems to indicate that a covering relation is very similar to the poset, except it doesn't have transitivity? The book says, ""We say that an element $y∈S$ covers an element $x∈S$ if $x≺y$ and there is no element $z∈S$ such that $x≺z≺y$."" Or is it just saying that there isn't an element between $x$ and $y$? But wouldn't that mean there is no transitivity? Also, is the symbol $≺$ just a generalization of the different symbols used, such as $\supset$, $\supseteq$,$\le$,$<$?","I am currently reading about partial ordering and covering relations. I just want to be certain that I am understanding these concepts correctly. A partial ordered set (poset) is just a relation on a set, right? It's used to order the elements the relation is a set on? And for the covering relation, the way the author describes seems to indicate that a covering relation is very similar to the poset, except it doesn't have transitivity? The book says, ""We say that an element $y∈S$ covers an element $x∈S$ if $x≺y$ and there is no element $z∈S$ such that $x≺z≺y$."" Or is it just saying that there isn't an element between $x$ and $y$? But wouldn't that mean there is no transitivity? Also, is the symbol $≺$ just a generalization of the different symbols used, such as $\supset$, $\supseteq$,$\le$,$<$?",,"['discrete-mathematics', 'relations']"
54,"Can this series be expressed in closed form, and if so, what is it?","Can this series be expressed in closed form, and if so, what is it?",,"Can this series be expressed in closed form, and if so, what is it? $$ \sum_{n=1}^\infty\frac{1}{9^{n+1}-1} $$","Can this series be expressed in closed form, and if so, what is it? $$ \sum_{n=1}^\infty\frac{1}{9^{n+1}-1} $$",,"['sequences-and-series', 'discrete-mathematics', 'closed-form']"
55,probability involving matching of discrete shapes on a square grid,probability involving matching of discrete shapes on a square grid,,"Figure F exists on a regular square grid. T transforms F by any combination of horizontal or vertical reflection as well as rotation by 90 or 180 degrees. A larger background grid of X by Y contains noise, where each square has a 50% chance of being 0 or 1. I am trying to produce a score for the probability that F will match the background grid. For a match, F or one of its transformations (T) must be able to be compared to the background grid at some location such that the squares that are part of F match the 1s on the background grid but none of the 0s. The score does not need to be in any particular unit, but must be comparable to other scores. For example: F ###   # Background (X = 7, Y = 4) 0001010 1001101 0101100 0100110 A match occured. I have replaced the units that match with . s, they matched because they were all 1 s and formed one of Fs transformations. 0001010 100..01 0101.00 0100.10 I'm currently using (64 / 2 ^ f_squares) + (c - 1) * 2 where c is the transformational symmetry of F, f_squares is the number of squares in F and ^ is ""to the power of"". I'm looking for a more accurate approximation. 64 / 2 ^ f_squares represents the decreasing probability of a match the more squares that F contains, with (c - 1) * 2 for the additional chance of a match if Fs transformations are different from each other.","Figure F exists on a regular square grid. T transforms F by any combination of horizontal or vertical reflection as well as rotation by 90 or 180 degrees. A larger background grid of X by Y contains noise, where each square has a 50% chance of being 0 or 1. I am trying to produce a score for the probability that F will match the background grid. For a match, F or one of its transformations (T) must be able to be compared to the background grid at some location such that the squares that are part of F match the 1s on the background grid but none of the 0s. The score does not need to be in any particular unit, but must be comparable to other scores. For example: F ###   # Background (X = 7, Y = 4) 0001010 1001101 0101100 0100110 A match occured. I have replaced the units that match with . s, they matched because they were all 1 s and formed one of Fs transformations. 0001010 100..01 0101.00 0100.10 I'm currently using (64 / 2 ^ f_squares) + (c - 1) * 2 where c is the transformational symmetry of F, f_squares is the number of squares in F and ^ is ""to the power of"". I'm looking for a more accurate approximation. 64 / 2 ^ f_squares represents the decreasing probability of a match the more squares that F contains, with (c - 1) * 2 for the additional chance of a match if Fs transformations are different from each other.",,"['probability', 'geometry', 'discrete-mathematics', 'approximation']"
56,inequality with sum of powers,inequality with sum of powers,,How to prove the following inequality: $$\forall n\geqslant 4:\dfrac {3^{n}+4^{n}+\cdots +\left( n+2\right) ^{n}} {\left( n+3\right) ^{n}} < 1$$,How to prove the following inequality: $$\forall n\geqslant 4:\dfrac {3^{n}+4^{n}+\cdots +\left( n+2\right) ^{n}} {\left( n+3\right) ^{n}} < 1$$,,"['algebra-precalculus', 'inequality', 'discrete-mathematics', 'induction']"
57,Understanding and using the transfer-matrix-method,Understanding and using the transfer-matrix-method,,"Let $G = (V,E,\Phi)$ be a weighted directed graph and $\mathcal{W}' : E \rightarrow \mathbb{C}$ the weighting. Let additionally $m = \# V$, $E_m$ the $m \times m$ identity matrix. Let $v,w \in V$ be in a fixed order in $V$ so $v$ is the i -th and $w$ the j -th element of $V$. Then applies $$f_{vw}(x) = (-1)^{i+j} \det((E_m - xA)^{(j,i)}) / \det(E_m-xA).$$ Example : Let $L$ be the set of all words over the alphabet $\Sigma = \{a,b\}$ that no not contain ""bb"". The following unique finite state-machine with the start state $S = q_0$ and final states $T = \{q_0,q_1\}$ accepts exactly these language: we now use a weighting $W'(e) = 1$. The matrix of the graph is given by   $$\mathcal{A} = \left( \begin{array}{cc}  1 & 1 \\                    1 & 0  \end{array} \right)  $$   while the first row and column relate to the node $q_0$ and the second row and column to the node $q_1$. Then applies for $f_L(x) = \sum_{n \geq 0} \sum_{w \in L \atop |w| = n} x^n$ that $f_L(x) = f_{q_0q_0}(x)+f_{q_0q_1}(x)$.   Because of $$\det(E_2-Ax) = (1-x)-x^2$$  we get using the the transfer-matrix-method (see definition above) $$f_{q_0q_0}(x) = (-1)^{1+1} \det((E_2-Ax)^{(1,1)})/\det(E_2-Ax) = 1/(1-x->x^2).$$    $$f_{q_0q_1}(x) = (-1)^{1+2} \det((E_2-Ax)^{(2,1)})/\det(E_2-Ax) = (-1) \cdot (-x)/(1-x-x^2).$$    Therefore applies   $$f_L(x)=f_{q_0q_0}(x) +f_{q_0q_1}(x) = \frac{1+x}{1-x-x^2}.$$ Exercise Let $g_n$ be the amount of words of the length $n$ over the alphabet $\Sigma = \{a,b,c\}$ that do not contain $ab,ac,bc$ or $ba$. Use the transfer-matrix-method. (a) Prove that $\sum_{n\geq 0} g_n t^n = \frac{1+t}{(1-t)^2}$ (b) Identifiy an explicit formula for $g_n, n \geq 0$ Hi! Sorry for the long introduction, but I just was not sure if your definitions and conventions match with the ones I have to use. We didn't get more information about the ""transfer-matrix-method"" then I wrote above, and I still don't get it completely. I created the finite state machine for the given language as The matrix of the corresponding graph according to the example would be  $$\mathcal{A} = \left( \begin{array}{ccc}  1 & 1 & 0 \\                    1 & 1 & 1 \\ 1 & 0 & 1 \end{array} \right)  $$. $\begin{eqnarray*} f_{q_0 q_0}(x) &=& (-1)^{1+1} \det((E_m - xA)^{(1,1)}) / \det(E_m-xA) \\ &=& \det \left(\left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right) - \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right)^{(1,1)} \right) \\ && / \det \left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right) - \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right) \\ &=& \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right)^{(1,1)} \\ &&/ \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right) \\  &=& \det \left( \begin{array}{ccc} (1-x) & (-x) \\ 0 & (1-x) \end{array} \right) / \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right) \\  &=& (1-x)^2 / ((1-x)^3 + (-x)^3 - (-x)(-x)(1-x)) \\  &=& \frac{(1-x)^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_1}(x) &=& (-1)^{1+2} \det((E_m - xA)^{(2,1)}) / \det(E_m-xA) \\  &=& (-1) \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right)^{(2,1)}  / \det(E_m-xA) \\   &=& (-1) \det \left( \begin{array}{cc} (-x) & 0 \\ 0 & (1-x)\end{array}  \right)^{(2,1)}  / \det(E_m-xA) \\  &=& (-1) (-x)(1-x) / -x^3 -x^2 (1-x)+(1-x)^3 \\  &=& \frac{(1-x)x}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_2}(x) &=& (-1)^{1+3} \det((E_m - xA)^{(3,1)}) / \det(E_m-xA) \\  &=&  \det \left( \begin{array}{cc} (-x) & 0 \\ (1-x) & (-x) \end{array}  \right)  / \det(E_m-xA) \\  &=& \frac{x^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_0} + f_{q_0 q_1} + f_{q_0 q_2} &=& \frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3} \end{eqnarray*} $ So $\frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3}$ should the answer to (a), shoudn't it? Unfortunately (according to Wolfram Alpha) it isn't. Is there still anything wrong? Thanks in advance!","Let $G = (V,E,\Phi)$ be a weighted directed graph and $\mathcal{W}' : E \rightarrow \mathbb{C}$ the weighting. Let additionally $m = \# V$, $E_m$ the $m \times m$ identity matrix. Let $v,w \in V$ be in a fixed order in $V$ so $v$ is the i -th and $w$ the j -th element of $V$. Then applies $$f_{vw}(x) = (-1)^{i+j} \det((E_m - xA)^{(j,i)}) / \det(E_m-xA).$$ Example : Let $L$ be the set of all words over the alphabet $\Sigma = \{a,b\}$ that no not contain ""bb"". The following unique finite state-machine with the start state $S = q_0$ and final states $T = \{q_0,q_1\}$ accepts exactly these language: we now use a weighting $W'(e) = 1$. The matrix of the graph is given by   $$\mathcal{A} = \left( \begin{array}{cc}  1 & 1 \\                    1 & 0  \end{array} \right)  $$   while the first row and column relate to the node $q_0$ and the second row and column to the node $q_1$. Then applies for $f_L(x) = \sum_{n \geq 0} \sum_{w \in L \atop |w| = n} x^n$ that $f_L(x) = f_{q_0q_0}(x)+f_{q_0q_1}(x)$.   Because of $$\det(E_2-Ax) = (1-x)-x^2$$  we get using the the transfer-matrix-method (see definition above) $$f_{q_0q_0}(x) = (-1)^{1+1} \det((E_2-Ax)^{(1,1)})/\det(E_2-Ax) = 1/(1-x->x^2).$$    $$f_{q_0q_1}(x) = (-1)^{1+2} \det((E_2-Ax)^{(2,1)})/\det(E_2-Ax) = (-1) \cdot (-x)/(1-x-x^2).$$    Therefore applies   $$f_L(x)=f_{q_0q_0}(x) +f_{q_0q_1}(x) = \frac{1+x}{1-x-x^2}.$$ Exercise Let $g_n$ be the amount of words of the length $n$ over the alphabet $\Sigma = \{a,b,c\}$ that do not contain $ab,ac,bc$ or $ba$. Use the transfer-matrix-method. (a) Prove that $\sum_{n\geq 0} g_n t^n = \frac{1+t}{(1-t)^2}$ (b) Identifiy an explicit formula for $g_n, n \geq 0$ Hi! Sorry for the long introduction, but I just was not sure if your definitions and conventions match with the ones I have to use. We didn't get more information about the ""transfer-matrix-method"" then I wrote above, and I still don't get it completely. I created the finite state machine for the given language as The matrix of the corresponding graph according to the example would be  $$\mathcal{A} = \left( \begin{array}{ccc}  1 & 1 & 0 \\                    1 & 1 & 1 \\ 1 & 0 & 1 \end{array} \right)  $$. $\begin{eqnarray*} f_{q_0 q_0}(x) &=& (-1)^{1+1} \det((E_m - xA)^{(1,1)}) / \det(E_m-xA) \\ &=& \det \left(\left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right) - \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right)^{(1,1)} \right) \\ && / \det \left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right) - \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right) \\ &=& \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right)^{(1,1)} \\ &&/ \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right) \\  &=& \det \left( \begin{array}{ccc} (1-x) & (-x) \\ 0 & (1-x) \end{array} \right) / \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right) \\  &=& (1-x)^2 / ((1-x)^3 + (-x)^3 - (-x)(-x)(1-x)) \\  &=& \frac{(1-x)^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_1}(x) &=& (-1)^{1+2} \det((E_m - xA)^{(2,1)}) / \det(E_m-xA) \\  &=& (-1) \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}  \right)^{(2,1)}  / \det(E_m-xA) \\   &=& (-1) \det \left( \begin{array}{cc} (-x) & 0 \\ 0 & (1-x)\end{array}  \right)^{(2,1)}  / \det(E_m-xA) \\  &=& (-1) (-x)(1-x) / -x^3 -x^2 (1-x)+(1-x)^3 \\  &=& \frac{(1-x)x}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_2}(x) &=& (-1)^{1+3} \det((E_m - xA)^{(3,1)}) / \det(E_m-xA) \\  &=&  \det \left( \begin{array}{cc} (-x) & 0 \\ (1-x) & (-x) \end{array}  \right)  / \det(E_m-xA) \\  &=& \frac{x^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\  f_{q_0 q_0} + f_{q_0 q_1} + f_{q_0 q_2} &=& \frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3} \end{eqnarray*} $ So $\frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3}$ should the answer to (a), shoudn't it? Unfortunately (according to Wolfram Alpha) it isn't. Is there still anything wrong? Thanks in advance!",,"['linear-algebra', 'abstract-algebra', 'matrices', 'discrete-mathematics', 'automata']"
58,"A very simple discrete dynamical system with pebbles, take two","A very simple discrete dynamical system with pebbles, take two",,"This is a followup to A very simple discrete dynamical system with pebbles The setup: we have slots $n$ slots labeled $1, \ldots, n$ and $k$ pebbles, each of which is initially placed in some slot. Now the pebbles want to space themselves out as evenly as possible, and so they do the following. At each time step $t$, each pebble moves to the slot closest to the halfway point between its neighboring pebbles; if there is a tie, it chooses the slot to the left.  The leftmost and rightmost pebbles apply the same procedure, but because they only have one neighbor, they imagine that there are slots numbered $0$ and $n+1$ with pebbles in them. Formally, numbering the pebbles  $1, \ldots, k$ from left to right, and letting $x_i(t)$ be the slot of the $i$'th pebble at time t, we have $$ x_i(t+1) = \lfloor \frac{x_{i-1}(t)+x_{i+1}(t)}{2} \rfloor, i = 2, \ldots, k-1$$ $\lfloor \cdot \rfloor$ rounds down to the closest integer. Similarly, $$ x_1(t+1) = \lfloor (1/2) x_2(t) \rfloor, x_k(t+1) = \lfloor \frac{x_{k-1}(t) + (n+1)}{2} \rfloor.$$ My question: I feel that it is true that (i) after sufficiently many iterations (ii) for large $n$ this dynamical system will spend all of its time in states which are approximately equispaced. Can this statement be made precise? Note: My earlier question ( A very simple discrete dynamical system with pebbles ) asked about one way to make the above statement precise, which turned out not to work.","This is a followup to A very simple discrete dynamical system with pebbles The setup: we have slots $n$ slots labeled $1, \ldots, n$ and $k$ pebbles, each of which is initially placed in some slot. Now the pebbles want to space themselves out as evenly as possible, and so they do the following. At each time step $t$, each pebble moves to the slot closest to the halfway point between its neighboring pebbles; if there is a tie, it chooses the slot to the left.  The leftmost and rightmost pebbles apply the same procedure, but because they only have one neighbor, they imagine that there are slots numbered $0$ and $n+1$ with pebbles in them. Formally, numbering the pebbles  $1, \ldots, k$ from left to right, and letting $x_i(t)$ be the slot of the $i$'th pebble at time t, we have $$ x_i(t+1) = \lfloor \frac{x_{i-1}(t)+x_{i+1}(t)}{2} \rfloor, i = 2, \ldots, k-1$$ $\lfloor \cdot \rfloor$ rounds down to the closest integer. Similarly, $$ x_1(t+1) = \lfloor (1/2) x_2(t) \rfloor, x_k(t+1) = \lfloor \frac{x_{k-1}(t) + (n+1)}{2} \rfloor.$$ My question: I feel that it is true that (i) after sufficiently many iterations (ii) for large $n$ this dynamical system will spend all of its time in states which are approximately equispaced. Can this statement be made precise? Note: My earlier question ( A very simple discrete dynamical system with pebbles ) asked about one way to make the above statement precise, which turned out not to work.",,"['discrete-mathematics', 'dynamical-systems']"
59,Inequalities for sum of $k$ smallest degrees of a graph,Inequalities for sum of  smallest degrees of a graph,k,"As part of a homework assignment, I am doing a proof for a generalised variant of Karger's algorithm and am stuck at a particular step. I have proven that for a graph $G=(V,E)$ [writing $n=|V|$ ] with a minimum $k$ -cut $C$ , $$|C| \leq \sum\limits_{l=1}^{k-1} d_l$$ where $d_1\leq d_2\leq\ldots$ are the degress of $v_1,v_2,\ldots\in V$ . I need some way of estimating an upper bound on $\frac{|C|}{|E|}$ in terms of $n$ and $k$ alone. For instance in the case of minimum two-cuts (what are normally just called ""minimum cuts"") we can show $$|C| \leq d_1 \leq \frac{1}{n}\sum\limits_{l=1}^n d_l = \frac{2|E|}{n} \Rightarrow \frac{|C|}{|E|} \leq \frac{2}{n}$$ Any help would be appreciated (there is a strong possibility of an "" XY problem ""-situation here but please give it a go anyways).","As part of a homework assignment, I am doing a proof for a generalised variant of Karger's algorithm and am stuck at a particular step. I have proven that for a graph [writing ] with a minimum -cut , where are the degress of . I need some way of estimating an upper bound on in terms of and alone. For instance in the case of minimum two-cuts (what are normally just called ""minimum cuts"") we can show Any help would be appreciated (there is a strong possibility of an "" XY problem ""-situation here but please give it a go anyways).","G=(V,E) n=|V| k C |C| \leq \sum\limits_{l=1}^{k-1} d_l d_1\leq d_2\leq\ldots v_1,v_2,\ldots\in V \frac{|C|}{|E|} n k |C| \leq d_1 \leq \frac{1}{n}\sum\limits_{l=1}^n d_l = \frac{2|E|}{n} \Rightarrow \frac{|C|}{|E|} \leq \frac{2}{n}","['discrete-mathematics', 'graph-theory', 'computer-science']"
60,Proving associativity of matrix multiplication,Proving associativity of matrix multiplication,,"I'm trying to prove that matrix multiplication is associative, but seem to be making mistakes in each of my past write-ups, so hopefully someone can check over my work. Theorem. Let $A$ be $\alpha \times \beta$, $B$ be $\beta \times \gamma$, and $C$ be $\gamma \times \delta$. Prove that $(AB)C = A(BC)$. Proof. Define general entries of the matrices $A$, $B$, and $C$ by $a_{g,h}$, $b_{i,j}$, and $c_{k,m}$, respectively. Then, for the LHS: \begin{align*} & (AB)_{\alpha, \gamma} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,\gamma} \\ & \left((AB)C\right)_{\alpha, \delta} = \sum\limits_{n=1}^{\gamma} \left(AB\right)_{\alpha, n} c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \left(\sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,n} \right) c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \sum\limits_{p=1}^{\beta} \left(a_{\alpha,p} b_{p,n}\right) c_{n, \delta}. \end{align*} For the RHS:  \begin{align*} & \left(BC\right)_{\beta, \delta} = \sum\limits_{n=1}^{\gamma} b_{\beta, n} c_{n, \delta} \\ & \left(A\left(BC\right)\right)_{\alpha,\delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} (BC)_{p, \delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} \left(\sum\limits_{n=1}^{\gamma} b_{p, n} c_{n, \delta} \right) = \sum\limits_{p=1}^{\beta} \sum\limits_{n=1}^{\gamma} a_{\alpha,p} \left(b_{p, n} c_{n, \delta} \right). \end{align*} Assuming I have written these correctly, we can make two observations: first, the summands are equivalent, as multiplication is associative. Second, the order of the summations doesn't matter when we're summing a finite number of entries. Thus, $(AB)C = A(BC)$. How does this look?","I'm trying to prove that matrix multiplication is associative, but seem to be making mistakes in each of my past write-ups, so hopefully someone can check over my work. Theorem. Let $A$ be $\alpha \times \beta$, $B$ be $\beta \times \gamma$, and $C$ be $\gamma \times \delta$. Prove that $(AB)C = A(BC)$. Proof. Define general entries of the matrices $A$, $B$, and $C$ by $a_{g,h}$, $b_{i,j}$, and $c_{k,m}$, respectively. Then, for the LHS: \begin{align*} & (AB)_{\alpha, \gamma} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,\gamma} \\ & \left((AB)C\right)_{\alpha, \delta} = \sum\limits_{n=1}^{\gamma} \left(AB\right)_{\alpha, n} c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \left(\sum\limits_{p=1}^{\beta} a_{\alpha,p} b_{p,n} \right) c_{n, \delta} = \sum\limits_{n=1}^{\gamma} \sum\limits_{p=1}^{\beta} \left(a_{\alpha,p} b_{p,n}\right) c_{n, \delta}. \end{align*} For the RHS:  \begin{align*} & \left(BC\right)_{\beta, \delta} = \sum\limits_{n=1}^{\gamma} b_{\beta, n} c_{n, \delta} \\ & \left(A\left(BC\right)\right)_{\alpha,\delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} (BC)_{p, \delta} = \sum\limits_{p=1}^{\beta} a_{\alpha,p} \left(\sum\limits_{n=1}^{\gamma} b_{p, n} c_{n, \delta} \right) = \sum\limits_{p=1}^{\beta} \sum\limits_{n=1}^{\gamma} a_{\alpha,p} \left(b_{p, n} c_{n, \delta} \right). \end{align*} Assuming I have written these correctly, we can make two observations: first, the summands are equivalent, as multiplication is associative. Second, the order of the summations doesn't matter when we're summing a finite number of entries. Thus, $(AB)C = A(BC)$. How does this look?",,['linear-algebra']
61,Probability that each person will get someone else's hat or/and coat.,Probability that each person will get someone else's hat or/and coat.,,"Each of the $5$ people leaves a coat and a hat in the cloakroom. The absent-minded cloakroom attendant gives each person a random coat and a random hat. Calculate the probability of the event that each of the $5$ people gets someone else's coat or someone else's hat. To count the good cases, I thought to subtract from all possibilities those where at least one person gets both their own coat and their own hat. I used the principle of inclusion and exclusion. Initially, we assume that one person has their things, so we have $|A_i| = \binom{5}{1} \cdot 4! \cdot 4!$ , where we choose the person with $\binom{5}{1}$ , and we assign the remaining hats and coats with $4! \cdot 4!$ . For two people it is $\binom{5}{2} \cdot 3! \cdot 3!$ , etc. So my solution is $$\frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i \color{blue}{\binom{n}{i}} ((n-i)!)^2 = \frac{11844}{14400} \approx 0.82$$ And in the answers, there is $$\frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i ((n-i)!)^2 = \frac{13856}{14400} \approx 0.96$$ Why don't we want to choose the person who will get their things?","Each of the people leaves a coat and a hat in the cloakroom. The absent-minded cloakroom attendant gives each person a random coat and a random hat. Calculate the probability of the event that each of the people gets someone else's coat or someone else's hat. To count the good cases, I thought to subtract from all possibilities those where at least one person gets both their own coat and their own hat. I used the principle of inclusion and exclusion. Initially, we assume that one person has their things, so we have , where we choose the person with , and we assign the remaining hats and coats with . For two people it is , etc. So my solution is And in the answers, there is Why don't we want to choose the person who will get their things?",5 5 |A_i| = \binom{5}{1} \cdot 4! \cdot 4! \binom{5}{1} 4! \cdot 4! \binom{5}{2} \cdot 3! \cdot 3! \frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i \color{blue}{\binom{n}{i}} ((n-i)!)^2 = \frac{11844}{14400} \approx 0.82 \frac{1}{n! \cdot n!}\sum_{i=0}^n (-1)^i ((n-i)!)^2 = \frac{13856}{14400} \approx 0.96,"['probability', 'combinatorics', 'discrete-mathematics']"
62,Proving the identity $\sum_{k=1}^n {k^3} = \big(\sum_{k=1}^n k\big)^2$ without induction,Proving the identity  without induction,\sum_{k=1}^n {k^3} = \big(\sum_{k=1}^n k\big)^2,"I recently proved that $$\sum_{k=1}^n k^3 = \left(\sum_{k=1}^n k \right)^2$$ using mathematical induction.  I'm interested if there's an intuitive explanation, or even a combinatorial interpretation of this property. I would also like to see any other proofs.","I recently proved that $$\sum_{k=1}^n k^3 = \left(\sum_{k=1}^n k \right)^2$$ using mathematical induction.  I'm interested if there's an intuitive explanation, or even a combinatorial interpretation of this property. I would also like to see any other proofs.",,"['sequences-and-series', 'algebra-precalculus', 'summation', 'visualization']"
63,"Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$, $\forall n\ge 2$.","Show that there does not exist a sequence  of positive integers such that , .",(a_n)_{n\ge1} a_{n-1}\le (a_{n+1}-a_n)^2\le a_n \forall n\ge 2,"Show that there does not exist a sequence $(a_n)_{n\ge1}$ of positive integers such that $a_{n-1}\le (a_{n+1}-a_n)^2\le a_n$ , $\forall n\ge 2$ . I have found a solution of this problem here . The solution goes as follows: if exists then, we have, $a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}}$ . so, either, $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ which means, $a_n\ge 1+a_{n-1}+2\sqrt{a_{n-1}}..(1)$ . Again, $a_{n-1}+\sqrt{a_{n-1}}\ge a_n...(2)$ So, $(1),(2)$ together gives a contradiction. Or, we must have, $a_n=m^2$ for some $m$ which again leads to a contradiction. Thus, no such sequence exists My doubts are: The person suggests from the expression $$a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}}$$ we get two possibilities that are, either $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ or $a_n=m^2$ . How does that expression suggests these two possibilities? Why is $\sqrt{a_n}-\sqrt{a_{n-1}}\ge 1$ ?","Show that there does not exist a sequence of positive integers such that , . I have found a solution of this problem here . The solution goes as follows: if exists then, we have, . so, either, which means, . Again, So, together gives a contradiction. Or, we must have, for some which again leads to a contradiction. Thus, no such sequence exists My doubts are: The person suggests from the expression we get two possibilities that are, either or . How does that expression suggests these two possibilities? Why is ?","(a_n)_{n\ge1} a_{n-1}\le (a_{n+1}-a_n)^2\le a_n \forall n\ge 2 a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}} \sqrt{a_n}-\sqrt{a_{n-1}}\ge 1 a_n\ge 1+a_{n-1}+2\sqrt{a_{n-1}}..(1) a_{n-1}+\sqrt{a_{n-1}}\ge a_n...(2) (1),(2) a_n=m^2 m a_n+\sqrt{a_n}\ge a_{n+1}\ge a_n+\sqrt{a_{n-1}} \sqrt{a_n}-\sqrt{a_{n-1}}\ge 1 a_n=m^2 \sqrt{a_n}-\sqrt{a_{n-1}}\ge 1","['discrete-mathematics', 'inequality']"
64,Expectation of r.v. in the proof of crossing number inequality,Expectation of r.v. in the proof of crossing number inequality,,"I was reading the proof of crossing number inequality and there was one step in the proof which I cannot prove rigorously.  Firstly, let me remind the definition of the crossing number and then I state the lemma and its proof. I will not write the whole proof since I understood it pretty well except one moment. Definition The crossing number of a graph $G=(V,E)$ , denoted $\text{cr}(G)$ , is the smallest integer $k$ such that we can draw $G$ in the plane with $k$ edge crossings. Lemma (the crossing lemma) Let $G=(V,E)$ be a graph with $|E|\geq 4|V|$ . Then $\text{cr}(G)\geq \frac{|E|^3}{64|V|^2}.$ Proof. Consider a drawing of $G$ with $\text{cr}(G)$ crossings. Set $p=\frac{4|V|}{|E|}$ . The assumption of the lemma implies that $0<p\leq 1$ . We remove every vertex of $V$ from the drawing with probability $1-p$ (together with the edges adjacent to the vertex). More precisely, we consider the probability space $\Omega:=\{V': V'\subset V\}$ and we define probability as follows: $\mathbb{P}(V'):=p^{|V'|}(1-p)^{n-|V'|}$ , where $n=|V|$ . We consider the following three random variables: $$\xi_1:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_1(V'):=|V'|,$$ $$\xi_2:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_2(V'):=|E'|,$$ $$\xi_3:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_3(V'):=\text{cr}(G'),$$ where $|E'|$ is the number of edges in the induced subgraph $G[V']$ and $\text{cr}(G')$ is crossing number of the induced subgraph $G':=G[V']$ . It is not difficult to compute that $\mathbb{E}[\xi_1]=p|V|$ and $\mathbb{E}[\xi_2]=p^2|E|$ . I computed them by the definition of expectation. Can anyone show rigorously why $\mathbb{E}[\xi_3]\leq p^4\text{cr}(G)$ ? The rest of the proof makes sense to me. I'd be very happy if someone can show the detailed proof of this inequality. EDIT: For example, this is how I computed $\mathbb{E}[\xi_1]$ : since $\xi_1(V')=|V'|=\sum_{k=1}^{n}\mathbb{1}_{v_k}(V')$ , where $$\mathbb{1}_{x}(V')=\begin{cases} 1, & \text{if } x\in V' \\ 0, & \text{if }x\notin V' \end{cases}$$ By linearity of expectation we have $\mathbb{E}[\xi_1]=\sum_{k=1}^n \mathbb{E}[1_{v_k}]=\sum_{k=1}^n p=p|V|.$ I wonder if it can be shown that $\mathbb{E}[\xi_3]\leq p^4\text{cr}(G)$ rigorously as I did?","I was reading the proof of crossing number inequality and there was one step in the proof which I cannot prove rigorously.  Firstly, let me remind the definition of the crossing number and then I state the lemma and its proof. I will not write the whole proof since I understood it pretty well except one moment. Definition The crossing number of a graph , denoted , is the smallest integer such that we can draw in the plane with edge crossings. Lemma (the crossing lemma) Let be a graph with . Then Proof. Consider a drawing of with crossings. Set . The assumption of the lemma implies that . We remove every vertex of from the drawing with probability (together with the edges adjacent to the vertex). More precisely, we consider the probability space and we define probability as follows: , where . We consider the following three random variables: where is the number of edges in the induced subgraph and is crossing number of the induced subgraph . It is not difficult to compute that and . I computed them by the definition of expectation. Can anyone show rigorously why ? The rest of the proof makes sense to me. I'd be very happy if someone can show the detailed proof of this inequality. EDIT: For example, this is how I computed : since , where By linearity of expectation we have I wonder if it can be shown that rigorously as I did?","G=(V,E) \text{cr}(G) k G k G=(V,E) |E|\geq 4|V| \text{cr}(G)\geq \frac{|E|^3}{64|V|^2}. G \text{cr}(G) p=\frac{4|V|}{|E|} 0<p\leq 1 V 1-p \Omega:=\{V': V'\subset V\} \mathbb{P}(V'):=p^{|V'|}(1-p)^{n-|V'|} n=|V| \xi_1:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_1(V'):=|V'|, \xi_2:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_2(V'):=|E'|, \xi_3:\Omega \to \mathbb{R} \quad \text{defined as} \quad \xi_3(V'):=\text{cr}(G'), |E'| G[V'] \text{cr}(G') G':=G[V'] \mathbb{E}[\xi_1]=p|V| \mathbb{E}[\xi_2]=p^2|E| \mathbb{E}[\xi_3]\leq p^4\text{cr}(G) \mathbb{E}[\xi_1] \xi_1(V')=|V'|=\sum_{k=1}^{n}\mathbb{1}_{v_k}(V') \mathbb{1}_{x}(V')=\begin{cases}
1, & \text{if } x\in V' \\
0, & \text{if }x\notin V'
\end{cases} \mathbb{E}[\xi_1]=\sum_{k=1}^n \mathbb{E}[1_{v_k}]=\sum_{k=1}^n p=p|V|. \mathbb{E}[\xi_3]\leq p^4\text{cr}(G)","['combinatorics', 'discrete-mathematics', 'graph-theory', 'extremal-combinatorics', 'additive-combinatorics']"
65,On the number of permutations on $n$ letters with the greatest cycle length of $k$,On the number of permutations on  letters with the greatest cycle length of,n k,"I am trying to understand the theory ""On the number of permutations on $n$ objects with the greatest cycle length of $k$ "" by Solomon W Golomb and Peter Gaal. Let $L_{k,n}$ denotes the number of permutation on $n$ letters having a greatest cycle of length $k$ . I want to understand the formula $$L_{k,n}=\sum_{j=1}^{\lfloor n/k \rfloor}\frac{1}{j!k^j}\frac{n!}{(n-kj)!}\sum_{t=1}^{min(k-1,n-kj)}L_{t,n-kj} \quad \quad (1 <k\le n)$$ I am considering the case $\frac n4<k \le \frac n3$ . Then $$L_{k,n}=\frac {n!}{k} \left \{ 1-\sum_{j=1}^{n-2k}\frac{1}{k+j}+\sum_{j=1}^{n-3k-1}\sum_{l=1}^{n-3k-j} \frac{1}{2(k+j)(k+l)}        -\frac {1}{2k} \bigg[ 1-\sum_{j=1}^{n-3k}\frac{1}{k+j} -\frac{1}{3k}\bigg] \right \}$$ By the given restriction on $n$ there can be at most three cycles of lentgth $k$ I think the authors considered the following types of permutations along with different categories of greatest cycle length . $(a)$ Permutations having one cycle of lenth $k$ . $(b)$ Permutations having one cycle of lenth $k$ and two cycles of length greater than $k$ $(c)$ Permutations having one cycle of length $k$ and one cycle of length greater than $k$ $(d)$ Permutations having two cycle of length $k$ and one cycle of length greater than $k$ $(e)$ Permutations having two cycle of length $k$ $(f)$ Permutations having three cycles of lenth $k$ The number of permutations in category $(a)$ are ${n \choose k }(k-1)!(n-k)!=\frac{n!}{k}$ In category $(b)$ , let the two cycles of length greater than $k$ be of length $k+j$ and $k+l$ We want to find the bounds on $j$ and $l$ . Clearly $j,l\ge 1$ . Now, $k+(k+1)+(k+j)\le n$ $\implies j\le n-3k-1$ And , $k+(k+j)+(k+l)\le n$ $\implies l\le n-3k-j$ Then the number of permutations having a cycle of length $k$ and cycles of length $(k+j)$ and $(k+l)$ is given by $$\frac 12 {n \choose k}{n-k  \choose k+j}{n-2k-j \choose k+l}(k-1)!(k+j-1)!(k+l-1)!(n-3k-j-l)!$$ $$=\frac 12\frac{n!}{k}\frac{1}{(k+j)(k+l)}$$ Not sure why is the  factor $\frac 12$ present. I think its because exactly after $j$ covers half the way between $1$ and $(n-3k-1)$ , the pairs of values of $k+j$ and $k+l$ interchange with respect to the first half. (Generally we are dividing by $k!$ when we are selecting $k$ groups of equals size from a given number of objects (say) $n$ ) In category $(c)$ , let the cycles  of length greater than $k$ be $(k+j)$ . Then $j\ge1$ and $k+(k+j)\le n$ $\implies j\le n-2k$ So , number of permutations having a cycle of length $k$ and a cycle of length greater than $(k+j)$ is given by $${n \choose k}{n-k \choose k+j}(k-1)!(k+j-1)!(n-2k-j)!$$ $$=\frac {n!}{k}\frac {1}{k+j}$$ Similarly the other terms in the expression, I understand that the overall aim  here is to eliminate from the set of all permutations having a cycle of length $k$ , other unnecessary permutations having two cycles of length $k$ (because of overcounting) or cycles of length greater than $k$ . But I don't understand the actual interplay between the plus and minuses signs between the  different terms in the expression and how does that help in leading to the general formula. I am self studying and apologise for my seemingly confused way of writing.Thanks in advance . $UPDATE :$ Here's one idea I have got after revisiting the theory today .. Let $P(\underbrace{k,k,\ldots,k}_{t\text{-times}},k+j,k+l,...k+p) $ denote the set of permutations having some $t$ cycles of length $k$ and other cycles of length greater than $k$ Then if $\frac n4<k \le \frac n3$ , we have $L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-\left \{ |P(k,k)|-|P(k,k,k+j)|-\big [   |P(k,k,k)| \big ]   \right \}     $ So the brackets are somehow going nested starting with $P(k),P(k,k)...$ in the expression along with the alternate signs as one goes within a chain of fixed number of cycles of length $k$ and increasing the number of cycles of length greater than $k$ along the chain . OK I  try writing out the $L_{k,n}$ for the case $\frac n5<k \le \frac n4$ $$L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-|P(k,k+j,k+l,k+m)|-\left \{ |P(k,k)|-|P(k,k,k+j)|+|P(k,k,k+j,k+l)|-\big [ |P(k,k,k)|-  |P(k,k,k,k+j)| -|P(k,k,k,k)|\big ]   \right \}     $$ Which turns out to be matching with the formula provided in the linked text. So I guess my understanding is correct. Now coming to the general formula , I think the upper bound for $j=\lfloor n/k   \rfloor$ says how many maximum $k$ length cycles can be formed from the $n$ objects . The expression $$\frac{1}{j!k^j}\frac{n!}{(n-kj)!}=\frac{\left \{(k-1) !\right \}^j}{j!}{n \choose k}{{n-k} \choose k}\dots {{n-kj+k} \choose k}$$ tells how many permutations on $n$ objects have $j$ cycles of length $k$ Now , for each of the above permutations (for a fixed $j$ ), the inner sum tells us how many permutations (on the remaining $(n-kj)$ objects )are there of greatest cycle length from $1$ upto $(k-1)$ or $(n-kj)$ whichever is smaller. So the general formula gives us  all permutations having a certain no of cycles each of length $k$ along with cycles of length shorter than $k$ thus giving all permutations having greatest cycle of length $k$ Suggestions ??","I am trying to understand the theory ""On the number of permutations on objects with the greatest cycle length of "" by Solomon W Golomb and Peter Gaal. Let denotes the number of permutation on letters having a greatest cycle of length . I want to understand the formula I am considering the case . Then By the given restriction on there can be at most three cycles of lentgth I think the authors considered the following types of permutations along with different categories of greatest cycle length . Permutations having one cycle of lenth . Permutations having one cycle of lenth and two cycles of length greater than Permutations having one cycle of length and one cycle of length greater than Permutations having two cycle of length and one cycle of length greater than Permutations having two cycle of length Permutations having three cycles of lenth The number of permutations in category are In category , let the two cycles of length greater than be of length and We want to find the bounds on and . Clearly . Now, And , Then the number of permutations having a cycle of length and cycles of length and is given by Not sure why is the  factor present. I think its because exactly after covers half the way between and , the pairs of values of and interchange with respect to the first half. (Generally we are dividing by when we are selecting groups of equals size from a given number of objects (say) ) In category , let the cycles  of length greater than be . Then and So , number of permutations having a cycle of length and a cycle of length greater than is given by Similarly the other terms in the expression, I understand that the overall aim  here is to eliminate from the set of all permutations having a cycle of length , other unnecessary permutations having two cycles of length (because of overcounting) or cycles of length greater than . But I don't understand the actual interplay between the plus and minuses signs between the  different terms in the expression and how does that help in leading to the general formula. I am self studying and apologise for my seemingly confused way of writing.Thanks in advance . Here's one idea I have got after revisiting the theory today .. Let denote the set of permutations having some cycles of length and other cycles of length greater than Then if , we have So the brackets are somehow going nested starting with in the expression along with the alternate signs as one goes within a chain of fixed number of cycles of length and increasing the number of cycles of length greater than along the chain . OK I  try writing out the for the case Which turns out to be matching with the formula provided in the linked text. So I guess my understanding is correct. Now coming to the general formula , I think the upper bound for says how many maximum length cycles can be formed from the objects . The expression tells how many permutations on objects have cycles of length Now , for each of the above permutations (for a fixed ), the inner sum tells us how many permutations (on the remaining objects )are there of greatest cycle length from upto or whichever is smaller. So the general formula gives us  all permutations having a certain no of cycles each of length along with cycles of length shorter than thus giving all permutations having greatest cycle of length Suggestions ??","n k L_{k,n} n k L_{k,n}=\sum_{j=1}^{\lfloor n/k \rfloor}\frac{1}{j!k^j}\frac{n!}{(n-kj)!}\sum_{t=1}^{min(k-1,n-kj)}L_{t,n-kj} \quad \quad (1 <k\le n) \frac n4<k \le \frac n3 L_{k,n}=\frac {n!}{k} \left \{ 1-\sum_{j=1}^{n-2k}\frac{1}{k+j}+\sum_{j=1}^{n-3k-1}\sum_{l=1}^{n-3k-j} \frac{1}{2(k+j)(k+l)}        -\frac {1}{2k} \bigg[ 1-\sum_{j=1}^{n-3k}\frac{1}{k+j} -\frac{1}{3k}\bigg] \right \} n k (a) k (b) k k (c) k k (d) k k (e) k (f) k (a) {n \choose k }(k-1)!(n-k)!=\frac{n!}{k} (b) k k+j k+l j l j,l\ge 1 k+(k+1)+(k+j)\le n \implies j\le n-3k-1 k+(k+j)+(k+l)\le n \implies l\le n-3k-j k (k+j) (k+l) \frac 12 {n \choose k}{n-k  \choose k+j}{n-2k-j \choose k+l}(k-1)!(k+j-1)!(k+l-1)!(n-3k-j-l)! =\frac 12\frac{n!}{k}\frac{1}{(k+j)(k+l)} \frac 12 j 1 (n-3k-1) k+j k+l k! k n (c) k (k+j) j\ge1 k+(k+j)\le n \implies j\le n-2k k (k+j) {n \choose k}{n-k \choose k+j}(k-1)!(k+j-1)!(n-2k-j)! =\frac {n!}{k}\frac {1}{k+j} k k k UPDATE : P(\underbrace{k,k,\ldots,k}_{t\text{-times}},k+j,k+l,...k+p)  t k k \frac n4<k \le \frac n3 L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-\left \{ |P(k,k)|-|P(k,k,k+j)|-\big [   |P(k,k,k)| \big ]   \right \}      P(k),P(k,k)... k k L_{k,n} \frac n5<k \le \frac n4 L_{k,n}=|P(k)|-|P(k,k+j)|+|P(k,k+j,k+l)|-|P(k,k+j,k+l,k+m)|-\left \{ |P(k,k)|-|P(k,k,k+j)|+|P(k,k,k+j,k+l)|-\big [ |P(k,k,k)|-  |P(k,k,k,k+j)| -|P(k,k,k,k)|\big ]   \right \}      j=\lfloor n/k   \rfloor k n \frac{1}{j!k^j}\frac{n!}{(n-kj)!}=\frac{\left \{(k-1) !\right \}^j}{j!}{n \choose k}{{n-k} \choose k}\dots {{n-kj+k} \choose k} n j k j (n-kj) 1 (k-1) (n-kj) k k k","['probability', 'abstract-algebra', 'combinatorics', 'discrete-mathematics', 'permutation-cycles']"
66,Is there a connection between shoelace formula and Stokes theorem?,Is there a connection between shoelace formula and Stokes theorem?,,"The shoelace-formula is a method to calculate the area of a polygon. It is given as $$ A = 1/2 \sum_i{(x_i-x_{i+1})*(y_i+y_{i+1})} $$ for cyclical $i$ . Expanding the product yields the terms $x_i y_i - x_{i+1}y_{i+1}$ which cancel with the next and previous terms of the sum, resulting in: $$ A = 1/2 \sum_i x_i y_{i+1} - x_{i+1} y_i = 1/2 \sum_i |\vec{X_i} \times \vec{X_{i+1}}| $$ Which prompted my intuition to look for connections with Stokes' theorem but so far I got nowhere. Is this just a coincidence and Stokes' has nothing to do with this? I can see how this is basically a discrete version of Green's Theorem , but that just doesn't feel like any kind of deep understanding. And yes, I'm asking to satisfy my curiosity. The best concrete question I could think of is: Is there a field $F$ for which Stokes' theorem and the shoelace formula coincide for infinitesimally close polygon-corners? What I've got so far: The absolute value of a cross product is cumbersome and can be replaced by either a dot-product with the surface-normal or limiting the calculation to the z-axis instead. So either $|\vec{X_i} \times \vec{X_{i+1}}| = (\vec{X_i} \times \vec{X_{i+1}}) * \left({\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}}\right) $ or use $A \simeq\left({\begin{smallmatrix}0\\0\\ A\end{smallmatrix}}\right)$ . To get from a sum to an integral, an infinitesimal delta is required. The delta is hidden in the cross-product: $$ \vec{X_i}\times\vec{X_{i+1}} = \vec{X_i}\times(\vec{X_i} + \vec{\Delta_i}) = \vec{X_i}\times\vec{\Delta_i} $$ taking the limit $\vec{\Delta} \rightarrow 0$ : $$ A = \vec{n} * 1/2 \int_{\partial A} \vec { X } \times \vec{\partial A} $$ The area $A$ can easily be interpreted as a surface integral over 1: $$ \int \int_A 1 = A $$ Or, more in line with the concept of a vector-field: $$ \int \int_A \left(\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}\right) = \left(\begin{smallmatrix}0\\0\\ A\end{smallmatrix}\right) $$ All the elements are there, but my main problem is the cross product is on the wrong side. I could construct a field with unit curl everywhere to satisfy the left hand side, but then I don't know what to do on the right.","The shoelace-formula is a method to calculate the area of a polygon. It is given as for cyclical . Expanding the product yields the terms which cancel with the next and previous terms of the sum, resulting in: Which prompted my intuition to look for connections with Stokes' theorem but so far I got nowhere. Is this just a coincidence and Stokes' has nothing to do with this? I can see how this is basically a discrete version of Green's Theorem , but that just doesn't feel like any kind of deep understanding. And yes, I'm asking to satisfy my curiosity. The best concrete question I could think of is: Is there a field for which Stokes' theorem and the shoelace formula coincide for infinitesimally close polygon-corners? What I've got so far: The absolute value of a cross product is cumbersome and can be replaced by either a dot-product with the surface-normal or limiting the calculation to the z-axis instead. So either or use . To get from a sum to an integral, an infinitesimal delta is required. The delta is hidden in the cross-product: taking the limit : The area can easily be interpreted as a surface integral over 1: Or, more in line with the concept of a vector-field: All the elements are there, but my main problem is the cross product is on the wrong side. I could construct a field with unit curl everywhere to satisfy the left hand side, but then I don't know what to do on the right.","
A = 1/2 \sum_i{(x_i-x_{i+1})*(y_i+y_{i+1})}
 i x_i y_i - x_{i+1}y_{i+1} 
A = 1/2 \sum_i x_i y_{i+1} - x_{i+1} y_i = 1/2 \sum_i |\vec{X_i} \times \vec{X_{i+1}}|
 F |\vec{X_i} \times \vec{X_{i+1}}| = (\vec{X_i} \times \vec{X_{i+1}}) * \left({\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}}\right)  A \simeq\left({\begin{smallmatrix}0\\0\\ A\end{smallmatrix}}\right) 
\vec{X_i}\times\vec{X_{i+1}} = \vec{X_i}\times(\vec{X_i} + \vec{\Delta_i}) = \vec{X_i}\times\vec{\Delta_i}
 \vec{\Delta} \rightarrow 0 
A = \vec{n} * 1/2 \int_{\partial A} \vec { X } \times \vec{\partial A}
 A 
\int \int_A 1 = A
 
\int \int_A \left(\begin{smallmatrix}0\\0\\ 1\end{smallmatrix}\right) = \left(\begin{smallmatrix}0\\0\\ A\end{smallmatrix}\right)
","['integration', 'geometry', 'discrete-mathematics', 'stokes-theorem', 'greens-theorem']"
67,K-Medoid Clustering,K-Medoid Clustering,,"The question I am trying to answer is: (k-medoids clustering) What are the resulting clusters when the k-medoids algorithm is used with $k = 2$ and initial random medoids $\{(1, 2), (2, 1)\}$ on the above dataset $S$ ? The dataset $S$ : \begin{array} {|c|c|}\hline 1 & 2 \\ \hline 2 & 1 \\ \hline 1 & 3 \\ \hline 5 & 4 \\ \hline 6 & 3 \\ \hline 7 & 2 \\ \hline 6 & 1 \\ \hline  \end{array} Where each row is $x_1,x_2,x_3,x_4,x_5,x_6,x_7$ respectively. I believe I am doing the steps correctly but I do not think my answer is correct: First I calculate the distance from each medoid point to each non-medoid point: $distance(1,2)=0,1.41,1,4.47,5.10,6,5.10$ $distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4$ Therefore the resulting cluster is: $\{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\}$ Now is where I could be going wrong. I believe I am supposed to pick a new medoid with the smallest distance. Therefore I choose $x_3 = (1,3)$ . Then I calculate the distances again: $distance(1,3)=1,2.23,0,4.12,5,6.10,5.40  $ $distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4$ However I believe I am doing something wrong because I get new clusters: $\{(x_1,x_3,x_4),(x_2,x_5,x_6,x_7)\}$ . If I were to repeat the steps, I would change the point back to $x_1$ which was the medoid point originally, causing a loop. I have to be doing something wrong, any help is appreciated. I am also fairly certain the final answer should be clusters: $\{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\}$ which was the original cluster I got.","The question I am trying to answer is: (k-medoids clustering) What are the resulting clusters when the k-medoids algorithm is used with and initial random medoids on the above dataset ? The dataset : Where each row is respectively. I believe I am doing the steps correctly but I do not think my answer is correct: First I calculate the distance from each medoid point to each non-medoid point: Therefore the resulting cluster is: Now is where I could be going wrong. I believe I am supposed to pick a new medoid with the smallest distance. Therefore I choose . Then I calculate the distances again: However I believe I am doing something wrong because I get new clusters: . If I were to repeat the steps, I would change the point back to which was the medoid point originally, causing a loop. I have to be doing something wrong, any help is appreciated. I am also fairly certain the final answer should be clusters: which was the original cluster I got.","k = 2 \{(1, 2), (2, 1)\} S S \begin{array} {|c|c|}\hline 1 & 2 \\ \hline 2 & 1 \\ \hline 1 & 3 \\ \hline 5 & 4 \\ \hline 6 & 3 \\ \hline 7 & 2 \\ \hline 6 & 1 \\ \hline  \end{array} x_1,x_2,x_3,x_4,x_5,x_6,x_7 distance(1,2)=0,1.41,1,4.47,5.10,6,5.10 distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4 \{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\} x_3 = (1,3) distance(1,3)=1,2.23,0,4.12,5,6.10,5.40   distance(2,1)=1.41,0,2.24,4.24,4.47,5.10,4 \{(x_1,x_3,x_4),(x_2,x_5,x_6,x_7)\} x_1 \{(x_1,x_3),(x_2,x_4,x_5,x_6,x_7)\}","['discrete-mathematics', 'algorithms', 'clustering', 'data-structure']"
68,If $𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B)$ then $A ⊆ B$ or $B ⊆ A$. Help needed.,If  then  or . Help needed.,𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B) A ⊆ B B ⊆ A,"I was trying to prove the following: For any sets $A$ and $B$ , if $𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B)$ then $A ⊆ B$ or $B ⊆ A$ . My proof: Let $A$ and $B$ be arbitrary sets. Suppose $𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B)$ . Assume, by way of contradiction, that $(¬(A ⊆ B) ∧ ¬(B ⊆ A))$ . Then there are elements $y$ and $x$ , such that $x ∈ A ∧ x ∉ B$ and $y ∈ B ∧ y ∉ A$ . Notice $x ∈ A ∪ B$ , and $y ∈ A ∪ B$ . Let $W = \lbrace x,y \rbrace$ . Then $W ⊆ A ∪ B$ . So $W ∈ 𝒫(A ∪ B)$ . Hence $W ∈ 𝒫(A) ∪ 𝒫(B)$ . Assume $W ∈ 𝒫(A)$ . Then $W ⊆ A$ , so $x,y ∈ A$ . But $y ∉ A$ , so $W ∉ 𝒫(A)$ . Then $W ∈ 𝒫(B)$ , but then $x,y ∈ B$ , in particular $x ∈ B$ . But that's a contradiction, so $W ∉ 𝒫(b)$ . Thus, we have $W ∈ 𝒫(A) ∪ 𝒫(B)$ and $W ∉ 𝒫(A)$ and $W ∉ 𝒫(B)$ . Therefore, our assumption that $(¬(A ⊆ B) ∧ ¬(B ⊆ A))$ must be wrong. Ergo, $A ⊆ B$ or $B ⊆ A$ . I am suspicious of the step where I create a set $W = \lbrace x,y\rbrace$ . It seems a bit odd, to be able to create a set out of the blue, especially since $W$ is not arbitrary, it includes $x$ and $y$ . While I can see that $x$ and $y$ must exist, I don't see why they must be in the same set. Is this proof correct?","I was trying to prove the following: For any sets and , if then or . My proof: Let and be arbitrary sets. Suppose . Assume, by way of contradiction, that . Then there are elements and , such that and . Notice , and . Let . Then . So . Hence . Assume . Then , so . But , so . Then , but then , in particular . But that's a contradiction, so . Thus, we have and and . Therefore, our assumption that must be wrong. Ergo, or . I am suspicious of the step where I create a set . It seems a bit odd, to be able to create a set out of the blue, especially since is not arbitrary, it includes and . While I can see that and must exist, I don't see why they must be in the same set. Is this proof correct?","A B 𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B) A ⊆ B B ⊆ A A B 𝒫(A) ∪ 𝒫(B) = 𝒫(A ∪ B) (¬(A ⊆ B) ∧ ¬(B ⊆ A)) y x x ∈ A ∧ x ∉ B y ∈ B ∧ y ∉ A x ∈ A ∪ B y ∈ A ∪ B W = \lbrace x,y \rbrace W ⊆ A ∪ B W ∈ 𝒫(A ∪ B) W ∈ 𝒫(A) ∪ 𝒫(B) W ∈ 𝒫(A) W ⊆ A x,y ∈ A y ∉ A W ∉ 𝒫(A) W ∈ 𝒫(B) x,y ∈ B x ∈ B W ∉ 𝒫(b) W ∈ 𝒫(A) ∪ 𝒫(B) W ∉ 𝒫(A) W ∉ 𝒫(B) (¬(A ⊆ B) ∧ ¬(B ⊆ A)) A ⊆ B B ⊆ A W = \lbrace x,y\rbrace W x y x y","['discrete-mathematics', 'elementary-set-theory', 'solution-verification', 'proof-writing', 'proof-explanation']"
69,Binomial Coefficient through multiplication only,Binomial Coefficient through multiplication only,,"Is there any method to obtain $n \choose k$ , for all $n, k \in \mathbb{N}$ , using only products of natural numbers without using recursion on binomial coeffcients? A method that allows one to compute the following binomial coefficients through these operations, for instance: $$ {9 \choose 6} = 2 \cdot 2 \cdot 3 \cdot 7 = 84$$ $$ {10 \choose 4} = 2 \cdot 3 \cdot 5 \cdot 7 = 210$$","Is there any method to obtain , for all , using only products of natural numbers without using recursion on binomial coeffcients? A method that allows one to compute the following binomial coefficients through these operations, for instance:","n \choose k n, k \in \mathbb{N}  {9 \choose 6} = 2 \cdot 2 \cdot 3 \cdot 7 = 84  {10 \choose 4} = 2 \cdot 3 \cdot 5 \cdot 7 = 210","['combinatorics', 'discrete-mathematics', 'binomial-coefficients']"
70,How many copies of $K_3$ in $K_n$,How many copies of  in,K_3 K_n,"I don't have any idea how to do these questions. Can I have some help ? $(i)$ How many copies of $K_3$ are there in $K_n$ ? I know there are $n$ choose $3$ ways of choosing the $3$ vertices. I could then multiply this by $3!$ to order them. Not sure what to do from there. $(ii)$ Fix distinct vertices $x$ and $y$ in $K_n$ . How many copies of $K_3$ in $K_n$ contain the edge $xy$ ? $(iii)$ Fix distinct vertices $x, y$ and $z$ in $K_n$ . How many copies of $K_3$ in $K_n$ contain at least one of the edges $xy, yz$ and $xz$ ? $(iv)$ Prove that every graph with $6$ vertices and at least $10$ edges contains a copy of $K_3$",I don't have any idea how to do these questions. Can I have some help ? How many copies of are there in ? I know there are choose ways of choosing the vertices. I could then multiply this by to order them. Not sure what to do from there. Fix distinct vertices and in . How many copies of in contain the edge ? Fix distinct vertices and in . How many copies of in contain at least one of the edges and ? Prove that every graph with vertices and at least edges contains a copy of,"(i) K_3 K_n n 3 3 3! (ii) x y K_n K_3 K_n xy (iii) x, y z K_n K_3 K_n xy, yz xz (iv) 6 10 K_3",['combinatorics']
71,How do you prove ${n \choose k}$ is maximum when $k$ is $ \lceil \tfrac n2 \rceil$ or $ \lfloor \tfrac n2\rfloor $?,How do you prove  is maximum when  is  or ?,{n \choose k} k  \lceil \tfrac n2 \rceil  \lfloor \tfrac n2\rfloor ,"How do you prove $n \choose k$ is maximum when $k$ is $\lceil n/2 \rceil$ or $\lfloor n/2 \rfloor$ ? This link provides a proof of sorts but it is not satisfying. From what I understand, it focuses on product pairings present in $k! (n-k)!$ term which are of the form $i \times (i-1)$ . Since these are minimized when $i=n/2$ , we get the result. But what about the reasoning for the rest of the terms?","How do you prove is maximum when is or ? This link provides a proof of sorts but it is not satisfying. From what I understand, it focuses on product pairings present in term which are of the form . Since these are minimized when , we get the result. But what about the reasoning for the rest of the terms?",n \choose k k \lceil n/2 \rceil \lfloor n/2 \rfloor k! (n-k)! i \times (i-1) i=n/2,"['combinatorics', 'inequality', 'optimization', 'binomial-coefficients']"
72,Prove Summation Converge to $\frac{1}{e}$ or find an Upper Bound,Prove Summation Converge to  or find an Upper Bound,\frac{1}{e},"An experiment can be modeled with these two equations. Let $n$ be an even perfect square. The total number of experiments is given by $$T = {n \choose \sqrt{n}} {n-\sqrt{n} \choose \sqrt{n}},$$ and the total number of Bads events is given by $$B = \sum_{d=0}^{\sqrt{n}/2} {n/2 \choose d} {n/2 - d \choose \sqrt{n} - 2d} 2^{\sqrt{n} - 2d} {n-2(\sqrt{n}-d) \choose \sqrt{n}}.$$ I wrote a script and found out that as $n$ gets bigger, $\frac{B}{T} \sim \frac{1}{e}$ . But I don't need a tight bound on these quantities because I'm trying to prove that the probability of a good event is at least $1/2$ . Showing that $$1 - \frac{B}{T} \geq \frac{1}{2}.$$ What I have tried: As I'm not sure how (or even if it possible) to simplify theses equations I computed the following: $$\frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2}$$ finding out that for a $n \geq 36$ the first $3$ terms are ""the ones that matter"" from $u_3$ onwards they start to get really small. So maybe a way to prove the probability of a bad event would be something like: $$\frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2} \leq u_0 + u_1 + u_2 + (\sqrt{n}/2 - 3) u_3 \leq \frac{1}{2}.$$ But I'm not sure if this is the right way to go, maybe there is some clever argument involving $\frac{1}{e}$ . Any help or tips on how to prove this would be much appreciated.","An experiment can be modeled with these two equations. Let be an even perfect square. The total number of experiments is given by and the total number of Bads events is given by I wrote a script and found out that as gets bigger, . But I don't need a tight bound on these quantities because I'm trying to prove that the probability of a good event is at least . Showing that What I have tried: As I'm not sure how (or even if it possible) to simplify theses equations I computed the following: finding out that for a the first terms are ""the ones that matter"" from onwards they start to get really small. So maybe a way to prove the probability of a bad event would be something like: But I'm not sure if this is the right way to go, maybe there is some clever argument involving . Any help or tips on how to prove this would be much appreciated.","n T = {n \choose \sqrt{n}} {n-\sqrt{n} \choose \sqrt{n}}, B = \sum_{d=0}^{\sqrt{n}/2} {n/2 \choose d} {n/2 - d \choose \sqrt{n} - 2d} 2^{\sqrt{n} - 2d} {n-2(\sqrt{n}-d) \choose \sqrt{n}}. n \frac{B}{T} \sim \frac{1}{e} 1/2 1 - \frac{B}{T} \geq \frac{1}{2}. \frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2} n \geq 36 3 u_3 \frac{B}{T} = u_0 + u_1 + u_2 + ... + u_{\sqrt{n}/2} \leq u_0 + u_1 + u_2 + (\sqrt{n}/2 - 3) u_3 \leq \frac{1}{2}. \frac{1}{e}","['probability', 'combinatorics', 'discrete-mathematics', 'binomial-coefficients']"
73,Well-Ordering Principle for Non-negative Integers,Well-Ordering Principle for Non-negative Integers,,"Let $\Bbb Z_0$ be the set of all nonnegative integers and $\emptyset \ne S \subseteq \Bbb Z_0$ . Then $S$ has a least element. Definition. A set $A \subseteq \Bbb Z_0$ is called inductive if $0 \in A$ and $x \in A \implies x+1\in A$ . Attempt: Let $A=\{n \in \Bbb Z_0 \mid n\le s, \forall s \in S\}$ . Suppose for the contrary that $S$ has no least element. Since $0$ is the least element in $\Bbb Z_0$ and $S \subseteq \Bbb Z_0$ , we have $0 \le s$ for any $s \in S$ . Hence, $0 \in A$ . Now, let $x \in A$ , then $x \le s$ for any $s \in S$ . Since $S$ has no least element, we have $x<s$ for any $s \in S$ . Hence, $x+1 \le s$ for all $s \in S$ . So, $x+1 \in A$ . Thus, $A$ is an inductive set. Hence, $A=\Bbb Z_0$ . Since $S \ne \emptyset$ , we can pick $a \in S \subseteq \Bbb Z_0$ such that $a+1 \in \Bbb Z_0$ . Since $A = \Bbb Z_0$ , then $a+1 \in A$ . Therefore, $a+1 \le a<a+1$ , which is a contradiction. Thus, $S$ has a least element. I already know the Well-Ordering Principle for positive integer $\Bbb N$ . But, I don't know yet that this approach above correct. Any helps would be appreciated. Thanks in advanced.","Let be the set of all nonnegative integers and . Then has a least element. Definition. A set is called inductive if and . Attempt: Let . Suppose for the contrary that has no least element. Since is the least element in and , we have for any . Hence, . Now, let , then for any . Since has no least element, we have for any . Hence, for all . So, . Thus, is an inductive set. Hence, . Since , we can pick such that . Since , then . Therefore, , which is a contradiction. Thus, has a least element. I already know the Well-Ordering Principle for positive integer . But, I don't know yet that this approach above correct. Any helps would be appreciated. Thanks in advanced.","\Bbb Z_0 \emptyset \ne S \subseteq \Bbb Z_0 S A \subseteq \Bbb Z_0 0 \in A x \in A \implies x+1\in A A=\{n \in \Bbb Z_0 \mid n\le s, \forall s \in S\} S 0 \Bbb Z_0 S \subseteq \Bbb Z_0 0 \le s s \in S 0 \in A x \in A x \le s s \in S S x<s s \in S x+1 \le s s \in S x+1 \in A A A=\Bbb Z_0 S \ne \emptyset a \in S \subseteq \Bbb Z_0 a+1 \in \Bbb Z_0 A = \Bbb Z_0 a+1 \in A a+1 \le a<a+1 S \Bbb N",['discrete-mathematics']
74,Letter Ordering,Letter Ordering,,"I am struggling when to use combinations and permutations. I know I have seen these questions on here, but my pertain of when to correctly apply combinations or permutations. I know permutations deals with ordering, and combinations deal with grouping (putting the definitions crudely). Questions How different linear arrangements are there of the letters $A,B,C,D,E,F$ for which: a) $A$ and $B$ are next to each other? b) $A$ is before $B$ ? c) $A$ is before $B$ , and $C$ is before $D$ ? My Answers a) I cannot think of a way to use combinations correctly with this questions, but we see that $AB$ and $BA$ can be used and had $2!$ arrangements, and with the remaining spots there is $5!$ arrangements, leaving us with an answer of $5! \times 2!$ b) $\binom{6}{2}\binom{4}{1}\binom{3}{1}\binom{2}{1}\binom{1}{1}$ explaination is as follows, we choose $A$ and then $B$ from our six letters, then for each letter we place it. c) $\binom{6}{2}\binom{4}{2}\binom{2}{1}\binom{1}{1}$ explaination is as follows, we choose $A$ then $B$ , after that we choose $C$ and then $D$ , then the rest of this is placed. My Questions We used these questions in class as examples, but they were slightly modified. In class my professor likes to do most of this using combinations, but when I am doing this in the book, the author likes to use permutations. Is this a correct way to use combinations? Can the first question (a) be answered using combinations? Lastly, why should I use combinations in this case when permutation can get the job done?","I am struggling when to use combinations and permutations. I know I have seen these questions on here, but my pertain of when to correctly apply combinations or permutations. I know permutations deals with ordering, and combinations deal with grouping (putting the definitions crudely). Questions How different linear arrangements are there of the letters for which: a) and are next to each other? b) is before ? c) is before , and is before ? My Answers a) I cannot think of a way to use combinations correctly with this questions, but we see that and can be used and had arrangements, and with the remaining spots there is arrangements, leaving us with an answer of b) explaination is as follows, we choose and then from our six letters, then for each letter we place it. c) explaination is as follows, we choose then , after that we choose and then , then the rest of this is placed. My Questions We used these questions in class as examples, but they were slightly modified. In class my professor likes to do most of this using combinations, but when I am doing this in the book, the author likes to use permutations. Is this a correct way to use combinations? Can the first question (a) be answered using combinations? Lastly, why should I use combinations in this case when permutation can get the job done?","A,B,C,D,E,F A B A B A B C D AB BA 2! 5! 5! \times 2! \binom{6}{2}\binom{4}{1}\binom{3}{1}\binom{2}{1}\binom{1}{1} A B \binom{6}{2}\binom{4}{2}\binom{2}{1}\binom{1}{1} A B C D","['combinatorics', 'discrete-mathematics', 'permutations', 'combinations']"
75,"Find the generating function, $f(x)$ of the squence $a_n =\binom{-3}{n}$","Find the generating function,  of the squence",f(x) a_n =\binom{-3}{n},"As you can expect, $f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3}$ by the ""generalized binomial theorem"". But I tryied to find $f(x)$ without using that formula, Binomial thm. I took $g(x) = \sum_{n=0}^{\infty} (-x)^n = \frac{1}{x+1}$ . Since $a_n = (-1)^n(n+1)(n+2)$ , I have to do only finding $f(x) = \frac{d^2}{dx^2}(x^2g(x))$ . The reason why claim like is $ \frac{d^2}{dx^2}(x^2g(x)) = \sum_{n=0}^{\infty} (-1)^n(n+1)(n+2)x^n$ . Therefore the answer is $f(x) = \frac{2}{(1+x)^3}$ When I used the formula $f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3}$ But without using the theorem, $f(x) = \frac{2}{(1+x)^3} (\neq (1+x)^{-3})$ . What the point did I missed? I can't find my errors in my solution. Regards.","As you can expect, by the ""generalized binomial theorem"". But I tryied to find without using that formula, Binomial thm. I took . Since , I have to do only finding . The reason why claim like is . Therefore the answer is When I used the formula But without using the theorem, . What the point did I missed? I can't find my errors in my solution. Regards.",f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3} f(x) g(x) = \sum_{n=0}^{\infty} (-x)^n = \frac{1}{x+1} a_n = (-1)^n(n+1)(n+2) f(x) = \frac{d^2}{dx^2}(x^2g(x))  \frac{d^2}{dx^2}(x^2g(x)) = \sum_{n=0}^{\infty} (-1)^n(n+1)(n+2)x^n f(x) = \frac{2}{(1+x)^3} f(x) =\sum_{n=0}^{\infty} \binom{-3}{n} x^n = (1+x)^{-3} f(x) = \frac{2}{(1+x)^3} (\neq (1+x)^{-3}),"['sequences-and-series', 'discrete-mathematics', 'solution-verification', 'generating-functions']"
76,"Proof that $n! > n + 1$, $\forall n > 2$","Proof that ,",n! > n + 1 \forall n > 2,"I'm trying to understand the usage of math induction, so I came up with this exercise: Show that $n! > n + 1$ , $\forall n > 2, n\in\mathbb N$ . So I started with the base case of $n = 3$ : \begin{gather*} 3! > 3 + 1 \\ 6 > 4. \end{gather*} Considering $k! > k+1$ and $k > 2$ , now to prove $(k + 1)! > k+1+1$ : \begin{gather*} (k+1)! = (k+1)k! = kk! + k! \\ kk! + k! > (k+1)k+k+1. \end{gather*} (Changing $k!$ to $k+1$ , because of the condition above.) Since $k > 2$ can I do the following? $$(k+1)! > (0+1)1 + k + 1.$$ Then $$(k+1)! > k + 1 + 1.$$ Is this the correct way to prove the statement?","I'm trying to understand the usage of math induction, so I came up with this exercise: Show that , . So I started with the base case of : Considering and , now to prove : (Changing to , because of the condition above.) Since can I do the following? Then Is this the correct way to prove the statement?","n! > n + 1 \forall n > 2, n\in\mathbb N n = 3 \begin{gather*}
3! > 3 + 1 \\
6 > 4.
\end{gather*} k! > k+1 k > 2 (k + 1)! > k+1+1 \begin{gather*}
(k+1)! = (k+1)k! = kk! + k! \\
kk! + k! > (k+1)k+k+1.
\end{gather*} k! k+1 k > 2 (k+1)! > (0+1)1 + k + 1. (k+1)! > k + 1 + 1.","['discrete-mathematics', 'solution-verification', 'induction']"
77,Is this proof of a binary representation rigorous?,Is this proof of a binary representation rigorous?,,"From what I have seen so far, intro discrete math books usually provide a proof that every natural number can be uniquely expressed as a sum of distinct powers of $2$ in two steps: proving the (1) existence, and then (2) uniqueness. I believe that I have found a way that combines both statements, and I wonder if it may be used as a rigorous proof since it does not look ""typical."" Claim: Any natural number $N\geq1$ may be uniquely expressed as a sum $N=\sum_{k=1}^{n}2^{a_k}$ , where $a_{1}<a_{2}<...<a_{n}.$ Proof: Suppose $\exists X\subset\mathbb{N} \ \ | \ \  X=\{N\in\mathbb{N}:N\neq\sum_{k=1}^{n}2^{a_k}$ }, then, by the Well-Ordering Principle, there exists the smallest element $x\in X,$ which is the smallest counterexample. Therefore, suppose the claim is true for $x-1$ . We need to rule out the possibility of $x=1,$ since $1-1=0\ngeq1.$ Thus, Basis case: $1=2^{0}.$ Since the claim is true for $x-1$ , $x-1=2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n},\implies x=1+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}=2^0+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}.$ If $\nexists a_i=0$ for $i= \{ 1,2,...,n\},$ then the binary representation for $x$ exists and is unique. Hence, we reach a contradiction, and the claim is true. If $\exists a_i=0$ for $i= \{ 1,2,...,n\},$ then $2^{0}+2^{a_i}=2^0+2^0=2^1.$ Similarly, if $\nexists a_i=1$ for $i= \{ 1,2,...,n\},$ then the binary representation for $x$ exists and is unique. However, once again, if $\exists a_i=1$ for $i= \{ 1,2,...,n\},$ then $2^{1}+2^{a_i}=2^1+2^1=2^2.$ Continuing like that, we either stop at a particular $a_i<n$ which was not used in the expression for $x-1$ (and get a unique representation), or finally run into the case $a_i>n$ , which is obviously also unique as $a_i$ has not been used in an expression for $x-1$ . Thus, since the binary representation for $x$ exists ( $\Rightarrow \! \Leftarrow$ ), the claim is proven.","From what I have seen so far, intro discrete math books usually provide a proof that every natural number can be uniquely expressed as a sum of distinct powers of in two steps: proving the (1) existence, and then (2) uniqueness. I believe that I have found a way that combines both statements, and I wonder if it may be used as a rigorous proof since it does not look ""typical."" Claim: Any natural number may be uniquely expressed as a sum , where Proof: Suppose }, then, by the Well-Ordering Principle, there exists the smallest element which is the smallest counterexample. Therefore, suppose the claim is true for . We need to rule out the possibility of since Thus, Basis case: Since the claim is true for , If for then the binary representation for exists and is unique. Hence, we reach a contradiction, and the claim is true. If for then Similarly, if for then the binary representation for exists and is unique. However, once again, if for then Continuing like that, we either stop at a particular which was not used in the expression for (and get a unique representation), or finally run into the case , which is obviously also unique as has not been used in an expression for . Thus, since the binary representation for exists ( ), the claim is proven.","2 N\geq1 N=\sum_{k=1}^{n}2^{a_k} a_{1}<a_{2}<...<a_{n}. \exists X\subset\mathbb{N} \ \ | \ \  X=\{N\in\mathbb{N}:N\neq\sum_{k=1}^{n}2^{a_k} x\in X, x-1 x=1, 1-1=0\ngeq1. 1=2^{0}. x-1 x-1=2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n},\implies x=1+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}=2^0+2^{a_1}+2^{a_2}+2^{a_3}+...+2^{a_n}. \nexists a_i=0 i= \{ 1,2,...,n\}, x \exists a_i=0 i= \{ 1,2,...,n\}, 2^{0}+2^{a_i}=2^0+2^0=2^1. \nexists a_i=1 i= \{ 1,2,...,n\}, x \exists a_i=1 i= \{ 1,2,...,n\}, 2^{1}+2^{a_i}=2^1+2^1=2^2. a_i<n x-1 a_i>n a_i x-1 x \Rightarrow \! \Leftarrow","['discrete-mathematics', 'induction', 'examples-counterexamples']"
78,Change all $2^m$ bits to the same value with some restrictions.,Change all  bits to the same value with some restrictions.,2^m,"There are $n$ bits $A_1, A_2,\cdots,A_n$ on a row $(n \in \mathbb{N}, n \ge 2)$ , each of them is either $0$ or $1$ . For every second, the bit $A_i ~ (i=1..n)$ changes its value according to the following rule: If $A_{i-1}$ and $A_{i+1}$ has the same value with $A_i$ (i.e $A_{i-1}=A_i=A_{i+1})$ then $A_i$ is changes to $0$ , otherwise it changes to $1$ (we always assume that $A_0=A_1$ and $A_n = A_{n+1}$ at any time). Proof that if $n=2^m$ for some positive integer $m$ , there will always be a time that all the bits have value $0$ . For example with $n=4$ and the initial values of the bits are $(1,1,0,1)$ , then we have: $$(1,1,0,1) \rightarrow (0,1,1,1) \rightarrow (1,1,0,0) \rightarrow (0,1,1,0) \rightarrow (1,1,1,1) \rightarrow (0,0,0,0)$$ My attempt: We call a sequence ""good"" if it changes to all $0$ s at some point. From what I've noticed, if a sequence of $2n$ bits is good then there should be a point where it becomes a palindrome (the first palindrome that appears in the above example is $(0,1,1,0)$ ) and the problem becomes proving an $n$ bits sequence is good. Unfortunately I had no idea of what to do next.","There are bits on a row , each of them is either or . For every second, the bit changes its value according to the following rule: If and has the same value with (i.e then is changes to , otherwise it changes to (we always assume that and at any time). Proof that if for some positive integer , there will always be a time that all the bits have value . For example with and the initial values of the bits are , then we have: My attempt: We call a sequence ""good"" if it changes to all s at some point. From what I've noticed, if a sequence of bits is good then there should be a point where it becomes a palindrome (the first palindrome that appears in the above example is ) and the problem becomes proving an bits sequence is good. Unfortunately I had no idea of what to do next.","n A_1, A_2,\cdots,A_n (n \in \mathbb{N}, n \ge 2) 0 1 A_i ~ (i=1..n) A_{i-1} A_{i+1} A_i A_{i-1}=A_i=A_{i+1}) A_i 0 1 A_0=A_1 A_n = A_{n+1} n=2^m m 0 n=4 (1,1,0,1) (1,1,0,1) \rightarrow (0,1,1,1) \rightarrow (1,1,0,0) \rightarrow (0,1,1,0) \rightarrow (1,1,1,1) \rightarrow (0,0,0,0) 0 2n (0,1,1,0) n","['combinatorics', 'discrete-mathematics']"
79,number of permutations maximizing a sum,number of permutations maximizing a sum,,"Let $n$ be an odd integer greater than $1$ . Find the number of permutations $\sigma$ of the set $\{1,\cdots, n\}$ for which $|\sigma(1) - 1| + |\sigma(2) - 2|+\cdots + |\sigma(n) - n| = \frac{n^2 - 1}2$ . I found the solution below from a book, but I have some questions: Why is $\frac{n^2 - 1}2$ the maximum possible value of $\sum_{i=1}^n |\sigma(i) - i|$ ? I find it hard to prove this formally as $\sigma(i)$ is a bijection. For instance, it might not be true that $|\sigma(i) - i|\leq \frac{1}n (n^2 - 1)/2$ for all $i$ . Why must $\{\sigma((n+2)/2),\sigma((n+5)/2),\cdots, \sigma(n)\}\subset \{1,2,\cdots, (n+1)/2\}$ and why must $\{\sigma(1),\cdots, \sigma((n-1) / 2)\}\subset \{(n+1) / 2, \cdots n\}$ ? If $\sigma((n+1)/2) = k\leq (n+1)/2$ , then how can one verify that $\sum_{i=1}^{n} |\sigma(i) - i|$ indeed achieves the maximum value? I'm not sure if $\sum_{i=1}^{(n-1)/2} |\sigma(i) - i|$ and $\sum_{i=(n+3)/2}^{n} |\sigma(i) - i|$ have values only dependent on $k$ . I've also read this post but I'm still unsure how to answer my questions.","Let be an odd integer greater than . Find the number of permutations of the set for which . I found the solution below from a book, but I have some questions: Why is the maximum possible value of ? I find it hard to prove this formally as is a bijection. For instance, it might not be true that for all . Why must and why must ? If , then how can one verify that indeed achieves the maximum value? I'm not sure if and have values only dependent on . I've also read this post but I'm still unsure how to answer my questions.","n 1 \sigma \{1,\cdots, n\} |\sigma(1) - 1| + |\sigma(2) - 2|+\cdots + |\sigma(n) - n| = \frac{n^2 - 1}2 \frac{n^2 - 1}2 \sum_{i=1}^n |\sigma(i) - i| \sigma(i) |\sigma(i) - i|\leq \frac{1}n (n^2 - 1)/2 i \{\sigma((n+2)/2),\sigma((n+5)/2),\cdots, \sigma(n)\}\subset \{1,2,\cdots, (n+1)/2\} \{\sigma(1),\cdots, \sigma((n-1) / 2)\}\subset \{(n+1) / 2, \cdots n\} \sigma((n+1)/2) = k\leq (n+1)/2 \sum_{i=1}^{n} |\sigma(i) - i| \sum_{i=1}^{(n-1)/2} |\sigma(i) - i| \sum_{i=(n+3)/2}^{n} |\sigma(i) - i| k","['combinatorics', 'discrete-mathematics', 'permutations', 'contest-math']"
80,Zarankiewicz's problem,Zarankiewicz's problem,,"I was reading about Zarankiewicz's problem from the book ""Extremal combinatorics"" by Stasys Jukna. The problem sounds very interesting and I was about to understand some basics about it. Here I am attaching an excerpt from the book: Here is my question: So we have two objects: the maximal number of $1$ 's in a $0-1$ matrix of size $n\times n$ such that any $a\times a$ submatrix has at least one $0$ among its entries. Here we assume that $n\geq a\geq 2$ . For concreteness let's denote it by $m_a(n)$ . $k_a(n)=\min \mathfrak{C}$ , where $\mathfrak{C}$ is the set of all $k \in \mathbb N$ such that any bipartite graph with parts of size $n$ and more than $k$ edges contains at least one $a\times a$ clique. Question 1: 1) I guess that $\min \mathfrak{C}$ exists because $n^2-1\in \mathfrak{C}$ . It also shows very trivial estimate $k_a(n)\leq n^2-1$ . Right? Question 2: I was able to show that $m_a(n)\leq k_a(n)$ . Am I right that we cannot claim the converse inequality, i.e. $m_a(n)\geq k_a(n)$ right?","I was reading about Zarankiewicz's problem from the book ""Extremal combinatorics"" by Stasys Jukna. The problem sounds very interesting and I was about to understand some basics about it. Here I am attaching an excerpt from the book: Here is my question: So we have two objects: the maximal number of 's in a matrix of size such that any submatrix has at least one among its entries. Here we assume that . For concreteness let's denote it by . , where is the set of all such that any bipartite graph with parts of size and more than edges contains at least one clique. Question 1: 1) I guess that exists because . It also shows very trivial estimate . Right? Question 2: I was able to show that . Am I right that we cannot claim the converse inequality, i.e. right?",1 0-1 n\times n a\times a 0 n\geq a\geq 2 m_a(n) k_a(n)=\min \mathfrak{C} \mathfrak{C} k \in \mathbb N n k a\times a \min \mathfrak{C} n^2-1\in \mathfrak{C} k_a(n)\leq n^2-1 m_a(n)\leq k_a(n) m_a(n)\geq k_a(n),"['combinatorics', 'discrete-mathematics', 'graph-theory', 'extremal-combinatorics', 'extremal-graph-theory']"
81,Restricting Domain in Wolfram Alpha for Nested Quantifiers,Restricting Domain in Wolfram Alpha for Nested Quantifiers,,"Is there a way to restrict the domain to all positive integers when you're using Resolve[] on nested quantifiers in Wolfram Alpha?  If so, how?  If not but there is another tool which can do this, would one be able to point me to it? For example, say I am determining the truth of the quantified expression $\forall_n \forall_m \exists_pP(n,m,p),\quad p = (m+n)/2 \quad$ where $n,m,p\in \Bbb R$ In Wolfram Alpha I could put: Resolve[ForAll[n, ForAll[m, Exists[p, p = (m+n)/2]]]] and it correctly comes back with True as a result since there is a real number $p$ for any real values $m,n$ . Suppose though I wanted the domain to be positive integers $\Bbb Z^+$ .  Now the answer to the same problem would be False.  Is it possible to add the restriction to the domain in Wolfram Alpha or another software tool?","Is there a way to restrict the domain to all positive integers when you're using Resolve[] on nested quantifiers in Wolfram Alpha?  If so, how?  If not but there is another tool which can do this, would one be able to point me to it? For example, say I am determining the truth of the quantified expression where In Wolfram Alpha I could put: Resolve[ForAll[n, ForAll[m, Exists[p, p = (m+n)/2]]]] and it correctly comes back with True as a result since there is a real number for any real values . Suppose though I wanted the domain to be positive integers .  Now the answer to the same problem would be False.  Is it possible to add the restriction to the domain in Wolfram Alpha or another software tool?","\forall_n \forall_m \exists_pP(n,m,p),\quad p = (m+n)/2 \quad n,m,p\in \Bbb R p m,n \Bbb Z^+","['discrete-mathematics', 'predicate-logic', 'quantifiers', 'wolfram-alpha']"
82,Prove $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p$ using logical equivalence,Prove  using logical equivalence,\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p,"I am a student in a first year discrete mathematics course at university, I have found the following problem within a practice paper. Let $p$ and $q$ be statement variables, prove that $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p$ using the laws of logical equivalence. I have included my attempt below. To me, it seems correct but I have been informed that I have made an error. Could someone please be kind enough to point it out for me. $\neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) $ $\equiv \neg \; (\neg \; p \; \land \; (q \; \lor \neg \;q))$ $\equiv \neg \; (\neg \; p \; \land \; \pmb t ) $ $ \equiv \neg \; ( \; \neg \;p) $ $\equiv p$","I am a student in a first year discrete mathematics course at university, I have found the following problem within a practice paper. Let and be statement variables, prove that using the laws of logical equivalence. I have included my attempt below. To me, it seems correct but I have been informed that I have made an error. Could someone please be kind enough to point it out for me.",p q \neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q)) \equiv p \neg (\;(\;\neg \; p \;\land q \;) \lor (\; \neg \; p \; \land \neg \; q))  \equiv \neg \; (\neg \; p \; \land \; (q \; \lor \neg \;q)) \equiv \neg \; (\neg \; p \; \land \; \pmb t )   \equiv \neg \; ( \; \neg \;p)  \equiv p,"['discrete-mathematics', 'logic']"
83,How to show that $\sum_{k=1}^n k(n+1-k)=\binom{n+2}3$?,How to show that ?,\sum_{k=1}^n k(n+1-k)=\binom{n+2}3,"While thinking about another question I found out that this equality might be useful there: $$n\cdot 1 + (n-1)\cdot 2 + \dots + 2\cdot (n-1) + 1\cdot n = \frac{n(n+1)(n+2)}6$$ To rewrite it in a more compact way: $$\sum_{k=1}^n k(n+1-k)=\frac{n(n+1)(n+2)}6.$$ This equality is relatively easy to prove: $$\sum_{k=1}^n k(n+1-k)= (n+1)\sum_{k=1}^n k - \sum_{k=1}^n k^2 = (n+1) \frac{n(n+1)}2 - \frac{n(n+1)(2n+1)}6  = n(n+1) \left(\frac{n+1}2-\frac{2n+1}6\right) = n(n+1)\frac{3(n+1)-(2n+1)}6 =  \frac{n(n+1)(n+2)}6.$$ (We only used the known formulas for the sum of the first $n$ squares and the sum of the first $n$ numbers .) Are there some other nice proofs of this equality? (Induction, combinatorial arguments, visual proofs, ...) EDIT: Now I found another question which asks about the same identity: Combinatorial interpretation of a sum identity: $\sum_{k=1}^n(k-1)(n-k)=\binom{n}{3}$ (I have tried to search before posting. But the answers posted here so far gave me some new ideas for good keywords to search which lead me to finding that question.) The questions are, in my opinion, not exact duplicates since the other question asks specifically about combinatorial proofs and my question does not have that restriction. But I agree that this is a very minor distinction. In any case, if you think that one of them should be closed as a duplicate, then you can vote to close. I will refrain from voting to close/reopen on this question. (If one of the two questions is voted to be a duplicate of the other one, they probably cannot be merged, since the summation variables are off by one.)","While thinking about another question I found out that this equality might be useful there: $$n\cdot 1 + (n-1)\cdot 2 + \dots + 2\cdot (n-1) + 1\cdot n = \frac{n(n+1)(n+2)}6$$ To rewrite it in a more compact way: $$\sum_{k=1}^n k(n+1-k)=\frac{n(n+1)(n+2)}6.$$ This equality is relatively easy to prove: $$\sum_{k=1}^n k(n+1-k)= (n+1)\sum_{k=1}^n k - \sum_{k=1}^n k^2 = (n+1) \frac{n(n+1)}2 - \frac{n(n+1)(2n+1)}6  = n(n+1) \left(\frac{n+1}2-\frac{2n+1}6\right) = n(n+1)\frac{3(n+1)-(2n+1)}6 =  \frac{n(n+1)(n+2)}6.$$ (We only used the known formulas for the sum of the first $n$ squares and the sum of the first $n$ numbers .) Are there some other nice proofs of this equality? (Induction, combinatorial arguments, visual proofs, ...) EDIT: Now I found another question which asks about the same identity: Combinatorial interpretation of a sum identity: $\sum_{k=1}^n(k-1)(n-k)=\binom{n}{3}$ (I have tried to search before posting. But the answers posted here so far gave me some new ideas for good keywords to search which lead me to finding that question.) The questions are, in my opinion, not exact duplicates since the other question asks specifically about combinatorial proofs and my question does not have that restriction. But I agree that this is a very minor distinction. In any case, if you think that one of them should be closed as a duplicate, then you can vote to close. I will refrain from voting to close/reopen on this question. (If one of the two questions is voted to be a duplicate of the other one, they probably cannot be merged, since the summation variables are off by one.)",,['summation']
84,"Did I prove $\;{^{n}2} \equiv {^{n-1}2} $ $\,$ mod $n\;\;\;$ for $\;n \geq 2\;$?",Did I prove   mod  for ?,"\;{^{n}2} \equiv {^{n-1}2}  \, n\;\;\; \;n \geq 2\;","Let's first state some properties of tetrations : They are one of the basic arithmetic functions, 4. hyperoperation to be exact. $\;{^{n}2}$ is in its basic, most elemental form. So we can't really simplify it or break it down. Tetrations also aren't associative, the ""agreement"" is that we exponentiate from right-to-left. So e.g. $\; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536 $ Not e.g. $\; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256 $ I stated these properties, because when I first started dealing with this problem, I had no idea that a thing like tetrations exists and did a lot of unnecessary work and faulty adjustments. The idea that helped me, hopefully, prove it came with the use of modular exponentiation. Modular exponentiation $a^b\;$ mod $c \; \equiv \; ((a \;$ mod $c)^b)\;$ mod $c$ Often used to calculate mod for large values of $b$ , which is exactly what I had to deal with. The use of it, illustrated by an example: For $n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536}$ Mod for ${^{n}2}$ : $\;{2^{2^{16}}}$ mod $5=\;{4^{2^{15}}}$ mod $5=\;{16^{2^{14}}}$ mod $5=\;{1^{2^{14}}}$ mod $5=\;1$ Mod for ${^{n-1}2}$ : $\;{2^{2^{4}}}$ mod $5=\;{4^{2^{3}}}$ mod $5=\;{16^{2^{2}}}$ mod $5=\;{1^{2^{2}}}$ mod $5=\;1$ So generally: $\;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\;$ ...and so on, until the basis is big enough to use mod Now from what I can tell, there are two or three scenarios depending on how you look at it. Stable and decisive results, such as $1$ or $0$ where, the function ends E.g. $n = 5\;$ so ${^{5}2}$ mod $5=1\;$ and $n = 8\;$ so ${^{8}2}$ mod $8=0$ Stable results, where if you square it and then subtract the mod, you'll land again on the same number E.g. $n = 6\;$ so ${^{6}2}$ mod $6=4\;$ and E.g. $n = 10\;$ so ${^{10}2}$ mod $10=6\;$ Looping/recurring results, that again will trap the function E.g. $n = 7\;$ so ${^{7}2}$ mod $7= 2\; or \;4\;$ and $n = 11\;$ so ${^{11}2}$ mod $11= 4\; or\; 5\; or\; 3\; or \;9$ From the three, the only one that could cause trouble, would be the looping one. The worry could be that the result will land on a different number for $\;{^{n}2}\;$ and $\;{^{n-1}2}$ But $\;{^{n}2}\;$ goes through the same proces as $\;{^{n-1}2}$ just twice. $\;{^{n-1}2}$ will finish first, but the result won't be changed while $\;{^{n}2}\;$ does the ""x more laps"" because it will be out of exponents to get it into the loop again and since the $\;{^{n}2}\;$ has exactly x times the exponents it will finish on the same result out of the loop. This bit was a bit informal, but I'm not sure how to write it down properly. Let's for take $n=5\;$ again For $\;{^{n}2}\; = {2^{2^{16}}}=2^{65536}$ For $\;{^{n-1}2}\; = {2^{2^{4}}} =2^{16} $ And since $\;65536 = 2^{16}\;$ is of course divisible by $\;16 = 2^{4}\;$ it makes sense, that they'll meet at the same result. Does it make sense ? Are there any gaps, that I forgot to cover ? Did I actually prove it ? I'm well aware that, my method and my record of it is far from optimal. If you have any suggestions on how to write it more formally and correctly, please tell me. Thanks a lot, any sort of advice and help is appreciated. Czech version ( needed it for my homework sorry ): Dokaž $\;{^{n}2} \equiv {^{n-1}2} $ $\,$ mod $n\;\;\;$ for $\;n \geq 2\;$ Jde o Tetratace Důležité vlastnosti tetratace: Jde o základní aritmetickou operaci. Konkrétně 4. Hyperoperaci. $\;{^{n}2}$ je poměrně jednoduchá operace co se tetrací týče. Tedy je ve své základní, nejvíce elementární formě. Takže jí upravit, zjednodušit už moc nemůžeme. Tetratace nejsou asociativní. Záleží z jakého směru exponujeme, jako standart se bere z prava do leve, nebo-li zhora dolů. Tedy e.g. $\; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536 $ Ne e.g. $\; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256 $ Tyto vlastnosti jsem uvedl, protože když jsem tento problém začal řešit, tak jsem nevěděl, že tetratace existují a kvůli neznalosti jejich vlastností jsem si nadělal spoustu zbytečné práce a nadělal spoustu chybných úprav ve snaze tvar zjednodušit. Myšlenka která mi pomohla problém dokázat, byla skrytá v Modulárním umocňování Modulárním umocňování $a^b\;$ mod $c \; \equiv \; ((a \;$ mod $c)^b)\;$ mod $c$ Často používané na počítání modula pro velké čísla, exponenty $b$ , což je přesně případ se kterým se potýkáme u této úlohy. Její použití illustruji na případu: Pro $n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536}$ Mod pro ${^{n}2}$ : $\;{2^{2^{16}}}$ mod $5=\;{4^{2^{15}}}$ mod $5=\;{16^{2^{14}}}$ mod $5=\;{1^{2^{14}}}$ mod $5=\;1$ Mod pro ${^{n-1}2}$ : $\;{2^{2^{4}}}$ mod $5=\;{4^{2^{3}}}$ mod $5=\;{16^{2^{2}}}$ mod $5=\;{1^{2^{2}}}$ mod $5=\;1$ Takže obecně: $\;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\;$ ...a takhle dál, dokud základ není dostatečně velký na to aby jsme použili modulo. Výsledky $\;{^{n}2}$ mod $n\;$ různých $\;n\;$ můžeme rozdělit do dvou či tří kategorii. Stabilní a konečné výsledky, tedy $1$ nebo $0$ kde, funkce skončí E.g. $n = 5\;$ takže ${^{5}2}$ mod $5=1\;$ a $n = 8\;$ takže ${^{8}2}$ mod $8=0$ Stabilní výsledky, které po mocnění dvojkou a odečtení modula, skončí zpět na čísle které jsme umocňovali, tedy funkci uvězní v smyčce o jedné otáčce. E.g. $n = 6\;$ takže ${^{6}2}$ mod $6=4\;$ a E.g. $n = 10\;$ takže ${^{10}2}$ mod $10=6\;$ Opakující se výsledky, tedy smyčka o více otáčkách, které funkci také uvězní. E.g. $n = 7\;$ takže ${^{7}2}$ mod $7= 2\; nebo \;4\;$ a $n = 11\;$ takže ${^{11}2}$ mod $11= 4\; nebo\; 5\; nebo\; 3\; nebo \;9$ Z těchto tří kategorií výsledků, jediná která by mohla tvrzení vyvrátit, jsou opakující se výsledky. Zdá se, že by problém mohl nastat kdyby funkce $\;{^{n}2}\;$ a $\;{^{n-1}2}$ skončily na různých číslech smyčky. Ale stačí si uvědomit, že $\;{^{n}2}\;$ prochází stejným procesem jako $\;{^{n-1}2}$ jen vícekrát. $\;{^{n-1}2}$ sice skončí první, ale její výsledek se nezmění už nezmění a počká, než $\;{^{n}2}\;$ také doběhne. Použil jsem analogii s okruhy, jako na běžecké trati, obě funkce mají stejný cíl, v oku matematiky tedy i start. Funkce $\;{^{n}2}\;$ jen běží více okruhů a to přesně $x$ krát. Má více exponentů na to aby proběhla víckrát. Uznávám, že tento segment byl poměrně dost neformální, ale pro mě to byla užitečná vizualizace. Také jsem si nebyl jistý jak to formálně zapsat. Pro přesnost ukáži na příkladě: Vezmeme si $n=5\;$ Pro $\;{^{n}2}\; = {2^{2^{16}}}=2^{65536}$ Pro $\;{^{n-1}2}\; = {2^{2^{4}}} =2^{16} $ A protože $\;65536 = 2^{16}\;$ je jasně dělitelné $\;16 = 2^{4}\;$ dává smysl, že dopadnou na stejný výsledek. $\;{^{n}2}\;$ akorát udělá $\;65536 \, : \, 16 \,=\, 4096 \,= \,2^{12}\; $ více kol. FJFI DIM1","Let's first state some properties of tetrations : They are one of the basic arithmetic functions, 4. hyperoperation to be exact. is in its basic, most elemental form. So we can't really simplify it or break it down. Tetrations also aren't associative, the ""agreement"" is that we exponentiate from right-to-left. So e.g. Not e.g. I stated these properties, because when I first started dealing with this problem, I had no idea that a thing like tetrations exists and did a lot of unnecessary work and faulty adjustments. The idea that helped me, hopefully, prove it came with the use of modular exponentiation. Modular exponentiation mod mod mod Often used to calculate mod for large values of , which is exactly what I had to deal with. The use of it, illustrated by an example: For Mod for : mod mod mod mod Mod for : mod mod mod mod So generally: ...and so on, until the basis is big enough to use mod Now from what I can tell, there are two or three scenarios depending on how you look at it. Stable and decisive results, such as or where, the function ends E.g. so mod and so mod Stable results, where if you square it and then subtract the mod, you'll land again on the same number E.g. so mod and E.g. so mod Looping/recurring results, that again will trap the function E.g. so mod and so mod From the three, the only one that could cause trouble, would be the looping one. The worry could be that the result will land on a different number for and But goes through the same proces as just twice. will finish first, but the result won't be changed while does the ""x more laps"" because it will be out of exponents to get it into the loop again and since the has exactly x times the exponents it will finish on the same result out of the loop. This bit was a bit informal, but I'm not sure how to write it down properly. Let's for take again For For And since is of course divisible by it makes sense, that they'll meet at the same result. Does it make sense ? Are there any gaps, that I forgot to cover ? Did I actually prove it ? I'm well aware that, my method and my record of it is far from optimal. If you have any suggestions on how to write it more formally and correctly, please tell me. Thanks a lot, any sort of advice and help is appreciated. Czech version ( needed it for my homework sorry ): Dokaž mod for Jde o Tetratace Důležité vlastnosti tetratace: Jde o základní aritmetickou operaci. Konkrétně 4. Hyperoperaci. je poměrně jednoduchá operace co se tetrací týče. Tedy je ve své základní, nejvíce elementární formě. Takže jí upravit, zjednodušit už moc nemůžeme. Tetratace nejsou asociativní. Záleží z jakého směru exponujeme, jako standart se bere z prava do leve, nebo-li zhora dolů. Tedy e.g. Ne e.g. Tyto vlastnosti jsem uvedl, protože když jsem tento problém začal řešit, tak jsem nevěděl, že tetratace existují a kvůli neznalosti jejich vlastností jsem si nadělal spoustu zbytečné práce a nadělal spoustu chybných úprav ve snaze tvar zjednodušit. Myšlenka která mi pomohla problém dokázat, byla skrytá v Modulárním umocňování Modulárním umocňování mod mod mod Často používané na počítání modula pro velké čísla, exponenty , což je přesně případ se kterým se potýkáme u této úlohy. Její použití illustruji na případu: Pro Mod pro : mod mod mod mod Mod pro : mod mod mod mod Takže obecně: ...a takhle dál, dokud základ není dostatečně velký na to aby jsme použili modulo. Výsledky mod různých můžeme rozdělit do dvou či tří kategorii. Stabilní a konečné výsledky, tedy nebo kde, funkce skončí E.g. takže mod a takže mod Stabilní výsledky, které po mocnění dvojkou a odečtení modula, skončí zpět na čísle které jsme umocňovali, tedy funkci uvězní v smyčce o jedné otáčce. E.g. takže mod a E.g. takže mod Opakující se výsledky, tedy smyčka o více otáčkách, které funkci také uvězní. E.g. takže mod a takže mod Z těchto tří kategorií výsledků, jediná která by mohla tvrzení vyvrátit, jsou opakující se výsledky. Zdá se, že by problém mohl nastat kdyby funkce a skončily na různých číslech smyčky. Ale stačí si uvědomit, že prochází stejným procesem jako jen vícekrát. sice skončí první, ale její výsledek se nezmění už nezmění a počká, než také doběhne. Použil jsem analogii s okruhy, jako na běžecké trati, obě funkce mají stejný cíl, v oku matematiky tedy i start. Funkce jen běží více okruhů a to přesně krát. Má více exponentů na to aby proběhla víckrát. Uznávám, že tento segment byl poměrně dost neformální, ale pro mě to byla užitečná vizualizace. Také jsem si nebyl jistý jak to formálně zapsat. Pro přesnost ukáži na příkladě: Vezmeme si Pro Pro A protože je jasně dělitelné dává smysl, že dopadnou na stejný výsledek. akorát udělá více kol. FJFI DIM1","\;{^{n}2} \; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536  \; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256  a^b\; c \; \equiv \; ((a \; c)^b)\; c b n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536} {^{n}2} \;{2^{2^{16}}} 5=\;{4^{2^{15}}} 5=\;{16^{2^{14}}} 5=\;{1^{2^{14}}} 5=\;1 {^{n-1}2} \;{2^{2^{4}}} 5=\;{4^{2^{3}}} 5=\;{16^{2^{2}}} 5=\;{1^{2^{2}}} 5=\;1 \;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\; 1 0 n = 5\; {^{5}2} 5=1\; n = 8\; {^{8}2} 8=0 n = 6\; {^{6}2} 6=4\; n = 10\; {^{10}2} 10=6\; n = 7\; {^{7}2} 7= 2\; or \;4\; n = 11\; {^{11}2} 11= 4\; or\; 5\; or\; 3\; or \;9 \;{^{n}2}\; \;{^{n-1}2} \;{^{n}2}\; \;{^{n-1}2} \;{^{n-1}2} \;{^{n}2}\; \;{^{n}2}\; n=5\; \;{^{n}2}\; = {2^{2^{16}}}=2^{65536} \;{^{n-1}2}\; = {2^{2^{4}}} =2^{16}  \;65536 = 2^{16}\; \;16 = 2^{4}\; \;{^{n}2} \equiv {^{n-1}2}  \, n\;\;\; \;n \geq 2\; \;{^{n}2} \; 2^{2^{2^{2}}} = 2^{(2^{(2^{2})})}=2^{(2^{4})}=2^{(16)}=65,536  \; 2^{2^{2^{2}}} = ((2^{2)^{2)^{2}}}=(4^{2)^{2}}=16^{2}=256  a^b\; c \; \equiv \; ((a \; c)^b)\; c b n=5 : \; {^{n}2} ={2^{2^{16}}}=2^{65536} {^{n}2} \;{2^{2^{16}}} 5=\;{4^{2^{15}}} 5=\;{16^{2^{14}}} 5=\;{1^{2^{14}}} 5=\;1 {^{n-1}2} \;{2^{2^{4}}} 5=\;{4^{2^{3}}} 5=\;{16^{2^{2}}} 5=\;{1^{2^{2}}} 5=\;1 \;{2^{2^{n}}} =\;{4^{2^{n-1}}}=\;{16^{2^{n-2}}}\; \;{^{n}2} n\; \;n\; 1 0 n = 5\; {^{5}2} 5=1\; n = 8\; {^{8}2} 8=0 n = 6\; {^{6}2} 6=4\; n = 10\; {^{10}2} 10=6\; n = 7\; {^{7}2} 7= 2\; nebo \;4\; n = 11\; {^{11}2} 11= 4\; nebo\; 5\; nebo\; 3\; nebo \;9 \;{^{n}2}\; \;{^{n-1}2} \;{^{n}2}\; \;{^{n-1}2} \;{^{n-1}2} \;{^{n}2}\; \;{^{n}2}\; x n=5\; \;{^{n}2}\; = {2^{2^{16}}}=2^{65536} \;{^{n-1}2}\; = {2^{2^{4}}} =2^{16}  \;65536 = 2^{16}\; \;16 = 2^{4}\; \;{^{n}2}\; \;65536 \, : \, 16 \,=\, 4096 \,= \,2^{12}\; ","['elementary-number-theory', 'discrete-mathematics', 'solution-verification', 'modular-arithmetic', 'tetration']"
85,Proof of discrete Hodge decomposition,Proof of discrete Hodge decomposition,,"In this survey by Lubotzky , he has the following: Proposition 2.1 (Hodge decomposition): The following are true: $C^i=B^i\oplus\mathcal{H}^i\oplus\mathcal{B}_i$ , $\mathcal{H}^i\cong H^i(X;\mathbb{R})$ , and $\Delta_i^\wedge(B^i\oplus\mathcal{H}^i)=0$ . I'll make all the definitions clear momentarily, but to avoid getting bogged down, first I'll ask my questions: Why is this true? In particular I've thought a bit about the third one and don't see how it could possibly be true on $\mathcal{H}^i$ (will elaborate below). I don't really see how any part is as elementary as is claimed in the survey though. He traces the result back to this paper of Eckmann , which I can't seem to find in English. Does there exist a translation or a later document recapping its proofs? More generally, the survey doesn't give many proofs (which makes sense, as it's a survey). Is there a treatment of the topic which does provide more proofs? Thanks in advance for any thoughts. Now, here are all the definitions. $X$ is a finite $d$ -dimensional simplicial complex. We study real (co)homology, so the $i$ th cochain group $C^i$ is the $\mathbb{R}$ -space of $\mathbb{R}$ -valued functions on $X^i$ (the set of pure $i$ -faces). The coboundary map is defined on pure faces $F\in X^{i+1}$ and extends linearly: $$(\delta_if)(F):=\sum\limits_{G\in X^i}[F:G]f(G)$$ where $[F:G]$ is $-1$ if $G$ comes from $F$ by removing an odd-indexed entry, $1$ if it arises by removing an even-indexed entry, and $0$ otherwise. Put $B^i:=\mathrm{im\,}\delta_{i-1}$ . Cohomology is defined in the standard way here (kernel mod image). We define $\deg F:=\#\{G\in X^d:F\subseteq G\}$ , so that we have the adjoint map generated by $$(\delta_i^*f)(G)=\frac{1}{\deg G}\sum\limits_{F\in X^{i+1}}[F:G]\deg F \cdot f(F)$$ and the inner product $$(f,g):=\sum\limits_{F\in X^i}\deg F\cdot f(F)g(F).$$ Put \begin{align*} \Delta_i^\wedge&:=\delta_i^*\circ\delta_i, \\ \Delta_i^\vee&:=\delta_{i-1}\circ\delta_{i-1}^*, \\ \Delta_i&:=\Delta_i^\wedge+\Delta_i^\vee.  \end{align*} We put $\mathcal{H}^i:=\ker\Delta_i$ and $\mathcal{B}_i:=\mathrm{im\,}\delta_i^*$ . Thinking just about the third claim in the Proposition, it's clear to me from the definitions why $\Delta_i^\wedge(B^i)=0$ . However, $\Delta_i^\wedge(\mathcal{H}^i)=0$ doesn't make much sense: $\mathcal{H}^i$ is precisely those $f$ for which $\Delta_i^\vee f=-\Delta_i^\wedge f$ . But if the RHS here is $0$ (as the Proposition seems to claim) then this would mean that individually, $\Delta_i^\vee$ and $\Delta_i^\wedge$ vanish on their sum's kernel. This seems like way too strong a statement. EDIT: This paper by Parzanchevski and Rosenthal also mentions the decomposition (page 10) and chalks it up to just linear algebra (and also clarifies that $\mathcal{H}^i$ is precisely the intersection of the two kernels). I guess what I'm looking for then is how to get started on thinking about the linear algebraic computations needed to show the results. EDIT: This paper by Parzanchevski, Rosenthal, and Tessler seems to have most of the details so I think my questions should be mainly resolved.","In this survey by Lubotzky , he has the following: Proposition 2.1 (Hodge decomposition): The following are true: , , and . I'll make all the definitions clear momentarily, but to avoid getting bogged down, first I'll ask my questions: Why is this true? In particular I've thought a bit about the third one and don't see how it could possibly be true on (will elaborate below). I don't really see how any part is as elementary as is claimed in the survey though. He traces the result back to this paper of Eckmann , which I can't seem to find in English. Does there exist a translation or a later document recapping its proofs? More generally, the survey doesn't give many proofs (which makes sense, as it's a survey). Is there a treatment of the topic which does provide more proofs? Thanks in advance for any thoughts. Now, here are all the definitions. is a finite -dimensional simplicial complex. We study real (co)homology, so the th cochain group is the -space of -valued functions on (the set of pure -faces). The coboundary map is defined on pure faces and extends linearly: where is if comes from by removing an odd-indexed entry, if it arises by removing an even-indexed entry, and otherwise. Put . Cohomology is defined in the standard way here (kernel mod image). We define , so that we have the adjoint map generated by and the inner product Put We put and . Thinking just about the third claim in the Proposition, it's clear to me from the definitions why . However, doesn't make much sense: is precisely those for which . But if the RHS here is (as the Proposition seems to claim) then this would mean that individually, and vanish on their sum's kernel. This seems like way too strong a statement. EDIT: This paper by Parzanchevski and Rosenthal also mentions the decomposition (page 10) and chalks it up to just linear algebra (and also clarifies that is precisely the intersection of the two kernels). I guess what I'm looking for then is how to get started on thinking about the linear algebraic computations needed to show the results. EDIT: This paper by Parzanchevski, Rosenthal, and Tessler seems to have most of the details so I think my questions should be mainly resolved.","C^i=B^i\oplus\mathcal{H}^i\oplus\mathcal{B}_i \mathcal{H}^i\cong H^i(X;\mathbb{R}) \Delta_i^\wedge(B^i\oplus\mathcal{H}^i)=0 \mathcal{H}^i X d i C^i \mathbb{R} \mathbb{R} X^i i F\in X^{i+1} (\delta_if)(F):=\sum\limits_{G\in X^i}[F:G]f(G) [F:G] -1 G F 1 0 B^i:=\mathrm{im\,}\delta_{i-1} \deg F:=\#\{G\in X^d:F\subseteq G\} (\delta_i^*f)(G)=\frac{1}{\deg G}\sum\limits_{F\in X^{i+1}}[F:G]\deg F \cdot f(F) (f,g):=\sum\limits_{F\in X^i}\deg F\cdot f(F)g(F). \begin{align*}
\Delta_i^\wedge&:=\delta_i^*\circ\delta_i, \\
\Delta_i^\vee&:=\delta_{i-1}\circ\delta_{i-1}^*, \\
\Delta_i&:=\Delta_i^\wedge+\Delta_i^\vee. 
\end{align*} \mathcal{H}^i:=\ker\Delta_i \mathcal{B}_i:=\mathrm{im\,}\delta_i^* \Delta_i^\wedge(B^i)=0 \Delta_i^\wedge(\mathcal{H}^i)=0 \mathcal{H}^i f \Delta_i^\vee f=-\Delta_i^\wedge f 0 \Delta_i^\vee \Delta_i^\wedge \mathcal{H}^i","['discrete-mathematics', 'homology-cohomology', 'simplicial-complex', 'hodge-theory']"
86,"Upper bound of number of triangles in an edge-colored graph, given that none of the triangles are rainbow.","Upper bound of number of triangles in an edge-colored graph, given that none of the triangles are rainbow.",,"Let $G$ be a simple edge-coloured graph with $m$ edges. Assume furthermore that it does not contain a rainbow triangle, (i.e a triangle such that each edge has different color) and each color class in $G$ has size at most $k$ . Let $t(G)$ be number of triangles contained in $G$ . Prove: $$ t(G) \leq \frac{1}{2} m(k-1) $$ Since the triangles are not rainbow, each one of them has a dominant color, i.e., at least two of the three edges in the triangle are in that color. This also gives a partition of the triangles based on the dominant colors, so I’m thinking of upper bounding the number of triangles $t_i$ dominant in color $i$ , summing up the bounds across all colors, and hopefully get a bound on $t(G)$ . Now any $i$ -dominant triangle use at least $2$ of the $m_i$ edges having color $i$ . And we know $m_i \leq k$ for all $i$ . So we have: $$t_i \leq {m_i \choose 2} = \frac{1}{2}m_i(m_i - 1) \leq \frac{1}{2}m_i(k-1)$$ We note that the $m_i$ ’s partition $m$ , so when summing up the $t_i$ ’s across all color classes, we get: $$t(G) = \sum t_i \leq \frac{1}{2} m (k-1)$$ Note: The following is my first attempt, which was wrong. Also there was a typo in the question that I was not aware of. I let $[r]$ be the color classes, and single out a color $i$ . The classes $[r] \setminus \{i\}$ have at least $k(r-1)$ edges in total, so $i$ has at most $m - k(r-1)$ edges. Therefore: $$t_i \leq \frac{m_i}{2} \leq \frac{1}{2} (m - k(r-1)),$$ where $m_i$ is the number of edges colored $i$ , and equality of the first $\leq$ is achieved when each of the $i$ -dominant triangles contains $2$ edges colored $i$ . Summing up the above bound across all colors, we get: $$t(G) \leq \frac{1}{2}r(m - k(r-1))$$ While I can upper bound $r$ with $\lfloor \frac{m}{k} \rfloor$ , I’m stuck at lower bounding it to make the above work. How should I proceed?","Let be a simple edge-coloured graph with edges. Assume furthermore that it does not contain a rainbow triangle, (i.e a triangle such that each edge has different color) and each color class in has size at most . Let be number of triangles contained in . Prove: Since the triangles are not rainbow, each one of them has a dominant color, i.e., at least two of the three edges in the triangle are in that color. This also gives a partition of the triangles based on the dominant colors, so I’m thinking of upper bounding the number of triangles dominant in color , summing up the bounds across all colors, and hopefully get a bound on . Now any -dominant triangle use at least of the edges having color . And we know for all . So we have: We note that the ’s partition , so when summing up the ’s across all color classes, we get: Note: The following is my first attempt, which was wrong. Also there was a typo in the question that I was not aware of. I let be the color classes, and single out a color . The classes have at least edges in total, so has at most edges. Therefore: where is the number of edges colored , and equality of the first is achieved when each of the -dominant triangles contains edges colored . Summing up the above bound across all colors, we get: While I can upper bound with , I’m stuck at lower bounding it to make the above work. How should I proceed?","G m G k t(G) G 
t(G) \leq \frac{1}{2} m(k-1)
 t_i i t(G) i 2 m_i i m_i \leq k i t_i \leq {m_i \choose 2} = \frac{1}{2}m_i(m_i - 1) \leq \frac{1}{2}m_i(k-1) m_i m t_i t(G) = \sum t_i \leq \frac{1}{2} m (k-1) [r] i [r] \setminus \{i\} k(r-1) i m - k(r-1) t_i \leq \frac{m_i}{2} \leq \frac{1}{2} (m - k(r-1)), m_i i \leq i 2 i t(G) \leq \frac{1}{2}r(m - k(r-1)) r \lfloor \frac{m}{k} \rfloor","['combinatorics', 'discrete-mathematics', 'graph-theory', 'solution-verification', 'coloring']"
87,Equality of two minimum spanning trees,Equality of two minimum spanning trees,,"Given weighted complete undirected graph $G=(V,E)$ with $n$ vertices and positive weights. Suppose we find minimum spanning tree $MST(G)$ as $T$ .Next we want to decrease weight of $n$ edges in $G$ to $-\infty$ . So we create new graph $G'=(V,E')$ that contains only edges that have $-\infty$ edge weights. Finally we compute Minimum spanning tree $T'$ from $T\cup G'$ . My question is, if at the first, decrease spcific edges in $G$ to $-\infty$ and then compute minimum spanning tree $T''$ , can we conclude that, edges in $T'$ are the same as $T''$ ?","Given weighted complete undirected graph with vertices and positive weights. Suppose we find minimum spanning tree as .Next we want to decrease weight of edges in to . So we create new graph that contains only edges that have edge weights. Finally we compute Minimum spanning tree from . My question is, if at the first, decrease spcific edges in to and then compute minimum spanning tree , can we conclude that, edges in are the same as ?","G=(V,E) n MST(G) T n G -\infty G'=(V,E') -\infty T' T\cup G' G -\infty T'' T' T''","['discrete-mathematics', 'graph-theory', 'algorithms', 'trees']"
88,Is $x = 2^6$ a statement,Is  a statement,x = 2^6,"I'm reading Susanna Epp's book on discrete mathematics. The exercise 2.1.5 ask which sentences are statements, a statement being something that is either true or false but not both. Question: is $x=2^6$ a statement? Can equations with variables be statements?","I'm reading Susanna Epp's book on discrete mathematics. The exercise 2.1.5 ask which sentences are statements, a statement being something that is either true or false but not both. Question: is a statement? Can equations with variables be statements?",x=2^6,['discrete-mathematics']
89,Find the sequence $a_k$ for generating function $\left(\frac{1-x^3}{1-x}\right)^n$,Find the sequence  for generating function,a_k \left(\frac{1-x^3}{1-x}\right)^n,We know that $\frac{1}{(1-x)^n} = \sum_{k=0}^\infty \binom{k+n-1}{n-1} x^k$ I also worked out $(1-x^3)^n$ using the binomial theorem and got $(1-x^3)^n = \sum_{i=0}^\infty (-1)^i \binom{n}{n-i} x^{3i}$ I'm not sure what to do with these to get $a_k$ from $\sum_{k=0}^\infty a_k x^k$ or if these are even what I need to solve the problem Any help is appreciated,We know that I also worked out using the binomial theorem and got I'm not sure what to do with these to get from or if these are even what I need to solve the problem Any help is appreciated,\frac{1}{(1-x)^n} = \sum_{k=0}^\infty \binom{k+n-1}{n-1} x^k (1-x^3)^n (1-x^3)^n = \sum_{i=0}^\infty (-1)^i \binom{n}{n-i} x^{3i} a_k \sum_{k=0}^\infty a_k x^k,"['sequences-and-series', 'discrete-mathematics', 'generating-functions']"
90,Proof by induction with inequalities,Proof by induction with inequalities,,"Prove $5n + 6 \leqslant n^2$ holds for all $n \geqslant N$ by induction. Here $N$ is the answer you get in (a). For (a) I got $6$ and I proceeded as follows: Base case: $n = 6$ : $5(6)+6 \leqslant 6^22$ , $36 \leqslant 36$ therefore base case is true. Assume $5k + 6 \leqslant k^2$ for $k \geqslant 6$ . Induction: $5(k+1) + 6 \leqslant (k+1)^2$ is true $$5k + 5 + 6  \leqslant k^2 + 2k +1$$ I'm some how confused as to what I need to do next.","Prove holds for all by induction. Here is the answer you get in (a). For (a) I got and I proceeded as follows: Base case: : , therefore base case is true. Assume for . Induction: is true I'm some how confused as to what I need to do next.",5n + 6 \leqslant n^2 n \geqslant N N 6 n = 6 5(6)+6 \leqslant 6^22 36 \leqslant 36 5k + 6 \leqslant k^2 k \geqslant 6 5(k+1) + 6 \leqslant (k+1)^2 5k + 5 + 6  \leqslant k^2 + 2k +1,"['discrete-mathematics', 'inequality', 'induction']"
91,Hamiltonian path exists if each two nonadjacent vertices has sum of degrees equal to n-1,Hamiltonian path exists if each two nonadjacent vertices has sum of degrees equal to n-1,,"Here is a question from a textbook I’m reading, prove that a simple graph with n vertices has a hamiltonian path if the sum of degree number of each two none adjacent vertices is n-1. I know A graph with n vertices (where n > 3) is Hamiltonian if the sum of the degrees of every pair of non-adjacent vertices is n or greater. I know Dirac's theorem for Hamiltonian graphs tells us that if a graph of order n greater than or equal to 3 has a minimum degree greater than or equal to half of n, then the graph is Hamiltonian How can we use them to achieve the proper answer ? A hint or an answer would be appreciated. Thank you in advance.","Here is a question from a textbook I’m reading, prove that a simple graph with n vertices has a hamiltonian path if the sum of degree number of each two none adjacent vertices is n-1. I know A graph with n vertices (where n > 3) is Hamiltonian if the sum of the degrees of every pair of non-adjacent vertices is n or greater. I know Dirac's theorem for Hamiltonian graphs tells us that if a graph of order n greater than or equal to 3 has a minimum degree greater than or equal to half of n, then the graph is Hamiltonian How can we use them to achieve the proper answer ? A hint or an answer would be appreciated. Thank you in advance.",,"['graph-theory', 'hamiltonian-path']"
92,Show for all n ∈ IN that the inequality $(n!)^2<2^{n^{2}}$ holds (by induction),Show for all n ∈ IN that the inequality  holds (by induction),(n!)^2<2^{n^{2}},"Show by induction that $$(n!)^2<2^{n^2} \quad \forall n \in \mathbb{N} $$ $n=1 \leftrightarrow  1<2$ $(k!)^2<2^{k^2}$ $((k+1)(k!))^2 < 2^{(k+1)^2}$ and $(k^2+2k+1)(k!)^2<2k^2 \cdot 2^{2k+1}$ , but our assumption told us that, $(k!)^2<2^{k^2}$ , therefore I rewrote the inequality to $(k+1)^2<2^{2k+1}$ . However, this got me nowhere, I kept doing this for four different inequalities down the line without getting to a satisfying answer. May I get some help? Please.","Show by induction that and , but our assumption told us that, , therefore I rewrote the inequality to . However, this got me nowhere, I kept doing this for four different inequalities down the line without getting to a satisfying answer. May I get some help? Please.",(n!)^2<2^{n^2} \quad \forall n \in \mathbb{N}  n=1 \leftrightarrow  1<2 (k!)^2<2^{k^2} ((k+1)(k!))^2 < 2^{(k+1)^2} (k^2+2k+1)(k!)^2<2k^2 \cdot 2^{2k+1} (k!)^2<2^{k^2} (k+1)^2<2^{2k+1},"['inequality', 'induction']"
93,Possible Solution to Ordered Games Problem,Possible Solution to Ordered Games Problem,,"This a followup to a question I asked here . I am hoping to get some feedback on a possible proof of the claim (showing that the probability of winning the match for either player is equal). Specifically, the ""Reversing Arrows"" section below and any glaring errors. Grid Representation Let us start by visualizing the problem using a grid. Consider the case where $n = 3$ . We have P1 starting on Char 1 and P2 starting on Char 3. We can represent this via the diagram below, where each box represents a possible matchup; moving to the right is a win for P2 and moving down is a win for P1. This means that P1 winning the match is represented by a sequence of down and right moves leading off the bottom of the grid whereas P2 winning the match is represented by a similar sequence leading off the right side of the grid. We also consider any set of probabilities (with the assumption of no 100/0 matchups). Now consider generalizing to a set of unique characters (so every matchup is unique). For convenience, let us label P2's characters 3 to 1 and P1's characters 4 through 6. The grid now looks like the following: Ordered Games BO5 as Motivation Taking a quick detour, let us consider a ""1-D"" version of this problem with ordered games in a best of five. Suppose we have two players (P1 and P2) who play games against each other. They play a match with 5 games (labeled 1 through 5) where each game has a unique probability for P1 or P2 to win (which is where the ordering comes into play). So Game 1 has probability $p_1$ for P1 and $q_1 = 1-p_1$ for P2, Game 2 is $p_2$ and $q_2$ and so on, with the assumption that $p_i$ is not necessarily equal to $p_j$ for all $i$ and $j$ (and standard assumption of no 100/0 matchups). In this situation an interesting symmetry arises: the probability for P1 to win when playing the games in order from 1 to 5 is the same as the probability of P1 winning when playing the games in the reverse order (or any other order for that matter). This is because once P1 has won three games, essentially the rest of the games in that particular match don't matter (playing them doesn't change the fact that P1 has already won). Mathematically this is accounted for in the probability when extending every match to five games and considering all the possible W/L sequences. Connection to Grid Idea (Reversing Arrows) How does this relate to the ""2-D"" version of the problem? Looking at our diagram, we have a parallel to the ""1-D"" example: P1 winning a ""2-D"" match consists of P1 winning valid BO5s (each path corresponding to a particular configuration) starting with the (3,4) matchup (the specific probabilities per game changing based on the W/L record). With this motivation, consider changing the order of the matchups -- specifically, let us reverse the order of the matchups. This amounts to reversing the arrows on the grid in a consistent manner. Instead of starting with P1 on 4 and P2 on 3, we start P1 on 6 and P2 on 1 -- the last possible matchup. In every game, we have two states that we can go to based on the W/L probability in the forward direction. When reversing we have to choose which probability leads to which outcome. For example, winning (5, 1) or losing (6, 2) both lead to the state (6, 1). Reversed, we have winning (6, 1) leads to (5, 1) and losing (6, 1) leads to (6, 2). This assignment counts the paths in the same way since we have the winner switching character. The following diagram shows the original paths on the left and the new reversed paths on the right with probabilities. Now consider the probability that P1 wins in this new grid. When reversing the arrows we must maintain the total probability of P1 winning because (1) every path to victory traverses the same P1 win matchups (the reversed arrows in the vertical direction are the same for every path) and (2) we counted the paths the same way. So the probability of P1 winning in the ""forward"" configuration is the same as the probability of P1 winning in the ""reverse"" configuration. Application to Original Problem Finally we consider our original case (with P1 playing 1 to 3 and P2 playing 3 to 1). It is clear to see that by symmetry, the paths defined by reversing arrows corresponding to P1 winning are exactly the paths that correspond to P2 winning (shown below). By the reverse arrow argument in the previous section, the total probabilities of winning are the same (being a specific case of the general claim above with 4 = 1, 5 = 2, and 6 = 3) and since the sum of their probabilities must add up to 1, the probability of P1 and P2 winning must both be 0.5. Therefore the claim is proved for $n = 3.$ We can extend this analysis to other $n$ with some minor modifications.","This a followup to a question I asked here . I am hoping to get some feedback on a possible proof of the claim (showing that the probability of winning the match for either player is equal). Specifically, the ""Reversing Arrows"" section below and any glaring errors. Grid Representation Let us start by visualizing the problem using a grid. Consider the case where . We have P1 starting on Char 1 and P2 starting on Char 3. We can represent this via the diagram below, where each box represents a possible matchup; moving to the right is a win for P2 and moving down is a win for P1. This means that P1 winning the match is represented by a sequence of down and right moves leading off the bottom of the grid whereas P2 winning the match is represented by a similar sequence leading off the right side of the grid. We also consider any set of probabilities (with the assumption of no 100/0 matchups). Now consider generalizing to a set of unique characters (so every matchup is unique). For convenience, let us label P2's characters 3 to 1 and P1's characters 4 through 6. The grid now looks like the following: Ordered Games BO5 as Motivation Taking a quick detour, let us consider a ""1-D"" version of this problem with ordered games in a best of five. Suppose we have two players (P1 and P2) who play games against each other. They play a match with 5 games (labeled 1 through 5) where each game has a unique probability for P1 or P2 to win (which is where the ordering comes into play). So Game 1 has probability for P1 and for P2, Game 2 is and and so on, with the assumption that is not necessarily equal to for all and (and standard assumption of no 100/0 matchups). In this situation an interesting symmetry arises: the probability for P1 to win when playing the games in order from 1 to 5 is the same as the probability of P1 winning when playing the games in the reverse order (or any other order for that matter). This is because once P1 has won three games, essentially the rest of the games in that particular match don't matter (playing them doesn't change the fact that P1 has already won). Mathematically this is accounted for in the probability when extending every match to five games and considering all the possible W/L sequences. Connection to Grid Idea (Reversing Arrows) How does this relate to the ""2-D"" version of the problem? Looking at our diagram, we have a parallel to the ""1-D"" example: P1 winning a ""2-D"" match consists of P1 winning valid BO5s (each path corresponding to a particular configuration) starting with the (3,4) matchup (the specific probabilities per game changing based on the W/L record). With this motivation, consider changing the order of the matchups -- specifically, let us reverse the order of the matchups. This amounts to reversing the arrows on the grid in a consistent manner. Instead of starting with P1 on 4 and P2 on 3, we start P1 on 6 and P2 on 1 -- the last possible matchup. In every game, we have two states that we can go to based on the W/L probability in the forward direction. When reversing we have to choose which probability leads to which outcome. For example, winning (5, 1) or losing (6, 2) both lead to the state (6, 1). Reversed, we have winning (6, 1) leads to (5, 1) and losing (6, 1) leads to (6, 2). This assignment counts the paths in the same way since we have the winner switching character. The following diagram shows the original paths on the left and the new reversed paths on the right with probabilities. Now consider the probability that P1 wins in this new grid. When reversing the arrows we must maintain the total probability of P1 winning because (1) every path to victory traverses the same P1 win matchups (the reversed arrows in the vertical direction are the same for every path) and (2) we counted the paths the same way. So the probability of P1 winning in the ""forward"" configuration is the same as the probability of P1 winning in the ""reverse"" configuration. Application to Original Problem Finally we consider our original case (with P1 playing 1 to 3 and P2 playing 3 to 1). It is clear to see that by symmetry, the paths defined by reversing arrows corresponding to P1 winning are exactly the paths that correspond to P2 winning (shown below). By the reverse arrow argument in the previous section, the total probabilities of winning are the same (being a specific case of the general claim above with 4 = 1, 5 = 2, and 6 = 3) and since the sum of their probabilities must add up to 1, the probability of P1 and P2 winning must both be 0.5. Therefore the claim is proved for We can extend this analysis to other with some minor modifications.",n = 3 p_1 q_1 = 1-p_1 p_2 q_2 p_i p_j i j n = 3. n,"['probability', 'combinatorics', 'discrete-mathematics', 'word-problem']"
94,Prove every permutation of the alphabet contains a subset of six letters in order,Prove every permutation of the alphabet contains a subset of six letters in order,,"Prove that no matter the arrangement of the alphabet you can find a subset of six letters in alphabetical order, read from left to right or right to left. For example the permutation {g,w,z,c,d,p,i,b,y,t,a,n,u,e,r,j,l,x,s,v,m,f,q,k,h,o} contains the subset {a,e,j,l,m,q}. I know I have to construct a partially ordered set and possibly use Dilworth's Theorem, however I don't know how to begin tackling this problem.","Prove that no matter the arrangement of the alphabet you can find a subset of six letters in alphabetical order, read from left to right or right to left. For example the permutation {g,w,z,c,d,p,i,b,y,t,a,n,u,e,r,j,l,x,s,v,m,f,q,k,h,o} contains the subset {a,e,j,l,m,q}. I know I have to construct a partially ordered set and possibly use Dilworth's Theorem, however I don't know how to begin tackling this problem.",,"['combinatorics', 'discrete-mathematics', 'order-theory']"
95,Counting number of vertices in a simplicial complex,Counting number of vertices in a simplicial complex,,"In some computation I have to do, I have to deal with the following situation: I have an arbitrary number of tetrahedra (=3 simplexes) and I am allowed to identify pairs of faces to each other. There is no restriction in how this is done, for example it is also allowed to identify two faces, which belong to the same tetrahedra. Each face is only allowed to be identified once, so I it not possible to have three faces identified to each other. Furthermore, it is also possible that after this procedure there are some faces, which are not identified to anything else. Now my question is the following: If i know the number of edges, faces and tetrahedra, as well as as the number of face identifications, is it possible to calculate the number of vertices? I naively guessed at the beginning the following formula $$4\text{ number of tetrahedra} - 3\text{ number of identifiactions}$$ in which i basically viewed every tetrahedron (which has 4 vertices) at the beginning to be disjoint and substract three vertices for each identification. However, this is obviously wrong, since when I glue 4 tetrahedra as in the following figure, the answer is clearly wrong: Here my calculation gives $4\cdot 4- 3\cdot 6=-2$ , which is quite far from the correct answer $5$ . Any idea how do it correctly?","In some computation I have to do, I have to deal with the following situation: I have an arbitrary number of tetrahedra (=3 simplexes) and I am allowed to identify pairs of faces to each other. There is no restriction in how this is done, for example it is also allowed to identify two faces, which belong to the same tetrahedra. Each face is only allowed to be identified once, so I it not possible to have three faces identified to each other. Furthermore, it is also possible that after this procedure there are some faces, which are not identified to anything else. Now my question is the following: If i know the number of edges, faces and tetrahedra, as well as as the number of face identifications, is it possible to calculate the number of vertices? I naively guessed at the beginning the following formula in which i basically viewed every tetrahedron (which has 4 vertices) at the beginning to be disjoint and substract three vertices for each identification. However, this is obviously wrong, since when I glue 4 tetrahedra as in the following figure, the answer is clearly wrong: Here my calculation gives , which is quite far from the correct answer . Any idea how do it correctly?",4\text{ number of tetrahedra} - 3\text{ number of identifiactions} 4\cdot 4- 3\cdot 6=-2 5,"['combinatorics', 'discrete-mathematics', 'simplicial-complex', 'triangulation']"
96,Question about discrete math: Division by integers and GCD,Question about discrete math: Division by integers and GCD,,"could anyone tell me if my resolution is correct? Problem : Let $a$ , $b$ , and $c$ be  integers satisfying: $a|(b+4c)$ $\qquad$ $(1)$ $a|(b-2c)$ $\qquad$ $(2)$ $a$ leaves remainder $1$ when divided by $3$ $\qquad$ $(3)$ Under these conditions can we say that $a$ divides $b$ ? My solution : Theorems and properties used: $1$ : If $a|b$ and $a|c$ then $a|(b+c)$ ; $2$ : If $a|b$ then $a|bc$ for any integer $c$ ; $3$ : If $a$ and $b$ are integers and $a = q*b + r$ where $q$ and $r$ are integers, then: $GCD(a,b)=GCD(b,r)$ ; $4$ : let $a$ , $b$ and $c$ be three integers such that $a$ divides $bc$ and $a$ and $b$ are prime to each other, then $a$ divides $c$ . Using Theorem $2$ and relation $(2)$ we can say that: $a|2(b-2c)$ $\longrightarrow$ $a|(2b-4c)$ $\qquad$ $(4)$ Using Theorem $1$ and relation $(4)$ we obtain that: $a|[(b+4c)+(2b-4c)]$ $\longrightarrow$ $a|3b$ $\qquad$ $(5)$ Due to condition $(3)$ we can write $a$ as: $|a|=3k+1$ , $\qquad$ $k\in \mathbb{Z}$ $\qquad$ $(6)$ Using property $3$ : $GCD(a,3)=GCD(3,1)=1$ $\qquad$ $(7)$ Finally, due to the result obtained in $(7)$ and property $4$ we can conclude that $a|b$ I believe it's all right, but I still don't have much confidence if the properties and theorems I've chosen can be applied here or if I've applied them correctly. If you can confirm that everything is correct, or if it is wrong, tell me where I went wrong and how to proceed, I would be grateful.","could anyone tell me if my resolution is correct? Problem : Let , , and be  integers satisfying: leaves remainder when divided by Under these conditions can we say that divides ? My solution : Theorems and properties used: : If and then ; : If then for any integer ; : If and are integers and where and are integers, then: ; : let , and be three integers such that divides and and are prime to each other, then divides . Using Theorem and relation we can say that: Using Theorem and relation we obtain that: Due to condition we can write as: , Using property : Finally, due to the result obtained in and property we can conclude that I believe it's all right, but I still don't have much confidence if the properties and theorems I've chosen can be applied here or if I've applied them correctly. If you can confirm that everything is correct, or if it is wrong, tell me where I went wrong and how to proceed, I would be grateful.","a b c a|(b+4c) \qquad (1) a|(b-2c) \qquad (2) a 1 3 \qquad (3) a b 1 a|b a|c a|(b+c) 2 a|b a|bc c 3 a b a = q*b + r q r GCD(a,b)=GCD(b,r) 4 a b c a bc a b a c 2 (2) a|2(b-2c) \longrightarrow a|(2b-4c) \qquad (4) 1 (4) a|[(b+4c)+(2b-4c)] \longrightarrow a|3b \qquad (5) (3) a |a|=3k+1 \qquad k\in \mathbb{Z} \qquad (6) 3 GCD(a,3)=GCD(3,1)=1 \qquad (7) (7) 4 a|b","['discrete-mathematics', 'education']"
97,Valid arguments and truth tables,Valid arguments and truth tables,,"I'm trying to understand validity of arguments and using truth tables. This concerns an example on a discrete math course on Linkedin Learning which I reproduce here. Essentially, trying to make sentences out of the truth table is not making much sense except for the first row. We have some sentence propositions: Today is Monday ( $p$ ) If today is Monday, then I will have a salad for lunch ( $p \rightarrow q$ ) Therefore, I will have a salad for lunch ( $q$ ) Which, symbolically, looks like $$ p \\ p \rightarrow q \\ \therefore q $$ This is then used to construct the following table $$ \begin{array}{|c|c|c|} \hline p& p \rightarrow q & q  \\ \hline T &\ \ T & T \\ \hline T & F & F \\ \hline F & T & T \\ \hline F & T & F \\ \hline \end{array} $$ I get how the columns for $p$ and $q$ are laid out but I don't understand how the true/false values for $p \rightarrow q$ for the 3rd, and 4th rows play out (and I'm not entirely sure about the second either): Second row: ""Today is Monday. If it is a Monday, then I will not have a salad for lunch. Therefore I am not having a salad for lunch"". Third row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will have a salad for lunch"" ? Fourth row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will not have a salad for lunch"" ?","I'm trying to understand validity of arguments and using truth tables. This concerns an example on a discrete math course on Linkedin Learning which I reproduce here. Essentially, trying to make sentences out of the truth table is not making much sense except for the first row. We have some sentence propositions: Today is Monday ( ) If today is Monday, then I will have a salad for lunch ( ) Therefore, I will have a salad for lunch ( ) Which, symbolically, looks like This is then used to construct the following table I get how the columns for and are laid out but I don't understand how the true/false values for for the 3rd, and 4th rows play out (and I'm not entirely sure about the second either): Second row: ""Today is Monday. If it is a Monday, then I will not have a salad for lunch. Therefore I am not having a salad for lunch"". Third row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will have a salad for lunch"" ? Fourth row: ""Today is not Monday. If it is Monday, then I will have a salad for lunch. Therefore I will not have a salad for lunch"" ?","p p \rightarrow q q 
p \\
p \rightarrow q \\
\therefore q
 
\begin{array}{|c|c|c|}
\hline
p& p \rightarrow q & q 
\\ \hline
T &\ \ T & T
\\ \hline
T & F & F
\\ \hline
F & T & T
\\ \hline
F & T & F
\\ \hline
\end{array}
 p q p \rightarrow q","['discrete-mathematics', 'logic']"
98,Convergence of the series $\sum k^2|a_k|$,Convergence of the series,\sum k^2|a_k|,"If $$\sum_{k>n}|a_k|=O\left( \frac{1}{n^\beta}\right)$$ for some $\beta>2$ . I need to get that $$\sum_{k=1}^{\infty}k^2|a_k|<\infty$$ I have a bit of a problem working with big $O$ notation, and I'm not getting an idea to solve this problem, because the information I have is about the ""tail"" of the series, not the series. Edit: Using Greg Martin's idea I believe I solved the problem, but the resolution was long. But I thought of another solution that I will post.","If for some . I need to get that I have a bit of a problem working with big notation, and I'm not getting an idea to solve this problem, because the information I have is about the ""tail"" of the series, not the series. Edit: Using Greg Martin's idea I believe I solved the problem, but the resolution was long. But I thought of another solution that I will post.",\sum_{k>n}|a_k|=O\left( \frac{1}{n^\beta}\right) \beta>2 \sum_{k=1}^{\infty}k^2|a_k|<\infty O,"['discrete-mathematics', 'summation', 'asymptotics']"
99,"Solve the equation with respect to $k_1,k_2\in \mathbb{Z}_{+}$",Solve the equation with respect to,"k_1,k_2\in \mathbb{Z}_{+}","I am struggling with solving the following equation for positive integers $k_1$ and $k_2$ in terms of $n\in \mathbb{Z}_+$ and $i,j\in \mathbb{Z}_+$ : $$n-1=\sum_{i\le k_1,j\le k_2}\sum_{\text{gcd}(i,j)=1}1.$$ Note that this equation can be interpreted also as $n-1=\sum_{i\le k_1}\sum_{j\le k_2}\sum_{d\ge 1,d|\text{gcd}(i,j)}\mu(d)$ where $\mu$ is the Mobius function defined as $\mu(n)=\begin{cases} 1 & \text{if $n$ is a square-free positive integer with an even number of prime factors}\\ -1 & \text{if $n$ is a square-free positive integers with an odd number of prime factors}\\ 0 & \text{if $n$ has a squared prime factor.} \end{cases}$",I am struggling with solving the following equation for positive integers and in terms of and : Note that this equation can be interpreted also as where is the Mobius function defined as,"k_1 k_2 n\in \mathbb{Z}_+ i,j\in \mathbb{Z}_+ n-1=\sum_{i\le k_1,j\le k_2}\sum_{\text{gcd}(i,j)=1}1. n-1=\sum_{i\le k_1}\sum_{j\le k_2}\sum_{d\ge 1,d|\text{gcd}(i,j)}\mu(d) \mu \mu(n)=\begin{cases}
1 & \text{if n is a square-free positive integer with an even number of prime factors}\\
-1 & \text{if n is a square-free positive integers with an odd number of prime factors}\\
0 & \text{if n has a squared prime factor.}
\end{cases}","['combinatorics', 'number-theory']"
