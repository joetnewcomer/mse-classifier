,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Let $A \in \mathbb{R}^{n \times n}$ such that for all $x \in \mathbb{R}^n, x^TAx \geq 0$. Prove that $\ker(A) = \ker(A^T)$",Let  such that for all . Prove that,"A \in \mathbb{R}^{n \times n} x \in \mathbb{R}^n, x^TAx \geq 0 \ker(A) = \ker(A^T)","Let $A \in \mathbb{R}^{n \times n}$ such that for all $x \in \mathbb{R}^n, x^TAx \geq 0$ . Prove that $\ker(A) = \ker(A^T)$ . My idea: if we can prove $A$ is symmetric, then we can solve it?","Let such that for all . Prove that . My idea: if we can prove is symmetric, then we can solve it?","A \in \mathbb{R}^{n \times n} x \in \mathbb{R}^n, x^TAx \geq 0 \ker(A) = \ker(A^T) A","['linear-algebra', 'matrices']"
1,How many matrices in which product of numbers in each row and column is 1?,How many matrices in which product of numbers in each row and column is 1?,,"We have a matrix $S$ . Every $S_{ij}$ is equal to $1$ or $-1$ . How many matrices we can have so that product of all entries in every row and column is $1$ ? This means number of $-1$ must be even in all rows and columns. And size of $S$ is $m\times n$ . My idea was that we can divide a matrix of $(m-1)\times (n-1)$ , then count all the possible ways this matrix can have $1$ and $-1$ which is $2^{(m-1)(n-1)}$ . Then add other part of matrix and set those values in a way that number of $-1$ become even. But this way doesn't work. Sorry if I didn't explain my way very clearly, it's because English is not my native language.","We have a matrix . Every is equal to or . How many matrices we can have so that product of all entries in every row and column is ? This means number of must be even in all rows and columns. And size of is . My idea was that we can divide a matrix of , then count all the possible ways this matrix can have and which is . Then add other part of matrix and set those values in a way that number of become even. But this way doesn't work. Sorry if I didn't explain my way very clearly, it's because English is not my native language.",S S_{ij} 1 -1 1 -1 S m\times n (m-1)\times (n-1) 1 -1 2^{(m-1)(n-1)} -1,"['matrices', 'number-theory', 'combinations']"
2,Is Matrix Calculus a specific case of Tensor Calculus?,Is Matrix Calculus a specific case of Tensor Calculus?,,"I'm an undergraduate student who already finished a linear algebra and vector calculus class. I'm wondering if matrix calculus is a specific case of tensor calculus and if theorems and results from matrix calculus can be deduced using tensor calculus. Finally, is it necessary to learn matrix calculus first to understand tensor calculus?","I'm an undergraduate student who already finished a linear algebra and vector calculus class. I'm wondering if matrix calculus is a specific case of tensor calculus and if theorems and results from matrix calculus can be deduced using tensor calculus. Finally, is it necessary to learn matrix calculus first to understand tensor calculus?",,"['matrices', 'matrix-calculus', 'tensors']"
3,Generalised Eigenvectors | Correct way to Approach,Generalised Eigenvectors | Correct way to Approach,,"I have a matrix $$A = \begin{bmatrix}1 & 1 \\ -1 & 3\end{bmatrix}$$ I want to find out the generalised Eigenvectors. The Eigen values corresponding to the characteristic equation is $\lambda = 2$ and the Eigenvector correspondig to the eigenvalue is found to be $\begin{bmatrix}1 \\ 1\end{bmatrix}$ . So how to calculate the generalised Eigen vector for this matrix. What I did is , I took $(A-\lambda\cdot I)^2 \nu = 0$ . then solving the $(A-\lambda I)^2 = \begin{bmatrix}1-\lambda & 1 \\ -1 & 3-\lambda \end{bmatrix}^2 = \begin{bmatrix}\lambda^2-2\lambda & 4 - 2\lambda \\ 2\lambda-4 & \lambda^2 - 6\lambda+8 \end{bmatrix}$ At this point I don't know whether I am doing the things correct . as finding the determinant will take this to $\lambda^4$ .","I have a matrix I want to find out the generalised Eigenvectors. The Eigen values corresponding to the characteristic equation is and the Eigenvector correspondig to the eigenvalue is found to be . So how to calculate the generalised Eigen vector for this matrix. What I did is , I took . then solving the At this point I don't know whether I am doing the things correct . as finding the determinant will take this to .",A = \begin{bmatrix}1 & 1 \\ -1 & 3\end{bmatrix} \lambda = 2 \begin{bmatrix}1 \\ 1\end{bmatrix} (A-\lambda\cdot I)^2 \nu = 0 (A-\lambda I)^2 = \begin{bmatrix}1-\lambda & 1 \\ -1 & 3-\lambda \end{bmatrix}^2 = \begin{bmatrix}\lambda^2-2\lambda & 4 - 2\lambda \\ 2\lambda-4 & \lambda^2 - 6\lambda+8 \end{bmatrix} \lambda^4,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'control-theory', 'generalized-eigenvector']"
4,Householder QR derivation,Householder QR derivation,,"Im looking at the Householder operation, does anyone know how this guy got from the first equation to the second? $$\forall x, Px = x - \frac{2v(x^Tv)}{v^Yv} \implies  P = I-\frac{2vv^T}{v^Tv}$$ I know this is probably very simple but I am very new to matrices and all their defined properties and operations","Im looking at the Householder operation, does anyone know how this guy got from the first equation to the second? I know this is probably very simple but I am very new to matrices and all their defined properties and operations","\forall x, Px = x - \frac{2v(x^Tv)}{v^Yv} \implies  P = I-\frac{2vv^T}{v^Tv}",['matrices']
5,What is the intuition for why row reduction can show linear dependence,What is the intuition for why row reduction can show linear dependence,,"You can determine whether a set of column vectors is dependent by placing them in a matrix and getting the matrix into RREF. If all columns have a leading entry of $1$ , then the set of vectors is independent. If a column does not have a leading entry but instead has one or more nonzero entries that are in the same row as a leading entry, then the set of vectors is dependent. For example, it can be shown that the following set of vectors $S$ is dependent: $S=\left( \begin{bmatrix}     1 \\     4 \\     0 \\     2  \end{bmatrix}, \begin{bmatrix}     3 \\     1 \\     4 \\     0  \end{bmatrix}, \begin{bmatrix}     -1 \\     7 \\     -4 \\     4  \end{bmatrix}, \begin{bmatrix}     0 \\     11 \\     -4 \\     6  \end{bmatrix} \right)$ This set of vectors can placed in a matrix $A$ and row reduced. $A = \begin{bmatrix}     1       & 3 & -1 & 0 \\     4       & 1 & 7 & 11 \\     0       & 4 & -4 & -4 \\     2       & 0 & 4 & 6  \end{bmatrix}$ Row reducing shows that the set of vectors is dependent because column $3$ and column $4$ have nonzero entries in the same row as leading entries. $RREF(A)=\begin{bmatrix}     1       & 0 & 2 & 3 \\     0       & 1 & -1 & -1 \\     0       & 0 & 0 & 0 \\     0       & 0 & 0 & 0  \end{bmatrix}$ Furthermore, the entries of these columns correspond to scalars of linear combinations which show that the set of vectors is linearly dependent. For example, using the elements $3$ and $-1$ as scalars, it can be shown that column vector $4$ is a linear combination of column vectors $1$ and $2$ .: $ \begin{bmatrix}     0 \\     11 \\     -4 \\     6  \end{bmatrix} = $ $ 3\begin{bmatrix}     1 \\     4 \\     0 \\     2  \end{bmatrix}$ $-\begin{bmatrix}     3 \\     1 \\     4 \\     0  \end{bmatrix}$ Logically, why does the process of row reduction reveal the scalars which prove linear dependence in a set of vectors? I understand that it does work, but not why it should work. EDIT: I understand that row operations don't change whether a set of column vectors are dependent/independent. My question was as follows: why in RREF do the entries of a column vector correspond to scalars in a linear combination which can prove linear dependence? I thought of an analogy which would serve as an adequate answer to my question, but I'm not sure if it is accurate or not. You can see how large certain numbers are in terms of other numbers through division. For example, how large is the number $5$ in terms of $4$ ? The quotient $5/4=1.25$ tells us that $5$ is $1.25$ times as large as $4$ . Similarly, through row reduction you can express column vectors in terms of other column vectors. When I row reduce columns $1$ and $2$ so that they have a leading entry of $1$ , I express columns $3$ and $4$ in terms of columns $1$ and $2$ . Is this understanding somewhat accurate (it only has to serve as a general intuition)?","You can determine whether a set of column vectors is dependent by placing them in a matrix and getting the matrix into RREF. If all columns have a leading entry of , then the set of vectors is independent. If a column does not have a leading entry but instead has one or more nonzero entries that are in the same row as a leading entry, then the set of vectors is dependent. For example, it can be shown that the following set of vectors is dependent: This set of vectors can placed in a matrix and row reduced. Row reducing shows that the set of vectors is dependent because column and column have nonzero entries in the same row as leading entries. Furthermore, the entries of these columns correspond to scalars of linear combinations which show that the set of vectors is linearly dependent. For example, using the elements and as scalars, it can be shown that column vector is a linear combination of column vectors and .: Logically, why does the process of row reduction reveal the scalars which prove linear dependence in a set of vectors? I understand that it does work, but not why it should work. EDIT: I understand that row operations don't change whether a set of column vectors are dependent/independent. My question was as follows: why in RREF do the entries of a column vector correspond to scalars in a linear combination which can prove linear dependence? I thought of an analogy which would serve as an adequate answer to my question, but I'm not sure if it is accurate or not. You can see how large certain numbers are in terms of other numbers through division. For example, how large is the number in terms of ? The quotient tells us that is times as large as . Similarly, through row reduction you can express column vectors in terms of other column vectors. When I row reduce columns and so that they have a leading entry of , I express columns and in terms of columns and . Is this understanding somewhat accurate (it only has to serve as a general intuition)?","1 S S=\left(
\begin{bmatrix}
    1 \\
    4 \\
    0 \\
    2 
\end{bmatrix},
\begin{bmatrix}
    3 \\
    1 \\
    4 \\
    0 
\end{bmatrix},
\begin{bmatrix}
    -1 \\
    7 \\
    -4 \\
    4 
\end{bmatrix},
\begin{bmatrix}
    0 \\
    11 \\
    -4 \\
    6 
\end{bmatrix}
\right) A A = \begin{bmatrix}
    1       & 3 & -1 & 0 \\
    4       & 1 & 7 & 11 \\
    0       & 4 & -4 & -4 \\
    2       & 0 & 4 & 6 
\end{bmatrix} 3 4 RREF(A)=\begin{bmatrix}
    1       & 0 & 2 & 3 \\
    0       & 1 & -1 & -1 \\
    0       & 0 & 0 & 0 \\
    0       & 0 & 0 & 0 
\end{bmatrix} 3 -1 4 1 2 
\begin{bmatrix}
    0 \\
    11 \\
    -4 \\
    6 
\end{bmatrix} =
 
3\begin{bmatrix}
    1 \\
    4 \\
    0 \\
    2 
\end{bmatrix} -\begin{bmatrix}
    3 \\
    1 \\
    4 \\
    0 
\end{bmatrix} 5 4 5/4=1.25 5 1.25 4 1 2 1 3 4 1 2","['linear-algebra', 'matrices']"
6,"Computing the quotient of SL(2,Z) by its commutator subgroup","Computing the quotient of SL(2,Z) by its commutator subgroup",,"So far, I know the matrices $$S=\left( \begin{matrix} 0&-1\\ 1&0 \end{matrix} \right), \quad T=\left( \begin{matrix} 1&1\\ 0&1 \end{matrix} \right) $$ generate SL $(2,\mathbb{Z})$ . $S^2=(ST)^3=-I$ . I also know the matrices $$X=\left( \begin{matrix} 1&1\\ 1&2 \end{matrix} \right), \quad Y=\left( \begin{matrix} 2&1\\ 1&1 \end{matrix} \right) $$ with their inverses $X^{-1},Y^{-1}$ generate SL $(2,\mathbb{Z})'$ , the commutator subgroup. I'm interested in what the quotient $SL(2,\mathbb{Z})/SL(2,\mathbb{Z})'$ is. In general, how would I go about computing the quotient of two matrix groups? I'm trying to use GAP but I'm having a lot of trouble!","So far, I know the matrices generate SL . . I also know the matrices with their inverses generate SL , the commutator subgroup. I'm interested in what the quotient is. In general, how would I go about computing the quotient of two matrix groups? I'm trying to use GAP but I'm having a lot of trouble!","S=\left( \begin{matrix} 0&-1\\ 1&0 \end{matrix} \right), \quad T=\left( \begin{matrix} 1&1\\ 0&1 \end{matrix} \right)  (2,\mathbb{Z}) S^2=(ST)^3=-I X=\left( \begin{matrix} 1&1\\ 1&2 \end{matrix} \right), \quad Y=\left( \begin{matrix} 2&1\\ 1&1 \end{matrix} \right)  X^{-1},Y^{-1} (2,\mathbb{Z})' SL(2,\mathbb{Z})/SL(2,\mathbb{Z})'","['matrices', 'group-theory', 'quotient-group', 'modular-group']"
7,A lower bound on the largest eigenvalue of a symmetric matrix,A lower bound on the largest eigenvalue of a symmetric matrix,,"I am trying to prove the following: Let $A$ be an $n \times n$ real symmetric matrix with eigenvalues $\lambda_1 \geq \cdots \geq \lambda_n$ ; the sum all entries in $A$ is $s$ . Prove that $\lambda_1\geq\frac{s}{n}$ . I have tried the following: because of symmetry, we have \begin{equation} n\lambda_1^2 \geq \lambda_1^2 + \cdots + \lambda_n^2 = \mbox{Trace}(M^2)=\sum_{i,j}(M_{i,j})^2 \geq \frac{s^2}{n^2}, \end{equation} by Cauchy-Schwarz inequality. Therefore, \begin{equation} \lambda_1^2\geq\frac{s^2}{n^3} \Longrightarrow \lambda_1\geq\frac{s}{n\sqrt{n}}. \end{equation} This is the best I can get, I appreciate any corrections and hints to the result $s/n$ .","I am trying to prove the following: Let be an real symmetric matrix with eigenvalues ; the sum all entries in is . Prove that . I have tried the following: because of symmetry, we have by Cauchy-Schwarz inequality. Therefore, This is the best I can get, I appreciate any corrections and hints to the result .","A n \times n \lambda_1 \geq \cdots \geq \lambda_n A s \lambda_1\geq\frac{s}{n} \begin{equation}
n\lambda_1^2 \geq \lambda_1^2 + \cdots + \lambda_n^2 = \mbox{Trace}(M^2)=\sum_{i,j}(M_{i,j})^2 \geq \frac{s^2}{n^2},
\end{equation} \begin{equation}
\lambda_1^2\geq\frac{s^2}{n^3} \Longrightarrow \lambda_1\geq\frac{s}{n\sqrt{n}}.
\end{equation} s/n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
8,Change integration variable from scalar to matrix,Change integration variable from scalar to matrix,,"Suppose $c$ is a scalar, $\mathbf{A}$ is a symmetric positive definite matrix and $g(.)$ is some real-valued function. Define $\mathbf{B} = c \, \mathbf{A}$ . In this integral, $$ \int_{-\infty}^{\infty} g(c) \; det(c \, \mathbf{A}) \; dc = \int ? d\mathbf{B}, $$ how do I change the integration variable to $\mathbf{B}$ ?","Suppose is a scalar, is a symmetric positive definite matrix and is some real-valued function. Define . In this integral, how do I change the integration variable to ?","c \mathbf{A} g(.) \mathbf{B} = c \, \mathbf{A} 
\int_{-\infty}^{\infty} g(c) \; det(c \, \mathbf{A}) \; dc = \int ? d\mathbf{B},
 \mathbf{B}","['integration', 'matrices', 'matrix-calculus', 'substitution']"
9,Decomposition of doubly stochastic matrices,Decomposition of doubly stochastic matrices,,"Suppose $A$ is a doubly stochastic matrix, that is each row and column have the sum of their entries as $1$ and the entries are all nonnegative. Now, suppose $A=BC$ where $B$ is idempotent and $C$ is unitary. Then, it is easy to see that $B$ and $C$ have the row and column sum of entries in each row and/ or column as $1$ . But, does this also imply that $B, C$ are also doubly stochastic? Any hints? Thanks beforehand.","Suppose is a doubly stochastic matrix, that is each row and column have the sum of their entries as and the entries are all nonnegative. Now, suppose where is idempotent and is unitary. Then, it is easy to see that and have the row and column sum of entries in each row and/ or column as . But, does this also imply that are also doubly stochastic? Any hints? Thanks beforehand.","A 1 A=BC B C B C 1 B, C","['linear-algebra', 'matrices', 'stochastic-matrices']"
10,"If $n > 1$, there are no non-zero $*$-homomorphisms $M_n(\Bbb{C}) \to \Bbb{C}$","If , there are no non-zero -homomorphisms",n > 1 * M_n(\Bbb{C}) \to \Bbb{C},"If $n > 1$ , there are no non-zero $*$ -homomorphisms $M_n(\Bbb{C}) \to \Bbb{C}$ . A $*$ -homomorphism is an algebra morphism $\varphi: M_n(\Bbb{C}) \to \Bbb{C}$ with $\varphi(\overline{A}^T) = \overline{\varphi(A)}$ I tried to show that every $*$ -morphism must be zero: if $\varphi$ is such a $*$ -morphism, then $E_{ij}^2 = 0$ for $i \neq j$ (here $E_{ij}$ is the matrix that is $1$ on position $(ij)$ , $0$ elsewhere). Thus $\varphi(E_{ij}) = 0$ for $i \neq j$ . Hence, $$\varphi(A) = \sum_i a_{ii} \varphi(E_{ii})$$ so if I can show that $\varphi(E_{ii}) = 0$ I will be done. Unfortunately, I can't see why this should be true. I did not yet fully use the fact that it is a $*$ -morphism. The only thing I can see is that $\varphi(A) \in \Bbb{R}$ for all $A$ since $\varphi(E_{ii})= \varphi(\overline{E_{ii}}^T) = \overline{\varphi(E_{ii}})$ . If $\varphi$ is non-zero, then there is $A$ with $\varphi(A) \in \Bbb{R}\setminus \{0\}$ . But then $\varphi(iA) = i\varphi(A) \notin \Bbb{R}$ , a contradiction. Thus $\varphi=0$ . Is this correct?","If , there are no non-zero -homomorphisms . A -homomorphism is an algebra morphism with I tried to show that every -morphism must be zero: if is such a -morphism, then for (here is the matrix that is on position , elsewhere). Thus for . Hence, so if I can show that I will be done. Unfortunately, I can't see why this should be true. I did not yet fully use the fact that it is a -morphism. The only thing I can see is that for all since . If is non-zero, then there is with . But then , a contradiction. Thus . Is this correct?",n > 1 * M_n(\Bbb{C}) \to \Bbb{C} * \varphi: M_n(\Bbb{C}) \to \Bbb{C} \varphi(\overline{A}^T) = \overline{\varphi(A)} * \varphi * E_{ij}^2 = 0 i \neq j E_{ij} 1 (ij) 0 \varphi(E_{ij}) = 0 i \neq j \varphi(A) = \sum_i a_{ii} \varphi(E_{ii}) \varphi(E_{ii}) = 0 * \varphi(A) \in \Bbb{R} A \varphi(E_{ii})= \varphi(\overline{E_{ii}}^T) = \overline{\varphi(E_{ii}}) \varphi A \varphi(A) \in \Bbb{R}\setminus \{0\} \varphi(iA) = i\varphi(A) \notin \Bbb{R} \varphi=0,"['matrices', 'functional-analysis']"
11,Constants in matrix integration,Constants in matrix integration,,"Suppose you have an integral of a matrix-valued function of the form: $$\int_a^b B A(t) C dt$$ In this case, the notation $A(t)$ is to denote a matrix that depends on the integrated variable $t$ (for example $A(t) = e^{tA}$ ), where the matrices $B$ and $C$ are independent of $t$ . Is the following necessarily true as it would be for the analogous scalar problem? $$\int_a^b BA(t)Cdt = B\int_a^bA(t)Cdt = \int_a^b BA(t)d t C = B\int_a^b A(t)dt C$$","Suppose you have an integral of a matrix-valued function of the form: In this case, the notation is to denote a matrix that depends on the integrated variable (for example ), where the matrices and are independent of . Is the following necessarily true as it would be for the analogous scalar problem?",\int_a^b B A(t) C dt A(t) t A(t) = e^{tA} B C t \int_a^b BA(t)Cdt = B\int_a^bA(t)Cdt = \int_a^b BA(t)d t C = B\int_a^b A(t)dt C,"['linear-algebra', 'matrices', 'matrix-calculus']"
12,"Let $A,B\in\mathbb{R}^{n\times n}$, where $A$ is PSD and $B$ NSD. If $\mathrm{tr}(AB)=0$, show that $AB=0$.","Let , where  is PSD and  NSD. If , show that .","A,B\in\mathbb{R}^{n\times n} A B \mathrm{tr}(AB)=0 AB=0","Let $A,B\in\mathbb{R}^{n\times n}$ , where $A$ is a positive semidefinite matrix and $B$ a negative semidefinite matrix. If $\mathrm{tr}(AB)=0$ , show that $AB=0$ .","Let , where is a positive semidefinite matrix and a negative semidefinite matrix. If , show that .","A,B\in\mathbb{R}^{n\times n} A B \mathrm{tr}(AB)=0 AB=0","['linear-algebra', 'matrices']"
13,Does the inverse of a matrix with complex conjugate rows have complex conjugate columns?,Does the inverse of a matrix with complex conjugate rows have complex conjugate columns?,,Let $A \in \mathbb{C}^{2n \times 2n}$ be an invertible matrix which consists of $n$ rows and their $n$ complex conjugates (in any random order). Is it true that $A^{-1}$ will consist of $n$ columns and their $n$ complex conjugates?,Let be an invertible matrix which consists of rows and their complex conjugates (in any random order). Is it true that will consist of columns and their complex conjugates?,A \in \mathbb{C}^{2n \times 2n} n n A^{-1} n n,"['linear-algebra', 'matrices', 'complex-numbers']"
14,Proving the cross product matrix tranformation identity with an alternative solution,Proving the cross product matrix tranformation identity with an alternative solution,,"I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let $N$ be the normal vector to a surface at a point $P$ , and let $S$ and $T$ be tangent vectors at the point $P$ such that $S \times T = N$ . Given an invertible 3 $\times$ 3 matrix $M$ , show that $(MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T)$ , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix $M$ . The author provided a hint stating we can represent $(MS) \times (MT)$ as $$ (MS) \times (MT) = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} MT $$ We then find a matrix $G$ such that $$ GU = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} M $$ where $$ U = \begin{bmatrix} 0 & -S_{z} & S_{y} \\  S_{z} & 0 & -S_{x} \\  -S_{y} & S_{x} & 0  \end{bmatrix} $$ and show that $G = (\text{det}M)(M^{-1})^{T}$ to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: $$ G = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} M U^{-1} $$ At this point, I do not know how to proceed with showing that $G = (\text{det}M)(M^{-1})^{T}$ . How would you proceed? I'd like to ask for hints on solving the problem.","I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let be the normal vector to a surface at a point , and let and be tangent vectors at the point such that . Given an invertible 3 3 matrix , show that , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix . The author provided a hint stating we can represent as We then find a matrix such that where and show that to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: At this point, I do not know how to proceed with showing that . How would you proceed? I'd like to ask for hints on solving the problem.","N P S T P S \times T = N \times M (MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T) M (MS) \times (MT) 
(MS) \times (MT) =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
MT
 G 
GU
=
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
 
U =
\begin{bmatrix}
0 & -S_{z} & S_{y} \\ 
S_{z} & 0 & -S_{x} \\ 
-S_{y} & S_{x} & 0 
\end{bmatrix}
 G = (\text{det}M)(M^{-1})^{T} 
G =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
U^{-1}
 G = (\text{det}M)(M^{-1})^{T}","['matrices', 'transformation']"
15,Upper bound on the $\|\cdot\|_2$ norm of a tridiagonal matrix,Upper bound on the  norm of a tridiagonal matrix,\|\cdot\|_2,"Let $T\in M_{n}(\mathbb{R})$ be a tridiagonal matrix. What can we say about operator norm $\|T\|_2$ ? I'm asking this question because we know that if $T$ were only diagonal, then $\|T\|_2$ is the largest absolute value of any diagonal entry of $T$ , as shown here , and so there might also a similar result for tridiagonal or $k$ -diagonal matrices in general.","Let be a tridiagonal matrix. What can we say about operator norm ? I'm asking this question because we know that if were only diagonal, then is the largest absolute value of any diagonal entry of , as shown here , and so there might also a similar result for tridiagonal or -diagonal matrices in general.",T\in M_{n}(\mathbb{R}) \|T\|_2 T \|T\|_2 T k,"['linear-algebra', 'matrices', 'matrix-norms', 'tridiagonal-matrices', 'spectral-norm']"
16,a problem regarding projection maps on finite dimensional vector space,a problem regarding projection maps on finite dimensional vector space,,"Let $V$ be a finite dimensional vector space over a field $F$ of characteristic zero. Let $E_1 , E_2, ...,E_k$ be projections of $V$ such that $E_1+E_2+...+E_k=I$ . Show that $E_iE_j = 0$ for all $i\neq j$ . Hint: Use the trace function. Using the hint I got that $\operatorname{trace}(E_i)=\dim(\operatorname{range}(E_i))$ for all $i,\;1\le i\le k$ . Again $I=E_1+E_2+\cdots+E_k \Rightarrow V=\operatorname{range}(E_1)+\operatorname{range}(E_2)+\cdots+\operatorname{range}(E_k)$ Combining both we get $V=\operatorname{range}(E_1)\oplus\operatorname{range}(E_2)\oplus\cdots\oplus\operatorname{range}(E_k)$ After this step I am not being able to progress further. Please help me. Though this problem has already been discussed in a earlier post, there was no hint regarding this approach. So I am posting this problem again. Please do not mark this problem as duplicate. Thank you in advance.","Let be a finite dimensional vector space over a field of characteristic zero. Let be projections of such that . Show that for all . Hint: Use the trace function. Using the hint I got that for all . Again Combining both we get After this step I am not being able to progress further. Please help me. Though this problem has already been discussed in a earlier post, there was no hint regarding this approach. So I am posting this problem again. Please do not mark this problem as duplicate. Thank you in advance.","V F E_1 , E_2, ...,E_k V E_1+E_2+...+E_k=I E_iE_j = 0 i\neq j \operatorname{trace}(E_i)=\dim(\operatorname{range}(E_i)) i,\;1\le i\le k I=E_1+E_2+\cdots+E_k \Rightarrow V=\operatorname{range}(E_1)+\operatorname{range}(E_2)+\cdots+\operatorname{range}(E_k) V=\operatorname{range}(E_1)\oplus\operatorname{range}(E_2)\oplus\cdots\oplus\operatorname{range}(E_k)","['linear-algebra', 'matrices']"
17,Let $T$ be the linear operator on $M_{n}(\textbf{R})$ defined by $T(A) = A^{t}$. Find a basis $\mathcal{B}$ s.t. $[T]_{\mathcal{B}}$ is diagonal.,Let  be the linear operator on  defined by . Find a basis  s.t.  is diagonal.,T M_{n}(\textbf{R}) T(A) = A^{t} \mathcal{B} [T]_{\mathcal{B}},"Let $T$ be the linear operator on $M_{n\times n}(\textbf{R})$ defined by $T(A) = A^{t}$ . (a) Show that $\pm 1$ are the only eigenvalues of $T$ . (b) Describe the eigenvectors corresponding to each eigenvalue of $T$ . (c) Find an ordered basis $\mathcal{B}$ for $M_{2\times 2}(\textbf{R})$ such that $[T]_{\mathcal{B}}$ is a diagonal matrix. (d) Find an ordered basis $\mathcal{B}$ for $M_{n\times n}(\textbf{R})$ such that $[T]_{\mathcal{B}}$ is a diagonal matrix for $n > 2$ . MY (NEW) ATTEMPT (a) Let $A\in M_{n\times n}(\textbf{R})$ s.t. $A \neq 0$ . Then $A$ is an eigenvector of $T$ iff $T(A) = \lambda A$ , that is to say, $A^{t} = \lambda A$ . Based on it, we conclude that $A = \lambda A^{t}$ . Finally, one has that \begin{align*} A^{t} = \lambda(\lambda A^{t}) \Rightarrow A^{t} = \lambda^{2}A^{t} \Rightarrow \det(A^{t}) = \lambda^{2n}\det(A^{t}) \Rightarrow \lambda^{2n} = 1 \Rightarrow \lambda = \pm 1 \end{align*} But I still do not know how to justify the implication where we consider $\det(A^{t}) \neq 0$ Any help in this sense is appreciated. (b) For $\lambda = 1$ , we have that $T(A) = A^{t} = A$ . Thus the eigenvectors corresponding to $\lambda = 1$ belongs to the subspace of symmetric $n\times n$ matrices. For $\lambda = -1$ , we have that $T(A) = A^{t} = -A$ . Thus the eigenvectors corresponding to $\lambda = -1$ belongs to the subspace of skew-symmetric $n\times n$ matrices. (c) Let us determine the eigenspace associated to each eigenvalue. We shall start with $\lambda =1$ : \begin{align*} T(A) = 1\cdot A \Rightarrow A^{t} = A \Rightarrow \begin{bmatrix} a & c\\ b & d \end{bmatrix} = \begin{bmatrix} a & b\\ c & d \end{bmatrix} \Rightarrow b = c \end{align*} Thus the eigenspace associated to the eigenvalue $\lambda = 1$ is spanned by \begin{align*} E_{1} = \left\{\begin{bmatrix} 1 & 0\\ 0 & 0\\ \end{bmatrix}, \begin{bmatrix} 0 & 1\\ 1 & 0\\ \end{bmatrix}, \begin{bmatrix} 0 & 0\\ 0 & 1\\ \end{bmatrix}\right\} \end{align*} We may now proceed and determine the eigenspace associated to $\lambda = -1$ : \begin{align*} T(A) = -1\cdot A \Rightarrow A^{t} = -A \Rightarrow \begin{bmatrix} a & c\\ b & d \end{bmatrix} = \begin{bmatrix} -a & -b\\ -c & -d \end{bmatrix} \Rightarrow b = -c,\,a = 0,\,d = 0. \end{align*} Thus the eigenspace associated to the eigenvalue $\lambda = -1$ is spanned by \begin{align*} E_{-1} = \left\{\begin{bmatrix} 0 & 1\\ -1 & 0\\ \end{bmatrix}\right\} \end{align*} Since $E_{1}\cup E_{-1}$ is LI and it spans $M_{2\times 2}(\textbf{R})$ , we conclude that $\mathcal{B} = E_{1}\cup E_{-1}$ is a basis indeed. (d) The same reasoning applies to this case. Precisely speaking, there are $\displaystyle\dim\text{span}\{E_{1}\} = \frac{n(n+1)}{2}$ symmetric matrices and $\displaystyle\dim\text{span}\{E_{-1}\} = \frac{n(n-1)}{2}$ skew-symmetric matrices in the corresponding bases. COMMENT This question has already been asked here , but I'd like to know if my new approach is fine. Any comments on the wording of my solution would be appreciated.","Let be the linear operator on defined by . (a) Show that are the only eigenvalues of . (b) Describe the eigenvectors corresponding to each eigenvalue of . (c) Find an ordered basis for such that is a diagonal matrix. (d) Find an ordered basis for such that is a diagonal matrix for . MY (NEW) ATTEMPT (a) Let s.t. . Then is an eigenvector of iff , that is to say, . Based on it, we conclude that . Finally, one has that But I still do not know how to justify the implication where we consider Any help in this sense is appreciated. (b) For , we have that . Thus the eigenvectors corresponding to belongs to the subspace of symmetric matrices. For , we have that . Thus the eigenvectors corresponding to belongs to the subspace of skew-symmetric matrices. (c) Let us determine the eigenspace associated to each eigenvalue. We shall start with : Thus the eigenspace associated to the eigenvalue is spanned by We may now proceed and determine the eigenspace associated to : Thus the eigenspace associated to the eigenvalue is spanned by Since is LI and it spans , we conclude that is a basis indeed. (d) The same reasoning applies to this case. Precisely speaking, there are symmetric matrices and skew-symmetric matrices in the corresponding bases. COMMENT This question has already been asked here , but I'd like to know if my new approach is fine. Any comments on the wording of my solution would be appreciated.","T M_{n\times n}(\textbf{R}) T(A) = A^{t} \pm 1 T T \mathcal{B} M_{2\times 2}(\textbf{R}) [T]_{\mathcal{B}} \mathcal{B} M_{n\times n}(\textbf{R}) [T]_{\mathcal{B}} n > 2 A\in M_{n\times n}(\textbf{R}) A
\neq 0 A T T(A) = \lambda A A^{t} = \lambda A A = \lambda A^{t} \begin{align*}
A^{t} = \lambda(\lambda A^{t}) \Rightarrow A^{t} = \lambda^{2}A^{t} \Rightarrow \det(A^{t}) = \lambda^{2n}\det(A^{t}) \Rightarrow \lambda^{2n} = 1 \Rightarrow \lambda = \pm 1
\end{align*} \det(A^{t}) \neq 0 \lambda = 1 T(A) = A^{t} = A \lambda = 1 n\times n \lambda = -1 T(A) = A^{t} = -A \lambda = -1 n\times n \lambda =1 \begin{align*}
T(A) = 1\cdot A \Rightarrow A^{t} = A \Rightarrow
\begin{bmatrix}
a & c\\
b & d
\end{bmatrix} =
\begin{bmatrix}
a & b\\
c & d
\end{bmatrix} \Rightarrow b = c
\end{align*} \lambda = 1 \begin{align*}
E_{1} = \left\{\begin{bmatrix}
1 & 0\\
0 & 0\\
\end{bmatrix},
\begin{bmatrix}
0 & 1\\
1 & 0\\
\end{bmatrix},
\begin{bmatrix}
0 & 0\\
0 & 1\\
\end{bmatrix}\right\}
\end{align*} \lambda = -1 \begin{align*}
T(A) = -1\cdot A \Rightarrow A^{t} = -A \Rightarrow \begin{bmatrix}
a & c\\
b & d
\end{bmatrix} =
\begin{bmatrix}
-a & -b\\
-c & -d
\end{bmatrix} \Rightarrow b = -c,\,a = 0,\,d = 0.
\end{align*} \lambda = -1 \begin{align*}
E_{-1} = \left\{\begin{bmatrix}
0 & 1\\
-1 & 0\\
\end{bmatrix}\right\}
\end{align*} E_{1}\cup E_{-1} M_{2\times 2}(\textbf{R}) \mathcal{B} = E_{1}\cup E_{-1} \displaystyle\dim\text{span}\{E_{1}\} = \frac{n(n+1)}{2} \displaystyle\dim\text{span}\{E_{-1}\} = \frac{n(n-1)}{2}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'solution-verification']"
18,Using permutations to find the determinant. [duplicate],Using permutations to find the determinant. [duplicate],,"This question already has answers here : If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ (7 answers) Closed 3 years ago . I came across the following problem Let matrix A be a $n\times n$ square matrix such that $a_{ij}$ = max{ i , j }. Prove that det(A) = $(-1)^{n-1}n$ I have read the post If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ and understood all the answers adressing the same problem. I just wanted to see an alternative method which does not use row subtractions . As, the determinant fromulla $n(-1)^{n-1}$ is reminiscent of $\frac{d}{dx}(x^{n})$ Is there some way we could use differentiation of determinant technique? How would it be if one goes by the formulla of determinant in terms of permutations?","This question already has answers here : If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ (7 answers) Closed 3 years ago . I came across the following problem Let matrix A be a square matrix such that = max{ i , j }. Prove that det(A) = I have read the post If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ and understood all the answers adressing the same problem. I just wanted to see an alternative method which does not use row subtractions . As, the determinant fromulla is reminiscent of Is there some way we could use differentiation of determinant technique? How would it be if one goes by the formulla of determinant in terms of permutations?",n\times n a_{ij} (-1)^{n-1}n n(-1)^{n-1} \frac{d}{dx}(x^{n}),"['linear-algebra', 'matrices', 'permutations', 'determinant']"
19,"For real symmetric $n$-by-$n$ matrices $A, B$ with $a_{ii}=b_{ii}=1$ and $0\leq a_{ij}\leq b_{ij}\leq1$, if $B$ is positive-definite, then so is $A$","For real symmetric -by- matrices  with  and , if  is positive-definite, then so is","n n A, B a_{ii}=b_{ii}=1 0\leq a_{ij}\leq b_{ij}\leq1 B A","Consider two real $n \times n$ symmetric matrices $A$ and $B$ , where $n$ is a given positive integer. Write the elements in $A$ and $B$ as $\{a_{ij}\}$ and $\{b_{ij}\}$ , where $1 \leq i \leq n$ and $1 \leq j \leq n$ . Assume that $a_{ii}=b_{ii}=1$ for $1 \leq i \leq n$ and $0 \leq a_{ij} \leq b_{ij} \leq 1$ for all $1 \leq i<j \leq n$ . We want to show the following: If $B$ is positive-definite, then $A$ is also positive-definite. Actually, from Sylvester's criterion, we can reduce the problem to prove the following (This is equivalent to our initial question): If the determinant of $B$ is a positive real number, then the determinant of $A$ is also a positive real number. Thus, for $n=2$ case, the proof is just a direct calculation. However, for $n \geq 3$ , the calculation becomes difficult and it is hard to prove the result in general. Any ideas and comments on the problem are very welcome. Thanks a lot!","Consider two real symmetric matrices and , where is a given positive integer. Write the elements in and as and , where and . Assume that for and for all . We want to show the following: If is positive-definite, then is also positive-definite. Actually, from Sylvester's criterion, we can reduce the problem to prove the following (This is equivalent to our initial question): If the determinant of is a positive real number, then the determinant of is also a positive real number. Thus, for case, the proof is just a direct calculation. However, for , the calculation becomes difficult and it is hard to prove the result in general. Any ideas and comments on the problem are very welcome. Thanks a lot!",n \times n A B n A B \{a_{ij}\} \{b_{ij}\} 1 \leq i \leq n 1 \leq j \leq n a_{ii}=b_{ii}=1 1 \leq i \leq n 0 \leq a_{ij} \leq b_{ij} \leq 1 1 \leq i<j \leq n B A B A n=2 n \geq 3,"['linear-algebra', 'matrices', 'analysis', 'symmetry', 'symmetric-matrices']"
20,"Etymology of the term ""positive definite""","Etymology of the term ""positive definite""",,"According to Wikipedia In linear algebra, a symmetric $n\times n$ real matrix $M$ is said to be positive-definite if the scalar ${\displaystyle z^{\textsf {T}}Mz}$ is strictly positive for every non-zero column vector $z$ of $n$ real numbers. From this definition, I understand how the label ""positive"" fits. However, I don't see how this makes a matrix ""definite"", in an intuitive sense. In everyday life, I think of something ""definite"" as being ""not vague"". Is there any connection between this understanding and the mathematical definition? And where did this terminology get established?","According to Wikipedia In linear algebra, a symmetric real matrix is said to be positive-definite if the scalar is strictly positive for every non-zero column vector of real numbers. From this definition, I understand how the label ""positive"" fits. However, I don't see how this makes a matrix ""definite"", in an intuitive sense. In everyday life, I think of something ""definite"" as being ""not vague"". Is there any connection between this understanding and the mathematical definition? And where did this terminology get established?",n\times n M {\displaystyle z^{\textsf {T}}Mz} z n,"['matrices', 'terminology', 'positive-definite']"
21,Is a circulant matrix irreducible?,Is a circulant matrix irreducible?,,"A circulant matrix $\mathbf{C} \in \mathbb{R}^{n \times n}$ is of the form \begin{equation} \label{circulantmat}     \mathbf{C} = \begin{pmatrix}      {c_0} & {c_1} & {\dots} & {c_{n-2}} & {c_{n-1}} \\     {c_{n-1}} & {c_0} & {c_1} & {} & {c_{n-2}} \\     {\vdots} & {c_{n-1}} & {c_0} & {\ddots} & {\vdots} \\     {c_2} & {} & {\ddots} & {\ddots} & {c_1} \\     {c_1} & {c_2} & {\dots} & {c_{n-1}} & {c_0} \\     \end{pmatrix}. \end{equation} In my case, I have that $\mathbf{C}$ has zero entries everywhere except $c_0$ , $c_1$ and $c_{n-1}$ , which are positive. I know that $\mathbf{C}$ is irreducible if and only if \begin{equation}     (\mathbf{I}_n + \mathbf{C})^{n-1} > 0. \end{equation} Is there a way to show that the above holds? By intuition, I know that as the non-zero entries of the circulant matrix neighbor each other, the directed graph of the adjacency matrix of $\mathbf{C}$ would be strongly connected, but I want to show this more mathematically. I have other circulant matrices where more neighboring points are non-zero, which I assume would mean that a similar result would hold.","A circulant matrix is of the form In my case, I have that has zero entries everywhere except , and , which are positive. I know that is irreducible if and only if Is there a way to show that the above holds? By intuition, I know that as the non-zero entries of the circulant matrix neighbor each other, the directed graph of the adjacency matrix of would be strongly connected, but I want to show this more mathematically. I have other circulant matrices where more neighboring points are non-zero, which I assume would mean that a similar result would hold.","\mathbf{C} \in \mathbb{R}^{n \times n} \begin{equation}
\label{circulantmat}
    \mathbf{C} = \begin{pmatrix} 
    {c_0} & {c_1} & {\dots} & {c_{n-2}} & {c_{n-1}} \\
    {c_{n-1}} & {c_0} & {c_1} & {} & {c_{n-2}} \\
    {\vdots} & {c_{n-1}} & {c_0} & {\ddots} & {\vdots} \\
    {c_2} & {} & {\ddots} & {\ddots} & {c_1} \\
    {c_1} & {c_2} & {\dots} & {c_{n-1}} & {c_0} \\
    \end{pmatrix}.
\end{equation} \mathbf{C} c_0 c_1 c_{n-1} \mathbf{C} \begin{equation}
    (\mathbf{I}_n + \mathbf{C})^{n-1} > 0.
\end{equation} \mathbf{C}","['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-calculus']"
22,On a real square matrix of order $10$,On a real square matrix of order,10,"Let $M_{10}$ be the set of $10×10$ real matrices; if $U\in M_{10}$ , then let $\rho(U)=rank(U)$ . Which of the followings are true for every $A\in M_{10}$ ? $(1)\rho(A^8)=\rho(A^9)$ $(2)\rho(A^9)=\rho(A^{10})$ $(3)\rho(A^{10})=\rho(A^{11})$ $(4)\rho(A^8)=\rho(A^7)$ I am able to discard options $(1),(2)$ & $ (4)$ by taking nilpotent matrices of order $9,10$ & $8$ respectively. This leaves $(3)$ as true but I am finding it difficult to write a general proof. I think characteristics polynomial can be used. Can you give any suggestions? Thanks for your time.","Let be the set of real matrices; if , then let . Which of the followings are true for every ? I am able to discard options & by taking nilpotent matrices of order & respectively. This leaves as true but I am finding it difficult to write a general proof. I think characteristics polynomial can be used. Can you give any suggestions? Thanks for your time.","M_{10} 10×10 U\in M_{10} \rho(U)=rank(U) A\in M_{10} (1)\rho(A^8)=\rho(A^9) (2)\rho(A^9)=\rho(A^{10}) (3)\rho(A^{10})=\rho(A^{11}) (4)\rho(A^8)=\rho(A^7) (1),(2)  (4) 9,10 8 (3)","['linear-algebra', 'matrices', 'solution-verification', 'matrix-rank', 'characteristic-polynomial']"
23,How to find the determinant A from an equation having A as variable?,How to find the determinant A from an equation having A as variable?,,"I'm currently struggling because I can't find the answer do this. If anyone can help me, it would be great. A is a $5\times 5$ non scalar matrix, $(A+2)(A+A^3+1)^2 (A^2+A^3+1)^3 =0 $ a) Find det(A) b) Is A diagonalizable or not? Explain.","I'm currently struggling because I can't find the answer do this. If anyone can help me, it would be great. A is a non scalar matrix, a) Find det(A) b) Is A diagonalizable or not? Explain.",5\times 5 (A+2)(A+A^3+1)^2 (A^2+A^3+1)^3 =0 ,"['linear-algebra', 'matrices', 'determinant', 'diagonalization']"
24,Does polynomial generated by repeated application of matrix divide characteristic polynomial?,Does polynomial generated by repeated application of matrix divide characteristic polynomial?,,"Given a square matrix $A\in \mathbb C^{n,n}$ and a vector $v\ne0$ , the vectors $$ v, Av, A^2 v, \dots, A^n v $$ are linearly dependent. Let now $m\le n$ be the smallest number such that $$ v, Av, A^2 v, \dots, A^m v $$ are linearly dependent.  Then there are coefficients, not all of them zero, such that $$ \sum_{i=0}^m a_i A^iv=0, $$ or equivalently, $$ p(A)v=0 $$ for $p$ given by $p= \sum_{i=0}^m a_i t^i$ . My question is: does this polynomial divide the characteristic polynomial $p_A$ of $A$ ? Of course, $p$ and $p_A$ share a non-trivial factor. I think a proof of the claim above can be achieved using Jordan decomposition, but it looks like it would be complicated. Is there a more elementary proof? Does such a proof also work for other fields different from $\mathbb R,\mathbb C$ ? (Using this polynomial is the way how Axler proves existence of eigenvectors for the complex case and existence of small invariant subspaces for the real case. )","Given a square matrix and a vector , the vectors are linearly dependent. Let now be the smallest number such that are linearly dependent.  Then there are coefficients, not all of them zero, such that or equivalently, for given by . My question is: does this polynomial divide the characteristic polynomial of ? Of course, and share a non-trivial factor. I think a proof of the claim above can be achieved using Jordan decomposition, but it looks like it would be complicated. Is there a more elementary proof? Does such a proof also work for other fields different from ? (Using this polynomial is the way how Axler proves existence of eigenvectors for the complex case and existence of small invariant subspaces for the real case. )","A\in \mathbb C^{n,n} v\ne0 
v, Av, A^2 v, \dots, A^n v
 m\le n 
v, Av, A^2 v, \dots, A^m v
 
\sum_{i=0}^m a_i A^iv=0,
 
p(A)v=0
 p p= \sum_{i=0}^m a_i t^i p_A A p p_A \mathbb R,\mathbb C","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
25,Is there a polynomial $p$ such that $x^n$ divides $1+x-p^2$??,Is there a polynomial  such that  divides ??,p x^n 1+x-p^2,"I'm studying Ring Theory and I have a list of exercises to do, actually I'm trying to prove that for each nilpotent matrix $A \in F^{n^2}$ , there is a matrix $N$ s.t $I_n+A = N^2$ . So, if we have for $n \geq 2$ a polynomial $p(x)$ s.t $(1+x - p^2) = x^n q(x)$ , then applying $A$ , we have $N= p(A)$ . May you help me with a tip about $p$ ??","I'm studying Ring Theory and I have a list of exercises to do, actually I'm trying to prove that for each nilpotent matrix , there is a matrix s.t . So, if we have for a polynomial s.t , then applying , we have . May you help me with a tip about ??",A \in F^{n^2} N I_n+A = N^2 n \geq 2 p(x) (1+x - p^2) = x^n q(x) A N= p(A) p,"['matrices', 'polynomials', 'ring-theory']"
26,Pseudoinverse of block diagonal matrix,Pseudoinverse of block diagonal matrix,,"Suppose I have some block diagonal matrix $A$ , defined as: $A = \begin{bmatrix} A_1 & 0 & ... & 0 \\ 0 & A_2 & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n \end{bmatrix}$ Where $\{A_i\}_{i \in \{1,...n\}}$ are the blocks of $A$ . Is it true that the Pseudo-inverse of $A$ , $A^+$ , is given by: $A^+ = \begin{bmatrix} A_1^+ & 0 & ... & 0 \\ 0 & A_2^+ & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n^+ \end{bmatrix}$ If so, why?/why not?","Suppose I have some block diagonal matrix , defined as: Where are the blocks of . Is it true that the Pseudo-inverse of , , is given by: If so, why?/why not?","A A = \begin{bmatrix} A_1 & 0 & ... & 0 \\ 0 & A_2 & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n \end{bmatrix} \{A_i\}_{i \in \{1,...n\}} A A A^+ A^+ = \begin{bmatrix} A_1^+ & 0 & ... & 0 \\ 0 & A_2^+ & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & A_n^+ \end{bmatrix}","['matrices', 'matrix-equations', 'matrix-rank', 'pseudoinverse']"
27,Derivative of a scalar-valued function of a matrix,Derivative of a scalar-valued function of a matrix,,"Consider a scalar-valued function of a Matrix: $$s = g(\mathbf{T})$$ where $\mathbf{T}$ is a matrix. Now consider $\mathbf{T}$ is also a function of a scalar variable $t$ : $$s = g(\mathbf{T}(t))$$ The goal is to find the derivative of $s$ with respect to $t$ . I approached this problem using the chain rule: $$\dot s=\frac{\partial g}{\partial t}=\frac{\partial g}{\partial \mathbf{T}}\cdot\frac{\partial \mathbf{T}}{\partial t}$$ The problem is, the above expression results in a matrix, whereas I am expecting a scalar. Where am I wrong?","Consider a scalar-valued function of a Matrix: where is a matrix. Now consider is also a function of a scalar variable : The goal is to find the derivative of with respect to . I approached this problem using the chain rule: The problem is, the above expression results in a matrix, whereas I am expecting a scalar. Where am I wrong?",s = g(\mathbf{T}) \mathbf{T} \mathbf{T} t s = g(\mathbf{T}(t)) s t \dot s=\frac{\partial g}{\partial t}=\frac{\partial g}{\partial \mathbf{T}}\cdot\frac{\partial \mathbf{T}}{\partial t},"['calculus', 'matrices', 'matrix-calculus']"
28,number of parameters of $SO(3)$ group,number of parameters of  group,SO(3),"The $SO(3)$ rotation group is defined by: $A\cdot A^T=\mathbb{1}$ and $\det{A}=1$ . The group is supposed to have 3 free parameters, as suggested by Euler's angles. However, I am doing something wrong with counting of the parameters because I cannot arrive at that number. $3\times3$ matrix has $9$ free parameters. The requirement $A\cdot A^T=\mathbb{1}$ produces 6 equations for those parameters: $$ \vec{a_1}\cdot\vec{a_1}=1\\ \vec{a_1}\cdot\vec{a_2}=0\\ \vec{a_1}\cdot\vec{a_3}=0\\ \vec{a_2}\cdot\vec{a_2}=1\\ \vec{a_2}\cdot\vec{a_3}=0\\ \vec{a_3}\cdot\vec{a_3}=1\\ $$ ( $\vec{a_i}$ being the columns of $A$ ) and reduces the number of free parameters to 3. But with this requirement $\det{A}=\pm 1$ and so $\det{A}=1$ presents another restricting euqation, and so i get 2 free parameters. If I do a similar counting for the $SU(2)$ group, i have 8 free parameters of $2\times2$ complex matrix, the requirement $A\cdot A^\dagger=\mathbb{1}$ provides 4 independent equations and the determinant provides 2 more conditions (complex and imaginary parts) which again reduces to only 2 free parameters. Where am i doing the mistake in counting the parameters and equations?","The rotation group is defined by: and . The group is supposed to have 3 free parameters, as suggested by Euler's angles. However, I am doing something wrong with counting of the parameters because I cannot arrive at that number. matrix has free parameters. The requirement produces 6 equations for those parameters: ( being the columns of ) and reduces the number of free parameters to 3. But with this requirement and so presents another restricting euqation, and so i get 2 free parameters. If I do a similar counting for the group, i have 8 free parameters of complex matrix, the requirement provides 4 independent equations and the determinant provides 2 more conditions (complex and imaginary parts) which again reduces to only 2 free parameters. Where am i doing the mistake in counting the parameters and equations?","SO(3) A\cdot A^T=\mathbb{1} \det{A}=1 3\times3 9 A\cdot A^T=\mathbb{1} 
\vec{a_1}\cdot\vec{a_1}=1\\
\vec{a_1}\cdot\vec{a_2}=0\\
\vec{a_1}\cdot\vec{a_3}=0\\
\vec{a_2}\cdot\vec{a_2}=1\\
\vec{a_2}\cdot\vec{a_3}=0\\
\vec{a_3}\cdot\vec{a_3}=1\\
 \vec{a_i} A \det{A}=\pm 1 \det{A}=1 SU(2) 2\times2 A\cdot A^\dagger=\mathbb{1}","['linear-algebra', 'matrices', 'group-theory', 'finite-groups', 'lie-groups']"
29,Number of $2 \times 2$ matrices over the finite field $\mathbb{F}_q$ whose minimal polynomial is divisible by $X-1$.,Number of  matrices over the finite field  whose minimal polynomial is divisible by .,2 \times 2 \mathbb{F}_q X-1,"I want to calculate the number of $2 \times 2$ matrices over the finite field $\mathbb{F}_q$ whose minimal polynomial is divisible by $X-1$ . The characteristic polynomial must be $(X-1)(X-a)$ for some $a \in \mathbb{F}_q$ . If $a \neq 1$ then each of them is similar to $\text{diag}(1,a)$ . but how do I count the number of matrices in this case ? Another case if $a=1$ , then either the matrix is $2 \times 2 $ identity or similar to its JCF which is upper triangular with all nonzero entry $1$ . But again how do I count total number of matrices in each cases except the identity situation ? I need help.","I want to calculate the number of matrices over the finite field whose minimal polynomial is divisible by . The characteristic polynomial must be for some . If then each of them is similar to . but how do I count the number of matrices in this case ? Another case if , then either the matrix is identity or similar to its JCF which is upper triangular with all nonzero entry . But again how do I count total number of matrices in each cases except the identity situation ? I need help.","2 \times 2 \mathbb{F}_q X-1 (X-1)(X-a) a \in \mathbb{F}_q a \neq 1 \text{diag}(1,a) a=1 2 \times 2  1","['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'finite-fields']"
30,Prove it is a matrix group.,Prove it is a matrix group.,,"Let $K$ be a field and let $X ∈ K^{n×n}$ be a matrix. Show   that the set $$U := \{A \in GL_n(K) | A^ TXA = X\}$$ is a group with respect to matrix multiplication. For this exercise,  associativity is pretty self explanatory since by nature matrices are associative. I'm confused about how to show that it has an inverse . For Identity, I think I would go about $A^ TXAI = XI=X$ but I'm not sure if it is correct.","Let be a field and let be a matrix. Show   that the set is a group with respect to matrix multiplication. For this exercise,  associativity is pretty self explanatory since by nature matrices are associative. I'm confused about how to show that it has an inverse . For Identity, I think I would go about but I'm not sure if it is correct.","K X ∈ K^{n×n} U := \{A \in GL_n(K) | A^
TXA = X\} A^
TXAI = XI=X","['linear-algebra', 'matrices', 'group-theory', 'proof-explanation', 'solution-verification']"
31,The gradient of an element wise matrix function,The gradient of an element wise matrix function,,Let $F = F[A]$ represent an element wise function $F$ applied to matrix $A$ . Here $F_{ij} = f(A_{ij})$ where $f$ is a scalar function. I would like to derive an expression for $\frac{\partial F}{\partial A}$ . My strategy was to use summation notation: $$\frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \frac{\partial A_{ij}}{\partial A_{pq}} $$ $$\frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \delta_{ip} \delta_{jq}$$ I know there should be 4th order tensor result but the implied sum is throwing me off. I am not too familiar with matrix manipulation when there are tensors of order 3 and higher so I did not try to construct a differential. Any walkthroughs/strategies would be much appreciated!,Let represent an element wise function applied to matrix . Here where is a scalar function. I would like to derive an expression for . My strategy was to use summation notation: I know there should be 4th order tensor result but the implied sum is throwing me off. I am not too familiar with matrix manipulation when there are tensors of order 3 and higher so I did not try to construct a differential. Any walkthroughs/strategies would be much appreciated!,F = F[A] F A F_{ij} = f(A_{ij}) f \frac{\partial F}{\partial A} \frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \frac{\partial A_{ij}}{\partial A_{pq}}  \frac{\partial F_{ij}}{\partial A_{pq}} = \frac{\partial f}{\partial A_{ij}} \delta_{ip} \delta_{jq},"['calculus', 'linear-algebra', 'matrices', 'matrix-calculus']"
32,How to compute the derivative of the following function with matrix $X$ as the variable?,How to compute the derivative of the following function with matrix  as the variable?,X,"The function is : $f(X)$ =Trace( 1 $^{T}_{n\times m}$ $\sqrt{(AX)^2+(XB^T)^2}$ ) 1 $_{n\times m}$ is a matrix with all the elements equal to 1. $A$ is an $n \times n$ matrix. $X$ is an $n \times m$ matrix. $B$ is an $m \times m$ matrix How to compute d $f(X)$ /d $X$ ? In $\sqrt{(AX)^2+(XB^T)^2}$ , the square root and square are defined element-wise.","The function is : =Trace( 1 ) 1 is a matrix with all the elements equal to 1. is an matrix. is an matrix. is an matrix How to compute d /d ? In , the square root and square are defined element-wise.",f(X) ^{T}_{n\times m} \sqrt{(AX)^2+(XB^T)^2} _{n\times m} A n \times n X n \times m B m \times m f(X) X \sqrt{(AX)^2+(XB^T)^2},"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
33,Assume $\lambda_\min(A_kA_k^{\rm T})>\varepsilon$ and show that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded,Assume  and show that  is bounded,\lambda_\min(A_kA_k^{\rm T})>\varepsilon A_k^{\rm T}(A_kA_k^{\rm T})^{-1},"For all $k\in\mathbb{N}$ , let $A_k\in\mathbb{R}^{n\times m}$ , where $n\leq m$ , and assume that there exists $\varepsilon>0$ such that for all $k\in\mathbb{N}$ , $\lambda_\min(A_kA_k^{\rm T})>\varepsilon$ , where $\lambda_\min$ denotes the minimum eigenvalue of a symmetric positive semidefinite matrix. Can we show that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded? For the case where $n=m$ , I can see that $A_k^{\rm T}(A_kA_k^{\rm T})^{-1}$ is bounded. My question is about the case where $n<m$ . Any hint is appreciated.","For all , let , where , and assume that there exists such that for all , , where denotes the minimum eigenvalue of a symmetric positive semidefinite matrix. Can we show that is bounded? For the case where , I can see that is bounded. My question is about the case where . Any hint is appreciated.",k\in\mathbb{N} A_k\in\mathbb{R}^{n\times m} n\leq m \varepsilon>0 k\in\mathbb{N} \lambda_\min(A_kA_k^{\rm T})>\varepsilon \lambda_\min A_k^{\rm T}(A_kA_k^{\rm T})^{-1} n=m A_k^{\rm T}(A_kA_k^{\rm T})^{-1} n<m,"['linear-algebra', 'matrices', 'inverse', 'matrix-rank']"
34,Proving $\det(AB) = \det(A) \det(B)$ with elementary matrices,Proving  with elementary matrices,\det(AB) = \det(A) \det(B),"I am trying to follow Artin's proof that $\det(AB) = \det(A) \det(B)$ , but many of the details are omitted, so I am having difficulty. I have established the following two lemmas. Lemma 1: $\det(EA) = \det(E) \det(A)$ for any elementary matrix $E$ and matrix $A$ . Lemma 2: For an invertible matrix $A = E_1 E_2 \cdots E_n$ , we have $$\det (A) = \det(E_1) \det(E_2) \cdots \det(E_n).$$ Artin uses these facts to conclude that $$\det(AB) = \det(E_1 \cdots E_k B) = \det(E_1) \cdots \det(E_n) \det(B). $$ I do not understand why this result follows from the lemmas. Help with this would be appreciated.","I am trying to follow Artin's proof that , but many of the details are omitted, so I am having difficulty. I have established the following two lemmas. Lemma 1: for any elementary matrix and matrix . Lemma 2: For an invertible matrix , we have Artin uses these facts to conclude that I do not understand why this result follows from the lemmas. Help with this would be appreciated.",\det(AB) = \det(A) \det(B) \det(EA) = \det(E) \det(A) E A A = E_1 E_2 \cdots E_n \det (A) = \det(E_1) \det(E_2) \cdots \det(E_n). \det(AB) = \det(E_1 \cdots E_k B) = \det(E_1) \cdots \det(E_n) \det(B). ,"['linear-algebra', 'matrices', 'proof-explanation', 'determinant']"
35,Find the $x$ and $y$ values that satisfy the constrained optimisation problem.,Find the  and  values that satisfy the constrained optimisation problem.,x y,"I have the following constrained optimisation problem. We have the following multivariable function: $$f(x,y) = 4x^2 +4y^2 +3xy -2x +4$$ This function is minimised on the line: $$0 = \vec n \ (\vec x -\vec p)= \begin{bmatrix}-1\\1\end{bmatrix} \left(\begin{bmatrix}x\\y\end{bmatrix}-\begin{bmatrix}-2\\3\end{bmatrix} \right)$$ I have to use the Lagrange Multipliers to solve this problem and get the $x$ and $y$ values. I'm not sure how to go about doing this problem because normally I work with $g(x,y)$ as a linear equation which equals a constant (i.e. $x+y=12$ ). My initial idea was to write down the Lagrange Multipliers, since $f$ is subject to the constraint $g(x,y) = 0$ and simplify the line equation into a polynomial function. The function is as follows: $$L(x, y, \lambda)=f(x,y)+\lambda (g(x,y)-c)$$ The line equation in its equivalent polynomial form: $$-x+y-5 \equiv y-x = 5$$ Thus $c = 5$ and we have: $$L(x,y, \lambda) = (4x^2+4y^2 +3xy -2x +4) - \lambda(-x+y-5)$$ From here I can find the gradient of $L$ , which would be: $$\nabla L= \begin{bmatrix}8x+3y-2+ \lambda\\8y +3x-\lambda\\x-y+5\end{bmatrix}$$ I then setup an augmented matrix and reduced it to RREF to solve for $x$ and $y$ . The augmented matrix $A$ is as follows: $$\left[\begin{array}{rrr|r} 8 & 3 & 1 & 2\\ 3 & 8 & -1 & 0\\ 1 & -1 & 0 & -5 \end{array}\right] \to  ... \to \left[\begin{array}{rrr|r} 1 & 0 & 0 & -\frac{55}{22}\\ 0 & 1 & 0 & -\frac{57}{22}\\ 0 & 0 & 1 & -\frac{27}{2} \end{array}\right]$$ Therefore, $x = -2.50$ and $y=-2.59$ . Hopefully this is correct but again, I'm not a 100% sure because of the line constraint. Edit: There is a computation error in reducing the augmented matrix (RREF). As stated in the comments and answers, the correct answers are $x=-2.41$ and $y=2.59$ .","I have the following constrained optimisation problem. We have the following multivariable function: This function is minimised on the line: I have to use the Lagrange Multipliers to solve this problem and get the and values. I'm not sure how to go about doing this problem because normally I work with as a linear equation which equals a constant (i.e. ). My initial idea was to write down the Lagrange Multipliers, since is subject to the constraint and simplify the line equation into a polynomial function. The function is as follows: The line equation in its equivalent polynomial form: Thus and we have: From here I can find the gradient of , which would be: I then setup an augmented matrix and reduced it to RREF to solve for and . The augmented matrix is as follows: Therefore, and . Hopefully this is correct but again, I'm not a 100% sure because of the line constraint. Edit: There is a computation error in reducing the augmented matrix (RREF). As stated in the comments and answers, the correct answers are and .","f(x,y) = 4x^2 +4y^2 +3xy -2x +4 0 = \vec n \ (\vec x -\vec p)= \begin{bmatrix}-1\\1\end{bmatrix} \left(\begin{bmatrix}x\\y\end{bmatrix}-\begin{bmatrix}-2\\3\end{bmatrix} \right) x y g(x,y) x+y=12 f g(x,y) = 0 L(x, y, \lambda)=f(x,y)+\lambda (g(x,y)-c) -x+y-5 \equiv y-x = 5 c = 5 L(x,y, \lambda) = (4x^2+4y^2 +3xy -2x +4) - \lambda(-x+y-5) L \nabla L= \begin{bmatrix}8x+3y-2+ \lambda\\8y +3x-\lambda\\x-y+5\end{bmatrix} x y A \left[\begin{array}{rrr|r}
8 & 3 & 1 & 2\\
3 & 8 & -1 & 0\\
1 & -1 & 0 & -5
\end{array}\right] \to  ... \to \left[\begin{array}{rrr|r}
1 & 0 & 0 & -\frac{55}{22}\\
0 & 1 & 0 & -\frac{57}{22}\\
0 & 0 & 1 & -\frac{27}{2}
\end{array}\right] x = -2.50 y=-2.59 x=-2.41 y=2.59","['real-analysis', 'linear-algebra', 'matrices', 'optimization', 'vectors']"
36,Notation to select column from matrix,Notation to select column from matrix,,"Consider a matrix $X \in \mathbb{R}_{n \times m}$ . One compact yet unclear notation to select a row or column from this matrix is: $$x \in X$$ How do you clearly select a row or column from a matrix? I know $X = (x_{ij})$ is a standard notation to select elements. Though I haven't seen this used, $x_i \in X_{ij}$ for rows and $x_j \in X_{ij}$ for columns might make sense. This is motivated by a similar notation I have seen, namely $\sum\limits_{i}X_{ij}$ for row sum or $\sum\limits_{j}X_{ij}$ for column sum.","Consider a matrix . One compact yet unclear notation to select a row or column from this matrix is: How do you clearly select a row or column from a matrix? I know is a standard notation to select elements. Though I haven't seen this used, for rows and for columns might make sense. This is motivated by a similar notation I have seen, namely for row sum or for column sum.",X \in \mathbb{R}_{n \times m} x \in X X = (x_{ij}) x_i \in X_{ij} x_j \in X_{ij} \sum\limits_{i}X_{ij} \sum\limits_{j}X_{ij},"['matrices', 'notation', 'vectors']"
37,Find $\Pr(\textbf{R}^2=\mathbf{0})$ if $\textbf{R}_{4\times 4}$ is a matrix with 1 in 2 random positions and zeros otherwise,Find  if  is a matrix with 1 in 2 random positions and zeros otherwise,\Pr(\textbf{R}^2=\mathbf{0}) \textbf{R}_{4\times 4},"Find $\Pr(\textbf{R}^2=\mathbf{0})$ if $\textbf{R}_{4\times 4}$ is a matrix with 1 in 2 random positions and zeros otherwise. (ITA entrance exam, Brazil, 2020) My attempt : let $\textbf{R}=[r_{ij}]$ and $\textbf{R}^2=[r^2_{ij}]$ , so that $$r^2_{ij}=\sum_{t=1}^4 r_{it} r_{tj},\forall i,j\in\{1,..,4\}\ \ \ (*).$$ (1) there are ${16\choose 2}=120$ ways to position the 2 ones in the matrix. And it is direct to see that if at least one 1 is in diagonal, $\textbf{R}^2\not =\mathbf{0}$ . (2) let us count the number of $\textbf{R}$ matrices leading to $\textbf{R}^2\not =\mathbf{0}$ considering  3 cases: Case 1 - Two 1s in the diagonal: ${4\choose 2}=6$ ways. Case 2 - One 1 in the diagonal  and the other off diagonal: ${4\choose 1}{12\choose 1}=48$ ways. Case 3 - Both ones off diagonal. Considering the notation on expression (*) above, let us consider $t=1$ for instance, so that $i$ and $j$ $\in \{2,3,4\}$ , to avoid elements in the diagonal. There are $3^2=9$ possible ways to have $r_{i1}r_{1j}=1$ in this case. Considering $t\in \{2,3,4\}$ , using the same argument, will lead to a a total of $4\times 3^2=36$ matrices $\textbf{R}$ with two 1s off diagonal leading to $\textbf{R}^2\not =\textbf{0}.$ By adding Cases 1 to 3, there are $6+48+36=90$ matrices $\textbf{R}$ , leading to $\textbf{R}^2\not =\textbf{0}$ or $120-90=30$ matrices $\textbf{R}$ with $\textbf{R}^2 =\textbf{0}.$ And the asked probability would be $30/120$ But this last answer is wrong as the argument for case 3 overcounts, as the total should be 30 and not 36, with final correct answer $36/120=3/10$ Asked: Please point me out the mistake I'm doing the counting in Case 3. And advise on an efficient and straightforward way to count cases in situations like this.","Find if is a matrix with 1 in 2 random positions and zeros otherwise. (ITA entrance exam, Brazil, 2020) My attempt : let and , so that (1) there are ways to position the 2 ones in the matrix. And it is direct to see that if at least one 1 is in diagonal, . (2) let us count the number of matrices leading to considering  3 cases: Case 1 - Two 1s in the diagonal: ways. Case 2 - One 1 in the diagonal  and the other off diagonal: ways. Case 3 - Both ones off diagonal. Considering the notation on expression (*) above, let us consider for instance, so that and , to avoid elements in the diagonal. There are possible ways to have in this case. Considering , using the same argument, will lead to a a total of matrices with two 1s off diagonal leading to By adding Cases 1 to 3, there are matrices , leading to or matrices with And the asked probability would be But this last answer is wrong as the argument for case 3 overcounts, as the total should be 30 and not 36, with final correct answer Asked: Please point me out the mistake I'm doing the counting in Case 3. And advise on an efficient and straightforward way to count cases in situations like this.","\Pr(\textbf{R}^2=\mathbf{0}) \textbf{R}_{4\times 4} \textbf{R}=[r_{ij}] \textbf{R}^2=[r^2_{ij}] r^2_{ij}=\sum_{t=1}^4 r_{it} r_{tj},\forall i,j\in\{1,..,4\}\ \ \ (*). {16\choose 2}=120 \textbf{R}^2\not =\mathbf{0} \textbf{R} \textbf{R}^2\not =\mathbf{0} {4\choose 2}=6 {4\choose 1}{12\choose 1}=48 t=1 i j \in \{2,3,4\} 3^2=9 r_{i1}r_{1j}=1 t\in \{2,3,4\} 4\times 3^2=36 \textbf{R} \textbf{R}^2\not =\textbf{0}. 6+48+36=90 \textbf{R} \textbf{R}^2\not =\textbf{0} 120-90=30 \textbf{R} \textbf{R}^2 =\textbf{0}. 30/120 36/120=3/10","['probability', 'combinatorics', 'matrices', 'contest-math']"
38,Higman’s Lemma for Matrices?,Higman’s Lemma for Matrices?,,"It is well-known that the set of finite sequences on a finite alphabet is well-quasi-ordered by the subsequence relation. Question : Is the set of finite matrices on a finite alphabet well-quasi-ordered by the submatrix relation? How to prove or disprove? In other words, if $(M_1, M_2,\dots)$ is an infinite sequence of finite matrices on a finite alphabet, is it necessary that there exist $i<j$ such that $M_i$ is a submatrix of $M_j$ ? (For matrices $A$ and $B$ , $A$ is said to be a submatrix of $B$ iff $A$ is obtainable by deleting some or no rows and/or columns of $B$ .) A general version of this question is posed (unanswered) as ""Exercise 1.12 (Higman’s Lemma for Matrices?)"" on p. 17 in Algorithmic Aspects of WQO Theory by Schmitz & Schnoebelen. (I have no idea how to approach this.)","It is well-known that the set of finite sequences on a finite alphabet is well-quasi-ordered by the subsequence relation. Question : Is the set of finite matrices on a finite alphabet well-quasi-ordered by the submatrix relation? How to prove or disprove? In other words, if is an infinite sequence of finite matrices on a finite alphabet, is it necessary that there exist such that is a submatrix of ? (For matrices and , is said to be a submatrix of iff is obtainable by deleting some or no rows and/or columns of .) A general version of this question is posed (unanswered) as ""Exercise 1.12 (Higman’s Lemma for Matrices?)"" on p. 17 in Algorithmic Aspects of WQO Theory by Schmitz & Schnoebelen. (I have no idea how to approach this.)","(M_1, M_2,\dots) i<j M_i M_j A B A B A B","['sequences-and-series', 'combinatorics', 'matrices', 'reference-request', 'order-theory']"
39,Square root of matrix over $F_p$,Square root of matrix over,F_p,"Let $M \in F_p^{n\times n}$ be a square matrix of dimension $n\times n$ with entries over $F_p$ . Then we want to obtain $M$ from $M^2=M\cdot M$ . Initially I think of the old diagonalisation procedure of solving the square root of a matrix by: $$M^2 = P\cdot D \cdot P^{-1}$$ $$M = P\cdot D^{\frac{1}{2}} \cdot P^{-1}$$ Where $D$ is a diagonal matrix with the eigenvalues as entries defined over $F_p$ . So $2^{-1} \pmod p$ must be a unit on $F_p^{*}$ , this is $2$ must be an unit on $F_p^{*}$ so it has an inverse. Moreover, when taking $F_2$ as the field, the diagonalisation method doesn't yield the square root matrix as $2$ is not an unit on $F_2^{*}$ . I know that if $M^k \in GL(n,F_2)$ then $\gcd(k,\vert GL(n,F_2) \vert)$ must be equal to $1$ so $M^{k\cdot d}=M$ where $d$ is the multiplicative inverse of $k$ modulo the order of the given general linear group. It works also with the multiplicative order of $M$ in $GL$ . But what if $k=2$ and the field is $F_2$ ? Is there any method for computing the square root matrix of $M^2$ over $F_2$ ?","Let be a square matrix of dimension with entries over . Then we want to obtain from . Initially I think of the old diagonalisation procedure of solving the square root of a matrix by: Where is a diagonal matrix with the eigenvalues as entries defined over . So must be a unit on , this is must be an unit on so it has an inverse. Moreover, when taking as the field, the diagonalisation method doesn't yield the square root matrix as is not an unit on . I know that if then must be equal to so where is the multiplicative inverse of modulo the order of the given general linear group. It works also with the multiplicative order of in . But what if and the field is ? Is there any method for computing the square root matrix of over ?","M \in F_p^{n\times n} n\times n F_p M M^2=M\cdot M M^2 = P\cdot D \cdot P^{-1} M = P\cdot D^{\frac{1}{2}} \cdot P^{-1} D F_p 2^{-1} \pmod p F_p^{*} 2 F_p^{*} F_2 2 F_2^{*} M^k \in GL(n,F_2) \gcd(k,\vert GL(n,F_2) \vert) 1 M^{k\cdot d}=M d k M GL k=2 F_2 M^2 F_2","['linear-algebra', 'matrices', 'group-theory', 'finite-fields', 'diagonalization']"
40,Rank-$1$ update of inverse of a matrix transpose-matrix product,Rank- update of inverse of a matrix transpose-matrix product,1,"I have a problem, I have access to a matrix $G = (A^t A)^{-1}$ , but I want to compute a second matrix $\bar{G} = (\bar{A}^t \bar{A})^{-1}$ , where $\bar{A} = A + b c^t$ is a rank one update of matrix $A$ . I was wondering if there exist a formula to easily obtain the second matrix from the first? I already know the Sherman-Morrison formula, I tried to use it by developing $\bar{G}$ into $ (A^t A + c b^t A + A^t b c^t + c b^t b c^t)^{-1}$ and recursively update it, but the result is too complicated. Thank you in advance.","I have a problem, I have access to a matrix , but I want to compute a second matrix , where is a rank one update of matrix . I was wondering if there exist a formula to easily obtain the second matrix from the first? I already know the Sherman-Morrison formula, I tried to use it by developing into and recursively update it, but the result is too complicated. Thank you in advance.",G = (A^t A)^{-1} \bar{G} = (\bar{A}^t \bar{A})^{-1} \bar{A} = A + b c^t A \bar{G}  (A^t A + c b^t A + A^t b c^t + c b^t b c^t)^{-1},"['linear-algebra', 'matrices', 'inverse']"
41,After performing KPA on Hill Cipher the matrix is formed wrong.,After performing KPA on Hill Cipher the matrix is formed wrong.,,"Whenever I'm solving the hill cipher's key the final matrix is not in the original form. When I do the one from Wikipedia and also the one that I made myself neither comes back in the original form for the encryption or decryption key. First Wikipedia example. C=Cipher-text Matrix. P=Plain-text Matrix. C= $\begin{bmatrix} 7&8& \\ 0&19 \\ \end{bmatrix}$ P= $\begin{bmatrix} 7&4\\ 11&15 \\ \end{bmatrix}$ To calculate the decryption key I have to setup the formula like so. $D = [C]^{-1} ~\cdot P$ Then I calculate the modular multiplicative inverse of C as follows. $[C]^{-1} = \det[C]^{-1} \cdot adj([C])$ $det[C]= (ad - bc) \mod 26$ $7*19 - 8*0 \mod 26 = 133 \mod 26 = 3 $ mod inverse of 3 mod 26 is 9. $adj([C]) = \begin{bmatrix}19&-8\\0&7 \end{bmatrix}$ $[C]^{-1} =9 \cdot \begin{bmatrix}19&-8\\0&7 \end{bmatrix} \mod 26 \Rightarrow \begin{bmatrix}15&6\\0&11\end{bmatrix} $ $D=\begin{bmatrix}15&6 \\ 0&11 \end{bmatrix} \cdot \begin{bmatrix}7&4\\11&15\end{bmatrix} \mod 26 \Rightarrow \begin{bmatrix}129&255\\44&165\end{bmatrix} \mod 26 \Rightarrow D = \begin{bmatrix} 15&20\\17&9\end{bmatrix}$ Wikipedia's decryption matrix though is $\begin{bmatrix}15&17\\20&9\end{bmatrix}$ It seems to hold true for all matricies that I calculate that the end result matrix is $\begin{bmatrix}a&c\\b&d\end{bmatrix}$ every single time. I don't know if this is normal or not but I don't get it. My own matricies. $K=\begin{bmatrix}7&11\\8&11\end{bmatrix}$ $P=\begin{bmatrix}7&11\\4&11\end{bmatrix}$ $C=\begin{bmatrix}15&16\\22&1\end{bmatrix}$ $D=\begin{bmatrix}25&1\\22&23\end{bmatrix}$ If I convert my $C^{-1}$ and rotate it to instead be $\begin{bmatrix}a&c\\b&d\end{bmatrix}$ then I get back the encryption key correctly. I don't know what's going on with it though as the vectors are setup like wikipedia. P.S. I'm writing a lab to show the rest of the students how I solved a hill cipher CTF challenge utilizing the KPA against it but it seems like I've forgotten how in the world I solved it. My goal is to make it so that everyone at my community college who's interested in doing such events has the knowledge as to how to do such events. Somehow, somewhere I'm rotating things and I don't know how/why/where. I had it working Tuesday Morning at 01:30 when I woke up with the answer and did it on my phone's calculator but I didn't write it down and now I'm back into the same boat again.","Whenever I'm solving the hill cipher's key the final matrix is not in the original form. When I do the one from Wikipedia and also the one that I made myself neither comes back in the original form for the encryption or decryption key. First Wikipedia example. C=Cipher-text Matrix. P=Plain-text Matrix. C= P= To calculate the decryption key I have to setup the formula like so. Then I calculate the modular multiplicative inverse of C as follows. mod inverse of 3 mod 26 is 9. Wikipedia's decryption matrix though is It seems to hold true for all matricies that I calculate that the end result matrix is every single time. I don't know if this is normal or not but I don't get it. My own matricies. If I convert my and rotate it to instead be then I get back the encryption key correctly. I don't know what's going on with it though as the vectors are setup like wikipedia. P.S. I'm writing a lab to show the rest of the students how I solved a hill cipher CTF challenge utilizing the KPA against it but it seems like I've forgotten how in the world I solved it. My goal is to make it so that everyone at my community college who's interested in doing such events has the knowledge as to how to do such events. Somehow, somewhere I'm rotating things and I don't know how/why/where. I had it working Tuesday Morning at 01:30 when I woke up with the answer and did it on my phone's calculator but I didn't write it down and now I'm back into the same boat again.","\begin{bmatrix}
7&8& \\
0&19 \\
\end{bmatrix} \begin{bmatrix}
7&4\\
11&15 \\
\end{bmatrix} D = [C]^{-1} ~\cdot P [C]^{-1} = \det[C]^{-1} \cdot adj([C]) det[C]= (ad - bc) \mod 26 7*19 - 8*0 \mod 26 = 133 \mod 26 = 3  adj([C]) = \begin{bmatrix}19&-8\\0&7 \end{bmatrix} [C]^{-1} =9 \cdot \begin{bmatrix}19&-8\\0&7 \end{bmatrix} \mod 26 \Rightarrow \begin{bmatrix}15&6\\0&11\end{bmatrix}  D=\begin{bmatrix}15&6 \\ 0&11 \end{bmatrix} \cdot \begin{bmatrix}7&4\\11&15\end{bmatrix} \mod 26 \Rightarrow \begin{bmatrix}129&255\\44&165\end{bmatrix} \mod 26 \Rightarrow D = \begin{bmatrix} 15&20\\17&9\end{bmatrix} \begin{bmatrix}15&17\\20&9\end{bmatrix} \begin{bmatrix}a&c\\b&d\end{bmatrix} K=\begin{bmatrix}7&11\\8&11\end{bmatrix} P=\begin{bmatrix}7&11\\4&11\end{bmatrix} C=\begin{bmatrix}15&16\\22&1\end{bmatrix} D=\begin{bmatrix}25&1\\22&23\end{bmatrix} C^{-1} \begin{bmatrix}a&c\\b&d\end{bmatrix}","['linear-algebra', 'matrices', 'modular-arithmetic', 'cryptography']"
42,Matrix exponentiation of a Kronecker product of Pauli matrices,Matrix exponentiation of a Kronecker product of Pauli matrices,,"I need to numerically compute the matrix-exponential of a Kronecker product of Pauli matrices (including the identity). For example, $$ \exp( X \otimes Y \otimes I \otimes Z \;\otimes \;... ) $$ or generally $$ \exp \bigotimes\limits_j \sigma_j, \; \; \text{where} \;\; \sigma_j \in \{I,X,Y,Z\}. $$ I can construct the Pauli product easily enough. However, I want to avoid implementing a numerical routine for exponentiating general (or just square) complex matrices, since the Hermitian & unitary matrix resulting from Pauli products is very particular. For example, it's clear that the resulting matrix will only contain the elements $\{ \pm 1, \pm i, 0 \}$ . Surely this begs an analytic form, or at least a significantly simplified numerical routine, for computing the matrix exponential!","I need to numerically compute the matrix-exponential of a Kronecker product of Pauli matrices (including the identity). For example, or generally I can construct the Pauli product easily enough. However, I want to avoid implementing a numerical routine for exponentiating general (or just square) complex matrices, since the Hermitian & unitary matrix resulting from Pauli products is very particular. For example, it's clear that the resulting matrix will only contain the elements . Surely this begs an analytic form, or at least a significantly simplified numerical routine, for computing the matrix exponential!","
\exp( X \otimes Y \otimes I \otimes Z \;\otimes \;... )
 
\exp \bigotimes\limits_j \sigma_j, \; \; \text{where} \;\; \sigma_j \in \{I,X,Y,Z\}.
 \{ \pm 1, \pm i, 0 \}","['matrices', 'matrix-exponential', 'kronecker-product', 'unitary-matrices']"
43,Show continuity of $f:\mathbb R \to \mathbb R^{2 \times 2}$,Show continuity of,f:\mathbb R \to \mathbb R^{2 \times 2},"$f:\mathbb R \to \mathbb R^{2 \times 2}$ $f(x)=\begin{pmatrix}1&&0\\1-x&&x\end{pmatrix}$ Now, I know that $f$ is continuous because it is continuous entry wise, but I want to show it directly. Let $|x-y|<\delta$ $\|f(x)-f(y)\|=\|\begin{pmatrix}0&&0\\y-x&&x-y\end{pmatrix}\|=\sup\{\|\begin{pmatrix}0&&0\\y-x&&x-y\end{pmatrix}\begin{pmatrix}v_1\\v_2\end{pmatrix}\|:v \in \mathbb R^2, \|v\|\le 1\}=\sup\{\|\begin{pmatrix}0\\(y-x)v_1+(x-y)v_2\end{pmatrix}\|:v \in \mathbb R^2, \|v\|\le 1\}$ How to continue?","Now, I know that is continuous because it is continuous entry wise, but I want to show it directly. Let How to continue?","f:\mathbb R \to \mathbb R^{2 \times 2} f(x)=\begin{pmatrix}1&&0\\1-x&&x\end{pmatrix} f |x-y|<\delta \|f(x)-f(y)\|=\|\begin{pmatrix}0&&0\\y-x&&x-y\end{pmatrix}\|=\sup\{\|\begin{pmatrix}0&&0\\y-x&&x-y\end{pmatrix}\begin{pmatrix}v_1\\v_2\end{pmatrix}\|:v \in \mathbb R^2, \|v\|\le 1\}=\sup\{\|\begin{pmatrix}0\\(y-x)v_1+(x-y)v_2\end{pmatrix}\|:v \in \mathbb R^2, \|v\|\le 1\}","['calculus', 'linear-algebra', 'matrices', 'continuity']"
44,Is the product of two unitary Householder matrices necessarily of finite order?,Is the product of two unitary Householder matrices necessarily of finite order?,,"Let $\mathbf A$ and $\mathbf B$ be two unitary Householder matrices. They do not necessarily commute. Is $\mathbf{AB}$ necessarily of finite order (i.e. $\exists$ $n\in \Bbb N$ s.t. $(\mathbf{AB})^n = \mathbb I$ )? If not, under what conditions on $\mathbf {A}$ and $\mathbf{B}$ would $\mathbf{AB}$ definitely be of finite order? Note : This question arose in the context of Grover's algorithm . In that case $\mathbf{A} = U_s$ and $\mathbf{B} = U_\omega$ . I want to know whether $(U_sU_\omega)^r|s\rangle$ necessarily coincides with $|s\rangle$ after a certain number of iterations $r$ .","Let and be two unitary Householder matrices. They do not necessarily commute. Is necessarily of finite order (i.e. s.t. )? If not, under what conditions on and would definitely be of finite order? Note : This question arose in the context of Grover's algorithm . In that case and . I want to know whether necessarily coincides with after a certain number of iterations .",\mathbf A \mathbf B \mathbf{AB} \exists n\in \Bbb N (\mathbf{AB})^n = \mathbb I \mathbf {A} \mathbf{B} \mathbf{AB} \mathbf{A} = U_s \mathbf{B} = U_\omega (U_sU_\omega)^r|s\rangle |s\rangle r,['linear-algebra']
45,Can a nonzero matrix be PSD and have zero trace?,Can a nonzero matrix be PSD and have zero trace?,,"Does there exist an $n \times n$ matrix $A$ which satisfies the following? $A \succeq 0$ ( $A$ is positive semidefinite) $\sum_i A_{ii} = 0$ (trace of $A$ is zero) $A \neq 0$ ( $A$ is not the zero matrix) I know that there exist no such matrices which are symmetric, but I am not sure how to prove that no matrices satisfy the above for the case of non-symmetric matrices, or to otherwise find a counterexample.","Does there exist an matrix which satisfies the following? ( is positive semidefinite) (trace of is zero) ( is not the zero matrix) I know that there exist no such matrices which are symmetric, but I am not sure how to prove that no matrices satisfy the above for the case of non-symmetric matrices, or to otherwise find a counterexample.",n \times n A A \succeq 0 A \sum_i A_{ii} = 0 A A \neq 0 A,"['matrices', 'examples-counterexamples', 'trace', 'positive-semidefinite']"
46,Is there a general matrix to reflect about the line $y=mx+c$?,Is there a general matrix to reflect about the line ?,y=mx+c,"I have been looking into matrix transformations and found the following matrix to reflect about the line $y=(\tan\theta)x$ . $$R = \begin{bmatrix}       \cos(2\theta)&  \sin(2\theta)\\       \sin(2\theta)& -\cos(2\theta)\\       \end{bmatrix}$$ However, is there a general matrix to reflect about the line $y=mx+c$ ?","I have been looking into matrix transformations and found the following matrix to reflect about the line . However, is there a general matrix to reflect about the line ?","y=(\tan\theta)x R = \begin{bmatrix}
      \cos(2\theta)&  \sin(2\theta)\\
      \sin(2\theta)& -\cos(2\theta)\\
      \end{bmatrix} y=mx+c","['matrices', 'reflection']"
47,Problem of values in a $3\times3$ magic square,Problem of values in a  magic square,3\times3,We give the following integers : $$\left[\begin{matrix} -2 & x_2 & x_1 \\ 17 & -8 & 9 \\ 3 & x_3 & x_4 \end{matrix}\right]$$ The magic constant is equal to $18$ then I found two possibilities for the four unknown integers. The first weird thing is the fact that $-8$ is at the center of the square... What value do I have to change to get only one solution? Thanks in advance!,We give the following integers : The magic constant is equal to then I found two possibilities for the four unknown integers. The first weird thing is the fact that is at the center of the square... What value do I have to change to get only one solution? Thanks in advance!,"\left[\begin{matrix}
-2 & x_2 & x_1 \\
17 & -8 & 9 \\
3 & x_3 & x_4
\end{matrix}\right] 18 -8","['matrices', 'arithmetic', 'integers', 'magic-square']"
48,Short and simple proof that matrix rank $\geq n-1$ [closed],Short and simple proof that matrix rank  [closed],\geq n-1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question An $n \times n$ matrix has zeros on the main diagonal and the off-diagonal entries are either $1$ or $1980$ . Prove that its rank is $\geq n-1$ . Thanks for helping.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question An matrix has zeros on the main diagonal and the off-diagonal entries are either or . Prove that its rank is . Thanks for helping.",n \times n 1 1980 \geq n-1,"['linear-algebra', 'matrices', 'matrix-rank']"
49,Singular value decomposition of matrix shifted by a constant,Singular value decomposition of matrix shifted by a constant,,This may be trivial but I couldn't figure it out. If SVD of matrix $X$ is $X = U \Sigma V^*$ . For a constant value $c$ what is the SVD of $X-c$ ? From experiments it looks like only the largest singular value is affected.,This may be trivial but I couldn't figure it out. If SVD of matrix is . For a constant value what is the SVD of ? From experiments it looks like only the largest singular value is affected.,X X = U \Sigma V^* c X-c,"['matrices', 'matrix-decomposition', 'svd']"
50,Comparison of two quadratic forms,Comparison of two quadratic forms,,Let $n \times n$ matrix $A$ be positive semidefinite and let $P$ be an orthogonal projector of some subspace of $\mathbb{R}^n$ into $\mathbb{R}^n$ .  Is the following correct? $$\left( \forall x \in \mathbb{R}^n \right) \left( x^T P A P x \leq x^T A x \right)$$,Let matrix be positive semidefinite and let be an orthogonal projector of some subspace of into .  Is the following correct?,n \times n A P \mathbb{R}^n \mathbb{R}^n \left( \forall x \in \mathbb{R}^n \right) \left( x^T P A P x \leq x^T A x \right),"['linear-algebra', 'matrices', 'quadratic-forms', 'positive-semidefinite']"
51,When does a quadratic form being equal to zero implies the underlying matrix is equal to zero?,When does a quadratic form being equal to zero implies the underlying matrix is equal to zero?,,"Let $X_1,\dots,X_n \in\mathbb{R}^m$ be vectors; and $M\in\mathbb{R}^{m\times m}$ is a symmetric matrix. My question is as follows. Is there any condition $\mathcal{C}$ on $X_1,\dots,X_n$ such that, under $\mathcal{C}$ , it holds: $$ X_i^T M X_i = 0 \iff M=0, $$ and in the absence of $\mathcal{C}$ , $X_i^T MX_i = 0\iff M=0$ fails, that is, there is a matrix $M\neq 0$ such that $X_i^T M X_i= 0$ but $M\neq 0$ . For instance, if ${\rm span}(X_iX_i^T)$ is the set of all ( $m\times m$ ) symmetric matrices, then one can establish $X_i^T M X_i = 0\iff M=0$ .","Let be vectors; and is a symmetric matrix. My question is as follows. Is there any condition on such that, under , it holds: and in the absence of , fails, that is, there is a matrix such that but . For instance, if is the set of all ( ) symmetric matrices, then one can establish .","X_1,\dots,X_n \in\mathbb{R}^m M\in\mathbb{R}^{m\times m} \mathcal{C} X_1,\dots,X_n \mathcal{C} 
X_i^T M X_i = 0 \iff M=0,
 \mathcal{C} X_i^T MX_i = 0\iff M=0 M\neq 0 X_i^T M X_i= 0 M\neq 0 {\rm span}(X_iX_i^T) m\times m X_i^T M X_i = 0\iff M=0","['linear-algebra', 'matrices', 'quadratic-forms']"
52,What are some lesser known unitary invariant norms for matrices?,What are some lesser known unitary invariant norms for matrices?,,"I'm interested to characterize and see matrix norms that are unitary invariant. I'm familiar with the $\|A\|_{\sigma p}$ norm (see Matrix Mathematics, Dennis S. Bernstein page 548) which is defined by $$ \|A\|_{\sigma p} := \begin{cases} \left( \sum_{i=1}^{\min\{n,m\}}\sigma_i^p(A) \right)^{1/p}\quad \quad 1 \leq p < \infty,\\ \sigma_{\max}(A) \quad \quad p =\infty. \end{cases} $$ This is a unitary norm, and in general any unitary invariant norm would only depend on singular values of matrix $A$ and visa versa [*] What are some other matrix norms that are unitary invariant? Is there a principled way to construct such a norm, i.e., come up with a function of singular values $$f: \mathbb{R}^{\min\{n,m\}} \to \mathbb{R},$$ that would necessary constitute a norm? i.e., $\|\cdot\| := f(\sigma_1(A), \ldots, \sigma_{\min\{n,m\}}(A))$ will be a norm? [*] Given a matrix $A$ and $U\Lambda V^\top$ , its SVD, use unitary matrices $U$ , $V^\top$ to leave out only the singular values $\Lambda$ .","I'm interested to characterize and see matrix norms that are unitary invariant. I'm familiar with the norm (see Matrix Mathematics, Dennis S. Bernstein page 548) which is defined by This is a unitary norm, and in general any unitary invariant norm would only depend on singular values of matrix and visa versa [*] What are some other matrix norms that are unitary invariant? Is there a principled way to construct such a norm, i.e., come up with a function of singular values that would necessary constitute a norm? i.e., will be a norm? [*] Given a matrix and , its SVD, use unitary matrices , to leave out only the singular values .","\|A\|_{\sigma p} 
\|A\|_{\sigma p} := \begin{cases}
\left(
\sum_{i=1}^{\min\{n,m\}}\sigma_i^p(A)
\right)^{1/p}\quad \quad 1 \leq p < \infty,\\
\sigma_{\max}(A) \quad \quad p =\infty.
\end{cases}
 A f:
\mathbb{R}^{\min\{n,m\}} \to \mathbb{R}, \|\cdot\| := f(\sigma_1(A), \ldots, \sigma_{\min\{n,m\}}(A)) A U\Lambda V^\top U V^\top \Lambda","['matrices', 'matrix-norms', 'unitary-matrices']"
53,Writing nonlinear ODE in matrix form,Writing nonlinear ODE in matrix form,,"I want to obtain the eigenvalues of the following nonlinear system ${\displaystyle {\dot {x}}_{1}(t)=x_{2}(t)}$ $  {\displaystyle{\dot {x}}_{2}(t)=-{\frac {g}{\ell }}\sin {x_{1}}(t)-{\frac {k}{m\ell }}{x_{2}}(t)}$ I have tried to convert to matrix form in order to find $A-I\lambda$ $A= [0, 1$ $-g/l\sin(x_1), -k/ml]$ But this doesn't seem right due to he sin term still being present, any advice?","I want to obtain the eigenvalues of the following nonlinear system I have tried to convert to matrix form in order to find But this doesn't seem right due to he sin term still being present, any advice?","{\displaystyle {\dot {x}}_{1}(t)=x_{2}(t)}   {\displaystyle{\dot {x}}_{2}(t)=-{\frac {g}{\ell }}\sin {x_{1}}(t)-{\frac {k}{m\ell }}{x_{2}}(t)} A-I\lambda A= [0, 1 -g/l\sin(x_1), -k/ml]","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'derivatives', 'eigenvalues-eigenvectors']"
54,Motivation of Adjoint Transformations,Motivation of Adjoint Transformations,,"Linear Algebra Done Right introduces the concept of Adjoint transfomations as Suppose $T \in \mathcal{L}(V, W)$ . The adjoint of T is the function $T^{*} : W \rightarrow V$ such that: $\langle T v, w\rangle=\left\langle v, T^{*} w\right\rangle$ for every $v \in V$ and $w \in W$ But this is introduced without any motivation. We work through some examples and find $T^*$ and I think it is interesting it exists. But other than being neat what is the motivation to introduce such a concept? What does the above definition say about $V,W$ or $T$ ? Why do we need this concept? I can't find a good motivation else where as well.",Linear Algebra Done Right introduces the concept of Adjoint transfomations as Suppose . The adjoint of T is the function such that: for every and But this is introduced without any motivation. We work through some examples and find and I think it is interesting it exists. But other than being neat what is the motivation to introduce such a concept? What does the above definition say about or ? Why do we need this concept? I can't find a good motivation else where as well.,"T \in \mathcal{L}(V, W) T^{*} : W \rightarrow V \langle T v, w\rangle=\left\langle v, T^{*} w\right\rangle v \in V w \in W T^* V,W T","['linear-algebra', 'abstract-algebra', 'matrices']"
55,Upper Triangular Implies Diagonal?,Upper Triangular Implies Diagonal?,,"If all matrices can be made upper - triangular with respect to some basis by: Suppose V is a finite-dimensional complex vector space and T is a linear transformation.   Then T has an upper-triangular matrix with respect to some basis of V. And any upper - triangular matrix can be made orthogonal: Suppose T is a linear transformation. If T has an upper-triangular matrix with respect to some   basis of V, then T has an upper-triangular matrix with respect to some   orthonormal basis of V. But it's clear that an upper-triangular and orthonormal matrix must be a diagonal matrix. This implies every matrix has a diagonal matrix which we know to false as it was stated earlier to not be true. What am I missing?","If all matrices can be made upper - triangular with respect to some basis by: Suppose V is a finite-dimensional complex vector space and T is a linear transformation.   Then T has an upper-triangular matrix with respect to some basis of V. And any upper - triangular matrix can be made orthogonal: Suppose T is a linear transformation. If T has an upper-triangular matrix with respect to some   basis of V, then T has an upper-triangular matrix with respect to some   orthonormal basis of V. But it's clear that an upper-triangular and orthonormal matrix must be a diagonal matrix. This implies every matrix has a diagonal matrix which we know to false as it was stated earlier to not be true. What am I missing?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
56,Discrete State Space Representation of a second order system,Discrete State Space Representation of a second order system,,"A second order disturbance-based model can be represented as follows: $$\ddot{y}=f(y,\dot{y},w,t)+bu(t)$$ where $f(y,\dot{y},w,t)$ is generalized disturbance. This can be represented in continuous time state as: $$\dot{x}=Ax(t)+Bu(t)+E\dot{f}(t)$$ $$y(t)=Cx(t)$$ where, $A=     \begin{bmatrix}     0 & 1 & 0 \\     0 & 0 & 1 \\     0 & 0 & 0 \\     \end{bmatrix}$ , $B=\begin{bmatrix}0\\b\\0\end{bmatrix}$ , $C=\begin{bmatrix}1 & 0 & 0\end{bmatrix}$ , $E=\begin{bmatrix}0\\0\\1\end{bmatrix}$ and $x=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}y\\\dot{y}\\f\end{bmatrix}$ . When the above state space equation is discretized, we can get the following: $$x(k+1)=\Phi x(k)+\Gamma u(k)+E_df(k+1)$$ $$y(k)=Cx(k)$$ My question is, Is the term $f(k+1)$ for discretized $\dot{f}(t)$ correct or should it be $f(k)$ ? How do you determine the term $E_d$ associated with $\dot{f}(t)$ using zero order hold?","A second order disturbance-based model can be represented as follows: where is generalized disturbance. This can be represented in continuous time state as: where, , , , and . When the above state space equation is discretized, we can get the following: My question is, Is the term for discretized correct or should it be ? How do you determine the term associated with using zero order hold?","\ddot{y}=f(y,\dot{y},w,t)+bu(t) f(y,\dot{y},w,t) \dot{x}=Ax(t)+Bu(t)+E\dot{f}(t) y(t)=Cx(t) A=
    \begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    \end{bmatrix} B=\begin{bmatrix}0\\b\\0\end{bmatrix} C=\begin{bmatrix}1 & 0 & 0\end{bmatrix} E=\begin{bmatrix}0\\0\\1\end{bmatrix} x=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}y\\\dot{y}\\f\end{bmatrix} x(k+1)=\Phi x(k)+\Gamma u(k)+E_df(k+1) y(k)=Cx(k) f(k+1) \dot{f}(t) f(k) E_d \dot{f}(t)","['matrices', 'control-theory']"
57,Condition for positive Determinant,Condition for positive Determinant,,"If we have that for any non-zero vector $x$ that $Ax\cdot x$ is a positive, why is determinant of $A$ positive?","If we have that for any non-zero vector that is a positive, why is determinant of positive?",x Ax\cdot x A,"['matrices', 'determinant']"
58,$\|A\|_\infty = \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}=\|A\operatorname{1}\|_\infty$,,\|A\|_\infty = \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}=\|A\operatorname{1}\|_\infty,"Let $A$ be a $n\times m$ matrix, where every entry is either positiv (all entries are positiv) or negativ (all entries are negativ). Then holds: $\|A\|_\infty = \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}=\|A\operatorname{1}\|_\infty$ where $1=(1,1,\dotso, 1)^T\in\mathbb{R}^m$ [It should be $x\in\mathbb{R}^m$ too, this is not mentioned...] The equality $\|A\|_\infty=\|A\cdot 1\|_\infty$ is easy to see. First of all without loss of generality, we can assume that every entry in $A$ is positiv. Definition: It is $\|A\|_\infty=\displaystyle{\max_{i=1,\dotso, n}}\{\sum_{k=1}^m |a_{ik}|\}$ and $\|A1\|_\infty = \|\begin{pmatrix}\sum_{k=1}^m a_{1k}\\\vdots\\\sum_{k=1}^m a_{nk} \end{pmatrix}\|_\infty=\displaystyle{\max_{i=1,\dotso n}}\{\sum_{k=1}^m a_{ik}\}$ Now, I want to see the other equality. Maybe, it is best to show something like $\|A1\|_\infty\leq \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}\leq \|A\|_\infty$ Where the first inequality, is trivial. So $\|A1\|_\infty\leq \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}$ holds for sure, because the RHS can not be smaller than $\|A1\|_\infty$ , because we take the supremum over every $x\neq 0$ , so espacially $x=1$ (here I mean equality of elements of $\mathbb{R}^m$ ). In that case, we have equality. We are left to show $\sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}\leq \|A\|_\infty$ Do you have a hint how to show this? Writing out the definition of $\|\dot\|_\infty$ for the fraction did not help me for now. Thanks in advance.","Let be a matrix, where every entry is either positiv (all entries are positiv) or negativ (all entries are negativ). Then holds: where [It should be too, this is not mentioned...] The equality is easy to see. First of all without loss of generality, we can assume that every entry in is positiv. Definition: It is and Now, I want to see the other equality. Maybe, it is best to show something like Where the first inequality, is trivial. So holds for sure, because the RHS can not be smaller than , because we take the supremum over every , so espacially (here I mean equality of elements of ). In that case, we have equality. We are left to show Do you have a hint how to show this? Writing out the definition of for the fraction did not help me for now. Thanks in advance.","A n\times m \|A\|_\infty = \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}=\|A\operatorname{1}\|_\infty 1=(1,1,\dotso, 1)^T\in\mathbb{R}^m x\in\mathbb{R}^m \|A\|_\infty=\|A\cdot 1\|_\infty A \|A\|_\infty=\displaystyle{\max_{i=1,\dotso, n}}\{\sum_{k=1}^m |a_{ik}|\} \|A1\|_\infty = \|\begin{pmatrix}\sum_{k=1}^m a_{1k}\\\vdots\\\sum_{k=1}^m a_{nk} \end{pmatrix}\|_\infty=\displaystyle{\max_{i=1,\dotso n}}\{\sum_{k=1}^m a_{ik}\} \|A1\|_\infty\leq \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}\leq \|A\|_\infty \|A1\|_\infty\leq \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty} \|A1\|_\infty x\neq 0 x=1 \mathbb{R}^m \sup_{x\neq 0} \frac{\|Ax\|_\infty}{\|x\|_\infty}\leq \|A\|_\infty \|\dot\|_\infty","['linear-algebra', 'matrices', 'functional-analysis', 'normed-spaces']"
59,Why Q-matrix often set to $C^TC$ in Algebraic Riccati Equations?,Why Q-matrix often set to  in Algebraic Riccati Equations?,C^TC,I wonder why many books say that the Q-matrix in the Algebraic Riccati Equations: $$A^T X + X A - X B R^{-1} B^T X + Q = 0 $$ or $$X = A^T X A -(A^T X B)(R + B^T X B)^{-1}(B^T X A) + Q$$ Is often set to $Q = C^TC$ when finding the steady state kalman gain matrix $K$ in the kalman filter state update: $$\hat x(k+1) = \hat x(k) + K(y(k) - C\hat x(k))$$ Or finding the optimal control law: $$u = r - Lx(k+1)$$ Why $Q = C^TC$ ? I think that is great that it can be at least one answer for the $Q$ matrix instead of saying that the $Q$ matrix should be greater than $0$ . But what results will I get if I always say that $Q=C^TC$ ?,I wonder why many books say that the Q-matrix in the Algebraic Riccati Equations: or Is often set to when finding the steady state kalman gain matrix in the kalman filter state update: Or finding the optimal control law: Why ? I think that is great that it can be at least one answer for the matrix instead of saying that the matrix should be greater than . But what results will I get if I always say that ?,A^T X + X A - X B R^{-1} B^T X + Q = 0  X = A^T X A -(A^T X B)(R + B^T X B)^{-1}(B^T X A) + Q Q = C^TC K \hat x(k+1) = \hat x(k) + K(y(k) - C\hat x(k)) u = r - Lx(k+1) Q = C^TC Q Q 0 Q=C^TC,"['matrices', 'matrix-equations', 'control-theory', 'optimal-control', 'linear-control']"
60,An optimal procedure for vertex elimination in a graph,An optimal procedure for vertex elimination in a graph,,"Figure: An example of my problem Hi everyone. I'm struggling with a problem, and I really appreciate any hint. I have attached picture of an example where vertex 1 and 2 are connected to multiple vertices (green circles and blue squares). I want to gradually eliminate some of these blue and green vertices such that the number of edges connected to vertex 1 or 2 does not exceed 4. Also, my objective is doing this with the minimum number of eliminations. So, let's define it mathematically. If $d_B$ and $d_G$ respectively denote the number of remaining blue squares and green circles after elimination, and also if $d_i$ denotes the connected edges to vertex $i$ after elimination,  my problem is how to organize an optimal elimination procedure such that it maximizes $min(d_B,d_G)$ while it satisfies $d_i\leq4$ for $i=1,2$ .","Figure: An example of my problem Hi everyone. I'm struggling with a problem, and I really appreciate any hint. I have attached picture of an example where vertex 1 and 2 are connected to multiple vertices (green circles and blue squares). I want to gradually eliminate some of these blue and green vertices such that the number of edges connected to vertex 1 or 2 does not exceed 4. Also, my objective is doing this with the minimum number of eliminations. So, let's define it mathematically. If and respectively denote the number of remaining blue squares and green circles after elimination, and also if denotes the connected edges to vertex after elimination,  my problem is how to organize an optimal elimination procedure such that it maximizes while it satisfies for .","d_B d_G d_i i min(d_B,d_G) d_i\leq4 i=1,2","['matrices', 'graph-theory', 'optimization', 'algorithms', 'coloring']"
61,Proof that the matrix multiplication is associative – is commutativity of the elements necessary?,Proof that the matrix multiplication is associative – is commutativity of the elements necessary?,,"This is a picture of the proof, we assume that the elements of the matrix are elements of a ring: I don't know how the associativity is proved here without using commutativity. I have changed the notation myself in order to understand the proof better: $$d_{ji}=(a_{j1}b_{11}+...+a_{jn}b_{n1})c_{1i}+...+(a_{j1}b_{1l}+...+a_{jn}b_{nl})c_{li}$$ is because of distributivity $$(a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i})+...+(a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li})$$ which is because of associativity the same as $$a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li}\tag{*}$$ which means I can put the parenthesis where I want. I want to show that this is equal to: $a_{j1}(b_{11}c_{1i}+...+b_{1l}c_{li})+...+a_{jn}(b_{n1}c_{1i}+...+b_{nl}c_{li})$ . However, because of distributivity and associativity, this is equal to $$a_{j1}b_{11}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{n1}c_{1i}+...+a_{jn}b_{nl}c_{li}\tag{**}$$ The array $(*)$ has a different order than the array $(**)$ . Therefore the commutativity was used but the proof says only associativity and distributivity is used. Is there a mistake in my reasoning or is commutativity unnecessary? EDIT Definiton of matrixmultiplication:","This is a picture of the proof, we assume that the elements of the matrix are elements of a ring: I don't know how the associativity is proved here without using commutativity. I have changed the notation myself in order to understand the proof better: is because of distributivity which is because of associativity the same as which means I can put the parenthesis where I want. I want to show that this is equal to: . However, because of distributivity and associativity, this is equal to The array has a different order than the array . Therefore the commutativity was used but the proof says only associativity and distributivity is used. Is there a mistake in my reasoning or is commutativity unnecessary? EDIT Definiton of matrixmultiplication:",d_{ji}=(a_{j1}b_{11}+...+a_{jn}b_{n1})c_{1i}+...+(a_{j1}b_{1l}+...+a_{jn}b_{nl})c_{li} (a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i})+...+(a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li}) a_{j1}b_{11}c_{1i}+...+a_{jn}b_{n1}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{nl}c_{li}\tag{*} a_{j1}(b_{11}c_{1i}+...+b_{1l}c_{li})+...+a_{jn}(b_{n1}c_{1i}+...+b_{nl}c_{li}) a_{j1}b_{11}c_{1i}+...+a_{j1}b_{1l}c_{li}+...+a_{jn}b_{n1}c_{1i}+...+a_{jn}b_{nl}c_{li}\tag{**} (*) (**),"['linear-algebra', 'matrices']"
62,X is to vector like matrix is to linear operator?,X is to vector like matrix is to linear operator?,,"In linear algebra texts there is usually a clear distinction between linear operators and matrices. A linear operator is a map between two spaces that fulfills a set of conditions. A matrix is a 2D array of numbers (elements of the underlying field) that can be used to represent a linear operator with respect to bases of the two spaces. Slightly complicating but not muddling this distinction is the fact that together with appropriate operations, a matrix can also be seen as a linear operator itself. What I'm missing is a corresponding distinction for vectors. The term ""vector"" is interchangeably used for elements of a space, and for their representation by 1D arrays with respect to a basis. Is there terminology to distinguish between these two meanings of ""vector"", like there is for ""linear operator"" vs ""matrix""?","In linear algebra texts there is usually a clear distinction between linear operators and matrices. A linear operator is a map between two spaces that fulfills a set of conditions. A matrix is a 2D array of numbers (elements of the underlying field) that can be used to represent a linear operator with respect to bases of the two spaces. Slightly complicating but not muddling this distinction is the fact that together with appropriate operations, a matrix can also be seen as a linear operator itself. What I'm missing is a corresponding distinction for vectors. The term ""vector"" is interchangeably used for elements of a space, and for their representation by 1D arrays with respect to a basis. Is there terminology to distinguish between these two meanings of ""vector"", like there is for ""linear operator"" vs ""matrix""?",,"['linear-algebra', 'matrices', 'terminology']"
63,Is there any matrix representation for the second order derivation of a determinant?,Is there any matrix representation for the second order derivation of a determinant?,,"$\newcommand{\piff}[2]{\frac{\partial{#1}}{\partial{#2}}}\newcommand{\ppiff}[3]{\frac{\partial^2 {#1}}{\partial{#2}\partial{#3}}}\newcommand{\pa}[2]{\ppiff{|A|}{#1}{#2}}$ Suppose matrix $$A = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1n}\\ x_{21} & x_{22} & \cdots & x_{2n}\\ \vdots & \vdots & \ddots & \vdots\\ x_{n1} & x_{n2} & \cdots & x_{nn} \end{pmatrix}$$ Consider $\det{A}$ , denoted $|A|$ , as a $n^2$ -variable function of variable $x_{ij}(1\le i,j \le n)$ . I've found that $$\piff{|A|}{A} = \begin{pmatrix} \piff{|A|}{x_{11}} & \piff{|A|}{x_{12}} & \cdots & \piff{|A|}{x_{1n}}\\ \piff{|A|}{x_{21}} & \piff{|A|}{x_{22}} & \cdots & \piff{|A|}{x_{2n}}\\ \vdots & \vdots & \ddots & \vdots\\ \piff{|A|}{x_{n1}} & \piff{|A|}{x_{n2}} & \cdots & \piff{|A|}{x_{nn}} \end{pmatrix} = C$$ where $C$ is the cofactor matrix . Is there any matrix representation for follwing $n^2$ by $n^2$ matrix? $$ \begin{pmatrix} \pa{x_{11}}{x_{11}} & \pa{x_{11}}{x_{12}} & \cdots\cdots & \pa{x_{11}}{x_{nn}} \\ \pa{x_{12}}{x_{11}} & \pa{x_{12}}{x_{12}} & \cdots\cdots & \pa{x_{12}}{x_{nn}} \\ \vdots & \vdots & \ddots & \vdots \\ \pa{x_{nn}}{x_{11}} & \pa{x_{nn}}{x_{12}} & \cdots\cdots & \pa{x_{nn}}{x_{nn}} \end{pmatrix} $$","Suppose matrix Consider , denoted , as a -variable function of variable . I've found that where is the cofactor matrix . Is there any matrix representation for follwing by matrix?","\newcommand{\piff}[2]{\frac{\partial{#1}}{\partial{#2}}}\newcommand{\ppiff}[3]{\frac{\partial^2 {#1}}{\partial{#2}\partial{#3}}}\newcommand{\pa}[2]{\ppiff{|A|}{#1}{#2}} A = \begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1n}\\
x_{21} & x_{22} & \cdots & x_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \cdots & x_{nn}
\end{pmatrix} \det{A} |A| n^2 x_{ij}(1\le i,j \le n) \piff{|A|}{A} = \begin{pmatrix}
\piff{|A|}{x_{11}} & \piff{|A|}{x_{12}} & \cdots & \piff{|A|}{x_{1n}}\\
\piff{|A|}{x_{21}} & \piff{|A|}{x_{22}} & \cdots & \piff{|A|}{x_{2n}}\\
\vdots & \vdots & \ddots & \vdots\\
\piff{|A|}{x_{n1}} & \piff{|A|}{x_{n2}} & \cdots & \piff{|A|}{x_{nn}}
\end{pmatrix} = C C n^2 n^2 
\begin{pmatrix}
\pa{x_{11}}{x_{11}} & \pa{x_{11}}{x_{12}} & \cdots\cdots & \pa{x_{11}}{x_{nn}} \\
\pa{x_{12}}{x_{11}} & \pa{x_{12}}{x_{12}} & \cdots\cdots & \pa{x_{12}}{x_{nn}} \\
\vdots & \vdots & \ddots & \vdots \\
\pa{x_{nn}}{x_{11}} & \pa{x_{nn}}{x_{12}} & \cdots\cdots & \pa{x_{nn}}{x_{nn}}
\end{pmatrix}
","['linear-algebra', 'matrices', 'multivariable-calculus', 'matrix-calculus']"
64,Formula for combinations with equally spaced points?,Formula for combinations with equally spaced points?,,"How do I calculate a specific value on this triangle array without doing it manually? https://i.sstatic.net/C03lM.jpg Please no sigma notation! I've already got it, what I'm really looking for is a pretty formula like nCr that doesn't make my calculator cry. These numbers are the ways you can place P objects in H slots, under the condition that the objects MUST be equally spaced from each other. The points columns increase like so: (0) + 0 + 0 + 0 + 1 + 1 + 1 + 2 + 2 + 2 + 3 + 3 + 3 + 4 + 4 + 4 + 5 + 5 + 5...       with the additions ticking up every (H-1) rows. This was the example for H = 4. Here's a diagram for H = 6 and P = 3 https://i.sstatic.net/rWooI.png","How do I calculate a specific value on this triangle array without doing it manually? https://i.sstatic.net/C03lM.jpg Please no sigma notation! I've already got it, what I'm really looking for is a pretty formula like nCr that doesn't make my calculator cry. These numbers are the ways you can place P objects in H slots, under the condition that the objects MUST be equally spaced from each other. The points columns increase like so: (0) + 0 + 0 + 0 + 1 + 1 + 1 + 2 + 2 + 2 + 3 + 3 + 3 + 4 + 4 + 4 + 5 + 5 + 5...       with the additions ticking up every (H-1) rows. This was the example for H = 4. Here's a diagram for H = 6 and P = 3 https://i.sstatic.net/rWooI.png",,"['combinatorics', 'matrices', 'combinations', 'binomial-coefficients']"
65,Determining the power of permutation matrix of order $N\times N$ to get identity matrix.,Determining the power of permutation matrix of order  to get identity matrix.,N\times N,"This particular 6x6 permutation matrix is P $$ P = \begin{pmatrix}             0 &  0 &  0 &  0 &  0 &  1 \\            0 &  0 &  1 &  0 &  0 &  0 \\            1 &  0 &  0 &  0 &  0 &  0 \\            0 &  0 &  0 &  0 &  1 &  0 \\            0 &  0 &  0 &  1 &  0 &  0 \\            0 &  1 &  0 &  0 &  0 &  0 \\ \end{pmatrix}$$ the least power of $P$ that gives identity is $8$ . However, lets consider $P_2$ $$ P_2 = \begin{pmatrix}          0 &  0 &  0 &  1 &  0 &  0 \\          0 &  0 &  1 &  0 &  0 &  0 \\          1 &  0 &  0 &  0 &  0 &  0 \\          0 &  0 &  0 &  0 &  1 &  0 \\          0 &  0 &  0 &  0 &  0 &  1 \\          0 &  1 &  0 &  0 &  0 &  0  \end{pmatrix} $$ the least power of $P_2$ that gives identity is $6$ . How to identify the least power of a permutation matrix for which it turns into identity.  2.If a number is given, say $n$ , how to construct a permutation matrix such that the minimum power the matrix has to be raised to get identity is that particular number $n$ ?","This particular 6x6 permutation matrix is P the least power of that gives identity is . However, lets consider the least power of that gives identity is . How to identify the least power of a permutation matrix for which it turns into identity.  2.If a number is given, say , how to construct a permutation matrix such that the minimum power the matrix has to be raised to get identity is that particular number ?"," P = \begin{pmatrix} 
           0 &  0 &  0 &  0 &  0 &  1 \\
           0 &  0 &  1 &  0 &  0 &  0 \\
           1 &  0 &  0 &  0 &  0 &  0 \\
           0 &  0 &  0 &  0 &  1 &  0 \\
           0 &  0 &  0 &  1 &  0 &  0 \\
           0 &  1 &  0 &  0 &  0 &  0 \\
\end{pmatrix} P 8 P_2 
P_2 = \begin{pmatrix}
         0 &  0 &  0 &  1 &  0 &  0 \\
         0 &  0 &  1 &  0 &  0 &  0 \\
         1 &  0 &  0 &  0 &  0 &  0 \\
         0 &  0 &  0 &  0 &  1 &  0 \\
         0 &  0 &  0 &  0 &  0 &  1 \\
         0 &  1 &  0 &  0 &  0 &  0 
\end{pmatrix}
 P_2 6 n n","['matrices', 'permutations']"
66,Prove that the rotation of sums is equal to the rotation of products,Prove that the rotation of sums is equal to the rotation of products,,"So the question starts off: Prove $$\ e^{t_1+t_2} = e^{t_1}e^{t_2}$$ E(t) is a unique solution to $\dot{E} = E, E(0) = 1$ . Let $E_1(t) = E(t_1 + t)$ , and E_2(t) = E(t_1)E(t) $$\dot E_1 (t) = \dot E_1 (t_1 + t) = E(t_1+t) = E_1 (t); E_1(0) = E(t_1)$$ $$\dot E_2 (t) = E(t_1)\dot E (t) = E(t_1)E(t) = E_2(t); E_2(0) = E(t_1)$$ Therefore(by uniqueness and existence theorem I think): $$E_1(t) = E_2(t)$$ Which implies: $$E(t_1+t) = E_1(t) = E_2(t) = E(t_1)E(t)$$ Then by setting $t = t_2$ we get our desired result. Then the question asks to prove: $$R(t_1+t_2) = R(t_1)R(t_2)$$ where $$R(t) = \begin{bmatrix}\cos(t)&-\sin(t)\\\sin(t)&\cos(t)\end{bmatrix} $$ Given: $R_1(t) = R(t+t_2)$ and $R_2(t) = R(t)R(t_2)$ Prove by a similar argument to the one above. Any suggestions on how to approach this?","So the question starts off: Prove E(t) is a unique solution to . Let , and E_2(t) = E(t_1)E(t) Therefore(by uniqueness and existence theorem I think): Which implies: Then by setting we get our desired result. Then the question asks to prove: where Given: and Prove by a similar argument to the one above. Any suggestions on how to approach this?","\ e^{t_1+t_2} = e^{t_1}e^{t_2} \dot{E} = E, E(0) = 1 E_1(t) = E(t_1 + t) \dot E_1 (t) = \dot E_1 (t_1 + t) = E(t_1+t) = E_1 (t); E_1(0) = E(t_1) \dot E_2 (t) = E(t_1)\dot E (t) = E(t_1)E(t) = E_2(t); E_2(0) = E(t_1) E_1(t) = E_2(t) E(t_1+t) = E_1(t) = E_2(t) = E(t_1)E(t) t = t_2 R(t_1+t_2) = R(t_1)R(t_2) R(t) = \begin{bmatrix}\cos(t)&-\sin(t)\\\sin(t)&\cos(t)\end{bmatrix}  R_1(t) = R(t+t_2) R_2(t) = R(t)R(t_2)","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'rotations']"
67,Multiplicatively closed set of matrices containing $A$ such that $X\ \mapsto\ AXA^3$ is surjective contains the identity?,Multiplicatively closed set of matrices containing  such that  is surjective contains the identity?,A X\ \mapsto\ AXA^3,"Let $n\geq 2$ and let $U$ be a set of $n\times n$ -matrices, such that 1) $\forall X,Y \in U:\ XY \in U$ , 2) $\exists A \in U$ such that $f: U \longrightarrow U:\ X\ \longmapsto AXA^3 $ is surjective. Show that $I_n \in U$ , where $I_n$ is the $n\times n$ identity matrix. I believe this to be false, but I was not able to find a counterexample.I tried setting, for example $AXA^3 = A^3$ and thus $A^3 = AXAXA^3 =\ldots= (AX)^n A^3$ and other tricks like that, but to no avail. Edit: I think I have a counterexample: If $U$ consists only of $\operatorname{diag}(1,0)$ , this satisfies the rules. Also $U = \left\{\operatorname{diag}(1,0,0),\operatorname{diag}(1,1,0)\right\}$ with $A = \operatorname{diag}(1,1,0)$ satisfies the rules. Are these counterexamples right?","Let and let be a set of -matrices, such that 1) , 2) such that is surjective. Show that , where is the identity matrix. I believe this to be false, but I was not able to find a counterexample.I tried setting, for example and thus and other tricks like that, but to no avail. Edit: I think I have a counterexample: If consists only of , this satisfies the rules. Also with satisfies the rules. Are these counterexamples right?","n\geq 2 U n\times n \forall X,Y \in U:\ XY \in U \exists A \in U f: U \longrightarrow U:\ X\ \longmapsto AXA^3  I_n \in U I_n n\times n AXA^3 = A^3 A^3 = AXAXA^3 =\ldots= (AX)^n A^3 U \operatorname{diag}(1,0) U = \left\{\operatorname{diag}(1,0,0),\operatorname{diag}(1,1,0)\right\} A = \operatorname{diag}(1,1,0)","['abstract-algebra', 'matrices', 'group-theory']"
68,Proving matrices equation when all the matrices in it may not be invertible,Proving matrices equation when all the matrices in it may not be invertible,,"I'm reviewing linear algebra for my exams this year, and I just encountered this problem. For an arbitrary matrix, $\boldsymbol{A} \in \mathcal{R}^{m \times n}$ , prove there must be a unique matrix $\boldsymbol{P} \in \mathcal{R}^{n \times m}$ matching the following 4 equations. $$ APA = A \\ PAP = P \\ (AP)^T = AP \\ (PA)^T = PA $$ Things will be easy if $A$ is invertible, but when $A$ is not invertible I have no ideas how to do it.","I'm reviewing linear algebra for my exams this year, and I just encountered this problem. For an arbitrary matrix, , prove there must be a unique matrix matching the following 4 equations. Things will be easy if is invertible, but when is not invertible I have no ideas how to do it.","\boldsymbol{A} \in \mathcal{R}^{m \times n} \boldsymbol{P} \in \mathcal{R}^{n \times m} 
APA = A \\
PAP = P \\
(AP)^T = AP \\
(PA)^T = PA
 A A","['linear-algebra', 'matrices', 'matrix-equations']"
69,"Determine all $2 \times 2$ real matrices $A$ such that $(1) \ \ A^2=I$, $(2) \ \ A^2=0$","Determine all  real matrices  such that ,",2 \times 2 A (1) \ \ A^2=I (2) \ \ A^2=0,"Determine all $2 \times 2$ real matrices $A$ such that $(1) \ \ A^2=I$ , $(2) \ \ A^2=0$ I came across this problem recently where I have to determine all the $2\times2$ matrices satisfying the aforementioned criteria. The problem is that the equations which I get after multiplication are not so easy to work with and I'm unable to proceed. If someone could help me with these equations and solve for the various cases, that'd be really helpful. Answer to part one: $$A=I$$ $$A=-I$$ $$a_{11}=a_{22}=0  \ , \ a_{12}a_{21}=1 $$ $$a_{11}=-a_{22}\neq0, \ a_{11}^2+a_{12}a_{21}=1$$ Answer to part two: $$a_{11}=a_{12}=a_{22}=0$$ $$a_{11}=a_{21}=a_{22}=0$$ $$a_{11}=-a_{22}\neq0, \ a_{11}^2+a_{12}a_{21}=0$$","Determine all real matrices such that , I came across this problem recently where I have to determine all the matrices satisfying the aforementioned criteria. The problem is that the equations which I get after multiplication are not so easy to work with and I'm unable to proceed. If someone could help me with these equations and solve for the various cases, that'd be really helpful. Answer to part one: Answer to part two:","2 \times 2 A (1) \ \ A^2=I (2) \ \ A^2=0 2\times2 A=I A=-I a_{11}=a_{22}=0  \ , \ a_{12}a_{21}=1  a_{11}=-a_{22}\neq0, \ a_{11}^2+a_{12}a_{21}=1 a_{11}=a_{12}=a_{22}=0 a_{11}=a_{21}=a_{22}=0 a_{11}=-a_{22}\neq0, \ a_{11}^2+a_{12}a_{21}=0","['linear-algebra', 'matrices', 'matrix-equations', 'nilpotence']"
70,$a_k = \mathrm{rank}(A^{k+1}) - \mathrm{rank}(A^k)$ is increasing [duplicate],is increasing [duplicate],a_k = \mathrm{rank}(A^{k+1}) - \mathrm{rank}(A^k),"This question already has an answer here : $\operatorname{rank}(T^3) + \operatorname{rank}(T) \geq 2\operatorname{rank}(T^2)$ for finite-dimensional $V$ (1 answer) Closed 4 years ago . $a_k = \mathrm{rank}(A^{k+1}) - \mathrm{rank}(A^k)$ is increasing. This is equivalent to $$\mathrm{rank}(A^{k+1}) + \mathrm{rank}(A^{k-1}) \geq 2 \mathrm{rank}(A^k) $$ which is a consequence of the Sylvester inequality: $$\mathrm{rank}(XZY) + \mathrm{rank}(Z) \geq \mathrm{rank}(XZ) + \mathrm{rank}(ZY)$$ for $ X = Y = A, Z = A^{k-1}$ . Is there a simpler proof of this fact, without using the full power of Sylvester?","This question already has an answer here : $\operatorname{rank}(T^3) + \operatorname{rank}(T) \geq 2\operatorname{rank}(T^2)$ for finite-dimensional $V$ (1 answer) Closed 4 years ago . is increasing. This is equivalent to which is a consequence of the Sylvester inequality: for . Is there a simpler proof of this fact, without using the full power of Sylvester?","a_k = \mathrm{rank}(A^{k+1}) - \mathrm{rank}(A^k) \mathrm{rank}(A^{k+1}) + \mathrm{rank}(A^{k-1}) \geq 2 \mathrm{rank}(A^k)  \mathrm{rank}(XZY) + \mathrm{rank}(Z) \geq \mathrm{rank}(XZ) + \mathrm{rank}(ZY)  X = Y = A, Z = A^{k-1}","['linear-algebra', 'matrices', 'matrix-rank']"
71,Trace of product of semidefinite matrices is nonnegative,Trace of product of semidefinite matrices is nonnegative,,"I want to prove this: $A$ is a symmetric positive semi-definite matrix $\Leftrightarrow$ $tr(AB) \geq 0$ $\forall $ B positive semi-definite. I tried using eigenvalues, because they all have to be non-negative, but that didn't help me too much, as the eigenvalues of $AB$ are different from those of $A$ or $B$ . I would like some tips about where to start.","I want to prove this: is a symmetric positive semi-definite matrix B positive semi-definite. I tried using eigenvalues, because they all have to be non-negative, but that didn't help me too much, as the eigenvalues of are different from those of or . I would like some tips about where to start.",A \Leftrightarrow tr(AB) \geq 0 \forall  AB A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
72,reflect a point over another point using matrix transformation,reflect a point over another point using matrix transformation,,"We know that if we want to reflect any point over an origin, i.e. $ O\left(0, 0\right) $ , we can use matrix transformation like this $$ \left(\begin{matrix}x' \\ y'\end{matrix}\right) = \left(\begin{matrix}-1 & 0 \\ 0 & -1\end{matrix}\right)\left(\begin{matrix}x \\ y\end{matrix}\right) = \left(\begin{matrix}-x \\ -y\end{matrix}\right). $$ But, what if we reflect any point over another point $ M\left(a, b\right) $ with $ a, b \ne 0 $ ?","We know that if we want to reflect any point over an origin, i.e. , we can use matrix transformation like this But, what if we reflect any point over another point with ?"," O\left(0, 0\right)   \left(\begin{matrix}x' \\ y'\end{matrix}\right) = \left(\begin{matrix}-1 & 0 \\ 0 & -1\end{matrix}\right)\left(\begin{matrix}x \\ y\end{matrix}\right) = \left(\begin{matrix}-x \\ -y\end{matrix}\right).   M\left(a, b\right)   a, b \ne 0 ","['matrices', 'coordinate-systems', 'transformation', 'reflection']"
73,Find all natural numbers $n \geq 3 $ such that if matrix $A$ has certain properties (e.g. $A^n = I$) then $A = CBC^{-1}$,Find all natural numbers  such that if matrix  has certain properties (e.g. ) then,n \geq 3  A A^n = I A = CBC^{-1},"Find all natural numbers $n \geq 3 $ such that: if: $A$ is a $2 \times 2$ matrix with real coefficients $\land \space A^n = I \land \space A^j \neq I$ for $ 0 <j <n$ , then: there are such invertible matrices $B$ and $C$ that $A = CBC^{-1}$ and \begin{equation}   B =       \begin{bmatrix}\cos\space t & \sin \space t \\-\sin \space t & \cos \space t \end{bmatrix} \end{equation} for certain $t \in \Bbb R $ I'm not really sure how to approach such problem. I guess one is supposed to use the properties of the determinant and the fact that $detB = $ cos $^2(t)+$ sin $^2(t)=1 $","Find all natural numbers such that: if: is a matrix with real coefficients for , then: there are such invertible matrices and that and for certain I'm not really sure how to approach such problem. I guess one is supposed to use the properties of the determinant and the fact that cos sin","n \geq 3  A 2 \times 2 \land \space A^n = I \land \space A^j \neq I  0 <j <n B C A = CBC^{-1} \begin{equation}
  B =
      \begin{bmatrix}\cos\space t & \sin \space t \\-\sin \space t & \cos \space t \end{bmatrix}
\end{equation} t \in \Bbb R  detB =  ^2(t)+ ^2(t)=1 ","['linear-algebra', 'matrices', 'determinant']"
74,Eigenvalues of a block off-diagonal matrix,Eigenvalues of a block off-diagonal matrix,,"Let $A_1,A_2 \in \mathbb{R}^{n \times n}$ . Construct the block matrix $A$ as follows: $$A: = \left[ {\begin{array}{*{20}{c}} 0&{{A_1}}\\ {{A_2}}&0 \end{array}} \right]$$ My observation is that matrix $A$ cannot have all its eigenvalues in the open left-half plane. In other worlds, if all eigenvalues of $A$ have non-positive real parts, then all of them are imaginary. I greatly appreciate it if someone can give me some idea/intuition why this happens. Thanks","Let . Construct the block matrix as follows: My observation is that matrix cannot have all its eigenvalues in the open left-half plane. In other worlds, if all eigenvalues of have non-positive real parts, then all of them are imaginary. I greatly appreciate it if someone can give me some idea/intuition why this happens. Thanks","A_1,A_2 \in \mathbb{R}^{n \times n} A A: = \left[ {\begin{array}{*{20}{c}}
0&{{A_1}}\\
{{A_2}}&0
\end{array}} \right] A A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
75,"How to show $\text{Tr}(M\log N)=\sum_{i,j}^n\lambda_i\log(\tilde{\lambda_j})(u_i^{\top}\tilde{u}_j)^2$?",How to show ?,"\text{Tr}(M\log N)=\sum_{i,j}^n\lambda_i\log(\tilde{\lambda_j})(u_i^{\top}\tilde{u}_j)^2","The above question is the equation $(2.4)$ of the following paper: MATRIX EXPONENTIATED GRADIENT UPDATES . Let $M$ and $N$ be two $n \times n$ positive definite matrices where $M=U\Lambda U^{\top}$ , $N=\tilde{U}\tilde{\Lambda} \tilde{U}^{\top}  $ and $(\lambda_i,v_i)$ are eigenpairs of $M$ , likewise for $N$ . How to show the following $$\text{Tr}(M\log N)=\sum_{i,j}\lambda_i\log(\tilde{\lambda_j})(u_i^{\top}\tilde{u}_j)^2$$ First I do not know what $i,j$ mean in summation, and how we have it using two summations. Second how to get that. My try: \begin{align} \text{Tr}(M\log N) &= \text{Tr}(U\Lambda U^{\top} \tilde{U}\log(\tilde{\Lambda})\tilde{U}^{\top}) \\ & = \text{Tr}(\Lambda U^{\top} \tilde{U}\log(\tilde{\Lambda})\tilde{U}^{\top}U) \end{align} How can I proceed using matrix calculus to get the result not by expanding? what is the hidden trick?","The above question is the equation of the following paper: MATRIX EXPONENTIATED GRADIENT UPDATES . Let and be two positive definite matrices where , and are eigenpairs of , likewise for . How to show the following First I do not know what mean in summation, and how we have it using two summations. Second how to get that. My try: How can I proceed using matrix calculus to get the result not by expanding? what is the hidden trick?","(2.4) M N n \times n M=U\Lambda U^{\top} N=\tilde{U}\tilde{\Lambda} \tilde{U}^{\top} 
 (\lambda_i,v_i) M N \text{Tr}(M\log N)=\sum_{i,j}\lambda_i\log(\tilde{\lambda_j})(u_i^{\top}\tilde{u}_j)^2 i,j \begin{align}
\text{Tr}(M\log N) &=
\text{Tr}(U\Lambda U^{\top}
\tilde{U}\log(\tilde{\Lambda})\tilde{U}^{\top}) \\
& = \text{Tr}(\Lambda U^{\top}
\tilde{U}\log(\tilde{\Lambda})\tilde{U}^{\top}U)
\end{align}","['linear-algebra', 'matrices']"
76,Numerator layout for derivatives and the chain rule,Numerator layout for derivatives and the chain rule,,"We have three matrices $\mathbf{W_2}$ , $\mathbf{W_1}$ and $\mathbf{h}$ (technically a column vector): $$ \mathbf{W_1} = \begin{bmatrix}     a & b \\     c & d \\ \end{bmatrix} \;\;\;\;\;\;\;\;\; \mathbf{W_2} = \begin{bmatrix}     e & f \\ \end{bmatrix}  \;\;\;\;\;\;\;\;\; \mathbf{h} = \begin{bmatrix}     h_1 \\     h_2 \\ \end{bmatrix}  $$ And a scalar $y$ , where: $$ y = \mathbf{W_2} \mathbf{W_1} \mathbf{h} $$ I'd like to compute the derivative of $y$ with respect to $\mathbf{W_1}$ , assuming numerator layout . Using the chain rule: $$ y = \mathbf{W_2} \mathbf{u} \;\;\;\;\;\;\;\;\; \mathbf{u} = \mathbf{W_1} \mathbf{h} \\ $$ $$ \begin{align} \frac{\partial y}{\partial \mathbf{W_1}} &= \frac{\partial y}{\partial \mathbf{u}} \frac{\partial \mathbf{u}}{\partial \mathbf{W_1}} \\ &= \mathbf{W_2} \frac{\partial \mathbf{u}}{\partial \mathbf{W_1}} \\ &= \mathbf{W_2} \mathbf{h}^{\top} \\ \end{align} $$ All well and good. Except - this isn't a $2x2$ matrix!! In fact, the dimensions don't match up for matrix multiplication, so something must be incorrect. If we take the Wikipedia definition of the derivative of a scalar by a matrix, using numerator layout, we know that actually: $$ \frac{\partial y}{\partial \mathbf{W_1}} = \begin{bmatrix}     \frac{\partial y}{\partial a} & \frac{\partial y}{\partial c} \\     \frac{\partial y}{\partial b} & \frac{\partial y}{\partial d} \\ \end{bmatrix} $$ Each element is just a scalar derivative, which we can calculate without any matric calculus. If we do that by hand and then factorise, we end up with: $$ \frac{\partial y}{\partial \mathbf{W_1}} = \mathbf{h} \mathbf{W_2} $$ Clearly, $\mathbf{h} \mathbf{W_2} \neq \mathbf{W_2} \mathbf{h}^\top $ . Can anybody suggest where I went wrong?","We have three matrices , and (technically a column vector): And a scalar , where: I'd like to compute the derivative of with respect to , assuming numerator layout . Using the chain rule: All well and good. Except - this isn't a matrix!! In fact, the dimensions don't match up for matrix multiplication, so something must be incorrect. If we take the Wikipedia definition of the derivative of a scalar by a matrix, using numerator layout, we know that actually: Each element is just a scalar derivative, which we can calculate without any matric calculus. If we do that by hand and then factorise, we end up with: Clearly, . Can anybody suggest where I went wrong?","\mathbf{W_2} \mathbf{W_1} \mathbf{h} 
\mathbf{W_1} =
\begin{bmatrix}
    a & b \\
    c & d \\
\end{bmatrix}
\;\;\;\;\;\;\;\;\;
\mathbf{W_2} =
\begin{bmatrix}
    e & f \\
\end{bmatrix} 
\;\;\;\;\;\;\;\;\;
\mathbf{h} =
\begin{bmatrix}
    h_1 \\
    h_2 \\
\end{bmatrix} 
 y 
y = \mathbf{W_2} \mathbf{W_1} \mathbf{h}
 y \mathbf{W_1} 
y = \mathbf{W_2} \mathbf{u}
\;\;\;\;\;\;\;\;\;
\mathbf{u} = \mathbf{W_1} \mathbf{h} \\
 
\begin{align}
\frac{\partial y}{\partial \mathbf{W_1}} &=
\frac{\partial y}{\partial \mathbf{u}} \frac{\partial \mathbf{u}}{\partial \mathbf{W_1}} \\
&= \mathbf{W_2} \frac{\partial \mathbf{u}}{\partial \mathbf{W_1}} \\
&= \mathbf{W_2} \mathbf{h}^{\top} \\
\end{align}
 2x2 
\frac{\partial y}{\partial \mathbf{W_1}} =
\begin{bmatrix}
    \frac{\partial y}{\partial a} & \frac{\partial y}{\partial c} \\
    \frac{\partial y}{\partial b} & \frac{\partial y}{\partial d} \\
\end{bmatrix}
 
\frac{\partial y}{\partial \mathbf{W_1}} = \mathbf{h} \mathbf{W_2}
 \mathbf{h} \mathbf{W_2} \neq \mathbf{W_2} \mathbf{h}^\top ","['matrices', 'matrix-calculus']"
77,How to show $\sqrt{\text{Tr}(A^2)} \leq \text{Tr}(A)$?,How to show ?,\sqrt{\text{Tr}(A^2)} \leq \text{Tr}(A),"Let $A$ be a positive semi-definite matrix. How to show that Frobenius norm is less than trace of the matrix? Formally, $$\sqrt{\text{Tr}(A^2)} \leq \text{Tr}(A)$$ Also, show when $A$ is an $n \times m$ the following is true $$\sqrt{\text{Tr}(A^TA)} \leq \|A\|_*$$ where $\|\cdot\|_*$ is nuclear norm which is the summation of the singular values.","Let be a positive semi-definite matrix. How to show that Frobenius norm is less than trace of the matrix? Formally, Also, show when is an the following is true where is nuclear norm which is the summation of the singular values.",A \sqrt{\text{Tr}(A^2)} \leq \text{Tr}(A) A n \times m \sqrt{\text{Tr}(A^TA)} \leq \|A\|_* \|\cdot\|_*,['matrices']
78,Invert a $4 \times 4$ matrix with a given structure?,Invert a  matrix with a given structure?,4 \times 4,"When tryig to fit $f(x,y) = a+bx+cy+dxy$ to the values of four points, we will have to invert following matrix. (Let us assume that $x_i,y_i$ are chosen suitably such that it is regular.) We could obviously solve this with any method (LU etc), but does the special structure of this matrix maybe allow for a compact representation of its inverse? I thought there might be a way because this matrix looks similar to a Vandermonde matrix (but it is obviously not the same), which do sometimes have inverses that are particularly easy to compute. $$M=\begin{bmatrix} 1 & x_1 & y_1 & x_1 y_1\\ 1 & x_2 & y_2 & x_2 y_2\\ 1 & x_3 & y_3 & x_3 y_3\\ 1 & x_4 & y_4 & x_4 y_4\end{bmatrix}$$","When tryig to fit to the values of four points, we will have to invert following matrix. (Let us assume that are chosen suitably such that it is regular.) We could obviously solve this with any method (LU etc), but does the special structure of this matrix maybe allow for a compact representation of its inverse? I thought there might be a way because this matrix looks similar to a Vandermonde matrix (but it is obviously not the same), which do sometimes have inverses that are particularly easy to compute.","f(x,y) = a+bx+cy+dxy x_i,y_i M=\begin{bmatrix}
1 & x_1 & y_1 & x_1 y_1\\
1 & x_2 & y_2 & x_2 y_2\\
1 & x_3 & y_3 & x_3 y_3\\
1 & x_4 & y_4 & x_4 y_4\end{bmatrix}","['linear-algebra', 'matrices', 'inverse', 'matrix-decomposition']"
79,"Prove that Gramian matrix is Invertible iff $(v_1,...,v_k) $ is linearly independent",Prove that Gramian matrix is Invertible iff  is linearly independent,"(v_1,...,v_k) ","Prove that Gramian matrix is Invertible iff $(v_1,...,v_k)  $ is linearly independent $G=G(v_1,...,v_k) = [\left\langle v_i,v_j\right\rangle ]_{i,j=1}^k $ I have great idea to calculate $\det G$ and show that if $(v_1,...,v_k)  $ is linearly independent then $\det G \neq0$ and vice versa. Plan sounds good (?) but how to calculate $\det$ of this? \begin{vmatrix} \langle v_1,v_1\rangle & \langle v_1,v_2\rangle &\dots & \langle v_1,v_n\rangle\\  \langle v_2,v_1\rangle & \langle v_2,v_2\rangle &\dots & \langle v_2,v_n\rangle\\ \vdots&\vdots&\ddots&\vdots\\  \langle v_n,v_1\rangle & \langle v_n,v_2\rangle &\dots & \langle v_n,v_n\rangle\end{vmatrix} . Probably I should use some scalar product propeties but it isn't clear for me how can I do that.",Prove that Gramian matrix is Invertible iff is linearly independent I have great idea to calculate and show that if is linearly independent then and vice versa. Plan sounds good (?) but how to calculate of this? . Probably I should use some scalar product propeties but it isn't clear for me how can I do that.,"(v_1,...,v_k)   G=G(v_1,...,v_k) = [\left\langle v_i,v_j\right\rangle ]_{i,j=1}^k  \det G (v_1,...,v_k)   \det G \neq0 \det \begin{vmatrix} \langle v_1,v_1\rangle & \langle v_1,v_2\rangle &\dots & \langle v_1,v_n\rangle\\
 \langle v_2,v_1\rangle & \langle v_2,v_2\rangle &\dots & \langle v_2,v_n\rangle\\
\vdots&\vdots&\ddots&\vdots\\
 \langle v_n,v_1\rangle & \langle v_n,v_2\rangle &\dots & \langle v_n,v_n\rangle\end{vmatrix}",['linear-algebra']
80,Second Order Matrix ODE,Second Order Matrix ODE,,"Given the equation $\frac{d^2X}{dt^2} = MX$ and the appropriate initial values, how would one go about solving this equation?  I've looked at Qualitative dependence of solution to second-order matrix differential equation on eigenvalues , which was very useful but I don't really understand how the change of basis was performed nor how the eigenvalues could be found. Are there any resources that explain how to tackle second order matrix equations out there?","Given the equation and the appropriate initial values, how would one go about solving this equation?  I've looked at Qualitative dependence of solution to second-order matrix differential equation on eigenvalues , which was very useful but I don't really understand how the change of basis was performed nor how the eigenvalues could be found. Are there any resources that explain how to tackle second order matrix equations out there?",\frac{d^2X}{dt^2} = MX,"['matrices', 'ordinary-differential-equations']"
81,$M_n(F)$ contains an isomorphic copy of every extension of $F$ of degree $d \leq n.$,contains an isomorphic copy of every extension of  of degree,M_n(F) F d \leq n.,"Source of the following Problem:Prob. $19.(b).,$ Section $13.2,$ from Abstract Algebra by Dummit and Foote(Second Edition). Let $K$ be an extension of $F$ of degree $n.$ Prove that $K$ is isomorphic to a subfield of the ring $M_n(F),$ so $M_n(F)$ contains an isomorphic copy of every extension of $F$ of degree $d \leq n.$ I proved that $K$ is isomorphic to a subfield of the ring $M_n(F)$ . I stuck in the next part, though from my previous question this it is clear that whenever $d | n$ it is true. But Is it true if $d<n$ and $d$ does not divide $n.$","Source of the following Problem:Prob. Section from Abstract Algebra by Dummit and Foote(Second Edition). Let be an extension of of degree Prove that is isomorphic to a subfield of the ring so contains an isomorphic copy of every extension of of degree I proved that is isomorphic to a subfield of the ring . I stuck in the next part, though from my previous question this it is clear that whenever it is true. But Is it true if and does not divide","19.(b)., 13.2, K F n. K M_n(F), M_n(F) F d \leq n. K M_n(F) d | n d<n d n.","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'field-theory']"
82,Is the kernel of a matrix its nullspace and nothing more? And what is the term for the dimension of a kernel?,Is the kernel of a matrix its nullspace and nothing more? And what is the term for the dimension of a kernel?,,"Are null space and kernel perfect synonyms? If so, why are there two different terms for them? Also, does the term ""nullity"" have a ""kernel"" equivalent? Nullity refers to the dimension of the null space (and thus dimension of the kernel), and seems very important, so is there another term I should be aware of that refers to nullity?","Are null space and kernel perfect synonyms? If so, why are there two different terms for them? Also, does the term ""nullity"" have a ""kernel"" equivalent? Nullity refers to the dimension of the null space (and thus dimension of the kernel), and seems very important, so is there another term I should be aware of that refers to nullity?",,"['linear-algebra', 'matrices', 'terminology']"
83,Does the power method converge?,Does the power method converge?,,I want to check if the power method converges for the matrix $A$ and the vector $\vec{v} $ where $$A=\begin{pmatrix}\lambda & 1 \\ 0 & -\lambda\end{pmatrix} \text{ and }  \vec{v} =\begin{pmatrix}1 \\ 1\end{pmatrix}$$ Do we have to apply some steps of the power method and see if the results converge to the absolute maximum eigenvalue of the matrix A or is there a criterion for convergence?,I want to check if the power method converges for the matrix and the vector where Do we have to apply some steps of the power method and see if the results converge to the absolute maximum eigenvalue of the matrix A or is there a criterion for convergence?,"A \vec{v}  A=\begin{pmatrix}\lambda & 1 \\ 0 & -\lambda\end{pmatrix} \text{ and } 
\vec{v} =\begin{pmatrix}1 \\ 1\end{pmatrix}","['matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'approximation']"
84,$\det(A^2+A-I_2)+\det(A^2+I_2) = 5$,,\det(A^2+A-I_2)+\det(A^2+I_2) = 5,Let $A \in M_{2\times 2}(\mathbb{C})$ and $\det(A)=1\DeclareMathOperator{\tr}{tr}$ Prove that $\det(A^2+A-I_2)+\det(A^2+I_2) = 5$ using Cayley-Hamilton Theorem $A^2-\tr(A)A+\det(A)I_2=0$ $\det\big(\tr(A)A-\det(A)I_2-I_2\big) + \det\big(A(A+A^{-1})\big)=5$ $\det\big(\tr(A)A-I_2(\det(A)+1)\big)+\det(A+A^{-1})=5$ using https://math.stackexchange.com/q/1937052 $\det(A+B)=\det A+\det B+\det A⋅\tr(A^{-1}B) $ $\tr(A)\det(A)-\big(\det(A)+1\big)\det(I_2)+\tr(A)\big(-\det(A)-1\big)^{-1}\tr(I^{-1}A)+\det(A)+\det(A^{-1})+\tr(A^2)=5 $ $\tr(A)-2-\tr(A)^20.5+1+1+\tr(A^2)=5$ $\tr(A)-\tr(A)^20.5+\tr\big(\tr(A)A-\det(A)I_2\big)=5$ $\tr(A)-\tr(A)^20.5+\tr(A)^2-\tr(I_2)=5$ $\tr(A)+0.5\tr(A)^2-2=5$ This is where I am stuck. And also I don't know how to prove the identity which I cited.,Let and Prove that using Cayley-Hamilton Theorem using https://math.stackexchange.com/q/1937052 This is where I am stuck. And also I don't know how to prove the identity which I cited.,"A \in M_{2\times 2}(\mathbb{C}) \det(A)=1\DeclareMathOperator{\tr}{tr} \det(A^2+A-I_2)+\det(A^2+I_2) = 5 A^2-\tr(A)A+\det(A)I_2=0 \det\big(\tr(A)A-\det(A)I_2-I_2\big) + \det\big(A(A+A^{-1})\big)=5 \det\big(\tr(A)A-I_2(\det(A)+1)\big)+\det(A+A^{-1})=5 \det(A+B)=\det A+\det B+\det A⋅\tr(A^{-1}B)  \tr(A)\det(A)-\big(\det(A)+1\big)\det(I_2)+\tr(A)\big(-\det(A)-1\big)^{-1}\tr(I^{-1}A)+\det(A)+\det(A^{-1})+\tr(A^2)=5
 \tr(A)-2-\tr(A)^20.5+1+1+\tr(A^2)=5 \tr(A)-\tr(A)^20.5+\tr\big(\tr(A)A-\det(A)I_2\big)=5 \tr(A)-\tr(A)^20.5+\tr(A)^2-\tr(I_2)=5 \tr(A)+0.5\tr(A)^2-2=5","['linear-algebra', 'matrices', 'proof-verification', 'determinant', 'trace']"
85,Does $0\prec B \prec A$ imply $A^{-1}B \prec I$,Does  imply,0\prec B \prec A A^{-1}B \prec I,"Does $0\prec B \prec A$ imply $A^{-1}B \prec I$ ? I know: $A,B$ are of the same size. $A$ , $B$ have strictly positive, real eigenvalues. $A^{-1}B$ may not be symmetric since $A^{-1}$ and $B$ may not commute. If $A$ and $B$ are simultaneous diagonalizable, this holds. This is because they have the same orthogonal eigenvectors. By diagonalization, we can prove this. But even though $A^{-1}B$ is not symmetric, the definition for $M\prec N$ is that $$x^TMx < x^TNx, \forall x\in \mathbb{R}^n.$$ Does this condition hold?","Does imply ? I know: are of the same size. , have strictly positive, real eigenvalues. may not be symmetric since and may not commute. If and are simultaneous diagonalizable, this holds. This is because they have the same orthogonal eigenvectors. By diagonalization, we can prove this. But even though is not symmetric, the definition for is that Does this condition hold?","0\prec B \prec A A^{-1}B \prec I A,B A B A^{-1}B A^{-1} B A B A^{-1}B M\prec N x^TMx < x^TNx, \forall x\in \mathbb{R}^n.","['matrices', 'inequality', 'positive-definite']"
86,"Why is $\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}])$, if [A,[A,B]]=0$?","Why is , if [A,[A,B]]=0$?","\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}])","Why is $\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}])$ , where $[A,B] = AB-BA$ for two quadratic matrices $A,B$ with $ [A,[A,B]]=0$ and $Tr$ is the trace of a matrix? I tried to rewrite this and reduce it to $\operatorname{Tr}([A,B]^{m-1} AB) = \operatorname{Tr}(BA [A,B]^{m-1} ) $ the following way: $$[A,B]^m = (AB-BA) [A,B]^{m-1} $$ $$[AB,[A,B]^{m-1}] = AB [A,B]^{m-1} - [A,B]^{m-1} AB$$ But now I do not see how to use $[A,[A,B]] = 0$ . Does anyone has hints for this or a hint how to advance?","Why is , where for two quadratic matrices with and is the trace of a matrix? I tried to rewrite this and reduce it to the following way: But now I do not see how to use . Does anyone has hints for this or a hint how to advance?","\operatorname{Tr}([A,B]^{m}) = \operatorname{Tr}([AB, [A,B]^{m-1}]) [A,B] = AB-BA A,B  [A,[A,B]]=0 Tr \operatorname{Tr}([A,B]^{m-1} AB) = \operatorname{Tr}(BA [A,B]^{m-1} )  [A,B]^m = (AB-BA) [A,B]^{m-1}  [AB,[A,B]^{m-1}] = AB [A,B]^{m-1} - [A,B]^{m-1} AB [A,[A,B]] = 0","['linear-algebra', 'matrices']"
87,General solution of ODE of a constant $3\times 3$ matrix,General solution of ODE of a constant  matrix,3\times 3,"Determine the general solution of the system $y'=Ay$ , where $A$ is a constant matrix, defined by $$A = \begin{pmatrix}-5&-8&4\\2&3&-2\\6&14&-5\end{pmatrix}$$ After attempting to find the eigenvalues of the system, I end up with eigenvalues $\lambda=-1,-3,-3$ , where $-3$ has a multiplicity of $2$ . Then, finding the corresponding vector for $\lambda=-1$ , $$ u= \begin{pmatrix}3\\-1\\1\end{pmatrix}, $$ for $\lambda=-3$ , $$v= \begin{pmatrix}-2\\1\\1\end{pmatrix}, $$ and for the second $ \lambda =-3$ the vector $$w=\begin{pmatrix}-1-2t\\\frac{1}{2}+t\\t\end{pmatrix}.$$ I understand how to get the first two eigevectors $u$ and $v$ , but how did they get the third eigenvector to be in terms of $t$ ? Is it because it has a multiplicity of $2$ , so there is a second solution that can be described in terms of a variable? If so, what is the process to finding that third eigenvector? And does this process generalize for an eigenvalue that would have perhaps a multiplicity of $3$ ? The general solution of the system is $$y(t) = C_{1}e^{-t}u + C_{2}e^{-3t}v + C_{3}e^{-3t}w$$ where $u$ , $v$ , and $w$ are defined above.","Determine the general solution of the system , where is a constant matrix, defined by After attempting to find the eigenvalues of the system, I end up with eigenvalues , where has a multiplicity of . Then, finding the corresponding vector for , for , and for the second the vector I understand how to get the first two eigevectors and , but how did they get the third eigenvector to be in terms of ? Is it because it has a multiplicity of , so there is a second solution that can be described in terms of a variable? If so, what is the process to finding that third eigenvector? And does this process generalize for an eigenvalue that would have perhaps a multiplicity of ? The general solution of the system is where , , and are defined above.","y'=Ay A A = \begin{pmatrix}-5&-8&4\\2&3&-2\\6&14&-5\end{pmatrix} \lambda=-1,-3,-3 -3 2 \lambda=-1 
u=
\begin{pmatrix}3\\-1\\1\end{pmatrix},
 \lambda=-3 v=
\begin{pmatrix}-2\\1\\1\end{pmatrix},
  \lambda =-3 w=\begin{pmatrix}-1-2t\\\frac{1}{2}+t\\t\end{pmatrix}. u v t 2 3 y(t) = C_{1}e^{-t}u + C_{2}e^{-3t}v + C_{3}e^{-3t}w u v w","['matrices', 'ordinary-differential-equations', 'proof-explanation', 'fundamental-solution']"
88,norm of positive semi-definite complex matrix,norm of positive semi-definite complex matrix,,"Suppose $0\neq X_n\in \mathbb{M}_{k(n)}(\mathbb{C})$ ,if $\lim_{n \to \infty}tr(X_n^*X_n)=0$ ,can we conclude that $\lim_{n \to \infty}\|X_n^*X_n\|=0$ ,where $tr$ is the standard trace on complex matrix, $\|·\|$ is the norm of matrix. My thought: the eigenvalues of $X_n^*X_n$ is non-negative, $ tr(X_n^*X_n) $ is equal to the sum of non-negative eigenvalues,so each non-negative eigenvalue is small enough,the norm of $X_n^*X_n$ is equal to the product of eigenvalues,so it tends to zero.Is my idea correct?","Suppose ,if ,can we conclude that ,where is the standard trace on complex matrix, is the norm of matrix. My thought: the eigenvalues of is non-negative, is equal to the sum of non-negative eigenvalues,so each non-negative eigenvalue is small enough,the norm of is equal to the product of eigenvalues,so it tends to zero.Is my idea correct?",0\neq X_n\in \mathbb{M}_{k(n)}(\mathbb{C}) \lim_{n \to \infty}tr(X_n^*X_n)=0 \lim_{n \to \infty}\|X_n^*X_n\|=0 tr \|·\| X_n^*X_n  tr(X_n^*X_n)  X_n^*X_n,"['linear-algebra', 'matrices', 'functional-analysis', 'adjoint-operators']"
89,Show that $SO(3)$ in an embedded submanifold of $\mathbb{R}^{3 \times 3}$,Show that  in an embedded submanifold of,SO(3) \mathbb{R}^{3 \times 3},"How does one go about proving that $SO(3)$ is an embedded submanifold of $\mathbb{R}^{3 \times 3}$ ? I know that there is a definition of embedded submanifold in terms of the induced topology, and I know the Cayley construction of charts for $SO(3)$ . I'm looking for a simpler method.","How does one go about proving that is an embedded submanifold of ? I know that there is a definition of embedded submanifold in terms of the induced topology, and I know the Cayley construction of charts for . I'm looking for a simpler method.",SO(3) \mathbb{R}^{3 \times 3} SO(3),"['matrices', 'differential-geometry', 'lie-groups', 'rotations', 'submanifold']"
90,The eigenvalues of $\begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix}$ are the singular values of $A$ along with the negative signs.,The eigenvalues of  are the singular values of  along with the negative signs.,\begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix} A,"The eigenvalues of $\begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix}$ are the singular values of $A$ along with the negative signs. Here $A$ is an $n \times n$ matrix, has $n$ singular values. Here we are consider both $\lambda $ and $- \lambda$ if $\lambda$ is an singular value of $A$ . Thus we have a set of $2n$ elements and we have to show that these are the eigenvalues of $\begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix}$ . Need some hint to proceed with the problem.","The eigenvalues of are the singular values of along with the negative signs. Here is an matrix, has singular values. Here we are consider both and if is an singular value of . Thus we have a set of elements and we have to show that these are the eigenvalues of . Need some hint to proceed with the problem.",\begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix} A A n \times n n \lambda  - \lambda \lambda A 2n \begin{pmatrix}0&A\\A^*&0\\ \end{pmatrix},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-analysis']"
91,singular matrix in numerical,singular matrix in numerical,,"I am trying to find all values for $\alpha$ and $\beta$ for which $$ A(\alpha, \beta)=  \left[ \begin{matrix} 3&0&-2\\\alpha&3&2\\-2&2&\beta  \end{matrix} \right] $$ is singular. My understanding is that I need to find values $\beta$ and $\alpha$ for which $det(A)=0$ Therefore so, $$\det(A) = 3 \begin{bmatrix} 3&2\\2&\beta \end{bmatrix} - 0 + (-2)\left[ \begin{matrix} \alpha&3\\-2&2 \end{matrix} \right]\\= 3(3\beta - 4) - 2(2\alpha +6) \\ = 9\beta -12 -4\alpha -12$$ If my workings are correct, then what is $\beta,\alpha?$","I am trying to find all values for and for which is singular. My understanding is that I need to find values and for which Therefore so, If my workings are correct, then what is","\alpha \beta  A(\alpha, \beta)= 
\left[
\begin{matrix}
3&0&-2\\\alpha&3&2\\-2&2&\beta 
\end{matrix}
\right]
 \beta \alpha det(A)=0 \det(A) = 3
\begin{bmatrix}
3&2\\2&\beta
\end{bmatrix} - 0 + (-2)\left[
\begin{matrix}
\alpha&3\\-2&2
\end{matrix}
\right]\\= 3(3\beta - 4) - 2(2\alpha +6) \\ = 9\beta -12 -4\alpha -12 \beta,\alpha?","['linear-algebra', 'matrices', 'algebra-precalculus', 'determinant']"
92,"If I subtract 1 from the n,n entry of a Pascal Matrix, why does the determinant become zero?","If I subtract 1 from the n,n entry of a Pascal Matrix, why does the determinant become zero?",,"In problem 19.2 , how did the author reach this statement: ""Since the n, n entry multiplies its cofactor positively, the overall determinant drops by 1 to become 0."" I was able to solve it a different way, but I feel that my approach was less efficient. I'd like to understand the solution's approach. Here is what I did: $det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&20\end{bmatrix}) - det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix})$ . and $ det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix}) = (-1)^3 * det(\begin{bmatrix}0&0&0&1\\1&1&1&1\\1&2&3&4\\1&3&6&10\end{bmatrix}) = (-1)^3 * (-1) * det(\begin{bmatrix}1&1&1\\1&2&3\\1&3&6\end{bmatrix})$ . therefore $det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = 1 - 1 = 0 $ . It feels like the solution skipped doing this step, I don't get it.","In problem 19.2 , how did the author reach this statement: ""Since the n, n entry multiplies its cofactor positively, the overall determinant drops by 1 to become 0."" I was able to solve it a different way, but I feel that my approach was less efficient. I'd like to understand the solution's approach. Here is what I did: . and . therefore . It feels like the solution skipped doing this step, I don't get it.",det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&20\end{bmatrix}) - det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix})  det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix}) = (-1)^3 * det(\begin{bmatrix}0&0&0&1\\1&1&1&1\\1&2&3&4\\1&3&6&10\end{bmatrix}) = (-1)^3 * (-1) * det(\begin{bmatrix}1&1&1\\1&2&3\\1&3&6\end{bmatrix}) det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = 1 - 1 = 0 ,"['linear-algebra', 'matrices', 'determinant']"
93,A interesting question on Skew-symmetric matrix...finding the determinant.,A interesting question on Skew-symmetric matrix...finding the determinant.,,"Let $a_1,a_2,\cdots ,a_{2n}$ be complex numbers. We construct a $2n \times 2n$ matrix, say $A$ which is skew symmetric and entries are from complex numbers.    $A=(\alpha_{ij})$, where $\alpha_{ij}=a_ia_j$ for $i<j$. To find the determinant of the matrix $A$. Since $A$ is a even order skew-symmetric matrix, we have determinant of $A$ a perfect square. My Intuition: $\det A = a_1^2 \times a_2^2 \times \cdots \times a_{2n}^2$. I was trying to see what happens when $n=2$, i.e. we have $4 \times 4$ matrix $A$. Thus we have complex numbers $a_1,a_2,a_3 \  \text{and} \ a_4$ and \ $A= \begin{bmatrix}       0 & a_1a_2 & a_1a_3 & a_1a_4 \\     -a_1a_2 & 0 & a_2a_3 & a_2a_4 \\     -a_1a_3 & -a_2a_3 & 0 & a_3a_4\\      -a_1a_4 & -a_2a_4 & -a_3a_4 & 0 \end{bmatrix} $ We can see  $ A= \left[ \begin{array}{c|c} D_1 & B \\ \hline -B^T & D_2 \end{array} \right] $, where $D_1 = \begin{bmatrix} 0 & a_1a_2\\ -a_1a_2 & 0  \end{bmatrix} $, $D_2 = \begin{bmatrix} 0 & a_3a_4\\ -a_3a_4 & 0  \end{bmatrix} $ and $ B = \begin{bmatrix} a_1a_3 & a_1a_4\\ a_2a_3 & a_2a_4 \end{bmatrix} $ Also I have noted that $\det B =0$. Can we use the result of determinant of block matrices? Can someone shed some light how to do the problem? Let $ D = \text{diag}(a_1,a_2,\cdots,a_{2n})$ , and $C = (c_{ij})$, where $C$ is a skew symmetric matrix with $c_{ij} = 1$ when $i<j$. Then one can easily see that $A=DCD$. So we are let to prove that $\det C =1$.","Let $a_1,a_2,\cdots ,a_{2n}$ be complex numbers. We construct a $2n \times 2n$ matrix, say $A$ which is skew symmetric and entries are from complex numbers.    $A=(\alpha_{ij})$, where $\alpha_{ij}=a_ia_j$ for $i<j$. To find the determinant of the matrix $A$. Since $A$ is a even order skew-symmetric matrix, we have determinant of $A$ a perfect square. My Intuition: $\det A = a_1^2 \times a_2^2 \times \cdots \times a_{2n}^2$. I was trying to see what happens when $n=2$, i.e. we have $4 \times 4$ matrix $A$. Thus we have complex numbers $a_1,a_2,a_3 \  \text{and} \ a_4$ and \ $A= \begin{bmatrix}       0 & a_1a_2 & a_1a_3 & a_1a_4 \\     -a_1a_2 & 0 & a_2a_3 & a_2a_4 \\     -a_1a_3 & -a_2a_3 & 0 & a_3a_4\\      -a_1a_4 & -a_2a_4 & -a_3a_4 & 0 \end{bmatrix} $ We can see  $ A= \left[ \begin{array}{c|c} D_1 & B \\ \hline -B^T & D_2 \end{array} \right] $, where $D_1 = \begin{bmatrix} 0 & a_1a_2\\ -a_1a_2 & 0  \end{bmatrix} $, $D_2 = \begin{bmatrix} 0 & a_3a_4\\ -a_3a_4 & 0  \end{bmatrix} $ and $ B = \begin{bmatrix} a_1a_3 & a_1a_4\\ a_2a_3 & a_2a_4 \end{bmatrix} $ Also I have noted that $\det B =0$. Can we use the result of determinant of block matrices? Can someone shed some light how to do the problem? Let $ D = \text{diag}(a_1,a_2,\cdots,a_{2n})$ , and $C = (c_{ij})$, where $C$ is a skew symmetric matrix with $c_{ij} = 1$ when $i<j$. Then one can easily see that $A=DCD$. So we are let to prove that $\det C =1$.",,"['linear-algebra', 'matrices', 'determinant', 'matrix-decomposition', 'symmetric-matrices']"
94,Large invertible submatrix in random sparse matrices,Large invertible submatrix in random sparse matrices,,"The Problem Informal statement of the problem: consider a natural distribution $D_n$ of very sparse $n\times n$ matrices over the binary field $\mathbb{F}_2$ - that is, a matrix sampled from $D_n$ contains a constant number of $1$ per row. How likely is a matrix sampled from $D_n$ to contain a large invertible submatrix? More formally, consider the two ""natural distributions"" over $n\times n$ matrices in $\mathsf{M}_{n\times n}(\mathbb{F}_2)$ : $D^0_n$ samples each entry of the matrix independently from the Bernouilli distribution with probability $p_n = d/n$ , where $d$ is a small constant (that is, each entry of a sampled matrix is $1$ with probability $d/n$ , and $0$ with probability $1-d/n$ ). $D^1_n$ samples each row of the matrix independently; to sample the $i$ th row, pick a random size- $d$ subset $S_i$ of $[1,\cdots, n]$ . The subset $S_i$ denotes the position of the $1$ s in the $i$ th row (and $[1,\cdots, n]\setminus S_i$ denotes the position of the $0$ s). Then, consider the following conjecture (with $b=0$ and $b=1$ giving two variants depending on the choice of the distribution): there exists constants $\gamma<1, N$ such that for any $n > N$ , the probability that a random matrix sampled from $D^b_n$ contains an invertible submatrix of size $\gamma n \times \gamma n$ is at least $1-f(n)$ . Here, $f(n)$ is some fixed function that goes to $0$ when $n$ grows, e.g. an inverse polynomial, or an inverse exponential. Question What is known about the above conjecture? Is it known to hold for one of $D^0_n,D^1_n$ ? Alternatively, is it known to hold if we relax the requirement to containing an invertible submatrix of size $\gamma n \times \gamma n$ with constant probability (instead of a probability that goes to $1$ when $n$ increases)? I'm specifically interested in the case $d = 5$ (that is, the sparse matrices have on average five $1$ s per row), in case this simplifies the problem in any way. I would also be happy with any partial answer, giving pointers to relevant literature, relating the problem to existing problems, or providing any clues. Thoughts on the question I have not found any literature directly addressing the problem above. There is, however, a rather large body of research on the rank of random less-sparse matrices, namely, $n\times n$ matrices containing an average of $O(\log n)$ $1$ s per row (as opposed to constant in my scenario). More specifically, this paper shows that the rank of random (Bernouilli) sparse matrices will be $n - O(1)$ with probability $1-o(1)$ , where the Bernouilli probability is $p_n = c\log n/n$ for some constant $c>0$ . A similar result for any field is proven in this other paper . However, it is not clear to me whether these results could be extended to guarantee a rank $\gamma\cdot n$ for some constant $\gamma$ , with probability $1-o(1)$ , in the setting $p_n = d/n$ for some small constant $d$ . Furthermore, I do not know either if these results can be extended to the stronger statement that a random sparse matrix has a large invertible subsystem (as opposed to a large rank). In another paper (which I could not find back), there was a more fine-grained analysis of the rank of a random $O(\log n)$ -sparse matrix over $\mathbb{F}_2$ . Unfortunately, replacing $O(\log n)$ by a constant in their calculations only leads to the statement that the number of dependencies in a random sparse matrix is at most $O(n)$ (with probability $1-o(1)$ ); it does not allow (as far as I could see) to prove the stronger statement that the number of dependencies will be upper bounded by $(1-\gamma)\cdot n$ for some constant $1>\gamma >0$ . And this is still only about the rank anyway. Note I've not verified this specific conjecture experimentally, but I've done so (well, some coauthors of mine have done so) for a more complex distribution over sparse matrices (for the context, it arose when analyzing the success probability of a subexponential-time attack on a cryptographic pseudorandom generator), and it seemed to be verified (with a constant $\gamma$ above $0.9$ ), for large values of $n$ (a few hundredth). The sampled sparse matrix always contained a $\gamma n\times \gamma n$ invertible submatrix in our experiments; the same seems very likely to hold also for the simpler distributions I consider in this question.","The Problem Informal statement of the problem: consider a natural distribution of very sparse matrices over the binary field - that is, a matrix sampled from contains a constant number of per row. How likely is a matrix sampled from to contain a large invertible submatrix? More formally, consider the two ""natural distributions"" over matrices in : samples each entry of the matrix independently from the Bernouilli distribution with probability , where is a small constant (that is, each entry of a sampled matrix is with probability , and with probability ). samples each row of the matrix independently; to sample the th row, pick a random size- subset of . The subset denotes the position of the s in the th row (and denotes the position of the s). Then, consider the following conjecture (with and giving two variants depending on the choice of the distribution): there exists constants such that for any , the probability that a random matrix sampled from contains an invertible submatrix of size is at least . Here, is some fixed function that goes to when grows, e.g. an inverse polynomial, or an inverse exponential. Question What is known about the above conjecture? Is it known to hold for one of ? Alternatively, is it known to hold if we relax the requirement to containing an invertible submatrix of size with constant probability (instead of a probability that goes to when increases)? I'm specifically interested in the case (that is, the sparse matrices have on average five s per row), in case this simplifies the problem in any way. I would also be happy with any partial answer, giving pointers to relevant literature, relating the problem to existing problems, or providing any clues. Thoughts on the question I have not found any literature directly addressing the problem above. There is, however, a rather large body of research on the rank of random less-sparse matrices, namely, matrices containing an average of s per row (as opposed to constant in my scenario). More specifically, this paper shows that the rank of random (Bernouilli) sparse matrices will be with probability , where the Bernouilli probability is for some constant . A similar result for any field is proven in this other paper . However, it is not clear to me whether these results could be extended to guarantee a rank for some constant , with probability , in the setting for some small constant . Furthermore, I do not know either if these results can be extended to the stronger statement that a random sparse matrix has a large invertible subsystem (as opposed to a large rank). In another paper (which I could not find back), there was a more fine-grained analysis of the rank of a random -sparse matrix over . Unfortunately, replacing by a constant in their calculations only leads to the statement that the number of dependencies in a random sparse matrix is at most (with probability ); it does not allow (as far as I could see) to prove the stronger statement that the number of dependencies will be upper bounded by for some constant . And this is still only about the rank anyway. Note I've not verified this specific conjecture experimentally, but I've done so (well, some coauthors of mine have done so) for a more complex distribution over sparse matrices (for the context, it arose when analyzing the success probability of a subexponential-time attack on a cryptographic pseudorandom generator), and it seemed to be verified (with a constant above ), for large values of (a few hundredth). The sampled sparse matrix always contained a invertible submatrix in our experiments; the same seems very likely to hold also for the simpler distributions I consider in this question.","D_n n\times n \mathbb{F}_2 D_n 1 D_n n\times n \mathsf{M}_{n\times n}(\mathbb{F}_2) D^0_n p_n = d/n d 1 d/n 0 1-d/n D^1_n i d S_i [1,\cdots, n] S_i 1 i [1,\cdots, n]\setminus S_i 0 b=0 b=1 \gamma<1, N n > N D^b_n \gamma n \times \gamma n 1-f(n) f(n) 0 n D^0_n,D^1_n \gamma n \times \gamma n 1 n d = 5 1 n\times n O(\log n) 1 n - O(1) 1-o(1) p_n = c\log n/n c>0 \gamma\cdot n \gamma 1-o(1) p_n = d/n d O(\log n) \mathbb{F}_2 O(\log n) O(n) 1-o(1) (1-\gamma)\cdot n 1>\gamma >0 \gamma 0.9 n \gamma n\times \gamma n","['linear-algebra', 'probability', 'matrices', 'random-matrices', 'sparse-matrices']"
95,"Understanding the concept of isomorphism between Hom(V,W) and $M_{m\times n}$","Understanding the concept of isomorphism between Hom(V,W) and",M_{m\times n},"I'd appreciate a clarification about the following issue. It's known that Hom(V,W) is isomorphic to $M_{m\times n}$ Correct me if I'm wrong, but as I get it, the meaning of the above statement is that every linear transformation from V to W is represented uniquely by an mxn matrix, and vice versa. However, I'm having a hard time understanding something. Since we are free to choose any bases for V and W, consequently we get different representation matrices. How does it not contradict the statement mentioning the isomorphism, according to which, as I get it (and probably not correctly), there is a unique respective matrix? Thanks in advance!","I'd appreciate a clarification about the following issue. It's known that Hom(V,W) is isomorphic to $M_{m\times n}$ Correct me if I'm wrong, but as I get it, the meaning of the above statement is that every linear transformation from V to W is represented uniquely by an mxn matrix, and vice versa. However, I'm having a hard time understanding something. Since we are free to choose any bases for V and W, consequently we get different representation matrices. How does it not contradict the statement mentioning the isomorphism, according to which, as I get it (and probably not correctly), there is a unique respective matrix? Thanks in advance!",,"['matrices', 'linear-transformations', 'vector-space-isomorphism']"
96,Rank of block matrix over $\mathbb{Q}$ and $\mathbb{F}_p$.,Rank of block matrix over  and .,\mathbb{Q} \mathbb{F}_p,"Let $M=\left[\begin{array}{c} A \\ B \end{array}\right]$ be a block matrix with integer entries. Let $p$ be a prime, and let $\mathbb{F}_p$ be the field with $p$ elements. Define $A_p, B_p,$ and $M_p$ be the matrices $A, B,$ and $M$ with entries reduced $\mod p$ respectively (considered as matrices with entries in $\mathbb{F}_p$). Let $\mathbb{Q}$ be the field of rational numbers. Suppose that the rank of $A$ over $\mathbb{Q}$ is the same as the rank of $A_p$ over $\mathbb{F}_p$ and that the rank of $B$ over $\mathbb{Q}$ is the same as the rank of $B_p$ over $\mathbb{F}_p$. Is it true that the rank of $M$ over $\mathbb{Q}$ is the same as the rank of $M_p$ over $\mathbb{F}_p$? If it is false, how would one go about producing a counterexample?","Let $M=\left[\begin{array}{c} A \\ B \end{array}\right]$ be a block matrix with integer entries. Let $p$ be a prime, and let $\mathbb{F}_p$ be the field with $p$ elements. Define $A_p, B_p,$ and $M_p$ be the matrices $A, B,$ and $M$ with entries reduced $\mod p$ respectively (considered as matrices with entries in $\mathbb{F}_p$). Let $\mathbb{Q}$ be the field of rational numbers. Suppose that the rank of $A$ over $\mathbb{Q}$ is the same as the rank of $A_p$ over $\mathbb{F}_p$ and that the rank of $B$ over $\mathbb{Q}$ is the same as the rank of $B_p$ over $\mathbb{F}_p$. Is it true that the rank of $M$ over $\mathbb{Q}$ is the same as the rank of $M_p$ over $\mathbb{F}_p$? If it is false, how would one go about producing a counterexample?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-rank']"
97,Bounding the solution to a Riccati equation,Bounding the solution to a Riccati equation,,"I have the following continuous-time matrix Riccati equation $$A X + X A' - X b b' X + Q = 0$$ where $Q>0$, $A$ is a diagonal matrix with strictly negative eigenvalues and $b$ is a (column) vector. Without knowing $b$, only that $\| b \|_2 \leq \beta$, can I find a diagonal matrix $Y$, with strictly positive eigenvalues, such that $Y\leq X$, (where $Y\leq X$ means that $v'Yv\leq v'Xv$ for any vector $v$)?","I have the following continuous-time matrix Riccati equation $$A X + X A' - X b b' X + Q = 0$$ where $Q>0$, $A$ is a diagonal matrix with strictly negative eigenvalues and $b$ is a (column) vector. Without knowing $b$, only that $\| b \|_2 \leq \beta$, can I find a diagonal matrix $Y$, with strictly positive eigenvalues, such that $Y\leq X$, (where $Y\leq X$ means that $v'Yv\leq v'Xv$ for any vector $v$)?",,"['matrices', 'matrix-equations', 'control-theory', 'positive-definite', 'kalman-filter']"
98,Show that $f(x) = x^TQx$ is convex using the inequality $f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0$,Show that  is convex using the inequality,f(x) = x^TQx f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0,"Suppose that $f(x) = x^TQx$, where $Q$ is $n\times n$ symmetric   positive semidefinite matrix. Show that $f(x)$ is convex on the domain   $\mathbb{R}^n$. Hint: it may be convenient to prove the following   equivalent inequality: $$f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0$$   for all $\alpha\in[0,1]$ UPDATE : I redid the calculations: I did and tried to prove it's $\le0$ by the following: $$(y^T + ax^T-ay^T)(Qy+aQx-aQy) - ax^TQx-(1-a)y^TQy = \\y^TQy + ay^TQx-ay^TQy+ax^TQy+a^2x^TQx-a^2x^TQy -ay^TQy -a^2y^TQx +a^2y^TQy- ax^TQx -(1-a)y^TQy = \\(1-2\alpha+\alpha^2-1+a)y^tQy + (\alpha^2-\alpha) x^tQx+(\alpha+\alpha-\alpha^2-\alpha^2)y^tQx =\\ (-2\alpha+\alpha^2+a)y^tQy+(\alpha^2-\alpha)x^tQx+(2\alpha + 2\alpha^2)y^tQx$$ But I still don't see why this should be less than $0$. We have that $y^tQy$ and $x^tQx$ are positive semidefinite matrices. However I don't know how to deal with the term $y^tQx$","Suppose that $f(x) = x^TQx$, where $Q$ is $n\times n$ symmetric   positive semidefinite matrix. Show that $f(x)$ is convex on the domain   $\mathbb{R}^n$. Hint: it may be convenient to prove the following   equivalent inequality: $$f(y+\alpha(x-y))-\alpha f(x)-(1-a)f(y)\le 0$$   for all $\alpha\in[0,1]$ UPDATE : I redid the calculations: I did and tried to prove it's $\le0$ by the following: $$(y^T + ax^T-ay^T)(Qy+aQx-aQy) - ax^TQx-(1-a)y^TQy = \\y^TQy + ay^TQx-ay^TQy+ax^TQy+a^2x^TQx-a^2x^TQy -ay^TQy -a^2y^TQx +a^2y^TQy- ax^TQx -(1-a)y^TQy = \\(1-2\alpha+\alpha^2-1+a)y^tQy + (\alpha^2-\alpha) x^tQx+(\alpha+\alpha-\alpha^2-\alpha^2)y^tQx =\\ (-2\alpha+\alpha^2+a)y^tQy+(\alpha^2-\alpha)x^tQx+(2\alpha + 2\alpha^2)y^tQx$$ But I still don't see why this should be less than $0$. We have that $y^tQy$ and $x^tQx$ are positive semidefinite matrices. However I don't know how to deal with the term $y^tQx$",,"['matrices', 'convex-analysis', 'quadratic-forms', 'positive-semidefinite']"
99,The Lie algebra version of $1 \to SU(N) \to U(N) \to U(1) \to 1$,The Lie algebra version of,1 \to SU(N) \to U(N) \to U(1) \to 1,"For the unitary group $U(N)$ and the special unitary group $SU(N)$, we have the exact sequence $$ 1 \to SU(N) \to U(N) \to U(1) \to 1. $$ How does this behave at the level of algebras? I suppose we still get an exact sequence of Lie algebras? In particular, I would be very interested in understanding this sequence in terms of the Gell-Mann matrix https://en.wikipedia.org/wiki/Gell-Mann_matrices presentation of $\frak{su}_3$: https://en.wikipedia.org/wiki/Special_unitary_group#Lie_algebra","For the unitary group $U(N)$ and the special unitary group $SU(N)$, we have the exact sequence $$ 1 \to SU(N) \to U(N) \to U(1) \to 1. $$ How does this behave at the level of algebras? I suppose we still get an exact sequence of Lie algebras? In particular, I would be very interested in understanding this sequence in terms of the Gell-Mann matrix https://en.wikipedia.org/wiki/Gell-Mann_matrices presentation of $\frak{su}_3$: https://en.wikipedia.org/wiki/Special_unitary_group#Lie_algebra",,"['matrices', 'representation-theory', 'lie-groups', 'lie-algebras']"
