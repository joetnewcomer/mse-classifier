,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove the result of the following integral? [duplicate],How to prove the result of the following integral? [duplicate],,"This question already has an answer here : Conjectured identity involving an elliptic integral and a Bessel function (1 answer) Closed 23 days ago . How to prove that $$ \int _0^{\infty }\frac{K\left(\frac{1}{2}-\frac{1}{2 \sqrt{x+1}}\right)}{\sqrt[4]{x+1}}e^{-x}{\rm d}x = \frac{1}{2} \sqrt{e \pi }  K_{1/4}\left(\frac{1}{2}\right) $$ where $K(x)$ in the integral is the complete elliptic integral of the first kind ( EllipticK[x] in Mathematica) and $K_{\frac{1}{4}}(x)$ in the RHS is the modified Bessel function of the second kind ( BesselK[1/4, x] in Mathematica). Mathematica cannot calculate the above integral, but the following Mathematica code NIntegrate[   EllipticK[1/2 - 1/2/Sqrt[x + 1]] (x + 1)^(-1/4) Exp[-x], {x,     0, \[Infinity]}, PrecisionGoal -> 100, WorkingPrecision -> 100] -   1/2 Sqrt[E  Pi] BesselK[1/4, 1/2] shows that they are of the same value. Update: How do I find the result? I tried to evaluate the following integral in two ways: $$\int_{-\infty}^{\infty} e^{-x^2-g x^4}dx$$ On the one hand, using Integrate[Exp[-x^2 - g  x^4], {x, -\[Infinity], \[Infinity]}] , Mathematica gives $\frac{e^{\left.\frac{1}{8}\right/g} K_{\frac{1}{4}}\left(\frac{1}{8 g}\right)}{2 \sqrt{g}}$ . On the other hand, we can expand the interaction part in a power series: $$\begin{aligned}\int_{-\infty}^\infty\mathrm{d}xe^{-x^2-g x^4}=\int_{-\infty}^\infty\mathrm{d}xe^{-x^2}\sum_{k=0}^\infty\frac{(-g x^4)^k}{k!}\end{aligned}=\sum_{k=0}^\infty\sqrt{\pi}\frac{(-g)^k(4k)!}{2^{4k}(2k)!k!}=\sum_{k=0}^\infty a_k g^k$$ However, the summation $f(x)=\sum a_k x^k$ does not converge. But we can apply the Borel summation to calculate $g(x)=\sum a_n x^n/n!$ and transform the result to $f(x)$ by $f(x)=\int_0^\infty g(b x)e^{-b}$ . The following code shows the procedure: Sum[Integrate[   SeriesCoefficient[Exp[-x^2 - g  x^4], {g, 0, k}] g^k/     k!, {x, -\[Infinity], \[Infinity]}], {k, 0, \[Infinity]}]  Assuming[g > 0,   Integrate[(% /. g -> x  g) Exp[-x], {x, 0, \[Infinity]}]] Mathematica can do the summation but cannot do the next integration. However, we expect the final result to be the same as the result of $\int_{-\infty}^\infty\mathrm{d}xe^{-x^2-g x^4}$ , which means that $$\int_0^{\infty } \frac{2 e^{-x} K\left(\frac{\sqrt{4 g x+1}-1}{2 \sqrt{4 g x+1}}\right)}{\sqrt{\pi } \sqrt[4]{4 g    x+1}} \, dx=\frac{e^{\left.\frac{1}{8}\right/g} K_{\frac{1}{4}}\left(\frac{1}{8 g}\right)}{2 \sqrt{g}} $$ Let $g=1/4$ we can get the integral. The above process is not particularly rigorous. I want to know if there is a rigorous method to prove this integral result.","This question already has an answer here : Conjectured identity involving an elliptic integral and a Bessel function (1 answer) Closed 23 days ago . How to prove that where in the integral is the complete elliptic integral of the first kind ( EllipticK[x] in Mathematica) and in the RHS is the modified Bessel function of the second kind ( BesselK[1/4, x] in Mathematica). Mathematica cannot calculate the above integral, but the following Mathematica code NIntegrate[   EllipticK[1/2 - 1/2/Sqrt[x + 1]] (x + 1)^(-1/4) Exp[-x], {x,     0, \[Infinity]}, PrecisionGoal -> 100, WorkingPrecision -> 100] -   1/2 Sqrt[E  Pi] BesselK[1/4, 1/2] shows that they are of the same value. Update: How do I find the result? I tried to evaluate the following integral in two ways: On the one hand, using Integrate[Exp[-x^2 - g  x^4], {x, -\[Infinity], \[Infinity]}] , Mathematica gives . On the other hand, we can expand the interaction part in a power series: However, the summation does not converge. But we can apply the Borel summation to calculate and transform the result to by . The following code shows the procedure: Sum[Integrate[   SeriesCoefficient[Exp[-x^2 - g  x^4], {g, 0, k}] g^k/     k!, {x, -\[Infinity], \[Infinity]}], {k, 0, \[Infinity]}]  Assuming[g > 0,   Integrate[(% /. g -> x  g) Exp[-x], {x, 0, \[Infinity]}]] Mathematica can do the summation but cannot do the next integration. However, we expect the final result to be the same as the result of , which means that Let we can get the integral. The above process is not particularly rigorous. I want to know if there is a rigorous method to prove this integral result.","
\int _0^{\infty }\frac{K\left(\frac{1}{2}-\frac{1}{2 \sqrt{x+1}}\right)}{\sqrt[4]{x+1}}e^{-x}{\rm d}x =
\frac{1}{2} \sqrt{e \pi }  K_{1/4}\left(\frac{1}{2}\right)
 K(x) K_{\frac{1}{4}}(x) \int_{-\infty}^{\infty} e^{-x^2-g x^4}dx \frac{e^{\left.\frac{1}{8}\right/g} K_{\frac{1}{4}}\left(\frac{1}{8 g}\right)}{2 \sqrt{g}} \begin{aligned}\int_{-\infty}^\infty\mathrm{d}xe^{-x^2-g x^4}=\int_{-\infty}^\infty\mathrm{d}xe^{-x^2}\sum_{k=0}^\infty\frac{(-g x^4)^k}{k!}\end{aligned}=\sum_{k=0}^\infty\sqrt{\pi}\frac{(-g)^k(4k)!}{2^{4k}(2k)!k!}=\sum_{k=0}^\infty a_k g^k f(x)=\sum a_k x^k g(x)=\sum a_n x^n/n! f(x) f(x)=\int_0^\infty g(b x)e^{-b} \int_{-\infty}^\infty\mathrm{d}xe^{-x^2-g x^4} \int_0^{\infty } \frac{2 e^{-x} K\left(\frac{\sqrt{4 g x+1}-1}{2 \sqrt{4 g x+1}}\right)}{\sqrt{\pi } \sqrt[4]{4 g
   x+1}} \, dx=\frac{e^{\left.\frac{1}{8}\right/g} K_{\frac{1}{4}}\left(\frac{1}{8 g}\right)}{2 \sqrt{g}}  g=1/4","['integration', 'special-functions', 'bessel-functions', 'elliptic-functions']"
1,closed form of $\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x$,closed form of,\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x,"Question: closed form of $$\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x$$ My try to solve the integral $$ \begin{aligned} & I=\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x \\ & \text { - } \operatorname{Li}_{s+1}(z)=\frac{(-1)^s}{s !} \int_{[0 ; 1]} \frac{z \cdot \log ^s(t)}{1-t z} d t \\ & \rightarrow \operatorname{Li}_3\left(-x^2\right)=-\frac{1}{2} \int_{[0 ; 1]} \frac{x^2 \cdot \log ^2(t)}{1+t x^2} d t \\ & I=-\frac{1}{2} \iint_{[0 ; 1]^2} \frac{x^2 \cdot \log ^2(t)}{(1+x)\left(1+t x^2\right)} d t d x \\ & \log t=-u \rightarrow d t=-e^{-u} \\ & {[0 ; 1] \rightarrow \mathrm{R}^{+}} \\ & I=-\frac{1}{2} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{x^2 \cdot t^2 e^{-u}}{(1+x)\left(1+x^2 e^{-u}\right)} d u d x \\ & I=-\frac{1}{2} \sum_{n \geqslant 0} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{(-1)^n x^{2+2 n} \cdot t^2 e^{-u(n+1)}}{(1+x)} d u d x \\ & u(n+1)=\varphi \rightarrow d u=d \varphi /(n+1) \\ & I=-\frac{1}{2} \sum_{n \geqslant 0} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{(-1)^n x^{2+2 n} \varphi^2 \cdot e^{-\varphi}}{(n+1)^3(1+x)} d \varphi d x \\ & I=-\sum_{n \geqslant 0} \int_{[0 ; 1]} \frac{(-1)^n}{(1+n)^3} \cdot \frac{x^{2 n+2}}{1+x} d x \\ & \text { - } \int_{[0 ; 1]} \frac{\Psi^{2 \lambda+2}}{1+\Psi} \mathrm{d} \Psi=\frac{1}{2}\left[H_{n+1}-H_{n+1 / 2}\right] ; n>-3 / 2 \\ & I=\frac{1}{2} \sum_{n \geqslant 0} \frac{(-1)^{n+1}}{(1+n)^3} \cdot\left[H_{n+1}-H_{n+1 / 2}\right] \\ & \therefore \int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x=\frac{1}{2} \sum_{n \geqslant 0} \frac{(-1)^{n+1}}{(1+n)^3} \cdot\left[H_{n+1}-H_{n+1 / 2}\right] \\ & \rightarrow \int_{[0 ; 1]} \frac{\mathrm{Li}_3\left(-x^2\right)}{1+x} d x=-0,181596 \ldots \\ & \end{aligned} $$ is there any way to evaluate last Euler sum",Question: closed form of My try to solve the integral is there any way to evaluate last Euler sum,"\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x 
\begin{aligned}
& I=\int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x \\
& \text { - } \operatorname{Li}_{s+1}(z)=\frac{(-1)^s}{s !} \int_{[0 ; 1]} \frac{z \cdot \log ^s(t)}{1-t z} d t \\
& \rightarrow \operatorname{Li}_3\left(-x^2\right)=-\frac{1}{2} \int_{[0 ; 1]} \frac{x^2 \cdot \log ^2(t)}{1+t x^2} d t \\
& I=-\frac{1}{2} \iint_{[0 ; 1]^2} \frac{x^2 \cdot \log ^2(t)}{(1+x)\left(1+t x^2\right)} d t d x \\
& \log t=-u \rightarrow d t=-e^{-u} \\
& {[0 ; 1] \rightarrow \mathrm{R}^{+}} \\
& I=-\frac{1}{2} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{x^2 \cdot t^2 e^{-u}}{(1+x)\left(1+x^2 e^{-u}\right)} d u d x \\
& I=-\frac{1}{2} \sum_{n \geqslant 0} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{(-1)^n x^{2+2 n} \cdot t^2 e^{-u(n+1)}}{(1+x)} d u d x \\
& u(n+1)=\varphi \rightarrow d u=d \varphi /(n+1) \\
& I=-\frac{1}{2} \sum_{n \geqslant 0} \iint_{[0 ; 1] \times \mathrm{R}^{+}} \frac{(-1)^n x^{2+2 n} \varphi^2 \cdot e^{-\varphi}}{(n+1)^3(1+x)} d \varphi d x \\
& I=-\sum_{n \geqslant 0} \int_{[0 ; 1]} \frac{(-1)^n}{(1+n)^3} \cdot \frac{x^{2 n+2}}{1+x} d x \\
& \text { - } \int_{[0 ; 1]} \frac{\Psi^{2 \lambda+2}}{1+\Psi} \mathrm{d} \Psi=\frac{1}{2}\left[H_{n+1}-H_{n+1 / 2}\right] ; n>-3 / 2 \\
& I=\frac{1}{2} \sum_{n \geqslant 0} \frac{(-1)^{n+1}}{(1+n)^3} \cdot\left[H_{n+1}-H_{n+1 / 2}\right] \\
& \therefore \int_{[0 ; 1]} \frac{\operatorname{Li}_3\left(-x^2\right)}{1+x} d x=\frac{1}{2} \sum_{n \geqslant 0} \frac{(-1)^{n+1}}{(1+n)^3} \cdot\left[H_{n+1}-H_{n+1 / 2}\right] \\
& \rightarrow \int_{[0 ; 1]} \frac{\mathrm{Li}_3\left(-x^2\right)}{1+x} d x=-0,181596 \ldots \\
&
\end{aligned}
","['calculus', 'integration']"
2,General formula for reversing double integral bounds,General formula for reversing double integral bounds,,"The double integral over the region: $$ R = \left\{ \left( x,\: y \right) : a \leqslant x \leqslant b,\: g\left( x \right) \leqslant y \leqslant h\left( x \right) \right\} $$ is expressed as $$ \iint_R f\left( x,\: y \right) \mathrm{d}A = \int_a^b \left[ \int_{g\left( x \right)}^{h\left( x \right)} f\left( x, \: y \right) \mathrm{d}y \right] \mathrm{d}x. $$ By Fubini's theorem, any permutation of the order of integration is equivalent if the function $f$ is integrable. Assuming $f$ is integrable, if $g$ and $h$ are invertible on the interval $a \leqslant x \leqslant b$ , is it correct to say that in general, the reversed double integral has the form: $$ \iint_R f\left( x, \: y \right) \mathrm{d}A = \int_{g\left( a \right)}^{h\left( b \right)} \left[ \int_{h^{-1}\left( y \right)}^{g^{-1}\left( y \right)} f\left( x, \: y \right) \mathrm{d}x \right] \mathrm{d}y. $$ Would there be any other restrictions on the functions $g$ and $h$ ? Thank you for your help.","The double integral over the region: is expressed as By Fubini's theorem, any permutation of the order of integration is equivalent if the function is integrable. Assuming is integrable, if and are invertible on the interval , is it correct to say that in general, the reversed double integral has the form: Would there be any other restrictions on the functions and ? Thank you for your help.","
R = \left\{ \left( x,\: y \right) : a \leqslant x \leqslant b,\: g\left( x \right) \leqslant y \leqslant h\left( x \right) \right\}
 
\iint_R f\left( x,\: y \right) \mathrm{d}A = \int_a^b \left[ \int_{g\left( x \right)}^{h\left( x \right)} f\left( x, \: y \right) \mathrm{d}y \right] \mathrm{d}x.
 f f g h a \leqslant x \leqslant b 
\iint_R f\left( x, \: y \right) \mathrm{d}A = \int_{g\left( a \right)}^{h\left( b \right)} \left[ \int_{h^{-1}\left( y \right)}^{g^{-1}\left( y \right)} f\left( x, \: y \right) \mathrm{d}x \right] \mathrm{d}y.
 g h","['integration', 'multivariable-calculus', 'inverse-function', 'multiple-integral', 'fubini-tonelli-theorems']"
3,Computing $\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx$ and $\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$,Computing  and,\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx \int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx,"I am trying to compute $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx$$ and $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$$ via the computation of $$\displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz$$ where $C$ is the keyhole contour with branch cut along the positive real axis. The poles of the function are at $\pm i$ and so by the Residue Theorem I found $$\displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz = \frac{\pi^2}{\sqrt{2}}$$ .The integrals on $C_R, C_{\varepsilon}$ vanish as $R\to\infty, \varepsilon\to 0$ . Writing $L_1$ for the line portion above the real axis, I found $$\displaystyle\int_{L_1} \frac{z^{1/2}\log z}{z^2 + 1}dz = \displaystyle\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx$$ and somewhat similarly for $L_2$ , the line portion below the real axis, $$\displaystyle\int_{L_2} \frac{z^{1/2}\log z}{z^2 + 1}dz = -\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx - 2\pi i \int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx$$ . However, combining all information together implies $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx = \frac{\pi^2}{2\sqrt2}$$ and $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx=0$$ . The first of these answers is correct, however I should have found $$\displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx = \frac{\pi}{\sqrt{2}}$$ and am unsure as to where I've gone wrong in my calculations. Any help would be greatly appreciated.","I am trying to compute and via the computation of where is the keyhole contour with branch cut along the positive real axis. The poles of the function are at and so by the Residue Theorem I found .The integrals on vanish as . Writing for the line portion above the real axis, I found and somewhat similarly for , the line portion below the real axis, . However, combining all information together implies and . The first of these answers is correct, however I should have found and am unsure as to where I've gone wrong in my calculations. Any help would be greatly appreciated.","\displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx \displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx \displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz C \pm i \displaystyle\oint_C \frac{z^{1/2}\log z}{z^2 + 1}dz = \frac{\pi^2}{\sqrt{2}} C_R, C_{\varepsilon} R\to\infty, \varepsilon\to 0 L_1 \displaystyle\int_{L_1} \frac{z^{1/2}\log z}{z^2 + 1}dz = \displaystyle\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx L_2 \displaystyle\int_{L_2} \frac{z^{1/2}\log z}{z^2 + 1}dz = -\int_{0}^{\infty}\frac{x^{1/2}\log x}{x^2 + 1}dx - 2\pi i \int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx \displaystyle\int_{0}^{\infty}\frac{x^{1/2} \log x}{x^2 + 1}dx = \frac{\pi^2}{2\sqrt2} \displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx=0 \displaystyle\int_{0}^{\infty}\frac{x^{1/2}}{x^2 + 1}dx = \frac{\pi}{\sqrt{2}}","['integration', 'complex-analysis', 'definite-integrals']"
4,"Integrate : $\int e^{x+e^{x+e^x}}\, dx$",Integrate :,"\int e^{x+e^{x+e^x}}\, dx",Question : $$\int e^{x+e^{x+e^x}} dx$$ source : Integration Techniques and Tricks University of Miami Mathematics Union Trevor Birenbaum My attempt: can be rewritten as $$\int e^x (e^{e^x})^{(e^{e^x})} dx$$ let $z=e^{e^x} \implies dz  = e^{e^x} e^x dx$ so the integral transforms to $$\int \frac{z^z}{z}dz$$ now is it even possible to solve this,Question : source : Integration Techniques and Tricks University of Miami Mathematics Union Trevor Birenbaum My attempt: can be rewritten as let so the integral transforms to now is it even possible to solve this,\int e^{x+e^{x+e^x}} dx \int e^x (e^{e^x})^{(e^{e^x})} dx z=e^{e^x} \implies dz  = e^{e^x} e^x dx \int \frac{z^z}{z}dz,"['integration', 'indefinite-integrals']"
5,Evaluate $\int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx$,Evaluate,\int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx,"I was curious about this integral so I asked about its convergence here: Does $\int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx$ converge? @jizert answered, explaining that it did converge. My follow-up question is what it converges to? I pasted my work from the the other question below: My attempts: $$\frac{dy}{dx}=\sin^{-1}\left(\frac{\sin(x)}{x}\right)$$ $$\sin\left(\frac{dy}{dx}\right)=\frac{\sin(x)}{x}$$ $$x\sin\left(\frac{dy}{dx}\right)=\sin(x)$$ Seems like a dead end. $$\int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx$$ $$2\int_{0}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx$$ $$2\int_{1}^{0}\sin^{-1}(u)\frac{du}{\frac{x\cos(x)-\sin(x)}{x^2}}$$ Also seems like a dead end because I can't figure out how to solve for $x$ in $u=\frac{\sin x}{x}$ . Edit: I also see that $\left|\frac{\sin(x)}{x}\right|\le\left|\sin^{-1}\left(\frac{\sin(x)}{x}\right)\right|\le\left|\frac{\pi}{2}\frac{\sin(x)}{x}\right|$ but I doubt anything can be done with that.","I was curious about this integral so I asked about its convergence here: Does $\int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx$ converge? @jizert answered, explaining that it did converge. My follow-up question is what it converges to? I pasted my work from the the other question below: My attempts: Seems like a dead end. Also seems like a dead end because I can't figure out how to solve for in . Edit: I also see that but I doubt anything can be done with that.",\frac{dy}{dx}=\sin^{-1}\left(\frac{\sin(x)}{x}\right) \sin\left(\frac{dy}{dx}\right)=\frac{\sin(x)}{x} x\sin\left(\frac{dy}{dx}\right)=\sin(x) \int_{-\infty}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx 2\int_{0}^{\infty}\sin^{-1}\left(\frac{\sin(x)}{x}\right)dx 2\int_{1}^{0}\sin^{-1}(u)\frac{du}{\frac{x\cos(x)-\sin(x)}{x^2}} x u=\frac{\sin x}{x} \left|\frac{\sin(x)}{x}\right|\le\left|\sin^{-1}\left(\frac{\sin(x)}{x}\right)\right|\le\left|\frac{\pi}{2}\frac{\sin(x)}{x}\right|,"['integration', 'definite-integrals', 'improper-integrals', 'trigonometric-integrals']"
6,Show that $\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi$,Show that,\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi,"Show that $$\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi$$ My try : We know that $$\sum_{n=1}^{\infty} \binom{2n}{n} (H_{2n} - H_{n}) t^n = - \frac{1}{\sqrt{1 - 4t}} \log\left(\frac{1 + \sqrt{1 - 4t}}{2}\right)$$ In particular we have $$\sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n} t^{2n} = - \frac{1}{\sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right)$$ ....(1) Next, dividing both sides of (1) by $t ^ 2$ and then integrating from 0 to 2, we get $$\sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n (2n - 1)} x^{2n - 1} = - \int_{0}^{x} \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt$$ Making the change of variable $t=sin(\theta)$ in $$J = \int \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt$$ $$J = \int \frac{1}{\sin^2 \theta \cos \theta} \log\left(\frac{1 + \cos \theta}{2}\right) \cos \theta \, d\theta$$","Show that My try : We know that In particular we have ....(1) Next, dividing both sides of (1) by and then integrating from 0 to 2, we get Making the change of variable in","\sum_{n=1}^{\infty} \frac{\binom{2n}{n} (H_{2n} - H_n)}{4^n (2n - 1)^2} = 2 + \frac{3\pi}{2} \log(2) - 2G - \pi \sum_{n=1}^{\infty} \binom{2n}{n} (H_{2n} - H_{n}) t^n = - \frac{1}{\sqrt{1 - 4t}} \log\left(\frac{1 + \sqrt{1 - 4t}}{2}\right) \sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n} t^{2n} = - \frac{1}{\sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) t ^ 2 \sum_{n=1}^{\infty} \binom{2n}{n} \frac{(H_{2n} - H_{n})}{4^n (2n - 1)} x^{2n - 1} = - \int_{0}^{x} \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt t=sin(\theta) J = \int \frac{1}{t^2 \sqrt{1 - t^2}} \log\left(\frac{1 + \sqrt{1 - t^2}}{2}\right) \, dt J = \int \frac{1}{\sin^2 \theta \cos \theta} \log\left(\frac{1 + \cos \theta}{2}\right) \cos \theta \, d\theta","['calculus', 'integration', 'definite-integrals', 'summation', 'harmonic-numbers']"
7,Scalar integrals in higher dimensions,Scalar integrals in higher dimensions,,"The thing I want to do The typical vector calculus course defines: A bunch of integrals of vector fields in $\mathbb R^2$ and $\mathbb R^3$ : line integrals of a vector field along a curve, flux integrals of a vector field across a curve in $\mathbb R^2$ , and flux integrals of a vector field across a surface in $\mathbb R^3$ . A bunch of integrals of scalar functions in $\mathbb R^2$ and $\mathbb R^3$ : here, we can just integrate any scalar function over any curve or surface. I know that the integrals of vector fields can be generalized to integrals of differential forms. For example, the flux of a vector field $\mathbf F = M\,\mathbf i + N\,\mathbf j + P\,\mathbf k$ across a surface is equivalent to integrating $M\,\mathrm dy \wedge \mathrm dz + N\,\mathrm dz \wedge \mathrm dx + P \, \mathrm dx \wedge \mathrm dy$ over the surface. If the surface is given a parameterization, writing $x(u,v)$ , $y(u,v)$ , and $z(u,v)$ as functions of $u$ and $v$ , then we can expand $\mathrm dx$ as $\frac{\partial x}{\partial u}\,\mathrm du + \frac{\partial x}{\partial v}\,\mathrm dv$ , do the same for $\mathrm dy$ and $\mathrm dz$ , and simplify the wedge products to get something we can integrate with respect to $u$ and $v$ . When I try to understand how to generalize scalar integrals, I run into trouble, because then I have to understand what a ""metric tensor"" or ""Riemannian volume form"" is, and I don't really understand those. However, I have come up with an approach that I do understand, and which seems to correctly handle all the special cases I am confident in. The approach I'd like to verify All the cases I understand seem to be based on the Jacobian determinant, which I'll write $\frac{\partial (x_1, x_2, \dots, x_n)}{\partial (u_1, u_2, \dots, u_n)}$ , and it is the determinant of the matrix whose $(i,j)$ entry is $\frac{\partial x_i}{\partial y_j}$ . When integrating by substitution in higher dimensions, we multiply by the absolute value of this integral. When integrating over a surface in $\mathbb R^3$ , we multiply by the norm of a cross product of partial derivatives, but it simplifies to the expression $$\sqrt{\left(\frac{\partial(x,y)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(x,z)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(y,z)}{\partial(u,v)}\right)^2}.$$ When integrating over a curve in $\mathbb R^n$ parameterized by $\mathbb r(t)$ , we multiply by $\left\|\frac{\mathrm d\mathbf r}{\mathrm dt}\right\|$ . But we can think of the components of $\frac{\mathrm d\mathbf r}{\mathrm dt}$ as $1\times 1$ Jacobian determinants of each of $x_1, x_2, \dots, x_n$ individually with respect to $t$ . So what I'd like to do in general, to integrate over a $k$ -dimensional object in $\mathbb R^n$ on which the variables $x_1, x_2, \dots, x_n$ are parameterized in terms of $u_1, u_2, \dots, u_k$ , is: Write out all $\binom nk$ Jacobian determinants $\frac{\partial(x_{i_1}, x_{i_2}, \dots, x_{i_k})}{\partial(u_1, u_2, \dots, u_k)}$ . Compute the norm of this $\binom nk$ -dimensional vector: the square root of the sum of squares of these determinants. Integrate my scalar function, multiplied by this norm, with respect to $u_1, u_2, \dots, u_k$ . If this works, I would be very happy, because I would not need to know anything more than how to parameterize my $k$ -dimensional object, and how to take partial derivatives. I looked up a question about how to integrate over a surface in 4 dimensions , which is a special case of what I want to know. The answer there gives a formula that looks very different, but I checked in Mathematica and the contents of the square root simplify to the same thing. That's reassuring, but it doesn't tell me that my approach will continue working when integrating over a $5$ -dimensional object in $17$ dimensions. My question about this approach Most importantly: does my approach work in general? If it does work: am I overcomplicating things - is there a simpler way to compute the same quantity? If it doesn't work: is there a correct method that's as concrete as my approach?","The thing I want to do The typical vector calculus course defines: A bunch of integrals of vector fields in and : line integrals of a vector field along a curve, flux integrals of a vector field across a curve in , and flux integrals of a vector field across a surface in . A bunch of integrals of scalar functions in and : here, we can just integrate any scalar function over any curve or surface. I know that the integrals of vector fields can be generalized to integrals of differential forms. For example, the flux of a vector field across a surface is equivalent to integrating over the surface. If the surface is given a parameterization, writing , , and as functions of and , then we can expand as , do the same for and , and simplify the wedge products to get something we can integrate with respect to and . When I try to understand how to generalize scalar integrals, I run into trouble, because then I have to understand what a ""metric tensor"" or ""Riemannian volume form"" is, and I don't really understand those. However, I have come up with an approach that I do understand, and which seems to correctly handle all the special cases I am confident in. The approach I'd like to verify All the cases I understand seem to be based on the Jacobian determinant, which I'll write , and it is the determinant of the matrix whose entry is . When integrating by substitution in higher dimensions, we multiply by the absolute value of this integral. When integrating over a surface in , we multiply by the norm of a cross product of partial derivatives, but it simplifies to the expression When integrating over a curve in parameterized by , we multiply by . But we can think of the components of as Jacobian determinants of each of individually with respect to . So what I'd like to do in general, to integrate over a -dimensional object in on which the variables are parameterized in terms of , is: Write out all Jacobian determinants . Compute the norm of this -dimensional vector: the square root of the sum of squares of these determinants. Integrate my scalar function, multiplied by this norm, with respect to . If this works, I would be very happy, because I would not need to know anything more than how to parameterize my -dimensional object, and how to take partial derivatives. I looked up a question about how to integrate over a surface in 4 dimensions , which is a special case of what I want to know. The answer there gives a formula that looks very different, but I checked in Mathematica and the contents of the square root simplify to the same thing. That's reassuring, but it doesn't tell me that my approach will continue working when integrating over a -dimensional object in dimensions. My question about this approach Most importantly: does my approach work in general? If it does work: am I overcomplicating things - is there a simpler way to compute the same quantity? If it doesn't work: is there a correct method that's as concrete as my approach?","\mathbb R^2 \mathbb R^3 \mathbb R^2 \mathbb R^3 \mathbb R^2 \mathbb R^3 \mathbf F = M\,\mathbf i + N\,\mathbf j + P\,\mathbf k M\,\mathrm dy \wedge \mathrm dz + N\,\mathrm dz \wedge \mathrm dx + P \, \mathrm dx \wedge \mathrm dy x(u,v) y(u,v) z(u,v) u v \mathrm dx \frac{\partial x}{\partial u}\,\mathrm du + \frac{\partial x}{\partial v}\,\mathrm dv \mathrm dy \mathrm dz u v \frac{\partial (x_1, x_2, \dots, x_n)}{\partial (u_1, u_2, \dots, u_n)} (i,j) \frac{\partial x_i}{\partial y_j} \mathbb R^3 \sqrt{\left(\frac{\partial(x,y)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(x,z)}{\partial(u,v)}\right)^2 + \left(\frac{\partial(y,z)}{\partial(u,v)}\right)^2}. \mathbb R^n \mathbb r(t) \left\|\frac{\mathrm d\mathbf r}{\mathrm dt}\right\| \frac{\mathrm d\mathbf r}{\mathrm dt} 1\times 1 x_1, x_2, \dots, x_n t k \mathbb R^n x_1, x_2, \dots, x_n u_1, u_2, \dots, u_k \binom nk \frac{\partial(x_{i_1}, x_{i_2}, \dots, x_{i_k})}{\partial(u_1, u_2, \dots, u_k)} \binom nk u_1, u_2, \dots, u_k k 5 17","['integration', 'multivariable-calculus', 'differential-forms', 'surface-integrals', 'scalar-fields']"
8,Can an exact vector field have loops as solutions?,Can an exact vector field have loops as solutions?,,"I was asking myself this question. We define for an exact vector field $\mathbf{v}(\mathbf x) \in \mathbb{R}^n$ , s.t. $\mathbf{v}(\mathbf x)=\nabla u(\mathbf x)$ for a potential $u$ , a solution as a function $f(t):[0,T] \rightarrow \mathbb{R}^n$ such that $f'(t)=\mathbf{v}(f(t))$ for every $t$ . The question is: if the vector field is exact, can there be solutions with $f(0)=f(T)$ , i.e. closed loops ? Observation: If we have the hypothesis that the vector field is always non zero maybe the situation is easier. In fact if such a solution would exist and we call $\gamma$ the associated loop, $\int_{\gamma} \mathbf{v}=\int_{\gamma} \nabla{u}=\mathbf{0}$ , so the integral of the vector field along the loop should be zero. This means that the component tangent to the loop should change sign, in particular there must be a point $p$ along the loop where the component of the vector field tangent to the loop vanishes. This should be true for every loop. For our loop the vector field is also always tangent to $\gamma$ , so this means that $\mathbf{v}(p)=\mathbf{0}$ . But this cannot be if the vector field is always non zero. Is the observation correct or formalizable with enough regularity conditions? And what about exact vector fields that can vanish? Are there general results on the loops that an exact vector field can have?","I was asking myself this question. We define for an exact vector field , s.t. for a potential , a solution as a function such that for every . The question is: if the vector field is exact, can there be solutions with , i.e. closed loops ? Observation: If we have the hypothesis that the vector field is always non zero maybe the situation is easier. In fact if such a solution would exist and we call the associated loop, , so the integral of the vector field along the loop should be zero. This means that the component tangent to the loop should change sign, in particular there must be a point along the loop where the component of the vector field tangent to the loop vanishes. This should be true for every loop. For our loop the vector field is also always tangent to , so this means that . But this cannot be if the vector field is always non zero. Is the observation correct or formalizable with enough regularity conditions? And what about exact vector fields that can vanish? Are there general results on the loops that an exact vector field can have?","\mathbf{v}(\mathbf x) \in \mathbb{R}^n \mathbf{v}(\mathbf x)=\nabla u(\mathbf x) u f(t):[0,T] \rightarrow \mathbb{R}^n f'(t)=\mathbf{v}(f(t)) t f(0)=f(T) \gamma \int_{\gamma} \mathbf{v}=\int_{\gamma} \nabla{u}=\mathbf{0} p \gamma \mathbf{v}(p)=\mathbf{0}","['integration', 'analysis', 'self-learning', 'closed-form', 'vector-fields']"
9,Series expansion of the integral $\int_{-\infty}^{\infty} \frac{\tanh{(x^2-a^2)}}{x^2-a^2} dx$ in the limit of small $a$,Series expansion of the integral  in the limit of small,\int_{-\infty}^{\infty} \frac{\tanh{(x^2-a^2)}}{x^2-a^2} dx a,"I found that when $a \gg 1$ , the function $I(a) = \int_{-\infty}^{\infty} \frac{\tanh{(x^2-a^2)}}{x^2-a^2} dx \approx \frac{2\gamma + 2 \log(16/\pi) + 4 \log a}{a}$ . How does the integral behave when $a < 1$ ? In this limit, we can write $I(a) = I(0) + d_1 a^2 + d_2 a^4 + \cdots$ I found that $I(0) = 4\sqrt{2 \pi} \eta\left(-\frac{1}{2}\right) \approx 1.90556$ , where $\eta(z)$ is the Dirichlet eta function . Using numerical integration, I found that $d_1 \approx 0.79$ and $d_2\approx-0.37$ . What are the analytical forms of $d_1, d_2$ and so on?","I found that when , the function . How does the integral behave when ? In this limit, we can write I found that , where is the Dirichlet eta function . Using numerical integration, I found that and . What are the analytical forms of and so on?","a \gg 1 I(a) = \int_{-\infty}^{\infty} \frac{\tanh{(x^2-a^2)}}{x^2-a^2} dx \approx \frac{2\gamma + 2 \log(16/\pi) + 4 \log a}{a} a < 1 I(a) = I(0) + d_1 a^2 + d_2 a^4 + \cdots I(0) = 4\sqrt{2 \pi} \eta\left(-\frac{1}{2}\right) \approx 1.90556 \eta(z) d_1 \approx 0.79 d_2\approx-0.37 d_1, d_2","['integration', 'power-series']"
10,"On $\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx$",On,"\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx","It seems that $$\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}.$$ Below I present a proof, but there is one problem with that ""proof"". First we use the pentagonal number theorem: $$I=\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\int_0^1 \sum_{k=-\infty}^\infty (-1)^k x^{\frac{k(3k-1)}{2}}\, dx,$$ then interchange integral and series to obtain $$I=2\sum_{k=-\infty}^\infty \frac{(-1)^k}{3k^2-k+2}.$$ Then $$I=-1+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2-k+2}+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2+k+2}$$ and we use partial fraction decomposition: $$I=-1+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k-\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k-\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right)+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k+\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k+\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right).$$ Now $$\Phi (z,s,a)=\sum_{k=0}^\infty \frac{z^k}{(a+k)^s},\quad \Re s\gt 1, |z|\ge 1.$$ If we suppose that $s$ is a positive integer then $a\notin -\mathbb{N}$ . If we suppose that $z\notin [1,\infty)$ , then the Lerch transcendent has the following integral representation: $$\Phi (z,s,a)=\frac{1}{\Gamma (s)}\int_0^\infty \frac{x^{s-1}e^{-ax}}{1-ze^{-x}}\, dx,\quad \Re s\gt 0,\Re a\gt 0.$$ Here comes the important part: If we ignore that $\Re s\gt 1$ in the series definition of $\Phi$ and if we ignore that $\Re a\gt 0$ in the integral representation, then we can write $I$ as $$\begin{align}I&=-1+\frac{2i}{\sqrt{23}}\left(\Phi \left(-1,1,-\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,-\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)+\Phi\left(-1,1,\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)\right)\\&=-1+\frac{2i}{\sqrt{23}}\int_0^\infty \frac{1}{1+e^{-x}}\left(e^{\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}+e^{-\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{-\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}\right)\, dx\\&=-1+\frac{8}{\sqrt{23}}\int_0^\infty \frac{\sin\frac{\sqrt{23}x}{6}\cosh\frac{x}{6}}{1+e^{-x}}\, dx.\end{align}$$ By utilizing Laplace transforms, this can be rewritten to $$I=\frac{4}{\sqrt{23}}\int_{-\infty}^\infty \frac{\sin\frac{\sqrt{23}x}{6}}{1+e^{-x}}e^{\frac{x}{6}}\, dx.$$ Then a routine application of the residue theorem yields $$I=\frac{4\pi}{\sqrt{23}\sinh \sqrt{23}\pi}\sum_{k=0}^5 \cosh\left(\frac{\sqrt{23}}{6}(2k+1)\pi-\sqrt{23}\pi\right)e^{(2k+1)\frac{i\pi}{6}},$$ which, after using Euler's formula, gives $$I=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}.$$ Question How to make the proof rigorous? In other words: notice how we ignored two conditions when using the Lerch transcendent. How can this be patched? As a reference for the Lerch transcendent, I used https://dlmf.nist.gov/25.14","It seems that Below I present a proof, but there is one problem with that ""proof"". First we use the pentagonal number theorem: then interchange integral and series to obtain Then and we use partial fraction decomposition: Now If we suppose that is a positive integer then . If we suppose that , then the Lerch transcendent has the following integral representation: Here comes the important part: If we ignore that in the series definition of and if we ignore that in the integral representation, then we can write as By utilizing Laplace transforms, this can be rewritten to Then a routine application of the residue theorem yields which, after using Euler's formula, gives Question How to make the proof rigorous? In other words: notice how we ignored two conditions when using the Lerch transcendent. How can this be patched? As a reference for the Lerch transcendent, I used https://dlmf.nist.gov/25.14","\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}. I=\int_0^1\prod_{k=1}^\infty (1-x^k)\, dx=\int_0^1 \sum_{k=-\infty}^\infty (-1)^k x^{\frac{k(3k-1)}{2}}\, dx, I=2\sum_{k=-\infty}^\infty \frac{(-1)^k}{3k^2-k+2}. I=-1+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2-k+2}+2\sum_{k=0}^\infty \frac{(-1)^k}{3k^2+k+2} I=-1+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k-\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k-\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right)+\frac{2i}{\sqrt{23}}\sum_{k=0}^\infty \left(\frac{(-1)^k}{k+\frac{1}{6}+i\frac{\sqrt{23}}{6}}-\frac{(-1)^k}{k+\frac{1}{6}-i\frac{\sqrt{23}}{6}}\right). \Phi (z,s,a)=\sum_{k=0}^\infty \frac{z^k}{(a+k)^s},\quad \Re s\gt 1, |z|\ge 1. s a\notin -\mathbb{N} z\notin [1,\infty) \Phi (z,s,a)=\frac{1}{\Gamma (s)}\int_0^\infty \frac{x^{s-1}e^{-ax}}{1-ze^{-x}}\, dx,\quad \Re s\gt 0,\Re a\gt 0. \Re s\gt 1 \Phi \Re a\gt 0 I \begin{align}I&=-1+\frac{2i}{\sqrt{23}}\left(\Phi \left(-1,1,-\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,-\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)+\Phi\left(-1,1,\frac{1}{6}+i\frac{\sqrt{23}}{6}\right)-\Phi\left(-1,1,\frac{1}{6}-i\frac{\sqrt{23}}{6}\right)\right)\\&=-1+\frac{2i}{\sqrt{23}}\int_0^\infty \frac{1}{1+e^{-x}}\left(e^{\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}+e^{-\frac{1}{6}x-i\frac{\sqrt{23}}{6}x}-e^{-\frac{1}{6}x+i\frac{\sqrt{23}}{6}x}\right)\, dx\\&=-1+\frac{8}{\sqrt{23}}\int_0^\infty \frac{\sin\frac{\sqrt{23}x}{6}\cosh\frac{x}{6}}{1+e^{-x}}\, dx.\end{align} I=\frac{4}{\sqrt{23}}\int_{-\infty}^\infty \frac{\sin\frac{\sqrt{23}x}{6}}{1+e^{-x}}e^{\frac{x}{6}}\, dx. I=\frac{4\pi}{\sqrt{23}\sinh \sqrt{23}\pi}\sum_{k=0}^5 \cosh\left(\frac{\sqrt{23}}{6}(2k+1)\pi-\sqrt{23}\pi\right)e^{(2k+1)\frac{i\pi}{6}}, I=\frac{8\sqrt{69}\pi \sinh (\sqrt{23}\pi/6)}{46\cosh (\sqrt{23}\pi /3)-23}.","['integration', 'sequences-and-series', 'complex-analysis', 'definite-integrals', 'closed-form']"
11,Complicated change of variables formula with CDF of normal distribution instead of diffeomorphism,Complicated change of variables formula with CDF of normal distribution instead of diffeomorphism,,"I have a parameter $\theta\in[0, 10]^4$ and a variable $z\in\mathbb{R}^m$ . Consider the following integral $$ I(\theta, z) := \int F(\theta, z) \mathbb{I}(\|f(\theta, z)\| \leq 1) p(\theta, z) dz d\theta $$ Is it possible to rewrite it using the variables $(\vartheta, z)$ where $\theta = G(\vartheta)$ and $$ G(\vartheta) = 10\cdot (\Phi(\vartheta_1), \Phi(\vartheta_2), \Phi(\vartheta_3), \Phi(\vartheta_4)) $$ with $\Phi$ being the CDF of a standard normal distribution. Attempt Informally, one could go ahead and simply replace $\theta$ with $G(\vartheta)$ and multiply the integrand by $|J_G(\vartheta)|$ , the absolute determinant of the Jacobian of $G$ $$ I(\theta, z) = \int F(G(\vartheta), z) \mathbb{I}(\|f(G(\vartheta), z)\| \leq 1) p(G(\vartheta), z) |J_G(\vartheta)| dz d\vartheta =: I(\vartheta, z) $$ However, this only works informally. The substitution I have just done is not correct according to the Change of Variables formula (e.g. see Billingsley). This is because the CDF of a normal distribution is not a diffeomorphism: it has no closed-form inverse.","I have a parameter and a variable . Consider the following integral Is it possible to rewrite it using the variables where and with being the CDF of a standard normal distribution. Attempt Informally, one could go ahead and simply replace with and multiply the integrand by , the absolute determinant of the Jacobian of However, this only works informally. The substitution I have just done is not correct according to the Change of Variables formula (e.g. see Billingsley). This is because the CDF of a normal distribution is not a diffeomorphism: it has no closed-form inverse.","\theta\in[0, 10]^4 z\in\mathbb{R}^m 
I(\theta, z) := \int F(\theta, z) \mathbb{I}(\|f(\theta, z)\| \leq 1) p(\theta, z) dz d\theta
 (\vartheta, z) \theta = G(\vartheta) 
G(\vartheta) = 10\cdot (\Phi(\vartheta_1), \Phi(\vartheta_2), \Phi(\vartheta_3), \Phi(\vartheta_4))
 \Phi \theta G(\vartheta) |J_G(\vartheta)| G 
I(\theta, z) = \int F(G(\vartheta), z) \mathbb{I}(\|f(G(\vartheta), z)\| \leq 1) p(G(\vartheta), z) |J_G(\vartheta)| dz d\vartheta =: I(\vartheta, z)
","['calculus', 'integration', 'measure-theory', 'change-of-variable']"
12,How to find the speed to maximise the distance travelled,How to find the speed to maximise the distance travelled,,"I'm given that a car travels at speed along its track it uses fuel at a rate of $R(v)=17‌v^2 –7v+7$ , in litres per sec. Where $v$ is the speed in metres per second. The fuel tank holds $Y$ litres of fuel: $$Y = \int_{0}^{T}  R(v(t))\,dt$$ $v(t)$ is the speed as a function of the time. I found the speed to maximise the time the car travels under power which was $v = 7/34$ . How should I find the what constant speed the car must run to maximize the distance $vT$ that it runs under power? Any help would be great, thank you. Edit: I integrated $Y = \frac{17T^3}{3} - \frac{7T}{2} +7T$ , I'm not sure what else to do, after following Nurator's suggestion. Calculus is my weakest topic.","I'm given that a car travels at speed along its track it uses fuel at a rate of , in litres per sec. Where is the speed in metres per second. The fuel tank holds litres of fuel: is the speed as a function of the time. I found the speed to maximise the time the car travels under power which was . How should I find the what constant speed the car must run to maximize the distance that it runs under power? Any help would be great, thank you. Edit: I integrated , I'm not sure what else to do, after following Nurator's suggestion. Calculus is my weakest topic.","R(v)=17‌v^2 –7v+7 v Y Y = \int_{0}^{T}  R(v(t))\,dt v(t) v = 7/34 vT Y = \frac{17T^3}{3} - \frac{7T}{2} +7T","['calculus', 'integration', 'multivariable-calculus', 'derivatives', 'partial-differential-equations']"
13,Stuck on (simple looking!) discontinuous double integral with gaussian weight and double pole,Stuck on (simple looking!) discontinuous double integral with gaussian weight and double pole,,"The question is what is the analytic answer to the limit of the difference of integrals $(I_{+\epsilon}-I_{-\epsilon})|_{\epsilon \rightarrow 0} =\int_{0}^{\infty}dx \int_{0}^{\infty}dy\left(\frac{e^{-\frac12 x^2-\frac12(y+i \epsilon)^2}}{(x-i (y+i \epsilon))^2}-\frac{e^{-\frac12 x^2-\frac12(y-i \epsilon)^2}}{(x-i (y-i \epsilon))^2}\right)|_{\epsilon \rightarrow 0}$ . This comes from considering the integral $I_0= \int_{0}^{\infty}dx \int_{0}^{\infty}dy\frac{e^{-\frac12 x^2-\frac12y^2}}{(x-i y)^2}$ . By itself this is not a convergent integral, and playing around with it, it seemed to me that the divergence is logarithmic. However, by shifting the y variable above and below the real axis there, one can find a slight alteration which should be convergent (I believe), given by the expression at the top of this question. I'm struggling to get a clean working calculation for this, but the most reasonable ones seemed to give essentially $\pi$ . Any idea if this is indeed the correct result and how to obtain it cleanly?","The question is what is the analytic answer to the limit of the difference of integrals . This comes from considering the integral . By itself this is not a convergent integral, and playing around with it, it seemed to me that the divergence is logarithmic. However, by shifting the y variable above and below the real axis there, one can find a slight alteration which should be convergent (I believe), given by the expression at the top of this question. I'm struggling to get a clean working calculation for this, but the most reasonable ones seemed to give essentially . Any idea if this is indeed the correct result and how to obtain it cleanly?",(I_{+\epsilon}-I_{-\epsilon})|_{\epsilon \rightarrow 0} =\int_{0}^{\infty}dx \int_{0}^{\infty}dy\left(\frac{e^{-\frac12 x^2-\frac12(y+i \epsilon)^2}}{(x-i (y+i \epsilon))^2}-\frac{e^{-\frac12 x^2-\frac12(y-i \epsilon)^2}}{(x-i (y-i \epsilon))^2}\right)|_{\epsilon \rightarrow 0} I_0= \int_{0}^{\infty}dx \int_{0}^{\infty}dy\frac{e^{-\frac12 x^2-\frac12y^2}}{(x-i y)^2} \pi,"['integration', 'limits', 'definite-integrals', 'contour-integration', 'branch-cuts']"
14,Finding $ \lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)$,Finding, \lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right),"By numerical estimation, I get $$ \lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)=\frac{6 \gamma+4 \ln 2+5}{24} . $$ But I can't prove it yet. I made some attempts： (1)  We can get $$ \lim _{y \rightarrow+\infty}\left(\int_0^y \frac{\sin ^4 x}{x} d x-\frac{3}{8} \ln y\right)=\frac{3 \gamma+2 \ln 2}{8} . $$ And for sufficiently small $c>0$ ,there is $$ \frac{x}{\sin ^4 x}-\frac{1}{x^3}-\frac{2}{3 x} \leqslant 1, x \in[0, c] . $$ (2)According to the Riemann Lemma, we have $$ \lim _{n \rightarrow \infty} \int_c^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x=\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x. $$ (3) By (1), we have \begin{aligned} \int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x & \leqslant \int_0^c \frac{\sin ^4(n x)}{x^3} d x+\frac{2}{3} \int_0^c \frac{\sin ^4(n x)}{x} d x+\int_0^c \sin ^4(n x) d x \\ & \leqslant n^2 \int_0^{n c} \frac{\sin ^4(x)}{x^3} d x+\frac{2}{3} \int_0^{n c} \frac{\sin ^4(x)}{x} d x+c \\ & =n^2\left(\ln 2-\int_{n c}^{\infty} \frac{\sin ^4(x)}{x^3} d x\right)+\frac{2}{3}\left(\frac{3}{8} \ln (n c)+\frac{3 \gamma+2 \ln 2}{8}\right)+c+o(1) \\ & =\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left(\frac{1}{4} \ln (n c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right)+c+o(1)  \end{aligned} i.e. $$ \int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x  \leqslant\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left[\frac{1}{4} (\ln n +\ln c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right]+c+o(1) $$ (4) By (2)(3), we have $$ \varlimsup_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)\leqslant \varlimsup_{n \rightarrow \infty}  (\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4}+\frac{\gamma+\frac{2 \ln 2}{3}}{4}+c+o(1)). $$ I want to prove that $$ \varlimsup_{n \rightarrow \infty}(\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4})= \frac{5}{24} ,\ c\to 0,$$ and $$ \varliminf_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right) \geqslant \frac{\gamma+\frac{2 \ln 2}{3}}{4}+\frac{5}{24}. $$ But I haven't found a way to prove it yet. Can someone give some hints? thanks","By numerical estimation, I get But I can't prove it yet. I made some attempts： (1)  We can get And for sufficiently small ,there is (2)According to the Riemann Lemma, we have (3) By (1), we have i.e. (4) By (2)(3), we have I want to prove that and But I haven't found a way to prove it yet. Can someone give some hints? thanks","
\lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)=\frac{6 \gamma+4 \ln 2+5}{24} .
 
\lim _{y \rightarrow+\infty}\left(\int_0^y \frac{\sin ^4 x}{x} d x-\frac{3}{8} \ln y\right)=\frac{3 \gamma+2 \ln 2}{8} .
 c>0 
\frac{x}{\sin ^4 x}-\frac{1}{x^3}-\frac{2}{3 x} \leqslant 1, x \in[0, c] .
 
\lim _{n \rightarrow \infty} \int_c^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x=\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x.
 \begin{aligned}
\int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x & \leqslant \int_0^c \frac{\sin ^4(n x)}{x^3} d x+\frac{2}{3} \int_0^c \frac{\sin ^4(n x)}{x} d x+\int_0^c \sin ^4(n x) d x \\
& \leqslant n^2 \int_0^{n c} \frac{\sin ^4(x)}{x^3} d x+\frac{2}{3} \int_0^{n c} \frac{\sin ^4(x)}{x} d x+c \\
& =n^2\left(\ln 2-\int_{n c}^{\infty} \frac{\sin ^4(x)}{x^3} d x\right)+\frac{2}{3}\left(\frac{3}{8} \ln (n c)+\frac{3 \gamma+2 \ln 2}{8}\right)+c+o(1) \\
& =\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left(\frac{1}{4} \ln (n c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right)+c+o(1) 
\end{aligned} 
\int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x  \leqslant\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left[\frac{1}{4} (\ln n +\ln c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right]+c+o(1)
 
\varlimsup_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)\leqslant \varlimsup_{n \rightarrow \infty}  (\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4}+\frac{\gamma+\frac{2 \ln 2}{3}}{4}+c+o(1)).
 
\varlimsup_{n \rightarrow \infty}(\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4})= \frac{5}{24}
,\ c\to 0, 
\varliminf_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right) \geqslant \frac{\gamma+\frac{2 \ln 2}{3}}{4}+\frac{5}{24}.
","['integration', 'limits']"
15,How to integrate $\int \frac{1}{x^6-1}dx$,How to integrate,\int \frac{1}{x^6-1}dx,How to integrate $$\int \frac{1}{x^6-1}dx$$ I have done it by using $$x^6-1=(x-1)(x+1)(x^2+x+1)(x^2-x+1)$$ and then applying partial fraction decomposition but this makes for a very long process. Is there a better way to integrate this function?,How to integrate I have done it by using and then applying partial fraction decomposition but this makes for a very long process. Is there a better way to integrate this function?,\int \frac{1}{x^6-1}dx x^6-1=(x-1)(x+1)(x^2+x+1)(x^2-x+1),"['integration', 'indefinite-integrals']"
16,Show $\lim_{n \to \infty}|\cos nx|^{1/n}=1$ almost everywhere,Show  almost everywhere,\lim_{n \to \infty}|\cos nx|^{1/n}=1,"This is an old problem in a set of qualifier exams in analysis  which I have not been able to solve completely; hence my decision to post it here. The problem  have us prove that if $f$ is a measurable complex function on $\mathbb{R}$ of period $T>0$ such that $\int^T_0|f(x)|\,dx <\infty$ , then (1) $\lim_{n\rightarrow\infty}\frac{f(nx)}{n^2}=0$ almost everywhere. (2) As an application of (1), show that $\lim_{n\rightarrow\infty}|\cos nx|^{1/n}=1$ . Part (1) is not very complicated. I consider the series $s(x)=\sum^\infty_{n=1}\frac{|f(nx)|}{n^2}$ . Then $$\frac{1}{n^2}\int^T_0|f(nx)|\,dx=\frac{1}{n^3}\int^{nT}_0|f(x)|\,dx=\frac{1}{n^2}\int^T_0|f(x)|\,dx$$ As a consequence, $\int^T_0 s(x)\,dx<\infty$ . From this it follows that the series $s(x)$ converges almost everywhere and (1) follows. Part (2) is where I am struggling, for $f_n(x)=|\cos nx|^{1/n}$ is not quite of the form $n^{-2} g(nx)$ for some periodic function. I would appreciate any hint to finish part (2) off.","This is an old problem in a set of qualifier exams in analysis  which I have not been able to solve completely; hence my decision to post it here. The problem  have us prove that if is a measurable complex function on of period such that , then (1) almost everywhere. (2) As an application of (1), show that . Part (1) is not very complicated. I consider the series . Then As a consequence, . From this it follows that the series converges almost everywhere and (1) follows. Part (2) is where I am struggling, for is not quite of the form for some periodic function. I would appreciate any hint to finish part (2) off.","f \mathbb{R} T>0 \int^T_0|f(x)|\,dx <\infty \lim_{n\rightarrow\infty}\frac{f(nx)}{n^2}=0 \lim_{n\rightarrow\infty}|\cos nx|^{1/n}=1 s(x)=\sum^\infty_{n=1}\frac{|f(nx)|}{n^2} \frac{1}{n^2}\int^T_0|f(nx)|\,dx=\frac{1}{n^3}\int^{nT}_0|f(x)|\,dx=\frac{1}{n^2}\int^T_0|f(x)|\,dx \int^T_0 s(x)\,dx<\infty s(x) f_n(x)=|\cos nx|^{1/n} n^{-2} g(nx)","['real-analysis', 'integration', 'lebesgue-integral']"
17,Can't find my error in integrating $\sin(1-2x)\cos(1+2x)$,Can't find my error in integrating,\sin(1-2x)\cos(1+2x),"My goal is to integrate $$I = \int \sin(1-2x)\cos(1+2x)\mathrm dx.$$ First, I let $u = -x$ and $\mathrm du=-\mathrm dx,$ and note that $$I = -\int \sin(1+2u) \cos(1-2u)\mathrm du.$$ Next, I try to integrate $I = \int \sin(1-2x)\cos(1+2x)\mathrm dx$ by parts using $u = \sin(1-2x)$ and $\mathrm dv=\cos(1+2x)\mathrm dx,$ which gives $\mathrm du = -2\cos(1-2x)$ and $v=\frac{\sin(1+2x)}{2}$ . So, $$I = \frac{\sin(1-2x)\sin(1+2x)}{2} - \int\frac{\sin(1+2x)}{2} (-2\cos(1-2x)) \mathrm dx\\= \frac{\sin(1-2x)\sin(1+2x)}{2} + \int\sin(1+2x)\cos(1-2x)\mathrm dx.$$ But $\int\sin(1+2x)\cos(1-2x)\mathrm dx = -I,$ so $$I= \frac{\sin(1-2x)\sin(1+2x)}{2} - I,$$ and so $$I = \frac{\sin(1-2x)\sin(1+2x)}{4} + C.$$ However, symbolab and and wolframalpha give different answers that don't seem to just differ by a constant. I'm having a hard time finding my mistake.","My goal is to integrate First, I let and and note that Next, I try to integrate by parts using and which gives and . So, But so and so However, symbolab and and wolframalpha give different answers that don't seem to just differ by a constant. I'm having a hard time finding my mistake.","I = \int \sin(1-2x)\cos(1+2x)\mathrm dx. u = -x \mathrm du=-\mathrm dx, I = -\int \sin(1+2u) \cos(1-2u)\mathrm du. I = \int \sin(1-2x)\cos(1+2x)\mathrm dx u = \sin(1-2x) \mathrm dv=\cos(1+2x)\mathrm dx, \mathrm du = -2\cos(1-2x) v=\frac{\sin(1+2x)}{2} I = \frac{\sin(1-2x)\sin(1+2x)}{2} - \int\frac{\sin(1+2x)}{2} (-2\cos(1-2x)) \mathrm dx\\= \frac{\sin(1-2x)\sin(1+2x)}{2} + \int\sin(1+2x)\cos(1-2x)\mathrm dx. \int\sin(1+2x)\cos(1-2x)\mathrm dx = -I, I= \frac{\sin(1-2x)\sin(1+2x)}{2} - I, I = \frac{\sin(1-2x)\sin(1+2x)}{4} + C.","['calculus', 'integration', 'indefinite-integrals', 'reduction-formula']"
18,Why can't the antiderivative of 1/x be found using the limit of the power rule? [duplicate],Why can't the antiderivative of 1/x be found using the limit of the power rule? [duplicate],,"This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 1 year ago . This post was edited and submitted for review 1 year ago and failed to reopen the post: Original close reason(s) were not resolved Which step does this go wrong? $$\frac{d}{dx} x^n = nx^{n-1}$$ $$∫ \frac{d}{dx} \left(x^{n} \right) dx = ∫ n x^{n-1} dx$$ $$x^n + c = n ∫ x^{n-1} dx$$ $$\frac{x^n}{n} + c = ∫ x^{n-1} dx, n≠0$$ $$\lim_{n→0} \left[ \frac{x^{n}}{n} + c = ∫ x^{n-1} dx \right]$$ However $$\lim_{n→0} \left[ \frac{x^{n}}{n}+c \right] DNE$$ Therefore $$ \lim_{n→0} \left[ ∫ x^{ n-1} dx \right] DNE$$ I understand that $∫ x^{-1} dx = \ln(x)$ , but why doesn't the limit work?","This question already has answers here : Demystify integration of $\int \frac{1}{x} \mathrm dx$ (11 answers) Closed 1 year ago . This post was edited and submitted for review 1 year ago and failed to reopen the post: Original close reason(s) were not resolved Which step does this go wrong? However Therefore I understand that , but why doesn't the limit work?","\frac{d}{dx} x^n = nx^{n-1} ∫ \frac{d}{dx} \left(x^{n} \right) dx = ∫ n x^{n-1} dx x^n + c = n ∫ x^{n-1} dx \frac{x^n}{n} + c = ∫ x^{n-1} dx, n≠0 \lim_{n→0} \left[ \frac{x^{n}}{n} + c = ∫ x^{n-1} dx \right] \lim_{n→0} \left[ \frac{x^{n}}{n}+c \right] DNE  \lim_{n→0} \left[ ∫ x^{ n-1} dx \right] DNE ∫ x^{-1} dx = \ln(x)","['calculus', 'integration', 'limits', 'improper-integrals']"
19,Evaluate $\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+\frac{1}{2}(1-i)}\right)^ndx$ for natural $n$,Evaluate  for natural,\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+\frac{1}{2}(1-i)}\right)^ndx n,"I was trying to solve this integral the another integral came up: $$I_n=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^ndx$$ There seems to be a pattern in the results $$I_0=1$$ $$I_2=-2+\frac{\pi}{\sqrt2}$$ $$I_4=\frac{4}{3}-\frac{3\pi}{2\sqrt2}$$ $$I_6=\frac{8}{5}-\frac{9\pi}{8\sqrt2}$$ $$I_8=\frac{-16}{7}+\frac{61\pi}{16\sqrt3}$$ The odd inputs seem to be rational multiples of $\frac{\pi i}{\sqrt2}$ . The difference in the odd and even inputs makes me think they should be evaluated separately. Differentiating with respect to n seemed to make it worse. $$I'(n)=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^n\ln\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)dx$$ I tried integration by parts with $u=(\frac{2x-1}{x^2-(1-i)x+(1-i)/2})^{n-1}$ and $dv=\frac{2x-1}{x^2-(1-i)x+(1-i)/2}$ but it seemed to make the integrand so much messier. Edit: We can substitute $u=\frac{2x-1}{x^2-(1-i)x+(1-i)/2}$ , and the problem reduces to finding $$J_n=\int_{-1-i}^{-1+i}\frac{x^n}{\sqrt{-2x^2-4ix+4}}dx$$ For which I asked another question for","I was trying to solve this integral the another integral came up: There seems to be a pattern in the results The odd inputs seem to be rational multiples of . The difference in the odd and even inputs makes me think they should be evaluated separately. Differentiating with respect to n seemed to make it worse. I tried integration by parts with and but it seemed to make the integrand so much messier. Edit: We can substitute , and the problem reduces to finding For which I asked another question for",I_n=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^ndx I_0=1 I_2=-2+\frac{\pi}{\sqrt2} I_4=\frac{4}{3}-\frac{3\pi}{2\sqrt2} I_6=\frac{8}{5}-\frac{9\pi}{8\sqrt2} I_8=\frac{-16}{7}+\frac{61\pi}{16\sqrt3} \frac{\pi i}{\sqrt2} I'(n)=\int_0^1\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)^n\ln\left(\frac{2x-1}{x^2-(1-i)x+(1-i)/2}\right)dx u=(\frac{2x-1}{x^2-(1-i)x+(1-i)/2})^{n-1} dv=\frac{2x-1}{x^2-(1-i)x+(1-i)/2} u=\frac{2x-1}{x^2-(1-i)x+(1-i)/2} J_n=\int_{-1-i}^{-1+i}\frac{x^n}{\sqrt{-2x^2-4ix+4}}dx,"['integration', 'definite-integrals']"
20,$\int_{1}^{\infty}\frac{[3x]}{[x]!}dx$,,\int_{1}^{\infty}\frac{[3x]}{[x]!}dx,I saw an interesting definite integral at somewhere and would like to share. $$\int_{1}^{\infty}\frac{[3x]}{[x]!}dx$$ The solution given is $$\int_{1}^{\infty}\frac{[3x]}{[x]!}dx=\lim_{n \to \infty}\sum_{k=1}^{n-1}\int_{k}^{k+1}\frac{[3x]}{[x]!}dx$$ $$=\lim_{n\to \infty}\sum_{k=1}^{n-1}\left ( \int_{k}^{k+\frac{1}{3}}\frac{3k}{k!}dx+\int_{k+\frac{1}{3}}^{k+\frac{2}{3}}\frac{3k+1}{k!}dx+\int_{k+\frac{2}{3}}^{k+1}\frac{3k+2}{k!}dx \right )$$ $$=\lim_{n \to \infty}\sum_{k=1}^{n-1}\left ( \frac{1}{3}\left ( \frac{3k}{k!} \right )+\frac{1}{3}\left ( \frac{3k+1}{k!} \right ) +\frac{1}{3}\left ( \frac{3k+2}{k!} \right )\right )=\lim_{n \to \infty}\sum_{k=1}^{n-1}\frac{3k+1}{k!}=3\sum_{k=1}^{\infty}\frac{1}{(k-1)!}+\sum_{k=1}^{\infty}\frac{1}{k!}$$ $$=3e+e-1=4e-1$$ I wonder is there another way to solve this definite integral?,I saw an interesting definite integral at somewhere and would like to share. The solution given is I wonder is there another way to solve this definite integral?,\int_{1}^{\infty}\frac{[3x]}{[x]!}dx \int_{1}^{\infty}\frac{[3x]}{[x]!}dx=\lim_{n \to \infty}\sum_{k=1}^{n-1}\int_{k}^{k+1}\frac{[3x]}{[x]!}dx =\lim_{n\to \infty}\sum_{k=1}^{n-1}\left ( \int_{k}^{k+\frac{1}{3}}\frac{3k}{k!}dx+\int_{k+\frac{1}{3}}^{k+\frac{2}{3}}\frac{3k+1}{k!}dx+\int_{k+\frac{2}{3}}^{k+1}\frac{3k+2}{k!}dx \right ) =\lim_{n \to \infty}\sum_{k=1}^{n-1}\left ( \frac{1}{3}\left ( \frac{3k}{k!} \right )+\frac{1}{3}\left ( \frac{3k+1}{k!} \right ) +\frac{1}{3}\left ( \frac{3k+2}{k!} \right )\right )=\lim_{n \to \infty}\sum_{k=1}^{n-1}\frac{3k+1}{k!}=3\sum_{k=1}^{\infty}\frac{1}{(k-1)!}+\sum_{k=1}^{\infty}\frac{1}{k!} =3e+e-1=4e-1,"['calculus', 'integration', 'definite-integrals']"
21,Proof Verification: $\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4}$,Proof Verification:,\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4},"While curiosity gets the best of me in the fascinating world of complex analysis, I have decided to tackle the following integral and prove it's equal to $\frac{\pi^2}{4}$ : $$\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx.$$ Proof. Let $f(z) = \frac{\text{Log}(z+1)}{z}$ where $\text{Log}$ denotes the principal logarithm. We will traverse, in a counterclockwise direction, a path that closely resembles a semicircle of radius 1 above the real axis such that: there is a small indented semicircle path $\gamma_1$ above $z=0$ ; another path $\gamma_2$ closely resembling a small quarter circle to the right of $z=-1$ ; a path $\Gamma$ representing the circumference. We will call this path $C$ , which is $$C = \left[-1+ \epsilon, -\epsilon\right] \cup \gamma_1 \cup \left[\epsilon,1\right] \cup \Gamma \cup \gamma_2.$$ There are no singularities inside $C$ , so by Cauchy's Theorem, we have $$\oint_Cf(z)dz = 0.$$ But we also have $$\eqalign{ \oint_Cf(z)dz &= \int_{-1+\epsilon}^{\epsilon}f(z)dz + \int_{\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz + \int_{\gamma_2}f(z)dz \cr &= \int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz. }$$ We will call each integral $I_1$ , $I_2$ , $...$ , $I_5$ , respectively. First, we will prove that $I_2$ goes to $0$ . Parameterizing $z = \epsilon e^{i\theta}$ where $\theta \in \left[0,\pi\right]$ , we get $$\eqalign{ I_2 :&= \int_{-\gamma_1}\frac{\text{Log}(z+1)}{z}dz \cr &= i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta. }$$ Finding the upper bounds of that integral, we observe that $$\eqalign{ 0 &\leq \left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \cr &\leq \epsilon \int_{0}^{\pi}\left|\text{Log}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr &= \epsilon \int_{0}^{\pi}\left|\text{Log}\left|\epsilon e^{i\theta} + 1\right| + i\text{Arg}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr &\leq \epsilon \left(\left|\text{Log}\left|\epsilon e^{i\theta}+1\right|\right| + \pi\right) \cr &\leq \epsilon(\epsilon + 1 + \pi). }$$ Taking the limit as $\epsilon$ goes to $0$ , we can apply the Squeeze Theorem: $$\lim_{\epsilon \to 0} 0 \leq \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq \lim_{\epsilon \to 0}\epsilon(\epsilon + 1 + \pi)$$ $$0 \leq  \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq 0.$$ This shows that $$\lim_{\epsilon \to 0}i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta = 0.$$ Albeit more tedious work, I can apply the same strategies for proving $I_5$ goes to $0$ . Let $\Phi_{\epsilon}$ represent the small arc length cut off from $\Gamma$ . Parameterize $z = -1 + \epsilon e^{i\Phi}$ where $\Phi \in \left[0, \frac{\pi}{2} - \Phi_{\epsilon}\right]$ Then (skipping some work) we get $$\eqalign{ I_5 :&= \int_{-\gamma_2}f(z)dz \cr &= \int_{0}^{\frac{\pi}{2} - \Phi_{\epsilon}}f(-1 + \epsilon e^{i\Phi})d(-1 + \epsilon e^{i\Phi}) \cr &= i\epsilon \ln{(\epsilon)}\int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi - \epsilon \int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi. }$$ Both integrals go to $0$ as $\epsilon$ goes to $0$ since $$\left|\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{1}{1-\epsilon} \text{ and } \left|\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{\Phi}{1-\epsilon}.$$ For $I_4$ , we can parameterize $z = e^{it}$ where $t \in \left[0, \pi - \Phi_{\epsilon}\right]$ to get $$\eqalign{ I_4 :&= \int_{\Gamma}f(z)dz \cr &= \int_{0}^{\pi - \Phi_{\epsilon}}f(e^{it})d(e^{it}) \cr &= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(1+e^{it}\right)dt \cr &= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(2e^{i\frac{t}{2}}\cos{\left(\frac{t}{2}\right)}\right)dt \cr &= i\int_{0}^{\pi - \Phi_{\epsilon}}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt - \frac{\left(\pi-\Phi_{\epsilon}\right)^2}{4}. }$$ As both $\epsilon$ and $\Phi_{\epsilon}$ approach $0$ , we see that $\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}$ is Lebesgue integrable on $\left[0,\pi\right]$ . It follows that $$\int_{0}^{\pi}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt = 0.$$ As $\Phi_{\epsilon}$ goes to $0$ , we see that $I_4$ goes to $-\frac{\pi^2}{4}$ . Going back to $C$ , as $\epsilon$ and $\Phi_{\epsilon}$ approach $0$ , we get $$\eqalign{ \lim_{\epsilon,\Phi_{\epsilon} \to 0}0 &= \lim_{\epsilon,\Phi_{\epsilon} \to 0}\left(\int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz\right) \cr 0 &= \int_{-1}^{0}f(z)dz - 0 + \int_{0}^{1}f(z)dz - \frac{\pi^2}{4} - 0. }$$ Therefore, $$\int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4}.$$ Q.E.D. Is there a need for the $\gamma_1$ indented path? Because we know on the real line, the limit as $x$ goes to $0$ of $f(x)$ exists unlike when $x$ goes to $-1$ from the right, which results in $f(x)$ going off to $\infty$ since $x=-1$ is a vertical asymptote. Other than that, I want to say my proof is good enough. Let me know if you know any other strategies you have (I know a much simpler way but I wanted to find different ways to solve the problem). If you have any suggestions on optimizing the solution or find any errors, please do not hesitate to share them with me!","While curiosity gets the best of me in the fascinating world of complex analysis, I have decided to tackle the following integral and prove it's equal to : Proof. Let where denotes the principal logarithm. We will traverse, in a counterclockwise direction, a path that closely resembles a semicircle of radius 1 above the real axis such that: there is a small indented semicircle path above ; another path closely resembling a small quarter circle to the right of ; a path representing the circumference. We will call this path , which is There are no singularities inside , so by Cauchy's Theorem, we have But we also have We will call each integral , , , , respectively. First, we will prove that goes to . Parameterizing where , we get Finding the upper bounds of that integral, we observe that Taking the limit as goes to , we can apply the Squeeze Theorem: This shows that Albeit more tedious work, I can apply the same strategies for proving goes to . Let represent the small arc length cut off from . Parameterize where Then (skipping some work) we get Both integrals go to as goes to since For , we can parameterize where to get As both and approach , we see that is Lebesgue integrable on . It follows that As goes to , we see that goes to . Going back to , as and approach , we get Therefore, Q.E.D. Is there a need for the indented path? Because we know on the real line, the limit as goes to of exists unlike when goes to from the right, which results in going off to since is a vertical asymptote. Other than that, I want to say my proof is good enough. Let me know if you know any other strategies you have (I know a much simpler way but I wanted to find different ways to solve the problem). If you have any suggestions on optimizing the solution or find any errors, please do not hesitate to share them with me!","\frac{\pi^2}{4} \int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx. f(z) = \frac{\text{Log}(z+1)}{z} \text{Log} \gamma_1 z=0 \gamma_2 z=-1 \Gamma C C = \left[-1+ \epsilon, -\epsilon\right] \cup \gamma_1 \cup \left[\epsilon,1\right] \cup \Gamma \cup \gamma_2. C \oint_Cf(z)dz = 0. \eqalign{
\oint_Cf(z)dz &= \int_{-1+\epsilon}^{\epsilon}f(z)dz + \int_{\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz + \int_{\gamma_2}f(z)dz \cr
&= \int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz.
} I_1 I_2 ... I_5 I_2 0 z = \epsilon e^{i\theta} \theta \in \left[0,\pi\right] \eqalign{
I_2 :&= \int_{-\gamma_1}\frac{\text{Log}(z+1)}{z}dz \cr
&= i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta.
} \eqalign{
0 &\leq \left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \cr
&\leq \epsilon \int_{0}^{\pi}\left|\text{Log}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr
&= \epsilon \int_{0}^{\pi}\left|\text{Log}\left|\epsilon e^{i\theta} + 1\right| + i\text{Arg}\left(\epsilon e^{i\theta} + 1\right)\right|d\theta \cr
&\leq \epsilon \left(\left|\text{Log}\left|\epsilon e^{i\theta}+1\right|\right| + \pi\right) \cr
&\leq \epsilon(\epsilon + 1 + \pi).
} \epsilon 0 \lim_{\epsilon \to 0} 0 \leq \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq \lim_{\epsilon \to 0}\epsilon(\epsilon + 1 + \pi) 0 \leq  \lim_{\epsilon \to 0}\left|i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta\right| \leq 0. \lim_{\epsilon \to 0}i\epsilon\int_0^{\pi}\text{Log}\left(\epsilon e^{i\theta} + 1\right)d\theta = 0. I_5 0 \Phi_{\epsilon} \Gamma z = -1 + \epsilon e^{i\Phi} \Phi \in \left[0, \frac{\pi}{2} - \Phi_{\epsilon}\right] \eqalign{
I_5 :&= \int_{-\gamma_2}f(z)dz \cr
&= \int_{0}^{\frac{\pi}{2} - \Phi_{\epsilon}}f(-1 + \epsilon e^{i\Phi})d(-1 + \epsilon e^{i\Phi}) \cr
&= i\epsilon \ln{(\epsilon)}\int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi - \epsilon \int_{0}^{\frac{\pi}{2}-\Phi_{\epsilon}}\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}d\Phi.
} 0 \epsilon 0 \left|\frac{e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{1}{1-\epsilon} \text{ and } \left|\frac{\Phi e^{i\Phi}}{-1 + \epsilon e^{i\Phi}}\right| \leq \frac{\Phi}{1-\epsilon}. I_4 z = e^{it} t \in \left[0, \pi - \Phi_{\epsilon}\right] \eqalign{
I_4 :&= \int_{\Gamma}f(z)dz \cr
&= \int_{0}^{\pi - \Phi_{\epsilon}}f(e^{it})d(e^{it}) \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(1+e^{it}\right)dt \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\text{Log}\left(2e^{i\frac{t}{2}}\cos{\left(\frac{t}{2}\right)}\right)dt \cr
&= i\int_{0}^{\pi - \Phi_{\epsilon}}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt - \frac{\left(\pi-\Phi_{\epsilon}\right)^2}{4}.
} \epsilon \Phi_{\epsilon} 0 \ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)} \left[0,\pi\right] \int_{0}^{\pi}\ln{\left(2\cos{\left(\frac{t}{2}\right)}\right)}dt = 0. \Phi_{\epsilon} 0 I_4 -\frac{\pi^2}{4} C \epsilon \Phi_{\epsilon} 0 \eqalign{
\lim_{\epsilon,\Phi_{\epsilon} \to 0}0 &= \lim_{\epsilon,\Phi_{\epsilon} \to 0}\left(\int_{-1+\epsilon}^{\epsilon}f(z)dz - \int_{-\gamma_1}f(z)dz + \int_{\epsilon}^{1}f(z)dz + \int_{\Gamma}f(z)dz - \int_{-\gamma_2}f(z)dz\right) \cr
0 &= \int_{-1}^{0}f(z)dz - 0 + \int_{0}^{1}f(z)dz - \frac{\pi^2}{4} - 0.
} \int_{-1}^{1}\frac{\ln{(x+1)}}{x}dx = \frac{\pi^2}{4}. \gamma_1 x 0 f(x) x -1 f(x) \infty x=-1","['integration', 'complex-analysis', 'definite-integrals', 'solution-verification', 'complex-integration']"
22,"Determining whether $A(x) = \int_0^x |\sin(1/u) |\,du$ is differentiable at $0$ [duplicate]",Determining whether  is differentiable at  [duplicate],"A(x) = \int_0^x |\sin(1/u) |\,du 0","This question already has answers here : Is $\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t$ differentiable at $0$? (3 answers) Closed 1 year ago . Let $$A(x) = \int_0^x |\sin(1/u) |\,du = \lim\limits_{a\to 0^+} \int_a^x |\sin(1/u)|\,du$$ Determine, with proof, whether $A$ is differentiable at $0$ . Clearly $A(0) = 0$ . By definition, I need to show that $$\lim\limits_{x\to 0^+} \frac{A(x)}x = \lim\limits_{x\to 0^-} \dfrac{A(x)}x$$ I know $A$ is odd. Also, $$A(x) = \sum_{k=k_x}^\infty \int_{1/((k+1)\pi)}^{1/(k\pi)} |\sin(1/u)| du + \int_{1/(k_x\pi)}^x |\sin(1/u)| du,$$ where $k_x = \lceil 1/(x\pi)\rceil$ . Using the latter expression, I can show that $$\left|\dfrac{A(x)}x\right|\leq \frac{1}{|x|}\left|\sum_{k=k_x}^\infty \frac{1}{k\pi} - \frac{1}{(k+1)\pi} + x-\frac{1}{k_x \pi}\right| = 1,$$ but obviously this isn't enough to show the derivative exists.","This question already has answers here : Is $\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t$ differentiable at $0$? (3 answers) Closed 1 year ago . Let Determine, with proof, whether is differentiable at . Clearly . By definition, I need to show that I know is odd. Also, where . Using the latter expression, I can show that but obviously this isn't enough to show the derivative exists.","A(x) = \int_0^x |\sin(1/u) |\,du = \lim\limits_{a\to 0^+} \int_a^x |\sin(1/u)|\,du A 0 A(0) = 0 \lim\limits_{x\to 0^+} \frac{A(x)}x = \lim\limits_{x\to 0^-} \dfrac{A(x)}x A A(x) = \sum_{k=k_x}^\infty \int_{1/((k+1)\pi)}^{1/(k\pi)} |\sin(1/u)| du + \int_{1/(k_x\pi)}^x |\sin(1/u)| du, k_x = \lceil 1/(x\pi)\rceil \left|\dfrac{A(x)}x\right|\leq \frac{1}{|x|}\left|\sum_{k=k_x}^\infty \frac{1}{k\pi} - \frac{1}{(k+1)\pi} + x-\frac{1}{k_x \pi}\right| = 1,","['real-analysis', 'calculus', 'integration', 'derivatives', 'trigonometry']"
23,Intuition of contour integration of |z| [duplicate],Intuition of contour integration of |z| [duplicate],,"This question already has answers here : What is a geometric explanation of complex integration in plain English? (5 answers) Closed 2 years ago . Given the following contour integral: $$ I=\int\limits_{-1}^1 |z| \ \mathrm{dz}, $$ with path of integration being the upper half of unit circle, it can be parameterized to give: $$ \int\limits_{-1}^1 |z| \ \mathrm{dz} = -\int\limits_\Gamma |z| \ \mathrm{dz}, $$ where $\Gamma = e^{it}, \ t\in[0,\pi]$ and $\mathrm{dz}=ie^{it}\ \mathrm{dt},$ so $$ -\int\limits_\Gamma |z| \ \mathrm{dz} = -\int\limits_0^\pi |e^{it}|\ ie^{it} \ \mathrm{dt} = -i\int\limits_0^\pi e^{it} \ \mathrm{dt} = -i\left[ -ie^{it} \right]_0^\pi = -i^2-i^2 = 2. $$ Since $f:\Gamma \to \mathbb{R} \ $ given by $f(z)=|z|$ is a real-valued function of a complex variable, it can be visualized in 3D. And interpreting integral as ""area under curve"" it would seem natural to assume that $I=\pi\ $ which is incorrect. The question is: what's wrong with this intuition and what's the correct way to think about the geometry of these types of integrals?","This question already has answers here : What is a geometric explanation of complex integration in plain English? (5 answers) Closed 2 years ago . Given the following contour integral: with path of integration being the upper half of unit circle, it can be parameterized to give: where and so Since given by is a real-valued function of a complex variable, it can be visualized in 3D. And interpreting integral as ""area under curve"" it would seem natural to assume that which is incorrect. The question is: what's wrong with this intuition and what's the correct way to think about the geometry of these types of integrals?"," I=\int\limits_{-1}^1 |z| \ \mathrm{dz},   \int\limits_{-1}^1 |z| \ \mathrm{dz} = -\int\limits_\Gamma |z| \ \mathrm{dz},  \Gamma = e^{it}, \ t\in[0,\pi] \mathrm{dz}=ie^{it}\ \mathrm{dt},  -\int\limits_\Gamma |z| \ \mathrm{dz} = -\int\limits_0^\pi |e^{it}|\ ie^{it} \ \mathrm{dt}
= -i\int\limits_0^\pi e^{it} \ \mathrm{dt} = -i\left[ -ie^{it} \right]_0^\pi = -i^2-i^2 = 2.  f:\Gamma \to \mathbb{R} \  f(z)=|z| I=\pi\ ","['integration', 'complex-analysis', 'complex-integration']"
24,Is it valid to say that $\int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx$?,Is it valid to say that ?,\int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx,"So, I'm a college freshman, and when I was doing a homework problem, I found that the definite integral was equal to zero. It was an integral of the form $\int_{-\infty}^\infty f(x)dx$ and $f(x)$ happened to be an odd function. I never questioned whether the property of odd functions, $\int_{-a}^a f(x)dx = 0$ still works when a is infinity instead of a finite number. But my intuition and that homework problem I did seem to suggest that. My reasoning to justify the idea is that for an odd function $f(x)$ , $\int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx = \lim_{t\to \infty} 0 = 0.$ This argument only works if the first step is valid. I'm wondering if there are any problems with it since you usually deal with this kind of improper integral as two integrals: $ \int_{-\infty}^\infty f(x)dx = \int_{-\infty}^a f(x)dx + \int_a^\infty f(x)dx$ .","So, I'm a college freshman, and when I was doing a homework problem, I found that the definite integral was equal to zero. It was an integral of the form and happened to be an odd function. I never questioned whether the property of odd functions, still works when a is infinity instead of a finite number. But my intuition and that homework problem I did seem to suggest that. My reasoning to justify the idea is that for an odd function , This argument only works if the first step is valid. I'm wondering if there are any problems with it since you usually deal with this kind of improper integral as two integrals: .",\int_{-\infty}^\infty f(x)dx f(x) \int_{-a}^a f(x)dx = 0 f(x) \int_{-\infty}^\infty f(x)dx = \lim_{t\to \infty} \int_{-t}^t f(x)dx = \lim_{t\to \infty} 0 = 0.  \int_{-\infty}^\infty f(x)dx = \int_{-\infty}^a f(x)dx + \int_a^\infty f(x)dx,"['calculus', 'integration']"
25,"Inequality of *Problem From The Book* 19.20, but Using Integrals","Inequality of *Problem From The Book* 19.20, but Using Integrals",,"The problem is here: Let $a_1,a_2,\dots,a_n$ be positive real numbers and let $S=a_1+a_2+\dots+a_n$ . Prove that $$\frac 1n \sum_{1=1}^n\frac 1{a_i}+\frac{n(n-2)}S\ge\sum_{i\ne j}\frac1{S-a_i+a_j}$$ The chapter of this problem is Solving Elementary Inequality Using Integrals . After I typed the problem, I noted that this problem is already asked here . The answer is using  Karamata's inequality, which... does not really fit the title of the chapter. This chapter suggested a technique: using $\int_0^1 x^{t-1}\mathrm dx=\frac 1t$ to transform the problems involving the fractions into polynomials. I tried this technique, let $y_i=x^{a_i-1/n}$ , and we have, before integral as $\int_0^1 \dots\mathrm dx$ , the inequality should be $$\sum_{i=1}^n y_i^n+n(n-2)y_1y_2\dots y_n\ge y_1y_2\dots y_n\sum_{i\ne j}\frac{y_i}{y_j}$$ and I don't know  how to continue from here... or, is there another integration method other than the techneque above? Many thanks!","The problem is here: Let be positive real numbers and let . Prove that The chapter of this problem is Solving Elementary Inequality Using Integrals . After I typed the problem, I noted that this problem is already asked here . The answer is using  Karamata's inequality, which... does not really fit the title of the chapter. This chapter suggested a technique: using to transform the problems involving the fractions into polynomials. I tried this technique, let , and we have, before integral as , the inequality should be and I don't know  how to continue from here... or, is there another integration method other than the techneque above? Many thanks!","a_1,a_2,\dots,a_n S=a_1+a_2+\dots+a_n \frac 1n \sum_{1=1}^n\frac 1{a_i}+\frac{n(n-2)}S\ge\sum_{i\ne j}\frac1{S-a_i+a_j} \int_0^1 x^{t-1}\mathrm dx=\frac 1t y_i=x^{a_i-1/n} \int_0^1 \dots\mathrm dx \sum_{i=1}^n y_i^n+n(n-2)y_1y_2\dots y_n\ge y_1y_2\dots y_n\sum_{i\ne j}\frac{y_i}{y_j}","['integration', 'inequality']"
26,Finding the approximate value of improper integral using the Monte-Carlo method,Finding the approximate value of improper integral using the Monte-Carlo method,,"Find the approximate value of the improper integral $$ \int_{-3}^{\infty} \left( \int_{0.5}^{\infty} \frac{2+\sin(x+y)}{e^{0.4x}+0.4y^2} \,dx \right) \, dy $$ using the Monte-Carlo method. I have managed to define the necessary functions in R in order to apply the MC method but the problem lies in the function under the integral itself. I have tried to use well-known distributions (exponential and normal) to rewrite the function and get an idea from which distributions should I generate $x$ and $y$ but due to lack of luck in doing that it seems like a wrong direction, perhaps there is an easier way. So any help, hints, tips and tricks on how to solve the problem would be greatly appreciated.","Find the approximate value of the improper integral using the Monte-Carlo method. I have managed to define the necessary functions in R in order to apply the MC method but the problem lies in the function under the integral itself. I have tried to use well-known distributions (exponential and normal) to rewrite the function and get an idea from which distributions should I generate and but due to lack of luck in doing that it seems like a wrong direction, perhaps there is an easier way. So any help, hints, tips and tricks on how to solve the problem would be greatly appreciated.","
\int_{-3}^{\infty} \left( \int_{0.5}^{\infty} \frac{2+\sin(x+y)}{e^{0.4x}+0.4y^2} \,dx \right) \, dy
 x y","['integration', 'probability-distributions', 'improper-integrals', 'monte-carlo']"
27,$\int_{-\infty}^{\infty} {\rm exp}\left(\frac{i b t - a}{t^2 + 1}\right) - {\rm exp}\left(\frac{i b}{t} - \frac{1}{t^2}\right) {\rm d}t$,,\int_{-\infty}^{\infty} {\rm exp}\left(\frac{i b t - a}{t^2 + 1}\right) - {\rm exp}\left(\frac{i b}{t} - \frac{1}{t^2}\right) {\rm d}t,"How to solve this integral? $$\int_{-\infty}^{\infty} {\rm exp}\left(\frac{i b t - a}{t^2 + 1}\right) -    {\rm exp}\left(\frac{i b}{t} - \frac{1}{t^2}\right) {\rm d}t\\ {\rm with}\, a>0,b\in \mathbb{R},i^2=-1$$ The integral converges only if both summands are considered together. The integral originates from the comments in this post .",How to solve this integral? The integral converges only if both summands are considered together. The integral originates from the comments in this post .,"\int_{-\infty}^{\infty} {\rm exp}\left(\frac{i b t - a}{t^2 + 1}\right) - 
  {\rm exp}\left(\frac{i b}{t} - \frac{1}{t^2}\right) {\rm d}t\\ {\rm with}\, a>0,b\in \mathbb{R},i^2=-1","['integration', 'definite-integrals', 'residue-calculus']"
28,For complex measure $\mu$ and $0< f \le g$ is it true that $\Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big|$?,For complex measure  and  is it true that ?,\mu 0< f \le g \Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big|,"Let $\mu$ be a complex measure on $\Bbb R^{n}$ and $f,g \in L^1(\mu)$ such that $0< f(x) \le g(x)$ for a.e. $x \in \Bbb R^n$ . Then is it true that $$\Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big| \text{ ? }$$ I was trying to break each integral first in its real and imaginary part and then their corresponding positive and negative parts, i.e. $$\int_{\Bbb R^n } f d\mu = Re\Big(\int_{\Bbb R^n } f d\mu\Big) +i Im \Big(\int_{\Bbb R^n } f d\mu\Big)$$ $$=Re^+\Big(\int_{\Bbb R^n } f d\mu\Big)-Re^-\Big(\int_{\Bbb R^n } f d\mu\Big)+iIm^+\Big(\int_{\Bbb R^n } f d\mu\Big)-iIm^-\Big(\int_{\Bbb R^n } f d\mu\Big)$$ and then wanted to look at corresponding decompositions of the measure and the integral $\int_{\Bbb R^n } g d\mu$ and obtain inequalities. But I can't figure them out and proceed from here. Can someone please help?","Let be a complex measure on and such that for a.e. . Then is it true that I was trying to break each integral first in its real and imaginary part and then their corresponding positive and negative parts, i.e. and then wanted to look at corresponding decompositions of the measure and the integral and obtain inequalities. But I can't figure them out and proceed from here. Can someone please help?","\mu \Bbb R^{n} f,g \in L^1(\mu) 0< f(x) \le g(x) x \in \Bbb R^n \Big|\int_{\Bbb R^n } f d\mu\Big| \le \Big|\int_{\Bbb R^n } g d\mu\Big| \text{ ? } \int_{\Bbb R^n } f d\mu = Re\Big(\int_{\Bbb R^n } f d\mu\Big) +i Im \Big(\int_{\Bbb R^n } f d\mu\Big) =Re^+\Big(\int_{\Bbb R^n } f d\mu\Big)-Re^-\Big(\int_{\Bbb R^n } f d\mu\Big)+iIm^+\Big(\int_{\Bbb R^n } f d\mu\Big)-iIm^-\Big(\int_{\Bbb R^n } f d\mu\Big) \int_{\Bbb R^n } g d\mu","['integration', 'analysis', 'measure-theory']"
29,"Mistake computing $\int_0^\infty\frac{x \ln(1+x^2)}{\sinh \pi x}\,dx=\frac{\ln 2}{3} - \log \pi - \frac{1}{2} + 6 \ln A $",Mistake computing,"\int_0^\infty\frac{x \ln(1+x^2)}{\sinh \pi x}\,dx=\frac{\ln 2}{3} - \log \pi - \frac{1}{2} + 6 \ln A ","Edit I found the mistake, see my answer below. I am trying to evaluate the integral $$\int_0^\infty\frac{x \ln(1+x^2)}{\sinh \pi x}\,dx=\frac{\ln 2}{3} - \log \pi - \frac{1}{2} + 6 \ln A $$ I know this integral was already evaluated in this post utilizing various different techniques. But I am trying to compute yet with another one, but due to a mistake which I cannot spot,  I  can´t finish. This is how I proceeded: First note that $$ \begin{aligned} &\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{2}{e^{2 \pi x}-1}\\ &\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{2e^{-\pi x}}{e^{ \pi x}-e^{-\pi x}}\\ &\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{e^{-\pi x}}{\sinh \pi x}\\ &\frac{e^{-\pi x}+1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}\\ &\frac{1}{\sinh \pi x}=\frac{2}{(e^{\pi x}-1)(e^{-\pi x}+1)}\\ &\frac{1}{\sinh \pi x}=\frac{1}{\sinh \pi x} \qquad \blacksquare\\ \end{aligned} $$ Then consider the following more general integral, which upon letting $z \to 1$ recovers the goal integral. $$I(z)=\int_0^\infty\frac{x \ln(z^2+x^2)}{\sinh \pi x}\,dx $$ It can be rewritten as $$I(z)=2\int_0^\infty\frac{x \ln(z^2+x^2)}{e^{\pi x}-1}\,dx-2\int_0^\infty\frac{x \ln(z^2+x^2)}{e^{2\pi x}-1}\,dx \tag{1}$$ Differentiating $(1)$ w.r. to $z$ we obtain $$I^{\prime}(z)=4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{\pi x}-1)}\,dx-4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{2\pi x}-1)}\,dx \tag{2}$$ Now recall Binet´s Integral representation for the Digamma function $$\int_0^\infty\frac{x }{(z^2+x^2)(e^{2\pi x}-1)}\,dx=\frac{\log(z)}{2}-\frac{\psi(z)}{2}-\frac{1}{4z} \tag{3}$$ Multiplying $(3)$ by $4 \, z$ $$4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{2\pi x}-1)}\,dx=2z\log(z)-2z\psi(z)-1 \tag{4}$$ Also, making the change of variable $2x \mapsto x$ and multiplying by $4z$ in $(3)$ we get $$4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{\pi x}-1)}\,dx=2z\log\left(\frac{z}{2}\right)-2z\psi\left(\frac{z}{2}\right)-2 \tag{5}$$ Plugging the R.H.S. of $(4)$ and $(5)$ back in $(2)$ we get $$\begin{aligned} I^{\prime}(z)&=2z\log\left(\frac{z}{2}\right)-2z\psi\left(\frac{z}{2}\right)-2-2z\log(z)+2z\psi(z)+1\\ &=2\left(z\ln z-z \ln2 \right)-2z\psi\left(\frac{z}{2}\right)-2z\ln z+2 z \psi(z)-1\\ &=-2 \ln2 z -2z\psi\left(\frac{z}{2}\right)+2 z \psi(z)-1\\ \end{aligned}$$ Now integrate from $0 \,\, \text{to} \,\, z$ $$\begin{aligned} I(z)&=-2 \ln2 \int_0^zx\,dx -2\int_0^z x\psi\left(\frac{x}{2}\right)\,dx+2 \int_0^z z \psi(x)\,dx-\int_0^z \,dx\\ &=-\ln2 z^2 -2\int_0^z x\psi\left(\frac{x}{2}\right)\,dx+2 \int_0^z x \psi(x)\,dx-z \end{aligned}$$ Letting $z \to 1$ $$I(1)= -\ln2  -8\int_0^{1/2} x\psi\left(x\right)\,dx+2 \int_0^1 x \psi(x)\,dx-\ln e \tag{6}$$ The two integrals involving the Digamma function I computed as follows $$ \begin{aligned} \int_0^1 x \psi(x)\,dx&=-\int_0^1 \ln \Gamma(x) dx\\ &=-\frac12 \ln 2 \pi \end{aligned} $$ $$ \begin{aligned} \int_0^1 x \psi\left(\frac{x}{2}\right)\,dx&=4\int_0^{1/2} x \psi(x)\,dx \qquad \left(\frac x2 \to x\right)\\ &=4\left(x\ln \Gamma(x) \Big|_0^{1/2}-\int_0^{1/2} \ln \Gamma(x) dx \right)\\ &= \ln  \pi-4\left(\frac32 \ln A+\frac{5}{24}\ln 2+ \frac14 \ln \pi\right)\\ &=-\ln A^6-\frac56 \ln 2 \end{aligned} $$ where in the last one I used the result $$\int_0^{1/2} \ln \Gamma(x) dx=\frac32 \ln A+\frac{5}{24}\ln 2+ \frac14 \ln \pi$$ Plugging back the values of the integrals in $(6)$ I got $$\begin{aligned} I(1)&=-\ln2  +2\left(\ln A^6+\frac56 \ln 2 \right)- \ln 2 -\ln\pi-\ln e\\ &=-2\ln2  +2\ln A^6+\frac53 \ln 2  -\ln\pi -\ln e\\ \end{aligned}$$ Which does not match with the right answer. Can someone point me where is my mistake(s)?","Edit I found the mistake, see my answer below. I am trying to evaluate the integral I know this integral was already evaluated in this post utilizing various different techniques. But I am trying to compute yet with another one, but due to a mistake which I cannot spot,  I  can´t finish. This is how I proceeded: First note that Then consider the following more general integral, which upon letting recovers the goal integral. It can be rewritten as Differentiating w.r. to we obtain Now recall Binet´s Integral representation for the Digamma function Multiplying by Also, making the change of variable and multiplying by in we get Plugging the R.H.S. of and back in we get Now integrate from Letting The two integrals involving the Digamma function I computed as follows where in the last one I used the result Plugging back the values of the integrals in I got Which does not match with the right answer. Can someone point me where is my mistake(s)?","\int_0^\infty\frac{x \ln(1+x^2)}{\sinh \pi x}\,dx=\frac{\ln 2}{3} - \log \pi - \frac{1}{2} + 6 \ln A  
\begin{aligned}
&\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{2}{e^{2 \pi x}-1}\\
&\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{2e^{-\pi x}}{e^{ \pi x}-e^{-\pi x}}\\
&\frac{1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}-\frac{e^{-\pi x}}{\sinh \pi x}\\
&\frac{e^{-\pi x}+1}{\sinh \pi x}=\frac{2}{e^{\pi x}-1}\\
&\frac{1}{\sinh \pi x}=\frac{2}{(e^{\pi x}-1)(e^{-\pi x}+1)}\\
&\frac{1}{\sinh \pi x}=\frac{1}{\sinh \pi x} \qquad \blacksquare\\
\end{aligned}
 z \to 1 I(z)=\int_0^\infty\frac{x \ln(z^2+x^2)}{\sinh \pi x}\,dx  I(z)=2\int_0^\infty\frac{x \ln(z^2+x^2)}{e^{\pi x}-1}\,dx-2\int_0^\infty\frac{x \ln(z^2+x^2)}{e^{2\pi x}-1}\,dx \tag{1} (1) z I^{\prime}(z)=4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{\pi x}-1)}\,dx-4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{2\pi x}-1)}\,dx \tag{2} \int_0^\infty\frac{x }{(z^2+x^2)(e^{2\pi x}-1)}\,dx=\frac{\log(z)}{2}-\frac{\psi(z)}{2}-\frac{1}{4z} \tag{3} (3) 4 \, z 4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{2\pi x}-1)}\,dx=2z\log(z)-2z\psi(z)-1 \tag{4} 2x \mapsto x 4z (3) 4\int_0^\infty\frac{zx }{(z^2+x^2)(e^{\pi x}-1)}\,dx=2z\log\left(\frac{z}{2}\right)-2z\psi\left(\frac{z}{2}\right)-2 \tag{5} (4) (5) (2) \begin{aligned}
I^{\prime}(z)&=2z\log\left(\frac{z}{2}\right)-2z\psi\left(\frac{z}{2}\right)-2-2z\log(z)+2z\psi(z)+1\\
&=2\left(z\ln z-z \ln2 \right)-2z\psi\left(\frac{z}{2}\right)-2z\ln z+2 z \psi(z)-1\\
&=-2 \ln2 z -2z\psi\left(\frac{z}{2}\right)+2 z \psi(z)-1\\
\end{aligned} 0 \,\, \text{to} \,\, z \begin{aligned}
I(z)&=-2 \ln2 \int_0^zx\,dx -2\int_0^z x\psi\left(\frac{x}{2}\right)\,dx+2 \int_0^z z \psi(x)\,dx-\int_0^z \,dx\\
&=-\ln2 z^2 -2\int_0^z x\psi\left(\frac{x}{2}\right)\,dx+2 \int_0^z x \psi(x)\,dx-z
\end{aligned} z \to 1 I(1)= -\ln2  -8\int_0^{1/2} x\psi\left(x\right)\,dx+2 \int_0^1 x \psi(x)\,dx-\ln e \tag{6} 
\begin{aligned}
\int_0^1 x \psi(x)\,dx&=-\int_0^1 \ln \Gamma(x) dx\\
&=-\frac12 \ln 2 \pi
\end{aligned}
 
\begin{aligned}
\int_0^1 x \psi\left(\frac{x}{2}\right)\,dx&=4\int_0^{1/2} x \psi(x)\,dx \qquad \left(\frac x2 \to x\right)\\
&=4\left(x\ln \Gamma(x) \Big|_0^{1/2}-\int_0^{1/2} \ln \Gamma(x) dx \right)\\
&= \ln  \pi-4\left(\frac32 \ln A+\frac{5}{24}\ln 2+ \frac14 \ln \pi\right)\\
&=-\ln A^6-\frac56 \ln 2
\end{aligned}
 \int_0^{1/2} \ln \Gamma(x) dx=\frac32 \ln A+\frac{5}{24}\ln 2+ \frac14 \ln \pi (6) \begin{aligned}
I(1)&=-\ln2  +2\left(\ln A^6+\frac56 \ln 2 \right)- \ln 2 -\ln\pi-\ln e\\
&=-2\ln2  +2\ln A^6+\frac53 \ln 2  -\ln\pi -\ln e\\
\end{aligned}","['integration', 'digamma-function']"
30,$\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx$ [duplicate],[duplicate],\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx,"This question already has an answer here : Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? [closed] (1 answer) Closed 2 years ago . The first problem I encountered $$\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx$$ It $\displaystyle\int_{-\infty}^{\infty} e^{-x^2} dx$ I tried to make it variable interchangeable, like in your problem, but I didn't get any results. How can you prove that a function has no closed form integral? Then I learned a lot through this post. But still $$\int_{a}^{b} f(x)^{g(x)} dx$$ $$\begin{array}{I|l|l|I}f(x)&g(x)\\ \hline \sin{(x)}&\cos{(x)} \\ \cos{(x)}&\sin{(x)} \\ \tan{(x)}&\cot{(x)} \\ \arcsin{(x)}&\arccos{(x)} \\ \tan{(x)} &\sin{(x)} \\ ...&...\end{array}$$ I realized I didn't know how to calculate specific integrals of its type. I'm looking for a solution to your first problem, can you help me? Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? It's the same problem here, but it looks like there's been no answer. That's why I wanted to ask you again. $WolframAlpha$ although he says there is a value to a particular integrale $$\lim_{x\to\pi^{-}} \sin{(x)}^{\cos{(x)}}=\exp{(-\log{0})}=+\infty$$ is.","This question already has an answer here : Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? [closed] (1 answer) Closed 2 years ago . The first problem I encountered It I tried to make it variable interchangeable, like in your problem, but I didn't get any results. How can you prove that a function has no closed form integral? Then I learned a lot through this post. But still I realized I didn't know how to calculate specific integrals of its type. I'm looking for a solution to your first problem, can you help me? Can anyone help in $\int_0^\pi (\sin x )^{\cos x} dx$? It's the same problem here, but it looks like there's been no answer. That's why I wanted to ask you again. although he says there is a value to a particular integrale is.",\int_{0}^{\pi} \sin{(x)}^{\cos{(x)}} dx \displaystyle\int_{-\infty}^{\infty} e^{-x^2} dx \int_{a}^{b} f(x)^{g(x)} dx \begin{array}{I|l|l|I}f(x)&g(x)\\ \hline \sin{(x)}&\cos{(x)} \\ \cos{(x)}&\sin{(x)} \\ \tan{(x)}&\cot{(x)} \\ \arcsin{(x)}&\arccos{(x)} \\ \tan{(x)} &\sin{(x)} \\ ...&...\end{array} WolframAlpha \lim_{x\to\pi^{-}} \sin{(x)}^{\cos{(x)}}=\exp{(-\log{0})}=+\infty,"['integration', 'definite-integrals']"
31,How to compute $\frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy$ and similar integrals,How to compute  and similar integrals,\frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy,"For an integer $ k \ge 0$ and a scalar $x \in \mathbb R$ , define $$ \begin{split} A_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy,\\ B_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^k(\sin y + x)^kdy,\\ C_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi\cos y (\cos y + x)^k(\sin y + x)^{k-1}dy \end{split} $$ Question. How to obtain analytic formulae for $A_k(x)$ , $B_k(x)$ , and $C_k(x)$ ? Attempt Let's attempt to compute $A_k(x)$ (since it looks the least intimidating). By Newton's binomial theorem, one can write $A_k(x) = \sum_{j=0}^k {2k\choose 2j} I_j(x) x^{2k-2j}$ , where $$ I_j(x) := \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y)^{2j} dy=\frac{1}{2\pi}\int_0^{2\pi}(\cos y)^{2j}dy = \frac{1}{2^{2j}}{2j \choose j}, $$ Putting things together gives $$ A_k(x) = \sum_{j=0}^k{2k\choose 2j}{2j\choose j}2^{-2j}x^{2k-2j} = (2k)!\sum_{j=0}^k \frac{1}{(j!2^j)^2}\frac{x^{2k-2j}}{(2k-2j)!} = \ldots $$ and I don't know how to go from here (though it looks like the coefficient of a certain order term in a series expansion).","For an integer and a scalar , define Question. How to obtain analytic formulae for , , and ? Attempt Let's attempt to compute (since it looks the least intimidating). By Newton's binomial theorem, one can write , where Putting things together gives and I don't know how to go from here (though it looks like the coefficient of a certain order term in a series expansion)."," k \ge 0 x \in \mathbb R 
\begin{split}
A_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^{2k}dy,\\
B_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y + x)^k(\sin y + x)^kdy,\\
C_k(x) &:= \frac{1}{2\pi}\int_{-\pi}^\pi\cos y (\cos y + x)^k(\sin y + x)^{k-1}dy
\end{split}
 A_k(x) B_k(x) C_k(x) A_k(x) A_k(x) = \sum_{j=0}^k {2k\choose 2j} I_j(x) x^{2k-2j} 
I_j(x) := \frac{1}{2\pi}\int_{-\pi}^\pi (\cos y)^{2j} dy=\frac{1}{2\pi}\int_0^{2\pi}(\cos y)^{2j}dy = \frac{1}{2^{2j}}{2j \choose j},
 
A_k(x) = \sum_{j=0}^k{2k\choose 2j}{2j\choose j}2^{-2j}x^{2k-2j} = (2k)!\sum_{j=0}^k \frac{1}{(j!2^j)^2}\frac{x^{2k-2j}}{(2k-2j)!} = \ldots
","['integration', 'power-series', 'special-functions', 'generating-functions', 'spherical-coordinates']"
32,Prove that $\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt$ exists and is a finite value.,Prove that  exists and is a finite value.,\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt,"I'm trying to prove that this limit $$\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt$$ exists and it's finite. I have proved that $$\int_{1}^{+\infty} \left|\arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t) \right|dt = +\infty$$ so I can't use summability. My other attempt to solve the given integral is the fact that $f(t)=\arctan\left(\frac{1}{t}\right)\tanh(t)\sin(t)$ have the same behaviour of the function $$g(t)=\frac{\sin(t)}{t}$$ as $t\to +\infty$ and I succeded in proving that $\int_{1}^{+\infty} g(t)dt$ converges. But I know from theory that the asymptotic method can only be used if the integrands have a constant sign. So how can I use what I achieved to prove that the given integral converges? And, more in general, when the integrand has a variable sign what criterion can I use? Can I use something similar to the asymptotic method to deduce that two improper integrals have the same behavior? PS: another attempt was to use the integral criterion for the series with the general term $f(n)$ and the Dirichlet's test, but the problem is that I can't use the integral criterion since the integrand is not decreasing. So I have another question: is there a way to extend the integral criterion? (for example for functions that are not decreasing)","I'm trying to prove that this limit exists and it's finite. I have proved that so I can't use summability. My other attempt to solve the given integral is the fact that have the same behaviour of the function as and I succeded in proving that converges. But I know from theory that the asymptotic method can only be used if the integrands have a constant sign. So how can I use what I achieved to prove that the given integral converges? And, more in general, when the integrand has a variable sign what criterion can I use? Can I use something similar to the asymptotic method to deduce that two improper integrals have the same behavior? PS: another attempt was to use the integral criterion for the series with the general term and the Dirichlet's test, but the problem is that I can't use the integral criterion since the integrand is not decreasing. So I have another question: is there a way to extend the integral criterion? (for example for functions that are not decreasing)",\int_{1}^{+\infty} \arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t)dt \int_{1}^{+\infty} \left|\arctan\left(\frac{1}{t}\right)\tanh(t)\operatorname{sin}(t) \right|dt = +\infty f(t)=\arctan\left(\frac{1}{t}\right)\tanh(t)\sin(t) g(t)=\frac{\sin(t)}{t} t\to +\infty \int_{1}^{+\infty} g(t)dt f(n),"['real-analysis', 'integration', 'improper-integrals']"
33,Can we substitute complex number into gamma function formula,Can we substitute complex number into gamma function formula,,"I came across the following ""proof"" of the Eulerian integrals $$ \int_0^{\infty} t^{s-1} \cos(bt) e^{-at} \, dt = \frac{\Gamma(s) \cos \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}}, \int_0^{\infty} t^{s-1} \sin(bt) e^{-at} \, dt = \frac{\Gamma(s) \sin \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}} $$ where $s>0, b \in \mathbb{R}, a>0 $ and I'm wondering if it is rigorous and if not, what details need to be filled in to make it rigorous. Begin with the gamma integral $$ \Gamma(s) = \int_0^{\infty} x^{s-1} e^{-x} \, dx $$ and make the substitution $x=zt$ to get $$ \frac{\Gamma(s)}{z^s} = \int_0^{\infty} t^{s-1} e^{-zt} \, dt. $$ This is fine as long as we assume $z$ is real and positive. But now the ""proof"" asserts that the formula also holds for complex $z$ with positive real part* (this is the step I'm unsure about) and substitutes $z=a-ib$ for $a>0$ and $b \in \mathbb{R}$ and simplifies the result with the aid of polar coordinates $$ \int_0^{\infty} t^{s-1} e^{ibt} e^{-at} \, dt = \frac{\Gamma(s)}{(a-ib)^s} = \frac{\Gamma(s)}{\left(\sqrt{a^2+b^2} e^{i\arctan \left(-\frac{b}{a} \right)} \right)^s} = \frac{\Gamma(s)}{(a^2+b^2)^{s/2}} e^{is \arctan \left(\frac{b}{a} \right)} $$ and the integrals above follow by equating real and imaginary parts. Can someone please explain why the complex number substitution is valid here, and how to justify it? In general when is it ok to do this in an integral? Thanks","I came across the following ""proof"" of the Eulerian integrals where and I'm wondering if it is rigorous and if not, what details need to be filled in to make it rigorous. Begin with the gamma integral and make the substitution to get This is fine as long as we assume is real and positive. But now the ""proof"" asserts that the formula also holds for complex with positive real part* (this is the step I'm unsure about) and substitutes for and and simplifies the result with the aid of polar coordinates and the integrals above follow by equating real and imaginary parts. Can someone please explain why the complex number substitution is valid here, and how to justify it? In general when is it ok to do this in an integral? Thanks"," \int_0^{\infty} t^{s-1} \cos(bt) e^{-at} \, dt = \frac{\Gamma(s) \cos \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}}, \int_0^{\infty} t^{s-1} \sin(bt) e^{-at} \, dt = \frac{\Gamma(s) \sin \left(s\arctan \left(\frac{b}{a} \right) \right)}{(a^2+b^2)^{s/2}}  s>0, b \in \mathbb{R}, a>0   \Gamma(s) = \int_0^{\infty} x^{s-1} e^{-x} \, dx  x=zt  \frac{\Gamma(s)}{z^s} = \int_0^{\infty} t^{s-1} e^{-zt} \, dt.  z z z=a-ib a>0 b \in \mathbb{R}  \int_0^{\infty} t^{s-1} e^{ibt} e^{-at} \, dt = \frac{\Gamma(s)}{(a-ib)^s} = \frac{\Gamma(s)}{\left(\sqrt{a^2+b^2} e^{i\arctan \left(-\frac{b}{a} \right)} \right)^s} = \frac{\Gamma(s)}{(a^2+b^2)^{s/2}} e^{is \arctan \left(\frac{b}{a} \right)} ","['integration', 'definite-integrals', 'gamma-function']"
34,"Evaluate Integral $ I:=\int_0^{2\pi} \cos s \,\log (\sqrt{c^2 + a - 2 \cos s}-c) \, \mathrm d s $ for radially magnetized cylinder",Evaluate Integral  for radially magnetized cylinder," I:=\int_0^{2\pi} \cos s \,\log (\sqrt{c^2 + a - 2 \cos s}-c) \, \mathrm d s ","When trying to evaluate the magnetic scalar potential $\Phi_m$ of a magnetized cylinder (Magnetization $M$ in $x$ -direction, height $Z$ , Radius $R$ , touching the $xy$ -plane from below), I was able to solve two of the three integrals in cylinder coordinates, leaving one to give a completely closed form: $$\Phi_\mathrm m(\rho,\varphi,z) = \frac {M R} {4 \pi} \cos\varphi \left[\int_0^{2\pi} \cos \tilde \varphi \log \left( \sqrt{\zeta^2 + \rho^2 + R^2 - 2\rho R \cos\tilde \varphi}+\zeta  \right)\,\mathrm d \tilde\varphi\right]_{\zeta=z}^{\zeta=z+Z}   $$ Now I'm desprerately trying to solve the remaining integral, which can be written in the form $$ I:=\int_0^{2\pi} \cos s \,\log (\sqrt{c^2 + a - 2 \cos s}-c) \, \mathrm d s $$ for some numbers $ c:=\zeta /\sqrt{\rho R}\in\mathbb R $ and $ a:=\rho/R+R/\rho > 2 $ . Transforming this into a contour integral around the unit circle then reads $$ I= \oint_{\partial B_1(0)} (1+z^{-2})\log ( \sqrt {c^2 + a - (z+z^{-1})} - c) \, \mathrm d z  ,$$ but the various branch points for the square root and the log exceed my personal skills in arguing correctly about branch cuts and choosing a suitable integration contour. Maybe someone could tell, if there is any hope that this integral is solvable in closed form, or if one is stuck with numerical evaluations? Solutions involving special functions (elliptic integrals / Bessel functions / etc.) are definitely no problem. In my further attempts to aviod the difficulties with the complex $ \log $ function, there is yet another form (via substitutions), which might be solved by a suitable contour integration(?): $$ I = 2 \int_0^2 \frac {\sqrt{1-v^2} \,\mathrm d v}{(\sqrt{c^2+a-2v}-c) \sqrt {c^2+a-2v}}  $$","When trying to evaluate the magnetic scalar potential of a magnetized cylinder (Magnetization in -direction, height , Radius , touching the -plane from below), I was able to solve two of the three integrals in cylinder coordinates, leaving one to give a completely closed form: Now I'm desprerately trying to solve the remaining integral, which can be written in the form for some numbers and . Transforming this into a contour integral around the unit circle then reads but the various branch points for the square root and the log exceed my personal skills in arguing correctly about branch cuts and choosing a suitable integration contour. Maybe someone could tell, if there is any hope that this integral is solvable in closed form, or if one is stuck with numerical evaluations? Solutions involving special functions (elliptic integrals / Bessel functions / etc.) are definitely no problem. In my further attempts to aviod the difficulties with the complex function, there is yet another form (via substitutions), which might be solved by a suitable contour integration(?):","\Phi_m M x Z R xy \Phi_\mathrm m(\rho,\varphi,z) = \frac {M R} {4 \pi} \cos\varphi \left[\int_0^{2\pi} \cos \tilde \varphi \log \left( \sqrt{\zeta^2 + \rho^2 + R^2 - 2\rho R \cos\tilde \varphi}+\zeta  \right)\,\mathrm d \tilde\varphi\right]_{\zeta=z}^{\zeta=z+Z}     I:=\int_0^{2\pi} \cos s \,\log (\sqrt{c^2 + a - 2 \cos s}-c) \, \mathrm d s   c:=\zeta /\sqrt{\rho R}\in\mathbb R   a:=\rho/R+R/\rho > 2   I= \oint_{\partial B_1(0)} (1+z^{-2})\log ( \sqrt {c^2 + a - (z+z^{-1})} - c) \, \mathrm d z  ,  \log   I = 2 \int_0^2 \frac {\sqrt{1-v^2} \,\mathrm d v}{(\sqrt{c^2+a-2v}-c) \sqrt {c^2+a-2v}}  ","['integration', 'definite-integrals', 'contour-integration', 'electromagnetism']"
35,"Is this function bounded? $g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p})^t i^{-p} \, di =\varepsilon$",Is this function bounded?,"g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p})^t i^{-p} \, di =\varepsilon","Let $p>1,t>1,\varepsilon>0$ . Assuming the function below exists, when is it bounded? $$g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p} )^t i^{-p} \,di =\varepsilon$$ From simulations I suspect it's bounded for all $p>1$ and $\varepsilon>0$ , can this be shown analytically? For instance, for $p=2$ and $\varepsilon=0.01$ the graph obtained numerically","Let . Assuming the function below exists, when is it bounded? From simulations I suspect it's bounded for all and , can this be shown analytically? For instance, for and the graph obtained numerically","p>1,t>1,\varepsilon>0 g(n)=t>1\text{ s.t. } \int_1^n (1-i^{-p} )^t i^{-p} \,di =\varepsilon p>1 \varepsilon>0 p=2 \varepsilon=0.01","['integration', 'sequences-and-series', 'limits']"
36,proving an integral inequality_,proving an integral inequality_,,"I'm having problem proving the following integral inequality. Let $f$ be a continuous function on $[0,1]$ such that $f(0)=0$ and $f'(x)\geq1$ for all $x\in(0,1)$ . Show the following inequailties hold. $$\left( \int^{1}_{0} f(x) dx \right)^2 \leq \int ^{1} _{0} (f(x))^3 dx$$ I have tried using C-S inequalities of integrals, and also integration by parts, but failed. Please help thank you!","I'm having problem proving the following integral inequality. Let be a continuous function on such that and for all . Show the following inequailties hold. I have tried using C-S inequalities of integrals, and also integration by parts, but failed. Please help thank you!","f [0,1] f(0)=0 f'(x)\geq1 x\in(0,1) \left( \int^{1}_{0} f(x) dx \right)^2 \leq \int ^{1} _{0} (f(x))^3 dx","['calculus', 'integration', 'definite-integrals', 'integral-inequality']"
37,Integral of square root of a quartic polynomial,Integral of square root of a quartic polynomial,,"I have the following definite integral to solve $$\int_{a_1}^{a_2} dx \sqrt{(a_1-x)(x-a_2)(x-c)(x-\bar{c})},$$ where $a_1$ is real and negative, $a_2$ is real and positive and $c$ is a complex number and $\bar{c}$ its complex conjugate. I am sure that the result involves the eliptic functions and since the boundaries are fixed it can probably be reduced to only having complete eliptic integrals. I tried to use Formulas like 259.03 together with 341.05 and (for $R_1$ needed) 361.54. in the book https://link.springer.com/book/10.1007%2F978-3-642-65138-0 by Byrd and Freeman. In order to use this formula I rewrote the integral as $$\int_{a_1}^{a_2} dx \frac{P(x)}{\sqrt{P(x)}}, \quad\text{where}\, P(x) = (a_1-x)(x-a_2)(x-c)(x-\bar{c}).$$ However, I was unable to find a pleasing form and I was wondering whether there is a simpler approach than starting with the quite general formula in 259.03. For example I found Integrals of the square root of a cubic polynomial which unfortunately only covers the cubic case. Any ideas are highly appreciated!","I have the following definite integral to solve where is real and negative, is real and positive and is a complex number and its complex conjugate. I am sure that the result involves the eliptic functions and since the boundaries are fixed it can probably be reduced to only having complete eliptic integrals. I tried to use Formulas like 259.03 together with 341.05 and (for needed) 361.54. in the book https://link.springer.com/book/10.1007%2F978-3-642-65138-0 by Byrd and Freeman. In order to use this formula I rewrote the integral as However, I was unable to find a pleasing form and I was wondering whether there is a simpler approach than starting with the quite general formula in 259.03. For example I found Integrals of the square root of a cubic polynomial which unfortunately only covers the cubic case. Any ideas are highly appreciated!","\int_{a_1}^{a_2} dx \sqrt{(a_1-x)(x-a_2)(x-c)(x-\bar{c})}, a_1 a_2 c \bar{c} R_1 \int_{a_1}^{a_2} dx \frac{P(x)}{\sqrt{P(x)}}, \quad\text{where}\, P(x) = (a_1-x)(x-a_2)(x-c)(x-\bar{c}).","['integration', 'elliptic-integrals']"
38,Divergent integral,Divergent integral,,"Given $$ \int^\infty_0 \frac{\sin^\alpha \left(\frac{1}{x+1}\right)\arctan(x)}                    {(x^2+1)^\beta \tan\left(\frac{1}{x^2+1}\right)}dx  $$ find for which values of $\alpha$ and $\beta$ the integral is divergent. My approach to such integrals is to solve them by finding the correct asymptotic relations between the separate parts of it. In this particular integral, $\infty$ is surely a ""problem point"" so we get asymptotically equal functions as $x \to \infty$ such as: $\sin^\alpha\left(\frac{1}{x+1}\right) \sim \frac{1}{x+1} \sim \frac{1}{x}$ and so on with the rest. My problem here is that I'm not sure if $0$ is a ""problem point"" (such point that the function in it has ""undefined behavior"") and are there any other ""problem points"" in the $[0, \infty]$ interval?","Given find for which values of and the integral is divergent. My approach to such integrals is to solve them by finding the correct asymptotic relations between the separate parts of it. In this particular integral, is surely a ""problem point"" so we get asymptotically equal functions as such as: and so on with the rest. My problem here is that I'm not sure if is a ""problem point"" (such point that the function in it has ""undefined behavior"") and are there any other ""problem points"" in the interval?","
\int^\infty_0 \frac{\sin^\alpha \left(\frac{1}{x+1}\right)\arctan(x)}
                   {(x^2+1)^\beta \tan\left(\frac{1}{x^2+1}\right)}dx 
 \alpha \beta \infty x \to \infty \sin^\alpha\left(\frac{1}{x+1}\right) \sim \frac{1}{x+1} \sim \frac{1}{x} 0 [0, \infty]","['integration', 'convergence-divergence', 'trigonometric-integrals']"
39,Summation by parts as a case of integration by parts,Summation by parts as a case of integration by parts,,"I was wondering if the summation by parts formula $$ f_ng_n - f_mg_m = \sum_{k=m}^{n-1} f_k(g_{k+1}-g_k)  + \sum_{k=m}^{n-1}(f_{k+1}-f_k)g_{k+1} $$ could be proven as a consequence of integration by parts over a suitable domain $$ \int_{[m,n]} d(fg) = \int_{[m,n]}  fdg + \int_{[m,n]} gdf $$ and given some appropriate measure, for example the Dirac comb . I haven't done measure / distribution theory in a while, and I'm not even sure what $df$ is supposed to mean if I'm integrating something using a Dirac comb measure, or how the fundamental theorem of calculus looks in measure theory terms, so a bit more detailed explanation would be appreciated. I have found this similar post but the response does not quite answer my question.","I was wondering if the summation by parts formula could be proven as a consequence of integration by parts over a suitable domain and given some appropriate measure, for example the Dirac comb . I haven't done measure / distribution theory in a while, and I'm not even sure what is supposed to mean if I'm integrating something using a Dirac comb measure, or how the fundamental theorem of calculus looks in measure theory terms, so a bit more detailed explanation would be appreciated. I have found this similar post but the response does not quite answer my question.","
f_ng_n - f_mg_m = \sum_{k=m}^{n-1} f_k(g_{k+1}-g_k)  + \sum_{k=m}^{n-1}(f_{k+1}-f_k)g_{k+1}
 
\int_{[m,n]} d(fg) = \int_{[m,n]}  fdg + \int_{[m,n]} gdf
 df","['integration', 'measure-theory', 'dirac-delta', 'summation-by-parts']"
40,Evaluate $\int_{0}^{\pi/2}\cos^{2n}(x)\text{d}x$.,Evaluate .,\int_{0}^{\pi/2}\cos^{2n}(x)\text{d}x,"I try this. Notice that, $$ \begin{split} \cos^{2n}x &= \left(\frac{e^{ix}+e^{-ix}}{2}\right)^{2n} = \frac{1}{2^{2n}} \sum_{k=0}^{2n} \binom{2n}{k}e^{ikx}e^{-i(2n-k)x} \\ &= \frac{1}{2^{2n}} \sum_{k=0}^{2n} \binom{2n}{k}e^{i(2k-2n)x} \end{split} $$ The terms with $k\ne n$ integrate to zero over $[0,\pi/2]$ , and we are left with $$ \int_0^{\pi/2}\cos^{2n}x \,dx = \int_0^{\pi/2}\frac{1}{2^{2n}}\binom{2n}{n} \,dx = \frac{\pi}{2^{2n+1}}\binom{2n}{n} $$ I'm right or not?","I try this. Notice that, The terms with integrate to zero over , and we are left with I'm right or not?","
\begin{split}
\cos^{2n}x &= \left(\frac{e^{ix}+e^{-ix}}{2}\right)^{2n} = \frac{1}{2^{2n}} \sum_{k=0}^{2n} \binom{2n}{k}e^{ikx}e^{-i(2n-k)x} \\ &= \frac{1}{2^{2n}} \sum_{k=0}^{2n} \binom{2n}{k}e^{i(2k-2n)x} \end{split}
 k\ne n [0,\pi/2] 
\int_0^{\pi/2}\cos^{2n}x \,dx = \int_0^{\pi/2}\frac{1}{2^{2n}}\binom{2n}{n} \,dx = \frac{\pi}{2^{2n+1}}\binom{2n}{n}
",['integration']
41,Discrepancy with absolute value in evaluating a triple integral,Discrepancy with absolute value in evaluating a triple integral,,"Evaluate $$\iiint z^2 \,dx\,dy\,dz $$ over the region common to the sphere $$x^2 + y^2 + z^2 = a^2$$ and the cylinder $$x^2 + y^2 = ax.$$ Approach : working in cylindrical co-ordinates, the integral should simply boil down to:- $$\int_{-\pi / 2}^{\pi / 2} \int_0^{a\cos \phi} \int_{-\sqrt{a^2-s^2}}^{\sqrt{a^2-s^2}} z^2 \,dz\,d\phi$$ Which boils down to evaluating : $$ \frac 2 3 \int_{-\pi / 2}^{\pi / 2}d\phi\int_0^{a\cos \phi} (a^2-s^2)^{3/2}s\,ds $$ Let $(a^2-s^2)=u^2$ . Then, we get $$ \frac 2 3 \int_{-\pi / 2}^{\pi / 2}\,d\phi \int_{a\left|\sin\phi\right|}^a u^4\,du \tag{E01} $$ Notice the $|\cdot|$ in the lower limit. (this was done as a result of $\sqrt{x^2}= |x|$ while changing the limits).  This will be the root cause of the trouble that follows:- $$\frac{2a^5}{15} \int_{-\pi / 2}^{\pi / 2} (1-\left| \sin^5\phi \right|)\,d\phi = \frac{2a^5}{15} ( \pi - I)$$ Now, had the $| \cdot |$ not been present in $E01$ , then $I$ would be $0$ , since $\sin^5\phi$ is odd. So the answer would be $2\pi/15$ . But, the $|\cdot|$ means that $I$ would be something positive, which would mean the answer is something less than $2\pi/15$ . However, the given answer is: $2\pi/15$ . Which made me question the existence of the $|\cdot|$ . Seems to be rather trivial that the $|\cdot|$ should be there.....The second scope for an error could be in determining the limits of the integral...But that seems fine too. Where have I gone wrong then? Or do I just accept that the book ""forgot"" the $|\cdot|$ and I am indeed right?","Evaluate over the region common to the sphere and the cylinder Approach : working in cylindrical co-ordinates, the integral should simply boil down to:- Which boils down to evaluating : Let . Then, we get Notice the in the lower limit. (this was done as a result of while changing the limits).  This will be the root cause of the trouble that follows:- Now, had the not been present in , then would be , since is odd. So the answer would be . But, the means that would be something positive, which would mean the answer is something less than . However, the given answer is: . Which made me question the existence of the . Seems to be rather trivial that the should be there.....The second scope for an error could be in determining the limits of the integral...But that seems fine too. Where have I gone wrong then? Or do I just accept that the book ""forgot"" the and I am indeed right?","\iiint z^2 \,dx\,dy\,dz  x^2 + y^2 + z^2 = a^2 x^2 + y^2 = ax. \int_{-\pi / 2}^{\pi / 2} \int_0^{a\cos \phi} \int_{-\sqrt{a^2-s^2}}^{\sqrt{a^2-s^2}} z^2 \,dz\,d\phi  \frac 2 3 \int_{-\pi / 2}^{\pi / 2}d\phi\int_0^{a\cos \phi} (a^2-s^2)^{3/2}s\,ds  (a^2-s^2)=u^2  \frac 2 3 \int_{-\pi / 2}^{\pi / 2}\,d\phi \int_{a\left|\sin\phi\right|}^a u^4\,du \tag{E01}  |\cdot| \sqrt{x^2}= |x| \frac{2a^5}{15} \int_{-\pi / 2}^{\pi / 2} (1-\left| \sin^5\phi \right|)\,d\phi = \frac{2a^5}{15} ( \pi - I) | \cdot | E01 I 0 \sin^5\phi 2\pi/15 |\cdot| I 2\pi/15 2\pi/15 |\cdot| |\cdot| |\cdot|","['calculus', 'integration', 'multivariable-calculus']"
42,"Polynomial integral calculation, maybe residue theorem","Polynomial integral calculation, maybe residue theorem",,"When evaluating a loop correction for a certain field theory, I arrived at the following integral $$\int\limits_{\mathbb{R}^{1,3}}\frac{d^4 k}{(2\pi)^4}\frac{1}{\left((k^0)^2-\dfrac{|k|^4}{M^2}-m^2+i\epsilon\right)^2}$$ where $d^4 k = dk^0\,d^3k$ and the $i\epsilon$ prescription is for the definition of the integration path for the propagator. I'm a bit rusty when it comes to evaluating integrals like this and my first thought was to use the residue theorem but I don't really know where to start since one pole is on the upper half plane and the other on the lower half plane! If there's even some better way of doing it beside using the residue theorem, I'll gladly have a look at them! Edit I intended to use the residue theorem first on $k^0$ and then, if necessary, on $k$ . I thought even about Wick rotating to simplify the problem but still I don't really know where to start.","When evaluating a loop correction for a certain field theory, I arrived at the following integral where and the prescription is for the definition of the integration path for the propagator. I'm a bit rusty when it comes to evaluating integrals like this and my first thought was to use the residue theorem but I don't really know where to start since one pole is on the upper half plane and the other on the lower half plane! If there's even some better way of doing it beside using the residue theorem, I'll gladly have a look at them! Edit I intended to use the residue theorem first on and then, if necessary, on . I thought even about Wick rotating to simplify the problem but still I don't really know where to start.","\int\limits_{\mathbb{R}^{1,3}}\frac{d^4 k}{(2\pi)^4}\frac{1}{\left((k^0)^2-\dfrac{|k|^4}{M^2}-m^2+i\epsilon\right)^2} d^4 k = dk^0\,d^3k i\epsilon k^0 k","['integration', 'definite-integrals', 'physics']"
43,Fourier Transform - Dirac Delta,Fourier Transform - Dirac Delta,,"How can I compute this Fourier transform? $$\int \frac{d^3q}{(2\pi)^3}(\vec q \cdot \vec a)(\vec q \cdot \vec b)\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }\,$$ My idea was to write it as $$-( \vec a \cdot \vec \nabla)(\vec b \cdot \vec \nabla)\int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }$$ and use the fact that $\int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }=\frac{1}{4\pi r}$ , together with $\Delta \frac{1}{r} = -4\,\pi \,\delta(\vec r)$ but is is not exactly the same.","How can I compute this Fourier transform? My idea was to write it as and use the fact that , together with but is is not exactly the same.","\int \frac{d^3q}{(2\pi)^3}(\vec q \cdot \vec a)(\vec q \cdot \vec b)\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }\, -( \vec a \cdot \vec \nabla)(\vec b \cdot \vec \nabla)\int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r } \int \frac{d^3q}{(2\pi)^3}\frac{1}{q^2}\,e^{i \vec q \cdot \vec r }=\frac{1}{4\pi r} \Delta \frac{1}{r} = -4\,\pi \,\delta(\vec r)","['integration', 'fourier-transform', 'dirac-delta']"
44,Is convolution area-preserving?,Is convolution area-preserving?,,"Consider two functions f(x) and g(x) with indefinite integrals (""area under the curve"") A f and A g . Does the convolution f * g preserve the area, i.e. is A f * g = A f * A g i.e. ∫ ( ∫f(x)g(t−x)dx ) dt = (∫f(x)dx) (∫g(x)dx) ? I've learned that convolution in one space is multiplication in the dual space, or more precisely, that direct multiplication in one space corresponds to polynomial multiplication in the dual space. There is also a nice, short video where this intuition is explained, but I'm having difficulty transferring this to the question regarding areas and drawing the right conclusions. To provide some practical context: Image formation in an optical system can be described as Image = Source * PSF with PSF being the point spread function describing the system's impulse response. The PSF is often approximated by a Gaussian and the convolution results in a Gaussian blur. If the Gaussian is normalized to unit area (i.e. a normal distribution), does this equate to the fact that the integrated intensities of source and image are preserved? Meaning that the photons are merely ""redistributed"" (blurring process), but the total photon number is preserved (= energy conservation in an ideal, lossless system)? Conversely, can I add e.g. a 10% loss of photons due to absorption by simply changing the normalization of the PSF from 1.0 to 0.9 (rescaling the Gaussian's normalization factor)?","Consider two functions f(x) and g(x) with indefinite integrals (""area under the curve"") A f and A g . Does the convolution f * g preserve the area, i.e. is A f * g = A f * A g i.e. ∫ ( ∫f(x)g(t−x)dx ) dt = (∫f(x)dx) (∫g(x)dx) ? I've learned that convolution in one space is multiplication in the dual space, or more precisely, that direct multiplication in one space corresponds to polynomial multiplication in the dual space. There is also a nice, short video where this intuition is explained, but I'm having difficulty transferring this to the question regarding areas and drawing the right conclusions. To provide some practical context: Image formation in an optical system can be described as Image = Source * PSF with PSF being the point spread function describing the system's impulse response. The PSF is often approximated by a Gaussian and the convolution results in a Gaussian blur. If the Gaussian is normalized to unit area (i.e. a normal distribution), does this equate to the fact that the integrated intensities of source and image are preserved? Meaning that the photons are merely ""redistributed"" (blurring process), but the total photon number is preserved (= energy conservation in an ideal, lossless system)? Conversely, can I add e.g. a 10% loss of photons due to absorption by simply changing the normalization of the PSF from 1.0 to 0.9 (rescaling the Gaussian's normalization factor)?",,"['integration', 'improper-integrals', 'area', 'convolution']"
45,When are definite integrals of continuous functions pairwise distinct?,When are definite integrals of continuous functions pairwise distinct?,,"Let $f_1, \ldots, f_n : [0,1]\rightarrow\mathbb{R}_{>0}$ be a finite family of (positive) continuous functions. I was wondering about the weakest condition to impose on the $f_i$ so as to  guarantee that $$\tag{1}\exists\, \text 0\leq s < t \leq 1 \quad \text{ s.t. } \ \text{ the numbers} \quad q_i(s,t):=\int_s^t\!f_i(u)\,\mathrm{d}u, \ \ i=1,\ldots, n, \quad \text{ are pairwise distinct}? $$ (In case someone's in doubt: This is not a homework question.)",Let be a finite family of (positive) continuous functions. I was wondering about the weakest condition to impose on the so as to  guarantee that (In case someone's in doubt: This is not a homework question.),"f_1, \ldots, f_n : [0,1]\rightarrow\mathbb{R}_{>0} f_i \tag{1}\exists\, \text 0\leq s < t \leq 1 \quad \text{ s.t. } \ \text{ the numbers} \quad q_i(s,t):=\int_s^t\!f_i(u)\,\mathrm{d}u, \ \ i=1,\ldots, n, \quad \text{ are pairwise distinct}? ","['real-analysis', 'calculus', 'integration']"
46,"Why is $ \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right) \neq \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) $",Why is," \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right) \neq \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) ","Known Definition : $$ \tan{\left(\frac{\pi x}{2}\right)} = \frac{4}{\pi} \sum_{n=0}^{\infty} \frac{x}{(2n+1)^{2}-x^2} $$ I integrate both sides with respect to $x$ to obtain : $$ \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right)= \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) $$ However, this appears to NOT be true due to the series on the RHS diverging, why does this occur? and where am I making the mistake? Thank you kindly for your help and time.","Known Definition : I integrate both sides with respect to to obtain : However, this appears to NOT be true due to the series on the RHS diverging, why does this occur? and where am I making the mistake? Thank you kindly for your help and time."," \tan{\left(\frac{\pi x}{2}\right)} = \frac{4}{\pi} \sum_{n=0}^{\infty} \frac{x}{(2n+1)^{2}-x^2}  x  \ln\left(\cos{\left(\frac{\pi x}{2}\right)}\right)= \sum_{n=0}^{\infty} \ln(\,(2n+1)^{2}-x^2) ","['integration', 'sequences-and-series']"
47,"$\frac{1}{2r}\int_{x-r}^{x+r}f(t)dt \to g(x)$ uniformly when $r\rightarrow \infty$, then $g(x)=ax+b$","uniformly when , then",\frac{1}{2r}\int_{x-r}^{x+r}f(t)dt \to g(x) r\rightarrow \infty g(x)=ax+b,"Suppose $f$ is a Continuous function on $\Bbb R$ and $g: \Bbb R \to \Bbb R$ a function such that $$ \frac{1}{2r}\int_{x-r}^{x+r}f(t)dt \to g(x) \text{ uniformly }$$ when $r\rightarrow \infty$ , then show that $g(x)=ax+b$ for some $a,b \in \Bbb R$ . I really have no idea on how to approach the problem. Thanks in advance for help!","Suppose is a Continuous function on and a function such that when , then show that for some . I really have no idea on how to approach the problem. Thanks in advance for help!","f \Bbb R g: \Bbb R \to \Bbb R  \frac{1}{2r}\int_{x-r}^{x+r}f(t)dt \to g(x) \text{ uniformly } r\rightarrow \infty g(x)=ax+b a,b \in \Bbb R","['real-analysis', 'integration', 'functional-analysis', 'uniform-convergence']"
48,Prove $\int_{0}^{+\infty}\frac{1}{f(x)}dx$ is convergent when $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx$ is convergent,Prove  is convergent when  is convergent,\int_{0}^{+\infty}\frac{1}{f(x)}dx \int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx,"Suppose $f(x)$ is positive monotone increasing function over $[0,\infty)$ , and it has derivative. Prove: if $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx$ is convergent, then $\int_{0}^{+\infty}\frac{1}{f(x)}dx$ is convergent. My work: $\int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}=\int_{0}^{\infty}\frac{1}{f(x)+f'(x)}\Bbb{dx}$ , but since $f'(x)>0$ I do not know what to do next.","Suppose is positive monotone increasing function over , and it has derivative. Prove: if is convergent, then is convergent. My work: , but since I do not know what to do next.","f(x) [0,\infty) \int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}dx \int_{0}^{+\infty}\frac{1}{f(x)}dx \int_{0}^{+\infty}\frac{e^x}{(e^xf(x))'}=\int_{0}^{\infty}\frac{1}{f(x)+f'(x)}\Bbb{dx} f'(x)>0","['integration', 'analysis', 'convergence-divergence']"
49,"Integrate $\int_0^{2\pi}\frac{\ln(a + b\cos x)}{c + d\cos x} dx$, Residue theorem","Integrate , Residue theorem",\int_0^{2\pi}\frac{\ln(a + b\cos x)}{c + d\cos x} dx,"I have recently been given a challenge problem in my Complex Analysis class. Suppose $a > b > 0$ and $c > d > 0$ . Evaluate $$\int_0^{2\pi} \frac{\ln(a + b\cos x)}{c + d\cos x} dx$$ using the Residue Theorem. Unfortunately, I don't even know where to begin with this one. I have managed to solve the integral where the integrand is $$\frac{a + b\cos x}{c + d\cos x}$$ where the contour I used was the usual a square but I'm not sure whether that can be applied here. If anyone could provide any assistance, that would be greatly appreciated!","I have recently been given a challenge problem in my Complex Analysis class. Suppose and . Evaluate using the Residue Theorem. Unfortunately, I don't even know where to begin with this one. I have managed to solve the integral where the integrand is where the contour I used was the usual a square but I'm not sure whether that can be applied here. If anyone could provide any assistance, that would be greatly appreciated!",a > b > 0 c > d > 0 \int_0^{2\pi} \frac{\ln(a + b\cos x)}{c + d\cos x} dx \frac{a + b\cos x}{c + d\cos x},"['integration', 'complex-analysis', 'residue-calculus']"
50,Is this function lebesgue integrable or not?,Is this function lebesgue integrable or not?,,I'm trying to see if this function is lebesgue integrable. $$\int_0^1 \frac{(-1)^{\lfloor 1/x \rfloor}}{x^2} dx.$$ How can I prove it? I try the following: Let $f(x)=\frac{(-1)^{\lfloor 1/x \rfloor}}{x^2}$ . \begin{align*} \int_0^1 |f(x)| dx&=\sum_{n=1}^{\infty} \int_{1/(n+1)}^{1/n} |f(x)| dx\\ &=\sum_{n=1}^{\infty} \int_{1/(n+1)}^{1/n} \frac{1}{x^2} dx\\ &=\sum_{n=1}^{\infty} \left(\frac{-1}{n}+\frac{1}{n+1}\right)<\infty. \end{align*} Thus $f(x)$ is L.I. I'm wrong?,I'm trying to see if this function is lebesgue integrable. How can I prove it? I try the following: Let . Thus is L.I. I'm wrong?,"\int_0^1 \frac{(-1)^{\lfloor 1/x \rfloor}}{x^2} dx. f(x)=\frac{(-1)^{\lfloor 1/x \rfloor}}{x^2} \begin{align*}
\int_0^1 |f(x)| dx&=\sum_{n=1}^{\infty} \int_{1/(n+1)}^{1/n} |f(x)| dx\\
&=\sum_{n=1}^{\infty} \int_{1/(n+1)}^{1/n} \frac{1}{x^2} dx\\
&=\sum_{n=1}^{\infty} \left(\frac{-1}{n}+\frac{1}{n+1}\right)<\infty.
\end{align*} f(x)","['real-analysis', 'integration', 'lebesgue-integral', 'solution-verification']"
51,Prove by definition that $ \int_a^b fdf = {f^2(b)-f^2(a) \over{2}}$ when $f$ is continuous,Prove by definition that  when  is continuous, \int_a^b fdf = {f^2(b)-f^2(a) \over{2}} f,"Let $f :[a,b] \rightarrow \mathbb{R}$ be a continuous function. Prove that $f$ is Riemann Stieltjes integral with respect to itself that is: $f\in RS_a^b(f)$ by definition and $ \int_a^b fdf = {f^2(b)-f^2(a) \over{2}}$ I can't use the Cauchy criterion nor integration by parts to solve this problem: My definition: Let $f,g:[a,b]\rightarrow \mathbb{R}$ bounded functions. $f$ is Riemann Stieltjes integrable with respect to $g$ iff there exists a real number $I$ such that $\forall \epsilon > 0$ there exists a partition $P_{\epsilon}$ in $[a,b]$ such that for every other partion $P$ finer than $P_\epsilon$ , $|S(P,f,g)-I|<\epsilon$ for every choice of numbers $c_i \in [x_{i-1},x_i]$ where $S(P,f,g)=\sum_{i=1}^nf(c_i)(g(x_i)-g(x_{i-1}))$ . In this case we define $\int_a^bfdg = I$ My attempt: Let $\epsilon > 0$ . By uniform continuity of $f$ (because $f$ is continuous on a compact set) there exists $\delta_\epsilon > 0$ such that for every $x,y \in [a,b]$ and $|x-y| < \delta$ then $|f(x)-f(y)|< \epsilon$ . We can construct a partition $P_{\delta_\epsilon}$ so that $||P_{\delta_\epsilon}|| < \delta_\epsilon$ . Take any other partition $P$ finer thatn $P_{\delta_\epsilon}$ and any choice of numbers $c_i\in [x_{i-1},x_i]$ subinterval of the partition $P$ ; then: \begin{align} |S(P,f,f)-{f^2(b)-f^2(a) \over{2}}| &= \frac{1}{2}|2 \sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1}))-f^2(b)+f^2(a)| \\ &= \frac{1}{2}|\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\  &= \frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1})+f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i)+f(x_i))(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\  &=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(x_{i-1})(f(x_{i})-f(x_{i-1})) +\sum_{i=1}^nf(x_{i})(f(x_{i})-f(x_{i-1}))-f^2(b)+f^2(a)| \\  &=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1}))| \\  &<\frac{1}{2}(\sum_{i=1}^n\epsilon^2 +\sum_{i=1}^n\epsilon^2) \\ &= \epsilon^2(n) \end{align} The problem is that the last part depends on $n$ so I can´t conclude that this is less than $\epsilon$ because $n$ depends on the partion $P$ . But I don´t know how to solve this part. I would really appreciate any hints or suggestions with this problem.","Let be a continuous function. Prove that is Riemann Stieltjes integral with respect to itself that is: by definition and I can't use the Cauchy criterion nor integration by parts to solve this problem: My definition: Let bounded functions. is Riemann Stieltjes integrable with respect to iff there exists a real number such that there exists a partition in such that for every other partion finer than , for every choice of numbers where . In this case we define My attempt: Let . By uniform continuity of (because is continuous on a compact set) there exists such that for every and then . We can construct a partition so that . Take any other partition finer thatn and any choice of numbers subinterval of the partition ; then: The problem is that the last part depends on so I can´t conclude that this is less than because depends on the partion . But I don´t know how to solve this part. I would really appreciate any hints or suggestions with this problem.","f :[a,b] \rightarrow \mathbb{R} f f\in RS_a^b(f)  \int_a^b fdf = {f^2(b)-f^2(a) \over{2}} f,g:[a,b]\rightarrow \mathbb{R} f g I \forall \epsilon > 0 P_{\epsilon} [a,b] P P_\epsilon |S(P,f,g)-I|<\epsilon c_i \in [x_{i-1},x_i] S(P,f,g)=\sum_{i=1}^nf(c_i)(g(x_i)-g(x_{i-1})) \int_a^bfdg = I \epsilon > 0 f f \delta_\epsilon > 0 x,y \in [a,b] |x-y| < \delta |f(x)-f(y)|< \epsilon P_{\delta_\epsilon} ||P_{\delta_\epsilon}|| < \delta_\epsilon P P_{\delta_\epsilon} c_i\in [x_{i-1},x_i] P \begin{align}
|S(P,f,f)-{f^2(b)-f^2(a) \over{2}}| &= \frac{1}{2}|2 \sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1}))-f^2(b)+f^2(a)| \\
&= \frac{1}{2}|\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(c_i)(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\ 
&= \frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1})+f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i)+f(x_i))(f(x_i)-f(x_{i-1})) -f^2(b)+f^2(a)| \\ 
&=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^nf(x_{i-1})(f(x_{i})-f(x_{i-1})) +\sum_{i=1}^nf(x_{i})(f(x_{i})-f(x_{i-1}))-f^2(b)+f^2(a)| \\ 
&=\frac{1}{2}|\sum_{i=1}^n(f(c_i)-f(x_{i-1}))(f(x_i)-f(x_{i-1})) +\sum_{i=1}^n(f(c_i)-f(x_i))(f(x_i)-f(x_{i-1}))| \\ 
&<\frac{1}{2}(\sum_{i=1}^n\epsilon^2 +\sum_{i=1}^n\epsilon^2) \\
&= \epsilon^2(n)
\end{align} n \epsilon n P","['real-analysis', 'integration', 'continuity', 'riemann-integration', 'stieltjes-integral']"
52,"Definite integration of trig functions: $\int_{0}^{\infty} \frac{\sin 2020x}{x} \prod_{k=1}^{n} \cos(kx) \, \mathrm d x$",Definite integration of trig functions:,"\int_{0}^{\infty} \frac{\sin 2020x}{x} \prod_{k=1}^{n} \cos(kx) \, \mathrm d x","Given $$\int_{0}^{\infty} \frac{\sin x}{x} \operatorname d x=\frac{\pi}{2}$$ Find the natural values of $n$ which satisfy , $$\int_{0}^{\infty} \cos x \cdot \cos 2 x\cdot  \cos 3 x\cdots\cos n x \cdot \frac{\sin 2020 x}{x} d x= \frac{\pi}{2}$$ My approach: Consider the given integral as $I(n)$ . Now if I find values of $I(n)$ corresponding to $n=1,2,3\dots$ I am getting value $\pi / 2$ . So I can see natural values of $n$ less than $2020 $ satisfy the equation. I don't know the answer to the problem. Is there some elegant approach to solve this question?","Given Find the natural values of which satisfy , My approach: Consider the given integral as . Now if I find values of corresponding to I am getting value . So I can see natural values of less than satisfy the equation. I don't know the answer to the problem. Is there some elegant approach to solve this question?","\int_{0}^{\infty} \frac{\sin x}{x} \operatorname d x=\frac{\pi}{2} n \int_{0}^{\infty} \cos x \cdot \cos 2 x\cdot  \cos 3 x\cdots\cos n x \cdot \frac{\sin 2020 x}{x} d x= \frac{\pi}{2} I(n) I(n) n=1,2,3\dots \pi / 2 n 2020 ","['integration', 'definite-integrals', 'improper-integrals']"
53,Swapping integral and sum using dominated convergence theorem,Swapping integral and sum using dominated convergence theorem,,"Show that $$ \sum_{n=1}^\infty \int_0^\infty t^{s/2-1}e^{-\pi n^2t}dt  = \int_0^\infty t^{s/2-1} \sum_{n=1}^\infty e^{-\pi n^2t} dt $$ with $s > 1$ using the dominated convergence theorem. If I have understood correctly, I can define $f_k:= \sum_{n=1}^k t^{s/2-1}e^{-\pi n^2t}$ which I have already been able to show integrable and that it converges pointwise ( $f = \lim_{k \rightarrow \infty} f_k$ ). Now the last part of the requirements of the theorem: I struggle to find an integrable function $g$ with $|f_k| \leq g$ for all $k \in \mathbb{N}$ .","Show that with using the dominated convergence theorem. If I have understood correctly, I can define which I have already been able to show integrable and that it converges pointwise ( ). Now the last part of the requirements of the theorem: I struggle to find an integrable function with for all .","
\sum_{n=1}^\infty \int_0^\infty t^{s/2-1}e^{-\pi n^2t}dt
 = \int_0^\infty t^{s/2-1} \sum_{n=1}^\infty e^{-\pi n^2t} dt
 s > 1 f_k:= \sum_{n=1}^k t^{s/2-1}e^{-\pi n^2t} f = \lim_{k \rightarrow \infty} f_k g |f_k| \leq g k \in \mathbb{N}","['real-analysis', 'integration', 'summation', 'lebesgue-integral']"
54,Using the complex definition of $\sin$ to solve an integral,Using the complex definition of  to solve an integral,\sin,"Is it possible to use the complex definition of trigonometric identities to simplify integrals? For instance, the integral: $$\int e^{-at}\cos(bt) \, dt$$ Would it be appropriate to use the definition of complex sine and then convert it back after the integration, versus using integration by parts?","Is it possible to use the complex definition of trigonometric identities to simplify integrals? For instance, the integral: Would it be appropriate to use the definition of complex sine and then convert it back after the integration, versus using integration by parts?","\int e^{-at}\cos(bt) \, dt","['integration', 'complex-numbers']"
55,Defining an Antiderivative for Monotone Functions,Defining an Antiderivative for Monotone Functions,,"I'm reading through a paper and I'm having trouble following the logic of the following step. ""Suppose $\psi:(0,\infty) \to [0,\infty)$ is a non-negative, non-decreasing function. Let $\Psi$ be the primitive function of $\psi$ , i.e. $\Psi' = \psi$ ."" Can we actually do this? I believe monotonicity of $\psi$ means that an antiderivative can indeed be defined, and would be continuous, but wouldn't it only be differentiable almost everywhere? In particular, at points of discontinuity of $\psi$ (the set of which I believe will have measure zero, again by monotonicity) we cannot say $\Psi' = \psi$ ? In summary, is it instead true that $\Psi' = \psi$ a.e.? Thank you for your time!","I'm reading through a paper and I'm having trouble following the logic of the following step. ""Suppose is a non-negative, non-decreasing function. Let be the primitive function of , i.e. ."" Can we actually do this? I believe monotonicity of means that an antiderivative can indeed be defined, and would be continuous, but wouldn't it only be differentiable almost everywhere? In particular, at points of discontinuity of (the set of which I believe will have measure zero, again by monotonicity) we cannot say ? In summary, is it instead true that a.e.? Thank you for your time!","\psi:(0,\infty) \to [0,\infty) \Psi \psi \Psi' = \psi \psi \psi \Psi' = \psi \Psi' = \psi","['real-analysis', 'integration', 'lebesgue-integral', 'monotone-functions']"
56,Absolute Value of Integrals,Absolute Value of Integrals,,"It is clear that $\left|\displaystyle\int fd\mu\right|\leq\displaystyle\int|f|d\mu$ in any general setting of measure space $(X,\Sigma,\mu)$ . But it is necessary that, for any $\delta>0$ , there exists some $A_{\delta}\in\Sigma$ such that $\left|\displaystyle\int_{A_{\delta}}fd\mu\right|>(1-\delta)\displaystyle\int_{X}|f|d\mu$ ? At least I think this is true in Euclidean Lebesgue integrals, as I have seen some authors take it for granted. One may have to choose $A_{\delta}$ to be ""switching"" the negative part of $f$ to the positive one to do the job, but I am struggling to make it. Any idea? Edit: So how does the author manage to choose such a $B_{n}$ to make the integral greater than the $1/4$ of the absolute integrand one?","It is clear that in any general setting of measure space . But it is necessary that, for any , there exists some such that ? At least I think this is true in Euclidean Lebesgue integrals, as I have seen some authors take it for granted. One may have to choose to be ""switching"" the negative part of to the positive one to do the job, but I am struggling to make it. Any idea? Edit: So how does the author manage to choose such a to make the integral greater than the of the absolute integrand one?","\left|\displaystyle\int fd\mu\right|\leq\displaystyle\int|f|d\mu (X,\Sigma,\mu) \delta>0 A_{\delta}\in\Sigma \left|\displaystyle\int_{A_{\delta}}fd\mu\right|>(1-\delta)\displaystyle\int_{X}|f|d\mu A_{\delta} f B_{n} 1/4","['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
57,Computing the integral of the following piece wise function,Computing the integral of the following piece wise function,,"Compute the integral of $$f(t)=\begin{cases}1, \text{ if $t$ is rational}\\0,\text{ otherwise}\end{cases}$$ on $(0,1)$ . The way I approached this problem was using the upper Darboux integrals, note that $\displaystyle{\sup_{x\in(0,1)}}\{f(t)\}=1$ and $\displaystyle{\inf_{x\in(0,1)}}\{f(t)\}=0$ , thus $$\overline{\int_{0}^1}f(t)\,dt=1\quad\text{and}\quad \underline{\int_{0}^1}f(t)\,dt=0$$ thus the integral does not exist. Is this correct ?","Compute the integral of on . The way I approached this problem was using the upper Darboux integrals, note that and , thus thus the integral does not exist. Is this correct ?","f(t)=\begin{cases}1, \text{ if t is rational}\\0,\text{ otherwise}\end{cases} (0,1) \displaystyle{\sup_{x\in(0,1)}}\{f(t)\}=1 \displaystyle{\inf_{x\in(0,1)}}\{f(t)\}=0 \overline{\int_{0}^1}f(t)\,dt=1\quad\text{and}\quad \underline{\int_{0}^1}f(t)\,dt=0","['real-analysis', 'integration']"
58,"Convolution of $f(x)={1 \over 2} \chi_{[-1,1]}*\chi_{[-5,5]}$",Convolution of,"f(x)={1 \over 2} \chi_{[-1,1]}*\chi_{[-5,5]}","Convolution of $f(x)={1 \over 2} \chi_{[-1,1]}*\chi_{[-5,5]}$ . This is my first exercise, it's basically an interpolation between the two functions. So here my results: Calling $u={1 \over 2} \chi_{[-1,1]}$ and $v=\chi_{[-5,5]}$ I can calculate the different values: $u*v(0)={ 3 \over 2}$ $u*v({1 \over 2})={ 3 \over 2}$ $u*v(-{1 \over 2})={ 3 \over 2}$ So for the borders, we have: $0$ for $x<-5,x>5$ $1$ for $x \geq -5,x \leq -{ 3\over 4}$ $-x+{ 3 \over 4}$ for $-{ 3\over 4} \leq x \leq -{ 1 \over 4}$ ${ 3 \over 2}$ for $-{ 1 \over 4} \leq x \leq { 1 \over 4}$ $x-{ 3 \over 4}$ for ${ 1 \over 4} \leq x \leq { 3 \over 4}$ $1$ for $x \geq { 3 \over 4},x\leq5$ I hope it's right (I can't draw the graphic here); if there is some error or not rigorous passage, please let me know I want to be capable to solve this at the best of possibilities","Convolution of . This is my first exercise, it's basically an interpolation between the two functions. So here my results: Calling and I can calculate the different values: So for the borders, we have: for for for for for for I hope it's right (I can't draw the graphic here); if there is some error or not rigorous passage, please let me know I want to be capable to solve this at the best of possibilities","f(x)={1 \over 2} \chi_{[-1,1]}*\chi_{[-5,5]} u={1 \over 2} \chi_{[-1,1]} v=\chi_{[-5,5]} u*v(0)={ 3 \over 2} u*v({1 \over 2})={ 3 \over 2} u*v(-{1 \over 2})={ 3 \over 2} 0 x<-5,x>5 1 x \geq -5,x \leq -{ 3\over 4} -x+{ 3 \over 4} -{ 3\over 4} \leq x \leq -{ 1 \over 4} { 3 \over 2} -{ 1 \over 4} \leq x \leq { 1 \over 4} x-{ 3 \over 4} { 1 \over 4} \leq x \leq { 3 \over 4} 1 x \geq { 3 \over 4},x\leq5","['real-analysis', 'integration', 'convolution']"
59,I found a theorem? Is this consistent with the change of variables formula?,I found a theorem? Is this consistent with the change of variables formula?,,"Let $f$ be a ${C}^{\infty}$ function defined on ${\mathbb{R}}^{2} $ . I considered the following function, $G$ and I probably found the following Theorem. However, I feel that something is wrong. $G(t,s):={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} f({u}_{1}\textbf{a}+{u}_{2}\textbf{b})\ d{u}_{1}d{u}_{2}$ Theorem? Let $f:{\mathbb{R}}^{2}\to {\mathbb{R}}$ be ${C}^{\infty}$ function, $\textbf{a},\textbf{b}\in {\mathbb{R}}^{2}$ :   These are linearly independent, $t,s\in\mathbb{R}$ , and G and g are defined as follows. $\ \ g(t,s):=f(t\textbf{a} + s\textbf{b})$ $\ \ G(t,s):={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} f({u}_{1}\textbf{a}+{u}_{2}\textbf{b})\ d{u}_{1}d{u}_{2}$ Then, $\ $ (1) $\frac{\partial^2 G}{\partial t\partial s}(t,s)=f(t\textbf{a}+s\textbf{b})$ $\ $ (2) $\frac{\partial^2 g}{\partial t\partial s}(t,s)={}^{T}\textbf{a}(Hf)_{(t\textbf{a}+s\textbf{b})}\textbf{b}$ $\ $ (3) $f(t\textbf{a} + s\textbf{b})={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t}  \ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b} \ d{u}_{1}d{u}_{2}$ Here, ${}^{T}\textbf{a}$ is the    Transpose vector of $\textbf{a}$ , and $(Hf)$ is Hessian matrix of $f$ . Proof of (1)? $G(t,s)={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}$ and, according to Fubini's theorem, the following is correct: $$g(t,s)=\frac{\partial^2}{\partial t\partial s}{\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2} =\frac{\partial^2 G}{\partial t\partial s}(t,s) ,$$ and $g(t,s):=f(t\textbf{a} + s\textbf{b})$ . Therefore, $$\frac{\partial^2 G}{\partial t\partial s}(t,s)=f(t\textbf{a} + s\textbf{b}).$$ Proof of (2)? $$\frac{\partial{g}}{\partial{t}}(t,s) = \left\langle gradf(t\textbf{a}+s\textbf{b})|\textbf{a}\right\rangle, $$ and $$\frac{\partial}{\partial s} (gradf(t\textbf{a}+s\textbf{b})) =(\frac{\partial gradf}{\partial x}(t\textbf{a}+s\textbf{b}), \frac{\partial gradf}{\partial y}(t\textbf{a}+s\textbf{b}) )\cdot\textbf{b} =(Hf)_{(t\textbf{a}+s\textbf{b})}\cdot\textbf{b}. $$ Here, $gradf$ is the gradient vector of $f$ , and $\left\langle \ \ |\ \ \right\rangle$ is dot product of $\mathbb{R}^2$ . Therefore, $$\frac{\partial^2 g}{\partial t\partial s}(t,s)={}^{T}\textbf{a}(Hf)_{(t\textbf{a}+s\textbf{b})}\textbf{b}\ \ $$ ■ Proof of (3)? Differentiate both sides of the following expression. $$G(t,s)={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}$$ Differentiation and integration are interchangeable. Therefore, considering (2) $$\frac{\partial^2}{\partial t\partial s}G(t,s) =\frac{\partial^2 }{\partial t\partial s}{\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}$$ $$={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} \frac{\partial^2 }{\partial t\partial s}g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t}  \ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b} \ d{u}_{1}d{u}_{2}$$ On the other hand, considering (1), The left side of the above formula is: $$f(t\textbf{a} + s\textbf{b}) = \frac{\partial^2}{\partial t\partial s}G(t,s)$$ Therefore, $$f(t\textbf{a} + s\textbf{b})  ={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t}  \ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b} \ d{u}_{1}d{u}_{2}$$ ■ My question Are these Theorem? (1)-(3) correct? If it is correct, is it consistent with the variable conversion formula? P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions.","Let be a function defined on . I considered the following function, and I probably found the following Theorem. However, I feel that something is wrong. Theorem? Let be function, :   These are linearly independent, , and G and g are defined as follows. Then, (1) (2) (3) Here, is the    Transpose vector of , and is Hessian matrix of . Proof of (1)? and, according to Fubini's theorem, the following is correct: and . Therefore, Proof of (2)? and Here, is the gradient vector of , and is dot product of . Therefore, ■ Proof of (3)? Differentiate both sides of the following expression. Differentiation and integration are interchangeable. Therefore, considering (2) On the other hand, considering (1), The left side of the above formula is: Therefore, ■ My question Are these Theorem? (1)-(3) correct? If it is correct, is it consistent with the variable conversion formula? P.S. I'm not very good at English, so I'm sorry if I have some impolite or unclear expressions.","f {C}^{\infty} {\mathbb{R}}^{2}  G G(t,s):={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} f({u}_{1}\textbf{a}+{u}_{2}\textbf{b})\ d{u}_{1}d{u}_{2} f:{\mathbb{R}}^{2}\to {\mathbb{R}} {C}^{\infty} \textbf{a},\textbf{b}\in {\mathbb{R}}^{2} t,s\in\mathbb{R} \ \ g(t,s):=f(t\textbf{a} + s\textbf{b}) \ \ G(t,s):={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} f({u}_{1}\textbf{a}+{u}_{2}\textbf{b})\ d{u}_{1}d{u}_{2} \  \frac{\partial^2 G}{\partial t\partial s}(t,s)=f(t\textbf{a}+s\textbf{b}) \  \frac{\partial^2 g}{\partial t\partial s}(t,s)={}^{T}\textbf{a}(Hf)_{(t\textbf{a}+s\textbf{b})}\textbf{b} \  f(t\textbf{a} + s\textbf{b})={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} 
\ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b}
\ d{u}_{1}d{u}_{2} {}^{T}\textbf{a} \textbf{a} (Hf) f G(t,s)={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2} g(t,s)=\frac{\partial^2}{\partial t\partial s}{\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}
=\frac{\partial^2 G}{\partial t\partial s}(t,s)
, g(t,s):=f(t\textbf{a} + s\textbf{b}) \frac{\partial^2 G}{\partial t\partial s}(t,s)=f(t\textbf{a} + s\textbf{b}). \frac{\partial{g}}{\partial{t}}(t,s)
= \left\langle gradf(t\textbf{a}+s\textbf{b})|\textbf{a}\right\rangle,
 \frac{\partial}{\partial s} (gradf(t\textbf{a}+s\textbf{b}))
=(\frac{\partial gradf}{\partial x}(t\textbf{a}+s\textbf{b}),
\frac{\partial gradf}{\partial y}(t\textbf{a}+s\textbf{b})
)\cdot\textbf{b}
=(Hf)_{(t\textbf{a}+s\textbf{b})}\cdot\textbf{b}.
 gradf f \left\langle \ \ |\ \ \right\rangle \mathbb{R}^2 \frac{\partial^2 g}{\partial t\partial s}(t,s)={}^{T}\textbf{a}(Hf)_{(t\textbf{a}+s\textbf{b})}\textbf{b}\ \  G(t,s)={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2} \frac{\partial^2}{\partial t\partial s}G(t,s)
=\frac{\partial^2 }{\partial t\partial s}{\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2} ={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} \frac{\partial^2 }{\partial t\partial s}g({u}_{1},{u}_{2})\ d{u}_{1}d{u}_{2}={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} 
\ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b}
\ d{u}_{1}d{u}_{2} f(t\textbf{a} + s\textbf{b}) = \frac{\partial^2}{\partial t\partial s}G(t,s) f(t\textbf{a} + s\textbf{b}) 
={\int}_{{{u}_{2}}=0}^{{{u}_{2}}=s}{\int}_{{{u}_{1}}=0}^{{{u}_{1}}=t} 
\ {}^{T}\textbf{a}(Hf)_{({u}_{1}\textbf{a}+{u}_{2}\textbf{b})}\textbf{b}
\ d{u}_{1}d{u}_{2}","['real-analysis', 'integration', 'multivariable-calculus', 'derivatives']"
60,Finding $\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}}$,Finding,\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}},"Inspired by these two questions asking about the case n = 3 and n=4 , I was wondering what is $$S =\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}}$$ for positive integer $n \ge 3$ . For $n = 3$ , the sum is $\frac{1}{4}\zeta (3)-\frac{1}{6}\ln^3(2) = \frac{1}{2}\operatorname{Li}_3\left(\frac{1}{2}\right)+\frac{1}{2}\ln(2)\operatorname{Li}_2\left(\frac{1}{2}\right)-\frac{3}{16}\zeta(3)$ . For $n = 4$ , the sum is $4\operatorname{Li}_4\left(\frac12\right)-\frac72\zeta(4)+\frac{13}4\ln2\zeta(3)-\ln^22\zeta(2)+\frac5{24}\ln^42$ . Using similar work as from the two linked questions, the sum can be re-expressed as the integral $$\frac{2\cdot(-1)^{n-1}}{(n-3)!}\int_0^1\text{arcsinh}^2\left(\sqrt{\frac{x}{8}}\right)\frac{\ln^{n-3}(x)}{x}dx$$ Setting $u = \text{arcsinh}\left(\sqrt{\frac{x}{8}}\right)$ , we get $$S = \frac{4\cdot(-1)^{n-1}}{(n-3)!}\underbrace{\int_0^{\frac{\ln(2)}{2}} u^2\ln^{n-3}(8\sinh^2(u))\coth(u) du}_{\large {I}}$$ $$I = \int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\sum_{k=0}^{n-3}\left({n-3\choose k}\ln^{n-3-k}(8)(2\ln(\sinh(u)))^{k}\right) du$$ $$I = \sum_{k=0}^{n-3}{n-3\choose k}\ln^{n-3-k}(8)2^{k}\underbrace{\int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\ln^{k}(\sinh(u))du}_{\large J}$$ Making the substitution $v = \sinh(u)$ and simplifying, we get $$J = \int_0^{\frac{1}{2\sqrt{2}}}\frac{\text{arcsinh}^2(v)\ln^k(v)}{v}dv$$ Although this may or may not help, making the substitution $w = \ln(v)$ , we get $$J = \int_{-\infty}^{-\ln(2\sqrt{2})}w^k\text{arcsinh}^2(e^w)dw$$ From here, I don't know what to do to find $J$ . How can I, either through this process or a completely different one, find: $1.$ The value of $S$ for integer $n \ge 3$ ? $2.$ The value of $J$ for integer $k \ge 0$ ?","Inspired by these two questions asking about the case n = 3 and n=4 , I was wondering what is for positive integer . For , the sum is . For , the sum is . Using similar work as from the two linked questions, the sum can be re-expressed as the integral Setting , we get Making the substitution and simplifying, we get Although this may or may not help, making the substitution , we get From here, I don't know what to do to find . How can I, either through this process or a completely different one, find: The value of for integer ? The value of for integer ?",S =\sum_{k=1}^\infty\frac{(-1)^{k-1}}{k^n 2^k {2k \choose k}} n \ge 3 n = 3 \frac{1}{4}\zeta (3)-\frac{1}{6}\ln^3(2) = \frac{1}{2}\operatorname{Li}_3\left(\frac{1}{2}\right)+\frac{1}{2}\ln(2)\operatorname{Li}_2\left(\frac{1}{2}\right)-\frac{3}{16}\zeta(3) n = 4 4\operatorname{Li}_4\left(\frac12\right)-\frac72\zeta(4)+\frac{13}4\ln2\zeta(3)-\ln^22\zeta(2)+\frac5{24}\ln^42 \frac{2\cdot(-1)^{n-1}}{(n-3)!}\int_0^1\text{arcsinh}^2\left(\sqrt{\frac{x}{8}}\right)\frac{\ln^{n-3}(x)}{x}dx u = \text{arcsinh}\left(\sqrt{\frac{x}{8}}\right) S = \frac{4\cdot(-1)^{n-1}}{(n-3)!}\underbrace{\int_0^{\frac{\ln(2)}{2}} u^2\ln^{n-3}(8\sinh^2(u))\coth(u) du}_{\large {I}} I = \int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\sum_{k=0}^{n-3}\left({n-3\choose k}\ln^{n-3-k}(8)(2\ln(\sinh(u)))^{k}\right) du I = \sum_{k=0}^{n-3}{n-3\choose k}\ln^{n-3-k}(8)2^{k}\underbrace{\int_0^{\frac{\ln(2)}{2}}u^2\coth(u)\ln^{k}(\sinh(u))du}_{\large J} v = \sinh(u) J = \int_0^{\frac{1}{2\sqrt{2}}}\frac{\text{arcsinh}^2(v)\ln^k(v)}{v}dv w = \ln(v) J = \int_{-\infty}^{-\ln(2\sqrt{2})}w^k\text{arcsinh}^2(e^w)dw J 1. S n \ge 3 2. J k \ge 0,"['integration', 'sequences-and-series', 'definite-integrals', 'binomial-coefficients', 'closed-form']"
61,Finding $\int \frac{1}{1+x^3}dx$ without partial fractions,Finding  without partial fractions,\int \frac{1}{1+x^3}dx,"Find, without partial fractions $$\int\dfrac{1}{x^3+1}dx$$ My Attempt: I was able to do it via partial fractions by factoring the denominator as $$(x+1)(x^2-x+1)$$ However, I then tried a different approach without using partial fractions. I added and subtracted $+x^3$ in the numerator and wrote the integrand as $$1-\dfrac{x^3}{x^3+1}.$$ Then, as the first term is easily integrable, I took the second term and wrote it as $$\dfrac{x^2\cdot x}{x^3+1}.$$ Using Integration by Parts, I integrated $$\dfrac{x^2}{x^3+1}$$ and differentiated $x$ . I ended up with a term and a new integral, $$\dfrac{x\cdot \ln{(x^3+1)}}{3} + \int \dfrac{\ln{(x^3+1)}}{3}dx$$ To evaluate the second integral, I again used integration by parts wherein I integrated $x$ and differentiated $$\ln{(x^3+1)}.$$ Finally, I got the original integral as one of the parts. However, when I undid all the integration by parts to substitute in the original integral, both sides had the same terms and I ended up with $$0 = 0.$$ Is there any other way to solve this integral?","Find, without partial fractions My Attempt: I was able to do it via partial fractions by factoring the denominator as However, I then tried a different approach without using partial fractions. I added and subtracted in the numerator and wrote the integrand as Then, as the first term is easily integrable, I took the second term and wrote it as Using Integration by Parts, I integrated and differentiated . I ended up with a term and a new integral, To evaluate the second integral, I again used integration by parts wherein I integrated and differentiated Finally, I got the original integral as one of the parts. However, when I undid all the integration by parts to substitute in the original integral, both sides had the same terms and I ended up with Is there any other way to solve this integral?",\int\dfrac{1}{x^3+1}dx (x+1)(x^2-x+1) +x^3 1-\dfrac{x^3}{x^3+1}. \dfrac{x^2\cdot x}{x^3+1}. \dfrac{x^2}{x^3+1} x \dfrac{x\cdot \ln{(x^3+1)}}{3} + \int \dfrac{\ln{(x^3+1)}}{3}dx x \ln{(x^3+1)}. 0 = 0.,['integration']
62,"Relating $\int_0^1\frac{(\ln x)^{n-1}(\ln(1-z\,x))^p}{x}dx$ and $\int_0^1\frac{(\ln x)^{n}(\ln(1-z\,x))^{p-1}}{1-z\,x}dx$",Relating  and,"\int_0^1\frac{(\ln x)^{n-1}(\ln(1-z\,x))^p}{x}dx \int_0^1\frac{(\ln x)^{n}(\ln(1-z\,x))^{p-1}}{1-z\,x}dx","This post , after a complicated analysis, evaluates the integral $$I=\int_0^1\frac{\ln^2(x)\,\ln^3(1+x)}xdx$$ simply as $$I =-\frac{\pi^6}{252}-18\zeta(\bar{5},1)+3\zeta^2(3)\tag1$$ where, $$\zeta(\bar{5},1)=\frac{1}{24}\int^1_0\frac{\ln^4{x}\ln(1+x)}{1+x}{\rm d}x$$ More succinctly, $$I = -12\,S_{3,3}(-1)\tag2$$ with Nielsen generalized polylogarithm $S_{n,p}(z)$ . Question: How do we show that $\zeta(\bar{5},1)$ is also a Nielsen generalized polylogarithm in disguise? More generally, for $-1\leq z\leq1$ , how to show $$\begin{aligned}S_{n,p}(z)   &= C_1\int_0^1\frac{(\ln x)^{n-1}\big(\ln(1-z\,x)\big)^p}{x}dx\\ &\overset{?}= C_2\int_0^1\frac{(\ln x)^{n}\;\big(\ln(1-z\,x)\big)^{p-1}}{1-z\,x}dx\end{aligned}\tag3$$ where, $$C_1 = \frac{(-1)^{n+p-1}}{(n-1)!\,p!},\qquad C_2 = \frac{(-1)^{n+p-1}}{n!\,(p-1)!}\color{red}z$$ If true, this implies, $$\zeta(\bar{5},1) \overset{\color{red}?}= S_{4,2}(-1)\tag4$$ Edit: It turns out the notation $\zeta(\bar{5},1)$ is a multiple zeta function so, $$\zeta(\bar{a},1)=\sum_{n=1}^{\infty}\frac{H_n}{(n+1)^a}\,(-1)^{n+1} = S_{a-1,2}(-1)$$ with harmonic numbers $H_n$ , hence $(4)$ indeed is true and is just the case $a=5$ . However, $(3)$ still needs to be proved in general.","This post , after a complicated analysis, evaluates the integral simply as where, More succinctly, with Nielsen generalized polylogarithm . Question: How do we show that is also a Nielsen generalized polylogarithm in disguise? More generally, for , how to show where, If true, this implies, Edit: It turns out the notation is a multiple zeta function so, with harmonic numbers , hence indeed is true and is just the case . However, still needs to be proved in general.","I=\int_0^1\frac{\ln^2(x)\,\ln^3(1+x)}xdx I
=-\frac{\pi^6}{252}-18\zeta(\bar{5},1)+3\zeta^2(3)\tag1 \zeta(\bar{5},1)=\frac{1}{24}\int^1_0\frac{\ln^4{x}\ln(1+x)}{1+x}{\rm d}x I = -12\,S_{3,3}(-1)\tag2 S_{n,p}(z) \zeta(\bar{5},1) -1\leq z\leq1 \begin{aligned}S_{n,p}(z)  
&= C_1\int_0^1\frac{(\ln x)^{n-1}\big(\ln(1-z\,x)\big)^p}{x}dx\\
&\overset{?}= C_2\int_0^1\frac{(\ln x)^{n}\;\big(\ln(1-z\,x)\big)^{p-1}}{1-z\,x}dx\end{aligned}\tag3 C_1 = \frac{(-1)^{n+p-1}}{(n-1)!\,p!},\qquad C_2 = \frac{(-1)^{n+p-1}}{n!\,(p-1)!}\color{red}z \zeta(\bar{5},1) \overset{\color{red}?}= S_{4,2}(-1)\tag4 \zeta(\bar{5},1) \zeta(\bar{a},1)=\sum_{n=1}^{\infty}\frac{H_n}{(n+1)^a}\,(-1)^{n+1} = S_{a-1,2}(-1) H_n (4) a=5 (3)","['integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
63,Is $\int_X d(F^{*}\omega) = 0$ because of a corollary of Stokes' Theorem?,Is  because of a corollary of Stokes' Theorem?,\int_X d(F^{*}\omega) = 0,"My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. The last part of Proposition 11.11 goes $$\int_X d(F^{*}\omega) = \int_X F^{*}(d\omega) = 0$$ Can we just skip commutativity of pullback and exterior derivative and directly say that $\int_X d(F^{*}\omega) = 0$ by Corollary 10.9 of Stokes' Theorem (Theorem 10.8) ? If we cannot skip, then why is it that $\int_X F^{*}(d\omega) = 0$ ? By the way, if anyone wants to know the definition of domain with smooth boundary: I think this is the book's term for a subset, of a manifold, that is manifold with boundary. See here: Why is there a form with compact support on a connected oriented manifold with integral one but with support contained in a given open proper subset? I realized my error: $X$ is not a manifold (without boundary)! We'll have to use commutativity and the fact that for any smooth $n$ -dimensional manifold (without boundary) $M$ , $\Omega^nM = Z^nM$ , as pointed out below.","My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. The last part of Proposition 11.11 goes Can we just skip commutativity of pullback and exterior derivative and directly say that by Corollary 10.9 of Stokes' Theorem (Theorem 10.8) ? If we cannot skip, then why is it that ? By the way, if anyone wants to know the definition of domain with smooth boundary: I think this is the book's term for a subset, of a manifold, that is manifold with boundary. See here: Why is there a form with compact support on a connected oriented manifold with integral one but with support contained in a given open proper subset? I realized my error: is not a manifold (without boundary)! We'll have to use commutativity and the fact that for any smooth -dimensional manifold (without boundary) , , as pointed out below.",\int_X d(F^{*}\omega) = \int_X F^{*}(d\omega) = 0 \int_X d(F^{*}\omega) = 0 \int_X F^{*}(d\omega) = 0 X n M \Omega^nM = Z^nM,"['calculus', 'integration']"
64,Simpler alternative for $\int\frac{dx}{2\sin x+3\cos x+7}$,Simpler alternative for,\int\frac{dx}{2\sin x+3\cos x+7},"Find $$\int\frac{dx}{2\sin{x}+3\cos{x}+7}$$ My attempt: $$I= \int\frac{dx}{\sqrt{13}\sin{\left(x+\theta\right)}+{7}}$$ where $\sin{\theta}=\frac{3}{\sqrt{13}}$ . Then, subsititute $$t = \tan{\frac{y}{2}}$$ where $y=x +\theta$ . But, I think there is a simpler solution than above substitution. Would you help me?","Find My attempt: where . Then, subsititute where . But, I think there is a simpler solution than above substitution. Would you help me?",\int\frac{dx}{2\sin{x}+3\cos{x}+7} I= \int\frac{dx}{\sqrt{13}\sin{\left(x+\theta\right)}+{7}} \sin{\theta}=\frac{3}{\sqrt{13}} t = \tan{\frac{y}{2}} y=x +\theta,"['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
65,Integration of $\int^{1}_{-1} \frac {1}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} (1-t^2) \right) dt$,Integration of,\int^{1}_{-1} \frac {1}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} (1-t^2) \right) dt,"Recently I came across with respect to this post of mine hyperbolic solution to the cubic equation for one real root given by $$ t=-2\sqrt \frac {p}{3} \sinh \left( \frac {1}{3} \sinh^{-1} \left( \frac {3q}{2p} \sqrt \frac {3}{p} \right) \right) $$ Intuitively I sought to find the related definitely integral, $$ I=\int^{1}_{-1} \frac {1}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} (1-t^2) \right) dt $$ Unfortunately, there was no closed form solution. However, the Integral is amazingly near $\sqrt 2$ . $$ I=0.8285267994716327, \frac {I}{2} +1=1.4142633998 $$ To investigated more, I tried a heuristic expansion of the integral into Egyptian fractions. Although it gets problematic after the 4th term, The first four terms are, $$ \frac {I}{2} +1 = 1+ \frac {1}{2} - \frac {1}{12}-\frac {1}{416} $$ Here the denominators can be given by, $$ a_n = \sum_{k=0}^{n} { }^nC_k (2^n - 2^kq)^{n-k}q^k , q=\sqrt 2 $$ (Likewise, the denominators in the expansion for $\sqrt 2$ are related to Pell numbers, which makes me believe that my integral too is somewhat related to the numbers $a_n$ .) Therefore, I am finding either a closed form or possibly a fast converging infinite series solution to the integral, just any of these. Thanks for any help. The indefinite integral For $t=\sin z$ and applying integration by parts, I get another, somewhat simpler, indefinite integral, $$ \frac {\sin z}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} \cos^2 z \right) + 2\sqrt 3 \int \frac {\sin^2 z \cos z dz}{\sqrt {27\cos^4 z + 4}} $$ Then again I am stuck. Moreover, this expression ensures that my definite integral is an improper one. Update A closed solution in terms of incomplete elliptic integrals with complex arguments is, as given by a user in the comments section, $$ \frac {4}{9} (9+2\sqrt 3 i) \left[ F \left( \sin^{-1} \sqrt {\frac {3}{31}(9+2\sqrt 3 i)} ; \frac {1}{31} (23-12\sqrt 3 i) \right)- E \left( \sin^{-1} \sqrt {\frac {3}{31}(9+2\sqrt 3 i)} ; \frac {1}{31} (23-12\sqrt 3 i) \right) \right] $$ However, I am still wondering how to transform this into a real number, especially the $a_n$ connection of the integral is fascinating my mind.","Recently I came across with respect to this post of mine hyperbolic solution to the cubic equation for one real root given by Intuitively I sought to find the related definitely integral, Unfortunately, there was no closed form solution. However, the Integral is amazingly near . To investigated more, I tried a heuristic expansion of the integral into Egyptian fractions. Although it gets problematic after the 4th term, The first four terms are, Here the denominators can be given by, (Likewise, the denominators in the expansion for are related to Pell numbers, which makes me believe that my integral too is somewhat related to the numbers .) Therefore, I am finding either a closed form or possibly a fast converging infinite series solution to the integral, just any of these. Thanks for any help. The indefinite integral For and applying integration by parts, I get another, somewhat simpler, indefinite integral, Then again I am stuck. Moreover, this expression ensures that my definite integral is an improper one. Update A closed solution in terms of incomplete elliptic integrals with complex arguments is, as given by a user in the comments section, However, I am still wondering how to transform this into a real number, especially the connection of the integral is fascinating my mind.","
t=-2\sqrt \frac {p}{3} \sinh \left( \frac {1}{3} \sinh^{-1} \left( \frac {3q}{2p} \sqrt \frac {3}{p} \right) \right)
 
I=\int^{1}_{-1} \frac {1}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} (1-t^2) \right) dt
 \sqrt 2 
I=0.8285267994716327, \frac {I}{2} +1=1.4142633998
 
\frac {I}{2} +1 = 1+ \frac {1}{2} - \frac {1}{12}-\frac {1}{416}
 
a_n = \sum_{k=0}^{n} { }^nC_k (2^n - 2^kq)^{n-k}q^k , q=\sqrt 2
 \sqrt 2 a_n t=\sin z 
\frac {\sin z}{3} \sinh^{-1} \left( \frac {3\sqrt 3}{2} \cos^2 z \right) + 2\sqrt 3 \int \frac {\sin^2 z \cos z dz}{\sqrt {27\cos^4 z + 4}}
 
\frac {4}{9} (9+2\sqrt 3 i) \left[ F \left( \sin^{-1} \sqrt {\frac {3}{31}(9+2\sqrt 3 i)} ; \frac {1}{31} (23-12\sqrt 3 i) \right)-
E \left( \sin^{-1} \sqrt {\frac {3}{31}(9+2\sqrt 3 i)} ; \frac {1}{31} (23-12\sqrt 3 i) \right) \right]
 a_n","['calculus', 'integration', 'definite-integrals', 'hypergeometric-function']"
66,Is this integral unsolvable?,Is this integral unsolvable?,,"So I took an integration test in AP Calculus yesterday and everything went smoothly except for one question. $$\int \frac{e^x}{x^2}dx$$ I tried chain rule, $u$ substitution, and all methods we have since been taught. Nothing worked. Now, I really wanted to know what the answer was, so I just now plugged it into a few antiderivative/integral calculator and all but two said something along the lines of ""cannot solve."" One said $\frac{exp(x)}{x^2} + C$ . The other said $Ei(x) - \frac{e^x}{x} + C$ . A physics forum about the same integration seemed to say that it is unsolvable. I am thoroughly confused. Can anybody help clarify? Edit : The teacher made a little error. The real question was supposed to be, which is of course much easier: $$\int \frac{e^\frac{1}{x}}{x^2}$$","So I took an integration test in AP Calculus yesterday and everything went smoothly except for one question. I tried chain rule, substitution, and all methods we have since been taught. Nothing worked. Now, I really wanted to know what the answer was, so I just now plugged it into a few antiderivative/integral calculator and all but two said something along the lines of ""cannot solve."" One said . The other said . A physics forum about the same integration seemed to say that it is unsolvable. I am thoroughly confused. Can anybody help clarify? Edit : The teacher made a little error. The real question was supposed to be, which is of course much easier:",\int \frac{e^x}{x^2}dx u \frac{exp(x)}{x^2} + C Ei(x) - \frac{e^x}{x} + C \int \frac{e^\frac{1}{x}}{x^2},"['integration', 'indefinite-integrals']"
67,Finding all continuous functions such that $\int_0^xf(t)dt=(f(x))^2+C $,Finding all continuous functions such that,\int_0^xf(t)dt=(f(x))^2+C ,"$C$ is a constant. FTC shows that $f(x)^2+C$ must be differentiable, which means that $f(x)^2$ is differentiable. But we don't know that $f(x)$ is differentiable then, right? I had the idea that the only two solutions are $f(x)=0$ or $f(x)=\frac{x}{2}$ , but this is assuming that $f(x)$ is differentiable. I'm not sure how to show that they are the only solutions (if they even are the only solutions).","is a constant. FTC shows that must be differentiable, which means that is differentiable. But we don't know that is differentiable then, right? I had the idea that the only two solutions are or , but this is assuming that is differentiable. I'm not sure how to show that they are the only solutions (if they even are the only solutions).",C f(x)^2+C f(x)^2 f(x) f(x)=0 f(x)=\frac{x}{2} f(x),"['real-analysis', 'integration']"
68,If $[F(x)]^{100} = \int_{0}^{x} (F(t))^{100} \frac{dt}{1+\sin t}$ then find $F(x)$,If  then find,[F(x)]^{100} = \int_{0}^{x} (F(t))^{100} \frac{dt}{1+\sin t} F(x),"If $[F(x)]^{100} = \int_{0}^{x} (F(t))^{100} \frac{dt}{1+\sin t}$ then find $F(x)$ . My attempt Differentiating both sides, $$100[F(x)]^{99} \frac{d F(x)}{dx} = \frac{F(x)^{100}}{1 + \sin x}$$ then $$\frac{d F(x)}{F(x)} = \frac{dx}{100(1+\sin x)}$$ and $$\int \frac{d F(x)}{F(x)} = \int \frac{dx}{100(1+\sin x)}$$ $$\log F(x) = -1/(50+50 \tan (x/2))$$ Hence $$F(x) = \exp(-1/(50+50\tan (x/2))$$ But, I am not getting my answer right. Where did I go wrong?","If then find . My attempt Differentiating both sides, then and Hence But, I am not getting my answer right. Where did I go wrong?",[F(x)]^{100} = \int_{0}^{x} (F(t))^{100} \frac{dt}{1+\sin t} F(x) 100[F(x)]^{99} \frac{d F(x)}{dx} = \frac{F(x)^{100}}{1 + \sin x} \frac{d F(x)}{F(x)} = \frac{dx}{100(1+\sin x)} \int \frac{d F(x)}{F(x)} = \int \frac{dx}{100(1+\sin x)} \log F(x) = -1/(50+50 \tan (x/2)) F(x) = \exp(-1/(50+50\tan (x/2)),"['calculus', 'integration']"
69,"Real Analysis, $\lim\limits_{n\rightarrow\infty} \int_{0}^{1} \frac{e^{-nt}-(1-t)^n}{t} dt$","Real Analysis,",\lim\limits_{n\rightarrow\infty} \int_{0}^{1} \frac{e^{-nt}-(1-t)^n}{t} dt,"I was trying to compute $\lim\limits_{n\rightarrow\infty} \int_{0}^{1} \frac{e^{-nt}-(1-t)^n}{t} dt$ using Lebesgue's dominated convergence THM, but I can't exactly figure out how to do. I mean, I managed to prove that each the integrand function $f_n(t)$ is less or equal than $g_n(t)=e^{-nt}\sqrt{n}e^\frac{1}{\sqrt{n}}\, \forall n\in\mathbb{N}$ . And since we are dealing with positive functions and $\int_{0}^{1} g_n(t)dt\leq\frac{e}{\sqrt{n}}\overset{\mathrm{n\rightarrow\infty}}{\rightarrow}0$ , I can deduce that the original limit is $0$ . Now, I was just wondering if anyone is able to show analytically that there exists a function in $\mathcal{L}^1$ which dominates all the $f_n$ in order to apply Lebesgue's dominated convergence THM. I made some attempts, but I failed. Thanks in advance, a humble half-mathematician.","I was trying to compute using Lebesgue's dominated convergence THM, but I can't exactly figure out how to do. I mean, I managed to prove that each the integrand function is less or equal than . And since we are dealing with positive functions and , I can deduce that the original limit is . Now, I was just wondering if anyone is able to show analytically that there exists a function in which dominates all the in order to apply Lebesgue's dominated convergence THM. I made some attempts, but I failed. Thanks in advance, a humble half-mathematician.","\lim\limits_{n\rightarrow\infty} \int_{0}^{1} \frac{e^{-nt}-(1-t)^n}{t} dt f_n(t) g_n(t)=e^{-nt}\sqrt{n}e^\frac{1}{\sqrt{n}}\, \forall n\in\mathbb{N} \int_{0}^{1} g_n(t)dt\leq\frac{e}{\sqrt{n}}\overset{\mathrm{n\rightarrow\infty}}{\rightarrow}0 0 \mathcal{L}^1 f_n","['real-analysis', 'integration', 'limits', 'convergence-divergence', 'definite-integrals']"
70,Deriving the Chi-squared distribution using characteristic functions,Deriving the Chi-squared distribution using characteristic functions,,"I would like to directly derive the probability density function (PDF) for a Chi-squared distribution with $k$ degrees of freedom using characteristic functions. If $X_{1}, X_{2}, \dots, X_{k}$ are independent, standard normal random variables, then $$ Y = \sum_{i=1}^{k} X_{i}^{2} $$ and $Y$ is chi-squared distributed with $k$ degrees of freedom. The PDF for $Y$ when $k = 1$ is given by $$ f(x, 1) = \frac{1}{\sqrt{2 \pi x}} e^{-\frac{1}{2} x} $$ and its respective characteristic function is $$ \varphi_{Y_{1}} (\omega) = \int_{-\infty}^{\infty}  f(x, 1) e^{iwx} dx = (1 - i2 \omega)^{-\frac{1}{2}} \text{,}$$ where $i$ is the imaginary number. For a general $k$ degrees of freedom, $Y$ 's characteristic function is given by $$\varphi_{Y} (\omega) = (1 - i2 \omega)^{-\frac{k}{2}} \text{.}$$ Is it possible to explicitly derive $f(x, k)$ using the inverse Fourier transform, where $$ f(x, k) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} (1 - i2 \omega)^{-\frac{k}{2}} e^{-iwx} d\omega ?$$ I have had no success with this approach, but I'm probably missing something very obvious. Failing that, is it possible to derive a formula for $k=2$ , where $$f(x, 2) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} (1 - i2 \omega)^{-1} e^{-iwx} d\omega ?$$ This would at least allow me derive the PDF inductively. In addition, I am aware that $f(x, 1)$ can be represented as the Gamma distribution $\text{Gamma}(x, \frac{1}{2}, \frac{1}{2})$ and the sum of independent Gamma random variables is known to be Gamma distributed, therefore, for this example, $$ Y \sim \text{Gamma} ( \cdot, \frac{k}{2}, \frac{1}{2}) \text{.} $$","I would like to directly derive the probability density function (PDF) for a Chi-squared distribution with degrees of freedom using characteristic functions. If are independent, standard normal random variables, then and is chi-squared distributed with degrees of freedom. The PDF for when is given by and its respective characteristic function is where is the imaginary number. For a general degrees of freedom, 's characteristic function is given by Is it possible to explicitly derive using the inverse Fourier transform, where I have had no success with this approach, but I'm probably missing something very obvious. Failing that, is it possible to derive a formula for , where This would at least allow me derive the PDF inductively. In addition, I am aware that can be represented as the Gamma distribution and the sum of independent Gamma random variables is known to be Gamma distributed, therefore, for this example,","k X_{1}, X_{2}, \dots, X_{k}  Y = \sum_{i=1}^{k} X_{i}^{2}  Y k Y k = 1  f(x, 1) = \frac{1}{\sqrt{2 \pi x}} e^{-\frac{1}{2} x}   \varphi_{Y_{1}} (\omega) = \int_{-\infty}^{\infty}  f(x, 1) e^{iwx} dx = (1 - i2 \omega)^{-\frac{1}{2}} \text{,} i k Y \varphi_{Y} (\omega) = (1 - i2 \omega)^{-\frac{k}{2}} \text{.} f(x, k)  f(x, k) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} (1 - i2 \omega)^{-\frac{k}{2}} e^{-iwx} d\omega ? k=2 f(x, 2) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} (1 - i2 \omega)^{-1} e^{-iwx} d\omega ? f(x, 1) \text{Gamma}(x, \frac{1}{2}, \frac{1}{2})  Y \sim \text{Gamma} ( \cdot, \frac{k}{2}, \frac{1}{2}) \text{.} ","['integration', 'probability-theory', 'probability-distributions', 'fourier-transform', 'chi-squared']"
71,"Other ways to find a limit where the denominator produces $0$, besides factoring and cancellation?","Other ways to find a limit where the denominator produces , besides factoring and cancellation?",0,"So I came across a seemingly innocent looking integral: $$\int_{-2}^1 \frac{1}{x^2}dx$$ Now of course, when taking the antiderivative then plugging in the values, we can see that we get a nonzero/ $0$ which causes problems (because the function is discontinuous). Now to solve this we can split up the integral into two limits, both tending to $0$ from either side: $$\left(\lim_{b\to0^-} \int_{-2}^b \frac{1}{x^2}dx\right) + \left(\lim_{a\to0^+} \int_{a}^1 \frac{1}{x^2}dx\right)$$ This would, in theory, solve the problem except for one issue, the same issue: nonzero/ $0$ when plugging in the numbers to the integral from the limit. So naturally, I thought to try and factor it somehow but within a few seconds, it became obvious that it wasn't possible. So the integral $\int_{-2}^1 \frac{1}{x^2}dx$ diverges Now I'm wondering, is there any way to solve a limit where the denominator would produce $0$ , besides factoring and cancellation? tl;dr: A limit produces $0$ in the denominator and it is unfactorable (in the sense that you cannot cancel out the bottom term). Is there any way to solve it, using another method besides factoring and cancellation?","So I came across a seemingly innocent looking integral: Now of course, when taking the antiderivative then plugging in the values, we can see that we get a nonzero/ which causes problems (because the function is discontinuous). Now to solve this we can split up the integral into two limits, both tending to from either side: This would, in theory, solve the problem except for one issue, the same issue: nonzero/ when plugging in the numbers to the integral from the limit. So naturally, I thought to try and factor it somehow but within a few seconds, it became obvious that it wasn't possible. So the integral diverges Now I'm wondering, is there any way to solve a limit where the denominator would produce , besides factoring and cancellation? tl;dr: A limit produces in the denominator and it is unfactorable (in the sense that you cannot cancel out the bottom term). Is there any way to solve it, using another method besides factoring and cancellation?",\int_{-2}^1 \frac{1}{x^2}dx 0 0 \left(\lim_{b\to0^-} \int_{-2}^b \frac{1}{x^2}dx\right) + \left(\lim_{a\to0^+} \int_{a}^1 \frac{1}{x^2}dx\right) 0 \int_{-2}^1 \frac{1}{x^2}dx 0 0,"['integration', 'limits']"
72,Finding the charge density in a solid,Finding the charge density in a solid,,"An object occupies the solid region in the first octant bounded by the coordinate planes and two cylinders $x^2 + y^2 = 4$ and $y^2 + z^2 = 4$ If the charge density at any point is $x$ , what's the total charge? So I know $Q = pV$ , where $p$ is charge density and $V$ is volume. I think that I need to do this in spherical coordinates, but I'm not too sure. I define $f(x, y, z) = x$ , then I have $f( \theta, \phi) = ??$ I've never used spherical coordinates before. I'm learning multivariable calc through self study, so any help would be appreciated. I think my integral will be something like this $$\int_{0}^{2\pi}\int_{0}^{\pi} f(\theta, \phi) d\theta d\phi \cdot x,$$ but I'm not sure about how to get the function. Any help is appreciated.","An object occupies the solid region in the first octant bounded by the coordinate planes and two cylinders and If the charge density at any point is , what's the total charge? So I know , where is charge density and is volume. I think that I need to do this in spherical coordinates, but I'm not too sure. I define , then I have I've never used spherical coordinates before. I'm learning multivariable calc through self study, so any help would be appreciated. I think my integral will be something like this but I'm not sure about how to get the function. Any help is appreciated.","x^2 + y^2 = 4 y^2 + z^2 = 4 x Q = pV p V f(x, y, z) = x f( \theta, \phi) = ?? \int_{0}^{2\pi}\int_{0}^{\pi} f(\theta, \phi) d\theta d\phi \cdot x,",['integration']
73,Sum of reciprocals of triangular numbers and calculus,Sum of reciprocals of triangular numbers and calculus,,"I've been searching for interesting calculus homework problems recently and came across the following: Partition the unit square $[0,1] \times [0,1]$ into regions using the curves $y=x^n$ for $n \in \{0,1,2,\ldots\}$ . Then, the area between any two consecutive curves is $$\int_0^1 x^{n-1}-x^n \, dx =\frac{1}{n(n+1)}.$$ Since the area of the unit square is $1$ , we get this nice sum: $$\sum_{n=1}^{\infty}\frac{1}{n(n+1)}=1$$ This is where the homework problem ends, but now notice that: $$\frac{1}{n(n+1)}=\frac{1}{2 {n+1 \choose 2}}$$ Thus: $$ \sum_{n=1}^{\infty}\frac{1}{{n+1 \choose 2}}=2.$$ To put it more gracefully: $$ \sum_{n=2}^{\infty}\frac{1}{{n \choose 2}}=2.$$ Here is what I am wondering: The series given in the last line seems extremely combinatorial. Does anyone know of any combinatorial interpretation of the series or its partial sums? I am also wondering if it is possible to derive any other infinite series involving reciprocals of binomial coefficients by using similar techniques.","I've been searching for interesting calculus homework problems recently and came across the following: Partition the unit square into regions using the curves for . Then, the area between any two consecutive curves is Since the area of the unit square is , we get this nice sum: This is where the homework problem ends, but now notice that: Thus: To put it more gracefully: Here is what I am wondering: The series given in the last line seems extremely combinatorial. Does anyone know of any combinatorial interpretation of the series or its partial sums? I am also wondering if it is possible to derive any other infinite series involving reciprocals of binomial coefficients by using similar techniques.","[0,1] \times [0,1] y=x^n n \in \{0,1,2,\ldots\} \int_0^1 x^{n-1}-x^n \, dx =\frac{1}{n(n+1)}. 1 \sum_{n=1}^{\infty}\frac{1}{n(n+1)}=1 \frac{1}{n(n+1)}=\frac{1}{2 {n+1 \choose 2}}  \sum_{n=1}^{\infty}\frac{1}{{n+1 \choose 2}}=2.  \sum_{n=2}^{\infty}\frac{1}{{n \choose 2}}=2.","['calculus', 'integration', 'combinatorics']"
74,Multiple integral: how to retrieve abscissa range,Multiple integral: how to retrieve abscissa range,,"We have the double integral: $$\int \int_D 2x + 3y \; dx\;dy$$ The domain in which we want to calculate this is the flat region defined by the curves: $$y = x^2 \; ; \; y=x$$ Then, through the decomposition rules we resolve the internal integral to $dy$, and to do this we find the copy ordinates of the minimum and maximum points of the domain, which are precisely $$y = x^2 \; ; \; y=x$$ While the minimum and maximum points abiscissas will be the external integral range $$\int_{0}^{1} dx \int^{x}_{x^2} 2x + 3y \; dy$$ The coordinates are found by solving to $y$ the curves  that define the domain : for the abscissas, does there exist a mathematical method, or should we simply be intuitive? Thank you in advance","We have the double integral: $$\int \int_D 2x + 3y \; dx\;dy$$ The domain in which we want to calculate this is the flat region defined by the curves: $$y = x^2 \; ; \; y=x$$ Then, through the decomposition rules we resolve the internal integral to $dy$, and to do this we find the copy ordinates of the minimum and maximum points of the domain, which are precisely $$y = x^2 \; ; \; y=x$$ While the minimum and maximum points abiscissas will be the external integral range $$\int_{0}^{1} dx \int^{x}_{x^2} 2x + 3y \; dy$$ The coordinates are found by solving to $y$ the curves  that define the domain : for the abscissas, does there exist a mathematical method, or should we simply be intuitive? Thank you in advance",,['integration']
75,Close form for a sequence of integrals involving the Gamma function,Close form for a sequence of integrals involving the Gamma function,,"I'm trying to find a close form for these integrals (with $n\in\mathbb{N}$) : $$\int_{0}^{1}\ln(\Gamma(x+1))\cdot x^n \, \text{d}x$$ Wolfram is able to give a close form for every specific value of n, and the result leads to believe there is indeed a pattern ; however it fails to find the general formulae for $n\in\mathbb{N}$. An integration by part turns it into similar-looking integrals involving the digamma function. Do you think there is any way to find a close form, or is it a lost battle ?","I'm trying to find a close form for these integrals (with $n\in\mathbb{N}$) : $$\int_{0}^{1}\ln(\Gamma(x+1))\cdot x^n \, \text{d}x$$ Wolfram is able to give a close form for every specific value of n, and the result leads to believe there is indeed a pattern ; however it fails to find the general formulae for $n\in\mathbb{N}$. An integration by part turns it into similar-looking integrals involving the digamma function. Do you think there is any way to find a close form, or is it a lost battle ?",,"['real-analysis', 'integration', 'gamma-function']"
76,Positivity of $\int_0^1 \sin^2(kx)f(x) dx$,Positivity of,\int_0^1 \sin^2(kx)f(x) dx,I'm interested in proving restrictions on $f$ necessary for the integral \begin{equation*} \int_0^1 \sin^2 (kx) f(x) dx \end{equation*} to be positive for all $k > 0$. Obviously a nonnegative $f$ would suffice (provided it was positive on a set of nonzero measure). I'm wondering if there are less restrictive conditions that may be placed on $f$. Thanks in advance.,I'm interested in proving restrictions on $f$ necessary for the integral \begin{equation*} \int_0^1 \sin^2 (kx) f(x) dx \end{equation*} to be positive for all $k > 0$. Obviously a nonnegative $f$ would suffice (provided it was positive on a set of nonzero measure). I'm wondering if there are less restrictive conditions that may be placed on $f$. Thanks in advance.,,"['integration', 'analysis', 'definite-integrals']"
77,Compute $\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n\left(1+\frac{k}{n^2}\right)^n$,Compute,\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n\left(1+\frac{k}{n^2}\right)^n,"I want to compute $$\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n\left(1+\frac{k}{n^2}\right)^n.$$ I really tried several thing, but this $\frac{1}{n^2}$ annoy me very much. It looks like a Riemann sum, but I can't conclude without more information.","I want to compute $$\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n\left(1+\frac{k}{n^2}\right)^n.$$ I really tried several thing, but this $\frac{1}{n^2}$ annoy me very much. It looks like a Riemann sum, but I can't conclude without more information.",,['integration']
78,"Why continuity of $X$ needed for $\int_{g^{-1}(y)}^\infty f_X(x) \, dx = 1-F_X(g^{-1}(y))$?",Why continuity of  needed for ?,"X \int_{g^{-1}(y)}^\infty f_X(x) \, dx = 1-F_X(g^{-1}(y))","Let $X$ be a random variable and $Y=g(X)$ Define $$\tag{1} \chi = \{x: f_X(x)>0\}\quad \text{and}\quad \mathcal{Y} = \{y:y=g(x) \text{ for some } x \in \chi\} $$ Define $g^{-1}(y) = \{x\in \chi:g(x) = y\}$ Define: A random variable $X$ is continuous if $F_X(x)$ is a continuous function of $x$. My question is: how come, in the theorem below, the statement in (b) requires X to be a continuous random variable but the statement in (a) does not The relevant theorem is (Theorem 2.1.3 in Casella and Berger 2nd Edition) Let $X$ have cdf $F_X(x)$, let $Y=g(X)$, and let $\chi$ and $\mathcal{Y}$ be defined as in (1) (a) If $g$ is an increasing function on $\chi$, $F_Y(y) = F_X(g^{-1}(y))$ for $y\in \mathcal{Y}$ (b) If $g$ is a decreasing function on $\chi$ and $X$ is a continuous random variable, $F_Y(y) = 1-F_X(g^{-1}(y))$ for $y\in\mathcal{Y}$ Another way of stating what I am asking is that, prior to stating this theorem, Casella and Berger state if $g(x)$ is an increasing function, then using the fact that $F_Y(y) = \int_{x\in\chi : g(x)\leq y} f_X(x)dx$, we can write   $$ F_Y(y) = \int_{x\in\chi : g(x)\leq y} f_X(x) \, dx = \int_{-\infty}^{g^{-1}(y)} f_X(x) \, dx = F_X(g^{-1}(y)) $$ If $g(x)$ is decreasing, then we have $$ F_Y(y) = \int_{g^{-1}(y)}^\infty f_X(x) \, dx = 1-F_X(g^{-1}(y)) $$ ""The continuity of $X$ is used to obtain the second equality My question(restated) is in yellow box below: My question (restated) is: How come, when $g(x)$ is an increasing function we do not need to use continuity of $X$, but we do for the case when $g(x)$ is decreasing? (A side question, I will accept answer so long as answers the above question): this is continuity of the random variable , but the integral uses the PDF. what is the relation between continuity of $X$ and it's pdf? (specifically, I think there may be some strangeness if $F_X$, the CDF of $X$ is continuous but not differentiable)? What came to my mind was Fundamental theorem of calculus maybe, but there is a version of it that doesn't require continuity of $f$ I think?  Plus, here we have $X$ is continuous, if that matters -- I'm not sure.","Let $X$ be a random variable and $Y=g(X)$ Define $$\tag{1} \chi = \{x: f_X(x)>0\}\quad \text{and}\quad \mathcal{Y} = \{y:y=g(x) \text{ for some } x \in \chi\} $$ Define $g^{-1}(y) = \{x\in \chi:g(x) = y\}$ Define: A random variable $X$ is continuous if $F_X(x)$ is a continuous function of $x$. My question is: how come, in the theorem below, the statement in (b) requires X to be a continuous random variable but the statement in (a) does not The relevant theorem is (Theorem 2.1.3 in Casella and Berger 2nd Edition) Let $X$ have cdf $F_X(x)$, let $Y=g(X)$, and let $\chi$ and $\mathcal{Y}$ be defined as in (1) (a) If $g$ is an increasing function on $\chi$, $F_Y(y) = F_X(g^{-1}(y))$ for $y\in \mathcal{Y}$ (b) If $g$ is a decreasing function on $\chi$ and $X$ is a continuous random variable, $F_Y(y) = 1-F_X(g^{-1}(y))$ for $y\in\mathcal{Y}$ Another way of stating what I am asking is that, prior to stating this theorem, Casella and Berger state if $g(x)$ is an increasing function, then using the fact that $F_Y(y) = \int_{x\in\chi : g(x)\leq y} f_X(x)dx$, we can write   $$ F_Y(y) = \int_{x\in\chi : g(x)\leq y} f_X(x) \, dx = \int_{-\infty}^{g^{-1}(y)} f_X(x) \, dx = F_X(g^{-1}(y)) $$ If $g(x)$ is decreasing, then we have $$ F_Y(y) = \int_{g^{-1}(y)}^\infty f_X(x) \, dx = 1-F_X(g^{-1}(y)) $$ ""The continuity of $X$ is used to obtain the second equality My question(restated) is in yellow box below: My question (restated) is: How come, when $g(x)$ is an increasing function we do not need to use continuity of $X$, but we do for the case when $g(x)$ is decreasing? (A side question, I will accept answer so long as answers the above question): this is continuity of the random variable , but the integral uses the PDF. what is the relation between continuity of $X$ and it's pdf? (specifically, I think there may be some strangeness if $F_X$, the CDF of $X$ is continuous but not differentiable)? What came to my mind was Fundamental theorem of calculus maybe, but there is a version of it that doesn't require continuity of $f$ I think?  Plus, here we have $X$ is continuous, if that matters -- I'm not sure.",,"['integration', 'probability-theory', 'continuity']"
79,How to show that $\mathscr{P} \int_{-\infty}^{\infty} \frac{d\omega}{\sqrt{\omega^2}} e^{ - i \omega t }$ is equal to $- \log(t^2)$,How to show that  is equal to,\mathscr{P} \int_{-\infty}^{\infty} \frac{d\omega}{\sqrt{\omega^2}} e^{ - i \omega t } - \log(t^2),"Fix $t>0$ and consider the following Principal Value integral: $$ \mathscr{P}\int_{-\infty}^{\infty}d\omega \frac{e^{-i \omega t}}{\sqrt{\omega^2}} = - \log(t^2) $$ This is the function that Mathematica spits out (and this also matches a result that I'm finding in Lighthill's ``An Introduction to Fourier Analysis and Generalized Functions'', up to a constant). I'm posting because when I check this calculation myself explicitly, I'm getting something strange. This is my method of computation: $$ \mathscr{P} \int_{-\infty}^{\infty} \frac{d\omega}{\sqrt{\omega^2}}  e^{ - i \omega t } =  \lim\limits_{\eta \to 0^{+}} \left\{ \int_{-\infty}^{-\eta} \frac{d\omega}{-\omega} e^{ - i \omega t } \ + \ \int_{\eta}^{\infty} \frac{d\omega}{\omega}  e^{ - i \omega t } \right\} $$ After switching the variable $\omega \mapsto -\omega$ in the first integral, the above simplifies to: $$ \ldots = \lim\limits_{\eta \to 0^{+}} \left\{ 2 \int_{\eta}^{\infty} \frac{d\omega}{\omega} \cos\left( \omega t \right) \right\} = \lim\limits_{\eta \to 0^{+}} \left\{ - 2 \mathrm{Ci}\left( \eta t \right) \right\} $$ Where $\mathrm{Ci}$ is the cosine-integral function. The problem is that I cannot take the limit $\eta \to 0^{+}$. Upon an expansion about $\eta =0$, I find that the above looks like: $$ \ldots = \lim\limits_{\eta \to 0^{+}} \left\{ - 2 \gamma - \log(\eta^2) - \log(t^2) + \mathscr{O}(\eta^2) \right\} $$ So it seems to me like the integral has the value $- 2\gamma - \lim\limits_{\eta\to 0^+} \log(\eta^2) - \log(t^2)$. So I get the right functional form in $t$, but I have an extra two constant appearing - one of which is infinite! The $\eta$ seems to be a 'regulator' for the normally divergent integral - how does one get rid of it? Am I using the wrong definition for the Cauchy Principal value in my computation? Mathematica and Lighthill seems to be throwing the $-2 \gamma - \log(\eta^2)$ away somehow. P.S. $\gamma$ is the Euler-Mascheroni constant.","Fix $t>0$ and consider the following Principal Value integral: $$ \mathscr{P}\int_{-\infty}^{\infty}d\omega \frac{e^{-i \omega t}}{\sqrt{\omega^2}} = - \log(t^2) $$ This is the function that Mathematica spits out (and this also matches a result that I'm finding in Lighthill's ``An Introduction to Fourier Analysis and Generalized Functions'', up to a constant). I'm posting because when I check this calculation myself explicitly, I'm getting something strange. This is my method of computation: $$ \mathscr{P} \int_{-\infty}^{\infty} \frac{d\omega}{\sqrt{\omega^2}}  e^{ - i \omega t } =  \lim\limits_{\eta \to 0^{+}} \left\{ \int_{-\infty}^{-\eta} \frac{d\omega}{-\omega} e^{ - i \omega t } \ + \ \int_{\eta}^{\infty} \frac{d\omega}{\omega}  e^{ - i \omega t } \right\} $$ After switching the variable $\omega \mapsto -\omega$ in the first integral, the above simplifies to: $$ \ldots = \lim\limits_{\eta \to 0^{+}} \left\{ 2 \int_{\eta}^{\infty} \frac{d\omega}{\omega} \cos\left( \omega t \right) \right\} = \lim\limits_{\eta \to 0^{+}} \left\{ - 2 \mathrm{Ci}\left( \eta t \right) \right\} $$ Where $\mathrm{Ci}$ is the cosine-integral function. The problem is that I cannot take the limit $\eta \to 0^{+}$. Upon an expansion about $\eta =0$, I find that the above looks like: $$ \ldots = \lim\limits_{\eta \to 0^{+}} \left\{ - 2 \gamma - \log(\eta^2) - \log(t^2) + \mathscr{O}(\eta^2) \right\} $$ So it seems to me like the integral has the value $- 2\gamma - \lim\limits_{\eta\to 0^+} \log(\eta^2) - \log(t^2)$. So I get the right functional form in $t$, but I have an extra two constant appearing - one of which is infinite! The $\eta$ seems to be a 'regulator' for the normally divergent integral - how does one get rid of it? Am I using the wrong definition for the Cauchy Principal value in my computation? Mathematica and Lighthill seems to be throwing the $-2 \gamma - \log(\eta^2)$ away somehow. P.S. $\gamma$ is the Euler-Mascheroni constant.",,"['integration', 'fourier-transform', 'harmonic-analysis', 'cauchy-principal-value']"
80,Evaluating $\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx$,Evaluating,\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx,Evaluate $\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx$ My attempt : $I=\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx=\int \sqrt{\frac{\cos x(1-\cos^2 x)}{1-\cos^3 x}}dx=\int \sqrt{\frac{\cos x\sin^2 x}{1-\cos^3 x}}dx=\int \sin x\sqrt{\frac{\cos x}{1-\cos^3 x}}dx=\int\sqrt{\frac{\cos x}{1-\cos^3 x}}(\sin x)dx=-\int \sqrt{\frac{\cos x}{1-\cos^3 x}}(-\sin x)dx$ Let $z=\cos x$ $\therefore dz=(-\sin x)dx$ $\therefore I=-\int\sqrt{\frac{z}{1-z^3}}dz$ I cannot understand how to proceed further. Please help.,Evaluate $\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx$ My attempt : $I=\int \sqrt{\frac{\cos x - \cos^3 x}{1-\cos^3 x}}dx=\int \sqrt{\frac{\cos x(1-\cos^2 x)}{1-\cos^3 x}}dx=\int \sqrt{\frac{\cos x\sin^2 x}{1-\cos^3 x}}dx=\int \sin x\sqrt{\frac{\cos x}{1-\cos^3 x}}dx=\int\sqrt{\frac{\cos x}{1-\cos^3 x}}(\sin x)dx=-\int \sqrt{\frac{\cos x}{1-\cos^3 x}}(-\sin x)dx$ Let $z=\cos x$ $\therefore dz=(-\sin x)dx$ $\therefore I=-\int\sqrt{\frac{z}{1-z^3}}dz$ I cannot understand how to proceed further. Please help.,,"['calculus', 'integration', 'indefinite-integrals']"
81,"$\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy$?",?,"\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy","Let $D:=[a,+\infty)\times[c,+\infty) $,where $a,c \in \mathbb{R};$  and let $f:D \mapsto \mathbb{R} $ be a nonnegative continuous function. Prove that if the existence of either of the iterated integrals $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy$ and  $\int_{c}^{+\infty}dy\int_{a}^{+\infty}f(x,y)dx$ ,then the improper double integral $\iint_{D}f(x,y)dxdy$ converges to the value of the iterated integral in question. WLOG,suppose $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I(\in\mathbb{R}).$ Given any $b,d\in\mathbb{R}$  with $b>a,d>c$,by Fubini's theorem,$$\iint_{R}f(x,y)dxdy=\int_{a}^{b}dx\int_{c}^{d}f(x,y)dy,R=[a,b]\times[c,d].$$ Let $\varphi(x,\beta ):=\int_{c}^{\beta}f(x,y)dy,$ then for each fixed $\beta_{0}(>c)$ and $\alpha^{""}>\alpha^{'}>a,$ we obtian $$\int_{a}^{\alpha^{""}}\varphi(x,\beta_{0} )dx\geq \int_{a}^{\alpha^{'}}\varphi(x,\beta_{0} )dx.$$ Since $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I,$ $\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ with respect to $\alpha$ has upper unbound on $[a,+\infty).$ So $\lim_{\alpha \rightarrow +\infty}\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ exists and is a real number . Next we have $$\iint_{[a,+\infty)\times[c,\beta_{0} ]}f(x,y)dxdy=\int_{a}^{+\infty}\varphi(x,\beta_{0} )dx.$$ At this time, If we  can prove  $$\lim_{\beta \rightarrow +\infty}\int_{a}^{+\infty}\varphi(x,\beta_{} )dx=\int_{a}^{+\infty}\left (  \lim_{\beta \rightarrow +\infty}\varphi(x,\beta_{} )\right )dx,\quad (*)$$then $$\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy=I$$ But until now,I have no idea to prove $(*)$ is true.I need some help to deal  with it ,or better solutions in other ways .","Let $D:=[a,+\infty)\times[c,+\infty) $,where $a,c \in \mathbb{R};$  and let $f:D \mapsto \mathbb{R} $ be a nonnegative continuous function. Prove that if the existence of either of the iterated integrals $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy$ and  $\int_{c}^{+\infty}dy\int_{a}^{+\infty}f(x,y)dx$ ,then the improper double integral $\iint_{D}f(x,y)dxdy$ converges to the value of the iterated integral in question. WLOG,suppose $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I(\in\mathbb{R}).$ Given any $b,d\in\mathbb{R}$  with $b>a,d>c$,by Fubini's theorem,$$\iint_{R}f(x,y)dxdy=\int_{a}^{b}dx\int_{c}^{d}f(x,y)dy,R=[a,b]\times[c,d].$$ Let $\varphi(x,\beta ):=\int_{c}^{\beta}f(x,y)dy,$ then for each fixed $\beta_{0}(>c)$ and $\alpha^{""}>\alpha^{'}>a,$ we obtian $$\int_{a}^{\alpha^{""}}\varphi(x,\beta_{0} )dx\geq \int_{a}^{\alpha^{'}}\varphi(x,\beta_{0} )dx.$$ Since $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I,$ $\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ with respect to $\alpha$ has upper unbound on $[a,+\infty).$ So $\lim_{\alpha \rightarrow +\infty}\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ exists and is a real number . Next we have $$\iint_{[a,+\infty)\times[c,\beta_{0} ]}f(x,y)dxdy=\int_{a}^{+\infty}\varphi(x,\beta_{0} )dx.$$ At this time, If we  can prove  $$\lim_{\beta \rightarrow +\infty}\int_{a}^{+\infty}\varphi(x,\beta_{} )dx=\int_{a}^{+\infty}\left (  \lim_{\beta \rightarrow +\infty}\varphi(x,\beta_{} )\right )dx,\quad (*)$$then $$\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy=I$$ But until now,I have no idea to prove $(*)$ is true.I need some help to deal  with it ,or better solutions in other ways .",,"['real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals', 'riemann-integration']"
82,Two different answers of same indefinite integral,Two different answers of same indefinite integral,,Evaluate $$\int \frac {dx}{x^2-x+1}$$ Method 1 $$\int \frac {dx}{x^2-x+1}=\frac {4}{3}\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}}$$ Put $u=x-1/2$ Hence $$\frac 43\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}} =\frac 43\int \frac {du}{1+\frac {4u^2}{3}}$$ And then I could use $$\int \frac {dx}{1+x^2}=\arctan x$$ Method 2 Let $$I=\int \frac {dx}{x^2-x+1}$$ Put $x=\frac 1y$ Hence $dx=\frac {-dy}{y^2}$ Hence $$I=\int \frac {\frac {-dy}{y^2}}{\frac {1}{y^2}-\frac 1y+1}=-\int \frac {dy}{y^2-y+1}=-\int \frac {dx}{x^2-x+1}=-I$$ Hence $$I=-I\Rightarrow I=0$$ Why am I getting two different answers?  I guess i am missing some link in method 2.,Evaluate $$\int \frac {dx}{x^2-x+1}$$ Method 1 $$\int \frac {dx}{x^2-x+1}=\frac {4}{3}\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}}$$ Put $u=x-1/2$ Hence $$\frac 43\int \frac {dx}{1+\frac {4(x-1/2)^2}{3}} =\frac 43\int \frac {du}{1+\frac {4u^2}{3}}$$ And then I could use $$\int \frac {dx}{1+x^2}=\arctan x$$ Method 2 Let $$I=\int \frac {dx}{x^2-x+1}$$ Put $x=\frac 1y$ Hence $dx=\frac {-dy}{y^2}$ Hence $$I=\int \frac {\frac {-dy}{y^2}}{\frac {1}{y^2}-\frac 1y+1}=-\int \frac {dy}{y^2-y+1}=-\int \frac {dx}{x^2-x+1}=-I$$ Hence $$I=-I\Rightarrow I=0$$ Why am I getting two different answers?  I guess i am missing some link in method 2.,,"['integration', 'indefinite-integrals']"
83,Convergence of an improper integral $\int_3^\infty \frac{\sin(x)}{x+2\cos(x)}dx$,Convergence of an improper integral,\int_3^\infty \frac{\sin(x)}{x+2\cos(x)}dx,I tried to check whether the following integral converges: $$\int_3^\infty \dfrac{\sin(x)}{x+2\cos(x)}dx$$ Dirichlet criterion doesn't work here since the function $$\dfrac{1}{x+2\cos(x)}$$ is not monotone.,I tried to check whether the following integral converges: $$\int_3^\infty \dfrac{\sin(x)}{x+2\cos(x)}dx$$ Dirichlet criterion doesn't work here since the function $$\dfrac{1}{x+2\cos(x)}$$ is not monotone.,,"['integration', 'improper-integrals']"
84,Exercise on properties of integration,Exercise on properties of integration,,"Suppose $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Suppose $m \leq f' \leq M$ on $(a, b)$ and denote $\mu = \dfrac{f(b) - f(a)}{b-a}$. Prove that  $$\left|\int_{a}^{b} f(x) \,\mathrm{d}x  - \frac{f(a) + f(b)}{2}(b-a)\right| \leq \frac{(M-\mu)(\mu - m)}{2(M-m)}(b-a)^2.$$ The book suggested to compare $f(x)$ with piecewise linear functions, but I do not really know how to go about it. I tried using MVT since $\mu$ was in that form, but I do not know how to proceed.","Suppose $f(x)$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Suppose $m \leq f' \leq M$ on $(a, b)$ and denote $\mu = \dfrac{f(b) - f(a)}{b-a}$. Prove that  $$\left|\int_{a}^{b} f(x) \,\mathrm{d}x  - \frac{f(a) + f(b)}{2}(b-a)\right| \leq \frac{(M-\mu)(\mu - m)}{2(M-m)}(b-a)^2.$$ The book suggested to compare $f(x)$ with piecewise linear functions, but I do not really know how to go about it. I tried using MVT since $\mu$ was in that form, but I do not know how to proceed.",,"['real-analysis', 'integration']"
85,Prove that $\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt \xrightarrow{\epsilon \to 0}f(x) $,Prove that,\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt \xrightarrow{\epsilon \to 0}f(x) ,"Prove that, for any $x \in \mathbb{R}$ and $f \in (L^1\cap C)(\mathbb{R})$,   $$\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt \xrightarrow{\epsilon \to 0}f(x) $$ By substituting $u=\frac{\sqrt{\pi}}{\epsilon}(t-x)$, we get $du=\frac{\sqrt{\pi}}{\epsilon}dt$ and  \begin{align*} \int_{\mathbb{R}}\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt=\frac{\epsilon}{\sqrt{\pi}}\int_{\mathbb{R}}\exp\left(-u^2\right)du=\epsilon\\ \end{align*} Therefore \begin{align*} \frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\\ &=\frac{1}{\epsilon}\int_{\mathbb{R}}(f(t)-f(x))\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt\\ &=\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du\\ \end{align*} Suppose that $f$ is compactly supported. Let $[-a,a]$ be the compact support for $f$. Since $f$ is continuous, given $\eta > 0$, there exists $\delta > 0$ such that whenever $|h| < \delta$, we have $|f(x+h)-f(x)| < \eta$. For all $\epsilon < \frac{\sqrt{\pi}\delta}{2a}$, we have $\left|\frac{\epsilon}{\sqrt{\pi}}u+x-x\right|=\frac{\epsilon |u|}{\sqrt{\pi}} < \frac{\epsilon 2a}{\sqrt{\pi}} < \delta$ giving us $\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right|< \eta$. Hence, for all $\epsilon < \frac{\sqrt{\pi}\delta}{2a}$ \begin{align*} \left|\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\right|&\le \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\right|\exp\left(-u^2\right)du\\ &\le\eta\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\exp(-u^2)du=\eta\\ \end{align*} Now, since compactly supported functions are dense in $L^1$, for any function $f \in L^1$, there exists a compactly supported function $h $ such that $\|f-h\|_1 < \frac{\eta\sqrt{\pi}}{3}$. Therefore, $$\left|\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\right|$$ $$\le  \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-h\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right)\right|\exp\left(-u^2\right)du$$ $$+\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(h\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-h(x)\right)\right|\exp(-u^2)du+\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}|h(x)-f(x)|\exp(-u^2)$$ I can make the first two integrals small. I am not able to make the third integral small. I am yet to use that $f$ is continuous. There is a way to do this problem using Dominated Convergence Theorem. I don't want to do that. (Using Hint) Let $g \in L^1(\mathbb{R})$. Let $E_n=[-n,n]$. Then $E_n \subset E_{n+1}$ for all $n$ and $\mathbb{R}=\cup_{n=1}^{\infty}E_n$. Therefore $$\int_{\mathbb{R}}|g|d\mu=\lim_ {n \to \infty}\int_{E_n}|f|d\mu$$ Let $\eta \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $$\left|\int_{\mathbb{R}}|g|d\mu-\int_{E_n}|g|d\mu\right|\lt \eta \implies \left|\int_{|x| \gt n}|g(x)|d\mu \right| \lt \eta$$  Since $f, e^{-u^{2}} \in L^1(\mathbb{R})$, there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $\int_{|u| \gt n}|f(u)|du \lt \frac{\sqrt{\pi}\eta}{4}$ and $\int_{|u| \gt n}e^{-u^2}du \lt \frac{\sqrt{\pi}\eta}{4|f(x)|}$. ` Now $$\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du=\frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du+\frac{1}{\sqrt{\pi}}\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du$$ Since $f$ is continuous at $x$, there exists a $\delta \gt 0$ such that whenever $|h| \lt \delta$, we have $|f(x+h)-f(x)| \lt \frac{\eta}{2}$. Thus, for $\epsilon \lt \pi\delta^2$, whenever $|u| \lt \frac{1}{\sqrt{\epsilon}}, |x+u\frac{\epsilon}{\sqrt{\pi}}-x| \lt \frac{\sqrt{\epsilon}}{\pi} \lt \delta$ and  \begin{align*} \frac{1}{\sqrt{\pi}}\left|\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du\right|\\&\le \frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\right|\exp\left(-u^2\right)du\\ &\le \frac{\eta}{2}\frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}e^{-u^2}du&(\text{ using continuity of f})\\ &\lt \frac{\eta}{2}\\ \end{align*} For $\epsilon \lt \frac{1}{n_0^2}, |u| \gt \frac{1}{\sqrt{\epsilon}} \gt n_0$ and $\left\{|u| \gt \frac{1}{\sqrt{\epsilon}}\right\} \subset \{|u| \gt n_0\}$. Therefore, \begin{align*} \frac{1}{\sqrt{\pi}}\left|\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)\right|du\\ &\le \frac{1}{\sqrt{\pi}}\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right|du +\frac{1}{\sqrt{\pi}}|f(x)|\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}e^{-u^2}du\\ &\le\frac{1}{\sqrt{\pi}}\int_{|u| \gt n_0}\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right|du +\frac{1}{\sqrt{\pi}}|f(x)|\int_{|u| \gt n_0}e^{-u^2}du\\ &\le \frac{\eta }{2}\\ \end{align*} Thus, for $\epsilon \lt \min\{\pi\delta^2,\frac{1}{n_0^2}\}$, we have the claim. Thanks for the help!!","Prove that, for any $x \in \mathbb{R}$ and $f \in (L^1\cap C)(\mathbb{R})$,   $$\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt \xrightarrow{\epsilon \to 0}f(x) $$ By substituting $u=\frac{\sqrt{\pi}}{\epsilon}(t-x)$, we get $du=\frac{\sqrt{\pi}}{\epsilon}dt$ and  \begin{align*} \int_{\mathbb{R}}\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt=\frac{\epsilon}{\sqrt{\pi}}\int_{\mathbb{R}}\exp\left(-u^2\right)du=\epsilon\\ \end{align*} Therefore \begin{align*} \frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\\ &=\frac{1}{\epsilon}\int_{\mathbb{R}}(f(t)-f(x))\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt\\ &=\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du\\ \end{align*} Suppose that $f$ is compactly supported. Let $[-a,a]$ be the compact support for $f$. Since $f$ is continuous, given $\eta > 0$, there exists $\delta > 0$ such that whenever $|h| < \delta$, we have $|f(x+h)-f(x)| < \eta$. For all $\epsilon < \frac{\sqrt{\pi}\delta}{2a}$, we have $\left|\frac{\epsilon}{\sqrt{\pi}}u+x-x\right|=\frac{\epsilon |u|}{\sqrt{\pi}} < \frac{\epsilon 2a}{\sqrt{\pi}} < \delta$ giving us $\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right|< \eta$. Hence, for all $\epsilon < \frac{\sqrt{\pi}\delta}{2a}$ \begin{align*} \left|\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\right|&\le \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\right|\exp\left(-u^2\right)du\\ &\le\eta\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\exp(-u^2)du=\eta\\ \end{align*} Now, since compactly supported functions are dense in $L^1$, for any function $f \in L^1$, there exists a compactly supported function $h $ such that $\|f-h\|_1 < \frac{\eta\sqrt{\pi}}{3}$. Therefore, $$\left|\frac{1}{\epsilon}\int_{\mathbb{R}}f(t).\exp\left(\frac{-\pi(x-t)^2}{\epsilon^2}\right)dt-f(x)\right|$$ $$\le  \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-h\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right)\right|\exp\left(-u^2\right)du$$ $$+\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left|\left(h\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-h(x)\right)\right|\exp(-u^2)du+\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}|h(x)-f(x)|\exp(-u^2)$$ I can make the first two integrals small. I am not able to make the third integral small. I am yet to use that $f$ is continuous. There is a way to do this problem using Dominated Convergence Theorem. I don't want to do that. (Using Hint) Let $g \in L^1(\mathbb{R})$. Let $E_n=[-n,n]$. Then $E_n \subset E_{n+1}$ for all $n$ and $\mathbb{R}=\cup_{n=1}^{\infty}E_n$. Therefore $$\int_{\mathbb{R}}|g|d\mu=\lim_ {n \to \infty}\int_{E_n}|f|d\mu$$ Let $\eta \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $$\left|\int_{\mathbb{R}}|g|d\mu-\int_{E_n}|g|d\mu\right|\lt \eta \implies \left|\int_{|x| \gt n}|g(x)|d\mu \right| \lt \eta$$  Since $f, e^{-u^{2}} \in L^1(\mathbb{R})$, there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $\int_{|u| \gt n}|f(u)|du \lt \frac{\sqrt{\pi}\eta}{4}$ and $\int_{|u| \gt n}e^{-u^2}du \lt \frac{\sqrt{\pi}\eta}{4|f(x)|}$. ` Now $$\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du=\frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du+\frac{1}{\sqrt{\pi}}\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du$$ Since $f$ is continuous at $x$, there exists a $\delta \gt 0$ such that whenever $|h| \lt \delta$, we have $|f(x+h)-f(x)| \lt \frac{\eta}{2}$. Thus, for $\epsilon \lt \pi\delta^2$, whenever $|u| \lt \frac{1}{\sqrt{\epsilon}}, |x+u\frac{\epsilon}{\sqrt{\pi}}-x| \lt \frac{\sqrt{\epsilon}}{\pi} \lt \delta$ and  \begin{align*} \frac{1}{\sqrt{\pi}}\left|\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)du\right|\\&\le \frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}\left|\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\right|\exp\left(-u^2\right)du\\ &\le \frac{\eta}{2}\frac{1}{\sqrt{\pi}}\int_{|u| \le \frac{1}{\sqrt{\epsilon}}}e^{-u^2}du&(\text{ using continuity of f})\\ &\lt \frac{\eta}{2}\\ \end{align*} For $\epsilon \lt \frac{1}{n_0^2}, |u| \gt \frac{1}{\sqrt{\epsilon}} \gt n_0$ and $\left\{|u| \gt \frac{1}{\sqrt{\epsilon}}\right\} \subset \{|u| \gt n_0\}$. Therefore, \begin{align*} \frac{1}{\sqrt{\pi}}\left|\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left(f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)-f(x)\right)\exp\left(-u^2\right)\right|du\\ &\le \frac{1}{\sqrt{\pi}}\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right|du +\frac{1}{\sqrt{\pi}}|f(x)|\int_{|u| \gt \frac{1}{\sqrt{\epsilon}}}e^{-u^2}du\\ &\le\frac{1}{\sqrt{\pi}}\int_{|u| \gt n_0}\left|f\left(\frac{\epsilon}{\sqrt{\pi}}u+x\right)\right|du +\frac{1}{\sqrt{\pi}}|f(x)|\int_{|u| \gt n_0}e^{-u^2}du\\ &\le \frac{\eta }{2}\\ \end{align*} Thus, for $\epsilon \lt \min\{\pi\delta^2,\frac{1}{n_0^2}\}$, we have the claim. Thanks for the help!!",,"['real-analysis', 'integration', 'measure-theory', 'dirac-delta']"
86,Spivak Calculus on Manifolds - Problem 3-18,Spivak Calculus on Manifolds - Problem 3-18,,"$\textbf{3.18}$ If $f: A \longrightarrow \mathbb{R}$ is non-negative and $\int_A f = 0$, show that $\{ x \in A : f(x) \neq 0 \}$ has measure $0$. Hint: prove that $\left\{ x \in A : f(x) > \frac{1}{n} \right\}$ has content $0$. I know that the set $\{ x \in A : f(x) \neq 0 \}$ has measure $0$. Hint: prove that $\{ x ; f(x) > \frac{1}{n} \}$ has content $0$ for each $n$, then the same set has measure $0$ and observing that $$\{ x \in A : f(x) \neq 0 \} = \bigcup_{i = 1}^{\infty} \left\{ x \in A : f(x) > \frac{1}{n} \right\}$$ i.e, $\{ x \in A : f(x) \neq 0 \}$ is a countable union of set with measure $0$, then $\{ x \in A : f(x) \neq 0 \}$ has measure $0$, therefore is sufficient prove the hint. I think to prove the hint I need to use the fact that a function $f: A \longrightarrow \mathbb{R}$ is discontinuous in $x \in A$ if and only if $o(f,x) > 0$ (theorem $1-10$ of Spivak's book) and use the fact that $f$ is integrable if and only if $B := \left\{ x \in A : f \  \text{is not continuous at} \ x \right\}$ has measure $0$ (theorem $3-8$ of Spivak's book). I'm trying relate that $f(x) > \frac{1}{n}$ with $o(f,x) > \frac{1}{n}$, but I don't know how to do this. I would like to receive a hint about how to do this. Thanks in advance! $\textbf{P.S.:}$ 1) $A \subset \mathbb{R}^n$ is a rectangle; 2) A set with content $0$ is an set $A$ such that for every $\varepsilon > 0$, we can find a finite cover $\{ U_1, \cdots, U_n \}$ of $A$ by closed rectangles such that $\sum_{i = 1}^{i = n} v(U_i) < \varepsilon$, where $v(U_i)$ is the volume of $U_i$. 3) The oscillation of $f$ at $x$ is defined by $o(f,x) := \lim_{\delta \rightarrow 0} \left[ \sup f \left( B(x, \delta \right) - \inf f \left( B(x, \delta) \right) \right]$.","$\textbf{3.18}$ If $f: A \longrightarrow \mathbb{R}$ is non-negative and $\int_A f = 0$, show that $\{ x \in A : f(x) \neq 0 \}$ has measure $0$. Hint: prove that $\left\{ x \in A : f(x) > \frac{1}{n} \right\}$ has content $0$. I know that the set $\{ x \in A : f(x) \neq 0 \}$ has measure $0$. Hint: prove that $\{ x ; f(x) > \frac{1}{n} \}$ has content $0$ for each $n$, then the same set has measure $0$ and observing that $$\{ x \in A : f(x) \neq 0 \} = \bigcup_{i = 1}^{\infty} \left\{ x \in A : f(x) > \frac{1}{n} \right\}$$ i.e, $\{ x \in A : f(x) \neq 0 \}$ is a countable union of set with measure $0$, then $\{ x \in A : f(x) \neq 0 \}$ has measure $0$, therefore is sufficient prove the hint. I think to prove the hint I need to use the fact that a function $f: A \longrightarrow \mathbb{R}$ is discontinuous in $x \in A$ if and only if $o(f,x) > 0$ (theorem $1-10$ of Spivak's book) and use the fact that $f$ is integrable if and only if $B := \left\{ x \in A : f \  \text{is not continuous at} \ x \right\}$ has measure $0$ (theorem $3-8$ of Spivak's book). I'm trying relate that $f(x) > \frac{1}{n}$ with $o(f,x) > \frac{1}{n}$, but I don't know how to do this. I would like to receive a hint about how to do this. Thanks in advance! $\textbf{P.S.:}$ 1) $A \subset \mathbb{R}^n$ is a rectangle; 2) A set with content $0$ is an set $A$ such that for every $\varepsilon > 0$, we can find a finite cover $\{ U_1, \cdots, U_n \}$ of $A$ by closed rectangles such that $\sum_{i = 1}^{i = n} v(U_i) < \varepsilon$, where $v(U_i)$ is the volume of $U_i$. 3) The oscillation of $f$ at $x$ is defined by $o(f,x) := \lim_{\delta \rightarrow 0} \left[ \sup f \left( B(x, \delta \right) - \inf f \left( B(x, \delta) \right) \right]$.",,"['calculus', 'integration', 'multivariable-calculus']"
87,"Rewrite $\int_{0}^{1}\int_{-\sqrt{y-y^2}}^{0}(x^2+y^2)\,\text{d}x \, \text{d}y$ in polar",Rewrite  in polar,"\int_{0}^{1}\int_{-\sqrt{y-y^2}}^{0}(x^2+y^2)\,\text{d}x \, \text{d}y","Rewrite $$\int_0^1 \int_{-\sqrt{y-y^2}}^{0}(x^2+y^2)\,\text{d}x \, \text{d}y$$ in polar. We see that $-\sqrt{y-y^2}\leq x\leq 0$. If $x=-\sqrt{y-y^2}$, then $x^2=y-y^2\to x^2+y^2-y=0\to \color{red}{x^2+(y-\dfrac{1}{2})^2=\dfrac{1}{4}}$ Clearly $0\leq\theta\leq\pi$. Also, $x^2+y^2=y\to r^2\sin^2(\theta)+ r^2\cos^2(\theta)=r\sin(\theta)\to r^2=r\sin(\theta)\to\color{red}{r=\sin(\theta)}$ Therefore, $0\leq r \leq \sin(\theta)$ Thus the new integral is $$\int^\pi_0 \int^{\sin(\theta)}_0 r^2 r \, \mathrm{d}r \, \mathrm{d}\theta=\frac{3\pi}{32}$$ But wolfram alpha says the original integral in cartesian coordinates evaluates to $\dfrac{3\pi}{64}$?","Rewrite $$\int_0^1 \int_{-\sqrt{y-y^2}}^{0}(x^2+y^2)\,\text{d}x \, \text{d}y$$ in polar. We see that $-\sqrt{y-y^2}\leq x\leq 0$. If $x=-\sqrt{y-y^2}$, then $x^2=y-y^2\to x^2+y^2-y=0\to \color{red}{x^2+(y-\dfrac{1}{2})^2=\dfrac{1}{4}}$ Clearly $0\leq\theta\leq\pi$. Also, $x^2+y^2=y\to r^2\sin^2(\theta)+ r^2\cos^2(\theta)=r\sin(\theta)\to r^2=r\sin(\theta)\to\color{red}{r=\sin(\theta)}$ Therefore, $0\leq r \leq \sin(\theta)$ Thus the new integral is $$\int^\pi_0 \int^{\sin(\theta)}_0 r^2 r \, \mathrm{d}r \, \mathrm{d}\theta=\frac{3\pi}{32}$$ But wolfram alpha says the original integral in cartesian coordinates evaluates to $\dfrac{3\pi}{64}$?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
88,What's the derivation of this integral formula?,What's the derivation of this integral formula?,,I was searching around the web for some information about integrals and I came across the formula: $$\int_{-\infty}^\infty \frac{\ln(x^2)e^{\frac{-x^2}{2\sigma}}}{(2\pi)^\frac{1}{2}\sigma}dx= \ln(\sigma^2)-\gamma-\ln(2)$$ $\gamma =$ the Euler-Mascheroni Constant I'm very unsure where the Euler-Mascheroni constant came from. I tried rearranging the integral to simpler terms but I end up getting: $$\int_{-\infty}^\infty \ln|x|e^{-x^2}dx$$ which isn't overtly integrable. Where does this formula come from?,I was searching around the web for some information about integrals and I came across the formula: $$\int_{-\infty}^\infty \frac{\ln(x^2)e^{\frac{-x^2}{2\sigma}}}{(2\pi)^\frac{1}{2}\sigma}dx= \ln(\sigma^2)-\gamma-\ln(2)$$ $\gamma =$ the Euler-Mascheroni Constant I'm very unsure where the Euler-Mascheroni constant came from. I tried rearranging the integral to simpler terms but I end up getting: $$\int_{-\infty}^\infty \ln|x|e^{-x^2}dx$$ which isn't overtly integrable. Where does this formula come from?,,"['calculus', 'integration', 'definite-integrals']"
89,Trouble in proving $\int_0^\infty x^3\frac{\sin\frac{\pi x}{2}\sinh\frac{\pi x}{2}}{\cos\pi x+\cosh\pi x}(J_0(x)+I_0(x))dx=0$,Trouble in proving,\int_0^\infty x^3\frac{\sin\frac{\pi x}{2}\sinh\frac{\pi x}{2}}{\cos\pi x+\cosh\pi x}(J_0(x)+I_0(x))dx=0,"I want to prove that $$ \int_0^\infty x^3\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}(J_0(x)+I_0(x))dx=0,\tag{1} $$ where $J_0(x)$ and $I_0(x)$ are Bessel function and modified Bessel function of the first kind. What I know? This $$ \int_0^\infty x^{4n-1}\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}dx=0,\,\,\,\,\, n\in\mathbb{N},\tag{2} $$ and this $$ J_0(x)+I_0(x)=\sum_{n=0}^\infty a_nx^{4n}.\tag{3} $$ However the trouble is I don't know how to combine $(2)$ and $(3)$ rigorously to get $(1)$. For example this is not true: $$ \int_0^\infty x^3\cos x^2\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}dx=0.\,\,\,\,\,(\text{wrong})\tag{4} $$ Q: Explain rigorously why $(1)$ is true and $(4)$ is wrong. Note. I believe closed forms of $(1)$ and $(4)$ can be derived by residue theorem. However in answering this question I want to avoid explicit calculations and understand how to get from $(2)$ and $(3)$ to $(1)$ and also to understand why the same reasoning fails in case of $(4)$.","I want to prove that $$ \int_0^\infty x^3\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}(J_0(x)+I_0(x))dx=0,\tag{1} $$ where $J_0(x)$ and $I_0(x)$ are Bessel function and modified Bessel function of the first kind. What I know? This $$ \int_0^\infty x^{4n-1}\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}dx=0,\,\,\,\,\, n\in\mathbb{N},\tag{2} $$ and this $$ J_0(x)+I_0(x)=\sum_{n=0}^\infty a_nx^{4n}.\tag{3} $$ However the trouble is I don't know how to combine $(2)$ and $(3)$ rigorously to get $(1)$. For example this is not true: $$ \int_0^\infty x^3\cos x^2\frac{\sin\frac{\pi  x}{2}\sinh\frac{\pi  x}{2}}{\cos\pi x+\cosh\pi x}dx=0.\,\,\,\,\,(\text{wrong})\tag{4} $$ Q: Explain rigorously why $(1)$ is true and $(4)$ is wrong. Note. I believe closed forms of $(1)$ and $(4)$ can be derived by residue theorem. However in answering this question I want to avoid explicit calculations and understand how to get from $(2)$ and $(3)$ to $(1)$ and also to understand why the same reasoning fails in case of $(4)$.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
90,Changing limits when switching order of integration.,Changing limits when switching order of integration.,,"Consider the following Lebesgue integral:   $$ \int_{0}^{\infty} \int_{|f|>y} g(y) \: |f(x)| \: \mathrm{d}x \: \mathrm{d}y , $$   where g is a non-negative measurable function defined on $ (0,\infty) $ and $ f $ is a measurable function defined on $ \mathbb{R}^n $. If I am correct, the Tonelli's version of the Fubini theorem allows us to switch the order of integration. Question: What will be the new limits of integration? My attempt: \begin{align} \{(x,y) : |f(x)|>y, \: y \in (0,\infty)\} &= \{(x,y) : x \in \mathbb{R}^n, \: |f(x)|>y, \: y \in (0,\infty)\} \\ &= \{(x,y) : x \in \mathbb{R}^n, \: |f(x)|>y>0\} . \end{align} Hence, the above double integral equals $$ \int_{\mathbb{R}^n} \int_{0}^{|f|} g(y) \: |f(x)| \: \mathrm{d}y \: \mathrm{d}x . $$ While I have no doubt that the the two integrals are indeed equal, I'm wondering whether the set-theoretic justification I gave is sufficient, or whether I missed some subtlety? Many thanks in advance! :)","Consider the following Lebesgue integral:   $$ \int_{0}^{\infty} \int_{|f|>y} g(y) \: |f(x)| \: \mathrm{d}x \: \mathrm{d}y , $$   where g is a non-negative measurable function defined on $ (0,\infty) $ and $ f $ is a measurable function defined on $ \mathbb{R}^n $. If I am correct, the Tonelli's version of the Fubini theorem allows us to switch the order of integration. Question: What will be the new limits of integration? My attempt: \begin{align} \{(x,y) : |f(x)|>y, \: y \in (0,\infty)\} &= \{(x,y) : x \in \mathbb{R}^n, \: |f(x)|>y, \: y \in (0,\infty)\} \\ &= \{(x,y) : x \in \mathbb{R}^n, \: |f(x)|>y>0\} . \end{align} Hence, the above double integral equals $$ \int_{\mathbb{R}^n} \int_{0}^{|f|} g(y) \: |f(x)| \: \mathrm{d}y \: \mathrm{d}x . $$ While I have no doubt that the the two integrals are indeed equal, I'm wondering whether the set-theoretic justification I gave is sufficient, or whether I missed some subtlety? Many thanks in advance! :)",,"['real-analysis', 'integration', 'lebesgue-integral', 'lebesgue-measure']"
91,Iterated integrals,Iterated integrals,,"Is there any example of a function $f:[a,b]\times[c,d]\to \mathbb R$ so that $\int_a^b\int_c^d f(x,y)\,dy\,dx$ and $\int_c^d\int_a^b f(x,y)\,dx\,dy$ exist and are equal but $\int\int f(x,y)\,dy\,dx$ does not exist in $[a,b]\times[c,d]$? I've been trying to find an example but I have nothing so far.","Is there any example of a function $f:[a,b]\times[c,d]\to \mathbb R$ so that $\int_a^b\int_c^d f(x,y)\,dy\,dx$ and $\int_c^d\int_a^b f(x,y)\,dx\,dy$ exist and are equal but $\int\int f(x,y)\,dy\,dx$ does not exist in $[a,b]\times[c,d]$? I've been trying to find an example but I have nothing so far.",,"['integration', 'multivariable-calculus', 'multiple-integral']"
92,"Given a continuous function which it's improper integral converges, prove/disprove $\lim_{x\to \infty}f(x)=0$","Given a continuous function which it's improper integral converges, prove/disprove",\lim_{x\to \infty}f(x)=0,"Prove/Disprove: $(1)$ If $f$ is a continuous function such that $f > 0, \forall x \in [0,\infty)$ and $\int_0^\infty f $ converges, then $\lim_{x\to \infty}f(x)=0.$ $(2)$ If $f$ is a continuous and monotonic decreasing function such that $f \geq 0, \forall x \in [0,\infty)$ and $\int_0^\infty f $ converges, then $\lim_{x\to \infty}f(x)=0.$ I think $(1)$ is false, but I couldn't find a counter example. I think $(2)$ is true, tried to prove with Cauchy's criterion for improper integrals and the definition of the limit, but got stuck. Any help is appreciated.","Prove/Disprove: If is a continuous function such that and converges, then If is a continuous and monotonic decreasing function such that and converges, then I think is false, but I couldn't find a counter example. I think is true, tried to prove with Cauchy's criterion for improper integrals and the definition of the limit, but got stuck. Any help is appreciated.","(1) f f > 0, \forall x \in [0,\infty) \int_0^\infty f  \lim_{x\to \infty}f(x)=0. (2) f f \geq 0, \forall x \in [0,\infty) \int_0^\infty f  \lim_{x\to \infty}f(x)=0. (1) (2)","['real-analysis', 'integration', 'improper-integrals']"
93,How to solve this integral using Gamma Functions,How to solve this integral using Gamma Functions,,"I have to evaluate following integral. $$ \int_{-c}^c \sqrt{b+\frac{c^6}{x^6-c^6}}dx$$ I know that final answer includes gamma function entries like ($\frac{\Gamma(1/6)\Gamma(1/2)}{\Gamma(2/3)})$. But, I'm having difficulty figuring out how to integrate this. Any help will be highly appreciated. Thank You,","I have to evaluate following integral. $$ \int_{-c}^c \sqrt{b+\frac{c^6}{x^6-c^6}}dx$$ I know that final answer includes gamma function entries like ($\frac{\Gamma(1/6)\Gamma(1/2)}{\Gamma(2/3)})$. But, I'm having difficulty figuring out how to integrate this. Any help will be highly appreciated. Thank You,",,"['integration', 'definite-integrals', 'improper-integrals', 'gamma-function']"
94,Evans' PDE -- mean-value property for heat equation,Evans' PDE -- mean-value property for heat equation,,"To quote from Evans' PDE book pg 53, Let $u \in C_1^2(U_T)$ solve the heat equation. Then \begin{equation} u(x, t) = \frac{1}{4 r^n} \int \int_{E(x, t; r)} u(y, s) \frac{|x - y|^2}{(t - s)^2} dy ds. \end{equation} In the proof of this theorem, the book says, ""Upon mollifying if necessary, we may assume $u$ is smooth."" I'm not sure how this is done (do we just replace $u$ everywhere with $u^\epsilon$? But then does the theorem statement have to be revised to have $u^\epsilon$ instead of $u$?) or why this is necessary (we already are given that $u \in C_1^2 (U_T)$).","To quote from Evans' PDE book pg 53, Let $u \in C_1^2(U_T)$ solve the heat equation. Then \begin{equation} u(x, t) = \frac{1}{4 r^n} \int \int_{E(x, t; r)} u(y, s) \frac{|x - y|^2}{(t - s)^2} dy ds. \end{equation} In the proof of this theorem, the book says, ""Upon mollifying if necessary, we may assume $u$ is smooth."" I'm not sure how this is done (do we just replace $u$ everywhere with $u^\epsilon$? But then does the theorem statement have to be revised to have $u^\epsilon$ instead of $u$?) or why this is necessary (we already are given that $u \in C_1^2 (U_T)$).",,"['calculus', 'integration', 'partial-differential-equations', 'heat-equation']"
95,Get an approximation of $\int_0^1\int_0^1\frac{1+x+y^2+x^3+\ldots}{1+y+x^2+y^3+\ldots}dxdy$,Get an approximation of,\int_0^1\int_0^1\frac{1+x+y^2+x^3+\ldots}{1+y+x^2+y^3+\ldots}dxdy,"I am interested in to know how get an approximation of the double integral defined as limit of these $$\int_0^1\int_0^1\frac{1+x}{1+y}dxdy\,,$$ $$\int_0^1\int_0^1\frac{1+x+y^2}{1+y+x^2}dxdy\,,$$ $$\int_0^1\int_0^1\frac{1+x+y^2+x^3}{1+y+x^2+y^3}dxdy\,,\ldots$$ Question. Provide me an approximation of the definite integral defined as limit of previous sequence. Thanks in advance. Using the formula to get the sum of a geometric series I know how get in closed-form the integrand. Addtitionally I presume that a it's possible to get a closed-form of it, since Wolfram Alpha can deduce the antiderivatives. Any case even the evaluation of the integral limits seem to me now complicated.","I am interested in to know how get an approximation of the double integral defined as limit of these $$\int_0^1\int_0^1\frac{1+x}{1+y}dxdy\,,$$ $$\int_0^1\int_0^1\frac{1+x+y^2}{1+y+x^2}dxdy\,,$$ $$\int_0^1\int_0^1\frac{1+x+y^2+x^3}{1+y+x^2+y^3}dxdy\,,\ldots$$ Question. Provide me an approximation of the definite integral defined as limit of previous sequence. Thanks in advance. Using the formula to get the sum of a geometric series I know how get in closed-form the integrand. Addtitionally I presume that a it's possible to get a closed-form of it, since Wolfram Alpha can deduce the antiderivatives. Any case even the evaluation of the integral limits seem to me now complicated.",,['integration']
96,"Let $f$ be a non-negative differentiable function on $[0,1]$.",Let  be a non-negative differentiable function on .,"f [0,1]","Let $f$ be a non-negative differentiable function on $[0,1]$ such that $\int_{0}^{x} \sqrt {1-\{f'(t)\}^2}\ dt = \int_{0}^{x} f(t)\ dt$, $0 \le x \le 1$ and $f(0)=0$. Then which of the following is true? $(1)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(2)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(3)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. $(4)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. I have first differentiated the above given expression and obtained $\{f(t)\}^2 + \{f'(t)\}^2=1$ for all $t \in [0,1].$ Clearly from this expression it follows that $|f(t)| \le 1$ and $|f'(t)| \le 1$ for all $t \in [0,1].$ Since $f$ is non-negative on $[0,1]$ so we have $0 \le f(t) \le 1$ for all $t \in [0,1].$ Now let us construct a function $g$ on $[0,1]$ defined by $g(t)=f(t)-t$ , $t \in [0,1]$.Then clearly $g$ is differentiable on $[0,1]$ since $f$ is so. Now $g'(t)=f'(t)-1$ , for all $ t \in [0,1]$. Since $|f'(t)| \le 1$ for all $t \in [0,1]$ so we have $-2 \le g'(t) \le 0$ for all $t \in [0,1].$ This shows that the function $g$ is monotonic decreasing on $[0,1].$ So $g(\frac {1} {2}) \le g(0)=0$ and $g(\frac {1} {3}) \le g(0)=0$ since it is given that $f(0)=0.$ So we have $f(\frac {1} {2}) \le \frac {1} {2}$ and $f(\frac {1} {3}) \le \frac {1} {3}.$ Now from the above fact it is clear that $(1), (2)$ and $(4)$ are all false. Hence I think $(3)$ is the correct option. But I have confused about the strict inequality in option $(3)$ which I have failed to found. Please help me in this regard and also please mention the reason for holding strict inequality. Thank you in advance.","Let $f$ be a non-negative differentiable function on $[0,1]$ such that $\int_{0}^{x} \sqrt {1-\{f'(t)\}^2}\ dt = \int_{0}^{x} f(t)\ dt$, $0 \le x \le 1$ and $f(0)=0$. Then which of the following is true? $(1)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(2)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) > \frac {1} {3}$. $(3)$ $f(\frac {1} {2}) < \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. $(4)$ $f(\frac {1} {2}) > \frac {1} {2}$ and $f(\frac {1} {3}) < \frac {1} {3}$. I have first differentiated the above given expression and obtained $\{f(t)\}^2 + \{f'(t)\}^2=1$ for all $t \in [0,1].$ Clearly from this expression it follows that $|f(t)| \le 1$ and $|f'(t)| \le 1$ for all $t \in [0,1].$ Since $f$ is non-negative on $[0,1]$ so we have $0 \le f(t) \le 1$ for all $t \in [0,1].$ Now let us construct a function $g$ on $[0,1]$ defined by $g(t)=f(t)-t$ , $t \in [0,1]$.Then clearly $g$ is differentiable on $[0,1]$ since $f$ is so. Now $g'(t)=f'(t)-1$ , for all $ t \in [0,1]$. Since $|f'(t)| \le 1$ for all $t \in [0,1]$ so we have $-2 \le g'(t) \le 0$ for all $t \in [0,1].$ This shows that the function $g$ is monotonic decreasing on $[0,1].$ So $g(\frac {1} {2}) \le g(0)=0$ and $g(\frac {1} {3}) \le g(0)=0$ since it is given that $f(0)=0.$ So we have $f(\frac {1} {2}) \le \frac {1} {2}$ and $f(\frac {1} {3}) \le \frac {1} {3}.$ Now from the above fact it is clear that $(1), (2)$ and $(4)$ are all false. Hence I think $(3)$ is the correct option. But I have confused about the strict inequality in option $(3)$ which I have failed to found. Please help me in this regard and also please mention the reason for holding strict inequality. Thank you in advance.",,"['real-analysis', 'integration', 'derivatives']"
97,"Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$","Theorem 6.12 (c) in Baby Rudin: If  on  and , then  on  and","f\in\mathscr{R}(\alpha) [a, b] a<c<b f\in\mathscr{R}(\alpha) [a, c] [c, b]","Here is Theorem 6.12 (c) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a < c < b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and    $$ \int_a^c f d \alpha + \int_c^b f d \alpha  =  \int_a^b f d \alpha. $$ Here is my proof: Let $\varepsilon > 0$ be given. As $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so we can find a partition $P$ of $[a, b]$ such that    $$ U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. $$   Let $Q$ be any refinement of $P$ such that $Q$ also contains the point $c$.    Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have    $$ L(P, f, \alpha ) \leq L(Q, f, \alpha) \leq U(Q, f, \alpha) \leq U(P, f, \alpha), $$   and so    $$ U(Q, f, \alpha) - L(Q, f, \alpha) \leq  U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. \tag{1} $$   Let $$ Q = \left\{ x_0, \ldots, x_{k-1}, c, x_k, \ldots, x_n \ \right\},$$   where    $$ a = x_0 < \cdots < x_{k-1} < c < x_k < \cdots < x_n = b.$$   Let $$Q_1 \colon= \left\{ \ x_0, \ldots, x_{k-1}, c \ \right\}, \qquad Q_2 \colon= \left\{\ c, x_k, \ldots, x_n \ \right\}.$$    Then $Q_1$ and $Q_2$ are partitions, respectively, of $[a, c]$ and $[c, b]$, and $$ Q = Q_1 \cup Q_2. $$   Also   $$ L(Q, f, \alpha) = L\left( Q_1, f, \alpha \right) + L\left( Q_2, f, \alpha \right), \tag{2}$$   and    $$ U(Q, f, \alpha) = U\left( Q_1, f, \alpha \right) + U\left( Q_2, f, \alpha \right), \tag{3}$$   where    $$ L\left( Q_1, f, \alpha \right) \colon= \sum_{i=1}^{k-1} \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right) + \left( \inf_{x_{k-1}\leq x\leq c} f(x) \right) \left( \alpha (c) - \alpha \left( x_{k-1} \right) \right), $$   and    $$L\left( Q_2, f, \alpha \right) \colon= \left( \inf_{c\leq x\leq x_k} f(x) \right) \left( \alpha \left(x_k \right) - \alpha(c) \right) +  \sum_{i=k+1}^n  \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right), $$   and similarly for $U\left( Q_1, f, \alpha \right)$ and $U\left( Q_2, f, \alpha \right)$. Moreover, for each $j= 1, 2$,    $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \geq 0, $$   which together with (1) implies that, for each $j = 1, 2$,    $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \leq U (Q, f, \alpha ) - L(Q, f, \alpha) < \varepsilon, $$   from which it follows that $f$ is Riemann-integrable with respect to $\alpha$ on $[a, c]$ and on $[c, b]$. And, from (1)and (2) above we obtain    \begin{align} \int_a^b f d \alpha &\leq U(Q, f, \alpha ) \\  &< L(Q, f, \alpha) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\ &= L\left(Q_1, f, \alpha \right) + L \left( Q_2, f, \alpha \right) + \varepsilon \qquad \mbox{ [ by (2) above ] } \\ &\leq \int_a^c f d \alpha + \int_c^b f d \alpha + \varepsilon  \end{align}   for every real number $\varepsilon > 0$, which implies that    $$ \int_a^b f d\alpha \leq \int_a^c f d\alpha + \int_c^b f d \alpha. \tag{A}$$ Now from (1) and (3) above, we obtain    \begin{align} \int_a^c f d \alpha + \int_c^b f d \alpha &\leq U \left( Q_1, f, \alpha \right) + U \left( Q_2, f, \alpha \right) \\ &= U(Q, f, \alpha ) \qquad \mbox{ [ by (3) above ] } \\ &< L(Q, f, \alpha ) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\ &\leq \int_a^b f d \alpha + \varepsilon \end{align}   for every real number $\varepsilon > 0$, which implies that    $$ \int_a^c f d \alpha + \int_c^b f d \alpha \leq  \int_a^b f d \alpha. \tag{B}$$    From (A) and (B), we conclude that    $$ \int_a^c f d \alpha + \int_c^b f d \alpha = \int_a^b f d \alpha, $$   as required. Is the above proof correct (and as required by Rudin)? If so, then is my presentation good enough too? If not, then where lie the pitfalls?","Here is Theorem 6.12 (c) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $a < c < b$, then $f \in \mathscr{R}(\alpha)$ on $[a, c]$ and on $[c, b]$, and    $$ \int_a^c f d \alpha + \int_c^b f d \alpha  =  \int_a^b f d \alpha. $$ Here is my proof: Let $\varepsilon > 0$ be given. As $f \in \mathscr{R}(\alpha)$ on $[a, b]$, so we can find a partition $P$ of $[a, b]$ such that    $$ U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. $$   Let $Q$ be any refinement of $P$ such that $Q$ also contains the point $c$.    Then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have    $$ L(P, f, \alpha ) \leq L(Q, f, \alpha) \leq U(Q, f, \alpha) \leq U(P, f, \alpha), $$   and so    $$ U(Q, f, \alpha) - L(Q, f, \alpha) \leq  U(P, f, \alpha ) - L(P, f, \alpha) < \varepsilon. \tag{1} $$   Let $$ Q = \left\{ x_0, \ldots, x_{k-1}, c, x_k, \ldots, x_n \ \right\},$$   where    $$ a = x_0 < \cdots < x_{k-1} < c < x_k < \cdots < x_n = b.$$   Let $$Q_1 \colon= \left\{ \ x_0, \ldots, x_{k-1}, c \ \right\}, \qquad Q_2 \colon= \left\{\ c, x_k, \ldots, x_n \ \right\}.$$    Then $Q_1$ and $Q_2$ are partitions, respectively, of $[a, c]$ and $[c, b]$, and $$ Q = Q_1 \cup Q_2. $$   Also   $$ L(Q, f, \alpha) = L\left( Q_1, f, \alpha \right) + L\left( Q_2, f, \alpha \right), \tag{2}$$   and    $$ U(Q, f, \alpha) = U\left( Q_1, f, \alpha \right) + U\left( Q_2, f, \alpha \right), \tag{3}$$   where    $$ L\left( Q_1, f, \alpha \right) \colon= \sum_{i=1}^{k-1} \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right) + \left( \inf_{x_{k-1}\leq x\leq c} f(x) \right) \left( \alpha (c) - \alpha \left( x_{k-1} \right) \right), $$   and    $$L\left( Q_2, f, \alpha \right) \colon= \left( \inf_{c\leq x\leq x_k} f(x) \right) \left( \alpha \left(x_k \right) - \alpha(c) \right) +  \sum_{i=k+1}^n  \left( \inf_{x_{i-1}\leq x \leq x_i} f(x) \right) \left( \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \right), $$   and similarly for $U\left( Q_1, f, \alpha \right)$ and $U\left( Q_2, f, \alpha \right)$. Moreover, for each $j= 1, 2$,    $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \geq 0, $$   which together with (1) implies that, for each $j = 1, 2$,    $$ U \left( Q_j, f, \alpha \right) -  L \left( Q_j, f, \alpha \right) \leq U (Q, f, \alpha ) - L(Q, f, \alpha) < \varepsilon, $$   from which it follows that $f$ is Riemann-integrable with respect to $\alpha$ on $[a, c]$ and on $[c, b]$. And, from (1)and (2) above we obtain    \begin{align} \int_a^b f d \alpha &\leq U(Q, f, \alpha ) \\  &< L(Q, f, \alpha) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\ &= L\left(Q_1, f, \alpha \right) + L \left( Q_2, f, \alpha \right) + \varepsilon \qquad \mbox{ [ by (2) above ] } \\ &\leq \int_a^c f d \alpha + \int_c^b f d \alpha + \varepsilon  \end{align}   for every real number $\varepsilon > 0$, which implies that    $$ \int_a^b f d\alpha \leq \int_a^c f d\alpha + \int_c^b f d \alpha. \tag{A}$$ Now from (1) and (3) above, we obtain    \begin{align} \int_a^c f d \alpha + \int_c^b f d \alpha &\leq U \left( Q_1, f, \alpha \right) + U \left( Q_2, f, \alpha \right) \\ &= U(Q, f, \alpha ) \qquad \mbox{ [ by (3) above ] } \\ &< L(Q, f, \alpha ) + \varepsilon \qquad \mbox{ [ by (1) above ] } \\ &\leq \int_a^b f d \alpha + \varepsilon \end{align}   for every real number $\varepsilon > 0$, which implies that    $$ \int_a^c f d \alpha + \int_c^b f d \alpha \leq  \int_a^b f d \alpha. \tag{B}$$    From (A) and (B), we conclude that    $$ \int_a^c f d \alpha + \int_c^b f d \alpha = \int_a^b f d \alpha, $$   as required. Is the above proof correct (and as required by Rudin)? If so, then is my presentation good enough too? If not, then where lie the pitfalls?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
98,Why use a rectangular contour to evaluate $\int_\mathbb{R}\frac{e^{-i\sigma x}}{e^{ax}+e^{-bx}}dx$,Why use a rectangular contour to evaluate,\int_\mathbb{R}\frac{e^{-i\sigma x}}{e^{ax}+e^{-bx}}dx,"I am trying to tackle the following integral: $$\int_\mathbb{R}\frac{e^{-i\sigma x}}{e^{ax}+e^{-bx}}dx$$ I am told that I should use a complex rectangular contour to evaluate this integral however I am unsure on how to proceed. Also I would like to know what characteristic of this integral hints on the type of contour that I must use and the type of complex integral I should consider. Any help is appreciated. EDIT: $a,b>0$ and $\sigma \in \mathbb{R}$","I am trying to tackle the following integral: $$\int_\mathbb{R}\frac{e^{-i\sigma x}}{e^{ax}+e^{-bx}}dx$$ I am told that I should use a complex rectangular contour to evaluate this integral however I am unsure on how to proceed. Also I would like to know what characteristic of this integral hints on the type of contour that I must use and the type of complex integral I should consider. Any help is appreciated. EDIT: $a,b>0$ and $\sigma \in \mathbb{R}$",,"['integration', 'complex-analysis']"
99,How do we show that $\int_{0}^{1}{\ln^k(x)\over x}\ln\left(1-\sqrt[n]{x}\right)\mathrm dx=(-n)^{k+1}k!\zeta(k+2)?$,How do we show that,\int_{0}^{1}{\ln^k(x)\over x}\ln\left(1-\sqrt[n]{x}\right)\mathrm dx=(-n)^{k+1}k!\zeta(k+2)?,"Proposed: A simple closed form $$\int_{0}^{1}{\ln^k(x)\over x}\ln\left(1-\sqrt[n]{x}\right)\mathrm dx=(-n)^{k+1}k!\zeta(k+2)\tag1$$ Where $n,k=1,2,3,\cdots$ My try: $u=\sqrt[n]{x}\implies {nx\over u}du=dx$, $x=u^n$, then $(1)$ becomes $$n^{k+1}\int_{0}^{1}{\ln^k(u)\over u}\ln(1-u)\mathrm du\tag2$$ $$-n^{k+1}\sum_{v\ge1}{1\over v}\int_{0}^{1}u^v\ln^k(u)\mathrm du\tag3$$ $$-n^{k+1}\sum_{v\ge1}{1\over v}\cdot{(-1)^kk!\over (v+1)^{k+1}}\tag4$$ $$(-n)^{k+1}k!\sum_{v\ge1}{1\over v(v+1)^{k+1}}\tag5$$ How may we prove $(1)?$","Proposed: A simple closed form $$\int_{0}^{1}{\ln^k(x)\over x}\ln\left(1-\sqrt[n]{x}\right)\mathrm dx=(-n)^{k+1}k!\zeta(k+2)\tag1$$ Where $n,k=1,2,3,\cdots$ My try: $u=\sqrt[n]{x}\implies {nx\over u}du=dx$, $x=u^n$, then $(1)$ becomes $$n^{k+1}\int_{0}^{1}{\ln^k(u)\over u}\ln(1-u)\mathrm du\tag2$$ $$-n^{k+1}\sum_{v\ge1}{1\over v}\int_{0}^{1}u^v\ln^k(u)\mathrm du\tag3$$ $$-n^{k+1}\sum_{v\ge1}{1\over v}\cdot{(-1)^kk!\over (v+1)^{k+1}}\tag4$$ $$(-n)^{k+1}k!\sum_{v\ge1}{1\over v(v+1)^{k+1}}\tag5$$ How may we prove $(1)?$",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
