,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Level set corresponding to the value $0$ for $f(x,y,z)=y^3-z^3$ is a smooth surface?",Level set corresponding to the value  for  is a smooth surface?,"0 f(x,y,z)=y^3-z^3","I was attempting exercises in Vector Calculus by Peter Baxandall. Definition $3.8.11$ : A set $S \subseteq \Bbb R^m$ is called a smooth surface if $S$ is a level set of a $C^1$ function $f: D \subseteq \Bbb R^m \rightarrow \Bbb R~$ s.t $~\text {grad}~ f(x) \ne 0~\forall~x \in S$ My Argument: We can see that $S$ is the level set corresponding to the value $0$ of the $C^1$ function $f:D \subseteq \Bbb R^m \rightarrow \Bbb R~|~f(x,y,z)=y^3-z^3.~~~\therefore~\text{grad}f(x,y,z) = \Big(0,3y^2,-3z^2\Big)$ . Clearly, $\text{grad}f(0,0,0)=0$ and $(0,0,0) \in S$ . Thus, $S$ cannot be a smooth surface. Why does $3.8.11$ not apply to the above function? Could someone point out any error in the above argument? Thanks a lot!","I was attempting exercises in Vector Calculus by Peter Baxandall. Definition : A set is called a smooth surface if is a level set of a function s.t My Argument: We can see that is the level set corresponding to the value of the function . Clearly, and . Thus, cannot be a smooth surface. Why does not apply to the above function? Could someone point out any error in the above argument? Thanks a lot!","3.8.11 S \subseteq \Bbb R^m S C^1 f: D \subseteq \Bbb R^m \rightarrow \Bbb R~ ~\text {grad}~ f(x) \ne 0~\forall~x \in S S 0 C^1 f:D \subseteq \Bbb R^m \rightarrow \Bbb R~|~f(x,y,z)=y^3-z^3.~~~\therefore~\text{grad}f(x,y,z) = \Big(0,3y^2,-3z^2\Big) \text{grad}f(0,0,0)=0 (0,0,0) \in S S 3.8.11","['multivariable-calculus', 'vector-analysis']"
1,How do you find the following multivariable limit? [duplicate],How do you find the following multivariable limit? [duplicate],,"This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed 3 years ago . How do you find the following limit: $$\lim_{x,y\to 0} \frac{|x||y|^4}{|x|^4+|y|^5}$$ I've tried to apply the squeeze theorem, but unlike a lot of other questions I encounter, the degree of $x$ in the numerator is too small for me to write the function as a constant times a function that goes to zero as $x,y \rightarrow 0$ .","This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed 3 years ago . How do you find the following limit: I've tried to apply the squeeze theorem, but unlike a lot of other questions I encounter, the degree of in the numerator is too small for me to write the function as a constant times a function that goes to zero as .","\lim_{x,y\to 0} \frac{|x||y|^4}{|x|^4+|y|^5} x x,y \rightarrow 0",['multivariable-calculus']
2,About a differential form in submanifolds of hyperboloid diffeomorphics to $S^1$,About a differential form in submanifolds of hyperboloid diffeomorphics to,S^1,"Let $$\omega=xz\,dx+yz\,dy-z^2\,dz$$ be a $1$ -form in $\Bbb{R}^3$ and let $H$ be the hyperboloid $x^2+y^2-z^2=1$ . I want to show that, if $M\subset H$ is a surface that is diffeomorphic to the sphere $S^1$ , so $\int_M \omega=0.$ My idea is to find a $0$ -form $\eta$ such that, in $M$ , $d\eta=\omega$ . My first try was $$\eta=z\frac{x^2}{2}+z\frac{y^2}{2}-\frac{z^3}{3}\implies  d\eta=xz\,dx+yz\,dy+\left(\frac{x^2+y^2-2z^2}{2}\right)\, dz $$ My hope was to use $x^2+y^2-z^2=1$ in the equation of $d\eta$ to cancel some terms and get $d\eta=\omega$ , but I didn't get it. What can I do?","Let be a -form in and let be the hyperboloid . I want to show that, if is a surface that is diffeomorphic to the sphere , so My idea is to find a -form such that, in , . My first try was My hope was to use in the equation of to cancel some terms and get , but I didn't get it. What can I do?","\omega=xz\,dx+yz\,dy-z^2\,dz 1 \Bbb{R}^3 H x^2+y^2-z^2=1 M\subset H S^1 \int_M \omega=0. 0 \eta M d\eta=\omega \eta=z\frac{x^2}{2}+z\frac{y^2}{2}-\frac{z^3}{3}\implies  d\eta=xz\,dx+yz\,dy+\left(\frac{x^2+y^2-2z^2}{2}\right)\, dz  x^2+y^2-z^2=1 d\eta d\eta=\omega","['multivariable-calculus', 'differential-forms']"
3,How to evaluate the double integral over a non-closed surface?,How to evaluate the double integral over a non-closed surface?,,"Let $\vec{F}=(x+2y)e^zi+(ye^z+x^2)j+y^2zk$ and let $S$ be the surface $x^2+y^2+z=1$ , $z\geq 0$ . If $\hat{n}$ is a unit normal to $S$ and $$\left|\iint_S(\nabla\times \vec{F})\cdot \hat{n}\, dS\right|=\alpha\pi.$$ Then $\alpha=?$ We can't apply Gauss divergence theorem here since the surface S is not closed. How to proceed in this question then? Please help.","Let and let be the surface , . If is a unit normal to and Then We can't apply Gauss divergence theorem here since the surface S is not closed. How to proceed in this question then? Please help.","\vec{F}=(x+2y)e^zi+(ye^z+x^2)j+y^2zk S x^2+y^2+z=1 z\geq 0 \hat{n} S \left|\iint_S(\nabla\times \vec{F})\cdot \hat{n}\, dS\right|=\alpha\pi. \alpha=?","['integration', 'multivariable-calculus', 'multiple-integral', 'divergence-theorem']"
4,Curl of $\frac{\hat r}{r^2}$ using two different coordinates,Curl of  using two different coordinates,\frac{\hat r}{r^2},"I am learning vector calculus. Here I wanted to take out the $\nabla\times(\frac{\hat r}{r^2})$ , So in spherical coordinates it is easy to take out. It is zero. but while doing in Cartesian coordinates $\begin{bmatrix}  \hat x & \hat y & \hat z \\  \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z}\\  \frac{1}{(x^2+y^2+z^2)} & \frac{1}{(x^2+y^2+z^2)} & \frac{1}{(x^2+y^2+z^2)} \\  \end{bmatrix} $ This on solving isn't coming to be zero. Why?","I am learning vector calculus. Here I wanted to take out the , So in spherical coordinates it is easy to take out. It is zero. but while doing in Cartesian coordinates This on solving isn't coming to be zero. Why?","\nabla\times(\frac{\hat r}{r^2}) \begin{bmatrix}
 \hat x & \hat y & \hat z \\
 \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z}\\
 \frac{1}{(x^2+y^2+z^2)} & \frac{1}{(x^2+y^2+z^2)} & \frac{1}{(x^2+y^2+z^2)} \\
 \end{bmatrix}
","['multivariable-calculus', 'polar-coordinates', 'vector-fields']"
5,Compute line integrals $\int_C\text{F}\cdot d\mathbb{x}$,Compute line integrals,\int_C\text{F}\cdot d\mathbb{x},"$\def\hl#1#2{\bbox[#1,1px]{#2}} \def\box#1#2#3#4#5{\color{#2}{\bbox[0px, border: 2px solid #2]{\hl{#3}{\color{white}{\color{#3}{\boxed{\underline{\large\color{#1}{\text{#4}}}\\\color{#1}{#5}\\}}}}}}} \def\verts#1{\left\vert#1\right\vert} \def\Verts#1{\left\Vert#1\right\Vert} \def\pra#1{\left(#1\right)} \def\R{\mathbb{R}}$ $\box{black}{black}{} {Question} {\text{Compute the following line integrals $\int_C\text{F}\cdot d\mathbb{x}$:}\\ \text{(a) F$(x,y)=(x^2,-y)$ and C is the graph of $y=e^x$ from $x=2$ to $x=1$}\\ \text{$(b)$ F$(x,y,z)=(z,-y,x)$ and C is line segment from $(5,0,2)$ to $(5,3,4)$}\\ \text{$(c)$ F$(x,y,z)=(x,y,z^2)$ and C is the intersection of cylinder $x^2+y^2=1$}\\ \text{and $z=x$ oriented counter-clockwise when viewed from above.}}$ My attempts $(a)$ Consider the parameterization i.e. $(x,y)=\left(1+t,e^{1+t}\right)$ , and $(dx,dy)=(1,e^{1+t})$ where $t\in[0,1]$ \begin{align} \int_C\text{F}\cdot d\text{x}=&\int_C x^2dx-ydy\\ =&\int_0^1(1+t)^2-e^{2+2t}dt\\ =&\frac{7}{3} + \frac{e^2-e^4}{2} \end{align} $(b)$ Let $(x,y,z)=(5,3t,2+2t)$ and $(dx,dy,dz)=(0,3,2)$ where $t\in[0,1]$ , have \begin{align} \int_C\text{F}\cdot d\text{x}=&\int_Czdx-ydy+xdz\\ =&\int_0^1-9t+10dt\\ =&\frac{11}{2} \end{align} $(c)$ Let $(x,y,z)=(\cos(t),\sin(t),\cos(t))$ , that $(dx,dy,dz)=(-\sin(t),\cos(t),-\sin(t))$ where $t\in[0,2\pi]$ \begin{align} \int_C\text{F}\cdot d\text{x}=&\int_Cxdx+ydy+z^2dz\\ =&\int_0^{2\pi}-\sin(t)\cos(t)+\sin(t)\cos(t)-\sin(t)\cos^2(t)dt\\ \vdots\\ =&0 \end{align} Another approach for $(c)$ might be use Stokes' Theorem. Let $S=\{(x,y,z)\in\R^3:x^2+y^2\le1,z=x\}$ , that C is the Stokes' boundary of S. \begin{align} \int_{C}\text{F}\cdot d\text{x}=&\iint_S\nabla\times\text{F}\cdot\text{n}dA\\ =&\iint_S(0,0,0)\cdot\text{n}dA\\ =&0 \end{align} Is my solutions correct?","My attempts Consider the parameterization i.e. , and where Let and where , have Let , that where Another approach for might be use Stokes' Theorem. Let , that C is the Stokes' boundary of S. Is my solutions correct?","\def\hl#1#2{\bbox[#1,1px]{#2}}
\def\box#1#2#3#4#5{\color{#2}{\bbox[0px, border: 2px solid #2]{\hl{#3}{\color{white}{\color{#3}{\boxed{\underline{\large\color{#1}{\text{#4}}}\\\color{#1}{#5}\\}}}}}}}
\def\verts#1{\left\vert#1\right\vert}
\def\Verts#1{\left\Vert#1\right\Vert}
\def\pra#1{\left(#1\right)}
\def\R{\mathbb{R}} \box{black}{black}{}
{Question}
{\text{Compute the following line integrals \int_C\text{F}\cdot d\mathbb{x}:}\\
\text{(a) F(x,y)=(x^2,-y) and C is the graph of y=e^x from x=2 to x=1}\\
\text{(b) F(x,y,z)=(z,-y,x) and C is line segment from (5,0,2) to (5,3,4)}\\
\text{(c) F(x,y,z)=(x,y,z^2) and C is the intersection of cylinder x^2+y^2=1}\\
\text{and z=x oriented counter-clockwise when viewed from above.}} (a) (x,y)=\left(1+t,e^{1+t}\right) (dx,dy)=(1,e^{1+t}) t\in[0,1] \begin{align}
\int_C\text{F}\cdot d\text{x}=&\int_C x^2dx-ydy\\
=&\int_0^1(1+t)^2-e^{2+2t}dt\\
=&\frac{7}{3} + \frac{e^2-e^4}{2}
\end{align} (b) (x,y,z)=(5,3t,2+2t) (dx,dy,dz)=(0,3,2) t\in[0,1] \begin{align}
\int_C\text{F}\cdot d\text{x}=&\int_Czdx-ydy+xdz\\
=&\int_0^1-9t+10dt\\
=&\frac{11}{2}
\end{align} (c) (x,y,z)=(\cos(t),\sin(t),\cos(t)) (dx,dy,dz)=(-\sin(t),\cos(t),-\sin(t)) t\in[0,2\pi] \begin{align}
\int_C\text{F}\cdot d\text{x}=&\int_Cxdx+ydy+z^2dz\\
=&\int_0^{2\pi}-\sin(t)\cos(t)+\sin(t)\cos(t)-\sin(t)\cos^2(t)dt\\
\vdots\\
=&0
\end{align} (c) S=\{(x,y,z)\in\R^3:x^2+y^2\le1,z=x\} \begin{align}
\int_{C}\text{F}\cdot d\text{x}=&\iint_S\nabla\times\text{F}\cdot\text{n}dA\\
=&\iint_S(0,0,0)\cdot\text{n}dA\\
=&0
\end{align}","['calculus', 'multivariable-calculus', 'definite-integrals', 'solution-verification', 'multiple-integral']"
6,Is there a proof for $\frac{\mathrm{d}y}{\mathrm{d}x} = - \frac{\frac{\partial }{\partial x}}{\frac{\partial }{\partial y}}$? [duplicate],Is there a proof for ? [duplicate],\frac{\mathrm{d}y}{\mathrm{d}x} = - \frac{\frac{\partial }{\partial x}}{\frac{\partial }{\partial y}},"This question already has an answer here : Proving $f' = - \frac{\partial{F}/\partial{x}}{\partial{F}/\partial{y}}$ (1 answer) Closed 3 years ago . I've read that $$\frac{\mathrm{d}y}{\mathrm{d}x} = - {\frac{\frac{\partial}{∂x}}{\frac{∂}{∂y}}}$$ For example, if $f(x,y) = 2xy + y^2,$ $$\frac{\partial f(x,y)}{\partial x} = 2y \ \ \text{ and }\ \ \frac{\partial f(x,y)}{\partial y} = 2x+2y, $$ so $\displaystyle \frac{\mathrm{d}y}{\mathrm{d}x} = -\frac{2y}{2x+2y}$ Is there a proof for this assertion? I can't seem to find one.","This question already has an answer here : Proving $f' = - \frac{\partial{F}/\partial{x}}{\partial{F}/\partial{y}}$ (1 answer) Closed 3 years ago . I've read that For example, if so Is there a proof for this assertion? I can't seem to find one.","\frac{\mathrm{d}y}{\mathrm{d}x} = - {\frac{\frac{\partial}{∂x}}{\frac{∂}{∂y}}} f(x,y) = 2xy + y^2, \frac{\partial f(x,y)}{\partial x} = 2y \ \ \text{ and }\ \ \frac{\partial f(x,y)}{\partial y} = 2x+2y,  \displaystyle \frac{\mathrm{d}y}{\mathrm{d}x} = -\frac{2y}{2x+2y}","['multivariable-calculus', 'derivatives', 'implicit-differentiation']"
7,Possibly a variation of the increment theorem for functions of multiple variables,Possibly a variation of the increment theorem for functions of multiple variables,,"Suppose that $F:\mathbb R^n\to\mathbb R$ is $C^\infty$ . I'd like to prove that $\forall a=(a_1,\ldots,a_n)\in\mathbb R^n$ , there exist $C^\infty$ functions $G_i,i=1\ldots,n,$ such that $\forall x=(x_1,\ldots,x_n)\in\mathbb R^n$ , we have $$F(x)=F(a)+\sum_{i=1}^n(x_i-a_i)G_i(x).$$ In fact, we have $$G_i(a)=\frac{\partial F}{\partial x_i}(a)$$ for all $i$ . This theorem is quoted in the appendix to my physics textbook, but I'm not sure about its legitimacy. According to my experience in the calculus course, the change in the value of $F$ from $a$ to $x$ should take the form $$\Delta F=\sum_{i=1}^n[(x_i-a_i)\frac{\partial F}{\partial x_i}(a)+\epsilon_i(x_i-a_i)],$$ where each $\epsilon_i$ goes to zero as all $x_i-a_i$ tend to zero. Are these two statements consistent with each other? Thank you.","Suppose that is . I'd like to prove that , there exist functions such that , we have In fact, we have for all . This theorem is quoted in the appendix to my physics textbook, but I'm not sure about its legitimacy. According to my experience in the calculus course, the change in the value of from to should take the form where each goes to zero as all tend to zero. Are these two statements consistent with each other? Thank you.","F:\mathbb R^n\to\mathbb R C^\infty \forall a=(a_1,\ldots,a_n)\in\mathbb R^n C^\infty G_i,i=1\ldots,n, \forall x=(x_1,\ldots,x_n)\in\mathbb R^n F(x)=F(a)+\sum_{i=1}^n(x_i-a_i)G_i(x). G_i(a)=\frac{\partial F}{\partial x_i}(a) i F a x \Delta F=\sum_{i=1}^n[(x_i-a_i)\frac{\partial F}{\partial x_i}(a)+\epsilon_i(x_i-a_i)], \epsilon_i x_i-a_i","['real-analysis', 'calculus', 'multivariable-calculus']"
8,How to Find Solutions to a Multivariate Polynomial System,How to Find Solutions to a Multivariate Polynomial System,,"I have a system of polynomials, where the first one is a multivariate linear polynomial, but the rest are univariate quadratic polynomials. How would I solve such a system (finding one or all solutions, or showing there are no solutions)? For example, $$17x+16y-5z-67=0 \\ x^2+3x-5=0 \\ 4y^2-7y-4=0  \\ z^2-6z-3=0$$","I have a system of polynomials, where the first one is a multivariate linear polynomial, but the rest are univariate quadratic polynomials. How would I solve such a system (finding one or all solutions, or showing there are no solutions)? For example,",17x+16y-5z-67=0 \\ x^2+3x-5=0 \\ 4y^2-7y-4=0  \\ z^2-6z-3=0,"['multivariable-calculus', 'systems-of-equations', 'nonlinear-system']"
9,"Compute $\iint (x+y)\,dx\, dy$ with circle constraint $x^{2}+y^{2}=x+y$",Compute  with circle constraint,"\iint (x+y)\,dx\, dy x^{2}+y^{2}=x+y","I have a double integral: $$\iint (x+y)\,dx\, dy$$ with circle constraint: $$x^{2}+y^{2}=x+y$$ I tried to calculate it with transition to polar coordinates: $$x^{2}+y^{2}=x+y$$ $$\left(x-\frac{1}{2}\right)^{2}+\left(y-\frac{1}{2}\right)^{2}=\frac{1}{2}$$ In polar coordinates: $$r^{2}(\cos(\varphi))^{2} + r^{2}(\sin(\varphi))^{2} = r\cos(\varphi) + r\sin(\varphi)$$ $$r = \cos(\varphi) + \sin(\varphi)$$ Graph looks like this: But i don't understand how to find polar radius change interval here. If i separate circle into two, for first half circle for example it will go from $\textbf{some point}$ to $\frac{\pi}{2}$ . I don't understand how to find that $\textbf{some point}$ , cause it starts from point ( $\frac{1}{2}-\frac{1}{\sqrt{2}} = -0.2071$ ).","I have a double integral: with circle constraint: I tried to calculate it with transition to polar coordinates: In polar coordinates: Graph looks like this: But i don't understand how to find polar radius change interval here. If i separate circle into two, for first half circle for example it will go from to . I don't understand how to find that , cause it starts from point ( ).","\iint (x+y)\,dx\, dy x^{2}+y^{2}=x+y x^{2}+y^{2}=x+y \left(x-\frac{1}{2}\right)^{2}+\left(y-\frac{1}{2}\right)^{2}=\frac{1}{2} r^{2}(\cos(\varphi))^{2} + r^{2}(\sin(\varphi))^{2} = r\cos(\varphi) + r\sin(\varphi) r = \cos(\varphi) + \sin(\varphi) \textbf{some point} \frac{\pi}{2} \textbf{some point} \frac{1}{2}-\frac{1}{\sqrt{2}} = -0.2071","['integration', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
10,"To calculate $\nabla f$ and $\nabla f(0,0)$",To calculate  and,"\nabla f \nabla f(0,0)","I need to calculate $\nabla f$ and $\nabla f(0,0)$ , where $$f(x,y)=10x^3-5x^2+5xy+5y^2+8.$$ My work so far: Using the formula $$\nabla f = \left(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right)$$ I got \begin{align*} \frac{\partial }{\partial x}(10x^3-5x^2+5xy+5y^2+8)&=30 x^{2} - 10 x + 5 y,\\ \frac{\partial }{\partial y}(10x^3-5x^2+5xy+5y^2+8)&=5x + 10 y. \end{align*} Thus $$\nabla f \left(x,y\right)=\left(30 x^{2} - 10 x + 5 y,5 x + 10 y\right)$$ Is my process correct so far? Also, to calculate $\nabla f(0,0)$ , would $$\nabla f \left(0,0\right)=\left(0,0\right)$$ as $(0,0)$ would provide no values?","I need to calculate and , where My work so far: Using the formula I got Thus Is my process correct so far? Also, to calculate , would as would provide no values?","\nabla f \nabla f(0,0) f(x,y)=10x^3-5x^2+5xy+5y^2+8. \nabla f = \left(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right) \begin{align*}
\frac{\partial }{\partial x}(10x^3-5x^2+5xy+5y^2+8)&=30 x^{2} - 10 x + 5 y,\\
\frac{\partial }{\partial y}(10x^3-5x^2+5xy+5y^2+8)&=5x + 10 y.
\end{align*} \nabla f \left(x,y\right)=\left(30 x^{2} - 10 x + 5 y,5 x + 10 y\right) \nabla f(0,0) \nabla f \left(0,0\right)=\left(0,0\right) (0,0)","['multivariable-calculus', 'partial-derivative']"
11,Continuous vector field $F$ with $F(\vec x) - F(\vec y)$ parallel to $\vec x - \vec y$,Continuous vector field  with  parallel to,F F(\vec x) - F(\vec y) \vec x - \vec y,"In Goldstein's classical mechanics, he makes an interesting claim, that if there is a continuous vector field $F$ where $F(\vec x) - F(\vec y)$ is parallel to $\vec x - \vec y$ , then $F$ must be a constant field. We can attempt a proof by contradiction. If such a non-constant field exists, it's clear that we can first choose some point $\vec x$ and decompose our vector field, into a component $F_{\vec x}^1$ that always points towards (or away from) $\vec x$ and another vector field $F^2_{\vec x}$ that is constant and equal to $F(\vec x)$ . We can then repeat the construction with some other point $\vec y$ . I've used most of the information from the problem hypothesis, except continuity, and I'm not so sure  how continuity and the above paragraph will yield a contradiction. Goldstein's claim is from his chapter on rigid body motion, in a discussion on angular velocity. The claim appears just before equation $5.1$ of the third edition.","In Goldstein's classical mechanics, he makes an interesting claim, that if there is a continuous vector field where is parallel to , then must be a constant field. We can attempt a proof by contradiction. If such a non-constant field exists, it's clear that we can first choose some point and decompose our vector field, into a component that always points towards (or away from) and another vector field that is constant and equal to . We can then repeat the construction with some other point . I've used most of the information from the problem hypothesis, except continuity, and I'm not so sure  how continuity and the above paragraph will yield a contradiction. Goldstein's claim is from his chapter on rigid body motion, in a discussion on angular velocity. The claim appears just before equation of the third edition.",F F(\vec x) - F(\vec y) \vec x - \vec y F \vec x F_{\vec x}^1 \vec x F^2_{\vec x} F(\vec x) \vec y 5.1,['multivariable-calculus']
12,Double integral of a shifted circle,Double integral of a shifted circle,,"Task: find a double integral $$\iint_D (x+y)dxdy,$$ where D is bound by $x^2 + y^2 = x + y$ . What I have done so far: turns out it's a circle $$(x-1)^2 + (y-1)^2 = 2$$ Calculating it as a common double integral is hard because I get something like this: $$\int_{1-\sqrt{2}}^{1+\sqrt{2}} dx \int_{1 - \sqrt{2 - (x-1)^2}}^{1 + \sqrt{2 - (x-1)^2}} (x + y) dy.$$ So, I decided to give up on this. My next idea is to transform it into Polar coordinates. And that's where I got stuck. $$dxdy = rdrd\theta \\ x = r \cos{\theta} \\ y = r \sin{\theta}.$$ What to do next? For me, it looks like $$0 \leq\theta \leq 2\pi \\ 0 \leq r \leq 2\sqrt{2},$$ but this seems like a case when the origin of a circle is $(0, 0)$ . I have my circle shifted and there should be some tricks. Any help would be appreciated.","Task: find a double integral where D is bound by . What I have done so far: turns out it's a circle Calculating it as a common double integral is hard because I get something like this: So, I decided to give up on this. My next idea is to transform it into Polar coordinates. And that's where I got stuck. What to do next? For me, it looks like but this seems like a case when the origin of a circle is . I have my circle shifted and there should be some tricks. Any help would be appreciated.","\iint_D (x+y)dxdy, x^2 + y^2 = x + y (x-1)^2 + (y-1)^2 = 2 \int_{1-\sqrt{2}}^{1+\sqrt{2}} dx \int_{1 - \sqrt{2 - (x-1)^2}}^{1 + \sqrt{2 - (x-1)^2}} (x + y) dy. dxdy = rdrd\theta \\ x = r \cos{\theta} \\ y = r \sin{\theta}. 0 \leq\theta \leq 2\pi \\ 0 \leq r \leq 2\sqrt{2}, (0, 0)","['integration', 'multivariable-calculus', 'polar-coordinates', 'volume', 'multiple-integral']"
13,Functions of several variables,Functions of several variables,,"If $f(x,y) = x^2 + xy + y^2 - 3x + 4y - 5$ . I know the domain is $\mathbb R^2$ . How to determine the image of f is my issue.",If . I know the domain is . How to determine the image of f is my issue.,"f(x,y) = x^2 + xy + y^2 - 3x + 4y - 5 \mathbb R^2",['multivariable-calculus']
14,"How to find the bounds of the volume integral $\int_\Omega (6xz + 2y +3z^2) \, \text{d} \tilde{x}$?",How to find the bounds of the volume integral ?,"\int_\Omega (6xz + 2y +3z^2) \, \text{d} \tilde{x}","I'm studying on integrating over volumes and I don't know how to set the bounds in this exercise: Let $\Omega := \left\{ (x,y,z) \in \mathbb{R}^3 \,\big| \,\frac{x^2}{4} + y^2 + \frac{z^2}{9} <1 \right\}$ and $\tilde{x} = (x,y,z)$ . I want to evaluate the following integral \begin{align} \int_\Omega (6xz + 2y +3z^2) \ \text{d} \tilde{x}. \end{align} I reckon it's best to start off with the first integrand, using Fubini: \begin{align} \int_\Omega 6xz \ \text{d} \tilde{x} = 6 \int_\Omega xz \ \text{d}x \text{d}y \text{d}z \\ 6 \int_?^? z \int_?^? \int_?^? x \ \text{d}x \text{d}y \text{d}z, \end{align} but I don't know how to define the bounds for each of the integrals. Obviously it depends on $\Omega$ , and its easy to see that $\Omega$ defines a contorted 3D-ellipsoid. My intuition is somehow using sphere coordinates, but Im just not sure about the exact procedure at all on how exactly can I set the bounds, any ideas? Thanks EDIT: Thanks to @heropup's answer, I can edit my question by the additional knowledge I have now. Mind that Im a newbie, so please point out anything that has room for improvement. First, we can transform $\Omega$ into a unit 3D-sphere, by using the transformation $(x,y,z) \mapsto (2u,v,3w)$ , which gives us \begin{align} \tilde{\Omega} := \left\{ (2u,v,3w) \in \mathbb{R}^3 \,\Big| \,\frac{(2u)^2}{4} + v^2 + \frac{(3w)^2}{9} = u^2 + v^2 + w^2 < 1\right\}. \end{align} The corresponding integral (using Transformation theorem) turns into \begin{align} \int_{\tilde{\Omega}} (36uw + 2v + 27w^2) \det \frac{\partial (x,y,z)}{\partial(u,v,w)} \ \ \text{d} u \text{d} v \text{d} w, \end{align} where \begin{align} \det \frac{\partial (x,y,z)}{\partial(u,v,w)} = \det \begin{pmatrix} 2 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} = 6, \end{align} so \begin{align} \int_{\tilde{\Omega}} (216uw + 12v + 162w^2) \ \text{d} u \text{d} v \text{d} w. \end{align} We can now solve this by using spherical coordinates \begin{align} u = r \, \sin(\phi) \cos(\theta) \\ v = r \, \sin (\phi) \sin(\theta) \\ w = r \, \cos(\phi) \end{align} and the Jacobian determinate $r^2 \sin(\phi)$ to derive the following integral \begin{align} \int_0^{2\pi} \int_0^\pi \int_0^1 \left( 216 r \sin(\phi) \cos(\theta) r \cos(\phi) + 12 r \sin(\phi) \sin(\theta) + 162 (r \cos(\phi))^2 \right) \\ r^2 \sin(\phi) \ \text{d} r \text{d} \theta \text{d} \phi \end{align} So it breaks down to solving this integral. Is everything correct so far? I would be grateful to know. Cheers","I'm studying on integrating over volumes and I don't know how to set the bounds in this exercise: Let and . I want to evaluate the following integral I reckon it's best to start off with the first integrand, using Fubini: but I don't know how to define the bounds for each of the integrals. Obviously it depends on , and its easy to see that defines a contorted 3D-ellipsoid. My intuition is somehow using sphere coordinates, but Im just not sure about the exact procedure at all on how exactly can I set the bounds, any ideas? Thanks EDIT: Thanks to @heropup's answer, I can edit my question by the additional knowledge I have now. Mind that Im a newbie, so please point out anything that has room for improvement. First, we can transform into a unit 3D-sphere, by using the transformation , which gives us The corresponding integral (using Transformation theorem) turns into where so We can now solve this by using spherical coordinates and the Jacobian determinate to derive the following integral So it breaks down to solving this integral. Is everything correct so far? I would be grateful to know. Cheers","\Omega := \left\{ (x,y,z) \in \mathbb{R}^3 \,\big| \,\frac{x^2}{4} + y^2 + \frac{z^2}{9} <1 \right\} \tilde{x} = (x,y,z) \begin{align}
\int_\Omega (6xz + 2y +3z^2) \ \text{d} \tilde{x}.
\end{align} \begin{align}
\int_\Omega 6xz \ \text{d} \tilde{x} = 6 \int_\Omega xz \ \text{d}x \text{d}y \text{d}z \\
6 \int_?^? z \int_?^? \int_?^? x \ \text{d}x \text{d}y \text{d}z,
\end{align} \Omega \Omega \Omega (x,y,z) \mapsto (2u,v,3w) \begin{align}
\tilde{\Omega} := \left\{ (2u,v,3w) \in \mathbb{R}^3 \,\Big| \,\frac{(2u)^2}{4} + v^2 + \frac{(3w)^2}{9} = u^2 + v^2 + w^2 < 1\right\}.
\end{align} \begin{align}
\int_{\tilde{\Omega}} (36uw + 2v + 27w^2) \det \frac{\partial (x,y,z)}{\partial(u,v,w)} \ \ \text{d} u \text{d} v \text{d} w,
\end{align} \begin{align} \det \frac{\partial (x,y,z)}{\partial(u,v,w)} = \det
\begin{pmatrix}
2 & 0 & 0\\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}
= 6,
\end{align} \begin{align}
\int_{\tilde{\Omega}} (216uw + 12v + 162w^2) \ \text{d} u \text{d} v \text{d} w.
\end{align} \begin{align}
u = r \, \sin(\phi) \cos(\theta) \\
v = r \, \sin (\phi) \sin(\theta) \\
w = r \, \cos(\phi)
\end{align} r^2 \sin(\phi) \begin{align}
\int_0^{2\pi} \int_0^\pi \int_0^1 \left( 216 r \sin(\phi) \cos(\theta) r \cos(\phi) + 12 r \sin(\phi) \sin(\theta) + 162 (r \cos(\phi))^2 \right) \\ r^2 \sin(\phi) \ \text{d} r \text{d} \theta \text{d} \phi
\end{align}","['integration', 'multivariable-calculus', 'spherical-coordinates', 'multiple-integral', 'bounds-of-integration']"
15,differential of normal vector,differential of normal vector,,"For regular surface $S\subset \mathbb{R}^3$ ,we have differential of normal vector $dN_p:T_p(S) \to T_p(S)$ . What's the size of linear map $dN_p$ ?I was a bit confuse here. Since we know dimension of $T_p(S)$ is 2,so $dN_p$ should be a $2\times 2$ matrix,but in another view,each point in $T_p(S)$ is tangent of curve on $S$ ,so it's $v\in \mathbb{R}^3$ so $dN_p$ is a $3\times 3 $ matrix. Can someone explain a bit more clear?","For regular surface ,we have differential of normal vector . What's the size of linear map ?I was a bit confuse here. Since we know dimension of is 2,so should be a matrix,but in another view,each point in is tangent of curve on ,so it's so is a matrix. Can someone explain a bit more clear?",S\subset \mathbb{R}^3 dN_p:T_p(S) \to T_p(S) dN_p T_p(S) dN_p 2\times 2 T_p(S) S v\in \mathbb{R}^3 dN_p 3\times 3 ,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'surfaces']"
16,Volume of the region of sphere between two planes.,Volume of the region of sphere between two planes.,,"I want to find the volume of the region of the sphere $x^2+y^2+z^2=1$ , between the planes $z=1$ and $z=\frac{\sqrt{3}}{2}$ I have used triple integral for calculating this $$\int _0^{2\pi }\int _{0}^{\frac{\pi }{6}}\int _{0 }^1\:\rho ^2sin\phi \:d\rho \:d\phi \:d\theta $$ Are the limits of integral i have chosen correct?? Edit: As from the comment, my integral is not correct,  so please clarify what region actually the above integral represents.","I want to find the volume of the region of the sphere , between the planes and I have used triple integral for calculating this Are the limits of integral i have chosen correct?? Edit: As from the comment, my integral is not correct,  so please clarify what region actually the above integral represents.",x^2+y^2+z^2=1 z=1 z=\frac{\sqrt{3}}{2} \int _0^{2\pi }\int _{0}^{\frac{\pi }{6}}\int _{0 }^1\:\rho ^2sin\phi \:d\rho \:d\phi \:d\theta ,"['multivariable-calculus', 'multiple-integral']"
17,"Calculate $\lim\limits_{(x,y) \to (0,0)} \frac{2^{xy}-1}{ |x|+|y| }$",Calculate,"\lim\limits_{(x,y) \to (0,0)} \frac{2^{xy}-1}{ |x|+|y| }","I need to calculate the following limit: $$\lim\limits_{(x,y) \to (0,0)} \frac{2^{xy}-1}{ |x|+|y| }$$ I know that the answer is $0$ , but I don't know how to prove it. I tried to use the squeeze theorem, but got stuck: $$ 0\le \Bigg|\frac{2^{xy}-1}{ |x|+|y|}\Bigg|\le \Bigg|\frac{2^{xy}-1}{ |x|}\Bigg|$$ Is there a way to continue from here?","I need to calculate the following limit: I know that the answer is , but I don't know how to prove it. I tried to use the squeeze theorem, but got stuck: Is there a way to continue from here?","\lim\limits_{(x,y) \to (0,0)} \frac{2^{xy}-1}{ |x|+|y| } 0  0\le \Bigg|\frac{2^{xy}-1}{ |x|+|y|}\Bigg|\le \Bigg|\frac{2^{xy}-1}{ |x|}\Bigg|","['limits', 'multivariable-calculus']"
18,"Find maximum point of $f(x,y,z) = 8x^2 +4yz -16z +600$ with one restriction",Find maximum point of  with one restriction,"f(x,y,z) = 8x^2 +4yz -16z +600","I need to find the critical points of $$f(x,y,z) = 8x^2 +4yz -16z +600$$ restricted by $4x^2+y^2+4z^2=16$ . I constructed the lagrangian function $$L(x, y, z, \lambda ) = 8x^2 +4yz -16z +600 - \lambda (4x^2+y^2+4z^2-16) $$ but I'm very confused about how to determine those points. I know I need to make a system with all the first derivatives of $L$ equaled to $0$ . I did it but every time I try to solve it I get different solutions. How can I get the points? Thanks.",I need to find the critical points of restricted by . I constructed the lagrangian function but I'm very confused about how to determine those points. I know I need to make a system with all the first derivatives of equaled to . I did it but every time I try to solve it I get different solutions. How can I get the points? Thanks.,"f(x,y,z) = 8x^2 +4yz -16z +600 4x^2+y^2+4z^2=16 L(x, y, z, \lambda ) = 8x^2 +4yz -16z +600 - \lambda (4x^2+y^2+4z^2-16)  L 0","['calculus', 'multivariable-calculus', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier']"
19,implicit differentiation and taking limit on derivative,implicit differentiation and taking limit on derivative,,"I have this equation $x^3-xy^2+y^3=0$ and I want to know the value of the derivative at $(0,0)$ . Through implicit differentiation I find $y'=\frac{y^2-3x^2}{3y^2-2xy}$ . Now for $x=0,y=0$ this fraction becomes an indeterminate form. Upon graphical inspection I think if I were to draw a tangent at the Origin, it will have a slope very close to $-0.75$ , but what is an easy method to find out the actual value? Note, when you graph the original function it LOOKS like a line, but it ISN'T. Thank you for your input.","I have this equation and I want to know the value of the derivative at . Through implicit differentiation I find . Now for this fraction becomes an indeterminate form. Upon graphical inspection I think if I were to draw a tangent at the Origin, it will have a slope very close to , but what is an easy method to find out the actual value? Note, when you graph the original function it LOOKS like a line, but it ISN'T. Thank you for your input.","x^3-xy^2+y^3=0 (0,0) y'=\frac{y^2-3x^2}{3y^2-2xy} x=0,y=0 -0.75","['calculus', 'multivariable-calculus']"
20,Spivak's Calculus on Manifolds theorem 2-9 why is continuous differentiable needed,Spivak's Calculus on Manifolds theorem 2-9 why is continuous differentiable needed,,"In the third last line, it is said that because each $g_i$ is continuously differentiable at $a$ then the constructed function $g$ is also differentiable at $a$ . I do not see why ""continuously"" differentiable is need cuz I think a function is differentiable if and only if each of its components is differentiable (theorem 2-3(3)). It seems that I am missing something very obvious Edit: theorem 2-8 is","In the third last line, it is said that because each is continuously differentiable at then the constructed function is also differentiable at . I do not see why ""continuously"" differentiable is need cuz I think a function is differentiable if and only if each of its components is differentiable (theorem 2-3(3)). It seems that I am missing something very obvious Edit: theorem 2-8 is",g_i a g a,"['calculus', 'multivariable-calculus']"
21,for which a values $\int_{\mathbb{R^{2}}} \frac{sin(x^2+y^2)}{(x^2+y^2+1)^a}dxdy $ converges?,for which a values  converges?,\int_{\mathbb{R^{2}}} \frac{sin(x^2+y^2)}{(x^2+y^2+1)^a}dxdy ,"For which a values $\displaystyle \int_{\mathbb{R^{2}}} \frac{\sin(x^2+y^2)}{(x^2+y^2+1)^a}\,dx\,dy$ converge? I believe that only for $a\geq 0.5$ but I don't how to prove it. I was able to reduce the problem for only one variable $\displaystyle \frac{1}{2r^2+1}$ but I don't know how to prove this converges for $a>0.5$ or diverges for $a\leq 0.5$ .",For which a values converge? I believe that only for but I don't how to prove it. I was able to reduce the problem for only one variable but I don't know how to prove this converges for or diverges for .,"\displaystyle \int_{\mathbb{R^{2}}} \frac{\sin(x^2+y^2)}{(x^2+y^2+1)^a}\,dx\,dy a\geq 0.5 \displaystyle \frac{1}{2r^2+1} a>0.5 a\leq 0.5","['multivariable-calculus', 'indefinite-integrals']"
22,When can you switch the limites of integration of a line integral?,When can you switch the limites of integration of a line integral?,,"I was looking into why the property that $\int_a^b f(x) \ dx = -\int_b^a f(x) \ dx$ holds true. I found that 2 common answers were that It comes from $\int_a^b f(x) dx + \int_b^c f(x) dx = \int_a^c f(x) dx$ for arbitrary $a \le b \le c$ (for example, in this answer ). It comes from the fundamental theorem of calculus $\int_a^b f(x)\,dx = F(b) - F(a)$ (for example, in this answer ). From my understanding of these answers, the first one has a more lenient hypothesis, since to apply F.T.C. we need the function to have an antiderivative, which is not always the case. Knowing this, I was wondering about the extension of this question into a line integral. Let's say that $C$ is a path that starts at point $p$ and ends at point $q$ . If I define $C^*$ to be the same path but staring at $q$ and ending at $p$ , is it generally true that $$ \int_{C} \mathbf{F} \cdot d\mathbf{r} = - \int_{C^*} \mathbf{F} \cdot d\mathbf{r} \quad ? $$ where here $r:[t_0, t_f]\subset \mathbb{R} \to C$ , with $r(t_0) = p$ and $r(t_f) = q$ being a biyective parametrization of our path. I know that I can show this to be true if $\mathbf{F}$ happens to be a conservative field using the gradient theorem , in a similar manner as the 1D case can be shown by F.T.C., but since this is not always true I don't know if I can say that this holds in general . Is there a way to show that this always holds? Or alternatively, is there a counterexample where this fails? Thank you!","I was looking into why the property that holds true. I found that 2 common answers were that It comes from for arbitrary (for example, in this answer ). It comes from the fundamental theorem of calculus (for example, in this answer ). From my understanding of these answers, the first one has a more lenient hypothesis, since to apply F.T.C. we need the function to have an antiderivative, which is not always the case. Knowing this, I was wondering about the extension of this question into a line integral. Let's say that is a path that starts at point and ends at point . If I define to be the same path but staring at and ending at , is it generally true that where here , with and being a biyective parametrization of our path. I know that I can show this to be true if happens to be a conservative field using the gradient theorem , in a similar manner as the 1D case can be shown by F.T.C., but since this is not always true I don't know if I can say that this holds in general . Is there a way to show that this always holds? Or alternatively, is there a counterexample where this fails? Thank you!","\int_a^b f(x) \ dx = -\int_b^a f(x) \ dx \int_a^b f(x) dx + \int_b^c f(x) dx = \int_a^c f(x) dx a \le b \le c \int_a^b f(x)\,dx = F(b) - F(a) C p q C^* q p 
\int_{C} \mathbf{F} \cdot d\mathbf{r} = - \int_{C^*} \mathbf{F} \cdot d\mathbf{r} \quad ?
 r:[t_0, t_f]\subset \mathbb{R} \to C r(t_0) = p r(t_f) = q \mathbf{F}","['integration', 'multivariable-calculus', 'vector-analysis', 'line-integrals']"
23,Diffeomorphism from $\mathbb{R}^m\to\mathbb{R}^n$,Diffeomorphism from,\mathbb{R}^m\to\mathbb{R}^n,"I have a question about diffeomorphism between $\mathbb{R}^m$ and $\mathbb{R}^n$ . From this page of the internet we have the following definition: Let $U\subseteq\mathbb{R}^m$ and $V\subseteq\mathbb{R}^n$ . A function $F:U\to V$ is called a Diffeomorphism from $U$ to $V$ if $F$ has the following properties: a) $F:U\to V$ is bijective. b) $F:U\to V$ is smooth. c) $F^{−1}:V\to U$ is smooth. But in this post , it is proven that there is no diffeomorphism between $\mathbb{R}^2$ and $\mathbb{R}^3$ . In fact, the spaces $\mathbb{R}^m$ and $\mathbb{R}^n$ are not diffeomorphic when $m \neq n$ . Therefore, there cannot be a diffeomorphism between $\mathbb{R}^m$ and $\mathbb{R}^n$ . But by this definition, as the symbol $\subseteq$ is used, it implies that the open sets $U$ and $V$ can be $\mathbb{R}^m$ and $\mathbb{R}^n$ . So, the definition is "" wrong "", in the sense that there is no diffeomorphism between $\mathbb{R}^m$ and $\mathbb{R}^n$ ? Would the definition be correct if the symbol $\subset$ was used? That is, is it possible to construct diffeomorphism between open sets of $\mathbb{R}^m$ and $\mathbb{R}^n$ ?","I have a question about diffeomorphism between and . From this page of the internet we have the following definition: Let and . A function is called a Diffeomorphism from to if has the following properties: a) is bijective. b) is smooth. c) is smooth. But in this post , it is proven that there is no diffeomorphism between and . In fact, the spaces and are not diffeomorphic when . Therefore, there cannot be a diffeomorphism between and . But by this definition, as the symbol is used, it implies that the open sets and can be and . So, the definition is "" wrong "", in the sense that there is no diffeomorphism between and ? Would the definition be correct if the symbol was used? That is, is it possible to construct diffeomorphism between open sets of and ?",\mathbb{R}^m \mathbb{R}^n U\subseteq\mathbb{R}^m V\subseteq\mathbb{R}^n F:U\to V U V F F:U\to V F:U\to V F^{−1}:V\to U \mathbb{R}^2 \mathbb{R}^3 \mathbb{R}^m \mathbb{R}^n m \neq n \mathbb{R}^m \mathbb{R}^n \subseteq U V \mathbb{R}^m \mathbb{R}^n \mathbb{R}^m \mathbb{R}^n \subset \mathbb{R}^m \mathbb{R}^n,"['analysis', 'multivariable-calculus', 'diffeomorphism']"
24,"Calculating $\int_A \frac{z}{y^2}$ with $A:=\{z\ge0,x\ge1,y\ge 0, x^2+z^2<\min(2x,y^2)\}$",Calculating  with,"\int_A \frac{z}{y^2} A:=\{z\ge0,x\ge1,y\ge 0, x^2+z^2<\min(2x,y^2)\}","I want to calculate the following improper integral: $$\int_A \frac{z}{y^2}\\ A:=\{z\ge0,x\ge1,y\ge 0, x^2+z^2<\min(2x,y^2)\}$$ First I noticed that the conditions imply $x^2<2x\rightarrow x<2;1<x^2<y^2\rightarrow y>1$ , and thus $B=A\cap \{y>2\}=\{z\in(0,2),x\in(1,2), x^2+z^2<2x\}$ . Thus this part of the integral is fairly easy: $$\int_B\frac{z}{y^2}=\int_2^\infty\int_1^2\int_0^\sqrt{2x-x^2}\frac{z}{y^2}dzdxdy=\int_2^\infty\frac1{y^2}dy\int_1^2\frac{2x-x^2}{2}dx=\frac{1}{6}$$ We are now left with a ""proper"" integral (i.e. the region on which we are integration is finite, and the integrand is bounded): $$\int_{A\cap\{y\in(1,2)\}}\frac{z}{y^2}$$ I tried to split the domain in two regions: $A'=\{x\in[1,2);y\in(x,\sqrt{2x});z\in [0,\sqrt{y^2-x^2})\}, A''=\{x\in[1,2);y\in[\sqrt{2x},2];z\in[0,\sqrt{2x-x^2})$ Is my approach correct (I'm not sure, since the computations that follow bring me to an uncorrect result)? Is there any other approach to computing this integral, perhaps less messy?","I want to calculate the following improper integral: First I noticed that the conditions imply , and thus . Thus this part of the integral is fairly easy: We are now left with a ""proper"" integral (i.e. the region on which we are integration is finite, and the integrand is bounded): I tried to split the domain in two regions: Is my approach correct (I'm not sure, since the computations that follow bring me to an uncorrect result)? Is there any other approach to computing this integral, perhaps less messy?","\int_A \frac{z}{y^2}\\
A:=\{z\ge0,x\ge1,y\ge 0, x^2+z^2<\min(2x,y^2)\} x^2<2x\rightarrow x<2;1<x^2<y^2\rightarrow y>1 B=A\cap \{y>2\}=\{z\in(0,2),x\in(1,2), x^2+z^2<2x\} \int_B\frac{z}{y^2}=\int_2^\infty\int_1^2\int_0^\sqrt{2x-x^2}\frac{z}{y^2}dzdxdy=\int_2^\infty\frac1{y^2}dy\int_1^2\frac{2x-x^2}{2}dx=\frac{1}{6} \int_{A\cap\{y\in(1,2)\}}\frac{z}{y^2} A'=\{x\in[1,2);y\in(x,\sqrt{2x});z\in [0,\sqrt{y^2-x^2})\}, A''=\{x\in[1,2);y\in[\sqrt{2x},2];z\in[0,\sqrt{2x-x^2})","['integration', 'multivariable-calculus', 'improper-integrals', 'solution-verification', 'multiple-integral']"
25,"Prove that $\lim_{(x,y) \to (0,0)} \frac{\sin^2 xy}{x^2 + y^2}=0$ without using inequalities",Prove that  without using inequalities,"\lim_{(x,y) \to (0,0)} \frac{\sin^2 xy}{x^2 + y^2}=0","I read a similar question that was solved by using the fact that $|\sin(x)| \leq |x|$ , but I tried a different approach, as I struggle with utilizing inequalities to solve limits of this kind. Please note that my textbook did not ask for the limit to be solved without using inequalities. I attempted to prove this result by first switching to polar coordinates, then applying Hopital's rule as follows: $$\lim_{(x,y) \to (0,0)} \frac{\sin^2 xy}{x^2 + y^2}=\lim_{\rho \to 0} \frac{\sin^2 (\rho^2\sin(\theta)\cos(\theta))}{\rho^2}$$ Now I use cosine duplication formula to rewrite $\sin^2(t)$ $$\lim_{\rho \to 0} \frac{\sin^2 (\rho^2\sin(\theta)\cos(\theta))}{\rho^2}=\lim_{\rho \to 0} \biggl(\frac{1}{2}\biggr)\frac{1-\cos(2\rho^2\sin(\theta)\cos(\theta)}{\rho^2}$$ and now I apply Hopital's rule $$\lim_{\rho \to 0} \biggl(\frac{1}{2}\biggr)\frac{4\rho\sin(\theta)\cos(\theta)\sin(2\rho^2\sin(\theta)\cos(\theta))}{2\rho}=\lim_{\rho \to 0} \sin(\theta)\cos(\theta)\sin(2\rho^2\sin(\theta)\cos(\theta))$$ Now if I'm understanding this correctly, because the argument of the second sine function goes to zero, the result is proven. Have I made any mistakes? Was there a faster or more intuitive approach to solving this problem without using inequalities?","I read a similar question that was solved by using the fact that , but I tried a different approach, as I struggle with utilizing inequalities to solve limits of this kind. Please note that my textbook did not ask for the limit to be solved without using inequalities. I attempted to prove this result by first switching to polar coordinates, then applying Hopital's rule as follows: Now I use cosine duplication formula to rewrite and now I apply Hopital's rule Now if I'm understanding this correctly, because the argument of the second sine function goes to zero, the result is proven. Have I made any mistakes? Was there a faster or more intuitive approach to solving this problem without using inequalities?","|\sin(x)| \leq |x| \lim_{(x,y) \to (0,0)} \frac{\sin^2 xy}{x^2 + y^2}=\lim_{\rho \to 0} \frac{\sin^2 (\rho^2\sin(\theta)\cos(\theta))}{\rho^2} \sin^2(t) \lim_{\rho \to 0} \frac{\sin^2 (\rho^2\sin(\theta)\cos(\theta))}{\rho^2}=\lim_{\rho \to 0} \biggl(\frac{1}{2}\biggr)\frac{1-\cos(2\rho^2\sin(\theta)\cos(\theta)}{\rho^2} \lim_{\rho \to 0} \biggl(\frac{1}{2}\biggr)\frac{4\rho\sin(\theta)\cos(\theta)\sin(2\rho^2\sin(\theta)\cos(\theta))}{2\rho}=\lim_{\rho \to 0} \sin(\theta)\cos(\theta)\sin(2\rho^2\sin(\theta)\cos(\theta))","['calculus', 'limits', 'multivariable-calculus']"
26,Showing that $\left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v|$ implies $|f(x)|\le\frac{1}{2}|x|^2$,Showing that  implies,\left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v| |f(x)|\le\frac{1}{2}|x|^2,"I got this question from my analysis book: Let $f:U\to\mathbb{R}^n$ be of class $\mathcal{C}^1$ in an open convex $U\subseteq\mathbb{R}^m$ , with $0\in U$ and $f(0)=0$ . (a) If $|f'(x)|\le|x|$ for all $x\in U$ then $|f(x)|\le\frac{1}{2}|x|^2$ for all $x\in U$ . (b) Conclude that if $f(0)=f'(0)=0$ with $f\in\mathcal{C}^2$ then $\left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v|$ implies $|f(x)|\le\frac{1}{2}|x|^2$ . I already done the (a) part and there should be only a few steps to conclude the (b) part but I'm struggling with it. I know that $\frac{\partial^2 f }{\partial u\partial v}(x)=(f''(x))(u,v)$ if we see $f''(x)$ as a bilinear function $f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n$ but from here on I don't know what to do. Can someone please shed some light on this? For the (a) part see this question . I also wonder how my question relates to this because it would imply that $|f(x)|\le\frac{1}{2}|x|^2$ for any $f$ satisfying the conditions in (b). I think that the author of that question misinterpreted the question statement given his comments on the answers, and the answers also not proving exactly what the author stated (and also he got the question from the same book that I'm reading).","I got this question from my analysis book: Let be of class in an open convex , with and . (a) If for all then for all . (b) Conclude that if with then implies . I already done the (a) part and there should be only a few steps to conclude the (b) part but I'm struggling with it. I know that if we see as a bilinear function but from here on I don't know what to do. Can someone please shed some light on this? For the (a) part see this question . I also wonder how my question relates to this because it would imply that for any satisfying the conditions in (b). I think that the author of that question misinterpreted the question statement given his comments on the answers, and the answers also not proving exactly what the author stated (and also he got the question from the same book that I'm reading).","f:U\to\mathbb{R}^n \mathcal{C}^1 U\subseteq\mathbb{R}^m 0\in U f(0)=0 |f'(x)|\le|x| x\in U |f(x)|\le\frac{1}{2}|x|^2 x\in U f(0)=f'(0)=0 f\in\mathcal{C}^2 \left|\frac{\partial^2 f }{\partial u\partial v}\right|\le|u||v| |f(x)|\le\frac{1}{2}|x|^2 \frac{\partial^2 f }{\partial u\partial v}(x)=(f''(x))(u,v) f''(x) f''(x):\mathbb{R}^m\times\mathbb{R}^m\to\mathbb{R}^n |f(x)|\le\frac{1}{2}|x|^2 f","['real-analysis', 'multivariable-calculus']"
27,Evaluate the integral $\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz$,Evaluate the integral,\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz,"I need to calculate the integral $$\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz$$ being $\Gamma=S_1\cap S_2$ , given: $S_1=\{(x,y,z)\in\mathbb{R}:2x+2y+z=3\}$ $S_2=\{(x,y,z)\in\mathbb{R}:z=9-x^2-y^2\}$ In my problem i'm asked to solve it using both direct integration and the Stokes Theorem . I've started trying the Stokes part, calculating the rotational of $F$ : $$F=((y^2-z^2),(z^2-x^2),(x^2-y^2))\Longrightarrow \text{rot}(F)=-2(y+z,x+z,x+y)$$ Now, I know that the normal vector $N$ is $(2,2,1)$ (because $S_1\subset\Gamma$ and that's $S_1$ 's normal vector at any point). So now I using Stokes I have that (after simplifying) $$\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz=\frac{-2}{3}\iint_S(3x+3y+4z)d\sigma$$ I'm stucked here. I don't get what $S$ am I supposed to use in the double integral (I guess  I must do some variable change). For the direct integration part, i don't know where to start. I will thank any help. Edit: I need two different solutions, one using direct integration, and another using Stokes Theorem.","I need to calculate the integral being , given: In my problem i'm asked to solve it using both direct integration and the Stokes Theorem . I've started trying the Stokes part, calculating the rotational of : Now, I know that the normal vector is (because and that's 's normal vector at any point). So now I using Stokes I have that (after simplifying) I'm stucked here. I don't get what am I supposed to use in the double integral (I guess  I must do some variable change). For the direct integration part, i don't know where to start. I will thank any help. Edit: I need two different solutions, one using direct integration, and another using Stokes Theorem.","\int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz \Gamma=S_1\cap S_2 S_1=\{(x,y,z)\in\mathbb{R}:2x+2y+z=3\} S_2=\{(x,y,z)\in\mathbb{R}:z=9-x^2-y^2\} F F=((y^2-z^2),(z^2-x^2),(x^2-y^2))\Longrightarrow \text{rot}(F)=-2(y+z,x+z,x+y) N (2,2,1) S_1\subset\Gamma S_1 \int_\Gamma(y^2-z^2)dx+(z^2-x^2)dy+(x^2-y^2)dz=\frac{-2}{3}\iint_S(3x+3y+4z)d\sigma S","['integration', 'multivariable-calculus', 'lebesgue-integral', 'surface-integrals']"
28,Higher order derivatives and the chain rule,Higher order derivatives and the chain rule,,"So here I have an assignment about higher order derivatives and the chain rule, and a relation to be proved:  Show that for a rotation in the plane $$\begin{bmatrix}u\\v \end{bmatrix} =\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}x\\y \end{bmatrix}$$ and any twice differentiable function $f,$ there holds $$\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}.$$ What I got so far is that $ f(u,v)=f(x\cos \theta-y\sin\theta, x\sin\theta + y\cos\theta) $ from the matrix multiplication.  But I don't really understand how to get $\frac{\partial f}{\partial u} $ and $\frac{\partial f}{\partial v}$ .","So here I have an assignment about higher order derivatives and the chain rule, and a relation to be proved:  Show that for a rotation in the plane and any twice differentiable function there holds What I got so far is that from the matrix multiplication.  But I don't really understand how to get and .","\begin{bmatrix}u\\v \end{bmatrix} =\begin{bmatrix}\cos\theta & -\sin\theta\\\sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}x\\y \end{bmatrix} f, \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} = \frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}.  f(u,v)=f(x\cos \theta-y\sin\theta, x\sin\theta + y\cos\theta)  \frac{\partial f}{\partial u}  \frac{\partial f}{\partial v}","['calculus', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
29,Show that the tangent plane of the saddle surface $z=xy$ at any point intersects the surface in a pair of lines.,Show that the tangent plane of the saddle surface  at any point intersects the surface in a pair of lines.,z=xy,"My attempt: Let $f(x,y,z)=xy-z$ , (a,b,c) $\in$ the saddle surface, and calculate the total derivative $Df(a,b,c)=(b,a,-1)$ Then the tangent plane is $$g(x,y,z)=f(a,b,c)+Df(a,b,c)(x-a,y-b,c-z)=bx+ay-z+ab$$ Set $g(x,y,z)=f(x,y,z)$ to get the intersection got $bx+ay-xy+ab=0$ .  I know that the equation can be written as $$bx+ay-xy+ab=bx-ay-z-c=0$$ But I have no idea to get a pair of lines which intersects the saddle surface.","My attempt: Let , (a,b,c) the saddle surface, and calculate the total derivative Then the tangent plane is Set to get the intersection got .  I know that the equation can be written as But I have no idea to get a pair of lines which intersects the saddle surface.","f(x,y,z)=xy-z \in Df(a,b,c)=(b,a,-1) g(x,y,z)=f(a,b,c)+Df(a,b,c)(x-a,y-b,c-z)=bx+ay-z+ab g(x,y,z)=f(x,y,z) bx+ay-xy+ab=0 bx+ay-xy+ab=bx-ay-z-c=0",['multivariable-calculus']
30,Definite Integral of $\int_{\frac{-1}{2}}^\frac{1}{2}\int_{\frac{-1}{2}}^\frac{1}{2} \sqrt{x^2+y^2} dxdy$,Definite Integral of,\int_{\frac{-1}{2}}^\frac{1}{2}\int_{\frac{-1}{2}}^\frac{1}{2} \sqrt{x^2+y^2} dxdy,"Here's my attempt at trying to evaluate the integral. Let $x = y tan\theta$ $$\frac{dx}{d\theta} = \frac{y}{cos^2\theta}$$ $$dx = \frac{y}{cos^2\theta}d\theta$$ The new bounds of inner integral would be $\theta = tan^-(\frac{1}{2y}) $ and $\theta = tan^-(\frac{-1}{2y})$ $$\int_{\frac{-1}{2}}^\frac{1}{2}\int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} y^2 sec^3{\theta} dy$$ Evaluating the innermost integral $$\int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} sec^3{\theta} dy =  {\huge|} \frac{sec\theta tan\theta + ln|sec\theta + tan\theta|}{2}{\huge|}_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})}$$ $$\begin{multline} = \left( \frac{sec(tan^-(\frac{1}{2y})) tan(tan^-(\frac{1}{2y})) + ln|sec(tan^-(\frac{1}{2y})) + tan(tan^-(\frac{1}{2y}))|}{2}\right)    - \\ \left( \frac{sec(tan^-(\frac{-1}{2y})) tan(tan^-(\frac{-1}{2y})) + ln|sec(tan^-(\frac{-1}{2y})) + tan(tan^-(\frac{-1}{2y}))|}{2}\right)  \end{multline}$$ $$\begin{multline} = \left( \frac{\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|}{2}\right)    - \left( \frac{-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|}{2}\right)  \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|\right)    - \left(-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|\right) \right] \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\sqrt{4y^2 + 1} + 1|   - ln|\sqrt{4y^2 + 1} - 1| \right) \right] \end{multline}$$ $$\begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}| \right) \right] \end{multline}$$ Evaluating the outermost integral $$\begin{multline} \int_{\frac{-1}{2}}^\frac{1}{2}\frac{\sqrt{4y^2 + 1}}{4} + y^2 \frac{ln{\large|}\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}{\large|}}{2} dy \end{multline}$$ I am kinda stuck at this point, any help is greatly appreciated -:). I may have done something wrong in the steps above.","Here's my attempt at trying to evaluate the integral. Let The new bounds of inner integral would be and Evaluating the innermost integral Evaluating the outermost integral I am kinda stuck at this point, any help is greatly appreciated -:). I may have done something wrong in the steps above.","x = y tan\theta \frac{dx}{d\theta} = \frac{y}{cos^2\theta} dx = \frac{y}{cos^2\theta}d\theta \theta = tan^-(\frac{1}{2y})  \theta = tan^-(\frac{-1}{2y}) \int_{\frac{-1}{2}}^\frac{1}{2}\int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} y^2 sec^3{\theta} dy \int_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} sec^3{\theta} dy =  {\huge|} \frac{sec\theta tan\theta + ln|sec\theta + tan\theta|}{2}{\huge|}_{tan^-(\frac{-1}{2y})}^{tan^-(\frac{1}{2y})} \begin{multline} = \left( \frac{sec(tan^-(\frac{1}{2y})) tan(tan^-(\frac{1}{2y})) + ln|sec(tan^-(\frac{1}{2y})) + tan(tan^-(\frac{1}{2y}))|}{2}\right)  
 - \\ \left( \frac{sec(tan^-(\frac{-1}{2y})) tan(tan^-(\frac{-1}{2y})) + ln|sec(tan^-(\frac{-1}{2y})) + tan(tan^-(\frac{-1}{2y}))|}{2}\right)  \end{multline} \begin{multline} = \left( \frac{\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|}{2}\right)  
 - \left( \frac{-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|}{2}\right)  \end{multline} \begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{2y}|\right)  
 - \left(-\frac{\sqrt{4y^2 + 1}}{4y^2} + ln|\frac{\sqrt{4y^2 + 1} - 1}{2y}|\right) \right] \end{multline} \begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\sqrt{4y^2 + 1} + 1| 
 - ln|\sqrt{4y^2 + 1} - 1| \right) \right] \end{multline} \begin{multline} = \frac{1}{2}\left[\left(\frac{\sqrt{4y^2 + 1}}{2y^2} + ln|\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}| \right) \right] \end{multline} \begin{multline} \int_{\frac{-1}{2}}^\frac{1}{2}\frac{\sqrt{4y^2 + 1}}{4} + y^2 \frac{ln{\large|}\frac{\sqrt{4y^2 + 1} + 1}{\sqrt{4y^2 + 1} - 1}{\large|}}{2} dy \end{multline}","['multivariable-calculus', 'definite-integrals']"
31,Double integral over odd region,Double integral over odd region,,"Let $(x,y)$ be Cartesian coordinates, let $\mathbf{i}$ and $\mathbf{j}$ be unit vectors in the $x$ - and $y$ -directions, let $(r,\theta)$ be polar coordinates, and let $\mathbf{e}_{\theta} = \left(-\sin\theta\right)\mathbf{i}+\left(\cos\theta\right)\mathbf{j}$ . Let $R$ be the region of points $(x,y)$ satisfying \begin{equation*}\left\lvert x\right\rvert^{\frac{1}{2}} + \left\lvert y\right\rvert^{\frac{1}{2}} \leq 1\end{equation*} I need to find the integral \begin{equation*}\iint\limits_{R}x\left(x^2+y^2\right)^{\frac{1}{2}}\mathbf{e}_{\theta} \mathop{}\!\mathrm{d}A\end{equation*} I tried using polar coordinates as suggested by the problem statement, but (for the $\mathbf{j}$ -component) I end up with the integral \begin{equation*}\int_{0}^{\frac{\pi}{2}}\frac{\cos^{2}\theta}{\left(\sqrt{\cos\theta}+\sqrt{\sin\theta}\right)^8}\end{equation*} which seems intractable. Any assistance would be much appreciated.","Let be Cartesian coordinates, let and be unit vectors in the - and -directions, let be polar coordinates, and let . Let be the region of points satisfying I need to find the integral I tried using polar coordinates as suggested by the problem statement, but (for the -component) I end up with the integral which seems intractable. Any assistance would be much appreciated.","(x,y) \mathbf{i} \mathbf{j} x y (r,\theta) \mathbf{e}_{\theta} = \left(-\sin\theta\right)\mathbf{i}+\left(\cos\theta\right)\mathbf{j} R (x,y) \begin{equation*}\left\lvert x\right\rvert^{\frac{1}{2}} + \left\lvert y\right\rvert^{\frac{1}{2}} \leq 1\end{equation*} \begin{equation*}\iint\limits_{R}x\left(x^2+y^2\right)^{\frac{1}{2}}\mathbf{e}_{\theta} \mathop{}\!\mathrm{d}A\end{equation*} \mathbf{j} \begin{equation*}\int_{0}^{\frac{\pi}{2}}\frac{\cos^{2}\theta}{\left(\sqrt{\cos\theta}+\sqrt{\sin\theta}\right)^8}\end{equation*}","['multivariable-calculus', 'vector-analysis']"
32,The image of a disc under a diffeomorphism.,The image of a disc under a diffeomorphism.,,"This question is a simpler version of this . Suppose that you have a diffeomorphism $f:\mathbb{R}^2\to \mathbb{R}^2$ acting on the unit disc $D=\{(x,y):x^2+y^2\leq 1\}$ . Let $D_xf$ denote the total derivative of $f$ at point $x$ . Assume also that the biggest eigenvalue of $D_xf$ at $x$ , let us call it $\lambda_x$ , satisfies $|\lambda_x|>M$ , for all $x\in D$ and some $M>0$ . Take now $S$ to be a square with center $f(0)$ and side length $2$ . Question: Is it true that the image of the unit disc under $f$ will intersect $S$ assuming $M$ big enough? In other words is it true that $f(D)\cap S\not=\emptyset$ for big enough $M$ ? The idea is that $f$ stretches locally the disc in one direction by a lot but of course not at the same direction at every point. So  the answer to my question might be no but it seems very hard to believe. Does anyone have any ideas?","This question is a simpler version of this . Suppose that you have a diffeomorphism acting on the unit disc . Let denote the total derivative of at point . Assume also that the biggest eigenvalue of at , let us call it , satisfies , for all and some . Take now to be a square with center and side length . Question: Is it true that the image of the unit disc under will intersect assuming big enough? In other words is it true that for big enough ? The idea is that stretches locally the disc in one direction by a lot but of course not at the same direction at every point. So  the answer to my question might be no but it seems very hard to believe. Does anyone have any ideas?","f:\mathbb{R}^2\to \mathbb{R}^2 D=\{(x,y):x^2+y^2\leq 1\} D_xf f x D_xf x \lambda_x |\lambda_x|>M x\in D M>0 S f(0) 2 f S M f(D)\cap S\not=\emptyset M f","['multivariable-calculus', 'differential-geometry', 'eigenvalues-eigenvectors', 'dynamical-systems']"
33,"Line Integral $\int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds \qquad \qquad \gamma(t)\colon t\mapsto(\cos^3t,3\cos^2t\sin t)$ as $t \in (a,b)$",Line Integral  as,"\int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds \qquad \qquad \gamma(t)\colon t\mapsto(\cos^3t,3\cos^2t\sin t) t \in (a,b)","Hey there, I'm asked to find $$\int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds \qquad \qquad \gamma(t)\colon t\mapsto(\cos^3t,3\cos^2t\sin t)$$ as $t \in (a,b)$ . As we have regularity both in the field and in the curve, we can easily apply the standard formula: $$\int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds = \int_a^bF(\gamma(t)) ||\gamma'(t)||dt$$ but this leads to terrible computations. Is there anything smart I can do? Can I do something smart to simplify my operations when I'm going (like in this case) over a curve which is of the kind $t \mapsto (f(t),kf'(t))$ where $k$ is a constant?","Hey there, I'm asked to find as . As we have regularity both in the field and in the curve, we can easily apply the standard formula: but this leads to terrible computations. Is there anything smart I can do? Can I do something smart to simplify my operations when I'm going (like in this case) over a curve which is of the kind where is a constant?","\int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds \qquad \qquad \gamma(t)\colon t\mapsto(\cos^3t,3\cos^2t\sin t) t \in (a,b) \int_{\gamma} \frac{y}{\sqrt{x^2+y^2}}ds = \int_a^bF(\gamma(t)) ||\gamma'(t)||dt t \mapsto (f(t),kf'(t)) k","['calculus', 'integration', 'multivariable-calculus', 'curves']"
34,Triple Integral to find volume of solid [closed],Triple Integral to find volume of solid [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Can someone tell me what the bounds of the triple integral would be? I am confused as to how to start the problem. **The previous integral I had was wrong so I am editing the post. I now think the integral would go from 0 to 2 for the outermost integral dx, then 0 to 2 for dy, and 0 to square root of y for dz the innermost integral.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Can someone tell me what the bounds of the triple integral would be? I am confused as to how to start the problem. **The previous integral I had was wrong so I am editing the post. I now think the integral would go from 0 to 2 for the outermost integral dx, then 0 to 2 for dy, and 0 to square root of y for dz the innermost integral.",,"['integration', 'multivariable-calculus', 'volume']"
35,Does this prove that the limit does not exist?,Does this prove that the limit does not exist?,,"$$\lim_{(x,y)\to(0,0)}\frac{x}{x^{2}-y^{2}}$$ I tried with $y=mx$ and lateral limits. I got that: $$\lim_{x\to 0^{+}}\frac{1}{x(1-m^{2})}=+\infty$$ and $$\lim_{x\to 0^{-}}\frac{1}{x(1-m^{2})}=-\infty$$ Assumming $1-m^{2}>0$ . So the limit does not exist. It's correct?",I tried with and lateral limits. I got that: and Assumming . So the limit does not exist. It's correct?,"\lim_{(x,y)\to(0,0)}\frac{x}{x^{2}-y^{2}} y=mx \lim_{x\to 0^{+}}\frac{1}{x(1-m^{2})}=+\infty \lim_{x\to 0^{-}}\frac{1}{x(1-m^{2})}=-\infty 1-m^{2}>0","['limits', 'multivariable-calculus']"
36,Framing problem : Show that $\frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1$,Framing problem : Show that,\frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1,"Let $x,y,z\gt 0$ reals and let $x^3+y^3+z^3=4xyz$ Show that : $$\frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1$$ Can I use Cauchy inequality to solve it ?",Let reals and let Show that : Can I use Cauchy inequality to solve it ?,"x,y,z\gt 0 x^3+y^3+z^3=4xyz \frac{x^2}{yz+1}+\frac{y^2}{zx+1}+\frac{z^2}{xy+1}\leq\frac{x^2+y^2+z^2}{4} +1","['algebra-precalculus', 'multivariable-calculus', 'cauchy-schwarz-inequality']"
37,Flow and flow rate... Halp!,Flow and flow rate... Halp!,,"I'm so confused...  I think I got the meaning of flux, it's a scalar that indicates the ""quantity of a vector field (of field lines)"" that crosses a surface of a given area. So no time relation implied right? First: is flow and flux the same thing in English when talking about physics? (in Italian we just refer to it with the word ""flusso"") Why I often read, about fluid flow (flux?) : it's the quantity that measures volume that crosses a surface per unit time? Is this a misunderstanding? is this the definition of flow rate? Is flow rate and flow the same thing in the field of velocity of a fluid? Is the mass ( or volume) of the fluid bond to the vectorial field in a fluid, or I can just pick the velocity of a point in the fluid without considering the mass? Is flow rate the dual of current for an electromagnetic field? How is current related to flux? Maybe the key is the divergence theorem... I need some sleep +.+ It's a lot of questions but they are strictly related to each other, I guess, the point is that I studied those things separately, I can't build an organic connection in my head.","I'm so confused...  I think I got the meaning of flux, it's a scalar that indicates the ""quantity of a vector field (of field lines)"" that crosses a surface of a given area. So no time relation implied right? First: is flow and flux the same thing in English when talking about physics? (in Italian we just refer to it with the word ""flusso"") Why I often read, about fluid flow (flux?) : it's the quantity that measures volume that crosses a surface per unit time? Is this a misunderstanding? is this the definition of flow rate? Is flow rate and flow the same thing in the field of velocity of a fluid? Is the mass ( or volume) of the fluid bond to the vectorial field in a fluid, or I can just pick the velocity of a point in the fluid without considering the mass? Is flow rate the dual of current for an electromagnetic field? How is current related to flux? Maybe the key is the divergence theorem... I need some sleep +.+ It's a lot of questions but they are strictly related to each other, I guess, the point is that I studied those things separately, I can't build an organic connection in my head.",,"['multivariable-calculus', 'fluid-dynamics', 'electromagnetism']"
38,"Prove $(x^a-1)(y^b-1)\geq (1-x^{-b})(1-y^{-a})$ where $x,y\geq 1$ and $a,b \geq 0$.",Prove  where  and .,"(x^a-1)(y^b-1)\geq (1-x^{-b})(1-y^{-a}) x,y\geq 1 a,b \geq 0",I am trying to see if the inequality always holds. The RHS of course is always $\leq 1$ whereas the LHS can be greater than 1. Any suggestions greatly appreciated.,I am trying to see if the inequality always holds. The RHS of course is always whereas the LHS can be greater than 1. Any suggestions greatly appreciated.,\leq 1,"['algebra-precalculus', 'analysis', 'multivariable-calculus', 'inequality']"
39,What are the unknown steps to show $\mathbf{F}=-\nabla\psi$?,What are the unknown steps to show ?,\mathbf{F}=-\nabla\psi,Given: $\mathbf{F}=F(r)\ \mathbf{\hat{r}}$ is a spherically symmetric radial vector field which is continuous everywhere except at origin. To prove: $\mathbf{F}=-\nabla \psi$ Proof: \begin{align} \mathbf{F} \cdot \mathbf{\hat{r}}&=F(r)\ \mathbf{\hat{r}} \cdot \mathbf{\hat{r}}=F(r)=\dfrac{d(\int F\ dr)}{dr}\\ &=\dfrac{d\psi}{dr}=\nabla \psi \cdot \mathbf{\hat{r}} \end{align} From here we cannot straight away deduce that: $\mathbf{F}=\nabla\psi$ UNKNOWN STEPS ??? $$\mathbf{F}=-\nabla\psi$$ $\mathbf{F}$ is conservative. Question: From what steps (unknown to me) can we get $\mathbf{F}=-\nabla\psi\ $ ?,Given: is a spherically symmetric radial vector field which is continuous everywhere except at origin. To prove: Proof: From here we cannot straight away deduce that: UNKNOWN STEPS ??? is conservative. Question: From what steps (unknown to me) can we get ?,"\mathbf{F}=F(r)\ \mathbf{\hat{r}} \mathbf{F}=-\nabla \psi \begin{align}
\mathbf{F} \cdot \mathbf{\hat{r}}&=F(r)\ \mathbf{\hat{r}} \cdot \mathbf{\hat{r}}=F(r)=\dfrac{d(\int F\ dr)}{dr}\\ &=\dfrac{d\psi}{dr}=\nabla \psi \cdot \mathbf{\hat{r}}
\end{align} \mathbf{F}=\nabla\psi \mathbf{F}=-\nabla\psi \mathbf{F} \mathbf{F}=-\nabla\psi\ ","['calculus', 'multivariable-calculus', 'physics']"
40,Differential $k$-form and integrating factor,Differential -form and integrating factor,k,"Consider a $C^1$ differential $k$ -form $\omega$ on an open set $U \subset \mathbb R^n$ . A non-vanishing $C^1$ function $f: \mathbb R^n \to \mathbb R$ is called an integrating factor for $\omega$ if $d(f\omega) = 0$ . If $k$ is odd and an integrating factor exists, I want to show that $\omega \land d\omega = 0 $ . Here's my proof: $$d(f\omega \land \omega) = d(f\omega) \land \omega + (-1)^k f\omega \land d\omega                           = 0 \land \omega + (-1)^k f\omega \land d\omega                           = (-1)^k f(\omega \land d\omega)$$ And $f\omega \land \omega = f(\omega \land \omega) = 0$ , so $d(f\omega \land \omega) = 0$ and hence $(-1)^k f(\omega \land d\omega) = 0$ . If $f$ never vanishes, then $(-1)^k f$ never vanishes, so that $\omega \land d\omega = 0$ . Clearly, my proof doesn't use that $k$ must be odd. What did I do wrong here?","Consider a differential -form on an open set . A non-vanishing function is called an integrating factor for if . If is odd and an integrating factor exists, I want to show that . Here's my proof: And , so and hence . If never vanishes, then never vanishes, so that . Clearly, my proof doesn't use that must be odd. What did I do wrong here?","C^1 k \omega U \subset \mathbb R^n C^1 f: \mathbb R^n \to \mathbb R \omega d(f\omega) = 0 k \omega \land d\omega = 0  d(f\omega \land \omega) = d(f\omega) \land \omega + (-1)^k f\omega \land d\omega
                          = 0 \land \omega + (-1)^k f\omega \land d\omega
                          = (-1)^k f(\omega \land d\omega) f\omega \land \omega = f(\omega \land \omega) = 0 d(f\omega \land \omega) = 0 (-1)^k f(\omega \land d\omega) = 0 f (-1)^k f \omega \land d\omega = 0 k","['multivariable-calculus', 'differential-geometry', 'differential-forms']"
41,Any shorter way to compute this integral over the unit ball?,Any shorter way to compute this integral over the unit ball?,,"Let $n$ be a non-negative integer, and let $V$ be the unit ball centered at the origin. To compute: \begin{equation*}I = \iiint\limits_{V}(x+y+z)^{2n}{\mathop{}\!\mathrm{d}}V\end{equation*} I substituted for $x$ , $y$ , and $z$ with spherical coordinates, did the $r$ -integral, then used the binomial expansion on the remaining $((\sin\theta\cos\phi+\sin\theta\sin\phi)+\cos\theta)^{2n}$ and integrated the resulting series. Eventually, after a lot of algebra, I got the result \begin{equation*}I = \frac{8 \cdot 3^n \pi(n+1)}{(2n+3)(2n+2)(2n+1)}\end{equation*} but I am wondering if there is any shorter way to do this computation. Any suggestions would be appreciated (or explanations as to why no shorter method is possible, if that is the case).","Let be a non-negative integer, and let be the unit ball centered at the origin. To compute: I substituted for , , and with spherical coordinates, did the -integral, then used the binomial expansion on the remaining and integrated the resulting series. Eventually, after a lot of algebra, I got the result but I am wondering if there is any shorter way to do this computation. Any suggestions would be appreciated (or explanations as to why no shorter method is possible, if that is the case).",n V \begin{equation*}I = \iiint\limits_{V}(x+y+z)^{2n}{\mathop{}\!\mathrm{d}}V\end{equation*} x y z r ((\sin\theta\cos\phi+\sin\theta\sin\phi)+\cos\theta)^{2n} \begin{equation*}I = \frac{8 \cdot 3^n \pi(n+1)}{(2n+3)(2n+2)(2n+1)}\end{equation*},"['multivariable-calculus', 'vector-analysis']"
42,How to find total derivative of function $f$ from $\mathbb R^2$ to $\mathbb R$,How to find total derivative of function  from  to,f \mathbb R^2 \mathbb R,"I'm trying to find the total derivative $Df(x,y)$ of $f(x,y)=y\cos x$ . I'm familiar with this definition mostly: https://en.wikipedia.org/wiki/Total_derivative#The_total_derivative_as_a_linear_map I believe there is a theorem that states that if the partial derivatives exist and are continuous, then $f$ is differentiable. I haven't learned this theorem in class however, and would like to avoid using it if possible. Is there any easy ways to see if this derivative even exists on $\mathbb R^2$ ?","I'm trying to find the total derivative of . I'm familiar with this definition mostly: https://en.wikipedia.org/wiki/Total_derivative#The_total_derivative_as_a_linear_map I believe there is a theorem that states that if the partial derivatives exist and are continuous, then is differentiable. I haven't learned this theorem in class however, and would like to avoid using it if possible. Is there any easy ways to see if this derivative even exists on ?","Df(x,y) f(x,y)=y\cos x f \mathbb R^2","['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'normed-spaces']"
43,"prove existence + compute $\lim_{(x, y) \to (0, 0)} \frac{e^{xy} - 1}{y}$ - is my proof correct?",prove existence + compute  - is my proof correct?,"\lim_{(x, y) \to (0, 0)} \frac{e^{xy} - 1}{y}","$\lim_{(x, y) \to (0, 0)} \frac{e^{xy} - 1}{y}$ i'm substituting $t := e^x, u := e^y$ , then the limit looks like this: $\lim_{(t, u) \to (1, 1)} \frac{tu - 1}{\ln u}$ i believe that's acceptable, because even though $\ln u$ is only defined for $u>0$ , it is defined in small enough neighbourhoods of 1. does that make sense? i'm squeezing the transformed limit, using $|\ln u| < u$ for $u > 0.5$ , again limiting the validity of my squeezing functions (with the same argument). $o \leq |\frac{tu - 1}{\ln u}| = \frac{|tu - 1|}{|\ln u|} \leq \frac{|tu - 1|}{u}$ and since both, lower and upper limit, converge to 0 for $(t, u) \to (1, 1)$ , so will the middle one. would that be an acceptable proof? if not, what would be better? thanks!","i'm substituting , then the limit looks like this: i believe that's acceptable, because even though is only defined for , it is defined in small enough neighbourhoods of 1. does that make sense? i'm squeezing the transformed limit, using for , again limiting the validity of my squeezing functions (with the same argument). and since both, lower and upper limit, converge to 0 for , so will the middle one. would that be an acceptable proof? if not, what would be better? thanks!","\lim_{(x, y) \to (0, 0)} \frac{e^{xy} - 1}{y} t := e^x, u := e^y \lim_{(t, u) \to (1, 1)} \frac{tu - 1}{\ln u} \ln u u>0 |\ln u| < u u > 0.5 o \leq |\frac{tu - 1}{\ln u}| = \frac{|tu - 1|}{|\ln u|} \leq \frac{|tu - 1|}{u} (t, u) \to (1, 1)","['real-analysis', 'limits', 'multivariable-calculus']"
44,How to find the stationary point under constraints analytically?,How to find the stationary point under constraints analytically?,,"I am working with the optimization problem from the paper, eq.(5) $$\max_{X=(x_1, x_2, \ldots, x_{n+1})} f(X)=(A-B\sum_{i=1}^n \frac{1}{x_i})\times x_{n+1}$$ subject to $$x_{n+1}=1-2k\sum_{i=1}^n x_i,$$ $$x_i \geq 0, \quad i = 1,2,\ldots, n+1.$$ Here $A \gg B > 0 \in \mathbb{R}$ , $k \in \mathbb{Z}^+$ , and $f(X)>1$ . From the paper I know the stationary point $$X^*=(\underbrace{\sqrt{\frac{B}{2kA}}, \ldots, \sqrt{\frac{B}{2kA}}}_{n\text{  times}}, 1 − 2 kn\sqrt{\frac{B}{2kA}} )$$ and the optimal value $$f(X^*)=(\sqrt{A} − n \sqrt{ 2 \cdot k \cdot B})^2.$$ Question. How to find the stationary point under constraints analytically? Is it possible for $n=3, k=10$ case? Attempt. I have tried to use the Lagrange multiplier: $$F(X, \lambda) = x_{n+1}(A-B\sum \frac{1}{x_i}) + \lambda(x_{n+1} - 1 + 2k\sum x_i)=0$$ and found the partial derivatives and have the $(n+2)$ system with $n+2$ variables: \begin{cases}  F'_{x_i}(X, \lambda)= x_{n+1}\frac{B}{x_i^2} +2 \lambda k x_i=0, \quad i=1,2,..., n, \\  F'_{x_{n+1}}(X, \lambda)= A - B\sum \frac{1}{x_i}+\lambda=0, \\  F'_{\lambda}(X, \lambda)=x_{n+1} -1+2k\sum x_i =0.  \end{cases} My problem now is how to express $x_i$ , $i=1,2,..., n$ , and $x_{n+1}$ through $\lambda$ and find roots. I know the stationary point and have found the A & Q . I will use the  notation $\sum_{i=1}^n x_i := n \cdot x$ $\sum_{i=1}^n \frac{1}{x_i} := \frac{n} {x}$ , and $x_{n+1}:=y$ . Then the system will be $$y\frac{B}{x^2}+2\lambda k x=0, \tag{2.1}$$ $$\lambda=B\frac{n}{x}-A, \tag{2.2}$$ $$y=1-2knx. \tag{2.3}$$ Put $(2.2)$ and $(2.3)$ in $(1.1)$ : $$(  1-2knx )\frac{B}{x^2}+2 (B\frac{n}{x}-A ) k x=0, \tag{3.1}$$ Multiple both sides $(3.1)$ on $x^2$ : $$(  1-2knx )B+2 (B\frac{n}{x}-A ) k x^3=0, \tag{4.1}$$ Open brackets and collect tems: $$2kAx^3-2nBkx^2+2nBkx-B=0, \tag{5.1}$$ divide both sides on $2kA$ : $$x^3 - n\frac{B}{A}x^2 + n \frac{B}{A}x-\frac{1}{2k}\frac{B}{A}=0. \tag{6.1}$$ One can see the equation of power $3$ , I am looking for a root $x \in \mathbb{R}$ . I think the equation $(6.1)$ should has a simple real root and complex pair.","I am working with the optimization problem from the paper, eq.(5) subject to Here , , and . From the paper I know the stationary point and the optimal value Question. How to find the stationary point under constraints analytically? Is it possible for case? Attempt. I have tried to use the Lagrange multiplier: and found the partial derivatives and have the system with variables: My problem now is how to express , , and through and find roots. I know the stationary point and have found the A & Q . I will use the  notation , and . Then the system will be Put and in : Multiple both sides on : Open brackets and collect tems: divide both sides on : One can see the equation of power , I am looking for a root . I think the equation should has a simple real root and complex pair.","\max_{X=(x_1, x_2, \ldots, x_{n+1})} f(X)=(A-B\sum_{i=1}^n \frac{1}{x_i})\times x_{n+1} x_{n+1}=1-2k\sum_{i=1}^n x_i, x_i \geq 0, \quad i = 1,2,\ldots, n+1. A \gg B > 0 \in \mathbb{R} k \in \mathbb{Z}^+ f(X)>1 X^*=(\underbrace{\sqrt{\frac{B}{2kA}}, \ldots, \sqrt{\frac{B}{2kA}}}_{n\text{  times}}, 1 − 2 kn\sqrt{\frac{B}{2kA}}
) f(X^*)=(\sqrt{A} − n \sqrt{ 2 \cdot k \cdot B})^2. n=3, k=10 F(X, \lambda) = x_{n+1}(A-B\sum \frac{1}{x_i}) + \lambda(x_{n+1} - 1 + 2k\sum x_i)=0 (n+2) n+2 \begin{cases} 
F'_{x_i}(X, \lambda)= x_{n+1}\frac{B}{x_i^2} +2 \lambda k x_i=0, \quad i=1,2,..., n, \\ 
F'_{x_{n+1}}(X, \lambda)= A - B\sum \frac{1}{x_i}+\lambda=0, \\ 
F'_{\lambda}(X, \lambda)=x_{n+1} -1+2k\sum x_i =0. 
\end{cases} x_i i=1,2,..., n x_{n+1} \lambda \sum_{i=1}^n x_i := n \cdot x \sum_{i=1}^n \frac{1}{x_i} := \frac{n} {x} x_{n+1}:=y y\frac{B}{x^2}+2\lambda k x=0, \tag{2.1} \lambda=B\frac{n}{x}-A, \tag{2.2} y=1-2knx. \tag{2.3} (2.2) (2.3) (1.1) (  1-2knx )\frac{B}{x^2}+2 (B\frac{n}{x}-A ) k x=0, \tag{3.1} (3.1) x^2 (  1-2knx )B+2 (B\frac{n}{x}-A ) k x^3=0, \tag{4.1} 2kAx^3-2nBkx^2+2nBkx-B=0, \tag{5.1} 2kA x^3 - n\frac{B}{A}x^2 + n \frac{B}{A}x-\frac{1}{2k}\frac{B}{A}=0. \tag{6.1} 3 x \in \mathbb{R} (6.1)","['multivariable-calculus', 'nonlinear-optimization', 'lagrange-multiplier']"
45,Given $h \in \mathbb R^n $ is it true that $\lim_{h \to 0 } \frac{|h^T S h|}{\|h\|}=0$,Given  is it true that,h \in \mathbb R^n  \lim_{h \to 0 } \frac{|h^T S h|}{\|h\|}=0,"Given a symmetric matrix $S $ and a vector $h =(h_1, \cdots , h_n ) \in \mathbb R^n $ , is it true that $$\lim_{h \to 0 } \frac{|h^T S h|}{\|h\|}=0$$ Expanding the numerator one may observe that it would be sufficient to check that $$\lim _ {h \to 0 } \frac {|h_1^{i_1}  h_2^{i_2 } ...h_n^{i_n }|} {h_1^2 + h_2^2 + \cdots + h_n^2 }  $$ where $i_1 + i_2 + \cdots + i_n = n$ Is this true and what inequality could one use? Thanks in advance!","Given a symmetric matrix and a vector , is it true that Expanding the numerator one may observe that it would be sufficient to check that where Is this true and what inequality could one use? Thanks in advance!","S  h =(h_1, \cdots , h_n ) \in \mathbb R^n  \lim_{h \to 0 } \frac{|h^T S h|}{\|h\|}=0 \lim _ {h \to 0 } \frac {|h_1^{i_1}  h_2^{i_2 } ...h_n^{i_n }|} {h_1^2 + h_2^2 + \cdots + h_n^2 }   i_1 + i_2 + \cdots + i_n = n","['limits', 'analysis', 'multivariable-calculus', 'quadratic-forms']"
46,$x+y$ is closed or discrete map,is closed or discrete map,x+y,"Is not the function $f:\mathbb{R}^2\to\mathbb{R}$ defined by $f(x,y)=x+y$ a closed map? And will not the image of any discrete set in $\mathbb{R}^2$ be discrete? I think it should be a closed map and that the image of any discrete set under $f$ must be discrete. Since $f$ is continuous and surjective, therefore, I think it must also take closed sets to closed sets. The portion concerning discrete sets seems obvious I think. Am I wrong? Thanks beforehand.","Is not the function defined by a closed map? And will not the image of any discrete set in be discrete? I think it should be a closed map and that the image of any discrete set under must be discrete. Since is continuous and surjective, therefore, I think it must also take closed sets to closed sets. The portion concerning discrete sets seems obvious I think. Am I wrong? Thanks beforehand.","f:\mathbb{R}^2\to\mathbb{R} f(x,y)=x+y \mathbb{R}^2 f f","['real-analysis', 'calculus', 'general-topology', 'multivariable-calculus']"
47,Omega Notation in Explanation of Chain Rule,Omega Notation in Explanation of Chain Rule,,"In his book Multivariate Calculus and Geometry, Sean Dineen explains the chain rule as follows: I understood well enough his point but, then he goes on to introduce an unknown and unexplained $\omega$ to explain something having to do with the unit basis:","In his book Multivariate Calculus and Geometry, Sean Dineen explains the chain rule as follows: I understood well enough his point but, then he goes on to introduce an unknown and unexplained to explain something having to do with the unit basis:",\omega,"['multivariable-calculus', 'notation', 'ordinals']"
48,Find max/min of the following function,Find max/min of the following function,,"Find the minimum e max distance (in $R^2$ ) ) between the point $Q = ( 3/ 2 , − 3/ 2 )$ and the set $$B = \{(x, y) ∈ R^2 : yx = 1, x ≥ 0, y ≥ 0\}$$ In other words I have to find max /min points of the function $${(x-3/2)^2 + (y+3/2)^2}$$ The set $B$ is clearly neither bounded nor convex. If I use Lagrange I can only find local min/max, but how do I show they are global? In a previous question : Does the following function admit a maximum? you suggested me to maximize/minimize the $x$ component and the $y$ component independently, but it is not clear to me if I can do it in this exercise as well. It seens to me that choosing $x=3/2$ , which clearly minimizes the first component of the sum, would restrict the choice of $y$ as $yx = 1 $ . Is there any way to show that the point found using lagrange is a global min? Possibly without using the bordered hessian method?","Find the minimum e max distance (in ) ) between the point and the set In other words I have to find max /min points of the function The set is clearly neither bounded nor convex. If I use Lagrange I can only find local min/max, but how do I show they are global? In a previous question : Does the following function admit a maximum? you suggested me to maximize/minimize the component and the component independently, but it is not clear to me if I can do it in this exercise as well. It seens to me that choosing , which clearly minimizes the first component of the sum, would restrict the choice of as . Is there any way to show that the point found using lagrange is a global min? Possibly without using the bordered hessian method?","R^2 Q =
(
3/
2
, −
3/
2
) B = \{(x, y) ∈ R^2
: yx = 1, x ≥ 0, y ≥ 0\} {(x-3/2)^2 + (y+3/2)^2} B x y x=3/2 y yx = 1 ","['multivariable-calculus', 'optimization', 'maxima-minima', 'lagrange-multiplier', 'qcqp']"
49,What is the solution to the following Lagrange system?,What is the solution to the following Lagrange system?,,"I have to maximize the function $$u(x_1, x_2) = x_1^{1/2} + x_2^{1/2}$$ in the set $$\{(x_1,x_2) \in \mathbb R^2 \mid x_1,x_2\geq0, (x_1p_1+x_2p_2) \leq N\},$$ where $p_1, p_2>0$ and $N>0$ . The Lagrange system is : $1/2x_1^{-1/2}+\lambda_1-\lambda_3p_1 =0$ $1/2x_2^{-1/2}+\lambda_2-\lambda_3p_1 =0$ $\lambda_1(-x_1)=0$ $\lambda_2(-x_2)=0$ $\lambda_3(x_1p_1+x_2p_2-N)=0$ $-x_1\leq 0$ $-x_2\leq 0$ $x_1p_1+x_2p_2 \leq N$ My professor claims that the only solution of the system is: $x_1=\frac{p_2}{p_1^2+p_1p_2}N$ and $x_2=\frac{p_1}{p_1+p_2}N$ . But I can not get that solution fro  the system. Can you help me please?",I have to maximize the function in the set where and . The Lagrange system is : My professor claims that the only solution of the system is: and . But I can not get that solution fro  the system. Can you help me please?,"u(x_1, x_2) = x_1^{1/2} + x_2^{1/2} \{(x_1,x_2) \in \mathbb R^2 \mid x_1,x_2\geq0, (x_1p_1+x_2p_2) \leq N\}, p_1, p_2>0 N>0 1/2x_1^{-1/2}+\lambda_1-\lambda_3p_1 =0 1/2x_2^{-1/2}+\lambda_2-\lambda_3p_1 =0 \lambda_1(-x_1)=0 \lambda_2(-x_2)=0 \lambda_3(x_1p_1+x_2p_2-N)=0 -x_1\leq 0 -x_2\leq 0 x_1p_1+x_2p_2 \leq N x_1=\frac{p_2}{p_1^2+p_1p_2}N x_2=\frac{p_1}{p_1+p_2}N","['calculus', 'multivariable-calculus', 'optimization', 'maxima-minima']"
50,Seeking a Generalization of Rudin 9.40: A Multivariable Mean Value Theorem,Seeking a Generalization of Rudin 9.40: A Multivariable Mean Value Theorem,,"Let $n\in\mathbb{Z}^+$ . Let $A$ be an open subset of $\mathbb{R}^n$ . Let $f : A\to\mathbb{R}$ be a map. Let $K = [a_1,b_1]\times\cdots\times[a_n,b_n]$ be an $n$ -cell contained in $A$ $(a_i<b_i \hspace{1mm}\forall i\in\{1,\ldots,n\})$ . To be concise, suppose that for each $k\in\{1,\ldots,n\},$ the partial derivative $$     \frac{\partial^k f(c)}{\partial x_k \partial x_{k-1} \cdots \partial x_1}(c) $$ exists at every $c \in K$ . In POMA (3e), Rudin shows (Thm 9.40) that in the case $n=2$ , there exists a $(c_1,c_2) \in K$ such that $$ \frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1} (b_2-a_2)(b_1-a_1) \hspace{1mm}=\hspace{1mm} f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big). $$ I believe I understand the proof well, which essentially consists of applying the ordinary MVT to the function $[a_1,b_1]\to\mathbb{R} : t \mapsto f(t,b_2) - f(t,a_2)$ . Then, $$ f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big) \hspace{1mm}=\hspace{1mm} (b_1-a_1)\left(\frac{\partial f(c_1,b_2)}{\partial x_1} - \frac{\partial f(c_1,a_2)}{\partial x_1}\right) \hspace{1mm}=\hspace{1mm} (b_1-a_1)(b_2-a_2) \frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1} $$ where the second equality follows from applying the MVT to the function $t \mapsto \frac{\partial f(c_1,t)}{\partial x_2}$ . My question is other. Preceding the theorem, he writes (pg. 235) For simplicity (and without loss of generality) we state our next two theorems for real functions of two variables. Man, that wouldn't have bothered me, if not for his unjustified claim that the omission of cases $n>2$ is WLOG. I'm currently stumped trying to derive an analogous formula for ${n=3}$ (I keep getting partial derivatives that don't cancel). My quesiton is: does someone happen to know the general formula? I'm really tempted to try to derive it, but realistically I just don't know if I can spare the time. If I were going to, a hint might not be a bad thing. My first several google searches have turned up nothing. Also, Rudin's choice of words suggests to me that we may yet get analogous results for functions which are not only real-valued. What would such results be? Thanks for your time.","Let . Let be an open subset of . Let be a map. Let be an -cell contained in . To be concise, suppose that for each the partial derivative exists at every . In POMA (3e), Rudin shows (Thm 9.40) that in the case , there exists a such that I believe I understand the proof well, which essentially consists of applying the ordinary MVT to the function . Then, where the second equality follows from applying the MVT to the function . My question is other. Preceding the theorem, he writes (pg. 235) For simplicity (and without loss of generality) we state our next two theorems for real functions of two variables. Man, that wouldn't have bothered me, if not for his unjustified claim that the omission of cases is WLOG. I'm currently stumped trying to derive an analogous formula for (I keep getting partial derivatives that don't cancel). My quesiton is: does someone happen to know the general formula? I'm really tempted to try to derive it, but realistically I just don't know if I can spare the time. If I were going to, a hint might not be a bad thing. My first several google searches have turned up nothing. Also, Rudin's choice of words suggests to me that we may yet get analogous results for functions which are not only real-valued. What would such results be? Thanks for your time.","n\in\mathbb{Z}^+ A \mathbb{R}^n f : A\to\mathbb{R} K = [a_1,b_1]\times\cdots\times[a_n,b_n] n A (a_i<b_i \hspace{1mm}\forall i\in\{1,\ldots,n\}) k\in\{1,\ldots,n\}, 
    \frac{\partial^k f(c)}{\partial x_k \partial x_{k-1} \cdots \partial x_1}(c)
 c \in K n=2 (c_1,c_2) \in K 
\frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1} (b_2-a_2)(b_1-a_1)
\hspace{1mm}=\hspace{1mm}
f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big).
 [a_1,b_1]\to\mathbb{R} : t \mapsto f(t,b_2) - f(t,a_2) 
f(b_1,b_2) - f(b_1,a_2) - \big(f(a_1,b_2) - f(a_1,a_2)\big)
\hspace{1mm}=\hspace{1mm}
(b_1-a_1)\left(\frac{\partial f(c_1,b_2)}{\partial x_1} - \frac{\partial f(c_1,a_2)}{\partial x_1}\right)
\hspace{1mm}=\hspace{1mm}
(b_1-a_1)(b_2-a_2) \frac{\partial^2f(c_1,c_2)}{\partial x_2 \partial x_1}
 t \mapsto \frac{\partial f(c_1,t)}{\partial x_2} n>2 {n=3}","['real-analysis', 'analysis', 'multivariable-calculus']"
51,Lee's Introduction to Smooth Manifolds Problem 3-1,Lee's Introduction to Smooth Manifolds Problem 3-1,,"Problem 3-1: Let $M$ and $N$ be smooth manifolds with or without boundary, and let $F:M\to N$ be a smooth map. Show that $dF_p:T_pM\to T_{F(p)}N$ is the zero map if and only if $F$ is constant on each component of $M$ . This is my attempt: In local coordinates, we have $$dF_p\left(\frac{\partial}{\partial x^i}\Big|_p\right)=\frac{\partial F^j}{\partial x^i}(p)\frac{\partial}{\partial x^j}\Big|_{F(p)}\,.$$ Hence, $dF_p=0$ if and only if, for all $i$ , $dF_p\left(\frac{\partial}{\partial x^i}|_p\right)=0$ , if and only if $\frac{\partial F^j}{\partial x^i}(p)=0$ for all $i,j$ , if and only if $F^j=$ constant for all $j$ . But after looking at this question I can see I am wrong and the solution is much more complicated. Why is my answer wrong?","Problem 3-1: Let and be smooth manifolds with or without boundary, and let be a smooth map. Show that is the zero map if and only if is constant on each component of . This is my attempt: In local coordinates, we have Hence, if and only if, for all , , if and only if for all , if and only if constant for all . But after looking at this question I can see I am wrong and the solution is much more complicated. Why is my answer wrong?","M N F:M\to N dF_p:T_pM\to T_{F(p)}N F M dF_p\left(\frac{\partial}{\partial x^i}\Big|_p\right)=\frac{\partial F^j}{\partial x^i}(p)\frac{\partial}{\partial x^j}\Big|_{F(p)}\,. dF_p=0 i dF_p\left(\frac{\partial}{\partial x^i}|_p\right)=0 \frac{\partial F^j}{\partial x^i}(p)=0 i,j F^j= j","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
52,Vector that realizes a given slope,Vector that realizes a given slope,,"On a book I read the following: the slope of the level set of a function G through the   point $x_0,y_0$ ,is $-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)}$ , so that the   vector which realizes the slope is: $v=(1,-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)})$ . What is not clear to me is why is $v=(1,-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)})$ the vector that realizes the slope $-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)}$ . Can you help me?","On a book I read the following: the slope of the level set of a function G through the   point ,is , so that the   vector which realizes the slope is: . What is not clear to me is why is the vector that realizes the slope . Can you help me?","x_0,y_0 -\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)} v=(1,-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)}) v=(1,-\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)}) -\frac{\frac{∂G}{∂x}(x_0,y_0)}{\frac{∂G}{∂y}(x_0,y_0)}","['calculus', 'multivariable-calculus', 'vectors']"
53,Calculus II: Limit exercise,Calculus II: Limit exercise,,"I'm currently studying for my Calc exam and I came across this exercise. The problem is to find the values for $a,b,c \in \mathbb{R}$ so that the following limit exists: $$ \lim_{(x,y) \rightarrow (0,0)} \frac{xy}{ax^2+bxy+cy^2} $$ Along the path $y=kx$ , the expression becomes: $$ \lim_{(x,kx) \rightarrow (0,0)}\frac{kx^2}{ax^2+kbx^2+k^2cx^2} $$ $$ \lim_{(x,kx) \rightarrow (0,0)}\frac{k}{a+kb+k^2c}$$ If we evaluate the limit along the path where $k=0$ or $k=1$ , we get $0$ and $\frac{1}{a+b+c}$ respectively. because the second one can never become zero for any value of $a,b,c$ , I would conclude that the limit doesn't exist, but I don't think that's the correct answer, so I'm probably doing something wrong. Some help would be appreciated!","I'm currently studying for my Calc exam and I came across this exercise. The problem is to find the values for so that the following limit exists: Along the path , the expression becomes: If we evaluate the limit along the path where or , we get and respectively. because the second one can never become zero for any value of , I would conclude that the limit doesn't exist, but I don't think that's the correct answer, so I'm probably doing something wrong. Some help would be appreciated!","a,b,c \in \mathbb{R}  \lim_{(x,y) \rightarrow (0,0)} \frac{xy}{ax^2+bxy+cy^2}  y=kx  \lim_{(x,kx) \rightarrow (0,0)}\frac{kx^2}{ax^2+kbx^2+k^2cx^2}   \lim_{(x,kx) \rightarrow (0,0)}\frac{k}{a+kb+k^2c} k=0 k=1 0 \frac{1}{a+b+c} a,b,c","['calculus', 'limits', 'multivariable-calculus']"
54,Coordinate change to make function linear on a neighborhood,Coordinate change to make function linear on a neighborhood,,"The following was stated over at Mathoverflow when discussing applications of the Implicit function theorem. By changing coordinates you can make a simple function appear complicated. Have you ever asked yourself if the opposite is true: given a ""complicated"" function, can I make it look simpler in a neighborhood of a point by changing the coordinates near that point? The implicit function theorem states that if that point is not a critical point, then you can find coordinates near that point such that, in these coordinates the function is a linear function. However, I do not quite see how this follows from the theorem I know: Let $F:\mathbb{R}^{n+m}\to\mathbb{R}^m $ be continuously differentiable, then at a point $(\mathbf{a},\mathbf{b})$ such that $F(\mathbf{a},\mathbf{b})=\mathbf{0}$ , the Jacobian $${\displaystyle J_{F,\mathbf {y} }(\mathbf {a} ,\mathbf {b} )=\left[{\frac {\partial F_{i}}{\partial y_{j}}}(\mathbf {a} ,\mathbf {b} )\right]}$$ tells us if locally near $(\mathbf{a},\mathbf{b})\in U$ an implicit function $f:U\subset \mathbb{R}^n\to\mathbb{R}^m$ exists such that $F(\mathbf{x},f(\mathbf{x}))=\mathbf{0}$ for all $x$ in the neighborhood $U$ . Additionally, it provides a formula for the Jacobian of the implicit function $f$ . Now, I cannot wrap my head around how you use this to show that you can make any multivariable function locally linear with respect to some coordinates.","The following was stated over at Mathoverflow when discussing applications of the Implicit function theorem. By changing coordinates you can make a simple function appear complicated. Have you ever asked yourself if the opposite is true: given a ""complicated"" function, can I make it look simpler in a neighborhood of a point by changing the coordinates near that point? The implicit function theorem states that if that point is not a critical point, then you can find coordinates near that point such that, in these coordinates the function is a linear function. However, I do not quite see how this follows from the theorem I know: Let be continuously differentiable, then at a point such that , the Jacobian tells us if locally near an implicit function exists such that for all in the neighborhood . Additionally, it provides a formula for the Jacobian of the implicit function . Now, I cannot wrap my head around how you use this to show that you can make any multivariable function locally linear with respect to some coordinates.","F:\mathbb{R}^{n+m}\to\mathbb{R}^m  (\mathbf{a},\mathbf{b}) F(\mathbf{a},\mathbf{b})=\mathbf{0} {\displaystyle J_{F,\mathbf {y} }(\mathbf {a} ,\mathbf {b} )=\left[{\frac {\partial F_{i}}{\partial y_{j}}}(\mathbf {a} ,\mathbf {b} )\right]} (\mathbf{a},\mathbf{b})\in U f:U\subset \mathbb{R}^n\to\mathbb{R}^m F(\mathbf{x},f(\mathbf{x}))=\mathbf{0} x U f","['multivariable-calculus', 'coordinate-systems', 'implicit-function-theorem']"
55,An example of differentiable functions with discontinuous partial derivatives,An example of differentiable functions with discontinuous partial derivatives,,"Let $$f(x,y)=\left\{\begin{array}{l} (x^2+y^2) \sin(\frac{1}{\sqrt{x^2+y^2}}),\:\text{if $(x,y) \not= (0,0)$;}\\  0,\:\text{if  $(x,y)=(0,0)$;} \end{array}\right.$$ Prove that the partial derivatives exist for all $c \in R^2$ but are not continuous in $(0,0)$ My approach: I have already proved that the partial derivatives exist if $(x,y) \neq (0,0)$ . and n the case that $(x,y) = (0,0)$ : \begin{align} \frac {\partial f}{\partial x}(0,0)&=\lim_{t \rightarrow 0} \frac{f[(0,0)+t(1,0)]-f(0,0)}{t}\\ &=\lim_{t \rightarrow 0} \frac{f[(t,0)]}{t} \\ &=\lim_{t \rightarrow 0} \frac{t^2 \sin (1/t)}{t} \\ &=\lim_{t \rightarrow 0} {t \sin (1/t)} \end{align} and due to $\lim_{t \rightarrow 0} t=0$ and $\sin (1/t) \le 1 $ then $=\lim_{t \rightarrow 0} {t \sin (1/t)}=0$ In the same way I can prove that $\frac {\partial f}{\partial y}(0,0)=0$ In order to prove that the partial derivatives are not continuous I must prove that $\lim_{(x,y) \rightarrow (0,0)} \frac {\partial f}{\partial x}(0,0) \neq 0$ I'm stuck here. Any ideas?",Let Prove that the partial derivatives exist for all but are not continuous in My approach: I have already proved that the partial derivatives exist if . and n the case that : and due to and then In the same way I can prove that In order to prove that the partial derivatives are not continuous I must prove that I'm stuck here. Any ideas?,"f(x,y)=\left\{\begin{array}{l}
(x^2+y^2) \sin(\frac{1}{\sqrt{x^2+y^2}}),\:\text{if (x,y) \not= (0,0);}\\ 
0,\:\text{if  (x,y)=(0,0);} \end{array}\right. c \in R^2 (0,0) (x,y) \neq (0,0) (x,y) = (0,0) \begin{align}
\frac {\partial f}{\partial x}(0,0)&=\lim_{t \rightarrow 0} \frac{f[(0,0)+t(1,0)]-f(0,0)}{t}\\
&=\lim_{t \rightarrow 0} \frac{f[(t,0)]}{t} \\
&=\lim_{t \rightarrow 0} \frac{t^2 \sin (1/t)}{t} \\
&=\lim_{t \rightarrow 0} {t \sin (1/t)}
\end{align} \lim_{t \rightarrow 0} t=0 \sin (1/t) \le 1  =\lim_{t \rightarrow 0} {t \sin (1/t)}=0 \frac {\partial f}{\partial y}(0,0)=0 \lim_{(x,y) \rightarrow (0,0)} \frac {\partial f}{\partial x}(0,0) \neq 0","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
56,Applications of Lagrange Multiplier in Economics or Computer Science,Applications of Lagrange Multiplier in Economics or Computer Science,,"I'm a high school student studying grade 12 maths and I need to write a 12 page paper on the Lagrange Multiplier. I have understood the mathematics behind it but I need a real equation or application of this concept which one can observe in the real world. Like an equation which has been derived from a company's revenue or something like that. This is required since I need to incorporate the use of mathematics to a real-life situation. If you cannot think of any equations right off the top of your head, could you please suggest methods to derive the equation using data? Any type of help is very much appreciated. Thank you","I'm a high school student studying grade 12 maths and I need to write a 12 page paper on the Lagrange Multiplier. I have understood the mathematics behind it but I need a real equation or application of this concept which one can observe in the real world. Like an equation which has been derived from a company's revenue or something like that. This is required since I need to incorporate the use of mathematics to a real-life situation. If you cannot think of any equations right off the top of your head, could you please suggest methods to derive the equation using data? Any type of help is very much appreciated. Thank you",,"['multivariable-calculus', 'optimization', 'computer-science', 'lagrange-multiplier', 'economics']"
57,How does one compute this double integral?,How does one compute this double integral?,,"$$\int_0^2\int_0^{y^3} x^5(2-x^{1/3})^{-1} \, dx \, dy$$ I've tried change of variables, which didn't seem to help much. I've also tried u-substitution and that didn't help. What I noticed is that there is $x=y^3$ so $y=x^{1/3}$ , but I wasn't sure how to use that fact besides change of variables.","I've tried change of variables, which didn't seem to help much. I've also tried u-substitution and that didn't help. What I noticed is that there is so , but I wasn't sure how to use that fact besides change of variables.","\int_0^2\int_0^{y^3} x^5(2-x^{1/3})^{-1} \, dx \, dy x=y^3 y=x^{1/3}","['integration', 'multivariable-calculus']"
58,A question from Spivak's Calculus on Manifolds,A question from Spivak's Calculus on Manifolds,,"$$ \begin{array}{l}{\text { EXERCISE } 32(2-2) . \text { A function } f: \mathbb{R}^{2} \rightarrow \mathbb{R} \text { is independent of the second vari- }} \\ {\text { able if for each } x \in \mathbb{R} \text { we have } f\left(x, y_{1}\right)=f\left(x, y_{2}\right) \text { for all } y_{1}, y_{2} \in \mathbb{R} . \text { Show that } f} \\ {\text { is independent of the second variable if and only if there is a function } g: \mathbb{R} \rightarrow \mathbb{R}} \\ {\text { such that } f(x, y)=g(x) . \text { What is } f^{\prime}(a, b) \text { in terms of } g^{\prime} ?}\end{array} $$ My question is short: What is the ''independent'' mean in the question? Thanks...",My question is short: What is the ''independent'' mean in the question? Thanks...,"
\begin{array}{l}{\text { EXERCISE } 32(2-2) . \text { A function } f: \mathbb{R}^{2} \rightarrow \mathbb{R} \text { is independent of the second vari- }} \\ {\text { able if for each } x \in \mathbb{R} \text { we have } f\left(x, y_{1}\right)=f\left(x, y_{2}\right) \text { for all } y_{1}, y_{2} \in \mathbb{R} . \text { Show that } f} \\ {\text { is independent of the second variable if and only if there is a function } g: \mathbb{R} \rightarrow \mathbb{R}} \\ {\text { such that } f(x, y)=g(x) . \text { What is } f^{\prime}(a, b) \text { in terms of } g^{\prime} ?}\end{array}
",['real-analysis']
59,The Hessian of a Radial Basis Function,The Hessian of a Radial Basis Function,,"In this paper the Hessian of a RBF is shown as $$ H_j = \frac{\beta_j}{r_j(x)} \bigg( \phi'(r_j) I + \bigg[ \phi''(r_j) - \frac{\phi'(r_j)}{r_j(x)}  \bigg] \bigg) (x-x_j) \frac{\partial r_j}{\partial x} $$ where $$ r_j = || x-x_j|| = \sqrt{(x-x_j) ^T (x-x_j)} $$ and $\phi(r)$ could be one of the many RBF functions, such as $\phi(r) = \exp(-cr^2)$ . An RBF is of course $$ f(x) = \beta^T g(x) = \sum_i \beta_i \phi(r_i) $$ I understand how the Jacobian is obtained, but was not able to derive the Hessian. Where did the identity matrix come from? Why $H_j$ and not $H_{ij}$ ? Any help would be appreciated.","In this paper the Hessian of a RBF is shown as where and could be one of the many RBF functions, such as . An RBF is of course I understand how the Jacobian is obtained, but was not able to derive the Hessian. Where did the identity matrix come from? Why and not ? Any help would be appreciated.","
H_j = \frac{\beta_j}{r_j(x)} \bigg(
\phi'(r_j) I +
\bigg[
\phi''(r_j) - \frac{\phi'(r_j)}{r_j(x)} 
\bigg]
\bigg) (x-x_j) \frac{\partial r_j}{\partial x}
 
r_j = || x-x_j|| = \sqrt{(x-x_j) ^T (x-x_j)}
 \phi(r) \phi(r) = \exp(-cr^2) 
f(x) = \beta^T g(x) = \sum_i \beta_i \phi(r_i)
 H_j H_{ij}","['multivariable-calculus', 'partial-derivative', 'matrix-calculus', 'hessian-matrix']"
60,Maximum value of $8v_1 - 6v_2 - v_1^2 - v_2^2$ subject to $v_1^2+v_2^2\leq 1$,Maximum value of  subject to,8v_1 - 6v_2 - v_1^2 - v_2^2 v_1^2+v_2^2\leq 1,"Given that $g:\mathbb{R}^2 \to \mathbb{R}$ defined by $$g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2$$ find the maximum value of $g$ subject to the constraint $v_1^2+v_2^2\leq 1.$ My attempt: Note that $$g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2 = -(v_1-4)^2 - (v_2+3)^2 + 25.$$ So $g$ is a decreasing function. So, the maximum value of $g$ lies on the circumference of $v_1^2+v_2^2 = 1.$ It suffices to find the intersection between $(0,0)$ and $(4,-3)$ as $(4,-3)$ is the peak point of $g.$ The intersection point lies on both $v_1^2+v_2^2 = 1$ and $v_2 = -\frac{3}{4}v_1.$ Solving the simultaneous equation gives that $$v_1 = \frac{4}{5}, \quad v_2 = -\frac{3}{5}.$$ So, maximum value of $g$ is $$g\left(\frac{4}{5}, -\frac{3}{5}\right) = 9.$$ Is my attempt correct?","Given that defined by find the maximum value of subject to the constraint My attempt: Note that So is a decreasing function. So, the maximum value of lies on the circumference of It suffices to find the intersection between and as is the peak point of The intersection point lies on both and Solving the simultaneous equation gives that So, maximum value of is Is my attempt correct?","g:\mathbb{R}^2 \to \mathbb{R} g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2 g v_1^2+v_2^2\leq 1. g(v_1,v_2) = 8v_1 - 6v_2 - v_1^2 - v_2^2 = -(v_1-4)^2 - (v_2+3)^2 + 25. g g v_1^2+v_2^2 = 1. (0,0) (4,-3) (4,-3) g. v_1^2+v_2^2 = 1 v_2 = -\frac{3}{4}v_1. v_1 = \frac{4}{5}, \quad v_2 = -\frac{3}{5}. g g\left(\frac{4}{5}, -\frac{3}{5}\right) = 9.","['multivariable-calculus', 'optimization', 'convex-optimization', 'qcqp']"
61,"When we find the norm of a vector, why don't we square $\vec{i}$ and $\vec{j}$? And what would it even mean to square them?","When we find the norm of a vector, why don't we square  and ? And what would it even mean to square them?",\vec{i} \vec{j},"It's been too long since I learned linear algebra, so apologies for the basic questions. Here's a snippet from my textbook: Why is it that in the calculation of $||\vec{v}||$ we don't square the $\vec{i}$ and $\vec{j}$ terms even though they're in the definition of $\vec{v}$ right above? Also, what does it mean to ""square"" a unit vector (or a vector in general)? I'm reading conflicting things online. Does $(\vec{i})^2$ denote calculating the dot product $<\vec{i},\vec{i}>$ and getting $1$ as the answer? Or would $(\vec{i})^2$ mean that we take the projection of $\vec{i}$ onto itself?","It's been too long since I learned linear algebra, so apologies for the basic questions. Here's a snippet from my textbook: Why is it that in the calculation of we don't square the and terms even though they're in the definition of right above? Also, what does it mean to ""square"" a unit vector (or a vector in general)? I'm reading conflicting things online. Does denote calculating the dot product and getting as the answer? Or would mean that we take the projection of onto itself?","||\vec{v}|| \vec{i} \vec{j} \vec{v} (\vec{i})^2 <\vec{i},\vec{i}> 1 (\vec{i})^2 \vec{i}","['calculus', 'linear-algebra', 'multivariable-calculus', 'vectors']"
62,"Calculate partial derivatives $f_x(x,y)$ and $f_y(x,y)$ if $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt$",Calculate partial derivatives  and  if,"f_x(x,y) f_y(x,y) f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt","Calculate partial derivatives $f_x(x,y)$ and $f_y(x,y)$ if $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt$ Idea: if $G'(t)=g(t)$ for all $t$ . Then $\int_y^xg(t)dt= G(x)-G(y)$ and $\int_x^yg(t)dt=G(y)-G(x)$ . Hence $f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt=\int_{G(x)-G(y)}^{G(y)-G(x)}g(t)dt=G(G(y)-G(x))-G(G(x)-G(y))$ . Then $f_x(x,y)=-G'(G(y)-G(x))G'(x)-G'(G(x)-G(y))G'(x)=-g(x)(g(\int_x^yg(t)dt)+g(\int_y^xg(t)dt))?$ Is correct my idea? Thanks...",Calculate partial derivatives and if Idea: if for all . Then and . Hence . Then Is correct my idea? Thanks...,"f_x(x,y) f_y(x,y) f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt G'(t)=g(t) t \int_y^xg(t)dt= G(x)-G(y) \int_x^yg(t)dt=G(y)-G(x) f(x,y)=\int_{\int_y^xg(t)dt}^{\int_x^yg(t)dt}g(t)dt=\int_{G(x)-G(y)}^{G(y)-G(x)}g(t)dt=G(G(y)-G(x))-G(G(x)-G(y)) f_x(x,y)=-G'(G(y)-G(x))G'(x)-G'(G(x)-G(y))G'(x)=-g(x)(g(\int_x^yg(t)dt)+g(\int_y^xg(t)dt))?","['calculus', 'analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
63,"Calculate the volume enclosed by the paraboloid $f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2$",Calculate the volume enclosed by the paraboloid,"f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2","Calculate the volume enclosed by the paraboloid $f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2$ and the plane $z=0$ , when $f(x,y)$ is defined in $D=[0,1]\times [0,1]$ . I used polar coordinates and I had the following integral, $$\int _{0}^{\frac{\pi}{2}} \int _{0}^{1} \left(1-\frac{1}{2}r^2\right)\,dr\,d\theta =\dfrac{3\pi}{16}$$ Is it right?","Calculate the volume enclosed by the paraboloid and the plane , when is defined in . I used polar coordinates and I had the following integral, Is it right?","f(x,y)=1-\frac{1}{2}x^2-\frac{1}{2}y^2 z=0 f(x,y) D=[0,1]\times [0,1] \int _{0}^{\frac{\pi}{2}} \int _{0}^{1} \left(1-\frac{1}{2}r^2\right)\,dr\,d\theta =\dfrac{3\pi}{16}","['multivariable-calculus', 'volume']"
64,Finding the gradient and Hessian of $\frac{1}{2}|Ax-By|^2$,Finding the gradient and Hessian of,\frac{1}{2}|Ax-By|^2,"I'm trying to compute the gradient and Hessian of the following function $$f(x,y) = \frac{1}{2}|Ax-By|^2$$ where $A$ and $B$ are $m \times n$ matrices, $x, y \in \mathbb{R}^n$ , and $f: \mathbb{R}^{2n} \to \mathbb{R}$ . I honestly don't have a clue on the best way to proceed. Usually, to find the gradient, I would rewrite the function in sums and derive from there - but the square and multiple vector arguments have me stumped. I am not looking for a solution but rather a hint on where to start. Furthermore, am I right in thinking that $\nabla f(x,y)$ is a vector in $\mathbb{R}^{2n}$ consisting of the partial derivatives along $x$ and $y$ , and $\nabla^2 f(x,y)$ to be a $2n \times 2n$ matrix? Thank you in advance.","I'm trying to compute the gradient and Hessian of the following function where and are matrices, , and . I honestly don't have a clue on the best way to proceed. Usually, to find the gradient, I would rewrite the function in sums and derive from there - but the square and multiple vector arguments have me stumped. I am not looking for a solution but rather a hint on where to start. Furthermore, am I right in thinking that is a vector in consisting of the partial derivatives along and , and to be a matrix? Thank you in advance.","f(x,y) = \frac{1}{2}|Ax-By|^2 A B m \times n x, y \in \mathbb{R}^n f: \mathbb{R}^{2n} \to \mathbb{R} \nabla f(x,y) \mathbb{R}^{2n} x y \nabla^2 f(x,y) 2n \times 2n","['multivariable-calculus', 'derivatives', 'quadratics', 'matrix-calculus', 'hessian-matrix']"
65,Can one always interchange the order of a surface and volume integral?,Can one always interchange the order of a surface and volume integral?,,"Consider a continuous charge distribution in volume $V'$ . Draw a closed surface $S$ inside the volume $V'$ . Consider the following multiple integral: $$A=\iiint_{V'}   \left[      \iint_S   \dfrac{\cos(\hat{R},\hat{n})}{R^2} dS    \right] \rho'\ dV' =4 \pi\ m_s$$ where $R=|\mathbf{r}-\mathbf{r'}|$ $\mathbf{r'}=(x',y',z')$ is coordinates of source points $\mathbf{r}=(x,y,z)$ is coordinates of field points $\cos(\hat{R},\hat{n})$ is the angle between $R$ and normal to surface element $\rho'$ is the charge density and is continuous throughout the volume $V'$ $m_s$ is the total charge inside surface $S$ Also consider the following multiple integral: $$B= \iint_S        \left[    \iiint_{V'}     \dfrac{\cos(\hat{R},\hat{n})}{R^2}   \rho'\ dV'  \right] dS$$ where the symbols have the meanings stated above. \begin{align} B &= \iint_S        \left[    \iiint_{V'}  \rho'   \dfrac{\hat{R} \cdot \hat{n}}{R^2}   \ dV'  \right] dS\\ &=\iint_S        \left[    \iiint_{V'}  \rho'   \dfrac{\hat{R} }{R^2}   \ dV'  \right] \cdot \hat{n}\ dS\\ &=\iint_S        \mathbf{E} \cdot \hat{n}\ dS \end{align} Is $A=B\ ?$ i.e. Is interchanging the order of surface and volume integration valid? I know it is usually valid but my doubt is due to the following reasons: In the surface integral of equation $A$ , when $\mathbf{r'} \in S$ , we can only use spherical coordinate system with origin at point $\mathbf{r'}$ (in order to avoid improper integral with limits). So while computing $A$ , we cannot use only one coordinate system. Instead, we have to use infinitely many coordinate systems. In the volume integral of equation $B$ , for all $\mathbf{r}$ , i.e. for all $\mathbf{r} \in S$ , we can only use spherical coordinate system with origin at point $\mathbf{r}$ (in order to avoid improper integral with limits). So while computing $B$ , we cannot use only one coordinate system. Instead, we have to use infinitely many coordinate systems. Edit: I know $\int \left[\int f(x,y)\,dx \right]dy = \int \left[\int f(x,y)\,dy \right]dx$ is true usually. Also, if in the diagram, if the volume $V'$ is contained within the surface $S$ , then it is valid to change the order of integration. But here the issue is a little different. The surface $S$ is inside the volume $V'$ (please have a look at my diagram) and thus improper integral comes into play. While computing $A$ , if we need to avoid improper integrals, we have no choice except to work with infinitely many spherical coordinate systems each having their origin at points $\in V'$ . Similarly while computing $B$ , if we need to avoid improper integrals, we have no choice except to work with infinitely many spherical coordinate systems each having their origin at points $\in S$ . Then how is it valid to change the order of integration in this situation? That is, how can $A=B?$","Consider a continuous charge distribution in volume . Draw a closed surface inside the volume . Consider the following multiple integral: where is coordinates of source points is coordinates of field points is the angle between and normal to surface element is the charge density and is continuous throughout the volume is the total charge inside surface Also consider the following multiple integral: where the symbols have the meanings stated above. Is i.e. Is interchanging the order of surface and volume integration valid? I know it is usually valid but my doubt is due to the following reasons: In the surface integral of equation , when , we can only use spherical coordinate system with origin at point (in order to avoid improper integral with limits). So while computing , we cannot use only one coordinate system. Instead, we have to use infinitely many coordinate systems. In the volume integral of equation , for all , i.e. for all , we can only use spherical coordinate system with origin at point (in order to avoid improper integral with limits). So while computing , we cannot use only one coordinate system. Instead, we have to use infinitely many coordinate systems. Edit: I know is true usually. Also, if in the diagram, if the volume is contained within the surface , then it is valid to change the order of integration. But here the issue is a little different. The surface is inside the volume (please have a look at my diagram) and thus improper integral comes into play. While computing , if we need to avoid improper integrals, we have no choice except to work with infinitely many spherical coordinate systems each having their origin at points . Similarly while computing , if we need to avoid improper integrals, we have no choice except to work with infinitely many spherical coordinate systems each having their origin at points . Then how is it valid to change the order of integration in this situation? That is, how can","V' S V' A=\iiint_{V'}   \left[      \iint_S   \dfrac{\cos(\hat{R},\hat{n})}{R^2} dS    \right] \rho'\ dV' =4 \pi\ m_s R=|\mathbf{r}-\mathbf{r'}| \mathbf{r'}=(x',y',z') \mathbf{r}=(x,y,z) \cos(\hat{R},\hat{n}) R \rho' V' m_s S B= \iint_S        \left[    \iiint_{V'}     \dfrac{\cos(\hat{R},\hat{n})}{R^2}   \rho'\ dV'  \right] dS \begin{align}
B &= \iint_S        \left[    \iiint_{V'}  \rho'   \dfrac{\hat{R} \cdot \hat{n}}{R^2}   \ dV'  \right] dS\\
&=\iint_S        \left[    \iiint_{V'}  \rho'   \dfrac{\hat{R} }{R^2}   \ dV'  \right] \cdot \hat{n}\ dS\\
&=\iint_S        \mathbf{E} \cdot \hat{n}\ dS
\end{align} A=B\ ? A \mathbf{r'} \in S \mathbf{r'} A B \mathbf{r} \mathbf{r} \in S \mathbf{r} B \int \left[\int f(x,y)\,dx \right]dy = \int \left[\int f(x,y)\,dy \right]dx V' S S V' A \in V' B \in S A=B?","['calculus', 'integration', 'multivariable-calculus', 'solid-angle']"
66,Sketching the region of $\int_0^2 \int^{y^2}_0dxdy$,Sketching the region of,\int_0^2 \int^{y^2}_0dxdy,"My textbook shows the answer as being: Note that I've interpreted the $y^2x$ to be extraneous information for the problem of simply sketching the region the double integral is specifying; maybe I'm wrong about this? My question is: isn't the region negated in the answer? It seems to me that the region should be the region above the line $y=0$ and below the curve $x=y^2$ However, the question shows the region as being the region above the curve $x=y^2$ and below the line $y=2$ Why is the latter and not the former correct? How can we know which is being specified by the integral?","My textbook shows the answer as being: Note that I've interpreted the to be extraneous information for the problem of simply sketching the region the double integral is specifying; maybe I'm wrong about this? My question is: isn't the region negated in the answer? It seems to me that the region should be the region above the line and below the curve However, the question shows the region as being the region above the curve and below the line Why is the latter and not the former correct? How can we know which is being specified by the integral?",y^2x y=0 x=y^2 x=y^2 y=2,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
67,"Critical point of $f(x,y) = x^{3} -3xy^{2}, x,y \in \Bbb{ R}$",Critical point of,"f(x,y) = x^{3} -3xy^{2}, x,y \in \Bbb{ R}","Consider the function $f(x,y) = x^{3} -3xy^{2}, x,y \in \Bbb{R}$ . Then what can you conclude about its critical point $(0,0) ?$ For this function $f_{xx} f_{yy} - f_{xy}^{2} = 0$ at $(0,0)$ So the test gives no result. The function can be written as $x(x^{2} - 3y^{2}) = x(x- √3 y)(x+√3 y)$ Now how do we know the nature of function near $(0,0) $ ?",Consider the function . Then what can you conclude about its critical point For this function at So the test gives no result. The function can be written as Now how do we know the nature of function near ?,"f(x,y) = x^{3} -3xy^{2}, x,y \in \Bbb{R} (0,0) ? f_{xx} f_{yy} - f_{xy}^{2} = 0 (0,0) x(x^{2} - 3y^{2}) = x(x- √3 y)(x+√3 y) (0,0) ","['calculus', 'multivariable-calculus']"
68,Zeroes of multivariate polynomials,Zeroes of multivariate polynomials,,"Assume you have $m$ points $x_1,\ldots,x_m$ in $\mathbb{R}^d$ and you want to find a polynomial $p$ in $d$ indeterminates such that $p(x_i)=0$ for all $i$ , and the value of $p(x)$ is negative at any other point. Is this possible/can you construct one such polynomial?","Assume you have points in and you want to find a polynomial in indeterminates such that for all , and the value of is negative at any other point. Is this possible/can you construct one such polynomial?","m x_1,\ldots,x_m \mathbb{R}^d p d p(x_i)=0 i p(x)","['multivariable-calculus', 'polynomials']"
69,Smooth Manifold Chart Lemma for Manifolds with Boundary,Smooth Manifold Chart Lemma for Manifolds with Boundary,,"Let's start with Lemma 1.35 (Smooth Manifold Chart Lemma for Manifolds Without Boundary) in John Lee's Textbook ""Introduction to Smooth Manifolds"" (Second Edition). The precise statement is: Let $M$ be a set and $\{U_\alpha\}_{\alpha\in J}$ be a collection of subsets of $M$ , along with maps $\varphi_\alpha:U_\alpha\to\mathbb R^n$ , such that the following properties are satisfied: (i) $\forall \alpha\in J$ : $\varphi_\alpha$ is an injective map and $\varphi_\alpha(U_\alpha)$ is open in $\mathbb R^n$ . (ii) $\forall \alpha,\beta\in J$ : the sets $\varphi_\alpha(U_\alpha\cap U_\beta)$ and $\varphi_\beta(U_\alpha\cap U_\beta)$ are open in $\mathbb R^n$ . (iii) $\forall\alpha,\beta\in J$ : $U_\alpha\cap U_\beta\neq \emptyset 			\quad 			\Rightarrow 			\quad \varphi_\beta\circ\varphi_\alpha^{-1}:\varphi_\alpha(U_\alpha\cap U_\beta)\to \varphi_\beta(U_\alpha\cap U_\beta)$ is smooth. (iv) Countably many of the sets $U_\alpha$ cover $M$ . (v) $ \left. \begin{array}{c} p,q\in M\\ p\neq q \end{array} \right\} \quad \Rightarrow \quad \left\{ \begin{array}{c} \exists \alpha\in J\text{ such that } p,q\in U_\alpha,\quad\text{ or}\\ \exists \alpha,\beta\in J\text{ such that } p\in U_\alpha, q\in U_\beta \text{ and } U_\alpha\cap U_\beta=\emptyset \end{array} \right. $ Then $M$ has a unique manifold structure such that each pair $(U_\alpha,\varphi_\alpha)$ is a smooth chart. Let $\mathcal B=\{\varphi_\alpha^{-1}(V):\alpha\in J, V\text{ open in } \mathbb R^n\}$ . From $(iv)$ we see that the elements of $\mathcal B$ cover $M$ . Now let $\varphi_\alpha^{-1}(V)$ and $\varphi_\beta^{-1}(W)$ be two elements of $\mathcal B$ , where $V$ and $W$ are open in $\mathbb R^n$ . To show that $\mathcal B$ forms a basis, it is enough to show that $ \varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W)$ itself lies in $\mathcal B$ . Note that \begin{equation*} \varphi_\alpha^{-1}(V)\cap \varphi_\beta^{-1}(W)=\varphi_\alpha^{-1}\Big(V\cap(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)\Big) \tag{1} \end{equation*} But by (iii), $\varphi_\beta\circ\varphi_\alpha^{-1}$ is continuous, and therefore $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)$ is open in $\varphi_\alpha(U_\alpha\cap U_\beta)$ . By (ii), $\varphi_\alpha(U_\alpha\cap U_\beta)$ is open in $\mathbb R^n$ and therefore $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)$ is open in $\mathbb R^n$ . Using this in $(1)$ , we immediately see that $\varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W)$ is in $\mathcal B$ . This settles the claim. The maps $\varphi_\alpha:U_\alpha \to \mathbb{R}^n$ are automatically continuous. To see they are homeomorphisms with the images, it is equivalent to show that $\varphi_\alpha$ is an open map. To this purpose, it is sufficient to show that $\varphi_\alpha(B)$ is open in $\mathbb{R}^n$ whenever $B$ is an element of $\mathcal{B}$ contained in $U_\alpha$ . An arbitrary element of $\mathcal {B}$ is of the form $\varphi^{-1}_\beta(W)$ with $W$ open in $\mathbb{R}^n$ .   We have $\varphi_\alpha(\varphi^{-1}_\beta(W))=\varphi_\alpha\circ\varphi_\beta^{-1}(W)$ is open in $\varphi_\alpha(U_\alpha \cap U_\beta)$ and thus in $\mathbb{R}^n$ . Question 1) Lee says that each map $\varphi_\alpha$ is an homeomorphism onto its image ""essentially by definition"" , but according to my argument above we are using again hypothesis (iii) and (ii). So, I would say that the continuity of the $\varphi_\alpha$ 's is ""essentially by definition"" (since we are putting in $\mathcal{B}$ all the counter images of the open subsets of $\mathbb{R}^n$ ) but not the openness. So, does my argument above (to show that the $\varphi_\alpha$ 's are homeomporphism onto their images) use unnecessaryly the hypothesis (iii) and (ii)? In other words, is there a simpler way (that justifies the sentence ""essentially by definition"") to see that the $\varphi_\alpha$ 's are homeomorphism onto their images? Question n° 2 On page 28 Exercise 1.42 says: Show that Lemma 1.35 holds with $\mathbb{R}^n$ replaced by $\mathbb{R}^n$ or $\mathbb{H}^n$ and ""smooth manifold"" replaced by ""smooth manifold with boundary"". I think I can copy the same proof of Lemma 1.35, but when I arrive to the point of showing that $ \varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W)$ itself lies in $\mathcal B$ I'm in trouble because I cannot show that $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)$ is open in $\mathbb{R}^n$ . What I know is that $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)$ is open in $\varphi_\alpha(U_\alpha\cap U_\beta)$ , and this last one can be open in $\mathbb{R}^n$ or $\mathbb{H}^n$ . In the latter case I have $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)=\mathbb{H}^n \cap S$ with $S$ open subet of $\mathbb{R}^n$ , but the set $\varphi_\alpha^{-1}(S)$ can be greater than the set $\varphi_\alpha^{-1}((\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W))$ . My current thought is that: if in the statement of Lemma 1.35 I change (ii) with (j) $\forall \alpha,\beta\in J$ : the set $\varphi_\alpha(U_\alpha\cap U_\beta)$ is open in $\varphi_\alpha(U_\alpha)$ nothing change in Lemma 1.35 but as regard Exercise 1.42, I have that $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)$ is open in $\varphi_\alpha(U_\alpha\cap U_\beta)$ , which is open in $\phi_\alpha(U_\alpha)$ and this last one can be open in $\mathbb{R}^n$ or $\mathbb{H}^n$ . In the latter case I have $(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)=\mathbb{H}^n \cap S$ with $S$ open subet of $\mathbb{R}^n$ , but since the image of $\varphi_\alpha$ lies in $\mathbb{H}^n$ I have also $\varphi_\alpha^{-1}(S)=\varphi_\alpha^{-1}((\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W))$ . Is my modification correct? Is my modification necessary? I suspect there are many simple things I'm missing, my apologies for this.","Let's start with Lemma 1.35 (Smooth Manifold Chart Lemma for Manifolds Without Boundary) in John Lee's Textbook ""Introduction to Smooth Manifolds"" (Second Edition). The precise statement is: Let be a set and be a collection of subsets of , along with maps , such that the following properties are satisfied: (i) : is an injective map and is open in . (ii) : the sets and are open in . (iii) : is smooth. (iv) Countably many of the sets cover . (v) Then has a unique manifold structure such that each pair is a smooth chart. Let . From we see that the elements of cover . Now let and be two elements of , where and are open in . To show that forms a basis, it is enough to show that itself lies in . Note that But by (iii), is continuous, and therefore is open in . By (ii), is open in and therefore is open in . Using this in , we immediately see that is in . This settles the claim. The maps are automatically continuous. To see they are homeomorphisms with the images, it is equivalent to show that is an open map. To this purpose, it is sufficient to show that is open in whenever is an element of contained in . An arbitrary element of is of the form with open in .   We have is open in and thus in . Question 1) Lee says that each map is an homeomorphism onto its image ""essentially by definition"" , but according to my argument above we are using again hypothesis (iii) and (ii). So, I would say that the continuity of the 's is ""essentially by definition"" (since we are putting in all the counter images of the open subsets of ) but not the openness. So, does my argument above (to show that the 's are homeomporphism onto their images) use unnecessaryly the hypothesis (iii) and (ii)? In other words, is there a simpler way (that justifies the sentence ""essentially by definition"") to see that the 's are homeomorphism onto their images? Question n° 2 On page 28 Exercise 1.42 says: Show that Lemma 1.35 holds with replaced by or and ""smooth manifold"" replaced by ""smooth manifold with boundary"". I think I can copy the same proof of Lemma 1.35, but when I arrive to the point of showing that itself lies in I'm in trouble because I cannot show that is open in . What I know is that is open in , and this last one can be open in or . In the latter case I have with open subet of , but the set can be greater than the set . My current thought is that: if in the statement of Lemma 1.35 I change (ii) with (j) : the set is open in nothing change in Lemma 1.35 but as regard Exercise 1.42, I have that is open in , which is open in and this last one can be open in or . In the latter case I have with open subet of , but since the image of lies in I have also . Is my modification correct? Is my modification necessary? I suspect there are many simple things I'm missing, my apologies for this.","M \{U_\alpha\}_{\alpha\in J} M \varphi_\alpha:U_\alpha\to\mathbb R^n \forall \alpha\in J \varphi_\alpha \varphi_\alpha(U_\alpha) \mathbb R^n \forall \alpha,\beta\in J \varphi_\alpha(U_\alpha\cap U_\beta) \varphi_\beta(U_\alpha\cap U_\beta) \mathbb R^n \forall\alpha,\beta\in J U_\alpha\cap U_\beta\neq \emptyset
			\quad
			\Rightarrow
			\quad \varphi_\beta\circ\varphi_\alpha^{-1}:\varphi_\alpha(U_\alpha\cap U_\beta)\to \varphi_\beta(U_\alpha\cap U_\beta) U_\alpha M 
\left.
\begin{array}{c}
p,q\in M\\
p\neq q
\end{array}
\right\}
\quad
\Rightarrow
\quad
\left\{
\begin{array}{c}
\exists \alpha\in J\text{ such that } p,q\in U_\alpha,\quad\text{ or}\\
\exists \alpha,\beta\in J\text{ such that } p\in U_\alpha, q\in U_\beta \text{ and } U_\alpha\cap U_\beta=\emptyset
\end{array}
\right.
 M (U_\alpha,\varphi_\alpha) \mathcal B=\{\varphi_\alpha^{-1}(V):\alpha\in J, V\text{ open in } \mathbb R^n\} (iv) \mathcal B M \varphi_\alpha^{-1}(V) \varphi_\beta^{-1}(W) \mathcal B V W \mathbb R^n \mathcal B  \varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W) \mathcal B \begin{equation*}
\varphi_\alpha^{-1}(V)\cap \varphi_\beta^{-1}(W)=\varphi_\alpha^{-1}\Big(V\cap(\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)\Big)
\tag{1}
\end{equation*} \varphi_\beta\circ\varphi_\alpha^{-1} (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W) \varphi_\alpha(U_\alpha\cap U_\beta) \varphi_\alpha(U_\alpha\cap U_\beta) \mathbb R^n (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W) \mathbb R^n (1) \varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W) \mathcal B \varphi_\alpha:U_\alpha \to \mathbb{R}^n \varphi_\alpha \varphi_\alpha(B) \mathbb{R}^n B \mathcal{B} U_\alpha \mathcal {B} \varphi^{-1}_\beta(W) W \mathbb{R}^n \varphi_\alpha(\varphi^{-1}_\beta(W))=\varphi_\alpha\circ\varphi_\beta^{-1}(W) \varphi_\alpha(U_\alpha \cap U_\beta) \mathbb{R}^n \varphi_\alpha \varphi_\alpha \mathcal{B} \mathbb{R}^n \varphi_\alpha \varphi_\alpha \mathbb{R}^n \mathbb{R}^n \mathbb{H}^n  \varphi_\alpha^{-1}(V)\cap\varphi_\beta^{-1}(W) \mathcal B (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W) \mathbb{R}^n (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W) \varphi_\alpha(U_\alpha\cap U_\beta) \mathbb{R}^n \mathbb{H}^n (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)=\mathbb{H}^n \cap S S \mathbb{R}^n \varphi_\alpha^{-1}(S) \varphi_\alpha^{-1}((\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)) \forall \alpha,\beta\in J \varphi_\alpha(U_\alpha\cap U_\beta) \varphi_\alpha(U_\alpha) (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W) \varphi_\alpha(U_\alpha\cap U_\beta) \phi_\alpha(U_\alpha) \mathbb{R}^n \mathbb{H}^n (\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W)=\mathbb{H}^n \cap S S \mathbb{R}^n \varphi_\alpha \mathbb{H}^n \varphi_\alpha^{-1}(S)=\varphi_\alpha^{-1}((\varphi_\beta\circ\varphi_\alpha^{-1})^{-1}(W))","['general-topology', 'multivariable-calculus', 'smooth-manifolds', 'smooth-functions']"
70,"Find the maximum of $\int_{\gamma} (x^2y-2y)dx + (2x-xy^2)dy$, and find the curve $\gamma$ that gives it.","Find the maximum of , and find the curve  that gives it.",\int_{\gamma} (x^2y-2y)dx + (2x-xy^2)dy \gamma,"The task is to find the maximum of $$\int_{\gamma} (x^2y-2y)dx + (2x-xy^2)dy$$ when $\gamma$ is a smooth, regular, closed and simple curve in $\mathbb{R}^2$ , and $\gamma = \partial G$ for a bounded domain $G$ . Also $\gamma$ is with positive orientation. Moreover, I need to find the curve $\gamma$ that gives the maximum. It's seems obvious to me to use Green's theorem here.  I denote $$\omega  = (x^2y-2y)dx + (2x-xy^2)dy$$ and then: $$\int_{\gamma} \omega = \int_G (2-y^2-x^2+2)dxdy = \int_G (4-(x^2+y^2))dxdy$$ Now, since $x^2+y^2>0$ , the it seems to me that the maximum is $4*vol(G)$ , but I am not entirely sure, since it depends on $G$ . Moreover, I was not able to find a curve $\gamma$ that gives me this result. Help would be appreciated.","The task is to find the maximum of when is a smooth, regular, closed and simple curve in , and for a bounded domain . Also is with positive orientation. Moreover, I need to find the curve that gives the maximum. It's seems obvious to me to use Green's theorem here.  I denote and then: Now, since , the it seems to me that the maximum is , but I am not entirely sure, since it depends on . Moreover, I was not able to find a curve that gives me this result. Help would be appreciated.",\int_{\gamma} (x^2y-2y)dx + (2x-xy^2)dy \gamma \mathbb{R}^2 \gamma = \partial G G \gamma \gamma \omega  = (x^2y-2y)dx + (2x-xy^2)dy \int_{\gamma} \omega = \int_G (2-y^2-x^2+2)dxdy = \int_G (4-(x^2+y^2))dxdy x^2+y^2>0 4*vol(G) G \gamma,"['multivariable-calculus', 'differential-forms', 'curves', 'greens-theorem']"
71,"Evaluating $\int_{0}^1\left(\int_0^{1}fdx\right)dy$, where $f(x,y)=\frac12$ for $x$ rational, and $f(x,y)=y$ for $x$ irrational","Evaluating , where  for  rational, and  for  irrational","\int_{0}^1\left(\int_0^{1}fdx\right)dy f(x,y)=\frac12 x f(x,y)=y x","So, while solving problems on double integral I came across this weird problem: Let $$f(x,y) = 1/2,\ \forall x\in \mathbb Q$$ $$f(x,y) = \ y, \ \forall x\in \mathbb Q^c$$ then find $$\int_{0}^1\left(\int_0^{1}fdx\right)dy$$ I have no idea how to approach this problem, First of all I am not sure whether this integral even exist or not . Can anyone please explain to me how to think in this question ? Thank you.","So, while solving problems on double integral I came across this weird problem: Let then find I have no idea how to approach this problem, First of all I am not sure whether this integral even exist or not . Can anyone please explain to me how to think in this question ? Thank you.","f(x,y) = 1/2,\ \forall x\in \mathbb Q f(x,y) = \ y, \ \forall x\in \mathbb Q^c \int_{0}^1\left(\int_0^{1}fdx\right)dy",['integration']
72,"Proof that $\lim_{(x,y)\to (0,1)} e^{xy} = 1$",Proof that,"\lim_{(x,y)\to (0,1)} e^{xy} = 1","Here is my proof: Let $\epsilon > 0$ be given. There exists $\delta >0$ such that $$|e^{xy}-1| < \epsilon$$ whenever $0<|x|<\delta$ and $0<|y-1|<\delta$ . Using exponential series, we write $$|e^{xy}-1|=|(1+xy+\frac{x^2y^2}{2!}+...)-1|$$ $$=|xy+\frac{x^2y^2}{2!}+\frac{x^3y^3}{3!}+...|$$ $$\le|xy|+|\frac{x^2y^2}{2!}|+|\frac{x^3y^3}{3!}|+...$$ $$\le|xy|+|x^2y^2|+|x^3y^3|+...$$ $$\lt\delta|y|+\delta^2|y|^2+\delta^3|y|^3+...$$ Let $\delta\le\frac12\implies\frac12\lt y\lt\frac32$ $$\therefore|e^{xy}-1|\lt\frac32\delta+(\frac32\delta)^2+(\frac32\delta)^3+...$$ $$= \frac{3\delta}{2-3\delta}$$ So, we choose $$\delta = min\{\frac12,\frac{2\epsilon}{3(\epsilon+1)}\}$$ $$\implies|e^{xy}-1|\lt\epsilon$$ whenever $$0<|x|<\delta $$ and $$0<|y-1|<\delta$$ Q.E.D. I would also like to know if there are any other ways to prove this.","Here is my proof: Let be given. There exists such that whenever and . Using exponential series, we write Let So, we choose whenever and Q.E.D. I would also like to know if there are any other ways to prove this.","\epsilon > 0 \delta >0 |e^{xy}-1| < \epsilon 0<|x|<\delta 0<|y-1|<\delta |e^{xy}-1|=|(1+xy+\frac{x^2y^2}{2!}+...)-1| =|xy+\frac{x^2y^2}{2!}+\frac{x^3y^3}{3!}+...| \le|xy|+|\frac{x^2y^2}{2!}|+|\frac{x^3y^3}{3!}|+... \le|xy|+|x^2y^2|+|x^3y^3|+... \lt\delta|y|+\delta^2|y|^2+\delta^3|y|^3+... \delta\le\frac12\implies\frac12\lt y\lt\frac32 \therefore|e^{xy}-1|\lt\frac32\delta+(\frac32\delta)^2+(\frac32\delta)^3+... = \frac{3\delta}{2-3\delta} \delta = min\{\frac12,\frac{2\epsilon}{3(\epsilon+1)}\} \implies|e^{xy}-1|\lt\epsilon 0<|x|<\delta  0<|y-1|<\delta","['real-analysis', 'multivariable-calculus', 'proof-verification', 'epsilon-delta']"
73,Maximizing the Distance Traveled by a Projectile by Applying Chain Rule to the Fundamental Theorem of Calculus,Maximizing the Distance Traveled by a Projectile by Applying Chain Rule to the Fundamental Theorem of Calculus,,"I'm having some difficulty reconciling my understanding of the fundamental theorem of calculus with a particular problem from Stewart Calculus. The question asks what value of the angle of elevation $\alpha$ maximizes the total distance traveled by the projectile with a position at time $t$ given by the parametric equations $x=(v\cos{\alpha})t$ , $y=(v\sin{\alpha})t-\frac{1}{2}gt^2$ , where $v$ is the initial speed of the projectile and $g$ is the gravitational constant. The projectile is launched at $t=0$ and necessarily returns to Earth at $t=\frac{2v\sin{\alpha}}{g}$ (as can be shown by setting $y=0$ and solving for $t$ ). It follows that the arc length of the parabola traced out by the projectile (as a function of the chosen angle of elevation) is $$L(\alpha)=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{\left(\frac{dx}{dt}\right)^2+\left(\frac{dy}{dt}\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{(v\cos{\alpha})^2+\left(v\sin{\alpha}-gt\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2\cos^2{\alpha}+v^2\sin^2{\alpha}-2vg(\sin{\alpha})t+g^2t^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2-2vg(\sin{\alpha})t+g^2t^2}\,dt}$$ From here, I see two possible ways to determine the $\alpha$ value that maximizes this function. The first solves the integral directly by factoring out $g$ from the square root to produce $$L(\alpha)=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{(v\cos{\alpha})^2+\left(v\sin{\alpha}-gt\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{{g}\sqrt{\left(t-\frac{v}{g}\sin{\alpha}\right)^2+\frac{v^2}{g^2}\cos^2{\alpha}}\,dt}$$ and then uses the identity $$\int{\sqrt{a^2+u^2}\,du}=\frac{u}{2}\sqrt{a^2+u^2}+\frac{a^2}{2}\ln{\left(u+\sqrt{a^2+u^2}\right)}+C$$ to eventually express $L$ as $$L(\alpha)=\frac{v^2}{g}\sin{\alpha}+\frac{v^2}{2g}\cos^2{\alpha}\ln{\left(\frac{1+\sin{\alpha}}{1-\sin{\alpha}}\right)}$$ At this point, $\frac{dL}{d\alpha}=\frac{v^2}{g}\cos{\alpha}\left[2-\sin{\alpha}\ln{\left(\frac{1+\sin{\alpha}}{1-\sin{\alpha}}\right)}\right]$ can be set equal to $0$ to determine the critical points of $L(\alpha)$ on the interval $0<\alpha<\frac{\pi}{2}$ , the only one being at $\alpha\approx56°$ . According to the textbook solutions manual, this is the correct answer. However, I also feel that it should be possible to take advantage of the fundamental theorem of calculus and the chain rule to avoid having to find that nasty integral altogether. Since $$\frac{d}{dx}\int_a^{g(x)}{f(t)\,dt}=\frac{d}{dx}\left(F(g(x))-F(a)\right)=f(g(x))\frac{d}{dx}g(x)$$ shouldn't it be that $$\frac{dL}{d\alpha}=\frac{d}{d\alpha}\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2-2vg(\sin{\alpha})t+g^2t^2}\,dt}=\sqrt{v^2-2vg(\sin{\alpha})\left(\frac{2v\sin{\alpha}}{g}\right)+g^2\left(\frac{2v\sin{\alpha}}{g}\right)^2}\,\frac{d}{d\alpha}\left(\frac{2v\sin{\alpha}}{g}\right)=\sqrt{v^2-4v^2\sin^2{\alpha}+g^2\left(\frac{4v^2\sin^2{\alpha}}{g^2}\right)}\,\left(\frac{2v\cos{\alpha}}{g}\right)=\sqrt{v^2}\,\frac{2v\cos{\alpha}}{g}=\frac{2v^2\cos{\alpha}}{g}$$ for which there is no critical value on the interval $0<\alpha<\frac{\pi}{2}$ . Where am I going wrong?","I'm having some difficulty reconciling my understanding of the fundamental theorem of calculus with a particular problem from Stewart Calculus. The question asks what value of the angle of elevation maximizes the total distance traveled by the projectile with a position at time given by the parametric equations , , where is the initial speed of the projectile and is the gravitational constant. The projectile is launched at and necessarily returns to Earth at (as can be shown by setting and solving for ). It follows that the arc length of the parabola traced out by the projectile (as a function of the chosen angle of elevation) is From here, I see two possible ways to determine the value that maximizes this function. The first solves the integral directly by factoring out from the square root to produce and then uses the identity to eventually express as At this point, can be set equal to to determine the critical points of on the interval , the only one being at . According to the textbook solutions manual, this is the correct answer. However, I also feel that it should be possible to take advantage of the fundamental theorem of calculus and the chain rule to avoid having to find that nasty integral altogether. Since shouldn't it be that for which there is no critical value on the interval . Where am I going wrong?","\alpha t x=(v\cos{\alpha})t y=(v\sin{\alpha})t-\frac{1}{2}gt^2 v g t=0 t=\frac{2v\sin{\alpha}}{g} y=0 t L(\alpha)=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{\left(\frac{dx}{dt}\right)^2+\left(\frac{dy}{dt}\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{(v\cos{\alpha})^2+\left(v\sin{\alpha}-gt\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2\cos^2{\alpha}+v^2\sin^2{\alpha}-2vg(\sin{\alpha})t+g^2t^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2-2vg(\sin{\alpha})t+g^2t^2}\,dt} \alpha g L(\alpha)=\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{(v\cos{\alpha})^2+\left(v\sin{\alpha}-gt\right)^2}\,dt}=\int_0^\frac{2v\sin{\alpha}}{g}{{g}\sqrt{\left(t-\frac{v}{g}\sin{\alpha}\right)^2+\frac{v^2}{g^2}\cos^2{\alpha}}\,dt} \int{\sqrt{a^2+u^2}\,du}=\frac{u}{2}\sqrt{a^2+u^2}+\frac{a^2}{2}\ln{\left(u+\sqrt{a^2+u^2}\right)}+C L L(\alpha)=\frac{v^2}{g}\sin{\alpha}+\frac{v^2}{2g}\cos^2{\alpha}\ln{\left(\frac{1+\sin{\alpha}}{1-\sin{\alpha}}\right)} \frac{dL}{d\alpha}=\frac{v^2}{g}\cos{\alpha}\left[2-\sin{\alpha}\ln{\left(\frac{1+\sin{\alpha}}{1-\sin{\alpha}}\right)}\right] 0 L(\alpha) 0<\alpha<\frac{\pi}{2} \alpha\approx56° \frac{d}{dx}\int_a^{g(x)}{f(t)\,dt}=\frac{d}{dx}\left(F(g(x))-F(a)\right)=f(g(x))\frac{d}{dx}g(x) \frac{dL}{d\alpha}=\frac{d}{d\alpha}\int_0^\frac{2v\sin{\alpha}}{g}{\sqrt{v^2-2vg(\sin{\alpha})t+g^2t^2}\,dt}=\sqrt{v^2-2vg(\sin{\alpha})\left(\frac{2v\sin{\alpha}}{g}\right)+g^2\left(\frac{2v\sin{\alpha}}{g}\right)^2}\,\frac{d}{d\alpha}\left(\frac{2v\sin{\alpha}}{g}\right)=\sqrt{v^2-4v^2\sin^2{\alpha}+g^2\left(\frac{4v^2\sin^2{\alpha}}{g^2}\right)}\,\left(\frac{2v\cos{\alpha}}{g}\right)=\sqrt{v^2}\,\frac{2v\cos{\alpha}}{g}=\frac{2v^2\cos{\alpha}}{g} 0<\alpha<\frac{\pi}{2}","['calculus', 'integration', 'multivariable-calculus', 'derivatives']"
74,Determining the angle between two intercepting curves,Determining the angle between two intercepting curves,,"I'm having trouble solving the following problem: So i have the following two curves $(x,y) = (t^2, t+1)$ ,  and $ 5x^2 +5xy +3y^2 - 8x-6y+3=0$ In the first part of the problem i'm asked to find the intercept point. I did this by inserting $(x,y) = (t^2, t+1)$ in the second curve and solving for t. Long story short, i get that the intercept points are (0.1) and (1.0). However in the second part of the problem im asked to find the angle between the curves at the intercepting points. Im aware that i find the angle between the curves by first determining their respective gradient at the point. However this is where i run into trouble. I know how to determine the gradient for the second curve. But for the first one, since its written on parametic form, i have no clue how to determine the gradient.","I'm having trouble solving the following problem: So i have the following two curves ,  and In the first part of the problem i'm asked to find the intercept point. I did this by inserting in the second curve and solving for t. Long story short, i get that the intercept points are (0.1) and (1.0). However in the second part of the problem im asked to find the angle between the curves at the intercepting points. Im aware that i find the angle between the curves by first determining their respective gradient at the point. However this is where i run into trouble. I know how to determine the gradient for the second curve. But for the first one, since its written on parametic form, i have no clue how to determine the gradient.","(x,y) = (t^2, t+1)  5x^2 +5xy +3y^2 - 8x-6y+3=0 (x,y) = (t^2, t+1)","['geometry', 'multivariable-calculus']"
75,Direction of gradient,Direction of gradient,,"I'm reading Binmore and Davies, Calculus Concepts and Methods . On page 105, there's a function $$\tau(x,y) = 32 - 3\ln(1 + x^2 + 4y^2).$$ The gradient is calculated as $$\left(\frac{-3 \cdot 2x}{1 + x^2 + 4y^2}, \frac{-3\cdot 8y}{1 + x^2 + 4y^2} \right)^T$$ The text then states that the gradient vector has direction $(-x, -4y)^T$ . Where did this $(-x, -4y)$ come from? The text evaluates the gradient at the point $(x, y)^T = (10, 10)^T$ which results in $(\frac{-60}{501}, \frac{-240}{501})$ . The text refers to the direction of this as $(-1, -4)^T$ . I thought that the direction might just be the unit vector of the gradient but the length of $(-1, -4)^T$ is not equal to 1. The other questions on here related to the direction of the gradient have to do with maximum/minimum steepness or the direction being reported as a unit vector. Appreciate any help.","I'm reading Binmore and Davies, Calculus Concepts and Methods . On page 105, there's a function The gradient is calculated as The text then states that the gradient vector has direction . Where did this come from? The text evaluates the gradient at the point which results in . The text refers to the direction of this as . I thought that the direction might just be the unit vector of the gradient but the length of is not equal to 1. The other questions on here related to the direction of the gradient have to do with maximum/minimum steepness or the direction being reported as a unit vector. Appreciate any help.","\tau(x,y) = 32 - 3\ln(1 + x^2 + 4y^2). \left(\frac{-3 \cdot 2x}{1 + x^2 + 4y^2}, \frac{-3\cdot 8y}{1 + x^2 + 4y^2} \right)^T (-x, -4y)^T (-x, -4y) (x, y)^T = (10, 10)^T (\frac{-60}{501}, \frac{-240}{501}) (-1, -4)^T (-1, -4)^T","['multivariable-calculus', 'derivatives', 'vector-analysis']"
76,General formula for volume integral of scalar field?,General formula for volume integral of scalar field?,,"I am looking to derive general formulas for the electric fields generated by general charged objects given their charge densities. For linear and surface charge densities I have been able to derive the following expressions for a linear charge density $\lambda(\vec{r})$ and surface charge density $\eta(\vec{r})$ , respectively: $$\vec{E}=\frac{1}{4\pi\varepsilon_0}\int_C\frac{\lambda(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,ds$$ $$\vec{E}=\frac{1}{4\pi\varepsilon_0}\iint_S\frac{\eta(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,dS$$ I am able to compute these given that I am able to find a parameterization of the curve $C$ or the surface $S$ , using the following formulas: $$\int_C f\,ds=\int_a^b f\big(\vec{r}(t)\big)\Vert\vec{r}'(t)\Vert\,dt$$ $$\iint_S f\,dS=\int_c^d\int_a^b f\big(\vec{r}(u,v)\big)\Vert\partial_u\vec{r}\times\partial_v\vec{r}\Vert\,du\,dv$$ However, I am uncertain how to do this in the case with a volume charge density $\rho(\vec r)$ . I do not know a formula to simplify the volume integral. My guess would be maybe something like: $$\iiint_Rf\,dV=\int_\alpha^\beta\int_\gamma^\delta\int_\epsilon^\zeta f\big(\vec r(u,v,w)\big)\Vert\partial_u\vec r\times\partial_v\vec{r}\times\partial_w\vec{r}\Vert\,du\,dv\,dw$$ But I do not know a method to derive this. Is there a method/formula for computing these types of integrals given a parameterization of the region $V\subset\mathbf{R}^3$ ?","I am looking to derive general formulas for the electric fields generated by general charged objects given their charge densities. For linear and surface charge densities I have been able to derive the following expressions for a linear charge density and surface charge density , respectively: I am able to compute these given that I am able to find a parameterization of the curve or the surface , using the following formulas: However, I am uncertain how to do this in the case with a volume charge density . I do not know a formula to simplify the volume integral. My guess would be maybe something like: But I do not know a method to derive this. Is there a method/formula for computing these types of integrals given a parameterization of the region ?","\lambda(\vec{r}) \eta(\vec{r}) \vec{E}=\frac{1}{4\pi\varepsilon_0}\int_C\frac{\lambda(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,ds \vec{E}=\frac{1}{4\pi\varepsilon_0}\iint_S\frac{\eta(\vec{r})}{\Vert\mathbf x_2-\mathbf x_1\Vert^3}(\mathbf x_2-\mathbf x_1)\,dS C S \int_C f\,ds=\int_a^b f\big(\vec{r}(t)\big)\Vert\vec{r}'(t)\Vert\,dt \iint_S f\,dS=\int_c^d\int_a^b f\big(\vec{r}(u,v)\big)\Vert\partial_u\vec{r}\times\partial_v\vec{r}\Vert\,du\,dv \rho(\vec r) \iiint_Rf\,dV=\int_\alpha^\beta\int_\gamma^\delta\int_\epsilon^\zeta f\big(\vec r(u,v,w)\big)\Vert\partial_u\vec r\times\partial_v\vec{r}\times\partial_w\vec{r}\Vert\,du\,dv\,dw V\subset\mathbf{R}^3","['calculus', 'integration', 'multivariable-calculus', 'physics', 'volume']"
77,Is the integral from $x$ to $y$ of a continuous function differentiable?,Is the integral from  to  of a continuous function differentiable?,x y,"Let $f(t)$ be integrable and continuous function on $[a,b]$ . Let ${ F(x,y) = \int_{x}^{y}{f(t)\,dt} }$ . Show that $F(x,y$ ) is differentiable on the rectangle $[a,b] \times [a,b]$ . I tried to prove that ${ \frac{{ f(x_0+ \triangle x, y_0+ \triangle y)  - f(x_0, y_0) - \frac{\partial f} {\partial x}  (x_0, y_0)\triangle x - \frac{\partial f} {\partial y}  (x_0, y_0)\triangle y}}{ \sqrt{(\triangle x)^2 + (\triangle x)^2}} \to 0}$ as $(\triangle x, \triangle y) \to (0,0)$ by the differential definition. But how do I found the partial derivatives? and even if I'm able to find them, how do I estimate the whole expression?","Let be integrable and continuous function on . Let . Show that ) is differentiable on the rectangle . I tried to prove that as by the differential definition. But how do I found the partial derivatives? and even if I'm able to find them, how do I estimate the whole expression?","f(t) [a,b] { F(x,y) = \int_{x}^{y}{f(t)\,dt} } F(x,y [a,b] \times [a,b] { \frac{{ f(x_0+ \triangle x, y_0+ \triangle y)  - f(x_0, y_0) - \frac{\partial f} {\partial x}  (x_0, y_0)\triangle x - \frac{\partial f} {\partial y}  (x_0, y_0)\triangle y}}{ \sqrt{(\triangle x)^2 + (\triangle x)^2}} \to 0} (\triangle x, \triangle y) \to (0,0)","['multivariable-calculus', 'derivatives', 'definite-integrals']"
78,"Limit as $(x,y)$ approaches $(0,0)$ of $(1+x^2+y^2)^{\frac{1}{x^2+y^2+xy^2}}$",Limit as  approaches  of,"(x,y) (0,0) (1+x^2+y^2)^{\frac{1}{x^2+y^2+xy^2}}","I have the function $$f(x,y)=(1+x^2+y^2)^{\frac{1}{x^2+y^2+xy^2}}$$ and I want to evaluate the limit as $(x,y)$ approaches zero. I have started thinking of a solution but get stuck. Taking the direct limit is not possible since that would make an undefined function. Approaching $(0,0)$ from different lines e.g $y=x$ and $y=0$ both gives hints that the limit could be $e$ but that does not really show anything.  I tried switching to polar coordinates which gives me $$(1+r^2)^{\frac{1}{r^2+r^3\cos(t)\sin^2(t)}}$$ Was it a good idea to switch to polar coordinates, can it be solved continuing with this approach, or could the function $f$ maybe be simplified and solved in a different way?","I have the function and I want to evaluate the limit as approaches zero. I have started thinking of a solution but get stuck. Taking the direct limit is not possible since that would make an undefined function. Approaching from different lines e.g and both gives hints that the limit could be but that does not really show anything.  I tried switching to polar coordinates which gives me Was it a good idea to switch to polar coordinates, can it be solved continuing with this approach, or could the function maybe be simplified and solved in a different way?","f(x,y)=(1+x^2+y^2)^{\frac{1}{x^2+y^2+xy^2}} (x,y) (0,0) y=x y=0 e (1+r^2)^{\frac{1}{r^2+r^3\cos(t)\sin^2(t)}} f","['limits', 'multivariable-calculus']"
79,"Prove that for x in [0,1] the inequality..","Prove that for x in [0,1] the inequality..",,"Prove that for $x_i\in [0, 1],\,i=1,\dots,n$ , the following inequality holds: $$n+x_1x_2...x_n \geq 1+x_1+x_2+...+x_n$$ I have tried Bernoulli's inequality which says $(1+x_1)(1+x_2)...(1+x_n)\geq 1+x_1+x_2+...+x_n$ for $x_i>-1$ and $x_i$ with the same sign.","Prove that for , the following inequality holds: I have tried Bernoulli's inequality which says for and with the same sign.","x_i\in [0, 1],\,i=1,\dots,n n+x_1x_2...x_n \geq 1+x_1+x_2+...+x_n (1+x_1)(1+x_2)...(1+x_n)\geq 1+x_1+x_2+...+x_n x_i>-1 x_i","['multivariable-calculus', 'convexity-inequality']"
80,Is divergence free property preserved here?,Is divergence free property preserved here?,,"Suppose I have a PDE $$F(x)\cdot \nabla f(x) = 0 $$ with $f:\mathbb{R}^n\mapsto \mathbb{R}$ and $F:\mathbb{R}^n\mapsto \mathbb{R}^n$ and $F\in C^\infty$ and $f$ is unknown and has appropriate boundary/initial conditions. Further suppose that $$\nabla\cdot F \equiv 0$$ Consider a variable change $$\tilde{f} := f\circ \eta^{-1}, \quad f = \tilde{f}\circ \eta $$ such that $$|D\eta|(x) = 1, \ \forall x\in\mathbb{R}^n$$ and furthermore $$(D\eta)^{-1} = D\eta^{-1} =  (D\eta)^T$$ So that $\eta$ is some smooth, invertible, orthogonal transformation whose Jacobian $D\eta$ has determinant equal to 1. Then it follows $$F(x) \cdot (D\eta)^T (\nabla \tilde{f} \circ\eta) = 0$$ or equivalently $$\bigg[[(D\eta)F]\circ \eta^{-1}(x)\bigg] \cdot \nabla \tilde{f} = 0$$ Question: Is it necessarily true that the divergence free property $$\nabla \cdot \bigg[[(D\eta)F]\circ \eta^{-1}\bigg] \equiv 0$$ is preserved? If not, what conditions can we impose on $\eta$ to make this hold? This post might help, but I am still struggling to prove it Divergence of matrix-vector product","Suppose I have a PDE with and and and is unknown and has appropriate boundary/initial conditions. Further suppose that Consider a variable change such that and furthermore So that is some smooth, invertible, orthogonal transformation whose Jacobian has determinant equal to 1. Then it follows or equivalently Question: Is it necessarily true that the divergence free property is preserved? If not, what conditions can we impose on to make this hold? This post might help, but I am still struggling to prove it Divergence of matrix-vector product","F(x)\cdot \nabla f(x) = 0  f:\mathbb{R}^n\mapsto \mathbb{R} F:\mathbb{R}^n\mapsto \mathbb{R}^n F\in C^\infty f \nabla\cdot F \equiv 0 \tilde{f} := f\circ \eta^{-1}, \quad f = \tilde{f}\circ \eta  |D\eta|(x) = 1, \ \forall x\in\mathbb{R}^n (D\eta)^{-1} = D\eta^{-1} =  (D\eta)^T \eta D\eta F(x) \cdot (D\eta)^T (\nabla \tilde{f} \circ\eta) = 0 \bigg[[(D\eta)F]\circ \eta^{-1}(x)\bigg] \cdot \nabla \tilde{f} = 0 \nabla \cdot \bigg[[(D\eta)F]\circ \eta^{-1}\bigg] \equiv 0 \eta","['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'divergence-operator', 'jacobian']"
81,Differential Forms without Leibniz notation,Differential Forms without Leibniz notation,,"I have been learning differential forms recently and I was wondering if there is a replacement for the usual notation involving differentials. What I mean is essentially a replacement of the ""differentials"" for example instead of the usual 1 form: $df=\sum\frac{\partial f}{\partial x_n}dx_n$ Something along the lines of: $?f=\sum f_{x_n}?x_n$ Where ?'s represent some new notation. My reasoning for wanting a different notation is 2 fold: I find Leibniz notation very vague and misleading in general and honestly much less intuitive than other derivative notations. My biggest issue with Leibniz notation is separation of variables because students learn about it through Leibniz notation instead of the chain rule. Ignoring any issues with Leibniz notation itself I don't really see a direct connection between these differential forms since they really just represent basis co-vectors. Here is an example of why this is bad notation other than my personal opinion. $$ \int \int f(x) dx\wedge dx = \int \int 0 = 0 $$ Instead of the classical double integral which does not always equal 0: $$ \int \int f(x) dxdy $$ A new student might naively think the above integrals are the same which is a reasonable assumption since $$ \int\int f(x,y)dx\wedge dy = \int \int f(x,y)dxdy $$","I have been learning differential forms recently and I was wondering if there is a replacement for the usual notation involving differentials. What I mean is essentially a replacement of the ""differentials"" for example instead of the usual 1 form: Something along the lines of: Where ?'s represent some new notation. My reasoning for wanting a different notation is 2 fold: I find Leibniz notation very vague and misleading in general and honestly much less intuitive than other derivative notations. My biggest issue with Leibniz notation is separation of variables because students learn about it through Leibniz notation instead of the chain rule. Ignoring any issues with Leibniz notation itself I don't really see a direct connection between these differential forms since they really just represent basis co-vectors. Here is an example of why this is bad notation other than my personal opinion. Instead of the classical double integral which does not always equal 0: A new student might naively think the above integrals are the same which is a reasonable assumption since","df=\sum\frac{\partial f}{\partial x_n}dx_n ?f=\sum f_{x_n}?x_n 
\int \int f(x) dx\wedge dx = \int \int 0 = 0
 
\int \int f(x) dxdy
 
\int\int f(x,y)dx\wedge dy = \int \int f(x,y)dxdy
","['multivariable-calculus', 'differential-geometry', 'differential-forms']"
82,Find the curve of the maximum value of work done?,Find the curve of the maximum value of work done?,,"Suppose $C$ is a simple close curve (i.e. it doesn’t intersect itself) in the first quadrant. If $F = (y^2/2 + x^2y, -x^2 + 8x)$ , find the curve that produces the maximum amount of work done by $F$ . What is the maximum value of work?","Suppose is a simple close curve (i.e. it doesn’t intersect itself) in the first quadrant. If , find the curve that produces the maximum amount of work done by . What is the maximum value of work?","C F = (y^2/2 + x^2y, -x^2 + 8x) F","['calculus', 'integration', 'multivariable-calculus', 'stokes-theorem', 'greens-theorem']"
83,"How to find $\frac{∂f}{∂y}$ for Functions of the form $f(x,y,z(y))$?",How to find  for Functions of the form ?,"\frac{∂f}{∂y} f(x,y,z(y))","This is the first time I'm using math.stackexchange. Please excuse me and correct me if I'm not doing things in the right format. So my question is this: given a function of the form $f(x,y,z(y))$ , and suppose we want to find $\frac{∂}{∂y} f(x,y,z(y))$ . Then by the Chain Rule, we would have something like this $$\frac{∂f}{∂y} = \frac{∂f}{∂y} + \frac{∂f}{∂z}\frac{dz}{dy}.$$ But this notation is really confusing since the two $\frac{∂f}{∂y}$ do not mean the same thing. Am I doing this correctly, or is there any better notation to clarify this expression? I would be much appreciated if someone could shed some light on me.","This is the first time I'm using math.stackexchange. Please excuse me and correct me if I'm not doing things in the right format. So my question is this: given a function of the form , and suppose we want to find . Then by the Chain Rule, we would have something like this But this notation is really confusing since the two do not mean the same thing. Am I doing this correctly, or is there any better notation to clarify this expression? I would be much appreciated if someone could shed some light on me.","f(x,y,z(y)) \frac{∂}{∂y} f(x,y,z(y)) \frac{∂f}{∂y} = \frac{∂f}{∂y} + \frac{∂f}{∂z}\frac{dz}{dy}. \frac{∂f}{∂y}","['calculus', 'multivariable-calculus', 'notation', 'chain-rule']"
84,Convexity of the logarithmic barrier function of SOCP,Convexity of the logarithmic barrier function of SOCP,,"The logarithmic barrier function for second-order cone programming (SOCP) is usually $$F(x) = \log \left( x_n^2 - x_1^2-\cdots - x_{n-1}^2 \right)$$ How to prove its convexity? The Hessian is too complicated to work with, and I couldn't find any convexity-preserving rules that can be applied here either.","The logarithmic barrier function for second-order cone programming (SOCP) is usually How to prove its convexity? The Hessian is too complicated to work with, and I couldn't find any convexity-preserving rules that can be applied here either.",F(x) = \log \left( x_n^2 - x_1^2-\cdots - x_{n-1}^2 \right),"['multivariable-calculus', 'convex-analysis', 'convex-optimization', 'hessian-matrix', 'second-order-cone-programming']"
85,"Finding the global minimum of $\int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx$ having just the local minimum.",Finding the global minimum of  having just the local minimum.,"\int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx","In order to calculate the values of $a$ and $b$ such we get the minimum possible for: $$\int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx$$ I got the help of @TheSimpliFire among others to get the respectively $a$ and $b$ here: Find $a$ and $b$ for which $\int_{0}^{1}( ax+b+\frac{1}{1+x^{2}} )^{2}\,dx$ takes its minimum possible value. Then, as we found were the partial derivatives of $a$ and $b$ are zero it is not hard to prove the founded $(a,b)$ satisfies that for: $$D(a,b) = f_{xx}'(a,b)f_{yy}'(a,b)-f_{xy}(a,b)^2  $$ As for $p=(a,b)$ ; $$D(p) > 0 \land f_{xx}'(p) > 0 \implies \text{minimum}$$ i s satisfied, then $(a,b)$ are locally minimum. My question is how to verify $(a,b)$ is also the global minimum?? Thanks!!!","In order to calculate the values of and such we get the minimum possible for: I got the help of @TheSimpliFire among others to get the respectively and here: Find $a$ and $b$ for which $\int_{0}^{1}( ax+b+\frac{1}{1+x^{2}} )^{2}\,dx$ takes its minimum possible value. Then, as we found were the partial derivatives of and are zero it is not hard to prove the founded satisfies that for: As for ; i s satisfied, then are locally minimum. My question is how to verify is also the global minimum?? Thanks!!!","a b \int_{0}^{1} \left( ax+b+\frac{1}{1+x^{2}} \right)^{2}\,dx a b a b (a,b) D(a,b) = f_{xx}'(a,b)f_{yy}'(a,b)-f_{xy}(a,b)^2 
 p=(a,b) D(p) > 0 \land f_{xx}'(p) > 0 \implies \text{minimum} (a,b) (a,b)","['real-analysis', 'calculus', 'integration', 'multivariable-calculus', 'derivatives']"
86,Transforming an arbitrary quadrilateral to a unit square,Transforming an arbitrary quadrilateral to a unit square,,"In this answer from Pedro Gimeno he proposed the following transformation to map the points of any arbitrary quadrilateral to the unit square $$\pmatrix{x'\\y'} = > \pmatrix{u_x&v_x&w_x\\u_y&v_y&w_y}\pmatrix{x\\y\\xy}$$ $$x'=u_xx+v_xy+w_xxy\\ y'=u_yx+v_yy+w_yxy$$ It transforms the unit square in a way controlled by the vectors $u=(u_x,u_y), v=(v_x,v_y), w=(w_x,w_y)$ as follows: Geometric interpretation of u, v, w (sorry, I can't post images) However, I'd like to do the opposite. I need to integrate on an arbitrary quadrilateral and I'd like to define my integral on the unit square and then map the coordinates of the unit square on the original quadrilateral . The matrix Pedro proposed, however, is not invertible and I'm not sure a pseudo-inverse matrix is what I'm looking for. How can I obtain the opposite transformation?","In this answer from Pedro Gimeno he proposed the following transformation to map the points of any arbitrary quadrilateral to the unit square It transforms the unit square in a way controlled by the vectors as follows: Geometric interpretation of u, v, w (sorry, I can't post images) However, I'd like to do the opposite. I need to integrate on an arbitrary quadrilateral and I'd like to define my integral on the unit square and then map the coordinates of the unit square on the original quadrilateral . The matrix Pedro proposed, however, is not invertible and I'm not sure a pseudo-inverse matrix is what I'm looking for. How can I obtain the opposite transformation?","\pmatrix{x'\\y'} =
> \pmatrix{u_x&v_x&w_x\\u_y&v_y&w_y}\pmatrix{x\\y\\xy} x'=u_xx+v_xy+w_xxy\\ y'=u_yx+v_yy+w_yxy u=(u_x,u_y), v=(v_x,v_y), w=(w_x,w_y)","['multivariable-calculus', 'analytic-geometry']"
87,Fitting points to curve $g(t) = \frac{100}{1+\alpha e^{-\beta t}}$ by thinking about projections and inner products,Fitting points to curve  by thinking about projections and inner products,g(t) = \frac{100}{1+\alpha e^{-\beta t}},"This is a reinterpretation of my old question Fit data to function $g(t) = \frac{100}{1+\alpha e^{-\beta t}}$ by using least squares method (projection/orthogonal families of polynomials) . I need to understand things in terms of orthogonal projections and inner products and the answers were for common regression techniques. t  ---   0 1 2 3 4 5 6 F(t)   10 15 23 33 45 58 69 Adjust $F$ by a function of the type $$g(t) = \frac{100}{1+\alpha  e^{-\beta t}}$$ by the discrete least squares method First of all, we cannot work with the function $g(t)$ as it is. The way I'm trying to see the problem is via projections. So let's try to transform the problem like this: $$\frac{100}{g(t)}-1 = \alpha e^{-\beta t}\implies \ln \left(\frac{100}{g(t)}-1\right) = \ln \alpha -\beta t$$ Since we want to fit the function to the points, we want to minimize the distance of the function from the set of points, that is: $$\min_{\alpha,\beta} \left(\ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t\right)$$ Without using derivative and equating things to $0$ , there's a way to see this problem as an orthogonal projection problem. I know I need to end up with something like this: $$\langle \ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t, 1\rangle = 0\\ \langle \ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t, t\rangle=0$$ And I know this comes from the knowledge that our minimum is related to some projection and this projection lives in a space where the inner product with $span\{1, t\}$ (because of $\ln\alpha,\beta t$ ), gives $0$ . In order to end up with $$\begin{bmatrix}     \langle 1,1\rangle & \langle t,1\rangle  \\     \langle 1,t\rangle & \langle t,t\rangle \\  \end{bmatrix} \begin{bmatrix}    \ln \alpha  \\     -\beta  \\  \end{bmatrix}= \begin{bmatrix}     \langle \ln\left(\frac{100}{g(t)}-1\right) , 1\rangle  \\     \langle \ln\left(\frac{100}{g(t)}-1\right) , t\rangle  \\  \end{bmatrix}$$ Where the inner product is $$\langle f,g\rangle = \sum f_i g_i $$ *why? Can someone tell me what reasoning gets me to the inner products above, if I did everything rigth and how to finish the exercise?","This is a reinterpretation of my old question Fit data to function $g(t) = \frac{100}{1+\alpha e^{-\beta t}}$ by using least squares method (projection/orthogonal families of polynomials) . I need to understand things in terms of orthogonal projections and inner products and the answers were for common regression techniques. t  ---   0 1 2 3 4 5 6 F(t)   10 15 23 33 45 58 69 Adjust by a function of the type by the discrete least squares method First of all, we cannot work with the function as it is. The way I'm trying to see the problem is via projections. So let's try to transform the problem like this: Since we want to fit the function to the points, we want to minimize the distance of the function from the set of points, that is: Without using derivative and equating things to , there's a way to see this problem as an orthogonal projection problem. I know I need to end up with something like this: And I know this comes from the knowledge that our minimum is related to some projection and this projection lives in a space where the inner product with (because of ), gives . In order to end up with Where the inner product is *why? Can someone tell me what reasoning gets me to the inner products above, if I did everything rigth and how to finish the exercise?","F g(t) = \frac{100}{1+\alpha
 e^{-\beta t}} g(t) \frac{100}{g(t)}-1 = \alpha e^{-\beta t}\implies \ln \left(\frac{100}{g(t)}-1\right) = \ln \alpha -\beta t \min_{\alpha,\beta} \left(\ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t\right) 0 \langle \ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t, 1\rangle = 0\\ \langle \ln\left(\frac{100}{g(t)}-1\right)-\ln\alpha + \beta t, t\rangle=0 span\{1, t\} \ln\alpha,\beta t 0 \begin{bmatrix}
    \langle 1,1\rangle & \langle t,1\rangle  \\
    \langle 1,t\rangle & \langle t,t\rangle \\ 
\end{bmatrix} \begin{bmatrix}
   \ln \alpha  \\
    -\beta  \\ 
\end{bmatrix}= \begin{bmatrix}
    \langle \ln\left(\frac{100}{g(t)}-1\right) , 1\rangle  \\
    \langle \ln\left(\frac{100}{g(t)}-1\right) , t\rangle  \\ 
\end{bmatrix} \langle f,g\rangle = \sum f_i g_i ","['linear-algebra', 'multivariable-calculus', 'numerical-methods', 'numerical-linear-algebra']"
88,Calculating Volume: how to find Z when it's given inside a range e.g. $x^2+y^2\leq z \leq 4$?,Calculating Volume: how to find Z when it's given inside a range e.g. ?,x^2+y^2\leq z \leq 4,"I'm having trouble with these questions, mainly because I don't know how to find the Z function when it's inside a range. Calculate the volume of: a) $x^2+y^2\le z \le 4$ (answer: $48\pi$ ) b) $x^2+y^2\le4 , x^2+y^2+z^2\le9$ (answer $\frac{(\pi)}{4}\times(e^4-e)$ ) For a) I tried the following: I converted to polar coordinates $x^2+y^2=r^2$ $r^2\le4, 0\le r\le2$ $0\le\theta\le2\pi$ For the Z function (height) I considered it as: $4-r^2$ since $x^2+y^2\le z\le4$ So I got this integral for the volume: $$\int_0^{2\pi}\int_0^2(4-r^2)rdrd\theta = 8\pi$$ which is not even close to $48\pi$ I also tried b) but got it wrong as well.","I'm having trouble with these questions, mainly because I don't know how to find the Z function when it's inside a range. Calculate the volume of: a) (answer: ) b) (answer ) For a) I tried the following: I converted to polar coordinates For the Z function (height) I considered it as: since So I got this integral for the volume: which is not even close to I also tried b) but got it wrong as well.","x^2+y^2\le z \le 4 48\pi x^2+y^2\le4 , x^2+y^2+z^2\le9 \frac{(\pi)}{4}\times(e^4-e) x^2+y^2=r^2 r^2\le4, 0\le r\le2 0\le\theta\le2\pi 4-r^2 x^2+y^2\le z\le4 \int_0^{2\pi}\int_0^2(4-r^2)rdrd\theta = 8\pi 48\pi","['multivariable-calculus', 'definite-integrals']"
89,An additional assumption to the inverse function theorem.,An additional assumption to the inverse function theorem.,,The theorem is given below: And here is the question: Could anyone give me a hint on how to prove the required in the question please?,The theorem is given below: And here is the question: Could anyone give me a hint on how to prove the required in the question please?,,"['real-analysis', 'analysis']"
90,Picking a Lyapunov function that is dependent on $\dot{x}$,Picking a Lyapunov function that is dependent on,\dot{x},"I have a system $\dot{x}=f(x)$ . Is it a good idea if I pick a Lyapunov function V that is dependent on $\dot{x}$ . So that $V=V(x,\dot{x})$ ? One of the conditions for stability is that the origin is stable, that is $x=0$ implies $V=0$ . But this won't be the case if V is dependent on $\dot{x}$ as well. Am I missing something or is it just a bad idea to pick a V dependent on $\dot{x}$ .","I have a system . Is it a good idea if I pick a Lyapunov function V that is dependent on . So that ? One of the conditions for stability is that the origin is stable, that is implies . But this won't be the case if V is dependent on as well. Am I missing something or is it just a bad idea to pick a V dependent on .","\dot{x}=f(x) \dot{x} V=V(x,\dot{x}) x=0 V=0 \dot{x} \dot{x}","['multivariable-calculus', 'dynamical-systems', 'control-theory', 'lyapunov-functions']"
91,Uniqueness of the dimension of a manifold,Uniqueness of the dimension of a manifold,,"Let $M$ be a $k$ -dimensional manifold. I want to prove that $M$ can't be also of dimension $m$ where $m \ne k$ . Meaning, there is no $x \in M$ and $x \in U_x$ a neighborhood of $x$ , such that $M \bigcap U_x$ has a good parametrization from $V_x \subset R^m$ (where $m \ne k$ ). I am not really sure how to prove it. I thought that I could use the fact that for every $x \in M$ there is a neighborhood $W_x$ where $M$ is a graph of a smooth function. Then I'll get that in the same neighborhood, $M$ is graph of two functions, each of different number of variables, which will lead to a contradiction. However, I got stuck. Any help would be appreciated.","Let be a -dimensional manifold. I want to prove that can't be also of dimension where . Meaning, there is no and a neighborhood of , such that has a good parametrization from (where ). I am not really sure how to prove it. I thought that I could use the fact that for every there is a neighborhood where is a graph of a smooth function. Then I'll get that in the same neighborhood, is graph of two functions, each of different number of variables, which will lead to a contradiction. However, I got stuck. Any help would be appreciated.",M k M m m \ne k x \in M x \in U_x x M \bigcap U_x V_x \subset R^m m \ne k x \in M W_x M M,"['calculus', 'multivariable-calculus', 'manifolds']"
92,Solving a PDE ( wave equation ),Solving a PDE ( wave equation ),,"$$u_{tt}- \Delta u= e^{t}, 0<x<2\pi, 0<y<1, t>0$$ $$u_x(0,y,t)=u_x(2\pi,y,t)=0, 0<x<2\pi, t>0$$ $$u_y(x,0,t)=u_y(x,1,t)=0, 0<x<2\pi, t>0$$ $$u(x,y,0)=0, 0<x<2\pi, 0<y<1$$ $$u_t(x,y,0)= 1+\cos(x), 0<x<2\pi, 0<y<1$$ Can anyone please help me? I am learning PDE and I have no idea how to solve this problem. Thanks.",Can anyone please help me? I am learning PDE and I have no idea how to solve this problem. Thanks.,"u_{tt}- \Delta u= e^{t}, 0<x<2\pi, 0<y<1, t>0 u_x(0,y,t)=u_x(2\pi,y,t)=0, 0<x<2\pi, t>0 u_y(x,0,t)=u_y(x,1,t)=0, 0<x<2\pi, t>0 u(x,y,0)=0, 0<x<2\pi, 0<y<1 u_t(x,y,0)= 1+\cos(x), 0<x<2\pi, 0<y<1","['multivariable-calculus', 'partial-differential-equations', 'wave-equation']"
93,What is meant by interval notation in $\mathbb{R}^n$?,What is meant by interval notation in ?,\mathbb{R}^n,"I am reading a version of the mean-value theorem and it goes as follows: What is meant by, 'the interval $[x,x+s]$ '?","I am reading a version of the mean-value theorem and it goes as follows: What is meant by, 'the interval '?","[x,x+s]","['calculus', 'multivariable-calculus']"
94,Continuity in multivariable calculus: How is it derived from single variable counterpart?,Continuity in multivariable calculus: How is it derived from single variable counterpart?,,"This is from Munkres' Analysis on manifolds: Theorem 3.6: (a) Let $X$ be a metric space. Let $f:X\to\Bbb R^n$ have the form $f(x)=(f_1(x),\ldots,f_n(x))$ . Then $f$ is continuous at $x_0$ iff each function $f_i:X\to\Bbb R$ is continuous at $x_0$ . (b) Let $f,g:X\to\Bbb R$ be continuous at $x_0$ , then $f+g, f-g, f\cdot g$ continuous at $x_0$ , and so is $f/g$ for $g(x_0)\ne0$ . (c) The projection function $\pi_i:\Bbb R^n\to\Bbb R$ given by $\pi(\vec x)=x_i$ is continuous. Then author says that: ""These theorems imply that functions formed from the familiar real-valued continuous functions of calculus, using algebraic operations and composites, are continuous in $\mathbb R^n$ . For instance, since one knows that the functions $e^x$ and $\sin x$ ; are continuous in $\mathbb R$ , it follows that such a function as $$f(s,t,u,v)=\frac{\sin (s+t)}{e^{uv}}$$ is continuous in $\Bbb R^4$ ."" I can't understand how continuity of $\frac{\sin (s+t)}{e^{uv}}$ follows from given theorems: in particular, how are $s+t$ and $uv$ continuous follow from theorem? Theorem says $f(x)+g(x)$ continuous, not something like $f(x)+g(y)$ continuous.","This is from Munkres' Analysis on manifolds: Theorem 3.6: (a) Let be a metric space. Let have the form . Then is continuous at iff each function is continuous at . (b) Let be continuous at , then continuous at , and so is for . (c) The projection function given by is continuous. Then author says that: ""These theorems imply that functions formed from the familiar real-valued continuous functions of calculus, using algebraic operations and composites, are continuous in . For instance, since one knows that the functions and ; are continuous in , it follows that such a function as is continuous in ."" I can't understand how continuity of follows from given theorems: in particular, how are and continuous follow from theorem? Theorem says continuous, not something like continuous.","X f:X\to\Bbb R^n f(x)=(f_1(x),\ldots,f_n(x)) f x_0 f_i:X\to\Bbb R x_0 f,g:X\to\Bbb R x_0 f+g, f-g, f\cdot g x_0 f/g g(x_0)\ne0 \pi_i:\Bbb R^n\to\Bbb R \pi(\vec x)=x_i \mathbb R^n e^x \sin x \mathbb R f(s,t,u,v)=\frac{\sin (s+t)}{e^{uv}} \Bbb R^4 \frac{\sin (s+t)}{e^{uv}} s+t uv f(x)+g(x) f(x)+g(y)","['real-analysis', 'multivariable-calculus', 'continuity']"
95,"Why is $\, \int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,$?",Why is ?,"\, \int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,","As far I know: Double Integrals of a function depend only on (i)Region of Integration and (ii)Function, and not on its order of integration . In this case : (i)Region of integration is a square (ABCD with AB=BC=CD=DA= $1$ units) where one of its vertices (A) lies on the origin and the opposite vertex is at C=( $1,1$ ); (ii) Function: $ f(x,y)=\frac{x-y}{(x+y)^3}$ Then, why is $$\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,\,\,\,?$$ i.e., $$\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, =0.5 \,\, \& \,\,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,=-0.5 $$ Have I missed any concepts? Please help...","As far I know: Double Integrals of a function depend only on (i)Region of Integration and (ii)Function, and not on its order of integration . In this case : (i)Region of integration is a square (ABCD with AB=BC=CD=DA= units) where one of its vertices (A) lies on the origin and the opposite vertex is at C=( ); (ii) Function: Then, why is i.e., Have I missed any concepts? Please help...","1 1,1  f(x,y)=\frac{x-y}{(x+y)^3} \int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, \neq \,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,\,\,\,? \int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dy \} \, dx \, =0.5 \,\, \& \,\,\int_0^1 \{ \int_0^1 \frac{x-y}{(x+y)^3} \, dx \} \, dy \,=-0.5 ","['multivariable-calculus', 'definite-integrals']"
96,Laplacian of the Euclidean Norm,Laplacian of the Euclidean Norm,,"Let $r:\mathbb{R}^n \rightarrow \mathbb{R}$ be defined by $r(x)=\|x\|,$ where $\large\|x\|=(x_1^2+\dots+x_n^2)^{\frac{1}{2}}.$ Compute $\nabla^2r,$ the Laplacian of $r.$ I perform the following computations, $$\large\partial_{x_i}r(x)=\frac{1}{2} \cdot \frac{2x_i}{(x_1^2+\dots+x_n^2)^{\frac{1}{2}}}=\frac{x_i}{r(x)},$$ so that we get $$\large\partial_{x_i}\partial_{x_i}r(x)=\frac{r(x)-\frac{x_i^2}{r(x)}}{r(x)^2},$$ and putting these together yields \begin{align*}\large\nabla^2r(x) &=\large\partial_{x_1}^2r(x)+\dots+\partial_{x_n}^2r(x) \\ &=\large\frac{r(x)-x_1^2\cdot r(x)^{-1}}{r(x)^2}+\dots+\frac{r(x)-x_n^2\cdot r(x)^{-1}}{r(x)^2}\\ &=\large\frac{n\cdot r(x)-r(x)^{-1}\cdot r(x)^2}{r(x)^2}\\ &=\frac{n-1}{r(x)}.\\ \end{align*} Is the above computation correct? I checked the calculation several times, but still have a feeling that I made a mistake somewhere","Let be defined by where Compute the Laplacian of I perform the following computations, so that we get and putting these together yields Is the above computation correct? I checked the calculation several times, but still have a feeling that I made a mistake somewhere","r:\mathbb{R}^n \rightarrow \mathbb{R} r(x)=\|x\|, \large\|x\|=(x_1^2+\dots+x_n^2)^{\frac{1}{2}}. \nabla^2r, r. \large\partial_{x_i}r(x)=\frac{1}{2} \cdot \frac{2x_i}{(x_1^2+\dots+x_n^2)^{\frac{1}{2}}}=\frac{x_i}{r(x)}, \large\partial_{x_i}\partial_{x_i}r(x)=\frac{r(x)-\frac{x_i^2}{r(x)}}{r(x)^2}, \begin{align*}\large\nabla^2r(x)
&=\large\partial_{x_1}^2r(x)+\dots+\partial_{x_n}^2r(x) \\
&=\large\frac{r(x)-x_1^2\cdot r(x)^{-1}}{r(x)^2}+\dots+\frac{r(x)-x_n^2\cdot r(x)^{-1}}{r(x)^2}\\
&=\large\frac{n\cdot r(x)-r(x)^{-1}\cdot r(x)^2}{r(x)^2}\\
&=\frac{n-1}{r(x)}.\\
\end{align*}","['calculus', 'multivariable-calculus', 'partial-derivative']"
97,Average area of a rectangle inside the unit square,Average area of a rectangle inside the unit square,,"I recently came across this problem while toying with the problem of the average distance between 2 points in the unit square . These two points also define a rectangle, so I was wondering: What is the expected value of the area of that rectangle? I took this problem to Python and it turns out to be somewhere around one-ninth. However, I have no idea where to start actually calculating the exact value. If someone could share how to get a more exact answer (more efficiently, obviously) via computer science, or how to get the precise answer with calculus, that would help out a bunch.","I recently came across this problem while toying with the problem of the average distance between 2 points in the unit square . These two points also define a rectangle, so I was wondering: What is the expected value of the area of that rectangle? I took this problem to Python and it turns out to be somewhere around one-ninth. However, I have no idea where to start actually calculating the exact value. If someone could share how to get a more exact answer (more efficiently, obviously) via computer science, or how to get the precise answer with calculus, that would help out a bunch.",,"['probability', 'geometry', 'multivariable-calculus', 'analytic-geometry', 'python']"
98,Lipschitz constant of a matrix,Lipschitz constant of a matrix,,"I am studying the Lipschitz continuity and trying to solve the following question: If a function $f(x)= Ax$ is defined for $x \in \mathbb{R}^2$ with $A= \begin{bmatrix}     a       & b \\     c       & d \end{bmatrix}$ , then find a constant L such that \begin{eqnarray*} ||Ax - Ay|| \le L||x - y||, x, y \in \mathbb{R}^2. \end{eqnarray*} I understand how to find the Lipschitz constant in $\mathbb{R}$ , but I have no idea about how to find it in $\mathbb{R}^2$ .","I am studying the Lipschitz continuity and trying to solve the following question: If a function is defined for with , then find a constant L such that I understand how to find the Lipschitz constant in , but I have no idea about how to find it in .","f(x)= Ax x \in \mathbb{R}^2 A= \begin{bmatrix}
    a       & b \\
    c       & d
\end{bmatrix} \begin{eqnarray*}
||Ax - Ay|| \le L||x - y||, x, y \in \mathbb{R}^2.
\end{eqnarray*} \mathbb{R} \mathbb{R}^2","['matrices', 'multivariable-calculus', 'continuity', 'lipschitz-functions']"
99,How does a gradient allow the calculation of the directional derivative?,How does a gradient allow the calculation of the directional derivative?,,"If the gradient only results in a vector telling you the steepest direction to travel, how can the ""slope"" in any direction be calculated? If the gradient is: $\nabla f(x,y,z) = \left[\begin{array}{c}\frac{\partial f}{\partial x}\\\frac{\partial f}{\partial y}\\\frac{\partial f}{\partial z} \end{array}\right]$ How can the directional derivative simply be a dot product between the gradient and the vector? $\nabla_{\vec{v}}\,f(x,y,z) = \nabla f(x,y,z) \cdot \vec{v} = \vec{v}_x\frac{\partial f}{\partial x}+\vec{v}_y\frac{\partial f}{\partial y}+\vec{v}_z\frac{\partial f}{\partial z}$","If the gradient only results in a vector telling you the steepest direction to travel, how can the ""slope"" in any direction be calculated? If the gradient is: How can the directional derivative simply be a dot product between the gradient and the vector?","\nabla f(x,y,z) = \left[\begin{array}{c}\frac{\partial f}{\partial x}\\\frac{\partial f}{\partial y}\\\frac{\partial f}{\partial z}
\end{array}\right] \nabla_{\vec{v}}\,f(x,y,z) = \nabla f(x,y,z) \cdot \vec{v} = \vec{v}_x\frac{\partial f}{\partial x}+\vec{v}_y\frac{\partial f}{\partial y}+\vec{v}_z\frac{\partial f}{\partial z}","['multivariable-calculus', 'vectors', 'partial-derivative', 'vector-analysis']"
