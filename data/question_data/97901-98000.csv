,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$f$ is holomorphic iff $df$ is $\Bbb C$-linear,is holomorphic iff  is -linear,f df \Bbb C,"Let $\Omega\subseteq\Bbb C^n$ open connected, $f:\Omega\to\Bbb C$ differentiable in the real sense. We know that $f$ is holomorphic iff $\partial_{\bar z_j}f=0\;\;\forall j=1,\dots,n$ . We know also that $df=\partial_zfdz+\partial_{\bar z}f d\bar z=:\partial f+\bar{\partial}f$. How can I prove from this that $f$ is holomorphic iff $df$, which is in general $\Bbb R$-linear, is now $\Bbb C$-linear? We know by definition that such an $f$ is differentiable in the complex sense in $w\in\Omega$ iff $\;\exists\; T:\Bbb C^n\to\Bbb C$ $\Bbb C$-linear such that $$ \lim_{z\to w,z\in\Omega}\frac{f(z)-f(w)-T(z-w)}{||z-w||_{\Bbb C^n}}=0 $$ and being $T=df$ we can conclude. But I would like to prove it directly, using that $df=\partial f+\bar{\partial}f$, showing that $df$ is $\Bbb C$-linear iff $\bar{\partial}f=0$. Even because trying in different ways it seems that $df$ be always $\Bbb C$-linear, and this is wrong, so showing what I've asked I'd shed some lights on this. Can somebody give me some hints? Many thanks!","Let $\Omega\subseteq\Bbb C^n$ open connected, $f:\Omega\to\Bbb C$ differentiable in the real sense. We know that $f$ is holomorphic iff $\partial_{\bar z_j}f=0\;\;\forall j=1,\dots,n$ . We know also that $df=\partial_zfdz+\partial_{\bar z}f d\bar z=:\partial f+\bar{\partial}f$. How can I prove from this that $f$ is holomorphic iff $df$, which is in general $\Bbb R$-linear, is now $\Bbb C$-linear? We know by definition that such an $f$ is differentiable in the complex sense in $w\in\Omega$ iff $\;\exists\; T:\Bbb C^n\to\Bbb C$ $\Bbb C$-linear such that $$ \lim_{z\to w,z\in\Omega}\frac{f(z)-f(w)-T(z-w)}{||z-w||_{\Bbb C^n}}=0 $$ and being $T=df$ we can conclude. But I would like to prove it directly, using that $df=\partial f+\bar{\partial}f$, showing that $df$ is $\Bbb C$-linear iff $\bar{\partial}f=0$. Even because trying in different ways it seems that $df$ be always $\Bbb C$-linear, and this is wrong, so showing what I've asked I'd shed some lights on this. Can somebody give me some hints? Many thanks!",,"['complex-analysis', 'several-complex-variables']"
1,Trying to understand a proof in Rudin concerning winding number,Trying to understand a proof in Rudin concerning winding number,,"In the proof of theorem 10.10 in Real and complex analysis Rudin states that if we will differentiate $$\phi(t) = \exp \left\{\int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s\right\}, \textrm{we obtain the equality: } \frac{\phi'(t)}{\phi(t)} = \frac{\gamma'(t)}{\gamma(t)-z} $$ which is true almost everywhere (with exception of set $S$, which is finite: $\gamma$ doesn't have to have the derivative everywhere). I can understand this: $$\ln \phi(t) = \int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s \textrm{ implies } (\ln \phi(t))' = \frac{\phi'(t)}{\phi(t)} = \dots = \frac{\gamma'(t)}{\gamma(t) - z}.$$ I hope that the preceeding reasoning is correct (if not, what to do?). But how to conclude that $\phi / (\gamma -z )$ is a continuous function on $[a, b]$ whose derivative is zero on $[a,b] \setminus S$? Here $\gamma \colon [a,b] \to \mathbb C$ is a closed path. Especially the derivative part is a trouble. Of course, it is true that $$\frac{\phi(t)}{\gamma(t)-z} = \frac{\phi'(t)}{\gamma'(t)}.$$ If RHS is constant, then $\phi'(t) = c \gamma'(t)$. Using the definition I write $$\phi'(t) = \exp \left\{\int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s\right\} \cdot \frac{1}{\gamma(t)-z} \cdot \gamma'(t) = c(t) \cdot \gamma'(t).$$ Looking at this I'm not convinced that $c$ is $t$-independent (constant).","In the proof of theorem 10.10 in Real and complex analysis Rudin states that if we will differentiate $$\phi(t) = \exp \left\{\int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s\right\}, \textrm{we obtain the equality: } \frac{\phi'(t)}{\phi(t)} = \frac{\gamma'(t)}{\gamma(t)-z} $$ which is true almost everywhere (with exception of set $S$, which is finite: $\gamma$ doesn't have to have the derivative everywhere). I can understand this: $$\ln \phi(t) = \int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s \textrm{ implies } (\ln \phi(t))' = \frac{\phi'(t)}{\phi(t)} = \dots = \frac{\gamma'(t)}{\gamma(t) - z}.$$ I hope that the preceeding reasoning is correct (if not, what to do?). But how to conclude that $\phi / (\gamma -z )$ is a continuous function on $[a, b]$ whose derivative is zero on $[a,b] \setminus S$? Here $\gamma \colon [a,b] \to \mathbb C$ is a closed path. Especially the derivative part is a trouble. Of course, it is true that $$\frac{\phi(t)}{\gamma(t)-z} = \frac{\phi'(t)}{\gamma'(t)}.$$ If RHS is constant, then $\phi'(t) = c \gamma'(t)$. Using the definition I write $$\phi'(t) = \exp \left\{\int_a^t \frac{\gamma'(s)}{\gamma(s)-z} \,\textrm{d}s\right\} \cdot \frac{1}{\gamma(t)-z} \cdot \gamma'(t) = c(t) \cdot \gamma'(t).$$ Looking at this I'm not convinced that $c$ is $t$-independent (constant).",,['complex-analysis']
2,How do I use residue theorem to evaluate this improper integral to get a good looking solution?,How do I use residue theorem to evaluate this improper integral to get a good looking solution?,,"The problem is $\int_{0}^{\infty} \frac{\sqrt{x}}{x^2+2x+5}dx$  I replace x with z, and did some algebra, but the solution was rather nasty. it contains exponential and arctan such and such. However, the solution given was $\frac{\pi}{2\sqrt{2}}{\sqrt{\sqrt{5}-1}}$. It was pretty neat and I have no idea how I could convert my solution to that form.","The problem is $\int_{0}^{\infty} \frac{\sqrt{x}}{x^2+2x+5}dx$  I replace x with z, and did some algebra, but the solution was rather nasty. it contains exponential and arctan such and such. However, the solution given was $\frac{\pi}{2\sqrt{2}}{\sqrt{\sqrt{5}-1}}$. It was pretty neat and I have no idea how I could convert my solution to that form.",,"['complex-analysis', 'residue-calculus', 'complex-integration']"
3,Residue at essential singularity,Residue at essential singularity,,"I need a little help with the following problem. I've tried many ways, but i didnt succeed. I think there needs to be a trick or something, some transformation. The task is to find the residue of the function at its singularity e.g. z=-3 \begin{equation} f(z)=\cos\left(\frac{z^2+4z-1}{z+3}\right) \end{equation} I tried to write it as \begin{align} \cos\left(\frac{z^2+4z-1}{z+3}\right)=1-\frac{1}{2!}\left((z+1)-\frac{4}{z+3}\right)^2+\frac{1}{4!}\left((z+1)-\frac{4}{z+3}\right)^4-\frac{1}{6!}\left((z+1)-\frac{4}{z+3}\right)^6+\ldots \end{align} and collect the coefficients at $\frac{1}{z+3}$ using binomial expansion of the brackets, but it seems to be a dead end, because there is to much of them and well hidden. If somebody could give me a hint, that would be great. Thanks.","I need a little help with the following problem. I've tried many ways, but i didnt succeed. I think there needs to be a trick or something, some transformation. The task is to find the residue of the function at its singularity e.g. z=-3 \begin{equation} f(z)=\cos\left(\frac{z^2+4z-1}{z+3}\right) \end{equation} I tried to write it as \begin{align} \cos\left(\frac{z^2+4z-1}{z+3}\right)=1-\frac{1}{2!}\left((z+1)-\frac{4}{z+3}\right)^2+\frac{1}{4!}\left((z+1)-\frac{4}{z+3}\right)^4-\frac{1}{6!}\left((z+1)-\frac{4}{z+3}\right)^6+\ldots \end{align} and collect the coefficients at $\frac{1}{z+3}$ using binomial expansion of the brackets, but it seems to be a dead end, because there is to much of them and well hidden. If somebody could give me a hint, that would be great. Thanks.",,"['complex-analysis', 'residue-calculus']"
4,Nonconstant holomorphic function f in unit disk such that f(0)=1,Nonconstant holomorphic function f in unit disk such that f(0)=1,,"This question is in light of a previous question $f$ be a nonconstant holomorphic in unit disk such that $f(0)=1$. Then it is necessary that There are infinitely many points inside unit disk such that $|f(z)|=1$ $f$ is bounded. There are at most finitely many points inside unit disk such that $|f(z)|=1$ $f$ is rational function. 2 is wrong by Liouville's theorem. The author has used $f(z)=e^z$, (holomorphic and $f(0)=1$) to show that option 4 is wrong. Isn't $|f(z)|=1$ only for $z=0$ in this case? That means there are only finite (more specifically one) point inside unit disc such that $|f(z)|=1$. But someone has proved that 1 is the right option. Where am I going wrong?","This question is in light of a previous question $f$ be a nonconstant holomorphic in unit disk such that $f(0)=1$. Then it is necessary that There are infinitely many points inside unit disk such that $|f(z)|=1$ $f$ is bounded. There are at most finitely many points inside unit disk such that $|f(z)|=1$ $f$ is rational function. 2 is wrong by Liouville's theorem. The author has used $f(z)=e^z$, (holomorphic and $f(0)=1$) to show that option 4 is wrong. Isn't $|f(z)|=1$ only for $z=0$ in this case? That means there are only finite (more specifically one) point inside unit disc such that $|f(z)|=1$. But someone has proved that 1 is the right option. Where am I going wrong?",,['complex-analysis']
5,Finding the roots of $(1 + i)^{\frac{1}{4}}$,Finding the roots of,(1 + i)^{\frac{1}{4}},"The professor says that the $n = 4$ roots of this are in the form: $\cos(\frac{\theta + 2k\pi}{n}) + i\sin(\frac{\theta + 2k\pi}{n})$, where $k = 0, 1, 2, 3$. So to find $\theta$, we find the $r = \sqrt{(1)^2 + (1)^2} = \sqrt{2}$ since $Re(1+i) = 1$ and $Im(1+i) = 1$. So $\sqrt{2}\cos\theta = 1$ and $\sqrt{2}\sin\theta = 1$, so the angle $\theta$ is $\frac{\pi}{4}$. However, if we do $k=0$, then we get that one of the roots is $1$, which is obviously not true since $1^4 \neq 1 + i$. The professor says that the solutions are: $k=1: \cos(\frac{9\pi}{16}) + i\sin(\frac{9\pi}{16})$, $k=2: \cos(\frac{17\pi}{16}) + i\sin(\frac{17\pi}{16})$, and $k=3: \cos(\frac{25\pi}{16}) + i\sin(\frac{25\pi}{16})$. I plugged these into WolfRamAlpha and rose them to the $4$th power, but none of them return the form $1+i$. What is incorrect about these steps?","The professor says that the $n = 4$ roots of this are in the form: $\cos(\frac{\theta + 2k\pi}{n}) + i\sin(\frac{\theta + 2k\pi}{n})$, where $k = 0, 1, 2, 3$. So to find $\theta$, we find the $r = \sqrt{(1)^2 + (1)^2} = \sqrt{2}$ since $Re(1+i) = 1$ and $Im(1+i) = 1$. So $\sqrt{2}\cos\theta = 1$ and $\sqrt{2}\sin\theta = 1$, so the angle $\theta$ is $\frac{\pi}{4}$. However, if we do $k=0$, then we get that one of the roots is $1$, which is obviously not true since $1^4 \neq 1 + i$. The professor says that the solutions are: $k=1: \cos(\frac{9\pi}{16}) + i\sin(\frac{9\pi}{16})$, $k=2: \cos(\frac{17\pi}{16}) + i\sin(\frac{17\pi}{16})$, and $k=3: \cos(\frac{25\pi}{16}) + i\sin(\frac{25\pi}{16})$. I plugged these into WolfRamAlpha and rose them to the $4$th power, but none of them return the form $1+i$. What is incorrect about these steps?",,"['complex-analysis', 'complex-numbers']"
6,Radius of convergence of a power serise involving the Fibonacci sequence.,Radius of convergence of a power serise involving the Fibonacci sequence.,,"Consider the power series $$\sum_{n=0}^{\infty}a_nz^n.$$ where, $a_0=0$ , $a_1=1$ , $a_n=a_{n-1}+a_{n-2}$ . Find the radius of convergence of the power series. MY Attempt : Clearly $\{a_n\}$ is a Fibonacci sequence. Let, $R$ be the radius of convergence of the power series. We have , $$\frac{1}{R}=\lim_n\sup\left|\frac{a_{n+1}}{a_n}\right|$$ $$=\lim_n\sup\left|\frac{a_n+a_{n-1}}{a_n}\right|$$ $$1+\lim_n\sup\left|\frac{a_{n-1}}{a_n}\right|.$$ But I can't write $\lim_n\sup\left|\frac{a_{n-1}}{a_n}\right|$ in terms of $R$ such that we can find out $R$ by solving the equation involving $R$ . Again we know that the $n$ -th term of Fibonacci sequence is $$a_n=\frac{1}{\sqrt 5}\left[\left(\frac{1+\sqrt 5}{2}\right)^n-\left(\frac{1-\sqrt 5}{2}\right)^n\right].$$ From this I find that the radius of convergence of the power series is $\frac{2}{1+\sqrt 5}$ . Is this answer correct ? If NOT what is the correct answer ? But I want to find the radius of convergence NOT using the $n$ -th term of Fibonacci sequence. How I can find it ? Please help... Thanks in Advance.........","Consider the power series where, , , . Find the radius of convergence of the power series. MY Attempt : Clearly is a Fibonacci sequence. Let, be the radius of convergence of the power series. We have , But I can't write in terms of such that we can find out by solving the equation involving . Again we know that the -th term of Fibonacci sequence is From this I find that the radius of convergence of the power series is . Is this answer correct ? If NOT what is the correct answer ? But I want to find the radius of convergence NOT using the -th term of Fibonacci sequence. How I can find it ? Please help... Thanks in Advance.........",\sum_{n=0}^{\infty}a_nz^n. a_0=0 a_1=1 a_n=a_{n-1}+a_{n-2} \{a_n\} R \frac{1}{R}=\lim_n\sup\left|\frac{a_{n+1}}{a_n}\right| =\lim_n\sup\left|\frac{a_n+a_{n-1}}{a_n}\right| 1+\lim_n\sup\left|\frac{a_{n-1}}{a_n}\right|. \lim_n\sup\left|\frac{a_{n-1}}{a_n}\right| R R R n a_n=\frac{1}{\sqrt 5}\left[\left(\frac{1+\sqrt 5}{2}\right)^n-\left(\frac{1-\sqrt 5}{2}\right)^n\right]. \frac{2}{1+\sqrt 5} n,"['sequences-and-series', 'complex-analysis', 'power-series']"
7,Show that the distribution is of the form $C \delta + f$,Show that the distribution is of the form,C \delta + f,"I'm trying to solve this problem: Let $ u = p.v.(1/x)$, $\phi$, $\psi \in C^{\infty}_c$. I want to show that the distribution $(\phi u )* (\psi u)$ is of the form $C \delta + f$ for some constant C and $f \in C^{\infty}$. I realize that $\widehat{(\phi u )* (\psi u)} = \widehat{\phi u} \,\cdot \widehat{\psi u}$, but how do I proceed from here? Thanks in advance.","I'm trying to solve this problem: Let $ u = p.v.(1/x)$, $\phi$, $\psi \in C^{\infty}_c$. I want to show that the distribution $(\phi u )* (\psi u)$ is of the form $C \delta + f$ for some constant C and $f \in C^{\infty}$. I realize that $\widehat{(\phi u )* (\psi u)} = \widehat{\phi u} \,\cdot \widehat{\psi u}$, but how do I proceed from here? Thanks in advance.",,"['complex-analysis', 'partial-differential-equations', 'fourier-analysis', 'distribution-theory']"
8,Find roots of $e^z=-3$ given that z=x+iy,Find roots of  given that z=x+iy,e^z=-3,Can you give me some idea of how to do this?  I'm really stuck.,Can you give me some idea of how to do this?  I'm really stuck.,,"['complex-analysis', 'complex-numbers']"
9,"Prove that $f$ analytic, $f(x) \in \mathbb{R}$ for all $x \in \mathbb{R}$ implies $f(\overline{z})=\overline{f(z)}$","Prove that  analytic,  for all  implies",f f(x) \in \mathbb{R} x \in \mathbb{R} f(\overline{z})=\overline{f(z)},"Let $U\subset \mathbb{C}$ be a nonempty connected open set such that for every $z\in U$, $\overline z\in U$. Let $f$ be analytic on $U$. Suppose $f(x)\in\mathbb R$ for every $x\in U\cap\mathbb R$. Prove that $f(\overline{z})=\overline{f(z)}$ for any $z \in U$. By definition, I know that $f$ analytic on $U$ means that for every $z_0 \in U$, there exists $r>0$ and a sequence of complex numbers $\left(a_n\right)_{n=0}^\infty$ such that $f(z)=\sum_{n=0}^\infty a_n\left(z-z_0\right)^n$ on the disc $D(z_0,r)\subseteq U$. I see that $f$ takes points without imaginary components to other points without imaginary components. But I don't see how this implies a symmetry that $f(\overline{z})=\overline{f(z)}$ for $z \in \mathbb C \setminus \mathbb R$. It seems like a very strong conclusion and I'm not sure how I would prove it. I have also shown in the preceding question that $U\cap\mathbb R$ contains an open interval, however I am not sure if that detail is meant to be helpful to this question.","Let $U\subset \mathbb{C}$ be a nonempty connected open set such that for every $z\in U$, $\overline z\in U$. Let $f$ be analytic on $U$. Suppose $f(x)\in\mathbb R$ for every $x\in U\cap\mathbb R$. Prove that $f(\overline{z})=\overline{f(z)}$ for any $z \in U$. By definition, I know that $f$ analytic on $U$ means that for every $z_0 \in U$, there exists $r>0$ and a sequence of complex numbers $\left(a_n\right)_{n=0}^\infty$ such that $f(z)=\sum_{n=0}^\infty a_n\left(z-z_0\right)^n$ on the disc $D(z_0,r)\subseteq U$. I see that $f$ takes points without imaginary components to other points without imaginary components. But I don't see how this implies a symmetry that $f(\overline{z})=\overline{f(z)}$ for $z \in \mathbb C \setminus \mathbb R$. It seems like a very strong conclusion and I'm not sure how I would prove it. I have also shown in the preceding question that $U\cap\mathbb R$ contains an open interval, however I am not sure if that detail is meant to be helpful to this question.",,"['complex-analysis', 'analyticity']"
10,"Show $\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$ if $|z_1| ,|z_2| < 1$ [duplicate]",Show  if  [duplicate],"\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1 |z_1| ,|z_2| < 1","This question already has answers here : Show that $\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$ when $|\alpha|,|\beta| < 1$ (4 answers) Closed 6 years ago . Show $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ if $|z_1| <1$ and $|z_2| < 1$ Consider: $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right|^2$$ $$={\frac{|z_1-z_2|^2}{|1-z_1 \overline{z_2}|^2}}$$ $$=\frac{(z_1-z_2)(\overline{z_1-z_2})}{{(1-z_1\overline{z_2})(\overline{1-z_1\overline{z_2}})}}$$ $$=\frac{(z_1-z_2)(\overline{z_1}-\overline{z_2})}{{(1-z_1\overline{z_2})(1-\overline{z_1}z_2)}}$$ $$=\frac{z_1\overline{z_2}+z_2\overline{z_2}-z_1\overline{z_2}-\overline{z_1}z_2}{1+z_1\overline{z_2}\overline{z_1}z_2-\overline{z_1}z_2-z_1\overline{z_2}}$$ $$=\frac{|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2})}{1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})}$$ Now, I need to show that: $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ So, $$1+\frac{|z_1|^2}{|z_2|^2} < \frac{1}{|z_2|^2}+|z_1|^2$$ $$\rightarrow 1-|z_1|^2 < \frac{1}{|z_2|^2} - \frac{|z_1|^2}{|z_2|^2}$$ $$\rightarrow 1-|z_1|^2 < \frac{1 -|z_1|^2}{|z_2|^2}$$ This shows $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ So, $$|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}| < 1$$ Is this proof correct? However I am also curious as to the following ""proof"". $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ $$\rightarrow |z_1|^2 - |z_1|^2|z_2|^2 < 1-|z_2|^2$$ $$\rightarrow |z_1|^2(1-|z_2|^2) < 1-|z_2|^2$$ $$\rightarrow |z_1|^2< 1$$ I am curious because since $|z_1|<1$, then this implies $|z_1|^2 < 1$. So, is it sufficient to show that  $|z_1|^2 < 1$ which implies $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ which implies $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ which implies $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ ?","This question already has answers here : Show that $\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$ when $|\alpha|,|\beta| < 1$ (4 answers) Closed 6 years ago . Show $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ if $|z_1| <1$ and $|z_2| < 1$ Consider: $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right|^2$$ $$={\frac{|z_1-z_2|^2}{|1-z_1 \overline{z_2}|^2}}$$ $$=\frac{(z_1-z_2)(\overline{z_1-z_2})}{{(1-z_1\overline{z_2})(\overline{1-z_1\overline{z_2}})}}$$ $$=\frac{(z_1-z_2)(\overline{z_1}-\overline{z_2})}{{(1-z_1\overline{z_2})(1-\overline{z_1}z_2)}}$$ $$=\frac{z_1\overline{z_2}+z_2\overline{z_2}-z_1\overline{z_2}-\overline{z_1}z_2}{1+z_1\overline{z_2}\overline{z_1}z_2-\overline{z_1}z_2-z_1\overline{z_2}}$$ $$=\frac{|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2})}{1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})}$$ Now, I need to show that: $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ So, $$1+\frac{|z_1|^2}{|z_2|^2} < \frac{1}{|z_2|^2}+|z_1|^2$$ $$\rightarrow 1-|z_1|^2 < \frac{1}{|z_2|^2} - \frac{|z_1|^2}{|z_2|^2}$$ $$\rightarrow 1-|z_1|^2 < \frac{1 -|z_1|^2}{|z_2|^2}$$ This shows $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ So, $$|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}| < 1$$ Is this proof correct? However I am also curious as to the following ""proof"". $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ $$\rightarrow |z_1|^2 - |z_1|^2|z_2|^2 < 1-|z_2|^2$$ $$\rightarrow |z_1|^2(1-|z_2|^2) < 1-|z_2|^2$$ $$\rightarrow |z_1|^2< 1$$ I am curious because since $|z_1|<1$, then this implies $|z_1|^2 < 1$. So, is it sufficient to show that  $|z_1|^2 < 1$ which implies $$|z_1|^2+|z_2|^2 < 1+|z_1|^2|z_2|^2$$ which implies $$|z_1|^2+|z_2|^2-2Re(z_1\overline{z_2}) < 1+|z_1|^2|z_2|^2-2Re(z_1\overline{z_2})$$ which implies $$\left|{\frac{z_1-z_2}{1-z_1 \overline{z_2}}}\right| < 1$$ ?",,"['complex-analysis', 'proof-verification', 'complex-numbers', 'complex-geometry']"
11,Show that sequence of composition of holomorphic functions is uniformly convergent,Show that sequence of composition of holomorphic functions is uniformly convergent,,"Let $f:B(0,1)\rightarrow B(0,r)$ where $r\in(0,1)$ be a holomorphic function such that $f(0)=0$. Let $f_1:=f, f_{n+1}:=f \circ f_n, n=1,2,\dots$ Show that sequence $(f_n)$ is uniformly convergent in $B(0,1)$ to a function identically equal to $0$. To be honest, I have no idea how can I even start this. Maybe people smarter than me can throw some input on this topic... Thank you all.","Let $f:B(0,1)\rightarrow B(0,r)$ where $r\in(0,1)$ be a holomorphic function such that $f(0)=0$. Let $f_1:=f, f_{n+1}:=f \circ f_n, n=1,2,\dots$ Show that sequence $(f_n)$ is uniformly convergent in $B(0,1)$ to a function identically equal to $0$. To be honest, I have no idea how can I even start this. Maybe people smarter than me can throw some input on this topic... Thank you all.",,['complex-analysis']
12,Prove the existence of an entire function $f$ such that $f(z)=\sin^2(\sqrt{z})$,Prove the existence of an entire function  such that,f f(z)=\sin^2(\sqrt{z}),"Prove there is an entire function $f$ such that for any branch $g$ of $\sqrt{z}$, $$\sin^2(g(z))=f(z)$$   for all $z$ in the domain of definition of $g$. I don't know how to overcome the fact that $g$ is not defined everywhere on the complex plane. Thanks for any help.","Prove there is an entire function $f$ such that for any branch $g$ of $\sqrt{z}$, $$\sin^2(g(z))=f(z)$$   for all $z$ in the domain of definition of $g$. I don't know how to overcome the fact that $g$ is not defined everywhere on the complex plane. Thanks for any help.",,['complex-analysis']
13,"An entire and one-to-one function must be of the form AZ+B, A non-zero. How to rule out higher degree polynomials in z? [duplicate]","An entire and one-to-one function must be of the form AZ+B, A non-zero. How to rule out higher degree polynomials in z? [duplicate]",,"This question already has answers here : Entire one-to-one functions are linear (8 answers) Closed 9 years ago . Show that if f is entire and one-to-one, then it must be of the form AZ+B, with A not equal to zero. I am editing my question, since there are duplicates on this forum to the question of why an entire and one-to-one function must be of the form AZ+B, with A non-zero. I am currently stuck at f(z)=AZ for A non-zero, from using Liouville's Theorem on g=z/f(z). I'd like to show that f(z) cannot also take the form AZ^2, AZ^3, and so on... I think that is done by using the Fundamental Theorem of Algebra and saying that an nth degree polynomial in z (with non-zero coefficient, A) has exactly n roots.  But I'm thinking about the situation when all of the roots are at one location, so that we have only one distinct root with multiplicity = n.  Then this doesn't rule out the case that f(z) is one-to-one. What can I do to show the polynomial must only be of degree 1?  (I've seen some derivative arguments now, including @JohnHuges ' argument below, where f' is not zero, but I don't understand this argument and why we can conclude from this that f is not one-to-one...) Thanks in advance,","This question already has answers here : Entire one-to-one functions are linear (8 answers) Closed 9 years ago . Show that if f is entire and one-to-one, then it must be of the form AZ+B, with A not equal to zero. I am editing my question, since there are duplicates on this forum to the question of why an entire and one-to-one function must be of the form AZ+B, with A non-zero. I am currently stuck at f(z)=AZ for A non-zero, from using Liouville's Theorem on g=z/f(z). I'd like to show that f(z) cannot also take the form AZ^2, AZ^3, and so on... I think that is done by using the Fundamental Theorem of Algebra and saying that an nth degree polynomial in z (with non-zero coefficient, A) has exactly n roots.  But I'm thinking about the situation when all of the roots are at one location, so that we have only one distinct root with multiplicity = n.  Then this doesn't rule out the case that f(z) is one-to-one. What can I do to show the polynomial must only be of degree 1?  (I've seen some derivative arguments now, including @JohnHuges ' argument below, where f' is not zero, but I don't understand this argument and why we can conclude from this that f is not one-to-one...) Thanks in advance,",,"['complex-analysis', 'analyticity']"
14,Is i holomorphic over the whole complex plane?,Is i holomorphic over the whole complex plane?,,"That is, is i entire? I know that it's derivative with respect to z bar is 0, so I would think that the answer is yes, although I'm not sure.","That is, is i entire? I know that it's derivative with respect to z bar is 0, so I would think that the answer is yes, although I'm not sure.",,"['complex-analysis', 'complex-numbers']"
15,Is it possible to have Logarithm with base 1 or 0?,Is it possible to have Logarithm with base 1 or 0?,,I am wondering is there any definition that allows logarithm to have base 0 or 1 in real or complex fields (considering Euclidean space)?? Out-coming question is if you can define a logarithm with negative base??,I am wondering is there any definition that allows logarithm to have base 0 or 1 in real or complex fields (considering Euclidean space)?? Out-coming question is if you can define a logarithm with negative base??,,"['complex-analysis', 'complex-numbers', 'logarithms']"
16,Laurent series of $f(z)=\frac{1}{z(z-1)}$ given four different conditions,Laurent series of  given four different conditions,f(z)=\frac{1}{z(z-1)},"Expand $f(z)=\frac{1}{z(z-1)}$ in a Laurent series valid for the follwing annular domains. $a)0\lt \vert z \rvert \lt 1 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,b)1\le\lvert z \rvert\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,c)0\le \lvert z-1 \rvert \lt1\,\,\,\,\,\,\,\,\,\,\,\,\,d) 1\le \lvert z-1 \rvert$ Ok, so here are some things I know, There are singularities at $z_0=0$and $z_0=1$ a) is the unit circle, shaded inside.$f(z)=\frac{1}{z(z-1)}=\frac{1}{z}*\frac{1}{z-1}=\frac{-1}{z}*\frac{1}{1-z}=\frac{-1}{z}*\sum_{n=0}^\infty Z^n=\frac{-1}{z}[z^0+z^1+z^2...]=\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,f(z)=\frac{-1}{z}-1-z-z^2-...\,$ which converges for $0\lt \vert z\rvert \lt 1$ b)is the unit circle shaded outside the circle. $f(z)=\frac{1}{z(z-1)}$$=\frac{1}{z}*\frac{1}{z-1}$$=\frac{1}{z}*\frac{1}{z(1-1/z)}=\frac{1}{z^2}*\frac{1}{1-1/z}=\frac{1}{z^2}*\sum_{n=0}^\infty(\frac{1}{z})^n=\frac{1}{z^2}[\frac{1}{z}^0+\frac{1}{z}^1+\frac{1}{z}^2+...]=\frac{1}{z^2}+\frac{1}{z^3}+\frac{1}{z^4}+...$ c)is a circle with r=1, shaded inside. Here, Im not so sure how to manipulate the function and have gotten stuck d)is a cirlce with r=1, shaded outside","Expand $f(z)=\frac{1}{z(z-1)}$ in a Laurent series valid for the follwing annular domains. $a)0\lt \vert z \rvert \lt 1 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,b)1\le\lvert z \rvert\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,c)0\le \lvert z-1 \rvert \lt1\,\,\,\,\,\,\,\,\,\,\,\,\,d) 1\le \lvert z-1 \rvert$ Ok, so here are some things I know, There are singularities at $z_0=0$and $z_0=1$ a) is the unit circle, shaded inside.$f(z)=\frac{1}{z(z-1)}=\frac{1}{z}*\frac{1}{z-1}=\frac{-1}{z}*\frac{1}{1-z}=\frac{-1}{z}*\sum_{n=0}^\infty Z^n=\frac{-1}{z}[z^0+z^1+z^2...]=\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,f(z)=\frac{-1}{z}-1-z-z^2-...\,$ which converges for $0\lt \vert z\rvert \lt 1$ b)is the unit circle shaded outside the circle. $f(z)=\frac{1}{z(z-1)}$$=\frac{1}{z}*\frac{1}{z-1}$$=\frac{1}{z}*\frac{1}{z(1-1/z)}=\frac{1}{z^2}*\frac{1}{1-1/z}=\frac{1}{z^2}*\sum_{n=0}^\infty(\frac{1}{z})^n=\frac{1}{z^2}[\frac{1}{z}^0+\frac{1}{z}^1+\frac{1}{z}^2+...]=\frac{1}{z^2}+\frac{1}{z^3}+\frac{1}{z^4}+...$ c)is a circle with r=1, shaded inside. Here, Im not so sure how to manipulate the function and have gotten stuck d)is a cirlce with r=1, shaded outside",,"['sequences-and-series', 'complex-analysis', 'laurent-series']"
17,Holomorphic function with a unique fixed point,Holomorphic function with a unique fixed point,,"Let $\omega \subset \mathbb C$ be a simple connected set and $\,f:\omega \to A$ is an analytic function where $A \subset \omega$ is compact. Show that $f$ has an unique fixed point. I think we can reduce the problem by helping from Riemann open mapping theorem to unit disk but I can't prove the existence of fixed point.","Let $\omega \subset \mathbb C$ be a simple connected set and $\,f:\omega \to A$ is an analytic function where $A \subset \omega$ is compact. Show that $f$ has an unique fixed point. I think we can reduce the problem by helping from Riemann open mapping theorem to unit disk but I can't prove the existence of fixed point.",,"['real-analysis', 'complex-analysis', 'fixed-point-theorems']"
18,How prove this $|z|>1$ with $1+z+\frac{z^2}{2!}+\cdots+\frac{z^n}{n!}=0$,How prove this  with,|z|>1 1+z+\frac{z^2}{2!}+\cdots+\frac{z^n}{n!}=0,"For give the  postive integer $n$ ,and $z\in C$ such this $$1+z+\dfrac{z^2}{2!}+\cdots+\dfrac{z^n}{n!}=0$$ show that $$|z|> 1$$ maybe we Assmue that exst $z$ such $$|z|\le 1$$ then we have $$z(1+\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!})=-1$$ so $$|z|\cdot\left|1+\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!}\right|=1$$ then we have $$\left|\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!}\right|\ge 1$$ then I can't have idea","For give the  postive integer ,and such this show that maybe we Assmue that exst such then we have so then we have then I can't have idea",n z\in C 1+z+\dfrac{z^2}{2!}+\cdots+\dfrac{z^n}{n!}=0 |z|> 1 z |z|\le 1 z(1+\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!})=-1 |z|\cdot\left|1+\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!}\right|=1 \left|\dfrac{z}{2}+\cdots+\dfrac{z^{n-1}}{n!}\right|\ge 1,"['complex-analysis', 'inequality']"
19,Complex Analysis-Maximum Modulus principle,Complex Analysis-Maximum Modulus principle,,"I have been given the following problem, If $f$ is an entire function with $f(z)=f(z+2)=f(z+i)$ for each $z \in \Bbb C $ show that f is constant. I think it suffices to show that f is constant in the disc centred at 0 with radius 2 since $f(z)=f(z+2n+mi)$ for $m,n \in \Bbb Z $ And since the restriction of the function to that disk is analytic can be straight away say that f is constant in the disk.?","I have been given the following problem, If $f$ is an entire function with $f(z)=f(z+2)=f(z+i)$ for each $z \in \Bbb C $ show that f is constant. I think it suffices to show that f is constant in the disc centred at 0 with radius 2 since $f(z)=f(z+2n+mi)$ for $m,n \in \Bbb Z $ And since the restriction of the function to that disk is analytic can be straight away say that f is constant in the disk.?",,['complex-analysis']
20,"Why is $e^{-f(z)} = 1-z$, when $f(z)=\sum_{n=1}^\infty \frac{z^n}{n}$?","Why is , when ?",e^{-f(z)} = 1-z f(z)=\sum_{n=1}^\infty \frac{z^n}{n},"Why does the following hold?     $$e^{-f(z)} = 1-z$$ where $f(z)$ is defined as:     $$f(z)=\sum_{n=1}^\infty \frac{z^n}{n}$$ Clearly, $f'(z)=\sum_{n=2}^\infty z^n =\frac{1}{1-z}$ by the known sum of geometric series, so the question is similar to:     $$e^{f(z)} = f'(z)$$ Thanks in advance.","Why does the following hold?     $$e^{-f(z)} = 1-z$$ where $f(z)$ is defined as:     $$f(z)=\sum_{n=1}^\infty \frac{z^n}{n}$$ Clearly, $f'(z)=\sum_{n=2}^\infty z^n =\frac{1}{1-z}$ by the known sum of geometric series, so the question is similar to:     $$e^{f(z)} = f'(z)$$ Thanks in advance.",,['complex-analysis']
21,Rudin's Proof on Riesz Representation Theorem,Rudin's Proof on Riesz Representation Theorem,,"The proof is from Rudin's Real and Complex Analysis. I am having a hard time understanding part of the Riesz Representation Theorem The Theorem states : Every open set $E$ satisfies $$\mu(E)=sup\,\{\mu(K):K\subset E,\,K\,\,\text{compact}\}\,\,\,\,\,\,(3)$$ (Note: Here $\mu$ is a positive measure, $\Lambda$ is a positive linear functional and $\mathfrak M_F$ is the class of all $E\subset X$ that satisfies $(3)$ and $\mu(E)<\infty$ .) Hence $\mathfrak M_F$ contains every open set $V$ with $\mu(V)<\infty$ The proof goes as : Let $\alpha$ be a real such that $\alpha<\mu(V)$ . There exists $f\prec V$ with $\alpha<\Lambda f$ . If $W$ is any open set which contains the support $K$ of $f$ , then $f\prec W$ , hence $\Lambda f\leq\mu(W)$ . Thus $\Lambda f\leq \mu(K)$ . This exhibits a compact $K\subset V$ with $\alpha<\mu(K)$ , so that $(3)$ holds for $V$ . Here is what I understand along with what I don't : $\alpha<\mu(V)$ is clear since $\mu(V)$ is a positive measure. I am not sure whether there exists an $f\prec V$ with $\alpha<\Lambda f$ . I think that the reasoning comes from the definition that $$\mu(V)=\sup\{\Lambda f:f\prec V\}$$ where $V$ is any open set in $X$ . Here the $f\prec V$ with $\alpha<\Lambda f$ would be the supremum. However, I am not sure if that is true. The fact that if $W$ is any open set which contains the support $K$ of $f\Rightarrow f\prec W\Rightarrow \Lambda f\leq \mu(W)$ is clear. However, I don't understand why this implies that $\Lambda f\leq\mu(K)$ . Nor do I understand how $K\subset V$ . Lastly, I don't think that I understand why the fact that $K\subset V$ and $\alpha<\mu (K)$ implies that $V$ satisfies $(3)$ . Wouldn't $\Lambda f\leq \mu (W)\Rightarrow\Lambda f\leq \mu (K)$ make sense only if $W\subset K$ ? But that is not the case since $K\subset W$ ... Also, how was the connection between $V$ and $K$ made, i.e. $K\subset V$ ? What is the connection between $W$ and $V$ ? And lastly why does $K\subset V$ and $\alpha<\mu (K)$ implies that $V$ satisfies $(3)$ ? Any advice or hints would be appreciated. Thank you in advance.","The proof is from Rudin's Real and Complex Analysis. I am having a hard time understanding part of the Riesz Representation Theorem The Theorem states : Every open set satisfies (Note: Here is a positive measure, is a positive linear functional and is the class of all that satisfies and .) Hence contains every open set with The proof goes as : Let be a real such that . There exists with . If is any open set which contains the support of , then , hence . Thus . This exhibits a compact with , so that holds for . Here is what I understand along with what I don't : is clear since is a positive measure. I am not sure whether there exists an with . I think that the reasoning comes from the definition that where is any open set in . Here the with would be the supremum. However, I am not sure if that is true. The fact that if is any open set which contains the support of is clear. However, I don't understand why this implies that . Nor do I understand how . Lastly, I don't think that I understand why the fact that and implies that satisfies . Wouldn't make sense only if ? But that is not the case since ... Also, how was the connection between and made, i.e. ? What is the connection between and ? And lastly why does and implies that satisfies ? Any advice or hints would be appreciated. Thank you in advance.","E \mu(E)=sup\,\{\mu(K):K\subset E,\,K\,\,\text{compact}\}\,\,\,\,\,\,(3) \mu \Lambda \mathfrak M_F E\subset X (3) \mu(E)<\infty \mathfrak M_F V \mu(V)<\infty \alpha \alpha<\mu(V) f\prec V \alpha<\Lambda f W K f f\prec W \Lambda f\leq\mu(W) \Lambda f\leq \mu(K) K\subset V \alpha<\mu(K) (3) V \alpha<\mu(V) \mu(V) f\prec V \alpha<\Lambda f \mu(V)=\sup\{\Lambda f:f\prec V\} V X f\prec V \alpha<\Lambda f W K f\Rightarrow f\prec W\Rightarrow \Lambda f\leq \mu(W) \Lambda f\leq\mu(K) K\subset V K\subset V \alpha<\mu (K) V (3) \Lambda f\leq \mu (W)\Rightarrow\Lambda f\leq \mu (K) W\subset K K\subset W V K K\subset V W V K\subset V \alpha<\mu (K) V (3)","['real-analysis', 'complex-analysis', 'analysis', 'lebesgue-measure', 'riesz-representation-theorem']"
22,Changing to polar form for Green's Theorem,Changing to polar form for Green's Theorem,,"In my text given the integral $\int_{\partial{D}} xy\,dx$, and that $$\int_{\partial{D}} xy\,dx=-\int\int_{D}x\,dx\,dy = - \int\int r\cos \theta\,r\,dr\,d\theta$$ I'm not really understanding the change to polar, and how we get this result. I see that $x=r\cos\theta$ but how does $dx \to r\,dr$ and $dy \to d\theta$?","In my text given the integral $\int_{\partial{D}} xy\,dx$, and that $$\int_{\partial{D}} xy\,dx=-\int\int_{D}x\,dx\,dy = - \int\int r\cos \theta\,r\,dr\,d\theta$$ I'm not really understanding the change to polar, and how we get this result. I see that $x=r\cos\theta$ but how does $dx \to r\,dr$ and $dy \to d\theta$?",,"['integration', 'complex-analysis', 'polar-coordinates']"
23,A suitable integration path for $\cos z/(2 + \cos z)$,A suitable integration path for,\cos z/(2 + \cos z),"I was solving the exercises and got stuck when trying to solve this with tools of residual calculus $$ \int_{0}^{2 \pi} \frac{\cos (z)}{2 + \cos (z)} \, dz = \int_{0}^{2 \pi} f(z) \, dz. $$ Isolated singularities are at $z = \pi (2k+1) - i \log(2 \pm \sqrt{3}),\ k \in \mathbb{Z}$. Also I noticed that constructing a rectangular path (let's say a positively oriented square of length $2 \pi$) has a nice property that vertical parts cancel each other out which leaves me with $$ \int_{0}^{2 \pi} f(z) \, dz + \int_{2 \pi}^{0} f(z+2 \pi i) \, dz = 2 \pi i \cdot \operatorname{Res} \left[ f(z), z = \pi- i \log(2 - \sqrt{3}) \right]; \\ \int_{0}^{2 \pi} \frac{\cos (z)}{2 + \cos (z)} \, dz = \frac{-4 \pi}{\sqrt{3}} + \int_{0}^{2 \pi} \frac{\cos (z+2 \pi i)}{2 + \cos (z+2 \pi i)} \, dz. $$ Numerical calculation reveals that the last integral is just $2 \pi$... Am I missing something obvious? Or maybe there is a smarter way to obtain this result?","I was solving the exercises and got stuck when trying to solve this with tools of residual calculus $$ \int_{0}^{2 \pi} \frac{\cos (z)}{2 + \cos (z)} \, dz = \int_{0}^{2 \pi} f(z) \, dz. $$ Isolated singularities are at $z = \pi (2k+1) - i \log(2 \pm \sqrt{3}),\ k \in \mathbb{Z}$. Also I noticed that constructing a rectangular path (let's say a positively oriented square of length $2 \pi$) has a nice property that vertical parts cancel each other out which leaves me with $$ \int_{0}^{2 \pi} f(z) \, dz + \int_{2 \pi}^{0} f(z+2 \pi i) \, dz = 2 \pi i \cdot \operatorname{Res} \left[ f(z), z = \pi- i \log(2 - \sqrt{3}) \right]; \\ \int_{0}^{2 \pi} \frac{\cos (z)}{2 + \cos (z)} \, dz = \frac{-4 \pi}{\sqrt{3}} + \int_{0}^{2 \pi} \frac{\cos (z+2 \pi i)}{2 + \cos (z+2 \pi i)} \, dz. $$ Numerical calculation reveals that the last integral is just $2 \pi$... Am I missing something obvious? Or maybe there is a smarter way to obtain this result?",,"['integration', 'complex-analysis', 'definite-integrals', 'residue-calculus']"
24,Integral of $\int_0^{2\pi} \frac{e^{-it }dt}{e^{it}-z}$,Integral of,\int_0^{2\pi} \frac{e^{-it }dt}{e^{it}-z},"Sorry if this question seems stupid, but I am confused here: Does it follow that $$ I(z)=\int_0^{2\pi} \frac{e^{-it}dt}{e^{it}-z} = 0 $$ For every $z$ with $|z|<1$? I think this is true. I converted this integral to a line integral around the unit circle $S$: $$ I(z)=\int_S \frac{d\xi}{i\xi^2(\xi-z)} $$ I wanted to say the integrand has a primitive, but it seems I need logarithm to express it, which I am not quite sure can be holomorphic on $S$.","Sorry if this question seems stupid, but I am confused here: Does it follow that $$ I(z)=\int_0^{2\pi} \frac{e^{-it}dt}{e^{it}-z} = 0 $$ For every $z$ with $|z|<1$? I think this is true. I converted this integral to a line integral around the unit circle $S$: $$ I(z)=\int_S \frac{d\xi}{i\xi^2(\xi-z)} $$ I wanted to say the integrand has a primitive, but it seems I need logarithm to express it, which I am not quite sure can be holomorphic on $S$.",,"['complex-analysis', 'contour-integration']"
25,Vanishing of Taylor series coefficient [duplicate],Vanishing of Taylor series coefficient [duplicate],,"This question already has an answer here : A question regarding power series expansion of an entire function [duplicate] (1 answer) Closed 8 years ago . I am solving previous year question paper some competitive exam. Give me some hint to solve the following problem. Let $f$ be an entire function. Suppose for each $a \in \mathbb{R} $ there exists at least one coefficient $c_n$ in $f(z) = \sum_{n=0}^{\infty} c_n (z-a)^n$ which is zero. Then, a) $f^{(n)}(0) = 0$ for infinitely many $n \ge 0$ b) $f^{(2n)}(0)=0$ $ \forall n \ge 0$ c) $f^{(2n+1)}(0)=0$  $ \forall n \ge 0$ d) $f^{(n)}(0) = 0$ for all sufficiently large n. Thanks in advance.","This question already has an answer here : A question regarding power series expansion of an entire function [duplicate] (1 answer) Closed 8 years ago . I am solving previous year question paper some competitive exam. Give me some hint to solve the following problem. Let $f$ be an entire function. Suppose for each $a \in \mathbb{R} $ there exists at least one coefficient $c_n$ in $f(z) = \sum_{n=0}^{\infty} c_n (z-a)^n$ which is zero. Then, a) $f^{(n)}(0) = 0$ for infinitely many $n \ge 0$ b) $f^{(2n)}(0)=0$ $ \forall n \ge 0$ c) $f^{(2n+1)}(0)=0$  $ \forall n \ge 0$ d) $f^{(n)}(0) = 0$ for all sufficiently large n. Thanks in advance.",,"['complex-analysis', 'power-series']"
26,"Prove a simplification of $B_{\frac{1}{2}}(n,n+1)$ for all complex n",Prove a simplification of  for all complex n,"B_{\frac{1}{2}}(n,n+1)","$$B_{\frac{1}{2}}(n,n+1)=\frac{2^{-2 n-1} \left(\Gamma \left(n+\frac{1}{2}\right)+\sqrt{\pi } n \Gamma (n)\right)}{n \Gamma \left(n+\frac{1}{2}\right)}$$ for $n>0$ where $B$ is the incomplete beta function and I have proof of that here . I do not know how to prove it for $n\leq 0$. Mathematica 9.0 produced this nice graph which is evidence that the equation in question is true for all complex n.","$$B_{\frac{1}{2}}(n,n+1)=\frac{2^{-2 n-1} \left(\Gamma \left(n+\frac{1}{2}\right)+\sqrt{\pi } n \Gamma (n)\right)}{n \Gamma \left(n+\frac{1}{2}\right)}$$ for $n>0$ where $B$ is the incomplete beta function and I have proof of that here . I do not know how to prove it for $n\leq 0$. Mathematica 9.0 produced this nice graph which is evidence that the equation in question is true for all complex n.",,"['complex-analysis', 'gamma-function']"
27,Find and sketch the line $x=1$ under the mapping $f(z)=1/z$,Find and sketch the line  under the mapping,x=1 f(z)=1/z,"Find and sketch the image of the vertical line $x=1$ under the mapping $f(z)=\frac 1z$ I started by using $u(x,y) + iv(x,y)=f(z)=\frac 1z = \frac 1{x+iy}$ From here I multiplied $f(z)$ by the conjugate to get $\frac x {x^2 + y^2} - \frac {iy} {x^2 + y^2}$ Then I took $u(x,y)=\frac {x} {x^2 + y^2}$ and $v(x,y)=- \frac {iy} {x^2 + y^2}$ Now, plugging in $x=1$ for $U(1,y)$ and $v(1,y)$ to get $u(1,y)=\frac {1} {1 + y^2}$n and $v(1,y)=\frac {-y} {1 + y^2}$ and then Im stuck. Im not sure how to go from here to a graph.","Find and sketch the image of the vertical line $x=1$ under the mapping $f(z)=\frac 1z$ I started by using $u(x,y) + iv(x,y)=f(z)=\frac 1z = \frac 1{x+iy}$ From here I multiplied $f(z)$ by the conjugate to get $\frac x {x^2 + y^2} - \frac {iy} {x^2 + y^2}$ Then I took $u(x,y)=\frac {x} {x^2 + y^2}$ and $v(x,y)=- \frac {iy} {x^2 + y^2}$ Now, plugging in $x=1$ for $U(1,y)$ and $v(1,y)$ to get $u(1,y)=\frac {1} {1 + y^2}$n and $v(1,y)=\frac {-y} {1 + y^2}$ and then Im stuck. Im not sure how to go from here to a graph.",,['complex-analysis']
28,Cauchy product $\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$,Cauchy product,\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k,"I have been told that, if $\{a_n\}_{n\in\mathbb{N}}$, $\{a_{-n}\}_{n\in\mathbb{N}^+}$, $\{b_n\}_{n\in\mathbb{N}}$ and $\{b_{-n}\}_{n\in\mathbb{N}^+}$ are absolutely summable complex sequences, then$$\Bigg(\sum_{n=-\infty}^{\infty}a_n\Bigg)\Bigg(\sum_{n=-\infty}^{\infty}b_n\Bigg)=\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$$where $\sum_{n=-\infty}^{\infty}a_n=\sum_{n=0}^{\infty}a_n+\sum_{n=1}^{\infty}a_{-n}$. I know that if $\{a_n\}_{n\in\mathbb{N}}$ or $\{b_n\}_{n\in\mathbb{N}}$ is absolutely summable, then $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k=\sum_{n=0}^{\infty}\sum_{k=0}^{\infty}a_{n-k}b_k$, i.e. the proposition is true if $\forall n\leq-1\quad a_n=0=b_n$, and have tried to use that to prove the general case, but I get nothing. I have not studied any theory of measure yet. I $+\infty$-ly thank you for any help!!! EDIT : my question had been considered as a duplicate to a question already asked, but that is about the case $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k$, which I do know, as I had written. Though, I cannot generalise that to show that $(\sum_{n=-\infty}^{\infty}a_n)(\sum_{n=-\infty}^{\infty}b_n)$, which I hope I correctly write as  $$(\sum_{k=0}^{\infty}a_k+\sum_{k=1}^{\infty}a_{-k}) (\sum_{k=0}^{\infty}b_k+\sum_{k=1}^{\infty}b_{-k})$$ $$=\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{n-k}b_k +\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{n-k}b_{-k-1}+\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{-n+k-1}b_{k}+\sum_{n=1}^{\infty}\sum_{k=1}^{n} a_{-n+k-1}b_{-k},$$ is equal to $\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$. The people who regarded this question of mine as a duplicate to that were enough to have this question closed for a while, therefore, I suppose that it may well be trivial that the equality $(\sum_{n=-\infty}^{\infty}a_n)(\sum_{n=-\infty}^{\infty}b_n)=\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$ derives from the equality, known by me, $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k$. If that is trivial, I do not realise that fact: could anybody, either from those considering the question identical to that or anybody else, please, show me how to derive the equality with the indices $n$ from $-\infty$ to $+\infty$? I... $+\infty$-ly thank you! ;-)","I have been told that, if $\{a_n\}_{n\in\mathbb{N}}$, $\{a_{-n}\}_{n\in\mathbb{N}^+}$, $\{b_n\}_{n\in\mathbb{N}}$ and $\{b_{-n}\}_{n\in\mathbb{N}^+}$ are absolutely summable complex sequences, then$$\Bigg(\sum_{n=-\infty}^{\infty}a_n\Bigg)\Bigg(\sum_{n=-\infty}^{\infty}b_n\Bigg)=\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$$where $\sum_{n=-\infty}^{\infty}a_n=\sum_{n=0}^{\infty}a_n+\sum_{n=1}^{\infty}a_{-n}$. I know that if $\{a_n\}_{n\in\mathbb{N}}$ or $\{b_n\}_{n\in\mathbb{N}}$ is absolutely summable, then $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k=\sum_{n=0}^{\infty}\sum_{k=0}^{\infty}a_{n-k}b_k$, i.e. the proposition is true if $\forall n\leq-1\quad a_n=0=b_n$, and have tried to use that to prove the general case, but I get nothing. I have not studied any theory of measure yet. I $+\infty$-ly thank you for any help!!! EDIT : my question had been considered as a duplicate to a question already asked, but that is about the case $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k$, which I do know, as I had written. Though, I cannot generalise that to show that $(\sum_{n=-\infty}^{\infty}a_n)(\sum_{n=-\infty}^{\infty}b_n)$, which I hope I correctly write as  $$(\sum_{k=0}^{\infty}a_k+\sum_{k=1}^{\infty}a_{-k}) (\sum_{k=0}^{\infty}b_k+\sum_{k=1}^{\infty}b_{-k})$$ $$=\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{n-k}b_k +\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{n-k}b_{-k-1}+\sum_{n=0}^{\infty}\sum_{k=0}^{n} a_{-n+k-1}b_{k}+\sum_{n=1}^{\infty}\sum_{k=1}^{n} a_{-n+k-1}b_{-k},$$ is equal to $\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$. The people who regarded this question of mine as a duplicate to that were enough to have this question closed for a while, therefore, I suppose that it may well be trivial that the equality $(\sum_{n=-\infty}^{\infty}a_n)(\sum_{n=-\infty}^{\infty}b_n)=\sum_{n=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{n-k}b_k$ derives from the equality, known by me, $(\sum_{n=0}^{\infty}a_n)(\sum_{n=0}^{\infty}b_n)=\sum_{n=0}^{\infty}\sum_{k=0}^{n}a_{n-k}b_k$. If that is trivial, I do not realise that fact: could anybody, either from those considering the question identical to that or anybody else, please, show me how to derive the equality with the indices $n$ from $-\infty$ to $+\infty$? I... $+\infty$-ly thank you! ;-)",,"['real-analysis', 'sequences-and-series', 'complex-analysis']"
29,Is any homeomorphism from Riemann sphere to Riemann sphere Mobius transformation?,Is any homeomorphism from Riemann sphere to Riemann sphere Mobius transformation?,,"Let $\hat{\mathbb{C}}$ be the Riemann sphere. Let $f:\hat{\mathbb{C}}\rightarrow \hat{\mathbb{C}}$ be a homeomorphism. Then, is $f$ a Mobius transformation?","Let $\hat{\mathbb{C}}$ be the Riemann sphere. Let $f:\hat{\mathbb{C}}\rightarrow \hat{\mathbb{C}}$ be a homeomorphism. Then, is $f$ a Mobius transformation?",,['complex-analysis']
30,How is the formal inverse of a power series with constant term developed ( for instance $\cosh^{-1}(x)$)?,How is the formal inverse of a power series with constant term developed ( for instance )?,\cosh^{-1}(x),"In an older question here in MSE I've asked for the term for the ""slicing"" of a power series in partial series and have learned that it is ""multisection"". I' ve been looking at the behaviour of the threefold-multisection of the exponential series $$ \begin{eqnarray}  g_0(x) &=& \sum_{k=0}^\infty {x^{3k} \over (3k)!} \\   g_1(x) &=& \sum_{k=0}^\infty {x^{3k+1} \over (3k+1)!} \\   g_2(x) &=& \sum_{k=0}^\infty {x^{3k+2} \over (3k+2)!} \\  \end{eqnarray} \\  g_0(x)+g_1(x)+g_2(x) = \exp(x) $$ I've just stepped into my older exercises with this and this time I want to work with the inverses of that functions. I know meanwhile how to invert a power series without constant but with linear term and can sometimes invert other powerseries using the recentering around one of its fixpoints. But I don't see how this can be done for $g_0(x)$ and for $g_2(x)$ . A very nice example for the inversion of such a series is that for the inverse of the $\cosh()$ function: $\cosh^{[-1]}(x)$ Its powerseries appears as very nice and smooth and I have no idea how this could have been made.  So my question is mainly a: for the method: how to develop the inverse of such a powerseries   (with constant term, here having the unit as value, or without constant and without linear term as in $g_2(0)$) b: but of course also simply for the solution for $g_0(x)$ and $g_2(x)$ if the methods need more then I can do myself. If I got a view into an article in the internet so far correctly a possible solution might have used the fact that for the cos and sin-function by periodicity $\cos(x) = \sin(\pi/2 + x)$ (at least over the reals) then the inverse for the $\cos()$ taken by the inverse of the powerseries of $\sin(x)$ and then drifted to the conversion of arguments between $\cosh(x)=\cos(i x)$, but I'm not yet sure about this and have to examine the argumentation step-by-step. Anyway, this does not yet help for my problem in question because I've not yet a transfer-function for the arguments of the $g_0(x)$ and the $g_1(x)$-function. If this of some help, there is a representation in terms of the exponential-function itself: $ \displaystyle  \text{ let } a=- \frac12 \text{ and } b= {\sqrt3 \over 2} \text{ such that over the complex } z=a+b \mathcal i \text { and } z^3 = 1 \text{ then } \\ \begin{eqnarray}   \qquad \qquad g_0(x) &=& { 1\over 3} \big( e^x +2e^{ax} \cos(bx)  \big) \\  \qquad \qquad  g_1(x) &=& { 1\over 3} \big( e^x +2e^{ax}\big( a\cos(bx)+b\sin(bx) \big) \big) \\  \qquad \qquad  g_2(x) &=& { 1\over 3} \big( e^x +2e^{ax}\big( a\cos(bx)-b\sin(bx) \big) \big) \\ \end{eqnarray}$ and also we have the circular relations of derivatives: $ \qquad \qquad g_0'(x)=g_2(x) \qquad g_1'(x)=g_0(x) \qquad g_2'(x) = g_1(x) $ . Here is a picture of $g_{0}(x)$  over the reals: The picture shows already that like with the $\cos^{[-1]}(x)$ and $\cosh^{[-1]}(x)$ we'll have very limited ranges for the inversion due to its multivaluedness and singularities in its derivatives.","In an older question here in MSE I've asked for the term for the ""slicing"" of a power series in partial series and have learned that it is ""multisection"". I' ve been looking at the behaviour of the threefold-multisection of the exponential series $$ \begin{eqnarray}  g_0(x) &=& \sum_{k=0}^\infty {x^{3k} \over (3k)!} \\   g_1(x) &=& \sum_{k=0}^\infty {x^{3k+1} \over (3k+1)!} \\   g_2(x) &=& \sum_{k=0}^\infty {x^{3k+2} \over (3k+2)!} \\  \end{eqnarray} \\  g_0(x)+g_1(x)+g_2(x) = \exp(x) $$ I've just stepped into my older exercises with this and this time I want to work with the inverses of that functions. I know meanwhile how to invert a power series without constant but with linear term and can sometimes invert other powerseries using the recentering around one of its fixpoints. But I don't see how this can be done for $g_0(x)$ and for $g_2(x)$ . A very nice example for the inversion of such a series is that for the inverse of the $\cosh()$ function: $\cosh^{[-1]}(x)$ Its powerseries appears as very nice and smooth and I have no idea how this could have been made.  So my question is mainly a: for the method: how to develop the inverse of such a powerseries   (with constant term, here having the unit as value, or without constant and without linear term as in $g_2(0)$) b: but of course also simply for the solution for $g_0(x)$ and $g_2(x)$ if the methods need more then I can do myself. If I got a view into an article in the internet so far correctly a possible solution might have used the fact that for the cos and sin-function by periodicity $\cos(x) = \sin(\pi/2 + x)$ (at least over the reals) then the inverse for the $\cos()$ taken by the inverse of the powerseries of $\sin(x)$ and then drifted to the conversion of arguments between $\cosh(x)=\cos(i x)$, but I'm not yet sure about this and have to examine the argumentation step-by-step. Anyway, this does not yet help for my problem in question because I've not yet a transfer-function for the arguments of the $g_0(x)$ and the $g_1(x)$-function. If this of some help, there is a representation in terms of the exponential-function itself: $ \displaystyle  \text{ let } a=- \frac12 \text{ and } b= {\sqrt3 \over 2} \text{ such that over the complex } z=a+b \mathcal i \text { and } z^3 = 1 \text{ then } \\ \begin{eqnarray}   \qquad \qquad g_0(x) &=& { 1\over 3} \big( e^x +2e^{ax} \cos(bx)  \big) \\  \qquad \qquad  g_1(x) &=& { 1\over 3} \big( e^x +2e^{ax}\big( a\cos(bx)+b\sin(bx) \big) \big) \\  \qquad \qquad  g_2(x) &=& { 1\over 3} \big( e^x +2e^{ax}\big( a\cos(bx)-b\sin(bx) \big) \big) \\ \end{eqnarray}$ and also we have the circular relations of derivatives: $ \qquad \qquad g_0'(x)=g_2(x) \qquad g_1'(x)=g_0(x) \qquad g_2'(x) = g_1(x) $ . Here is a picture of $g_{0}(x)$  over the reals: The picture shows already that like with the $\cos^{[-1]}(x)$ and $\cosh^{[-1]}(x)$ we'll have very limited ranges for the inversion due to its multivaluedness and singularities in its derivatives.",,"['sequences-and-series', 'complex-analysis', 'taylor-expansion']"
31,Prove that if $z_n \rightarrow z$ then $\theta_n \rightarrow \theta$ and $r_n \rightarrow r$.,Prove that if  then  and .,z_n \rightarrow z \theta_n \rightarrow \theta r_n \rightarrow r,"Suppose that $z_n,z \in G = \mathbb{C} - \{z:z\leq 0\}$ and $z_n=r_ne^{i\theta_n}, z = re^{i\theta}$ where $- \pi < \theta_n,\theta< \pi$. Prove that if $z_n \rightarrow z$ then $\theta_n \rightarrow \theta$ and $r_n \rightarrow r$.","Suppose that $z_n,z \in G = \mathbb{C} - \{z:z\leq 0\}$ and $z_n=r_ne^{i\theta_n}, z = re^{i\theta}$ where $- \pi < \theta_n,\theta< \pi$. Prove that if $z_n \rightarrow z$ then $\theta_n \rightarrow \theta$ and $r_n \rightarrow r$.",,"['complex-analysis', 'complex-numbers']"
32,Prove that for any non constant entire function $\infty$ is an asymptotic value,Prove that for any non constant entire function  is an asymptotic value,\infty,"Let $F$ be an entire function. We say that $a \in \mathbb{C} \cup \{\infty\}$ is an asymptotic value for $F$ if there exists a continuous curve going from a finite point to infinity such that $F$ tends to $a$ along that curve. Prove that for any non constant entire function $\infty$ is an asymptotic value. I encountered this problem in an old qualifying exam. My thoughts on this are that if we assume that $\infty$ is not an asymptotic value, then for every continuous curve from a finite  point to infinity $F$ tends to some finite value along that curve. One possibility is that $F$ tends to the same finite value in which case $F$ has a finite limit at $\infty$. Hence on the compact set $\mathbb{C}\cup\{\infty\}$, $F$ is bounded. Then we reach the contradiction that $F$ is bounded by Liouville's theorem. The other possibility is that $F$ does not have a limit at $\infty$ but takes on finite values in the neighborhood of $\infty$. In this case, we are probably getting a contradiction from Picard's Great Theorem. Can someone please comment on this argument and point out possible wrong inferences or missing details?","Let $F$ be an entire function. We say that $a \in \mathbb{C} \cup \{\infty\}$ is an asymptotic value for $F$ if there exists a continuous curve going from a finite point to infinity such that $F$ tends to $a$ along that curve. Prove that for any non constant entire function $\infty$ is an asymptotic value. I encountered this problem in an old qualifying exam. My thoughts on this are that if we assume that $\infty$ is not an asymptotic value, then for every continuous curve from a finite  point to infinity $F$ tends to some finite value along that curve. One possibility is that $F$ tends to the same finite value in which case $F$ has a finite limit at $\infty$. Hence on the compact set $\mathbb{C}\cup\{\infty\}$, $F$ is bounded. Then we reach the contradiction that $F$ is bounded by Liouville's theorem. The other possibility is that $F$ does not have a limit at $\infty$ but takes on finite values in the neighborhood of $\infty$. In this case, we are probably getting a contradiction from Picard's Great Theorem. Can someone please comment on this argument and point out possible wrong inferences or missing details?",,['complex-analysis']
33,Conformal mapping between symmetric region and unit disc,Conformal mapping between symmetric region and unit disc,,"Exercise 3 of VII.4 of Conway's Complex Analysis states Let $G$ be a simply connected region which is not the whole plane, and suppose that $\bar{z}\in G$ whenever $z\in G$. Let $a\in G\cap\mathbb{R}$ and suppose that $f:G\rightarrow D=\{z:|z|<1\}$ is a one-one analytic function with $f(a)=0,\ f'(a)>0$ and $f(G) = D$. Let $G_+=\{z\in G:\text{Im }z>0\}$. Show that $f(G_+)$ must lie entirely above or below the real axis. I've been working on the proof via a suggestion given in another posting : show that $f(z) = \overline{f(\bar{z})}$ by looking at the conformal automorphisms of the unit disc.  I see how the result follows directly if this claim is proven, but I'm having difficulty in showing the claim itself. My work so far: Let $g:D\rightarrow D$ be a conformal automorphism of the unit disc.  Then $g$ must have the following form: $g(z) = e^{i\theta}\dfrac{z-\alpha}{1-\bar{z}\alpha}$. I then look at $g(f(z))$ and demand that it have the same properties as $f$, namely, that $g(f(a)) = 0$ and $g'(f(a))f'(a)>0$.  But then $g(z) = z$, which I then realized was obvious from the beginning, because $f$ is the unique function guaranteed by the Riemann Mapping Theorem.... I've also played around with specific curves in $G$.  Let $C_1$ denote the real axis passing through $G$, and $C_2$ the perpendicular line passing through $a+iy$ and $a-iy$, where $y>0$ is such that $a+iy\in G$.  Clearly, the angle between $C_1$ and $C_2$ is $\dfrac{\pi}{2}$ at $a$.  Shifting $C_1$ by $it$, we maintain this right angle at $a+it$, $0\leq t\leq y$. By the conformality of $f$ (and some unsightly calculations), I can then show that $f'(a+it) = f'(a-it)$, $0\leq t\leq y$.  This is nifty.  It'd be niftier if I could get it for all $a\in G\cap\mathbb{R}$ and then maybe do an integration trick. Any help would be greatly appreciated.  I am teaching myself these things in preparation for a qualifying exam, so nothing is too basic to point out. (Meta comment: I would have asked this question in response to the cited posting, but my reputation isn't high enough yet.  Also, I think the solution to this particular question must be simpler than the generalization provided in the previous posting.)","Exercise 3 of VII.4 of Conway's Complex Analysis states Let $G$ be a simply connected region which is not the whole plane, and suppose that $\bar{z}\in G$ whenever $z\in G$. Let $a\in G\cap\mathbb{R}$ and suppose that $f:G\rightarrow D=\{z:|z|<1\}$ is a one-one analytic function with $f(a)=0,\ f'(a)>0$ and $f(G) = D$. Let $G_+=\{z\in G:\text{Im }z>0\}$. Show that $f(G_+)$ must lie entirely above or below the real axis. I've been working on the proof via a suggestion given in another posting : show that $f(z) = \overline{f(\bar{z})}$ by looking at the conformal automorphisms of the unit disc.  I see how the result follows directly if this claim is proven, but I'm having difficulty in showing the claim itself. My work so far: Let $g:D\rightarrow D$ be a conformal automorphism of the unit disc.  Then $g$ must have the following form: $g(z) = e^{i\theta}\dfrac{z-\alpha}{1-\bar{z}\alpha}$. I then look at $g(f(z))$ and demand that it have the same properties as $f$, namely, that $g(f(a)) = 0$ and $g'(f(a))f'(a)>0$.  But then $g(z) = z$, which I then realized was obvious from the beginning, because $f$ is the unique function guaranteed by the Riemann Mapping Theorem.... I've also played around with specific curves in $G$.  Let $C_1$ denote the real axis passing through $G$, and $C_2$ the perpendicular line passing through $a+iy$ and $a-iy$, where $y>0$ is such that $a+iy\in G$.  Clearly, the angle between $C_1$ and $C_2$ is $\dfrac{\pi}{2}$ at $a$.  Shifting $C_1$ by $it$, we maintain this right angle at $a+it$, $0\leq t\leq y$. By the conformality of $f$ (and some unsightly calculations), I can then show that $f'(a+it) = f'(a-it)$, $0\leq t\leq y$.  This is nifty.  It'd be niftier if I could get it for all $a\in G\cap\mathbb{R}$ and then maybe do an integration trick. Any help would be greatly appreciated.  I am teaching myself these things in preparation for a qualifying exam, so nothing is too basic to point out. (Meta comment: I would have asked this question in response to the cited posting, but my reputation isn't high enough yet.  Also, I think the solution to this particular question must be simpler than the generalization provided in the previous posting.)",,"['complex-analysis', 'conformal-geometry']"
34,Calculating Riemann zeta function of a complex number given the complex contour integral,Calculating Riemann zeta function of a complex number given the complex contour integral,,"Can you please demonstrate how one would calculate the Riemann Zeta function of any complex number, given that the Riemann Zeta function is equal to the following (shown in http://arxiv.org/pdf/1208.3429v1.pdf ): If you utilize a technique in complex analysis (Such as Cauchy's Integral Formula), may you please explain the process step-by-step. Thank you, Best Regards, J.M","Can you please demonstrate how one would calculate the Riemann Zeta function of any complex number, given that the Riemann Zeta function is equal to the following (shown in http://arxiv.org/pdf/1208.3429v1.pdf ): If you utilize a technique in complex analysis (Such as Cauchy's Integral Formula), may you please explain the process step-by-step. Thank you, Best Regards, J.M",,"['integration', 'complex-analysis', 'functions', 'contour-integration', 'riemann-zeta']"
35,Which contour is best for $\int_0^\infty\frac{1}{x^2 + x + 1}dx$,Which contour is best for,\int_0^\infty\frac{1}{x^2 + x + 1}dx,The following is a complex analysis problem.  Does anyone have any idea what contour would be good to use? $$\int_0^\infty\frac{1}{x^2 + x + 1}dx$$ Its roots on the bottom are are $\frac{-1 \pm i\sqrt{3}}{2}$.,The following is a complex analysis problem.  Does anyone have any idea what contour would be good to use? $$\int_0^\infty\frac{1}{x^2 + x + 1}dx$$ Its roots on the bottom are are $\frac{-1 \pm i\sqrt{3}}{2}$.,,['complex-analysis']
36,Radius of Convergence for $f(z) = \dfrac{1}{1+z^2+z^4}$ at $\dfrac{1}{2}$,Radius of Convergence for  at,f(z) = \dfrac{1}{1+z^2+z^4} \dfrac{1}{2},"I am practicing some qualifying problems, and I cannot compute the following: Find the radius of convergence $R$ of the Taylor series of $f(z) = \dfrac{1}{1+z^2+z^4}$ centered at $\dfrac{1}{2}$. I'm thinking I am having just a technical issue by not seeing the trick for finding the Taylor series for this function. I tried rewriting $f(z)$ and got to $f(z) = -\dfrac{4}{3} \dfrac{1}{1-\frac{4}{3}\left(\left(z-\frac{1}{2}\right)^2+z+\frac{1}{4}\right)^2}$ although I couldn't find the Taylor series from here.","I am practicing some qualifying problems, and I cannot compute the following: Find the radius of convergence $R$ of the Taylor series of $f(z) = \dfrac{1}{1+z^2+z^4}$ centered at $\dfrac{1}{2}$. I'm thinking I am having just a technical issue by not seeing the trick for finding the Taylor series for this function. I tried rewriting $f(z)$ and got to $f(z) = -\dfrac{4}{3} \dfrac{1}{1-\frac{4}{3}\left(\left(z-\frac{1}{2}\right)^2+z+\frac{1}{4}\right)^2}$ although I couldn't find the Taylor series from here.",,['complex-analysis']
37,Show that the complex closed line integral $\oint\frac{\mathrm{d}z}{p(z)}$ is $0$ ($p$ is polynomial),Show that the complex closed line integral  is  ( is polynomial),\oint\frac{\mathrm{d}z}{p(z)} 0 p,"Let $p$ be a polynomial of degree $n\geq2$ and has $n$ different roots $z_1,\dots,z_n$. Prove that $\oint\frac{\mathrm{d}z}{p(z)}=0$ where the closed path is large enough so that all roots are in the interior of the path. I need this as a lemma to a question that was asked in complex analysis. I tried to think of something clever to do using the Argument Principle but couldn't find a function whose logarithmic derivative would be proportional to $\frac{1}{p}$. I thought something along the lines of $p \cdot p^{(1)} \cdot p^{(2)} \cdot \dots \cdot p^{(n)}$ would work but had a bunch of things ""left over"" in the enumerator that ruined it. That's where I'm at...","Let $p$ be a polynomial of degree $n\geq2$ and has $n$ different roots $z_1,\dots,z_n$. Prove that $\oint\frac{\mathrm{d}z}{p(z)}=0$ where the closed path is large enough so that all roots are in the interior of the path. I need this as a lemma to a question that was asked in complex analysis. I tried to think of something clever to do using the Argument Principle but couldn't find a function whose logarithmic derivative would be proportional to $\frac{1}{p}$. I thought something along the lines of $p \cdot p^{(1)} \cdot p^{(2)} \cdot \dots \cdot p^{(n)}$ would work but had a bunch of things ""left over"" in the enumerator that ruined it. That's where I'm at...",,"['complex-analysis', 'polynomials', 'contour-integration']"
38,Complex measures vs. Positive Measures,Complex measures vs. Positive Measures,,"In his real and complex analysis, Rudin writes that the right hand side of the expression $\mu(E) = \Sigma \mu(E_i)$ must necessarily converge for any countable partition $\{E_i\}$ of a measurable E, with respect to any complex measure. He points out that this convergence requirement would not apply to a positive measure. (The first paragraphs of chapter 6.) This confuses me - 1) I don't see why the definition implies that $\Sigma \mu(E_i)$ must converge unless $\mu(E)$ is given to be finite,  2) and if $\mu(E)$ is finite, then positive additivity would imply that $\Sigma \mu(E_i)$ converges even if $\mu$ is only a positive measure. Thank you.","In his real and complex analysis, Rudin writes that the right hand side of the expression $\mu(E) = \Sigma \mu(E_i)$ must necessarily converge for any countable partition $\{E_i\}$ of a measurable E, with respect to any complex measure. He points out that this convergence requirement would not apply to a positive measure. (The first paragraphs of chapter 6.) This confuses me - 1) I don't see why the definition implies that $\Sigma \mu(E_i)$ must converge unless $\mu(E)$ is given to be finite,  2) and if $\mu(E)$ is finite, then positive additivity would imply that $\Sigma \mu(E_i)$ converges even if $\mu$ is only a positive measure. Thank you.",,"['real-analysis', 'complex-analysis', 'measure-theory']"
39,convergence to exponential with order 1/n [duplicate],convergence to exponential with order 1/n [duplicate],,This question already has an answer here : Limit involving the exponential of an expression plus a small-o (1 answer) Closed 5 years ago . We know that  limit $\left(1+\dfrac{x}{n}\right)^n$ converges to $e^x$ but how can we prove that limit $\left(1+\dfrac{x}{n}+o(\frac{1}{n})\right)^n$ converges to $e^x$.,This question already has an answer here : Limit involving the exponential of an expression plus a small-o (1 answer) Closed 5 years ago . We know that  limit $\left(1+\dfrac{x}{n}\right)^n$ converges to $e^x$ but how can we prove that limit $\left(1+\dfrac{x}{n}+o(\frac{1}{n})\right)^n$ converges to $e^x$.,,"['real-analysis', 'probability', 'complex-analysis']"
40,Prove that These Families of Level Curves are Orthogonal,Prove that These Families of Level Curves are Orthogonal,,"From p. 79 in Brown's and Churchill's ""Complex Variable and Application"": Let the function $f(z) = u(x, y)+iv(x, y)$ be analytic in a domain $D$, and consider the family of level curves $u(x, y) = c_1$ and $v(x, y) = c_2$.  Prove that these families are orthogonal. Specifically, show that if $z_0 = (x_0, y_0)$ is a point in $D$ which is common to two particular curves $u(x, y) = c_1$ and $v(x, y) = c_2$, and if $f'(z_0) \ne 0$, then the lines tangent to the curves at $(x_0, y_0)$ are perpendicular to each other.  Then, the question gave a clue that: $\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$ and  $\frac{\partial v}{\partial x}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$ (*) At first, I thought that the authors meant $\frac{\partial u}{\partial x} = \frac{\partial u}{\partial x}\frac{dx}{dx}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$, $\frac{\partial v}{\partial x} = \frac{\partial v}{\partial x}\frac{dx}{dx}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$.  However, that cannot be the case, since we were told that $f'(z_0) = u_x(x_0, y_0)+iv_x(x_0, y_0) \ne 0$.  Furthermore, $\frac{\partial u}{\partial x} = 0$ implies that $u(x, y)$ is constant in a direction parallel to the $x$-axis, but that certainly is not the case in general.  So, how do you get the two equalities in (*)? Furthermore, what is the significance in $f'(z_0) \ne 0$?  In the next question, we are asked to show that with $f(z) = z^2$, the level curves $u(x, y) = x^2-y^2=0$ and $v(x,y)= 2xy = 0$ are not orthogonal.  But a straight-forward computation $\nabla u · \nabla v = u_xv_x + u_yv_y = 4xy - 4xy = 0$, a constant zero.  What did I gloss over?","From p. 79 in Brown's and Churchill's ""Complex Variable and Application"": Let the function $f(z) = u(x, y)+iv(x, y)$ be analytic in a domain $D$, and consider the family of level curves $u(x, y) = c_1$ and $v(x, y) = c_2$.  Prove that these families are orthogonal. Specifically, show that if $z_0 = (x_0, y_0)$ is a point in $D$ which is common to two particular curves $u(x, y) = c_1$ and $v(x, y) = c_2$, and if $f'(z_0) \ne 0$, then the lines tangent to the curves at $(x_0, y_0)$ are perpendicular to each other.  Then, the question gave a clue that: $\frac{\partial u}{\partial x}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$ and  $\frac{\partial v}{\partial x}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$ (*) At first, I thought that the authors meant $\frac{\partial u}{\partial x} = \frac{\partial u}{\partial x}\frac{dx}{dx}+\frac{\partial u}{\partial y}\frac{dy}{dx} = 0$, $\frac{\partial v}{\partial x} = \frac{\partial v}{\partial x}\frac{dx}{dx}+\frac{\partial v}{\partial y}\frac{dy}{dx} = 0$.  However, that cannot be the case, since we were told that $f'(z_0) = u_x(x_0, y_0)+iv_x(x_0, y_0) \ne 0$.  Furthermore, $\frac{\partial u}{\partial x} = 0$ implies that $u(x, y)$ is constant in a direction parallel to the $x$-axis, but that certainly is not the case in general.  So, how do you get the two equalities in (*)? Furthermore, what is the significance in $f'(z_0) \ne 0$?  In the next question, we are asked to show that with $f(z) = z^2$, the level curves $u(x, y) = x^2-y^2=0$ and $v(x,y)= 2xy = 0$ are not orthogonal.  But a straight-forward computation $\nabla u · \nabla v = u_xv_x + u_yv_y = 4xy - 4xy = 0$, a constant zero.  What did I gloss over?",,"['complex-analysis', 'multivariable-calculus']"
41,Determining the nature of isolated singularities,Determining the nature of isolated singularities,,"I am working on some exam practice questions (and do have the answers to these questions) but I am having trouble developing proper 'insight' as to how these answers are derived: (a) $$\frac{\sin z}{z^2}, \frac{\cos z-1}{z^3}$$ In both these cases, we have a pole of order 1 at $z=0$, and in evaluating these, we evaluate the limits: $$\lim_{z \to 0} \frac{\sin z}{z}, \frac{\cos z-1}{z^2}$$ When I did these questions myself, my first thought was to set: $$g(0)=\lim_{z \to 0}(z^2f(z))=\lim_{z \to 0} \sin z = 0 $$ which obviously would be incorrect. I'm assuming the solution is actually  to do $$g(0)=\lim_{z \to 0}(zf(z))=\lim_{z \to 0} \frac{\sin z}{z} = 1 $$ So my question is this: How do we know to multiply by $z$ rather than $z^2$? Is this just by intuition and knowing that this would 'fix' the function to make it analytic? (b) For the function $$ z^4\sin \left(\frac{1}{z}\right)$$ we have a singularity at $z=0$, my intuition is that this is an essential singularity because there is no way to 'fix' the function to make it analytic, but the solutions suggest writing it as a Laurent expansion, to which it then simply says that it is clear $z=0$ is an essential singularity. Is this because in the Laurent expansion none of the coefficients are $0$? (c) For the function $$ \frac{1+z}{1-z^4}$$ we have singularities at $z=1,-1,i,-i$, If I evaluate the limits (for example) $(z-1)f(z)$ as $z$ approaches 1, then they all give something non-zero, except $z=-1$, then I get simple poles at $z=1,i,-i$ and an essential pole at $z=-1$. But the solutions says that $z=-1$ is a removable pole? Why is this? I would greatly appreciate some help in order to clear up my confusions. Many thanks!","I am working on some exam practice questions (and do have the answers to these questions) but I am having trouble developing proper 'insight' as to how these answers are derived: (a) $$\frac{\sin z}{z^2}, \frac{\cos z-1}{z^3}$$ In both these cases, we have a pole of order 1 at $z=0$, and in evaluating these, we evaluate the limits: $$\lim_{z \to 0} \frac{\sin z}{z}, \frac{\cos z-1}{z^2}$$ When I did these questions myself, my first thought was to set: $$g(0)=\lim_{z \to 0}(z^2f(z))=\lim_{z \to 0} \sin z = 0 $$ which obviously would be incorrect. I'm assuming the solution is actually  to do $$g(0)=\lim_{z \to 0}(zf(z))=\lim_{z \to 0} \frac{\sin z}{z} = 1 $$ So my question is this: How do we know to multiply by $z$ rather than $z^2$? Is this just by intuition and knowing that this would 'fix' the function to make it analytic? (b) For the function $$ z^4\sin \left(\frac{1}{z}\right)$$ we have a singularity at $z=0$, my intuition is that this is an essential singularity because there is no way to 'fix' the function to make it analytic, but the solutions suggest writing it as a Laurent expansion, to which it then simply says that it is clear $z=0$ is an essential singularity. Is this because in the Laurent expansion none of the coefficients are $0$? (c) For the function $$ \frac{1+z}{1-z^4}$$ we have singularities at $z=1,-1,i,-i$, If I evaluate the limits (for example) $(z-1)f(z)$ as $z$ approaches 1, then they all give something non-zero, except $z=-1$, then I get simple poles at $z=1,i,-i$ and an essential pole at $z=-1$. But the solutions says that $z=-1$ is a removable pole? Why is this? I would greatly appreciate some help in order to clear up my confusions. Many thanks!",,"['complex-analysis', 'singularity']"
42,Infinitely real-differentiable function with $f(0)=0$ but $\int_{\partial B_1(0)}\frac{f(z)}{z}dz\ne0$,Infinitely real-differentiable function with  but,f(0)=0 \int_{\partial B_1(0)}\frac{f(z)}{z}dz\ne0,"I'm searching for a infinitely real-differentiable function $f:\mathbb{C}\to\mathbb{C}$ with $f(0)=0$ but $$(*)\;\;\;\;\;\int_{\partial B_1(0)}\frac{f(z)}{z}dz\ne0$$ where $$B_r(z_0):=\left\{z\in\mathbb{C}:|z-z_0|<r\right\}$$ Cauchy's integral formula yields that $f$ can't be holomorphic. In addition : I want to show that $(*)$ holds, if $f$ is holomorphic and $\text{Re}(f(z))>0$ for all $z\in\partial B_1(0)$.","I'm searching for a infinitely real-differentiable function $f:\mathbb{C}\to\mathbb{C}$ with $f(0)=0$ but $$(*)\;\;\;\;\;\int_{\partial B_1(0)}\frac{f(z)}{z}dz\ne0$$ where $$B_r(z_0):=\left\{z\in\mathbb{C}:|z-z_0|<r\right\}$$ Cauchy's integral formula yields that $f$ can't be holomorphic. In addition : I want to show that $(*)$ holds, if $f$ is holomorphic and $\text{Re}(f(z))>0$ for all $z\in\partial B_1(0)$.",,"['integration', 'complex-analysis', 'analysis', 'contour-integration']"
43,A continuous and holomorphic function on $D^2$ that takes pure imaginary values on $S^1$ is costant,A continuous and holomorphic function on  that takes pure imaginary values on  is costant,D^2 S^1,"Let $D := \{ |z| < 1\}$ and $f : \overline{D} \rightarrow \mathbb{C}$ be a continuous and holomorphic function on $D$ that takes pure imaginary values on $\partial D$. Why $f$ is constant? From this, if you take two functions that are continuous on $\overline{D}$ and holomorphic on $D$ such that their real parts coincide on $\partial D$, why they differ for a constant?","Let $D := \{ |z| < 1\}$ and $f : \overline{D} \rightarrow \mathbb{C}$ be a continuous and holomorphic function on $D$ that takes pure imaginary values on $\partial D$. Why $f$ is constant? From this, if you take two functions that are continuous on $\overline{D}$ and holomorphic on $D$ such that their real parts coincide on $\partial D$, why they differ for a constant?",,"['complex-analysis', 'analysis', 'functions']"
44,Finding the number of zeros of $f(z) = z^n$ if $|f(z)| < 1 $ for all $z$ with $|z|=1$.,Finding the number of zeros of  if  for all  with .,f(z) = z^n |f(z)| < 1  z |z|=1,"Suppose $f: \overline{\mathbb{D}} \to \mathbb{C}$ is continuous, analytic in $\mathbb{D}$ and satisfies $|f(z)|<1$ for $|z|=1$. Find the number of solutions to the equation $f(z) = z^n$ where $n$ is a positive integer. ($\mathbb{D}$ is the unit disk). Some of my own attempts: Using the Maximum/Minimum Principle, we can split up the the problem in $2$ parts: Suppose $f$ is constant. It then holds that $z^n = c$ has exactly $n$ zeros because of the fundamental theorem of algebra. Suppose $f$ is not constant. It holds that $f$ has at least one zero  in $\mathbb{D}$. It of course also holds that $z^n$ has a zero of multiplicity $n$ at the origin. I think I'm supposed to hit with stuff like Rouché's theorem, the Argument Principle or Schwartz lemma, but nothing seems to fit. Thanks in advance.","Suppose $f: \overline{\mathbb{D}} \to \mathbb{C}$ is continuous, analytic in $\mathbb{D}$ and satisfies $|f(z)|<1$ for $|z|=1$. Find the number of solutions to the equation $f(z) = z^n$ where $n$ is a positive integer. ($\mathbb{D}$ is the unit disk). Some of my own attempts: Using the Maximum/Minimum Principle, we can split up the the problem in $2$ parts: Suppose $f$ is constant. It then holds that $z^n = c$ has exactly $n$ zeros because of the fundamental theorem of algebra. Suppose $f$ is not constant. It holds that $f$ has at least one zero  in $\mathbb{D}$. It of course also holds that $z^n$ has a zero of multiplicity $n$ at the origin. I think I'm supposed to hit with stuff like Rouché's theorem, the Argument Principle or Schwartz lemma, but nothing seems to fit. Thanks in advance.",,"['complex-analysis', 'analysis', 'analyticity']"
45,Is there a simpler form for $\Re \frac{\Gamma(1/2-i)}{\Gamma(1-i)}$?,Is there a simpler form for ?,\Re \frac{\Gamma(1/2-i)}{\Gamma(1-i)},"Is there a simpler (i.e. manifestly real) form for $\Re \frac{\Gamma(1/2-i)}{\Gamma(1-i)}$ or $\Im \frac{\Gamma(1/2-i)}{\Gamma(1-i)}$, or more generally for $\frac{\Gamma(1/2-ia)}{\Gamma(1-ia)}$ with $a \in \mathbb{R}$? This question arose in the evaluation of the integral $$ \int_0^{\pi/2}\cos\left(2a \ln \sin x \right) \, dx= \frac{\sqrt \pi}{2}\Re \frac{\Gamma(1/2-ia)}{\Gamma(1-ia)} $$ After 'FullSimplify'-ing, Mathematica gives (for $a=1$) $$ \frac{1}{\pi }2^{-3-2 i} \text{Sinh}[2 \pi ] \left(2^{2 i} \sqrt{\pi } \text{Csch}[\pi ] \left(\text{Gamma}\left[\frac{1}{2}+i\right] \text{Gamma}[1-i]-\text{Gamma}\left[\frac{1}{2}-i\right] \text{Gamma}[1+i]\right)+\text{Gamma}[1-i]^2 \text{Gamma}[1+2 i] (-1+\text{Tanh}[\pi ])+2^{4 i} \text{Gamma}[1+i]^2 \text{Gamma}[1-2 i] (1+\text{Tanh}[\pi ])\right) $$ which I don't find simpler at all and still contain's $i$'s. I tried to use the product representation for $\Gamma$ and $1/\Gamma$ but those attempts did not result in anything useful.","Is there a simpler (i.e. manifestly real) form for $\Re \frac{\Gamma(1/2-i)}{\Gamma(1-i)}$ or $\Im \frac{\Gamma(1/2-i)}{\Gamma(1-i)}$, or more generally for $\frac{\Gamma(1/2-ia)}{\Gamma(1-ia)}$ with $a \in \mathbb{R}$? This question arose in the evaluation of the integral $$ \int_0^{\pi/2}\cos\left(2a \ln \sin x \right) \, dx= \frac{\sqrt \pi}{2}\Re \frac{\Gamma(1/2-ia)}{\Gamma(1-ia)} $$ After 'FullSimplify'-ing, Mathematica gives (for $a=1$) $$ \frac{1}{\pi }2^{-3-2 i} \text{Sinh}[2 \pi ] \left(2^{2 i} \sqrt{\pi } \text{Csch}[\pi ] \left(\text{Gamma}\left[\frac{1}{2}+i\right] \text{Gamma}[1-i]-\text{Gamma}\left[\frac{1}{2}-i\right] \text{Gamma}[1+i]\right)+\text{Gamma}[1-i]^2 \text{Gamma}[1+2 i] (-1+\text{Tanh}[\pi ])+2^{4 i} \text{Gamma}[1+i]^2 \text{Gamma}[1-2 i] (1+\text{Tanh}[\pi ])\right) $$ which I don't find simpler at all and still contain's $i$'s. I tried to use the product representation for $\Gamma$ and $1/\Gamma$ but those attempts did not result in anything useful.",,"['complex-analysis', 'special-functions', 'gamma-function']"
46,Minimum Modulus Principle for a constant fuction in a simple closed curve,Minimum Modulus Principle for a constant fuction in a simple closed curve,,"Suppose that $f$ is analytic on a domain $D$, which contains a simple closed curve $\gamma$ and the inside of $\gamma$. If $|f|$ is constant on $\gamma$, then I want to prove that either $f$ is constant or $f$ has a zero inside $\gamma$ Here is my take: if $f$ is constant, i dont see a reason why $|f|$ wouldnt be constant :) if $f$ is not constant, then the max/min modulus principle applies ... meaning $|f|$ can not have any local max/min on D now i am lost at this point ...","Suppose that $f$ is analytic on a domain $D$, which contains a simple closed curve $\gamma$ and the inside of $\gamma$. If $|f|$ is constant on $\gamma$, then I want to prove that either $f$ is constant or $f$ has a zero inside $\gamma$ Here is my take: if $f$ is constant, i dont see a reason why $|f|$ wouldnt be constant :) if $f$ is not constant, then the max/min modulus principle applies ... meaning $|f|$ can not have any local max/min on D now i am lost at this point ...",,"['complex-analysis', 'proof-writing', 'roots', 'maximum-principle']"
47,Residue Calculation,Residue Calculation,,"I am stuck on what should be a trivial residue calculation. Any suggestions?  Compute the residue of $\frac{e^{2iz}-1}{z^2}$ at $z=0$. Isn't it a double pole, i.e., shouldn't I be able to take the limit of the derivative of the function multiplied by $z^2$ to get $0$? ...for some reason the answer is $2i$?","I am stuck on what should be a trivial residue calculation. Any suggestions?  Compute the residue of $\frac{e^{2iz}-1}{z^2}$ at $z=0$. Isn't it a double pole, i.e., shouldn't I be able to take the limit of the derivative of the function multiplied by $z^2$ to get $0$? ...for some reason the answer is $2i$?",,"['complex-analysis', 'residue-calculus']"
48,Let $f$ have a zero at $z_0$ of multiplicity $k$. Show that the residue of $\frac{f'}{f}$ at $z_0$ is $k$.,Let  have a zero at  of multiplicity . Show that the residue of  at  is .,f z_0 k \frac{f'}{f} z_0 k,Let $f$ have a zero at $z_0$ of multiplicity $k$. Show that the residue of $\frac{f'}{f}$ at $z_0$ is $k$. I tried approaching it by saying that $h(z) = \frac{f'}{f}$ has a simple pole because $f'$ has zero of order $k-1$ and $f$ has zero of order $k$ but I don't know if that is the correct strategy to calculate residue. Any help will be appreciated.,Let $f$ have a zero at $z_0$ of multiplicity $k$. Show that the residue of $\frac{f'}{f}$ at $z_0$ is $k$. I tried approaching it by saying that $h(z) = \frac{f'}{f}$ has a simple pole because $f'$ has zero of order $k-1$ and $f$ has zero of order $k$ but I don't know if that is the correct strategy to calculate residue. Any help will be appreciated.,,['complex-analysis']
49,Caccioppoli inequality,Caccioppoli inequality,,"Assume we have established the following version of Caccioppoli inequality $$\int |\nabla u|^2 \psi^2 dA\leq C \int u^2 |\nabla \psi| ^2 dA$$ for $C^2(\mathbb C)$- smooth functions $u\geq 0$ with $\Delta u\geq 0$, and $\psi\in C_c^\infty (\mathbb C)$ (compactly supported, smooth) test functions. Is there a way to upgrade this inequality, so that it holds for $\psi \in C_c(\mathbb C)$ (continuous, compactly supported), such that $\nabla \psi$ exists almost everywhere, and it is bounded, and supported on a finite measure set? The reason is that I want to use a bump function $\psi$ such that $\psi=1$ on a disk $D(0,a)$, $\psi=0$ outside $D(0,b)$ ($b>a$), but $\nabla \psi$ does not exist on $|z|=a,|z|=b$.","Assume we have established the following version of Caccioppoli inequality $$\int |\nabla u|^2 \psi^2 dA\leq C \int u^2 |\nabla \psi| ^2 dA$$ for $C^2(\mathbb C)$- smooth functions $u\geq 0$ with $\Delta u\geq 0$, and $\psi\in C_c^\infty (\mathbb C)$ (compactly supported, smooth) test functions. Is there a way to upgrade this inequality, so that it holds for $\psi \in C_c(\mathbb C)$ (continuous, compactly supported), such that $\nabla \psi$ exists almost everywhere, and it is bounded, and supported on a finite measure set? The reason is that I want to use a bump function $\psi$ such that $\psi=1$ on a disk $D(0,a)$, $\psi=0$ outside $D(0,b)$ ($b>a$), but $\nabla \psi$ does not exist on $|z|=a,|z|=b$.",,"['real-analysis', 'complex-analysis', 'partial-differential-equations']"
50,Almost everywhere convergence of some series,Almost everywhere convergence of some series,,Let $\{r_n\}$ be an arbitrary numerical sequence. Prove that $\sum_{n=1}^\infty\frac{1}{2^n\sqrt{|x-r_n|}}$. Prove that it converges almost everywhere on $\Bbb R$.,Let $\{r_n\}$ be an arbitrary numerical sequence. Prove that $\sum_{n=1}^\infty\frac{1}{2^n\sqrt{|x-r_n|}}$. Prove that it converges almost everywhere on $\Bbb R$.,,"['real-analysis', 'complex-analysis']"
51,"How to prove that $\{(x,y) \in \mathbb{C}: y =0 \}$ is the ""same"" as $\mathbb{R}$?","How to prove that  is the ""same"" as ?","\{(x,y) \in \mathbb{C}: y =0 \} \mathbb{R}","I'm reading a book about complex analysis. And in this book they define complex numbers as pairs of real numbers with complex multiplication and addition. Then they say that we can associate $(x,0)$ as the element $x$ in $\mathbb{R}$, because addition and multiplication is preserved. I can verify this fact and I know from ring theory that we have a field isomorphism. But I was thinking, that in analysis we use more facts about the real numbers than just it's field properties right ? Shouldn't I proof ""more"" if we want to call $\{(x,0)\}$ and $\mathbb{R}$ the same ? The book I read about real analysis (Kenneth Ross: Elementary Analysis) the define the real numbers as the unique set that satsify the field axioms, ordered set axioms and the ""complete"" axiom. If I want to use $\{(x,0)\}$ in the same was as $\mathbb{R}$ defined in my analysis book. Should I just proof it is field isomorphism as in ring theory, or should I prove more ?","I'm reading a book about complex analysis. And in this book they define complex numbers as pairs of real numbers with complex multiplication and addition. Then they say that we can associate $(x,0)$ as the element $x$ in $\mathbb{R}$, because addition and multiplication is preserved. I can verify this fact and I know from ring theory that we have a field isomorphism. But I was thinking, that in analysis we use more facts about the real numbers than just it's field properties right ? Shouldn't I proof ""more"" if we want to call $\{(x,0)\}$ and $\mathbb{R}$ the same ? The book I read about real analysis (Kenneth Ross: Elementary Analysis) the define the real numbers as the unique set that satsify the field axioms, ordered set axioms and the ""complete"" axiom. If I want to use $\{(x,0)\}$ in the same was as $\mathbb{R}$ defined in my analysis book. Should I just proof it is field isomorphism as in ring theory, or should I prove more ?",,['complex-analysis']
52,"If a function is complex differentiable, how do we know its real and imaginary parts are infinitely differentiable?","If a function is complex differentiable, how do we know its real and imaginary parts are infinitely differentiable?",,"Sorry I'm really rusty on multivariable calc. Suppose $f: \mathbb{C} \to \mathbb{C}$ is holomorphic and $f(x,y) = u(x,y) + v(x,y)i$, then we know that the partials $u_x, u_y, v_x, v_y$ exist and are continuous, so $u$ and $v$ are real differentiable. But how do we use the infinite complex differentiability of $f$ to prove the infinite real differentiability of $u$ and $v$?","Sorry I'm really rusty on multivariable calc. Suppose $f: \mathbb{C} \to \mathbb{C}$ is holomorphic and $f(x,y) = u(x,y) + v(x,y)i$, then we know that the partials $u_x, u_y, v_x, v_y$ exist and are continuous, so $u$ and $v$ are real differentiable. But how do we use the infinite complex differentiability of $f$ to prove the infinite real differentiability of $u$ and $v$?",,"['complex-analysis', 'multivariable-calculus']"
53,Order of $\frac{f}{g}$,Order of,\frac{f}{g},"An entire function is of finite order $\rho$ if $$\rho = \inf \{\lambda \geq 0  \ | \ \exists A, B > 0 \ s.t. \  |f(z)|\leq Ae^{B|z|^{\lambda}} \forall z \in \mathbb{C} \}$$ Prove that if $f$ and $g$ are entire functions of finite order  $\rho$, and $\frac{f}{g}$ is entire,then $\frac{f}{g}$ is of order $\leq \rho$. Any hint ?","An entire function is of finite order $\rho$ if $$\rho = \inf \{\lambda \geq 0  \ | \ \exists A, B > 0 \ s.t. \  |f(z)|\leq Ae^{B|z|^{\lambda}} \forall z \in \mathbb{C} \}$$ Prove that if $f$ and $g$ are entire functions of finite order  $\rho$, and $\frac{f}{g}$ is entire,then $\frac{f}{g}$ is of order $\leq \rho$. Any hint ?",,"['complex-analysis', 'analysis']"
54,An exercise in complex analysis,An exercise in complex analysis,,"Consider the following question: Let $f: \mathbb C \rightarrow \mathbb C$ be a function such that it's real and imaginary part is differentiable at $z = 0$ in the sense of $\mathbb R^2$, assume further  $$       L =   \lim_{z \rightarrow 0} \biggr| \frac{  f(z)  }{  z  } \biggr| $$ exists. Prove that either $f(z)$ or $\overline{f(z)}$ is holomorphic at $z = 0$. I am asking is there a better solution rather then the following brutal force one I will present. Since $u_x, u_y, v_x, v_y$ exists, we know $f(z) = u(x, y) + i v(x, y)$ is a continuous function. So $$ 	|f(0) | = \lim_{z \rightarrow 0 } | f(z) | = \lim_{z \rightarrow 0} |z | \biggr| \frac{ f(z ) }{ z } \biggr| = \lim_{z \rightarrow 0} | z | \cdot \lim_{z \rightarrow 0} \biggr| \frac{ f(z) }{ z } \biggr| = 0 \cdot L = 0 $$ So, $f(0) = 0 \Longrightarrow u(0, 0) = v(0 , 0) = 0$. We split the case for $L = 0$ and $L \ne 0$. If $L = 0$, we have $$ 	\lim_{z \rightarrow 0} \biggr| \frac{ f(z + 0) - f(0) }{z} - 0  \biggr|  = \lim_{z \rightarrow 0} \biggr| \frac{ f(z) }{ z } \biggr| = 0 \Longrightarrow \lim_{z \rightarrow 0} \frac{ f(z + 0) - f(0) }{z } = 0  $$ In this case $f$ is holomorphic at $z = 0$ and $f'(0) = 0$. Now, assume $L \ne 0$, from the assumption, we know $u, v: \mathbb R^2 \rightarrow \mathbb R$ are differentiable function at $(x, y) = (0, 0)$ in the $\mathbb R^2$ sense. So $f(z) = f(x, y) = (u(x, y), v(x, y))$ when viewed as function from $\mathbb R^2$ to itself is also differentiable at the origin. From definition, there exists a linear map $A: \mathbb R^2 \rightarrow \mathbb R^2$ such that for $h \in \mathbb R^2$ sufficiently close to $(0, 0)$ we have $$ 	f(h) = f(0) + A \cdot h + \epsilon(h) \text{ where } \lim_{h \rightarrow 0} \frac{ | \epsilon( h) | }{ | h| } = 0 \tag{$\ast$} $$ It follows from $(\ast)$ that  $$ 	\frac{ | f(h) | }{ | h| } = \frac{ | A \cdot h + \epsilon (h) | }{ | h| } $$ By triangle inequality,  $$ 	\biggr| \frac{ | A \cdot h | }{ | h | } - \frac{ | \epsilon (h) | }{ | h| } \biggr| \leq \frac{ | f(h) | }{ | h | } \leq \frac{ | A \cdot h | }{ | h | } + \frac{ | \epsilon(h) | }{ | h| } $$ Take limit as $h$ approach to the origin on both sides of above inequality, we have $$ 	\biggr| \lim_{h \rightarrow 0 }  \frac{ | A \cdot h | }{ | h | } - \lim_{h \rightarrow 0 } \frac{ | \epsilon (h) | }{ | h| } \biggr| \leq L \leq \lim_{h \rightarrow 0 } \frac{ | A \cdot h | }{ | h | } + \lim_{h \rightarrow 0 } \frac{ | \epsilon (h) | }{ | h | } $$ because $\lim_{z \rightarrow 0 } | f(z) | / | z | = L$ from assumption. Next, we put $\lim_{h \rightarrow 0 } | \epsilon (h) | / | h | = 0$ obtained in $(\ast)$ to above inequality  $$ 	\biggr| \lim_{h \rightarrow 0 }  \frac{ | A \cdot h | }{ | h | } - 0 \biggr| \leq L \leq \lim_{h \rightarrow 0 } \frac{ | A \cdot h | }{ | h | } + 0 \Longrightarrow \lim_{h \rightarrow 0} \frac{ | Ah | }{ | h | } = L \in \mathbb R_+ $$ So $| A h | / | h | \rightarrow L \in \mathbb R_+$ for any vector which approaches to the origin from any direction. Let  $$ 	A =  	\begin{pmatrix} 		a & b \\ 		c & d 	\end{pmatrix} 	\text{ where } a, b, c, d \in \mathbb R $$ Take $h = (-tb, ta) \in \mathbb R^2$. By calculation, $A h = (0, (ad -bc)t )$, it follows that $| h| = \sqrt{a^2 + b^2} |t |, | Ah | = | ad - bc | \cdot |t |$,  $$ 	\lim_{h \rightarrow 0} \frac{ | Ah | }{ | h | } = \lim_{t \rightarrow 0} \frac{ | (ad - bc) t | }{ \sqrt{a^2 + b^2} |t | } =  \frac{ | ad - bc | }{\sqrt{a^2 + b^2} } $$ Since $L \ne 0$, we have $L = | ad - bc | / \sqrt{a^2 + b^2}  = \det A / \sqrt{a^2 + b^2} \ne 0 \Rightarrow | \det A | \ne 0 \Longrightarrow A$ is invertible. Now, take $h = (t, 0)$ then $Ah = (at, ct), | h | = | t |, | Ah | = \sqrt{a^2 + c^2} | t |$, it follows that $$ 	L = \lim_{h \rightarrow 0 } \frac{ |A h | }{ | h | } = \lim_{t \rightarrow 0} \frac{ \sqrt{a^2 + c^2 } | t | }{ | t | } = \sqrt{a^2 + c^2} $$ Similarly, direct calculation can show $$ 	L =  	\begin{cases} 		\sqrt{b^2 + d^2 }							&\text{ when we take $h = (0, t)$} \\ 		\sqrt{ (a + b)^2 + (c + d)^2 } / \sqrt{2}				&\text{ when we take $h = (t, t)$} \\ 		\sqrt{ (a - b)^2 + (c - d)^2 }  / \sqrt{2}				&\text{ when we take $h = (t, -t)$} 		 	\end{cases} $$ Observe  $$ 	\frac{ \sqrt{(a + b)^2 + (c + d)^2 } }{ \sqrt{2} } = \frac{ \sqrt{ (a - b)^2 + (c - d )^2 }}{ \sqrt{2} } \Longrightarrow ab + cd = 0 \tag{7.1} $$ Since $L^2 = a^2 + c^2 = b^2 + d^2$, we deduce $a^2 - b^2 = d^2 - c^2$, square both sides we have $$ 	(a^2 - b^2)^2 = (d^2 - c^2)^2 \iff a^4 + b^4 - 2a^2b^2 = c^4 + d^4 - 2c^2d^2 $$ It follows from above that $a^4 + b^4 = c^4 + d^4$ for $ab + cd = 0 \rightarrow ab = -cd \rightarrow a^2b^2 = c^2d^2$. So $$ 	a^4 + b^4 = c^4 + d^4 \Longrightarrow a^4 - c^4 = d^4 - b^4 \Longrightarrow (a^2 + c^2)(a^2 - c^2) = (d^2 + b^2)(d^2 - b^2) $$ Because $L^2 = a^2 + c^2 = b^2 + d^2 > 0$, $(a^2 + c^2)(a^2 - c^2) = (d^2 + b^2)(d^2 - b^2) \rightarrow L^2(a^2 - c^2) = L^2(d^2 - b^2) \rightarrow a^2 - c^2 = d^2 - b^2$. Together with $a^2 + c^2 = b^2 + d^2$, we get $$ 	a^2 = \frac{a^2 + c^2 }{2} + \frac{a^2 - c^2}{2} = \frac{b^2 + d^2}{2} + \frac{d^2 - b^2}{2} = d^2, c^2 = b^2 + d^2 - a^2 = b^2 $$ So, $d = \pm a, b = \pm c$. Hence we have to consider the following four cases $$ 	(1) \, d = a, b = c; (2)\, d = -a, b = c; (3)\, d = a, b = -c; (4) \, d= -a, b = -c \tag{7.2} $$ Recall if $f(z)$ or $\overline{f(z)}$ is holomorphic at $z = 0$, then the Jacobian matrix of $f(z) $ and $\overline{ f(z) }$  $$ 	J(f(z) ) =  	\begin{pmatrix} 		u_x & u_y \\ 		v_x & v_y  	\end{pmatrix} \; 	J (\overline{f(z) } ) =  	\begin{pmatrix} 		u_x & -u_y \\ 		v_x & -v_y  	\end{pmatrix} 	\text{ is of the form} 	\begin{pmatrix} 	 	p & -q \\ 		q & p 	\end{pmatrix} 	\tag{$\ast \ast$} $$ We want to show in any of the cases from $(7.2)$, the linear map $A$ when treated as the Jacobian for $f(z)$ or $\overline{f(z) }$ satisfies condition $(\ast \ast)$. That is either $a = d, c = -b$ or $a = -d, c = b$. For the first case, since $ab + cd = 2ac = 0 \Rightarrow a =0$ or $c = 0$. Since $\det A = a^2 - c^2 \ne 0$, if $a = 0$ then $c \ne 0 \Rightarrow a = 0 = -d, b = c$. If $c = 0$ then $a \ne 0 \Rightarrow a = d, c = 0 = -b$ both satisfies $(\ast \ast)$. For the second case, $d = -a \iff a = -d$ together with $c = b$ they automatically satisfies condition $(\ast)$. For the third case, same result follows because $b = -c \iff c = -b$. Last but not least, for $d = -a, b = -c$, we deduce $ab + cd = a(-c) + c(-a) = -2ac = 0 \Rightarrow ac = 0 \Rightarrow a = 0$ or $c = 0$. Since $\det A = ad - bc = a(-a) - (-c)c = c^2 - a^2 \ne 0$, in this case we have either $a = 0, c \ne 0$ or $a \ne 0, c =0$. If $a = 0, c \ne 0$, $a = 0 = d, b = -c$. If $c = 0, a \ne 0$, $a = -d, c = 0 = b$ both also satisfy condition $(\ast \ast)$. Hence, we have checked all possible cases for $A$ which implies either $f(z)$ or $\overline{f(z) }$ satisfies Cauchy-Riemann conditions at $z = 0$. Therefore either $f(z)$ or $\overline{f(z) }$ is holomorphic at $z = 0$.","Consider the following question: Let $f: \mathbb C \rightarrow \mathbb C$ be a function such that it's real and imaginary part is differentiable at $z = 0$ in the sense of $\mathbb R^2$, assume further  $$       L =   \lim_{z \rightarrow 0} \biggr| \frac{  f(z)  }{  z  } \biggr| $$ exists. Prove that either $f(z)$ or $\overline{f(z)}$ is holomorphic at $z = 0$. I am asking is there a better solution rather then the following brutal force one I will present. Since $u_x, u_y, v_x, v_y$ exists, we know $f(z) = u(x, y) + i v(x, y)$ is a continuous function. So $$ 	|f(0) | = \lim_{z \rightarrow 0 } | f(z) | = \lim_{z \rightarrow 0} |z | \biggr| \frac{ f(z ) }{ z } \biggr| = \lim_{z \rightarrow 0} | z | \cdot \lim_{z \rightarrow 0} \biggr| \frac{ f(z) }{ z } \biggr| = 0 \cdot L = 0 $$ So, $f(0) = 0 \Longrightarrow u(0, 0) = v(0 , 0) = 0$. We split the case for $L = 0$ and $L \ne 0$. If $L = 0$, we have $$ 	\lim_{z \rightarrow 0} \biggr| \frac{ f(z + 0) - f(0) }{z} - 0  \biggr|  = \lim_{z \rightarrow 0} \biggr| \frac{ f(z) }{ z } \biggr| = 0 \Longrightarrow \lim_{z \rightarrow 0} \frac{ f(z + 0) - f(0) }{z } = 0  $$ In this case $f$ is holomorphic at $z = 0$ and $f'(0) = 0$. Now, assume $L \ne 0$, from the assumption, we know $u, v: \mathbb R^2 \rightarrow \mathbb R$ are differentiable function at $(x, y) = (0, 0)$ in the $\mathbb R^2$ sense. So $f(z) = f(x, y) = (u(x, y), v(x, y))$ when viewed as function from $\mathbb R^2$ to itself is also differentiable at the origin. From definition, there exists a linear map $A: \mathbb R^2 \rightarrow \mathbb R^2$ such that for $h \in \mathbb R^2$ sufficiently close to $(0, 0)$ we have $$ 	f(h) = f(0) + A \cdot h + \epsilon(h) \text{ where } \lim_{h \rightarrow 0} \frac{ | \epsilon( h) | }{ | h| } = 0 \tag{$\ast$} $$ It follows from $(\ast)$ that  $$ 	\frac{ | f(h) | }{ | h| } = \frac{ | A \cdot h + \epsilon (h) | }{ | h| } $$ By triangle inequality,  $$ 	\biggr| \frac{ | A \cdot h | }{ | h | } - \frac{ | \epsilon (h) | }{ | h| } \biggr| \leq \frac{ | f(h) | }{ | h | } \leq \frac{ | A \cdot h | }{ | h | } + \frac{ | \epsilon(h) | }{ | h| } $$ Take limit as $h$ approach to the origin on both sides of above inequality, we have $$ 	\biggr| \lim_{h \rightarrow 0 }  \frac{ | A \cdot h | }{ | h | } - \lim_{h \rightarrow 0 } \frac{ | \epsilon (h) | }{ | h| } \biggr| \leq L \leq \lim_{h \rightarrow 0 } \frac{ | A \cdot h | }{ | h | } + \lim_{h \rightarrow 0 } \frac{ | \epsilon (h) | }{ | h | } $$ because $\lim_{z \rightarrow 0 } | f(z) | / | z | = L$ from assumption. Next, we put $\lim_{h \rightarrow 0 } | \epsilon (h) | / | h | = 0$ obtained in $(\ast)$ to above inequality  $$ 	\biggr| \lim_{h \rightarrow 0 }  \frac{ | A \cdot h | }{ | h | } - 0 \biggr| \leq L \leq \lim_{h \rightarrow 0 } \frac{ | A \cdot h | }{ | h | } + 0 \Longrightarrow \lim_{h \rightarrow 0} \frac{ | Ah | }{ | h | } = L \in \mathbb R_+ $$ So $| A h | / | h | \rightarrow L \in \mathbb R_+$ for any vector which approaches to the origin from any direction. Let  $$ 	A =  	\begin{pmatrix} 		a & b \\ 		c & d 	\end{pmatrix} 	\text{ where } a, b, c, d \in \mathbb R $$ Take $h = (-tb, ta) \in \mathbb R^2$. By calculation, $A h = (0, (ad -bc)t )$, it follows that $| h| = \sqrt{a^2 + b^2} |t |, | Ah | = | ad - bc | \cdot |t |$,  $$ 	\lim_{h \rightarrow 0} \frac{ | Ah | }{ | h | } = \lim_{t \rightarrow 0} \frac{ | (ad - bc) t | }{ \sqrt{a^2 + b^2} |t | } =  \frac{ | ad - bc | }{\sqrt{a^2 + b^2} } $$ Since $L \ne 0$, we have $L = | ad - bc | / \sqrt{a^2 + b^2}  = \det A / \sqrt{a^2 + b^2} \ne 0 \Rightarrow | \det A | \ne 0 \Longrightarrow A$ is invertible. Now, take $h = (t, 0)$ then $Ah = (at, ct), | h | = | t |, | Ah | = \sqrt{a^2 + c^2} | t |$, it follows that $$ 	L = \lim_{h \rightarrow 0 } \frac{ |A h | }{ | h | } = \lim_{t \rightarrow 0} \frac{ \sqrt{a^2 + c^2 } | t | }{ | t | } = \sqrt{a^2 + c^2} $$ Similarly, direct calculation can show $$ 	L =  	\begin{cases} 		\sqrt{b^2 + d^2 }							&\text{ when we take $h = (0, t)$} \\ 		\sqrt{ (a + b)^2 + (c + d)^2 } / \sqrt{2}				&\text{ when we take $h = (t, t)$} \\ 		\sqrt{ (a - b)^2 + (c - d)^2 }  / \sqrt{2}				&\text{ when we take $h = (t, -t)$} 		 	\end{cases} $$ Observe  $$ 	\frac{ \sqrt{(a + b)^2 + (c + d)^2 } }{ \sqrt{2} } = \frac{ \sqrt{ (a - b)^2 + (c - d )^2 }}{ \sqrt{2} } \Longrightarrow ab + cd = 0 \tag{7.1} $$ Since $L^2 = a^2 + c^2 = b^2 + d^2$, we deduce $a^2 - b^2 = d^2 - c^2$, square both sides we have $$ 	(a^2 - b^2)^2 = (d^2 - c^2)^2 \iff a^4 + b^4 - 2a^2b^2 = c^4 + d^4 - 2c^2d^2 $$ It follows from above that $a^4 + b^4 = c^4 + d^4$ for $ab + cd = 0 \rightarrow ab = -cd \rightarrow a^2b^2 = c^2d^2$. So $$ 	a^4 + b^4 = c^4 + d^4 \Longrightarrow a^4 - c^4 = d^4 - b^4 \Longrightarrow (a^2 + c^2)(a^2 - c^2) = (d^2 + b^2)(d^2 - b^2) $$ Because $L^2 = a^2 + c^2 = b^2 + d^2 > 0$, $(a^2 + c^2)(a^2 - c^2) = (d^2 + b^2)(d^2 - b^2) \rightarrow L^2(a^2 - c^2) = L^2(d^2 - b^2) \rightarrow a^2 - c^2 = d^2 - b^2$. Together with $a^2 + c^2 = b^2 + d^2$, we get $$ 	a^2 = \frac{a^2 + c^2 }{2} + \frac{a^2 - c^2}{2} = \frac{b^2 + d^2}{2} + \frac{d^2 - b^2}{2} = d^2, c^2 = b^2 + d^2 - a^2 = b^2 $$ So, $d = \pm a, b = \pm c$. Hence we have to consider the following four cases $$ 	(1) \, d = a, b = c; (2)\, d = -a, b = c; (3)\, d = a, b = -c; (4) \, d= -a, b = -c \tag{7.2} $$ Recall if $f(z)$ or $\overline{f(z)}$ is holomorphic at $z = 0$, then the Jacobian matrix of $f(z) $ and $\overline{ f(z) }$  $$ 	J(f(z) ) =  	\begin{pmatrix} 		u_x & u_y \\ 		v_x & v_y  	\end{pmatrix} \; 	J (\overline{f(z) } ) =  	\begin{pmatrix} 		u_x & -u_y \\ 		v_x & -v_y  	\end{pmatrix} 	\text{ is of the form} 	\begin{pmatrix} 	 	p & -q \\ 		q & p 	\end{pmatrix} 	\tag{$\ast \ast$} $$ We want to show in any of the cases from $(7.2)$, the linear map $A$ when treated as the Jacobian for $f(z)$ or $\overline{f(z) }$ satisfies condition $(\ast \ast)$. That is either $a = d, c = -b$ or $a = -d, c = b$. For the first case, since $ab + cd = 2ac = 0 \Rightarrow a =0$ or $c = 0$. Since $\det A = a^2 - c^2 \ne 0$, if $a = 0$ then $c \ne 0 \Rightarrow a = 0 = -d, b = c$. If $c = 0$ then $a \ne 0 \Rightarrow a = d, c = 0 = -b$ both satisfies $(\ast \ast)$. For the second case, $d = -a \iff a = -d$ together with $c = b$ they automatically satisfies condition $(\ast)$. For the third case, same result follows because $b = -c \iff c = -b$. Last but not least, for $d = -a, b = -c$, we deduce $ab + cd = a(-c) + c(-a) = -2ac = 0 \Rightarrow ac = 0 \Rightarrow a = 0$ or $c = 0$. Since $\det A = ad - bc = a(-a) - (-c)c = c^2 - a^2 \ne 0$, in this case we have either $a = 0, c \ne 0$ or $a \ne 0, c =0$. If $a = 0, c \ne 0$, $a = 0 = d, b = -c$. If $c = 0, a \ne 0$, $a = -d, c = 0 = b$ both also satisfy condition $(\ast \ast)$. Hence, we have checked all possible cases for $A$ which implies either $f(z)$ or $\overline{f(z) }$ satisfies Cauchy-Riemann conditions at $z = 0$. Therefore either $f(z)$ or $\overline{f(z) }$ is holomorphic at $z = 0$.",,['complex-analysis']
55,"Does $u,v$ possess continuous partials at $z_0?$",Does  possess continuous partials at,"u,v z_0?","I know from Cauchy-Riemann equation that if $f=u+iv$ is differentiable at a point $z_0$ then the 1st order partials of $u,v$ exist at $z_0$ and satisfy C-R equation there. My question is in such a case does $u,v$ possess continuous partials at $z_0?$","I know from Cauchy-Riemann equation that if $f=u+iv$ is differentiable at a point $z_0$ then the 1st order partials of $u,v$ exist at $z_0$ and satisfy C-R equation there. My question is in such a case does $u,v$ possess continuous partials at $z_0?$",,['complex-analysis']
56,"$ 2|f^{'}(0)| = \sup_{z, w \in D} |f(z)-f(w)|$",," 2|f^{'}(0)| = \sup_{z, w \in D} |f(z)-f(w)|","Let $D = B(0,1) \subset \mathbb{C} $ a disc, $f$ holomorphic on $D$. I want to demonstrate that if $$ 2|f^{'}(0)| = \sup_{z, w \in D} |f(z)-f(w)|$$  then $f$ is linear. I know this is a well-known result. Where can I find the proof ?","Let $D = B(0,1) \subset \mathbb{C} $ a disc, $f$ holomorphic on $D$. I want to demonstrate that if $$ 2|f^{'}(0)| = \sup_{z, w \in D} |f(z)-f(w)|$$  then $f$ is linear. I know this is a well-known result. Where can I find the proof ?",,"['complex-analysis', 'analysis', 'reference-request', 'derivatives']"
57,Prove that $f(\Bbb C)=\Bbb C$,Prove that,f(\Bbb C)=\Bbb C,I need some help with the following problem: Let $f:\Bbb C \to \Bbb C$ be continuous satisfying that $f(\Bbb C)$ is an open set and that $|f(z)| \to \infty$ as $z\to \infty$. Prove that $f(\Bbb C)=\Bbb C$. My idea on this one is to prove by contradiction and assume that $S=f(\Bbb C)\ne\Bbb C$ to get some contradiction with the given two properties of the function. But I have no idea on how to proceed next. Thanks in advance.,I need some help with the following problem: Let $f:\Bbb C \to \Bbb C$ be continuous satisfying that $f(\Bbb C)$ is an open set and that $|f(z)| \to \infty$ as $z\to \infty$. Prove that $f(\Bbb C)=\Bbb C$. My idea on this one is to prove by contradiction and assume that $S=f(\Bbb C)\ne\Bbb C$ to get some contradiction with the given two properties of the function. But I have no idea on how to proceed next. Thanks in advance.,,['complex-analysis']
58,Polynomial Bound on $\mathbb{C}$,Polynomial Bound on,\mathbb{C},This is an old qual problem I can't seem to solve: Let $p$ be a polynomial with degree $n$ s.t. $|p(z)| \leq 1$ on the closed unit disk. Show $|p(z)| \leq |z|^n$ for $z$ outside the unit disk. I tried looking at $ z^n p(\frac{1}{z})$ since its holomorphic and has the same norm on the boundary but can't get things to work out.,This is an old qual problem I can't seem to solve: Let $p$ be a polynomial with degree $n$ s.t. $|p(z)| \leq 1$ on the closed unit disk. Show $|p(z)| \leq |z|^n$ for $z$ outside the unit disk. I tried looking at $ z^n p(\frac{1}{z})$ since its holomorphic and has the same norm on the boundary but can't get things to work out.,,['complex-analysis']
59,$uv$ is harmonic if and only if $u+icv$ is analytic for some real c,is harmonic if and only if  is analytic for some real c,uv u+icv,"Let $u$ and $v$ be non constant harmonic functions on a complex domain. Prove that $uv$ is harmonic if and only if $u+icv$ is analytic for some real $c$. I can prove the ""if"" part. I am having some trouble with the ""only if"" part. My argument is : $uv$ is harmonic implies $u_xv_x+u_yv_y=0$. This means that $<u_x,u_y>$ is perpendicular to $<v_x,v_y>$. This implies that $<v_x,v_y> = c<-u_y,u_x>$. This proves the result. My question is - does this sound rigorous enough?","Let $u$ and $v$ be non constant harmonic functions on a complex domain. Prove that $uv$ is harmonic if and only if $u+icv$ is analytic for some real $c$. I can prove the ""if"" part. I am having some trouble with the ""only if"" part. My argument is : $uv$ is harmonic implies $u_xv_x+u_yv_y=0$. This means that $<u_x,u_y>$ is perpendicular to $<v_x,v_y>$. This implies that $<v_x,v_y> = c<-u_y,u_x>$. This proves the result. My question is - does this sound rigorous enough?",,"['complex-analysis', 'harmonic-functions']"
60,Two simple questions about the complex function $f(z)=e^z$,Two simple questions about the complex function,f(z)=e^z,"I am working on basic problems about complex numbers and functions in order to learn complex analysis from Lang's textbook. I am trying to solve the following exercise: Let $f:\mathbb C \to \mathbb C$, $f(z)=e^z$ (if $z=a+bi$, then $e^z=e^ae^{ib}$) a) Find the image under $f$ of the set $S=\{z \in \mathbb C : 0\leq Im(z)<2\pi\}$. b) Show that the image of the line $\{t+it : t \in \mathbb R\}$ is a spiral. For a), I am not so sure how to describe the image, I know that if $z=a+bi$, then $b \in [0,2\pi)$. By definition, $e^{ib}=\cos(b)+i\sin(b)$, so the image of the set $S$ is $T=\{w \in \mathbb C : w=f(z)=e^a(\cos(b)+i\sin(b)), \space b \in [0,2\pi)\}$ Is this a correct way of describing the image of the given set? For part b), I don't know how to show this, maybe I need to find a possible parameterization $\phi(t)$ of the set $\{(u(t),v(t),t) \in \mathbb R^3\}$ where if $w=f(z(t))$, then $w=u(t)+iv(t)$. If $z$ is in the line, then $z=t+it$, and $f(z)=e^t(\cos(t)+i\sin(t))=e^t\cos(t)+ie^t\sin(t)$, so the parametrization would be $\phi(t)=(e^t\cos(t),e^t\sin(t),t)$ Is this the parameterization of a spiral?","I am working on basic problems about complex numbers and functions in order to learn complex analysis from Lang's textbook. I am trying to solve the following exercise: Let $f:\mathbb C \to \mathbb C$, $f(z)=e^z$ (if $z=a+bi$, then $e^z=e^ae^{ib}$) a) Find the image under $f$ of the set $S=\{z \in \mathbb C : 0\leq Im(z)<2\pi\}$. b) Show that the image of the line $\{t+it : t \in \mathbb R\}$ is a spiral. For a), I am not so sure how to describe the image, I know that if $z=a+bi$, then $b \in [0,2\pi)$. By definition, $e^{ib}=\cos(b)+i\sin(b)$, so the image of the set $S$ is $T=\{w \in \mathbb C : w=f(z)=e^a(\cos(b)+i\sin(b)), \space b \in [0,2\pi)\}$ Is this a correct way of describing the image of the given set? For part b), I don't know how to show this, maybe I need to find a possible parameterization $\phi(t)$ of the set $\{(u(t),v(t),t) \in \mathbb R^3\}$ where if $w=f(z(t))$, then $w=u(t)+iv(t)$. If $z$ is in the line, then $z=t+it$, and $f(z)=e^t(\cos(t)+i\sin(t))=e^t\cos(t)+ie^t\sin(t)$, so the parametrization would be $\phi(t)=(e^t\cos(t),e^t\sin(t),t)$ Is this the parameterization of a spiral?",,"['complex-analysis', 'complex-numbers']"
61,"Weierstrass $\wp$-function: $(\partial_z \wp(z,\omega))^2$",Weierstrass -function:,"\wp (\partial_z \wp(z,\omega))^2","Let $\vartheta(z,\omega)$ be the Riemann theta function. For $j \in \mathbb{Z}$ let $c_j$ be the coefficient of $z^{j}$ in the Laurent expansion of $\partial_z \log \vartheta \left(z + \frac{1 + \omega}{2}, \omega \right)$ at $z$ = 0. The Weierstrass $\wp$ function is $$ \wp(z, \omega) = - \partial_z^{2} \log  \vartheta \left(z +  \frac{1 + \omega}{2}, \omega \right) + c_1. $$ I have successfully shown, that $\wp$ is $\omega$- and $1$-periodical and a few other properties. Im stuck at the last property I have to show. Let $ e_1 = \wp \left(\frac{1}{2}, \omega \right), \; e_2 = \wp \left(\frac{\omega}{2}, \omega \right)$ and $e_3 = \wp \left(\frac{1+\omega}{2}, \omega \right)$. Then  $$ (\partial_z \wp(z,\omega))^{2} = 4(\wp(z,\omega) -e_1)(\wp(z,\omega) - e_2)(\wp(z,\omega) -e_3) $$ Any hints on how I could start the proof are very much appreciated!","Let $\vartheta(z,\omega)$ be the Riemann theta function. For $j \in \mathbb{Z}$ let $c_j$ be the coefficient of $z^{j}$ in the Laurent expansion of $\partial_z \log \vartheta \left(z + \frac{1 + \omega}{2}, \omega \right)$ at $z$ = 0. The Weierstrass $\wp$ function is $$ \wp(z, \omega) = - \partial_z^{2} \log  \vartheta \left(z +  \frac{1 + \omega}{2}, \omega \right) + c_1. $$ I have successfully shown, that $\wp$ is $\omega$- and $1$-periodical and a few other properties. Im stuck at the last property I have to show. Let $ e_1 = \wp \left(\frac{1}{2}, \omega \right), \; e_2 = \wp \left(\frac{\omega}{2}, \omega \right)$ and $e_3 = \wp \left(\frac{1+\omega}{2}, \omega \right)$. Then  $$ (\partial_z \wp(z,\omega))^{2} = 4(\wp(z,\omega) -e_1)(\wp(z,\omega) - e_2)(\wp(z,\omega) -e_3) $$ Any hints on how I could start the proof are very much appreciated!",,"['complex-analysis', 'elliptic-functions']"
62,Cauchy integral formula problem,Cauchy integral formula problem,,"Let $C$ be the unit circle centered at the origin and $a \in \mathbb{R}$. $$\int_0^{2\pi}\frac{dt}{1 + a^2 - 2a\cos(t)} = \int_C \frac{i\;dz}{(z-a)(az-1)}$$ Use Cauchy's integral formula to deduce if $0 \leq a < 1$ then, $$\int_0^{2\pi}\frac{dt}{1 + a^2 - 2a\cos(t)} = \frac{2\pi}{1 - a^2}$$ I was unsure how to go about the first part. I could just try to compute both integrals and show they are equal but that doesn't seem to be what is wanted. Is there is a trick that I am missing? As for the second part, I keep getting $0$. I use the first part, and see there are singularities at $z = a$ and $z = 1/a$. If $a < 1$ then the singularities lie within $C$ and Cauchy's integral formula can be used. I think I can split the integral into two, where I take $1 / (z - a)$ to be my function and evaluate the integral around a circle centered at $1/a$, and vice versa take $1/(az - 1)$ to be my function and evaluate the integral around a circle centered at $a$. Many thanks.","Let $C$ be the unit circle centered at the origin and $a \in \mathbb{R}$. $$\int_0^{2\pi}\frac{dt}{1 + a^2 - 2a\cos(t)} = \int_C \frac{i\;dz}{(z-a)(az-1)}$$ Use Cauchy's integral formula to deduce if $0 \leq a < 1$ then, $$\int_0^{2\pi}\frac{dt}{1 + a^2 - 2a\cos(t)} = \frac{2\pi}{1 - a^2}$$ I was unsure how to go about the first part. I could just try to compute both integrals and show they are equal but that doesn't seem to be what is wanted. Is there is a trick that I am missing? As for the second part, I keep getting $0$. I use the first part, and see there are singularities at $z = a$ and $z = 1/a$. If $a < 1$ then the singularities lie within $C$ and Cauchy's integral formula can be used. I think I can split the integral into two, where I take $1 / (z - a)$ to be my function and evaluate the integral around a circle centered at $1/a$, and vice versa take $1/(az - 1)$ to be my function and evaluate the integral around a circle centered at $a$. Many thanks.",,['integration']
63,Find the series expansion of $f(z)=\frac{4}{(z-1)(z+3)}$ around $z_0=-1$.,Find the series expansion of  around .,f(z)=\frac{4}{(z-1)(z+3)} z_0=-1,"I have a homework problem that states: Let $$f(z)=\dfrac{4}{(z-1)(z+3)}.$$ Then $f$ has a power series expansion at each point $z_0 \in \Bbb{C}\backslash\{ 1,-3\}.$ Give a formula for the radius of convergence of the series at $z_0 \neq 1, -3.$ Find the series expansion for $z_0=-1.$ Here's what I've done so far. I know that $$\dfrac{4}{(z-1)(z+3)} = \dfrac{1}{z-1}-\dfrac{1}{z+3}=\dfrac{-1}{1-z}-\dfrac{1}{3(1-[\frac{z}{3}])}=-\sum_{n=0}^\infty z^n -\dfrac{1}{3}\sum_{n=0}^\infty \left(-\frac{z}{3}\right)^n.$$ Now, I know the radius of convergence will be the smaller of the two radii. The series on the left has a R.O.C. of $|z|<1$ and the series on the right has a R.O.C. of $|z|<3.$ So the R.O.C. must be $|z|<1.$ However, I am stuck on how to rewrite this series around $z_0 = -1.$ I'm assuming that I can just use the formula for Taylor series expansion and just take a bunch of derivatives. But this seems tedious. I'm certain there's a way to just rewrite the current summations... but what is that method? I certainly can't just replace $z$ with $z+1.$ ...can I? Any help would be appreciated. Thanks.","I have a homework problem that states: Let $$f(z)=\dfrac{4}{(z-1)(z+3)}.$$ Then $f$ has a power series expansion at each point $z_0 \in \Bbb{C}\backslash\{ 1,-3\}.$ Give a formula for the radius of convergence of the series at $z_0 \neq 1, -3.$ Find the series expansion for $z_0=-1.$ Here's what I've done so far. I know that $$\dfrac{4}{(z-1)(z+3)} = \dfrac{1}{z-1}-\dfrac{1}{z+3}=\dfrac{-1}{1-z}-\dfrac{1}{3(1-[\frac{z}{3}])}=-\sum_{n=0}^\infty z^n -\dfrac{1}{3}\sum_{n=0}^\infty \left(-\frac{z}{3}\right)^n.$$ Now, I know the radius of convergence will be the smaller of the two radii. The series on the left has a R.O.C. of $|z|<1$ and the series on the right has a R.O.C. of $|z|<3.$ So the R.O.C. must be $|z|<1.$ However, I am stuck on how to rewrite this series around $z_0 = -1.$ I'm assuming that I can just use the formula for Taylor series expansion and just take a bunch of derivatives. But this seems tedious. I'm certain there's a way to just rewrite the current summations... but what is that method? I certainly can't just replace $z$ with $z+1.$ ...can I? Any help would be appreciated. Thanks.",,"['calculus', 'complex-analysis']"
64,doubt in complex integration,doubt in complex integration,,"I was doing problems on complex integration and got stuck at one question. The question is $$ \int_{\gamma}{{\rm e}^{{\rm i}\pi z}\left(z + i\right)^{2}\cos\left(nz\right)                                     \over z^{2} - 1}\,{\rm d}z $$ where $\gamma= \left\{z: \left\vert\,z\,\right\vert=2\cos\left(\theta\right)\,, -\pi/2 \leq \theta \leq \pi/2\right\}$. I am not getting any thought in which way to consider  $\gamma$ in this question. Any hint will be sufficient for me. thanks a lot for help.","I was doing problems on complex integration and got stuck at one question. The question is $$ \int_{\gamma}{{\rm e}^{{\rm i}\pi z}\left(z + i\right)^{2}\cos\left(nz\right)                                     \over z^{2} - 1}\,{\rm d}z $$ where $\gamma= \left\{z: \left\vert\,z\,\right\vert=2\cos\left(\theta\right)\,, -\pi/2 \leq \theta \leq \pi/2\right\}$. I am not getting any thought in which way to consider  $\gamma$ in this question. Any hint will be sufficient for me. thanks a lot for help.",,"['complex-analysis', 'complex-numbers', 'complex-integration']"
65,Counterexample: For real functions existence of all higher order derivatives doesn't imply analycity.,Counterexample: For real functions existence of all higher order derivatives doesn't imply analycity.,,"In the lecture we had an example for a function $f: \mathbb R \to \mathbb R$, which is not analytic. We defined, that a function is said to be analytic at some point $x_0$ if a Taylor series expansion is valid at that point. The following example is supposed to demonstrate that for real functions the existence of all higher order derivatives does not imply analycity. Let $f: \mathbb R \to \mathbb R$ be a real function defined by $f(x) = e^{-1/x^2}$. Then $f$ is infinitely many times differentiable and $f^{(n)}(0) = 0$, $\forall n \in \mathbb N \cup \{0\}$.  But $f \ne 0$ in some neighbourhood of $x = 0$ and therefore $f$ is not analytic. I understand the logic behind that counterexample, but I don't understand why we are allowed to take the point $x = 0$, since $f$ is actually not defined at that point?! Thanks for help.","In the lecture we had an example for a function $f: \mathbb R \to \mathbb R$, which is not analytic. We defined, that a function is said to be analytic at some point $x_0$ if a Taylor series expansion is valid at that point. The following example is supposed to demonstrate that for real functions the existence of all higher order derivatives does not imply analycity. Let $f: \mathbb R \to \mathbb R$ be a real function defined by $f(x) = e^{-1/x^2}$. Then $f$ is infinitely many times differentiable and $f^{(n)}(0) = 0$, $\forall n \in \mathbb N \cup \{0\}$.  But $f \ne 0$ in some neighbourhood of $x = 0$ and therefore $f$ is not analytic. I understand the logic behind that counterexample, but I don't understand why we are allowed to take the point $x = 0$, since $f$ is actually not defined at that point?! Thanks for help.",,"['real-analysis', 'complex-analysis', 'taylor-expansion']"
66,"If $U$ is connected, any two sections $U \to \mathfrak S$ either coincide or have disjoint images (Is my proof correct?)","If  is connected, any two sections  either coincide or have disjoint images (Is my proof correct?)",U U \to \mathfrak S,"I tried proving the following statement by Ahlfors, page 287: If $U$ is connected and $\varphi,\psi: U \to \Gamma(U,\mathfrak S)$, then either $\varphi$ and $\psi$ are identical, or the images $\varphi(U)$ and $\psi(U)$ are disjoint. Indeed, the sets with $\varphi - \psi = 0$ and $\varphi- \psi \neq 0$ are both open. Here $\mathfrak S$ is the sheaf of germs of analytic functions over some open set $D \subseteq \mathbb C$, and $\Gamma(U,\mathfrak S)$ is the set of all sections from an open set $U \subseteq D$. Mainly, what I tried is expanding on the openness of the sets $$A=\{ \zeta \in U: \varphi(\zeta)-\psi(\zeta)=\mathbf{0}_\zeta \} \\ B=\{ \zeta \in U: \varphi(\zeta)-\psi(\zeta) \neq \mathbf{0}_\zeta \} $$ $A$ can be viewed as the inverse image $$(\varphi-\psi)^{-1}[\omega(U)] $$ where $\omega:\zeta \mapsto \mathbf{0}_\zeta$ is the zero section . Since it is an open map, and $\varphi-\psi$ is continuous it follows that $A$ is open. However, proving that $B$ is open turned out to be more problematic for me. I couldn't do it using representation as an inverse image, so I tried the more direct approach: Let $\zeta_0 \in B$ and suppose that in every arbitrarily small disk $\Delta(\zeta_0,r)$ there exists a point $\zeta_r$ such that $(\varphi-\psi)(\zeta_r)=\mathbf{0}_{\zeta_r} \equiv \omega(\zeta_r)$. Thus we may extract a sequence $\{\zeta_n \}_{n=1}^\infty$ which tends to $\zeta_0$, such that for all $n$, $(\varphi-\psi)(\zeta_n)=\omega(\zeta_n)$. Since both $\varphi-\psi$ and $\omega$ are continuous, they are sequentially continuous, and taking the limit as $n \to \infty$ yields the contradiction $\varphi(z_0)-\psi(\zeta_0)=\mathbf{0}_{\zeta_0}$. It follows that $B$ is open as well. Lastly, $U=A \coprod B$, and from connectedness either $U=A$ or $U=B$. In the former $\varphi,\psi$ coincide, and in the latter they have disjoint images (this is because if they share a value, they must share it at the same point). Is this all correct? If not, please help me correct it. Thanks!","I tried proving the following statement by Ahlfors, page 287: If $U$ is connected and $\varphi,\psi: U \to \Gamma(U,\mathfrak S)$, then either $\varphi$ and $\psi$ are identical, or the images $\varphi(U)$ and $\psi(U)$ are disjoint. Indeed, the sets with $\varphi - \psi = 0$ and $\varphi- \psi \neq 0$ are both open. Here $\mathfrak S$ is the sheaf of germs of analytic functions over some open set $D \subseteq \mathbb C$, and $\Gamma(U,\mathfrak S)$ is the set of all sections from an open set $U \subseteq D$. Mainly, what I tried is expanding on the openness of the sets $$A=\{ \zeta \in U: \varphi(\zeta)-\psi(\zeta)=\mathbf{0}_\zeta \} \\ B=\{ \zeta \in U: \varphi(\zeta)-\psi(\zeta) \neq \mathbf{0}_\zeta \} $$ $A$ can be viewed as the inverse image $$(\varphi-\psi)^{-1}[\omega(U)] $$ where $\omega:\zeta \mapsto \mathbf{0}_\zeta$ is the zero section . Since it is an open map, and $\varphi-\psi$ is continuous it follows that $A$ is open. However, proving that $B$ is open turned out to be more problematic for me. I couldn't do it using representation as an inverse image, so I tried the more direct approach: Let $\zeta_0 \in B$ and suppose that in every arbitrarily small disk $\Delta(\zeta_0,r)$ there exists a point $\zeta_r$ such that $(\varphi-\psi)(\zeta_r)=\mathbf{0}_{\zeta_r} \equiv \omega(\zeta_r)$. Thus we may extract a sequence $\{\zeta_n \}_{n=1}^\infty$ which tends to $\zeta_0$, such that for all $n$, $(\varphi-\psi)(\zeta_n)=\omega(\zeta_n)$. Since both $\varphi-\psi$ and $\omega$ are continuous, they are sequentially continuous, and taking the limit as $n \to \infty$ yields the contradiction $\varphi(z_0)-\psi(\zeta_0)=\mathbf{0}_{\zeta_0}$. It follows that $B$ is open as well. Lastly, $U=A \coprod B$, and from connectedness either $U=A$ or $U=B$. In the former $\varphi,\psi$ coincide, and in the latter they have disjoint images (this is because if they share a value, they must share it at the same point). Is this all correct? If not, please help me correct it. Thanks!",,"['complex-analysis', 'sheaf-theory']"
67,Solving equation system of complex funtions,Solving equation system of complex funtions,,Does there exist two complex functions $f$ and $g$ satisfy below equation system? $$ \begin{cases} f=e^g\\ g=e^f \end{cases} $$ What about analytic funtions?,Does there exist two complex functions $f$ and $g$ satisfy below equation system? $$ \begin{cases} f=e^g\\ g=e^f \end{cases} $$ What about analytic funtions?,,"['complex-analysis', 'functional-equations']"
68,Bessel function with complex argument,Bessel function with complex argument,,So I understand that the bessel functions of the first kind are the ones that satisfy this equation: $$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x^2-\alpha^2)y = 0$$ and the result is a linear combination of the bessel functions of the first and second kind. equation(1): $$ A J_a(x) + B Y_a(x) $$ Now let: $x = iv$ $$ \dfrac{dy}{dx} = \dfrac{dy}{dv} \dfrac{1}{i} \\ \dfrac{d^2y}{dx^2} = -\dfrac{d^2y}{dx^2}$$ Substituting in the original equation we get: $$(iv)^2(-1)\frac{d^2y}{dv^2}+(iv)\dfrac{1}{i}\frac{dy}{dv}+((iv)^2-\alpha^2)y = 0 \\ v^2\frac{d^2y}{dv^2}+v\frac{dy}{dv}- (v^2+\alpha^2)y = 0 $$ This is the equation which has solutions the modified bessel functions. Is equation 1 with x = iv a solution to this equation? ( I think it is but not sure ) and the second equation is: Why is this then not true? $$ J_a(ix) = I_a(x) \\ Y_a(ix) = K_a(x) $$,So I understand that the bessel functions of the first kind are the ones that satisfy this equation: $$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x^2-\alpha^2)y = 0$$ and the result is a linear combination of the bessel functions of the first and second kind. equation(1): $$ A J_a(x) + B Y_a(x) $$ Now let: $x = iv$ $$ \dfrac{dy}{dx} = \dfrac{dy}{dv} \dfrac{1}{i} \\ \dfrac{d^2y}{dx^2} = -\dfrac{d^2y}{dx^2}$$ Substituting in the original equation we get: $$(iv)^2(-1)\frac{d^2y}{dv^2}+(iv)\dfrac{1}{i}\frac{dy}{dv}+((iv)^2-\alpha^2)y = 0 \\ v^2\frac{d^2y}{dv^2}+v\frac{dy}{dv}- (v^2+\alpha^2)y = 0 $$ This is the equation which has solutions the modified bessel functions. Is equation 1 with x = iv a solution to this equation? ( I think it is but not sure ) and the second equation is: Why is this then not true? $$ J_a(ix) = I_a(x) \\ Y_a(ix) = K_a(x) $$,,"['complex-analysis', 'ordinary-differential-equations', 'special-functions']"
69,Why is the modular $\lambda$ function a quotient of two meromorphic functions in the U.H.P.?,Why is the modular  function a quotient of two meromorphic functions in the U.H.P.?,\lambda,"In Ahlfors' complex analysis text, page 278 it says: It is quite clear from (9) that $\lambda(\tau)$ is the quotient of two analytic functions in the upper half plane $\text{Im} \tau > 0$. Formula (9) is the definition of the Weierstrass $\wp$ function $$\wp(z;\omega_1, \omega_2)=\frac{1}{z^2}+ \sum_{\omega \neq 0} \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} $$ where the sum is taken over all linear combinations $$\omega=n_1 \omega_1+n_2 \omega_2 $$ with integer coefficients (except the zero combination). The $\lambda$ function is defined as $$\lambda(\tau)= \frac{e_3-e_2}{e_1-e_2} $$ where $e_1=\wp(\omega_1/2),e_2=\wp(\omega_2/2),e_3=\wp((\omega_1+\omega_2)/2)$, and $\tau$ is the ratio $\omega_2/\omega_1$. My attempt: Following the author's advice I found that $\lambda$ equals $$\frac{\frac{1}{\left( \frac{\omega_1+\omega_2}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{\omega_1+\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]-\frac{1}{\left( \frac{\omega_2}{2} \right)^2}- \sum \left[ \frac{1}{\left( \frac{\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2}\right]}{\frac{1}{\left( \frac{\omega_1}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{\omega_1}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]-\frac{1}{\left( \frac{\omega_2}{2} \right)^2}-\sum \left[ \frac{1}{\left( \frac{\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]}  $$ multiplying both the numerator and the denominator by $\omega_1^2$ we get the numerator $$ \frac{1}{\left( \frac{1+\tau}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{1+\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2} \right]-\frac{1}{\left( \frac{\tau}{2} \right)^2}- \sum \left[ \frac{1}{\left( \frac{\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2}\right]$$ and the denominator $$\frac{1}{\left( \frac{1}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{1}{2}-n_1 -n_2 \tau \right)^2}-\frac{1}{\left( n_1+n_2 \tau \right)^2} \right]-\frac{1}{\left( \frac{\tau}{2} \right)^2}-\sum \left[ \frac{1}{\left( \frac{\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2} \right] $$ I think the next step is proving that both the numerator and denominator are analytic as a function of $\tau$ in $\Im \tau>0$. Firstly, I don't know how to do that, and secondly, shouldn't they be analytic for $\Im \tau<0$ as well? Could anyone please tell me why are the numerator and denominator analytic in $\Im \tau>0$?","In Ahlfors' complex analysis text, page 278 it says: It is quite clear from (9) that $\lambda(\tau)$ is the quotient of two analytic functions in the upper half plane $\text{Im} \tau > 0$. Formula (9) is the definition of the Weierstrass $\wp$ function $$\wp(z;\omega_1, \omega_2)=\frac{1}{z^2}+ \sum_{\omega \neq 0} \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} $$ where the sum is taken over all linear combinations $$\omega=n_1 \omega_1+n_2 \omega_2 $$ with integer coefficients (except the zero combination). The $\lambda$ function is defined as $$\lambda(\tau)= \frac{e_3-e_2}{e_1-e_2} $$ where $e_1=\wp(\omega_1/2),e_2=\wp(\omega_2/2),e_3=\wp((\omega_1+\omega_2)/2)$, and $\tau$ is the ratio $\omega_2/\omega_1$. My attempt: Following the author's advice I found that $\lambda$ equals $$\frac{\frac{1}{\left( \frac{\omega_1+\omega_2}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{\omega_1+\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]-\frac{1}{\left( \frac{\omega_2}{2} \right)^2}- \sum \left[ \frac{1}{\left( \frac{\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2}\right]}{\frac{1}{\left( \frac{\omega_1}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{\omega_1}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]-\frac{1}{\left( \frac{\omega_2}{2} \right)^2}-\sum \left[ \frac{1}{\left( \frac{\omega_2}{2}-n_1 \omega_1-n_2 \omega_2 \right)^2}-\frac{1}{\left( n_1 \omega_1+n_2 \omega_2 \right)^2} \right]}  $$ multiplying both the numerator and the denominator by $\omega_1^2$ we get the numerator $$ \frac{1}{\left( \frac{1+\tau}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{1+\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2} \right]-\frac{1}{\left( \frac{\tau}{2} \right)^2}- \sum \left[ \frac{1}{\left( \frac{\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2}\right]$$ and the denominator $$\frac{1}{\left( \frac{1}{2} \right)^2}+\sum \left[ \frac{1}{\left( \frac{1}{2}-n_1 -n_2 \tau \right)^2}-\frac{1}{\left( n_1+n_2 \tau \right)^2} \right]-\frac{1}{\left( \frac{\tau}{2} \right)^2}-\sum \left[ \frac{1}{\left( \frac{\tau}{2}-n_1-n_2 \tau \right)^2}-\frac{1}{\left( n_1 +n_2 \tau \right)^2} \right] $$ I think the next step is proving that both the numerator and denominator are analytic as a function of $\tau$ in $\Im \tau>0$. Firstly, I don't know how to do that, and secondly, shouldn't they be analytic for $\Im \tau<0$ as well? Could anyone please tell me why are the numerator and denominator analytic in $\Im \tau>0$?",,"['complex-analysis', 'modular-forms', 'elliptic-functions']"
70,On entire functions of slow growth,On entire functions of slow growth,,"Could someone shed light on the following problem? It is Problem 2 in Page 3 of Lectures on Entire Functions by Levin. Let $\psi: [0, \infty]\to [0, \infty] $ be an arbitrary function   which is monotonically increasing without bound. Construct an entire   function $f(z)$ which is not a polynomial and satisfies the inequality   $$\max_{|z|=r} |f(z)|\leqslant 1+ r^{\psi(r)}$$ This is not homework by the way. Just something that I am curious to know. The hint given in the book is: Look for a function in the form of a power series with positive coefficients.","Could someone shed light on the following problem? It is Problem 2 in Page 3 of Lectures on Entire Functions by Levin. Let $\psi: [0, \infty]\to [0, \infty] $ be an arbitrary function   which is monotonically increasing without bound. Construct an entire   function $f(z)$ which is not a polynomial and satisfies the inequality   $$\max_{|z|=r} |f(z)|\leqslant 1+ r^{\psi(r)}$$ This is not homework by the way. Just something that I am curious to know. The hint given in the book is: Look for a function in the form of a power series with positive coefficients.",,['complex-analysis']
71,Does the Weierstrass $\wp$ function have any double values besides $\infty$?,Does the Weierstrass  function have any double values besides ?,\wp \infty,"Given nonzero complex constants $\omega_1,\omega_2$, with nonreal ratio, we define $$\wp(z;\omega_1,\omega_2)=\frac{1}{z^2}+ \sum_\omega \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} $$ where the sum is taken over all nonzero linear combinations $\omega=n_1 \omega_1+n_2 \omega_2$ with integer coefficients. It is known that $\wp$ is of order 2, which means that for any $c \in \hat{\mathbb C}$ the equation $\wp(z)=c$ has two non-congruent solutions (two points are called congruent if their difference is linear combination of $\omega_1,\omega_2$ with integer coefficients). In addition, it is known that $\wp$ is even, and that the poles on the ""lattice"" $ \omega_1 \mathbb Z+\omega_2 \mathbb Z$ are all of order 2. My question is: Could there be a point $z_0$ such that $\wp'(z_0)=0$? This would imply that the value $\wp(z_0)$ is taken twice at $z_0$. I tried using the evenness of the function to show that there isn't such point. However starting with $z_0=\frac{1}{2} \omega_1+\frac{1}{2} \omega_2$ this approach fails. Thanks.","Given nonzero complex constants $\omega_1,\omega_2$, with nonreal ratio, we define $$\wp(z;\omega_1,\omega_2)=\frac{1}{z^2}+ \sum_\omega \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} $$ where the sum is taken over all nonzero linear combinations $\omega=n_1 \omega_1+n_2 \omega_2$ with integer coefficients. It is known that $\wp$ is of order 2, which means that for any $c \in \hat{\mathbb C}$ the equation $\wp(z)=c$ has two non-congruent solutions (two points are called congruent if their difference is linear combination of $\omega_1,\omega_2$ with integer coefficients). In addition, it is known that $\wp$ is even, and that the poles on the ""lattice"" $ \omega_1 \mathbb Z+\omega_2 \mathbb Z$ are all of order 2. My question is: Could there be a point $z_0$ such that $\wp'(z_0)=0$? This would imply that the value $\wp(z_0)$ is taken twice at $z_0$. I tried using the evenness of the function to show that there isn't such point. However starting with $z_0=\frac{1}{2} \omega_1+\frac{1}{2} \omega_2$ this approach fails. Thanks.",,"['complex-analysis', 'analysis', 'elliptic-functions']"
72,Find the Laurent series and residue of $\frac{z}{(\sin(z))^2}$ at $z_0 = 0$.,Find the Laurent series and residue of  at .,\frac{z}{(\sin(z))^2} z_0 = 0,"Find the Laurent series for the given function about the indicated point. Also, give the residue of the function at the point. $$\frac{z}{(\sin(z))^2}\quad \text{at}\quad z_0 = 0 \quad\text{four terms of the Laurent  series}$$ I am not sure how to approach this question. Can anyone help me with this? Thank you.","Find the Laurent series for the given function about the indicated point. Also, give the residue of the function at the point. I am not sure how to approach this question. Can anyone help me with this? Thank you.",\frac{z}{(\sin(z))^2}\quad \text{at}\quad z_0 = 0 \quad\text{four terms of the Laurent  series},"['complex-analysis', 'laurent-series']"
73,Calculate $i ^ {i+1}$ and also $i^{i^{i^{\dots}}}$,Calculate  and also,i ^ {i+1} i^{i^{i^{\dots}}},I just wanted to ask the following questions please.  The first I have is calculate $i^{(i+1)}$ and also $i^i$  I was just wondering if anyone can nudge me in the right direction to solve these questions. many thanks!,I just wanted to ask the following questions please.  The first I have is calculate $i^{(i+1)}$ and also $i^i$  I was just wondering if anyone can nudge me in the right direction to solve these questions. many thanks!,,"['complex-analysis', 'complex-numbers']"
74,Calculating residue in exponential fraction,Calculating residue in exponential fraction,,"I want to calculate the residue of $$\dfrac{e^{it}}{e^t+e^{-t}}$$ at $t=\pi i/2$. To calculate the residue at $\pi i/2$, I write $$\frac{e^{it}}{e^t+e^{-t}}=\frac{e^{it}e^t}{e^{2t}+1}=\frac{e^{it}e^t}{(e^t+i)(e^t-i)}$$so the residue is $$\frac{e^{i\pi i/2}e^{\pi i/2}}{(e^{\pi i/2}+i)}=\frac{e^{-\pi/2}i}{2i}=\dfrac{e^{-\pi/2}}{2}$$ Is this a correct way to calculate the residue? I just want to make sure I understand it correctly.","I want to calculate the residue of $$\dfrac{e^{it}}{e^t+e^{-t}}$$ at $t=\pi i/2$. To calculate the residue at $\pi i/2$, I write $$\frac{e^{it}}{e^t+e^{-t}}=\frac{e^{it}e^t}{e^{2t}+1}=\frac{e^{it}e^t}{(e^t+i)(e^t-i)}$$so the residue is $$\frac{e^{i\pi i/2}e^{\pi i/2}}{(e^{\pi i/2}+i)}=\frac{e^{-\pi/2}i}{2i}=\dfrac{e^{-\pi/2}}{2}$$ Is this a correct way to calculate the residue? I just want to make sure I understand it correctly.",,['complex-analysis']
75,Proving the cotangent function is uniformly bounded on the complex plane,Proving the cotangent function is uniformly bounded on the complex plane,,I'm trying to prove that the function $\cot\left(z\right)=i\frac{e^{iz}+e^{-iz}}{e^{iz}-e^{-iz}}$   is uniformly bounded in the complex plane outside $\varepsilon$   neighborhoods of the poles (with the bound depending on $\varepsilon$).   The suggested method in my text is to first show that if $z=x+iy$   and $y>0$   then: $$\frac{e^{-2y}}{1+e^{-2y}}<\left|\cot\left(x+iy\right)+i\right|<\frac{e^{-2y}}{1-e^{-2y}}$$  And if $y<0$   then:$$\frac{e^{2y}}{1+e^{-2y}}<\left|\cot\left(x+iy\right)+i\right|<\frac{e^{2y}}{1-e^{-2y}}$$  The calculations are a bit tedious and they aren't coming out right for me for some reason: \begin{align*} \left|\cot\left(x+i\cdot y\right)+i\right| &=\left|i\cdot\frac{e^{iz}+e^{-iz}}{e^{iz}-e^{-iz}}+i\right| \\ &=\left|i\cdot\frac{e^{2iz}+1}{e^{2iz}-1}+i\right| \\ &=\left|\frac{i\cdot\left(e^{2iz}+1\right)+i\left(e^{2iz}-1\right)}{e^{2iz}-1}\right| \\ &=\left|\frac{2ie^{2i\left(x+i\cdot y\right)}}{e^{2i\left(x+i\cdot y\right)}-1}\right| \\ &=\left|2i\right|\cdot\left|e^{2iz}\right|\left|\frac{1}{e^{2iz}-1}\right|=2\cdot e^{-2y}\cdot\left|\frac{1}{e^{2iz}-1}\right| \\ &=2\cdot e^{-2y}\cdot\left|\frac{e^{2y}}{e^{2ix}-e^{2y}}\right| \\ &=2\left|\frac{1}{e^{2ix}-e^{2y}}\right| =2\frac{1}{\left|e^{2ix}-e^{2y}\right|} \\ &=2\cdot\frac{1}{\left|e^{ix}-e^{y}\right|\left|e^{ix}+e^{y}\right|} \\ &=\frac{2}{\sqrt{\left(e^{y}+\cos\left(x\right)\right)^{2}+\sin^{2}\left(x\right)}\cdot\sqrt{\left(e^{y}-\cos\left(x\right)\right)^{2}+\sin^{2}\left(x\right)}} \\ &=\frac{2}{\sqrt{e^{2y}+2e^{y}\cos\left(x\right)+1}\cdot\sqrt{e^{2y}-2e^{y}\cos\left(x\right)+1}} \\ &=\frac{2}{\sqrt{e^{4y}+2e^{2y}-4e^{2y}\cos^{2}\left(x\right)+1}} \\ &=\frac{2}{\sqrt{\left(1+e^{2y}\right)^{2}-4e^{2y}\cos^{2}\left(x\right)}} \end{align*} The denominator is maximal when $\cos^{2}\left(x\right)=1$ and minimal when $\cos^{2}\left(x\right)=0$ and thus: $$\frac{2e^{-2y}}{1+e^{-2y}}=\frac{2}{1+e^{2y}}\leq\left|\cot\left(x+i\cdot y\right)+i\right|\leq\frac{2}{\sqrt{\left(e^{2y}-1\right)^{2}}}=\frac{2}{\left|1-e^{2y}\right|}=\frac{2e^{-2y}}{1-e^{-2y}}.$$   I can't figure out whether I made an error in the calculations or whether there was an error in the suggested bound. I'm also not sure how to use these bounds in order to reach the required conclusion. Regardless of this method I'm also curious whether someone has an alternative and perhaps less technical method of proving the claim. Thanks in advance!,I'm trying to prove that the function $\cot\left(z\right)=i\frac{e^{iz}+e^{-iz}}{e^{iz}-e^{-iz}}$   is uniformly bounded in the complex plane outside $\varepsilon$   neighborhoods of the poles (with the bound depending on $\varepsilon$).   The suggested method in my text is to first show that if $z=x+iy$   and $y>0$   then: $$\frac{e^{-2y}}{1+e^{-2y}}<\left|\cot\left(x+iy\right)+i\right|<\frac{e^{-2y}}{1-e^{-2y}}$$  And if $y<0$   then:$$\frac{e^{2y}}{1+e^{-2y}}<\left|\cot\left(x+iy\right)+i\right|<\frac{e^{2y}}{1-e^{-2y}}$$  The calculations are a bit tedious and they aren't coming out right for me for some reason: \begin{align*} \left|\cot\left(x+i\cdot y\right)+i\right| &=\left|i\cdot\frac{e^{iz}+e^{-iz}}{e^{iz}-e^{-iz}}+i\right| \\ &=\left|i\cdot\frac{e^{2iz}+1}{e^{2iz}-1}+i\right| \\ &=\left|\frac{i\cdot\left(e^{2iz}+1\right)+i\left(e^{2iz}-1\right)}{e^{2iz}-1}\right| \\ &=\left|\frac{2ie^{2i\left(x+i\cdot y\right)}}{e^{2i\left(x+i\cdot y\right)}-1}\right| \\ &=\left|2i\right|\cdot\left|e^{2iz}\right|\left|\frac{1}{e^{2iz}-1}\right|=2\cdot e^{-2y}\cdot\left|\frac{1}{e^{2iz}-1}\right| \\ &=2\cdot e^{-2y}\cdot\left|\frac{e^{2y}}{e^{2ix}-e^{2y}}\right| \\ &=2\left|\frac{1}{e^{2ix}-e^{2y}}\right| =2\frac{1}{\left|e^{2ix}-e^{2y}\right|} \\ &=2\cdot\frac{1}{\left|e^{ix}-e^{y}\right|\left|e^{ix}+e^{y}\right|} \\ &=\frac{2}{\sqrt{\left(e^{y}+\cos\left(x\right)\right)^{2}+\sin^{2}\left(x\right)}\cdot\sqrt{\left(e^{y}-\cos\left(x\right)\right)^{2}+\sin^{2}\left(x\right)}} \\ &=\frac{2}{\sqrt{e^{2y}+2e^{y}\cos\left(x\right)+1}\cdot\sqrt{e^{2y}-2e^{y}\cos\left(x\right)+1}} \\ &=\frac{2}{\sqrt{e^{4y}+2e^{2y}-4e^{2y}\cos^{2}\left(x\right)+1}} \\ &=\frac{2}{\sqrt{\left(1+e^{2y}\right)^{2}-4e^{2y}\cos^{2}\left(x\right)}} \end{align*} The denominator is maximal when $\cos^{2}\left(x\right)=1$ and minimal when $\cos^{2}\left(x\right)=0$ and thus: $$\frac{2e^{-2y}}{1+e^{-2y}}=\frac{2}{1+e^{2y}}\leq\left|\cot\left(x+i\cdot y\right)+i\right|\leq\frac{2}{\sqrt{\left(e^{2y}-1\right)^{2}}}=\frac{2}{\left|1-e^{2y}\right|}=\frac{2e^{-2y}}{1-e^{-2y}}.$$   I can't figure out whether I made an error in the calculations or whether there was an error in the suggested bound. I'm also not sure how to use these bounds in order to reach the required conclusion. Regardless of this method I'm also curious whether someone has an alternative and perhaps less technical method of proving the claim. Thanks in advance!,,['complex-analysis']
76,For what complex $z$ the series converges,For what complex  the series converges,z,Can someone help me with this assignment? Find for what $z \in \mathbb{C}$ the series converges   $\sum_{n=1}^{\infty} \frac{(2n)!}{(n!)^2}z^n$. I've just calculated (by using Cauchy-Hadamard theorem) that for all $z$ such that $|z|<\frac{1}{4}$ this series converges and for all $z$ such that $|z|>\frac{1}{4}$ it doesn't. I don't know how to check $|z|=\frac{1}{4}$.,Can someone help me with this assignment? Find for what $z \in \mathbb{C}$ the series converges   $\sum_{n=1}^{\infty} \frac{(2n)!}{(n!)^2}z^n$. I've just calculated (by using Cauchy-Hadamard theorem) that for all $z$ such that $|z|<\frac{1}{4}$ this series converges and for all $z$ such that $|z|>\frac{1}{4}$ it doesn't. I don't know how to check $|z|=\frac{1}{4}$.,,"['complex-analysis', 'convergence-divergence', 'power-series']"
77,Evaluating $\int_{-\infty}^\infty \frac{dx}{\cosh(x-a)\cos(2x)}$,Evaluating,\int_{-\infty}^\infty \frac{dx}{\cosh(x-a)\cos(2x)},"I have been asked to evaluate $$\int_{-\infty}^\infty \frac{dx}{\cosh(x-a)\cos(2x)}$$. I'm deliberating on whether this indefinite integral exists or not. The integrand diverges when $x=\frac{1}{2}(n+\frac{1}{2})\pi$ but the $\cosh(x-a)$ term relaxes these singularities exponentially as $x$ goes to infinity. If it does exist, then I'm left with the problem of computing it. This is for a complex variables class, so I was using residue methods. However, there are countably many simple poles along the real axis as well as for each $z=i(n+\frac{1}{2})\pi +a$ in the complex plane, so I don't even know what contour to use. It looks like rectangles and semicircles are out. Any suggestions?","I have been asked to evaluate $$\int_{-\infty}^\infty \frac{dx}{\cosh(x-a)\cos(2x)}$$. I'm deliberating on whether this indefinite integral exists or not. The integrand diverges when $x=\frac{1}{2}(n+\frac{1}{2})\pi$ but the $\cosh(x-a)$ term relaxes these singularities exponentially as $x$ goes to infinity. If it does exist, then I'm left with the problem of computing it. This is for a complex variables class, so I was using residue methods. However, there are countably many simple poles along the real axis as well as for each $z=i(n+\frac{1}{2})\pi +a$ in the complex plane, so I don't even know what contour to use. It looks like rectangles and semicircles are out. Any suggestions?",,"['complex-analysis', 'integration', 'improper-integrals', 'residue-calculus']"
78,Getting value of $\sin x + \cos x$ with complex exponential,Getting value of  with complex exponential,\sin x + \cos x,"I have a bit of difficulty with this. I am trying to express $\sin x + \cos x$ with complex exponential. I started by using Euler's equations. Then, I used the trigonometric substitution $\sin x = \cos(x+\pi/2)$. The problem is that I always end up with $i - 1$ and $i + 1$ (by using different exponent operations), and the fact $e^{i \pi/2} = 1$ and such. I'm lost. It would be supposed to give me $\sqrt{2}\cos(X)$, where $X$ is undetermined. I just don't get it. Could anyone help? Thanks","I have a bit of difficulty with this. I am trying to express $\sin x + \cos x$ with complex exponential. I started by using Euler's equations. Then, I used the trigonometric substitution $\sin x = \cos(x+\pi/2)$. The problem is that I always end up with $i - 1$ and $i + 1$ (by using different exponent operations), and the fact $e^{i \pi/2} = 1$ and such. I'm lost. It would be supposed to give me $\sqrt{2}\cos(X)$, where $X$ is undetermined. I just don't get it. Could anyone help? Thanks",,"['complex-analysis', 'complex-numbers']"
79,Laurent expansion,Laurent expansion,,I have $\dfrac{z}{(z-1)(z+1)}$ and wish to find the Laurent expansion about $z = 1$. I have simplified this down to $\dfrac{z}{z^2 - 1}$ but I'm unsure of where to go from here. A hint has been given that I can use a substitution but I don't know what to use. Any help at all is greatly appreciated.,I have $\dfrac{z}{(z-1)(z+1)}$ and wish to find the Laurent expansion about $z = 1$. I have simplified this down to $\dfrac{z}{z^2 - 1}$ but I'm unsure of where to go from here. A hint has been given that I can use a substitution but I don't know what to use. Any help at all is greatly appreciated.,,"['complex-analysis', 'laurent-series']"
80,I cannot see why Ahlfors' statement is true (Extending a conformal map),I cannot see why Ahlfors' statement is true (Extending a conformal map),,"In page 234 in Ahlfors' complex analysis text, the author talks about extending a conformal map. During the proof he states: We note further that $f'(z) \neq 0$ on $\gamma$· Indeed, $f'(x_0)= 0$ would imply that $f(x_0)$ were a multiple value, in which case the two subarcs of $\gamma$ that meet at $x_0$ would be mapped on arcs that form an angle $\pi/n$ with $n \geq 2$; this is clearly impossible. Here $\gamma$ is a line segment on the real axis, which is contained in the boundary of the domain $\Omega$ (which is mapped conformally onto $|w|<1$). I think that his statemet is false, as $\Omega=(-1,1) \times (0,1)$, with $f(z)=z^2$ is a counterexample: Take $x_0=0$, $f(x_0)=0$ is a double value then, and the segment $\gamma=(-1,1)$ breaks into the subarcs $(-1,0),(0,1)$ which are being mapped onto the same arc (the angle between them is $2\pi$). Is his statement false indeed? If so, do you know what did he mean to say? Thanks.","In page 234 in Ahlfors' complex analysis text, the author talks about extending a conformal map. During the proof he states: We note further that $f'(z) \neq 0$ on $\gamma$· Indeed, $f'(x_0)= 0$ would imply that $f(x_0)$ were a multiple value, in which case the two subarcs of $\gamma$ that meet at $x_0$ would be mapped on arcs that form an angle $\pi/n$ with $n \geq 2$; this is clearly impossible. Here $\gamma$ is a line segment on the real axis, which is contained in the boundary of the domain $\Omega$ (which is mapped conformally onto $|w|<1$). I think that his statemet is false, as $\Omega=(-1,1) \times (0,1)$, with $f(z)=z^2$ is a counterexample: Take $x_0=0$, $f(x_0)=0$ is a double value then, and the segment $\gamma=(-1,1)$ breaks into the subarcs $(-1,0),(0,1)$ which are being mapped onto the same arc (the angle between them is $2\pi$). Is his statement false indeed? If so, do you know what did he mean to say? Thanks.",,"['complex-analysis', 'conformal-geometry']"
81,"Analyticity of a real function on $[0,\infty)$",Analyticity of a real function on,"[0,\infty)","I'm struggling to understand the difference of the analyticity of a real and a complex functions. Consider the following real valued function which is a minimal example of a somewhat more involved problem. I'm only interested in the behavior on the real axis in the interval $[0,\infty)$ and for $b>1$ $$f(x)=\exp\left( -x^b\right)$$ Since the composition of two analytic functions is again holomorphic, I guess we could reduce this further, leave out the exponential and concentrate on the power function. For this, a related problem was already given in this question although there, the goal was to investigate in the complex plane. Question: What can be said about the analyticity of $f$ especially at $x=0$ when the real valued $b>1$ ? Some observations and side questions I chose $b>1$ to ensure that the derivative exists at $x=0$ . If we derive we get $$\partial_x \,f(x)=-bx^{b-1}\cdot \exp\left(-x^b\right)$$ and choosing $b\leq1$ would lead in the limit $x\to0$ to either $0^0$ or infinity. This doesn't seem to solve anything because even when $b>1$ I cannot calculated the limit $x\to-0$ because the expression is complex (and not real valued) for $x<0$ . This leads to some side questions which I really like to have an answer for: Can one say the first derivative of $f$ exist at $r=0$ , although I cannot take the limit from the right side? If I only need the function for $x\in[0,\infty)$ , what are the practical considerations I have to take care of? Obviously, the second derivative is not ensured to exists. Something more?","I'm struggling to understand the difference of the analyticity of a real and a complex functions. Consider the following real valued function which is a minimal example of a somewhat more involved problem. I'm only interested in the behavior on the real axis in the interval and for Since the composition of two analytic functions is again holomorphic, I guess we could reduce this further, leave out the exponential and concentrate on the power function. For this, a related problem was already given in this question although there, the goal was to investigate in the complex plane. Question: What can be said about the analyticity of especially at when the real valued ? Some observations and side questions I chose to ensure that the derivative exists at . If we derive we get and choosing would lead in the limit to either or infinity. This doesn't seem to solve anything because even when I cannot calculated the limit because the expression is complex (and not real valued) for . This leads to some side questions which I really like to have an answer for: Can one say the first derivative of exist at , although I cannot take the limit from the right side? If I only need the function for , what are the practical considerations I have to take care of? Obviously, the second derivative is not ensured to exists. Something more?","[0,\infty) b>1 f(x)=\exp\left( -x^b\right) f x=0 b>1 b>1 x=0 \partial_x \,f(x)=-bx^{b-1}\cdot \exp\left(-x^b\right) b\leq1 x\to0 0^0 b>1 x\to-0 x<0 f r=0 x\in[0,\infty)","['real-analysis', 'complex-analysis', 'analyticity']"
82,Analyticity of Products,Analyticity of Products,,"Assume we have two functions $f,g:\Omega\rightarrow\mathbb{C}$ that are analytic and a third function $h:\Omega\rightarrow\mathbb{C}$ with $f=g\cdot h$. Can one now show that $h$ is analytic as well? Of course $\Omega\subset\mathbb{C}$ is open.","Assume we have two functions $f,g:\Omega\rightarrow\mathbb{C}$ that are analytic and a third function $h:\Omega\rightarrow\mathbb{C}$ with $f=g\cdot h$. Can one now show that $h$ is analytic as well? Of course $\Omega\subset\mathbb{C}$ is open.",,"['complex-analysis', 'analyticity']"
83,Is this function holomorphic at 0?,Is this function holomorphic at 0?,,"This is for homework in my complex analysis class, and I think there may be a mistake.  I wanted to make sure I didn't miss anything obvious before I bring it up to the professor.  The problem asks to show that the function $$ f(z) = \begin{cases} e^{-\frac{1}{z^4}}, & \text{if } z \neq 0 \\ 0, & \text{if } z = 0 \end{cases} $$ is not holomorphic at $z = 0$.  The definition of holomorphic that we are using is: A function $f$ is holomorphic at $z_0$ if $\lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0}$ exists. For the $f$ I defined above, I found that (and WolframAlpha agrees) $$ \lim_{z \to 0} \frac{e^{-\frac{1}{z^4}}}{z} = 0, $$ implying the function is indeed holomorphic at 0. Did I miss anything obvious, or should I bring this to the professor's attention?","This is for homework in my complex analysis class, and I think there may be a mistake.  I wanted to make sure I didn't miss anything obvious before I bring it up to the professor.  The problem asks to show that the function $$ f(z) = \begin{cases} e^{-\frac{1}{z^4}}, & \text{if } z \neq 0 \\ 0, & \text{if } z = 0 \end{cases} $$ is not holomorphic at $z = 0$.  The definition of holomorphic that we are using is: A function $f$ is holomorphic at $z_0$ if $\lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0}$ exists. For the $f$ I defined above, I found that (and WolframAlpha agrees) $$ \lim_{z \to 0} \frac{e^{-\frac{1}{z^4}}}{z} = 0, $$ implying the function is indeed holomorphic at 0. Did I miss anything obvious, or should I bring this to the professor's attention?",,['complex-analysis']
84,please help me grasp the literal meaning of residue,please help me grasp the literal meaning of residue,,"In complex analysis we study a term RESIDUE of a function given by some formulas. While going through its meaning I found that it means left out term or remainder kind of thing. so I was wondering why is this term given such a terminology, can anyone enlighten me towards connection of this mathematical term with its literal meaning.","In complex analysis we study a term RESIDUE of a function given by some formulas. While going through its meaning I found that it means left out term or remainder kind of thing. so I was wondering why is this term given such a terminology, can anyone enlighten me towards connection of this mathematical term with its literal meaning.",,"['complex-analysis', 'terminology']"
85,Book searching in Pluripotential theory,Book searching in Pluripotential theory,,"Can anyone recommend me a book on pluripotential theory with an intuitive approach? I have some course notes on that subject, but it's really abstract and theoretical. I want to understand why pluripotential/subharmonic/... were introduced and so on. My teacher wanted us to learn the book's Z.Blocki... Ex: I don't understand :( 1/ Suppose that $u: \Omega \to \mathbb{R} $, a function $u$ is called harmonic if $u$ is continouns and $$u(x_0)=\dfrac{1}{V_n(B(x_0,r))}\int_{B(x_0,r)}u(x)\mathrm{d}V_n(x)$$. 2/ We have $u(x_0)=\dfrac{1}{\sigma(\partial B(x_0,r))}\int_{\partial B(x_0,r)}u(x)\mathrm{d}\sigma(x)$. Your comments & suggestions are ALWAYS appreciated.","Can anyone recommend me a book on pluripotential theory with an intuitive approach? I have some course notes on that subject, but it's really abstract and theoretical. I want to understand why pluripotential/subharmonic/... were introduced and so on. My teacher wanted us to learn the book's Z.Blocki... Ex: I don't understand :( 1/ Suppose that $u: \Omega \to \mathbb{R} $, a function $u$ is called harmonic if $u$ is continouns and $$u(x_0)=\dfrac{1}{V_n(B(x_0,r))}\int_{B(x_0,r)}u(x)\mathrm{d}V_n(x)$$. 2/ We have $u(x_0)=\dfrac{1}{\sigma(\partial B(x_0,r))}\int_{\partial B(x_0,r)}u(x)\mathrm{d}\sigma(x)$. Your comments & suggestions are ALWAYS appreciated.",,"['complex-analysis', 'measure-theory', 'reference-request', 'harmonic-analysis', 'several-complex-variables']"
86,"Check whether the image of any circle in $\Bbb C \backslash \{0\}$,is again a circle.","Check whether the image of any circle in ,is again a circle.",\Bbb C \backslash \{0\},"I am stuck with the following problem : Let $f(z)=z+\frac 1 z$ for $z \in \Bbb C$ with $z \neq 0$ .Then I have to check whether the following statement is true/fase? The image of any circle in $\Bbb C \backslash \{0\}$ ,is again a circle. I tried by putting $z=re^{i \theta}$ [keeping in mind that $z$ can be on any circle with radius $r$ ] in $f(z)=z+\frac 1 z$ but could not make any conclusion from it. Can someone explain?","I am stuck with the following problem : Let for with .Then I have to check whether the following statement is true/fase? The image of any circle in ,is again a circle. I tried by putting [keeping in mind that can be on any circle with radius ] in but could not make any conclusion from it. Can someone explain?",f(z)=z+\frac 1 z z \in \Bbb C z \neq 0 \Bbb C \backslash \{0\} z=re^{i \theta} z r f(z)=z+\frac 1 z,['complex-analysis']
87,criterions for holomorphic functions,criterions for holomorphic functions,,"What are the criterions for holomorphic functions except $\frac{\partial f}{\partial \overline z}=0$ and $f$ has a power series extension? I was considering the problem, which is the extension of a bounded non-vanishing holomorphic function $f$ such that $|f(z)|=1$ when $|z|=1$ from the closure of the unit disk to the whole plane. After guessing the function $g(z)=1/ \overline{f(1/\overline{z})}$ for $|z|>1$, I can't prove that g is holomorphic. However, I know if $f$ is holomorphic, then we also have $\frac{\partial \overline{f}}{\partial z}=\frac{\partial f}{\partial \overline z}=0$, but we don't have $\frac{\partial f(\overline z)}{\partial z}=\frac{\partial f}{\partial \overline z}$ so the first criterion can't be applied. Also I can't check whether g is holomorphic by the power series criterion since if $f(1/\overline z)=\Sigma a_n (1/\overline z-z_0)^n$ when $|z|>1$, I don't know whether $\frac{1}{\Sigma a_n (1/\overline z-z_0)^n}$ has a power series. I would really appreciate any help. Thanks","What are the criterions for holomorphic functions except $\frac{\partial f}{\partial \overline z}=0$ and $f$ has a power series extension? I was considering the problem, which is the extension of a bounded non-vanishing holomorphic function $f$ such that $|f(z)|=1$ when $|z|=1$ from the closure of the unit disk to the whole plane. After guessing the function $g(z)=1/ \overline{f(1/\overline{z})}$ for $|z|>1$, I can't prove that g is holomorphic. However, I know if $f$ is holomorphic, then we also have $\frac{\partial \overline{f}}{\partial z}=\frac{\partial f}{\partial \overline z}=0$, but we don't have $\frac{\partial f(\overline z)}{\partial z}=\frac{\partial f}{\partial \overline z}$ so the first criterion can't be applied. Also I can't check whether g is holomorphic by the power series criterion since if $f(1/\overline z)=\Sigma a_n (1/\overline z-z_0)^n$ when $|z|>1$, I don't know whether $\frac{1}{\Sigma a_n (1/\overline z-z_0)^n}$ has a power series. I would really appreciate any help. Thanks",,['complex-analysis']
88,finding the points where a complex function is differentiable (Need guidance),finding the points where a complex function is differentiable (Need guidance),,Recently I have encountered the topic on complex differentiation and i had these two questions $f(z) = (z+5)/(z-5i) + (z-5)^10$ and $f(x+iy) = (6x+y^2) + i(5xy+y)$ What I had to do was to determine every point that the functions can be differentiated and also provide them with the formula for the derivative. I have been reading through lectures notes and websites but I still don't understand the theory behind how to actually determine where the points are and differentiate them. can anyone provide any hints and guides on how to do so?,Recently I have encountered the topic on complex differentiation and i had these two questions $f(z) = (z+5)/(z-5i) + (z-5)^10$ and $f(x+iy) = (6x+y^2) + i(5xy+y)$ What I had to do was to determine every point that the functions can be differentiated and also provide them with the formula for the derivative. I have been reading through lectures notes and websites but I still don't understand the theory behind how to actually determine where the points are and differentiate them. can anyone provide any hints and guides on how to do so?,,['complex-analysis']
89,is $g$ is a harmonic?harmonic polynomial?,is  is a harmonic?harmonic polynomial?,g,"Let $u$ be a real valued harmonic function on $\mathbb{C}$ and $$g:\mathbb{R}^2\to\mathbb{R},~~~~~g(x,y)=\int_{0}^{2\pi} u(e^{i\theta}(x+iy))\sin\theta d\theta$$ Then is $g$ is a harmonic?harmonic polynomial? Could any one help me how to proceed?Thank you.","Let $u$ be a real valued harmonic function on $\mathbb{C}$ and $$g:\mathbb{R}^2\to\mathbb{R},~~~~~g(x,y)=\int_{0}^{2\pi} u(e^{i\theta}(x+iy))\sin\theta d\theta$$ Then is $g$ is a harmonic?harmonic polynomial? Could any one help me how to proceed?Thank you.",,"['complex-analysis', 'harmonic-functions']"
90,"Integration of $z^{1/2}$ along a contour winding the origin twice, without introducing Riemann surface","Integration of  along a contour winding the origin twice, without introducing Riemann surface",z^{1/2},"If we introduce a Riemann surface, it is easy to show that the integral of $z^{1/2}$ along a contour winding the origin twice is zero. An anti-derivative exists everywhere, so the integral depends only on the end points of the contour. Thus, every closed contour in the surface gives zero. Is there any way to ubderstand this result only with branchcuts and without introducing multi-valued functions or Riemann surfaces?","If we introduce a Riemann surface, it is easy to show that the integral of $z^{1/2}$ along a contour winding the origin twice is zero. An anti-derivative exists everywhere, so the integral depends only on the end points of the contour. Thus, every closed contour in the surface gives zero. Is there any way to ubderstand this result only with branchcuts and without introducing multi-valued functions or Riemann surfaces?",,['complex-analysis']
91,Analytic function as a limit of polynomials,Analytic function as a limit of polynomials,,"Let $f$ be a continuous function on $\mathbb{T}=\{|z|=1\}$ so that   there exist a series of polynomials that converges to $f$ uniformly on   $\mathbb{T}$. Prove that there is a function $F$ that is continuos on   $\{|z|\leq 1\}$, analytic on $\{ |z|<1 \}$ and $F=f$ on  $\mathbb{T}$. I thought to define $F=\lim_{n\rightarrow \infty}P_n(z)$ but I dont know to explainwhy should it converge in $\{ |z|<1 \}$, and if it does, why need $\{ P_n \}$ converge uniformly in $\mathbb{T}$. Thanks.","Let $f$ be a continuous function on $\mathbb{T}=\{|z|=1\}$ so that   there exist a series of polynomials that converges to $f$ uniformly on   $\mathbb{T}$. Prove that there is a function $F$ that is continuos on   $\{|z|\leq 1\}$, analytic on $\{ |z|<1 \}$ and $F=f$ on  $\mathbb{T}$. I thought to define $F=\lim_{n\rightarrow \infty}P_n(z)$ but I dont know to explainwhy should it converge in $\{ |z|<1 \}$, and if it does, why need $\{ P_n \}$ converge uniformly in $\mathbb{T}$. Thanks.",,['complex-analysis']
92,The partial fraction expansion of $\frac{1}{\cos z}$,The partial fraction expansion of,\frac{1}{\cos z},"According to the Mittag-Leffler partial fraction expansion theorem for meromorphic functons, $$ \frac{1}{\cos z}= 1+\sum_{n=-\infty}^{\infty}(-1)^{n+1}\Big (\frac{1}{z-\frac{\pi}{2}-\pi n}+\frac{1}{\frac{\pi}{2}+\pi n}\Big). $$ But how does one rearrange terms (and justify the rearrangement) to show that  $$\frac{1}{\cos z}  = \sum_{n=0}^{\infty}(-1)^{n+1}\frac{(2n+1)\pi}{z^{2}-(\frac{2n+1}{2})^{2}\pi^2} \ ? $$ EDIT : An alternative version of the theorem (which requires showing that $\int \frac{\sec w}{w-z} \ dw$ vanishes around an appropriate contour) allows one to conclude that $$ \begin{align} \frac{1}{\cos z} &= \sum_{n=-\infty}^{\infty} \frac{(-1)^{n+1}}{z - \frac{\pi}{2}- \pi n} \\ &= \sum_{n=0}^{\infty} \frac{(-1)^{(-n-1)+1}}{z - \frac{\pi}{2} - \pi (-n-1)} + \sum_{n=0}^{\infty} \frac{(-1)^{n+1}}{z - \frac{\pi}{2} - \pi n} \\ &= \sum_{n=0}^{\infty} (-1)^{n+1} \left( \frac{-1}{z+ \frac{\pi}{2} + \pi n} + \frac{1}{z- \frac{\pi}{2} - \pi n} \right) \\ &= \sum_{n=0}^{\infty} (-1)^{n+1} \frac{(2n+1) \pi}{z^{2}- (\frac{2n+1}{2})^{2}\pi^{2}}. \end{align}$$","According to the Mittag-Leffler partial fraction expansion theorem for meromorphic functons, $$ \frac{1}{\cos z}= 1+\sum_{n=-\infty}^{\infty}(-1)^{n+1}\Big (\frac{1}{z-\frac{\pi}{2}-\pi n}+\frac{1}{\frac{\pi}{2}+\pi n}\Big). $$ But how does one rearrange terms (and justify the rearrangement) to show that  $$\frac{1}{\cos z}  = \sum_{n=0}^{\infty}(-1)^{n+1}\frac{(2n+1)\pi}{z^{2}-(\frac{2n+1}{2})^{2}\pi^2} \ ? $$ EDIT : An alternative version of the theorem (which requires showing that $\int \frac{\sec w}{w-z} \ dw$ vanishes around an appropriate contour) allows one to conclude that $$ \begin{align} \frac{1}{\cos z} &= \sum_{n=-\infty}^{\infty} \frac{(-1)^{n+1}}{z - \frac{\pi}{2}- \pi n} \\ &= \sum_{n=0}^{\infty} \frac{(-1)^{(-n-1)+1}}{z - \frac{\pi}{2} - \pi (-n-1)} + \sum_{n=0}^{\infty} \frac{(-1)^{n+1}}{z - \frac{\pi}{2} - \pi n} \\ &= \sum_{n=0}^{\infty} (-1)^{n+1} \left( \frac{-1}{z+ \frac{\pi}{2} + \pi n} + \frac{1}{z- \frac{\pi}{2} - \pi n} \right) \\ &= \sum_{n=0}^{\infty} (-1)^{n+1} \frac{(2n+1) \pi}{z^{2}- (\frac{2n+1}{2})^{2}\pi^{2}}. \end{align}$$",,"['sequences-and-series', 'complex-analysis']"
93,I don't understand this remark regarding Weierstrass' Theorem (Ahlfors' Complex Analysis),I don't understand this remark regarding Weierstrass' Theorem (Ahlfors' Complex Analysis),,"In Ahlfors' text of Complex Analysis, chapter 5 theorem 1, he proves the following: Theorem I.    Suppose that $f_n(z)$ is analytic in the region $\Omega_n$, and that the sequence $\{f_n(z)\}$ converges to a limit function $f(z)$ in a region $\Omega$, uniformly on every compact subset of $\Omega$. Then $f(z)$ is analytic in $\Omega$. Moreover, $f_n'(z)$ converges uniformly to $f' (z)$ on every compact subset of $\Omega$. In the proof he examines a closed disk $|z-a| \leq r$ which is contained in $\Omega$. He claims that from the assumptions of the theorem the disk lies in all $\Omega_n$ for $n>n_0$ for some $n_0$.( I think this is clear, due to the uniform convergence on compact subsets.) Under this sentence there is a footnote which states: In fact, the regions $\Omega_n$. form an open covering of $|z - a| \leq r$. The disk is compact and hence has a finite subcovering. This means that it is contained in a fixed $\Omega_{n_0}$. I don't see the need for this footnote: above we already saw that the disk lies in each of the domains $\Omega_{n_0+1},\Omega_{n_0+2}, \dots$, so clearly it lies within some domain of the $\Omega_n$'s. What am I doing wrong here? Thanks. EDIT: actually, I can't see how it that remark correct. Even if there is a finite subcovering of the disk $\{\Omega_{n_k} \}_{k=1}^N$, why should it lie in a single disk $\Omega_{n_0}$?","In Ahlfors' text of Complex Analysis, chapter 5 theorem 1, he proves the following: Theorem I.    Suppose that $f_n(z)$ is analytic in the region $\Omega_n$, and that the sequence $\{f_n(z)\}$ converges to a limit function $f(z)$ in a region $\Omega$, uniformly on every compact subset of $\Omega$. Then $f(z)$ is analytic in $\Omega$. Moreover, $f_n'(z)$ converges uniformly to $f' (z)$ on every compact subset of $\Omega$. In the proof he examines a closed disk $|z-a| \leq r$ which is contained in $\Omega$. He claims that from the assumptions of the theorem the disk lies in all $\Omega_n$ for $n>n_0$ for some $n_0$.( I think this is clear, due to the uniform convergence on compact subsets.) Under this sentence there is a footnote which states: In fact, the regions $\Omega_n$. form an open covering of $|z - a| \leq r$. The disk is compact and hence has a finite subcovering. This means that it is contained in a fixed $\Omega_{n_0}$. I don't see the need for this footnote: above we already saw that the disk lies in each of the domains $\Omega_{n_0+1},\Omega_{n_0+2}, \dots$, so clearly it lies within some domain of the $\Omega_n$'s. What am I doing wrong here? Thanks. EDIT: actually, I can't see how it that remark correct. Even if there is a finite subcovering of the disk $\{\Omega_{n_k} \}_{k=1}^N$, why should it lie in a single disk $\Omega_{n_0}$?",,"['sequences-and-series', 'complex-analysis', 'compactness', 'uniform-convergence']"
94,Proving that $\frac{1}{2\pi}\int_{0}^{2\pi}|f(re^{it})|^2dt=\sum_{n=0}^{\infty}|a_n|^2r^{2n}$,Proving that,\frac{1}{2\pi}\int_{0}^{2\pi}|f(re^{it})|^2dt=\sum_{n=0}^{\infty}|a_n|^2r^{2n},Let $f(z)=\sum_{n=0}^{\infty}a_nz^n$ with radius of convergence equals to $R$. Show that for every $r<R$: $$\frac{1}{2\pi}\int_{0}^{2\pi}|f(re^{it})|^2dt=\sum_{n=0}^{\infty}|a_n|^2r^{2n}$$ I tried this: $$\int_{0}^{2\pi}|f(re^{it})|^2dt=\int_{0}^{2\pi}f(re^{it})\overline{f(re^{it})}dt=\int_{0}^{2\pi}\sum_{n=0}^{\infty}a_nr^ne^{int}\sum_{n=0}^{\infty}\overline{a_n}r^ne^{-int}dt= \int_{0}^{2\pi}{\sum_{n=0}^{\infty}\sum_{k=0}^{n}}a_k\overline{a_{n-k}}r^ne^{int}$$ But this is about it. Any ideas? Thanks!,Let $f(z)=\sum_{n=0}^{\infty}a_nz^n$ with radius of convergence equals to $R$. Show that for every $r<R$: $$\frac{1}{2\pi}\int_{0}^{2\pi}|f(re^{it})|^2dt=\sum_{n=0}^{\infty}|a_n|^2r^{2n}$$ I tried this: $$\int_{0}^{2\pi}|f(re^{it})|^2dt=\int_{0}^{2\pi}f(re^{it})\overline{f(re^{it})}dt=\int_{0}^{2\pi}\sum_{n=0}^{\infty}a_nr^ne^{int}\sum_{n=0}^{\infty}\overline{a_n}r^ne^{-int}dt= \int_{0}^{2\pi}{\sum_{n=0}^{\infty}\sum_{k=0}^{n}}a_k\overline{a_{n-k}}r^ne^{int}$$ But this is about it. Any ideas? Thanks!,,['complex-analysis']
95,Winding number for Lipschitz curves,Winding number for Lipschitz curves,,"For a closed curve $\Gamma: [0,1] \to \mathbb{C}$ (i.e. $\Gamma(0) = \Gamma(1)$) let $$ \iota_\Gamma(z) := \frac{1}{2\pi i} \int_\Gamma \frac{dw}{z-w} = \frac{1}{2\pi i} \int_0^1 \frac{\Gamma'(t)}{z-\Gamma(t)}dt $$ denote its winding number around $z \in \mathbb{C}$. We know very well, that this is well defined not only for piecewise continously differentiable curves but also for Lipschitz curves (which are, of course, only absolutely continuous and hence only almost everywhere differentiable). By an exercise in Alfohrs' Complex Analysis book the notion of the winding number can even be extended to arbitrary continuous loops. Apart from that the standard literature only seems to mention further properties of the winding number for piecewise differentiable curves. I wonder which results still hold for Lipschitz curves. A standard result is, that the winding number takes integer values and is constant in each connected component of $\mathbb{C}\setminus im(\Gamma)$. (A proof for piecewise continuously differentiable functions can be found in Rudin's Complex Analysis, p. 203. I don't even see, why this argument wouldn't extend to Lipschitz loops, as well.) In general, I wonder about the continuity of $\Gamma \mapsto \iota_\Gamma(z)$ for fixed $z$ with respect to the sup norm. It seems intuitive to me, but I can't find a proof in the literature and was not able to prove it myself. Are there further properties of the winding number for Lipschitz loops? Can you point me to literature that covers winding number for Lipschitz or more general weakly differentiable (in the Sobolev sense) curves?","For a closed curve $\Gamma: [0,1] \to \mathbb{C}$ (i.e. $\Gamma(0) = \Gamma(1)$) let $$ \iota_\Gamma(z) := \frac{1}{2\pi i} \int_\Gamma \frac{dw}{z-w} = \frac{1}{2\pi i} \int_0^1 \frac{\Gamma'(t)}{z-\Gamma(t)}dt $$ denote its winding number around $z \in \mathbb{C}$. We know very well, that this is well defined not only for piecewise continously differentiable curves but also for Lipschitz curves (which are, of course, only absolutely continuous and hence only almost everywhere differentiable). By an exercise in Alfohrs' Complex Analysis book the notion of the winding number can even be extended to arbitrary continuous loops. Apart from that the standard literature only seems to mention further properties of the winding number for piecewise differentiable curves. I wonder which results still hold for Lipschitz curves. A standard result is, that the winding number takes integer values and is constant in each connected component of $\mathbb{C}\setminus im(\Gamma)$. (A proof for piecewise continuously differentiable functions can be found in Rudin's Complex Analysis, p. 203. I don't even see, why this argument wouldn't extend to Lipschitz loops, as well.) In general, I wonder about the continuity of $\Gamma \mapsto \iota_\Gamma(z)$ for fixed $z$ with respect to the sup norm. It seems intuitive to me, but I can't find a proof in the literature and was not able to prove it myself. Are there further properties of the winding number for Lipschitz loops? Can you point me to literature that covers winding number for Lipschitz or more general weakly differentiable (in the Sobolev sense) curves?",,"['complex-analysis', 'weak-derivatives', 'winding-number']"
96,"Which of these statements about biholomorphic functions $f \colon D(0, 1) → D(0, 1)$ is true?",Which of these statements about biholomorphic functions  is true?,"f \colon D(0, 1) → D(0, 1)","$f \colon D(0, 1) → D(0, 1)$ is a biholomorphic function. a) $f$ must be constant b) $f$ must have a fixed point c) $f$ must be a rotation d) $f$ must fix the origin. Any such map looks like $e^{i\alpha}{(z-a)\over (1-\bar{a}z)}$ , $a\in D$ , $\alpha\in [0,2\pi]$ , so only c is correct option?","is a biholomorphic function. a) must be constant b) must have a fixed point c) must be a rotation d) must fix the origin. Any such map looks like , , , so only c is correct option?","f \colon D(0, 1) → D(0, 1) f f f f e^{i\alpha}{(z-a)\over (1-\bar{a}z)} a\in D \alpha\in [0,2\pi]",['complex-analysis']
97,Entire function $f$ such that $|f(z)| ≤ K |z|^3$ for $|z|\ge1$ and $f(z) = f(iz)$ for every $z∈C$,Entire function  such that  for  and  for every,f |f(z)| ≤ K |z|^3 |z|\ge1 f(z) = f(iz) z∈C,"Let $f(z)$ be an entire function such that for some constant $K$ , $|f(z)| ≤ K$ $|z|^3<$ for $|z|\ge1$ and $f(z) = f(iz)$ $∀z∈C$ , then which of the following are correct ? (A)$ f(z) = Kz^3$ $∀ z∈\Bbb{C}$ (B) $f(z)$ is a constant function (C) $f(z)$ is quadratic function (D) No such $f$ exists My attempt : I can deduce that since $f(z) = f(iz)$ and since $i^4=1 \implies f$ must involve only fourth powers of $z$ . Hence $f(z)=a_1z^4+b_1z^8+c_1z^{16}+ \cdots $-------(1) Also for $|z| \ge 1 $: let C. Then $|g(z)| = |f(z)|/|z^3| \le k$ => By the Cauchy estimate theorem we can prove that :  $f(z)/z^3$ is a constant function $= c $ $\implies f(z)=cz^3$ for $|z| > 1$     ...........(2) From (1) and (2) ; we get  there does not exist a function like this ? My textbook answer says it's a constant function ?","Let $f(z)$ be an entire function such that for some constant $K$ , $|f(z)| ≤ K$ $|z|^3<$ for $|z|\ge1$ and $f(z) = f(iz)$ $∀z∈C$ , then which of the following are correct ? (A)$ f(z) = Kz^3$ $∀ z∈\Bbb{C}$ (B) $f(z)$ is a constant function (C) $f(z)$ is quadratic function (D) No such $f$ exists My attempt : I can deduce that since $f(z) = f(iz)$ and since $i^4=1 \implies f$ must involve only fourth powers of $z$ . Hence $f(z)=a_1z^4+b_1z^8+c_1z^{16}+ \cdots $-------(1) Also for $|z| \ge 1 $: let C. Then $|g(z)| = |f(z)|/|z^3| \le k$ => By the Cauchy estimate theorem we can prove that :  $f(z)/z^3$ is a constant function $= c $ $\implies f(z)=cz^3$ for $|z| > 1$     ...........(2) From (1) and (2) ; we get  there does not exist a function like this ? My textbook answer says it's a constant function ?",,"['complex-analysis', 'complex-numbers']"
98,Cauchy's Theorem- Trigonometric application,Cauchy's Theorem- Trigonometric application,,any help would be very much appreciated. The question asks to evaluate the given integral using Cauchy's formula. I plugged in the formulas for $\sin$ and $\cos$ ($\sin= \frac{1}{2i}(z-1/z)$ and $\cos= \frac12(z+1/z)$) but did not know how to proceed from there. $$\int_0^{2\pi} \frac{dθ}{3+\sinθ+\cosθ}$$ Thanks.,any help would be very much appreciated. The question asks to evaluate the given integral using Cauchy's formula. I plugged in the formulas for $\sin$ and $\cos$ ($\sin= \frac{1}{2i}(z-1/z)$ and $\cos= \frac12(z+1/z)$) but did not know how to proceed from there. $$\int_0^{2\pi} \frac{dθ}{3+\sinθ+\cosθ}$$ Thanks.,,['complex-analysis']
99,"When speaking of neighbourhoods in complex analysis, are we always referring to circular neighbourhoods?","When speaking of neighbourhoods in complex analysis, are we always referring to circular neighbourhoods?",,"In Complex Analysis, does ""neighbourhood"" automatically mean ""circular neighbourhood"", or do non-circular ones exist?","In Complex Analysis, does ""neighbourhood"" automatically mean ""circular neighbourhood"", or do non-circular ones exist?",,"['real-analysis', 'complex-analysis']"
