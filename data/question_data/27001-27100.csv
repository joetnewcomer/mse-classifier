,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rank of a rectangular Vandermonde Matrix to which weighted columns are added,Rank of a rectangular Vandermonde Matrix to which weighted columns are added,,"A Vandermonde matrix: $\left(\begin{array}{ccc} 1 & \alpha_{0} & \dots & \alpha_{0}^{n} \\ 1 & \alpha_{1} & \dots & \alpha_{1}^{n} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_{2n} & \dots & \alpha_{2n}^{n} \end{array}\right)$ has full rank $n+1$, provided $\alpha_{i}\neq \alpha_{j}$ for al $i\neq j$. Can we say that a matrix of the form: $\left(\begin{array}{cccccc} 1 & \alpha_{0} & \dots & \alpha_{0}^{n} & v_{0} & v_{0}\alpha_{0} & \dots & v_{0}\alpha_{0}^{n} \\ 1 & \alpha_{1} & \dots & \alpha_{1}^{n} & v_{1} & v_{1} \alpha_{1} & \dots & v_{1}\alpha_{1}^{n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_{2n} & \dots & \alpha_{2n}^{n} & v_{2n} & v_{2n} \alpha_{2n} & \dots & v_{2n}\alpha_{2n}^{n} \end{array}\right)$ where $v_{0}, \dots, v_{2n}$ are constants, $v_{i}\neq v_{j}$. will continue to have rank $(2n+1)$ ? Do any more constrains on $\{v_{i}\}$ need to be assumed ? I know that if we consider the above matrix as $(V | M)$ where $V$ is the initial Vandermonde matrix, both $V$ and $M$ will have rank $(2n+1)$. Edit: Sorry, I meant row rank , in all cases. The context is that I want to use the matrix $(V|M)$ to describe a solution for $2n+1$ variables $\{x_{0}, \dots, x_{2n}\}$. If this variable vector is $\vec{x}$, then I want to solve for $\vec{x}$, in: $(V|M)\cdot \vec{x} = 0$. Hence I wanted to know whether the rows of $(V|M)$ specify linearly independent constraints on $\{x_{i}\}$.","A Vandermonde matrix: $\left(\begin{array}{ccc} 1 & \alpha_{0} & \dots & \alpha_{0}^{n} \\ 1 & \alpha_{1} & \dots & \alpha_{1}^{n} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_{2n} & \dots & \alpha_{2n}^{n} \end{array}\right)$ has full rank $n+1$, provided $\alpha_{i}\neq \alpha_{j}$ for al $i\neq j$. Can we say that a matrix of the form: $\left(\begin{array}{cccccc} 1 & \alpha_{0} & \dots & \alpha_{0}^{n} & v_{0} & v_{0}\alpha_{0} & \dots & v_{0}\alpha_{0}^{n} \\ 1 & \alpha_{1} & \dots & \alpha_{1}^{n} & v_{1} & v_{1} \alpha_{1} & \dots & v_{1}\alpha_{1}^{n} \\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & \alpha_{2n} & \dots & \alpha_{2n}^{n} & v_{2n} & v_{2n} \alpha_{2n} & \dots & v_{2n}\alpha_{2n}^{n} \end{array}\right)$ where $v_{0}, \dots, v_{2n}$ are constants, $v_{i}\neq v_{j}$. will continue to have rank $(2n+1)$ ? Do any more constrains on $\{v_{i}\}$ need to be assumed ? I know that if we consider the above matrix as $(V | M)$ where $V$ is the initial Vandermonde matrix, both $V$ and $M$ will have rank $(2n+1)$. Edit: Sorry, I meant row rank , in all cases. The context is that I want to use the matrix $(V|M)$ to describe a solution for $2n+1$ variables $\{x_{0}, \dots, x_{2n}\}$. If this variable vector is $\vec{x}$, then I want to solve for $\vec{x}$, in: $(V|M)\cdot \vec{x} = 0$. Hence I wanted to know whether the rows of $(V|M)$ specify linearly independent constraints on $\{x_{i}\}$.",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
1,How to prove $\det(e^{\lambda_ix_j})\not=0$ where $\lambda_i\not=\lambda_j$ and $x_i\not=x_j$ if $i\not=j$,How to prove  where  and  if,\det(e^{\lambda_ix_j})\not=0 \lambda_i\not=\lambda_j x_i\not=x_j i\not=j,"In try to figure out the exercise: Let   $$f(x)=\sum_{k=1}^{n}c_ke^{\lambda_kx}$$where $\lambda_i \not=\lambda_j,i\not=j$,and $c_1^2+c_2^2+\dots+c_n^2\not=0$, then the number of $f(x)$'s roots is strictly less than $n$. My approach(this way can't deal with $f(x)$ has repeated root): assume $x_1,x_2,\dots,x_n$ are $f(x)$'s roots,and $x_i\not=x_j$ if $i\not=j$. then I get a linear equations about $c_1,c_2,\dots,c_n$: $$e^{\lambda_1x_1}c_1+e^{\lambda_2x_1}c_2+\dots+e^{\lambda_nx_1}c_n=0$$ $$e^{\lambda_1x_2}c_1+e^{\lambda_2x_2}c_2+\dots+e^{\lambda_nx_2}c_n=0$$ $$\dots\dots\dots\dots\dots\dots$$ $$e^{\lambda_1x_n}c_1+e^{\lambda_2x_n}c_2+\dots+e^{\lambda_nx_n}c_n=0$$ I want to show that the solution to this linear equations are $0$,it will be a contradiction.but i can't figure out its determinant of coefficient: $$\begin{vmatrix}  e^{\lambda_1x_1}& e^{\lambda_2x_1} &\dots &e^{\lambda_nx_1}\\   e^{\lambda_1x_2}& e^{\lambda_2x_2} &\dots&e^{\lambda_nx_2} \\   \dots&\dots  &\dots&\dots \\ e^{\lambda_1x_n}&e^{\lambda_2x_n} &\dots &e^{\lambda_nx_n} \end{vmatrix}\not=0$$","In try to figure out the exercise: Let   $$f(x)=\sum_{k=1}^{n}c_ke^{\lambda_kx}$$where $\lambda_i \not=\lambda_j,i\not=j$,and $c_1^2+c_2^2+\dots+c_n^2\not=0$, then the number of $f(x)$'s roots is strictly less than $n$. My approach(this way can't deal with $f(x)$ has repeated root): assume $x_1,x_2,\dots,x_n$ are $f(x)$'s roots,and $x_i\not=x_j$ if $i\not=j$. then I get a linear equations about $c_1,c_2,\dots,c_n$: $$e^{\lambda_1x_1}c_1+e^{\lambda_2x_1}c_2+\dots+e^{\lambda_nx_1}c_n=0$$ $$e^{\lambda_1x_2}c_1+e^{\lambda_2x_2}c_2+\dots+e^{\lambda_nx_2}c_n=0$$ $$\dots\dots\dots\dots\dots\dots$$ $$e^{\lambda_1x_n}c_1+e^{\lambda_2x_n}c_2+\dots+e^{\lambda_nx_n}c_n=0$$ I want to show that the solution to this linear equations are $0$,it will be a contradiction.but i can't figure out its determinant of coefficient: $$\begin{vmatrix}  e^{\lambda_1x_1}& e^{\lambda_2x_1} &\dots &e^{\lambda_nx_1}\\   e^{\lambda_1x_2}& e^{\lambda_2x_2} &\dots&e^{\lambda_nx_2} \\   \dots&\dots  &\dots&\dots \\ e^{\lambda_1x_n}&e^{\lambda_2x_n} &\dots &e^{\lambda_nx_n} \end{vmatrix}\not=0$$",,"['calculus', 'linear-algebra', 'determinant']"
2,Two matrices of complementary rank that sum to the identity have zero product.,Two matrices of complementary rank that sum to the identity have zero product.,,"Suppose $A$, $B$ are real $n \times n$ matrices with $A + B = I$ and $\operatorname{rank} (A) + \operatorname{rank} (B) = n$. How can one show that $AB = BA = 0$?","Suppose $A$, $B$ are real $n \times n$ matrices with $A + B = I$ and $\operatorname{rank} (A) + \operatorname{rank} (B) = n$. How can one show that $AB = BA = 0$?",,"['linear-algebra', 'matrices']"
3,Spectral radius and positive definite of matrices,Spectral radius and positive definite of matrices,,"Denote $ \rho(A)$ to be the spectral radius of a matrix $A,$ that is the maximal eigenvalue of $A.$ We say that a matrix $M$ is positive definite, respectively positive semidefinite, if $x^TMx>0$ and $ x^TMx \geq 0$ respectively for all vectors $x$ with nonzero entries. I want to show that if $ \rho(A)>1,$ then there exists a real symmetric matrix $B$ that is not positive semidefinite such that $A^TBA - B = -C$ holds for some positive definite matrix $C.$ Any hints or proof would be appreciated. I've shown the part where, if $ \rho(A)<1$ then for every positive definite matrix $C,$ $ A^TBA - B = -C$ has a unique solution $B$ that is also symmetric and positive definite. ;)","Denote $ \rho(A)$ to be the spectral radius of a matrix $A,$ that is the maximal eigenvalue of $A.$ We say that a matrix $M$ is positive definite, respectively positive semidefinite, if $x^TMx>0$ and $ x^TMx \geq 0$ respectively for all vectors $x$ with nonzero entries. I want to show that if $ \rho(A)>1,$ then there exists a real symmetric matrix $B$ that is not positive semidefinite such that $A^TBA - B = -C$ holds for some positive definite matrix $C.$ Any hints or proof would be appreciated. I've shown the part where, if $ \rho(A)<1$ then for every positive definite matrix $C,$ $ A^TBA - B = -C$ has a unique solution $B$ that is also symmetric and positive definite. ;)",,"['linear-algebra', 'matrices', 'spectral-radius']"
4,Gram-Schmidt and zero vector,Gram-Schmidt and zero vector,,"I have a problem concerning the orthogonalization of a coordinate system; this is necessary in the context of a normal mode analysis of molecular vibrations. I am working on H2O, giving me a 9-dimensional vector space, with six (orthogonal) basis vectors predetermined by describing rotational and translational motion of the entire molecule. I want to determine the three remaining vectors by a modified Gram-Schmidt process, but in my case, this somehow fails due to G-S constructing a zero vector. As far as I understand, zero vectors from Gram-Schmidt may occur if there is linear dependency somewhere in my set of vectors, but given that my six vectors are mutually orthogonal I don't know how this might be the case (let alone how I could avoid it). The six predetermined vectors are: trans-x   trans-y   trans-z   rot-xx    rot-yy    rot-zz 3.9994         0         0         0    0.2552         0      0    3.9994         0   -0.2552         0         0      0         0    3.9994         0         0         0 1.0039         0         0         0   -0.5084   -0.7839      0    1.0039         0    0.5084         0         0      0         0    1.0039    0.7839         0         0 1.0039         0         0         0   -0.5084    0.7839      0    1.0039         0    0.5084         0         0      0         0    1.0039   -0.7839         0         0 Can you see where the problem lies? I've been looking over this for a few days now, including trying alternative approaches at the orthogonalization problem, and I am starting to get frustrated. Given that my Gram-Schmidt algorithm produces a valid 9-dimensional orthogonal set if I use only the first three vectors (the translational coordinates), I assume my implementation to be correct and the problem to be somewhere in the rotational coordinate vectors. But I am at loss about what exactly is going wrong here. (In the end, it's probably just an example of not seeing the forest for the trees ...) Regards -M.","I have a problem concerning the orthogonalization of a coordinate system; this is necessary in the context of a normal mode analysis of molecular vibrations. I am working on H2O, giving me a 9-dimensional vector space, with six (orthogonal) basis vectors predetermined by describing rotational and translational motion of the entire molecule. I want to determine the three remaining vectors by a modified Gram-Schmidt process, but in my case, this somehow fails due to G-S constructing a zero vector. As far as I understand, zero vectors from Gram-Schmidt may occur if there is linear dependency somewhere in my set of vectors, but given that my six vectors are mutually orthogonal I don't know how this might be the case (let alone how I could avoid it). The six predetermined vectors are: trans-x   trans-y   trans-z   rot-xx    rot-yy    rot-zz 3.9994         0         0         0    0.2552         0      0    3.9994         0   -0.2552         0         0      0         0    3.9994         0         0         0 1.0039         0         0         0   -0.5084   -0.7839      0    1.0039         0    0.5084         0         0      0         0    1.0039    0.7839         0         0 1.0039         0         0         0   -0.5084    0.7839      0    1.0039         0    0.5084         0         0      0         0    1.0039   -0.7839         0         0 Can you see where the problem lies? I've been looking over this for a few days now, including trying alternative approaches at the orthogonalization problem, and I am starting to get frustrated. Given that my Gram-Schmidt algorithm produces a valid 9-dimensional orthogonal set if I use only the first three vectors (the translational coordinates), I assume my implementation to be correct and the problem to be somewhere in the rotational coordinate vectors. But I am at loss about what exactly is going wrong here. (In the end, it's probably just an example of not seeing the forest for the trees ...) Regards -M.",,"['linear-algebra', 'physics']"
5,Matrix with a prime $A'$,Matrix with a prime,A',"I can't find this over the Internet: what does a matrix with a prime $A'$ mean? I found this in an exercise, this is in basic linear algebra.","I can't find this over the Internet: what does a matrix with a prime $A'$ mean? I found this in an exercise, this is in basic linear algebra.",,"['linear-algebra', 'matrices', 'notation']"
6,"principal ""pseudo eigenvector"" of a real symmetric positive-semidefinite matrix","principal ""pseudo eigenvector"" of a real symmetric positive-semidefinite matrix",,"Let $A$ be a real symmetric positive-semidefinite matrix and suppose that $c>0$ is a sufficiently small number. I wonder if it is possible to solve the non-convex optimization  $$\arg\max_u\ u^\mathrm{T}Au\\ \mathrm{subject\ to\ }\left\Vert u\right\Vert_2\leq 1\\ \quad\quad\quad\quad\left\Vert u\right\Vert_1\leq c,$$ efficiently. For solving the optimization, I couldn't get farther than writing KKT conditions which do not help much in specifying the multipliers. Given that without the $\ell_1$-norm constraint (i.e. $c\to\infty$), the problem reduces to finding the principal eigenvector of $A$ that can be solved efficiently (e.g., using power iteration method), we can think of the solution to the optimization above as ""psuedo eigenvector"".","Let $A$ be a real symmetric positive-semidefinite matrix and suppose that $c>0$ is a sufficiently small number. I wonder if it is possible to solve the non-convex optimization  $$\arg\max_u\ u^\mathrm{T}Au\\ \mathrm{subject\ to\ }\left\Vert u\right\Vert_2\leq 1\\ \quad\quad\quad\quad\left\Vert u\right\Vert_1\leq c,$$ efficiently. For solving the optimization, I couldn't get farther than writing KKT conditions which do not help much in specifying the multipliers. Given that without the $\ell_1$-norm constraint (i.e. $c\to\infty$), the problem reduces to finding the principal eigenvector of $A$ that can be solved efficiently (e.g., using power iteration method), we can think of the solution to the optimization above as ""psuedo eigenvector"".",,"['linear-algebra', 'optimization', 'numerical-methods', 'numerical-linear-algebra']"
7,Eigenvalues of a special block matrix associated with strongly connected graph,Eigenvalues of a special block matrix associated with strongly connected graph,,"Let $G = (V, E, A)$ be a strongly connected directed graph, where $V = \{1, 2,\dots, n\}$ denotes the vertex set, $E$ is the edge set, and $A$ is the associated (binary) adjacency matrix with $a_{i,j} = 1$ if $(j,i) \in E$ , and $a_{i,j}=0$ otherwise. $B$ and $D$ are two diagonal matrices, where $b_{ii}=\sum_{j=1}^n a_{i,j}$ and $d_{ii}=\sum_{j=1}^na_{j,i}$ . In other words, the diagonal entries of $B$ are the row sums of $A$ , and the diagonal entries of $D$ are the column sums of $A$ . Let $$ M := \left[\begin{array}{c|c} B-A & -A \\ \hline A-B & D \end{array}\right]$$ Since the column sum of $M$ are identical zeros, zero must be one of its eigenvalue. Can I claim that the rest eigenvalues all have positive real parts? I tried many numerical examples, the rest eigenvalues have positive real parts. Anyone can help prove or disprove it? The Gershgorin Circle Theorem does not apply here because $M$ is not diagonally dominant.","Let be a strongly connected directed graph, where denotes the vertex set, is the edge set, and is the associated (binary) adjacency matrix with if , and otherwise. and are two diagonal matrices, where and . In other words, the diagonal entries of are the row sums of , and the diagonal entries of are the column sums of . Let Since the column sum of are identical zeros, zero must be one of its eigenvalue. Can I claim that the rest eigenvalues all have positive real parts? I tried many numerical examples, the rest eigenvalues have positive real parts. Anyone can help prove or disprove it? The Gershgorin Circle Theorem does not apply here because is not diagonally dominant.","G = (V, E, A) V = \{1, 2,\dots, n\} E A a_{i,j} = 1 (j,i) \in E a_{i,j}=0 B D b_{ii}=\sum_{j=1}^n a_{i,j} d_{ii}=\sum_{j=1}^na_{j,i} B A D A  M := \left[\begin{array}{c|c} B-A & -A \\ \hline A-B & D \end{array}\right] M M","['linear-algebra', 'matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'block-matrices']"
8,"Finite dimensional function space ""different"" from $\mathbb{R}^n$ generically","Finite dimensional function space ""different"" from  generically",\mathbb{R}^n,"If you pick a random vector in $\mathbb{R}^n$ with some fixed basis, there is no special relationship between components. The relationship between the $1^{st}$ component and the $5^{th}$ component is the same as the relationship between the $82^{nd}$ component and the $1001^{th}$ component. On the other hand, if the space $\mathbb{R}^n$ is viewed as a discretization of a function space (eg, n nodal values for a piecewise linear basis of hat functions), then there is a special relationship between components based on nearness in the underlying domain. If 2 nodes are close in physical space, then the basis vectors corresponding to those nodes are more highly related in the function space. So, somehow $\mathbb{R}^n$ as a function space has more structure and is different than $\mathbb{R}^n$ generically. What is this difference and how can it be made precise? My thoughts so far are as follows: this seems similar to the ideas of function space regularity (the more regular the space, the more nearby points are ""related"" to each other). However I don't think this is the whole picture since one could also imagine defining additional structure on the function space over nodes in a n-node graph $\{f:G\rightarrow\mathbb{R}\}$, where there is no notion of continuity, differentiability, etc.","If you pick a random vector in $\mathbb{R}^n$ with some fixed basis, there is no special relationship between components. The relationship between the $1^{st}$ component and the $5^{th}$ component is the same as the relationship between the $82^{nd}$ component and the $1001^{th}$ component. On the other hand, if the space $\mathbb{R}^n$ is viewed as a discretization of a function space (eg, n nodal values for a piecewise linear basis of hat functions), then there is a special relationship between components based on nearness in the underlying domain. If 2 nodes are close in physical space, then the basis vectors corresponding to those nodes are more highly related in the function space. So, somehow $\mathbb{R}^n$ as a function space has more structure and is different than $\mathbb{R}^n$ generically. What is this difference and how can it be made precise? My thoughts so far are as follows: this seems similar to the ideas of function space regularity (the more regular the space, the more nearby points are ""related"" to each other). However I don't think this is the whole picture since one could also imagine defining additional structure on the function space over nodes in a n-node graph $\{f:G\rightarrow\mathbb{R}\}$, where there is no notion of continuity, differentiability, etc.",,"['linear-algebra', 'functional-analysis', 'graph-theory']"
9,An Expression for the Wedge Product,An Expression for the Wedge Product,,"For the question below, I have the following definitions and concepts in mind: The $k^{th}$ exterior power of a real vector space $V$, denoted $\Lambda^k(V)$ can be realized as the quotient of the tensor product $\bigotimes^k V$ with the subspace of $\bigotimes^k V$ generated by all elements of the form $v_1 \otimes \dots \otimes v_k$ where $v_i = v_j$ for some $i \neq j$. The equivalence class of $(v_1, \dots, v_k)$ in $\Lambda^k(V)$ is denoted by $v_1 \wedge \cdots \wedge v_k$; it can be thought of as the image of $(v_1, \dots, v_k)$ under the canonical alternating multilinear map that sends each element in $V^k$ to its equivalence class in $\Lambda^k(V)$. By the universal property of the exterior product, I understand that every alternating multilinear form defined on $V^k$ can identified with a unique linear form with domain $\Lambda^k(V)$. Finally, I know that the determinant of an endomorphism $T:V\rightarrow V$ can be defined as the unique real number $\det T$ such that  $$  Tv_1 \wedge \cdots \wedge Tv_n = (\det T)v_1 \wedge \cdots \wedge v_n $$ My Question : It is a fact that if $\phi^i, \dots, \phi^k$ are linear forms on $V$ then $$ \phi^1 \wedge \cdots \wedge \phi^k(v_1, \dots, v_k) = \det[\phi^i(v_j)] $$ Can this be proved using the facts outlined above without resorting to the combinatorial definition of the determinant/wedge product?","For the question below, I have the following definitions and concepts in mind: The $k^{th}$ exterior power of a real vector space $V$, denoted $\Lambda^k(V)$ can be realized as the quotient of the tensor product $\bigotimes^k V$ with the subspace of $\bigotimes^k V$ generated by all elements of the form $v_1 \otimes \dots \otimes v_k$ where $v_i = v_j$ for some $i \neq j$. The equivalence class of $(v_1, \dots, v_k)$ in $\Lambda^k(V)$ is denoted by $v_1 \wedge \cdots \wedge v_k$; it can be thought of as the image of $(v_1, \dots, v_k)$ under the canonical alternating multilinear map that sends each element in $V^k$ to its equivalence class in $\Lambda^k(V)$. By the universal property of the exterior product, I understand that every alternating multilinear form defined on $V^k$ can identified with a unique linear form with domain $\Lambda^k(V)$. Finally, I know that the determinant of an endomorphism $T:V\rightarrow V$ can be defined as the unique real number $\det T$ such that  $$  Tv_1 \wedge \cdots \wedge Tv_n = (\det T)v_1 \wedge \cdots \wedge v_n $$ My Question : It is a fact that if $\phi^i, \dots, \phi^k$ are linear forms on $V$ then $$ \phi^1 \wedge \cdots \wedge \phi^k(v_1, \dots, v_k) = \det[\phi^i(v_j)] $$ Can this be proved using the facts outlined above without resorting to the combinatorial definition of the determinant/wedge product?",,"['linear-algebra', 'multilinear-algebra']"
10,Rank of matrices of the form $M^\mathrm{T}M$.,Rank of matrices of the form .,M^\mathrm{T}M,"The following is a problem from Daniel Norman's Introduction to Linear Algebra . Consider a $m\times n$ matrix, $M$, with rank $r$. We wish to show that the matrix $M^\mathrm{T}M$ also has rank $r$. Now, I can show this fact by proving that the nullspace of $M^\mathrm{T}M$ is the same as the nullspace of $M$ whereby the rank-nullity theorem will take care of the rest, but the hint of the problem proposes something confusing to me. The hint suggests looking at the matrix $N$ obtained by exchanging two columns of $M$. It then asks ""What is the relationship between $N^\mathrm{T}N$ and $M^\mathrm{T}M$?"" I can see that the two matrices are orthogonally similar, more specifically I can see that $$E^\mathrm{T}M^\mathrm{T}ME = N^\mathrm{T}N$$  where $E = E^\mathrm{T}$ is an elementary matrix exchanging the columns in question. But I fail to see anything which leads to a resolution of this problem. Can anyone offer some help?","The following is a problem from Daniel Norman's Introduction to Linear Algebra . Consider a $m\times n$ matrix, $M$, with rank $r$. We wish to show that the matrix $M^\mathrm{T}M$ also has rank $r$. Now, I can show this fact by proving that the nullspace of $M^\mathrm{T}M$ is the same as the nullspace of $M$ whereby the rank-nullity theorem will take care of the rest, but the hint of the problem proposes something confusing to me. The hint suggests looking at the matrix $N$ obtained by exchanging two columns of $M$. It then asks ""What is the relationship between $N^\mathrm{T}N$ and $M^\mathrm{T}M$?"" I can see that the two matrices are orthogonally similar, more specifically I can see that $$E^\mathrm{T}M^\mathrm{T}ME = N^\mathrm{T}N$$  where $E = E^\mathrm{T}$ is an elementary matrix exchanging the columns in question. But I fail to see anything which leads to a resolution of this problem. Can anyone offer some help?",,"['linear-algebra', 'matrices']"
11,Matrix over $\mathbb Z[X]$ which is not equivalent to a diagonal matrix,Matrix over  which is not equivalent to a diagonal matrix,\mathbb Z[X],"How can I find $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ , where $a,b,c,d \in \mathbb Z[x]$ , such that there does not exist invertible matrices $B, C \in M_2(\mathbb Z[x])$ such that $B^{-1}AC$ is diagonal?","How can I find , where , such that there does not exist invertible matrices such that is diagonal?","A=\begin{pmatrix} a & b \\ c & d \end{pmatrix} a,b,c,d \in \mathbb Z[x] B, C \in M_2(\mathbb Z[x]) B^{-1}AC","['linear-algebra', 'matrices', 'ring-theory']"
12,How would changing of elements of matrix(from zeros to non-zeros) affect its rank.,How would changing of elements of matrix(from zeros to non-zeros) affect its rank.,,"Given a block diagonal matrix of the form $$\left[\begin{array}{ccc}A_1 & & O\\ & \ddots & \\ O & & A_L\end{array}\right]$$ where $A_1,\ldots,A_L$ are matrices of size $K\times K$ , the rank of it is $R\leq KL$ , I randomly pick up one zero from the off-block-diagonal part and change that to a non-zero element. How would that affect the rank of this matrix? Would it decrease, increase, non-increase, non-decrease, stay equal? Intuitively, it looks like non-decreasing, but how to give a mathematical proof on that? Thanks.","Given a block diagonal matrix of the form where are matrices of size , the rank of it is , I randomly pick up one zero from the off-block-diagonal part and change that to a non-zero element. How would that affect the rank of this matrix? Would it decrease, increase, non-increase, non-decrease, stay equal? Intuitively, it looks like non-decreasing, but how to give a mathematical proof on that? Thanks.","\left[\begin{array}{ccc}A_1 & & O\\ & \ddots & \\ O & & A_L\end{array}\right] A_1,\ldots,A_L K\times K R\leq KL","['linear-algebra', 'matrices', 'matrix-rank', 'block-matrices']"
13,Extensions of finite-rank operators,Extensions of finite-rank operators,,Let $V$ be a vector space and let $W$ be its subspace of infinite codimension. Let $\mathcal{F}_W$ be the family of all finite-rank operators on $V$ with range contained in $W$. Consider the left-sided ideal in $\mathcal{L}(V)$ generated by $\mathcal{F}_W$. Does this ideal contain all the finite-rank operators on $V$?,Let $V$ be a vector space and let $W$ be its subspace of infinite codimension. Let $\mathcal{F}_W$ be the family of all finite-rank operators on $V$ with range contained in $W$. Consider the left-sided ideal in $\mathcal{L}(V)$ generated by $\mathcal{F}_W$. Does this ideal contain all the finite-rank operators on $V$?,,"['linear-algebra', 'vector-spaces', 'operator-theory']"
14,Effect the zero vector has on the dimension of affine hulls and linear hulls,Effect the zero vector has on the dimension of affine hulls and linear hulls,,"I am currently working through ""An Introduction to Convex Polytopes"" by Arne Brondsted and there is a question in the exercises that I would like a hint, or a nudge in the right direction, please no full solutions (yet)! The question is as follows, For any subset $M$ of $\mathbb{R}^d$, show that $\dim(\text{aff M}) = \dim(\text{span M})$ when $\textbf{0} \in \text{aff M}$, and $\dim(\text{aff M}) = \dim(\text{span M}) - 1$ when $\textbf{0} \notin \text{aff M}$. My general approach thus far has been to try and interpret the dimension of the affine hull aff $M$ in terms of the affine basis of $M$ and the linear hull span $M$ in terms of the linear basis of $M$. I tried to prove a lemma Let $L=(x_{1},...,x_{n})$ be the linear basis of $M$ and let $A=(x_{1},...,x_{k})$ be the affine basis of $M$. For any $M \subseteq \mathbb{R}^d$, $\dim(L)=\dim(\text{span M})=n$ and $\dim(A)=\dim(\text{aff M})=k-1$. I'm fairly certain that it is true, but I'm having trouble coming up with a rigorous proof. In any case, taking that lemma to be true I was able to come up with following attempt at a proof, We are guaranteed that there exists a linearly independent n-family $(x_{1},...,x_{n})$ of vectors from $M$ such that $\text{span M}$ is the set of all linear combinations $\sum_{i=1}^{n} \lambda_{i}x_{i}$; and that there exists an affinely independent k-family of $(x_{1},...,x_{k})$ of points from $M$ such that $\text{aff M}$ is the set of all linear combinations $\sum_{i=1}^{k} \lambda_{i} x_{i}$, with $\sum_{i=1}^{k} \lambda_{i} = 1$. This is equivalent to saying that for any $M \subseteq \mathbb{R}^d$, there exists a linear basis $L=(x_{1},...,x_{n})$ of $M$ and there exists an affine basis $A=(x_{1},...,x_{k})$ of $M$. We show that when $\textbf{0} \in \text{aff M}$ that $\dim(\text{aff M}) = \dim(\text{span M})$ and that when $\textbf{0} \notin \text{aff M}$ that $\dim(A) = \dim(L)$. By the lemma, this is equivalent to saying when $\textbf{0} \in \text{aff M}$ that $\dim(A)=\dim(L)$ and that when $\textbf{0} \notin \text{aff M}$ that $\dim(A)= \dim(L) -1$.   Assume that for an arbitrary $M \subseteq \mathbb{R}^d$ that $\textbf{0} \in \text{aff M}$. We want to prove that $\dim(A)=\dim(L)$. Since $A=(x_{1},...,x_{k})$ is an affine basis of $M$, $A$ has dimension $k-1$ and since $L=(x_{1},...,x_{n})$ is a linear basis of $M$, $L$ has dimension $n$. We show that $k-1=n$. Since $\textbf{0} \in \text{aff M}$, then some affine combination from $M$ is equal to the zero vector. So, for $\lambda_{1}+...+\lambda_{k}=1$ we have that,    \begin{equation}   \sum_{i=1}^{k} \lambda_{i} x_{i} =\sum_{i=1}^{k} \lambda_{i} \cdot \sum_{i=1}^{k} x_{i} = \sum_{i=1}^{k} x_{i} = 0   \end{equation}   [From here somehow relate this to a property of linear dependence or something else that shows $k-1=n$].   Assume that for an arbitrary $M \subseteq \mathbb{R}^d$ that $\textbf{0} \notin \text{aff M}$. We want to prove that $\dim(A)=\dim(L)-1$, which is that $k-1 = n-1$, so $k=n$. [Try a similar approach to the first part if it ends up working and show that $k=n$].   Therefore, $\dim(\text{aff M}) = \dim(\text{span M})$ when $\textbf{0} \in \text{aff M}$, and $\dim(\text{aff M}) = \dim(\text{span M}) - 1$ when $\textbf{0} \notin \text{aff M}$. I would appreciate some suggestions that have a fair bit of detail, but nothing like a full solution please. If you happen to be able to rip off a quick proof of the lemma I'd like to see that, and if it is wrong or useless please tell me! EDIT: I realize my other approach was a bit off. From the suggestion Robert gave I was able to construct the outline of the proof (I just need to actually show that either $B$ or $B \cup \textbf{0}$ is an affine basis of aff $M$ depending on the condition) as follows, Let $M$ be an arbitrary subset of $\mathbb{R}^d$. We want to show that if $\textbf{0} \in \text{aff M}$, then $\dim(\text{aff M}) = \dim(\text{span M})$ and that if $\textbf{0} \notin \text{aff M}$, then $\dim(\text{aff M}) = \dim(\text{span M}) - 1$. Since aff $M$ is an affine subspace of $\mathbb{R}^d$, then for an affine basis $A=(x_{1},...,x_{n})$ of aff $M$, $\dim(A)=\dim(\text{aff M})=n-1$. Similarly, since span $M$ is a linear subspace of $\mathbb{R}^d$, then for a linear basis $L=(x_{1},...,x_{n})$ of span $M$, $\dim(L)=\dim(\text{span M})=n$. So, we equivalently show for a linear basis $B$ of span $M$ that if $\textbf{0} \in \text{aff M}$, then $B \cup \{\textbf{0}\}$ is an affine basis for aff $M$ and that if $\textbf{0} \notin \text{aff M}$, then $B$ is an affine basis for aff $M$.   Assume that $\textbf{0} \in \text{aff M}$ and that $B=(x_{1},...,x_{n})$ is a linear basis of span $M$. We want to prove that $B \cup \{\textbf{0}\}$ is an affine basis of aff $M$, since that would show $\dim(B \cup \{\textbf{0}\})=\dim(\text{aff M})=n$. Since $B$ is a linear basis of span $M$, then it would show that $\dim(B)=\dim(\text{span M})=n$, so we would have that $\dim(\text{aff M}) = \dim(\text{span M})$. [Show here that $B \cup \{\textbf{0}\}$ is an affine basis of aff $M$] .   Assume that $\textbf{0} \notin \text{aff M}$ and that $B=(x_{1},...,x_{n})$ is a linear basis of span $M$. We want to prove that $B$ is an affine basis of aff $M$, since that would show $\dim(B)=\dim(\text{aff M})=n-1$. Since $B$ is a linear basis of span $M$, then it would show that $\dim(B)=\dim(\text{span M})=n$, so we would have that $\dim(\text{aff M})=\dim(\text{span M}) -1$. [Show here that $B$ is an affine basis of aff $M$] .   Therefore, for any subset $M$ of $\mathbb{R}^d$, if $\textbf{0} \in \text{aff M}$ then $\dim(\text{aff M}) = \dim(\text{span M})$, and if  $\textbf{0} \notin \text{aff M}$ then $\dim(\text{aff M}) = \dim(\text{span M}) - 1$. Given that I can prove the condition for the affine basis in each case, would this be a complete proof? I'm still working on showing the condition, but I want to know if the construction of the proof is correct. Thanks! SECOND EDIT: I have posted an attempted proof as an answer, please comment on it and let me know if it is correct.","I am currently working through ""An Introduction to Convex Polytopes"" by Arne Brondsted and there is a question in the exercises that I would like a hint, or a nudge in the right direction, please no full solutions (yet)! The question is as follows, For any subset $M$ of $\mathbb{R}^d$, show that $\dim(\text{aff M}) = \dim(\text{span M})$ when $\textbf{0} \in \text{aff M}$, and $\dim(\text{aff M}) = \dim(\text{span M}) - 1$ when $\textbf{0} \notin \text{aff M}$. My general approach thus far has been to try and interpret the dimension of the affine hull aff $M$ in terms of the affine basis of $M$ and the linear hull span $M$ in terms of the linear basis of $M$. I tried to prove a lemma Let $L=(x_{1},...,x_{n})$ be the linear basis of $M$ and let $A=(x_{1},...,x_{k})$ be the affine basis of $M$. For any $M \subseteq \mathbb{R}^d$, $\dim(L)=\dim(\text{span M})=n$ and $\dim(A)=\dim(\text{aff M})=k-1$. I'm fairly certain that it is true, but I'm having trouble coming up with a rigorous proof. In any case, taking that lemma to be true I was able to come up with following attempt at a proof, We are guaranteed that there exists a linearly independent n-family $(x_{1},...,x_{n})$ of vectors from $M$ such that $\text{span M}$ is the set of all linear combinations $\sum_{i=1}^{n} \lambda_{i}x_{i}$; and that there exists an affinely independent k-family of $(x_{1},...,x_{k})$ of points from $M$ such that $\text{aff M}$ is the set of all linear combinations $\sum_{i=1}^{k} \lambda_{i} x_{i}$, with $\sum_{i=1}^{k} \lambda_{i} = 1$. This is equivalent to saying that for any $M \subseteq \mathbb{R}^d$, there exists a linear basis $L=(x_{1},...,x_{n})$ of $M$ and there exists an affine basis $A=(x_{1},...,x_{k})$ of $M$. We show that when $\textbf{0} \in \text{aff M}$ that $\dim(\text{aff M}) = \dim(\text{span M})$ and that when $\textbf{0} \notin \text{aff M}$ that $\dim(A) = \dim(L)$. By the lemma, this is equivalent to saying when $\textbf{0} \in \text{aff M}$ that $\dim(A)=\dim(L)$ and that when $\textbf{0} \notin \text{aff M}$ that $\dim(A)= \dim(L) -1$.   Assume that for an arbitrary $M \subseteq \mathbb{R}^d$ that $\textbf{0} \in \text{aff M}$. We want to prove that $\dim(A)=\dim(L)$. Since $A=(x_{1},...,x_{k})$ is an affine basis of $M$, $A$ has dimension $k-1$ and since $L=(x_{1},...,x_{n})$ is a linear basis of $M$, $L$ has dimension $n$. We show that $k-1=n$. Since $\textbf{0} \in \text{aff M}$, then some affine combination from $M$ is equal to the zero vector. So, for $\lambda_{1}+...+\lambda_{k}=1$ we have that,    \begin{equation}   \sum_{i=1}^{k} \lambda_{i} x_{i} =\sum_{i=1}^{k} \lambda_{i} \cdot \sum_{i=1}^{k} x_{i} = \sum_{i=1}^{k} x_{i} = 0   \end{equation}   [From here somehow relate this to a property of linear dependence or something else that shows $k-1=n$].   Assume that for an arbitrary $M \subseteq \mathbb{R}^d$ that $\textbf{0} \notin \text{aff M}$. We want to prove that $\dim(A)=\dim(L)-1$, which is that $k-1 = n-1$, so $k=n$. [Try a similar approach to the first part if it ends up working and show that $k=n$].   Therefore, $\dim(\text{aff M}) = \dim(\text{span M})$ when $\textbf{0} \in \text{aff M}$, and $\dim(\text{aff M}) = \dim(\text{span M}) - 1$ when $\textbf{0} \notin \text{aff M}$. I would appreciate some suggestions that have a fair bit of detail, but nothing like a full solution please. If you happen to be able to rip off a quick proof of the lemma I'd like to see that, and if it is wrong or useless please tell me! EDIT: I realize my other approach was a bit off. From the suggestion Robert gave I was able to construct the outline of the proof (I just need to actually show that either $B$ or $B \cup \textbf{0}$ is an affine basis of aff $M$ depending on the condition) as follows, Let $M$ be an arbitrary subset of $\mathbb{R}^d$. We want to show that if $\textbf{0} \in \text{aff M}$, then $\dim(\text{aff M}) = \dim(\text{span M})$ and that if $\textbf{0} \notin \text{aff M}$, then $\dim(\text{aff M}) = \dim(\text{span M}) - 1$. Since aff $M$ is an affine subspace of $\mathbb{R}^d$, then for an affine basis $A=(x_{1},...,x_{n})$ of aff $M$, $\dim(A)=\dim(\text{aff M})=n-1$. Similarly, since span $M$ is a linear subspace of $\mathbb{R}^d$, then for a linear basis $L=(x_{1},...,x_{n})$ of span $M$, $\dim(L)=\dim(\text{span M})=n$. So, we equivalently show for a linear basis $B$ of span $M$ that if $\textbf{0} \in \text{aff M}$, then $B \cup \{\textbf{0}\}$ is an affine basis for aff $M$ and that if $\textbf{0} \notin \text{aff M}$, then $B$ is an affine basis for aff $M$.   Assume that $\textbf{0} \in \text{aff M}$ and that $B=(x_{1},...,x_{n})$ is a linear basis of span $M$. We want to prove that $B \cup \{\textbf{0}\}$ is an affine basis of aff $M$, since that would show $\dim(B \cup \{\textbf{0}\})=\dim(\text{aff M})=n$. Since $B$ is a linear basis of span $M$, then it would show that $\dim(B)=\dim(\text{span M})=n$, so we would have that $\dim(\text{aff M}) = \dim(\text{span M})$. [Show here that $B \cup \{\textbf{0}\}$ is an affine basis of aff $M$] .   Assume that $\textbf{0} \notin \text{aff M}$ and that $B=(x_{1},...,x_{n})$ is a linear basis of span $M$. We want to prove that $B$ is an affine basis of aff $M$, since that would show $\dim(B)=\dim(\text{aff M})=n-1$. Since $B$ is a linear basis of span $M$, then it would show that $\dim(B)=\dim(\text{span M})=n$, so we would have that $\dim(\text{aff M})=\dim(\text{span M}) -1$. [Show here that $B$ is an affine basis of aff $M$] .   Therefore, for any subset $M$ of $\mathbb{R}^d$, if $\textbf{0} \in \text{aff M}$ then $\dim(\text{aff M}) = \dim(\text{span M})$, and if  $\textbf{0} \notin \text{aff M}$ then $\dim(\text{aff M}) = \dim(\text{span M}) - 1$. Given that I can prove the condition for the affine basis in each case, would this be a complete proof? I'm still working on showing the condition, but I want to know if the construction of the proof is correct. Thanks! SECOND EDIT: I have posted an attempted proof as an answer, please comment on it and let me know if it is correct.",,"['linear-algebra', 'convex-analysis', 'affine-geometry']"
15,Cauchy interlacing theorem - how to complete this proof,Cauchy interlacing theorem - how to complete this proof,,"I am looking for a proof using the min-max principle. Wikipedia seem to provide just that: http://en.wikipedia.org/wiki/Min-max_theorem#Cauchy_interlacing_theorem But this part seems to be wrong: This can be proven using the min-max principle. Let $\beta_i$ have corresponding eigenvector $b_i$ and $S_j$ be the $j$ dimensional subspace $S_j=\operatorname{span}\{b_1,\dots, b_j\}$, then    $$ \beta_j = \max_{x\in S_j,\|x\|=1}(Bx,x) =\max_{x\in S_j,\|x\|=1}(PAPx,x) =\max_{x\in S_j,\|x\|=1}(Ax,x)$$ How is the shift from $PAPx$ to $Ax$ legal? $PAP$ is an $m\times m$ matrix while $A$ is an $n\times n$ matrix. $x$ can't fit both. Can anyone correct the proof?","I am looking for a proof using the min-max principle. Wikipedia seem to provide just that: http://en.wikipedia.org/wiki/Min-max_theorem#Cauchy_interlacing_theorem But this part seems to be wrong: This can be proven using the min-max principle. Let $\beta_i$ have corresponding eigenvector $b_i$ and $S_j$ be the $j$ dimensional subspace $S_j=\operatorname{span}\{b_1,\dots, b_j\}$, then    $$ \beta_j = \max_{x\in S_j,\|x\|=1}(Bx,x) =\max_{x\in S_j,\|x\|=1}(PAPx,x) =\max_{x\in S_j,\|x\|=1}(Ax,x)$$ How is the shift from $PAPx$ to $Ax$ legal? $PAP$ is an $m\times m$ matrix while $A$ is an $n\times n$ matrix. $x$ can't fit both. Can anyone correct the proof?",,"['linear-algebra', 'matrices', 'functional-analysis']"
16,Geometry problem from a Berkeley course,Geometry problem from a Berkeley course,,"I've been trying to solve this problem proposed as part of one of the first lectures of a Berkeley linear algebra course: ""What Good is a Basis ? The freedom to choose a basis often simplifies calculations and proofs. For instance, here is a phenomenon first noticed by G. Desargues (1593 - 1662), a contemporary of R. Descartes: In the plane, fix two intersecting straight lines B and C and a point p on neither. Through p draw two straight lines X and Y that intersect B and C in four points all told. Two pairs of those points are not yet joined by straight lines; draw those lines now and, if they intersect, call their intersection q . As X and Y move, always passing through p , so does q move; show that it moves along some fixed straight line D . To prove the existence of D takes some ingenuity if none but the methods of Euclidean plane geometry may be used; and if rectangular Cartesian coordinates must be used the proof is a tedious computation. But a relatively short computation suffices if we choose an apt basis. Test these claims by trying to verify Desargues’ observation above using only the ideas you learned in High-School. Then you will be better able to appreciate the strategy motivating vector notation and its algebra used in the following proof."" The construction suggested reminds me of concepts like Ceva's theorem, complete quadrangle, internal/external division, which I learned about in Geometria Moderna (Modern Geometry required subject for math majors at UNAM Mexico). Sadly, though, I can't think of a way to use any of those concepts to solve the problem. Suggestions, please!","I've been trying to solve this problem proposed as part of one of the first lectures of a Berkeley linear algebra course: ""What Good is a Basis ? The freedom to choose a basis often simplifies calculations and proofs. For instance, here is a phenomenon first noticed by G. Desargues (1593 - 1662), a contemporary of R. Descartes: In the plane, fix two intersecting straight lines B and C and a point p on neither. Through p draw two straight lines X and Y that intersect B and C in four points all told. Two pairs of those points are not yet joined by straight lines; draw those lines now and, if they intersect, call their intersection q . As X and Y move, always passing through p , so does q move; show that it moves along some fixed straight line D . To prove the existence of D takes some ingenuity if none but the methods of Euclidean plane geometry may be used; and if rectangular Cartesian coordinates must be used the proof is a tedious computation. But a relatively short computation suffices if we choose an apt basis. Test these claims by trying to verify Desargues’ observation above using only the ideas you learned in High-School. Then you will be better able to appreciate the strategy motivating vector notation and its algebra used in the following proof."" The construction suggested reminds me of concepts like Ceva's theorem, complete quadrangle, internal/external division, which I learned about in Geometria Moderna (Modern Geometry required subject for math majors at UNAM Mexico). Sadly, though, I can't think of a way to use any of those concepts to solve the problem. Suggestions, please!",,"['linear-algebra', 'geometry', 'vector-spaces']"
17,"Isomorphism between $\operatorname{Hom}_{\operatorname{End}(V)}(V,W) \otimes V$ and $W$",Isomorphism between  and,"\operatorname{Hom}_{\operatorname{End}(V)}(V,W) \otimes V W","Let $V$ be a vector space over $\mathbb C$ and $W$ a $\operatorname{End}(V)$-module.  I'm having difficulty seeing why the map $$ \operatorname{Hom}_{\operatorname{End}(V)}(V,W) \otimes V \to W $$ $$ \phi \otimes v \mapsto \phi(v) $$ is surjective.  I'm having trouble because I can't construct elements of $\operatorname{Hom}_{\operatorname{End}(V)} (V,W)$.  It seems hard because I know that the above map is injective which means showing surjectiveness is not as simple as showing that for any $w \in W$ there exists a module map and $v \in V$ such that $\phi(v) = w$. Thanks!","Let $V$ be a vector space over $\mathbb C$ and $W$ a $\operatorname{End}(V)$-module.  I'm having difficulty seeing why the map $$ \operatorname{Hom}_{\operatorname{End}(V)}(V,W) \otimes V \to W $$ $$ \phi \otimes v \mapsto \phi(v) $$ is surjective.  I'm having trouble because I can't construct elements of $\operatorname{Hom}_{\operatorname{End}(V)} (V,W)$.  It seems hard because I know that the above map is injective which means showing surjectiveness is not as simple as showing that for any $w \in W$ there exists a module map and $v \in V$ such that $\phi(v) = w$. Thanks!",,"['linear-algebra', 'abstract-algebra', 'representation-theory']"
18,Vector subspaces proof check,Vector subspaces proof check,,"Suppose I wish to show that for a finite dimensional vector space, $V$, with basis $B=\{b_1,...,b_n\}$ and a given subspace $X$ of $V$, there exists a subset of $B$ that generates a subspace $Y$, such that $V=X\bigoplus Y$. Would the following argument make sense? Suppose $X$ has basis $B_X$ which elements are expressed in terms of $B$. List them, together with the elements of $B$, in a matrix and perform Gaussian elimination preserving the $B_X$ rows. then the remaining (non-zero) vectors in the matrix, $M$, will be a basis for $V$. Let ($M$ \ $B_X$)  be the basis of $Y$. Then, $V=X+Y$. Added: The matrix $M$ should only have $n$ non-zero rows. What it meant by ""perform[ing] Gaussian elimination"" is just to list all the basis vectors in $B$ and those for $X$. Then there are linearly dependent vectors. So we use Gaussian elimination to kill those off. But we choose to kill off the basis vectors in the original $B$, leaving those of $X$ in our matrix $M$. So that $M$ contains the basis vectors of $X$ but has only $n$ entries/L.I, vectors. $X\cap Y=\{0\}$ because the the bases for $X$ and $Y$ are linearly independent. Therefore, the statement is true. Comment : Firstly, I am not entirely sure that my argument is necessarily valid. Secondly, it seems to lack rigor. Thanks in advance for any help!","Suppose I wish to show that for a finite dimensional vector space, $V$, with basis $B=\{b_1,...,b_n\}$ and a given subspace $X$ of $V$, there exists a subset of $B$ that generates a subspace $Y$, such that $V=X\bigoplus Y$. Would the following argument make sense? Suppose $X$ has basis $B_X$ which elements are expressed in terms of $B$. List them, together with the elements of $B$, in a matrix and perform Gaussian elimination preserving the $B_X$ rows. then the remaining (non-zero) vectors in the matrix, $M$, will be a basis for $V$. Let ($M$ \ $B_X$)  be the basis of $Y$. Then, $V=X+Y$. Added: The matrix $M$ should only have $n$ non-zero rows. What it meant by ""perform[ing] Gaussian elimination"" is just to list all the basis vectors in $B$ and those for $X$. Then there are linearly dependent vectors. So we use Gaussian elimination to kill those off. But we choose to kill off the basis vectors in the original $B$, leaving those of $X$ in our matrix $M$. So that $M$ contains the basis vectors of $X$ but has only $n$ entries/L.I, vectors. $X\cap Y=\{0\}$ because the the bases for $X$ and $Y$ are linearly independent. Therefore, the statement is true. Comment : Firstly, I am not entirely sure that my argument is necessarily valid. Secondly, it seems to lack rigor. Thanks in advance for any help!",,"['linear-algebra', 'vector-spaces']"
19,Find the image of a vector by using the standard matrix (for the linear transformation T),Find the image of a vector by using the standard matrix (for the linear transformation T),,"Was wondering if anyone can help out with the following problem: Use the standard matrix for the linear transformation $T$ to find the image of the vector $\mathbf{v}$, where   $$T(x,y) = (x+y,x-y, 2x,2y),\qquad \mathbf{v}=(3,-3).$$ I found out the standard matrix for $T$ to be: $$\begin{bmatrix}1&1\\1&-1\\2&0\\0&2\end{bmatrix}$$ From here I honestly don't know how to find the ""image of the vector $\mathbf{v}$"". Does anyone have any suggestions?","Was wondering if anyone can help out with the following problem: Use the standard matrix for the linear transformation $T$ to find the image of the vector $\mathbf{v}$, where   $$T(x,y) = (x+y,x-y, 2x,2y),\qquad \mathbf{v}=(3,-3).$$ I found out the standard matrix for $T$ to be: $$\begin{bmatrix}1&1\\1&-1\\2&0\\0&2\end{bmatrix}$$ From here I honestly don't know how to find the ""image of the vector $\mathbf{v}$"". Does anyone have any suggestions?",,[]
20,Database of binary linear codes (including bad ones),Database of binary linear codes (including bad ones),,"I am looking for all $k$-dimensional subspaces of $(\mathbb{Z}/2\mathbb{Z})^n$ up to permutational equivalence. Is there a database of all $[n,k]$-codes up to equivalence for reasonable values of $(n,k)$? For instance, $1 \leq k \leq 6$ and $6 \leq n \leq 12$ (giving around 35,000 codes). When $k=1$, there are $n$ such subspaces, each uniquely determined by the Hamming weight of the nonzero element. When $k=2$, there are 0, 1, 3, 6, 10, 16, 23, 32 … ( OEIS:A034198 ) such subspaces. When $k=3$, there are 0, 0, 1, 4, 10, 22, 43, 77, … ( OEIS:A034357 ) such subspaces. When $k=4$, there are 0, 0, 0, 1, 5, 16, 43, 106, … ( OEIS:A034358 ) such subspaces. For each $k\geq 2$, I have trouble finding representatives of the codes for $n \geq 8$, but surely someone has recorded them somewhere.  For $n=8$, there are just 8, 32, 77, 106 such codes for $k=1,2,3,4$, so it seems reasonable to me that someone wrote them down. I have found several databases of ""good"" codes, but I actually want the bad codes too.","I am looking for all $k$-dimensional subspaces of $(\mathbb{Z}/2\mathbb{Z})^n$ up to permutational equivalence. Is there a database of all $[n,k]$-codes up to equivalence for reasonable values of $(n,k)$? For instance, $1 \leq k \leq 6$ and $6 \leq n \leq 12$ (giving around 35,000 codes). When $k=1$, there are $n$ such subspaces, each uniquely determined by the Hamming weight of the nonzero element. When $k=2$, there are 0, 1, 3, 6, 10, 16, 23, 32 … ( OEIS:A034198 ) such subspaces. When $k=3$, there are 0, 0, 1, 4, 10, 22, 43, 77, … ( OEIS:A034357 ) such subspaces. When $k=4$, there are 0, 0, 0, 1, 5, 16, 43, 106, … ( OEIS:A034358 ) such subspaces. For each $k\geq 2$, I have trouble finding representatives of the codes for $n \geq 8$, but surely someone has recorded them somewhere.  For $n=8$, there are just 8, 32, 77, 106 such codes for $k=1,2,3,4$, so it seems reasonable to me that someone wrote them down. I have found several databases of ""good"" codes, but I actually want the bad codes too.",,"['linear-algebra', 'combinatorics', 'coding-theory']"
21,How to detect antitransitivity from an adjacency matrix?,How to detect antitransitivity from an adjacency matrix?,,"I'm trying to derive a taxonomy from sparse ""is-a"" relationships and am looking for a linear algebraic solution. More specifically, the data is noisy and I want to detect relations that violate anti-transitivity, remove the least offenders, and then fill-in the transitive relationships, inheriting all relations from parents to children. Is anyone aware of mathematical methods to: compute a general statistic of ""transitivity"" of some graph matrix? compute a matrix of relations in violation of anti-transitivity? I could remove the offending rows from (2) and see how they affect (1).  Then, I could compute transitive closure, which I already know how to do [1]. [1] http://www.cs.nmsu.edu/~ipivkina/TransClosure/index.html Warning: contains god-forsaken Java Applet.","I'm trying to derive a taxonomy from sparse ""is-a"" relationships and am looking for a linear algebraic solution. More specifically, the data is noisy and I want to detect relations that violate anti-transitivity, remove the least offenders, and then fill-in the transitive relationships, inheriting all relations from parents to children. Is anyone aware of mathematical methods to: compute a general statistic of ""transitivity"" of some graph matrix? compute a matrix of relations in violation of anti-transitivity? I could remove the offending rows from (2) and see how they affect (1).  Then, I could compute transitive closure, which I already know how to do [1]. [1] http://www.cs.nmsu.edu/~ipivkina/TransClosure/index.html Warning: contains god-forsaken Java Applet.",,"['linear-algebra', 'matrices']"
22,A relation between permanents and determinants,A relation between permanents and determinants,,"I have skimmed this video that I found on mathoverflow: http://tube.sfu-kras.ru/video/407?playlist=397 At about 15:05 the lecturer wrote down an equality $\sum F(m_1, \ldots, m_m)z^{m_1}\ldots z^{m_m} = \frac{1}{det(zI - A)}$, where F is given in terms of coefficients of the $m$-by-$m$ matrix $A$. The definition of $F$ doesn't yet make much sense to me, but that's beyond the point. My question is, how is this equality possible? the right hand side obviously has poles—the eigenvalues of $A$, but the left hand side is just a polinomial, it can't have poles! Is there something I'm missing? EDIT: well, it was actually $\sum F(m_1, \ldots, m_m) z_1^{m_1}\ldots z_m^{m_m} = \frac{1}{det(z - A)}$. Here I have abused the notation slightly: the z means actually the diagonalization of the corresponding vector. However, when you take $z_1 = \ldots = z_m$, the question still stands. The domain is real, as I understand from the context of the lecture :), so the use of the term pole isn't really correct, I just forgot the right word in English :)","I have skimmed this video that I found on mathoverflow: http://tube.sfu-kras.ru/video/407?playlist=397 At about 15:05 the lecturer wrote down an equality $\sum F(m_1, \ldots, m_m)z^{m_1}\ldots z^{m_m} = \frac{1}{det(zI - A)}$, where F is given in terms of coefficients of the $m$-by-$m$ matrix $A$. The definition of $F$ doesn't yet make much sense to me, but that's beyond the point. My question is, how is this equality possible? the right hand side obviously has poles—the eigenvalues of $A$, but the left hand side is just a polinomial, it can't have poles! Is there something I'm missing? EDIT: well, it was actually $\sum F(m_1, \ldots, m_m) z_1^{m_1}\ldots z_m^{m_m} = \frac{1}{det(z - A)}$. Here I have abused the notation slightly: the z means actually the diagonalization of the corresponding vector. However, when you take $z_1 = \ldots = z_m$, the question still stands. The domain is real, as I understand from the context of the lecture :), so the use of the term pole isn't really correct, I just forgot the right word in English :)",,"['linear-algebra', 'real-analysis', 'abstract-algebra']"
23,Finding the inverse of a matrix via the adjugate and the determinant,Finding the inverse of a matrix via the adjugate and the determinant,,"Let's consider a $3 \times 3$ matrix, $\bf A$ . Its inverse can be found by computing the adjoint of the matrix and dividing it by its determinant. Can someone please explain why this process work and how it can be visualized? I know that the determinant can be thought of as the volume spanned by the column vectors and that the adjoint of matrix $\bf A$ is $\bf A^\top$ 's cofactors. The cofactors of the matrix $\bf A$ can be found by the cross product of each of its column vectors. So what does the cofactor matrix tell us?","Let's consider a matrix, . Its inverse can be found by computing the adjoint of the matrix and dividing it by its determinant. Can someone please explain why this process work and how it can be visualized? I know that the determinant can be thought of as the volume spanned by the column vectors and that the adjoint of matrix is 's cofactors. The cofactors of the matrix can be found by the cross product of each of its column vectors. So what does the cofactor matrix tell us?",3 \times 3 \bf A \bf A \bf A^\top \bf A,"['linear-algebra', 'matrices', 'determinant', 'inverse']"
24,Why did my teacher solved this problem this way?,Why did my teacher solved this problem this way?,,"The problem asks the following: which vector of the subspace $$V= \{\mathbf{x} \in \Bbb{R}^4: 2x_1+x_2+x_3+3x_4=0; 3x_1+2x_2+2x_3+x_4=0; x_1+2x_2+2x_3-9x_4=0\}$$ gives the best approximation to $(7,-4,-1,2)$ . My proffesor started by saying that: $$\mathbf{w}=(A\mathbf{z}-\mathbf{a})$$ Where $\mathbf{z}$ is the vector we are looking for and the minimum $\lVert\mathbf{w}\rVert$ will show its value. Then, he solved $\mathbf{z}$ out of the previous equation: $$(A\mathbf{z}-\mathbf{b})·A\mathbf{x}=\mathbf{x}^T(A^TA\mathbf{z}-A^T\mathbf{z})=0$$ And from there: $$\mathbf{z}=(A^TA)^{-1}A^T\mathbf{a}$$ Then he calculated the following basis of $V$ : $$V=span\{\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix}\}$$ And said that $A=\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix}$ . From there, he used the formula he previously obtained for $\mathbf{z}$ and found out that $$\mathbf{z}=-\frac{3}{2}(0,1,-1,0)$$ My question is basically why did he use these formulas insead of calculating the projection of $\mathbf{a}$ over $V$ , which I thougth it was the best approximation of a vector in a subspace.","The problem asks the following: which vector of the subspace gives the best approximation to . My proffesor started by saying that: Where is the vector we are looking for and the minimum will show its value. Then, he solved out of the previous equation: And from there: Then he calculated the following basis of : And said that . From there, he used the formula he previously obtained for and found out that My question is basically why did he use these formulas insead of calculating the projection of over , which I thougth it was the best approximation of a vector in a subspace.","V= \{\mathbf{x} \in \Bbb{R}^4: 2x_1+x_2+x_3+3x_4=0; 3x_1+2x_2+2x_3+x_4=0; x_1+2x_2+2x_3-9x_4=0\} (7,-4,-1,2) \mathbf{w}=(A\mathbf{z}-\mathbf{a}) \mathbf{z} \lVert\mathbf{w}\rVert \mathbf{z} (A\mathbf{z}-\mathbf{b})·A\mathbf{x}=\mathbf{x}^T(A^TA\mathbf{z}-A^T\mathbf{z})=0 \mathbf{z}=(A^TA)^{-1}A^T\mathbf{a} V V=span\{\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix}\} A=\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix} \mathbf{z} \mathbf{z}=-\frac{3}{2}(0,1,-1,0) \mathbf{a} V","['linear-algebra', 'proof-explanation', 'orthogonality', 'orthonormal']"
25,"If $B^3=B$, is $B$ diagonalizable?","If , is  diagonalizable?",B^3=B B,"Let $B\in M_n(\mathbb{F})$ such that $B^3=B$ . Is $B$ diagonalizable? If $B^3=B$ , then $B^3-B=0$ . Consider the polynomial $p(x)=x^3-x$ . If $p(B)= B^3-B=0$ . Since we know that the minimal polynomial of $B$ must divide any polynomial $g(x)\in \mathbb{F}[x]$ such that $g(B)=0$ , then $m_B(x)|p(x)$ and $p(x)=x(x-1)(x+1)$ . This means that $m_B(x)$ splits and each root of $m_B$ has multiplicity 1. So, $B$ is diagonalizable. Is this correct?","Let such that . Is diagonalizable? If , then . Consider the polynomial . If . Since we know that the minimal polynomial of must divide any polynomial such that , then and . This means that splits and each root of has multiplicity 1. So, is diagonalizable. Is this correct?",B\in M_n(\mathbb{F}) B^3=B B B^3=B B^3-B=0 p(x)=x^3-x p(B)= B^3-B=0 B g(x)\in \mathbb{F}[x] g(B)=0 m_B(x)|p(x) p(x)=x(x-1)(x+1) m_B(x) m_B B,"['linear-algebra', 'matrices', 'solution-verification', 'diagonalization', 'minimal-polynomials']"
26,Solving Xa=b for an unknown matrix X,Solving Xa=b for an unknown matrix X,,"I'm interested in studying the solutions of $Xa=b$ for an unknown square matrix $X$ , and given (known) column vectors $a$ and $b$ in $\mathbb{R}^n$ . For any numerical $a, b$ , one can directly attempt solving the aforementioned system. But I'm interested in understanding the general setting to answer questions similar to the ones below. (1) Under what conditions a solution $X$ exists? (2) When does a symmetric solution exist? Under what conditions, no symmetric solution exists? (3) When does a unique, invertible, symmetric solution exist? The answer to (1) is easy: a solution exists whenever $a\ne \vec{0}$ , or both $a, b$ are zero vectors. $Xa=b$ is, of course, a system of $n$ linear equations in $n^2$ variables if there are no additional constraints on $X$ , in which case the equations are ""decoupled"" because of having disjoint set of variables. But, for instance, the number of variables is cut down to $n(n-1)/2$ if we require X to be symmetric. So, if $n(n-1)/2=n$ , that is, $n=3$ I expect (3) to be likely than when $n>3$ . What are some good ways to think about problems like this involving $Xa=b$ ? Any suggestions, or references are appreciated.","I'm interested in studying the solutions of for an unknown square matrix , and given (known) column vectors and in . For any numerical , one can directly attempt solving the aforementioned system. But I'm interested in understanding the general setting to answer questions similar to the ones below. (1) Under what conditions a solution exists? (2) When does a symmetric solution exist? Under what conditions, no symmetric solution exists? (3) When does a unique, invertible, symmetric solution exist? The answer to (1) is easy: a solution exists whenever , or both are zero vectors. is, of course, a system of linear equations in variables if there are no additional constraints on , in which case the equations are ""decoupled"" because of having disjoint set of variables. But, for instance, the number of variables is cut down to if we require X to be symmetric. So, if , that is, I expect (3) to be likely than when . What are some good ways to think about problems like this involving ? Any suggestions, or references are appreciated.","Xa=b X a b \mathbb{R}^n a, b X a\ne \vec{0} a, b Xa=b n n^2 X n(n-1)/2 n(n-1)/2=n n=3 n>3 Xa=b","['linear-algebra', 'matrices', 'systems-of-equations', 'symmetric-matrices']"
27,On the walk-generating function,On the walk-generating function,,"While I was reading Norman Biggs' Algebraic Graph Theory I came across the following (Page 12 2g ) exercise/additional result: Let $g_{ij}(r)$ denote the number of walks of length $r$ in $\Gamma$ from $v_i$ to $v_j$ . If we write $\mathbf{G}(z)$ for the matrix $$ \mathbf{G}(z)_{ij}=\sum_{r=1}^{\infty} g_{ij}(r)z^r, $$ then $\mathbf{G}(z)=(\mathbf{I}-z \mathbf{A})^{-1}$ , where $\mathbf{A}$ is the adjacency matrix of $\Gamma$ . This may be regarded as a matrix over the ring of formal power series in $z$ , or as a real matrix defined whenever $z^{-1} \notin \text{Spec}(\Gamma)$ . From the formula for the inverse matrix and 2e , we obtain $$ \mathbf{G}(z)_{ii}=\frac{\chi(\Gamma_i;z^{-1})}{z\chi(\Gamma;z^{-1})}, \hspace{1cm}\text{tr}(\mathbf{G}(z))=\frac{\chi'(\Gamma;z^{-1})}{z \chi(\Gamma,z^{-1})}. $$ I was able to prove everything except the part where the matrix is real if $z^{-1} \notin \text{Spec}(\Gamma)$ . The book has a typo, where instead of $z^{-1}$ it's written $z$ , but according to the following paper it must be $z^{-1}$ . Paper: Walk generating functions and spectral measures of infinite graphs Can someone give me some hints of why this happens? Note: $\chi(\Gamma;\lambda)$ means the characteristic polynomial of $\Gamma$ and $\chi(\Gamma_i;\lambda)$ is the characteristic polynomial of the induced subgraph obtained from $\Gamma$ by removing the vertex $v_i$ .","While I was reading Norman Biggs' Algebraic Graph Theory I came across the following (Page 12 2g ) exercise/additional result: Let denote the number of walks of length in from to . If we write for the matrix then , where is the adjacency matrix of . This may be regarded as a matrix over the ring of formal power series in , or as a real matrix defined whenever . From the formula for the inverse matrix and 2e , we obtain I was able to prove everything except the part where the matrix is real if . The book has a typo, where instead of it's written , but according to the following paper it must be . Paper: Walk generating functions and spectral measures of infinite graphs Can someone give me some hints of why this happens? Note: means the characteristic polynomial of and is the characteristic polynomial of the induced subgraph obtained from by removing the vertex .","g_{ij}(r) r \Gamma v_i v_j \mathbf{G}(z) 
\mathbf{G}(z)_{ij}=\sum_{r=1}^{\infty} g_{ij}(r)z^r,
 \mathbf{G}(z)=(\mathbf{I}-z \mathbf{A})^{-1} \mathbf{A} \Gamma z z^{-1} \notin \text{Spec}(\Gamma) 
\mathbf{G}(z)_{ii}=\frac{\chi(\Gamma_i;z^{-1})}{z\chi(\Gamma;z^{-1})}, \hspace{1cm}\text{tr}(\mathbf{G}(z))=\frac{\chi'(\Gamma;z^{-1})}{z \chi(\Gamma,z^{-1})}.
 z^{-1} \notin \text{Spec}(\Gamma) z^{-1} z z^{-1} \chi(\Gamma;\lambda) \Gamma \chi(\Gamma_i;\lambda) \Gamma v_i","['linear-algebra', 'matrices', 'graph-theory', 'algebraic-graph-theory']"
28,"Determine whether there's permutation $p$ such that for matrices $A_{i}$, $A_{p_{1}}A_{p_{2}} \dots A_{p_{n}} = B$","Determine whether there's permutation  such that for matrices ,",p A_{i} A_{p_{1}}A_{p_{2}} \dots A_{p_{n}} = B,"Given $m$ $n\times n$ matrices $A_{1},A_{2} \dots A_{m}$ and a matrix $B$ , is there a way to determine whether there's permutation $p$ such that for matrices $A_{i}$ , $A_{p_{1}}A_{p_{2}} \dots A_{p_{m}} = B$ . The solution should be ""almost correct"" in all cases, and hopefully, in polynomial time. Moreover, one way is to check whether $\prod{\det(A_{i})} = \det(B)$ , however it is ""correct on almost every cases"". I tried to find some other values to estimate its possibility but failed. Are there any possible solutions? EDIT: Almost correct here means for $\epsilon$ possibility being wrong, you can get it in $O(f(\epsilon))$ time for some $f$ , still we want $f$ not being to large. And some one pointed out that it may be a NP problem in the comment.","Given matrices and a matrix , is there a way to determine whether there's permutation such that for matrices , . The solution should be ""almost correct"" in all cases, and hopefully, in polynomial time. Moreover, one way is to check whether , however it is ""correct on almost every cases"". I tried to find some other values to estimate its possibility but failed. Are there any possible solutions? EDIT: Almost correct here means for possibility being wrong, you can get it in time for some , still we want not being to large. And some one pointed out that it may be a NP problem in the comment.","m n\times n A_{1},A_{2} \dots A_{m} B p A_{i} A_{p_{1}}A_{p_{2}} \dots A_{p_{m}} = B \prod{\det(A_{i})} = \det(B) \epsilon O(f(\epsilon)) f f",['linear-algebra']
29,Max Eigenvalue Norm,Max Eigenvalue Norm,,"Let $X$ be a vector space over $\mathbb{C}$ . The set $\mathcal{L}(X)$ of all linear mappings $A: X \to X$ is a vector space over $\mathbb{C}$ . Prove, or disprove with a counterexample, the following assertion: \begin{align*} \rho(A) = \textrm{max}\{|\lambda| \in \mathbb{R} : \lambda \textrm{ is an eigenvalue of $A$}\} \end{align*} defines a norm on $\mathcal{L}(X)$ . It seems to me that this is not a norm. For a counterexample, consider any nonzero nilpotent operator $N$ . Since $N$ is nilpotent, its only eigenvalues are 0, hence $\rho(N) = 0$ . However, for $\rho$ to be a norm, $\rho(X) = 0$ if and only if $X = 0$ , so $\rho$ is not a norm. Is there something I'm missing here?","Let be a vector space over . The set of all linear mappings is a vector space over . Prove, or disprove with a counterexample, the following assertion: defines a norm on . It seems to me that this is not a norm. For a counterexample, consider any nonzero nilpotent operator . Since is nilpotent, its only eigenvalues are 0, hence . However, for to be a norm, if and only if , so is not a norm. Is there something I'm missing here?","X \mathbb{C} \mathcal{L}(X) A: X \to X \mathbb{C} \begin{align*}
\rho(A) = \textrm{max}\{|\lambda| \in \mathbb{R} : \lambda \textrm{ is an eigenvalue of A}\}
\end{align*} \mathcal{L}(X) N N \rho(N) = 0 \rho \rho(X) = 0 X = 0 \rho","['linear-algebra', 'normed-spaces']"
30,Determinant for a non-linear system,Determinant for a non-linear system,,"Given a system of $n$ linear equations $$ \displaystyle \sum_{j=1}^n a_{ij}x_j = 0, \quad i=1,\dots, n $$ for some finite $n$ and unknowns $x_1,\dots,x_n$ , the statement that the system has a non-zero solution implies that the determinant of the coefficient matrix is $0$ , that is $$ \det(A)=0,\quad A=(a_{ij})_{i,j=1}^n \,. $$ Question : Is there any analogous statement for non-linear systems? For example, consider the system of $n-1$ equations $$ \displaystyle \sum_{j=1}^{n} a_{ij}x_j = 0, \quad i=1,\dots, n-1 $$ but with the definition $x_{n} = f(x_1)$ where $f$ is some function. If my system has a non-zero solution for $x_1,\dots,x_{n-1}$ is there an equivalent statement that the determinant of the coefficient matrix must vanish? Based on some experiments, it seems to be that if I define $A_1 = (a_{ij})_{i,j=1}^{n-1}$ and $A_2 = (a_{ij})_{i,j=2}^{n}$ then the existence of a non-trivial solution requires both $A_1$ and $A_2$ to have a non-vanishing determinant . Of course, from this I can guess that if I have a system of $n-k$ equations on $n$ unknowns but with $x_{n+1-k}=f_k(x_k)$ then I would require that all minors of $A$ of size $n-k$ have non-vanishing determinant. However, I am not interested in non-vanishing determinants. I would like to know is there anything which must vanish .","Given a system of linear equations for some finite and unknowns , the statement that the system has a non-zero solution implies that the determinant of the coefficient matrix is , that is Question : Is there any analogous statement for non-linear systems? For example, consider the system of equations but with the definition where is some function. If my system has a non-zero solution for is there an equivalent statement that the determinant of the coefficient matrix must vanish? Based on some experiments, it seems to be that if I define and then the existence of a non-trivial solution requires both and to have a non-vanishing determinant . Of course, from this I can guess that if I have a system of equations on unknowns but with then I would require that all minors of of size have non-vanishing determinant. However, I am not interested in non-vanishing determinants. I would like to know is there anything which must vanish .","n 
\displaystyle \sum_{j=1}^n a_{ij}x_j = 0, \quad i=1,\dots, n
 n x_1,\dots,x_n 0 
\det(A)=0,\quad A=(a_{ij})_{i,j=1}^n \,.
 n-1 
\displaystyle \sum_{j=1}^{n} a_{ij}x_j = 0, \quad i=1,\dots, n-1
 x_{n} = f(x_1) f x_1,\dots,x_{n-1} A_1 = (a_{ij})_{i,j=1}^{n-1} A_2 = (a_{ij})_{i,j=2}^{n} A_1 A_2 n-k n x_{n+1-k}=f_k(x_k) A n-k","['linear-algebra', 'nonlinear-system']"
31,"The $n\times n$ matrices $A^n, A^{n-1},\dots,\mathrm{id}_n$ are linearly dependent",The  matrices  are linearly dependent,"n\times n A^n, A^{n-1},\dots,\mathrm{id}_n","Let $A$ be an $n\times n$ matrix over some field $K$ . Then its characteristic polynomial $\mathrm{char}_A(X)\in K[X]$ is monic of degree $n$ and annihilated by $A$ (Cayley-Hamilton). It follows that Corollary. $A^n, A^{n-1},\dots,\mathrm{id}_n$ are linearly dependent. I wonder if this Corollary can be obtained without making use of the determinant (which is required to define $\mathrm{char}_A(X)$ )? Otherwise, can we show that there is some $m<n^2$ for which $A^m, A^{m-1},\dots,\mathrm{id}_n$ are linearly dependent? Note that if we take $m=n^2$ , then the statement is obvious, because the $n\times n$ matrices form a vector space of dimension $n^2$ , and $A^m, A^{m-1},\dots,\mathrm{id}_n$ are $n^2+1$ matrices.","Let be an matrix over some field . Then its characteristic polynomial is monic of degree and annihilated by (Cayley-Hamilton). It follows that Corollary. are linearly dependent. I wonder if this Corollary can be obtained without making use of the determinant (which is required to define )? Otherwise, can we show that there is some for which are linearly dependent? Note that if we take , then the statement is obvious, because the matrices form a vector space of dimension , and are matrices.","A n\times n K \mathrm{char}_A(X)\in K[X] n A A^n, A^{n-1},\dots,\mathrm{id}_n \mathrm{char}_A(X) m<n^2 A^m, A^{m-1},\dots,\mathrm{id}_n m=n^2 n\times n n^2 A^m, A^{m-1},\dots,\mathrm{id}_n n^2+1","['linear-algebra', 'matrices', 'linear-independence']"
32,Concatenated diagonalization of combined real/imaginary diagonalization,Concatenated diagonalization of combined real/imaginary diagonalization,,"I have a problem involving a complex matrix that I need to diagonalize and apply weights to the entries according to a function relating only to the eigenvalues of the real part of the matrix. Suppose that I have diagonalized separately the real and imaginary parts of $ A=A_r+iA_i $ such that: $ A_r=U_r\Lambda_rU_r^{-1} $ and $ A_i=U_i\Lambda_iU_i^{-1} $ . What I need to do next is calculate a weighted ""error"" norm which, given two complex vectors $ x,\ y $ is written as: $ \sum_{i=1}^N w_i|y-Ax|_i^2 $ and $ A=U_r\Lambda_rU_r^{-1}+iU_i\Lambda_iU_i^{-1}  $ I then rewrite the problem as: $ \begin{bmatrix} y_r \newline y_i \end{bmatrix}=\begin{bmatrix} A_r & -A_i \newline A_i & A_r \end{bmatrix} \cdot \begin{bmatrix} x_r \newline x_i \end{bmatrix} $ I thought that to correctly apply weights to my rows, I would first need to find a new $ 2N\times 2N $ orthogonal matrix $ U $ such that $ \tilde{y}=U^{-1}\begin{bmatrix} y_r \newline y_i \end{bmatrix},\ \tilde{x}=U^{-1}\begin{bmatrix} x_r \newline x_i \end{bmatrix} $ and $ \sum w_i|y-Ax|_i^2 = \sum w_i|\tilde{y}-\Lambda\tilde{x}|_i^2 $ where $ \Lambda $ would be a new diagonal matrix* and then I could correctly apply the weights to my rows according to some function of the $ [\Lambda_r]_{ii} $ for each $ i $ . Essentially, is there any way to find a new diagonalization $ U,\ \Lambda $ such that: $ \begin{bmatrix} U_r\Lambda_r U_r^{-1} & -U_i\Lambda_i U_i^{-1} \newline U_i\Lambda_i U_i^{-1} & U_r\Lambda_r U_r^{-1} \end{bmatrix} = U\Lambda U^{-1} $ Thanks for your inputs in advance! *(possibly $ \begin{bmatrix} \Lambda_r & 0 \newline 0 & \Lambda_i \end{bmatrix} $ ?)","I have a problem involving a complex matrix that I need to diagonalize and apply weights to the entries according to a function relating only to the eigenvalues of the real part of the matrix. Suppose that I have diagonalized separately the real and imaginary parts of such that: and . What I need to do next is calculate a weighted ""error"" norm which, given two complex vectors is written as: and I then rewrite the problem as: I thought that to correctly apply weights to my rows, I would first need to find a new orthogonal matrix such that and where would be a new diagonal matrix* and then I could correctly apply the weights to my rows according to some function of the for each . Essentially, is there any way to find a new diagonalization such that: Thanks for your inputs in advance! *(possibly ?)"," A=A_r+iA_i   A_r=U_r\Lambda_rU_r^{-1}   A_i=U_i\Lambda_iU_i^{-1}   x,\ y   \sum_{i=1}^N w_i|y-Ax|_i^2   A=U_r\Lambda_rU_r^{-1}+iU_i\Lambda_iU_i^{-1}    \begin{bmatrix} y_r \newline y_i \end{bmatrix}=\begin{bmatrix} A_r & -A_i \newline A_i & A_r \end{bmatrix} \cdot \begin{bmatrix} x_r \newline x_i \end{bmatrix}   2N\times 2N   U   \tilde{y}=U^{-1}\begin{bmatrix} y_r \newline y_i \end{bmatrix},\ \tilde{x}=U^{-1}\begin{bmatrix} x_r \newline x_i \end{bmatrix}   \sum w_i|y-Ax|_i^2 = \sum w_i|\tilde{y}-\Lambda\tilde{x}|_i^2   \Lambda   [\Lambda_r]_{ii}   i   U,\ \Lambda   \begin{bmatrix} U_r\Lambda_r U_r^{-1} & -U_i\Lambda_i U_i^{-1} \newline U_i\Lambda_i U_i^{-1} & U_r\Lambda_r U_r^{-1} \end{bmatrix} = U\Lambda U^{-1}   \begin{bmatrix} \Lambda_r & 0 \newline 0 & \Lambda_i \end{bmatrix} ","['linear-algebra', 'diagonalization', 'orthogonal-matrices']"
33,A set of algebra homomorphisms from the real sequence space,A set of algebra homomorphisms from the real sequence space,,"I was myself wondering the following question. Consider the sequence space $\mathbb{R}^{\mathbb{N}}$ , that is $\mathbb{R}^{\mathbb{N}} := \{ (x_{k})_{k=1}^{\infty} \; | \; x_{k} \in \mathbb{R} \}$ . It has a structure of a (real) commutative associative algebra with the unit, with the multiplication given by $(x_{k})_{k=1}^{\infty} \cdot (y_{k})_{k=1}^{\infty} = (x_{k} \cdot y_{k})_{k=1}^{\infty}$ . Equivalently, $\mathbb{R}^{\mathbb{N}}$ is a direct product algebra $\prod_{k=1}^{\infty}\mathbb{R}$ or an algebra of maps $f: \mathbb{N} \rightarrow \mathbb{R}$ . Question : I am interested in the set of all algebra homomorphisms (preserving the unit) from $\mathbb{R}^{\mathbb{N}}$ to the algebra $\mathbb{R}$ . I have the following remarks/observations: This set certainly contains all projections $\pi_{j}(x_{k})_{k=1}^{\infty} = x_{j}$ , so this set has to be at least as big as $\mathbb{N}$ . If one replaces $\mathbb{R}$ with $\mathbb{Z}$ , it contains only these projections. This is a well-known statement, see e.g. What are the ring homomorphisms $\mathbb{Z}^\mathbb{N} \to \mathbb{Z}$? . This means that in this case, the set in question is precisely $\mathbb{N}$ . In the same question they claim that the result holds for any ring $R$ which is UFD with at least two distinct prime elements. But $\mathbb{R}$ has no prime elements. Any homomorphism $f: \mathbb{R}^{\mathbb{N}} \rightarrow \mathbb{R}$ must act as a scalar multiple of a projection $\pi_{j}$ on sequences with finitely many non-zero elements. This is easy to see - if $\mathbf{e}_{a}$ is the sequence with $1$ on the $a$ -th place and zeros everywhere else, one can show that $f(\mathbf{e}_{a}) \neq 0$ only for a single $a \in \mathbb{N}$ , which follows from the equation $f(\mathbf{e}_{a} \cdot \mathbf{e}_{b}) = f(\mathbf{e}_{a}) \cdot f(\mathbf{e}_{b})$ and the fact that $\mathbf{e}_{a} \cdot \mathbf{e}_{b} = 0$ for $a \neq b$ . Hence $f(x)_{k=1}^{n} = \lambda x_{a}$ for some $a \in \mathbb{N}$ and $\lambda \in \mathbb{R}$ for any sequence $(x_{k})_{k=1}^{n}$ with finitely many non-zero elements. However, one cannot repeat the argument from the reference in 2. to show that $f$ acts in this way on any sequence. Clearly, the set in question is a subset of the dual space $(\mathbb{R}^{\mathbb{N}})^{\ast}$ . Since $\mathbb{R}^{\mathbb{N}} = (\mathbb{R}^{\infty})^{\ast}$ , where $\mathbb{R}^{\infty}$ is a set of all sequences with finitely many non-zero terms (that is $\mathbb{R}^{\infty} = \bigoplus_{k=1}^{\infty} \mathbb{R}$ ), we are looking for a subset of the double dual $(\mathbb{R}^{\infty})^{\ast \ast}$ which is supposedly an ugly space. It known that $\mathbb{R}^{\infty}$ embeds as a proper subspace into $(\mathbb{R}^{\infty})^{\ast \ast}$ . Is there some explicit or useful description (or at least some nice examples) of elements of the double dual (that is of $(\mathbb{R}^{\mathbb{N}})^{\ast}$ ) which are not in this subspace? I was thinking that maybe some of these examples would be also an algebra homomorphism, proving that the set of all homomorphisms from $\mathbb{R}^{\mathbb{N}}$ to $\mathbb{R}$ is strictly bigger then $\mathbb{N}$ . Bonus Question: Instead of $\mathbb{N}$ , I would like to know the result for general set $M$ , that is consider a direct product algebra $\prod_{m \in M} \mathbb{R}$ , or equivalently the algebra of all functions $f: M \rightarrow \mathbb{R}$ , and ask the same question - what is the set of all homomorphisms from this algebra to $\mathbb{R}$ ? It is easy to show that for a finite $M$ , it is isomorphic to $M$ . Is there some answer for a general $M$ ?","I was myself wondering the following question. Consider the sequence space , that is . It has a structure of a (real) commutative associative algebra with the unit, with the multiplication given by . Equivalently, is a direct product algebra or an algebra of maps . Question : I am interested in the set of all algebra homomorphisms (preserving the unit) from to the algebra . I have the following remarks/observations: This set certainly contains all projections , so this set has to be at least as big as . If one replaces with , it contains only these projections. This is a well-known statement, see e.g. What are the ring homomorphisms $\mathbb{Z}^\mathbb{N} \to \mathbb{Z}$? . This means that in this case, the set in question is precisely . In the same question they claim that the result holds for any ring which is UFD with at least two distinct prime elements. But has no prime elements. Any homomorphism must act as a scalar multiple of a projection on sequences with finitely many non-zero elements. This is easy to see - if is the sequence with on the -th place and zeros everywhere else, one can show that only for a single , which follows from the equation and the fact that for . Hence for some and for any sequence with finitely many non-zero elements. However, one cannot repeat the argument from the reference in 2. to show that acts in this way on any sequence. Clearly, the set in question is a subset of the dual space . Since , where is a set of all sequences with finitely many non-zero terms (that is ), we are looking for a subset of the double dual which is supposedly an ugly space. It known that embeds as a proper subspace into . Is there some explicit or useful description (or at least some nice examples) of elements of the double dual (that is of ) which are not in this subspace? I was thinking that maybe some of these examples would be also an algebra homomorphism, proving that the set of all homomorphisms from to is strictly bigger then . Bonus Question: Instead of , I would like to know the result for general set , that is consider a direct product algebra , or equivalently the algebra of all functions , and ask the same question - what is the set of all homomorphisms from this algebra to ? It is easy to show that for a finite , it is isomorphic to . Is there some answer for a general ?",\mathbb{R}^{\mathbb{N}} \mathbb{R}^{\mathbb{N}} := \{ (x_{k})_{k=1}^{\infty} \; | \; x_{k} \in \mathbb{R} \} (x_{k})_{k=1}^{\infty} \cdot (y_{k})_{k=1}^{\infty} = (x_{k} \cdot y_{k})_{k=1}^{\infty} \mathbb{R}^{\mathbb{N}} \prod_{k=1}^{\infty}\mathbb{R} f: \mathbb{N} \rightarrow \mathbb{R} \mathbb{R}^{\mathbb{N}} \mathbb{R} \pi_{j}(x_{k})_{k=1}^{\infty} = x_{j} \mathbb{N} \mathbb{R} \mathbb{Z} \mathbb{N} R \mathbb{R} f: \mathbb{R}^{\mathbb{N}} \rightarrow \mathbb{R} \pi_{j} \mathbf{e}_{a} 1 a f(\mathbf{e}_{a}) \neq 0 a \in \mathbb{N} f(\mathbf{e}_{a} \cdot \mathbf{e}_{b}) = f(\mathbf{e}_{a}) \cdot f(\mathbf{e}_{b}) \mathbf{e}_{a} \cdot \mathbf{e}_{b} = 0 a \neq b f(x)_{k=1}^{n} = \lambda x_{a} a \in \mathbb{N} \lambda \in \mathbb{R} (x_{k})_{k=1}^{n} f (\mathbb{R}^{\mathbb{N}})^{\ast} \mathbb{R}^{\mathbb{N}} = (\mathbb{R}^{\infty})^{\ast} \mathbb{R}^{\infty} \mathbb{R}^{\infty} = \bigoplus_{k=1}^{\infty} \mathbb{R} (\mathbb{R}^{\infty})^{\ast \ast} \mathbb{R}^{\infty} (\mathbb{R}^{\infty})^{\ast \ast} (\mathbb{R}^{\mathbb{N}})^{\ast} \mathbb{R}^{\mathbb{N}} \mathbb{R} \mathbb{N} \mathbb{N} M \prod_{m \in M} \mathbb{R} f: M \rightarrow \mathbb{R} \mathbb{R} M M M,"['linear-algebra', 'abstract-algebra']"
34,Answer clarification for why the parallelogram law implies that the norm is induced by an inner product.,Answer clarification for why the parallelogram law implies that the norm is induced by an inner product.,,"In short: If a norm $\Vert\cdot\Vert$ on a real vector space satisfies the parallelogram law, and $\langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right)$ , then how can we show $ \langle\lambda x,y\rangle = \langle x,\lambda y\rangle $ for $\lambda\in \Bbb R$ ? This is a request for clarification on this specific answer given by zodiac. The post is many years old, so I thought it better to open a new question rather than leave a comment. The problem is to show that if $\Vert\cdot\Vert$ is a norm on a real vector space that satisfies the parallelogram law, $\Vert x+y\Vert^2 + \Vert x-y\Vert^2  = 2\Vert x\Vert^2 + 2\Vert y\Vert^2$ , then $\Vert\cdot\Vert$ is induced by an inner product. We proceed by defining $$\langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right)$$ (from the polarization identity ), and show that this is indeed an inner product, and that it induces $\Vert\cdot\Vert$ . The hardest part is arguably to show scalar linearity, i.e. that $$ \langle\lambda x,y\rangle = \lambda \langle x,y\rangle. $$ The proofs I've seen show it first for $\lambda \in \Bbb Q$ and then appeal to some form of continuity of the purported inner product to extend to real scalars. However, the (or a) central step of the linked answer is instead to show $$ \langle\lambda x,y\rangle = \langle x,\lambda y\rangle. $$ How can we prove the above identity? Of course, the proof should hopefully not use continuity, which is what made this specific answer particularly interesting. We may assume that all other axioms of inner products are already proven, including additive linearity. zodiac also gives the hint of using $\langle x+y,z\rangle = \langle x,z\rangle+ \langle y,z\rangle$ and $\langle -x,y\rangle =-\langle x,y\rangle =\langle x,-y\rangle$ , both of which may be taken as given. (I made an attempt where I considered the difference, regrouped, applied the parallelogram law in two places, and neatly ended up right where I started...)","In short: If a norm on a real vector space satisfies the parallelogram law, and , then how can we show for ? This is a request for clarification on this specific answer given by zodiac. The post is many years old, so I thought it better to open a new question rather than leave a comment. The problem is to show that if is a norm on a real vector space that satisfies the parallelogram law, , then is induced by an inner product. We proceed by defining (from the polarization identity ), and show that this is indeed an inner product, and that it induces . The hardest part is arguably to show scalar linearity, i.e. that The proofs I've seen show it first for and then appeal to some form of continuity of the purported inner product to extend to real scalars. However, the (or a) central step of the linked answer is instead to show How can we prove the above identity? Of course, the proof should hopefully not use continuity, which is what made this specific answer particularly interesting. We may assume that all other axioms of inner products are already proven, including additive linearity. zodiac also gives the hint of using and , both of which may be taken as given. (I made an attempt where I considered the difference, regrouped, applied the parallelogram law in two places, and neatly ended up right where I started...)","\Vert\cdot\Vert \langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right) 
\langle\lambda x,y\rangle = \langle x,\lambda y\rangle
 \lambda\in \Bbb R \Vert\cdot\Vert \Vert x+y\Vert^2 + \Vert x-y\Vert^2  = 2\Vert x\Vert^2 + 2\Vert y\Vert^2 \Vert\cdot\Vert \langle x,y\rangle := \frac14\left(\Vert x+y\Vert^2 - \Vert x-y\Vert^2\right) \Vert\cdot\Vert 
\langle\lambda x,y\rangle = \lambda \langle x,y\rangle.
 \lambda \in \Bbb Q 
\langle\lambda x,y\rangle = \langle x,\lambda y\rangle.
 \langle x+y,z\rangle = \langle x,z\rangle+ \langle y,z\rangle \langle -x,y\rangle =-\langle x,y\rangle =\langle x,-y\rangle","['linear-algebra', 'normed-spaces', 'inner-products']"
35,"For every matrix $A$ mod $r$, is there a nonzero vector $\vec x$ so that $A \vec x$ has 0 mod $r$ nonzero entries?","For every matrix  mod , is there a nonzero vector  so that  has 0 mod  nonzero entries?",A r \vec x A \vec x r,"Definitions: Let $\lVert \vec x \rVert_0$ be equal to the number of nonzero entries in $\vec x$ . I'll write $\mathbb{Z} / r\mathbb{Z}$ , the ring of integers mod $r$ , as $\mathbb{Z}_r$ for brevity. What I'd like to prove: For all $r \geq 2 \in \mathbb{N}$ (not necessarily prime), for all sufficiently large $n \in \mathbb{N}$ , for all $m \in \mathbb{N}$ , and for all $A \in \mathbb{Z}_r^{m \times n}$ , there exists some nonzero $\vec x \in \mathbb{Z}_r^{n \times 1}$ such that $\lVert A \vec x  \rVert_0 \equiv 0 \mod r$ , where both the matrix multiplication and the 0-""norm"" are taken mod $r$ . Current progress: Certainly when $m < n$ , $A$ is singular, so the statement holds. When $r$ is prime, the full statement holds: we can use Fermat's Little Theorem to rewrite $\lVert A \vec x \rVert_0 \mod r$ as a polynomial in the entries of $\vec x$ . The degree of the polynomial (in terms of the components of $\vec x$ ) is $r-1$ . But any polynomial that is nonzero exactly on nonzero inputs (that is, a polynomial representing $OR$ over $\mathbb{Z}_r$ ) has degree at least $n$ : the unique polynomial that is 1 only on $\vec 0$ and 0 otherwise is $NOR(\vec x) = \prod_i (1-x_i^{r-1})$ , which has degree $n(r-1)$ . If we had a polynomial representing $OR$ of degree $< n$ , we could write $NOR(\vec x) = 1 - OR(\vec x)^{r-1}$ , which would have degree $< n(r-1)$ , a contradiction. So, as soon as $n \geq r$ there must be a nonzero $\vec x$ that produces a 0-norm congruent to 0 mod $r$ . I'm pretty sure when $r$ is a prime power, similar polynomial tricks will also work. But when $r$ is composite with distinct prime factors, I run out of tricks and get stuck.","Definitions: Let be equal to the number of nonzero entries in . I'll write , the ring of integers mod , as for brevity. What I'd like to prove: For all (not necessarily prime), for all sufficiently large , for all , and for all , there exists some nonzero such that , where both the matrix multiplication and the 0-""norm"" are taken mod . Current progress: Certainly when , is singular, so the statement holds. When is prime, the full statement holds: we can use Fermat's Little Theorem to rewrite as a polynomial in the entries of . The degree of the polynomial (in terms of the components of ) is . But any polynomial that is nonzero exactly on nonzero inputs (that is, a polynomial representing over ) has degree at least : the unique polynomial that is 1 only on and 0 otherwise is , which has degree . If we had a polynomial representing of degree , we could write , which would have degree , a contradiction. So, as soon as there must be a nonzero that produces a 0-norm congruent to 0 mod . I'm pretty sure when is a prime power, similar polynomial tricks will also work. But when is composite with distinct prime factors, I run out of tricks and get stuck.",\lVert \vec x \rVert_0 \vec x \mathbb{Z} / r\mathbb{Z} r \mathbb{Z}_r r \geq 2 \in \mathbb{N} n \in \mathbb{N} m \in \mathbb{N} A \in \mathbb{Z}_r^{m \times n} \vec x \in \mathbb{Z}_r^{n \times 1} \lVert A \vec x  \rVert_0 \equiv 0 \mod r r m < n A r \lVert A \vec x \rVert_0 \mod r \vec x \vec x r-1 OR \mathbb{Z}_r n \vec 0 NOR(\vec x) = \prod_i (1-x_i^{r-1}) n(r-1) OR < n NOR(\vec x) = 1 - OR(\vec x)^{r-1} < n(r-1) n \geq r \vec x r r r,"['linear-algebra', 'combinatorics', 'matrices', 'number-theory']"
36,Can I bound the spectral radius (or maximum eigenvalue) of a positive definite matrix?,Can I bound the spectral radius (or maximum eigenvalue) of a positive definite matrix?,,"This is a problem motivated by spatial autoregressive models. Assume $A$ is an $n\times n$ non-negative matrix with the sum of each row equals to $1$ . Let $I$ be an $n\times n$ identity matrix and $c\in\mathbb{R}$ . Define \begin{align*} A_1 &= (I-cA)^{-1}(I-cA^\top)^{-1},\\ A_2 &= (I-cA)^{-1}(I-cA^\top)^{-1}(A^\top+A-2cA^\top A)(I-cA)^{-1}(I-cA^\top)^{-1}. \end{align*} When $n$ goes to infinity, can we upper bound the spectral radius $\rho(A_1)$ (or the largest eigenvalue $\lambda_{\max}(A_1)$ ) and $\rho(A_2)$ by adding some appropriate assumptions (maybe on $A+A^\top$ or $A^\top A$ )? This is my consideration. Assume $\lambda_i$ s are the eigenvalue of $A+A^\top - cA^\top A$ for $1\leq i\leq n$ . Then $A_1$ 's eigenvalue should be $(1-c\lambda_i)^{-1}$ . So I guess we should avoid every $\lambda_i$ to be close to $c^{-1}$ , but I have no idea how to describe it mathematically. For $A_2$ , we want to show $\lambda_{\max}(U^{-1}VU^{-1})\leq K$ for some K>0, where $U = (I-cA^\top)(I-cA)$ and $V = A^\top+A-2cA^\top A$ . I believe It suffices to show that \begin{align*} \quad & \lambda_{\max}(U^{-1}VU^{-1})\leq K\\ \Leftrightarrow & \frac{y^\top U^{-1}VU^{-1} y}{y^\top y} \leq K \quad (\text{for all }y\neq 0)\\ \Leftrightarrow & x^\top V x\leq K x^\top U^2 x\quad (x = U^{-1}y)\\ \Leftrightarrow & V\leq KU^2 \end{align*} in Lowner order. I am not sure if it is the right way.","This is a problem motivated by spatial autoregressive models. Assume is an non-negative matrix with the sum of each row equals to . Let be an identity matrix and . Define When goes to infinity, can we upper bound the spectral radius (or the largest eigenvalue ) and by adding some appropriate assumptions (maybe on or )? This is my consideration. Assume s are the eigenvalue of for . Then 's eigenvalue should be . So I guess we should avoid every to be close to , but I have no idea how to describe it mathematically. For , we want to show for some K>0, where and . I believe It suffices to show that in Lowner order. I am not sure if it is the right way.","A n\times n 1 I n\times n c\in\mathbb{R} \begin{align*}
A_1 &= (I-cA)^{-1}(I-cA^\top)^{-1},\\
A_2 &= (I-cA)^{-1}(I-cA^\top)^{-1}(A^\top+A-2cA^\top A)(I-cA)^{-1}(I-cA^\top)^{-1}.
\end{align*} n \rho(A_1) \lambda_{\max}(A_1) \rho(A_2) A+A^\top A^\top A \lambda_i A+A^\top - cA^\top A 1\leq i\leq n A_1 (1-c\lambda_i)^{-1} \lambda_i c^{-1} A_2 \lambda_{\max}(U^{-1}VU^{-1})\leq K U = (I-cA^\top)(I-cA) V = A^\top+A-2cA^\top A \begin{align*}
\quad & \lambda_{\max}(U^{-1}VU^{-1})\leq K\\
\Leftrightarrow & \frac{y^\top U^{-1}VU^{-1} y}{y^\top y} \leq K \quad (\text{for all }y\neq 0)\\
\Leftrightarrow & x^\top V x\leq K x^\top U^2 x\quad (x = U^{-1}y)\\
\Leftrightarrow & V\leq KU^2
\end{align*}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'upper-lower-bounds', 'spectral-radius']"
37,Conjecture: Any two matrices of size $n×n$ with same characteristic and minimal polynomial are similar implies $n\le 3$.,Conjecture: Any two matrices of size  with same characteristic and minimal polynomial are similar implies .,n×n n\le 3,"Notations: $\mathcal{M}_n(\Bbb{R}) $ : the set of all $n×n$ matrices over $\Bbb{R}$ $\chi_A(x)$ : Characteristic polynomial of $A$ $m_A(x)$ : Minimal polynomial of $A$ $A\sim B$ : $\exists P\in Gl_n(\Bbb{R})$ such that $B=P^{-1}AP$ Conjecture : $(\forall A, B\in \mathcal{M}_n(\Bbb{R})$ with $\chi_A=\chi_B$ and $m_A=m_B$ implies $A\sim B) $ implies $n\le 3$ Attempt: Let's work with converse. For $n\ge 4,\exists A, B\in\mathcal{M}_n(\Bbb{R}) $ with $\chi_A=\chi_B$ and $m_A=m_B$ such that $A\not\sim B\tag{1}$ Special cases : $\boxed{n=4}:$ $A=\left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0 \end{array}\right)$ $B= \left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 \end{array}\right)$ $$\chi_A(x) =x^4=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ $\boxed{n=5}$ : $A=\left(\begin{array}{ccccc} 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 1\\ 0 & 0 & 0 & 0 & 0 \end{array}\right)$ $B= \left(\begin{array}{ccccc} 0 & 1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 0 \end{array}\right)$ $$\chi_A(x) =x^5=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ $\boxed{\text{General cases}(n\ge 4)}$ : Strategy: Find two nilpotent matrices $A,B \in \mathcal{M}_n(\Bbb{R})$ with degree of nilpotency $2$ and $\textrm{rank}(A) \neq \textrm{rank}(B)$ $A=(a_{ij}) $ and $B=(b_{ij}) $ where $a_{ij}=\begin{cases} 1 &(i,j)\in\{(2,1),(n-1,n)\}\\0& \text{otherwise}\end{cases}$ $b_{ij}=\begin{cases} 1 &(i,j) =(2,1)\\0& \text{otherwise}\end{cases}$ Then $$\chi_A(x) =x^n=\chi_B(x) $$ and $$m_A(x) =x^2=m_B(x) $$ $\begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align}$ For $n\le 3$ and $\forall A,B\in\mathcal{M}_n(\Bbb{R})$ with $\chi_A=\chi_B $ and $m_A =m_B $ implies $A\sim B $ ( here ) $\tag{2}$ Conclusion: $(1) $ and $(2) $ together implies that the conjecture is true. Question: Is my attempt correct? What about the direct proof $[$ starting with any two matrices having the said properties and then showing $\textrm{deg}(\chi_A) =\textrm{deg}(\chi_B)\le 3]$ ? Note: Two matrices are similar iff both have the same Jordan normal form upto the permutation of Jordan blocks.",Notations: : the set of all matrices over : Characteristic polynomial of : Minimal polynomial of : such that Conjecture : with and implies implies Attempt: Let's work with converse. For with and such that Special cases : and : and : Strategy: Find two nilpotent matrices with degree of nilpotency and and where Then and For and with and implies ( here ) Conclusion: and together implies that the conjecture is true. Question: Is my attempt correct? What about the direct proof starting with any two matrices having the said properties and then showing ? Note: Two matrices are similar iff both have the same Jordan normal form upto the permutation of Jordan blocks.,"\mathcal{M}_n(\Bbb{R})  n×n \Bbb{R} \chi_A(x) A m_A(x) A A\sim B \exists P\in Gl_n(\Bbb{R}) B=P^{-1}AP (\forall A, B\in \mathcal{M}_n(\Bbb{R}) \chi_A=\chi_B m_A=m_B A\sim B)  n\le 3 n\ge 4,\exists A, B\in\mathcal{M}_n(\Bbb{R})  \chi_A=\chi_B m_A=m_B A\not\sim B\tag{1} \boxed{n=4}: A=\left(\begin{array}{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 0 & 0
\end{array}\right) B=
\left(\begin{array}{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}\right) \chi_A(x) =x^4=\chi_B(x)  m_A(x) =x^2=m_B(x)  \begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align} \boxed{n=5} A=\left(\begin{array}{ccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 0 & 0 & 0
\end{array}\right) B=
\left(\begin{array}{ccccc}
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0
\end{array}\right) \chi_A(x) =x^5=\chi_B(x)  m_A(x) =x^2=m_B(x)  \begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align} \boxed{\text{General cases}(n\ge 4)} A,B \in \mathcal{M}_n(\Bbb{R}) 2 \textrm{rank}(A) \neq \textrm{rank}(B) A=(a_{ij})  B=(b_{ij})  a_{ij}=\begin{cases} 1 &(i,j)\in\{(2,1),(n-1,n)\}\\0& \text{otherwise}\end{cases} b_{ij}=\begin{cases} 1 &(i,j) =(2,1)\\0& \text{otherwise}\end{cases} \chi_A(x) =x^n=\chi_B(x)  m_A(x) =x^2=m_B(x)  \begin{align}\textrm{rank}(A)=2\neq 1=\textrm{rank}(B) &\implies A\not\sim B\\\end{align} n\le 3 \forall A,B\in\mathcal{M}_n(\Bbb{R}) \chi_A=\chi_B  m_A =m_B  A\sim B  \tag{2} (1)  (2)  [ \textrm{deg}(\chi_A) =\textrm{deg}(\chi_B)\le 3]","['linear-algebra', 'abstract-algebra', 'jordan-normal-form', 'characteristic-polynomial', 'similar-matrices']"
38,Does an algebra automorphism of $M_n(R)$ preserves the characteristic polynomial?,Does an algebra automorphism of  preserves the characteristic polynomial?,M_n(R),"These days, I am playing around Skolem Noether for matrix algebras, and a question arises. If $F$ is a field , a by-product of Skolem Noether implies that any $F$ -algebra automorphism $\rho$ of $M_n(F)$ preserves the characteric polynomial, that is $\chi_{\rho(M)}=\chi_M$ for all $M\in M_n(F)$ . Question 1. Can we prove that any $F$ -algebra automorphism $\rho$ of $M_n(F)$ preserves the characteric polynomial without using the fact that $\rho$ is inner ? (I would like to use this fact to give a constructive proof of Skolem Noether) Question 2. More generally, let $R$ be a commutative ring with $1$ , and let $\rho$ be an $R$ -algebra automorphism. Is is true that $\rho$ preserves the characteristic polynomial ?? It seems to be true for integral domains: if $R$ is a domain, let $K$ be its quotient field. Then $\rho\otimes Id_K$ is an automorphism of $M_n(K)$ and hence is inner (even if $\rho$ isn't), so the result follows easily, since $M$ may be viewed as a matrix with values in $K$ . If the answer for Question 2 is positive in full generality, I would be grateful to have an elementary argument (or at least, as much as possible).","These days, I am playing around Skolem Noether for matrix algebras, and a question arises. If is a field , a by-product of Skolem Noether implies that any -algebra automorphism of preserves the characteric polynomial, that is for all . Question 1. Can we prove that any -algebra automorphism of preserves the characteric polynomial without using the fact that is inner ? (I would like to use this fact to give a constructive proof of Skolem Noether) Question 2. More generally, let be a commutative ring with , and let be an -algebra automorphism. Is is true that preserves the characteristic polynomial ?? It seems to be true for integral domains: if is a domain, let be its quotient field. Then is an automorphism of and hence is inner (even if isn't), so the result follows easily, since may be viewed as a matrix with values in . If the answer for Question 2 is positive in full generality, I would be grateful to have an elementary argument (or at least, as much as possible).",F F \rho M_n(F) \chi_{\rho(M)}=\chi_M M\in M_n(F) F \rho M_n(F) \rho R 1 \rho R \rho R K \rho\otimes Id_K M_n(K) \rho M K,"['linear-algebra', 'abstract-algebra', 'commutative-algebra', 'characteristic-polynomial']"
39,Reflections of a point about n lines returns point to its original position,Reflections of a point about n lines returns point to its original position,,"Here's a very interesting problem that I made up with a friend this morning: For which even $n$ does there exist a permutation $\pi$ of $\{1,2,\cdots,n\}$ such that when we reflect any point $P$ in the $xy$ plane about the lines $y=\pi(1)x, y=\pi(2)x, \cdots, y=\pi(n)x$ in that order, $P$ is returned to it's original position? For these $n$ , how many such permutations are there? So far, we've noted the fact that when we reflect a point about two lines, it's equivalent to a rotation about the intersection of the lines of angle twice the angle between the two lines. Thus, if we let $\{a_1,a_2,\cdots,a_{n/2}\}=\{\pi(1),\pi(3),\cdots,\pi(n-1)\}$ and $\{b_1,b_2,\cdots,b_{n/2}\}=\{\pi(2),\pi(4),\cdots,\pi(n)\}$ , the image of $P$ is a rotation of angle $2\left(\sum \arctan(a_k)-\sum \arctan(b_k)\right)$ . This needs to be divisible by $2\pi$ . With $\arctan(t)=-\arctan(-t)$ and some basic facts about complex numbers, this is equivalent to requiring $\prod (1+a_ki)\prod (1-b_ki)\in\mathbb{R}$ . From here, there are two main paths that I tried. The first is using that $z\in\mathbb{R}\iff z=\overline{z}$ . This doesn't get far. The second is trying to find the imaginary part and setting equal to $0$ . I couldn't finish this approach off either. I wrote a C++ program and found that this is impossible for $n=4,6,8,10,12,14$ . Any ideas?","Here's a very interesting problem that I made up with a friend this morning: For which even does there exist a permutation of such that when we reflect any point in the plane about the lines in that order, is returned to it's original position? For these , how many such permutations are there? So far, we've noted the fact that when we reflect a point about two lines, it's equivalent to a rotation about the intersection of the lines of angle twice the angle between the two lines. Thus, if we let and , the image of is a rotation of angle . This needs to be divisible by . With and some basic facts about complex numbers, this is equivalent to requiring . From here, there are two main paths that I tried. The first is using that . This doesn't get far. The second is trying to find the imaginary part and setting equal to . I couldn't finish this approach off either. I wrote a C++ program and found that this is impossible for . Any ideas?","n \pi \{1,2,\cdots,n\} P xy y=\pi(1)x, y=\pi(2)x, \cdots, y=\pi(n)x P n \{a_1,a_2,\cdots,a_{n/2}\}=\{\pi(1),\pi(3),\cdots,\pi(n-1)\} \{b_1,b_2,\cdots,b_{n/2}\}=\{\pi(2),\pi(4),\cdots,\pi(n)\} P 2\left(\sum \arctan(a_k)-\sum \arctan(b_k)\right) 2\pi \arctan(t)=-\arctan(-t) \prod (1+a_ki)\prod (1-b_ki)\in\mathbb{R} z\in\mathbb{R}\iff z=\overline{z} 0 n=4,6,8,10,12,14","['linear-algebra', 'contest-math', 'rotations', 'reflection']"
40,Solving a gradient equation involving Jacobian of orthonormal vectors,Solving a gradient equation involving Jacobian of orthonormal vectors,,"Revised to more specific question after realizing some steps Let $N(x)$ be a $K\times D$ matrix function of $x\in \mathbb{R}^D$ with orthonormal rows, so that $N(x)N(x)^T=I_{K\times K}$ . Let $\vec{n}_k$ be the $k$ -th row of $N$ , written as a column vector, and let $D[\vec{n}_k]$ be the Jacobian matrix of $\vec{n}_k$ . Consider the matrix equation $$P(x) \nabla \log p(x) = N(x)^T \vec{q}(x)+\sum_{k=1}^K D[\vec{n}_k(x)]\vec{n}_k(x),$$ where the vector $\vec{q}(x)$ has $r$ -th component $$q^r(x) = \operatorname{Tr}(N(x) D[n_r(x)] N^T(x)),$$ for $r=1,2,\dotsc, K$ and where $P(x)=I_{D\times D} - N(x)^T N(x)$ is the orthogonal projection of $\mathbb{R}^D$ onto the tangent space $T_x M$ at $x$ of some manifold $M$ , and the unknown $p(x)$ is a nice enough function of $x\in \mathbb{R}^D$ . Question: Can we solve this equation explicitly for the unknown function $p(x)$ ? Some simplifications and easy cases: When $K=1$ , and $\operatorname{Tr}(n(x)^T D[n(x)] n(x)) n=0$ we have $P(x)\nabla \log p(x) = D[n]n$ , I know how to proceed, see this answer . This more general case is giving me some trouble however. In general, notice that applying $P(x)$ to both sides of the original equation and using the fact that $P(x)^2=P(x)$ , that $D[\vec{n}_k(x)]\vec{n}_k(x)$ already lies on $T_x M$ , and that (suppresing notation on $x$ ) \begin{align*} PN^T q &= (I-N^TN)N^T q\\ & = N^Tq -N^T N N^Tq\\ & = N^Tq - N^T I_{p\times p} q\\ & = N^Tq-N^Tq=\vec{0} \end{align*} where we have used the orthogonality of $N$ , i.e. $NN^T=I_{K\times K}$ , we obtain the equation $$P \nabla \log p = \sum_{k=1}^K D[\vec{n}_k]\vec{n}_k.$$ It follows that $$\nabla \log p = \sum_{k=1}^K \left(D[\vec{n}_k] +c_kI_{D\times D} \right)\vec{n}_k$$ for some scalar functions $c_k$ (since $\{n_1,\dotsc, n_K\}$ is an orthonormal basis for the kernel of $P$ . It remains to show the RHS is a gradient, analogous to the case $K=1$ . I think it is possible to generalize the argument for $K=1$ but the exact details are escaping me at the moment, so I would appreciate any tips or ideas. Update 2/26/2023: When $N$ comes from orthonormalizing a Jacobian matrix $D[f]$ with full rank on $f^{-1}(\{0\})$ of some smooth function, and we also assume that the rows of $D[f]$ are already orthogonal then we can directly use the previous method linked above with the appropriate changes. We obtain in this case that $\sum_{k=1}^K D[\vec{n}_k]\vec{n}_k = \frac12 \nabla \left[\sum_{k=1}^K H_k\right] - \sum_{k=1}^K \mu_k \vec{n}_k$ and then we get that $$\nabla \log p(x) = \nabla \left[\sum_{k=1}^K \frac12 H_k\right]$$ $$ = \nabla \left[\log \prod_{k=1}^K \|\nabla f_k\|\right],$$ hence $p(x) = \prod_{k=1}^K \|\nabla f_k(x)\|$ . Here $H_k = 2\log \|\nabla f_k\|$ and $$\mu_k = \frac{1}{\|\nabla f_k\|} n_k^T (\nabla^2 f_k)n_k,$$ and $n_k = \nabla f_k / \|\nabla f_k\|$ . I have omitted a bit of the details but if anyone wants to see them I can add them in later when I have more time. In general, my conjecture is the solution is $p(x) = \sqrt{\det J_f(x) J_f(x)^T}$ , where $J_f := D[f]$ .","Revised to more specific question after realizing some steps Let be a matrix function of with orthonormal rows, so that . Let be the -th row of , written as a column vector, and let be the Jacobian matrix of . Consider the matrix equation where the vector has -th component for and where is the orthogonal projection of onto the tangent space at of some manifold , and the unknown is a nice enough function of . Question: Can we solve this equation explicitly for the unknown function ? Some simplifications and easy cases: When , and we have , I know how to proceed, see this answer . This more general case is giving me some trouble however. In general, notice that applying to both sides of the original equation and using the fact that , that already lies on , and that (suppresing notation on ) where we have used the orthogonality of , i.e. , we obtain the equation It follows that for some scalar functions (since is an orthonormal basis for the kernel of . It remains to show the RHS is a gradient, analogous to the case . I think it is possible to generalize the argument for but the exact details are escaping me at the moment, so I would appreciate any tips or ideas. Update 2/26/2023: When comes from orthonormalizing a Jacobian matrix with full rank on of some smooth function, and we also assume that the rows of are already orthogonal then we can directly use the previous method linked above with the appropriate changes. We obtain in this case that and then we get that hence . Here and and . I have omitted a bit of the details but if anyone wants to see them I can add them in later when I have more time. In general, my conjecture is the solution is , where .","N(x) K\times D x\in \mathbb{R}^D N(x)N(x)^T=I_{K\times K} \vec{n}_k k N D[\vec{n}_k] \vec{n}_k P(x) \nabla \log p(x) = N(x)^T \vec{q}(x)+\sum_{k=1}^K D[\vec{n}_k(x)]\vec{n}_k(x), \vec{q}(x) r q^r(x) = \operatorname{Tr}(N(x) D[n_r(x)] N^T(x)), r=1,2,\dotsc, K P(x)=I_{D\times D} - N(x)^T N(x) \mathbb{R}^D T_x M x M p(x) x\in \mathbb{R}^D p(x) K=1 \operatorname{Tr}(n(x)^T D[n(x)] n(x)) n=0 P(x)\nabla \log p(x) = D[n]n P(x) P(x)^2=P(x) D[\vec{n}_k(x)]\vec{n}_k(x) T_x M x \begin{align*}
PN^T q &= (I-N^TN)N^T q\\
& = N^Tq -N^T N N^Tq\\
& = N^Tq - N^T I_{p\times p} q\\
& = N^Tq-N^Tq=\vec{0}
\end{align*} N NN^T=I_{K\times K} P \nabla \log p = \sum_{k=1}^K D[\vec{n}_k]\vec{n}_k. \nabla \log p = \sum_{k=1}^K \left(D[\vec{n}_k] +c_kI_{D\times D} \right)\vec{n}_k c_k \{n_1,\dotsc, n_K\} P K=1 K=1 N D[f] f^{-1}(\{0\}) D[f] \sum_{k=1}^K D[\vec{n}_k]\vec{n}_k = \frac12 \nabla \left[\sum_{k=1}^K H_k\right] - \sum_{k=1}^K \mu_k \vec{n}_k \nabla \log p(x) = \nabla \left[\sum_{k=1}^K \frac12 H_k\right]  = \nabla \left[\log \prod_{k=1}^K \|\nabla f_k\|\right], p(x) = \prod_{k=1}^K \|\nabla f_k(x)\| H_k = 2\log \|\nabla f_k\| \mu_k = \frac{1}{\|\nabla f_k\|} n_k^T (\nabla^2 f_k)n_k, n_k = \nabla f_k / \|\nabla f_k\| p(x) = \sqrt{\det J_f(x) J_f(x)^T} J_f := D[f]","['linear-algebra', 'ordinary-differential-equations', 'differential-geometry']"
41,Is $A^tA$ Zariski dense in symmetric matrices?,Is  Zariski dense in symmetric matrices?,A^tA,"Let $k$ be a field with char $(k)\neq 2$ , and $n$ be a positive integrer. Denote $\operatorname{M}_n(k)$ to be all $n\times n$ matrices over $k$ . Define $\mathcal{A}=\{A^tA\ |\ A\in\operatorname{M}_n(k)\}$ , and $\mathcal{S}=\{A\in\operatorname{M}_n(k)|\ A^t=A\ \text{i.e. } A\text{ is a symmetric matrix.}\}$ . Question1 : When does $\mathcal{A}=\mathcal{S}$ ? (For example, when $n=1$ , we know that $\mathcal{A}=\mathcal{S}$ iff $k$ has no extension of degree 2.) Question2 : Is $\mathcal{A}$ Zariski dense in $\mathcal{S}$ ? If not in general, when it is dense? (For example, when $n=1$ , we know that $\mathcal{A}$ in dense in $\mathcal{S}$ iff $|k|=\infty$ ) Question3 : Question 2, for $k=\mathbb{R}$ or $\mathbb{C}$ .","Let be a field with char , and be a positive integrer. Denote to be all matrices over . Define , and . Question1 : When does ? (For example, when , we know that iff has no extension of degree 2.) Question2 : Is Zariski dense in ? If not in general, when it is dense? (For example, when , we know that in dense in iff ) Question3 : Question 2, for or .",k (k)\neq 2 n \operatorname{M}_n(k) n\times n k \mathcal{A}=\{A^tA\ |\ A\in\operatorname{M}_n(k)\} \mathcal{S}=\{A\in\operatorname{M}_n(k)|\ A^t=A\ \text{i.e. } A\text{ is a symmetric matrix.}\} \mathcal{A}=\mathcal{S} n=1 \mathcal{A}=\mathcal{S} k \mathcal{A} \mathcal{S} n=1 \mathcal{A} \mathcal{S} |k|=\infty k=\mathbb{R} \mathbb{C},"['linear-algebra', 'algebraic-geometry', 'bilinear-form']"
42,Are trace of powers of linear combinations of self-adjoint matrices a complete invariant?,Are trace of powers of linear combinations of self-adjoint matrices a complete invariant?,,"Consider $A_1, \dots, A_N$ self-adjoint $d \times d$ matrices. I was wandering if the knowledge of the polynomials $$ P_k(x_1,\dots,x_N) = \mathrm{Tr} \left( (x_1A_1 + \dots + x_NA_N)^k \right) $$ for every integer $k \geq 1$ suffices to determine $(A_1,\dots,A_N)$ up to rotation. That is, if another $N$ -tuple of self-adjoint matrices $B_1,\dots,B_N$ gives the same sequence of polynomials $(P_k)_{k \geq 1}$ , does it follow that $B_i=UA_iU^{-1}$ for some orthogonal matrix $U$ ? This is easy for $N=1$ and I also checked it for $d=2$ .","Consider self-adjoint matrices. I was wandering if the knowledge of the polynomials for every integer suffices to determine up to rotation. That is, if another -tuple of self-adjoint matrices gives the same sequence of polynomials , does it follow that for some orthogonal matrix ? This is easy for and I also checked it for .","A_1, \dots, A_N d \times d  P_k(x_1,\dots,x_N) = \mathrm{Tr} \left( (x_1A_1 + \dots + x_NA_N)^k \right)  k \geq 1 (A_1,\dots,A_N) N B_1,\dots,B_N (P_k)_{k \geq 1} B_i=UA_iU^{-1} U N=1 d=2","['linear-algebra', 'matrices']"
43,Tensor product plus (identity times) its trace,Tensor product plus (identity times) its trace,,"Let $\xi$ and $\zeta$ be two covectors in $V^*$ . What can we say about the $(1,1)$ tensor $$ \xi \otimes \zeta^\sharp + \text{tr}(\xi\otimes \zeta^\sharp)I? $$ Here $\zeta^\sharp$ is the metric dual to $\zeta$ (assume we have some inner product - which is Lorentzian in my case - so that this is defined). More concretely I am wondering if the resulting matrix representation is a nonsingular matrix. The context is that I am trying to prove existence and uniqueness of solutions to the system of equations $$ (a b^T + b^T a I)x = c, $$ where $a, b, c$ are known vectors. Here $I$ is the $n \times n$ identity matrix. More context/conditions: The covectors $\xi, \zeta$ are both null with respect to an ambient Lorentzian inner product $g$ on $V$ . Furthermore, $g(\xi, \zeta) = -2$ . Therefore the covectors are linearly independent.","Let and be two covectors in . What can we say about the tensor Here is the metric dual to (assume we have some inner product - which is Lorentzian in my case - so that this is defined). More concretely I am wondering if the resulting matrix representation is a nonsingular matrix. The context is that I am trying to prove existence and uniqueness of solutions to the system of equations where are known vectors. Here is the identity matrix. More context/conditions: The covectors are both null with respect to an ambient Lorentzian inner product on . Furthermore, . Therefore the covectors are linearly independent.","\xi \zeta V^* (1,1) 
\xi \otimes \zeta^\sharp + \text{tr}(\xi\otimes \zeta^\sharp)I?
 \zeta^\sharp \zeta 
(a b^T + b^T a I)x = c,
 a, b, c I n \times n \xi, \zeta g V g(\xi, \zeta) = -2","['linear-algebra', 'tensor-products']"
44,"If a vector identifies a hyperplane, what does a matrix identify?","If a vector identifies a hyperplane, what does a matrix identify?",,"Given a vector $x\in\mathbb{R}^n$ this identifies a hyperplane (through the origin) of equation $$ w^\top x = 0. $$ How to generalize this to a matrix $X\in\mathbb{R}^{m\times n}$ ? What geometrical object can it identify? Presumably something ""linear"" like a hyperplane but with larger codimension? Possible Idea My guess is it would identify the following space $$ \{w\in\mathbb{R}^n\,:\, Xw = 0\} = \text{nullspace}(X). $$ Now this is a subspace but what is it geometrically? Does is still look and behave like a hyperplane? Presumably, each row vector in $X$ essentially identifies a hyperplane. So perhaps it's something like this? Looks like an ""envelope"" of hyperplanes..","Given a vector this identifies a hyperplane (through the origin) of equation How to generalize this to a matrix ? What geometrical object can it identify? Presumably something ""linear"" like a hyperplane but with larger codimension? Possible Idea My guess is it would identify the following space Now this is a subspace but what is it geometrically? Does is still look and behave like a hyperplane? Presumably, each row vector in essentially identifies a hyperplane. So perhaps it's something like this? Looks like an ""envelope"" of hyperplanes..","x\in\mathbb{R}^n 
w^\top x = 0.
 X\in\mathbb{R}^{m\times n} 
\{w\in\mathbb{R}^n\,:\, Xw = 0\} = \text{nullspace}(X).
 X","['linear-algebra', 'matrices', 'vector-spaces']"
45,Different approaches to Jordan Canonical Form,Different approaches to Jordan Canonical Form,,"I know two different proofs of the existence of JCF. Let $V$ be a finite-dimensional vector space over base field $\mathbb{C}$ and $\alpha \in \mathsf{End}_{\mathbb{C}}(V)$ . Given transformation $\alpha$ we can define the action of polynomial $f(z) \in \mathbb{C}[z]$ on $v \in V$ as $f(z)v := f(\alpha)(v)$ . This turns $V$ into a $\mathbb{C}[z]$ -module. Since $\mathbb{C}[z]$ is a PID, $V \cong \oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})}$ , where $p_i(z) \in \mathbb{C}[z]$ are monic and irreducible and $r_{ij} > 0$ (polynomials $p_i(z)^{r_{ij}}$ are called 'elementary divisors'). Question 1 . Why after going from $V$ to $\oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})}$ via this isomorphism, operator $\alpha \colon V \to V$ still acts via multiplication by $t$ ? UPD (Answer to Q1) . If $\pi\colon V \to \oplus_{i,j} \mathbb{C}[z] / (\cdots)$ is iso, then $$(\pi \circ \alpha \circ \pi^{-1})(v) = \pi(\alpha(\pi^{-1}(v))) = \pi(t \pi^{-1}(v)) = t \pi(\pi^{-1}(v)) = t v$$ by definition of $\alpha$ and linearity of $\pi$ . (Sorry, it was obvious). Hence it is easy to see that characteristic polynomial $\chi_\alpha(z)$ is equal to $\prod_{i,j} p_i(z)^{r_{ij}}$ , and $p_i(z) = z - \lambda_i$ . Moreover, $\alpha$ acts on each summand $U := \frac{\mathbb{C}[z]}{(z - \lambda)^r}$ with multiplication by Jordan cell $J_{\lambda, r}$ if we choose $$(z - \lambda)^{r-1}, (z-\lambda)^{r-2}, \ldots, (z - \lambda)^0 = 1$$ for basis of $U$ . The second approach is as follows. We can prove that $V$ is the direct sum of its root subspaces $V = \oplus_{i = 1}^s V^{\lambda_i}(\alpha)$ . We can prove that for each nilpotent operator $\beta := \alpha - \lambda \cdot I$ the space $V^\lambda(\alpha)$ is the direct sum of some $\beta$ -cyclic subspaces and on each of them $\alpha$ acts with multiplication by some Jordan cell $J_{\lambda,-}$ . If the first approach was chosen, then I don't understand how to obtain Jordan basis in terms of initial basis of $V$ ; using second approach it is an easy process, but the first approach seems more natural to me. Question 2 . How could we construct the Jordan basis of $V$ or, more specifically, how could we construct the isomorphism between $U := \frac{\mathbb{C}[z]}{((z - \lambda_i)^{r_{ij}})}$ and $r_{ij}$ -dimensional cyclic subspace of $V_{\lambda_i}(\alpha)$ ? Is the corresponding subspace actually $\pi^{-1}(U)$ ? Thank you in advance for any help!","I know two different proofs of the existence of JCF. Let be a finite-dimensional vector space over base field and . Given transformation we can define the action of polynomial on as . This turns into a -module. Since is a PID, , where are monic and irreducible and (polynomials are called 'elementary divisors'). Question 1 . Why after going from to via this isomorphism, operator still acts via multiplication by ? UPD (Answer to Q1) . If is iso, then by definition of and linearity of . (Sorry, it was obvious). Hence it is easy to see that characteristic polynomial is equal to , and . Moreover, acts on each summand with multiplication by Jordan cell if we choose for basis of . The second approach is as follows. We can prove that is the direct sum of its root subspaces . We can prove that for each nilpotent operator the space is the direct sum of some -cyclic subspaces and on each of them acts with multiplication by some Jordan cell . If the first approach was chosen, then I don't understand how to obtain Jordan basis in terms of initial basis of ; using second approach it is an easy process, but the first approach seems more natural to me. Question 2 . How could we construct the Jordan basis of or, more specifically, how could we construct the isomorphism between and -dimensional cyclic subspace of ? Is the corresponding subspace actually ? Thank you in advance for any help!","V \mathbb{C} \alpha \in \mathsf{End}_{\mathbb{C}}(V) \alpha f(z) \in \mathbb{C}[z] v \in V f(z)v := f(\alpha)(v) V \mathbb{C}[z] \mathbb{C}[z] V \cong \oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})} p_i(z) \in \mathbb{C}[z] r_{ij} > 0 p_i(z)^{r_{ij}} V \oplus_{i,j} \frac{\mathbb{C}[z]}{(p_i(z)^{r_{ij}})} \alpha \colon V \to V t \pi\colon V \to \oplus_{i,j} \mathbb{C}[z] / (\cdots) (\pi \circ \alpha \circ \pi^{-1})(v) = \pi(\alpha(\pi^{-1}(v))) = \pi(t \pi^{-1}(v)) = t \pi(\pi^{-1}(v)) = t v \alpha \pi \chi_\alpha(z) \prod_{i,j} p_i(z)^{r_{ij}} p_i(z) = z - \lambda_i \alpha U := \frac{\mathbb{C}[z]}{(z - \lambda)^r} J_{\lambda, r} (z - \lambda)^{r-1}, (z-\lambda)^{r-2}, \ldots, (z - \lambda)^0 = 1 U V V = \oplus_{i = 1}^s V^{\lambda_i}(\alpha) \beta := \alpha - \lambda \cdot I V^\lambda(\alpha) \beta \alpha J_{\lambda,-} V V U := \frac{\mathbb{C}[z]}{((z - \lambda_i)^{r_{ij}})} r_{ij} V_{\lambda_i}(\alpha) \pi^{-1}(U)","['linear-algebra', 'linear-transformations', 'jordan-normal-form', 'principal-ideal-domains']"
46,Finding a unit vector $v$ that makes only one quadratic form vanish,Finding a unit vector  that makes only one quadratic form vanish,v,"I was reading a proof on the non-convexity (even locally) of loss landscape in high-dimensional neural networks. Specifically, in the paper , it seems like the proof of proposition 2 at some point uses the following fact: Let $A,B \in \mathbb{R}^{m\times m}$ be symmetric matrices. Suppose that $\operatorname{rank}(A) \leq n$ and that $\operatorname{rank}(B) > 2n$ . Then we can always find a unit vector $v$ such that $v^{\intercal}Av = 0$ but $v^{\intercal}Bv \neq 0$ . Is there a reason we should expect such a $v$ to exist? If so, is the condition $\operatorname{rank}(B) > 2n$ necessary? Or is it just sufficient? Simultaneous vanishing of quadratic forms seems to be similar but not quite the same thing, and I wasn't able to find anything else about this. Any insight into this would be appreciated! Edit: Here's what I've come up with thus far. The problem is equivalent to showing that there exists a vector $v$ such that $v\in null(A)\cap col(B)$ . This is equivalent to showing that there is a zero eigenvalue to the generalized eigenvector problem: $$    Av = \lambda Bv   $$ With $\lambda = 0$ . Let $U = null(A)$ , $V = col(B)$ . Since $rank(A) \leq n$ , $dim(U) > m-n$ and $dim(V) > n$ . Thus, $$    dim(U+V) = dim(U) + dim(V) - dim(V\cap U) $$ Since $dim(U + V) \leq m$ and $dim(U) + dim(V) > m$ , then $dim(V \cap U) \geq 1$ , showing that $V \cap U \neq \emptyset$ .","I was reading a proof on the non-convexity (even locally) of loss landscape in high-dimensional neural networks. Specifically, in the paper , it seems like the proof of proposition 2 at some point uses the following fact: Let be symmetric matrices. Suppose that and that . Then we can always find a unit vector such that but . Is there a reason we should expect such a to exist? If so, is the condition necessary? Or is it just sufficient? Simultaneous vanishing of quadratic forms seems to be similar but not quite the same thing, and I wasn't able to find anything else about this. Any insight into this would be appreciated! Edit: Here's what I've come up with thus far. The problem is equivalent to showing that there exists a vector such that . This is equivalent to showing that there is a zero eigenvalue to the generalized eigenvector problem: With . Let , . Since , and . Thus, Since and , then , showing that .","A,B \in \mathbb{R}^{m\times m} \operatorname{rank}(A) \leq n \operatorname{rank}(B) > 2n v v^{\intercal}Av = 0 v^{\intercal}Bv \neq 0 v \operatorname{rank}(B) > 2n v v\in null(A)\cap col(B)     Av = \lambda Bv    \lambda = 0 U = null(A) V = col(B) rank(A) \leq n dim(U) > m-n dim(V) > n     dim(U+V) = dim(U) + dim(V) - dim(V\cap U)
 dim(U + V) \leq m dim(U) + dim(V) > m dim(V \cap U) \geq 1 V \cap U \neq \emptyset","['linear-algebra', 'quadratic-forms', 'generalized-eigenvector']"
47,When does a symmetric matrix admit a positive eigenvector (i.e. $v_i >0 \forall i$) for a positive eigenvalue?,When does a symmetric matrix admit a positive eigenvector (i.e. ) for a positive eigenvalue?,v_i >0 \forall i,"Let $S \in \mathbb{R}^{n \times n}$ be a symmetric matrix. I am trying to understand when such a matrix possesses a positive eigenvector for a positive eigenvalue, that is a $v \in \mathbb{R}^n, v_i >0 \forall i$ , $\lambda>0$ , such that $$Sv = \lambda v.$$ So, basically I am looking for a more general answer to this question . It is obvious that if we have one such eigenvector then by orthogonality the other eigenvectors have to have alternating signs. Also, one can see that in general $S$ should be similar to a matrix with constant row sum (see my question here ). But this holds in the general case and does not yet use anything about the symmetry of the matrix. Can we somehow make use of this special structure to find another necessary condition on $S$ , or can we maybe get some bounds on $\lambda$ ? Edit Of course one can get a very crude bound on the eigenvalues by the row sum of absolute values: Let $v$ be a positive eigenvector for eigenvalue $\lambda$ , $||v|| =1$ . Then $$\lambda v_i = \sum_j S_{ij} v_j \leq \sum_j |S_{ij}| v_j \leq \max_k v_k \sum_j |S_{ij}|$$ so $$ \lambda \leq \max_i \sum_j |S_{ij}|.$$ But this bound is rather useless, since for example $$S = \begin{pmatrix} n & -n+\epsilon \\ -n+\epsilon & n \end{pmatrix}$$ has eigenpair $\left( (1,1), \epsilon\right)$ , while the bound is $2n + \epsilon$ . Edit2 : After some more thought one can obtain a bound by the (signed) rowsums, which is better than the above: Let $v$ be positive eigenvector for $\lambda$ such that $||v||_1 = \sum_i v_i = 1$ . Then by summing up the equation above we get $$ \lambda \sum_i v_i = \sum_i (\sum_j S_{ij} v_j)  = \sum_i v_i (\sum_j S_{ij})$$ Now for a positive, unit $l_1$ -norm vector $v$ and another vector $w$ we have $\min_i w_i \leq v^Tw \leq \max_i w_i$ . Thus, with $w_i = (\sum_j S_{ij})$ we obtain $$ \min_i (\sum_j S_{ij}) \leq \lambda \leq \max_i (\sum_j S_{ij}),$$ which in our above example already determines $\epsilon$ . (In general this shows that for constant row sum $c$ this is the only possible eigenvalue with a positive eigenvector. In fact, one can also say that in the other direction, there only is a positive  eigenvector  corresponding to the eigenvalue $\lambda =  \max_i (\sum_j S_{ij})$ if the row-sum is constant: Since we required $v_i>0$ , the greater/lesser equal above only allows for equality when the row sum is the same for all $i$ ) Another (obvious) condition that $S$ has to fulfill, which does not seem to follow from the above, is that in every row, there has to be at least one positive entry (else for that row $\sum_j S_{ij} v_j \leq 0$ .)","Let be a symmetric matrix. I am trying to understand when such a matrix possesses a positive eigenvector for a positive eigenvalue, that is a , , such that So, basically I am looking for a more general answer to this question . It is obvious that if we have one such eigenvector then by orthogonality the other eigenvectors have to have alternating signs. Also, one can see that in general should be similar to a matrix with constant row sum (see my question here ). But this holds in the general case and does not yet use anything about the symmetry of the matrix. Can we somehow make use of this special structure to find another necessary condition on , or can we maybe get some bounds on ? Edit Of course one can get a very crude bound on the eigenvalues by the row sum of absolute values: Let be a positive eigenvector for eigenvalue , . Then so But this bound is rather useless, since for example has eigenpair , while the bound is . Edit2 : After some more thought one can obtain a bound by the (signed) rowsums, which is better than the above: Let be positive eigenvector for such that . Then by summing up the equation above we get Now for a positive, unit -norm vector and another vector we have . Thus, with we obtain which in our above example already determines . (In general this shows that for constant row sum this is the only possible eigenvalue with a positive eigenvector. In fact, one can also say that in the other direction, there only is a positive  eigenvector  corresponding to the eigenvalue if the row-sum is constant: Since we required , the greater/lesser equal above only allows for equality when the row sum is the same for all ) Another (obvious) condition that has to fulfill, which does not seem to follow from the above, is that in every row, there has to be at least one positive entry (else for that row .)","S \in \mathbb{R}^{n \times n} v \in \mathbb{R}^n, v_i >0 \forall i \lambda>0 Sv = \lambda v. S S \lambda v \lambda ||v|| =1 \lambda v_i = \sum_j S_{ij} v_j \leq \sum_j |S_{ij}| v_j \leq \max_k v_k \sum_j |S_{ij}|  \lambda \leq \max_i \sum_j |S_{ij}|. S = \begin{pmatrix} n & -n+\epsilon \\ -n+\epsilon & n \end{pmatrix} \left( (1,1), \epsilon\right) 2n + \epsilon v \lambda ||v||_1 = \sum_i v_i = 1  \lambda \sum_i v_i = \sum_i (\sum_j S_{ij} v_j)  = \sum_i v_i (\sum_j S_{ij}) l_1 v w \min_i w_i \leq v^Tw \leq \max_i w_i w_i = (\sum_j S_{ij})  \min_i (\sum_j S_{ij}) \leq \lambda \leq \max_i (\sum_j S_{ij}), \epsilon c \lambda =  \max_i (\sum_j S_{ij}) v_i>0 i S \sum_j S_{ij} v_j \leq 0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
48,Convergence of the Implicitly Restarted Arnoldi Method (IRAM),Convergence of the Implicitly Restarted Arnoldi Method (IRAM),,"I know that the Implicitly Restarted Arnoldi Method (IRAM) consists of using the implicitly shifted QR algorithm to restart the Arnoldi process without losing to much information about the eigenspace. In particular, taking $$p(t) = (t-k_1 I)\cdots(t-k_{m-k}I)$$ where $k_1,\dots,k_{m-k}$ are values we are not interested in, then $$\tilde{u}_1 = \frac{p(A)u_1}{\|p(A)u_1\|}$$ should be a good restarting vector. My question is whether the convergence of the Arnoldi method with this starting new vector is guaranteed by some theorem, or in general is just a good choice according to literature, or there are some sort of bound on the residual norm of the method, i.e $\|AV_ky -V_kH_ky\|$ and why it should converge to zero (where $y$ is the primitive Ritz vector). Any help in understanding this would be appreciated. Matrix $A$ is non-normal (any reference would be as well appreciated though).","I know that the Implicitly Restarted Arnoldi Method (IRAM) consists of using the implicitly shifted QR algorithm to restart the Arnoldi process without losing to much information about the eigenspace. In particular, taking where are values we are not interested in, then should be a good restarting vector. My question is whether the convergence of the Arnoldi method with this starting new vector is guaranteed by some theorem, or in general is just a good choice according to literature, or there are some sort of bound on the residual norm of the method, i.e and why it should converge to zero (where is the primitive Ritz vector). Any help in understanding this would be appreciated. Matrix is non-normal (any reference would be as well appreciated though).","p(t) = (t-k_1 I)\cdots(t-k_{m-k}I) k_1,\dots,k_{m-k} \tilde{u}_1 = \frac{p(A)u_1}{\|p(A)u_1\|} \|AV_ky -V_kH_ky\| y A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-methods', 'numerical-linear-algebra']"
49,What is the intuition behind a G-matrix? Any important applications in science and engineering?,What is the intuition behind a G-matrix? Any important applications in science and engineering?,,"In 2012, Miroslav Fiedler and Frank J. Hall (Linear Algebra) introduced a new class of matrices called as ""G-matrices"". In fact, a real square matrix $A$ is called a $G$ -matrix, if $A$ is nonsingular and there exist nonsingular diagonal matrices $D_1$ and $D_2$ such that $$ (A^{-1})^\top = D_1 A D_2 $$ They present a potpourri of results like all orthogonal matrices, diagonal matrices are $G$ -matrices. Otherwise, they don't give much insight into how they came up with this new class of matrices. The conclusion in the paper gives a summary of mathematical properties but not any ideas like where these $G$ -matrices can be applied. Is it correct to say that $G$ -matrices are some generalization of orthogonal matrices? G-matrix (2012) Paper Link","In 2012, Miroslav Fiedler and Frank J. Hall (Linear Algebra) introduced a new class of matrices called as ""G-matrices"". In fact, a real square matrix is called a -matrix, if is nonsingular and there exist nonsingular diagonal matrices and such that They present a potpourri of results like all orthogonal matrices, diagonal matrices are -matrices. Otherwise, they don't give much insight into how they came up with this new class of matrices. The conclusion in the paper gives a summary of mathematical properties but not any ideas like where these -matrices can be applied. Is it correct to say that -matrices are some generalization of orthogonal matrices? G-matrix (2012) Paper Link","A G A D_1 D_2 
(A^{-1})^\top = D_1 A D_2
 G G G","['linear-algebra', 'matrices']"
50,"Square summability of sequences of the form $\int_0^1 f(x)x^n\,dx$",Square summability of sequences of the form,"\int_0^1 f(x)x^n\,dx","For a function $f\in L^2(0,1)$ let $$a_n=\int_0^1 f(x)x^n\,dx.$$ Is the sequence $\{a_n\}_{n=0}^\infty$ square summable for any $f$ ? I have tried to prove or disprove that by testing  specific examples, but with no success. I have impression that the question might be associated with the boundedness of the Hilbert matrix $H=\{(n+m+1^{-1}\}_{n,m=0}^\infty $ on $\ell^2$ space, i.e. $$\|Ha\|_{\ell^2}\le \pi \|a\|_{\ell^2},\qquad a=\{a_n\}_{n=0}^\infty$$","For a function let Is the sequence square summable for any ? I have tried to prove or disprove that by testing  specific examples, but with no success. I have impression that the question might be associated with the boundedness of the Hilbert matrix on space, i.e.","f\in L^2(0,1) a_n=\int_0^1 f(x)x^n\,dx. \{a_n\}_{n=0}^\infty f H=\{(n+m+1^{-1}\}_{n,m=0}^\infty  \ell^2 \|Ha\|_{\ell^2}\le \pi \|a\|_{\ell^2},\qquad a=\{a_n\}_{n=0}^\infty","['linear-algebra', 'functional-analysis']"
51,What is the operator norm of the symmetric difference operator on a finite periodic lattice?,What is the operator norm of the symmetric difference operator on a finite periodic lattice?,,"Let \begin{equation} \Lambda_n:=\{ -\frac{1}{2}, -\frac{1}{2}+\frac{1}{2^n}, \cdots, 0 ,\cdots ,\frac{1}{2}-\frac{1}{2^n} , \frac{1}{2} \} \end{equation} be a finite lattice and think of the vector space over $\mathbb{C}$ defined as \begin{equation} V_n:=\{ \text{all }f: \Lambda_n \to \mathbb{C} \mid f\text{ is periodic } \} \end{equation} Then, since $\Lambda_n$ is a finite set, $V_n$ is clearly a finite dimensional vector space. We can give the inner product on $V_n$ in an obvious way as well. However, I am quite confused about the following linear operator: \begin{equation} D : V_n \to V_n  \end{equation} defined as \begin{equation} (Df)(x):=2^{n+1}[f(x+\frac{1}{2^n})-f(x-\frac{1}{2^n})] \end{equation} This is clearly a well-defined linear operator on $V_n$ and thus must have a matrix representation. I can see that it has no diagonal components so that its trace is always zero, regardless of $n$ . However, what about its operator norm? I am stuck at calculating it. Does the norm depend on $n$ ? Could anyone please help me?","Let be a finite lattice and think of the vector space over defined as Then, since is a finite set, is clearly a finite dimensional vector space. We can give the inner product on in an obvious way as well. However, I am quite confused about the following linear operator: defined as This is clearly a well-defined linear operator on and thus must have a matrix representation. I can see that it has no diagonal components so that its trace is always zero, regardless of . However, what about its operator norm? I am stuck at calculating it. Does the norm depend on ? Could anyone please help me?","\begin{equation}
\Lambda_n:=\{ -\frac{1}{2}, -\frac{1}{2}+\frac{1}{2^n}, \cdots, 0 ,\cdots ,\frac{1}{2}-\frac{1}{2^n} , \frac{1}{2} \}
\end{equation} \mathbb{C} \begin{equation}
V_n:=\{ \text{all }f: \Lambda_n \to \mathbb{C} \mid f\text{ is periodic } \}
\end{equation} \Lambda_n V_n V_n \begin{equation}
D : V_n \to V_n 
\end{equation} \begin{equation}
(Df)(x):=2^{n+1}[f(x+\frac{1}{2^n})-f(x-\frac{1}{2^n})]
\end{equation} V_n n n","['calculus', 'linear-algebra', 'functional-analysis']"
52,A few questions on Linear Algebra,A few questions on Linear Algebra,,"I actually posted this question a few weeks ago where I wanted my solutions to a few Linear Algebra questions checked. Now thanks to useful links provided by @GerryMyerson I can ask my questions (and verify my answers). The first three questions I just want solution verifications and suggestions for methods which are faster than the ones I used (also no answers were provided by the creator, so i'm not even sure of the correct answer). Question 1 : Show that the equation: $$x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0 $$ represents a sphere, and find its centre and radius. My Solution: $$x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0 $$ By completing the square we have that: $$((x+4)^2 -16) + ((y-3)^2 -9) + ((z+1)^2 -1) + 17 = 0 $$ Simplifying and collecting we have that: $$(x+4)^2 + (y-3)^2 + (z+1)^2 = 9 .$$ Since a sphere is an equation of the form: $$(x-x_0)^2 + (y-y_0)^2 + (z-z_0)^2 = r^2$$ where $x_0,y_0,z_0 = (-4,3,-1)$ are the coordinates of the centre of the sphere. Since $r^2 = 9$ the radius is $3$ . Our expression can be rewritten as: $$(x-(-4))^2 + (y-(3))^2 + (z-(-1))^2 = 0,$$ which is indeed the representation of a sphere (I think). Question 2: Find the area of the triangle with vertices $P(1,-2,3), Q(0,3,1)$ and $R(-1,1,0)$ . My Solution: Since $\vec{PQ} = \langle -1,5,-2 \rangle$ and $\vec{PQ} = \langle -2,3,-3 \rangle$ we take $\vec{PQ} \cdot \vec{QR}$ and have that: \begin{vmatrix} i & j & k\\  -1 & 5 & -2 \\  -2 & 3 & -3 \end{vmatrix} Simplifying we get: $-9i + j + 7k$ therefore $\vec{PQ} \cdot \vec{QR} = \langle -9, 1, 7\rangle$ . $\Vert \langle \vec{PQ} \cdot \vec{QR} \rangle \Vert = \sqrt{(-9)^2 + (1)^2 + (7)^2} = \sqrt{131}$ . Therefore using the formula $A = \frac{1}{2}\Vert \vec{v} \cdot \vec{u} \Vert$ we get: $$A = \frac{1}{2} \cdot \sqrt{131}$$ $$A = 0.5 \cdot 11.4455$$ $$A = 5.723$$ units squared. Question 3: Find the volume of the paralelpiped spanned by the vectors $a = \langle 1,2,3 \rangle$ , $b = \langle 0,1,-1 \rangle$ and $c = \textbf{i} + \textbf{j}$ My Solution: Vector $\textbf{c}$ can be rewritten as $\langle 1,1,0 \rangle$ . Taking the $3\times 3$ matrix we get that: $$\begin{vmatrix} 1 & 2 & 3\\  0 & 1 & -1 \\  1 & 1 & 0 \end{vmatrix} = 1 \begin{vmatrix} 1 & -1 \\  1 & 0 \end{vmatrix} - 2 \begin{vmatrix} 0 & -1 \\  1 & 0 \end{vmatrix} + 3 \begin{vmatrix} 0 & 1 \\  1 & 1 \end{vmatrix} = \vert -4 \vert$$ Simplifying the determinant we get that the volume is $4$ units cubed. Those were the three question's that I need method/solution verification and improvements (if needed) for. I am still a beginner so if you do have any advice could you please explain it in a more fundamental way.","I actually posted this question a few weeks ago where I wanted my solutions to a few Linear Algebra questions checked. Now thanks to useful links provided by @GerryMyerson I can ask my questions (and verify my answers). The first three questions I just want solution verifications and suggestions for methods which are faster than the ones I used (also no answers were provided by the creator, so i'm not even sure of the correct answer). Question 1 : Show that the equation: represents a sphere, and find its centre and radius. My Solution: By completing the square we have that: Simplifying and collecting we have that: Since a sphere is an equation of the form: where are the coordinates of the centre of the sphere. Since the radius is . Our expression can be rewritten as: which is indeed the representation of a sphere (I think). Question 2: Find the area of the triangle with vertices and . My Solution: Since and we take and have that: Simplifying we get: therefore . . Therefore using the formula we get: units squared. Question 3: Find the volume of the paralelpiped spanned by the vectors , and My Solution: Vector can be rewritten as . Taking the matrix we get that: Simplifying the determinant we get that the volume is units cubed. Those were the three question's that I need method/solution verification and improvements (if needed) for. I am still a beginner so if you do have any advice could you please explain it in a more fundamental way.","x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0  x^2 + y^2 + z^2 + 8x -6y + 2x + 17 = 0  ((x+4)^2 -16) + ((y-3)^2 -9) + ((z+1)^2 -1) + 17 = 0  (x+4)^2 + (y-3)^2 + (z+1)^2 = 9 . (x-x_0)^2 + (y-y_0)^2 + (z-z_0)^2 = r^2 x_0,y_0,z_0 = (-4,3,-1) r^2 = 9 3 (x-(-4))^2 + (y-(3))^2 + (z-(-1))^2 = 0, P(1,-2,3), Q(0,3,1) R(-1,1,0) \vec{PQ} = \langle -1,5,-2 \rangle \vec{PQ} = \langle -2,3,-3 \rangle \vec{PQ} \cdot \vec{QR} \begin{vmatrix}
i & j & k\\ 
-1 & 5 & -2 \\ 
-2 & 3 & -3
\end{vmatrix} -9i + j + 7k \vec{PQ} \cdot \vec{QR} = \langle -9, 1, 7\rangle \Vert \langle \vec{PQ} \cdot \vec{QR} \rangle \Vert = \sqrt{(-9)^2 + (1)^2 + (7)^2} = \sqrt{131} A = \frac{1}{2}\Vert \vec{v} \cdot \vec{u} \Vert A = \frac{1}{2} \cdot \sqrt{131} A = 0.5 \cdot 11.4455 A = 5.723 a = \langle 1,2,3 \rangle b = \langle 0,1,-1 \rangle c = \textbf{i} + \textbf{j} \textbf{c} \langle 1,1,0 \rangle 3\times 3 \begin{vmatrix}
1 & 2 & 3\\ 
0 & 1 & -1 \\ 
1 & 1 & 0
\end{vmatrix} = 1 \begin{vmatrix}
1 & -1 \\ 
1 & 0 \end{vmatrix} - 2 \begin{vmatrix}
0 & -1 \\ 
1 & 0 \end{vmatrix} + 3 \begin{vmatrix}
0 & 1 \\ 
1 & 1 \end{vmatrix} = \vert -4 \vert 4","['linear-algebra', 'matrices', 'solution-verification', 'vectors']"
53,This matrix defined by a polynomial $f$ has rank less than $\deg(f)+1$,This matrix defined by a polynomial  has rank less than,f \deg(f)+1,"Let $f\in \mathbb{R}[x]$ be a polynomial of degree $d$ . Let $x_1, \dots, x_n$ be real numbers, I want to show the matrix $A$ given by $A_{ij} = f(x_i + x_j)$ has rank $\le d+1$ . My attempt: I tried writing the polynomial as $f(t)=\sum_{k=0}^d c_k t^k$ , then I can write $A = \sum_{k=0}^d c_k B^{(k)},$ where $B^{(k)}$ is a matrix with entries $B^{(k)}_{ij} = (x_i+x_j)^k$ . But this does not seem to be helpful. Thank you in advance.","Let be a polynomial of degree . Let be real numbers, I want to show the matrix given by has rank . My attempt: I tried writing the polynomial as , then I can write where is a matrix with entries . But this does not seem to be helpful. Thank you in advance.","f\in \mathbb{R}[x] d x_1, \dots, x_n A A_{ij} = f(x_i + x_j) \le d+1 f(t)=\sum_{k=0}^d c_k t^k A = \sum_{k=0}^d c_k B^{(k)}, B^{(k)} B^{(k)}_{ij} = (x_i+x_j)^k",['linear-algebra']
54,An embedding of $U(n)\to \text{Spin}^c(2n)$,An embedding of,U(n)\to \text{Spin}^c(2n),"Let $(V,\langle,\rangle)$ be an $2n$ -dimensional real inner product space and consider its Spin $^c$ group $\text{Spin}^c(V)\subset Cl(V)\otimes_{\Bbb R} \Bbb C$ . Suppose there is a compatible (orthogonal) almost complex structure $J:V\to V$ . We can then view $V$ as an $n$ -dimensional complex vector space with a hermitian inner product $h$ given by $h(u,v)=\langle u,v\rangle -i\langle Ju,v\rangle$ . In particular we can consider the unitary group $U(V)$ . Define a map $\rho:U(V)\to \text{Spin}^c(V)$ as follows: Given $A\in U(V)$ , we can choose a unitary basis $v_1,\dots,v_n$ for $V$ such that $Av_k=e^{i\theta_k}v_k$ . Put $$\rho(A)=e^{(i\sum_k \theta_k)/2}\prod_{k=1}^n \left(\cos \frac{\theta_k}{2}+\sin\frac{\theta_k}{2}\cdot v_kJv_k \right)$$ In p.393 of Lawson & Michelson's book Spin Geometry , it is asserted that this map is a well-defined continuous group homomorphism, but I can't see why. For well-definedness, we have to care about two choices: the choices of $\theta_1,\dots,\theta_n$ , and the choice of basis. Clearly $\rho(A)$ is independent of the choice of the $\theta_k$ 's. Also it was quite easy to show that $\rho(A)$ is independent of the order of $v_1,\dots,v_n$ , because $v_kJv_k v_{k+1}Jv_{k+1}=v_{k+1}Jv_{k+1} v_kJv_k$ . So now if $\{v_1,\dots,v_n\}$ and $\{w_1,\dots,w_n\}$ are two unitary bases with $Av_k=e^{i\theta_k}v_k$ and $Aw_k=e^{i\psi_k}w_k$ , then we may assume $\theta_k=\psi_k$ for all $k$ . If the $\theta_k$ 's are all distinct (i.e. $A$ has $n$ distinct eigenvalues), then it is easy to see that $\rho(A)$ is well-defined. But I can't handle the case where $A$ has an eigenvalue with eigenspace $\dim>1$ . Also I can't see why $\rho$ is a continuous group homomorphism. Any helps will be very appreciated. Edit: There is a double cover $p:\text{Spin}^c(V)\to SO(V)\times S^1$ . Consider the natural map $f:U(V)\to SO(V)\times S^1$ given by $f(A)=(A,\det A)$ . Assuming $\rho$ is well-defined, I have shown that $\rho$ is a lift of $f$ , i.e. $p\rho=f$ . On the other hand, by covering space theory, there is a unique lift $\tilde{f}:U(V)\to SO(V)\times S^1$ of $f$ such that $\tilde{f}(\text{id})=1$ . Thus, if $\rho$ is continuous, then we must have $\rho=\tilde{f}$ .","Let be an -dimensional real inner product space and consider its Spin group . Suppose there is a compatible (orthogonal) almost complex structure . We can then view as an -dimensional complex vector space with a hermitian inner product given by . In particular we can consider the unitary group . Define a map as follows: Given , we can choose a unitary basis for such that . Put In p.393 of Lawson & Michelson's book Spin Geometry , it is asserted that this map is a well-defined continuous group homomorphism, but I can't see why. For well-definedness, we have to care about two choices: the choices of , and the choice of basis. Clearly is independent of the choice of the 's. Also it was quite easy to show that is independent of the order of , because . So now if and are two unitary bases with and , then we may assume for all . If the 's are all distinct (i.e. has distinct eigenvalues), then it is easy to see that is well-defined. But I can't handle the case where has an eigenvalue with eigenspace . Also I can't see why is a continuous group homomorphism. Any helps will be very appreciated. Edit: There is a double cover . Consider the natural map given by . Assuming is well-defined, I have shown that is a lift of , i.e. . On the other hand, by covering space theory, there is a unique lift of such that . Thus, if is continuous, then we must have .","(V,\langle,\rangle) 2n ^c \text{Spin}^c(V)\subset Cl(V)\otimes_{\Bbb R} \Bbb C J:V\to V V n h h(u,v)=\langle u,v\rangle -i\langle Ju,v\rangle U(V) \rho:U(V)\to \text{Spin}^c(V) A\in U(V) v_1,\dots,v_n V Av_k=e^{i\theta_k}v_k \rho(A)=e^{(i\sum_k \theta_k)/2}\prod_{k=1}^n \left(\cos \frac{\theta_k}{2}+\sin\frac{\theta_k}{2}\cdot v_kJv_k \right) \theta_1,\dots,\theta_n \rho(A) \theta_k \rho(A) v_1,\dots,v_n v_kJv_k v_{k+1}Jv_{k+1}=v_{k+1}Jv_{k+1} v_kJv_k \{v_1,\dots,v_n\} \{w_1,\dots,w_n\} Av_k=e^{i\theta_k}v_k Aw_k=e^{i\psi_k}w_k \theta_k=\psi_k k \theta_k A n \rho(A) A \dim>1 \rho p:\text{Spin}^c(V)\to SO(V)\times S^1 f:U(V)\to SO(V)\times S^1 f(A)=(A,\det A) \rho \rho f p\rho=f \tilde{f}:U(V)\to SO(V)\times S^1 f \tilde{f}(\text{id})=1 \rho \rho=\tilde{f}","['linear-algebra', 'abstract-algebra', 'group-theory', 'clifford-algebras', 'spin-geometry']"
55,Are all derivations of a matrix algebra inner derivations?,Are all derivations of a matrix algebra inner derivations?,,"Let $\mathcal{A}$ be a subalgebra of $M_n(\mathbb{R}).$ Let $$d:\mathcal{A} \to \mathcal{A}$$ be a linear map satisfying the Leibnitz rule $$d(XY)=d(X)\cdot Y-X\cdot d(Y)$$ for all $X,Y \in \mathcal{A}$ Is it true that $$d(X)=AX-XA$$ for some $A \in M_n(\mathbb{R})?$ In case $\mathcal{A}=M_n(\mathbb{R})$ the statement is well known to be true.",Let be a subalgebra of Let be a linear map satisfying the Leibnitz rule for all Is it true that for some In case the statement is well known to be true.,"\mathcal{A} M_n(\mathbb{R}). d:\mathcal{A} \to \mathcal{A} d(XY)=d(X)\cdot Y-X\cdot d(Y) X,Y \in \mathcal{A} d(X)=AX-XA A \in M_n(\mathbb{R})? \mathcal{A}=M_n(\mathbb{R})","['linear-algebra', 'matrices', 'lie-algebras']"
56,On the angle of a polar decomposition of a matrix,On the angle of a polar decomposition of a matrix,,"Let $A\in\mathbb{M}_{n\times n}(\mathbb{C})$ . $A$ is non-singular. Then $A=UR$ where $U$ is a unitary matrix, and $R$ is a positive-semidefinite Hermitian matrix. I understand the intuition that this is analogous to $U$ encoding some $\theta$ and $R$ encoding the modulus/radius $r$ , taking the complex plane in polar coordinates. I have seen it confidently written that $|A|=|U|\cdot|R|=\exp(i\theta)\cdot r$ . Intuitively, by the definition above, this should make sense, since any unit square/cube/etc. would be rotated and scaled under $A$ , and this determinant agrees with that. However, being more formal, I wonder how a person might prove this, or find $\theta.$ $|\exp(A)|=\exp(tr(A))$ , and also any unitary $U=\exp(iH)$ , where $H$ is Hermitian. Therefore $|U|=|\exp(iH)|=\exp(tr(iH))=\exp(i\cdot tr(H))$ . If $|U|$ is to be $\exp(i\theta)$ , then the mystery $H$ must have a trace equal to $\theta$ . Is this correct? As a complete side-question from the main focus of this post, is the finding of $H$ a messy problem almost always left to computers - can the trace= $\theta$ idea be leveraged for this? And when Wikipedia mentioned $U=\exp(iH)$ , did they mention it out of trivia, or because of some utility, some application of this fact?","Let . is non-singular. Then where is a unitary matrix, and is a positive-semidefinite Hermitian matrix. I understand the intuition that this is analogous to encoding some and encoding the modulus/radius , taking the complex plane in polar coordinates. I have seen it confidently written that . Intuitively, by the definition above, this should make sense, since any unit square/cube/etc. would be rotated and scaled under , and this determinant agrees with that. However, being more formal, I wonder how a person might prove this, or find , and also any unitary , where is Hermitian. Therefore . If is to be , then the mystery must have a trace equal to . Is this correct? As a complete side-question from the main focus of this post, is the finding of a messy problem almost always left to computers - can the trace= idea be leveraged for this? And when Wikipedia mentioned , did they mention it out of trivia, or because of some utility, some application of this fact?",A\in\mathbb{M}_{n\times n}(\mathbb{C}) A A=UR U R U \theta R r |A|=|U|\cdot|R|=\exp(i\theta)\cdot r A \theta. |\exp(A)|=\exp(tr(A)) U=\exp(iH) H |U|=|\exp(iH)|=\exp(tr(iH))=\exp(i\cdot tr(H)) |U| \exp(i\theta) H \theta H \theta U=\exp(iH),"['linear-algebra', 'unitary-matrices', 'hermitian-matrices']"
57,Dimension free gram matrix inner product,Dimension free gram matrix inner product,,"Let $\{x_i\}_{i = 1}^n$ be $n$ vectors of $d$ dimesnions. We stack each $x_i$ as a row vector to form a matrix $X$ of dimension $\mathbb{R}^{n\times d}$ Let $\{y_i\}_{i = 1}^n$ be scalars (say all are $1$ ) and these are stacked to form a matrix $Y$ of dimension $\mathbb{R}^{n\times 1}$ We compute the following inner product $$Y^TX(X^TX+ \lambda I)^{-2}X^TY$$ where $\lambda \gt 0$ . Can we show that this seems that does not grow at an order of $n$ . Experimantally it appears so For $d = 3$ and $n$ running from $1$ to $1000$ we have For $d = 10$ we have and for $d=100$ we have So it seems that for $n$ much greater than $d$ it seems that the hunch is correct. Any idea on how to proceed. Thanks EDIT 1: Here is a proof that $$Y^TX(X^TX+ \lambda I)^{-1}X^TY$$ is $O(n)$ Let $X = U\Sigma V^T$ Then $$Y^TX(X^TX+ \lambda I)^{-1}X^TY = Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY$$ . Now we can note that diagonal elements of $\Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T$ is less than 1. Therefore $$Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY \leq Y^T UU^T Y$$ which is less than $n\|Y\|^2_\infty$ My hunch is that $Y^TX(X^TX+ \lambda I)^{-2}X^TY$ will be less than $d\|Y\|^2_\infty$ . This is easy if $X^T X$ is diagonal. But is it true for general. EDIT 2: This is another line of thought I was having, $$Y^TX(X^TX+ \lambda I)^{-2}X^TY \leq n\|Y\|^2_\infty tr(XX^T) tr ((X^TX)^{-2})$$ Now $tr(XX^T) = O(n)$ . If I can say $tr ((X^TX + \lambda I)^{-2})$ is $O(n^{-2})$ Then I can be done. But is this true? EDIT 3: So this is a line of thought from the previous update Let $\lambda_1,...\lambda_d$ be the eigenvalues of $X^TX + \lambda I$ . Then the eigen values of $(X^TX + \lambda I)^{-2}$ are $\frac{1}{\lambda_1^2},...\frac{1}{\lambda_d^2}$ . So we we have the following, $$\lambda_1 + \cdots + \lambda_d = O(n)$$ , $\lambda_1,...\lambda_d$ are strictly positive. So we can have $$\lambda_1^2 + \cdots + \lambda_d^2 = O(n^2)$$ . Can we write from this step $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = O(1/n^2)$$ ? EDIT 4: I can show that $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = \Omega(1/n^2)$$ $$\frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} \geq \frac{d^2}{\lambda_1^2 + \cdots + \lambda_d^2}$$ by AM-HM inequality which is $\Omega(1/n^2)$ . So we have $trace((X^TX + \lambda I)^{-2})$ of the order $\Omega(1/n^2)$ . Therefore $$Y^TX(X^TX+ \lambda I)^{-2}X^TY = O(n^2)\Omega(1/n^2)$$ Are my calculations correct? Can we have anything from here?","Let be vectors of dimesnions. We stack each as a row vector to form a matrix of dimension Let be scalars (say all are ) and these are stacked to form a matrix of dimension We compute the following inner product where . Can we show that this seems that does not grow at an order of . Experimantally it appears so For and running from to we have For we have and for we have So it seems that for much greater than it seems that the hunch is correct. Any idea on how to proceed. Thanks EDIT 1: Here is a proof that is Let Then . Now we can note that diagonal elements of is less than 1. Therefore which is less than My hunch is that will be less than . This is easy if is diagonal. But is it true for general. EDIT 2: This is another line of thought I was having, Now . If I can say is Then I can be done. But is this true? EDIT 3: So this is a line of thought from the previous update Let be the eigenvalues of . Then the eigen values of are . So we we have the following, , are strictly positive. So we can have . Can we write from this step ? EDIT 4: I can show that by AM-HM inequality which is . So we have of the order . Therefore Are my calculations correct? Can we have anything from here?","\{x_i\}_{i = 1}^n n d x_i X \mathbb{R}^{n\times d} \{y_i\}_{i = 1}^n 1 Y \mathbb{R}^{n\times 1} Y^TX(X^TX+ \lambda I)^{-2}X^TY \lambda \gt 0 n d = 3 n 1 1000 d = 10 d=100 n d Y^TX(X^TX+ \lambda I)^{-1}X^TY O(n) X = U\Sigma V^T Y^TX(X^TX+ \lambda I)^{-1}X^TY = Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T Y^T U \Sigma (\Sigma^T\Sigma + \lambda I)^{-1}\Sigma^T U^TY \leq Y^T UU^T Y n\|Y\|^2_\infty Y^TX(X^TX+ \lambda I)^{-2}X^TY d\|Y\|^2_\infty X^T X Y^TX(X^TX+ \lambda I)^{-2}X^TY \leq n\|Y\|^2_\infty tr(XX^T) tr ((X^TX)^{-2}) tr(XX^T) = O(n) tr ((X^TX + \lambda I)^{-2}) O(n^{-2}) \lambda_1,...\lambda_d X^TX + \lambda I (X^TX + \lambda I)^{-2} \frac{1}{\lambda_1^2},...\frac{1}{\lambda_d^2} \lambda_1 + \cdots + \lambda_d = O(n) \lambda_1,...\lambda_d \lambda_1^2 + \cdots + \lambda_d^2 = O(n^2) \frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = O(1/n^2) \frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} = \Omega(1/n^2) \frac{1}{\lambda_1^2}+ \cdots +\frac{1}{\lambda_d^2} \geq \frac{d^2}{\lambda_1^2 + \cdots + \lambda_d^2} \Omega(1/n^2) trace((X^TX + \lambda I)^{-2}) \Omega(1/n^2) Y^TX(X^TX+ \lambda I)^{-2}X^TY = O(n^2)\Omega(1/n^2)","['linear-algebra', 'functional-analysis', 'matrix-decomposition', 'linear-regression']"
58,Using dimension formula to prove subspace dimension formula,Using dimension formula to prove subspace dimension formula,,"I've always suspected the formulas $\dim F+G = \dim F + \dim G - \dim F\cap G$ and $\dim V = \dim  \mathrm{Ker} T + \dim \mathrm{Im} T$ were related someway. So I tried proving the former using the latter. It is easy to see that the direct outer product $F\times G$ has dimension the sum of dimensions of $F$ and $G$ . Define $\pi: F\times G \twoheadrightarrow F+G$ by $\pi(f,g) = f+g$ . It is clearly surjective. I claim that the kernel of $\pi$ is the subspace $H$ of $(F\cap G)^2$ of elements of form $(f, -f)$ , and therefore of dimension equal to the one of $F\cap G$ . Therefore the former result results from applying the dimension formula to $\pi$ . Is this correct?","I've always suspected the formulas and were related someway. So I tried proving the former using the latter. It is easy to see that the direct outer product has dimension the sum of dimensions of and . Define by . It is clearly surjective. I claim that the kernel of is the subspace of of elements of form , and therefore of dimension equal to the one of . Therefore the former result results from applying the dimension formula to . Is this correct?","\dim F+G = \dim F + \dim G - \dim F\cap G \dim V = \dim  \mathrm{Ker} T + \dim \mathrm{Im} T F\times G F G \pi: F\times G \twoheadrightarrow F+G \pi(f,g) = f+g \pi H (F\cap G)^2 (f, -f) F\cap G \pi","['linear-algebra', 'linear-transformations']"
59,Relationship between the inverse of the right eigenvector of a matrix $A$ and the transpose of the right eigenvector of $A^T$,Relationship between the inverse of the right eigenvector of a matrix  and the transpose of the right eigenvector of,A A^T,"I should start by stating that this question is related to the Koopman's theory in dynamical system, which gathers growing interests in the fluid dynamics/data science areas such as dynamic mode decomposition (DMD). Suppose $A$ has the eigen decomposition $P_1DP_1^{-1}$ where the columns of $P_1$ are the right eigenvectors of $A$ . Then $A^{\text{T}}$ has an eigen decomposition $P_2DP_2^{-1}$ where columns of $P_2$ are the right eigenvectors of $A^{\text{T}}$ . We can write down these relations explicitly \begin{align} AP_1&=P_1D\,,\\ A^{\text{T}}P_2&=P_2D\,. \end{align} From these equations, there are two ways of getting the left eigenvectors, namely \begin{align} P_1^{-1}A&=DP_1^{-1}\,,\\ P_2^{\text{T}}A&=DP_2^{\text{T}}\,. \end{align} Therefore, the rows in both $P_1^{-1}$ and $P_2^{\text{T}}$ are the left eigenvectors of $A$ , but $P_1^{-1}\neq P_2^{\text{T}}$ in general. I believe \begin{equation} P=aP_1^{-1}+bP_2^{\text{T}}\,, \end{equation} is also a left eigenvector of $A$ for any scalars $a$ and $b$ that are not both zeros. My questions are why the matrix $P$ is singular for $a=-b$ even though they are not equal (my guess is something to do with linear dependency), and what is the relationship between $P_1^{-1}$ and $P_2^{\text{T}}$ since both were derived using valid approaches for finding the left eigenvectors of the matrix $A$ . I asked this question because the Koopman eigenfunction is found by multiplying the left eigen vectors of the Koopman operator with its invariant subspace (linear variables derived from the original nonlinear variables). And I wish to understand whether the approach for obtaining the left eigenvectors of the Koopman operator matters in terms of the final Koopman eigenfunction. I would appreciate if anyone could help me on this. Thanks a lot!","I should start by stating that this question is related to the Koopman's theory in dynamical system, which gathers growing interests in the fluid dynamics/data science areas such as dynamic mode decomposition (DMD). Suppose has the eigen decomposition where the columns of are the right eigenvectors of . Then has an eigen decomposition where columns of are the right eigenvectors of . We can write down these relations explicitly From these equations, there are two ways of getting the left eigenvectors, namely Therefore, the rows in both and are the left eigenvectors of , but in general. I believe is also a left eigenvector of for any scalars and that are not both zeros. My questions are why the matrix is singular for even though they are not equal (my guess is something to do with linear dependency), and what is the relationship between and since both were derived using valid approaches for finding the left eigenvectors of the matrix . I asked this question because the Koopman eigenfunction is found by multiplying the left eigen vectors of the Koopman operator with its invariant subspace (linear variables derived from the original nonlinear variables). And I wish to understand whether the approach for obtaining the left eigenvectors of the Koopman operator matters in terms of the final Koopman eigenfunction. I would appreciate if anyone could help me on this. Thanks a lot!","A P_1DP_1^{-1} P_1 A A^{\text{T}} P_2DP_2^{-1} P_2 A^{\text{T}} \begin{align}
AP_1&=P_1D\,,\\
A^{\text{T}}P_2&=P_2D\,.
\end{align} \begin{align}
P_1^{-1}A&=DP_1^{-1}\,,\\
P_2^{\text{T}}A&=DP_2^{\text{T}}\,.
\end{align} P_1^{-1} P_2^{\text{T}} A P_1^{-1}\neq P_2^{\text{T}} \begin{equation}
P=aP_1^{-1}+bP_2^{\text{T}}\,,
\end{equation} A a b P a=-b P_1^{-1} P_2^{\text{T}} A","['linear-algebra', 'eigenvalues-eigenvectors', 'dynamical-systems', 'data-analysis', 'eigenfunctions']"
60,Prove that $\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)$ is independent of $B$,Prove that  is independent of,\mathrm{Tr}(B^\mathsf{T}Y^{-1}B) B,"Given diagonal $A\in\mathbb{R}^{n\times n}$ with all eigenvalues larger than $1$ , and minimal polynomial $\alpha(\lambda)$ . Matrix is called cyclic if its minimal polynomial is equal to characteristic polynomial. Here $A=\begin{bmatrix} A_1 & \\  & A_2 \end{bmatrix}$ , where $A_i$ , for $i=1,2,$ are cyclic with minimal polynomials $\alpha_i(\lambda)$ , such that $\alpha_1(\lambda)=\alpha(\lambda)$ and $\alpha_2(\lambda)$ divides $\alpha_1(\lambda)$ . For example: $A=\mathrm{diag}(5,4,3,2,3,2)$ , here $A_1=\begin{bmatrix} 5 & &&\\  & 4&&\\  & &3&\\  & &&2 \end{bmatrix}$ and $A_2=\begin{bmatrix} 3 & \\  & 2 \end{bmatrix}$ . Basically, $A_2$ collects all repeating diagonal elements, but $A$ cannot have more that $2$ same diagonal elements. $A=\mathrm{diag}(2,2,2)$ is not possible. And we assume $\mathrm{rank}[\lambda I-A \quad B]=n$ for all $\lambda\in\mathbb{R^+}$ , i.e. $(A,B)$ is stabilizable. If $BB^\mathsf{T}=AYA-Y$ where $B\in\mathbb{R}^{n\times2}$ , prove that $\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)$ is independent of $B$ . My attempt: For special case when all eigenvalues of $A$ are equal to $a$ , we have $\mathrm{vec}(BB^\mathsf{T}) = (A \otimes A - I)\mathrm{vec}(Y)$ , where $\otimes$ denote the Kronecker product. And since $(A \otimes A - I)$ is nonsingular, we have \begin{align} \mathrm{vec}(Y) &= (A \otimes A - I)^{-1}\mathrm{vec}(BB^\mathsf{T})\\ &=\frac{1}{a^2-1}\mathrm{vec}(BB^\mathsf{T})\\ \end{align} Which means $Y=\frac{1}{a^2-1}BB^\mathsf{T}$ and $$\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{Tr}(BB^\mathsf{T}Y^{-1})=\mathrm{Tr}(BB^\mathsf{T}(a^2-1)(BB^\mathsf{T})^{-1})=2(a^2-1).$$ I think $BB^\mathsf{T}$ is singular, but maybe we can take pseudo-inverse. Conjecture: For general case \begin{align} \mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{det}(A_1)+\mathrm{det}(A_2)-2, \end{align} Matlab simulation agrees with the conjecture. I have edited the question. Since this math problem arises from engineering problem, I think my initial question was not clear for mathematical audience. I hope now it is self-containing.","Given diagonal with all eigenvalues larger than , and minimal polynomial . Matrix is called cyclic if its minimal polynomial is equal to characteristic polynomial. Here , where , for are cyclic with minimal polynomials , such that and divides . For example: , here and . Basically, collects all repeating diagonal elements, but cannot have more that same diagonal elements. is not possible. And we assume for all , i.e. is stabilizable. If where , prove that is independent of . My attempt: For special case when all eigenvalues of are equal to , we have , where denote the Kronecker product. And since is nonsingular, we have Which means and I think is singular, but maybe we can take pseudo-inverse. Conjecture: For general case Matlab simulation agrees with the conjecture. I have edited the question. Since this math problem arises from engineering problem, I think my initial question was not clear for mathematical audience. I hope now it is self-containing.","A\in\mathbb{R}^{n\times n} 1 \alpha(\lambda) A=\begin{bmatrix}
A_1 & \\
 & A_2
\end{bmatrix} A_i i=1,2, \alpha_i(\lambda) \alpha_1(\lambda)=\alpha(\lambda) \alpha_2(\lambda) \alpha_1(\lambda) A=\mathrm{diag}(5,4,3,2,3,2) A_1=\begin{bmatrix}
5 & &&\\
 & 4&&\\
 & &3&\\
 & &&2
\end{bmatrix} A_2=\begin{bmatrix}
3 & \\
 & 2
\end{bmatrix} A_2 A 2 A=\mathrm{diag}(2,2,2) \mathrm{rank}[\lambda I-A \quad B]=n \lambda\in\mathbb{R^+} (A,B) BB^\mathsf{T}=AYA-Y B\in\mathbb{R}^{n\times2} \mathrm{Tr}(B^\mathsf{T}Y^{-1}B) B A a \mathrm{vec}(BB^\mathsf{T}) = (A \otimes A - I)\mathrm{vec}(Y) \otimes (A \otimes A - I) \begin{align}
\mathrm{vec}(Y) &= (A \otimes A - I)^{-1}\mathrm{vec}(BB^\mathsf{T})\\
&=\frac{1}{a^2-1}\mathrm{vec}(BB^\mathsf{T})\\
\end{align} Y=\frac{1}{a^2-1}BB^\mathsf{T} \mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{Tr}(BB^\mathsf{T}Y^{-1})=\mathrm{Tr}(BB^\mathsf{T}(a^2-1)(BB^\mathsf{T})^{-1})=2(a^2-1). BB^\mathsf{T} \begin{align}
\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{det}(A_1)+\mathrm{det}(A_2)-2,
\end{align}","['linear-algebra', 'trace', 'optimal-control', 'kronecker-product']"
61,Proving that the determinant of a real symplectic matrix is $1$ from its eigenvalues,Proving that the determinant of a real symplectic matrix is  from its eigenvalues,1,"Let $A$ be a $2n \times 2n$ real symplectic matrix, i.e., $A$ satisfies $$A^TJA = J$$ where $J = \begin{pmatrix} 0 & I_n\\ -I_n & 0 \end{pmatrix}$ . It is a well-known fact that $\det(A) = 1$ . There are several proofs of this, including elementary ones such as outlined here and here . However, I would like to know if there is a direct way of showing that $\det(A) = 1$ from its eigenvalues, and none of the methods I found employed such an argument. If $\lambda$ is an eigenvalue of $A$ , then it can be shown that $\bar{\lambda}$ , $\lambda^{-1}$ , and $\bar{\lambda}^{-1}$ are also eigenvalues of $A$ (they need not be distinct from each other). At first glance, this observation seems to suggest that the eigenvalues cancel each other out when multiplied together, so that the determinant of $A$ is $1$ . Nevertheless, I have yet to find a rigorous formulation of this argument. Besides, this property of eigenvalues alone seems to be insufficient to prove our claim. As an illustration of this inadequacy, suppose $A$ is a $2 \times 2$ real symplectic matrix with eigenvalues $1$ and $-1$ . Then they satisfy the property outlined above, but this implies that $\det(A) = -1$ (of course $A$ is not actually a symplectic matrix in this case). I think there are other properties of $A$ that need to be considered so that we can use the property of eigenvalues to prove the claim that $\det(A) = 1$ . In conclusion, my question is: Can we prove that $\det(A)$ by utilizing the observation that the eigenvalues of $A$ come in ''quadruplets'' of $\lambda$ , $\bar{\lambda}$ , $\lambda^{-1}$ , and $\bar{\lambda}^{-1}$ ? An elementary argument with linear algebra is preferred (without Pfaffians or manifold theory), but I welcome any insight.","Let be a real symplectic matrix, i.e., satisfies where . It is a well-known fact that . There are several proofs of this, including elementary ones such as outlined here and here . However, I would like to know if there is a direct way of showing that from its eigenvalues, and none of the methods I found employed such an argument. If is an eigenvalue of , then it can be shown that , , and are also eigenvalues of (they need not be distinct from each other). At first glance, this observation seems to suggest that the eigenvalues cancel each other out when multiplied together, so that the determinant of is . Nevertheless, I have yet to find a rigorous formulation of this argument. Besides, this property of eigenvalues alone seems to be insufficient to prove our claim. As an illustration of this inadequacy, suppose is a real symplectic matrix with eigenvalues and . Then they satisfy the property outlined above, but this implies that (of course is not actually a symplectic matrix in this case). I think there are other properties of that need to be considered so that we can use the property of eigenvalues to prove the claim that . In conclusion, my question is: Can we prove that by utilizing the observation that the eigenvalues of come in ''quadruplets'' of , , , and ? An elementary argument with linear algebra is preferred (without Pfaffians or manifold theory), but I welcome any insight.",A 2n \times 2n A A^TJA = J J = \begin{pmatrix} 0 & I_n\\ -I_n & 0 \end{pmatrix} \det(A) = 1 \det(A) = 1 \lambda A \bar{\lambda} \lambda^{-1} \bar{\lambda}^{-1} A A 1 A 2 \times 2 1 -1 \det(A) = -1 A A \det(A) = 1 \det(A) A \lambda \bar{\lambda} \lambda^{-1} \bar{\lambda}^{-1},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'symplectic-linear-algebra']"
62,Finding all $T$-invariant subspaces of a $T$-cyclic vector space,Finding all -invariant subspaces of a -cyclic vector space,T T,"Let $V$ be a $4$ -dimensional real vector space and let $T:V \rightarrow V$ be an endomorphism such that its characteristic polynomial $\chi_T$ is given by $\chi_T = (t-2)^3t$ . Suppose also that $V$ is $T$ -cylic, that is to say, $V = \{p(T)(v_0): p(t) \in \mathbb R[t] \}$ for some $v_0 \in V$ . Find all $T$ -invariant subspaces of $V$ . Let $W\subseteq V$ be a $T$ -invariant subspace. By previous results, recall that the induced endomorphism in the quotient space $\tilde T: V/W \rightarrow V/W$ and the restriction endomorphism $T_W:W \rightarrow W$ are such that $\chi_T = \chi_{T_W} \cdot \chi_{\tilde T}$ For $p \in \mathbb R[t]$ , write $V_{p}$ to denote $V_p = \{v\in V: p(T)(v) = 0\}$ . Investigating the $T$ -invariant subspaces by its dimension (the $0$ -dimensional and $4$ -dimensional being the trivial ones), the condition $\chi_T = \chi_{T_W} \cdot \chi_{\tilde T}$ gives us the following possibilities $\chi_{T_W} \in \{ t-2, t\} $ for $W$ 1-dimensional, $\chi_{T_W} \in \{ (t-2)t, (t-2)^2\}$ for $W$ 2-dimensional, $\chi_{T_W} \in \{t(t-2)^2, (t-2)^3\}$ for $W$ 3-dimensional. Now, since $V$ is $T$ -cyclic, all its $T$ -invariant subspaces are cyclic as well (see Invariant subspace of cyclic space is cyclic ). In particular, the minimal polynomial of $T_W$ equals $\chi_{T_W}.$ It seems like this conditions is able to ensure that the $T$ -invariant spaces are $V_{t-2}, V_t$ (1-dimensional), $ V_{(t-2)t}, V_{(t-2)^2}$ (2-dimensional) and $V_{t(t-2)^2}, V_{(t-2)^3}$ (3-dimensional), but I'm not sure on how to prove this. Am I going in the right direction?","Let be a -dimensional real vector space and let be an endomorphism such that its characteristic polynomial is given by . Suppose also that is -cylic, that is to say, for some . Find all -invariant subspaces of . Let be a -invariant subspace. By previous results, recall that the induced endomorphism in the quotient space and the restriction endomorphism are such that For , write to denote . Investigating the -invariant subspaces by its dimension (the -dimensional and -dimensional being the trivial ones), the condition gives us the following possibilities for 1-dimensional, for 2-dimensional, for 3-dimensional. Now, since is -cyclic, all its -invariant subspaces are cyclic as well (see Invariant subspace of cyclic space is cyclic ). In particular, the minimal polynomial of equals It seems like this conditions is able to ensure that the -invariant spaces are (1-dimensional), (2-dimensional) and (3-dimensional), but I'm not sure on how to prove this. Am I going in the right direction?","V 4 T:V \rightarrow V \chi_T \chi_T = (t-2)^3t V T V = \{p(T)(v_0): p(t) \in \mathbb R[t] \} v_0 \in V T V W\subseteq V T \tilde T: V/W \rightarrow V/W T_W:W \rightarrow W \chi_T = \chi_{T_W} \cdot \chi_{\tilde T} p \in \mathbb R[t] V_{p} V_p = \{v\in V: p(T)(v) = 0\} T 0 4 \chi_T = \chi_{T_W} \cdot \chi_{\tilde T} \chi_{T_W} \in \{ t-2, t\}  W \chi_{T_W} \in \{ (t-2)t, (t-2)^2\} W \chi_{T_W} \in \{t(t-2)^2, (t-2)^3\} W V T T T_W \chi_{T_W}. T V_{t-2}, V_t  V_{(t-2)t}, V_{(t-2)^2} V_{t(t-2)^2}, V_{(t-2)^3}",['linear-algebra']
63,A vector space $V$ such that $V\otimes V \not \cong B(V^*)$,A vector space  such that,V V\otimes V \not \cong B(V^*),"Let $V$ be a vector space (not necessarily finite-dimensional) over a field $\mathbb F$ . Let $B(V^*)$ denote the vector space of all bilinear forms $V^*\times V^* \rightarrow \mathbb F$ . It is ""well-known"" that the linear map $\phi: V\otimes V \rightarrow B(V^*)$ such that $$\phi(v \otimes w)(f,g) = f(v)g(w)$$ for all $v,w\in V$ and $f,g\in V^*$ establishes an isomorphism $V\otimes V \cong B(V^*)$ when $V$ is finite-dimensional. I want to prove that this hyphotesis cannot be dropped , that is, I'm looking forward a vector space $V$ of infinite dimension such that $V\otimes V \not \cong B(V^*).$ I am not familiar at all with infinite-dimensional vector spaces, besides the classical ones $\mathbb F[x]$ and $\mathscr C([0,1],\mathbb R)$ , for instance. Which vector space should I look for? Is this problem more related to the cardinality of basis, in the sense that $\dim B(V^*)$ is also uncountable if $\dim V^*$ is uncountable? Any help is very much appreciated.","Let be a vector space (not necessarily finite-dimensional) over a field . Let denote the vector space of all bilinear forms . It is ""well-known"" that the linear map such that for all and establishes an isomorphism when is finite-dimensional. I want to prove that this hyphotesis cannot be dropped , that is, I'm looking forward a vector space of infinite dimension such that I am not familiar at all with infinite-dimensional vector spaces, besides the classical ones and , for instance. Which vector space should I look for? Is this problem more related to the cardinality of basis, in the sense that is also uncountable if is uncountable? Any help is very much appreciated.","V \mathbb F B(V^*) V^*\times V^* \rightarrow \mathbb F \phi: V\otimes V \rightarrow B(V^*) \phi(v \otimes w)(f,g) = f(v)g(w) v,w\in V f,g\in V^* V\otimes V \cong B(V^*) V V V\otimes V \not \cong B(V^*). \mathbb F[x] \mathscr C([0,1],\mathbb R) \dim B(V^*) \dim V^*","['linear-algebra', 'examples-counterexamples']"
64,Trace norm of a complex $|B| \times|A|$ matrix $M$.,Trace norm of a complex  matrix .,|B| \times|A| M,"Let $A$ and $B$ be two finite-dimensional Hilbert spaces, and let $M: A \rightarrow B$ be a $|B| \times|A|$ complex matrix. Show that the trace norm of $M$ can be expressed as $$ \|M\|_{1}=\max _{V: B \rightarrow A} \operatorname{Tr}[V M] $$ where the maximum is over all partial isometries $V: B \rightarrow A$ . $\mathbf{NOTE:}$ $V$ is a partial isometry if $V^*V=I^B$ or $VV^*=I^A$ . $\mathbf{ATTEMPT:}$ I have two ideas, one of them is that we have to show that $\max _{V: B \rightarrow A} \operatorname{Tr}[V M] \geqslant \|M\|_{1}$ and $\max _{V: B \rightarrow A} \operatorname{Tr}[V M] \leqslant \|M\|_{1}$ . So then we can conclude $\|M\|_{1}=\max _{V: B \rightarrow A} \operatorname{Tr}[V M]$ . "" $\leqslant$ "":The singular values of $M$ are $\mu_1 \geqslant \mu_2 \geqslant \cdots \geqslant \mu_n$ and the singular values of $V$ are $\nu_1 \geqslant \nu_2 \geqslant \cdots \geqslant \nu_n$ .  Now, using von Neumann's trace inequality: $$\operatorname{Tr}[V M] \leqslant \sum_{i=1}^{n} \mu_i \nu_i \leqslant \max _{i \in \{1,\cdots,n\}} \{\nu_i\}\sum_{i=1}^{n} \mu_i \leqslant \nu_{max} \|M\|_{1} \leqslant \|M\|_{1}$$ But for the other direction I have no idea and I'm not sure that the above proof is true. I should mention that I'm also struggling to understand the von Neumann's trace inequality. Could you please help me to complete the proof or give me an alternative proof? Thanks","Let and be two finite-dimensional Hilbert spaces, and let be a complex matrix. Show that the trace norm of can be expressed as where the maximum is over all partial isometries . is a partial isometry if or . I have two ideas, one of them is that we have to show that and . So then we can conclude . "" "":The singular values of are and the singular values of are .  Now, using von Neumann's trace inequality: But for the other direction I have no idea and I'm not sure that the above proof is true. I should mention that I'm also struggling to understand the von Neumann's trace inequality. Could you please help me to complete the proof or give me an alternative proof? Thanks","A B M: A \rightarrow B |B| \times|A| M 
\|M\|_{1}=\max _{V: B \rightarrow A} \operatorname{Tr}[V M]
 V: B \rightarrow A \mathbf{NOTE:} V V^*V=I^B VV^*=I^A \mathbf{ATTEMPT:} \max _{V: B \rightarrow A} \operatorname{Tr}[V M] \geqslant \|M\|_{1} \max _{V: B \rightarrow A} \operatorname{Tr}[V M] \leqslant \|M\|_{1} \|M\|_{1}=\max _{V: B \rightarrow A} \operatorname{Tr}[V M] \leqslant M \mu_1 \geqslant \mu_2 \geqslant \cdots \geqslant \mu_n V \nu_1 \geqslant \nu_2 \geqslant \cdots \geqslant \nu_n \operatorname{Tr}[V M] \leqslant \sum_{i=1}^{n} \mu_i \nu_i \leqslant \max _{i \in \{1,\cdots,n\}} \{\nu_i\}\sum_{i=1}^{n} \mu_i \leqslant \nu_{max} \|M\|_{1} \leqslant \|M\|_{1}","['linear-algebra', 'matrices', 'normed-spaces', 'mathematical-physics', 'trace']"
65,Invertible matrix whos square is not invertible?,Invertible matrix whos square is not invertible?,,"Of course, this scenario is impossible for finite dimensional matrices. I am trying to understand what it means for an unbounded linear operator to have a well defined inverse, but the square of its inverse does not exist. It's pretty easy to find examples of this happening. Take the matrix $$M_{nm}=2\delta_{nm}-\delta_{n+1m}-\delta_{n-1m}$$ Defined for $n,m\geq 1$ . This has the inverse $M^{-1}_{nm}=\text{min}(n,m)$ . The square of the inverse obviously diverges and so does not exist. The square goes like $M^{-2}_{nm}\sim nm/3*N$ where $N$ is the size of the matrix. What does this mean for the matrix $M^2$ ? Does $M^2$ somehow pick up a zero eigenvalue that $M$ didn't have? Edit 1: I'd like to say also that $M$ itself can be said to have zero eigenvalues, with eigensequence $a_n=n$ , but if we define everything to be in the Hilbert space $l^2$ of finite norm sequences $\sum_{n=1}^{\infty}|a_n|^2<\infty$ , then those sequences don't belong to the space, hence why we are able to define an inverse. Perhaps theres something about the incompatibility of the linear operator $M$ with the Hilbert space? Edit 2: After playing around with this a little more, I've come to the conclusion that the operator $M^{-1}_{nm}=\text{min}(n,m)$ maps $l^2$ vectors to a different space (that I don't know), in the same sense that the derivative operator takes $x^{1/2}$ out of $L^2([0,1])$ . It just so happens that one of the problem $l^2$ vectors is $a^{(m)}_{n}=\delta_{mn}$ , hence why the norm $|M^{-1}a^{(m)}|^2=M^{-2}_{mm}=\infty$ . If this is the case, in what sense can we define $M^{-1}$ ? do we need to use a different basis other than $a^{(m)}_n$ ?","Of course, this scenario is impossible for finite dimensional matrices. I am trying to understand what it means for an unbounded linear operator to have a well defined inverse, but the square of its inverse does not exist. It's pretty easy to find examples of this happening. Take the matrix Defined for . This has the inverse . The square of the inverse obviously diverges and so does not exist. The square goes like where is the size of the matrix. What does this mean for the matrix ? Does somehow pick up a zero eigenvalue that didn't have? Edit 1: I'd like to say also that itself can be said to have zero eigenvalues, with eigensequence , but if we define everything to be in the Hilbert space of finite norm sequences , then those sequences don't belong to the space, hence why we are able to define an inverse. Perhaps theres something about the incompatibility of the linear operator with the Hilbert space? Edit 2: After playing around with this a little more, I've come to the conclusion that the operator maps vectors to a different space (that I don't know), in the same sense that the derivative operator takes out of . It just so happens that one of the problem vectors is , hence why the norm . If this is the case, in what sense can we define ? do we need to use a different basis other than ?","M_{nm}=2\delta_{nm}-\delta_{n+1m}-\delta_{n-1m} n,m\geq 1 M^{-1}_{nm}=\text{min}(n,m) M^{-2}_{nm}\sim nm/3*N N M^2 M^2 M M a_n=n l^2 \sum_{n=1}^{\infty}|a_n|^2<\infty M M^{-1}_{nm}=\text{min}(n,m) l^2 x^{1/2} L^2([0,1]) l^2 a^{(m)}_{n}=\delta_{mn} |M^{-1}a^{(m)}|^2=M^{-2}_{mm}=\infty M^{-1} a^{(m)}_n","['linear-algebra', 'hilbert-spaces']"
66,Solving Vandermonde-style set of simultaneous equations,Solving Vandermonde-style set of simultaneous equations,,"Imagine there's a set of ordered coefficients $\lambda_1>\lambda_2>\ldots>\lambda_n>0$ which I don't know. However, I know the set of relations $$ \sum_{i=1}^n\lambda_i^k(-1)^{i+1}=a_k $$ for $k=1$ to $n$ , with known values $a_k$ . Is there anything smart that I can do in order to determine the $\lambda_i$ ? I realise that if it weren't for the $(-1)^{i+1}$ term in my sum, we could use Newton's identities/Viete's formula to determine the polynomial which has roots $\{\lambda_i\}$ , but have so far failed to spot a way of making use of this in my problem. An alternative way to view this problem is that there's an unknown Vandermonde matrix $V$ for which we have to solve $Vx=a$ given known $x,a$ . (Indeed, $x=(1,-1,1,-1,1,\ldots,1)^T$ .) Original context: I have a real symmetric tridiagonal matrix H with 0 on the diagonal, and it is also centrosymmetric. So, it looks something like $$ H=\left(\begin{array}{ccccccc} 0 & J_1 & 0 & \ldots & 0 & 0 \\ J_1 & 0 & J_2 &&0 & 0 \\ 0 & J_2 & 0 && 0 & 0 \\ \vdots &&& \ddots &\vdots & 0 \\ 0 & 0 & 0 & \ldots & 0 & J_1 \\ 0 & 0 & 0 & \ldots & J_1 & 0 \end{array}\right). $$ For the $N\times N$ matrix, I want to fix $2k+1$ of the eigenvalues. If $2k+1=N$ , this is a standard symmetric inverse eigenvalue problem, but I want to consider smaller values of $k$ . You might formulate this generally, although I'm actually interested in the case where the eigenvalues are $0,\pm 1,\pm 2,\ldots \pm k$ . The remaining eigenvalues are unspecified. They have to occur in $\pm\lambda$ pairs, and I want the smallest (positive) value to be larger than $k$ . These will be determined by the additional constraint - that I want to fix the $N-1-2k$ central coupling strengths to be a specific set of values (again, in the specific case I'm looking at, I'm assuming they're all the same value $J$ , and that $J$ happens to be in a range that allows a solution. I'm deferring the problem of determining what that range might be). The way that I'm trying to approach this is to use some symmetry properties. If $S$ is the swap operator, we can evaluate $\text{Tr}(SH^k)$ both in terms of the eigenvalues of $H$ and in terms of the known coupling strengths. I then get a bunch of equations for my unknown eigenvalues which are those of the original problem statement. With those eigenvalues, I can run the standard inverse eigenvalue routine to determine the matrix $H$ . I've got some good numerical techniques for finding solutions based on this paper . I was just wondering if I could find solutions more directly.","Imagine there's a set of ordered coefficients which I don't know. However, I know the set of relations for to , with known values . Is there anything smart that I can do in order to determine the ? I realise that if it weren't for the term in my sum, we could use Newton's identities/Viete's formula to determine the polynomial which has roots , but have so far failed to spot a way of making use of this in my problem. An alternative way to view this problem is that there's an unknown Vandermonde matrix for which we have to solve given known . (Indeed, .) Original context: I have a real symmetric tridiagonal matrix H with 0 on the diagonal, and it is also centrosymmetric. So, it looks something like For the matrix, I want to fix of the eigenvalues. If , this is a standard symmetric inverse eigenvalue problem, but I want to consider smaller values of . You might formulate this generally, although I'm actually interested in the case where the eigenvalues are . The remaining eigenvalues are unspecified. They have to occur in pairs, and I want the smallest (positive) value to be larger than . These will be determined by the additional constraint - that I want to fix the central coupling strengths to be a specific set of values (again, in the specific case I'm looking at, I'm assuming they're all the same value , and that happens to be in a range that allows a solution. I'm deferring the problem of determining what that range might be). The way that I'm trying to approach this is to use some symmetry properties. If is the swap operator, we can evaluate both in terms of the eigenvalues of and in terms of the known coupling strengths. I then get a bunch of equations for my unknown eigenvalues which are those of the original problem statement. With those eigenvalues, I can run the standard inverse eigenvalue routine to determine the matrix . I've got some good numerical techniques for finding solutions based on this paper . I was just wondering if I could find solutions more directly.","\lambda_1>\lambda_2>\ldots>\lambda_n>0 
\sum_{i=1}^n\lambda_i^k(-1)^{i+1}=a_k
 k=1 n a_k \lambda_i (-1)^{i+1} \{\lambda_i\} V Vx=a x,a x=(1,-1,1,-1,1,\ldots,1)^T 
H=\left(\begin{array}{ccccccc}
0 & J_1 & 0 & \ldots & 0 & 0 \\
J_1 & 0 & J_2 &&0 & 0 \\
0 & J_2 & 0 && 0 & 0 \\
\vdots &&& \ddots &\vdots & 0 \\
0 & 0 & 0 & \ldots & 0 & J_1 \\
0 & 0 & 0 & \ldots & J_1 & 0
\end{array}\right).
 N\times N 2k+1 2k+1=N k 0,\pm 1,\pm 2,\ldots \pm k \pm\lambda k N-1-2k J J S \text{Tr}(SH^k) H H","['linear-algebra', 'polynomials', 'systems-of-equations', 'roots']"
67,Matricial Cauchy-Euler equation,Matricial Cauchy-Euler equation,,"It's well-known how to find, in theory, solutions to a Cauchy-Euler equation $$a_kt^k x^{(k)}(t)+\cdots + a_1tx'(t)+a_0x(t)=0:$$ one sets $x(t)=t^p$ , plug that into the equation, solves a (polynomial) characteristic equation for $p$ , and proceeds from there looking at the multiplicities of the roots. I believe I ran into a matrix version of that in $\Bbb R^n$ : $$t^k A_k {\bf x}^{(k)}(t)+\cdots+tA_1{\bf x}'(t)+A_0{\bf x}(t) = {\bf 0},$$ where the $A_i$ are square $n\times n$ matrices. Is there any known general method to attack this? Thanks.","It's well-known how to find, in theory, solutions to a Cauchy-Euler equation one sets , plug that into the equation, solves a (polynomial) characteristic equation for , and proceeds from there looking at the multiplicities of the roots. I believe I ran into a matrix version of that in : where the are square matrices. Is there any known general method to attack this? Thanks.","a_kt^k x^{(k)}(t)+\cdots + a_1tx'(t)+a_0x(t)=0: x(t)=t^p p \Bbb R^n t^k A_k {\bf x}^{(k)}(t)+\cdots+tA_1{\bf x}'(t)+A_0{\bf x}(t) = {\bf 0}, A_i n\times n","['linear-algebra', 'ordinary-differential-equations']"
68,Is (a version of) the Cayley-Hamilton theorem true for $\mathbb{N}\times\mathbb{N}$ matrices?,Is (a version of) the Cayley-Hamilton theorem true for  matrices?,\mathbb{N}\times\mathbb{N},"Let $A\in \mathbb{C}^{n\times n}$ . The Cayley-Hamilton theorem states that if $p(x)$ is the characteristic polynomial of $A$ , i.e. $p(\lambda) = \det(\lambda I-A)$ , then $A$ satisfies the corresponding matrix polynomial: $p(A)=0$ . I am wondering if the theorem is still true or can be adapted if instead $A$ is an infinite matrix over $\mathbb{C}$ , i.e. $A\in \mathbb{C}^{\mathbb{N}\times\mathbb{N}}$ . The proofs I have seen involve finite-dimensional vector spaces or the division algorithm, which may not hold in infinite-dimensional spaces. Here we must take $A$ to be trace class; roughly speaking, this asserts that if $(e_k)_k$ is the standard basis, then the sum $\sum_{k=1}^{\infty}\langle |A| e_k,e_k\rangle$ is finite, where $|A|$ is the operator norm . One could define the identity matrix $I$ in an analogous fashion to the finite case and obtain the characteristic function $P$ : $$ P(\lambda) = \det(\lambda I-A) $$ Here I take $\det$ to be the Fredholm determinant . Apologies if this post is unclear as I haven't studied functional analysis, so I may be misunderstanding the subtleties involved.","Let . The Cayley-Hamilton theorem states that if is the characteristic polynomial of , i.e. , then satisfies the corresponding matrix polynomial: . I am wondering if the theorem is still true or can be adapted if instead is an infinite matrix over , i.e. . The proofs I have seen involve finite-dimensional vector spaces or the division algorithm, which may not hold in infinite-dimensional spaces. Here we must take to be trace class; roughly speaking, this asserts that if is the standard basis, then the sum is finite, where is the operator norm . One could define the identity matrix in an analogous fashion to the finite case and obtain the characteristic function : Here I take to be the Fredholm determinant . Apologies if this post is unclear as I haven't studied functional analysis, so I may be misunderstanding the subtleties involved.","A\in \mathbb{C}^{n\times n} p(x) A p(\lambda) = \det(\lambda I-A) A p(A)=0 A \mathbb{C} A\in \mathbb{C}^{\mathbb{N}\times\mathbb{N}} A (e_k)_k \sum_{k=1}^{\infty}\langle |A| e_k,e_k\rangle |A| I P 
P(\lambda) = \det(\lambda I-A)
 \det","['linear-algebra', 'matrices', 'functional-analysis', 'characteristic-polynomial', 'cayley-hamilton']"
69,Linearly dependent vector fields that are not spanned by fewer (continuous) vector fields.,Linearly dependent vector fields that are not spanned by fewer (continuous) vector fields.,,"Consider the following continuous vector fields in $\mathbb{C}^3$ : \begin{equation*} w_1(x,y,z):=(x,y,0), \; \; w_2(x,y,z):=(-z,0,y), \;\; w_3(x,y,z):= (0,z,x). \end{equation*} At every point $(x,y,z) \in \mathbb{C}^3$ , the vectors are linearly dependent, as the matrix \begin{pmatrix} x & y & 0 \\ -z & 0 & y \\ 0 & z & x \end{pmatrix} has determinant zero. In particular, this means that at each point there are two vectors $v_1, v_2$ that linearly generate $w_1,w_2,w_3$ at that point. Choosing such a pair of vectors at each point we end up with two vector fields $v_1(x,y,z),v_2(x,y,z)$ which span the other three as $w_i(x,y,z) = \alpha_i(x,y,z)v_1(x,y,z) + \beta_i(x,y,z) v_2(x,y,z)$ for some functions $\alpha_i, \beta_i : \mathbb{C}^3 \rightarrow\mathbb{C}$ . The question is, can there be two continuous vector fields $v_1, v_2$ with that property? (that they span $w_1,w_2,w_3$ at all points). I strongly suspect that this is not the case, but I am not sure how to approach this. Am I missing some obvious condition the vector fields should satisfy for this to be true? I am actually interested in a higher dimensional example, so I am looking for a general proof strategy or hints on how to attack this problem in general rather than a trick that works for this particular example.","Consider the following continuous vector fields in : At every point , the vectors are linearly dependent, as the matrix has determinant zero. In particular, this means that at each point there are two vectors that linearly generate at that point. Choosing such a pair of vectors at each point we end up with two vector fields which span the other three as for some functions . The question is, can there be two continuous vector fields with that property? (that they span at all points). I strongly suspect that this is not the case, but I am not sure how to approach this. Am I missing some obvious condition the vector fields should satisfy for this to be true? I am actually interested in a higher dimensional example, so I am looking for a general proof strategy or hints on how to attack this problem in general rather than a trick that works for this particular example.","\mathbb{C}^3 \begin{equation*}
w_1(x,y,z):=(x,y,0), \; \; w_2(x,y,z):=(-z,0,y), \;\; w_3(x,y,z):= (0,z,x).
\end{equation*} (x,y,z) \in \mathbb{C}^3 \begin{pmatrix}
x & y & 0 \\ -z & 0 & y \\ 0 & z & x
\end{pmatrix} v_1, v_2 w_1,w_2,w_3 v_1(x,y,z),v_2(x,y,z) w_i(x,y,z) = \alpha_i(x,y,z)v_1(x,y,z) + \beta_i(x,y,z) v_2(x,y,z) \alpha_i, \beta_i : \mathbb{C}^3 \rightarrow\mathbb{C} v_1, v_2 w_1,w_2,w_3","['linear-algebra', 'vector-fields']"
70,"Another proof that $Hom(V,W)\cong V^*\otimes W$.",Another proof that .,"Hom(V,W)\cong V^*\otimes W","This question was already discussed here: Why is Hom(V,W) the same thing as V∗⊗W? But, I want to take a different approach in proving that $$\phi:V^*\otimes W\to \text{Hom}(V,W)$$ is an isomorphism where $V$ and $W$ are finite dimensional vector spaces. Let's fix basis $\{w_1,\dots,w_m\}$ , $\{v_1,\dots,v_m\}$ , and $\{\alpha_1,\dots,\alpha_m\}$ for $W$ , $V$ , and $V^*$ correspondingly where $\{\alpha_i\}$ is a dual basis for $\{v_i\}$ . Next, let's define $\phi$ on basis tensors $\alpha_i\otimes w_j\in V^*\otimes W$ via $$\phi(\alpha_i\otimes w_j)=f_{\alpha_i}(w_j)$$ where $f_{\alpha_i}(w_j)(v)=\alpha_i(v)w_j$ and extend it linearly since every element $x$ of $V^*\otimes W$ can be written as a finite sum of the basis elements i.e. if $x\in V^*\otimes W$ , then $x=\sum_{i,j}a_{ij}\alpha_i\otimes w_j$ and $$\phi(x)=\sum_{i,j}a_{ij}\phi(\alpha_i\otimes w_j)=\sum_{i,j}a_{ij}f_{\alpha_i}(w_j).$$ Next, I claim that it is enough to show that $\ker(\phi)=0$ since $V^*\otimes W$ and $Hom(V,W)$ have the same dimension and $\phi$ is a linear map (rank-nullity Theorem). So, let's show that $\ker(\phi)=0$ . We can do that by fixing the basis for $V$ , $W$ , and $V^*$ and showing that if $\phi(x)=0$ then $x=0$ (Details are skipped due to computations). Does it make sense? A follow-up question: Do we have an isomorphism if we take either of these spaces to be infinite dimensional? What would we do in that case?","This question was already discussed here: Why is Hom(V,W) the same thing as V∗⊗W? But, I want to take a different approach in proving that is an isomorphism where and are finite dimensional vector spaces. Let's fix basis , , and for , , and correspondingly where is a dual basis for . Next, let's define on basis tensors via where and extend it linearly since every element of can be written as a finite sum of the basis elements i.e. if , then and Next, I claim that it is enough to show that since and have the same dimension and is a linear map (rank-nullity Theorem). So, let's show that . We can do that by fixing the basis for , , and and showing that if then (Details are skipped due to computations). Does it make sense? A follow-up question: Do we have an isomorphism if we take either of these spaces to be infinite dimensional? What would we do in that case?","\phi:V^*\otimes W\to \text{Hom}(V,W) V W \{w_1,\dots,w_m\} \{v_1,\dots,v_m\} \{\alpha_1,\dots,\alpha_m\} W V V^* \{\alpha_i\} \{v_i\} \phi \alpha_i\otimes w_j\in V^*\otimes W \phi(\alpha_i\otimes w_j)=f_{\alpha_i}(w_j) f_{\alpha_i}(w_j)(v)=\alpha_i(v)w_j x V^*\otimes W x\in V^*\otimes W x=\sum_{i,j}a_{ij}\alpha_i\otimes w_j \phi(x)=\sum_{i,j}a_{ij}\phi(\alpha_i\otimes w_j)=\sum_{i,j}a_{ij}f_{\alpha_i}(w_j). \ker(\phi)=0 V^*\otimes W Hom(V,W) \phi \ker(\phi)=0 V W V^* \phi(x)=0 x=0","['linear-algebra', 'vector-spaces', 'solution-verification', 'tensor-products']"
71,What's this matrix transformation called?,What's this matrix transformation called?,,"Given an augmented matrix $$M = [A \mid X] = \left[\begin{array}{cc|c}    a & b & x\\    c & d & y   \end{array}\right],$$ there's an associated augmented matrix defined like so $$M' = [A' \mid X'] = \left[\begin{array}{cc|c}    \dfrac{ax}{ax+cy} & \dfrac{cy}{ax+cy} & ax+cy\\    \dfrac{bx}{bx+dy} & \dfrac{dy}{bx+dy} & bx+dy \end{array}\right],$$ as long as neither of the denominators vanish. It's probably best to assume $x,y>0$ and $a,b,c,d \geq 0$ together with $a + b = 1$ and $c + d = 1$ to prevent division by zero. Anyway, this function $M \mapsto M'$ has good theoretical properties; it's analytic, involutive, and so long as $A$ is row-stochastic and $X$ is column-stochastic, it follows that $A'$ is row stochastic and $X'$ is column stochastic. Going beyond theoretical considerations, I'll note that this transform shows up in a lot of real-world applications, whenever you're trying to perform binary classification based on an imperfect test. Example. If $M(1,1)$ is the probability of the test coming up positive assuming you have the disease $M(1,2)$ is the probability of the test coming up negative assuming you have the disease $M(1,3)$ is the probability of having the disease $M(2,1)$ is the probability of the test coming up positive assuming you don't have the disease $M(2,2)$ is the probability of the test coming up negative assuming you don't have the disease $M(2,3)$ is the probability of not having the disease then $M'(1,1)$ is the probability you have the disease assuming that the test comes up positive $M'(1,2)$ is the probability you don't have the disease assuming that the test comes up positive $M'(1,3)$ is the probability that the test comes up positive $M'(2,1)$ is the probability you have the disease assuming that the test comes up negative $M'(2,2)$ is the probability you don't have the disease assuming that the test comes up negative $M'(2,3)$ is the probability that the test comes up negative It's straightforward to generalise the function $(M \mapsto M')$ so that it can be applied to any augmented square matrix in which the denominators don't vanish. Question. Does this transform (and/or the matrix it produces) have a name? Addendum 5th Septemeber, 2020. I think I'll call it the conditional dual for now. I remain very interested in this notion, and would love to hear from anyone who knows more about it.","Given an augmented matrix there's an associated augmented matrix defined like so as long as neither of the denominators vanish. It's probably best to assume and together with and to prevent division by zero. Anyway, this function has good theoretical properties; it's analytic, involutive, and so long as is row-stochastic and is column-stochastic, it follows that is row stochastic and is column stochastic. Going beyond theoretical considerations, I'll note that this transform shows up in a lot of real-world applications, whenever you're trying to perform binary classification based on an imperfect test. Example. If is the probability of the test coming up positive assuming you have the disease is the probability of the test coming up negative assuming you have the disease is the probability of having the disease is the probability of the test coming up positive assuming you don't have the disease is the probability of the test coming up negative assuming you don't have the disease is the probability of not having the disease then is the probability you have the disease assuming that the test comes up positive is the probability you don't have the disease assuming that the test comes up positive is the probability that the test comes up positive is the probability you have the disease assuming that the test comes up negative is the probability you don't have the disease assuming that the test comes up negative is the probability that the test comes up negative It's straightforward to generalise the function so that it can be applied to any augmented square matrix in which the denominators don't vanish. Question. Does this transform (and/or the matrix it produces) have a name? Addendum 5th Septemeber, 2020. I think I'll call it the conditional dual for now. I remain very interested in this notion, and would love to hear from anyone who knows more about it.","M = [A \mid X] = \left[\begin{array}{cc|c}  
 a & b & x\\  
 c & d & y  
\end{array}\right], M' = [A' \mid X'] = \left[\begin{array}{cc|c}  
 \dfrac{ax}{ax+cy} & \dfrac{cy}{ax+cy} & ax+cy\\  
 \dfrac{bx}{bx+dy} & \dfrac{dy}{bx+dy} & bx+dy
\end{array}\right], x,y>0 a,b,c,d \geq 0 a + b = 1 c + d = 1 M \mapsto M' A X A' X' M(1,1) M(1,2) M(1,3) M(2,1) M(2,2) M(2,3) M'(1,1) M'(1,2) M'(1,3) M'(2,1) M'(2,2) M'(2,3) (M \mapsto M')","['linear-algebra', 'probability', 'matrices', 'conditional-probability', 'stochastic-matrices']"
72,Where's the catch? Game involving taking turns choosing $n \times n$ matrices over $\mathbb{F}_p$ that all commute with each other.,Where's the catch? Game involving taking turns choosing  matrices over  that all commute with each other.,n \times n \mathbb{F}_p,"Let $S$ be the group of invertible $n \times n$ matrices over $\mathbb{F}_p.$ Alice and Bob take turns choosing elements of $S$ such that no element is repeated and every element must commute with all previous matrices. Whoever cannot move loses. Who wins? My solution: Let $A = \{ X \in S : X^2 = I\}, B = \{ X \in S : X^2 = -I\}, C = S \setminus (A \cup B).$ If $p \ne 2,$ then $X \ne -X$ for all $X \in S.$ Thus, Bob may win by playing $f(X)$ if Alice plays $X,$ where $$f(X) = \begin{cases} -X, \, X \in A \text{ or } X \in B \text{ and n odd}\\ X^{-1}, \text{ else} \end{cases}.$$ Suppose Alice just played $X$ and previously played $Y \ne X.$ To show Bob never repeats a move, it suffices to show that $f(X) \notin \{Y, f(Y)\}.$ Since $f$ is an involution, $f(X) \in \{Y,f(Y)\} \Rightarrow X \in \{f(Y), Y\},$ which means Alice repeated a move and already lost. Thus, Bob wins if $p \ne 2.$ Suppose $p = 2$ and $X \in A.$ The minimal polynomial of $X$ divides $t^2 - 1 = (t-1)^2 \mod 2,$ so the only eigenvalue of $X$ is $1.$ Let $X = PJP^{-1}$ and let $J_i = I + N$ where $N$ is the matrix with ones on the diagonal right above the main one be a Jordan block of $J.$ We must have $I = J_i^2 = I + N^2 \Rightarrow N^2 = 0 \Rightarrow J_i$ has size $1$ or $2.$ I suspect that in the case $p = 2,$ Alice wins by playing $I,$ then playing $X^{-1}$ whenever Bob plays $X \notin A,$ but I have no idea what she should do in the case $X \in A$ is played. For $n=1,2,$ we can see that Alice wins easily, but trying to analyze all $n$ choices for $J$ is unfeasible for larger $n$ since we have tons of choices for $P$ as well. I believe the catch is that this final case is much harder than the rest of the problem, and I've only scratched the surface of the tip of the iceberg when it comes to solving it. How should I proceed? Any hints, approaches, or ideas? If we look for a strategy involving just polynomials, we will observe that $\mathbb{F}_2[X]/\langle X^2 - 1 \rangle = \{ O, I, X, X+I\}.$ However, $(X+I)^2 = X^2 + I = 2I = O$ and the other choices are not viable either. Thus, our strategy (which ostensibly must rely on performing some sensible operations on the set of previously played matrices) has to rely on the products of matrices. But which matrices shall we multiply?","Let be the group of invertible matrices over Alice and Bob take turns choosing elements of such that no element is repeated and every element must commute with all previous matrices. Whoever cannot move loses. Who wins? My solution: Let If then for all Thus, Bob may win by playing if Alice plays where Suppose Alice just played and previously played To show Bob never repeats a move, it suffices to show that Since is an involution, which means Alice repeated a move and already lost. Thus, Bob wins if Suppose and The minimal polynomial of divides so the only eigenvalue of is Let and let where is the matrix with ones on the diagonal right above the main one be a Jordan block of We must have has size or I suspect that in the case Alice wins by playing then playing whenever Bob plays but I have no idea what she should do in the case is played. For we can see that Alice wins easily, but trying to analyze all choices for is unfeasible for larger since we have tons of choices for as well. I believe the catch is that this final case is much harder than the rest of the problem, and I've only scratched the surface of the tip of the iceberg when it comes to solving it. How should I proceed? Any hints, approaches, or ideas? If we look for a strategy involving just polynomials, we will observe that However, and the other choices are not viable either. Thus, our strategy (which ostensibly must rely on performing some sensible operations on the set of previously played matrices) has to rely on the products of matrices. But which matrices shall we multiply?","S n \times n \mathbb{F}_p. S A = \{ X \in S : X^2 = I\}, B = \{ X \in S : X^2 = -I\}, C = S \setminus (A \cup B). p \ne 2, X \ne -X X \in S. f(X) X, f(X) = \begin{cases} -X, \, X \in A \text{ or } X \in B \text{ and n odd}\\ X^{-1}, \text{ else} \end{cases}. X Y \ne X. f(X) \notin \{Y, f(Y)\}. f f(X) \in \{Y,f(Y)\} \Rightarrow X \in \{f(Y), Y\}, p \ne 2. p = 2 X \in A. X t^2 - 1 = (t-1)^2 \mod 2, X 1. X = PJP^{-1} J_i = I + N N J. I = J_i^2 = I + N^2 \Rightarrow N^2 = 0 \Rightarrow J_i 1 2. p = 2, I, X^{-1} X \notin A, X \in A n=1,2, n J n P \mathbb{F}_2[X]/\langle X^2 - 1 \rangle = \{ O, I, X, X+I\}. (X+I)^2 = X^2 + I = 2I = O","['linear-algebra', 'group-theory', 'combinatorial-game-theory']"
73,Deciding if $\mathbb{Z}\ltimes_A \mathbb{Z}^5$ and $\mathbb{Z}\ltimes_B \mathbb{Z}^5$ are isomorphic or not,Deciding if  and  are isomorphic or not,\mathbb{Z}\ltimes_A \mathbb{Z}^5 \mathbb{Z}\ltimes_B \mathbb{Z}^5,"I have the two following groups $G_A=\mathbb{Z}\ltimes_A \mathbb{Z}^5$ , where $A=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&0&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix}$ and $G_B=\mathbb{Z}\ltimes_B \mathbb{Z}^5$ , where $B=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&1&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix}$ . The product is given (for example in $G_A$ ) by $(k,m)\cdot(\ell,n)=(k+\ell, m+A^k n)$ . Problem : Decide if $G_A$ is isomorphic to $G_B$ or not. My thoughts : I think strongly that they are not isomorphic. The matrices $A$ and $B$ are both of order 4, they're not conjugate in $\mathsf{GL}(n,\mathbb{Z})$ (neither $B$ and $A^{-1}$ ) but they are conjugate in $\mathsf{GL}(n,\mathbb{Q})$ . In some other cases, I've seen that they're not isomorphic by computing the abelianization, but in this case both have the same abelianization, namely $\mathbb{Z}\oplus\mathbb{Z}\oplus\mathbb{Z}_2\oplus\mathbb{Z}_2$ . Even worse, both have 1 as an eigenvalue. In my previous MO question there is a counterexample for the implication "" $G_A\cong G_B\Rightarrow A\sim B^{\pm 1}$ "" so I cannot use that. Can anyone give me any other ideas to prove that they aren't isomorphic? Or maybe to prove that they are isomorphic (if they were). Now I posted it in this MO question Thanks!","I have the two following groups , where and , where . The product is given (for example in ) by . Problem : Decide if is isomorphic to or not. My thoughts : I think strongly that they are not isomorphic. The matrices and are both of order 4, they're not conjugate in (neither and ) but they are conjugate in . In some other cases, I've seen that they're not isomorphic by computing the abelianization, but in this case both have the same abelianization, namely . Even worse, both have 1 as an eigenvalue. In my previous MO question there is a counterexample for the implication "" "" so I cannot use that. Can anyone give me any other ideas to prove that they aren't isomorphic? Or maybe to prove that they are isomorphic (if they were). Now I posted it in this MO question Thanks!","G_A=\mathbb{Z}\ltimes_A \mathbb{Z}^5 A=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&0&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix} G_B=\mathbb{Z}\ltimes_B \mathbb{Z}^5 B=\begin{pmatrix} 1&0&0&1&0\\0&-1&0&1&0\\0&0&-1&0&0\\0&0&0&0&-1\\0&0&0&1&0\end{pmatrix} G_A (k,m)\cdot(\ell,n)=(k+\ell, m+A^k n) G_A G_B A B \mathsf{GL}(n,\mathbb{Z}) B A^{-1} \mathsf{GL}(n,\mathbb{Q}) \mathbb{Z}\oplus\mathbb{Z}\oplus\mathbb{Z}_2\oplus\mathbb{Z}_2 G_A\cong G_B\Rightarrow A\sim B^{\pm 1}","['linear-algebra', 'abstract-algebra', 'matrices', 'group-isomorphism', 'semidirect-product']"
74,What is the geometric interpretation of the transpose?,What is the geometric interpretation of the transpose?,,"I can follow the definition of the transpose algebraically, i.e. as a reflection of a matrix across its diagonal, or in terms of dual spaces, but I lack any sort of geometric understanding of the transpose, or even symmetric matrices. For example, if I have a linear transformation, say on the plane, my intuition is to visualize it as some linear distortion of the plane via scaling and rotation. I do not know how this distortion compares to the distortion that results from applying the transpose, or what one can say if the linear transformation is symmetric. Geometrically, why might we expect orthogonal matrices to be combinations of rotations and reflections?","I can follow the definition of the transpose algebraically, i.e. as a reflection of a matrix across its diagonal, or in terms of dual spaces, but I lack any sort of geometric understanding of the transpose, or even symmetric matrices. For example, if I have a linear transformation, say on the plane, my intuition is to visualize it as some linear distortion of the plane via scaling and rotation. I do not know how this distortion compares to the distortion that results from applying the transpose, or what one can say if the linear transformation is symmetric. Geometrically, why might we expect orthogonal matrices to be combinations of rotations and reflections?",,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'transpose', 'geometric-interpretation']"
75,When is $\det (X+Y) = \det X + \det Y$ [closed],When is  [closed],\det (X+Y) = \det X + \det Y,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $X$ be a symmetric $n\times n$ matrix and $Y$ an $n\times n$ matrix. Are there conditions which forces $\det (X+Y) = \det X + \det Y$ . Any comments, references are appreciated. Thank you.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be a symmetric matrix and an matrix. Are there conditions which forces . Any comments, references are appreciated. Thank you.",X n\times n Y n\times n \det (X+Y) = \det X + \det Y,"['linear-algebra', 'matrices', 'determinant']"
76,"Is it possible to check the validity of the following theorem in Group Theory automatically by a computer? If so, how?","Is it possible to check the validity of the following theorem in Group Theory automatically by a computer? If so, how?",,"I'm learning right now how to prove the following very basic theorem in Group Theory, which we learn in a Linear Algebra course in my university (I study Computer Science): Let's have a group $(M,*)$ and $a,b\in M$ . Then $x=a^{-1}*b$ is the only solution to the equation $a*x=b$ . I know how to proof the theorem in a ""Linear Algebra style"". First, I would show that $x=a^{-1}*b$ is a solution to the equation $a*x=b$ , for example like this: $a*(a^{-1}*b) = (a*a^{-1})*b = e*b = b$ . Then I would show that it is the only solution to that equation and the proof would be done (at least my university professor would accept it). However, in the Linear Algebra course we don't use the style of a proof that I have learnt in a Mathematical Logic course that I've just completed (e.g. we were learning to proof a particular logical consequence of a set of first order logic formulas by a method called semantic trees ). I wonder, how this proof of the theorem would look like in this purely ""Mathematical Logic style"", e.g. by using a semantic tree method or some other method which a computer could use to check whether the theorem logically follows from the axioms of the Group Theory. Suppose I have the following definition of a Group Theory (this definition we learnt in a Mathematical Logic course): $\mathscr L=\{f,e\}$ is a language, where $f$ is a binary function symbol and $e$ is a constant called ""neutral element"". The theory of groups has these axioms: Associativity: $(\forall x)(\forall y)(\forall z)(f(f(x,y),z))=f(x,f(y,z))$ Identity: $(\forall x)(f(x,e)=x)$ Inverse: $(\forall x)(\exists z)(f(x,z)=e)$ If it is possible, how do I check the validity of the theorem in a purely formal logical way (for example by a semantic tree method) so that even a computer can check the validity?","I'm learning right now how to prove the following very basic theorem in Group Theory, which we learn in a Linear Algebra course in my university (I study Computer Science): Let's have a group and . Then is the only solution to the equation . I know how to proof the theorem in a ""Linear Algebra style"". First, I would show that is a solution to the equation , for example like this: . Then I would show that it is the only solution to that equation and the proof would be done (at least my university professor would accept it). However, in the Linear Algebra course we don't use the style of a proof that I have learnt in a Mathematical Logic course that I've just completed (e.g. we were learning to proof a particular logical consequence of a set of first order logic formulas by a method called semantic trees ). I wonder, how this proof of the theorem would look like in this purely ""Mathematical Logic style"", e.g. by using a semantic tree method or some other method which a computer could use to check whether the theorem logically follows from the axioms of the Group Theory. Suppose I have the following definition of a Group Theory (this definition we learnt in a Mathematical Logic course): is a language, where is a binary function symbol and is a constant called ""neutral element"". The theory of groups has these axioms: Associativity: Identity: Inverse: If it is possible, how do I check the validity of the theorem in a purely formal logical way (for example by a semantic tree method) so that even a computer can check the validity?","(M,*) a,b\in M x=a^{-1}*b a*x=b x=a^{-1}*b a*x=b a*(a^{-1}*b) = (a*a^{-1})*b = e*b = b \mathscr L=\{f,e\} f e (\forall x)(\forall y)(\forall z)(f(f(x,y),z))=f(x,f(y,z)) (\forall x)(f(x,e)=x) (\forall x)(\exists z)(f(x,z)=e)","['linear-algebra', 'first-order-logic', 'formal-proofs', 'automated-theorem-proving']"
77,Finding a class $C$ of bipartite PPT states such that entanglement of $\rho \in C$ implies entanglement of $\rho + \rho^{\Gamma}$.,Finding a class  of bipartite PPT states such that entanglement of  implies entanglement of .,C \rho \in C \rho + \rho^{\Gamma},"Consider an entangled bipartite quantum state $\rho \in \mathcal{M}_d(\mathbb{C}) \otimes \mathcal{M}_{d'}(\mathbb{C})$ which is positive under partial transposition, i.e., $\rho^\Gamma \geq 0$ . As separability of $\rho$ is equivalent to separability of its partial transpose $\rho^\Gamma$ , we know that $\rho^\Gamma$ is entangled. What are the conditions on $\rho$ which will guarantee that the sum $\rho + \rho^\Gamma$ (ignoring trace normalization) is also entangled? It turns out that the above proposition does not hold for arbitrary PPT entangled states. Easiest counterexamples can be found in $\mathcal{M}_2(\mathbb{C}) \otimes \mathcal{M}_{d}(\mathbb{C})$ , where $\rho + \rho^\Gamma$ is separable for all quantum states $\rho \in \mathcal{M}_2(\mathbb{C}) \otimes \mathcal{M}_d(\mathbb{C})$ (see separability in 2xN systems ). In the language of entanglement witnesses, the problem reduces to finding a common witness that detects both $\rho$ and $\rho^\Gamma$ . Let $W$ be the entanglement witness detecting $\rho$ , i.e., $\text{Tr} (W\rho) < 0$ . Then $W$ is non-decomposable (as $\rho$ is PPT) and is of the canonical form $P+Q^\Gamma - \epsilon \mathbb{I}$ , where $P, Q \geq 0$ are such that $\text{range}(P) \subseteq\text{ker}(\delta)$ and $\text{range}(Q) \subseteq \text{ker}(\delta^\Gamma)$ for some bipartite edge state $\delta$ (these are special states that violate the range criterion for separability in an extreme manner, see edge states ) and $0 < \epsilon \leq \text{inf}_{|e,f\rangle} \langle e,f | P+Q^\Gamma | e,f \rangle$ . If $\delta$ is such that $\text{ker}(\delta) \cap \text{ker}(\delta^\Gamma)$ is not empty, then we can choose $P=Q$ to be the orthogonal projector on $\text{ker}(\delta) \cap \text{ker}(\delta^\Gamma)$ , in which case $W=W^\Gamma$ is the common witness. Can we find a class of PPT entangled states for which the previous statement holds? Can optimization of entanglement witnesses be somehow used to ensure this condition? Cross-posted on physics.SE Cross-posted on quantumcomputing.SE","Consider an entangled bipartite quantum state which is positive under partial transposition, i.e., . As separability of is equivalent to separability of its partial transpose , we know that is entangled. What are the conditions on which will guarantee that the sum (ignoring trace normalization) is also entangled? It turns out that the above proposition does not hold for arbitrary PPT entangled states. Easiest counterexamples can be found in , where is separable for all quantum states (see separability in 2xN systems ). In the language of entanglement witnesses, the problem reduces to finding a common witness that detects both and . Let be the entanglement witness detecting , i.e., . Then is non-decomposable (as is PPT) and is of the canonical form , where are such that and for some bipartite edge state (these are special states that violate the range criterion for separability in an extreme manner, see edge states ) and . If is such that is not empty, then we can choose to be the orthogonal projector on , in which case is the common witness. Can we find a class of PPT entangled states for which the previous statement holds? Can optimization of entanglement witnesses be somehow used to ensure this condition? Cross-posted on physics.SE Cross-posted on quantumcomputing.SE","\rho \in \mathcal{M}_d(\mathbb{C}) \otimes \mathcal{M}_{d'}(\mathbb{C}) \rho^\Gamma \geq 0 \rho \rho^\Gamma \rho^\Gamma \rho \rho + \rho^\Gamma \mathcal{M}_2(\mathbb{C}) \otimes \mathcal{M}_{d}(\mathbb{C}) \rho + \rho^\Gamma \rho \in \mathcal{M}_2(\mathbb{C}) \otimes \mathcal{M}_d(\mathbb{C}) \rho \rho^\Gamma W \rho \text{Tr} (W\rho) < 0 W \rho P+Q^\Gamma - \epsilon \mathbb{I} P, Q \geq 0 \text{range}(P) \subseteq\text{ker}(\delta) \text{range}(Q) \subseteq \text{ker}(\delta^\Gamma) \delta 0 < \epsilon \leq \text{inf}_{|e,f\rangle} \langle e,f | P+Q^\Gamma | e,f \rangle \delta \text{ker}(\delta) \cap \text{ker}(\delta^\Gamma) P=Q \text{ker}(\delta) \cap \text{ker}(\delta^\Gamma) W=W^\Gamma","['linear-algebra', 'matrices', 'mathematical-physics', 'positive-semidefinite', 'quantum-information']"
78,When does $\| AB \| =\| A \| \| B\| $ hold?,When does  hold?,\| AB \| =\| A \| \| B\| ,"A matrix norm that satisfies $\| AB\| \leq  \|  A\| \|  B \| $ is also a submultiplicative norm, so, if $A,B$ are both square matrices, when does the equality hold?","A matrix norm that satisfies is also a submultiplicative norm, so, if are both square matrices, when does the equality hold?","\| AB\| \leq  \|  A\| \|  B \|  A,B","['linear-algebra', 'matrices', 'inequality', 'matrix-equations', 'matrix-calculus']"
79,Good revision notes for complex variable and linear algebra,Good revision notes for complex variable and linear algebra,,"Are there any short notes on complex analysis and linear algebra (around 100 pages) with some exercises so I can refresh my mind on these two subjects? Initially, I had my study notes but I lost it due to a flood. Now I am going to graduate school and I wish to do some revision before I go. I don't mind if the content is dense since I learnt them before.","Are there any short notes on complex analysis and linear algebra (around 100 pages) with some exercises so I can refresh my mind on these two subjects? Initially, I had my study notes but I lost it due to a flood. Now I am going to graduate school and I wish to do some revision before I go. I don't mind if the content is dense since I learnt them before.",,"['linear-algebra', 'complex-analysis', 'linear-transformations', 'book-recommendation', 'complex-integration']"
80,Equalities of the Petersson inner product for two related modular forms,Equalities of the Petersson inner product for two related modular forms,,"Let $\Gamma_1(N)$ be the usual congruence subgroup of $\text{SL}_2(\mathbb{Z})$ and let $f\in S_k(\Gamma_1(N))_{\text{new}}$ be a normalized primitive form, and write $f=\sum_{n\geq 1}a_nq^n$ . Let $w_N$ be the Fricke, or Atkin-Lehner, operator, which is known to preserve the space of newforms. Define the newform $f^*\in S_k(\Gamma_1(N))_{\text{new}}$ by $f^*(z)=\overline{f(-\overline{z})}$ . It can be shown that $w_Nf=\eta_f f^*$ for some $\eta_f\in\mathbb{C}$ , known as the Atkin-Lehner pseudo-eigenvalue. For proving that this pseudo-eigenvalue satisfies, amongst other interesting properties, $(-1)^k\eta_{f^*}=\overline{\eta_f}$ , I write, with the Petersson inner product $$ \langle f^*,w_nf\rangle_{\Gamma_1(N)}=\overline{\eta_f}\langle f^*,f^*\rangle_{\Gamma_1(N)} $$ $$ \langle f^*,w_nf\rangle_{\Gamma_1(N)}=\langle w_n^\dagger f^*,f\rangle_{\Gamma_1(N)}=(-1)^k\eta_{f^*}\langle \underbrace{f}_{=f^{**}},f\rangle_{\Gamma_1(N)}, $$ but now I need these two inner product to be equal, but I don't see how this is true. I first tried to show that $f\cdot \bar{f}=f^*\cdot\bar{f^*}$ , but this didn't seem to hold, and then tried to prove by subsitution of variables ( $z\mapsto -\bar{z})$ in the Petersson inner product, but I don't see how the fundamental domain is invariant under this transformation, as it's not given by a $\text{SL}_2(\mathbb{Z})$ -matrix. What is a way to show that the inner product are in fact equal?","Let be the usual congruence subgroup of and let be a normalized primitive form, and write . Let be the Fricke, or Atkin-Lehner, operator, which is known to preserve the space of newforms. Define the newform by . It can be shown that for some , known as the Atkin-Lehner pseudo-eigenvalue. For proving that this pseudo-eigenvalue satisfies, amongst other interesting properties, , I write, with the Petersson inner product but now I need these two inner product to be equal, but I don't see how this is true. I first tried to show that , but this didn't seem to hold, and then tried to prove by subsitution of variables ( in the Petersson inner product, but I don't see how the fundamental domain is invariant under this transformation, as it's not given by a -matrix. What is a way to show that the inner product are in fact equal?","\Gamma_1(N) \text{SL}_2(\mathbb{Z}) f\in S_k(\Gamma_1(N))_{\text{new}} f=\sum_{n\geq 1}a_nq^n w_N f^*\in S_k(\Gamma_1(N))_{\text{new}} f^*(z)=\overline{f(-\overline{z})} w_Nf=\eta_f f^* \eta_f\in\mathbb{C} (-1)^k\eta_{f^*}=\overline{\eta_f} 
\langle f^*,w_nf\rangle_{\Gamma_1(N)}=\overline{\eta_f}\langle f^*,f^*\rangle_{\Gamma_1(N)}
 
\langle f^*,w_nf\rangle_{\Gamma_1(N)}=\langle w_n^\dagger f^*,f\rangle_{\Gamma_1(N)}=(-1)^k\eta_{f^*}\langle \underbrace{f}_{=f^{**}},f\rangle_{\Gamma_1(N)},
 f\cdot \bar{f}=f^*\cdot\bar{f^*} z\mapsto -\bar{z}) \text{SL}_2(\mathbb{Z})","['real-analysis', 'linear-algebra']"
81,Is there a matrix $X$ that conserves orthgonality through this function $f_X$?,Is there a matrix  that conserves orthgonality through this function ?,X f_X,"Let $\mathcal{M}_2(\mathbb{C})$ be the set of square matrices of dimension 2, and $\mathcal{S}_2(\mathbb{C})$ the subset of self-adjoint matrices. The function $f_X: \mathcal{S}_2(\mathbb{C}) \rightarrow \mathcal{S}_2(\mathbb{C})$ is defined by $$ f_X(A) = \sum_{n=0}^\infty X^n A (X^\dagger)^n $$ where $X \in \mathcal{M}_2(\mathbb{C})$ , and where $X^\dagger$ is the Hermitian adjoint of $X$ . The problem is the following: Is there a non-trivial matrix $X$ such that, $\forall A, B \in \mathcal{S}_2(\mathbb{C})$ , if $\mathrm{Tr}\left[AB\right] = 0$ , then $\mathrm{Tr}\left[f_X(A)f_X(B)\right] =0$ ? Any hint would be appreciated! EDIT: As mentionned in the comments, $X=0$ and $X = cI$ with $|c| < 1$ are obvious solutions. Also, if $X^d = cI$ , then $f_X(A) = \sum_{n=0}^\infty c^n \sum_{m=0}^{d-1} X^m A (X^\dagger)^m = \frac{1}{1-c} \sum_{m=0}^{d-1} X^m A (X^\dagger)^m$ . Furthermore, the eigenvalues $\lambda_i$ of $X$ are such that $\lambda_i^d = c$ , so one can diagonalize $X = PDP^{-1}$ but I did not manage to go further when substituting in the trace.","Let be the set of square matrices of dimension 2, and the subset of self-adjoint matrices. The function is defined by where , and where is the Hermitian adjoint of . The problem is the following: Is there a non-trivial matrix such that, , if , then ? Any hint would be appreciated! EDIT: As mentionned in the comments, and with are obvious solutions. Also, if , then . Furthermore, the eigenvalues of are such that , so one can diagonalize but I did not manage to go further when substituting in the trace.","\mathcal{M}_2(\mathbb{C}) \mathcal{S}_2(\mathbb{C}) f_X: \mathcal{S}_2(\mathbb{C}) \rightarrow \mathcal{S}_2(\mathbb{C})  f_X(A) = \sum_{n=0}^\infty X^n A (X^\dagger)^n  X \in \mathcal{M}_2(\mathbb{C}) X^\dagger X X \forall A, B \in \mathcal{S}_2(\mathbb{C}) \mathrm{Tr}\left[AB\right] = 0 \mathrm{Tr}\left[f_X(A)f_X(B)\right] =0 X=0 X = cI |c| < 1 X^d = cI f_X(A) = \sum_{n=0}^\infty c^n \sum_{m=0}^{d-1} X^m A (X^\dagger)^m = \frac{1}{1-c} \sum_{m=0}^{d-1} X^m A (X^\dagger)^m \lambda_i X \lambda_i^d = c X = PDP^{-1}","['linear-algebra', 'matrices']"
82,Are there any tricks to find an orthonormal basis for polynomials by hand?,Are there any tricks to find an orthonormal basis for polynomials by hand?,,"I am taking a Numerical Analysis class. On a previous exam, there is a question which involved finding an orthonormal basis for $\mathbb{P}_2$ w.r.t the inner product $(u, v) = \int_0^2 u(t)v(t) dt.$ I did this with Gram-Schmidt, and it took me about $15-20$ minutes (and this was only half a question!). Plus I of course made arithmetic mistakes, and I tired myself out working at $100 \%$ speed. Along the way, I had to solve integrals such as $$\int_0^2 \left((x-\frac 34)^2 - \frac {21}{16} \right)^2 dx$$ which take long and are very prone to mistakes. The only thing I could think of was to make a u substitution, $u = x-3/4$ . I am wondering: I applied Gram-Schmidt to $\{1, x, x^2 \}$ ; is this the way to go, or is there a better way? (I have seen the 3-term reccurence, but it seems like a lot of memorization, and seems to involve a lot of integrals too). Also, when I do Gram-Schmidt, I orthogonalize, and then normalize everything at the end. Is it better to normalize along the way? Lastly, if Gram-Schmidt is the way to go, would you have any advice on how to quickly evaluate these types of integrals that come out of Gram-Schmidt? (integrals of polynomials of low degree, often squared). Thank you very much, I greatly appreciate any help.","I am taking a Numerical Analysis class. On a previous exam, there is a question which involved finding an orthonormal basis for w.r.t the inner product I did this with Gram-Schmidt, and it took me about minutes (and this was only half a question!). Plus I of course made arithmetic mistakes, and I tired myself out working at speed. Along the way, I had to solve integrals such as which take long and are very prone to mistakes. The only thing I could think of was to make a u substitution, . I am wondering: I applied Gram-Schmidt to ; is this the way to go, or is there a better way? (I have seen the 3-term reccurence, but it seems like a lot of memorization, and seems to involve a lot of integrals too). Also, when I do Gram-Schmidt, I orthogonalize, and then normalize everything at the end. Is it better to normalize along the way? Lastly, if Gram-Schmidt is the way to go, would you have any advice on how to quickly evaluate these types of integrals that come out of Gram-Schmidt? (integrals of polynomials of low degree, often squared). Thank you very much, I greatly appreciate any help.","\mathbb{P}_2 (u, v) = \int_0^2 u(t)v(t) dt. 15-20 100 \% \int_0^2 \left((x-\frac 34)^2 - \frac {21}{16} \right)^2 dx u = x-3/4 \{1, x, x^2 \}","['real-analysis', 'calculus', 'linear-algebra', 'integration', 'numerical-methods']"
83,The parametric ratio $\frac{x}{y}$ with known $x+y$ and $x\cdot y$,The parametric ratio  with known  and,\frac{x}{y} x+y x\cdot y,"$x$ and $y$ are in fact $\lambda_1$ and $\lambda_2$ , the bigger and smaller eigenvalues of a parametric matrix $A'A$ , and $t$ is a very small constant. I have that $$ \begin{split} x+y &= 1+ \frac{t^2}{2} \\  xy &= \frac{t^2}{4} \end{split} $$ How would I prove that $\frac{x}{y}\ge \frac{1}{t}$ ? The solutions of the system are here , but it gets pretty messy. Is there an elegant way to go about this?","and are in fact and , the bigger and smaller eigenvalues of a parametric matrix , and is a very small constant. I have that How would I prove that ? The solutions of the system are here , but it gets pretty messy. Is there an elegant way to go about this?","x y \lambda_1 \lambda_2 A'A t 
\begin{split}
x+y &= 1+ \frac{t^2}{2} \\
 xy &= \frac{t^2}{4}
\end{split}
 \frac{x}{y}\ge \frac{1}{t}","['linear-algebra', 'algebra-precalculus', 'inequality']"
84,Coideals in the grouplike colagebra are spanned by differences,Coideals in the grouplike colagebra are spanned by differences,,"Let $k$ be a field, and let $S$ be a nonempty set. Let $k[S]$ be the grouplike coalgebra of $S$ over $k$ , i.e. the free vector space with basis $S$ equipped with the coproduct $\Delta(s)=s\otimes s$ for all $s\in S$ . (If $S$ is a group, then $k[S]$ is the usual group algebra of $S$ - which is also a Hopf algebra.) Let $I$ be a coideal in $k[S]$ . Then, the coideal $I$ is spanned by differences $s-s'$ for some $s,s'$ in $S$ . This is Exercise 2.1.26(b) in Radford's Hopf algebras. It is clear that differences $s-s'$ for some $s,s'$ in $S$ always span a coideal in $k[S]$ . But how to prove the other direction, that every coideal in $k[S]$ is of this form? It should be simple because the other exercises in this book are not too hard. I was thinking that for a coideal $I$ in $k[S]$ , we have $I\subseteq\bigoplus_{\text{for some }s\neq s'}k(s-s')$ , where each summand $k(s-s')$ is a simple coideal, and conclude from this... Another thought: As in other exercises in this book, I think the rank of $\Delta(c)$ where $c\in I$ plays a role, possibly of $c$ written as a sum with the least possible amount of nonzero summands $\lambda_{s,s'}(s-s')$ for some $s\neq s'$ ( $\lambda_{s,s'}\in k$ ).","Let be a field, and let be a nonempty set. Let be the grouplike coalgebra of over , i.e. the free vector space with basis equipped with the coproduct for all . (If is a group, then is the usual group algebra of - which is also a Hopf algebra.) Let be a coideal in . Then, the coideal is spanned by differences for some in . This is Exercise 2.1.26(b) in Radford's Hopf algebras. It is clear that differences for some in always span a coideal in . But how to prove the other direction, that every coideal in is of this form? It should be simple because the other exercises in this book are not too hard. I was thinking that for a coideal in , we have , where each summand is a simple coideal, and conclude from this... Another thought: As in other exercises in this book, I think the rank of where plays a role, possibly of written as a sum with the least possible amount of nonzero summands for some ( ).","k S k[S] S k S \Delta(s)=s\otimes s s\in S S k[S] S I k[S] I s-s' s,s' S s-s' s,s' S k[S] k[S] I k[S] I\subseteq\bigoplus_{\text{for some }s\neq s'}k(s-s') k(s-s') \Delta(c) c\in I c \lambda_{s,s'}(s-s') s\neq s' \lambda_{s,s'}\in k","['linear-algebra', 'group-theory', 'hopf-algebras', 'coalgebras']"
85,Partial trace and preserving positive semidefiniteness,Partial trace and preserving positive semidefiniteness,,"This question is related to topics in quantum information but I will present it as a linear algebra question here. Consider some matrix $\delta_{AB}$ that lives in a bipartite Hilbert space $H_A\otimes H_B$ . The partial trace map traces over one of the Hilbert space so we obtain the reduced matrices as below $$\delta_A = \text{Tr}_B(\delta_{AB}), \, \, \delta_B = \text{Tr}_A(\delta_{AB})$$ Denote $A\geq B$ to mean that $A-B$ is positive semidefinite. Let $\rho_{AB}$ and $\sigma_{AB}$ both be positive semidefinite matrices with trace $1$ . Let $\lambda, \lambda_1, \lambda_2$ each be the smallest real numbers such that the following relationships hold $$\rho_{AB} - \lambda\sigma_{AB} \geq 0$$ $$\rho_{A} - {\lambda_1}\sigma_{A} \geq 0$$ $$\rho_{B} - {\lambda_2}\sigma_{B} \geq 0$$ What is the relationship between $\lambda$ , $\lambda_1$ and $\lambda_2$ ? I'm looking for some kind of non-trivial inequality that relates all three. By partial tracing over the conditions above, one sees that $\lambda\geq \lambda_1$ and similarly $\lambda\geq\lambda_2$ but I feel that more can be said.","This question is related to topics in quantum information but I will present it as a linear algebra question here. Consider some matrix that lives in a bipartite Hilbert space . The partial trace map traces over one of the Hilbert space so we obtain the reduced matrices as below Denote to mean that is positive semidefinite. Let and both be positive semidefinite matrices with trace . Let each be the smallest real numbers such that the following relationships hold What is the relationship between , and ? I'm looking for some kind of non-trivial inequality that relates all three. By partial tracing over the conditions above, one sees that and similarly but I feel that more can be said.","\delta_{AB} H_A\otimes H_B \delta_A = \text{Tr}_B(\delta_{AB}), \, \, \delta_B = \text{Tr}_A(\delta_{AB}) A\geq B A-B \rho_{AB} \sigma_{AB} 1 \lambda, \lambda_1, \lambda_2 \rho_{AB} - \lambda\sigma_{AB} \geq 0 \rho_{A} - {\lambda_1}\sigma_{A} \geq 0 \rho_{B} - {\lambda_2}\sigma_{B} \geq 0 \lambda \lambda_1 \lambda_2 \lambda\geq \lambda_1 \lambda\geq\lambda_2","['linear-algebra', 'matrices', 'inequality', 'trace', 'positive-semidefinite']"
86,Number of possible rotational domains of one 2D lattice on top of another?,Number of possible rotational domains of one 2D lattice on top of another?,,"There are four (or five) two dimensional Bravais lattices which I refer to as oblique, rectangular, hexagonal and square . I'll discuss the 17 symmetry wallpaper groups below. Ignoring translation, if I ask someone to lay one hexagonal lattice on top of another with one of their axes parallel, there's only one distinct rotational way to do it. One might think there are six (since the lattice is p6 or six-fold rotationally symmetric) but they are indistinguishable. If I ask for an offset of 10 degrees then there are two ways (+/- 10°). If I ask for an offset of 60/2 = 30 degrees, the number of ways drops back to one. The expression or algorithm I need would fill out the following table: Oblique   Rectangular   Hexagonal     Square                       2           2            6            3  Oblique      2       1/1  Rectangular  2  Hexagonal    6       3/6 (30)              1/2 (30)  Square       4                                          1/2 (45) This is read as follows: For hexagonal on hexagonal every 30 degrees there is 1 way, and for angles that are non-zero modulus 30 degrees there are 2 ways. For square on hexagonal the number is three and six, also every 30 degrees as shown here Square and hexagonal lattices shown separately: Square and hexagonal lattices shown at 0, 30 and 60 degrees: Square and hexagonal lattices shown at +/-10 degrees (only two of the possible six are shown): In surface science if we had islands of one lattice on top of another that different only in this way we'd call them domains. Question: For pairs of 2D lattices each with either a 2, 3, 4 or 6-fold rotational symmetry (ten possible pairs in total), how can I calculate the number of possible ways I can uniquely configure them with a given angle who's absolute value is $\theta$ . This is of course likely to be a solved problem, so if one wants to point me to a description of the solution that is not written at a high level rather than doing so here, that would also be great! Wallpaper The problem is more complex if the specific symmetry group of each lattice is defined and if that can be included in the answer at the same time it will be even more helpful. I'm not sure how to formulate that question exactly so I've ask the question who's answer will at least get me started. Are planar symmetry groups and wallpaper groups the same things?","There are four (or five) two dimensional Bravais lattices which I refer to as oblique, rectangular, hexagonal and square . I'll discuss the 17 symmetry wallpaper groups below. Ignoring translation, if I ask someone to lay one hexagonal lattice on top of another with one of their axes parallel, there's only one distinct rotational way to do it. One might think there are six (since the lattice is p6 or six-fold rotationally symmetric) but they are indistinguishable. If I ask for an offset of 10 degrees then there are two ways (+/- 10°). If I ask for an offset of 60/2 = 30 degrees, the number of ways drops back to one. The expression or algorithm I need would fill out the following table: Oblique   Rectangular   Hexagonal     Square                       2           2            6            3  Oblique      2       1/1  Rectangular  2  Hexagonal    6       3/6 (30)              1/2 (30)  Square       4                                          1/2 (45) This is read as follows: For hexagonal on hexagonal every 30 degrees there is 1 way, and for angles that are non-zero modulus 30 degrees there are 2 ways. For square on hexagonal the number is three and six, also every 30 degrees as shown here Square and hexagonal lattices shown separately: Square and hexagonal lattices shown at 0, 30 and 60 degrees: Square and hexagonal lattices shown at +/-10 degrees (only two of the possible six are shown): In surface science if we had islands of one lattice on top of another that different only in this way we'd call them domains. Question: For pairs of 2D lattices each with either a 2, 3, 4 or 6-fold rotational symmetry (ten possible pairs in total), how can I calculate the number of possible ways I can uniquely configure them with a given angle who's absolute value is . This is of course likely to be a solved problem, so if one wants to point me to a description of the solution that is not written at a high level rather than doing so here, that would also be great! Wallpaper The problem is more complex if the specific symmetry group of each lattice is defined and if that can be included in the answer at the same time it will be even more helpful. I'm not sure how to formulate that question exactly so I've ask the question who's answer will at least get me started. Are planar symmetry groups and wallpaper groups the same things?",\theta,"['linear-algebra', 'abstract-algebra', 'group-theory', 'integer-lattices']"
87,Analyticity of determinant formula for Gaussian integral,Analyticity of determinant formula for Gaussian integral,,"It is a well known fact that $\int_{\mathbb{R}^n} e^{-\frac{1}{2}x \cdot A x} dx = \sqrt{\frac{(2\pi)^n}{\det{A}}}$ for real, positive definite $A$ . The left hand side of the equation make sense for any complex-symmetric $A$ with real part positive definite. The left hand side is also analytic in $A$ under this assumption of absolute convergence. The right hand side is analytic in $A$ except for a potential branch cut of the square root function. However I'm unsure under what conditions one can analytically continue this formula. I have produced a counterexample where $A$ is complex symmetric and $\Re{A}$ is positive definite but yet the formula is wrong by a phase, e.g. take $A = e^{i\phi} \text{Id}$ and let $\frac{\pi}{n} < |\phi|< \frac{\pi}{2}$ (obviously we need $n \geq 3$ for this example). Under what circumstances is the formula valid?","It is a well known fact that for real, positive definite . The left hand side of the equation make sense for any complex-symmetric with real part positive definite. The left hand side is also analytic in under this assumption of absolute convergence. The right hand side is analytic in except for a potential branch cut of the square root function. However I'm unsure under what conditions one can analytically continue this formula. I have produced a counterexample where is complex symmetric and is positive definite but yet the formula is wrong by a phase, e.g. take and let (obviously we need for this example). Under what circumstances is the formula valid?",\int_{\mathbb{R}^n} e^{-\frac{1}{2}x \cdot A x} dx = \sqrt{\frac{(2\pi)^n}{\det{A}}} A A A A A \Re{A} A = e^{i\phi} \text{Id} \frac{\pi}{n} < |\phi|< \frac{\pi}{2} n \geq 3,"['linear-algebra', 'complex-analysis', 'analytic-continuation', 'gaussian']"
88,"let $A,B$ matrices $n \times n$ and $A \ne B , AB=0$ and $A\ne 0 , B\ne0$ prove $\left|A\right|^2+\left|B\right|^2=0$",let  matrices  and  and  prove,"A,B n \times n A \ne B , AB=0 A\ne 0 , B\ne0 \left|A\right|^2+\left|B\right|^2=0","let $A,B$ matrices $n \times n$ such that $A \ne B $ and $AB=0$ and $A\ne 0 , B\ne0$ prove $\left|A\right|^2+\left|B\right|^2=0$ My attempt: $$AB=0 \implies \det(AB)=0 \implies \det(A)*\det(B)=0 \iff $$ $$|B|=0\qquad\text{or}\qquad |A|=0\qquad\text{or}\qquad|A|=|B|=0.$$ if $|A|=0 , |B| \ne0$ then $B^{-1}$ exist $\implies ABB^{-1}=0 \implies A=0 \implies$ contradiction if $|B|=0 , |A| \ne0$ same as before and if $|B|=0 , |A| =0$ then also $\left|A\right|^2+\left|B\right|^2=0$ My question is does this correct what I did and if not where did I did wrong ? thanks",let matrices such that and and prove My attempt: if then exist contradiction if same as before and if then also My question is does this correct what I did and if not where did I did wrong ? thanks,"A,B n \times n A \ne B  AB=0 A\ne 0 , B\ne0 \left|A\right|^2+\left|B\right|^2=0 AB=0 \implies \det(AB)=0 \implies \det(A)*\det(B)=0 \iff  |B|=0\qquad\text{or}\qquad |A|=0\qquad\text{or}\qquad|A|=|B|=0. |A|=0 , |B| \ne0 B^{-1} \implies ABB^{-1}=0 \implies A=0 \implies |B|=0 , |A| \ne0 |B|=0 , |A| =0 \left|A\right|^2+\left|B\right|^2=0","['linear-algebra', 'determinant', 'solution-verification']"
89,Combinatorial approach to prove divisibility results with the sequence $u_{n+p+1} = u_{n+p} + u_{n}$ in $\mathbb{F}_p$,Combinatorial approach to prove divisibility results with the sequence  in,u_{n+p+1} = u_{n+p} + u_{n} \mathbb{F}_p,"For a fixed prime number $p$ , we consider the sequence : $$\left\{ \begin{array}{l} u_n = 1, & 0\leq n \leq p \\ u_{n+p+1} = u_{n+p} + u_n \end{array}\right. $$ I want to prove that $\forall a \in \mathbb{N},\ p|u_{pa}-u_a$ My attempt : I think that there are two natural approaches to tackle this problem: One way is to apply results about linear sequences, but i'm not very familiar with how it should be done in the field $\mathbb{F}_p$ . We thus consider the characteristic polynomial $Q(X) := X^{p+1}-X^p-1$ in the field $\mathbb{F}_p$ . One can easily check that $Q'(X) = X^p$ and has only $0$ as a root, which is not a root of $Q(X)$ , thus every roots of $Q(X)$ are simple. If we could say that $u_n = \sum_{k} \lambda_k\alpha_k^n$ with $\alpha_k$ roots of $Q(X)$ , then we would be done by Fermat's theorem. My problem is that I think that $Q(X)$ has at most $2$ roots, that are those of $X^2-X-1$ (and actually we would need that $5$ is a square $mod(p)$ , ie. that $p$ must be either $1\ mod(5)$ or $-1\  mod(5)$ by quadratic reciprocity) The second approach is more combinatorial, and use the fact that $u_n$ can be interpreted as the number of ways one can tile a $(p+1)\times n$ rectangle with $(p+1)\times 1$ dominos. Indeed the relationship can be derived by observing that a $(p+1)\times(n+p+1)$ tiling is either a $(p+1)\times(n+p)$ tiling with a vertical domino filing the last slice, or a $(p+1)\times n$ tiling with $(p+1)$ horizontal domino filing the remaining square. If we consider a $(p+1)\times pa$ rectangle, then we can ""naturally"" slice it in $p$ rectangles of size $(p+1)\times a$ . There are $u_a^p$ ways of tiling the initial rectangle with dominos that doesn't ""cut"" a small rectangle. By Fermat's theorem, we need to show that the number of tilings $v_{pa}$ having a domino that ""cut"" a small rectangle is a multiple of $p$ , because we would have $u_{pa} = u_a^p + v_{pa} = u_a\ mod(p)$ . I don't know how to show this. The ideal proof would be to find an action splitting the possible tilings into $p$ orbits of the same size for example. I also tried to explicit $v_{bp}$ with the inclusion-exclusion principle (but I assumed $a\geq 2p$ so that's not very satisfying): If we note the inner vertical edges of the small rectangles from left to right by $1,\dots,p-1$ we can try to split by the events $A_j = \{ \text{tilings with a domino cuting the edge}\ j\}$ and I found, if not mistaken: $$ v_{pa} = \sum_{k=1}^{p-1}(-1)^{k-1}\sum_{1\leq i_1 \leq \dots \leq i_k \leq p-1}\ \sum_{n^k \in \{1,\dots,p\}^k}\ \prod_{j=1}^{k+1}(u_{i_j a - (p+1-n_{j-1}^k+n_j^k)}) $$ with the boundary conditions $i_{k+1} = p-\sum_{j=0}^k i_j$ and $n_0^k = n_{k+1}^k = 0$ , but I'm not sure we can make something out of it. Because I haven't got a proof of this result, anything would be welcomed but I'm particularly curious about where the combinatorial approach would lead as it seems more elementary.","For a fixed prime number , we consider the sequence : I want to prove that My attempt : I think that there are two natural approaches to tackle this problem: One way is to apply results about linear sequences, but i'm not very familiar with how it should be done in the field . We thus consider the characteristic polynomial in the field . One can easily check that and has only as a root, which is not a root of , thus every roots of are simple. If we could say that with roots of , then we would be done by Fermat's theorem. My problem is that I think that has at most roots, that are those of (and actually we would need that is a square , ie. that must be either or by quadratic reciprocity) The second approach is more combinatorial, and use the fact that can be interpreted as the number of ways one can tile a rectangle with dominos. Indeed the relationship can be derived by observing that a tiling is either a tiling with a vertical domino filing the last slice, or a tiling with horizontal domino filing the remaining square. If we consider a rectangle, then we can ""naturally"" slice it in rectangles of size . There are ways of tiling the initial rectangle with dominos that doesn't ""cut"" a small rectangle. By Fermat's theorem, we need to show that the number of tilings having a domino that ""cut"" a small rectangle is a multiple of , because we would have . I don't know how to show this. The ideal proof would be to find an action splitting the possible tilings into orbits of the same size for example. I also tried to explicit with the inclusion-exclusion principle (but I assumed so that's not very satisfying): If we note the inner vertical edges of the small rectangles from left to right by we can try to split by the events and I found, if not mistaken: with the boundary conditions and , but I'm not sure we can make something out of it. Because I haven't got a proof of this result, anything would be welcomed but I'm particularly curious about where the combinatorial approach would lead as it seems more elementary.","p \left\{
\begin{array}{l}
u_n = 1, & 0\leq n \leq p \\
u_{n+p+1} = u_{n+p} + u_n
\end{array}\right.
 \forall a \in \mathbb{N},\ p|u_{pa}-u_a \mathbb{F}_p Q(X) := X^{p+1}-X^p-1 \mathbb{F}_p Q'(X) = X^p 0 Q(X) Q(X) u_n = \sum_{k} \lambda_k\alpha_k^n \alpha_k Q(X) Q(X) 2 X^2-X-1 5 mod(p) p 1\ mod(5) -1\  mod(5) u_n (p+1)\times n (p+1)\times 1 (p+1)\times(n+p+1) (p+1)\times(n+p) (p+1)\times n (p+1) (p+1)\times pa p (p+1)\times a u_a^p v_{pa} p u_{pa} = u_a^p + v_{pa} = u_a\ mod(p) p v_{bp} a\geq 2p 1,\dots,p-1 A_j = \{ \text{tilings with a domino cuting the edge}\ j\} 
v_{pa} = \sum_{k=1}^{p-1}(-1)^{k-1}\sum_{1\leq i_1 \leq \dots \leq i_k \leq p-1}\ \sum_{n^k \in \{1,\dots,p\}^k}\ \prod_{j=1}^{k+1}(u_{i_j a - (p+1-n_{j-1}^k+n_j^k)})
 i_{k+1} = p-\sum_{j=0}^k i_j n_0^k = n_{k+1}^k = 0","['linear-algebra', 'combinatorics', 'finite-fields', 'combinatorial-proofs']"
90,Singular vectors of random Gaussian matrix with non-isotropic rows,Singular vectors of random Gaussian matrix with non-isotropic rows,,"Suppose $G \in \mathbb{R}^{m \times n}$ has i.i.d. rows $g_i \sim \mathcal{N}(0, \Sigma)$ for some diagonal matrix $\Sigma = \text{diag}(\lambda_1,\dots,\lambda_n)$ where the diagonal entries satisfy $\lambda_1 \geqslant \dots \geqslant \lambda_n > 0$ . Can anything be said about the distribution of the columns of $V$ where $G = U\Lambda V^T$ is the singular value decomposition of $G$ ? The following question considers the case when the entries of $G$ are i.i.d. $\mathcal{N}(0,1)$ and shows that the singular vectors are uniformly distributed on a sphere of radius $1$ : Singular vector of random Gaussian matrix The proof capitalizes on the rotational invariance of the standard normal distribution but I don't think the same argument can be made here due to the form of $\Sigma$ . Any help or references would be greatly appreciated.",Suppose has i.i.d. rows for some diagonal matrix where the diagonal entries satisfy . Can anything be said about the distribution of the columns of where is the singular value decomposition of ? The following question considers the case when the entries of are i.i.d. and shows that the singular vectors are uniformly distributed on a sphere of radius : Singular vector of random Gaussian matrix The proof capitalizes on the rotational invariance of the standard normal distribution but I don't think the same argument can be made here due to the form of . Any help or references would be greatly appreciated.,"G \in \mathbb{R}^{m \times n} g_i \sim \mathcal{N}(0, \Sigma) \Sigma = \text{diag}(\lambda_1,\dots,\lambda_n) \lambda_1 \geqslant \dots \geqslant \lambda_n > 0 V G = U\Lambda V^T G G \mathcal{N}(0,1) 1 \Sigma","['linear-algebra', 'probability-theory', 'svd', 'random-matrices', 'geometric-probability']"
91,Prove that if $AA^T=A^TA$ and $AB=BA$ then $AB^T=B^TA$,Prove that if  and  then,AA^T=A^TA AB=BA AB^T=B^TA,Prove that if $AA^T=A^TA$ and $AB=BA$ then $AB^T=B^TA$ where $A$ and $B$ are matrices. It doesn't say of what order they are. Can somebody help me with this exam problem? I know I should put my work here but I really don't know what to do. I would really like some help.,Prove that if and then where and are matrices. It doesn't say of what order they are. Can somebody help me with this exam problem? I know I should put my work here but I really don't know what to do. I would really like some help.,AA^T=A^TA AB=BA AB^T=B^TA A B,"['linear-algebra', 'matrices', 'matrix-equations']"
92,How many commuting pairs of unitriangular matrices are there in $GL_{n}(F_{p})$?,How many commuting pairs of unitriangular matrices are there in ?,GL_{n}(F_{p}),"I've been doing some work counting commuting pairs of unitriangular matrices over $GL_{n}(F_{p})$ .  So far, I believe that for $n=2$ , there are $p^2$ such pairs, and for $n=3$ there are $p^5+p^4-p^3$ such pairs.  Can anybody recognize these polynomials, generalize to arbitrary $n<p$ ? Recall that unitriangular matrices are upper-triangular matrices of having entries of $1$ on the diagonal. Any help would be appreciated so much. Thank you all.","I've been doing some work counting commuting pairs of unitriangular matrices over .  So far, I believe that for , there are such pairs, and for there are such pairs.  Can anybody recognize these polynomials, generalize to arbitrary ? Recall that unitriangular matrices are upper-triangular matrices of having entries of on the diagonal. Any help would be appreciated so much. Thank you all.",GL_{n}(F_{p}) n=2 p^2 n=3 p^5+p^4-p^3 n<p 1,"['linear-algebra', 'combinatorics', 'matrices', 'group-theory', 'finite-fields']"
93,Flat extensions of group rings,Flat extensions of group rings,,"Let $R$ be a commutative ring, $f:H\to G$ a surjective group homomorphism and consider $RG$ as a $(RG,RH)$ -module via $g\cdot h := g\cdot f(h)$ as usual. Now suppose that $RG$ is flat over $H$ , meaning that $$RG\otimes -:H\text{-}\mathbf{Mod} \to G\text{-}\mathbf{Mod}$$ is exact. What can we say about $f$ ? If $RG$ was even projective over $H$ , we would get a section $s:RG\to RH$ telling us that $\#\mathrm{ker}(f)<\infty$ is a unit in $R$ , but I suppose, something like this does not work if $RG$ is only assumed to be flat?","Let be a commutative ring, a surjective group homomorphism and consider as a -module via as usual. Now suppose that is flat over , meaning that is exact. What can we say about ? If was even projective over , we would get a section telling us that is a unit in , but I suppose, something like this does not work if is only assumed to be flat?","R f:H\to G RG (RG,RH) g\cdot h := g\cdot f(h) RG H RG\otimes -:H\text{-}\mathbf{Mod} \to G\text{-}\mathbf{Mod} f RG H s:RG\to RH \#\mathrm{ker}(f)<\infty R RG","['linear-algebra', 'group-theory', 'representation-theory', 'group-rings']"
94,Methods for spline fitting for transcendental functions? How to place the knots?,Methods for spline fitting for transcendental functions? How to place the knots?,,"I was thinking about the following problem the other day: Fit a spline $s(t)$ to some transcendental function $f(t)$ , so that: $$s(t) = \cases{P_k(t), \text{ if } t_k \leq t \leq t_{k+1}}$$ For polynomials $P_k(t)$ : $$P_k(t) = \sum_{i=0}^{N} c_{ik}t^i$$ Now we seek $c_{ik}$ so that $s(t)$ approximates $f(t)$ well on interval $t\in [t_0,t_{max}]$ . How can we do this? How to choose the $t_k$ knot points? Which boundary conditions should be obeyed there? If the knot points were fixed then the problem could be approached like some least squares norm minimization: $$\sum_l\|s(t_l)-f(t_l)\|_2^2 + \epsilon(\text{boundary terms})$$ All would be linear. But how to tackle the non-linearity that is introduced with not knowing where knot points should be placed?","I was thinking about the following problem the other day: Fit a spline to some transcendental function , so that: For polynomials : Now we seek so that approximates well on interval . How can we do this? How to choose the knot points? Which boundary conditions should be obeyed there? If the knot points were fixed then the problem could be approached like some least squares norm minimization: All would be linear. But how to tackle the non-linearity that is introduced with not knowing where knot points should be placed?","s(t) f(t) s(t) = \cases{P_k(t), \text{ if } t_k \leq t \leq t_{k+1}} P_k(t) P_k(t) = \sum_{i=0}^{N} c_{ik}t^i c_{ik} s(t) f(t) t\in [t_0,t_{max}] t_k \sum_l\|s(t_l)-f(t_l)\|_2^2 + \epsilon(\text{boundary terms})","['calculus', 'linear-algebra', 'optimization', 'numerical-methods', 'spline']"
95,Symmetric Rank-1 Decomposition for Density Matrices,Symmetric Rank-1 Decomposition for Density Matrices,,"Let $(H,\langle\cdot,\cdot\rangle)$ be an $n$ -dimensional complex Hilbert space. For concreteness, you can just take $H=\mathbb{C}^n$ with standard inner product. Note that we will use the physicist's convention of the inner product being complex-linear in the second entry. Consider the symmetric $k$ -fold tensor product product of $H$ with itself, which we will denote by $H^{\otimes_s^k}$ . Similarly, consider the symmetric $k$ -fold tensor product of the dual of $H$ with itself, which we will denote by $H^{*,\otimes_s^k}$ . We are interested in elements of the tensor product $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ . Of course using the Riesz representation theorem, we can identify $H^*$ with $H$ using the inner product and so $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ is the complex span of elements of the form $$|f\rangle\langle g|, \qquad f,g\in H^{\otimes_s^k}, \tag{1}$$ where we have used Dirac's bra-ket notation. We are not interested in the whole space $H^{\otimes _s^k} \otimes H^{*,\otimes_s^k}$ , but only those elements which are self-adjoint , by for an element of the form (1) means $f=g$ . My question is the following: Question. Is it possible to write every self-adjoint element of $H^{\otimes_s^k} \otimes H^{*,\otimes_s^k}$ into a linear combination $$\sum_{j=1}^N a_j |f_j^{\otimes k}\rangle\langle f_j^{\otimes_k}|, \tag{2}$$ where $N\in\mathbb{N}$ , $a_j\in\mathbb{C}$ , and $f_j\in H$ ? I know that it is possible to write every element $f\in H^{\otimes_s^k}$ as $$f=\sum_{j=1}^N a_j f_j^{\otimes k},$$ and consequently every self-adjoint of element of $H^{\otimes_s^k}\otimes H^{*,\otimes_s^k}$ can be written as $$\sum_{j=1}^N a_j(|f_j^{\otimes k}\rangle\langle g_j^{\otimes k}| + |g_j^{\otimes k}\rangle\langle f_j^{\otimes k}|).$$ However, I do not know how to prove such a symmetric rank-1 type decomposition like (2).","Let be an -dimensional complex Hilbert space. For concreteness, you can just take with standard inner product. Note that we will use the physicist's convention of the inner product being complex-linear in the second entry. Consider the symmetric -fold tensor product product of with itself, which we will denote by . Similarly, consider the symmetric -fold tensor product of the dual of with itself, which we will denote by . We are interested in elements of the tensor product . Of course using the Riesz representation theorem, we can identify with using the inner product and so is the complex span of elements of the form where we have used Dirac's bra-ket notation. We are not interested in the whole space , but only those elements which are self-adjoint , by for an element of the form (1) means . My question is the following: Question. Is it possible to write every self-adjoint element of into a linear combination where , , and ? I know that it is possible to write every element as and consequently every self-adjoint of element of can be written as However, I do not know how to prove such a symmetric rank-1 type decomposition like (2).","(H,\langle\cdot,\cdot\rangle) n H=\mathbb{C}^n k H H^{\otimes_s^k} k H H^{*,\otimes_s^k} H^{\otimes _s^k} \otimes H^{*,\otimes_s^k} H^* H H^{\otimes _s^k} \otimes H^{*,\otimes_s^k} |f\rangle\langle g|, \qquad f,g\in H^{\otimes_s^k}, \tag{1} H^{\otimes _s^k} \otimes H^{*,\otimes_s^k} f=g H^{\otimes_s^k} \otimes H^{*,\otimes_s^k} \sum_{j=1}^N a_j |f_j^{\otimes k}\rangle\langle f_j^{\otimes_k}|, \tag{2} N\in\mathbb{N} a_j\in\mathbb{C} f_j\in H f\in H^{\otimes_s^k} f=\sum_{j=1}^N a_j f_j^{\otimes k}, H^{\otimes_s^k}\otimes H^{*,\otimes_s^k} \sum_{j=1}^N a_j(|f_j^{\otimes k}\rangle\langle g_j^{\otimes k}| + |g_j^{\otimes k}\rangle\langle f_j^{\otimes k}|).","['linear-algebra', 'abstract-algebra', 'commutative-algebra', 'tensor-products', 'tensors']"
96,"If $\mathbf{R}$ is an upper triangular matrix, then does $\|\mathbf{R}\| \le \|\mathbf{R} + \mathbf{R}^T\|$ hold?","If  is an upper triangular matrix, then does  hold?",\mathbf{R} \|\mathbf{R}\| \le \|\mathbf{R} + \mathbf{R}^T\|,"Let $\mathbf{R}\in\mathbb{R}^{n\times n}$ be upper triangular and $\|\cdot\|$ be the induced 2-norm of matrices. Then, does $\|\mathbf{R}\| \le \|\mathbf{R} + \mathbf{R}^T\|$ hold?","Let be upper triangular and be the induced 2-norm of matrices. Then, does hold?",\mathbf{R}\in\mathbb{R}^{n\times n} \|\cdot\| \|\mathbf{R}\| \le \|\mathbf{R} + \mathbf{R}^T\|,['linear-algebra']
97,Find B such that $\det(B)>0$ but there is no real $A$ with $\exp{(A)}=B$,Find B such that  but there is no real  with,\det(B)>0 A \exp{(A)}=B,I have proved that $\exp(\text{tr}(A)) = \det (\exp (A))$ . An easy corollary is that $\exp (A)$ has positive determinant whenever $A$ is real. Now the question asks us to find a B such that $\det(B)>0$ but there is no real $A$ with $\exp{(A)}=B$ . I have no idea how to proceed so it would be greatly appreciated if someone could drop me a hint.,I have proved that . An easy corollary is that has positive determinant whenever is real. Now the question asks us to find a B such that but there is no real with . I have no idea how to proceed so it would be greatly appreciated if someone could drop me a hint.,\exp(\text{tr}(A)) = \det (\exp (A)) \exp (A) A \det(B)>0 A \exp{(A)}=B,"['linear-algebra', 'matrices', 'matrix-exponential']"
98,Why did we study “defect” in the late 1800s?,Why did we study “defect” in the late 1800s?,,"I just began reading Weibel’s An introduction to Homological Algebra and stuck at the first page, where the author attempts to give a historical motivation for the homology construction: Homological algebra is a tool used in several branches of mathematics: algebraic topology, group theory, commutative ring theory, and algebraic geometry come to mind. It arose in the late 1800s in the following manner. Let $f$ and $g$ be matrices whose product is zero. If $g\cdot v=0$ for some column vector $v$ , say, of length $n$ , we cannot always write $v=f\cdot u$ . This failure is measured by the defect $$d=n-\text{rank}(f)-\text{rank}(g).$$ In modern language, $f$ and $g$ represent linear maps $$U\xrightarrow{f} V\xrightarrow{g} W$$ with $gf=0$ , and $d$ is the dimension of the homology module $$H=\ker (g)/f(U).$$ I am not seeing any motivation for studying the “defect” of the above phenomenon. Could anyone give a brief explanation for why did the mathematicians study how to write a column vector of $f$ by product of $f$ and another column vector? Much appreciated.","I just began reading Weibel’s An introduction to Homological Algebra and stuck at the first page, where the author attempts to give a historical motivation for the homology construction: Homological algebra is a tool used in several branches of mathematics: algebraic topology, group theory, commutative ring theory, and algebraic geometry come to mind. It arose in the late 1800s in the following manner. Let and be matrices whose product is zero. If for some column vector , say, of length , we cannot always write . This failure is measured by the defect In modern language, and represent linear maps with , and is the dimension of the homology module I am not seeing any motivation for studying the “defect” of the above phenomenon. Could anyone give a brief explanation for why did the mathematicians study how to write a column vector of by product of and another column vector? Much appreciated.",f g g\cdot v=0 v n v=f\cdot u d=n-\text{rank}(f)-\text{rank}(g). f g U\xrightarrow{f} V\xrightarrow{g} W gf=0 d H=\ker (g)/f(U). f f,"['linear-algebra', 'matrices', 'homological-algebra']"
99,Show $x^2 A + x B +C$ is positive semi-definite,Show  is positive semi-definite,x^2 A + x B +C,"EDIT: Only $A$ is allowed to be positive semi-definite. I'm interested in showing that the matrix $f(x)=x^2 A + x B +C$ is positive semi-definite (PSD) for any scalar $x$ , and PSD matricex $A$ . I use the following definition for a PSD matrix: \begin{align} u^T f(x) u &= x^2\, u^T A u + x\, u^T B u + u^T C u \\ &\geq 0 \end{align} for any vector $u$ . Then I use the quadratic formula to obtain the following condition: \begin{align} (u^T B u )^2 - 4(u^T A u)(u^T C u)  &= u^T B u u^T B u - 4 u^T A u u^T C u \\ &= u^T (B u u^T B - A u u^T C) u \\ &\leq 0 \end{align} for any vector $u$ . I don't know how to get rid of $u$ and translate this condition to one involving $A$ , $B$ and $C$ only.","EDIT: Only is allowed to be positive semi-definite. I'm interested in showing that the matrix is positive semi-definite (PSD) for any scalar , and PSD matricex . I use the following definition for a PSD matrix: for any vector . Then I use the quadratic formula to obtain the following condition: for any vector . I don't know how to get rid of and translate this condition to one involving , and only.","A f(x)=x^2 A + x B +C x A \begin{align}
u^T f(x) u &= x^2\, u^T A u + x\, u^T B u + u^T C u \\
&\geq 0
\end{align} u \begin{align}
(u^T B u )^2 - 4(u^T A u)(u^T C u) 
&= u^T B u u^T B u - 4 u^T A u u^T C u \\
&= u^T (B u u^T B - A u u^T C) u \\
&\leq 0
\end{align} u u A B C","['linear-algebra', 'quadratic-forms', 'positive-semidefinite']"
