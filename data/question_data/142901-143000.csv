,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit of sequence with floor function problem,Limit of sequence with floor function problem,,"I have to evaluate limit of sequence $ \lim_{n \rightarrow\infty } \frac{n \sqrt{n} \sqrt[n]{(n+1)^{n}+n^{n+1}}}{[\sqrt n]+[2 \sqrt n]+...+[n\sqrt n]} $. ""[...]"" here denotes floor function. What troubles me here the most is the floor function in it, I am not sure what to do with it.","I have to evaluate limit of sequence $ \lim_{n \rightarrow\infty } \frac{n \sqrt{n} \sqrt[n]{(n+1)^{n}+n^{n+1}}}{[\sqrt n]+[2 \sqrt n]+...+[n\sqrt n]} $. ""[...]"" here denotes floor function. What troubles me here the most is the floor function in it, I am not sure what to do with it.",,"['calculus', 'limits', 'ceiling-and-floor-functions']"
1,How to find the limit of given function [duplicate],How to find the limit of given function [duplicate],,"This question already has answers here : What is the limit of $n \sin (2 \pi \cdot e \cdot n!)$ as $n$ goes to infinity? (3 answers) Closed 7 years ago . How would I find this limit? $\lim_{n \to \infty}\ n\ \sin\ (2\pi en!)$, where $e$ is the exponential.","This question already has answers here : What is the limit of $n \sin (2 \pi \cdot e \cdot n!)$ as $n$ goes to infinity? (3 answers) Closed 7 years ago . How would I find this limit? $\lim_{n \to \infty}\ n\ \sin\ (2\pi en!)$, where $e$ is the exponential.",,['limits']
2,Limits infinite with fraction [closed],Limits infinite with fraction [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question $$\lim_{x\to\infty} \frac{(x+1)^{10}+(x+2)^{10}+\cdots+(x+100)^{10}}{x^{10}+10^{10}}$$ Seems easy but no idea how to do it Thanks","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question $$\lim_{x\to\infty} \frac{(x+1)^{10}+(x+2)^{10}+\cdots+(x+100)^{10}}{x^{10}+10^{10}}$$ Seems easy but no idea how to do it Thanks",,"['calculus', 'limits']"
3,"Real Analysis, limit of function using binomial theorem.","Real Analysis, limit of function using binomial theorem.",,"We have $b \in \mathbb{R}$ and $ 0<b<1$, I need to show that $\lim(nb^n)=0$ My attempt: Since $0<b<1$ we can express b as $\frac{1}{1+a_n}$ so now our sequence is $nb^n=n\left(\frac{1}{1+a_n}\right)^n=\frac{n}{(1+a_n)^n}$, we can use the binomial theorem to express $b^n=\frac{1}{1+na_n+\frac{1}{2}n(n-1)a_n^2+...}\leq\frac{1}{1+na_n+\frac{1}{2}n(n-1)a_n^2}$ So we have $nb^n \leq \frac{n}{1+na_n+\frac{1}{2}n(n-1)a_n^2}$. I am stuck after this, I am uncertain how to keep going from there.  Thanks!","We have $b \in \mathbb{R}$ and $ 0<b<1$, I need to show that $\lim(nb^n)=0$ My attempt: Since $0<b<1$ we can express b as $\frac{1}{1+a_n}$ so now our sequence is $nb^n=n\left(\frac{1}{1+a_n}\right)^n=\frac{n}{(1+a_n)^n}$, we can use the binomial theorem to express $b^n=\frac{1}{1+na_n+\frac{1}{2}n(n-1)a_n^2+...}\leq\frac{1}{1+na_n+\frac{1}{2}n(n-1)a_n^2}$ So we have $nb^n \leq \frac{n}{1+na_n+\frac{1}{2}n(n-1)a_n^2}$. I am stuck after this, I am uncertain how to keep going from there.  Thanks!",,"['real-analysis', 'limits']"
4,Limit Proof Check: $\lim _{x \to a} x^4 = a^4$,Limit Proof Check:,\lim _{x \to a} x^4 = a^4,"Reviewing limits and I'm afraid I may be making mistakes, just looking for a quick proof check. $f(x)=x^4$, prove that $\lim _{x \rightarrow a}f(x)=a^4$ by showing how to find $\delta$ . This is my work. $|x^4 - a^4 | < \epsilon$ and $0<|x-a|<\delta$ Factoring: $|x^2 +a^2||x-a||x+a|< \epsilon$ next we set $|x-a|<1$ which means $-1+a<|x|<1+a$ so that \begin{align}  |x^2 +a^2||x-a||x+a|& =((-1+a)^2 +a^2 )((1+a)+a)(x-a))\\  & = |2a(1+2a^2 -2a)||x-a|\\   & <\epsilon \\ \end{align} We then have $$\delta =\min\lbrace 1, \frac{\epsilon}{ |2a(1+2a^2 -2a)|} \rbrace$$ How's it look?","Reviewing limits and I'm afraid I may be making mistakes, just looking for a quick proof check. $f(x)=x^4$, prove that $\lim _{x \rightarrow a}f(x)=a^4$ by showing how to find $\delta$ . This is my work. $|x^4 - a^4 | < \epsilon$ and $0<|x-a|<\delta$ Factoring: $|x^2 +a^2||x-a||x+a|< \epsilon$ next we set $|x-a|<1$ which means $-1+a<|x|<1+a$ so that \begin{align}  |x^2 +a^2||x-a||x+a|& =((-1+a)^2 +a^2 )((1+a)+a)(x-a))\\  & = |2a(1+2a^2 -2a)||x-a|\\   & <\epsilon \\ \end{align} We then have $$\delta =\min\lbrace 1, \frac{\epsilon}{ |2a(1+2a^2 -2a)|} \rbrace$$ How's it look?",,"['real-analysis', 'limits', 'proof-verification', 'epsilon-delta']"
5,find a limit of $\lim_{n\rightarrow\infty}\left(\frac{a^{1/n}+b^{1/n}}{2}\right)^n$ [duplicate],find a limit of  [duplicate],\lim_{n\rightarrow\infty}\left(\frac{a^{1/n}+b^{1/n}}{2}\right)^n,This question already has answers here : Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$ (10 answers) Closed 10 years ago . $$\lim_{n\rightarrow\infty}\left(\frac{a^{1/n}+b^{1/n}}{2}\right)^n$$ No idea how to do it please help step by step,This question already has answers here : Proving that: $\lim\limits_{n\to\infty} \left(\frac{a^{\frac{1}{n}}+b^{\frac{1}{n}}}{2}\right)^n =\sqrt{ab}$ (10 answers) Closed 10 years ago . $$\lim_{n\rightarrow\infty}\left(\frac{a^{1/n}+b^{1/n}}{2}\right)^n$$ No idea how to do it please help step by step,,"['limits', 'radicals']"
6,Prove this limit as $x\to\infty$ [duplicate],Prove this limit as  [duplicate],x\to\infty,"This question already has an answer here : If $\lim_{x\to\infty} [f(x+1)-f(x)] =l$ then $\lim_{ x\to\infty}f(x)/x =l$ ($f$ is continuous) (1 answer) Closed 5 years ago . Let $f:(a,\infty)\to\mathbb{R}$ be a function. Suppose for each $b>a$, $f$ is bounded on $(a,b)$ and $\lim_{x\to\infty}f(x+1)-f(x)=A$. Prove $$\lim_{x\to\infty}\frac{f(x)}{x}=A.$$ Here we assume $f$ to be arbitrary and no further conditions. $f$ could be continuous,discontinuous,as long as it fits all assumptions. I have some trouble with the intermediate steps.","This question already has an answer here : If $\lim_{x\to\infty} [f(x+1)-f(x)] =l$ then $\lim_{ x\to\infty}f(x)/x =l$ ($f$ is continuous) (1 answer) Closed 5 years ago . Let $f:(a,\infty)\to\mathbb{R}$ be a function. Suppose for each $b>a$, $f$ is bounded on $(a,b)$ and $\lim_{x\to\infty}f(x+1)-f(x)=A$. Prove $$\lim_{x\to\infty}\frac{f(x)}{x}=A.$$ Here we assume $f$ to be arbitrary and no further conditions. $f$ could be continuous,discontinuous,as long as it fits all assumptions. I have some trouble with the intermediate steps.",,"['calculus', 'real-analysis', 'limits']"
7,Finding $\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n}$ if it exists [duplicate],Finding  if it exists [duplicate],\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n},"This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 10 years ago . I've known the following: $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^k}{n^n}=1.$$ Then, I got interested in the following similar limitation: $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n}$$ By using computer, it seems that $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n}=\frac{e}{e-1}.$$  However, I haven't been able to prove this.  I need your help.","This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 10 years ago . I've known the following: $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^k}{n^n}=1.$$ Then, I got interested in the following similar limitation: $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n}$$ By using computer, it seems that $$\lim_{n\to\infty}\sum_{k=1}^{n}\frac{k^n}{n^n}=\frac{e}{e-1}.$$  However, I haven't been able to prove this.  I need your help.",,"['limits', 'summation']"
8,Limit of $\frac{x^y-y^x}{x^x-y^y} $ while $x\to y$,Limit of  while,\frac{x^y-y^x}{x^x-y^y}  x\to y,$$\lim_{x\to y}\frac{x^y-y^x}{x^x-y^y} $$ I've tried L'Hospital and this. Both doesn't tends to work. Please help someone.,$$\lim_{x\to y}\frac{x^y-y^x}{x^x-y^y} $$ I've tried L'Hospital and this. Both doesn't tends to work. Please help someone.,,"['calculus', 'limits', 'exponential-function']"
9,"Let $f:[a,b]\to\mathbb R$. Evaluate $\lim_{n\to\infty}\int_a^bf(x)\sin(nx)\,dx$ [duplicate]",Let . Evaluate  [duplicate],"f:[a,b]\to\mathbb R \lim_{n\to\infty}\int_a^bf(x)\sin(nx)\,dx","This question already has answers here : How to prove that $\lim\limits_{n\to\infty}\int\limits _{a}^{b}\sin\left(nt\right)f\left(t\right)dt=0\text { ? }$ (3 answers) Closed 10 years ago . Let $f:[a,b]\to\mathbb R$. Evaluate $\lim_{n\to\infty}\int_a^bf(x)\sin(nx)\,dx$. $f$ is continuously differentiable. I'm told this can be done using basic calculus. It's difficult for me to see where I should begin. I'd like some hints.","This question already has answers here : How to prove that $\lim\limits_{n\to\infty}\int\limits _{a}^{b}\sin\left(nt\right)f\left(t\right)dt=0\text { ? }$ (3 answers) Closed 10 years ago . Let $f:[a,b]\to\mathbb R$. Evaluate $\lim_{n\to\infty}\int_a^bf(x)\sin(nx)\,dx$. $f$ is continuously differentiable. I'm told this can be done using basic calculus. It's difficult for me to see where I should begin. I'd like some hints.",,"['calculus', 'analysis', 'limits', 'fourier-analysis']"
10,"Taking the limit of an integral using residues, why is this wrong?","Taking the limit of an integral using residues, why is this wrong?",,"I have the integral $\lim\limits_{R\to\infty}\int_{-R}^{R} \frac{\cos(x)}{x^2+a^2} dx$ where $a$ is a positive real number. The strategy was to evaluate the limit of the integral on the boundary of a half-disk of radius $R$ and then subtract the limit of the integral on the half-circle. What I am confused with, is that when computing the integral on the boundary of the half-disk, I used the residue theorem to compute the integral thinking that $\cos(x)$ is meromorphic everywhere but infinity, and our domain is a finite (open) disk, and then took the limit.  However, the reference I am using, says that $\cos(x)$ has an essential singularity at infinity and thus uses a different strategy (computing the limit for $\frac{\exp(ix)}{x^2+a^2}$ and then taking the real part). I don't understand why I cannot use the residue theorem, and then take the limit in this case, because our domain (although gets unbounded when taking a limit) is a finite disk on which cosine is meromorphic. Can someone please explain where I am going wrong in my thinking? Thank you very much.","I have the integral $\lim\limits_{R\to\infty}\int_{-R}^{R} \frac{\cos(x)}{x^2+a^2} dx$ where $a$ is a positive real number. The strategy was to evaluate the limit of the integral on the boundary of a half-disk of radius $R$ and then subtract the limit of the integral on the half-circle. What I am confused with, is that when computing the integral on the boundary of the half-disk, I used the residue theorem to compute the integral thinking that $\cos(x)$ is meromorphic everywhere but infinity, and our domain is a finite (open) disk, and then took the limit.  However, the reference I am using, says that $\cos(x)$ has an essential singularity at infinity and thus uses a different strategy (computing the limit for $\frac{\exp(ix)}{x^2+a^2}$ and then taking the real part). I don't understand why I cannot use the residue theorem, and then take the limit in this case, because our domain (although gets unbounded when taking a limit) is a finite disk on which cosine is meromorphic. Can someone please explain where I am going wrong in my thinking? Thank you very much.",,"['limits', 'infinity', 'residue-calculus']"
11,Prove: $\lim_{x\to\infty}x^{2/3}((x+1)^{1/3}-x^{1/3})=1/3$,Prove:,\lim_{x\to\infty}x^{2/3}((x+1)^{1/3}-x^{1/3})=1/3,"How can I show: $$\lim_{x\to\infty}x^{2/3}((x+1)^{1/3}-x^{1/3})=1/3$$ I've tried multiplying with its ""conjugate"" but that doesn't seem to help that much. Thanks!","How can I show: $$\lim_{x\to\infty}x^{2/3}((x+1)^{1/3}-x^{1/3})=1/3$$ I've tried multiplying with its ""conjugate"" but that doesn't seem to help that much. Thanks!",,['limits']
12,A problem with limit,A problem with limit,,How to attack this one? Does the following limit exists: $$\lim_{x\to +\infty}\dfrac {\cos^5x\sin^5x} {x^8\sin^2x-2x^7\sin x\cos^2x+x^6\cos^4x+x^2\cos^8x}$$,How to attack this one? Does the following limit exists: $$\lim_{x\to +\infty}\dfrac {\cos^5x\sin^5x} {x^8\sin^2x-2x^7\sin x\cos^2x+x^6\cos^4x+x^2\cos^8x}$$,,['calculus']
13,$\frac{\mathrm d^n}{\mathrm d x^n} e^{-\frac {1}{x^2}} = 0$ at $x=0$ [duplicate],at  [duplicate],\frac{\mathrm d^n}{\mathrm d x^n} e^{-\frac {1}{x^2}} = 0 x=0,"This question already has answers here : Let $g(x)=e^{-1/x^2}$ for $x\not=0$ and $g(0)=0$. Show that $g^{(n)}(0)=0$ for all $n\in\Bbb N$. (2 answers) Closed 11 years ago . This is an exercise from David Brannan's Mathematical Analysis. I've proved parts (a) - (c) but need help with Part (d). Any guidance appreciated. EDIT I have solved it, by induction using the results of parts (a) and (c).","This question already has answers here : Let $g(x)=e^{-1/x^2}$ for $x\not=0$ and $g(0)=0$. Show that $g^{(n)}(0)=0$ for all $n\in\Bbb N$. (2 answers) Closed 11 years ago . This is an exercise from David Brannan's Mathematical Analysis. I've proved parts (a) - (c) but need help with Part (d). Any guidance appreciated. EDIT I have solved it, by induction using the results of parts (a) and (c).",,"['real-analysis', 'analysis', 'limits', 'derivatives', 'taylor-expansion']"
14,Questions Based On A Couple Of Weird Limits,Questions Based On A Couple Of Weird Limits,,"When evaluating certain limits,i get an answer with which i'am not fully convinced despite following steps which i claim is correct.Answers given by analytic method and L'Hopital's rule differs! I wish i could have used numerical method using MATLAB or other packages to see if that's giving me another different answer.I would like to know the subtlety behind this computation if any.$$1.\lim_{x\to1}\frac{x^2 - 1}{x + 1}$$ $$=\lim_{x\to1}\frac{(x+1)(x-1)}{x+1} = 0$$ When i use L'Hopital's rule for the above limit i.e $$\frac{\frac{\mathrm{d} }{\mathrm{d} x}(x^2-1)}{\frac{\mathrm{d} }{\mathrm{d} x}(x+1)}= \frac{2x}{1} = 2$$ I get different answers when evaluating $$\lim_{x\to2}\frac{x^3-8}{x-2} = \lim_{x\to2}\frac{(x-2)(x^2+2x+4)}{(x-2)}=4+8+4=16$$ Using L'Hopital's rule$$\frac{\frac{\mathrm{d} }{\mathrm{d} x}(x^3-8)}{\frac{\mathrm{d} }{\mathrm{d} x}(x-2)}=\frac{3x^2}{1}=12$$(after taking the limit) Edit:I realised that L'Hopital's rule can't be applied for the first one as it's not in indeterminate form!But what about the second one?","When evaluating certain limits,i get an answer with which i'am not fully convinced despite following steps which i claim is correct.Answers given by analytic method and L'Hopital's rule differs! I wish i could have used numerical method using MATLAB or other packages to see if that's giving me another different answer.I would like to know the subtlety behind this computation if any.$$1.\lim_{x\to1}\frac{x^2 - 1}{x + 1}$$ $$=\lim_{x\to1}\frac{(x+1)(x-1)}{x+1} = 0$$ When i use L'Hopital's rule for the above limit i.e $$\frac{\frac{\mathrm{d} }{\mathrm{d} x}(x^2-1)}{\frac{\mathrm{d} }{\mathrm{d} x}(x+1)}= \frac{2x}{1} = 2$$ I get different answers when evaluating $$\lim_{x\to2}\frac{x^3-8}{x-2} = \lim_{x\to2}\frac{(x-2)(x^2+2x+4)}{(x-2)}=4+8+4=16$$ Using L'Hopital's rule$$\frac{\frac{\mathrm{d} }{\mathrm{d} x}(x^3-8)}{\frac{\mathrm{d} }{\mathrm{d} x}(x-2)}=\frac{3x^2}{1}=12$$(after taking the limit) Edit:I realised that L'Hopital's rule can't be applied for the first one as it's not in indeterminate form!But what about the second one?",,"['calculus', 'limits']"
15,How to prove that $\frac1{\sqrt{n}}\sum\limits_{k=2}^{n+1} \prod\limits_{\ell=1}^{k-2}\frac{n-\ell}{n}$ converges to $\sqrt{\pi/2}$?,How to prove that  converges to ?,\frac1{\sqrt{n}}\sum\limits_{k=2}^{n+1} \prod\limits_{\ell=1}^{k-2}\frac{n-\ell}{n} \sqrt{\pi/2},Consider $$ X_k =\prod_{\ell=1}^{k-2}\frac{n-\ell}{n} \ \textrm{ for every } 2\leqslant k\leqslant n+1. $$ How can you prove the following?  $$ \lim_{n\rightarrow \infty} \frac1{\sqrt{n}}\sum_{k=2}^{n+1} X_k= \sqrt{\frac{\pi}{2}} $$ A heuristic argument replaces $\frac{n-\ell}{n}$ by $e^{-\ell}$ and the sum by an integral. How can this (or another method) be made rigorous?,Consider $$ X_k =\prod_{\ell=1}^{k-2}\frac{n-\ell}{n} \ \textrm{ for every } 2\leqslant k\leqslant n+1. $$ How can you prove the following?  $$ \lim_{n\rightarrow \infty} \frac1{\sqrt{n}}\sum_{k=2}^{n+1} X_k= \sqrt{\frac{\pi}{2}} $$ A heuristic argument replaces $\frac{n-\ell}{n}$ by $e^{-\ell}$ and the sum by an integral. How can this (or another method) be made rigorous?,,['limits']
16,"Is there a ""functional"" definition of a limit?","Is there a ""functional"" definition of a limit?",,"Say we have a convergent sequence $(x_n)$ where $x_n \in E$ for all $n \in \mathbb{N}$ and $E$ is a subset of a metric space $(X,d)$. With this setup, we usually define it's limit as a point $x \in X$ such that for every $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that $n > N$ implies $d(x_n,x) < \epsilon$. In some sense, I think that the above definition of the limit effectively mean that given any  ""$\epsilon > 0$, there is some kind of $N$ that we can use to get the sequence within $\epsilon$ of the limit $x$. I am wondering whether this can be reformulated as follows: there is a function $f: \mathbb{R} \rightarrow \mathbb{N}$, so that $f(\epsilon) = N$ and $n > N$ implies  $d(x_n,x) < \epsilon$. If so, the function $f$ would have some nice properties (it would be onto, and monotonically decreasing in $\epsilon$ for instance). Is there any use to thinking about functions in this way / has it been introduced in this way?","Say we have a convergent sequence $(x_n)$ where $x_n \in E$ for all $n \in \mathbb{N}$ and $E$ is a subset of a metric space $(X,d)$. With this setup, we usually define it's limit as a point $x \in X$ such that for every $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that $n > N$ implies $d(x_n,x) < \epsilon$. In some sense, I think that the above definition of the limit effectively mean that given any  ""$\epsilon > 0$, there is some kind of $N$ that we can use to get the sequence within $\epsilon$ of the limit $x$. I am wondering whether this can be reformulated as follows: there is a function $f: \mathbb{R} \rightarrow \mathbb{N}$, so that $f(\epsilon) = N$ and $n > N$ implies  $d(x_n,x) < \epsilon$. If so, the function $f$ would have some nice properties (it would be onto, and monotonically decreasing in $\epsilon$ for instance). Is there any use to thinking about functions in this way / has it been introduced in this way?",,"['real-analysis', 'limits']"
17,Proof that $\lim_{m\to\infty}(1+\frac{r}{m})^{mt}=e^{rt}$,Proof that,\lim_{m\to\infty}(1+\frac{r}{m})^{mt}=e^{rt},Can someone show me a straightforward proof that $\lim_{m\to\infty}(1+\frac{r}{m})^{mt}=e^{rt}$ Thanks!,Can someone show me a straightforward proof that $\lim_{m\to\infty}(1+\frac{r}{m})^{mt}=e^{rt}$ Thanks!,,"['limits', 'proof-writing']"
18,Evaluating limit $\lim_{x\to\infty}(x^2-\sqrt{x^4 + 7x^2 + 1})$,Evaluating limit,\lim_{x\to\infty}(x^2-\sqrt{x^4 + 7x^2 + 1}),"The problem is evaluate $$\lim_{x\to\infty}(x^2-\sqrt{x^4 + 7x^2 + 1})$$ I understand all of the calculus involved, but am having trouble figuring out how to get started with the algebra. I have tried factoring and using conjugates, but the only answer I am able to get is $-7$, which is incorrect. Any help would be appreciated. What I have done so far: $$\frac{(x^2-\sqrt{x^4+7x^2+1})(x^2+\sqrt{x^4+7x^2+1})}{ x^2+\sqrt{x^4+7x^2+1}}$$ results in $$\frac{-7x^2-1}{x^2+\sqrt{x^4+7x^2+1}}$$ factor out the $x^4$ under the radical, then divide numerator and denominator by $x^2$ to get $$\frac{-7-1/x^2}{1 + \sqrt{1+7/x^2+1/x^4}}$$ at this point the limit as x approaches infinity would be -7/2 or -3.5.","The problem is evaluate $$\lim_{x\to\infty}(x^2-\sqrt{x^4 + 7x^2 + 1})$$ I understand all of the calculus involved, but am having trouble figuring out how to get started with the algebra. I have tried factoring and using conjugates, but the only answer I am able to get is $-7$, which is incorrect. Any help would be appreciated. What I have done so far: $$\frac{(x^2-\sqrt{x^4+7x^2+1})(x^2+\sqrt{x^4+7x^2+1})}{ x^2+\sqrt{x^4+7x^2+1}}$$ results in $$\frac{-7x^2-1}{x^2+\sqrt{x^4+7x^2+1}}$$ factor out the $x^4$ under the radical, then divide numerator and denominator by $x^2$ to get $$\frac{-7-1/x^2}{1 + \sqrt{1+7/x^2+1/x^4}}$$ at this point the limit as x approaches infinity would be -7/2 or -3.5.",,"['calculus', 'limits']"
19,A limitation related to multinomial distribution.,A limitation related to multinomial distribution.,,"recently I have a problem about the multinomial distribution. Here, for positive integer $n$, $$ t_{n}=\sum_{i=1}^{n}a^{i}\sum_{i_{1},\ldots i_{n}}\left(\begin{array}{c} i\\ i_{1},\ldots i_{n} \end{array}\right)p_{1}^{i_{1}}\ldots p_{n}^{i_{n}}  $$ where $a\in\left(0,1\right)$. But the second summation has two conditions: $$ \begin{cases} i_{1}+i_{2}+\cdots+i_{n}=i\\ i_{1}+2i_{2}+\cdots+ni_{n}=n \end{cases} $$ In this case, can you obtain an easier expression for $t_n$. Actually, what I need is this: I know $\sum_{i=1}^{\infty}p_{i}=1$, and $\lim_{n\rightarrow\infty}\frac{p_{n+1}}{p_{n}}=c\in\left(0,1\right)$. Under these conditions, I need to get the limit of $t_{n}$  ratio, i.e., $\lim_{n\rightarrow\infty}\frac{t_{n+1}}{t_{n}}$. From the above expression, can I get the expression of $\lim_{n\rightarrow\infty}\frac{t_{n+1}}{t_{n}}$? Thank you in advance.","recently I have a problem about the multinomial distribution. Here, for positive integer $n$, $$ t_{n}=\sum_{i=1}^{n}a^{i}\sum_{i_{1},\ldots i_{n}}\left(\begin{array}{c} i\\ i_{1},\ldots i_{n} \end{array}\right)p_{1}^{i_{1}}\ldots p_{n}^{i_{n}}  $$ where $a\in\left(0,1\right)$. But the second summation has two conditions: $$ \begin{cases} i_{1}+i_{2}+\cdots+i_{n}=i\\ i_{1}+2i_{2}+\cdots+ni_{n}=n \end{cases} $$ In this case, can you obtain an easier expression for $t_n$. Actually, what I need is this: I know $\sum_{i=1}^{\infty}p_{i}=1$, and $\lim_{n\rightarrow\infty}\frac{p_{n+1}}{p_{n}}=c\in\left(0,1\right)$. Under these conditions, I need to get the limit of $t_{n}$  ratio, i.e., $\lim_{n\rightarrow\infty}\frac{t_{n+1}}{t_{n}}$. From the above expression, can I get the expression of $\lim_{n\rightarrow\infty}\frac{t_{n+1}}{t_{n}}$? Thank you in advance.",,"['calculus', 'limits', 'probability-distributions']"
20,How to strictly mathematically prove that definition is wrong?,How to strictly mathematically prove that definition is wrong?,,"I started to learn calculus by myself. First chapter of my textbook is about the limit of the sequence. I did all exercises in my textbook except one problem. There is some special problems in the end of this chapter: you need to find a mistake in the given definition. The last one is very weird. I don't understand how to strictly mathematically prove why this definition is wrong: $L(a_n)$ - length of the curve $a_n$. $D(a_n(P),S_{AB})$ - distance between point $P \in a_n$ and segment $S_{AB}$ (perpendicular from the point P to the segment $S_{AB}$). Definition: Sequence of smooth continuous curves  $a_n$ is called an approximation for segment $S_{AB}$ if: All curves $a_n$ begins at point A and ends at B. For any $m<n, \{m,n\} \in \mathbb{N}, \ L(a_m) \geq L(a_n)$. For each $\epsilon >0$ there exists a natural number $N$ such that, for every $n\geq N$, for every points $P \in a_n$  we have $D(a_n(P),S_{AB})<\epsilon$. If (1-3) true then the sequence of smooth continuous curves $a_n$ is an approximation for a segment $S_{AB}$, their length tends to the limit L, which is length of a segment $S_{AB}$. It is definitely wrong. With this definition we can prove that $5=4$. May be we should change in 2) that $L(a_m) > L(a_n)$? Or this is unfixable?","I started to learn calculus by myself. First chapter of my textbook is about the limit of the sequence. I did all exercises in my textbook except one problem. There is some special problems in the end of this chapter: you need to find a mistake in the given definition. The last one is very weird. I don't understand how to strictly mathematically prove why this definition is wrong: $L(a_n)$ - length of the curve $a_n$. $D(a_n(P),S_{AB})$ - distance between point $P \in a_n$ and segment $S_{AB}$ (perpendicular from the point P to the segment $S_{AB}$). Definition: Sequence of smooth continuous curves  $a_n$ is called an approximation for segment $S_{AB}$ if: All curves $a_n$ begins at point A and ends at B. For any $m<n, \{m,n\} \in \mathbb{N}, \ L(a_m) \geq L(a_n)$. For each $\epsilon >0$ there exists a natural number $N$ such that, for every $n\geq N$, for every points $P \in a_n$  we have $D(a_n(P),S_{AB})<\epsilon$. If (1-3) true then the sequence of smooth continuous curves $a_n$ is an approximation for a segment $S_{AB}$, their length tends to the limit L, which is length of a segment $S_{AB}$. It is definitely wrong. With this definition we can prove that $5=4$. May be we should change in 2) that $L(a_m) > L(a_n)$? Or this is unfixable?",,"['calculus', 'limits']"
21,Limit of Integral of square of function,Limit of Integral of square of function,,"Let $f: [0,1)\rightarrow \mathbb{R}$ be continuous and nonnegative on $[0,1)$. Prove that if $\lim_{a\rightarrow 1^{-}}{\int_{0}^{a}{f(x)^{2}dx}}$ exists, then $\lim_{a\rightarrow 1^{-}}{\int_{0}^{a}{f(x)dx}}$ exists too.","Let $f: [0,1)\rightarrow \mathbb{R}$ be continuous and nonnegative on $[0,1)$. Prove that if $\lim_{a\rightarrow 1^{-}}{\int_{0}^{a}{f(x)^{2}dx}}$ exists, then $\lim_{a\rightarrow 1^{-}}{\int_{0}^{a}{f(x)dx}}$ exists too.",,"['real-analysis', 'limits', 'integration']"
22,Computing: $\lim_{x\rightarrow0} \frac{\log(1+x)}{x^2}-\frac{1}{x}$,Computing:,\lim_{x\rightarrow0} \frac{\log(1+x)}{x^2}-\frac{1}{x},Could the following limit be computed without L'Hopital and Taylor? Thanks. $$\lim_{x\rightarrow0} \frac{\log(1+x)}{x^2}-\frac{1}{x}$$,Could the following limit be computed without L'Hopital and Taylor? Thanks. $$\lim_{x\rightarrow0} \frac{\log(1+x)}{x^2}-\frac{1}{x}$$,,"['real-analysis', 'limits', 'logarithms']"
23,"Why are discontinuities of monotonic $f : (a, b) \to \mathbb R$ countable? [duplicate]",Why are discontinuities of monotonic  countable? [duplicate],"f : (a, b) \to \mathbb R","This question already has answers here : Closed 12 years ago . Possible Duplicate: Showing properties of discontinuous points of a strictly increasing function How to show that a set of discontinuous points of an increasing function is at most countable I'm struggling to find an elegant proof of the following problem Let $f : (a, b) \to \mathbb R$ be non-decreasing, $a, b \in \mathbb R$, then $f$ only has countably many discontinuities. My intuition was to show by contradiction that the set of discontinuities $N \subseteq (a,b)$ is discrete, i.e. all discontinuities are isolated. From there on it's easy to prove that there is an injective function $N \to \mathbb Q$. But does my first step make sense? Say we had non-isolated discontinuities like $\epsilon > 0, x_0 \in N$ such that $B_\epsilon(x_0) \subseteq N$ - how could one derive a contradiction? I've already shown that $$\lim_{x \nearrow x_0} f(x) \text{ and } \lim_{x \searrow x_0} f(x)$$ exist for all points $x_0 \in (a, b)$ and that $f$ is continuous at $x_0$ iff both limits equal. I just somehow fail to do the final step properly. Any thoughts, please?","This question already has answers here : Closed 12 years ago . Possible Duplicate: Showing properties of discontinuous points of a strictly increasing function How to show that a set of discontinuous points of an increasing function is at most countable I'm struggling to find an elegant proof of the following problem Let $f : (a, b) \to \mathbb R$ be non-decreasing, $a, b \in \mathbb R$, then $f$ only has countably many discontinuities. My intuition was to show by contradiction that the set of discontinuities $N \subseteq (a,b)$ is discrete, i.e. all discontinuities are isolated. From there on it's easy to prove that there is an injective function $N \to \mathbb Q$. But does my first step make sense? Say we had non-isolated discontinuities like $\epsilon > 0, x_0 \in N$ such that $B_\epsilon(x_0) \subseteq N$ - how could one derive a contradiction? I've already shown that $$\lim_{x \nearrow x_0} f(x) \text{ and } \lim_{x \searrow x_0} f(x)$$ exist for all points $x_0 \in (a, b)$ and that $f$ is continuous at $x_0$ iff both limits equal. I just somehow fail to do the final step properly. Any thoughts, please?",,"['real-analysis', 'limits']"
24,Can I apply taylor expansion to exponents?,Can I apply taylor expansion to exponents?,,"If I have a limit in this form: $$\lim_{x\to x_0}f(x)^{g(x)}$$ can I expand $f(x)$ and $g(x)$ to end up with a limit like: $$\lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2 + \cdots\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2 + \cdots}$$ then cut up the series and write: $$\lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2}$$ and work on this one instead? Or is this not allowed for some obscure reason? $x_0$ is within the radius of convergence of both expansions. EDIT: I'll add an example which I've seen today on this website: $$\lim_{\theta\to0^+}(\sin\theta)^{\sin\theta-\sin^2\theta}$$ Can I use the fact that $\sin\theta\sim\theta$ and say that this limit is equivalent to the following? $$\lim_{\theta\to0^+}\theta^{\theta(1-\theta)}$$ The result is the same, but it could be a coincidence. Is this procedure correct here? In the general case? Thanks in advance.","If I have a limit in this form: can I expand and to end up with a limit like: then cut up the series and write: and work on this one instead? Or is this not allowed for some obscure reason? is within the radius of convergence of both expansions. EDIT: I'll add an example which I've seen today on this website: Can I use the fact that and say that this limit is equivalent to the following? The result is the same, but it could be a coincidence. Is this procedure correct here? In the general case? Thanks in advance.",\lim_{x\to x_0}f(x)^{g(x)} f(x) g(x) \lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2 + \cdots\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2 + \cdots} \lim_{x\to x_0}\left(f(0) + f'(0)x + \frac{f''(0)}2x^2\right)^{g(0) + g'(0)x + \frac{g''(0)}2x^2} x_0 \lim_{\theta\to0^+}(\sin\theta)^{\sin\theta-\sin^2\theta} \sin\theta\sim\theta \lim_{\theta\to0^+}\theta^{\theta(1-\theta)},"['calculus', 'limits', 'taylor-expansion']"
25,"If $\lim_{n\to \infty} x_n =a$ and $\lim_{n\to \infty} (x_n - y_n) =0$, then $\lim_{n\to \infty} y_n =a$?","If  and , then ?",\lim_{n\to \infty} x_n =a \lim_{n\to \infty} (x_n - y_n) =0 \lim_{n\to \infty} y_n =a,"I guess I get the gist of the idea but I don't know how to write it rigorously, I know that we have: For all $\epsilon > 0$ , there is $n_0 \in N$ such that if $n>n_0$ , then $|x_n - a| <\epsilon$ . For all $\epsilon > 0$ , there is $n_0 \in N$ such that if $n>n_0$ , then $|x_n - y_n - 0| <\epsilon$ . What do I do now? I tried to use the triangle inequality: $$|(x_n - a)| -|(y_n -a)| \leq|(x_n - a) -(y_n -a)|=|x_n - y_n - (a-a)| <\epsilon$$ And due to the first inequality ( $|x_n - a| <\epsilon$ ), we can ""squeeze it to 0"" but I guess that with this inequality, we can't guarantee that we can ""squeeze"" ( $|y_n - a| <\epsilon$ ) to 0.","I guess I get the gist of the idea but I don't know how to write it rigorously, I know that we have: For all , there is such that if , then . For all , there is such that if , then . What do I do now? I tried to use the triangle inequality: And due to the first inequality ( ), we can ""squeeze it to 0"" but I guess that with this inequality, we can't guarantee that we can ""squeeze"" ( ) to 0.",\epsilon > 0 n_0 \in N n>n_0 |x_n - a| <\epsilon \epsilon > 0 n_0 \in N n>n_0 |x_n - y_n - 0| <\epsilon |(x_n - a)| -|(y_n -a)| \leq|(x_n - a) -(y_n -a)|=|x_n - y_n - (a-a)| <\epsilon |x_n - a| <\epsilon |y_n - a| <\epsilon,"['real-analysis', 'limits']"
26,"Do I understand the definition of limit existence at a point, the definition of continuity at a point, and the difference between them?","Do I understand the definition of limit existence at a point, the definition of continuity at a point, and the difference between them?",,"I am trying to make sure I understand the definition of limit existence at a point, the definition of continuity at a point, and the exact difference between the two definitions. So below I state the definitions as I understand them, and I hope that someone can tell me whether or not the definitions I have given are correct, specifically if what I have identified as the difference between them is correct. The limit $L$ of a function $f$ exists at a point $p$ if and only if: $\forall\epsilon>0$ , $\exists\delta>0$ such that if $x$ is in the punctured $\delta$ -neighbourhood of $p$ , then $f(x)$ must be in the given $\epsilon$ -neighbourhood of $L$ (because $x$ is never equal to $p$ , it doesn't matter if the $\epsilon$ -neighbourhood does or does not contain $L$ , nor what the value of $L$ is, aslong as it is a real number). A function $f$ is continuous at a point $p$ if and only if: $\forall\epsilon>0$ , $\exists\delta>0$ such that if $x$ is in the unpunctured $\delta$ -neighbourhood of $p$ , then $f(x)$ must be in the given $\epsilon$ -neighbourhood of $L$ (because $p$ must be contained in every $\delta$ -neighbourhood we can create, this implies that all $\epsilon$ -neighbourhoods we choose must always contain $f(p)$ , and thus $f(p)$ must exist, and $f(p)$ must be equal to $L$ , if the statement is to hold true). Is my understanding correct? Are the two definitions essentially identical except for whether or not the $\delta$ -neighbourhood is punctured? EDIT: From Vincent Batens' answer I have learned that my understanding of the first definition (the limit definition) is wrong ; the $\epsilon$ -neighbourhood must be unpunctured because there are many situations in which the punctured $\delta$ -neighbourhood contains an $x$ value such that $f(x)=L$ . For example, if $f(x)$ is a constant function.. or if $f(x)$ is constant in a neighbourhood around $L$ .. or if we have something like the case of $\lim_{x\to0}x\sin(\frac{1}{x})=0$ where $f(x)$ hits $L$ at an ever-increasing frequency as $x$ approaches $0$ . Essentially, in any situation where we can have $f(x)=L$ for $x$ -values inside the punctured $\delta$ -neighbourhood, the limit definition as I have described it breaks down and doesn't work as it should; the definition would say that the limit doesn't exist when it clearly does. Thus it is not true that ""it doesn't matter if the $\epsilon$ -neighbourhood does or does not contain $L$ "".. there are many cases in which it does matter, and so we should specify that, in general, the $\epsilon$ -neighbourhood must be unpunctured, so that we account for such cases in our definition. And of course when we have $f(x)\ne L$ for all $x$ in any punctured $\delta$ -neighbourhood of $p$ we can create, then the fact that there is an $L$ sitting in every $\epsilon$ -neighbourhood given has no bearing on the limit statement; it does no harm.","I am trying to make sure I understand the definition of limit existence at a point, the definition of continuity at a point, and the exact difference between the two definitions. So below I state the definitions as I understand them, and I hope that someone can tell me whether or not the definitions I have given are correct, specifically if what I have identified as the difference between them is correct. The limit of a function exists at a point if and only if: , such that if is in the punctured -neighbourhood of , then must be in the given -neighbourhood of (because is never equal to , it doesn't matter if the -neighbourhood does or does not contain , nor what the value of is, aslong as it is a real number). A function is continuous at a point if and only if: , such that if is in the unpunctured -neighbourhood of , then must be in the given -neighbourhood of (because must be contained in every -neighbourhood we can create, this implies that all -neighbourhoods we choose must always contain , and thus must exist, and must be equal to , if the statement is to hold true). Is my understanding correct? Are the two definitions essentially identical except for whether or not the -neighbourhood is punctured? EDIT: From Vincent Batens' answer I have learned that my understanding of the first definition (the limit definition) is wrong ; the -neighbourhood must be unpunctured because there are many situations in which the punctured -neighbourhood contains an value such that . For example, if is a constant function.. or if is constant in a neighbourhood around .. or if we have something like the case of where hits at an ever-increasing frequency as approaches . Essentially, in any situation where we can have for -values inside the punctured -neighbourhood, the limit definition as I have described it breaks down and doesn't work as it should; the definition would say that the limit doesn't exist when it clearly does. Thus it is not true that ""it doesn't matter if the -neighbourhood does or does not contain "".. there are many cases in which it does matter, and so we should specify that, in general, the -neighbourhood must be unpunctured, so that we account for such cases in our definition. And of course when we have for all in any punctured -neighbourhood of we can create, then the fact that there is an sitting in every -neighbourhood given has no bearing on the limit statement; it does no harm.",L f p \forall\epsilon>0 \exists\delta>0 x \delta p f(x) \epsilon L x p \epsilon L L f p \forall\epsilon>0 \exists\delta>0 x \delta p f(x) \epsilon L p \delta \epsilon f(p) f(p) f(p) L \delta \epsilon \delta x f(x)=L f(x) f(x) L \lim_{x\to0}x\sin(\frac{1}{x})=0 f(x) L x 0 f(x)=L x \delta \epsilon L \epsilon f(x)\ne L x \delta p L \epsilon,"['real-analysis', 'calculus', 'limits', 'continuity']"
27,Trick to find continuity of a multivariate function [duplicate],Trick to find continuity of a multivariate function [duplicate],,"This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed last year . Let $f(x,y)=\frac{x^2y}{x^4+y^2} $ if $(x,y) \ne (0,0)$ and $f(0,0)=0$ if $(x,y)=(0,0)$ This is a question from a university entrance exam. Generally i get stuck in these types of problems where the numerator and denominator powers are tough to cancel out and using the polar coordinates makes it more complicated.(Atleast on my part) I came across the solution which went like this: $m=2,n=1,i=4,j=2 (even)$ and $mj+ni=ij$ so the function is not continuous at $(0,0)$ Does anyone have any idea behind the logic of this trick?","This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed last year . Let if and if This is a question from a university entrance exam. Generally i get stuck in these types of problems where the numerator and denominator powers are tough to cancel out and using the polar coordinates makes it more complicated.(Atleast on my part) I came across the solution which went like this: and so the function is not continuous at Does anyone have any idea behind the logic of this trick?","f(x,y)=\frac{x^2y}{x^4+y^2}  (x,y) \ne (0,0) f(0,0)=0 (x,y)=(0,0) m=2,n=1,i=4,j=2 (even) mj+ni=ij (0,0)","['limits', 'multivariable-calculus']"
28,Can I use L'Hopital's rule when finding a derivative using the limit definition?,Can I use L'Hopital's rule when finding a derivative using the limit definition?,,"When finding the derivative of $f(x) = \sqrt x$ via the limit definition, one gets $$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h \to 0} \frac{\sqrt{x+h} - \sqrt x}{h}$$ For this, I could get the answer from applying L'Hopital's rule... but to me, this line of reasoning is a bit circular. I'm computing a derivative from first principles while needing to use derivative rules (from L'Hopital's) to compute this derivative. Can I use L'Hopital's rule when finding a derivative using the limit definition?","When finding the derivative of via the limit definition, one gets For this, I could get the answer from applying L'Hopital's rule... but to me, this line of reasoning is a bit circular. I'm computing a derivative from first principles while needing to use derivative rules (from L'Hopital's) to compute this derivative. Can I use L'Hopital's rule when finding a derivative using the limit definition?",f(x) = \sqrt x f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h \to 0} \frac{\sqrt{x+h} - \sqrt x}{h},"['calculus', 'limits', 'derivatives', 'limits-without-lhopital']"
29,Evaluate the limit: $\lim\limits_{n\to \infty}\sqrt[n]{\frac{\ln(n)}{2^n+1}}$,Evaluate the limit:,\lim\limits_{n\to \infty}\sqrt[n]{\frac{\ln(n)}{2^n+1}},"My problem is actually deeper than just evaluation this limit, but learning how to solve the following limit will help me a lot: $$\lim_{n\to \infty}\sqrt[n]{\frac{\ln(n)}{2^n+1}}$$ Just some context. I am studying Convergence Tests for Infinite Series. And as it is known, one of the tests is the root test. I understand how it works and I have a good feeling of when I can use it, but that implies that I must know how to solve this kind of limit. My textbook assumes that I already know how to solve this kind of limit and does not provide the step by step. I assume that most limits of this kind will have a similar strategy to solve. It would help me a lot if someone could help me. Specially if you could provide a solution without using l'Hopital.","My problem is actually deeper than just evaluation this limit, but learning how to solve the following limit will help me a lot: Just some context. I am studying Convergence Tests for Infinite Series. And as it is known, one of the tests is the root test. I understand how it works and I have a good feeling of when I can use it, but that implies that I must know how to solve this kind of limit. My textbook assumes that I already know how to solve this kind of limit and does not provide the step by step. I assume that most limits of this kind will have a similar strategy to solve. It would help me a lot if someone could help me. Specially if you could provide a solution without using l'Hopital.",\lim_{n\to \infty}\sqrt[n]{\frac{\ln(n)}{2^n+1}},"['real-analysis', 'limits', 'limits-without-lhopital']"
30,"Prove That The Directional Derivative Exists at (0,0)","Prove That The Directional Derivative Exists at (0,0)",,"Define $f:\mathbf{R}^{2}\rightarrow\mathbf{R}$ by $f(x,y) = \frac{x^{2}y}{x^{4}+y^{2}}, (x,y)\neq (0,0)$ with $f(0,0):=0$ I am attempting to show that the directional derivative, $\partial_{(a,b)}f(0,0)$ exists for all real $(a,b)\neq (0,0)$ I have managed to show that the directional derivative is $\frac{a^{2}}{b}$ , for $b\neq0$ , but I am unable to figure out how to prove it for the $b=0$ case. Here is my workings for $b\neq0$ : The directional derivative exists if the limit $\lim_{t\rightarrow0}\frac{f(t(a,b))-f(0,0)}{t}$ exists. $\partial_{(a,b)}f(0,0) = \lim_{t\rightarrow0}\frac{\frac{(at)^{2}(bt)}{(at)^{4}+(bt)^{2}}}{t}=\lim_{t\rightarrow0}\frac{a^{2}b}{t^{2}a^{4}+b^{2}}=\frac{a^2}{b}$ This clearly is not the case for $b=0$ , but, provided $a\neq0$ , the directional derivative should exist.","Define by with I am attempting to show that the directional derivative, exists for all real I have managed to show that the directional derivative is , for , but I am unable to figure out how to prove it for the case. Here is my workings for : The directional derivative exists if the limit exists. This clearly is not the case for , but, provided , the directional derivative should exist.","f:\mathbf{R}^{2}\rightarrow\mathbf{R} f(x,y) = \frac{x^{2}y}{x^{4}+y^{2}}, (x,y)\neq (0,0) f(0,0):=0 \partial_{(a,b)}f(0,0) (a,b)\neq (0,0) \frac{a^{2}}{b} b\neq0 b=0 b\neq0 \lim_{t\rightarrow0}\frac{f(t(a,b))-f(0,0)}{t} \partial_{(a,b)}f(0,0) = \lim_{t\rightarrow0}\frac{\frac{(at)^{2}(bt)}{(at)^{4}+(bt)^{2}}}{t}=\lim_{t\rightarrow0}\frac{a^{2}b}{t^{2}a^{4}+b^{2}}=\frac{a^2}{b} b=0 a\neq0","['calculus', 'limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
31,Should we write $\lim_\limits{x\to -\infty} \frac{3x^2-1}{x^3+4x+3}=0^-$ instead of just $0$?,Should we write  instead of just ?,\lim_\limits{x\to -\infty} \frac{3x^2-1}{x^3+4x+3}=0^- 0,"This may sounds silly but I have a doubt about a notation. Say I have $$\lim_{x\to -\infty} \dfrac{3x^2-1}{x^3+4x+3}$$ The limit is obviously zero, for the polynomial at the denominator is of a higher degree than the one at the numerator. Yet I was thinking: should we (or could we, or must we...) be more precise and write the result as $0^-$ instead of just $0$ , or is that something wrong or nonsensical? It's like when $$\lim_{x\to 0} \dfrac{1}{x}$$ does not exist unless we specify if it's $x\to 0^+$ or $x\to 0^-$ , but reversed.","This may sounds silly but I have a doubt about a notation. Say I have The limit is obviously zero, for the polynomial at the denominator is of a higher degree than the one at the numerator. Yet I was thinking: should we (or could we, or must we...) be more precise and write the result as instead of just , or is that something wrong or nonsensical? It's like when does not exist unless we specify if it's or , but reversed.",\lim_{x\to -\infty} \dfrac{3x^2-1}{x^3+4x+3} 0^- 0 \lim_{x\to 0} \dfrac{1}{x} x\to 0^+ x\to 0^-,"['calculus', 'limits', 'notation']"
32,Why does the delta depend on the epsilon in the definition of the limit of a function?,Why does the delta depend on the epsilon in the definition of the limit of a function?,,"For example, why can't it be a definition of the limit? $$\lim_{x \to a}f(x)=l\Leftrightarrow( \forall \delta> 0,\exists\varepsilon > 0:|f(x)-l|<\varepsilon \Rightarrow|x-a|<\delta ) $$","For example, why can't it be a definition of the limit?","\lim_{x \to a}f(x)=l\Leftrightarrow( \forall \delta> 0,\exists\varepsilon > 0:|f(x)-l|<\varepsilon \Rightarrow|x-a|<\delta ) ","['calculus', 'limits', 'analysis']"
33,Evaluating $\lim_{x\to0}{{\left(\frac{\tan{(1+\tan x)}}{\tan{(1+\sin x)}}\right)}^{1/x^3}}$ [closed],Evaluating  [closed],\lim_{x\to0}{{\left(\frac{\tan{(1+\tan x)}}{\tan{(1+\sin x)}}\right)}^{1/x^3}},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am working on $$\lim_{x\to0}{{\left(\frac{\tan{(1+\tan x)}}{\tan{(1+\sin x)}}\right)}^{1/x^3}}$$ I can see a successful strategy is to take $\ln$ first and then use L'Hospital rule for 3 times. However, I wonder if there are simpler methods. Any ideas would be appreciated!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am working on I can see a successful strategy is to take first and then use L'Hospital rule for 3 times. However, I wonder if there are simpler methods. Any ideas would be appreciated!",\lim_{x\to0}{{\left(\frac{\tan{(1+\tan x)}}{\tan{(1+\sin x)}}\right)}^{1/x^3}} \ln,"['real-analysis', 'limits', 'limits-without-lhopital']"
34,Limit of $(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2}$,Limit of,(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2},"Find $a,b$ of: $$\lim_{x \to \infty}(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2}$$ I can't use L'hopital, I tried multiplying by the conjugate, and solving it, $$\lim_{x \to \infty}(\sqrt{4x^2+2x+1}-(ax+b))\cdot \frac{\sqrt{4x^2+2x+1}+(ax+b)}{\sqrt{4x^2+2x+1}+(ax+b)}$$ $$=\lim_{x \to \infty}\frac{4x^2+2x+1-(ax+b)^2}{\sqrt{4x^2+2x+1}+(ax+b)} = \lim_{x \to \infty}\frac{x^2\left(4+\frac{2}{x}+\frac{1}{x^2}-a^2-\frac{2ab}{x}-\frac{b^2}{x^2}\right)}{x^2\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)}$$ Applying limit on the numerator and denominator $$\frac{\lim_{x \to \infty}\left(4-a^2+\frac{2-2ab}{x}+\frac{1-b^2}{x^2}\right)}{\lim_{x \to \infty}\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)}$$ It can be seen that the denominator tends to $0$","Find of: I can't use L'hopital, I tried multiplying by the conjugate, and solving it, Applying limit on the numerator and denominator It can be seen that the denominator tends to","a,b \lim_{x \to \infty}(\sqrt{4x^2+2x+1}-ax-b) = -\frac{1}{2} \lim_{x \to \infty}(\sqrt{4x^2+2x+1}-(ax+b))\cdot \frac{\sqrt{4x^2+2x+1}+(ax+b)}{\sqrt{4x^2+2x+1}+(ax+b)} =\lim_{x \to \infty}\frac{4x^2+2x+1-(ax+b)^2}{\sqrt{4x^2+2x+1}+(ax+b)} = \lim_{x \to \infty}\frac{x^2\left(4+\frac{2}{x}+\frac{1}{x^2}-a^2-\frac{2ab}{x}-\frac{b^2}{x^2}\right)}{x^2\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)} \frac{\lim_{x \to \infty}\left(4-a^2+\frac{2-2ab}{x}+\frac{1-b^2}{x^2}\right)}{\lim_{x \to \infty}\left(\sqrt{\frac{4x^2+2x+1}{x^4}}+\frac{a}{x}+\frac{b}{x^2}\right)} 0","['calculus', 'limits', 'limits-without-lhopital']"
35,"How to prove that the limit $\lim_{(x,y) \to (0,0)} \frac{xy^2}{|x|^5 + y^2}$ does not exist?",How to prove that the limit  does not exist?,"\lim_{(x,y) \to (0,0)} \frac{xy^2}{|x|^5 + y^2}","At first I thought that the limit existed, but since the degree of the numerator is less than the degree of the denominator, that gives a hint as to the nonexistence of the limit. As such, the squeeze theorem cannot be applied and I have to find 2 paths where the limits give different values. I've tried many ways but they all end up giving 0.","At first I thought that the limit existed, but since the degree of the numerator is less than the degree of the denominator, that gives a hint as to the nonexistence of the limit. As such, the squeeze theorem cannot be applied and I have to find 2 paths where the limits give different values. I've tried many ways but they all end up giving 0.",,"['calculus', 'limits']"
36,Does the limit $\lim_{x\to 0} \sqrt{x^3 - x^2}$ exist or not?,Does the limit  exist or not?,\lim_{x\to 0} \sqrt{x^3 - x^2},"I am having some arguments with a friend about the following limit: $$\lim_{x\to 0} \sqrt{x^3 - x^2}$$ FACTS: the domain of the function is $x\in \{0\}\cup [1,\ +\infty)$ and $0$ is an isolated point. My friend says the limit doesn't exist, whilst to me it is $0$ . Who is right? In my opinion, if we take $\epsilon > 0$ and $\delta = 1/2$ , we calculate $|f(x) - f(0)| < 0$ hence less than $\epsilon$ , so for any positive $\epsilon$ we have positive $\delta$ such that whenever $x$ is in $\delta$ -neighbourhood of $0$ , the quantity $|f(x) - f(0)|$ is less than $\epsilon$ so $f$ is continuous at $0$ . Am I right or wrong? Can someone pease make a limpid clarification of the existence (or not) of this limit? Thank you in advance!","I am having some arguments with a friend about the following limit: FACTS: the domain of the function is and is an isolated point. My friend says the limit doesn't exist, whilst to me it is . Who is right? In my opinion, if we take and , we calculate hence less than , so for any positive we have positive such that whenever is in -neighbourhood of , the quantity is less than so is continuous at . Am I right or wrong? Can someone pease make a limpid clarification of the existence (or not) of this limit? Thank you in advance!","\lim_{x\to 0} \sqrt{x^3 - x^2} x\in \{0\}\cup [1,\ +\infty) 0 0 \epsilon > 0 \delta = 1/2 |f(x) - f(0)| < 0 \epsilon \epsilon \delta x \delta 0 |f(x) - f(0)| \epsilon f 0","['calculus', 'limits', 'analysis', 'limits-without-lhopital', 'epsilon-delta']"
37,Double limit of the nth derivative of $f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x})$,Double limit of the nth derivative of,f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x}),"I have tried to compute $$\lim\limits_{n\to \infty}\lim\limits_{x\searrow 0}f^{(n)}(x)$$ for $f:[0,\infty)\to \mathbb{R},~f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x})$ . By noticing that $f$ and its derivatives satisfy $2f'(x)+4xf''(x)=f(x)$ and differentiating successively in this equality one can deduce a recurrence relation for $\lim\limits_{x\searrow 0}f^{(n)}(x)$ and thus obtain $\lim\limits_{x\searrow 0}f^{(n)}(x)=2\cdot\frac{n!}{(2n)!}$ , but only assuming that the limit $\lim\limits_{x\searrow 0}~xf^{(n)}(x)$ is $0$ for any $n\in \mathbb{N}$ , which I could not prove rigorously, but I ""feel"" that is true. Of course, any other idea for the computation of the above double limit is welcomed.","I have tried to compute for . By noticing that and its derivatives satisfy and differentiating successively in this equality one can deduce a recurrence relation for and thus obtain , but only assuming that the limit is for any , which I could not prove rigorously, but I ""feel"" that is true. Of course, any other idea for the computation of the above double limit is welcomed.","\lim\limits_{n\to \infty}\lim\limits_{x\searrow 0}f^{(n)}(x) f:[0,\infty)\to \mathbb{R},~f(x)=\exp(\sqrt{x})+\exp(-\sqrt{x}) f 2f'(x)+4xf''(x)=f(x) \lim\limits_{x\searrow 0}f^{(n)}(x) \lim\limits_{x\searrow 0}f^{(n)}(x)=2\cdot\frac{n!}{(2n)!} \lim\limits_{x\searrow 0}~xf^{(n)}(x) 0 n\in \mathbb{N}","['real-analysis', 'limits', 'derivatives', 'recurrence-relations']"
38,Does $\lim_{x\to\infty} f(x+1)/f(x) = 1$ imply $\lim_{x\to\infty} f(f(x+1))/f(f(x)) = 1$?,Does  imply ?,\lim_{x\to\infty} f(x+1)/f(x) = 1 \lim_{x\to\infty} f(f(x+1))/f(f(x)) = 1,"As mentioned above, does \begin{align} \lim_{x\to\infty} \frac{f(x+1)}{f(x)} = 1 \end{align} imply \begin{align} \lim_{x\to\infty} \frac{f(f(x+1))}{f(f(x))} = 1? \end{align} Here, $f:\mathbb{R}\to \mathbb{R}$ is a function such that (1) $f(x)\neq 0$ for all $x\in\mathbb{R}$ (or for all $x\ge x_0$ for some $x_0$ ); (2) $\lim_{x\to\infty} f(x) =\infty$ ; (3) $f$ is increasing. This is not a homework question, but I just wonder if this statement is true (Indeed, I observe that this holds for $f(x)=\log(x)$ so I have this question). I would appreciate any counterexample/proof/hints. Many thanks!","As mentioned above, does imply Here, is a function such that (1) for all (or for all for some ); (2) ; (3) is increasing. This is not a homework question, but I just wonder if this statement is true (Indeed, I observe that this holds for so I have this question). I would appreciate any counterexample/proof/hints. Many thanks!","\begin{align}
\lim_{x\to\infty} \frac{f(x+1)}{f(x)} = 1
\end{align} \begin{align}
\lim_{x\to\infty} \frac{f(f(x+1))}{f(f(x))} = 1?
\end{align} f:\mathbb{R}\to \mathbb{R} f(x)\neq 0 x\in\mathbb{R} x\ge x_0 x_0 \lim_{x\to\infty} f(x) =\infty f f(x)=\log(x)","['calculus', 'limits']"
39,How to take limits of 'almost Riemann' sums like $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n)$,How to take limits of 'almost Riemann' sums like,\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n),"How can I solve limits of sums that are 'almost' Riemann, but can't be written in the typical form (i.e, $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k/n)$ which we can rewrite as an integral $\int_0^1 f(x) dx$ )? For example, I'd like to turn $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n)$ into an integral. More generally, is there a closed-form way to convert sums like $\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k h(n)/n)$ into integrals, when $h(n) = o(n)$ ? Clearly such sums don't always converge - if $h(n) = \log(n)$ , and $f(kh(n)/n) = k\log(n)/n$ , the limit is unbounded. What conditions on $f, h$ might we require for convergence?","How can I solve limits of sums that are 'almost' Riemann, but can't be written in the typical form (i.e, which we can rewrite as an integral )? For example, I'd like to turn into an integral. More generally, is there a closed-form way to convert sums like into integrals, when ? Clearly such sums don't always converge - if , and , the limit is unbounded. What conditions on might we require for convergence?","\lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k/n) \int_0^1 f(x) dx \lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} \cos (a \pi k \log(n)/n) \lim_{n \to \infty} \sum_{k=0}^n \frac{1}{n} f(k h(n)/n) h(n) = o(n) h(n) = \log(n) f(kh(n)/n) = k\log(n)/n f, h","['calculus', 'integration', 'limits', 'riemann-integration', 'riemann-sum']"
40,"Prove that $\lim_{n\infty}\frac{\sqrt{n^2+a^2}}{n}=1$ using ""$\varepsilon \to N$"" definition","Prove that  using """" definition",\lim_{n\infty}\frac{\sqrt{n^2+a^2}}{n}=1 \varepsilon \to N,"how to prove this equation with the "" $\varepsilon \to N$ ""  definition?  I feel have some trouble.Thanks! $\lim_{n \to\infty}\frac{\sqrt{n^2+a^2}}{n}=1\\$ I complete my solution: when $a=0$ it's obviously true. When $a \neq 0$ : $$\begin{align} \frac{\sqrt{n^2+a^2}}{n}-1&=\frac{\sqrt{n^2+a^2}}{n}-\frac{n}{n} \\&=\frac{\sqrt{n^2+a^2}-\sqrt{n^2}}{n}\\&=\frac{1}{n}\cdot \frac{a^2}{\sqrt{n^2+a^2}+n}\lt\frac{a^2}{\sqrt{n^2}+n}\cdot\frac{1}{n}\lt\frac{a^2}{n(\sqrt{n^2}+\sqrt{n^2})}=\frac{a^2}{2n} \\ for\quad \forall \varepsilon \gt0, \quad when\quad \frac{a^2}{2N^2}\lt \varepsilon, \mathit N\gt \frac{a}{\sqrt{2\varepsilon}}+1,|\frac{\sqrt{n^2+a^2}}{n}-1|\lt\varepsilon. \end{align}$$","how to prove this equation with the "" ""  definition?  I feel have some trouble.Thanks! I complete my solution: when it's obviously true. When :","\varepsilon \to N \lim_{n \to\infty}\frac{\sqrt{n^2+a^2}}{n}=1\\ a=0 a \neq 0 \begin{align}
\frac{\sqrt{n^2+a^2}}{n}-1&=\frac{\sqrt{n^2+a^2}}{n}-\frac{n}{n}
\\&=\frac{\sqrt{n^2+a^2}-\sqrt{n^2}}{n}\\&=\frac{1}{n}\cdot \frac{a^2}{\sqrt{n^2+a^2}+n}\lt\frac{a^2}{\sqrt{n^2}+n}\cdot\frac{1}{n}\lt\frac{a^2}{n(\sqrt{n^2}+\sqrt{n^2})}=\frac{a^2}{2n}
\\ for\quad \forall \varepsilon \gt0, \quad when\quad \frac{a^2}{2N^2}\lt \varepsilon, \mathit N\gt \frac{a}{\sqrt{2\varepsilon}}+1,|\frac{\sqrt{n^2+a^2}}{n}-1|\lt\varepsilon.
\end{align}","['real-analysis', 'limits']"
41,Prove that $f( \limsup \limits_{x \to +\infty} x_n)$ = $ \limsup \limits_{x \to +\infty} f(x_n)$ [closed],Prove that  =  [closed],f( \limsup \limits_{x \to +\infty} x_n)  \limsup \limits_{x \to +\infty} f(x_n),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The problem is divided into two questions: $x_n$ is a real valued sequence. Prove that $f( \limsup \limits_{n \to +\infty} x_n)$ = $ \limsup \limits_{n \to +\infty} f(x_n)$ given that $f$ is continuous and increasing. What can we say when $f$ is decreasing. If it were a regular limit I can just pass the $\lim$ inside the function and we're done. But this is $\limsup$ . Thanks for your help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The problem is divided into two questions: is a real valued sequence. Prove that = given that is continuous and increasing. What can we say when is decreasing. If it were a regular limit I can just pass the inside the function and we're done. But this is . Thanks for your help.",x_n f( \limsup \limits_{n \to +\infty} x_n)  \limsup \limits_{n \to +\infty} f(x_n) f f \lim \limsup,"['real-analysis', 'limits', 'functions']"
42,"If $f(s) =\frac{s}{\sqrt{1-s^2}}$, how to evaluate $\lim_{s\to \pm\infty}f^{-1}(s)$?","If , how to evaluate ?",f(s) =\frac{s}{\sqrt{1-s^2}} \lim_{s\to \pm\infty}f^{-1}(s),"Let $f:[0,1]\to\mathbb{R}$ be such that $$f(s) =\frac{s}{\sqrt{1-s^2}}.$$ I wrote on my notes during the calculus class that $$\lim_{s\to +\infty}f^{-1}(s)=1\quad\mbox{ and }\quad \lim_{s\to -\infty}f^{-1}(s)=-1.$$ Looking back, how it is possible to evaluate these limits? Actually, Im not even able to explicitly compute $f^{-1}(s)$ . Could someone please help me to understand it? Thank you in advance!","Let be such that I wrote on my notes during the calculus class that Looking back, how it is possible to evaluate these limits? Actually, Im not even able to explicitly compute . Could someone please help me to understand it? Thank you in advance!","f:[0,1]\to\mathbb{R} f(s) =\frac{s}{\sqrt{1-s^2}}. \lim_{s\to +\infty}f^{-1}(s)=1\quad\mbox{ and }\quad \lim_{s\to -\infty}f^{-1}(s)=-1. f^{-1}(s)","['real-analysis', 'calculus', 'limits', 'inverse-function']"
43,"Understanding what $\displaystyle \lim\sup_{x \to a, x \in E} f(x)$ means",Understanding what  means,"\displaystyle \lim\sup_{x \to a, x \in E} f(x)","I recently learned of the following extension to the definition of a limit: Let $S \subset \mathbb{R}$ , let $f: S \rightarrow \mathbb{R}$ , let $a$ be a limit point of $S$ , and let $L \in \mathbb{R}$ . Then $$\lim_{x \to a \\ x \in S } f(x) = L$$ means the following: For each $\epsilon > 0$ there exists some $\delta > 0$ such that $$x \in S \text{ and } |x-a| < \delta \implies |f(x) - L| < \epsilon.$$ (I assume this definition extends similarly for general metric spaces.) I am now trying to understand the analogous extension for limit sup's and lim inf's. To quote Wikipedia : There is a notion of lim sup and lim inf for functions defined on a metric space whose relationship to limits of real-valued functions mirrors that of the relation between the lim sup, lim inf, and the limit of a real sequence. Take a metric space $X$ , a subspace $E$ contained in $X$ , and a function $f : E \rightarrow R$ . Define, for any limit point a $a$ of $E$ , \begin{align} \limsup_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\sup \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3cm} (1) \end{align} and $$ \liminf_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\inf \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3.3cm} (2) $$ where $B(a;\epsilon)$ denotes the metric ball of radius $\epsilon$ about $a$ . My first question is: In the context of the setting described above, do $\displaystyle \lim\sup_{x \to a \\ x \in E} f(x) = L$ and $\displaystyle \lim\sup_{x \to a} f(x) = L$ mean the same thing? (And similarly for $\lim\inf$ ?). My question comes from my reading of Dini numbers in Stein and Shakarchi's Real Analysis . They define $$D^{+}(F)(x) := \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x),  \hspace{3cm} (3) $$ where $\Delta_h(F)(x) := \frac{F(x + h) - F(x)}{h}$ (where $F:[a,b] \rightarrow \mathbb{R}$ ), and similarly for the other three Dini numbers. I just want to ensure that I fully understand what this means. Does $(3)$ translate to the following? \begin{align*}     \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x) &:=  \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\infty) \cap (-\epsilon, \epsilon) \setminus \{0\}  \right\}\right) \\[4pt] &= \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\epsilon) \right\}\right) \quad ? \end{align*} (I take it that $h > 0$ means that $E = (0,\infty)$ in this case?)","I recently learned of the following extension to the definition of a limit: Let , let , let be a limit point of , and let . Then means the following: For each there exists some such that (I assume this definition extends similarly for general metric spaces.) I am now trying to understand the analogous extension for limit sup's and lim inf's. To quote Wikipedia : There is a notion of lim sup and lim inf for functions defined on a metric space whose relationship to limits of real-valued functions mirrors that of the relation between the lim sup, lim inf, and the limit of a real sequence. Take a metric space , a subspace contained in , and a function . Define, for any limit point a of , and where denotes the metric ball of radius about . My first question is: In the context of the setting described above, do and mean the same thing? (And similarly for ?). My question comes from my reading of Dini numbers in Stein and Shakarchi's Real Analysis . They define where (where ), and similarly for the other three Dini numbers. I just want to ensure that I fully understand what this means. Does translate to the following? (I take it that means that in this case?)","S \subset \mathbb{R} f: S \rightarrow \mathbb{R} a S L \in \mathbb{R} \lim_{x \to a \\ x \in S } f(x) = L \epsilon > 0 \delta > 0 x \in S \text{ and } |x-a| < \delta \implies |f(x) - L| < \epsilon. X E X f : E \rightarrow R a E \begin{align} \limsup_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\sup \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3cm} (1) \end{align}  \liminf_{x \to a} f(x) := \lim_{\epsilon \to 0} \left(\inf \big\{f(x):x \in E \cap B (a;\epsilon) \setminus \{a\} \big\}\right) \hspace{3.3cm} (2)  B(a;\epsilon) \epsilon a \displaystyle \lim\sup_{x \to a \\ x \in E} f(x) = L \displaystyle \lim\sup_{x \to a} f(x) = L \lim\inf D^{+}(F)(x) := \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x),  \hspace{3cm} (3)  \Delta_h(F)(x) := \frac{F(x + h) - F(x)}{h} F:[a,b] \rightarrow \mathbb{R} (3) \begin{align*}
    \lim \sup_{h \to 0, \\ h > 0} \Delta_h(F)(x) &:=  \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\infty) \cap (-\epsilon, \epsilon) \setminus \{0\}  \right\}\right) \\[4pt]
&= \lim_{\epsilon \to 0^{+}} \left(\sup \left\{\Delta_h(F)(x): x \in (0,\epsilon) \right\}\right) \quad ?
\end{align*} h > 0 E = (0,\infty)","['real-analysis', 'limits', 'analysis', 'epsilon-delta', 'limsup-and-liminf']"
44,Definition of multivariable function,Definition of multivariable function,,"A multivariable function has been defined as given below: $f(x,y) =\begin{cases} \dfrac{x^3\cos\frac{1}{y} + y^3\cos\frac{1}{x} }{x^2+y^2} &x,y \neq  0,\\ \\0&\text{otherwise}. \end{cases}$ I have to investigate its continuity and differentiability at point $(0,0)$ . But I am not sure about the definition of the given function. Does the above definition mean that $f(x,0)=0,x\neq 0$ and $f(0,y)=0,y\neq 0$ ? This is important because when I consider this definition for $f(x,0)$ and $f(0,y)$ then both partial derivatives exist and are equal to $0$ . Otherwise if we define $f(x,y)=0$ only for $(x,y)=(0,0)$ then partial derivatives do not exist and I can readily conclude the non-differentiability of $f(x,y)$ at $(0,0)$ as existence of partial derivatives is necessary for the function to be differentiable. Please help me as to how to interpret this definition. Also is the function differentiable if I interpret this definition in above mentioned manner i.e. $f(x,0)=0,x\neq 0$ and $f(0,y)=0,y\neq 0$ . Here is my work on differentiability part: $z=ax+by+k_1x+k_2y$ where $a$ and $b$ are the partial derivatives w.r.t. $x$ and $y$ respectively. For differentiability, $k_1$ and $k_2$ should tend to $0$ as $x$ and $y$ tend to $0$ . $z= \dfrac{(x)^3\cos\frac{1}{y} + (y)^3\cos\frac{1}{x} }{(x)^2+(y)^2} $ . Thus $k_1=\dfrac{(x)^2\cos\frac{1}{y}} {(x)^2+(y)^2} $ and $k_2=\dfrac{(y)^2\cos\frac{1}{x}} {(x)^2+(y)^2} $ . Now do $k_1$ and $k_2$ tend to $0$ as $x$ and $y$ go to $0$ ? If I take $x=y$ then $k_1$ comes out to be limit of $\frac {1}{2}cos\frac{1}{y}$ as $y$ goes to $0$ which does not exist. Am I doing it correctly? Please suggest.","A multivariable function has been defined as given below: I have to investigate its continuity and differentiability at point . But I am not sure about the definition of the given function. Does the above definition mean that and ? This is important because when I consider this definition for and then both partial derivatives exist and are equal to . Otherwise if we define only for then partial derivatives do not exist and I can readily conclude the non-differentiability of at as existence of partial derivatives is necessary for the function to be differentiable. Please help me as to how to interpret this definition. Also is the function differentiable if I interpret this definition in above mentioned manner i.e. and . Here is my work on differentiability part: where and are the partial derivatives w.r.t. and respectively. For differentiability, and should tend to as and tend to . . Thus and . Now do and tend to as and go to ? If I take then comes out to be limit of as goes to which does not exist. Am I doing it correctly? Please suggest.","f(x,y) =\begin{cases} \dfrac{x^3\cos\frac{1}{y} + y^3\cos\frac{1}{x} }{x^2+y^2} &x,y \neq  0,\\
\\0&\text{otherwise}. \end{cases} (0,0) f(x,0)=0,x\neq 0 f(0,y)=0,y\neq 0 f(x,0) f(0,y) 0 f(x,y)=0 (x,y)=(0,0) f(x,y) (0,0) f(x,0)=0,x\neq 0 f(0,y)=0,y\neq 0 z=ax+by+k_1x+k_2y a b x y k_1 k_2 0 x y 0 z=
\dfrac{(x)^3\cos\frac{1}{y} + (y)^3\cos\frac{1}{x} }{(x)^2+(y)^2}  k_1=\dfrac{(x)^2\cos\frac{1}{y}} {(x)^2+(y)^2}  k_2=\dfrac{(y)^2\cos\frac{1}{x}} {(x)^2+(y)^2}  k_1 k_2 0 x y 0 x=y k_1 \frac {1}{2}cos\frac{1}{y} y 0","['limits', 'multivariable-calculus', 'derivatives', 'continuity']"
45,How can I find the limit of the following?,How can I find the limit of the following?,,What is the limit of this $$\lim_{x\to+\infty}\left(1+\frac{4}{2x+3}\right)^x$$ I know that $$\lim_{x\to+\infty}\left(1+\frac{4}{2x}\right)^x$$ will give me $$e^2$$ but the I dont know what to do with the 3. I have tried bringing them to a common denominator so I got $$\lim_{x\to+\infty}\left(\frac{2x+7}{2x+3}\right)^x=\lim_{x\to+\infty}\left(e^{x\ln{(\frac{2x+7}{2x+3})}}\right)$$ And then Im stuck again,What is the limit of this I know that will give me but the I dont know what to do with the 3. I have tried bringing them to a common denominator so I got And then Im stuck again,\lim_{x\to+\infty}\left(1+\frac{4}{2x+3}\right)^x \lim_{x\to+\infty}\left(1+\frac{4}{2x}\right)^x e^2 \lim_{x\to+\infty}\left(\frac{2x+7}{2x+3}\right)^x=\lim_{x\to+\infty}\left(e^{x\ln{(\frac{2x+7}{2x+3})}}\right),['limits']
46,Question about my proof of: $ \lim_{h \to 0}f(ch)=\lim_{ch \to 0}f(ch)$ for $c\neq 0$,Question about my proof of:  for, \lim_{h \to 0}f(ch)=\lim_{ch \to 0}f(ch) c\neq 0,"This post will be broken up into two sections: the first section will contain the proof, and the second section will contain the question. The proof will be written formally, as the question is more easily understood referencing the formal description. (Note, the context of this post is in Spivak's Calculus , which treats all functions, unless otherwise stated, as having a domain of $\mathbb R$ ). Prove: $\displaystyle \lim_{h \to 0}f(ch)=\displaystyle \lim_{ch \to 0}f(ch)$ for $c\neq 0$ , which is equivalent to: If $c\neq 0$ and $\displaystyle \lim_{h \to 0}f(ch)=L$ , then $\displaystyle \lim_{ch \to 0}f(ch)=L$ $\quad$ and $\quad$ If $c\neq 0$ and $\displaystyle \lim_{ch \to 0}f(ch)=L$ , then $\displaystyle \lim_{h \to 0}f(ch)=L$ We will only prove the first implication (the converse is completed similarly): By assumption: $\displaystyle \lim_{h \to 0}f(ch)=L \iff \forall \varepsilon \gt 0 \ \exists \delta \gt 0 \  \forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right ]$ We want to show that for an arbitrary $\varepsilon$ , we can construct a $\delta$ such that $\color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right]$ For $\varepsilon$ , we know by assumption that there is a $\delta_{\varepsilon}$ such that: $\forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta_{\varepsilon} \rightarrow |f(ch)-L| \lt \varepsilon \right ]$ Now, consider a $\delta ^* = \min\left(\delta_{\varepsilon},\frac{\delta_{\varepsilon}}{|c|}\right)$ . If $0 \lt |h| \lt \delta^* \leq \delta_{\varepsilon}$ , by assumption we have: $|f(ch)-L| \lt \varepsilon$ . Further, if $0 \lt |h| \lt \delta^* \leq \frac{\delta_{\varepsilon}}{|c|}$ , then $0 \lt |ch| \lt |c|\delta^*$ . Therefore, let our desired $\delta$ be defined as $\delta = |c|\delta^*$ . As long as $0\lt|ch| \lt \delta$ , all of our criteria is met. In the above proof, I made use of the following statement: $\color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right]$ Through my brief experience in maths, the universally quantified object $ch$ is atypical. I suspect the proper way to denote this is by establishing a function of the form: $g(h)=ch$ and then defining a single symbol as representing its output. i.e. something like $s_h :=g(h)$ . More specifically, we should write $g$ formally as: $g: \mathbb R \to \mathbb R$ where $h \mapsto ch$ . We would then rewrite the statement as: $\color{red}{\forall s_h} \in \mathbb R \left [ 0 \lt |s_h| \lt \delta \rightarrow |f(s_h)-L| \lt \varepsilon \right]$ This seems to emulate the more familiar notation of a universal quantifier, where only one symbol follows the quantifier. I do not really know the deep theory behind first-order logic, but I suspect the reason this proof ""works out"" is because my new symbol $s_h$ has the capacity of sweeping through all objects within $\mathbb R$ . Said differently, the previously defined function $g$ can be shown to be surjective with respect to $\mathbb R$ . If the above is true, are there times where the change of variable function is not surjective, and this causes the equality between two limits to fail ?","This post will be broken up into two sections: the first section will contain the proof, and the second section will contain the question. The proof will be written formally, as the question is more easily understood referencing the formal description. (Note, the context of this post is in Spivak's Calculus , which treats all functions, unless otherwise stated, as having a domain of ). Prove: for , which is equivalent to: If and , then and If and , then We will only prove the first implication (the converse is completed similarly): By assumption: We want to show that for an arbitrary , we can construct a such that For , we know by assumption that there is a such that: Now, consider a . If , by assumption we have: . Further, if , then . Therefore, let our desired be defined as . As long as , all of our criteria is met. In the above proof, I made use of the following statement: Through my brief experience in maths, the universally quantified object is atypical. I suspect the proper way to denote this is by establishing a function of the form: and then defining a single symbol as representing its output. i.e. something like . More specifically, we should write formally as: where . We would then rewrite the statement as: This seems to emulate the more familiar notation of a universal quantifier, where only one symbol follows the quantifier. I do not really know the deep theory behind first-order logic, but I suspect the reason this proof ""works out"" is because my new symbol has the capacity of sweeping through all objects within . Said differently, the previously defined function can be shown to be surjective with respect to . If the above is true, are there times where the change of variable function is not surjective, and this causes the equality between two limits to fail ?","\mathbb R \displaystyle \lim_{h \to 0}f(ch)=\displaystyle \lim_{ch \to 0}f(ch) c\neq 0 c\neq 0 \displaystyle \lim_{h \to 0}f(ch)=L \displaystyle \lim_{ch \to 0}f(ch)=L \quad \quad c\neq 0 \displaystyle \lim_{ch \to 0}f(ch)=L \displaystyle \lim_{h \to 0}f(ch)=L \displaystyle \lim_{h \to 0}f(ch)=L \iff \forall \varepsilon \gt 0 \ \exists \delta \gt 0 \  \forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right ] \varepsilon \delta \color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right] \varepsilon \delta_{\varepsilon} \forall h \in \mathbb R \left [ 0 \lt |h| \lt \delta_{\varepsilon} \rightarrow |f(ch)-L| \lt \varepsilon \right ] \delta ^* = \min\left(\delta_{\varepsilon},\frac{\delta_{\varepsilon}}{|c|}\right) 0 \lt |h| \lt \delta^* \leq \delta_{\varepsilon} |f(ch)-L| \lt \varepsilon 0 \lt |h| \lt \delta^* \leq \frac{\delta_{\varepsilon}}{|c|} 0 \lt |ch| \lt |c|\delta^* \delta \delta = |c|\delta^* 0\lt|ch| \lt \delta \color{red}{\forall ch} \in \mathbb R \left [ 0 \lt |ch| \lt \delta \rightarrow |f(ch)-L| \lt \varepsilon \right] ch g(h)=ch s_h :=g(h) g g: \mathbb R \to \mathbb R h \mapsto ch \color{red}{\forall s_h} \in \mathbb R \left [ 0 \lt |s_h| \lt \delta \rightarrow |f(s_h)-L| \lt \varepsilon \right] s_h \mathbb R g \mathbb R","['real-analysis', 'calculus', 'limits', 'logic', 'first-order-logic']"
47,What went wrong in my evaluation of this limit? $\lim_{x\to1}\frac{1-\sqrt x}{(\cos^{-1} x)^2}$,What went wrong in my evaluation of this limit?,\lim_{x\to1}\frac{1-\sqrt x}{(\cos^{-1} x)^2},My process: \begin{align} \lim_{x\to 1}\frac{1-\sqrt x}{(\cos^{-1} x)^2} &=\lim_{x\to 1}\frac{1- x^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\ &=\lim_{x\to1}\frac{(\sin(\cos^{-1}x))^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\ &=\lim_{x\to1}\frac{1}{(1+\sqrt x)}=\frac{1}{2}. \end{align} The answer is infact $\frac{1}{4}$ what went wrong ?,My process: The answer is infact what went wrong ?,"\begin{align}
\lim_{x\to 1}\frac{1-\sqrt x}{(\cos^{-1} x)^2} &=\lim_{x\to 1}\frac{1- x^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\
&=\lim_{x\to1}\frac{(\sin(\cos^{-1}x))^2}{(\cos^{-1} x)^2(1+\sqrt x)}\\
&=\lim_{x\to1}\frac{1}{(1+\sqrt x)}=\frac{1}{2}.
\end{align} \frac{1}{4}","['calculus', 'limits']"
48,Proving $0/0$ case of L'Hospital's rule,Proving  case of L'Hospital's rule,0/0,"In the proofs of the $0/0$ case of L'Hospital's rule that I have seen so far, if we are interested in determining $$\lim_{x \to a}\dfrac{f(x)}{g(x)}$$ and given that $f, g$ are differentiable at $x = a$ and their derivatives are continuous at $x = a$ , one always assumes that $f(a) = g(a) = 0$ , but isn't the requirement; $$\lim_{x \to a} f(x) = \lim_{x \to a}g(x) = 0$$ is enough, meaning that the functions doesn't necessarily have to be defined at this point. The proof I'm referring to goes as follow: Given that $f(a) = g(a) = 0$ , we know that $$ \lim_{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L $$ that means we can find a $\delta$ -neighborhood around $a$ such that if $x \in V_{s}(a)$ implies this limit is $\epsilon$ close to $L$ . Now, if we apply the GMVT to some points $x, a \in V_{\delta}(a)$ , WLOG $x>a$ , then we have $$ \frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f(x)}{g(x)}=\frac{f^{\prime}(c)}{g^{\prime}(c)} $$ for some $c \in(a, x) \subseteq V_{\delta}(a)$ . Since this $c$ is also in $V_{\delta}(a)$ , we conclude that $$ \left|\frac{f(x)}{g(x)}-L\right|=\left|\frac{f^{\prime}(c)}{g^{\prime}(c)}-L\right|<\epsilon $$ which proves that $\lim _{x \rightarrow a} \frac{f(x)}{g(x)}=\lim _{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L$ What would one have to change such that the proof is also applicable if the functions $f$ and $g$ are not defined at $x = a$ and we only have $\lim\limits_{x \to a}f(x) = \lim\limits_{x \to a}g(x) = 0$ ? What would one have to change if $L = \infty$ ?","In the proofs of the case of L'Hospital's rule that I have seen so far, if we are interested in determining and given that are differentiable at and their derivatives are continuous at , one always assumes that , but isn't the requirement; is enough, meaning that the functions doesn't necessarily have to be defined at this point. The proof I'm referring to goes as follow: Given that , we know that that means we can find a -neighborhood around such that if implies this limit is close to . Now, if we apply the GMVT to some points , WLOG , then we have for some . Since this is also in , we conclude that which proves that What would one have to change such that the proof is also applicable if the functions and are not defined at and we only have ? What would one have to change if ?","0/0 \lim_{x \to a}\dfrac{f(x)}{g(x)} f, g x = a x = a f(a) = g(a) = 0 \lim_{x \to a} f(x) = \lim_{x \to a}g(x) = 0 f(a) = g(a) = 0 
\lim_{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L
 \delta a x \in V_{s}(a) \epsilon L x, a \in V_{\delta}(a) x>a 
\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f(x)}{g(x)}=\frac{f^{\prime}(c)}{g^{\prime}(c)}
 c \in(a, x) \subseteq V_{\delta}(a) c V_{\delta}(a) 
\left|\frac{f(x)}{g(x)}-L\right|=\left|\frac{f^{\prime}(c)}{g^{\prime}(c)}-L\right|<\epsilon
 \lim _{x \rightarrow a} \frac{f(x)}{g(x)}=\lim _{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L f g x = a \lim\limits_{x \to a}f(x) = \lim\limits_{x \to a}g(x) = 0 L = \infty","['real-analysis', 'calculus', 'limits']"
49,What is wrong with this proof that L'Hospital works both ways?,What is wrong with this proof that L'Hospital works both ways?,,"Our professor proved L'Hpital's rule through Cauchy's mean value theorem. He proved it in multiple parts, I have trouble following the first, most basic example. Theorem: Let $f, g$ be continuous functions on $(a,b)$ where $a < b$ . Let: $g(x) \neq 0$ and $g'(x) \neq 0$ , $\forall x \in (a,b)$ $ \lim_{x \downarrow a} f(x) = \lim_{x \downarrow a} g(x) = 0  $ If limit $B = \lim_{x \downarrow a} \frac{f'(x)}{g'(x)}$ exists, then $A = \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ also exists and $A = B$ . Proof: We define $f(a)=0$ and $g(a)=0$ . Then, by condition $(2)$ , we see that $f, g$ are continuous on $[a,b)$ . Let $x \in (a,b)$ . Then on $[a,x]$ all necessary conditions for Cauchy's mean value theorem are fulfilled. Therefore, there exists a $c_x \in (a,x)$ , such that: $$\frac{f'(c_x)}{g'(c_x)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac{f(x)}{g(x)}$$ When $x \downarrow a$ , also $c_x \downarrow a$ . Therefore: if $B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)}$ exists, $A= \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ exists as well and $A=B$ . Question: Now, my main question is, why does this last conclusion work? Why couldn't we say the same thing in the other direction: if $A= \lim_{x \downarrow a} \frac{f(x)}{g(x)}$ exists, $B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)}$ exists as well and $A=B$ . What would be wrong with that conclusion? Everyone says that L'Hopital only works one way, but I don't see why that is true from the proof.","Our professor proved L'Hpital's rule through Cauchy's mean value theorem. He proved it in multiple parts, I have trouble following the first, most basic example. Theorem: Let be continuous functions on where . Let: and , If limit exists, then also exists and . Proof: We define and . Then, by condition , we see that are continuous on . Let . Then on all necessary conditions for Cauchy's mean value theorem are fulfilled. Therefore, there exists a , such that: When , also . Therefore: if exists, exists as well and . Question: Now, my main question is, why does this last conclusion work? Why couldn't we say the same thing in the other direction: if exists, exists as well and . What would be wrong with that conclusion? Everyone says that L'Hopital only works one way, but I don't see why that is true from the proof.","f, g (a,b) a < b g(x) \neq 0 g'(x) \neq 0 \forall x \in (a,b)  \lim_{x \downarrow a} f(x) = \lim_{x \downarrow a} g(x) = 0   B = \lim_{x \downarrow a} \frac{f'(x)}{g'(x)} A = \lim_{x \downarrow a} \frac{f(x)}{g(x)} A = B f(a)=0 g(a)=0 (2) f, g [a,b) x \in (a,b) [a,x] c_x \in (a,x) \frac{f'(c_x)}{g'(c_x)} = \frac{f(x)-f(a)}{g(x)-g(a)} = \frac{f(x)}{g(x)} x \downarrow a c_x \downarrow a B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)} A= \lim_{x \downarrow a} \frac{f(x)}{g(x)} A=B A= \lim_{x \downarrow a} \frac{f(x)}{g(x)} B= \lim_{x \downarrow a} \frac{f'(c_x)}{g'(c_x)} A=B","['real-analysis', 'calculus', 'limits', 'analysis', 'derivatives']"
50,Show that $\lim_{x\to 0} \frac{g(x)}{\sqrt{x}}=2+\sqrt{2}$,Show that,\lim_{x\to 0} \frac{g(x)}{\sqrt{x}}=2+\sqrt{2},"Let $g: (0,\infty)\rightarrow\mathbb{R}$ satisfies $\lim_{x\to 0}g(x)=0$ and $\lim_{x\to 0} \frac{g(x)-g(\frac{x}{2})}{\sqrt{x}}=1$ . Show that $$\lim_{x\to 0}\frac{g(x)}{\sqrt{x}}=2+\sqrt{2}$$ Here is what I think about. If I let $l= \lim_{x\to 0}\frac{g(x)}{\sqrt{x}}$ then I can find that $l=2+\sqrt{2}$ . Because it is likely to calculate. It is not proving. For showing this, I use definition of limit Given $\epsilon>0,\exists \delta>0$ Such that $0<|x-0|<\delta$ And $$\left|\frac{g(x)-g(\frac{x}{2})}{\sqrt{x}}-1\right|<\epsilon$$ Then I dont know how can I do more.Thank in advance!","Let satisfies and . Show that Here is what I think about. If I let then I can find that . Because it is likely to calculate. It is not proving. For showing this, I use definition of limit Given Such that And Then I dont know how can I do more.Thank in advance!","g: (0,\infty)\rightarrow\mathbb{R} \lim_{x\to 0}g(x)=0 \lim_{x\to 0} \frac{g(x)-g(\frac{x}{2})}{\sqrt{x}}=1 \lim_{x\to 0}\frac{g(x)}{\sqrt{x}}=2+\sqrt{2} l= \lim_{x\to 0}\frac{g(x)}{\sqrt{x}} l=2+\sqrt{2} \epsilon>0,\exists \delta>0 0<|x-0|<\delta \left|\frac{g(x)-g(\frac{x}{2})}{\sqrt{x}}-1\right|<\epsilon","['calculus', 'limits', 'epsilon-delta']"
51,Calculus Infinite Limit Proof,Calculus Infinite Limit Proof,,"I'm trying to prove: $\lim_{x\rightarrow5^+} \frac{3}{(x-5)(x-2)}=\infty$ Let $M>0$ . Choose $\delta =$ Assume $0<x-5<\delta$ . Show $\frac{3}{(x-5)(x-2)}>M$ $\frac{3}{(x-5)(x-2)}>M$ $\frac{3}{x-5}>M(x-2)$ $x-5<\frac{3}{M(x-2)}$ I'm getting stuck here, I can't find $\delta$ and I've been working for hours on this. Any help would be greatly appreciated! I'm trying to generalize this for a bigger theorem.","I'm trying to prove: Let . Choose Assume . Show I'm getting stuck here, I can't find and I've been working for hours on this. Any help would be greatly appreciated! I'm trying to generalize this for a bigger theorem.",\lim_{x\rightarrow5^+} \frac{3}{(x-5)(x-2)}=\infty M>0 \delta = 0<x-5<\delta \frac{3}{(x-5)(x-2)}>M \frac{3}{(x-5)(x-2)}>M \frac{3}{x-5}>M(x-2) x-5<\frac{3}{M(x-2)} \delta,"['real-analysis', 'calculus', 'limits']"
52,Right versus left derivative by increasing one-sided derivatives,Right versus left derivative by increasing one-sided derivatives,,"Condider a map $f:D\to\mathbb{R}$ , $D$ open interval. Let be $y,z\in D$ , $y<z$ . Suppose that $f$ has everywhere in $D$ both left and right derivative (this implies that $f$ is continuous), both increasing . I want to show that $$ f'(y^+)\le f'(z^-). $$ I have found a way, but I'm not completely convinced that it is formally correct: $$ f'(y^+) =\lim_{\varepsilon\to 0^+}\frac{f(y+\varepsilon)-f(y)}{\varepsilon} \overset{\forall\delta}{\le}    f'((z-\delta)^+) = \lim_{\varepsilon\to 0^+}\frac{f(z-\delta+\varepsilon)-f(z-\delta)}{\varepsilon} \overset{\varepsilon=\delta}{=} \lim_{\varepsilon\to 0^+}\frac{f(z)-f(z-\varepsilon)}{\varepsilon} =f'(z^-) $$ where $\delta>0$ and $y<z-\delta$ . Is there a way to formalise my proof?","Condider a map , open interval. Let be , . Suppose that has everywhere in both left and right derivative (this implies that is continuous), both increasing . I want to show that I have found a way, but I'm not completely convinced that it is formally correct: where and . Is there a way to formalise my proof?","f:D\to\mathbb{R} D y,z\in D y<z f D f 
f'(y^+)\le f'(z^-).
 
f'(y^+)
=\lim_{\varepsilon\to 0^+}\frac{f(y+\varepsilon)-f(y)}{\varepsilon}
\overset{\forall\delta}{\le} 
  f'((z-\delta)^+)
= \lim_{\varepsilon\to 0^+}\frac{f(z-\delta+\varepsilon)-f(z-\delta)}{\varepsilon}
\overset{\varepsilon=\delta}{=} \lim_{\varepsilon\to 0^+}\frac{f(z)-f(z-\varepsilon)}{\varepsilon}
=f'(z^-)
 \delta>0 y<z-\delta","['real-analysis', 'limits', 'derivatives', 'solution-verification']"
53,Finding the limits of integration for the volume of a region inside a cube,Finding the limits of integration for the volume of a region inside a cube,,"Premise We are restricted to the region $x, y, z \in [f, 1]$ where $0 \leq f < 1$ . The surface $y^2=4xz$ divides this cube into two regions. We are interested in finding the volume of the region where $y^2 - 4xz \geq 0$ . My Attempt Let $t=\frac{y^2}{4}$ for brevity. When $f=0$ , it is relatively easy to figure out the limits. We can separate the region into two where $x \leq t$ and $x > t$ . In the first case, we have $y^2 - 4xz \geq 0$ for all $z \in [0, 1]$ because $xz \leq x\cdot 1 \leq t$ . Similarly, in the second case, we have $y^2 - 4xz \geq 0$ for all $z \in [0, \frac{t}{x}]$ because $xz \leq x\cdot \frac{t}{x} = t$ . Applying these limits, we have $$ \begin{align} V &= \int_{0}^{1}{dy\int_{0}^{t}{dx\int_{0}^{1}{dz}}} + \int_{0}^{1}{dy\int_{0}^{t}{dx \int_{0}^{\frac{t}{x}}{dz}}}\\   &= \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot (1)}} + \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot \frac{t}{x}}}\\   &= \frac{1}{12} + \frac{1 + 3\log{2}}{18} \\   &\approx 0.254 \end{align} $$ However, when $f > 0$ , I cannot seem to understand the proper limits for $z$ . Is it $f$ to $\frac{t}{x}$ or is the lower limit $f^2$ ? What happens when $f$ is less than or equal to $\frac{y^2}{4}$ ? I tried a few times, but the results ended up being negative, which is clearly wrong as volumes are nonnegative. Specific Questions Primary objective I would like help in figuring out the limits in the case where $f>0$ . Secondary objectives Is this approach sensible? Perhaps there is a better way to solve this problem? Are there any glaring oversights? What books/courses/reference should I follow to be able to solve these types of problems?","Premise We are restricted to the region where . The surface divides this cube into two regions. We are interested in finding the volume of the region where . My Attempt Let for brevity. When , it is relatively easy to figure out the limits. We can separate the region into two where and . In the first case, we have for all because . Similarly, in the second case, we have for all because . Applying these limits, we have However, when , I cannot seem to understand the proper limits for . Is it to or is the lower limit ? What happens when is less than or equal to ? I tried a few times, but the results ended up being negative, which is clearly wrong as volumes are nonnegative. Specific Questions Primary objective I would like help in figuring out the limits in the case where . Secondary objectives Is this approach sensible? Perhaps there is a better way to solve this problem? Are there any glaring oversights? What books/courses/reference should I follow to be able to solve these types of problems?","x, y, z \in [f, 1] 0 \leq f < 1 y^2=4xz y^2 - 4xz \geq 0 t=\frac{y^2}{4} f=0 x \leq t x > t y^2 - 4xz \geq 0 z \in [0, 1] xz \leq x\cdot 1 \leq t y^2 - 4xz \geq 0 z \in [0, \frac{t}{x}] xz \leq x\cdot \frac{t}{x} = t 
\begin{align}
V &= \int_{0}^{1}{dy\int_{0}^{t}{dx\int_{0}^{1}{dz}}} + \int_{0}^{1}{dy\int_{0}^{t}{dx \int_{0}^{\frac{t}{x}}{dz}}}\\
  &= \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot (1)}} + \int_{0}^{1}{dy\int_{0}^{t}{dx\cdot \frac{t}{x}}}\\
  &= \frac{1}{12} + \frac{1 + 3\log{2}}{18} \\
  &\approx 0.254
\end{align}
 f > 0 z f \frac{t}{x} f^2 f \frac{y^2}{4} f>0","['integration', 'limits', 'inequality', 'definite-integrals', 'volume']"
54,alternative way of proving $\frac{1}{n}+\frac{1}{n+1}+\frac{1}{n+2}+...+\frac{1}{2n}$ converges to $\ln 2$ without using integrals,alternative way of proving  converges to  without using integrals,\frac{1}{n}+\frac{1}{n+1}+\frac{1}{n+2}+...+\frac{1}{2n} \ln 2,"I found a lot of answered to this problem using Riemann - sums, I myself solved it rewriting the logarithm sum, but there is another way where you should use the following hints. Show that $$\exp \left( \lim_{n \to \infty} \sum_{k=1}^n \frac{1}{k+n} \right) =  2$$ using the following identities (1). There is a nonnegative sequence $x_k$ , converging to zero, and $$\exp \left(\frac{1}{k} \right) = \left(1+\frac{1}{k} \right)\times  \exp\left(\frac{x_k}{k} \right)$$ (2). $$\lim_{x \to 0} \frac{\exp(x)-1}{x}=1$$ To prove $(1)$ , I wanted to rewrite $$\exp\left(\frac{1}{k} \right)\times \frac{1}{1+\frac{1}{k}}$$ and expand $$\frac{1}{1+\frac{1}{k}} = \frac{1}{1-\frac{-1}{k}}$$ as geometric sequence to find $x_k$ , but I dont get anything useful. For $(2)$ I thought may one should stretch $$\sum_{k=1}^n \left(\exp\left(\frac{1}{k+n}\right)-1 \right)$$ which has the same limit (I proved that) as $\sum_{k=1}^n \frac{1}{k+n}$ and rewrite it as $$\sum_{k=1}^n \frac{\exp\left(\frac{1}{k+n}\right)-1}{\frac{1}{k+n}}\frac{1}{k+n}$$ Im thankful for any tips or answers!","I found a lot of answered to this problem using Riemann - sums, I myself solved it rewriting the logarithm sum, but there is another way where you should use the following hints. Show that using the following identities (1). There is a nonnegative sequence , converging to zero, and (2). To prove , I wanted to rewrite and expand as geometric sequence to find , but I dont get anything useful. For I thought may one should stretch which has the same limit (I proved that) as and rewrite it as Im thankful for any tips or answers!","\exp \left( \lim_{n \to \infty} \sum_{k=1}^n \frac{1}{k+n} \right) =
 2 x_k \exp \left(\frac{1}{k} \right) = \left(1+\frac{1}{k} \right)\times
 \exp\left(\frac{x_k}{k} \right) \lim_{x \to 0} \frac{\exp(x)-1}{x}=1 (1) \exp\left(\frac{1}{k} \right)\times \frac{1}{1+\frac{1}{k}} \frac{1}{1+\frac{1}{k}} = \frac{1}{1-\frac{-1}{k}} x_k (2) \sum_{k=1}^n \left(\exp\left(\frac{1}{k+n}\right)-1 \right) \sum_{k=1}^n \frac{1}{k+n} \sum_{k=1}^n \frac{\exp\left(\frac{1}{k+n}\right)-1}{\frac{1}{k+n}}\frac{1}{k+n}","['real-analysis', 'limits', 'power-series', 'exponential-function']"
55,Does being well approximated by a polynomial characterize higher differentiability?,Does being well approximated by a polynomial characterize higher differentiability?,,"Suppose $f:\mathbb R\to \mathbb R$ admits a polynomial $\sum_{k=0}^n\lambda_k(x-a)^k$ of degree $\leq n$ satisfying $f(x)-\sum_{k=0}^n\lambda_k(x-a)^k\in \mathrm o(|x-a|^n)$ as $x\to a$ . Then dividing by powers of $(x-a)$ shows $$\begin{aligned}\lambda_0 & =\lim_{x\to a}fx, \\ \lambda_1 & =\lim_{x\to a}\tfrac{fx-\lambda_0}{x-a}, \\ \lambda _2 & =\lim_{x\to a}\tfrac{fx-fa-\lambda_1(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-\lambda_1}{x-a}, \\  & \vdots \\ \lambda_n & = \lim_{x\to a}\tfrac{fx-\sum_{k=0}^{n-1}\lambda_k(x-a)^k}{(x-a)^n}. \end{aligned}$$ Thus the asymptotic condition determines the polynomial, so it is unique when it exists. If $f$ is furthermore continuous at $x=a$ then $\lambda_0=fa$ whence $\lambda_1=f^\prime(a)$ . If $f$ is $n$ -times differentiable at $x=a$ then Peano's form of Taylor's theorem tells us $R^n_af\in \mathrm o(|x-a|^n)$ as $x\to a$ . Here $R^n_af(x)=f(x)-j^n_af(x)$ and $$j^n_af(x)=\sum_{k=0}^n\tfrac 1{k!}f^{(k)}(a)(x-a)^k$$ is the $n^\text{th}$ order Taylor series of $f$ about $x=a$ . By uniqueness we therefore have $\lambda_k=\tfrac 1{k!}f^{(k)}(a)$ . For instance taking $k=0,1,2$ gives $$\tfrac 12f^{(2)}(a)=\lambda _2  =\lim_{x\to a}\tfrac{fx-fa-f^\prime(a)(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-f^\prime(a)}{x-a}.$$ I am interested in the converse: are continuous maps which are well approximated by a polynomial of degree $\leq n$ necessarily $n$ -times differentiable? Question. Suppose $f$ is continuous and admits a polynomial $\sum_{k=0}^n\lambda_k(x-a)^k$ of degree $\leq n$ satisfying $f(x)-\sum_{k=0}^n\lambda_k(x-a)^k\in \mathrm o(|x-a|^n)$ as $x\to a$ . Does it follow that $f$ is $n$ -times differentiable at $x=a$ , with $\lambda_k=\tfrac 1{k!}f^{(k)}(a)$ ? Already for $k=2$ this amounts to the assertion $$\tfrac 12f^{(2)}(a)=\lambda _2  =\lim_{x\to a}\tfrac{fx-fa-f^\prime(a)(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-f^\prime(a)}{x-a}.$$ This is not obvious at all to me.","Suppose admits a polynomial of degree satisfying as . Then dividing by powers of shows Thus the asymptotic condition determines the polynomial, so it is unique when it exists. If is furthermore continuous at then whence . If is -times differentiable at then Peano's form of Taylor's theorem tells us as . Here and is the order Taylor series of about . By uniqueness we therefore have . For instance taking gives I am interested in the converse: are continuous maps which are well approximated by a polynomial of degree necessarily -times differentiable? Question. Suppose is continuous and admits a polynomial of degree satisfying as . Does it follow that is -times differentiable at , with ? Already for this amounts to the assertion This is not obvious at all to me.","f:\mathbb R\to \mathbb R \sum_{k=0}^n\lambda_k(x-a)^k \leq n f(x)-\sum_{k=0}^n\lambda_k(x-a)^k\in \mathrm o(|x-a|^n) x\to a (x-a) \begin{aligned}\lambda_0 & =\lim_{x\to a}fx, \\ \lambda_1 & =\lim_{x\to a}\tfrac{fx-\lambda_0}{x-a}, \\ \lambda _2 & =\lim_{x\to a}\tfrac{fx-fa-\lambda_1(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-\lambda_1}{x-a}, \\  & \vdots \\ \lambda_n & = \lim_{x\to a}\tfrac{fx-\sum_{k=0}^{n-1}\lambda_k(x-a)^k}{(x-a)^n}. \end{aligned} f x=a \lambda_0=fa \lambda_1=f^\prime(a) f n x=a R^n_af\in \mathrm o(|x-a|^n) x\to a R^n_af(x)=f(x)-j^n_af(x) j^n_af(x)=\sum_{k=0}^n\tfrac 1{k!}f^{(k)}(a)(x-a)^k n^\text{th} f x=a \lambda_k=\tfrac 1{k!}f^{(k)}(a) k=0,1,2 \tfrac 12f^{(2)}(a)=\lambda _2  =\lim_{x\to a}\tfrac{fx-fa-f^\prime(a)(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-f^\prime(a)}{x-a}. \leq n n f \sum_{k=0}^n\lambda_k(x-a)^k \leq n f(x)-\sum_{k=0}^n\lambda_k(x-a)^k\in \mathrm o(|x-a|^n) x\to a f n x=a \lambda_k=\tfrac 1{k!}f^{(k)}(a) k=2 \tfrac 12f^{(2)}(a)=\lambda _2  =\lim_{x\to a}\tfrac{fx-fa-f^\prime(a)(x-a)}{(x-a)^2}=\lim_{x\to a}\tfrac{\tfrac{fx-fa}{x-a}-f^\prime(a)}{x-a}.","['calculus', 'limits', 'derivatives', 'taylor-expansion']"
56,"Verify Functional Limit of $\lim_{h \to 0} \frac{\int_{1-h}^{1+h} f(x) \,dx}{h}$",Verify Functional Limit of,"\lim_{h \to 0} \frac{\int_{1-h}^{1+h} f(x) \,dx}{h}","I want to find $$\lim_{h \to 0} \dfrac{\displaystyle\int_{1-h}^{1+h} f(x) \,dx}{h}$$ . Where $f$ is a continuous function defined on $\mathbb{R}$ What I have so far: Since $$\lim_{h \to 0} \int_{1-h}^{1+h} f(x) \,dx = 0 \text{ and } \lim_{h \to 0} h = 0$$ we can use L'Hopital's rule. $\dfrac{d}{dh}h =1$ and $\frac{d}{dh} \displaystyle\int_{1-h}^{1+h} f(x) \,dx = f(1+h)+f(1-h)$ . (Is this part right?). Then from that we apply L'Hopital's rule: $$\lim_{h \to 0} \frac{\displaystyle \int_{1-h}^{1+h} f(x) \,dx}{h} = \lim_{h \to 0} f(1+h)+f(1-h) = f(1) +f(1)=2f(1)$$ Does this look right? Thanks!",I want to find . Where is a continuous function defined on What I have so far: Since we can use L'Hopital's rule. and . (Is this part right?). Then from that we apply L'Hopital's rule: Does this look right? Thanks!,"\lim_{h \to 0} \dfrac{\displaystyle\int_{1-h}^{1+h} f(x) \,dx}{h} f \mathbb{R} \lim_{h \to 0} \int_{1-h}^{1+h} f(x) \,dx = 0 \text{ and } \lim_{h \to 0} h = 0 \dfrac{d}{dh}h =1 \frac{d}{dh} \displaystyle\int_{1-h}^{1+h} f(x) \,dx = f(1+h)+f(1-h) \lim_{h \to 0} \frac{\displaystyle \int_{1-h}^{1+h} f(x) \,dx}{h} = \lim_{h \to 0} f(1+h)+f(1-h) = f(1) +f(1)=2f(1)","['real-analysis', 'integration', 'limits']"
57,"How to calculate $\lim_{(x, y) \to (0,0)} \frac{xy^2}{x^2 - y^2}$ [duplicate]",How to calculate  [duplicate],"\lim_{(x, y) \to (0,0)} \frac{xy^2}{x^2 - y^2}","This question already has answers here : What paths to choose to prove that $\lim_{(x,y) \to (0,0)}\frac{xy^2}{x^2 - y^2}$ does not exist? (5 answers) Closed 3 years ago . How do I show that $\lim_{(x, y) \to (0,0)}\frac{xy^2}{x^2 - y^2} = 0$ ? I tried using polar coordinates, and arrived at $\lim_{r \to 0^+}r \tan{(2\theta)} \sin \theta$ . But then, I couldn't find a nice way to prove that this is zero, because the function multplying $r$ isn't bounded. Does anyone have a nice, simple solution?","This question already has answers here : What paths to choose to prove that $\lim_{(x,y) \to (0,0)}\frac{xy^2}{x^2 - y^2}$ does not exist? (5 answers) Closed 3 years ago . How do I show that ? I tried using polar coordinates, and arrived at . But then, I couldn't find a nice way to prove that this is zero, because the function multplying isn't bounded. Does anyone have a nice, simple solution?","\lim_{(x, y) \to (0,0)}\frac{xy^2}{x^2 - y^2} = 0 \lim_{r \to 0^+}r \tan{(2\theta)} \sin \theta r","['limits', 'multivariable-calculus']"
58,"When evaluating the limit of $f(x, y)$ as $(x, y)$ approaches $(x_0, y_0)$, should we consider only those $(x, y)$ in the domain of $f$?","When evaluating the limit of  as  approaches , should we consider only those  in the domain of ?","f(x, y) (x, y) (x_0, y_0) (x, y) f","When evaluating the limit of $f(x, y)$ as $(x, y)$ approaches $(x_0, y_0)$ , we should or should not consider only those $(x, y)$ in the domain of $f(x, y)$ ? I am confused by different practices of Calculus textbooks. Have anyone searched and found some authoritative opinion ? Thomas Calculus 14e 14.2 Example 2 (Page 802-803) $\lim_{(x, y) \to (0, 0)} \frac{x^2 - x y}{\sqrt{x} - \sqrt{y}}$ considers only those $(x, y)$ in the domain. The authors' answer ( $\mathbf{0}$ ) is the same as the answer by WolframAlpha . See textbook page 802 and textbook page 803 . Larson Calculus 10e 13.2 Exercise 27 (Page 887) $\lim_{(x, y) \to (0, 0)} \frac{x - y}{\sqrt{x} - \sqrt{y}}$ considers NOT only those $(x, y)$ in the domain. The authors' answer ( DNE ) is NOT the same as the answer by WolframAlpha ( $\mathbf{0}$ ). See textbook page 887 and solution manual page 1268 .","When evaluating the limit of as approaches , we should or should not consider only those in the domain of ? I am confused by different practices of Calculus textbooks. Have anyone searched and found some authoritative opinion ? Thomas Calculus 14e 14.2 Example 2 (Page 802-803) considers only those in the domain. The authors' answer ( ) is the same as the answer by WolframAlpha . See textbook page 802 and textbook page 803 . Larson Calculus 10e 13.2 Exercise 27 (Page 887) considers NOT only those in the domain. The authors' answer ( DNE ) is NOT the same as the answer by WolframAlpha ( ). See textbook page 887 and solution manual page 1268 .","f(x, y) (x, y) (x_0, y_0) (x, y) f(x, y) \lim_{(x, y) \to (0, 0)} \frac{x^2 - x y}{\sqrt{x} - \sqrt{y}} (x, y) \mathbf{0} \lim_{(x, y) \to (0, 0)} \frac{x - y}{\sqrt{x} - \sqrt{y}} (x, y) \mathbf{0}","['calculus', 'limits']"
59,Is $\lim_{s \to \infty} \int f(x) g(s)dx$ equal to $\int f(x) (\lim_{s \to \infty}g(s) ) dx$?,Is  equal to ?,\lim_{s \to \infty} \int f(x) g(s)dx \int f(x) (\lim_{s \to \infty}g(s) ) dx,"$$\lim_{s \to \infty} \int f(x) g(s)dx = \int f(x) (\lim_{s \to \infty}g(s) ) dx$$ Is this equality true? Can you move the limit operator inside of the integral, since we're not integrating with respect to the variable in the limit?","Is this equality true? Can you move the limit operator inside of the integral, since we're not integrating with respect to the variable in the limit?",\lim_{s \to \infty} \int f(x) g(s)dx = \int f(x) (\lim_{s \to \infty}g(s) ) dx,"['calculus', 'limits']"
60,A property of the function $\frac{\sin x}{x}$,A property of the function,\frac{\sin x}{x},"How can one prove, that $0$ is the only value of $\frac{\sin x}{x}$ taken infinitely often? What I tried: To see how the graph looks like https://www.wolframalpha.com/input/?i=%28sin+x%29%2Fx The function is continuous and has infinitely many positive and negative values, so by Darboux it has infinitely many zeroes. Also, the line $y=0$ is an asymptote both in $\pm\infty$ , but this thing only, doesn't imply the result. What else should I use?","How can one prove, that is the only value of taken infinitely often? What I tried: To see how the graph looks like https://www.wolframalpha.com/input/?i=%28sin+x%29%2Fx The function is continuous and has infinitely many positive and negative values, so by Darboux it has infinitely many zeroes. Also, the line is an asymptote both in , but this thing only, doesn't imply the result. What else should I use?",0 \frac{\sin x}{x} y=0 \pm\infty,"['real-analysis', 'limits', 'functions']"
61,How can we tell the relationship between two lim-sup sets?,How can we tell the relationship between two lim-sup sets?,,"Let $f_n(x)$ be sequence of functions, and $\epsilon>0$ . Denote two sets as follows $$E(\epsilon) = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| >\epsilon \}$$ $$F = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| > 1/n \}.$$ Based on the definitions above, can we conclude $E(\epsilon) \subset F$ for any $\epsilon>0$ ?","Let be sequence of functions, and . Denote two sets as follows Based on the definitions above, can we conclude for any ?",f_n(x) \epsilon>0 E(\epsilon) = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| >\epsilon \} F = \limsup_{n\rightarrow\infty}\{x:|f_n(x)| > 1/n \}. E(\epsilon) \subset F \epsilon>0,['limits']
62,limit related to the Lambert function,limit related to the Lambert function,,I am trying to evaluate the following limit $$ L=\lim_{x \rightarrow 0^+}\frac{2 \operatorname{W}\left( -{{ e}^{-x-1}}\right)  \left( {{\operatorname{W}\left( -{{e}^{-x-1}}\right) }^{2}}+2 \operatorname{W}\left( -{{ e}^{-x-1}}\right) -2 x+1\right) }{{{\left( \operatorname{W}\left( -{{ e}^{-x-1}}\right) +1\right) }^{3}}}$$ where $W(z)$ is the principal branch of Lambert's function. The numerical experiments show that it is $\sqrt{2}$ but the l'Hopital's rule does not produce anything useful. Here is the numerical experiment computed with Maxima: $L(x) - \sqrt{2}$,I am trying to evaluate the following limit where is the principal branch of Lambert's function. The numerical experiments show that it is but the l'Hopital's rule does not produce anything useful. Here is the numerical experiment computed with Maxima:,"
L=\lim_{x \rightarrow 0^+}\frac{2 \operatorname{W}\left( -{{ e}^{-x-1}}\right)  \left( {{\operatorname{W}\left( -{{e}^{-x-1}}\right) }^{2}}+2 \operatorname{W}\left( -{{ e}^{-x-1}}\right) -2 x+1\right) }{{{\left( \operatorname{W}\left( -{{ e}^{-x-1}}\right) +1\right) }^{3}}} W(z) \sqrt{2} L(x) - \sqrt{2}","['limits', 'lambert-w']"
63,Michael Spivak Calculus Limits problem proof verification,Michael Spivak Calculus Limits problem proof verification,,"I would like to know whether my answer to the following exercise problem in Calculus by Spivak is correct. The statement of the problem is: Prove that if $f(x)=x$ for rational $x$ , and $f(x) = -x$ for irrational $x$ , then $\lim_{x\to a}f(x)$ does not exist when $a\neq0$ My answer: Proof by contradiction: Let $\lim_{x\to a}f(x)=L$ Case 1: When $a > 0 $ , $\exists\  \delta > 0 $ such that $0<|x-a|<\delta \implies |f(x)-L|<a$ Let $x_1 \in \mathbb{Q}$ and $x_2 \in \mathbb{R-Q}$ such that $x_1,x_2 \in (a, a+\delta)$ , then $|f(x_1)-L|<a$ and $|f(x_2)-L|<a$ and hence by the triangle inequality we have $|f(x_1) - f(x_2)|=|f(x_1)-L-(f(x_2)-L)| \leq |f(x_1) - L| + |f(x_2) - L| < a + a = 2a$ Therefore we have $|f(x_1)-f(x_2)| < 2a$ But, $f(x_1)=x_1 > a$ and $f(x_2)=-x_2 < -a$ Therefore $f(x_1) - f(x_2) = x_1 + x_2 > 2a > 0 \implies |f(x_1) - f(x_2)| > 2a$ , a contradiction. The case when $a<0$ is very similar, we just set $\epsilon = -a$ in the definition of the limit and let $x_1,x_2 \in (a-\delta, a)$","I would like to know whether my answer to the following exercise problem in Calculus by Spivak is correct. The statement of the problem is: Prove that if for rational , and for irrational , then does not exist when My answer: Proof by contradiction: Let Case 1: When , such that Let and such that , then and and hence by the triangle inequality we have Therefore we have But, and Therefore , a contradiction. The case when is very similar, we just set in the definition of the limit and let","f(x)=x x f(x) = -x x \lim_{x\to a}f(x) a\neq0 \lim_{x\to a}f(x)=L a > 0  \exists\  \delta > 0  0<|x-a|<\delta \implies |f(x)-L|<a x_1 \in \mathbb{Q} x_2 \in \mathbb{R-Q} x_1,x_2 \in (a, a+\delta) |f(x_1)-L|<a |f(x_2)-L|<a |f(x_1) - f(x_2)|=|f(x_1)-L-(f(x_2)-L)| \leq |f(x_1) - L| + |f(x_2) - L| < a + a = 2a |f(x_1)-f(x_2)| < 2a f(x_1)=x_1 > a f(x_2)=-x_2 < -a f(x_1) - f(x_2) = x_1 + x_2 > 2a > 0 \implies |f(x_1) - f(x_2)| > 2a a<0 \epsilon = -a x_1,x_2 \in (a-\delta, a)","['calculus', 'limits', 'solution-verification', 'limits-without-lhopital', 'epsilon-delta']"
64,How find the limit $\lim_{n\to\infty}\frac{\sum_{k=1}^n|\cos(k^2)|}{n}$.,How find the limit .,\lim_{n\to\infty}\frac{\sum_{k=1}^n|\cos(k^2)|}{n},"Compute $$\displaystyle\lim_{n\to\infty}\dfrac{\displaystyle\sum_{k=1}^n|\cos(k^2)|}{n}$$ . I guess is $\dfrac{2}{\pi}$ ,because the summation is essentially equal to computing the average value of $|\cos k|$ on the interval from $[0, \pi]$ , which is $\boxed{\dfrac{2}{\pi}}$ ,it's right?Thanks","Compute . I guess is ,because the summation is essentially equal to computing the average value of on the interval from , which is ,it's right?Thanks","\displaystyle\lim_{n\to\infty}\dfrac{\displaystyle\sum_{k=1}^n|\cos(k^2)|}{n} \dfrac{2}{\pi} |\cos k| [0, \pi] \boxed{\dfrac{2}{\pi}}",['limits']
65,limit trough path,limit trough path,,"I am just trying to find a formal proof for this statement: Let $f:\mathbb R^n \backslash \ \{0\} \rightarrow \Bbb R$ be a function, and assume that there exits $L\in \Bbb R$ that for every path $$\gamma: (a,b)\rightarrow \Bbb R^n\backslash\{0\} $$ that satisfies $$\lim_{t\rightarrow b}\gamma(t) = 0 $$ the limit $$\lim_{t\rightarrow b} f(\gamma(t)) = L $$ show that : $\lim_{x\rightarrow 0} f(x) =L$","I am just trying to find a formal proof for this statement: Let be a function, and assume that there exits that for every path that satisfies the limit show that :","f:\mathbb R^n \backslash \ \{0\} \rightarrow \Bbb R L\in \Bbb R \gamma: (a,b)\rightarrow \Bbb R^n\backslash\{0\}  \lim_{t\rightarrow b}\gamma(t) = 0  \lim_{t\rightarrow b} f(\gamma(t)) = L  \lim_{x\rightarrow 0} f(x) =L","['calculus', 'limits', 'multivariable-calculus']"
66,Proving $\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7$ using $ \varepsilon - \delta$,Proving  using,\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7  \varepsilon - \delta,Prove that $$\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7$$ using $\varepsilon - \delta$ . We find $\delta$ such that $0<|x-4| <\delta$ $$\left|\frac{\sqrt {2x-1}}{\sqrt {x-3}}-\sqrt 7\right|= \left|\frac{\sqrt {2x-1}-\sqrt{7x-21}}{\sqrt {x-3}}\right|$$ I know that we should get to $|x-4|$ but i dont know how,Prove that using . We find such that I know that we should get to but i dont know how,\lim_{x\to 4} \left(\frac{\sqrt {2x-1}}{\sqrt {x-3}}\right) = \sqrt 7 \varepsilon - \delta \delta 0<|x-4| <\delta \left|\frac{\sqrt {2x-1}}{\sqrt {x-3}}-\sqrt 7\right|= \left|\frac{\sqrt {2x-1}-\sqrt{7x-21}}{\sqrt {x-3}}\right| |x-4|,"['calculus', 'limits', 'epsilon-delta']"
67,Proving whether or not this limit exists,Proving whether or not this limit exists,,"We're given the function $f(x,y) = \frac{3x^2y}{x^2+y^2}$ . We're interested in determining whether $\lim_{(x,y)\to(0,0)}{f(x,y)}$ exists. Clearly the function is not continuous at (0,0). Approaching along the x and y axes gives a limit of 0 in both cases. I'm struggling to find a relationship between x and y which gives me a different limit. Any help would be much appreciated. For the record, I tried $x=y$ , $x^2 = y$ , $x=y^2$ and $x=1/y$ , and none of them gave a limit which wasn't 0.","We're given the function . We're interested in determining whether exists. Clearly the function is not continuous at (0,0). Approaching along the x and y axes gives a limit of 0 in both cases. I'm struggling to find a relationship between x and y which gives me a different limit. Any help would be much appreciated. For the record, I tried , , and , and none of them gave a limit which wasn't 0.","f(x,y) = \frac{3x^2y}{x^2+y^2} \lim_{(x,y)\to(0,0)}{f(x,y)} x=y x^2 = y x=y^2 x=1/y","['limits', 'multivariable-calculus']"
68,Limit question unknown function,Limit question unknown function,,"If $\lim_{x \rightarrow 0} f(x)+f(2x)=0$ , prove or disprove with example, that $\lim_{x \rightarrow 0} f(x)=0$ for any function $f(x)$ . f(x) can be a piecewise functions as well. I tried too disprove it considering several functions but I wasn't able to do so. So I guess that there statement is true but how do we prove it?","If , prove or disprove with example, that for any function . f(x) can be a piecewise functions as well. I tried too disprove it considering several functions but I wasn't able to do so. So I guess that there statement is true but how do we prove it?",\lim_{x \rightarrow 0} f(x)+f(2x)=0 \lim_{x \rightarrow 0} f(x)=0 f(x),"['calculus', 'limits', 'functions']"
69,Limit And Continuity of $f(x) = \left\{\begin{matrix} x^{2} ; x \in \mathbb{Q} & \\ 0 ; x \not\in \mathbb{Q} & \end{matrix}\right.$,Limit And Continuity of,f(x) = \left\{\begin{matrix} x^{2} ; x \in \mathbb{Q} & \\ 0 ; x \not\in \mathbb{Q} & \end{matrix}\right.,I am having difficulty in the following function $f(x) = \left\{\begin{matrix} x^{2} ; x \in \mathbb{Q}  & \\  0 ; x  \not\in  \mathbb{Q} &  \end{matrix}\right.$ I have following fundamental doubts. 1) Does this function have limit at all rational number including zero? 2) Is this function continuous at every rational number? Thanks,I am having difficulty in the following function I have following fundamental doubts. 1) Does this function have limit at all rational number including zero? 2) Is this function continuous at every rational number? Thanks,"f(x) = \left\{\begin{matrix}
x^{2} ; x \in \mathbb{Q}  & \\ 
0 ; x  \not\in  \mathbb{Q} & 
\end{matrix}\right.","['calculus', 'limits', 'continuity', 'epsilon-delta']"
70,Evaluate the limit $\lim\limits_{n\to0}\frac{(x)+(2x)+\cdots (nx)}{n^2}$,Evaluate the limit,\lim\limits_{n\to0}\frac{(x)+(2x)+\cdots (nx)}{n^2},"Find the limit of $\lim_{n\rightarrow ~0}\frac{(x)+(2x)+\cdots (nx)}{n^2}$ , where, $(x)=x-[x]$ and $[x] $ is the greatest integer function(the fractional part function). I feel, as $n \rightarrow 0$ this limit  goes to infinity, but the options given are $x~,~x/2,~x/3,~x/4$ . How this is happening, I double checked the question paper, in question $n$ . is tending to 0 only not to $\infty$ . I found a similar question here","Find the limit of , where, and is the greatest integer function(the fractional part function). I feel, as this limit  goes to infinity, but the options given are . How this is happening, I double checked the question paper, in question . is tending to 0 only not to . I found a similar question here","\lim_{n\rightarrow ~0}\frac{(x)+(2x)+\cdots (nx)}{n^2} (x)=x-[x] [x]  n \rightarrow 0 x~,~x/2,~x/3,~x/4 n \infty","['real-analysis', 'limits', 'functions', 'sequence-of-function', 'fractional-part']"
71,Evaluating $ \lim_{x\to \infty} x \left({{\left(\frac{x}{x+1}\right)}^{x}-\frac{1}{e}}\right)$ [duplicate],Evaluating  [duplicate], \lim_{x\to \infty} x \left({{\left(\frac{x}{x+1}\right)}^{x}-\frac{1}{e}}\right),This question already has answers here : Computing $\lim_{x \to \infty} x \biggl[ \frac{1}{e} - \left( \frac{x}{x+1} \right)^x \biggr]$ (3 answers) Closed 3 years ago . Evaluate the following: $$ \lim_{x\to \infty}  x \left({{\left(\frac{x}{x+1}\right)}^{x}-\frac{1}{e}}\right)$$ I tried to first solve the interior portion as $1^\infty$ indeterminate form but ended up getting a different indeterminate form of $\infty\cdot 0$ .,This question already has answers here : Computing $\lim_{x \to \infty} x \biggl[ \frac{1}{e} - \left( \frac{x}{x+1} \right)^x \biggr]$ (3 answers) Closed 3 years ago . Evaluate the following: I tried to first solve the interior portion as indeterminate form but ended up getting a different indeterminate form of .,"
\lim_{x\to \infty}
 x \left({{\left(\frac{x}{x+1}\right)}^{x}-\frac{1}{e}}\right) 1^\infty \infty\cdot 0","['limits', 'limits-without-lhopital']"
72,Can we rearrange primes to make this ratio converge to any real?,Can we rearrange primes to make this ratio converge to any real?,,"Let $p_n$ be the $n$ -th prime and let $q_1, q_2, \ldots, q_n$ be any rearrangement of the first $n$ primes. Using the rearrangement inequality and the solution to this problem , we can prove that $$ 1 \le \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots + np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} \le 2 $$ I was wondering if we can make the above ratio arbitrarily close to any real number in the interval $(1,2)$ by choosing a suitable rearrangement of the first $n$ primes i.e. Question : Let $\alpha, 1 \le \alpha \le 2$ be any real. Does there always  exist a rearrangement such that, $$ \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots +  np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} = \alpha$$","Let be the -th prime and let be any rearrangement of the first primes. Using the rearrangement inequality and the solution to this problem , we can prove that I was wondering if we can make the above ratio arbitrarily close to any real number in the interval by choosing a suitable rearrangement of the first primes i.e. Question : Let be any real. Does there always  exist a rearrangement such that,","p_n n q_1, q_2, \ldots, q_n n 
1 \le \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots + np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} \le 2
 (1,2) n \alpha, 1 \le \alpha \le 2  \lim_{n \to \infty}\frac{p_1 + 2p_2 + 3p_3 + \cdots +
 np_n}{q_1 + 2q_2 + 3q_3 + \cdots + nq_n} = \alpha","['real-analysis', 'number-theory', 'limits', 'convergence-divergence', 'prime-numbers']"
73,Evaluate this $\lim\limits_{n \to \infty}\int\limits_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}dx$,Evaluate this,\lim\limits_{n \to \infty}\int\limits_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}dx,"We seeking to evaluate this integral $$\lim_{n \to \infty}\int_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx$$ using $\cos a -\cos b=-2\sin[(a+b)/2]\sin[(a-b)/2]$ $$\lim_{n \to \infty}-2\int_{1/n}^{n}\sin\left(\frac{3x}{4}\right)\sin\left(\frac{x}{4}\right)\frac{\ln x}{x}\mathrm dx$$ I stuck, not sure what to do next. $v=\int \frac{\ln x}{x}=\frac{1}{2}\ln^2 x$ $u^{'}=-\sin x+\frac{1}{2}\sin(x/2)$ $$\int \left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx=\left[\cos x-\cos(x/2)\right]\frac{\ln^2 x}{2}-\frac{1}{2}\int \left[-\sin x+\frac{1}{2}\sin(x/2)\right]\ln^2 x$$ this integral it getting more complicated due to $\ln^2 x$","We seeking to evaluate this integral using I stuck, not sure what to do next. this integral it getting more complicated due to",\lim_{n \to \infty}\int_{1/n}^{n}\left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx \cos a -\cos b=-2\sin[(a+b)/2]\sin[(a-b)/2] \lim_{n \to \infty}-2\int_{1/n}^{n}\sin\left(\frac{3x}{4}\right)\sin\left(\frac{x}{4}\right)\frac{\ln x}{x}\mathrm dx v=\int \frac{\ln x}{x}=\frac{1}{2}\ln^2 x u^{'}=-\sin x+\frac{1}{2}\sin(x/2) \int \left(\cos x-\cos(x/2)\right)\frac{\ln x}{x}\mathrm dx=\left[\cos x-\cos(x/2)\right]\frac{\ln^2 x}{2}-\frac{1}{2}\int \left[-\sin x+\frac{1}{2}\sin(x/2)\right]\ln^2 x \ln^2 x,['integration']
74,"Prove $a^x> x^a$ for all $x>c$, for some real $c$. $(a>1)$","Prove  for all , for some real .",a^x> x^a x>c c (a>1),"To prove $a^x> x^a$ for all $x>c$ , for some real $c$ . ( $a>1$ ). Proof Attempt: We first try to prove that $\lim_{x \to \infty}\frac{a^x}{x^a}=\infty$ . Both of the functions in the numerator and the denominator goes to $\infty$ as $x\to\infty$ . So, L'Hospital's rule is applicable. We get $\lim_{x \to \infty}\frac{a^x \log (a)}{ax^{a-1}}$ . We repeat the operation $\lceil a \rceil = p$ times. We get $\lim_{x \to \infty}\frac{a^x(\ln (a))^{p} x^{p-a}}{a(a-1)(a-2)...(a-(p-1))} =\infty$ A function $f(x)$ is said to diverge to positive infinity, if $\forall G>0$ , $\exists k>0$ such that $f(x)>G$ , $\forall x>k$ . We fix $G=1$ here. By the very definition, we can find a $k>0$ , such that $\frac{a^x}{x^a}>1$ , $\forall x>k$ , i.e. $a^x> x^a, \forall x>k$ . Is this method valid?","To prove for all , for some real . ( ). Proof Attempt: We first try to prove that . Both of the functions in the numerator and the denominator goes to as . So, L'Hospital's rule is applicable. We get . We repeat the operation times. We get A function is said to diverge to positive infinity, if , such that , . We fix here. By the very definition, we can find a , such that , , i.e. . Is this method valid?","a^x> x^a x>c c a>1 \lim_{x \to \infty}\frac{a^x}{x^a}=\infty \infty x\to\infty \lim_{x \to \infty}\frac{a^x \log (a)}{ax^{a-1}} \lceil a \rceil = p \lim_{x \to \infty}\frac{a^x(\ln (a))^{p} x^{p-a}}{a(a-1)(a-2)...(a-(p-1))} =\infty f(x) \forall G>0 \exists k>0 f(x)>G \forall x>k G=1 k>0 \frac{a^x}{x^a}>1 \forall x>k a^x> x^a, \forall x>k","['real-analysis', 'calculus', 'limits', 'proof-verification', 'derivatives']"
75,Understanding why a limit proof using another limit works,Understanding why a limit proof using another limit works,,"Sorry for the title, hopefully I can explain it better. I think the title is about as good as I could get in terms of description. I have a problem: Let $x_n \ge 0$ for all $ N \in \mathbb{N}$ If $(x_n) \to x$ , show that $(\sqrt{x_n}) \to \sqrt(x)$ Assume that we have already proved the limit going to zero. My proof was as follows: Our goal is to find an $N$ that satisfies the inequality: $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ with epsilon being arbitrary. So: $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ $|\sqrt{x_n|}| \lt \epsilon + \sqrt{x}$ $|\sqrt{x_n|}|^2 \lt (\epsilon + \sqrt{x})^2$ $|x_n| \lt (\epsilon + \sqrt{x})^2$ $|x_n| \lt \epsilon^2 + 2 \epsilon \sqrt{x} + x$ $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ Since we already know $|x_n - x|$ can be made arbitrarily small we are ready to proceed. Then allow $\epsilon > 0$ to be arbitrary and choose an $N \in \mathbb{N}$ satisfying: $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ For $n \ge N$ we find after some algebra (to save typing the above backwards) $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ Which shows that given the limit we can choose an $N$ for any given $\epsilon$ and find that all $n \ge N$ will be inside the $\epsilon$ -neighborhood of $\sqrt{x}$ . Where I am confused is how I am using the given limit $(x_n) \to x$ . I am sort of following a template here from the author. Adding this extra limit has confused me. How does reducing the inequality $|\sqrt{x_n|} - \sqrt{x}| \lt \epsilon$ to $|x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x}$ and then knowing ""we can make it arbitrarily small"" help us prove the given limit? What is the intuition?","Sorry for the title, hopefully I can explain it better. I think the title is about as good as I could get in terms of description. I have a problem: Let for all If , show that Assume that we have already proved the limit going to zero. My proof was as follows: Our goal is to find an that satisfies the inequality: with epsilon being arbitrary. So: Since we already know can be made arbitrarily small we are ready to proceed. Then allow to be arbitrary and choose an satisfying: For we find after some algebra (to save typing the above backwards) Which shows that given the limit we can choose an for any given and find that all will be inside the -neighborhood of . Where I am confused is how I am using the given limit . I am sort of following a template here from the author. Adding this extra limit has confused me. How does reducing the inequality to and then knowing ""we can make it arbitrarily small"" help us prove the given limit? What is the intuition?",x_n \ge 0  N \in \mathbb{N} (x_n) \to x (\sqrt{x_n}) \to \sqrt(x) N |\sqrt{x_n|} - \sqrt{x}| \lt \epsilon |\sqrt{x_n|} - \sqrt{x}| \lt \epsilon |\sqrt{x_n|}| \lt \epsilon + \sqrt{x} |\sqrt{x_n|}|^2 \lt (\epsilon + \sqrt{x})^2 |x_n| \lt (\epsilon + \sqrt{x})^2 |x_n| \lt \epsilon^2 + 2 \epsilon \sqrt{x} + x |x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x} |x_n - x| \epsilon > 0 N \in \mathbb{N} |x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x} n \ge N |\sqrt{x_n|} - \sqrt{x}| \lt \epsilon N \epsilon n \ge N \epsilon \sqrt{x} (x_n) \to x |\sqrt{x_n|} - \sqrt{x}| \lt \epsilon |x_n - x| \lt \epsilon^2 + 2 \epsilon \sqrt{x},"['real-analysis', 'limits', 'epsilon-delta']"
76,"Please prove $n! > (n/3)^n$ is true, without using mathematical induction","Please prove  is true, without using mathematical induction",n! > (n/3)^n,"Please prove $n! > \left(\frac{n}{3}\right)^n$ is true, without using mathematical induction. I've proved it using mathematical induction, but our teacher asked us to derive it using limits $n$ pre-calculus. I tried, but I'm stuck.","Please prove is true, without using mathematical induction. I've proved it using mathematical induction, but our teacher asked us to derive it using limits pre-calculus. I tried, but I'm stuck.",n! > \left(\frac{n}{3}\right)^n n,['limits']
77,Search what if the nature of the limit,Search what if the nature of the limit,,"I must find the limit $\ell$ defined bellow. I was wondering if $\ell<1$ . This limit came from the following series: $$ \sum_{n=0}^{\infty} 2^{n}\frac{\sqrt{n+1}}{3^{n}}. $$ Let's call $$c_{n}=2^{n}\frac{\sqrt{n+1}}{3^{n}}$$ What I did: was to consider the quotient $$\frac{c_{n+1}}{c_{n}}$$ which gives $$\ell = \underset{n\to \infty}{\lim}\ \frac{c_{n+1}}{c_{n}} =\underset{n\to \infty}{\lim}\ \frac{\displaystyle{2^{n+1}\frac{\sqrt{n+2}}{3^{n+1}}}}{\displaystyle{2^{n}\frac{\sqrt{n+1}}{3^{n}}}}.$$ after :I multiply and  find $\ell<1$ . But, I don't know if it is right. Can you please check if you find the same? I may have done something wrong..","I must find the limit defined bellow. I was wondering if . This limit came from the following series: Let's call What I did: was to consider the quotient which gives after :I multiply and  find . But, I don't know if it is right. Can you please check if you find the same? I may have done something wrong..",\ell \ell<1  \sum_{n=0}^{\infty} 2^{n}\frac{\sqrt{n+1}}{3^{n}}.  c_{n}=2^{n}\frac{\sqrt{n+1}}{3^{n}} \frac{c_{n+1}}{c_{n}} \ell = \underset{n\to \infty}{\lim}\ \frac{c_{n+1}}{c_{n}} =\underset{n\to \infty}{\lim}\ \frac{\displaystyle{2^{n+1}\frac{\sqrt{n+2}}{3^{n+1}}}}{\displaystyle{2^{n}\frac{\sqrt{n+1}}{3^{n}}}}. \ell<1,"['linear-algebra', 'limits']"
78,Asymptotic of sum $\sum_{j=1}^n j^{f(n)}$,Asymptotic of sum,\sum_{j=1}^n j^{f(n)},"What is known about the asymptotic of $\sum_{j=1}^n j^{f(n)}$ where the exponent is some function that grows with $n$ ?  For instance, if $f(n) = k$ is constant, then we know it's $\frac{1}{k+1}n^{k+1} + O(n^k)$ .  If $f(n) = n$ , it seems that the sum is dominated by the last few terms and behaves like a geometric series with $r=1/e$ , so that the sum grows as $n^n\frac{e}{e-1}$ (plus some error term).  What happens if e.g. $f(n) = n^\alpha$ for $0 < \alpha < 1$ or $f(n) = \log n$ ?","What is known about the asymptotic of where the exponent is some function that grows with ?  For instance, if is constant, then we know it's .  If , it seems that the sum is dominated by the last few terms and behaves like a geometric series with , so that the sum grows as (plus some error term).  What happens if e.g. for or ?",\sum_{j=1}^n j^{f(n)} n f(n) = k \frac{1}{k+1}n^{k+1} + O(n^k) f(n) = n r=1/e n^n\frac{e}{e-1} f(n) = n^\alpha 0 < \alpha < 1 f(n) = \log n,"['real-analysis', 'limits', 'analysis', 'summation', 'asymptotics']"
79,Proof verification of $\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0$ using $\epsilon$-$\delta$,Proof verification of  using -,\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0 \epsilon \delta,"I am trying to prove that $$\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0$$ and this is my approach. Let $\epsilon>0,\delta>0\,$ so that $$\left|\frac{x^2+x-2}{\sqrt{x+2}}\right|<\epsilon\quad\text{whenever}\quad-2<x<-2+\delta$$ We have that $$\left|\frac{x^2+x-2}{\sqrt{x+2}}\right|=|x-1|\sqrt{x+2}$$ Let $\delta<1$ $$-2<x<-2+\delta<-1\implies-3<x-1<-2\implies2<|x-1|<3$$ so $$|x-1|\sqrt{x+2}\leq3\sqrt{x+2}<\epsilon$$ if we pick $\,\delta\leq\min\{1,\epsilon^2/9\}.$ Would this be correct?",I am trying to prove that and this is my approach. Let so that We have that Let so if we pick Would this be correct?,"\lim_{x\to-2^+}\frac{x^2+x-2}{\sqrt{x+2}}=0 \epsilon>0,\delta>0\, \left|\frac{x^2+x-2}{\sqrt{x+2}}\right|<\epsilon\quad\text{whenever}\quad-2<x<-2+\delta \left|\frac{x^2+x-2}{\sqrt{x+2}}\right|=|x-1|\sqrt{x+2} \delta<1 -2<x<-2+\delta<-1\implies-3<x-1<-2\implies2<|x-1|<3 |x-1|\sqrt{x+2}\leq3\sqrt{x+2}<\epsilon \,\delta\leq\min\{1,\epsilon^2/9\}.","['calculus', 'limits', 'proof-verification', 'epsilon-delta']"
80,Finding the limit by using the definition of derivative.,Finding the limit by using the definition of derivative.,,"Here is the problem. Let $f$ be the function that has the value of $f(1)=1$ and $f'(1)=2$. Find the value of   $$ L = \lim_{x \to 1} {\frac{\arctan{\sqrt{f(x)}-\arctan{f(x)}}}{ \left (\arcsin{\sqrt{f(x)}}-\arcsin{f(x)}\right)^2}} $$ I have tried using $$ L=\lim_{x\to 1} \frac{1}{x-1}\frac{\frac{\arctan{\sqrt{f(x)}}-\arctan{\sqrt{f(1)}}}{x-1}-\frac{\arctan{{f(x)}}-\arctan{{f(1)}}}{x-1}} {\left [\frac{\arcsin{\sqrt{f(x)}}-\arcsin{\sqrt{f(1)}}}{x-1}-\frac{\arcsin{{f(x)}}-\arcsin{{f(1)}}}{x-1}  \right ]^2} $$ and reduced that big chunks by using $f'(a)=\lim_{x \to a} \frac{f(x)-f(a)}{x-a}$ which I got $$ \begin{split} L&=\lim_{a\to 1} \frac{1}{a-1}\frac{\Big [ \arctan\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arctan{f(x)} \Big ]'_{x=a}} {\Big [ \arcsin\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arcsin{f(x)} \Big ]'_{x=a}}\\[2em] &=\lim_{a\to 1} \frac{1}{a-1} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{1+(f(a))^2}f'(a)} {\left [  \frac{1}{\sqrt{1-f(a)}}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{\sqrt{1-(f(a))^2}}f'(a) \right ]^2}\\[2em] &=\lim_{a\to 1} \frac{1}{f'(a)\frac{a-1}{1-f(a)}} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}} {\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}\\[2em] &=\lim_{a\to 1}{-\frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}} {\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}}\\[2em] &=\boxed{(\sqrt{2}+1)^2} \end{split} $$ but the answer keys tell me that the answer of this problem is $L=\left( \frac{\sqrt{2}+1}{2}\right)^2$. So, Can someone please explain to me what did I do wrong?","Here is the problem. Let $f$ be the function that has the value of $f(1)=1$ and $f'(1)=2$. Find the value of   $$ L = \lim_{x \to 1} {\frac{\arctan{\sqrt{f(x)}-\arctan{f(x)}}}{ \left (\arcsin{\sqrt{f(x)}}-\arcsin{f(x)}\right)^2}} $$ I have tried using $$ L=\lim_{x\to 1} \frac{1}{x-1}\frac{\frac{\arctan{\sqrt{f(x)}}-\arctan{\sqrt{f(1)}}}{x-1}-\frac{\arctan{{f(x)}}-\arctan{{f(1)}}}{x-1}} {\left [\frac{\arcsin{\sqrt{f(x)}}-\arcsin{\sqrt{f(1)}}}{x-1}-\frac{\arcsin{{f(x)}}-\arcsin{{f(1)}}}{x-1}  \right ]^2} $$ and reduced that big chunks by using $f'(a)=\lim_{x \to a} \frac{f(x)-f(a)}{x-a}$ which I got $$ \begin{split} L&=\lim_{a\to 1} \frac{1}{a-1}\frac{\Big [ \arctan\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arctan{f(x)} \Big ]'_{x=a}} {\Big [ \arcsin\sqrt{f(x)} \Big ]'_{x=a} - \Big [ \arcsin{f(x)} \Big ]'_{x=a}}\\[2em] &=\lim_{a\to 1} \frac{1}{a-1} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{1+(f(a))^2}f'(a)} {\left [  \frac{1}{\sqrt{1-f(a)}}\frac{1}{2\sqrt{f(a)}}f'(a)-\frac{1}{\sqrt{1-(f(a))^2}}f'(a) \right ]^2}\\[2em] &=\lim_{a\to 1} \frac{1}{f'(a)\frac{a-1}{1-f(a)}} \frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}} {\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}\\[2em] &=\lim_{a\to 1}{-\frac{\frac{1}{1+f(a)}\frac{1}{2\sqrt{f(a)}}-\frac{1}{1+(f(a))^2}} {\left [  \frac{1}{2\sqrt{f(a)}}-\frac{1}{\sqrt{1+f(a)}} \right ]^2}}\\[2em] &=\boxed{(\sqrt{2}+1)^2} \end{split} $$ but the answer keys tell me that the answer of this problem is $L=\left( \frac{\sqrt{2}+1}{2}\right)^2$. So, Can someone please explain to me what did I do wrong?",,"['calculus', 'limits', 'derivatives', 'limits-without-lhopital']"
81,Find $a$ that makes $f$ continuous and doesn't have extrema,Find  that makes  continuous and doesn't have extrema,a f,"Find $a$ that makes $f$ continuous and doesn't have extrema: $f(x)=\begin{cases}        ax^2 & x\leq 1 \\       a^2x-2 & x>1     \end{cases}$ Here's what I've been doing: $\lim \limits_{x \to 1^-} ax^2=a$ and $\lim \limits_{x \to 1^+}a^2x-2=a^2-2$ Then I found the roots for $a^2-a-2=0$, which are $-1$ and $2$. Ok, so here's my problem, if you plug in both values for $a$, $f(x)$ is continous for both values, so now I have to find which one makes the function not have extrema... (Clearly looking at the graphic, the answer is $-1$, I just don't know how to prove it by ""algebraic means"") What I tried (I don't know if it's correct), is to find the limit as $x \rightarrow \infty$ for both cases: When $a=-1$ $\lim \limits_{x \to \infty} -x^2=-\infty$ and $\lim \limits_{x \to \infty} x-2=\infty$ By this I just assumed that $f(x)$ doesn't have extrema. When $a=2$ $\lim \limits_{x \to \infty} 2x^2=\infty$ and $\lim \limits_{x \to \infty} 4x-2=\infty$ ??? Btw, I can't use derivatives to find the extrema... Thank you :-)","Find $a$ that makes $f$ continuous and doesn't have extrema: $f(x)=\begin{cases}        ax^2 & x\leq 1 \\       a^2x-2 & x>1     \end{cases}$ Here's what I've been doing: $\lim \limits_{x \to 1^-} ax^2=a$ and $\lim \limits_{x \to 1^+}a^2x-2=a^2-2$ Then I found the roots for $a^2-a-2=0$, which are $-1$ and $2$. Ok, so here's my problem, if you plug in both values for $a$, $f(x)$ is continous for both values, so now I have to find which one makes the function not have extrema... (Clearly looking at the graphic, the answer is $-1$, I just don't know how to prove it by ""algebraic means"") What I tried (I don't know if it's correct), is to find the limit as $x \rightarrow \infty$ for both cases: When $a=-1$ $\lim \limits_{x \to \infty} -x^2=-\infty$ and $\lim \limits_{x \to \infty} x-2=\infty$ By this I just assumed that $f(x)$ doesn't have extrema. When $a=2$ $\lim \limits_{x \to \infty} 2x^2=\infty$ and $\lim \limits_{x \to \infty} 4x-2=\infty$ ??? Btw, I can't use derivatives to find the extrema... Thank you :-)",,"['calculus', 'real-analysis', 'limits']"
82,$\lim_{n\to\infty}n^2\int_0^1x^nf(x)dx$ if $f(1)=0$,if,\lim_{n\to\infty}n^2\int_0^1x^nf(x)dx f(1)=0,Evaluate $\lim_{n\to\infty}n^2\int_0^1x^nf(x)dx$ if $f(1)=0$. I know that if $f$ is continuous then $\int_0^1x^nf(x)dx=0$ by applying substitution $u=x^n$ and using the same substitution $n\times\int_0^1 x^nf(x)dx \to f(1)$ as $n\to\infty$.But what happens if i still have an $n$left in the front from $n^2$ and $f(1)=0$. My particular case is: $f(x)=(x-1)\times e^{-\frac{1}{x+5}}$. Any hints?,Evaluate $\lim_{n\to\infty}n^2\int_0^1x^nf(x)dx$ if $f(1)=0$. I know that if $f$ is continuous then $\int_0^1x^nf(x)dx=0$ by applying substitution $u=x^n$ and using the same substitution $n\times\int_0^1 x^nf(x)dx \to f(1)$ as $n\to\infty$.But what happens if i still have an $n$left in the front from $n^2$ and $f(1)=0$. My particular case is: $f(x)=(x-1)\times e^{-\frac{1}{x+5}}$. Any hints?,,"['calculus', 'integration', 'limits']"
83,How to find the finite limit of this function?,How to find the finite limit of this function?,,"Let $f(x) = \dfrac{1-\cos \{x\}}{(x^4 + ax^3 +bx^2 +cx)^2}$. If $l= \lim_{x\to 1^+}f(x), m = \lim_{x\to 2^+}f(x) $ and $n= \lim_{x\to 3^+}f(x),$ where $l,m$ and $n$ non-zero finite then: $a+b+c=? $ $l+m+n=?$ $\lim_{x\to 0^+}f(x)=? $   where {} denotes the fractional part function. The trouble I am facing with this question is that for $n^+$ for $n \in \mathbb N$, the numerator is turning out to be $0$ as $\{x\}= 0$ and $\cos 0 =1$ and the deominator is finite. I even tried the taylor expansions of $\cos \{x\}$ but that didn't help.  I don't need the full solution, just want a hint to be able to proceed.","Let $f(x) = \dfrac{1-\cos \{x\}}{(x^4 + ax^3 +bx^2 +cx)^2}$. If $l= \lim_{x\to 1^+}f(x), m = \lim_{x\to 2^+}f(x) $ and $n= \lim_{x\to 3^+}f(x),$ where $l,m$ and $n$ non-zero finite then: $a+b+c=? $ $l+m+n=?$ $\lim_{x\to 0^+}f(x)=? $   where {} denotes the fractional part function. The trouble I am facing with this question is that for $n^+$ for $n \in \mathbb N$, the numerator is turning out to be $0$ as $\{x\}= 0$ and $\cos 0 =1$ and the deominator is finite. I even tried the taylor expansions of $\cos \{x\}$ but that didn't help.  I don't need the full solution, just want a hint to be able to proceed.",,"['calculus', 'limits']"
84,compute $\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)}$,compute,\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)},"$\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)}$ the form of limits reminds me $\iint\limits_{D} f(x,y)dxdy =  \lim\limits_{\lambda\to 0}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}f(x,y)dxdy$ $D=\{(x,y)|x\in [1,\infty],y\in [1,\infty]\}$ but that is all I can think about this problem","$\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)}$ the form of limits reminds me $\iint\limits_{D} f(x,y)dxdy =  \lim\limits_{\lambda\to 0}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}f(x,y)dxdy$ $D=\{(x,y)|x\in [1,\infty],y\in [1,\infty]\}$ but that is all I can think about this problem",,"['limits', 'multiple-integral']"
85,Evaluating $\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx$,Evaluating,\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx,"Greetings I want to evaluate $\displaystyle\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx$. Here is my try: We have that $x\in[0,\pi]$ so $$\cos(n\pi)\le \cos(nx) \le 1.$$ Here I am not sure, but if it's correct then it gives: $$\frac{1}{2}\le\frac{1}{1+\cos^2(nx)}\le \frac{1}{1+\cos^2(n\pi)},$$ giving $$\lim_{n\rightarrow\infty} \frac{1}{2}\int_0^{\pi} \sin x dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (n\pi)} dx.$$ Since $$\int_0^{\pi} \sin x dx =2$$ By squeeze theorem we may conclude that $\displaystyle \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx=1$. Could you help me evaluate this, if it's wrong?","Greetings I want to evaluate $\displaystyle\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx$. Here is my try: We have that $x\in[0,\pi]$ so $$\cos(n\pi)\le \cos(nx) \le 1.$$ Here I am not sure, but if it's correct then it gives: $$\frac{1}{2}\le\frac{1}{1+\cos^2(nx)}\le \frac{1}{1+\cos^2(n\pi)},$$ giving $$\lim_{n\rightarrow\infty} \frac{1}{2}\int_0^{\pi} \sin x dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (n\pi)} dx.$$ Since $$\int_0^{\pi} \sin x dx =2$$ By squeeze theorem we may conclude that $\displaystyle \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx=1$. Could you help me evaluate this, if it's wrong?",,"['real-analysis', 'integration', 'limits', 'proof-verification', 'definite-integrals']"
86,Limit of a Function involving tangent function and limits at infinity,Limit of a Function involving tangent function and limits at infinity,,Determine $$\lim_{x \to \infty}\left(\tan{\frac{\pi x}{2x+1}}\right)^\frac{1}{x}$$. Attempt Let $$y=\left(\tan{\frac{\pi x}{2x+1}}\right)^\frac{1}{x}$$ Put $\frac{1}{x}=p$. $$\lim_{p \to 0}\left(\tan{\frac{\pi}{2+p}}\right)^p$$. We have   $$\lim_{x \to \infty} y=\lim_{p \to 0}\left(\tan{\frac{\pi}{2+p}}\right)^p$$. Now consider the function $y$ in variable $p$  Taking $ln$ both sides $$ln\left(y\right)=p.ln\left(\tan{\frac{\pi}{2+p}}\right)$$. $$ln\left(y\right)=p.\frac{ln\left(\tan{\frac{\pi}{2+p}}\right)}{\tan{\frac{\pi}{2+p}}}.\tan{\frac{\pi}{2+p}}$$. Putting $\tan{\frac{\pi}{2+p}}=m$ We have $$ln\left(y\right)=p.\frac{ln\left(m\right)}{m}.\tan{\frac{\pi}{2+p}}$$. As $x \to \infty$ we have $p \to 0$ and hence $m \to \infty$ Hence the limit of $\frac{ln\left(m\right)}{m}$ is $0$. But I am unable to show the limit of other to part of the product. Please help me out.,Determine $$\lim_{x \to \infty}\left(\tan{\frac{\pi x}{2x+1}}\right)^\frac{1}{x}$$. Attempt Let $$y=\left(\tan{\frac{\pi x}{2x+1}}\right)^\frac{1}{x}$$ Put $\frac{1}{x}=p$. $$\lim_{p \to 0}\left(\tan{\frac{\pi}{2+p}}\right)^p$$. We have   $$\lim_{x \to \infty} y=\lim_{p \to 0}\left(\tan{\frac{\pi}{2+p}}\right)^p$$. Now consider the function $y$ in variable $p$  Taking $ln$ both sides $$ln\left(y\right)=p.ln\left(\tan{\frac{\pi}{2+p}}\right)$$. $$ln\left(y\right)=p.\frac{ln\left(\tan{\frac{\pi}{2+p}}\right)}{\tan{\frac{\pi}{2+p}}}.\tan{\frac{\pi}{2+p}}$$. Putting $\tan{\frac{\pi}{2+p}}=m$ We have $$ln\left(y\right)=p.\frac{ln\left(m\right)}{m}.\tan{\frac{\pi}{2+p}}$$. As $x \to \infty$ we have $p \to 0$ and hence $m \to \infty$ Hence the limit of $\frac{ln\left(m\right)}{m}$ is $0$. But I am unable to show the limit of other to part of the product. Please help me out.,,"['limits', 'limits-without-lhopital']"
87,Is a function continuous at the point where it ends abruptly?,Is a function continuous at the point where it ends abruptly?,,"Is a function continuous at the point where it ends abruptly? A function $f(x)$ to said to be continuous at a point $a$ iff: 1) $f(a)$ is defined, 2)$\lim\limits_{x \to a} f(x)$ exists, and 3)$\lim\limits_{x \to a} f(x)=f(a)$ At point $P_3$: 1)Left-hand limit exists, which is equal to 1. 2)The function is defined at $P_3$, which is also equal to 1. Therefore $$\lim\limits_{x \to P_3^-} f(x)=f(P_3)$$ So, can it be inferred that the function is continuous at $P_3$ or the rhight hand limit should also exist for the function to be continious at $P_3$?","Is a function continuous at the point where it ends abruptly? A function $f(x)$ to said to be continuous at a point $a$ iff: 1) $f(a)$ is defined, 2)$\lim\limits_{x \to a} f(x)$ exists, and 3)$\lim\limits_{x \to a} f(x)=f(a)$ At point $P_3$: 1)Left-hand limit exists, which is equal to 1. 2)The function is defined at $P_3$, which is also equal to 1. Therefore $$\lim\limits_{x \to P_3^-} f(x)=f(P_3)$$ So, can it be inferred that the function is continuous at $P_3$ or the rhight hand limit should also exist for the function to be continious at $P_3$?",,['limits']
88,What exactly does the $\varepsilon$-$\delta$ definition of limits prove?,What exactly does the - definition of limits prove?,\varepsilon \delta,"e.g. Find the limit, $\lim\limits_{x \to 2} \ {\frac{2(x^2-4)}{x-2}}$, and prove it exists using the $\varepsilon$-$\delta$ definition of limits. This might be a stupid question, but I'm having a hard time wrapping my head around the $\varepsilon$-$\delta$ definition. To my understanding, the limit exists if I'm able to find it, and the $\varepsilon$-$\delta$ proof requires that I already know the limit. So, what exactly does the $\varepsilon$-$\delta$ definition prove ? Seems to me like it's about confirming the already-found-limit by showing the continuity of the limit's immediate surrounding. It seems unnecessary.","e.g. Find the limit, $\lim\limits_{x \to 2} \ {\frac{2(x^2-4)}{x-2}}$, and prove it exists using the $\varepsilon$-$\delta$ definition of limits. This might be a stupid question, but I'm having a hard time wrapping my head around the $\varepsilon$-$\delta$ definition. To my understanding, the limit exists if I'm able to find it, and the $\varepsilon$-$\delta$ proof requires that I already know the limit. So, what exactly does the $\varepsilon$-$\delta$ definition prove ? Seems to me like it's about confirming the already-found-limit by showing the continuity of the limit's immediate surrounding. It seems unnecessary.",,"['calculus', 'limits']"
89,Integrability condition implies boundedness and limit is zero,Integrability condition implies boundedness and limit is zero,,"I'm having trouble with the following problem. Suppose that $f$ is a uniformly continuous function on $(0,\infty)$ with derivative $f^\prime(x)$ satisfying, $$ \int_0^\infty xf^2(x)dx < \infty, \;\;\;\;\;\;\int_0^\infty x^3(f^\prime(x))^2dx < \infty$$ Prove that $\lim\limits_{x\rightarrow\infty}xf(x) = 0$ . I tried to prove by contradiction. I assumed that there exists $\epsilon > 0$ and a sequence $x_n \rightarrow \infty$ such that $|x_nf(x_n)| > \epsilon$ for all $n$ . Using uniform continuity, I showed that, $$ \int_{x_n-\delta}^{x_n+\delta}xf(x)dx > \epsilon\delta $$ where $\delta$ is the modulus of continuity. Therefore, $$ \int_0^\infty xf(x)dx \geq \sum\limits_{n=1}^\infty\int_{x_n-\delta}^{x_n+\delta}xf(x)dx > \infty $$ My initial reasoning for using this approach was to hopefully use the Cauchy-Schwarz inequality to arrive at the contradiction, $$ \int_0^\infty xf(x)dx \leq \left(\int_0^\infty xf^2(x)dx\right)^{1/2}\left(\int_0^\infty xdx\right)^{1/2} $$ However, obviously the second integral is not finite so this approach probably will not work. Is there a slight modification I can use?","I'm having trouble with the following problem. Suppose that is a uniformly continuous function on with derivative satisfying, Prove that . I tried to prove by contradiction. I assumed that there exists and a sequence such that for all . Using uniform continuity, I showed that, where is the modulus of continuity. Therefore, My initial reasoning for using this approach was to hopefully use the Cauchy-Schwarz inequality to arrive at the contradiction, However, obviously the second integral is not finite so this approach probably will not work. Is there a slight modification I can use?","f (0,\infty) f^\prime(x)  \int_0^\infty xf^2(x)dx < \infty, \;\;\;\;\;\;\int_0^\infty x^3(f^\prime(x))^2dx < \infty \lim\limits_{x\rightarrow\infty}xf(x) = 0 \epsilon > 0 x_n \rightarrow \infty |x_nf(x_n)| > \epsilon n  \int_{x_n-\delta}^{x_n+\delta}xf(x)dx > \epsilon\delta  \delta  \int_0^\infty xf(x)dx \geq \sum\limits_{n=1}^\infty\int_{x_n-\delta}^{x_n+\delta}xf(x)dx > \infty   \int_0^\infty xf(x)dx \leq \left(\int_0^\infty xf^2(x)dx\right)^{1/2}\left(\int_0^\infty xdx\right)^{1/2} ","['real-analysis', 'limits', 'uniform-continuity']"
90,Showing a limit is 1,Showing a limit is 1,,Let $m(n) = \min \{ m : 2^mm^{3/2} \geq n \}$. I want to show that $\lim_{n \rightarrow \infty} m(n)/ \log_2(n) = 1$. I have been able to show that the limsup of this limit is at most 1. How can I show the other direction? I'm sure the minimality of $m(n)$ has to be used somwhere.,Let $m(n) = \min \{ m : 2^mm^{3/2} \geq n \}$. I want to show that $\lim_{n \rightarrow \infty} m(n)/ \log_2(n) = 1$. I have been able to show that the limsup of this limit is at most 1. How can I show the other direction? I'm sure the minimality of $m(n)$ has to be used somwhere.,,"['analysis', 'limits']"
91,"Computing: $ \lim_{(x,y)(0,0)}\frac{(x^5+y^5)\ln(x^2+y^2)}{(x^2+y^2)^2} $",Computing:," \lim_{(x,y)(0,0)}\frac{(x^5+y^5)\ln(x^2+y^2)}{(x^2+y^2)^2} ","$$ \lim_{(x,y)(0,0)}\frac{(x^5+y^5)\ln(x^2+y^2)}{(x^2+y^2)^2} $$ The answer is 0. Cannot seem to understand how the answer is 0.I know that the first part is 0 but I'm confused on how to deal with the natural log? Why is squeeze theorem not a good approach? $0<|(x^5)<(x^2)^2|=|x|$ so $|x^5+y^5|<|(x^2+y^2)^2|$ so therefor  $$0< \dfrac{|x^5+y^5|}{|(x^2+y^2)^2|}<1$$ then multiply both sides with $\ln(x^2+y^2)$. and take the $$\lim_{(x,y)\to (0,0)} \ln(x^2+y^2).$$ and since it is not $0$ but its negative infinity the limit doesn't exist by the squeeze theorem. which is not the right answer.","$$ \lim_{(x,y)(0,0)}\frac{(x^5+y^5)\ln(x^2+y^2)}{(x^2+y^2)^2} $$ The answer is 0. Cannot seem to understand how the answer is 0.I know that the first part is 0 but I'm confused on how to deal with the natural log? Why is squeeze theorem not a good approach? $0<|(x^5)<(x^2)^2|=|x|$ so $|x^5+y^5|<|(x^2+y^2)^2|$ so therefor  $$0< \dfrac{|x^5+y^5|}{|(x^2+y^2)^2|}<1$$ then multiply both sides with $\ln(x^2+y^2)$. and take the $$\lim_{(x,y)\to (0,0)} \ln(x^2+y^2).$$ and since it is not $0$ but its negative infinity the limit doesn't exist by the squeeze theorem. which is not the right answer.",,"['limits', 'multivariable-calculus']"
92,Epsilon-Delta proof for a limit of a function,Epsilon-Delta proof for a limit of a function,,"$\lim\limits_{x \to 2} \frac{x^2+4}{x+2}=2$ I understand the structure of the epsilon delta proof, but I need help with the scratchwork/setup. Using |$\frac{x^2+4}{x+2}-2$| $<\epsilon$ , you can factor and you're left with |$x-4$|$< \epsilon$. What I'm stuck on is solving for $x-a$ or $x-2$. Can I do something with the Triangle Inequality to say |$x-2-2$| $\le$ |$x-2$|$ \;+\; 2$ $< \epsilon$ Any help would be appreciated!","$\lim\limits_{x \to 2} \frac{x^2+4}{x+2}=2$ I understand the structure of the epsilon delta proof, but I need help with the scratchwork/setup. Using |$\frac{x^2+4}{x+2}-2$| $<\epsilon$ , you can factor and you're left with |$x-4$|$< \epsilon$. What I'm stuck on is solving for $x-a$ or $x-2$. Can I do something with the Triangle Inequality to say |$x-2-2$| $\le$ |$x-2$|$ \;+\; 2$ $< \epsilon$ Any help would be appreciated!",,"['real-analysis', 'analysis', 'limits', 'epsilon-delta']"
93,Limit involving inner products [duplicate],Limit involving inner products [duplicate],,"This question already has an answer here : Limit of Matrices (1 answer) Closed 6 years ago . Let $A$ be an $n \times n$ real symmetric matrix with eigenvalues $\lambda_1,\ldots,\lambda_n.$ Find the set of $v \in \mathbb{R}^n$ such that $$\lim_{k\to\infty}\left<A^{2k}v,v\right>^{1/k}$$ exists and find the set of possible limits. Since $A$ is symmetric there exists an orthonormal basis $v_1,\ldots,v_n$ of eigenvectors of $A$. Let $\lambda_j$ be the eigenvector corresponding to $v_j$. If $v \in \mathbb{R}^n$, then $$v = c_1v_1 + \cdots c_nv_n, c_j \in \mathbb{R}.$$ Since $<v_i, v_j> = 0$ for $i \neq j$ and $= 1$ if $i = j$, and $A^{2k}v_j = \lambda_j^{2k}v_j$, by the linearity of the inner product the limit becomes $$\lim_{k\to\infty} \left(c_1^2\lambda_1^{2k} + \cdots c_n^2\lambda_n^{2k}\right)^{1/k}.$$ This is where I'm stuck. I don't know how to evaluate the limit. I tried taking logs and using L'Hopital's rule, but that only gave another limit I couldn't compute. Since $$c_1^2\lambda_1^{2k} + \cdots + c_n^2\lambda_n^{2k} \leq (c_1^{2/k}\lambda_1^2 + \cdots + c_n^{2/k}\lambda_n^2)^k,$$ I know that if the limit exists it's bounded above by $\lambda_1^2 + \cdots \lambda_n^2$. Any help proceeding would be great.","This question already has an answer here : Limit of Matrices (1 answer) Closed 6 years ago . Let $A$ be an $n \times n$ real symmetric matrix with eigenvalues $\lambda_1,\ldots,\lambda_n.$ Find the set of $v \in \mathbb{R}^n$ such that $$\lim_{k\to\infty}\left<A^{2k}v,v\right>^{1/k}$$ exists and find the set of possible limits. Since $A$ is symmetric there exists an orthonormal basis $v_1,\ldots,v_n$ of eigenvectors of $A$. Let $\lambda_j$ be the eigenvector corresponding to $v_j$. If $v \in \mathbb{R}^n$, then $$v = c_1v_1 + \cdots c_nv_n, c_j \in \mathbb{R}.$$ Since $<v_i, v_j> = 0$ for $i \neq j$ and $= 1$ if $i = j$, and $A^{2k}v_j = \lambda_j^{2k}v_j$, by the linearity of the inner product the limit becomes $$\lim_{k\to\infty} \left(c_1^2\lambda_1^{2k} + \cdots c_n^2\lambda_n^{2k}\right)^{1/k}.$$ This is where I'm stuck. I don't know how to evaluate the limit. I tried taking logs and using L'Hopital's rule, but that only gave another limit I couldn't compute. Since $$c_1^2\lambda_1^{2k} + \cdots + c_n^2\lambda_n^{2k} \leq (c_1^{2/k}\lambda_1^2 + \cdots + c_n^{2/k}\lambda_n^2)^k,$$ I know that if the limit exists it's bounded above by $\lambda_1^2 + \cdots \lambda_n^2$. Any help proceeding would be great.",,"['real-analysis', 'linear-algebra', 'limits', 'inner-products']"
94,"Construct a continuous function such that $\int_{-\infty}^\infty|f(x)|\,dx<\infty$ but $\lim_{x\to \infty}|f(x)|$ does not exists. [duplicate]",Construct a continuous function such that  but  does not exists. [duplicate],"\int_{-\infty}^\infty|f(x)|\,dx<\infty \lim_{x\to \infty}|f(x)|","This question already has answers here : Give an example of a continuous function $f : [0, ) \to [0, )$ such that $\int_{0}^{\infty}f(x)dx$ exists but $f$ is unbounded. (6 answers) Closed 6 years ago . I have to construct a continuous function such that $$\int_{-\infty}^\infty |f(x)| \, dx<\infty$$ but $$\lim_{x\to \infty}|f(x)|$$ does not exists. I have already known one messy example that deals with lines and minimum distance. I just want to see different examples. I know we can construct a triangle with fixed height and decreasing base such that the area get's smaller and the integral is like the geometric series. However, the fixed height makes the limit inexistent. I just dont know how to describe that properly.  Thanks.","This question already has answers here : Give an example of a continuous function $f : [0, ) \to [0, )$ such that $\int_{0}^{\infty}f(x)dx$ exists but $f$ is unbounded. (6 answers) Closed 6 years ago . I have to construct a continuous function such that $$\int_{-\infty}^\infty |f(x)| \, dx<\infty$$ but $$\lim_{x\to \infty}|f(x)|$$ does not exists. I have already known one messy example that deals with lines and minimum distance. I just want to see different examples. I know we can construct a triangle with fixed height and decreasing base such that the area get's smaller and the integral is like the geometric series. However, the fixed height makes the limit inexistent. I just dont know how to describe that properly.  Thanks.",,"['real-analysis', 'limits', 'continuity', 'improper-integrals']"
95,Limit of the fraction of numbers $\le n$,Limit of the fraction of numbers,\le n,"For a set $E \subseteq \mathbb{N}$, we defined $r_n(E):=\left |{E \cap[n]}\right |$, where $[n]:=\left\{{1,2,...,n}\right\}$. I need to find a set $E^* \subseteq \mathbb{N}$ for which $\nexists \lim_{n \to \infty} \frac{r_n(E^*)}{n}$. I have tried with some set formed by unions, intersections, and complements between the sets of multiples of different prime numbers, but I could not find this set.","For a set $E \subseteq \mathbb{N}$, we defined $r_n(E):=\left |{E \cap[n]}\right |$, where $[n]:=\left\{{1,2,...,n}\right\}$. I need to find a set $E^* \subseteq \mathbb{N}$ for which $\nexists \lim_{n \to \infty} \frac{r_n(E^*)}{n}$. I have tried with some set formed by unions, intersections, and complements between the sets of multiples of different prime numbers, but I could not find this set.",,"['number-theory', 'limits', 'prime-numbers']"
96,Cesaro continiuity leads linearity,Cesaro continiuity leads linearity,,"I need just a hint please. It seems that I have to prove that  $f(x)=mx$ in which  $m\in \mathbb{R}.$ But I couldn't handle it. Problem: We say that a sequence $x_{n}\; , n = 1, 2,\cdots ,$ Cesaro converges to $a,$    if   $$ \lim\limits_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}x_{i}=a. $$   A function    $f$    is Cesaro continuous at    $a$    if    $x_{n}\to a$ (in Cesaro mean)    implies    $f(x_{n})\to a$ (in Cesaro mean).    Prove that, if   $f : \mathbb{R}\to\mathbb{R}$    is Cesaro continuous at    $0$    and    $f(0) = 0,$ then $f$ is linear.","I need just a hint please. It seems that I have to prove that  $f(x)=mx$ in which  $m\in \mathbb{R}.$ But I couldn't handle it. Problem: We say that a sequence $x_{n}\; , n = 1, 2,\cdots ,$ Cesaro converges to $a,$    if   $$ \lim\limits_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}x_{i}=a. $$   A function    $f$    is Cesaro continuous at    $a$    if    $x_{n}\to a$ (in Cesaro mean)    implies    $f(x_{n})\to a$ (in Cesaro mean).    Prove that, if   $f : \mathbb{R}\to\mathbb{R}$    is Cesaro continuous at    $0$    and    $f(0) = 0,$ then $f$ is linear.",,"['real-analysis', 'analysis', 'limits', 'cesaro-summable']"
97,"If $f:[a,\infty)\to\mathbb{R}$ is monotonically decreasing, and $\int_{1}^{\infty}f(x)dx$ is convergent, then $\underset {x\to\infty} \lim f(x)=0$.","If  is monotonically decreasing, and  is convergent, then .","f:[a,\infty)\to\mathbb{R} \int_{1}^{\infty}f(x)dx \underset {x\to\infty} \lim f(x)=0","I have the following true/false question If $f:[a,\infty)\to\mathbb{R}$ is monotonically decreasing, and the integral $\int_{1}^{\infty}f(x)dx$ is convergent, then $\underset {x\to\infty} \lim f(x)=0$. My intuition is that it's true but not sure how to prove it or if actually there is a counter example. Thanks!","I have the following true/false question If $f:[a,\infty)\to\mathbb{R}$ is monotonically decreasing, and the integral $\int_{1}^{\infty}f(x)dx$ is convergent, then $\underset {x\to\infty} \lim f(x)=0$. My intuition is that it's true but not sure how to prove it or if actually there is a counter example. Thanks!",,"['calculus', 'limits', 'improper-integrals']"
98,Is this limit dependent on x. Derivative natural log.,Is this limit dependent on x. Derivative natural log.,,"$$ \begin{align}  \frac{d}{dx}\ln x &= \lim_{h \to 0} \, \frac{\ln{(x+h)} - \ln x}{h} \\ \\ &= \lim_{h \to 0} \, \frac{1}{h} \, {\ln{\bigg(\frac{x+h}{x}\bigg)}} \\ \\ &= \lim_{h \to 0} \, {\ln{\bigg(1+\frac{h}{x}\bigg)}}^{1/h} \\ \\ &=  {\ln{\lim_{h \to 0} \, \bigg(1+\frac{h}{x}\bigg)}}^{1/h}, \text{ let } \frac{h}{x} = \frac{1}{n}  \tag{*}\\ \\  &= {\ln{\lim_{n \to \infty} \, \bigg [\bigg(1+\frac{1}{n}\bigg)}}^{n} \,  \bigg]^{1/x} \tag{A} \\ \\ &= \frac{1}{x} \ln \lim_{n \to \infty} \, \bigg(1+\frac{1}{n}\bigg)^{n} \tag{B} \\ \\ &=  \frac{1}{x} \ln e = \frac{1}{x}  \end{align}$$ So my question is: is this a legal step from (A) to (B)? I removed $x$ from the limit, but in step (*) I sort of created $ x = h\cdot n$ which is the part that's confusing me. Does this indicate $x$ depends on $n$? Any other tips on the clarity of the proof would be great, thanks!","$$ \begin{align}  \frac{d}{dx}\ln x &= \lim_{h \to 0} \, \frac{\ln{(x+h)} - \ln x}{h} \\ \\ &= \lim_{h \to 0} \, \frac{1}{h} \, {\ln{\bigg(\frac{x+h}{x}\bigg)}} \\ \\ &= \lim_{h \to 0} \, {\ln{\bigg(1+\frac{h}{x}\bigg)}}^{1/h} \\ \\ &=  {\ln{\lim_{h \to 0} \, \bigg(1+\frac{h}{x}\bigg)}}^{1/h}, \text{ let } \frac{h}{x} = \frac{1}{n}  \tag{*}\\ \\  &= {\ln{\lim_{n \to \infty} \, \bigg [\bigg(1+\frac{1}{n}\bigg)}}^{n} \,  \bigg]^{1/x} \tag{A} \\ \\ &= \frac{1}{x} \ln \lim_{n \to \infty} \, \bigg(1+\frac{1}{n}\bigg)^{n} \tag{B} \\ \\ &=  \frac{1}{x} \ln e = \frac{1}{x}  \end{align}$$ So my question is: is this a legal step from (A) to (B)? I removed $x$ from the limit, but in step (*) I sort of created $ x = h\cdot n$ which is the part that's confusing me. Does this indicate $x$ depends on $n$? Any other tips on the clarity of the proof would be great, thanks!",,"['calculus', 'limits', 'derivatives', 'logarithms', 'exponential-function']"
99,Existence of derivative at the origin,Existence of derivative at the origin,,"I'm supposed to find for which values of $p$ and $q$ the following function is differentiable at the origin: $$    f(x,y,z) =       \begin{cases}        \dfrac{(x^2y^2)^p(1-\cos(z))^q}{x^2+y^2+z^2} & \text{if } (x,y,z) \neq (0,0,0) \\         0 & \text{if } (x,y,z) = (0,0,0)\\       \end{cases} $$ I've made some progress using the Taylor expansion of the cosine about $0$, but couldn't get far even doing so. Could anyone share a solution?","I'm supposed to find for which values of $p$ and $q$ the following function is differentiable at the origin: $$    f(x,y,z) =       \begin{cases}        \dfrac{(x^2y^2)^p(1-\cos(z))^q}{x^2+y^2+z^2} & \text{if } (x,y,z) \neq (0,0,0) \\         0 & \text{if } (x,y,z) = (0,0,0)\\       \end{cases} $$ I've made some progress using the Taylor expansion of the cosine about $0$, but couldn't get far even doing so. Could anyone share a solution?",,"['real-analysis', 'limits', 'derivatives']"
