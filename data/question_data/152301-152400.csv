,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What extra assumption makes this transformation affine?,What extra assumption makes this transformation affine?,,"Let a vector space $V$ be given.  Let $f:V\to V$ have the property that for all $x,y,a\in V$, $$   f(x+a)-f(y+a) = f(x) - f(y) \tag{$\star$} $$ Q1. I'd like to know how weak one can make additional assumptions to guarantee that $f$ is affine, namely that there exists a linear transformation $L:V\to V$, and an element $v\in V$ such that \begin{align}   f(x) = L(x) + v \end{align} for all $x\in V$. I think, for example, that if one assumes that $V = \mathbb R^n$, and if $f$ is differentiable, then $(\star)$ implies that $f$ is affine. This makes me think that perhaps one needs a notion of smoothness or continuity in general and therefore possibly a norm?  Also Q2. Is there a common name for the property $(\star)$?","Let a vector space $V$ be given.  Let $f:V\to V$ have the property that for all $x,y,a\in V$, $$   f(x+a)-f(y+a) = f(x) - f(y) \tag{$\star$} $$ Q1. I'd like to know how weak one can make additional assumptions to guarantee that $f$ is affine, namely that there exists a linear transformation $L:V\to V$, and an element $v\in V$ such that \begin{align}   f(x) = L(x) + v \end{align} for all $x\in V$. I think, for example, that if one assumes that $V = \mathbb R^n$, and if $f$ is differentiable, then $(\star)$ implies that $f$ is affine. This makes me think that perhaps one needs a notion of smoothness or continuity in general and therefore possibly a norm?  Also Q2. Is there a common name for the property $(\star)$?",,"['linear-algebra', 'analysis', 'vector-spaces', 'continuity', 'normed-spaces']"
1,Prove $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$ with the epsilon-delta definition of limit.,Prove  with the epsilon-delta definition of limit.,\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1,"It is well known that $$\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$$ I know several proofs of this: the geometric proof shows that $\cos(\theta)\leq\frac {\sin(\theta)}{\theta}\leq1$ and using the Squeeze Theorem I conclude that $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$, other proof uses the Maclaurin series of $\sin(x)$. My question is: is there a demonstration of this limit using the epsilon-delta definition of limit?","It is well known that $$\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$$ I know several proofs of this: the geometric proof shows that $\cos(\theta)\leq\frac {\sin(\theta)}{\theta}\leq1$ and using the Squeeze Theorem I conclude that $\lim_{x \rightarrow 0} \frac {\sin(x)}{x} = 1$, other proof uses the Maclaurin series of $\sin(x)$. My question is: is there a demonstration of this limit using the epsilon-delta definition of limit?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
2,Human accessible mathematics : Objects defined by a finite number of steps,Human accessible mathematics : Objects defined by a finite number of steps,,"This is about the uncountable spaces like real numbers : When i was a student, I was proud to talk about them as if they were little toys I could play with. But ... I once realized that ""most"" of their elements simply couldn't be mentioned for they couldn't be defined by a finite number of words (or steps.) The Proof is quite easy : the sub set of the real numbers defined by a finite number of steps is countable. But R is not. For example : x = 0.10110111011110111110111111 ... is easy to mention for the decimal sequence that defines it is the result of a simple process. For many other numbers, there is no finite process to define them. Let us consider Rf the subset of R made of ""mentionable"" numbers (defined by a finite number of words.) Rf is stable for addition , multiplication , etc ...  if a and b are mentionable a + b is mentionable. So Rf has interesting algebraical properties. It is not a closed subset : a sequence of mentionable numbers might not be mentionable itself : For example if x is a non-mentionable number, the sequence of its decimal approximations (1 , 1.2 , 1.25 , 1.258 , ...) is made of mentionable numbers but can not be mentioned (if it was, x could be defined as the limit of this sequence and would be mentionable itself.) This can be done for many other structures. The algebraic stability of the mentionable subset lets me think that some interesting results exist. (mentionable fields like Rf, lead to vector spaces, etc ...) may be there is somewhere a little theory about this all. I guess I am not the first one to think about this. :o) This Rf subset of R is surely well known and has a proper name.  Anybody knows how this is called and where i can find results about this ? (Sorry if this question is stupid I stopped doing math 20 years ago ...) THANKS","This is about the uncountable spaces like real numbers : When i was a student, I was proud to talk about them as if they were little toys I could play with. But ... I once realized that ""most"" of their elements simply couldn't be mentioned for they couldn't be defined by a finite number of words (or steps.) The Proof is quite easy : the sub set of the real numbers defined by a finite number of steps is countable. But R is not. For example : x = 0.10110111011110111110111111 ... is easy to mention for the decimal sequence that defines it is the result of a simple process. For many other numbers, there is no finite process to define them. Let us consider Rf the subset of R made of ""mentionable"" numbers (defined by a finite number of words.) Rf is stable for addition , multiplication , etc ...  if a and b are mentionable a + b is mentionable. So Rf has interesting algebraical properties. It is not a closed subset : a sequence of mentionable numbers might not be mentionable itself : For example if x is a non-mentionable number, the sequence of its decimal approximations (1 , 1.2 , 1.25 , 1.258 , ...) is made of mentionable numbers but can not be mentioned (if it was, x could be defined as the limit of this sequence and would be mentionable itself.) This can be done for many other structures. The algebraic stability of the mentionable subset lets me think that some interesting results exist. (mentionable fields like Rf, lead to vector spaces, etc ...) may be there is somewhere a little theory about this all. I guess I am not the first one to think about this. :o) This Rf subset of R is surely well known and has a proper name.  Anybody knows how this is called and where i can find results about this ? (Sorry if this question is stupid I stopped doing math 20 years ago ...) THANKS",,"['analysis', 'logic']"
3,The cardinality of the function,The cardinality of the function,,"I'm reading a book about cardinality of functions and while I was solving some problems of the book I saw this: Prove that the cardinality of a general function $f:K \to K$ is $n^n$, where $n$ is the number of elements in $K$. I think I do understand the logic behind this,for example let's say: $K=\{a,b,c,d,e\}$. Now the function $f$ has a cardinality of $5^5$ because the first element ($a$) has $5$ opportunities, the second element ($b$) has also $5$ opportunities,the third element ($c$) has also $5$ opportunities and so on. Now I'm trying to prove this as the task says,but I don't know how to do it,how to write it in a more mathematical way.","I'm reading a book about cardinality of functions and while I was solving some problems of the book I saw this: Prove that the cardinality of a general function $f:K \to K$ is $n^n$, where $n$ is the number of elements in $K$. I think I do understand the logic behind this,for example let's say: $K=\{a,b,c,d,e\}$. Now the function $f$ has a cardinality of $5^5$ because the first element ($a$) has $5$ opportunities, the second element ($b$) has also $5$ opportunities,the third element ($c$) has also $5$ opportunities and so on. Now I'm trying to prove this as the task says,but I don't know how to do it,how to write it in a more mathematical way.",,"['analysis', 'functions']"
4,Integral with absolut-value function,Integral with absolut-value function,,How do I seperate the following integral? The integral of $|x^2-y|$ with $|y| \leq 1$ and $|x| \leq 1$. I know that the absolute value is positive for $x^2 \geq 1$ and negative for $x^2$ but I am getting the wrong answer when I write them as a sum of integrals. I think I chose wrong bounds.,How do I seperate the following integral? The integral of $|x^2-y|$ with $|y| \leq 1$ and $|x| \leq 1$. I know that the absolute value is positive for $x^2 \geq 1$ and negative for $x^2$ but I am getting the wrong answer when I write them as a sum of integrals. I think I chose wrong bounds.,,"['calculus', 'analysis']"
5,Showing that the n first derivatives of (x²-1)^n have at least r roots (for the r-th derivative)?,Showing that the n first derivatives of (x²-1)^n have at least r roots (for the r-th derivative)?,,"I have f(x) = (x²-1)^n. I want to show that, for r = 0,1,2,...,n, the r-th derivative is a polynomial (that's easy to show) that has no fewer than r distinct roots in (-1,1). I guess I need to use induction here. I've showed that these derivatives have actually no more than r distinct roots but I can't show the actual result... My only idea was to use the Rolle's Theorem but it does not ensure enough roots.. Can you help me? Thank you","I have f(x) = (x²-1)^n. I want to show that, for r = 0,1,2,...,n, the r-th derivative is a polynomial (that's easy to show) that has no fewer than r distinct roots in (-1,1). I guess I need to use induction here. I've showed that these derivatives have actually no more than r distinct roots but I can't show the actual result... My only idea was to use the Rolle's Theorem but it does not ensure enough roots.. Can you help me? Thank you",,"['analysis', 'polynomials', 'derivatives', 'roots']"
6,Arzela Ascoli variation theorem,Arzela Ascoli variation theorem,,"I am trying to prove the following variation of the Arzel-Ascoli theorem: Let $(f_n)n$ be a sequence of differentiable functions on $[a,b]$ whose derivatives are uniformly bounded and there is an $x_0 \in [a,b]$ such that $(f_n(x_0))_n$ is bounded in $R$. Prove that $f_n$ has a uniformly convergent subsequence. k I can prove that the sequence $f_n$ is equicontinuous but I am not sure how to use the fact that it is bounded at one point. It mean it is just a sequence of real numbers which is bounded thus we can find a converging subsequence $f_{n_k}(x_0)$ but how to continue for the uniform convergence part? I would appreciate some help. Thank you!","I am trying to prove the following variation of the Arzel-Ascoli theorem: Let $(f_n)n$ be a sequence of differentiable functions on $[a,b]$ whose derivatives are uniformly bounded and there is an $x_0 \in [a,b]$ such that $(f_n(x_0))_n$ is bounded in $R$. Prove that $f_n$ has a uniformly convergent subsequence. k I can prove that the sequence $f_n$ is equicontinuous but I am not sure how to use the fact that it is bounded at one point. It mean it is just a sequence of real numbers which is bounded thus we can find a converging subsequence $f_{n_k}(x_0)$ but how to continue for the uniform convergence part? I would appreciate some help. Thank you!",,"['real-analysis', 'analysis', 'uniform-convergence', 'uniform-continuity', 'bounded-variation']"
7,Rudin: Supremum of Finite Decimals,Rudin: Supremum of Finite Decimals,,"I am reading Rudin. Note the following construction: Let $x>0$ be real. Let $n_0$ be the largest integer s.t. $n_0 \leq x$. Then, having chosen $n_0, \ldots, n_{k-1}$, let $n_k$ be the largest integer s.t. $n_0 + \dfrac{n_1}{10} + \ldots + \dfrac{n_k}{10^k} \leq x$. Let $E$ be the set of numbers $n_0 + \dfrac{n_1}{10} + \ldots + \dfrac{n_k}{10^k} \leq x$, where $k$ is a non-negative integer. Then $x = \sup E$. Conversely, for any infinite decimal, the set $E$ is bounded above, and the infinite decimal expansion is the decimal expansion of $\sup E$. How does one prove both directions? I was thinking that for the first part, we must use precisely the definition of the supremum. That is, if $\epsilon > 0$, we must find a number $h \in E$ s.t. $h> x - \epsilon$. However, it isn't clear to me that such a decimal number can be constructed. And how is the second direction the converse? I think it is a reiteration of the first direction. Set $x$ to be an infinite decimal expansion, and the result follows immediately from the proof of the first direction.","I am reading Rudin. Note the following construction: Let $x>0$ be real. Let $n_0$ be the largest integer s.t. $n_0 \leq x$. Then, having chosen $n_0, \ldots, n_{k-1}$, let $n_k$ be the largest integer s.t. $n_0 + \dfrac{n_1}{10} + \ldots + \dfrac{n_k}{10^k} \leq x$. Let $E$ be the set of numbers $n_0 + \dfrac{n_1}{10} + \ldots + \dfrac{n_k}{10^k} \leq x$, where $k$ is a non-negative integer. Then $x = \sup E$. Conversely, for any infinite decimal, the set $E$ is bounded above, and the infinite decimal expansion is the decimal expansion of $\sup E$. How does one prove both directions? I was thinking that for the first part, we must use precisely the definition of the supremum. That is, if $\epsilon > 0$, we must find a number $h \in E$ s.t. $h> x - \epsilon$. However, it isn't clear to me that such a decimal number can be constructed. And how is the second direction the converse? I think it is a reiteration of the first direction. Set $x$ to be an infinite decimal expansion, and the result follows immediately from the proof of the first direction.",,"['real-analysis', 'analysis', 'decimal-expansion']"
8,"$f, f|f| \in L^{1} (\mathbb R) \cap C_{0} (\mathbb R) \implies |f| \in H_{1} (\mathbb R)$?",?,"f, f|f| \in L^{1} (\mathbb R) \cap C_{0} (\mathbb R) \implies |f| \in H_{1} (\mathbb R)","Suppose $f \  \text{and} \ f|f|\in L^{1}(\mathbb R).$ Then, clearly, $|f|\in L^{2}(\mathbb R)$ and therefore by Plancheral theorem , we get, $\widehat{|f|} \in L^{2}(\mathbb R).$ Also, assume, $f, |f|f, \hat{f}, \widehat{f|f|} \in L^{1}(\mathbb R) \cap C_{0}(\mathbb R).$ My Question : Can we expect $|f|\in H_{1}(\mathbb R)=\{ f\in \mathcal{S'}(\mathbb R): (1+|\xi|^{2})^{1/2}\hat{f}\in L^{2}(\mathbb R) \}$ ( Sobolev spaces ), Or, we we can  produce a counter example ? (If we put, additional condition, $|(\widehat{|f|}(\xi))| \leq \frac{1}{\sqrt{1 + |\xi|^{2}}},  \ (\xi \in \mathbb R)$, then, certainly, by definition of sobolev space,  it will follows; so is this condition necessary ?) Motivation : Actually, I want to show: $\widehat{|f|}\in L^{1}(\mathbb R);$ so it sufficient to show, $|f|\in H_{1}$. If there is other way, I would like to have your suggestions. Thanks,","Suppose $f \  \text{and} \ f|f|\in L^{1}(\mathbb R).$ Then, clearly, $|f|\in L^{2}(\mathbb R)$ and therefore by Plancheral theorem , we get, $\widehat{|f|} \in L^{2}(\mathbb R).$ Also, assume, $f, |f|f, \hat{f}, \widehat{f|f|} \in L^{1}(\mathbb R) \cap C_{0}(\mathbb R).$ My Question : Can we expect $|f|\in H_{1}(\mathbb R)=\{ f\in \mathcal{S'}(\mathbb R): (1+|\xi|^{2})^{1/2}\hat{f}\in L^{2}(\mathbb R) \}$ ( Sobolev spaces ), Or, we we can  produce a counter example ? (If we put, additional condition, $|(\widehat{|f|}(\xi))| \leq \frac{1}{\sqrt{1 + |\xi|^{2}}},  \ (\xi \in \mathbb R)$, then, certainly, by definition of sobolev space,  it will follows; so is this condition necessary ?) Motivation : Actually, I want to show: $\widehat{|f|}\in L^{1}(\mathbb R);$ so it sufficient to show, $|f|\in H_{1}$. If there is other way, I would like to have your suggestions. Thanks,",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'fourier-analysis', 'sobolev-spaces']"
9,Geometric Interpretation of Laplace's Equation,Geometric Interpretation of Laplace's Equation,,"Let $f: \mathbb{C} \rightarrow \mathbb{C}$ be analytic.  In the natural way, let $f = u + vi$ for $u,v : \mathbb{C} \rightarrow  \mathbb{R}$.  Let $z \in \mathbb{C}$. Suppose that $u$ and $v$ satisfy Laplace's equation at $z$ (which they do since $f$ is analytic) so that $$ \Delta u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0 $$ and $$ \Delta v = \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2} = 0 $$ Question 1: Does this mean that the real and complex parts of $f''(z)$ are equal to each other in magnitude? EDIT: This is why I think that the Laplace Equations holding for $u$ and $v$ implies that $f''(z)$ has real and complex components which are equal in magnitude to each other. Since $f$ is analytic, we have that the Cauchy-Riemann Equations hold for $u$ and $v$.  In particular, we have that $$ \frac{\partial u}{\partial x} = \frac{\partial v }{\partial y} $$ and $$ \frac{\partial v}{\partial x} = - \frac{\partial u}{\partial y} $$ This then means that $f'(z) = \frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x} = \frac{\partial u}{\partial x} - i \frac{\partial u}{\partial y}$. Then $\color{red}{f''(z) = \frac{\partial^2 u}{\partial x^2} - i \frac{\partial^2 u}{\partial y^2}}$. Yet since $\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0$ via Laplace, we have that $f''(z)$ satisfies that its real and complex parts are equal in magnitude to each other. Is this correct? Question 2: If no to the above answer, what is a geometric interpretation of this statement?","Let $f: \mathbb{C} \rightarrow \mathbb{C}$ be analytic.  In the natural way, let $f = u + vi$ for $u,v : \mathbb{C} \rightarrow  \mathbb{R}$.  Let $z \in \mathbb{C}$. Suppose that $u$ and $v$ satisfy Laplace's equation at $z$ (which they do since $f$ is analytic) so that $$ \Delta u = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0 $$ and $$ \Delta v = \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2} = 0 $$ Question 1: Does this mean that the real and complex parts of $f''(z)$ are equal to each other in magnitude? EDIT: This is why I think that the Laplace Equations holding for $u$ and $v$ implies that $f''(z)$ has real and complex components which are equal in magnitude to each other. Since $f$ is analytic, we have that the Cauchy-Riemann Equations hold for $u$ and $v$.  In particular, we have that $$ \frac{\partial u}{\partial x} = \frac{\partial v }{\partial y} $$ and $$ \frac{\partial v}{\partial x} = - \frac{\partial u}{\partial y} $$ This then means that $f'(z) = \frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x} = \frac{\partial u}{\partial x} - i \frac{\partial u}{\partial y}$. Then $\color{red}{f''(z) = \frac{\partial^2 u}{\partial x^2} - i \frac{\partial^2 u}{\partial y^2}}$. Yet since $\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0$ via Laplace, we have that $f''(z)$ satisfies that its real and complex parts are equal in magnitude to each other. Is this correct? Question 2: If no to the above answer, what is a geometric interpretation of this statement?",,"['complex-analysis', 'analysis', 'harmonic-functions']"
10,Understanding the definition of the order of an entire function in Ahlfors's Complex Analysis,Understanding the definition of the order of an entire function in Ahlfors's Complex Analysis,,"Let $f: \mathbb C \to \mathbb C$ be an entire function. The order of $f$ is defined by $$\lambda=\limsup_{r \to \infty} \frac{\log \log M(r)}{\log r}, $$ where $$M(r)=\max_{|z|=r} |f(z)| .$$ Ahlfors in his Complex Analysis claims that ""According to this definition $\lambda$ is the smallest number such that       $$M(r)\leq e^{r^{\lambda+\varepsilon}} $$ for any given $\varepsilon > 0$ as soon as $r$ is sufficiently large."" Why is this true? My attempt: We know that $$\lambda=\lim_{\rho \to \infty} \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}. $$ From the definition of the limit we have that for any $\varepsilon>0$, there exists some $\rho_0>0$, such that $$\left\lvert \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}-\lambda \right\rvert \leq \varepsilon ,$$ for every $\rho \geq \rho_0$. In other words $$\frac{\log \log M(r)}{\log r} \leq \lambda+\varepsilon $$ for every $r \geq \rho_0$. From here it is easy to see that $$M(r)\leq e^{r^{\lambda+\varepsilon}}, $$ for all $r \geq \rho_0$. I cannot see why $\lambda$ is the smallest number with this property. Thanks in advance.","Let $f: \mathbb C \to \mathbb C$ be an entire function. The order of $f$ is defined by $$\lambda=\limsup_{r \to \infty} \frac{\log \log M(r)}{\log r}, $$ where $$M(r)=\max_{|z|=r} |f(z)| .$$ Ahlfors in his Complex Analysis claims that ""According to this definition $\lambda$ is the smallest number such that       $$M(r)\leq e^{r^{\lambda+\varepsilon}} $$ for any given $\varepsilon > 0$ as soon as $r$ is sufficiently large."" Why is this true? My attempt: We know that $$\lambda=\lim_{\rho \to \infty} \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}. $$ From the definition of the limit we have that for any $\varepsilon>0$, there exists some $\rho_0>0$, such that $$\left\lvert \sup_{r \geq \rho} \frac{\log \log M(r)}{\log r}-\lambda \right\rvert \leq \varepsilon ,$$ for every $\rho \geq \rho_0$. In other words $$\frac{\log \log M(r)}{\log r} \leq \lambda+\varepsilon $$ for every $r \geq \rho_0$. From here it is easy to see that $$M(r)\leq e^{r^{\lambda+\varepsilon}}, $$ for all $r \geq \rho_0$. I cannot see why $\lambda$ is the smallest number with this property. Thanks in advance.",,"['complex-analysis', 'limsup-and-liminf']"
11,Ratio of maximal to minimal jump in the set of angle multiples (corrected),Ratio of maximal to minimal jump in the set of angle multiples (corrected),,"(This is the corrected version of the question I asked here: Ratio of maximal to minimal jump in the set of angle multiples .) Let $S^1$ be the unit circle in the complex plain. Let $d:S^1\times S^1\to\mathbb{R}$ be the distance function given by the arc length. Let $\theta\in S^1$ be an element of infinite order, that is $\theta^n\neq 1$ for any $n\neq 0$.  Define $J_N$ to be the set of those pairs $(i,j)$, such that $1\leq i,j\leq N$ and the smaller open arc between $\theta^i$ and $\theta^j$ does not contain any other $\theta^k$, $1\leq k\leq N$. I would like to know what is the behavior of the ratio  \begin{equation} \frac{\max_{(i,j)\in J_N} d(\theta^i,\theta^j)}{\min_{(i,j)\in J_N}d(\theta^i,\theta^j)} \end{equation} when $N$ goes to $\infty$. I would like it to be bounded above and below, but any information would be useful.","(This is the corrected version of the question I asked here: Ratio of maximal to minimal jump in the set of angle multiples .) Let $S^1$ be the unit circle in the complex plain. Let $d:S^1\times S^1\to\mathbb{R}$ be the distance function given by the arc length. Let $\theta\in S^1$ be an element of infinite order, that is $\theta^n\neq 1$ for any $n\neq 0$.  Define $J_N$ to be the set of those pairs $(i,j)$, such that $1\leq i,j\leq N$ and the smaller open arc between $\theta^i$ and $\theta^j$ does not contain any other $\theta^k$, $1\leq k\leq N$. I would like to know what is the behavior of the ratio  \begin{equation} \frac{\max_{(i,j)\in J_N} d(\theta^i,\theta^j)}{\min_{(i,j)\in J_N}d(\theta^i,\theta^j)} \end{equation} when $N$ goes to $\infty$. I would like it to be bounded above and below, but any information would be useful.",,"['complex-analysis', 'analysis', 'number-theory', 'analytic-number-theory']"
12,Local minimum global,Local minimum global,,"Let $f:(a,b)\to\Bbb R$ be continuous. Assume that $f$ has a local minimum at some point $x_0$. Further assume that this is the only point where $f$ has a local extremum. Does it follow that $f$ has a global minimum at $x_0$. Thanks","Let $f:(a,b)\to\Bbb R$ be continuous. Assume that $f$ has a local minimum at some point $x_0$. Further assume that this is the only point where $f$ has a local extremum. Does it follow that $f$ has a global minimum at $x_0$. Thanks",,"['real-analysis', 'analysis', 'functions', 'optimization']"
13,"Bounding a Bilinear Map $||A(v,w)||\leq M||v||||w||$",Bounding a Bilinear Map,"||A(v,w)||\leq M||v||||w||","In a normed vector space I know that for a linear map $L:E\rightarrow F$ that there exists an $M\in \mathbb{R}$ such that $\forall x\in E$ $||L(x)||\leq M||x||$. The proof is this is quite straightforward but I am unsure how to generalize to a bilinear map. Let $A:E\times F\rightarrow G$. I would like to show that $\exists N \in \mathbb{R}$ such that $\forall x\in E,y\in G$ that $||A(x)(y)||\leq N||x||||y||$. I am unsure if my generalization is permissible: Attempt: $A(x):F\rightarrow G$, $A(x)$ linear, so $\exists B$ such that $||A(x)(y)||\leq B||y||$. Let $N=\frac{B}{||x||}$, it then follows that $||A(x,y)||\leq N||x||||y||$ but this isn't right because N shouldn't depend on $||x||$. Edit: I guess i should say that I am considering only finite dimensional spaces.","In a normed vector space I know that for a linear map $L:E\rightarrow F$ that there exists an $M\in \mathbb{R}$ such that $\forall x\in E$ $||L(x)||\leq M||x||$. The proof is this is quite straightforward but I am unsure how to generalize to a bilinear map. Let $A:E\times F\rightarrow G$. I would like to show that $\exists N \in \mathbb{R}$ such that $\forall x\in E,y\in G$ that $||A(x)(y)||\leq N||x||||y||$. I am unsure if my generalization is permissible: Attempt: $A(x):F\rightarrow G$, $A(x)$ linear, so $\exists B$ such that $||A(x)(y)||\leq B||y||$. Let $N=\frac{B}{||x||}$, it then follows that $||A(x,y)||\leq N||x||||y||$ but this isn't right because N shouldn't depend on $||x||$. Edit: I guess i should say that I am considering only finite dimensional spaces.",,['analysis']
14,Is this a decomposition of the same function?,Is this a decomposition of the same function?,,"Let's say we have some integral, such that for a particular function $f: \mathbb{R}^n \rightarrow \mathbb{R}$  $$\int_{\mathbb{R}^{n-m}} \int_{\mathbb{R}^m}f^+ - \int_{\mathbb{R}^{n-m}}\int_{\mathbb{R}^m}f^-$$ with $f^+:=max(f,0),f^-:=max(-f,0)$ and the following integrals are defined. How do I see that this is the same as:  $$\int_{\mathbb{R}^{n-m}} (\int_{\mathbb{R}^m}f^+-\int_{\mathbb{R}^m}f^-)^+ - \int_{\mathbb{R}^{n-m}}(\int_{\mathbb{R}^m}f^+-\int_{\mathbb{R}^m}f^-)^-$$ if I know that my integral is invariant under different decompositions. So my question is: Is there an easy argument, why both decompositions define the same function and both integrals consequently coincide? ( The outer plus and minus in the second integral are defined in the same way as they were earlier). If anything is unclear, please let me know.","Let's say we have some integral, such that for a particular function $f: \mathbb{R}^n \rightarrow \mathbb{R}$  $$\int_{\mathbb{R}^{n-m}} \int_{\mathbb{R}^m}f^+ - \int_{\mathbb{R}^{n-m}}\int_{\mathbb{R}^m}f^-$$ with $f^+:=max(f,0),f^-:=max(-f,0)$ and the following integrals are defined. How do I see that this is the same as:  $$\int_{\mathbb{R}^{n-m}} (\int_{\mathbb{R}^m}f^+-\int_{\mathbb{R}^m}f^-)^+ - \int_{\mathbb{R}^{n-m}}(\int_{\mathbb{R}^m}f^+-\int_{\mathbb{R}^m}f^-)^-$$ if I know that my integral is invariant under different decompositions. So my question is: Is there an easy argument, why both decompositions define the same function and both integrals consequently coincide? ( The outer plus and minus in the second integral are defined in the same way as they were earlier). If anything is unclear, please let me know.",,"['real-analysis', 'integration']"
15,Crosscap function in $\mathbb{R}^4$ - and how to show it is proper?,Crosscap function in  - and how to show it is proper?,\mathbb{R}^4,"I found the Cross-cap function in $\mathbb{R}^3$ as follows: $$f(x,y,z)=(yz,2xy,x^2-y^2),$$ My questions are (I couldn't show any progress for Q1,2.I have thought hard but had no clue): Q1: Is there a form for crosscap in $\mathbb{R}^4$? Q2: Is there a way to show this is proper (the preimage of a compact space is compact)? Q3: Is there a way to show this is injective? All my attempts are for linear functions, so none of them work. I tried to show $f(x,y,z) = 0 \Rightarrow x = y = z = 0$. But I get $x=y=0$, with no constraint on $z$. I tried to show $f(x_1,y_1,z_1) = f(x_2,y_2,z_2) \Rightarrow (x_1,y_1,z_1) = (x_2,y_2,z_2)$ but made no progress. I tried to solve $f(x,y,z) = (a,b,c,d)$ for $x,y,z$ on mathematica , and the result is scary.","I found the Cross-cap function in $\mathbb{R}^3$ as follows: $$f(x,y,z)=(yz,2xy,x^2-y^2),$$ My questions are (I couldn't show any progress for Q1,2.I have thought hard but had no clue): Q1: Is there a form for crosscap in $\mathbb{R}^4$? Q2: Is there a way to show this is proper (the preimage of a compact space is compact)? Q3: Is there a way to show this is injective? All my attempts are for linear functions, so none of them work. I tried to show $f(x,y,z) = 0 \Rightarrow x = y = z = 0$. But I get $x=y=0$, with no constraint on $z$. I tried to show $f(x_1,y_1,z_1) = f(x_2,y_2,z_2) \Rightarrow (x_1,y_1,z_1) = (x_2,y_2,z_2)$ but made no progress. I tried to solve $f(x,y,z) = (a,b,c,d)$ for $x,y,z$ on mathematica , and the result is scary.",,"['general-topology', 'geometry', 'analysis', 'differential-topology']"
16,How to calculate $\delta(x^4-\alpha^4)$?,How to calculate ?,\delta(x^4-\alpha^4),Does the following equality hold? \begin{equation} \delta(x^4-\alpha^4)=\frac{1}{4\vert \alpha \vert} \left[ \delta(x-\alpha)+ \delta(x+\alpha) + \delta(x+i\alpha) + \delta(x-i\alpha)\right]. \end{equation},Does the following equality hold? \begin{equation} \delta(x^4-\alpha^4)=\frac{1}{4\vert \alpha \vert} \left[ \delta(x-\alpha)+ \delta(x+\alpha) + \delta(x+i\alpha) + \delta(x-i\alpha)\right]. \end{equation},,"['calculus', 'integration', 'analysis']"
17,Hardy-Littlewood maximal function weak type estimate,Hardy-Littlewood maximal function weak type estimate,,"Show that if $f\in L^1(\mathbb{R}^d)$ and $E\subset \mathbb{R}^d$ has finite measure, then for any $0<q<1$, $$\int_E |f^{*}(x)|^q dx\leq C_q|E|^{1-q}||f||_{L^1(\mathbb{R}^d)}^{q}$$ where $C_q$ is a positive constant depending only on $q$ and $d$. Here the function $f^*(x)=\sup_{x\in B}\frac{1}{|B|}\int_B |f(y)|dy$ is the Hardy-Littlewood maximal function. Notes It seems to me the weak type estimate $\forall \alpha>0,\enspace |\{x: f^*(x)>\alpha\}|\leq \frac{3^d}{\alpha}||f||_{L^1(\mathbb{R}^d)}$ is of great use but I am having trouble putting this to any use. Any help is appreciated.","Show that if $f\in L^1(\mathbb{R}^d)$ and $E\subset \mathbb{R}^d$ has finite measure, then for any $0<q<1$, $$\int_E |f^{*}(x)|^q dx\leq C_q|E|^{1-q}||f||_{L^1(\mathbb{R}^d)}^{q}$$ where $C_q$ is a positive constant depending only on $q$ and $d$. Here the function $f^*(x)=\sup_{x\in B}\frac{1}{|B|}\int_B |f(y)|dy$ is the Hardy-Littlewood maximal function. Notes It seems to me the weak type estimate $\forall \alpha>0,\enspace |\{x: f^*(x)>\alpha\}|\leq \frac{3^d}{\alpha}||f||_{L^1(\mathbb{R}^d)}$ is of great use but I am having trouble putting this to any use. Any help is appreciated.",,"['analysis', 'measure-theory', 'lebesgue-integral']"
18,Can the sigmoid function approximate any function (or relation) where $0<y<1$,Can the sigmoid function approximate any function (or relation) where,0<y<1,"I'm studying Machine Learning and Artificial Neural Networks. Some basic principles of Machine Learning are linear regression, multivariate linear regression, and nonlinear regression. The last of these, nonlinear regression, involves fitting a curve to a set of data. This curve can be any shape (it might fail the vertical and horizontal line test). In the tutorial I am following, Stanford Machine Learning by Andrew Ng, he says that the sigmoid function is used for non linear regression, and the more polynomial terms used the more complex the curve can be. I'll give an example: $$y=\frac{1}{1+e^{-\left(100x^2-100y\right)}}$$ approximates the polynomial: $$y=x^2$$ The sigmoid function can also approximate curves that are relations instead of functions. For example: $$y=\space \frac{1}{1+e^{-\left(5+1x\space -3y\space +4xy\space -1yx^2\space -2yx^3-1xy^2\space +30y^3\right)}}$$ But if I use enough polynomial terms can this function approximate any function or relation? It seems like this needs to be true otherwise the sigmoid function would be a bad choice of hypothesis for nonlinear regression.","I'm studying Machine Learning and Artificial Neural Networks. Some basic principles of Machine Learning are linear regression, multivariate linear regression, and nonlinear regression. The last of these, nonlinear regression, involves fitting a curve to a set of data. This curve can be any shape (it might fail the vertical and horizontal line test). In the tutorial I am following, Stanford Machine Learning by Andrew Ng, he says that the sigmoid function is used for non linear regression, and the more polynomial terms used the more complex the curve can be. I'll give an example: $$y=\frac{1}{1+e^{-\left(100x^2-100y\right)}}$$ approximates the polynomial: $$y=x^2$$ The sigmoid function can also approximate curves that are relations instead of functions. For example: $$y=\space \frac{1}{1+e^{-\left(5+1x\space -3y\space +4xy\space -1yx^2\space -2yx^3-1xy^2\space +30y^3\right)}}$$ But if I use enough polynomial terms can this function approximate any function or relation? It seems like this needs to be true otherwise the sigmoid function would be a bad choice of hypothesis for nonlinear regression.",,"['analysis', 'functions', 'regression']"
19,munkres analysis integration question,munkres analysis integration question,,"Let $[0,1]^2 = [0,1] \times [0,1]$. Let $f: [0,1]^2 \to \mathbb{R}$ be defined by setting $f(x,y)=0$ if $y \neq x$, and $f(x,y) = 1$ if $y=x$. Show that $f$ is integrable over $[0,1]^2$.","Let $[0,1]^2 = [0,1] \times [0,1]$. Let $f: [0,1]^2 \to \mathbb{R}$ be defined by setting $f(x,y)=0$ if $y \neq x$, and $f(x,y) = 1$ if $y=x$. Show that $f$ is integrable over $[0,1]^2$.",,"['integration', 'analysis', 'measure-theory']"
20,Strictly convex self-concordant function,Strictly convex self-concordant function,,"Some definitions: A function $f:R^n\rightarrow R$ is convex[strictly convex] if for every $\lambda\in[0,1]$ [$\lambda\in(0,1)$] and for every $x,y$ [$x\neq y$] in $R^n$ we have $f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$ $\qquad$ [$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$] A convex function $f:R\rightarrow R$ is $ self-concordant$ if $|f^{\prime\prime\prime}(x)|\leq 2f^{\prime\prime}(x)^{\frac{3}{2}}$ for every $x\in domf$. We say a function $f:R^n\rightarrow R$ is $self-concordant$ if it is $self-concordant$ along every line in its domain, i.e., if the function $\tilde{f}(t) = f(x + tv)$ is a self-concordant function of t for all $x\in domf$ and for all $v$. Question: Let $f:R^n\rightarrow R$ be a strictly convex $self-concordant$ function. Suppose $\theta(x)=:(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x))^{\frac{1}{2}}<1$, and define $x^+=x-\nabla^2f(x)^{-1}\nabla f(x)$. Prove that $\theta(x^+)\leq \frac{\theta(x)^2}{(1-\theta(x))^2}$ with all details.(Where the $\nabla^2$ is Hessian and $\nabla$ is gradient)","Some definitions: A function $f:R^n\rightarrow R$ is convex[strictly convex] if for every $\lambda\in[0,1]$ [$\lambda\in(0,1)$] and for every $x,y$ [$x\neq y$] in $R^n$ we have $f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$ $\qquad$ [$f(\lambda x+(1-\lambda)y)<\lambda f(x)+(1-\lambda)f(y)$] A convex function $f:R\rightarrow R$ is $ self-concordant$ if $|f^{\prime\prime\prime}(x)|\leq 2f^{\prime\prime}(x)^{\frac{3}{2}}$ for every $x\in domf$. We say a function $f:R^n\rightarrow R$ is $self-concordant$ if it is $self-concordant$ along every line in its domain, i.e., if the function $\tilde{f}(t) = f(x + tv)$ is a self-concordant function of t for all $x\in domf$ and for all $v$. Question: Let $f:R^n\rightarrow R$ be a strictly convex $self-concordant$ function. Suppose $\theta(x)=:(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x))^{\frac{1}{2}}<1$, and define $x^+=x-\nabla^2f(x)^{-1}\nabla f(x)$. Prove that $\theta(x^+)\leq \frac{\theta(x)^2}{(1-\theta(x))^2}$ with all details.(Where the $\nabla^2$ is Hessian and $\nabla$ is gradient)",,"['calculus', 'analysis', 'multivariable-calculus', 'optimization', 'convex-optimization']"
21,Integral of the product of the Gamma function and the Confluent Hypergeometric function,Integral of the product of the Gamma function and the Confluent Hypergeometric function,,"This question was posted previously on stats.SE. The characteristic function of the Fisher$(1,\alpha)$ distribution is: $$C(t)=\frac{\Gamma \left(\frac{\alpha +1}{2}\right) U\left(\frac{1}{2},1-\frac{\alpha }{2},-i t \alpha \right)}{\Gamma \left(\frac{\alpha }{2}\right)}$$ What is the Fourier transform, $\mathcal {F} _ {t,x}^{-1}$ , of $\lim_{n \to \infty} (C(t))^n$ Where $U$ is the confluent hypergeometric function. I did some simplification in the hope of making things simpler: $$C(t) = \frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2}\right) U\left(\frac{1}{2},1-\frac{\alpha }{2},-i t \alpha \right)}{\Gamma \left(\frac{\alpha }{2}\right)}$$ $$= \frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2} \right) \Gamma \left( 1-\frac{\alpha}{2} \right) }{\Gamma \left(\frac{\alpha }{2}\right) \Gamma \left( 1 -\frac{\alpha}{2} - \frac{1}{2} \right) \Gamma \left( \frac{1}{2} \right)}.\int^1_0 e^{-i t^2} t^{-\frac{1}{2}}  (1-t)^{-\frac{\alpha }{2}-\frac{1}{2}}dt \;\;(1)$$ Using the properties of the gamma function and this post here ` Rewrite $\Gamma(-z)$ in terms of $\Gamma(z)$ I was able to simplify (If my math is correct) ,  $$\frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2} \right) \Gamma \left( 1-\frac{\alpha}{2} \right) }{\Gamma \left(\frac{\alpha }{2}\right) \Gamma \left( 1 -\frac{\alpha}{2} - \frac{1}{2} \right) \Gamma \left( \frac{1}{2} \right)} = \frac{-2 *\pi * \sin(\pi(1+\alpha)) (\Gamma \left( \alpha \right) )^2}{\sin(\pi(1+\frac{\alpha}{2}))}$$ Combining this with (1), We get a product of the gamma function with the integral in (1). Any ideas?","This question was posted previously on stats.SE. The characteristic function of the Fisher$(1,\alpha)$ distribution is: $$C(t)=\frac{\Gamma \left(\frac{\alpha +1}{2}\right) U\left(\frac{1}{2},1-\frac{\alpha }{2},-i t \alpha \right)}{\Gamma \left(\frac{\alpha }{2}\right)}$$ What is the Fourier transform, $\mathcal {F} _ {t,x}^{-1}$ , of $\lim_{n \to \infty} (C(t))^n$ Where $U$ is the confluent hypergeometric function. I did some simplification in the hope of making things simpler: $$C(t) = \frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2}\right) U\left(\frac{1}{2},1-\frac{\alpha }{2},-i t \alpha \right)}{\Gamma \left(\frac{\alpha }{2}\right)}$$ $$= \frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2} \right) \Gamma \left( 1-\frac{\alpha}{2} \right) }{\Gamma \left(\frac{\alpha }{2}\right) \Gamma \left( 1 -\frac{\alpha}{2} - \frac{1}{2} \right) \Gamma \left( \frac{1}{2} \right)}.\int^1_0 e^{-i t^2} t^{-\frac{1}{2}}  (1-t)^{-\frac{\alpha }{2}-\frac{1}{2}}dt \;\;(1)$$ Using the properties of the gamma function and this post here ` Rewrite $\Gamma(-z)$ in terms of $\Gamma(z)$ I was able to simplify (If my math is correct) ,  $$\frac{\Gamma \left(\frac{\alpha}{2}+ \frac{1}{2} \right) \Gamma \left( 1-\frac{\alpha}{2} \right) }{\Gamma \left(\frac{\alpha }{2}\right) \Gamma \left( 1 -\frac{\alpha}{2} - \frac{1}{2} \right) \Gamma \left( \frac{1}{2} \right)} = \frac{-2 *\pi * \sin(\pi(1+\alpha)) (\Gamma \left( \alpha \right) )^2}{\sin(\pi(1+\frac{\alpha}{2}))}$$ Combining this with (1), We get a product of the gamma function with the integral in (1). Any ideas?",,"['real-analysis', 'probability', 'analysis', 'gamma-function', 'hypergeometric-function']"
22,Growth rate of $\exp(log^{a}(x))$ slower then any power of $x$.,Growth rate of  slower then any power of .,\exp(log^{a}(x)) x,So I'm trying to show that for $0<a<1$ and for $\epsilon >0$ that $\exp((\log x)^{a})=\mathcal{O}(x^{\epsilon}).$ So this amounts to showing that $\limsup_{x\rightarrow\infty}\dfrac{\exp((\log x)^{a})}{x^{\epsilon}}<\infty$. I tryed to use l'hopital's on $\lim_{x\rightarrow 0}\dfrac{\exp((-\log x)^{a})}{1/x^{\epsilon}}$ but that did not seem to work any ideas on how to do this?,So I'm trying to show that for $0<a<1$ and for $\epsilon >0$ that $\exp((\log x)^{a})=\mathcal{O}(x^{\epsilon}).$ So this amounts to showing that $\limsup_{x\rightarrow\infty}\dfrac{\exp((\log x)^{a})}{x^{\epsilon}}<\infty$. I tryed to use l'hopital's on $\lim_{x\rightarrow 0}\dfrac{\exp((-\log x)^{a})}{1/x^{\epsilon}}$ but that did not seem to work any ideas on how to do this?,,"['analysis', 'asymptotics']"
23,"Premetrics, where are they useful?","Premetrics, where are they useful?",,"Wikipedia defines a premetric as a function $d : X\times X \to \mathbb R$ such that $d(x,y) \ge 0$ $d(x,x) = 0$. For me these axioms are so weak that I am wondering where they are used, do you know any examples? I can not find anything, and when I look for the term premetric I just found references to pseudometrics.","Wikipedia defines a premetric as a function $d : X\times X \to \mathbb R$ such that $d(x,y) \ge 0$ $d(x,x) = 0$. For me these axioms are so weak that I am wondering where they are used, do you know any examples? I can not find anything, and when I look for the term premetric I just found references to pseudometrics.",,"['functional-analysis', 'analysis', 'metric-spaces']"
24,Deducing that polynomials span,Deducing that polynomials span,,"Let us say that we are dealing with a countable family of polynomials with real coefficients in $n$ indeterminates that commute.  Are there any known/common nice systematic ways to tell if their span is the whole space of polynomials when they have a nice generating function?  What about the closure of the span in the supremum norm restricted to our polynomial space defined on a full-dimensional set such as $[0, 1]^n$? (I am not restricting only to methods that use the generating function.  But that is just one possibility.)","Let us say that we are dealing with a countable family of polynomials with real coefficients in $n$ indeterminates that commute.  Are there any known/common nice systematic ways to tell if their span is the whole space of polynomials when they have a nice generating function?  What about the closure of the span in the supremum norm restricted to our polynomial space defined on a full-dimensional set such as $[0, 1]^n$? (I am not restricting only to methods that use the generating function.  But that is just one possibility.)",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'analysis']"
25,Properties of the first eigenvalue of the $p$-Laplace operator,Properties of the first eigenvalue of the -Laplace operator,p,"Let $\Omega\subset\mathbb{R}^n$ be a bounded domain and $p\in (1,\infty)$, $p\neq 2$. Consider the usual Sobolev space $W_0^{1,p}(\Omega)$ and its dual $W^{-1,p'}(\Omega)$, where $1/p+1/p'=1$. Define $\lambda_1>0$ by $$\tag{1}\lambda_1=\inf_{u\in W_0^{1,p}(\Omega)}\frac{\|u\|_{1,p}^p}{\|u\|_p^p}$$ $\lambda_1$ is the first eigenvalue associated with the problem $$-\Delta _p u=\lambda |u|^{p-2}u,\ u\in W_0^{1,p}(\Omega),\tag{2}$$ where the above equation is understood as an equation in $W^{-1,p'}(\Omega)$ and $\Delta_p u=|\nabla u|^{p-2}\nabla u$. As you can see in these notes from Peral, the infimum in $(1)$ is achieved by, let's say, $u_1\in W_0^{1,p}(\Omega)$. Moreover, $u_1$ does not change sign, i.e. we can choose it to be strictly positive in $\Omega$ and if $v$ is another minimum point of $(1)$, we have that $v=\alpha u_1$, $\alpha\in \mathbb{R}$ ($\lambda_1$ is simple). I would like to note that $(1)$ can also be viewed as a problem of minimization on the Banach manifold $$S_p=\{u\in W_0^{1,p}(\Omega):\ \|u\|_p=1\}$$ i.e. $$\lambda_1=\inf_{u\in W_0^{1,p}(\Omega),\ u\in S_p}\|u\|^p_{1,p}\tag{3}$$ We have from the simplicity of $\lambda_1$ that there are two eigenvectors of $(2)$ in $S_p$. Fix one of them and let's call it $u_1$. The tangent space in the point $u_1$ in $S_p$ can be indetified as $$T=T_{u_1}S_p=\left\{v\in W_0^{1,p}:\ \int_\Omega |u_1|^{p-2}u_1 v=0\right\}$$ Let  $T_1=\{v\in T:\ \|v\|_{1,p}=1\}$. Consider the function $F:\mathbb{R}\times T_1\to \mathbb{R}$ defined by $$F(t,v)=\frac{\int_\Omega|\nabla (u_1+tv)|^p}{\int_\Omega|u_1+tv|^p}=\frac{\|u_1+tv\|_{1,p}^p}{\|u_1+tv\|_p^p}=\left\|\frac{u_1+tv}{\|u_1+tv\|_p}\right\|_{1,p}^p$$ I am trying to prove  that there exist $\epsilon>0$ such that for all fixed $v\in T_1$ the function $f(t,v)$ is strictly convex for $t\in (-\epsilon,\epsilon)$. Geometrically, I am taking curves in $S_p$ which starts in $u_1$. If someone has  an idea, maybe some trick inequality or even a counter example, I would be grateful.","Let $\Omega\subset\mathbb{R}^n$ be a bounded domain and $p\in (1,\infty)$, $p\neq 2$. Consider the usual Sobolev space $W_0^{1,p}(\Omega)$ and its dual $W^{-1,p'}(\Omega)$, where $1/p+1/p'=1$. Define $\lambda_1>0$ by $$\tag{1}\lambda_1=\inf_{u\in W_0^{1,p}(\Omega)}\frac{\|u\|_{1,p}^p}{\|u\|_p^p}$$ $\lambda_1$ is the first eigenvalue associated with the problem $$-\Delta _p u=\lambda |u|^{p-2}u,\ u\in W_0^{1,p}(\Omega),\tag{2}$$ where the above equation is understood as an equation in $W^{-1,p'}(\Omega)$ and $\Delta_p u=|\nabla u|^{p-2}\nabla u$. As you can see in these notes from Peral, the infimum in $(1)$ is achieved by, let's say, $u_1\in W_0^{1,p}(\Omega)$. Moreover, $u_1$ does not change sign, i.e. we can choose it to be strictly positive in $\Omega$ and if $v$ is another minimum point of $(1)$, we have that $v=\alpha u_1$, $\alpha\in \mathbb{R}$ ($\lambda_1$ is simple). I would like to note that $(1)$ can also be viewed as a problem of minimization on the Banach manifold $$S_p=\{u\in W_0^{1,p}(\Omega):\ \|u\|_p=1\}$$ i.e. $$\lambda_1=\inf_{u\in W_0^{1,p}(\Omega),\ u\in S_p}\|u\|^p_{1,p}\tag{3}$$ We have from the simplicity of $\lambda_1$ that there are two eigenvectors of $(2)$ in $S_p$. Fix one of them and let's call it $u_1$. The tangent space in the point $u_1$ in $S_p$ can be indetified as $$T=T_{u_1}S_p=\left\{v\in W_0^{1,p}:\ \int_\Omega |u_1|^{p-2}u_1 v=0\right\}$$ Let  $T_1=\{v\in T:\ \|v\|_{1,p}=1\}$. Consider the function $F:\mathbb{R}\times T_1\to \mathbb{R}$ defined by $$F(t,v)=\frac{\int_\Omega|\nabla (u_1+tv)|^p}{\int_\Omega|u_1+tv|^p}=\frac{\|u_1+tv\|_{1,p}^p}{\|u_1+tv\|_p^p}=\left\|\frac{u_1+tv}{\|u_1+tv\|_p}\right\|_{1,p}^p$$ I am trying to prove  that there exist $\epsilon>0$ such that for all fixed $v\in T_1$ the function $f(t,v)$ is strictly convex for $t\in (-\epsilon,\epsilon)$. Geometrically, I am taking curves in $S_p$ which starts in $u_1$. If someone has  an idea, maybe some trick inequality or even a counter example, I would be grateful.",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
26,"Complex analysis visualization (Cauchy Theorem, Residue Theorem)?","Complex analysis visualization (Cauchy Theorem, Residue Theorem)?",,I usually think of complex functions on the complex plane like vector fields. So basically what I have problems with is visualizing firstly Holomorphic functions. I have also read and successfully understood the proofs of the Cauchy Theorem over cycles as well as the Residue Theorem (Rudin) and some of their applications. I would like to know if any of you has a way of visualizing or making these theorems more intuitive as to why they happen.,I usually think of complex functions on the complex plane like vector fields. So basically what I have problems with is visualizing firstly Holomorphic functions. I have also read and successfully understood the proofs of the Cauchy Theorem over cycles as well as the Residue Theorem (Rudin) and some of their applications. I would like to know if any of you has a way of visualizing or making these theorems more intuitive as to why they happen.,,"['complex-analysis', 'analysis', 'visualization']"
27,Bounded functions and infimum/supremum,Bounded functions and infimum/supremum,,"Question from my homework: A function $f \colon S \to \mathbb{R}$ is said to be decreasing on the domain $S \subset \mathbb{R}$ if for every $x, y \in S$ with $y > x$, we have $f(y) < f(x)$.   Show that if $f \colon \mathbb{R} \to \mathbb{R}$ is decreasing and bounded on $\mathbb{R}$, then the limits $\lim_{x \to \infty} f(x)$ and $\lim_{x \to -\infty} f(x)$ both exist. Caution: While this claim is clearly related to the theorem that bounded monotone sequences always converge, it is not an immediate corollary of it.   You will have to use the definition of the limit directly. (Original picture of the problem here .) My proof: Let $ S = \{f(x) | x \in \mathbb{R} \} $, since $f$ is bounded this set has both an infimum and supremum. (Is this true? I tried to prove it, but have no idea as it seems fairly obvious.) Now, I claim that $\lim_{x\to \infty} f(x) = \inf(S) = L$. By definition, given $\epsilon > 0$, there exists $K \in \mathbb{R}$ such that $x>K \implies |f(x) - L| < \epsilon$. Now since $f(x) \geq L$ by definition of the infimum, choose $x_1$ such that $f(x_1) < L + \epsilon$ for $x_1 > K$. But since $f$ is a decreasing function we have $f(x) \leq f(x_1)$, and thus $$   L - \epsilon < L \leq f(x) \leq f(x_1) < L + \epsilon $$ which proves our claim. I use a similar argument for the supremum. Could anyone help me out on making this more rigorous as I feel as though I've skipped some steps.","Question from my homework: A function $f \colon S \to \mathbb{R}$ is said to be decreasing on the domain $S \subset \mathbb{R}$ if for every $x, y \in S$ with $y > x$, we have $f(y) < f(x)$.   Show that if $f \colon \mathbb{R} \to \mathbb{R}$ is decreasing and bounded on $\mathbb{R}$, then the limits $\lim_{x \to \infty} f(x)$ and $\lim_{x \to -\infty} f(x)$ both exist. Caution: While this claim is clearly related to the theorem that bounded monotone sequences always converge, it is not an immediate corollary of it.   You will have to use the definition of the limit directly. (Original picture of the problem here .) My proof: Let $ S = \{f(x) | x \in \mathbb{R} \} $, since $f$ is bounded this set has both an infimum and supremum. (Is this true? I tried to prove it, but have no idea as it seems fairly obvious.) Now, I claim that $\lim_{x\to \infty} f(x) = \inf(S) = L$. By definition, given $\epsilon > 0$, there exists $K \in \mathbb{R}$ such that $x>K \implies |f(x) - L| < \epsilon$. Now since $f(x) \geq L$ by definition of the infimum, choose $x_1$ such that $f(x_1) < L + \epsilon$ for $x_1 > K$. But since $f$ is a decreasing function we have $f(x) \leq f(x_1)$, and thus $$   L - \epsilon < L \leq f(x) \leq f(x_1) < L + \epsilon $$ which proves our claim. I use a similar argument for the supremum. Could anyone help me out on making this more rigorous as I feel as though I've skipped some steps.",,"['analysis', 'supremum-and-infimum']"
28,Second derivative,Second derivative,,"I have this functional on $H^1_0$ defined by $J(u)=\frac12||u||^2-\int_0^1 F(t,u(t)) dt $ where $F(t,u(t))=\int_0^u f(t,s) ds $ and i have  $J'(u)h= \int_0^1u'(t)h'(t) dt - \int_0^1f(t,u(t)) h(t) dt $ How to do to find  $J''(u)(h,k)$ ?? Thank you","I have this functional on $H^1_0$ defined by $J(u)=\frac12||u||^2-\int_0^1 F(t,u(t)) dt $ where $F(t,u(t))=\int_0^u f(t,s) ds $ and i have  $J'(u)h= \int_0^1u'(t)h'(t) dt - \int_0^1f(t,u(t)) h(t) dt $ How to do to find  $J''(u)(h,k)$ ?? Thank you",,"['analysis', 'derivatives']"
29,Is there a gap in Serre's proof of inverse function theorem?,Is there a gap in Serre's proof of inverse function theorem?,,"On page 73 of 'Lie algebras and Lie groups', Serre proves the inverse function theorem for complete fields. I would like to have some clarification about the following point. Let $K$ be a complete ultrametric field. Let $f = (f_1,\dots,f_n)$ be an $n$-uple of formal power series in $n$ variables (i.e. $f_i \in K[[X_1, \dots,X_n]]$). Suppose that $f$ is convergent and that $D_0 f$ is invertible. Serre proves that there exists $g=(g_1,\dots,g_n)$ a formal power series convergent satisfying : $f \circ g(\underline{T}) = \underline{T} = (T_1,\dots,T_n)$ as formal power series (and also satisfying $g \circ f(\underline{T}) = \underline{T} = (T_1,\dots,T_n)$ if I am not mistaken). Serre ends the proof here. Why does this prove the theorem ? To conclude, we have to prove that there exist two opens $U,V$ such that $f$ maps U into $V$ and $g$ maps $V$ into $U$. But I can't see why these opens exist. It seems to me that we have to prove $f$ is an open map.","On page 73 of 'Lie algebras and Lie groups', Serre proves the inverse function theorem for complete fields. I would like to have some clarification about the following point. Let $K$ be a complete ultrametric field. Let $f = (f_1,\dots,f_n)$ be an $n$-uple of formal power series in $n$ variables (i.e. $f_i \in K[[X_1, \dots,X_n]]$). Suppose that $f$ is convergent and that $D_0 f$ is invertible. Serre proves that there exists $g=(g_1,\dots,g_n)$ a formal power series convergent satisfying : $f \circ g(\underline{T}) = \underline{T} = (T_1,\dots,T_n)$ as formal power series (and also satisfying $g \circ f(\underline{T}) = \underline{T} = (T_1,\dots,T_n)$ if I am not mistaken). Serre ends the proof here. Why does this prove the theorem ? To conclude, we have to prove that there exist two opens $U,V$ such that $f$ maps U into $V$ and $g$ maps $V$ into $U$. But I can't see why these opens exist. It seems to me that we have to prove $f$ is an open map.",,"['analysis', 'power-series', 'lie-groups']"
30,conjecture regarding the cosine fixed point,conjecture regarding the cosine fixed point,,"context/motivation if the angle on a calculator is set to radians, then it is very easy to demonstrate that iteration of $cos x$ (for arbitrary initial x) converges - simply keep pressing the $cos$ button! this unique fixed point $\alpha$ might reasonably be expected to be a transcendental number. (perhaps the answer to that is already known?). the conjecture outlined here suggests that $\alpha$ is an upper bound for a whole family of numbers defined in terms of iteration of particular sequences of cosine and sine functions. since these mixed iterations give rise to limit cycles rather than fixed points, we use the Cesaro mean to give a characteristic number for each cycle. the cycles i initially considered are easily defined in terms of the periodic binary representation of fractions whose denominator is not a power of 2. however whilst these are the only numbers corresponding to the sequences of sine and cosine that converge towards stable orbits, it seems likely that the periodicity itself is not the key factor ensuring Cesaro convergence, but that this is achieved due to a weaker asymptotic density condition which is necessary but not sufficient for periodicity. i apologise for any mistakes or lack of clarity in my (necessarily brief) presentation. the basic idea is simpler than may appear from a first glimpse of the definitions. preliminary definitions let $I$ be the closed unit interval $[0,1]$ so that the sine and cosine functions restrict to injective maps of $I$ into itself. for integers $n \gt 0$ define $\beta_n:I \rightarrow \{0,1\}$ to be the $n^{th}$ binary digit of its argument, so $\beta_n(\lambda)=\lfloor 2^n\lambda \rfloor$ now define $\psi:\{0,1\} \times I \rightarrow I $ by: $$ \psi(0,\theta) = cos \theta \\  \psi(1,\theta) = sin \theta $$ every $\lambda \in I$ can be associated with a function  $\Psi_{\lambda}:I \rightarrow I^{\omega}$ which generates a sequence in $I$, i.e. $$ \forall \theta \in I, \Psi_{\lambda}(\theta) = \{\theta_n\}_{n=0,1,2,...} $$ with $\theta_0=\theta$ and for $n \ge 0$ $$ \theta_{n+1} = \psi(\beta_{n+1}(\lambda),\theta_n) $$ let us now call $\lambda \in I$ a $\beta$-number if an asymptotic density of $1$s in its binary representation exists, i.e. if the sequence $\{\beta_n(\lambda)\}$ has a Cesaro-mean. this mean, if it exists, we may denote by $\beta^*(\lambda)$ let us also define $\alpha$ as the unique fixed point in I of the cosine function, i.e. $$cos \; \alpha = \alpha$$ conjecture 1. $\forall \theta \in I$ the sequence $\Psi_{\lambda}(\theta)$ has a Cesaro mean if and only if $\lambda$ is a $\beta$-number, and in this case the Cesaro mean is independent of $\theta$ and may be denoted $\Psi_{\lambda}^*$ 2. if the sequence $\Psi_{\lambda}(\theta)$ has Cesaro mean, then this mean is equal to $\alpha$ if and only if $\beta^*(\lambda)=0$ 3. for any $\beta$-number $\lambda$ if $\beta^*(\lambda) \gt 0$ then  $\Psi_{\lambda}^*\lt \alpha$","context/motivation if the angle on a calculator is set to radians, then it is very easy to demonstrate that iteration of $cos x$ (for arbitrary initial x) converges - simply keep pressing the $cos$ button! this unique fixed point $\alpha$ might reasonably be expected to be a transcendental number. (perhaps the answer to that is already known?). the conjecture outlined here suggests that $\alpha$ is an upper bound for a whole family of numbers defined in terms of iteration of particular sequences of cosine and sine functions. since these mixed iterations give rise to limit cycles rather than fixed points, we use the Cesaro mean to give a characteristic number for each cycle. the cycles i initially considered are easily defined in terms of the periodic binary representation of fractions whose denominator is not a power of 2. however whilst these are the only numbers corresponding to the sequences of sine and cosine that converge towards stable orbits, it seems likely that the periodicity itself is not the key factor ensuring Cesaro convergence, but that this is achieved due to a weaker asymptotic density condition which is necessary but not sufficient for periodicity. i apologise for any mistakes or lack of clarity in my (necessarily brief) presentation. the basic idea is simpler than may appear from a first glimpse of the definitions. preliminary definitions let $I$ be the closed unit interval $[0,1]$ so that the sine and cosine functions restrict to injective maps of $I$ into itself. for integers $n \gt 0$ define $\beta_n:I \rightarrow \{0,1\}$ to be the $n^{th}$ binary digit of its argument, so $\beta_n(\lambda)=\lfloor 2^n\lambda \rfloor$ now define $\psi:\{0,1\} \times I \rightarrow I $ by: $$ \psi(0,\theta) = cos \theta \\  \psi(1,\theta) = sin \theta $$ every $\lambda \in I$ can be associated with a function  $\Psi_{\lambda}:I \rightarrow I^{\omega}$ which generates a sequence in $I$, i.e. $$ \forall \theta \in I, \Psi_{\lambda}(\theta) = \{\theta_n\}_{n=0,1,2,...} $$ with $\theta_0=\theta$ and for $n \ge 0$ $$ \theta_{n+1} = \psi(\beta_{n+1}(\lambda),\theta_n) $$ let us now call $\lambda \in I$ a $\beta$-number if an asymptotic density of $1$s in its binary representation exists, i.e. if the sequence $\{\beta_n(\lambda)\}$ has a Cesaro-mean. this mean, if it exists, we may denote by $\beta^*(\lambda)$ let us also define $\alpha$ as the unique fixed point in I of the cosine function, i.e. $$cos \; \alpha = \alpha$$ conjecture 1. $\forall \theta \in I$ the sequence $\Psi_{\lambda}(\theta)$ has a Cesaro mean if and only if $\lambda$ is a $\beta$-number, and in this case the Cesaro mean is independent of $\theta$ and may be denoted $\Psi_{\lambda}^*$ 2. if the sequence $\Psi_{\lambda}(\theta)$ has Cesaro mean, then this mean is equal to $\alpha$ if and only if $\beta^*(\lambda)=0$ 3. for any $\beta$-number $\lambda$ if $\beta^*(\lambda) \gt 0$ then  $\Psi_{\lambda}^*\lt \alpha$",,"['analysis', 'trigonometry']"
31,"Convergence of the series $\sum_{\xi\in\mathbb Z^n} e^{2\pi ix\cdot \xi} a(x, \xi)\hat{f}(\xi)$?",Convergence of the series ?,"\sum_{\xi\in\mathbb Z^n} e^{2\pi ix\cdot \xi} a(x, \xi)\hat{f}(\xi)","I need some help with the following problem: let $a:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb C$ be a smooth function and suppose there are constantes $C_{\alpha, \beta}$ and $M(\alpha, \beta)$ such that $$|\partial^\alpha_x \partial^\beta_\xi a(x, \xi)|\leq C_{\alpha, \beta} \langle \xi\rangle^{M(\alpha, \beta)},$$ for every pair of multi-indices $\alpha, \beta$. How is this condition related to the convergence of the series $$\sum_{\xi\in\mathbb Z^n} e^{2\pi ix\cdot \xi} a(x, \xi)\hat{f}(\xi),$$ where $f\in\mathcal{S}(\mathbb R^n)$? The condition $$|\partial^\alpha_x \partial^\beta_\xi a(x, \xi)|\leq C_{\alpha, \beta} \langle \xi\rangle^{M(\alpha, \beta)},$$ showed up in the hypothesis of a theorem that I should prove and I didn't see how to use this in the proof, so I conjecture it must be for assuring the convergence of the above series.. Any help will be valuable.. Thanks. Some Definitions: 1. We define $\langle \xi\rangle:=(1+|\xi|^2)^{1/2}$. 2. $\displaystyle f\in\mathcal{S}(\mathbb R^n)\Leftrightarrow \sup_{x\in\mathbb R^n}|x^\beta \partial^\alpha f(x)|<\infty \forall \alpha, \beta\in\mathbb N_0^n.$ The space $\mathcal{S}(\mathbb R^n)$ is called Schwartz space. ** 3.** The convergence of the series $\displaystyle \sum_{\xi\in\mathbb Z^n}e^{2\pi ix\cdot \xi}a(x, \xi) \hat{f}(\xi)$ is in the sense there exists the limit $\displaystyle \lim_{k\to \infty} \sum_{|\xi|\leq k} e^{2\pi ix\cdot \xi} a(x, \xi) \hat{f}(\xi)$.","I need some help with the following problem: let $a:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb C$ be a smooth function and suppose there are constantes $C_{\alpha, \beta}$ and $M(\alpha, \beta)$ such that $$|\partial^\alpha_x \partial^\beta_\xi a(x, \xi)|\leq C_{\alpha, \beta} \langle \xi\rangle^{M(\alpha, \beta)},$$ for every pair of multi-indices $\alpha, \beta$. How is this condition related to the convergence of the series $$\sum_{\xi\in\mathbb Z^n} e^{2\pi ix\cdot \xi} a(x, \xi)\hat{f}(\xi),$$ where $f\in\mathcal{S}(\mathbb R^n)$? The condition $$|\partial^\alpha_x \partial^\beta_\xi a(x, \xi)|\leq C_{\alpha, \beta} \langle \xi\rangle^{M(\alpha, \beta)},$$ showed up in the hypothesis of a theorem that I should prove and I didn't see how to use this in the proof, so I conjecture it must be for assuring the convergence of the above series.. Any help will be valuable.. Thanks. Some Definitions: 1. We define $\langle \xi\rangle:=(1+|\xi|^2)^{1/2}$. 2. $\displaystyle f\in\mathcal{S}(\mathbb R^n)\Leftrightarrow \sup_{x\in\mathbb R^n}|x^\beta \partial^\alpha f(x)|<\infty \forall \alpha, \beta\in\mathbb N_0^n.$ The space $\mathcal{S}(\mathbb R^n)$ is called Schwartz space. ** 3.** The convergence of the series $\displaystyle \sum_{\xi\in\mathbb Z^n}e^{2\pi ix\cdot \xi}a(x, \xi) \hat{f}(\xi)$ is in the sense there exists the limit $\displaystyle \lim_{k\to \infty} \sum_{|\xi|\leq k} e^{2\pi ix\cdot \xi} a(x, \xi) \hat{f}(\xi)$.",,"['sequences-and-series', 'analysis', 'partial-differential-equations', 'fourier-series']"
32,Show that there exists a constant $c$ such that $\left|\int_0^b\frac{\sin ax}{x}dx\right|\le c$ [duplicate],Show that there exists a constant  such that  [duplicate],c \left|\int_0^b\frac{\sin ax}{x}dx\right|\le c,"This question already has an answer here : Real Analysis, Existence of a Limit Boundedly:$|\int_0^b\frac{sin(ax)}{x}dx | \le c$ (1 answer) Closed 6 years ago . Show that there exists a constant $c$ such that $$\left|\int_0^b\frac{\sin ax}{x}dx\right|\le c$$ In fact, show that the smallest such number is $c=\int_0^\pi\frac{\sin x}xdx$. Well, I'm thinking of a change of variable $y=ax$, and we want to show $\left|\int_0^B\frac{\sin x}xdx\right|$ is bounded. Then consider the intervals $[n\pi,(n+1)\pi)$ for natural number n. Thank you.","This question already has an answer here : Real Analysis, Existence of a Limit Boundedly:$|\int_0^b\frac{sin(ax)}{x}dx | \le c$ (1 answer) Closed 6 years ago . Show that there exists a constant $c$ such that $$\left|\int_0^b\frac{\sin ax}{x}dx\right|\le c$$ In fact, show that the smallest such number is $c=\int_0^\pi\frac{\sin x}xdx$. Well, I'm thinking of a change of variable $y=ax$, and we want to show $\left|\int_0^B\frac{\sin x}xdx\right|$ is bounded. Then consider the intervals $[n\pi,(n+1)\pi)$ for natural number n. Thank you.",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'fourier-analysis']"
33,Question on series,Question on series,,Suppose $ a_i $ be a sequence of positive real numbers such that $ \sum_{i=1}^{\infty}a_i < \infty $. Is it true that $ \sum_{i=1}^{\infty} \log(i) \cdot a_i < \infty $? Thanks,Suppose $ a_i $ be a sequence of positive real numbers such that $ \sum_{i=1}^{\infty}a_i < \infty $. Is it true that $ \sum_{i=1}^{\infty} \log(i) \cdot a_i < \infty $? Thanks,,['sequences-and-series']
34,"For the sequence $u_n$, $u_n \to +\infty \iff \frac{1}{u_n} \to 0$","For the sequence ,",u_n u_n \to +\infty \iff \frac{1}{u_n} \to 0,"Let $u=(u_n)_{n \in \mathbb{N}}$ be a sequence such that $u_n \neq 0$ , $u_n \to +\infty$ , for $ n \to +\infty$ . Proof that $u_n \to + \infty , ( n \to +\infty)  \iff \left(( \exists n_0 , \forall n \geq n_0: u_n > 0) \wedge \left(\frac{1}{u_n} \to 0, (n \to +\infty)\right) \right)$ First I want to highlight that Theorem: For any sequence , $b_n >0$ then $b_n \to 0$ iff limit of $\frac{1}{b_n} = \infty$ as $n \to \infty$ is closely (if not completely) analogous to this question. However, the answers given there are 'useless' for me since I'd much rather have someone to verify/falsify my steps. Since the idea of this theorem is very simple and intuitive, I thought that the proof might as well be, however when it comes to applying the rigorous definitions I begin to struggle. My attempt : ""For $ \implies$ "" $u_n \to +\infty$ for big $n$ means that $u_n$ does not converge. In special, the sequence does not converge to $a \in \mathbb{R}$ . Definition (Convergence): Let $u=(u_n)_{n \in \mathbb{N}}$ be a sequence and $a \in \mathbb{R}$ . The sequence is convergent and $a \in \mathbb{R}$ is its limit if and only if $\forall \epsilon >, \exists n_0 \in \mathbb{N}, \forall n \in \mathbb{N},(n \geq n_0 \rightarrow |u_n-a|< \epsilon)$ I wanted to use this definition because it's easy to negate, the negation would be: \begin{align}\exists \epsilon >0, \forall n_0 \in \mathbb{N}, \exists n \in \mathbb{N},(n \geq n_0 \wedge |u_n-a| \geq \epsilon) \tag{NOT converge} \end{align} My next step was to play a bit with this expression, especially with the distance, I obtained that: \begin{align}|u_n-a| \geq \epsilon \implies |u_n| \leq \frac{1}{a+\epsilon} \iff |u_n-0|\leq \frac{1}{a+\epsilon} \end{align} with the same quantors as in the negation. Here I could tell that if I choose $\epsilon$ to be very large, then the sequence converges to zero, as desired. However it looks edgy and incomplete to me, would I have to choose for an $\epsilon_1$ and define it such that $|u_n-0| < \epsilon$ turns out? For "" $\Longleftarrow$ "" The sequence $1/u_n$ converges to zero for big $n$ \begin{align} \left| \frac{1}{u_n}-0\right|< \epsilon \implies |u_n| < \frac{1}{\epsilon} \end{align} If $\epsilon$ is small, then the sequence gets very big. I do not see how I could formulate this well in an analytic statement to conclude that $u_n$ must diverge now. Idea (but not worked on yet): Maybe use the contrapositive?","Let be a sequence such that , , for . Proof that First I want to highlight that Theorem: For any sequence , $b_n >0$ then $b_n \to 0$ iff limit of $\frac{1}{b_n} = \infty$ as $n \to \infty$ is closely (if not completely) analogous to this question. However, the answers given there are 'useless' for me since I'd much rather have someone to verify/falsify my steps. Since the idea of this theorem is very simple and intuitive, I thought that the proof might as well be, however when it comes to applying the rigorous definitions I begin to struggle. My attempt : ""For "" for big means that does not converge. In special, the sequence does not converge to . Definition (Convergence): Let be a sequence and . The sequence is convergent and is its limit if and only if I wanted to use this definition because it's easy to negate, the negation would be: My next step was to play a bit with this expression, especially with the distance, I obtained that: with the same quantors as in the negation. Here I could tell that if I choose to be very large, then the sequence converges to zero, as desired. However it looks edgy and incomplete to me, would I have to choose for an and define it such that turns out? For "" "" The sequence converges to zero for big If is small, then the sequence gets very big. I do not see how I could formulate this well in an analytic statement to conclude that must diverge now. Idea (but not worked on yet): Maybe use the contrapositive?","u=(u_n)_{n \in \mathbb{N}} u_n \neq 0 u_n \to +\infty  n \to +\infty u_n \to + \infty , ( n \to +\infty)  \iff \left(( \exists n_0 , \forall n \geq n_0: u_n > 0) \wedge \left(\frac{1}{u_n} \to 0, (n \to +\infty)\right) \right)  \implies u_n \to +\infty n u_n a \in \mathbb{R} u=(u_n)_{n \in \mathbb{N}} a \in \mathbb{R} a \in \mathbb{R} \forall \epsilon >, \exists n_0 \in \mathbb{N}, \forall n \in \mathbb{N},(n \geq n_0 \rightarrow |u_n-a|< \epsilon) \begin{align}\exists \epsilon >0, \forall n_0 \in \mathbb{N}, \exists n \in \mathbb{N},(n \geq n_0 \wedge |u_n-a| \geq \epsilon) \tag{NOT converge} \end{align} \begin{align}|u_n-a| \geq \epsilon \implies |u_n| \leq \frac{1}{a+\epsilon} \iff |u_n-0|\leq \frac{1}{a+\epsilon} \end{align} \epsilon \epsilon_1 |u_n-0| < \epsilon \Longleftarrow 1/u_n n \begin{align} \left| \frac{1}{u_n}-0\right|< \epsilon \implies |u_n| < \frac{1}{\epsilon} \end{align} \epsilon u_n","['real-analysis', 'analysis', 'proof-verification']"
35,Integrating the exponential of a complex quadratic matrix,Integrating the exponential of a complex quadratic matrix,,"Problem statement I'm trying to do a discretized path integral/functional integral. The integral that I'm stuck with is of the form $$ \int_{-\infty}^{+\infty} \mathrm{d}^3\vec{x}_1\, \mathrm{d}^3\vec{x}_2\, \ldots\, \mathrm{d}^3\vec{x}_N\ \exp\left\{X^t \cdot (A + iB) \cdot X\right\} $$ where $X$ is a column 'vector' of all $\vec{x}_i$ vectors: $X = (\vec{x}_1\ \vec{x}_2\ \ldots\ \vec{x}_N)$, $A$ is a real symmetric pentadiagonal matrix and $B$ is a real symmetric tridiagonal matrix. The integration is over all real space. Additional info The $B$ matrix actually depends on lots of real scalar parameters $\lambda_1 \ldots \lambda_N$ that will later be integrated over as well (over all space). These are Lagrange multipliers to force some constraint on $X$, hence the $i$. I guess that for the vast majority of these parameters, the complete matrix $C \equiv A + iB$ will be diagonalizable (expect for a few pathological cases, of measure zero(?)). In that case, it can be written as $C = Z^t D Z$ with some diagonal matrix $D$ and $Z$ a complex matrix with $Z^tZ=1$ ($C$ is symmetric). If $Z$ were real, then a change of variables to $X' = ZX$ would turn the integration into a product of simple 1D gaussians over the diagonal entries of D. However, $Z$ is complex-valued (and D isn't necessarily real either(?)), so I guess that this won't quite work. All pointers are welcome!","Problem statement I'm trying to do a discretized path integral/functional integral. The integral that I'm stuck with is of the form $$ \int_{-\infty}^{+\infty} \mathrm{d}^3\vec{x}_1\, \mathrm{d}^3\vec{x}_2\, \ldots\, \mathrm{d}^3\vec{x}_N\ \exp\left\{X^t \cdot (A + iB) \cdot X\right\} $$ where $X$ is a column 'vector' of all $\vec{x}_i$ vectors: $X = (\vec{x}_1\ \vec{x}_2\ \ldots\ \vec{x}_N)$, $A$ is a real symmetric pentadiagonal matrix and $B$ is a real symmetric tridiagonal matrix. The integration is over all real space. Additional info The $B$ matrix actually depends on lots of real scalar parameters $\lambda_1 \ldots \lambda_N$ that will later be integrated over as well (over all space). These are Lagrange multipliers to force some constraint on $X$, hence the $i$. I guess that for the vast majority of these parameters, the complete matrix $C \equiv A + iB$ will be diagonalizable (expect for a few pathological cases, of measure zero(?)). In that case, it can be written as $C = Z^t D Z$ with some diagonal matrix $D$ and $Z$ a complex matrix with $Z^tZ=1$ ($C$ is symmetric). If $Z$ were real, then a change of variables to $X' = ZX$ would turn the integration into a product of simple 1D gaussians over the diagonal entries of D. However, $Z$ is complex-valued (and D isn't necessarily real either(?)), so I guess that this won't quite work. All pointers are welcome!",,"['real-analysis', 'linear-algebra', 'integration', 'complex-analysis', 'analysis']"
36,Difference of two convex functions,Difference of two convex functions,,"This is an exercise from a probability textbook on Ito's formula, basically Ito's formula extends to functions of this type. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function such that $f$ is twice differentiable everywhere except on a set $\{a_1,...a_k\}$. At these isolated points, $f'(a_n+)$ $f'(a_n-)$ $f''(a_n+)$ and $f''(a_n-)$ exists. (These are the left and right limits of $f'$ and $f''$ at these points.) The question is: how can we show such a function is the difference of two convex functions. I would also be interested in the following question: Which conditions could we have weakened such that it is still the difference of two convex functions?","This is an exercise from a probability textbook on Ito's formula, basically Ito's formula extends to functions of this type. Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a function such that $f$ is twice differentiable everywhere except on a set $\{a_1,...a_k\}$. At these isolated points, $f'(a_n+)$ $f'(a_n-)$ $f''(a_n+)$ and $f''(a_n-)$ exists. (These are the left and right limits of $f'$ and $f''$ at these points.) The question is: how can we show such a function is the difference of two convex functions. I would also be interested in the following question: Which conditions could we have weakened such that it is still the difference of two convex functions?",,"['real-analysis', 'analysis', 'stochastic-calculus']"
37,Clarification of Hölder norm in terms of oscillation,Clarification of Hölder norm in terms of oscillation,,"Let $\Omega\subset\mathbb{R}^2$ be an open bounded set, $B(x_0, \rho)=\{x\in\mathbb{R}^2\ |\ |x-x_0|\leq \rho\}$, $\Omega(x_0, \rho)\equiv B(x_0, \rho)\cap \Omega$, $u\in L^{\infty}\big(\Omega(x, \rho);\mathbb{R}^N\big)$ and $O(u, x, \rho)=\max_{1\leq k\leq N} \text{osc}_{\Omega(x, \rho)}u^k$, where $\text{osc}_{S} v\equiv\text{ess max}_{S}v-\text{ess min}_{S}v$. In Wiegner's 1981 paper, ""On Two-Dimensional Elliptic Systems with a One-sided Condition"", he defines the following norm for the Hölder space $C^{0, \alpha}(\overline{\Omega};\mathbb{R}^N)$: \begin{equation} \|u\|_{C^{0, \alpha}(\Omega;\mathbb{R}^N)}\equiv \sup_{\Omega}|u(x)|+\sup_{\substack{x\in \Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}\cdot O(u, x, \rho)\} \end{equation} Does anyone understand what $\rho_0$ is? He certainly doesn't define it before that line in his paper. I am assuming that if $u\in C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)$ then  \begin{equation} \|u\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\equiv \sum_{k=1}^N\|u^k\|_{L^{\infty}(\Omega)}+[u^k]_{C^{0, \alpha}(\Omega)} \end{equation} would be the typical Holder norm we would assign to the space $C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)$. Wiegner's norm is as in the question but I will denote it as $\|\cdot\|_W$ to distinguish it from the one above. If Ray Yang's interpretation (see comments) is true then I should be able to show an equivalence between the two norms. To show $\|\cdot\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\leq K\|\cdot\|_W$ I argued as follows: Firstly, we see that: \begin{equation} \sup_{\Omega} |u|\geq \|u^k\|_{L^{\infty}(\Omega)}\quad\forall \ k \end{equation}which implies \begin{equation}\tag{1} N\sup_{\Omega} |u|\geq \sum_{i=1}^N\|u^k\|_{L^{\infty}(\Omega)}. \end{equation} Now if we let $x\in\Omega$ and $\rho_o=\text{dist}(x, \partial\Omega)$ then for all $0<\rho\leq\rho_0$ and $y: |x-y|=\rho$ we deduce: \begin{align} \frac{|u^k(x)-u^k(y)|}{|x-y|^{\alpha}}&\leq\frac{\max_{B(x, \rho)}u^k(z)-\min_{B(x, \rho)}u^k(z)}{\rho^{\alpha}}\\ &\leq \rho^{-\alpha}O(u, x, \rho)\quad\ \forall\ k. \end{align} In this way as we vary $x$, we vary $\rho_0$ and $y$ , so for each $k$ we have \begin{equation} [u^k]_{C^{0, \alpha}(\Omega)}\leq \sup_{\substack{x\in\Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}O(u, x, \rho)\} \end{equation} and therefore \begin{equation}\tag{2} \sum_{k=1}^{N}[u^k]_{C^{0, \alpha}(\Omega)}\leq N\sup_{\substack{x\in\Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}O(u, x, \rho)\}. \end{equation} Putting (1) and (2) together we have \begin{equation} \|u\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\leq N\|u\|_W \end{equation} I don't know how to get  \begin{equation} \|\cdot\|_W\leq K\|\cdot\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}. \end{equation}","Let $\Omega\subset\mathbb{R}^2$ be an open bounded set, $B(x_0, \rho)=\{x\in\mathbb{R}^2\ |\ |x-x_0|\leq \rho\}$, $\Omega(x_0, \rho)\equiv B(x_0, \rho)\cap \Omega$, $u\in L^{\infty}\big(\Omega(x, \rho);\mathbb{R}^N\big)$ and $O(u, x, \rho)=\max_{1\leq k\leq N} \text{osc}_{\Omega(x, \rho)}u^k$, where $\text{osc}_{S} v\equiv\text{ess max}_{S}v-\text{ess min}_{S}v$. In Wiegner's 1981 paper, ""On Two-Dimensional Elliptic Systems with a One-sided Condition"", he defines the following norm for the Hölder space $C^{0, \alpha}(\overline{\Omega};\mathbb{R}^N)$: \begin{equation} \|u\|_{C^{0, \alpha}(\Omega;\mathbb{R}^N)}\equiv \sup_{\Omega}|u(x)|+\sup_{\substack{x\in \Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}\cdot O(u, x, \rho)\} \end{equation} Does anyone understand what $\rho_0$ is? He certainly doesn't define it before that line in his paper. I am assuming that if $u\in C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)$ then  \begin{equation} \|u\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\equiv \sum_{k=1}^N\|u^k\|_{L^{\infty}(\Omega)}+[u^k]_{C^{0, \alpha}(\Omega)} \end{equation} would be the typical Holder norm we would assign to the space $C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)$. Wiegner's norm is as in the question but I will denote it as $\|\cdot\|_W$ to distinguish it from the one above. If Ray Yang's interpretation (see comments) is true then I should be able to show an equivalence between the two norms. To show $\|\cdot\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\leq K\|\cdot\|_W$ I argued as follows: Firstly, we see that: \begin{equation} \sup_{\Omega} |u|\geq \|u^k\|_{L^{\infty}(\Omega)}\quad\forall \ k \end{equation}which implies \begin{equation}\tag{1} N\sup_{\Omega} |u|\geq \sum_{i=1}^N\|u^k\|_{L^{\infty}(\Omega)}. \end{equation} Now if we let $x\in\Omega$ and $\rho_o=\text{dist}(x, \partial\Omega)$ then for all $0<\rho\leq\rho_0$ and $y: |x-y|=\rho$ we deduce: \begin{align} \frac{|u^k(x)-u^k(y)|}{|x-y|^{\alpha}}&\leq\frac{\max_{B(x, \rho)}u^k(z)-\min_{B(x, \rho)}u^k(z)}{\rho^{\alpha}}\\ &\leq \rho^{-\alpha}O(u, x, \rho)\quad\ \forall\ k. \end{align} In this way as we vary $x$, we vary $\rho_0$ and $y$ , so for each $k$ we have \begin{equation} [u^k]_{C^{0, \alpha}(\Omega)}\leq \sup_{\substack{x\in\Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}O(u, x, \rho)\} \end{equation} and therefore \begin{equation}\tag{2} \sum_{k=1}^{N}[u^k]_{C^{0, \alpha}(\Omega)}\leq N\sup_{\substack{x\in\Omega\\ \rho\leq\rho_0}}\{\rho^{-\alpha}O(u, x, \rho)\}. \end{equation} Putting (1) and (2) together we have \begin{equation} \|u\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}\leq N\|u\|_W \end{equation} I don't know how to get  \begin{equation} \|\cdot\|_W\leq K\|\cdot\|_{C^{0, \alpha}(\overline{\Omega}, \mathbb{R}^N)}. \end{equation}",,"['analysis', 'partial-differential-equations', 'definition', 'holder-spaces']"
38,inequality of some integrals of continuously differential function.,inequality of some integrals of continuously differential function.,,"Let $f:[a,b]$→$\mathbb{R}$ be a continuously differential fuction satisfying f(a)=0. My goal is to show that $$\int_{a}^{b} |f(x)|^2 dx \le  \frac{(b-a)^2}{2} \int_{a}^{b} |f'(x)|^2 dx $$ My attempt is following: By FTC, $f(x)=f(a)+\int_{a}^{x} f'(t) dt$ = $\int_{a}^{x} f'(t) dt$ However, by M.V.T for integral $\int_{a}^{x} f'(t) dt=(x-a)f'(\zeta_x)$ for some $\zeta_x \in [a, x]$ So, $\int_{a}^{b} |f(x)|^2 dx$=$\int_{a}^{b} |\int_{a}^{x} f'(t) dt|^2 dx$ = $\int_{a}^{b} |(x-a)f'(\zeta_x) dt|^2 dx$ $\le$ $(b-a)^2\int_{a}^{b} |f'(\zeta_x) dt|^2 dx$ $…(*)$ For a convenience, we define a map $g:[a,b]→[a,b]$ by  $g(x)=\zeta_x$. Then the right integral in the inequality $(*)$ becomes $(b-a)^2\int_{a}^{b} |f'(g(x)) dt|^2 dx$. Consequently, $\int_{a}^{b} |f(x)|^2 dx \le (b-a)^2\int_{a}^{b} |f'(g(x)) dt|^2 dx$. Hereby, I tried. But I don't know next step..  please inform me how to proceed the problem.","Let $f:[a,b]$→$\mathbb{R}$ be a continuously differential fuction satisfying f(a)=0. My goal is to show that $$\int_{a}^{b} |f(x)|^2 dx \le  \frac{(b-a)^2}{2} \int_{a}^{b} |f'(x)|^2 dx $$ My attempt is following: By FTC, $f(x)=f(a)+\int_{a}^{x} f'(t) dt$ = $\int_{a}^{x} f'(t) dt$ However, by M.V.T for integral $\int_{a}^{x} f'(t) dt=(x-a)f'(\zeta_x)$ for some $\zeta_x \in [a, x]$ So, $\int_{a}^{b} |f(x)|^2 dx$=$\int_{a}^{b} |\int_{a}^{x} f'(t) dt|^2 dx$ = $\int_{a}^{b} |(x-a)f'(\zeta_x) dt|^2 dx$ $\le$ $(b-a)^2\int_{a}^{b} |f'(\zeta_x) dt|^2 dx$ $…(*)$ For a convenience, we define a map $g:[a,b]→[a,b]$ by  $g(x)=\zeta_x$. Then the right integral in the inequality $(*)$ becomes $(b-a)^2\int_{a}^{b} |f'(g(x)) dt|^2 dx$. Consequently, $\int_{a}^{b} |f(x)|^2 dx \le (b-a)^2\int_{a}^{b} |f'(g(x)) dt|^2 dx$. Hereby, I tried. But I don't know next step..  please inform me how to proceed the problem.",,['analysis']
39,$|f(t) - f(s) |\leq \int_s^t g $ then $f(t) - f(s) = \int_s^t h.$,then,|f(t) - f(s) |\leq \int_s^t g  f(t) - f(s) = \int_s^t h.,"Let $f : [0,1] \rightarrow [0, + \infty)$. If there exists $g \in L^1([0,1]) $ s.t. for every $t,s \in [0,1]$ holds   $$ |f(t) - f(s)| \leq \int_s^t g(u) \, du \quad (t>s),$$    then there exists $h \in L^1([0,1])$ s.t. for almost every $t,s \in [0,1]$ holds   $$f(t) - f(s) = \int_s^t h(u) \, du \quad (t>s).$$ Is this true? I suspect that there is a quite straightforward way to show this, but can't figure it out. I suppose that one could prove the statement using absoulte continuity and Lebesgue integral theorem, but I'm actually looking for a direct proof of the statement.","Let $f : [0,1] \rightarrow [0, + \infty)$. If there exists $g \in L^1([0,1]) $ s.t. for every $t,s \in [0,1]$ holds   $$ |f(t) - f(s)| \leq \int_s^t g(u) \, du \quad (t>s),$$    then there exists $h \in L^1([0,1])$ s.t. for almost every $t,s \in [0,1]$ holds   $$f(t) - f(s) = \int_s^t h(u) \, du \quad (t>s).$$ Is this true? I suspect that there is a quite straightforward way to show this, but can't figure it out. I suppose that one could prove the statement using absoulte continuity and Lebesgue integral theorem, but I'm actually looking for a direct proof of the statement.",,"['real-analysis', 'analysis', 'measure-theory', 'integration', 'lebesgue-integral']"
40,the solution of Fredholm´s integral equation,the solution of Fredholm´s integral equation,,"Be  $\lambda \in \mathbb{R}$ such that $\left | \lambda  \right |> \left \| \kappa  \right \|_{\infty }(b-a)$. Prove that the solution $f^*$ of the integral equation of Fredholm $$\lambda f -\int_{a}^{b}\kappa (x,y)f(y)dy=g(x)$$  for all $x\in [a,b]$ satisfies $$\left \| f^*-\sum_{m=1}^{k}\frac{1}{\lambda ^m}\Im ^{m-1}g \right \|_{\infty }\leq \frac{\alpha ^{k}}{(1-\alpha) \left | \lambda  \right |} \left \| g \right \|_{\infty }$$ for all $k\in \mathbb{N}$ where $$\alpha :=\frac{\left \| \kappa  \right \|(b-a)}{\left |\lambda  \right | }$$ i´m really stuck in this problem, I know that I can use that $$\Im :C_{\infty }^{0}[a,b]\rightarrow C_{\infty }^{0}[a,b]$$ is Lipschitz continuous and that linear Fredholm´s operator is linear, can anybody just give a hint please? thanks!","Be  $\lambda \in \mathbb{R}$ such that $\left | \lambda  \right |> \left \| \kappa  \right \|_{\infty }(b-a)$. Prove that the solution $f^*$ of the integral equation of Fredholm $$\lambda f -\int_{a}^{b}\kappa (x,y)f(y)dy=g(x)$$  for all $x\in [a,b]$ satisfies $$\left \| f^*-\sum_{m=1}^{k}\frac{1}{\lambda ^m}\Im ^{m-1}g \right \|_{\infty }\leq \frac{\alpha ^{k}}{(1-\alpha) \left | \lambda  \right |} \left \| g \right \|_{\infty }$$ for all $k\in \mathbb{N}$ where $$\alpha :=\frac{\left \| \kappa  \right \|(b-a)}{\left |\lambda  \right | }$$ i´m really stuck in this problem, I know that I can use that $$\Im :C_{\infty }^{0}[a,b]\rightarrow C_{\infty }^{0}[a,b]$$ is Lipschitz continuous and that linear Fredholm´s operator is linear, can anybody just give a hint please? thanks!",,"['real-analysis', 'analysis', 'functional-analysis', 'integral-equations']"
41,Understanding Dini continuity for lifts of circle endomorhpisms,Understanding Dini continuity for lifts of circle endomorhpisms,,"A function $F:\mathbb{R}\rightarrow\mathbb{R}$ is said to be Dini continuous if its modulus of continuity $$\omega(t)=\sup_{|x-y|\leq t}|F(x)-F(y)|$$ satisfies $$\int_0^1\frac{\omega(t)}{t}dt<\infty.$$ I can clearly see that a Dini continuous function must be continuous, for otherwise $\omega(t)\nrightarrow0$ as $t\rightarrow0$ and the integral would be infinite. And in the case where the domain of $F$ is open, it is easy to come up with examples of functions which are continuous (and, in fact, differentiable) but not Dini continuous, e.g. $F(x)=x^2, e^x, \log(x)$, etc with domain $\mathbb{R^{+}}$. However, I am currently reading a paper that uses the notion of Dini continuity for lifts $F:\mathbb{R}\rightarrow\mathbb{R}$ of circle endomorphisms $f:S^1\rightarrow S^1$ of degree $n$. That is, $\pi\circ F=f\circ\pi$, where $\pi:\mathbb{R}\rightarrow S^1$ is the usual covering map $\pi(x)=e^{2\pi ix}$, and $F(x+1)=F(x)+n$. Assume from now on that $F$ is such a function. This seems to place a tight restriction on the types of functions we are considering. I wonder if anyone could help my understanding by supplying any of the following examples. (1) An $F$ that is continuous but not Dini continuous. (2) An $F$ that is differentiable but not Dini Continuous. (3) An $F$ that is differentiable but $\log F'(x)$ is not Dini continuous. Thanks.","A function $F:\mathbb{R}\rightarrow\mathbb{R}$ is said to be Dini continuous if its modulus of continuity $$\omega(t)=\sup_{|x-y|\leq t}|F(x)-F(y)|$$ satisfies $$\int_0^1\frac{\omega(t)}{t}dt<\infty.$$ I can clearly see that a Dini continuous function must be continuous, for otherwise $\omega(t)\nrightarrow0$ as $t\rightarrow0$ and the integral would be infinite. And in the case where the domain of $F$ is open, it is easy to come up with examples of functions which are continuous (and, in fact, differentiable) but not Dini continuous, e.g. $F(x)=x^2, e^x, \log(x)$, etc with domain $\mathbb{R^{+}}$. However, I am currently reading a paper that uses the notion of Dini continuity for lifts $F:\mathbb{R}\rightarrow\mathbb{R}$ of circle endomorphisms $f:S^1\rightarrow S^1$ of degree $n$. That is, $\pi\circ F=f\circ\pi$, where $\pi:\mathbb{R}\rightarrow S^1$ is the usual covering map $\pi(x)=e^{2\pi ix}$, and $F(x+1)=F(x)+n$. Assume from now on that $F$ is such a function. This seems to place a tight restriction on the types of functions we are considering. I wonder if anyone could help my understanding by supplying any of the following examples. (1) An $F$ that is continuous but not Dini continuous. (2) An $F$ that is differentiable but not Dini Continuous. (3) An $F$ that is differentiable but $\log F'(x)$ is not Dini continuous. Thanks.",,"['real-analysis', 'analysis']"
42,Surjectivity of a function considering $f \circ h=Id_Y$,Surjectivity of a function considering,f \circ h=Id_Y,"\begin{align} f: \mathbb{R}^2 &\longrightarrow \mathbb{R} \\ (x,y) & \longmapsto x+y \end{align} Question: Is this function surjective? It seems clear to me that this function must be surjective, because the point $(x,y) \in \mathbb{R}^2$ maps to the entire codomain $\mathbb{R}$ under the addition given by function. But I wanted to try a different approach: Question (refined): Is this function surjective, using  $\ f\circ h=Id_\mathbb{R}$ Here are my steps. I defined $h$ as the following function: \begin{align}h: \mathbb{R} &\longrightarrow \mathbb{R}^2 \\ x &\longmapsto (x,0) \end{align} Such that: \begin{align} (f \circ h)(x)=x&=f(h(x)) \\ &=f(x,0)=x \end{align} Are these steps correct, or aren't they even valid? Note : I am very new to this subject and in my homework assigment I am also allowed to use examples and counter examples, but I am always very eager to expand my knowledge to such sentences as introduced as above. Namely: if a right inverse exists, such that $f \circ h= Id_Y$, where $h$ is called the right inverse , then the function $f$ is surjective.","\begin{align} f: \mathbb{R}^2 &\longrightarrow \mathbb{R} \\ (x,y) & \longmapsto x+y \end{align} Question: Is this function surjective? It seems clear to me that this function must be surjective, because the point $(x,y) \in \mathbb{R}^2$ maps to the entire codomain $\mathbb{R}$ under the addition given by function. But I wanted to try a different approach: Question (refined): Is this function surjective, using  $\ f\circ h=Id_\mathbb{R}$ Here are my steps. I defined $h$ as the following function: \begin{align}h: \mathbb{R} &\longrightarrow \mathbb{R}^2 \\ x &\longmapsto (x,0) \end{align} Such that: \begin{align} (f \circ h)(x)=x&=f(h(x)) \\ &=f(x,0)=x \end{align} Are these steps correct, or aren't they even valid? Note : I am very new to this subject and in my homework assigment I am also allowed to use examples and counter examples, but I am always very eager to expand my knowledge to such sentences as introduced as above. Namely: if a right inverse exists, such that $f \circ h= Id_Y$, where $h$ is called the right inverse , then the function $f$ is surjective.",,['analysis']
43,Decomposability in the tensor product sense of functions of two variables,Decomposability in the tensor product sense of functions of two variables,,"Let $S$ and $T$ be ""nice"" metric spaces, e.g. complete normed fields like $\Bbb R$, $\Bbb C$ or $\Bbb Q_p$. Let $F$ be a function $$ F:S\times T\longrightarrow K $$ where $K$ is a topological field which has some nice regularity: let's say continuous or even differentiable or analytic whenever the concept applies. Are there criteria to decide if there are functions $f:S\rightarrow K$ and $g:T\rightarrow K$ such that $$ F(s,t)=f(s)g(t)\qquad\qquad\qquad(\ast) $$ for all $(s,t)\in S\times T$? In particular, I was thinking of something of this sort: if $F$ is $0$ everywhere, $(\ast)$ works obviously with $f\equiv0$ and $g\equiv0$, so assume there exists $(s',t')\in S\times T$ such that $F(s',t')\neq0$. Up to rescaling $F$ we may assume $F(s',t')=1$. Let $$ f(s)=F(s,t'),\qquad g(t)=F(s',t). $$ Are there non-tautological conditions under which $(\ast)$ holds? Is the special case $S=T$ more treatable? Of course, the problem is a special case of the problem of ""recognizing"" the vectors of the form $v\otimes w$ in a tensor product $V\otimes W$ ( see this SE question ) except that now the spaces have infinite dimension.","Let $S$ and $T$ be ""nice"" metric spaces, e.g. complete normed fields like $\Bbb R$, $\Bbb C$ or $\Bbb Q_p$. Let $F$ be a function $$ F:S\times T\longrightarrow K $$ where $K$ is a topological field which has some nice regularity: let's say continuous or even differentiable or analytic whenever the concept applies. Are there criteria to decide if there are functions $f:S\rightarrow K$ and $g:T\rightarrow K$ such that $$ F(s,t)=f(s)g(t)\qquad\qquad\qquad(\ast) $$ for all $(s,t)\in S\times T$? In particular, I was thinking of something of this sort: if $F$ is $0$ everywhere, $(\ast)$ works obviously with $f\equiv0$ and $g\equiv0$, so assume there exists $(s',t')\in S\times T$ such that $F(s',t')\neq0$. Up to rescaling $F$ we may assume $F(s',t')=1$. Let $$ f(s)=F(s,t'),\qquad g(t)=F(s',t). $$ Are there non-tautological conditions under which $(\ast)$ holds? Is the special case $S=T$ more treatable? Of course, the problem is a special case of the problem of ""recognizing"" the vectors of the form $v\otimes w$ in a tensor product $V\otimes W$ ( see this SE question ) except that now the spaces have infinite dimension.",,"['linear-algebra', 'analysis', 'functions']"
44,Dido's problem with Euler equations,Dido's problem with Euler equations,,"I'm considering Dido's problem: Consider 2 differentiable arcs $C$ and $C_0$ in $\mathbb{R}^2$ from the point $P$ to $Q$ and back. We keep $C_0,P,Q$ fixed, and want to choose the arc $C$ such that under all arcs of a specified length larther then $|PQ|$ the area $A$ enclosed by the 2 curves is maximized. $$A = \frac{1}{2}\int_{C\cup C_0}xdy-ydx  $$ Solutions to this problem using variational calculus are sketched in (1), (2) (1) http://galileo.phys.virginia.edu/classes/321.jvn.fall02/var_meth.pdf (2) http://mathematicalgarden.wordpress.com/2008/12/21/the-problem-of-dido/ I want to show that a necessary condition is that the curvature $\kappa$ is constant $$\kappa = \frac{\dot{x}\ddot{y}-\dot{y}\ddot{x}}{(\dot{x}^2+\dot{y}^2)^{3/2}}$$ Ofcourse knowing that the solution to this problem is a circular arc, we know that it is. But I want to derive this... It appears that the Lagrangian of this problem (see (2)) is $$\frac{1}{2}(x\dot{y}-y\dot{x})+\lambda\sqrt{\dot{x}^2+\dot{y}^2}  $$ And in (1) we see that using Eulers equations \begin{align*} \dot{y}\kappa+\lambda x =0\\ -\dot{x}\kappa + \lambda y =0 \end{align*} These can be combined to see that $\lambda(x\dot{x}+y\dot{y}) =0$ with solution $x^2+y^2 = C$. But I simply want to show that $\kappa$ is constant is a necessary condition, but I cant see how. How can we derive this? Thanks for any enlighting remark.","I'm considering Dido's problem: Consider 2 differentiable arcs $C$ and $C_0$ in $\mathbb{R}^2$ from the point $P$ to $Q$ and back. We keep $C_0,P,Q$ fixed, and want to choose the arc $C$ such that under all arcs of a specified length larther then $|PQ|$ the area $A$ enclosed by the 2 curves is maximized. $$A = \frac{1}{2}\int_{C\cup C_0}xdy-ydx  $$ Solutions to this problem using variational calculus are sketched in (1), (2) (1) http://galileo.phys.virginia.edu/classes/321.jvn.fall02/var_meth.pdf (2) http://mathematicalgarden.wordpress.com/2008/12/21/the-problem-of-dido/ I want to show that a necessary condition is that the curvature $\kappa$ is constant $$\kappa = \frac{\dot{x}\ddot{y}-\dot{y}\ddot{x}}{(\dot{x}^2+\dot{y}^2)^{3/2}}$$ Ofcourse knowing that the solution to this problem is a circular arc, we know that it is. But I want to derive this... It appears that the Lagrangian of this problem (see (2)) is $$\frac{1}{2}(x\dot{y}-y\dot{x})+\lambda\sqrt{\dot{x}^2+\dot{y}^2}  $$ And in (1) we see that using Eulers equations \begin{align*} \dot{y}\kappa+\lambda x =0\\ -\dot{x}\kappa + \lambda y =0 \end{align*} These can be combined to see that $\lambda(x\dot{x}+y\dot{y}) =0$ with solution $x^2+y^2 = C$. But I simply want to show that $\kappa$ is constant is a necessary condition, but I cant see how. How can we derive this? Thanks for any enlighting remark.",,"['real-analysis', 'analysis', 'functional-analysis', 'calculus-of-variations']"
45,Need help showing supremum does not exist,Need help showing supremum does not exist,,"(i) S is a subset of R and β ∈ R. Let T = {s + β : s ∈  S }. Show that if Sup S exists, then so does sup T. Moreover, Sup T = β + Sup S. (ii) Using what you have proved in (i), show that if x ∈ R, x ≠ 0 and S =  {gx : g ∈ Z }, then Sup S does not exist. Here is my proof for (i) Let the upper bound of S be called m. Then m > s for all s ∈ S.  By property of real numbers, we can say that m + β > s+ β for some β ∈  R. Thus, we see that every s+ β ∈ T is an upper bound of T. T is bounded upwards. Our hypothesis implies that S has a least upper bound. Let Sup S = s' => if x < s', then x is not an upper bound of S. By property of real numbers, if x + β < s' + β, then x  + β is not an upper bound of T. T also has a least upper bound, which is Sup T = s' + β => Sup T = β + Sup S as desired. I got to (ii) and somehow am not seeing the light on where to start. Any pointers?","(i) S is a subset of R and β ∈ R. Let T = {s + β : s ∈  S }. Show that if Sup S exists, then so does sup T. Moreover, Sup T = β + Sup S. (ii) Using what you have proved in (i), show that if x ∈ R, x ≠ 0 and S =  {gx : g ∈ Z }, then Sup S does not exist. Here is my proof for (i) Let the upper bound of S be called m. Then m > s for all s ∈ S.  By property of real numbers, we can say that m + β > s+ β for some β ∈  R. Thus, we see that every s+ β ∈ T is an upper bound of T. T is bounded upwards. Our hypothesis implies that S has a least upper bound. Let Sup S = s' => if x < s', then x is not an upper bound of S. By property of real numbers, if x + β < s' + β, then x  + β is not an upper bound of T. T also has a least upper bound, which is Sup T = s' + β => Sup T = β + Sup S as desired. I got to (ii) and somehow am not seeing the light on where to start. Any pointers?",,['analysis']
46,Borel lemma on rationality,Borel lemma on rationality,,I am looking for an enlightening proof of the following fact: Let $F(t)\in \mathbb{Z}[[t]]$ and suppose $S$ is a finite set of places on $\mathbb Q$ containing $\infty$. If for every $v\in S$ $F(t)$ is meromorphic of radius $r_v$ in $\mathbb Q_v$ and $\prod_{v\in S} r_v>1$ then $F(t)\in \mathbb Q(t)$ (is rational over $\mathbb Q$).,I am looking for an enlightening proof of the following fact: Let $F(t)\in \mathbb{Z}[[t]]$ and suppose $S$ is a finite set of places on $\mathbb Q$ containing $\infty$. If for every $v\in S$ $F(t)$ is meromorphic of radius $r_v$ in $\mathbb Q_v$ and $\prod_{v\in S} r_v>1$ then $F(t)\in \mathbb Q(t)$ (is rational over $\mathbb Q$).,,"['analysis', 'number-theory', 'p-adic-number-theory']"
47,existence theorem of eliptic equation,existence theorem of eliptic equation,,"Consider $\Omega \subset R^n$ a bounded and open set with $\partial \Omega$ smooth . Consider the problem: $$ - \Delta u  +  au = f   \text{ in } \Omega $$  $$ u = 0    \text{ in } \partial \Omega $$ where $a \geq 0$ is a constant. If $f \in C^1 (\Omega) \cap C(\overline{\Omega})$ , then exist a solution $u$ for the problem with $u \in C^{2}(\Omega) \cap C(\overline{\Omega})$? If the answer is yes, please say to me a basic reference to find this result. thanks in advance !","Consider $\Omega \subset R^n$ a bounded and open set with $\partial \Omega$ smooth . Consider the problem: $$ - \Delta u  +  au = f   \text{ in } \Omega $$  $$ u = 0    \text{ in } \partial \Omega $$ where $a \geq 0$ is a constant. If $f \in C^1 (\Omega) \cap C(\overline{\Omega})$ , then exist a solution $u$ for the problem with $u \in C^{2}(\Omega) \cap C(\overline{\Omega})$? If the answer is yes, please say to me a basic reference to find this result. thanks in advance !",,"['analysis', 'reference-request', 'partial-differential-equations']"
48,"For what $f$ is it true that $\lim_{n\to\infty}\sum_{k=0}^nf(n,k)=\sum_{k=0}^{\infty}\lim_{n\to\infty}f(n,k)$",For what  is it true that,"f \lim_{n\to\infty}\sum_{k=0}^nf(n,k)=\sum_{k=0}^{\infty}\lim_{n\to\infty}f(n,k)","Let $f:\mathbb{N_0}^2\to\mathbb{R}$.  For what $f$ is it true that $$\lim_{n\to\infty}\sum_{k=0}^nf(n,k)=\sum_{k=0}^{\infty}\lim_{n\to\infty}f(n,k):=\lim_{m\to\infty}\sum_{k=0}^m\lim_{n\to\infty}f(n,k)$$ . For example, if $f(n,k)=(1-k/n)^n$ satisfies this property, then $$\lim_{n\to\infty}(1/n)^n+(2/n)^n+...+(n/n)^n=\lim_{n\to\infty}\sum_{k=0}^{n}(1-k/n)^n=\sum_{k=0}^{\infty}e^{-k}=e/(e-1)$$","Let $f:\mathbb{N_0}^2\to\mathbb{R}$.  For what $f$ is it true that $$\lim_{n\to\infty}\sum_{k=0}^nf(n,k)=\sum_{k=0}^{\infty}\lim_{n\to\infty}f(n,k):=\lim_{m\to\infty}\sum_{k=0}^m\lim_{n\to\infty}f(n,k)$$ . For example, if $f(n,k)=(1-k/n)^n$ satisfies this property, then $$\lim_{n\to\infty}(1/n)^n+(2/n)^n+...+(n/n)^n=\lim_{n\to\infty}\sum_{k=0}^{n}(1-k/n)^n=\sum_{k=0}^{\infty}e^{-k}=e/(e-1)$$",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'summation']"
49,Solution of nonlinear waves( breathers),Solution of nonlinear waves( breathers),,"The sine-Gordon equation is known as $$\frac{\partial^2 u}{\partial t^2} - \frac{\partial^2 u}{\partial x^2} + \sin u = 0,$$ Can you please derive the equation which is known as breather equation and obtained by collitsion of two solitions $$u = 4 \arctan\left(\frac{\sqrt{1-\omega^2}\;\cos(\omega t)}{\omega\;\cosh(\sqrt{1-\omega^2}\; x)}\right).$$","The sine-Gordon equation is known as $$\frac{\partial^2 u}{\partial t^2} - \frac{\partial^2 u}{\partial x^2} + \sin u = 0,$$ Can you please derive the equation which is known as breather equation and obtained by collitsion of two solitions $$u = 4 \arctan\left(\frac{\sqrt{1-\omega^2}\;\cos(\omega t)}{\omega\;\cosh(\sqrt{1-\omega^2}\; x)}\right).$$",,"['real-analysis', 'analysis']"
50,calculate the volume formed by rotation of a given region,calculate the volume formed by rotation of a given region,,"I have a question in elementary differential calculus: Let $S$ be a region given by $$S=\{(x,y): 0\leq x\leq 1,\ \ 3^x-x-1\leq y\leq x\}$$ then define $V$ the solid obtained by rotating $S$ around $$y=x$$, how to calculate the volume of $V$? Thanks for the help!","I have a question in elementary differential calculus: Let $S$ be a region given by $$S=\{(x,y): 0\leq x\leq 1,\ \ 3^x-x-1\leq y\leq x\}$$ then define $V$ the solid obtained by rotating $S$ around $$y=x$$, how to calculate the volume of $V$? Thanks for the help!",,"['calculus', 'analysis', 'integration']"
51,total variation measure vs. total variation of its distribution function,total variation measure vs. total variation of its distribution function,,"Let's say that $f:[0, \infty)\rightarrow \mathbb{R}$ for simplicity, although the question is also intended for a variety of other types of domains. (Probably interval is all that is required.) Suppose that $f$ is of bounded variation on finite intervals, and cadlag.  Then although it is not possible to find a signed measure $\mu$ for which $\mu((a, b])=f(b)-f(a)$, it is possible to do so uniquely on finite intervals within the domain of f of that type. (left open, right closed.)  So let us say that we take $(a', b'] \subset [0, \infty)$ and we obtain the measure $\nu$ corresponding to f restricted to $(a', b']$  Is it true that $|\nu|((a', b'])=TV(f|_{[a,b]})$?  Here TV means total variation, and the absolute value signs denote total variation norm of a measure.  The sup implied on the LHS is clearly more inclusive than the one on the right, which allows only finite interval partitions.  How do I go the other way?","Let's say that $f:[0, \infty)\rightarrow \mathbb{R}$ for simplicity, although the question is also intended for a variety of other types of domains. (Probably interval is all that is required.) Suppose that $f$ is of bounded variation on finite intervals, and cadlag.  Then although it is not possible to find a signed measure $\mu$ for which $\mu((a, b])=f(b)-f(a)$, it is possible to do so uniquely on finite intervals within the domain of f of that type. (left open, right closed.)  So let us say that we take $(a', b'] \subset [0, \infty)$ and we obtain the measure $\nu$ corresponding to f restricted to $(a', b']$  Is it true that $|\nu|((a', b'])=TV(f|_{[a,b]})$?  Here TV means total variation, and the absolute value signs denote total variation norm of a measure.  The sup implied on the LHS is clearly more inclusive than the one on the right, which allows only finite interval partitions.  How do I go the other way?",,"['real-analysis', 'analysis', 'measure-theory']"
52,Doubling measure is absolutely continuous with respect to Lebesgue,Doubling measure is absolutely continuous with respect to Lebesgue,,"Let $\mu$ be a fixed finite measure on $\mathbb R$. We say that $\mu$ is doubling if there exists a constant $C>0$, such that for any two adjacent intervals $I=[x−h,x]$ and $J=[x,x+h]$, $$C^{−1}\mu(I)≤\mu(J)≤C\mu(I).$$  Assuming that $\mu$ is doubling, show that there exist positive constants $B$ and $a$, such that for every interval $I$, $$\mu(I)≤B[length(I)]^a$$ By Radon-Nikodim, I solved but I use the fact that $\mu$ is absolutely continous with respect to Lebesgue measure, but I don't know to prove this last part, i.e., that $\mu$ is absolutely continuous with respect to Lebesgue.","Let $\mu$ be a fixed finite measure on $\mathbb R$. We say that $\mu$ is doubling if there exists a constant $C>0$, such that for any two adjacent intervals $I=[x−h,x]$ and $J=[x,x+h]$, $$C^{−1}\mu(I)≤\mu(J)≤C\mu(I).$$  Assuming that $\mu$ is doubling, show that there exist positive constants $B$ and $a$, such that for every interval $I$, $$\mu(I)≤B[length(I)]^a$$ By Radon-Nikodim, I solved but I use the fact that $\mu$ is absolutely continous with respect to Lebesgue measure, but I don't know to prove this last part, i.e., that $\mu$ is absolutely continuous with respect to Lebesgue.",,"['real-analysis', 'analysis', 'measure-theory']"
53,"When does it make sense to say ""the smallest measurable set containing $x\,$""?","When does it make sense to say ""the smallest measurable set containing ""?","x\,","We know for the Borel $\sigma$-algebra that each singleton set is measurable. I was working on the problem of proving that each infinite $\sigma$-algebra has uncountably many members. My solution went something like this: if $\sigma$ were countable, intersect all of the measurable sets which contain $x$. I derive a contradiction from this. Of course, $\sigma$-algebras aren't necessarily closed under arbitrary intersection, so it might not make sense to talk about ""smallest measurable set"" containing a point. My question is what conditions can we put on the measure space so that it does make sense to talk of the smallest measurable set containing a point?","We know for the Borel $\sigma$-algebra that each singleton set is measurable. I was working on the problem of proving that each infinite $\sigma$-algebra has uncountably many members. My solution went something like this: if $\sigma$ were countable, intersect all of the measurable sets which contain $x$. I derive a contradiction from this. Of course, $\sigma$-algebras aren't necessarily closed under arbitrary intersection, so it might not make sense to talk about ""smallest measurable set"" containing a point. My question is what conditions can we put on the measure space so that it does make sense to talk of the smallest measurable set containing a point?",,"['real-analysis', 'analysis', 'measure-theory']"
54,"upper semi-continuity of a multi-valued function $T$ and lower semi-continuity of $d(x,T(x))$",upper semi-continuity of a multi-valued function  and lower semi-continuity of,"T d(x,T(x))","Let $(X,d)$ be a complete metric space, $CB(X)$ the set of closed and bounded subsets of $X$, and $T:X\rightarrow C(X)$ be a multi-valued function. How can you  prove this: If $T$ is upper semi-continuous then $f(x)=d(x,T(x))$ is lower semi-continuous And the 2nd question is that Does the reverse hold?","Let $(X,d)$ be a complete metric space, $CB(X)$ the set of closed and bounded subsets of $X$, and $T:X\rightarrow C(X)$ be a multi-valued function. How can you  prove this: If $T$ is upper semi-continuous then $f(x)=d(x,T(x))$ is lower semi-continuous And the 2nd question is that Does the reverse hold?",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'fixed-point-theorems']"
55,Asymptotics of Green's function of laplacian,Asymptotics of Green's function of laplacian,,"Let $ \Omega $ be a domain in $ \mathbb{R}^2 $ with a Riemannian metric $ g $, and let $\Delta $ be the laplace-beltrami operator induced by the metric, with dirichlet boundary conditions. For $ x,y \in \Omega $, let $ d(x,y) $ be the geodesic distance between $ x$ and $ y$. The green's function $ G(x,y) $ can be written as: $$ G(x,y) = -\frac{1}{4 \pi} \log(d(x,y)^2) + \omega(x,y) $$ where $\omega $ is some harmonic function that corrects for the boundary condition. Now for $ x$ and $ y $ close by one can write: $$ d(x,y)^2 = g_{ij} (x-y)^i (x-y)^j + O((x-y)^3) $$ Then one can write: $$ G(x,y) =  -\frac{1}{4 \pi} \log(g_{ij} (x-y)^i (x-y)^j) + f(x,y) $$ where $ f(x,y) $ is continuous. I would be grateful if someone provide some clarification/reference on these facts, which I believe to be roughly true, but have never seen studied formally.","Let $ \Omega $ be a domain in $ \mathbb{R}^2 $ with a Riemannian metric $ g $, and let $\Delta $ be the laplace-beltrami operator induced by the metric, with dirichlet boundary conditions. For $ x,y \in \Omega $, let $ d(x,y) $ be the geodesic distance between $ x$ and $ y$. The green's function $ G(x,y) $ can be written as: $$ G(x,y) = -\frac{1}{4 \pi} \log(d(x,y)^2) + \omega(x,y) $$ where $\omega $ is some harmonic function that corrects for the boundary condition. Now for $ x$ and $ y $ close by one can write: $$ d(x,y)^2 = g_{ij} (x-y)^i (x-y)^j + O((x-y)^3) $$ Then one can write: $$ G(x,y) =  -\frac{1}{4 \pi} \log(g_{ij} (x-y)^i (x-y)^j) + f(x,y) $$ where $ f(x,y) $ is continuous. I would be grateful if someone provide some clarification/reference on these facts, which I believe to be roughly true, but have never seen studied formally.",,"['analysis', 'differential-geometry', 'partial-differential-equations']"
56,"To calculate a derivative of a set of points, is it more correct to interpolate finite differences or to derivate the interpolation?","To calculate a derivative of a set of points, is it more correct to interpolate finite differences or to derivate the interpolation?",,"I have a series of points extracted from numerical simulations. I also recently discovered the amazing power of finite differences. Nevertheless, I was used to estimate my derivatives from the analytic expression of the interpolation between my points. My question: Is it better, in the sense more correct : to calculate numerically the partial derivatives using finite differences and then interpolating them, or to interpolate my points and derivate analytically the interpolation? I feel like way 1 is better for my sets. I also imagine that interpolation or trend curves may have non-physical variations, e.g. Runge oscillations in the case of polynomial interpolation. Such oscillations would lead way 2 to being completely non-sense. Of course I did extensive testings on my sets of points, but I'd like a more rigorous answer than I'm able to give.","I have a series of points extracted from numerical simulations. I also recently discovered the amazing power of finite differences. Nevertheless, I was used to estimate my derivatives from the analytic expression of the interpolation between my points. My question: Is it better, in the sense more correct : to calculate numerically the partial derivatives using finite differences and then interpolating them, or to interpolate my points and derivate analytically the interpolation? I feel like way 1 is better for my sets. I also imagine that interpolation or trend curves may have non-physical variations, e.g. Runge oscillations in the case of polynomial interpolation. Such oscillations would lead way 2 to being completely non-sense. Of course I did extensive testings on my sets of points, but I'd like a more rigorous answer than I'm able to give.",,"['analysis', 'derivatives', 'numerical-methods', 'partial-derivative']"
57,Green's Function Divergence,Green's Function Divergence,,"Given a domain $ \Omega \in \mathbb{R}^2 $, and a PDE of the form $ L = a(x) \partial_x^2 + b(x) \partial_y ^2 $ for $ x \in \Omega $ , the green's function $ G(x,y) : \Omega \times \Omega \rightarrow R $ satisfies $ L G(p,q) = \delta(q-p) $ and $ G(p,q) = 0 $ for $ q \in \partial \Omega $. I would like to understand the divergences of the green's function near the diagonal. In particular, what sort of divergent terms will appear? My feeling is that there is only a  logarithmic divergences, and a dipole-like term. Reasoning is that roughly speaking, I should be able to take care of the $ \delta $ with a logarithm, and then correct with bounded functions. Precisely for $ x_0 \in \Omega $, split off the zeroth order term of the PDE. $$ L  = L_0 + L_r $$ where $$ L_0 =  a(x_0) \partial_x^2 + b(x_0) \partial_y ^2  + L_r  \\ L_r = ( da \cdot (x-x_0)\partial_x^2 +  db \cdot (x-x_0)\partial_x^2) + O((x-x_0)^2) $$ Now we know the Green's function for $ L_0 $, $ G_0(x,y) $, which will be a logarithm. Then one can correct for $ L_r $ by adding the term $$ L^{-1} \left( L_r G_0(x,y)  \right) $$ My intuition is that the linear part of $ L_r $ will produce a dipole term: $ L_r G_0(x,y) $ itself diverges, but in a small neighborhood $ B_\epsilon $, the ""total charge"" goes to zero. Apart from this, there will be no divergences. Is this true, and is there some source where this is studied?","Given a domain $ \Omega \in \mathbb{R}^2 $, and a PDE of the form $ L = a(x) \partial_x^2 + b(x) \partial_y ^2 $ for $ x \in \Omega $ , the green's function $ G(x,y) : \Omega \times \Omega \rightarrow R $ satisfies $ L G(p,q) = \delta(q-p) $ and $ G(p,q) = 0 $ for $ q \in \partial \Omega $. I would like to understand the divergences of the green's function near the diagonal. In particular, what sort of divergent terms will appear? My feeling is that there is only a  logarithmic divergences, and a dipole-like term. Reasoning is that roughly speaking, I should be able to take care of the $ \delta $ with a logarithm, and then correct with bounded functions. Precisely for $ x_0 \in \Omega $, split off the zeroth order term of the PDE. $$ L  = L_0 + L_r $$ where $$ L_0 =  a(x_0) \partial_x^2 + b(x_0) \partial_y ^2  + L_r  \\ L_r = ( da \cdot (x-x_0)\partial_x^2 +  db \cdot (x-x_0)\partial_x^2) + O((x-x_0)^2) $$ Now we know the Green's function for $ L_0 $, $ G_0(x,y) $, which will be a logarithm. Then one can correct for $ L_r $ by adding the term $$ L^{-1} \left( L_r G_0(x,y)  \right) $$ My intuition is that the linear part of $ L_r $ will produce a dipole term: $ L_r G_0(x,y) $ itself diverges, but in a small neighborhood $ B_\epsilon $, the ""total charge"" goes to zero. Apart from this, there will be no divergences. Is this true, and is there some source where this is studied?",,"['analysis', 'partial-differential-equations']"
58,Question on Morse lemma,Question on Morse lemma,,"I have this: (Page 421, heading Asymptotically quadratic functionals) Remark 2.2. (a) If $N$ is any neighbourhood of $x_0$, then the excision property of homology theory implies   $$C_k(f,x_0) \cong H_k(f^C \cap N, f^C \cap N - \{x_0\};R), \quad k\in\mathbb{Z}.$$   Using the Morse lemma, it follows that for a nondegenerate critical point $x_0$ with Morse index $\mu_0$, the critical groups are   $$C_k(f,x_0) \cong \delta_{k\mu_0} R = \begin{cases} R & \text{for } k=\mu_0; \\ 0 & \text{for } k\neq\mu_0. \end{cases}$$ I don’t understand how the Morse lemma is used here. Please could somebody explain?","I have this: (Page 421, heading Asymptotically quadratic functionals) Remark 2.2. (a) If $N$ is any neighbourhood of $x_0$, then the excision property of homology theory implies   $$C_k(f,x_0) \cong H_k(f^C \cap N, f^C \cap N - \{x_0\};R), \quad k\in\mathbb{Z}.$$   Using the Morse lemma, it follows that for a nondegenerate critical point $x_0$ with Morse index $\mu_0$, the critical groups are   $$C_k(f,x_0) \cong \delta_{k\mu_0} R = \begin{cases} R & \text{for } k=\mu_0; \\ 0 & \text{for } k\neq\mu_0. \end{cases}$$ I don’t understand how the Morse lemma is used here. Please could somebody explain?",,"['analysis', 'algebraic-topology', 'morse-theory']"
59,Tangent bundle of a surface is a manifold,Tangent bundle of a surface is a manifold,,"My differential geometry textbook defined the tangent bundle of a surface as the set of all tangent vectors to M at all points of M. The abstract patches are also given : $$y(p_{1},p_{2},p_{3},p_{4})=p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})$$ where $x$ is patch in surface $M$. $(p_{3},p_{4})$ is  vector part and $(p_{1},p_{2})$ is a point of application. I checked that $y$ is one-to-one. I also checked that $T(M)$ satisfies the covering property and the Hausdorff property. Thus if I show that it satisfies the smooth overlap property, $T(M)$ will become manifold. The smooth overlap property : For any patches $x,y$ in collection of patches $P$, the composite functions $y^{-1}x$ and $x^{-1}y$ are Euclidean differentiable. I assumed that $x,z$ are patches in M and $y_{1},y_{2}$ are patches in $T(M)$ $$y_{1}^{-1}y_{2}(p_{1},p_{2},p_{3},p_{4})=y_{1}^{-1}[p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})]=(q_{1},q_{2},q_{3},q_{4})$$ Then we get $$y(q_{1},q_{2},q_{3},q_{4})=q_{3}z_{u}(q_{1},q_{2})+q_{4}z_{v}(q_{1},q_{2})=p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})$$ Thus $q_{3}=p_{3}$, $q_{4}=p_{4}$, $z_{u}(q_{1},q_{2})=x_{u}(p_{1},p_{2})$, $z_{v}(q_{1},q_{2})=x_{v}(p_{1},p_{2})$. In conclusion, $$y_{1}^{-1}y_{2}(p_{1},p_{2},p_{3},p_{4})=((z_{u}^{-1}x_{u})(p_{1},p_{2}),p_{3},p_{4})$$ $$=(P_{1}((z_{u}^{-1}x_{u})(p_{1},p_{2}),P_{2}(z_{u}^{-1}x_{u})(p_{1},p_{2}),p_{3},p_{4})$$ where $P_{1},P_{2}$ is a projection function. Since $P(z_{u}^{-1}x_{u})(p_{1},p_{2})$ are differentiable, $y_{1}^{-1}y_{2}$ is differentiable. I think I solved it.","My differential geometry textbook defined the tangent bundle of a surface as the set of all tangent vectors to M at all points of M. The abstract patches are also given : $$y(p_{1},p_{2},p_{3},p_{4})=p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})$$ where $x$ is patch in surface $M$. $(p_{3},p_{4})$ is  vector part and $(p_{1},p_{2})$ is a point of application. I checked that $y$ is one-to-one. I also checked that $T(M)$ satisfies the covering property and the Hausdorff property. Thus if I show that it satisfies the smooth overlap property, $T(M)$ will become manifold. The smooth overlap property : For any patches $x,y$ in collection of patches $P$, the composite functions $y^{-1}x$ and $x^{-1}y$ are Euclidean differentiable. I assumed that $x,z$ are patches in M and $y_{1},y_{2}$ are patches in $T(M)$ $$y_{1}^{-1}y_{2}(p_{1},p_{2},p_{3},p_{4})=y_{1}^{-1}[p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})]=(q_{1},q_{2},q_{3},q_{4})$$ Then we get $$y(q_{1},q_{2},q_{3},q_{4})=q_{3}z_{u}(q_{1},q_{2})+q_{4}z_{v}(q_{1},q_{2})=p_{3}x_{u}(p_{1},p_{2})+p_{4}x_{v}(p_{1},p_{2})$$ Thus $q_{3}=p_{3}$, $q_{4}=p_{4}$, $z_{u}(q_{1},q_{2})=x_{u}(p_{1},p_{2})$, $z_{v}(q_{1},q_{2})=x_{v}(p_{1},p_{2})$. In conclusion, $$y_{1}^{-1}y_{2}(p_{1},p_{2},p_{3},p_{4})=((z_{u}^{-1}x_{u})(p_{1},p_{2}),p_{3},p_{4})$$ $$=(P_{1}((z_{u}^{-1}x_{u})(p_{1},p_{2}),P_{2}(z_{u}^{-1}x_{u})(p_{1},p_{2}),p_{3},p_{4})$$ where $P_{1},P_{2}$ is a projection function. Since $P(z_{u}^{-1}x_{u})(p_{1},p_{2})$ are differentiable, $y_{1}^{-1}y_{2}$ is differentiable. I think I solved it.",,"['analysis', 'differential-geometry', 'multivariable-calculus']"
60,Continuity of an Integral (Dominated Convergence),Continuity of an Integral (Dominated Convergence),,"I'm doing some reading in analysis, and am having trouble understanding the following idea. Given an integral $I(v) = \int f(v,w) dw$, we want to show that it's continuous as a function of $v$.  The source I'm reading says that this follows by Lesbesgue's theorem (I assume the D.C.T.) once we show that $I(v)$ is locally bounded. The function $f$ is continuous in $v$. I know we need to see that  $$\lim_{v\to v_0} |I(v) - I(v_0)| = \lim_{v\to v_0} |\int f(v,w)-f(v_0,w) dw| = 0 = \int \lim_{v \to v_0} |f(v_0,w)-f(v,w)| dw, $$ but I don't see how getting a bound $|I(v)| \leq M$ for $v$ near $v_0$ helps with that. You get $\lim_{v\to v_0} |I(v) - I(v_0)| \leq 2M,$ of course, but that doesn't seem like enough. The measure space is not finite, and there's not a lot of information about $f$ (not known to be bounded, etc). Any guidance would be appreciated. I think this must be simple, but I'm just not seeing it. Thanks. [Specifically, $w = (y,s) \in \mathbb{R}^2$ and $v = (x,t) \in \mathbb{R} \times \mathbb{R}^+$. The function $f$ is  $$ f(v,w) = f(x,t,y,s) = \frac{\chi_{[0,t]}(s)}{(t-s)^{1/2}}e^{i(x-y)^2/(4t-4s)} F(y,s)$$ where $F$ is a function which I know little about. ] To apply the D.C.T. to this function, I would need an integrable  $g(y,s)$ s.t.  $$ \frac{\chi_{(0,t_0+\delta)}(s)}{(t_0+\delta - s)^{1/2}}|F(y,s)| \leq g(y,s)$$ for fixed $t_0$ and all $\delta$ sufficiently small.  This doesn't seem possible, since the L.H.S. of the above approaches infinity as $s \to t_0 + \delta$, so there can't be a dominating function that works for all small $\delta$. Is there possibly another Lebesgue's theorem that would be applicable here?","I'm doing some reading in analysis, and am having trouble understanding the following idea. Given an integral $I(v) = \int f(v,w) dw$, we want to show that it's continuous as a function of $v$.  The source I'm reading says that this follows by Lesbesgue's theorem (I assume the D.C.T.) once we show that $I(v)$ is locally bounded. The function $f$ is continuous in $v$. I know we need to see that  $$\lim_{v\to v_0} |I(v) - I(v_0)| = \lim_{v\to v_0} |\int f(v,w)-f(v_0,w) dw| = 0 = \int \lim_{v \to v_0} |f(v_0,w)-f(v,w)| dw, $$ but I don't see how getting a bound $|I(v)| \leq M$ for $v$ near $v_0$ helps with that. You get $\lim_{v\to v_0} |I(v) - I(v_0)| \leq 2M,$ of course, but that doesn't seem like enough. The measure space is not finite, and there's not a lot of information about $f$ (not known to be bounded, etc). Any guidance would be appreciated. I think this must be simple, but I'm just not seeing it. Thanks. [Specifically, $w = (y,s) \in \mathbb{R}^2$ and $v = (x,t) \in \mathbb{R} \times \mathbb{R}^+$. The function $f$ is  $$ f(v,w) = f(x,t,y,s) = \frac{\chi_{[0,t]}(s)}{(t-s)^{1/2}}e^{i(x-y)^2/(4t-4s)} F(y,s)$$ where $F$ is a function which I know little about. ] To apply the D.C.T. to this function, I would need an integrable  $g(y,s)$ s.t.  $$ \frac{\chi_{(0,t_0+\delta)}(s)}{(t_0+\delta - s)^{1/2}}|F(y,s)| \leq g(y,s)$$ for fixed $t_0$ and all $\delta$ sufficiently small.  This doesn't seem possible, since the L.H.S. of the above approaches infinity as $s \to t_0 + \delta$, so there can't be a dominating function that works for all small $\delta$. Is there possibly another Lebesgue's theorem that would be applicable here?",,"['calculus', 'analysis']"
61,Monotonicity of a finite sum,Monotonicity of a finite sum,,"How can I prove that the following function $f(p)$ is non-increasing in $p$: \begin{align*} f(p)=\sum_{i=a}^{N}\left(1-\frac{1}{b \cdot(i-1)}\right)\binom{N}{i}(1-p)^ip^{N-i} \end{align*} where $N$ and $a$ ($2\le a \le N$) are integer constants and $b$ is a positive real constant? I verify this by plotting the function for certain values. Note: Without the first coefficient $\left(1-\frac{1}{b(i-1)}\right)$, which does not depend on $p$, the function is close to cdf of binomial distribution.","How can I prove that the following function $f(p)$ is non-increasing in $p$: \begin{align*} f(p)=\sum_{i=a}^{N}\left(1-\frac{1}{b \cdot(i-1)}\right)\binom{N}{i}(1-p)^ip^{N-i} \end{align*} where $N$ and $a$ ($2\le a \le N$) are integer constants and $b$ is a positive real constant? I verify this by plotting the function for certain values. Note: Without the first coefficient $\left(1-\frac{1}{b(i-1)}\right)$, which does not depend on $p$, the function is close to cdf of binomial distribution.",,"['analysis', 'binomial-coefficients', 'summation']"
62,Pointwise convergence and uniform convergence,Pointwise convergence and uniform convergence,,Suppose that $f$ has a uniformly continuous derivative. We define $\ f_n: \Bbb R\to\Bbb R  $  by $$\ f_n(x) = n \left( f \left(x + \frac{1}{n}\right) - f(x)\right) $$ Find a pointwise convergence $\ f_n$. Prove that the sequence $\ f_n$ converges uniformly to its limit.,Suppose that $f$ has a uniformly continuous derivative. We define $\ f_n: \Bbb R\to\Bbb R  $  by $$\ f_n(x) = n \left( f \left(x + \frac{1}{n}\right) - f(x)\right) $$ Find a pointwise convergence $\ f_n$. Prove that the sequence $\ f_n$ converges uniformly to its limit.,,"['real-analysis', 'analysis']"
63,Question on derivative,Question on derivative,,"I have this : And i don't understand (3.5) . i.e : why $\displaystyle\frac{d}{dt} G_t(\eta(t)u)=(G'_t(\eta),\eta ')+\partial_tG_t(\eta))$ Please Thank you .","I have this : And i don't understand (3.5) . i.e : why $\displaystyle\frac{d}{dt} G_t(\eta(t)u)=(G'_t(\eta),\eta ')+\partial_tG_t(\eta))$ Please Thank you .",,"['analysis', 'functional-analysis', 'derivatives']"
64,First time dealing with limits with complex numbers in it.,First time dealing with limits with complex numbers in it.,,"I am solving the following problem. Investigate the behavior (convergence of divergence) of $\Sigma a_n$ if $$a_n = \frac{1}{1+z^n}, \quad \text{ for } z \in \Bbb C.$$ First of all, I am not quite sure what it means when $\lim_{n \to \infty} z^n$ . I am thinking if $|z| < 1$ it goes to 0, but from my understanding, multiplying complex numbers is like multiplying the magnitude of the vectors and adding the angles of the vectors. So I am imagining a hand of a clock increasing its length and rotating over an over... to something analogous to $\infty$ . Second, I tried to algebraically deal with this if I could get anywhere with the ratio test. And I got $$\lim_{n \to \infty} |\frac{1+z^n}{1+z^{n+1}}| = \lim_{n \to \infty} |{1 \over z}| $$ and it will converge when $$|{1 \over z}| \lt 1.$$ Putting $z = a + bi$ , I found that $$\sqrt{1 \over {a^2+b^2}} \lt 1$$ is the necessary condition for the series to converge. Despite not knowing what it means, I got something. Am I on the right track ? Or am I dating myself here?","I am solving the following problem. Investigate the behavior (convergence of divergence) of if First of all, I am not quite sure what it means when . I am thinking if it goes to 0, but from my understanding, multiplying complex numbers is like multiplying the magnitude of the vectors and adding the angles of the vectors. So I am imagining a hand of a clock increasing its length and rotating over an over... to something analogous to . Second, I tried to algebraically deal with this if I could get anywhere with the ratio test. And I got and it will converge when Putting , I found that is the necessary condition for the series to converge. Despite not knowing what it means, I got something. Am I on the right track ? Or am I dating myself here?","\Sigma a_n a_n = \frac{1}{1+z^n}, \quad \text{ for } z \in \Bbb C. \lim_{n \to \infty} z^n |z| < 1 \infty \lim_{n \to \infty} |\frac{1+z^n}{1+z^{n+1}}| = \lim_{n \to \infty} |{1 \over z}|  |{1 \over z}| \lt 1. z = a + bi \sqrt{1 \over {a^2+b^2}} \lt 1","['analysis', 'complex-numbers']"
65,"Does $\lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=f(x_0,y_0)$?",Does ?,"\lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=f(x_0,y_0)","I'm wondering about the following situation. Suppose $f\colon\mathbb{R}^2\to \mathbb{R}$ is a function. If $f$ is continuous at $(x_0,y_0)$, why does $$ \lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=f(x_0,y_0)=\lim_{y\to y_0}\limsup_{x\to x_0}f(x,y) $$ and similarly if we take $\liminf$ instead of $\limsup$? Here I'm using the definition $$ \limsup_{y\to y_0}f(y)=\inf_{\epsilon>0}\sup_{y:|y-y_0|\leq\epsilon}f(y). $$ I'm unsure how to compute $\limsup_{y\to y_0}f(x,y)$ since I've only dealt with limit superiors of sequences, and limits of single variable functions. My guess is $\limsup_{y\to y_0}f(x,y)=f(x,y_0)$. After that, since $f$ is continuous at $(x_0,y_0)$, it is continuous at each coordinate, so $f(x,y_0)$ should be continuous at $x_0$, so $\lim_{x\to x_0}f(x,y_0)=f(x_0,y_0)$. I'm interested in seeing how you could rigorously compute $\limsup_{y\to y_0}f(x,y)$. I think I could try the other three situations once I see that. Thanks!","I'm wondering about the following situation. Suppose $f\colon\mathbb{R}^2\to \mathbb{R}$ is a function. If $f$ is continuous at $(x_0,y_0)$, why does $$ \lim_{x\to x_0}\limsup_{y\to y_0}f(x,y)=f(x_0,y_0)=\lim_{y\to y_0}\limsup_{x\to x_0}f(x,y) $$ and similarly if we take $\liminf$ instead of $\limsup$? Here I'm using the definition $$ \limsup_{y\to y_0}f(y)=\inf_{\epsilon>0}\sup_{y:|y-y_0|\leq\epsilon}f(y). $$ I'm unsure how to compute $\limsup_{y\to y_0}f(x,y)$ since I've only dealt with limit superiors of sequences, and limits of single variable functions. My guess is $\limsup_{y\to y_0}f(x,y)=f(x,y_0)$. After that, since $f$ is continuous at $(x_0,y_0)$, it is continuous at each coordinate, so $f(x,y_0)$ should be continuous at $x_0$, so $\lim_{x\to x_0}f(x,y_0)=f(x_0,y_0)$. I'm interested in seeing how you could rigorously compute $\limsup_{y\to y_0}f(x,y)$. I think I could try the other three situations once I see that. Thanks!",,"['real-analysis', 'analysis', 'limits', 'limsup-and-liminf']"
66,Differentiation in DFT,Differentiation in DFT,,"Let's consider some function $f: \mathbb R \to \mathbb R$ vanishing outside of some interval $(0,L)$. Then I can approximate the Fourier transform $\hat f$ by $$ \hat f (k) = \int_{\mathbb R} e^{-i k x} f(x) \mathrm d x = \int_0^L e^{-i k x} f(x) \mathrm d x \approx \sum_{j=0}^{N-1} e^{-i k x_j} f(x_j) \Delta x = \Delta x \sum_{j=0}^{N-1} e^{-i k j \Delta x} f_j, $$ where $\Delta x = \frac L N$ and $x_j = j \Delta x$. This permits the interpretation of the DFT $\hat f_l = \sum_{j=0}^{N-1} e^{-i 2 \pi lj/N} f_j$ as an approximation for the Fourier transform (except normalization) where the index $l$ belongs to the momentum/frequency $\frac{2 \pi l}{\Delta x N}$. For this approximation it is obvious that $\hat f(k + \frac{2 \pi}{\Delta x}) = \hat f(k)$ (or $\hat f_{l+N} = \hat f_l$). As we know the Fourier transform of $f'$ is $\widehat {(f')}(k) = - ik\hat f (k)$ which is clearly no more periodic. By which reason do I have to use momenta/frequencies symmetrically distributed around 0? (Do I have to do that?)","Let's consider some function $f: \mathbb R \to \mathbb R$ vanishing outside of some interval $(0,L)$. Then I can approximate the Fourier transform $\hat f$ by $$ \hat f (k) = \int_{\mathbb R} e^{-i k x} f(x) \mathrm d x = \int_0^L e^{-i k x} f(x) \mathrm d x \approx \sum_{j=0}^{N-1} e^{-i k x_j} f(x_j) \Delta x = \Delta x \sum_{j=0}^{N-1} e^{-i k j \Delta x} f_j, $$ where $\Delta x = \frac L N$ and $x_j = j \Delta x$. This permits the interpretation of the DFT $\hat f_l = \sum_{j=0}^{N-1} e^{-i 2 \pi lj/N} f_j$ as an approximation for the Fourier transform (except normalization) where the index $l$ belongs to the momentum/frequency $\frac{2 \pi l}{\Delta x N}$. For this approximation it is obvious that $\hat f(k + \frac{2 \pi}{\Delta x}) = \hat f(k)$ (or $\hat f_{l+N} = \hat f_l$). As we know the Fourier transform of $f'$ is $\widehat {(f')}(k) = - ik\hat f (k)$ which is clearly no more periodic. By which reason do I have to use momenta/frequencies symmetrically distributed around 0? (Do I have to do that?)",,"['analysis', 'numerical-methods']"
67,limiting value of a function,limiting value of a function,,"I have the following entire function defined $\forall x,y \in \mathbb{N}\,\,;\,\,x,y > 1$: $$f(s) = \sum_{n=0}^{\infty} \frac{s^n}{(x \,[n]\, y)n!}$$ Where here $x\, [n] \,y$ is the n'th hyper operator, so $x\,[0]\,y = x+y$, $x\,[1]\,y = x \cdot y$ and $x\,[2]\,y = x^y$ where generally the pattern to the operators is: $$x\, [n] \,(x\,[n+1]\,y) = x\,[n+1]\,(y+1)$$ and $$x \,[n]\,1 = x\,\,;\,\,\forall n > 0$$ This sequence of numbers tends to infinity much faster than factorial, double or triple exponential, or pretty much any elementary sequence. So the function f is entire. I'm wondering how I can find if $\lim_{s\to \infty}f(-s) = 0$ or not. Calculating or plotting this function to see evidence of this is impossible because computer memory is insufficient. I am very keen on this function and if it decays to zero I have a use for it. To generalize the question, I am wondering if perhaps there is some criteria an entire function $$g(s) =\sum_{n=0}^{\infty}a_n \frac{s^n}{n!}$$ has on $a_n$ so that $\lim_{x \to \infty} g(-x) = 0$","I have the following entire function defined $\forall x,y \in \mathbb{N}\,\,;\,\,x,y > 1$: $$f(s) = \sum_{n=0}^{\infty} \frac{s^n}{(x \,[n]\, y)n!}$$ Where here $x\, [n] \,y$ is the n'th hyper operator, so $x\,[0]\,y = x+y$, $x\,[1]\,y = x \cdot y$ and $x\,[2]\,y = x^y$ where generally the pattern to the operators is: $$x\, [n] \,(x\,[n+1]\,y) = x\,[n+1]\,(y+1)$$ and $$x \,[n]\,1 = x\,\,;\,\,\forall n > 0$$ This sequence of numbers tends to infinity much faster than factorial, double or triple exponential, or pretty much any elementary sequence. So the function f is entire. I'm wondering how I can find if $\lim_{s\to \infty}f(-s) = 0$ or not. Calculating or plotting this function to see evidence of this is impossible because computer memory is insufficient. I am very keen on this function and if it decays to zero I have a use for it. To generalize the question, I am wondering if perhaps there is some criteria an entire function $$g(s) =\sum_{n=0}^{\infty}a_n \frac{s^n}{n!}$$ has on $a_n$ so that $\lim_{x \to \infty} g(-x) = 0$",,"['real-analysis', 'analysis']"
68,The derivative of a family of flows,The derivative of a family of flows,,"If one has a family of flows, can one describe the derivative in the ""family"" direction? Specifically, let $M$ be a smooth manifold and let   $X_{s,t}$ be a 2-parameter family of fields on $M$. That is, it's a map $$ \mathbb{R}^2 \to \Gamma(TM), \qquad (s,t) \mapsto X_{s,t}. $$ This generates a 2-parameter family $\Phi_{s,t}$ of diffeomorphisms of $M$, but not uniquely. Let's choose one as follows: $$ \Phi_{s,t}(x) =  \text{the time $t$ flow of the $t$-dependent vector field $X_s(t)$}. $$ So we have a map $$ \Phi: \mathbb{R} \times \mathbb{R} \times M \to M, \qquad (s,t,x) \mapsto \Phi_{s,t}(x). $$ Question: Is there a concrete way to write down the $s$ component of the derivative of $\Phi$? That is, is there a formula for $D \Phi(\partial_s)$ in terms of $X$? For instance, by definition, the $t$ component of the derivative is simply $$ D\Phi|_{x,s,t}(\partial_t) = X_s(t). $$ Some remarks: To be explicit, the map $\Phi$ is as follows: For each $s$, consider the $t$-family of vector fields $X_s(t)$. One can flow the manifold along this $t$-family, and the image of $x$ at time $t$ is $\Phi_{s,t}(x)$.  (An inequivalent map would send $x$ to the time $s$ flow of the $s$-dependent vector field $X_t(s)$.) I believe $\Phi$ is smooth if one assumes that $X_{s,t}$ is smooth in both $s$ and $t$. (I don't know if the proof of the existence of solutions to ODEs guarantees regularity of families of flows.) This is probably either (a) a simple exercise with answers in terms of Lie derivatives and connections, or (b) unreasonable to ask. Regardless I haven't been able to write down any useful answer. Note there's a non-trivial $t$ dependence on $D\Phi(\partial_s)$. One can see this because at $t=0$, $D\Phi(\partial_s) = 0$, while for $t \neq 0$, the derivative is not zero on $\partial_s$. I haven't tried writing anything in terms of connections, as I didn't want to impose any metrics on the situation--clearly the answer is independent of any such choices--but I will certainly accept answers that impose a metric and write the answer in terms of the Levi-Civita connection. Here are some motivating examples: $f_s: M \to \mathbb{R}$ is a family of Morse functions, and you're curious how their time $t$ Morse flows are related. (Here, $\nabla f$ has no $t$-dependance.) Let $M$ be symplectic, and assume $H_{s,t}: M \to \mathbb{R}$ is a family of $t$-dependent Hamiltonians on $M$. Then one is interested in how the Hamiltonian flows of $H_{s,t}$ are related as one varies $s$.","If one has a family of flows, can one describe the derivative in the ""family"" direction? Specifically, let $M$ be a smooth manifold and let   $X_{s,t}$ be a 2-parameter family of fields on $M$. That is, it's a map $$ \mathbb{R}^2 \to \Gamma(TM), \qquad (s,t) \mapsto X_{s,t}. $$ This generates a 2-parameter family $\Phi_{s,t}$ of diffeomorphisms of $M$, but not uniquely. Let's choose one as follows: $$ \Phi_{s,t}(x) =  \text{the time $t$ flow of the $t$-dependent vector field $X_s(t)$}. $$ So we have a map $$ \Phi: \mathbb{R} \times \mathbb{R} \times M \to M, \qquad (s,t,x) \mapsto \Phi_{s,t}(x). $$ Question: Is there a concrete way to write down the $s$ component of the derivative of $\Phi$? That is, is there a formula for $D \Phi(\partial_s)$ in terms of $X$? For instance, by definition, the $t$ component of the derivative is simply $$ D\Phi|_{x,s,t}(\partial_t) = X_s(t). $$ Some remarks: To be explicit, the map $\Phi$ is as follows: For each $s$, consider the $t$-family of vector fields $X_s(t)$. One can flow the manifold along this $t$-family, and the image of $x$ at time $t$ is $\Phi_{s,t}(x)$.  (An inequivalent map would send $x$ to the time $s$ flow of the $s$-dependent vector field $X_t(s)$.) I believe $\Phi$ is smooth if one assumes that $X_{s,t}$ is smooth in both $s$ and $t$. (I don't know if the proof of the existence of solutions to ODEs guarantees regularity of families of flows.) This is probably either (a) a simple exercise with answers in terms of Lie derivatives and connections, or (b) unreasonable to ask. Regardless I haven't been able to write down any useful answer. Note there's a non-trivial $t$ dependence on $D\Phi(\partial_s)$. One can see this because at $t=0$, $D\Phi(\partial_s) = 0$, while for $t \neq 0$, the derivative is not zero on $\partial_s$. I haven't tried writing anything in terms of connections, as I didn't want to impose any metrics on the situation--clearly the answer is independent of any such choices--but I will certainly accept answers that impose a metric and write the answer in terms of the Levi-Civita connection. Here are some motivating examples: $f_s: M \to \mathbb{R}$ is a family of Morse functions, and you're curious how their time $t$ Morse flows are related. (Here, $\nabla f$ has no $t$-dependance.) Let $M$ be symplectic, and assume $H_{s,t}: M \to \mathbb{R}$ is a family of $t$-dependent Hamiltonians on $M$. Then one is interested in how the Hamiltonian flows of $H_{s,t}$ are related as one varies $s$.",,['analysis']
69,Finding the maximal $t$ satisfying a family of inequalities,Finding the maximal  satisfying a family of inequalities,t,"Given $c \in (0,1)$, find the maximal positive $t$ satisfying the following: $$\forall n \in \{1,2,\ldots \}: 1+\frac{c}{n+(1-c)} \le \left(1+\frac{1}{n+t}\right)^{c}$$ My progress thus far: A special case is $c = \frac{1}{2}$: the inequality simplifies to $3t-1 \le n(1-4t)$, which implies $t \le \frac{1}{4}$. $t=\frac{1}{4}$ actually works. The inequality $\left(1+\frac{1}{n+t}\right)^{c} \le \frac{c}{n+t}$ shows $t \le 1-c$.","Given $c \in (0,1)$, find the maximal positive $t$ satisfying the following: $$\forall n \in \{1,2,\ldots \}: 1+\frac{c}{n+(1-c)} \le \left(1+\frac{1}{n+t}\right)^{c}$$ My progress thus far: A special case is $c = \frac{1}{2}$: the inequality simplifies to $3t-1 \le n(1-4t)$, which implies $t \le \frac{1}{4}$. $t=\frac{1}{4}$ actually works. The inequality $\left(1+\frac{1}{n+t}\right)^{c} \le \frac{c}{n+t}$ shows $t \le 1-c$.",,['analysis']
70,Lemma of Whitehead,Lemma of Whitehead,,"this is the lemma of Whitehead And i really don't understand the proof How to see that $k$ is well defined (i.e how to write an element from $X\cup_{\varphi_i}e^{\lambda} , i=0,1$ ) and how to write $l$ ? Please Thank you","this is the lemma of Whitehead And i really don't understand the proof How to see that $k$ is well defined (i.e how to write an element from $X\cup_{\varphi_i}e^{\lambda} , i=0,1$ ) and how to write $l$ ? Please Thank you",,"['analysis', 'algebraic-topology', 'definition', 'homotopy-theory']"
71,Geometrical Inequality,Geometrical Inequality,,"Let $ABCD$ be a quadrilateral on the unit circle, and the diagonals $AC$ and $BD$ intersects at $E$. If the shortest height of the triangle $ACD$ equals the radius of the incircle of the triangle $ABE$ that is denoted by $r$, is $r$ less than $0.4$?","Let $ABCD$ be a quadrilateral on the unit circle, and the diagonals $AC$ and $BD$ intersects at $E$. If the shortest height of the triangle $ACD$ equals the radius of the incircle of the triangle $ABE$ that is denoted by $r$, is $r$ less than $0.4$?",,"['calculus', 'real-analysis', 'geometry', 'analysis', 'inequality']"
72,"Is the $ϵ,δ$ definition of a limit not well-defined? [closed]",Is the  definition of a limit not well-defined? [closed],"ϵ,δ","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . Improve this question I just watched this youtube video: http://www.youtube.com/watch?v=K4eAyn-oK4M He lays out his objections against the $ϵ,δ$ definition around 14 min. Here is the discription of the video: In this video we aim to give a precise and simpler definition for what   it means to say that: a rational polynumber on-sequence p(n) has a   limit A, for some rational number A. Our definition is both much   simpler and more logical than the usual epsilon -delta definition   found in calculus texts. What is required is that we need to find two   natural numbers: k called the scale, and m called the start that allow   us to bound in a pretty simple way the difference between p(n) and A. The epsilon-delta definition of a limit is usually considered a high   point of logical rigour. Not so. It is also considered too logically   involving to be taken seriously as a pedagogical pillar for most   undergrads. Hence students may be told about the definition, but are   not required to seriously understand it, or be able to use it--unless   they are prospective maths majors. There is a subtle ambiguity in the definition: given an epsilon we are   supposed to demonstrate there is a delta (with certain properties) but   how are we to do this, since an potential infinity of epsilons are   involved? In practice what is required is a correspondence   (function/relation etc) between epsilon and delta but the nature of   this required correspondence is not clear. We return to our familiar   conundrum of using the work``function'' without a proper definition of   it. The key point that makes our simpler more intuitive notion of limit of   a sequence work is that we are dealing with very particular and   clearly defined on-sequences: those generated by a rationl polynumber.   A good example of the benefits of being careful rather than casual   when dealing with the foundations of analysis! My question is: Is this an opinion shared by more mathematicians ? I kind a feel like that this Professor of the University of New South Wales is standing completely alone as it comes to this. I don't really undestand his objections, but I don't think I'm skilled enough to understand if his objections are legit.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 2 years ago . Improve this question I just watched this youtube video: http://www.youtube.com/watch?v=K4eAyn-oK4M He lays out his objections against the $ϵ,δ$ definition around 14 min. Here is the discription of the video: In this video we aim to give a precise and simpler definition for what   it means to say that: a rational polynumber on-sequence p(n) has a   limit A, for some rational number A. Our definition is both much   simpler and more logical than the usual epsilon -delta definition   found in calculus texts. What is required is that we need to find two   natural numbers: k called the scale, and m called the start that allow   us to bound in a pretty simple way the difference between p(n) and A. The epsilon-delta definition of a limit is usually considered a high   point of logical rigour. Not so. It is also considered too logically   involving to be taken seriously as a pedagogical pillar for most   undergrads. Hence students may be told about the definition, but are   not required to seriously understand it, or be able to use it--unless   they are prospective maths majors. There is a subtle ambiguity in the definition: given an epsilon we are   supposed to demonstrate there is a delta (with certain properties) but   how are we to do this, since an potential infinity of epsilons are   involved? In practice what is required is a correspondence   (function/relation etc) between epsilon and delta but the nature of   this required correspondence is not clear. We return to our familiar   conundrum of using the work``function'' without a proper definition of   it. The key point that makes our simpler more intuitive notion of limit of   a sequence work is that we are dealing with very particular and   clearly defined on-sequences: those generated by a rationl polynumber.   A good example of the benefits of being careful rather than casual   when dealing with the foundations of analysis! My question is: Is this an opinion shared by more mathematicians ? I kind a feel like that this Professor of the University of New South Wales is standing completely alone as it comes to this. I don't really undestand his objections, but I don't think I'm skilled enough to understand if his objections are legit.",,"['real-analysis', 'analysis', 'soft-question']"
73,Closed form formula for a double series related to wave equation,Closed form formula for a double series related to wave equation,,Does anyone have a closed form formula for the double series $$\sum_{n=0}^\infty\sum_{m=0}^\infty \frac{(-1)^{m+n}}{(2m+1)^3(2n+1)^3}\cos\left(\pi t \sqrt{(2m+1)^2+(2n+1)^2}\right)?$$ This is related to a solution of a two dimensional wave equation.,Does anyone have a closed form formula for the double series $$\sum_{n=0}^\infty\sum_{m=0}^\infty \frac{(-1)^{m+n}}{(2m+1)^3(2n+1)^3}\cos\left(\pi t \sqrt{(2m+1)^2+(2n+1)^2}\right)?$$ This is related to a solution of a two dimensional wave equation.,,"['sequences-and-series', 'complex-analysis', 'analysis', 'partial-differential-equations', 'special-functions']"
74,fancy about some properties of kernel functions at infinity,fancy about some properties of kernel functions at infinity,,"Consider the two common types of kernel functions $\sum\limits_{t=a}^bf(t)K(x,t)$ and $\int_a^bf(t)K(x,t)~dt$ , prove whether the following properties are correct or not: $1.$ If $K(x,t)$ is bounded but not converge with respect to positive (or negative) $x$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should be indeterminate. $2.$ If $\lim\limits_{x\to+\infty}K(x,t)=0$ $\biggl(\text{or}\lim\limits_{x\to-\infty}K(x,t)=0\biggr)$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should equal to $0$ . $3.$ If $\lim\limits_{x\to+\infty}K(x,t)=\infty$ $\biggl(\text{or}\lim\limits_{x\to-\infty}K(x,t)=\infty\biggr)$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should tend to $\infty$ .","Consider the two common types of kernel functions $\sum\limits_{t=a}^bf(t)K(x,t)$ and $\int_a^bf(t)K(x,t)~dt$ , prove whether the following properties are correct or not: $1.$ If $K(x,t)$ is bounded but not converge with respect to positive (or negative) $x$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should be indeterminate. $2.$ If $\lim\limits_{x\to+\infty}K(x,t)=0$ $\biggl(\text{or}\lim\limits_{x\to-\infty}K(x,t)=0\biggr)$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should equal to $0$ . $3.$ If $\lim\limits_{x\to+\infty}K(x,t)=\infty$ $\biggl(\text{or}\lim\limits_{x\to-\infty}K(x,t)=\infty\biggr)$ , then $\lim\limits_{x\to+\infty}\sum\limits_{t=a}^bf(t)K(x,t)$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\sum\limits_{t=a}^bf(t)K(x,t)\biggr)$ and $\lim\limits_{x\to+\infty}\int_a^bf(t)K(x,t)~dt$ $\biggl(\text{or}\lim\limits_{x\to-\infty}\int_a^bf(t)K(x,t)~dt\biggr)$ should tend to $\infty$ .",,"['analysis', 'definite-integrals', 'integral-transforms']"
75,Gelfand-Leray integral for forms with noncompact support,Gelfand-Leray integral for forms with noncompact support,,"Let $\omega$ be a smooth $n$-form with compact support on domain $\Omega \subseteq{\mathbb{R}^n}$ and let $f \colon \Omega \to \mathbb{R}$ be a smooth function with nonvanishing differential. Then for any point $x \in \Omega$ there exists a neighborhood $U_\alpha$ in which we can take $f(x)$ as one of local coordinates and so there exists a smooth form $\psi_\alpha$ defined in this neighborhood with property $df \wedge \psi_\alpha = \omega$. Restriction of $\psi_\alpha$ on any level set $f(x)=t$ is unique. Let $U_{\alpha}$ be a finite cover of support of $\omega$ with such neighborhoods and $\chi_\alpha$ be the associated partition of unity. The form $$    \frac{\omega}{d f} = \sum_\alpha \chi_\alpha \psi_\alpha  $$ is then the unique smooth form on each level set such that $df \wedge \frac{\omega}{df} = \omega$. This form is called Gelfand-Leray form and its integral $$     J(t) = \int\limits_{f(x)=t} \frac{\omega}{df} $$ is called Gelfand-Leray function. It has the property that $$    \int\limits_\Omega \omega = \int\limits_{-\infty}^{+\infty} J(t) \, dt. \tag{1} $$ But is it possible to say something about existence of Gelfand-Leray form $\frac{\omega}{d f}$ in the case when $\omega$ is not compactly supported? Where can I read about this case? Leaving aside question of existence let $\frac{\omega}{df}$ be such form. In which case the equalty (1) holds? In book [1] one says that to proove this we have to use the Fubini theorem. References [1] V. I. Arnold, S. M. Gusein-Zade, A. N. Varchenko , Singularities of Differentiable Maps, V.2, 1988","Let $\omega$ be a smooth $n$-form with compact support on domain $\Omega \subseteq{\mathbb{R}^n}$ and let $f \colon \Omega \to \mathbb{R}$ be a smooth function with nonvanishing differential. Then for any point $x \in \Omega$ there exists a neighborhood $U_\alpha$ in which we can take $f(x)$ as one of local coordinates and so there exists a smooth form $\psi_\alpha$ defined in this neighborhood with property $df \wedge \psi_\alpha = \omega$. Restriction of $\psi_\alpha$ on any level set $f(x)=t$ is unique. Let $U_{\alpha}$ be a finite cover of support of $\omega$ with such neighborhoods and $\chi_\alpha$ be the associated partition of unity. The form $$    \frac{\omega}{d f} = \sum_\alpha \chi_\alpha \psi_\alpha  $$ is then the unique smooth form on each level set such that $df \wedge \frac{\omega}{df} = \omega$. This form is called Gelfand-Leray form and its integral $$     J(t) = \int\limits_{f(x)=t} \frac{\omega}{df} $$ is called Gelfand-Leray function. It has the property that $$    \int\limits_\Omega \omega = \int\limits_{-\infty}^{+\infty} J(t) \, dt. \tag{1} $$ But is it possible to say something about existence of Gelfand-Leray form $\frac{\omega}{d f}$ in the case when $\omega$ is not compactly supported? Where can I read about this case? Leaving aside question of existence let $\frac{\omega}{df}$ be such form. In which case the equalty (1) holds? In book [1] one says that to proove this we have to use the Fubini theorem. References [1] V. I. Arnold, S. M. Gusein-Zade, A. N. Varchenko , Singularities of Differentiable Maps, V.2, 1988",,"['analysis', 'reference-request', 'differential-geometry']"
76,Why does the Gamma function interpolate $(n-1)!$?,Why does the Gamma function interpolate ?,(n-1)!,Why does the Gamma function interpolate $(n-1)!$ and not $n!$ instead? What is the historical reason?,Why does the Gamma function interpolate $(n-1)!$ and not $n!$ instead? What is the historical reason?,,"['real-analysis', 'analysis', 'special-functions', 'gamma-function']"
77,an identity regarding the summation,an identity regarding the summation,,"Assuming necessary convergence, why does the following identity hold? $$\sum_{n=1}^\infty\sum_{k=0}^{n-1} a_n b_k = \sum_{k=0}^\infty\sum_{n=k+1}^{\infty} a_n b_k$$ Intuitively this is clear, but what's the formal justification? Thanks!","Assuming necessary convergence, why does the following identity hold? $$\sum_{n=1}^\infty\sum_{k=0}^{n-1} a_n b_k = \sum_{k=0}^\infty\sum_{n=k+1}^{\infty} a_n b_k$$ Intuitively this is clear, but what's the formal justification? Thanks!",,"['analysis', 'summation']"
78,Looking for an analysis book which uses linear maps notation for multivariable differentiation,Looking for an analysis book which uses linear maps notation for multivariable differentiation,,"I'm taking an analysis course and I find it quite hard to follow what the professor is saying. So far we've been following elementary real analysis by bruckner^2 and Thompson but for the topic on multivariable differentiation, my professor opted not to follow the textbook. He mentioned that the mentioned book's approach is rather painful and introduced the following  approach in class via a lemma by Hadamard. Everything else we discuss is built on this lemma which places emphasis on linear maps, space of linear maps, etc. The Lemma is as follows: Let $U \subset \mathbb{R}^d$ be open and $f:U\rightarrow \mathbb{R}^n$ then $f$ is differentiable at $a \Leftrightarrow \exists \,\,\,\gamma_a:U\rightarrow L(\mathbb{R}^d,\mathbb{R^n})$ continuous at $a$ such that $$f(x)=f(a)+\gamma_a(x-a)$$ Here, $L$ is the space of linear maps. I am wondering if anyone here knows of a book which follows such an approach in the study of multivariable differentiation. Honestly, I understand what the professor is saying but the notation he has presented so far is what I find confusing. I'd prefer to be able to read a book as I don't catch everything during the lecture. Thanks for the help/suggestions!","I'm taking an analysis course and I find it quite hard to follow what the professor is saying. So far we've been following elementary real analysis by bruckner^2 and Thompson but for the topic on multivariable differentiation, my professor opted not to follow the textbook. He mentioned that the mentioned book's approach is rather painful and introduced the following  approach in class via a lemma by Hadamard. Everything else we discuss is built on this lemma which places emphasis on linear maps, space of linear maps, etc. The Lemma is as follows: Let $U \subset \mathbb{R}^d$ be open and $f:U\rightarrow \mathbb{R}^n$ then $f$ is differentiable at $a \Leftrightarrow \exists \,\,\,\gamma_a:U\rightarrow L(\mathbb{R}^d,\mathbb{R^n})$ continuous at $a$ such that $$f(x)=f(a)+\gamma_a(x-a)$$ Here, $L$ is the space of linear maps. I am wondering if anyone here knows of a book which follows such an approach in the study of multivariable differentiation. Honestly, I understand what the professor is saying but the notation he has presented so far is what I find confusing. I'd prefer to be able to read a book as I don't catch everything during the lecture. Thanks for the help/suggestions!",,"['real-analysis', 'analysis', 'derivatives']"
79,Rigorous hypothesis for Reynolds' transport theorem,Rigorous hypothesis for Reynolds' transport theorem,,"I'm looking for rigorous hypothesis for the application of Reynolds' transport theorem : $$   \frac{\mathrm{d}}{\mathrm{d}t}\left[     \int_{\Omega(t)}       \phi({\bf x},t)     \mathrm{d}{\bf x}   \right] =   \int_{\Omega(t)}      \frac{\partial}{\partial t}\phi({\bf x},t)   \mathrm{d}{\bf x} +   \int_{\partial\Omega(t)}     \phi({\bf x},t)\frac{\mathrm{d} {\bf x}}{\mathrm{d} t}.{\bf n}_b   \mathrm{d}{\bf x}, $$ where $\Omega(t)$ is a piecewice smooth manifold with boundaries (a portion of a polyhedron for instance), $\partial\Omega(t)$ is the boundary of $\Omega(t)$ and ${\bf n}_b$ is the normal of $\partial\Omega(t)$ at ${\bf x}$. In particular, provided all the integrals are well defined of course, I'm interested in the case when $\phi$ is only piecewise smooth, and when the parameterizations I can find of the boundary $\partial\Omega(t)$ are only piecewise continuously differentiable. Thanks for any help.","I'm looking for rigorous hypothesis for the application of Reynolds' transport theorem : $$   \frac{\mathrm{d}}{\mathrm{d}t}\left[     \int_{\Omega(t)}       \phi({\bf x},t)     \mathrm{d}{\bf x}   \right] =   \int_{\Omega(t)}      \frac{\partial}{\partial t}\phi({\bf x},t)   \mathrm{d}{\bf x} +   \int_{\partial\Omega(t)}     \phi({\bf x},t)\frac{\mathrm{d} {\bf x}}{\mathrm{d} t}.{\bf n}_b   \mathrm{d}{\bf x}, $$ where $\Omega(t)$ is a piecewice smooth manifold with boundaries (a portion of a polyhedron for instance), $\partial\Omega(t)$ is the boundary of $\Omega(t)$ and ${\bf n}_b$ is the normal of $\partial\Omega(t)$ at ${\bf x}$. In particular, provided all the integrals are well defined of course, I'm interested in the case when $\phi$ is only piecewise smooth, and when the parameterizations I can find of the boundary $\partial\Omega(t)$ are only piecewise continuously differentiable. Thanks for any help.",,['analysis']
80,Different functional brachystochrone,Different functional brachystochrone,,"Until today I thought that $$ \int_0^b \sqrt{\frac{1+y'(x)^2}{2gy(x)}} dx$$ would be the only functional to derive the brachystochrone, but in the textbook Variational Methods in Mathematical Physics by Blanchard and Brünining I found the following integral: $$ \int_0^b  \sqrt{\frac{1+y'(x)^2}{2gx}} dx $$ Is this possible? If yes, how can one derive this equation? I might have found something that could have to do with this but I am not sure whether it is the right reasoning, namely: $ds^2=\frac{dx^2+dy^2}{2x}$ and $ds^2=\frac{dx^2+dy^2}{2y}$ are both line elements for a cycloidal metric .","Until today I thought that $$ \int_0^b \sqrt{\frac{1+y'(x)^2}{2gy(x)}} dx$$ would be the only functional to derive the brachystochrone, but in the textbook Variational Methods in Mathematical Physics by Blanchard and Brünining I found the following integral: $$ \int_0^b  \sqrt{\frac{1+y'(x)^2}{2gx}} dx $$ Is this possible? If yes, how can one derive this equation? I might have found something that could have to do with this but I am not sure whether it is the right reasoning, namely: $ds^2=\frac{dx^2+dy^2}{2x}$ and $ds^2=\frac{dx^2+dy^2}{2y}$ are both line elements for a cycloidal metric .",,"['real-analysis', 'analysis']"
81,Smooth dependence of diagonalization,Smooth dependence of diagonalization,,"I guess the following question refers to a standard result, but I cannot find a good reference or figure out the details. Suppose $S \in \mathbb{R}^{n\times n}$ is symmetric and positive definite and $A \in \mathbb{R}^{n \times n}$ is self-adjoint with respect to $S$, i.e. $A^T S = S A$. Then it follows from basic linear algebra that there exists a decomposition $$ A = U^T D U$$ where $D\in \mathbb{R}^{n\times n}$ is a diagonal matrix and $U \in \text{GL}(n,\mathbb{R})$ is $S$-orthogonal, i.e. $U^T S U = I$ where $I$ denotes the identity matrix. I wonder now if the matrices $(U,D)$ depend smoothly on $S$ and $A$? To make this more precise, replace $S$ by a smooth family $S_t$ of positive definite matrices and let $A_t$ be a smooth family satisfying $A_t^T S_t = S_t A_t$. As above there exists for every fixed $t$ a decomposition $$A_t = U_t^T D_t U^T_t$$ with a diagonal matrix $D_t$ and a $S_t$-orthogonal matrix $U_t$. But is it possible to choose $U_t$ and $D_t$ to depend smooth on $t$? I think it should be possible to solve this problem by the implicit function theorem, but I cannot figure out the details..","I guess the following question refers to a standard result, but I cannot find a good reference or figure out the details. Suppose $S \in \mathbb{R}^{n\times n}$ is symmetric and positive definite and $A \in \mathbb{R}^{n \times n}$ is self-adjoint with respect to $S$, i.e. $A^T S = S A$. Then it follows from basic linear algebra that there exists a decomposition $$ A = U^T D U$$ where $D\in \mathbb{R}^{n\times n}$ is a diagonal matrix and $U \in \text{GL}(n,\mathbb{R})$ is $S$-orthogonal, i.e. $U^T S U = I$ where $I$ denotes the identity matrix. I wonder now if the matrices $(U,D)$ depend smoothly on $S$ and $A$? To make this more precise, replace $S$ by a smooth family $S_t$ of positive definite matrices and let $A_t$ be a smooth family satisfying $A_t^T S_t = S_t A_t$. As above there exists for every fixed $t$ a decomposition $$A_t = U_t^T D_t U^T_t$$ with a diagonal matrix $D_t$ and a $S_t$-orthogonal matrix $U_t$. But is it possible to choose $U_t$ and $D_t$ to depend smooth on $t$? I think it should be possible to solve this problem by the implicit function theorem, but I cannot figure out the details..",,"['linear-algebra', 'analysis']"
82,Copies of finite sets in sets of positive measure,Copies of finite sets in sets of positive measure,,"We say a set $A \subseteq \mathbb{R}^n$ contains the pattern of a finite set $B \subseteq \mathbb{R}^n$ if there exists a shift $t \in \mathbb{R}^n$ and scale $s > 0$ such that $t+sB \subseteq A$. I've read that if $A$ has positive measure, then for any choice of finite set $B$, $A$ contains the pattern of $B$, but proofs are never provided (they say it is ""clear""), and I don't know how to show this rigorously. I was able to show this in the easier case when $B \subseteq \mathbb{R}$ is of the form $\{x,x+y,x+2y\}$ by using the Lebesgue density theorem and considering a sufficiently small ball around a density point intersected with $A$, say with measure at least 3/4 of that of the ball (anything strictly larger than 1/2 will do) and making a measure argument, but I needed the symmetry of the set $B$ around the center $x+y$ in order for my argument to work. How would one prove this fact in full generality?","We say a set $A \subseteq \mathbb{R}^n$ contains the pattern of a finite set $B \subseteq \mathbb{R}^n$ if there exists a shift $t \in \mathbb{R}^n$ and scale $s > 0$ such that $t+sB \subseteq A$. I've read that if $A$ has positive measure, then for any choice of finite set $B$, $A$ contains the pattern of $B$, but proofs are never provided (they say it is ""clear""), and I don't know how to show this rigorously. I was able to show this in the easier case when $B \subseteq \mathbb{R}$ is of the form $\{x,x+y,x+2y\}$ by using the Lebesgue density theorem and considering a sufficiently small ball around a density point intersected with $A$, say with measure at least 3/4 of that of the ball (anything strictly larger than 1/2 will do) and making a measure argument, but I needed the symmetry of the set $B$ around the center $x+y$ in order for my argument to work. How would one prove this fact in full generality?",,['analysis']
83,Polar representation of continuous curve.,Polar representation of continuous curve.,,"Given a continuous curve $\gamma:[a,b]\to\Bbb R^2$ with $\gamma(t)\ne(0,0)^T$, can I always find continous functions $\rho:[a,b]\to\Bbb [0,\infty)$ and $\theta:[a,b]\to\Bbb R$ so that $\gamma(t)=\rho(t)(cos(\theta(t)),sin(\theta(t)))^T$ ?. Do these functions have the same smoothness properies as $\gamma$? Thanks","Given a continuous curve $\gamma:[a,b]\to\Bbb R^2$ with $\gamma(t)\ne(0,0)^T$, can I always find continous functions $\rho:[a,b]\to\Bbb [0,\infty)$ and $\theta:[a,b]\to\Bbb R$ so that $\gamma(t)=\rho(t)(cos(\theta(t)),sin(\theta(t)))^T$ ?. Do these functions have the same smoothness properies as $\gamma$? Thanks",,"['real-analysis', 'general-topology', 'analysis']"
84,Boundedness for Reaction Diffusion BVP with Arbitrary Exponent $\alpha$,Boundedness for Reaction Diffusion BVP with Arbitrary Exponent,\alpha,"Let $U\subset\mathbb{R}^{n}$ be open, $U_{T}$ and $\Gamma_{T}$ be the parabolic cylinder and boundary of $U$ for arbitrary $0\leq t\leq T$, respectively, and suppose $u$ solves $$ \left\{\begin{array}{rl} u_{t}-\Delta u=u^{\alpha}&\text{in}\;U_{T}\\ u=g&\text{on}\;\Gamma_{T}\end{array}\right. $$ where $0\leq g\leq1$.  If $u\in\mathscr{C}^{2,1}(U_{T})\cap\mathscr{C}(U_{T}\cup\Gamma_{T})$, then determine which values of $\alpha$ yield uniformly bounded solutions for all $T>0$. First of all, I just want to say I am only starting to learn about PDEs from Evans' text, and am only on Chapter 2 of Part I, which deals with the usual linear second order PDE.  I suspect the restriction of initial/boundary conditions to some $g$ with $0\leq g\leq1$ is probably fairly significant in simplifying the problem.  But even for this case, I am not sure how to attack the problem.  But is it reasonable to expect that one should know how to analyze the solutions to this PDE for arbitrary $g$? Some of the ways I tried to attack the problem included finding a simplifying substitution (for example, when $\alpha=1$ I was able to use $u=e^{t}v$ to eliminate the RHS term), but I could not think of any that would work for all $\alpha$.  Next I tried applying usual energy techniques, but this did not seem to lead anywhere fruitful (things like the arbitrariness of $\alpha$ and the non-periodic boundary conditions obstructed a clear path to making use any possible candidate for the energy functional). Next I wondered if solutions to the PDE itself (without any specified boundary conditions) satisfies a maximum principle.  But I found this hard to prove using straightforward techniques like $\epsilon$ regularization, and the methods Evans uses to prove the strong maximum principle for just the heat equation via time-dependent mean-value properties was already complicated enough as it was to even think about extending the arguments to the non-linear PDE above.  On the other hand, the non-linearity likely precludes this possibility, at least for all $\alpha$.  Furthermore, if for all $\alpha$ the equation did satisfy a weak maximum principle, then it would imply the answer is all $\alpha\in\mathbb{R}$, and this also seems unlikely.  Why it seems unlikely is related to my next thought in considering the ODE $$u_{t}=u^{\alpha}.$$ For $t>0$ and $0\leq u(0)=g(\cdot,0)\leq1$, solutions to this ODE exhibit a wide variety of behavior depending on $\alpha$, and even lacks uniqueness in many cases.  But in general, either the solution blows up (e.g. $\alpha\geq2$), grows exponentially (e.g. $\alpha=1$), or else grows in some sort of algebraic fashion like $O(x^{\beta})$ (correct me if I am wrong, please!). The point of this breakdown into various classes of solutions to the above ODE is to somehow relate the behavior back to the behavior of solutions to the PDE with the diffusion term $\Delta u$ added back.  Heuristically, the diffusion term $\Delta u$ ""smoothes"" the solution and ""spreads"" it out over the spatial domain $U$ as $t>0$ progreses.  On the other hand, $u^{\alpha}$ heuristically causes growth in the solution in space as $t>0$ evolves.  If the growth term $u^{\alpha}$ overpowers the diffusion term $\Delta u$, then it seems the solution would not remain uniformly bounded in $t$, and if the growth term is very strong, then singularities could develop in finite time $t$.  Anyway, that is all fine (I think), but the practical question is how does one convert heuristics about the PDE into specific quantitative information/estimates about the solution (in this case, uniform $t$-boundedness of $u$ as a function of $\alpha$)? One last comment.  I haven't really considered (from a purely heuristic point of view) the boundary terms, and the special assumption $0\leq g\leq1$ (I would still like to know in any answer how we could generalize $g$ a bit more, if it can be reasonably done).  $U$ is of course bounded, so the diffusion of the system is constrained to a bounded region kept at some constant positive density on the boundary.  This would seem to me that the growth term inevitably dominates, since the solution will continuously grow and grow (as mass is added into the system restricted to a bounded region).  Whether the solution develops singularities would depend on the strength of the diffusion term.  If the diffusion term is strong enough, then the mass added to the system has enough time to spread out, whereas if it is too weak, then the concentration will spike into a singularity at some point(s) in $U$.  Either way, it would appear that unless $g(x,0)\equiv0$, the solution is at best not uniformly bounded, and at worst actually blows up in finite time $t<\infty.$  Maybe this answers the question?  If it helps, I was only asked to ""explain"" my answer, so given the non-linearity of the PDE itself, perhaps we aren't expected to provide a rigorous argument, though I would like to develop one if it is possible. Any hints are appreciated -- thanks! EDIT 1: I should clarify since I don't know if the terminology parabolic boundary/cylinder is standard.  $U$ is an open subset of $\mathbb{R}^{n}$ (spatial region) and $$U_{T}:=U\times(0,T],$$ $$\Gamma_{T}:=\partial U\times[0,T]\cup U\times\{t=0\}=\overline{U_{T}}-U_{T}.$$ EDIT 2: For problems addressing behavior, such as boundedness, for large $t$, is it helpful to consider the time-independent case?  e.g. the elliptic PDE $$-\Delta u=u^{\alpha}.$$ In other words, if the solution to this elliptic PDE (with appropriate boundary conditions as intimated by the original parabolic-type PDE) is bounded, could we somehow deduce boundedness (and hence uniform boundedness) for all $t$ before ""$t=\infty$?""  I guess the only obstruction to this would be the possibility of singularities developing and then evaporating over time, which doesn't seem likely (though I am not going to say it's impossible, since I have no mathematical justification for it, and also black holes can be thought of as ""singularities"" which evaporate over large periods of time as they radiate energy, this probably going way off topic).","Let $U\subset\mathbb{R}^{n}$ be open, $U_{T}$ and $\Gamma_{T}$ be the parabolic cylinder and boundary of $U$ for arbitrary $0\leq t\leq T$, respectively, and suppose $u$ solves $$ \left\{\begin{array}{rl} u_{t}-\Delta u=u^{\alpha}&\text{in}\;U_{T}\\ u=g&\text{on}\;\Gamma_{T}\end{array}\right. $$ where $0\leq g\leq1$.  If $u\in\mathscr{C}^{2,1}(U_{T})\cap\mathscr{C}(U_{T}\cup\Gamma_{T})$, then determine which values of $\alpha$ yield uniformly bounded solutions for all $T>0$. First of all, I just want to say I am only starting to learn about PDEs from Evans' text, and am only on Chapter 2 of Part I, which deals with the usual linear second order PDE.  I suspect the restriction of initial/boundary conditions to some $g$ with $0\leq g\leq1$ is probably fairly significant in simplifying the problem.  But even for this case, I am not sure how to attack the problem.  But is it reasonable to expect that one should know how to analyze the solutions to this PDE for arbitrary $g$? Some of the ways I tried to attack the problem included finding a simplifying substitution (for example, when $\alpha=1$ I was able to use $u=e^{t}v$ to eliminate the RHS term), but I could not think of any that would work for all $\alpha$.  Next I tried applying usual energy techniques, but this did not seem to lead anywhere fruitful (things like the arbitrariness of $\alpha$ and the non-periodic boundary conditions obstructed a clear path to making use any possible candidate for the energy functional). Next I wondered if solutions to the PDE itself (without any specified boundary conditions) satisfies a maximum principle.  But I found this hard to prove using straightforward techniques like $\epsilon$ regularization, and the methods Evans uses to prove the strong maximum principle for just the heat equation via time-dependent mean-value properties was already complicated enough as it was to even think about extending the arguments to the non-linear PDE above.  On the other hand, the non-linearity likely precludes this possibility, at least for all $\alpha$.  Furthermore, if for all $\alpha$ the equation did satisfy a weak maximum principle, then it would imply the answer is all $\alpha\in\mathbb{R}$, and this also seems unlikely.  Why it seems unlikely is related to my next thought in considering the ODE $$u_{t}=u^{\alpha}.$$ For $t>0$ and $0\leq u(0)=g(\cdot,0)\leq1$, solutions to this ODE exhibit a wide variety of behavior depending on $\alpha$, and even lacks uniqueness in many cases.  But in general, either the solution blows up (e.g. $\alpha\geq2$), grows exponentially (e.g. $\alpha=1$), or else grows in some sort of algebraic fashion like $O(x^{\beta})$ (correct me if I am wrong, please!). The point of this breakdown into various classes of solutions to the above ODE is to somehow relate the behavior back to the behavior of solutions to the PDE with the diffusion term $\Delta u$ added back.  Heuristically, the diffusion term $\Delta u$ ""smoothes"" the solution and ""spreads"" it out over the spatial domain $U$ as $t>0$ progreses.  On the other hand, $u^{\alpha}$ heuristically causes growth in the solution in space as $t>0$ evolves.  If the growth term $u^{\alpha}$ overpowers the diffusion term $\Delta u$, then it seems the solution would not remain uniformly bounded in $t$, and if the growth term is very strong, then singularities could develop in finite time $t$.  Anyway, that is all fine (I think), but the practical question is how does one convert heuristics about the PDE into specific quantitative information/estimates about the solution (in this case, uniform $t$-boundedness of $u$ as a function of $\alpha$)? One last comment.  I haven't really considered (from a purely heuristic point of view) the boundary terms, and the special assumption $0\leq g\leq1$ (I would still like to know in any answer how we could generalize $g$ a bit more, if it can be reasonably done).  $U$ is of course bounded, so the diffusion of the system is constrained to a bounded region kept at some constant positive density on the boundary.  This would seem to me that the growth term inevitably dominates, since the solution will continuously grow and grow (as mass is added into the system restricted to a bounded region).  Whether the solution develops singularities would depend on the strength of the diffusion term.  If the diffusion term is strong enough, then the mass added to the system has enough time to spread out, whereas if it is too weak, then the concentration will spike into a singularity at some point(s) in $U$.  Either way, it would appear that unless $g(x,0)\equiv0$, the solution is at best not uniformly bounded, and at worst actually blows up in finite time $t<\infty.$  Maybe this answers the question?  If it helps, I was only asked to ""explain"" my answer, so given the non-linearity of the PDE itself, perhaps we aren't expected to provide a rigorous argument, though I would like to develop one if it is possible. Any hints are appreciated -- thanks! EDIT 1: I should clarify since I don't know if the terminology parabolic boundary/cylinder is standard.  $U$ is an open subset of $\mathbb{R}^{n}$ (spatial region) and $$U_{T}:=U\times(0,T],$$ $$\Gamma_{T}:=\partial U\times[0,T]\cup U\times\{t=0\}=\overline{U_{T}}-U_{T}.$$ EDIT 2: For problems addressing behavior, such as boundedness, for large $t$, is it helpful to consider the time-independent case?  e.g. the elliptic PDE $$-\Delta u=u^{\alpha}.$$ In other words, if the solution to this elliptic PDE (with appropriate boundary conditions as intimated by the original parabolic-type PDE) is bounded, could we somehow deduce boundedness (and hence uniform boundedness) for all $t$ before ""$t=\infty$?""  I guess the only obstruction to this would be the possibility of singularities developing and then evaporating over time, which doesn't seem likely (though I am not going to say it's impossible, since I have no mathematical justification for it, and also black holes can be thought of as ""singularities"" which evaporate over large periods of time as they radiate energy, this probably going way off topic).",,"['analysis', 'partial-differential-equations']"
85,Morphology of binary images,Morphology of binary images,,"During the lecture we talked about analysis of pictures and got some exrecises. Other students say that this is very easy but I don't get a good answer. Here the facts: Suppose $A$ is a bounded subset of $\Bbb{Z}^2$ (the image) and $B\subset\Bbb{Z}^2$ a structureing element (for example $B_1=\{z\in\Bbb{Z}:|z_1|\leq1,|z_2|\leq1\}$ thus a square). Now define the following operations: Erosion: $A-B=\{z\in A:z+B\subset A\}$ Dilation: $A+B=\{z\in\Bbb{Z^2}:z+B\cap A\neq \emptyset\}$ Opening: $A\circ B=(A-B)+B$ Closing: $A\bullet B=(A+B)-B$ Now we should prove the following three facts: $A-B\subset A\circ B\subset A\subset A\bullet B\subset A+B$ (from my point of view this fact is very simple and there is nothing to prove but how to do this on the best way because you get some strange sets) $(A\circ B)\circ B=A\circ B$ and $(A\bullet B)\bullet B=A\bullet B$ (one inclusion is per definition always trivial but the other side?) Suppose $p\in A$. Define $X_1=\{p\}$ and $X_{k+1}=X_k+B_1$ (with $B_1$ the square defined above). Show that $X_k$ converges in finitely many steps to the connected component of $A$ containing $p$ (sorry, i have no idea ...) Image analysis is very interesting, please help me to understand all facts of this topic :) Thank you for help and ideas.","During the lecture we talked about analysis of pictures and got some exrecises. Other students say that this is very easy but I don't get a good answer. Here the facts: Suppose $A$ is a bounded subset of $\Bbb{Z}^2$ (the image) and $B\subset\Bbb{Z}^2$ a structureing element (for example $B_1=\{z\in\Bbb{Z}:|z_1|\leq1,|z_2|\leq1\}$ thus a square). Now define the following operations: Erosion: $A-B=\{z\in A:z+B\subset A\}$ Dilation: $A+B=\{z\in\Bbb{Z^2}:z+B\cap A\neq \emptyset\}$ Opening: $A\circ B=(A-B)+B$ Closing: $A\bullet B=(A+B)-B$ Now we should prove the following three facts: $A-B\subset A\circ B\subset A\subset A\bullet B\subset A+B$ (from my point of view this fact is very simple and there is nothing to prove but how to do this on the best way because you get some strange sets) $(A\circ B)\circ B=A\circ B$ and $(A\bullet B)\bullet B=A\bullet B$ (one inclusion is per definition always trivial but the other side?) Suppose $p\in A$. Define $X_1=\{p\}$ and $X_{k+1}=X_k+B_1$ (with $B_1$ the square defined above). Show that $X_k$ converges in finitely many steps to the connected component of $A$ containing $p$ (sorry, i have no idea ...) Image analysis is very interesting, please help me to understand all facts of this topic :) Thank you for help and ideas.",,"['combinatorics', 'analysis']"
86,Singular derivative matrix implies not one to one?,Singular derivative matrix implies not one to one?,,I am trying to show that if $f:\mathbb{R}^n\to \mathbb{R}^n $ is continuously differentiable and that for all $x\in \mathbb{R}^n$ $Df(x)$ is singular implies that $f$ is not 1-1. The singularity of the derivative clearly implies that at each point it has a nontrivial kernel and so my thinking was to use try to use the mean value theorem somehow with a vector which would make the derivative term vanish so that I get equality  between two different image points.  But in general the kernel of the derivative will depend on where it is evaluated in $\mathbb{R}^n$ so I haven't been able to do this directly since  I can't choose the kernel before choosing where to evaluate the derivative in the mean value theorem. Maybe it's false though?  Counterexamples?,I am trying to show that if $f:\mathbb{R}^n\to \mathbb{R}^n $ is continuously differentiable and that for all $x\in \mathbb{R}^n$ $Df(x)$ is singular implies that $f$ is not 1-1. The singularity of the derivative clearly implies that at each point it has a nontrivial kernel and so my thinking was to use try to use the mean value theorem somehow with a vector which would make the derivative term vanish so that I get equality  between two different image points.  But in general the kernel of the derivative will depend on where it is evaluated in $\mathbb{R}^n$ so I haven't been able to do this directly since  I can't choose the kernel before choosing where to evaluate the derivative in the mean value theorem. Maybe it's false though?  Counterexamples?,,"['analysis', 'multivariable-calculus']"
87,Under which hypotheses is switching between sum and integral signs legit?,Under which hypotheses is switching between sum and integral signs legit?,,"Which hypotheses are needed to change the order of sum and integral signs? Concrete example: consider the expression $$ \int_{\gamma}\frac{f(\zeta)}{\zeta-z_0}\sum_{k=0}^{\infty}\left(\frac{\zeta-z_0}{z-z_0}\right)^k \, d\zeta $$ where $\gamma$ is a circle of radius $r$ of the complex plane. Suppose that the sum  $$ \sum_{k=0}^{\infty}\left(\frac{\zeta-z_0}{z-z_0}\right)^k  $$ is uniformly convergent because $\left|\frac{\zeta-z_0}{z-z_0}\right|<1$. $f$ is holomorphic on a compact set which is a closed annulus of width $R-r$. In this setting, is it sufficient to put the sum sign outside the integral to obtain $$ \sum_{k=0}^{\infty} \int_{\gamma}\frac{f(\zeta)}{\zeta-z_0}\left(\frac{\zeta-z_0}{z-z_0}\right)^k \, d\zeta \quad ? $$","Which hypotheses are needed to change the order of sum and integral signs? Concrete example: consider the expression $$ \int_{\gamma}\frac{f(\zeta)}{\zeta-z_0}\sum_{k=0}^{\infty}\left(\frac{\zeta-z_0}{z-z_0}\right)^k \, d\zeta $$ where $\gamma$ is a circle of radius $r$ of the complex plane. Suppose that the sum  $$ \sum_{k=0}^{\infty}\left(\frac{\zeta-z_0}{z-z_0}\right)^k  $$ is uniformly convergent because $\left|\frac{\zeta-z_0}{z-z_0}\right|<1$. $f$ is holomorphic on a compact set which is a closed annulus of width $R-r$. In this setting, is it sufficient to put the sum sign outside the integral to obtain $$ \sum_{k=0}^{\infty} \int_{\gamma}\frac{f(\zeta)}{\zeta-z_0}\left(\frac{\zeta-z_0}{z-z_0}\right)^k \, d\zeta \quad ? $$",,"['sequences-and-series', 'complex-analysis', 'analysis', 'integration', 'power-series']"
88,Exercise from textbook about norm,Exercise from textbook about norm,,"The below diagram included a relevant page and the question 4 of the exercise which i am not sure how to do. Any hint or solution are welcome Attempts: a) I have done it b) i have tried to show that $\sum_{i=1}^{n}|x^i|^p$ is a norm. If it is a norm, it seems that i can apply (2.4) and immediately follows $K$ has the 4 properties. But it seems it isn't a norm so is there any other way to prove the property? c) I don't know what does ""norm associated with K"" mean.","The below diagram included a relevant page and the question 4 of the exercise which i am not sure how to do. Any hint or solution are welcome Attempts: a) I have done it b) i have tried to show that $\sum_{i=1}^{n}|x^i|^p$ is a norm. If it is a norm, it seems that i can apply (2.4) and immediately follows $K$ has the 4 properties. But it seems it isn't a norm so is there any other way to prove the property? c) I don't know what does ""norm associated with K"" mean.",,"['real-analysis', 'general-topology', 'analysis', 'normed-spaces']"
89,Where can I find the proofs of these measure theory Propositions?,Where can I find the proofs of these measure theory Propositions?,,"These propositions are listed as examples in my script and I want to know the proofs (I am not able to prove them myself, unfortunately, because im not smart and very new to measure theory) : If $I=I_{1}\times ... I_n$ is a bounded Interval on $\mathbb{R}^n$, then I is measurable and it holds that $m(I)=|I_{1}|...|I_n|$ (m(I) is called the elementary n-dimensional volume Every unbounded Interval$\subset \mathbb{R}^n$ is measurable with $m(I) = \infty$ and $\mathbb{R}^n$ is measurable with $m(\mathbb{R}^n) = \infty$ Every open Subset $G\subset \mathbb{R}^n$ is measurable. Every closed subset of $\mathbb{R}^n$ is measurable Every compact Subset of $\mathbb{R}^n$ has a finite measure Do you know of a source where I can find proofs to these examples or what I have to search for to find them. Thx in advance","These propositions are listed as examples in my script and I want to know the proofs (I am not able to prove them myself, unfortunately, because im not smart and very new to measure theory) : If $I=I_{1}\times ... I_n$ is a bounded Interval on $\mathbb{R}^n$, then I is measurable and it holds that $m(I)=|I_{1}|...|I_n|$ (m(I) is called the elementary n-dimensional volume Every unbounded Interval$\subset \mathbb{R}^n$ is measurable with $m(I) = \infty$ and $\mathbb{R}^n$ is measurable with $m(\mathbb{R}^n) = \infty$ Every open Subset $G\subset \mathbb{R}^n$ is measurable. Every closed subset of $\mathbb{R}^n$ is measurable Every compact Subset of $\mathbb{R}^n$ has a finite measure Do you know of a source where I can find proofs to these examples or what I have to search for to find them. Thx in advance",,"['real-analysis', 'analysis', 'measure-theory', 'reference-request']"
90,How discontinuous can a derivative be?,How discontinuous can a derivative be?,,"There is a well-known result in elementary analysis due to Darboux which says if $f$ is a differentiable function then $f'$ satisfies the intermediate value property.  To my knowledge, not many ""highly"" discontinuous Darboux functions are known--the only one I am aware of being the Conway base 13 function--and few (none?) of these are derivatives of differentiable functions.  In fact they generally cannot be since an application of Baire's theorem gives that the set of continuity points of the derivative is dense $G_\delta$. Is it known how sharp that last result is?  Are there known Darboux functions which are derivatives and are discontinuous on ""large"" sets in some appropriate sense?","There is a well-known result in elementary analysis due to Darboux which says if $f$ is a differentiable function then $f'$ satisfies the intermediate value property.  To my knowledge, not many ""highly"" discontinuous Darboux functions are known--the only one I am aware of being the Conway base 13 function--and few (none?) of these are derivatives of differentiable functions.  In fact they generally cannot be since an application of Baire's theorem gives that the set of continuity points of the derivative is dense $G_\delta$. Is it known how sharp that last result is?  Are there known Darboux functions which are derivatives and are discontinuous on ""large"" sets in some appropriate sense?",,"['real-analysis', 'derivatives', 'examples-counterexamples']"
91,Generalizations of derivatives using distance measures,Generalizations of derivatives using distance measures,,"Let $d(x,y)$ be a distance metric for two points $x,y\in \mathbb{R}^p$. Further, suppose that there are two real or complex sequences $X_n(x)$ and $X_n(y)$, $n=1,2,\ldots$ that depend on $x$ and $y$ respectively. Let $D(X_n(x),X_n(y))$ be a distance metric between two such sequences. Then one can form some kind of derivative by taking the limit $\delta \rightarrow 0$ of $${ d(x,x+\delta)} \over {D(X_n(x),X_n(x+\delta)) }$$ The question is simply, does this kind of derivative have a name? Are there some particular distance measures that are frequently used in this context? I would appreciate any pointers to further reading. A further question on notation, since this appears to have caused some confusion: What would be a good way of indicating that an infinite sequence $X_n$ depends on the parameters $a \in \mathbb{R}^p$? Is there any other notation that is prefered over the one I used, $X_n(a)$?","Let $d(x,y)$ be a distance metric for two points $x,y\in \mathbb{R}^p$. Further, suppose that there are two real or complex sequences $X_n(x)$ and $X_n(y)$, $n=1,2,\ldots$ that depend on $x$ and $y$ respectively. Let $D(X_n(x),X_n(y))$ be a distance metric between two such sequences. Then one can form some kind of derivative by taking the limit $\delta \rightarrow 0$ of $${ d(x,x+\delta)} \over {D(X_n(x),X_n(x+\delta)) }$$ The question is simply, does this kind of derivative have a name? Are there some particular distance measures that are frequently used in this context? I would appreciate any pointers to further reading. A further question on notation, since this appears to have caused some confusion: What would be a good way of indicating that an infinite sequence $X_n$ depends on the parameters $a \in \mathbb{R}^p$? Is there any other notation that is prefered over the one I used, $X_n(a)$?",,"['analysis', 'metric-spaces']"
92,Asymptotic behaviour of a function whose derivatives vanish asymptotically,Asymptotic behaviour of a function whose derivatives vanish asymptotically,,"Let $f\colon\mathbb{R}\to\mathbb{R}^n$ be a $C^\infty$ function such that $\lim_{t\to\pm\infty}f^{(k)}(t)$ exists for all $k\in\mathbb{N}\cup\{0\}$. A neat little trick (apparently due to Thomas Hill ) shows that $\lim_{t\to\pm\infty}f^{(k)}=0$ for $k\geq 1$. Indeed by l'Hopital's rule $$\lim_{t\to\pm\infty}f(t) = \lim_{t\to\pm\infty}\frac{e^t f(t)}{e^t} = \lim_{t\to\pm\infty}\frac{e^t(f(t)+f'(t))}{e^t}=\lim_{t\to\pm\infty}(f(t)+f'(t)),$$ so the claim follows inductively. Is it possible to draw conclusions about the asymptotic decay of $f'$? For example, is there a $\delta > 0$ such that $\lim_{t\to\pm\infty}f'(t)\cdot e^{\delta |t|}=0$? Are there examples when this is not the case?","Let $f\colon\mathbb{R}\to\mathbb{R}^n$ be a $C^\infty$ function such that $\lim_{t\to\pm\infty}f^{(k)}(t)$ exists for all $k\in\mathbb{N}\cup\{0\}$. A neat little trick (apparently due to Thomas Hill ) shows that $\lim_{t\to\pm\infty}f^{(k)}=0$ for $k\geq 1$. Indeed by l'Hopital's rule $$\lim_{t\to\pm\infty}f(t) = \lim_{t\to\pm\infty}\frac{e^t f(t)}{e^t} = \lim_{t\to\pm\infty}\frac{e^t(f(t)+f'(t))}{e^t}=\lim_{t\to\pm\infty}(f(t)+f'(t)),$$ so the claim follows inductively. Is it possible to draw conclusions about the asymptotic decay of $f'$? For example, is there a $\delta > 0$ such that $\lim_{t\to\pm\infty}f'(t)\cdot e^{\delta |t|}=0$? Are there examples when this is not the case?",,"['calculus', 'real-analysis', 'analysis']"
93,Invertible idempotent in a C-star algebra question,Invertible idempotent in a C-star algebra question,,"Let $J$ be an idempotent element in a unital $C^*$ algebra.  Why is $I+(J-J^*)(J^*-J)$ invertible? I have been trying to show that $\|(J-J^*)(J^*-J)\|<1$, but I could not do this.","Let $J$ be an idempotent element in a unital $C^*$ algebra.  Why is $I+(J-J^*)(J^*-J)$ invertible? I have been trying to show that $\|(J-J^*)(J^*-J)\|<1$, but I could not do this.",,['analysis']
94,Riemann integral vs Lebesgue integral,Riemann integral vs Lebesgue integral,,"Let $f$ be analytic on a domain $\Omega$ of the complex plane, such that the closed disc $\overline{D(0,R)}$ is contained in $\Omega$. What is the difference between $$ \int_{D(0,R)}|f(w)|dm(w)$$ and $$\iint_{D(0,R)}|f(x,y)|dxdy$$ where $dm$ is the Lebesgue measure in $\mathbb R^2=\mathbb C$?","Let $f$ be analytic on a domain $\Omega$ of the complex plane, such that the closed disc $\overline{D(0,R)}$ is contained in $\Omega$. What is the difference between $$ \int_{D(0,R)}|f(w)|dm(w)$$ and $$\iint_{D(0,R)}|f(x,y)|dxdy$$ where $dm$ is the Lebesgue measure in $\mathbb R^2=\mathbb C$?",,"['real-analysis', 'analysis', 'integration', 'lebesgue-integral']"
95,How can I show this function is Weakly Sequentially Lower Semicontinuos?,How can I show this function is Weakly Sequentially Lower Semicontinuos?,,Suppoe $X$ is a Banach spaces and $G\subset X$ is a convex open set. Let $\phi:G\rightarrow \mathbb{R}$ be a $C^1$ function and assume that $\phi'$ is a bounded and peseudo-monotone map (see here for a definiton of pseudo-monotone). We say that $\phi$ is weakly sequentially lower semicontinuos (WSLSC) in $G$ if for every sequence $x_n$ in $G$ which converges weakly to $x\in G$ we have that $$\phi(x)\leq\liminf \phi(x_n)$$ How can i show that $\phi$ is WSLSC?,Suppoe $X$ is a Banach spaces and $G\subset X$ is a convex open set. Let $\phi:G\rightarrow \mathbb{R}$ be a $C^1$ function and assume that $\phi'$ is a bounded and peseudo-monotone map (see here for a definiton of pseudo-monotone). We say that $\phi$ is weakly sequentially lower semicontinuos (WSLSC) in $G$ if for every sequence $x_n$ in $G$ which converges weakly to $x\in G$ we have that $$\phi(x)\leq\liminf \phi(x_n)$$ How can i show that $\phi$ is WSLSC?,,['analysis']
96,Proof of differentiability,Proof of differentiability,,"A function $f$ is defined in $R$, and $f'(0)$ exist. Let $f(x+y)=f(x)f(y)$ then prove that $f'$ exists for all $x$ in $R$. I find this problem in here,  and wonder the proof stated below has any problems? Since $f'(0)=\lim_{h \rightarrow  0} \frac{f(0)(f(h)-1)}{h}=f(0)\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ it means $\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ exists. $f'(x)==\lim_{h \rightarrow  0} \frac{f(x)(f(h)-1)}{h}=f(x)\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ thus $f'(x)$ exists if $f(x)$ exists. But we can differentiate a function if it has a function value $f(x)$, thus  $f'$ exists for all $x$ in $R$. I know it's not a perfect one, just want to know it can be another way to solve that problem or not. ================================================================ $f(0)=0$ case: If $f(0)=0$, because $f(x+y)=f(x)f(y)$, $f(x)=f(x+0)=f(x)f(0)=0$ thus $f'(x)=f'(0)$. Is that right?","A function $f$ is defined in $R$, and $f'(0)$ exist. Let $f(x+y)=f(x)f(y)$ then prove that $f'$ exists for all $x$ in $R$. I find this problem in here,  and wonder the proof stated below has any problems? Since $f'(0)=\lim_{h \rightarrow  0} \frac{f(0)(f(h)-1)}{h}=f(0)\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ it means $\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ exists. $f'(x)==\lim_{h \rightarrow  0} \frac{f(x)(f(h)-1)}{h}=f(x)\lim_{h \rightarrow  0} \frac{f(h)-1}{h}$ thus $f'(x)$ exists if $f(x)$ exists. But we can differentiate a function if it has a function value $f(x)$, thus  $f'$ exists for all $x$ in $R$. I know it's not a perfect one, just want to know it can be another way to solve that problem or not. ================================================================ $f(0)=0$ case: If $f(0)=0$, because $f(x+y)=f(x)f(y)$, $f(x)=f(x+0)=f(x)f(0)=0$ thus $f'(x)=f'(0)$. Is that right?",,['analysis']
97,Is it possible to algebraically prove that the $n$th degree Taylor remainder of $f(x)$ is less than $K|\Delta x|^{n+1}$ for $K \in \mathbb{R^+}$?,Is it possible to algebraically prove that the th degree Taylor remainder of  is less than  for ?,n f(x) K|\Delta x|^{n+1} K \in \mathbb{R^+},"I found a purely algebraic proof, given below, that for a mononomial $f(x) = x^n$ the magnitude of the error of its linear approximation  $| f(x) - [f(a) + f'(a)(x-a)] |$ is less than $K(x-a)^2$ for $K \in \mathbb{R^+}$. I can generalize this result to polynomials, and hence any Taylor series. I was wondering if it's possible, using algebraic means*, to generalize this proof for the error of any kind of approximation. For example, I'm looking to prove that the magnitude of the error of the quadratic approximation of a monomial, $| f(x) - [f(a) + f'(a)(x-a) + f''(a)(x - a)^2 / 2! ] |$, is less than or equal to $K(x-a)^3$ . Furthermore, is it possible to prove that it is impossible to find $K$ such that $| f(x) - [f(a) + f'(a)(x-a)] | < K(x-a)^n$ for $n > 2$? Or to find $K$ such that the $n$th degree Taylor remainder is less than $K(x-a)^{n+2}$? I'm asking all of this mainly to attain a better understanding of the Taylor series, and the limitations of algebra as opposed to tools such as the bisection algorithm. *(I'm aware that what I'm asking can be proven using, for example, the Lagrange form of the Taylor remainder.) Proposition 1 (What I've proved). Let $f(x) = x^n$. Then for all $a, x \in [-I,I]$, there exists $K \in \mathbb{R^+}$ such that the error of the linear approximation of $f(x)$ at $a$ is less than $K(x-a)^2$: $$|f(x) - f(a) - f'(a)(x-a)| \leq K(x-a)^2$$ Proof. \begin{align*}   f(x) - f(a) - f'(a)(x-a)&=     (x-a)\sum_{i=1}^n x^{n-i}a^{i-1} - (x-a)na^{n-1} \\   &=     (x-a)\sum_{i=1}^n(x^{n-i}a^{i-1}-a^{n-1}) \\   &=     (x-a)\sum_{i=1}^n(x^{n-i}-a^{n-i})a^{i-1}.\\   &=     (x-a)\sum_{i=1}^{n-1}(x^{n-i}-a^{n-i})a^{i-1}\\   &=     (x-a)\sum_{i=1}^{n-1}(x-a)\sum_{j=1}^{n-i}(x^{n-i-j}-a^{j-1})a^{i-1} \\   &=     (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2}.\\ \therefore \quad |f(x) - f(a) - f'(a)(x-a)| &=   (x-a)^2\left|\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}a^{n-i-j}x^{i+j-2}\right|\\ &\leq    (x-a)^2\left|\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}I^{n-i-j}I^{i+j-2}\right|.\\ \end{align*} Proposition 2 (An attempt to generalize Proposition 1). Let $f(x) = x^n$. Then for all $a, x \in [-I,I]$, there exists $K \in \mathbb{R^+}$ such that the error of the quadratic approximation of $f(x)$ at $a$ is less than $K(x-a)^3$:    $$|f(x) - f(a) - f'(a)(x-a) - f''(a)(x-a)^2| \leq K(x-a)^3$$ Proof (Incomplete). The right hand side of this proof is identical to the proof for proposition 1, save for the final set of equations after the $\therefore$ sign. \begin{array}{l}   f(x) - f(a) - f'(a)(x-a) - \frac{f''(a)}{2!}(x-a)^2 + \frac{n(n-1)a^{n-2}}{2}(x-a)^2\\   = x^n - a^n - (x-a)na^{n-1} \\   = (x-a)\sum_{i=1}^n x^{n-i}a^{i-1} - (x-a)na^{n-1}\\    =     (x-a)\sum_{i=1}^n(x^{n-i}a^{i-1}-a^{n-1})\\    =     (x-a)\sum_{i=1}^n(x^{n-i}-a^{n-i})a^{i-1}\\     =     (x-a)\sum_{i=1}^{n-1}(x^{n-i}-a^{n-i})a^{i-1} \\   =     (x-a)\sum_{i=1}^{n-1}(x-a)\sum_{j=1}^{n-i}(x^{n-i-j}-a^{j-1})a^{i-1} \\   =     (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2}\\ \therefore\quad f(x) - f(a) - f'(a)(x-a) - \frac{f''(a)}{2!}(x-a)^2 \\   = (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2} - \frac{n(n-1)a^{n-2}}{2}(x-a)^2\\   = (x-a)^2\left[\sum_{i=1}^{n-1}\left(\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2} - \frac{na^{n-2}}{2}\right)\right]\\ \end{array}","I found a purely algebraic proof, given below, that for a mononomial $f(x) = x^n$ the magnitude of the error of its linear approximation  $| f(x) - [f(a) + f'(a)(x-a)] |$ is less than $K(x-a)^2$ for $K \in \mathbb{R^+}$. I can generalize this result to polynomials, and hence any Taylor series. I was wondering if it's possible, using algebraic means*, to generalize this proof for the error of any kind of approximation. For example, I'm looking to prove that the magnitude of the error of the quadratic approximation of a monomial, $| f(x) - [f(a) + f'(a)(x-a) + f''(a)(x - a)^2 / 2! ] |$, is less than or equal to $K(x-a)^3$ . Furthermore, is it possible to prove that it is impossible to find $K$ such that $| f(x) - [f(a) + f'(a)(x-a)] | < K(x-a)^n$ for $n > 2$? Or to find $K$ such that the $n$th degree Taylor remainder is less than $K(x-a)^{n+2}$? I'm asking all of this mainly to attain a better understanding of the Taylor series, and the limitations of algebra as opposed to tools such as the bisection algorithm. *(I'm aware that what I'm asking can be proven using, for example, the Lagrange form of the Taylor remainder.) Proposition 1 (What I've proved). Let $f(x) = x^n$. Then for all $a, x \in [-I,I]$, there exists $K \in \mathbb{R^+}$ such that the error of the linear approximation of $f(x)$ at $a$ is less than $K(x-a)^2$: $$|f(x) - f(a) - f'(a)(x-a)| \leq K(x-a)^2$$ Proof. \begin{align*}   f(x) - f(a) - f'(a)(x-a)&=     (x-a)\sum_{i=1}^n x^{n-i}a^{i-1} - (x-a)na^{n-1} \\   &=     (x-a)\sum_{i=1}^n(x^{n-i}a^{i-1}-a^{n-1}) \\   &=     (x-a)\sum_{i=1}^n(x^{n-i}-a^{n-i})a^{i-1}.\\   &=     (x-a)\sum_{i=1}^{n-1}(x^{n-i}-a^{n-i})a^{i-1}\\   &=     (x-a)\sum_{i=1}^{n-1}(x-a)\sum_{j=1}^{n-i}(x^{n-i-j}-a^{j-1})a^{i-1} \\   &=     (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2}.\\ \therefore \quad |f(x) - f(a) - f'(a)(x-a)| &=   (x-a)^2\left|\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}a^{n-i-j}x^{i+j-2}\right|\\ &\leq    (x-a)^2\left|\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}I^{n-i-j}I^{i+j-2}\right|.\\ \end{align*} Proposition 2 (An attempt to generalize Proposition 1). Let $f(x) = x^n$. Then for all $a, x \in [-I,I]$, there exists $K \in \mathbb{R^+}$ such that the error of the quadratic approximation of $f(x)$ at $a$ is less than $K(x-a)^3$:    $$|f(x) - f(a) - f'(a)(x-a) - f''(a)(x-a)^2| \leq K(x-a)^3$$ Proof (Incomplete). The right hand side of this proof is identical to the proof for proposition 1, save for the final set of equations after the $\therefore$ sign. \begin{array}{l}   f(x) - f(a) - f'(a)(x-a) - \frac{f''(a)}{2!}(x-a)^2 + \frac{n(n-1)a^{n-2}}{2}(x-a)^2\\   = x^n - a^n - (x-a)na^{n-1} \\   = (x-a)\sum_{i=1}^n x^{n-i}a^{i-1} - (x-a)na^{n-1}\\    =     (x-a)\sum_{i=1}^n(x^{n-i}a^{i-1}-a^{n-1})\\    =     (x-a)\sum_{i=1}^n(x^{n-i}-a^{n-i})a^{i-1}\\     =     (x-a)\sum_{i=1}^{n-1}(x^{n-i}-a^{n-i})a^{i-1} \\   =     (x-a)\sum_{i=1}^{n-1}(x-a)\sum_{j=1}^{n-i}(x^{n-i-j}-a^{j-1})a^{i-1} \\   =     (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2}\\ \therefore\quad f(x) - f(a) - f'(a)(x-a) - \frac{f''(a)}{2!}(x-a)^2 \\   = (x-a)^2\sum_{i=1}^{n-1}\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2} - \frac{n(n-1)a^{n-2}}{2}(x-a)^2\\   = (x-a)^2\left[\sum_{i=1}^{n-1}\left(\sum_{j=1}^{n-i}x^{n-i-j}a^{i+j-2} - \frac{na^{n-2}}{2}\right)\right]\\ \end{array}",,"['sequences-and-series', 'analysis', 'taylor-expansion', 'summation']"
98,Help with Statistical Analysis,Help with Statistical Analysis,,"I have been asked to conduct a small survey and then analyse the results of the said survey for a class project. I decided to conduct an anonymous survey of voters for the 2012 presidential elections. The Information collected was : Age Group Gender Preference for Presidential Candidate Personal Political Views (Liberal, Conservative, etc) So I now have the data, but I am stuck as to what I can do with it. I know I can get the numbers for the voters in a state and compare them, but I need to initially present data summaries such as summary statistics or graphical displays and justify if my sample is a valid representation of the population. I fear I have taken on a bit too much and I would be very grateful for any advice as to what I can do and calculate with this data.","I have been asked to conduct a small survey and then analyse the results of the said survey for a class project. I decided to conduct an anonymous survey of voters for the 2012 presidential elections. The Information collected was : Age Group Gender Preference for Presidential Candidate Personal Political Views (Liberal, Conservative, etc) So I now have the data, but I am stuck as to what I can do with it. I know I can get the numbers for the voters in a state and compare them, but I need to initially present data summaries such as summary statistics or graphical displays and justify if my sample is a valid representation of the population. I fear I have taken on a bit too much and I would be very grateful for any advice as to what I can do and calculate with this data.",,"['analysis', 'statistics']"
99,Drawing inferences about equation solving,Drawing inferences about equation solving,,"I have two systems of non-linear equations.  The equations are continuously differentiable in the domain that I am interested in. Let $x_1, y_1$ solve the following system of equations. $f(x,y)=0\\ g(x,y)=0$ Let $x_2,y_2$ solve $f(x,y)=0\\ g(x,y)=c$ Were $c>0$ is a constant. For simplicity and for starters we can assume that these solutions are unique. I have the functional forms of $f$ and $g$ and they are very long, painful and non-linear. I have some non-math intuition that the following are true and I would love some help and suggestions on the things I can try to do to prove these statements: $x_2 > x_1  \\  y_2 > y_1 \\ x_2-x_1 > y_2-y_1 $ Thanks a lot, EDIT Please feel free to make simplifying assumptions. For example, we can take $c>0$ to arbitrarily small for starters and see where that takes us. I'm not sure if I can show this but suppose $f(x,y)$ and $g(x,y)$ satisfied the implicit function theorem then perhaps we could get some traction on this.","I have two systems of non-linear equations.  The equations are continuously differentiable in the domain that I am interested in. Let $x_1, y_1$ solve the following system of equations. $f(x,y)=0\\ g(x,y)=0$ Let $x_2,y_2$ solve $f(x,y)=0\\ g(x,y)=c$ Were $c>0$ is a constant. For simplicity and for starters we can assume that these solutions are unique. I have the functional forms of $f$ and $g$ and they are very long, painful and non-linear. I have some non-math intuition that the following are true and I would love some help and suggestions on the things I can try to do to prove these statements: $x_2 > x_1  \\  y_2 > y_1 \\ x_2-x_1 > y_2-y_1 $ Thanks a lot, EDIT Please feel free to make simplifying assumptions. For example, we can take $c>0$ to arbitrarily small for starters and see where that takes us. I'm not sure if I can show this but suppose $f(x,y)$ and $g(x,y)$ satisfied the implicit function theorem then perhaps we could get some traction on this.",,"['calculus', 'analysis']"
