,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Playing 4 games, then playing until a loss - expected number of losses","Playing 4 games, then playing until a loss - expected number of losses",,"Consider a game where you win with probability $p$. We play 5 such games, and if we win game number 5, we keep playing until we lose. a) Find the expected number of games played b) Find the expected number of games lost Considering the a) part of problem, I defined X to be a random variable that measures number of games, starting at 5 (since we are sure the game is played at least 5 times) and came up with $$E(X) = \frac{5-4p}{1-p}$$ which appears to be correct. I have no idea how to approach the second part of the problem. The official solution states that it is the expected number of games played times the probability of loss, i.e. $E(X)\cdot(1-p) = 5-4p$ with no explanation whatsoever. So any sort of intuition or explanation (or an alternative solution) would be greatly appreciated. $\bf{edit}$ I have arrived at E(X) by defining a discreet random variable with the following distribution  $$X\sim \left( \begin{array}{ c c c }  5 & 6 & 7 & ... & k \\   1 - p & p(1-p) & p^2(1-p) & ...  & p^{k-5}(1-p)   \end{array} \right) $$ and applying the definition of expectation. $\bf{edit 2}$ I tried defining a random variable $Y$ which measures the number of losses in first four games. So for 0 losses we must win every game, so the probability is p^4, etc. I arrived at the following distribution: $$X\sim \left( \begin{array}{ c c c }  0 & 1 & 2 & 3 & 4 \\   p^4 & p^3(1-p) & p^2(1-p)^2 & p(1-p)^3  & (1-p)^4   \end{array} \right) $$ However the expectation of this variable doesn't match the solution $5-4p$.","Consider a game where you win with probability $p$. We play 5 such games, and if we win game number 5, we keep playing until we lose. a) Find the expected number of games played b) Find the expected number of games lost Considering the a) part of problem, I defined X to be a random variable that measures number of games, starting at 5 (since we are sure the game is played at least 5 times) and came up with $$E(X) = \frac{5-4p}{1-p}$$ which appears to be correct. I have no idea how to approach the second part of the problem. The official solution states that it is the expected number of games played times the probability of loss, i.e. $E(X)\cdot(1-p) = 5-4p$ with no explanation whatsoever. So any sort of intuition or explanation (or an alternative solution) would be greatly appreciated. $\bf{edit}$ I have arrived at E(X) by defining a discreet random variable with the following distribution  $$X\sim \left( \begin{array}{ c c c }  5 & 6 & 7 & ... & k \\   1 - p & p(1-p) & p^2(1-p) & ...  & p^{k-5}(1-p)   \end{array} \right) $$ and applying the definition of expectation. $\bf{edit 2}$ I tried defining a random variable $Y$ which measures the number of losses in first four games. So for 0 losses we must win every game, so the probability is p^4, etc. I arrived at the following distribution: $$X\sim \left( \begin{array}{ c c c }  0 & 1 & 2 & 3 & 4 \\   p^4 & p^3(1-p) & p^2(1-p)^2 & p(1-p)^3  & (1-p)^4   \end{array} \right) $$ However the expectation of this variable doesn't match the solution $5-4p$.",,"['probability', 'expectation']"
1,Exponential conditional probability,Exponential conditional probability,,"There are two types of claims that are made to an insurance company. Let $N_i(t)$ denote the number of type $i$ claims made by time $t$, and suppose that $\{N_1(t), t \ge 0\}$ and $\{N_2(t), t \ge 0\}$ are independent Poisson processes with rates $位_1 = 10$ and $位_2 = 1$. The amounts of successive type 1 claims are independent exponential random variables with mean 1000 dollars whereas the amounts from type 2 claims are independent exponential random variables with mean 5000 dollars. A claim for 4000 dollars has just been received; what is the probability it is a type 1 claim? Here's my approach to the problem: \begin{align} & P(\text{claim} = \text{type 1}\mid 4000) \\[10pt] = {} & \frac{P(4000\mid \text{claim} = \text{type 1})(\text{claim} = \text{type 1})}{P(4000\mid \text{claim} = \text{type 1})P(\text{claim} = \text{type 1}) + P(4000\mid\text{claim} = \text{type 2})P(\text{claim} = \text{type 2})} \end{align} by Bayes' formula. My question is, how do I calculate $P(4000\mid \text{claim} = \text{type 1})$ and $P(4000\mid \text{claim} = \text{type 2})$? The dollar value of claims is exponentially distributed, and since the distribution is continuous I can't fix a value to the pdf, I need a range. Any tips on how to calculate these two values?","There are two types of claims that are made to an insurance company. Let $N_i(t)$ denote the number of type $i$ claims made by time $t$, and suppose that $\{N_1(t), t \ge 0\}$ and $\{N_2(t), t \ge 0\}$ are independent Poisson processes with rates $位_1 = 10$ and $位_2 = 1$. The amounts of successive type 1 claims are independent exponential random variables with mean 1000 dollars whereas the amounts from type 2 claims are independent exponential random variables with mean 5000 dollars. A claim for 4000 dollars has just been received; what is the probability it is a type 1 claim? Here's my approach to the problem: \begin{align} & P(\text{claim} = \text{type 1}\mid 4000) \\[10pt] = {} & \frac{P(4000\mid \text{claim} = \text{type 1})(\text{claim} = \text{type 1})}{P(4000\mid \text{claim} = \text{type 1})P(\text{claim} = \text{type 1}) + P(4000\mid\text{claim} = \text{type 2})P(\text{claim} = \text{type 2})} \end{align} by Bayes' formula. My question is, how do I calculate $P(4000\mid \text{claim} = \text{type 1})$ and $P(4000\mid \text{claim} = \text{type 2})$? The dollar value of claims is exponentially distributed, and since the distribution is continuous I can't fix a value to the pdf, I need a range. Any tips on how to calculate these two values?",,"['probability', 'statistics', 'bayes-theorem', 'exponential-distribution']"
2,Expectation of HTH (fair coin tosses),Expectation of HTH (fair coin tosses),,"There is a sequence of fair coin tosses with Head (H) and Tail (T). I have the stopping time $S=\text{{the first time we get HTH}}$. I want to calculate the $E[S]$, which is $10$. I have to use the optional stopping theorem , but I do not know, how to calculate this. Can somebody help me? I found a solution by combinatoric and by Marcov Chain, but I need one by Optional Stopping..","There is a sequence of fair coin tosses with Head (H) and Tail (T). I have the stopping time $S=\text{{the first time we get HTH}}$. I want to calculate the $E[S]$, which is $10$. I have to use the optional stopping theorem , but I do not know, how to calculate this. Can somebody help me? I found a solution by combinatoric and by Marcov Chain, but I need one by Optional Stopping..",,"['probability', 'probability-theory', 'expectation', 'stopping-times']"
3,Probability of sixth ball to be white,Probability of sixth ball to be white,,"A box contains 7 identical white balls and 5 identical black balls. They are to be drawn randomly one at a time without replacement until the box is empty. Find the probability that the 6th ball drawn is white, while before that exactly 3 black balls are drawn. Source : Principle and techniques in combinatorics by Chen Chuan Chong , Ch 1, question 24 My Approach Let '1' denote a white ball and '0' denote a black ball. Consider following sequence: 1 1 0 0 0 1 (i.e., first two balls drawn are white then next three balls are black and finally a white ball) The probability of this event is: $\displaystyle \frac 7{12} \cdot \frac 6{11} \cdot \frac 5{10} \cdot \frac 49 \cdot \frac 38 \cdot \frac 27 \cdot = \frac 1{132}$ Now, there are 10 binary sequences in which 6th digit is 1 and before them three digits are 0 hence, probability should be $\displaystyle \frac {10}{132}$ Correct Answer : $\displaystyle \frac{25}{132}$ Please indicate my mistake and if possible please give a complete solution. Thank You","A box contains 7 identical white balls and 5 identical black balls. They are to be drawn randomly one at a time without replacement until the box is empty. Find the probability that the 6th ball drawn is white, while before that exactly 3 black balls are drawn. Source : Principle and techniques in combinatorics by Chen Chuan Chong , Ch 1, question 24 My Approach Let '1' denote a white ball and '0' denote a black ball. Consider following sequence: 1 1 0 0 0 1 (i.e., first two balls drawn are white then next three balls are black and finally a white ball) The probability of this event is: $\displaystyle \frac 7{12} \cdot \frac 6{11} \cdot \frac 5{10} \cdot \frac 49 \cdot \frac 38 \cdot \frac 27 \cdot = \frac 1{132}$ Now, there are 10 binary sequences in which 6th digit is 1 and before them three digits are 0 hence, probability should be $\displaystyle \frac {10}{132}$ Correct Answer : $\displaystyle \frac{25}{132}$ Please indicate my mistake and if possible please give a complete solution. Thank You",,"['probability', 'combinatorics', 'proof-verification', 'permutations']"
4,"expected sum of the numbers that appear on two dice, each biased so that a 3 comes up twice as often as each other number?","expected sum of the numbers that appear on two dice, each biased so that a 3 comes up twice as often as each other number?",,"This question is already posted here ,but i want to check my approach. Question What is the expected sum of the numbers that appear on two dice, each biased so that a $3$ comes up twice as often as each other number? My Approach Let the probability of getting number other than $3$ is $p$ so $$\frac{1}{p}+\frac{1}{p}+\frac{2}{p}+\frac{1}{p}+\frac{1}{p}+\frac{1}{p}=1 \Rightarrow p=\frac{1}{7}$$ Proability of getting $3=\frac{2}{7}$ and rest other $=\frac{1}{7}$ let $E(X_1)$ be the expectation of getting sum on rolling $1$ dice. $E(X_1)=1 \times \frac{1}{7}+2 \times \frac{1}{7}+3 \times \frac{2}{7}+4 \times \frac{1}{7}+5 \times \frac{1}{7}+6 \times \frac{1}{7}=\frac{24}{7}$ Now expected sum of the numbers that appear on two dice $$E(X_1 +X_2)=E(X_1)+E(X_2)=\frac{24}{7}+\frac{24}{7}=\frac{48}{7}\approx 6.86$$ Is my approach correct?","This question is already posted here ,but i want to check my approach. Question What is the expected sum of the numbers that appear on two dice, each biased so that a comes up twice as often as each other number? My Approach Let the probability of getting number other than is so Proability of getting and rest other let be the expectation of getting sum on rolling dice. Now expected sum of the numbers that appear on two dice Is my approach correct?",3 3 p \frac{1}{p}+\frac{1}{p}+\frac{2}{p}+\frac{1}{p}+\frac{1}{p}+\frac{1}{p}=1 \Rightarrow p=\frac{1}{7} 3=\frac{2}{7} =\frac{1}{7} E(X_1) 1 E(X_1)=1 \times \frac{1}{7}+2 \times \frac{1}{7}+3 \times \frac{2}{7}+4 \times \frac{1}{7}+5 \times \frac{1}{7}+6 \times \frac{1}{7}=\frac{24}{7} E(X_1 +X_2)=E(X_1)+E(X_2)=\frac{24}{7}+\frac{24}{7}=\frac{48}{7}\approx 6.86,"['probability', 'expectation']"
5,Dice game - compute the probability player A wins,Dice game - compute the probability player A wins,,"A few weeks ago, I came across the following problem. Alice and Bob are planning to gamble against each other. Before they start to play the game, Bob wants to calculate its chance of winning to determine if he has a positive expectation. The game works as follows. Charlie starts throwing two dice and he records the sum of the two dice each time. If at a certain point a sequence of two $7$s in a row occurs, Alice wins the game and the game ends. If on the contrary, a sequence of three strictly increasing numbers occurs, Bob wins the game and the game ends. What is the probability that Bob (or Alice) wins the game? I am looking for a theoretical method of solving this problem. I tried a few different ones but it all didn't seem to work. I am thinking that some version of using Markov chains could work, although I don't see how. After doing some simulations I found: The expectation of the number of tries before a sequence of two $7$s is a row occurs, is approximately $42$. The expectation of the number of tries before a sequence of three increasing numbers is a row occurs, is approximately $10.5$. It doesn't seem to work to divide the expectations of the number of tries although you come close, i.e., there is probably some dependence relation going on. The probability of Alice winning is approximately $19\%$. Edit : I got the following probabilty of Alice winning to be equal to $$ \frac{450074090171}{556534555787} \approx 80.87\% $$ after applying joriki's method. Via simulations and doing some confidence interval analysis, this appears to be correct since it is inside some of the 95% confidence intervals I constructed: $$ [80.76215853,80.98464147]. $$ I used the following ordering of the $20$ unknowns: $$(p_2,p_3^-,p_4^-,...,p_{11}^-,p_3^+,p_4^+,...,p_{11}^+,p_{12})$$ The matrix induced by the linear equations is given by $[A_1,A_2]$ (I multiplied the equations by $36$, note that at two places I have written a * which is related to Alice winning, i.e., two $7$s in a row) $$ A_1 = \begin{pmatrix}-35&0&0&0&0&0&0&0&0&0\\  1&-34&0&0&0&0&0&0&0&0\\  1&2&-33&0&0&0&0&0&0&0\\  1&2&3&-32&0&0&0&0&0&0\\  1&2&3&4&-31&0&0&0&0&0\\  1&2&3&4&5&-36*&0&0&0&0\\  1&2&3&4&5&6&-31&0&0&0\\  1&2&3&4&5&6&5&-32&0&0\\  1&2&3&4&5&6&5&4&-33&0\\  1&2&3&4&5&6&5&4&3&-34\\  1&2&0&0&0&0&0&0&0&0\\  1&2&3&0&0&0&0&0&0&0\\  1&2&3&4&0&0&0&0&0&0\\  1&2&3&4&5&0&0&0&0&0\\  1&2&3&4&5&0*&0&0&0&0\\  1&2&3&4&5&6&5&0&0&0\\  1&2&3&4&5&6&5&4&0&0\\  1&2&3&4&5&6&5&4&3&0\\  1&2&3&4&5&6&5&4&3&2\\  1&2&3&4&5&6&5&4&3&2\\  \end{pmatrix} $$ $$ A_2 = \begin{pmatrix}2&3&4&5&6&5&4&3&2&1\\  0&3&4&5&6&5&4&3&2&1\\  0&0&4&5&6&5&4&3&2&1\\  0&0&0&5&6&5&4&3&2&1\\  0&0&0&0&6&5&4&3&2&1\\  0&0&0&0&0&5&4&3&2&1\\  0&0&0&0&0&0&4&3&2&1\\  0&0&0&0&0&0&0&3&2&1\\  0&0&0&0&0&0&0&0&2&1\\  0&0&0&0&0&0&0&0&0&1\\  -36&0&0&0&0&0&0&0&0&0\\  0&-36&0&0&0&0&0&0&0&0\\  0&0&-36&0&0&0&0&0&0&0\\  0&0&0&-36&0&0&0&0&0&0\\  0&0&0&0&-36&0&0&0&0&0\\  0&0&0&0&0&-36&0&0&0&0\\  0&0&0&0&0&0&-36&0&0&0\\  0&0&0&0&0&0&0&-36&0&0\\  0&0&0&0&0&0&0&0&-36&0\\  0&0&0&0&0&0&0&0&0&-35\\  \end{pmatrix} $$ Its inverse should now be calculated and multiplied with the vector $$ \begin{pmatrix}0&0&0&0&0&0&0&0&0&0&-33&-30&-26&-21&-15&-10&-6&-3&-1&0&\end{pmatrix}'. $$ The $20$th component of the resulting vector is now equal to $p_{12}$, the probability of Bob winning the game. Thanks to joriki for finding the solution method and finding an error I made!","A few weeks ago, I came across the following problem. Alice and Bob are planning to gamble against each other. Before they start to play the game, Bob wants to calculate its chance of winning to determine if he has a positive expectation. The game works as follows. Charlie starts throwing two dice and he records the sum of the two dice each time. If at a certain point a sequence of two $7$s in a row occurs, Alice wins the game and the game ends. If on the contrary, a sequence of three strictly increasing numbers occurs, Bob wins the game and the game ends. What is the probability that Bob (or Alice) wins the game? I am looking for a theoretical method of solving this problem. I tried a few different ones but it all didn't seem to work. I am thinking that some version of using Markov chains could work, although I don't see how. After doing some simulations I found: The expectation of the number of tries before a sequence of two $7$s is a row occurs, is approximately $42$. The expectation of the number of tries before a sequence of three increasing numbers is a row occurs, is approximately $10.5$. It doesn't seem to work to divide the expectations of the number of tries although you come close, i.e., there is probably some dependence relation going on. The probability of Alice winning is approximately $19\%$. Edit : I got the following probabilty of Alice winning to be equal to $$ \frac{450074090171}{556534555787} \approx 80.87\% $$ after applying joriki's method. Via simulations and doing some confidence interval analysis, this appears to be correct since it is inside some of the 95% confidence intervals I constructed: $$ [80.76215853,80.98464147]. $$ I used the following ordering of the $20$ unknowns: $$(p_2,p_3^-,p_4^-,...,p_{11}^-,p_3^+,p_4^+,...,p_{11}^+,p_{12})$$ The matrix induced by the linear equations is given by $[A_1,A_2]$ (I multiplied the equations by $36$, note that at two places I have written a * which is related to Alice winning, i.e., two $7$s in a row) $$ A_1 = \begin{pmatrix}-35&0&0&0&0&0&0&0&0&0\\  1&-34&0&0&0&0&0&0&0&0\\  1&2&-33&0&0&0&0&0&0&0\\  1&2&3&-32&0&0&0&0&0&0\\  1&2&3&4&-31&0&0&0&0&0\\  1&2&3&4&5&-36*&0&0&0&0\\  1&2&3&4&5&6&-31&0&0&0\\  1&2&3&4&5&6&5&-32&0&0\\  1&2&3&4&5&6&5&4&-33&0\\  1&2&3&4&5&6&5&4&3&-34\\  1&2&0&0&0&0&0&0&0&0\\  1&2&3&0&0&0&0&0&0&0\\  1&2&3&4&0&0&0&0&0&0\\  1&2&3&4&5&0&0&0&0&0\\  1&2&3&4&5&0*&0&0&0&0\\  1&2&3&4&5&6&5&0&0&0\\  1&2&3&4&5&6&5&4&0&0\\  1&2&3&4&5&6&5&4&3&0\\  1&2&3&4&5&6&5&4&3&2\\  1&2&3&4&5&6&5&4&3&2\\  \end{pmatrix} $$ $$ A_2 = \begin{pmatrix}2&3&4&5&6&5&4&3&2&1\\  0&3&4&5&6&5&4&3&2&1\\  0&0&4&5&6&5&4&3&2&1\\  0&0&0&5&6&5&4&3&2&1\\  0&0&0&0&6&5&4&3&2&1\\  0&0&0&0&0&5&4&3&2&1\\  0&0&0&0&0&0&4&3&2&1\\  0&0&0&0&0&0&0&3&2&1\\  0&0&0&0&0&0&0&0&2&1\\  0&0&0&0&0&0&0&0&0&1\\  -36&0&0&0&0&0&0&0&0&0\\  0&-36&0&0&0&0&0&0&0&0\\  0&0&-36&0&0&0&0&0&0&0\\  0&0&0&-36&0&0&0&0&0&0\\  0&0&0&0&-36&0&0&0&0&0\\  0&0&0&0&0&-36&0&0&0&0\\  0&0&0&0&0&0&-36&0&0&0\\  0&0&0&0&0&0&0&-36&0&0\\  0&0&0&0&0&0&0&0&-36&0\\  0&0&0&0&0&0&0&0&0&-35\\  \end{pmatrix} $$ Its inverse should now be calculated and multiplied with the vector $$ \begin{pmatrix}0&0&0&0&0&0&0&0&0&0&-33&-30&-26&-21&-15&-10&-6&-3&-1&0&\end{pmatrix}'. $$ The $20$th component of the resulting vector is now equal to $p_{12}$, the probability of Bob winning the game. Thanks to joriki for finding the solution method and finding an error I made!",,"['probability', 'markov-chains', 'game-theory', 'dice']"
6,Probability path - Exercise 6.14 : on almost sure divergence,Probability path - Exercise 6.14 : on almost sure divergence,,Let $\{X_n\}$ be independent with $P(X_n = n^2) = \frac{1}{n}$ and $P(X_n = -1) = 1 - \frac{1}{n}$ . Show that $\sum_{n=1}^{\infty}X_n  = -\infty$ almost surely. I found that $E[X_n] = n + \frac{1}{n} - 1 \to \infty$ . So intuitively it has to tend to $\infty$ . but why $-\infty$ ?,Let be independent with and . Show that almost surely. I found that . So intuitively it has to tend to . but why ?,\{X_n\} P(X_n = n^2) = \frac{1}{n} P(X_n = -1) = 1 - \frac{1}{n} \sum_{n=1}^{\infty}X_n  = -\infty E[X_n] = n + \frac{1}{n} - 1 \to \infty \infty -\infty,"['probability', 'expectation', 'almost-everywhere']"
7,Sampling from partitions of graph vertices into connected subsets,Sampling from partitions of graph vertices into connected subsets,,"Suppose I have a connected graph $G=(V,E)$, where $E\subseteq V\times V$ contains undirected edges.  For $V'\subseteq V$, denote $G_{V'}$ as the induced subgraph $G_{V'}:=(V',E\cap(V'\times V')).$ Denote $S$ as the set of partitions into two connected components: $$ S:=\{V'\subseteq V : G_{V'}\textrm{ and }G_{V\backslash V'}\textrm{ are connected subgraphs}\}. $$ Is it tractable to sample from $S$ with uniform probability for each element? Apologies for clunky notation.  Some ideas that sample non-uniformly include: Randomly draw a spanning tree and cut one edge Merge pairs of vertices together until two clusters are left","Suppose I have a connected graph $G=(V,E)$, where $E\subseteq V\times V$ contains undirected edges.  For $V'\subseteq V$, denote $G_{V'}$ as the induced subgraph $G_{V'}:=(V',E\cap(V'\times V')).$ Denote $S$ as the set of partitions into two connected components: $$ S:=\{V'\subseteq V : G_{V'}\textrm{ and }G_{V\backslash V'}\textrm{ are connected subgraphs}\}. $$ Is it tractable to sample from $S$ with uniform probability for each element? Apologies for clunky notation.  Some ideas that sample non-uniformly include: Randomly draw a spanning tree and cut one edge Merge pairs of vertices together until two clusters are left",,"['probability', 'discrete-mathematics', 'graph-theory', 'sampling', 'set-partition']"
8,Probabilistic/combinatoric proof of $\sum_{k=0}^{n}\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}\frac{r}{tk+r}=\binom{tn+r+s}{n}$,Probabilistic/combinatoric proof of,\sum_{k=0}^{n}\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}\frac{r}{tk+r}=\binom{tn+r+s}{n},"In this posting , OP asks a proof of the following identity $$ \sum_{k=0}^{n} \binom{tk+r}{k}\binom{t(n-k)+s}{n-k} \frac{r}{tk+r} = \binom{tn+r+s}{n} \tag{1} $$ for non-negative integers $t, n, r, s$ with $t, n \geq 1$. I would like to reformulate this question in terms of probability. Here is my try: Setting. Let $(\Omega, 2^{\Omega}, \mathbb{P})$ be the probability space where $\Omega = \{ \omega \subseteq [tn+r+s] : |\omega| = n\}$ is the family of all subsets of $[tn+r+s]$ with size $n$, and $\mathbb{P}$ is the law of uniform distribution on $\Omega$, i.e., $\mathbb{P}(\{\omega\}) = \frac{1}{|\Omega|}$ for each $\omega \in \Omega$. Then define random variables $S_k$ and $T$ on $\Omega$ by $S_k(\omega) = \left|\omega\cap[tk+r]\right|$, for $k = 0, \cdots, n$, $T(\omega) := \min\{ k \geq 0 : S_k(\omega) = k \}$. $\hspace{9em}$ Since $k \mapsto S_k(\omega)$ is a non-decreasing function from $\{0,\cdots,n\}$ to itself, this map has a fixed point and the above definition makes sense. Then I am interested the following claim: Claim. We have $$ \mathbb{P}(T = k) = \frac{r}{tk+r} \frac{\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}}{\binom{tn+r+s}{n}}. \tag{2} $$ Given this claim, the identity $\text{(1)}$ is simply $1 = \mathbb{P}(T < \infty) = \sum_{k=0}^{n} \mathbb{P}(T = k) $. I checked this claim for various values of $t, n, r, s$ but was unable to establish a proof. So, here is a question: Although there is a proof using complex analysis in the original posting , I would be happy to know whether a probabilistic or combinatorial proof of $\text{(1)}$ or $\text{(2)}$ is available, not necessarily based on the setting above. It might be helpful to note that $\text{(2)}$ is equivalent to proving the following problem: For each $k$, the number of subsets $A$ of $[tk+r]$ satisfying $|A| = k$ and $|A\cap[tj+r]| > j$ for all $j<k$ is given by $\frac{r}{tk+r}\binom{tk+r}{k}$. This sounds like a generalization of Catalan numbers, although I have no good idea for this.","In this posting , OP asks a proof of the following identity $$ \sum_{k=0}^{n} \binom{tk+r}{k}\binom{t(n-k)+s}{n-k} \frac{r}{tk+r} = \binom{tn+r+s}{n} \tag{1} $$ for non-negative integers $t, n, r, s$ with $t, n \geq 1$. I would like to reformulate this question in terms of probability. Here is my try: Setting. Let $(\Omega, 2^{\Omega}, \mathbb{P})$ be the probability space where $\Omega = \{ \omega \subseteq [tn+r+s] : |\omega| = n\}$ is the family of all subsets of $[tn+r+s]$ with size $n$, and $\mathbb{P}$ is the law of uniform distribution on $\Omega$, i.e., $\mathbb{P}(\{\omega\}) = \frac{1}{|\Omega|}$ for each $\omega \in \Omega$. Then define random variables $S_k$ and $T$ on $\Omega$ by $S_k(\omega) = \left|\omega\cap[tk+r]\right|$, for $k = 0, \cdots, n$, $T(\omega) := \min\{ k \geq 0 : S_k(\omega) = k \}$. $\hspace{9em}$ Since $k \mapsto S_k(\omega)$ is a non-decreasing function from $\{0,\cdots,n\}$ to itself, this map has a fixed point and the above definition makes sense. Then I am interested the following claim: Claim. We have $$ \mathbb{P}(T = k) = \frac{r}{tk+r} \frac{\binom{tk+r}{k}\binom{t(n-k)+s}{n-k}}{\binom{tn+r+s}{n}}. \tag{2} $$ Given this claim, the identity $\text{(1)}$ is simply $1 = \mathbb{P}(T < \infty) = \sum_{k=0}^{n} \mathbb{P}(T = k) $. I checked this claim for various values of $t, n, r, s$ but was unable to establish a proof. So, here is a question: Although there is a proof using complex analysis in the original posting , I would be happy to know whether a probabilistic or combinatorial proof of $\text{(1)}$ or $\text{(2)}$ is available, not necessarily based on the setting above. It might be helpful to note that $\text{(2)}$ is equivalent to proving the following problem: For each $k$, the number of subsets $A$ of $[tk+r]$ satisfying $|A| = k$ and $|A\cap[tj+r]| > j$ for all $j<k$ is given by $\frac{r}{tk+r}\binom{tk+r}{k}$. This sounds like a generalization of Catalan numbers, although I have no good idea for this.",,"['probability', 'combinatorics', 'alternative-proof']"
9,pdf of a member of a sequence of dependent random variables,pdf of a member of a sequence of dependent random variables,,"I would very much appreciate a hint for the following problem Let $\left(X_n\right)_{n=1}^\infty$ be a sequence of random variables s.t.: $$X_1 \sim U_{[0,1]}$$ and for all $n>1$: $$X_n \sim U_{[0,X_{n-1}]}.$$ Give a general expression for $f_{X_n}$ the pdf of $X_n$. Thanks","I would very much appreciate a hint for the following problem Let $\left(X_n\right)_{n=1}^\infty$ be a sequence of random variables s.t.: $$X_1 \sim U_{[0,1]}$$ and for all $n>1$: $$X_n \sim U_{[0,X_{n-1}]}.$$ Give a general expression for $f_{X_n}$ the pdf of $X_n$. Thanks",,"['probability', 'probability-theory']"
10,Strong law of large numbers for function of random vector: can we apply it for a component only?,Strong law of large numbers for function of random vector: can we apply it for a component only?,,"Consider i.i.d. random variables $\{X_1,..., X_n\}$ with well defined first moment i.i.d. random variables $\{Y_1,..., Y_n\}$ with well defined first moment By the strong law of large numbers:  $$ \frac{1}{n}\sum_{i=1}^n Y_i \rightarrow_{a.s.} E(Y_i) \text{ }\text{ as $n\rightarrow \infty$} $$ Consider these three objects for any function $g: \mathbb{R}^2\rightarrow \mathbb{R}$ (take $Y_i$ discrete with support $\mathcal{Y}$ for simplicity) 1) for a given realisation $x$ of $X_k$,  $E(g(X_k, Y_i)| X_k=x)\equiv \sum_{y\in \mathcal{Y}} g(x, y)P(Y_i=y|X_k=x)$ which is a scalar 2) $E(g(X_k, Y_i)| X_k)\equiv \sum_{y\in \mathcal{Y}} g(X_k, y)P(Y_i=y|X_k)$ which is a random variable because $g(X_k,y)$ and $P(Y_i=y|X_k)$ are both functions of the random variable  $X_k$ 3) $F(X_k)\equiv\sum_{y\in \mathcal{Y}}g(X_k, y)\mathbb{P}(Y_i=y)$ which is a random variable because $g(X_k, y)$ is a function of $X_k$. When $X_k\perp Y_i$, then $(2)=(3)$. Question : 1) Is it true that $\forall k=1,...,n$ $$ \frac{1}{n}\sum_{i=1}^n g(X_k, Y_i) \rightarrow_{a.s.} F(X_k) \text{ }\text{ as $n\rightarrow \infty$} $$ If yes, under which conditions? 2) Is it true that $\forall k=1,...,n$ $$ \frac{1}{n}\sum_{i=1}^n g(X_k, Y_i) \rightarrow_{a.s.} E(g(X_k, Y_i)|X_k) \text{ }\text{ as $n\rightarrow \infty$} $$ If yes, under which conditions? EDIT : This question here is close to mine and includes also an answer. However it is for $g(X_k, Y_i)=Y_i\times X_k$","Consider i.i.d. random variables $\{X_1,..., X_n\}$ with well defined first moment i.i.d. random variables $\{Y_1,..., Y_n\}$ with well defined first moment By the strong law of large numbers:  $$ \frac{1}{n}\sum_{i=1}^n Y_i \rightarrow_{a.s.} E(Y_i) \text{ }\text{ as $n\rightarrow \infty$} $$ Consider these three objects for any function $g: \mathbb{R}^2\rightarrow \mathbb{R}$ (take $Y_i$ discrete with support $\mathcal{Y}$ for simplicity) 1) for a given realisation $x$ of $X_k$,  $E(g(X_k, Y_i)| X_k=x)\equiv \sum_{y\in \mathcal{Y}} g(x, y)P(Y_i=y|X_k=x)$ which is a scalar 2) $E(g(X_k, Y_i)| X_k)\equiv \sum_{y\in \mathcal{Y}} g(X_k, y)P(Y_i=y|X_k)$ which is a random variable because $g(X_k,y)$ and $P(Y_i=y|X_k)$ are both functions of the random variable  $X_k$ 3) $F(X_k)\equiv\sum_{y\in \mathcal{Y}}g(X_k, y)\mathbb{P}(Y_i=y)$ which is a random variable because $g(X_k, y)$ is a function of $X_k$. When $X_k\perp Y_i$, then $(2)=(3)$. Question : 1) Is it true that $\forall k=1,...,n$ $$ \frac{1}{n}\sum_{i=1}^n g(X_k, Y_i) \rightarrow_{a.s.} F(X_k) \text{ }\text{ as $n\rightarrow \infty$} $$ If yes, under which conditions? 2) Is it true that $\forall k=1,...,n$ $$ \frac{1}{n}\sum_{i=1}^n g(X_k, Y_i) \rightarrow_{a.s.} E(g(X_k, Y_i)|X_k) \text{ }\text{ as $n\rightarrow \infty$} $$ If yes, under which conditions? EDIT : This question here is close to mine and includes also an answer. However it is for $g(X_k, Y_i)=Y_i\times X_k$",,"['probability', 'expectation', 'law-of-large-numbers']"
11,Probability the driver has no accident in the next 365 days,Probability the driver has no accident in the next 365 days,,"The time until the next car accident for a particular driver is exponentially distributed with a mean of 200 days. Calculate the probability that the driver has no accidents in the next 365 days, but then has at least one accident in the 365-day period that follows this initial 365-day period. Attempt Let $T$ be the time it takes for a driver to have a car accident. We are given $T$ is $exp( \lambda = 1/200 )$ . We need to find $$ P(T > 365) = 1 - F(365) = 1 - 1 + e^{-365/200} = 0.1612 $$ Is this correct? MY answer key says the correct answer should be $\boxed{0.1352}$ . What am I missing here?","The time until the next car accident for a particular driver is exponentially distributed with a mean of 200 days. Calculate the probability that the driver has no accidents in the next 365 days, but then has at least one accident in the 365-day period that follows this initial 365-day period. Attempt Let be the time it takes for a driver to have a car accident. We are given is . We need to find Is this correct? MY answer key says the correct answer should be . What am I missing here?",T T exp( \lambda = 1/200 )  P(T > 365) = 1 - F(365) = 1 - 1 + e^{-365/200} = 0.1612  \boxed{0.1352},['probability']
12,Probability of a coin stack being greater than a value? What's wrong with my reasoning?,Probability of a coin stack being greater than a value? What's wrong with my reasoning?,,"Basic probability question. Consider a pile of 9 coins where each could either be 1 cent or 10   cents and the distribution of the coin combinations is uniform.   Knowing that the upper 4 coins are all 10 cents, what is the   probability that the total value is greater than 50 cents? My reasoning was simply that we have 5 coins leftover and we needs at least 10 more cents to get to 50 cents. We have a total of $2^5$ combinations for the remaining 5 coins. Our sample space size is $2^5-1$ because the only way which wouldn't work out is if we get all pennies. So the probability should be $\frac{2^5-1}{2^5}$ What's wrong here?","Basic probability question. Consider a pile of 9 coins where each could either be 1 cent or 10   cents and the distribution of the coin combinations is uniform.   Knowing that the upper 4 coins are all 10 cents, what is the   probability that the total value is greater than 50 cents? My reasoning was simply that we have 5 coins leftover and we needs at least 10 more cents to get to 50 cents. We have a total of $2^5$ combinations for the remaining 5 coins. Our sample space size is $2^5-1$ because the only way which wouldn't work out is if we get all pennies. So the probability should be $\frac{2^5-1}{2^5}$ What's wrong here?",,['probability']
13,Constant random variable,Constant random variable,,How do I plot the cumulative distribution function and probability mass function of the constant random variable $X(\omega)=2$ for all $\omega$?,How do I plot the cumulative distribution function and probability mass function of the constant random variable $X(\omega)=2$ for all $\omega$?,,"['probability', 'statistics', 'probability-distributions', 'random-variables', 'constants']"
14,Probability distribution over bijections,Probability distribution over bijections,,"Does there exist a probability distribution over bijections $f:[0,1]\rightarrow [0,1]$ such that for any $x,y,z\in[0,1]$, the probability that $f(x)=y$ is the same as the probability that $f(x)=z$? I'm not sure if I understand this question correctly. Since there are an infinite number of possible $y,z$, this probability must be $0$. But does that already mean that no such distribution exists?","Does there exist a probability distribution over bijections $f:[0,1]\rightarrow [0,1]$ such that for any $x,y,z\in[0,1]$, the probability that $f(x)=y$ is the same as the probability that $f(x)=z$? I'm not sure if I understand this question correctly. Since there are an infinite number of possible $y,z$, this probability must be $0$. But does that already mean that no such distribution exists?",,"['real-analysis', 'probability', 'functions', 'probability-distributions']"
15,What is the probability that $n$ integers chosen at random are coprime?,What is the probability that  integers chosen at random are coprime?,n,"It is extremely well-known that the probability of any two random integers being relatively prime is $\zeta(2)^{-1}$ ( see here ). From An Introduction to Analytic Number Theory by Apostol, the proof involves counting lattice points and finding the limit $$\lim_{r\to\infty}\frac{N'(r)}{N(r)}$$ where $N(r)$ is the number of lattice points in the square governed by $|x|\le r$ and $|y|\le r$, and $N'(r)$ is the number of lattice points visible from the origin. For some background, the term ""visible"" is defined as follows: Two lattice points $P$ and $Q$ are said to be visible if the line joining the two does not go through any other lattice points. Now what if $n\neq2$; that is, What is the limit of the probability that $n$ integers chosen at random in the interval $[1,N]$ are coprime as $N\to\infty$ with $n>2$ ? Let's consider the simplest case: $n=3$. We imitate the proof for when $n=2$. Firstly, an extension to Thm 3.8 can be easily proven. Theorem: Two lattice points $(a_1,\cdots,a_n)$ and $(b_1,\cdots,b_n)$ are visible iff $a_1-b_1$, $a_2-b_2$ up to $a_n-b_n$ are relatively prime. The $24$ lattice points nearest the origin are all visible from the origin - there are $24$ points ""surrounding"" the origin of unit distance. By symmetry, we see that $N'(r)$ is equal to $24$, plus $24$ times the number of visible points in the region $$\{(x,y,z):2\le x\le r, ???\}$$ We cannot use $1\le y\le x$ since the gradient of the line joining the origin and $(r,r,r)$ is no longer $1$. Of course, we can try to use this , but I feel that this makes it more complicated than it should be. So how should I continue? Is there an alternative method? And what would be the general approach for large $n$, ie. is there an expression (in terms of $n$) that finds the probability that $n$ integers chosen at random are coprime?","It is extremely well-known that the probability of any two random integers being relatively prime is $\zeta(2)^{-1}$ ( see here ). From An Introduction to Analytic Number Theory by Apostol, the proof involves counting lattice points and finding the limit $$\lim_{r\to\infty}\frac{N'(r)}{N(r)}$$ where $N(r)$ is the number of lattice points in the square governed by $|x|\le r$ and $|y|\le r$, and $N'(r)$ is the number of lattice points visible from the origin. For some background, the term ""visible"" is defined as follows: Two lattice points $P$ and $Q$ are said to be visible if the line joining the two does not go through any other lattice points. Now what if $n\neq2$; that is, What is the limit of the probability that $n$ integers chosen at random in the interval $[1,N]$ are coprime as $N\to\infty$ with $n>2$ ? Let's consider the simplest case: $n=3$. We imitate the proof for when $n=2$. Firstly, an extension to Thm 3.8 can be easily proven. Theorem: Two lattice points $(a_1,\cdots,a_n)$ and $(b_1,\cdots,b_n)$ are visible iff $a_1-b_1$, $a_2-b_2$ up to $a_n-b_n$ are relatively prime. The $24$ lattice points nearest the origin are all visible from the origin - there are $24$ points ""surrounding"" the origin of unit distance. By symmetry, we see that $N'(r)$ is equal to $24$, plus $24$ times the number of visible points in the region $$\{(x,y,z):2\le x\le r, ???\}$$ We cannot use $1\le y\le x$ since the gradient of the line joining the origin and $(r,r,r)$ is no longer $1$. Of course, we can try to use this , but I feel that this makes it more complicated than it should be. So how should I continue? Is there an alternative method? And what would be the general approach for large $n$, ie. is there an expression (in terms of $n$) that finds the probability that $n$ integers chosen at random are coprime?",,"['probability', 'analytic-number-theory']"
16,Is the probability of getting the mail today vs tomorrow independent from the day?,Is the probability of getting the mail today vs tomorrow independent from the day?,,"I need some help seeing if I understand independence and conditional probability. Let's say I'm expecting a package. It's priority mail so I know at most it will take $3$ days to come. The probability of getting it in the first day $\Pr(A)$ is $0.\overline{33}$. The probability of getting it the second day given it didn't arrive the day before $\Pr(B \mid \neg A)$ is $0.50$ (right?). And the probability of getting it in the $3$rd day given not day $1$ or day $2$ $\Pr(C \mid \neg A \wedge \neg B)$ is $1$ (right?). So if my logic is correct so far, if I investigate independence, I will find: $\Pr(B \mid \neg A)$ is $0.5$ and $\Pr(B)$ is $0.\overline{33}$; therefore, the events are not independent. Same applies for the third day. In English, not getting a package in a particular day influences the probability of getting it in the other two days by improving the odds, which intuitively makes sense. Or am I off the mark?","I need some help seeing if I understand independence and conditional probability. Let's say I'm expecting a package. It's priority mail so I know at most it will take $3$ days to come. The probability of getting it in the first day $\Pr(A)$ is $0.\overline{33}$. The probability of getting it the second day given it didn't arrive the day before $\Pr(B \mid \neg A)$ is $0.50$ (right?). And the probability of getting it in the $3$rd day given not day $1$ or day $2$ $\Pr(C \mid \neg A \wedge \neg B)$ is $1$ (right?). So if my logic is correct so far, if I investigate independence, I will find: $\Pr(B \mid \neg A)$ is $0.5$ and $\Pr(B)$ is $0.\overline{33}$; therefore, the events are not independent. Same applies for the third day. In English, not getting a package in a particular day influences the probability of getting it in the other two days by improving the odds, which intuitively makes sense. Or am I off the mark?",,"['probability', 'independence']"
17,Conditional Variance of the sum of two variables,Conditional Variance of the sum of two variables,,"$\newcommand{\v}{\operatorname{Var}}\newcommand{\c}{\operatorname{Cov}}$I have a simple question that, after thinking for a while, got me confused and I cannot figure it out. Does the following statements hold? 1 $$ \v(A + B + C \mid \theta ) = \v(A \mid \theta) + \v(B + C \mid \theta) + 2 \c(A, B + C \mid \theta)$$ 2 $$ \c(A, B + C \mid \theta ) = \c(A, B \mid \theta) + \c(A, C \mid \theta)$$ I am not sure whether I can do these two steps above. Thanks","$\newcommand{\v}{\operatorname{Var}}\newcommand{\c}{\operatorname{Cov}}$I have a simple question that, after thinking for a while, got me confused and I cannot figure it out. Does the following statements hold? 1 $$ \v(A + B + C \mid \theta ) = \v(A \mid \theta) + \v(B + C \mid \theta) + 2 \c(A, B + C \mid \theta)$$ 2 $$ \c(A, B + C \mid \theta ) = \c(A, B \mid \theta) + \c(A, C \mid \theta)$$ I am not sure whether I can do these two steps above. Thanks",,"['probability', 'statistics', 'covariance', 'variance']"
18,Proving that softmax converges to argmax as we scale x,Proving that softmax converges to argmax as we scale x,,"For a vector $\mathbb{x}$ , the softmax function $S:\mathbb{R}^d\times \mathbb{R}\rightarrow \mathbb{R}^d$ is defined as $$ S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{k=1}^{d} e^{c\cdot x_k}} $$ Consider if we scale the softmax with constant $c$ , $$ S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{j=1}^{d} e^{c\cdot x_j}} $$ Now since $e^x$ is an increasing and diverging function, as $c$ grows, $S(x)$ will emphasize more and more the max value. At $c \rightarrow \infty$ , $S(x)$ outputs a one-hot vector with 1 at the position of the maximum element. Now this is my intuition, but how do I prove this?","For a vector , the softmax function is defined as Consider if we scale the softmax with constant , Now since is an increasing and diverging function, as grows, will emphasize more and more the max value. At , outputs a one-hot vector with 1 at the position of the maximum element. Now this is my intuition, but how do I prove this?","\mathbb{x} S:\mathbb{R}^d\times \mathbb{R}\rightarrow \mathbb{R}^d 
S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{k=1}^{d} e^{c\cdot x_k}}
 c 
S(x;c)_i = \frac{e^{c\cdot x_i}}{\sum_{j=1}^{d} e^{c\cdot x_j}}
 e^x c S(x) c \rightarrow \infty S(x)","['probability', 'proof-writing']"
19,Let X be a Poisson random variables with $\lambda = 3.$ Find $E(X\mid X>2).$,Let X be a Poisson random variables with  Find,\lambda = 3. E(X\mid X>2).,"So I write this out as, $$E(X\mid X>2) = \frac 1 {P(X>2)}\sum_{k=2}^\infty k\frac{3^k e^{-3}}{k!}$$ For $P(X>2)$ I get .80085 and when I multiply by the sum I get 3.55947. My book, however, says that the answer is 4.16525. PS, I computed $P(X>2)$ by subtracting $P(X=0) + P(X=1)$ from $1.$ I hope I didn't mess this up somehow so I assume I made a mistake in computing the summation. Thank you in advance. Pardon any mistakes in my math. Hopefully it's not something too silly :P","So I write this out as, $$E(X\mid X>2) = \frac 1 {P(X>2)}\sum_{k=2}^\infty k\frac{3^k e^{-3}}{k!}$$ For $P(X>2)$ I get .80085 and when I multiply by the sum I get 3.55947. My book, however, says that the answer is 4.16525. PS, I computed $P(X>2)$ by subtracting $P(X=0) + P(X=1)$ from $1.$ I hope I didn't mess this up somehow so I assume I made a mistake in computing the summation. Thank you in advance. Pardon any mistakes in my math. Hopefully it's not something too silly :P",,"['probability', 'conditional-expectation']"
20,An Isserlis theorem for Elliptical Distributions?,An Isserlis theorem for Elliptical Distributions?,,"For my purposes, I am only interested in rotationally invariant Elliptical Distributions. So suppose $x$ is a $p$-dimensional random variable with zero mean and identity covariance, with density equal to $g(x^{\top}x)$. I am interested in third and fourth moments: $$ E\left[x_i x_j x_k \right] \quad\mbox{and}\quad E\left[x_i x_j x_k x_l \right], $$ where the indices may be duplicated. I have some hints from this paper by Kan, but not enough.","For my purposes, I am only interested in rotationally invariant Elliptical Distributions. So suppose $x$ is a $p$-dimensional random variable with zero mean and identity covariance, with density equal to $g(x^{\top}x)$. I am interested in third and fourth moments: $$ E\left[x_i x_j x_k \right] \quad\mbox{and}\quad E\left[x_i x_j x_k x_l \right], $$ where the indices may be duplicated. I have some hints from this paper by Kan, but not enough.",,"['probability', 'expectation']"
21,Expected number of same-colour regions in a tiled floor,Expected number of same-colour regions in a tiled floor,,"Suppose we have a rectangular floor, $a$ units long by $b$ units wide, which we need to tile with black and white unit square tiles. We flip a coin to decide whether the first tile will be black or white, and lay it down in the top left corner. The second and all subsequent tiles will also have a 50-50 chance of being black and white. A 'region' is defined as a contiguous area of same-colour tiles, touching each other by their sides (just a corner is not enough). Thus, the maximum number of regions possible is $ab$ (checkerboard tiling), while the minimum number is 1 (the whole floor is either black or white). What is the expected number of regions on the board, as a function of $a$ and $b$?","Suppose we have a rectangular floor, $a$ units long by $b$ units wide, which we need to tile with black and white unit square tiles. We flip a coin to decide whether the first tile will be black or white, and lay it down in the top left corner. The second and all subsequent tiles will also have a 50-50 chance of being black and white. A 'region' is defined as a contiguous area of same-colour tiles, touching each other by their sides (just a corner is not enough). Thus, the maximum number of regions possible is $ab$ (checkerboard tiling), while the minimum number is 1 (the whole floor is either black or white). What is the expected number of regions on the board, as a function of $a$ and $b$?",,"['probability', 'combinatorics', 'geometry']"
22,Why does a filtration represent information over time?,Why does a filtration represent information over time?,,"The book I am reading says that Ft contains all events whose occurrence or not is fixed by time t. Why is this the case? Let say A is an event that has not been fixed yet, B is an event that has been fixed. Then A union B has to be in Ft, but A union B is not fixed, is it? How should I think about this statement? Another example of this: if E is an event that contains all elements of $\Omega$ that have been fixed. Then E is in Ft and E complement contains no event that has been fixed. E complement is still in Ft. But E U $E^c$ is an event that is neither fixed, nor unfixed, but it is still in Ft? Sth is off.","The book I am reading says that Ft contains all events whose occurrence or not is fixed by time t. Why is this the case? Let say A is an event that has not been fixed yet, B is an event that has been fixed. Then A union B has to be in Ft, but A union B is not fixed, is it? How should I think about this statement? Another example of this: if E is an event that contains all elements of $\Omega$ that have been fixed. Then E is in Ft and E complement contains no event that has been fixed. E complement is still in Ft. But E U $E^c$ is an event that is neither fixed, nor unfixed, but it is still in Ft? Sth is off.",,['probability']
23,Seat friends at a dinner table,Seat friends at a dinner table,,"I have the following question: At a squared table I seat $8$ friends of mine. At each side, I place two friends. Always two friends know each other. What's the probability that no friend knows it's sided, neighbor? I tried to solve it by: Calculating all different seating possibilities: $4!$ $\Rightarrow 24$ And writing down all options on paper where no side neighbors know the other side neighbor. For that, I got 9 options. So I guess my final result is  $$\frac{9}{4!}\Rightarrow 0.375$$ $37.5\%$ that none of the side neighbors knows the other side neighbor. Is this correct? How would I calculate the count of all possible seating options where no side neighbor knows each other? Thanks","I have the following question: At a squared table I seat $8$ friends of mine. At each side, I place two friends. Always two friends know each other. What's the probability that no friend knows it's sided, neighbor? I tried to solve it by: Calculating all different seating possibilities: $4!$ $\Rightarrow 24$ And writing down all options on paper where no side neighbors know the other side neighbor. For that, I got 9 options. So I guess my final result is  $$\frac{9}{4!}\Rightarrow 0.375$$ $37.5\%$ that none of the side neighbors knows the other side neighbor. Is this correct? How would I calculate the count of all possible seating options where no side neighbor knows each other? Thanks",,"['probability', 'permutations']"
24,How to compute $\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)}$?,How to compute ?,\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)},"This basic course on probability and statistics is the first course where I feel like a total idiot... especially since I've already forgotten much from the basic courses at discrete mathematics and analysis. Sigh... The task from a former test: A HDD contains $x+y$ infected programs; $x$ are infected by malware $X$ and $y$ are infected by malware $Y$. The user runs random programs. Each time a program infected by malware $X$ is run $m$ uninfected programs become infected by malware $X$; same for malware $Y$. Compute the probability that when the $k$th infected program is run it is infected by malware $X$. Let $X_1=1$  iff the first infected program is infected by $X$, similary let's define $Y_1$, $X_2$, etc. We have: $\operatorname{P}(X_1)=\frac{x}{x+y}$ $\operatorname{P}(Y_1)=\frac{y}{x+y}$ $\operatorname{P}(X_2)=\operatorname{P}(X_1)\operatorname{P}(X_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(X_2|Y_1)=\frac{x}{x+y}\frac{x+m}{x+m+y}+\frac{y}{x+y}\frac{x}{x+y+m}$ $P(Y_2)=\operatorname{P}(X_1)\operatorname{P}(Y_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(Y_2|Y_1)=\frac{x}{x+y}\frac{y}{x+m+y}+\frac{y}{x+y}\frac{y+m}{x+y+m}$ $\operatorname{P}(X_3)=\frac{x}{x+y}\frac{x+m}{x+m+y}\frac{x+2m}{x+2m+y}+\frac{y}{x+y}\frac{x}{x+y+m}\frac{x+m}{x+m+y+m}+\frac{x}{x+y}\frac{y}{x+m+y}\frac{x+m}{x+m+y+m}+\frac{y}{y+m}\frac{y+m}{x+y+m}+\frac{x}{x+y+2m}$ More generally: $\operatorname{P}(X_k)=\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)}$ This is madness for me. That's not even binomial theorem, albeit it's a little bit similar. Am I doing something wrong? Or how am I supposed to get a closed formula from this sum?","This basic course on probability and statistics is the first course where I feel like a total idiot... especially since I've already forgotten much from the basic courses at discrete mathematics and analysis. Sigh... The task from a former test: A HDD contains $x+y$ infected programs; $x$ are infected by malware $X$ and $y$ are infected by malware $Y$. The user runs random programs. Each time a program infected by malware $X$ is run $m$ uninfected programs become infected by malware $X$; same for malware $Y$. Compute the probability that when the $k$th infected program is run it is infected by malware $X$. Let $X_1=1$  iff the first infected program is infected by $X$, similary let's define $Y_1$, $X_2$, etc. We have: $\operatorname{P}(X_1)=\frac{x}{x+y}$ $\operatorname{P}(Y_1)=\frac{y}{x+y}$ $\operatorname{P}(X_2)=\operatorname{P}(X_1)\operatorname{P}(X_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(X_2|Y_1)=\frac{x}{x+y}\frac{x+m}{x+m+y}+\frac{y}{x+y}\frac{x}{x+y+m}$ $P(Y_2)=\operatorname{P}(X_1)\operatorname{P}(Y_2|X_1)+\operatorname{P}(Y_1)\operatorname{P}(Y_2|Y_1)=\frac{x}{x+y}\frac{y}{x+m+y}+\frac{y}{x+y}\frac{y+m}{x+y+m}$ $\operatorname{P}(X_3)=\frac{x}{x+y}\frac{x+m}{x+m+y}\frac{x+2m}{x+2m+y}+\frac{y}{x+y}\frac{x}{x+y+m}\frac{x+m}{x+m+y+m}+\frac{x}{x+y}\frac{y}{x+m+y}\frac{x+m}{x+m+y+m}+\frac{y}{y+m}\frac{y+m}{x+y+m}+\frac{x}{x+y+2m}$ More generally: $\operatorname{P}(X_k)=\sum^k_{i=0}{k\choose i}\frac{\prod_{j=0}^{i-1}(x+jm)\prod_{j=0}^{k-i-1}(y+jm)}{\prod_{j=0}^{k-1}(x+y+jm)}$ This is madness for me. That's not even binomial theorem, albeit it's a little bit similar. Am I doing something wrong? Or how am I supposed to get a closed formula from this sum?",,"['probability', 'discrete-mathematics', 'summation', 'binomial-coefficients']"
25,Bounds for change of variable,Bounds for change of variable,,"U and V are independent and uniformly distributed on (0,1). Find the joint density of $X=\frac{\sqrt{U}}{\sqrt{U}+\sqrt{V}}$ and $Y={\sqrt{U}+\sqrt{V}}$. I've been able to find without a hitch that $f_{XY}(x, y)=4(1-x)xy^3$ for some values of X and Y. My textbook confirmed that this is correct. My problem comes from evaluation the values of X and Y over which the joint pdf takes a positive value. I reasoned that, since $0<u<1$ and $0<v<1$, that $0<x<1$ and $0<y<2$. Integrating the pdf over these values of x and y, however, does not yield one. The textbooks says that the bounds are $0<x<1$ and $0<y<\min\left(\frac{1}{x}, \frac{1}{1-x}\right)$, which indeeds integrate to 1. How were these bounds for y obtained? I have a feeling that the answer might be connected to the fact that $X=\frac{\sqrt{U}}{Y}$, but I am unable to explain why.","U and V are independent and uniformly distributed on (0,1). Find the joint density of $X=\frac{\sqrt{U}}{\sqrt{U}+\sqrt{V}}$ and $Y={\sqrt{U}+\sqrt{V}}$. I've been able to find without a hitch that $f_{XY}(x, y)=4(1-x)xy^3$ for some values of X and Y. My textbook confirmed that this is correct. My problem comes from evaluation the values of X and Y over which the joint pdf takes a positive value. I reasoned that, since $0<u<1$ and $0<v<1$, that $0<x<1$ and $0<y<2$. Integrating the pdf over these values of x and y, however, does not yield one. The textbooks says that the bounds are $0<x<1$ and $0<y<\min\left(\frac{1}{x}, \frac{1}{1-x}\right)$, which indeeds integrate to 1. How were these bounds for y obtained? I have a feeling that the answer might be connected to the fact that $X=\frac{\sqrt{U}}{Y}$, but I am unable to explain why.",,"['probability', 'change-of-variable']"
26,"Sharpness of Cramer-Wold: A pair $(X,Y)$ where a finite set of projections are normal but $(X,Y)$ not joint normal",Sharpness of Cramer-Wold: A pair  where a finite set of projections are normal but  not joint normal,"(X,Y) (X,Y)","In the case of normality, the Cramer-Wold Theorem states: Let $(X,Y)$ be a bivariate random variable.  If for all $a,b \in \mathbb{R}$, $aX + bY$ is Gaussian, then $(X,Y)$ is jointly Gaussian. I'm aware that Cramer-Wold can be improved: you only need to check a countably infinite collection of pairs $(a,b)$.  My question is: Let $(a_i,b_i)_{i = 1}^N$ be a finite collection of pairs of real numbers.  Can you construct a bivariate random variable $(X,Y)$ so that $a_i X + b_i Y$ is Gaussian for each $i$, but $(X,Y)$ is not jointly Gaussian? I'm struggling to come up with a robust way of making counterexamples for this problem; I know the classic example when we want $X$ and $Y$ to be normal but not jointly normal, but am stuck here.  Any help would be appreciated.","In the case of normality, the Cramer-Wold Theorem states: Let $(X,Y)$ be a bivariate random variable.  If for all $a,b \in \mathbb{R}$, $aX + bY$ is Gaussian, then $(X,Y)$ is jointly Gaussian. I'm aware that Cramer-Wold can be improved: you only need to check a countably infinite collection of pairs $(a,b)$.  My question is: Let $(a_i,b_i)_{i = 1}^N$ be a finite collection of pairs of real numbers.  Can you construct a bivariate random variable $(X,Y)$ so that $a_i X + b_i Y$ is Gaussian for each $i$, but $(X,Y)$ is not jointly Gaussian? I'm struggling to come up with a robust way of making counterexamples for this problem; I know the classic example when we want $X$ and $Y$ to be normal but not jointly normal, but am stuck here.  Any help would be appreciated.",,"['probability', 'probability-theory', 'measure-theory']"
27,What is wrong with this attempt at figuring out the probability of drawing a 5-card poker hand with at least one pair?,What is wrong with this attempt at figuring out the probability of drawing a 5-card poker hand with at least one pair?,,"What is the probability that a 5-card poker hand has at least one pair? Note that this is the same as: probability of exactly one pair + probability of exactly two pairs + probability of exactly 3 of a kind + probability of exaclty 4 of a kind. Let us study a reduced example to help us figure out how to tackle the problem. Imagine that there are 3 balls numbered 1 to 3, coloured red, and 3 similarly numbered balls coloured black. What is the probability of picking 3 balls out of the 6 such that at least two of the balls have the same number value. There are ${6 \choose 3} = 20$ ways of picking out 3 balls from 6:     \begin{align*}     &R1, R2, R3 \quad B1, B2, B3 \\     &\color{blue}{R1, R2, B1} \quad \color{blue}{R1, R2, B2} \quad R1, R2, B3 \\     &\color{blue}{R1, R3, B1} \quad R1, R3, B2 \quad \color{blue}{R1, R3, B3} \\     &R2, R3, B1 \quad \color{blue}{R2, R3, B2} \quad \color{blue}{R2, R3, B3} \\       &\color{blue}{R1, B1, B2} \quad R1, B2, B3 \quad \color{blue}{R1, B1, B3} \\     &\color{blue}{R2, B1, B2} \quad \color{blue}{R2, B2, B3} \quad R2, B1, B3 \\     &R3, B1, B2 \quad \color{blue}{R3, B2, B3} \quad \color{blue}{R3, B1, B3} \end{align*} We see through brute force that the probability of picking 3 balls where at least two balls have the same number value is $12/20 = 3/5$. Let us attempt to arrive at this answer computationally. Now, there are 6 ways from which we can make the initial selection of a ball. There is only one choice for the second ball, since it must have the same number value as the first. There are 4 balls left now, and from it we can choose any one, so there are 4 choices. Thus, in total, there $6 \times 1 \times 4$ ways of making this choice, but $6 \times 4 = 24 > 20$, so $24/20 > 1$. One could imagine that what is going wrong is that we are making subtle assumptions regarding the order in which the balls are picked! So, let's say we divided the numerator out by the number of ways in which we can arrange 3 balls, $3! = 6$; then we get $4/20 = 1/5$, which is still not the right answer. Let us attempt to get rid of order entirely when constructing our solution. There are 3 number values from which we can pick a pair of balls. There are now 4 balls left, and out of them we can pick any ball. So, we have $3 \times 4 = 12$, and indeed $12/20 = 3/5$. Let us apply this method to yet another case: get rid of balls $R3$ and $B3$, so our scenario only has 4 balls. What is the probability of picking two balls with the same value? Well, there are 2 number values from which we can pair of balls, but ${4 \choose 2} = 6$ ways we can pick a pair of balls overall, so the probability of picking two balls with the same value should be $2/6 = 1/3$. Let us verify that this is the case using brute force:     \begin{align*}     &R1, R2 \quad B1, B2 \\     &\color{blue}{R1, B1} \quad R1, B2 \\     &R2, B1 \quad \color{blue}{R2, B2} \end{align*} Okay, so it works here too. Another simple case is 3 colours (say, red, black and white), but only 2 values---what is the probability of picking out a pair of balls with the same number value? In this case, there are 2 choices for the value from which we can pick a pair of balls, but ${6 \choose 2}$ ways of picking out a pair of balls in general. So the answer should be $2/15$? Let us verify using brute force:     \begin{align*}     &R1, R2 \quad B1, B2 \quad W1, W2 \\     &\color{blue}{R1, B1} \quad R1, B2 \\     &\color{blue}{R1, W1} \quad R1, W2 \\     &R2, B1 \quad \color{blue}{R2, B2} \\     &R2, W1 \quad \color{blue}{R2, W2} \\     &\color{blue}{B1, W1} \quad B2, W1 \\     &B2, W1 \quad \color{blue}{B2, W2} \end{align*} The correct answer is actually $6/15 = 2/5$, and it seems the counting method we developed so far does not account for more than 2 colours. There are 3 ways we can pick 2 colours out of 3. There are the 2 number values. So, there are $3 \times 2 = 6$ ways of picking out a pair of balls with the same number value. Hence, the probability of picking out 2 balls with the same number value is $6/15$. Let us apply this method then to the original problem: there are 4 suites from which we can pick 2 out of. There are 13 number values from which we can pick a pair of cards. There are now 50 cards left, so there are ${50 \choose 3}$ ways of picking out out the remaining cards. So the probability should be:     $$\frac{{50 \choose 3} \times {13 \choose 1} \times {4 \choose 2}}{{52 \choose 5}} = 0.59$$ We can no longer brute force to verify, but we can use the work of others to help us. It has been determined that the probability of drawing a hand with no interesting characteristics (i.e. only a high card) is approximately $0.5$. Since probabilities must add up to $1$, there is no way that the probability of drawing at least one pair is greater than approximately $0.5$, so our above answer is incorrect. What is the method developed to help count in such problems missing?","What is the probability that a 5-card poker hand has at least one pair? Note that this is the same as: probability of exactly one pair + probability of exactly two pairs + probability of exactly 3 of a kind + probability of exaclty 4 of a kind. Let us study a reduced example to help us figure out how to tackle the problem. Imagine that there are 3 balls numbered 1 to 3, coloured red, and 3 similarly numbered balls coloured black. What is the probability of picking 3 balls out of the 6 such that at least two of the balls have the same number value. There are ${6 \choose 3} = 20$ ways of picking out 3 balls from 6:     \begin{align*}     &R1, R2, R3 \quad B1, B2, B3 \\     &\color{blue}{R1, R2, B1} \quad \color{blue}{R1, R2, B2} \quad R1, R2, B3 \\     &\color{blue}{R1, R3, B1} \quad R1, R3, B2 \quad \color{blue}{R1, R3, B3} \\     &R2, R3, B1 \quad \color{blue}{R2, R3, B2} \quad \color{blue}{R2, R3, B3} \\       &\color{blue}{R1, B1, B2} \quad R1, B2, B3 \quad \color{blue}{R1, B1, B3} \\     &\color{blue}{R2, B1, B2} \quad \color{blue}{R2, B2, B3} \quad R2, B1, B3 \\     &R3, B1, B2 \quad \color{blue}{R3, B2, B3} \quad \color{blue}{R3, B1, B3} \end{align*} We see through brute force that the probability of picking 3 balls where at least two balls have the same number value is $12/20 = 3/5$. Let us attempt to arrive at this answer computationally. Now, there are 6 ways from which we can make the initial selection of a ball. There is only one choice for the second ball, since it must have the same number value as the first. There are 4 balls left now, and from it we can choose any one, so there are 4 choices. Thus, in total, there $6 \times 1 \times 4$ ways of making this choice, but $6 \times 4 = 24 > 20$, so $24/20 > 1$. One could imagine that what is going wrong is that we are making subtle assumptions regarding the order in which the balls are picked! So, let's say we divided the numerator out by the number of ways in which we can arrange 3 balls, $3! = 6$; then we get $4/20 = 1/5$, which is still not the right answer. Let us attempt to get rid of order entirely when constructing our solution. There are 3 number values from which we can pick a pair of balls. There are now 4 balls left, and out of them we can pick any ball. So, we have $3 \times 4 = 12$, and indeed $12/20 = 3/5$. Let us apply this method to yet another case: get rid of balls $R3$ and $B3$, so our scenario only has 4 balls. What is the probability of picking two balls with the same value? Well, there are 2 number values from which we can pair of balls, but ${4 \choose 2} = 6$ ways we can pick a pair of balls overall, so the probability of picking two balls with the same value should be $2/6 = 1/3$. Let us verify that this is the case using brute force:     \begin{align*}     &R1, R2 \quad B1, B2 \\     &\color{blue}{R1, B1} \quad R1, B2 \\     &R2, B1 \quad \color{blue}{R2, B2} \end{align*} Okay, so it works here too. Another simple case is 3 colours (say, red, black and white), but only 2 values---what is the probability of picking out a pair of balls with the same number value? In this case, there are 2 choices for the value from which we can pick a pair of balls, but ${6 \choose 2}$ ways of picking out a pair of balls in general. So the answer should be $2/15$? Let us verify using brute force:     \begin{align*}     &R1, R2 \quad B1, B2 \quad W1, W2 \\     &\color{blue}{R1, B1} \quad R1, B2 \\     &\color{blue}{R1, W1} \quad R1, W2 \\     &R2, B1 \quad \color{blue}{R2, B2} \\     &R2, W1 \quad \color{blue}{R2, W2} \\     &\color{blue}{B1, W1} \quad B2, W1 \\     &B2, W1 \quad \color{blue}{B2, W2} \end{align*} The correct answer is actually $6/15 = 2/5$, and it seems the counting method we developed so far does not account for more than 2 colours. There are 3 ways we can pick 2 colours out of 3. There are the 2 number values. So, there are $3 \times 2 = 6$ ways of picking out a pair of balls with the same number value. Hence, the probability of picking out 2 balls with the same number value is $6/15$. Let us apply this method then to the original problem: there are 4 suites from which we can pick 2 out of. There are 13 number values from which we can pick a pair of cards. There are now 50 cards left, so there are ${50 \choose 3}$ ways of picking out out the remaining cards. So the probability should be:     $$\frac{{50 \choose 3} \times {13 \choose 1} \times {4 \choose 2}}{{52 \choose 5}} = 0.59$$ We can no longer brute force to verify, but we can use the work of others to help us. It has been determined that the probability of drawing a hand with no interesting characteristics (i.e. only a high card) is approximately $0.5$. Since probabilities must add up to $1$, there is no way that the probability of drawing at least one pair is greater than approximately $0.5$, so our above answer is incorrect. What is the method developed to help count in such problems missing?",,"['probability', 'combinatorics']"
28,Is rolling 10% 10 times just as good as 1 100% or better?,Is rolling 10% 10 times just as good as 1 100% or better?,,"How do you determine the value of a percentage based on how many rolls? For example would 6 rolls at 10% be better than 1 roll at 60%? 1 roll at 100% to 100 rolls at 1%? I know each probability is independent of each other, but I don't know how to determine which is a better odd.","How do you determine the value of a percentage based on how many rolls? For example would 6 rolls at 10% be better than 1 roll at 60%? 1 roll at 100% to 100 rolls at 1%? I know each probability is independent of each other, but I don't know how to determine which is a better odd.",,"['probability', 'probability-theory']"
29,Expected Payoff for Dice Game Where Six = No Payoff,Expected Payoff for Dice Game Where Six = No Payoff,,"In a dice game where a player's payoff is whatever the die is rolled, each player can roll how many times they want. Each payoff gets added cumulatively (e.g. roll 5 and 5, then payoff = 10). The catch is that if the user rolled a six at any point in time, they get 0 (e.g. rolled 5 and 6, then payoff = 0). Intuitively, raising $n$ (the number of die rolls) could raise the expected payoff but also increases the probability of at least rolling one six and getting 0 as a payoff. For example if you picked a really large $n$, the likelihood of rolling at least 1 six and getting 0 payout is extremely likely. But going from $n=1$ to $n=2$ you get a little higher expected payoff (checked the math manually in lieu of a general formula). Going with the weighted average approach to come up with a formula for expected value, one part of the formula must be the weighted average of getting a 0 payout, i.e. the probability of rolling at least one six out of $n$ dice rolls equals $1-\left(\frac{5}{6}\right)^n$. As such, we get: $E(X) =$ weighted average for each payoff $E(X) =$ weighted average of getting 0 + the rest of the weighted average of payoffs $E(X) = \left(1-\left(\frac{5}{6}\right)^n\right) 0$ + the rest of the weighted average of payoffs However, I'm having trouble wrapping my head around coming up with the rest of the equation. How can I solve this problem?","In a dice game where a player's payoff is whatever the die is rolled, each player can roll how many times they want. Each payoff gets added cumulatively (e.g. roll 5 and 5, then payoff = 10). The catch is that if the user rolled a six at any point in time, they get 0 (e.g. rolled 5 and 6, then payoff = 0). Intuitively, raising $n$ (the number of die rolls) could raise the expected payoff but also increases the probability of at least rolling one six and getting 0 as a payoff. For example if you picked a really large $n$, the likelihood of rolling at least 1 six and getting 0 payout is extremely likely. But going from $n=1$ to $n=2$ you get a little higher expected payoff (checked the math manually in lieu of a general formula). Going with the weighted average approach to come up with a formula for expected value, one part of the formula must be the weighted average of getting a 0 payout, i.e. the probability of rolling at least one six out of $n$ dice rolls equals $1-\left(\frac{5}{6}\right)^n$. As such, we get: $E(X) =$ weighted average for each payoff $E(X) =$ weighted average of getting 0 + the rest of the weighted average of payoffs $E(X) = \left(1-\left(\frac{5}{6}\right)^n\right) 0$ + the rest of the weighted average of payoffs However, I'm having trouble wrapping my head around coming up with the rest of the equation. How can I solve this problem?",,['probability']
30,Proving that the Bayes optimal predictor is in fact optimal,Proving that the Bayes optimal predictor is in fact optimal,,"This is exercise 3.8(a) from Understanding Machine Learning: from Theory to Algorithms by Shalev-Shwartz and Ben-David. I am trying to figure the exercise out for a course, but it is not homework. (I have a decent mathematical background but less so in stochastics.) Consider a set $\mathcal X$, and joint random variables $(x,y)$ distributed according to probability distribution $\mathcal D$ on $\mathcal X \times \{0,1\}$. We think informally of the probability of $(x, 0)$ as that of finding an $x$ without a certain property, and of the probability of $(x, 1)$ as that of finding an $x$ with a certain property. Note that I'm not requiring that the distribution is discrete. We define $f: \mathcal X \to \{0,1\}$, the ""Bayes optimal predictor"", by  $$ f(x) = 0 \iff \mathbb P(y = 0\mid x ) \geq \frac12. $$ (I'm not 100% sure this definition even makes sense in the general case. Do we need a well-behaved distribution to speak about $\mathbb P(y = 0 \mid x)$?) Now I want to prove that $f$ is ""the best at predicting whether or not an $x$ has the property"". That is, for any $h : \mathcal X \to [0, 1]$ (note the codomain) we have that $$ \mathbb P(f(x) \neq y) \leq \mathbb E[|h(x) - y|]. $$ Intuitively, this is totally obvious to me: $f$ always makes the ""best guess"", so even in the optimal scenario, $h$ should incur more loss. I've even proved it in the case that $h$ maps into $\{0,1\}$. However, I can't seem to be able to get the general case. I've tried to condition and split the expectation and probability into the right cases, but I keep getting stuck, partially on notation and partially on insight. I hope someone can point me in the right direction.","This is exercise 3.8(a) from Understanding Machine Learning: from Theory to Algorithms by Shalev-Shwartz and Ben-David. I am trying to figure the exercise out for a course, but it is not homework. (I have a decent mathematical background but less so in stochastics.) Consider a set $\mathcal X$, and joint random variables $(x,y)$ distributed according to probability distribution $\mathcal D$ on $\mathcal X \times \{0,1\}$. We think informally of the probability of $(x, 0)$ as that of finding an $x$ without a certain property, and of the probability of $(x, 1)$ as that of finding an $x$ with a certain property. Note that I'm not requiring that the distribution is discrete. We define $f: \mathcal X \to \{0,1\}$, the ""Bayes optimal predictor"", by  $$ f(x) = 0 \iff \mathbb P(y = 0\mid x ) \geq \frac12. $$ (I'm not 100% sure this definition even makes sense in the general case. Do we need a well-behaved distribution to speak about $\mathbb P(y = 0 \mid x)$?) Now I want to prove that $f$ is ""the best at predicting whether or not an $x$ has the property"". That is, for any $h : \mathcal X \to [0, 1]$ (note the codomain) we have that $$ \mathbb P(f(x) \neq y) \leq \mathbb E[|h(x) - y|]. $$ Intuitively, this is totally obvious to me: $f$ always makes the ""best guess"", so even in the optimal scenario, $h$ should incur more loss. I've even proved it in the case that $h$ maps into $\{0,1\}$. However, I can't seem to be able to get the general case. I've tried to condition and split the expectation and probability into the right cases, but I keep getting stuck, partially on notation and partially on insight. I hope someone can point me in the right direction.",,"['probability', 'random-variables', 'conditional-expectation']"
31,intuition behind discrepancy in expected number of coin tosses,intuition behind discrepancy in expected number of coin tosses,,"Yesterday evening I read a very interesting article on prime numbers which contained the following paragraph on coin tosses: If Alice tosses a coin until she sees a head followed by a tail, and   Bob tosses a coin until he sees two heads in a row, then on average,   Alice will require four tosses while Bob will require six tosses (try   this at home!), even though head-tail and head-head have an equal   chance of appearing after two coin tosses. I immediately tried to look into this question. Using the formula provided by Andr茅 Nicolas , it can be shown that the expected number of coin tosses we need before getting $n$ consecutive heads is given by: \begin{equation} e_n = \sum_{k=1}^n\frac{1}{2^k}(e_n+k)+\frac{n}{2^n} \end{equation} For $n=2$, the expected number of heads($e_2$) is 6. Analytically, this makes sense once you become familiar with the above equation. Now, what I find interesting is that, as mentioned in the article, the probability of obtaining two heads is the same as the probability of obtaining a head followed by a tail: \begin{equation} P(HH)=P(HT)=\frac{1}{4} \end{equation} However, the expected number of coin tosses required to get the $HT$ pattern is 4 not 6. I still find this quite counter-intuitive. In fact,  I ran a simulation using the following python code: import numpy as np  head_tail = np.zeros(10000) two_heads = np.zeros(10000)  for i in range(10000):     z = np.random.randint(2, size=100)      for j in range(100):         if z[j] == 1 and z[j+1] == 0:              head_tail[i] = j+2              break      for j in range(100):          if z[j] == 1 and z[j+1] == 1:              two_heads[i] = j+2              break And I noticed that their distributions behaved very differently: It's by no means intuitive to me that the behaviour of these two distributions should be very different and yet they are remarkably different. Is there an intuitive reason why this must be the case?","Yesterday evening I read a very interesting article on prime numbers which contained the following paragraph on coin tosses: If Alice tosses a coin until she sees a head followed by a tail, and   Bob tosses a coin until he sees two heads in a row, then on average,   Alice will require four tosses while Bob will require six tosses (try   this at home!), even though head-tail and head-head have an equal   chance of appearing after two coin tosses. I immediately tried to look into this question. Using the formula provided by Andr茅 Nicolas , it can be shown that the expected number of coin tosses we need before getting $n$ consecutive heads is given by: \begin{equation} e_n = \sum_{k=1}^n\frac{1}{2^k}(e_n+k)+\frac{n}{2^n} \end{equation} For $n=2$, the expected number of heads($e_2$) is 6. Analytically, this makes sense once you become familiar with the above equation. Now, what I find interesting is that, as mentioned in the article, the probability of obtaining two heads is the same as the probability of obtaining a head followed by a tail: \begin{equation} P(HH)=P(HT)=\frac{1}{4} \end{equation} However, the expected number of coin tosses required to get the $HT$ pattern is 4 not 6. I still find this quite counter-intuitive. In fact,  I ran a simulation using the following python code: import numpy as np  head_tail = np.zeros(10000) two_heads = np.zeros(10000)  for i in range(10000):     z = np.random.randint(2, size=100)      for j in range(100):         if z[j] == 1 and z[j+1] == 0:              head_tail[i] = j+2              break      for j in range(100):          if z[j] == 1 and z[j+1] == 1:              two_heads[i] = j+2              break And I noticed that their distributions behaved very differently: It's by no means intuitive to me that the behaviour of these two distributions should be very different and yet they are remarkably different. Is there an intuitive reason why this must be the case?",,['real-analysis']
32,Does convergence in every $L^p$ imply convergence in $L^\infty$?,Does convergence in every  imply convergence in ?,L^p L^\infty,"Consider a sequence of functions $(f_n)$ on a probability space $(X,\mu)$ such that (i) $f_n \to f$ in $L^p$ for every $p\in [1,\infty)$ (ii) $f,f_1,f_2,\cdots\in L^\infty$ and they are uniformly bounded in $L^\infty$ by $\Lambda\in [0,\infty)$, i.e.  $$\|f\|_\infty,\|f_1\|_\infty,\|f_2\|_\infty,\cdots\le \Lambda.$$ Does then $f_n\to f$ in $L^\infty$?","Consider a sequence of functions $(f_n)$ on a probability space $(X,\mu)$ such that (i) $f_n \to f$ in $L^p$ for every $p\in [1,\infty)$ (ii) $f,f_1,f_2,\cdots\in L^\infty$ and they are uniformly bounded in $L^\infty$ by $\Lambda\in [0,\infty)$, i.e.  $$\|f\|_\infty,\|f_1\|_\infty,\|f_2\|_\infty,\cdots\le \Lambda.$$ Does then $f_n\to f$ in $L^\infty$?",,"['real-analysis', 'probability', 'lp-spaces']"
33,Expected number of draws without replacement before drawing the same card twice in a row,Expected number of draws without replacement before drawing the same card twice in a row,,"I recently came across the question of how many cards we expect to draw before drawing an ace. I found a nice solution with indicator variables, and it seemed natural to me to ask this question next: Suppose we draw cards one by one from a standard deck without replacement. How many cards do we expect to draw before our first consecutive pair, e.g. two 7s in a row? My previous method doesn't work for this question, and I haven't found it tackled anywhere. What are some different proofs for it? I would like as many as possible! Is the problem even tractable, or is the solution always going to be ugly (and if so, why?). And, if anyone is able to answer a slightly broader question: where might I find a resource that has many of these kinds of problems (with dice, cards and such) and maybe general methods for solving them? I am awful at probabilities but would like to get really good at this. This is my first post, so please let me know if I have made any fatal errors (I have really tried not to!) I apologise if anything is too general, and I'm really grateful for any comments :)","I recently came across the question of how many cards we expect to draw before drawing an ace. I found a nice solution with indicator variables, and it seemed natural to me to ask this question next: Suppose we draw cards one by one from a standard deck without replacement. How many cards do we expect to draw before our first consecutive pair, e.g. two 7s in a row? My previous method doesn't work for this question, and I haven't found it tackled anywhere. What are some different proofs for it? I would like as many as possible! Is the problem even tractable, or is the solution always going to be ugly (and if so, why?). And, if anyone is able to answer a slightly broader question: where might I find a resource that has many of these kinds of problems (with dice, cards and such) and maybe general methods for solving them? I am awful at probabilities but would like to get really good at this. This is my first post, so please let me know if I have made any fatal errors (I have really tried not to!) I apologise if anything is too general, and I'm really grateful for any comments :)",,"['probability', 'combinatorics']"
34,"Let $X,Y$ be independent r.v. on a probability space, then are the conditional expectations $E[X|\Sigma]$ and $E[Y|\Sigma]$ independent?","Let  be independent r.v. on a probability space, then are the conditional expectations  and  independent?","X,Y E[X|\Sigma] E[Y|\Sigma]","Fix a probability space $(\Omega,\mathcal{F},\mathbb{P})$, and let $X,Y:\Omega\to\mathbb{R}$ be $L^1$ random variables. Let $\Sigma\subset\mathcal{F}$ be a sub-$\sigma$-algebra. Is it true that $$\mathbb{E}[XY|\Sigma]=\mathbb{E}[X|\Sigma]\,\mathbb{E}[Y|\Sigma]?$$ I'm totally unable to prove this, and I'm beginning to suspect that it's false.","Fix a probability space $(\Omega,\mathcal{F},\mathbb{P})$, and let $X,Y:\Omega\to\mathbb{R}$ be $L^1$ random variables. Let $\Sigma\subset\mathcal{F}$ be a sub-$\sigma$-algebra. Is it true that $$\mathbb{E}[XY|\Sigma]=\mathbb{E}[X|\Sigma]\,\mathbb{E}[Y|\Sigma]?$$ I'm totally unable to prove this, and I'm beginning to suspect that it's false.",,"['probability', 'probability-theory', 'conditional-expectation']"
35,monty hall multiple prizes,monty hall multiple prizes,,"Really surprised I didn't see this already posted on math SE...(as far as I can tell) I understand the N door, k-revealed Monty Hall extension: Monty hall problem extended. The solution is to switch if: (n-1)/n * (1 / (n-k-1)) > 1/n which occurs for any k>=1 Now let's extend this to say: of the N doors, P are prize doors and N-P are non-prize doors. Monty Hall reveals k doors. In this case, k will have to be small enough that Monty Hall can reveal a nonprize door, meaning you cannot have k so big that Monty is forced to revealed one or more of the prize doors. So as usual,  he can only reveal nonprize doors. And also does not open your original door. In this case, what is the inequality determing if you should switch or not, similarly to the inequality in the accepted answer given above for the Ndoor, k revealed version? This time it will also be a function of P the #prize doors.","Really surprised I didn't see this already posted on math SE...(as far as I can tell) I understand the N door, k-revealed Monty Hall extension: Monty hall problem extended. The solution is to switch if: (n-1)/n * (1 / (n-k-1)) > 1/n which occurs for any k>=1 Now let's extend this to say: of the N doors, P are prize doors and N-P are non-prize doors. Monty Hall reveals k doors. In this case, k will have to be small enough that Monty Hall can reveal a nonprize door, meaning you cannot have k so big that Monty is forced to revealed one or more of the prize doors. So as usual,  he can only reveal nonprize doors. And also does not open your original door. In this case, what is the inequality determing if you should switch or not, similarly to the inequality in the accepted answer given above for the Ndoor, k revealed version? This time it will also be a function of P the #prize doors.",,"['probability', 'monty-hall']"
36,Finite sample bounds for generalized coupon collector problem,Finite sample bounds for generalized coupon collector problem,,"Consider the usual generalization of the coupon collector problem where $m$ copies of each coupon need to be collected. Let $T_m$ be the first time m copies of each coupon are collected. Donald J. Newman and Lawrence Shepp showed that  $$\mathbf{E} (T_{m})=n\log n+(m-1)n\log \log n+O(n),\ {\text{as}}\ n\to \infty $$ Then Erds and R茅nyi showed that: $$\displaystyle \operatorname {P} {\bigl (}T_{m}<n\log n+(m-1)n\log \log n+cn{\bigr )}\to e^{-e^{-c}/(m-1)!},\ \ {\text{as}}\ n\to \infty .$$ But these statement are asymptotic. Are there known finite sample (upper and lower) bounds on $T_m$ ?","Consider the usual generalization of the coupon collector problem where $m$ copies of each coupon need to be collected. Let $T_m$ be the first time m copies of each coupon are collected. Donald J. Newman and Lawrence Shepp showed that  $$\mathbf{E} (T_{m})=n\log n+(m-1)n\log \log n+O(n),\ {\text{as}}\ n\to \infty $$ Then Erds and R茅nyi showed that: $$\displaystyle \operatorname {P} {\bigl (}T_{m}<n\log n+(m-1)n\log \log n+cn{\bigr )}\to e^{-e^{-c}/(m-1)!},\ \ {\text{as}}\ n\to \infty .$$ But these statement are asymptotic. Are there known finite sample (upper and lower) bounds on $T_m$ ?",,"['probability', 'coupon-collector', 'upper-lower-bounds', 'concentration-of-measure']"
37,A probability problem involving two decks of cards,A probability problem involving two decks of cards,,Here is the problem: Player A chooses 5 cards from a deck of cards. Player B also chooses 5 cards from ANOTHER deck of cards. Player B wins if his cards match at least 3 cards of player A. What is the probability that the number of cards of player B matches that of player A's is A) 0 B) 1 C) 2 D) 3 E) 4 F) 5 I attempted to solve A) this way: ((52C5)* (47C5)) / ((52C5)*(52C5)) = 0.5902 but I'm not sure if this is right. Thanks in advance for any help!,Here is the problem: Player A chooses 5 cards from a deck of cards. Player B also chooses 5 cards from ANOTHER deck of cards. Player B wins if his cards match at least 3 cards of player A. What is the probability that the number of cards of player B matches that of player A's is A) 0 B) 1 C) 2 D) 3 E) 4 F) 5 I attempted to solve A) this way: ((52C5)* (47C5)) / ((52C5)*(52C5)) = 0.5902 but I'm not sure if this is right. Thanks in advance for any help!,,['probability']
38,"Why is it more likely to be in the lead 0 or 40 times, rather than 20 in this example?","Why is it more likely to be in the lead 0 or 40 times, rather than 20 in this example?",,"( https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf ) (this question comes from here page 6) Peter and Paul play a game called heads or tails. In this game, a fair coin is tossed a sequence of timeswe choose 40. Each time a head comes up Peter wins 1 penny from Paul, and each time a tail comes up Peter loses 1 penny to Paul. We adopt the convention that, when Peters winnings are 0, he is in the lead if he was ahead at the previous toss and not if he was behind at the previous toss. With this convention, Peter is in the lead 34 times in our example. Again, our intuition might suggest that the most likely number of times to be in the lead is 1/2 of 40, or 20, and the least likely numbers are the extreme cases of 40 or 0. Our intuition about Peters final winnings was quite correct, but our intuition about the number of times Peter was in the lead was completely wrong. The simulation suggests that the least likely number of times in the lead is 20 and the most likely is 0 or 40. This is indeed correct, and the explanation for it is suggested by playing the game of heads or tails with a large number of tosses and looking at a graph of Peters winnings. In Figure 1.4 we show the results of a simulation of the game, for 1000 tosses and in Figure 1.5 for 10,000 tosses"" figure1.4","( https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf ) (this question comes from here page 6) Peter and Paul play a game called heads or tails. In this game, a fair coin is tossed a sequence of timeswe choose 40. Each time a head comes up Peter wins 1 penny from Paul, and each time a tail comes up Peter loses 1 penny to Paul. We adopt the convention that, when Peters winnings are 0, he is in the lead if he was ahead at the previous toss and not if he was behind at the previous toss. With this convention, Peter is in the lead 34 times in our example. Again, our intuition might suggest that the most likely number of times to be in the lead is 1/2 of 40, or 20, and the least likely numbers are the extreme cases of 40 or 0. Our intuition about Peters final winnings was quite correct, but our intuition about the number of times Peter was in the lead was completely wrong. The simulation suggests that the least likely number of times in the lead is 20 and the most likely is 0 or 40. This is indeed correct, and the explanation for it is suggested by playing the game of heads or tails with a large number of tosses and looking at a graph of Peters winnings. In Figure 1.4 we show the results of a simulation of the game, for 1000 tosses and in Figure 1.5 for 10,000 tosses"" figure1.4",,"['probability', 'statistics', 'education']"
39,Probability-Expectation of the maximum of 5 dice,Probability-Expectation of the maximum of 5 dice,,"I am trying to understand part c of the following question (taken from Pitman's Probability, Chapter 3.1). I understand the solution using the difference of the max probabilities but I don't get the symmetry argument. More specifically where is the 7 coming from? Intuitively it makes sense that the minimum and maximum would be symmetrical but I just can't figure out why it's this particular relation. A hint would be much appreciated.","I am trying to understand part c of the following question (taken from Pitman's Probability, Chapter 3.1). I understand the solution using the difference of the max probabilities but I don't get the symmetry argument. More specifically where is the 7 coming from? Intuitively it makes sense that the minimum and maximum would be symmetrical but I just can't figure out why it's this particular relation. A hint would be much appreciated.",,"['probability', 'expectation']"
40,What happens if I throw an Oblate Die?,What happens if I throw an Oblate Die?,,"Suppose I have a die in the shape of a rectangular prism that has eight edges with a length of $1$ unit and four edges with a length of $2$ units so that there are two faces that are $1$ by $1$ and four faces that are $2$ by $1$. If I place this die on the table at a random angle, what is the probability that it will land on a small face? This problem is very hard to start on; however, I have managed to formulate a strategy. I am going to need to use integrals to calculate this probability. I can represent all possible landing angles of the die by letting $\theta$ represent its ""horizontal tilt"" and $\phi$ represent its ""vertical tilt"": I'm going to need to integrate over $\phi$ and $\theta$ to account for all possible angles for each from $0$ to $2\pi$, but I have no idea how to find a ""probability function"" that will do this. My best guess was to find a function $f(\theta,\phi)$ whose output is a value from $0$ to $1$ that expresses the probability of the event $S$ that the die lands on a small face given the vertical angle $\phi$, which is written $P(S|\phi)$. Of course, $P(S|\phi)$ will be in terms of $\theta$ and $\phi$. Then my answer will be $$P(S)=\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}P(S|\phi)d\theta d\phi$$ But I have no idea how to find $P(S|\phi)$. This problem is a little bit of physics and a little bit of probability, and I'm not quite sure how to go at it from here. Any help or hints are appreciated!","Suppose I have a die in the shape of a rectangular prism that has eight edges with a length of $1$ unit and four edges with a length of $2$ units so that there are two faces that are $1$ by $1$ and four faces that are $2$ by $1$. If I place this die on the table at a random angle, what is the probability that it will land on a small face? This problem is very hard to start on; however, I have managed to formulate a strategy. I am going to need to use integrals to calculate this probability. I can represent all possible landing angles of the die by letting $\theta$ represent its ""horizontal tilt"" and $\phi$ represent its ""vertical tilt"": I'm going to need to integrate over $\phi$ and $\theta$ to account for all possible angles for each from $0$ to $2\pi$, but I have no idea how to find a ""probability function"" that will do this. My best guess was to find a function $f(\theta,\phi)$ whose output is a value from $0$ to $1$ that expresses the probability of the event $S$ that the die lands on a small face given the vertical angle $\phi$, which is written $P(S|\phi)$. Of course, $P(S|\phi)$ will be in terms of $\theta$ and $\phi$. Then my answer will be $$P(S)=\frac{1}{4\pi^2}\int_0^{2\pi}\int_0^{2\pi}P(S|\phi)d\theta d\phi$$ But I have no idea how to find $P(S|\phi)$. This problem is a little bit of physics and a little bit of probability, and I'm not quite sure how to go at it from here. Any help or hints are appreciated!",,"['probability', 'physics', 'dice']"
41,A function whose first and second derivatives have zero expectation but not the third derivative,A function whose first and second derivatives have zero expectation but not the third derivative,,"I came across this interesting problem through a friend and I am trying to find a solution to it. Let $X \sim \mathcal{N}(0,1)$ be the standard Gaussian random variable. The goal is to find a function (can assume all nice properties ) $f: \mathbb{R} \to \mathbb{R}$ such that  $$ E[f'(X)]=0, E[f''(X)]=0, \text{ and } E[f'''(X)] \neq 0 . $$ Does there exist any such $f$ satisfying the above properties? It's not sure if it's true or not. I feel this problem is closely related to Hermite polynomials but not sure. Can anyone suggest a method to solve this? The approach that I am currently using is through Taylor expansion. Suppose $f(x)=\sum_{i} \alpha_i x^i$. Then the above conditions imply that $$ \sum_k \alpha_{2k+1} \cdot (2k+1)!!=0, \\ \sum_k \alpha_{2k+2} \cdot (2k+2) \cdot (2k+1)!!=0,\\ \sum_k \alpha_{2k+3} \cdot (2k+3) \cdot (2k+2) \cdot (2k+1)!! \neq 0, $$ where $!!$ is the double factorial. This doesn't shed much light on how the coeffecients $\alpha_i$ should be.","I came across this interesting problem through a friend and I am trying to find a solution to it. Let $X \sim \mathcal{N}(0,1)$ be the standard Gaussian random variable. The goal is to find a function (can assume all nice properties ) $f: \mathbb{R} \to \mathbb{R}$ such that  $$ E[f'(X)]=0, E[f''(X)]=0, \text{ and } E[f'''(X)] \neq 0 . $$ Does there exist any such $f$ satisfying the above properties? It's not sure if it's true or not. I feel this problem is closely related to Hermite polynomials but not sure. Can anyone suggest a method to solve this? The approach that I am currently using is through Taylor expansion. Suppose $f(x)=\sum_{i} \alpha_i x^i$. Then the above conditions imply that $$ \sum_k \alpha_{2k+1} \cdot (2k+1)!!=0, \\ \sum_k \alpha_{2k+2} \cdot (2k+2) \cdot (2k+1)!!=0,\\ \sum_k \alpha_{2k+3} \cdot (2k+3) \cdot (2k+2) \cdot (2k+1)!! \neq 0, $$ where $!!$ is the double factorial. This doesn't shed much light on how the coeffecients $\alpha_i$ should be.",,"['probability', 'probability-theory', 'functions', 'random-variables']"
42,Finding MLE for $P[X > x]$ with $x>0$ given for exponential distribution?,Finding MLE for  with  given for exponential distribution?,P[X > x] x>0,"Problem: Let $X_1, \ldots, X_n$ be a random sample from an exponential distribution with parameter $\theta$. Find a MLE for $P[X > x]$ with $x>0$ given (fixed). My attempt: I first calculated: $$ P[X > x] = \int_x^{\infty} \theta e^{-\theta y} dy = e^{-\theta x}. $$ I also know that $$ L(\theta; \vec{x}) = \prod_{i=1}^n f(x_i; \theta) = \theta^n \exp \left(- \theta \sum_{i=1}^n x_i \right).$$ Then $$ \ln L(\theta; \vec{x}) = n \ln(\theta) - \theta \sum_{i=1}^n x_i. $$ How to proceed now? Normally when we are looking for the MLE for a parameter $\theta$, we differentiate with respect to that parameter. But now I'm asked to find the MLE for a probability. Do I just differentiate with respect to $e^{-\theta x}$? Help is appreciated.","Problem: Let $X_1, \ldots, X_n$ be a random sample from an exponential distribution with parameter $\theta$. Find a MLE for $P[X > x]$ with $x>0$ given (fixed). My attempt: I first calculated: $$ P[X > x] = \int_x^{\infty} \theta e^{-\theta y} dy = e^{-\theta x}. $$ I also know that $$ L(\theta; \vec{x}) = \prod_{i=1}^n f(x_i; \theta) = \theta^n \exp \left(- \theta \sum_{i=1}^n x_i \right).$$ Then $$ \ln L(\theta; \vec{x}) = n \ln(\theta) - \theta \sum_{i=1}^n x_i. $$ How to proceed now? Normally when we are looking for the MLE for a parameter $\theta$, we differentiate with respect to that parameter. But now I'm asked to find the MLE for a probability. Do I just differentiate with respect to $e^{-\theta x}$? Help is appreciated.",,"['probability', 'statistics', 'statistical-inference']"
43,Conditional Probability with dice,Conditional Probability with dice,,"Two dice are rolled and their number is added. If the sum equal 7, the game is lost. If the sum equal 9, the game is won. If the sum is any other number, the dice are rolled again, until the sum equals 7 or 9. What is the probability that someone wins 1 out of 5 games? My approach: Sum of 9: $2 \times (6,3) + 2 \times (5,4) \implies p(9) = \frac{1}{9}$ Sum of 7: $2 \times (6,1) + 2 \times (5,2) + 2 \times (4,3) \implies p(7) = \frac{1}{6}$ $$P(x=1) = {5 \choose 1} \bigg(\frac{1}{9}\bigg)^1 \bigg(\frac{1}{6}\bigg)^{4} = 4.29 \times 10^{-4}$$","Two dice are rolled and their number is added. If the sum equal 7, the game is lost. If the sum equal 9, the game is won. If the sum is any other number, the dice are rolled again, until the sum equals 7 or 9. What is the probability that someone wins 1 out of 5 games? My approach: Sum of 9: $2 \times (6,3) + 2 \times (5,4) \implies p(9) = \frac{1}{9}$ Sum of 7: $2 \times (6,1) + 2 \times (5,2) + 2 \times (4,3) \implies p(7) = \frac{1}{6}$ $$P(x=1) = {5 \choose 1} \bigg(\frac{1}{9}\bigg)^1 \bigg(\frac{1}{6}\bigg)^{4} = 4.29 \times 10^{-4}$$",,"['probability', 'dice']"
44,"After the Cylon Holocaust, how many people would know each other?","After the Cylon Holocaust, how many people would know each other?",,"If you are not familiar, in the show Battlestar Galactica (2003) the Twelve Colonies (planets) are wiped out by the Cylons which they created. Only about 50,000 humans survive in the form of a rag-tag fleet of civilian ships and one Battlestar (give or take). There are at least a few instances in the show of people who were not on the same ship who are re-united or at least know each other from before the attack. Considering that -99.9998% of the human population was wiped out I feel like this would be unlikely, but I want to know exactly how unlikely. Of course, I am not concerned with celebrities and notorious individuals who were ""known"" by a large number of people. I am interested in personal connections. Obviously the definition of ""knowing"" someone is fuzzy, but at least one study, cited in the New York Times, estimates that the Average American knows 600 people. In reality I suspect the real number could be significantly different, but this seems like a realistic ballpark number of people for which the average American could put a name to a face. I'd even be willing to round it up to a generous 1000 personal connections. American society seems like a good proxy for the societies of the Twelve Colonies. According to Wikipedia, there were about 28 billion people alive before the Cylons attacked. So we have all the numbers we need: 28 billion to start with, culled down to 50,000. Average connections of, say, 1000, generously speaking. To abstract and simplify the situation a little, why don't we just imagine that people everywhere died at random, ignoring the fact that survivors were clumped on ships. I am not interested in the intra-ship relationships anyway. I can calculate that the average person would know about 0.0018 people after the attack. What I don't know how to figure out is how many surviving connections between people there would be in a population of 50,000. And that is the real question. It's not just 0.0018  50,000, right? I can't figure out the logic, but I don't expect this to be too hard for some math major to solve. Once you figure out the formula then we could see how likely it would be for you to know people in various social circles, like say 50 for close friends, family, and work colleagues. Bonus question: would the fact that survivors were grouped into ships have a significant effect on the likelihood of inter ship connections?","If you are not familiar, in the show Battlestar Galactica (2003) the Twelve Colonies (planets) are wiped out by the Cylons which they created. Only about 50,000 humans survive in the form of a rag-tag fleet of civilian ships and one Battlestar (give or take). There are at least a few instances in the show of people who were not on the same ship who are re-united or at least know each other from before the attack. Considering that -99.9998% of the human population was wiped out I feel like this would be unlikely, but I want to know exactly how unlikely. Of course, I am not concerned with celebrities and notorious individuals who were ""known"" by a large number of people. I am interested in personal connections. Obviously the definition of ""knowing"" someone is fuzzy, but at least one study, cited in the New York Times, estimates that the Average American knows 600 people. In reality I suspect the real number could be significantly different, but this seems like a realistic ballpark number of people for which the average American could put a name to a face. I'd even be willing to round it up to a generous 1000 personal connections. American society seems like a good proxy for the societies of the Twelve Colonies. According to Wikipedia, there were about 28 billion people alive before the Cylons attacked. So we have all the numbers we need: 28 billion to start with, culled down to 50,000. Average connections of, say, 1000, generously speaking. To abstract and simplify the situation a little, why don't we just imagine that people everywhere died at random, ignoring the fact that survivors were clumped on ships. I am not interested in the intra-ship relationships anyway. I can calculate that the average person would know about 0.0018 people after the attack. What I don't know how to figure out is how many surviving connections between people there would be in a population of 50,000. And that is the real question. It's not just 0.0018  50,000, right? I can't figure out the logic, but I don't expect this to be too hard for some math major to solve. Once you figure out the formula then we could see how likely it would be for you to know people in various social circles, like say 50 for close friends, family, and work colleagues. Bonus question: would the fact that survivors were grouped into ships have a significant effect on the likelihood of inter ship connections?",,"['probability', 'graph-theory']"
45,Question related to Polya urn model,Question related to Polya urn model,,"Just in case I will remind what is called Polya urn: Suppose you have an urn containing one red and one blue ball. You draw one at random. Then if the ball is red, put it back in the urn with an additional red ball, otherwise put it back and add a blue ball. Now denote by $N$ number of balls drawn before first appearance of a blue one. What will be $\operatorname{E}(N+2)^{-1}$? I can find a mean for the $N$ itself just by $$\operatorname{E}(N) = \left(\frac 1 2 \cdot\frac 1 3 \right)\cdot1+ \left( \frac 1 2 \cdot \frac 2 3 \cdot\frac 1 4  \right)\cdot2+\cdots = \sum_{i=1}^\infty \left( \prod_{k=1}^i \frac{k}{k+1}\cdot\frac{1}{i+2}\right)\cdot i$$ but get stuck what to do with $\operatorname{E}(N+2)^{-1}$. Can anyone explain it?","Just in case I will remind what is called Polya urn: Suppose you have an urn containing one red and one blue ball. You draw one at random. Then if the ball is red, put it back in the urn with an additional red ball, otherwise put it back and add a blue ball. Now denote by $N$ number of balls drawn before first appearance of a blue one. What will be $\operatorname{E}(N+2)^{-1}$? I can find a mean for the $N$ itself just by $$\operatorname{E}(N) = \left(\frac 1 2 \cdot\frac 1 3 \right)\cdot1+ \left( \frac 1 2 \cdot \frac 2 3 \cdot\frac 1 4  \right)\cdot2+\cdots = \sum_{i=1}^\infty \left( \prod_{k=1}^i \frac{k}{k+1}\cdot\frac{1}{i+2}\right)\cdot i$$ but get stuck what to do with $\operatorname{E}(N+2)^{-1}$. Can anyone explain it?",,"['probability', 'martingales']"
46,"If John is the talles in his group, what are the odds he is the tallest of the whole University? [closed]","If John is the talles in his group, what are the odds he is the tallest of the whole University? [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question There is a University with $r$ groups. Each group has $n$ students, students are randomly distributed within groups. From one of those groups, John is the tallest kid. Which is the expected ranking of John according to height in the whole University? In general, if a kid is ranked the $k-$th tallest kid in his group, in which position will he be in the whole University rank? For example, with two groups and two students, brute force calculations show that if John is the tallest in one group he will be in the 1.66 position in the general ranking. Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question There is a University with $r$ groups. Each group has $n$ students, students are randomly distributed within groups. From one of those groups, John is the tallest kid. Which is the expected ranking of John according to height in the whole University? In general, if a kid is ranked the $k-$th tallest kid in his group, in which position will he be in the whole University rank? For example, with two groups and two students, brute force calculations show that if John is the tallest in one group he will be in the 1.66 position in the general ranking. Thanks.",,"['probability', 'combinatorics', 'statistics']"
47,Function Composition and Expected Value,Function Composition and Expected Value,,"I was presented with the question The function $f(x)$ is equal to $x^2$ and the function $\chi(x)$ gives a random real number from 0 to $x$. Which usually has a greater expected value, $(f \circ \chi)(x)$ or $(\chi \circ f)(x)$ For what values of $x$? I am almost entirely unexperienced in the field of probability. My approach to this was first to find the expected value of $\chi(x)$, which I think should be $$\frac{1}{x}\int_0^x x dx=\frac{1}{2}x$$ meaning that the expected value of $(f \circ \chi)(x)$ should be  $$f(\frac{1}{2}x)=\frac{1}{4}x^2$$ Then I figured that the expected value of $(\chi \circ f)(x)$ would be $$\frac{1}{x^2}\int_0^{x^2} x dx=\frac{1}{2}x^2$$ Are my methods correct here? If not, how would I go about doing this?","I was presented with the question The function $f(x)$ is equal to $x^2$ and the function $\chi(x)$ gives a random real number from 0 to $x$. Which usually has a greater expected value, $(f \circ \chi)(x)$ or $(\chi \circ f)(x)$ For what values of $x$? I am almost entirely unexperienced in the field of probability. My approach to this was first to find the expected value of $\chi(x)$, which I think should be $$\frac{1}{x}\int_0^x x dx=\frac{1}{2}x$$ meaning that the expected value of $(f \circ \chi)(x)$ should be  $$f(\frac{1}{2}x)=\frac{1}{4}x^2$$ Then I figured that the expected value of $(\chi \circ f)(x)$ would be $$\frac{1}{x^2}\int_0^{x^2} x dx=\frac{1}{2}x^2$$ Are my methods correct here? If not, how would I go about doing this?",,"['probability', 'expectation', 'means', 'random-functions']"
48,Product of $\min$ and $\max$ of $n$ i.i.d. random variables.,Product of  and  of  i.i.d. random variables.,\min \max n,"Let $X_1, \ldots, X_n$ be i.i.d. continuous uniform random variables on $\left[\vartheta-\frac{1}{2},\vartheta+\frac{1}{2}\right]$ for some $\vartheta \in \mathbb R$. I am trying to solve $\mathbb E[(T-\vartheta)^2]$ where $T(X) = \frac{1}{2}\left(\min_{1 \le i \le n} X_i + \max_{1 \le i \le n} X_i\right)$ or simplify it such that it becomes clear that it is independent of $\vartheta$. I have tried to approach this by a variety of conversion but always end up with a term that involes $\mathbb E\left[\left(\min_{1 \le i \le n} X_i\right)\left(\max_{1 \le i \le n} X_i\right)\right]$. I can't find a way to actually calculate it. Do you have a hint or another way of approaching the task?","Let $X_1, \ldots, X_n$ be i.i.d. continuous uniform random variables on $\left[\vartheta-\frac{1}{2},\vartheta+\frac{1}{2}\right]$ for some $\vartheta \in \mathbb R$. I am trying to solve $\mathbb E[(T-\vartheta)^2]$ where $T(X) = \frac{1}{2}\left(\min_{1 \le i \le n} X_i + \max_{1 \le i \le n} X_i\right)$ or simplify it such that it becomes clear that it is independent of $\vartheta$. I have tried to approach this by a variety of conversion but always end up with a term that involes $\mathbb E\left[\left(\min_{1 \le i \le n} X_i\right)\left(\max_{1 \le i \le n} X_i\right)\right]$. I can't find a way to actually calculate it. Do you have a hint or another way of approaching the task?",,"['probability', 'probability-theory', 'statistics']"
49,Probability of rolling 3 and 4 in a row with 4 6-sided dice,Probability of rolling 3 and 4 in a row with 4 6-sided dice,,"So I was playing a game and was wondering what the probability was to roll numbers in a row. Four fair six-sided dice are rolled. What is the probability that three of the numbers will be in a row. Also, that all 4 of them will be in a row. I've tried solving it but I couldn't get it. How can I solve this? I know that with 4 dice there are 1296 combinations but I have not come up with a way to determine the outcome.","So I was playing a game and was wondering what the probability was to roll numbers in a row. Four fair six-sided dice are rolled. What is the probability that three of the numbers will be in a row. Also, that all 4 of them will be in a row. I've tried solving it but I couldn't get it. How can I solve this? I know that with 4 dice there are 1296 combinations but I have not come up with a way to determine the outcome.",,['probability']
50,Applications of matrix Chernoff bounds,Applications of matrix Chernoff bounds,,"The Chernoff bound, being a sharp quantitative version of the law of large numbers, is incredibly useful in many contexts. Some general applications that come to mind (which I guess are really the same idea) are: bounding the sample complexity of PAC algorithms; estimating confidence intervals for polling (somewhat surprisingly, the Chernoff bound tells you that if you want to poll a population of $N$ people, the number $N$ doesn't really matter for the tradeoff between randomly sampled people and accuracy of the empirical average) more generally, very often in the analysis of randomized algorithms you need to argue that you have 'enough' samples, and Chernoff bounds are the way to go. By now I feel like I have a good intuitive grasp of the power and limitations of Chernoff bounds. Basically, my question is about getting a similar understanding of matrix Chernoff bounds: How do I obtain a similar palette of 'classical' applications of matrix Chernoff bounds? What are some of the nice proofs they give us? Have they substantially simplified previous work that didn't explicitly use them?","The Chernoff bound, being a sharp quantitative version of the law of large numbers, is incredibly useful in many contexts. Some general applications that come to mind (which I guess are really the same idea) are: bounding the sample complexity of PAC algorithms; estimating confidence intervals for polling (somewhat surprisingly, the Chernoff bound tells you that if you want to poll a population of $N$ people, the number $N$ doesn't really matter for the tradeoff between randomly sampled people and accuracy of the empirical average) more generally, very often in the analysis of randomized algorithms you need to argue that you have 'enough' samples, and Chernoff bounds are the way to go. By now I feel like I have a good intuitive grasp of the power and limitations of Chernoff bounds. Basically, my question is about getting a similar understanding of matrix Chernoff bounds: How do I obtain a similar palette of 'classical' applications of matrix Chernoff bounds? What are some of the nice proofs they give us? Have they substantially simplified previous work that didn't explicitly use them?",,"['linear-algebra', 'probability', 'big-list', 'random-matrices', 'concentration-of-measure']"
51,Correlation of $X+Y$ and $X-Y$,Correlation of  and,X+Y X-Y,"Suppose that $X$ and $Y$ are random variables with the same variance. Show that $X-Y$ and $X+Y$ are uncorrelated. The solution is below, however, I am confused about a portion of the explanation. Solution: Because the covariance remains unchanged when we add a constant to a random variable, we can assume without loss of generality that $X$ and $Y$ have zero mean. We then have $$\operatorname{cov}(X-Y, X+Y ) = \operatorname{E}[(X-Y )(X + Y )] = \operatorname{E}[X^2] -E[Y^2] = \operatorname{var}(X) - \operatorname{var}(Y ) = 0$$ since $X$ and $Y$ were assumed to have the same variance. My question: I cannot tie the relevance of the phrase ""Because the covariance remains unchanged when we add a constant to a random variable"" to this question. An explanation would be appreciated.","Suppose that $X$ and $Y$ are random variables with the same variance. Show that $X-Y$ and $X+Y$ are uncorrelated. The solution is below, however, I am confused about a portion of the explanation. Solution: Because the covariance remains unchanged when we add a constant to a random variable, we can assume without loss of generality that $X$ and $Y$ have zero mean. We then have $$\operatorname{cov}(X-Y, X+Y ) = \operatorname{E}[(X-Y )(X + Y )] = \operatorname{E}[X^2] -E[Y^2] = \operatorname{var}(X) - \operatorname{var}(Y ) = 0$$ since $X$ and $Y$ were assumed to have the same variance. My question: I cannot tie the relevance of the phrase ""Because the covariance remains unchanged when we add a constant to a random variable"" to this question. An explanation would be appreciated.",,['probability']
52,Expectation value of random length sum,Expectation value of random length sum,,"It's well-known that the expectation value meets the linearity property, which can be described as follows, $$\mathbb{E}_X \left[\sum_{i=1}^t s_i(X)\right]=\sum_{i=1}^t\mathbb{E}_X \left[ s_i(X)\right],$$ where the randomness arises only from $X$. Now let's consider when $t$ is not a fix number but also a random variable whose randomness also comes from $X$. Obviously, the simple linearity equation does not hold since the left-hand is a const and right-hand is random variable, namely  $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right] \neq \sum_{i=1}^{t(X)} \mathbb{E}_X \left[ s_i(X)\right]$$ My question is how to expand or estimate the following one into a linear combination or other simpler formulations?    $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right]$$ update: When $s_i$-s share the identical independent distribution, then it's not hard to obtain that  $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right] = \mathbb{E}_X \left[ t(X)\right] \mathbb{E}_X \left[ s_i(X)\right]$$ but what if they are just not i.i.d.?","It's well-known that the expectation value meets the linearity property, which can be described as follows, $$\mathbb{E}_X \left[\sum_{i=1}^t s_i(X)\right]=\sum_{i=1}^t\mathbb{E}_X \left[ s_i(X)\right],$$ where the randomness arises only from $X$. Now let's consider when $t$ is not a fix number but also a random variable whose randomness also comes from $X$. Obviously, the simple linearity equation does not hold since the left-hand is a const and right-hand is random variable, namely  $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right] \neq \sum_{i=1}^{t(X)} \mathbb{E}_X \left[ s_i(X)\right]$$ My question is how to expand or estimate the following one into a linear combination or other simpler formulations?    $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right]$$ update: When $s_i$-s share the identical independent distribution, then it's not hard to obtain that  $$\mathbb{E}_X \left[\sum_{i=1}^{t(X)} s_i(X)\right] = \mathbb{E}_X \left[ t(X)\right] \mathbb{E}_X \left[ s_i(X)\right]$$ but what if they are just not i.i.d.?",,"['probability', 'stochastic-processes', 'expectation']"
53,We throw 6-side dice 5 times. Find probability that we will have exactly 4 different values.,We throw 6-side dice 5 times. Find probability that we will have exactly 4 different values.,,I am not sure if I did it correctly. Of course $|\Omega|=6^5$ Now $A=\{$event that we obtained exactly 4 different results on 5 throws$\}$ $$|A|= {{6}\choose{1}}{5\choose2}{5\choose1}{4\choose1}{3\choose1}$$ Which means that firstly we choose value which will occur twice. Next we choose on which throws it will occur. Afterwards we just choose 3 different values for remaining spots.,I am not sure if I did it correctly. Of course $|\Omega|=6^5$ Now $A=\{$event that we obtained exactly 4 different results on 5 throws$\}$ $$|A|= {{6}\choose{1}}{5\choose2}{5\choose1}{4\choose1}{3\choose1}$$ Which means that firstly we choose value which will occur twice. Next we choose on which throws it will occur. Afterwards we just choose 3 different values for remaining spots.,,"['probability', 'probability-theory']"
54,Maximizing the probability of a urn problem,Maximizing the probability of a urn problem,,"How can 20 balls, 10 white and 10 black, be put   into two urns so as to maximize the probability of   drawing a white ball if an urn is selected at random   and a ball is drawn at random from it? Intuitively i know the right answer: put 1 white, 0 blacks in one urn and 9 white and 10 blacks in the other, but i just want to arrive at it with more mathematical arguments. Here's my attempt: Define the events: W:= the ball taken is white B:=the ball taken is black $U_1$:=the ball taken is from urn 1 $U_2$:= the ball taken is from urn 2. We are interested in W event, which can be written as the following disjoint union: $W= WU_1\cup WU_2 $. Hence, $P(W)=P(W|U_1)P(U_1)+P(W|U_2)P(U_2)$. Supposing that we put $u$ balls in the first urn, and that from these $w$ are white, we have the following distribuition: $U_1:$  $u$ balls, $w$ white and $u-w$ black. $U_2:$ $20-u$ balls, $10-w$ white and $20-10+w$ black. Therefore, $P(W)=\frac{w}{2u}+\frac{10-w}{2(20-u)}$ I'm actually having trouble in order to maximize it. Can someone help?","How can 20 balls, 10 white and 10 black, be put   into two urns so as to maximize the probability of   drawing a white ball if an urn is selected at random   and a ball is drawn at random from it? Intuitively i know the right answer: put 1 white, 0 blacks in one urn and 9 white and 10 blacks in the other, but i just want to arrive at it with more mathematical arguments. Here's my attempt: Define the events: W:= the ball taken is white B:=the ball taken is black $U_1$:=the ball taken is from urn 1 $U_2$:= the ball taken is from urn 2. We are interested in W event, which can be written as the following disjoint union: $W= WU_1\cup WU_2 $. Hence, $P(W)=P(W|U_1)P(U_1)+P(W|U_2)P(U_2)$. Supposing that we put $u$ balls in the first urn, and that from these $w$ are white, we have the following distribuition: $U_1:$  $u$ balls, $w$ white and $u-w$ black. $U_2:$ $20-u$ balls, $10-w$ white and $20-10+w$ black. Therefore, $P(W)=\frac{w}{2u}+\frac{10-w}{2(20-u)}$ I'm actually having trouble in order to maximize it. Can someone help?",,['probability']
55,Out of 10 people whats the probability that none gets his own umbrella...,Out of 10 people whats the probability that none gets his own umbrella...,,"In a party there are 10 people with same umbrellas,at the end of the party they get one umbrella randomly.What is the probability that none gets its own umbrella? I was thinking on solving this in the followin way: The probability of the first person picking a wrong umbrella is $\frac{9}{10} $, then for the second guy it will be $\frac{8}{9}$, for the third $\frac {7}{8}$ and so on,so the probability of event A to happen would be:  $$p(A) = \frac{9}{10} \cdot \frac{8}{9} \cdot \frac{7}{8} \cdots \frac{1}{2} \cdot 1$$ Is this solution correct,does the possibility of the first person getting the second person hats affect this solution?","In a party there are 10 people with same umbrellas,at the end of the party they get one umbrella randomly.What is the probability that none gets its own umbrella? I was thinking on solving this in the followin way: The probability of the first person picking a wrong umbrella is $\frac{9}{10} $, then for the second guy it will be $\frac{8}{9}$, for the third $\frac {7}{8}$ and so on,so the probability of event A to happen would be:  $$p(A) = \frac{9}{10} \cdot \frac{8}{9} \cdot \frac{7}{8} \cdots \frac{1}{2} \cdot 1$$ Is this solution correct,does the possibility of the first person getting the second person hats affect this solution?",,"['probability', 'combinatorics']"
56,Finding the variance of this uniform distribution.,Finding the variance of this uniform distribution.,,"Let $f(x) = \frac{1}{\theta}$ for $0 \leq x \leq \theta$. Let $X_{(n)} = \text{max}(X_1, \ldots, X_n)$, where $X_1, \ldots, X_n$ are i.i.d. Then we want to find $V(X_{(n)})$. I feel like I have the answer but mine differs from my professors, so if someone could point out where I went wrong that would be immensely helpful. We can find the PDF of $X_{(n)}$ by using the following trick: by definition, the CDF of $X_{(n)}$ is $P(X_{(n)} \leq x)$. However, if the maximum is less than or equal to x, then it follows that all $X_i$ are less than or equal to $x$, and so we have $P(X_{(n)} \leq x) = P(X_1 \leq n, \ldots, X_n \leq n)$. Since the $X_i$ are i.i.d, we have $P(X_{(n)} \leq x) = (F(x))^{n}$, where $F(x)$ denotes the CDF of $f(x)$. Note that the CDF of $f(x)$ is  $$F(x) = \int_0^x \frac{1}{\theta} dy = \frac{y}{\theta} \bigg|_{y=0}^x = \frac{x}{\theta}.$$ Substituting this in, we have that then $P(X_{(n)} \leq x) = \frac{x^n}{\theta^n}$, and by taking the derivative we have that the PDF is $\frac{nx^{n-1}}{\theta^n}$. By a lemma, we may note that $V(X_{(n)}) = \mathbb{E}(X_{(n)}^2) - \mathbb{E}(X_{(n)})^2.$ Hence, we need to then find $\mathbb{E}(X_{(n)}^2)$ and $\mathbb{E}(X_{(n)})^2$. To find $\mathbb{E}(X_{(n)})$, we must solve the integral $$\int_0^{\theta} x \frac{nx^{n-1}}{\theta^n}dx = \int_0^{\theta} \frac{nx^{n}}{\theta^n}dx = \frac{nx^{n+1}}{(n+1)\theta^n} \bigg|_{x=0}^{\theta} = \frac{n\theta}{n+1}.$$ To find $\mathbb{E}(X_{(n)}^2)$, we must solve the integral $$\int_0^{\theta} x^2 \frac{nx^{n-1}}{\theta^n}dx = \int_0^{\theta} \frac{nx^{n+1}}{\theta^n}dx = \frac{nx^{n+2}}{(n+2)\theta^n} \bigg|_{x=0}^{\theta} = \frac{n \theta^2}{n+2}.$$ Using the prior lemma, we have $$V(X_{(n)}) = \mathbb{E}(X_{(n)}^2) - \mathbb{E}(X_{(n)})^2 = \frac{n \theta^2}{n+2} - \bigg(\frac{n\theta}{n+1} \bigg)^2 $$ $$=\frac{n\theta^2}{n+2} - \frac{n^2\theta^2}{n^2+2n+1} = \frac{n\theta^2(n^2+2n+1)-n^2\theta^2(n+2)}{(n+2)(n+1)^2} $$ $$=\frac{n^3\theta^2+2n^2\theta^2+n\theta^2-n^3\theta^2-2n^2\theta^2}{(n+2)(n+1)^2} = \frac{n\theta^2}{(n+2)(n+1)^2}.$$ However, my professor has $V(X_{(n)}) = \frac{n}{(n+2)(n+1)^2}$. Like I said, I don't know where I went wrong, so any help would be nice. Our expected value and PDF agree, so it must be somewhere in the calculation of variance.","Let $f(x) = \frac{1}{\theta}$ for $0 \leq x \leq \theta$. Let $X_{(n)} = \text{max}(X_1, \ldots, X_n)$, where $X_1, \ldots, X_n$ are i.i.d. Then we want to find $V(X_{(n)})$. I feel like I have the answer but mine differs from my professors, so if someone could point out where I went wrong that would be immensely helpful. We can find the PDF of $X_{(n)}$ by using the following trick: by definition, the CDF of $X_{(n)}$ is $P(X_{(n)} \leq x)$. However, if the maximum is less than or equal to x, then it follows that all $X_i$ are less than or equal to $x$, and so we have $P(X_{(n)} \leq x) = P(X_1 \leq n, \ldots, X_n \leq n)$. Since the $X_i$ are i.i.d, we have $P(X_{(n)} \leq x) = (F(x))^{n}$, where $F(x)$ denotes the CDF of $f(x)$. Note that the CDF of $f(x)$ is  $$F(x) = \int_0^x \frac{1}{\theta} dy = \frac{y}{\theta} \bigg|_{y=0}^x = \frac{x}{\theta}.$$ Substituting this in, we have that then $P(X_{(n)} \leq x) = \frac{x^n}{\theta^n}$, and by taking the derivative we have that the PDF is $\frac{nx^{n-1}}{\theta^n}$. By a lemma, we may note that $V(X_{(n)}) = \mathbb{E}(X_{(n)}^2) - \mathbb{E}(X_{(n)})^2.$ Hence, we need to then find $\mathbb{E}(X_{(n)}^2)$ and $\mathbb{E}(X_{(n)})^2$. To find $\mathbb{E}(X_{(n)})$, we must solve the integral $$\int_0^{\theta} x \frac{nx^{n-1}}{\theta^n}dx = \int_0^{\theta} \frac{nx^{n}}{\theta^n}dx = \frac{nx^{n+1}}{(n+1)\theta^n} \bigg|_{x=0}^{\theta} = \frac{n\theta}{n+1}.$$ To find $\mathbb{E}(X_{(n)}^2)$, we must solve the integral $$\int_0^{\theta} x^2 \frac{nx^{n-1}}{\theta^n}dx = \int_0^{\theta} \frac{nx^{n+1}}{\theta^n}dx = \frac{nx^{n+2}}{(n+2)\theta^n} \bigg|_{x=0}^{\theta} = \frac{n \theta^2}{n+2}.$$ Using the prior lemma, we have $$V(X_{(n)}) = \mathbb{E}(X_{(n)}^2) - \mathbb{E}(X_{(n)})^2 = \frac{n \theta^2}{n+2} - \bigg(\frac{n\theta}{n+1} \bigg)^2 $$ $$=\frac{n\theta^2}{n+2} - \frac{n^2\theta^2}{n^2+2n+1} = \frac{n\theta^2(n^2+2n+1)-n^2\theta^2(n+2)}{(n+2)(n+1)^2} $$ $$=\frac{n^3\theta^2+2n^2\theta^2+n\theta^2-n^3\theta^2-2n^2\theta^2}{(n+2)(n+1)^2} = \frac{n\theta^2}{(n+2)(n+1)^2}.$$ However, my professor has $V(X_{(n)}) = \frac{n}{(n+2)(n+1)^2}$. Like I said, I don't know where I went wrong, so any help would be nice. Our expected value and PDF agree, so it must be somewhere in the calculation of variance.",,"['probability', 'statistics']"
57,Conditional expectation of random function,Conditional expectation of random function,,"Let $f:\mathbb{R}\times\Omega\to\mathbb{R}$ be a bounded measurable random function. Assume that for each $x$ the random variable $f(x,\cdot)$ is independent of a $\sigma$-algebra $\mathcal G$. Let $X$ be a $\mathcal G$-measurable random variable. How can we prove that $$ \mathsf E (f(X,\omega)|\mathcal{G})=\mathsf E f(x,\omega)|_{x=X(\omega)} $$ Does someone maybe know a specific reference to a book where this is proven? Thanks!","Let $f:\mathbb{R}\times\Omega\to\mathbb{R}$ be a bounded measurable random function. Assume that for each $x$ the random variable $f(x,\cdot)$ is independent of a $\sigma$-algebra $\mathcal G$. Let $X$ be a $\mathcal G$-measurable random variable. How can we prove that $$ \mathsf E (f(X,\omega)|\mathcal{G})=\mathsf E f(x,\omega)|_{x=X(\omega)} $$ Does someone maybe know a specific reference to a book where this is proven? Thanks!",,"['probability', 'measure-theory', 'random-variables', 'random-functions']"
58,Distribution of unitary matrices generated using SVD of a random matrix in the Ginibre ensemble,Distribution of unitary matrices generated using SVD of a random matrix in the Ginibre ensemble,,"Let $A \in \mathbb C^{n\times n}$. Let $A$ be in the Ginibre ensemble, meaning each element of $A$ is independently drawn from the normal distribution $\mathcal{N}(\mu = 0, \sigma = 1)$. Let $A = U\Sigma V^*$ be its singular value decomposition, where $U$, $V$ are unitary and $\Sigma$ is a diagonal matrix. What is the distribution of $UV^*$? Is it uniform? Note: If we use the QR decomposition instead, $A = QR$, then $Q$ is a uniformly random unitary matrix. See: http://home.lu.lv/~sd20008/papers/essays/Random%20unitary%20%5Bpaper%5D.pdf","Let $A \in \mathbb C^{n\times n}$. Let $A$ be in the Ginibre ensemble, meaning each element of $A$ is independently drawn from the normal distribution $\mathcal{N}(\mu = 0, \sigma = 1)$. Let $A = U\Sigma V^*$ be its singular value decomposition, where $U$, $V$ are unitary and $\Sigma$ is a diagonal matrix. What is the distribution of $UV^*$? Is it uniform? Note: If we use the QR decomposition instead, $A = QR$, then $Q$ is a uniformly random unitary matrix. See: http://home.lu.lv/~sd20008/papers/essays/Random%20unitary%20%5Bpaper%5D.pdf",,"['linear-algebra', 'probability']"
59,Probability (Bayes' Theorem),Probability (Bayes' Theorem),,"The probability that a particular day is a rainy day is $3/4$. Two persons whose credibility are $4/5$ and $2/3$ claim that the day was a rainy day. What is the probability that it was actually a rainy day? I think that I need to apply Bayes' theorem here, but don't know how? This question had come in my exam.","The probability that a particular day is a rainy day is $3/4$. Two persons whose credibility are $4/5$ and $2/3$ claim that the day was a rainy day. What is the probability that it was actually a rainy day? I think that I need to apply Bayes' theorem here, but don't know how? This question had come in my exam.",,"['probability', 'bayes-theorem']"
60,How to calculate the summation $(\sum_{p = k}^{n} \binom{n}{p}) / 2^n$ quickly?,How to calculate the summation  quickly?,(\sum_{p = k}^{n} \binom{n}{p}) / 2^n,"I was solving a question which technically reduces to the following Given $N$ items, it is equiprobable for each item to be good or bad, what is the probability that number of good items in the set are greater than or equal to $K$. This can be reduced to $\dfrac{x}{2^n}$ where $\displaystyle x = \sum_{p = k}^{n} \binom{n}{p}$. Is there a more simplified form which is easier to calculate for large values of $N, K$? Note: It may be safe to assume that we do not require extremely high precision while calculating(first 5-10 digits only). Thanks!","I was solving a question which technically reduces to the following Given $N$ items, it is equiprobable for each item to be good or bad, what is the probability that number of good items in the set are greater than or equal to $K$. This can be reduced to $\dfrac{x}{2^n}$ where $\displaystyle x = \sum_{p = k}^{n} \binom{n}{p}$. Is there a more simplified form which is easier to calculate for large values of $N, K$? Note: It may be safe to assume that we do not require extremely high precision while calculating(first 5-10 digits only). Thanks!",,"['probability', 'combinatorics', 'summation']"
61,Why is negative binomial distribution a sum of independent geometric distributions?,Why is negative binomial distribution a sum of independent geometric distributions?,,"I was trying to prove the expected value of $X$ when $X\sim NB(n,p)$, then I came across this proof: for geometric distribution $X_1, X_2,\ldots, X_n$ with the same parameter $p$, $$E(X)=\sum_{i=1}^n E(X_i)=\frac{n}{p}$$ (At least I think) I get the idea; since NBD is focusing on the number of trials before we get $n$ successes, it would be reasonable to think of it as a combination of $n$ independent GDs. But we need at least $n$ trials for $n$ successes, whereas only 1 trial is enough for each $X_i$. That is, $X$ prevents $n$th success from occuring before $n$th trial, but it is not the case for $X_i$s (since they are all independent). Why is it okay to express $X$ as $\sum X_i$?","I was trying to prove the expected value of $X$ when $X\sim NB(n,p)$, then I came across this proof: for geometric distribution $X_1, X_2,\ldots, X_n$ with the same parameter $p$, $$E(X)=\sum_{i=1}^n E(X_i)=\frac{n}{p}$$ (At least I think) I get the idea; since NBD is focusing on the number of trials before we get $n$ successes, it would be reasonable to think of it as a combination of $n$ independent GDs. But we need at least $n$ trials for $n$ successes, whereas only 1 trial is enough for each $X_i$. That is, $X$ prevents $n$th success from occuring before $n$th trial, but it is not the case for $X_i$s (since they are all independent). Why is it okay to express $X$ as $\sum X_i$?",,"['probability', 'probability-distributions']"
62,Probability of possessing a position among all.,Probability of possessing a position among all.,,"Suppose with staggered entry, $5$ children have come to a teacher to learn alphabets. The teacher assigned them ID according to their arrival, that is, the teacher assigned ID $1$ to the child who came first to her, and then ID $2$ to the next arrival child and so on. Now the teacher wants to give them prize according to their learning ability. That is, the child who learned the alphabets in the shortest time among the $5$ children will be given the first prize, then the child who learned the alphabets in the second shortest time among the $5$ children will be given the second prize, and so on. I know that any child can possess any prize. That is, ID $1$ can have the quickest learning ability or can have the 2nd quickest learning ability or he may be the slowest learner among all. The same applies for any ID. Does the probability that the $i$th arrival possesses the $j$th prize depend on previous $(j-1)$ prizes? That is, can  the probability that the 1st arrival gets ""the 1st prize"" and the probability that the 1st arrival gets ""the 3rd prize"" be different?","Suppose with staggered entry, $5$ children have come to a teacher to learn alphabets. The teacher assigned them ID according to their arrival, that is, the teacher assigned ID $1$ to the child who came first to her, and then ID $2$ to the next arrival child and so on. Now the teacher wants to give them prize according to their learning ability. That is, the child who learned the alphabets in the shortest time among the $5$ children will be given the first prize, then the child who learned the alphabets in the second shortest time among the $5$ children will be given the second prize, and so on. I know that any child can possess any prize. That is, ID $1$ can have the quickest learning ability or can have the 2nd quickest learning ability or he may be the slowest learner among all. The same applies for any ID. Does the probability that the $i$th arrival possesses the $j$th prize depend on previous $(j-1)$ prizes? That is, can  the probability that the 1st arrival gets ""the 1st prize"" and the probability that the 1st arrival gets ""the 3rd prize"" be different?",,['probability']
63,Determining expected number of rounds,Determining expected number of rounds,,"I'm trying to determine a formulation for the expected number of rounds for a game of chance based on dice. The details of the game are as follows: The game initially consists of $N$ players Each player has a fair die or access to one. For each round each player will roll their die once The mean of the rolls is determined Any player that rolled a value less than the mean is eliminated from the game The rounds are repeated until their is only player left. My question is how does one determined the expected number of round in terms of $N$. My thinking is that for any round the expectation is that half of the players will be eliminated. This leads to a continuous halving of the number of players, which seems to suggest that $\text{E}(\text{Rounds}) \approx \log_2 {N}$, or specifically: $$ \lim_{N \to \infty}  \text{E}(\text{Rounds}) =\log_2 {N} $$ I wasn't able to formulate a definitive relation, so I ran a simple monte-carlo of the game, for numbers of players ranging from 2 players to 200 players, one million simulations per game size. The results can be found here: https://gist.github.com/anonymous/f0db85f06343070045b78f7494f19565 The graph for the results and $\log_2 {N}$ is shown in the following as red and blue respectively: Where I'm having difficulty is explaining the continuous and consistent over-estimate of the result curve (via simulation) when compared to the $\log_2{N}$ curve.","I'm trying to determine a formulation for the expected number of rounds for a game of chance based on dice. The details of the game are as follows: The game initially consists of $N$ players Each player has a fair die or access to one. For each round each player will roll their die once The mean of the rolls is determined Any player that rolled a value less than the mean is eliminated from the game The rounds are repeated until their is only player left. My question is how does one determined the expected number of round in terms of $N$. My thinking is that for any round the expectation is that half of the players will be eliminated. This leads to a continuous halving of the number of players, which seems to suggest that $\text{E}(\text{Rounds}) \approx \log_2 {N}$, or specifically: $$ \lim_{N \to \infty}  \text{E}(\text{Rounds}) =\log_2 {N} $$ I wasn't able to formulate a definitive relation, so I ran a simple monte-carlo of the game, for numbers of players ranging from 2 players to 200 players, one million simulations per game size. The results can be found here: https://gist.github.com/anonymous/f0db85f06343070045b78f7494f19565 The graph for the results and $\log_2 {N}$ is shown in the following as red and blue respectively: Where I'm having difficulty is explaining the continuous and consistent over-estimate of the result curve (via simulation) when compared to the $\log_2{N}$ curve.",,"['probability', 'algebra-precalculus']"
64,Minimum Covariance Between Bernoulli Variables,Minimum Covariance Between Bernoulli Variables,,"Suppose $X_1,...,X_n \sim Ber(\frac{1}{2})$, and that $COV(X_i,X_j) = COV(X_k,X_l)$ for $k\neq l,j\neq i$. How small can the covariance be? My attempt: We know that $COV(X_i,X_j) = E(X_1X_2)-E(X_1)E(X_2) = E(X_1X_2)-\frac{1}{4}\geq -\frac{1}{4}$ since $Img(X_iX_j) = \{0,1\}$. And I have an example where that number is attained, simply letting $X_2 =1$ iff $X_1 = 0$. So the minimum is $-\frac{1}{4}$. How off is this?","Suppose $X_1,...,X_n \sim Ber(\frac{1}{2})$, and that $COV(X_i,X_j) = COV(X_k,X_l)$ for $k\neq l,j\neq i$. How small can the covariance be? My attempt: We know that $COV(X_i,X_j) = E(X_1X_2)-E(X_1)E(X_2) = E(X_1X_2)-\frac{1}{4}\geq -\frac{1}{4}$ since $Img(X_iX_j) = \{0,1\}$. And I have an example where that number is attained, simply letting $X_2 =1$ iff $X_1 = 0$. So the minimum is $-\frac{1}{4}$. How off is this?",,"['probability', 'probability-theory', 'covariance']"
65,solving/approximating integral of lognormal cdf,solving/approximating integral of lognormal cdf,,"I am trying to solve the integral, \begin{equation} \int_{0}^b\Phi_{LN}(x, \mu,\sigma)dx \end{equation} where $\Phi_{LN}(x, \mu,\sigma)$ is the lognormal cdf, evaluated at $x$.  i.e., $$\int^x_{0}\frac{1}{\sqrt{2\pi\sigma}y}\exp\left({-\frac{1}{2}\left(\frac{\log y-\mu}{\sigma}\right)^2}\right)dy$$. Being able to efficiently compute the outer integral (top equation) would be great. This would ideally involve expressing it in a form that most software could efficiently solve/approximate. I thought I could have solved this problem by solving the integral to a standard normal cdf, and simply exchanging $x$ with $\frac{log(x)-\mu}{\sigma}$, however this does not seemed to have worked. In case someone wants to use the integral of the standard normal cdf to answer this question it is shown following, $$ \int\Phi_{N(0,1)}(x) = x\Phi_{N(0,1)}(x) - \int x\Phi^{'}_{N(0,1)}(x)dx $$ and here is my failed attempt to convert it to work as the lognormal equivalent, > # define function to integrate N(0,1) CDF > int_LN_CDF = function(x, mu, sigma){ +   # transform to lognormal RV +   y = (log(x) - mu)/sigma +   # integral of standard normal cdf +   y*pnorm(y) + 1/sqrt(2*pi)*exp(-(y^2)/2) + } > # set arbitary values of mu and sigma  > mu = log(2); sigma = 0.4 > # test function against numerical approach > int_LN_CDF(2, mu, sigma) - int_LN_CDF(1, mu, sigma) [1] 0.3820694 > integrate(function(y) pnorm(y, mu, sigma), lower = 1, upper = 2) [1] 0.9491263 with absolute error < 1.1e-14 Any help or guidance would be appreciated. Thanks","I am trying to solve the integral, \begin{equation} \int_{0}^b\Phi_{LN}(x, \mu,\sigma)dx \end{equation} where $\Phi_{LN}(x, \mu,\sigma)$ is the lognormal cdf, evaluated at $x$.  i.e., $$\int^x_{0}\frac{1}{\sqrt{2\pi\sigma}y}\exp\left({-\frac{1}{2}\left(\frac{\log y-\mu}{\sigma}\right)^2}\right)dy$$. Being able to efficiently compute the outer integral (top equation) would be great. This would ideally involve expressing it in a form that most software could efficiently solve/approximate. I thought I could have solved this problem by solving the integral to a standard normal cdf, and simply exchanging $x$ with $\frac{log(x)-\mu}{\sigma}$, however this does not seemed to have worked. In case someone wants to use the integral of the standard normal cdf to answer this question it is shown following, $$ \int\Phi_{N(0,1)}(x) = x\Phi_{N(0,1)}(x) - \int x\Phi^{'}_{N(0,1)}(x)dx $$ and here is my failed attempt to convert it to work as the lognormal equivalent, > # define function to integrate N(0,1) CDF > int_LN_CDF = function(x, mu, sigma){ +   # transform to lognormal RV +   y = (log(x) - mu)/sigma +   # integral of standard normal cdf +   y*pnorm(y) + 1/sqrt(2*pi)*exp(-(y^2)/2) + } > # set arbitary values of mu and sigma  > mu = log(2); sigma = 0.4 > # test function against numerical approach > int_LN_CDF(2, mu, sigma) - int_LN_CDF(1, mu, sigma) [1] 0.3820694 > integrate(function(y) pnorm(y, mu, sigma), lower = 1, upper = 2) [1] 0.9491263 with absolute error < 1.1e-14 Any help or guidance would be appreciated. Thanks",,"['probability', 'statistics', 'definite-integrals', 'normal-distribution']"
66,Law of Iterated Expectation applied to a ratio?,Law of Iterated Expectation applied to a ratio?,,"Consider the random variables $\epsilon_1, X_1, X:=(X_1,\dots,X_n)$ with $X_1,...,X_n$ i.i.d. Is it true that $$\mathbb E\left(\frac{\epsilon_1}{X_1}\right)=\mathbb E\left(\frac{\mathbb E(\epsilon_1\mid X)}{X_1} \right)$$ Sketch of proof?","Consider the random variables $\epsilon_1, X_1, X:=(X_1,\dots,X_n)$ with $X_1,...,X_n$ i.i.d. Is it true that $$\mathbb E\left(\frac{\epsilon_1}{X_1}\right)=\mathbb E\left(\frac{\mathbb E(\epsilon_1\mid X)}{X_1} \right)$$ Sketch of proof?",,"['probability', 'random-variables', 'expectation', 'conditional-expectation']"
67,why scale a normal distribution by the square root of the variance?,why scale a normal distribution by the square root of the variance?,,"Given a standard normal distribution $N(0,1)$ why, if you wish to scale it, you need to multiply by the square root of the variance?  Ie given a variance t, the new scaled distribution is $N(0, t)$ which is equivalent to $\sqrt{t}*N(0,1)$? Isn't the pdf of the general normal $\frac{1}{\sigma}$*(standard normal pdf)? An intuitive explanation and proof would be quite helpful.","Given a standard normal distribution $N(0,1)$ why, if you wish to scale it, you need to multiply by the square root of the variance?  Ie given a variance t, the new scaled distribution is $N(0, t)$ which is equivalent to $\sqrt{t}*N(0,1)$? Isn't the pdf of the general normal $\frac{1}{\sigma}$*(standard normal pdf)? An intuitive explanation and proof would be quite helpful.",,['probability']
68,"Suppose that $X_i$ are independent random variables, with finite absolute moment. Then $Max(X_1, \ldots, X_n) / n \to 0$ a.s.?","Suppose that  are independent random variables, with finite absolute moment. Then  a.s.?","X_i Max(X_1, \ldots, X_n) / n \to 0","Suppose that $X_i$ are i.i.d. random variables, with finite absolute moment: $E|X_1| < \infty$. Then $\max(X_1, \ldots, X_n) / n \to 0$ a.s. ? Let $M_n = \max(X_1, \ldots, X_n)$. I know that $M_n$ has a CDF which is $F_n(x) = F_1(x)^n$. I would like to apply Borel-Cantelli to the sets $A_n = \{ M_n > n \epsilon \}$. So I would like to show that $\Sigma P(A_n) < \infty$. I suspect that one can obtain a bound that is something like $\Sigma P(A_n) \leq E|X_1|$. However, because one doesn't have $\Sigma 1_{A_n} \leq X_1$, this is unlikely to be literally true. At some point one will need to use the independence. So I was trying to proceed by analyzing the product $\Pi ( 1 - P(A_n))$ instead. We would like this to converge to a nonzero limit. This means that we want $P( \cap A_n^c) > 0$, at least for small epsilon, because of the assumed independence. But the problem is that $P(A_1) = 0$ is already possible. I would appreciate a hint to set me on the right track. :)","Suppose that $X_i$ are i.i.d. random variables, with finite absolute moment: $E|X_1| < \infty$. Then $\max(X_1, \ldots, X_n) / n \to 0$ a.s. ? Let $M_n = \max(X_1, \ldots, X_n)$. I know that $M_n$ has a CDF which is $F_n(x) = F_1(x)^n$. I would like to apply Borel-Cantelli to the sets $A_n = \{ M_n > n \epsilon \}$. So I would like to show that $\Sigma P(A_n) < \infty$. I suspect that one can obtain a bound that is something like $\Sigma P(A_n) \leq E|X_1|$. However, because one doesn't have $\Sigma 1_{A_n} \leq X_1$, this is unlikely to be literally true. At some point one will need to use the independence. So I was trying to proceed by analyzing the product $\Pi ( 1 - P(A_n))$ instead. We would like this to converge to a nonzero limit. This means that we want $P( \cap A_n^c) > 0$, at least for small epsilon, because of the assumed independence. But the problem is that $P(A_1) = 0$ is already possible. I would appreciate a hint to set me on the right track. :)",,"['probability', 'probability-theory', 'random-variables', 'independence', 'borel-cantelli-lemmas']"
69,What is the probability that the GCD of $n$ naturals $\le m$ is 1?,What is the probability that the GCD of  naturals  is 1?,n \le m,"What is the probability that the GCD of $n$ naturals $\le m$ is 1? For example, if the there are $n=2$ numbers less than or equal to $m=2$, there are four combinations - {1,1},{1,2},{2,1},{2,2}.  Only the last combination, {2,2}, has a GCD greater than 1.  So with $n=2, m=2$, the probability is 3/4.","What is the probability that the GCD of $n$ naturals $\le m$ is 1? For example, if the there are $n=2$ numbers less than or equal to $m=2$, there are four combinations - {1,1},{1,2},{2,1},{2,2}.  Only the last combination, {2,2}, has a GCD greater than 1.  So with $n=2, m=2$, the probability is 3/4.",,"['probability', 'elementary-number-theory', 'intuition', 'prime-factorization', 'gcd-and-lcm']"
70,Finding $E(X)$ and $Var(X)$ of a uniformally distributed continuous variable,Finding  and  of a uniformally distributed continuous variable,E(X) Var(X),"I am having a problem finding E(X), Var(X) of a uniformly distributed variable. Can someone please help or correct me. The question and solution is below. Thanks","I am having a problem finding E(X), Var(X) of a uniformly distributed variable. Can someone please help or correct me. The question and solution is below. Thanks",,"['probability', 'proof-verification', 'expectation', 'uniform-distribution']"
71,Conditional Probability for a coin to be fair,Conditional Probability for a coin to be fair,,"A gambler has in his pocket a fair coin and a biased coin which will land heads with probability $\frac34$. He selects one of the coins at random; when he tosses it, it lands heads. What is the probability it is the fair coin? (II)If he tosses the same coin a second time, and again it lands heads. What now is the probability it is the fair coin? (III) If he tosses the same coin for a third time, and this time it lands tails. What now is the probability it is the fair coin? My solution using Bayes: (I)$$P(\text{fair}|\text{heads})=\frac{P(\text{fair}\cap\text{heads})}{P(\text{heads})}=\frac{\frac12\cdot\frac12}{\frac12\cdot\frac12+\frac12\cdot\frac34}=\frac{\frac14}{\frac58}=\frac25$$ (II) $$P(\text{fair}|\text{heads,heads})=\frac{P(\text{fair}\cap\text{heads}\cap\text{heads})}{P(\text{heads}\cap\text{heads})}=\frac{\frac12\cdot\frac12\cdot\frac12}{\frac12\cdot\frac12\cdot\frac12+\frac12\cdot\frac34\cdot\frac34}=\frac{4}{13}$$ (III) $$P(\text{fair}|\text{heads,heads,Tails})=\frac{P(\text{fair}\cap\text{heads}\cap\text{heads}\cap\text{tails})}{P(\text{heads}\cap\text{heads}\cap\text{tails})}=\frac{\frac12\cdot\frac12\cdot\frac12\cdot\frac12}{\frac12\cdot\frac12\cdot\frac12\cdot\frac12+\frac12\cdot\frac34\cdot\frac34\cdot\frac14}=\frac{8}{17}$$ Please can someone help me if my understanding is correct.","A gambler has in his pocket a fair coin and a biased coin which will land heads with probability $\frac34$. He selects one of the coins at random; when he tosses it, it lands heads. What is the probability it is the fair coin? (II)If he tosses the same coin a second time, and again it lands heads. What now is the probability it is the fair coin? (III) If he tosses the same coin for a third time, and this time it lands tails. What now is the probability it is the fair coin? My solution using Bayes: (I)$$P(\text{fair}|\text{heads})=\frac{P(\text{fair}\cap\text{heads})}{P(\text{heads})}=\frac{\frac12\cdot\frac12}{\frac12\cdot\frac12+\frac12\cdot\frac34}=\frac{\frac14}{\frac58}=\frac25$$ (II) $$P(\text{fair}|\text{heads,heads})=\frac{P(\text{fair}\cap\text{heads}\cap\text{heads})}{P(\text{heads}\cap\text{heads})}=\frac{\frac12\cdot\frac12\cdot\frac12}{\frac12\cdot\frac12\cdot\frac12+\frac12\cdot\frac34\cdot\frac34}=\frac{4}{13}$$ (III) $$P(\text{fair}|\text{heads,heads,Tails})=\frac{P(\text{fair}\cap\text{heads}\cap\text{heads}\cap\text{tails})}{P(\text{heads}\cap\text{heads}\cap\text{tails})}=\frac{\frac12\cdot\frac12\cdot\frac12\cdot\frac12}{\frac12\cdot\frac12\cdot\frac12\cdot\frac12+\frac12\cdot\frac34\cdot\frac34\cdot\frac14}=\frac{8}{17}$$ Please can someone help me if my understanding is correct.",,"['probability', 'bayes-theorem']"
72,Probability that the final molecule removed from urn $ 1 $ is red,Probability that the final molecule removed from urn  is red, 1 ,"$ \textbf{Question:} $ (from Ross' $ \textit{A First Course in Probability}) $ Urn $ 1 $ initially has $ n $ red molecules and urn $ 2 $ has $ n $ blue molecules. Molecules are randomly removed from urn $ 1 $ in the following manner: After each removal from urn $ 1, $ a molecule is taken from urn $ 2 $ (if urn $ 2 $ has any molecules) and placed in urn $ 1. $ The process continues until all the molecules have been removed. (Thus, there are $ 2n $ removals in all.) What is the probability that the final molecule removed from urn $ 1 $ is red? I have one question about a step in the solution that is confusing me so any help would be appreciated. $ \textbf{Solution:} $ Focus attention on any particular red molecule, and let $ F $ be the event that this molecule is the 铿nal one selected. Now, in order for $ F $ to occur, the molecule in question must still be in the urn after the 铿rst $ n $ molecules have been removed (at which time urn $ 2 $ is empty). My question is in the previous sentence why is it $ n $ but not $ 2n -1? $ In order for $ F $ to happen, doesn't that mean the selected molecule must still remain in urn $ 1 $ after $ 2n - 1 $ molecules have been removed? $ \dots $ The proof continues and the desired probability is $ \displaystyle \left(1 - \frac{1}{n} \right)^n $","$ \textbf{Question:} $ (from Ross' $ \textit{A First Course in Probability}) $ Urn $ 1 $ initially has $ n $ red molecules and urn $ 2 $ has $ n $ blue molecules. Molecules are randomly removed from urn $ 1 $ in the following manner: After each removal from urn $ 1, $ a molecule is taken from urn $ 2 $ (if urn $ 2 $ has any molecules) and placed in urn $ 1. $ The process continues until all the molecules have been removed. (Thus, there are $ 2n $ removals in all.) What is the probability that the final molecule removed from urn $ 1 $ is red? I have one question about a step in the solution that is confusing me so any help would be appreciated. $ \textbf{Solution:} $ Focus attention on any particular red molecule, and let $ F $ be the event that this molecule is the 铿nal one selected. Now, in order for $ F $ to occur, the molecule in question must still be in the urn after the 铿rst $ n $ molecules have been removed (at which time urn $ 2 $ is empty). My question is in the previous sentence why is it $ n $ but not $ 2n -1? $ In order for $ F $ to happen, doesn't that mean the selected molecule must still remain in urn $ 1 $ after $ 2n - 1 $ molecules have been removed? $ \dots $ The proof continues and the desired probability is $ \displaystyle \left(1 - \frac{1}{n} \right)^n $",,[]
73,Markov Chain and Forward and Backward Probabilities with Alice and Bob,Markov Chain and Forward and Backward Probabilities with Alice and Bob,,"System Alice and Bob are moving independently from one city to another. There are $d$ cities, the probability of moving to another city (for each individual) is $m$ and each move is equiprobable (there is no preferred city). The choice of moving and choice of where to move to of Alice are independent of the choices of Bob. Terminology Let $X_t$ be the state of the system at time $t$. Let $S$ be the state in which Alice and Bob are in the same city, while $\bar S$ is the state in which Alice and Bob are in different cities. therefore, $P(X_{t-1}=S \space|\space X_{t}=\bar S)$ is the probability that at time $t-1$ Alice and Bob were in the same city given that they currently are in different cities. Previous post? FYI, we have shown in this post (no need to read it) that $P(X_t = S \space|\space X_{t-1} = \bar S) = \frac{m(2d-md-2)}{(d-1)^2}$. Question I am trying to understand the relationship between the following eight probabilities Forward Probabilities $P(X_t = S \space|\space X_{t-1} = \bar S)$ $P(X_t = \bar S \space|\space X_{t-1} = \bar S)$ $P(X_t = S \space|\space X_{t-1} = S)$ $P(X_t = \bar S \space|\space X_{t-1} = S)$ Backward Probabilities $P(X_{t-1} = S \space|\space X_t = \bar S)$ $P(X_{t-1} = \bar S \space|\space X_t = \bar S)$ $P(X_{t-1} = S \space|\space X_t = S)$ $P(X_{t-1} = \bar S \space|\space X_t = S)$ We will assume that the markov process started at $t=-\infty$. What relationships are there between these probabilities? How many probabilities do we need to know to infer all the others? My thoughts Let $A$ and $B$ be independent variables that can take either values $S$ or $\bar S$. It is clear for me that (forward probabilities) $$P(X_t = S \space|\space X_{t-1} = A) = 1 - P(X_t = \bar S \space|\space X_{t-1} = A) \space \forall \space A$$ and (backward probabilities) $$P(X_{t-1} = S \space|\space X = A) = 1 - P(X_{t-1} = \bar S \space|\space X = A) \space\forall \space A$$ Now it feels to me that $$P(X_t = A \space|\space X_{t-1} = B) = P(X_{t-1} = B \space|\space X = A) \space\forall\space A,B$$ Is it true? What characteristic of my system make it true? (Is it true for my system because $m$ is the same for all pair of cities?)","System Alice and Bob are moving independently from one city to another. There are $d$ cities, the probability of moving to another city (for each individual) is $m$ and each move is equiprobable (there is no preferred city). The choice of moving and choice of where to move to of Alice are independent of the choices of Bob. Terminology Let $X_t$ be the state of the system at time $t$. Let $S$ be the state in which Alice and Bob are in the same city, while $\bar S$ is the state in which Alice and Bob are in different cities. therefore, $P(X_{t-1}=S \space|\space X_{t}=\bar S)$ is the probability that at time $t-1$ Alice and Bob were in the same city given that they currently are in different cities. Previous post? FYI, we have shown in this post (no need to read it) that $P(X_t = S \space|\space X_{t-1} = \bar S) = \frac{m(2d-md-2)}{(d-1)^2}$. Question I am trying to understand the relationship between the following eight probabilities Forward Probabilities $P(X_t = S \space|\space X_{t-1} = \bar S)$ $P(X_t = \bar S \space|\space X_{t-1} = \bar S)$ $P(X_t = S \space|\space X_{t-1} = S)$ $P(X_t = \bar S \space|\space X_{t-1} = S)$ Backward Probabilities $P(X_{t-1} = S \space|\space X_t = \bar S)$ $P(X_{t-1} = \bar S \space|\space X_t = \bar S)$ $P(X_{t-1} = S \space|\space X_t = S)$ $P(X_{t-1} = \bar S \space|\space X_t = S)$ We will assume that the markov process started at $t=-\infty$. What relationships are there between these probabilities? How many probabilities do we need to know to infer all the others? My thoughts Let $A$ and $B$ be independent variables that can take either values $S$ or $\bar S$. It is clear for me that (forward probabilities) $$P(X_t = S \space|\space X_{t-1} = A) = 1 - P(X_t = \bar S \space|\space X_{t-1} = A) \space \forall \space A$$ and (backward probabilities) $$P(X_{t-1} = S \space|\space X = A) = 1 - P(X_{t-1} = \bar S \space|\space X = A) \space\forall \space A$$ Now it feels to me that $$P(X_t = A \space|\space X_{t-1} = B) = P(X_{t-1} = B \space|\space X = A) \space\forall\space A,B$$ Is it true? What characteristic of my system make it true? (Is it true for my system because $m$ is the same for all pair of cities?)",,"['probability', 'markov-chains', 'problem-solving', 'markov-process']"
74,Expectation of Product of Ito Integrals wrt Independent Brownian Motions,Expectation of Product of Ito Integrals wrt Independent Brownian Motions,,"Let $W_1(t)$, $W_2(t)$, $W_3(t)$ be independent Brownian motions and $f$, $g$ smooth functions. I want to know if the following is true: $$    \mathbb{E}\left[       \left(          \int\limits_0^t f(W_3(s),s) \,dW_1(s)       \right)       \left(          \int\limits_0^t g(W_3(s),s) \,dW_2(s)       \right)    \right] = 0 $$ I think it is, because, as an argument from intuition: $$ \begin{align}    \mathbb{E}&\left[       \left(          \int\limits_0^t  f(W_3(s),s) \,dW_1(s)       \right)       \left(          \int\limits_0^t g(W_3(s),s) \,dW_2(s)       \right)    \right]\\  &= \mathbb{E}\left[       \left(          \lim_{n\rightarrow\infty}\sum\limits_{i=0}^{n-1}                    f(W_3(t_i),t_i) [W_1(t_{i+1})-W_1(t_{i})]       \right)       \left(          \lim_{n\rightarrow\infty}\sum\limits_{j=0}^{n-1}                    g(W_3(t_j),t_j) [W_2(t_{j+1})-W_2(t_{j})]       \right)    \right]\\ &= \lim_{n\rightarrow\infty} \sum\limits_{i=0}^{n-1} \sum\limits_{j=0}^{n-1} \mathbb{E}\left[       f(W_3(t_i),t_i) g(W_3(t_j),t_j)    \right] \underbrace{ \mathbb{E}\left[     W_1(t_{i+1})-W_1(t_{i}) \right] }_0 \underbrace{ \mathbb{E}\left[     W_2(t_{j+1})-W_2(t_{j}) \right] }_0\\ &=0 \end{align} $$ since increments of $W$ are Gaussian. But I am not sure if there are some stochastic analysis concepts I am missing. Thanks!","Let $W_1(t)$, $W_2(t)$, $W_3(t)$ be independent Brownian motions and $f$, $g$ smooth functions. I want to know if the following is true: $$    \mathbb{E}\left[       \left(          \int\limits_0^t f(W_3(s),s) \,dW_1(s)       \right)       \left(          \int\limits_0^t g(W_3(s),s) \,dW_2(s)       \right)    \right] = 0 $$ I think it is, because, as an argument from intuition: $$ \begin{align}    \mathbb{E}&\left[       \left(          \int\limits_0^t  f(W_3(s),s) \,dW_1(s)       \right)       \left(          \int\limits_0^t g(W_3(s),s) \,dW_2(s)       \right)    \right]\\  &= \mathbb{E}\left[       \left(          \lim_{n\rightarrow\infty}\sum\limits_{i=0}^{n-1}                    f(W_3(t_i),t_i) [W_1(t_{i+1})-W_1(t_{i})]       \right)       \left(          \lim_{n\rightarrow\infty}\sum\limits_{j=0}^{n-1}                    g(W_3(t_j),t_j) [W_2(t_{j+1})-W_2(t_{j})]       \right)    \right]\\ &= \lim_{n\rightarrow\infty} \sum\limits_{i=0}^{n-1} \sum\limits_{j=0}^{n-1} \mathbb{E}\left[       f(W_3(t_i),t_i) g(W_3(t_j),t_j)    \right] \underbrace{ \mathbb{E}\left[     W_1(t_{i+1})-W_1(t_{i}) \right] }_0 \underbrace{ \mathbb{E}\left[     W_2(t_{j+1})-W_2(t_{j}) \right] }_0\\ &=0 \end{align} $$ since increments of $W$ are Gaussian. But I am not sure if there are some stochastic analysis concepts I am missing. Thanks!",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-integrals']"
75,Finding Size-Bias Distributions,Finding Size-Bias Distributions,,"For a RV $W$ with mean $\mu$, let $W^*$ denote the $W$-size biased distribution (so that $EG(W^*)=\frac{E(WG(W))}{\mu}$ for all functions $G$ for which the expectations exist). I would like to learn how to find a formula for $W^* $ when $W=X_1+\cdots +X_n$ is a sum of dependent RVs. From ""Multivariate Normal Approximations by Stein's Method and Size Bias Couplings"", https://arxiv.org/pdf/math/0510586v1.pdf page 3: We replace $X_I$ with $X^* _I$ where the random index is chosen independently with $\mathbb{P}(I=i)=\frac{EX_i}{\sum EX_j}$, and adjusting the remaining variables to their conditional distribution given the new value of $X_I$. I cannot quite understand this statement. I want to be able to apply this to find $W^*$ for particular examples. For example, I'd like to know how to find $W^*$ when $X_i \sim Be(p_i)$ for $i=1,\ldots, n$.","For a RV $W$ with mean $\mu$, let $W^*$ denote the $W$-size biased distribution (so that $EG(W^*)=\frac{E(WG(W))}{\mu}$ for all functions $G$ for which the expectations exist). I would like to learn how to find a formula for $W^* $ when $W=X_1+\cdots +X_n$ is a sum of dependent RVs. From ""Multivariate Normal Approximations by Stein's Method and Size Bias Couplings"", https://arxiv.org/pdf/math/0510586v1.pdf page 3: We replace $X_I$ with $X^* _I$ where the random index is chosen independently with $\mathbb{P}(I=i)=\frac{EX_i}{\sum EX_j}$, and adjusting the remaining variables to their conditional distribution given the new value of $X_I$. I cannot quite understand this statement. I want to be able to apply this to find $W^*$ for particular examples. For example, I'd like to know how to find $W^*$ when $X_i \sim Be(p_i)$ for $i=1,\ldots, n$.",,"['probability', 'statistics', 'probability-distributions', 'sampling']"
76,Proving that a positive-integer valued random variable has the lack of memory property iff it has a geometric distribution.,Proving that a positive-integer valued random variable has the lack of memory property iff it has a geometric distribution.,,"Suppose that $X$ is a positive-integer valued random variable with the lack of memory property which states: Given that $X>n$, then $\mathbb{P}(X=n+k) = \mathbb{P}(X=k)$. Consider the case where $p=\mathbb{P}(X=1)$ and let $q_n = \mathbb{P}(X>n)$ for each $n \in \Bbb N$ and $k=1$. $$\mathbb{P}(X=n+1\mid X>n)=\dfrac {\mathbb{P} (X>n, X=n+1)}{\mathbb{P}(X>n)}$$ which by the conditional probability rule $$= \frac {\mathbb{P} (X=n+1)}{\mathbb{P}(X>n)} = \frac {\mathbb{P} (X=n+1)}{q_n}$$ Here's where I get tripped up: My textbook shows that $$\frac {\mathbb{P} (X=n+1)}{q_n} = \frac {q_n-q_{n+1}}{q_n}.$$ How is that $q_n-q_{n+1}$ can be substituted for $\mathbb{P}(X=n+1)$? I guess this is really a question of algebra, but I'm confused because my textbook skips over so many steps when it is showing how to derive certain equations.","Suppose that $X$ is a positive-integer valued random variable with the lack of memory property which states: Given that $X>n$, then $\mathbb{P}(X=n+k) = \mathbb{P}(X=k)$. Consider the case where $p=\mathbb{P}(X=1)$ and let $q_n = \mathbb{P}(X>n)$ for each $n \in \Bbb N$ and $k=1$. $$\mathbb{P}(X=n+1\mid X>n)=\dfrac {\mathbb{P} (X>n, X=n+1)}{\mathbb{P}(X>n)}$$ which by the conditional probability rule $$= \frac {\mathbb{P} (X=n+1)}{\mathbb{P}(X>n)} = \frac {\mathbb{P} (X=n+1)}{q_n}$$ Here's where I get tripped up: My textbook shows that $$\frac {\mathbb{P} (X=n+1)}{q_n} = \frac {q_n-q_{n+1}}{q_n}.$$ How is that $q_n-q_{n+1}$ can be substituted for $\mathbb{P}(X=n+1)$? I guess this is really a question of algebra, but I'm confused because my textbook skips over so many steps when it is showing how to derive certain equations.",,"['probability', 'probability-distributions', 'geometric-probability']"
77,"If A can either increase by 100% or decrease by 50% with equal probability, what will be the arithmetic mean return over n periods?","If A can either increase by 100% or decrease by 50% with equal probability, what will be the arithmetic mean return over n periods?",,"This is more of a finance related question but deals with some discrete probability and or combinations.  The question goes like this.  If you buy stock A, and it has a 50% chance of going up 100% in a period, or 50% chance of going down 50%, what is the arithmetic mean return over n periods? Some people are saying the answer is 25%.  But, I don't see how that works out beyond 1 period.  For illustration, consider all possible combination of returns over 1 and 2 periods.  One has the following, one period: 1 , 2 1 , 0.5 two periods: 1 , 2 , 1 1 , 0.5, 1 1 , 2 , 4 1 , 0.5 , 0.25 The total and arithmetic mean returns for each scenario are one period: 100%     100% -50%     -50% two periods: 0%         0% 0%         0% 300%     150% -75%   -37.5% So, averaging the first periods mean arithmetic returns, one gets 25%.  But, for two periods, averaging the mean arithmetic returns one gets a bit over 28%.  Maybe I'm not understanding the definitions of arithmetic returns, but can anyone here tell me what I'm doing wrong?  Hopefully someone has some finance knowledge and understands how to calculate arithmetic mean returns.","This is more of a finance related question but deals with some discrete probability and or combinations.  The question goes like this.  If you buy stock A, and it has a 50% chance of going up 100% in a period, or 50% chance of going down 50%, what is the arithmetic mean return over n periods? Some people are saying the answer is 25%.  But, I don't see how that works out beyond 1 period.  For illustration, consider all possible combination of returns over 1 and 2 periods.  One has the following, one period: 1 , 2 1 , 0.5 two periods: 1 , 2 , 1 1 , 0.5, 1 1 , 2 , 4 1 , 0.5 , 0.25 The total and arithmetic mean returns for each scenario are one period: 100%     100% -50%     -50% two periods: 0%         0% 0%         0% 300%     150% -75%   -37.5% So, averaging the first periods mean arithmetic returns, one gets 25%.  But, for two periods, averaging the mean arithmetic returns one gets a bit over 28%.  Maybe I'm not understanding the definitions of arithmetic returns, but can anyone here tell me what I'm doing wrong?  Hopefully someone has some finance knowledge and understands how to calculate arithmetic mean returns.",,"['probability', 'finance']"
78,Mean and Variance Geometric Brownian Motion with not constant drift and volatility,Mean and Variance Geometric Brownian Motion with not constant drift and volatility,,"I have to derive the Geometric Brownian motion (with not constant drift and volatility), and to find the mean and variance of the solution. $\quad \left\{\begin{aligned} & d X_t = \mu(t) X_t d t + \sigma(t) X_t d W_t \\ & X_0 = \xi \end{aligned}\right.$ The solution can be obtained in a classical manner by Ito's Lemma: $X_t = \xi e^{\int_0^t \left(\mu(s) - \frac{\sigma^2(s)}{2}\right) d s + \int_0^t \sigma(s) d W_s}$ And we can find the mean and variance: $\mathbb{E}[X_t] = \xi e^{\int_0^t \left(\mu(s) - \frac{\sigma^2(s)}{2}\right) ds} \mathbb{E}\left[e^{\int_0^t \sigma(s) dW_s}\right]$ $Var(X_t) = \xi^2 e^{\int_0^t \left(2\mu(s) - \sigma^2(s)\right) d s} \left(\mathbb{E}\left[e^{2 \int_0^t \sigma(s) d W_s}\right] - \mathbb{E}\left[e^{\int_0^t \sigma(s) d W_s}\right]^2\right)\\$ These expression are not really simple, as they are when $\mu$ and $\sigma$ are constant. Does someone know if we have a general expression for the expectation of the exponential of an It么's Integral? (in this case of a deterministic function). I.e: $\mathbb{E}\left[e^{\int_0^t \sigma(s) dW_s}\right]$ Thanks.","I have to derive the Geometric Brownian motion (with not constant drift and volatility), and to find the mean and variance of the solution. $\quad \left\{\begin{aligned} & d X_t = \mu(t) X_t d t + \sigma(t) X_t d W_t \\ & X_0 = \xi \end{aligned}\right.$ The solution can be obtained in a classical manner by Ito's Lemma: $X_t = \xi e^{\int_0^t \left(\mu(s) - \frac{\sigma^2(s)}{2}\right) d s + \int_0^t \sigma(s) d W_s}$ And we can find the mean and variance: $\mathbb{E}[X_t] = \xi e^{\int_0^t \left(\mu(s) - \frac{\sigma^2(s)}{2}\right) ds} \mathbb{E}\left[e^{\int_0^t \sigma(s) dW_s}\right]$ $Var(X_t) = \xi^2 e^{\int_0^t \left(2\mu(s) - \sigma^2(s)\right) d s} \left(\mathbb{E}\left[e^{2 \int_0^t \sigma(s) d W_s}\right] - \mathbb{E}\left[e^{\int_0^t \sigma(s) d W_s}\right]^2\right)\\$ These expression are not really simple, as they are when $\mu$ and $\sigma$ are constant. Does someone know if we have a general expression for the expectation of the exponential of an It么's Integral? (in this case of a deterministic function). I.e: $\mathbb{E}\left[e^{\int_0^t \sigma(s) dW_s}\right]$ Thanks.",,"['probability', 'expectation', 'brownian-motion', 'stochastic-integrals']"
79,Special dice generating non-decreasing sequence,Special dice generating non-decreasing sequence,,"Suppose that, when rolled for the first time, a special 6-sided dice shows $1,\ldots, 6$ with probability $\frac{1}{6}$ each, and then, upon rerolling, shows with equal probability a number greater or equal to the previous outcome (so e.g. if you roll a $4$, the subsequent outcome is going to be one of $4, 5, 6$, each with probability $\frac{1}{3}$). Now, what is the probability that the $n$-th roll is a $6$? This is a homemade question and I'm wondering if there is an elegant way to solve it, avoiding messy computation.","Suppose that, when rolled for the first time, a special 6-sided dice shows $1,\ldots, 6$ with probability $\frac{1}{6}$ each, and then, upon rerolling, shows with equal probability a number greater or equal to the previous outcome (so e.g. if you roll a $4$, the subsequent outcome is going to be one of $4, 5, 6$, each with probability $\frac{1}{3}$). Now, what is the probability that the $n$-th roll is a $6$? This is a homemade question and I'm wondering if there is an elegant way to solve it, avoiding messy computation.",,"['probability', 'dice']"
80,Flipping 100 coins and getting all heads.,Flipping 100 coins and getting all heads.,,"I hold 100 fair coins in my hands. Then, I toss the 100 coins high into the air all at once. I understand that the chance of getting 100 heads when they land is 1 in 2 exp 100. Can someone help me determine, on average, how many attempts at tossing 100 coins all at once need to be made to obtain 100 heads.  (I believe this might be the ""expected"" number of tosses.) I'm asking because my intuition says that if the 100 coin toss was made once per second then the universe is not old enough to expect a toss with 100 heads as an outcome and I'd like to confirm this. (The universe is roughly 13.82 billion years or approximately 4.35 exp 17 seconds old.)  Thank you.","I hold 100 fair coins in my hands. Then, I toss the 100 coins high into the air all at once. I understand that the chance of getting 100 heads when they land is 1 in 2 exp 100. Can someone help me determine, on average, how many attempts at tossing 100 coins all at once need to be made to obtain 100 heads.  (I believe this might be the ""expected"" number of tosses.) I'm asking because my intuition says that if the 100 coin toss was made once per second then the universe is not old enough to expect a toss with 100 heads as an outcome and I'd like to confirm this. (The universe is roughly 13.82 billion years or approximately 4.35 exp 17 seconds old.)  Thank you.",,['probability']
81,"Let. $X \sim \mathcal{U}(0,1)$. Given $X = x$, let $Y \sim \mathcal{U}(0,x)$. How can I calculate $\mathbb{E}(X|Y = y)$?","Let. . Given , let . How can I calculate ?","X \sim \mathcal{U}(0,1) X = x Y \sim \mathcal{U}(0,x) \mathbb{E}(X|Y = y)","Suppose that $X$ is uniformly distributed over $[0,1]$. Now choose $X = x$ and let $Y$ be uniformly distributed over $[0,x]$. Is it possible for us to calculate the ""expected value of $X$ given $Y = y$"", i.e. , $\mathbb{E}(X|Y = y)$? Intuitively, it seems that if $y = 0$, we gain no information, and so $\mathbb{E}(X|Y = 0) = \mathbb{E}(X) = \dfrac{1}{2}$, and also that if $y = 1$, it must be the case that $x = 1$, so $\mathbb{E}(X|Y = 1) = 1$. I don't know if this reasoning is correct, but if it is, then it seems to suggest that $\mathbb{E}(X|Y = y)$ should be a monotonic function of $y$ over $[0,1]$, increasing from $\dfrac{1}{2}$ to $1$. I can also try to do some calculations with probability densities. From the statement of the problem, we have $f_X = 1$ and $f_{Y|X} = \dfrac{1}{x}$. Now, the joint distribution is $f_{XY} = f_X f_{Y|X} = \dfrac{1}{x}$, and as a sanity check we can verify that indeed $\displaystyle\int_0^1 \int_0^x \dfrac{1}{x} \,\mathrm{d}y \,\mathrm{d}x = 1$. Now it seems to me that $\displaystyle f_Y = \int_y^1 f_{XY} \,\mathrm{d}y = -\ln(y)$, and so we can do the following calculation: $$f_{X|Y} = \frac{f_{Y|X} f_X}{f_Y} = -\frac{1}{x \ln y}.$$ Now, I thought that I would be able to perform the following calculation to arrive at my result: $$\mathbb{E}(X|Y = y) = \int_y^1 -\frac{1}{x \ln y} \cdot x \,\mathrm{d}x = \frac{y-1}{\ln y}.$$ Is this right? If not, where have I gone wrong?","Suppose that $X$ is uniformly distributed over $[0,1]$. Now choose $X = x$ and let $Y$ be uniformly distributed over $[0,x]$. Is it possible for us to calculate the ""expected value of $X$ given $Y = y$"", i.e. , $\mathbb{E}(X|Y = y)$? Intuitively, it seems that if $y = 0$, we gain no information, and so $\mathbb{E}(X|Y = 0) = \mathbb{E}(X) = \dfrac{1}{2}$, and also that if $y = 1$, it must be the case that $x = 1$, so $\mathbb{E}(X|Y = 1) = 1$. I don't know if this reasoning is correct, but if it is, then it seems to suggest that $\mathbb{E}(X|Y = y)$ should be a monotonic function of $y$ over $[0,1]$, increasing from $\dfrac{1}{2}$ to $1$. I can also try to do some calculations with probability densities. From the statement of the problem, we have $f_X = 1$ and $f_{Y|X} = \dfrac{1}{x}$. Now, the joint distribution is $f_{XY} = f_X f_{Y|X} = \dfrac{1}{x}$, and as a sanity check we can verify that indeed $\displaystyle\int_0^1 \int_0^x \dfrac{1}{x} \,\mathrm{d}y \,\mathrm{d}x = 1$. Now it seems to me that $\displaystyle f_Y = \int_y^1 f_{XY} \,\mathrm{d}y = -\ln(y)$, and so we can do the following calculation: $$f_{X|Y} = \frac{f_{Y|X} f_X}{f_Y} = -\frac{1}{x \ln y}.$$ Now, I thought that I would be able to perform the following calculation to arrive at my result: $$\mathbb{E}(X|Y = y) = \int_y^1 -\frac{1}{x \ln y} \cdot x \,\mathrm{d}x = \frac{y-1}{\ln y}.$$ Is this right? If not, where have I gone wrong?",,"['probability', 'statistics', 'bayesian']"
82,"If you throw a fair dice $10$ times, what is the probability to throw number $6$ at most once?","If you throw a fair dice  times, what is the probability to throw number  at most once?",10 6,"If you throw a fair die $10$ times, what is the probability to throw number $6$ at most once? I thought the answer was the sum of probability to throw $6$ once in $10$ throws plus probability to throw $6$ zero times in $10$ throws: $$\frac{1}{6}\left(\frac{5}{6}\right)^9+\left(\frac{5}{6}\right)^{10}$$  Why is this not correct?","If you throw a fair die $10$ times, what is the probability to throw number $6$ at most once? I thought the answer was the sum of probability to throw $6$ once in $10$ throws plus probability to throw $6$ zero times in $10$ throws: $$\frac{1}{6}\left(\frac{5}{6}\right)^9+\left(\frac{5}{6}\right)^{10}$$  Why is this not correct?",,"['probability', 'binomial-distribution']"
83,Confusion of expectation of maximum exponential random variables,Confusion of expectation of maximum exponential random variables,,"Let $X_i \sim \mathrm{Exp}(\lambda_i)$, $i = 1,2,3$ be independent, find $\mathsf E(\max(X_i) \mid X_1<X_2<X_3)$ I have found out two solutions as follow: Solution 1 I am wondering if someone could explain this solution 1 intuitively while referring to the memoryless property of the exponential distribution. I am not able to connect them with M.L.P. Solution 2 For solution 2, can anyone explain why $\mathsf E(X_2 - X_1 \mid X_1 \lt X_2 \lt X_3)$ implies $\mathsf E(X_2 \mid X_2 \lt X_3)$, why we could drop the $X_1$. Also, why $\mathsf E(X_3 - X_2 \mid X_1 \lt X_2 \lt X3)$ implies $\mathsf E(X_3)$ and why we could drop the $X_1 \lt X_2 \lt X_3$ I know those two solutions are both connected to the memoryless property of the exponential distribution, but I couldn't get through them. Hope someone could make it clear for me. Any help will be appreciated, Thanks in advance :)","Let $X_i \sim \mathrm{Exp}(\lambda_i)$, $i = 1,2,3$ be independent, find $\mathsf E(\max(X_i) \mid X_1<X_2<X_3)$ I have found out two solutions as follow: Solution 1 I am wondering if someone could explain this solution 1 intuitively while referring to the memoryless property of the exponential distribution. I am not able to connect them with M.L.P. Solution 2 For solution 2, can anyone explain why $\mathsf E(X_2 - X_1 \mid X_1 \lt X_2 \lt X_3)$ implies $\mathsf E(X_2 \mid X_2 \lt X_3)$, why we could drop the $X_1$. Also, why $\mathsf E(X_3 - X_2 \mid X_1 \lt X_2 \lt X3)$ implies $\mathsf E(X_3)$ and why we could drop the $X_1 \lt X_2 \lt X_3$ I know those two solutions are both connected to the memoryless property of the exponential distribution, but I couldn't get through them. Hope someone could make it clear for me. Any help will be appreciated, Thanks in advance :)",,"['probability', 'stochastic-processes', 'self-learning', 'expectation', 'conditional-expectation']"
84,Expected Grade on Strange Testing Scheme,Expected Grade on Strange Testing Scheme,,"A teacher wants to give a test for a class. She gives the students 12 problems to study, 8 of which will be on the test. Students can pick 4 questions to answer and only those will be graded. If a student memorizes the answer to k problems, what is his expected grade? My teacher did this for one of our tests and I got curious. I can work out the expected grade for k < 4 with a simple binomial distribution, but I'm not sure how to do this for the other possibilities.","A teacher wants to give a test for a class. She gives the students 12 problems to study, 8 of which will be on the test. Students can pick 4 questions to answer and only those will be graded. If a student memorizes the answer to k problems, what is his expected grade? My teacher did this for one of our tests and I got curious. I can work out the expected grade for k < 4 with a simple binomial distribution, but I'm not sure how to do this for the other possibilities.",,['probability']
85,"If $3$ married couples are seated randomly in a row, then find the probability that exactly $2$ couples can sit together.","If  married couples are seated randomly in a row, then find the probability that exactly  couples can sit together.",3 2,"If $3$ married couples are seated randomly in a row, then find the probability that exactly $2$ couples can sit together. My attempt is like this:  $$\frac{4!(2!)^2}{6!}$$ My understanding: The probability of $3$ couples ($6$ people) sit in a row is 6!.  Then, if $2$ couples must sit together, there will be $4$ units in a row (i.e. $4!$). But inside those $2$ couples, they can still exchange the seat. Therefore, it will be $(2!)^2$. However, the answer is  $$3C1 \cdot \left[\frac{4!(2!)^2}{6!} - \frac{3!(2!)^3}{6!}\right]$$ I don't understand why my attempt is incorrect. Please Help. Thanks!","If $3$ married couples are seated randomly in a row, then find the probability that exactly $2$ couples can sit together. My attempt is like this:  $$\frac{4!(2!)^2}{6!}$$ My understanding: The probability of $3$ couples ($6$ people) sit in a row is 6!.  Then, if $2$ couples must sit together, there will be $4$ units in a row (i.e. $4!$). But inside those $2$ couples, they can still exchange the seat. Therefore, it will be $(2!)^2$. However, the answer is  $$3C1 \cdot \left[\frac{4!(2!)^2}{6!} - \frac{3!(2!)^3}{6!}\right]$$ I don't understand why my attempt is incorrect. Please Help. Thanks!",,['probability']
86,Bounds on conditional expectation in terms of unconditional?,Bounds on conditional expectation in terms of unconditional?,,"Suppose I am interested in calculating $\mathbb{E}(X|A)$, where $A$ is a certain fixed event and $X\geq{0}$. I am motivated by an application in which $0<P(A)=1-\varepsilon$, where $\varepsilon$ is small (i.e. conditioning on a highly probable event ). Intuitively, it seems to me that if $P(A)$ gets arbitrarily close to 1, the effect of conditioning should vanish and $\mathbb{E}(X|A)$ should get close to $\mathbb{E}X$. I can formalise this by giving an upper bound:  $$\mathbb{E}(X|A)\leq\frac{\mathbb{E}X}{P(A)}.$$ However, can a lower bound (or, alternatively, some approximation) be given?","Suppose I am interested in calculating $\mathbb{E}(X|A)$, where $A$ is a certain fixed event and $X\geq{0}$. I am motivated by an application in which $0<P(A)=1-\varepsilon$, where $\varepsilon$ is small (i.e. conditioning on a highly probable event ). Intuitively, it seems to me that if $P(A)$ gets arbitrarily close to 1, the effect of conditioning should vanish and $\mathbb{E}(X|A)$ should get close to $\mathbb{E}X$. I can formalise this by giving an upper bound:  $$\mathbb{E}(X|A)\leq\frac{\mathbb{E}X}{P(A)}.$$ However, can a lower bound (or, alternatively, some approximation) be given?",,"['probability', 'probability-theory', 'conditional-expectation']"
87,"Suppose that X  U ( $ /2$ , $/2$ ) . Find the pdf of Y = tan(X).","Suppose that X  U (  ,  ) . Find the pdf of Y = tan(X).", /2 /2,"Suppose that X  U $(/2,/2$) . Find the pdf of Y = tan(X). Make sure to define the support of the density function. My work so far:  $F_Y(y) = P(Y < y) = P (tan(x) < y) = P (x < tan^{-1}(y)) = F_X(tan^{-1}(y)) = \int_{-\pi/2}^{tan^{-1}(y)} 1/\pi = tan^{-1}(y)/\pi + 1/2 -> derivative -> 1/(\pi + \pi y^{2})$ This is the cauchy distribution. Thanks for the help in the comments!","Suppose that X  U $(/2,/2$) . Find the pdf of Y = tan(X). Make sure to define the support of the density function. My work so far:  $F_Y(y) = P(Y < y) = P (tan(x) < y) = P (x < tan^{-1}(y)) = F_X(tan^{-1}(y)) = \int_{-\pi/2}^{tan^{-1}(y)} 1/\pi = tan^{-1}(y)/\pi + 1/2 -> derivative -> 1/(\pi + \pi y^{2})$ This is the cauchy distribution. Thanks for the help in the comments!",,"['probability', 'statistics', 'probability-distributions']"
88,Homogenous vs. Non-Homogenous Poisson Processes,Homogenous vs. Non-Homogenous Poisson Processes,,"I understand that at the main difference between a homogenous vs. non-homogenous Poisson process is that a homogenous Poisson process has a constant rate parameter $\lambda$ while a non-homogenous Poisson process can have a variable rater parameter $\lambda(t)$ that is a function of time. My Question: Let's say I want to model the number of cars that arrive at an intersection in one hour, and for the first 30 minutes cars come every 5 minutes on average, while in the second 30 minutes cars come every 3 minutes on average. I know that it is certainly possible to model this process with a non-homogenous Poisson process, but could I also model this with 2 homogenous Poisson processes added together or something like that? Follow-up: One thing that I might want to answer is the probability that no arrivals happen throughout the hour. It seems intuitive to me that I could find the probability of 0 arrivals in the first 30 minutes with a standard Poisson process with rate 5, and then add this probability to the probability of 0 arrivals in the second 30 minutes with a standard Poisson process with rate 3. True?","I understand that at the main difference between a homogenous vs. non-homogenous Poisson process is that a homogenous Poisson process has a constant rate parameter $\lambda$ while a non-homogenous Poisson process can have a variable rater parameter $\lambda(t)$ that is a function of time. My Question: Let's say I want to model the number of cars that arrive at an intersection in one hour, and for the first 30 minutes cars come every 5 minutes on average, while in the second 30 minutes cars come every 3 minutes on average. I know that it is certainly possible to model this process with a non-homogenous Poisson process, but could I also model this with 2 homogenous Poisson processes added together or something like that? Follow-up: One thing that I might want to answer is the probability that no arrivals happen throughout the hour. It seems intuitive to me that I could find the probability of 0 arrivals in the first 30 minutes with a standard Poisson process with rate 5, and then add this probability to the probability of 0 arrivals in the second 30 minutes with a standard Poisson process with rate 3. True?",,"['probability', 'measure-theory', 'stochastic-processes']"
89,"Prove that if X, Y, Z independent, then X + Y and Z are independent","Prove that if X, Y, Z independent, then X + Y and Z are independent",,"Is it possible to prove that if $X, Y, Z$ independent, then $X + Y$ and $Z$ are independent? Is it even true? Intuitively it seems true but I am unsure.","Is it possible to prove that if $X, Y, Z$ independent, then $X + Y$ and $Z$ are independent? Is it even true? Intuitively it seems true but I am unsure.",,"['probability', 'probability-theory']"
90,Why isn't the probability that Alice will have classes every weekday $\dfrac{{6^5}{25 \choose 2}}{30 \choose 7}$?,Why isn't the probability that Alice will have classes every weekday ?,\dfrac{{6^5}{25 \choose 2}}{30 \choose 7},"This is a problem from Harvard Stat 110 Probability Homework set 2, and Blitzstein's Introduction to Probability (2019 2 ed) Ch 1, Exercise 54, p 51. Alice attends a small college in which each class meets only once a week. She is deciding between 30 non-overlapping classes. There are 6 classes to choose from for each day of the week, Monday through Friday. Trusting in the benevolence of randomness, Alice decides to register for 7 randomly selected classes out of the 30, with all choices equally likely. What is the probability that she will have classes every day, Monday through Friday? (This problem can be done either directly using the naive de铿nition of probability, or using inclusion-exclusion.) I know how to solve the problem (the solution is provided), but I got it wrong on the first attempt and I can't figure out why. Clearly, ${30 \choose 7}$ is a total number of ways to make a 7-class schedule. Then I need to count how many ways there are to have at least one class every day. First, Alice chooses one class on Monday through Friday, and there are $6^5$ ways to do it. After fixing these five classes, she goes on to choose other two classes among $30-5=25$ classes that are left, and there are ${25 \choose 2}$ ways to do it. Thus, by the naive definition of probability, $\frac{{6^5}{25 \choose 2}}{30 \choose 7}$ . This greatly overcounts the number of ""favorable"" schedules and yields the probability greater than 1, which is impossible. Can anyone explain where exactly my logic fails?","This is a problem from Harvard Stat 110 Probability Homework set 2, and Blitzstein's Introduction to Probability (2019 2 ed) Ch 1, Exercise 54, p 51. Alice attends a small college in which each class meets only once a week. She is deciding between 30 non-overlapping classes. There are 6 classes to choose from for each day of the week, Monday through Friday. Trusting in the benevolence of randomness, Alice decides to register for 7 randomly selected classes out of the 30, with all choices equally likely. What is the probability that she will have classes every day, Monday through Friday? (This problem can be done either directly using the naive de铿nition of probability, or using inclusion-exclusion.) I know how to solve the problem (the solution is provided), but I got it wrong on the first attempt and I can't figure out why. Clearly, is a total number of ways to make a 7-class schedule. Then I need to count how many ways there are to have at least one class every day. First, Alice chooses one class on Monday through Friday, and there are ways to do it. After fixing these five classes, she goes on to choose other two classes among classes that are left, and there are ways to do it. Thus, by the naive definition of probability, . This greatly overcounts the number of ""favorable"" schedules and yields the probability greater than 1, which is impossible. Can anyone explain where exactly my logic fails?",{30 \choose 7} 6^5 30-5=25 {25 \choose 2} \frac{{6^5}{25 \choose 2}}{30 \choose 7},"['probability', 'combinatorics']"
91,"Find the probabilities that the fourth White ball is the fourth, fifth, sixth or seventh ball drawn if the sampling is done without replacement","Find the probabilities that the fourth White ball is the fourth, fifth, sixth or seventh ball drawn if the sampling is done without replacement",,"An urn contains 10 Red and 10 White balls. The balls are drawn from the urn at random, one at a time. Find the probabilities that the fourth White ball is the fourth, fifth, sixth or seventh ball drawn if the sampling is done without replacement. Here's what I've done so far: $$\text{Let } B_k = \text{ Probability that the fourth White ball is the } k^{th} \text{ ball drawn}$$ $$P(B_4) = \frac{10 \choose 4}{20 \choose 4} = \frac{14}{323}$$ since clearly all 4 balls need to be white so are chosen from the 10 white options. Now here is where I start having a problem: For $P(B_5)$, I was thinking that obviously the fifth ball needs to be white so of the first 4 balls one can be red and the other 3 white. $$P(B_5) = \frac{{10\choose 4} {10\choose 1} {4 \choose 1}}{20 \choose 5} = \frac{175}{323}$$ but I don't think this is right. The solution I've been given is $\frac{35}{323}$ What I thought was choose 4 White and 1 Red; then of the first 4 places choose 1 to be Red (which would be the same as choosing 3 White) If I divide my answer by 5 I get the solution. I don't know why this is. I am also not very confident in my solution because I feel like there is something wrong with the ordering.","An urn contains 10 Red and 10 White balls. The balls are drawn from the urn at random, one at a time. Find the probabilities that the fourth White ball is the fourth, fifth, sixth or seventh ball drawn if the sampling is done without replacement. Here's what I've done so far: $$\text{Let } B_k = \text{ Probability that the fourth White ball is the } k^{th} \text{ ball drawn}$$ $$P(B_4) = \frac{10 \choose 4}{20 \choose 4} = \frac{14}{323}$$ since clearly all 4 balls need to be white so are chosen from the 10 white options. Now here is where I start having a problem: For $P(B_5)$, I was thinking that obviously the fifth ball needs to be white so of the first 4 balls one can be red and the other 3 white. $$P(B_5) = \frac{{10\choose 4} {10\choose 1} {4 \choose 1}}{20 \choose 5} = \frac{175}{323}$$ but I don't think this is right. The solution I've been given is $\frac{35}{323}$ What I thought was choose 4 White and 1 Red; then of the first 4 places choose 1 to be Red (which would be the same as choosing 3 White) If I divide my answer by 5 I get the solution. I don't know why this is. I am also not very confident in my solution because I feel like there is something wrong with the ordering.",,"['probability', 'combinatorics']"
92,Probability that $7^m+7^n$ is divisible by $5$,Probability that  is divisible by,7^m+7^n 5,"If $m,n$ are chosen from the first hundred natural numbers with replacement, the probability that $7^m+7^n$ is divisible by $5$ is? $$7^m+7^n=7^m(1+7^{n-m}), n\ge m$$ The above expression is divisible by $5$ only if $n-m=4k+2$. The max value of $k$ is $24$. So is the number of possibilities $48$? ($24$ for each $n>m$ and $m>n$)","If $m,n$ are chosen from the first hundred natural numbers with replacement, the probability that $7^m+7^n$ is divisible by $5$ is? $$7^m+7^n=7^m(1+7^{n-m}), n\ge m$$ The above expression is divisible by $5$ only if $n-m=4k+2$. The max value of $k$ is $24$. So is the number of possibilities $48$? ($24$ for each $n>m$ and $m>n$)",,"['probability', 'combinatorics', 'divisibility']"
93,Card Shuffling Mathematics,Card Shuffling Mathematics,,"In the paper ""Trailing the Dovetail Shuffle to its Lair"", Bayer and Diaconis give a formula for showing how many times a deck of $N$ cards has to be riffle shuffled for the deck to be considered random. The formula they came up with is $\frac32 \log_2 N$, where $N$ is the number of cards in the deck. But what if your deck only contains $2$ cards, does the formula still hold , and what does it mean for a deck of just $2$ cards to be ""random""?","In the paper ""Trailing the Dovetail Shuffle to its Lair"", Bayer and Diaconis give a formula for showing how many times a deck of $N$ cards has to be riffle shuffled for the deck to be considered random. The formula they came up with is $\frac32 \log_2 N$, where $N$ is the number of cards in the deck. But what if your deck only contains $2$ cards, does the formula still hold , and what does it mean for a deck of just $2$ cards to be ""random""?",,['probability']
94,Pairwise independence vs independence,Pairwise independence vs independence,,"Two fair dice are thrown. We have three events: A: The first die shows an odd number B: The second die shows an even number C: Both are odd or both are ven Show that $A,B,C$ are piecewise independent but not independent. My answer: $P(A) = P(B) = P(C) = \frac{1}{2}$ . $P( A \cap B) = P( A \cap C) = P( B \cap C) = \frac{1}{4}.$ This means that all the events are pairwise independent. However: $P(A \cap B \cap C) = 0$ while $P(A)P(B)P(C) = \frac{1}{8}$ , so the events are not independent. Is this correct (disregarding that I didn't explain how I got those probabilities)?","Two fair dice are thrown. We have three events: A: The first die shows an odd number B: The second die shows an even number C: Both are odd or both are ven Show that are piecewise independent but not independent. My answer: . This means that all the events are pairwise independent. However: while , so the events are not independent. Is this correct (disregarding that I didn't explain how I got those probabilities)?","A,B,C P(A) = P(B) = P(C) = \frac{1}{2} P( A \cap B) = P( A \cap C) = P( B \cap C) = \frac{1}{4}. P(A \cap B \cap C) = 0 P(A)P(B)P(C) = \frac{1}{8}",['probability']
95,Ehrenfest Chain: stationary distribution,Ehrenfest Chain: stationary distribution,,"In the Ehrenfest Chain model: There are M balls which are divided between urn A and urn B. At each stage, if a ball is chosen, then it would be moved into a different urn. Let $X_n$ be the # of balls in urn A, then the transistion is $P_{i,i+1}=1-i/m$, $P_{i,i-1}=i/m$. It turns out that the stationary distribution is $\pi_j=$$M\choose j$$2^{-M}$. The textbook claims this is quit intuitive. I don't understand what is ""by symmetry, it is equally likely to be in either urn"". This, however, is quite intuitive, for if we focus on any one ball, it   becomes quite clear that its position will be independent of the   positions of the other balls (since no matter where the other M  1   balls are, the ball under consideration at each stage will be moved   with probability 1/M) and by symmetry, it is equally likely to be in   either urn.","In the Ehrenfest Chain model: There are M balls which are divided between urn A and urn B. At each stage, if a ball is chosen, then it would be moved into a different urn. Let $X_n$ be the # of balls in urn A, then the transistion is $P_{i,i+1}=1-i/m$, $P_{i,i-1}=i/m$. It turns out that the stationary distribution is $\pi_j=$$M\choose j$$2^{-M}$. The textbook claims this is quit intuitive. I don't understand what is ""by symmetry, it is equally likely to be in either urn"". This, however, is quite intuitive, for if we focus on any one ball, it   becomes quite clear that its position will be independent of the   positions of the other balls (since no matter where the other M  1   balls are, the ball under consideration at each stage will be moved   with probability 1/M) and by symmetry, it is equally likely to be in   either urn.",,"['probability', 'probability-theory', 'probability-distributions', 'markov-chains', 'markov-process']"
96,Upper bound for difference of Poisson random variables,Upper bound for difference of Poisson random variables,,"Let $X, Y$ be random variables with Poisson$(\lambda)$ and Poisson$(2\lambda)$ distributions, respectively.Then (i) If we assume that $X, Y$ are independent, $$\mathbb{P}(X \geq Y) \leq e^{-(3-\sqrt8)\lambda}.$$ (ii) Now, assume that $X, Y$ are not independent, then there exist $A, c >0$,  both not depending on $\lambda$, such that $$\mathbb{P}(X \geq Y) \leq Ae^{-c\lambda}.$$ I have no idea how to solve this problems. I try setting $Z = X - Y$, but I do not know how to find the upper bound, and how that $e^{-(3 - \sqrt8)\lambda}$ comes from. Note: $X$ has a Poisson$(\lambda)$ distribution if, for any integer $j\geq 0$, $$\mathbb{P}(X = j) = \frac{\lambda^je^{-j}}{j!}.$$","Let $X, Y$ be random variables with Poisson$(\lambda)$ and Poisson$(2\lambda)$ distributions, respectively.Then (i) If we assume that $X, Y$ are independent, $$\mathbb{P}(X \geq Y) \leq e^{-(3-\sqrt8)\lambda}.$$ (ii) Now, assume that $X, Y$ are not independent, then there exist $A, c >0$,  both not depending on $\lambda$, such that $$\mathbb{P}(X \geq Y) \leq Ae^{-c\lambda}.$$ I have no idea how to solve this problems. I try setting $Z = X - Y$, but I do not know how to find the upper bound, and how that $e^{-(3 - \sqrt8)\lambda}$ comes from. Note: $X$ has a Poisson$(\lambda)$ distribution if, for any integer $j\geq 0$, $$\mathbb{P}(X = j) = \frac{\lambda^je^{-j}}{j!}.$$",,"['probability', 'probability-theory', 'probability-distributions']"
97,What is the probability that a randomly chosen positive three-digit integer is a multiple of $7$?,What is the probability that a randomly chosen positive three-digit integer is a multiple of ?,7,"What is the probability that a randomly chosen positive three-digit integer is a multiple of $7$ ? Is my answer right?: $$\frac{100}{7} = 14 , \qquad \frac{999}{7} = 142$$ Then there are $142 - 14 = 128$ numbers that are multiples of $7$ . Then the probability is: $\frac{128}{900}$ .",What is the probability that a randomly chosen positive three-digit integer is a multiple of ? Is my answer right?: Then there are numbers that are multiples of . Then the probability is: .,"7 \frac{100}{7} = 14 , \qquad \frac{999}{7} = 142 142 - 14 = 128 7 \frac{128}{900}",['probability']
98,Defining the states when we roll one single die repeatedly,Defining the states when we roll one single die repeatedly,,"We roll a single die and the game stops as soon as the sum of two successive rolls is either 5 or 7.   We want to find the probability that the game stops at a sum of 5. It seems like Markov chain with first-step analysis. To find the transition matrix, I first need to define the states. They way I define it is that if we have (1,2) then next state should be (2,x) for x=1,2,3,4,5,6. And (1,2) is different from (2,1). So, there must be 36 states?","We roll a single die and the game stops as soon as the sum of two successive rolls is either 5 or 7.   We want to find the probability that the game stops at a sum of 5. It seems like Markov chain with first-step analysis. To find the transition matrix, I first need to define the states. They way I define it is that if we have (1,2) then next state should be (2,x) for x=1,2,3,4,5,6. And (1,2) is different from (2,1). So, there must be 36 states?",,"['probability', 'stochastic-processes', 'markov-chains']"
99,Who has revealed more about a secret password?,Who has revealed more about a secret password?,,"Today, Bob, a colleague of mine, accidentally revealed that his password contains a . Alice laughed, but then also inadvertently said her password does not contain any a . Who has given away more about his/her password? In other words, whose password is now easier to guess? EDIT: I have purposedly not provided information about the length of any password, or even maximum length. One can assume the maximum length to be unbounded. I.e. let the variable m representing the maximum password length in any formulae, tend towards infiny. EDIT2: I picked ""Alice"" and ""Bob"" as a homage/reference to the homework problems in my undergraduate course's books. But this really happened. A colleague of mine has a defective a in his keyboard. EDIT3: Here's my stab at the question that I arrived at with my colleagues over lunch yesterday: Let's write the formulae for the number of possible passwords given an alphabet of integer size $\alpha$ and a maximum password length of $m$ $\sum_{n=1}^m \alpha^n$ This I arrived at by summing ""$\alpha$ passwords of length 1"" + ""$\alpha^2$ passwords of length 2"" + ... + ""$\alpha^m$ passwords of length $m$"". Now, Alice has revealed she doesn't have an a anywhere in her password. She has reduced the alphabet size to the attacker by 1. So the total number of passwords for her is: $N_{Alice}=\sum_{n=1}^m (\alpha-1)^n$ Bob's total number of possible passwords has also decreased: $N_{Bob}=\sum_{n=1}^m \alpha^n -\sum_{n=1}^m (\alpha-1)^n$ In other words, $N_B$ has decreased exactly the same amount as the number of possible passwords for Alice, which have the letter a . Now, if $m$ is [the totally unrealistic value of] 1, Alice has clearly given away her password completely, while Bob still has $\alpha - 1$ total passwords. But, as an intuition as $m$ increases towards infinity, I think $N_{Alice} << N_{Bob}$, so I think it is Alice who is now more likely to have his password brute forced!! My calculus is a bit shaky :-) so I do not know yet if it is true that $N_{Alice} << N_{Bob}$ as $m$ tends towards infinity. If this question is reopened, perhaps someone can help me. EDIT4: Trial-and-error in a simple program seems to confirm my intuition. For an alphabet size of 26, if the maximum password length is 17, Bob has given away more. If it is 18, he wins (Alice has given away more).","Today, Bob, a colleague of mine, accidentally revealed that his password contains a . Alice laughed, but then also inadvertently said her password does not contain any a . Who has given away more about his/her password? In other words, whose password is now easier to guess? EDIT: I have purposedly not provided information about the length of any password, or even maximum length. One can assume the maximum length to be unbounded. I.e. let the variable m representing the maximum password length in any formulae, tend towards infiny. EDIT2: I picked ""Alice"" and ""Bob"" as a homage/reference to the homework problems in my undergraduate course's books. But this really happened. A colleague of mine has a defective a in his keyboard. EDIT3: Here's my stab at the question that I arrived at with my colleagues over lunch yesterday: Let's write the formulae for the number of possible passwords given an alphabet of integer size $\alpha$ and a maximum password length of $m$ $\sum_{n=1}^m \alpha^n$ This I arrived at by summing ""$\alpha$ passwords of length 1"" + ""$\alpha^2$ passwords of length 2"" + ... + ""$\alpha^m$ passwords of length $m$"". Now, Alice has revealed she doesn't have an a anywhere in her password. She has reduced the alphabet size to the attacker by 1. So the total number of passwords for her is: $N_{Alice}=\sum_{n=1}^m (\alpha-1)^n$ Bob's total number of possible passwords has also decreased: $N_{Bob}=\sum_{n=1}^m \alpha^n -\sum_{n=1}^m (\alpha-1)^n$ In other words, $N_B$ has decreased exactly the same amount as the number of possible passwords for Alice, which have the letter a . Now, if $m$ is [the totally unrealistic value of] 1, Alice has clearly given away her password completely, while Bob still has $\alpha - 1$ total passwords. But, as an intuition as $m$ increases towards infinity, I think $N_{Alice} << N_{Bob}$, so I think it is Alice who is now more likely to have his password brute forced!! My calculus is a bit shaky :-) so I do not know yet if it is true that $N_{Alice} << N_{Bob}$ as $m$ tends towards infinity. If this question is reopened, perhaps someone can help me. EDIT4: Trial-and-error in a simple program seems to confirm my intuition. For an alphabet size of 26, if the maximum password length is 17, Bob has given away more. If it is 18, he wins (Alice has given away more).",,"['probability', 'combinatorics', 'cryptography']"
