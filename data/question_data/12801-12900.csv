,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Integral: $\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx$ for $a,b>0$",Integral:  for,"\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx a,b>0","I tried this: $$\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx=\Re\left(\int_0^{\infty} e^{-ib^2x^2+ia^2/x^2}\,dx\right)=\Re\left(\int_0^{\infty} e^{-\left(ib^2x^2+i^3a^2/x^2\right)}\,dx\right)$$ Sometime back, I stumbled upon the following result: $$\int_0^{\infty} e^{-\left(p^2x^2+m^2/x^2\right)}\,dx=\frac{\sqrt{\pi}}{2p}e^{-2pm}$$ Replacing $p$ with $i^{1/2}b$ and $m$ with $i^{3/2}a$, I get: $$\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx=\Re\left(\frac{1}{2b}\sqrt{\frac{\pi}{i}}e^{2ab}\right)$$ But this is supposed to be incorrect and I do not see where did I go wrong. Any help is greatly appreciated. Thanks! (I do know that this is easily doable using contour integration but I would like to know what's wrong with above)","I tried this: $$\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx=\Re\left(\int_0^{\infty} e^{-ib^2x^2+ia^2/x^2}\,dx\right)=\Re\left(\int_0^{\infty} e^{-\left(ib^2x^2+i^3a^2/x^2\right)}\,dx\right)$$ Sometime back, I stumbled upon the following result: $$\int_0^{\infty} e^{-\left(p^2x^2+m^2/x^2\right)}\,dx=\frac{\sqrt{\pi}}{2p}e^{-2pm}$$ Replacing $p$ with $i^{1/2}b$ and $m$ with $i^{3/2}a$, I get: $$\int_0^{\infty} \cos\left(\frac{a^2}{x^2}-b^2x^2\right)\,dx=\Re\left(\frac{1}{2b}\sqrt{\frac{\pi}{i}}e^{2ab}\right)$$ But this is supposed to be incorrect and I do not see where did I go wrong. Any help is greatly appreciated. Thanks! (I do know that this is easily doable using contour integration but I would like to know what's wrong with above)",,"['calculus', 'integration', 'definite-integrals']"
1,Does the change of variable function have to be injective?,Does the change of variable function have to be injective?,,"Please note that I'm only interested in the one-variable case here. The change of variables formula for integration is: $$\int^{\phi(b)}_{\phi(a)}f(x)\ \text{d}x= \int^b_a f(\phi(x))\phi'(x)\ \text{d}x $$ Where $\phi$ and $f$ are sufficiently nice (I suppose $f$ has to be integrable and I think $\phi$ needs to be continuously differentiable). However, my Analysis teacher once mentioned to me that $\phi$ has to be injective as well. But I can't find any statements of the theorem (in one variable) that include this condition. It makes sense to me that it wouldn't be necessary: thinking about Riemann sums, if $\phi$ is non-monotonic, then the subdivision will ""backtrack"" at some point, but since we're multiplying by the derivative, those rectangles will be negative, so it's plausible that they would cancel out in such a way that we don't ""double count"" that area.","Please note that I'm only interested in the one-variable case here. The change of variables formula for integration is: $$\int^{\phi(b)}_{\phi(a)}f(x)\ \text{d}x= \int^b_a f(\phi(x))\phi'(x)\ \text{d}x $$ Where $\phi$ and $f$ are sufficiently nice (I suppose $f$ has to be integrable and I think $\phi$ needs to be continuously differentiable). However, my Analysis teacher once mentioned to me that $\phi$ has to be injective as well. But I can't find any statements of the theorem (in one variable) that include this condition. It makes sense to me that it wouldn't be necessary: thinking about Riemann sums, if $\phi$ is non-monotonic, then the subdivision will ""backtrack"" at some point, but since we're multiplying by the derivative, those rectangles will be negative, so it's plausible that they would cancel out in such a way that we don't ""double count"" that area.",,"['calculus', 'integration']"
2,Difference between parentheses and angle brackets in vector notation,Difference between parentheses and angle brackets in vector notation,,"In my calculus class we used angle brackets to describe vectors, $\langle a, b, c\rangle $. But in my linear algebra class we use parenthesis.  I've read here the angle brackets are for inner products but in calc we've used them generally, not necessarily when computing a product; we've said u $= \langle a,b,c\rangle $.  When are parentheses or angle brackets used properly?","In my calculus class we used angle brackets to describe vectors, $\langle a, b, c\rangle $. But in my linear algebra class we use parenthesis.  I've read here the angle brackets are for inner products but in calc we've used them generally, not necessarily when computing a product; we've said u $= \langle a,b,c\rangle $.  When are parentheses or angle brackets used properly?",,"['calculus', 'linear-algebra', 'notation', 'vectors']"
3,Integrating each side of an equation w.r.t. to a different variable?,Integrating each side of an equation w.r.t. to a different variable?,,"Say I have $\frac{dy}{dt} = a$, for some constant $a$ So $dy = a dt$ $\int_{y0}^y dy = \int_{t0}^t a dt$ $y - y_0 = at - at_0$ How come I am allowed to integrate each side with respect to a different variable? If I had an equation $y = 5x$ and I differentiated the LHS w.r.t. to y, and the RHS w.r.t. x I would get $1 = 5$...so differentiating both sides w.r.t. to different variables doesn't work. Yet integrating does?","Say I have $\frac{dy}{dt} = a$, for some constant $a$ So $dy = a dt$ $\int_{y0}^y dy = \int_{t0}^t a dt$ $y - y_0 = at - at_0$ How come I am allowed to integrate each side with respect to a different variable? If I had an equation $y = 5x$ and I differentiated the LHS w.r.t. to y, and the RHS w.r.t. x I would get $1 = 5$...so differentiating both sides w.r.t. to different variables doesn't work. Yet integrating does?",,"['calculus', 'integration']"
4,"Volumes of cones, spheres, and cylinders","Volumes of cones, spheres, and cylinders",,"Given a sphere with radius r, a cone with radius r and height 2r, and a cylinder with radius r and height 2r, the sum of the volume of the cone and sphere is equal to the volume of the cylinder. If we look at the volume formulas, this is obvious. However, any ordinary person without mathematical training probably wouldn't find this intuitive. I recall reading in a museum exhibit that before proving anything, Archimedes was able to slice up the sphere and cone and fit the pieces together into the cylinder--all in his mind. Can someone explain how one can slice up the shapes to do that?","Given a sphere with radius r, a cone with radius r and height 2r, and a cylinder with radius r and height 2r, the sum of the volume of the cone and sphere is equal to the volume of the cylinder. If we look at the volume formulas, this is obvious. However, any ordinary person without mathematical training probably wouldn't find this intuitive. I recall reading in a museum exhibit that before proving anything, Archimedes was able to slice up the sphere and cone and fit the pieces together into the cylinder--all in his mind. Can someone explain how one can slice up the shapes to do that?",,"['calculus', 'geometry', 'math-history']"
5,"Do Vector Calculus Cartesian coordinates identities with Div, Grad, Curl hold in cylindrical and spherical coordinates?","Do Vector Calculus Cartesian coordinates identities with Div, Grad, Curl hold in cylindrical and spherical coordinates?",,"These operators are written in different forms in Cartesian, cylindrical and spherical coordinates. For instance, in spherical coordinate system, one has $$\nabla \cdot \overrightarrow{F}=\frac{1}{r^{2}}\frac{\partial }{\partial r} \left( r^{2}F_{r}\right) +\frac{1}{r\sin \theta }\frac{\partial }{\partial \theta }\left( \sin \theta \cdot F_{\theta }\right) +\frac{1}{r\sin \theta } \frac{\partial F_{\varphi }}{\partial \varphi }.$$ Question : Do identities such as $$\nabla \cdot \left( \overrightarrow{A}\times \overrightarrow{B}\right) =% \overrightarrow{B}\cdot \nabla \times \overrightarrow{A}-\overrightarrow{A}% \cdot \nabla \times \overrightarrow{B}$$ hold in general when cylindrical and spherical coordinate systems are used or do they have to be adapted? Added : After having read the comments it occured to me that the invariance of these identities with regard to the coordinate system is a consequence of the definitions of the mentioned operators in terms of integrals, e.g.: $$\nabla \cdot \overrightarrow{F}=\underset{V\rightarrow 0}{\lim }\frac{1}{V}% \underset{S}{\int \int }\overrightarrow{F}\cdot \overrightarrow{n}\;dA$$ where $V$ is the volume of a bounded closed region $T$, $S$ is the surface of $T$, and $\overrightarrow{n}$ the unit outer normal vector to $S$.","These operators are written in different forms in Cartesian, cylindrical and spherical coordinates. For instance, in spherical coordinate system, one has $$\nabla \cdot \overrightarrow{F}=\frac{1}{r^{2}}\frac{\partial }{\partial r} \left( r^{2}F_{r}\right) +\frac{1}{r\sin \theta }\frac{\partial }{\partial \theta }\left( \sin \theta \cdot F_{\theta }\right) +\frac{1}{r\sin \theta } \frac{\partial F_{\varphi }}{\partial \varphi }.$$ Question : Do identities such as $$\nabla \cdot \left( \overrightarrow{A}\times \overrightarrow{B}\right) =% \overrightarrow{B}\cdot \nabla \times \overrightarrow{A}-\overrightarrow{A}% \cdot \nabla \times \overrightarrow{B}$$ hold in general when cylindrical and spherical coordinate systems are used or do they have to be adapted? Added : After having read the comments it occured to me that the invariance of these identities with regard to the coordinate system is a consequence of the definitions of the mentioned operators in terms of integrals, e.g.: $$\nabla \cdot \overrightarrow{F}=\underset{V\rightarrow 0}{\lim }\frac{1}{V}% \underset{S}{\int \int }\overrightarrow{F}\cdot \overrightarrow{n}\;dA$$ where $V$ is the volume of a bounded closed region $T$, $S$ is the surface of $T$, and $\overrightarrow{n}$ the unit outer normal vector to $S$.",,"['calculus', 'physics', 'multivariable-calculus']"
6,Show that $\sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k^2} \sum_{n=1}^k \frac{1}{n}=\frac{5\zeta(3)}{8}$,Show that,\sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k^2} \sum_{n=1}^k \frac{1}{n}=\frac{5\zeta(3)}{8},$$\sum_{k=1}^{\infty} \dfrac{(-1)^{k+1}}{k^2} \sum_{n=1}^k \dfrac{1}{n}=\frac{5\zeta(3)}{8}$$ I tried to create a proof from some lemmas some are suggested by my Senior friends Lemma 1 $$ {H_n} = \sum\limits_{k = 1}^\infty  {\left( {\frac{1}{k} - \frac{1}{{n + k}}} \right)}   = \sum\limits_{k = 1}^\infty  {\frac{n}{{k \cdot \left( {n + k} \right)}}} $$ Lemma 2 $$ \begin{split} \sum\limits_{n = 1}^\infty  {\frac{{{x^{2n - 1}}}}{{2n - 1}}}  &= \sum\limits_{n = 1}^\infty  {\frac{{{x^n}}}{n}}  - \sum\limits_{n = 1}^\infty  {\frac{{{x^{2n}}}}{{2n}}}  - \log \left( {1 - x} \right) - \frac{1}{2}\sum\limits_{n = 1}^\infty  {\frac{{{x^{2n}}}}{n}}  \\ &= \frac{1}{2} \cdot \log \left( {1 - {x^2}} \right) - \log \left( {1 - x} \right) \\ &= \frac{1}{2} \cdot \log \frac{{1 + x}}{{1 - x}} \end{split} $$ Lemma 3 $$ \begin{split} \int\limits_0^1 {\frac{{\log }^2 x}{1 - x}dx}   & = \int\limits_0^1 {{{\log }^2}x \cdot \sum\limits_{n = 0}^\infty  {{x^n}}   dx}  \\ &= \sum\limits_{n = 0}^\infty  {\int\limits_0^1 {{{\log }^2}x \cdot {x^n}  dx} }  \\ &= 2 \cdot \sum\limits_{n = 0}^\infty  {\frac{1}{{{{\left( {n + 1} \right)}^3}}}}  = 2 \cdot \zeta \left( 3 \right)  \end{split}$$ Lemma 4 $$ \begin{split} \int\limits_0^1 {\frac{{{{\log }^2}x}}{{1 + x}} dx} & = \int\limits_0^1 {{{\log }^2}x \cdot \sum\limits_{n = 0}^\infty  {{{\left( { - 1} \right)}^n} \cdot {x^n}} dx}  \\ & = \sum\limits_{n = 0}^\infty  {{{\left( { - 1} \right)}^n} \cdot \int\limits_0^1 {{{\log }^2}x \cdot {x^n} dx} }  \\ & = 2 \cdot \sum\limits_{n = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^n} \cdot }}{{{{\left( {n + 1} \right)}^3}}}}  = \frac{3}{2} \cdot \zeta \left( 3 \right) \end{split}$$,I tried to create a proof from some lemmas some are suggested by my Senior friends Lemma 1 Lemma 2 Lemma 3 Lemma 4,"\sum_{k=1}^{\infty} \dfrac{(-1)^{k+1}}{k^2} \sum_{n=1}^k \dfrac{1}{n}=\frac{5\zeta(3)}{8} 
{H_n} = \sum\limits_{k = 1}^\infty  {\left( {\frac{1}{k} - \frac{1}{{n + k}}} \right)}   = \sum\limits_{k = 1}^\infty  {\frac{n}{{k \cdot \left( {n + k} \right)}}}  
\begin{split}
\sum\limits_{n = 1}^\infty  {\frac{{{x^{2n - 1}}}}{{2n - 1}}}  &= \sum\limits_{n = 1}^\infty  {\frac{{{x^n}}}{n}}  - \sum\limits_{n = 1}^\infty  {\frac{{{x^{2n}}}}{{2n}}}  - \log \left( {1 - x} \right) - \frac{1}{2}\sum\limits_{n = 1}^\infty  {\frac{{{x^{2n}}}}{n}}  \\
&= \frac{1}{2} \cdot \log \left( {1 - {x^2}} \right) - \log \left( {1 - x} \right) \\
&= \frac{1}{2} \cdot \log \frac{{1 + x}}{{1 - x}}
\end{split}
 
\begin{split}
\int\limits_0^1 {\frac{{\log }^2 x}{1 - x}dx}   & = \int\limits_0^1 {{{\log }^2}x \cdot \sum\limits_{n = 0}^\infty  {{x^n}}   dx} 
\\
&= \sum\limits_{n = 0}^\infty  {\int\limits_0^1 {{{\log }^2}x \cdot {x^n}  dx} }  \\
&= 2 \cdot \sum\limits_{n = 0}^\infty  {\frac{1}{{{{\left( {n + 1} \right)}^3}}}}  = 2 \cdot \zeta \left( 3 \right) 
\end{split} 
\begin{split}
\int\limits_0^1 {\frac{{{{\log }^2}x}}{{1 + x}} dx} & = \int\limits_0^1 {{{\log }^2}x \cdot \sum\limits_{n = 0}^\infty  {{{\left( { - 1} \right)}^n} \cdot {x^n}} dx}  \\
& = \sum\limits_{n = 0}^\infty  {{{\left( { - 1} \right)}^n} \cdot \int\limits_0^1 {{{\log }^2}x \cdot {x^n} dx} }  \\
& = 2 \cdot \sum\limits_{n = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^n} \cdot }}{{{{\left( {n + 1} \right)}^3}}}}  = \frac{3}{2} \cdot \zeta \left( 3 \right)
\end{split}","['calculus', 'sequences-and-series']"
7,let $f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}$ prove that $\lim\limits_{x \to \infty}f(x) = \infty$.,let  prove that .,f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)} \lim\limits_{x \to \infty}f(x) = \infty,"Let $f:(0,\infty)\to\mathbb R$ be a differentiable function such that $$f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}$$ for all $x\gt1$ . Prove that $\lim\limits_{x \to \infty}f(x) = \infty$ . Here is what i thought: $$f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}=\frac{1-\frac{f(x)^2}{x^2}}{f(x)^2+1}$$ if somehow it can be proved that $\frac{f(x)^2}{x^2}$ lies in range $(-1,1)$ then it can be concluded that $f'(x)\gt0$ , which means function is strictly increasing and for increasing function $\lim\limits_{x \to \infty}f(x) = \infty$ is always true. But am not sure how to do that and if some one can come up with other creative solution that would be great! and also please explain thought process behind your given solution. EDIT : My assertion that increasing function tends to limit $\infty$ is wrong for some cases, for an example in case of $f(x)=\arctan(x)$ . But it can be explained since derivative of $\arctan(x)$ is $\frac{1}{1+x^2}$ and $\lim\limits_{x \to \infty}\frac{1}{1+x^2}=0$ which means, if we can prove other condition which is $\lim\limits_{x \to \infty}f'(x)\ne0$ then it would be complete proof.","Let be a differentiable function such that for all . Prove that . Here is what i thought: if somehow it can be proved that lies in range then it can be concluded that , which means function is strictly increasing and for increasing function is always true. But am not sure how to do that and if some one can come up with other creative solution that would be great! and also please explain thought process behind your given solution. EDIT : My assertion that increasing function tends to limit is wrong for some cases, for an example in case of . But it can be explained since derivative of is and which means, if we can prove other condition which is then it would be complete proof.","f:(0,\infty)\to\mathbb R f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)} x\gt1 \lim\limits_{x \to \infty}f(x) = \infty f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}=\frac{1-\frac{f(x)^2}{x^2}}{f(x)^2+1} \frac{f(x)^2}{x^2} (-1,1) f'(x)\gt0 \lim\limits_{x \to \infty}f(x) = \infty \infty f(x)=\arctan(x) \arctan(x) \frac{1}{1+x^2} \lim\limits_{x \to \infty}\frac{1}{1+x^2}=0 \lim\limits_{x \to \infty}f'(x)\ne0","['calculus', 'limits', 'derivatives']"
8,Difficult Laplace Transform Type of Integral,Difficult Laplace Transform Type of Integral,,"Good afternoon.  I have the following integral that I need help integrating; $$ \mathrm{F}\left(x\right) = \int_{0}^{\infty}\mathrm{e}^{s\left(j - 1/x\right)}\, \left[\mathrm{T}_{N}\left(s\right)\right]^{k - j}\,\mathrm{d}s $$ where $\mathrm{T}_{N}\left(s\right)$ is the truncated exponential function $$ \mathrm{T}_{N}\left(s\right) = \sum_{n = 0}^{N}\frac{s^{n}}{n!} $$ and $j,k$ are whole numbers. I figured that tackling this using integration by parts is the best choice, but naturally this could take a while given that my choice of $u_{1}=\mathrm{T}_{N}^{k - j}\left(s\right)$ yields a $\mathrm{d}u_{1}$ of $$ \mathrm{d}u_{1} = \left(k - j\right)\mathrm{T}_{N}^{k-j-1}\left(s\right) \mathrm{T}_{N - 1}\left(s\right)\,\mathrm{d}s $$ since $\mathrm{d}\mathrm{T}_{N}\left(s\right)/\mathrm{d}s = \mathrm{T}_{N - 1}\left(s\right)$ .  This does not seem to be simplifying the problem though as the next iteration would require me to then choose my $u_{2}$ to be $$ u_{2} = \frac{1}{k - j}\,\frac{\mathrm{d}u_{1}}{\mathrm{d}s} = \mathrm{T}_{N}^{k - j - 1}\left(s\right) \mathrm{T}_{N - 1}\left(s\right) $$ Are there any other approaches to this without this drawn out IBP method or without considering expanding the truncated exponential ?.","Good afternoon.  I have the following integral that I need help integrating; where is the truncated exponential function and are whole numbers. I figured that tackling this using integration by parts is the best choice, but naturally this could take a while given that my choice of yields a of since .  This does not seem to be simplifying the problem though as the next iteration would require me to then choose my to be Are there any other approaches to this without this drawn out IBP method or without considering expanding the truncated exponential ?.","
\mathrm{F}\left(x\right) =
\int_{0}^{\infty}\mathrm{e}^{s\left(j - 1/x\right)}\,
\left[\mathrm{T}_{N}\left(s\right)\right]^{k - j}\,\mathrm{d}s
 \mathrm{T}_{N}\left(s\right) 
\mathrm{T}_{N}\left(s\right) = \sum_{n = 0}^{N}\frac{s^{n}}{n!}
 j,k u_{1}=\mathrm{T}_{N}^{k - j}\left(s\right) \mathrm{d}u_{1} 
\mathrm{d}u_{1} =
\left(k - j\right)\mathrm{T}_{N}^{k-j-1}\left(s\right)
\mathrm{T}_{N - 1}\left(s\right)\,\mathrm{d}s
 \mathrm{d}\mathrm{T}_{N}\left(s\right)/\mathrm{d}s = \mathrm{T}_{N - 1}\left(s\right) u_{2} 
u_{2} =
\frac{1}{k - j}\,\frac{\mathrm{d}u_{1}}{\mathrm{d}s} =
\mathrm{T}_{N}^{k - j - 1}\left(s\right)
\mathrm{T}_{N - 1}\left(s\right)
","['calculus', 'integration', 'integral-transforms']"
9,Proving $\int_0^1 \frac{(\ln(x))^5}{1+x} \mathrm{d}x = -\frac{31\pi^6}{252}$,Proving,\int_0^1 \frac{(\ln(x))^5}{1+x} \mathrm{d}x = -\frac{31\pi^6}{252},"I would like to show the following identity: $$\boxed{ I :=  \int_0^1 \dfrac{(\ln(x))^5}{1+x} \mathrm{d}x = -\dfrac{31\pi^6}{252} }$$ Here is what I tried. The change of variables $u=1/x$ yields $$I= \int_1^{\infty} \dfrac{(\ln(x))^5}{1+1/u} \dfrac{1}{u^2} \mathrm{d}u = \int_1^{\infty} \dfrac{(\ln(x))^5}{u^2+u} \mathrm{d}u$$ Then $z=u-1$ gives $$I = \int_{0}^{\infty} \dfrac{(\ln(z+1))^5}{z^2+3z+2} \mathrm{d}z $$ with $z^2+3z+2=(z+1)(z+2)$. I wanted to use contour integration like here , but I was not sure how to proceed in this case. Anyway, the computations of the residues (of which ""well-chosen"" function? Maybe something like this ?) seem to be difficult. I believe that we can generalize to $\frac{(\ln(x))^n}{1+x}$, or maybe even more (e.g. $\frac{(\ln(x))^n}{ax^2+bx+c}$). Related computations are: (1) , (2) , (3) , (4) . Thank you for your detailed help.","I would like to show the following identity: $$\boxed{ I :=  \int_0^1 \dfrac{(\ln(x))^5}{1+x} \mathrm{d}x = -\dfrac{31\pi^6}{252} }$$ Here is what I tried. The change of variables $u=1/x$ yields $$I= \int_1^{\infty} \dfrac{(\ln(x))^5}{1+1/u} \dfrac{1}{u^2} \mathrm{d}u = \int_1^{\infty} \dfrac{(\ln(x))^5}{u^2+u} \mathrm{d}u$$ Then $z=u-1$ gives $$I = \int_{0}^{\infty} \dfrac{(\ln(z+1))^5}{z^2+3z+2} \mathrm{d}z $$ with $z^2+3z+2=(z+1)(z+2)$. I wanted to use contour integration like here , but I was not sure how to proceed in this case. Anyway, the computations of the residues (of which ""well-chosen"" function? Maybe something like this ?) seem to be difficult. I believe that we can generalize to $\frac{(\ln(x))^n}{1+x}$, or maybe even more (e.g. $\frac{(\ln(x))^n}{ax^2+bx+c}$). Related computations are: (1) , (2) , (3) , (4) . Thank you for your detailed help.",,"['calculus', 'integration', 'definite-integrals', 'contour-integration', 'closed-form']"
10,PDF of the difference between two independent beta random variables,PDF of the difference between two independent beta random variables,,"I am having trouble deriving the distribution of the difference of two beta random variables and would like some help verifying the steps I have taken.  In particular calculating the bounds. Say I have $X_1\sim\text{Beta}(a_1,b_1)$ and $X_2\sim\text{Beta}(a_2,b_2)$, independent, and am interested in calculating the distribution of $X_1-X_2$. So here is what I have come up with so far: Let $Z=X_1-X_2$ and $W=X_1$ where $0\leq X_i\leq 1$ for $i=1,2$. So $X_1=W$ and $X2 = W-Z$. Likewise $\frac{dX_1}{dW}=1$, $\frac{dX_1}{dZ}=0$, $\frac{dX_2}{dW}=1$, and $\frac{dX_2}{dZ}=-1$. Then the determinant of the Jacobian would be $|J| = 0\times1 - (-1)\times1=1 $ Then we have that \begin{align} f_{Z,W}(z,w) &=f_{X_1,X_2}(J_1(z,w),J_2(z,w))\times|J|\\ &=f_{X_1,X_2}(w,z-w)\\ &=f_{X_1}(w) f_{X_2}(z-w)\\ &=\text{Beta}(w;a_1,b_1)\times\text{Beta}(w-z;a_2,b_2)\\ &=\frac{(w)^{1-a_1}(1-w)^{1-b_1}}{\beta(a_1,b_1)}\times\frac{(w-z)^{1-a_2}(1-(w-z))^{1-b_2}}{\beta(a_2,b_2)} \end{align} From there I could integrate out $W$ from $f_{Z,W}(z,w)$ to get the quantity of interest, i.e., the distribution of $Z=X_1-X_2$. So now this is where I am stuck.  I have the following: $$f_Z(z)=\int f_{Z,W}(z,w)dw=\int \text{Beta}(w;a_1,b_1)\times\text{Beta}(w-z;a_2,b_2) dw$$ But I do not understand how to obtain the bounds for the integral, and if the integral needs to be broken into parts or not.  Let me know also if any of the above steps are incorrect.","I am having trouble deriving the distribution of the difference of two beta random variables and would like some help verifying the steps I have taken.  In particular calculating the bounds. Say I have $X_1\sim\text{Beta}(a_1,b_1)$ and $X_2\sim\text{Beta}(a_2,b_2)$, independent, and am interested in calculating the distribution of $X_1-X_2$. So here is what I have come up with so far: Let $Z=X_1-X_2$ and $W=X_1$ where $0\leq X_i\leq 1$ for $i=1,2$. So $X_1=W$ and $X2 = W-Z$. Likewise $\frac{dX_1}{dW}=1$, $\frac{dX_1}{dZ}=0$, $\frac{dX_2}{dW}=1$, and $\frac{dX_2}{dZ}=-1$. Then the determinant of the Jacobian would be $|J| = 0\times1 - (-1)\times1=1 $ Then we have that \begin{align} f_{Z,W}(z,w) &=f_{X_1,X_2}(J_1(z,w),J_2(z,w))\times|J|\\ &=f_{X_1,X_2}(w,z-w)\\ &=f_{X_1}(w) f_{X_2}(z-w)\\ &=\text{Beta}(w;a_1,b_1)\times\text{Beta}(w-z;a_2,b_2)\\ &=\frac{(w)^{1-a_1}(1-w)^{1-b_1}}{\beta(a_1,b_1)}\times\frac{(w-z)^{1-a_2}(1-(w-z))^{1-b_2}}{\beta(a_2,b_2)} \end{align} From there I could integrate out $W$ from $f_{Z,W}(z,w)$ to get the quantity of interest, i.e., the distribution of $Z=X_1-X_2$. So now this is where I am stuck.  I have the following: $$f_Z(z)=\int f_{Z,W}(z,w)dw=\int \text{Beta}(w;a_1,b_1)\times\text{Beta}(w-z;a_2,b_2) dw$$ But I do not understand how to obtain the bounds for the integral, and if the integral needs to be broken into parts or not.  Let me know also if any of the above steps are incorrect.",,"['calculus', 'probability', 'integration', 'multivariable-calculus', 'probability-distributions']"
11,Why is the gradient perpendicular to the tangent of a plane?,Why is the gradient perpendicular to the tangent of a plane?,,"Given a function $f(x,y)$, its gradient is defined to be: $\nabla f(x,y) = \frac{\partial f}{\partial x} \hat{i} + \frac{\partial f}{\partial y} \hat{j}$. [$\hat{i}$ and $\hat{j}$ are unit vectors in the $x$ and $y$ direction] Given this definition, the gradient vector will always be parallel to the $x$-$y$ plane. The gradient is also supposed to be perpendicular to the tangent of a plane (its  ""normal"" vector). How, however, could it be perpendicular to the tangent of the plane if it is always parallel to the $x$-$y$ plane?","Given a function $f(x,y)$, its gradient is defined to be: $\nabla f(x,y) = \frac{\partial f}{\partial x} \hat{i} + \frac{\partial f}{\partial y} \hat{j}$. [$\hat{i}$ and $\hat{j}$ are unit vectors in the $x$ and $y$ direction] Given this definition, the gradient vector will always be parallel to the $x$-$y$ plane. The gradient is also supposed to be perpendicular to the tangent of a plane (its  ""normal"" vector). How, however, could it be perpendicular to the tangent of the plane if it is always parallel to the $x$-$y$ plane?",,"['calculus', 'multivariable-calculus']"
12,"Closed-form of the hypergeometric function ${_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-t\right)$",Closed-form of the hypergeometric function,"{_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-t\right)","Inspired by this question and by using Mathematica the following conjecture seems to be true for all nonzero complex $t$ number: $${_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-t\right) \stackrel{?}{=} \frac{16}{3t}\ln\left(\tfrac14\sqrt{1+\sqrt{1+t}}\left(\sqrt{1+\sqrt{1+t}}+\sqrt{2}\right)\right),$$ where ${_4F_3}$ is a generalized hypergeometric function . How could we prove this conjectured identity? Some special cases: $$\begin{align} {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-4\right) &\stackrel{?}{=} \frac{4}{3}\ln\left(\frac{\sqrt{\varphi}+\varphi}{2}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-8\right) &\stackrel{?}{=} \frac{2}{3}\ln\left(\frac{\sqrt2 + 2}{2}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-15\right) &\stackrel{?}{=} \frac{16}{45}\ln\left(\frac{\sqrt{10} + 5}{4}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-35\right) &\stackrel{?}{=} \frac{16}{105}\ln\left(\frac{\sqrt{14} + 7}{4}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-48\right) &\stackrel{?}{=} \frac{1}{9}\ln 3, \end{align}$$ where $\varphi$ is the golden ratio . Specially for all $n \neq 1$ nonnegative integers $${_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,1-n^2\right) \stackrel{?}{=} \frac{16}{3n^2-3}\ln\left(\frac{\sqrt{2n+2}+(n+1)}{4}\right).$$","Inspired by this question and by using Mathematica the following conjecture seems to be true for all nonzero complex $t$ number: $${_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-t\right) \stackrel{?}{=} \frac{16}{3t}\ln\left(\tfrac14\sqrt{1+\sqrt{1+t}}\left(\sqrt{1+\sqrt{1+t}}+\sqrt{2}\right)\right),$$ where ${_4F_3}$ is a generalized hypergeometric function . How could we prove this conjectured identity? Some special cases: $$\begin{align} {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-4\right) &\stackrel{?}{=} \frac{4}{3}\ln\left(\frac{\sqrt{\varphi}+\varphi}{2}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-8\right) &\stackrel{?}{=} \frac{2}{3}\ln\left(\frac{\sqrt2 + 2}{2}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-15\right) &\stackrel{?}{=} \frac{16}{45}\ln\left(\frac{\sqrt{10} + 5}{4}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-35\right) &\stackrel{?}{=} \frac{16}{105}\ln\left(\frac{\sqrt{14} + 7}{4}\right),\\ {_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,-48\right) &\stackrel{?}{=} \frac{1}{9}\ln 3, \end{align}$$ where $\varphi$ is the golden ratio . Specially for all $n \neq 1$ nonnegative integers $${_4F_3}\left(\begin{array}c1,1,\tfrac54,\tfrac74\\\tfrac32,2,2\end{array}\middle|\,1-n^2\right) \stackrel{?}{=} \frac{16}{3n^2-3}\ln\left(\frac{\sqrt{2n+2}+(n+1)}{4}\right).$$",,"['calculus', 'special-functions', 'closed-form', 'hypergeometric-function']"
13,"Prove the integral inequality on interval [-1,1]","Prove the integral inequality on interval [-1,1]",,"Let $f(x)$ be continuous function on $[-1,1]$ which satisfies: 1. $f(-1)\ge f(1)$. 2. $x+f(x)$ is non-decreasing. 3. $\int_{-1}^1 f(x)dx=0$. Prove that $\int_{-1}^1 f^2(x)dx \le \frac2 3$","Let $f(x)$ be continuous function on $[-1,1]$ which satisfies: 1. $f(-1)\ge f(1)$. 2. $x+f(x)$ is non-decreasing. 3. $\int_{-1}^1 f(x)dx=0$. Prove that $\int_{-1}^1 f^2(x)dx \le \frac2 3$",,['calculus']
14,Quaternion integration,Quaternion integration,,"If the angular velocity is changing continuously, the following holds true $ q(t)=q(0)\exp\left({\int_{0}^{t}\frac{q_\omega(\tau)}{2}\ d\tau}\right) \tag 1$ Specifications and Data $q(t),q(0)$ represents quaternions $q_\omega(\tau)$ represents the quaternion representation of angular velocity at $\tau$. It implies  if $\omega(\tau) \in R^3 $ is the angular velocity,then  $q_\omega(\tau)=(0,\omega(\tau))$ at $\tau$ Exponent of a quaternion $J=( p,v)$  can be defined as  \begin{eqnarray}  e^{J}=e^{p}\left(cos|v| ,\frac{v}{|v|}sin|v|  \right)  \end{eqnarray}$v$ is a vector. If you are given a vector, make it as a quaternion with $p=0$ Question How do we prove equation $ q(t)=q(0)\exp\left({\int_{0}^{t}\frac{q_\omega(\tau)}{2}\ d\tau}\right)  $ precisely?","If the angular velocity is changing continuously, the following holds true $ q(t)=q(0)\exp\left({\int_{0}^{t}\frac{q_\omega(\tau)}{2}\ d\tau}\right) \tag 1$ Specifications and Data $q(t),q(0)$ represents quaternions $q_\omega(\tau)$ represents the quaternion representation of angular velocity at $\tau$. It implies  if $\omega(\tau) \in R^3 $ is the angular velocity,then  $q_\omega(\tau)=(0,\omega(\tau))$ at $\tau$ Exponent of a quaternion $J=( p,v)$  can be defined as  \begin{eqnarray}  e^{J}=e^{p}\left(cos|v| ,\frac{v}{|v|}sin|v|  \right)  \end{eqnarray}$v$ is a vector. If you are given a vector, make it as a quaternion with $p=0$ Question How do we prove equation $ q(t)=q(0)\exp\left({\int_{0}^{t}\frac{q_\omega(\tau)}{2}\ d\tau}\right)  $ precisely?",,"['calculus', 'linear-algebra', 'integration', 'rotations', 'quaternions']"
15,Definition of tangent,Definition of tangent,,What is the formal definition of a tangent to a curve? The only one I can find is that it is a straight line drawn between two infinitely close points on the curve.,What is the formal definition of a tangent to a curve? The only one I can find is that it is a straight line drawn between two infinitely close points on the curve.,,"['calculus', 'infinitesimals']"
16,Is there a function such that $f' = f\circ f$?,Is there a function such that ?,f' = f\circ f,"Is there a function $f:\mathbb{R}\rightarrow (0,\infty)$, such that $f' = f\circ f$? Apparently, I should assume by contradiction there is, and then it should imply that $f$ is increasing but I can't see the reason for that. EDIT: Now, we know that $f(0)$ is a lower bound for $f'(x).\forall x \in \mathbb{R}$. The next claim is for $x<0.f(x) <f(0) + xf(0) = (1+x)f(0)$. Why it the last claim true?","Is there a function $f:\mathbb{R}\rightarrow (0,\infty)$, such that $f' = f\circ f$? Apparently, I should assume by contradiction there is, and then it should imply that $f$ is increasing but I can't see the reason for that. EDIT: Now, we know that $f(0)$ is a lower bound for $f'(x).\forall x \in \mathbb{R}$. The next claim is for $x<0.f(x) <f(0) + xf(0) = (1+x)f(0)$. Why it the last claim true?",,"['calculus', 'functions']"
17,Are there Taylor series for functions of a matrix?,Are there Taylor series for functions of a matrix?,,"Say you have a scalar function $f(x,A)$ of a vector $x$ and a matrix $A$. Does there exist a Taylor series of sorts for the matrix $A$? I was thinking naively that this would simply be of the form $\sum_{n} \frac{\partial^{n}f(x,0)}{\partial A^{n}}\frac{A^{n}}{n!}$ Where the derivatives are taken with respect to the matrix $A$. The sum should still be a scalar so that the powers of $A$ match with the derivatives taken with respect to $A$, so that all indices are summed over implicitly. Any suggestions or comments?","Say you have a scalar function $f(x,A)$ of a vector $x$ and a matrix $A$. Does there exist a Taylor series of sorts for the matrix $A$? I was thinking naively that this would simply be of the form $\sum_{n} \frac{\partial^{n}f(x,0)}{\partial A^{n}}\frac{A^{n}}{n!}$ Where the derivatives are taken with respect to the matrix $A$. The sum should still be a scalar so that the powers of $A$ match with the derivatives taken with respect to $A$, so that all indices are summed over implicitly. Any suggestions or comments?",,"['calculus', 'analysis', 'matrices']"
18,How to find finite trigonometric products [closed],How to find finite trigonometric products [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I wonder how to prove ? $$\prod_{k=1}^{n}\left(1+2\cos\frac{2\pi 3^k}{3^n+1} \right)=1$$ give me a tip","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I wonder how to prove ? $$\prod_{k=1}^{n}\left(1+2\cos\frac{2\pi 3^k}{3^n+1} \right)=1$$ give me a tip",,"['calculus', 'sequences-and-series', 'trigonometry', 'products']"
19,Integrating $\int_0^{\infty} \frac{\sin^n (ax)}{x^n \cdot (x^2+1)} dx$,Integrating,\int_0^{\infty} \frac{\sin^n (ax)}{x^n \cdot (x^2+1)} dx,"I was trying to evaluate an expression for the family of integrals $$ I(a,n) = \int_0^{\infty} \frac{\sin^n(ax)}{x^n(x^2+1)} dx$$ For positive $a$ and $n$ The expression for the first few $n$ can be computed by hand. For example, for the case when $n=1$ we have: $$ I(a,1) = \int_0^{\infty} \frac{\sin(ax)}{x(x^2+1)} dx$$ Differentiating, we get: $$ I'(a,1) = \int_0^{\infty} \frac{\cos(ax)}{x^2+1} dx = \frac{\pi}{2}\cdot e^{-a}$$ Since $I(0,1) = 0$ , we can write $I(a,1)$ as: $$ I(a,1) = \frac{\pi}{2} \cdot (1-e^{-a})$$ For $n=2$ , we again differentiate with respect to $a$ . We get: $$I'(a,2) = \int_0^{\infty} \frac{\sin(2ax)}{x(x^2+1)} dx = I(2a,1) = \frac{\pi}{2} \cdot (1-e^{-2a})$$ Again observing that $I(0,2) = 0$ , we can write $I(a,2)$ as: $$I(a,2) = \frac{\pi}{2} \cdot (a + \frac{1}{2}\cdot e^{-2a} - \frac{1}{2})$$ With some rather tedious effort in computing the third derivative, we can use the same strategy to evaluate $I(a,3)$ . I obtained: $$I(a,3) = \frac{\pi}{8} \cdot (3a^2+ 3e^{-a} -e^{-3a}-2)$$ I hypothesize after looking at the pattern that the expression for $I(a,n)$ must be some fraction of $\pi$ , multiplied by the sum of a polynomial of degree $n-1$ added to some exponentials of the form $e^{-ka}$ where $k$ ranges from $1$ to $n$ . But I was wondering if we can afford to keep doing this for higher and higher values of $n$ . I don't have a surefire way of obtaining an expression for general $n$ , except for repeated differentiation, simplification and reverse integration. Can anyone help me in evaluating an expression that holds for all natural $n$ ? Any help is appreciated. Thank you for reading!","I was trying to evaluate an expression for the family of integrals For positive and The expression for the first few can be computed by hand. For example, for the case when we have: Differentiating, we get: Since , we can write as: For , we again differentiate with respect to . We get: Again observing that , we can write as: With some rather tedious effort in computing the third derivative, we can use the same strategy to evaluate . I obtained: I hypothesize after looking at the pattern that the expression for must be some fraction of , multiplied by the sum of a polynomial of degree added to some exponentials of the form where ranges from to . But I was wondering if we can afford to keep doing this for higher and higher values of . I don't have a surefire way of obtaining an expression for general , except for repeated differentiation, simplification and reverse integration. Can anyone help me in evaluating an expression that holds for all natural ? Any help is appreciated. Thank you for reading!"," I(a,n) = \int_0^{\infty} \frac{\sin^n(ax)}{x^n(x^2+1)} dx a n n n=1  I(a,1) = \int_0^{\infty} \frac{\sin(ax)}{x(x^2+1)} dx  I'(a,1) = \int_0^{\infty} \frac{\cos(ax)}{x^2+1} dx = \frac{\pi}{2}\cdot e^{-a} I(0,1) = 0 I(a,1)  I(a,1) = \frac{\pi}{2} \cdot (1-e^{-a}) n=2 a I'(a,2) = \int_0^{\infty} \frac{\sin(2ax)}{x(x^2+1)} dx = I(2a,1) = \frac{\pi}{2} \cdot (1-e^{-2a}) I(0,2) = 0 I(a,2) I(a,2) = \frac{\pi}{2} \cdot (a + \frac{1}{2}\cdot e^{-2a} - \frac{1}{2}) I(a,3) I(a,3) = \frac{\pi}{8} \cdot (3a^2+ 3e^{-a} -e^{-3a}-2) I(a,n) \pi n-1 e^{-ka} k 1 n n n n",['calculus']
20,Integrals Involving GCD Function,Integrals Involving GCD Function,,"Recently I got interested in Integrals involving GCD. I was wondering if I could get more such Integrals, Here are some examples: $$\int_0^\pi \sin^2(ab x) \cot(ax)\cot(bx)dx=\left(\frac{2\color{red}{\gcd(a,b)}-1}{2}\right)\pi$$ $$\int_0^{\pi/2}\ln{\lvert\sin(mx)\rvert}\cdot \ln{\lvert\sin(nx)\rvert}\, dx=\frac{\pi^3}{24}\frac{\color{red}{\gcd^2(m,n)}}{mn}+\frac{\pi\ln^2(2)}{2}$$ $$\int_{0}^{\pi}\arctan\left(\cot\left(mt\right)\right)\arctan\left(\cot\left(nt\right)\right)dt=\frac{\pi^{3}}{12}\cdot\frac{\color{red}{\gcd^2\left(m,n\right)}}{mn}$$ $$\int_{0}^{1}\lfloor{ax}\rfloor \lfloor{bx}\rfloor  dx=\frac{\color{red}{\gcd^2\left(a,b\right)}}{12ab}-\frac{a}{4}-\frac{b}{4}-\frac{b}{12a}-\frac{a}{12b}+\frac{ab}{3}+\frac{1}{4}$$ Here is a Conditional One: If $c$ is not a multiple of $\gcd(a,b)$ then, $$\int_{0}^{\pi/2}\cos^{2}\left(ct\right)\ln\left(\left|\sin at\right|\right)\ln\left(\left|\sin bt\right|\right)dt= \frac{\pi^{3}}{48}\frac{\color{red}{\gcd^{2}\left(a,b\right)}}{ab}+\frac{\pi\ln^{2}\left(2\right)}{4}$$ This is not a question per say but I am just curious about more of these Integrals. You can answer with a new Integral and maybe its solution. I will also keep adding more such Integrals.","Recently I got interested in Integrals involving GCD. I was wondering if I could get more such Integrals, Here are some examples: Here is a Conditional One: If is not a multiple of then, This is not a question per say but I am just curious about more of these Integrals. You can answer with a new Integral and maybe its solution. I will also keep adding more such Integrals.","\int_0^\pi \sin^2(ab x) \cot(ax)\cot(bx)dx=\left(\frac{2\color{red}{\gcd(a,b)}-1}{2}\right)\pi \int_0^{\pi/2}\ln{\lvert\sin(mx)\rvert}\cdot \ln{\lvert\sin(nx)\rvert}\, dx=\frac{\pi^3}{24}\frac{\color{red}{\gcd^2(m,n)}}{mn}+\frac{\pi\ln^2(2)}{2} \int_{0}^{\pi}\arctan\left(\cot\left(mt\right)\right)\arctan\left(\cot\left(nt\right)\right)dt=\frac{\pi^{3}}{12}\cdot\frac{\color{red}{\gcd^2\left(m,n\right)}}{mn} \int_{0}^{1}\lfloor{ax}\rfloor \lfloor{bx}\rfloor 
dx=\frac{\color{red}{\gcd^2\left(a,b\right)}}{12ab}-\frac{a}{4}-\frac{b}{4}-\frac{b}{12a}-\frac{a}{12b}+\frac{ab}{3}+\frac{1}{4} c \gcd(a,b) \int_{0}^{\pi/2}\cos^{2}\left(ct\right)\ln\left(\left|\sin at\right|\right)\ln\left(\left|\sin bt\right|\right)dt=
\frac{\pi^{3}}{48}\frac{\color{red}{\gcd^{2}\left(a,b\right)}}{ab}+\frac{\pi\ln^{2}\left(2\right)}{4}","['calculus', 'integration', 'definite-integrals']"
21,Help me prove $K(k) = \frac{1}{1+k}K\left(\frac{2\sqrt{k}}{1+k}\right)$ where $K(k) = \int^{\pi/2}_0 \frac{du}{\sqrt{1-(k\sin{u})^2}}$,Help me prove  where,K(k) = \frac{1}{1+k}K\left(\frac{2\sqrt{k}}{1+k}\right) K(k) = \int^{\pi/2}_0 \frac{du}{\sqrt{1-(k\sin{u})^2}},"Hi this problem is posed in Sean M. Stewart's How to Integrate It book pg.143 $$K(k) = \int^{\pi/2}_0 \frac{du}{\sqrt{1-(k\sin{u})^2}}$$ where $ 0 \leq k < 1$ Based on the substitution: $$ \tan{u} = \frac{\sin{2\theta}}{k+k\cos{2\theta}} =\frac{\tan{\theta}}{k}  $$ Show that: $$K(k) = \frac{1}{1+k}K\left(\frac{2\sqrt{k}}{1+k}\right)$$ $K(k) $ is some function defined in term of definite-integral called complete elliptic integral which is not easily computable, so attacking $K(k)$ gave no fruitful result. Well then proceeding with substitution ( It is easy to lose track as it  involved a lot of trigonometric manipulations ) I reduced the integral into this form : $$K(k) = k\int^{\pi/2}_0 \frac{\sec^2{\theta}d\theta}{\sqrt{k^2 + \tan^2{\theta}}\sqrt{k^2 + \tan^2{\theta} - k^2\tan^2{\theta}}}$$ Using substitution : $\tan{\theta} = p$ $$K(k) = k\int^{\infty}_0 \frac{dp}{\sqrt{k^2 +p^2}\sqrt{k^2 +p^2 - (kp)^2}}$$ To my naive eyes this looked manageable but WolframAlpha says otherwise. Any hint or ideas on how to attack this problem? I thought  the trig manipulations to find a more suitable form but in the I end up transform $\sin {u}$ term into $\tan{u}$ to use the given substitution EDIT: It has been suggested in commens that there is typo in the substituion(given in book) , it should be : $$ \tan{u} = \frac{\sin{2\theta}}{k+\cos{2\theta}} $$","Hi this problem is posed in Sean M. Stewart's How to Integrate It book pg.143 where Based on the substitution: Show that: is some function defined in term of definite-integral called complete elliptic integral which is not easily computable, so attacking gave no fruitful result. Well then proceeding with substitution ( It is easy to lose track as it  involved a lot of trigonometric manipulations ) I reduced the integral into this form : Using substitution : To my naive eyes this looked manageable but WolframAlpha says otherwise. Any hint or ideas on how to attack this problem? I thought  the trig manipulations to find a more suitable form but in the I end up transform term into to use the given substitution EDIT: It has been suggested in commens that there is typo in the substituion(given in book) , it should be :",K(k) = \int^{\pi/2}_0 \frac{du}{\sqrt{1-(k\sin{u})^2}}  0 \leq k < 1  \tan{u} = \frac{\sin{2\theta}}{k+k\cos{2\theta}} =\frac{\tan{\theta}}{k}   K(k) = \frac{1}{1+k}K\left(\frac{2\sqrt{k}}{1+k}\right) K(k)  K(k) K(k) = k\int^{\pi/2}_0 \frac{\sec^2{\theta}d\theta}{\sqrt{k^2 + \tan^2{\theta}}\sqrt{k^2 + \tan^2{\theta} - k^2\tan^2{\theta}}} \tan{\theta} = p K(k) = k\int^{\infty}_0 \frac{dp}{\sqrt{k^2 +p^2}\sqrt{k^2 +p^2 - (kp)^2}} \sin {u} \tan{u}  \tan{u} = \frac{\sin{2\theta}}{k+\cos{2\theta}} ,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'elliptic-integrals']"
22,Help figuring a formula for my job,Help figuring a formula for my job,,"I'm a metal worker, I cut, weld, whatever. I'm trying to figure out a formula where I could take my cutting list  And figure out the most efficient way to cut it with the materials I have. For example  I have 2 20ft lengths of tubes I want 5 peices cut at 1.5ft, 3 at 2ft and 1 at 2ft What I'm trying to do is calculate the most efficient cutting order to save the most material I'm using. My education is limited so any help would be appreciated, thank you.","I'm a metal worker, I cut, weld, whatever. I'm trying to figure out a formula where I could take my cutting list  And figure out the most efficient way to cut it with the materials I have. For example  I have 2 20ft lengths of tubes I want 5 peices cut at 1.5ft, 3 at 2ft and 1 at 2ft What I'm trying to do is calculate the most efficient cutting order to save the most material I'm using. My education is limited so any help would be appreciated, thank you.",,['calculus']
23,The $n$th derivative of $f(e^x)$,The th derivative of,n f(e^x),"Because it comes up every so often, I was wondering if there was a nice general expansion for the $n$ th derivative of $f(e^x)$ . $\frac{\mathrm d}{\mathrm dx}f(e^x)=e^xf'(e^x)$ $\frac{\mathrm d^2}{\mathrm dx^2}f(e^x)=e^xf'(e^x)+e^{2x}f''(e^x)$ $\frac{\mathrm d^3}{\mathrm dx^3}f(e^x)=e^xf'(e^x)+3e^{2x}f''(e^x)+e^{3x}f'''(e^x)$ $\vdots$ One can see that we end up with the form $$\frac{\mathrm d^n}{\mathrm dx^n}f(e^x)=\sum_{k=1}^nC(n,k)e^{kx}f^{(k)}(e^x)$$ for some constants $C(n,k)$ . I've found that it reduces to solving the following recurrence relation: $$C(n,k)=\begin{cases}0,&n<k\\1,&n>0\land k=1\\kC(n-1,k)+C(n-1,k-1),&n>0\land k>1\end{cases}$$ From which you can deduce things such as $C(n,2)=2^{n-1}-1$ . Is there a nice general solution?","Because it comes up every so often, I was wondering if there was a nice general expansion for the th derivative of . One can see that we end up with the form for some constants . I've found that it reduces to solving the following recurrence relation: From which you can deduce things such as . Is there a nice general solution?","n f(e^x) \frac{\mathrm d}{\mathrm dx}f(e^x)=e^xf'(e^x) \frac{\mathrm d^2}{\mathrm dx^2}f(e^x)=e^xf'(e^x)+e^{2x}f''(e^x) \frac{\mathrm d^3}{\mathrm dx^3}f(e^x)=e^xf'(e^x)+3e^{2x}f''(e^x)+e^{3x}f'''(e^x) \vdots \frac{\mathrm d^n}{\mathrm dx^n}f(e^x)=\sum_{k=1}^nC(n,k)e^{kx}f^{(k)}(e^x) C(n,k) C(n,k)=\begin{cases}0,&n<k\\1,&n>0\land k=1\\kC(n-1,k)+C(n-1,k-1),&n>0\land k>1\end{cases} C(n,2)=2^{n-1}-1","['calculus', 'sequences-and-series', 'combinatorics', 'derivatives', 'recurrence-relations']"
24,Does $\lim_{n\rightarrow \infty} \left(r+\frac{1}{n^2}\right)\uparrow \uparrow n=e$ hold?,Does  hold?,\lim_{n\rightarrow \infty} \left(r+\frac{1}{n^2}\right)\uparrow \uparrow n=e,"Does $$\lim_{n\rightarrow \infty}\left (r+\frac{1}{n^2}\right)\uparrow \uparrow n=e$$ hold ? $r$ is the number $e^{e^{-1}}$ , the largest real number for which the infinite power tower $r\uparrow r\uparrow r\uparrow \cdots$ converges. So we have a power tower of $n$ numbers having the value $e^{e^{-1}}+\frac{1}{n^2}$. Numerical examples I calculated with PARI/GP indicate that the error is about $\frac{3.614}{n}$.","Does $$\lim_{n\rightarrow \infty}\left (r+\frac{1}{n^2}\right)\uparrow \uparrow n=e$$ hold ? $r$ is the number $e^{e^{-1}}$ , the largest real number for which the infinite power tower $r\uparrow r\uparrow r\uparrow \cdots$ converges. So we have a power tower of $n$ numbers having the value $e^{e^{-1}}+\frac{1}{n^2}$. Numerical examples I calculated with PARI/GP indicate that the error is about $\frac{3.614}{n}$.",,"['calculus', 'limits', 'tetration', 'complex-dynamics']"
25,Could a version of probability theory be made rigorous with only calculus?,Could a version of probability theory be made rigorous with only calculus?,,"I am wondering if one could, or if there has been built a version of probability theory that could exist rigorous on its own without real analysis and measure theory? The motivation for this quesiton is that in almost every introductory text in mathematical statistics I've seen, they only rely on calculus as a prerequisite. Could one only rely on calculus and multivariate calculus to do this rigorous? In introductory texts random variables are described as random functions, but their characteristics is their pmf or pdf, would this be enough? So if we only restrict ourselves to what is called discrete or continuous random variables(or vectors) in introductory statistics text, is calculus and multivariate calculus enough to get the theory to stand on its own? or will we need measure theory? If it is the case that this is impossible to do without measure theory, does this mean that most introductory texts in statistics are wrong? Since they base everything on Riemann-integrals, but the theory is wrong? UPDATE: From the discussion in the comments we see that by the definition of a probability space we need it to be a measure, and we also need the events to be a sigma-algebra. Hence we need these two concepts from measure theory. So I should have asked the question like this instead: Will the theory of continuous random variables in introductory texts work out if we only allow Riemann-integration and not Lebesgue-integration? Is the integration-theory in introductory texts in calculus which only deals with Riemann-integration enough to make sure that the theory is rigorous?","I am wondering if one could, or if there has been built a version of probability theory that could exist rigorous on its own without real analysis and measure theory? The motivation for this quesiton is that in almost every introductory text in mathematical statistics I've seen, they only rely on calculus as a prerequisite. Could one only rely on calculus and multivariate calculus to do this rigorous? In introductory texts random variables are described as random functions, but their characteristics is their pmf or pdf, would this be enough? So if we only restrict ourselves to what is called discrete or continuous random variables(or vectors) in introductory statistics text, is calculus and multivariate calculus enough to get the theory to stand on its own? or will we need measure theory? If it is the case that this is impossible to do without measure theory, does this mean that most introductory texts in statistics are wrong? Since they base everything on Riemann-integrals, but the theory is wrong? UPDATE: From the discussion in the comments we see that by the definition of a probability space we need it to be a measure, and we also need the events to be a sigma-algebra. Hence we need these two concepts from measure theory. So I should have asked the question like this instead: Will the theory of continuous random variables in introductory texts work out if we only allow Riemann-integration and not Lebesgue-integration? Is the integration-theory in introductory texts in calculus which only deals with Riemann-integration enough to make sure that the theory is rigorous?",,"['calculus', 'probability-theory']"
26,For which values of $\alpha$ and $\beta$ does the integral $\int\limits_2^{\infty}\frac{dx}{x^{\alpha}\ln^{\beta}x}$ converge?,For which values of  and  does the integral  converge?,\alpha \beta \int\limits_2^{\infty}\frac{dx}{x^{\alpha}\ln^{\beta}x},"I'm trying to find out for which values of $\alpha$ and $\beta$ the integral $\int\limits_2^{\infty}\frac{dx}{x^{\alpha}\ln^{\beta}x}$ does converge. I know that when $\alpha=1$ then $\beta$ must be greater than $1$ . I tried to use integration by  parts but It didn't work, so I would appreciate some hints. Thanks in advance.","I'm trying to find out for which values of and the integral does converge. I know that when then must be greater than . I tried to use integration by  parts but It didn't work, so I would appreciate some hints. Thanks in advance.",\alpha \beta \int\limits_2^{\infty}\frac{dx}{x^{\alpha}\ln^{\beta}x} \alpha=1 \beta 1,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
27,convergence of a tower power,convergence of a tower power,,Prove that the sequence of general term $(\frac 12)^{(\frac 13)^{(\frac 14)^{...\frac 1n}}}$ is convergent. the three dots are antidiagonal of course :) My try was to compare it with some easier one like the power tower of $\frac 12$. I conjectured with wolframalpha that it can be divided to two subsequences which seem to be adjacent but i'm finding it hard to prove it Can anyone give some hints.,Prove that the sequence of general term $(\frac 12)^{(\frac 13)^{(\frac 14)^{...\frac 1n}}}$ is convergent. the three dots are antidiagonal of course :) My try was to compare it with some easier one like the power tower of $\frac 12$. I conjectured with wolframalpha that it can be divided to two subsequences which seem to be adjacent but i'm finding it hard to prove it Can anyone give some hints.,,['calculus']
28,Proof of Multivariable Implicit Differentiation Formula,Proof of Multivariable Implicit Differentiation Formula,,"If the equation $F(x,y,z)=0$ defines $z$ implicitly as a differentiable function of x and y, then by taking a partial derivative with respect to one of the independent variables (in this case x), you get $\large F_x(x,y,z)\frac{\partial x}{\partial x}+F_y(x,y,z)\frac{\partial y}{\partial x}+F_z(x,y,z)\frac{\partial z}{\partial x}=0.$ Because dx/dx = 1 and dy/dx = 0 , you can solve for the desired partial derivative: $\large \frac{\partial z}{\partial x}=-\frac{F_x(x,y,z)}{F_z(x,y,z)} $ The bolded dy/dx = 0 is what I don't get. I mean, it makes sense that an independent variable doesn't change in response to another, but it doesn't seem very formal and I feel like there's more to it than that. So basically, is there a more formal or detailed explanation or is that all there is to it?","If the equation $F(x,y,z)=0$ defines $z$ implicitly as a differentiable function of x and y, then by taking a partial derivative with respect to one of the independent variables (in this case x), you get $\large F_x(x,y,z)\frac{\partial x}{\partial x}+F_y(x,y,z)\frac{\partial y}{\partial x}+F_z(x,y,z)\frac{\partial z}{\partial x}=0.$ Because dx/dx = 1 and dy/dx = 0 , you can solve for the desired partial derivative: $\large \frac{\partial z}{\partial x}=-\frac{F_x(x,y,z)}{F_z(x,y,z)} $ The bolded dy/dx = 0 is what I don't get. I mean, it makes sense that an independent variable doesn't change in response to another, but it doesn't seem very formal and I feel like there's more to it than that. So basically, is there a more formal or detailed explanation or is that all there is to it?",,"['calculus', 'multivariable-calculus', 'implicit-differentiation']"
29,"Sum of two gamma/Erlang random variables $\Gamma(m,\lambda)$ and $\Gamma(n, \mu)$ with integer numbers $m \neq n, \lambda \neq \mu$",Sum of two gamma/Erlang random variables  and  with integer numbers,"\Gamma(m,\lambda) \Gamma(n, \mu) m \neq n, \lambda \neq \mu","The gamma distribution with parameters $m > 0$ and $\lambda > 0$ (denoted $\Gamma(m, \lambda)$ ) has density function $$f(x) = \frac{\lambda e^{-\lambda x} (\lambda x)^{m - 1}}{\Gamma(m)}, x > 0$$ Given two independent gamma random variables $X = \Gamma(m,\lambda)$ and $Y = \Gamma(n, \mu)$ with integer numbers $m \neq n, \lambda \neq \mu$ , what is the density function of their sum $X + Y = \Gamma(m,\lambda) + \Gamma(n,\mu)$ ? Notice that both $X$ and $Y$ are also Erlang distribution since $m,n$ are positive integers. My attempt First, I searched for well-known results about gamma distribution and I got: (1) If $\lambda = \mu$ , the sum random variable is a Gamma distribution $\sim \Gamma(m+n, \lambda)$ (See Math.SE ). (2) $\Gamma(m, \lambda)$ (or $\Gamma(n, \mu)$ ) is the sum of $m$ (or $n$ ) independent exponential random variables each having rate $\lambda$ (or $\mu$ ). The hypoexponential distribution is related to the sum of independent exponential random variables. However, it require all the rates distinct . (3) This site is devoted to the problem of sums of gamma random variables . In section 3.1, it claims that if $m$ and $n$ are integer numbers (which is my case), the density  function can be expressed in terms of elementary functions (proved in section 3.4). The answer is likely buried under a haystack of formulas (however, I failed to find it; you are recommended to have a try) . Then, I try to calculate it: $$f_{X+Y}(a) = \int_{0}^{a} f_{X}(a-y) f_{Y}(y) dy \\ = \int_{0}^{a} \frac{\lambda e^{-\lambda (a-y)} (\lambda (a-y))^{m-1}}{\Gamma(m)} \frac{\mu e^{-\mu y} (\mu y)^{n-1}}{\Gamma(n)} dy \\ = e^{-\lambda a} \frac{\lambda^m \mu^n}{\Gamma(m) \Gamma(n)} \int_{0}^{a} e^{(\lambda - \mu) y} (a-y)^{m-1} y^{n-1} dy$$ Here, I am stuck with the integral and gain nothing ... Therefore, How to compute the density function of $\Gamma(m,\lambda) + \Gamma(n,\mu)$ with integer numbers $m \neq n, \lambda \neq \mu$ ? Added: The answers assuming $m = n$ ( $\lambda \neq \mu$ ) are also appreciated.","The gamma distribution with parameters and (denoted ) has density function Given two independent gamma random variables and with integer numbers , what is the density function of their sum ? Notice that both and are also Erlang distribution since are positive integers. My attempt First, I searched for well-known results about gamma distribution and I got: (1) If , the sum random variable is a Gamma distribution (See Math.SE ). (2) (or ) is the sum of (or ) independent exponential random variables each having rate (or ). The hypoexponential distribution is related to the sum of independent exponential random variables. However, it require all the rates distinct . (3) This site is devoted to the problem of sums of gamma random variables . In section 3.1, it claims that if and are integer numbers (which is my case), the density  function can be expressed in terms of elementary functions (proved in section 3.4). The answer is likely buried under a haystack of formulas (however, I failed to find it; you are recommended to have a try) . Then, I try to calculate it: Here, I am stuck with the integral and gain nothing ... Therefore, How to compute the density function of with integer numbers ? Added: The answers assuming ( ) are also appreciated.","m > 0 \lambda > 0 \Gamma(m, \lambda) f(x) = \frac{\lambda e^{-\lambda x} (\lambda x)^{m - 1}}{\Gamma(m)}, x > 0 X = \Gamma(m,\lambda) Y = \Gamma(n, \mu) m \neq n, \lambda \neq \mu X + Y = \Gamma(m,\lambda) + \Gamma(n,\mu) X Y m,n \lambda = \mu \sim \Gamma(m+n, \lambda) \Gamma(m, \lambda) \Gamma(n, \mu) m n \lambda \mu m n f_{X+Y}(a) = \int_{0}^{a} f_{X}(a-y) f_{Y}(y) dy \\ = \int_{0}^{a} \frac{\lambda e^{-\lambda (a-y)} (\lambda (a-y))^{m-1}}{\Gamma(m)} \frac{\mu e^{-\mu y} (\mu y)^{n-1}}{\Gamma(n)} dy \\ = e^{-\lambda a} \frac{\lambda^m \mu^n}{\Gamma(m) \Gamma(n)} \int_{0}^{a} e^{(\lambda - \mu) y} (a-y)^{m-1} y^{n-1} dy \Gamma(m,\lambda) + \Gamma(n,\mu) m \neq n, \lambda \neq \mu m = n \lambda \neq \mu","['calculus', 'probability', 'probability-theory', 'reference-request', 'probability-distributions']"
30,How to bound this integral?,How to bound this integral?,,"I want to ask how a hint how to show this integral inequality: $$ \frac{1}{\pi}\int^{\infty}_{0}\frac{x}{y^{2}+x^{2}}\log\frac{1}{1-e^{-2\pi y}}dy< \frac{1}{12x} $$ This is from Ahlfors, Complex Analysis, page $206$. I tried to compute a rough bound, but my bound is too rough and it did not work. Explicit computation showed the bound is quite delicate, for example for $x=6000$ the value on the left hand side is $0.000013888888876028806686426283065381014133382406390603...$ as opposed to $0.00001388888888...$. For $x=60000$ the accuracy is about $0.99999999999074074074$, for $x=75000$ the accuracy is about $0.99999999999407407407$. This is the refinement term in the usual Stirling's formula. I know it for a long time but I never knew how to prove it.","I want to ask how a hint how to show this integral inequality: $$ \frac{1}{\pi}\int^{\infty}_{0}\frac{x}{y^{2}+x^{2}}\log\frac{1}{1-e^{-2\pi y}}dy< \frac{1}{12x} $$ This is from Ahlfors, Complex Analysis, page $206$. I tried to compute a rough bound, but my bound is too rough and it did not work. Explicit computation showed the bound is quite delicate, for example for $x=6000$ the value on the left hand side is $0.000013888888876028806686426283065381014133382406390603...$ as opposed to $0.00001388888888...$. For $x=60000$ the accuracy is about $0.99999999999074074074$, for $x=75000$ the accuracy is about $0.99999999999407407407$. This is the refinement term in the usual Stirling's formula. I know it for a long time but I never knew how to prove it.",,['calculus']
31,just for fun integrate $\int \frac{1}{x^2+1}dx$ using Partial Fraction Decomposition,just for fun integrate  using Partial Fraction Decomposition,\int \frac{1}{x^2+1}dx,"on the one hand, clearly $$\int \frac{1}{x^2+1}dx=\arctan(x)+c$$ on the other hand using partial fraction decomposition $$\frac{1}{x^2+1}=\frac{A}{x+i} +\frac{B}{x-i}$$ with $A=1/(-2i)$ and $B=1/(2i)$ leading to  $$\int \frac{1}{x^2+1}dx=\frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i)$$ leading to the conclusion that [up to a constant..]  $$\arctan(x) = \frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i)$$ leading to two thoughts..  1) is there a flaw in above reasoning? 2) is there another way to verify that  $$\arctan(x) = \frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i) + c$$ other than taking derivatives on both sides..","on the one hand, clearly $$\int \frac{1}{x^2+1}dx=\arctan(x)+c$$ on the other hand using partial fraction decomposition $$\frac{1}{x^2+1}=\frac{A}{x+i} +\frac{B}{x-i}$$ with $A=1/(-2i)$ and $B=1/(2i)$ leading to  $$\int \frac{1}{x^2+1}dx=\frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i)$$ leading to the conclusion that [up to a constant..]  $$\arctan(x) = \frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i)$$ leading to two thoughts..  1) is there a flaw in above reasoning? 2) is there another way to verify that  $$\arctan(x) = \frac{-1}{2i}\ln(x+i)+\frac{1}{2i}\ln(x-i) + c$$ other than taking derivatives on both sides..",,"['calculus', 'integration']"
32,Need a hint to evaluate the indefinite integral $\int\frac{e^x(2-x^2)}{(1-x)\sqrt{1-x^2}}dx$?,Need a hint to evaluate the indefinite integral ?,\int\frac{e^x(2-x^2)}{(1-x)\sqrt{1-x^2}}dx,"So, the question says I have to perform the indefinite integration $$\int\frac{e^x(2-x^2)}{(1-x)\sqrt{1-x^2}}dx$$ I know that $$\int e^x(f(x)+f'(x))dx=e^xf(x)+C$$ Since any other substitution (using $x=ln(t)$ etc.) doesn't work, I expect I have to break the fraction with $e^x$ in the above integral to make separate fractions for $f(x)$ and $f'(x)$ somehow. I separate $2-x^2=1+(1-x^2)$, but that doesn't work (leaves $x$ in the numerator of derivative which I don't see in the question). Any other tricks?","So, the question says I have to perform the indefinite integration $$\int\frac{e^x(2-x^2)}{(1-x)\sqrt{1-x^2}}dx$$ I know that $$\int e^x(f(x)+f'(x))dx=e^xf(x)+C$$ Since any other substitution (using $x=ln(t)$ etc.) doesn't work, I expect I have to break the fraction with $e^x$ in the above integral to make separate fractions for $f(x)$ and $f'(x)$ somehow. I separate $2-x^2=1+(1-x^2)$, but that doesn't work (leaves $x$ in the numerator of derivative which I don't see in the question). Any other tricks?",,"['calculus', 'integration', 'indefinite-integrals']"
33,taylor's formula in $\mathbb{R}^n$ as exponential of derivative operator,taylor's formula in  as exponential of derivative operator,\mathbb{R}^n,"I hope this question is not too vague.  There is some relationship between the formal operator $$e^\frac{d}{dx} = 1 + \frac{d}{dx} + \frac{1}{2!}\frac{d^2}{dx^2} + \ldots  $$ and taylor's formula.  Is there a precise relationship that can be stated here? Specifically, I was taught a mnemonic once for taylors formula in multiple dimensions in terms of a similar exponential.  Does anybody know what I am referring to?","I hope this question is not too vague.  There is some relationship between the formal operator $$e^\frac{d}{dx} = 1 + \frac{d}{dx} + \frac{1}{2!}\frac{d^2}{dx^2} + \ldots  $$ and taylor's formula.  Is there a precise relationship that can be stated here? Specifically, I was taught a mnemonic once for taylors formula in multiple dimensions in terms of a similar exponential.  Does anybody know what I am referring to?",,"['calculus', 'multivariable-calculus', 'taylor-expansion']"
34,The continuity of multivariable function,The continuity of multivariable function,,"$F$ is a function on $\mathbb R^n$ such that for every smooth curve $\gamma:[0,1] \rightarrow \mathbb R^n, \gamma(0)=0 $, we have $\mathop {\lim }\limits_{t \to 0} F(\gamma (t)) = 0$, is it necessary that $\mathop {\lim }\limits_{\left| x \right| \to 0} F(x) = 0$ ?","$F$ is a function on $\mathbb R^n$ such that for every smooth curve $\gamma:[0,1] \rightarrow \mathbb R^n, \gamma(0)=0 $, we have $\mathop {\lim }\limits_{t \to 0} F(\gamma (t)) = 0$, is it necessary that $\mathop {\lim }\limits_{\left| x \right| \to 0} F(x) = 0$ ?",,"['calculus', 'differential-geometry']"
35,Integrating $\log(x)\cdot\exp(x)$,Integrating,\log(x)\cdot\exp(x),"How do I evaluate $\int \log(x) e^x\;dx ?$ I tried to do integration by parts... $$\int\log(x) \; dx = (x-1)\log(x) $$ Let $I=\int\log(x) e^x \; dx$. Therefore, $$ I = (x-1)\log(x) e^x - \int (x-1)\log(x) e^x \; dx $$ $$= (x-1)\log(x) e^x - \int (x)\log(x) e^x \; dx +\int \log(x) e^x \; dx$$ $$= (x-1)\log(x) e^x - \int  x \log(x) e^x \; dx + I$$ Now what to do, $I$ is on both LHS and RHS??","How do I evaluate $\int \log(x) e^x\;dx ?$ I tried to do integration by parts... $$\int\log(x) \; dx = (x-1)\log(x) $$ Let $I=\int\log(x) e^x \; dx$. Therefore, $$ I = (x-1)\log(x) e^x - \int (x-1)\log(x) e^x \; dx $$ $$= (x-1)\log(x) e^x - \int (x)\log(x) e^x \; dx +\int \log(x) e^x \; dx$$ $$= (x-1)\log(x) e^x - \int  x \log(x) e^x \; dx + I$$ Now what to do, $I$ is on both LHS and RHS??",,"['calculus', 'integration']"
36,"Describing Mean Value Theorem as ""Fundamental Theorem Of Differential Calculus""","Describing Mean Value Theorem as ""Fundamental Theorem Of Differential Calculus""",,"Referring to ""Introduction to Real Analysis"" 4th Ed (2011) by Bartle and Sherbert. (ch 6, sec 2, page 174) In fact the Mean Value Theorem is a wolf in sheep's clothing and is the Fundamental Theorem of Differential Calculus. What could have lead them to state this? All the other calculus books (Stewart, Apostol, Thomas...) have given special prominence to this theorem. I am definitely not confusing this with Fundamental Theorem of Calculus. What I mean is : What could have lead to this heightened significance of this theorem? Although this could be a subjective opinion, I would love to receive some expansive insights. I further request relative comparison of all the significant theorems of differential calculus namely, Extreme value , Mean value and Intermediate value theorem. PS: I emphasize that this question is completely disassociated with FTOC. If someone feels like the intent of author in the quote above is ambiguous/subjective, they may feel free to disregard it in their answers.","Referring to ""Introduction to Real Analysis"" 4th Ed (2011) by Bartle and Sherbert. (ch 6, sec 2, page 174) In fact the Mean Value Theorem is a wolf in sheep's clothing and is the Fundamental Theorem of Differential Calculus. What could have lead them to state this? All the other calculus books (Stewart, Apostol, Thomas...) have given special prominence to this theorem. I am definitely not confusing this with Fundamental Theorem of Calculus. What I mean is : What could have lead to this heightened significance of this theorem? Although this could be a subjective opinion, I would love to receive some expansive insights. I further request relative comparison of all the significant theorems of differential calculus namely, Extreme value , Mean value and Intermediate value theorem. PS: I emphasize that this question is completely disassociated with FTOC. If someone feels like the intent of author in the quote above is ambiguous/subjective, they may feel free to disregard it in their answers.",,['calculus']
37,How would I simplify this function $\rho(x)=x+\sqrt{x-\sqrt{x-\sqrt{x+\sqrt{\dots}}}}$,How would I simplify this function,\rho(x)=x+\sqrt{x-\sqrt{x-\sqrt{x+\sqrt{\dots}}}},"How do I simplify $\rho(x)$ into simple terms? $$\rho(x)=x+\sqrt{x-\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x+\sqrt{x-\sqrt{\dots}}}}}}}}$$ where the subtracting and the adding follows the Thue–Morse sequence $$+,-,-,+,-,+,+,-,-,+,+,-,+,-,-,+,\dots$$ I tried doing it with $x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{\dots}}}}}}}}$ and  got a answer by myself and I did it with $x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{\dots}}}}}}}}$ and found a post here Simplify the radical $\sqrt{x-\sqrt{x+\sqrt{x-...}}}$ and I understood how it worked I would like to know how to solve a problem like this? where the adding and the subtracting never repeats.",How do I simplify into simple terms? where the subtracting and the adding follows the Thue–Morse sequence I tried doing it with and  got a answer by myself and I did it with and found a post here Simplify the radical $\sqrt{x-\sqrt{x+\sqrt{x-...}}}$ and I understood how it worked I would like to know how to solve a problem like this? where the adding and the subtracting never repeats.,"\rho(x) \rho(x)=x+\sqrt{x-\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x+\sqrt{x-\sqrt{\dots}}}}}}}} +,-,-,+,-,+,+,-,-,+,+,-,+,-,-,+,\dots x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{\dots}}}}}}}} x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{x+\sqrt{x-\sqrt{\dots}}}}}}}}","['calculus', 'functions']"
38,Is it possible to solve $\frac{dy}{dx}=y^x$?,Is it possible to solve ?,\frac{dy}{dx}=y^x,"I was trying to solve the equation below just for curiosity, $\frac{dy}{dx}=y^x$ I found that i could only equate $y^x$ with $e^{-i^2x\ln(y)}=\cos(ix\ln(y))-i\sin(ix\ln(y))$ , so the original equation became, $\frac{dy}{dx}+i\sin(ix\ln(y))=\cos(ix\ln(y))$ After I arrived here I couldn't continue. Can someome solve this problem, please?","I was trying to solve the equation below just for curiosity, I found that i could only equate with , so the original equation became, After I arrived here I couldn't continue. Can someome solve this problem, please?",\frac{dy}{dx}=y^x y^x e^{-i^2x\ln(y)}=\cos(ix\ln(y))-i\sin(ix\ln(y)) \frac{dy}{dx}+i\sin(ix\ln(y))=\cos(ix\ln(y)),"['calculus', 'ordinary-differential-equations']"
39,Prove $\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^3=-\frac5{16}\zeta(3)$,Prove,\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^3=-\frac5{16}\zeta(3),"How to prove that $$S=\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^3=-\frac5{16}\zeta(3)$$ where $\overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k}$ is the alternating harmonic number. I came up with this problem after I solved a similar one here . I managed to prove the equality above but I am not happy with my solution as I used Mathematica for calculating the blue integral below, so any better ideas and without using softwares? Thank you, My solution: In page $105$ of this paper we have $$\overline{H}_n-\ln2=(-1)^{n-1}\int_0^1\frac{x^n}{1+x}dx$$ $$\Longrightarrow S=-\int_0^1\int_0^1\int_0^1\frac{1}{(1+x)(1+y)(1+z)}\sum_{n=0}^\infty(xyz)^n\ dx\ dy\ dz$$ $$=-\int_0^1\int_0^1\int_0^1\frac{dx\ dy\ dz}{(1+x)(1+y)(1+z)(1-xyz)}$$ $$=-\int_0^1\int_0^1\frac{dx\ dy}{(1+x)(1+y)}\left(\int_0^1\frac{dz}{(1+z)(1-xyz)}\right)$$ $$=-\int_0^1\int_0^1\frac{dx\ dy}{(1+x)(1+y)}\left(-\frac{\ln(1-xy)-\ln2}{1+xy}\right)$$ $$=\int_0^1\frac{dx}{1+x}\left(\int_0^1\frac{\ln(1-xy)-\ln2}{(1+y)(1+xy)}dy\right)$$ Mathematica gives $$\color{blue}{\int_0^1\frac{\ln(1-xy)-\ln2}{(1+y)(1+xy)}dy}$$ $$\small{=\frac{1}{x-1}\left[\frac{\pi^2}{12}+\frac12\ln^22-\ln(1-x)\ln\left(\frac{2x}{1+x}\right)+\operatorname{Li}_2\left(\frac{1}{1+x}\right)-\operatorname{Li}_2\left(\frac{1+x}{2}\right)-\operatorname{Li}_2\left(\frac{1-x}{1+x}\right)\right]}$$ giving us $$ S=-\int_0^1\frac{\frac{\pi^2}{12}+\frac12\ln^22-\ln(1-x)\ln\left(\frac{2x}{1+x}\right)+\operatorname{Li}_2\left(\frac{1}{1+x}\right)-\operatorname{Li}_2\left(\frac{1+x}{2}\right)-\operatorname{Li}_2\left(\frac{1-x}{1+x}\right)}{1-x^2}dx$$ By integration by parts and some simplifications, we get $$S=\underbrace{2\int_0^1\tanh^{-1}x\frac{\ln(1-x)-\ln2}{1+x}dx}_{\Large\mathcal{I}_1}-\underbrace{\int_0^1\frac{\tanh^{-1}x\ln(1-x)}{x}dx}_{\Large\mathcal{I}_2}$$ For $\mathcal{I}_1$ we know that $\tanh^{-1}x=-\frac12\ln\left(\frac{1-x}{1+x}\right)$ , so set $\frac{1-x}{1+x}=u$ $$\Longrightarrow \mathcal{I}_1=\int_0^1\ln u\frac{\ln(1+u)-\ln u}{1+u}du=\boxed{-\frac{13}{8}\zeta(3)}$$ For $\mathcal{I}_2$ use $\tanh^{-1}x=\sum_{n=0}^\infty\frac{x^{2n+1}}{2n+1}$ $$\Longrightarrow \mathcal{I}_2=\sum_{n=0}^\infty\frac1{2n+1}\int_0^1 x^{2n}\ln(1-x)\ dx=-\sum_{n=0}^\infty\frac{H_{2n+1}}{(2n+1)^2}$$ $$=-\sum_{n=0}^\infty\frac{H_{n+1}}{(n+1)^2}\left(\frac{1+(1)^n}{2}\right)=-\sum_{n=1}^\infty\frac{H_{n}}{n^2}\left(\frac{1-(1)^n}{2}\right)$$ $$=-\frac12\sum_{n=1}^\infty\frac{H_n}{n^2}+\frac12\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2}=\boxed{-\frac{21}{16}\zeta(3)}$$ where we used $\sum_{n=1}^\infty\frac{H_n}{n^2}=2\zeta(3)$ and $\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2}=-\frac58\zeta(3)$ Combine the boxed results, we get the claimed closed form of $S$ .","How to prove that where is the alternating harmonic number. I came up with this problem after I solved a similar one here . I managed to prove the equality above but I am not happy with my solution as I used Mathematica for calculating the blue integral below, so any better ideas and without using softwares? Thank you, My solution: In page of this paper we have Mathematica gives giving us By integration by parts and some simplifications, we get For we know that , so set For use where we used and Combine the boxed results, we get the claimed closed form of .",S=\sum_{n=0}^\infty(-1)^n(\overline{H}_n-\ln2)^3=-\frac5{16}\zeta(3) \overline{H}_n=\sum_{k=1}^n\frac{(-1)^{k-1}}{k} 105 \overline{H}_n-\ln2=(-1)^{n-1}\int_0^1\frac{x^n}{1+x}dx \Longrightarrow S=-\int_0^1\int_0^1\int_0^1\frac{1}{(1+x)(1+y)(1+z)}\sum_{n=0}^\infty(xyz)^n\ dx\ dy\ dz =-\int_0^1\int_0^1\int_0^1\frac{dx\ dy\ dz}{(1+x)(1+y)(1+z)(1-xyz)} =-\int_0^1\int_0^1\frac{dx\ dy}{(1+x)(1+y)}\left(\int_0^1\frac{dz}{(1+z)(1-xyz)}\right) =-\int_0^1\int_0^1\frac{dx\ dy}{(1+x)(1+y)}\left(-\frac{\ln(1-xy)-\ln2}{1+xy}\right) =\int_0^1\frac{dx}{1+x}\left(\int_0^1\frac{\ln(1-xy)-\ln2}{(1+y)(1+xy)}dy\right) \color{blue}{\int_0^1\frac{\ln(1-xy)-\ln2}{(1+y)(1+xy)}dy} \small{=\frac{1}{x-1}\left[\frac{\pi^2}{12}+\frac12\ln^22-\ln(1-x)\ln\left(\frac{2x}{1+x}\right)+\operatorname{Li}_2\left(\frac{1}{1+x}\right)-\operatorname{Li}_2\left(\frac{1+x}{2}\right)-\operatorname{Li}_2\left(\frac{1-x}{1+x}\right)\right]}  S=-\int_0^1\frac{\frac{\pi^2}{12}+\frac12\ln^22-\ln(1-x)\ln\left(\frac{2x}{1+x}\right)+\operatorname{Li}_2\left(\frac{1}{1+x}\right)-\operatorname{Li}_2\left(\frac{1+x}{2}\right)-\operatorname{Li}_2\left(\frac{1-x}{1+x}\right)}{1-x^2}dx S=\underbrace{2\int_0^1\tanh^{-1}x\frac{\ln(1-x)-\ln2}{1+x}dx}_{\Large\mathcal{I}_1}-\underbrace{\int_0^1\frac{\tanh^{-1}x\ln(1-x)}{x}dx}_{\Large\mathcal{I}_2} \mathcal{I}_1 \tanh^{-1}x=-\frac12\ln\left(\frac{1-x}{1+x}\right) \frac{1-x}{1+x}=u \Longrightarrow \mathcal{I}_1=\int_0^1\ln u\frac{\ln(1+u)-\ln u}{1+u}du=\boxed{-\frac{13}{8}\zeta(3)} \mathcal{I}_2 \tanh^{-1}x=\sum_{n=0}^\infty\frac{x^{2n+1}}{2n+1} \Longrightarrow \mathcal{I}_2=\sum_{n=0}^\infty\frac1{2n+1}\int_0^1 x^{2n}\ln(1-x)\ dx=-\sum_{n=0}^\infty\frac{H_{2n+1}}{(2n+1)^2} =-\sum_{n=0}^\infty\frac{H_{n+1}}{(n+1)^2}\left(\frac{1+(1)^n}{2}\right)=-\sum_{n=1}^\infty\frac{H_{n}}{n^2}\left(\frac{1-(1)^n}{2}\right) =-\frac12\sum_{n=1}^\infty\frac{H_n}{n^2}+\frac12\sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2}=\boxed{-\frac{21}{16}\zeta(3)} \sum_{n=1}^\infty\frac{H_n}{n^2}=2\zeta(3) \sum_{n=1}^\infty\frac{(-1)^nH_n}{n^2}=-\frac58\zeta(3) S,"['calculus', 'integration', 'sequences-and-series', 'alternative-proof', 'harmonic-numbers']"
40,Can a vector be integrated with respect to another vector?,Can a vector be integrated with respect to another vector?,,I know it is possible to differentiate a vector with respect to another vector. However this left me wondering if it is possible to integrate a vector with respect to another vector. Such as for example: $$ \int \vec{v} \space d\vec{v} $$ Where $\vec{v}$ is a unknown vector. Is this or something similar possible? If yes under what conditions?,I know it is possible to differentiate a vector with respect to another vector. However this left me wondering if it is possible to integrate a vector with respect to another vector. Such as for example: Where is a unknown vector. Is this or something similar possible? If yes under what conditions?, \int \vec{v} \space d\vec{v}  \vec{v},"['calculus', 'integration', 'vectors']"
41,A twisted hypergeometric series $\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2$,A twisted hypergeometric series,\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2,"Question. I was given that $$S=\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2=\frac{32}\pi G\ln2+\frac{64}\pi\Im\operatorname{Li}_3\left(\frac{1+i}2\right)-2\ln^22-\frac53\pi^2$$ where $H_n$ harmonic numbers, $G$ Catalan and $\operatorname{Li}_n$ polylogarithm. How can it be proved? My Approach. Using $\int_0^1x^{n-1}\ln(1-x)dx=-\frac{H_n}n$ one have $$S=\int_0^1-\ln(1-x)\sum_{n=1}^\infty\left(\frac{(2n)!}{4^n(n!)^2}\right)^2x^{n-1}dx =\int_0^1-\ln(1-x)\left(\frac2\pi\frac{\mathbf{K}(x)}x-\frac1x\right)dx$$ where $\mathbf{K}$ denotes elliptic integral of the first kind. The question boils down to finding $$\int_0^1\frac{\mathbf{K}(x)\ln(1-x)}xdx$$ For this integral, I tried to use the integral representation of the elliptic integral and got: $$\int_{(0,1)^3}\frac{dxdydz}{\sqrt{1-y^2}\sqrt{1-xy^2}(zx-1)}$$ This is the furthermost step I can get.","Question. I was given that where harmonic numbers, Catalan and polylogarithm. How can it be proved? My Approach. Using one have where denotes elliptic integral of the first kind. The question boils down to finding For this integral, I tried to use the integral representation of the elliptic integral and got: This is the furthermost step I can get.","S=\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2=\frac{32}\pi G\ln2+\frac{64}\pi\Im\operatorname{Li}_3\left(\frac{1+i}2\right)-2\ln^22-\frac53\pi^2 H_n G \operatorname{Li}_n \int_0^1x^{n-1}\ln(1-x)dx=-\frac{H_n}n S=\int_0^1-\ln(1-x)\sum_{n=1}^\infty\left(\frac{(2n)!}{4^n(n!)^2}\right)^2x^{n-1}dx
=\int_0^1-\ln(1-x)\left(\frac2\pi\frac{\mathbf{K}(x)}x-\frac1x\right)dx \mathbf{K} \int_0^1\frac{\mathbf{K}(x)\ln(1-x)}xdx \int_{(0,1)^3}\frac{dxdydz}{\sqrt{1-y^2}\sqrt{1-xy^2}(zx-1)}","['calculus', 'integration', 'definite-integrals', 'hypergeometric-function', 'polylogarithm']"
42,Is there a way to perform this integration such that the answer is $e^{-|y|}$?,Is there a way to perform this integration such that the answer is ?,e^{-|y|},"Consider the function $f(y)=e^{-|y|}e^{y}$ I am trying to integrate this function with respect to another variable (such as $x$ ) so that the result from the integration is $e^{-|y|}$ ? The function $f(y)$ can be changed in anyway as long as 1) the powers of $y$ and $|y|$ stay equal to one. 2) and the boundaries of integration do not include $y$ in them. 3) The result of the integral is $e^{-|y|}$ . Of course the answer may be of the form $A e^{-a|y+b|}$ where $A$ , $a$ and $b$ are constants. So for example we can add a constant or $x$ inside the $||$ or multiply $y$ . So for example the integral $$\int^{c_2}_{c1}e^{-|x y+a|}e^{y/x}dx$$ or $$\int^{c_2}_{c1}(e^{-|ay+x|}e^{y+b}+d)dx$$ Is there a way to integrate this so that the answer is $e^{-|y|}$ ? We are free to choose where to put the constants or $x$ as long as the three conditions are satisfied.","Consider the function I am trying to integrate this function with respect to another variable (such as ) so that the result from the integration is ? The function can be changed in anyway as long as 1) the powers of and stay equal to one. 2) and the boundaries of integration do not include in them. 3) The result of the integral is . Of course the answer may be of the form where , and are constants. So for example we can add a constant or inside the or multiply . So for example the integral or Is there a way to integrate this so that the answer is ? We are free to choose where to put the constants or as long as the three conditions are satisfied.",f(y)=e^{-|y|}e^{y} x e^{-|y|} f(y) y |y| y e^{-|y|} A e^{-a|y+b|} A a b x || y \int^{c_2}_{c1}e^{-|x y+a|}e^{y/x}dx \int^{c_2}_{c1}(e^{-|ay+x|}e^{y+b}+d)dx e^{-|y|} x,"['calculus', 'integration', 'definite-integrals']"
43,Solve differential equation: $f'''(x)=f(x)f'(x)f''(x)$,Solve differential equation:,f'''(x)=f(x)f'(x)f''(x),I came across $f'''(x)=f(x)f'(x)f''(x)$ but I don't know how to solve it. I tried $\frac{f'''(x)}{f''(x)}=f(x)f'(x)$ $\ln|f''(x)|=\frac{1}{2}f(x)^2+c_{1}$ But from there I have no idea how to proceed. Please help me solve this if possible.,I came across but I don't know how to solve it. I tried But from there I have no idea how to proceed. Please help me solve this if possible.,f'''(x)=f(x)f'(x)f''(x) \frac{f'''(x)}{f''(x)}=f(x)f'(x) \ln|f''(x)|=\frac{1}{2}f(x)^2+c_{1},"['calculus', 'ordinary-differential-equations']"
44,Smoothness of $\frac12[W_0(x)+W_{-1}(x)]$ for real $x<0$,Smoothness of  for real,\frac12[W_0(x)+W_{-1}(x)] x<0,"The Lambert W-function, i.e. the multivalued inverse of $z=we^w$, has countably many complex-valued branches $W_k(z)$. The relations between the branches are a bit involved and are summarized here .  We will consider the behavior of the $k=0,-1$ branches for $x<0$. Using Mathematica, we obtain the following plots of $W_k(x)$ along the negative real axis: Setting aside the green line for the moment, the two plots give the real and imaginary parts respectively of $W_0(x)$ (blue) and $W_{-1}(x)$ (orange). From this, we see that both branches are real for $x\in (-1/e,0)$. (The $k=0$ branch is additionally real for positive real $x$; no other branches obtain real values along the real line.) This is not surprising, as these branches correspond to the two real-valued inverses of $z=we^w$ along the real line. What's perhaps surprising, though, is that (according to this plot) $\overline{W_{-1}(x)}=W_{0}(x)$ for all $x\in (-\infty,-1/e)$. (A derivation of this fact may be found in the Q&A linked in the comments below.) From this, we conclude that the average of these two branches, $\frac{1}{2}(W_0(x)+W{-1}(x))$, is real for all negative real $x$. This is the green line plotted above, and from the first plot we further ascertain that $\frac12 (W_0+W_{-1})$ is smooth across the point $x=-1/e$. (Further plotting in Mathematica suggests that $\frac12 (W_0+W_{-1})$ is analytic for all $x\neq 0$ such that $-\pi < \text{arg }x \leq \pi$.) for all $x<0$ but not holomorphic across the real line.) By contrast, the two branches have square-root branching at $x=-1/e$. This last property of $\frac{1}{2}(W_0+W_{-1})$ remains mysterious to me, so my question is: Why is $\frac{1}{2}(W_0(x)+W{-1}(x))$ a smooth function for all real $x<0$?","The Lambert W-function, i.e. the multivalued inverse of $z=we^w$, has countably many complex-valued branches $W_k(z)$. The relations between the branches are a bit involved and are summarized here .  We will consider the behavior of the $k=0,-1$ branches for $x<0$. Using Mathematica, we obtain the following plots of $W_k(x)$ along the negative real axis: Setting aside the green line for the moment, the two plots give the real and imaginary parts respectively of $W_0(x)$ (blue) and $W_{-1}(x)$ (orange). From this, we see that both branches are real for $x\in (-1/e,0)$. (The $k=0$ branch is additionally real for positive real $x$; no other branches obtain real values along the real line.) This is not surprising, as these branches correspond to the two real-valued inverses of $z=we^w$ along the real line. What's perhaps surprising, though, is that (according to this plot) $\overline{W_{-1}(x)}=W_{0}(x)$ for all $x\in (-\infty,-1/e)$. (A derivation of this fact may be found in the Q&A linked in the comments below.) From this, we conclude that the average of these two branches, $\frac{1}{2}(W_0(x)+W{-1}(x))$, is real for all negative real $x$. This is the green line plotted above, and from the first plot we further ascertain that $\frac12 (W_0+W_{-1})$ is smooth across the point $x=-1/e$. (Further plotting in Mathematica suggests that $\frac12 (W_0+W_{-1})$ is analytic for all $x\neq 0$ such that $-\pi < \text{arg }x \leq \pi$.) for all $x<0$ but not holomorphic across the real line.) By contrast, the two branches have square-root branching at $x=-1/e$. This last property of $\frac{1}{2}(W_0+W_{-1})$ remains mysterious to me, so my question is: Why is $\frac{1}{2}(W_0(x)+W{-1}(x))$ a smooth function for all real $x<0$?",,"['calculus', 'complex-analysis', 'analyticity', 'lambert-w', 'branch-cuts']"
45,"Examining the convergence of $\int_{0}^{1}\left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx$",Examining the convergence of,"\int_{0}^{1}\left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx","Okay so I'm trying to determine whether $\int_{0}^{1}\left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx$ converges and if so, to what value? So the function $\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor = 0$ when $x=\frac{1}{k}$ where $k$ is a natural number, and everywhere else $\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor = 1$ From there I split the integral: \begin{align*} \int_0^1 \left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx &= \int_{1/2}^1 1 \, dx +\int_{1/3}^{1/2} 1 \, dx + \int_{1/3}^{1/4} 1 \, dx + \cdots \\[10pt] &= (1 - 1/2) + (1/2 - 1/3) + (1/3 - 1/4) + \cdots \\[10pt] &= \lim_{m \to \infty} 1-1/m = 1\;. \end{align*} Am I correct?","Okay so I'm trying to determine whether $\int_{0}^{1}\left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx$ converges and if so, to what value? So the function $\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor = 0$ when $x=\frac{1}{k}$ where $k$ is a natural number, and everywhere else $\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor = 1$ From there I split the integral: \begin{align*} \int_0^1 \left(\left\lceil \frac{1}{x} \right\rceil-\left\lfloor \frac{1}{x} \right\rfloor\right) \, dx &= \int_{1/2}^1 1 \, dx +\int_{1/3}^{1/2} 1 \, dx + \int_{1/3}^{1/4} 1 \, dx + \cdots \\[10pt] &= (1 - 1/2) + (1/2 - 1/3) + (1/3 - 1/4) + \cdots \\[10pt] &= \lim_{m \to \infty} 1-1/m = 1\;. \end{align*} Am I correct?",,['calculus']
46,Why are numeric methods the only technique available to solving $\ln(x) = \sin(x)$? Is this $x$ transcendental?,Why are numeric methods the only technique available to solving ? Is this  transcendental?,\ln(x) = \sin(x) x,"I just read this question about finding the solution to the equation $\ln(x) = \sin(x)$. All the answers focus on using a numerical method to approximate the solution. This is interesting in its own regard, but I am interested in why this is the case. Several questions immediately jump out at me: Is it really the case that there is no analytic expression that represents the solution? Are there general techniques one can use to prove this is the case for similar equations? Is there a connection between these types of solutions and transcendental numbers? Is $x$ a transcendental number? Are there additional complex solutions? Are they more easily expressed? I am not very familiar with complex analysis. For the purposes of this question, I will follow the wikipedia definition of an Analytic Expression. This allows the following constructions in the expression: Constant, Variable, Elementary arithmetic operation, Factorial, Integer exponent, N-th root, Rational exponent, Irrational exponent, Logarithm, Trigonometric function, Inverse trigonometric function, Hyperbolic function, Inverse hyperbolic function, Gamma function Bessel function, Special function, Continued fraction, Infinite series If it is more interesting, what about just a 'closed-form', which allows everything up to the Gamma function in the above list. EDIT: I've attempted to use PARI to calculate the continued fraction without much luck. I used the command ""x = exp(sin(x))"", which returns a truncated power series. I can't really find anything in the documentation for how to get this as a continued fraction; simply doing ""contfrac(x)"" just returns the power series again inside of some square brackets. Even if I could get it to display a continued fraction, it seems that PARI is resorting to a numerical method to calculate an approximate value and then displaying this approximate value in different representations. I believe this means the continued fraction series in not simply a truncated version of the ""real"" (probably) infinite continued fraction, but the entire finite sequence representation of the approximate value. If this is the case, then I cannot look at the sequence to search for a pattern... Thus question 4. Is there a numerical method for generating the continued fraction?","I just read this question about finding the solution to the equation $\ln(x) = \sin(x)$. All the answers focus on using a numerical method to approximate the solution. This is interesting in its own regard, but I am interested in why this is the case. Several questions immediately jump out at me: Is it really the case that there is no analytic expression that represents the solution? Are there general techniques one can use to prove this is the case for similar equations? Is there a connection between these types of solutions and transcendental numbers? Is $x$ a transcendental number? Are there additional complex solutions? Are they more easily expressed? I am not very familiar with complex analysis. For the purposes of this question, I will follow the wikipedia definition of an Analytic Expression. This allows the following constructions in the expression: Constant, Variable, Elementary arithmetic operation, Factorial, Integer exponent, N-th root, Rational exponent, Irrational exponent, Logarithm, Trigonometric function, Inverse trigonometric function, Hyperbolic function, Inverse hyperbolic function, Gamma function Bessel function, Special function, Continued fraction, Infinite series If it is more interesting, what about just a 'closed-form', which allows everything up to the Gamma function in the above list. EDIT: I've attempted to use PARI to calculate the continued fraction without much luck. I used the command ""x = exp(sin(x))"", which returns a truncated power series. I can't really find anything in the documentation for how to get this as a continued fraction; simply doing ""contfrac(x)"" just returns the power series again inside of some square brackets. Even if I could get it to display a continued fraction, it seems that PARI is resorting to a numerical method to calculate an approximate value and then displaying this approximate value in different representations. I believe this means the continued fraction series in not simply a truncated version of the ""real"" (probably) infinite continued fraction, but the entire finite sequence representation of the approximate value. If this is the case, then I cannot look at the sequence to search for a pattern... Thus question 4. Is there a numerical method for generating the continued fraction?",,"['calculus', 'analyticity', 'transcendental-numbers']"
47,Infinite Series -: $\psi(s)=\psi(0)+\psi_1(0)s+\psi_2(0)\frac{s^2}{2!}+\psi_3(0)\frac{s^3}{3!}+.+.+ $.,Infinite Series -: .,\psi(s)=\psi(0)+\psi_1(0)s+\psi_2(0)\frac{s^2}{2!}+\psi_3(0)\frac{s^3}{3!}+.+.+ ,"We have a given converging series using derivatives and matrices(Analogue to Taylor's series) $\psi(s)_{3 \times 3}=\psi(0)+\psi_1(0)s+\psi_2(0)\frac{s^2}{2!}+\psi_3(0)\frac{s^3}{3!}+..+.. \tag 1$. (Note the notional convention used here $\frac{\mathrm{d}^2 \psi(s) }{\mathrm{d} s^2}=\psi_2(s),\frac{\mathrm{d}^p \psi(s) }{\mathrm{d} s^p}=\psi_p(s) $) Given data and Observation in the question It is given that $ \psi_2(s)=(A+Bs)\psi_1(s)\tag 2$ where A,B are constant $3 \times 3$ skew symmetric matrices with determinant $0$  $$A=\left( \begin{array}{ccc}     0 & -c_0 & b_0 \\    c_0 & 0 & -a_0 \\   -b_0 & a_0 & 0 \\    \end{array} \right).$$ $$B=\left( \begin{array}{ccc}     0 & -c_1 & b_1 \\    c_1 & 0 & -a_1\\   -b_1 & a_1 & 0 \\    \end{array} \right).$$ Note : All entries of the matrices $A$, $B$ are constants,can't be altered $\psi_1(0)$ has determinant $1$ and  orthogonal. No information about the same property on other derivatives. $\psi_1(0),\psi(0)$, are given Question Can we re write equation (1) as $\psi(s)_{3 \times 3}=\tau(s)_{3 \times 3} \psi_1(0)+\psi(0) \tag 3$? Can we re write  $\psi(s)_{3 \times 3}$  in a finite Closed form by summing all the terms? NB :: Means what could be the finite function $\tau(s)$ which is defined with out any derivatives of $\psi(s)$. Implies you can write $\tau(s)$ using $A,B,s$ as per your convenience . Please check my attempts to solve it as answers below. If you have different idea you can write new one ATTEMPT #1#Answer When you look at the relationship  $ \psi_2(s)=(A+Bs)\psi_1(s) $ we can make recursion out of it $\psi(0)_n=A\psi(0)_{n-1}+(n-2)B \hspace{.2cm}\psi(0)_{n-2}\tag 4$ For simplicity let me write $y_n=\psi(0)_n,C_1=A,C_2=B$,because we are going to make recursion on n. All s based variables are now become constant $y_n = \left\{ 	\begin{array}{ll} 		 \psi_1(0)  & \mbox{if } n = 1 \\ 		 C_1\psi_1(0)& \mbox{if }n=2\\         C_1y_{n-1}+(n-2)C_2 y_{n-2} & \mbox{if }n> 2\\ 	\end{array} \right.\tag 5$ Question 1 Find out the solution of recursion in equation (5). The solution may  contain initial terms $y_1,y_2$(as most of the recursion solution), then substitute for  each $y_n$. Then you can take out $y_1$ as mentioned in the question.You may need to solve the infinite series after taking out $y_1$. I am trying to solve it. Till not yet successful. Expecting suggestions Question 2 Multiply by $x^n$ ,then take summation $\sum_{n=1}^{\infty}y_nx^{n}= xC_1\sum_{n=2}^{\infty}y_{n-1}x^{n-1}+x^2C_2\sum_{n=3}^{\infty} (n-2)y_{n-2}x^{n-2} \tag 6  $ $\sum_{n=1}^{\infty}y_nx^n= xC_1\sum_{n=1}^{\infty}y_{n}x^{n}+x^2C_2\sum_{n=1}^{\infty}  n y_{n}x^{n} \tag 7  $ $\sum_{n=1}^{\infty}y_nx^n= xC_1\sum_{n=1}^{\infty}y_{n}x^{n}+x^3C_2\sum_{n=1}^{\infty}  n y_{n}x^{n-1} \tag 8  $ Assume $F(x)_{3 \times 3}=\sum_{n=1}^{\infty}y_nx^n$ then we get an ODE $ F(x)=xC_1F(x)+x^3C_2F'(x)\tag 9$ Solving $F(x)$ will give solution of question number 2.More precisely $F(1)$. But the issue is I couldnt not find a solution. Expecting suggestions to solve it","We have a given converging series using derivatives and matrices(Analogue to Taylor's series) $\psi(s)_{3 \times 3}=\psi(0)+\psi_1(0)s+\psi_2(0)\frac{s^2}{2!}+\psi_3(0)\frac{s^3}{3!}+..+.. \tag 1$. (Note the notional convention used here $\frac{\mathrm{d}^2 \psi(s) }{\mathrm{d} s^2}=\psi_2(s),\frac{\mathrm{d}^p \psi(s) }{\mathrm{d} s^p}=\psi_p(s) $) Given data and Observation in the question It is given that $ \psi_2(s)=(A+Bs)\psi_1(s)\tag 2$ where A,B are constant $3 \times 3$ skew symmetric matrices with determinant $0$  $$A=\left( \begin{array}{ccc}     0 & -c_0 & b_0 \\    c_0 & 0 & -a_0 \\   -b_0 & a_0 & 0 \\    \end{array} \right).$$ $$B=\left( \begin{array}{ccc}     0 & -c_1 & b_1 \\    c_1 & 0 & -a_1\\   -b_1 & a_1 & 0 \\    \end{array} \right).$$ Note : All entries of the matrices $A$, $B$ are constants,can't be altered $\psi_1(0)$ has determinant $1$ and  orthogonal. No information about the same property on other derivatives. $\psi_1(0),\psi(0)$, are given Question Can we re write equation (1) as $\psi(s)_{3 \times 3}=\tau(s)_{3 \times 3} \psi_1(0)+\psi(0) \tag 3$? Can we re write  $\psi(s)_{3 \times 3}$  in a finite Closed form by summing all the terms? NB :: Means what could be the finite function $\tau(s)$ which is defined with out any derivatives of $\psi(s)$. Implies you can write $\tau(s)$ using $A,B,s$ as per your convenience . Please check my attempts to solve it as answers below. If you have different idea you can write new one ATTEMPT #1#Answer When you look at the relationship  $ \psi_2(s)=(A+Bs)\psi_1(s) $ we can make recursion out of it $\psi(0)_n=A\psi(0)_{n-1}+(n-2)B \hspace{.2cm}\psi(0)_{n-2}\tag 4$ For simplicity let me write $y_n=\psi(0)_n,C_1=A,C_2=B$,because we are going to make recursion on n. All s based variables are now become constant $y_n = \left\{ 	\begin{array}{ll} 		 \psi_1(0)  & \mbox{if } n = 1 \\ 		 C_1\psi_1(0)& \mbox{if }n=2\\         C_1y_{n-1}+(n-2)C_2 y_{n-2} & \mbox{if }n> 2\\ 	\end{array} \right.\tag 5$ Question 1 Find out the solution of recursion in equation (5). The solution may  contain initial terms $y_1,y_2$(as most of the recursion solution), then substitute for  each $y_n$. Then you can take out $y_1$ as mentioned in the question.You may need to solve the infinite series after taking out $y_1$. I am trying to solve it. Till not yet successful. Expecting suggestions Question 2 Multiply by $x^n$ ,then take summation $\sum_{n=1}^{\infty}y_nx^{n}= xC_1\sum_{n=2}^{\infty}y_{n-1}x^{n-1}+x^2C_2\sum_{n=3}^{\infty} (n-2)y_{n-2}x^{n-2} \tag 6  $ $\sum_{n=1}^{\infty}y_nx^n= xC_1\sum_{n=1}^{\infty}y_{n}x^{n}+x^2C_2\sum_{n=1}^{\infty}  n y_{n}x^{n} \tag 7  $ $\sum_{n=1}^{\infty}y_nx^n= xC_1\sum_{n=1}^{\infty}y_{n}x^{n}+x^3C_2\sum_{n=1}^{\infty}  n y_{n}x^{n-1} \tag 8  $ Assume $F(x)_{3 \times 3}=\sum_{n=1}^{\infty}y_nx^n$ then we get an ODE $ F(x)=xC_1F(x)+x^3C_2F'(x)\tag 9$ Solving $F(x)$ will give solution of question number 2.More precisely $F(1)$. But the issue is I couldnt not find a solution. Expecting suggestions to solve it",,"['calculus', 'sequences-and-series', 'matrices', 'derivatives', 'taylor-expansion']"
48,Convergence of a series (quite frustrating),Convergence of a series (quite frustrating),,"The sum is $$\sum_{k=1}^{\infty}\frac{1}{k}\left( \frac{\sin{k} + 2}{3}  \right) ^k$$ Here's what I've tried so far: Root test (in its stronger limsup form), gives nothing, so I didn't bother with the ratio test. This also rules out any hope of forming bounds and comparing to another (geometric) series. Actual numerical investigation, which was totally futile. Some other tests that I happen to know, which were inconclusive. How might this be done?","The sum is $$\sum_{k=1}^{\infty}\frac{1}{k}\left( \frac{\sin{k} + 2}{3}  \right) ^k$$ Here's what I've tried so far: Root test (in its stronger limsup form), gives nothing, so I didn't bother with the ratio test. This also rules out any hope of forming bounds and comparing to another (geometric) series. Actual numerical investigation, which was totally futile. Some other tests that I happen to know, which were inconclusive. How might this be done?",,"['calculus', 'sequences-and-series']"
49,The Lebesgue Integral and the Dirichlet function,The Lebesgue Integral and the Dirichlet function,,"I am considering the function $$f(x)=\begin{cases} 1 &\text{if } x\in [0,1]\setminus \Bbb Q  \\{}\\ 0 &\text{if } x\in [0,1] \cap \Bbb Q\end{cases}$$ I am trying to evaluate this using the Lebesgue integral. Quite simply, I am letting: $$E_0=\{\text{$x$ such that $x$ is rational on $[0,1]$}\}, \text{ and } E_1=\{\text{$x$ such that $x$ is irrational on $[0,1]$}\}.$$ $\mu (E_0)$ is the size of the rational numbers, I am saying is $0$ . $\mu (E_1)$ is the size of the irrational numbers, I am saying is $1$ . When I evaluate the integral; $\int_a^b f(x) \, d \mu = 0\cdot0 + 1\cdot1 = 1$ . My questions are: 1) Am I correct? 2) I am stuck on the understanding the measure of the sets.","I am considering the function I am trying to evaluate this using the Lebesgue integral. Quite simply, I am letting: is the size of the rational numbers, I am saying is . is the size of the irrational numbers, I am saying is . When I evaluate the integral; . My questions are: 1) Am I correct? 2) I am stuck on the understanding the measure of the sets.","f(x)=\begin{cases} 1 &\text{if } x\in [0,1]\setminus \Bbb Q  \\{}\\ 0 &\text{if } x\in [0,1] \cap \Bbb Q\end{cases} E_0=\{\text{x such that x is rational on [0,1]}\}, \text{ and } E_1=\{\text{x such that x is irrational on [0,1]}\}. \mu (E_0) 0 \mu (E_1) 1 \int_a^b f(x) \, d \mu = 0\cdot0 + 1\cdot1 = 1","['calculus', 'analysis']"
50,Calculus in ordered fields,Calculus in ordered fields,,"Is there any ordered field smaller that the set of real numbers in which we can do calculus, also with many restrictions ? If not why ?","Is there any ordered field smaller that the set of real numbers in which we can do calculus, also with many restrictions ? If not why ?",,"['calculus', 'ordered-fields']"
51,Dimensions and Space-filling curves,Dimensions and Space-filling curves,,"If a space-filling curve can continuously map a 1-dimensional interval onto a 2-dimensional region, then what actually makes the region 2-dimensional? Doesn't this mean that only 1-dimension is required to actually describe all the points? Furthermore, couldn't a space-filling curve reduce a surface integral into a line integral? I feel like I'm missing a core-concept here and I would like to know what it is.","If a space-filling curve can continuously map a 1-dimensional interval onto a 2-dimensional region, then what actually makes the region 2-dimensional? Doesn't this mean that only 1-dimension is required to actually describe all the points? Furthermore, couldn't a space-filling curve reduce a surface integral into a line integral? I feel like I'm missing a core-concept here and I would like to know what it is.",,"['calculus', 'general-topology', 'geometry']"
52,Limit of a sequence with indeterminate form,Limit of a sequence with indeterminate form,,Let $\displaystyle u_n =\frac{n}{2}-\sum_{k=1}^n\frac{n^2}{(n+k)^2}$. The question is: Find the limit of the sequence $(u_n)$. The problem is if we write $\displaystyle u_n=n\left(\frac{1}{2}-\frac{1}{n}\sum_{k=1}^n\frac{1}{(1+\frac{k}{n})^2}\right)$ and we use the fact that the limit of Riemann sum $\displaystyle \frac{1}{n}\sum_{k=1}^n\frac{1}{(1+\frac{k}{n})^2}$ is $\displaystyle \int_0^1 \frac{dx}{(1+x)^2}=\frac{1}{2}$ we find the indeterminate form $\infty\times 0$. How can we avoid this problem? Thanks for help.,Let $\displaystyle u_n =\frac{n}{2}-\sum_{k=1}^n\frac{n^2}{(n+k)^2}$. The question is: Find the limit of the sequence $(u_n)$. The problem is if we write $\displaystyle u_n=n\left(\frac{1}{2}-\frac{1}{n}\sum_{k=1}^n\frac{1}{(1+\frac{k}{n})^2}\right)$ and we use the fact that the limit of Riemann sum $\displaystyle \frac{1}{n}\sum_{k=1}^n\frac{1}{(1+\frac{k}{n})^2}$ is $\displaystyle \int_0^1 \frac{dx}{(1+x)^2}=\frac{1}{2}$ we find the indeterminate form $\infty\times 0$. How can we avoid this problem? Thanks for help.,,[]
53,Second derivative of a vector field,Second derivative of a vector field,,"I wonder how to treat the ""second derivative"" of a vector field. For example, imagine we have a vector field $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$. Then we evaluate the derivative at two points $Df(a)$ and $Df(b)$ which are matrices! Now, $$D[Df(a)Df(b)] = D^2f(a)Df(b)+Df(a)D^2f(b).$$ My question is, what is $D^2f(a)$? How can I treat this? I imagine is something identifiable with $\mathbb{R}^{n\times n \times n}$. In such a case, if I wish to compute the ""matrix"" norm of $D[Df(a)Df(b)]$ (as the sum of all entries) is this then the sum of all possible combinations of $$\frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j} \frac{\partial}{\partial x_k} f(a)  \ ?$$ Thank you very much for your help!","I wonder how to treat the ""second derivative"" of a vector field. For example, imagine we have a vector field $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$. Then we evaluate the derivative at two points $Df(a)$ and $Df(b)$ which are matrices! Now, $$D[Df(a)Df(b)] = D^2f(a)Df(b)+Df(a)D^2f(b).$$ My question is, what is $D^2f(a)$? How can I treat this? I imagine is something identifiable with $\mathbb{R}^{n\times n \times n}$. In such a case, if I wish to compute the ""matrix"" norm of $D[Df(a)Df(b)]$ (as the sum of all entries) is this then the sum of all possible combinations of $$\frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j} \frac{\partial}{\partial x_k} f(a)  \ ?$$ Thank you very much for your help!",,"['calculus', 'multivariable-calculus', 'derivatives', 'vector-fields']"
54,A complex approach to integral of cardinal sine [duplicate],A complex approach to integral of cardinal sine [duplicate],,"This question already has answers here : Evaluating the integral $\int_0^\infty \frac{\sin x} x \,\mathrm dx = \frac \pi 2$? (32 answers) Closed 6 years ago . this integral: $$\int_0^{+\infty}\frac{\sin x}{x}\text{d}x=\frac{\pi}{2}$$ is very famous and had been discussed in the past days in this forum. and I have learned some elegant way to computer it. for example: using the identity: $\int_0^{+\infty}e^{-xy}\sin x\text{d}x=\frac{1}{1+y^2}$ and $\int_0^{\infty}\int_0^{\infty}e^{-xy}\sin x\text{d}y\text{d}x$ and Fubini theorem. the link is here: Post concern with sine integral In this post, I want to discuss another way to computer it. since $$\int_0^{+\infty}\frac{\sin x}{x}\text{d}x=\frac{1}{2i}\int_{-\infty}^{+\infty}\frac{e^{ix}-1}{x}\text{d}x$$ this fact inspire me to consider the complex integral: $$\int_{\Gamma}\frac{e^{iz}-1}{z}\text{d}z$$ and $\Gamma$ is the red path in the above figure, with counter-clockwise orientation, by Cauchy's theorem, we have $$\int_{\Gamma}\frac{e^{iz}-1}{z}\text{d}z=0$$ the above integral can be written as: $$\int_{-R}^{-\epsilon}\frac{e^{ix}-1}{x}\text{d}x+\int_{\Gamma_{\epsilon}}\frac{e^{iz}-1}{z}\text{d}z+\int_{\epsilon}^{R}\frac{e^{ix}-1}{x}\text{d}x+\int_{\Gamma_{R}}\frac{e^{iz}-1}{z}\text{d}z$$ Let $R\rightarrow +\infty$ and $\epsilon \rightarrow 0$ , we have: $$\int_{-R}^{-\epsilon}\frac{e^{ix}-1}{x}\text{d}x+\int_{\epsilon}^{R}\frac{e^{ix}-1}{x}\text{d}x \rightarrow \int_{-\infty}^{+\infty}\frac{e^{ix}-1}{x}\text{d}x=2i\int_0^{+\infty}\frac{\sin x}{x}\text{d}x$$ and $$\int_{\Gamma_{\epsilon}}\frac{e^{iz}-1}{z}\text{d}z=\int_\pi^0\frac{e^{i\epsilon e^{i\theta}}-1}{\epsilon e^{i\theta}}i\epsilon e^{i\theta}\text{d}\theta=i\int_\pi^0(\cos(\epsilon e^{i\theta})+i\sin(\epsilon e^{i\theta})-1)\text{d}\theta \rightarrow 0$$ as $\epsilon \rightarrow 0$ so I am expecting that: $$\int_{\Gamma_{R}}\frac{e^{iz}-1}{z}\text{d}z=-i\pi$$ when $$R \rightarrow +\infty$$ but I can't find it. Could you help me? Thanks very much.","This question already has answers here : Evaluating the integral $\int_0^\infty \frac{\sin x} x \,\mathrm dx = \frac \pi 2$? (32 answers) Closed 6 years ago . this integral: is very famous and had been discussed in the past days in this forum. and I have learned some elegant way to computer it. for example: using the identity: and and Fubini theorem. the link is here: Post concern with sine integral In this post, I want to discuss another way to computer it. since this fact inspire me to consider the complex integral: and is the red path in the above figure, with counter-clockwise orientation, by Cauchy's theorem, we have the above integral can be written as: Let and , we have: and as so I am expecting that: when but I can't find it. Could you help me? Thanks very much.",\int_0^{+\infty}\frac{\sin x}{x}\text{d}x=\frac{\pi}{2} \int_0^{+\infty}e^{-xy}\sin x\text{d}x=\frac{1}{1+y^2} \int_0^{\infty}\int_0^{\infty}e^{-xy}\sin x\text{d}y\text{d}x \int_0^{+\infty}\frac{\sin x}{x}\text{d}x=\frac{1}{2i}\int_{-\infty}^{+\infty}\frac{e^{ix}-1}{x}\text{d}x \int_{\Gamma}\frac{e^{iz}-1}{z}\text{d}z \Gamma \int_{\Gamma}\frac{e^{iz}-1}{z}\text{d}z=0 \int_{-R}^{-\epsilon}\frac{e^{ix}-1}{x}\text{d}x+\int_{\Gamma_{\epsilon}}\frac{e^{iz}-1}{z}\text{d}z+\int_{\epsilon}^{R}\frac{e^{ix}-1}{x}\text{d}x+\int_{\Gamma_{R}}\frac{e^{iz}-1}{z}\text{d}z R\rightarrow +\infty \epsilon \rightarrow 0 \int_{-R}^{-\epsilon}\frac{e^{ix}-1}{x}\text{d}x+\int_{\epsilon}^{R}\frac{e^{ix}-1}{x}\text{d}x \rightarrow \int_{-\infty}^{+\infty}\frac{e^{ix}-1}{x}\text{d}x=2i\int_0^{+\infty}\frac{\sin x}{x}\text{d}x \int_{\Gamma_{\epsilon}}\frac{e^{iz}-1}{z}\text{d}z=\int_\pi^0\frac{e^{i\epsilon e^{i\theta}}-1}{\epsilon e^{i\theta}}i\epsilon e^{i\theta}\text{d}\theta=i\int_\pi^0(\cos(\epsilon e^{i\theta})+i\sin(\epsilon e^{i\theta})-1)\text{d}\theta \rightarrow 0 \epsilon \rightarrow 0 \int_{\Gamma_{R}}\frac{e^{iz}-1}{z}\text{d}z=-i\pi R \rightarrow +\infty,"['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'improper-integrals']"
55,Optimization of the area of a cross inscribed in a circle,Optimization of the area of a cross inscribed in a circle,,"I've really been scratching my head over this optimization problem. ""Consider a symmetric cross inscribed in a circle of radius $r$."" The length from the center of the cross to the middle of one of its arms is $x$. Also, the angle between two line segments drawn from the cross's center to the vertices of one of its arms has a measure of $\theta$. Here's a diagram: There are three parts to the problem: ""(a) Write the area $A$ of the cross as a function of $x$ and find the value of $x$ that maximizes the area. (b) Write the area $A$ of the cross as a function of $\theta$ and find the value of $\theta$ that maximizes the area. (c) Show that the critical numbers of parts (a) and (b) yield the same maximum area. What is that area?"" So, let me show you what I've done so far. For part (a), I decided to break the cross into two middle rectangles and two side rectangles. I saw that a middle rectangle (from the center to the top) would have an area of $$x \cdot 2 \sqrt{r^2 - x^2}$$ using the Pythagorean theorem. I worked out that a side rectangle (the remaining area on the right, adjacent to the middle rectangles) would have an area of $$2 \sqrt{r^2-x^2} \cdot \left( x - \sqrt{r^2 - x^2} \right) .$$ So, the area of the cross is $$A = 2 \bigg( x \cdot 2 \sqrt{r^2 - x^2} + 2 \sqrt{r^2 - x^2} \cdot \Big( x - \sqrt{r^2 - x^2} \Big) \bigg) = 8x \sqrt{r^2 - x^2} - 4r^2 + 4x^2 .$$ If my math is right there (fingers crossed), then I'll take the first derivative to locate a maximum. $$A^\prime = 8 \sqrt{r^2 - x^2} + 8x \left( 1 \over 2 \right) \left( r^2 - x^2 \right)^{- {1 \over 2}} \left( -2x \right) + 8x.$$ I was a little unsure about what to do at this point. I plugged the $A^\prime$ equation into my graphing calculator, substituting $1^2$ for $r^2$ (for a radius of $1$). The graph crosses the $x$-axis at $x \approx 0.85$. Substituting $2^2$ for $r^2$ (for a radius of $2$) gives me $x \approx 1.70$. From this, I concluded that $$A^\prime = 0 \; \mathbf{at} \; x \approx 0.85r.$$ Analysis of graphs of $A$ for various values of $r$ concludes that, indeed, maxima do appear at $x \approx 0.85r$. So, I have the function $A$ in terms of $x$, but I'm curious: What should my final answer be for the second part of (a)? All I have is $x \approx 0.85r$. Is that a sufficient answer? As for part (b), I really have no idea how to write $A$ in terms of $\theta$. I know that $\text{area} = {1 \over 2} b \cdot c \cdot \sin A$ for triangles, but I really need help writing the area of this cross in terms of $\theta$ . Part (c) should be easy enough once I finish (b). If you got to the end of this, I sincerely thank you for reading, and I would really appreciate an answer (and any corrections to my math). Thanks!","I've really been scratching my head over this optimization problem. ""Consider a symmetric cross inscribed in a circle of radius $r$."" The length from the center of the cross to the middle of one of its arms is $x$. Also, the angle between two line segments drawn from the cross's center to the vertices of one of its arms has a measure of $\theta$. Here's a diagram: There are three parts to the problem: ""(a) Write the area $A$ of the cross as a function of $x$ and find the value of $x$ that maximizes the area. (b) Write the area $A$ of the cross as a function of $\theta$ and find the value of $\theta$ that maximizes the area. (c) Show that the critical numbers of parts (a) and (b) yield the same maximum area. What is that area?"" So, let me show you what I've done so far. For part (a), I decided to break the cross into two middle rectangles and two side rectangles. I saw that a middle rectangle (from the center to the top) would have an area of $$x \cdot 2 \sqrt{r^2 - x^2}$$ using the Pythagorean theorem. I worked out that a side rectangle (the remaining area on the right, adjacent to the middle rectangles) would have an area of $$2 \sqrt{r^2-x^2} \cdot \left( x - \sqrt{r^2 - x^2} \right) .$$ So, the area of the cross is $$A = 2 \bigg( x \cdot 2 \sqrt{r^2 - x^2} + 2 \sqrt{r^2 - x^2} \cdot \Big( x - \sqrt{r^2 - x^2} \Big) \bigg) = 8x \sqrt{r^2 - x^2} - 4r^2 + 4x^2 .$$ If my math is right there (fingers crossed), then I'll take the first derivative to locate a maximum. $$A^\prime = 8 \sqrt{r^2 - x^2} + 8x \left( 1 \over 2 \right) \left( r^2 - x^2 \right)^{- {1 \over 2}} \left( -2x \right) + 8x.$$ I was a little unsure about what to do at this point. I plugged the $A^\prime$ equation into my graphing calculator, substituting $1^2$ for $r^2$ (for a radius of $1$). The graph crosses the $x$-axis at $x \approx 0.85$. Substituting $2^2$ for $r^2$ (for a radius of $2$) gives me $x \approx 1.70$. From this, I concluded that $$A^\prime = 0 \; \mathbf{at} \; x \approx 0.85r.$$ Analysis of graphs of $A$ for various values of $r$ concludes that, indeed, maxima do appear at $x \approx 0.85r$. So, I have the function $A$ in terms of $x$, but I'm curious: What should my final answer be for the second part of (a)? All I have is $x \approx 0.85r$. Is that a sufficient answer? As for part (b), I really have no idea how to write $A$ in terms of $\theta$. I know that $\text{area} = {1 \over 2} b \cdot c \cdot \sin A$ for triangles, but I really need help writing the area of this cross in terms of $\theta$ . Part (c) should be easy enough once I finish (b). If you got to the end of this, I sincerely thank you for reading, and I would really appreciate an answer (and any corrections to my math). Thanks!",,"['calculus', 'geometry', 'optimization']"
56,What is the significance of the Increment Theorem in non-standard analysis?,What is the significance of the Increment Theorem in non-standard analysis?,,"A bit of background: I'm an engineer, not a mathematician, and I need to review and improve my calculus.  In college, I never liked how they said $dy/dx$ was a single symbol, not a ratio; and then proceeded to write things like $dy = f(x) dx$ and integrate.  So I'm trying a different angle this time, and reading the textbook Elementary Calculus: An Infinitesimal Approach , by H. Jerome Keisler, which is available online. I'm at the part (p.55) that discusses the Increment Theorem .  It says: Let $y = f(x)$. Suppose $f'(x)$ exists at a certain point x, and $\Delta x$    is infinitesimal.  Then $\Delta y$ is infinitesimal, and $\Delta y = f'(x)\Delta x + \epsilon\Delta x$ for some infinitesimal $\epsilon$, which depends on $x$ and $\Delta x$. And then he works some examples, finding $\epsilon$. For example, with $y = x^3$... $$ y' = 3x^2 \\ \Delta y = (x + \Delta x)^3 - x^3 \\ \epsilon = \Delta y / \Delta x - y' \\ ...\\ \epsilon = 3x \Delta x + (\Delta x)^2 $$ I'm left wondering... what is the point?  Where are we going with this? We seem to be revisiting the definition of the derivative, where $\epsilon$ is the part of the equation that we were able to discard because it was infinitesimal.  For example, to get the derivative of $y = x^3$ $$ st( \frac{\Delta y}{\Delta x} ) = st(\frac{(x + \Delta x)^3 - x^3}{\Delta x}) \\ = st(3x^2 + 3x\Delta x + (\Delta x)^2) \\  = 3x^2 $$","A bit of background: I'm an engineer, not a mathematician, and I need to review and improve my calculus.  In college, I never liked how they said $dy/dx$ was a single symbol, not a ratio; and then proceeded to write things like $dy = f(x) dx$ and integrate.  So I'm trying a different angle this time, and reading the textbook Elementary Calculus: An Infinitesimal Approach , by H. Jerome Keisler, which is available online. I'm at the part (p.55) that discusses the Increment Theorem .  It says: Let $y = f(x)$. Suppose $f'(x)$ exists at a certain point x, and $\Delta x$    is infinitesimal.  Then $\Delta y$ is infinitesimal, and $\Delta y = f'(x)\Delta x + \epsilon\Delta x$ for some infinitesimal $\epsilon$, which depends on $x$ and $\Delta x$. And then he works some examples, finding $\epsilon$. For example, with $y = x^3$... $$ y' = 3x^2 \\ \Delta y = (x + \Delta x)^3 - x^3 \\ \epsilon = \Delta y / \Delta x - y' \\ ...\\ \epsilon = 3x \Delta x + (\Delta x)^2 $$ I'm left wondering... what is the point?  Where are we going with this? We seem to be revisiting the definition of the derivative, where $\epsilon$ is the part of the equation that we were able to discard because it was infinitesimal.  For example, to get the derivative of $y = x^3$ $$ st( \frac{\Delta y}{\Delta x} ) = st(\frac{(x + \Delta x)^3 - x^3}{\Delta x}) \\ = st(3x^2 + 3x\Delta x + (\Delta x)^2) \\  = 3x^2 $$",,"['calculus', 'nonstandard-analysis']"
57,Calculus - can I do this?,Calculus - can I do this?,,"Apologies if this is a dumb question. I learned at school that I can differentiate $$y=x^{2}$$  to give $$\frac{dy}{dx}=2x.$$  But, if I have a multivariable function, for example$$y=4x^{2}+3z+t^{3}$$  am I allowed to differentiate it to give$$dy=8x\;dx+3\;dz+3t^{2}\;dt$$  and, if valid, what is this procedure called exactly? Thank you","Apologies if this is a dumb question. I learned at school that I can differentiate $$y=x^{2}$$  to give $$\frac{dy}{dx}=2x.$$  But, if I have a multivariable function, for example$$y=4x^{2}+3z+t^{3}$$  am I allowed to differentiate it to give$$dy=8x\;dx+3\;dz+3t^{2}\;dt$$  and, if valid, what is this procedure called exactly? Thank you",,"['calculus', 'multivariable-calculus']"
58,Understanding the line integral,Understanding the line integral,,"I have some trouble understanding every component of the line integral formula. Say I have a curve $c : [a,b] \mapsto \mathbb R^n$ and a scalar field $f : \mathbb{R^n} \mapsto \mathbb{R}$. According to Wikipedia, the integral equation is then: $$\int_c f \;ds = \int_a^b f(c(t)) |c'(t)| \;dt$$ I understand that $f(c(t))$ is the value of the scalar field on each point on the curve, and that $\int_c ds = \int_c |c'(t)|\;dt$ is the length of the curve. Things I don't understand: What is $|h(x)|$, in general? Does it have any meaning outside the context of arc length? Is the result of the line integral the sum of all values of $f$ along the curve?... ... If yes, why is must we multiply $f$ by $ds$?","I have some trouble understanding every component of the line integral formula. Say I have a curve $c : [a,b] \mapsto \mathbb R^n$ and a scalar field $f : \mathbb{R^n} \mapsto \mathbb{R}$. According to Wikipedia, the integral equation is then: $$\int_c f \;ds = \int_a^b f(c(t)) |c'(t)| \;dt$$ I understand that $f(c(t))$ is the value of the scalar field on each point on the curve, and that $\int_c ds = \int_c |c'(t)|\;dt$ is the length of the curve. Things I don't understand: What is $|h(x)|$, in general? Does it have any meaning outside the context of arc length? Is the result of the line integral the sum of all values of $f$ along the curve?... ... If yes, why is must we multiply $f$ by $ds$?",,"['calculus', 'integration']"
59,Question about square-wheeled cars,Question about square-wheeled cars,,"It's kind of an infamous problem in differential equations to find the correct road surface so that a car with square wheels (and an axle located in the center) keeps its axle level as it drives along.  I hope I won't offend anybody by saying that one smooth piece of the solution (for a wheel with sides of length 2) is $y = -\cosh(x)$ If you actually take this solution and describe the position of the axle at any given point, unless I have calculated incorrectly you find that the axle is always positioned directly over the point where the wheel makes contact with the road.  I've been unable to come up with a physical justification of this phenomenon and it seems fairly non-obvious to me. Is there a straightforward reason why this must be true?  Is it specific to this wheel shape?","It's kind of an infamous problem in differential equations to find the correct road surface so that a car with square wheels (and an axle located in the center) keeps its axle level as it drives along.  I hope I won't offend anybody by saying that one smooth piece of the solution (for a wheel with sides of length 2) is $y = -\cosh(x)$ If you actually take this solution and describe the position of the axle at any given point, unless I have calculated incorrectly you find that the axle is always positioned directly over the point where the wheel makes contact with the road.  I've been unable to come up with a physical justification of this phenomenon and it seems fairly non-obvious to me. Is there a straightforward reason why this must be true?  Is it specific to this wheel shape?",,"['calculus', 'ordinary-differential-equations']"
60,Proving $\int_0^{1/2}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3)$,Proving,\int_0^{1/2}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3),"By comparing some results, I found that $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3).\tag{1}$$ I tried to prove it starting with applying IBP: $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2\left(-\frac12\right)-\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx$$ then using the fact that $\ln(1-x)\ln(1+x)=\frac14\ln^2(1-x^2)-\frac14\ln^2\left(\frac{1-x}{1+x}\right)$ : $$\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2(1-x^2)}{x}dx}_{1-x^2\to x}-\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2\left(\frac{1-x}{1+x}\right)}{x}dx}_{(1-x)/(1+x)\to x}$$ $$=\frac18\int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx.$$ Using: \begin{gather} \int\frac{\ln^2(x)}{1-x}dx=\sum_{n=1}^\infty\int x^{n-1}\ln^2(x)dx\\\ \overset{\text{IBP}}{=}\sum_{n=1}^\infty\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\ =-\ln^2(x)\ln(1-x)-2\ln(x)\operatorname{Li}_2(x)+2\operatorname{Li}_3(x), \end{gather} and \begin{gather} \int\frac{\ln^2(x)}{1+x}dx=\sum_{n=1}^\infty(-1)^{n-1}\int x^{n-1}\ln^2(x)dx\\\ \overset{\text{IBP}}{=}\sum_{n=1}^\infty (-1)^{n-1}\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\\ =\ln^2(x)\ln(1+x)+2\ln(x)\operatorname{Li}_2(-x)-2\operatorname{Li}_3(-x). \end{gather} we have: $$\int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)-2\ln(2)\ln^2(3/4)+2\ln(3/4)\text{Li}_2(3/4)-2\text{Li}_3(3/4),$$ $$\int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)+\ln^2(3)\ln(2/3)-2\ln(3)\text{Li}_2(1/3)-2\text{Li}_3(1/3),$$ $$\int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx=\frac32\zeta(3)-\ln^2(3)\ln(4/3)+2\ln(3)\text{Li}_2(-1/3)+2\text{Li}_3(-1/3).$$ Combining the three integrals, we get $$\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))-\frac13\text{Li}_3(3/4)$$ $$+\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))+\frac14\ln(3/4)\text{Li}_2(3/4)$$ $$-\frac14\ln(2)\ln^2(3/4)+\frac14\ln(2)\ln^2(3)-\frac58\zeta(3)$$ and finally $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2(-1/2)-\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))+\frac13\text{Li}_3(3/4)$$ $$-\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))-\frac14\ln(3/4)\text{Li}_2(3/4)$$ $$+\frac14\ln(2)\ln^2(3/4)-\frac14\ln(2)\ln^2(3)+\frac58\zeta(3).$$ and I think by using the polylogarithm identities, we can simplify this result into (1). My question is how to prove (1) without going through all this mess if possible? Edit :  I also tried the Cauchy product $$\left(\sum_{n=1}^\infty a_n x^n\right)\left(\sum_{n=1}^\infty b_n x^n\right)=\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n a_k b_{n-k+1}\right)$$ of the integrand: $$\frac{\text{Li}_2(-x)}{1-x}=\left(\text{Li}_2(-x)\right)\left(\frac1{1-x}\right)=\left(\sum_{n=1}^\infty\frac{(-1)^n x^n}{n^2}\right)\left(\frac1x\sum_{n=1}^\infty x^{n}\right)$$ take $a_n=\frac{(-1)^n}{n^2}$ and $b_n=1=n^0$ $$=\frac1x\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n\frac{(-1)^k(n-k+1)^0}{k^2}\right)=\sum_{n=1}^\infty x^{n}\left(\sum_{k=1}^n\frac{(-1)^k}{k^2}\right).$$ By using the definition of the $n$ th generalized skew harmonic number of order $2$ : $$\overline{H}_n^{(2)}=\sum_{k=1}^n\frac{(-1)^{k-1}}{k^2}$$ we have $$\frac{\text{Li}_2(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(2)}$$ or in general: $$\frac{\text{Li}_a(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(a)}.$$ Employing this series expansion, we have $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty  \overline{H}_n^{(2)}\int_0^{\frac12}x^n dx$$ $$=-\sum_{n=1}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}}$$ let the index start from zero since $\overline{H}_0^{(a)}=0$ $$=-\sum_{n=0}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}}$$ shift the index $$=-\sum_{n=1}^\infty \frac{\overline{H}_{n-1}^{(2)}}{n2^{n}}.$$ Write $\overline{H}_{n-1}^{(2)}=\overline{H}_{n}^{(2)}+\frac{(-1)^n}{n^2}$ , we get $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\sum_{n=1}^\infty \frac{(-1)^n}{n^32^{n}}=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\text{Li}_3\left(-\frac12\right).$$ Now we need to find this sum, which is, by comparing with (1), equal to $\frac{13}{24}\zeta(3).$","By comparing some results, I found that I tried to prove it starting with applying IBP: then using the fact that : Using: and we have: Combining the three integrals, we get and finally and I think by using the polylogarithm identities, we can simplify this result into (1). My question is how to prove (1) without going through all this mess if possible? Edit :  I also tried the Cauchy product of the integrand: take and By using the definition of the th generalized skew harmonic number of order : we have or in general: Employing this series expansion, we have let the index start from zero since shift the index Write , we get Now we need to find this sum, which is, by comparing with (1), equal to","\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3).\tag{1} \int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2\left(-\frac12\right)-\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx \ln(1-x)\ln(1+x)=\frac14\ln^2(1-x^2)-\frac14\ln^2\left(\frac{1-x}{1+x}\right) \int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2(1-x^2)}{x}dx}_{1-x^2\to x}-\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2\left(\frac{1-x}{1+x}\right)}{x}dx}_{(1-x)/(1+x)\to x} =\frac18\int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx. \begin{gather}
\int\frac{\ln^2(x)}{1-x}dx=\sum_{n=1}^\infty\int x^{n-1}\ln^2(x)dx\\\
\overset{\text{IBP}}{=}\sum_{n=1}^\infty\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\
=-\ln^2(x)\ln(1-x)-2\ln(x)\operatorname{Li}_2(x)+2\operatorname{Li}_3(x),
\end{gather} \begin{gather}
\int\frac{\ln^2(x)}{1+x}dx=\sum_{n=1}^\infty(-1)^{n-1}\int x^{n-1}\ln^2(x)dx\\\
\overset{\text{IBP}}{=}\sum_{n=1}^\infty (-1)^{n-1}\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\\
=\ln^2(x)\ln(1+x)+2\ln(x)\operatorname{Li}_2(-x)-2\operatorname{Li}_3(-x).
\end{gather} \int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)-2\ln(2)\ln^2(3/4)+2\ln(3/4)\text{Li}_2(3/4)-2\text{Li}_3(3/4), \int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)+\ln^2(3)\ln(2/3)-2\ln(3)\text{Li}_2(1/3)-2\text{Li}_3(1/3), \int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx=\frac32\zeta(3)-\ln^2(3)\ln(4/3)+2\ln(3)\text{Li}_2(-1/3)+2\text{Li}_3(-1/3). \int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))-\frac13\text{Li}_3(3/4) +\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))+\frac14\ln(3/4)\text{Li}_2(3/4) -\frac14\ln(2)\ln^2(3/4)+\frac14\ln(2)\ln^2(3)-\frac58\zeta(3) \int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2(-1/2)-\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))+\frac13\text{Li}_3(3/4) -\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))-\frac14\ln(3/4)\text{Li}_2(3/4) +\frac14\ln(2)\ln^2(3/4)-\frac14\ln(2)\ln^2(3)+\frac58\zeta(3). \left(\sum_{n=1}^\infty a_n x^n\right)\left(\sum_{n=1}^\infty b_n x^n\right)=\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n a_k b_{n-k+1}\right) \frac{\text{Li}_2(-x)}{1-x}=\left(\text{Li}_2(-x)\right)\left(\frac1{1-x}\right)=\left(\sum_{n=1}^\infty\frac{(-1)^n x^n}{n^2}\right)\left(\frac1x\sum_{n=1}^\infty x^{n}\right) a_n=\frac{(-1)^n}{n^2} b_n=1=n^0 =\frac1x\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n\frac{(-1)^k(n-k+1)^0}{k^2}\right)=\sum_{n=1}^\infty x^{n}\left(\sum_{k=1}^n\frac{(-1)^k}{k^2}\right). n 2 \overline{H}_n^{(2)}=\sum_{k=1}^n\frac{(-1)^{k-1}}{k^2} \frac{\text{Li}_2(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(2)} \frac{\text{Li}_a(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(a)}. \int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty  \overline{H}_n^{(2)}\int_0^{\frac12}x^n dx =-\sum_{n=1}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}} \overline{H}_0^{(a)}=0 =-\sum_{n=0}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}} =-\sum_{n=1}^\infty \frac{\overline{H}_{n-1}^{(2)}}{n2^{n}}. \overline{H}_{n-1}^{(2)}=\overline{H}_{n}^{(2)}+\frac{(-1)^n}{n^2} \int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\sum_{n=1}^\infty \frac{(-1)^n}{n^32^{n}}=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\text{Li}_3\left(-\frac12\right). \frac{13}{24}\zeta(3).","['calculus', 'integration', 'definite-integrals', 'alternative-proof', 'polylogarithm']"
61,If $f\left(\pi\right)=\pi$ and $\int_{0}^{\pi}\left(f\left(x\right)+f''\left(x\right)\right)\sin x\ dx\ =\ 7\pi$ then find $f\left(0\right)$,If  and  then find,f\left(\pi\right)=\pi \int_{0}^{\pi}\left(f\left(x\right)+f''\left(x\right)\right)\sin x\ dx\ =\ 7\pi f\left(0\right),"$\color{orange}{\mathrm{Question:}}$ If $f\left(\pi\right)=\pi$ and $\int_{0}^{\pi}\left(f (x)+f''(x)\right)\sin x\ dx\ =\ 7\pi$ then find $f(0)$ given that $f(x)$ is continuous in $\left[0,\pi\right]$ $\color{green}{\mathrm{Solution:}}$ Given: $$\int_{0}^{\pi}\left(f(x)+f''(x)\right)\sin x\ dx\ =\ \int_{0}^{\pi}f(x)\sin x\ dx\ +\int_{0}^{\pi}f''(x)\sin x\ dx$$ By ILATE ( Integration by parts), keeping $f''(x)$ as the first function and $\sin x$ as the second function: $$7\pi\ =\int_{0}^{\pi}f(x)\sin x\ dx\ +\ \left[\sin x\cdot f'(x)-\int_{0}^{\pi}\cos x\cdot f'(x)dx\right]$$ $$7\pi\ =\int_{0}^{\pi}f(x)\sin x\ dx\ +\ \left[\sin x\cdot f'(x)-\left[\cos x\cdot f(x)-\int_{0}^{\pi}\left(-\sin x\right)\left(f(x)dx\right)\right]\right]$$ $$7\pi=\sin x\cdot f'(x)-\cos x\cdot f(x)$$ The limits being of integration being from $0$ to $\pi$ , (sorry i don't know Latex much :( ) $$7\pi=\left[\sin\pi\cdot f'(\pi)-\sin0\cdot f'(0)\right]-\left[\cos\pi\cdot f(\pi)-\cos0\cdot f(0)\right]$$ Solving this I  got $f(0)=6\pi$ , which is the correct answer, no issues with that but... $\color{pink}{\mathrm{Doubt}}$ When using the ILATE rule, we don't know what kind of function $f(x)$ is, so how can we decide whether to take it as the first function or the second function, I just did that for my convenience because I thought that will give me the solution. Secondly, what is the importance of the statement of the question: $f(x)$ is continuous? $\color{red}{\mathrm{Edit}}$ Basically it looks like ILATE is not a very good rule and Integration by parts is OP!","If and then find given that is continuous in Given: By ILATE ( Integration by parts), keeping as the first function and as the second function: The limits being of integration being from to , (sorry i don't know Latex much :( ) Solving this I  got , which is the correct answer, no issues with that but... When using the ILATE rule, we don't know what kind of function is, so how can we decide whether to take it as the first function or the second function, I just did that for my convenience because I thought that will give me the solution. Secondly, what is the importance of the statement of the question: is continuous? Basically it looks like ILATE is not a very good rule and Integration by parts is OP!","\color{orange}{\mathrm{Question:}} f\left(\pi\right)=\pi \int_{0}^{\pi}\left(f (x)+f''(x)\right)\sin x\ dx\ =\ 7\pi f(0) f(x) \left[0,\pi\right] \color{green}{\mathrm{Solution:}} \int_{0}^{\pi}\left(f(x)+f''(x)\right)\sin x\ dx\ =\ \int_{0}^{\pi}f(x)\sin x\ dx\ +\int_{0}^{\pi}f''(x)\sin x\ dx f''(x) \sin x 7\pi\ =\int_{0}^{\pi}f(x)\sin x\ dx\ +\ \left[\sin x\cdot f'(x)-\int_{0}^{\pi}\cos x\cdot f'(x)dx\right] 7\pi\ =\int_{0}^{\pi}f(x)\sin x\ dx\ +\ \left[\sin x\cdot f'(x)-\left[\cos x\cdot f(x)-\int_{0}^{\pi}\left(-\sin x\right)\left(f(x)dx\right)\right]\right] 7\pi=\sin x\cdot f'(x)-\cos x\cdot f(x) 0 \pi 7\pi=\left[\sin\pi\cdot f'(\pi)-\sin0\cdot f'(0)\right]-\left[\cos\pi\cdot f(\pi)-\cos0\cdot f(0)\right] f(0)=6\pi \color{pink}{\mathrm{Doubt}} f(x) f(x) \color{red}{\mathrm{Edit}}","['calculus', 'definite-integrals', 'contest-math']"
62,Finding out the perimeter of the ellipse?,Finding out the perimeter of the ellipse?,,"I was just messing around with the maths when I realised a way of finding the perimeter of the ellipse. First you start by representing any given point in the ellipse in this way: $$ \vec{r} = \begin{pmatrix}  a \cos{\theta} \\ b \sin{\theta} \end{pmatrix} $$ Taking in account that a represents the semi-major axes of the ellipse and b the semi-minor axes. Then, you find the distance between two points $$ \Delta s^2 = \Delta x^2 + \Delta y^2 \\ $$ Take the sum of all of this distances: $$ \lim_{n \to \infty} \sum_{i=1}^{n} \sqrt{\Delta x_i^2 + \Delta y_i^2} \equiv \int_{a}^{b} \sqrt{dx^2 + dy^2} $$ Parametrize the curve with respect to $\theta$ : $$ \int_{a}^{b} \sqrt{\left ( \frac{dx}{d\theta} \right ) ^2 + \left (\frac{dy}{d\theta} \right ) ^2 } \;d\theta $$ $$ \frac{dx}{d\theta} = a \frac{d \cos{\theta}}{d \theta} = -a \sin{\theta} $$ $$ \frac{dy}{d\theta} = b \frac{d \sin{\theta}}{d \theta} = b \cos{\theta} $$ And finally you get this formula: $$ \int_{0}^{\varphi} \sqrt{ a^2 \sin^2({\theta}) + b^2 \cos^2({\theta})}    \;d\theta   $$ Taking in account that $\varphi$ represents until what ""angle"" you want to find the perimeter (if you could talk about angles in an ellipse) I used an online numerical integrator to try to find a solution. This is my formula compared to Ramanujan's one: https://www.desmos.com/calculator/f3qcjauuq3 . Both values are pretty close to each other. Also I've tried to solve this, when $a = b$ . In other words, when the ellipse is no more an ellipse and it is a circunference. Giving me a good and coherent result: $$a = b = r$$ $$ \int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = \int_{0}^{2 \pi} r\sqrt{\sin^2({\theta}) + \cos^2({\theta}) }  \;d\theta = \int_{0}^{2 \pi} r d\theta = \theta r \Big|_0^{2 \pi} = 2 \pi r - 0 r = 2 \pi r $$ $$ \therefore \int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = 2 \pi r $$ The thing is, I have trouble in finding the antiderivative of the ellipse formula. What are the methods that I could use to find it? Could you give me any exact solutions? I could give further explanation if needed, with pictures and animations, etc. Note : Correct me if I made any mistake or if I forgot something in the process. Thanks for the help :)","I was just messing around with the maths when I realised a way of finding the perimeter of the ellipse. First you start by representing any given point in the ellipse in this way: Taking in account that a represents the semi-major axes of the ellipse and b the semi-minor axes. Then, you find the distance between two points Take the sum of all of this distances: Parametrize the curve with respect to : And finally you get this formula: Taking in account that represents until what ""angle"" you want to find the perimeter (if you could talk about angles in an ellipse) I used an online numerical integrator to try to find a solution. This is my formula compared to Ramanujan's one: https://www.desmos.com/calculator/f3qcjauuq3 . Both values are pretty close to each other. Also I've tried to solve this, when . In other words, when the ellipse is no more an ellipse and it is a circunference. Giving me a good and coherent result: The thing is, I have trouble in finding the antiderivative of the ellipse formula. What are the methods that I could use to find it? Could you give me any exact solutions? I could give further explanation if needed, with pictures and animations, etc. Note : Correct me if I made any mistake or if I forgot something in the process. Thanks for the help :)"," \vec{r} = \begin{pmatrix} 
a \cos{\theta} \\
b \sin{\theta}
\end{pmatrix}
 
\Delta s^2 = \Delta x^2 + \Delta y^2 \\
 
\lim_{n \to \infty} \sum_{i=1}^{n} \sqrt{\Delta x_i^2 + \Delta y_i^2} \equiv \int_{a}^{b} \sqrt{dx^2 + dy^2}
 \theta 
\int_{a}^{b} \sqrt{\left ( \frac{dx}{d\theta} \right ) ^2 + \left (\frac{dy}{d\theta} \right ) ^2 } \;d\theta
 
\frac{dx}{d\theta} = a \frac{d \cos{\theta}}{d \theta} = -a \sin{\theta}
 
\frac{dy}{d\theta} = b \frac{d \sin{\theta}}{d \theta} = b \cos{\theta}
 
\int_{0}^{\varphi} \sqrt{ a^2 \sin^2({\theta}) + b^2 \cos^2({\theta})} 
  \;d\theta  
 \varphi a = b a = b = r 
\int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = \int_{0}^{2 \pi} r\sqrt{\sin^2({\theta}) + \cos^2({\theta}) }  \;d\theta =
\int_{0}^{2 \pi} r d\theta = \theta r \Big|_0^{2 \pi} = 2 \pi r - 0 r = 2 \pi r
 
\therefore \int_{0}^{2 \pi} \sqrt{ r^2 \sin^2({\theta}) + r^2 \cos^2({\theta}) }  \;d\theta = 2 \pi r
","['calculus', 'integration', 'geometry', 'conic-sections']"
63,Summation Formula for Tangent/Secant Numbers,Summation Formula for Tangent/Secant Numbers,,"I came across the following expressions: $$\begin{align} \widehat{S}_{2n} &:= \sum_{1 \leq k_1<\cdots<k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-2\ell)^2, \\ \widehat{T}_{2n+1}&:=\sum_{1 \leq k_1 <\cdots <k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-(2\ell+1))(k_\ell-2\ell) \end{align} $$ and I suspect that they equal the secant $S_{2n}$ and tangent $T_{2n+1}$ numbers , respectively. The secant and tangent numbers may be defined using Taylor series: $$\begin{align} \sec x &= \sum_{n=0}^\infty \frac{S_{2n}}{(2n)!} x^{2n}, \\ \tan x &= \sum_{n=0}^\infty  \frac{T_{2n+1}}{(2n+1)!} x^{2n+1} .\end{align}  $$ I have verified that $\widehat{S}_{2n}= S_{2n}$ and that $\widehat{T}_{2n+1}=T_{2n+1}$ for $n \leq 10$ , but I couldn't find a proof for the general case. I should also mention that the transformation $k_i \mapsto m_i+i$ , which maps strictly increasing sequences to non-decreasing sequences produces the nicer-looking formulas: $$\begin{align} \widehat{S}_{2n} &:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-\ell)^2, \\ \widehat{T}_{2n+1}&:=\sum_{0 \leq m_1 \leq \cdots  \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-(\ell+1))(m_\ell-\ell). \end{align} $$ In search of a proof, I have tried generalizing this pattern. For example, the numbers $$\widehat{S}^{(N)}_{2n}:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (\ell-m_\ell)^N, $$ appear to be the Taylor coefficients of the function $$f_N(x) = \frac{1}{1-\frac{1^N x}{1-\frac{2^N x}{1-\frac{3^N x}{1-\dots}}}}, $$ for any natural number $N$ . That is, it seems that $$f_N(x) = \sum_{n=0}^\infty \widehat{S}^{(N)}_{2n} x^n. $$ That made me think that a proof could be obtained using a ""continued-fraction-to-power-series"" formula. Unfortunately, I do not know of such a formula. I would appreciate help in confirming or denying the equalities $\widehat{S}_{2n}= S_{2n}$ and $\widehat{T}_{2n+1}=T_{2n+1}$ for all $n$ . Also, a proof (or disproof) that $\widehat{S}^{(N)}_{2n}$ are indeed related to continued fractions as above would be great. Thanks!","I came across the following expressions: and I suspect that they equal the secant and tangent numbers , respectively. The secant and tangent numbers may be defined using Taylor series: I have verified that and that for , but I couldn't find a proof for the general case. I should also mention that the transformation , which maps strictly increasing sequences to non-decreasing sequences produces the nicer-looking formulas: In search of a proof, I have tried generalizing this pattern. For example, the numbers appear to be the Taylor coefficients of the function for any natural number . That is, it seems that That made me think that a proof could be obtained using a ""continued-fraction-to-power-series"" formula. Unfortunately, I do not know of such a formula. I would appreciate help in confirming or denying the equalities and for all . Also, a proof (or disproof) that are indeed related to continued fractions as above would be great. Thanks!","\begin{align} \widehat{S}_{2n} &:= \sum_{1 \leq k_1<\cdots<k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-2\ell)^2, \\
\widehat{T}_{2n+1}&:=\sum_{1 \leq k_1 <\cdots <k_n \leq 2n} \prod_{\ell=1}^n (k_\ell-(2\ell+1))(k_\ell-2\ell) \end{align}  S_{2n} T_{2n+1} \begin{align} \sec x &= \sum_{n=0}^\infty \frac{S_{2n}}{(2n)!} x^{2n}, \\ \tan x &= \sum_{n=0}^\infty  \frac{T_{2n+1}}{(2n+1)!} x^{2n+1} .\end{align}   \widehat{S}_{2n}= S_{2n} \widehat{T}_{2n+1}=T_{2n+1} n \leq 10 k_i \mapsto m_i+i \begin{align} \widehat{S}_{2n} &:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-\ell)^2, \\
\widehat{T}_{2n+1}&:=\sum_{0 \leq m_1 \leq \cdots  \leq m_n \leq n} \prod_{\ell=1}^n (m_\ell-(\ell+1))(m_\ell-\ell). \end{align}  \widehat{S}^{(N)}_{2n}:= \sum_{0 \leq m_1 \leq \cdots \leq m_n \leq n} \prod_{\ell=1}^n (\ell-m_\ell)^N,  f_N(x) = \frac{1}{1-\frac{1^N x}{1-\frac{2^N x}{1-\frac{3^N x}{1-\dots}}}},  N f_N(x) = \sum_{n=0}^\infty \widehat{S}^{(N)}_{2n} x^n.  \widehat{S}_{2n}= S_{2n} \widehat{T}_{2n+1}=T_{2n+1} n \widehat{S}^{(N)}_{2n}","['calculus', 'trigonometry', 'taylor-expansion', 'generating-functions', 'continued-fractions']"
64,Evaluating $\int_{0}^{2\pi}x^2\ln^2(1-\cos x)dx$,Evaluating,\int_{0}^{2\pi}x^2\ln^2(1-\cos x)dx,"I learnt that $$\int_{0}^{\frac{\pi}{2}} x^2 \ln^2 \cos x \ dx = \frac{11 \pi^5}{1440} + \frac{\pi^3}{24} \ln^2 2 + \frac{\pi}{2}\zeta(3) \ln 2$$ from Sangchul Lee's answer on How to evaluate $I=\int_0^{\pi/2}x^2\ln(\sin x)\ln(\cos x)\ dx$ I did some other calculations, and it appears that $$I=\int_{0}^{2\pi}x^2\ln^2(1-\cos x)~dx = \frac{48\pi\zeta(3)\ln2+8\pi^3\ln^22}{3}+\frac{52\pi^5}{45}.$$ However, I am not sure how to verify the result. What method should I use to calculate $I$ ?","I learnt that from Sangchul Lee's answer on How to evaluate I did some other calculations, and it appears that However, I am not sure how to verify the result. What method should I use to calculate ?","\int_{0}^{\frac{\pi}{2}} x^2 \ln^2 \cos x \ dx
= \frac{11 \pi^5}{1440} + \frac{\pi^3}{24} \ln^2 2 + \frac{\pi}{2}\zeta(3) \ln 2 I=\int_0^{\pi/2}x^2\ln(\sin x)\ln(\cos x)\ dx I=\int_{0}^{2\pi}x^2\ln^2(1-\cos x)~dx = \frac{48\pi\zeta(3)\ln2+8\pi^3\ln^22}{3}+\frac{52\pi^5}{45}. I","['calculus', 'integration', 'improper-integrals']"
65,"Prove that $\sum\limits_{cyc}\frac{a^2}{a^3+2}\leq\frac{4}{3}$ if $a, b, c, d > 0$ and $abcd=1$",Prove that  if  and,"\sum\limits_{cyc}\frac{a^2}{a^3+2}\leq\frac{4}{3} a, b, c, d > 0 abcd=1","Let $a$ , $b$ , $c$ and $d$ be positive numbers such that $abcd=1$ . Prove that: $$\frac{a^2}{a^3+2}+\frac{b^2}{b^3+2}+\frac{c^2}{c^3+2}+\frac{d^2}{d^3+2}\leq\frac{4}{3}.$$ Vasc's LCF Theorem does not help here. Also I tried MV method, but without success.","Let , , and be positive numbers such that . Prove that: Vasc's LCF Theorem does not help here. Also I tried MV method, but without success.",a b c d abcd=1 \frac{a^2}{a^3+2}+\frac{b^2}{b^3+2}+\frac{c^2}{c^3+2}+\frac{d^2}{d^3+2}\leq\frac{4}{3}.,"['calculus', 'inequality', 'contest-math']"
66,Limits of functions that can't be attacked by Taylor series or L'hopital's rule,Limits of functions that can't be attacked by Taylor series or L'hopital's rule,,"Often on this site there is posed a question about a limit which is hard to resolve using l'Hopital.  (I don't mean probelms asking to find a limit without using 'Lhopital, I mean problems where using l'Hopital leads to roadblocks or subtleties.) I always attack those posed problems using Taylor series. For limit problems involving functions (as opposed to infinite sums or products) this pretty much always allows the limit to be found -- for the problems people pose here. I'm interested now in problems of the form ""find $\lim_{x\to a}f(x)$"" that resist solution by  l'Hopital's rule and also by Taylor series methods.  I have a fairly contrived example: $$\lim_{x\to 0}\frac{1-\cos\left(\frac{2}{x+e^{-1/x^2}} \right)}{\sin^2\left(\frac{1}{x}\right)} $$ The combination of the topologist's sin curve in the denominator and the infinitely differentiable but non-analytic $e^{-1/x^2}$ as part of the numerator makes this poison to Taylor series methods, and differentiating the numerator or denominator only makes the behavior worse.  Yet the answer for this case is fairly obvious. My question is, can you find that limit (and prove the value you find is indeed the limit). And can anybody come up with a less contrived function whose limit resists Tayler series, yet can be found by a different technique?","Often on this site there is posed a question about a limit which is hard to resolve using l'Hopital.  (I don't mean probelms asking to find a limit without using 'Lhopital, I mean problems where using l'Hopital leads to roadblocks or subtleties.) I always attack those posed problems using Taylor series. For limit problems involving functions (as opposed to infinite sums or products) this pretty much always allows the limit to be found -- for the problems people pose here. I'm interested now in problems of the form ""find $\lim_{x\to a}f(x)$"" that resist solution by  l'Hopital's rule and also by Taylor series methods.  I have a fairly contrived example: $$\lim_{x\to 0}\frac{1-\cos\left(\frac{2}{x+e^{-1/x^2}} \right)}{\sin^2\left(\frac{1}{x}\right)} $$ The combination of the topologist's sin curve in the denominator and the infinitely differentiable but non-analytic $e^{-1/x^2}$ as part of the numerator makes this poison to Taylor series methods, and differentiating the numerator or denominator only makes the behavior worse.  Yet the answer for this case is fairly obvious. My question is, can you find that limit (and prove the value you find is indeed the limit). And can anybody come up with a less contrived function whose limit resists Tayler series, yet can be found by a different technique?",,"['calculus', 'limits', 'limits-without-lhopital']"
67,Solution of an integral with strange imprecision of gamma functions,Solution of an integral with strange imprecision of gamma functions,,"Trying to solve the following integral, with $n,m \in \mathbb{Z^+}$, $\alpha>1$,  $0 < \epsilon < 1$, and $\Gamma(.)$ and $\Gamma(.,.)$ the gamma and incomplete gamma functions, respectively: $$I(\alpha,m,n) = \frac{(\alpha  (n-1))^n  }{\Gamma (n)-\Gamma \left(n,\frac{(n-1) \alpha }{\epsilon +1}\right)}\int_1^{1/\epsilon}e^{\frac{(y-1) (\alpha -\alpha  n)}{y}}(y-1)^{n-1} y^{m-n-1} dy$$ My approach has been as follows, leading to expressing the integral with gamma functions, which causes precision problems.  Since $(y-1)^{n-1}= \sum _{i=0}^{n-1} (-1)^i \binom{n-1}{i} y^{n-1-i}$, and since $$\int_1^{1/\epsilon } y^{m-2-i} e^{\frac{(y-1) }{y}(\alpha -\alpha n)} \, dy = \frac{e^{\alpha (1- n)} \left(\frac{1}{\alpha -\alpha  n}\right)^{i-m} (\Gamma (i-m+1,\alpha -n \alpha )-\Gamma (i-m+1,-(n-1) \alpha  \epsilon ))}{\alpha  (n-1)},$$ we get as a possible solution a summation showing ratios of differences of various gamma functions: $I(\alpha, m,n)=e^{\alpha (1- n)} (\alpha  (n-1))^{n-1} $ $$\sum _{i=0}^{n-1} (-1)^i \binom{n-1}{i}\frac{ \left(\frac{1}{\alpha -\alpha  n}\right)^{i-m} (\Gamma (i-m+1,\alpha -n \alpha )-\Gamma (i-m+1,-(n-1) \alpha  \epsilon ))}{\Gamma (n)-\Gamma \left(n,\frac{(n-1) \alpha }{\epsilon +1}\right)}$$ Now the hitch. I am computing (for control) the expression of $I(\alpha,m, n)$ integral using high precision  numerical integration. With n=10, the numerical integration matches the gamma function. Beyond, it, the gamma functions (using Wolfram's Mathematica) produce different results (and from familiarity with the problem, I know that the gamma is wrong compared to the numerical).  Yet I need $n$ around $10^4$. Further, other special functions such as the exponential integral give even worse results. The results are in the attached picture. With $\epsilon=\frac{1}{100}$,  $I(\frac{3}{2},1,n)$: The questions are: 1) Is there a possible closed form for $I$not entailing special functions? 2) Is there a better way to solve the integral by avoiding the summation? With gratitude.","Trying to solve the following integral, with $n,m \in \mathbb{Z^+}$, $\alpha>1$,  $0 < \epsilon < 1$, and $\Gamma(.)$ and $\Gamma(.,.)$ the gamma and incomplete gamma functions, respectively: $$I(\alpha,m,n) = \frac{(\alpha  (n-1))^n  }{\Gamma (n)-\Gamma \left(n,\frac{(n-1) \alpha }{\epsilon +1}\right)}\int_1^{1/\epsilon}e^{\frac{(y-1) (\alpha -\alpha  n)}{y}}(y-1)^{n-1} y^{m-n-1} dy$$ My approach has been as follows, leading to expressing the integral with gamma functions, which causes precision problems.  Since $(y-1)^{n-1}= \sum _{i=0}^{n-1} (-1)^i \binom{n-1}{i} y^{n-1-i}$, and since $$\int_1^{1/\epsilon } y^{m-2-i} e^{\frac{(y-1) }{y}(\alpha -\alpha n)} \, dy = \frac{e^{\alpha (1- n)} \left(\frac{1}{\alpha -\alpha  n}\right)^{i-m} (\Gamma (i-m+1,\alpha -n \alpha )-\Gamma (i-m+1,-(n-1) \alpha  \epsilon ))}{\alpha  (n-1)},$$ we get as a possible solution a summation showing ratios of differences of various gamma functions: $I(\alpha, m,n)=e^{\alpha (1- n)} (\alpha  (n-1))^{n-1} $ $$\sum _{i=0}^{n-1} (-1)^i \binom{n-1}{i}\frac{ \left(\frac{1}{\alpha -\alpha  n}\right)^{i-m} (\Gamma (i-m+1,\alpha -n \alpha )-\Gamma (i-m+1,-(n-1) \alpha  \epsilon ))}{\Gamma (n)-\Gamma \left(n,\frac{(n-1) \alpha }{\epsilon +1}\right)}$$ Now the hitch. I am computing (for control) the expression of $I(\alpha,m, n)$ integral using high precision  numerical integration. With n=10, the numerical integration matches the gamma function. Beyond, it, the gamma functions (using Wolfram's Mathematica) produce different results (and from familiarity with the problem, I know that the gamma is wrong compared to the numerical).  Yet I need $n$ around $10^4$. Further, other special functions such as the exponential integral give even worse results. The results are in the attached picture. With $\epsilon=\frac{1}{100}$,  $I(\frac{3}{2},1,n)$: The questions are: 1) Is there a possible closed form for $I$not entailing special functions? 2) Is there a better way to solve the integral by avoiding the summation? With gratitude.",,"['calculus', 'integration', 'special-functions', 'gamma-function']"
68,Differential geometry: restriction of differentiable map to regular surface is differentiable,Differential geometry: restriction of differentiable map to regular surface is differentiable,,"From Do Carmo: Let $S_1$, $S_2$ be regular surfaces. Suppose $S_1\subset V\subset \mathbb{R}^3$ and $\varphi:V\rightarrow \mathbb{R}^3$ is a differentiable map such that $\varphi(S_1)\subset S_2$. Then the restriction \begin{align*} \varphi\big|_{S_1}:S_1\rightarrow S_2 \end{align*} is differentiable. The statement seems obvious but I am having trouble proving it. Perhaps I am thinking about it too hard. I am comfortable with teh statement: 'The restriction of a smooth map is also smooth' - but this is in terms of smooth maps from Euclidean space to Euclidean space. In order to show $\hat{\varphi}:=\varphi|_{S_1}$ is differentiable at some point $p\in S_1$ (when considered as a map between two regular surfaces), we must show that given parameterisations  \begin{align*} \textbf{x}_1:U_1\subset\mathbb{R}^2&\rightarrow S_1\\ \textbf{x}_2:U_2\subset\mathbb{R}^2&\rightarrow S_2 \end{align*} such that $p\in\textbf{x}_1(U_1)$ and $\varphi(p)\in\textbf{x}_2(U_2)$, that the map \begin{align*} \textbf{x}_2^{-1}\circ {\varphi}\circ \textbf{x}_1:U_1\rightarrow\mathbb{R}^2 \end{align*} is a differentiable map between Euclidean space at $\textbf{x}_1^{-1}(p)$. So how would one show this? The immediate 'argument' I think of is chain rule -  'the composition of differentiable maps is differentiable' -- We have that $\textbf{x}_1$ is a differentiable map between Euclidean space by definition, as is $\varphi$. However, the map $\textbf{x}_2^{-1}$ is differentiable in the sense of a map between two regular surfaces ($S_2$ and the $xy$-plane).  So can we really employ the chain rule here given that $\textbf{x}_2^{-1}$ is defined as only a map from a regular surface to $\mathbb{R}^2$? What am I missing? Edit: From the advice below, I have tried to adapt the argument on Pg 70 of Do Carmo: is this a suitable justification that  \begin{align*} \textbf{x}_2^{-1}\circ\varphi\circ\textbf{x}_1 \end{align*} is differentiable at $\textbf{x}_1^{-1}(p )$? Given that $\varphi(\textbf{x}_1(U_1))\subset\textbf{x}_2(U_2)$, it follows there exists $q\in U_2$ such that $\varphi(p )=\textbf{x}_2(q)$. Extend the map $\textbf{x}_2=(x_2(u,v),y_2(u,v),z_2(u,v))$ to a map $\tilde{\textbf{x}}_2:U_2\times \mathbb{R}\rightarrow \mathbb{R}^3$ defined by \begin{align*} \tilde{\textbf{x}}_2:(u,v,t)\rightarrow (x_2,y_2,z_2+t). \end{align*} Then $\tilde{\textbf{x}}_2$ is differentiable and $\tilde{\textbf{x}}_2|_{U_2\times\{0\}}=\textbf{x}_2$. The determinant of $d\tilde{\textbf{x}}_2(q)\neq 0$. Thus, by the inverse function theorem, there exists a neighbourhood $M$ ($\subset\mathbb{R}^3$) of $\textbf{x}_2(q)=\varphi(p )$ such that $\tilde{\textbf{x}}_2^{-1}$ exists AND is differentiable on $M$ (as a map between Euclidean spaces). Define $N:=\textbf{x}_2(U_2)\cap M$. Then $\tilde{\textbf{x}}_2^{-1}|_N$ is smooth (as a map between a subset of $\mathbb{R}^3$ and $U_2\times \mathbb{R}$ (restriction of smooth map is smooth)). If $\pi$ is the projection on to the first two factors, then $\pi$ is smooth and thus so is $\pi\circ \tilde{\textbf{x}}_2^{-1}|_N=\textbf{x}_2^{-1}$. Thus we have \begin{align*} \textbf{x}_2^{-1}\circ\varphi\circ\textbf{x}_1=\pi\circ\tilde{\textbf{x}}_2^{-1}\circ\varphi\circ\textbf{x}_1 \end{align*} This map on the right hand side is a composition of smooth maps between Euclidean spaces. Thus, by the chain rule, it is smooth.","From Do Carmo: Let $S_1$, $S_2$ be regular surfaces. Suppose $S_1\subset V\subset \mathbb{R}^3$ and $\varphi:V\rightarrow \mathbb{R}^3$ is a differentiable map such that $\varphi(S_1)\subset S_2$. Then the restriction \begin{align*} \varphi\big|_{S_1}:S_1\rightarrow S_2 \end{align*} is differentiable. The statement seems obvious but I am having trouble proving it. Perhaps I am thinking about it too hard. I am comfortable with teh statement: 'The restriction of a smooth map is also smooth' - but this is in terms of smooth maps from Euclidean space to Euclidean space. In order to show $\hat{\varphi}:=\varphi|_{S_1}$ is differentiable at some point $p\in S_1$ (when considered as a map between two regular surfaces), we must show that given parameterisations  \begin{align*} \textbf{x}_1:U_1\subset\mathbb{R}^2&\rightarrow S_1\\ \textbf{x}_2:U_2\subset\mathbb{R}^2&\rightarrow S_2 \end{align*} such that $p\in\textbf{x}_1(U_1)$ and $\varphi(p)\in\textbf{x}_2(U_2)$, that the map \begin{align*} \textbf{x}_2^{-1}\circ {\varphi}\circ \textbf{x}_1:U_1\rightarrow\mathbb{R}^2 \end{align*} is a differentiable map between Euclidean space at $\textbf{x}_1^{-1}(p)$. So how would one show this? The immediate 'argument' I think of is chain rule -  'the composition of differentiable maps is differentiable' -- We have that $\textbf{x}_1$ is a differentiable map between Euclidean space by definition, as is $\varphi$. However, the map $\textbf{x}_2^{-1}$ is differentiable in the sense of a map between two regular surfaces ($S_2$ and the $xy$-plane).  So can we really employ the chain rule here given that $\textbf{x}_2^{-1}$ is defined as only a map from a regular surface to $\mathbb{R}^2$? What am I missing? Edit: From the advice below, I have tried to adapt the argument on Pg 70 of Do Carmo: is this a suitable justification that  \begin{align*} \textbf{x}_2^{-1}\circ\varphi\circ\textbf{x}_1 \end{align*} is differentiable at $\textbf{x}_1^{-1}(p )$? Given that $\varphi(\textbf{x}_1(U_1))\subset\textbf{x}_2(U_2)$, it follows there exists $q\in U_2$ such that $\varphi(p )=\textbf{x}_2(q)$. Extend the map $\textbf{x}_2=(x_2(u,v),y_2(u,v),z_2(u,v))$ to a map $\tilde{\textbf{x}}_2:U_2\times \mathbb{R}\rightarrow \mathbb{R}^3$ defined by \begin{align*} \tilde{\textbf{x}}_2:(u,v,t)\rightarrow (x_2,y_2,z_2+t). \end{align*} Then $\tilde{\textbf{x}}_2$ is differentiable and $\tilde{\textbf{x}}_2|_{U_2\times\{0\}}=\textbf{x}_2$. The determinant of $d\tilde{\textbf{x}}_2(q)\neq 0$. Thus, by the inverse function theorem, there exists a neighbourhood $M$ ($\subset\mathbb{R}^3$) of $\textbf{x}_2(q)=\varphi(p )$ such that $\tilde{\textbf{x}}_2^{-1}$ exists AND is differentiable on $M$ (as a map between Euclidean spaces). Define $N:=\textbf{x}_2(U_2)\cap M$. Then $\tilde{\textbf{x}}_2^{-1}|_N$ is smooth (as a map between a subset of $\mathbb{R}^3$ and $U_2\times \mathbb{R}$ (restriction of smooth map is smooth)). If $\pi$ is the projection on to the first two factors, then $\pi$ is smooth and thus so is $\pi\circ \tilde{\textbf{x}}_2^{-1}|_N=\textbf{x}_2^{-1}$. Thus we have \begin{align*} \textbf{x}_2^{-1}\circ\varphi\circ\textbf{x}_1=\pi\circ\tilde{\textbf{x}}_2^{-1}\circ\varphi\circ\textbf{x}_1 \end{align*} This map on the right hand side is a composition of smooth maps between Euclidean spaces. Thus, by the chain rule, it is smooth.",,"['calculus', 'differential-geometry', 'surfaces']"
69,"Need help with a definite integral $\int_0^{\infty} \frac{x-1}{\sqrt{2^x-1}\ln(2^x-1)}\,dx$ [duplicate]",Need help with a definite integral  [duplicate],"\int_0^{\infty} \frac{x-1}{\sqrt{2^x-1}\ln(2^x-1)}\,dx","This question already has answers here : A conjectured closed form of $\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx$ (5 answers) Closed 10 years ago . Evaluate: $$\int_0^{\infty} \frac{x-1}{\sqrt{2^x-1}\ln(2^x-1)}\,dx$$ I am not sure where to start or what should be the best approach towards this problem. I tried the substitution $2^x-1=t^2$ but that seems to make things more worse. Using this substitution, I got: $$\int_0^{\infty} \frac{1}{\ln^2 2}\frac{\ln\left(\frac{1+t^2}{2}\right)}{(1+t^2)\ln t}\,dt$$ I don't see how to proceed after this. :( Any help is appreciated. Thanks!","This question already has answers here : A conjectured closed form of $\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx$ (5 answers) Closed 10 years ago . Evaluate: $$\int_0^{\infty} \frac{x-1}{\sqrt{2^x-1}\ln(2^x-1)}\,dx$$ I am not sure where to start or what should be the best approach towards this problem. I tried the substitution $2^x-1=t^2$ but that seems to make things more worse. Using this substitution, I got: $$\int_0^{\infty} \frac{1}{\ln^2 2}\frac{\ln\left(\frac{1+t^2}{2}\right)}{(1+t^2)\ln t}\,dt$$ I don't see how to proceed after this. :( Any help is appreciated. Thanks!",,"['calculus', 'integration', 'definite-integrals']"
70,Limit $\mathop {\lim }\limits_{n \to \infty } \frac{{n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_1} + ... + {a_n}}}$,Limit,\mathop {\lim }\limits_{n \to \infty } \frac{{n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_1} + ... + {a_n}}},"Evaluate the following limit. $a_i > 0.\forall i\in \mathbb{N}$. $$\mathop {\lim }\limits_{n \to \infty } \frac{{n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_1} + ... + {a_n}}}$$ I tried to use the Stolz-Cesaro Theorem which I thought might feet here. I got this expression: $$\mathop {\lim }\limits_{n \to \infty } \frac{{(n + 1){{\left( {{a_1}...{a_{n + 1}}} \right)}^{\frac{1}{{n + 1}}}} - n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_{n + 1}}}}$$ Which looked a little promising, but I don't know how to take it from here.","Evaluate the following limit. $a_i > 0.\forall i\in \mathbb{N}$. $$\mathop {\lim }\limits_{n \to \infty } \frac{{n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_1} + ... + {a_n}}}$$ I tried to use the Stolz-Cesaro Theorem which I thought might feet here. I got this expression: $$\mathop {\lim }\limits_{n \to \infty } \frac{{(n + 1){{\left( {{a_1}...{a_{n + 1}}} \right)}^{\frac{1}{{n + 1}}}} - n{{\left( {{a_1}...{a_n}} \right)}^{\frac{1}{n}}}}}{{{a_{n + 1}}}}$$ Which looked a little promising, but I don't know how to take it from here.",,"['calculus', 'sequences-and-series', 'limits']"
71,A functional relation which is satisfied by $\cos x$ and $\sin x$,A functional relation which is satisfied by  and,\cos x \sin x,"Assume that the functions $f,g : \mathbb R\to \mathbb R$ satisfy the relations \begin{align} \left\{ \begin{array}{ll} f(x+y) &=& f(x)f(y)-g(x)g(y), \\ g(x+y) &=& f(x)g(y)+f(y)g(x), \end{array} \right. \tag{$\star$} \end{align} for all $x,y\in \mathbb R$. Find $f,g$, if they are continuous. Is there a pair functions $f,g$ satisfying $(\star)$ which are not continuous? Characterization of all such pairs. Update. This problem is clearly harder than the characterization of functions satisfying $$f(x+y)=f(x)+f(y),$$ as it is a system of non-linear functional equations. I imagine that, in order to attack this problem, one has to make it linear.","Assume that the functions $f,g : \mathbb R\to \mathbb R$ satisfy the relations \begin{align} \left\{ \begin{array}{ll} f(x+y) &=& f(x)f(y)-g(x)g(y), \\ g(x+y) &=& f(x)g(y)+f(y)g(x), \end{array} \right. \tag{$\star$} \end{align} for all $x,y\in \mathbb R$. Find $f,g$, if they are continuous. Is there a pair functions $f,g$ satisfying $(\star)$ which are not continuous? Characterization of all such pairs. Update. This problem is clearly harder than the characterization of functions satisfying $$f(x+y)=f(x)+f(y),$$ as it is a system of non-linear functional equations. I imagine that, in order to attack this problem, one has to make it linear.",,"['calculus', 'analysis', 'recreational-mathematics', 'contest-math', 'functional-equations']"
72,"Question regarding calculus, graph of functions, point of inflection.","Question regarding calculus, graph of functions, point of inflection.",,"We're studying the application of derivatives in mathematics right now. This refers to a question which arose in my head while solving a particular problem. The problem was: A function $f(x)$ is continuous and twice differentiable. The line segment joining $\big(a, f(a)\big)$ and $\big(b, f(b)\big)$ intersects the graph at $\big(c,f(c)\big)$, prove that there exists a $t\epsilon(a,b)$ such that $f''(t) = 0$. Now this part is easy enough. There will exist a point in the interval $(a,c)$ such that the slope of the tangent at the point will be equal to the slope of the line segment $AB$. Similarly, another point with the same condition will exist in the interval  $(c,b)$. We know that the concavity of a graph changes between two extremas (for a continuous and differentiable curve), and therefore there will exist a point of inflection $\big(t, f(t)\big)$ around which the second derivative $\frac{d^2y}{dx^2}$ $\big(= f''(x)\big) $ changes its sign and thus the value of $f''(t)$ will be $0$. Now my confusion starts. A student raised a doubt, saying that although there HAS to be a point $t$ such that $f''(t) = 0$ it is possible that there is no point of inflection . He said that this may happen when there is a 'm'-shaped graph: To my surprise, my professor accepted the argument ! I felt that it could not be true, since there ARE existing points between A and C and between C and B where the concavity IS changing! The portion of the graph just about C is concave upwards, while the other parts of the graph are concave downwards. When I pointed this out to the professor, he disagreed, saying that if the graph narrows to a point at C, then there won't be any portion facing upwards. I have a problem with this too. If the graph narrows to a point, then that means that $f(x)$ will NOT be differentiable at C! And if it does not, there always WILL be some part - no matter how small - which faces upwards. When I raised objections again, I was told to observe the graph of $f(x) = \sin^2(x)$ - which - at points $\pi, 2\pi, 3\pi$, etc($±n\pi$). touches the x-axis, but still doesn't have a point of inflection. If I still had a doubt, I was to meet him after class (which didn't happen since he left before our last lecture was over). So I checked the $f(x) = \sin^2(x)$ graph too. I checked it's second derivative - which is $2\cos(2x)$. The graph of this is a wave, which intersects the $x$ axis at $\frac{n\pi}{4}$ as shown here . This graph indicates that $f(x) = \sin^2(x)$ does have inflection points about which the sign of $f''(x)$ (and so the concavity of the graph) changes. I'm not going to meet this professor for another week, and I need this cleared. Can anybody confirm whether I am correct or my professor is?","We're studying the application of derivatives in mathematics right now. This refers to a question which arose in my head while solving a particular problem. The problem was: A function $f(x)$ is continuous and twice differentiable. The line segment joining $\big(a, f(a)\big)$ and $\big(b, f(b)\big)$ intersects the graph at $\big(c,f(c)\big)$, prove that there exists a $t\epsilon(a,b)$ such that $f''(t) = 0$. Now this part is easy enough. There will exist a point in the interval $(a,c)$ such that the slope of the tangent at the point will be equal to the slope of the line segment $AB$. Similarly, another point with the same condition will exist in the interval  $(c,b)$. We know that the concavity of a graph changes between two extremas (for a continuous and differentiable curve), and therefore there will exist a point of inflection $\big(t, f(t)\big)$ around which the second derivative $\frac{d^2y}{dx^2}$ $\big(= f''(x)\big) $ changes its sign and thus the value of $f''(t)$ will be $0$. Now my confusion starts. A student raised a doubt, saying that although there HAS to be a point $t$ such that $f''(t) = 0$ it is possible that there is no point of inflection . He said that this may happen when there is a 'm'-shaped graph: To my surprise, my professor accepted the argument ! I felt that it could not be true, since there ARE existing points between A and C and between C and B where the concavity IS changing! The portion of the graph just about C is concave upwards, while the other parts of the graph are concave downwards. When I pointed this out to the professor, he disagreed, saying that if the graph narrows to a point at C, then there won't be any portion facing upwards. I have a problem with this too. If the graph narrows to a point, then that means that $f(x)$ will NOT be differentiable at C! And if it does not, there always WILL be some part - no matter how small - which faces upwards. When I raised objections again, I was told to observe the graph of $f(x) = \sin^2(x)$ - which - at points $\pi, 2\pi, 3\pi$, etc($±n\pi$). touches the x-axis, but still doesn't have a point of inflection. If I still had a doubt, I was to meet him after class (which didn't happen since he left before our last lecture was over). So I checked the $f(x) = \sin^2(x)$ graph too. I checked it's second derivative - which is $2\cos(2x)$. The graph of this is a wave, which intersects the $x$ axis at $\frac{n\pi}{4}$ as shown here . This graph indicates that $f(x) = \sin^2(x)$ does have inflection points about which the sign of $f''(x)$ (and so the concavity of the graph) changes. I'm not going to meet this professor for another week, and I need this cleared. Can anybody confirm whether I am correct or my professor is?",,"['calculus', 'derivatives', 'graphing-functions']"
73,Calculating $\int_{0}^{\infty}\sin(x^{2})dx$,Calculating,\int_{0}^{\infty}\sin(x^{2})dx,"I am supposed, in an  exercise, to calculate the above integral by integrating $f(z) = e^{-z^{2}}$ on the following countor: I began by separating the path $\gamma$ into three paths (obvious from the picture), and parametrizing each as follows: $\gamma_{1} : [0, R] \rightarrow \mathbb{C}$ with $\gamma_{1}(t) = t$ $\gamma_{2} : [0, \frac{\pi}{4}] \rightarrow \mathbb{C}$ with $\gamma_{2}(t) = Re^{it}$ $\gamma_{3} : [0, \frac{\sqrt{2}R}{2}] \rightarrow \mathbb{C}$ with $\gamma_{3}^{-}(t) = t + it$ (with reverse orientation). Then we can say that $\displaystyle\int_{\gamma} f(z) dz = \displaystyle\int_{\gamma_{1}} f(z) dz + \displaystyle\int_{\gamma_{2}} f(z) dz - \displaystyle\int_{\gamma_{3}^{-}} f(z) dz = 0$ since the path is closed. Now $\displaystyle\int_{\gamma_{1}} f(z) dz = \displaystyle\int\limits_{0}^{R} e^{-t^{2}} dt$. We also get $\displaystyle\int_{\gamma_{3}^{-}} f(z) dz = -(i + 1) \displaystyle\int\limits_{0}^{\frac{\sqrt{2}R}{2}}e^{-2it^{2}} dt$. After playing around with sine and cosine a bunch to evaluate that last integral, I get: $$0 = \int\limits_{0}^{R} e^{-t^{2}} dt + \int\limits_{\gamma_{2}} f(z) dz - \frac{i + 1}{\sqrt{2}} \int\limits_{0}^{R} \cos(u^{2}) du + \frac{i - 1}{\sqrt{2}} \int\limits_{0}^{R} \sin(u^{2}) du$$ I could not evaluate the integral along the second path, but I thought it might tend to 0 as $R \rightarrow \infty$. Then taking limits and equating real parts we get $$\frac{\sqrt{2 \pi}}{2} = \displaystyle\int\limits_{0}^{\infty} \sin(u^{2}) du + \displaystyle\int\limits_{0}^{\infty} \cos(u^{2}) du$$ If I could argue that the integrals are equal, I would have my result.. But how do I? So I need to justify two things: why the integral along $\gamma_{2}$ tends to zero and why are the last two integrals equal.","I am supposed, in an  exercise, to calculate the above integral by integrating $f(z) = e^{-z^{2}}$ on the following countor: I began by separating the path $\gamma$ into three paths (obvious from the picture), and parametrizing each as follows: $\gamma_{1} : [0, R] \rightarrow \mathbb{C}$ with $\gamma_{1}(t) = t$ $\gamma_{2} : [0, \frac{\pi}{4}] \rightarrow \mathbb{C}$ with $\gamma_{2}(t) = Re^{it}$ $\gamma_{3} : [0, \frac{\sqrt{2}R}{2}] \rightarrow \mathbb{C}$ with $\gamma_{3}^{-}(t) = t + it$ (with reverse orientation). Then we can say that $\displaystyle\int_{\gamma} f(z) dz = \displaystyle\int_{\gamma_{1}} f(z) dz + \displaystyle\int_{\gamma_{2}} f(z) dz - \displaystyle\int_{\gamma_{3}^{-}} f(z) dz = 0$ since the path is closed. Now $\displaystyle\int_{\gamma_{1}} f(z) dz = \displaystyle\int\limits_{0}^{R} e^{-t^{2}} dt$. We also get $\displaystyle\int_{\gamma_{3}^{-}} f(z) dz = -(i + 1) \displaystyle\int\limits_{0}^{\frac{\sqrt{2}R}{2}}e^{-2it^{2}} dt$. After playing around with sine and cosine a bunch to evaluate that last integral, I get: $$0 = \int\limits_{0}^{R} e^{-t^{2}} dt + \int\limits_{\gamma_{2}} f(z) dz - \frac{i + 1}{\sqrt{2}} \int\limits_{0}^{R} \cos(u^{2}) du + \frac{i - 1}{\sqrt{2}} \int\limits_{0}^{R} \sin(u^{2}) du$$ I could not evaluate the integral along the second path, but I thought it might tend to 0 as $R \rightarrow \infty$. Then taking limits and equating real parts we get $$\frac{\sqrt{2 \pi}}{2} = \displaystyle\int\limits_{0}^{\infty} \sin(u^{2}) du + \displaystyle\int\limits_{0}^{\infty} \cos(u^{2}) du$$ If I could argue that the integrals are equal, I would have my result.. But how do I? So I need to justify two things: why the integral along $\gamma_{2}$ tends to zero and why are the last two integrals equal.",,"['calculus', 'complex-analysis']"
74,A little-o dilemma or the expectation of the KDE,A little-o dilemma or the expectation of the KDE,,"This question arose out of this answer on Cross Validated, but there is no need to click the link since all the necessary details will be summarized here. The level of probability theory and statistics involved in this question is very basic. It is about calculus if anything. This question assumes the following definition of the little-o if given a function $f(x)$ : $$ f\in o(x) \iff \lim_{x\to x_0} \frac{f(x)}{x} = 0,$$ where $x_0$ is a real number, a complex number or $\pm \infty$ . Background Suppose $x_1, ..., x_n$ are independent and identically distributed observations of a random variable $X$ with unknown distribution function $F$ and probability density function $f\in C^m$ , for some $m>1$ fixed. Let $k\in C^{m+1}$ be a given fixed function such that \begin{align} k&\geq 0, \\ \mathrm{supp} (k)&=[-1,1], \\ \int_{\mathbb{R}} k(u)\mathrm{d}u&=1, \\ \int_{\mathbb{R}} k(u)u^l\mathrm{d}u&=0 \ \text{for all} \ 1\leq l<m \ \text{and}\\ \int_{\mathbb{R}} k(u)u^m\mathrm{d}u&<\infty . \end{align} Define the so-called kernel density estimator (KDE) $f_n$ of $f$ by $$f_n(t)=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}k\left(\frac{t-x_i}{h}\right),$$ where $h=h(n)$ is the bandwidth. What is the expectation of $f_n$ , i.e. $\mathbb{E}[f_n(t)]$ ?. By linearity of the expectation, identical distribution of $x_1,...,x_n$ , the law of the unconscious statistician and the change of variables $u=(t-x)/h$ , \begin{align} \mathbb{E}[f_n(t)]&=\frac{1}{n}\sum_{i=1}^n \mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x_i}{h}\right)\right]\\ &=\mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x}{h}\right)\right]\\ &=\int_{\mathbb{R}}\frac{1}{h}k\left(\frac{t-x}{h}\right)f(x)\mathrm{d}x\\ &=\int_{\mathbb{R}}\frac{1}{h}k(u)f(t-hu)h\mathrm{d}u\\ &=\int_{\mathbb{R}}k(u)f(t-hu)\mathrm{d}u. \tag{1}  \end{align} From $f\in C^m$ , it follows that $$f(t-hu)=\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m).$$ Then from $(1)$ and linearity of integration, \begin{align} \mathbb{E}[f_n(t)]&=\int_{\mathbb{R}}k(u)\left(\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m)\right)\mathrm{d}u \\ &=\sum_{l=0}^m\int_{\mathbb{R}}k(u)\frac{f^{(l)}(t)(-hu)^l}{l!}\mathrm{d}u+\int_{\mathbb{R}}k(u)o((hu)^m)\mathrm{d}u. \tag{2} \end{align} From the given conditions on $k$ , the $l=0$ term reads $$\int_{\mathbb{R}} k(u)f(t)\mathrm{d}u=f(t)\int_{\mathbb{R}} k(u) \mathrm{d}u=f(t).$$ The $1\leq l<m$ terms are $$\int_{\mathbb{R}} k(u)\frac{f^{(l)}(t)}{l!} (-hu)^l\mathrm{d}u=\frac{f^{(l)}(t)(-h)^l}{l!}\int_{\mathbb{R}} k(u)u^l\mathrm{d}u=0.$$ Finally, the $l=m$ term is $$ \frac{f^{(m)}(t)(-h)^m}{m!}\int_{\mathbb{R}} k(u)u^m\mathrm{d}u<\infty.$$ According to the above linked answer, it holds that $o((hu)^m) = u^m o(h^m)$ and thus the remainder term in $(2)$ is \begin{equation} \int_\mathbb{R} k(u) o((hu)^m)\mathrm{d}u = o(h^m)\int_\mathbb{R} k(u) u^m\mathrm{d}u = o(h^m). \end{equation} Question Why does $o((hu)^m) = u^m o(h^m)$ hold? According to the Taylor expansion and the given definition of little-o, $o((hu)^m)$ means all functions $f$ that satisfy $\lim_{hu\to 0} \frac{f(hu)}{(hu)^m} = 0$ . One can pull out factors from the little-o, i.e. $o((hu)^m)=huo((hu)^{m-1})$ , but $o((hu)^m) = u^m o(h^m)$ suggests that the variable which the limit in the definition of little-o is taken with respect to has changed.","This question arose out of this answer on Cross Validated, but there is no need to click the link since all the necessary details will be summarized here. The level of probability theory and statistics involved in this question is very basic. It is about calculus if anything. This question assumes the following definition of the little-o if given a function : where is a real number, a complex number or . Background Suppose are independent and identically distributed observations of a random variable with unknown distribution function and probability density function , for some fixed. Let be a given fixed function such that Define the so-called kernel density estimator (KDE) of by where is the bandwidth. What is the expectation of , i.e. ?. By linearity of the expectation, identical distribution of , the law of the unconscious statistician and the change of variables , From , it follows that Then from and linearity of integration, From the given conditions on , the term reads The terms are Finally, the term is According to the above linked answer, it holds that and thus the remainder term in is Question Why does hold? According to the Taylor expansion and the given definition of little-o, means all functions that satisfy . One can pull out factors from the little-o, i.e. , but suggests that the variable which the limit in the definition of little-o is taken with respect to has changed.","f(x)  f\in o(x) \iff \lim_{x\to x_0} \frac{f(x)}{x} = 0, x_0 \pm \infty x_1, ..., x_n X F f\in C^m m>1 k\in C^{m+1} \begin{align}
k&\geq 0, \\
\mathrm{supp} (k)&=[-1,1], \\
\int_{\mathbb{R}} k(u)\mathrm{d}u&=1, \\
\int_{\mathbb{R}} k(u)u^l\mathrm{d}u&=0 \ \text{for all} \ 1\leq l<m \ \text{and}\\
\int_{\mathbb{R}} k(u)u^m\mathrm{d}u&<\infty .
\end{align} f_n f f_n(t)=\frac{1}{n}\sum_{i=1}^n \frac{1}{h}k\left(\frac{t-x_i}{h}\right), h=h(n) f_n \mathbb{E}[f_n(t)] x_1,...,x_n u=(t-x)/h \begin{align}
\mathbb{E}[f_n(t)]&=\frac{1}{n}\sum_{i=1}^n \mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x_i}{h}\right)\right]\\
&=\mathbb{E}\left[\frac{1}{h}k\left(\frac{t-x}{h}\right)\right]\\
&=\int_{\mathbb{R}}\frac{1}{h}k\left(\frac{t-x}{h}\right)f(x)\mathrm{d}x\\
&=\int_{\mathbb{R}}\frac{1}{h}k(u)f(t-hu)h\mathrm{d}u\\
&=\int_{\mathbb{R}}k(u)f(t-hu)\mathrm{d}u. \tag{1} 
\end{align} f\in C^m f(t-hu)=\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m). (1) \begin{align}
\mathbb{E}[f_n(t)]&=\int_{\mathbb{R}}k(u)\left(\sum_{l=0}^m \frac{f^{(l)}(t)}{l!} (-hu)^l+o((hu)^m)\right)\mathrm{d}u \\
&=\sum_{l=0}^m\int_{\mathbb{R}}k(u)\frac{f^{(l)}(t)(-hu)^l}{l!}\mathrm{d}u+\int_{\mathbb{R}}k(u)o((hu)^m)\mathrm{d}u. \tag{2}
\end{align} k l=0 \int_{\mathbb{R}} k(u)f(t)\mathrm{d}u=f(t)\int_{\mathbb{R}} k(u) \mathrm{d}u=f(t). 1\leq l<m \int_{\mathbb{R}} k(u)\frac{f^{(l)}(t)}{l!} (-hu)^l\mathrm{d}u=\frac{f^{(l)}(t)(-h)^l}{l!}\int_{\mathbb{R}} k(u)u^l\mathrm{d}u=0. l=m  \frac{f^{(m)}(t)(-h)^m}{m!}\int_{\mathbb{R}} k(u)u^m\mathrm{d}u<\infty. o((hu)^m) = u^m o(h^m) (2) \begin{equation}
\int_\mathbb{R} k(u) o((hu)^m)\mathrm{d}u = o(h^m)\int_\mathbb{R} k(u) u^m\mathrm{d}u = o(h^m).
\end{equation} o((hu)^m) = u^m o(h^m) o((hu)^m) f \lim_{hu\to 0} \frac{f(hu)}{(hu)^m} = 0 o((hu)^m)=huo((hu)^{m-1}) o((hu)^m) = u^m o(h^m)","['calculus', 'probability', 'limits', 'statistics', 'asymptotics']"
75,Finding a summation form of this sum - and its value,Finding a summation form of this sum - and its value,,"I was just thinking about this sequence: $$ 1, 2^{-1}, 3^{ - (1 + 2^{-1})}, 4^{-(1 + 2^{-1}+3^{ - (1 + 2^{-1})})} , \dots$$ The previous element in the sum is of the form $$n^{- \sum \text{previous elements}}$$ Because of this, I think this is the right way of writing it: $$a_1 = 1, ~~ a_2 = 2^{-1}, ~~ $$ $$ a_n = n^{-\sum_{k=1}^{n-1}a_k}$$ I was wondering - is there a way of finding the sum itself? $$\sum_{n=1}^{\infty} a_n$$ it seems very difficult and even impossible! On the other hand, my knowledge on sums is very minor, I don't know even how to begin with calculating this! My Python program showed this converges to about: $$ \approx 2.045290822396635$$","I was just thinking about this sequence: The previous element in the sum is of the form Because of this, I think this is the right way of writing it: I was wondering - is there a way of finding the sum itself? it seems very difficult and even impossible! On the other hand, my knowledge on sums is very minor, I don't know even how to begin with calculating this! My Python program showed this converges to about:"," 1, 2^{-1}, 3^{ - (1 + 2^{-1})}, 4^{-(1 + 2^{-1}+3^{ - (1 + 2^{-1})})} , \dots n^{- \sum \text{previous elements}} a_1 = 1, ~~ a_2 = 2^{-1}, ~~   a_n = n^{-\sum_{k=1}^{n-1}a_k} \sum_{n=1}^{\infty} a_n  \approx 2.045290822396635","['calculus', 'sequences-and-series', 'recurrence-relations']"
76,How to show that $\int_{0}^{\frac{\pi}{4}}\ln(\sqrt{\tan x}+\sqrt{\cot x} -\sqrt{2})\ dx=0$,How to show that,\int_{0}^{\frac{\pi}{4}}\ln(\sqrt{\tan x}+\sqrt{\cot x} -\sqrt{2})\ dx=0,"How can I show that $$\int_{0}^{\frac{\pi}{4}}\ln(\sqrt{\tan x}+\sqrt{\cot x} -\sqrt{2})\ dx=0$$ I saw this integral on AoPS, and one person provides a solution: Let $$I:=\int_0^\frac{\pi}{4} \ln(\sqrt{\tan x}+\sqrt{\cot x}-\sqrt 2 ) \ dx, \ \ \ \ \ J:=\int_0^\frac{\pi}{4} \ln(\sqrt{\tan x}+\sqrt{\cot x}+\sqrt 2 ) \ dx.$$ I show that $$I=0, \ \ \ \ \ J=\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)$$ We have $$I+J=\int_0^{\pi/4}\ln(\tan x+\cot x) \ dx=\int_0^{\pi/4}(\ln 2-\ln(\sin(2x)) \ dx=\frac{1}{2}\int_0^{\pi/2}(\ln 2-\ln(\sin x)) \ dx$$ $$=\frac{\pi}{4}\ln 2-\frac{1}{2}\int_0^{\pi/2}\ln(\sin x) \ dx=\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ \ \ (2)$$ Next is to compute $I-J.$ To avoid the mess, as much as we can, we put $\tan x = t^2$ to get $$I=\int_0^1\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{2t}{t^4+1} \ dt, \ \ \ \ \ J=\int_0^1\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{2t}{t^4+1} \ dt$$ and changing $t$ to $1/t$ gives $$I=\int_1^{\infty}\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{2t}{t^4+1} \ dt, \ \ \ \ \ \ J=\int_1^{\infty}\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{2t}{t^4+1} \ dt.$$ Thus $$I=\int_0^{\infty}\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{t}{t^4+1} \ dt, \ \ \ \ \ \ J=\int_0^{\infty}\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{t}{t^4+1} \ dt$$ and hence, since $\int_0^{\infty} \frac{t\ln t}{t^4+1} \ dt=0$ (just change $t$ to $1/t$ to see that), we get $$I=\int_0^{\infty}\frac{t\ln(t^2-\sqrt{2}t+1)}{t^4+1} \ dt, \ \ \ \ \ J=\int_0^{\infty}\frac{t\ln(t^2+\sqrt{2}t+1)}{t^4+1} \ dt.$$ Therefore $$I-J=\int_0^{\infty}\frac{t}{t^4+1}(\ln(t^2-\sqrt{2}t+1)-\ln(t^2+\sqrt{2}t+1)) \ dt. \ \ \ \ \ \ \ \ \ \ \ (3)$$ We now use integration by parts with $\frac{t}{t^4+1} \ dt=dv, \ \ \ \ln(t^2-\sqrt{2}t+1)-\ln(t^2+\sqrt{2}t+1)=u.$ Then $$v=\frac{1}{2}\tan^{-1}(t^2), \ \ \ du=\frac{2\sqrt{2}(t^2-1)}{t^4+1} \ dt$$ and so $(3)$ becomes $$I-J=-\sqrt{2}\int_0^{\infty}\frac{t^2-1}{t^4+1}\tan^{-1}(t^2) \ dt=-\sqrt{2}\int_0^{\infty}\frac{t^2-1}{t^4+1}\int_0^1\frac{t^2}{s^2t^4+1} \ ds \ dt=-\sqrt{2}\int_0^1\int_0^{\infty}\frac{t^2(t^2-1)}{(t^4+1)(s^2t^4+1)} \ dt \ ds$$ $$=-\sqrt{2}\int_0^1\frac{1}{1-s^2}\left(\int_0^{\infty}\frac{s^2t^2+1}{s^2t^4+1} \ dt-\int_0^{\infty}\frac{t^2+1}{t^4+1} \ dt \right)ds.$$ So changing $t$ to $t/\sqrt{s}$ in $\int_0^{\infty}\frac{s^2t^2+1}{s^2t^4+1} \ dt$ gives $$I-J=-\sqrt{2}\int_0^1\frac{1}{1-s^2}\left((\sqrt{s}-1)\int_0^{\infty}\frac{t^2}{t^4+1} \ dt+\left(\frac{1}{\sqrt{s}}-1\right)\int_0^{\infty} \frac{dt}{t^4+1}\right)ds$$ $$=-\frac{\pi}{2}\int_0^1\frac{1}{1-s^2}\left(\sqrt{s}+\frac{1}{\sqrt{s}}-2\right)ds=-\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ (4)$$ Now $(1)$ follows from $(2)$ and $(4).$ This is an elegant solution, but I wonder if there are other ways to solve the integral.","How can I show that I saw this integral on AoPS, and one person provides a solution: Let I show that We have Next is to compute To avoid the mess, as much as we can, we put to get and changing to gives Thus and hence, since (just change to to see that), we get Therefore We now use integration by parts with Then and so becomes So changing to in gives Now follows from and This is an elegant solution, but I wonder if there are other ways to solve the integral.","\int_{0}^{\frac{\pi}{4}}\ln(\sqrt{\tan x}+\sqrt{\cot x} -\sqrt{2})\ dx=0 I:=\int_0^\frac{\pi}{4} \ln(\sqrt{\tan x}+\sqrt{\cot x}-\sqrt 2 ) \ dx, \ \ \ \ \ J:=\int_0^\frac{\pi}{4} \ln(\sqrt{\tan x}+\sqrt{\cot x}+\sqrt 2 ) \ dx. I=0, \ \ \ \ \ J=\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1) I+J=\int_0^{\pi/4}\ln(\tan x+\cot x) \ dx=\int_0^{\pi/4}(\ln 2-\ln(\sin(2x)) \ dx=\frac{1}{2}\int_0^{\pi/2}(\ln 2-\ln(\sin x)) \ dx =\frac{\pi}{4}\ln 2-\frac{1}{2}\int_0^{\pi/2}\ln(\sin x) \ dx=\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ \ \ (2) I-J. \tan x = t^2 I=\int_0^1\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{2t}{t^4+1} \ dt, \ \ \ \ \ J=\int_0^1\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{2t}{t^4+1} \ dt t 1/t I=\int_1^{\infty}\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{2t}{t^4+1} \ dt, \ \ \ \ \ \ J=\int_1^{\infty}\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{2t}{t^4+1} \ dt. I=\int_0^{\infty}\ln\left(t+\frac{1}{t}-\sqrt{2}\right)\frac{t}{t^4+1} \ dt, \ \ \ \ \ \ J=\int_0^{\infty}\ln\left(t+\frac{1}{t}+\sqrt{2}\right)\frac{t}{t^4+1} \ dt \int_0^{\infty} \frac{t\ln t}{t^4+1} \ dt=0 t 1/t I=\int_0^{\infty}\frac{t\ln(t^2-\sqrt{2}t+1)}{t^4+1} \ dt, \ \ \ \ \ J=\int_0^{\infty}\frac{t\ln(t^2+\sqrt{2}t+1)}{t^4+1} \ dt. I-J=\int_0^{\infty}\frac{t}{t^4+1}(\ln(t^2-\sqrt{2}t+1)-\ln(t^2+\sqrt{2}t+1)) \ dt. \ \ \ \ \ \ \ \ \ \ \ (3) \frac{t}{t^4+1} \ dt=dv, \ \ \ \ln(t^2-\sqrt{2}t+1)-\ln(t^2+\sqrt{2}t+1)=u. v=\frac{1}{2}\tan^{-1}(t^2), \ \ \ du=\frac{2\sqrt{2}(t^2-1)}{t^4+1} \ dt (3) I-J=-\sqrt{2}\int_0^{\infty}\frac{t^2-1}{t^4+1}\tan^{-1}(t^2) \ dt=-\sqrt{2}\int_0^{\infty}\frac{t^2-1}{t^4+1}\int_0^1\frac{t^2}{s^2t^4+1} \ ds \ dt=-\sqrt{2}\int_0^1\int_0^{\infty}\frac{t^2(t^2-1)}{(t^4+1)(s^2t^4+1)} \ dt \ ds =-\sqrt{2}\int_0^1\frac{1}{1-s^2}\left(\int_0^{\infty}\frac{s^2t^2+1}{s^2t^4+1} \ dt-\int_0^{\infty}\frac{t^2+1}{t^4+1} \ dt \right)ds. t t/\sqrt{s} \int_0^{\infty}\frac{s^2t^2+1}{s^2t^4+1} \ dt I-J=-\sqrt{2}\int_0^1\frac{1}{1-s^2}\left((\sqrt{s}-1)\int_0^{\infty}\frac{t^2}{t^4+1} \ dt+\left(\frac{1}{\sqrt{s}}-1\right)\int_0^{\infty} \frac{dt}{t^4+1}\right)ds =-\frac{\pi}{2}\int_0^1\frac{1}{1-s^2}\left(\sqrt{s}+\frac{1}{\sqrt{s}}-2\right)ds=-\frac{\pi}{2}\ln 2. \ \ \ \ \ \ \ \ \ \ \ (4) (1) (2) (4).","['calculus', 'integration', 'trigonometry']"
77,A fun Valentine's day chocolate box optimization problem...,A fun Valentine's day chocolate box optimization problem...,,"Today I received a small box with chocolate almonds in it. Here's a photo: The top and bottom of the box are held together by a band that ""cuts across"" alternating corners. It got me thinking: ha, I'll make this into a fun little calculus optimization problem: what distance $x$ from the corner should the band cross each edge as to minimize the total length of the band? And more specifically, what is this distance in terms of the length, width, and height of the box? We're defining $x$ as the following (view from the top of the box, bands are red): Let's call $b_T$ the total band length, $b_h$ the sum of the portions of the band on the top and bottom of the box, $b_l$ the sum of the portions of the band on the side $l$ of the box, and $b_w$ the portions of the band on the side $w$ of the box. And of course, $b_T = b_h + b_l + b_w$. Considering just the top and bottom, we can see: $b_h = 4x\sqrt2$. Similarly, since the total side length of two sides of the box are $l$ and the other two are $w$, and the height of the box is $h$, using the Pythagorean Formula we can get the following two formulas: $$b_w=2\sqrt{h^2+(w-2x)^2}$$ $$b_l=2\sqrt{h^2+(l-2x)^2}$$ Thus, our total band length in terms of x is: $$b_T = 4x\sqrt2 + 2\sqrt{h^2+(w-2x)^2} + 2\sqrt{h^2+(l-2x)^2}$$ Since this is an optimization problem and we wish to find the minimum value of $b_T$, we will differentiate and set equal to zero: $$b_T' = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ $$0 = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ By rearranging a bit more, we can rationalize it to: $$\left(2+ \frac{(w-2x)^2}{h^2+(w-2x)^2} - \frac{(2x-l)^2}{h^2+(l-2x)^2}\right)^2 = \frac{8(w-2x)^2}{h^2+(w-2x)^2}$$ Remember, the goal is to find, given the dimensions of the box, some optimum placement of the band. So we need to rearrange this formula to get $x$ in terms of $l$, $w$, and $h$. This is where I am at a total loss. Distributing out the left side would give us too many terms to keep track of; even substituting each terms for $a$, $b$, and $c$ respectively, distributing that, and then plugging back in results in a ridiculous number of terms. This would be absurd; how can we do this a better way and get $x$ in terms of $l$, $w$, and $h$? (Also, I hope I haven't messed up any of the formulas while typesetting them. I don't think I did, but you never know...)","Today I received a small box with chocolate almonds in it. Here's a photo: The top and bottom of the box are held together by a band that ""cuts across"" alternating corners. It got me thinking: ha, I'll make this into a fun little calculus optimization problem: what distance $x$ from the corner should the band cross each edge as to minimize the total length of the band? And more specifically, what is this distance in terms of the length, width, and height of the box? We're defining $x$ as the following (view from the top of the box, bands are red): Let's call $b_T$ the total band length, $b_h$ the sum of the portions of the band on the top and bottom of the box, $b_l$ the sum of the portions of the band on the side $l$ of the box, and $b_w$ the portions of the band on the side $w$ of the box. And of course, $b_T = b_h + b_l + b_w$. Considering just the top and bottom, we can see: $b_h = 4x\sqrt2$. Similarly, since the total side length of two sides of the box are $l$ and the other two are $w$, and the height of the box is $h$, using the Pythagorean Formula we can get the following two formulas: $$b_w=2\sqrt{h^2+(w-2x)^2}$$ $$b_l=2\sqrt{h^2+(l-2x)^2}$$ Thus, our total band length in terms of x is: $$b_T = 4x\sqrt2 + 2\sqrt{h^2+(w-2x)^2} + 2\sqrt{h^2+(l-2x)^2}$$ Since this is an optimization problem and we wish to find the minimum value of $b_T$, we will differentiate and set equal to zero: $$b_T' = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ $$0 = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ By rearranging a bit more, we can rationalize it to: $$\left(2+ \frac{(w-2x)^2}{h^2+(w-2x)^2} - \frac{(2x-l)^2}{h^2+(l-2x)^2}\right)^2 = \frac{8(w-2x)^2}{h^2+(w-2x)^2}$$ Remember, the goal is to find, given the dimensions of the box, some optimum placement of the band. So we need to rearrange this formula to get $x$ in terms of $l$, $w$, and $h$. This is where I am at a total loss. Distributing out the left side would give us too many terms to keep track of; even substituting each terms for $a$, $b$, and $c$ respectively, distributing that, and then plugging back in results in a ridiculous number of terms. This would be absurd; how can we do this a better way and get $x$ in terms of $l$, $w$, and $h$? (Also, I hope I haven't messed up any of the formulas while typesetting them. I don't think I did, but you never know...)",,"['calculus', 'algebra-precalculus', 'optimization']"
78,A New Definition of Derivative,A New Definition of Derivative,,"Update 2018/4/18: I've found a book in which the definition 5) is discussed. See Topology, Calculus and Approximation by Vilmos Komornik, published by Springer-Verlag , page 98, Lemma 4.1. Original Question: I've come across ""Carathéodory Derivative"" in my textbook, which is, Definition 1) Let $f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t)$ be a function, $a\in \mathbb{R}.$ Then if there exists a map $\varphi:\mathbb{R}\to \mathbb{R}, \quad t\mapsto \varphi(t)$ , which satisfies $$1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R};$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a$ . And compared with the traditional definition of derivative: Definition 2) Let $f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t)$ be a function, $a\in \mathbb{R}.$ Then if the limit $$\lim_{x\to a}{f(x)-f(a)\over{x-a}}$$ exists, then the value of this limit is called the derivative of $f$ at point $a$ . I can prove that (it's not difficult) these two definitions above are equivalent to each other. But when I look at the high-dimensional condition, things get complicated. Definition 3) Let $f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t)$ be a multivariate function, $a\in \mathbb{R}^n,$ Then if there exists a map $\varphi:\mathbb{R}\to M_{m\times n}(\mathbb{R}),\quad t\mapsto \varphi(t)$ , which satisfies $$1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R}^n;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a$ . And consider the traditional definition of derivative Definition 4) Let $f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t)$ be a multivariate function, $a\in \mathbb{R}^n.$ Then if there exists a matrix $A\in M_{m\times n}(\mathbb{R}),$ such that $$\lim_{x\to a}{||f(x)-f(a)-A\cdot (x-a)||\over{||x-a||}}=0,$$ then matrix $A$ is called the derivative of $f$ at point $a$ . Question: I expect that definition 3) is equivalent to definition 4), but I can only prove that $\mathrm{def}\ 3)\Rightarrow \mathrm{def}\ 4).$ I doubt whether $\mathrm{def}\ 4)\Rightarrow \mathrm{def}\ 3)$ is correct. Any help is appreciated. P.S. Now I am able to do some generalization to definition 3). Definition 5) Let $E,F$ be two Banach spaces, $a\in E.$ $\mathcal{L}(E;F)$ be the set of continuous linear map $E\to F,$ then consider function $f:E\to F, \quad t\mapsto f(t),$ then if there exists a map $\varphi:E\to \mathcal{L}(E;F), \ t\mapsto \varphi(t),$ such that $$1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ the derivative of $f$ at point $a.$ Using Hahn-Banach theorem, we can see this definition is also equivalent to the classic definition of derivative on Banach space. P.P.S: A more general condition is, Definition 6) Let $E,F$ be two additive groups, and $\mathcal{T}$ be a topology over $E,$ $\mathcal{T'}$ be a topology over $\mathcal{L}(E;F)$ , $a\in E.$ Here $\mathcal{L}(E;F)$ is the set of continuous linear map $E\to F.$ Consider function $f:E\to F, \quad t\mapsto f(t),$ then if there exists a map $\varphi:(E,\mathcal{T})\to (\mathcal{L}(E;F),\mathcal{T'}), \ t\mapsto \varphi(t),$ such that $$1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E;$$ $$2) \quad  \text{$\varphi $ is continuous at the point a} ,$$ then we call $\varphi(a)$ a derivative of $f$ at point $a,$ with respect to topology $\mathcal{T}$ and topology $\mathcal{T'}.$ (Under this condition the derivative may not be unique.)","Update 2018/4/18: I've found a book in which the definition 5) is discussed. See Topology, Calculus and Approximation by Vilmos Komornik, published by Springer-Verlag , page 98, Lemma 4.1. Original Question: I've come across ""Carathéodory Derivative"" in my textbook, which is, Definition 1) Let be a function, Then if there exists a map , which satisfies then we call the derivative of at point . And compared with the traditional definition of derivative: Definition 2) Let be a function, Then if the limit exists, then the value of this limit is called the derivative of at point . I can prove that (it's not difficult) these two definitions above are equivalent to each other. But when I look at the high-dimensional condition, things get complicated. Definition 3) Let be a multivariate function, Then if there exists a map , which satisfies then we call the derivative of at point . And consider the traditional definition of derivative Definition 4) Let be a multivariate function, Then if there exists a matrix such that then matrix is called the derivative of at point . Question: I expect that definition 3) is equivalent to definition 4), but I can only prove that I doubt whether is correct. Any help is appreciated. P.S. Now I am able to do some generalization to definition 3). Definition 5) Let be two Banach spaces, be the set of continuous linear map then consider function then if there exists a map such that then we call the derivative of at point Using Hahn-Banach theorem, we can see this definition is also equivalent to the classic definition of derivative on Banach space. P.P.S: A more general condition is, Definition 6) Let be two additive groups, and be a topology over be a topology over , Here is the set of continuous linear map Consider function then if there exists a map such that then we call a derivative of at point with respect to topology and topology (Under this condition the derivative may not be unique.)","f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t) a\in \mathbb{R}. \varphi:\mathbb{R}\to \mathbb{R}, \quad t\mapsto \varphi(t) 1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R}; 2) \quad  \text{\varphi  is continuous at the point a} , \varphi(a) f a f:\mathbb{R}\to \mathbb{R},\quad t\mapsto f(t) a\in \mathbb{R}. \lim_{x\to a}{f(x)-f(a)\over{x-a}} f a f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t) a\in \mathbb{R}^n, \varphi:\mathbb{R}\to M_{m\times n}(\mathbb{R}),\quad t\mapsto \varphi(t) 1) \quad f(x)-f(a)=\varphi(x)\cdot(x-a),\forall x\in \mathbb{R}^n; 2) \quad  \text{\varphi  is continuous at the point a} , \varphi(a) f a f:\mathbb{R}^n\to \mathbb{R}^m,\quad t\mapsto f(t) a\in \mathbb{R}^n. A\in M_{m\times n}(\mathbb{R}), \lim_{x\to a}{||f(x)-f(a)-A\cdot (x-a)||\over{||x-a||}}=0, A f a \mathrm{def}\ 3)\Rightarrow \mathrm{def}\ 4). \mathrm{def}\ 4)\Rightarrow \mathrm{def}\ 3) E,F a\in E. \mathcal{L}(E;F) E\to F, f:E\to F, \quad t\mapsto f(t), \varphi:E\to \mathcal{L}(E;F), \ t\mapsto \varphi(t), 1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E; 2) \quad  \text{\varphi  is continuous at the point a} , \varphi(a) f a. E,F \mathcal{T} E, \mathcal{T'} \mathcal{L}(E;F) a\in E. \mathcal{L}(E;F) E\to F. f:E\to F, \quad t\mapsto f(t), \varphi:(E,\mathcal{T})\to (\mathcal{L}(E;F),\mathcal{T'}), \ t\mapsto \varphi(t), 1) \quad f(x)-f(a)=(\varphi(x))(x-a),\forall x\in E; 2) \quad  \text{\varphi  is continuous at the point a} , \varphi(a) f a, \mathcal{T} \mathcal{T'}.","['calculus', 'derivatives']"
79,Very accurate approximations for $\sum\limits_{n=0}^\infty \frac{n}{a^n-1}$ and $\sum\limits_{n=0}^\infty \frac{n^{2m+1}}{e^n-1}$,Very accurate approximations for  and,\sum\limits_{n=0}^\infty \frac{n}{a^n-1} \sum\limits_{n=0}^\infty \frac{n^{2m+1}}{e^n-1},"Apologies if this has been asked before, since it seems rather simple. I enjoy messing around with formal mathematics, and I once derived the following formula which holds for many functions (see my question here ; no-one has yet responded to my request for criteria describing for which $q(x)$ it holds): $$\sum_{n=0}^{\infty}q(n)=\int_{0}^{\infty}q(s)ds-\sum_{k=1}^{\infty}\frac{B_{k}q^{[k-1]}(0)}{k!}\tag{*}$$ where $B_n$ are the Bernoulli numbers . Using this formula with $q(x)=\frac{x}{e^x-1}$ and the identities $\int_0^\infty \frac{x^{s-1}}{e^x-1}=\Gamma(s)\zeta(s)$ and $\frac{x}{e^x-1}=\sum\limits_{n=0}^\infty \frac{B_n}{n!}x^n$, it was easy to show (since $\lim\limits_{n\rightarrow\infty}{\frac{n}{e^n-1}}=0$) that: $$\sum_{n=1}^\infty \frac{n}{e^n-1}=\frac{\pi^2}{6}-1-\sum_{n=0}^\infty \frac{B_n B_{n+1}}{n!(n+1)!}$$ But the odd Bernoulli numbers are zero past $n=3$ so that we get: $$\sum_{n=1}^\infty \frac{n}{e^n-1}=\frac{\pi^2}{6}-\frac{11}{24}\tag{1}$$ Now this is very accurate indeed; Wolfram Alpha declares the error to be on the order of $10^{-16}$. Further, using $(*)$ for $q(x)=\frac{x^{2m+1}}{e^x-1}$ in a similar manner gives: $$\sum_{n=1}^\infty \frac{n^{2m+1}}{e^n-1}=\Gamma(2m+2)\zeta(2m+2)-\sum_{n=0}^\infty \frac{B_n B_{2m+n+1}}{n!(2m+n+1)!}$$ which simplifies to: $$\sum_{n=1}^\infty \frac{n^{2m+1}}{e^n-1}=(2m+1)!\zeta(2m+2)+\frac{B_{2m+2}}{2(2m+2)!}\tag{2}$$ For $m=1$ this gives: $$\sum_{n=1}^\infty \frac{n^3}{e^n-1}=\frac{\pi^4}{15}-\frac{1}{1440}$$ which Wolfram Alpha declares to have an error on the order of $10^{-3}$. The $m=2$ case appears to also have an error on the order of $10^{-3}$. If we use $q(x)=\frac{bx}{e^{bx}-1}$ in $(*)$ and then set $b=\ln{a}$ then I think we get the following purported identity: $$\sum_{n=1}^\infty \frac{n}{a^n-1}=\frac{\pi^2}{6(\ln{a})^2}-\frac{1}{2\ln{a}}+\frac{1}{24}\tag{3}$$ The accuracy of $(3)$ appears extremely good; e.g. for $a=2$ it is $10^{-16}$ , for $a=3$ it is $10^{-15}$ , and for $a=6$ it is still $10^{-9}$ . All of this makes me believe that these formula are not exactly correct, but their accuracy amazes me. I don't know if what's going on in this question has any relevance here. My questions are: Can anyone confirm that all of the highlighted expressions are in fact only approximations, or are some of them actually correct? What in general are the error terms for $(2)$ and $(3)$? Can anyone explain why these expressions are in fact so accurate?","Apologies if this has been asked before, since it seems rather simple. I enjoy messing around with formal mathematics, and I once derived the following formula which holds for many functions (see my question here ; no-one has yet responded to my request for criteria describing for which $q(x)$ it holds): $$\sum_{n=0}^{\infty}q(n)=\int_{0}^{\infty}q(s)ds-\sum_{k=1}^{\infty}\frac{B_{k}q^{[k-1]}(0)}{k!}\tag{*}$$ where $B_n$ are the Bernoulli numbers . Using this formula with $q(x)=\frac{x}{e^x-1}$ and the identities $\int_0^\infty \frac{x^{s-1}}{e^x-1}=\Gamma(s)\zeta(s)$ and $\frac{x}{e^x-1}=\sum\limits_{n=0}^\infty \frac{B_n}{n!}x^n$, it was easy to show (since $\lim\limits_{n\rightarrow\infty}{\frac{n}{e^n-1}}=0$) that: $$\sum_{n=1}^\infty \frac{n}{e^n-1}=\frac{\pi^2}{6}-1-\sum_{n=0}^\infty \frac{B_n B_{n+1}}{n!(n+1)!}$$ But the odd Bernoulli numbers are zero past $n=3$ so that we get: $$\sum_{n=1}^\infty \frac{n}{e^n-1}=\frac{\pi^2}{6}-\frac{11}{24}\tag{1}$$ Now this is very accurate indeed; Wolfram Alpha declares the error to be on the order of $10^{-16}$. Further, using $(*)$ for $q(x)=\frac{x^{2m+1}}{e^x-1}$ in a similar manner gives: $$\sum_{n=1}^\infty \frac{n^{2m+1}}{e^n-1}=\Gamma(2m+2)\zeta(2m+2)-\sum_{n=0}^\infty \frac{B_n B_{2m+n+1}}{n!(2m+n+1)!}$$ which simplifies to: $$\sum_{n=1}^\infty \frac{n^{2m+1}}{e^n-1}=(2m+1)!\zeta(2m+2)+\frac{B_{2m+2}}{2(2m+2)!}\tag{2}$$ For $m=1$ this gives: $$\sum_{n=1}^\infty \frac{n^3}{e^n-1}=\frac{\pi^4}{15}-\frac{1}{1440}$$ which Wolfram Alpha declares to have an error on the order of $10^{-3}$. The $m=2$ case appears to also have an error on the order of $10^{-3}$. If we use $q(x)=\frac{bx}{e^{bx}-1}$ in $(*)$ and then set $b=\ln{a}$ then I think we get the following purported identity: $$\sum_{n=1}^\infty \frac{n}{a^n-1}=\frac{\pi^2}{6(\ln{a})^2}-\frac{1}{2\ln{a}}+\frac{1}{24}\tag{3}$$ The accuracy of $(3)$ appears extremely good; e.g. for $a=2$ it is $10^{-16}$ , for $a=3$ it is $10^{-15}$ , and for $a=6$ it is still $10^{-9}$ . All of this makes me believe that these formula are not exactly correct, but their accuracy amazes me. I don't know if what's going on in this question has any relevance here. My questions are: Can anyone confirm that all of the highlighted expressions are in fact only approximations, or are some of them actually correct? What in general are the error terms for $(2)$ and $(3)$? Can anyone explain why these expressions are in fact so accurate?",,"['calculus', 'sequences-and-series', 'approximation', 'bernoulli-numbers']"
80,Closed-forms for $\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}}$ and $\int_0^\infty\frac{dx}{\sqrt[3]{45\big(23+4\sqrt{33}\big)+\cosh x}}$,Closed-forms for  and,\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}} \int_0^\infty\frac{dx}{\sqrt[3]{45\big(23+4\sqrt{33}\big)+\cosh x}},"( This summarizes results for cube roots from here and here . The fourth root version is this post .) Define $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}=\frac1{B\big(\tfrac{1}{3},\tfrac{1}{2}\big)}$ with beta function $B(a,b)$. Then we have the nice evaluations, $$\begin{aligned}\frac{3}{5^{5/6}} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-4\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+4x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{9+4\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{9+\cosh x}} \end{aligned}\tag1$$ and, $$\begin{aligned}\frac{4}{7} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-27\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+27x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{55+12\sqrt{21}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}} \end{aligned}\tag2$$ Note the powers of fundamental units , $$U_{5}^6 = \big(\tfrac{1+\sqrt{5}}{2}\big)^6=\color{blue}{9+4\sqrt{5}}$$ $$U_{21}^3 = \big(\tfrac{5+\sqrt{21}}{2}\big)^3=\color{blue}{55+12\sqrt{21}}$$ Those two instances can't be coincidence. Question: Is it true this observation can be explained by, let $b=2a+1$, then,   $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+ax^3}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{b+\sqrt{b^2-1}\,x}}=2^{1/3}\int_0^\infty\frac{dx}{\sqrt[3]{b+\cosh x}}$$ Example: We assume it is true and use one of Noam Elkies' results as, $$\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6}; -a\big)  = \frac{6}{11^{11/12}\, U_{33}^{1/4}} $$ where $a=\sqrt{11}\,(U_{33})^{3/2}$ with fundamental unit $U_{33}=23+4\sqrt{33}$. Since $b=2a+1=45\,U_{33}$, we then have the nice integral, $$2^{1/3}\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{45\big(23+4\sqrt{33}\big)+\cosh x}}=\frac{6}{11^{11/12}\,U_{33}^{1/4}}=0.255802\dots$$ where $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}.\,$ So is it true in general?","( This summarizes results for cube roots from here and here . The fourth root version is this post .) Define $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}=\frac1{B\big(\tfrac{1}{3},\tfrac{1}{2}\big)}$ with beta function $B(a,b)$. Then we have the nice evaluations, $$\begin{aligned}\frac{3}{5^{5/6}} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-4\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+4x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{9+4\sqrt{5}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{9+\cosh x}} \end{aligned}\tag1$$ and, $$\begin{aligned}\frac{4}{7} &=\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6};-27\big)\\ &=\beta\,\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+27x^3}}\\[1.7mm] &=\beta\,\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{\color{blue}{55+12\sqrt{21}}\,x}}\\[1.7mm] &=2^{1/3}\,\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}} \end{aligned}\tag2$$ Note the powers of fundamental units , $$U_{5}^6 = \big(\tfrac{1+\sqrt{5}}{2}\big)^6=\color{blue}{9+4\sqrt{5}}$$ $$U_{21}^3 = \big(\tfrac{5+\sqrt{21}}{2}\big)^3=\color{blue}{55+12\sqrt{21}}$$ Those two instances can't be coincidence. Question: Is it true this observation can be explained by, let $b=2a+1$, then,   $$\int_0^1 \frac{dx}{\sqrt{1-x}\,\sqrt[3]{x^2+ax^3}}=\int_{-1}^1\frac{dx}{\left(1-x^2\right)^{\small2/3} \sqrt[3]{b+\sqrt{b^2-1}\,x}}=2^{1/3}\int_0^\infty\frac{dx}{\sqrt[3]{b+\cosh x}}$$ Example: We assume it is true and use one of Noam Elkies' results as, $$\,_2F_1\big(\tfrac{1}{3},\tfrac{1}{3};\tfrac{5}{6}; -a\big)  = \frac{6}{11^{11/12}\, U_{33}^{1/4}} $$ where $a=\sqrt{11}\,(U_{33})^{3/2}$ with fundamental unit $U_{33}=23+4\sqrt{33}$. Since $b=2a+1=45\,U_{33}$, we then have the nice integral, $$2^{1/3}\beta\,\int_0^\infty\frac{dx}{\sqrt[3]{45\big(23+4\sqrt{33}\big)+\cosh x}}=\frac{6}{11^{11/12}\,U_{33}^{1/4}}=0.255802\dots$$ where $\beta= \tfrac{\Gamma\big(\tfrac56\big)}{\Gamma\big(\tfrac13\big)\sqrt{\pi}}.\,$ So is it true in general?",,"['calculus', 'integration', 'definite-integrals', 'hypergeometric-function', 'pell-type-equations']"
81,"Prove that $\exists x_0, x_1\in (0,1)$, such that $\frac{f'(x_0)}{x_0}+\frac{f'(x_1)}{x_1^2}=5$","Prove that , such that","\exists x_0, x_1\in (0,1) \frac{f'(x_0)}{x_0}+\frac{f'(x_1)}{x_1^2}=5","Let $f:[0,1]\to\mathbb{R}$ be a differentiable function, such that   $f(0)=0$ and $f(1)=1$. Prove that there exist different $x_0, x_1\in (0,1)$, such that $$\frac{f'(x_0)}{x_0}+\frac{f'(x_1)}{x_1^2}=5$$ I have thought of possibly using Cauchy's theorem, so that we would only need to prove that there exist $k, l\in (0,1)$ such that $f(k)=k^2$ and $f(l)=l^3$, but I don't know how to prove these. Any hint? Edit 1 : Apparently my thought here is wrong, so any ideas?","Let $f:[0,1]\to\mathbb{R}$ be a differentiable function, such that   $f(0)=0$ and $f(1)=1$. Prove that there exist different $x_0, x_1\in (0,1)$, such that $$\frac{f'(x_0)}{x_0}+\frac{f'(x_1)}{x_1^2}=5$$ I have thought of possibly using Cauchy's theorem, so that we would only need to prove that there exist $k, l\in (0,1)$ such that $f(k)=k^2$ and $f(l)=l^3$, but I don't know how to prove these. Any hint? Edit 1 : Apparently my thought here is wrong, so any ideas?",,['calculus']
82,Anti-derivative of continuous function $\frac{1}{2+\sin x}$,Anti-derivative of continuous function,\frac{1}{2+\sin x},"I use tangent half-angle substitution to calculate this indefinite integral: $$ \int \frac{1}{2+\sin x}\,dx = \frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}+\text{constant}. $$ Wolfram Alpha also give the same answer. However, $\frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}$ is discontinuous on $(n+1)\pi$ where $n$ is any integer. Why is an anti-derivative of a continuous function discontinuous?","I use tangent half-angle substitution to calculate this indefinite integral: $$ \int \frac{1}{2+\sin x}\,dx = \frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}+\text{constant}. $$ Wolfram Alpha also give the same answer. However, $\frac{2}{\sqrt{3}}\tan^{-1}\frac{2\tan \frac{x}{2}+1}{\sqrt{3}}$ is discontinuous on $(n+1)\pi$ where $n$ is any integer. Why is an anti-derivative of a continuous function discontinuous?",,"['calculus', 'integration']"
83,Find the value of $\sum_{n =1}^\infty \frac 1 {5^{n+1}-5^n+1}$,Find the value of,\sum_{n =1}^\infty \frac 1 {5^{n+1}-5^n+1},"$$\sum_{n = 1}^\infty \dfrac 1 {5^{n+1}-5^n+1}$$ I can factorize denominator to $4\times5^n+1$ to confirm the series does not diverge, But how do I calculate its actual sum? The series is not a telescoping series nor I can partial factorise. I get confused due to $+1$  in the denominator. Thanks a lot","$$\sum_{n = 1}^\infty \dfrac 1 {5^{n+1}-5^n+1}$$ I can factorize denominator to $4\times5^n+1$ to confirm the series does not diverge, But how do I calculate its actual sum? The series is not a telescoping series nor I can partial factorise. I get confused due to $+1$  in the denominator. Thanks a lot",,"['calculus', 'sequences-and-series', 'summation']"
84,Prove that if $a_1 + a_2 + \ldots$ converges then $a_1+2a_2+4a_4+8 a_8+\ldots$ converges and $\lim na_n=0$,Prove that if  converges then  converges and,a_1 + a_2 + \ldots a_1+2a_2+4a_4+8 a_8+\ldots \lim na_n=0,"Let $a_1,a_2,a_3,\ldots$ be a decreasing sequence of positive numbers.   Show that (a) if $a_1+a_2+\ldots$ converges then $\lim_{n\rightarrow\infty} n a_n=0$ (b) $a_1+a_2+\ldots$ converges if and only if $a_1+2 a_2+4 a_4 +\ldots  $ converges. (a) If $\sum a_i$ converges then for any $\epsilon>0$ there is natural number $N_1$ such that if $n>N_1$ then $$2n \cdot a_{2n} \le\sum_{i=n}^{2n} a_i <\epsilon$$ We cam deal in the same way with the odd terms and for given $\epsilon>0$ find $N_2$ such that $$(2n+1) \cdot a_{2n+1} \le\sum_{i=n+1}^{2n+1} a_i <\epsilon$$ So for every $\epsilon>0$ there is $N=\max\{N_1,N_2\}$ such that whenever $n>N$ then $na_n <\epsilon$. Is this the correct way of proving that fascinating fact? (b) If the second series converges then since $a_1,a_2,\ldots$ is decreasing sequence of nonegative numbers, from comparison test we know that the first series converges too. For the converse I will show that partial sums of the second series are bounded. $$\begin{align*} a_1+\frac12\sum_{i=1}^N2^ia_{2^i}&=a_1+a_2+2a_4+4a_8+\dots+2^{N-1}a_{2^N}\\ &\leq a_1+ a_2+a_3+a_4+a_5+a_6+a_7+a_8+\dots+a_{2^{N-1}+1}+\dots+a_{2^N-1}+a_{2^N}\\ &\leq \sum_{i=1}^\infty a_i<\infty \end{align*}$$","Let $a_1,a_2,a_3,\ldots$ be a decreasing sequence of positive numbers.   Show that (a) if $a_1+a_2+\ldots$ converges then $\lim_{n\rightarrow\infty} n a_n=0$ (b) $a_1+a_2+\ldots$ converges if and only if $a_1+2 a_2+4 a_4 +\ldots  $ converges. (a) If $\sum a_i$ converges then for any $\epsilon>0$ there is natural number $N_1$ such that if $n>N_1$ then $$2n \cdot a_{2n} \le\sum_{i=n}^{2n} a_i <\epsilon$$ We cam deal in the same way with the odd terms and for given $\epsilon>0$ find $N_2$ such that $$(2n+1) \cdot a_{2n+1} \le\sum_{i=n+1}^{2n+1} a_i <\epsilon$$ So for every $\epsilon>0$ there is $N=\max\{N_1,N_2\}$ such that whenever $n>N$ then $na_n <\epsilon$. Is this the correct way of proving that fascinating fact? (b) If the second series converges then since $a_1,a_2,\ldots$ is decreasing sequence of nonegative numbers, from comparison test we know that the first series converges too. For the converse I will show that partial sums of the second series are bounded. $$\begin{align*} a_1+\frac12\sum_{i=1}^N2^ia_{2^i}&=a_1+a_2+2a_4+4a_8+\dots+2^{N-1}a_{2^N}\\ &\leq a_1+ a_2+a_3+a_4+a_5+a_6+a_7+a_8+\dots+a_{2^{N-1}+1}+\dots+a_{2^N-1}+a_{2^N}\\ &\leq \sum_{i=1}^\infty a_i<\infty \end{align*}$$",,"['calculus', 'sequences-and-series', 'proof-verification']"
85,How to estimate solutions to an ODE with an asymptotically nilpotent coefficient?,How to estimate solutions to an ODE with an asymptotically nilpotent coefficient?,,"Suppose $f:\mathbb R\to\mathbb R^n$ satisfies $$ f'(t) = A(t)f(t), $$ where $A$ is a smooth matrix-valued function. If I know that the matrix $A(t)$ is asymptotically nilpotent, how could I prove a sub-exponential estimate for the solution $f$? To be more explicit, suppose $A(t)^2\to0$ but $A(t)\not\to0$ as $t\to\infty$. Then one would expect slower than exponential (perhaps even linear) growth for $f$; if $A$ and $A^{-1}$ had roughly constant norm, then one would expect exponential growth. My main interest is in the case when $A(t)^2\to0$, but also $A(t)^k\to0$ for $k>2$ is interesting. If I apply Grönwall's inequality to the function $t\mapsto|f(t)|^2$ and observe that $$ \frac{d}{dt}|f(t)|^2 = 2\langle f(t),A(t)f(t)\rangle \leq 2\|A(t)\|\cdot|f(t)|^2, $$ I get the exponential estimate $$ |f(t)| \leq |f(0)|\exp\left(\int_0^t\|A(s)\|ds\right) $$ for $t>0$. This estimate is much worse than I would expect in an asymptotically nilpotent case, but I don't know how to get a polynomial (or other sub-exponential) estimate. Example: $n=2$ and $A(t)=\begin{pmatrix}0&1\\(1+t^2)^{-2}&0\end{pmatrix}$. Now $A(t)^2=(1+t^2)^{-2}I$ which goes to zero as $t\to\infty$. The solution to our ODE with $f(0)=(a,b)$ is $$ f(t) = \begin{pmatrix} \sqrt{1+t^2}(a+b\arctan(t)) \\ \frac1{\sqrt{1+t^2}}(at+b+bt\arctan(t)) \end{pmatrix}. $$ The solution grows essentially linearly: $|f(t)|\leq C|f(0)|(1+t)$ for any $t>0$ and some constant $C$. On the other hand, if I use Grönwall's inequality, I have the estimate $$ 2\langle f(t),A(t)f(t)\rangle = 2f_1(t)f_2(t)[1+(1+t^2)^{-2}] \leq |f(t)|^2[1+(1+t^2)^{-2}], $$ which cannot be significantly improved. Plugging this into Grönwall's inequality gives an exponential growth estimate for $f$, which much weaker than the linear estimate from the explicit solution. [The example ends here.] I could promote the ODE to a second order one: $f''(t)=[A(t)^2+A'(t)]f(t)$. Now the coefficient $A(t)^2$ is asymptotically small, but $A'(t)$ need not be. And even if it were, I don't know how to use Grönwall for a second order ODE. If $A$ was constant, I could use nilpotency to get $f(t)=e^{At}f(0)=(I+At)f(0)$. There is a series expansion also for time-dependent $A$ (the Dyson series ), but I couldn't see how to turn that into a rigorous estimate. I do not assume that $A(t)$ is nilpotent for any $t$, just that some power tends to zero as $t\to\infty$. Question: Given some assumptions on the decay rate of $A(t)^2$ (or $A(t)^k$ for some $k>2$), what tools could I use to prove a growth estimate for norm of the solution $f(t)$? I am looking for an estimate that I could play with to see how different decay rates for $A^2$ give different growth rates for $f$. Edit: If we denote $B(t)=A(t)+\phi(t)I$ for some scalar function $\phi$ and $g(t)=\exp\left(\int_0^t\phi(s)ds\right)f(t)$, then $g'(t)=B(t)g(t)$. One could try to get estimates for $g$ and convert them to estimates for $f$, but it seems to me that this method cannot add much. (The exponentials of integrals coming from this change of functions and Grönwall's estimate cancel each other.) This is a generalization of an idea Normal Human gave in a comment below (there $\phi$ was constant).","Suppose $f:\mathbb R\to\mathbb R^n$ satisfies $$ f'(t) = A(t)f(t), $$ where $A$ is a smooth matrix-valued function. If I know that the matrix $A(t)$ is asymptotically nilpotent, how could I prove a sub-exponential estimate for the solution $f$? To be more explicit, suppose $A(t)^2\to0$ but $A(t)\not\to0$ as $t\to\infty$. Then one would expect slower than exponential (perhaps even linear) growth for $f$; if $A$ and $A^{-1}$ had roughly constant norm, then one would expect exponential growth. My main interest is in the case when $A(t)^2\to0$, but also $A(t)^k\to0$ for $k>2$ is interesting. If I apply Grönwall's inequality to the function $t\mapsto|f(t)|^2$ and observe that $$ \frac{d}{dt}|f(t)|^2 = 2\langle f(t),A(t)f(t)\rangle \leq 2\|A(t)\|\cdot|f(t)|^2, $$ I get the exponential estimate $$ |f(t)| \leq |f(0)|\exp\left(\int_0^t\|A(s)\|ds\right) $$ for $t>0$. This estimate is much worse than I would expect in an asymptotically nilpotent case, but I don't know how to get a polynomial (or other sub-exponential) estimate. Example: $n=2$ and $A(t)=\begin{pmatrix}0&1\\(1+t^2)^{-2}&0\end{pmatrix}$. Now $A(t)^2=(1+t^2)^{-2}I$ which goes to zero as $t\to\infty$. The solution to our ODE with $f(0)=(a,b)$ is $$ f(t) = \begin{pmatrix} \sqrt{1+t^2}(a+b\arctan(t)) \\ \frac1{\sqrt{1+t^2}}(at+b+bt\arctan(t)) \end{pmatrix}. $$ The solution grows essentially linearly: $|f(t)|\leq C|f(0)|(1+t)$ for any $t>0$ and some constant $C$. On the other hand, if I use Grönwall's inequality, I have the estimate $$ 2\langle f(t),A(t)f(t)\rangle = 2f_1(t)f_2(t)[1+(1+t^2)^{-2}] \leq |f(t)|^2[1+(1+t^2)^{-2}], $$ which cannot be significantly improved. Plugging this into Grönwall's inequality gives an exponential growth estimate for $f$, which much weaker than the linear estimate from the explicit solution. [The example ends here.] I could promote the ODE to a second order one: $f''(t)=[A(t)^2+A'(t)]f(t)$. Now the coefficient $A(t)^2$ is asymptotically small, but $A'(t)$ need not be. And even if it were, I don't know how to use Grönwall for a second order ODE. If $A$ was constant, I could use nilpotency to get $f(t)=e^{At}f(0)=(I+At)f(0)$. There is a series expansion also for time-dependent $A$ (the Dyson series ), but I couldn't see how to turn that into a rigorous estimate. I do not assume that $A(t)$ is nilpotent for any $t$, just that some power tends to zero as $t\to\infty$. Question: Given some assumptions on the decay rate of $A(t)^2$ (or $A(t)^k$ for some $k>2$), what tools could I use to prove a growth estimate for norm of the solution $f(t)$? I am looking for an estimate that I could play with to see how different decay rates for $A^2$ give different growth rates for $f$. Edit: If we denote $B(t)=A(t)+\phi(t)I$ for some scalar function $\phi$ and $g(t)=\exp\left(\int_0^t\phi(s)ds\right)f(t)$, then $g'(t)=B(t)g(t)$. One could try to get estimates for $g$ and convert them to estimates for $f$, but it seems to me that this method cannot add much. (The exponentials of integrals coming from this change of functions and Grönwall's estimate cancel each other.) This is a generalization of an idea Normal Human gave in a comment below (there $\phi$ was constant).",,"['calculus', 'analysis', 'ordinary-differential-equations', 'asymptotics']"
86,Clarifying and sketching the differences between proper and improper nodes in phase space for first-order differential equations?,Clarifying and sketching the differences between proper and improper nodes in phase space for first-order differential equations?,,"So I recently read this question: Difference between improper node and proper node for phase portrait and I find myself still needing some more concrete clarification about the differences between the two and how to draw or identify them based solely on their eigenvalues and eigenvectors (if that is even possible). Here's what I've learnt so far for first-order differentials in the phase plane - please correct me if I'm wrong: Any critical point classified as a ""node"" must either be proper or improper. An improper node (with two eigenvectors) has trajectories which are parallel to the eigenvector near the fixed point and parallel to the second eigenvector far away from the fixed point. An improper node (with one eigenvector) has trajectories which are parallel to the eigenvector near the fixed point and then loop backwards and are parallel to the same eigenvector far away from the fixed point. A proper node (with two eigenvectors) has only straight-line trajectories which intersect the fixed point. A proper node with one eigenvector does not exist, right? Here are some examples - all pictures taken from this source (1) Proper node (stable), also known as a ""star node"" or ""nodal sink"" : λ are real, repeated and negative. Two linearly independent eigenvectors. (2) Proper node (unstable), also known as a ""star node"" or ""nodal source"" : λ are real, repeated and positive. Two linearly independent eigenvectors. (3) Improper node (stable), also known as a ""degenerate node"" : λ are real, repeated and negative. One independent eigenvector. (4) Improper node (unstable), also known as a ""degenerate node"" : λ are real, repeated and positive. One independent eigenvector. (5) Improper node (stable), also known as a ""degenerate node"" : λ are real, distinct and negative. Two linearly independent eigenvectors. (6) Improper node (unstable), also known as a ""degenerate node"" : λ are real, distinct and positive. Two linearly independent eigenvectors. So here is my flood of questions: For (1) and (2), if you were given their system of equations ($\vec{x}'=A\vec{x}$), how the hell can you get TWO eigenvectors given that $A$ would only have repeated eigenvalues? Another way to ask this is: if $A$ has a repeated real eigenvalue, whats the difference between a matrix that gives you one or two eigenvectors? For (1) and (2), what would the trajectories look like? Just an infinite number of straight lines intersecting the center? For examples (3) and (4), is it possible to tell from the (eigenvalues and eigenvectors) in what direction (either left or right) the trajectories trail off far away and become parallel to the eigenvector? For examples (5) and (6), is it possible to tell (from the eigenvalues and eigenvectors) which eigenvector will have the trajectories become parallel to it near the fixed point (will hug it at close range so to speak)? For centers and spirals, you could take a point on one of the axis and substitute it into the system of equations to get a vector pointing in the direction the trajectories move but this method doesn't seem as viable for improper nodes - e.g. if you picked a point on (3) or (4) and got a vector, how would you know if this vector's direction represented the direction of the trajectories that had already ""looped back"" instead of the trajectories that were hugging the eigenvector near the fixed point?  You might say ""Then don't pick a point so close to the eigenvector you silly goose!"" but how do you tell what is and isn't ""close"" enough for any example of (3) or (4)? EDIT: Figured out the answers to my own questions (I think).","So I recently read this question: Difference between improper node and proper node for phase portrait and I find myself still needing some more concrete clarification about the differences between the two and how to draw or identify them based solely on their eigenvalues and eigenvectors (if that is even possible). Here's what I've learnt so far for first-order differentials in the phase plane - please correct me if I'm wrong: Any critical point classified as a ""node"" must either be proper or improper. An improper node (with two eigenvectors) has trajectories which are parallel to the eigenvector near the fixed point and parallel to the second eigenvector far away from the fixed point. An improper node (with one eigenvector) has trajectories which are parallel to the eigenvector near the fixed point and then loop backwards and are parallel to the same eigenvector far away from the fixed point. A proper node (with two eigenvectors) has only straight-line trajectories which intersect the fixed point. A proper node with one eigenvector does not exist, right? Here are some examples - all pictures taken from this source (1) Proper node (stable), also known as a ""star node"" or ""nodal sink"" : λ are real, repeated and negative. Two linearly independent eigenvectors. (2) Proper node (unstable), also known as a ""star node"" or ""nodal source"" : λ are real, repeated and positive. Two linearly independent eigenvectors. (3) Improper node (stable), also known as a ""degenerate node"" : λ are real, repeated and negative. One independent eigenvector. (4) Improper node (unstable), also known as a ""degenerate node"" : λ are real, repeated and positive. One independent eigenvector. (5) Improper node (stable), also known as a ""degenerate node"" : λ are real, distinct and negative. Two linearly independent eigenvectors. (6) Improper node (unstable), also known as a ""degenerate node"" : λ are real, distinct and positive. Two linearly independent eigenvectors. So here is my flood of questions: For (1) and (2), if you were given their system of equations ($\vec{x}'=A\vec{x}$), how the hell can you get TWO eigenvectors given that $A$ would only have repeated eigenvalues? Another way to ask this is: if $A$ has a repeated real eigenvalue, whats the difference between a matrix that gives you one or two eigenvectors? For (1) and (2), what would the trajectories look like? Just an infinite number of straight lines intersecting the center? For examples (3) and (4), is it possible to tell from the (eigenvalues and eigenvectors) in what direction (either left or right) the trajectories trail off far away and become parallel to the eigenvector? For examples (5) and (6), is it possible to tell (from the eigenvalues and eigenvectors) which eigenvector will have the trajectories become parallel to it near the fixed point (will hug it at close range so to speak)? For centers and spirals, you could take a point on one of the axis and substitute it into the system of equations to get a vector pointing in the direction the trajectories move but this method doesn't seem as viable for improper nodes - e.g. if you picked a point on (3) or (4) and got a vector, how would you know if this vector's direction represented the direction of the trajectories that had already ""looped back"" instead of the trajectories that were hugging the eigenvector near the fixed point?  You might say ""Then don't pick a point so close to the eigenvector you silly goose!"" but how do you tell what is and isn't ""close"" enough for any example of (3) or (4)? EDIT: Figured out the answers to my own questions (I think).",,"['calculus', 'ordinary-differential-equations']"
87,The difference between an affine k-simplex and a rectilinear k-simplex,The difference between an affine k-simplex and a rectilinear k-simplex,,"The notion of rectilinear k-simplex appears in Theorem 10.27 of Rudin's book ""Principles of Mathematical analysis"", then what is the definition of a rectilinear k-simplex?  I read the proof of Theorem 10.27 and think that the proof treats oriented affine k-simplex instead of rectilinear k-simplex, is it right?","The notion of rectilinear k-simplex appears in Theorem 10.27 of Rudin's book ""Principles of Mathematical analysis"", then what is the definition of a rectilinear k-simplex?  I read the proof of Theorem 10.27 and think that the proof treats oriented affine k-simplex instead of rectilinear k-simplex, is it right?",,"['calculus', 'simplex']"
88,A formula for n-derivative of the inverse of a function?,A formula for n-derivative of the inverse of a function?,,Let $y=f^{-1}(x)$. As we know: \begin{align}  \frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{{f}'(y)}  \end{align} Thereof we have: \begin{align}  \frac{\mathrm{d^2} y}{\mathrm{d} x^2}=\frac{-{f}''(y)}{({f}'(y))^3}  \end{align} \begin{align}  \frac{\mathrm{d^3} y}{\mathrm{d} x^3}=\frac{3({f}''(y))^2-{f}'(y){f}'''(y)}{({f}'(y))^5}  \end{align} Is there a general rule for \begin{align}  \frac{\mathrm{d^n} y}{\mathrm{d} x^n}=?\end{align},Let $y=f^{-1}(x)$. As we know: \begin{align}  \frac{\mathrm{d} y}{\mathrm{d} x}=\frac{1}{{f}'(y)}  \end{align} Thereof we have: \begin{align}  \frac{\mathrm{d^2} y}{\mathrm{d} x^2}=\frac{-{f}''(y)}{({f}'(y))^3}  \end{align} \begin{align}  \frac{\mathrm{d^3} y}{\mathrm{d} x^3}=\frac{3({f}''(y))^2-{f}'(y){f}'''(y)}{({f}'(y))^5}  \end{align} Is there a general rule for \begin{align}  \frac{\mathrm{d^n} y}{\mathrm{d} x^n}=?\end{align},,"['calculus', 'functions', 'derivatives', 'induction', 'inverse']"
89,Prove That If $(x^{2}+y^{2})\cos^{2}\psi+z^{2}\cot^{2}\psi=A^2$ then $\nabla ^2 \psi=0$.,Prove That If  then .,(x^{2}+y^{2})\cos^{2}\psi+z^{2}\cot^{2}\psi=A^2 \nabla ^2 \psi=0,"Assume $$(x^{2}+y^{2})\cos^{2}\psi+z^{2}\cot^{2}\psi=A^2$$ which $A$ is constant. How we can show $\psi(x,y,z)$ satisfies the Laplacian equation $\psi_{xx}+\psi_{yy}+\psi_{zz}=0$ ($\operatorname{div}\nabla\psi=0$) without calculating $\psi(x,y,z)$? I calculate $\psi(x,y,z)$ itself and differentiate, but I'm looking for easier methods, It's not important to use what, only the time that it takes is important.","Assume $$(x^{2}+y^{2})\cos^{2}\psi+z^{2}\cot^{2}\psi=A^2$$ which $A$ is constant. How we can show $\psi(x,y,z)$ satisfies the Laplacian equation $\psi_{xx}+\psi_{yy}+\psi_{zz}=0$ ($\operatorname{div}\nabla\psi=0$) without calculating $\psi(x,y,z)$? I calculate $\psi(x,y,z)$ itself and differentiate, but I'm looking for easier methods, It's not important to use what, only the time that it takes is important.",,"['calculus', 'multivariable-calculus']"
90,How many roots are there of function $f(x)=x^4-3x^2+x-1$,How many roots are there of function,f(x)=x^4-3x^2+x-1,"Problem: How many (real) roots are there of function $f(x)=x^4-3x^2+x-1$ First, I can directly say since degree is $4$ then there can be at most $4$ roots of functions. Moreover, apparently $f(0)=-1$ and $\lim_ {x \to \pm \infty} = \infty$ , so because of the IVT there are at least $2$ numbers $c,d \in \mathbb R$ such that $f(c)=f(d)=0$ . One from negative and other one from positive side. Actually, I can conclude the answer by sketching graph but which tool I can use to show that there are exactly $2$ roots for it without using graph.","Problem: How many (real) roots are there of function First, I can directly say since degree is then there can be at most roots of functions. Moreover, apparently and , so because of the IVT there are at least numbers such that . One from negative and other one from positive side. Actually, I can conclude the answer by sketching graph but which tool I can use to show that there are exactly roots for it without using graph.","f(x)=x^4-3x^2+x-1 4 4 f(0)=-1 \lim_ {x \to \pm \infty} = \infty 2 c,d \in \mathbb R f(c)=f(d)=0 2","['calculus', 'algebra-precalculus', 'polynomials']"
91,$\lim x_n^{x_n}=4$ prove that $\lim x_n=2$ [duplicate],prove that  [duplicate],\lim x_n^{x_n}=4 \lim x_n=2,"This question already has answers here : Prove $\lim_{n\to\infty}x_n=2$ Given $\lim_{n \to \infty} x_n^{x_n} = 4$ (4 answers) Closed 8 years ago . Let $(x_n)$ be a sequence of real numbers, such that: $\lim x_n^{x_n}=4$, prove that $\lim x_n=2$ I'm not sure if my proof is right. I assumed that $\lim x_n $ isn't 2 and using Cauchy's criterion: $|x_n-2|>\epsilon$  so $ x_n>\epsilon+2$  or $x_n<-\epsilon+2$ $|x_n^{x_n}-4|<\epsilon $ so $x_n<\sqrt[x_n]{\epsilon+4}$ and then we combine what we've found and get: $\epsilon+2<\sqrt[x_n]{\epsilon+4}$ $\epsilon+4<(\epsilon+2)^2<(\epsilon+2)^{\epsilon+2}<(\epsilon+2)^{x_n}<\epsilon+4$  and it's not true so  $\lim x_n=2$. Is that okay? Edit: I just wanted to know if my solution was right but the other post helped as well, thanks.","This question already has answers here : Prove $\lim_{n\to\infty}x_n=2$ Given $\lim_{n \to \infty} x_n^{x_n} = 4$ (4 answers) Closed 8 years ago . Let $(x_n)$ be a sequence of real numbers, such that: $\lim x_n^{x_n}=4$, prove that $\lim x_n=2$ I'm not sure if my proof is right. I assumed that $\lim x_n $ isn't 2 and using Cauchy's criterion: $|x_n-2|>\epsilon$  so $ x_n>\epsilon+2$  or $x_n<-\epsilon+2$ $|x_n^{x_n}-4|<\epsilon $ so $x_n<\sqrt[x_n]{\epsilon+4}$ and then we combine what we've found and get: $\epsilon+2<\sqrt[x_n]{\epsilon+4}$ $\epsilon+4<(\epsilon+2)^2<(\epsilon+2)^{\epsilon+2}<(\epsilon+2)^{x_n}<\epsilon+4$  and it's not true so  $\lim x_n=2$. Is that okay? Edit: I just wanted to know if my solution was right but the other post helped as well, thanks.",,"['calculus', 'sequences-and-series', 'proof-verification']"
92,Finding $\lim_{x\to 0} \frac {2\sin x-\sin 2x}{x-\sin x}$ geometrically,Finding  geometrically,\lim_{x\to 0} \frac {2\sin x-\sin 2x}{x-\sin x},"While looking at this question , I noticed an interesting geometric interpretation of the limit the OP was trying to evaluate.  His limit came to twice the value of the limit $$\lim_{x\to 0}\frac{\sin x (1-\cos(x))}{x-\sin x}$$ which can be interpreted as the limit of the ratio of the area of $\triangle DBC$ and the area bounded between the line $CB$ and the arc $CB$. (See the image below). I know that the limit $\lim_{x\to 0} \frac{\sin x}{x} = 1$ is often derived through a geometric method.  Can this the limit I've given be evaluated geometrically?","While looking at this question , I noticed an interesting geometric interpretation of the limit the OP was trying to evaluate.  His limit came to twice the value of the limit $$\lim_{x\to 0}\frac{\sin x (1-\cos(x))}{x-\sin x}$$ which can be interpreted as the limit of the ratio of the area of $\triangle DBC$ and the area bounded between the line $CB$ and the arc $CB$. (See the image below). I know that the limit $\lim_{x\to 0} \frac{\sin x}{x} = 1$ is often derived through a geometric method.  Can this the limit I've given be evaluated geometrically?",,"['calculus', 'geometry']"
93,How to evaluate the integral $\int_0^\infty \frac{x^{a-1}}{1+bx^a} e^{-x} dx$,How to evaluate the integral,\int_0^\infty \frac{x^{a-1}}{1+bx^a} e^{-x} dx,How to evaluate this integral? \begin{equation}   \int_0^\infty \frac{x^{a-1}}{1+bx^a} e^{-x} dx \end{equation} I think it will use a gamma function or a exponential integral. I really need an advice to continue. $a$ and $b$ are real constants.,How to evaluate this integral? \begin{equation}   \int_0^\infty \frac{x^{a-1}}{1+bx^a} e^{-x} dx \end{equation} I think it will use a gamma function or a exponential integral. I really need an advice to continue. $a$ and $b$ are real constants.,,"['calculus', 'integration', 'definite-integrals']"
94,"If all convex combinations of $p(x)$ and $q(x)$ have real roots, then $p,q$ have a common interlacing poly","If all convex combinations of  and  have real roots, then  have a common interlacing poly","p(x) q(x) p,q","I heard this result in a talk the other day: Suppose $p$ and $q$ are polynomials. Suppose $p$ is a polynomial of degree $n$ and $q$ a polynomial of degree $n-1$ . Call $q$ an interlacer of $p$ if the roots $a_i$ of $p$ and $b_i$ of $q$ are such that $$a_1 \leq b_1 \leq a_2 \leq b_2 \leq \dotsb \leq b_{n-1} \leq a_n.$$ Suppose $a,b$ are polynomials of the same degree such that $\lambda a(x) + (1-\lambda )b(x)$ has only real roots for all $\lambda \in [0,1]$ . Then $a$ and $b$ have a common interlacing polynomial. I've been thinking about how to prove this. Does anyone have an idea?",I heard this result in a talk the other day: Suppose and are polynomials. Suppose is a polynomial of degree and a polynomial of degree . Call an interlacer of if the roots of and of are such that Suppose are polynomials of the same degree such that has only real roots for all . Then and have a common interlacing polynomial. I've been thinking about how to prove this. Does anyone have an idea?,"p q p n q n-1 q p a_i p b_i q a_1 \leq b_1 \leq a_2 \leq b_2 \leq \dotsb \leq b_{n-1} \leq a_n. a,b \lambda a(x) + (1-\lambda )b(x) \lambda \in [0,1] a b","['calculus', 'polynomials', 'roots']"
95,"Let $f:[a,b]\to\mathbb R$ be Riemann integrable and $f>0$. Prove that $\int_a^bf>0$. (Without Measure theory)",Let  be Riemann integrable and . Prove that . (Without Measure theory),"f:[a,b]\to\mathbb R f>0 \int_a^bf>0","I've been struggling with this for a while, and I have a couple of leads that kind of got me nowhere: At first I thought that if $f$ is continuous somewhere then the integral will be $>0$. So, if the integral was $0$ then that would mean it would need to be nowhere continuous. That seemed unlikely to me, but I couldn't prove the existence of a point at which it is continuous. For the integral to be $0$ it would necessitate that for any sub interval of $[a,b]$ the function's infimum would have to be $0$. Also seems weird for $f>0$. Again, got me nowhere. I should mention that I'm aware that it's possible to prove this by defining ""Measure"" and all that, but I don't want to go there. I'm wondering if there are more elementary tools to show the above. Thank you!","I've been struggling with this for a while, and I have a couple of leads that kind of got me nowhere: At first I thought that if $f$ is continuous somewhere then the integral will be $>0$. So, if the integral was $0$ then that would mean it would need to be nowhere continuous. That seemed unlikely to me, but I couldn't prove the existence of a point at which it is continuous. For the integral to be $0$ it would necessitate that for any sub interval of $[a,b]$ the function's infimum would have to be $0$. Also seems weird for $f>0$. Again, got me nowhere. I should mention that I'm aware that it's possible to prove this by defining ""Measure"" and all that, but I don't want to go there. I'm wondering if there are more elementary tools to show the above. Thank you!",,"['calculus', 'integration']"
96,Convergence of this series,Convergence of this series,,"From an old Putnam : Prove that if $(x_n)$ is a sequence of positive real numbers, and $\sum{x_n}$ converges, then so does $\sum{(x_n)^{\frac{n}{n+1}}}$.","From an old Putnam : Prove that if $(x_n)$ is a sequence of positive real numbers, and $\sum{x_n}$ converges, then so does $\sum{(x_n)^{\frac{n}{n+1}}}$.",,"['calculus', 'sequences-and-series']"
97,"""Cut"" (hexagon-like) Reuleaux triangle area","""Cut"" (hexagon-like) Reuleaux triangle area",,"Let me start by giving the reason my question: as part of a 3D printer I'm building ( Rostock ), I'm trying to figure out the work area of the printer. The printer consists of 3 arms, each attached at the points of an equilateral triangle. Each arm can rotate in every direction in front of its attachment point, so the reachable area of each arm is a half circle. The work area of the printer is the intersection of each of these half circles. My question is: how does one calculate the area of the ""cut"" (hexagon-like) Reuleaux triangle that appears once the length of the arms becomes longer than the distance between the points of the equilateral triangle? I realize this might be quite vague, so I've attached two pictures. In the first, the arm reach is short and the intersection is a Reuleaux triangle. In the second, the arms are longer and the ""cut"" (hexagon-like) Reuleaux triangle appears. It's the area of this shape that I'm trying to calculate. Normal Reuleaux triangle ""Cut"" (hexagon-like) Reuleaux triangle","Let me start by giving the reason my question: as part of a 3D printer I'm building ( Rostock ), I'm trying to figure out the work area of the printer. The printer consists of 3 arms, each attached at the points of an equilateral triangle. Each arm can rotate in every direction in front of its attachment point, so the reachable area of each arm is a half circle. The work area of the printer is the intersection of each of these half circles. My question is: how does one calculate the area of the ""cut"" (hexagon-like) Reuleaux triangle that appears once the length of the arms becomes longer than the distance between the points of the equilateral triangle? I realize this might be quite vague, so I've attached two pictures. In the first, the arm reach is short and the intersection is a Reuleaux triangle. In the second, the arms are longer and the ""cut"" (hexagon-like) Reuleaux triangle appears. It's the area of this shape that I'm trying to calculate. Normal Reuleaux triangle ""Cut"" (hexagon-like) Reuleaux triangle",,"['calculus', 'geometry', 'computational-geometry']"
98,Evaluating the integral $ \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv$,Evaluating the integral, \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv,"I am trying to evaluate the integral $$  \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv = e^{-y^2}\int_0^1 \frac{e^{-y^2v^2}}{(1+v^2)^n}dv $$ for $n\in \mathbb{N}$ .For n=1 one finds Owen's T function , i.e. \begin{align} \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)}dv=2\pi \operatorname{T}\left(\sqrt{2} y,1\right) = \frac{\pi}{2} \operatorname{erfc}(y) \left(1 - \frac{1}{2} \operatorname{erfc}(y)\right) \end{align} A nice source on the Owen's T function is [ 2 ]. In [ 3 ] they state that \begin{align} \int \frac{e^{-v^2}}{v^2 + 1} dv , \end{align} has no anti-derivative. Hence, I do not suspect one can find one for our integral. This integral occurs in a series I am integrating over for a approximation I am performing. Hence, it would already be nice if I could find the second (n=2) and third (n=3) term. Has someone an idea how to evaluate the integral. Many thank in advance!","I am trying to evaluate the integral for .For n=1 one finds Owen's T function , i.e. A nice source on the Owen's T function is [ 2 ]. In [ 3 ] they state that has no anti-derivative. Hence, I do not suspect one can find one for our integral. This integral occurs in a series I am integrating over for a approximation I am performing. Hence, it would already be nice if I could find the second (n=2) and third (n=3) term. Has someone an idea how to evaluate the integral. Many thank in advance!","
 \int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)^n}dv = e^{-y^2}\int_0^1 \frac{e^{-y^2v^2}}{(1+v^2)^n}dv
 n\in \mathbb{N} \begin{align}
\int_0^1 \frac{e^{-y^2(1+v^2)}}{(1+v^2)}dv=2\pi \operatorname{T}\left(\sqrt{2} y,1\right) = \frac{\pi}{2} \operatorname{erfc}(y) \left(1 - \frac{1}{2} \operatorname{erfc}(y)\right)
\end{align} \begin{align}
\int \frac{e^{-v^2}}{v^2 + 1} dv ,
\end{align}","['calculus', 'integration', 'definite-integrals', 'gaussian-integral', 'gaussian']"
99,The meaning of $dx$ in an indefinite integral,The meaning of  in an indefinite integral,dx,"This semester I'm taking integral calculus for the first time. We started with the differential (i.e. $dy=f'(x)\,dx$ ) and right after that with the indefinite integral. Since then, I've been trying to make sense of the $dx$ when it's part of an indefinite integral (i.e. $\int f(x) \, \boldsymbol{dx}$ ). I know there are already a bazillion answers regarding this question, but all of them refer to the definite integral and the ones that do touch on indefinite integrals only say things like "" it's just a syntactical device to tell you the variable to differentiate with respect to or the integration variable "" (Ihf, 2012, web). I don't like this answer, especially because I was taught that given two functions $f(x)$ , $g(x)$ and the antiderivative of their product $\int f(x)g(x)\,dx$ , if I were to assign $f(x)$ to $u$ and $g(x)\,dx$ to $dv$ in order to integrate by parts, then I would have to integrate $dv$ to find $v$ . This only makes sense if $dx$ means something by itself and is not just a quirk of the notation, otherwise we would end up with something like the following: $$\int g(x)\,dx\space dx$$ After thinking about it a lot, I believe I finally found a way to make sense of the $dx$ as something that isn't purely and simply a notation device. My reasoning is as follows: Given a function $f(x)$ , let $y=f(x)$ . Then $$\frac{dy}{dx}=f'(x)$$ Then from the fundamental theorem of calculus, we know that $$\int\frac{dy}{dx}\,dx=y$$ Let $dy=f'(x)\,dx$ . Then the equation $$dy=\frac{dy}{dx}\,dx$$ holds and $$\int\frac{dy}{dx}\,dx=\int dy=y$$ Finally, integrate both sides of the equation $dy=f'(x)\,dx$ in order that $$\int dy=\int f'(x)\,dx\implies y=f(x)$$ From this I conclude that, by integrating a function, what we are really doing is integrating the differential of that function. This makes perfect sense to me, although I'm aware that seemingly logical things aren't necessarily logical. That's why I would appreciate it if someone could tell me whether the above is mathematically correct or pure gibberish. PS, I had never written a mathematical proof before and I have taken no proofs courses yet, so any suggestions are welcome. Reference: lhf . (2012, May 9). What does $dx$ mean? . Mathematics Stack Exchange. https://math.stackexchange.com/q/143262","This semester I'm taking integral calculus for the first time. We started with the differential (i.e. ) and right after that with the indefinite integral. Since then, I've been trying to make sense of the when it's part of an indefinite integral (i.e. ). I know there are already a bazillion answers regarding this question, but all of them refer to the definite integral and the ones that do touch on indefinite integrals only say things like "" it's just a syntactical device to tell you the variable to differentiate with respect to or the integration variable "" (Ihf, 2012, web). I don't like this answer, especially because I was taught that given two functions , and the antiderivative of their product , if I were to assign to and to in order to integrate by parts, then I would have to integrate to find . This only makes sense if means something by itself and is not just a quirk of the notation, otherwise we would end up with something like the following: After thinking about it a lot, I believe I finally found a way to make sense of the as something that isn't purely and simply a notation device. My reasoning is as follows: Given a function , let . Then Then from the fundamental theorem of calculus, we know that Let . Then the equation holds and Finally, integrate both sides of the equation in order that From this I conclude that, by integrating a function, what we are really doing is integrating the differential of that function. This makes perfect sense to me, although I'm aware that seemingly logical things aren't necessarily logical. That's why I would appreciate it if someone could tell me whether the above is mathematically correct or pure gibberish. PS, I had never written a mathematical proof before and I have taken no proofs courses yet, so any suggestions are welcome. Reference: lhf . (2012, May 9). What does mean? . Mathematics Stack Exchange. https://math.stackexchange.com/q/143262","dy=f'(x)\,dx dx \int f(x) \, \boldsymbol{dx} f(x) g(x) \int f(x)g(x)\,dx f(x) u g(x)\,dx dv dv v dx \int g(x)\,dx\space dx dx f(x) y=f(x) \frac{dy}{dx}=f'(x) \int\frac{dy}{dx}\,dx=y dy=f'(x)\,dx dy=\frac{dy}{dx}\,dx \int\frac{dy}{dx}\,dx=\int dy=y dy=f'(x)\,dx \int dy=\int f'(x)\,dx\implies y=f(x) dx","['calculus', 'integration', 'notation', 'indefinite-integrals', 'differential']"
