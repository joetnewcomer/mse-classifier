,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Total differential of a composite function,Total differential of a composite function,,"Let $w=f(u)$ . Determine $dw$ , if $u=x^2+xy+y^2+z^2-3x+5y-2z$ I found that $\ du = (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz .$ Assuming $ dw = \frac{\partial w}{\partial u} du,$ I obtained: $$ dw = \frac{\partial w}{\partial u} \ du = \frac{\partial w}{\partial u}( (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz) . $$ Is this correct? Is it sufficient? Is there anything else I should do next?","Let . Determine , if I found that Assuming I obtained: Is this correct? Is it sufficient? Is there anything else I should do next?","w=f(u) dw u=x^2+xy+y^2+z^2-3x+5y-2z \ du = (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz .  dw = \frac{\partial w}{\partial u} du, 
dw = \frac{\partial w}{\partial u} \ du = \frac{\partial w}{\partial u}( (2x + y - 3) dx + (x + 2y + 5)dy + (2z - 2)dz) .
","['multivariable-calculus', 'derivatives', 'partial-derivative']"
1,Relationship between a differential form and the tangent plane,Relationship between a differential form and the tangent plane,,"Consider a point $(1, 2, 3) \in \Bbb R^3$ , if we consider an exterior form of degree $1$ , given by $$w=\sum_{i=1}^3 a_{i} dx_{i}$$ So, for our case $$ w(1, 2, 3) = \sum_{i=1}^3 a_{i}(1, 2, 3) \, dx_{i(1, 2, 3)} := a_{1}(1, 2, 3) \, dx_{(1, 2, 3)} + 2 \, a_{2}(1, 2, 3) \, dy_{(1, 2, 3)} + 3 \, a_{3}(1, 2, 3) \, dz_{(1, 2, 3)} $$ If we try to take the kernel of that application for that point, we could make a particular case and find the points $x_1,$ , $x_2$ and $x_3$ for which \begin{align*}    dx_{(1, 2, 3)} (x_1, x_2, x_3)=0 &\implies dx_{(1, 2, 3)} (x_{1}(1, 0, 0)+x_{2}(0, 1, 0)+x_{3}(0, 0, 1))=0\\     &\implies x_{1}dx_{(1, 2, 3)}(1, 0, 0)+x_{2}dx_{(1, 2, 3)}(0, 1, 0)+x_{3}dx_{(1, 2, 3)}(0, 0, 1)=0 \end{align*} Here, starting from kronecker's delta function, i.e. knowing that $dx_{i}(e_{j})=0$ if $i \neq j$ and $dx_{i}(e_{j})=1$ if $i = j$ Perhaps I could state that $0+x_2+x_3=0$ And it would get a value that belongs to the kernel of that application, say $v \in \mathbb{R}_{(1, 2, 3)}^3$ , we could state that $$w(p) \cdot v=0?$$ My main idea of all this is to consider the tangent plane of a surface associated to the point $(1, 2)$ and determine what relation has the kernel of the application, with that tangent plane searched. Any idea or suggestion that helps me to get this idea on track, it was an idea subtly mentioned by my professor, but I can't find the right formalization to express it. Thanks!","Consider a point , if we consider an exterior form of degree , given by So, for our case If we try to take the kernel of that application for that point, we could make a particular case and find the points , and for which Here, starting from kronecker's delta function, i.e. knowing that if and if Perhaps I could state that And it would get a value that belongs to the kernel of that application, say , we could state that My main idea of all this is to consider the tangent plane of a surface associated to the point and determine what relation has the kernel of the application, with that tangent plane searched. Any idea or suggestion that helps me to get this idea on track, it was an idea subtly mentioned by my professor, but I can't find the right formalization to express it. Thanks!","(1, 2, 3) \in \Bbb R^3 1 w=\sum_{i=1}^3 a_{i} dx_{i} 
w(1, 2, 3) = \sum_{i=1}^3 a_{i}(1, 2, 3) \, dx_{i(1, 2, 3)} := a_{1}(1, 2, 3) \, dx_{(1, 2, 3)} + 2 \, a_{2}(1, 2, 3) \, dy_{(1, 2, 3)} + 3 \, a_{3}(1, 2, 3) \, dz_{(1, 2, 3)}
 x_1, x_2 x_3 \begin{align*}
   dx_{(1, 2, 3)} (x_1, x_2, x_3)=0 &\implies dx_{(1, 2, 3)} (x_{1}(1, 0, 0)+x_{2}(0, 1, 0)+x_{3}(0, 0, 1))=0\\
    &\implies x_{1}dx_{(1, 2, 3)}(1, 0, 0)+x_{2}dx_{(1, 2, 3)}(0, 1, 0)+x_{3}dx_{(1, 2, 3)}(0, 0, 1)=0
\end{align*} dx_{i}(e_{j})=0 i \neq j dx_{i}(e_{j})=1 i = j 0+x_2+x_3=0 v \in \mathbb{R}_{(1, 2, 3)}^3 w(p) \cdot v=0? (1, 2)","['multivariable-calculus', 'differential-geometry']"
2,Stokes theorem to calculate line integral,Stokes theorem to calculate line integral,,"Let $\gamma$ be the intersection between $z=x^2+y^2$ and the plane $z=1+2x$ . Calculate the work done by the field $F=(0,x,-y)$ when the curve $\gamma$ traverses on lap in positive direction seen from z-axis. I should solve this using stokes theorem. Setting both equations above equal we get $1+2x=x^2+y^2\iff 2=(x-1)^2+y^2$ . Then I set $x=s \cos t+1$ and $y=s\sin t$ with $0\leq s\leq \sqrt{2}$ and $0\leq t\leq2\pi$ . Then parametrization of the plane gives $r(s,t)=(s \cos t+1,s\sin t, 2+s\cos t)$ . Further, $r_s=(\cos t, \sin t,\cos t)$ and $r_t=(-s \sin t, s \cos t, -s \sin t)$ and $r_s \times r_t=(-s,0,s)$ , also $\text{curl }F=(-1,0,1) $ so $\int\int_\Gamma \text{curl } F\cdot ndS=\int^{2\pi}_0\int^{\sqrt{2}}_0 2s dsdt=4\pi $ . But the answer is $6\pi$ . So how do I solve this without calculating the area of the ellipse given of the intersecting line? Does not the parametrization I wrote give the surface which is enclosed by the line of the intersection?","Let be the intersection between and the plane . Calculate the work done by the field when the curve traverses on lap in positive direction seen from z-axis. I should solve this using stokes theorem. Setting both equations above equal we get . Then I set and with and . Then parametrization of the plane gives . Further, and and , also so . But the answer is . So how do I solve this without calculating the area of the ellipse given of the intersecting line? Does not the parametrization I wrote give the surface which is enclosed by the line of the intersection?","\gamma z=x^2+y^2 z=1+2x F=(0,x,-y) \gamma 1+2x=x^2+y^2\iff 2=(x-1)^2+y^2 x=s \cos t+1 y=s\sin t 0\leq s\leq \sqrt{2} 0\leq t\leq2\pi r(s,t)=(s \cos t+1,s\sin t, 2+s\cos t) r_s=(\cos t, \sin t,\cos t) r_t=(-s \sin t, s \cos t, -s \sin t) r_s \times r_t=(-s,0,s) \text{curl }F=(-1,0,1)  \int\int_\Gamma \text{curl } F\cdot ndS=\int^{2\pi}_0\int^{\sqrt{2}}_0 2s dsdt=4\pi  6\pi","['multivariable-calculus', 'vector-analysis']"
3,If $F$ attains its local minimum at $x=0$ then $F(x)-F(0)\geq\xi x$,If  attains its local minimum at  then,F x=0 F(x)-F(0)\geq\xi x,"Let $F:\mathbb{R}\to\mathbb{R}$ be the convex mapping. Prove that there exists derivatives $F'(0+), F'(0-)$ and if $F$ attains its local minimum at $x=0$ then $$F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)].$$ I have proved that there exists $F'(0+), F'(0-)$ . I'm stuck at proving if $F$ attains its local minimum at $x=0$ then $F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)].$ I know that if $F$ attains its local minimum at $x=0$ then $F(x)>F(0)\quad\forall V(0,\epsilon)\setminus\{0\}$ , where $V(0,\epsilon)$ is the neighborhood of $0$ . Could someone help me to deal with the rest of the problem? Thanks in advance!","Let be the convex mapping. Prove that there exists derivatives and if attains its local minimum at then I have proved that there exists . I'm stuck at proving if attains its local minimum at then I know that if attains its local minimum at then , where is the neighborhood of . Could someone help me to deal with the rest of the problem? Thanks in advance!","F:\mathbb{R}\to\mathbb{R} F'(0+), F'(0-) F x=0 F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)]. F'(0+), F'(0-) F x=0 F(x)-F(0)\geq \xi x,\quad\forall x,\xi\in[F'(0-),F'(0+)]. F x=0 F(x)>F(0)\quad\forall V(0,\epsilon)\setminus\{0\} V(0,\epsilon) 0","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives']"
4,Semi-differentiable function vs. restriction of a differentiable function,Semi-differentiable function vs. restriction of a differentiable function,,"I am studying manifolds with boundary, and the definition of a continuous or smooth function on the domain $\mathbb{H}^n \subset \mathbb{R}^n = \{(x_1, \dots, x_n): x_n \geq 0\}$ is a function that is the restriction of a continuous or smooth function defined on a neighborhood of $\mathbb{H}^n$ . Is this definition equivalent to the definition that a function on $\mathbb{H}^n$ is continuous or smooth if we only look at limits of the function or limits of the function's difference quotient at sequences of points that stay in the function's domain? (This is the notion of left- and right- handed limits, or I think semi-differentiability .) Does this equivalence hold for an arbitrary subset of Euclidean space that is not $\mathbb{H}^n$ , possibly with some assumptions? Note: My question seems to be a common one, but I can't find an answer for it. It is unanswered in the comments to the first answer here . I have been searching up ""extensions of continuous functions on an arbitrary subset of Euclidean space,"" and the results do not seem to be what I am asking. Call the definition in paragraph 1 Def 1, and the one in paragraph 2 Def 2. I believe this asks about taking Def 1 and extending to all of Euclidean space, this asks about enlarging the domain for an already open set, and several questions are about converting Def 1 locally into Def 1 globally by a partition of unity. I am asking about assuming Def 2 and ""extending to a neighborhood of the set"" to get Def 1.","I am studying manifolds with boundary, and the definition of a continuous or smooth function on the domain is a function that is the restriction of a continuous or smooth function defined on a neighborhood of . Is this definition equivalent to the definition that a function on is continuous or smooth if we only look at limits of the function or limits of the function's difference quotient at sequences of points that stay in the function's domain? (This is the notion of left- and right- handed limits, or I think semi-differentiability .) Does this equivalence hold for an arbitrary subset of Euclidean space that is not , possibly with some assumptions? Note: My question seems to be a common one, but I can't find an answer for it. It is unanswered in the comments to the first answer here . I have been searching up ""extensions of continuous functions on an arbitrary subset of Euclidean space,"" and the results do not seem to be what I am asking. Call the definition in paragraph 1 Def 1, and the one in paragraph 2 Def 2. I believe this asks about taking Def 1 and extending to all of Euclidean space, this asks about enlarging the domain for an already open set, and several questions are about converting Def 1 locally into Def 1 globally by a partition of unity. I am asking about assuming Def 2 and ""extending to a neighborhood of the set"" to get Def 1.","\mathbb{H}^n \subset \mathbb{R}^n = \{(x_1, \dots, x_n): x_n \geq 0\} \mathbb{H}^n \mathbb{H}^n \mathbb{H}^n","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
5,What coordinate substitution should I perform to evaluate this triple integral?,What coordinate substitution should I perform to evaluate this triple integral?,,"I am trying to evaluate the following triple integral: \begin{equation} \int_{-1}^1 \int_{-\sqrt{4-4x^2}}^{\sqrt{4-4x^2}} \int_{\sqrt{4x^2 + z^2}}^2 ye^{4x^2 + y^2 + z^2} \, dy\, dz\, dx \end{equation} My work so far is as follows. Firstly, I noticed that trying to integrate $e^{4x^2 + y^2 + z^2}$ directly with respect to $x$ , $y$ , or $z$ is probably futile. As a result, I've been trying to change the coordinate system from Cartesian to another coordinate system (probably cylindrical or spherical). For reference, the region of integration is a cone with height along the $y$ -axis: As a result, I was motivated to use cylindrical coordinates . However, I've had difficulties with this, due to the $\sqrt{4x^2 + z^2}$ in the limits of integration not seeming to correspond well to cylindrical coordinates, and also the same issue for the $4x^2 + y^2 + z^2$ exponent in the function. My only other idea was to use spherical coordinates, motivated by the presence of $4x^2 + y^2 + z^2$ (since $x^2 + y^2 + z^2 = \rho$ in spherical coordinates). However, describing the region of integration using spherical coordinates is difficult. Can anyone provide hints/a solution to this problem? I've tried everything I know, but I'm still stuck.","I am trying to evaluate the following triple integral: My work so far is as follows. Firstly, I noticed that trying to integrate directly with respect to , , or is probably futile. As a result, I've been trying to change the coordinate system from Cartesian to another coordinate system (probably cylindrical or spherical). For reference, the region of integration is a cone with height along the -axis: As a result, I was motivated to use cylindrical coordinates . However, I've had difficulties with this, due to the in the limits of integration not seeming to correspond well to cylindrical coordinates, and also the same issue for the exponent in the function. My only other idea was to use spherical coordinates, motivated by the presence of (since in spherical coordinates). However, describing the region of integration using spherical coordinates is difficult. Can anyone provide hints/a solution to this problem? I've tried everything I know, but I'm still stuck.","\begin{equation}
\int_{-1}^1 \int_{-\sqrt{4-4x^2}}^{\sqrt{4-4x^2}} \int_{\sqrt{4x^2 + z^2}}^2 ye^{4x^2 + y^2 + z^2} \, dy\, dz\, dx
\end{equation} e^{4x^2 + y^2 + z^2} x y z y \sqrt{4x^2 + z^2} 4x^2 + y^2 + z^2 4x^2 + y^2 + z^2 x^2 + y^2 + z^2 = \rho","['integration', 'multivariable-calculus', 'definite-integrals', 'spherical-coordinates', 'cylindrical-coordinates']"
6,Implicit differentiation with 3 codependent variables,Implicit differentiation with 3 codependent variables,,"I have been going over some exercises on multivariable calculus and have stumbled across the following: Suppose we have some implicit multivariable function of x,y,z: $$F(x,y,z)=0$$ such that $$x=h(y,z), y=g(x,z), z=f(x,y).$$ show that $$ \frac{\partial z}{\partial x}  \frac{\partial x}{\partial y}  \frac{\partial y}{\partial z} = -1$$ Whenever I see the solutions presented to this, they immediately just cite the theorem for implicit differentiation in 3 variables, even though this theorem seems to hinge on the fact that we can, for example, when considering $\frac{dF}{dx}$ , let $\frac{dy}{dx}$ be equal to zero since $y$ is not a function of $x$ , when clearly it is in this specific case. I am slightly confused by this - is this a consequence of us reducing our problem to one where we need only consider $x,z$ since $y$ wholly depends on them, meaning that we need only consider these two variables instead of all three, bringing us back to one with just 2 independent variables?","I have been going over some exercises on multivariable calculus and have stumbled across the following: Suppose we have some implicit multivariable function of x,y,z: such that show that Whenever I see the solutions presented to this, they immediately just cite the theorem for implicit differentiation in 3 variables, even though this theorem seems to hinge on the fact that we can, for example, when considering , let be equal to zero since is not a function of , when clearly it is in this specific case. I am slightly confused by this - is this a consequence of us reducing our problem to one where we need only consider since wholly depends on them, meaning that we need only consider these two variables instead of all three, bringing us back to one with just 2 independent variables?","F(x,y,z)=0 x=h(y,z), y=g(x,z), z=f(x,y).  \frac{\partial z}{\partial x}  \frac{\partial x}{\partial y}  \frac{\partial y}{\partial z} = -1 \frac{dF}{dx} \frac{dy}{dx} y x x,z y",['multivariable-calculus']
7,How to generalize curvature to n dimensions parameterized by time instead of arc length?,How to generalize curvature to n dimensions parameterized by time instead of arc length?,,"I am a novice in mathematics in general and even more so in differential geometry. Currently, I am looking to generalize the Frenet-Serret formulas to $n$ dimensions. At the moment, I am interested in the generalization of curvature to n dimensions. On this Wikipedia page , a formula for generalized curvature is proposed, which is as follows: $$ X_i(s)=\frac{\langle e'_i(s),e_{i+1}(s)\rangle}{\|r'(s)\|} $$ On the same page, $$e_j(s) = \frac{\bar{e}_j(s)}{\|\bar{e}_j(s)\|}$$ with $$\bar{e}_j(s) = r^{(j)}(s) - \sum_{i=1}^{j-1} \langle r^{(j)}(s),e_i(s)\rangle e_i(s)$$ . The problem, for me, is that both $X_i(s)$ and $e_j(s)$ are parameterized by the arc length $s$ . What I am looking for is rather than these expressions being expressed as a function of time $t$ . My question is, how do I transition from $X_i(s)$ and $e_j(s)$ to $X_i(t)$ and $e_j(t)$ ?","I am a novice in mathematics in general and even more so in differential geometry. Currently, I am looking to generalize the Frenet-Serret formulas to dimensions. At the moment, I am interested in the generalization of curvature to n dimensions. On this Wikipedia page , a formula for generalized curvature is proposed, which is as follows: On the same page, with . The problem, for me, is that both and are parameterized by the arc length . What I am looking for is rather than these expressions being expressed as a function of time . My question is, how do I transition from and to and ?","n 
X_i(s)=\frac{\langle e'_i(s),e_{i+1}(s)\rangle}{\|r'(s)\|}
 e_j(s) = \frac{\bar{e}_j(s)}{\|\bar{e}_j(s)\|} \bar{e}_j(s) = r^{(j)}(s) - \sum_{i=1}^{j-1} \langle r^{(j)}(s),e_i(s)\rangle e_i(s) X_i(s) e_j(s) s t X_i(s) e_j(s) X_i(t) e_j(t)","['multivariable-calculus', 'differential-geometry', 'curves', 'applications']"
8,Integrability of a vector field and its topology,Integrability of a vector field and its topology,,"Today, in the lecture, we covered an example of a vector field which suffices the necessary condition for integrability, yet is not integrable. The following field also known as the angular form is an example of such a case. $$\omega = -\frac{y}{r^2}dx + \frac{x}{r^2}dy$$ Where $r = \sqrt{x^2+y^2}$ and the domain is $\Omega = \mathbb{R}^2/$ {(0,0)} Clearly, we can check that the mixed partials are equal and that the necessary condition holds. But if we compute the circulation about the origin say a unit circle, we notice that it is not zero. I am aware that the necessary condition is a sufficient condition once we have a simply connected domain $\Omega$ . By slicing the plane of the above example we obtain a simply connected domain without the origin and have an integrable vector field. My question is the following: When we have holes in the domain of the vector field, do we have to check the circulation around each hole to make sure it is zero and then we can find its potential and state that it is integrable? If it is non-zero do we slice the domain and state that it is integrable on the restricted domain? I hope my question makes sense, thanks for any help, ideas or clarification! Edit: Furthermore after the slicing (removing any smooth curve that connects 0 to $\infty$ ) I claim that $\phi(x,y) = \arctan(\frac{y}{x})$ But it is discontinuous along the y-axis, so then it cannot be the potential function because it is not differentiable there. Am I making a mistake in this logic? Thanks!","Today, in the lecture, we covered an example of a vector field which suffices the necessary condition for integrability, yet is not integrable. The following field also known as the angular form is an example of such a case. Where and the domain is {(0,0)} Clearly, we can check that the mixed partials are equal and that the necessary condition holds. But if we compute the circulation about the origin say a unit circle, we notice that it is not zero. I am aware that the necessary condition is a sufficient condition once we have a simply connected domain . By slicing the plane of the above example we obtain a simply connected domain without the origin and have an integrable vector field. My question is the following: When we have holes in the domain of the vector field, do we have to check the circulation around each hole to make sure it is zero and then we can find its potential and state that it is integrable? If it is non-zero do we slice the domain and state that it is integrable on the restricted domain? I hope my question makes sense, thanks for any help, ideas or clarification! Edit: Furthermore after the slicing (removing any smooth curve that connects 0 to ) I claim that But it is discontinuous along the y-axis, so then it cannot be the potential function because it is not differentiable there. Am I making a mistake in this logic? Thanks!","\omega = -\frac{y}{r^2}dx + \frac{x}{r^2}dy r = \sqrt{x^2+y^2} \Omega = \mathbb{R}^2/ \Omega \infty \phi(x,y) = \arctan(\frac{y}{x})","['calculus', 'integration', 'multivariable-calculus', 'vector-analysis', 'vector-fields']"
9,Line integral where $C$ is the boundary of the square -Green's Theorem,Line integral where  is the boundary of the square -Green's Theorem,C,"I am trying to solve the following problem but I face difficulties with the results. Any suggestion? The line integral $\int_C y^2 dx - x dy$ , where $C$ is the boundary of the square $[-1,1]\times[-1,1]$ oriented counterclockwise, can be evaluated in two ways: a) Using the definition of the line integral: To compute the line integral directly from its definition, we break the path $C$ into four segments corresponding to the sides of the square. For each segment, we parameterize the path, compute $dx$ and $dy$ , substitute into the integral, and evaluate. b) Using Green's theorem: Green's theorem relates a line integral around a simple closed curve $C$ to a double integral over the plane region $D$ bounded by $C$ . It states that $\int_C P dx + Q dy = \int\int_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA$ where $P(x,y) = -x$ and $Q(x,y) = y^2$ . We can compute the partial derivatives, substitute them into the double integral, and evaluate over the region $D$ , which is the square \$[-1,1]\times[-1,1]$. SOLUTION a) To solve the line integral $\int_C y^2 dx - x dy$ using the definition, we need to consider the path $C$ which is the boundary of the square with vertices at $(-1,-1)$ , $(-1,1)$ , $(1,1)$ , and $(1,-1)$ , oriented counterclockwise. We'll break $C$ into four segments corresponding to the sides of the square: $C_1$ from $(-1,-1)$ to $(-1,1)$ where $x = -1$ and y varies from $-1$ to $1$ . $C_2$ from $(-1,1)$ to $(1,1)$ where $y = 1$ and $x$ varies from $-1$ to $1$ . $C_3$ from $(1,1)$ to $(1,-1)$ where $x = 1$ and $y$ varies from $1$ to $-1$ . $C_4$ from $(1,-1)$ to $(-1,-1)$ where $y =-1$ and $x$ varies from $1$ to $-1$ . Now we'll compute the integral over each segment and sum them: On $C_1$ , dx = 0 $ (since $ x$ is constant), so the integral becomes: $$ \int_{-1}^{1} -x dy = \int_{-1}^{1} -(-1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1} = 2 $$ On $C_2$ , $dy = 0$ (since $y$ is constant), so the integral becomes: $$    \int_{-1}^{1} y^2 dx = \int_{-1}^{1} 1^2 dx = \int_{-1}^{1} dx = x\Big|_{-1}^{1} = 2    $$ On $C_3$ , similar to $C_1$ , $dx = 0$ , so the integral becomes: $$    \int_{1}^{-1} -x dy = \int_{1}^{-1} -(1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1}  = 2    $$ On $C_4$ ,  so the integral becomes: $$    \int_{1}^{-1} y^2 dx = \int_{1}^{-1} (-1)^2 dx = -\int_{-1}^{1} dx = -x\Big|_{-1}^{1}  = -2    $$ Therefore, the total integral around $C$ is $2 + 2 + 2 -2 = 4$ . b)To correctly apply Green's Theorem for the line integral around the boundary $C $ of the square $[-1,1]\times[-1,1]$ , we need to consider the functions $ P $ and $ Q $ as given in the integral: [ \int_C y^2 dx - x dy ] Here, $ P(x, y) = y^2 $ and $ Q(x, y) = -x $ . According to Green's Theorem, the line integral over the closed curve $ C $ can be transformed into a double integral over the region $ R $ enclosed by $ C $ : $$ \oint_C P\,dx + Q\,dy = \iint_R \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA $$ First, we calculate the partial derivatives of $ P$ and $ Q $ : $$\frac{\partial Q}{\partial x} = \frac{\partial (-x)}{\partial x} = -1$$ $$(\frac{\partial P}{\partial y} = \frac{\partial (y^2)}{\partial y} = 2y$$ So, the double integral becomes: $$ \iint_R (-1 - 2y) dA $$ The region $ R $ is the square $[-1,1] \times [-1,1]$ , thus the integral is over $ x $ from $-1$ to $1$ and $ y $ from $-1$ to $1$ . Calculating the double integral: $$ \int_{-1}^{1} \int_{-1}^{1} (-1 - 2y) dx dy=-4$$","I am trying to solve the following problem but I face difficulties with the results. Any suggestion? The line integral , where is the boundary of the square oriented counterclockwise, can be evaluated in two ways: a) Using the definition of the line integral: To compute the line integral directly from its definition, we break the path into four segments corresponding to the sides of the square. For each segment, we parameterize the path, compute and , substitute into the integral, and evaluate. b) Using Green's theorem: Green's theorem relates a line integral around a simple closed curve to a double integral over the plane region bounded by . It states that where and . We can compute the partial derivatives, substitute them into the double integral, and evaluate over the region , which is the square \$[-1,1]\times[-1,1]$. SOLUTION a) To solve the line integral using the definition, we need to consider the path which is the boundary of the square with vertices at , , , and , oriented counterclockwise. We'll break into four segments corresponding to the sides of the square: from to where and y varies from to . from to where and varies from to . from to where and varies from to . from to where and varies from to . Now we'll compute the integral over each segment and sum them: On , dx = 0 x$ is constant), so the integral becomes: $$ \int_{-1}^{1} -x dy = \int_{-1}^{1} -(-1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1} = 2 $$ On , (since is constant), so the integral becomes: On , similar to , , so the integral becomes: On ,  so the integral becomes: Therefore, the total integral around is . b)To correctly apply Green's Theorem for the line integral around the boundary of the square , we need to consider the functions and as given in the integral: [ \int_C y^2 dx - x dy ] Here, and . According to Green's Theorem, the line integral over the closed curve can be transformed into a double integral over the region enclosed by : First, we calculate the partial derivatives of and : So, the double integral becomes: The region is the square , thus the integral is over from to and from to . Calculating the double integral:","\int_C y^2 dx - x dy C [-1,1]\times[-1,1] C dx dy C D C \int_C P dx + Q dy = \int\int_D \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA P(x,y) = -x Q(x,y) = y^2 D \int_C y^2 dx - x dy C (-1,-1) (-1,1) (1,1) (1,-1) C C_1 (-1,-1) (-1,1) x = -1 -1 1 C_2 (-1,1) (1,1) y = 1 x -1 1 C_3 (1,1) (1,-1) x = 1 y 1 -1 C_4 (1,-1) (-1,-1) y =-1 x 1 -1 C_1  (since  C_2 dy = 0 y 
   \int_{-1}^{1} y^2 dx = \int_{-1}^{1} 1^2 dx = \int_{-1}^{1} dx = x\Big|_{-1}^{1} = 2
    C_3 C_1 dx = 0 
   \int_{1}^{-1} -x dy = \int_{1}^{-1} -(1) dy = \int_{-1}^{1} dy = y\Big|_{-1}^{1}  = 2
    C_4 
   \int_{1}^{-1} y^2 dx = \int_{1}^{-1} (-1)^2 dx = -\int_{-1}^{1} dx = -x\Big|_{-1}^{1}  = -2
    C 2 + 2 + 2 -2 = 4 C  [-1,1]\times[-1,1]  P   Q   P(x, y) = y^2   Q(x, y) = -x   C   R   C  
\oint_C P\,dx + Q\,dy = \iint_R \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right) dA
  P  Q  \frac{\partial Q}{\partial x} = \frac{\partial (-x)}{\partial x} = -1 (\frac{\partial P}{\partial y} = \frac{\partial (y^2)}{\partial y} = 2y 
\iint_R (-1 - 2y) dA
  R  [-1,1] \times [-1,1]  x  -1 1  y  -1 1 
\int_{-1}^{1} \int_{-1}^{1} (-1 - 2y) dx dy=-4","['integration', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
10,Neighboring property irrespective of continuity.,Neighboring property irrespective of continuity.,,"Given, $f(x,y),g(x,y)$ functions with $f(x,y)\gt g(x,y)\gt 1$ , where $(x, y)\in [0,1]\times[0,1]$ . Now, irrespective of continuity of $f(x,y),g(x,y)$ , can we say that there is an open ball $B$ about $(1,1)$ such that $\left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0$ for $(x,y)\in B\cap ([0,1]\times[0,1])$ ? At $(1,1)$ , clearly, $\left(g^{-1}-f^{-1}\right)-(f-g)\lt0$ . Then, without continuity, can I say $\left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0$ is true for $(x,y)\in B\cap ([0,1]\times[0,1])$ ?","Given, functions with , where . Now, irrespective of continuity of , can we say that there is an open ball about such that for ? At , clearly, . Then, without continuity, can I say is true for ?","f(x,y),g(x,y) f(x,y)\gt g(x,y)\gt 1 (x, y)\in [0,1]\times[0,1] f(x,y),g(x,y) B (1,1) \left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0 (x,y)\in B\cap ([0,1]\times[0,1]) (1,1) \left(g^{-1}-f^{-1}\right)-(f-g)\lt0 \left(g^{-\frac{x+y}{x+xy}}-f^{-\frac{x+y}{y+xy}}\right)-(f-g)\lt0 (x,y)\in B\cap ([0,1]\times[0,1])","['real-analysis', 'multivariable-calculus']"
11,"why is $f(x,y)=\begin{cases}\frac{4(x^2-y)(2y-x^2)}{y^2} & y>0\\ 0 & y\leq 0.\end{cases}$ continuous on $\mathbb{R}^2\setminus\{(0,0)\}$?",why is  continuous on ?,"f(x,y)=\begin{cases}\frac{4(x^2-y)(2y-x^2)}{y^2} & y>0\\ 0 & y\leq 0.\end{cases} \mathbb{R}^2\setminus\{(0,0)\}","I am reading a real analysis book which states that the function $f:\mathbb{R}^2\to\mathbb{R}$ defined by $$f(x,y)=\begin{cases}     \frac{4(x^2-y)(2y-x^2)}{y^2} & \text{if } y>0\\     0 & \text{if }y\leq 0. \end{cases}$$ is continuous on $\mathbb{R}^2\setminus\{(0,0)\}$ , is not continuous at $(0,0)$ and if we fix any $y\in\mathbb{R}$ then $f(\cdot,y)$ is continuous on $\mathbb{R}$ and if we fix any $x\in\mathbb{R}$ then $f(x,\cdot)$ is continuous on $\mathbb{R}.$ Now, it is easy to see that $f$ is not continuous at $(0,0)$ since if we set $x=0$ then $f(0,y)=\begin{cases}-8 &\text{ if }y>0\\ 0&\text{ if }y\leq 0\end{cases}$ and it is also easy to see that for any fixed $y\in\mathbb{R}$ the function $f(\cdot,y)$ is continuous because this last function is a polynomial in the $x$ variable. What I don't understand is why this function (and also $f(x,\cdot)$ ) is continuous on $\mathbb{R}\setminus\{(0,0)\}$ ; it seems to me that it should be continuous on $\mathbb{R}^2\setminus\{(x,0):x\in\mathbb{R}\}=\mathbb{R}^2\setminus\{x-\text{axis}\}$ infact if we consider $f(1,y)=\begin{cases}\frac{4(1-y)(2y-1)}{y^2}=\frac{-8y^2+12y-4}{y^2} & \text{if }y>0\\ 0&\text{if }y\leq 0\end{cases}$ it evaluates to $0$ for $y\leq 0$ but it goes to $-\infty$ for $y\to 0^+$ . Could someone please explain this to me? Why is $f$ continuous on $\mathbb{R}^2\setminus\{(0,0)\}$ and why is $f(x,\cdot)$ continuous on $\mathbb{R}$ for any fixed $x$ ? Thanks.","I am reading a real analysis book which states that the function defined by is continuous on , is not continuous at and if we fix any then is continuous on and if we fix any then is continuous on Now, it is easy to see that is not continuous at since if we set then and it is also easy to see that for any fixed the function is continuous because this last function is a polynomial in the variable. What I don't understand is why this function (and also ) is continuous on ; it seems to me that it should be continuous on infact if we consider it evaluates to for but it goes to for . Could someone please explain this to me? Why is continuous on and why is continuous on for any fixed ? Thanks.","f:\mathbb{R}^2\to\mathbb{R} f(x,y)=\begin{cases}
    \frac{4(x^2-y)(2y-x^2)}{y^2} & \text{if } y>0\\
    0 & \text{if }y\leq 0.
\end{cases} \mathbb{R}^2\setminus\{(0,0)\} (0,0) y\in\mathbb{R} f(\cdot,y) \mathbb{R} x\in\mathbb{R} f(x,\cdot) \mathbb{R}. f (0,0) x=0 f(0,y)=\begin{cases}-8 &\text{ if }y>0\\ 0&\text{ if }y\leq 0\end{cases} y\in\mathbb{R} f(\cdot,y) x f(x,\cdot) \mathbb{R}\setminus\{(0,0)\} \mathbb{R}^2\setminus\{(x,0):x\in\mathbb{R}\}=\mathbb{R}^2\setminus\{x-\text{axis}\} f(1,y)=\begin{cases}\frac{4(1-y)(2y-1)}{y^2}=\frac{-8y^2+12y-4}{y^2} & \text{if }y>0\\ 0&\text{if }y\leq 0\end{cases} 0 y\leq 0 -\infty y\to 0^+ f \mathbb{R}^2\setminus\{(0,0)\} f(x,\cdot) \mathbb{R} x","['real-analysis', 'multivariable-calculus', 'continuity']"
12,Equation 8.39 to 8.41 in the book Neuronal Dynamics,Equation 8.39 to 8.41 in the book Neuronal Dynamics,,"In the book neuronal dynamics chapter 8.4 ( https://neuronaldynamics.epfl.ch/online/Ch8.S4.html ), the authors derive the Fokker-Planck equation for a noisy neuron. Starting from equation 8.39: $ p(u, t+\Delta t) = [1- \Delta t \sum_{k} v_k(t)] e^{\Delta t / \tau} p(e^{\Delta t / \tau} u, t) + \Delta t \sum_k v_k(t) [p(u-w_k, t) - p(u,t)] $ First, they do a Taylor approximation of $ \Delta t  $ around 0. Doing it myself, $ p(e^{\Delta t / \tau} u, t) \approx p(u, t) + u \Delta t \frac{\partial}{\partial u} p(u, t) $ $ e^{\Delta t / \tau} \approx 1 + \frac{\Delta t}{\tau} $ Ignoring all the terms with $ \Delta t^2 $ , I reach the following result $ \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t)) $ However, the book claims that there is a factor of $ \frac{1}{\tau} $ multiplying $u \frac{\partial}{\partial u} p(u,t)$ . In equation 8.40, $ \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + \frac{1}{\tau} u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t)) $ More interestingly, this term somehow disappears when going from 8.40 to 8.41. The goal is to approximate the equation once more with Taylor expansion with $w_k$ going to 0, keeping the terms up to $w_k^2$ . When I assume 8.40 to be true and use the expansion: $p(u-w, t) = p(u, t) - w\frac{\partial}{\partial u} p(u,t) + \frac{1}{2} w^2 \frac{\partial^2}{\partial u^2} p(u,t)$ I reach the following solution: $\tau \frac{\partial}{\partial t} p(u, t) \approx u \frac{\partial}{\partial u} p(u, t) - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t) $ The solution in the book (equation 8.41) omits the term $u \frac{\partial}{\partial u} p(u, t)$ : $\tau \frac{\partial}{\partial t} p(u, t) \approx - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t) $ I would appreciate any help showing where I'm being mistaken.","In the book neuronal dynamics chapter 8.4 ( https://neuronaldynamics.epfl.ch/online/Ch8.S4.html ), the authors derive the Fokker-Planck equation for a noisy neuron. Starting from equation 8.39: First, they do a Taylor approximation of around 0. Doing it myself, Ignoring all the terms with , I reach the following result However, the book claims that there is a factor of multiplying . In equation 8.40, More interestingly, this term somehow disappears when going from 8.40 to 8.41. The goal is to approximate the equation once more with Taylor expansion with going to 0, keeping the terms up to . When I assume 8.40 to be true and use the expansion: I reach the following solution: The solution in the book (equation 8.41) omits the term : I would appreciate any help showing where I'm being mistaken."," p(u, t+\Delta t) = [1- \Delta t \sum_{k} v_k(t)] e^{\Delta t / \tau} p(e^{\Delta t / \tau} u, t) + \Delta t \sum_k v_k(t) [p(u-w_k, t) - p(u,t)]   \Delta t    p(e^{\Delta t / \tau} u, t) \approx p(u, t) + u \Delta t \frac{\partial}{\partial u} p(u, t)   e^{\Delta t / \tau} \approx 1 + \frac{\Delta t}{\tau}   \Delta t^2   \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t))   \frac{1}{\tau}  u \frac{\partial}{\partial u} p(u,t)  \frac{\partial}{\partial t} p(u, t) = \frac{1}{\tau} p(u, t) + \frac{1}{\tau} u \frac{\partial}{\partial u} p(u,t) - \sum_k v_k ( p(u-w_k, t) - p(u, t))  w_k w_k^2 p(u-w, t) = p(u, t) - w\frac{\partial}{\partial u} p(u,t) + \frac{1}{2} w^2 \frac{\partial^2}{\partial u^2} p(u,t) \tau \frac{\partial}{\partial t} p(u, t) \approx u \frac{\partial}{\partial u} p(u, t) - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t)  u \frac{\partial}{\partial u} p(u, t) \tau \frac{\partial}{\partial t} p(u, t) \approx - \frac{\partial}{\partial u} [-u + \tau \sum_k v_k(t) w_k] p(u, t) + \frac{1}{2}[\tau \sum_k v_k(t) w_k^2] \frac{\partial^2}{\partial u^2} p(u, t) ",['multivariable-calculus']
13,Evaluate $\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx$. Given $\int_{0}^{c}e^{s^2}ds = 3$,Evaluate . Given,\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx \int_{0}^{c}e^{s^2}ds = 3,"Evaluate $$\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx$$ Given $$\int_{0}^{c}e^{s^2}ds = 3$$ where c is a positive constant. The solution to this question mentioned that: $$\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx = \frac{1}{2}  \int_{0}^{c} \int_{0}^{c} e^{x^2+y^2}dydx$$ Because the integral in the RHS is evaluated on a triangular region bounded by: $$ y = x \,\,\,\&\,\,\, y = c$$ whereas the one on the LHS is a square region whose area is double the triangular region. Though I can observe that the region on XY - plane is indeed double, I'm not sure if the relationship would hold while taking a double integral over a non-constant function.","Evaluate Given where c is a positive constant. The solution to this question mentioned that: Because the integral in the RHS is evaluated on a triangular region bounded by: whereas the one on the LHS is a square region whose area is double the triangular region. Though I can observe that the region on XY - plane is indeed double, I'm not sure if the relationship would hold while taking a double integral over a non-constant function.","\int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx \int_{0}^{c}e^{s^2}ds = 3 \int_{0}^{c} \int_{x}^{c} e^{x^2+y^2}dydx = \frac{1}{2}  \int_{0}^{c} \int_{0}^{c} e^{x^2+y^2}dydx  y = x \,\,\,\&\,\,\, y = c","['integration', 'multivariable-calculus', 'definite-integrals', 'iterated-integrals']"
14,Integration by parts on surfaces in weighted $L^2$ spaces,Integration by parts on surfaces in weighted  spaces,L^2,"I am on a closed and bounded surface $S$ without boundary. Then it holds that $$ \int_S \nabla f \cdot \nabla f  \mathrm dA = -\int_S f\Delta f  \mathrm dA. $$ This is the $H^1(S)$ seminorm, meaning that I can express the $H^1(S)$ norm as $$ \|f\|_1 =  \int_S f(1-\Delta) f  \mathrm dA. $$ Imagine now I am on a weighted $L^2$ space with weight function $w$ . What can be said for $ \|f\|_1^w $ ? The problem becomes to integrate $$ \int_S w\nabla f \cdot \nabla f  \mathrm dA.  $$ I tried to play around with various integration by parts, but I end up getting gradients of the weights and so on. I see no reason why it shouldn't hold that $$ \|f\|_1^w = \int_S wf(1-\Delta)f \mathrm dA.  $$ Here $\mathrm d A$ is just the Lebesgue measure, so if one says that we are instead integrating against $\mathrm dW = w\mathrm d A$ , then it just seems like it would hold, but I don't know. Is it at all possible to rewrite in terms of only Laplacians? $$ \int_S w \nabla f \cdot \nabla f \mathrm d A?  $$","I am on a closed and bounded surface without boundary. Then it holds that This is the seminorm, meaning that I can express the norm as Imagine now I am on a weighted space with weight function . What can be said for ? The problem becomes to integrate I tried to play around with various integration by parts, but I end up getting gradients of the weights and so on. I see no reason why it shouldn't hold that Here is just the Lebesgue measure, so if one says that we are instead integrating against , then it just seems like it would hold, but I don't know. Is it at all possible to rewrite in terms of only Laplacians?","S 
\int_S \nabla f \cdot \nabla f  \mathrm dA = -\int_S f\Delta f  \mathrm dA.
 H^1(S) H^1(S) 
\|f\|_1 =  \int_S f(1-\Delta) f  \mathrm dA.
 L^2 w 
\|f\|_1^w
 
\int_S w\nabla f \cdot \nabla f  \mathrm dA. 
 
\|f\|_1^w = \int_S wf(1-\Delta)f \mathrm dA. 
 \mathrm d A \mathrm dW = w\mathrm d A 
\int_S w \nabla f \cdot \nabla f \mathrm d A? 
","['multivariable-calculus', 'sobolev-spaces']"
15,Multidimensional Legendre polynomials?,Multidimensional Legendre polynomials?,,"Legendre polynomials can be given by several expressions, but perhaps the most compact way to represent them is by Rodrigues' formula as $$P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} (x^2 - 1)^n.$$ I was wondering if there exists a multidimensional variant of the Legendre polynomials, e.g. an analogous $P_n(x,y,z)$ for instance. Does anyone know anything about this? I can't seem to find anything about it.","Legendre polynomials can be given by several expressions, but perhaps the most compact way to represent them is by Rodrigues' formula as I was wondering if there exists a multidimensional variant of the Legendre polynomials, e.g. an analogous for instance. Does anyone know anything about this? I can't seem to find anything about it.","P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} (x^2 - 1)^n. P_n(x,y,z)","['calculus', 'multivariable-calculus', 'polynomials', 'legendre-polynomials']"
16,an expansion in terms of norm and inner product in $\mathbb{R}^n$,an expansion in terms of norm and inner product in,\mathbb{R}^n,"I want to prove the following inequality in $\mathbb{R}^n$ For any $s\geq2$ there exists some constant $C(s)>0$ such that $$|y|^s>|x|^s+s|x|^{s-2}\langle x,y-x\rangle+\frac{C(s)}{2^s-1}|y-x|^s$$ for all $x,y\in\mathbb{R}^n$ . Here $|\cdot|$ and $\langle\cdot,\cdot\rangle$ denote the usual norm and inner product in $\mathbb{R}^n$ respectively. I was initially thinking to use convexity of the map $x\mapsto|x|^s$ for $x\in\mathbb{R}^n$ . Then $$|y|^s\leq2^{s-1}(|x|^s+|y-x|^s)$$ But the inequality is in reverse order of what is required, also I have no idea to bring the inner product into picture. I have a intuition of the Taylor's expansion, but I am not very sure. Any help is appreciated.","I want to prove the following inequality in For any there exists some constant such that for all . Here and denote the usual norm and inner product in respectively. I was initially thinking to use convexity of the map for . Then But the inequality is in reverse order of what is required, also I have no idea to bring the inner product into picture. I have a intuition of the Taylor's expansion, but I am not very sure. Any help is appreciated.","\mathbb{R}^n s\geq2 C(s)>0 |y|^s>|x|^s+s|x|^{s-2}\langle x,y-x\rangle+\frac{C(s)}{2^s-1}|y-x|^s x,y\in\mathbb{R}^n |\cdot| \langle\cdot,\cdot\rangle \mathbb{R}^n x\mapsto|x|^s x\in\mathbb{R}^n |y|^s\leq2^{s-1}(|x|^s+|y-x|^s)","['real-analysis', 'linear-algebra', 'multivariable-calculus']"
17,Help showing this integral is positive.,Help showing this integral is positive.,,"I would some help on showing that the following integral is positive: Fix $1\leq p < \infty$ and let $(a,b) \in (0.5,1)^2$ $$ \int_0^1 \int_0^1 a(p+2)|(a,b)|^p \Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p} - p|(a,b)|^{p+2}(a-x)\Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p-2}dydx $$ Context: I am trying to show that the quotient $$\frac{|(a,b)|^{p+2}}{\int_0^1\int_0^1 |(x,y) - (a,b)|^pdydx}$$ achieves its maximum at $(a,b) = (1,1)$ . So, I took its derivative with respect to $a$ , which resulted in the integral above (numerator only), and now I'm trying to show it is positive, i.e. the quotient is increasing as $a$ goes to 1. Something analogous for $b$ will allow me to conclude. I know that the maximum happens at $(1,1)$ for $(a,b) \in [0.5,1]^2$ by using a graphing calculator. See Here","I would some help on showing that the following integral is positive: Fix and let Context: I am trying to show that the quotient achieves its maximum at . So, I took its derivative with respect to , which resulted in the integral above (numerator only), and now I'm trying to show it is positive, i.e. the quotient is increasing as goes to 1. Something analogous for will allow me to conclude. I know that the maximum happens at for by using a graphing calculator. See Here","1\leq p < \infty (a,b) \in (0.5,1)^2 
\int_0^1 \int_0^1 a(p+2)|(a,b)|^p \Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p} - p|(a,b)|^{p+2}(a-x)\Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p-2}dydx
 \frac{|(a,b)|^{p+2}}{\int_0^1\int_0^1 |(x,y) - (a,b)|^pdydx} (a,b) = (1,1) a a b (1,1) (a,b) \in [0.5,1]^2","['multivariable-calculus', 'maxima-minima']"
18,"how to evaluate $\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$",how to evaluate,"\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx","how to evaluate $$\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$$ My attempt $$ \Omega =\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx$$ $$ * \Omega = \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,dy $$ $$ = \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{x} \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,d(1 - xy)\right] $$ $$ = \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{2x} \ln^2(1-xy)\bigg|_{0}^{1}\right] $$ $$= - \frac{1}{2} \int_{0}^{1} \frac{\ln^2(1 - x) \cdot \text{Li}_{4}(1 - x)}{x^2(1 - x)} \,dx$$",how to evaluate My attempt,"\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx  \Omega =\int_{0}^{1} \int_{0}^{1} \frac{\ln(1 - xy) \cdot \text{Li}_{4}(1 - x)}{x(1 - x)(1 - xy)} \,dy\,dx 
* \Omega = \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,dy
 
= \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{x} \int_{0}^{1} \frac{\ln(1 - xy)}{1 - xy} \,d(1 - xy)\right]
 
= \int_{0}^{1} \frac{\text{Li}_{4}(1 - x)}{x(1 - x)} \,dx \cdot \left[-\frac{1}{2x} \ln^2(1-xy)\bigg|_{0}^{1}\right]
 = - \frac{1}{2} \int_{0}^{1} \frac{\ln^2(1 - x) \cdot \text{Li}_{4}(1 - x)}{x^2(1 - x)} \,dx","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'closed-form']"
19,using $\epsilon-\delta$ to prove the continuity of a multivariable function,using  to prove the continuity of a multivariable function,\epsilon-\delta,"How can I prove $f(x,y)=x^y$ is continuous on $U=\{(x,y)\in\mathbb{R}^2|x>0\}$ by using $\epsilon-\delta$ ? For $(x_0,y_0)\in U$ , I tried to do something with $y\ln x$ : $$y\ln x-y_0\ln x_0=(y-y_0)\ln x+y_0\ln\frac{x}{x_0}.$$ I want $x-x_0$ to occur in the equality, so that I can choose $\delta$ from $\sqrt{(x-x_0)^2+(y-y_0)^2}<\delta$ . But I don't know how to do it :( . Could someone help me?","How can I prove is continuous on by using ? For , I tried to do something with : I want to occur in the equality, so that I can choose from . But I don't know how to do it :( . Could someone help me?","f(x,y)=x^y U=\{(x,y)\in\mathbb{R}^2|x>0\} \epsilon-\delta (x_0,y_0)\in U y\ln x y\ln x-y_0\ln x_0=(y-y_0)\ln x+y_0\ln\frac{x}{x_0}. x-x_0 \delta \sqrt{(x-x_0)^2+(y-y_0)^2}<\delta","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
20,Directional derivative confusion about terminology,Directional derivative confusion about terminology,,"I am confused about a definition of directional derivatives I found in the MIT Deep Learning book. The directional derivative in direction $u$ (a unit vector) is the slope of the function $f$ in direction $u$ . In other words, the directional derivative is the derivative of the function $f (x + \alpha u)$ with respect to $\alpha$ , evaluated at $\alpha = 0$ . Using the chain rule, we can see that $ \frac \partial {\partial \alpha} f(x + \alpha u) $ evaluates to $u^T \nabla_{x} f(x)$ when $ \alpha = 0$ I understand the first sentence but I don't understand what they are trying to say after that. What I know: If $f: \mathbb{R}^n \to \mathbb{R}$ , the directional derivative of $f$ at $x_0$ in the direction of the vector $v$ is defined as the limit $$D_v f(x_0) = \lim_{h\to 0}\frac{f(x_0+hv)-f(x_0)}{h}.$$ Some people use $ \frac {\partial f} {\partial x}$ to talk about $f'(x)$ What I am confused about: derivative of the function $f (x + \alpha u)$ with respect to $\alpha$ For me this sentence means that we have another function $g(\alpha) = f (x + \alpha u)$ and then compute the derivative of $g$ with respect to $\alpha$ $$ g'(\alpha) = \lim_{h\to 0} \frac {g(\alpha + h) - g(\alpha)} h = \lim_{h\to 0} \frac {f (x + (\alpha + h) u) - f (x + \alpha u)} h $$ But I don't see how this is equivalent to the definition with $D_v f(x_0)$ . Where does $u^T$ come from and what does this $\nabla_{x} f(x)$ even mean? Is it just the partial derivative with respect to $x$ ? I only the chain rule written in terms of functions $(f(g(x)))' = f'(g(x)) g'(x)$ I don't understand what chain rule they are applying there. Please be patient with me, I am just trying to understand how to reason about directional derivatives.","I am confused about a definition of directional derivatives I found in the MIT Deep Learning book. The directional derivative in direction (a unit vector) is the slope of the function in direction . In other words, the directional derivative is the derivative of the function with respect to , evaluated at . Using the chain rule, we can see that evaluates to when I understand the first sentence but I don't understand what they are trying to say after that. What I know: If , the directional derivative of at in the direction of the vector is defined as the limit Some people use to talk about What I am confused about: derivative of the function with respect to For me this sentence means that we have another function and then compute the derivative of with respect to But I don't see how this is equivalent to the definition with . Where does come from and what does this even mean? Is it just the partial derivative with respect to ? I only the chain rule written in terms of functions I don't understand what chain rule they are applying there. Please be patient with me, I am just trying to understand how to reason about directional derivatives.",u f u f (x + \alpha u) \alpha \alpha = 0  \frac \partial {\partial \alpha} f(x + \alpha u)  u^T \nabla_{x} f(x)  \alpha = 0 f: \mathbb{R}^n \to \mathbb{R} f x_0 v D_v f(x_0) = \lim_{h\to 0}\frac{f(x_0+hv)-f(x_0)}{h}.  \frac {\partial f} {\partial x} f'(x) f (x + \alpha u) \alpha g(\alpha) = f (x + \alpha u) g \alpha  g'(\alpha) = \lim_{h\to 0} \frac {g(\alpha + h) - g(\alpha)} h = \lim_{h\to 0} \frac {f (x + (\alpha + h) u) - f (x + \alpha u)} h  D_v f(x_0) u^T \nabla_{x} f(x) x (f(g(x)))' = f'(g(x)) g'(x),"['multivariable-calculus', 'partial-derivative', 'vector-analysis']"
21,When and how do we dehomogenize a homogeneous function?,When and how do we dehomogenize a homogeneous function?,,"When and how do we dehomogenize a homogeneous function? To solve Prove the sign and zeroes of $Ax^2 + 2Bxy + Cy^2$ (without using the second derivative test) , ""user"" set $$t = \frac x y$$ and wrote $$Ax^2 + 2Bxy + Cy^2 =y^2\left(At^2+2Bt+C\right)$$ to solve the problem. Where did that come from? Ted Shifrin wrote When you study a homogeneous function, it is natural to dehomogenize by evaluating $f(x,y)$ at either $(1,y/x)$ or $(x/y,1)$ I'd like to learn more about this technique, and am unable to find a reference here or via Google.  What does it mean to dehomogenize a function? When and how do we do it? Update Although I'm still looking for a reference, I worked out a ""derivation"" in this case, to which I ask for feedback and verification: Goal: Characterize the sign of $$P(x,y) = Ax^2 + 2Bxy + Cy^2$$ on $\mathbb R^2 - \{(0,0)\}$ . Approach: Before attempting any algebra, understand $P$ qualitatively .  Two things are clear: (i) When $|x|$ is large, $A$ dominates, and when $|y|$ is large, $C$ dominates; it's only when $|x| \approx |y|$ that things are messy (ii) The absolute magnitude of $x$ and $y$ are irrelevant; it's only their ratio that matters. This suggests that we divide by $x^2$ , which will preserve the sign (which is what we care about) and perhaps replace absolute magnitude with ratio.  (We can handle $x = 0$ later.) $$P(x,y) =  {x^2} (A + B \frac y x + C \frac {y^2} {x^2}) \\ \operatorname{sgn}[P(x,y)] =  \operatorname{sgn}(A + B \frac y x + C \frac {y^2} {x^2})$$ which is a polynomial in $\frac y x$ , whose range of signs is obvious from its discriminant. Furthermore, we can assume WLOG that $x \neq 0$ , since if $x = 0$ , then $y \neq 0$ , and $\operatorname{sgn} P$ is the same by symmetry.","When and how do we dehomogenize a homogeneous function? To solve Prove the sign and zeroes of $Ax^2 + 2Bxy + Cy^2$ (without using the second derivative test) , ""user"" set and wrote to solve the problem. Where did that come from? Ted Shifrin wrote When you study a homogeneous function, it is natural to dehomogenize by evaluating at either or I'd like to learn more about this technique, and am unable to find a reference here or via Google.  What does it mean to dehomogenize a function? When and how do we do it? Update Although I'm still looking for a reference, I worked out a ""derivation"" in this case, to which I ask for feedback and verification: Goal: Characterize the sign of on . Approach: Before attempting any algebra, understand qualitatively .  Two things are clear: (i) When is large, dominates, and when is large, dominates; it's only when that things are messy (ii) The absolute magnitude of and are irrelevant; it's only their ratio that matters. This suggests that we divide by , which will preserve the sign (which is what we care about) and perhaps replace absolute magnitude with ratio.  (We can handle later.) which is a polynomial in , whose range of signs is obvious from its discriminant. Furthermore, we can assume WLOG that , since if , then , and is the same by symmetry.","t = \frac x y Ax^2 + 2Bxy + Cy^2 =y^2\left(At^2+2Bt+C\right) f(x,y) (1,y/x) (x/y,1) P(x,y) = Ax^2 + 2Bxy + Cy^2 \mathbb R^2 - \{(0,0)\} P |x| A |y| C |x| \approx |y| x y x^2 x = 0 P(x,y) =  {x^2} (A + B \frac y x + C \frac {y^2} {x^2}) \\
\operatorname{sgn}[P(x,y)] =  \operatorname{sgn}(A + B \frac y x + C \frac {y^2} {x^2}) \frac y x x \neq 0 x = 0 y \neq 0 \operatorname{sgn} P","['real-analysis', 'calculus', 'multivariable-calculus', 'homogeneous-equation']"
22,"Prove that if $F: \mathbb R^n \to \mathbb R^n$, wth $n \geq 2$, is a smooth proper map with finitely many critical points, then $F$ is surjective","Prove that if , wth , is a smooth proper map with finitely many critical points, then  is surjective",F: \mathbb R^n \to \mathbb R^n n \geq 2 F,"Prove that if $F: \mathbb R^n \to \mathbb R^n$ , $n \geq 2$ , is a smooth proper map with finitely many critical points, then $F$ is surjective. Here is my work so far: the set $R$ of all regular points of $F$ is open and path-connected, because $n \geq 2$ , and hence connected. For any $u \in R$ we have that $F_{*, p}$ for any $p \in F^{-1}(u)$ is surjective linear map from $\mathbb R^n$ to $\mathbb R^n$ so that it's invertible; thus, by the inverse function theorem, $F$ is locally invertible from a neighborhood of $p$ to a neighborhood $U$ of $u$ . By the inverse function theorem again, all points of $U$ are regular values. I am not sure how to proceed from here. I know that $R$ is path connected. If $v \in R$ is another regular point, then there exists is a path entirely contained within $R$ going from $u$ to $v$ . IF there exists a sequence of points that are in the image of $F$ that connects $u$ and $v$ in a dense way (I'm being very loose in language here, I hope you will get the picture), then we'll be done by using the compactness of the path and the inverse function theorem. However, I am not sure if this a priori is the case. I am not sure how to use the fact that $F$ is a proper map. How can I solve this problem?","Prove that if , , is a smooth proper map with finitely many critical points, then is surjective. Here is my work so far: the set of all regular points of is open and path-connected, because , and hence connected. For any we have that for any is surjective linear map from to so that it's invertible; thus, by the inverse function theorem, is locally invertible from a neighborhood of to a neighborhood of . By the inverse function theorem again, all points of are regular values. I am not sure how to proceed from here. I know that is path connected. If is another regular point, then there exists is a path entirely contained within going from to . IF there exists a sequence of points that are in the image of that connects and in a dense way (I'm being very loose in language here, I hope you will get the picture), then we'll be done by using the compactness of the path and the inverse function theorem. However, I am not sure if this a priori is the case. I am not sure how to use the fact that is a proper map. How can I solve this problem?","F: \mathbb R^n \to \mathbb R^n n \geq 2 F R F n \geq 2 u \in R F_{*, p} p \in F^{-1}(u) \mathbb R^n \mathbb R^n F p U u U R v \in R R u v F u v F","['analysis', 'multivariable-calculus', 'derivatives', 'differential-topology']"
23,"Is this equality obvious? (""Calculus on Manifolds"" by Michael Spivak)","Is this equality obvious? (""Calculus on Manifolds"" by Michael Spivak)",,"I am reading ""Calculus on Manifolds"" by Michael Spivak. I can prove the following equality, but is it obvious? $$\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}).$$ My proof of the above equality is here (I used Theorem 4-3(1).): $$ \text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\= -\text{Alt}(T)(v_1,\dots,v_k) \\= \frac{1}{k!}\sum_{\sigma\in S_k}-\text{sgn }\sigma'\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(k)}) \\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(i)},\dots,v_{\sigma'(j)},\dots, v_{\sigma'(k)}) \\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}). $$ My proof of Theorem 4-3(1) is as follows: Let $\sigma_1:=(i,j)$ . $$\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\text{Alt}(T)(v_{\sigma_1(1)},\dots,v_{\sigma_1(i)},\dots,v_{\sigma_1(j)},\dots,v_{\sigma_1(k)}) \\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})\\= \frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma_1\cdot\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))}) \\=-\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))}) \\=-\text{Alt}(T)(v_1,\dots,v_k). $$","I am reading ""Calculus on Manifolds"" by Michael Spivak. I can prove the following equality, but is it obvious? My proof of the above equality is here (I used Theorem 4-3(1).): My proof of Theorem 4-3(1) is as follows: Let .","\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}). 
\text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=
-\text{Alt}(T)(v_1,\dots,v_k)
\\=
\frac{1}{k!}\sum_{\sigma\in S_k}-\text{sgn }\sigma'\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma'(1)},\dots,v_{\sigma'(i)},\dots,v_{\sigma'(j)},\dots, v_{\sigma'(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma(1)},\dots,v_{\sigma(j)},\dots,v_{\sigma(i)},\dots,v_{\sigma(k)}).
 \sigma_1:=(i,j) \text{Alt}(T)(v_1,\dots,v_j,\dots,v_i,\dots,v_k)\\=\text{Alt}(T)(v_{\sigma_1(1)},\dots,v_{\sigma_1(i)},\dots,v_{\sigma_1(j)},\dots,v_{\sigma_1(k)})
\\=\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})\\=
\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }\sigma_1\cdot\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})
\\=-\frac{1}{k!}\sum_{\sigma\in S_k}\text{sgn }(\sigma_1\circ\sigma)\cdot T(v_{\sigma_1(\sigma(1))},\dots,v_{\sigma_1(\sigma(i))},\dots,v_{\sigma_1(\sigma(j))},\dots,v_{\sigma_1(\sigma(k))})
\\=-\text{Alt}(T)(v_1,\dots,v_k).
","['multivariable-calculus', 'permutations', 'tensors']"
24,Let $f$ be the sum of distances to vertices. The Fermat point $p_0$ satisfies $ \left.\frac{df}{dt}(p_0+t\vec{u})\right|_{t=0}=0$ for all $\vec{u}$.,Let  be the sum of distances to vertices. The Fermat point  satisfies  for all .,f p_0  \left.\frac{df}{dt}(p_0+t\vec{u})\right|_{t=0}=0 \vec{u},"Problem: We consider a triangle $a=\left(x_1, y_1\right), b=\left(x_2, y_2\right)$ and $c=\left(x_3, y_3\right)$ with interior angles strictly less than $2 \pi / 3$ . Let $p=(x, y) \in \mathbb{R}^2$ with $d_1(x, y)=|\overrightarrow{p a}|, d_2(x, y)=$ $|\overrightarrow{p b}|$ and $d_3(x, y)=|\overrightarrow{p c}|$ . Prove that if $f(x, y)=d_1+d_2+d_3$ has a minimum value at $p_0=\left(x_0, y_0\right)$ then for all $\vec{u} \in \mathbb{R}^2$ ,we have $$ \left. \frac{d f}{d t}\left(p_0+t \vec{u}\right) \right|_{t=0}=0 $$ and $\frac{\partial f}{\partial x}\left(p_0\right)=\frac{\partial f}{\partial y}\left(p_0\right)=0$ . We say that $p_0$ is a Fermat's point of $abc$ . My solution: Let $g(t) = f(p_0 + tu)$ , $t\in \mathbb{R}$ . Since $f$ has attains its minimum value at $p_0(x_0,y_0)$ then $g$ attains its minimum value at $t=0$ . By the first order necessary condition, we have $$g'(0) = 0 \Leftrightarrow \left.\dfrac{df}{dt}(p_0 + tu)\right|_{t=0} = 0.$$ This is equivalent to \begin{align*} &\langle u, \nabla f(p_0)\rangle = 0,\ \forall u \in \mathbb{R}^2\\ \Leftrightarrow\ & \nabla f(p_0) = (0,0)^T. \end{align*} Hence, $\dfrac{df}{dx}(p_0) = \dfrac{df}{dy}(p_0) = 0$ . I hope everyone could take a look at my solution and correct it if you are interested.","Problem: We consider a triangle and with interior angles strictly less than . Let with and . Prove that if has a minimum value at then for all ,we have and . We say that is a Fermat's point of . My solution: Let , . Since has attains its minimum value at then attains its minimum value at . By the first order necessary condition, we have This is equivalent to Hence, . I hope everyone could take a look at my solution and correct it if you are interested.","a=\left(x_1, y_1\right), b=\left(x_2, y_2\right) c=\left(x_3, y_3\right) 2 \pi / 3 p=(x, y) \in \mathbb{R}^2 d_1(x, y)=|\overrightarrow{p a}|, d_2(x, y)= |\overrightarrow{p b}| d_3(x, y)=|\overrightarrow{p c}| f(x, y)=d_1+d_2+d_3 p_0=\left(x_0, y_0\right) \vec{u} \in \mathbb{R}^2 
\left.
\frac{d f}{d t}\left(p_0+t \vec{u}\right)
\right|_{t=0}=0
 \frac{\partial f}{\partial x}\left(p_0\right)=\frac{\partial f}{\partial y}\left(p_0\right)=0 p_0 abc g(t) = f(p_0 + tu) t\in \mathbb{R} f p_0(x_0,y_0) g t=0 g'(0) = 0 \Leftrightarrow \left.\dfrac{df}{dt}(p_0 + tu)\right|_{t=0} = 0. \begin{align*}
&\langle u, \nabla f(p_0)\rangle = 0,\ \forall u \in \mathbb{R}^2\\
\Leftrightarrow\ & \nabla f(p_0) = (0,0)^T.
\end{align*} \dfrac{df}{dx}(p_0) = \dfrac{df}{dy}(p_0) = 0","['geometry', 'analysis', 'multivariable-calculus', 'derivatives']"
25,Solution of integral $\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{ 2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx dy$,Solution of integral,\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{ 2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx dy,"I am working on a simulation experiment on light where I concluded that the magnitude of intensity at each pixel is given by the equation - $$C_\text{total}=\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{ 2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx\space dy$$ where $x$ and $y$ are the variables and rest of the symbols are constants. Problem and Motivation I am unable to solve the double integral expression on the RHS. The solution of this expression will enable achieving highest possible level of accuracy in the simulation. Current Efforts In present, I am using the Monte-Carlo method for evaluation of integrals. But, it is compute intensive. Greater the precision, greater the amount of computation required since more random sample points are required for precise evaluation of integrals. Considering my requirement of precisions up to $10^{-9}$ , this method is not enough to produce satisfactory accuracy in simulation since computing power is practically limited. Expectations To achieve satisfactory level of accuracy, I expect to find an analytical solution (rather than the numerical one being used presently). The best case is to have a proper solution, but, concepts like power-series expansion of integral would also be great.","I am working on a simulation experiment on light where I concluded that the magnitude of intensity at each pixel is given by the equation - where and are the variables and rest of the symbols are constants. Problem and Motivation I am unable to solve the double integral expression on the RHS. The solution of this expression will enable achieving highest possible level of accuracy in the simulation. Current Efforts In present, I am using the Monte-Carlo method for evaluation of integrals. But, it is compute intensive. Greater the precision, greater the amount of computation required since more random sample points are required for precise evaluation of integrals. Considering my requirement of precisions up to , this method is not enough to produce satisfactory accuracy in simulation since computing power is practically limited. Expectations To achieve satisfactory level of accuracy, I expect to find an analytical solution (rather than the numerical one being used presently). The best case is to have a proper solution, but, concepts like power-series expansion of integral would also be great.","C_\text{total}=\int_{y_1}^{y_2}\int_{x_1}^{x_2}A\sin\left(\frac{
2\pi\sqrt{(X-x)^2 + (Y-y)^2 + Z^2}}{\lambda}\right) dx\space dy x y 10^{-9}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
26,Calculus of variations integral with additional condition,Calculus of variations integral with additional condition,,"Find the functions $y_1$ and $y_2$ with which the functional $$J = \int_{0}^{\pi/2}(2y_1y_2 + (y_1^{'})^2+(y_2{'})^2)dx$$ has conditional extremum. $$\begin{cases}y_1(0)=-1,\\y_2(0)=1,&\end{cases}\quad\quad\begin{cases}y_1(\pi/2)=\frac{\pi^2}4-1,\\y_2(\pi/2)=\frac{\pi^2}4+1.&\end{cases}$$ With additional condition $y_1^{\prime}+y_2^{\prime}=4x.$ Here are my ideas or hints I've been given so far: It is a conditional functional problem (due to additional condition). We need to set up a ""helping"" functional such $J^{*}=\int_{a}^{b}(F+\lambda(x)g)dx$ and solve using Lagrange multipliers. Then, I am not sure I know how to proceed further. The answer should look like $y_1=$ and $y_2=$ , without constants because to find them we are given the boundary conditions. I would appreciate if someone could show how to solve this problem. Here is my work so far: $J^{*}=\int_{a}^{b}((F+\lambda(x)g)dx=\int_{0}^{\pi/2}(2y_1y_2+ (y_1^{'})^2+(y_2{'})^2)+\lambda (y'_1+y_2^{'}))  dx $ Now I was suggested to set some system of equations, but I don't know what they mean by that.","Find the functions and with which the functional has conditional extremum. With additional condition Here are my ideas or hints I've been given so far: It is a conditional functional problem (due to additional condition). We need to set up a ""helping"" functional such and solve using Lagrange multipliers. Then, I am not sure I know how to proceed further. The answer should look like and , without constants because to find them we are given the boundary conditions. I would appreciate if someone could show how to solve this problem. Here is my work so far: Now I was suggested to set some system of equations, but I don't know what they mean by that.","y_1 y_2 J = \int_{0}^{\pi/2}(2y_1y_2 + (y_1^{'})^2+(y_2{'})^2)dx \begin{cases}y_1(0)=-1,\\y_2(0)=1,&\end{cases}\quad\quad\begin{cases}y_1(\pi/2)=\frac{\pi^2}4-1,\\y_2(\pi/2)=\frac{\pi^2}4+1.&\end{cases} y_1^{\prime}+y_2^{\prime}=4x. J^{*}=\int_{a}^{b}(F+\lambda(x)g)dx y_1= y_2= J^{*}=\int_{a}^{b}((F+\lambda(x)g)dx=\int_{0}^{\pi/2}(2y_1y_2+ (y_1^{'})^2+(y_2{'})^2)+\lambda (y'_1+y_2^{'}))  dx ","['calculus', 'multivariable-calculus', 'systems-of-equations', 'calculus-of-variations']"
27,"Computing the surface integral of $\ G(x,y,z) = xyz $ over the triangular surface with vertices $(1,0,0),\,(0,2,0)$ and $(0,1,1)$.",Computing the surface integral of  over the triangular surface with vertices  and .,"\ G(x,y,z) = xyz  (1,0,0),\,(0,2,0) (0,1,1)","I was attempting to compute the surface integral of $\ G(x,y,z) = \ xyz $ over the triangular surface with vertices at $\ (1,0,0)$ , $\ (0,2,0)$ and $\ (0,1,1)$ . Clearly, the first step is to parametrize the region. I thought of two approaches : The first approach was choosing $\ x$ and $\ y $ as my parameters and rewriting $\ z $ as a function of $\ x$ and $\ y$ , then projecting the above triangular surface onto the $\ xy $ plane to find the upper and lower bounds of the required surface integral. The surface is given by the equation: $$\ 2x + y + z = 2 $$ Then, we can use the formula: $$ \ dS = \sqrt(1+f_x^2+f_y^2) \cdot dx dy $$ Since the normal vector of our projection is along the positive z direction. So, I expressed $\ z$ in terms of $\ x$ and $\ y$ , substituted $\ z(x,y) $ into G and obtained the double integral in terms of $\ x$ and $\ y$ . But I am having some trouble identifying the bounds of the double integral. It is obvious that 0 <= $\ x $ <= 1, but what should be the bounds for y? I tried sketching the shadow of the triangular region on the $\ xy $ plane by moving the point $\ (0,1,1)$ to $\ (0,1,0)$ and computing the bounds for $\ y $ for a given $\ x$ . Doing that, I got $\ (1-x) <= y <= (2-2x) $ but I am unable to obtain the correct answer via this method. So then, I thought of another method. I thought of doing the same thing with the $\ x-z $ plane instead of the $\ xy $ plane. The triangular surface remains the same and the integrand also remains the same if we simply swap out $\ y$ with $\ z$ . But I am also confused about the bounds in this case. If I project the point $\ (0,1,1)$ onto $\ (0,0,1)$ and the point $\ (0,2,0)$ to $\ (0,0,0)$ , I get the bounds of z as $\ 0$ <= $\ z$ <= $\ (1-x) $ . I am yet again getting an incorrect answer with this. It is possible that the source I am checking for answers is incorrect itself. The answers I am obtaining via both methods is $\frac{\sqrt6}{30}$ , while the answer given is $\frac{\sqrt6}{15}$ . Can someone please clarify the appropriate procedure to find the correct bounds in this question, and the correct answer if possible?","I was attempting to compute the surface integral of over the triangular surface with vertices at , and . Clearly, the first step is to parametrize the region. I thought of two approaches : The first approach was choosing and as my parameters and rewriting as a function of and , then projecting the above triangular surface onto the plane to find the upper and lower bounds of the required surface integral. The surface is given by the equation: Then, we can use the formula: Since the normal vector of our projection is along the positive z direction. So, I expressed in terms of and , substituted into G and obtained the double integral in terms of and . But I am having some trouble identifying the bounds of the double integral. It is obvious that 0 <= <= 1, but what should be the bounds for y? I tried sketching the shadow of the triangular region on the plane by moving the point to and computing the bounds for for a given . Doing that, I got but I am unable to obtain the correct answer via this method. So then, I thought of another method. I thought of doing the same thing with the plane instead of the plane. The triangular surface remains the same and the integrand also remains the same if we simply swap out with . But I am also confused about the bounds in this case. If I project the point onto and the point to , I get the bounds of z as <= <= . I am yet again getting an incorrect answer with this. It is possible that the source I am checking for answers is incorrect itself. The answers I am obtaining via both methods is , while the answer given is . Can someone please clarify the appropriate procedure to find the correct bounds in this question, and the correct answer if possible?","\ G(x,y,z) = \ xyz  \ (1,0,0) \ (0,2,0) \ (0,1,1) \ x \ y  \ z  \ x \ y \ xy  \ 2x + y + z = 2   \ dS = \sqrt(1+f_x^2+f_y^2) \cdot dx dy  \ z \ x \ y \ z(x,y)  \ x \ y \ x  \ xy  \ (0,1,1) \ (0,1,0) \ y  \ x \ (1-x) <= y <= (2-2x)  \ x-z  \ xy  \ y \ z \ (0,1,1) \ (0,0,1) \ (0,2,0) \ (0,0,0) \ 0 \ z \ (1-x)  \frac{\sqrt6}{30} \frac{\sqrt6}{15}",['multivariable-calculus']
28,"Problem 3-32 in ""Calculus on Manifolds"" by Michael Spivak. What are ""considerably weaker hypotheses""?","Problem 3-32 in ""Calculus on Manifolds"" by Michael Spivak. What are ""considerably weaker hypotheses""?",,"Problem 3-32. Let $f:[a,b]\times [c,d]\to\mathbb{R}$ be continuous and suppose $D_2f$ is continuous. Define $F(y)=\int_a^b f(x,y) dx$ . Prove Leibnitz's rule: $F'(y)=\int_a^b D_2f(x,y) dx$ . Hint: $F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx$ . (The proof will show that continuity of $D_2f$ may be replaced by considerably weaker hypotheses.) I solved this problem as follows. Since $D_2f$ is continuous on $[a,b]\times [c,d]$ , $D_2f$ is uniformly continuous on $[a,b]\times [c,d]$ . So, for arbitrary positive real number $\varepsilon$ , there is a positive real number $\delta$ such that $(x_1,y_1)\in [a,b]\times [c,d]$ and $(x_2,y_2)\in [a,b]\times [c,d]$ and $|(x_1,y_1)-(x_2,y_2)|<\delta\implies |D_2f(x_1,y_1)-D_2f(x_2,y_2)|<\frac{\varepsilon}{b-a}$ . By Fubini's theorem, $F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C$ , where $C$ is some real number. Let $(x,y)\in [a,b]\times [c,d]$ . Let $z\in [c,d]$ and $|z-y|<\delta$ . Then $|(x,z)-(x,y)|=|z-y|<\delta$ . So, $|D_2f(x,z)-D_2f(x,y)|<\frac{\varepsilon}{b-a}$ . So, $\left|\int_a^b D_2f(x,z)dx-\int_a^b D_2f(x,y)dx\right|\leq\int_a^b \left|D_2f(x,z)-D_2f(x,y)\right|dx<\int_a^b \frac{\varepsilon}{b-a}dx=\varepsilon$ . So, $G$ such that $G(y):=\int_a^b D_2f(x,y)dx$ is continuous on $[c,d]$ . So, $F(y)=\int_c^y G(y)dy+C$ is differentiable at $y\in [c,d]$ and $F'(y)=G(y)=\int_a^b D_2f(x,y)dx$ . The author wrote ""The proof will show that continuity of $D_2f$ may be replaced by considerably weaker hypotheses."". What are ""considerably weaker hypotheses""? My attempt: Is there a function $f:[a,b]\times [c,d]\to\mathbb{R}$ which satisfies the following conditions? $D_2f$ exists on $[a,b]\times [c,d]$ . $D_2f$ is integrable on $[a,b]\times [c,d]$ . $D_2f$ is not continuous on $[a,b]\times [c,d]$ . For any $\varepsilon>0$ and any $y\in [c,d]$ , there is $\delta>0$ such that $|D_2f(x,z)-D_2f(x,y)|<\varepsilon$ if $x\in [a,b]$ and $z\in [c,d]$ and $|y-z|<\delta$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto f(x,y)\in\mathbb{R}$ is integrable on $[a,b]$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto\int_c^y D_2f(x,y)dy\in\mathbb{R}$ is integrable on $[a,b]$ . For any $y\in [c,d]$ , $[a,b]\ni x\mapsto D_2f(x,y)\in\mathbb{R}$ is integrable on $[a,b]$ . Suppose such $f$ exists. By 5, $F(y)=\int_a^b f(x,y)dx$ exists for any $y\in [c,d]$ . By 4, $\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx$ . By 6 and 5, $\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C$ , where $C$ is some real number. By 2 and 4 and 7 and Fubini's theorem, $\int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C$ . By 4, $G$ such that $G(y):=\int_a^b D_2f(x,y)dx$ is continuous on $[c,d]$ . So, $F(y)=\int_c^y G(y)dy+C$ is differentiable at $y\in [c,d]$ and $F'(y)=G(y)=\int_a^b D_2f(x,y)dx$ . But by 3, $D_2f$ is not continuous on $[a,b]\times [c,d]$ .","Problem 3-32. Let be continuous and suppose is continuous. Define . Prove Leibnitz's rule: . Hint: . (The proof will show that continuity of may be replaced by considerably weaker hypotheses.) I solved this problem as follows. Since is continuous on , is uniformly continuous on . So, for arbitrary positive real number , there is a positive real number such that and and . By Fubini's theorem, , where is some real number. Let . Let and . Then . So, . So, . So, such that is continuous on . So, is differentiable at and . The author wrote ""The proof will show that continuity of may be replaced by considerably weaker hypotheses."". What are ""considerably weaker hypotheses""? My attempt: Is there a function which satisfies the following conditions? exists on . is integrable on . is not continuous on . For any and any , there is such that if and and . For any , is integrable on . For any , is integrable on . For any , is integrable on . Suppose such exists. By 5, exists for any . By 4, . By 6 and 5, , where is some real number. By 2 and 4 and 7 and Fubini's theorem, . By 4, such that is continuous on . So, is differentiable at and . But by 3, is not continuous on .","f:[a,b]\times [c,d]\to\mathbb{R} D_2f F(y)=\int_a^b f(x,y) dx F'(y)=\int_a^b D_2f(x,y) dx F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx D_2f D_2f [a,b]\times [c,d] D_2f [a,b]\times [c,d] \varepsilon \delta (x_1,y_1)\in [a,b]\times [c,d] (x_2,y_2)\in [a,b]\times [c,d] |(x_1,y_1)-(x_2,y_2)|<\delta\implies |D_2f(x_1,y_1)-D_2f(x_2,y_2)|<\frac{\varepsilon}{b-a} F(y)=\int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C C (x,y)\in [a,b]\times [c,d] z\in [c,d] |z-y|<\delta |(x,z)-(x,y)|=|z-y|<\delta |D_2f(x,z)-D_2f(x,y)|<\frac{\varepsilon}{b-a} \left|\int_a^b D_2f(x,z)dx-\int_a^b D_2f(x,y)dx\right|\leq\int_a^b \left|D_2f(x,z)-D_2f(x,y)\right|dx<\int_a^b \frac{\varepsilon}{b-a}dx=\varepsilon G G(y):=\int_a^b D_2f(x,y)dx [c,d] F(y)=\int_c^y G(y)dy+C y\in [c,d] F'(y)=G(y)=\int_a^b D_2f(x,y)dx D_2f f:[a,b]\times [c,d]\to\mathbb{R} D_2f [a,b]\times [c,d] D_2f [a,b]\times [c,d] D_2f [a,b]\times [c,d] \varepsilon>0 y\in [c,d] \delta>0 |D_2f(x,z)-D_2f(x,y)|<\varepsilon x\in [a,b] z\in [c,d] |y-z|<\delta y\in [c,d] [a,b]\ni x\mapsto f(x,y)\in\mathbb{R} [a,b] y\in [c,d] [a,b]\ni x\mapsto\int_c^y D_2f(x,y)dy\in\mathbb{R} [a,b] y\in [c,d] [a,b]\ni x\mapsto D_2f(x,y)\in\mathbb{R} [a,b] f F(y)=\int_a^b f(x,y)dx y\in [c,d] \int_a^b f(x,y)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx \int_a^b\left(\int_c^y D_2f(x,y)dy+f(x,c)\right)dx=\int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C C \int_a^b\left(\int_c^y D_2f(x,y)dy\right)dx+C=\int_c^y\left(\int_a^b D_2f(x,y)dx\right)dy+C G G(y):=\int_a^b D_2f(x,y)dx [c,d] F(y)=\int_c^y G(y)dy+C y\in [c,d] F'(y)=G(y)=\int_a^b D_2f(x,y)dx D_2f [a,b]\times [c,d]","['integration', 'multivariable-calculus', 'partial-derivative', 'fubini-tonelli-theorems']"
29,Finding flux of a vector field through a hemisphere [duplicate],Finding flux of a vector field through a hemisphere [duplicate],,"This question already has an answer here : Stokes theorem normalization (1 answer) Closed 6 months ago . This question is given in a (publicly shared) past exam at my university: Let S be the upper hemisphere of $x^2 + y^2 + z^2 = 4$ with normal vector pointing toward the origin, and $\vec F = z \vec x / |\vec x|$ where $\vec x = <x, y, z>$ . Compute $\iint\vec F \cdot d\vec S $ The answer is given as $-8\pi$ ; I understand that it can be obtained by parameterizing the sphere and computing $\vec t_u \times \vec t_v = <x/z, y/z, 1>$ as the normal vector where $z = \sqrt{4 - x^2 - y^2}$ , then computing the integral. However, I am confused by this as I have read that the flux across a sphere can be obtained using $\vec r / | \vec r |$ as the normal, where $\vec r = <x, y, z>$ . This results in a different answer, albeit not far off from $-8\pi$ . Which is the proper normal to use in a flux surface integral where S is a sphere?","This question already has an answer here : Stokes theorem normalization (1 answer) Closed 6 months ago . This question is given in a (publicly shared) past exam at my university: Let S be the upper hemisphere of with normal vector pointing toward the origin, and where . Compute The answer is given as ; I understand that it can be obtained by parameterizing the sphere and computing as the normal vector where , then computing the integral. However, I am confused by this as I have read that the flux across a sphere can be obtained using as the normal, where . This results in a different answer, albeit not far off from . Which is the proper normal to use in a flux surface integral where S is a sphere?","x^2 + y^2 + z^2 = 4 \vec F = z \vec x / |\vec x| \vec x = <x, y, z> \iint\vec F \cdot d\vec S  -8\pi \vec t_u \times \vec t_v = <x/z, y/z, 1> z = \sqrt{4 - x^2 - y^2} \vec r / | \vec r | \vec r = <x, y, z> -8\pi","['calculus', 'multivariable-calculus']"
30,"$f,g$ are real-valued, $f(x,y)=g(h_1(x),h_2(y))$, $f,h_i$ are continuous, $g$ is increasing, does $g$ must be continuous?","are real-valued, ,  are continuous,  is increasing, does  must be continuous?","f,g f(x,y)=g(h_1(x),h_2(y)) f,h_i g g","Assumptions: $f,g$ are real-valued. $g:[0,1]^2\to\mathbb R$ . Functions $h_1:X\to\mathbb [0,1]$ and $h_2:Y\to\mathbb [0,1]$ are surjective continuous. $X,Y$ are connected separable. $f$ is continuous, $f(x,y)=g(h_1(x),h_2(y))$ . Questions: Does the strict increasingness of $g$ in all variables imply the continuity of $g$ ? Backgrounds : It is known the composite of continuous functions is continuous. What about the other way around? What we know: even without increasingness, $g$ is continuous if one of the followings hold: X, Y are path-connected X,Y are compact Hausdorff $h_i$ are homomorphisms $h_i$ are quotient maps To prove the claim, we only need to show that if $g$ is continuous restricted to one direction, then $g$ is continuous. Also, since $g$ is increasing, the restricted function of $g$ can only contain a jump discontinuity. (If we allow essential discontinuity, a simple counterexample exists , this is why the increasingness is powerful here). I've been trying to construct a counterexample with jump discontinuity for a while, with no luck.","Assumptions: are real-valued. . Functions and are surjective continuous. are connected separable. is continuous, . Questions: Does the strict increasingness of in all variables imply the continuity of ? Backgrounds : It is known the composite of continuous functions is continuous. What about the other way around? What we know: even without increasingness, is continuous if one of the followings hold: X, Y are path-connected X,Y are compact Hausdorff are homomorphisms are quotient maps To prove the claim, we only need to show that if is continuous restricted to one direction, then is continuous. Also, since is increasing, the restricted function of can only contain a jump discontinuity. (If we allow essential discontinuity, a simple counterexample exists , this is why the increasingness is powerful here). I've been trying to construct a counterexample with jump discontinuity for a while, with no luck.","f,g g:[0,1]^2\to\mathbb R h_1:X\to\mathbb [0,1] h_2:Y\to\mathbb [0,1] X,Y f f(x,y)=g(h_1(x),h_2(y)) g g g h_i h_i g g g g","['real-analysis', 'general-topology', 'multivariable-calculus', 'differential-topology']"
31,Verify (or critique) this informal proof of Green's theorem,Verify (or critique) this informal proof of Green's theorem,,"In order to better understand Green's Theorem, I developed this informal proof, which I request verification and critique of (both the proof and its writing).  Of course, any textbook has a proof: my goal here is to improve my intuition by developing a proof from a simple argument I could visualize, and then passing to the limit. Consider a linear function $\mathbf F: \mathbb R^2 \to \mathbb R^2$ .   Write $\mathbf{F}(x,y) = P(x,y) \mathbf i + Q(x,y) \mathbf j$ , and let $p = P(0,0)$ and $q = Q(0,0)$ . Then, for any rectangle $C$ parallel to the $x$ and $y$ axes with width $w$ and height $h$ , $$ \frac 1 {hw} \oint_C \mathbf F \cdot d\mathbf l$$ is the circulation of $\mathbf F$ around $C$ divided by the area of $C$ . Define the 2D curl of $\bf F$ at $(x,y)$ as $$\lim_{h,w \to 0} \frac 1 {hw} \oint_C \mathbf F \cdot d\mathbf l$$ for any rectangle $C$ parallel to the $x$ and $y$ axes with width $w$ and height $h$ , provided that this quantity exists and is identical for any sequence of such rectangles each containing $(x,y)$ . We claim that the 2D curl of $\mathbf F$ is $\frac {\partial Q}{\partial x} - \frac {\partial P}{\partial y}$ .  For consider any rectangle $C$ with corners $(-u, -v), (u, -v), (u,v), (-u,v)$ .   Then, $hw = 4uv$ and, since $\mathbf F$ is linear, \begin{align} \oint_C \mathbf F \cdot d \mathbf l &= 2u\left(p - \frac {\partial P}{\partial y}v\right) + 2v\left(q + \frac {\partial Q}{\partial x}u\right) - 2u\left(p + \frac {\partial P}{\partial y}v\right) - 2v\left(q - \frac {\partial Q}{\partial u}v\right) \\ &= 4uv\left(\frac {\partial Q}{\partial x} - \frac {\partial P}{\partial y}\right). \end{align} (This is the crux of the argument.  The rest is simply set up or justification of using the linear approximation to find the limit.) If we relax the assumption that $\mathbf F$ is linear, but still assume it to be smooth then over a sufficiently small rectangle, the linear approximation holds to arbitrary accuracy, and the limit still holds.  So the 2D curl of $\mathbf F$ at $(x,y)$ is $\frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y)$ . Now consider an arbitrary smooth region $R$ .  It can be shown via a simple geometric argument that the circulation around $R$ is equal to the sum of the circulations around any decomposition of $R$ .  Then for any grain size $G > 0$ , we can decompose $R$ into rectangles parallel to $x$ and $y$ axes of height and width $G$ , plus a peripheral region.  As $G \to 0$ , the area of this periphery shrinks faster than $G$ , and since $F$ is assumed smooth, the circulation of the periphery goes to zero as well.  Thus, as $G \to 0$ ,  the sum of the circulation around all of the rectangles goes to the circulation around the entire region.  But the sum of the circulation around all of the rectangles is simply $\iint_R \frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y) \, dx \, dy$ , giving as desired $$\oint_{\partial R} \mathbf F \cdot d\mathbf l = \iint_R \frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y) \, dx \, dy.$$","In order to better understand Green's Theorem, I developed this informal proof, which I request verification and critique of (both the proof and its writing).  Of course, any textbook has a proof: my goal here is to improve my intuition by developing a proof from a simple argument I could visualize, and then passing to the limit. Consider a linear function .   Write , and let and . Then, for any rectangle parallel to the and axes with width and height , is the circulation of around divided by the area of . Define the 2D curl of at as for any rectangle parallel to the and axes with width and height , provided that this quantity exists and is identical for any sequence of such rectangles each containing . We claim that the 2D curl of is .  For consider any rectangle with corners .   Then, and, since is linear, (This is the crux of the argument.  The rest is simply set up or justification of using the linear approximation to find the limit.) If we relax the assumption that is linear, but still assume it to be smooth then over a sufficiently small rectangle, the linear approximation holds to arbitrary accuracy, and the limit still holds.  So the 2D curl of at is . Now consider an arbitrary smooth region .  It can be shown via a simple geometric argument that the circulation around is equal to the sum of the circulations around any decomposition of .  Then for any grain size , we can decompose into rectangles parallel to and axes of height and width , plus a peripheral region.  As , the area of this periphery shrinks faster than , and since is assumed smooth, the circulation of the periphery goes to zero as well.  Thus, as ,  the sum of the circulation around all of the rectangles goes to the circulation around the entire region.  But the sum of the circulation around all of the rectangles is simply , giving as desired","\mathbf F: \mathbb R^2 \to \mathbb R^2 \mathbf{F}(x,y) = P(x,y) \mathbf i + Q(x,y) \mathbf j p = P(0,0) q = Q(0,0) C x y w h  \frac 1 {hw} \oint_C \mathbf F \cdot d\mathbf l \mathbf F C C \bf F (x,y) \lim_{h,w \to 0} \frac 1 {hw} \oint_C \mathbf F \cdot d\mathbf l C x y w h (x,y) \mathbf F \frac {\partial Q}{\partial x} - \frac {\partial P}{\partial y} C (-u, -v), (u, -v), (u,v), (-u,v) hw = 4uv \mathbf F \begin{align}
\oint_C \mathbf F \cdot d \mathbf l &= 2u\left(p - \frac {\partial P}{\partial y}v\right) + 2v\left(q + \frac {\partial Q}{\partial x}u\right) - 2u\left(p + \frac {\partial P}{\partial y}v\right) - 2v\left(q - \frac {\partial Q}{\partial u}v\right) \\
&= 4uv\left(\frac {\partial Q}{\partial x} - \frac {\partial P}{\partial y}\right).
\end{align} \mathbf F \mathbf F (x,y) \frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y) R R R G > 0 R x y G G \to 0 G F G \to 0 \iint_R \frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y) \, dx \, dy \oint_{\partial R} \mathbf F \cdot d\mathbf l = \iint_R \frac {\partial Q}{\partial x}(x,y) - \frac {\partial P}{\partial y}(x,y) \, dx \, dy.","['multivariable-calculus', 'solution-verification', 'vector-analysis', 'intuition']"
32,What is the change in angle of of a symmetric matrix when summed over a plane?,What is the change in angle of of a symmetric matrix when summed over a plane?,,"A symmetric matrix has orthogonal eigenvectors with real eigenvalues, and hence can be thought of as scaling along a particular orthogonal set of axes. Of course, not all vectors are (usually) eigenvectors, and therefore many individual vectors will have their direction changed under a symmetric matrix. It seems to me, however, that these direction changes will somehow ""cancel out"" in the sense that there is no net rotation by a symmetric matrix.  I don't know how to state this more precisely, other than to observe that $A_{ij} - A_{ji}$ is a measure of the rotation in the $i,j$ plane (see here ). That is, if $A$ is a symmetric matrix, for any plane $P$ , the net direction change of $Av$ over all vectors $v \in P$ is zero.  Again, I'm not sure how state this more precisely, but perhaps it could be formulated as an integral. Is my intuition correct? And how can it be stated more precisely?","A symmetric matrix has orthogonal eigenvectors with real eigenvalues, and hence can be thought of as scaling along a particular orthogonal set of axes. Of course, not all vectors are (usually) eigenvectors, and therefore many individual vectors will have their direction changed under a symmetric matrix. It seems to me, however, that these direction changes will somehow ""cancel out"" in the sense that there is no net rotation by a symmetric matrix.  I don't know how to state this more precisely, other than to observe that is a measure of the rotation in the plane (see here ). That is, if is a symmetric matrix, for any plane , the net direction change of over all vectors is zero.  Again, I'm not sure how state this more precisely, but perhaps it could be formulated as an integral. Is my intuition correct? And how can it be stated more precisely?","A_{ij} - A_{ji} i,j A P Av v \in P","['linear-algebra', 'matrices', 'multivariable-calculus', 'intuition', 'conjectures']"
33,Seeking guidance on calculating the volume of a four-dimensional cone,Seeking guidance on calculating the volume of a four-dimensional cone,,"I'm working on a problem involving a four-dimensional cone defined as follows: $C = \left\{\left(x,y,z,t\right)| (x,y,z) \in (1 - \frac{t}{12})B,0 \leq t \leq 12\right\}$ where the base $B$ is described by $x^2 + (y - 1)^2 + z^2 \leq 1$ . I'm tasked with finding the volume (measure) of $\mu(C)$ . I understand that the base $B$ is a three-dimensional ball centered at $(0,1,0)$ with a radius of 1. However, I'm unsure how to approach calculating the volume of this four-dimensional cone based on this information. Could someone please provide guidance on how to compute the volume of this four-dimensional cone? Any step-by-step approach or insights into higher-dimensional geometry would be greatly appreciated!","I'm working on a problem involving a four-dimensional cone defined as follows: where the base is described by . I'm tasked with finding the volume (measure) of . I understand that the base is a three-dimensional ball centered at with a radius of 1. However, I'm unsure how to approach calculating the volume of this four-dimensional cone based on this information. Could someone please provide guidance on how to compute the volume of this four-dimensional cone? Any step-by-step approach or insights into higher-dimensional geometry would be greatly appreciated!","C = \left\{\left(x,y,z,t\right)| (x,y,z) \in (1 - \frac{t}{12})B,0 \leq t \leq 12\right\} B x^2 + (y - 1)^2 + z^2 \leq 1 \mu(C) B (0,1,0)","['geometry', 'measure-theory', 'multivariable-calculus', 'volume', 'solid-geometry']"
34,Tangent Bundle of a smooth manifold with boundary,Tangent Bundle of a smooth manifold with boundary,,"This is exercise 3.19 from John Lee's Introduction to Smooth Manifolds that I can't quite prove. Suppose $M$ is a smooth manifold with boundary. Show that $TM$ has a natural topology and smooth structure making it into a smooth manifold with boundary, such that if $(U,(x^i))$ is any choundary chart for $M$ , then rearranging the coordinates in the natural chart $(\pi^{-1}(U),(x^i,v^i))$ for $TM$ yields a boundary chart $(\pi^{-1}(U),(v^i,x^i))$ . For interior charts we define the chart as given an interior chart $(U,\phi)$ for $M$ , since $\pi^{-1}(U) \subset TM$ is the set of all tangent vectors to $M$ at all points of $U$ , and letting $(x^1, \dots, x^n)$ denote the coordinate functions of $\phi$ , we can define a map $\tilde{\phi}:\pi^{-1}(U)\to \mathbb{R}^{2n}$ by $$\tilde{\phi}(v^i\frac{\partial}{\partial x^i}|_p)=(x^1(p),\dots, x^n(p), v^1, \dots, v^n).$$ Its image set is $\phi(U)\times \mathbb{R}^n$ which is an open subset of $\mathbb{R}^{2n}$ , and is a bijection onto its image where its inverse is $$\tilde{\phi}^{-1}(x^1,\dots, x^n, v^1, \dots, v^n)=v^i \frac{\partial}{\partial x^i}|_{\phi^{-1}(x)}.$$ Then smooth compatibility given two smooth charts $(U,\varphi)$ and $(V,\psi)$ for $M$ , we have the transition map $\tilde{\psi}\circ \tilde{\varphi}^{-1}:\varphi(U \cap V)\times \mathbb{R}^n \to \psi(U\cap V) \times \mathbb{R}^n$ written as $$\tilde{\psi} \circ \tilde{\varphi}^{-1}(x^1, \dots, x^n, v^1, \dots, v^n)=(\tilde{x}^1(x),\dots \tilde{x}^n(x), \frac{\partial \tilde{x}^1}{\partial x^j}(x)v^j,\frac{\partial \tilde{x}^n}{\partial x^j}(x)v^j)$$ which is clearly smooth. The problem I have is establishing smooth compatibility between interior charts and boundary charts. By the Smooth Manifold Chart Lemma (Lemma 1.35), we need to show that whenever $\pi^{-1}(U)$ and $\pi^{-1}(V)$ intersect, the map $\tilde{\psi} \circ \tilde{\varphi}^{-1}$ is smooth. If we take the transition of two boundary charts as suggested by rearranging the order of $v^i$ and $x^i$ , we would get the same kind of equation as above with just a rearrangement and the final component will be in the half plane $\mathbb{H}^n=\{x: (x)_n \ge 0\}$ . However, how do we ensure smooth compatibility for interior and boundary charts, i.e. the case where $\psi$ is a boundary chart and $\varphi$ is an interior chart and vice versa? I think this can be resolved by the invariance theorem of boundary. So $U\cap V$ can only be interior or boundary domain but not both. So in the case where $(U,\phi)$ is an interior chart and $(V,\psi)$ is a boundary chart, $U\cap V$ cannot contain boundary points, so the transition map $\tilde{\psi} \circ \tilde{\phi}^{-1}(x^1, \dots, x^n, v^1, \dots , v^n)=(\frac{\partial \tilde{x}^1}{\partial x^j}(x)v^j,\dots, \frac{\partial \tilde{x}^n}{\partial x^j}(x)v^j, \tilde{x}^1(x),\dots, \tilde{x}^n(x))$ , and $\tilde{\phi} \circ \tilde{\psi}^{-1}(v^1, \dots , v^n, \tilde{x}^1, \dots, \tilde{x}^n,)=(x^1(\tilde{x}),\dots, x^n(\tilde{x}), \frac{\partial x^1}{\partial \tilde{x}^j}(\tilde{x})v^j,\dots, \frac{\partial x^n}{\partial \tilde{x}^j}(\tilde{x})v^j)$ which are smooth as maps with domain and codomain contained in $\mathbb{R}^{2n}$ and the first transition is just a rearrangement of the case with two interior charts.","This is exercise 3.19 from John Lee's Introduction to Smooth Manifolds that I can't quite prove. Suppose is a smooth manifold with boundary. Show that has a natural topology and smooth structure making it into a smooth manifold with boundary, such that if is any choundary chart for , then rearranging the coordinates in the natural chart for yields a boundary chart . For interior charts we define the chart as given an interior chart for , since is the set of all tangent vectors to at all points of , and letting denote the coordinate functions of , we can define a map by Its image set is which is an open subset of , and is a bijection onto its image where its inverse is Then smooth compatibility given two smooth charts and for , we have the transition map written as which is clearly smooth. The problem I have is establishing smooth compatibility between interior charts and boundary charts. By the Smooth Manifold Chart Lemma (Lemma 1.35), we need to show that whenever and intersect, the map is smooth. If we take the transition of two boundary charts as suggested by rearranging the order of and , we would get the same kind of equation as above with just a rearrangement and the final component will be in the half plane . However, how do we ensure smooth compatibility for interior and boundary charts, i.e. the case where is a boundary chart and is an interior chart and vice versa? I think this can be resolved by the invariance theorem of boundary. So can only be interior or boundary domain but not both. So in the case where is an interior chart and is a boundary chart, cannot contain boundary points, so the transition map , and which are smooth as maps with domain and codomain contained in and the first transition is just a rearrangement of the case with two interior charts.","M TM (U,(x^i)) M (\pi^{-1}(U),(x^i,v^i)) TM (\pi^{-1}(U),(v^i,x^i)) (U,\phi) M \pi^{-1}(U) \subset TM M U (x^1, \dots, x^n) \phi \tilde{\phi}:\pi^{-1}(U)\to \mathbb{R}^{2n} \tilde{\phi}(v^i\frac{\partial}{\partial x^i}|_p)=(x^1(p),\dots, x^n(p), v^1, \dots, v^n). \phi(U)\times \mathbb{R}^n \mathbb{R}^{2n} \tilde{\phi}^{-1}(x^1,\dots, x^n, v^1, \dots, v^n)=v^i \frac{\partial}{\partial x^i}|_{\phi^{-1}(x)}. (U,\varphi) (V,\psi) M \tilde{\psi}\circ \tilde{\varphi}^{-1}:\varphi(U \cap V)\times \mathbb{R}^n \to \psi(U\cap V) \times \mathbb{R}^n \tilde{\psi} \circ \tilde{\varphi}^{-1}(x^1, \dots, x^n, v^1, \dots, v^n)=(\tilde{x}^1(x),\dots \tilde{x}^n(x), \frac{\partial \tilde{x}^1}{\partial x^j}(x)v^j,\frac{\partial \tilde{x}^n}{\partial x^j}(x)v^j) \pi^{-1}(U) \pi^{-1}(V) \tilde{\psi} \circ \tilde{\varphi}^{-1} v^i x^i \mathbb{H}^n=\{x: (x)_n \ge 0\} \psi \varphi U\cap V (U,\phi) (V,\psi) U\cap V \tilde{\psi} \circ \tilde{\phi}^{-1}(x^1, \dots, x^n, v^1, \dots , v^n)=(\frac{\partial \tilde{x}^1}{\partial x^j}(x)v^j,\dots, \frac{\partial \tilde{x}^n}{\partial x^j}(x)v^j, \tilde{x}^1(x),\dots, \tilde{x}^n(x)) \tilde{\phi} \circ \tilde{\psi}^{-1}(v^1, \dots , v^n, \tilde{x}^1, \dots, \tilde{x}^n,)=(x^1(\tilde{x}),\dots, x^n(\tilde{x}), \frac{\partial x^1}{\partial \tilde{x}^j}(\tilde{x})v^j,\dots, \frac{\partial x^n}{\partial \tilde{x}^j}(\tilde{x})v^j) \mathbb{R}^{2n}","['multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
35,Evaluate the volume between a parabola and a sphere in high dimension,Evaluate the volume between a parabola and a sphere in high dimension,,"Consider the unit ball $\mathbb{B}^{n+1}:=\{\boldsymbol{x}\in\mathbb{R}^{n+1}:\|\boldsymbol{x}\|_2\le 1\}$ and a parabola $p:\mathbb{R}^{n}\rightarrow\mathbb{R}$ for which $p(\boldsymbol{x}):=\rho\|\boldsymbol{x}\|_2^2/2$ , where $\rho>0$ is some parameter. I would like to know how the volume of $\mathbb{B}^{n+1}\cap\operatorname{epi}(p)=\mathbb{B}^{n+1}\cap\{(\boldsymbol{x},t)\in\mathbb{R}^{n}\times\mathbb{R}:p(\boldsymbol{x})\le t\}$ scales w.r.t. $\rho$ . (Notice that when $\rho=0$ , the interested volume is exactly the half of that of $\mathbb{B}^{n+1}$ ). Thanks in advance.","Consider the unit ball and a parabola for which , where is some parameter. I would like to know how the volume of scales w.r.t. . (Notice that when , the interested volume is exactly the half of that of ). Thanks in advance.","\mathbb{B}^{n+1}:=\{\boldsymbol{x}\in\mathbb{R}^{n+1}:\|\boldsymbol{x}\|_2\le 1\} p:\mathbb{R}^{n}\rightarrow\mathbb{R} p(\boldsymbol{x}):=\rho\|\boldsymbol{x}\|_2^2/2 \rho>0 \mathbb{B}^{n+1}\cap\operatorname{epi}(p)=\mathbb{B}^{n+1}\cap\{(\boldsymbol{x},t)\in\mathbb{R}^{n}\times\mathbb{R}:p(\boldsymbol{x})\le t\} \rho \rho=0 \mathbb{B}^{n+1}","['integration', 'geometry', 'multivariable-calculus', 'volume', 'spheres']"
36,Is there a 3D version of the convolution and cross-correlation theorems for the Laplace transform?,Is there a 3D version of the convolution and cross-correlation theorems for the Laplace transform?,,"The Laplace-transform version of the convolution and cross-correlation theorems is essentially the same as the ""usual"" (Fourier-transform) version: if $\mathcal{L}[f(t)]$ is the Laplace transform of a function $f(t)$ , given by $$\mathcal{L}[f(t)]=\int_0^\infty f(t)\,e^{-st}\,\text{d}t,$$ and if $f\ast g$ is the convolution of $f$ and $g$ and $f★ g$ is the cross-correlation of $f$ and $g$ , given by $$(f\ast g)(t)=\int_0^t f(\tau)\,g(t-\tau)\,\text{d}\tau$$ and $$(f\star g)(t)=\int_0^\infty f^\ast(\tau)\,g(t+\tau)\,\text{d}\tau,$$ then $$\mathcal{L}[(f\ast g)(t)]=\mathcal{L}[f(t)]\,\mathcal{L}[g(t)]$$ and $$\mathcal{L}[(f\star g)(t)]=\mathcal{L}[f^\ast(-t^\ast)]\,\mathcal{L}[g(t)],$$ where $^\ast$ denotes complex conjugation. I'd like to know whether those also hold for functions of multiple variables (e.g. $f(t_1,t_2,t_3)$ and $g(t_1,t_2,t_3)$ ) and, if they do, whether any of the usual shenanigans involving higher-dimensional calculus occur here (e.g. whether the argument of $g$ in the convolution integral is $g(t_1-\tau_1,t_2-\tau_2,t_3-\tau_3)$ or $g(|\vec{t}-\vec{\tau}|)$ with $\vec{t}=(t_1,t_2,t_3),\vec{\tau}=(\tau_1,\tau_2,\tau_3)$ or something else, and similarly for the argument of $g$ in the correlation integral). The Wikipedia article on the convolution theorem states the theorem for the Fourier transform (as is usual as far as I'm aware), says that ""[t]he theorem also generally applies to multi-dimensional functions"" and then (in a different section) states that ""[t]his theorem also holds for the Laplace transform"", but it doesn't specifically state that the generalisation to functions of multiple variables also holds for the Laplace transform. Intuition says it should, but when I tried my hand at a simple proof I ran into a wall. Wikipedia has no article on the cross-correlation theorem, but it mentions the theorem on its article on the discrete Fourier transform has a section on the theorem (in which it doesn't mention the Laplace transform at all). The Wikipedia article on the Laplace transform has a section in which it states both theorems for single-variable functions. I haven't been able to find the required information, and especially the bit about the argument of $g$ in the integrals, anywhere.","The Laplace-transform version of the convolution and cross-correlation theorems is essentially the same as the ""usual"" (Fourier-transform) version: if is the Laplace transform of a function , given by and if is the convolution of and and is the cross-correlation of and , given by and then and where denotes complex conjugation. I'd like to know whether those also hold for functions of multiple variables (e.g. and ) and, if they do, whether any of the usual shenanigans involving higher-dimensional calculus occur here (e.g. whether the argument of in the convolution integral is or with or something else, and similarly for the argument of in the correlation integral). The Wikipedia article on the convolution theorem states the theorem for the Fourier transform (as is usual as far as I'm aware), says that ""[t]he theorem also generally applies to multi-dimensional functions"" and then (in a different section) states that ""[t]his theorem also holds for the Laplace transform"", but it doesn't specifically state that the generalisation to functions of multiple variables also holds for the Laplace transform. Intuition says it should, but when I tried my hand at a simple proof I ran into a wall. Wikipedia has no article on the cross-correlation theorem, but it mentions the theorem on its article on the discrete Fourier transform has a section on the theorem (in which it doesn't mention the Laplace transform at all). The Wikipedia article on the Laplace transform has a section in which it states both theorems for single-variable functions. I haven't been able to find the required information, and especially the bit about the argument of in the integrals, anywhere.","\mathcal{L}[f(t)] f(t) \mathcal{L}[f(t)]=\int_0^\infty f(t)\,e^{-st}\,\text{d}t, f\ast g f g f★ g f g (f\ast g)(t)=\int_0^t f(\tau)\,g(t-\tau)\,\text{d}\tau (f\star g)(t)=\int_0^\infty f^\ast(\tau)\,g(t+\tau)\,\text{d}\tau, \mathcal{L}[(f\ast g)(t)]=\mathcal{L}[f(t)]\,\mathcal{L}[g(t)] \mathcal{L}[(f\star g)(t)]=\mathcal{L}[f^\ast(-t^\ast)]\,\mathcal{L}[g(t)], ^\ast f(t_1,t_2,t_3) g(t_1,t_2,t_3) g g(t_1-\tau_1,t_2-\tau_2,t_3-\tau_3) g(|\vec{t}-\vec{\tau}|) \vec{t}=(t_1,t_2,t_3),\vec{\tau}=(\tau_1,\tau_2,\tau_3) g g","['multivariable-calculus', 'laplace-transform', 'convolution']"
37,"Find all possible values of $H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x}$ if $x, y, z, t > 0$.",Find all possible values of  if .,"H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x} x, y, z, t > 0","If $x, y, z, t > 0$ , find all possible values of $H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x}$ . How I think this can be solved: First off, note that $H$ is an homogeneous function of grade $0$ , which implies that for any point $x_1\in\mathbb{R^+}^4$ , there exists another point $x_2$ in the hypersphere $x^2+y^2+z^2+t^2=1$ which has the same image (you just have to normalize $x_1$ to find $x_2$ ). So by adding the condition $x^2+y^2+z^2+t^2=1$ you are not excluding any value from the image of the function. This may be useful to apply some kind of inequality. I think this problem can be solved by finding a maximum and a minimum value for this expression and then using the Intermediate Value Theorem to justify all values in between the maximum and minimum are in the image of the function, but I haven't succeeded finding that maximum or minimum. This exercise is right next to another one that is solved using Cauchy-Schwarz inequality, so I suspect it might have something to do with a famous inequality.","If , find all possible values of . How I think this can be solved: First off, note that is an homogeneous function of grade , which implies that for any point , there exists another point in the hypersphere which has the same image (you just have to normalize to find ). So by adding the condition you are not excluding any value from the image of the function. This may be useful to apply some kind of inequality. I think this problem can be solved by finding a maximum and a minimum value for this expression and then using the Intermediate Value Theorem to justify all values in between the maximum and minimum are in the image of the function, but I haven't succeeded finding that maximum or minimum. This exercise is right next to another one that is solved using Cauchy-Schwarz inequality, so I suspect it might have something to do with a famous inequality.","x, y, z, t > 0 H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x} H 0 x_1\in\mathbb{R^+}^4 x_2 x^2+y^2+z^2+t^2=1 x_1 x_2 x^2+y^2+z^2+t^2=1","['multivariable-calculus', 'inequality', 'maxima-minima', 'functional-inequalities']"
38,Any integral is equal to the integral of the indicator function of its graph,Any integral is equal to the integral of the indicator function of its graph,,"The following seems intuitively true to me but I’m not sure if it is true in general, and if so how to prove it. For any $T \subset \mathbb{R}^n$ and function $f: T \to \mathbb{R}$ , let $A = \{ \mathbf{x}~|~f(\mathbf{x}) > 0, \mathbf{x} \in T\}$ be where the function is positive and $B = T \setminus{A}$ where it is non-positive, and define the graphs on $A$ and $B$ to be the sets $$ S_1 = \{(\mathbf{x}, y))~|~\mathbf{x} \in A, 0 < y < f(\mathbf{x})\} $$ and $$ S_2 = \{(\mathbf{x}, y))~|~\mathbf{x} \in B, f(\mathbf{x}) < y \leq 0\} ,$$ respectively. Then we have that $$ \int_{T} f(\mathbf{x})~d^n\mathbf{x} = \int_{\mathbb{R}^{n+1}} \mathbf{1}_{S_1}~d^{n+1}\mathbf{x} - \int_{\mathbb{R}^{n+1}} \mathbf{1}_{S_2}~d^{n+1}\mathbf{x} ,$$ provided the first integral exists. Any ideas?","The following seems intuitively true to me but I’m not sure if it is true in general, and if so how to prove it. For any and function , let be where the function is positive and where it is non-positive, and define the graphs on and to be the sets and respectively. Then we have that provided the first integral exists. Any ideas?","T \subset \mathbb{R}^n f: T \to \mathbb{R} A = \{ \mathbf{x}~|~f(\mathbf{x}) > 0, \mathbf{x} \in T\} B = T \setminus{A} A B  S_1 = \{(\mathbf{x}, y))~|~\mathbf{x} \in A, 0 < y < f(\mathbf{x})\}   S_2 = \{(\mathbf{x}, y))~|~\mathbf{x} \in B, f(\mathbf{x}) < y \leq 0\} ,  \int_{T} f(\mathbf{x})~d^n\mathbf{x} = \int_{\mathbb{R}^{n+1}} \mathbf{1}_{S_1}~d^{n+1}\mathbf{x} - \int_{\mathbb{R}^{n+1}} \mathbf{1}_{S_2}~d^{n+1}\mathbf{x} ,","['real-analysis', 'multivariable-calculus']"
39,"Consider the limit $\lim_{(x,y)\to(0,0)}\frac{y^4}{x^2+y^4}.$ Where did I mistake?",Consider the limit  Where did I mistake?,"\lim_{(x,y)\to(0,0)}\frac{y^4}{x^2+y^4}.","Determine whether the limit $$\lim_{(x,y)\to(0,0)}\quad \frac{y^4}{x^2+y^4}.$$ If $(x,y)$ approaches $(0,0)$ satisfying $x=y^2$ , then $\frac{y^4}{x^2+y^4}=\frac{1}{2}\to \frac{1}{2}$ , whilst if $(x,y)$ approaches $(0,0)$ satisfying $x=0$ , then $\frac{y^4}{x^2+y^4}=1\to 1$ . Thus the limit doesn't exist. However, if I consider the polar coodinates, then $$\frac{y^4}{x^2+y^4}=\frac{r^4\sin^4 t}{r^2\cos^2 t+r^4\sin^4 t} =\frac{r^2\sin^4 t}{\cos^2 t+r^2\sin^4 t} \to 0$$ as $r\to 0.$ Thus the limit is $0$ ? There should be a mistake somewhere but I cannot find. Where did I mistake ?","Determine whether the limit If approaches satisfying , then , whilst if approaches satisfying , then . Thus the limit doesn't exist. However, if I consider the polar coodinates, then as Thus the limit is ? There should be a mistake somewhere but I cannot find. Where did I mistake ?","\lim_{(x,y)\to(0,0)}\quad \frac{y^4}{x^2+y^4}. (x,y) (0,0) x=y^2 \frac{y^4}{x^2+y^4}=\frac{1}{2}\to \frac{1}{2} (x,y) (0,0) x=0 \frac{y^4}{x^2+y^4}=1\to 1 \frac{y^4}{x^2+y^4}=\frac{r^4\sin^4 t}{r^2\cos^2 t+r^4\sin^4 t}
=\frac{r^2\sin^4 t}{\cos^2 t+r^2\sin^4 t}
\to 0 r\to 0. 0","['calculus', 'limits', 'multivariable-calculus']"
40,How do I implicitly differentiate $\frac{d}{dx}(2x(y')^2y'')$,How do I implicitly differentiate,\frac{d}{dx}(2x(y')^2y''),"I know that I will probably need to use the 3-way chain rule: $$(fgh)'=f'gh+fg'h+fgh',$$ but since I'm differentiating in terms of $x$ , I'm very confused as to how to to ""append"" $y'$ terms to the end of each term in the product rule expansion. For instance, I believe $\frac{d}{dx}(xy)=y+xy'(y')$ , where I have ""appended"" a $y'$ to the latter term. This makes sense from the (informal) ""fraction analogy"" in Leibniz notation: $$\frac{d}{dx}=\frac{d}{dy}\cdot\frac{dy}{dx},$$ but I have trouble generalising to more complicated examples like the one mentioned above. It would be much appreciated if someone can write out the full solution or explain how they got it. Note: $y=y(x)$ (is this noteworthy?)","I know that I will probably need to use the 3-way chain rule: but since I'm differentiating in terms of , I'm very confused as to how to to ""append"" terms to the end of each term in the product rule expansion. For instance, I believe , where I have ""appended"" a to the latter term. This makes sense from the (informal) ""fraction analogy"" in Leibniz notation: but I have trouble generalising to more complicated examples like the one mentioned above. It would be much appreciated if someone can write out the full solution or explain how they got it. Note: (is this noteworthy?)","(fgh)'=f'gh+fg'h+fgh', x y' \frac{d}{dx}(xy)=y+xy'(y') y' \frac{d}{dx}=\frac{d}{dy}\cdot\frac{dy}{dx}, y=y(x)","['calculus', 'multivariable-calculus', 'derivatives', 'implicit-differentiation']"
41,Order of the volume growth of logarithmic spherical wedge,Order of the volume growth of logarithmic spherical wedge,,"I'm trying to find the volume growth of a wedge cut from a sphere in $\mathbb{R}^3$ . In particular the constraints are: $$-log(z+1)^2 \leq x \leq log(z+1)^2,$$ $$-z \leq y \leq z,$$ $$x^2+y^2+z^2\leq r^2.$$ I'm expecting the volume to be of order $\Omega(r^2)$ but I'm struggling with the computation. The problem where $x$ is bounded by a general smooth, positive, increasing function of $z$ is also of interest. ie $$-f(z) \leq x \leq f(z),$$ $$-z \leq y \leq z,$$ $$x^2+y^2+z^2\leq r^2.$$ I wonder what's an useful technique to deal with these computations?","I'm trying to find the volume growth of a wedge cut from a sphere in . In particular the constraints are: I'm expecting the volume to be of order but I'm struggling with the computation. The problem where is bounded by a general smooth, positive, increasing function of is also of interest. ie I wonder what's an useful technique to deal with these computations?","\mathbb{R}^3 -log(z+1)^2 \leq x \leq log(z+1)^2, -z \leq y \leq z, x^2+y^2+z^2\leq r^2. \Omega(r^2) x z -f(z) \leq x \leq f(z), -z \leq y \leq z, x^2+y^2+z^2\leq r^2.","['multivariable-calculus', 'asymptotics', 'volume']"
42,Help finding extrema of a function using lagrange multipliers,Help finding extrema of a function using lagrange multipliers,,"I am trying to find the extreme values of the function $f(x,y,z)=x^2+y^2+z^2$ given the constraints $g(x,y,z)=x-y=1$ and $h(x,y,z)=y^2-z^2=1$ . I am rather lost on this question as it feels as though I am taking the right steps but keep coming to impossible solutions. Here is my current work: $$\nabla f = \begin{bmatrix}2x\\ 2y\\ 2z\end{bmatrix}$$ $$\nabla g = \begin{bmatrix}1\\ -1\\ 0\end{bmatrix}$$ $$\nabla h = \begin{bmatrix}0\\ 2y\\ -2z\end{bmatrix}$$ Let $2z = -2bz\implies b = -1$ , and $a=2x$ ; we have $$2y = -a + 2by$$ $$2y = -a - 2y$$ $$4y = -a$$ $$2x = -4y$$ $$x = -2y$$ $x - y = 1\implies$ $$-3y = 1 \implies \begin{cases} y = -\frac{1}{3} \\ x = \frac23\end{cases} $$ Substituting this into $$y^2 - z^2 = 1$$ Gives us $$z^2 = -\frac89$$ A squared value cannot be negative (complex numbers don't make sense in this case). I don't understand where I have gone wrong. I would like to know where I might've gone wrong.","I am trying to find the extreme values of the function given the constraints and . I am rather lost on this question as it feels as though I am taking the right steps but keep coming to impossible solutions. Here is my current work: Let , and ; we have Substituting this into Gives us A squared value cannot be negative (complex numbers don't make sense in this case). I don't understand where I have gone wrong. I would like to know where I might've gone wrong.","f(x,y,z)=x^2+y^2+z^2 g(x,y,z)=x-y=1 h(x,y,z)=y^2-z^2=1 \nabla f = \begin{bmatrix}2x\\ 2y\\ 2z\end{bmatrix} \nabla g = \begin{bmatrix}1\\ -1\\ 0\end{bmatrix} \nabla h = \begin{bmatrix}0\\ 2y\\ -2z\end{bmatrix} 2z = -2bz\implies b = -1 a=2x 2y = -a + 2by 2y = -a - 2y 4y = -a 2x = -4y x = -2y x - y = 1\implies -3y = 1 \implies \begin{cases} y = -\frac{1}{3} \\ x = \frac23\end{cases}  y^2 - z^2 = 1 z^2 = -\frac89","['calculus', 'multivariable-calculus', 'solution-verification', 'lagrange-multiplier']"
43,Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function $F:\mathbb{R}^3\to\mathbb{R}$?,Is  true for any  function ?,F_xF_{zy}=F_yF_{zx}  C^{\infty} F:\mathbb{R}^3\to\mathbb{R},"Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function $F:\mathbb{R}^3\to\mathbb{R}$ ? What if we add the assumption $F_z\ne 0$ ? Motivation: If $F:\mathbb{R}^3\to\mathbb{R}$ is $C^\infty$ , and at $(x_0,y_0,z_0)$ , $F_z\ne 0$ , then in a neighbourhood of $(x_0,y_0,z_0)$ , $F(x,y,z)=F(x_0,y_0,z_0)$ gives a function $z=z(x,y)$ s.t. $F(x,y,z(x,y))=F(x_0,y_0,z_0)$ . We have $$z_{yx}=(z_x)_y\ \  \underbrace{=}_{?}\ \  \left(-\frac{F_x}{F_z}\right)_y=-\frac{F_{yx}F_z-F_xF_{yz}}{F_z^2}$$ ( Edit : The underbraced equality is incorrect, since $z_y=-\frac{F_y}{F_z}$ is not true for all $(x,y,z)\in\mathbb{R}$ .) Symmetrically we have $$z_{xy}=-\frac{F_{xy}F_z-F_yF_{xz}}{F_z^2} $$ Since $F_{xy}=F_{yx}$ and $z_{yx}=z_{xy}$ we yield $$F_xF_{zy}=F_yF_{zx}. $$ However, the above argument assumes $F'z\ne 0$ , and it is very indirect. Is $F_xF_{zy}=F_yF_{zx} $ true for any $C^{\infty}$ function? Do we have a direct proof?","Is true for any function ? What if we add the assumption ? Motivation: If is , and at , , then in a neighbourhood of , gives a function s.t. . We have ( Edit : The underbraced equality is incorrect, since is not true for all .) Symmetrically we have Since and we yield However, the above argument assumes , and it is very indirect. Is true for any function? Do we have a direct proof?","F_xF_{zy}=F_yF_{zx}  C^{\infty} F:\mathbb{R}^3\to\mathbb{R} F_z\ne 0 F:\mathbb{R}^3\to\mathbb{R} C^\infty (x_0,y_0,z_0) F_z\ne 0 (x_0,y_0,z_0) F(x,y,z)=F(x_0,y_0,z_0) z=z(x,y) F(x,y,z(x,y))=F(x_0,y_0,z_0) z_{yx}=(z_x)_y\ \  \underbrace{=}_{?}\ \  \left(-\frac{F_x}{F_z}\right)_y=-\frac{F_{yx}F_z-F_xF_{yz}}{F_z^2} z_y=-\frac{F_y}{F_z} (x,y,z)\in\mathbb{R} z_{xy}=-\frac{F_{xy}F_z-F_yF_{xz}}{F_z^2}  F_{xy}=F_{yx} z_{yx}=z_{xy} F_xF_{zy}=F_yF_{zx}.  F'z\ne 0 F_xF_{zy}=F_yF_{zx}  C^{\infty}","['calculus', 'multivariable-calculus']"
44,Convergence in an average integral,Convergence in an average integral,,"Let $f(\theta, \epsilon)$ be smooth on $[0,2\pi] \times [0,\infty)$ , $f(\theta, \epsilon)>0$ for $\epsilon>0$ , and $f(\theta, 0)=(\theta -\theta^*)^2h(\theta)$ with $h(\theta)>0$ for all $\theta \in [0,2\pi)$ . It is shown in this post that, for $\theta^*\neq 0$ , $\lim _{\epsilon \rightarrow 0}\theta(\epsilon)=\theta^*$ , where $$\theta(\epsilon):=\int_0^{2\pi}\frac{\theta }{f(\theta, \epsilon)}d\theta (\int_0^{2\pi}\frac{1}{f(\theta, \epsilon)  d \theta})^{-1}.$$ Now let $$\alpha(\epsilon):=f(\theta(\epsilon), \epsilon), \ \ and \ \  \beta(\epsilon) :=\min_{\theta \in [0,2\pi]}f(\theta, \epsilon).$$ Both $\alpha(\epsilon)$ and $\beta(\epsilon)$ converge to zero as $\epsilon \rightarrow 0$ with $\beta(\epsilon) \leq \alpha (\epsilon)$ . For $\theta^* \neq 0$ , does there exists a function $g:\mathbb{R}\rightarrow \mathbb{R}$ such that $g(0)=0$ , and $g(s)>0$ for $s\neq 0$ such that $$\lim_{\epsilon \rightarrow 0} \frac{g(\alpha (\epsilon))}{\beta(\epsilon)}=0?$$","Let be smooth on , for , and with for all . It is shown in this post that, for , , where Now let Both and converge to zero as with . For , does there exists a function such that , and for such that","f(\theta, \epsilon) [0,2\pi] \times [0,\infty) f(\theta, \epsilon)>0 \epsilon>0 f(\theta, 0)=(\theta -\theta^*)^2h(\theta) h(\theta)>0 \theta \in [0,2\pi) \theta^*\neq 0 \lim _{\epsilon \rightarrow 0}\theta(\epsilon)=\theta^* \theta(\epsilon):=\int_0^{2\pi}\frac{\theta }{f(\theta, \epsilon)}d\theta (\int_0^{2\pi}\frac{1}{f(\theta, \epsilon)  d \theta})^{-1}. \alpha(\epsilon):=f(\theta(\epsilon), \epsilon), \ \ and \ \  \beta(\epsilon) :=\min_{\theta \in [0,2\pi]}f(\theta, \epsilon). \alpha(\epsilon) \beta(\epsilon) \epsilon \rightarrow 0 \beta(\epsilon) \leq \alpha (\epsilon) \theta^* \neq 0 g:\mathbb{R}\rightarrow \mathbb{R} g(0)=0 g(s)>0 s\neq 0 \lim_{\epsilon \rightarrow 0} \frac{g(\alpha (\epsilon))}{\beta(\epsilon)}=0?","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'multivariable-calculus']"
45,Line Integral along a Ellipse over a Scalar field,Line Integral along a Ellipse over a Scalar field,,"Given is the Line Integral: $$\int_C = \sqrt{\frac{a^2y^2}{b^2} + \frac{b^2x^2}{a^2}}ds$$ the Path $C$ is along the border of the ellipse with: $$\frac{x^2}{a^2}+\frac{y^2}{b^2} = 1$$ and moves against Clockwise. My attampt: $$\int_C f(x,y) ds = \int_{t_1}^{t_2} f(x(t),y(t)) ||\frac{dr(t)}{dt}|| dt$$ with $$r(t) = (a\cos(t),b\sin(t)) \implies r'(t) = (-a\sin(t), b\cos(t)) \ \ \ \ t\in [0,2\pi]$$ Plugging this in and simplify comes out to: $$I = \int_{0}^{2\pi} (a^2\sin(t)^2+b^2\cos(t)^2) dt$$ Now i am kinda stuck. I did not use the restricting condition of the ellipse yet. I tried to solve this restriction for either $a$ or $b$ and the plug it in, but this did not really work. Thank you for answering in advance.","Given is the Line Integral: the Path is along the border of the ellipse with: and moves against Clockwise. My attampt: with Plugging this in and simplify comes out to: Now i am kinda stuck. I did not use the restricting condition of the ellipse yet. I tried to solve this restriction for either or and the plug it in, but this did not really work. Thank you for answering in advance.","\int_C = \sqrt{\frac{a^2y^2}{b^2} + \frac{b^2x^2}{a^2}}ds C \frac{x^2}{a^2}+\frac{y^2}{b^2} = 1 \int_C f(x,y) ds = \int_{t_1}^{t_2} f(x(t),y(t)) ||\frac{dr(t)}{dt}|| dt r(t) = (a\cos(t),b\sin(t)) \implies r'(t) = (-a\sin(t), b\cos(t)) \ \ \ \ t\in [0,2\pi] I = \int_{0}^{2\pi} (a^2\sin(t)^2+b^2\cos(t)^2) dt a b","['integration', 'multivariable-calculus', 'line-integrals']"
46,Poincaré–Miranda \ Intermediate Value theorem for mappings from $\mathbb{R}^n$ to $\mathbb{R}^m$,Poincaré–Miranda \ Intermediate Value theorem for mappings from  to,\mathbb{R}^n \mathbb{R}^m,"currently, I am looking for the existence of zeros of a vector valued function $\vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m$ . For my specific problem, $n=2$ and $m=6$ (The dimensions could change as I make changes to function $f$ ). I know that I can expect a continuity of the function, but not continuous derivability due to the underlying physics. I saw that there is a theorem called Poincaré–Miranda or Intermediate Value Theorem, which basically tells me that If I have $\vec{f}(\vec{x}_1) \leq \vec{0} \leq \vec{f}(\vec{x}_2)$ that there is a $\vec{f}(\vec{x_3}) = \vec{0}$ . From the related Wikpedia entry it seems that this only holds for projections to the same dimensional space $\mathbb{R}^n \rightarrow \mathbb{R}^n$ . Question 1 Is there an extension of Poincaré–Miranda Theorem to functions $\vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m$ ? Question 2 What if I artificially make my projection fulfill the requirements for the Poincaré–Miranda Theorem? E.g.: I have a function $\vec{f}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x}))$ . I know that both $f_1$ and $f_2$ are smooth. Then $f_3(\vec{x}) = f_1(\vec{x})+f_2(\vec{x})$ is also smooth. Also for $f_1(\vec{x_a}) \geq 0$ and $f_2(\vec{x_a}) \geq 0$ it follows that $f_3(\vec{x_a}) \geq 0$ and vice versa for $f_1(\vec{x_b}) \leq 0$ and $f_2(\vec{x_b}) \leq 0$ . Hence, the theorem should be applicable to the new function $\vec{f_{new}}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x}), f_3(\vec{x}))$ . Am I missing something obvious here? Thanks :)","currently, I am looking for the existence of zeros of a vector valued function . For my specific problem, and (The dimensions could change as I make changes to function ). I know that I can expect a continuity of the function, but not continuous derivability due to the underlying physics. I saw that there is a theorem called Poincaré–Miranda or Intermediate Value Theorem, which basically tells me that If I have that there is a . From the related Wikpedia entry it seems that this only holds for projections to the same dimensional space . Question 1 Is there an extension of Poincaré–Miranda Theorem to functions ? Question 2 What if I artificially make my projection fulfill the requirements for the Poincaré–Miranda Theorem? E.g.: I have a function . I know that both and are smooth. Then is also smooth. Also for and it follows that and vice versa for and . Hence, the theorem should be applicable to the new function . Am I missing something obvious here? Thanks :)","\vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m n=2 m=6 f \vec{f}(\vec{x}_1) \leq \vec{0} \leq \vec{f}(\vec{x}_2) \vec{f}(\vec{x_3}) = \vec{0} \mathbb{R}^n \rightarrow \mathbb{R}^n \vec{f}(\vec{x}): \mathbb{R}^n \rightarrow \mathbb{R}^m \vec{f}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x})) f_1 f_2 f_3(\vec{x}) = f_1(\vec{x})+f_2(\vec{x}) f_1(\vec{x_a}) \geq 0 f_2(\vec{x_a}) \geq 0 f_3(\vec{x_a}) \geq 0 f_1(\vec{x_b}) \leq 0 f_2(\vec{x_b}) \leq 0 \vec{f_{new}}(\vec{x}): (x_1,x_2,x_3) \rightarrow (f_1(\vec{x}), f_2(\vec{x}), f_3(\vec{x}))","['multivariable-calculus', 'continuity', 'roots']"
47,Integration of connected variable,Integration of connected variable,,"In my research, I am trying to Model an Ebby Current Braking system based on W. R. Smythe work ( On Eddy Currents in a Rotating Disk ) In this paper the Eddy Currents Torque can be estimated using the following integration: $$ T = \frac{\omega c b \gamma \Phi^2}{\pi^2 a^4} \times \int_{c-a}^{c+a}  \left( r^2 \sin{\theta_1} - \frac{a^2 A^2 r^2 \sin{\theta_1}}{c^2 r^2 + A^4 - 2 A^2 r c \cos{\theta_1}} \right) dr $$ where $\theta_1$ and $r$ are connected by the relation: $$ r^2 + c^2 — 2 r c \cos{\theta_1} = a^2 $$ Note that all parameters are constants with the exception of $\theta_1$ and $r$ So, I need some help in at least how the first term ( $r^2 \sin(\theta_1)$ ) is integrated. My field is Mechanical Engineering and I think that I do not have enough mathematical background when it comes to integration of connected variables.","In my research, I am trying to Model an Ebby Current Braking system based on W. R. Smythe work ( On Eddy Currents in a Rotating Disk ) In this paper the Eddy Currents Torque can be estimated using the following integration: where and are connected by the relation: Note that all parameters are constants with the exception of and So, I need some help in at least how the first term ( ) is integrated. My field is Mechanical Engineering and I think that I do not have enough mathematical background when it comes to integration of connected variables.","
T = \frac{\omega c b \gamma \Phi^2}{\pi^2 a^4} \times \int_{c-a}^{c+a} 
\left( r^2 \sin{\theta_1} - \frac{a^2 A^2 r^2 \sin{\theta_1}}{c^2 r^2 + A^4 - 2 A^2 r c \cos{\theta_1}} \right) dr
 \theta_1 r 
r^2 + c^2 — 2 r c \cos{\theta_1} = a^2
 \theta_1 r r^2 \sin(\theta_1)","['calculus', 'integration', 'multivariable-calculus']"
48,Implicit function theorem / Implicit selections when Jacobian not invertible,Implicit function theorem / Implicit selections when Jacobian not invertible,,"I saw the attached result in the book by Dontchev and Rockafellar. It requires the Jacobian to be of full rank m. I suspect this condition can be further relaxed. Assume that we know that the columns of $\nabla_p f(\bar p, \bar x)$ are in the  column space of A. Couldn't we conclude that $$ \nabla s(\bar p) = A^\dagger \nabla_p f(\bar p, \bar x), $$ where $\dagger$ represents the Moore-Penrose pseudo inverse. If true, is there a good reference I can read to learn about such a result and its extensions?","I saw the attached result in the book by Dontchev and Rockafellar. It requires the Jacobian to be of full rank m. I suspect this condition can be further relaxed. Assume that we know that the columns of are in the  column space of A. Couldn't we conclude that where represents the Moore-Penrose pseudo inverse. If true, is there a good reference I can read to learn about such a result and its extensions?","\nabla_p f(\bar p, \bar x) 
\nabla s(\bar p) = A^\dagger \nabla_p f(\bar p, \bar x),
 \dagger","['real-analysis', 'multivariable-calculus', 'differential-forms', 'implicit-function-theorem']"
49,Multivariable Calc. Integration by Parts,Multivariable Calc. Integration by Parts,,$$ \iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} - \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S} $$ Is anything wrong with this. My textbook says it should be $$ \iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} + \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S} $$ Which of the two is correct? I used integration by parts and Stoke's theorem.,Is anything wrong with this. My textbook says it should be Which of the two is correct? I used integration by parts and Stoke's theorem.,"
\iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} - \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S}
 
\iint_S f(\nabla \times \mathbf{A}) \cdot d\mathbf{S} = \oint_{\partial S} f \mathbf{A} \cdot d\mathbf{r} + \iint_S \mathbf{A} \times (\nabla f) \cdot d\mathbf{S}
",['multivariable-calculus']
50,"How to understand Euler's criterion for exactness of differential equation as presented in the book ""Physical chemistry"" by Silbey, Alberty, Bawendy?","How to understand Euler's criterion for exactness of differential equation as presented in the book ""Physical chemistry"" by Silbey, Alberty, Bawendy?",,"While reading a chapter about thermodynamics in a book, I encountered a small section on exact and inexact differentials. Since the book is a chemistry book, I am not sure if the math details/assumptions are being partially omitted from the reader. Says the book, There is a simple test to see whether a differential is exact. For a system with just two independent degrees of freedom, the total differential $dz$ of a quantity $z$ may be determined by the differentials $dx$ and $dy$ in two other quantities $x$ and $y$ . In general, $$dz=M(x,y)dx+N(x,y)dy\tag{2.17}$$ where $M$ and $N$ are functions of the independent variables $x$ and $y$ . To show the test for exactness we now consider a function $z$ that has an exact differential. If $z$ has a definite value at each point in the $xy$ -plane, then it must be a function of $x$ and $y$ . If $z=f(x,y)$ , then $$dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left  (\frac{\partial z}{\partial y}\right )_x dy\tag{2.18}$$ Comparing (2.17) and (2.18) we find $$M(x,y)=\left (\frac{\partial z}{\partial x}\right )_y\tag{2.19}$$ $$N(x,y)=\left (\frac{\partial z}{\partial y}\right )_x\tag{2.20}$$ Since the mixed partial derivatives are equal, $$\left [ \frac{\partial}{\partial y} \left (\frac{\partial  z}{\partial x}\right )_y\right ]=\left [ \frac{\partial}{\partial x}  \left (\frac{\partial z}{\partial y}\right )_x \right ]\tag{2.21}$$ then $$\left (\frac{\partial M}{\partial y}\right )_x=\left (\frac{\partial  N}{\partial y}\right )_y\tag{2.22}$$ This equation must be satisfied if $dz$ is an exact differential. It is Euler's criterion for exactness. Here is what I think is being said, in my own words and with more assumptions made explicit. (2.17) denotes a general differential. We don't know if it is exact or not. We wish to find some test that enables us to determine if this differential equation is exact or not. Then we are given a function $z=z(x,y)$ that has, by assumption, an exact differential (equivalently, is differentiable). The exact (total) differential of $z$ is $$dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left (\frac{\partial z}{\partial y}\right )_x dy\tag{2.18}$$ It is stated that the mixed partial derivatives of $z$ are equal. This occurs if $D_1 z$ , $D_2 z$ , and one of the mixed partial derivatives exist and, in addition, the mixed partial derivative is continuous in the open set $S$ that contains the point in consideration in this analysis. This is implicit in the snippet. The snippet contains two conditions for the functions $M(x,y)$ and $N(x,y)$ . $M$ and $N$ are the partial derivatives of some function (called $z$ as well, but I am going to call it $\varphi$ ) The mixed partials of $\varphi$ are equal. Euler's criterion for exactness seems to be stated as being requirement 2). As far as I know, given $M$ and $N$ , if all we know is that $D_2 M=D_1 N$ , then all we have is a necessary condition for $\langle M, N \rangle$ to be a gradient on an open set $S$ (unless the set $S$ is convex, in which case we have a sufficient condition). My questions are: is my interpretation correct? Also, the way the book presents all of this, doesn't 2) require 1) to be true? Shouldn't Euler's criterion be 1) and 2) together? What is Euler's criterion for exactness after all?","While reading a chapter about thermodynamics in a book, I encountered a small section on exact and inexact differentials. Since the book is a chemistry book, I am not sure if the math details/assumptions are being partially omitted from the reader. Says the book, There is a simple test to see whether a differential is exact. For a system with just two independent degrees of freedom, the total differential of a quantity may be determined by the differentials and in two other quantities and . In general, where and are functions of the independent variables and . To show the test for exactness we now consider a function that has an exact differential. If has a definite value at each point in the -plane, then it must be a function of and . If , then Comparing (2.17) and (2.18) we find Since the mixed partial derivatives are equal, then This equation must be satisfied if is an exact differential. It is Euler's criterion for exactness. Here is what I think is being said, in my own words and with more assumptions made explicit. (2.17) denotes a general differential. We don't know if it is exact or not. We wish to find some test that enables us to determine if this differential equation is exact or not. Then we are given a function that has, by assumption, an exact differential (equivalently, is differentiable). The exact (total) differential of is It is stated that the mixed partial derivatives of are equal. This occurs if , , and one of the mixed partial derivatives exist and, in addition, the mixed partial derivative is continuous in the open set that contains the point in consideration in this analysis. This is implicit in the snippet. The snippet contains two conditions for the functions and . and are the partial derivatives of some function (called as well, but I am going to call it ) The mixed partials of are equal. Euler's criterion for exactness seems to be stated as being requirement 2). As far as I know, given and , if all we know is that , then all we have is a necessary condition for to be a gradient on an open set (unless the set is convex, in which case we have a sufficient condition). My questions are: is my interpretation correct? Also, the way the book presents all of this, doesn't 2) require 1) to be true? Shouldn't Euler's criterion be 1) and 2) together? What is Euler's criterion for exactness after all?","dz z dx dy x y dz=M(x,y)dx+N(x,y)dy\tag{2.17} M N x y z z xy x y z=f(x,y) dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left
 (\frac{\partial z}{\partial y}\right )_x dy\tag{2.18} M(x,y)=\left (\frac{\partial z}{\partial x}\right )_y\tag{2.19} N(x,y)=\left (\frac{\partial z}{\partial y}\right )_x\tag{2.20} \left [ \frac{\partial}{\partial y} \left (\frac{\partial
 z}{\partial x}\right )_y\right ]=\left [ \frac{\partial}{\partial x}
 \left (\frac{\partial z}{\partial y}\right )_x \right ]\tag{2.21} \left (\frac{\partial M}{\partial y}\right )_x=\left (\frac{\partial
 N}{\partial y}\right )_y\tag{2.22} dz z=z(x,y) z dz=\left (\frac{\partial z}{\partial x}\right )_ydx +\left
(\frac{\partial z}{\partial y}\right )_x dy\tag{2.18} z D_1 z D_2 z S M(x,y) N(x,y) M N z \varphi \varphi M N D_2 M=D_1 N \langle M, N \rangle S S","['ordinary-differential-equations', 'multivariable-calculus']"
51,Derivative comparison: direct formula vs Jacobian,Derivative comparison: direct formula vs Jacobian,,"Let's use the following defininition of the derivative (Hubbard, 5th ed.) Let $U \subset \mathbb R^n$ be open and let $f:U \to R^m$ ; let $a \in U$ , $ h \in \mathbb R^n$ If there exists a linear transformation $[Df(a)]:\mathbb R^n \to R^m$ such that $$\lim_{h \to [0]} {( f(a+h) - f(a) - [Df(a)]h ) \over {|h|}} =[0]$$ then $f$ is differentiable at $a$ and $[Df(a)]$ is unique derivative of $f$ at $a$ . Now, let's solve the following task: let $ A = \begin{bmatrix} a & b \\\\ c & d \end{bmatrix} $ . Compute the derivative for the function $f(A) = A^{-1}$ Remark: I'll be using 2x2 matrix -> 4x1 vector transformation by traversing top left, top right, bottom left, bottom right. If we use direct definition of the derivative, and come up with the $[Df(a)]H = -A^{-1}HA^{-1}$ , where $H = \begin{bmatrix} h_1 & h_2 \\\\ h_3 & h_4 \end{bmatrix} \to [0]$ But there derivative times H is wired with H: $-A^{-1}HA^{-1}$ . Moreover, this derivative times H is a $2 \times 2$ matrix, so it has 4 entries in total: $$ {1 \over (ad-bc)^2} \begin{bmatrix} -bch_4 +bdh_3 +cdh_2 -d^2 h_1 & abh_4-adh_2-b^2 h_3+bdh_1 \\\\ ach_4-adh_3-c^2 h_2+cdh_1 & -a^2 h_4+abh_3+ach_2-bch_1 \end{bmatrix} $$ Now, another approach giving pure derivative by computing Jacobian. If we interpret $A$ as vector of 4 entries, then $A^{-1}$ is also vector of 4 entries. So $$[Df(A)] = -{1 \over (ad-bc)^2} \begin{bmatrix} d^2 & -cd & -db & bc \\\\ -bd & ad & b^2 & -ab \\\\ -dc & c^2 & ad & -ac \\\\ bc & -ac & -ab & a^2 \end{bmatrix}$$ What I see is: if we factor out ""-"" from the first approach matrix, we'll get top left element of first approach matrix coeffs equal to first row, top right - to second row, bottom left - to 3rd, and bottom right - to 4th. So we factor out ""-"", then each item of resulting first-approach matrix is dot product between some vector and H: $$ -{1 \over (ad-bc)^2} \begin{bmatrix} d^2 \\\\ -cd \\\\ -bd \\\\ bc  \\\\                  -bd \\\\ ad  \\\\ b^2 \\\\ -ab \\\\                  -cd \\\\ c^2 \\\\ ad  \\\\ -ac \\\\                 bc  \\\\ -ac \\\\ -ab \\\\ a^2 \end{bmatrix} \bullet \begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\                  h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\                  h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\                  h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \end{bmatrix} $$ Now we rearrange first vector into 4x4 matrix and we're done. Now, my questions are: Even though I matched direct and Jacobian approaches, I just guessed. As I calculated Jacobian as well and I knew what I had to match and how to get from direct to Jacobian. But what If I didn't have Jacobian? Would my way of factoring out H work?(I think it will, as D is linear, but just to make sure) while meaning of $[Df(a)]$ is clear to me, what is meaning or intuitive interpretation of $[Df(a)]H$ (to me, this meaning now is given any arbitrary H it will give good enough approximation to $f(A+H) - f(A)$ . But as $H \to [0]$ , lim of $[Df(a)]H = [0]$ which makes no sense to me...","Let's use the following defininition of the derivative (Hubbard, 5th ed.) Let be open and let ; let , If there exists a linear transformation such that then is differentiable at and is unique derivative of at . Now, let's solve the following task: let . Compute the derivative for the function Remark: I'll be using 2x2 matrix -> 4x1 vector transformation by traversing top left, top right, bottom left, bottom right. If we use direct definition of the derivative, and come up with the , where But there derivative times H is wired with H: . Moreover, this derivative times H is a matrix, so it has 4 entries in total: Now, another approach giving pure derivative by computing Jacobian. If we interpret as vector of 4 entries, then is also vector of 4 entries. So What I see is: if we factor out ""-"" from the first approach matrix, we'll get top left element of first approach matrix coeffs equal to first row, top right - to second row, bottom left - to 3rd, and bottom right - to 4th. So we factor out ""-"", then each item of resulting first-approach matrix is dot product between some vector and H: Now we rearrange first vector into 4x4 matrix and we're done. Now, my questions are: Even though I matched direct and Jacobian approaches, I just guessed. As I calculated Jacobian as well and I knew what I had to match and how to get from direct to Jacobian. But what If I didn't have Jacobian? Would my way of factoring out H work?(I think it will, as D is linear, but just to make sure) while meaning of is clear to me, what is meaning or intuitive interpretation of (to me, this meaning now is given any arbitrary H it will give good enough approximation to . But as , lim of which makes no sense to me...","U \subset \mathbb R^n f:U \to R^m a \in U  h \in \mathbb R^n [Df(a)]:\mathbb R^n \to R^m \lim_{h \to [0]} {( f(a+h) - f(a) - [Df(a)]h ) \over {|h|}} =[0] f a [Df(a)] f a  A = \begin{bmatrix} a & b \\\\
c & d
\end{bmatrix}
 f(A) = A^{-1} [Df(a)]H = -A^{-1}HA^{-1} H = \begin{bmatrix} h_1 & h_2 \\\\
h_3 & h_4
\end{bmatrix} \to [0] -A^{-1}HA^{-1} 2 \times 2  {1 \over (ad-bc)^2}
\begin{bmatrix} -bch_4 +bdh_3 +cdh_2 -d^2 h_1 & abh_4-adh_2-b^2 h_3+bdh_1 \\\\
ach_4-adh_3-c^2 h_2+cdh_1 & -a^2 h_4+abh_3+ach_2-bch_1
\end{bmatrix}
 A A^{-1} [Df(A)] = -{1 \over (ad-bc)^2} \begin{bmatrix} d^2 & -cd & -db & bc \\\\
-bd & ad & b^2 & -ab \\\\
-dc & c^2 & ad & -ac \\\\
bc & -ac & -ab & a^2
\end{bmatrix} 
-{1 \over (ad-bc)^2}
\begin{bmatrix} d^2 \\\\ -cd \\\\ -bd \\\\ bc  \\\\ 
                -bd \\\\ ad  \\\\ b^2 \\\\ -ab \\\\ 
                -cd \\\\ c^2 \\\\ ad  \\\\ -ac \\\\
                bc  \\\\ -ac \\\\ -ab \\\\ a^2
\end{bmatrix} \bullet \begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4  \\\\ 
                h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4
 \end{bmatrix}
 [Df(a)] [Df(a)]H f(A+H) - f(A) H \to [0] [Df(a)]H = [0]",['multivariable-calculus']
52,Calculating the volume of $(x-z)^2+(y-z)^2 \le \sin^2(z) $,Calculating the volume of,(x-z)^2+(y-z)^2 \le \sin^2(z) ,I am trying to calculate the volume of $(x-z)^2+(y-z)^2 \le \sin^2(z) $ and $0 \le z \le \pi $ . I am having trouble parameterising the equation and setting the bounds for the parameters. I think the $-z$ in the equation is just rescaling the object so it does not affect the volume. I have set the volume integral as $$ \pi \int_{0}^{\pi}\sin^2(z) dz$$ and found the volume as $\frac{\pi^2}{2} $ . Is this the correct answer or is my calculation wrong? Thank you very much!,I am trying to calculate the volume of and . I am having trouble parameterising the equation and setting the bounds for the parameters. I think the in the equation is just rescaling the object so it does not affect the volume. I have set the volume integral as and found the volume as . Is this the correct answer or is my calculation wrong? Thank you very much!,(x-z)^2+(y-z)^2 \le \sin^2(z)  0 \le z \le \pi  -z  \pi \int_{0}^{\pi}\sin^2(z) dz \frac{\pi^2}{2} ,"['multivariable-calculus', 'volume', 'cylindrical-coordinates']"
53,Local extremum of multivariable functions,Local extremum of multivariable functions,,"Consider $C^\infty$ functions. For functions of one variable, we know that in general, at $x=c$ if the lowest order nonzero derivative is an even order say $2k$ , then $f(c)$ is an extremum, depending on the sign of $f^{(2k)}(c)$ . For example, if $f'(c)=f''(c)=f'''(c)=0$ and $f''''(c)>0$ , then it is a minimum; if $f'(c)=f''(c)=0$ , $f'''(c)\neq 0$ , then it is not a local extremum. Now for multivariable functions. We know the theory of Hessian matrix and eigenvalues. What if some of the second derivatives are zero? For example, consider $f(x,y)$ at $(a,b)$ : $f_x=f_y=f_{xx}=f_{xy}=0$ . Apparently if $f_{xxx}\neq 0$ the function cannot have a local extremum there, as the trend along the $x$ -axis is monotone. Suppose $f_{xxxx},f_{yy}$ do not vanish. Should we examine the eigenvalues of \begin{equation} H=\begin{bmatrix} f_{xxxx} & f_{xxy}\\ f_{xxy} & f_{yy}\end{bmatrix}? \end{equation} What is the general theory about it? Any reference on this? thanks!","Consider functions. For functions of one variable, we know that in general, at if the lowest order nonzero derivative is an even order say , then is an extremum, depending on the sign of . For example, if and , then it is a minimum; if , , then it is not a local extremum. Now for multivariable functions. We know the theory of Hessian matrix and eigenvalues. What if some of the second derivatives are zero? For example, consider at : . Apparently if the function cannot have a local extremum there, as the trend along the -axis is monotone. Suppose do not vanish. Should we examine the eigenvalues of What is the general theory about it? Any reference on this? thanks!","C^\infty x=c 2k f(c) f^{(2k)}(c) f'(c)=f''(c)=f'''(c)=0 f''''(c)>0 f'(c)=f''(c)=0 f'''(c)\neq 0 f(x,y) (a,b) f_x=f_y=f_{xx}=f_{xy}=0 f_{xxx}\neq 0 x f_{xxxx},f_{yy} \begin{equation}
H=\begin{bmatrix} f_{xxxx} & f_{xxy}\\ f_{xxy} & f_{yy}\end{bmatrix}?
\end{equation}",['multivariable-calculus']
54,Uniformly pick a sphere's center...,Uniformly pick a sphere's center...,,"Let's pick the point of the center of a sphere on a 3-d axis the $x$ coordinate, $y$ coordinate, and $z$ coordinate of the sphere's center, $(x, y, z)$ are each separately from a uniform distribution $[0, 1] $ From these coordinates, we make a sphere with radius $1$ . What is the percent of the surface area (on average) that lies within the cube region with vertices at: $(0,0,1), (1,0,1), (0,1,1), (1,1,1),$ $(0,0,2), (1,0,2), (0,1,2), (1,1,2)$ So my first thought was to just assume that $x,y,z$ are each $0.5$ since that is the expected value of each (or the average) and then calculate it from there, but I know that that logic is flawed because of monte carlo simulations- although I can't figure out why. Also, I know that the absolute upper bound would be $1/6$ as the percent of the surface area that is in the cube region above it's center on average should be the same percent of the surface area that is in the cube region below and likewise the the $4$ around it as well by symmetry. It cannot equal $1/6$ because some of the non orthagonally adjacent cube regions will have some of the surface area in them.","Let's pick the point of the center of a sphere on a 3-d axis the coordinate, coordinate, and coordinate of the sphere's center, are each separately from a uniform distribution From these coordinates, we make a sphere with radius . What is the percent of the surface area (on average) that lies within the cube region with vertices at: So my first thought was to just assume that are each since that is the expected value of each (or the average) and then calculate it from there, but I know that that logic is flawed because of monte carlo simulations- although I can't figure out why. Also, I know that the absolute upper bound would be as the percent of the surface area that is in the cube region above it's center on average should be the same percent of the surface area that is in the cube region below and likewise the the around it as well by symmetry. It cannot equal because some of the non orthagonally adjacent cube regions will have some of the surface area in them.","x y z (x, y, z) [0, 1]  1 (0,0,1), (1,0,1), (0,1,1), (1,1,1), (0,0,2), (1,0,2), (0,1,2), (1,1,2) x,y,z 0.5 1/6 4 1/6","['calculus', 'multivariable-calculus', 'analytic-geometry']"
55,Derivative of integral of function over n-sphere is the flux outwards - step in proof involving partition of unity.,Derivative of integral of function over n-sphere is the flux outwards - step in proof involving partition of unity.,,"Let $N:\mathbb R^{n+1} \to \mathbb R^{n+1}$ be a vector field defined by $N(x) = \frac{x}{\lVert x \rVert}$ . Let $f:\mathbb R^{n+1} \to \mathbb R$ be a $C^1$ function. Given a coordinate system $\mathbb X:U\to S_1^n$ for $U$ open in $\mathbb R^n$ , we define $\mathbb X_r (u) = r \mathbb X(u)$ . $\mathbb X_r$ is indeed a coordinate system. The next step would be to prove the following: Using partition of unity, show we can assume w.l.o.g $\operatorname{supp}f|_{S^n_r} \subset \operatorname {Im}(\mathbb X_r)$ . This is the first part in an exercise in which the following is proven: $$ \frac{d}{dr} \left(\frac{1}{r^n} \int_{S^n_r}fd\sigma \right)=\frac{1}{r^n}\int_{S^r_n}\langle\nabla f,N \rangle d\sigma$$ I usually try my best to have some sort of attempt in finding a solution before posting here, but I'm not sure where to begin. I know the n-sphere is compact so given  coordinate systems with $U_\alpha$ as their domains we get an open cover for $S^n_r$ we have a finite subcover that can induce a partition of unity. But I don't really know how to proceed. Would appreciate any help. A small note: This exercise was given in an analysis over manifolds I'm taking, but we haven't really covered anything of substance in differential geometry or more advanced fields (the most ""advanced"" integration theorem we've seen was Stokes' theorem), and so I'm not sure if the differential geometry tag is appropriate here.","Let be a vector field defined by . Let be a function. Given a coordinate system for open in , we define . is indeed a coordinate system. The next step would be to prove the following: Using partition of unity, show we can assume w.l.o.g . This is the first part in an exercise in which the following is proven: I usually try my best to have some sort of attempt in finding a solution before posting here, but I'm not sure where to begin. I know the n-sphere is compact so given  coordinate systems with as their domains we get an open cover for we have a finite subcover that can induce a partition of unity. But I don't really know how to proceed. Would appreciate any help. A small note: This exercise was given in an analysis over manifolds I'm taking, but we haven't really covered anything of substance in differential geometry or more advanced fields (the most ""advanced"" integration theorem we've seen was Stokes' theorem), and so I'm not sure if the differential geometry tag is appropriate here.","N:\mathbb R^{n+1} \to \mathbb R^{n+1} N(x) = \frac{x}{\lVert x \rVert} f:\mathbb R^{n+1} \to \mathbb R C^1 \mathbb X:U\to S_1^n U \mathbb R^n \mathbb X_r (u) = r \mathbb X(u) \mathbb X_r \operatorname{supp}f|_{S^n_r} \subset \operatorname {Im}(\mathbb X_r)  \frac{d}{dr} \left(\frac{1}{r^n} \int_{S^n_r}fd\sigma \right)=\frac{1}{r^n}\int_{S^r_n}\langle\nabla f,N \rangle d\sigma U_\alpha S^n_r","['real-analysis', 'multivariable-calculus', 'differential-geometry']"
56,Volume of intersection of two partial spheres having origins at different coordinate frames using spherical coordinates,Volume of intersection of two partial spheres having origins at different coordinate frames using spherical coordinates,,"Assume I have a world coordinate frame $\mathbf{w}$ . Assume I have a second coordinate frame that can be parameterized as a $4\times4$ homogenous transformation matrix with respect to the world coordinate frame: $\mathbf{^{w}T_{s_1}}$ . The origin of the first sphere $s_1$ has a 6DoF pose represented by the transformation matrix $\mathbf{^{w}T_{s_1}}$ . $s_1$ can be parameterized in spherical coordinates (mathematical convention) as $(\rho_1, \theta_1, \phi_1)$ where $(0 \leq \theta_1 \leq 2\pi)$ and $(-\frac{\pi}{4} \leq \phi_1 \leq \frac{\pi}{4})$ . The restriction of $\phi_1$ means that $s_1$ is in fact a partial sphere that looks like a round bottomed cone. The central axis of the cone is pointing in the direction of the Z-axis of coordinate frame $\mathbf{^{w}T_{s_1}}$ . Assume I have a third coordinate frame that can be parameterized as a $4x4$ homogenous transformation matrix also with respect to the world coordinate frame: $\mathbf{^{w}T_{s_2}}$ . The origin of the second sphere $s_2$ has a 6DoF pose represented by the transformation matrix $\mathbf{^{w}T_{s_2}}$ . $s_2$ can be parameterized in spherical coordinates (mathematical convention) as $(\rho_2, \theta_2, \phi_2)$ where $(0 \leq \theta_2 \leq 2\pi)$ and $(-\frac{\pi}{4} \leq \phi_2 \leq \frac{\pi}{4})$ . The restriction of $\phi_2$ means that $s_2$ is also a partial sphere that looks like a round bottomed cone. The central axis of the cone is pointing in the direction of the Z-axis of coordinate frame $\mathbf{^{w}T_{s_2}}$ . What is the volume of intersection of the partial spheres $s_1$ and $s_2$ ? It seems that given the coordinate transformations and different axis orientations, I may need to convert the spherical coordinates back to cartesian coordinates and then try to solve but I am not sure. Thanks!","Assume I have a world coordinate frame . Assume I have a second coordinate frame that can be parameterized as a homogenous transformation matrix with respect to the world coordinate frame: . The origin of the first sphere has a 6DoF pose represented by the transformation matrix . can be parameterized in spherical coordinates (mathematical convention) as where and . The restriction of means that is in fact a partial sphere that looks like a round bottomed cone. The central axis of the cone is pointing in the direction of the Z-axis of coordinate frame . Assume I have a third coordinate frame that can be parameterized as a homogenous transformation matrix also with respect to the world coordinate frame: . The origin of the second sphere has a 6DoF pose represented by the transformation matrix . can be parameterized in spherical coordinates (mathematical convention) as where and . The restriction of means that is also a partial sphere that looks like a round bottomed cone. The central axis of the cone is pointing in the direction of the Z-axis of coordinate frame . What is the volume of intersection of the partial spheres and ? It seems that given the coordinate transformations and different axis orientations, I may need to convert the spherical coordinates back to cartesian coordinates and then try to solve but I am not sure. Thanks!","\mathbf{w} 4\times4 \mathbf{^{w}T_{s_1}} s_1 \mathbf{^{w}T_{s_1}} s_1 (\rho_1, \theta_1, \phi_1) (0 \leq \theta_1 \leq 2\pi) (-\frac{\pi}{4} \leq \phi_1 \leq \frac{\pi}{4}) \phi_1 s_1 \mathbf{^{w}T_{s_1}} 4x4 \mathbf{^{w}T_{s_2}} s_2 \mathbf{^{w}T_{s_2}} s_2 (\rho_2, \theta_2, \phi_2) (0 \leq \theta_2 \leq 2\pi) (-\frac{\pi}{4} \leq \phi_2 \leq \frac{\pi}{4}) \phi_2 s_2 \mathbf{^{w}T_{s_2}} s_1 s_2","['multivariable-calculus', 'differential-geometry', 'spherical-coordinates', 'spherical-geometry']"
57,A ladybug is moving with velocity vector $v$. At what rate is the sum of her distances from two points decreasing at a certain moment?,A ladybug is moving with velocity vector . At what rate is the sum of her distances from two points decreasing at a certain moment?,v,"Problem : At a certain moment, a ladybug is at position $x_0$ and moving with velocity vector $v$ . At the moment, the angle $<ax_0b = \pi/2$ , her velocity bisects the angle, and her speed is $5$ units/sec. At what rate is the sum of her distances from $a$ and $b$ decreasing at that moment? My question is why my final answer is positive instead of negative. Solution : I will begin by defining two functions. let $p$ be the function that gives the position of the bug at time $t$ such that $p(t) = x_0 + tv$ , and let $s$ be the function that gives the sum of the distances between the current position of the bug and the points $a$ and $b$ such that $s(x) = \rVert x - a\lVert + \rVert x - b\lVert$ . Now I need the function which gives the sum of the distances given the time. This function which I will call $h$ is the composition of $s$ and $t$ , so $h = s \circ p$ . The question is asking for the rate of decrease of $h$ at $t = 0$ so I need the derivative of $h$ at $t = 0$ which is $h'(0) = Ds(p(0))Dp(0)$ . $h'(0)$ is a scalar so it equals its transpose, so $h'(0) = (Ds(p(0))Dp(0))^\intercal = (Dp(0))^\intercal \nabla s(p(0))$ . Now $(Dp(0))^\intercal = v^\intercal$ , and $\nabla s(p(0)) = \dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}$ . So now $h'(0) = v^\intercal(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = v \cdot(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = (\dfrac{v \cdot (p(0) - a)}{\rVert p(0) - a\lVert} + \dfrac{v \cdot (p(0) - b)}{\rVert p(0) - b\lVert})$ . From the question we know that $\rVert v\lVert = 5$ and that the angle between $v$ and the two vectors is $\pi/4$ , so $h'(0) = 5 \cos{45} + 5 \cos{45} = 5\sqrt{2}$ but that is a positive number which is wrong as the distance should be decreasing. It seem to me from the picture as if the bug is moving in the direction of the smallest rate of change, so maybe somehow my mistake was in confusing $v$ with $-v$ which should be the direction of the greatest rate of change and the correct answer should be $-5\sqrt{2}$ . Perhaps all my solution is just wrong.","Problem : At a certain moment, a ladybug is at position and moving with velocity vector . At the moment, the angle , her velocity bisects the angle, and her speed is units/sec. At what rate is the sum of her distances from and decreasing at that moment? My question is why my final answer is positive instead of negative. Solution : I will begin by defining two functions. let be the function that gives the position of the bug at time such that , and let be the function that gives the sum of the distances between the current position of the bug and the points and such that . Now I need the function which gives the sum of the distances given the time. This function which I will call is the composition of and , so . The question is asking for the rate of decrease of at so I need the derivative of at which is . is a scalar so it equals its transpose, so . Now , and . So now . From the question we know that and that the angle between and the two vectors is , so but that is a positive number which is wrong as the distance should be decreasing. It seem to me from the picture as if the bug is moving in the direction of the smallest rate of change, so maybe somehow my mistake was in confusing with which should be the direction of the greatest rate of change and the correct answer should be . Perhaps all my solution is just wrong.",x_0 v <ax_0b = \pi/2 5 a b p t p(t) = x_0 + tv s a b s(x) = \rVert x - a\lVert + \rVert x - b\lVert h s t h = s \circ p h t = 0 h t = 0 h'(0) = Ds(p(0))Dp(0) h'(0) h'(0) = (Ds(p(0))Dp(0))^\intercal = (Dp(0))^\intercal \nabla s(p(0)) (Dp(0))^\intercal = v^\intercal \nabla s(p(0)) = \dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert} h'(0) = v^\intercal(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = v \cdot(\dfrac{p(0) - a}{\rVert p(0) - a\lVert} + \dfrac{p(0) - b}{\rVert p(0) - b\lVert}) = (\dfrac{v \cdot (p(0) - a)}{\rVert p(0) - a\lVert} + \dfrac{v \cdot (p(0) - b)}{\rVert p(0) - b\lVert}) \rVert v\lVert = 5 v \pi/4 h'(0) = 5 \cos{45} + 5 \cos{45} = 5\sqrt{2} v -v -5\sqrt{2},"['multivariable-calculus', 'solution-verification', 'word-problem']"
58,Visualizing the curl of curl,Visualizing the curl of curl,,"In some differential equations from phyiscs, e.g. in elastodynamics, terms with the curl of the curl of a vector field appear. For example $(\lambda + 2 \mu) \nabla(\nabla\cdot \mathbf{x}) - \mu \nabla\times\nabla \times \mathbf{x} = \rho \ddot{\mathbf{x}}$ . Is there a way to visualize or have an intuitive/physical understanding of what is represented by the curl of a curl of a vector field.","In some differential equations from phyiscs, e.g. in elastodynamics, terms with the curl of the curl of a vector field appear. For example . Is there a way to visualize or have an intuitive/physical understanding of what is represented by the curl of a curl of a vector field.",(\lambda + 2 \mu) \nabla(\nabla\cdot \mathbf{x}) - \mu \nabla\times\nabla \times \mathbf{x} = \rho \ddot{\mathbf{x}},"['multivariable-calculus', 'curl']"
59,Find the inverse of the map,Find the inverse of the map,,"I am trying to compute the inverse of $$F(x,y)=\left(\sqrt{x^2+y^2+1},xy+\frac{1}{2}\log{\left(\frac{x+y}{y-x}\right)}\right)$$ in a suitable domain $U$ of $\mathbb{R}^2$ . I have checked that $F$ is a local diffeomorphism by the Implicit function theorem, however I completely failed finding the inverse. I appreciate any help.","I am trying to compute the inverse of in a suitable domain of . I have checked that is a local diffeomorphism by the Implicit function theorem, however I completely failed finding the inverse. I appreciate any help.","F(x,y)=\left(\sqrt{x^2+y^2+1},xy+\frac{1}{2}\log{\left(\frac{x+y}{y-x}\right)}\right) U \mathbb{R}^2 F","['real-analysis', 'multivariable-calculus']"
60,Chain rule (multivariate) and implicit differentiation problem,Chain rule (multivariate) and implicit differentiation problem,,"I have been worked out a solution for the following problem, but I am wondering if there is an easier way to solve it. I would be very grateful for any suggestions! Let $z=f(x,y)$ a function of two independent variables $x$ . and $y$ . Show that, if we perform a change of variables with $u$ and $v$ $u=x^2+y^2$ and $u=e^{\frac{y}{x}}$ , then $$x \frac{\partial z}{\partial x} + y \frac{\partial z}{\partial y} = 2v \frac{\partial z}{\partial v} \:.$$ My solution: By the multivariate chain rule, we have $$\frac{\partial z}{\partial v} = \frac{\partial z}{\partial x} \cdot \frac{\partial x}{\partial v} + \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial v} \:.$$ Hence it is sufficient to show that $\frac{\partial x}{\partial v} = \frac{x}{2v}$ and $\frac{\partial y}{\partial v} = \frac{y}{2v}$ . There are two ways in which we can do this (I will consider only $\frac{\partial x}{\partial v}$ ): Either we calculate the partial derivatives $\frac{\partial v}{\partial x}$ , $\frac{\partial v}{\partial y}$ , $\frac{\partial u}{\partial x}$ and $\frac{\partial u}{\partial y}$ and use the fact that, by the inverse function rule , $$ \begin{bmatrix}     \frac{\partial x}{\partial v} & \frac{\partial x}{\partial u} \\     \frac{\partial y}{\partial v} & \frac{\partial y}{\partial u} \end{bmatrix} = \begin{bmatrix}     \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y} \\     \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y} \end{bmatrix}^{-1} $$ to obtain $$\frac{\partial x}{\partial v} = \frac{\frac{\partial u}{\partial y}}{\frac{\partial v}{\partial x} \cdot \frac{\partial u}{\partial y} - \frac{\partial v}{\partial y} \frac{\partial u}{\partial x}} = \frac{x}{2v} .$$ Or we simply notice that $\ln(u) = \frac{y}{x}$ and thus $y = x \cdot \ln(u)$ . Substituting this into $v=x^2+y^2$ , we get $x^2=\frac{v}{1+[\ln(u)]^2}$ and, if we partially differentiate both sides with respect to $v$ , we obtain $2x\frac{\partial x}{\partial v} = \frac{1}{1+[\ln(u)]^2} = \frac{1}{1+\left[\frac{y}{x}\right]^2} = \frac{x^2}{x^2+y^2} = \frac{x^2}{v}$ , as required.","I have been worked out a solution for the following problem, but I am wondering if there is an easier way to solve it. I would be very grateful for any suggestions! Let a function of two independent variables . and . Show that, if we perform a change of variables with and and , then My solution: By the multivariate chain rule, we have Hence it is sufficient to show that and . There are two ways in which we can do this (I will consider only ): Either we calculate the partial derivatives , , and and use the fact that, by the inverse function rule , to obtain Or we simply notice that and thus . Substituting this into , we get and, if we partially differentiate both sides with respect to , we obtain , as required.","z=f(x,y) x y u v u=x^2+y^2 u=e^{\frac{y}{x}} x \frac{\partial z}{\partial x} + y \frac{\partial z}{\partial y} = 2v \frac{\partial z}{\partial v} \:. \frac{\partial z}{\partial v} = \frac{\partial z}{\partial x} \cdot \frac{\partial x}{\partial v} + \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial v}
\:. \frac{\partial x}{\partial v} = \frac{x}{2v} \frac{\partial y}{\partial v} = \frac{y}{2v} \frac{\partial x}{\partial v} \frac{\partial v}{\partial x} \frac{\partial v}{\partial y} \frac{\partial u}{\partial x} \frac{\partial u}{\partial y} 
\begin{bmatrix}
    \frac{\partial x}{\partial v} & \frac{\partial x}{\partial u} \\
    \frac{\partial y}{\partial v} & \frac{\partial y}{\partial u}
\end{bmatrix}
=
\begin{bmatrix}
    \frac{\partial v}{\partial x} & \frac{\partial v}{\partial y} \\
    \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}
\end{bmatrix}^{-1}
 \frac{\partial x}{\partial v} = \frac{\frac{\partial u}{\partial y}}{\frac{\partial v}{\partial x} \cdot \frac{\partial u}{\partial y} - \frac{\partial v}{\partial y} \frac{\partial u}{\partial x}} = \frac{x}{2v}
. \ln(u) = \frac{y}{x} y = x \cdot \ln(u) v=x^2+y^2 x^2=\frac{v}{1+[\ln(u)]^2} v 2x\frac{\partial x}{\partial v} = \frac{1}{1+[\ln(u)]^2} = \frac{1}{1+\left[\frac{y}{x}\right]^2} = \frac{x^2}{x^2+y^2} = \frac{x^2}{v}","['multivariable-calculus', 'partial-derivative', 'chain-rule', 'implicit-differentiation', 'change-of-variable']"
61,"How to graph $\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta$",How to graph,"\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta","I want to graph the region that the integral $\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta$ integrates over, which  is a little weird because I don't understand how $\theta$ goes from $0$ to $\pi$ . It would be obvious if $\theta$ goes from $-\frac{\pi}{2}$ to $\frac{\pi}{2}$ . I checked answers in the back of the book and it was a full circle with center of $(1,0)$ and radius of $1$ .","I want to graph the region that the integral integrates over, which  is a little weird because I don't understand how goes from to . It would be obvious if goes from to . I checked answers in the back of the book and it was a full circle with center of and radius of .","\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta \theta 0 \pi \theta -\frac{\pi}{2} \frac{\pi}{2} (1,0) 1",['calculus']
62,Is there something called function of two dependent variables?,Is there something called function of two dependent variables?,,"While reading in Stewart calculus in the section that talks about implicit differentiation in multivarible functions he wrote this : if $F(x,y)=0$ that define implicitly a differentiable function in $x$ , $y=f(x)$ where $F(x,f(x))=0$ and then he applied the chain rule \begin{gather} \frac{\partial F}{\partial x}+\frac{\partial F}{\partial y}\frac{d y}{dx}=0 \end{gather} so he treated $F$ as being function of $2$ variables although one of them is depending on the other I think this like the composite function in single variable calc, sometimes we write functions in terms of composition between others \begin{gather} h(x)=f(x^2)=sin(x^2) \end{gather} the function $h$ it's input is $x$ while the function $f$ it's input it $x^2$ and that is obvious while taking the chain rule we write \begin{gather} \frac{d h(x)}{dx}=\frac{d f(x^2)}{d(x^2)}\frac{d(x^2) }{dx} \end{gather} which is an indication that the term $(x^2)$ is the variable to $f$ even that it's connected to $x$ then if it applicable to single variable calc , then it can also be applicable to multivariable ? correct me if I wrote anything wrong .","While reading in Stewart calculus in the section that talks about implicit differentiation in multivarible functions he wrote this : if that define implicitly a differentiable function in , where and then he applied the chain rule so he treated as being function of variables although one of them is depending on the other I think this like the composite function in single variable calc, sometimes we write functions in terms of composition between others the function it's input is while the function it's input it and that is obvious while taking the chain rule we write which is an indication that the term is the variable to even that it's connected to then if it applicable to single variable calc , then it can also be applicable to multivariable ? correct me if I wrote anything wrong .","F(x,y)=0 x y=f(x) F(x,f(x))=0 \begin{gather} \frac{\partial F}{\partial x}+\frac{\partial F}{\partial y}\frac{d y}{dx}=0 \end{gather} F 2 \begin{gather} h(x)=f(x^2)=sin(x^2) \end{gather} h x f x^2 \begin{gather} \frac{d h(x)}{dx}=\frac{d f(x^2)}{d(x^2)}\frac{d(x^2) }{dx} \end{gather} (x^2) f x","['calculus', 'multivariable-calculus', 'functions']"
63,Choosing number of subdivisions for approximating double integrals in context of image processing,Choosing number of subdivisions for approximating double integrals in context of image processing,,"I have a few double integrals that I have to approximate that are based on getting motion parameters from a sequence of images, and I have chosen to use Riemann sums for that purpose. These integrals are over the domain $R = [-w,w] \times [-h,h]$ , where $2w$ and $2h$ are the image width and height respectively. What would be an appropriate number of subdivisions $m$ and $n$ to choose to ensure that I can use code to evaluate the integrals in all cases? I am confused as we don't have access to all necessary values of $x_{ij}$ and $y_{ij}$ as $m$ and $n$ increase. I was thinking of considering the image frames as my grids of values, but I don't know what the pixel widths and heights would be. I am not even sure if this is the correct approach for approximating the integrals, so please suggest if there are other appropriate approaches. Here is one of the integrals that I am approximating: $$\iint ((u-u_r)\beta - (v - v_r)\alpha)(-xy\beta + (y^2 + 1)\alpha) \ dxdy$$","I have a few double integrals that I have to approximate that are based on getting motion parameters from a sequence of images, and I have chosen to use Riemann sums for that purpose. These integrals are over the domain , where and are the image width and height respectively. What would be an appropriate number of subdivisions and to choose to ensure that I can use code to evaluate the integrals in all cases? I am confused as we don't have access to all necessary values of and as and increase. I was thinking of considering the image frames as my grids of values, but I don't know what the pixel widths and heights would be. I am not even sure if this is the correct approach for approximating the integrals, so please suggest if there are other appropriate approaches. Here is one of the integrals that I am approximating:","R = [-w,w] \times [-h,h] 2w 2h m n x_{ij} y_{ij} m n \iint ((u-u_r)\beta - (v - v_r)\alpha)(-xy\beta + (y^2 + 1)\alpha) \ dxdy","['multivariable-calculus', 'approximation', 'computer-vision']"
64,Compact Manifolds in Euclidean Spaces,Compact Manifolds in Euclidean Spaces,,"I'm watching Professor Shifrin's wonderful lectures on Stokes's Theorem, and he uses the term ""compact"" to describe a manifold. He defines a compact set of $\mathbb{R}^n$ as one that is closed and bounded. Does this definition carry over to manifolds?","I'm watching Professor Shifrin's wonderful lectures on Stokes's Theorem, and he uses the term ""compact"" to describe a manifold. He defines a compact set of as one that is closed and bounded. Does this definition carry over to manifolds?",\mathbb{R}^n,"['real-analysis', 'multivariable-calculus', 'manifolds', 'stokes-theorem']"
65,About regularity of solutions of this PDE,About regularity of solutions of this PDE,,"Let us consider the equation in the 2d torus $$\nabla \cdot f=g^2,$$ where $\nabla \cdot f$ denotes the divergence operator and $g^2$ has zero mean. In particular, we can see these functions as they were defined in $\mathbb{R} ^2$ and they were $2 \pi-$ periodic in each variable. I know that this equation doesn't admit a solution $f \in W^{1,1}$ if $g \in L^2$ . My question is: Is there a solution in $W^{1,1}$ for every $g^2$ in the Lorentz space $L^{2,1}$ ? If this is not possible, What is the best regularity that we can expect for the solution given $g$ under this condition?","Let us consider the equation in the 2d torus where denotes the divergence operator and has zero mean. In particular, we can see these functions as they were defined in and they were periodic in each variable. I know that this equation doesn't admit a solution if . My question is: Is there a solution in for every in the Lorentz space ? If this is not possible, What is the best regularity that we can expect for the solution given under this condition?","\nabla \cdot f=g^2, \nabla \cdot f g^2 \mathbb{R} ^2 2 \pi- f \in W^{1,1} g \in L^2 W^{1,1} g^2 L^{2,1} g","['functional-analysis', 'multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'geometric-topology']"
66,Integrating monomials over $\mathbb{S}^{n-1}$ - Folland,Integrating monomials over  - Folland,\mathbb{S}^{n-1},"This is an exercise from Folland: Suppose $f(x)=\prod_{j=1}^n x_j^{\alpha_j}$ where all $\alpha_j$ are even. Then show that $$\int_{\mathbb{S}^{n-1}} f d \sigma= \frac{2 \prod_{j=1}^{n}\Gamma(\beta_j)}{\Gamma\left(\sum_{j=1}^n\beta_j\right)} \quad \text{where }\beta_j=\frac{\alpha_j+1}{2}$$ . As hint Folland suggests to calculate $\displaystyle \int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x$ . Attempt: By Tonelli's we have $$\int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\prod_{j=1}^{n}\int_{-\infty}^{\infty}e^{-y^2}y^{\alpha_j} \mathrm{d}y=\prod_{j=1}^{n}\Gamma(\beta_j)$$ I think I am supposed to change to polar coordinates now $(r=|x|, \, x'=\frac{x}{|x|})$ . Since $f(rx')=r^{\sum \alpha_j}f(x')$ , we get: $$\prod_{j=1}^{n}\Gamma(\beta_j)=\int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\int_0^{\infty}e^{-r^2}r^{(\sum \alpha_j)+n-1}\mathrm{d}r \int_{\mathbb{S}^{n-1}} f \mathrm{d}\sigma$$ from which the result follows. Is this correct?","This is an exercise from Folland: Suppose where all are even. Then show that . As hint Folland suggests to calculate . Attempt: By Tonelli's we have I think I am supposed to change to polar coordinates now . Since , we get: from which the result follows. Is this correct?","f(x)=\prod_{j=1}^n x_j^{\alpha_j} \alpha_j \int_{\mathbb{S}^{n-1}} f d \sigma= \frac{2 \prod_{j=1}^{n}\Gamma(\beta_j)}{\Gamma\left(\sum_{j=1}^n\beta_j\right)} \quad \text{where }\beta_j=\frac{\alpha_j+1}{2} \displaystyle \int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x \int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\prod_{j=1}^{n}\int_{-\infty}^{\infty}e^{-y^2}y^{\alpha_j} \mathrm{d}y=\prod_{j=1}^{n}\Gamma(\beta_j) (r=|x|, \, x'=\frac{x}{|x|}) f(rx')=r^{\sum \alpha_j}f(x') \prod_{j=1}^{n}\Gamma(\beta_j)=\int_{\mathbb{R}^n} e^{-|x|^2}f(x)\mathrm{d}x=\int_0^{\infty}e^{-r^2}r^{(\sum \alpha_j)+n-1}\mathrm{d}r \int_{\mathbb{S}^{n-1}} f \mathrm{d}\sigma","['real-analysis', 'integration', 'multivariable-calculus', 'solution-verification', 'gaussian-integral']"
67,Minimize $x+y$ given $(C - 1)xy + x=D$ [closed],Minimize  given  [closed],x+y (C - 1)xy + x=D,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Minimize $x+y$ given $(C - 1)xy + x=D$ where $x$ and $y$ are positive reals I am looking to derive an equation such that given a constant $C$ and an objective value $D$ , I am able to determine the minimum needed combined $x$ and $y$ to reach it. objective function with a horizontal plane at D","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Minimize given where and are positive reals I am looking to derive an equation such that given a constant and an objective value , I am able to determine the minimum needed combined and to reach it. objective function with a horizontal plane at D",x+y (C - 1)xy + x=D x y C D x y,"['calculus', 'multivariable-calculus', 'optimization', 'maxima-minima']"
68,Interpreting exterior products as intersection of diagrams,Interpreting exterior products as intersection of diagrams,,"This paper provides an intuition of $k$ -forms. On page $4$ it reads: ""In order to represent the exterior product of two $n$ -forms you find the intersection of the diagrams representing the forms. For example if we draw $dx$ as vertical lines and $dy$ as horizontal lines then $dx ∧ dy$ is the set of intersection points which will form a uniform grid."" Why can the exterior product of two forms be interepreted as the intersection of the diagrams representing the forms? This is a follow-up to this post .","This paper provides an intuition of -forms. On page it reads: ""In order to represent the exterior product of two -forms you find the intersection of the diagrams representing the forms. For example if we draw as vertical lines and as horizontal lines then is the set of intersection points which will form a uniform grid."" Why can the exterior product of two forms be interepreted as the intersection of the diagrams representing the forms? This is a follow-up to this post .",k 4 n dx dy dx ∧ dy,"['analysis', 'multivariable-calculus', 'differential-geometry', 'intuition', 'differential-forms']"
69,A difficulty in the surface integral of $\frac{1}{\sqrt{1-y^4}}$ over a certain surface $S$,A difficulty in the surface integral of  over a certain surface,\frac{1}{\sqrt{1-y^4}} S,"Consider the following surface integral of the first kind: $\iint_{S}\frac{1}{\sqrt{1-y^4}}dS$ where $S=[(x,y,z)\in R^3:z=x+\frac{y^2}{\sqrt{2}},x\in[0,\frac{\pi}{2}],y\in[0,\frac{1}{\sqrt{2}}]]$ . I found this exercise on Youtube and the solver opted for a parametric approach to the surface $S$ obtainig the result $\frac{\sqrt{2}\pi^2}{8}$ . I opted for a cartesian approach to the surface $S: dS=\sqrt{1+||\nabla g||^2}dxdy$ where $z=g(x,y)=x+\frac{y^2}{\sqrt{2}}$ . By calculation, $dS=\sqrt{1+2y^2}dxdy$ as $\nabla g=(1,\sqrt{2}y)$ . Now, $\iint_{S}\frac{1}{\sqrt{1-y^4}}dS=\iint_{D}\frac{1}{\sqrt{1-y^4}}\sqrt{1+2y^2}dD=\int_{0}^{\pi/2}dx\int_{0}^{1/\sqrt{2}}\frac{\sqrt{1+2y^2}}{\sqrt{1-y^4}}dy=\frac{\pi}{2}\int_{0}^{1/\sqrt{2}}\frac{\sqrt{1+2y^2}}{\sqrt{1-y^4}}dy$ . To me it's not clear how to proceed except doing a simple $\sqrt{1-y^4}=\sqrt{1-y^2}\sqrt{1+y^2}$ but in the numerator I have a different quantity even after trying to factor a $2$ outside the square root. Is my approach even correct for this kind of problem or should I parametrize $S$ ?","Consider the following surface integral of the first kind: where . I found this exercise on Youtube and the solver opted for a parametric approach to the surface obtainig the result . I opted for a cartesian approach to the surface where . By calculation, as . Now, . To me it's not clear how to proceed except doing a simple but in the numerator I have a different quantity even after trying to factor a outside the square root. Is my approach even correct for this kind of problem or should I parametrize ?","\iint_{S}\frac{1}{\sqrt{1-y^4}}dS S=[(x,y,z)\in R^3:z=x+\frac{y^2}{\sqrt{2}},x\in[0,\frac{\pi}{2}],y\in[0,\frac{1}{\sqrt{2}}]] S \frac{\sqrt{2}\pi^2}{8} S: dS=\sqrt{1+||\nabla g||^2}dxdy z=g(x,y)=x+\frac{y^2}{\sqrt{2}} dS=\sqrt{1+2y^2}dxdy \nabla g=(1,\sqrt{2}y) \iint_{S}\frac{1}{\sqrt{1-y^4}}dS=\iint_{D}\frac{1}{\sqrt{1-y^4}}\sqrt{1+2y^2}dD=\int_{0}^{\pi/2}dx\int_{0}^{1/\sqrt{2}}\frac{\sqrt{1+2y^2}}{\sqrt{1-y^4}}dy=\frac{\pi}{2}\int_{0}^{1/\sqrt{2}}\frac{\sqrt{1+2y^2}}{\sqrt{1-y^4}}dy \sqrt{1-y^4}=\sqrt{1-y^2}\sqrt{1+y^2} 2 S",['multivariable-calculus']
70,How to calculate the volume of this particular solid using triple integrals,How to calculate the volume of this particular solid using triple integrals,,"Evaluate the volume of the solid $$A=\{(x,y,z)\in \mathbb{R}^3: x^2+y^2-2y\le0; 0\le z\le 10-3\sqrt {x^2+y^2}\}.$$ Knowing that integration ""by wires"" can be used when the domain of integration $\Omega$ can be written as $$\Omega=\{(x,y,z)\in  \mathbb{R}^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in D\}$$ we can setup the integral $$\begin{align}\iiint_{A}dxdydz&=\iint_{x^2+y^2-2y\le 0}\Big(\int_{0}^{10-3\sqrt{x^2+y^2}}dz\Big)dxdy \\&=\iint_{x^2+y^2-2y\le 0}\Big(10-3\sqrt {x^2+y^2}\Big)dxdy.\end{align}$$ Now, the domain $D=\{(x,y)\in  \mathbb{R}^2: x^2+y^2-2y\le 0\}$ is a circle with centre in $(0,1)$ and radius $1$ . I used the following polar coordinates: $x=r\cos(\theta)$ , $y=1+r\sin(\theta)$ because of the translation of the circle and the integral became: $$\int_{0}^{2\pi}\int_{0}^{1}(10-3\sqrt{r^2\cos^2(\theta)+1+2r\sin(\theta)+r^2\sin^2(\theta)})rdrd\theta\\ =\int_{0}^{2\pi}\int_{0}^{1}\Big(10-3\sqrt{r^2+1+2r\sin(\theta)}\Big)rdrd\theta.$$ As the final calculations are not so straightforward I'm perplexed about my method and, in the first place, I'm asking if this is correct, in the second place if there exists another method which is simpler.","Evaluate the volume of the solid Knowing that integration ""by wires"" can be used when the domain of integration can be written as we can setup the integral Now, the domain is a circle with centre in and radius . I used the following polar coordinates: , because of the translation of the circle and the integral became: As the final calculations are not so straightforward I'm perplexed about my method and, in the first place, I'm asking if this is correct, in the second place if there exists another method which is simpler.","A=\{(x,y,z)\in \mathbb{R}^3: x^2+y^2-2y\le0; 0\le z\le 10-3\sqrt {x^2+y^2}\}. \Omega \Omega=\{(x,y,z)\in  \mathbb{R}^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in D\} \begin{align}\iiint_{A}dxdydz&=\iint_{x^2+y^2-2y\le 0}\Big(\int_{0}^{10-3\sqrt{x^2+y^2}}dz\Big)dxdy
\\&=\iint_{x^2+y^2-2y\le 0}\Big(10-3\sqrt {x^2+y^2}\Big)dxdy.\end{align} D=\{(x,y)\in  \mathbb{R}^2: x^2+y^2-2y\le 0\} (0,1) 1 x=r\cos(\theta) y=1+r\sin(\theta) \int_{0}^{2\pi}\int_{0}^{1}(10-3\sqrt{r^2\cos^2(\theta)+1+2r\sin(\theta)+r^2\sin^2(\theta)})rdrd\theta\\
=\int_{0}^{2\pi}\int_{0}^{1}\Big(10-3\sqrt{r^2+1+2r\sin(\theta)}\Big)rdrd\theta.",[]
71,normal vector stokes theorem calculation,normal vector stokes theorem calculation,,"$$\int\int_S (\nabla \times F) \cdot n \, dS$$ Solving Stokes' theorem using Curl form, I'm having an issue with the calculation of the normal vector when using parameterization of the surface and a cross product vs. $f(x,y,z)=z-g(x,y)$ form. Curl is given as $\langle-2z, -2x, -2y\rangle$ and C as the intersection of $z=\sqrt{5-x^2 -y^2}$ and $z=1$ Parameterizing results in $r(x,y)=\langle x, y, 1\rangle$ . The cross product of the partial with respect to x and y gives a normal vector of $\langle 0,0,1 \rangle$ However if I want to use the form $z-g(x,y)$ with $z=\sqrt{5-x^2 -y^2} $ it is not evident to me how I get to the same normal vector after taking the gradient. I get $\left\langle \frac{x}{\sqrt{5-x^2 -y^2}}，\frac{y}{\sqrt{5-x^2 -y^2}}, 1 \right\rangle$ Can someone point me in the right direction?","Solving Stokes' theorem using Curl form, I'm having an issue with the calculation of the normal vector when using parameterization of the surface and a cross product vs. form. Curl is given as and C as the intersection of and Parameterizing results in . The cross product of the partial with respect to x and y gives a normal vector of However if I want to use the form with it is not evident to me how I get to the same normal vector after taking the gradient. I get Can someone point me in the right direction?","\int\int_S (\nabla \times F) \cdot n \, dS f(x,y,z)=z-g(x,y) \langle-2z, -2x, -2y\rangle z=\sqrt{5-x^2 -y^2} z=1 r(x,y)=\langle x, y, 1\rangle \langle 0,0,1 \rangle z-g(x,y) z=\sqrt{5-x^2 -y^2}  \left\langle \frac{x}{\sqrt{5-x^2 -y^2}}，\frac{y}{\sqrt{5-x^2 -y^2}}, 1 \right\rangle","['multivariable-calculus', 'stokes-theorem']"
72,"Level curves of $f(x,y) : f_x < 0, f_y > 0$",Level curves of,"f(x,y) : f_x < 0, f_y > 0","I am trying to construct a differentiable, concave function $f(x,y)$ where $f : \mathbb{R}^2 \to \mathbb{R}$ (but the domain I'll mainly be focussing on is $\mathbb{R}^2_{\geq 0}$ ) that satisfies $f_x < 0$ and $f_y > 0$ . Query $1:$ What could be a possible construction? Query $2:$ Which of the following figures below is the correct representation of the level curves of the function?","I am trying to construct a differentiable, concave function where (but the domain I'll mainly be focussing on is ) that satisfies and . Query What could be a possible construction? Query Which of the following figures below is the correct representation of the level curves of the function?","f(x,y) f : \mathbb{R}^2 \to \mathbb{R} \mathbb{R}^2_{\geq 0} f_x < 0 f_y > 0 1: 2:","['calculus', 'multivariable-calculus', 'convex-analysis', 'partial-derivative']"
73,Single objective optimization with $n$ independent variables,Single objective optimization with  independent variables,n,"I have a single objective optimization problem. The problem can consist of $n$ independent variables $(x_1,x_2,...,x_n)$ four functions $f_i(\bf{x})$ and a function $Q(\bf{x})$ . I want to monitor how a composite function $F$ , composed of four individual functions $f_1(x_1,x_2\ldots x_n),f_2(x_1,x_2,\ldots,x_n),f_3(x_1,x_2\ldots,x_n),f_4(x_1,x_2,\ldots,x_n)$ , tracks a the rate of change (gradient) of the function $Q(x_1,x_2,\ldots,x_n)$ for $x_n\in[\alpha,\beta]$ with $\alpha,\beta\in\mathbb{R}$ . Ι will probably use the weighted sum method to compose $F$ so $F$ will be in the form $F=a_1f_1+a_2f_2+a_3f_3+a_4f_4$ with $\displaystyle \sum_ia_i=1$ to sweep through different combinations of weights. My question is how would I compare the rates of change between $F$ and $Q$ ? If I had a single independent variable, I would probably create $J = Q'-F'$ and I would try to minimize it. I thought something along the lines of $J = \nabla Q -\nabla F$ but would this hold? Is there any general approach in these kinds of problems?","I have a single objective optimization problem. The problem can consist of independent variables four functions and a function . I want to monitor how a composite function , composed of four individual functions , tracks a the rate of change (gradient) of the function for with . Ι will probably use the weighted sum method to compose so will be in the form with to sweep through different combinations of weights. My question is how would I compare the rates of change between and ? If I had a single independent variable, I would probably create and I would try to minimize it. I thought something along the lines of but would this hold? Is there any general approach in these kinds of problems?","n (x_1,x_2,...,x_n) f_i(\bf{x}) Q(\bf{x}) F f_1(x_1,x_2\ldots x_n),f_2(x_1,x_2,\ldots,x_n),f_3(x_1,x_2\ldots,x_n),f_4(x_1,x_2,\ldots,x_n) Q(x_1,x_2,\ldots,x_n) x_n\in[\alpha,\beta] \alpha,\beta\in\mathbb{R} F F F=a_1f_1+a_2f_2+a_3f_3+a_4f_4 \displaystyle \sum_ia_i=1 F Q J = Q'-F' J = \nabla Q -\nabla F","['calculus', 'multivariable-calculus', 'optimization', 'vector-analysis', 'nonlinear-optimization']"
74,question of partition in double integral,question of partition in double integral,,"in the definition of double integral,what the first to do is to divide region into small subrectangles . my question is :do these partitions have to be rectangles? for example ,when dealing with polar coordinates,region is divided into many small polar rectangles which are not rectangles.so can I use other shape to divide a region ,for exmple:parallelgram or triangle ,Are these also called  double integral? if these are also double integral why does the definition of double integral only use subrectangle for partition","in the definition of double integral,what the first to do is to divide region into small subrectangles . my question is :do these partitions have to be rectangles? for example ,when dealing with polar coordinates,region is divided into many small polar rectangles which are not rectangles.so can I use other shape to divide a region ,for exmple:parallelgram or triangle ,Are these also called  double integral? if these are also double integral why does the definition of double integral only use subrectangle for partition",,"['multivariable-calculus', 'multiple-integral', 'partitions-for-integration']"
75,Apostol: How to calculate work by force field along intersection of sphere and cylinder.,Apostol: How to calculate work by force field along intersection of sphere and cylinder.,,"This question is about line integrals in cartesian and cylindrical coordinates. It is based on the following problem from Apostol's Calculus , Volume II, chapter 10 ""Line Integrals"", section 10.9 Calculate the work done by the force field $$f(x,y,z)=y^2\hat{i}+z^2\hat{j}+x^2\hat{k}\tag{1}$$ along the curve of intersection of the sphere $x^2+y^2+z^2=a^2$ and the cylinder $$x^2+y^2=ax\tag{2}$$ where $$z\geq 0\tag{3}$$ and $$a>0\tag{4}$$ The path is traversed in a direction that appears clockwise when viewed from high above the $xy$ -plane. The answer from Apostol's book is $\pi a^3/4$ . Initial Attempt At first I did everything in cartesian coordinates. From (2) we have $$y=\pm\sqrt{ax-x^2}\tag{5}$$ Subbing (2) into (1) and solving for $z$ we have $$z=\sqrt{a^2-ax}\tag{6}$$ From (5) and (6) we have the condition $x\in [0,a]$ . If we let $x$ be our parameter, a parametrization of the intersection of the sphere and the cylinder is given by the two equations in (7) (notice the $\pm$ on the $\hat{j}$ component) $$\vec{r}(t)=t\hat{i}\pm\sqrt{at-t^2}\hat{j}+\sqrt{a^2-at}\hat{k}\tag{7}, t\in [0,a]$$ Here is a plot of the two curves that give the full intersection (for $a=2$ ) Hence $$d\vec{r}=(\hat{i}\pm\frac{a-2t}{2\sqrt{at-t^2}}\hat{j}+\frac{-a}{2\sqrt{a^2-at}}\hat{k})dt\tag{8}$$ and $$f(\vec{r}(t))=(at-t^2)\hat{i}+(a^2-at)\hat{j}+t^2\hat{k}\tag{9}$$ And the work done by the force field on this curve is $$\int_C \vec{f}\cdot d\vec{r}=\int_0^a \vec{f}(\vec{r}(t))\cdot d\vec{r}(t)dt\tag{10}$$ $$=\int_0^a \left(at-t^2+\frac{(a^2-at)(a-2t)}{2\sqrt{at-t^2}}-\frac{at^2}{2\sqrt{a^2-at}}\right )dt \tag{11}$$ My first question is if this approach is correct so far. This integral does not seem easy, and using Maple to solve it doesn't yield an answer. Second Attempt Then I tried with cylindrical coordinates, a topic that still eludes me. The sphere is $$r^2+z^2=a^2\tag{12}$$ and the cylinder is $$r^2=ar\cos{\theta} \implies r=a\cos{\theta}\tag{13}$$ From these two equations we obtain $$z=\sqrt{a^2-r^2}=\sqrt{a^2-a^2\cos^2{\theta}}=a\sin{\theta}\tag{14}$$ So at this point we have a parametrization of the intersection. $$r=a\cos{\theta}$$ $$\theta=\theta$$ $$z=a\sin{\theta}$$ And if we plot this, we get something that seems to make sense as well except that we have to remember that our intersection has $z\geq 0$ , so it our intersection is the top part of the plot above, and so coincides with the plot we obtained when we used cartesian coordinates. Now is the part that is more murky for me. We have $$\vec{f}(\vec{r}(\theta))=a^2\cos^2{\theta}\sin^2{\theta}\hat{i}+a^2\sin^2{\theta}\hat{j}+a^2\cos^4{\theta}\hat{k}\tag{15}$$ $$\vec{r}(\theta)=r(\theta)\hat{r}(\theta)+z(\theta)\hat{k}\tag{16}$$ $$=a\cos{\theta}\hat{r}(\theta)+a\sin{\theta}\hat{k}\tag{17}$$ $$=a\cos^2{\theta}\hat{i}+a\cos{\theta}\sin{\theta}\hat{j}+a\sin{\theta}\hat{k}\tag{18}$$ Then $$\frac{d\vec{r}(\theta)}{d\theta}=-2a\cos{\theta}\sin{\theta}\hat{i}+(-a\sin^2{\theta}+a^2\cos^2{\theta})\hat{j}+a\cos{\theta}\hat{k}\tag{18}$$ Thus, if this is correct, then the line integral is $$\int_C \vec{f}\cdot d\vec{r}=\int_{\pi}^0\left ( -2a^3\sin^3{\theta}\cos^3{\theta}-a^3\sin^4{\theta}+a^4\sin^2{\theta}\cos^2{\theta}+a^3\cos^5{\theta} \right )d\theta\tag{19}$$ Note that I chose the integration limits by looking at the parameterized equations and figuring out the values of $\theta$ that trace out the trajectory we want. For example, consider $a=2$ , as in the plots above. If we start at $\theta=0$ which corresponds to $(r,\theta,z)=(2,0,0)$ and move to $\theta=\pi/2$ which is $(r,\theta,z)=(0,\pi/2,2)$ . This is halfway. Then we go to $\theta=\pi$ which takes us back to the starting point, but now with coordinates $(-2,\pi,0)$ . However, this is counterclockwise, and we want clockwise, as per the problem statement. Hence the limits go from $\pi$ to $0$ . Using Maple, (19) evaluates to $$\frac{\pi a^3}{4}\tag{2}$$ So my questions are the following is the approach in Cartesian coordinates just destined to fail because that integral is too difficult to solve? is there some easier way than what I did above with the cylindrical coordinates? Final Note: As sometimes happens, in the course of carefully writing out my question and making sure it is well organized and clear for the reader, I sometimes end up learning. And it seems that I did reach the correct result after all with the cylindrical coordinates. However, my questions remain about if this was all way too cumbersome and if there is a smarter way.","This question is about line integrals in cartesian and cylindrical coordinates. It is based on the following problem from Apostol's Calculus , Volume II, chapter 10 ""Line Integrals"", section 10.9 Calculate the work done by the force field along the curve of intersection of the sphere and the cylinder where and The path is traversed in a direction that appears clockwise when viewed from high above the -plane. The answer from Apostol's book is . Initial Attempt At first I did everything in cartesian coordinates. From (2) we have Subbing (2) into (1) and solving for we have From (5) and (6) we have the condition . If we let be our parameter, a parametrization of the intersection of the sphere and the cylinder is given by the two equations in (7) (notice the on the component) Here is a plot of the two curves that give the full intersection (for ) Hence and And the work done by the force field on this curve is My first question is if this approach is correct so far. This integral does not seem easy, and using Maple to solve it doesn't yield an answer. Second Attempt Then I tried with cylindrical coordinates, a topic that still eludes me. The sphere is and the cylinder is From these two equations we obtain So at this point we have a parametrization of the intersection. And if we plot this, we get something that seems to make sense as well except that we have to remember that our intersection has , so it our intersection is the top part of the plot above, and so coincides with the plot we obtained when we used cartesian coordinates. Now is the part that is more murky for me. We have Then Thus, if this is correct, then the line integral is Note that I chose the integration limits by looking at the parameterized equations and figuring out the values of that trace out the trajectory we want. For example, consider , as in the plots above. If we start at which corresponds to and move to which is . This is halfway. Then we go to which takes us back to the starting point, but now with coordinates . However, this is counterclockwise, and we want clockwise, as per the problem statement. Hence the limits go from to . Using Maple, (19) evaluates to So my questions are the following is the approach in Cartesian coordinates just destined to fail because that integral is too difficult to solve? is there some easier way than what I did above with the cylindrical coordinates? Final Note: As sometimes happens, in the course of carefully writing out my question and making sure it is well organized and clear for the reader, I sometimes end up learning. And it seems that I did reach the correct result after all with the cylindrical coordinates. However, my questions remain about if this was all way too cumbersome and if there is a smarter way.","f(x,y,z)=y^2\hat{i}+z^2\hat{j}+x^2\hat{k}\tag{1} x^2+y^2+z^2=a^2 x^2+y^2=ax\tag{2} z\geq 0\tag{3} a>0\tag{4} xy \pi a^3/4 y=\pm\sqrt{ax-x^2}\tag{5} z z=\sqrt{a^2-ax}\tag{6} x\in [0,a] x \pm \hat{j} \vec{r}(t)=t\hat{i}\pm\sqrt{at-t^2}\hat{j}+\sqrt{a^2-at}\hat{k}\tag{7}, t\in [0,a] a=2 d\vec{r}=(\hat{i}\pm\frac{a-2t}{2\sqrt{at-t^2}}\hat{j}+\frac{-a}{2\sqrt{a^2-at}}\hat{k})dt\tag{8} f(\vec{r}(t))=(at-t^2)\hat{i}+(a^2-at)\hat{j}+t^2\hat{k}\tag{9} \int_C \vec{f}\cdot d\vec{r}=\int_0^a \vec{f}(\vec{r}(t))\cdot d\vec{r}(t)dt\tag{10} =\int_0^a \left(at-t^2+\frac{(a^2-at)(a-2t)}{2\sqrt{at-t^2}}-\frac{at^2}{2\sqrt{a^2-at}}\right )dt \tag{11} r^2+z^2=a^2\tag{12} r^2=ar\cos{\theta} \implies r=a\cos{\theta}\tag{13} z=\sqrt{a^2-r^2}=\sqrt{a^2-a^2\cos^2{\theta}}=a\sin{\theta}\tag{14} r=a\cos{\theta} \theta=\theta z=a\sin{\theta} z\geq 0 \vec{f}(\vec{r}(\theta))=a^2\cos^2{\theta}\sin^2{\theta}\hat{i}+a^2\sin^2{\theta}\hat{j}+a^2\cos^4{\theta}\hat{k}\tag{15} \vec{r}(\theta)=r(\theta)\hat{r}(\theta)+z(\theta)\hat{k}\tag{16} =a\cos{\theta}\hat{r}(\theta)+a\sin{\theta}\hat{k}\tag{17} =a\cos^2{\theta}\hat{i}+a\cos{\theta}\sin{\theta}\hat{j}+a\sin{\theta}\hat{k}\tag{18} \frac{d\vec{r}(\theta)}{d\theta}=-2a\cos{\theta}\sin{\theta}\hat{i}+(-a\sin^2{\theta}+a^2\cos^2{\theta})\hat{j}+a\cos{\theta}\hat{k}\tag{18} \int_C \vec{f}\cdot d\vec{r}=\int_{\pi}^0\left ( -2a^3\sin^3{\theta}\cos^3{\theta}-a^3\sin^4{\theta}+a^4\sin^2{\theta}\cos^2{\theta}+a^3\cos^5{\theta} \right )d\theta\tag{19} \theta a=2 \theta=0 (r,\theta,z)=(2,0,0) \theta=\pi/2 (r,\theta,z)=(0,\pi/2,2) \theta=\pi (-2,\pi,0) \pi 0 \frac{\pi a^3}{4}\tag{2}","['multivariable-calculus', 'solution-verification', 'vectors', 'line-integrals']"
76,Establishing a simple upper bound (with one less parameter) to an unwieldy function with non-negative discrete parameters,Establishing a simple upper bound (with one less parameter) to an unwieldy function with non-negative discrete parameters,,"I have this function at hand with 3 non-negative integer parameters that I would like to put a bound to: $$f(n, b, d) = 1 - \left(1 - \left(\frac1n\right)^{b+d-1}\right)^{\left(n^{b-1}\right)}$$ My end goal is to show that for any positive $\varepsilon_0$ there is a $n_0$ and $d^*$ such that for all $n \ge n_0$ and $b > 0$ , $f(n, b, d^*) < \varepsilon_0$ . In simpler terms, I want to show that $f$ can be made arbitrarily small for any $n$ larger than a constant by affixing $d$ to be another large-enough constant. To do this, I am looking for a decreasing function $g(n, d)$ which satisfies $f(n, b, d) \le g(n, d)$ for all $b$ . Playing around with $f$ in Desmos's graphing calculator, it seems that $g(n, d) = \left(\frac{2}{d}\right)^d$ is such a function and seems very tight. link to graph I don't need it to be tight. I can make do with any other decreasing function that allows me to reach my goal. (I can even do away with the bounding function altogether so long as I can reach my goal, but I still am curious about how one can bound a function like $f$ .)","I have this function at hand with 3 non-negative integer parameters that I would like to put a bound to: My end goal is to show that for any positive there is a and such that for all and , . In simpler terms, I want to show that can be made arbitrarily small for any larger than a constant by affixing to be another large-enough constant. To do this, I am looking for a decreasing function which satisfies for all . Playing around with in Desmos's graphing calculator, it seems that is such a function and seems very tight. link to graph I don't need it to be tight. I can make do with any other decreasing function that allows me to reach my goal. (I can even do away with the bounding function altogether so long as I can reach my goal, but I still am curious about how one can bound a function like .)","f(n, b, d) = 1 - \left(1 - \left(\frac1n\right)^{b+d-1}\right)^{\left(n^{b-1}\right)} \varepsilon_0 n_0 d^* n \ge n_0 b > 0 f(n, b, d^*) < \varepsilon_0 f n d g(n, d) f(n, b, d) \le g(n, d) b f g(n, d) = \left(\frac{2}{d}\right)^d f","['multivariable-calculus', 'upper-lower-bounds', 'discrete-calculus']"
77,How to show that $\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1$,How to show that,\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1,"I am trying to show the stability of the Crank-Nicholson schema for the heat equation in 2 spatial dimensions, using the Von Neumann's stability analysis. After doing the algebra I was left with showing that $\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1$ . Note that $\lambda>0$ . My reasoning. Consider the function $f(x, y)= \cos k_{m}\Delta x+\sin q_{m}\Delta y$ , its critical points are $(0,0)$ and $(\pi, \pi)$ . Note that $(0, 0)$ minimizes the denominator. Evaluating the numerator at $(0,0)$ the expression reduces to $1\leq 1$ and evaluating at $(\pi, \pi)$ it simplifies to $\mid 1- 4\lambda\mid \leq 1$ . I was wondering if you could provide some feedback on this. Is there a more rigorous way to go about it? Thanks in advance!","I am trying to show the stability of the Crank-Nicholson schema for the heat equation in 2 spatial dimensions, using the Von Neumann's stability analysis. After doing the algebra I was left with showing that . Note that . My reasoning. Consider the function , its critical points are and . Note that minimizes the denominator. Evaluating the numerator at the expression reduces to and evaluating at it simplifies to . I was wondering if you could provide some feedback on this. Is there a more rigorous way to go about it? Thanks in advance!","\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1 \lambda>0 f(x, y)= \cos k_{m}\Delta x+\sin q_{m}\Delta y (0,0) (\pi, \pi) (0, 0) (0,0) 1\leq 1 (\pi, \pi) \mid 1- 4\lambda\mid \leq 1","['multivariable-calculus', 'partial-differential-equations', 'numerical-methods']"
78,"Minimizing $f(t,s) = a \cos(t) + b \cos(s) + c \cos(t - s) $",Minimizing,"f(t,s) = a \cos(t) + b \cos(s) + c \cos(t - s) ","I am trying to minimize $$ f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s) $$ Where $a, b, c \gt 0 $ . My effort: Find the partial derivatives of $f$ with respect to $t$ and $s$ and equate them to zero: $$f_t = - a \sin(t) - c \sin(t - s) = 0 $$ $$f_s = - b \sin(s) + c \sin(t - s) = 0 $$ It follows from these two equations that $$ - a \sin(t) = b \sin(s) $$ Now, $$\cos^2(t) = 1 - \sin^2(t) = 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 $$ We now have: $$ c \sin(t - s) = - a \sin(t) $$ Expanding the left hand side, $$ c \bigg( \sin(t) \cos(s) - \cos(t) \sin(s) \bigg) = - a \sin(t) $$ Eliminate $\sin(t) $ and $\cos(t) $ , and substitute for them in terms of $\sin(s)$ : $$ c \bigg( - \dfrac{b}{a} \sin(s) \cos(s) - \sin(s) \left( \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2} \right) \bigg) = b \sin(s) $$ This implies that either $ \sin(s) = 0 $ , in which case $\sin(t) = 0 $ as well, and the value of the function in this case will be $ a + b + c $ , which is obviously the maximum, or it is non-zero, and that $$ c \bigg( - \dfrac{b}{a} \cos(s) \mp \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } \bigg) = b $$ So that $$    \dfrac{b}{a} \cos(s) \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } = -\dfrac{b}{c} $$ Re-arranging and squaring $$ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 = \left(\dfrac{b}{c}\right)^2 + \left( \dfrac{b}{a} \cos(s) \right)^2 + 2 \left( \dfrac{b^2}{ac} \cos(s) \right) $$ And this simplifies to $$ 1 - \left( \dfrac{b}{a} \right)^2 = \left( \dfrac{b}{c} \right)^2 +2 \left( \dfrac{b^2}{ac} \cos(s) \right) $$ Multiplying through by $ a^2 c^2 $ $$ a^2 c^2 - b^2 c^2 = a^2 b^2 + 2  a b^2 c \cos(s) $$ From which we have $$ \cos(s) =  \dfrac{ a^2 c^2 - b^2 c^2 - a^2 b^2 }{ 2 a b^2 c } $$ And this can be written as $$ \cos(s) = \dfrac{1}{2 b} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right)$$ From symmetry of the function $f(t,s) $ this implies that $$ \cos(t) = \dfrac{1}{2a} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right)$$ It follows that $$ \sin^2(s)= \dfrac{1}{4 b^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right) $$ And $$ \sin^2(t) = \dfrac{1}{4 a^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right) $$ We now have $$ \cos(t - s) = \cos(t) \cos(s) + \sin(t) \sin(s) $$ $$ \cos(t) \cos(s) = \dfrac{1}{4 a b} \left( \left( \dfrac{a b}{c} \right)^2 - \left( \dfrac{a c}{b} \right)^2 - \left( \dfrac{b c}{a} \right)^2 + 2 c^2 \right) $$ $$ \sin(t) \sin(s) = -\dfrac{1}{4 a b} \left( 2 a^2 + 2 b^2 + 2 c^2 - \left( \dfrac{a c }{b} \right)^2 - \left( \dfrac{ b c }{a } \right)^2 - \left( \dfrac{a b }{c} \right)^2 \right)$$ Adding the two expressions $$ \cos(t - s) = \dfrac{1}{2 a b} \left( - a^2 - b^2 + \left(\dfrac{a b }{c}\right)^2 \right) $$ Now $$f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s) $$ Substituting the above expressions for $\cos(t), \cos(s)$ and $\cos(t - s) $ gives us $$ f^* = \dfrac{1}{2} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right) + \dfrac{1}{2} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right) + \dfrac{1}{2} \left( - \dfrac{ a c }{b} - \dfrac{b c }{a} + \dfrac{a b }{c} \right) $$ And this reduces to $$ f^* = - \dfrac{1}{2} \left( \dfrac{ab}{c} + \dfrac{ac}{b} + \dfrac{bc}{a} \right) $$ So in conclusion, the maximum is $ a + b + c $ and the minimum is given by the expression for $f^*$ . My question is:  Are my findings correct ? Any helpful comments, suggestions, and answers are highly appreciated. EDIT: After a comment by Andrei, I found that a necessary condition for the formula for $f^*$ to work is that $$ c \ge \dfrac{a b}{a + b} , \ a \ge \dfrac{b c}{b+c} , \ b \ge \dfrac{a c}{a + c} $$","I am trying to minimize Where . My effort: Find the partial derivatives of with respect to and and equate them to zero: It follows from these two equations that Now, We now have: Expanding the left hand side, Eliminate and , and substitute for them in terms of : This implies that either , in which case as well, and the value of the function in this case will be , which is obviously the maximum, or it is non-zero, and that So that Re-arranging and squaring And this simplifies to Multiplying through by From which we have And this can be written as From symmetry of the function this implies that It follows that And We now have Adding the two expressions Now Substituting the above expressions for and gives us And this reduces to So in conclusion, the maximum is and the minimum is given by the expression for . My question is:  Are my findings correct ? Any helpful comments, suggestions, and answers are highly appreciated. EDIT: After a comment by Andrei, I found that a necessary condition for the formula for to work is that"," f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s)  a, b, c \gt 0  f t s f_t = - a \sin(t) - c \sin(t - s) = 0  f_s = - b \sin(s) + c \sin(t - s) = 0   - a \sin(t) = b \sin(s)  \cos^2(t) = 1 - \sin^2(t) = 1 - \left( \dfrac{b}{a} \sin(s) \right)^2   c \sin(t - s) = - a \sin(t)   c \bigg( \sin(t) \cos(s) - \cos(t) \sin(s) \bigg) = - a \sin(t)  \sin(t)  \cos(t)  \sin(s)  c \bigg( - \dfrac{b}{a} \sin(s) \cos(s) - \sin(s) \left( \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2} \right) \bigg) = b \sin(s)   \sin(s) = 0  \sin(t) = 0   a + b + c   c \bigg( - \dfrac{b}{a} \cos(s) \mp \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } \bigg) = b      \dfrac{b}{a} \cos(s) \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } = -\dfrac{b}{c}   1 - \left( \dfrac{b}{a} \sin(s) \right)^2 = \left(\dfrac{b}{c}\right)^2 + \left( \dfrac{b}{a} \cos(s) \right)^2 + 2 \left( \dfrac{b^2}{ac} \cos(s) \right)   1 - \left( \dfrac{b}{a} \right)^2 = \left( \dfrac{b}{c} \right)^2 +2 \left( \dfrac{b^2}{ac} \cos(s) \right)   a^2 c^2   a^2 c^2 - b^2 c^2 = a^2 b^2 + 2  a b^2 c \cos(s)   \cos(s) =  \dfrac{ a^2 c^2 - b^2 c^2 - a^2 b^2 }{ 2 a b^2 c }   \cos(s) = \dfrac{1}{2 b} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right) f(t,s)   \cos(t) = \dfrac{1}{2a} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right)  \sin^2(s)= \dfrac{1}{4 b^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right)   \sin^2(t) = \dfrac{1}{4 a^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right)   \cos(t - s) = \cos(t) \cos(s) + \sin(t) \sin(s)   \cos(t) \cos(s) = \dfrac{1}{4 a b} \left( \left( \dfrac{a b}{c} \right)^2 - \left( \dfrac{a c}{b} \right)^2 - \left( \dfrac{b c}{a} \right)^2 + 2 c^2 \right)   \sin(t) \sin(s) = -\dfrac{1}{4 a b} \left( 2 a^2 + 2 b^2 + 2 c^2 - \left( \dfrac{a c }{b} \right)^2 - \left( \dfrac{ b c }{a } \right)^2 - \left( \dfrac{a b }{c} \right)^2 \right)  \cos(t - s) = \dfrac{1}{2 a b} \left( - a^2 - b^2 + \left(\dfrac{a b }{c}\right)^2 \right)  f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s)  \cos(t), \cos(s) \cos(t - s)   f^* = \dfrac{1}{2} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right) + \dfrac{1}{2} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right) + \dfrac{1}{2} \left( - \dfrac{ a c }{b} - \dfrac{b c }{a} + \dfrac{a b }{c} \right)   f^* = - \dfrac{1}{2} \left( \dfrac{ab}{c} + \dfrac{ac}{b} + \dfrac{bc}{a} \right)   a + b + c  f^* f^*  c \ge \dfrac{a b}{a + b} , \ a \ge \dfrac{b c}{b+c} , \ b \ge \dfrac{a c}{a + c} ","['calculus', 'multivariable-calculus', 'trigonometry']"
79,The Sub Gradient and the Proximal Operator of ${L}_1$ Norm with Metric,The Sub Gradient and the Proximal Operator of  Norm with Metric,{L}_1,"I want to solve the optimization: $$\arg\underset{x}{\min}f(x) = \arg\underset{x}{\min}\lambda\lVert Mx\rVert_1 + \frac{1}{2}\lVert x-y\rVert_2^2$$ Where $x,y\in\mathbb{R}^{n}$ and $M\in\mathbb{R}^{n\times n}$ (a positive semi-definite matrix representing a metric in my case). Taking n=2 as example, after trying to develop the sub gradient of this target function I got (for component $x_1$ ) : $$\frac{\partial{f}}{\partial{x_1}}=\sum_{i=1}^2 m_{i1}\ \text{sign}(m_{i1}x_1 + m_{i2}x_2)+(x_1-y_1)$$ By following the derivation of $L_1$ regularized least squares (ref. https://angms.science/doc/CVX/ISTA0.pdf ), we can draw polylines in $x_1x_2$ plane for a $y$ value, and finally find the intersection point where $\vec{0}\in\frac{\partial{f}}{\partial{x}}$ . But this method is hard to be generalized to 3D and higher dimension. I'm looking forward to some general solutions. Thanks a lot!","I want to solve the optimization: Where and (a positive semi-definite matrix representing a metric in my case). Taking n=2 as example, after trying to develop the sub gradient of this target function I got (for component ) : By following the derivation of regularized least squares (ref. https://angms.science/doc/CVX/ISTA0.pdf ), we can draw polylines in plane for a value, and finally find the intersection point where . But this method is hard to be generalized to 3D and higher dimension. I'm looking forward to some general solutions. Thanks a lot!","\arg\underset{x}{\min}f(x) = \arg\underset{x}{\min}\lambda\lVert Mx\rVert_1 + \frac{1}{2}\lVert x-y\rVert_2^2 x,y\in\mathbb{R}^{n} M\in\mathbb{R}^{n\times n} x_1 \frac{\partial{f}}{\partial{x_1}}=\sum_{i=1}^2 m_{i1}\ \text{sign}(m_{i1}x_1 + m_{i2}x_2)+(x_1-y_1) L_1 x_1x_2 y \vec{0}\in\frac{\partial{f}}{\partial{x}}","['linear-algebra', 'multivariable-calculus', 'convex-optimization', 'proximal-operators']"
80,"Calculate $\int \limits_{A}^{ }\frac{y}{x^3}\, dy\, dx$ with $A=\{(x,y)\in \mathbb{R}^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}$, coordinate-transfo.","Calculate  with , coordinate-transfo.","\int \limits_{A}^{ }\frac{y}{x^3}\, dy\, dx A=\{(x,y)\in \mathbb{R}^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}","Calculate $\int \limits_{A}^{ }\frac{y}{x^3}\, dy\, dx$ with $A=\{(x,y)\in (\mathbb{R^+})^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}$ . Define new coordinates $u=xy\,v=\frac yx$ . Now define $B:=[1,2]^2$ and $h: A\to B, (x,y)\mapsto (u,v)=(xy,\frac yx)$ It is known that $h$ is bijective and continuously differentiable on $B$ . Now The following theorem $(*)$ gets used: ""Let $U\subset \mathbb{R}^n$ be open, $f:U\to \mathbb{R}^n$ continuously differentiable (a $C^1$ -function) and $a \in U$ . If $J_f(a)$ is invertable then $f$ is a local $C^1$ -diffeomorphism. "" Now $J_h((x,y))=\begin{pmatrix} y & x \\ \frac{-y}{x^2} & \frac{1}{x} \end{pmatrix}\Rightarrow  \det \begin{pmatrix} y & x \\ \frac{-y}{x^2} & \frac{1}{x} \end{pmatrix}=2\frac{y}{x} >0$ for every $(x,y)\in A$ So $J_h$ is ivertable. First Question In the solution to the problem it is stated that $h$ satifys the conditions of $(*)$ and is a $C^1$ -diffeomorphism but $A$ is not an open set. Why is $(*)$ applicable? Next step: Define $g=h^{-1}$ . Now one can apply the transformation formula: $\int \limits_{g(B)}^{}f(x,y)d(x,y)=\int \limits_{B}^{ }f(g(u,v))\cdot|\det J_g(u,v)|d(u,v)$ Now the second question: It is stated that the following is true: $f(g(u,v))\cdot|\det J_g(u,v)|= f(x,y)\cdot \frac{1}{|\det J_h(u,v)|}$ Do you have an idea, what the justification could be? Important Edit I made a mistake. I wrote $A=\{(x,y)\in \mathbb{R}^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}$ instead of $A=\{(x,y)\in (\mathbb{R^+})^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}$ !","Calculate with . Define new coordinates . Now define and It is known that is bijective and continuously differentiable on . Now The following theorem gets used: ""Let be open, continuously differentiable (a -function) and . If is invertable then is a local -diffeomorphism. "" Now for every So is ivertable. First Question In the solution to the problem it is stated that satifys the conditions of and is a -diffeomorphism but is not an open set. Why is applicable? Next step: Define . Now one can apply the transformation formula: Now the second question: It is stated that the following is true: Do you have an idea, what the justification could be? Important Edit I made a mistake. I wrote instead of !","\int \limits_{A}^{ }\frac{y}{x^3}\, dy\, dx A=\{(x,y)\in (\mathbb{R^+})^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\} u=xy\,v=\frac yx B:=[1,2]^2 h: A\to B, (x,y)\mapsto (u,v)=(xy,\frac yx) h B (*) U\subset \mathbb{R}^n f:U\to \mathbb{R}^n C^1 a \in U J_f(a) f C^1 J_h((x,y))=\begin{pmatrix} y & x \\ \frac{-y}{x^2} & \frac{1}{x} \end{pmatrix}\Rightarrow  \det \begin{pmatrix} y & x \\ \frac{-y}{x^2} & \frac{1}{x} \end{pmatrix}=2\frac{y}{x} >0 (x,y)\in A J_h h (*) C^1 A (*) g=h^{-1} \int \limits_{g(B)}^{}f(x,y)d(x,y)=\int \limits_{B}^{ }f(g(u,v))\cdot|\det J_g(u,v)|d(u,v) f(g(u,v))\cdot|\det J_g(u,v)|= f(x,y)\cdot \frac{1}{|\det J_h(u,v)|} A=\{(x,y)\in \mathbb{R}^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\} A=\{(x,y)\in (\mathbb{R^+})^2|1\leq xy\leq 2,\, 1\leq \frac xy\leq 2\}","['integration', 'multivariable-calculus', 'inverse', 'diffeomorphism', 'inverse-function-theorem']"
81,May the proof of Calculus on Manifolds Theorem 3-11 (on compact sets) be simplified as follows?,May the proof of Calculus on Manifolds Theorem 3-11 (on compact sets) be simplified as follows?,,"Theorem: Let $A\subseteq \mathbb{R}^d$ be compact, and let $O$ be an open cover of $A$ . Then there is a partition of unity subordinate to $O$ . Below I give Spivak's proof and my slight simplification. Is my proof (the simplified version of Spivak's) correct? Spivak's proof: My Proof: Let $U_1,\ldots, U_n$ be a finite subcover of $A$ in $O$ . By Lemma 1 there are compact sets $D_i$ such that $$A\subseteq \bigcup_{i=1}^nD_i^{\circ}  \ \ \ \ \text{and} \ \ \ \  D_i\subseteq U_i.$$ Let $U:=U_1\cup \ldots \cup U_n$ . By Lemma 2 there are functions $\psi_i:U\to[0,1]$ so that $\psi_i$ is $C^{\infty}$ . $\psi_i$ is $1$ on $D_i$ . $\psi_i$ is $0$ outside of some closed set contained in $U_i$ . Define $$\phi_i : U\to [0,1] : x\mapsto \frac{\psi_i(x)}{\psi_1(x) + \ldots + \psi_n(x)}.$$ The partition of unity $\Phi:=\{\phi_1,\ldots ,\phi_n\}$ is subordinate to $O$ . Lemma 1 (akin to Spivak's Problem 1-22): Let $U$ be open and $K\subseteq U$ compact. Then there is a compact set $D$ such that $$K\subseteq D^{\circ}  \ \ \ \ \text{and} \ \ \ \  D\subseteq U.$$ Let $\{U_1, \ldots , U_n\}$ be an open cover of the compact set $K$ . Then there are compact sets $D_i$ such that $$K \subseteq \bigcup_{i=1}^nD_i^{\circ} \ \ \ \ \text{and} \ \ \ \  D_i\subseteq U_i.$$ Lemma 2 (akin to Spivak's Problem 2-26): Let $U$ be open and $D\subseteq U$ be compact. Then there is a function $f:\mathbb{R}^n\to\mathbb{R}^n$ such that $f$ is $C^{\infty}$ . $f$ is $1$ on $D$ . $f$ is $0$ outside of some closed set contained in $U$ .","Theorem: Let be compact, and let be an open cover of . Then there is a partition of unity subordinate to . Below I give Spivak's proof and my slight simplification. Is my proof (the simplified version of Spivak's) correct? Spivak's proof: My Proof: Let be a finite subcover of in . By Lemma 1 there are compact sets such that Let . By Lemma 2 there are functions so that is . is on . is outside of some closed set contained in . Define The partition of unity is subordinate to . Lemma 1 (akin to Spivak's Problem 1-22): Let be open and compact. Then there is a compact set such that Let be an open cover of the compact set . Then there are compact sets such that Lemma 2 (akin to Spivak's Problem 2-26): Let be open and be compact. Then there is a function such that is . is on . is outside of some closed set contained in .","A\subseteq \mathbb{R}^d O A O U_1,\ldots, U_n A O D_i A\subseteq \bigcup_{i=1}^nD_i^{\circ} 
\ \ \ \ \text{and} \ \ \ \ 
D_i\subseteq U_i. U:=U_1\cup \ldots \cup U_n \psi_i:U\to[0,1] \psi_i C^{\infty} \psi_i 1 D_i \psi_i 0 U_i \phi_i : U\to [0,1] : x\mapsto \frac{\psi_i(x)}{\psi_1(x) + \ldots + \psi_n(x)}. \Phi:=\{\phi_1,\ldots ,\phi_n\} O U K\subseteq U D K\subseteq D^{\circ}
 \ \ \ \ \text{and} \ \ \ \ 
D\subseteq U. \{U_1, \ldots , U_n\} K D_i K \subseteq \bigcup_{i=1}^nD_i^{\circ}
\ \ \ \ \text{and} \ \ \ \ 
D_i\subseteq U_i. U D\subseteq U f:\mathbb{R}^n\to\mathbb{R}^n f C^{\infty} f 1 D f 0 U","['real-analysis', 'multivariable-calculus', 'solution-verification', 'alternative-proof']"
82,Flux through surface,Flux through surface,,"Can someone help/correct me in my thinking when working calculating the flux through a surface. Given is the following example: When a vector field is described by: $$ \vec{E} = xz\,\hat{\imath} - yz\,\hat{\jmath} + (x^2 + y^2)\,\hat{k}$$ And a circular disk through which we want to calculate the flux is described by: $$ D \rightarrow\; z=0,\, x^2+y^2 \leq a^2 $$ I am aware that in order to calculate the flux through the surface we need a vector perpendicular to the surface of the disk to evaluate the integral. Since $z=0$ , a unit normal vector is given by $\hat{n} = 0\,\hat{\imath} + 0\,\hat{\jmath} + 1\,\hat{k}$ this could be used. Right? We are taught that another way to determine a vector perpendicular to the surface $D$ is by parametrizing the surface $D$ , as below for example, and then determining $\frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta}$ . $$ D\rightarrow\;  r(\rho, \theta)  = \rho \cos(\theta)\,\hat{\imath} + \rho\sin(\theta)\,\hat{\jmath} + 0\,\hat{k} $$ with $\rho \in [0, a] \land \theta \in [0, 2\pi]$ . Is it correct that the flux is then given by (question mark indicating I'm not sure about my understanding, and I might have gone wrong somewhere way earlier): $$ \iint_D \vec{B} \cdot d \vec{S} \;\overset{?}{=}\, \iint_D \vec{B}(\vec{r}(\rho, \theta)) \cdot  \biggl( \frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta} \biggr) \, dA $$ The length of this normal vector does matter right. So how has this been taken into account for in the integral with the cross product? Is it because you substitute the parameterization of the surface into the vector field?","Can someone help/correct me in my thinking when working calculating the flux through a surface. Given is the following example: When a vector field is described by: And a circular disk through which we want to calculate the flux is described by: I am aware that in order to calculate the flux through the surface we need a vector perpendicular to the surface of the disk to evaluate the integral. Since , a unit normal vector is given by this could be used. Right? We are taught that another way to determine a vector perpendicular to the surface is by parametrizing the surface , as below for example, and then determining . with . Is it correct that the flux is then given by (question mark indicating I'm not sure about my understanding, and I might have gone wrong somewhere way earlier): The length of this normal vector does matter right. So how has this been taken into account for in the integral with the cross product? Is it because you substitute the parameterization of the surface into the vector field?","
\vec{E} = xz\,\hat{\imath} - yz\,\hat{\jmath} + (x^2 + y^2)\,\hat{k} 
D \rightarrow\; z=0,\, x^2+y^2 \leq a^2
 z=0 \hat{n} = 0\,\hat{\imath} + 0\,\hat{\jmath} + 1\,\hat{k} D D \frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta} 
D\rightarrow\; 
r(\rho, \theta) 
= \rho \cos(\theta)\,\hat{\imath} + \rho\sin(\theta)\,\hat{\jmath} + 0\,\hat{k}
 \rho \in [0, a] \land \theta \in [0, 2\pi] 
\iint_D \vec{B} \cdot d \vec{S} \;\overset{?}{=}\, \iint_D \vec{B}(\vec{r}(\rho, \theta)) \cdot 
\biggl( \frac{\partial r}{\partial \rho} \times \frac{\partial r}{\partial \theta} \biggr) \, dA
","['calculus', 'multivariable-calculus', 'vector-analysis', 'polar-coordinates']"
83,Evaluation of a multi-variable integral,Evaluation of a multi-variable integral,,"I was reading this answer: Fourier transform of the indicator of the unit ball and couldn't really follow the computations in the answer. Specifically the equality $$\int_{\|x\| \leq 1} e^{-ix_n\rho} dx_1 \dots dx_n = \int_{-1}^1 (1−x_n^2)^{(n−1)/2} \alpha_{n−1} e^{-ix_n\rho} dx_n$$ where $\rho >0$ and $\alpha_{n-1}$ is the volume of the unit ball in $\mathbb{R}^{n-1}$ . The computation looks a lot like the ones in https://en.wikipedia.org/wiki/Volume_of_an_n-ball#The_one-dimension_recursion_formula where a recursion formula for the volume of the unit ball is derived, but I can't really wrap my head around it. Could anyone help me understand what is going on here?","I was reading this answer: Fourier transform of the indicator of the unit ball and couldn't really follow the computations in the answer. Specifically the equality where and is the volume of the unit ball in . The computation looks a lot like the ones in https://en.wikipedia.org/wiki/Volume_of_an_n-ball#The_one-dimension_recursion_formula where a recursion formula for the volume of the unit ball is derived, but I can't really wrap my head around it. Could anyone help me understand what is going on here?",\int_{\|x\| \leq 1} e^{-ix_n\rho} dx_1 \dots dx_n = \int_{-1}^1 (1−x_n^2)^{(n−1)/2} \alpha_{n−1} e^{-ix_n\rho} dx_n \rho >0 \alpha_{n-1} \mathbb{R}^{n-1},"['integration', 'multivariable-calculus', 'definite-integrals']"
84,Is Ted Shifrin's definition (8-2.3) missing the verbalization of the pullback of the product of a function with a basis?,Is Ted Shifrin's definition (8-2.3) missing the verbalization of the pullback of the product of a function with a basis?,,"My question pertains to Ted Shifrin's Multivariable Mathematics Section 8-2.3 Pullbacks. We are to use the following pieces, given in the definition, to build the pullback $\mathbf{g}^{*}\omega\in\mathcal{A}^{k}\left(U\right)$ of $\omega$ by $\mathbf{g}.$ The symbol $\mathbb{I}$ is a multi-index. The last four lines are definitive, giving the pullback of a function; the pullback of a basis 1-form; the pullback of a wedge product; and the pullback of a sum, respectively: \begin{align*} U\subset & \mathbb{R}^{m}\\ \mathbf{g}: & U\to\mathbb{R}^{n}\\ f: & \mathbb{R}^{n}\to\mathbb{R}\\ \mathbf{g}\left(\mathbf{u}\right)= & \mathbf{x}\\ \mathbf{g}^{*}f= & f\circ\mathbf{g}\\ \mathbf{g}^{*}dx_{i}= & dg_{i}=\sum_{j=1}^{m}\frac{\partial g_{i}}{\partial u_{j}}du_{j}\\ \mathbf{g}^{*}\left(dx_{i_{1}}\wedge\dots\wedge dx_{i_{k}}\right)= & dg_{i_{1}}\wedge\dots\wedge dg_{i_{k}}=d\mathbf{g}_{\mathbb{I}}\\ \mathbf{g}^{*}\left(\sum_{\mathbb{I}}f_{\mathbb{I}}d\mathbf{x}_{\mathbb{I}}\right)= & \sum_{\mathbb{I}}\left(f_{\mathbb{I}}\circ\mathbf{g}\right)d\mathbf{g}_{\mathbb{I}} \end{align*} Before the final equation he states ""Last, we take the pullback of the sum to be the sum of the pullbacks."" It may seem pedantic, but this is mathematics. The last equation appears to define both the pullback of a sum, as well as the pullback of the product of a scalar function with a basis k-form. I'm left wondering if this is something I should derive from the previous parts of the definition, or it is simply an unverbalized part of the final part of the definition. So which is it? To my way of thinking, that product ""distribution"" rule is the most significant part of the definition.","My question pertains to Ted Shifrin's Multivariable Mathematics Section 8-2.3 Pullbacks. We are to use the following pieces, given in the definition, to build the pullback of by The symbol is a multi-index. The last four lines are definitive, giving the pullback of a function; the pullback of a basis 1-form; the pullback of a wedge product; and the pullback of a sum, respectively: Before the final equation he states ""Last, we take the pullback of the sum to be the sum of the pullbacks."" It may seem pedantic, but this is mathematics. The last equation appears to define both the pullback of a sum, as well as the pullback of the product of a scalar function with a basis k-form. I'm left wondering if this is something I should derive from the previous parts of the definition, or it is simply an unverbalized part of the final part of the definition. So which is it? To my way of thinking, that product ""distribution"" rule is the most significant part of the definition.","\mathbf{g}^{*}\omega\in\mathcal{A}^{k}\left(U\right) \omega \mathbf{g}. \mathbb{I} \begin{align*}
U\subset & \mathbb{R}^{m}\\
\mathbf{g}: & U\to\mathbb{R}^{n}\\
f: & \mathbb{R}^{n}\to\mathbb{R}\\
\mathbf{g}\left(\mathbf{u}\right)= & \mathbf{x}\\
\mathbf{g}^{*}f= & f\circ\mathbf{g}\\
\mathbf{g}^{*}dx_{i}= & dg_{i}=\sum_{j=1}^{m}\frac{\partial g_{i}}{\partial u_{j}}du_{j}\\
\mathbf{g}^{*}\left(dx_{i_{1}}\wedge\dots\wedge dx_{i_{k}}\right)= & dg_{i_{1}}\wedge\dots\wedge dg_{i_{k}}=d\mathbf{g}_{\mathbb{I}}\\
\mathbf{g}^{*}\left(\sum_{\mathbb{I}}f_{\mathbb{I}}d\mathbf{x}_{\mathbb{I}}\right)= & \sum_{\mathbb{I}}\left(f_{\mathbb{I}}\circ\mathbf{g}\right)d\mathbf{g}_{\mathbb{I}}
\end{align*}","['multivariable-calculus', 'definition', 'differential-forms']"
85,"Is my proof of the fact that continuous partials imply differentiability, absolutely rigorous?","Is my proof of the fact that continuous partials imply differentiability, absolutely rigorous?",,"Theorem: Let $V$ , $W$ be finite-dimensional normed linear spaces and $W$ be over $\mathbb R$ . ( $V$ can possibly be over $\mathbb C$ .) Let $(e_1, \ldots, e_n)$ and $(\tilde e_1,\ldots, \tilde e_m)$ be bases for $V$ and $W$ . Let $F\colon \Omega\to W$ where $\Omega$ is open in $V$ such that each of the $\partial_i f_j$ 's (see the note at the end) exist throughout $\Omega$ and are continuous at $c\in\Omega$ . Then $f$ is differentiable at $c$ . My proof: Since differentiability (and the partials) are independent of the norms on either of the spaces, we choose to work with $\lVert\cdot\rVert_1$ norms so that, for instance, $\lVert \sum_i x_i e_i\rVert = \sum_i |x_i|$ in $V$ . Similarly in $W$ . Let $\delta > 0$ such that $B_\delta(c)\subseteq\Omega$ and let $h\in B_\delta(0)\setminus\{0\}$ . Now, define a sequence $x^{(0)}, \ldots, x^{(n)}\in V$ by $x^{(0)} := c$ and $x^{(k+1)} := x^{(k)} + h_{k + 1} e_{k + 1}$ $(h_k$ 's are components of $h$ ). Since we are considering $\lVert\cdot\rVert_1$ norm, each $x^{(k)}\in B_\delta(c)$ . Now, $$ f(c + h) - f(c) = \sum_{k = 1}^n \bigl(f(x^{(k)}) - f(x^{(k-1)})\bigr).\tag{1} $$ Fix a $1\le k\le n$ . Now there exists an $\epsilon > 0$ such that for each $t\in(-\epsilon, 1 + \epsilon)$ , we have $x^{(k-1)} + t h_k e_k\in B_\delta(c)$ . Hence we can define $g\colon (-\epsilon, 1 + \epsilon)\to V$ by $t\mapsto f(x^{(k-1)} + th_k e_k)$ . Now, decomposing $g$ in basis $(\tilde e_1, \ldots, \tilde e_m)$ , we get $g_j$ 's on $(-\epsilon, 1 + \epsilon)\to \mathbb R$ and now we are in a position to apply MVT. Before that we show that each $g_j$ is differentiable: $$ \begin{align*} \lim_{\xi\to 0}\frac{g_j(t + \xi) - g_j(t)}{\xi} & = \lim_{\xi\to 0}\frac{f_j(v + \xi h e_k) - f_j(v)}{\xi}\\ & = h\; \partial_k f_j(v) \end{align*} $$ where $v := x^{(k-1)} + th_k e_k$ . Now, applying MVT on the interval $[0, 1]$ , we get a $\theta\in (0, 1)$ such that $$ \begin{align*} g_j(1) - g_j(0) & = g'_j(\theta)\\ & = h\; \partial_k f_j(x^{(k-1)} + \theta h_k e_k). \tag{2} \end{align*} $$ Now using the continuity of the partials, we can assume w.l.o.g. that $\delta$ is small enough so that $|\partial_k f_j(c + h) - \partial_k f_j(c)|\le \epsilon/m$ for all $k$ , $j$ . Since $x^{(k - 1)} + \theta h e_k\in B_\delta(c)$ (easy), (2) yields $$ | g_j(1) - g_j(0) - h_k \; \partial_k f_j(c) |\le |h_k| \epsilon/m $$ for all $j$ . Summing the $g_i$ 's to yield $g$ and noting that $g(1) - g(0) = f(x^{(k)}) - f(x^{(k-1)})$ , we get $$ \Big\lVert f(x^{(k)}) - f(x^{(k-1)}) - \sum_{j = 1}^m h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert\le |h_k|\epsilon. $$ Now using (1) and triangle inequality, we get $$ \Big\lVert f(x) - f(c) - \sum_{j = 1}^m\sum_{k = 1}^n h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert \le\epsilon\lVert h\rVert $$ and we conclude by noting that the double sum is (the obviously given linear map $V\to W$ ) acting on $h$ . By $\partial_i f_j(c)$ , I mean the following limit: $$ \lim_{t\to 0}\frac{f_j(c + t e_i) - f_j(c)}{t} $$ $f_j$ 's on $\Omega\to\mathbb R$ are components of $f\colon\Omega\to W$ in $W$ 's basis.","Theorem: Let , be finite-dimensional normed linear spaces and be over . ( can possibly be over .) Let and be bases for and . Let where is open in such that each of the 's (see the note at the end) exist throughout and are continuous at . Then is differentiable at . My proof: Since differentiability (and the partials) are independent of the norms on either of the spaces, we choose to work with norms so that, for instance, in . Similarly in . Let such that and let . Now, define a sequence by and 's are components of ). Since we are considering norm, each . Now, Fix a . Now there exists an such that for each , we have . Hence we can define by . Now, decomposing in basis , we get 's on and now we are in a position to apply MVT. Before that we show that each is differentiable: where . Now, applying MVT on the interval , we get a such that Now using the continuity of the partials, we can assume w.l.o.g. that is small enough so that for all , . Since (easy), (2) yields for all . Summing the 's to yield and noting that , we get Now using (1) and triangle inequality, we get and we conclude by noting that the double sum is (the obviously given linear map ) acting on . By , I mean the following limit: 's on are components of in 's basis.","V W W \mathbb R V \mathbb C (e_1, \ldots, e_n) (\tilde e_1,\ldots, \tilde e_m) V W F\colon \Omega\to W \Omega V \partial_i f_j \Omega c\in\Omega f c \lVert\cdot\rVert_1 \lVert \sum_i x_i e_i\rVert = \sum_i |x_i| V W \delta > 0 B_\delta(c)\subseteq\Omega h\in B_\delta(0)\setminus\{0\} x^{(0)}, \ldots, x^{(n)}\in V x^{(0)} := c x^{(k+1)} := x^{(k)} + h_{k + 1} e_{k + 1} (h_k h \lVert\cdot\rVert_1 x^{(k)}\in B_\delta(c) 
f(c + h) - f(c) = \sum_{k = 1}^n \bigl(f(x^{(k)}) - f(x^{(k-1)})\bigr).\tag{1}
 1\le k\le n \epsilon > 0 t\in(-\epsilon, 1 + \epsilon) x^{(k-1)} + t h_k e_k\in B_\delta(c) g\colon (-\epsilon, 1 + \epsilon)\to V t\mapsto f(x^{(k-1)} + th_k e_k) g (\tilde e_1, \ldots, \tilde e_m) g_j (-\epsilon, 1 + \epsilon)\to \mathbb R g_j 
\begin{align*}
\lim_{\xi\to 0}\frac{g_j(t + \xi) - g_j(t)}{\xi}
& = \lim_{\xi\to 0}\frac{f_j(v + \xi h e_k) - f_j(v)}{\xi}\\
& = h\; \partial_k f_j(v)
\end{align*}
 v := x^{(k-1)} + th_k e_k [0, 1] \theta\in (0, 1) 
\begin{align*}
g_j(1) - g_j(0) & = g'_j(\theta)\\
& = h\; \partial_k f_j(x^{(k-1)} + \theta h_k e_k). \tag{2}
\end{align*}
 \delta |\partial_k f_j(c + h) - \partial_k f_j(c)|\le \epsilon/m k j x^{(k - 1)} + \theta h e_k\in B_\delta(c) 
| g_j(1) - g_j(0) - h_k \; \partial_k f_j(c) |\le |h_k| \epsilon/m
 j g_i g g(1) - g(0) = f(x^{(k)}) - f(x^{(k-1)}) 
\Big\lVert f(x^{(k)}) - f(x^{(k-1)}) - \sum_{j = 1}^m h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert\le |h_k|\epsilon.
 
\Big\lVert f(x) - f(c) - \sum_{j = 1}^m\sum_{k = 1}^n h_k\; \partial_k f_j(c)\; \tilde e_j \Big\rVert \le\epsilon\lVert h\rVert
 V\to W h \partial_i f_j(c) 
\lim_{t\to 0}\frac{f_j(c + t e_i) - f_j(c)}{t}
 f_j \Omega\to\mathbb R f\colon\Omega\to W W","['multivariable-calculus', 'solution-verification', 'normed-spaces', 'partial-derivative']"
86,"Find the volume of the following region $E= (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) $",Find the volume of the following region,"E= (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) ","Find the volume of the following region $E= \{ (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) \}  $ I figured that the region E is formed by the points that belong in the gap between the two paraboloids $z=\sqrt{2}(x^2 + y^2)$ $\hspace{20pt}$ and $\hspace{20pt}$ $ z= \sqrt{6}(x^2 + y^2)$ that are as well inside of the sphere $x^2 + y^2 + z^2 = 1$ . $E$ can be obtained by making a whole rotation around the $z$ -axis in the plane- $xy$ then using Guldino's theorem can be appropriate. Considering now cylindrical coordinates we've that $E$ becomes the following: $F= \{ (\rho, \theta, z): \rho^2 + z^2 ≤1, \sqrt{2}\rho^2 ≤z≤ \sqrt{6}\rho^2,  0≤ \theta≤ 2\pi \}$ Then we've that: $vol \hspace{2pt} E = \int _{F} \rho \hspace{2pt} d\rho d\theta dz = 2\pi \int\int _{D}  \rho d\rho dz $ I'm struggling to find the region $D$ . Any hint?",Find the volume of the following region I figured that the region E is formed by the points that belong in the gap between the two paraboloids and that are as well inside of the sphere . can be obtained by making a whole rotation around the -axis in the plane- then using Guldino's theorem can be appropriate. Considering now cylindrical coordinates we've that becomes the following: Then we've that: I'm struggling to find the region . Any hint?,"E= \{ (x,y,z) \in \mathbb{R}^3 : x^2 + y^2 + z^2 ≤1, \sqrt{2}(x^2 + y^2) ≤z≤ \sqrt{6}(x^2 + y^2) \}   z=\sqrt{2}(x^2 + y^2) \hspace{20pt} \hspace{20pt}  z= \sqrt{6}(x^2 + y^2) x^2 + y^2 + z^2 = 1 E z xy E F= \{ (\rho, \theta, z): \rho^2 + z^2 ≤1, \sqrt{2}\rho^2 ≤z≤ \sqrt{6}\rho^2,  0≤ \theta≤ 2\pi \} vol \hspace{2pt} E = \int _{F} \rho \hspace{2pt} d\rho d\theta dz = 2\pi \int\int _{D}  \rho d\rho dz  D","['integration', 'multivariable-calculus']"
87,Calculate $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$,Calculate,\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx,"Calculate $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$ I'd like to exchange the integral with the serie in order to do the calculus. I'm not sure under which conditions I can do the following (and why is permitted theoretically): $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx$ = $ \int_{0}^{1}\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \frac{x^{n+1}}{x+1}dx$ Because then, knowing that $\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} x^{n+1} = ln(1+x)$ solving the integral becomes simpler.","Calculate I'd like to exchange the integral with the serie in order to do the calculus. I'm not sure under which conditions I can do the following (and why is permitted theoretically): = Because then, knowing that solving the integral becomes simpler.",\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx \sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \int_{0}^{1}\frac{x^{n+1}}{x+1}dx  \int_{0}^{1}\sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} \frac{x^{n+1}}{x+1}dx \sum_{n=0}^\infty \frac{(-1)^{n}}{n+1} x^{n+1} = ln(1+x),"['calculus', 'sequences-and-series', 'multivariable-calculus']"
88,"Using the implicit function theorem for a function $f(x,y,z)$ , what if $\frac{\partial f}{\partial (y,z)} $ is not a square matrix?","Using the implicit function theorem for a function  , what if  is not a square matrix?","f(x,y,z) \frac{\partial f}{\partial (y,z)} ","I want to use the implicit function theorem for a function $f:\mathbb{R}^3\rightarrow \mathbb{R}$ . The set $M\subset \mathbb{R}^3$ is defined by the equation $f(x,y,z)=\text{function rule}=0$ for every $(x,y,z) \in M$ . I need to show that the equation $f(x,y,z)=0$ is locally solvable for one of the variables $x,y,z$ at every point of a set $M\subset \mathbb{R}^3$ i.e. if there exist a neighborhood $U\subset \mathbb{R}$ of $x$ and a continuous differentiable function $g: U\rightarrow \mathbb{R}^2$ with $g(x)=(y,z)$ and $f(x,g(x))=0$ for every $x\in U$ The best tool I have for this is the implicit function theorem : But if I calculate $$\frac{\partial f}{\partial (y,z)} =\begin{pmatrix}\frac{\partial f}{\partial (y)} (x,y,z)&,&\frac{\partial f}{\partial (z)} (x,y,z)\end{pmatrix}$$ I get a $1\times 2$ -Matrix of the form , so the Matrix cannot be invertable because $\frac{\partial f}{\partial (y,z)} $ is not a square matrix. It would only make sense to me if $\frac{\partial f}{\partial (z)}(x,y,z)=0$ for every $(x,y,z)\in M$ but that is not the case. Did I misunderstand the exercise and/or the meaning of the IFT? $(*)$ The function $f$ is part of a homework exercise. I don't want to post the function rule here because my question is more general and I have some reservations about posting homework questions online. I hope my question is still clear.","I want to use the implicit function theorem for a function . The set is defined by the equation for every . I need to show that the equation is locally solvable for one of the variables at every point of a set i.e. if there exist a neighborhood of and a continuous differentiable function with and for every The best tool I have for this is the implicit function theorem : But if I calculate I get a -Matrix of the form , so the Matrix cannot be invertable because is not a square matrix. It would only make sense to me if for every but that is not the case. Did I misunderstand the exercise and/or the meaning of the IFT? The function is part of a homework exercise. I don't want to post the function rule here because my question is more general and I have some reservations about posting homework questions online. I hope my question is still clear.","f:\mathbb{R}^3\rightarrow \mathbb{R} M\subset \mathbb{R}^3 f(x,y,z)=\text{function rule}=0 (x,y,z) \in M f(x,y,z)=0 x,y,z M\subset \mathbb{R}^3 U\subset \mathbb{R} x g: U\rightarrow \mathbb{R}^2 g(x)=(y,z) f(x,g(x))=0 x\in U \frac{\partial f}{\partial (y,z)} =\begin{pmatrix}\frac{\partial f}{\partial (y)} (x,y,z)&,&\frac{\partial f}{\partial (z)} (x,y,z)\end{pmatrix} 1\times 2 \frac{\partial f}{\partial (y,z)}  \frac{\partial f}{\partial (z)}(x,y,z)=0 (x,y,z)\in M (*) f","['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
89,Stuck (I think quite far) on a hard nonlinear optimization problem,Stuck (I think quite far) on a hard nonlinear optimization problem,,"The problem is this: The standard-form of Pringles chips is achieved by creating the hyperbolic paraboloïd $z=\frac{y^2}{4}-\frac{x^2}{2}$ on a domain given by the elliptic plate $x^2+\frac{y^2}{2}\le q^2$ , $q\in\mathbb R_+$ . For a special edition they change the domain to $x^2+y^4-ay^2=1$ . For which values of $a$ does the maximum of $z$ on this curve lie on the $y$ -axis, but the minimum of $z$ on this curve not on the $x$ -axis. I have tried and tried but I can't seem to get past this: $$f(x,y)=\frac{y^2}{4}-\frac{x^2}{2}=\frac{y^2}{4}-\frac{1}{2}(1-y^4+ay^2)=\frac{y^4}{2}+\frac{y^2}{4}-\frac{a}{2}y^2-\frac{1}{2}$$ Then $$\frac{df}{dy}=2y^3+\frac{y}{2}-ay=y(2y^2+\frac{1}{2}-a)=0$$ if $y=0$ or $2y^2=a-\frac{1}{2} \Leftrightarrow y^2=\frac{2a-1}{4}$ $(a\ne1/2)$ . If $y=0$ then $f(y)=-1/2$ and so $x^2=1$ . (probably minima, so then $a\ne1/2$ ) Is this enough? Do I have to use the second derivative test here to show that this is a minima (but then I wouldn't have showed that there aren't other minima, right?)? If $y^2=\frac{2a-1}{4}$ where $a\ne1/2$ then $f(y)=\ldots=\frac{1}{4}(-a^2+a-\frac{17}{32})=\frac{2a-1}{8}-x^2$ $\Leftrightarrow$ $\frac{a^2}{4}+\frac{1}{128}=x^2$ . Do I have to do anything with this?","The problem is this: The standard-form of Pringles chips is achieved by creating the hyperbolic paraboloïd on a domain given by the elliptic plate , . For a special edition they change the domain to . For which values of does the maximum of on this curve lie on the -axis, but the minimum of on this curve not on the -axis. I have tried and tried but I can't seem to get past this: Then if or . If then and so . (probably minima, so then ) Is this enough? Do I have to use the second derivative test here to show that this is a minima (but then I wouldn't have showed that there aren't other minima, right?)? If where then . Do I have to do anything with this?","z=\frac{y^2}{4}-\frac{x^2}{2} x^2+\frac{y^2}{2}\le q^2 q\in\mathbb R_+ x^2+y^4-ay^2=1 a z y z x f(x,y)=\frac{y^2}{4}-\frac{x^2}{2}=\frac{y^2}{4}-\frac{1}{2}(1-y^4+ay^2)=\frac{y^4}{2}+\frac{y^2}{4}-\frac{a}{2}y^2-\frac{1}{2} \frac{df}{dy}=2y^3+\frac{y}{2}-ay=y(2y^2+\frac{1}{2}-a)=0 y=0 2y^2=a-\frac{1}{2} \Leftrightarrow y^2=\frac{2a-1}{4} (a\ne1/2) y=0 f(y)=-1/2 x^2=1 a\ne1/2 y^2=\frac{2a-1}{4} a\ne1/2 f(y)=\ldots=\frac{1}{4}(-a^2+a-\frac{17}{32})=\frac{2a-1}{8}-x^2 \Leftrightarrow \frac{a^2}{4}+\frac{1}{128}=x^2","['multivariable-calculus', 'nonlinear-optimization', 'lagrange-multiplier']"
90,Determining the critical points of a two-variable function,Determining the critical points of a two-variable function,,"Edit So it seems that, if we assume the function is correct, then the suggested solution is most likely wrong. However, I am inclined to think that the question has been tweaked while the answer remained as it is. Thus, as a challenge to myself, I am trying to figure out the original function, assuming the suggested solution is correct. Just putting this out there too since I’ve already made the post; if anyone wants to try (and succeeds), feel free to pen down what the function could be (either in an answer or a comment) :) Question Find all the critical points of $f$ , where $$f(x, y) = \frac 1 2 x^2 - x + ay(x - 1) - \frac 1 3 y^3 - a^2y^2$$ and $a$ is a constant. My working I know that, for a two-variable function, a critical point is one where the partial derivates with respect to both variables is zero. In our case, we have $$\begin{aligned} \frac {\partial{f}} {\partial{x}} & = x - 1 + ay\\[2 mm] & = 0 \end{aligned}$$ and $$\begin{aligned} \frac {\partial{f}} {\partial{y}} & = a(x - 1) - y^2 - 2a^2y\\[2 mm] & = 0. \end{aligned}$$ Now, substituting the first equation into the second gives $$a(-ay) - y^2 -2a^2y = 0$$ which implies that $$y = 0$$ or $$y = -3a^2.$$ Finally, we see that the critical points are $(1, 0)$ and $(1 + 3a^2, -3a^2)$ . Answer The answer, however, is $(0, 0)$ and $(1 - a^3, a^2)$ . Is the suggested answer incorrect? If not, then where have I gone wrong? Any intuitive explanations or suggestions will be greatly appreciated!","Edit So it seems that, if we assume the function is correct, then the suggested solution is most likely wrong. However, I am inclined to think that the question has been tweaked while the answer remained as it is. Thus, as a challenge to myself, I am trying to figure out the original function, assuming the suggested solution is correct. Just putting this out there too since I’ve already made the post; if anyone wants to try (and succeeds), feel free to pen down what the function could be (either in an answer or a comment) :) Question Find all the critical points of , where and is a constant. My working I know that, for a two-variable function, a critical point is one where the partial derivates with respect to both variables is zero. In our case, we have and Now, substituting the first equation into the second gives which implies that or Finally, we see that the critical points are and . Answer The answer, however, is and . Is the suggested answer incorrect? If not, then where have I gone wrong? Any intuitive explanations or suggestions will be greatly appreciated!","f f(x, y) = \frac 1 2 x^2 - x + ay(x - 1) - \frac 1 3 y^3 - a^2y^2 a \begin{aligned}
\frac {\partial{f}} {\partial{x}} & = x - 1 + ay\\[2 mm]
& = 0
\end{aligned} \begin{aligned}
\frac {\partial{f}} {\partial{y}} & = a(x - 1) - y^2 - 2a^2y\\[2 mm]
& = 0.
\end{aligned} a(-ay) - y^2 -2a^2y = 0 y = 0 y = -3a^2. (1, 0) (1 + 3a^2, -3a^2) (0, 0) (1 - a^3, a^2)","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
91,"Can we take the partial derivative of a function $f(x, y, z)$ with respect to $g(x, y, z)$",Can we take the partial derivative of a function  with respect to,"f(x, y, z) g(x, y, z)","In Calculus 1 we differentiated $f(x)$ with respect to $g(x)$ that is, $\frac{df(x)}{dg(x)}$ by using chain rule: $$\frac{df(x)}{dx}\cdot\frac{dx}{dg(x)}= \frac{f'(x)}{g'(x)}$$ Is there something like this in partial derivatives too? For example: $$\frac{\partial f(x,y,z)}{\partial g(x,y,z)}$$ where $f(x, y, z)=xyz$ and $g(x, y, z)=x+y+z$","In Calculus 1 we differentiated with respect to that is, by using chain rule: Is there something like this in partial derivatives too? For example: where and","f(x) g(x) \frac{df(x)}{dg(x)} \frac{df(x)}{dx}\cdot\frac{dx}{dg(x)}= \frac{f'(x)}{g'(x)} \frac{\partial f(x,y,z)}{\partial g(x,y,z)} f(x, y, z)=xyz g(x, y, z)=x+y+z","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
92,Chain rule for matrix derivatives & multiplication of matrix with rank-3-tensor,Chain rule for matrix derivatives & multiplication of matrix with rank-3-tensor,,"While studying, I noticed that in some lectures notes of mine we hand-wavingly use the chain rule formula to design a gradient descent algorithm with respect to some matrix $W \in \mathbb{R}^{m \times n}$ (i.e. the variable we want to minimize is a $m \times n$ matrix). Curious about how one would define the jacobian with respect to a matrix, I stumbled upon this question and how to represent rank-3-tensors as stacked matrices. With this, we define the Jacobian of a function $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l}$ with respect to the variable $W$ as $$\nabla_W f = \left({\frac{\partial f_i}{\partial w_{jk}}}\right)_{ijk} = \left(\left(\frac{\partial f_i}{\partial w_{j k}}\right)_{jk}\right)_{i}$$ Visually, this Jacobian consists of $l$ stacked $m \times n$ matrices each containing the individual $W$ derivatives (but fixing the output components $f_i$ ). With this we can try to prove the chain rule for matrix derivatives. Let $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l}$ and $g: \mathbb{R}^l \rightarrow \mathbb{R}^s$ . The chain rule should be $$J_W(g \circ f) = J(g)(f) \cdot J_W(f)$$ On the one hand, treating a matrix in this multiplication as a ""scalar"" and elementwise-multiplying it with the rank-3-tensor (seems natural in a module-theoretic sense) would consequently yield $$\frac{\partial g_i}{\partial w_{jk}}(f(W)) = (\nabla g_j)(f(W)) \cdot \left(\frac{\partial f_i}{\partial w_{ak}}(W)\right)_{a} = \sum\limits_{a = 1}^{l} \partial_a g_j(f(W)) \frac{\partial f_i}{\partial w_{ak}}(W)$$ On the other hand, employing the usual chain rule and some mildly sketchy argumentation, we have $$\frac{\partial}{\partial w_{jk}} (g_i(f(W))) = \frac{\partial}{\partial w_{jk}} (g_i(f_1(W),\ldots,f_l(W))) = \sum\limits_{a = 1}^{l} (\partial_a g_i(f(W)))\frac{\partial f_a}{\partial w_{jk}}(W) = (\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a$$ for arbitrary $i,j,k$ . This means our Jacobian now looks like $$\nabla_W (g \circ f) = \left(\left((\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_i = \left(\left(\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_{i}$$ By writing this out in the stacked matrix format, it seems that this calculation only differs up to a swap of indices. This is where I am stuck. If I take $s = 1$ (which corresponds to the case examined in my notes), the definition seems to fit, but my derivation of the chain rule seems to be wrong. It also might be that my understanding of tensor multiplication is not correct, such that both definition and calculation could be correct and just me not realizing it. EDIT: Reading through the (concerningly sparse) material on the net about this, I might have found out that both formulations are equivalent under the premise that matrix-tensor-multiplications (generalized by tensor contractions) are the right way to go about this. Visually, employing the stacked matrix representation again, a multiplication of a rank-3-tensor with a matrix from the left would look like this: To obtain the value at index $ijk$ , take the $i$ -th row vector of $J(g)(f)$ and multiply it by the column vector gained by taking the $(j,k)$ -th index of each submatrix of $D_W(f)$ in order (!). This should make both arguments coincide (we especially get scalar-matrix-multiplication back for $l = s = 1$ ) and it seems natural for some reason. Is my reasoning about tensor multiplication correct? Summarizing, my questions now are: Is my derivation of the chain rule up to the last point correct? If not, where exactly am I going wrong? How does the tensor multiplication work in the presumably correct statement of the chain rule above? With this I might be able to reverse engineer my error(s). Thank you for reading through this barrage of indices and matrices!","While studying, I noticed that in some lectures notes of mine we hand-wavingly use the chain rule formula to design a gradient descent algorithm with respect to some matrix (i.e. the variable we want to minimize is a matrix). Curious about how one would define the jacobian with respect to a matrix, I stumbled upon this question and how to represent rank-3-tensors as stacked matrices. With this, we define the Jacobian of a function with respect to the variable as Visually, this Jacobian consists of stacked matrices each containing the individual derivatives (but fixing the output components ). With this we can try to prove the chain rule for matrix derivatives. Let and . The chain rule should be On the one hand, treating a matrix in this multiplication as a ""scalar"" and elementwise-multiplying it with the rank-3-tensor (seems natural in a module-theoretic sense) would consequently yield On the other hand, employing the usual chain rule and some mildly sketchy argumentation, we have for arbitrary . This means our Jacobian now looks like By writing this out in the stacked matrix format, it seems that this calculation only differs up to a swap of indices. This is where I am stuck. If I take (which corresponds to the case examined in my notes), the definition seems to fit, but my derivation of the chain rule seems to be wrong. It also might be that my understanding of tensor multiplication is not correct, such that both definition and calculation could be correct and just me not realizing it. EDIT: Reading through the (concerningly sparse) material on the net about this, I might have found out that both formulations are equivalent under the premise that matrix-tensor-multiplications (generalized by tensor contractions) are the right way to go about this. Visually, employing the stacked matrix representation again, a multiplication of a rank-3-tensor with a matrix from the left would look like this: To obtain the value at index , take the -th row vector of and multiply it by the column vector gained by taking the -th index of each submatrix of in order (!). This should make both arguments coincide (we especially get scalar-matrix-multiplication back for ) and it seems natural for some reason. Is my reasoning about tensor multiplication correct? Summarizing, my questions now are: Is my derivation of the chain rule up to the last point correct? If not, where exactly am I going wrong? How does the tensor multiplication work in the presumably correct statement of the chain rule above? With this I might be able to reverse engineer my error(s). Thank you for reading through this barrage of indices and matrices!","W \in \mathbb{R}^{m \times n} m \times n f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l} W \nabla_W f = \left({\frac{\partial f_i}{\partial w_{jk}}}\right)_{ijk} = \left(\left(\frac{\partial f_i}{\partial w_{j k}}\right)_{jk}\right)_{i} l m \times n W f_i f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}^{l} g: \mathbb{R}^l \rightarrow \mathbb{R}^s J_W(g \circ f) = J(g)(f) \cdot J_W(f) \frac{\partial g_i}{\partial w_{jk}}(f(W)) = (\nabla g_j)(f(W)) \cdot \left(\frac{\partial f_i}{\partial w_{ak}}(W)\right)_{a} = \sum\limits_{a = 1}^{l} \partial_a g_j(f(W)) \frac{\partial f_i}{\partial w_{ak}}(W) \frac{\partial}{\partial w_{jk}} (g_i(f(W))) = \frac{\partial}{\partial w_{jk}} (g_i(f_1(W),\ldots,f_l(W))) = \sum\limits_{a = 1}^{l} (\partial_a g_i(f(W)))\frac{\partial f_a}{\partial w_{jk}}(W) = (\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a i,j,k \nabla_W (g \circ f) = \left(\left((\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_i = \left(\left(\nabla g_i)(f(W)) \cdot \left(\frac{\partial f_a}{\partial w_{jk}}(W)\right)_a\right)_{jk}\right)_{i} s = 1 ijk i J(g)(f) (j,k) D_W(f) l = s = 1","['multivariable-calculus', 'tensor-products', 'chain-rule']"
93,Is this double integral improper?,Is this double integral improper?,,"Consider the double integral of $f(x,y)=\frac{y^2}{x^2}$ over the triangle with vertices $(0,0)$ , $(2,3)$ , and $(5,3)$ , which is written as $$ \int_0^3\int_{\frac{2}{3}y}^{\frac{5}{3}y}\frac{y^2}{x^2}\,dx\,dy. $$ This integral can be easily evaluated through conventional means, yielding $$ =\int_0^3\frac{-y^2}{x}\bigg|_{\frac{2}{3}y}^{\frac{5}{3}y}\,dy=\int_0^3\frac{9}{10}y\,dy=\frac{9}{20}y^2\bigg|_0^3=\frac{81}{20}. $$ One can observe that $f$ is undefined at $(0,0)$ .  Furthermore, the discontinuity of $f$ at $(0,0)$ is nonremovable, as $$ \lim_{(x,y)\to(0,0)}f(x,y)\text{ does not exist.} $$ And yet, the fact that $f$ is ""bad"" at $(0,0)$ never seems to to show up anywhere in our previous calculation.  (This brings to mind $\int_0^1\frac{1}{\sqrt{x}}\,dx$ , which is improper, but ""takes care of itself"" enough that a blind calculation never notices.)  I still have a few questions: Is this integration, in fact, improper? $f$ is bounded on the triangle described above, as the largest value it can acquire is on the boundary line $y=\frac{3}{2}x$ : $f(x,\frac{3}{2}x)=\frac{9}{4}$ . Does the nonexistence of the limit of $f$ (and $f$ itself, for that matter) at $(0,0)$ cause any problems for us?  Or can we simply say "" $(0,0)$ is a set of measure zero, so no problems""? Can we run into problems if our ""bad"" set of domain values is measure zero but dimension 1?  (Unlike our current example, which is measure zero and dimension 0.) In general, can we ignore any bad behavior on a measure zero set as long as our function is bounded?","Consider the double integral of over the triangle with vertices , , and , which is written as This integral can be easily evaluated through conventional means, yielding One can observe that is undefined at .  Furthermore, the discontinuity of at is nonremovable, as And yet, the fact that is ""bad"" at never seems to to show up anywhere in our previous calculation.  (This brings to mind , which is improper, but ""takes care of itself"" enough that a blind calculation never notices.)  I still have a few questions: Is this integration, in fact, improper? is bounded on the triangle described above, as the largest value it can acquire is on the boundary line : . Does the nonexistence of the limit of (and itself, for that matter) at cause any problems for us?  Or can we simply say "" is a set of measure zero, so no problems""? Can we run into problems if our ""bad"" set of domain values is measure zero but dimension 1?  (Unlike our current example, which is measure zero and dimension 0.) In general, can we ignore any bad behavior on a measure zero set as long as our function is bounded?","f(x,y)=\frac{y^2}{x^2} (0,0) (2,3) (5,3) 
\int_0^3\int_{\frac{2}{3}y}^{\frac{5}{3}y}\frac{y^2}{x^2}\,dx\,dy.
 
=\int_0^3\frac{-y^2}{x}\bigg|_{\frac{2}{3}y}^{\frac{5}{3}y}\,dy=\int_0^3\frac{9}{10}y\,dy=\frac{9}{20}y^2\bigg|_0^3=\frac{81}{20}.
 f (0,0) f (0,0) 
\lim_{(x,y)\to(0,0)}f(x,y)\text{ does not exist.}
 f (0,0) \int_0^1\frac{1}{\sqrt{x}}\,dx f y=\frac{3}{2}x f(x,\frac{3}{2}x)=\frac{9}{4} f f (0,0) (0,0)","['integration', 'multivariable-calculus']"
94,Doubts on the solution of a differential equation,Doubts on the solution of a differential equation,,"I've encountered myself with a differential equation which I'm not sure that I've solved correctly. The situation is the following: I have a set of parametric equations, which I'll call $\vec{r}$ \begin{array}{ll} x(\theta) &= R \cdot \cos(\theta) \cr y(\theta) &= k \cdot \theta \cr z(\theta) &= R \cdot \sin(\theta) \end{array} $x$ , $y$ and $z$ represent position in all three axes. As I'm going to need them later, I'll calculate the first and second derivative of $\vec{r}$ \begin{array}{ll} x\prime(\theta) &= - R \cdot \sin(\theta) \cr y\prime(\theta) &= k \cr z\prime(\theta) &= R \cdot \cos(\theta) \end{array} \begin{array}{ll} x\prime\prime(\theta) &= - R \cdot \cos(\theta) \cr y\prime\prime(\theta) &= 0 \cr z\prime\prime(\theta) &= - R \cdot \sin(\theta) \end{array} The resulting directional vector would be written as follows: $\vec{u} = (- R \cdot \sin(\theta), k, R \cdot \cos(\theta))$ . Using the Pythagorean theorem, we can calculate the modulus of this vector: $\|\vec{u}\| = \sqrt{R^2 \cdot \sin^2(\theta) + k^2 + R^2 \cdot \cos^2(\theta)}$ . Combining these, we obtain the unitary vector: $$\hat{u} = \frac{\vec{u}}{\|\vec{u}\|} = \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}}$$ Therefore, using the vector for the gravitational acceleration $\vec{v} = (0, 0, g)$ and knowing that $\|\vec{a}_T\| = \|\vec{v}\| \cdot \cos(\beta) = \|(\vec{v} \cdot \hat{u})\|$ , we can obtain the modulus of the total acceleration: $$\|\vec{a}_T\| = \frac{\sqrt{(0^2 + 0^2 + g^2 \cdot R^2 \cdot \cos^2(\theta))}}{\sqrt{R^2 + k^2}} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}}$$ The total acceleration will then result in multiplying this modulus we have just obtained by the unit vector which points towards the direction of the acceleration: $$\vec{a} = \|\vec{a_T}\| \cdot \hat{u} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}} \cdot \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}}$$ $$\vec{a} = \frac{(- g \cdot R^2 \cdot \cos(\theta) \cdot \sin(\theta), g \cdot k \cdot R \cdot \cos(\theta), g \cdot R^2 \cdot \cos^2(\theta))}{R^2 + k^2}$$ Writing it in parameterized form (I used the chain rule on the left side): \begin{array}{rl} x: \ddot{\theta}(t) \cdot x\prime(\theta) + \dot{\theta}(t) \cdot x\prime\prime(\theta) &= \displaystyle \frac{- g \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr y: \ddot{\theta}(t) \cdot y\prime(\theta) + \dot{\theta}(t) \cdot y\prime\prime(\theta) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr z: \ddot{\theta}(t) \cdot z\prime(\theta) + \dot{\theta}(t) \cdot z\prime\prime(\theta) &= \displaystyle \frac{g \cdot \cos^2(\theta)}{1 + k^2} \end{array} Isolating $\ddot{\theta}(t)$ and subtituting with the formulas above, I get the following differential equations which I know how to solve numerically by using Euler's method: \begin{array}{rl} x: \ddot{\theta}(t) &= \displaystyle \frac{- g \cdot \cos(\theta)}{R \cdot (1 + k^2)} + \dot{\theta}^2(t) \cdot \tan(\theta) \cr y: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta)}{1 + k^2} \cr z: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot \cos(\theta)}{R \cdot (1 + k^2)} - \dot{\theta}^2(t) \cdot \tan^{-1}(\theta) \end{array} My question is: shouldn't the three last formulas be the same or equivalent to each other? Edit: for anyone wondering, the final objective of this is to make an animation using Python. Here's the final code (edited out to only include the math): import math  # VARIABLES g = - 9.8 R = 10 k = 2 ddtheta = 0  theta0 = math.radians(90) dtheta0 = 1 x0 = R * math.cos(theta0) y0 = k * theta0 z0 = R * math.sin(theta0)  x1 = 0 y1 = 0 z1 = 0 theta1 = 0 dtheta1 = 0  object.position[0] = x0 object.position[1] = y0 object.position[2] = z0  # ANIMATION for i in range(1, 200):     t = 0.00003 # Small step size     costheta0 = math.cos(theta0)     sintheta0 = math.sin(theta0)          # EULER'S METHOD FOR THETA     ddtheta = (g * R * costheta0)/(R**2 + k**2)     dtheta1 = ddtheta * t + dtheta0     theta1 = dtheta0 * t + theta0          # THE THREE AXES     x1 = R * costheta0     y1 = k * theta0     z1 = R * sintheta0              # RESET VALUES FOR NEXT ITERATION     x0 = x1     y0 = y1     z0 = z1     theta0 = theta1     dtheta0 = dtheta1          object.position[0] = x1 # X-axis     object.position[1] = y1 # Y-axis     object.position[2] = z1 # Z-axis EDIT : The issue is solved... See Lutz's answer below... But now, I tried applying the same method to another curve: the clothoid, but it hasn't worked. The clothoid has the following formulas for $\vec{q}$ : $$\begin{array}{rl} x(\theta) &= \displaystyle \int^\theta_0 \cos(t^2)\ dt \cr y(\theta) &= \displaystyle \int^\theta_0 \sin(t^2)\ dt \end{array}$$ The derivatives are: $$\begin{array}{rl} x'(\theta) &= \cos(\theta^2) \cr y'(\theta) &= \sin(\theta^2) \end{array}$$ $$\begin{array}{rl} x''(\theta) &= -2 \cdot \theta \cdot \sin(\theta^2) \cr y''(\theta) &= 2 \cdot \theta \cos(\theta^2) \end{array}$$ Knowing that the chain rule is $\vec{a} = \vec{q}'' \cdot \dot{\theta}^2 + \vec{q}' \cdot \ddot{\theta}$ and that $\vec{q}'$ and $\vec{q}''$ are orthogonal ( $\cos\alpha = 0$ ): $$\require{cancel}\begin{array}{c} \vec{q}''(\theta) \cdot \vec{q}'(\theta) = \|\vec{q}''\| \cdot \|\vec{q}'\| \cdot \cancelto{0}{\cos 90} = 0 \cr \cr \vec{a} \cdot \vec{q}' = \underset{0}{\underbrace{\vec{q}''(\theta) \cdot \vec{q}'(\theta) \cdot \dot{\theta}^2}} + \vec{q}'(\theta)^2 \cdot \ddot{\theta}(t) \cr \cr \dfrac{\vec{F}}{m} \cdot \vec{q}'(\theta) = (0, g) \cdot (\cos(\theta^2), \sin(\theta^2)) = g \cdot \sin(\theta^2) \end{array}$$ By combining the expressions above (and knowing that the denominator $\sin^2(\theta^2) + \cos^2(\theta^2) = 1$ ): $$\require{cancel}\begin{array}{c} \cancelto{1}{\|\vec{q}\|(\theta)^2} \cdot \ddot{\theta}(t) = g \cdot \sin(\theta^2) \end{array}$$ Why can't I apply the same method as explained by @Lutz Lehmann?? Here's the code for this equation: include math # VARIABLES fps = 30 g = - 9.8 ddtheta = 0  theta0 = 0 dtheta0 = 5 x0 = 0 y0 = 0 vx0 = math.cos(theta0**2) vy0 = math.sin(theta0**2)  x1 = 0 y1 = 0 vx1 = 0 vy1 = 0 theta1 = 0 dtheta1 = 0  object.position[0] = x0 object.position[2] = y0  # ANIMATION for i in range(1, 2000):     t = 0.00003 # Small step size     ddtheta = g * math.sin(theta0**2)     dtheta1 = ddtheta * t + dtheta0     theta1 = dtheta0 * t + theta0              vx1 = math.cos(theta0**2)     x1 = vx0 * t + x0     vy1 = math.sin(theta0**2)     y1 = vy0 * t + y0          x0 = x1     y0 = y1     vx0 = vx1     vy0 = vy1     theta0 = theta1     dtheta0 = dtheta1          object.position[0] = x1 # X-axis     object.position[2] = y1 # Y-axis ```","I've encountered myself with a differential equation which I'm not sure that I've solved correctly. The situation is the following: I have a set of parametric equations, which I'll call , and represent position in all three axes. As I'm going to need them later, I'll calculate the first and second derivative of The resulting directional vector would be written as follows: . Using the Pythagorean theorem, we can calculate the modulus of this vector: . Combining these, we obtain the unitary vector: Therefore, using the vector for the gravitational acceleration and knowing that , we can obtain the modulus of the total acceleration: The total acceleration will then result in multiplying this modulus we have just obtained by the unit vector which points towards the direction of the acceleration: Writing it in parameterized form (I used the chain rule on the left side): Isolating and subtituting with the formulas above, I get the following differential equations which I know how to solve numerically by using Euler's method: My question is: shouldn't the three last formulas be the same or equivalent to each other? Edit: for anyone wondering, the final objective of this is to make an animation using Python. Here's the final code (edited out to only include the math): import math  # VARIABLES g = - 9.8 R = 10 k = 2 ddtheta = 0  theta0 = math.radians(90) dtheta0 = 1 x0 = R * math.cos(theta0) y0 = k * theta0 z0 = R * math.sin(theta0)  x1 = 0 y1 = 0 z1 = 0 theta1 = 0 dtheta1 = 0  object.position[0] = x0 object.position[1] = y0 object.position[2] = z0  # ANIMATION for i in range(1, 200):     t = 0.00003 # Small step size     costheta0 = math.cos(theta0)     sintheta0 = math.sin(theta0)          # EULER'S METHOD FOR THETA     ddtheta = (g * R * costheta0)/(R**2 + k**2)     dtheta1 = ddtheta * t + dtheta0     theta1 = dtheta0 * t + theta0          # THE THREE AXES     x1 = R * costheta0     y1 = k * theta0     z1 = R * sintheta0              # RESET VALUES FOR NEXT ITERATION     x0 = x1     y0 = y1     z0 = z1     theta0 = theta1     dtheta0 = dtheta1          object.position[0] = x1 # X-axis     object.position[1] = y1 # Y-axis     object.position[2] = z1 # Z-axis EDIT : The issue is solved... See Lutz's answer below... But now, I tried applying the same method to another curve: the clothoid, but it hasn't worked. The clothoid has the following formulas for : The derivatives are: Knowing that the chain rule is and that and are orthogonal ( ): By combining the expressions above (and knowing that the denominator ): Why can't I apply the same method as explained by @Lutz Lehmann?? Here's the code for this equation: include math # VARIABLES fps = 30 g = - 9.8 ddtheta = 0  theta0 = 0 dtheta0 = 5 x0 = 0 y0 = 0 vx0 = math.cos(theta0**2) vy0 = math.sin(theta0**2)  x1 = 0 y1 = 0 vx1 = 0 vy1 = 0 theta1 = 0 dtheta1 = 0  object.position[0] = x0 object.position[2] = y0  # ANIMATION for i in range(1, 2000):     t = 0.00003 # Small step size     ddtheta = g * math.sin(theta0**2)     dtheta1 = ddtheta * t + dtheta0     theta1 = dtheta0 * t + theta0              vx1 = math.cos(theta0**2)     x1 = vx0 * t + x0     vy1 = math.sin(theta0**2)     y1 = vy0 * t + y0          x0 = x1     y0 = y1     vx0 = vx1     vy0 = vy1     theta0 = theta1     dtheta0 = dtheta1          object.position[0] = x1 # X-axis     object.position[2] = y1 # Y-axis","\vec{r} \begin{array}{ll}
x(\theta) &= R \cdot \cos(\theta) \cr
y(\theta) &= k \cdot \theta \cr
z(\theta) &= R \cdot \sin(\theta)
\end{array} x y z \vec{r} \begin{array}{ll}
x\prime(\theta) &= - R \cdot \sin(\theta) \cr
y\prime(\theta) &= k \cr
z\prime(\theta) &= R \cdot \cos(\theta)
\end{array} \begin{array}{ll}
x\prime\prime(\theta) &= - R \cdot \cos(\theta) \cr
y\prime\prime(\theta) &= 0 \cr
z\prime\prime(\theta) &= - R \cdot \sin(\theta)
\end{array} \vec{u} = (- R \cdot \sin(\theta), k, R \cdot \cos(\theta)) \|\vec{u}\| = \sqrt{R^2 \cdot \sin^2(\theta) + k^2 + R^2 \cdot \cos^2(\theta)} \hat{u} = \frac{\vec{u}}{\|\vec{u}\|} = \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}} \vec{v} = (0, 0, g) \|\vec{a}_T\| = \|\vec{v}\| \cdot \cos(\beta) = \|(\vec{v} \cdot \hat{u})\| \|\vec{a}_T\| = \frac{\sqrt{(0^2 + 0^2 + g^2 \cdot R^2 \cdot \cos^2(\theta))}}{\sqrt{R^2 + k^2}} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}} \vec{a} = \|\vec{a_T}\| \cdot \hat{u} = \frac{g \cdot R \cdot \cos(\theta)}{\sqrt{R^2 + k^2}} \cdot \frac{(- R \cdot \sin(\theta), k, R \cdot \cos(\theta))}{\sqrt{R^2 + k^2}} \vec{a} = \frac{(- g \cdot R^2 \cdot \cos(\theta) \cdot \sin(\theta), g \cdot k \cdot R \cdot \cos(\theta), g \cdot R^2 \cdot \cos^2(\theta))}{R^2 + k^2} \begin{array}{rl}
x: \ddot{\theta}(t) \cdot x\prime(\theta) + \dot{\theta}(t) \cdot x\prime\prime(\theta) &= \displaystyle \frac{- g \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr
y: \ddot{\theta}(t) \cdot y\prime(\theta) + \dot{\theta}(t) \cdot y\prime\prime(\theta) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta) \cdot \sin(\theta)}{1 + k^2} \cr
z: \ddot{\theta}(t) \cdot z\prime(\theta) + \dot{\theta}(t) \cdot z\prime\prime(\theta) &= \displaystyle \frac{g \cdot \cos^2(\theta)}{1 + k^2}
\end{array} \ddot{\theta}(t) \begin{array}{rl}
x: \ddot{\theta}(t) &= \displaystyle \frac{- g \cdot \cos(\theta)}{R \cdot (1 + k^2)} + \dot{\theta}^2(t) \cdot \tan(\theta) \cr
y: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot k \cdot \cos(\theta)}{1 + k^2} \cr
z: \ddot{\theta}(t) &= \displaystyle \frac{g \cdot \cos(\theta)}{R \cdot (1 + k^2)} - \dot{\theta}^2(t) \cdot \tan^{-1}(\theta)
\end{array} \vec{q} \begin{array}{rl}
x(\theta) &= \displaystyle \int^\theta_0 \cos(t^2)\ dt \cr
y(\theta) &= \displaystyle \int^\theta_0 \sin(t^2)\ dt
\end{array} \begin{array}{rl}
x'(\theta) &= \cos(\theta^2) \cr
y'(\theta) &= \sin(\theta^2)
\end{array} \begin{array}{rl}
x''(\theta) &= -2 \cdot \theta \cdot \sin(\theta^2) \cr
y''(\theta) &= 2 \cdot \theta \cos(\theta^2)
\end{array} \vec{a} = \vec{q}'' \cdot \dot{\theta}^2 + \vec{q}' \cdot \ddot{\theta} \vec{q}' \vec{q}'' \cos\alpha = 0 \require{cancel}\begin{array}{c}
\vec{q}''(\theta) \cdot \vec{q}'(\theta) = \|\vec{q}''\| \cdot \|\vec{q}'\| \cdot \cancelto{0}{\cos 90} = 0 \cr \cr
\vec{a} \cdot \vec{q}' = \underset{0}{\underbrace{\vec{q}''(\theta) \cdot \vec{q}'(\theta) \cdot \dot{\theta}^2}} + \vec{q}'(\theta)^2 \cdot \ddot{\theta}(t) \cr \cr
\dfrac{\vec{F}}{m} \cdot \vec{q}'(\theta) = (0, g) \cdot (\cos(\theta^2), \sin(\theta^2)) = g \cdot \sin(\theta^2)
\end{array} \sin^2(\theta^2) + \cos^2(\theta^2) = 1 \require{cancel}\begin{array}{c}
\cancelto{1}{\|\vec{q}\|(\theta)^2} \cdot \ddot{\theta}(t) = g \cdot \sin(\theta^2)
\end{array} ```","['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'numerical-methods', 'eulers-method']"
95,How do we know that a surface integral is the same as an iterated double integral?,How do we know that a surface integral is the same as an iterated double integral?,,"I'm going through my professor's calculus notes for integration of a function $f(x,y)$ over a rectangle in $\mathbb{R}^2$ . It defines partitions of a rectangle $R$ into subrectangles, and then says that the integral of $f(x,y)$ is the supremum of the lower sum (or infimum of upper sum) defined here : So in essence the integral of $f(x,y)$ over $R$ : $$\int^{}_{R} f\,dV$$ is the sum of the volume of these infinitesimal rectangles, he posts another picture to illustrate : Here's my question : I know that the volume underneath the surface described by $f(x,y)$ is computed as an iterated double integral : $\int_{c}^{d} \left(\int_{a}^{b} f(x,y) \,dx\right)\,dy$ ,     (where the rectangle $R$ in $\mathbb{R}^2$ is described as $R = [a,b] \times [c,d]$ ). However from my understanding, what the iterated double integral does is calculate the area underneath an ' $x$ -slice' as a function of $y$ ( as $F(y) = \int_{a}^{b} f(x,y) \,dx$ ) , and then sums up all these areas as $\int_{c}^{d} F(y) \,dy$ . Professor provides animation here for reference . This is not the same as measuring the volume underneath every infinitesmal subrectangle, and adding them up. If we were talking about the volume of a cheese, the above partition integral definition $\int_{R} f\,dV$ amounts to slicing the cheese into a bunch of rectangles along $x$ and $y$ , then measuring the volume of each and adding them. What an iterated double integral does as I've described it, would be to simply slice the cheese into a bunch of slices along $x$ only, measure the volume of those, and add them together. My question is how do I rigorously know these operations are equivalent? In short, why is the following true? : $$\int_{R} f\,dV = \int_{c}^{d} \left(\int_{a}^{b} f(x,y) \, dx\right) \, dy $$ And why is $dV = dxdy$ ?","I'm going through my professor's calculus notes for integration of a function over a rectangle in . It defines partitions of a rectangle into subrectangles, and then says that the integral of is the supremum of the lower sum (or infimum of upper sum) defined here : So in essence the integral of over : is the sum of the volume of these infinitesimal rectangles, he posts another picture to illustrate : Here's my question : I know that the volume underneath the surface described by is computed as an iterated double integral : ,     (where the rectangle in is described as ). However from my understanding, what the iterated double integral does is calculate the area underneath an ' -slice' as a function of ( as ) , and then sums up all these areas as . Professor provides animation here for reference . This is not the same as measuring the volume underneath every infinitesmal subrectangle, and adding them up. If we were talking about the volume of a cheese, the above partition integral definition amounts to slicing the cheese into a bunch of rectangles along and , then measuring the volume of each and adding them. What an iterated double integral does as I've described it, would be to simply slice the cheese into a bunch of slices along only, measure the volume of those, and add them together. My question is how do I rigorously know these operations are equivalent? In short, why is the following true? : And why is ?","f(x,y) \mathbb{R}^2 R f(x,y) f(x,y) R \int^{}_{R} f\,dV f(x,y) \int_{c}^{d} \left(\int_{a}^{b} f(x,y) \,dx\right)\,dy R \mathbb{R}^2 R = [a,b] \times [c,d] x y F(y) = \int_{a}^{b} f(x,y) \,dx \int_{c}^{d} F(y) \,dy \int_{R} f\,dV x y x \int_{R} f\,dV = \int_{c}^{d} \left(\int_{a}^{b} f(x,y) \, dx\right) \, dy  dV = dxdy","['integration', 'multivariable-calculus']"
96,How should I interpret these integrals from Griffiths 'Intro to Electrodynamics'?,How should I interpret these integrals from Griffiths 'Intro to Electrodynamics'?,,"The book defines the electric field at a point $P$ a distance $r$ due to a point charge $q$ as : $$ E = \frac{1}{4\pi \epsilon _0} \frac{q}{r^2}$$ it then tells us that the electric field at a point $P$ due to $n$ point charges $q_i$ is  : $$ E = \sum^n_{i=1} \frac{1}{4\pi \epsilon _0} \frac{q}{r_i^2}  $$ where $r_i$ is the distance between $q_i$ and $P$ . As in the electric field follows the superposition principle. It then tells us that when we have instead of discrete point charges, have a continuous charge (either a line charge, a surface charge, or volume charge) then the electric fields given for the respective situations are given as : $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl' $$ $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}da' $$ $$ E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}d \tau' $$ Where for a line charge : $dq= \lambda dl' $ , for a surface charge : $dq = \sigma da'$ for a volume charge : $dq = \rho d\tau'$ Here's where my question comes in, I am having a great trouble understanding how to 'interpret' these integrals. Firstly, I tried viewing these integrals mathematically. For example for the surface charge integral (eq 2.) I tried to express it as the Electric field at a point $P$ due to a charged rectangle in cartesian coordinates of length $a$ and width $b$ : $$ E = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}dx'dy' = \frac{1}{4\pi \epsilon _0}(\int_{0}^{a} (\int_{0}^{b}  \frac{\sigma(r')}{(\sqrt{(x_1 - x)^2 + (y_1 - y)^2)}^2}dx')dy') $$ Where $r$ is the distance of each point in the rectangle to the point $P = (x_1,y_1)$ from the arbitrary points to integrate over $(x,y)$ ,  (assuming the charged rectangle and $P$ are on the same plane here. I made a rough paint to illustrate I tried to interpret the last expression on the RHS as the volume above a rectangle of length $a$ and width $b$ , with a height of function $\frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2}$ . Where the 'height' of each point above this rectangle, is the magnitude of the contribution for a point charge located at that point. Kind of like the height above each point is $dE$ at $P$ , and adding them all together gives us $E$ at $P$ . I made another rough paint . But we're talking about 'surface charges' not point charges. So this interpretation shouldn't hold for a line charge, because it uses a different variable for some reason ( $\lambda$ instead of $\sigma$ ). But I don't see why you can't similarly just interpret equation 1. as the area above a line with height $\frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl'$ , similar to how one interprets definite integrals in Integral Calculus. Which leads to my second question Why are we using different terms for the integration variable? At first I thought it was so you get the correct units but I'm not so sure how to justify that intuition. How does the integral 'know' I'm talking about areas when I say $da'$ and lines or volumes when I say $dl'$ and $d\tau '$ ? The easiest way I've found to think of all this is to think of the function under the integral 'as a whole'. As in to think of equation 2. As taking an infinitesmal area of the charged rectangle (surface element $da'$ ) and multiplying it by the charge density ( $\sigma(r')$ ), which would give you the total charge on the surface element ( $dq$ ). Then picking a point on this infinitesmal surface area and measuring the distance from this point to $P$ (this distance would be $r$ ). Then multiplying this entire expression by $\frac{1}{4 \pi \epsilon _0}$ . Which gives the expression $$ dE = \frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \frac{dq}{r^2}$$ as the electric field due to a singular surface element a distance $r$ away from a point $P$ . And then when you take the integral of this above expression, since integrals represent infinite sums, the above integrated would represent adding up the contribution of all these surface elements, to give the total electric field of the entire surface. And this makes a lot of sense to me, however it is not actually how integration works to my knowledge. What I've described is multiplying the area of a bunch of squares ( $da$ ) by a factor dependent on this squares position and charge density ( $\frac{1}{4 \pi \epsilon _0} \frac{\sigma(r')}{r^2})$ . Which gives an associated $dE$ with each square, and adding them (integrating) to get an $E$ associated with the whole rectangle. To my knowledge integrating the above expression over a rectangle for example, would be to integrate the contribution of a line on this rectangle, with its height determined as $\frac{1}{4\pi \epsilon _0} \frac{dq}{r^2}$ , then sweeping it along the rectangle. My calculus professor made a demo of what I mean here . Sorry if this question is extremely long or difficult to follow. Part of my question is me not really understanding what I'm missing about how to interpret these integrals. I suspect I also have some fundamental flaw in my understanding of integrals that I'm missing. Any answers appreciated.","The book defines the electric field at a point a distance due to a point charge as : it then tells us that the electric field at a point due to point charges is  : where is the distance between and . As in the electric field follows the superposition principle. It then tells us that when we have instead of discrete point charges, have a continuous charge (either a line charge, a surface charge, or volume charge) then the electric fields given for the respective situations are given as : Where for a line charge : , for a surface charge : for a volume charge : Here's where my question comes in, I am having a great trouble understanding how to 'interpret' these integrals. Firstly, I tried viewing these integrals mathematically. For example for the surface charge integral (eq 2.) I tried to express it as the Electric field at a point due to a charged rectangle in cartesian coordinates of length and width : Where is the distance of each point in the rectangle to the point from the arbitrary points to integrate over ,  (assuming the charged rectangle and are on the same plane here. I made a rough paint to illustrate I tried to interpret the last expression on the RHS as the volume above a rectangle of length and width , with a height of function . Where the 'height' of each point above this rectangle, is the magnitude of the contribution for a point charge located at that point. Kind of like the height above each point is at , and adding them all together gives us at . I made another rough paint . But we're talking about 'surface charges' not point charges. So this interpretation shouldn't hold for a line charge, because it uses a different variable for some reason ( instead of ). But I don't see why you can't similarly just interpret equation 1. as the area above a line with height , similar to how one interprets definite integrals in Integral Calculus. Which leads to my second question Why are we using different terms for the integration variable? At first I thought it was so you get the correct units but I'm not so sure how to justify that intuition. How does the integral 'know' I'm talking about areas when I say and lines or volumes when I say and ? The easiest way I've found to think of all this is to think of the function under the integral 'as a whole'. As in to think of equation 2. As taking an infinitesmal area of the charged rectangle (surface element ) and multiplying it by the charge density ( ), which would give you the total charge on the surface element ( ). Then picking a point on this infinitesmal surface area and measuring the distance from this point to (this distance would be ). Then multiplying this entire expression by . Which gives the expression as the electric field due to a singular surface element a distance away from a point . And then when you take the integral of this above expression, since integrals represent infinite sums, the above integrated would represent adding up the contribution of all these surface elements, to give the total electric field of the entire surface. And this makes a lot of sense to me, however it is not actually how integration works to my knowledge. What I've described is multiplying the area of a bunch of squares ( ) by a factor dependent on this squares position and charge density ( . Which gives an associated with each square, and adding them (integrating) to get an associated with the whole rectangle. To my knowledge integrating the above expression over a rectangle for example, would be to integrate the contribution of a line on this rectangle, with its height determined as , then sweeping it along the rectangle. My calculus professor made a demo of what I mean here . Sorry if this question is extremely long or difficult to follow. Part of my question is me not really understanding what I'm missing about how to interpret these integrals. I suspect I also have some fundamental flaw in my understanding of integrals that I'm missing. Any answers appreciated.","P r q  E = \frac{1}{4\pi \epsilon _0} \frac{q}{r^2} P n q_i  E = \sum^n_{i=1} \frac{1}{4\pi \epsilon _0} \frac{q}{r_i^2}   r_i q_i P  E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl'   E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}da'   E = \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\sigma(r')}{r^2}d \tau'  dq= \lambda dl'  dq = \sigma da' dq = \rho d\tau' P a b  E = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \int_{area}^{}  \frac{\sigma(r')}{r^2}dx'dy' = \frac{1}{4\pi \epsilon _0}(\int_{0}^{a} (\int_{0}^{b}  \frac{\sigma(r')}{(\sqrt{(x_1 - x)^2 + (y_1 - y)^2)}^2}dx')dy')  r P = (x_1,y_1) (x,y) P a b \frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2} dE P E P \lambda \sigma \frac{1}{4\pi \epsilon _0} \int_{}^{}  \frac{\lambda(r')}{r^2}dl' da' dl' d\tau ' da' \sigma(r') dq P r \frac{1}{4 \pi \epsilon _0}  dE = \frac{1}{4\pi \epsilon _0} \frac{\sigma(r')}{r^2}da' = \frac{1}{4\pi \epsilon _0} \frac{dq}{r^2} r P da \frac{1}{4 \pi \epsilon _0} \frac{\sigma(r')}{r^2}) dE E \frac{1}{4\pi \epsilon _0} \frac{dq}{r^2}","['integration', 'multivariable-calculus', 'physics']"
97,Double Integral Question: How are the limits set up?,Double Integral Question: How are the limits set up?,,"I am trying to find the area of the line y = x and below by the parabola $y = x^2-2x. $ I am using a double integral. However, for the integral, this is the correct answer: $$ \int_0^3\int_{x^2-2x}^xxdydx $$ I don't understand how the dy limits of integration are set up? Isn't it right function-left function? The graph on the right is the $x^2-2x$ . But in the limits of integration, this is reversed? In the picture the blue graph is $x^2-2x$ and the red is $y=x$","I am trying to find the area of the line y = x and below by the parabola I am using a double integral. However, for the integral, this is the correct answer: I don't understand how the dy limits of integration are set up? Isn't it right function-left function? The graph on the right is the . But in the limits of integration, this is reversed? In the picture the blue graph is and the red is",y = x^2-2x.   \int_0^3\int_{x^2-2x}^xxdydx  x^2-2x x^2-2x y=x,"['multivariable-calculus', 'multiple-integral']"
98,Optimising a business strategy,Optimising a business strategy,,"My brother in year 8 was given this scenario: ""You sell chairs and make sales at monthly intervals. You start with $16,000. Each month, 17000 people buy chairs, The market average price for chairs is $72. Chairs cost $40 each when you buy 400 or more, and 35 dollars each when you buy 1000 or more. Premium chairs cost 45 dollars each when you buy 400 or more, and 40 dollars each when you buy 1000 or more. Your business sets the price of chairs. The price must be between $24-$ 150. For every $4 under the market average, your business gains 0.5% of the market. For each $0.50 over the market average, your business loses 0.05% of its customers when selling standard chairs, and 0.01% of its customers when selling standard chairs, and 0.01% of its customers when selling premium chairs. For every 1000 chairs sold in a month, your business must pay an employee $5000.10% of sales must go to the government for GST. Your business may take out one loan at a time. You can only loan out as much money as you currently have. The loan compounds monthly at a rate of 18% per month. You must pay the entire loan off in a single month. The loan will continue to compound until you repay it. Make the most profit possible within a year "" I think the purpose of this was for them to try different strategies and play around with them to see what works best. However, I'm wondering how this could be optimized to give the best possible result. As the problem is very convoluted I assume one could maybe use a program to simulate different situations however I wouldn't know how to do this. Feel free to simplify the problem if you wish to try it (eg. Don't use GST, premium chairs, or loans). I'm interested to see how a situation with multiple variables (the price each year) can be optimized.","My brother in year 8 was given this scenario: ""You sell chairs and make sales at monthly intervals. You start with $16,000. Each month, 17000 people buy chairs, The market average price for chairs is $72. Chairs cost $40 each when you buy 400 or more, and 35 dollars each when you buy 1000 or more. Premium chairs cost 45 dollars each when you buy 400 or more, and 40 dollars each when you buy 1000 or more. Your business sets the price of chairs. The price must be between 150. For every $4 under the market average, your business gains 0.5% of the market. For each $0.50 over the market average, your business loses 0.05% of its customers when selling standard chairs, and 0.01% of its customers when selling standard chairs, and 0.01% of its customers when selling premium chairs. For every 1000 chairs sold in a month, your business must pay an employee $5000.10% of sales must go to the government for GST. Your business may take out one loan at a time. You can only loan out as much money as you currently have. The loan compounds monthly at a rate of 18% per month. You must pay the entire loan off in a single month. The loan will continue to compound until you repay it. Make the most profit possible within a year "" I think the purpose of this was for them to try different strategies and play around with them to see what works best. However, I'm wondering how this could be optimized to give the best possible result. As the problem is very convoluted I assume one could maybe use a program to simulate different situations however I wouldn't know how to do this. Feel free to simplify the problem if you wish to try it (eg. Don't use GST, premium chairs, or loans). I'm interested to see how a situation with multiple variables (the price each year) can be optimized.",24-,"['calculus', 'multivariable-calculus', 'optimization', 'finance', 'word-problem']"
99,"Calculate $\int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx$ for $t > 0, n \ge 1$",Calculate  for,"\int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx t > 0, n \ge 1","The problem is as follows: Show that for all $t > 0, n \ge 1$ , $$ \int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx = {2n-2 \choose n -1}\frac{\pi}{(2t)^{2n -1}} $$ What I have so far: Let $f(t) = \int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx$ $$ \frac{df}{dt} = \int_{0}^{\infty}\frac{\partial}{\partial t}\frac{1}{(x^2+t^2)^n}dx $$ After simplification $$ \frac{df}{dt} = \int_{0}^{\infty}-2nt(x^2+t^2)^{-n-1}dx $$ However, I am not sure how to integrate this and WolframAlpha outputs a complicated solution, so any help would be greatly appreciated.","The problem is as follows: Show that for all , What I have so far: Let After simplification However, I am not sure how to integrate this and WolframAlpha outputs a complicated solution, so any help would be greatly appreciated.","t > 0, n \ge 1 
\int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx = {2n-2 \choose n -1}\frac{\pi}{(2t)^{2n -1}}
 f(t) = \int_{0}^{\infty}\frac{1}{(x^2 + t^2)^n}dx 
\frac{df}{dt} = \int_{0}^{\infty}\frac{\partial}{\partial t}\frac{1}{(x^2+t^2)^n}dx
 
\frac{df}{dt} = \int_{0}^{\infty}-2nt(x^2+t^2)^{-n-1}dx
","['calculus', 'integration', 'multivariable-calculus', 'partial-derivative']"
