,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differentiation of a complex-valued function of a real variable,Differentiation of a complex-valued function of a real variable,,"Let $f:[a,b] \to \mathbb{C}$ be a complex-valued function of a real variable. Say $f(t) = u(t) + i v(t)$.  Then we define the derivative of $f$ at $t \in [a,b]$ by $$ f'(t) := u'(t) + i v'(t), $$ with one-sided derivatives understood for $t \in \{a, b\}$. Why is it that to compute $f'(t)$ one can differentiate $f$ as it was a real function by treating the complex number $i$ as a real constant ? For example, to differentiate $e^{it}$, instead of writing it as $\cos(t) + i \sin(t)$ and differentiating the real and imaginary parts, it suffices to treat $i$ as a real constant and differentiate $e^{it}$ as it was a real function, that is, with derivative $i e^{it}$.","Let $f:[a,b] \to \mathbb{C}$ be a complex-valued function of a real variable. Say $f(t) = u(t) + i v(t)$.  Then we define the derivative of $f$ at $t \in [a,b]$ by $$ f'(t) := u'(t) + i v'(t), $$ with one-sided derivatives understood for $t \in \{a, b\}$. Why is it that to compute $f'(t)$ one can differentiate $f$ as it was a real function by treating the complex number $i$ as a real constant ? For example, to differentiate $e^{it}$, instead of writing it as $\cos(t) + i \sin(t)$ and differentiating the real and imaginary parts, it suffices to treat $i$ as a real constant and differentiate $e^{it}$ as it was a real function, that is, with derivative $i e^{it}$.",,"['calculus', 'complex-analysis']"
1,Calculate $ \int_{-\infty}^{\infty} x^4 H(x)^2 e^{-x^2} dx$ where $H(x)$ is a Hermite polynomial,Calculate  where  is a Hermite polynomial, \int_{-\infty}^{\infty} x^4 H(x)^2 e^{-x^2} dx H(x),"I need to calculate the following integral $$ \int_{-\infty}^{\infty} x^4 H(x)^2 e^{-x^2} dx.$$ where $H(x)$ is a Hermite polynomial. I tried using the recurrence relation, but I don't get the answer.","I need to calculate the following integral $$ \int_{-\infty}^{\infty} x^4 H(x)^2 e^{-x^2} dx.$$ where $H(x)$ is a Hermite polynomial. I tried using the recurrence relation, but I don't get the answer.",,"['calculus', 'orthogonal-polynomials']"
2,How find the value $\lim_{x\to 0}\dfrac{1}{x^2}\left(1-\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\right)$,How find the value,\lim_{x\to 0}\dfrac{1}{x^2}\left(1-\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\right),find the value $$I_{n}=\lim_{x\to 0}\dfrac{1}{x^2}\left(1-\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\right)$$ This is my methods: \begin{align*}I_{n+1}-I_{n}&=\lim_{x\to 0}\dfrac{1}{x^2}\left(\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\left(1-\sqrt[n+1]{\cos{(n+1)x}}\right)\right)\\ &=\lim_{x\to 0}\dfrac{1-\sqrt[n+1]{\cos{(n+1)x}}}{x^2}\\ &=\dfrac{n+1}{2} \end{align*} so $$I_{n}=\dfrac{n(n+1)}{4}$$ Have you other nice methods? Thank you,find the value $$I_{n}=\lim_{x\to 0}\dfrac{1}{x^2}\left(1-\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\right)$$ This is my methods: \begin{align*}I_{n+1}-I_{n}&=\lim_{x\to 0}\dfrac{1}{x^2}\left(\cos{x}\cdot\sqrt{\cos{(2x)}}\cdots\sqrt[n]{\cos{(nx)}}\left(1-\sqrt[n+1]{\cos{(n+1)x}}\right)\right)\\ &=\lim_{x\to 0}\dfrac{1-\sqrt[n+1]{\cos{(n+1)x}}}{x^2}\\ &=\dfrac{n+1}{2} \end{align*} so $$I_{n}=\dfrac{n(n+1)}{4}$$ Have you other nice methods? Thank you,,"['calculus', 'limits']"
3,Interesting Differentiation Technique,Interesting Differentiation Technique,,"@HansEngler Left the following response to this question regarding ""bad math"" that works, Here's another classical freshman calculus example: Find $\frac{d}{dx}x^x$. Alice says ""this is like  $\frac{d}{dx}x^n = nx^{n-1}$, so the answer is $x x^{x-1} = x^x$.""   Bob says ""no, this is like  $\frac{d}{dx}a^x = \log a \cdot a^x$, so the answer is $\log x \cdot x^x$.""    Charlie says ""if you're not sure, just add the two terms, so you'll get partial credit"". The answer  $\frac{d}{dx}x^x = (1 + \log x)x^x $ turns out to be correct. In this comment , @joriki asserts that this is not ""bad math"" but rather a legitimate technique, You get the derivative of any expression with respect to $x$ as the sums of all the derivatives with respect to the individual instances of $x$ while holding other instances constant. I had never previously seen such a technique so naturally I tested it on a few examples, including $\frac{d}{dx} \left( x^{ \sin x}\right)$, etc. and it provided the correct result. The following three questions arose, $ \ \ $ 1. What is the proof of its is validity? $ \ \ $ 2. Are there any examples where this technique outshines standard methods?","@HansEngler Left the following response to this question regarding ""bad math"" that works, Here's another classical freshman calculus example: Find $\frac{d}{dx}x^x$. Alice says ""this is like  $\frac{d}{dx}x^n = nx^{n-1}$, so the answer is $x x^{x-1} = x^x$.""   Bob says ""no, this is like  $\frac{d}{dx}a^x = \log a \cdot a^x$, so the answer is $\log x \cdot x^x$.""    Charlie says ""if you're not sure, just add the two terms, so you'll get partial credit"". The answer  $\frac{d}{dx}x^x = (1 + \log x)x^x $ turns out to be correct. In this comment , @joriki asserts that this is not ""bad math"" but rather a legitimate technique, You get the derivative of any expression with respect to $x$ as the sums of all the derivatives with respect to the individual instances of $x$ while holding other instances constant. I had never previously seen such a technique so naturally I tested it on a few examples, including $\frac{d}{dx} \left( x^{ \sin x}\right)$, etc. and it provided the correct result. The following three questions arose, $ \ \ $ 1. What is the proof of its is validity? $ \ \ $ 2. Are there any examples where this technique outshines standard methods?",,"['calculus', 'derivatives']"
4,Given that $x = 4\sin \left( {2y + 6} \right)$ find dy/dx in terms of x,Given that  find dy/dx in terms of x,x = 4\sin \left( {2y + 6} \right),"My attempt: $\eqalign{   & x = 4\sin \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = \left( 2 \right)\left( 4 \right)\cos \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = 8\cos \left( {2y + 6} \right)  \cr    & {{dy} \over {dx}} = {1 \over {8\cos \left( {2y + 6} \right)}} \cr} $ $\eqalign{   & x = 4\sin \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = \left( 2 \right)\left( 4 \right)\cos \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = 8\cos \left( {2y + 6} \right)  \cr    & {{dy} \over {dx}} = {1 \over {8\cos \left( {2y + 6} \right)}}  \cr    & {\cos ^2}\left( {2y + 6} \right) + {\sin ^2}\left( {2y + 6} \right) = 1  \cr    & {\cos ^2}\left( {2y + 6} \right) + {\left( {{x \over 4}} \right)^2} = 1  \cr    & {\cos ^2}\left( {2y + 6} \right) = 1 - {{{x^2}} \over {16}}  \cr    & \cos \left( {2y + 6} \right) = \sqrt {{{16 - {x^2}} \over {16}}}   \cr    & \cos \left( {2y + 6} \right) = {{\sqrt {16 - {x^2}} } \over 4}  \cr    & {{dy} \over {dx}} = {1 \over {2\sqrt {16 - {x^2}} }} = 1 \cr} $ Okay I've got it right, but the official answer confuses me, it says: ${{dy} \over {dx}} = {1 \over {8cos\left( {\arcsin \left( {{x \over 4}} \right)} \right)}} = \left( {\left(  \pm  \right){1 \over {2\sqrt {\left( {16 - {x^2}} \right)} }}} \right)$ This is the part i'm struggling to get my head around, although I arrive at the same answer. Okay $\arcsin {x \over 4} = 2y + 6$ but how does the answer then go : $\left( {\left(  \pm  \right){1 \over {2\sqrt {\left( {16 - {x^2}} \right)} }}} \right)$ is there a shortcut or trick I overlooked? I think I need some sleep, thanks...","My attempt: $\eqalign{   & x = 4\sin \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = \left( 2 \right)\left( 4 \right)\cos \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = 8\cos \left( {2y + 6} \right)  \cr    & {{dy} \over {dx}} = {1 \over {8\cos \left( {2y + 6} \right)}} \cr} $ $\eqalign{   & x = 4\sin \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = \left( 2 \right)\left( 4 \right)\cos \left( {2y + 6} \right)  \cr    & {{dx} \over {dy}} = 8\cos \left( {2y + 6} \right)  \cr    & {{dy} \over {dx}} = {1 \over {8\cos \left( {2y + 6} \right)}}  \cr    & {\cos ^2}\left( {2y + 6} \right) + {\sin ^2}\left( {2y + 6} \right) = 1  \cr    & {\cos ^2}\left( {2y + 6} \right) + {\left( {{x \over 4}} \right)^2} = 1  \cr    & {\cos ^2}\left( {2y + 6} \right) = 1 - {{{x^2}} \over {16}}  \cr    & \cos \left( {2y + 6} \right) = \sqrt {{{16 - {x^2}} \over {16}}}   \cr    & \cos \left( {2y + 6} \right) = {{\sqrt {16 - {x^2}} } \over 4}  \cr    & {{dy} \over {dx}} = {1 \over {2\sqrt {16 - {x^2}} }} = 1 \cr} $ Okay I've got it right, but the official answer confuses me, it says: ${{dy} \over {dx}} = {1 \over {8cos\left( {\arcsin \left( {{x \over 4}} \right)} \right)}} = \left( {\left(  \pm  \right){1 \over {2\sqrt {\left( {16 - {x^2}} \right)} }}} \right)$ This is the part i'm struggling to get my head around, although I arrive at the same answer. Okay $\arcsin {x \over 4} = 2y + 6$ but how does the answer then go : $\left( {\left(  \pm  \right){1 \over {2\sqrt {\left( {16 - {x^2}} \right)} }}} \right)$ is there a shortcut or trick I overlooked? I think I need some sleep, thanks...",,"['calculus', 'trigonometry', 'derivatives', 'implicit-differentiation']"
5,Show a sequence is decreasing,Show a sequence is decreasing,,I'm stuck trying to show that the following sequence is decreasing $$a_{n} = \left(\frac{n+x}{n+2x}\right)^{n}$$ where $x>0$.   I've tried treating $n$ as a real number and took derivatives but it didn't lead to anything promising.   Any hints would be appreciated.,I'm stuck trying to show that the following sequence is decreasing $$a_{n} = \left(\frac{n+x}{n+2x}\right)^{n}$$ where $x>0$.   I've tried treating $n$ as a real number and took derivatives but it didn't lead to anything promising.   Any hints would be appreciated.,,"['calculus', 'analysis']"
6,Inverse of the natural log function $y =\ln x$,Inverse of the natural log function,y =\ln x,"Of course, it is a well known fact that the inverse of $y=\ln x$ (natural logarithm of x) is $e^x$. Assuming we haven't heard of the exponential function at all, how do we prove that the inverse of $\ln x$ i.e ($\ln^{-1} x$ ) is some other function, which indeed is the so called exponential function $e^x$? Let me be a little more concrete. If $y = \ln x$, then $x= A^y$, how do I prove that $A= e$? There's another method by which I tried to arrive at the inverse of natural logarithm of $x$. By a theorem of differentiation of inverse functions $$\left.\dfrac{df}{dx}\right|_{x=a} \cdot\left.\dfrac{df^{-1}}{dx}\right|_{x=f(a)}=1$$ I got an equation which looked like $$\left.\dfrac{dF(x)}{dx}\right|_{x=\ln k}=k$$  where $k$ is a real number. How do I actually show that $F(x)$ is that same exponential function $e^x$? Is there any way of solving such an equation, or such a problem? Or am I just talking nonsense? I would love to be enlightened by all of you. Thanks in advance. :)","Of course, it is a well known fact that the inverse of $y=\ln x$ (natural logarithm of x) is $e^x$. Assuming we haven't heard of the exponential function at all, how do we prove that the inverse of $\ln x$ i.e ($\ln^{-1} x$ ) is some other function, which indeed is the so called exponential function $e^x$? Let me be a little more concrete. If $y = \ln x$, then $x= A^y$, how do I prove that $A= e$? There's another method by which I tried to arrive at the inverse of natural logarithm of $x$. By a theorem of differentiation of inverse functions $$\left.\dfrac{df}{dx}\right|_{x=a} \cdot\left.\dfrac{df^{-1}}{dx}\right|_{x=f(a)}=1$$ I got an equation which looked like $$\left.\dfrac{dF(x)}{dx}\right|_{x=\ln k}=k$$  where $k$ is a real number. How do I actually show that $F(x)$ is that same exponential function $e^x$? Is there any way of solving such an equation, or such a problem? Or am I just talking nonsense? I would love to be enlightened by all of you. Thanks in advance. :)",,"['calculus', 'algebra-precalculus', 'logarithms']"
7,The control of eigenvalue,The control of eigenvalue,,"$A = ({a_{ij}})$ and $B = ({b_{ij}})$ are $n \times n$ symmetric real matrixes whose eigenvalue are ${\lambda _1} \le {\lambda _2} \le  \cdots  \le {\lambda _n}$ and ${\mu _1} \le {\mu _2} \le  \cdots  \le {\mu _n}$ respectively. If $\left| {{a_{ij}} - {b_{ij}}} \right| \le \varepsilon $, please show that $\left| {{\lambda _i} - {\mu _i}} \right| \le n\varepsilon $.","$A = ({a_{ij}})$ and $B = ({b_{ij}})$ are $n \times n$ symmetric real matrixes whose eigenvalue are ${\lambda _1} \le {\lambda _2} \le  \cdots  \le {\lambda _n}$ and ${\mu _1} \le {\mu _2} \le  \cdots  \le {\mu _n}$ respectively. If $\left| {{a_{ij}} - {b_{ij}}} \right| \le \varepsilon $, please show that $\left| {{\lambda _i} - {\mu _i}} \right| \le n\varepsilon $.",,"['calculus', 'linear-algebra']"
8,Proving a limit involved in the Lagrangian inversion of $\frac{\log\sqrt{1+x}}{\sqrt{1+x}}$,Proving a limit involved in the Lagrangian inversion of,\frac{\log\sqrt{1+x}}{\sqrt{1+x}},"In my attempt to complete this answer , I hit a snag in showing that $$\lim_{t\to 0} \dfrac{\mathrm d^{k-1}}{\mathrm dt^{k-1}}\left(\frac{t\sqrt{1+t}}{\log\sqrt{1+t}}\right)^k=2(k+2)^{k-1}$$ This shows up when trying to apply Lagrangian inversion to the function $\dfrac{\log\sqrt{1+x}}{\sqrt{1+x}}$. My sticking point here is that I am unable to find a convenient expression for the derivatives. Is there an easy proof for this?","In my attempt to complete this answer , I hit a snag in showing that $$\lim_{t\to 0} \dfrac{\mathrm d^{k-1}}{\mathrm dt^{k-1}}\left(\frac{t\sqrt{1+t}}{\log\sqrt{1+t}}\right)^k=2(k+2)^{k-1}$$ This shows up when trying to apply Lagrangian inversion to the function $\dfrac{\log\sqrt{1+x}}{\sqrt{1+x}}$. My sticking point here is that I am unable to find a convenient expression for the derivatives. Is there an easy proof for this?",,"['calculus', 'limits']"
9,Generating an independent set,Generating an independent set,,"Suppose $f_1, f_2,...$ are a set of functions $\mathbb{R} \rightarrow \mathbb{R}$ so that each is the power of some non-constant function h. So $ f_ i=h^{n_i}$ for some natural number n. Is it possible for such a set to be linearly dependent? What additional conditions could ensure independence? I was thinking about this because I need to show that a set in linearly independent. And I was not convinced that having the functions be powered lhjlj would be sufficient.","Suppose $f_1, f_2,...$ are a set of functions $\mathbb{R} \rightarrow \mathbb{R}$ so that each is the power of some non-constant function h. So $ f_ i=h^{n_i}$ for some natural number n. Is it possible for such a set to be linearly dependent? What additional conditions could ensure independence? I was thinking about this because I need to show that a set in linearly independent. And I was not convinced that having the functions be powered lhjlj would be sufficient.",,['calculus']
10,"Compute $\lim\limits_{a \to 0^+} \left(a \int_1^{\infty} e^{-ax}\cos \left(\frac{2\pi}{1+x^{2}} \right)\,\mathrm dx\right)$",Compute,"\lim\limits_{a \to 0^+} \left(a \int_1^{\infty} e^{-ax}\cos \left(\frac{2\pi}{1+x^{2}} \right)\,\mathrm dx\right)","How can I compute the following limit? $$\lim_{a \to 0^+} \left(a \int_{1}^{\infty} e^{-ax}\cos \left(\frac{2\pi}{1+x^{2}} \right)\,\mathrm dx\right)$$ Any hints you can please give? Cheers","How can I compute the following limit? $$\lim_{a \to 0^+} \left(a \int_{1}^{\infty} e^{-ax}\cos \left(\frac{2\pi}{1+x^{2}} \right)\,\mathrm dx\right)$$ Any hints you can please give? Cheers",,"['calculus', 'integration']"
11,To what extent can you manipulate differentials like dy and dt like actual values?,To what extent can you manipulate differentials like dy and dt like actual values?,,"I have been thinking about the differentials that we use in derivatives and integrals. For example, I have an equation: $${\int{w}{dr}} = \text{other stuff}$$ The context for this strange equation was: $$\begin{align*} q'(t) &= a - \frac{b}{r(t)},\\ r'(t) &= c - d r(t) - e q(t)\\ r''(t)&= d r'(t) - e q'(t)\\ r''(t)&= d r' - e a + \frac{eb}{r}\\ w&=r'\\ w' &= d w - ea + \frac{eb}{r}\\ \frac{dw}{dr} \frac{dr}{dt}&= d w - ea + \frac{eb}{r}\\ w \frac{dw}{dr} &= dw - ea + \frac{eb}{r} \end{align*}$$ Integrate both sides with respect to $r$: $$\frac{w^2}{2} = \int{d*w dr} - ea r + eb log(r)$$ Shift the integral to the left hand side: $$\int d * w dr = - \frac{w^2}{2} - ea * r - eb * log(r)$$ Divide by $d$: $$\int w dr = \frac{w^2}{2d} - \frac{ear}{d}-\frac{eb}{d} log(r)$$ And I get to that $w\, dr$. So if I can replace $w$ with ${\frac{dr}{dt}}$, and differentials can be multiplied like normal, I would get ${\frac{dr^2}{dt}}$. Then I have the idea of multiplying by ${\frac{dt}{dt}}$. Can I go ahead and do this? That would give me  $${\int{\frac{dr^2}{dt^2} dt}}$$ Which looks like ${w^2}$ to me. But can I do that?","I have been thinking about the differentials that we use in derivatives and integrals. For example, I have an equation: $${\int{w}{dr}} = \text{other stuff}$$ The context for this strange equation was: $$\begin{align*} q'(t) &= a - \frac{b}{r(t)},\\ r'(t) &= c - d r(t) - e q(t)\\ r''(t)&= d r'(t) - e q'(t)\\ r''(t)&= d r' - e a + \frac{eb}{r}\\ w&=r'\\ w' &= d w - ea + \frac{eb}{r}\\ \frac{dw}{dr} \frac{dr}{dt}&= d w - ea + \frac{eb}{r}\\ w \frac{dw}{dr} &= dw - ea + \frac{eb}{r} \end{align*}$$ Integrate both sides with respect to $r$: $$\frac{w^2}{2} = \int{d*w dr} - ea r + eb log(r)$$ Shift the integral to the left hand side: $$\int d * w dr = - \frac{w^2}{2} - ea * r - eb * log(r)$$ Divide by $d$: $$\int w dr = \frac{w^2}{2d} - \frac{ear}{d}-\frac{eb}{d} log(r)$$ And I get to that $w\, dr$. So if I can replace $w$ with ${\frac{dr}{dt}}$, and differentials can be multiplied like normal, I would get ${\frac{dr^2}{dt}}$. Then I have the idea of multiplying by ${\frac{dt}{dt}}$. Can I go ahead and do this? That would give me  $${\int{\frac{dr^2}{dt^2} dt}}$$ Which looks like ${w^2}$ to me. But can I do that?",,"['calculus', 'ordinary-differential-equations']"
12,How to evaluate $\int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx$?,How to evaluate ?,\int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx,"I want to evaluate $$\int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx$$ . This question is from RMM. I tried to separate the integral into: $$\begin{align*} & \int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx \\ &= \int_0 ^ 1 \frac{\ln (x)}{x^2+1}dx+\int_0 ^ 1 \frac{\ln^2 (x)}{x^2+1}dx + \int_0 ^ 1 \frac{\ln (x)\ln(1-x)}{x^2+1}dx \\ &= \int_0^{\frac{\pi}{4}} \ln(\tan(x))dx+\int_0^{\frac{\pi}{4}} \ln^2(\tan(x))dx+\int_0^{\frac{\pi}{4}}\ln(\tan(x))\ln(1-\tan(x)) dx \end{align*}$$ I have tried all the integration methods that I know, but I couldn't reach any useful result.","I want to evaluate . This question is from RMM. I tried to separate the integral into: I have tried all the integration methods that I know, but I couldn't reach any useful result.","\int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx \begin{align*}
& \int_0 ^ 1 \frac{\ln (x)(1+\ln(x) +\ln(1-x))}{x^2+1}dx \\
&= \int_0 ^ 1 \frac{\ln (x)}{x^2+1}dx+\int_0 ^ 1 \frac{\ln^2 (x)}{x^2+1}dx + \int_0 ^ 1 \frac{\ln (x)\ln(1-x)}{x^2+1}dx \\
&= \int_0^{\frac{\pi}{4}} \ln(\tan(x))dx+\int_0^{\frac{\pi}{4}} \ln^2(\tan(x))dx+\int_0^{\frac{\pi}{4}}\ln(\tan(x))\ln(1-\tan(x)) dx
\end{align*}","['calculus', 'integration', 'definite-integrals']"
13,"$\int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u = K_{4m}(\sigma) + ?$",,"\int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u = K_{4m}(\sigma) + ?","As I delved into the intricacies of a fluid mechanics problem that revolved around Green's functions in porous media, I stumbled upon the following intriguing infinite integral: $$ \phi_m (\sigma) = \int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u \, ,  $$ wherein $\sigma \ge 0$ and $m \in \mathbb{N}$ . The first four terms can be obtained as \begin{align} \phi_0  &= K_0(\sigma) \, , \\ \phi_1  &= K_4(\sigma) + \frac{4}{\sigma^4} \left( \sigma^2-12\right) \, , \\ \phi_2  &= K_8(\sigma) + \frac{8}{\sigma^8} \left( \sigma^6-60\sigma^4+2\,880\sigma^2-80\,640 \right) \, , \\ \phi_3 &= K_{12} (\sigma) + \frac{12}{\sigma^{12}} \left( \sigma^{10}-140\sigma^8 + 17\,920\sigma^6 - 1\,935\,360\sigma^4 + 154\,828\,800\sigma^2-6\,812\,467\,200 \right) \, . \end{align} It appears that the overall expression will begin with the first term, denoted as $K_{4m}(\sigma)$ , accompanied by a generalized hypergeometric function for which I have yet to discover a general expression. Any assistance in this matter would be greatly valued. Thank you.","As I delved into the intricacies of a fluid mechanics problem that revolved around Green's functions in porous media, I stumbled upon the following intriguing infinite integral: wherein and . The first four terms can be obtained as It appears that the overall expression will begin with the first term, denoted as , accompanied by a generalized hypergeometric function for which I have yet to discover a general expression. Any assistance in this matter would be greatly valued. Thank you.","
\phi_m (\sigma) = \int_0^\infty \frac{u}{u^2+\sigma^2} \, J_{4m} (u) \, \mathrm{d} u \, , 
 \sigma \ge 0 m \in \mathbb{N} \begin{align}
\phi_0  &= K_0(\sigma) \, , \\
\phi_1  &= K_4(\sigma) + \frac{4}{\sigma^4} \left( \sigma^2-12\right) \, , \\
\phi_2  &= K_8(\sigma) + \frac{8}{\sigma^8}
\left( \sigma^6-60\sigma^4+2\,880\sigma^2-80\,640 \right) \, , \\
\phi_3 &= K_{12} (\sigma)
+ \frac{12}{\sigma^{12}}
\left( \sigma^{10}-140\sigma^8 + 17\,920\sigma^6 - 1\,935\,360\sigma^4 + 154\,828\,800\sigma^2-6\,812\,467\,200 \right) \, .
\end{align} K_{4m}(\sigma)","['calculus', 'integration', 'improper-integrals', 'indefinite-integrals', 'bessel-functions']"
14,What is the area of $y^2=\sqrt x-x$ (Guitar Pick),What is the area of  (Guitar Pick),y^2=\sqrt x-x,"I made a typo while experimenting on Desmos and typed $y^2=\sqrt x-x$ . It drew a shape, one that I've never seen before: With my very limited knowledge of calculus, I know the area would be equal to: $$2\int_0^1 \sqrt{\sqrt x - x} \,dx$$ However, I have no clue how to evaluate this integral. Using Desmos, I can get a decimal approximation (it's about 0.785), and Wolfram Alpha can give me the final result ( $\pi/4$ ). No site I can think of has the solution and steps to solve it, so I figured I'd ask it here. How would you evaluate this integral?","I made a typo while experimenting on Desmos and typed . It drew a shape, one that I've never seen before: With my very limited knowledge of calculus, I know the area would be equal to: However, I have no clue how to evaluate this integral. Using Desmos, I can get a decimal approximation (it's about 0.785), and Wolfram Alpha can give me the final result ( ). No site I can think of has the solution and steps to solve it, so I figured I'd ask it here. How would you evaluate this integral?","y^2=\sqrt x-x 2\int_0^1 \sqrt{\sqrt x - x} \,dx \pi/4","['calculus', 'integration']"
15,Why do the partial sums of the Maclaurin series expansion of $\sin$ approximate it better than their hyperbolic counterparts approximate $\sinh$?,Why do the partial sums of the Maclaurin series expansion of  approximate it better than their hyperbolic counterparts approximate ?,\sin \sinh,"While exploring Taylor series with numerical and graphing tools, I noticed a very peculiar and interesting fact: the partial sums of the Maclaurin series expansion of $\sin(x)$ approximate it better than their hyperbolic counterparts approximate $\sinh(x)$ . To be specific, I noticed that for every $x\in\mathbb{R}$ and $k\in\mathbb{N}$ , the following inequality seems (haven't proven it) to hold $$\left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right|$$ For all $x\neq 0$ , the inequality is strict, showing that $\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}$ does indeed approximate $\sin(x)$ better than $\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}$ approximates $\sinh(x)$ . But why do they provide better approximations? The remainders $$\sin(x)-\sum_{n=0}^{k}(-1)^n\frac{x^{2n+1}}{(2n+1)!},\text{ }\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}$$ obviously can't be equal everywhere, but given the similarity between the partial sums for each function, it's unclear to me why the $\sin$ remainder should be bounded by an inequality as ""uniform"" as the one above. Why isn't it the other way around, with $\sinh$ 's remainder being bounded by $\sin$ 's? Why is the inequality true for all $x\in\mathbb{R}$ , and not just some disjoint intervals? Looking at the partial sums for each series, it can be seen that the only difference is the $(-1)^n$ factor for the $\sin$ 's series, which doesn't seem like the kind of thing that can make the difference between $$\left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right|$$ and some other inequality relating the two remainders. Can someone give an intuitive explanation for why this happens?","While exploring Taylor series with numerical and graphing tools, I noticed a very peculiar and interesting fact: the partial sums of the Maclaurin series expansion of approximate it better than their hyperbolic counterparts approximate . To be specific, I noticed that for every and , the following inequality seems (haven't proven it) to hold For all , the inequality is strict, showing that does indeed approximate better than approximates . But why do they provide better approximations? The remainders obviously can't be equal everywhere, but given the similarity between the partial sums for each function, it's unclear to me why the remainder should be bounded by an inequality as ""uniform"" as the one above. Why isn't it the other way around, with 's remainder being bounded by 's? Why is the inequality true for all , and not just some disjoint intervals? Looking at the partial sums for each series, it can be seen that the only difference is the factor for the 's series, which doesn't seem like the kind of thing that can make the difference between and some other inequality relating the two remainders. Can someone give an intuitive explanation for why this happens?","\sin(x) \sinh(x) x\in\mathbb{R} k\in\mathbb{N} \left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right| x\neq 0 \sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!} \sin(x) \sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!} \sinh(x) \sin(x)-\sum_{n=0}^{k}(-1)^n\frac{x^{2n+1}}{(2n+1)!},\text{ }\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!} \sin \sinh \sin x\in\mathbb{R} (-1)^n \sin \left|\sin(x)-\sum_{n=0}^{k}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}\right|\leq\left|\sinh(x)-\sum_{n=0}^{k}\frac{x^{2n+1}}{(2n+1)!}\right|","['calculus', 'sequences-and-series', 'taylor-expansion']"
16,Evaluating a definite integral by introducing change of variables,Evaluating a definite integral by introducing change of variables,,"In a Physics class, I encountered the following integral: $$\int_{0}^{\infty} \left(\frac{x^2}{\exp\left(x-a\right)+1}-\frac{x^2}{\exp\left(x+a\right)+1}\right)\,\mathrm{d}x\text{,}$$ where $a$ is a constant. It is stated to yield: $$\int_{0}^{\infty}\left(\frac{x^2}{\exp\left(x-a\right)+1}-\frac{x^2}{\exp\left(x+a\right)+1}\right)\,\mathrm{d}x=\frac{1}{3}\left[\pi^2a+a^3\right]$$ I first introduced a change of variables: $$y:=x-a$$ $$z:=x^\prime+a$$ This gives: $$\int_{0}^{\infty}\frac{x^2}{\exp\left(x-a\right)+1}\,\mathrm{d}x-\int_{0}^{\infty}\frac{\left(x^\prime\right)^2}{\exp\left(x^\prime+a\right)+1}\,\mathrm{d}x^\prime=\int_{-a}^{\infty}\frac{\left(y+a\right)^2}{e^y + 1}\,\mathrm{d}y-\int_{a}^{\infty}\frac{\left(z-a\right)^2}{e^z+1}\,\mathrm{d}z$$ However, when introduced to a computer, the definite integrals are stated to have no solution. I actually expected to get integrals of the form: $$\int_{0}^{\infty}\frac{y}{e^y+1}\,\mathrm{d}y=\frac{\pi^2}{12}$$ What am I missing here?","In a Physics class, I encountered the following integral: where is a constant. It is stated to yield: I first introduced a change of variables: This gives: However, when introduced to a computer, the definite integrals are stated to have no solution. I actually expected to get integrals of the form: What am I missing here?","\int_{0}^{\infty} \left(\frac{x^2}{\exp\left(x-a\right)+1}-\frac{x^2}{\exp\left(x+a\right)+1}\right)\,\mathrm{d}x\text{,} a \int_{0}^{\infty}\left(\frac{x^2}{\exp\left(x-a\right)+1}-\frac{x^2}{\exp\left(x+a\right)+1}\right)\,\mathrm{d}x=\frac{1}{3}\left[\pi^2a+a^3\right] y:=x-a z:=x^\prime+a \int_{0}^{\infty}\frac{x^2}{\exp\left(x-a\right)+1}\,\mathrm{d}x-\int_{0}^{\infty}\frac{\left(x^\prime\right)^2}{\exp\left(x^\prime+a\right)+1}\,\mathrm{d}x^\prime=\int_{-a}^{\infty}\frac{\left(y+a\right)^2}{e^y + 1}\,\mathrm{d}y-\int_{a}^{\infty}\frac{\left(z-a\right)^2}{e^z+1}\,\mathrm{d}z \int_{0}^{\infty}\frac{y}{e^y+1}\,\mathrm{d}y=\frac{\pi^2}{12}","['calculus', 'integration', 'definite-integrals']"
17,Definition of differentiation in context of abstract algebra.,Definition of differentiation in context of abstract algebra.,,"In any regular calculus or real analysis course, we learn the definition of the derivative of a function $f(x)$ as $$f^\prime (x)=\lim\limits_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h}$$ However while studying abstract algebra we come to know that differentiation is just like any operation (like addition, multiplication etc.) but on functions. So I want to know that is there a way to define an algebraic structure with the underlying set as the set of all differentiable functions and the operation of differentiation. And also if it's possible to define differentiation in such a manner, how to connect it with the analytical definition of differentiation.","In any regular calculus or real analysis course, we learn the definition of the derivative of a function as However while studying abstract algebra we come to know that differentiation is just like any operation (like addition, multiplication etc.) but on functions. So I want to know that is there a way to define an algebraic structure with the underlying set as the set of all differentiable functions and the operation of differentiation. And also if it's possible to define differentiation in such a manner, how to connect it with the analytical definition of differentiation.",f(x) f^\prime (x)=\lim\limits_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h},"['calculus', 'abstract-algebra']"
18,"Spivak vs uncommon calc texts (Silverman, Lax, Sasane) for a rigorous introductions","Spivak vs uncommon calc texts (Silverman, Lax, Sasane) for a rigorous introductions",,"Long story short, I'm an aspiring physics person, probably theoretical, looking to study introductory at or approaching the rigor of a math major. Spivak is often recommended, but I was wondering if anyone had experience with the books of Lax, Silverman, or Sasane. I've been out of undergrad for almost a decade but already reviewed and feel totally comfortable with pre-calc. I have no deadlines and can devote as much time as needed to a book, but I want to set off on the right foot. My ideal book, if it exists: Gets to the why . I hate knowing how to do a thing without knowing why it works. Hence the rigor. Should be as as self-study-friendly as possible. As in, I shouldn't have to rely on outside resources to understand problems in the book. Isn't too dry. It helps if the book has some sort of beauty and/or humor. Has some applications. This isn't totally essential, but I would like to some relevance to the real world to keep me entertained. Likewise, I do appreciate historical applications and reasoning for context. The length of the book itself could not matter less to me. If anything, the wordier, probably the better. Spivak seems to be the go-to recommendation. I started it (just on Chapter 2 now) and have liked it, but some of the calc books that most caught my eye are ones with few reviews or impressions. So I was hoping people here might've read one/some of these and compared to spivak. -Calculus with Applications by Lax and Terell(Second Edition, 2014): Strong emphasis on applications, with proofs. -Modern Calculus and Analytic Geometry by Silverman: At first glance, looks great. It's an old book ('69) but seems to have been very well written, with plenty of explanations between proofs. -The How and Why of One Variable Calculus by Sasane: Somewhat similar to the above but is quite a new book (2015ish). I like that it has full solutions to every problem. Any chance someone has read these? Or have another perhaps better-than-spivak recommendation? I think I'll be okay whichever book I end up choosing, but it'd be nice to hear some opinions from people with more experience.","Long story short, I'm an aspiring physics person, probably theoretical, looking to study introductory at or approaching the rigor of a math major. Spivak is often recommended, but I was wondering if anyone had experience with the books of Lax, Silverman, or Sasane. I've been out of undergrad for almost a decade but already reviewed and feel totally comfortable with pre-calc. I have no deadlines and can devote as much time as needed to a book, but I want to set off on the right foot. My ideal book, if it exists: Gets to the why . I hate knowing how to do a thing without knowing why it works. Hence the rigor. Should be as as self-study-friendly as possible. As in, I shouldn't have to rely on outside resources to understand problems in the book. Isn't too dry. It helps if the book has some sort of beauty and/or humor. Has some applications. This isn't totally essential, but I would like to some relevance to the real world to keep me entertained. Likewise, I do appreciate historical applications and reasoning for context. The length of the book itself could not matter less to me. If anything, the wordier, probably the better. Spivak seems to be the go-to recommendation. I started it (just on Chapter 2 now) and have liked it, but some of the calc books that most caught my eye are ones with few reviews or impressions. So I was hoping people here might've read one/some of these and compared to spivak. -Calculus with Applications by Lax and Terell(Second Edition, 2014): Strong emphasis on applications, with proofs. -Modern Calculus and Analytic Geometry by Silverman: At first glance, looks great. It's an old book ('69) but seems to have been very well written, with plenty of explanations between proofs. -The How and Why of One Variable Calculus by Sasane: Somewhat similar to the above but is quite a new book (2015ish). I like that it has full solutions to every problem. Any chance someone has read these? Or have another perhaps better-than-spivak recommendation? I think I'll be okay whichever book I end up choosing, but it'd be nice to hear some opinions from people with more experience.",,"['calculus', 'reference-request', 'book-recommendation']"
19,An integration-via-summation formula,An integration-via-summation formula,,"For symbolic transformation of integrals and series I occasionally use this formula: $$\int_0^1f(x)\,dx=-\sum_{n=1}^\infty\sum_{m=1}^{2^n-1}\frac{(-1)^m}{2^n}f\left(\frac m{2^n}\right)\tag{$\diamond$}$$ I believe it holds for all piecewise- smooth functions $f$ of bounded variation defined on $(0,1).$ Is it correct? I also think this condition might be too tight and can be relaxed. Could you please suggest a wider natural class of functions for which $(\diamond)$ holds? Is there a known name for this formula? Could you provide some reference for it?",For symbolic transformation of integrals and series I occasionally use this formula: I believe it holds for all piecewise- smooth functions of bounded variation defined on Is it correct? I also think this condition might be too tight and can be relaxed. Could you please suggest a wider natural class of functions for which holds? Is there a known name for this formula? Could you provide some reference for it?,"\int_0^1f(x)\,dx=-\sum_{n=1}^\infty\sum_{m=1}^{2^n-1}\frac{(-1)^m}{2^n}f\left(\frac m{2^n}\right)\tag{\diamond} f (0,1). (\diamond)","['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'summation']"
20,Evaluate $\int_0^{\infty} \frac {\ln(1+x^3)}{1+x^2}dx$,Evaluate,\int_0^{\infty} \frac {\ln(1+x^3)}{1+x^2}dx,"Prove that $$\int_0^{\infty} \frac {\ln(1+x^3)}{1+x^2}dx=\frac {\pi \ln 2}{4}-\frac {G}{3}+\frac {2\pi}{3}\ln(2+\sqrt 3)$$ Where $G$ is the Catalan's constant. Actually I proved this using the Feynman's trick namely by introducing the parameter $a$ such that $$\xi(a)=\int_0^{\infty} \frac {\ln(1+ax^3)}{1+x^2}dx$$ Where it is clear that $\xi(0)=0$ , hence we just need $$\int_0^1 \xi'(a)da$$ which I found too. Hence proving the statement, but this method was too much lengthy because it involved heavy partial fraction decomposition and one infinite summation. Can someone suggest some better method? Edit: I also tried some trigonometry bashing by using the substitution $x=\tan \theta$ but got stuck midway","Prove that Where is the Catalan's constant. Actually I proved this using the Feynman's trick namely by introducing the parameter such that Where it is clear that , hence we just need which I found too. Hence proving the statement, but this method was too much lengthy because it involved heavy partial fraction decomposition and one infinite summation. Can someone suggest some better method? Edit: I also tried some trigonometry bashing by using the substitution but got stuck midway",\int_0^{\infty} \frac {\ln(1+x^3)}{1+x^2}dx=\frac {\pi \ln 2}{4}-\frac {G}{3}+\frac {2\pi}{3}\ln(2+\sqrt 3) G a \xi(a)=\int_0^{\infty} \frac {\ln(1+ax^3)}{1+x^2}dx \xi(0)=0 \int_0^1 \xi'(a)da x=\tan \theta,"['calculus', 'integration', 'definite-integrals']"
21,$1+\frac {1}{4}(1+\frac {1}{4}) +\frac {1}{9}(1+\frac {1}{4} +\frac {1}{9})+....$,,1+\frac {1}{4}(1+\frac {1}{4}) +\frac {1}{9}(1+\frac {1}{4} +\frac {1}{9})+....,Show that $$1+\frac {1}{4} \bigg(1+\frac {1}{4}\bigg) +\frac {1}{9} \bigg(1+\frac {1}{4} +\frac {1}{9}\bigg)+.....$$ converges. Can you find the exact value of the sum. My effort: I have proved the convergence with comparing to $$\bigg(\sum _1^\infty \frac {1}{n^2}\bigg)^2$$ I have not figure out the exact sum. Any suggestions??,Show that converges. Can you find the exact value of the sum. My effort: I have proved the convergence with comparing to I have not figure out the exact sum. Any suggestions??,1+\frac {1}{4} \bigg(1+\frac {1}{4}\bigg) +\frac {1}{9} \bigg(1+\frac {1}{4} +\frac {1}{9}\bigg)+..... \bigg(\sum _1^\infty \frac {1}{n^2}\bigg)^2,"['calculus', 'sequences-and-series', 'convergence-divergence']"
22,Solve $\lim_{n\to \infty} \int_{0}^{\pi\over3} {{\sin^nx}\over \sin^nx+\cos^nx}dx$,Solve,\lim_{n\to \infty} \int_{0}^{\pi\over3} {{\sin^nx}\over \sin^nx+\cos^nx}dx,"$$\lim_{n\to \infty} \int_{0}^{\pi\over3} {{\sin^nx}\over \sin^nx+\cos^nx}dx$$ I really don't know what to do. It was really simple if the upper limit was $\pi/2$, because I can change $x=\pi/2-t$, but in my case I have no idea. Some help please? Thank you!","$$\lim_{n\to \infty} \int_{0}^{\pi\over3} {{\sin^nx}\over \sin^nx+\cos^nx}dx$$ I really don't know what to do. It was really simple if the upper limit was $\pi/2$, because I can change $x=\pi/2-t$, but in my case I have no idea. Some help please? Thank you!",,"['calculus', 'integration', 'limits']"
23,How does one show that $\lim_{n\to\infty}\ln{n}-\int_{0}^{n}{e^x-x-1\over x(e^x+1)}=\ln{\pi}-\gamma$,How does one show that,\lim_{n\to\infty}\ln{n}-\int_{0}^{n}{e^x-x-1\over x(e^x+1)}=\ln{\pi}-\gamma,Consider $$\lim_{n\to\infty}\ln{n}-\int_{0}^{n}{e^x-x-1\over x(e^x+1)}=\ln{\pi}-\gamma\tag1$$ How does one show that? Wolfram integrator can't evaluate the indefinite integral $(1)$,Consider $$\lim_{n\to\infty}\ln{n}-\int_{0}^{n}{e^x-x-1\over x(e^x+1)}=\ln{\pi}-\gamma\tag1$$ How does one show that? Wolfram integrator can't evaluate the indefinite integral $(1)$,,"['calculus', 'integration', 'limits']"
24,Explain why $\int_0^\infty\frac{\sin{4x}}{x}\prod\limits_{k=1}^n \cos\left(\frac{x}{k}\right) dx\approx\frac{\pi}{2}$,Explain why,\int_0^\infty\frac{\sin{4x}}{x}\prod\limits_{k=1}^n \cos\left(\frac{x}{k}\right) dx\approx\frac{\pi}{2},"Why do we have, for every $n\in\mathbb N$, $$\int_0^\infty\left(\prod_{k=1}^n \cos\left(\frac{x}{k}\right)\right)\frac{\sin{4x}}{x}dx\approx\frac{\pi}{2}\ ?$$","Why do we have, for every $n\in\mathbb N$, $$\int_0^\infty\left(\prod_{k=1}^n \cos\left(\frac{x}{k}\right)\right)\frac{\sin{4x}}{x}dx\approx\frac{\pi}{2}\ ?$$",,"['calculus', 'integration', 'sequences-and-series', 'pi', 'trigonometric-integrals']"
25,"Prove via differentiation that integral $\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta = \pi(\frac{\pi}{2}-\alpha) $",Prove via differentiation that integral,"\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta = \pi(\frac{\pi}{2}-\alpha) ","Prove via differentiation that integral    $$\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta$$   is equal to   $$ \pi\left(\frac{\pi}{2} - \alpha\right) $$   where $0\leq \alpha\leq \frac{\pi}{2}.$ After partially differentiating wrt $\alpha$ I have tried via substitution; $$u=\log(1+\cos\alpha\cos\theta)$$ Then I get; $$\frac{dI(\alpha)}{d\alpha}=-\sin\alpha\int_0^\pi \frac{1}{1+\cos\alpha\cos\theta}\,d\theta$$ I can not currently see how to progress this to stated answer. Please show full working. The question is from G Stephenson's 'Mathematical Methods for Science students' - page 172.","Prove via differentiation that integral    $$\int_0^\pi \frac{\log(1+\cos\alpha\cos\theta)}{\cos\theta}\,d\theta$$   is equal to   $$ \pi\left(\frac{\pi}{2} - \alpha\right) $$   where $0\leq \alpha\leq \frac{\pi}{2}.$ After partially differentiating wrt $\alpha$ I have tried via substitution; $$u=\log(1+\cos\alpha\cos\theta)$$ Then I get; $$\frac{dI(\alpha)}{d\alpha}=-\sin\alpha\int_0^\pi \frac{1}{1+\cos\alpha\cos\theta}\,d\theta$$ I can not currently see how to progress this to stated answer. Please show full working. The question is from G Stephenson's 'Mathematical Methods for Science students' - page 172.",,"['calculus', 'derivatives', 'definite-integrals']"
26,"Given $P(t): [0,1]\to [0,1]^2$ a space filling curve, can we calculate $\iint_{[0,1]^2}f(x,y) dxdy$ as $\int_0^1 f(P(t))\,dt$ or something alike?","Given  a space filling curve, can we calculate  as  or something alike?","P(t): [0,1]\to [0,1]^2 \iint_{[0,1]^2}f(x,y) dxdy \int_0^1 f(P(t))\,dt","Given $P(t): [0,1]\to [0,1]^2$ a continuous bijection, can we calculate $\iint_{[0,1]^2}f(x,y)\, dx\,dy$ as $\int_0^1 f(P(t))\,dt$ or something alike? I'm thinking of the $P(t)$s as  peano curves: we know such continuous bijections exist, thus, with a single parameter $t$, we can fill up the entire domain of integration $D\subseteq \Bbb R^2$ and so, I'd think that we should be able to calculate the double integral in the title with a single integral (integrating with respect to $t$). Is this possible? E: As discussed in the comments of the only answer, there may be a few annoying technicalities here ($P$ not being a bijection), I'd rather not bother with them, but see if this idea is usable somehow. I'm mostly interested in the Riemann or R-S integral, but related stuff about the lebesgue integral is also welcome.","Given $P(t): [0,1]\to [0,1]^2$ a continuous bijection, can we calculate $\iint_{[0,1]^2}f(x,y)\, dx\,dy$ as $\int_0^1 f(P(t))\,dt$ or something alike? I'm thinking of the $P(t)$s as  peano curves: we know such continuous bijections exist, thus, with a single parameter $t$, we can fill up the entire domain of integration $D\subseteq \Bbb R^2$ and so, I'd think that we should be able to calculate the double integral in the title with a single integral (integrating with respect to $t$). Is this possible? E: As discussed in the comments of the only answer, there may be a few annoying technicalities here ($P$ not being a bijection), I'd rather not bother with them, but see if this idea is usable somehow. I'm mostly interested in the Riemann or R-S integral, but related stuff about the lebesgue integral is also welcome.",,"['calculus', 'integration', 'multiple-integral']"
27,Is there a notion of a complex derivative or complex integral?,Is there a notion of a complex derivative or complex integral?,,"While reading about fractional calculus in http://arxiv.org/pdf/math/0110241.pdf , I came across the following quote: Fractional integration and fractional differentiation are generalisations of notions of integer-order integration and differentiation, and include n-th derivatives and n-folded integrals (n denotes an integer number) as particular cases. Let $D = \frac{d}{dx}$ . We have found meaningful notions of $D^2$ and $D^{-1}$ (derivative and antiderivative, respectively, of integer-order) and $D^{\frac{1}{2}}$ (derivative of fractional-order) , and we can say that integer differentiation (where given $D^n , n \in \mathbb{Z}$) is a special case of more general fractional differentiation (where given $D^n , n \in \mathbb{R}$). I'm wondering if there's some meaningful notion of ""complex differentiation"", say something like $D^i$, where fractional differentiation is a special case (note that anti differentiation is a special case of differentiation; namely, given $D^n$, anti differentiation occurs when $n$ is real and negative). Is this conceivable? If so, are there any apparent applications of this? Sorry if this is a dumb question (by dumb, I mean something that I could've found elsewhere online). I've searched around and haven't found anything on this yet.","While reading about fractional calculus in http://arxiv.org/pdf/math/0110241.pdf , I came across the following quote: Fractional integration and fractional differentiation are generalisations of notions of integer-order integration and differentiation, and include n-th derivatives and n-folded integrals (n denotes an integer number) as particular cases. Let $D = \frac{d}{dx}$ . We have found meaningful notions of $D^2$ and $D^{-1}$ (derivative and antiderivative, respectively, of integer-order) and $D^{\frac{1}{2}}$ (derivative of fractional-order) , and we can say that integer differentiation (where given $D^n , n \in \mathbb{Z}$) is a special case of more general fractional differentiation (where given $D^n , n \in \mathbb{R}$). I'm wondering if there's some meaningful notion of ""complex differentiation"", say something like $D^i$, where fractional differentiation is a special case (note that anti differentiation is a special case of differentiation; namely, given $D^n$, anti differentiation occurs when $n$ is real and negative). Is this conceivable? If so, are there any apparent applications of this? Sorry if this is a dumb question (by dumb, I mean something that I could've found elsewhere online). I've searched around and haven't found anything on this yet.",,"['calculus', 'analysis', 'complex-numbers', 'fractional-calculus']"
28,Calculus of variation with inequality constraints,Calculus of variation with inequality constraints,,"Find the function $y$ which maximizes the functional $$J[y] = \int_0^1 g(x) y(x) dx$$ subject to $0 \leq y(x) \leq 1$ for all $x\in [0,1]$ and $$\int_0^1 y(x) dx = k$$ where $g$ is a strictly increasing function. I know that I can take care of the isoperimetric constraint quite easily using the Lagrangian $$K[y] = \int_0^1 (g(x) y(x) + \lambda y(x)) dx$$ I also know that I can take care of constraints of the form $y(x) \leq 1$ using a substitution such as $u^2(x) = 1 - y(x)\geq 0$ to get $$K[u] = \int_0^1 \left( g(x) \left( 1 - u^2(x) \right) + \lambda \left( 1 - u^2(x) \right) \right) dx$$ However, I am quite at a loss with a constraint of the form $0 \leq y(x) \leq 1$ , i.e., when two inequalities are involved at the same time. How can I take care of this?","Find the function which maximizes the functional subject to for all and where is a strictly increasing function. I know that I can take care of the isoperimetric constraint quite easily using the Lagrangian I also know that I can take care of constraints of the form using a substitution such as to get However, I am quite at a loss with a constraint of the form , i.e., when two inequalities are involved at the same time. How can I take care of this?","y J[y] = \int_0^1 g(x) y(x) dx 0 \leq y(x) \leq 1 x\in [0,1] \int_0^1 y(x) dx = k g K[y] = \int_0^1 (g(x) y(x) + \lambda y(x)) dx y(x) \leq 1 u^2(x) = 1 - y(x)\geq 0 K[u] = \int_0^1 \left( g(x) \left( 1 - u^2(x) \right) + \lambda \left( 1 - u^2(x) \right) \right) dx 0 \leq y(x) \leq 1","['calculus', 'inequality', 'calculus-of-variations', 'constraints']"
29,Prove that $\cosh^{-1}(1+x)=\sqrt{2x}(1-\frac{1}{12}x+\frac{3}{160}x^2-\frac{5}{896}x^3+....)$,Prove that,\cosh^{-1}(1+x)=\sqrt{2x}(1-\frac{1}{12}x+\frac{3}{160}x^2-\frac{5}{896}x^3+....),"How can we prove the series expansion of $$\cosh^{-1}(1+x)=\sqrt{2x}\left(1-\frac{1}{12}x+\frac{3}{160}x^2-\frac{5}{896}x^3+...\right)$$ I know the formula for $\cosh^{-1}(x)=\ln(x+\sqrt{x^2-1})$ so, $$\cosh^{-1}(1+x)=\ln(1+x+\sqrt{x^2+2x}).$$ I tried to apply Maclaurin  series but i could not find the $f(0),f'(0),f''(0),f'''(0)$ Is there any other method available to prove this series expansion like Laurent series etc. Please help me.Thanks.","How can we prove the series expansion of $$\cosh^{-1}(1+x)=\sqrt{2x}\left(1-\frac{1}{12}x+\frac{3}{160}x^2-\frac{5}{896}x^3+...\right)$$ I know the formula for $\cosh^{-1}(x)=\ln(x+\sqrt{x^2-1})$ so, $$\cosh^{-1}(1+x)=\ln(1+x+\sqrt{x^2+2x}).$$ I tried to apply Maclaurin  series but i could not find the $f(0),f'(0),f''(0),f'''(0)$ Is there any other method available to prove this series expansion like Laurent series etc. Please help me.Thanks.",,"['calculus', 'asymptotics', 'taylor-expansion', 'hyperbolic-functions']"
30,How to Prove the Chain Rule for Limits Using a $\varepsilon$-$\delta$ Argument?,How to Prove the Chain Rule for Limits Using a - Argument?,\varepsilon \delta,I came across the chain rule for limits the other day and it interested me quite a bit and surprisingly I couldn't find the proof on the internet anywhere. From what I understand the chain rule for limits states that if: $$ \lim_{x\to c} g(x)=M$$ and $$\lim_{x\to M} f(x)=L$$ then$$\lim_{x\to c} \ f(g(x))=L$$ 1.Under what conditions does this hold true? 2.What is the epsilon-delta proof for the rule?,I came across the chain rule for limits the other day and it interested me quite a bit and surprisingly I couldn't find the proof on the internet anywhere. From what I understand the chain rule for limits states that if: $$ \lim_{x\to c} g(x)=M$$ and $$\lim_{x\to M} f(x)=L$$ then$$\lim_{x\to c} \ f(g(x))=L$$ 1.Under what conditions does this hold true? 2.What is the epsilon-delta proof for the rule?,,"['calculus', 'limits', 'proof-verification', 'alternative-proof', 'epsilon-delta']"
31,Summation of the reciprocals of the product of consecutive integers,Summation of the reciprocals of the product of consecutive integers,,"It is well known that there is a closed formula for: $$\frac{1}{1 \cdot 2} + \frac{1}{2 \cdot 3} + \cdots + \frac{1}{(n)(n + 1)}$$ And likewise for: $$\frac{1}{1 \cdot 2 \cdot 3} + \frac{1}{2 \cdot 3 \cdot 4} + \cdots + \frac{1}{(n)(n + 1)(n+2)}$$ I am wondering if there is a closed formula for: $$f(n, k) = \sum_{i=1}^n \frac{1}{\prod_{j=0}^k (i + j)}$$ Note that putting $k = 1$ and $k = 2$ in the above function yields the above two series.","It is well known that there is a closed formula for: $$\frac{1}{1 \cdot 2} + \frac{1}{2 \cdot 3} + \cdots + \frac{1}{(n)(n + 1)}$$ And likewise for: $$\frac{1}{1 \cdot 2 \cdot 3} + \frac{1}{2 \cdot 3 \cdot 4} + \cdots + \frac{1}{(n)(n + 1)(n+2)}$$ I am wondering if there is a closed formula for: $$f(n, k) = \sum_{i=1}^n \frac{1}{\prod_{j=0}^k (i + j)}$$ Note that putting $k = 1$ and $k = 2$ in the above function yields the above two series.",,"['calculus', 'integration', 'summation', 'binomial-coefficients']"
32,When does convergence of subsequences implies convergence of sequence?,When does convergence of subsequences implies convergence of sequence?,,"Let $X$ be a compact metric space and $(a_n)_{n \in \mathbb{N}}$ be a sequence in $X$ and $(a_{n_i})_{i \in \mathbb{N}}$ be a convergent subsequence (we know that such exists because $X$ is compact). Denote by $l$ its limit. My question is: Suppose that, for every integer $k \geq 0$, the subsequence $(a_{n_i+k})_{i \in \mathbb{N}}$ of $(a_n)$ converges to $l$. Is it necessary that $(a_n)$ also converges to $l$? I think that this is true, but I could not prove it playing around with the $\epsilon - \delta$ definition of limit. Any help? Thank you in advance!","Let $X$ be a compact metric space and $(a_n)_{n \in \mathbb{N}}$ be a sequence in $X$ and $(a_{n_i})_{i \in \mathbb{N}}$ be a convergent subsequence (we know that such exists because $X$ is compact). Denote by $l$ its limit. My question is: Suppose that, for every integer $k \geq 0$, the subsequence $(a_{n_i+k})_{i \in \mathbb{N}}$ of $(a_n)$ converges to $l$. Is it necessary that $(a_n)$ also converges to $l$? I think that this is true, but I could not prove it playing around with the $\epsilon - \delta$ definition of limit. Any help? Thank you in advance!",,"['calculus', 'sequences-and-series', 'general-topology', 'limits']"
33,"Prove that there exists some $c\in(-3,3)$ such that$ \ \ g(c) \cdot g''(c)<0$.",Prove that there exists some  such that.,"c\in(-3,3)  \ \ g(c) \cdot g''(c)<0","$f(x)$ is a differentiable function and $g(x)$ is a double differentiable function such that $|f(x)|\leqslant 1$ and $f'(x)=g(x)$ . If $$f(0)^2+g(0)^2=9$$ then prove that there exists some $c\in(-3,3)$ such that $ \ \ g(c) \cdot g''(c)<0$ . Attempt: Let us define a function $h(x) = g(x) g~'(x)$ . Then $$h'(x) =  g(x)g''(x) + \left( g'(x) \right)^2 \tag 1$$ If we prove that for some $c \in (-3,3), h~'(c) < 0,$ then $$g(c)g''(c) <0 \tag 2$$ Also, $$\left|f(0)\right| < 1 \implies f'(0) \in (-3,-2\sqrt 2 ) \cup (2\sqrt 2,3) $$ Could someone please advise me how do I move forward from here. Thank you very much for your help in this regard.","is a differentiable function and is a double differentiable function such that and . If then prove that there exists some such that . Attempt: Let us define a function . Then If we prove that for some then Also, Could someone please advise me how do I move forward from here. Thank you very much for your help in this regard.","f(x) g(x) |f(x)|\leqslant 1 f'(x)=g(x) f(0)^2+g(0)^2=9 c\in(-3,3)  \ \ g(c) \cdot g''(c)<0 h(x) = g(x) g~'(x) h'(x) =  g(x)g''(x) + \left( g'(x) \right)^2 \tag 1 c \in (-3,3), h~'(c) < 0, g(c)g''(c) <0 \tag 2 \left|f(0)\right| < 1 \implies f'(0) \in (-3,-2\sqrt 2 ) \cup (2\sqrt 2,3) ",['calculus']
34,Feynman technique of integration for $\int^\infty_0 \exp\left(\frac{-x^2}{y^2}-y^2\right) dx$,Feynman technique of integration for,\int^\infty_0 \exp\left(\frac{-x^2}{y^2}-y^2\right) dx,"I've been learning a technique that Feynman describes in some of his books to integrate. The source can be found here: http://ocw.mit.edu/courses/mathematics/18-304-undergraduate-seminar-in-discrete-mathematics-spring-2006/projects/integratnfeynman.pdf The first few examples are integrals in $x$ and $y$ variables, and I can't see a good way to simplify them using differentiation, particularly the example: $$\int^\infty_0 \exp\left(\frac{-x^2}{y^2}-y^2\right) dx$$","I've been learning a technique that Feynman describes in some of his books to integrate. The source can be found here: http://ocw.mit.edu/courses/mathematics/18-304-undergraduate-seminar-in-discrete-mathematics-spring-2006/projects/integratnfeynman.pdf The first few examples are integrals in $x$ and $y$ variables, and I can't see a good way to simplify them using differentiation, particularly the example: $$\int^\infty_0 \exp\left(\frac{-x^2}{y^2}-y^2\right) dx$$",,"['calculus', 'integration']"
35,Why is the composition of smooth multivariable functions smooth?,Why is the composition of smooth multivariable functions smooth?,,"Is an an easy way to see that if $f: U  \subset  \mathbb R^n \rightarrow V  \subset \mathbb R^m$ and $g: V  \subset \mathbb R^m \rightarrow \mathbb R^p$ are smooth functions then their composite $g \circ f: U \rightarrow \mathbb R^p$ is smooth ? I've been looking at the definiton of the derivative (multivariable) and the chain rule. However, help is needed in order to fully see that $g \circ f$ is smooth. Looking at the one-dimensional case, I see easily that $g \circ f$ is smooth by elementary rules for differentiation.","Is an an easy way to see that if $f: U  \subset  \mathbb R^n \rightarrow V  \subset \mathbb R^m$ and $g: V  \subset \mathbb R^m \rightarrow \mathbb R^p$ are smooth functions then their composite $g \circ f: U \rightarrow \mathbb R^p$ is smooth ? I've been looking at the definiton of the derivative (multivariable) and the chain rule. However, help is needed in order to fully see that $g \circ f$ is smooth. Looking at the one-dimensional case, I see easily that $g \circ f$ is smooth by elementary rules for differentiation.",,"['calculus', 'multivariable-calculus']"
36,Evaluating $\lim\limits_{x \to 0}\left(\frac{\sin x}{x}\right)^{\frac{1}{1-\cos x}}$,Evaluating,\lim\limits_{x \to 0}\left(\frac{\sin x}{x}\right)^{\frac{1}{1-\cos x}},How do I evaluate $$\lim_{x \to 0}\left(\frac{\sin x}{x}\right)^{\dfrac{1}{1-\cos x}}\ ?$$ I tried using the fact that $\left(\frac{\sin x}{x}\right)^{\frac{1}{1-\cos x}} = \exp\left(\ln\bigg(\frac{\sin x}{x}\right)\frac{1}{1-\cos x}\bigg)$ and then I am already stuck.,How do I evaluate $$\lim_{x \to 0}\left(\frac{\sin x}{x}\right)^{\dfrac{1}{1-\cos x}}\ ?$$ I tried using the fact that $\left(\frac{\sin x}{x}\right)^{\frac{1}{1-\cos x}} = \exp\left(\ln\bigg(\frac{\sin x}{x}\right)\frac{1}{1-\cos x}\bigg)$ and then I am already stuck.,,"['calculus', 'limits']"
37,A bessel function integral,A bessel function integral,,$$\int_{-\infty}^{\infty} dy \frac{J_1 \left ( \pi\sqrt{x^2+y^2} \right )}{\sqrt{x^2+y^2}} = \frac{2 \sin{\pi x}}{\pi x} $$ How do I show this?,$$\int_{-\infty}^{\infty} dy \frac{J_1 \left ( \pi\sqrt{x^2+y^2} \right )}{\sqrt{x^2+y^2}} = \frac{2 \sin{\pi x}}{\pi x} $$ How do I show this?,,"['calculus', 'laplace-transform', 'contour-integration', 'bessel-functions']"
38,"Solve $(\alpha,\beta)$ for $\lim_{n\to\infty} \frac{\sqrt[n^2]{1!2!\cdots n!}}{n^\alpha} = \beta$",Solve  for,"(\alpha,\beta) \lim_{n\to\infty} \frac{\sqrt[n^2]{1!2!\cdots n!}}{n^\alpha} = \beta","Find the ordered pair $(\alpha,\beta)$ with non-infinite $\beta \ne 0$ such that $$\lim_{n\to\infty} \frac{\sqrt[n^2]{1!2!\cdots n!}}{n^\alpha} = \beta$$ My approach: $$\ln (1!2!\cdots n!) = (n)\ln 1 + (n-1)\ln 2 + \cdots + (2)\ln (n-1) + \ln(n) \\ \begin{align} = n\ln\left(\frac{1}{n}\right) + (n-1)\ln\left(\frac{2}{n}\right) + \cdots + \ln\left(\frac{n}{n}\right) + \frac{(n)(n+1)}{2} \ln (n)\end{align}$$ Then $$\ln(\sqrt[n^2]{1!2!\cdots n!}) = \frac{1}{n^2} \ln(1!2!\cdots n!) = \frac{n+1}{n} \cdot \ln n + \frac{1}{n} \left[\sum_{m=1}^n \left(\frac{n+1-m}{n} \cdot \ln \frac{m}{n}\right)\right]$$ And that's about as far as I got. Any ideas about proceeding with this method or perhaps even with a different method? Thanks A","Find the ordered pair $(\alpha,\beta)$ with non-infinite $\beta \ne 0$ such that $$\lim_{n\to\infty} \frac{\sqrt[n^2]{1!2!\cdots n!}}{n^\alpha} = \beta$$ My approach: $$\ln (1!2!\cdots n!) = (n)\ln 1 + (n-1)\ln 2 + \cdots + (2)\ln (n-1) + \ln(n) \\ \begin{align} = n\ln\left(\frac{1}{n}\right) + (n-1)\ln\left(\frac{2}{n}\right) + \cdots + \ln\left(\frac{n}{n}\right) + \frac{(n)(n+1)}{2} \ln (n)\end{align}$$ Then $$\ln(\sqrt[n^2]{1!2!\cdots n!}) = \frac{1}{n^2} \ln(1!2!\cdots n!) = \frac{n+1}{n} \cdot \ln n + \frac{1}{n} \left[\sum_{m=1}^n \left(\frac{n+1-m}{n} \cdot \ln \frac{m}{n}\right)\right]$$ And that's about as far as I got. Any ideas about proceeding with this method or perhaps even with a different method? Thanks A",,"['calculus', 'limits', 'summation']"
39,"Proving if an integral is positive, negative, or zero","Proving if an integral is positive, negative, or zero",,"Find $$\text{sgn}\left[\int_0^{2\pi} e^{-2014x^2}\sin(x) \, dx\right]$$ Source: TCU Calculus Bee 2014 (Held 8 months ago) By logic because the $\displaystyle\int_0^\pi \sin(x)\,dx=-\int_\pi^{2\pi} \sin(x)\,dx$ and the exponential is smaller in the interval $[\pi,2\pi]$, the area of the 2nd part is minimized, making it positive. Can anyone provide a more rigorous proof of the answer?","Find $$\text{sgn}\left[\int_0^{2\pi} e^{-2014x^2}\sin(x) \, dx\right]$$ Source: TCU Calculus Bee 2014 (Held 8 months ago) By logic because the $\displaystyle\int_0^\pi \sin(x)\,dx=-\int_\pi^{2\pi} \sin(x)\,dx$ and the exponential is smaller in the interval $[\pi,2\pi]$, the area of the 2nd part is minimized, making it positive. Can anyone provide a more rigorous proof of the answer?",,"['calculus', 'integration', 'definite-integrals']"
40,Can we teach calculus without reals?,Can we teach calculus without reals?,,"This question is related to another question, Do we really need reals? ,  and could be considered a duplicate, so I would not be surprised if it will be put on hold. But I'm especially interested in the teaching aspects of the problem so I ask it in the following form. An anecdote. Years ago, when I was a high school teacher, I used to introduce real numbers showing first that $\sqrt{2}$ is irrational and that there are infinitely many algebraic numbers of the same type. Then I used to add (obviously without any proof) that there are other numbers, such as $\pi, e, 2^{\sqrt{2}}$ (said transcendental ) that are not algebraic.  All of these new numbers have a non periodic representation and, added to the rationals, form the set of real numbers. To taste the beauty of mathematics, I  was then used to sketch the Cantor's diagonal proof, to show that the real numbers are much more numerous than the rationals and form a set called continuous . Once a student asked me if  were the transcendental numbers (as $\pi, e ...$) that make the set of reals continuous. I was a bit uncomfortable and I thought about it for a while  before I gave an answer; finally the answer was: NO, we don't really know the numbers that make the reals continuous because those numbers are not computable. The student was a bit astonished by that answer and he commented that mathematics was not such an exact knowledge as he hoped. After that day I was convinced that students have to be exposed with caution to the mysteries of real numbers. Now the question. What is the minimal extension of the rational field that we need to teach (and learn) the calculus at a beginner level? My guess is that  is enough an exponential extension $\mathbb{E} / \mathbb{A}$ of the algebraic numbers field $\mathbb{A}$ considered as a subfield of the complex numbers $\mathbb{C}$ and constructed as in Exponential extension of $\mathbb{Q}$ $. As shown in that post, such a field is countable and all its elements are obviously computable. As far as I know, we don't know if $e$ is an element of that field, but however if we add it to $\mathbb{A}$  (possibly with some other helpful transcendent numbers) the field closure is anyway countable and its exponential estension is entirely computable.","This question is related to another question, Do we really need reals? ,  and could be considered a duplicate, so I would not be surprised if it will be put on hold. But I'm especially interested in the teaching aspects of the problem so I ask it in the following form. An anecdote. Years ago, when I was a high school teacher, I used to introduce real numbers showing first that $\sqrt{2}$ is irrational and that there are infinitely many algebraic numbers of the same type. Then I used to add (obviously without any proof) that there are other numbers, such as $\pi, e, 2^{\sqrt{2}}$ (said transcendental ) that are not algebraic.  All of these new numbers have a non periodic representation and, added to the rationals, form the set of real numbers. To taste the beauty of mathematics, I  was then used to sketch the Cantor's diagonal proof, to show that the real numbers are much more numerous than the rationals and form a set called continuous . Once a student asked me if  were the transcendental numbers (as $\pi, e ...$) that make the set of reals continuous. I was a bit uncomfortable and I thought about it for a while  before I gave an answer; finally the answer was: NO, we don't really know the numbers that make the reals continuous because those numbers are not computable. The student was a bit astonished by that answer and he commented that mathematics was not such an exact knowledge as he hoped. After that day I was convinced that students have to be exposed with caution to the mysteries of real numbers. Now the question. What is the minimal extension of the rational field that we need to teach (and learn) the calculus at a beginner level? My guess is that  is enough an exponential extension $\mathbb{E} / \mathbb{A}$ of the algebraic numbers field $\mathbb{A}$ considered as a subfield of the complex numbers $\mathbb{C}$ and constructed as in Exponential extension of $\mathbb{Q}$ $. As shown in that post, such a field is countable and all its elements are obviously computable. As far as I know, we don't know if $e$ is an element of that field, but however if we add it to $\mathbb{A}$  (possibly with some other helpful transcendent numbers) the field closure is anyway countable and its exponential estension is entirely computable.",,"['calculus', 'soft-question', 'education', 'real-numbers']"
41,"Prove reduction formula for $\int \cos^n (x)\sin^m (x) \, dx$",Prove reduction formula for,"\int \cos^n (x)\sin^m (x) \, dx","$$\displaystyle\int \:\sin^n\left(x\right)\cos^m\left(x\right)\mathrm  dx=\frac{\sin^{n+1}x\cos^{m-1}x}{m+n}+\frac{m-1}{m+n}\int \:\sin^nx\cos^{m-2}x\,\mathrm dx$$ I have been trying to solve for over a week now can someone please help me.","$$\displaystyle\int \:\sin^n\left(x\right)\cos^m\left(x\right)\mathrm  dx=\frac{\sin^{n+1}x\cos^{m-1}x}{m+n}+\frac{m-1}{m+n}\int \:\sin^nx\cos^{m-2}x\,\mathrm dx$$ I have been trying to solve for over a week now can someone please help me.",,"['calculus', 'integration', 'indefinite-integrals']"
42,Find the power series of $x\ln(1-x)$.,Find the power series of .,x\ln(1-x),"So the exercise I had to do was: Find the power representation of $x\ln(1-x)$. The way to go was finding the power series representation of $\ln(1-x)$ and then multiply it with $x$. But why can't you first find the derative of $x\ln(1-x)$ which is $x/(1-x) + \ln(1-x)$ and then make power series of them both. And then find the antiderative of that. Because when they ask to find power series representation of $\ln(1-x)$ you have to first find derivative of this then convert it into series and then find antiderivative of this series. So why can't i do the same with $x\ln(1-x)$..? So why can't I apply the same method? Why can't I solve this exercise in same way, by first finding the derivative then turning it into a power series and then integrating it? I did $f(x)= x\ln (1-x)$ $f^\prime(x) = -x/(1-x) + \ln(1-x)$ I already knew $\ln(1-x)= Σ-(1/n) x^{n+1}$ (with n=0) This in power series gives $-x\sum x^n + \sum -1/n x^{n+1}$ (both n=0) This gives $\sum-x^{n+1} + \sum-(1/n) x^{n+1}$ Now I integrate both and get $\sum-(1/(n+2)) \cdot x^{n+2} + \sum-1/n \cdot 1/(n+2) x^{n+2} $","So the exercise I had to do was: Find the power representation of $x\ln(1-x)$. The way to go was finding the power series representation of $\ln(1-x)$ and then multiply it with $x$. But why can't you first find the derative of $x\ln(1-x)$ which is $x/(1-x) + \ln(1-x)$ and then make power series of them both. And then find the antiderative of that. Because when they ask to find power series representation of $\ln(1-x)$ you have to first find derivative of this then convert it into series and then find antiderivative of this series. So why can't i do the same with $x\ln(1-x)$..? So why can't I apply the same method? Why can't I solve this exercise in same way, by first finding the derivative then turning it into a power series and then integrating it? I did $f(x)= x\ln (1-x)$ $f^\prime(x) = -x/(1-x) + \ln(1-x)$ I already knew $\ln(1-x)= Σ-(1/n) x^{n+1}$ (with n=0) This in power series gives $-x\sum x^n + \sum -1/n x^{n+1}$ (both n=0) This gives $\sum-x^{n+1} + \sum-(1/n) x^{n+1}$ Now I integrate both and get $\sum-(1/(n+2)) \cdot x^{n+2} + \sum-1/n \cdot 1/(n+2) x^{n+2} $",,"['calculus', 'power-series']"
43,Spivak Ch 11 Theorem 7,Spivak Ch 11 Theorem 7,,"Can someone please explain how to supply a rigorous $\epsilon,\delta$ argument for this theorem as Spivak says ? My argument is: $f'(a)=\lim_{h\to 0} f'(\alpha_h)$ equivalent to $|f'(\alpha_h)-f'(a)|<\epsilon$ if $0<|h|<\delta$. Since $\alpha_h$ takes on every value in the interval $(a,a+h)$ and approaches $a$ as $h\to 0$, thus $\alpha_h=x=a+h$. So $|f'(x) -f'(a)|<\epsilon$ if $0<|x-a|<\delta$ which is equivalent to $\lim_{x\to a}f'(x)=f'(a)$. Can someone please explain how the correct argument would be ?","Can someone please explain how to supply a rigorous $\epsilon,\delta$ argument for this theorem as Spivak says ? My argument is: $f'(a)=\lim_{h\to 0} f'(\alpha_h)$ equivalent to $|f'(\alpha_h)-f'(a)|<\epsilon$ if $0<|h|<\delta$. Since $\alpha_h$ takes on every value in the interval $(a,a+h)$ and approaches $a$ as $h\to 0$, thus $\alpha_h=x=a+h$. So $|f'(x) -f'(a)|<\epsilon$ if $0<|x-a|<\delta$ which is equivalent to $\lim_{x\to a}f'(x)=f'(a)$. Can someone please explain how the correct argument would be ?",,"['calculus', 'limits', 'derivatives', 'proof-writing', 'epsilon-delta']"
44,Prove $\lim_{x\to0}\cos(\frac{1}{x})$ does not exist,Prove  does not exist,\lim_{x\to0}\cos(\frac{1}{x}),"Prove $\lim_{x\to0}\cos\left(\frac{1}{x}\right)$ does not exist using $\epsilon$-$\delta$ proof. I think my professor wants this to be done by letting $L$ be arbitrary. Then have two cases: $L > 0$ and $L < 1$. Then I would pick an epsilon, for all delta, and pick an $x$. Any help would be appreciated.","Prove $\lim_{x\to0}\cos\left(\frac{1}{x}\right)$ does not exist using $\epsilon$-$\delta$ proof. I think my professor wants this to be done by letting $L$ be arbitrary. Then have two cases: $L > 0$ and $L < 1$. Then I would pick an epsilon, for all delta, and pick an $x$. Any help would be appreciated.",,"['calculus', 'limits']"
45,help me understand derivatives and their purpose,help me understand derivatives and their purpose,,"I am only starting learning calculus and it's difficult for me to understand the main concept behind calculus ideas particularly differentiation I have searched many resources but most of them are very similar explaining things with words like ""speed"", ""rate of change"", ""tangent"" and ""function change in respect to input change""... I know the rules of computation but  the purpose is not very clear for me I would be very grateful if somebody could help me grasp this concept and explain why one would want to compute the ""rate of change"" of a function and what exactly problem do derivatives solve. Pretend that I am very stupid (unfortunately I am :) ) and don't use any abstract concepts (even if they are  intuitive to a human being )  as ""speed"" if possible Thanks","I am only starting learning calculus and it's difficult for me to understand the main concept behind calculus ideas particularly differentiation I have searched many resources but most of them are very similar explaining things with words like ""speed"", ""rate of change"", ""tangent"" and ""function change in respect to input change""... I know the rules of computation but  the purpose is not very clear for me I would be very grateful if somebody could help me grasp this concept and explain why one would want to compute the ""rate of change"" of a function and what exactly problem do derivatives solve. Pretend that I am very stupid (unfortunately I am :) ) and don't use any abstract concepts (even if they are  intuitive to a human being )  as ""speed"" if possible Thanks",,"['calculus', 'derivatives']"
46,Difficult Derivative?,Difficult Derivative?,,"I'm in a single-variable calculus course, in which we recently covered logarithmic differentiation.  The professor proved it that works when $f(x)>0$, and when $f(x)<0$.  I've been trying to find a way to derive that kind of function when $f(x)=0$, but I'm not sure if it's possible, or what.  I've thought of this example, that resists all my efforts to differentiate, but seems to be differentiable, (and even appears to have a value of zero). Find $$f'\left(\frac{3\pi}{2}\right)\quad \rm where \quad f(x)=(\sin{x} + 1)^x .$$ Is there any way I can find this derivative (if it exists), beyond numerically computing the limit from the definition of the derivative? Or, vice versa, how can I prove that this derivative doesn't exist? Thanks,   Reggie","I'm in a single-variable calculus course, in which we recently covered logarithmic differentiation.  The professor proved it that works when $f(x)>0$, and when $f(x)<0$.  I've been trying to find a way to derive that kind of function when $f(x)=0$, but I'm not sure if it's possible, or what.  I've thought of this example, that resists all my efforts to differentiate, but seems to be differentiable, (and even appears to have a value of zero). Find $$f'\left(\frac{3\pi}{2}\right)\quad \rm where \quad f(x)=(\sin{x} + 1)^x .$$ Is there any way I can find this derivative (if it exists), beyond numerically computing the limit from the definition of the derivative? Or, vice versa, how can I prove that this derivative doesn't exist? Thanks,   Reggie",,"['calculus', 'derivatives']"
47,How does one show that a function is continuous over some interval?,How does one show that a function is continuous over some interval?,,"As title says, how does one show that a function is continuous over some interval (let us say over some interval of real numbers?) Would(Can) this involve derivative?","As title says, how does one show that a function is continuous over some interval (let us say over some interval of real numbers?) Would(Can) this involve derivative?",,"['calculus', 'functions']"
48,Arc length of logarithm function,Arc length of logarithm function,,"I need to find the length of $y = \ln(x)$ (natural logarithm) from $x=\sqrt3$ to $x=\sqrt8$. So, if I am not mistake, the length should be $$\int^\sqrt8_\sqrt3\sqrt{1+\frac{1}{x^2}}dx$$ I am having trouble calculating the integral. I tried to do substitution, but I still fail to think of a way to integrate it. This is what I have done so far: $$\sqrt{1+\frac{1}{x^2}}=u-\frac{1}{x}$$ $$x=\frac{2u}{u-1}$$ $$dx=\frac{2}{(u-1)^2}du$$ $$\sqrt{1+\frac{1}{x^2}}=u-\frac{1}{x}=u-\frac{1}{2}+\frac{1}{2u}$$ $$\int\sqrt{1+\frac{1}{x^2}}dx=2\int\frac{u-\frac{1}{2}+\frac{1}{2u}}{(u-1)^2}du$$ And I am stuck.","I need to find the length of $y = \ln(x)$ (natural logarithm) from $x=\sqrt3$ to $x=\sqrt8$. So, if I am not mistake, the length should be $$\int^\sqrt8_\sqrt3\sqrt{1+\frac{1}{x^2}}dx$$ I am having trouble calculating the integral. I tried to do substitution, but I still fail to think of a way to integrate it. This is what I have done so far: $$\sqrt{1+\frac{1}{x^2}}=u-\frac{1}{x}$$ $$x=\frac{2u}{u-1}$$ $$dx=\frac{2}{(u-1)^2}du$$ $$\sqrt{1+\frac{1}{x^2}}=u-\frac{1}{x}=u-\frac{1}{2}+\frac{1}{2u}$$ $$\int\sqrt{1+\frac{1}{x^2}}dx=2\int\frac{u-\frac{1}{2}+\frac{1}{2u}}{(u-1)^2}du$$ And I am stuck.",,"['calculus', 'integration', 'definite-integrals']"
49,Proof by induction using Fubini's Theorem,Proof by induction using Fubini's Theorem,,"I am asked for the volume of the region $x_1+\cdots+x_n\leq 1$ where $x_1,...,x_n\geq 0$. I am proposing that the volume $V(n)$, is given by $$ V(n) = \int\limits_0^1\int\limits_0^{(1-x_1)}\cdots\int\limits_0^{(1-\cdots-x_{n-1})} \,dx_n\cdots\,dx_2\,dx_1 = \frac{1}{n!} \ . $$ I am trying to prove the formula by induction. The base case is easy, but I am having a problem showing that if $n=k$ holds, then $n=k+1$ holds. I cannot figure out how to apply the inductive hypothesis. Am I missing something obvious or is there an easier method?","I am asked for the volume of the region $x_1+\cdots+x_n\leq 1$ where $x_1,...,x_n\geq 0$. I am proposing that the volume $V(n)$, is given by $$ V(n) = \int\limits_0^1\int\limits_0^{(1-x_1)}\cdots\int\limits_0^{(1-\cdots-x_{n-1})} \,dx_n\cdots\,dx_2\,dx_1 = \frac{1}{n!} \ . $$ I am trying to prove the formula by induction. The base case is easy, but I am having a problem showing that if $n=k$ holds, then $n=k+1$ holds. I cannot figure out how to apply the inductive hypothesis. Am I missing something obvious or is there an easier method?",,['calculus']
50,Trigonometrical limit $\lim\limits_{ x\to 0 } \frac{\sin x - x\cos x}{x^3}?$,Trigonometrical limit,\lim\limits_{ x\to 0 } \frac{\sin x - x\cos x}{x^3}?,Can you help me solve this without using de l'Hôpital's rule (just using Standard rules): $$ \lim_{ x\to 0 } \frac{\sin x - x\cos x}{x^3}? $$,Can you help me solve this without using de l'Hôpital's rule (just using Standard rules): $$ \lim_{ x\to 0 } \frac{\sin x - x\cos x}{x^3}? $$,,"['calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
51,Calculate an integral involving Hermite polynomials,Calculate an integral involving Hermite polynomials,,"I have to calculate the integral $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}\int_{-\infty}^{+\infty}H_n(x)e^{-x^2+kx}H_l(x)\;\mathrm{d}x$$ where $H_n(x)$ is the $n^{th}$ Hermite polynomial and prove that it equals $$\sqrt{\frac{m_<!}{m_>!}}\left(\frac{k}{\sqrt{2}}\right)^{|n-l|}L_{m_<}^{|n-l|}\left(-\frac{k^2}{2}\right)\exp\left(\frac{k^2}{4}\right)$$ where $m_<$ and $m_>$ denote the smaller and the larger respectively of the two indices $n$ and $l$ and where $L_n^m$ are the associated Laguerre polynomials. The last term is $\exp(k^2/4)$, hence I suppose that I begin with $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}\int_{-\infty}^{+\infty}H_n(x)e^{-x^2+kx-\frac{k^2}{4}}e^{\frac{k^2}{4}}H_l(x)\;\mathrm{d}x$$ $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}e^{\frac{k^2}{4}}\int_{-\infty}^{+\infty}H_n(x)e^{-(x-\frac{k}{2})^2}H_l(x)\;\mathrm{d}x$$ but here I'm stuck... Thanks for your help!","I have to calculate the integral $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}\int_{-\infty}^{+\infty}H_n(x)e^{-x^2+kx}H_l(x)\;\mathrm{d}x$$ where $H_n(x)$ is the $n^{th}$ Hermite polynomial and prove that it equals $$\sqrt{\frac{m_<!}{m_>!}}\left(\frac{k}{\sqrt{2}}\right)^{|n-l|}L_{m_<}^{|n-l|}\left(-\frac{k^2}{2}\right)\exp\left(\frac{k^2}{4}\right)$$ where $m_<$ and $m_>$ denote the smaller and the larger respectively of the two indices $n$ and $l$ and where $L_n^m$ are the associated Laguerre polynomials. The last term is $\exp(k^2/4)$, hence I suppose that I begin with $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}\int_{-\infty}^{+\infty}H_n(x)e^{-x^2+kx-\frac{k^2}{4}}e^{\frac{k^2}{4}}H_l(x)\;\mathrm{d}x$$ $$\frac{1}{\sqrt{2^nn!}\sqrt{2^ll!}}\frac{1}{\sqrt{\pi}}e^{\frac{k^2}{4}}\int_{-\infty}^{+\infty}H_n(x)e^{-(x-\frac{k}{2})^2}H_l(x)\;\mathrm{d}x$$ but here I'm stuck... Thanks for your help!",,"['calculus', 'integration', 'special-functions', 'orthogonal-polynomials']"
52,How do I prove that $\exp(\frac{h}{1+h})\leq 1+h$?,How do I prove that ?,\exp(\frac{h}{1+h})\leq 1+h,"I have come across the inequality $$\exp\left(\frac{h}{1+h}\right)\leq 1+h,\quad\forall h>-1,$$ on http://functions.wolfram.com/ElementaryFunctions/Exp/29/ . I would like some help proving this. A straightforward expansion of the exponential doesn't seem to yield anything.","I have come across the inequality $$\exp\left(\frac{h}{1+h}\right)\leq 1+h,\quad\forall h>-1,$$ on http://functions.wolfram.com/ElementaryFunctions/Exp/29/ . I would like some help proving this. A straightforward expansion of the exponential doesn't seem to yield anything.",,"['calculus', 'algebra-precalculus', 'inequality', 'exponential-function']"
53,How to guess the number of inflection points?,How to guess the number of inflection points?,,"I am asked to find fast the number of possible inflection points of: $$y=(x-1)(x-2)^2(x-3)^4(x-4)^3$$ I know if the degree of any polynomial is even, its plot starts from the 2th quadrant to 1st quadrant of $\mathbb R^2$. This was what I could do fast . Any Ideas? Thanks.","I am asked to find fast the number of possible inflection points of: $$y=(x-1)(x-2)^2(x-3)^4(x-4)^3$$ I know if the degree of any polynomial is even, its plot starts from the 2th quadrant to 1st quadrant of $\mathbb R^2$. This was what I could do fast . Any Ideas? Thanks.",,['calculus']
54,Big Oh notation Question in calculus,Big Oh notation Question in calculus,,"In my text book, they state the following: $$\begin{align*}f(x) &= (\frac{1}{x} + \frac{1}{2}) (x-\frac{1}{2}x^2+\frac{1}{3}x^3+O(x^4))-1& ,x \rightarrow 0\\&= 1-\frac{1}{2}x+\frac{1}{3}x^2+\frac{1}{2}x-\frac{1}{4}x^3+O(x^3)-1& ,x \rightarrow 0 \end{align*}$$ However, when I calculate this, I get $1-\frac{1}{2}x+\frac{1}{3}x^2+\frac{1}{2}x-\frac{1}{4}x^3+O(x^3)+\frac{O(x^4)}{2}-1$. That $O(x^4)$ part disappears I guess, due to the big O notation. However, I cannot figure out why. Furthermore, a few pages later, they say that $\lim_{x\rightarrow 0} O(x) = 0$. Which I do not really understand, since $O(x)$ defines a set of functions, no?","In my text book, they state the following: $$\begin{align*}f(x) &= (\frac{1}{x} + \frac{1}{2}) (x-\frac{1}{2}x^2+\frac{1}{3}x^3+O(x^4))-1& ,x \rightarrow 0\\&= 1-\frac{1}{2}x+\frac{1}{3}x^2+\frac{1}{2}x-\frac{1}{4}x^3+O(x^3)-1& ,x \rightarrow 0 \end{align*}$$ However, when I calculate this, I get $1-\frac{1}{2}x+\frac{1}{3}x^2+\frac{1}{2}x-\frac{1}{4}x^3+O(x^3)+\frac{O(x^4)}{2}-1$. That $O(x^4)$ part disappears I guess, due to the big O notation. However, I cannot figure out why. Furthermore, a few pages later, they say that $\lim_{x\rightarrow 0} O(x) = 0$. Which I do not really understand, since $O(x)$ defines a set of functions, no?",,"['calculus', 'notation', 'asymptotics']"
55,Prove $\lim\limits_{n\to\infty}na_n\ln(n)=0$ [duplicate],Prove  [duplicate],\lim\limits_{n\to\infty}na_n\ln(n)=0,"This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 6 years ago . Let $a_n>0$ , $\sum a_n$ is convergent, $na_n$ is monotone, Prove: $$\lim\limits_{n\to\infty}na_n\ln(n)=0.$$ I try to prove $a_n=o(n\ln(n))$ , but it doesn't work.","This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 6 years ago . Let , is convergent, is monotone, Prove: I try to prove , but it doesn't work.",a_n>0 \sum a_n na_n \lim\limits_{n\to\infty}na_n\ln(n)=0. a_n=o(n\ln(n)),"['calculus', 'sequences-and-series', 'limits']"
56,"Is the ""limit function"" a continuous function?","Is the ""limit function"" a continuous function?",,Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that for all $x_0\in\mathbb{R}$ we have $\lim\limits_{x\to x_0}f(x)=g(x_0)\in \mathbb{R}$. Is $g$ a continuous function?,Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that for all $x_0\in\mathbb{R}$ we have $\lim\limits_{x\to x_0}f(x)=g(x_0)\in \mathbb{R}$. Is $g$ a continuous function?,,"['calculus', 'functions']"
57,How to prove $\limsup (x_{n}+y_{n})=\lim x_{n}+\limsup y_{n}$?,How to prove ?,\limsup (x_{n}+y_{n})=\lim x_{n}+\limsup y_{n},"Given a convergent sequence $x_{n}$ and bounded sequence $y_{n}$  I need to prove that $\limsup (x_{n}+y_{n})=\lim x_{n}+\limsup y_{n}$, when $n$ tends to $\infty$. I chose $z_{n}=x_{n}+y_{n}$, we know that $z_{n}$ is bounded as being sum of two bounded sequences, so from the Bolzano-Weierstrass Theorem, we know that there is a  subsequence of $z_{n}$, let's call it $z_{n_{k}}$, that converges to $\limsup (x_{n}+y_{n})$. $y_{n_{k}}$ is bounded as well, so there is a convergent subsequence $y_{n_{k_{j}}}$. All this gives me that  $\limsup (x_{n}+y_{n})\leq \lim x_{n}+\limsup y_{n}$, What can I do for getting the other inequality? Thank you and Good evening.","Given a convergent sequence $x_{n}$ and bounded sequence $y_{n}$  I need to prove that $\limsup (x_{n}+y_{n})=\lim x_{n}+\limsup y_{n}$, when $n$ tends to $\infty$. I chose $z_{n}=x_{n}+y_{n}$, we know that $z_{n}$ is bounded as being sum of two bounded sequences, so from the Bolzano-Weierstrass Theorem, we know that there is a  subsequence of $z_{n}$, let's call it $z_{n_{k}}$, that converges to $\limsup (x_{n}+y_{n})$. $y_{n_{k}}$ is bounded as well, so there is a convergent subsequence $y_{n_{k_{j}}}$. All this gives me that  $\limsup (x_{n}+y_{n})\leq \lim x_{n}+\limsup y_{n}$, What can I do for getting the other inequality? Thank you and Good evening.",,['calculus']
58,Struggling to figure out why $\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4}$.,Struggling to figure out why .,\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4},"Struggling to figure out why $$\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4}$$ I've taken note of the exponent in the logarithm, tried substituting $x=\tan\theta$ , and even differentiating under the integral sign with the function $J(a)=\int\limits_{0}^{1}\frac{\ln\left(ax\right)}{\left(1+x^{2}\right)^{2}}$ , but all to no avail. The trigonometric substitution leads me to $$\int\limits_{0}^{\frac{\pi}{4}}\ln\left(\tan^{2}\theta\right)\cos^{2}\theta d\theta$$ which is verifiably equivalent to the original expression, but it also looks ugly and I don't know where to go from here (Surely not integration by parts, since then it either gets very ugly or births a divergent integral). I haven't tried complex analysis but since the bounds are 0 to 1 I doubt that would work. I know that $-G=\int\limits_{0}^{1}\frac{\ln x}{1+x^{2}}dx$ but I can't tell how to transform my expressions into this one. There must be some sneaky but potent substitution that I'm missing. Judging by the Catalan's constant in the solution, I assume there's also some series, presumably for the logarithm, that could be used. Could anyone share some insights, please? This is my first time using this forum, apologies in advance if I did anything wrong.","Struggling to figure out why I've taken note of the exponent in the logarithm, tried substituting , and even differentiating under the integral sign with the function , but all to no avail. The trigonometric substitution leads me to which is verifiably equivalent to the original expression, but it also looks ugly and I don't know where to go from here (Surely not integration by parts, since then it either gets very ugly or births a divergent integral). I haven't tried complex analysis but since the bounds are 0 to 1 I doubt that would work. I know that but I can't tell how to transform my expressions into this one. There must be some sneaky but potent substitution that I'm missing. Judging by the Catalan's constant in the solution, I assume there's also some series, presumably for the logarithm, that could be used. Could anyone share some insights, please? This is my first time using this forum, apologies in advance if I did anything wrong.",\int\limits_{0}^{1}\frac{\ln \left(x^{2}\right)}{\left(1+x^{2}\right)^{2}}dx = -G-\frac{\pi}{4} x=\tan\theta J(a)=\int\limits_{0}^{1}\frac{\ln\left(ax\right)}{\left(1+x^{2}\right)^{2}} \int\limits_{0}^{\frac{\pi}{4}}\ln\left(\tan^{2}\theta\right)\cos^{2}\theta d\theta -G=\int\limits_{0}^{1}\frac{\ln x}{1+x^{2}}dx,"['calculus', 'integration', 'catalans-constant']"
59,Getting two different answers on differentiating $\cos^{-1}(\frac{3x+4\sqrt{1-x^2}}{5})$,Getting two different answers on differentiating,\cos^{-1}(\frac{3x+4\sqrt{1-x^2}}{5}),"Question given in my book asks to find $\frac{dy}{dx} $ from the following equation. $$y=\cos^{-1}\left(\frac{3x+4\sqrt{1-x^2}}{5}\right)$$ My Attempt: Starting with substitutions, Putting $\frac35=\cos\alpha\implies \frac45 = \sin\alpha$ . Putting $x = \cos\beta\implies \sqrt{1-x^2} = \sin\beta$ . $$\begin{align}&y=\cos^{-1}\Big(\frac{3x+4\sqrt{1-x^2}}{5}\Big)\\ \implies& y = \cos^{-1}\Big(\frac{3}{5}x + \frac{4}{5}\sqrt{1-x^2}\Big)\\\implies& y = \cos^{-1}(\cos\alpha\cos\beta + \sin\alpha \sin \beta)\\\implies& y  = \cos^{-1}[\cos(\alpha-\beta )] \tag{1}\\\implies& y = \alpha-\beta\\\implies& y = \cos^{-1}\left(\frac35\right) - \cos^{-1}(x)\\\implies&\color{blue}{\boxed{ \dfrac{dy}{dx}  =\frac{1}{\sqrt{1-x^2}}}}.\end{align}$$ But, if I consider $(1.)$ again, $$y = \cos^{-1}\left[\cos\color{red}{(\alpha-\beta)}\right]$$ This is also equals to, $$\cos^{-1}\left[\cos\color{red}{(\beta - \alpha)}\right].$$ differentiating this, will give the negative of the answer which I got earlier. My book shows that $\frac{-1}{\sqrt{1-x^2}}$ is correct. But why? I think the mistakes lie in the very first step of substitution i.e., $\frac35=\cos\alpha$ doesn't imply $\frac45 = \sin\alpha$ . It should be $\sin\alpha = \pm \frac45$ . Similary $x = \cos\beta$ doesn't imply $\sqrt{1-x^2} = \sin\beta$ instead $\sin\beta = \pm \sqrt{1-x^2}$ . But how can I make sure that in the equation $(1.)$ , $(\alpha - \beta)$ lies in the principal branch of the inverse cosine function?","Question given in my book asks to find from the following equation. My Attempt: Starting with substitutions, Putting . Putting . But, if I consider again, This is also equals to, differentiating this, will give the negative of the answer which I got earlier. My book shows that is correct. But why? I think the mistakes lie in the very first step of substitution i.e., doesn't imply . It should be . Similary doesn't imply instead . But how can I make sure that in the equation , lies in the principal branch of the inverse cosine function?",\frac{dy}{dx}  y=\cos^{-1}\left(\frac{3x+4\sqrt{1-x^2}}{5}\right) \frac35=\cos\alpha\implies \frac45 = \sin\alpha x = \cos\beta\implies \sqrt{1-x^2} = \sin\beta \begin{align}&y=\cos^{-1}\Big(\frac{3x+4\sqrt{1-x^2}}{5}\Big)\\ \implies& y = \cos^{-1}\Big(\frac{3}{5}x + \frac{4}{5}\sqrt{1-x^2}\Big)\\\implies& y = \cos^{-1}(\cos\alpha\cos\beta + \sin\alpha \sin \beta)\\\implies& y  = \cos^{-1}[\cos(\alpha-\beta )] \tag{1}\\\implies& y = \alpha-\beta\\\implies& y = \cos^{-1}\left(\frac35\right) - \cos^{-1}(x)\\\implies&\color{blue}{\boxed{ \dfrac{dy}{dx}  =\frac{1}{\sqrt{1-x^2}}}}.\end{align} (1.) y = \cos^{-1}\left[\cos\color{red}{(\alpha-\beta)}\right] \cos^{-1}\left[\cos\color{red}{(\beta - \alpha)}\right]. \frac{-1}{\sqrt{1-x^2}} \frac35=\cos\alpha \frac45 = \sin\alpha \sin\alpha = \pm \frac45 x = \cos\beta \sqrt{1-x^2} = \sin\beta \sin\beta = \pm \sqrt{1-x^2} (1.) (\alpha - \beta),['calculus']
60,Calculate the closed form of the following series,Calculate the closed form of the following series,,"$$\sum_{m=r}^{\infty}\binom{m-1}{r-1}\frac{1}{4^m}$$ The answer given is $$\frac{1}{3^r}$$ I tried expanding the expression so it becomes $$\sum_{m=r}^{\infty}\frac{(m-1)!}{(r-1)!(m-r)!}\frac{1}{4^m}$$ but I do not know how to follow. Any help will be appreciated, thanks.","The answer given is I tried expanding the expression so it becomes but I do not know how to follow. Any help will be appreciated, thanks.",\sum_{m=r}^{\infty}\binom{m-1}{r-1}\frac{1}{4^m} \frac{1}{3^r} \sum_{m=r}^{\infty}\frac{(m-1)!}{(r-1)!(m-r)!}\frac{1}{4^m},"['calculus', 'summation']"
61,Finding the anti-derivative of $ \frac{e^{-c y^2 }}{y\sqrt{y^2-1}}$,Finding the anti-derivative of, \frac{e^{-c y^2 }}{y\sqrt{y^2-1}},"I am trying to evaluate the integral \begin{align} \frac{1}{2\sqrt{2}\pi}\int_{0^{-}}^{t} ds \ \frac{e^{-x^2/2S^2(t,s) }}{\Sigma(s) S(t,s)} \end{align} where $S(t, s) = 2D(t-s)+\frac{\Sigma(s)}{2}$ and $\Sigma(s)= \sigma^2+2Ds$ . I found a rather neat change of variable by taking $\xi=S^{-1}(t,s)$ so that \begin{align} \Sigma(s)&=\sqrt{2} \ \xi^{-1}\sqrt{\xi^2\Sigma^2(t)-1}\\ 2D(t-s)&=2 \ \xi^{-2}-\Sigma^2(t)\\ ds&=\frac{2}{D\xi^3}\ d\xi \end{align} Applying this to the integral gives the result \begin{align} \frac{1}{2 \pi D}\int_{\sqrt{2}/\Sigma (2 t)}^{\sqrt{2}/\Sigma (t)} d{\xi} \ \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}=\frac{1}{2 \pi D}{\int_{\xi_L}^{\xi_H} d{\xi} \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}}. \end{align} and at last applying the change of variable $y=\Sigma(t)\xi$ gives \begin{align} \frac{1}{2 \pi D}{\int_{\xi_L}^{\xi_H} d{\xi} \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}} = \frac{1}{2 \pi D}{\int_{\sqrt{2}\Sigma(t)/\Sigma(2t)}^{\sqrt{2}} d{y} \frac{e^{-(x^2/2\Sigma^2(t)) y^2 }}{y\sqrt{y^2-1}}} =\frac{1}{2 \pi D}\color{blue}{\int_{a}^{b} d{y} \frac{e^{-c y^2 }}{y\sqrt{y^2-1}}} \end{align} Above calculation indeed check out numerically. It seems the integral in blue could have an anti-derivative. However, no luck just yet!  Someone who knows a way forward? Many thanks in advance!","I am trying to evaluate the integral where and . I found a rather neat change of variable by taking so that Applying this to the integral gives the result and at last applying the change of variable gives Above calculation indeed check out numerically. It seems the integral in blue could have an anti-derivative. However, no luck just yet!  Someone who knows a way forward? Many thanks in advance!","\begin{align}
\frac{1}{2\sqrt{2}\pi}\int_{0^{-}}^{t} ds \ \frac{e^{-x^2/2S^2(t,s) }}{\Sigma(s) S(t,s)}
\end{align} S(t, s) = 2D(t-s)+\frac{\Sigma(s)}{2} \Sigma(s)= \sigma^2+2Ds \xi=S^{-1}(t,s) \begin{align}
\Sigma(s)&=\sqrt{2} \ \xi^{-1}\sqrt{\xi^2\Sigma^2(t)-1}\\
2D(t-s)&=2 \ \xi^{-2}-\Sigma^2(t)\\
ds&=\frac{2}{D\xi^3}\ d\xi
\end{align} \begin{align}
\frac{1}{2 \pi D}\int_{\sqrt{2}/\Sigma (2 t)}^{\sqrt{2}/\Sigma (t)} d{\xi} \ \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}=\frac{1}{2 \pi D}{\int_{\xi_L}^{\xi_H} d{\xi} \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}}.
\end{align} y=\Sigma(t)\xi \begin{align}
\frac{1}{2 \pi D}{\int_{\xi_L}^{\xi_H} d{\xi} \frac{e^{-x^2 \ \xi^2/2 }}{\xi\sqrt{\Sigma^2(t)\xi^2-1}}} = \frac{1}{2 \pi D}{\int_{\sqrt{2}\Sigma(t)/\Sigma(2t)}^{\sqrt{2}} d{y} \frac{e^{-(x^2/2\Sigma^2(t)) y^2 }}{y\sqrt{y^2-1}}} =\frac{1}{2 \pi D}\color{blue}{\int_{a}^{b} d{y} \frac{e^{-c y^2 }}{y\sqrt{y^2-1}}}
\end{align}","['calculus', 'integration', 'definite-integrals', 'gaussian-integral']"
62,"$f(x),g(x)$,2 quadratic polynomials:$|f(x)|≥|g(x)|∀x ∈ R$. Find the number of distinct roots of equation $h(x)h''(x)+(h'(x))^2=0$ if $h(x)=f(x)g(x)$",",2 quadratic polynomials:. Find the number of distinct roots of equation  if","f(x),g(x) |f(x)|≥|g(x)|∀x ∈ R h(x)h''(x)+(h'(x))^2=0 h(x)=f(x)g(x)","Question: If $f(x)$ and $g(x)$ are two distinct quadratic polynomials and $|f(x)|≥|g(x)|$ $∀$ $x ∈ R$ . Also $f(x)=0$ has real roots. Find the number of distinct roots of equation $$h(x)h''(x)+(h'(x))^2=0$$ where $h(x)=f(x)g(x)$ What I tried: I attempted to find $h(x)h''(x)+(h'(x))^2=0$ in terms of $f(x)$ and $g(x)$ using $h(x)=f(x)g(x)$ , upon which I got the following equation, $$g(x)^2[f(x)f''(x)+(f'(x))^2]+f(x)^2[g(x)g''(x)+g'(x)^2]+4f(x)f'(x)g(x)g'(x)=0$$ I don't know how to proceed, or where to use the fact that $|f(x)|≥|g(x)|$ $∀$ $x ∈ R$ and that $f(x)=0$ has real roots. I also tried actually using general equations for the quadratic polynomials $f(x)$ and $g(x)$ $$f(x)=a_1x^2+b_1x+c_1$$ $$g(x)=a_2x^2+b_2x+c_2$$ I then tried deducing some information from $|f(x)|≥|g(x)|$ , coming to the conclusion that $|a_1|<|a_2|$ , and that $$|\frac{b_1^2}{4a_1}-c_1|>|\frac{b_2^2}{4a_2}-c_2|$$ I found the expressions for $f'(x)$ , $f''(x)$ , $g'(x)$ and $g''(x)$ and plugged them into the equation I had obtained. This lead to a rather complicated degree 6 equation, as one would expect. I've no idea what to do next. Any help or hints are appreciated... Thanks in advance! Regards","Question: If and are two distinct quadratic polynomials and . Also has real roots. Find the number of distinct roots of equation where What I tried: I attempted to find in terms of and using , upon which I got the following equation, I don't know how to proceed, or where to use the fact that and that has real roots. I also tried actually using general equations for the quadratic polynomials and I then tried deducing some information from , coming to the conclusion that , and that I found the expressions for , , and and plugged them into the equation I had obtained. This lead to a rather complicated degree 6 equation, as one would expect. I've no idea what to do next. Any help or hints are appreciated... Thanks in advance! Regards",f(x) g(x) |f(x)|≥|g(x)| ∀ x ∈ R f(x)=0 h(x)h''(x)+(h'(x))^2=0 h(x)=f(x)g(x) h(x)h''(x)+(h'(x))^2=0 f(x) g(x) h(x)=f(x)g(x) g(x)^2[f(x)f''(x)+(f'(x))^2]+f(x)^2[g(x)g''(x)+g'(x)^2]+4f(x)f'(x)g(x)g'(x)=0 |f(x)|≥|g(x)| ∀ x ∈ R f(x)=0 f(x) g(x) f(x)=a_1x^2+b_1x+c_1 g(x)=a_2x^2+b_2x+c_2 |f(x)|≥|g(x)| |a_1|<|a_2| |\frac{b_1^2}{4a_1}-c_1|>|\frac{b_2^2}{4a_2}-c_2| f'(x) f''(x) g'(x) g''(x),"['calculus', 'polynomials', 'roots', 'quadratics']"
63,Compute $\sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3}$,Compute,\sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3},"How to prove that $$S=\displaystyle \sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3} \quad=\quad \frac{\pi^2G}{4}-\frac{21\zeta(3)\ln(2)}{8}+\frac{\pi^4}{64}+\frac{\Psi^{(3)}(\frac{1}{4})}{512}- \frac{\Psi^{(3)}(\frac{3}{4})} {512}$$ This problem was proposed by @Ahmad Bow but unfortunately it was closed as off-topic and you can find it here . Any way, I tried hard on this one but no success yet. here is what I did: Using the identity $$H_{n/2}=H_n-n\int_0^1 x^{n-1}\ln(1+x)\ dx, \quad x\mapsto x^2$$ $$H_{n/2}=H_n-2n\int_0^1 x^{2n-1}\ln(1+x^2)\ dx$$ We can write $$S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\int_0^1\frac{\ln(1+x^2)}{x}\sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}\ dx$$ where \begin{align} \sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}&=\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^2}-\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^3}\\ &=\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^2}(1+(-1)^n-\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^3}(1+(-1)^n\\ &=\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^2}(1-(-1)^n-\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^3}(1-(-1)^n\\ &=\frac1{2x}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right) \end{align} Therefore $$S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\frac12\int_0^1\frac{\ln(1+x^2)}{x^2}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right)\ dx$$ The sum can be done using the following identity $$ \sum_{n=1}^{\infty} \frac{H_{n}}{ (n+a)^{2}}= \left(\gamma + \psi(a) \right) \psi_{1}(a) - \frac{\psi_{2}(a)}{2} \, , \quad a >0.$$ Differentiate both sides with respect to $a$ then set $a=1/2$ we get $$\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}=\frac{45}{32}\zeta(4)-\frac74\ln2\zeta(3)$$ and the question here is how to calculate the the remaining integral or a different way to tackle the sum $S$ ? Thanks","How to prove that This problem was proposed by @Ahmad Bow but unfortunately it was closed as off-topic and you can find it here . Any way, I tried hard on this one but no success yet. here is what I did: Using the identity We can write where Therefore The sum can be done using the following identity Differentiate both sides with respect to then set we get and the question here is how to calculate the the remaining integral or a different way to tackle the sum ? Thanks","S=\displaystyle \sum_{n=1}^{\infty} \frac{ H_{n/2}}{(2n+1)^3} \quad=\quad \frac{\pi^2G}{4}-\frac{21\zeta(3)\ln(2)}{8}+\frac{\pi^4}{64}+\frac{\Psi^{(3)}(\frac{1}{4})}{512}- \frac{\Psi^{(3)}(\frac{3}{4})}
{512} H_{n/2}=H_n-n\int_0^1 x^{n-1}\ln(1+x)\ dx, \quad x\mapsto x^2 H_{n/2}=H_n-2n\int_0^1 x^{2n-1}\ln(1+x^2)\ dx S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\int_0^1\frac{\ln(1+x^2)}{x}\sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}\ dx \begin{align}
\sum_{n=0}^\infty \frac{2nx^{2n}}{(2n+1)^3}&=\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^2}-\frac1x\sum_{n=0}^\infty \frac{x^{2n+1}}{(2n+1)^3}\\
&=\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^2}(1+(-1)^n-\frac1{2x}\sum_{n=0}^\infty \frac{x^{n+1}}{(n+1)^3}(1+(-1)^n\\
&=\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^2}(1-(-1)^n-\frac1{2x}\sum_{n=1}^\infty \frac{x^{n}}{n^3}(1-(-1)^n\\
&=\frac1{2x}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right)
\end{align} S=\sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}-\frac12\int_0^1\frac{\ln(1+x^2)}{x^2}\left(\operatorname{Li}_2(x)-\operatorname{Li}_2(-x)-\operatorname{Li}_3(x)+\operatorname{Li}_3(-x)\right)\ dx  \sum_{n=1}^{\infty} \frac{H_{n}}{ (n+a)^{2}}= \left(\gamma + \psi(a) \right) \psi_{1}(a) - \frac{\psi_{2}(a)}{2} \, , \quad a >0. a a=1/2 \sum_{n=0}^\infty\frac{H_n}{(2n+1)^3}=\frac{45}{32}\zeta(4)-\frac74\ln2\zeta(3) S","['calculus', 'integration', 'sequences-and-series', 'closed-form', 'harmonic-numbers']"
64,"How to compare $\pi, e\cdot 2^{1/3}, \frac{1+\sqrt{2}}{\sqrt{3}-1}$",How to compare,"\pi, e\cdot 2^{1/3}, \frac{1+\sqrt{2}}{\sqrt{3}-1}","This is in the GRE exam where we are supposed to answer fast so I think there might be some trick behind this to allow us to do that. But so far the best I can do is to write $\frac{1+\sqrt{2}}{\sqrt{3}-1}=\frac{1+\sqrt{6}+\sqrt{2}+\sqrt{3}}{2}$ and compute the nominator with the value of square root 2 and 3 memorized. And as to $e\cdot 2^{1/3}$ , I just don't see how to compare it to other two items without take cubic and compute. This whole process is very time consuming. I have seen some tricks to compare say $2^\pi,\pi^2$ . But the technique does not seem to apply here.","This is in the GRE exam where we are supposed to answer fast so I think there might be some trick behind this to allow us to do that. But so far the best I can do is to write and compute the nominator with the value of square root 2 and 3 memorized. And as to , I just don't see how to compare it to other two items without take cubic and compute. This whole process is very time consuming. I have seen some tricks to compare say . But the technique does not seem to apply here.","\frac{1+\sqrt{2}}{\sqrt{3}-1}=\frac{1+\sqrt{6}+\sqrt{2}+\sqrt{3}}{2} e\cdot 2^{1/3} 2^\pi,\pi^2","['calculus', 'algebra-precalculus', 'approximation']"
65,Does the sum $\sum_{n = 1} ^\infty \frac{\sqrt {n!}} {(3+\sqrt 1)(3+\sqrt 2)...(3+\sqrt n)} $ converge or diverge?,Does the sum  converge or diverge?,\sum_{n = 1} ^\infty \frac{\sqrt {n!}} {(3+\sqrt 1)(3+\sqrt 2)...(3+\sqrt n)} ,The task is to study the convergence of the series $\sum_{n = 1} ^\infty \frac{\sqrt {n!} } {(3+\sqrt 1)(3+\sqrt 2)...(3+\sqrt n)} $ . I've tried applying Gauss's and ratio test but it didn't get me anywhere. In my textbook it says that this series is convergent.,The task is to study the convergence of the series . I've tried applying Gauss's and ratio test but it didn't get me anywhere. In my textbook it says that this series is convergent.,\sum_{n = 1} ^\infty \frac{\sqrt {n!} } {(3+\sqrt 1)(3+\sqrt 2)...(3+\sqrt n)} ,"['calculus', 'sequences-and-series']"
66,Evaluate $\int_0^1\frac{\ln x\ln(1-x)}{1+x^2}\ dx$,Evaluate,\int_0^1\frac{\ln x\ln(1-x)}{1+x^2}\ dx,"How to prove $\ \displaystyle\int_0^1\frac{\ln x\ln(1-x)}{1+x^2}\ dx=\frac{3\pi^3}{64}+\frac{\pi}{16}\ln^22-\Im\operatorname{Li}_3(1+i)$ Where $\ \displaystyle\operatorname{Li}_3(x)=\sum_{n=1}^\infty\frac{x^n}{n^3}\ $ is the trilogarithm. I managed to prove the above equality using integral manipulation, but I would like to see different approaches.","How to prove Where is the trilogarithm. I managed to prove the above equality using integral manipulation, but I would like to see different approaches.",\ \displaystyle\int_0^1\frac{\ln x\ln(1-x)}{1+x^2}\ dx=\frac{3\pi^3}{64}+\frac{\pi}{16}\ln^22-\Im\operatorname{Li}_3(1+i) \ \displaystyle\operatorname{Li}_3(x)=\sum_{n=1}^\infty\frac{x^n}{n^3}\ ,"['calculus', 'integration', 'definite-integrals', 'harmonic-numbers', 'polylogarithm']"
67,How can I prove $\frac {d}{dx} {x^n} = n x^{n-1}$ for $ n \in \Bbb R$ without circular reasoning? [duplicate],How can I prove  for  without circular reasoning? [duplicate],\frac {d}{dx} {x^n} = n x^{n-1}  n \in \Bbb R,"This question already has answers here : Prove that the derivative of $x^w$ is $w x^{w-1}$ for real $w$ (10 answers) Closed 5 years ago . I just cannot prove that $$\frac {d}{dx} {x^n} = n x^{n-1}$$ for $ n  \in \Bbb R$ . For $n \in \Bbb{N}$ , I can use the definition of a derivative : $$\frac {d}{dx}x^n = \lim_{h \rightarrow 0} \frac{(x+h)^n - x^n}{h}$$ Now applying ""Binomial Expansion"" for $\displaystyle (x+h)^n=\sum_{i=0}^{n}{n \choose i }x^{n-i}h^i$ and expanding, the $x^n$ term in the numerator cancels out and the $h$ from denominator divides the entire remaining expression . Taking limit $h$ tending to $0$ gives the required result. I have been taught that the derivative result holds for all real $n$ . But I am not aware of any ""formula"" which can allow me to expand a binomial expression with real index. I do know about the Taylor Expansion, but if I remember correctly, it utilises the very derivative that I am trying to find. How can I proceed ?","This question already has answers here : Prove that the derivative of $x^w$ is $w x^{w-1}$ for real $w$ (10 answers) Closed 5 years ago . I just cannot prove that for . For , I can use the definition of a derivative : Now applying ""Binomial Expansion"" for and expanding, the term in the numerator cancels out and the from denominator divides the entire remaining expression . Taking limit tending to gives the required result. I have been taught that the derivative result holds for all real . But I am not aware of any ""formula"" which can allow me to expand a binomial expression with real index. I do know about the Taylor Expansion, but if I remember correctly, it utilises the very derivative that I am trying to find. How can I proceed ?",\frac {d}{dx} {x^n} = n x^{n-1}  n  \in \Bbb R n \in \Bbb{N} \frac {d}{dx}x^n = \lim_{h \rightarrow 0} \frac{(x+h)^n - x^n}{h} \displaystyle (x+h)^n=\sum_{i=0}^{n}{n \choose i }x^{n-i}h^i x^n h h 0 n,"['calculus', 'limits']"
68,Improper integral $ \int\limits_0^{\infty} \frac{ x^2 \arctan x }{x^4 + x^2 + 1 } dx $,Improper integral, \int\limits_0^{\infty} \frac{ x^2 \arctan x }{x^4 + x^2 + 1 } dx ,"Im trying to find $$ \int\limits_0^{\infty} \frac{ x^2 \arctan x }{x^4 + x^2 + 1 } dx $$ Thoughts: My first thought was to write the denominator as $(x^2+1)^2 -x^2 $ and then by difference of squares we have $(x^2+1-x)(x^2+1+x)$ . Then write the numerator as $x^2+x+1-x-1 = (x^2+x+1) - (x+1) $ and so our integral reduces to $$ I = \int\limits_0^{\infty} \frac{ \arctan x }{x^2-x+1} +\int\limits_0^{\infty} \frac{ (-x-1) \arctan x}{x^4+x^2+1}$$ Also, write $-x-1= -x-1 +x^2 +1 - 1 - x^2 = (x^2-x+1) - (x^2+2)$ to reduce $$ \int\limits_0^{\infty} \frac{ (-x-1) \arctan x}{x^4+x^2+1} =  \int\limits_0^{\infty} \frac{ \arctan x }{x^2+x+1} - \int\limits_0^{\infty} \frac{ (x^2+2) \arctan x }{x^4+x^2+1} $$ Therefore, so far we got $$ 2I = \int\limits_0^{\infty} \frac{ \arctan x }{x^2-x+1}  + \int\limits_0^{\infty} \frac{ \arctan x }{x^2+x+1} - \int\limits_0^{\infty} \frac{ 2 \arctan x }{x^4+x^2+1}$$ But, here I get stuck again. Perhaps my approach is bad? I dont see a way out from there, I suck at integration :x. Any ideas?","Im trying to find Thoughts: My first thought was to write the denominator as and then by difference of squares we have . Then write the numerator as and so our integral reduces to Also, write to reduce Therefore, so far we got But, here I get stuck again. Perhaps my approach is bad? I dont see a way out from there, I suck at integration :x. Any ideas?", \int\limits_0^{\infty} \frac{ x^2 \arctan x }{x^4 + x^2 + 1 } dx  (x^2+1)^2 -x^2  (x^2+1-x)(x^2+1+x) x^2+x+1-x-1 = (x^2+x+1) - (x+1)   I = \int\limits_0^{\infty} \frac{ \arctan x }{x^2-x+1} +\int\limits_0^{\infty} \frac{ (-x-1) \arctan x}{x^4+x^2+1} -x-1= -x-1 +x^2 +1 - 1 - x^2 = (x^2-x+1) - (x^2+2)  \int\limits_0^{\infty} \frac{ (-x-1) \arctan x}{x^4+x^2+1} =  \int\limits_0^{\infty} \frac{ \arctan x }{x^2+x+1} - \int\limits_0^{\infty} \frac{ (x^2+2) \arctan x }{x^4+x^2+1}   2I = \int\limits_0^{\infty} \frac{ \arctan x }{x^2-x+1}  + \int\limits_0^{\infty} \frac{ \arctan x }{x^2+x+1} - \int\limits_0^{\infty} \frac{ 2 \arctan x }{x^4+x^2+1},['calculus']
69,A variant of Heron's shortest distance problem (with three points instead of two),A variant of Heron's shortest distance problem (with three points instead of two),,"In the question "" Minimum Distance Problem with several points "", the OP has asked for a geometric solution of a generalization of Heron's problem (for $n$ points). I am interested in the (much more modest) special case of three points: ""Given three points $A$, $B$ and $C$ on the same side of a straight line, find $X$, a point on the straight line, such that it minimizes $AX+BX+CX$."" Since with two points one can solve it geometrically by (among many other methods) thinking of an ellipse expanding until it just touches the straight line, I thought that maybe one can solve the three points case by thinking on some other expanding shape (the $n$-ellipse?). P.S. https://www.cut-the-knot.org/Curriculum/Geometry/HeronsProblem.shtml P.P.S. https://en.wikipedia.org/wiki/N-ellipse","In the question "" Minimum Distance Problem with several points "", the OP has asked for a geometric solution of a generalization of Heron's problem (for $n$ points). I am interested in the (much more modest) special case of three points: ""Given three points $A$, $B$ and $C$ on the same side of a straight line, find $X$, a point on the straight line, such that it minimizes $AX+BX+CX$."" Since with two points one can solve it geometrically by (among many other methods) thinking of an ellipse expanding until it just touches the straight line, I thought that maybe one can solve the three points case by thinking on some other expanding shape (the $n$-ellipse?). P.S. https://www.cut-the-knot.org/Curriculum/Geometry/HeronsProblem.shtml P.P.S. https://en.wikipedia.org/wiki/N-ellipse",,"['calculus', 'geometry']"
70,"How to find values of $f(m,k)=\prod\limits_{n=1}^{\infty}\left[\frac{(mn-k)^2}{(mn-k+1)(mn-k-1)}\right]$?",How to find values of ?,"f(m,k)=\prod\limits_{n=1}^{\infty}\left[\frac{(mn-k)^2}{(mn-k+1)(mn-k-1)}\right]","$f(2,0)=\frac{\pi}{2}$ is very popular result, but there are a lot of them, not involving $\pi$, for example $$f(4,2)=\prod\limits_{n=1}^{\infty}\left[\frac{(4n-2)^2}{(4n-1)(4n-3)}\right]=\sqrt{2}$$ $$f(6,3)=\prod\limits_{n=1}^{\infty}\left[\frac{(6n-3)^2}{(6n-2)(6n-4)}\right]=\frac{2}{\sqrt{3}}$$ $$f(6,4)=\prod\limits_{n=1}^{\infty}\left[\frac{(6n-4)^2}{(6n-3)(6n-5)}\right]=\frac{3^{1/2}}{2^{1/3}}$$ How to find values of $f(m,k)$ for all $m>1$ and $(m-1)>k\geqslant0$?","$f(2,0)=\frac{\pi}{2}$ is very popular result, but there are a lot of them, not involving $\pi$, for example $$f(4,2)=\prod\limits_{n=1}^{\infty}\left[\frac{(4n-2)^2}{(4n-1)(4n-3)}\right]=\sqrt{2}$$ $$f(6,3)=\prod\limits_{n=1}^{\infty}\left[\frac{(6n-3)^2}{(6n-2)(6n-4)}\right]=\frac{2}{\sqrt{3}}$$ $$f(6,4)=\prod\limits_{n=1}^{\infty}\left[\frac{(6n-4)^2}{(6n-3)(6n-5)}\right]=\frac{3^{1/2}}{2^{1/3}}$$ How to find values of $f(m,k)$ for all $m>1$ and $(m-1)>k\geqslant0$?",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'closed-form', 'infinite-product']"
71,All the solutions of $f'(x)=f(x+\pi/2)$,All the solutions of,f'(x)=f(x+\pi/2),"Consider the following equation (with $f \in C^{\infty}(\mathbb{R})$): $$f'(x)=f(x+\pi/2)$$ This equation is satisfied by $f(x) = A\cos(x) +B\sin(x)$, for any $A,B \in \mathbb{R}$. Question : What are all the (other) solutions of this equation (if any)?","Consider the following equation (with $f \in C^{\infty}(\mathbb{R})$): $$f'(x)=f(x+\pi/2)$$ This equation is satisfied by $f(x) = A\cos(x) +B\sin(x)$, for any $A,B \in \mathbb{R}$. Question : What are all the (other) solutions of this equation (if any)?",,"['calculus', 'ordinary-differential-equations', 'trigonometry', 'delay-differential-equations']"
72,Does this simple 2D dynamical system have a conserved quantity?,Does this simple 2D dynamical system have a conserved quantity?,,"Consider the following two-dimensional dynamical system, $$\dot{x} = y -y^3$$ $$\dot{y}= -x-y^2$$ where as usual the dot represents a time derivative. (This system is found on page 165 of Strogatz book 'Nonlinear Dynamics and Chaos'.) I am trying to find a conserved quantity for this system , apparently it does exist and there is a simple way to find it. Strogatz shows that the system is reversible with a nonlinear center at the origin. The phase portrait looks like: Does this system have a conserved quantity? Something like $x^2+y^2 = \text{constant}$, for example (this does not work though...). If so how would I go about finding it? I have tried playing around with the system but have not been able to find one, however the phase portrait leads me to believe that something should be conserved.","Consider the following two-dimensional dynamical system, $$\dot{x} = y -y^3$$ $$\dot{y}= -x-y^2$$ where as usual the dot represents a time derivative. (This system is found on page 165 of Strogatz book 'Nonlinear Dynamics and Chaos'.) I am trying to find a conserved quantity for this system , apparently it does exist and there is a simple way to find it. Strogatz shows that the system is reversible with a nonlinear center at the origin. The phase portrait looks like: Does this system have a conserved quantity? Something like $x^2+y^2 = \text{constant}$, for example (this does not work though...). If so how would I go about finding it? I have tried playing around with the system but have not been able to find one, however the phase portrait leads me to believe that something should be conserved.",,"['calculus', 'dynamical-systems', 'nonlinear-system']"
73,An interesting pattern in the general solution of the $n$-th order ODE: $\frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0$.,An interesting pattern in the general solution of the -th order ODE: .,n \frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0,"Context: My friend (The same one who gave me this ODE) originally challenged me to obtain the general solution to this ODE: $$\frac{d^3 y}{dx^3}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{1}$$ Where $\alpha,\beta \in \mathbb{R}$. I could not figure out any substitution without the use of power series. I thus decided to play around with Wolfram|Alpha , and decided to generalize the order of the ODE, like this: $$\frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{2}$$ Upon increasing the value of $n$, I noticed an interesting pattern: When $n=3$, the solution is: $$\small y(x)=\frac{\sqrt[3]{\alpha}\cdot c_2 x \cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{1}{3};\frac{2}{3},\frac{4}{3}; -\frac{x^3 \alpha}{9}\right)}{3^{2/3}}+c_1\cdot {_1F_2}\left(\frac{\beta}{3\alpha};\frac{1}{3};\frac{2}{3};-\frac{x^3\alpha}{9}\right)+\frac{\alpha^{2/3} c_3 x^2\cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{2}{3};\frac{4}{3},\frac{5}{3};-\frac{x^3 \alpha}{9}\right)}{\sqrt[3]{3}} \tag{3}$$ When $n=4$, the solution is: $$\small y(x)=\frac{\sqrt[4]{\alpha}c_2 \cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{4};\frac{1}{2},\frac{3}{4},\frac{5}{4};-\frac{x^4\alpha}{64}\right)}{\sqrt{2}}+c_1\cdot {_1 F_3}\left(\frac{\beta}{4\alpha};\frac{1}{4},\frac{1}{2},\frac{3}{4};-\frac{x^4 \alpha}{64}\right)+\frac{\alpha^{3/4} c_4 x^3\cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{3}{4};\frac{5}{4},\frac{3}{2},\frac{7}{4};-\frac{x^4 \alpha}{64}\right)}{\sqrt{2}}+\sqrt{\alpha}c_3 x^2 {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{2};\frac{3}{4},\frac{5}{4},\frac{3}{2};-\frac{x^4 \alpha}{64}\right)$$ When $n=5$, the solution is: $$\small y(x)=\frac{\sqrt[5]{\alpha} c_2 x \cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{1}{5};\frac{2}{5},\frac{3}{5},\frac{4}{5},\frac{6}{5};-\frac{x^5 \alpha}{625}\right)}{5^{4/5}}+c_1\cdot {_1F_4}\left(\frac{\beta}{5\alpha};\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};-\frac{x^5 \alpha}{625}\right)+\frac{\alpha^{4/5}c_5 x^4\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{4}{5};\frac{6}{5},\frac{7}{5},\frac{8}{5},\frac{9}{5};-\frac{x^5 \alpha}{625}\right)}{\sqrt[5]{5}}+\frac{\alpha^{3/5} c_4 x^3\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{3}{5};\frac{4}{5},\frac{6}{5},\frac{7}{5},\frac{8}{5};-\frac{x^5 \alpha}{625}\right)}{5^{2/5}}+\frac{\alpha^{2/5} c_3 x^2 {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{2}{5};\frac{3}{5},\frac{4}{5},\frac{6}{5},\frac{7}{5};-\frac{x^5 \alpha}{625}\right)}{5^{3/5}}$$ If anyone is interested, here are the solutions for $n=6$ , $n=7$ , $n=8$ and $n=9$ . I apologize if I mistyped one of the solutions. As we can see, there is a pattern. I would therefore like to find a reason for this pattern. Here, I attempt to explain it using a power series solution: We make the ansatz: $$y=\sum_{k=0}^{\infty} A_k x^k$$ Thus, if we substitute this into our ODE on $(2)$, we obtain: $$\sum_{k=n}^{\infty} k(k-1)(k-2)\cdots (k-n+1)A_{k}x^{k-n}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$ Shifting the first sum, we obtain: $$\sum_{k=0}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$ Hence, we have: $$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=1}^{\infty} A_k x^k=0$$ Thus, we obtain: $$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} \left[(k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}+\alpha k A_k+\beta\cdot A_k\right]\cdot x^k=0$$ If the series is a solution, then all the coefficients must equal zero: $$\begin{cases} A_n=-\frac{\beta}{n!}\cdot A_0 & k=0\\ A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k &  k>0, k\in \mathbb{N}\end{cases}$$ The system of equations is redundant, thus one may just solve the following recurrence relation: $$\small A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k=-\frac{k!(\alpha k+\beta)}{(k+n)!}A_k=-\frac{\alpha k+\beta}{(k+1)_n} A_k \tag{4}$$ Where $(x)_n$ is the Pochhammer Symbol . The Hypergeometric function ${_1P_q}(a_1; b_1,\cdots,b_q;x)$ is defined as : $${_1P_q}(a_1; b_1,\cdots,b_q;x)=\sum_{k=0}^{\infty} \frac{(a_1)_k}{(b_1)_k\cdots (b_q)_k}\cdot \frac{x^k}{k!}$$ Given the recurrence, this should follow the pattern for general values of $n\geq 3, n \in \mathbb{Z}^+$. The problem: However, I am not satisfied with this. I would prefer if one could use one (or several) substitution(s) which works for general values of $n$ and which does not involve a series solution. My idea was to reduce the differential equation on $(2)$ to Kummer's Equation to obtain these hypergeometric functions on their solutions: $$z\frac{d^2 w}{dz^2}+(b-z)\frac{dw}{dz}-aw=0 \tag{5}$$ To do this, I've attempted to use the same substitution as done on my previous question : $$\ln y=\ln{z}-\frac{(1+x)^2}{4}$$ However, the resulting ODE turns out to be even worse than it was previously (Even for $n=3$). The question: Is there any way we can make several substitutions such that we obtain the general solution for general values of $n$ where $n\geq 3, n \in \mathbb{Z^+}$? If this is too difficult or not possible, is there a way we can use several substitutions for the specific case $n=3$ to obtain the general solution? i.e. The same one, or in a similar form to the one on equation $(3)$. Thanks in advance.","Context: My friend (The same one who gave me this ODE) originally challenged me to obtain the general solution to this ODE: $$\frac{d^3 y}{dx^3}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{1}$$ Where $\alpha,\beta \in \mathbb{R}$. I could not figure out any substitution without the use of power series. I thus decided to play around with Wolfram|Alpha , and decided to generalize the order of the ODE, like this: $$\frac{d^n y}{dx^n}+\alpha x\frac{dy}{dx}+\beta y=0 \tag{2}$$ Upon increasing the value of $n$, I noticed an interesting pattern: When $n=3$, the solution is: $$\small y(x)=\frac{\sqrt[3]{\alpha}\cdot c_2 x \cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{1}{3};\frac{2}{3},\frac{4}{3}; -\frac{x^3 \alpha}{9}\right)}{3^{2/3}}+c_1\cdot {_1F_2}\left(\frac{\beta}{3\alpha};\frac{1}{3};\frac{2}{3};-\frac{x^3\alpha}{9}\right)+\frac{\alpha^{2/3} c_3 x^2\cdot {_1F_2}\left(\frac{\beta}{3\alpha}+\frac{2}{3};\frac{4}{3},\frac{5}{3};-\frac{x^3 \alpha}{9}\right)}{\sqrt[3]{3}} \tag{3}$$ When $n=4$, the solution is: $$\small y(x)=\frac{\sqrt[4]{\alpha}c_2 \cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{4};\frac{1}{2},\frac{3}{4},\frac{5}{4};-\frac{x^4\alpha}{64}\right)}{\sqrt{2}}+c_1\cdot {_1 F_3}\left(\frac{\beta}{4\alpha};\frac{1}{4},\frac{1}{2},\frac{3}{4};-\frac{x^4 \alpha}{64}\right)+\frac{\alpha^{3/4} c_4 x^3\cdot {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{3}{4};\frac{5}{4},\frac{3}{2},\frac{7}{4};-\frac{x^4 \alpha}{64}\right)}{\sqrt{2}}+\sqrt{\alpha}c_3 x^2 {_1F_3}\left(\frac{\beta}{4\alpha}+\frac{1}{2};\frac{3}{4},\frac{5}{4},\frac{3}{2};-\frac{x^4 \alpha}{64}\right)$$ When $n=5$, the solution is: $$\small y(x)=\frac{\sqrt[5]{\alpha} c_2 x \cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{1}{5};\frac{2}{5},\frac{3}{5},\frac{4}{5},\frac{6}{5};-\frac{x^5 \alpha}{625}\right)}{5^{4/5}}+c_1\cdot {_1F_4}\left(\frac{\beta}{5\alpha};\frac{1}{5},\frac{2}{5},\frac{3}{5},\frac{4}{5};-\frac{x^5 \alpha}{625}\right)+\frac{\alpha^{4/5}c_5 x^4\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{4}{5};\frac{6}{5},\frac{7}{5},\frac{8}{5},\frac{9}{5};-\frac{x^5 \alpha}{625}\right)}{\sqrt[5]{5}}+\frac{\alpha^{3/5} c_4 x^3\cdot {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{3}{5};\frac{4}{5},\frac{6}{5},\frac{7}{5},\frac{8}{5};-\frac{x^5 \alpha}{625}\right)}{5^{2/5}}+\frac{\alpha^{2/5} c_3 x^2 {_1F_4}\left(\frac{\beta}{5\alpha}+\frac{2}{5};\frac{3}{5},\frac{4}{5},\frac{6}{5},\frac{7}{5};-\frac{x^5 \alpha}{625}\right)}{5^{3/5}}$$ If anyone is interested, here are the solutions for $n=6$ , $n=7$ , $n=8$ and $n=9$ . I apologize if I mistyped one of the solutions. As we can see, there is a pattern. I would therefore like to find a reason for this pattern. Here, I attempt to explain it using a power series solution: We make the ansatz: $$y=\sum_{k=0}^{\infty} A_k x^k$$ Thus, if we substitute this into our ODE on $(2)$, we obtain: $$\sum_{k=n}^{\infty} k(k-1)(k-2)\cdots (k-n+1)A_{k}x^{k-n}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$ Shifting the first sum, we obtain: $$\sum_{k=0}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=0}^{\infty} A_k x^k=0$$ Hence, we have: $$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} (k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}x^{k}+\alpha\cdot \sum_{k=1}^{\infty} k A_k x^{k}+\beta\cdot \sum_{k=1}^{\infty} A_k x^k=0$$ Thus, we obtain: $$\small n!\cdot A_n+\beta\cdot A_0+\sum_{k=1}^{\infty} \left[(k+n)(k+n-1)(k+n-2)\cdots (k+1)A_{k+n}+\alpha k A_k+\beta\cdot A_k\right]\cdot x^k=0$$ If the series is a solution, then all the coefficients must equal zero: $$\begin{cases} A_n=-\frac{\beta}{n!}\cdot A_0 & k=0\\ A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k &  k>0, k\in \mathbb{N}\end{cases}$$ The system of equations is redundant, thus one may just solve the following recurrence relation: $$\small A_{k+n}=-\frac{\alpha k+\beta}{(k+n)(k+n-1)(k+n-2)\cdots (k+1)}A_k=-\frac{k!(\alpha k+\beta)}{(k+n)!}A_k=-\frac{\alpha k+\beta}{(k+1)_n} A_k \tag{4}$$ Where $(x)_n$ is the Pochhammer Symbol . The Hypergeometric function ${_1P_q}(a_1; b_1,\cdots,b_q;x)$ is defined as : $${_1P_q}(a_1; b_1,\cdots,b_q;x)=\sum_{k=0}^{\infty} \frac{(a_1)_k}{(b_1)_k\cdots (b_q)_k}\cdot \frac{x^k}{k!}$$ Given the recurrence, this should follow the pattern for general values of $n\geq 3, n \in \mathbb{Z}^+$. The problem: However, I am not satisfied with this. I would prefer if one could use one (or several) substitution(s) which works for general values of $n$ and which does not involve a series solution. My idea was to reduce the differential equation on $(2)$ to Kummer's Equation to obtain these hypergeometric functions on their solutions: $$z\frac{d^2 w}{dz^2}+(b-z)\frac{dw}{dz}-aw=0 \tag{5}$$ To do this, I've attempted to use the same substitution as done on my previous question : $$\ln y=\ln{z}-\frac{(1+x)^2}{4}$$ However, the resulting ODE turns out to be even worse than it was previously (Even for $n=3$). The question: Is there any way we can make several substitutions such that we obtain the general solution for general values of $n$ where $n\geq 3, n \in \mathbb{Z^+}$? If this is too difficult or not possible, is there a way we can use several substitutions for the specific case $n=3$ to obtain the general solution? i.e. The same one, or in a similar form to the one on equation $(3)$. Thanks in advance.",,"['calculus', 'ordinary-differential-equations', 'special-functions', 'hypergeometric-function']"
74,"Closed form for $\int_{0}^{\infty }\!{\rm erf} \left(cx\right) \left( {\rm erf} \left(x \right) \right) ^{2}{{\rm e}^{-{x}^{2}}}\,{\rm d}x$",Closed form for,"\int_{0}^{\infty }\!{\rm erf} \left(cx\right) \left( {\rm erf} \left(x \right) \right) ^{2}{{\rm e}^{-{x}^{2}}}\,{\rm d}x","I encountered this integral in my calculations: $$\int_{0}^{\infty }\!{\rm erf} \left(cx\right) \left( {\rm erf} \left(x \right) \right) ^{2}{{\rm e}^{-{x}^{2}}}\,{\rm d}x$$ where $c>0$ and $c\in \mathbb{R}$ but could not find a closed-form representation for it. I also tried to find possible closed forms using Inverse Symbolic Calculator and WolframAlpha but they did not find anything. I was looking in the book ""Integrals ans Series Volume 2-Prudnikov-Brychkov-Marychev"" but did not find a similar formula. I am not sure it exists, but if it exists I want to know it. Closed forms are easier to manipulate, sometimes closed forms of different integrals or sums contain terms that cancel each other etc Could you please help me to find a closed form (even using non-elementary special functions), if it exists?","I encountered this integral in my calculations: $$\int_{0}^{\infty }\!{\rm erf} \left(cx\right) \left( {\rm erf} \left(x \right) \right) ^{2}{{\rm e}^{-{x}^{2}}}\,{\rm d}x$$ where $c>0$ and $c\in \mathbb{R}$ but could not find a closed-form representation for it. I also tried to find possible closed forms using Inverse Symbolic Calculator and WolframAlpha but they did not find anything. I was looking in the book ""Integrals ans Series Volume 2-Prudnikov-Brychkov-Marychev"" but did not find a similar formula. I am not sure it exists, but if it exists I want to know it. Closed forms are easier to manipulate, sometimes closed forms of different integrals or sums contain terms that cancel each other etc Could you please help me to find a closed form (even using non-elementary special functions), if it exists?",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'error-function']"
75,Prove this $ \int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45 $,Prove this, \int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45 ,I have trouble with this seemingly simple problem $$ K=\int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45.\tag{A} $$ Here is the Wolfram Alpha computation Proof . I tried to find residue at the pole $x=\frac{\pi i}{2}$ and get $\frac{9}{5\pi i}$ ( link to WA). Therefore residue theorem tell me that $K=\frac{18}{5}\neq\frac45$. I'm stuck. How to prove (A)?,I have trouble with this seemingly simple problem $$ K=\int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45.\tag{A} $$ Here is the Wolfram Alpha computation Proof . I tried to find residue at the pole $x=\frac{\pi i}{2}$ and get $\frac{9}{5\pi i}$ ( link to WA). Therefore residue theorem tell me that $K=\frac{18}{5}\neq\frac45$. I'm stuck. How to prove (A)?,,"['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'contour-integration']"
76,Why is substitution allowed in Taylor Series?,Why is substitution allowed in Taylor Series?,,"While finding the Taylor Series of a function, when are you allowed to substitute? And why ? For example: Around $x=0$ for $e^{2x}$ I apparently am allowed to substitute $u=2x$ and then use the known series for $e^u$. But for $e^{x+1}$ I am not allowed to substitute $u=x+1$. I know the technique for finding the Taylor Series of $e^{x+1}$ around $x=0$ by taking $e^{x+1}=e\times e^x$. However, I am looking for understanding and intuition for when and why it is allowed to apply substitution. Note: there are several question that are similar to this one, but I have found none that actually answers the question ""why""; or that shows a complete proof. EDIT: Thanks to the answer of Markus Scheuer I should refine the question to cases where the series is finite, for example $n\to3$","While finding the Taylor Series of a function, when are you allowed to substitute? And why ? For example: Around $x=0$ for $e^{2x}$ I apparently am allowed to substitute $u=2x$ and then use the known series for $e^u$. But for $e^{x+1}$ I am not allowed to substitute $u=x+1$. I know the technique for finding the Taylor Series of $e^{x+1}$ around $x=0$ by taking $e^{x+1}=e\times e^x$. However, I am looking for understanding and intuition for when and why it is allowed to apply substitution. Note: there are several question that are similar to this one, but I have found none that actually answers the question ""why""; or that shows a complete proof. EDIT: Thanks to the answer of Markus Scheuer I should refine the question to cases where the series is finite, for example $n\to3$",,"['calculus', 'taylor-expansion']"
77,Convergence of $\sum_{n=1}^\infty\left(\sqrt[n]{2}-1\right)$,Convergence of,\sum_{n=1}^\infty\left(\sqrt[n]{2}-1\right),"I'm trying to determine whether $$\sum_{n=1}^\infty \left ( \sqrt[n]{2}-1\right )$$ converges or diverges.  Ratio, root, nth term, etc tests are either inconclusive or too difficult to simplify.  I feel like there must be something I can bound this series by but I can't think what. In this question the answerer very smartly (I have no idea how he/ she thought to do that) used the fact that $(1+\frac 1n)^n \le e \le 3$ to bound $\sqrt[n]{3} -1$ by $\frac 1n$ but that only worked because $3\ge e$.  So even though that question looks very similar I don't think I can apply that idea here. Edit: This is in the section before power/ Taylor series so I don't think I'm allowed to use that.","I'm trying to determine whether $$\sum_{n=1}^\infty \left ( \sqrt[n]{2}-1\right )$$ converges or diverges.  Ratio, root, nth term, etc tests are either inconclusive or too difficult to simplify.  I feel like there must be something I can bound this series by but I can't think what. In this question the answerer very smartly (I have no idea how he/ she thought to do that) used the fact that $(1+\frac 1n)^n \le e \le 3$ to bound $\sqrt[n]{3} -1$ by $\frac 1n$ but that only worked because $3\ge e$.  So even though that question looks very similar I don't think I can apply that idea here. Edit: This is in the section before power/ Taylor series so I don't think I'm allowed to use that.",,"['calculus', 'sequences-and-series']"
78,Integral $\int_0^1\arctan(x)\arctan\left(x\sqrt3\right)\ln(x)dx$,Integral,\int_0^1\arctan(x)\arctan\left(x\sqrt3\right)\ln(x)dx,"I need to evaluate this integral: $$\int_0^1\arctan(x)\arctan\left(x\sqrt3\right)\ln(x)dx$$ Apparently, Maple and Mathematica cannot do anything with it, but I saw similar integrals to be evaluated in terms of polylogarithms (unfortunately, I have not yet mastered them enough to do it myself). Could anybody please help me with it?","I need to evaluate this integral: $$\int_0^1\arctan(x)\arctan\left(x\sqrt3\right)\ln(x)dx$$ Apparently, Maple and Mathematica cannot do anything with it, but I saw similar integrals to be evaluated in terms of polylogarithms (unfortunately, I have not yet mastered them enough to do it myself). Could anybody please help me with it?",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'logarithms']"
79,Find $\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}}$ using Riemann sums,Find  using Riemann sums,\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}},"Find   $$\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}}$$   using Riemann sums. I got $$\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}} = \lim_{n \to \infty} \sum^n_{k=1} {1 \over {n^2}} {1 \over {(1+({k \over n})^2)^2}} $$ Now, this is not the classic $\lim_{n \to \infty} \sum^n_{k=1} {1 \over {n}} {1 \over {1+({k \over n})^2}}$ that I can just define $f(x)={1 \over {1+x^2}}$. The summation is to $n$ and not $n^2$... what should I do?","Find   $$\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}}$$   using Riemann sums. I got $$\lim_{n \to \infty} n^2 \sum^n_{k=1} {1 \over {(n^2+k^2)^2}} = \lim_{n \to \infty} \sum^n_{k=1} {1 \over {n^2}} {1 \over {(1+({k \over n})^2)^2}} $$ Now, this is not the classic $\lim_{n \to \infty} \sum^n_{k=1} {1 \over {n}} {1 \over {1+({k \over n})^2}}$ that I can just define $f(x)={1 \over {1+x^2}}$. The summation is to $n$ and not $n^2$... what should I do?",,"['calculus', 'integration', 'limits', 'riemann-sum']"
80,How to evaluate $\int_0^{\pi /2}\frac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du$?,How to evaluate ?,\int_0^{\pi /2}\frac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du,"I want to find the value of  $$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du.$$ Let $v=\frac{\pi}{2}-u$, then $$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du=\int_0^{\pi /2}\dfrac{(\pi /2-v)^2\ln{(2\sin v)}}{((\pi /2-v)^2+\ln^2{(2\sin v)})^2}dv.$$ I don't know how to go from here. Thank you.","I want to find the value of  $$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du.$$ Let $v=\frac{\pi}{2}-u$, then $$\int_0^{\pi /2}\dfrac{u^2\ln{(2\cos u)}}{(u^2+\ln^2{(2\cos u)})^2}du=\int_0^{\pi /2}\dfrac{(\pi /2-v)^2\ln{(2\sin v)}}{((\pi /2-v)^2+\ln^2{(2\sin v)})^2}dv.$$ I don't know how to go from here. Thank you.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
81,How would I go about proving this expression?,How would I go about proving this expression?,,"Expression: $$ \sum_{c=1}^{n-1} \sum_{k=c+1}^{n} \sum_{j=0}^{min\{c,k-c\}} {c \choose j} \frac{(k-c)!}{(k-c-j)!} f^{k-c-j} g^{c-j} B_{n,k}^{(g \diamond f)^c}(x) =$$ $$ \sum_{w=1}^n \sum_{m_1=1}^w \sum_{m_2 = 1}^w \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(m_1+m_2-w)!}f^{w-m_1}g^{w-m_2} \sum_{l=1}^{n-1} {n \choose l} B_{n-l,m_1}^f(x)  B_{l,m_2}^g(x) $$ It is known that: $$ B_{a,b}^f(x) = B_{a,b}^g(x) = 0 \quad\quad\quad (b > a) $$ $B_{n,k}^f(x)$ is the partial bell polynomial of the function $f(x)$ where: $$ B_{n,k}^f(x) = B_{n,k}\left(f'(x),f''(x),\ldots,f^{(n-k+1)}(x)\right) $$ For more information on Bell Polynomials click here , but please be aware that no information regarding Bell Polynomials is needed to complete the proof. Note that: $$ B_{n,k}^{(g \diamond f)^c}(x) = \sum_{l=1}^{n-1} {n \choose l} B_{n-l,c}^f(x)B_{l,k-c}^g(x) $$ My Observations: I have no formal education in techniques for proving these kinds of expressions, but I will share what I have observed: By setting  $$m_1 = c$$ $$m_2 = k-c$$ $$w = k -j$$ We can see (disregarding the summations) that the expressions are exactly the same. This is what I mean: $$ \sum_{c=1}^{n-1} \sum_{k=c+1}^{n} \sum_j {c \choose j} \frac{(k-c)!}{(k-c-j)!} f^{k-c-j} g^{c-j} \Phi(n,k,c) = $$ $$ \sum_{k-j=1}^n \sum_{c=1}^{k-j} \sum_{k-c = 1}^{k-j}\frac{c!(k-c)!}{j!(k-c-j)!(c-j)!}f^{k-c-j}g^{c-j} \sum_{l=1}^{n-1} {n \choose l} \rho(l,c,f) \rho(n-l,k-c,g) $$ Where as stated previously, the summations do not even make sense, but the terms inside the summations are identical. Not sure if this helps but it is interesting and it is something I observed. We can also see that in order for the terms to be non-zero the following inequality must be true: $$ m_1+m_2 \ge w $$ This is because of the coefficients: $$ \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(m_1+m_2-w)!} $$ Lets assume that $m_1+m_2 \le w$, then we have $m_1+m_2 - w = -I$ where $-I$ is a negative whole number, therefore the coefficient would be $0$ by taking a negative factorial of an integer: $$ \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(-I)!} = \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!} \frac{1}{\pm \infty} = 0 $$ Since we know $w \ge m_1 \ge 1$, $w \ge m_2 \ge 1$, and $n \ge w \ge 1$ What I am looking for: I am looking for a good place or technique to start or complete the proof. Feel free to present your own proof if you wish as well.","Expression: $$ \sum_{c=1}^{n-1} \sum_{k=c+1}^{n} \sum_{j=0}^{min\{c,k-c\}} {c \choose j} \frac{(k-c)!}{(k-c-j)!} f^{k-c-j} g^{c-j} B_{n,k}^{(g \diamond f)^c}(x) =$$ $$ \sum_{w=1}^n \sum_{m_1=1}^w \sum_{m_2 = 1}^w \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(m_1+m_2-w)!}f^{w-m_1}g^{w-m_2} \sum_{l=1}^{n-1} {n \choose l} B_{n-l,m_1}^f(x)  B_{l,m_2}^g(x) $$ It is known that: $$ B_{a,b}^f(x) = B_{a,b}^g(x) = 0 \quad\quad\quad (b > a) $$ $B_{n,k}^f(x)$ is the partial bell polynomial of the function $f(x)$ where: $$ B_{n,k}^f(x) = B_{n,k}\left(f'(x),f''(x),\ldots,f^{(n-k+1)}(x)\right) $$ For more information on Bell Polynomials click here , but please be aware that no information regarding Bell Polynomials is needed to complete the proof. Note that: $$ B_{n,k}^{(g \diamond f)^c}(x) = \sum_{l=1}^{n-1} {n \choose l} B_{n-l,c}^f(x)B_{l,k-c}^g(x) $$ My Observations: I have no formal education in techniques for proving these kinds of expressions, but I will share what I have observed: By setting  $$m_1 = c$$ $$m_2 = k-c$$ $$w = k -j$$ We can see (disregarding the summations) that the expressions are exactly the same. This is what I mean: $$ \sum_{c=1}^{n-1} \sum_{k=c+1}^{n} \sum_j {c \choose j} \frac{(k-c)!}{(k-c-j)!} f^{k-c-j} g^{c-j} \Phi(n,k,c) = $$ $$ \sum_{k-j=1}^n \sum_{c=1}^{k-j} \sum_{k-c = 1}^{k-j}\frac{c!(k-c)!}{j!(k-c-j)!(c-j)!}f^{k-c-j}g^{c-j} \sum_{l=1}^{n-1} {n \choose l} \rho(l,c,f) \rho(n-l,k-c,g) $$ Where as stated previously, the summations do not even make sense, but the terms inside the summations are identical. Not sure if this helps but it is interesting and it is something I observed. We can also see that in order for the terms to be non-zero the following inequality must be true: $$ m_1+m_2 \ge w $$ This is because of the coefficients: $$ \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(m_1+m_2-w)!} $$ Lets assume that $m_1+m_2 \le w$, then we have $m_1+m_2 - w = -I$ where $-I$ is a negative whole number, therefore the coefficient would be $0$ by taking a negative factorial of an integer: $$ \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!(-I)!} = \frac{m_1!m_2!}{(w-m_1)!(w-m_2)!} \frac{1}{\pm \infty} = 0 $$ Since we know $w \ge m_1 \ge 1$, $w \ge m_2 \ge 1$, and $n \ge w \ge 1$ What I am looking for: I am looking for a good place or technique to start or complete the proof. Feel free to present your own proof if you wish as well.",,"['calculus', 'combinatorics', 'summation']"
82,Integral $ \int_{0}^1 \sqrt{\frac{\ln{x}}{x^2-1}} dx$,Integral, \int_{0}^1 \sqrt{\frac{\ln{x}}{x^2-1}} dx,"Please help evaluating this integral $$ \large\int_{0}^1 \sqrt{\frac{\ln{x}}{x^2-1}}  dx$$ Mathematica could not evaluate it in a closed form. Numerically it is about $$I\approx1.060837861412045137097...,$$ ISC+ did not return any closed form for it. It is related to a previous post","Please help evaluating this integral $$ \large\int_{0}^1 \sqrt{\frac{\ln{x}}{x^2-1}}  dx$$ Mathematica could not evaluate it in a closed form. Numerically it is about $$I\approx1.060837861412045137097...,$$ ISC+ did not return any closed form for it. It is related to a previous post",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
83,Premises of the Mean Value Theorem,Premises of the Mean Value Theorem,,"Why does the statement of the mean value theorem requires that: (1)The function $f$ be continuous on the closed interval $[a,b]$ (2)Differentiable on the open interval $(a,b)$. Couldn't we just require (2) and the the first premise will be met because of the fact that differientiability implies continuity ?","Why does the statement of the mean value theorem requires that: (1)The function $f$ be continuous on the closed interval $[a,b]$ (2)Differentiable on the open interval $(a,b)$. Couldn't we just require (2) and the the first premise will be met because of the fact that differientiability implies continuity ?",,['calculus']
84,Let $a=\dfrac{3+\sqrt{5}}{2}.$ Show that $\lfloor a \lfloor an \rfloor \rfloor+n$ is divisible by $3$.,Let  Show that  is divisible by .,a=\dfrac{3+\sqrt{5}}{2}. \lfloor a \lfloor an \rfloor \rfloor+n 3,"Let $a=\dfrac{3+\sqrt{5}}{2}.$ Show that for all $n\in\mathbb N$ , $\lfloor a \lfloor an \rfloor \rfloor+n$ is divisible by $3$ . My teacher solve this problem with induction, I am just curious if we can do this exercise without it ?","Let Show that for all , is divisible by . My teacher solve this problem with induction, I am just curious if we can do this exercise without it ?",a=\dfrac{3+\sqrt{5}}{2}. n\in\mathbb N \lfloor a \lfloor an \rfloor \rfloor+n 3,['calculus']
85,Find volume between two spheres using cylindrical & spherical coordinates,Find volume between two spheres using cylindrical & spherical coordinates,,"I've got two spheres, one of which is the other sphere just shifted, and I'm trying to find the volume of the shared region. The spheres are $x^2 + y^2 +z^2 = 1$ and $x^2 + y^2 +(z-1)^2 = 1$ I know how to transform the variables into cylindrical and spherical coordinates but I'm having trouble figuring out the bounds. How do I do this? EDIT: Based on Kaladin's answer, which helped me realize the bounds for $r$, would it be correct to express the volume of the region as follows? (as cylindrical coordinates) $$V = 2\int_0^{2\pi} \int_{1/2}^1 \int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}} rdzdrd\theta$$ EDIT 2: Assuming I integrated the above integral properly, that equals $\frac{2\pi\sqrt{2}}{3}$, which is obviously not Kaladin's answer. What's the problem?","I've got two spheres, one of which is the other sphere just shifted, and I'm trying to find the volume of the shared region. The spheres are $x^2 + y^2 +z^2 = 1$ and $x^2 + y^2 +(z-1)^2 = 1$ I know how to transform the variables into cylindrical and spherical coordinates but I'm having trouble figuring out the bounds. How do I do this? EDIT: Based on Kaladin's answer, which helped me realize the bounds for $r$, would it be correct to express the volume of the region as follows? (as cylindrical coordinates) $$V = 2\int_0^{2\pi} \int_{1/2}^1 \int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}} rdzdrd\theta$$ EDIT 2: Assuming I integrated the above integral properly, that equals $\frac{2\pi\sqrt{2}}{3}$, which is obviously not Kaladin's answer. What's the problem?",,"['calculus', 'integration', 'multivariable-calculus', 'spherical-coordinates']"
86,nth derivative of a finite amount of composite functions,nth derivative of a finite amount of composite functions,,"I was curious to see whether or not there is a formula for the $n$th derivative of $k$ composite functions. If $F(x)=(f_1\circ f_2\circ...\circ f_k(x))$ then is there a formula for $$\frac{d^n} {dx^n}(F(x)).$$ For the $n$th derivative of two composite functions we use Faa di Bruno's rule, or $$\frac{d^n} {dx^n}(f(g(x)) =\sum \frac {n!} {m_1! 1!^{m_1} ... m_n! n!^{m_n}} \cdot f^{(m_1 + ... + m_n)} (g(x)) \prod_{i=1}^{n}(g^{(i)}(x))^{m_i}, $$ where the sum is over all the values of $m_1,...,m_n$ such that $m_1+2m_2+...+nm_n=n$. Also, for the first derivative of $k$ composite functions, user Yangzhe Lau gave me the formula $$F'(x)=\prod_{i=1}^{k}f'_i(f_{i+1}\circ f_{i+2} \circ \cdot \cdot\cdot\circ f_k(x)).$$ Therefore, is there a formula for that essentially combines these two, I suppose.","I was curious to see whether or not there is a formula for the $n$th derivative of $k$ composite functions. If $F(x)=(f_1\circ f_2\circ...\circ f_k(x))$ then is there a formula for $$\frac{d^n} {dx^n}(F(x)).$$ For the $n$th derivative of two composite functions we use Faa di Bruno's rule, or $$\frac{d^n} {dx^n}(f(g(x)) =\sum \frac {n!} {m_1! 1!^{m_1} ... m_n! n!^{m_n}} \cdot f^{(m_1 + ... + m_n)} (g(x)) \prod_{i=1}^{n}(g^{(i)}(x))^{m_i}, $$ where the sum is over all the values of $m_1,...,m_n$ such that $m_1+2m_2+...+nm_n=n$. Also, for the first derivative of $k$ composite functions, user Yangzhe Lau gave me the formula $$F'(x)=\prod_{i=1}^{k}f'_i(f_{i+1}\circ f_{i+2} \circ \cdot \cdot\cdot\circ f_k(x)).$$ Therefore, is there a formula for that essentially combines these two, I suppose.",,[]
87,"If I found that a series converges, how can I know to what number it's converging to?","If I found that a series converges, how can I know to what number it's converging to?",,"I started learning series in calculus and I have trouble catching a basic concept. When I try to find if a series converges or diverges I have many ways to go about it. If I see that the series diverges than I stop there. If I see that the series converges than there is a number it's converging to right? For example: $\sum\frac 2 {n^3+4}$. I do the limit comparison test with the series $\sum\frac 1 {n^3}$ and get a finite number $2$. I know that that $\sum\frac1{n^3}$ converges, so now I know that $\sum\frac2{n^3+4}$ converges also. How do I know to what number it converges to?","I started learning series in calculus and I have trouble catching a basic concept. When I try to find if a series converges or diverges I have many ways to go about it. If I see that the series diverges than I stop there. If I see that the series converges than there is a number it's converging to right? For example: $\sum\frac 2 {n^3+4}$. I do the limit comparison test with the series $\sum\frac 1 {n^3}$ and get a finite number $2$. I know that that $\sum\frac1{n^3}$ converges, so now I know that $\sum\frac2{n^3+4}$ converges also. How do I know to what number it converges to?",,"['calculus', 'sequences-and-series', 'limits']"
88,Why this limit is $-\frac{1}{4}$?,Why this limit is ?,-\frac{1}{4},"Find $$\lim_{x\rightarrow\infty} \bigg(1-x^{1/2x}\bigg)\cdot\frac{x}{2\ln{x}}$$ I tried this method: $$\begin{align} \bigg(1-x^{1/{2x}}\bigg)\frac{x}{2\ln{x}} & = \frac{x}{2}\frac{1-x^{1/{2x}}}{\ln{x}}\frac{1+x^{1/{2x}}}{1+x^{1/{2x}}} \\ & = \frac{1}{2} \frac{x}{\ln{x}} \frac{1-\sqrt[x]{x}}{1+\sqrt{\sqrt[x]{x}}} \\ & = \frac{1}{2 \cfrac{\ln{x}}{x} \cfrac{1+\sqrt{\sqrt[x]{x}}}{1-\sqrt[x]{x}}} \\ \end{align}$$ I know that whenever $x \to \infty$, so does $\frac{\ln{x}}{{x}}\to{0}$. Likewise, $\sqrt[x]{x}\to{1}$. If I apply L'Hopital to last altered equation, the fraction will blow off to infinity. My approach is not the best, can you help me?","Find $$\lim_{x\rightarrow\infty} \bigg(1-x^{1/2x}\bigg)\cdot\frac{x}{2\ln{x}}$$ I tried this method: $$\begin{align} \bigg(1-x^{1/{2x}}\bigg)\frac{x}{2\ln{x}} & = \frac{x}{2}\frac{1-x^{1/{2x}}}{\ln{x}}\frac{1+x^{1/{2x}}}{1+x^{1/{2x}}} \\ & = \frac{1}{2} \frac{x}{\ln{x}} \frac{1-\sqrt[x]{x}}{1+\sqrt{\sqrt[x]{x}}} \\ & = \frac{1}{2 \cfrac{\ln{x}}{x} \cfrac{1+\sqrt{\sqrt[x]{x}}}{1-\sqrt[x]{x}}} \\ \end{align}$$ I know that whenever $x \to \infty$, so does $\frac{\ln{x}}{{x}}\to{0}$. Likewise, $\sqrt[x]{x}\to{1}$. If I apply L'Hopital to last altered equation, the fraction will blow off to infinity. My approach is not the best, can you help me?",,"['calculus', 'limits']"
89,Why Not Define $0/0$ To Be $0$?,Why Not Define  To Be ?,0/0 0,"For every number $x$, $x\times 0=0$, hence $\dfrac{0}{0}$ can be any number!  So $\dfrac{0}{0}$ ""is knows as indeterminate"" [1].  But what if we define it to be $0$? I already have an answer, but don't know how convincing it is: $1=\dfrac{1}{1}=\dfrac{1}{1}+0=\dfrac{1}{1}+\dfrac{0}{0}=\dfrac{1\times 0}{1\times 0}+\dfrac{0\times 1}{0\times 1}=\dfrac{0}{0}+\dfrac{0}{0}=0+0=0$, a contradiction. Is there any better explanation why not to define $\dfrac{0}{0}$ to be $0$ (or any other number)? Thanks. [1] http://en.wikipedia.org/wiki/Division_by_zero#In_algebra","For every number $x$, $x\times 0=0$, hence $\dfrac{0}{0}$ can be any number!  So $\dfrac{0}{0}$ ""is knows as indeterminate"" [1].  But what if we define it to be $0$? I already have an answer, but don't know how convincing it is: $1=\dfrac{1}{1}=\dfrac{1}{1}+0=\dfrac{1}{1}+\dfrac{0}{0}=\dfrac{1\times 0}{1\times 0}+\dfrac{0\times 1}{0\times 1}=\dfrac{0}{0}+\dfrac{0}{0}=0+0=0$, a contradiction. Is there any better explanation why not to define $\dfrac{0}{0}$ to be $0$ (or any other number)? Thanks. [1] http://en.wikipedia.org/wiki/Division_by_zero#In_algebra",,"['calculus', 'soft-question', 'arithmetic', 'fractions', 'alternative-proof']"
90,finding examples for a non negative and continuous function for which the infinite integral is finite but the limit at infinity doesn't exist,finding examples for a non negative and continuous function for which the infinite integral is finite but the limit at infinity doesn't exist,,"Question: a. Find an example for a non-negative and continuous function s.t. $\int _0^\infty f(x)dx$ is finite but the following limit doesn't exist: $\lim_{x\to \infty} f(x)$. b. Is it possible that $\int _0^\infty f(x)dx$ is bounded but $f(x)$ is not bounded? What we did with A: We suggested the function $|sin(x^2)|$ which has periods that get smaller and smaller until they no longer imply on the sum. But since it's a trig function it doesn't have a determinate limit. Wolfram didn't have the integral value for that func, and we were wondering if it really converges and if it is really a good example. What we did with B: We thought about the function: $f(x)=  { x\in \Bbb N },{e^{-x} \notin \Bbb N }$ and we had a disagreement whether f(x) is integrable. I said no because similarly to Dirichlet function, one sum's limit will be 0 while the other one's will be infinite. My partner said that if I find a $\delta<1$ then the definition of Riemann's integral does hold and so this integral is equal to that of $f(x)=e^{-x}$ She was trying to say this functions integral will be 0, but still it won't be bounded. I disagreed. Who's right?","Question: a. Find an example for a non-negative and continuous function s.t. $\int _0^\infty f(x)dx$ is finite but the following limit doesn't exist: $\lim_{x\to \infty} f(x)$. b. Is it possible that $\int _0^\infty f(x)dx$ is bounded but $f(x)$ is not bounded? What we did with A: We suggested the function $|sin(x^2)|$ which has periods that get smaller and smaller until they no longer imply on the sum. But since it's a trig function it doesn't have a determinate limit. Wolfram didn't have the integral value for that func, and we were wondering if it really converges and if it is really a good example. What we did with B: We thought about the function: $f(x)=  { x\in \Bbb N },{e^{-x} \notin \Bbb N }$ and we had a disagreement whether f(x) is integrable. I said no because similarly to Dirichlet function, one sum's limit will be 0 while the other one's will be infinite. My partner said that if I find a $\delta<1$ then the definition of Riemann's integral does hold and so this integral is equal to that of $f(x)=e^{-x}$ She was trying to say this functions integral will be 0, but still it won't be bounded. I disagreed. Who's right?",,['calculus']
91,How to solve the limit $\lim_{n \to \infty} \sqrt[8]{n^2+1} - \sqrt[4]{n+1}$?,How to solve the limit ?,\lim_{n \to \infty} \sqrt[8]{n^2+1} - \sqrt[4]{n+1},"How do I show that this limit is zero? $$\lim_{n \to \infty} \sqrt[8]{n^2+1} - \sqrt[4]{n+1} = 0$$ I've done the multiply by conjugates thing, which seems to lead nowhere: $$\lim_{n \to \infty} (\sqrt[8]{n^2+1} - \sqrt[4]{n+1}) (\frac{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}}{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}})$$ $$=$$ $$\lim_{n \to \infty} \frac{\sqrt[4]{n^2+1} + \sqrt[2]{n+1}}{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}}$$ I also considered using the squeeze theorem, as $\sqrt[8]{n^2+1} - \sqrt[4]{n+1} \leq \sqrt[8]{n^2+1} - \sqrt[4]{n}$, but I'm not sure how to bound it from below. What's the correct approach to solving this limit?","How do I show that this limit is zero? $$\lim_{n \to \infty} \sqrt[8]{n^2+1} - \sqrt[4]{n+1} = 0$$ I've done the multiply by conjugates thing, which seems to lead nowhere: $$\lim_{n \to \infty} (\sqrt[8]{n^2+1} - \sqrt[4]{n+1}) (\frac{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}}{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}})$$ $$=$$ $$\lim_{n \to \infty} \frac{\sqrt[4]{n^2+1} + \sqrt[2]{n+1}}{\sqrt[8]{n^2+1} + \sqrt[4]{n+1}}$$ I also considered using the squeeze theorem, as $\sqrt[8]{n^2+1} - \sqrt[4]{n+1} \leq \sqrt[8]{n^2+1} - \sqrt[4]{n}$, but I'm not sure how to bound it from below. What's the correct approach to solving this limit?",,"['calculus', 'analysis', 'limits', 'radicals']"
92,A (not so?) simple calculus problem,A (not so?) simple calculus problem,,"I'm the teaching assistant for a first semester calculus course, and the professor has given the students the following problem: Find the points on the curve $xy=\sin(x+y)$ that have a vertical tangent line. Here is a picture of the curve: My attempt to solve this problem led me to finding points on the given curve which also satisfy $x=\cos(x+y)$, but I can't figure out how to simultaneously solve these equations (without resulting to numerical methods, which the students are not assumed to know).  Is there something I'm missing, or has the professor given the students a problem more difficult than he intended? Edit: I'm still looking for a solution not requiring numerical methods, or proof that no such solution exists.","I'm the teaching assistant for a first semester calculus course, and the professor has given the students the following problem: Find the points on the curve $xy=\sin(x+y)$ that have a vertical tangent line. Here is a picture of the curve: My attempt to solve this problem led me to finding points on the given curve which also satisfy $x=\cos(x+y)$, but I can't figure out how to simultaneously solve these equations (without resulting to numerical methods, which the students are not assumed to know).  Is there something I'm missing, or has the professor given the students a problem more difficult than he intended? Edit: I'm still looking for a solution not requiring numerical methods, or proof that no such solution exists.",,"['calculus', 'derivatives']"
93,Seeking formal explanation of definite integral over infinitesimal interval,Seeking formal explanation of definite integral over infinitesimal interval,,"Why does : $$\frac{\displaystyle\int_t^{t+h}f(s) \, ds}{h} = f(t)\text{ as }h \rightarrow 0\text{ ?}$$ Intuitively this makes sense, because the value of the integral is infinitesimally close to $f(t)$, you have $h$ of them, and you divide the result by $h$.  However, I'd like a more formal explanation if possible.","Why does : $$\frac{\displaystyle\int_t^{t+h}f(s) \, ds}{h} = f(t)\text{ as }h \rightarrow 0\text{ ?}$$ Intuitively this makes sense, because the value of the integral is infinitesimally close to $f(t)$, you have $h$ of them, and you divide the result by $h$.  However, I'd like a more formal explanation if possible.",,['calculus']
94,Making both bounds of integration zero,Making both bounds of integration zero,,"I came across a question while evaluating the integral: $$ \int_{0}^{\pi}\frac{\cos{t}}{1+9\sin^2{t}}\, dt $$ If you substitute $u=3\sin{t}$, you get: $$ \int_{0}^{0}\frac{1}{3+3u^2}\, du $$ Which evaluates to zero because(?) the bounds are both zero. But then, can't you substitute any arbitrary expression to change both bounds to zero -- making the value zero? So in evaluating: $$\int_{0}^{1}x\,dx$$ We could substitute $u = x^2-x$ or some trigonometric expression to change both bounds to zero. Clearly there is a misstep here, but which part of this substitution is invalid?","I came across a question while evaluating the integral: $$ \int_{0}^{\pi}\frac{\cos{t}}{1+9\sin^2{t}}\, dt $$ If you substitute $u=3\sin{t}$, you get: $$ \int_{0}^{0}\frac{1}{3+3u^2}\, du $$ Which evaluates to zero because(?) the bounds are both zero. But then, can't you substitute any arbitrary expression to change both bounds to zero -- making the value zero? So in evaluating: $$\int_{0}^{1}x\,dx$$ We could substitute $u = x^2-x$ or some trigonometric expression to change both bounds to zero. Clearly there is a misstep here, but which part of this substitution is invalid?",,"['calculus', 'integration']"
95,convergence of a series involving $x^\sqrt{n}$,convergence of a series involving,x^\sqrt{n},"I was trying to prove the convergence of the series $\sum_{n=1}^{\infty}x^{\sqrt{n}}$, for $0<x<1$. Unfortunately, I could not make one of the standard convergence tests give me an answer. Does anybody of you have a suggestion? any help is much appreciated! many thanks!","I was trying to prove the convergence of the series $\sum_{n=1}^{\infty}x^{\sqrt{n}}$, for $0<x<1$. Unfortunately, I could not make one of the standard convergence tests give me an answer. Does anybody of you have a suggestion? any help is much appreciated! many thanks!",,['calculus']
96,Prove identity concerning successive derivative of $e^{x^2/2a}$,Prove identity concerning successive derivative of,e^{x^2/2a},Prove the following identity: \begin{equation} \left[\frac{d^{2n}}{dx^{2n}}e^{x^2/2a}\right]_{x=0}=\frac{(2n-1)!!}{a^n} \end{equation} The final derivative must be evaluated at $x=0$.,Prove the following identity: \begin{equation} \left[\frac{d^{2n}}{dx^{2n}}e^{x^2/2a}\right]_{x=0}=\frac{(2n-1)!!}{a^n} \end{equation} The final derivative must be evaluated at $x=0$.,,"['calculus', 'derivatives']"
97,Proving that $\displaystyle \int_0^x \frac{\sin t}{t+1}dt > 0$ for all $x >0$,Proving that  for all,\displaystyle \int_0^x \frac{\sin t}{t+1}dt > 0 x >0,"I devised this proof that $$\tag{1} \int_0^x \frac{\sin t}{t+1}dt > 0 \text{ ; } \forall x >0$$ The idea is to prove that the area from $(0,\pi)$ is greater than the absolute value of the negative area in $(\pi, 2\pi)$, and so on, so that the final area is always positive. $f(x) = \dfrac{\sin x}{x+1}$ is positive if $\sin x >0$ and negative if $\sin x <0$. This is to say $$f(x) >0 \Leftrightarrow x \in \bigcup_{k=0}^{\infty}(2k\pi,(2k+1)\pi)$$ $$f(x) <0 \Leftrightarrow x \in \bigcup_{k=1}^{\infty}((2k-1)\pi,2k\pi)$$ If we prove that $$\tag{2} |f(x)| > |f(x+\pi)|$$ for every $x$ then we prove $(1)$. But, $|f(x)| =\left| \dfrac{\sin x}{x+1} \right|$ $|f(x+\pi)| =\left| \dfrac{\sin (x+\pi)}{x+\pi+1} \right|=\left| \dfrac{\sin x}{x+\pi+1} \right|$ Thus $(2)$ is proven, and we then have that in general, $$ |f(x+n \pi)| > |f(x+(n+1) \pi)|$$, thus $$\int\limits_{\left( {2k} \right)\pi }^{\left( {2k + 1} \right)\pi } {\frac{{\sin t}}{{t + 1}}dt}  + \int\limits_{\left( {2k + 1} \right)\pi }^{\left( {2k + 2} \right)\pi } {\frac{{\sin t}}{{t + 1}}dt}  > 0$$ and then $$ \int_0^x \frac{\sin t}{t+1}dt > 0 \text{ ; } \forall x >0$$ Is it right? And if it is right - is it understandable?","I devised this proof that $$\tag{1} \int_0^x \frac{\sin t}{t+1}dt > 0 \text{ ; } \forall x >0$$ The idea is to prove that the area from $(0,\pi)$ is greater than the absolute value of the negative area in $(\pi, 2\pi)$, and so on, so that the final area is always positive. $f(x) = \dfrac{\sin x}{x+1}$ is positive if $\sin x >0$ and negative if $\sin x <0$. This is to say $$f(x) >0 \Leftrightarrow x \in \bigcup_{k=0}^{\infty}(2k\pi,(2k+1)\pi)$$ $$f(x) <0 \Leftrightarrow x \in \bigcup_{k=1}^{\infty}((2k-1)\pi,2k\pi)$$ If we prove that $$\tag{2} |f(x)| > |f(x+\pi)|$$ for every $x$ then we prove $(1)$. But, $|f(x)| =\left| \dfrac{\sin x}{x+1} \right|$ $|f(x+\pi)| =\left| \dfrac{\sin (x+\pi)}{x+\pi+1} \right|=\left| \dfrac{\sin x}{x+\pi+1} \right|$ Thus $(2)$ is proven, and we then have that in general, $$ |f(x+n \pi)| > |f(x+(n+1) \pi)|$$, thus $$\int\limits_{\left( {2k} \right)\pi }^{\left( {2k + 1} \right)\pi } {\frac{{\sin t}}{{t + 1}}dt}  + \int\limits_{\left( {2k + 1} \right)\pi }^{\left( {2k + 2} \right)\pi } {\frac{{\sin t}}{{t + 1}}dt}  > 0$$ and then $$ \int_0^x \frac{\sin t}{t+1}dt > 0 \text{ ; } \forall x >0$$ Is it right? And if it is right - is it understandable?",,"['calculus', 'proof-writing']"
98,Why do we treat $\mathrm{d}x$ in the indefinite integral as if $f(x)$ were multiplied by it?,Why do we treat  in the indefinite integral as if  were multiplied by it?,\mathrm{d}x f(x),"The first explanation I heard for the $\mathrm{d}x$ - it just shows by which variable we are integrating. Which made sense because $(F(x)+C)^\prime=f(x)$, not $f(x)\mathrm{d}x$. Now, some time later, the $\mathrm{d}x$ has become a source of confusion again. If there's $\int \frac{x}{x^2+1} \mathrm{d}x$, then why can we solve it like that: $\int \frac{1}{x^2+1} x \mathrm{d}x= \frac{1}{2}\int\frac{1}{x^2+1} 2 x \mathrm{d} x=\frac{1}{2}\int \frac{1}{x^2+1} \mathrm{d}(x^2+1)$ ? The other parts seem more or less normal but the transition from $\int\frac{x}{x^2+1} \mathrm{d}x$ to $\int \frac{1}{x^2+1} x \mathrm{d}x$ seems very strange. It works but why does it? If $\mathrm{d}x$ just shows by which variable we are integrating $f(x)$ then we cannot treat it as if $f(x)$ were multiplied by it. And on the other hand, if $f(x)$ IS actually multiplied by $\mathrm{d}x$ then why can we do it? I know there's simple explanation for it when we calculate the definite integral, that we break up some line or surface or volume into infinitely little pieces and then add up those infinitely little pieces to get the whole thing, so it makes sense. But why do we treat $\mathrm{d}x$ in the indefinite integral as if $f(x)$ were multiplied by it? Thanks.","The first explanation I heard for the $\mathrm{d}x$ - it just shows by which variable we are integrating. Which made sense because $(F(x)+C)^\prime=f(x)$, not $f(x)\mathrm{d}x$. Now, some time later, the $\mathrm{d}x$ has become a source of confusion again. If there's $\int \frac{x}{x^2+1} \mathrm{d}x$, then why can we solve it like that: $\int \frac{1}{x^2+1} x \mathrm{d}x= \frac{1}{2}\int\frac{1}{x^2+1} 2 x \mathrm{d} x=\frac{1}{2}\int \frac{1}{x^2+1} \mathrm{d}(x^2+1)$ ? The other parts seem more or less normal but the transition from $\int\frac{x}{x^2+1} \mathrm{d}x$ to $\int \frac{1}{x^2+1} x \mathrm{d}x$ seems very strange. It works but why does it? If $\mathrm{d}x$ just shows by which variable we are integrating $f(x)$ then we cannot treat it as if $f(x)$ were multiplied by it. And on the other hand, if $f(x)$ IS actually multiplied by $\mathrm{d}x$ then why can we do it? I know there's simple explanation for it when we calculate the definite integral, that we break up some line or surface or volume into infinitely little pieces and then add up those infinitely little pieces to get the whole thing, so it makes sense. But why do we treat $\mathrm{d}x$ in the indefinite integral as if $f(x)$ were multiplied by it? Thanks.",,"['calculus', 'integration', 'notation']"
99,Proving $\sum_{n=1}^{\infty}\binom{2n}{n}^2\frac{4H_{2n}-3H_n}{n2^{4n}}=\zeta(2)$,Proving,\sum_{n=1}^{\infty}\binom{2n}{n}^2\frac{4H_{2n}-3H_n}{n2^{4n}}=\zeta(2),"While trying to solve Prove $\int_{0}^{1}\frac1k K(k)\ln\left[\frac{\left(1+k \right)^3}{1-k} \right]\text{d}k=\frac{\pi^3}{4}$ I was able to reduce it to the following form, $$\sum_{n=1}^{\infty}\binom{2n}{n}^2\frac{4H_{2n}-3H_n}{n2^{4n}}=\zeta(2)$$ Where, $H_n$ is the $n$ th Harmonic Number. This should be correct if I did all the steps correctly, but I am unable to prove this further. I tried searching more about series of this form which have the Central Binomial Coefficient Squared with Harmonic Numbers but was not able to find any. Any help would be appreciated. EDIT: Considering, $$M(x)=\int_{0}^{x}\left(\frac{2K\left(\sqrt{s}\right)-\pi}{s}\right)ds$$ where $K(k)$ is the Complete Elliptical Integral of the First Kind. Then the question is reducible to, $$\int_{0}^{1}\frac{M\left(1\right)+3M\left(x\right)-4M\left(x^{2}\right)}{1-x}dx=\frac{\pi^3}{6}$$ Though no idea comes to mind after this. Here are some related results: $$M(1)=4\pi\ln2-8G$$ $$\int_{0}^{1}M\left(x\right)dx=-8G-4+\pi+4\pi\ln2$$ where $G$ is Catalan's Constant. EDIT: Let, $$A=\Im \operatorname{Li_{3}}\left(\frac{1+i}{2}\right)$$ where $\operatorname{Li_{3}}(.)$ is the trilogarithm. and $G$ be the Catalan's Constant. Using the Twin Integrals Mentioned in the Post at the Beginning, was able to arrive at these. $$\sum_{r=1}^{\infty}\binom{2r}{r}^2\frac{H_{2n}}{n2^{4n}}=24\frac{G\ln2}{\pi}+48\frac{A}{\pi}-\frac{3}{2}\ln\left(2\right)^{2}-\frac{29}{24}\pi^{2}$$ $$\sum_{r=1}^{\infty}\binom{2r}{r}^2\frac{H_{n}}{n2^{4n}}=32\frac{G\ln2}{\pi}+64\frac{A}{\pi}-2\ln\left(2\right)^{2}-\frac{5}{3}\pi^{2}$$","While trying to solve Prove I was able to reduce it to the following form, Where, is the th Harmonic Number. This should be correct if I did all the steps correctly, but I am unable to prove this further. I tried searching more about series of this form which have the Central Binomial Coefficient Squared with Harmonic Numbers but was not able to find any. Any help would be appreciated. EDIT: Considering, where is the Complete Elliptical Integral of the First Kind. Then the question is reducible to, Though no idea comes to mind after this. Here are some related results: where is Catalan's Constant. EDIT: Let, where is the trilogarithm. and be the Catalan's Constant. Using the Twin Integrals Mentioned in the Post at the Beginning, was able to arrive at these.",\int_{0}^{1}\frac1k K(k)\ln\left[\frac{\left(1+k \right)^3}{1-k} \right]\text{d}k=\frac{\pi^3}{4} \sum_{n=1}^{\infty}\binom{2n}{n}^2\frac{4H_{2n}-3H_n}{n2^{4n}}=\zeta(2) H_n n M(x)=\int_{0}^{x}\left(\frac{2K\left(\sqrt{s}\right)-\pi}{s}\right)ds K(k) \int_{0}^{1}\frac{M\left(1\right)+3M\left(x\right)-4M\left(x^{2}\right)}{1-x}dx=\frac{\pi^3}{6} M(1)=4\pi\ln2-8G \int_{0}^{1}M\left(x\right)dx=-8G-4+\pi+4\pi\ln2 G A=\Im \operatorname{Li_{3}}\left(\frac{1+i}{2}\right) \operatorname{Li_{3}}(.) G \sum_{r=1}^{\infty}\binom{2r}{r}^2\frac{H_{2n}}{n2^{4n}}=24\frac{G\ln2}{\pi}+48\frac{A}{\pi}-\frac{3}{2}\ln\left(2\right)^{2}-\frac{29}{24}\pi^{2} \sum_{r=1}^{\infty}\binom{2r}{r}^2\frac{H_{n}}{n2^{4n}}=32\frac{G\ln2}{\pi}+64\frac{A}{\pi}-2\ln\left(2\right)^{2}-\frac{5}{3}\pi^{2},"['calculus', 'integration', 'summation', 'binomial-coefficients', 'harmonic-numbers']"
