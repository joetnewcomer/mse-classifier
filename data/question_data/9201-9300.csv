,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How is an integral with respect to a Hausdorff measure defined?,How is an integral with respect to a Hausdorff measure defined?,,"In a reply by Corey : For integrals of scalar-valued functions on unoriented subsets of $\mathbb{R}^n$, one can use the Lebesgue integral with respect to $k$-dimensional Hausdorff measure $\mathcal{H}^k$. The line integral of a scalar function $f$ over a curve $C$ in $\mathbb{R}^3$ is then: $$ \int_C f \, ds = \int_{\mathbb{R}^3} f \, d\mathcal{H}^1,$$ where I assume that $f$ is defined to be 0 off of $C$. A Hausdorff measure is an outer measure on the power set of a metric space induced from the metric. I know how an integral wrt a measure is defined, but I wonder how an integral wrt a Hausdorff measure is defined? Or more generally, how is an integral wrt an outer measure defined, if it exists? Or is it an integral because $\mathbb{R}^3$ is measurable wrt the Hausdorff measure, and the Hausdorff measure is a measure on the set of subsets that are measurable wrt it? Thanks and regards!","In a reply by Corey : For integrals of scalar-valued functions on unoriented subsets of $\mathbb{R}^n$, one can use the Lebesgue integral with respect to $k$-dimensional Hausdorff measure $\mathcal{H}^k$. The line integral of a scalar function $f$ over a curve $C$ in $\mathbb{R}^3$ is then: $$ \int_C f \, ds = \int_{\mathbb{R}^3} f \, d\mathcal{H}^1,$$ where I assume that $f$ is defined to be 0 off of $C$. A Hausdorff measure is an outer measure on the power set of a metric space induced from the metric. I know how an integral wrt a measure is defined, but I wonder how an integral wrt a Hausdorff measure is defined? Or more generally, how is an integral wrt an outer measure defined, if it exists? Or is it an integral because $\mathbb{R}^3$ is measurable wrt the Hausdorff measure, and the Hausdorff measure is a measure on the set of subsets that are measurable wrt it? Thanks and regards!",,"['real-analysis', 'measure-theory', 'functional-analysis', 'metric-spaces']"
1,Convergence of a sequence with lots of powers,Convergence of a sequence with lots of powers,,"A problem from ""Problems in Real Analysis - Advanced Calculus on the Real Axis"": Let $p$ be a nonnegative real number. Study the convergence of the sequence $(x_n)_{n\ge1}$ defined by $$x_n=\left(1^{1^p}\cdot2^{2^p}\cdots(n+1)^{(n+1)^p}\right)^{1/(n+1)^{p+1}}-\left(1^{1^p}\cdot2^{2^p}\cdots n^{n^p}\right)^{1/n^{p+1}}.$$ If $p=0$, it is already proved in the book that $x_n$ converges to $\dfrac1e$. So just consider the case that $p>0$. If the sequence converges, I can prove by Stolz-Cesàro Theorem that the sequence must converge to $0$. But I don't know how to determine which $p$ gives convergent sequences. Please don't give complete solution, just helpful hints. Thanks in advance.","A problem from ""Problems in Real Analysis - Advanced Calculus on the Real Axis"": Let $p$ be a nonnegative real number. Study the convergence of the sequence $(x_n)_{n\ge1}$ defined by $$x_n=\left(1^{1^p}\cdot2^{2^p}\cdots(n+1)^{(n+1)^p}\right)^{1/(n+1)^{p+1}}-\left(1^{1^p}\cdot2^{2^p}\cdots n^{n^p}\right)^{1/n^{p+1}}.$$ If $p=0$, it is already proved in the book that $x_n$ converges to $\dfrac1e$. So just consider the case that $p>0$. If the sequence converges, I can prove by Stolz-Cesàro Theorem that the sequence must converge to $0$. But I don't know how to determine which $p$ gives convergent sequences. Please don't give complete solution, just helpful hints. Thanks in advance.",,"['real-analysis', 'convergence-divergence']"
2,"Organizing types of functions by their calculus-related properties, in diagram form?","Organizing types of functions by their calculus-related properties, in diagram form?",,"Does anyone know of a diagram that displays and organizes categories of functions according to their calculus-related properties (e.g. continuous, $C^\infty$, degrees of differentiability and integrability; not so much things like even/odd, one-to-one)?  Something along the lines of what this diagram does for complex numbers. [The original of this (and more) can be found here .] I would be grateful if you could direct me to any good resources that categorize types of functions in a systematic and succinct manner.  Illuminating examples of the different types of functions (e.g. Weierstrass's continuous-everywhere-but-differentiable-nowhere function) and schematic clarity would be pluses. Let me know if you need more information.  Thanks! Edit : I've look around more on this site at related questions (notably Are the smooth functions dense in either $\mathcal L_2$ or $\mathcal L_1$? and what is the cardinality of set of all smooth functions in $L^1$? ) and found them intriguing and somewhat helpful. I could really use help putting all of these and many other pieces together, though.  Any takers?","Does anyone know of a diagram that displays and organizes categories of functions according to their calculus-related properties (e.g. continuous, $C^\infty$, degrees of differentiability and integrability; not so much things like even/odd, one-to-one)?  Something along the lines of what this diagram does for complex numbers. [The original of this (and more) can be found here .] I would be grateful if you could direct me to any good resources that categorize types of functions in a systematic and succinct manner.  Illuminating examples of the different types of functions (e.g. Weierstrass's continuous-everywhere-but-differentiable-nowhere function) and schematic clarity would be pluses. Let me know if you need more information.  Thanks! Edit : I've look around more on this site at related questions (notably Are the smooth functions dense in either $\mathcal L_2$ or $\mathcal L_1$? and what is the cardinality of set of all smooth functions in $L^1$? ) and found them intriguing and somewhat helpful. I could really use help putting all of these and many other pieces together, though.  Any takers?",,"['calculus', 'real-analysis', 'reference-request', 'complex-analysis', 'functions']"
3,Theorem on behaviour of real continuous functions on the integers,Theorem on behaviour of real continuous functions on the integers,,"I tried (and still try) to prove that $\sin (\log n)$ doesn't have a limit at $\infty$. I know it is enough to show that a subsequence of $\log n$ approaches, modulo $2\pi$, arbitrarily close to 2 distinct values $\alpha, \beta$ (such that $\sin \alpha \neq \sin \beta$), which is a much weaker statement than ""$\ \frac{\log n}{2\pi}$ is equidistributed modulo $1$"" (which is even false...). My questions are: How do you prove my specific case? Is there a general theory of limit of $f \circ g (n)$ where $f$ is a periodic continuous function and $g$ is continuous, (possibly monotone increasing) function diverging to $\infty$ at $\infty$? What is a good source about equidistribution?","I tried (and still try) to prove that $\sin (\log n)$ doesn't have a limit at $\infty$. I know it is enough to show that a subsequence of $\log n$ approaches, modulo $2\pi$, arbitrarily close to 2 distinct values $\alpha, \beta$ (such that $\sin \alpha \neq \sin \beta$), which is a much weaker statement than ""$\ \frac{\log n}{2\pi}$ is equidistributed modulo $1$"" (which is even false...). My questions are: How do you prove my specific case? Is there a general theory of limit of $f \circ g (n)$ where $f$ is a periodic continuous function and $g$ is continuous, (possibly monotone increasing) function diverging to $\infty$ at $\infty$? What is a good source about equidistribution?",,"['real-analysis', 'sequences-and-series', 'equidistribution']"
4,Modified Dirichlet function non-differentiability,Modified Dirichlet function non-differentiability,,"I need some ideas to start with this problem. Show that the modified Dirichlet function defined as $D_M(x)=\begin{cases}0&\mbox{if }x \notin \mathbb{Q} , \\ \frac{1}{b}&\mbox{for } x = \frac{a}{b} \mbox {with}\gcd(a,b) = 1\;.\end{cases}$ Is not differentiable for any $x_0 \in(c,d)\subset\mathbb{R}$","I need some ideas to start with this problem. Show that the modified Dirichlet function defined as $D_M(x)=\begin{cases}0&\mbox{if }x \notin \mathbb{Q} , \\ \frac{1}{b}&\mbox{for } x = \frac{a}{b} \mbox {with}\gcd(a,b) = 1\;.\end{cases}$ Is not differentiable for any $x_0 \in(c,d)\subset\mathbb{R}$",,"['calculus', 'real-analysis']"
5,Characterization of convergence in measure,Characterization of convergence in measure,,"Prove that $f_n\to f$ in measure on $E$ if and only if given $\varepsilon>0$, there exists $K$ such that |{$x\in E : |f(x)-f_k(x)|>\varepsilon$}|$<\varepsilon$ for $k\ge K$. The ""only if"" direction of this is immediate from the definition of convergence in measure, but the other direction is less obvious to me. Conversely, we suppose that given $\varepsilon >0$, there is a $K$ such that |{$x\in E : |f(x)-f_k(x)|>\varepsilon$}|$<\varepsilon$ for $k\ge K$. My initial thought was to bound the measure of the set in question by, say, $\frac{1}{k}$. But I'm not sure I can do that because $\varepsilon$ not only bounds the measure of the set, but the set also depends on the choice of $\varepsilon$. To show something convergence in measure, I need to show that for every $\varepsilon$ the limit as $k\to\infty$ of the measures of those sets is zero... [Subquestion: is the use of |$\cdot$| standard for denoting Lebesgue measure? I had never seen it until this course. I had always seen $m(-)$.] [Sub-subquestion: is there any particular reason that set brackets don't display in math mode? The commands \ { and \ } didn't do anything...]","Prove that $f_n\to f$ in measure on $E$ if and only if given $\varepsilon>0$, there exists $K$ such that |{$x\in E : |f(x)-f_k(x)|>\varepsilon$}|$<\varepsilon$ for $k\ge K$. The ""only if"" direction of this is immediate from the definition of convergence in measure, but the other direction is less obvious to me. Conversely, we suppose that given $\varepsilon >0$, there is a $K$ such that |{$x\in E : |f(x)-f_k(x)|>\varepsilon$}|$<\varepsilon$ for $k\ge K$. My initial thought was to bound the measure of the set in question by, say, $\frac{1}{k}$. But I'm not sure I can do that because $\varepsilon$ not only bounds the measure of the set, but the set also depends on the choice of $\varepsilon$. To show something convergence in measure, I need to show that for every $\varepsilon$ the limit as $k\to\infty$ of the measures of those sets is zero... [Subquestion: is the use of |$\cdot$| standard for denoting Lebesgue measure? I had never seen it until this course. I had always seen $m(-)$.] [Sub-subquestion: is there any particular reason that set brackets don't display in math mode? The commands \ { and \ } didn't do anything...]",,"['real-analysis', 'measure-theory']"
6,Direct proof that f(x)=x sin(1/x) does not satisfy Lusin N condition,Direct proof that f(x)=x sin(1/x) does not satisfy Lusin N condition,,Let $f$ be defined as $$f(x)=\begin{cases} x \sin(\frac{1}{x}) & x\ne 0 \\ 0 & x=0 \end{cases}$$ $f(x)$  is not absolutely continuous so it cannot might not satisfy the Lusin N condition. Is there a direct proof of that it does not?  i.e. I wanted to know how to construct a set of zero measure which does not satisfy Lusin's Condition for this function.,Let $f$ be defined as $$f(x)=\begin{cases} x \sin(\frac{1}{x}) & x\ne 0 \\ 0 & x=0 \end{cases}$$ $f(x)$  is not absolutely continuous so it cannot might not satisfy the Lusin N condition. Is there a direct proof of that it does not?  i.e. I wanted to know how to construct a set of zero measure which does not satisfy Lusin's Condition for this function.,,['real-analysis']
7,How to construct a nonzero real number between two given nonzero real numbers?,How to construct a nonzero real number between two given nonzero real numbers?,,"Statement: Let $$X=$$ $$\{(a,b) \in \mathbb{R} \setminus \{0\} \times \mathbb{R}\setminus \{0\}:a<b\}$$ There exists a function $f:X \rightarrow \mathbb{R} \setminus \{0\}$ such that for all $(a,b) \in X$ , $|f(a,b)-b| \le \frac{3}{4}|a-b|$ and $|f(a,b)-a| \le \frac{3}{4}|a-b|$ . I proved this statement constructively without using the axiom of countable choice, but for doing that, I assumed real numbers are regular sequences of rational numbers (according to Bishop). The idea of the proof is that you construct a rational number $q$ such that $|q-b| < \frac{3}{4}|a-b|$ and $|q-a| < \frac{3}{4}|a-b|$ . If $q=0$ then put $f(a,b)=\frac{\frac{a+b}{2}+b}{2}$ and if $\neg(q=0)$ then put $f(a,b)=q$ . Now I wonder how can we prove this statement by considering real numbers as Dedekind cuts of rational numbers? i.e if we consider real numbers as Dedekind cuts, can we prove this statement constructively without using the axiom of countable choice? The proof that I mentioned doesn't go well if reals are Dedekind reals, because we cannot easily construct the desired rational number, if we restrict ourselves to not use the axiom of countable choice. Can anyone help me with this? Thank you.","Statement: Let There exists a function such that for all , and . I proved this statement constructively without using the axiom of countable choice, but for doing that, I assumed real numbers are regular sequences of rational numbers (according to Bishop). The idea of the proof is that you construct a rational number such that and . If then put and if then put . Now I wonder how can we prove this statement by considering real numbers as Dedekind cuts of rational numbers? i.e if we consider real numbers as Dedekind cuts, can we prove this statement constructively without using the axiom of countable choice? The proof that I mentioned doesn't go well if reals are Dedekind reals, because we cannot easily construct the desired rational number, if we restrict ourselves to not use the axiom of countable choice. Can anyone help me with this? Thank you.","X= \{(a,b) \in \mathbb{R} \setminus \{0\} \times \mathbb{R}\setminus \{0\}:a<b\} f:X \rightarrow \mathbb{R} \setminus \{0\} (a,b) \in X |f(a,b)-b| \le \frac{3}{4}|a-b| |f(a,b)-a| \le \frac{3}{4}|a-b| q |q-b| < \frac{3}{4}|a-b| |q-a| < \frac{3}{4}|a-b| q=0 f(a,b)=\frac{\frac{a+b}{2}+b}{2} \neg(q=0) f(a,b)=q","['real-analysis', 'real-numbers', 'rational-numbers', 'axiom-of-choice', 'constructive-mathematics']"
8,$\lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2} + \cdots + \frac{n + n}{n^2 + n} \right)$ [duplicate],[duplicate],\lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2} + \cdots + \frac{n + n}{n^2 + n} \right),This question already has answers here : How to prove $ \lim_{n\to\infty}\sum_{k=1}^{n}\frac{n+k}{n^2+k}=\frac{3}{2}$? (4 answers) Closed 4 months ago . I want to find $$\lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2}  + \cdots + \frac{n + n}{n^2 + n} \right)$$ Here is my solution. $$\lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2}  + \cdots + \frac{n + n}{n^2 + n} \right) = \lim_{n \rightarrow \infty} \left(\frac{n}{n^2 + 1} + \frac{n}{n^2 + 2}  + \cdots + \frac{n}{n^2 + n} \right) + \left(\frac{1}{n^2 + 1} + \frac{2}{n^2 + 2}  + \cdots + \frac{n}{n^2 + n} \right)$$ Lets call the first sum $S^1_n$ and the second sum $S^2_n$ . Then we have: $$1 \leftarrow n \frac{n}{n^2 + n} \leq S^1_n \leq n \frac{n}{n^2 + 1} \rightarrow 1$$ And by the sandwich criterium $S^1_n \rightarrow 1$ . On the other hand we have: $$ \frac{1}{2} \leftarrow \frac{\frac{n (n + 1)}{2}}{n^2 + n} = \frac{1 + \cdots + n}{n^2 + n} \leq S^2_n \leq \frac{1 + \cdots + n}{n^2 + 1} = \frac{\frac{n (n + 1)}{2}}{n^2 + 1} \rightarrow \frac{1}{2}$$ So by the sandwich criterium $S^2_n \rightarrow \frac{1}{2}$ . So the limit is $\frac{3}{2}$ . Is this correct? Are there any other alternatives?,This question already has answers here : How to prove $ \lim_{n\to\infty}\sum_{k=1}^{n}\frac{n+k}{n^2+k}=\frac{3}{2}$? (4 answers) Closed 4 months ago . I want to find Here is my solution. Lets call the first sum and the second sum . Then we have: And by the sandwich criterium . On the other hand we have: So by the sandwich criterium . So the limit is . Is this correct? Are there any other alternatives?,\lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2}  + \cdots + \frac{n + n}{n^2 + n} \right) \lim_{n \rightarrow \infty} \left(\frac{n + 1}{n^2 + 1} + \frac{n + 2}{n^2 + 2}  + \cdots + \frac{n + n}{n^2 + n} \right) = \lim_{n \rightarrow \infty} \left(\frac{n}{n^2 + 1} + \frac{n}{n^2 + 2}  + \cdots + \frac{n}{n^2 + n} \right) + \left(\frac{1}{n^2 + 1} + \frac{2}{n^2 + 2}  + \cdots + \frac{n}{n^2 + n} \right) S^1_n S^2_n 1 \leftarrow n \frac{n}{n^2 + n} \leq S^1_n \leq n \frac{n}{n^2 + 1} \rightarrow 1 S^1_n \rightarrow 1  \frac{1}{2} \leftarrow \frac{\frac{n (n + 1)}{2}}{n^2 + n} = \frac{1 + \cdots + n}{n^2 + n} \leq S^2_n \leq \frac{1 + \cdots + n}{n^2 + 1} = \frac{\frac{n (n + 1)}{2}}{n^2 + 1} \rightarrow \frac{1}{2} S^2_n \rightarrow \frac{1}{2} \frac{3}{2},"['real-analysis', 'solution-verification']"
9,Proving a limit using the dominated converge theorem.,Proving a limit using the dominated converge theorem.,,"Let $\Omega \subset \mathbb{R}^n$ be an open set and consider the usual Lebesgue space $L^p(\Omega)$ . Adicionally, consider the space $$ U(\Omega) =  \left\{ f \in L^p(\Omega) \, \colon \, \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \,  \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0 \right\} .$$ My goal is to show that $L^p(\Omega) = U(\Omega)$ , using the Dominated Convergence Theorem. My attempt. The inclusion $U(\Omega) \subset L^p(\Omega)$ is direct just by definition of $U(\Omega)$ . On the other hand, let $f \in L^p(\Omega)$ be arbitrary. In order to show that $f \in U(\Omega)$ , it suffices to guarantee that $$ \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0. $$ For every $x \in \mathbb{R}^n, r > 0$ we have that $$ \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = \int_{\mathbb R^n}|f(y)|^p \chi_{B(x,r) \cap \Omega}(y) \, dy. $$ Now, there are two questions that arise: The first question is how to deal with the supremum. I believe that before applying the DCT I should find a way to get rid of the supremum but I've been unable to find one. The second question is related to applying the DCT with limits that tend to $0$ .There are some posts on MSE about this (e.g. [ $1$ ] , [ $2$ ] and [ $3$ ] ). The main reasoning presented in such posts is to use the sequential formulation of the limit to retrieve conclusions. I believe that I understand this reasoning but I have a more informal question: in pratice (say in articles or books, for example), what is the most common way of dealing with the DCT when limits tend to zero? Do the authors explicitly describe the use of the sequential formulation of a limit or do they just present a dominating function and move the limit (tending to $0$ ) inside of the integral ? Thanks for any help in advance.","Let be an open set and consider the usual Lebesgue space . Adicionally, consider the space My goal is to show that , using the Dominated Convergence Theorem. My attempt. The inclusion is direct just by definition of . On the other hand, let be arbitrary. In order to show that , it suffices to guarantee that For every we have that Now, there are two questions that arise: The first question is how to deal with the supremum. I believe that before applying the DCT I should find a way to get rid of the supremum but I've been unable to find one. The second question is related to applying the DCT with limits that tend to .There are some posts on MSE about this (e.g. [ ] , [ ] and [ ] ). The main reasoning presented in such posts is to use the sequential formulation of the limit to retrieve conclusions. I believe that I understand this reasoning but I have a more informal question: in pratice (say in articles or books, for example), what is the most common way of dealing with the DCT when limits tend to zero? Do the authors explicitly describe the use of the sequential formulation of a limit or do they just present a dominating function and move the limit (tending to ) inside of the integral ? Thanks for any help in advance.","\Omega \subset \mathbb{R}^n L^p(\Omega)  U(\Omega) =  \left\{ f \in L^p(\Omega) \, \colon \, \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \,  \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0 \right\} . L^p(\Omega) = U(\Omega) U(\Omega) \subset L^p(\Omega) U(\Omega) f \in L^p(\Omega) f \in U(\Omega)  \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0.  x \in \mathbb{R}^n, r > 0  \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = \int_{\mathbb R^n}|f(y)|^p \chi_{B(x,r) \cap \Omega}(y) \, dy.  0 1 2 3 0","['real-analysis', 'functional-analysis', 'limits', 'lebesgue-integral', 'supremum-and-infimum']"
10,Reciprocal of power series has a power series; Hunter,Reciprocal of power series has a power series; Hunter,,"I'm reading these lecture notes by Hunter, specifically chapter 10 on power series, and I have some elementary questions about a proof on the proposition stating that the reciprocal of a convergent power series that is nonzero at its center also has a power series. Everything is real here. The following proposition is used in the proof: Proposition 10.15. If $R, S>0$ and the functions $$ f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R, \quad g(x)=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S $$ are sums of convergent power series, then $$ \begin{aligned} (f+g)(x) & =\sum_{n=0}^{\infty}\left(a_{n}+b_{n}\right) x^{n} & & \text { in }|x|<T, \\ (f g)(x) & =\sum_{n=0}^{\infty} c_{n} x^{n} & & \text { in }|x|<T \end{aligned} $$ where $T=\min (R, S)$ and $$ c_{n}=\sum_{k=0}^{n} a_{n-k} b_{k} . $$ Now follows the proposition and its proof that I have some questions about. Proposition 10.16. If $R>0$ and $$ f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R, $$ is the sum of a power series with $a_{0} \neq 0$ , then there exists $S>0$ such that $$ \frac{1}{f(x)}=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S . $$ The coefficients $b_{n}$ are determined recursively by $$ b_{0}=\frac{1}{a_{0}}, \quad b_{n}=-\frac{1}{a_{0}} \sum_{k=0}^{n-1} a_{n-k} b_{k}, \quad \text { for } n \geq 1 . $$ Proof. First, we look for a formal power series expansion (i.e., without regard to its convergence) $$ g(x)=\sum_{n=0}^{\infty} b_{n} x^{n} $$ such that the formal Cauchy product $f g$ is equal to 1 . This condition is satisfied if $$ \left(\sum_{n=0}^{\infty} a_{n} x^{n}\right)\left(\sum_{n=0}^{\infty} b_{n} x^{n}\right)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n} a_{n-k} b_{k}\right) x^{n}=1 . $$ Matching the coefficients of $x^{n}$ , we find that $$ a_{0} b_{0}=1, \quad a_{0} b_{n}+\sum_{k=0}^{n-1} a_{n-k} b_{k}=0 \quad \text { for } n \geq 1, $$ which gives the stated recursion relation. To complete the proof, we need to show that the formal power series for $g$ has a nonzero radius of convergence. In that case, Proposition 10.15 shows that $f g=1$ inside the common interval of convergence of $f$ and $g$ , so $1 / f=g$ has a power series expansion. I'm a bit confused about the last paragraph. I don't see why $1/f=g$ has a power series expansion. The whole proposition is about showing that $1/f$ can be written as a power series. This has not been proven yet, right? We assume without loss of generality that $a_{0}=1$ ; otherwise replace $f$ by $f / a_{0}$ . The power series for $f$ converges absolutely and uniformly on compact sets inside its interval of convergence, so the function $$ \sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n} $$ is continuous in $|x|<R$ and vanishes at $x=0$ . It follows that there exists $\delta>0$ such that $$ \sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n} \leq 1 \quad \text { for }|x| \leq \delta \tag{1} $$ Then $f(x) \neq 0$ for $|x|<\delta$ , since $$ |f(x)| \geq 1-\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}>0 \tag{2} $$ so $1 / f(x)$ is well defined. The author states a power series converges absolutely and uniformly on compact sets within its radius of convergence. The latter implies that a power series therefor is continuous within its radius of convergence. That said, I'd like to verify why $h(x)=\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}$ is continuous, as claimed above. If $f(x)=\sum_{n=0}^{\infty}a_{n}x^{n}$ converges for $|x|<R$ , so does $g(x)=\sum_{n=0}^{\infty}\left|a_{n}\right||x|^{n}$ on $|x|<R$ according to the author. $g(x)$ is just another power series, so it is continuous on $|x|<R$ . Hence, since $a_0=1$ , $h(x)=g(x)-1$ and this is continuous since it is the difference of two continuous functions. Is this reasoning correct? I am a little confused about $(1)$ and $(2)$ . I suppose $(1)$ is just the definition of continuity with $<$ replaced by $\leq$ . However, where do the two inequalities in $(2)$ follow from? Why do they hold for $|x|<\delta$ ? We claim that $$ \left|b_{n}\right| \leq \frac{1}{\delta^{n}} \quad \text { for } n=0,1,2, \ldots $$ The proof is by induction. Since $b_{0}=1$ , this inequality is true for $n=0$ . If $n \geq 1$ and the inequality holds for $b_{k}$ with $0 \leq k \leq n-1$ , then by taking the absolute value of the recursion relation for $b_{n}$ , we get $$ \left|b_{n}\right| \leq \sum_{k=1}^{n}\left|a_{k}\right|\left|b_{n-k}\right| \leq \sum_{k=1}^{n} \frac{\left|a_{k}\right|}{\delta^{n-k}} \leq \frac{1}{\delta^{n}} \sum_{k=1}^{\infty}\left|a_{k}\right| \delta^{k} \leq \frac{1}{\delta^{n}}, $$ so the inequality holds for $b_{k}$ with $0 \leq k \leq n$ , and the claim follows. We then get that $$ \limsup _{n \rightarrow \infty}\left|b_{n}\right|^{1 / n} \leq \frac{1}{\delta} $$ so the Hadamard formula in Theorem 10.6 implies that the radius of convergence of $\sum b_{n} x^{n}$ is greater than or equal to $\delta>0$ , which completes the proof.","I'm reading these lecture notes by Hunter, specifically chapter 10 on power series, and I have some elementary questions about a proof on the proposition stating that the reciprocal of a convergent power series that is nonzero at its center also has a power series. Everything is real here. The following proposition is used in the proof: Proposition 10.15. If and the functions are sums of convergent power series, then where and Now follows the proposition and its proof that I have some questions about. Proposition 10.16. If and is the sum of a power series with , then there exists such that The coefficients are determined recursively by Proof. First, we look for a formal power series expansion (i.e., without regard to its convergence) such that the formal Cauchy product is equal to 1 . This condition is satisfied if Matching the coefficients of , we find that which gives the stated recursion relation. To complete the proof, we need to show that the formal power series for has a nonzero radius of convergence. In that case, Proposition 10.15 shows that inside the common interval of convergence of and , so has a power series expansion. I'm a bit confused about the last paragraph. I don't see why has a power series expansion. The whole proposition is about showing that can be written as a power series. This has not been proven yet, right? We assume without loss of generality that ; otherwise replace by . The power series for converges absolutely and uniformly on compact sets inside its interval of convergence, so the function is continuous in and vanishes at . It follows that there exists such that Then for , since so is well defined. The author states a power series converges absolutely and uniformly on compact sets within its radius of convergence. The latter implies that a power series therefor is continuous within its radius of convergence. That said, I'd like to verify why is continuous, as claimed above. If converges for , so does on according to the author. is just another power series, so it is continuous on . Hence, since , and this is continuous since it is the difference of two continuous functions. Is this reasoning correct? I am a little confused about and . I suppose is just the definition of continuity with replaced by . However, where do the two inequalities in follow from? Why do they hold for ? We claim that The proof is by induction. Since , this inequality is true for . If and the inequality holds for with , then by taking the absolute value of the recursion relation for , we get so the inequality holds for with , and the claim follows. We then get that so the Hadamard formula in Theorem 10.6 implies that the radius of convergence of is greater than or equal to , which completes the proof.","R, S>0 
f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R, \quad g(x)=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S
 
\begin{aligned}
(f+g)(x) & =\sum_{n=0}^{\infty}\left(a_{n}+b_{n}\right) x^{n} & & \text { in }|x|<T, \\
(f g)(x) & =\sum_{n=0}^{\infty} c_{n} x^{n} & & \text { in }|x|<T
\end{aligned}
 T=\min (R, S) 
c_{n}=\sum_{k=0}^{n} a_{n-k} b_{k} .
 R>0 
f(x)=\sum_{n=0}^{\infty} a_{n} x^{n} \quad \text { in }|x|<R,
 a_{0} \neq 0 S>0 
\frac{1}{f(x)}=\sum_{n=0}^{\infty} b_{n} x^{n} \quad \text { in }|x|<S .
 b_{n} 
b_{0}=\frac{1}{a_{0}}, \quad b_{n}=-\frac{1}{a_{0}} \sum_{k=0}^{n-1} a_{n-k} b_{k}, \quad \text { for } n \geq 1 .
 
g(x)=\sum_{n=0}^{\infty} b_{n} x^{n}
 f g 
\left(\sum_{n=0}^{\infty} a_{n} x^{n}\right)\left(\sum_{n=0}^{\infty} b_{n} x^{n}\right)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n} a_{n-k} b_{k}\right) x^{n}=1 .
 x^{n} 
a_{0} b_{0}=1, \quad a_{0} b_{n}+\sum_{k=0}^{n-1} a_{n-k} b_{k}=0 \quad \text { for } n \geq 1,
 g f g=1 f g 1 / f=g 1/f=g 1/f a_{0}=1 f f / a_{0} f 
\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}
 |x|<R x=0 \delta>0 
\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n} \leq 1 \quad \text { for }|x| \leq \delta \tag{1}
 f(x) \neq 0 |x|<\delta 
|f(x)| \geq 1-\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n}>0 \tag{2}
 1 / f(x) h(x)=\sum_{n=1}^{\infty}\left|a_{n}\right||x|^{n} f(x)=\sum_{n=0}^{\infty}a_{n}x^{n} |x|<R g(x)=\sum_{n=0}^{\infty}\left|a_{n}\right||x|^{n} |x|<R g(x) |x|<R a_0=1 h(x)=g(x)-1 (1) (2) (1) < \leq (2) |x|<\delta 
\left|b_{n}\right| \leq \frac{1}{\delta^{n}} \quad \text { for } n=0,1,2, \ldots
 b_{0}=1 n=0 n \geq 1 b_{k} 0 \leq k \leq n-1 b_{n} 
\left|b_{n}\right| \leq \sum_{k=1}^{n}\left|a_{k}\right|\left|b_{n-k}\right| \leq \sum_{k=1}^{n} \frac{\left|a_{k}\right|}{\delta^{n-k}} \leq \frac{1}{\delta^{n}} \sum_{k=1}^{\infty}\left|a_{k}\right| \delta^{k} \leq \frac{1}{\delta^{n}},
 b_{k} 0 \leq k \leq n 
\limsup _{n \rightarrow \infty}\left|b_{n}\right|^{1 / n} \leq \frac{1}{\delta}
 \sum b_{n} x^{n} \delta>0","['real-analysis', 'calculus', 'sequences-and-series', 'proof-explanation', 'power-series']"
11,The definition of $C_0^\infty(\overline{\Omega})$ and $C^\infty(\overline{\Omega}).$,The definition of  and,C_0^\infty(\overline{\Omega}) C^\infty(\overline{\Omega}).,"I am reading a famous book Introduction to the Theory of Linear Partial Differential Equation written by Chazarain and Piriou. In page 63, the authors define $C^\infty(\overline{\Omega})$ is the space of functions $\phi \in C^\infty(\Omega)$ such that $\partial^\alpha\phi \in C^0(\overline{\Omega})$ , for any multi-index $\alpha$ . Here $\Omega$ is a regular open subset of $\mathbb{R}^n.$ (For the definition of regular open set, see also page 63. But it is not important here.) The authors then define $C_0^\infty(\overline{\Omega})$ for the functions $\phi \in C^\infty(\overline{\Omega})$ with compact support in $\overline{\Omega}.$ I am confused about the definition of $C^\infty(\overline{\Omega}).$ We only know $\phi \in C^\infty(\Omega),$ how can we define $\partial^\alpha \phi(x)$ for $x \in \partial \Omega$ ? Besides, for $\phi \in C_0^\infty(\overline{\Omega}),$ is $\phi(x)$ for $x\in \partial \Omega$ necessarily $0$ ? We know that $\text{supp}\phi \subset \overline{\Omega}$ and $\text{supp}\phi$ is $K \cap \overline{\Omega},$ where $K$ is a compact set in $\mathbb{R}^n.$ I don't think it can assure $\phi(x)=0$ for $\in \partial \Omega.$ Am I right?","I am reading a famous book Introduction to the Theory of Linear Partial Differential Equation written by Chazarain and Piriou. In page 63, the authors define is the space of functions such that , for any multi-index . Here is a regular open subset of (For the definition of regular open set, see also page 63. But it is not important here.) The authors then define for the functions with compact support in I am confused about the definition of We only know how can we define for ? Besides, for is for necessarily ? We know that and is where is a compact set in I don't think it can assure for Am I right?","C^\infty(\overline{\Omega}) \phi \in C^\infty(\Omega) \partial^\alpha\phi \in C^0(\overline{\Omega}) \alpha \Omega \mathbb{R}^n. C_0^\infty(\overline{\Omega}) \phi \in C^\infty(\overline{\Omega}) \overline{\Omega}. C^\infty(\overline{\Omega}). \phi \in C^\infty(\Omega), \partial^\alpha \phi(x) x \in \partial \Omega \phi \in C_0^\infty(\overline{\Omega}), \phi(x) x\in \partial \Omega 0 \text{supp}\phi \subset \overline{\Omega} \text{supp}\phi K \cap \overline{\Omega}, K \mathbb{R}^n. \phi(x)=0 \in \partial \Omega.","['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
12,Asymptotic sum of set of natural numbers with positive natural density,Asymptotic sum of set of natural numbers with positive natural density,,"Let $d(A)$ denote the natural density of $A\subset \mathbb{N}.$ If $0 < \alpha < 1\ $ and $\ d(A) = \alpha,\ $ then is this enough to imply $$ \lim_{ N\to\infty } \left( \frac{ \displaystyle\sum_{n\in A}^{n\leq N} \frac{1}{n} }{ \displaystyle \sum_{n\in \mathbb{N}}^{n\leq N} \frac{1}{n} } \right) \equiv \lim_{ N\to\infty } \left( \frac{ \displaystyle\sum_{n\in A}^{n\leq N} \frac{1}{n} }{ \log(N) } \right) = \alpha\ ? $$ I think it should be ""obviously"" true if, for example, $\ A = \lbrace{ \left\lceil \frac{n}{\alpha} \right\rceil: n\in\mathbb{N} \rbrace},\ $ but to me it is not so obvious for general $A.$ I was hoping to use Stolz–Cesàro, but don't quite see how. Or maybe by looking at lots of intervals like $[N, 1.01N]$ for very large $N,$ but I don't have anything concrete yet. Or maybe there is a counter-example. Edit: maybe the result is (also?) true for $\ \alpha=0\ $ and/or $\ \alpha = 1.$","Let denote the natural density of If and then is this enough to imply I think it should be ""obviously"" true if, for example, but to me it is not so obvious for general I was hoping to use Stolz–Cesàro, but don't quite see how. Or maybe by looking at lots of intervals like for very large but I don't have anything concrete yet. Or maybe there is a counter-example. Edit: maybe the result is (also?) true for and/or","d(A) A\subset \mathbb{N}. 0 < \alpha < 1\  \ d(A) = \alpha,\   \lim_{ N\to\infty } \left( \frac{ \displaystyle\sum_{n\in A}^{n\leq N} \frac{1}{n} }{ \displaystyle \sum_{n\in \mathbb{N}}^{n\leq N} \frac{1}{n} } \right) \equiv \lim_{ N\to\infty } \left( \frac{ \displaystyle\sum_{n\in A}^{n\leq N} \frac{1}{n} }{ \log(N) } \right) = \alpha\ ?  \ A = \lbrace{ \left\lceil \frac{n}{\alpha} \right\rceil: n\in\mathbb{N} \rbrace},\  A. [N, 1.01N] N, \ \alpha=0\  \ \alpha = 1.","['real-analysis', 'sequences-and-series', 'limits', 'divergent-series', 'natural-numbers']"
13,An inequality for the derivative of Japanese bracket,An inequality for the derivative of Japanese bracket,,"I'm reading a book that uses some properties of Japanese bracket. The author claims that, for any real number $m$ and multi-index $\beta$ , it holds $$|\partial_x^\beta\langle x\rangle^m|\leq C^{|\beta|}\beta!\langle x\rangle^{m-|\beta|},$$ for some constant $C>0$ which doesn't depend on $\beta$ . I did some computations with some fix $m$ , for example $m=1$ or $m=2$ , etc., and assuming that $x\in\Bbb{R}$ (the property should be true for $\Bbb{R}^n$ ). But, by taking large $\beta\in\Bbb{N}$ , the derivatives starts to get complicated (a lot of computations). Does anybody know about a simple way to prove this inequality? Of course, we define $\langle x\rangle:=(1+|x|^2)^{1/2}$ ,","I'm reading a book that uses some properties of Japanese bracket. The author claims that, for any real number and multi-index , it holds for some constant which doesn't depend on . I did some computations with some fix , for example or , etc., and assuming that (the property should be true for ). But, by taking large , the derivatives starts to get complicated (a lot of computations). Does anybody know about a simple way to prove this inequality? Of course, we define ,","m \beta |\partial_x^\beta\langle x\rangle^m|\leq C^{|\beta|}\beta!\langle x\rangle^{m-|\beta|}, C>0 \beta m m=1 m=2 x\in\Bbb{R} \Bbb{R}^n \beta\in\Bbb{N} \langle x\rangle:=(1+|x|^2)^{1/2}","['real-analysis', 'derivatives']"
14,"Prove that $f(s) = \sup\{x \in [a,b] : f(x) > x\}$, where $f$ is increasing","Prove that , where  is increasing","f(s) = \sup\{x \in [a,b] : f(x) > x\} f","Given $f: [a,b] \rightarrow [a,b]$ increasing, where $f(a) > a$ and $A = \{x \in [a,b] : f(x) > x\}. \ $ If $s = \sup(A), $ prove that $f(s) = s$ . In order to show that $f(s) = s$ , I was trying to prove that $f(s) \leq s$ and $f(s) \geq s$ . Can I simply state that since $f$ is an increasing function then $f(s) \geq s$ ?  How can I prove that $f(s) \leq s$ ?","Given increasing, where and If prove that . In order to show that , I was trying to prove that and . Can I simply state that since is an increasing function then ?  How can I prove that ?","f: [a,b] \rightarrow [a,b] f(a) > a A = \{x \in [a,b] : f(x) > x\}. \  s = \sup(A),  f(s) = s f(s) = s f(s) \leq s f(s) \geq s f f(s) \geq s f(s) \leq s","['real-analysis', 'supremum-and-infimum']"
15,Half spaces are measurable,Half spaces are measurable,,"I am trying to do exercise 7.4.3. in Tao's Analysis II: Prove that the half space $E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\}$ is measurable. i.e. $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ for any subset $A\subseteq \mathbb{R}^n.$ The hint he gave is to first prove that if $A$ is an open box $(a_1,b_1)\times \cdots \times (a_n,b_n)$ in $\mathbb{R}^n$ , and $E$ is the half-plane $E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\}$ , then $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ . I was able to prove the hint but I am not sure how it helps to prove half spaces are measurable. I have to show that for any set $A$ , we have $m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E)$ . We have showed that it is true if $A$ is an open box but I don't know how to extend that to arbitrary sets. Any help would be appreciated.Thanks.","I am trying to do exercise 7.4.3. in Tao's Analysis II: Prove that the half space is measurable. i.e. for any subset The hint he gave is to first prove that if is an open box in , and is the half-plane , then . I was able to prove the hint but I am not sure how it helps to prove half spaces are measurable. I have to show that for any set , we have . We have showed that it is true if is an open box but I don't know how to extend that to arbitrary sets. Any help would be appreciated.Thanks.","E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\} m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E) A\subseteq \mathbb{R}^n. A (a_1,b_1)\times \cdots \times (a_n,b_n) \mathbb{R}^n E E:=\{(x_1,\cdots, x_n)\in \mathbb{R}^n| x_n>0\} m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E) A m^{\ast}(A)=m^{\ast}(A\cap E)+m^{\ast}(A\setminus E) A","['real-analysis', 'measure-theory', 'lebesgue-measure']"
16,When is a cdf absolutely continuous?,When is a cdf absolutely continuous?,,"Let $\mu$ be a probability distribution on $(\mathbb R, \mathcal B(\mathbb R))$ and $X$ be a random variable with distribution $\mu$ . According to the theory developed in Section 3.5 of Folland's Real Analysis , $\mu$ is absolutely continuous w.r.t. Lebesgue measure iff the cdf $F_X$ is absolutely continuous. Assume that $F_X$ is known. What are tractable conditions on $F_X$ which guarantee that $F_X$ is absolutely continuous ? For example, I know that the following condition is sufficient (since it implies that $F_X$ is Lipschitz): $F_X$ is continuous over $\mathbb R$ , $F_X$ is differentiable except perhaps at finitely many points, and $(F_X)'$ is bounded. Are the following conditions also sufficient ? $F_X$ is continuous over $\mathbb R$ and $F_X$ is differentiable except perhaps at finitely many points. $F_X$ is continuous over $\mathbb R$ and $F_X$ is differentiable except perhaps at countably many points. $F_X$ is continuous over $\mathbb R$ and $\int_{\mathbb R} F_X'(x) dx = 1$ . Note that $F_X$ already has some regularity (non-decreasing, cadlag, limits at $\pm \infty$ , differentiable a.e.).","Let be a probability distribution on and be a random variable with distribution . According to the theory developed in Section 3.5 of Folland's Real Analysis , is absolutely continuous w.r.t. Lebesgue measure iff the cdf is absolutely continuous. Assume that is known. What are tractable conditions on which guarantee that is absolutely continuous ? For example, I know that the following condition is sufficient (since it implies that is Lipschitz): is continuous over , is differentiable except perhaps at finitely many points, and is bounded. Are the following conditions also sufficient ? is continuous over and is differentiable except perhaps at finitely many points. is continuous over and is differentiable except perhaps at countably many points. is continuous over and . Note that already has some regularity (non-decreasing, cadlag, limits at , differentiable a.e.).","\mu (\mathbb R, \mathcal B(\mathbb R)) X \mu \mu F_X F_X F_X F_X F_X F_X \mathbb R F_X (F_X)' F_X \mathbb R F_X F_X \mathbb R F_X F_X \mathbb R \int_{\mathbb R} F_X'(x) dx = 1 F_X \pm \infty","['real-analysis', 'probability-theory', 'measure-theory', 'absolute-continuity']"
17,"For any $\epsilon>0$, there exists arbitrarily large $x$ with $\cos( x^2)>1-\epsilon$ and $\cos[ (x+1)^2]<-1+\epsilon$","For any , there exists arbitrarily large  with  and",\epsilon>0 x \cos( x^2)>1-\epsilon \cos[ (x+1)^2]<-1+\epsilon,"For any $\epsilon>0$ , there exists arbitrarily large $x$ with $\cos (x^2)>1-\epsilon$ and $\cos [(x+1)^2]<-1+\epsilon$ . This is an exercise in ""Uniform Distribution of Sequences"" by Kuipers, Niederreiter. If I remember correctly, I may have been given a hint that we need to use the fact that the sequence $\left\{\frac {n^2}{2\pi}\right\}_{n=1}^\infty$ is uniformly distributed modulo $1$ (although I'm not sure whether this was indeed the hint), but I can't figure out how to use that. Also, since this question sounds like a question from Calc I, is there a way to solve this using only properties of the cosine function without using results from Uniform Distribution of sequences? Note: AlvinL posted this link which does look like the first half of this question, but we should note that this question asks to prove that there are arbitrarily large $x$ which simultaneously satisfy the two given conditions.","For any , there exists arbitrarily large with and . This is an exercise in ""Uniform Distribution of Sequences"" by Kuipers, Niederreiter. If I remember correctly, I may have been given a hint that we need to use the fact that the sequence is uniformly distributed modulo (although I'm not sure whether this was indeed the hint), but I can't figure out how to use that. Also, since this question sounds like a question from Calc I, is there a way to solve this using only properties of the cosine function without using results from Uniform Distribution of sequences? Note: AlvinL posted this link which does look like the first half of this question, but we should note that this question asks to prove that there are arbitrarily large which simultaneously satisfy the two given conditions.",\epsilon>0 x \cos (x^2)>1-\epsilon \cos [(x+1)^2]<-1+\epsilon \left\{\frac {n^2}{2\pi}\right\}_{n=1}^\infty 1 x,"['real-analysis', 'ergodic-theory', 'equidistribution']"
18,Cosine series for Dirac Delta comb,Cosine series for Dirac Delta comb,,"I am learning a bit about distributions and came across the following... In ""Theory of Distributions, a nontechnical introduction"" by Richards and Youn there is a formula with no explanation in the introduction. It states that as a distribution the following makes sense: $$\sum_{n=-\infty}^\infty \delta''(x-2\pi n)=\frac{-1}{\pi}\sum_{n=1}^\infty n^2\cos(nx) $$ Could someone please explain why the singularities don't pose an issue- how does one make sene of this summation? i am most curious about the singularity when the $n$ on the LHS=0. is this a known cosine representation of $\delta''$ ? thanks","I am learning a bit about distributions and came across the following... In ""Theory of Distributions, a nontechnical introduction"" by Richards and Youn there is a formula with no explanation in the introduction. It states that as a distribution the following makes sense: Could someone please explain why the singularities don't pose an issue- how does one make sene of this summation? i am most curious about the singularity when the on the LHS=0. is this a known cosine representation of ? thanks",\sum_{n=-\infty}^\infty \delta''(x-2\pi n)=\frac{-1}{\pi}\sum_{n=1}^\infty n^2\cos(nx)  n \delta'',"['real-analysis', 'calculus', 'sequences-and-series', 'distribution-theory']"
19,Definition of limits in metric spaces Tao's Analysis II,Definition of limits in metric spaces Tao's Analysis II,,"In the third edition of Tao's Analysis II he gives the following definition of limiting value of a function: Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, let $E$ be a subset of $X$ , and let $f:X\rightarrow Y$ be a function. If $x_0\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_0 $ in $E$ , or write $\lim_{x\to x_0 ; x\in E} f(x) =L$ , if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),L)< \epsilon$ for all $x\in E$ such that $d_X(x,x_0) < \delta$ But in the corrected third edition he changes the definition slightly: Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, let $E$ be a subset of $X$ , and let $f:E\rightarrow Y$ be a function. If $x_0\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_0$ in $E$ , or write $\lim_{x\to x_0 ; x\in E} f(x) =L$ , if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),L)< \epsilon$ for all $x\in E$ such that $d_X(x,x_0) < \delta$ Basically he changes the domain of $f$ , my question is: what is the motivation in this change? The first definition looks more useful to me, since the idea of taking a subset $E$ of $X$ and the notation $\lim_{x\to x_0 ; x\in E} f(x) =L$ is to be able of taking limits in subsets of $X$ , also we could perform this idea with the second definition, if we have a function $f:X\rightarrow Y$ and a subset $E$ of $X$ is enought to take the restriction $f|_E$ , but we can perform the same idea with the first definition with no need of these ""extra steps"".","In the third edition of Tao's Analysis II he gives the following definition of limiting value of a function: Let and be metric spaces, let be a subset of , and let be a function. If is an adherent point of , and , we say that converges to in as converges to in , or write , if for every there exists a such that for all such that But in the corrected third edition he changes the definition slightly: Let and be metric spaces, let be a subset of , and let be a function. If is an adherent point of , and , we say that converges to in as converges to in , or write , if for every there exists a such that for all such that Basically he changes the domain of , my question is: what is the motivation in this change? The first definition looks more useful to me, since the idea of taking a subset of and the notation is to be able of taking limits in subsets of , also we could perform this idea with the second definition, if we have a function and a subset of is enought to take the restriction , but we can perform the same idea with the first definition with no need of these ""extra steps"".","(X,d_X) (Y,d_Y) E X f:X\rightarrow Y x_0\in X E L\in Y f(x) L Y x x_0
 E \lim_{x\to x_0 ; x\in E} f(x) =L \epsilon > 0 \delta > 0 d_Y(f(x),L)< \epsilon x\in E d_X(x,x_0) < \delta (X,d_X) (Y,d_Y) E X f:E\rightarrow Y x_0\in X E L\in Y f(x) L Y x x_0 E \lim_{x\to x_0 ; x\in E} f(x) =L \epsilon > 0 \delta > 0 d_Y(f(x),L)< \epsilon x\in E d_X(x,x_0) < \delta f E X \lim_{x\to x_0 ; x\in E} f(x) =L X f:X\rightarrow Y E X f|_E","['real-analysis', 'metric-spaces', 'definition']"
20,Can Peano's existence theorem be proved by the implicit function theorem?,Can Peano's existence theorem be proved by the implicit function theorem?,,"In The implicit function theorem:history,theory and applications written by Krantz & Parks, it's said that the implicit function theorem can prove the following existence theorem of ODE: Theorem 4.1.1 If $F(t,x), (t,x) \in R×R^N$ , is continuous in the $(N+1)$ -dimensional region $(t_0−a,t_0+a)×B(x_0,r)$ , then there exists a solution $x(t)$ of $$\frac{\mathrm{d} x}{\mathrm{d} t} =F(t,x),x(t_0)=x_0$$ defined over an interval $(t_0−h,t_0+h)$ . which is also called Peano existence theorem. The proof in the book uses this version of the implicit function theorem： Theorem 3.4.10 Let $X,Y,Z$ be Banach spaces. Let $U \times V$ be an open subset of $X \times Y$ . Suppose that $G:U \times V \to Z$ is continuous and has the property that $d_2 G$ exists and is continuous at each point of $U \times V$ . Assume that the point $(x, y) \in X \times Y$ has the property that $G(x,y)=0$ and that $d_2G(x,y)$ is invertible. Then there are open balls $M=B_X(x, r)$ and $N=B_Y(y,s)$ such that,for each $\zeta \in M$ , there is a unique $\eta \in N$ satisfying $G(\zeta,\eta)=0$ . The function $f$ , thereby uniquely defined near $x$ by the condition $f(\zeta)=\eta$ , is continuous. The proof process in the book is as follows： The proof process in the book For the proof given in the book, I am puzzled: if the implicit function given by Theorem 3.4.10 is unique, why is the function $x(t)$ finally obtained in this proof not necessarily unique? My questions are as follows: Is the proof given in the book correct? If the proof in the book is wrong, is there a method to prove Peano's existence theorem by using the implicit function theorem? I have this doubt because in some papers the authors claim that some version of the implicit function theorem can be used to prove Peano's existence theorem, but I looked at their references and found nothing. The most relevant literature I can find on the Internet is this book. I also didn't find convincing results in the Q&A on MSE. Related link: Does the implicit function theorem imply Peano existence theorem Using the Peano existence theorem (ODE's) to imply the implicit function theorem About the second link: In Hale's Ordinary Differential Equations (1980 version) that I can find, Peano's existence theorem is proved by Schauder fixed point theorem instead of the implicit function theorem.","In The implicit function theorem:history,theory and applications written by Krantz & Parks, it's said that the implicit function theorem can prove the following existence theorem of ODE: Theorem 4.1.1 If , is continuous in the -dimensional region , then there exists a solution of defined over an interval . which is also called Peano existence theorem. The proof in the book uses this version of the implicit function theorem： Theorem 3.4.10 Let be Banach spaces. Let be an open subset of . Suppose that is continuous and has the property that exists and is continuous at each point of . Assume that the point has the property that and that is invertible. Then there are open balls and such that,for each , there is a unique satisfying . The function , thereby uniquely defined near by the condition , is continuous. The proof process in the book is as follows： The proof process in the book For the proof given in the book, I am puzzled: if the implicit function given by Theorem 3.4.10 is unique, why is the function finally obtained in this proof not necessarily unique? My questions are as follows: Is the proof given in the book correct? If the proof in the book is wrong, is there a method to prove Peano's existence theorem by using the implicit function theorem? I have this doubt because in some papers the authors claim that some version of the implicit function theorem can be used to prove Peano's existence theorem, but I looked at their references and found nothing. The most relevant literature I can find on the Internet is this book. I also didn't find convincing results in the Q&A on MSE. Related link: Does the implicit function theorem imply Peano existence theorem Using the Peano existence theorem (ODE's) to imply the implicit function theorem About the second link: In Hale's Ordinary Differential Equations (1980 version) that I can find, Peano's existence theorem is proved by Schauder fixed point theorem instead of the implicit function theorem.","F(t,x), (t,x) \in R×R^N (N+1) (t_0−a,t_0+a)×B(x_0,r) x(t) \frac{\mathrm{d} x}{\mathrm{d} t} =F(t,x),x(t_0)=x_0 (t_0−h,t_0+h) X,Y,Z U \times V X \times Y G:U \times V \to Z d_2 G U \times V (x, y) \in X \times Y G(x,y)=0 d_2G(x,y) M=B_X(x, r) N=B_Y(y,s) \zeta \in M \eta \in N G(\zeta,\eta)=0 f x f(\zeta)=\eta x(t)","['real-analysis', 'ordinary-differential-equations', 'analysis', 'implicit-function-theorem', 'inverse-function-theorem']"
21,"$f,g$ convex, increasing real functions with $\frac{f(x)}{g(x)}\to 1.$ Does $\frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ?$","convex, increasing real functions with  Does","f,g \frac{f(x)}{g(x)}\to 1. \frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ?","Let $f,g:\mathbb{R}\to\mathbb{R}$ be convex strictly increasing real functions (so we have both $f(x)\to\infty $ and $g(x)\to\infty$ as $x\to\infty),$ and suppose further that $\frac{f(x)}{g(x)}\to 1.$ Then is it true that $\frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ?$ I know that since $f,g$ are convex with domain $\mathbb{R}$ , they are continuous, which is why I left this out of the question. And if $f,g$ did not have to be convex, then $f(x) = \ln(x),\ g(x) = \ln(x)+1$ would be a counter-example, as $\frac{f^{-1}(x)}{g^{-1}(x)}\to e\neq 1.$ I'm not coming up with a counter-example, and am also not sure how to approach the question otherwise as there seems to be a lot of things to consider in order to prove it true.","Let be convex strictly increasing real functions (so we have both and as and suppose further that Then is it true that I know that since are convex with domain , they are continuous, which is why I left this out of the question. And if did not have to be convex, then would be a counter-example, as I'm not coming up with a counter-example, and am also not sure how to approach the question otherwise as there seems to be a lot of things to consider in order to prove it true.","f,g:\mathbb{R}\to\mathbb{R} f(x)\to\infty  g(x)\to\infty x\to\infty), \frac{f(x)}{g(x)}\to 1. \frac{f^{-1}(x)}{g^{-1}(x)}\to 1\ ? f,g \mathbb{R} f,g f(x) = \ln(x),\ g(x) = \ln(x)+1 \frac{f^{-1}(x)}{g^{-1}(x)}\to e\neq 1.","['real-analysis', 'functions', 'convex-analysis', 'graphing-functions']"
22,Conjecture: $\lim_{x\to 0^+}f(2x)\log f(x)=0$.,Conjecture: .,\lim_{x\to 0^+}f(2x)\log f(x)=0,"I encountered the following problem: Suppose $f$ is a strictly increasing $C^1$ function defined on $[0,a]$ for some $a>0$ , such that $f(0)=f'(0)=0$ . Is it true that $\lim_{x\to 0^+}f(2x)\log f(x)=0$ ? I tried to use the fact that $z^{\alpha}\log z\to 0$ as $z\to 0^+$ for any $\alpha>0$ , and write $f(2x)\log f(x) = \frac{f(2x)}{f(x)^{\alpha}}f(x)^{\alpha}\log f(x)$ . It would then be sufficient to show that there always exists an $\alpha>0$ such that $\frac{f(2x)}{f(x)^{\alpha}}$ is bounded at zero. However, it turns out this is not the case. Here's a counterexample: $f(x) = e^{-e^{1/x}}$ (augmented with $f(0)=0$ ). But even for this counterexample the initial conjecture works. Any help would be appreciated.","I encountered the following problem: Suppose is a strictly increasing function defined on for some , such that . Is it true that ? I tried to use the fact that as for any , and write . It would then be sufficient to show that there always exists an such that is bounded at zero. However, it turns out this is not the case. Here's a counterexample: (augmented with ). But even for this counterexample the initial conjecture works. Any help would be appreciated.","f C^1 [0,a] a>0 f(0)=f'(0)=0 \lim_{x\to 0^+}f(2x)\log f(x)=0 z^{\alpha}\log z\to 0 z\to 0^+ \alpha>0 f(2x)\log f(x) = \frac{f(2x)}{f(x)^{\alpha}}f(x)^{\alpha}\log f(x) \alpha>0 \frac{f(2x)}{f(x)^{\alpha}} f(x) = e^{-e^{1/x}} f(0)=0",['real-analysis']
23,"What functions satisfy $\int_a^b f(x) c(x) \, dx \ge 0$ for all convex functions $f$?",What functions satisfy  for all convex functions ?,"\int_a^b f(x) c(x) \, dx \ge 0 f","This is an attempt to generalize Prove that $\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0$ for every $k \geq 1$ given that $f$ is convex. Inspired by that question and the given answers, I have the following Conjecture: Let $c:[a, b] \to \Bbb R$ be a continuous function. Then $$ \tag{1} \int_a^b f(x) c(x) \, dx \ge 0 $$ holds for all convex functions $f:[a, b] \to \Bbb R$ if and only if $$ \tag{2} \int_a^b c(x) \,dx = \int_a^b x c(x) \, dx = 0 \, . $$ The “only if” direction is easy: If $(1)$ holds for the four convex functions $x \mapsto \pm 1$ , $x \mapsto \pm x$ then $(2)$ holds. So the interesting part is the “if” direction. In the above mentioned Q&A this has been proven for the functions $c(x) = \cos(k x)$ (on the interval $[0, 2 \pi]$ ). Some of the proofs given there use the fact that $$ \int_0^{2\pi}  \cos (x) \, dx = \int_0^{2\pi} x \cos (x) \, dx = 0 \, , $$ but all proofs use also symmetries of the cosine function, trigonometric identities, or where the cosine is positive and negative in $[0, 2 \pi]$ . My conjecture is that these additional properties of the cosine are not needed, and that $(2)$ alone is sufficient to prove $(1)$ for all convex functions $f$ . As pointed out in the comments, the following is wrong: There is a simple proof for the “if” direction under the additional assumption that $f$ is twice continuously differentiable: Let $c_1$ be an antiderivative of $c$ , and $c_2$ be an antiderivative of $c_1$ . By adding a constant to $c_2$ , if necessary, we can assume that $c_2(x) \ge 0$ on $[a, b]$ . If $(1)$ holds then $c_1(a) = c_1(b)$ and $c_2(a) = c_2(b)$ , and for all convex functions $f$ on $[a, b]$ is, using integration by parts (twice): $$  \int_a^b f(x) c(x) \, dx  = \int_a^b f''(x) c_2(x) \, dx  \ge 0 \,. $$ What I am looking for is a proof of the conjecture (the “if” direction) which works for all convex functions $f$ , without additional assumptions on differentiability.","This is an attempt to generalize Prove that $\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0$ for every $k \geq 1$ given that $f$ is convex. Inspired by that question and the given answers, I have the following Conjecture: Let be a continuous function. Then holds for all convex functions if and only if The “only if” direction is easy: If holds for the four convex functions , then holds. So the interesting part is the “if” direction. In the above mentioned Q&A this has been proven for the functions (on the interval ). Some of the proofs given there use the fact that but all proofs use also symmetries of the cosine function, trigonometric identities, or where the cosine is positive and negative in . My conjecture is that these additional properties of the cosine are not needed, and that alone is sufficient to prove for all convex functions . As pointed out in the comments, the following is wrong: There is a simple proof for the “if” direction under the additional assumption that is twice continuously differentiable: Let be an antiderivative of , and be an antiderivative of . By adding a constant to , if necessary, we can assume that on . If holds then and , and for all convex functions on is, using integration by parts (twice): What I am looking for is a proof of the conjecture (the “if” direction) which works for all convex functions , without additional assumptions on differentiability.","c:[a, b] \to \Bbb R  \tag{1} \int_a^b f(x) c(x) \, dx \ge 0  f:[a, b] \to \Bbb R  \tag{2} \int_a^b c(x) \,dx = \int_a^b x c(x) \, dx = 0 \, .  (1) x \mapsto \pm 1 x \mapsto \pm x (2) c(x) = \cos(k x) [0, 2 \pi] 
\int_0^{2\pi}  \cos (x) \, dx = \int_0^{2\pi} x \cos (x) \, dx = 0 \, ,
 [0, 2 \pi] (2) (1) f f c_1 c c_2 c_1 c_2 c_2(x) \ge 0 [a, b] (1) c_1(a) = c_1(b) c_2(a) = c_2(b) f [a, b]   \int_a^b f(x) c(x) \, dx  = \int_a^b f''(x) c_2(x) \, dx  \ge 0 \,.  f","['real-analysis', 'convex-analysis', 'integral-inequality']"
24,Let $1=n_0<n_1<\ldots$ be an increasing sequence of positive integers. True/False: $\sum_{i=1}^\infty\frac{n_{i+1}-n_i}{n_{i+1}}$ diverges to $\infty$ [duplicate],Let  be an increasing sequence of positive integers. True/False:  diverges to  [duplicate],1=n_0<n_1<\ldots \sum_{i=1}^\infty\frac{n_{i+1}-n_i}{n_{i+1}} \infty,"This question already has answers here : $\sum_{n=1}^\infty \frac{a_{n+1}-a_n}{a_{n+1}}$ diverges [duplicate] (2 answers) If $a_n$ is a strictly increasing unbounded sequence, does $\sum_n \frac{a_{n+1} - a_n}{a_n}$ diverge? (4 answers) Closed 2 years ago . Let $1=n_0<n_1<n_2<\ldots$ be an increasing sequence of positive integers. Is it true that $\displaystyle\sum_{i=0}^{\infty} \frac{n_{i+1}-n_i}{n_{i+1}} $ diverges to $+\infty?$ For example, if $n_1=2,n_2=3,n_3=53,n_4=54,$ then the first few terms are $\frac{1}{2}, \frac{1}{3}, \frac{50}{53}, \frac{1}{54}.$ Since $\frac{50}{53}<\displaystyle\sum_{k=4}^9 \frac{1}{k},$ this makes me think it might be possible to come up with a counter-exmaple.","This question already has answers here : $\sum_{n=1}^\infty \frac{a_{n+1}-a_n}{a_{n+1}}$ diverges [duplicate] (2 answers) If $a_n$ is a strictly increasing unbounded sequence, does $\sum_n \frac{a_{n+1} - a_n}{a_n}$ diverge? (4 answers) Closed 2 years ago . Let be an increasing sequence of positive integers. Is it true that diverges to For example, if then the first few terms are Since this makes me think it might be possible to come up with a counter-exmaple.","1=n_0<n_1<n_2<\ldots \displaystyle\sum_{i=0}^{\infty} \frac{n_{i+1}-n_i}{n_{i+1}}  +\infty? n_1=2,n_2=3,n_3=53,n_4=54, \frac{1}{2}, \frac{1}{3}, \frac{50}{53}, \frac{1}{54}. \frac{50}{53}<\displaystyle\sum_{k=4}^9 \frac{1}{k},","['real-analysis', 'sequences-and-series', 'examples-counterexamples', 'problem-solving']"
25,Inequality help: $\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc}$,Inequality help:,\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc},"I'm working through ""Calculus of One Variable"" by Joseph Kitchen, and this inequality (problem 3 from section 1.3) is causing me quite a bit of pain to prove: $$\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc},$$ where $a,b,c\geq 0.$ The main issue I'm having is that I cannot use the fact that $0\leq a < b$ if and only if $a^{2} < b^{2}$ , as this fact is proven in a later exercise. If you want to take a look at what is proven and what isn't, the first 80 pages of the book is available on a Google Books preview. Although a hint for this problem is provided in the back of the book, it has not been useful to me. The hint states: ""Use Example 2 of the text. Define $A = (x+y+z)/3$ and $H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3$ ."" Note that $A$ and $H$ refer to the arithmetic and harmonic means, respectively. Example 2 proved that if $a$ , $b$ , and $c$ are positive numbers which are not all equal, then $(a+b+c)(bc+ca+ab) > 9abc$ . This was proved by noting that $(a+b+c)(bc+ca+ab)-9abc = a(b-c)^{2} + b(c-a)^{2} + c(a-b)^{2}$ . With little work, it is not hard to see that when $a$ , $b$ , and $c$ are non-negative , then $(a+b+c)(bc+ca+ab) \geq 9abc$ . However, I'm having a difficult time seeing how this fact is useful for my current problem. Using the hint to define $A = (x+y+z)/3$ and $H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3$ , I proved that $A\geq H$ : $$A = \frac{x+y+z}{3}\geq^{(1)} \frac{9xyz}{yz+zx+xy} = \frac{9}{x^{-1}+y^{-1}+z^{-1}}\geq H,$$ where the first inequality (1) holds by Example 2. However, I do not see how this is at all helpful. Here's some of the things that I've tried (that do not involve the hint). Define $x = \sqrt[3]{a}$ , $y = \sqrt[3]{b}$ , and $z = \sqrt[3]{c}$ . Then, observe that $$\frac{a+b+c}{3} - \sqrt[3]{abc} \,=\, \frac{x^{3} + y^{3} + z^{3} - 3xyz}{3} = \frac{(x+y+z)\left((x-y)^{2} + (y-z)^{2} + (z-x)^{2}\right)}{6}\geq 0.$$ Thus, $(a+b+c)/3 \geq \sqrt[3]{abc}$ . I was also able to prove that $(a+b+c)/3\geq\sqrt{(ab+bc+ca)/3}$ , however, I used the fact that $0\leq a < b$ if and only if $a^{2} < b^{2}$ , which I'm not supposed to use. I only include this proof since it may give insight into the problem. (Some of these algebraic ""tricks"" were hard to figure out!) Since $\frac{1}{2}\left((a-b)^{2} + (b-c)^{2} + (c-a)^{2}\right) = (a+b+c)^{2} - 3(ab + bc + ca)\geq 0$ , it follows that $3(a+b+c)^{2}\geq 9(ab+bc+ca)$ . And therefore, $(a+b+c)^{2}/9 \geq (ab+bc+ca)/3 \geq 0$ . Thus, it follows that $(a+b+c)/3\geq\sqrt{(ab+bc+ca)/3}$ . So... to recap, using the appropriate methods, I have prove that $(a+b+c)/3 \geq \sqrt[3]{abc}$ and that $(a+b+c)/3 \geq3/(a^{-1}+b^{-1}+c^{-1})$ . However, the rest is giving me a headache. Perhaps I've missed something obvious, but I'm stuck...","I'm working through ""Calculus of One Variable"" by Joseph Kitchen, and this inequality (problem 3 from section 1.3) is causing me quite a bit of pain to prove: where The main issue I'm having is that I cannot use the fact that if and only if , as this fact is proven in a later exercise. If you want to take a look at what is proven and what isn't, the first 80 pages of the book is available on a Google Books preview. Although a hint for this problem is provided in the back of the book, it has not been useful to me. The hint states: ""Use Example 2 of the text. Define and ."" Note that and refer to the arithmetic and harmonic means, respectively. Example 2 proved that if , , and are positive numbers which are not all equal, then . This was proved by noting that . With little work, it is not hard to see that when , , and are non-negative , then . However, I'm having a difficult time seeing how this fact is useful for my current problem. Using the hint to define and , I proved that : where the first inequality (1) holds by Example 2. However, I do not see how this is at all helpful. Here's some of the things that I've tried (that do not involve the hint). Define , , and . Then, observe that Thus, . I was also able to prove that , however, I used the fact that if and only if , which I'm not supposed to use. I only include this proof since it may give insight into the problem. (Some of these algebraic ""tricks"" were hard to figure out!) Since , it follows that . And therefore, . Thus, it follows that . So... to recap, using the appropriate methods, I have prove that and that . However, the rest is giving me a headache. Perhaps I've missed something obvious, but I'm stuck...","\frac{a+b+c}{3}\geq\sqrt{\frac{ab+bc+ca}{3}}\geq\sqrt[3]{abc}, a,b,c\geq 0. 0\leq a < b a^{2} < b^{2} A = (x+y+z)/3 H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3 A H a b c (a+b+c)(bc+ca+ab) > 9abc (a+b+c)(bc+ca+ab)-9abc = a(b-c)^{2} + b(c-a)^{2} + c(a-b)^{2} a b c (a+b+c)(bc+ca+ab) \geq 9abc A = (x+y+z)/3 H^{-1} = (x^{-1}+y^{-1}+z^{-1})/3 A\geq H A = \frac{x+y+z}{3}\geq^{(1)} \frac{9xyz}{yz+zx+xy} = \frac{9}{x^{-1}+y^{-1}+z^{-1}}\geq H, x = \sqrt[3]{a} y = \sqrt[3]{b} z = \sqrt[3]{c} \frac{a+b+c}{3} - \sqrt[3]{abc} \,=\, \frac{x^{3} + y^{3} + z^{3} - 3xyz}{3} = \frac{(x+y+z)\left((x-y)^{2} + (y-z)^{2} + (z-x)^{2}\right)}{6}\geq 0. (a+b+c)/3 \geq \sqrt[3]{abc} (a+b+c)/3\geq\sqrt{(ab+bc+ca)/3} 0\leq a < b a^{2} < b^{2} \frac{1}{2}\left((a-b)^{2} + (b-c)^{2} + (c-a)^{2}\right) = (a+b+c)^{2} - 3(ab + bc + ca)\geq 0 3(a+b+c)^{2}\geq 9(ab+bc+ca) (a+b+c)^{2}/9 \geq (ab+bc+ca)/3 \geq 0 (a+b+c)/3\geq\sqrt{(ab+bc+ca)/3} (a+b+c)/3 \geq \sqrt[3]{abc} (a+b+c)/3 \geq3/(a^{-1}+b^{-1}+c^{-1})","['real-analysis', 'calculus']"
26,"If $\underbrace{f*f*\ldots*f}_{n\text{ times}}\to f$ uniformly, then the continuous $2\pi$-periodic function $f$ is a trigonometric polynomial","If  uniformly, then the continuous -periodic function  is a trigonometric polynomial",\underbrace{f*f*\ldots*f}_{n\text{ times}}\to f 2\pi f,"Let $f$ be a $2 \pi$ -periodic continuous function. Given $$g_1 = f, \qquad g_2 = f * f, \quad \cdots \quad g_{n} = \underbrace{f * f * \cdots * f}_{n \text{ times}} $$ where $*$ denotes convolution, and assume $ g_{n} \xrightarrow{n\to\infty} f $ uniformly. Prove that $f $ is a trigonometric polynomial. Im not sure where to start, I have proved in previous parts that if $f$ is Dirichlet kernel $$\sum_{n=-N}^{N}e^{i n x}$$ then for any $n\in \mathbb{N} $ we have $g_n=f$ , but I am not sure how to use it, if it is relevant at all. Any help would be appreciated. Thanks in advance.","Let be a -periodic continuous function. Given where denotes convolution, and assume uniformly. Prove that is a trigonometric polynomial. Im not sure where to start, I have proved in previous parts that if is Dirichlet kernel then for any we have , but I am not sure how to use it, if it is relevant at all. Any help would be appreciated. Thanks in advance.","f 2 \pi g_1 = f, \qquad g_2 = f * f, \quad \cdots \quad g_{n} = \underbrace{f * f * \cdots * f}_{n \text{ times}}  *  g_{n} \xrightarrow{n\to\infty} f  f  f \sum_{n=-N}^{N}e^{i n x} n\in \mathbb{N}  g_n=f","['real-analysis', 'calculus', 'fourier-analysis']"
27,"$f\in C^2((0,1)),\lim_{x\to 1^{-}}f(x)=0,\exists C\forall x\in (0,1)$,$(1-x)^2|f''(x)|\leqslant C$,then $\lim_{x\to 1^{-}} (1-x)f'(x)=0$ [duplicate]",",,then  [duplicate]","f\in C^2((0,1)),\lim_{x\to 1^{-}}f(x)=0,\exists C\forall x\in (0,1) (1-x)^2|f''(x)|\leqslant C \lim_{x\to 1^{-}} (1-x)f'(x)=0","This question already has answers here : Prove or disprove that if $\lim\limits_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq c$ then $\lim\limits_{x\to0^+}xf'(x)=0$ (2 answers) Closed 2 years ago . Question : Suppose that $f\in C^2((0,1))$ , $\lim\limits_{x\to 1^{-}}f(x)=0$ . Assume that there exists a constant $C>0$ such that $\forall x\in (0,1)$ , $(1-x)^2|f''(x)|\leqslant C$ . Prove that $\lim_\limits{x\to 1^{-}} (1-x)f'(x)=0$ . Attempt : I've tried to use Taylor expansion and got $0=f(x)+f'(x)(1-x)+\frac{f''(c)}{2}(1-x)^2,c\in (x,1).$ But that's not enough.","This question already has answers here : Prove or disprove that if $\lim\limits_{x\to0^+}f(x)=0$ and $|x^2f''(x)|\leq c$ then $\lim\limits_{x\to0^+}xf'(x)=0$ (2 answers) Closed 2 years ago . Question : Suppose that , . Assume that there exists a constant such that , . Prove that . Attempt : I've tried to use Taylor expansion and got But that's not enough.","f\in C^2((0,1)) \lim\limits_{x\to 1^{-}}f(x)=0 C>0 \forall x\in (0,1) (1-x)^2|f''(x)|\leqslant C \lim_\limits{x\to 1^{-}} (1-x)f'(x)=0 0=f(x)+f'(x)(1-x)+\frac{f''(c)}{2}(1-x)^2,c\in (x,1).","['real-analysis', 'analysis', 'functions']"
28,Calculate and prove the limit $\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n}$ Given $a_n$ is an increasing monotone sequence of integers,Calculate and prove the limit  Given  is an increasing monotone sequence of integers,\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n} a_n,"$\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n}$ given $(a_n)$ is an increasing sequence of integers NOTE - I noticed these 2 questions here (1) and here (2) but I believe that my question is a bit different because these questions answer only the case of $a_1 \geq 0$ according to the information we can understand that if $a_1 \geq 0$ then it is immediately solved as $\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n} =e$ since then we can look at $a_n$ as in $a_n=n$ and then the limit would be just an identity $\lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e$ otherwise what if $a_1 <0$ ? we will need to show that  there exists an $N \in \Bbb N$ such that $a_N \geq 0$ we can also understand that $a_{n+1}-a_n \geq 1$ then for ever $n$ we get $a_{n+1}-a_n = (a_{n+1}-a_n)+(a_n-a_{n+1})...+(a_2 - a_1) \geq 1+1+1...+1 =n$ I do not know how to continue from here.. I usually have more ideas and stuff I tried on my posts but I really cannot figure out what to do here. thanks for any help and tips! edit - Thanks to all the comments I tried and cannot figure out on how to actually prove it , I do realize what the limit is worth now and why but I am struggling to prove it as I stated before EDIT (point of the edit is to solve it organized and in a better way according to information I have and from the comments) - Organizing information a. $(a_n)$ is an increasing sequence therefore we get $(1)$ $a_{n+1} > a_n$ , $(2)$ thanks to nejimban an increasing sequence of integers must tend to $\infty$ $(3)$ an increasing sequence of integers therefore $a_{n+1}-a_n \geq 1$ b. $\lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e$ c. Need to prove that there is an $N \in \Bbb N$ such that for every $n>N$ we get $a_n>0$ Solving - posted as an answer to the question","given is an increasing sequence of integers NOTE - I noticed these 2 questions here (1) and here (2) but I believe that my question is a bit different because these questions answer only the case of according to the information we can understand that if then it is immediately solved as since then we can look at as in and then the limit would be just an identity otherwise what if ? we will need to show that  there exists an such that we can also understand that then for ever we get I do not know how to continue from here.. I usually have more ideas and stuff I tried on my posts but I really cannot figure out what to do here. thanks for any help and tips! edit - Thanks to all the comments I tried and cannot figure out on how to actually prove it , I do realize what the limit is worth now and why but I am struggling to prove it as I stated before EDIT (point of the edit is to solve it organized and in a better way according to information I have and from the comments) - Organizing information a. is an increasing sequence therefore we get , thanks to nejimban an increasing sequence of integers must tend to an increasing sequence of integers therefore b. c. Need to prove that there is an such that for every we get Solving - posted as an answer to the question",\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n} (a_n) a_1 \geq 0 a_1 \geq 0 \lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n} =e a_n a_n=n \lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e a_1 <0 N \in \Bbb N a_N \geq 0 a_{n+1}-a_n \geq 1 n a_{n+1}-a_n = (a_{n+1}-a_n)+(a_n-a_{n+1})...+(a_2 - a_1) \geq 1+1+1...+1 =n (a_n) (1) a_{n+1} > a_n (2) \infty (3) a_{n+1}-a_n \geq 1 \lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e N \in \Bbb N n>N a_n>0,"['real-analysis', 'sequences-and-series', 'limits', 'solution-verification', 'eulers-number-e']"
29,"If $\int_1^\infty f(x) x^n dx=0$ for all $n>1$, is $f=0$?","If  for all , is ?",\int_1^\infty f(x) x^n dx=0 n>1 f=0,"For a continuous function $f$ , if $$ \int_1^\infty f(x) x^n dx =0 $$ For all integers $n>1$ , is it true that $f=0$ ? Here, $\int_1^\infty f(x) \text d x = 0$ means $\lim_{A \to \infty} \int_1^A f(x)\text dx =0$ . By looking at the real and imaginary parts, we can assume $f$ is real. If $f$ has compact support, then approximating $f$ with Stone-Weierstrass shows that $\int f^2=0$ so $f=0$ . If $f$ stops changing signs outside an interval $[1,a]$ , then $\int_1^a f(x) (x/a)^n \to 0$ by the DCT, and the rest of the integral must diverge, contradicting the assumption, so the sets where $f$ is positive or negative must be unbounded. The problem is that the approximation theorems I know only apply in compact domains, so integrating to infinity makes it more complicated. The fact that I do not know if $f$ is bounded also complicates matters, I know that if $f$ were decaying better than $e^{-x}$ this would be more tractable since I could just cut off at some point. So far, I am coming up empty. Is it true that $f = 0$ or is there some counterexample?","For a continuous function , if For all integers , is it true that ? Here, means . By looking at the real and imaginary parts, we can assume is real. If has compact support, then approximating with Stone-Weierstrass shows that so . If stops changing signs outside an interval , then by the DCT, and the rest of the integral must diverge, contradicting the assumption, so the sets where is positive or negative must be unbounded. The problem is that the approximation theorems I know only apply in compact domains, so integrating to infinity makes it more complicated. The fact that I do not know if is bounded also complicates matters, I know that if were decaying better than this would be more tractable since I could just cut off at some point. So far, I am coming up empty. Is it true that or is there some counterexample?","f 
\int_1^\infty f(x) x^n dx =0
 n>1 f=0 \int_1^\infty f(x) \text d x = 0 \lim_{A \to \infty} \int_1^A f(x)\text dx =0 f f f \int f^2=0 f=0 f [1,a] \int_1^a f(x) (x/a)^n \to 0 f f f e^{-x} f = 0","['real-analysis', 'integration']"
30,Union of collection of non-trivial intervals of $\mathbb{R}$ can be written as the union of a countable subset of that collection,Union of collection of non-trivial intervals of  can be written as the union of a countable subset of that collection,\mathbb{R},"I am thinking about the following statement: ""the union of each collection of nontrivial intervals of $\mathbb{R}$ is the union of a countable subset of that collection"" and I came up with the following explanation: Let $\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ , where $\mathcal{A}$ is uncountable be such a union of nontrivial intervals of $\mathbb{R}$ . Remove from this union every interval that is a subset of the union of other intervals: in this way we get a union of nontrivial intervals $\bigcup_{\alpha\in\mathcal{A'}}I_{\alpha},\ \mathcal{A'}\subset\mathcal{A}$ such that $\bigcup_{\alpha\in\mathcal{A'}}I_{\alpha}=\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ , in each interval we can pick a unique rational number (since each $I_{\alpha}$ is not completely inside any union of other intervals) and from this it follows that $\mathcal{A'}$ is countable, as desired. Now, I know that another question about the same problem has already been asked , and since the proofs posted there by experienced users are much more complicated I guess there must be something wrong with my reasoning above, but since I don't see what that is and I find it to be an appealing argument I would like to see where it fails and/or/how it could be improved, and, if there are any, other proofs of this interesting statement, thanks. EDIT 09/19: It might be worthwhile to try by contradiction: Suppose there existed an uncountable collection of nontrivial intervals $(I_{\alpha})_{\alpha\in\mathcal{A}}$ such that for every countable subset $\mathcal{C}\subset \mathcal{A}$ we have that $\bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ . Then it follows that: $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot contain $(-\infty,\infty)$ ; $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot contain any countable number of intervals whose union is $(-\infty,\infty)$ ; the $(I_{\alpha})_{\alpha\in\mathcal{A}}$ cannot be all disjoint, since in that case it is possible to establish an injection to the rational numbers by picking a distinct rational number from each of them, showing that $\mathcal{A}$ is countable, a contradiction. ( EDIT 09/20 ) $\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ cannot be bounded, because from the fact that every non-trivial interval has positive outer measure and $\bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}$ for every $\mathcal{C}\subset\mathcal{A}$ countable follows that $|\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}|=\infty$ . So, now the task is to get from these two pieces of information (and perhaps others I haven't yet found out) a contradiction. Comments are welcome. NOTE: I have never studied topology, only self-studied calculus/linear algebra/real analysis (and currently measure theory) so I am interested in proofs that don't use too much topological machinery, if that is at all possible.","I am thinking about the following statement: ""the union of each collection of nontrivial intervals of is the union of a countable subset of that collection"" and I came up with the following explanation: Let , where is uncountable be such a union of nontrivial intervals of . Remove from this union every interval that is a subset of the union of other intervals: in this way we get a union of nontrivial intervals such that , in each interval we can pick a unique rational number (since each is not completely inside any union of other intervals) and from this it follows that is countable, as desired. Now, I know that another question about the same problem has already been asked , and since the proofs posted there by experienced users are much more complicated I guess there must be something wrong with my reasoning above, but since I don't see what that is and I find it to be an appealing argument I would like to see where it fails and/or/how it could be improved, and, if there are any, other proofs of this interesting statement, thanks. EDIT 09/19: It might be worthwhile to try by contradiction: Suppose there existed an uncountable collection of nontrivial intervals such that for every countable subset we have that . Then it follows that: cannot contain ; cannot contain any countable number of intervals whose union is ; the cannot be all disjoint, since in that case it is possible to establish an injection to the rational numbers by picking a distinct rational number from each of them, showing that is countable, a contradiction. ( EDIT 09/20 ) cannot be bounded, because from the fact that every non-trivial interval has positive outer measure and for every countable follows that . So, now the task is to get from these two pieces of information (and perhaps others I haven't yet found out) a contradiction. Comments are welcome. NOTE: I have never studied topology, only self-studied calculus/linear algebra/real analysis (and currently measure theory) so I am interested in proofs that don't use too much topological machinery, if that is at all possible.","\mathbb{R} \bigcup_{\alpha\in\mathcal{A}}I_{\alpha} \mathcal{A} \mathbb{R} \bigcup_{\alpha\in\mathcal{A'}}I_{\alpha},\ \mathcal{A'}\subset\mathcal{A} \bigcup_{\alpha\in\mathcal{A'}}I_{\alpha}=\bigcup_{\alpha\in\mathcal{A}}I_{\alpha} I_{\alpha} \mathcal{A'} (I_{\alpha})_{\alpha\in\mathcal{A}} \mathcal{C}\subset \mathcal{A} \bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha} (I_{\alpha})_{\alpha\in\mathcal{A}} (-\infty,\infty) (I_{\alpha})_{\alpha\in\mathcal{A}} (-\infty,\infty) (I_{\alpha})_{\alpha\in\mathcal{A}} \mathcal{A} \bigcup_{\alpha\in\mathcal{A}}I_{\alpha} \bigcup_{\alpha\in\mathcal{C}}I_{\alpha}\subsetneqq\bigcup_{\alpha\in\mathcal{A}}I_{\alpha} \mathcal{C}\subset\mathcal{A} |\bigcup_{\alpha\in\mathcal{A}}I_{\alpha}|=\infty","['real-analysis', 'alternative-proof']"
31,Find the values $a>0$ for which this improper integral converges,Find the values  for which this improper integral converges,a>0,"Find the values of $a > 0$ for which the improper integral $$\int_{0}^{\infty} \frac{\sin x} {x^a} dx$$ converges. This question is from my analysis quiz (now over). I have studied improper integrals but Dirichlet test, Comparison tests and other results can't be used in this case as Dirichlet test is used if $\sin x$ used to tend to $\infty$ , also 2 comparison tests are not suited for two functions inside the integral. If I use Abel test and write $$\int_{0}^{\infty} \frac{\sin x} {x^a} dx = \int_{0}^{a} \frac{\sin x} {x^a} dx + \int_{a}^{\infty} \frac{\sin x} {x^a} dx$$ then $\int_{a}^{\infty} \frac{\sin x} {x^a} dx$ converges at infinity for $a<1$ but $\sin x$ is not bounded and monotonic in $(a, \infty)$ . So, it can't be used as well. So, what result should I use. (I have done a course on real analysis.)","Find the values of for which the improper integral converges. This question is from my analysis quiz (now over). I have studied improper integrals but Dirichlet test, Comparison tests and other results can't be used in this case as Dirichlet test is used if used to tend to , also 2 comparison tests are not suited for two functions inside the integral. If I use Abel test and write then converges at infinity for but is not bounded and monotonic in . So, it can't be used as well. So, what result should I use. (I have done a course on real analysis.)","a > 0 \int_{0}^{\infty} \frac{\sin x} {x^a} dx \sin x \infty \int_{0}^{\infty} \frac{\sin x} {x^a} dx = \int_{0}^{a} \frac{\sin x} {x^a} dx + \int_{a}^{\infty} \frac{\sin x} {x^a} dx \int_{a}^{\infty} \frac{\sin x} {x^a} dx a<1 \sin x (a, \infty)",['real-analysis']
32,Do simple paths have unique endpoints?,Do simple paths have unique endpoints?,,"Let $Y \subset \mathbb{R}^n$ be a simple path. That is there exists a continuous injective function $f : [0, 1] \rightarrow \mathbb{R}^n$ such that the image of $f$ is $Y$ . Note, by this definition $f(0)$ cannot equal $f(1)$ . Now assume there exists another continuous injective function $g : [0, 1] \rightarrow \mathbb{R}^n$ whose image is also $Y$ . Can $\{f(0), f(1)\} \neq \{g(0), g(1)\}$ ?","Let be a simple path. That is there exists a continuous injective function such that the image of is . Note, by this definition cannot equal . Now assume there exists another continuous injective function whose image is also . Can ?","Y \subset \mathbb{R}^n f : [0, 1] \rightarrow \mathbb{R}^n f Y f(0) f(1) g : [0, 1] \rightarrow \mathbb{R}^n Y \{f(0), f(1)\} \neq \{g(0), g(1)\}","['real-analysis', 'general-topology']"
33,"If $a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}}$ and $\lim\limits_{n\to\infty}a_n=\ell$, prove $\lim_{n\to\infty}[(\ell-a_n)^{1/n}n^{1/2}]=\frac{\sqrt e}2$","If  and , prove",a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}} \lim\limits_{n\to\infty}a_n=\ell \lim_{n\to\infty}[(\ell-a_n)^{1/n}n^{1/2}]=\frac{\sqrt e}2,"$$a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}}$$ We can prove that $\{a_n\}$ is convergent (using mathematical induction, $\sqrt {k+\sqrt{k+1+\cdots\sqrt{n}}}\leq k-1, for \ k\geq3$ ). If $$ \lim\limits_{n\to\infty} a_n=\ell, $$ prove: $$\lim\limits_{n\to\infty} \left[\,(\ell-a_n)^{1/n}\cdot n^{1/2}\,\right]=\frac{\sqrt e}{2}$$","We can prove that is convergent (using mathematical induction, ). If prove:","a_n=\sqrt{1+\sqrt{2+\cdots\sqrt{n}}} \{a_n\} \sqrt {k+\sqrt{k+1+\cdots\sqrt{n}}}\leq k-1, for \ k\geq3 
\lim\limits_{n\to\infty} a_n=\ell,
 \lim\limits_{n\to\infty} \left[\,(\ell-a_n)^{1/n}\cdot n^{1/2}\,\right]=\frac{\sqrt e}{2}","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'functions']"
34,"$C([0,1])$ with norm $\|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}|$ complete?",with norm  complete?,"C([0,1]) \|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}|","Let $(q_n)$ be a sequence of all rational numbers from $[0, 1]$ . On $C([0,1])$ let's consider norm: $$\|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}|$$ I want to check whether norm $\|f\|$ is complete. My work so far Let's take $(f_k) \subset C([0,1 ])$ Cauchy sequence. We want to prove or disaprove that $$\exists{f \in C([0, 1])}: \|f_k - f\|\rightarrow 0 $$ I wanted first to focus on more general problem: $$\exists f: \|f_k - f\| \rightarrow 0$$ and after proving that, we can focus on showing that such $f$ has to be in $C([0, 1])$ . So - because $(f_k)$ is a Cauchy Sequence then: $$\exists_l : \|f_k  - f_l\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} \le \epsilon$$ for any $\epsilon > 0$ $$\|f_k  - f\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f(q_n)|}{2^n}= \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n) + f_l(q_n) - f(q_n)|}{2^n} \le$$ $$\le \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} + \frac{|f_l(q_n) - f(q_n)|}{2^n} \le$$ $$\le \sum_{n=1}^\infty \frac{\epsilon}{2^n} + \frac{\sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}}{2^n} = \epsilon + \sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}$$ . And here I got stuck, because $\sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}$ is not arbitrary small. Do you know how to fix this/could you please give me a hint what is correct approach to this problem","Let be a sequence of all rational numbers from . On let's consider norm: I want to check whether norm is complete. My work so far Let's take Cauchy sequence. We want to prove or disaprove that I wanted first to focus on more general problem: and after proving that, we can focus on showing that such has to be in . So - because is a Cauchy Sequence then: for any . And here I got stuck, because is not arbitrary small. Do you know how to fix this/could you please give me a hint what is correct approach to this problem","(q_n) [0, 1] C([0,1]) \|f\| := \sum_{n=1}^\infty|\frac{f(q_n)}{2^n}| \|f\| (f_k) \subset C([0,1 ]) \exists{f \in C([0, 1])}: \|f_k - f\|\rightarrow 0  \exists f: \|f_k - f\| \rightarrow 0 f C([0, 1]) (f_k) \exists_l : \|f_k  - f_l\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} \le \epsilon \epsilon > 0 \|f_k  - f\| = \sum_{n=1}^\infty \frac{|f_k(q_n) - f(q_n)|}{2^n}= \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n) + f_l(q_n) - f(q_n)|}{2^n} \le \le \sum_{n=1}^\infty \frac{|f_k(q_n) - f_l(q_n)|}{2^n} + \frac{|f_l(q_n) - f(q_n)|}{2^n} \le \le \sum_{n=1}^\infty \frac{\epsilon}{2^n} + \frac{\sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}}{2^n} = \epsilon + \sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \} \sup\{|f_l(q_n) - f(q_n)| : n \in \mathbb{N} \}","['real-analysis', 'functional-analysis', 'convergence-divergence', 'normed-spaces']"
35,"If $f:B\to\mathbb{R},\ B\subset\mathbb{R}$ is increasing then there is a sequence of strictly increasing functions whose pointwise limit is $f$",If  is increasing then there is a sequence of strictly increasing functions whose pointwise limit is,"f:B\to\mathbb{R},\ B\subset\mathbb{R} f","I have proved the following statement and I would like to know if my proof is correct and/or how it could be improved. Suppose $B\subset\mathbb{R}$ and $f:B\to\mathbb{R}$ is an increasing function. Prove that there exists a sequence $f_1, f_2,\dots$ of strictly increasing functions from $B$ to $\mathbb{R}$ such that $f(x)=\lim_{k\to\infty}f_k(x)$ for every $x\in B$ . My proof: The sequence defined by $f_k(x):=f(x)+\frac{x}{k},\ k\geq 1, x\in B$ satisfies the requirements for if we let $k\geq 1$ and $x,y\in B,\ x<y$ then $f_k(y)-f_k(x)=f(y)-f(x)+\frac{y-x}{k}>0$ so $f_k(y)>f_k(x)$ ie $f_k$ is strictly increasing and also, for any $x\in B$ we have $\lim_{k\to\infty}f_k(x)=\lim_{k\to\infty}(f(x)+\frac{x}{k})=\lim_{k\to\infty}f(x)+\lim_{k\to\infty}\frac{x}{k}=f(x)+0=f(x)$ . $\square$","I have proved the following statement and I would like to know if my proof is correct and/or how it could be improved. Suppose and is an increasing function. Prove that there exists a sequence of strictly increasing functions from to such that for every . My proof: The sequence defined by satisfies the requirements for if we let and then so ie is strictly increasing and also, for any we have .","B\subset\mathbb{R} f:B\to\mathbb{R} f_1, f_2,\dots B \mathbb{R} f(x)=\lim_{k\to\infty}f_k(x) x\in B f_k(x):=f(x)+\frac{x}{k},\ k\geq 1, x\in B k\geq 1 x,y\in B,\ x<y f_k(y)-f_k(x)=f(y)-f(x)+\frac{y-x}{k}>0 f_k(y)>f_k(x) f_k x\in B \lim_{k\to\infty}f_k(x)=\lim_{k\to\infty}(f(x)+\frac{x}{k})=\lim_{k\to\infty}f(x)+\lim_{k\to\infty}\frac{x}{k}=f(x)+0=f(x) \square","['real-analysis', 'solution-verification', 'monotone-functions', 'sequence-of-function']"
36,Set of points at which the sequence $(f_n(x))_{n=1}^{\infty}$ converges is $\mathcal{S}$-measurable,Set of points at which the sequence  converges is -measurable,(f_n(x))_{n=1}^{\infty} \mathcal{S},"I have proved the following statement(s) and I would like to know if my proof is correct and/or if it could be improved somehow. (a) Suppose $f_1,f_2,\dots$ is a sequence of functions from a set $X$ to $\mathbb{R}$ . Explain why $\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}=\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ (b) Suppose $(X,\mathcal{S})$ is a measurable space and $f_1,f_2,\dots$ is  sequence of $\mathcal{S}$ -measurable functions from $X$ to $\mathbb{R}$ . Prove that $\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ is an $\mathcal{S}$ -measurable subset of $X$ . My proofs: (a) Let $x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ : then $(f_n(x))_{n=1}^{\infty}$ is a convergent sequence of real numbers so it is also a Cauchy sequence of real numbers which implies* that for every $n\geq 1$ there exists $N\geq 1$ such that $|f_j(x)-f_k(x)|<\frac{1}{n}$ for all $j,k>N$ so if we set $j_{N}:=N+1$ we have that $|f_{j_N}(x)-f_k(x)|<\frac{1}{n}$ for all $k>j_N$ thus $-\frac{1}{n}<f_{j_N}(x)-f_k(x)<\frac{1}{n}\Leftrightarrow (f_{j_N}-f_k)(x)\in (-\frac{1}{n},\frac{1}{n})\Leftrightarrow x\in (f_{j_N}-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ ; since this is valid for every $n\geq 1$ , for a certain $j\geq 1$ (which we have called $j_N$ ), and for all the $k$ s equal or greater than this $j$ it follows that $x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ . Now, let $x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ : then for every $n\geq 1$ there exists some $j\geq 1$ such that for all $k\geq j$ it is $x\in (f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ (which is equivalent to saying that $|f_j(x)-f_k(x)|<\frac{1}{n}$ ) so if we fix $n^*\geq 1$ there is $j^*\geq 1$ such that for all $k\geq j^*,\ |f_j(x)-f_k(x)|<\frac{1}{2n}$ thus $|f_j(x)-f_k(x)|=|f_j(x)-f_{j^*}(x)+f_{j^*}(x)-f_k(x)|\leq |f_j(x)-f_{j^*}(x)|+|f_{j^*}(x)-f_k(x)|<\frac{1}{2n}+\frac{1}{2n}=\frac{1}{n}$ for all $n\geq 1$ and $j,k\geq j^*$ . In summary we have found that for every $n\geq 1$ there exists $N\geq 1$ (the one we called $j^*$ in the previous sentence) such that $|f_j(x)-f_k(x)|<\frac{1}{n}$ for all $j,k\geq N$ so $(f_n(x))_{n=1}^{\infty}$ is a Cauchy sequence* of real numbers and hence convergent thus $x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ . (b) By hypothesis all the $f_n$ are $\mathcal{S}$ -measurable functions so their difference is also a $\mathcal{S}$ -measurable function and since $(-\frac{1}{n},\frac{1}{n})$ is a Borel set for every $n\geq 1$ (being open) the preimages $(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S}$ and being $\mathcal{S}$ a $\sigma$ -algebra it is closed under intersections and unions so $\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S}$ , as desired. Note: in part (a) of the proof I have implicitly used the fact that if $(a_n)_{n=1}^{\infty}$ is a Cauchy sequence of real numbers then for every $n\geq 1$ there exists $j\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ . Proof. Let $n\geq 1$ and set $\varepsilon=\frac{1}{n}$ : then being $(a_n)_{n=1}^{\infty}$ Cauchy we have that there exists $N\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $j,k>N$ so if we set $j:=N+1$ we have $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ and so for $n\geq 1$ we have thus found that there exists $j\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ , and the claim is proved.","I have proved the following statement(s) and I would like to know if my proof is correct and/or if it could be improved somehow. (a) Suppose is a sequence of functions from a set to . Explain why (b) Suppose is a measurable space and is  sequence of -measurable functions from to . Prove that is an -measurable subset of . My proofs: (a) Let : then is a convergent sequence of real numbers so it is also a Cauchy sequence of real numbers which implies* that for every there exists such that for all so if we set we have that for all thus ; since this is valid for every , for a certain (which we have called ), and for all the s equal or greater than this it follows that . Now, let : then for every there exists some such that for all it is (which is equivalent to saying that ) so if we fix there is such that for all thus for all and . In summary we have found that for every there exists (the one we called in the previous sentence) such that for all so is a Cauchy sequence* of real numbers and hence convergent thus . (b) By hypothesis all the are -measurable functions so their difference is also a -measurable function and since is a Borel set for every (being open) the preimages and being a -algebra it is closed under intersections and unions so , as desired. Note: in part (a) of the proof I have implicitly used the fact that if is a Cauchy sequence of real numbers then for every there exists such that for all . Proof. Let and set : then being Cauchy we have that there exists such that for all so if we set we have for all and so for we have thus found that there exists such that for all , and the claim is proved.","f_1,f_2,\dots X \mathbb{R} \{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}=\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n})) (X,\mathcal{S}) f_1,f_2,\dots \mathcal{S} X \mathbb{R} \{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\} \mathcal{S} X x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\} (f_n(x))_{n=1}^{\infty} n\geq 1 N\geq 1 |f_j(x)-f_k(x)|<\frac{1}{n} j,k>N j_{N}:=N+1 |f_{j_N}(x)-f_k(x)|<\frac{1}{n} k>j_N -\frac{1}{n}<f_{j_N}(x)-f_k(x)<\frac{1}{n}\Leftrightarrow (f_{j_N}-f_k)(x)\in (-\frac{1}{n},\frac{1}{n})\Leftrightarrow x\in (f_{j_N}-f_k)^{-1}((-\frac{1}{n},\frac{1}{n})) n\geq 1 j\geq 1 j_N k j x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n})) x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n})) n\geq 1 j\geq 1 k\geq j x\in (f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n})) |f_j(x)-f_k(x)|<\frac{1}{n} n^*\geq 1 j^*\geq 1 k\geq j^*,\ |f_j(x)-f_k(x)|<\frac{1}{2n} |f_j(x)-f_k(x)|=|f_j(x)-f_{j^*}(x)+f_{j^*}(x)-f_k(x)|\leq |f_j(x)-f_{j^*}(x)|+|f_{j^*}(x)-f_k(x)|<\frac{1}{2n}+\frac{1}{2n}=\frac{1}{n} n\geq 1 j,k\geq j^* n\geq 1 N\geq 1 j^* |f_j(x)-f_k(x)|<\frac{1}{n} j,k\geq N (f_n(x))_{n=1}^{\infty} x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\} f_n \mathcal{S} \mathcal{S} (-\frac{1}{n},\frac{1}{n}) n\geq 1 (f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S} \mathcal{S} \sigma \bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S} (a_n)_{n=1}^{\infty} n\geq 1 j\geq 1 |a_j-a_k|<\frac{1}{n} k\geq j n\geq 1 \varepsilon=\frac{1}{n} (a_n)_{n=1}^{\infty} N\geq 1 |a_j-a_k|<\frac{1}{n} j,k>N j:=N+1 |a_j-a_k|<\frac{1}{n} k\geq j n\geq 1 j\geq 1 |a_j-a_k|<\frac{1}{n} k\geq j","['real-analysis', 'measure-theory', 'solution-verification']"
37,Understanding Rudin Theorem 3.3(c) [duplicate],Understanding Rudin Theorem 3.3(c) [duplicate],,"This question already has an answer here : Question about writing proofs for limit (1 answer) Closed 3 years ago . Rudin's Theorem 3.3 states: Suppose $\{s_n\}$ , $\{t_n\}$ are complex sequences, and $\lim\limits_{n \to \infty} s_n = s$ , $\lim\limits_{n \to \infty} t_n = t$ . Then (a) $\lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t$ ; (b) $\lim\limits_{n \to \infty} cs_n = cs$ , $\lim\limits_{n \to \infty} \left(c + s_n\right) = c + s$ , for any number $c$ ; (c) $\lim\limits_{n \to \infty} s_n t_n = st$ ; (d) $\lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s}$ , provided $s_n \neq 0$ ( $n = 1, 2, 3, \ldots$ ), and $s \neq 0$ . I proved (a) and (b) without trouble. I'm trying to understand his proof of (c). He notes that for any $s$ we have: $$  s_n t_n - st = (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s). $$ My first question is: where does this come from? By brute-forcing the right-side, I can tell that this amounts to adding and subtracting $st$ , $ts_n$ , and $st_n$ . But is there another reason why this holds? Rudin calls it an ""identity,' but I've never seen it before. Then, given $\epsilon > 0$ , he takes $N_1, N_2$ , so that $n \geq N_1$ implies $|s_n - s| < \sqrt{\epsilon}$ and $n \geq N_2$ implies $|t_n - t| < \sqrt{\epsilon}$ . So for $n \geq \max(N_1, N_2)$ , we have \begin{align*} |(s_n - s)(t_n - t) - 0| = |s_n - s||t_n - t| < \sqrt{\epsilon} \cdot \sqrt{\epsilon} = \epsilon,  \end{align*} so $\lim\limits_{n \to \infty} (s_n - s)(t_n - t) = 0$ . Rudin then claims that the remaining terms in the first identity converge to $0$ by using $(a)$ and $b$ . The first term converges, as we just proved, and the second and third are scaled versions of $t_n$ and $s_n$ , so they converge. (I'm not sure how to word this exactly, since I need to have convergence first to use this identity, but I'm using the identity to prove they converge, so this is a very circular argument. I'm trying to figure out how best to word it.) With that aside, we have: \begin{align*} \lim\limits_{n \to \infty} [(s_n t_n - st) & = \lim\limits_{n \to \infty}  (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s)] \\ & = \lim\limits_{n \to \infty} (s_n - s)(t_n - t) + \lim\limits_{n \to \infty} s (t_n - t) + \lim\limits_{n \to \infty} t(s_n - s) \\ & = 0 + \lim\limits_{n \to \infty} (st_n - st) + \lim\limits_{n \to \infty} (ts_n - ts) \\ & = s \lim\limits_{n \to \infty}t_n - st + t \lim\limits_{n \to \infty} s_n - ts \\ & = st - st + ts - ts \\ & = 0 \end{align*} So $\lim\limits_{n \to \infty} (s_n t_n - st) = 0$ . I'm trying to figure out exactly how this implies the result, because I can't just invoke part (a)/b to break the limit apart, as that requires convergence of $s_n t_n$ , which is what I need. I think, however, it can be used to prove the fact directly. Given $\epsilon > 0$ , we use this limit to find $N$ so that for $n \geq N$ , we have $|s_n t_n - st| < \epsilon$ for all $n \geq N$ , so $(s_n t_n) \to st$ . As a general principle, am I allowed to just ""break it apart,"" or was that just a coincidence? I would appreciate any help and any feedback on what I've written above.","This question already has an answer here : Question about writing proofs for limit (1 answer) Closed 3 years ago . Rudin's Theorem 3.3 states: Suppose , are complex sequences, and , . Then (a) ; (b) , , for any number ; (c) ; (d) , provided ( ), and . I proved (a) and (b) without trouble. I'm trying to understand his proof of (c). He notes that for any we have: My first question is: where does this come from? By brute-forcing the right-side, I can tell that this amounts to adding and subtracting , , and . But is there another reason why this holds? Rudin calls it an ""identity,' but I've never seen it before. Then, given , he takes , so that implies and implies . So for , we have so . Rudin then claims that the remaining terms in the first identity converge to by using and . The first term converges, as we just proved, and the second and third are scaled versions of and , so they converge. (I'm not sure how to word this exactly, since I need to have convergence first to use this identity, but I'm using the identity to prove they converge, so this is a very circular argument. I'm trying to figure out how best to word it.) With that aside, we have: So . I'm trying to figure out exactly how this implies the result, because I can't just invoke part (a)/b to break the limit apart, as that requires convergence of , which is what I need. I think, however, it can be used to prove the fact directly. Given , we use this limit to find so that for , we have for all , so . As a general principle, am I allowed to just ""break it apart,"" or was that just a coincidence? I would appreciate any help and any feedback on what I've written above.","\{s_n\} \{t_n\} \lim\limits_{n \to \infty} s_n = s \lim\limits_{n \to \infty} t_n = t \lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t \lim\limits_{n \to \infty} cs_n = cs \lim\limits_{n \to \infty} \left(c + s_n\right) = c + s c \lim\limits_{n \to \infty} s_n t_n = st \lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s} s_n \neq 0 n = 1, 2, 3, \ldots s \neq 0 s  
s_n t_n - st = (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s).
 st ts_n st_n \epsilon > 0 N_1, N_2 n \geq N_1 |s_n - s| < \sqrt{\epsilon} n \geq N_2 |t_n - t| < \sqrt{\epsilon} n \geq \max(N_1, N_2) \begin{align*}
|(s_n - s)(t_n - t) - 0| = |s_n - s||t_n - t| < \sqrt{\epsilon} \cdot \sqrt{\epsilon} = \epsilon, 
\end{align*} \lim\limits_{n \to \infty} (s_n - s)(t_n - t) = 0 0 (a) b t_n s_n \begin{align*}
\lim\limits_{n \to \infty} [(s_n t_n - st) & = \lim\limits_{n \to \infty}  (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s)] \\
& = \lim\limits_{n \to \infty} (s_n - s)(t_n - t) + \lim\limits_{n \to \infty} s (t_n - t) + \lim\limits_{n \to \infty} t(s_n - s) \\
& = 0 + \lim\limits_{n \to \infty} (st_n - st) + \lim\limits_{n \to \infty} (ts_n - ts) \\
& = s \lim\limits_{n \to \infty}t_n - st + t \lim\limits_{n \to \infty} s_n - ts \\
& = st - st + ts - ts \\
& = 0
\end{align*} \lim\limits_{n \to \infty} (s_n t_n - st) = 0 s_n t_n \epsilon > 0 N n \geq N |s_n t_n - st| < \epsilon n \geq N (s_n t_n) \to st","['real-analysis', 'sequences-and-series']"
38,Calculate the integral ...,Calculate the integral ...,,We need to find the integral $$\iiiint\limits_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}dxdydudv$$ I was only able to get to this point ... $$\iiiint\limits_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}dxdydudv=\iint\limits_{x^2+y^2\leq 1}e^{x^2+y^2}\left ( \iint\limits_{u^2+v^2\leq 1-x^2-y^2}\frac{dudv}{e^{u^2+v^2}} \right )dxdy$$ I don’t know how to solve it further ...,We need to find the integral I was only able to get to this point ... I don’t know how to solve it further ...,\iiiint\limits_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}dxdydudv \iiiint\limits_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}dxdydudv=\iint\limits_{x^2+y^2\leq 1}e^{x^2+y^2}\left ( \iint\limits_{u^2+v^2\leq 1-x^2-y^2}\frac{dudv}{e^{u^2+v^2}} \right )dxdy,"['real-analysis', 'calculus']"
39,A calculus problem on the poisson equation,A calculus problem on the poisson equation,,"Let $u(x, y)$ be a function that equals its Laplacian ( $u \equiv \Delta u$ ) on the unit disk $\mathbb{D}$ ( $x^2 + y^2 \leq 1$ ) and $u \mid_{\partial \mathbb{D}} \equiv 0$ ( $\partial \mathbb{D}$ is the circle $x^2 + y^2 = 1$ ). Please prove that $u \equiv 0$ on $\mathbb{D}$ . Note: This problem is taken from the Ph. D. qualification test of Peking University, 2019. My failed attempts: Construct a function $f$ related to $u$ such that $\Delta f = 0$ , and apply the Maximum Principle of harmonic functions. Consider the integral $F(r) = \int_0^{2\pi} u(x + r\cos\theta, y + r\sin\theta) \, d\theta$ and get $rF(r) = rF''(r) + F'(r)$ but fail to continue. Consider the Fourier transform of $u$ . This problem might be easy for the scholars who have a profound expertise in functional analysis and PDE (However, I do not). Would you please offer several hints or point out the insights behind this problem? 50 reputation will be awarded if you can prove the case where $u \in C^\infty(\mathbb{R}^2)$ . I will keep my promise like How to calculate the Fourier transform of the Kaiser-Bessel window? .","Let be a function that equals its Laplacian ( ) on the unit disk ( ) and ( is the circle ). Please prove that on . Note: This problem is taken from the Ph. D. qualification test of Peking University, 2019. My failed attempts: Construct a function related to such that , and apply the Maximum Principle of harmonic functions. Consider the integral and get but fail to continue. Consider the Fourier transform of . This problem might be easy for the scholars who have a profound expertise in functional analysis and PDE (However, I do not). Would you please offer several hints or point out the insights behind this problem? 50 reputation will be awarded if you can prove the case where . I will keep my promise like How to calculate the Fourier transform of the Kaiser-Bessel window? .","u(x, y) u \equiv \Delta u \mathbb{D} x^2 + y^2 \leq 1 u \mid_{\partial \mathbb{D}} \equiv 0 \partial \mathbb{D} x^2 + y^2 = 1 u \equiv 0 \mathbb{D} f u \Delta f = 0 F(r) = \int_0^{2\pi} u(x + r\cos\theta, y + r\sin\theta) \, d\theta rF(r) = rF''(r) + F'(r) u u \in C^\infty(\mathbb{R}^2)","['real-analysis', 'calculus', 'functional-analysis', 'partial-differential-equations']"
40,"Clever substitution on evaluating $\int_{0}^{\infty} \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx$",Clever substitution on evaluating,"\int_{0}^{\infty} \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx","I've been working on evaluating some integrals that come up when studying Fresnel integrals, and I've come across this one: $$\int_{0}^{t^2}  \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx$$ and immediately it reminded me of the famous integral: $$\int_{0}^{\infty} \frac{\sin{x}}{x} \, dx = \frac{\pi}{2}$$ with which I dared to conjecture that: $$\lim_{t\rightarrow \infty}{\int_{0}^{t^2}  \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx}=\frac{\pi}{4}$$ I have tried to do substitutions without success, I also tried to use something like the dominated convergence theorem to swap the integral with the limit and I think it may work, but, will there be some substitution that allows me to evaluate the integral without using something like the dominated convergence theorem ? Thank you all","I've been working on evaluating some integrals that come up when studying Fresnel integrals, and I've come across this one: and immediately it reminded me of the famous integral: with which I dared to conjecture that: I have tried to do substitutions without success, I also tried to use something like the dominated convergence theorem to swap the integral with the limit and I think it may work, but, will there be some substitution that allows me to evaluate the integral without using something like the dominated convergence theorem ? Thank you all","\int_{0}^{t^2}  \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx \int_{0}^{\infty} \frac{\sin{x}}{x} \, dx = \frac{\pi}{2} \lim_{t\rightarrow \infty}{\int_{0}^{t^2}  \frac{\sin(2x+\frac{x^2}{t^2})}{2x+\frac{x^2}{t^2}} \, dx}=\frac{\pi}{4}","['real-analysis', 'calculus', 'definite-integrals', 'improper-integrals']"
41,Prove $\lim\limits_{n\to\infty}(\frac{a}{n})=0$,Prove,\lim\limits_{n\to\infty}(\frac{a}{n})=0,"I'm very new to the epsilon definition of a limit and I was hoping I could get some feedback to see if I have the right idea. For any $a\in\mathbb{R}$ , $$\lim\limits_{n\to\infty}\left(\frac{a}{n}\right)=0$$ Proof: Let $\epsilon >0$ . Choose $N>\frac{|a|}{\epsilon}$ . Then for all $n\geq N$ , since $n\geq N>\frac{|a|}{\epsilon}$ , we have $$n>\frac{|a|}{\epsilon}\;\;\;\Rightarrow\;\;\;\epsilon >\frac{|a|}{n}$$ Therefore, $$\left|\frac{a}{n}-0\right|=\frac{|a|}{n}<\epsilon$$ $\blacksquare$","I'm very new to the epsilon definition of a limit and I was hoping I could get some feedback to see if I have the right idea. For any , Proof: Let . Choose . Then for all , since , we have Therefore,",a\in\mathbb{R} \lim\limits_{n\to\infty}\left(\frac{a}{n}\right)=0 \epsilon >0 N>\frac{|a|}{\epsilon} n\geq N n\geq N>\frac{|a|}{\epsilon} n>\frac{|a|}{\epsilon}\;\;\;\Rightarrow\;\;\;\epsilon >\frac{|a|}{n} \left|\frac{a}{n}-0\right|=\frac{|a|}{n}<\epsilon \blacksquare,"['real-analysis', 'sequences-and-series', 'solution-verification']"
42,Bound the absolute value of the partial sums of $\sum \frac{\sin(nx)}{n}$,Bound the absolute value of the partial sums of,\sum \frac{\sin(nx)}{n},"I would like to prove that for every $x \in \mathbb{R}, n$ natural, we have $$\!\left|\sin(x)+\frac{\sin(2x)}{2}+\ldots+\frac{\sin{(nx)}}{n}\right| \le \int_{0}^{\pi}\frac{\sin x}{x}\,dx$$ We can of course restrict our attention for $x \in [0, 2\pi]$ . So far I tried a few things: letting $S_n(x)$ denote the quantity inside the absolute value, we have $$S_n'(x) = \sum_{k=1}^{n}\cos(kx) = -\frac1{2}+\frac{\sin(n+\frac12)x}{2\sin\frac{x}{2}}$$ and hence $S_n(x)$ has critical points at $x = \frac{2k\pi}{n}$ or $x = \frac{(2k+1)\pi}{n+1}$ where $k$ is an integer. Moreover: $$S_n(x) = -\frac{x}{2}+\int_{0}^{x}\frac{\sin(n+\frac12)t}{2\sin\frac{t}{2}}\,dt$$ I also know that the function $F(x) = \int_{0}^{x}\frac{\sin(t)}{t}\,dt$ has an absolute maximum for $x = \pi$ . I struggle to bound the integral in the expression for $S_n$ . Can you help me? Or maybe there's a different approach to solve this?","I would like to prove that for every natural, we have We can of course restrict our attention for . So far I tried a few things: letting denote the quantity inside the absolute value, we have and hence has critical points at or where is an integer. Moreover: I also know that the function has an absolute maximum for . I struggle to bound the integral in the expression for . Can you help me? Or maybe there's a different approach to solve this?","x \in \mathbb{R}, n \!\left|\sin(x)+\frac{\sin(2x)}{2}+\ldots+\frac{\sin{(nx)}}{n}\right| \le \int_{0}^{\pi}\frac{\sin x}{x}\,dx x \in [0, 2\pi] S_n(x) S_n'(x) = \sum_{k=1}^{n}\cos(kx) = -\frac1{2}+\frac{\sin(n+\frac12)x}{2\sin\frac{x}{2}} S_n(x) x = \frac{2k\pi}{n} x = \frac{(2k+1)\pi}{n+1} k S_n(x) = -\frac{x}{2}+\int_{0}^{x}\frac{\sin(n+\frac12)t}{2\sin\frac{t}{2}}\,dt F(x) = \int_{0}^{x}\frac{\sin(t)}{t}\,dt x = \pi S_n","['real-analysis', 'integration', 'sequences-and-series']"
43,"Find all differentiable functions $f : [0, +\infty) \rightarrow \mathbb{R}$ such that $f(1) = 1$ and $f(x)f(y) \leq f(xy)$",Find all differentiable functions  such that  and,"f : [0, +\infty) \rightarrow \mathbb{R} f(1) = 1 f(x)f(y) \leq f(xy)","The exercise Find all differentiable functions $f : [0, +\infty) \rightarrow \mathbb{R}$ such that: $f(1) = 1$ $f(x)f(y) \leq f(xy)$ for all $x, y \geq 0$ My try I have found that the functions $x \mapsto x^\alpha$ for $\alpha \in [1, +\infty)$ are solutions. The constant function $x \mapsto 1$ is also solution. As $f(0)^2 \leq f(0)$ one also has that $f(0) \in [0, 1]$ . If I take a solution $f$ , my idea would be to try proving it is of the form $f(x) = x^\alpha$ for some $\alpha$ . Necessarily this $\alpha$ would be equal to $f'(1)$ . So I set $\alpha := f'(1)$ and considered $g(x) = \frac{f(x)}{x^\alpha}$ for $x > 0$ . One can show $g$ is derivable and $g'(1) = 0$ and $g$ verifies the functional equation (for $x, y > 0$ ). So the goal would be to show that $g = 1$ , but I've not managed to prove it yet. Any help is welcome folks!","The exercise Find all differentiable functions such that: for all My try I have found that the functions for are solutions. The constant function is also solution. As one also has that . If I take a solution , my idea would be to try proving it is of the form for some . Necessarily this would be equal to . So I set and considered for . One can show is derivable and and verifies the functional equation (for ). So the goal would be to show that , but I've not managed to prove it yet. Any help is welcome folks!","f : [0, +\infty) \rightarrow \mathbb{R} f(1) = 1 f(x)f(y) \leq f(xy) x, y \geq 0 x \mapsto x^\alpha \alpha \in [1, +\infty) x \mapsto 1 f(0)^2 \leq f(0) f(0) \in [0, 1] f f(x) = x^\alpha \alpha \alpha f'(1) \alpha := f'(1) g(x) = \frac{f(x)}{x^\alpha} x > 0 g g'(1) = 0 g x, y > 0 g = 1","['real-analysis', 'derivatives', 'functional-equations']"
44,Ideals of ring of continuous functions on a compact Hausdorff space.,Ideals of ring of continuous functions on a compact Hausdorff space.,,"I am curious to know if the following question can be answered by my naive approach. I understand that there is a huge body of work in rings of continuous function and some really deep ideas. Consider the ring of continuous functions on a compact Hausdorff space $X$ (taking $X=[0,1]$ is also fine as far as this question is concerned), denoted $C(X)$ . Let $I$ be a proper ideal. Is $I=I(S)$ for some $S$ , where $I(S):=\{f\in C(X)\mid  f(s)=0\, \forall s\in S\}$ ?? My naive approach: We know that $I$ is contained in some maximal ideals which are of the form $I(p)$ for some point $p\in X$ according to the above notation. We take all such maximal ideals containing $I$ and intersect them. So $I\subset \bigcap I(p)=I(\bigcup \{p\})$ . I was hoping to show an equality here, but I am not sure how to approach this or whether it is even true that they are equal?? I wonder if taking $f\in \bigcap I(p)$ and $f\notin I$ leads to a contradiction by breaking some maximality condition; like now we have $I\subset (I,f)\subset I(\bigcup \{p\})$ an so on... Any help will be appreciated. Thanks in advance.","I am curious to know if the following question can be answered by my naive approach. I understand that there is a huge body of work in rings of continuous function and some really deep ideas. Consider the ring of continuous functions on a compact Hausdorff space (taking is also fine as far as this question is concerned), denoted . Let be a proper ideal. Is for some , where ?? My naive approach: We know that is contained in some maximal ideals which are of the form for some point according to the above notation. We take all such maximal ideals containing and intersect them. So . I was hoping to show an equality here, but I am not sure how to approach this or whether it is even true that they are equal?? I wonder if taking and leads to a contradiction by breaking some maximality condition; like now we have an so on... Any help will be appreciated. Thanks in advance.","X X=[0,1] C(X) I I=I(S) S I(S):=\{f\in C(X)\mid  f(s)=0\, \forall s\in S\} I I(p) p\in X I I\subset \bigcap I(p)=I(\bigcup \{p\}) f\in \bigcap I(p) f\notin I I\subset (I,f)\subset I(\bigcup \{p\})","['real-analysis', 'general-topology', 'ring-theory', 'ideals']"
45,Supremum of $\sin(x) + \sin(\varphi x)$,Supremum of,\sin(x) + \sin(\varphi x),"Let $\varphi$ be the golden ratio. What is the supremum of $f(x)=\sin(x) + \sin(\varphi x)$ ? My idea was that we can consider the sequence of functions $f_n(x) = \sin(x) + \sin(\frac{F_{n+1}}{F_n} x)$ for the Fibonacci numbers $F_n$ , which converges to $f(x)$ . The sequence $\{x_n\}_{n \geq 1}=F_n \pi + a_n$ satisfies $f_n(x_n) = \sin(a_n) + \sin(F_n a_n)$ , so that if we choose the correct $a_n$ , then $f_n(x_n) =2$ $\forall n$ ; which changes the problem to one with an integer instead of $\varphi$ , but I'm not sure where to proceed from here.","Let be the golden ratio. What is the supremum of ? My idea was that we can consider the sequence of functions for the Fibonacci numbers , which converges to . The sequence satisfies , so that if we choose the correct , then ; which changes the problem to one with an integer instead of , but I'm not sure where to proceed from here.",\varphi f(x)=\sin(x) + \sin(\varphi x) f_n(x) = \sin(x) + \sin(\frac{F_{n+1}}{F_n} x) F_n f(x) \{x_n\}_{n \geq 1}=F_n \pi + a_n f_n(x_n) = \sin(a_n) + \sin(F_n a_n) a_n f_n(x_n) =2 \forall n \varphi,"['real-analysis', 'trigonometry', 'maxima-minima']"
46,Showing that $x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)}$ for all $x>1$,Showing that  for all,x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)} x>1,"When trying to come up with an asymptotic formula for the partial sums $\sum_{n\leq x}(\sqrt[n]{n}-1)$ , I came across the following inequality which seems to be to holding for all $x>1$ : \begin{equation}\tag{$*$} x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)}. \label{eq:special} \end{equation} I am unable to prove this, but numerical evidences suggest that it is correct. I have only been able to prove a weaker inequality for all $x>1$ $$x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{ex}.$$ This is obtained by rewriting $x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\frac{1}{x}\int_{1}^{x}\frac{t^{1/x}-1}{t}dt$ and noticing that for all $x>1$ and $t>0$ , we have (by computing the derivative of $f(t)=\frac{t^{1/x}-1}{t}$ and solving $f'(t)=0$ ), $$\frac{t^{1/x}-1}{t}\leq\frac{(x-1)^{x-1}}{x^x}=\frac{1}{x-1}\left(1-\frac{1}{x}\right)^{x}\leq\frac{1}{(x-1)e}.$$ Something that also comes to mind when dealing with \eqref{eq:special} is to write \begin{equation} x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\exp{\left(\frac{\ln(x)}{x}\right)}-1-\frac{\ln(x)}{x}=\sum_{k=2}^{+\infty}\frac{1}{k!}\frac{\ln^{k}(x)}{x^{k}} \end{equation} and try to bound the infinite sum by $\frac{1}{x\ln(x)}$ . This however didn't help me prove the inequality. Does anyone have any hints on how to prove \eqref{eq:special}.","When trying to come up with an asymptotic formula for the partial sums , I came across the following inequality which seems to be to holding for all : I am unable to prove this, but numerical evidences suggest that it is correct. I have only been able to prove a weaker inequality for all This is obtained by rewriting and noticing that for all and , we have (by computing the derivative of and solving ), Something that also comes to mind when dealing with \eqref{eq:special} is to write and try to bound the infinite sum by . This however didn't help me prove the inequality. Does anyone have any hints on how to prove \eqref{eq:special}.","\sum_{n\leq x}(\sqrt[n]{n}-1) x>1 \begin{equation}\tag{*}
x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{x\ln(x)}.
\label{eq:special}
\end{equation} x>1 x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}<\frac{1}{ex}. x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\frac{1}{x}\int_{1}^{x}\frac{t^{1/x}-1}{t}dt x>1 t>0 f(t)=\frac{t^{1/x}-1}{t} f'(t)=0 \frac{t^{1/x}-1}{t}\leq\frac{(x-1)^{x-1}}{x^x}=\frac{1}{x-1}\left(1-\frac{1}{x}\right)^{x}\leq\frac{1}{(x-1)e}. \begin{equation}
x^{\frac{1}{x}}-1-\frac{\ln(x)}{x}=\exp{\left(\frac{\ln(x)}{x}\right)}-1-\frac{\ln(x)}{x}=\sum_{k=2}^{+\infty}\frac{1}{k!}\frac{\ln^{k}(x)}{x^{k}}
\end{equation} \frac{1}{x\ln(x)}","['real-analysis', 'inequality', 'logarithms']"
47,Proving Brouwer's fixed point theorem in $\mathbb{R}$,Proving Brouwer's fixed point theorem in,\mathbb{R},"Brouwer's fixed point theorem says that any continuous function $f$ mapping a compact convex set $\mathbb{\Omega}$ to itself has a fixed point. The other day I was reading a piece in a pop-sci magazine, which asked to prove the theorem in $\mathbb{R}$ . I got this: could you please confirm if it's correct, and in case it's not, help me find a valid proof? Also, if you have a more elegant proof, I'd love to hear about it. Thanks! A compact convex set in $\mathbb{R}$ is a closed interval, so I need to prove this for $\mathbb{\Omega}=[a,b]$ . Now, let's consider $g(x)=f(x)-x$ , which is also continuous. We assume $g(a)\cdot g(b)\neq0$ , otherwise we already found a fixed point. Of course, we must have $g(a)>0$ , otherwise we would have $f(a)<a$ which contradicts the hypothesis that $f$ maps $[a,b]$ into itself. Similarly, we must have $g(b)<0$ . Since $g(a)\cdot g(b)<0$ , then by Bolzano's theorem there is at least one $x_0\in[a,b]$ such that $g(x_0)=0\implies f(x_0)=x_0\ \square$","Brouwer's fixed point theorem says that any continuous function mapping a compact convex set to itself has a fixed point. The other day I was reading a piece in a pop-sci magazine, which asked to prove the theorem in . I got this: could you please confirm if it's correct, and in case it's not, help me find a valid proof? Also, if you have a more elegant proof, I'd love to hear about it. Thanks! A compact convex set in is a closed interval, so I need to prove this for . Now, let's consider , which is also continuous. We assume , otherwise we already found a fixed point. Of course, we must have , otherwise we would have which contradicts the hypothesis that maps into itself. Similarly, we must have . Since , then by Bolzano's theorem there is at least one such that","f \mathbb{\Omega} \mathbb{R} \mathbb{R} \mathbb{\Omega}=[a,b] g(x)=f(x)-x g(a)\cdot g(b)\neq0 g(a)>0 f(a)<a f [a,b] g(b)<0 g(a)\cdot g(b)<0 x_0\in[a,b] g(x_0)=0\implies f(x_0)=x_0\ \square","['real-analysis', 'fixed-point-theorems']"
48,Importance of Tannery's theorem,Importance of Tannery's theorem,,"Tannery's theorem : Let $S_n=\sum_{k=0}^\infty a_k(n)$ and $\lim_{n\to\infty}a_k(n)=b_k$ . If $|a_k(n)|\le M_k$ and $\sum_{k=0}^\infty M_k\lt\infty$ , then $\lim_{n\to\infty}S_n=\sum_{k=0}^\infty b_k$ . This can be used to prove that $$\lim_{n\to\infty}\sum_{k=0}^n \frac{x^k}{k!}\prod_{m=1}^{k-1}\left(1-\frac{m}{n}\right) =\sum_{k=0}^\infty \frac{x^k}{k!}.$$ But I've never really understood why is the theorem ""needed"", as $\prod_{m=1}^{k-1}\left(1-\frac{m}{n}\right)$ tends to $1$ with increasing $n$ , so the result should be obvious. What could go wrong here? Could someone provide a counterexample to this reasoning?","Tannery's theorem : Let and . If and , then . This can be used to prove that But I've never really understood why is the theorem ""needed"", as tends to with increasing , so the result should be obvious. What could go wrong here? Could someone provide a counterexample to this reasoning?",S_n=\sum_{k=0}^\infty a_k(n) \lim_{n\to\infty}a_k(n)=b_k |a_k(n)|\le M_k \sum_{k=0}^\infty M_k\lt\infty \lim_{n\to\infty}S_n=\sum_{k=0}^\infty b_k \lim_{n\to\infty}\sum_{k=0}^n \frac{x^k}{k!}\prod_{m=1}^{k-1}\left(1-\frac{m}{n}\right) =\sum_{k=0}^\infty \frac{x^k}{k!}. \prod_{m=1}^{k-1}\left(1-\frac{m}{n}\right) 1 n,"['real-analysis', 'sequences-and-series', 'exponential-function']"
49,A property of the floor function $x\mapsto \lfloor x\rfloor$,A property of the floor function,x\mapsto \lfloor x\rfloor,"This is problem is from Vinogradov's elementary number theory book. For any real number $a>0$ , define $f_a:\mathbb{N}\rightarrow\mathbb{Z_+}$ by $x\mapsto \lfloor ax\rfloor$ . Given $\alpha,\beta>0$ , show that $f_\alpha$ , $f_\beta$ are injective, $f_\alpha(\mathbb {N})\cap f_\beta(\mathbb{N})=\emptyset$ , and $\mathbb{N}=f_\alpha(\mathbb{N})\cup f_\beta(\mathbb{N})$ if and only if $\alpha,\beta\in\mathbb{R}\setminus\mathbb{Q}$ and $\frac1\alpha + \frac1\beta =1$ Sufficiency is not difficult to prove. The part I am struggling with is necessity, where I don't seem able to control the gaps between values of $f_\alpha$ (or $f_\beta$ for that matter.) Any hints will be appreciated.","This is problem is from Vinogradov's elementary number theory book. For any real number , define by . Given , show that , are injective, , and if and only if and Sufficiency is not difficult to prove. The part I am struggling with is necessity, where I don't seem able to control the gaps between values of (or for that matter.) Any hints will be appreciated.","a>0 f_a:\mathbb{N}\rightarrow\mathbb{Z_+} x\mapsto \lfloor ax\rfloor \alpha,\beta>0 f_\alpha f_\beta f_\alpha(\mathbb {N})\cap f_\beta(\mathbb{N})=\emptyset \mathbb{N}=f_\alpha(\mathbb{N})\cup f_\beta(\mathbb{N}) \alpha,\beta\in\mathbb{R}\setminus\mathbb{Q} \frac1\alpha + \frac1\beta =1 f_\alpha f_\beta","['real-analysis', 'elementary-number-theory']"
50,Integrable function $f$ such that $\int_I f(x)dx=0$ for intervals of arbitrarily small length.,Integrable function  such that  for intervals of arbitrarily small length.,f \int_I f(x)dx=0,"A past qual question from my university reads: Let $f$ be an integrable function satisfying $\int_0^1 f(x)dx=0$ . Prove that there are intervals $I$ of arbitrarily small positive length such that $$\int_I f(x)dx=0$$ I'm not sure how to approach the problem. One has that $\nu(E)=\int_E f(x)dx$ is a signed measure with a Hahn decomposition of $[0,1]=P\cup N$ where $f\geq 0$ on P and $f\leq 0$ on N. But I can't seem to be able to come up with a way of finding an interval with the desired property.",A past qual question from my university reads: Let be an integrable function satisfying . Prove that there are intervals of arbitrarily small positive length such that I'm not sure how to approach the problem. One has that is a signed measure with a Hahn decomposition of where on P and on N. But I can't seem to be able to come up with a way of finding an interval with the desired property.,"f \int_0^1 f(x)dx=0 I \int_I f(x)dx=0 \nu(E)=\int_E f(x)dx [0,1]=P\cup N f\geq 0 f\leq 0","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
51,Calculate $\int \left(1+\ln \left(1+\ln (...+\left(1+ \ln(x))\right)\right)\right) dx$.,Calculate .,\int \left(1+\ln \left(1+\ln (...+\left(1+ \ln(x))\right)\right)\right) dx,"This is not particularly a useful integral or one asked in an exam or anything. I just really enjoy doing random integrals and derivatives. With that being said, how can I find: $$\int \left(1+\ln \left(1+\ln (...+\left(1+ \ln(x))\right)\right)\right) dx$$ I tried to star with the simple case of: $$\int \left(1+\ln(x)\right)dx=x\ln \left(x\right)+C$$ I then introduced the first nest. Here I did Integration by Parts with $u=1+\ln \left(1+\ln \left(x\right)\right), v'=1$ : $$\int \left(1+\ln \left(1+\ln \left(x\right)\right)\right)dx=x\left(1+\ln \left(1+\ln \left(x\right)\right)\right)-\frac{1}{e}\text{Ei}\left(\ln \left(x\right)+1\right)+C$$ However, these take quite some time and it's only the first nest. Therefore, I'm curious as to if $(a)$ such an integral does have solution and $(b)$ how to derive that situation. PS - Again this is just a fun question and not from an exam, website or anywhere important.","This is not particularly a useful integral or one asked in an exam or anything. I just really enjoy doing random integrals and derivatives. With that being said, how can I find: I tried to star with the simple case of: I then introduced the first nest. Here I did Integration by Parts with : However, these take quite some time and it's only the first nest. Therefore, I'm curious as to if such an integral does have solution and how to derive that situation. PS - Again this is just a fun question and not from an exam, website or anywhere important.","\int \left(1+\ln \left(1+\ln (...+\left(1+ \ln(x))\right)\right)\right) dx \int \left(1+\ln(x)\right)dx=x\ln \left(x\right)+C u=1+\ln \left(1+\ln \left(x\right)\right), v'=1 \int \left(1+\ln \left(1+\ln \left(x\right)\right)\right)dx=x\left(1+\ln \left(1+\ln \left(x\right)\right)\right)-\frac{1}{e}\text{Ei}\left(\ln \left(x\right)+1\right)+C (a) (b)","['real-analysis', 'integration', 'complex-analysis', 'logarithms', 'indefinite-integrals']"
52,Evaluating $\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx$,Evaluating,\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx,"I was able to evaluate this using Feynman's trick and managed to find a closed form though it has strict conditions that make this integral converge but the main thing is how can one evaluate this using other techniques? i find it very hard to come up with anything else, this time ill checkmark the best approach in my opinion. My attempt. To find the closed form of the integral i heavily relied on the following identity, $$\int _0^{\infty }x^ne^{-ax^b}\:dx=\frac{\Gamma \left(\frac{n+1}{b}\right)}{b\:a^{\frac{n+1}{b}}}$$ Now resuming on the integral. $$I\left(a\right)=\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx$$ $$I'\left(a\right)=-\int _0^{\infty }x^{m-p}\:e^{-ax^m}\:dx$$ $$I'\left(a\right)=-\frac{\Gamma \left(\frac{1-p}{m}+1\right)}{m\:a^{\frac{1-p}{m}+1}}$$ $$\int _{\infty }^aI'\left(a\right)\:da=-\frac{\Gamma \left(\frac{1-p}{m}+1\right)}{m}\int _{\infty }^aa^{\frac{p-1}{m}-1}\:da$$ We can also use the same identity we used earlier to calculate $I\left(\infty \right)$ so, $$I\left(\infty \right)=-\int _0^{\infty }x^{-p}e^{-bx^n}dx=-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n\:b^{\frac{1-p}{n}}}$$ Resuming on the original expression we now have: $$I\left(a\right)+\frac{\Gamma \left(\frac{1-p}{n}\right)}{n\:b^{\frac{1-p}{n}}}=-\left(\frac{1-p}{m}\right)\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\left(\frac{m}{p-1}\:a^{\frac{p-1}{m}}\right)$$ $$I\left(a\right)=\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\:a^{\frac{p-1}{m}}-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n}\:b^{\frac{p-1}{n}}$$ Meaning that: $$\boxed{I\left(a\right)=\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx=\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\:a^{\frac{p-1}{m}}-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n}\:b^{\frac{p-1}{n}}}$$ I tried using this to calculate for some values and in all cases it agrees with mathematica even when the integral diverges. Noticed immediately after posting that i couldve bring the $x^p$ up and use the same identity not having to go through all of Feynman's trick, -.- at least its more fancy.","I was able to evaluate this using Feynman's trick and managed to find a closed form though it has strict conditions that make this integral converge but the main thing is how can one evaluate this using other techniques? i find it very hard to come up with anything else, this time ill checkmark the best approach in my opinion. My attempt. To find the closed form of the integral i heavily relied on the following identity, Now resuming on the integral. We can also use the same identity we used earlier to calculate so, Resuming on the original expression we now have: Meaning that: I tried using this to calculate for some values and in all cases it agrees with mathematica even when the integral diverges. Noticed immediately after posting that i couldve bring the up and use the same identity not having to go through all of Feynman's trick, -.- at least its more fancy.",\int _0^{\infty }x^ne^{-ax^b}\:dx=\frac{\Gamma \left(\frac{n+1}{b}\right)}{b\:a^{\frac{n+1}{b}}} I\left(a\right)=\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx I'\left(a\right)=-\int _0^{\infty }x^{m-p}\:e^{-ax^m}\:dx I'\left(a\right)=-\frac{\Gamma \left(\frac{1-p}{m}+1\right)}{m\:a^{\frac{1-p}{m}+1}} \int _{\infty }^aI'\left(a\right)\:da=-\frac{\Gamma \left(\frac{1-p}{m}+1\right)}{m}\int _{\infty }^aa^{\frac{p-1}{m}-1}\:da I\left(\infty \right) I\left(\infty \right)=-\int _0^{\infty }x^{-p}e^{-bx^n}dx=-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n\:b^{\frac{1-p}{n}}} I\left(a\right)+\frac{\Gamma \left(\frac{1-p}{n}\right)}{n\:b^{\frac{1-p}{n}}}=-\left(\frac{1-p}{m}\right)\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\left(\frac{m}{p-1}\:a^{\frac{p-1}{m}}\right) I\left(a\right)=\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\:a^{\frac{p-1}{m}}-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n}\:b^{\frac{p-1}{n}} \boxed{I\left(a\right)=\int _0^{\infty }\frac{e^{-ax^m}-e^{-bx^n}}{x^p}\:dx=\frac{\Gamma \left(\frac{1-p}{m}\right)}{m}\:a^{\frac{p-1}{m}}-\frac{\Gamma \left(\frac{1-p}{n}\right)}{n}\:b^{\frac{p-1}{n}}} x^p,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'improper-integrals']"
53,How to improve $\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{4}$,How to improve,\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{4},"I have proved this inequality $\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{4}$ . Using $\left|\sin(nx)\right|\leq n\left|\sin(x)\right|$ on $[0,\frac{\pi}{2n}]$ and $\frac{\left|\sin(nx)\right|}{\left|\sin(x)\right|}\leq\frac{\pi}{2x}$ on $[\frac{\pi}{2n},\frac{\pi}{2}]$ ,we can have $$\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{8}+\frac{\pi^{2}}{8}\left(n^{2}-1\right)<\frac{\pi^{2}n^{2}}{4}.$$ But using mathematica I found this inequality can still be improved. And after calculating some terms I found it seems that when $n\geq 2$ we can have $$\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{8}.$$ But I cannot prove this.So is there any method to improve my result?Any help will be thanked.","I have proved this inequality . Using on and on ,we can have But using mathematica I found this inequality can still be improved. And after calculating some terms I found it seems that when we can have But I cannot prove this.So is there any method to improve my result?Any help will be thanked.","\int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{4} \left|\sin(nx)\right|\leq n\left|\sin(x)\right| [0,\frac{\pi}{2n}] \frac{\left|\sin(nx)\right|}{\left|\sin(x)\right|}\leq\frac{\pi}{2x} [\frac{\pi}{2n},\frac{\pi}{2}] \int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{8}+\frac{\pi^{2}}{8}\left(n^{2}-1\right)<\frac{\pi^{2}n^{2}}{4}. n\geq 2 \int_{0}^{\frac{\pi}{2}}x\left(\frac{\sin(nx)}{\sin(x)}\right)^{4}dx<\frac{\pi^{2}n^{2}}{8}.","['real-analysis', 'calculus', 'integration', 'inequality']"
54,Prove that a parametric curve does not intersect itself,Prove that a parametric curve does not intersect itself,,"Consider the following parametric curve $(x(t), y(t))_{t \in \mathbb{R}_+}$ : $$x(t) = \frac{(1+e)(1 - \cos(t)) - t(e-1) \sin (t)}{e t^2 - t^2 \cos(t) - t \sin(t)} $$ $$y(t) = \frac{e(1+t^2)(1 - \cos(t))}{e t^2 - t^2 \cos(t) - t \sin(t)}, $$ where $e = \exp(1)$ . How to prove that this curve has no multiple points, except the two ones located on the axis $x = 0$ ?","Consider the following parametric curve : where . How to prove that this curve has no multiple points, except the two ones located on the axis ?","(x(t), y(t))_{t \in \mathbb{R}_+} x(t) = \frac{(1+e)(1 - \cos(t)) - t(e-1) \sin (t)}{e t^2 - t^2 \cos(t) - t \sin(t)}  y(t) = \frac{e(1+t^2)(1 - \cos(t))}{e t^2 - t^2 \cos(t) - t \sin(t)},  e = \exp(1) x = 0","['real-analysis', 'curves', 'plane-curves']"
55,Problem involving maximum and minimum of a continuous function,Problem involving maximum and minimum of a continuous function,,"Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function. We denote, for every $x\in\mathbb{R}$ , with $m(x)$ and $M(x)$ the minimum and maximum of $f$ on the interval $[x-1,x]$ . Show that, if $m(x)+M(x)=0,\forall x\in\mathbb{R}$ , then the functions $m$ and $M$ are constant. I have tried to prove that $f$ is periodic, but I didn't managed to do it; I'm not sure it's even true...","Let be a continuous function. We denote, for every , with and the minimum and maximum of on the interval . Show that, if , then the functions and are constant. I have tried to prove that is periodic, but I didn't managed to do it; I'm not sure it's even true...","f:\mathbb{R}\to\mathbb{R} x\in\mathbb{R} m(x) M(x) f [x-1,x] m(x)+M(x)=0,\forall x\in\mathbb{R} m M f","['real-analysis', 'analysis']"
56,For $p\geqslant 1$ there is some $f\in L^1$ s.t. $\sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty $,For  there is some  s.t.,p\geqslant 1 f\in L^1 \sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty ,"Im having trouble with this exercise. Show that for any chosen $p\geqslant 1$ there is some $f\in L^1((-\pi,\pi])$ such that $\sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty $ There $$ \hat f(k):=\frac1{2\pi}\int_{-\pi }^{\pi }f(x)e^{-ikt}\,\mathrm d t\tag1 $$ is the classical Fourier coefficient. Let $f(z):=\sum_{k\geqslant 1}\frac1{k^r} z^k$ , then $f\in L^2(\partial \Bbb D )$ when $r>1/2$ , and because for any space of finite measure we knows that $L^p\subset L^q$ for $p>q$ , then $f\in L^1(\partial \Bbb D )$ also and $\sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty $ whenever $rp\leqslant 1$ , what happens when $p\in[1,1/r]$ , and so we proved the case for all $p\in[1,2)$ . However Im having trouble finding a way to show the statement for $p\geqslant 2$ . I tried to find some lower bound for $|\hat f(k)|^p$ (or it sum) using Jensen's inequality and similar ideas but I dont find something useful.  Some help will be appreciated, thank you.","Im having trouble with this exercise. Show that for any chosen there is some such that There is the classical Fourier coefficient. Let , then when , and because for any space of finite measure we knows that for , then also and whenever , what happens when , and so we proved the case for all . However Im having trouble finding a way to show the statement for . I tried to find some lower bound for (or it sum) using Jensen's inequality and similar ideas but I dont find something useful.  Some help will be appreciated, thank you.","p\geqslant 1 f\in L^1((-\pi,\pi]) \sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty  
\hat f(k):=\frac1{2\pi}\int_{-\pi }^{\pi }f(x)e^{-ikt}\,\mathrm d t\tag1
 f(z):=\sum_{k\geqslant 1}\frac1{k^r} z^k f\in L^2(\partial \Bbb D ) r>1/2 L^p\subset L^q p>q f\in L^1(\partial \Bbb D ) \sum_{k\in \Bbb Z }|\hat f(k)|^p=\infty  rp\leqslant 1 p\in[1,1/r] p\in[1,2) p\geqslant 2 |\hat f(k)|^p","['real-analysis', 'analysis', 'fourier-analysis']"
57,Is $f$ also necessarily of bounded variation?,Is  also necessarily of bounded variation?,f,"Let $g:[0,1] \to \mathbb{R}$ be an increasing function. Define $f:[0,1] \to \mathbb{R}~$ by $f(x)=0$ if $x=0$ and $$f(x)=\frac 1x {\int_0 ^x g(t)~dt} $$ otherwise. Is $f$ also necessarily of bounded variation? My thought was to try to prove that $f$ is increasing (which is even stronger), but the problem is it might not be true. [For example, if $g(x)=x-10$ , we have $f(0)>f(1)$ ] . Now, I see that $f$ has to be increasing on $(0,1]$ . So far so good, but this  still doesn't convince me that $f$ is of bounded variation.","Let be an increasing function. Define by if and otherwise. Is also necessarily of bounded variation? My thought was to try to prove that is increasing (which is even stronger), but the problem is it might not be true. [For example, if , we have ] . Now, I see that has to be increasing on . So far so good, but this  still doesn't convince me that is of bounded variation.","g:[0,1] \to \mathbb{R} f:[0,1] \to \mathbb{R}~ f(x)=0 x=0 f(x)=\frac 1x {\int_0 ^x g(t)~dt}  f f g(x)=x-10 f(0)>f(1) f (0,1] f","['real-analysis', 'lebesgue-integral', 'bounded-variation']"
58,What is the largest sequence of open balls around rational numbers that does not cover $\mathbb R$?,What is the largest sequence of open balls around rational numbers that does not cover ?,\mathbb R,"Let $\mathbb Q = \{q_1,q_2,\dots\}$ be the rational numbers. Let $B_n = \{x \in \mathbb R: |x-q_n| < 2^{-n}\}$ be the open ball of radius $2^{-n}$ around $q_n$ . Then the Lebesgue measure of $U=\cup_{n=1}^\infty B_n$ is at most $$\lambda(\cup_{n=1}^\infty B_n) \le \sum_{n=1}^\infty \lambda(B_n) = 1$$ In particular, $U \subsetneq \mathbb R$ . But this is surprising since $\mathbb Q$ is dense in $\mathbb R$ ! Now let $$C_n = \{x \in \mathbb R: |x-q_n| < c_n\}$$ where the $c_n$ are positive numbers in $\mathbb R$ . Obviously, if $\sum_{n=1}^\infty c_n < \infty$ the same argument as above shows that $\cup_{n=1}^\infty C_n \subsetneq \mathbb R$ . But what about nonsummable $(c_n)$ ? For example, if $c_n = \frac{1}{n}$ , do we still get $\cup_{n=1}^\infty C_n \subsetneq \mathbb R$ or will $\cup_{n=1}^\infty C_n$ now cover $\mathbb R$ ? More generally, given a fixed enumeration of the rationals, can we characterize a ""slowest-declining"" monotonous sequence $(c_n)$ such that $\cup_{n=1}^\infty C_n \subsetneq \mathbb R$ ?","Let be the rational numbers. Let be the open ball of radius around . Then the Lebesgue measure of is at most In particular, . But this is surprising since is dense in ! Now let where the are positive numbers in . Obviously, if the same argument as above shows that . But what about nonsummable ? For example, if , do we still get or will now cover ? More generally, given a fixed enumeration of the rationals, can we characterize a ""slowest-declining"" monotonous sequence such that ?","\mathbb Q = \{q_1,q_2,\dots\} B_n = \{x \in \mathbb R: |x-q_n| < 2^{-n}\} 2^{-n} q_n U=\cup_{n=1}^\infty B_n \lambda(\cup_{n=1}^\infty B_n) \le \sum_{n=1}^\infty \lambda(B_n) = 1 U \subsetneq \mathbb R \mathbb Q \mathbb R C_n = \{x \in \mathbb R: |x-q_n| < c_n\} c_n \mathbb R \sum_{n=1}^\infty c_n < \infty \cup_{n=1}^\infty C_n \subsetneq \mathbb R (c_n) c_n = \frac{1}{n} \cup_{n=1}^\infty C_n \subsetneq \mathbb R \cup_{n=1}^\infty C_n \mathbb R (c_n) \cup_{n=1}^\infty C_n \subsetneq \mathbb R","['real-analysis', 'sequences-and-series', 'general-topology', 'measure-theory', 'soft-question']"
59,Constructing analytic solutions to the delay differential equation $f'(x) = x f(x-1) - f(x)$,Constructing analytic solutions to the delay differential equation,f'(x) = x f(x-1) - f(x),"While working on my answer to another question, I noticed that the function $$ f(x) = \int_0^\infty t^{x-t} dt $$ satisfies the delay differential equation $$ f'(x) = x f(x-1) - f(x) $$ for $x>0$ . To prove this, observe that $f'(x) = \int_0^\infty (\log t) t^{x-t} dt$ . The equation rearranges to give $0 = xf(x-1) - f(x) - f'(x) = \int_0^\infty t^{x-t}\left(\frac{x}{t} - 1 - \log t)\right)dt$ . This integrates to 0 because the integrand is $\frac{d}{dt} t^{x-t}$ . Is there a way that you could construct the function $f$ from this equation? What about other analytic solutions? In the case of ordinary differential equations, I know there are some tricks, and in general $f'(x) = g(x,f(x))$ has a 1-parameter solution space. For DDEs it's a lot more complicated. Typically you're given an initial condition and you solve by piecewise extensions. If you're interested in analytic solutions and you don't care about initial conditions, that's not much help though. My question is: can you construct an analytic solution to a delay differential equation? In this example, given the equation $f'(x) = x f(x-1) - f(x)$ , is there a way to arrive at an analytic solution that's not guess-and-check? Furthermore, it seems intuitive that if one analytic solution exists, there should be an infinite dimensional family of solutions (similar to how the family of solutions to the difference equation $f(x) = x f(x-1)$ is essentially parametrized by 1-periodic functions). For example, solutions to $f'(x) = f(x-1)$ have a Fourier series-like form $$f(x) = \sum_{\{c \space:\space c e^c = 1\}} a_c e^{c x}$$ For $f'(x) = xf(x-1) - f(x)$ , I have discovered one particular solution. How would you going about constructing other analytic solutions? Is there anything analogous to $e^{c x}$ for $f'(x) = f(x-1)$ ? Update (5/2/2020): As I suspected, there is an infinite dimensional space of solutions, which can be naturally parametrized by 1-periodic functions. Given any 1-periodic function $g$ , then $$ f_g(x) = \int_0^\infty t^{x-t} g(x-t) dt $$ solves the same DDE. (This can be proven similarly to above). It's clear that $f_g$ is going to be analytic if $g$ is. I suppose showing only if wouldn't be difficult either. So that answers one part of the question. I'm still curious how one would arrive at this solution from the equation directly. It's easy to show this function solves the DDE, but how would one construct $f$ given $f'(x) = xf(x-1) - f(x)$ .","While working on my answer to another question, I noticed that the function satisfies the delay differential equation for . To prove this, observe that . The equation rearranges to give . This integrates to 0 because the integrand is . Is there a way that you could construct the function from this equation? What about other analytic solutions? In the case of ordinary differential equations, I know there are some tricks, and in general has a 1-parameter solution space. For DDEs it's a lot more complicated. Typically you're given an initial condition and you solve by piecewise extensions. If you're interested in analytic solutions and you don't care about initial conditions, that's not much help though. My question is: can you construct an analytic solution to a delay differential equation? In this example, given the equation , is there a way to arrive at an analytic solution that's not guess-and-check? Furthermore, it seems intuitive that if one analytic solution exists, there should be an infinite dimensional family of solutions (similar to how the family of solutions to the difference equation is essentially parametrized by 1-periodic functions). For example, solutions to have a Fourier series-like form For , I have discovered one particular solution. How would you going about constructing other analytic solutions? Is there anything analogous to for ? Update (5/2/2020): As I suspected, there is an infinite dimensional space of solutions, which can be naturally parametrized by 1-periodic functions. Given any 1-periodic function , then solves the same DDE. (This can be proven similarly to above). It's clear that is going to be analytic if is. I suppose showing only if wouldn't be difficult either. So that answers one part of the question. I'm still curious how one would arrive at this solution from the equation directly. It's easy to show this function solves the DDE, but how would one construct given .","
f(x) = \int_0^\infty t^{x-t} dt
 
f'(x) = x f(x-1) - f(x)
 x>0 f'(x) = \int_0^\infty (\log t) t^{x-t} dt 0 = xf(x-1) - f(x) - f'(x) = \int_0^\infty t^{x-t}\left(\frac{x}{t} - 1 - \log t)\right)dt \frac{d}{dt} t^{x-t} f f'(x) = g(x,f(x)) f'(x) = x f(x-1) - f(x) f(x) = x f(x-1) f'(x) = f(x-1) f(x) = \sum_{\{c \space:\space c e^c = 1\}} a_c e^{c x} f'(x) = xf(x-1) - f(x) e^{c x} f'(x) = f(x-1) g 
f_g(x) = \int_0^\infty t^{x-t} g(x-t) dt
 f_g g f f'(x) = xf(x-1) - f(x)","['real-analysis', 'integration', 'delay-differential-equations']"
60,Proving a function is continuous everywhere,Proving a function is continuous everywhere,,"Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ continuous at 0, and $f(a+b) = f(a) + f(b) \forall a,b \epsilon \mathbb{R}$ . Show that f is in fact continuous everywhere. Let $\epsilon > 0$ . There exists $\delta_1>0$ such that $|x - 0| < \delta_1 \rightarrow |f(x) - f(0)|<\epsilon$ . We want to show that, for c other than 0, there exists $\delta>0$ such that $|x-c|<\delta \rightarrow |f(x) - f(c)|<\epsilon$ . We note that $|f(x) - f(c)| = |f(x-c)|$ .  Let $y = x-c$ . Then, we want to prove $|y| < \delta \rightarrow |f(y)| < \epsilon$ . But, we set $\delta = \delta_1$ . Thus, $|f(y)| = |f(x-c)| < \epsilon.$ Thus $f(x)$ is continuous everywhere. Is this proof correct? If not, I would appreciate guidance towards the right direction.","Suppose continuous at 0, and . Show that f is in fact continuous everywhere. Let . There exists such that . We want to show that, for c other than 0, there exists such that . We note that .  Let . Then, we want to prove . But, we set . Thus, Thus is continuous everywhere. Is this proof correct? If not, I would appreciate guidance towards the right direction.","f:\mathbb{R} \rightarrow \mathbb{R} f(a+b) = f(a) + f(b) \forall a,b \epsilon \mathbb{R} \epsilon > 0 \delta_1>0 |x - 0| < \delta_1 \rightarrow |f(x) - f(0)|<\epsilon \delta>0 |x-c|<\delta \rightarrow |f(x) - f(c)|<\epsilon |f(x) - f(c)| = |f(x-c)| y = x-c |y| < \delta \rightarrow |f(y)| < \epsilon \delta = \delta_1 |f(y)| = |f(x-c)| < \epsilon. f(x)","['real-analysis', 'continuity']"
61,does a small variation in the constants of a power series result in a small variation in its radius of convergence,does a small variation in the constants of a power series result in a small variation in its radius of convergence,,"in $cs$ , the space of all sequences $(a_n)$ such that the series ${\displaystyle \sum _{i=1}^{\infty }a_{n}}$ is convergent (possibly conditionally), the norm of a sequence is defined as ${\displaystyle \left\|(x_n)\right\|_{cs}=\sup _{n}\left|\sum _{i=1}^{n}x_{i}\right|}$ . for $(a_n)\in cs$ we can also talk about an induced radius ${}_i{r}((a_n))$ defined as the radius of convergence for $\displaystyle \sum _{i=1}^{\infty }a_{n}x^n$ which is given by ${}_ir((a_n)) = \frac{1}{\limsup_{n\rightarrow\infty}{|a_n|^{\frac{1}{n}}}}$ . My question then is whether the induced radius is a continuous function where it is a real number, that is if we know ${}_i{r}((a_n)) \in \mathbb{R}$ $$\forall \epsilon>0 \stackrel{?}{\exists} \delta>0 \ (\displaystyle \left\|(h_n)\right\|_{cs}<\delta \Rightarrow |{}_i{r}((a_n+h_n))-{}_i{r}((a_n))|<\epsilon)$$ sommewhat helpful source: https://en.wikipedia.org/wiki/Bs_space","in , the space of all sequences such that the series is convergent (possibly conditionally), the norm of a sequence is defined as . for we can also talk about an induced radius defined as the radius of convergence for which is given by . My question then is whether the induced radius is a continuous function where it is a real number, that is if we know sommewhat helpful source: https://en.wikipedia.org/wiki/Bs_space",cs (a_n) {\displaystyle \sum _{i=1}^{\infty }a_{n}} {\displaystyle \left\|(x_n)\right\|_{cs}=\sup _{n}\left|\sum _{i=1}^{n}x_{i}\right|} (a_n)\in cs {}_i{r}((a_n)) \displaystyle \sum _{i=1}^{\infty }a_{n}x^n {}_ir((a_n)) = \frac{1}{\limsup_{n\rightarrow\infty}{|a_n|^{\frac{1}{n}}}} {}_i{r}((a_n)) \in \mathbb{R} \forall \epsilon>0 \stackrel{?}{\exists} \delta>0 \ (\displaystyle \left\|(h_n)\right\|_{cs}<\delta \Rightarrow |{}_i{r}((a_n+h_n))-{}_i{r}((a_n))|<\epsilon),"['real-analysis', 'sequences-and-series', 'continuity', 'metric-spaces', 'epsilon-delta']"
62,Continuity of Weak Solutions to Navier-Stokes Equations at $t = 0$ in $L^2$,Continuity of Weak Solutions to Navier-Stokes Equations at  in,t = 0 L^2,"The following definitions/theorems/etc come from this paper , by Masuda, 1984. Statement of the Navier-Stokes Problem For $n \geq 3$ , let $\Omega \subseteq \mathbb{R}^n$ , $T > 0$ . We write the incompressible NS equations as follows: (NS) $\begin{cases} \text{d}_t u - \Delta u + (u \cdot \nabla) u + \nabla p = f; \ \nabla \cdot u = 0, \ x \in \Omega, \ 0<t<T \\ u|_{\partial \Omega} = 0 ; \ u|_{t=0} = a   \end{cases}$ Where $u : \Omega \times [0,T) \rightarrow \mathbb{R}^n$ is the unknown vector velocity, $ p : \Omega \times [0,T) \rightarrow \mathbb{R}^n$ is the unknown pressure, $ f : \Omega \times [0,T) \rightarrow \mathbb{R}^n $ is a given external force, and $a:\Omega  \rightarrow \mathbb{R}^n$ is the given initial velocity. Function Spaces \begin{align} C^{\infty}_{0, \sigma}(\Omega) & := \{ f \in C^{\infty}_{0}(\Omega)^{n} \ | \ \nabla \cdot f = 0 \} \\ L_{\sigma}^{2}(\Omega) & := \overline{C^{\infty}_{0, \sigma}(\Omega)}^{||\cdot||_{L^2}} \\ H^1(\Omega) = W^{1,2}(\Omega) & := \{ f \in L^2 \ | \ \partial_{x_i} f \in L^2, \ i=1,...,n \} \\ H^{1}_{0,\sigma}(\Omega) & := \overline{C^{\infty}_{0, \sigma}(\Omega)}^{||\cdot||_{H^1}} \\ Y &:= H^{1}_{0,\sigma}(\Omega) \cap L^n \end{align} Assumptions on $a$ and $f$ (1.) $a \in L^2_\sigma$ (2.) $f(\cdot , t) \in L^2$ for almost all $t \in (0,T)$ , and $Pf \in L^1(0,t ; L^2_\sigma)$ , where $P$ is the Helmholtz projection from $L^2$ into $L^2_\sigma$ . Definition of Weak Solution A weak solution of (NS) is a function, $u$ , on $\Omega \times [0,T)$ such that: (1.) $u \in L^2(0,T' ; H^{1}_{0,\sigma})$ , for all $T' \in (0,T)$ (2.) $u \in L^{\infty}(0,T ; L^{2}_{\sigma})$ (3.) $\int^s_{r} -(u,\partial_{t}\Phi) + (\nabla u , \nabla \Phi) + (u \cdot \nabla u, \Phi) \text{d}t = \int^s_r (f, \Phi) \text{d}t - (u(s), \Phi(s)) + (u(r), \Phi(r))$ for all $0 \leq r \leq s < T$ and all $\Phi \in H^1(r,s; Y)$ . Theorem (Masuda, 1984) There exists a weak solution, $u$ , of (NS). In particular, this function is weak continuous in $L^2_\sigma$ with respect to $t \in [0,T)$ , and $||u(t)||^2_{L^2(\Omega)} + 2 \int^{s}_{0} ||\nabla u(t)||^{2}_{L^2(\Omega)} \text{d}t \leq 2 \int^{s}_{0} (f, u) \text{d}t + ||a||^{2}_{L^2(\Omega)}$ for all $0 \leq t < T$ (Energy Inequality) and $\lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0 \ \ \ $ (Continuity at $t=0$ ) Here, $(u,f)$ denotes the inner-product of $u$ and $f$ in $L^2(\Omega)$ . That is, $(u,f) = \int_{\Omega} u \cdot f \ \text{d}x$ . We explain the meaning of weak continuity of $u$ . $u$ is weakly continuous in $L^2(\Omega)$ with respect to $t \in [0,T)$ if: $(u(t), g) = \int_{\Omega} u(t) \cdot g \ \text{d}x$ is continuous in $t$ , for all $g \in L^2_\sigma$ . Proof of Existence (Outline) (Please feel free to skip this and see the remaining problem of continuity at $t=0.$ ) I have proven the existence of a weak solution, $u$ , as described in the definition above. I have also shown that this $u$ is weakly continuous in $L^2_\sigma$ with respect to $t$ and satisfies the Energy Inequality. We prove this using a Galerkin Method. We find a countable set $\{\phi_l \}_{l=1}^{\infty} \subseteq C^{\infty}_{0,\sigma}$ whose span is dense in $Y$ and in $L^{2}_{\sigma}$ (with respect to their respective norms). We then define $u_{m}(x,t) = \sum^m_{l=1} c_{ml}(t) \phi_{l}(x)$ , where $c_{ml}$ are solutions in $C^1(0,T)$ to the below locally Lipschitz ODE: \begin{cases}\text{d}_t c_{ml} + \sum^m_{i=1} (\nabla \phi_{i} , \nabla \phi_{l})c_{mi} + \sum^m_{i,j=1} (\phi_i \nabla \phi_{j} , \nabla \phi_{l})c_{mi}c_{mj} = (f,\phi_l), \ \text{for all } l=1,...,m \\ c_{ml}(0) = (a, \phi_l), \ \text{for all } l=1,...,m. \end{cases} We find that these $u_m$ are uniform bounded in $L^2(0,T' ; H^{1}_{0,\sigma})$ , for all $T' \in (0,T)$ and in $L^{\infty}(0,T ; L^{2}_{\sigma})$ . Thus, since these are Hilbert spaces, there exists subsequences which converge weakly. It is easy to show that $u_m$ convergese to the same function, $u$ , in both spaces. Thus we have satisfied (1.) and (2.) from the definition of Weak Solutions. We can use Arzela-Ascoli to show that this $u$ is weak continuous, as described above. See this previous question for details on the weak convergence and weak continuity of $u$ . Using the weak convergence and definition of $u_m$ , we show that this $u$ satisfies the Energy Inequality. Using weak convergence and several Lemmas from Masuda's paper, we show that $u$ satisfies the integral equation (3.) from the definition of Weak Solutions. Last Remaining Problem, Continuity in $L^2$ at $t=0$ It remains only to show that $\lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0$ . In fact, Masuda makes no mention of this property in his proof of the theorem!!! We know that $u$ is weakly continuous with respect to $t$ , as explained above. Also, by the definition of $u_m$ , each $u_m$ is continuous with respect to $t$ . That is, $u_m \in C((0,T); L^2_{\sigma})$ , for all $m$ . I suspect this should be enough to obtain the final result, but I am struggling to show it without requiring uniform continuity. Here is my initial attempt to prove: $\lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = \lim_{t \rightarrow 0}(u(t) - a, u(t) - a) $ $ = \lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t)) - (u_{m}(t) - a, a)$ By an earlier step in the existence proof, we know that $(u_{m}(t), g)$ converges as $m \rightarrow \infty$ uniformly with respect to $t$ . (Again, see this previous question for details.) Thus the term on the right, $(u_{m}(t) - a, a)$ , converges uniformly with respect to $t$ . Therefore, we are able to swap the position of the limits in $t$ and $m$ . By definition of $u_m$ , it is clear that $\lim_{m \rightarrow \infty} ||u_m(t=0)||_{L^2(\Omega)} = ||a||_{L^2(\Omega)}$ , so the term on the right vanishes. We are stuck with the term on the left, $\lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t))$ . As demonstrated in another previous question , we know that, for each fixed $m$ , this is continuous in $t$ . So we would be able to show that this term vanishes, if we are able to swap the limits in $t$ and $m$ . I know we could do this if we have uniform convergence again, but I am not sure if this is possible, as we now have dependence on $t$ for both sides of the inner-product. We do have uniform boundedness of this term, as the $u_m$ are uniformly bounded in $L^2(0,T' ; H^{1}_{0,\sigma})$ , for all $T' \in (0,T)$ and $L^{\infty}(0,T ; L^{2}_{\sigma})$ , so we could obtain uniform convergence by showing equicontinuity. But I'm not sure if this can be done. In short, the question is as follows: can we swap the limits in $t$ and $m$ in $\lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t))$ ? Or is there another way to show that $\lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0$ ?","The following definitions/theorems/etc come from this paper , by Masuda, 1984. Statement of the Navier-Stokes Problem For , let , . We write the incompressible NS equations as follows: (NS) Where is the unknown vector velocity, is the unknown pressure, is a given external force, and is the given initial velocity. Function Spaces Assumptions on and (1.) (2.) for almost all , and , where is the Helmholtz projection from into . Definition of Weak Solution A weak solution of (NS) is a function, , on such that: (1.) , for all (2.) (3.) for all and all . Theorem (Masuda, 1984) There exists a weak solution, , of (NS). In particular, this function is weak continuous in with respect to , and for all (Energy Inequality) and (Continuity at ) Here, denotes the inner-product of and in . That is, . We explain the meaning of weak continuity of . is weakly continuous in with respect to if: is continuous in , for all . Proof of Existence (Outline) (Please feel free to skip this and see the remaining problem of continuity at ) I have proven the existence of a weak solution, , as described in the definition above. I have also shown that this is weakly continuous in with respect to and satisfies the Energy Inequality. We prove this using a Galerkin Method. We find a countable set whose span is dense in and in (with respect to their respective norms). We then define , where are solutions in to the below locally Lipschitz ODE: We find that these are uniform bounded in , for all and in . Thus, since these are Hilbert spaces, there exists subsequences which converge weakly. It is easy to show that convergese to the same function, , in both spaces. Thus we have satisfied (1.) and (2.) from the definition of Weak Solutions. We can use Arzela-Ascoli to show that this is weak continuous, as described above. See this previous question for details on the weak convergence and weak continuity of . Using the weak convergence and definition of , we show that this satisfies the Energy Inequality. Using weak convergence and several Lemmas from Masuda's paper, we show that satisfies the integral equation (3.) from the definition of Weak Solutions. Last Remaining Problem, Continuity in at It remains only to show that . In fact, Masuda makes no mention of this property in his proof of the theorem!!! We know that is weakly continuous with respect to , as explained above. Also, by the definition of , each is continuous with respect to . That is, , for all . I suspect this should be enough to obtain the final result, but I am struggling to show it without requiring uniform continuity. Here is my initial attempt to prove: By an earlier step in the existence proof, we know that converges as uniformly with respect to . (Again, see this previous question for details.) Thus the term on the right, , converges uniformly with respect to . Therefore, we are able to swap the position of the limits in and . By definition of , it is clear that , so the term on the right vanishes. We are stuck with the term on the left, . As demonstrated in another previous question , we know that, for each fixed , this is continuous in . So we would be able to show that this term vanishes, if we are able to swap the limits in and . I know we could do this if we have uniform convergence again, but I am not sure if this is possible, as we now have dependence on for both sides of the inner-product. We do have uniform boundedness of this term, as the are uniformly bounded in , for all and , so we could obtain uniform convergence by showing equicontinuity. But I'm not sure if this can be done. In short, the question is as follows: can we swap the limits in and in ? Or is there another way to show that ?","n \geq 3 \Omega \subseteq \mathbb{R}^n T > 0 \begin{cases} \text{d}_t u - \Delta u + (u \cdot \nabla) u + \nabla p = f; \ \nabla \cdot u = 0, \ x \in \Omega, \ 0<t<T \\ u|_{\partial \Omega} = 0 ; \ u|_{t=0} = a  
\end{cases} u : \Omega \times [0,T) \rightarrow \mathbb{R}^n  p : \Omega \times [0,T) \rightarrow \mathbb{R}^n  f : \Omega \times [0,T) \rightarrow \mathbb{R}^n  a:\Omega  \rightarrow \mathbb{R}^n \begin{align}
C^{\infty}_{0, \sigma}(\Omega)
& := \{ f \in C^{\infty}_{0}(\Omega)^{n} \ | \ \nabla \cdot f = 0 \} \\
L_{\sigma}^{2}(\Omega)
& := \overline{C^{\infty}_{0, \sigma}(\Omega)}^{||\cdot||_{L^2}} \\
H^1(\Omega) = W^{1,2}(\Omega)
& := \{ f \in L^2 \ | \ \partial_{x_i} f \in L^2, \ i=1,...,n \} \\
H^{1}_{0,\sigma}(\Omega)
& := \overline{C^{\infty}_{0, \sigma}(\Omega)}^{||\cdot||_{H^1}} \\
Y
&:= H^{1}_{0,\sigma}(\Omega) \cap L^n
\end{align} a f a \in L^2_\sigma f(\cdot , t) \in L^2 t \in (0,T) Pf \in L^1(0,t ; L^2_\sigma) P L^2 L^2_\sigma u \Omega \times [0,T) u \in L^2(0,T' ; H^{1}_{0,\sigma}) T' \in (0,T) u \in L^{\infty}(0,T ; L^{2}_{\sigma}) \int^s_{r} -(u,\partial_{t}\Phi) + (\nabla u , \nabla \Phi) + (u \cdot \nabla u, \Phi) \text{d}t = \int^s_r (f, \Phi) \text{d}t - (u(s), \Phi(s)) + (u(r), \Phi(r)) 0 \leq r \leq s < T \Phi \in H^1(r,s; Y) u L^2_\sigma t \in [0,T) ||u(t)||^2_{L^2(\Omega)} + 2 \int^{s}_{0} ||\nabla u(t)||^{2}_{L^2(\Omega)} \text{d}t \leq 2 \int^{s}_{0} (f, u) \text{d}t + ||a||^{2}_{L^2(\Omega)} 0 \leq t < T \lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0 \ \ \  t=0 (u,f) u f L^2(\Omega) (u,f) = \int_{\Omega} u \cdot f \ \text{d}x u u L^2(\Omega) t \in [0,T) (u(t), g) = \int_{\Omega} u(t) \cdot g \ \text{d}x t g \in L^2_\sigma t=0. u u L^2_\sigma t \{\phi_l \}_{l=1}^{\infty} \subseteq C^{\infty}_{0,\sigma} Y L^{2}_{\sigma} u_{m}(x,t) = \sum^m_{l=1} c_{ml}(t) \phi_{l}(x) c_{ml} C^1(0,T) \begin{cases}\text{d}_t c_{ml} + \sum^m_{i=1} (\nabla \phi_{i} , \nabla \phi_{l})c_{mi} + \sum^m_{i,j=1} (\phi_i \nabla \phi_{j} , \nabla \phi_{l})c_{mi}c_{mj} = (f,\phi_l), \ \text{for all } l=1,...,m \\
c_{ml}(0) = (a, \phi_l), \ \text{for all } l=1,...,m.
\end{cases} u_m L^2(0,T' ; H^{1}_{0,\sigma}) T' \in (0,T) L^{\infty}(0,T ; L^{2}_{\sigma}) u_m u u u u_m u u L^2 t=0 \lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0 u t u_m u_m t u_m \in C((0,T); L^2_{\sigma}) m \lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = \lim_{t \rightarrow 0}(u(t) - a, u(t) - a)   = \lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t)) - (u_{m}(t) - a, a) (u_{m}(t), g) m \rightarrow \infty t (u_{m}(t) - a, a) t t m u_m \lim_{m \rightarrow \infty} ||u_m(t=0)||_{L^2(\Omega)} = ||a||_{L^2(\Omega)} \lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t)) m t t m t u_m L^2(0,T' ; H^{1}_{0,\sigma}) T' \in (0,T) L^{\infty}(0,T ; L^{2}_{\sigma}) t m \lim_{t \rightarrow 0} \lim_{m \rightarrow \infty}(u(t) - a, u_{m}(t)) \lim_{t \rightarrow 0}||u(t) - a||_{L^2(\Omega)} = 0","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'lp-spaces', 'weak-convergence']"
63,"The derivatives of a real polynomial function $f(x,y)$ are divisible by a certain polynomial of $x$ and $y$. What can we know about $f\mkern1mu$?",The derivatives of a real polynomial function  are divisible by a certain polynomial of  and . What can we know about ?,"f(x,y) x y f\mkern1mu","Given $f(x,y)$ , a two real variable polynomial such that $\frac{\partial f(x,y)}{\partial x}$ and $\frac{\partial f(x,y)}{\partial y}$ are divisible by $x^2+y^2-1$ . Prove that there exist a polynomial $g(x,y)$ and a constant $c$ such that: $$f(x,y)= g(x,y){(x^2+y^2-1)}^2+c$$ That is the problem I'm currently trying to solve. I first tried to write de derivatives in the form $P(x,y)* ({x^2+y^2-1})$ with $P(x,y)$ being a polynomial. Then tried to write both $P$ and ${x^2+y^2-1}$ as derivatives of a function (one at a time) to perform integrarion by parts, but it led me nowhere. Any ideas in how to solve it? If it's possible to just give me a hint in how to start and not the whole solution I'd appreciate it very much","Given , a two real variable polynomial such that and are divisible by . Prove that there exist a polynomial and a constant such that: That is the problem I'm currently trying to solve. I first tried to write de derivatives in the form with being a polynomial. Then tried to write both and as derivatives of a function (one at a time) to perform integrarion by parts, but it led me nowhere. Any ideas in how to solve it? If it's possible to just give me a hint in how to start and not the whole solution I'd appreciate it very much","f(x,y) \frac{\partial f(x,y)}{\partial x} \frac{\partial f(x,y)}{\partial y} x^2+y^2-1 g(x,y) c f(x,y)= g(x,y){(x^2+y^2-1)}^2+c P(x,y)*
({x^2+y^2-1}) P(x,y) P {x^2+y^2-1}","['real-analysis', 'multivariable-calculus', 'polynomials']"
64,Is there accepted notation for specifying just the domain of a function?,Is there accepted notation for specifying just the domain of a function?,,"Question. Is there a notation like $$f(x \in \mathbb{R}) = x^2 + 2x + 1$$ or some variant on that, satisfying the following conditions? (a) Like the above syntax, it allows us to define a function by specifying its domain without worrying about the codomain (b) Like the above syntax, it does not force us to write $x$ more times than strictly necessary, and (c) Unlike the above syntax, it's fairly standard and won't cause too many eyebrows to be raised. The only such ""accepted notations"" I can think of are $$f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R}$$ $$\forall x \in \mathbb{R}, f(x) = x^2 + 2x + 1$$ which force us to mention $x$ an ""extra"" time, violating condition (b). Motivation 1. Promoting Readability. In structuralist mathematics codomains are fundamental, however in more 'down-to-earth' math they're often irrelevant, and cluttering the page with such details can sometimes reduce readability e.g. through misdirection. Motivation 2. Pedagogy. In my opinion, that students should encounter the concepts of ""function"" and ""domain"" at age 12 or thereabouts, while the concept of a ""codomain"" should be saved for university and the initial forays into structuralist mathematics. This means that having an alternative to the $f : X \rightarrow Y, x \mapsto E(x)$ notation often used in structural mathematics is important. Motivation 3. Laziness. Realistically people are going to leave off the $x \in \mathbb{R}$ part from expressions like $$f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R},$$ partly because it's at the end of the expression, but more fundamentally because we're repeating the $x$ unnecessarily and it starts to feel tiresome. A good notation would address this purely psychological issue.","Question. Is there a notation like or some variant on that, satisfying the following conditions? (a) Like the above syntax, it allows us to define a function by specifying its domain without worrying about the codomain (b) Like the above syntax, it does not force us to write more times than strictly necessary, and (c) Unlike the above syntax, it's fairly standard and won't cause too many eyebrows to be raised. The only such ""accepted notations"" I can think of are which force us to mention an ""extra"" time, violating condition (b). Motivation 1. Promoting Readability. In structuralist mathematics codomains are fundamental, however in more 'down-to-earth' math they're often irrelevant, and cluttering the page with such details can sometimes reduce readability e.g. through misdirection. Motivation 2. Pedagogy. In my opinion, that students should encounter the concepts of ""function"" and ""domain"" at age 12 or thereabouts, while the concept of a ""codomain"" should be saved for university and the initial forays into structuralist mathematics. This means that having an alternative to the notation often used in structural mathematics is important. Motivation 3. Laziness. Realistically people are going to leave off the part from expressions like partly because it's at the end of the expression, but more fundamentally because we're repeating the unnecessarily and it starts to feel tiresome. A good notation would address this purely psychological issue.","f(x \in \mathbb{R}) = x^2 + 2x + 1 x f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R} \forall x \in \mathbb{R}, f(x) = x^2 + 2x + 1 x f : X \rightarrow Y, x \mapsto E(x) x \in \mathbb{R} f(x) = x^2 + 2x + 1, \;\;x \in \mathbb{R}, x","['real-analysis', 'functions', 'notation']"
65,The $|\cdot|_{p}$ norm will become the maximum norm when $p \to \infty$,The  norm will become the maximum norm when,|\cdot|_{p} p \to \infty,"I'm trying to prove the $|\cdot|_{p}$ norm will become the maximum norm when $p \to \infty$ . Let $\mathbb K$ denote $\mathbb R$ or $\mathbb C$ , and $x= (x_1, \ldots, x_m) \in \mathbb K^m$ . Then $$\lim_{p \to \infty} \left ( \sum_{i=1}^m |x_i|^p \right )^{1/p} = \max _{1 \leq i\leq m} |x_{i}|$$ Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! My attempt: It suffices to prove the statement in case $x \in \mathbb {(R^+)}^{m}$ , where it becomes $$\lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} = \max _{1 \leq i\leq m} x_{i}$$ Let $l:= \max _{1 \leq i\leq m} x_{i}$ . We have $$l = (l^p)^{1/p} \le \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} \le (ml^p)^{1/p} = m^{1/p}l$$ Then $$l = \lim_{p \to \infty} l \le \lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} \le \lim_{p \to \infty} (m^{1/p}l) = l$$ and thus by squeeze theorem $$\lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} = l$$ This completes the proof.","I'm trying to prove the norm will become the maximum norm when . Let denote or , and . Then Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! My attempt: It suffices to prove the statement in case , where it becomes Let . We have Then and thus by squeeze theorem This completes the proof.","|\cdot|_{p} p \to \infty \mathbb K \mathbb R \mathbb C x= (x_1, \ldots, x_m) \in \mathbb K^m \lim_{p \to \infty} \left ( \sum_{i=1}^m |x_i|^p \right )^{1/p} = \max _{1 \leq i\leq m} |x_{i}| x \in \mathbb {(R^+)}^{m} \lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} = \max _{1 \leq i\leq m} x_{i} l:= \max _{1 \leq i\leq m} x_{i} l = (l^p)^{1/p} \le \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} \le (ml^p)^{1/p} = m^{1/p}l l = \lim_{p \to \infty} l \le \lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} \le \lim_{p \to \infty} (m^{1/p}l) = l \lim_{p \to \infty} \left ( \sum_{i=1}^m (x_i)^p \right )^{1/p} = l","['real-analysis', 'proof-verification', 'normed-spaces']"
66,"Prove $\bigcup\limits_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]} = [0,1)$",Prove,"\bigcup\limits_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]} = [0,1)","First, I apologize for my English, it is not my native language. To solve this exercise I started by proving that $\displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}  {n}\right]} \subseteq [0,1)$ : Let $x \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}  {n}\right]}$ . Therefore, there exist some $N \in \mathbb{N}$ such that $x \in \left[0,1-\frac{1}{N}\right]$ Since $N \in \mathbb{N}$ , $\frac{1}{N} > 0$ . (Note that in my case the natural numbers start at 1) Thus $0 \leq x \leq 1- \frac{1}{N} < 1$ , Then $x \in [0,1)$ This is, $\left[0,1-\frac{1}{N}\right] \subseteq [0,1)$ Then, $\displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]} \subseteq [0,1)$ . Edit (I used the suggestions received to finish the second part of the proof. Is this well? ) To obtain the opposite inclusion, suppose $x\subseteq [0,1)$ and choose $ \varepsilon =1-x >0$ . By the Archimedean property follows that there exist $N \in \mathbb{N}$ such that $\frac{1}{N}<1-x$ Thus, $0 \leqslant x<1-\frac{1}{N}$ . Therefore, $x \in \left[0,1-\frac{1}{N}\right]$ . This means, $x \in \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]}$ . We conclude that $[0,1) \subseteq \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}  {n}\right]}$ .","First, I apologize for my English, it is not my native language. To solve this exercise I started by proving that : Let . Therefore, there exist some such that Since , . (Note that in my case the natural numbers start at 1) Thus , Then This is, Then, . Edit (I used the suggestions received to finish the second part of the proof. Is this well? ) To obtain the opposite inclusion, suppose and choose . By the Archimedean property follows that there exist such that Thus, . Therefore, . This means, . We conclude that .","\displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}
 {n}\right]} \subseteq [0,1) x \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}
 {n}\right]} N \in \mathbb{N} x \in \left[0,1-\frac{1}{N}\right] N \in \mathbb{N} \frac{1}{N} > 0 0 \leq x \leq 1- \frac{1}{N} < 1 x \in [0,1) \left[0,1-\frac{1}{N}\right] \subseteq [0,1) \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]} \subseteq [0,1) x\subseteq [0,1)  \varepsilon =1-x >0 N \in \mathbb{N} \frac{1}{N}<1-x 0 \leqslant x<1-\frac{1}{N} x \in \left[0,1-\frac{1}{N}\right] x \in \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}{n}\right]} [0,1) \subseteq \displaystyle\bigcup_{n \in \mathbb{ N} }{\left[0,1-\frac{1}
 {n}\right]}","['real-analysis', 'calculus', 'elementary-set-theory', 'metric-spaces']"
67,Euler (equidimensional) equation question,Euler (equidimensional) equation question,,"Consider the equation $$x^2y''-8xy'+20y=0.$$ From an undergraduate ODE course, it is known that the two linearly are $y_1=x^5$ and $y_2=x^4$ . However, why don't we consider solutions, for example, like the following one: $$ y=\left\{ \begin{array}{c} x^5\;\;{\mbox{if}}\;\;x\leq 0\\ 0\;\;{\mbox{if}}\;\;x> 0. \end{array} \right. $$ While the solution above is only differentiable 4 times, it is a perfectly good solution to the equation. I understand that the point $x=0$ is singular and thus the uniqueness theorem does not apply there. Also this is not unique to that equation or that solution. The same type of solutions could be obtained for other equations with singular points. The example above is convenient because it is a simple differential equation with explicit solutions. But my question is this: is there a reason why solutions such as the one above are not considered? Is it just because it is too complicated to be included in textbooks? Perhaps because they do not extend to solutions when the independent variable is allowed to be complex? Or perhaps people shy away to solutions that are not smooth (i.e. not in $C^\infty$ ) when smooth solutions exist?","Consider the equation From an undergraduate ODE course, it is known that the two linearly are and . However, why don't we consider solutions, for example, like the following one: While the solution above is only differentiable 4 times, it is a perfectly good solution to the equation. I understand that the point is singular and thus the uniqueness theorem does not apply there. Also this is not unique to that equation or that solution. The same type of solutions could be obtained for other equations with singular points. The example above is convenient because it is a simple differential equation with explicit solutions. But my question is this: is there a reason why solutions such as the one above are not considered? Is it just because it is too complicated to be included in textbooks? Perhaps because they do not extend to solutions when the independent variable is allowed to be complex? Or perhaps people shy away to solutions that are not smooth (i.e. not in ) when smooth solutions exist?","x^2y''-8xy'+20y=0. y_1=x^5 y_2=x^4 
y=\left\{
\begin{array}{c}
x^5\;\;{\mbox{if}}\;\;x\leq 0\\
0\;\;{\mbox{if}}\;\;x> 0.
\end{array}
\right.
 x=0 C^\infty","['real-analysis', 'complex-analysis', 'ordinary-differential-equations', 'frobenius-method', 'singular-solution']"
68,Find a closed form of $\sum_{n=1}^{\infty} \frac{{H_{n-1}^{(2)}}x^{2n}}{n^2{{2n}\choose{n}}}$.,Find a closed form of .,\sum_{n=1}^{\infty} \frac{{H_{n-1}^{(2)}}x^{2n}}{n^2{{2n}\choose{n}}},"I would like to know if it is possible to find a closed form of the sum $$\sum_{n=1}^{\infty} \frac{{H_{n-1}^{(2)}}x^{2n}}{n^2{{2n}\choose{n}}}.$$ Here $H_{n}^{(2)}$ denotes the generalized harmonic number . This series look similar to the $\arcsin(x)$ series $$\sum_{n=1}^{\infty} \frac{x^{2n}}{n^2{{2n}\choose{n}}}$$ but I don't know how to combine it with harmonic number. I tried converting it to Beta function but that didn't seem to help much. I am interested in an even more general question, i.e. finding the closed form of $$\sum_{n=1}^{\infty} \frac{{H_{n-1}^{(k)}}x^{2n}}{n^2{{2n}\choose{n}}}$$ for any positive integer $k$ . Any help is very appreciated.","I would like to know if it is possible to find a closed form of the sum Here denotes the generalized harmonic number . This series look similar to the series but I don't know how to combine it with harmonic number. I tried converting it to Beta function but that didn't seem to help much. I am interested in an even more general question, i.e. finding the closed form of for any positive integer . Any help is very appreciated.",\sum_{n=1}^{\infty} \frac{{H_{n-1}^{(2)}}x^{2n}}{n^2{{2n}\choose{n}}}. H_{n}^{(2)} \arcsin(x) \sum_{n=1}^{\infty} \frac{x^{2n}}{n^2{{2n}\choose{n}}} \sum_{n=1}^{\infty} \frac{{H_{n-1}^{(k)}}x^{2n}}{n^2{{2n}\choose{n}}} k,"['real-analysis', 'sequences-and-series', 'number-theory', 'harmonic-numbers']"
69,Decay of non-negative functions with compact Fourier support,Decay of non-negative functions with compact Fourier support,,"Let $f$ be a function on the real line with $\widehat{f}$ supported in the interval $[-1,1]$ . Let's denote the space of such functions with $W_0$ . Let $g\ge 0$ denote a rapidly decaying (and say, continuous, if that matters) function on the real line; what I have in mind is something like the absolute value of a Schwartz function. Question: Given such a function $g$ , does there always exist $f\in W_0$ such that $g(x)\le f(x)$ for all $x\in\mathbb{R}$ ? If yes, can we also find $f\in W=W_0\cap \mathcal{S}$ ? (Here $\mathcal{S}$ denotes Schwartz functions.) Presumably, for the second part, rapid decay is not sufficient. On the other hand if $g$ is compactly supported, then the answer to the second question seems to be yes, let $f=\widehat{\phi*\phi}$ for an approriately chosen smooth $\phi$ supported in $[-1/2,1/2]$ . Partial  answers or helpful suggestions are welcome as well. Edit: Sorry for confusion over multiple edits.","Let be a function on the real line with supported in the interval . Let's denote the space of such functions with . Let denote a rapidly decaying (and say, continuous, if that matters) function on the real line; what I have in mind is something like the absolute value of a Schwartz function. Question: Given such a function , does there always exist such that for all ? If yes, can we also find ? (Here denotes Schwartz functions.) Presumably, for the second part, rapid decay is not sufficient. On the other hand if is compactly supported, then the answer to the second question seems to be yes, let for an approriately chosen smooth supported in . Partial  answers or helpful suggestions are welcome as well. Edit: Sorry for confusion over multiple edits.","f \widehat{f} [-1,1] W_0 g\ge 0 g f\in W_0 g(x)\le f(x) x\in\mathbb{R} f\in W=W_0\cap \mathcal{S} \mathcal{S} g f=\widehat{\phi*\phi} \phi [-1/2,1/2]","['real-analysis', 'fourier-analysis', 'fourier-transform', 'harmonic-analysis']"
70,Solution of a coupled gradient system?,Solution of a coupled gradient system?,,"Suppose $U \times V \subset \mathbb R^n \times \mathbb R^n$ is an open set and $\phi, \psi: U \times V \to \mathbb R$ are two $C^{\infty}$ -smooth functions. Furthermore, for each $y \in V$ , $\phi_y: U \to \mathbb R$ has the property: all sublevel sets are compact, i.e., $$S_a = \{x \in U: \phi_y(x) = \phi(x, y) \le a\}$$ is compact for every $a \in \mathbb R$ . For $\psi$ , we have that for each $x$ , the function $\psi_x: V \to \mathbb R$ has compact sublevel sets. We define a system of ODEs as follows $$\begin{align*} \dot{x}(t) = -\partial_x \phi(x, y), \\ \dot{y}(t) = -\partial_y \psi(x, y). \end{align*}$$ Since the partial derivatives are smooth and thus locally Lipschitz, for any initial condition, there should be a local unique solution. My questions: Is it possible to infer the solutions are defined for all time? Is there a name (and reference) for such defined systems? It seems to resemble a gradient system however they are coupled together.","Suppose is an open set and are two -smooth functions. Furthermore, for each , has the property: all sublevel sets are compact, i.e., is compact for every . For , we have that for each , the function has compact sublevel sets. We define a system of ODEs as follows Since the partial derivatives are smooth and thus locally Lipschitz, for any initial condition, there should be a local unique solution. My questions: Is it possible to infer the solutions are defined for all time? Is there a name (and reference) for such defined systems? It seems to resemble a gradient system however they are coupled together.","U \times V \subset \mathbb R^n \times \mathbb R^n \phi, \psi: U \times V \to \mathbb R C^{\infty} y \in V \phi_y: U \to \mathbb R S_a = \{x \in U: \phi_y(x) = \phi(x, y) \le a\} a \in \mathbb R \psi x \psi_x: V \to \mathbb R \begin{align*}
\dot{x}(t) = -\partial_x \phi(x, y), \\
\dot{y}(t) = -\partial_y \psi(x, y).
\end{align*}","['real-analysis', 'ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'gradient-flows']"
71,Existence of continuous $r(t)$ with $\lim_{t \to \infty} \frac{f(r(t))}{g(t)} = 1$,Existence of continuous  with,r(t) \lim_{t \to \infty} \frac{f(r(t))}{g(t)} = 1,"Let $ \ f,g: \mathbb{R} \to \mathbb{R} \ $ be continuously differentiable functions such that $$\lim_{t \to \infty} f(t) = \infty = \lim_{t \to \infty} g(t) \ \ . $$ My question is: Is there a continuous function $ \ r: \mathbb{R} \to \mathbb{R} \ $ such that $$\lim_{t \to \infty} \frac{f \big( r(t) \big)}{g(t)} = 1 \ \ \ \ ? $$ I would like hints for a proof or a counterexample. You may feel free to modify the assumptions about $f$ and $g$ as you please. Thanks in advance.",Let be continuously differentiable functions such that My question is: Is there a continuous function such that I would like hints for a proof or a counterexample. You may feel free to modify the assumptions about and as you please. Thanks in advance.," \ f,g: \mathbb{R} \to \mathbb{R} \  \lim_{t \to \infty} f(t) = \infty = \lim_{t \to \infty} g(t) \ \ .   \ r: \mathbb{R} \to \mathbb{R} \  \lim_{t \to \infty} \frac{f \big( r(t) \big)}{g(t)} = 1 \ \ \ \ ?  f g","['real-analysis', 'calculus', 'limits', 'analysis']"
72,Is a differentiable (but not $C^1$) function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ with invertible derivative everywhere an open map?,Is a differentiable (but not ) function  with invertible derivative everywhere an open map?,C^1 f: \mathbb{R}^n \rightarrow \mathbb{R}^n,"Is a differentiable (but not $C^1$ ) function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ with invertible derivative everywhere an open map?  I know that if we assume the function is $C^1$ , then this is a consequence of the inverse function theorem (or a step on the way to proving the inverse function theorem).  I've convinced myself no counter-example exists if $n = 1$ , but I haven't been able to prove it or come up with a counter-example in the case $n = 2$ .  I've spent more time trying to come up with a counter-example, mostly trying variants involving $x^2 \sin(1/x)$ .","Is a differentiable (but not ) function with invertible derivative everywhere an open map?  I know that if we assume the function is , then this is a consequence of the inverse function theorem (or a step on the way to proving the inverse function theorem).  I've convinced myself no counter-example exists if , but I haven't been able to prove it or come up with a counter-example in the case .  I've spent more time trying to come up with a counter-example, mostly trying variants involving .",C^1 f: \mathbb{R}^n \rightarrow \mathbb{R}^n C^1 n = 1 n = 2 x^2 \sin(1/x),"['real-analysis', 'inverse-function-theorem', 'open-map']"
73,How to prove $f$ is $C^\infty$,How to prove  is,f C^\infty,"Suppose $f:U \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ is  continous  and $$(x^2+y^4)f(x,y)+(f(x,y))^3=1 \: \text{for all} \: (x,y) \in U. $$ Prove $f$ is $C^\infty$ . This kind of exercise is new to me and I don't really have any idea how to derive that the derivative exist  infinitely and it's continous.",Suppose is  continous  and Prove is . This kind of exercise is new to me and I don't really have any idea how to derive that the derivative exist  infinitely and it's continous.,"f:U \subseteq \mathbb{R}^2 \rightarrow \mathbb{R} (x^2+y^4)f(x,y)+(f(x,y))^3=1 \: \text{for all} \: (x,y) \in U.  f C^\infty","['real-analysis', 'multivariable-calculus', 'functional-equations']"
74,Show that there exist $f \in L^1$ but $f^*$ is not integrable over the unit ball .,Show that there exist  but  is not integrable over the unit ball .,f \in L^1 f^*,"In Stein's functional analysis, for some $0\lt t\le 1$ he defined $$ f(x)= \begin{cases} |x|^{-d}\log\left(\dfrac{1}{|x|}\right)^{-1-t}&\text{whenever }0\le |x| \le \frac{1}{2}\\  \\ f(x)=0 &\text{ otherwise} \end{cases} $$ and he state that $$ f^*(x)\ge c|x|^{-d}\log\left(\frac{1}{|x|}\right)^{-t}, $$ then $f^*(x)$ is not integrable over the unit ball. However, I can prove this only when $d=1$ . My attempt : When $d=1$ , $$\int\limits_0^{\frac12} \frac cx \log\left(\frac1x\right)^{\!-t-1} \mathrm{d}x =c \frac{(\log 2) ^{-t}}{t}$$ so $f \in L^1$ , and also we have $$ f^*(y) \ge \frac1y\int\limits_0^y \frac cx \log\left(\frac1x\right)^{\!-t-1} \mathrm{d}x=\frac{c'}{y} \log\left(\frac1y\right)^{\!-t} $$ whenever $y \in \left(0,\frac12\right)$ and RHS is not integrable over the unit ball . However, when $d \neq 1$ , we can not use the relation $$ \int \frac{f(x)}{x} \mathrm{d}x =\int f(x)\, \mathrm{d} \log(x), $$ so how to deduce the desired conclusion?","In Stein's functional analysis, for some he defined and he state that then is not integrable over the unit ball. However, I can prove this only when . My attempt : When , so , and also we have whenever and RHS is not integrable over the unit ball . However, when , we can not use the relation so how to deduce the desired conclusion?","0\lt t\le 1 
f(x)=
\begin{cases}
|x|^{-d}\log\left(\dfrac{1}{|x|}\right)^{-1-t}&\text{whenever }0\le |x| \le \frac{1}{2}\\ 
\\
f(x)=0 &\text{ otherwise}
\end{cases}
 
f^*(x)\ge c|x|^{-d}\log\left(\frac{1}{|x|}\right)^{-t},
 f^*(x) d=1 d=1 \int\limits_0^{\frac12} \frac cx \log\left(\frac1x\right)^{\!-t-1} \mathrm{d}x =c \frac{(\log 2) ^{-t}}{t} f \in L^1 
f^*(y) \ge \frac1y\int\limits_0^y \frac cx \log\left(\frac1x\right)^{\!-t-1} \mathrm{d}x=\frac{c'}{y} \log\left(\frac1y\right)^{\!-t}
 y \in \left(0,\frac12\right) d \neq 1 
\int \frac{f(x)}{x} \mathrm{d}x =\int f(x)\, \mathrm{d} \log(x),
","['real-analysis', 'functional-analysis']"
75,"Show $f(x) = 1/x$ is in $L^2\left([1, +\infty)\right)$ but not in $L^1\left([1, +\infty)\right)$.",Show  is in  but not in .,"f(x) = 1/x L^2\left([1, +\infty)\right) L^1\left([1, +\infty)\right)","Proposition $f(x) = 1/x$ is in $L^2\left([1, +\infty)\right)$ but not in $L^1\left([1, +\infty)\right)$ . Discussion So my issue here is that I don't know how to use infinity in Lebesgue integration. It is intuitive (I think) that evaluation of the improper Riemann integrals \begin{align} \int_1^\infty \left|f(x)\right| &= \int_1^\infty \frac{1}{x} = \lim_{c \to \infty} \ln c = + \infty \\ \\ \int_1^\infty \left|f(x)\right|^2 &= \int_1^\infty \frac{1}{x^2} = 1 - \lim_{c \to \infty} \frac{1}{c} = 1 \end{align} would imply our proposition, but I've only seen $L^p$ -spaces defined in the sense of Lebesgue integrals. So when I get to these steps: \begin{align} \int_{[1, \infty)} \left|f(x)\right| &= \int_{[1, \infty)} \frac{1}{x} = \cdots \\ \\ \int_{[1, \infty)} \left|f(x)\right|^2 &= \int_{[1, \infty)} \frac{1}{x^2} = \cdots \end{align} I'm not sure how to proceed. I'm guessing we need an argument for switching between the two types of integration, which I've read up on a little bit, but am not sure how to apply here in the improper case.","Proposition is in but not in . Discussion So my issue here is that I don't know how to use infinity in Lebesgue integration. It is intuitive (I think) that evaluation of the improper Riemann integrals would imply our proposition, but I've only seen -spaces defined in the sense of Lebesgue integrals. So when I get to these steps: I'm not sure how to proceed. I'm guessing we need an argument for switching between the two types of integration, which I've read up on a little bit, but am not sure how to apply here in the improper case.","f(x) = 1/x L^2\left([1, +\infty)\right) L^1\left([1, +\infty)\right) \begin{align}
\int_1^\infty \left|f(x)\right| &= \int_1^\infty \frac{1}{x} = \lim_{c \to \infty} \ln c = + \infty \\ \\
\int_1^\infty \left|f(x)\right|^2 &= \int_1^\infty \frac{1}{x^2} = 1 - \lim_{c \to \infty} \frac{1}{c} = 1
\end{align} L^p \begin{align}
\int_{[1, \infty)} \left|f(x)\right| &= \int_{[1, \infty)} \frac{1}{x} = \cdots \\ \\
\int_{[1, \infty)} \left|f(x)\right|^2 &= \int_{[1, \infty)} \frac{1}{x^2} = \cdots
\end{align}","['real-analysis', 'integration', 'functional-analysis', 'improper-integrals', 'normed-spaces']"
76,Show that this map is a contraction (PDE),Show that this map is a contraction (PDE),,"I was asked a question by my research advisor and I really don't know how to think about it. The goal is to prove the existence of a function $u(x,t):\mathbb{R}\times[0,T]\to\mathbb{R}$ satisfying the following PDE: $$\begin{cases}\partial_t^2 u-\partial_x^2 u=(\partial_t u)(\partial_x u)\\u(-,0)=f\in H^s(\mathbb{R})\\\partial_tu(-,0)=g\in H^{s-1}(\mathbb{R}),\end{cases}$$ where $H^s(\mathbb R)$ is the Sobolev space $$H^s(\mathbb{R})=\{f:\lVert f\rVert_{s,2}=\lVert\hat f(\xi)(1+\lvert\xi\rvert^2)^{s/2}\rVert_2<\infty\},$$ and $s$ is sufficiently large. To do this, I was told to define the norm $$\lVert u\rVert_X=\sup_{t\in[0,T]}(\lVert u\rVert_{s,2}+\lVert\partial_t u\rVert_{s-1,2})$$ on the space $$X=\{u\in C([0,T];H^s(\mathbb R))\cap C^1([0,T];H^{s-1}(\mathbb R)) :\lVert u\rVert_X\leq c(\lVert f\rVert_{s,2}+\lVert g\rVert_{s-1,2})\}.$$ Now let $w_0$ satisfy the wave equation $\partial_t^2 w_0-\partial_x^2 w_0=0$ with initial data $w_0(-,0)=f$ and $\partial_t w_0(-,0)=g$ , and for $n>0$ let $w_n$ satisfy $$\begin{cases}\partial_t^2 w_n-\partial_x^2 w_n=(\partial_t w_{n-1})(\partial_x w_{n-1})\\w_n(-,0)=f\\\partial_t w_n(-,0)=g.\end{cases}$$ If we can show $\Phi=(w_n\mapsto w_{n+1}):X\to X$ is a contraction mapping and that $X$ is complete, then we are done by the Picard iteration theorem. I do not know how to prove $\Phi$ is a contraction. I understand all of the definitions, I've tried to prove through direct computation, and I've tried to apply any of the relevant theorems I know, but I have had no luck. I also don't know how to show that $\Phi(u)$ even exists for general $u$ . If anyone has an idea of how to proceed with this, I would very much appreciate hearing their thoughts. Thank you.","I was asked a question by my research advisor and I really don't know how to think about it. The goal is to prove the existence of a function satisfying the following PDE: where is the Sobolev space and is sufficiently large. To do this, I was told to define the norm on the space Now let satisfy the wave equation with initial data and , and for let satisfy If we can show is a contraction mapping and that is complete, then we are done by the Picard iteration theorem. I do not know how to prove is a contraction. I understand all of the definitions, I've tried to prove through direct computation, and I've tried to apply any of the relevant theorems I know, but I have had no luck. I also don't know how to show that even exists for general . If anyone has an idea of how to proceed with this, I would very much appreciate hearing their thoughts. Thank you.","u(x,t):\mathbb{R}\times[0,T]\to\mathbb{R} \begin{cases}\partial_t^2 u-\partial_x^2 u=(\partial_t u)(\partial_x u)\\u(-,0)=f\in H^s(\mathbb{R})\\\partial_tu(-,0)=g\in H^{s-1}(\mathbb{R}),\end{cases} H^s(\mathbb R) H^s(\mathbb{R})=\{f:\lVert f\rVert_{s,2}=\lVert\hat f(\xi)(1+\lvert\xi\rvert^2)^{s/2}\rVert_2<\infty\}, s \lVert u\rVert_X=\sup_{t\in[0,T]}(\lVert u\rVert_{s,2}+\lVert\partial_t u\rVert_{s-1,2}) X=\{u\in C([0,T];H^s(\mathbb R))\cap C^1([0,T];H^{s-1}(\mathbb R)) :\lVert u\rVert_X\leq c(\lVert f\rVert_{s,2}+\lVert g\rVert_{s-1,2})\}. w_0 \partial_t^2 w_0-\partial_x^2 w_0=0 w_0(-,0)=f \partial_t w_0(-,0)=g n>0 w_n \begin{cases}\partial_t^2 w_n-\partial_x^2 w_n=(\partial_t w_{n-1})(\partial_x w_{n-1})\\w_n(-,0)=f\\\partial_t w_n(-,0)=g.\end{cases} \Phi=(w_n\mapsto w_{n+1}):X\to X X \Phi \Phi(u) u","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'fourier-analysis']"
77,"Is the set $\{\delta_x\}_{x \in [a, \ b]}$ a basis for the set of distributions on $C^{\infty}_c([a, \ b])$?",Is the set  a basis for the set of distributions on ?,"\{\delta_x\}_{x \in [a, \ b]} C^{\infty}_c([a, \ b])","$\def\braket#1#2{\langle#1|#2\rangle}\def\bra#1{\langle#1}\def\ket#1{#1\rangle}$ Is the set $\{\delta_x\}_{x \in [a,  b]}$ a basis for the set of distributions on $C^{\infty}_c([a, \ b])$ ? Below are p.59 and p.60 from Shankar's book Principles of Quantum Mechanics. According to Shankar, the set $\{\delta_x\}_{x \in [a, \ b]}$ is a basis for  the space of functions on $[a, \ b]$ that vanishes on the set $\{ a, b\}$ . Here, $\delta_x(y) = \delta(x-y) = \braket{x}{y}$ and $\delta_x = |\ket{x}$ . However, Dirac deltas are not functions. Is the set $\{\delta_x\}_{x \in (a, \ b)}$ a basis for the set of distributions on $C^{\infty}_c((a, \ b))$ ? $\varphi:= \int_a^b | \ket{y} \bra{y} | dy $ $\varphi [f] = \int_a^b | \ket{y} \bra{y} | \ket{f} dy = \int_a^b | \ket{y} f(y) dy $ $ \braket{x}{\varphi(f)} =  \int_a^b \braket{x}{y} f(y) dy = \int_a^b \delta(x-y) f(y) dy = f(x) = \braket{x}{f}$ Hence $\varphi(f) = f$ , and $\varphi = id$ . $ \int_a^b | \ket{y} \bra{y} | dy  = id \in \operatorname{Hom}(C_c^{\infty}((a, \ b), C_c^{\infty}((a, \ b))$ (This is equation (1.10.11) in Shankar) $\varphi_g :=\int_a^b \ dy \ g(y) \bra{y}|$ Then $ \varphi_g (|\ket{f}) = \int_a^b \ dy \ g(y) \braket{y}{f} = \int_a^b \ dy \ g(y) f(y) = \int_a^b g f \ dy = \braket{g}{f}$ Hence $g = \int_a^b \ dy \ g(y) \bra{y}|$ in $\operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R})$ ? My questions are: Is the set $\{\delta_x\}_{x \in (a, \ b)}$ a basis for the set of distributions on $C^{\infty}_c((a, \ b))$ ? If 1 is true, then in what sense (e.g. finite sum , countable sum ...) does $\{\delta_x\}_{x \in (a, \ b)}$ span the space of distributions on $C^{\infty}_c((a, \ b))$ ? $g = \int_a^b \ dy \ g(y) \bra{y}|$ in $\operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R})$ ?","Is the set a basis for the set of distributions on ? Below are p.59 and p.60 from Shankar's book Principles of Quantum Mechanics. According to Shankar, the set is a basis for  the space of functions on that vanishes on the set . Here, and . However, Dirac deltas are not functions. Is the set a basis for the set of distributions on ? Hence , and . (This is equation (1.10.11) in Shankar) Then Hence in ? My questions are: Is the set a basis for the set of distributions on ? If 1 is true, then in what sense (e.g. finite sum , countable sum ...) does span the space of distributions on ? in ?","\def\braket#1#2{\langle#1|#2\rangle}\def\bra#1{\langle#1}\def\ket#1{#1\rangle} \{\delta_x\}_{x \in [a,  b]} C^{\infty}_c([a, \ b]) \{\delta_x\}_{x \in [a, \ b]} [a, \ b] \{ a, b\} \delta_x(y) = \delta(x-y) = \braket{x}{y} \delta_x = |\ket{x} \{\delta_x\}_{x \in (a, \ b)} C^{\infty}_c((a, \ b)) \varphi:= \int_a^b | \ket{y} \bra{y} | dy  \varphi [f] = \int_a^b | \ket{y} \bra{y} | \ket{f} dy = \int_a^b | \ket{y} f(y) dy   \braket{x}{\varphi(f)} =  \int_a^b \braket{x}{y} f(y) dy = \int_a^b \delta(x-y) f(y) dy = f(x) = \braket{x}{f} \varphi(f) = f \varphi = id  \int_a^b | \ket{y} \bra{y} | dy  = id \in \operatorname{Hom}(C_c^{\infty}((a, \ b), C_c^{\infty}((a, \ b)) \varphi_g :=\int_a^b \ dy \ g(y) \bra{y}|  \varphi_g (|\ket{f}) = \int_a^b \ dy \ g(y) \braket{y}{f} = \int_a^b \ dy \ g(y) f(y) = \int_a^b g f \ dy = \braket{g}{f} g = \int_a^b \ dy \ g(y) \bra{y}| \operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R}) \{\delta_x\}_{x \in (a, \ b)} C^{\infty}_c((a, \ b)) \{\delta_x\}_{x \in (a, \ b)} C^{\infty}_c((a, \ b)) g = \int_a^b \ dy \ g(y) \bra{y}| \operatorname{Hom}(C^{\infty}_c((a, \ b), \mathbb{R})","['real-analysis', 'functional-analysis', 'mathematical-physics', 'distribution-theory', 'quantum-mechanics']"
78,prove that this function has Lebesgue measurable image,prove that this function has Lebesgue measurable image,,"Denote by $\lambda$ the standard Lebesgue measure. Let $E$ be a Lebesgue-measurable subset of $\mathbb{R}$ with $\lambda(E)<\infty$ . By an initial segment of $E$ we mean a set $E'\subseteq E$ satisfying $E'<E\setminus E'$ (in the sense that $x<y$ for all $x\in E'$ and $y\in E\setminus E'$ ).  Note that initial segments of $E$ must always have the form $E\cap(-\infty,y)$ or $E\cap(-\infty,y]$ for some $y\in[-\infty,\infty]$ . It can be shown that for each $t\in[0,\lambda(E)]$ , there exists an initial segment $E_t$ of $E$ with $\lambda(E_t)=t$ . Let us define the function $m:E\to[0,\lambda(E)]$ by the rule $$m(x)=\inf\{t\in[0,\lambda(E)]:x\in E_t\}.$$ Conjecture 1. The image $m(E)$ is a Lebesgue-measurable set. Discussion. (i) Clearly, $m$ is order-preserving (i.e., nondecreasing) in the sense that $x\leq y$ if and only if $m(x)\leq m(y)$ . (ii) It can be shown that $m$ is measure-preserving in the following sense:  If $A\subseteq[0,\lambda(E)]$ is Lebesgue-measurable then $m^{-1}(A)$ is also Lebesgue-measurable with $\lambda(A)=\lambda[m^{-1}(A)]$ . (iii) If $F\subseteq E$ and $m(F)$ is measurable then $\lambda[m(F)]=\lambda(F)$ . (iv) There are definite counter-examples showing that $m$ need not be surjective. In fact, $[0,\lambda(E)]\setminus m(E)$ may even be uncountable. Sorry to keep asking so many similar questions.  I keep running into these technical, seemingly obvious facts which are resistant to a simple proof (that I can find, anyway).","Denote by the standard Lebesgue measure. Let be a Lebesgue-measurable subset of with . By an initial segment of we mean a set satisfying (in the sense that for all and ).  Note that initial segments of must always have the form or for some . It can be shown that for each , there exists an initial segment of with . Let us define the function by the rule Conjecture 1. The image is a Lebesgue-measurable set. Discussion. (i) Clearly, is order-preserving (i.e., nondecreasing) in the sense that if and only if . (ii) It can be shown that is measure-preserving in the following sense:  If is Lebesgue-measurable then is also Lebesgue-measurable with . (iii) If and is measurable then . (iv) There are definite counter-examples showing that need not be surjective. In fact, may even be uncountable. Sorry to keep asking so many similar questions.  I keep running into these technical, seemingly obvious facts which are resistant to a simple proof (that I can find, anyway).","\lambda E \mathbb{R} \lambda(E)<\infty E E'\subseteq E E'<E\setminus E' x<y x\in E' y\in E\setminus E' E E\cap(-\infty,y) E\cap(-\infty,y] y\in[-\infty,\infty] t\in[0,\lambda(E)] E_t E \lambda(E_t)=t m:E\to[0,\lambda(E)] m(x)=\inf\{t\in[0,\lambda(E)]:x\in E_t\}. m(E) m x\leq y m(x)\leq m(y) m A\subseteq[0,\lambda(E)] m^{-1}(A) \lambda(A)=\lambda[m^{-1}(A)] F\subseteq E m(F) \lambda[m(F)]=\lambda(F) m [0,\lambda(E)]\setminus m(E)","['real-analysis', 'measure-theory', 'lebesgue-measure']"
79,Find whether : $\sum_{ I \subset \mathbb{N}} e^{-\sqrt{S(I)}}$ converges,Find whether :  converges,\sum_{ I \subset \mathbb{N}} e^{-\sqrt{S(I)}},"Does the following sum converges : $$\sum_{ I \subset \mathbb{N}} e^{-\sqrt{S(I)}}$$ where : $$ S(I) =\sum_{ i \in I} i$$ I don’t know how to approach this problem. Nevertheless, maybe it’s possible to have some intuition about the problem. If $\mid I \mid = n$ then we know that the $S(I)$ which are going to give an important weight to the sum are in $O(n^2)$ . Hence maybe luckily : $$\sum_{I \subset \mathbb{N}, S(I) = O(\mid I \mid ^2)} e^{-\sqrt{S(I)}}$$ has the same nature of our sum. In this case the square root makes it easy and the sum converges. Yet it’s possible that this intuition is false since there are a lot of $S(I) \ne O(\mid I \mid ^2)$ (infinitely many actually) so it might give a lot of weight to the sum... I don’t really know. Note that in order that $S(I)$ makes sens, we have : $I < \infty$ .","Does the following sum converges : where : I don’t know how to approach this problem. Nevertheless, maybe it’s possible to have some intuition about the problem. If then we know that the which are going to give an important weight to the sum are in . Hence maybe luckily : has the same nature of our sum. In this case the square root makes it easy and the sum converges. Yet it’s possible that this intuition is false since there are a lot of (infinitely many actually) so it might give a lot of weight to the sum... I don’t really know. Note that in order that makes sens, we have : .","\sum_{ I \subset \mathbb{N}} e^{-\sqrt{S(I)}}  S(I) =\sum_{ i \in I} i \mid I \mid = n S(I) O(n^2) \sum_{I \subset \mathbb{N}, S(I) = O(\mid I \mid ^2)} e^{-\sqrt{S(I)}} S(I) \ne O(\mid I \mid ^2) S(I) I < \infty","['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'limits']"
80,"Are there any non-algebraic, non-transcendental complex numbers? Is $0$ a pure imaginary number?","Are there any non-algebraic, non-transcendental complex numbers? Is  a pure imaginary number?",0,"I came across this question here: Difference between imaginary and complex numbers The top answer contains this diagram: Here we see numbers like $e - \pi i$ and $\pi + 3i$ existing outside of transcendental and algebraic numbers but within the realm of complex numbers. Is this accurate or should they technically be in the transcendental area? Are there any complex non-transcendental non-algebraic numbers? We also see $0$ as a whole number, an integer, a rational number, a real number, an algebraic number, and a complex number, but is it not also a pure imaginary number?","I came across this question here: Difference between imaginary and complex numbers The top answer contains this diagram: Here we see numbers like and existing outside of transcendental and algebraic numbers but within the realm of complex numbers. Is this accurate or should they technically be in the transcendental area? Are there any complex non-transcendental non-algebraic numbers? We also see as a whole number, an integer, a rational number, a real number, an algebraic number, and a complex number, but is it not also a pure imaginary number?",e - \pi i \pi + 3i 0,"['real-analysis', 'complex-analysis', 'number-theory', 'complex-numbers', 'definition']"
81,approximate identities for convolution with measure,approximate identities for convolution with measure,,"Convolution makes $L^1(\mathbb{R}^n, \mathbb{C})$ into an associative algebra that has no identity, but that does have an ""approximate identity"" in the sense that for any sequence $\varphi_1, \varphi_2,...$ of nonnegative $L^1$ functions with $\int \varphi_k~d\lambda=1$ and $$\lim_{k\rightarrow \infty}\int_{|x|>\epsilon}\varphi_k ~~d\lambda(x)  =0~~~\text{for any }\epsilon>0$$ and any $f\in L^1$ , we have $ \varphi_k \ast f \rightarrow f$ in $L^1$ (where $\lambda$ is Lebesgue measure). Is the same true (and in what sense?) if we replace $f$ with a Borel probability measure which is not necessarily absolutely continuous w.r.t. Lebesgue measure? I.e. with $\varphi_k$ as above, define $$(\varphi_k \ast \mu)(y) := \int_{\mathbb{R^n}} \varphi_k(y-x)d\mu(x).$$ Do we have $$(\varphi_k \ast \mu) \lambda \rightarrow \mu$$ in some sense? Still another way to ask it is whether we have $$(\varphi_k \lambda) \ast \mu \rightarrow \mu$$ in some sense (where this $\ast$ means convolution of measures)?","Convolution makes into an associative algebra that has no identity, but that does have an ""approximate identity"" in the sense that for any sequence of nonnegative functions with and and any , we have in (where is Lebesgue measure). Is the same true (and in what sense?) if we replace with a Borel probability measure which is not necessarily absolutely continuous w.r.t. Lebesgue measure? I.e. with as above, define Do we have in some sense? Still another way to ask it is whether we have in some sense (where this means convolution of measures)?","L^1(\mathbb{R}^n, \mathbb{C}) \varphi_1, \varphi_2,... L^1 \int \varphi_k~d\lambda=1 \lim_{k\rightarrow \infty}\int_{|x|>\epsilon}\varphi_k ~~d\lambda(x)  =0~~~\text{for any }\epsilon>0 f\in L^1  \varphi_k \ast f \rightarrow f L^1 \lambda f \varphi_k (\varphi_k \ast \mu)(y) := \int_{\mathbb{R^n}} \varphi_k(y-x)d\mu(x). (\varphi_k \ast \mu) \lambda \rightarrow \mu (\varphi_k \lambda) \ast \mu \rightarrow \mu \ast","['real-analysis', 'analysis']"
82,Proving a convexity inequality,Proving a convexity inequality,,"Given $f: \mathbb{R} \to \mathbb{R}$ convex, show that: $$ \frac{2}{3}\left(f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right)\right) \leq f\left(\frac{x+y+z}{3}\right) + \frac{f(x) + f(y) + f(z)}{3}.$$ I have tried some ideas, such as transforming it into $$ f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right) - 3f\left(\frac{x+y+z}{3}\right)\\ \leq  f(x) + f(y) + f(z) - f\left(\frac{x+y}{2}\right) - f\left(\frac{z+y}{2}\right) - f\left(\frac{x+z}{2}\right) $$ (which graphically seemed intuitive) and using that such an $f$ lies above its tangents, but did not succeed… Ideas are welcome :)","Given convex, show that: I have tried some ideas, such as transforming it into (which graphically seemed intuitive) and using that such an lies above its tangents, but did not succeed… Ideas are welcome :)","f: \mathbb{R} \to \mathbb{R}  \frac{2}{3}\left(f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right)\right) \leq f\left(\frac{x+y+z}{3}\right) + \frac{f(x) + f(y) + f(z)}{3}.  f\left(\frac{x+y}{2}\right) + f\left(\frac{z+y}{2}\right) + f\left(\frac{x+z}{2}\right) - 3f\left(\frac{x+y+z}{3}\right)\\
\leq  f(x) + f(y) + f(z) - f\left(\frac{x+y}{2}\right) - f\left(\frac{z+y}{2}\right) - f\left(\frac{x+z}{2}\right)  f","['real-analysis', 'inequality', 'convex-analysis', 'jensen-inequality', 'karamata-inequality']"
83,Continuity of a harmonic integral,Continuity of a harmonic integral,,"Consider the open unit disk $\mathbb{D}$ in $\mathbb{R}^2$ . Given a continuous function $f : \partial\mathbb{D} \to \mathbb{R}$ we define $$ u(x) \overset{\texttt{def}}{=} \int_{\partial \mathbb{D}} f(y)\ln\left\vert x-y\right\vert\,\mathrm{d}S(y) $$ for any point $x \in \overline{\mathbb{D}}$ . I have proven that this integral exists for all $x$ belonging to the closure of $\mathbb{D}$ . Using the dominated convergence theorem, I have also shown that $u$ is harmonic in $\mathbb{D}$ . In particular, $u \in C(\mathbb{D})$ . However, I have been unable to show that $u$ is continuous up to the boundary of $\mathbb{D}$ . Dominated convergence does not seem to apply here since the logarithm blows up as $x$ approaches a point on the boundary.  I suspect that I will have to use the fact that $u$ is harmonic in the open disk to prove this. Any help would be greatly appreciated.","Consider the open unit disk in . Given a continuous function we define for any point . I have proven that this integral exists for all belonging to the closure of . Using the dominated convergence theorem, I have also shown that is harmonic in . In particular, . However, I have been unable to show that is continuous up to the boundary of . Dominated convergence does not seem to apply here since the logarithm blows up as approaches a point on the boundary.  I suspect that I will have to use the fact that is harmonic in the open disk to prove this. Any help would be greatly appreciated.","\mathbb{D} \mathbb{R}^2 f : \partial\mathbb{D} \to \mathbb{R} 
u(x) \overset{\texttt{def}}{=} \int_{\partial \mathbb{D}} f(y)\ln\left\vert x-y\right\vert\,\mathrm{d}S(y)
 x \in \overline{\mathbb{D}} x \mathbb{D} u \mathbb{D} u \in C(\mathbb{D}) u \mathbb{D} x u",['real-analysis']
84,$\int_{1}^\infty\frac{\sin(x^2)}{x^p}dx$ for which p values does the integral converge in condition,for which p values does the integral converge in condition,\int_{1}^\infty\frac{\sin(x^2)}{x^p}dx,"$$\int_1^\infty\frac{\sin(x^2)}{x^p} \, dx$$  for which p values does the integral converge in condition? and for which values does it converge absolutely? I managed to find the values of $p$ which will make the integral to converge absolutely , they are $p>1$, but i could not manage to solve for the condition values of $p.$ $$\int_1^\infty\frac{\sin(x^2)}{x^p} \, dx $$ assign $t=x^2$ you will get :  $$\frac{1}{2}\int_1^\infty\frac{\sin(t)}{t^\frac{p+1}{2}} \, dt$$ $$\int_1^\infty \left|\frac{\sin(t)}{t^\frac{p+1}{2}}\,dt\right| \le \int_1^\infty\frac{1}{t^\frac{p+1}{2}}\,dt$$ hence if $\frac{p+1}{2}>1 \to p>1$ the integral converges. In the answers the value for the conditional convergence is for $0<p\le1$ can't understand why","$$\int_1^\infty\frac{\sin(x^2)}{x^p} \, dx$$  for which p values does the integral converge in condition? and for which values does it converge absolutely? I managed to find the values of $p$ which will make the integral to converge absolutely , they are $p>1$, but i could not manage to solve for the condition values of $p.$ $$\int_1^\infty\frac{\sin(x^2)}{x^p} \, dx $$ assign $t=x^2$ you will get :  $$\frac{1}{2}\int_1^\infty\frac{\sin(t)}{t^\frac{p+1}{2}} \, dt$$ $$\int_1^\infty \left|\frac{\sin(t)}{t^\frac{p+1}{2}}\,dt\right| \le \int_1^\infty\frac{1}{t^\frac{p+1}{2}}\,dt$$ hence if $\frac{p+1}{2}>1 \to p>1$ the integral converges. In the answers the value for the conditional convergence is for $0<p\le1$ can't understand why",,"['calculus', 'real-analysis', 'integration']"
85,Riemann sum of $x\cdot \ln(x)$,Riemann sum of,x\cdot \ln(x),"I did not find any information regarding this Riemann sum anywhere: Riemann sum of $f(x)=\begin{cases} 0& x=0 \\ x\cdot \ln(x)& \text{otherwise}\end{cases}$ in the interval $[0, 1]$. I don't want the answer given to me, I'm only looking for a hint that could guide me in the right direction. I have already proved that the function is continuous in this interval, i.e can be integrated in it also.","I did not find any information regarding this Riemann sum anywhere: Riemann sum of $f(x)=\begin{cases} 0& x=0 \\ x\cdot \ln(x)& \text{otherwise}\end{cases}$ in the interval $[0, 1]$. I don't want the answer given to me, I'm only looking for a hint that could guide me in the right direction. I have already proved that the function is continuous in this interval, i.e can be integrated in it also.",,"['real-analysis', 'riemann-sum']"
86,Finding a set of continuous functions with a certain property 2,Finding a set of continuous functions with a certain property 2,,"I need help finding the set of continuous functions $f : \Bbb R \to \Bbb R$ such that for all $x \in \Bbb R$, the following integral converges: $$\int_0^1 \frac {f(x+t) - f(x)} {t^2} \ \mathrm dt$$ I think it might be the set of constant functions but i havent been able to prove it :( I was thinking that you can use the stone weiestrass theorem considering the set of continuous functions on a closed  interval(non trivial) ,and a subset which contains the set of continuous functions whose integral above diverges in some point in that interval along with  with the set of constant functions. So in order to solve the problem i need only to prove that if two functions do not meet the condition of the problem then their product does not as well . I hope you can provide some insight and thank you .","I need help finding the set of continuous functions $f : \Bbb R \to \Bbb R$ such that for all $x \in \Bbb R$, the following integral converges: $$\int_0^1 \frac {f(x+t) - f(x)} {t^2} \ \mathrm dt$$ I think it might be the set of constant functions but i havent been able to prove it :( I was thinking that you can use the stone weiestrass theorem considering the set of continuous functions on a closed  interval(non trivial) ,and a subset which contains the set of continuous functions whose integral above diverges in some point in that interval along with  with the set of constant functions. So in order to solve the problem i need only to prove that if two functions do not meet the condition of the problem then their product does not as well . I hope you can provide some insight and thank you .",,"['calculus', 'real-analysis', 'general-topology']"
87,"Doubt about the convergence of $\displaystyle \sum_k^{n-1}f(\frac{k+\theta} n)-n\int_0^1 f(x)\,dx$",Doubt about the convergence of,"\displaystyle \sum_k^{n-1}f(\frac{k+\theta} n)-n\int_0^1 f(x)\,dx","Let $\theta\in[0,1]$ be a constant and $f\in C^1[0,1] $. Show that $$\sum_k^{n-1}f\left(\frac{k+\theta} n\right)-n\int_0^1 f(x)\,dx$$ converges to $(\theta-\frac{1}{2})(f(1)-f(0))$, as $n\to\infty.$ \begin{align} & S_n=\sum_k^{n-1}f\left(\frac{k+\theta} n\right)-n\int_0^1 f(x)\,dx \\[10pt] ={} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \left( f \left(\frac{k+\theta} n\right)-f(x)\right) \, dx \\[10pt] = {} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \int_x^{(k+\theta)/n} f'(s) \, ds \, dx \\[10pt] = {} &  n\sum_k^{n-1} \left( \int_{k/n}^{(k+\theta)/n} \int_x^{(k+\theta)/n}-\int_{k+\theta/n}^{(k+1)/n} \int_{(k+\theta)/n}^x \right)f'(s)\,ds\,dx \end{align} \begin{align} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \int_x^{(k+\theta)/n} f'(s) \, ds \, dx \\[10pt] = {} & n\sum_k^{n-1} \left( \int_{k/n}^{(k+\theta)/n}\int_x^{(k+\theta)/n}-\int_{k+\theta/n}^{(k+1)/n} \int_{(k+\theta)/n}^x \right) f'(s) \, ds \, dx \end{align} \begin{align}\int_{k/n}^{(k+\theta)/n}\int_x^{(k+\theta)/n} f'(s) \, ds \, dx=\int_{k/n}^{(k+\theta)/n}\int_{(k)/n}^s f'(s) \, ds \, dx=\int_{k/n}^{(k+\theta)/n}f'(s)(s-\frac{k}{n}) \, ds  \int_{k+\theta/n}^{(k+1)/n}\int_{(k+\theta)/n}^x f'(s) \, ds \, dx=\int_{k+\theta/n}^{(k+1)/n}\int_s^{(k+1)/n} f'(s) \, ds \, dx=\int_{k+\theta/n}^{(k+1)/n}f'(s)(\frac{k+1}{n}-s) \, ds\end{align} Therefore \begin{align} \sum_k^{n-1} \int_{k/n}^{(k+1)/n} f'(s)\phi_k(s) \, ds \end{align} where \begin{align} \phi_k(s)=ns-k-\begin{cases} 0, & \mbox{ if }(k\leqslant ns <k+\theta)\\1, & \mbox{ if }k+\theta\leqslant ns <k+1\end{cases}\end{align}, Denoting by $\{x\}$ the fractional part of $x$ and noticing that $k=[ns]$ for $k\leqslant ns\leqslant k+1$, we see that $\phi_k(s)=g(ns)$, where \begin{align}g(x)=\{x\}-\begin{cases} 0, & \mbox{ if }(\{x\}<\theta)\\1, & \mbox{ if }(\{x\}\geqslant \theta)\end{cases}=\{x-\theta\}+\theta-1\end{align} is a piecewise linear periodic function of period 1.Any piecewise continuous periodic function $g(x)$, we conclude that \begin{align}S_n=\int_0^1 f'(x)g(nx)dx\to \mathscr{M}_[0,1](g)\times \int_0^1 f'(x) dx=(\theta-\frac{1}{2})(f(1)-f(0))\end{align} as $n\to\infty$ because $f'\in\mathscr{C}_[0,1]$ Question : I do not understand what the author does when it is defined  $g(x)=\{x\}-\begin{cases} 0, & \mbox{ if }(\{x\}<\theta)\\1, & \mbox{ if }(\{x\}\geqslant \theta)\end{cases}=\{x-\theta\}+\theta-1$ Where does this come from? How did he derive it? How does the author prove \begin{align}S_n=\int_0^1 f'(x)g(nx)dx\to \mathscr{M}_[0,1](g)\times \int_0^1 f'(x) dx=(\theta-\frac{1}{2})(f(1)-f(0))\end{align} Where did $\sum_{k=0}^{n-1}$ go to? Thanks in advance!","Let $\theta\in[0,1]$ be a constant and $f\in C^1[0,1] $. Show that $$\sum_k^{n-1}f\left(\frac{k+\theta} n\right)-n\int_0^1 f(x)\,dx$$ converges to $(\theta-\frac{1}{2})(f(1)-f(0))$, as $n\to\infty.$ \begin{align} & S_n=\sum_k^{n-1}f\left(\frac{k+\theta} n\right)-n\int_0^1 f(x)\,dx \\[10pt] ={} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \left( f \left(\frac{k+\theta} n\right)-f(x)\right) \, dx \\[10pt] = {} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \int_x^{(k+\theta)/n} f'(s) \, ds \, dx \\[10pt] = {} &  n\sum_k^{n-1} \left( \int_{k/n}^{(k+\theta)/n} \int_x^{(k+\theta)/n}-\int_{k+\theta/n}^{(k+1)/n} \int_{(k+\theta)/n}^x \right)f'(s)\,ds\,dx \end{align} \begin{align} & n\sum_k^{n-1} \int_{k/n}^{(k+1)/n} \int_x^{(k+\theta)/n} f'(s) \, ds \, dx \\[10pt] = {} & n\sum_k^{n-1} \left( \int_{k/n}^{(k+\theta)/n}\int_x^{(k+\theta)/n}-\int_{k+\theta/n}^{(k+1)/n} \int_{(k+\theta)/n}^x \right) f'(s) \, ds \, dx \end{align} \begin{align}\int_{k/n}^{(k+\theta)/n}\int_x^{(k+\theta)/n} f'(s) \, ds \, dx=\int_{k/n}^{(k+\theta)/n}\int_{(k)/n}^s f'(s) \, ds \, dx=\int_{k/n}^{(k+\theta)/n}f'(s)(s-\frac{k}{n}) \, ds  \int_{k+\theta/n}^{(k+1)/n}\int_{(k+\theta)/n}^x f'(s) \, ds \, dx=\int_{k+\theta/n}^{(k+1)/n}\int_s^{(k+1)/n} f'(s) \, ds \, dx=\int_{k+\theta/n}^{(k+1)/n}f'(s)(\frac{k+1}{n}-s) \, ds\end{align} Therefore \begin{align} \sum_k^{n-1} \int_{k/n}^{(k+1)/n} f'(s)\phi_k(s) \, ds \end{align} where \begin{align} \phi_k(s)=ns-k-\begin{cases} 0, & \mbox{ if }(k\leqslant ns <k+\theta)\\1, & \mbox{ if }k+\theta\leqslant ns <k+1\end{cases}\end{align}, Denoting by $\{x\}$ the fractional part of $x$ and noticing that $k=[ns]$ for $k\leqslant ns\leqslant k+1$, we see that $\phi_k(s)=g(ns)$, where \begin{align}g(x)=\{x\}-\begin{cases} 0, & \mbox{ if }(\{x\}<\theta)\\1, & \mbox{ if }(\{x\}\geqslant \theta)\end{cases}=\{x-\theta\}+\theta-1\end{align} is a piecewise linear periodic function of period 1.Any piecewise continuous periodic function $g(x)$, we conclude that \begin{align}S_n=\int_0^1 f'(x)g(nx)dx\to \mathscr{M}_[0,1](g)\times \int_0^1 f'(x) dx=(\theta-\frac{1}{2})(f(1)-f(0))\end{align} as $n\to\infty$ because $f'\in\mathscr{C}_[0,1]$ Question : I do not understand what the author does when it is defined  $g(x)=\{x\}-\begin{cases} 0, & \mbox{ if }(\{x\}<\theta)\\1, & \mbox{ if }(\{x\}\geqslant \theta)\end{cases}=\{x-\theta\}+\theta-1$ Where does this come from? How did he derive it? How does the author prove \begin{align}S_n=\int_0^1 f'(x)g(nx)dx\to \mathscr{M}_[0,1](g)\times \int_0^1 f'(x) dx=(\theta-\frac{1}{2})(f(1)-f(0))\end{align} Where did $\sum_{k=0}^{n-1}$ go to? Thanks in advance!",,"['calculus', 'real-analysis', 'integration']"
88,Compactly supported bounded functions with zero integral are dense in Lp spaces,Compactly supported bounded functions with zero integral are dense in Lp spaces,,"As in the title, I want to know if compactly supported bounded functions with zero mean, i.e. $\int f dx= 0$, are dense in $L^p$ for any $ 1 <p< \infty $. I feel like there should be some counterexamples to this but could not think of one. Thanks in advance.","As in the title, I want to know if compactly supported bounded functions with zero mean, i.e. $\int f dx= 0$, are dense in $L^p$ for any $ 1 <p< \infty $. I feel like there should be some counterexamples to this but could not think of one. Thanks in advance.",,['real-analysis']
89,Some nice theorem to find the value of serie,Some nice theorem to find the value of serie,,"I know a lot of techniques to show that a serie converge or diverge (Cauchy condensation test, Abel's test, comparision with an integrale, root test, ratio test...). On the other hand I know nothing about caculating the value of a serie. Is there any theorems that may help ? From now on I know the following : integrale test can be combine with squeeze theorem geometric serie For example in this [ How to prove $\lim \limits_{x \to 1^-} \sum\limits_{n=0}^\infty (-1)^nx^{n²} = \frac{1}{2} \ $? there are talking about : Herglotz' trick Weierstrass products. ... So I would like to know if there are some other nice theorem or slick techniques to find the value of a serie. Thank you !","I know a lot of techniques to show that a serie converge or diverge (Cauchy condensation test, Abel's test, comparision with an integrale, root test, ratio test...). On the other hand I know nothing about caculating the value of a serie. Is there any theorems that may help ? From now on I know the following : integrale test can be combine with squeeze theorem geometric serie For example in this [ How to prove $\lim \limits_{x \to 1^-} \sum\limits_{n=0}^\infty (-1)^nx^{n²} = \frac{1}{2} \ $? there are talking about : Herglotz' trick Weierstrass products. ... So I would like to know if there are some other nice theorem or slick techniques to find the value of a serie. Thank you !",,"['calculus', 'real-analysis', 'sequences-and-series', 'big-list']"
90,"$X_n \to X, Y_n \to c$ in distribution implies $X_n Y_n \to Xc$ in distribution",in distribution implies  in distribution,"X_n \to X, Y_n \to c X_n Y_n \to Xc","I am trying to prove $$X_n \xrightarrow{d} X, Y_n \xrightarrow{d} a \implies Y_n X_n \xrightarrow{d} aX$$ where $a$ is a constant. What I tried: Let $g:\mathbb R\to \mathbb R$ an arbitrary uniformly continuous, bounded function. It suffices to show $\mathbb E[g(Y_n X_n)] \to \mathbb E[g(aX)]$. We have  $$\left \lvert \int g(Y_nX_n) - g(aX) \,dP \right \rvert \leq \left \lvert \int g(Y_n X_n) - g(aX_n) \, dP \right \rvert +\left \lvert \int g(aX_n) - g(aX) \, dP \right \rvert,$$ where the right summand goes to $0$ by assumption as $a$ is constant. Now I want to use uniform continuity of $g$ to estimate the left summand: Choose $\delta > 0 $ such that $$\left \lvert g(Y_n X_n) - g(aX_n) \right \rvert < \epsilon,$$ whenever $|Y_n X_n - aX_n| < \delta.$ Since convergence in distribution to a constant implies convergence in probability, we have can control $P(|Y_n - a | > \delta)$. But is it possible to get a bound on $|X_n|$? I assume that probability in distribution does not imply some form of boundedness... Any help is appreciated.","I am trying to prove $$X_n \xrightarrow{d} X, Y_n \xrightarrow{d} a \implies Y_n X_n \xrightarrow{d} aX$$ where $a$ is a constant. What I tried: Let $g:\mathbb R\to \mathbb R$ an arbitrary uniformly continuous, bounded function. It suffices to show $\mathbb E[g(Y_n X_n)] \to \mathbb E[g(aX)]$. We have  $$\left \lvert \int g(Y_nX_n) - g(aX) \,dP \right \rvert \leq \left \lvert \int g(Y_n X_n) - g(aX_n) \, dP \right \rvert +\left \lvert \int g(aX_n) - g(aX) \, dP \right \rvert,$$ where the right summand goes to $0$ by assumption as $a$ is constant. Now I want to use uniform continuity of $g$ to estimate the left summand: Choose $\delta > 0 $ such that $$\left \lvert g(Y_n X_n) - g(aX_n) \right \rvert < \epsilon,$$ whenever $|Y_n X_n - aX_n| < \delta.$ Since convergence in distribution to a constant implies convergence in probability, we have can control $P(|Y_n - a | > \delta)$. But is it possible to get a bound on $|X_n|$? I assume that probability in distribution does not imply some form of boundedness... Any help is appreciated.",,"['real-analysis', 'probability', 'probability-theory', 'probability-distributions', 'convergence-divergence']"
91,Set of equalities,Set of equalities,,"If $x, y$ and $z$ are natural numbers for which $x^{2}-y = k^{2},$ $y^{2}-z = m^{2},$ $z^{2}-x = n^{2},$ (where $k, m, n$ are integers),  how can we prove that the only solution is $x=y=z=1$? I have tried the basics, $x^{2}-k^{2}=y$ and $y^{2}=m^{2}+z$ etc but I can't get anywhere. Any ideas are most welcome!","If $x, y$ and $z$ are natural numbers for which $x^{2}-y = k^{2},$ $y^{2}-z = m^{2},$ $z^{2}-x = n^{2},$ (where $k, m, n$ are integers),  how can we prove that the only solution is $x=y=z=1$? I have tried the basics, $x^{2}-k^{2}=y$ and $y^{2}=m^{2}+z$ etc but I can't get anywhere. Any ideas are most welcome!",,"['calculus', 'real-analysis']"
92,Prove the maximum of two integrals of functions is less than or equal to the integral of the maximum of two functions,Prove the maximum of two integrals of functions is less than or equal to the integral of the maximum of two functions,,"Prove that on an interval $[a,b]$, if $f$ and $g$ are integrable, than $\max(f,g)$ is integrable and  $$\max(\int_a^b f(x)dx, \int_a^b g(x)dx) \leq \int_a^b \max(f(x), g(x))dx. $$ I proved the first part by $$\max(a,b)=\frac{a+b+|a-b|}{2}$$ so $$\max(f(x),g(x))=\frac{f(x)+g(x)+|f(x)-g(x)|}{2}$$ The absolute value, the sum, and the difference, and the multiplication by a constant of Riemann integrable functions are all Riemann integrable, hence the max is also integrable. Can anyone help with the second part? Many thanks!","Prove that on an interval $[a,b]$, if $f$ and $g$ are integrable, than $\max(f,g)$ is integrable and  $$\max(\int_a^b f(x)dx, \int_a^b g(x)dx) \leq \int_a^b \max(f(x), g(x))dx. $$ I proved the first part by $$\max(a,b)=\frac{a+b+|a-b|}{2}$$ so $$\max(f(x),g(x))=\frac{f(x)+g(x)+|f(x)-g(x)|}{2}$$ The absolute value, the sum, and the difference, and the multiplication by a constant of Riemann integrable functions are all Riemann integrable, hence the max is also integrable. Can anyone help with the second part? Many thanks!",,"['real-analysis', 'integration', 'riemann-integration']"
93,Numerically stable simplification of sinc function,Numerically stable simplification of sinc function,,"I would like to know if there is an alternate, explicit (non-iterative) form of the sinc function which behaves in a numerically stable way for all real numbers. The definition I am aware of is: $$\texttt{sinc}(a) = \dfrac{\sin(a)}{a}$$ However, although this function is well-defined everywhere (even at 0) , this function presents problems when implemented in code and evaluated at 0. I would like to know if there is an alternate form involving simple functions which would avoid possible division by 0 when we try to implement this is code. Some programming packages (like numpy in python) provide a sinc function, presumably to sidestep this issue. But I would like to know generally if there is a better way of implementing it in an arbitrary language without resorting to some if statement. For instance, numpy simply checks if a=0 . If it is, then it replaces a with some small value $\neq 0$. Specifically: def sinc(x):     y = pi * where(x == 0, 1.0e-20, x)     return sin(y)/y EDIT As Sangchul Lee points out, you could consider the series form, but that representation will break down for large $a$. So the series form is essentially pushing the problem somewhere else (from a numerical point of view). Furthermore, you might consider switching between the two forms depending on whether $a$ is small, but that is introducing its own if statement, and I would like to avoid a piecewise solution. Note: I have no idea what tags to use here. So please update the tags to what you view as appropriate.","I would like to know if there is an alternate, explicit (non-iterative) form of the sinc function which behaves in a numerically stable way for all real numbers. The definition I am aware of is: $$\texttt{sinc}(a) = \dfrac{\sin(a)}{a}$$ However, although this function is well-defined everywhere (even at 0) , this function presents problems when implemented in code and evaluated at 0. I would like to know if there is an alternate form involving simple functions which would avoid possible division by 0 when we try to implement this is code. Some programming packages (like numpy in python) provide a sinc function, presumably to sidestep this issue. But I would like to know generally if there is a better way of implementing it in an arbitrary language without resorting to some if statement. For instance, numpy simply checks if a=0 . If it is, then it replaces a with some small value $\neq 0$. Specifically: def sinc(x):     y = pi * where(x == 0, 1.0e-20, x)     return sin(y)/y EDIT As Sangchul Lee points out, you could consider the series form, but that representation will break down for large $a$. So the series form is essentially pushing the problem somewhere else (from a numerical point of view). Furthermore, you might consider switching between the two forms depending on whether $a$ is small, but that is introducing its own if statement, and I would like to avoid a piecewise solution. Note: I have no idea what tags to use here. So please update the tags to what you view as appropriate.",,"['real-analysis', 'geometry']"
94,"$\frac{dx}{dt}, \frac{dx}{d\frac{dx}{dt}}, \frac{dx}{d\frac{dx}{d\frac{dx}{dt}}}$ ad infinitum to $\sqrt{2x}$",ad infinitum to,"\frac{dx}{dt}, \frac{dx}{d\frac{dx}{dt}}, \frac{dx}{d\frac{dx}{d\frac{dx}{dt}}} \sqrt{2x}","Firstly, for a finite such tower, say $n$ deep, assuming there are no divisions by $0$ to get in the way $$\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}} = \frac{\mathrm{d}x}{\mathrm{d}t}\Big(\frac d{\mathrm{d}t}\big(\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}}\big)\Big)^{-1} = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}(...)}\Big)}\Big)}$$ The bottom of this tower is $$ \frac{\mathrm{d}x}{d \frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{\mathrm{d}x}{\mathrm{d}t}\frac{\mathrm{d}t}{d\frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{x'}{x''}$$ and we can, at least in principle, work back up to the top to have the whole tower expressed as upto $n^{th}$ derivatives. I did write out what the next layers would look like but I couldn't see any nice pattern. Ok so now my question is whether it would be in any sense meaningful to consider the infinite tower, in which case the following analysis happens: Writing $F = \frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}}$ , we have $$F = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac{dF}{\mathrm{d}t}} \implies F = \sqrt{2x + c}$$","Firstly, for a finite such tower, say deep, assuming there are no divisions by to get in the way The bottom of this tower is and we can, at least in principle, work back up to the top to have the whole tower expressed as upto derivatives. I did write out what the next layers would look like but I couldn't see any nice pattern. Ok so now my question is whether it would be in any sense meaningful to consider the infinite tower, in which case the following analysis happens: Writing , we have",n 0 \frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}} = \frac{\mathrm{d}x}{\mathrm{d}t}\Big(\frac d{\mathrm{d}t}\big(\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}}\big)\Big)^{-1} = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}(...)}\Big)}\Big)}  \frac{\mathrm{d}x}{d \frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{\mathrm{d}x}{\mathrm{d}t}\frac{\mathrm{d}t}{d\frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{x'}{x''} n^{th} F = \frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}} F = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac{dF}{\mathrm{d}t}} \implies F = \sqrt{2x + c},"['real-analysis', 'derivatives']"
95,"Does this Riemann integral over $[0,1]^2$ exist?",Does this Riemann integral over  exist?,"[0,1]^2","While waiting for responses to this question , I did some research and came across this function on $[0,1]^2$: $f(x,y) = 0$ if $x$ or $y$ is irrational and $f(x,y) = 1/q$ if $x$ and $y$ are rational and $x = p/q$ in lowest terms. Its claimed that the double Riemann integral $\int_{[0,1]^2}f $ exists since $f$ is continuous almost everywhere, but if $x$ is rational then $\int_0^1 f(x,y)dy $ does not exist as a Riemann integral. I understand the second part since $f(x,y)$ looks like the Dirichlet function (when $x =p/q$ fixed) alternating between $1/q$ and $0$ for rational and irrational $y$.  Just because $f$ alternates between $0$ and a variable non-zero value off and on a rational grid does not make it completely obvious  about the continuity. So I would like to see how to prove the first part directly using Darboux sums.","While waiting for responses to this question , I did some research and came across this function on $[0,1]^2$: $f(x,y) = 0$ if $x$ or $y$ is irrational and $f(x,y) = 1/q$ if $x$ and $y$ are rational and $x = p/q$ in lowest terms. Its claimed that the double Riemann integral $\int_{[0,1]^2}f $ exists since $f$ is continuous almost everywhere, but if $x$ is rational then $\int_0^1 f(x,y)dy $ does not exist as a Riemann integral. I understand the second part since $f(x,y)$ looks like the Dirichlet function (when $x =p/q$ fixed) alternating between $1/q$ and $0$ for rational and irrational $y$.  Just because $f$ alternates between $0$ and a variable non-zero value off and on a rational grid does not make it completely obvious  about the continuity. So I would like to see how to prove the first part directly using Darboux sums.",,"['real-analysis', 'integration', 'multivariable-calculus', 'riemann-integration', 'multiple-integral']"
96,Exercise on integration of a function in two variables,Exercise on integration of a function in two variables,,"For which values of $a,b > 0$ is the function $f : (0, \infty)$ x $\mathbb{R} \to \mathbb{R}: (x,y) \mapsto \dfrac{1}{x^a(1+x^2+y^2)^b}$ integrable? I have tried to perform to change this function to a function in polar coordinates but then I get a $\int_{-\pi/2}^{\pi/2} \dfrac{1}{cos^a(\theta)}d\theta$ which is not integrable for any $a > 0$. Does somebody have an idea how I can solve this problem with another method or substitution?","For which values of $a,b > 0$ is the function $f : (0, \infty)$ x $\mathbb{R} \to \mathbb{R}: (x,y) \mapsto \dfrac{1}{x^a(1+x^2+y^2)^b}$ integrable? I have tried to perform to change this function to a function in polar coordinates but then I get a $\int_{-\pi/2}^{\pi/2} \dfrac{1}{cos^a(\theta)}d\theta$ which is not integrable for any $a > 0$. Does somebody have an idea how I can solve this problem with another method or substitution?",,"['real-analysis', 'integration', 'lebesgue-integral', 'multiple-integral']"
97,Continuous functions whose Riemann sums are zero,Continuous functions whose Riemann sums are zero,,"here is my question : Let's denote $F$ the following set  $$ F=\left\{ f \in \mathcal{C}^0([0,1],\mathbb{C}) / \forall n \in \mathbb{N}^*, \sum_{k=0}^{n-1}f \left( \dfrac{k}{n} \right) =0 \right\}. $$ $F$ contains the line spanned by $f_0 : x \in [0,1] \mapsto \sin (2 \pi x)$ (but not $\cos$ because of $n=1$), but is it bigger ? I hope not, but I may be wrong. Thanks in advance.","here is my question : Let's denote $F$ the following set  $$ F=\left\{ f \in \mathcal{C}^0([0,1],\mathbb{C}) / \forall n \in \mathbb{N}^*, \sum_{k=0}^{n-1}f \left( \dfrac{k}{n} \right) =0 \right\}. $$ $F$ contains the line spanned by $f_0 : x \in [0,1] \mapsto \sin (2 \pi x)$ (but not $\cos$ because of $n=1$), but is it bigger ? I hope not, but I may be wrong. Thanks in advance.",,"['calculus', 'real-analysis', 'analysis']"
98,How irregular can $f'$ be beyond Darboux's Theorem?,How irregular can  be beyond Darboux's Theorem?,f',"By Darboux's theorem if $f:D\to\mathbb R$ is differentiable then $f'$ satisfies the intermediate value property $-$ even if it is discontinuous. In particular I am interested in the following: Assume $f'(a)<f'(b)$ for some $a<b$. We know that then $f'$ assumes every value in the interval $I=[f'(a),f'(b)]$ within $[a,b]$. Does this imply that $(f')^{-1}\big(I\big)$ has postive measure? Intuitively it seems like it must be true; but how to prove it? This question is related to a comment I made on this thread . Note that if $g:D\to\mathbb R$ has the intermediate value property, but is not the derivative of a differentiable function, then it can be quite volatile $-$ see for instance Conways base 13 function . Here, the pre-image of $I$ could only be described as a mess. Some related stuff: https://math.stackexchange.com/a/292380/99220 Volterra's function as an example of a very badly behaved derivative (set of discontinuities of $V'$ has positive measure). Cantors function (or rather its integral) is not a counter-example since the intersection of the complement of the cantor set with any (open) interval contains an (open) interval.","By Darboux's theorem if $f:D\to\mathbb R$ is differentiable then $f'$ satisfies the intermediate value property $-$ even if it is discontinuous. In particular I am interested in the following: Assume $f'(a)<f'(b)$ for some $a<b$. We know that then $f'$ assumes every value in the interval $I=[f'(a),f'(b)]$ within $[a,b]$. Does this imply that $(f')^{-1}\big(I\big)$ has postive measure? Intuitively it seems like it must be true; but how to prove it? This question is related to a comment I made on this thread . Note that if $g:D\to\mathbb R$ has the intermediate value property, but is not the derivative of a differentiable function, then it can be quite volatile $-$ see for instance Conways base 13 function . Here, the pre-image of $I$ could only be described as a mess. Some related stuff: https://math.stackexchange.com/a/292380/99220 Volterra's function as an example of a very badly behaved derivative (set of discontinuities of $V'$ has positive measure). Cantors function (or rather its integral) is not a counter-example since the intersection of the complement of the cantor set with any (open) interval contains an (open) interval.",,"['real-analysis', 'derivatives', 'lebesgue-measure']"
99,"Preparation needed for a book like Munkres' ""Analysis on Manifolds"" or Spivaks' ""Calculus on Manifolds""","Preparation needed for a book like Munkres' ""Analysis on Manifolds"" or Spivaks' ""Calculus on Manifolds""",,"At the present moment I find reading the chapters in Munkres' ""Analysis on Manifolds"" and Spivaks' ""Calculus on Manifolds"" to be a very tough read. By that I mean I often don't understand what the theorems and lemmas are truly trying to convey, and that then causes me problems when trying to solve the exercises. My plan is to build my mathematical maturity/strength by reading a good linear algebra book that focuses on proofs, then reading a multivariable calculus text(or notes) to get a basic understanding of the concepts like the ""Inverse function theorem"", and doing computations with them. I did a quick review of the linear algebra concepts used in Munkres and Spivak and took a bit of time to get familiar with mulitvariable calculus topics--such as how the single variable definition of the derivative is modified to better suit the multivariable case-- and that hasn't made the process of engaging with munkres and spivak any easier.  So i've decided to go through an entire linear algebra text making sure I understand every theorem and corresponding proof, and force myself to prove each excercise, then doing the same thing for a multivariable calculus text. Before I begin this process, I want to know if doing this will make reading and solving problems in munkres/spivak a breeze.  In other words, is my inability to properly engage with munkres/spivak due to my lack of proper engagement with a good linear algebra text, working out proofs in a single variable calculus text, and reviewing the multivariable calc text first?  Or must I persist with studying munkres/spivak directly until it becomes clear?","At the present moment I find reading the chapters in Munkres' ""Analysis on Manifolds"" and Spivaks' ""Calculus on Manifolds"" to be a very tough read. By that I mean I often don't understand what the theorems and lemmas are truly trying to convey, and that then causes me problems when trying to solve the exercises. My plan is to build my mathematical maturity/strength by reading a good linear algebra book that focuses on proofs, then reading a multivariable calculus text(or notes) to get a basic understanding of the concepts like the ""Inverse function theorem"", and doing computations with them. I did a quick review of the linear algebra concepts used in Munkres and Spivak and took a bit of time to get familiar with mulitvariable calculus topics--such as how the single variable definition of the derivative is modified to better suit the multivariable case-- and that hasn't made the process of engaging with munkres and spivak any easier.  So i've decided to go through an entire linear algebra text making sure I understand every theorem and corresponding proof, and force myself to prove each excercise, then doing the same thing for a multivariable calculus text. Before I begin this process, I want to know if doing this will make reading and solving problems in munkres/spivak a breeze.  In other words, is my inability to properly engage with munkres/spivak due to my lack of proper engagement with a good linear algebra text, working out proofs in a single variable calculus text, and reviewing the multivariable calc text first?  Or must I persist with studying munkres/spivak directly until it becomes clear?",,"['real-analysis', 'linear-algebra', 'analysis', 'soft-question']"
