,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Calculating the eigenvalues of a matrix,Calculating the eigenvalues of a matrix,,"How to find the eigenvalues of $$\begin{bmatrix} 0 & 1   &   &  &\\ k & 0   & 2 &  &\\   & k-1 & 0 & 3 &\\   &     &\ddots&\ddots&\ddots\\   &     &      &2 & 0 &k \\   &     &      & & 1 &0 \end{bmatrix}$$ I tried recurrence equation, but it doesn't work to find the characteristic polynomial. Any hint or solution are welcomed, thanks for your help! PS:The missing parts are all zeroes.","How to find the eigenvalues of $$\begin{bmatrix} 0 & 1   &   &  &\\ k & 0   & 2 &  &\\   & k-1 & 0 & 3 &\\   &     &\ddots&\ddots&\ddots\\   &     &      &2 & 0 &k \\   &     &      & & 1 &0 \end{bmatrix}$$ I tried recurrence equation, but it doesn't work to find the characteristic polynomial. Any hint or solution are welcomed, thanks for your help! PS:The missing parts are all zeroes.",,"['linear-algebra', 'matrices']"
1,Showing vectors span a vector space by definition,Showing vectors span a vector space by definition,,"I need to show that the vectors $v_1 = \langle 2, 1\rangle$ and $v_2 = \langle 4, 3\rangle$ span $\mathbb R^2$ by definition. By definition if I can write any vector in $\mathbb R^2$ as a linear combination of $v_1$ and $v_2$ then the vectors span $\mathbb R^2$. How do I show this? Here is what I have been working with: Let $v_x = \langle c_1, c_2\rangle$ be any vector in $\mathbb R^2$ where $c_1$ and $c_2$ are in $\mathbb R$. $v_x = c_1\langle 1, 0\rangle + c_2\langle 0, 1\rangle$ Set $v_x$ = a linear combination of $v_1$ and $v_2$? How do I proceed from here?","I need to show that the vectors $v_1 = \langle 2, 1\rangle$ and $v_2 = \langle 4, 3\rangle$ span $\mathbb R^2$ by definition. By definition if I can write any vector in $\mathbb R^2$ as a linear combination of $v_1$ and $v_2$ then the vectors span $\mathbb R^2$. How do I show this? Here is what I have been working with: Let $v_x = \langle c_1, c_2\rangle$ be any vector in $\mathbb R^2$ where $c_1$ and $c_2$ are in $\mathbb R$. $v_x = c_1\langle 1, 0\rangle + c_2\langle 0, 1\rangle$ Set $v_x$ = a linear combination of $v_1$ and $v_2$? How do I proceed from here?",,"['linear-algebra', 'vector-spaces']"
2,Block inverse of symmetric matrices,Block inverse of symmetric matrices,,"Let us assume we have a symmetric $n \times n$ matrix $A$. We know the inverse of $A$. Let us say that we now add one column and one row to $A$, in a way that the resulting matrix ($B$) is an $(n+1) \times (n+1)$ matrix that is still symmetric. For instance, $A = \begin{pmatrix}a & b \\b & d \\\end{pmatrix}$ and $B = \begin{pmatrix}a & b & X \\b & d & Y \\X & Y & Z\end{pmatrix}$ Given that I know $A^{-1}$, is there any way of using this information to find $B^{-1}$ without having to compute this latter inverse from scratch? If an exact solution is not possible, approximations would also help. Thanks, Bruno P.S. in case it makes any difference, both $A$ and $B$ are covariance matrices.","Let us assume we have a symmetric $n \times n$ matrix $A$. We know the inverse of $A$. Let us say that we now add one column and one row to $A$, in a way that the resulting matrix ($B$) is an $(n+1) \times (n+1)$ matrix that is still symmetric. For instance, $A = \begin{pmatrix}a & b \\b & d \\\end{pmatrix}$ and $B = \begin{pmatrix}a & b & X \\b & d & Y \\X & Y & Z\end{pmatrix}$ Given that I know $A^{-1}$, is there any way of using this information to find $B^{-1}$ without having to compute this latter inverse from scratch? If an exact solution is not possible, approximations would also help. Thanks, Bruno P.S. in case it makes any difference, both $A$ and $B$ are covariance matrices.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
3,Maximizing the value of a determinant,Maximizing the value of a determinant,,"Given the entries of a matrix how can we optimize its determinant? So, if the entries of a $n\times n$ matrix belong to the set $\{a_1,a_2,\ldots ,a_p\}$ , how to arrange them to maximize or minimize the determinant? I have seen a result concerning $3 \times 3$ matrices,which says that determinant is maximum when all the diagonal elements are $\min\{a_i\}_{1\leq i\leq p}$ and all the off-diagonal elements are $\max\{a_i\}_{1\leq i\leq p}$ . And once we maximize it, switching two rows or two columns we get the minimum value. Also, i don't have any clue for higher order matrices. Any help would be appreciated. Thanks in advance.","Given the entries of a matrix how can we optimize its determinant? So, if the entries of a matrix belong to the set , how to arrange them to maximize or minimize the determinant? I have seen a result concerning matrices,which says that determinant is maximum when all the diagonal elements are and all the off-diagonal elements are . And once we maximize it, switching two rows or two columns we get the minimum value. Also, i don't have any clue for higher order matrices. Any help would be appreciated. Thanks in advance.","n\times n \{a_1,a_2,\ldots ,a_p\} 3 \times 3 \min\{a_i\}_{1\leq i\leq p} \max\{a_i\}_{1\leq i\leq p}","['linear-algebra', 'matrices', 'optimization', 'determinant', 'discrete-optimization']"
4,"When calculating P, for diagonalization, does the order of eigenvalues matter?","When calculating P, for diagonalization, does the order of eigenvalues matter?",,"I'm trying to find the value of this matrix, $A = \begin{pmatrix} 1 & 4 \\ 3 & 2\end{pmatrix}$ to the power of $10$. I've determined (and confirmed on Wolfram) that the eigenvalues are $5$, and $-2$. I started looking for P by using the eigenvalue $5$, and found the eigenvector $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$. I did the same for the eigenvalue $-2$, and found eigenvector $\begin{bmatrix} -4 \\ 3 \end{bmatrix}$ I then put these together to get $P: \begin{bmatrix} 1 & -4\\ 1 & 3 \end{bmatrix}$. I followed that by finding $P^{-1}: \dfrac{1}{7} \begin{bmatrix} 3 & 4\\ -1 & 1 \end{bmatrix}$. The problem is, that $PAP^{-1}$ is not diagonalizing. I checked Wolfram, and I found that the reason is because my $P$ should have been $\begin{bmatrix} -4 & 1\\ 3 & 1 \end{bmatrix}$. My question is, what is the general rule about what eigenvalue to plug in first and how to arrange the columns of $P$?","I'm trying to find the value of this matrix, $A = \begin{pmatrix} 1 & 4 \\ 3 & 2\end{pmatrix}$ to the power of $10$. I've determined (and confirmed on Wolfram) that the eigenvalues are $5$, and $-2$. I started looking for P by using the eigenvalue $5$, and found the eigenvector $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$. I did the same for the eigenvalue $-2$, and found eigenvector $\begin{bmatrix} -4 \\ 3 \end{bmatrix}$ I then put these together to get $P: \begin{bmatrix} 1 & -4\\ 1 & 3 \end{bmatrix}$. I followed that by finding $P^{-1}: \dfrac{1}{7} \begin{bmatrix} 3 & 4\\ -1 & 1 \end{bmatrix}$. The problem is, that $PAP^{-1}$ is not diagonalizing. I checked Wolfram, and I found that the reason is because my $P$ should have been $\begin{bmatrix} -4 & 1\\ 3 & 1 \end{bmatrix}$. My question is, what is the general rule about what eigenvalue to plug in first and how to arrange the columns of $P$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
5,"Generate a $5 × 5$ matrix such that the each entry is an integer between $1$ and $9$, inclusive, and whose determinant is divisible by $271$.","Generate a  matrix such that the each entry is an integer between  and , inclusive, and whose determinant is divisible by .",5 × 5 1 9 271,"Generate a $5 × 5$ matrix such that the each entry is an integer between $1$ and $9$, inclusive, and whose determinant is divisible by $271$. This is a practice problem for a linear algebra exam I have coming up, and can't for the life of me figure it out. I was thinking maybe making a triangular matrix ($0$s below main diagonal, determinant would be the product of the diagonal), but that wouldn't work because it says to use integers $1$-$9$. Been thinking of this problem all day. The only way we've covered the determinant for a large matrix ($3 × 3$ or larger) has been through summing up the signed elementary product , but for a $5 × 5$ matrix, you'd need to make sure that all $5! = 120$ signed elementary products would need to be divisible by $271$. If anyone has a better way to approach and solve this problem, it would be very much appreciated.","Generate a $5 × 5$ matrix such that the each entry is an integer between $1$ and $9$, inclusive, and whose determinant is divisible by $271$. This is a practice problem for a linear algebra exam I have coming up, and can't for the life of me figure it out. I was thinking maybe making a triangular matrix ($0$s below main diagonal, determinant would be the product of the diagonal), but that wouldn't work because it says to use integers $1$-$9$. Been thinking of this problem all day. The only way we've covered the determinant for a large matrix ($3 × 3$ or larger) has been through summing up the signed elementary product , but for a $5 × 5$ matrix, you'd need to make sure that all $5! = 120$ signed elementary products would need to be divisible by $271$. If anyone has a better way to approach and solve this problem, it would be very much appreciated.",,"['linear-algebra', 'matrices', 'determinant']"
6,Winning strategies in multidimensional tic-tac-toe,Winning strategies in multidimensional tic-tac-toe,,"This question is a result of having too much free time years ago during military service. One of the many pastimes was playing tic-tac-toe in varying grid sizes and dimensions, and it lead me to a conjecture. Now, after several years of mathematical training at a university, I am still unable to settle the conjecture, so I present it to you. The classical tic-tac-toe game is played on a $3\times3$ grid and two players take turns to put their mark somewhere in the grid. The first one to get three collinear marks wins. Collinear includes horizontal, vertical and diagonal lines. Experience shows that the game always ends in a draw if both players play wisely. Let us write the grid size $3\times3$ as $3^2$. We can change the edge length by playing on any $a^2$ grid (where each player tries to get $a$ marks in a row on the $a\times a$ grid). We can also change dimension by playing on any $a^d$ grid, for example $3^3=3\times3\times3$. I want to understand something about this game for general $a$ and $d$. Let me repeat: The goal is to make $a$ collinear marks. I assume both players play in an optimal way. It is quite easy to see that the first player wins on a $2^d$ grid for any $d\geq2$ but the game is a tie on $2^1$. The game is a tie also on $3^1$ and $3^2$, but my experience suggests that the first player wins on $3^3$ but the game ties on $4^d$ for $d\leq3$. It seems quite credible that if there is a winning strategy on $a^d$, there is one also on $a^{d'}$ for any $d'\geq d$, since more dimensions to move in gives more room for winning rows. This answer to a related question tells that for any $a$ there is $d$ so that there is a winning strategy on $a^d$. This brings me to the conjecture: There is a winning strategy for tic-tac-toe on an $a^d$ grid if and only if $d\geq a$. (Refuted by TonyK's answer below.) Is there a characterization of the cases where a winning strategy exists? It turns out not to be as simple as I thought. To fix notation, let $$ \delta(a)=\min\{d;\text{first player wins on }a^d\} $$ and $$ \alpha(d)=\max\{a;\text{first player wins on }a^d\}. $$ The main question is: Is there an explicit expression for either of these functions?   Or decent bounds?   Partial answers are also welcome. Note that the second player never wins, as was discussed in this earlier post . A remark for the algebraically-minded: We can also allow the lines of marks to continue at the opposite face when they exit the grid; this amounts to giving the grid a torus-like structure. Now there are no special points, unlike in the usual case with boundaries. Collinear points on a toric grid of size $a^d$ corresponds to a line (maximal collinear set) in the module $(\mathbb Z/a\mathbb Z)^d$. (If $a$ is odd, then $a$ collinear points in the mentioned module add up to zero, but the converse does not always hold: the nine points in $(\mathbb Z/9\mathbb Z)^3$ with multiples of three as all coordinates add up to zero but are not collinear.) This approach might be more useful when $a$ is a prime and the module becomes a vector space. Anyway, if this version of the game seems more manageable, I'm happy with answers about it as well (although the conjecture as stated is not true in this setting; the first player wins on $3^2$).","This question is a result of having too much free time years ago during military service. One of the many pastimes was playing tic-tac-toe in varying grid sizes and dimensions, and it lead me to a conjecture. Now, after several years of mathematical training at a university, I am still unable to settle the conjecture, so I present it to you. The classical tic-tac-toe game is played on a $3\times3$ grid and two players take turns to put their mark somewhere in the grid. The first one to get three collinear marks wins. Collinear includes horizontal, vertical and diagonal lines. Experience shows that the game always ends in a draw if both players play wisely. Let us write the grid size $3\times3$ as $3^2$. We can change the edge length by playing on any $a^2$ grid (where each player tries to get $a$ marks in a row on the $a\times a$ grid). We can also change dimension by playing on any $a^d$ grid, for example $3^3=3\times3\times3$. I want to understand something about this game for general $a$ and $d$. Let me repeat: The goal is to make $a$ collinear marks. I assume both players play in an optimal way. It is quite easy to see that the first player wins on a $2^d$ grid for any $d\geq2$ but the game is a tie on $2^1$. The game is a tie also on $3^1$ and $3^2$, but my experience suggests that the first player wins on $3^3$ but the game ties on $4^d$ for $d\leq3$. It seems quite credible that if there is a winning strategy on $a^d$, there is one also on $a^{d'}$ for any $d'\geq d$, since more dimensions to move in gives more room for winning rows. This answer to a related question tells that for any $a$ there is $d$ so that there is a winning strategy on $a^d$. This brings me to the conjecture: There is a winning strategy for tic-tac-toe on an $a^d$ grid if and only if $d\geq a$. (Refuted by TonyK's answer below.) Is there a characterization of the cases where a winning strategy exists? It turns out not to be as simple as I thought. To fix notation, let $$ \delta(a)=\min\{d;\text{first player wins on }a^d\} $$ and $$ \alpha(d)=\max\{a;\text{first player wins on }a^d\}. $$ The main question is: Is there an explicit expression for either of these functions?   Or decent bounds?   Partial answers are also welcome. Note that the second player never wins, as was discussed in this earlier post . A remark for the algebraically-minded: We can also allow the lines of marks to continue at the opposite face when they exit the grid; this amounts to giving the grid a torus-like structure. Now there are no special points, unlike in the usual case with boundaries. Collinear points on a toric grid of size $a^d$ corresponds to a line (maximal collinear set) in the module $(\mathbb Z/a\mathbb Z)^d$. (If $a$ is odd, then $a$ collinear points in the mentioned module add up to zero, but the converse does not always hold: the nine points in $(\mathbb Z/9\mathbb Z)^3$ with multiples of three as all coordinates add up to zero but are not collinear.) This approach might be more useful when $a$ is a prime and the module becomes a vector space. Anyway, if this version of the game seems more manageable, I'm happy with answers about it as well (although the conjecture as stated is not true in this setting; the first player wins on $3^2$).",,"['linear-algebra', 'modules', 'game-theory', 'combinatorial-game-theory', 'tic-tac-toe']"
7,When does $V/\operatorname{ker}(\phi)\simeq\phi(V)$ imply $V\simeq\operatorname{ker}(\phi)\oplus\phi(V)$?,When does  imply ?,V/\operatorname{ker}(\phi)\simeq\phi(V) V\simeq\operatorname{ker}(\phi)\oplus\phi(V),"When does $V/\operatorname{ker}(\phi)\simeq\phi(V)$ imply $V\simeq\operatorname{ker}(\phi)\oplus\phi(V)$? I wrote it down in an imprecise way on purpose. The notation above is the linear algebra one: we have $V,W$ vector spaces over a field $\mathbb K$ and a $\mathbb K$-linear map $\phi:V\to W$. Then we know that isomorphism theorem says that $$ V/\operatorname{ker}(\phi)\simeq\phi(V) $$ and we also know that $\operatorname{dim_{\mathbb K}}V= \operatorname{dim_{\mathbb K}}(\operatorname{ker}(\phi))+ \operatorname{dim_{\mathbb K}}(\phi(V))$. But when do we have that from this it follows that $$ V\simeq\operatorname{ker}(\phi)\oplus\phi(V)\;\;? $$ This was to be more precise on the particular case of the vector space, but it would be nice to know how it goes in every situation in which the isomorphism theorem works (rings homomorphism, groups homomorphism and so on). EDIT: This question came to my mind originally because I was studying the group of units in a number ring $R$ (i.e. $R=\mathbb A\cap K$, where $\mathbb Q\le K\le\mathbb C$, $[K:\mathbb Q]=n$ and $\mathbb A$ is the ring of algebraic integers of $\mathbb C$). There is a theorem which states that in this case the group of units $U$ is a direct product of a finite cyclic group formed by the roots of $1$, call it $V$ and a free abelian group of finite rank $W$ (the rank is $r+s-1$ where $r$ is the number of real embeddings of $K$ and $2s$ is the number of complex embedding of $K$; and clearly $n=r+2s$). In order to show this, we build a moltiplicative-to-additive group homomorphism $U\to\mathbb R^{r+s}$ called $\log$ (it's a special logarithm), whose kernel is exactly $V$ and whose image is $W\simeq\mathbb Z^{r+s-1}$. From this, my book (Daniel Marcus' Number Fields) let follow that $U\simeq V\times W$, and even though it sounds good, I didn't understand exactly why. SECOND EDIT: if it could help I'll write better the map $\log$. Consider the $r$ real embeddings of $K$: $\sigma_i$, and the $2s$ complex ones $\tau_j, \bar{\tau_j}$. We can define an additive groups monomorphism $$ \varphi:K\to\mathbb R^n $$ defined by $$ \varphi(x)=(\sigma_1(x),\dots,\sigma_r(x),\Re(\tau_1(x)),\Im(\tau_1(x)),\dots,\Re(\tau_s(x)),\Im(\tau_s(x)))\;\;. $$ Then given an integral basis of $R$, say $\{\alpha_i\}_{i=1}^n$ we can consider the $\varphi(\alpha_i)$'s: they form a basis for $\mathbb R^n$, hence we can consider the $n$-dimensional lattice generated by them: $$ \Lambda_{R}:=\operatorname{Span}(\varphi(\alpha_1),\dots,\varphi(\alpha_n{})):=\langle\varphi(\alpha_1),\dots,\varphi(\alpha_n{})\rangle_{\mathbb Z}\;. $$ It's clear that $\varphi$ sends $R$ onto $\Lambda_R$ hence they are isomorphic. Then consider $$ U\subset R\setminus\{0\}\stackrel{\varphi}{\longrightarrow}\Lambda_R\setminus\{0\}\stackrel{\log}{\longrightarrow}\mathbb R^{r+s} $$ where $\log$ is defined as (given $x=(x_1,\dots,x_n)\in\Lambda_R\subset\mathbb R^n$) $$ \log(x):=(\log|x_1|,\dots,\log|x_r|,\log(x_{r+1}^2+x_{r+2}^2),\dots,\log(x_{n-1}^2+x_n^2)). $$","When does $V/\operatorname{ker}(\phi)\simeq\phi(V)$ imply $V\simeq\operatorname{ker}(\phi)\oplus\phi(V)$? I wrote it down in an imprecise way on purpose. The notation above is the linear algebra one: we have $V,W$ vector spaces over a field $\mathbb K$ and a $\mathbb K$-linear map $\phi:V\to W$. Then we know that isomorphism theorem says that $$ V/\operatorname{ker}(\phi)\simeq\phi(V) $$ and we also know that $\operatorname{dim_{\mathbb K}}V= \operatorname{dim_{\mathbb K}}(\operatorname{ker}(\phi))+ \operatorname{dim_{\mathbb K}}(\phi(V))$. But when do we have that from this it follows that $$ V\simeq\operatorname{ker}(\phi)\oplus\phi(V)\;\;? $$ This was to be more precise on the particular case of the vector space, but it would be nice to know how it goes in every situation in which the isomorphism theorem works (rings homomorphism, groups homomorphism and so on). EDIT: This question came to my mind originally because I was studying the group of units in a number ring $R$ (i.e. $R=\mathbb A\cap K$, where $\mathbb Q\le K\le\mathbb C$, $[K:\mathbb Q]=n$ and $\mathbb A$ is the ring of algebraic integers of $\mathbb C$). There is a theorem which states that in this case the group of units $U$ is a direct product of a finite cyclic group formed by the roots of $1$, call it $V$ and a free abelian group of finite rank $W$ (the rank is $r+s-1$ where $r$ is the number of real embeddings of $K$ and $2s$ is the number of complex embedding of $K$; and clearly $n=r+2s$). In order to show this, we build a moltiplicative-to-additive group homomorphism $U\to\mathbb R^{r+s}$ called $\log$ (it's a special logarithm), whose kernel is exactly $V$ and whose image is $W\simeq\mathbb Z^{r+s-1}$. From this, my book (Daniel Marcus' Number Fields) let follow that $U\simeq V\times W$, and even though it sounds good, I didn't understand exactly why. SECOND EDIT: if it could help I'll write better the map $\log$. Consider the $r$ real embeddings of $K$: $\sigma_i$, and the $2s$ complex ones $\tau_j, \bar{\tau_j}$. We can define an additive groups monomorphism $$ \varphi:K\to\mathbb R^n $$ defined by $$ \varphi(x)=(\sigma_1(x),\dots,\sigma_r(x),\Re(\tau_1(x)),\Im(\tau_1(x)),\dots,\Re(\tau_s(x)),\Im(\tau_s(x)))\;\;. $$ Then given an integral basis of $R$, say $\{\alpha_i\}_{i=1}^n$ we can consider the $\varphi(\alpha_i)$'s: they form a basis for $\mathbb R^n$, hence we can consider the $n$-dimensional lattice generated by them: $$ \Lambda_{R}:=\operatorname{Span}(\varphi(\alpha_1),\dots,\varphi(\alpha_n{})):=\langle\varphi(\alpha_1),\dots,\varphi(\alpha_n{})\rangle_{\mathbb Z}\;. $$ It's clear that $\varphi$ sends $R$ onto $\Lambda_R$ hence they are isomorphic. Then consider $$ U\subset R\setminus\{0\}\stackrel{\varphi}{\longrightarrow}\Lambda_R\setminus\{0\}\stackrel{\log}{\longrightarrow}\mathbb R^{r+s} $$ where $\log$ is defined as (given $x=(x_1,\dots,x_n)\in\Lambda_R\subset\mathbb R^n$) $$ \log(x):=(\log|x_1|,\dots,\log|x_r|,\log(x_{r+1}^2+x_{r+2}^2),\dots,\log(x_{n-1}^2+x_n^2)). $$",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'number-theory', 'ring-theory']"
8,"Invertibility, eigenvalues and singular values","Invertibility, eigenvalues and singular values",,"I am confused about the relationship between the invertibility of a matrix and its eigenvalues.  What do the eigenvalues of a matrix tell you about whether a matrix is invertible or not?  Also, what about the ""singular values"" of a matrix?  If there are good online resources which answer these question I would be grateful for any pointers.","I am confused about the relationship between the invertibility of a matrix and its eigenvalues.  What do the eigenvalues of a matrix tell you about whether a matrix is invertible or not?  Also, what about the ""singular values"" of a matrix?  If there are good online resources which answer these question I would be grateful for any pointers.",,['linear-algebra']
9,Commuting matrices are simultaneously triangularizable,Commuting matrices are simultaneously triangularizable,,"Let $A$, $B$ be two $n\times n$ matrices over $\mathbb{C}$. If $AB=BA$ then there exists a  basis $\mathcal B$ such that under this basis, the matrices of $A$, $B$ are both upper triangular. How to prove this? I know how to prove the following: If $A$, $B$ are diagonalizable and commute, then they are simultaneously diagonalizable. Will the proof here be similar? Moreover, give an example such that $A$, $B$ don't commute and are not simultaneously triangularizable. Also give an example such that $A$, $B$ commute but are not diagonalizable, then they are not simultaneously diagonalizable. I will be very grateful if you post your answers and share with me. Helps is really in need urgently. Thanks a lot.","Let $A$, $B$ be two $n\times n$ matrices over $\mathbb{C}$. If $AB=BA$ then there exists a  basis $\mathcal B$ such that under this basis, the matrices of $A$, $B$ are both upper triangular. How to prove this? I know how to prove the following: If $A$, $B$ are diagonalizable and commute, then they are simultaneously diagonalizable. Will the proof here be similar? Moreover, give an example such that $A$, $B$ don't commute and are not simultaneously triangularizable. Also give an example such that $A$, $B$ commute but are not diagonalizable, then they are not simultaneously diagonalizable. I will be very grateful if you post your answers and share with me. Helps is really in need urgently. Thanks a lot.",,"['linear-algebra', 'matrices']"
10,Generic topology on a vector space?,Generic topology on a vector space?,,"For a (possibly infinite-dimensional) vector space $V$, I thought about the following topology $\tau$: Let $O \in \tau$ if every $x \in O$ has the property that for every $v \in V$, there is an $\epsilon > 0$ such that $x + \alpha v \in O$ for every $\alpha$ with $| \alpha | < \epsilon$. This gives a topology indeed (right?). I think that in finite-dimensional vector spaces, this gives the standard topology (right?). What about infinite-dimensional vector spaces? Is there a name for this topology? To me, this looks like a very generic way to define a topology on a vector space, but I've never seen this definition of a topology. Is there any connection to locally convex topologies? (I'm not very familiar with this concept) Is this topology important in any application?","For a (possibly infinite-dimensional) vector space $V$, I thought about the following topology $\tau$: Let $O \in \tau$ if every $x \in O$ has the property that for every $v \in V$, there is an $\epsilon > 0$ such that $x + \alpha v \in O$ for every $\alpha$ with $| \alpha | < \epsilon$. This gives a topology indeed (right?). I think that in finite-dimensional vector spaces, this gives the standard topology (right?). What about infinite-dimensional vector spaces? Is there a name for this topology? To me, this looks like a very generic way to define a topology on a vector space, but I've never seen this definition of a topology. Is there any connection to locally convex topologies? (I'm not very familiar with this concept) Is this topology important in any application?",,"['linear-algebra', 'functional-analysis', 'topological-vector-spaces']"
11,connection between graphs and the eigenvectors of their matrix representation,connection between graphs and the eigenvectors of their matrix representation,,"I am trying to learn graph theory and the linear algebra used to analyse graphs. The texts I have read through have lots of lemmas and theorems proved. The proofs are convincing but I fail to see the intuition behind them. How do they connect to the properties of the graphs? I understand eigenvectors very clearly when they are taken from a transition matrix for a Markov chain, because very simply they represent a stationary distribution of the matrix. But for the matrices derived from graphs? From the adjacency matrix? Here I cannot understand what relevance the eigenvectors/eigenvalues have to the graph. I cannot imagine using those matrices in the first place for multiplying anything by them. Maybe I don't completely understand totally the power and depth of eigenvectors/eigenvalues. But I guess this application would reveal more. Best,","I am trying to learn graph theory and the linear algebra used to analyse graphs. The texts I have read through have lots of lemmas and theorems proved. The proofs are convincing but I fail to see the intuition behind them. How do they connect to the properties of the graphs? I understand eigenvectors very clearly when they are taken from a transition matrix for a Markov chain, because very simply they represent a stationary distribution of the matrix. But for the matrices derived from graphs? From the adjacency matrix? Here I cannot understand what relevance the eigenvectors/eigenvalues have to the graph. I cannot imagine using those matrices in the first place for multiplying anything by them. Maybe I don't completely understand totally the power and depth of eigenvectors/eigenvalues. But I guess this application would reveal more. Best,",,"['linear-algebra', 'matrices', 'graph-theory']"
12,Time complexity of LU decomposition,Time complexity of LU decomposition,,"I am trying to derive the LU decomposition time complexity for an $n \times n$ matrix. Eliminating the first column will require $n$ additions and $n$ multiplications for $n-1$ rows. Therefore, the number of operations for the first column is $2n(n-1)$. For the second column, we have $n-1$ additions and $n-1$ multiplications, and we do this for $(n-2)$ rows giving us $2(n-1)(n-2)$. Therefore, the total number of operations required for the full decomposition can be written as $$ \sum_i^n 2(n-i) (n-i+1) $$ How do we get from this sum to a total cost of $\frac{2}{3}n^3$?","I am trying to derive the LU decomposition time complexity for an $n \times n$ matrix. Eliminating the first column will require $n$ additions and $n$ multiplications for $n-1$ rows. Therefore, the number of operations for the first column is $2n(n-1)$. For the second column, we have $n-1$ additions and $n-1$ multiplications, and we do this for $(n-2)$ rows giving us $2(n-1)(n-2)$. Therefore, the total number of operations required for the full decomposition can be written as $$ \sum_i^n 2(n-i) (n-i+1) $$ How do we get from this sum to a total cost of $\frac{2}{3}n^3$?",,"['linear-algebra', 'asymptotics', 'numerical-linear-algebra', 'matrix-decomposition', 'gaussian-elimination']"
13,"Is it true that $A^2\succ B^2$ implies that $A\succ B$, but not the converse?","Is it true that  implies that , but not the converse?",A^2\succ B^2 A\succ B,"I remember reading somewhere about the following properties of non-negative definite matrix. But I don't know how to prove it now. Let $A$ and $B$ be two non-negative definite matrices. If $A^2\succ B^2$ , then it necessarily follows that $A\succ B$ , but $A\succ B$ doesn't necessarily leads to $A^2\succ B^2$ . How can you prove it?","I remember reading somewhere about the following properties of non-negative definite matrix. But I don't know how to prove it now. Let and be two non-negative definite matrices. If , then it necessarily follows that , but doesn't necessarily leads to . How can you prove it?",A B A^2\succ B^2 A\succ B A\succ B A^2\succ B^2,"['linear-algebra', 'linear-matrix-inequality']"
14,Singular value decomposition of positive definite matrix,Singular value decomposition of positive definite matrix,,"Let $A$ be a positive definite matrix, and let $A = U \Sigma V^*$ be its singular value decomposition (SVD). Show that $U=V$ . What I have done: $A$ is Hermitian, so $A$ is unitarily diagonalizable, say, $A=WDW^*$ where $D$ consists of the eigenvalues (decreasing order). Also $D=\Sigma$ since $A$ is positive definite. From $A^2=AA^*=UD^2U^*$ , and similarly I have $A^2=UD^2U^*=VD^2V^*=WD^2W^*$ so the column vectors of $U,V,W$ corresponds to same eigenvalues of $A^2$ . And I'm now stuck. How could I proceed?","Let be a positive definite matrix, and let be its singular value decomposition (SVD). Show that . What I have done: is Hermitian, so is unitarily diagonalizable, say, where consists of the eigenvalues (decreasing order). Also since is positive definite. From , and similarly I have so the column vectors of corresponds to same eigenvalues of . And I'm now stuck. How could I proceed?","A A = U \Sigma V^* U=V A A A=WDW^* D D=\Sigma A A^2=AA^*=UD^2U^* A^2=UD^2U^*=VD^2V^*=WD^2W^* U,V,W A^2","['linear-algebra', 'matrices', 'positive-definite', 'svd']"
15,"If $C$ commutes with certain matrices $A$ and $B$, why is $C$ a scalar multiple of the identity?","If  commutes with certain matrices  and , why is  a scalar multiple of the identity?",C A B C,"I'm self studying Steven Roman's Advanced Linear Algebra , and this is problem 10 of Chapter 8. Let $A,B\in M_2(\mathbb{C})$, $A^2=B^3=I$, $ABA=B^{-1}$, but $A\neq I$ and $B\neq I$. If $C\in M_2(\mathbb{C})$ commutes with $A$ and $B$, then $C=rI$ for some $r\in\mathbb{C}$. Is there a way to solve this without writing out arbitrary matrices and attempting to solve a huge system of equations? The only thing I observe is that $A=A^{-1}$, so $B\sim B^{-1}$, so $B$ and $B^{-1}$ have the same characteristic polynomial. I'm stymied trying to show $C$ is diagonal, let alone a multiple of $I$. Thanks for any ideas. I should add that I know that the center $Z(M_n(\mathbb{C}))$ consists of scalar multiples of $I$, but I don't see any reason to assume or prove $C$ commutes with everything.","I'm self studying Steven Roman's Advanced Linear Algebra , and this is problem 10 of Chapter 8. Let $A,B\in M_2(\mathbb{C})$, $A^2=B^3=I$, $ABA=B^{-1}$, but $A\neq I$ and $B\neq I$. If $C\in M_2(\mathbb{C})$ commutes with $A$ and $B$, then $C=rI$ for some $r\in\mathbb{C}$. Is there a way to solve this without writing out arbitrary matrices and attempting to solve a huge system of equations? The only thing I observe is that $A=A^{-1}$, so $B\sim B^{-1}$, so $B$ and $B^{-1}$ have the same characteristic polynomial. I'm stymied trying to show $C$ is diagonal, let alone a multiple of $I$. Thanks for any ideas. I should add that I know that the center $Z(M_n(\mathbb{C}))$ consists of scalar multiples of $I$, but I don't see any reason to assume or prove $C$ commutes with everything.",,"['linear-algebra', 'matrices']"
16,Is axiom of choice required for there to be an infinite linearly independent set in a (non-finite-dimensional) vector space?,Is axiom of choice required for there to be an infinite linearly independent set in a (non-finite-dimensional) vector space?,,"In discussing this answer , I noted that while the statement: Any vector space has a basis is equivalent to the axiom of choice, I wondered if the statement that: Any vector space either has a finite basis or an infinite set of linear independent vectors was weaker than the axiom of choice. It feels weaker - it feels like you can inductively define a countably infinite set of linearly independent vectors without full choice, but I'm not sure if that is possible with ZF alone, or requires all of the Axiom of Choice, or is weaker but requires some form of choice.","In discussing this answer , I noted that while the statement: Any vector space has a basis is equivalent to the axiom of choice, I wondered if the statement that: Any vector space either has a finite basis or an infinite set of linear independent vectors was weaker than the axiom of choice. It feels weaker - it feels like you can inductively define a countably infinite set of linearly independent vectors without full choice, but I'm not sure if that is possible with ZF alone, or requires all of the Axiom of Choice, or is weaker but requires some form of choice.",,"['linear-algebra', 'set-theory', 'axiom-of-choice']"
17,Motivation for adjoint operators in finite dimensional inner-product-spaces,Motivation for adjoint operators in finite dimensional inner-product-spaces,,"Given a finite dimensional inner-product-space $(V,\langle\;,\rangle)$ and an endomorphism $A\in\mathrm{End}(V)$ we can define its adjoint $A^*$ as the only endomorphism such that $\langle Ax, y\rangle=\langle x, A^*y\rangle $ for all $x,y\in V$. While all of this lets us prove some stuff about unitary, normal and hermitian matrices, I'd like to know if there's some other motivation behind its introduction (Is there a geometric interpretation? Any other algebraic remark?)","Given a finite dimensional inner-product-space $(V,\langle\;,\rangle)$ and an endomorphism $A\in\mathrm{End}(V)$ we can define its adjoint $A^*$ as the only endomorphism such that $\langle Ax, y\rangle=\langle x, A^*y\rangle $ for all $x,y\in V$. While all of this lets us prove some stuff about unitary, normal and hermitian matrices, I'd like to know if there's some other motivation behind its introduction (Is there a geometric interpretation? Any other algebraic remark?)",,"['linear-algebra', 'adjoint-operators']"
18,"What is the difference between Projective Geometric, Clifford Algebra, Grassman Algebra, Geometric Algebra, Quaternion Algebra and Exterior Algebra?","What is the difference between Projective Geometric, Clifford Algebra, Grassman Algebra, Geometric Algebra, Quaternion Algebra and Exterior Algebra?",,"Since a few years now, Special Interest Group on Computer Graphics have been shilling this new type of algebra that they advertise fixes all the problem with Linear Algebra like no Gimbal locks, error free transformations, co-ordinate free representation of primitives and robust collision detection as well as trivially generalizing to higher dimensions, among other things. All of these seems too good to be true and it feels like I am being sold on some mathematical cult. Nevertheless, this got me interested and everytime I try to read the literature, I get all confused by these algebras: Projective Geometric Algebra PGA, Clifford Algebra, Grassman Algebra, Geometric Algebra, Exterior Algebra, Quaternion Algebra I maybe wrong but I observe: Quaternion algebra feels like the odd one out (but everyone mentions it). All other algebras support the wedge product. Some algebra merge dot and wedge into one operations and some don't. Some algebra is fixed only to three (or four homogeneous) dimensions. Clifford seems like a superset of all except Quaternion. Exterior and Grassman seems to be the same thing but it doesn't merge dot and wedge together. Geometric Algebra does seem merge them together and PGA is like Geometric but in 3D. For those who know, please tell what is the difference between all these algebras and a little history and chronology (if that is too much to ask, a link to appropriate resource will be appreciated) and which algebra should I pick to study (and optionally suggest a good book) if my interest is mostly computer graphics and I want to overcome the limitations that vanilla Linear Algebra has. I don't want a deeper understanding of Spinors or Minkowski's space time or condensing Maxwell 4 EM equations into one.","Since a few years now, Special Interest Group on Computer Graphics have been shilling this new type of algebra that they advertise fixes all the problem with Linear Algebra like no Gimbal locks, error free transformations, co-ordinate free representation of primitives and robust collision detection as well as trivially generalizing to higher dimensions, among other things. All of these seems too good to be true and it feels like I am being sold on some mathematical cult. Nevertheless, this got me interested and everytime I try to read the literature, I get all confused by these algebras: Projective Geometric Algebra PGA, Clifford Algebra, Grassman Algebra, Geometric Algebra, Exterior Algebra, Quaternion Algebra I maybe wrong but I observe: Quaternion algebra feels like the odd one out (but everyone mentions it). All other algebras support the wedge product. Some algebra merge dot and wedge into one operations and some don't. Some algebra is fixed only to three (or four homogeneous) dimensions. Clifford seems like a superset of all except Quaternion. Exterior and Grassman seems to be the same thing but it doesn't merge dot and wedge together. Geometric Algebra does seem merge them together and PGA is like Geometric but in 3D. For those who know, please tell what is the difference between all these algebras and a little history and chronology (if that is too much to ask, a link to appropriate resource will be appreciated) and which algebra should I pick to study (and optionally suggest a good book) if my interest is mostly computer graphics and I want to overcome the limitations that vanilla Linear Algebra has. I don't want a deeper understanding of Spinors or Minkowski's space time or condensing Maxwell 4 EM equations into one.",,"['linear-algebra', 'exterior-algebra', 'clifford-algebras', 'geometric-algebras']"
19,Finite vs infinite dimensional vector spaces,Finite vs infinite dimensional vector spaces,,"What familiar and intuitive properties of finite dimensional vector spaces fails in infinite dimensions? For example: In infinite dimensions there are non-continuous linear maps. In infinite dimensions always $\dim V <\dim V^*$ and, in particular, $V\not\simeq V^{**}$.","What familiar and intuitive properties of finite dimensional vector spaces fails in infinite dimensions? For example: In infinite dimensions there are non-continuous linear maps. In infinite dimensions always $\dim V <\dim V^*$ and, in particular, $V\not\simeq V^{**}$.",,"['linear-algebra', 'big-list']"
20,"Show that the set of all symmetric, real matrices is a subspace, determine the dimension","Show that the set of all symmetric, real matrices is a subspace, determine the dimension",,"Question: Let $V \subset M(n,n,\mathbb{R})$ be the set of all symmetric, real $(n \times n)$ matrices, that is $a_{ij} = a_{ji}$ for all $i,j$. Show that $V$ is a subspace of $M(n,n,\mathbb{R})$ and calculate dim$(V)$. My attempt so far: First part: To show that $V$ is a subspace I need to show:  (a) $ 0 \in V$ and   (b) $\forall A,B \in V:  	(i) A + B \in V  	(ii) \lambda A \in V$ For (a) I would say: Let $a_{ij} \in 0$(this should represent a zero matrix, is that how to write it?) $a_{ij} = 0 = a_{ji} \Rightarrow 0 \in V$ For (b) I am actually confused since I would first think: both a $(2 \times 2)$ matrix and a $(3 \times 3)$ matrix belong to $V$ but addition of matrices of different size is undefined $\Rightarrow$ $V$ is not closed under addition $\Rightarrow$ $V$ is not a subspace of $M(n,n,\mathbb{R})$... what am I missing here? (To start I don't really understand the notation $M(n,n,\mathbb{R})$... what exactly does the $\mathbb{R}$ represent there?). Disregarding my confusion I would still try to show (b), but my mathematical notation is still lacking competence... Is the following somewhat clear? Would anyone ever use ""$\in$"" to denote ""is an element of matrix""? (i)Let $a_{ij},a_{ji} \in A$ and $b_{ij}, b_{ji} \in B$. Let $A,B \in V$ $\Rightarrow a_{ij} = a_{ji}, b_{ij} = b_{ji}$ $A + B = C \Rightarrow c_{ij} = (a_{ij}+b_{ij}) = (a_{ji} + b_{ij}) = (a_{ij} + b_{ji}) = c_{ji} = (a_{ji} + b_{ji})$ $\Rightarrow C \in V$ (ii) Let $A\in V, \lambda \in \mathbb{R}$. Let $a_{ij},a_{ji} \in A$. $\Rightarrow a_{ij} = a_{ji}$ $\lambda \cdot A = A'$ with $\lambda a_{ij} = \lambda a_{ji} \Rightarrow A' \in V$ Second part: I feel that I understand the answer... For an $(n \times n)$ matrix, the diagonal length $ = n$ and these are the elements which have no counterpart and are not critical to the symmetry. When these elements are subtracted from the total$(n^{2})$, half of the remainder can be independently selected and the other half will follow as a result. Therefore I think it makes sense to write that dim$(V) = n + \frac{n^{2}-n}{2}$. Is this correct? If so, given the context of the exercise, how could I make my answer more acceptable?","Question: Let $V \subset M(n,n,\mathbb{R})$ be the set of all symmetric, real $(n \times n)$ matrices, that is $a_{ij} = a_{ji}$ for all $i,j$. Show that $V$ is a subspace of $M(n,n,\mathbb{R})$ and calculate dim$(V)$. My attempt so far: First part: To show that $V$ is a subspace I need to show:  (a) $ 0 \in V$ and   (b) $\forall A,B \in V:  	(i) A + B \in V  	(ii) \lambda A \in V$ For (a) I would say: Let $a_{ij} \in 0$(this should represent a zero matrix, is that how to write it?) $a_{ij} = 0 = a_{ji} \Rightarrow 0 \in V$ For (b) I am actually confused since I would first think: both a $(2 \times 2)$ matrix and a $(3 \times 3)$ matrix belong to $V$ but addition of matrices of different size is undefined $\Rightarrow$ $V$ is not closed under addition $\Rightarrow$ $V$ is not a subspace of $M(n,n,\mathbb{R})$... what am I missing here? (To start I don't really understand the notation $M(n,n,\mathbb{R})$... what exactly does the $\mathbb{R}$ represent there?). Disregarding my confusion I would still try to show (b), but my mathematical notation is still lacking competence... Is the following somewhat clear? Would anyone ever use ""$\in$"" to denote ""is an element of matrix""? (i)Let $a_{ij},a_{ji} \in A$ and $b_{ij}, b_{ji} \in B$. Let $A,B \in V$ $\Rightarrow a_{ij} = a_{ji}, b_{ij} = b_{ji}$ $A + B = C \Rightarrow c_{ij} = (a_{ij}+b_{ij}) = (a_{ji} + b_{ij}) = (a_{ij} + b_{ji}) = c_{ji} = (a_{ji} + b_{ji})$ $\Rightarrow C \in V$ (ii) Let $A\in V, \lambda \in \mathbb{R}$. Let $a_{ij},a_{ji} \in A$. $\Rightarrow a_{ij} = a_{ji}$ $\lambda \cdot A = A'$ with $\lambda a_{ij} = \lambda a_{ji} \Rightarrow A' \in V$ Second part: I feel that I understand the answer... For an $(n \times n)$ matrix, the diagonal length $ = n$ and these are the elements which have no counterpart and are not critical to the symmetry. When these elements are subtracted from the total$(n^{2})$, half of the remainder can be independently selected and the other half will follow as a result. Therefore I think it makes sense to write that dim$(V) = n + \frac{n^{2}-n}{2}$. Is this correct? If so, given the context of the exercise, how could I make my answer more acceptable?",,['linear-algebra']
21,Most efficient method for computing Singular Value Decomposition of a triangular matrix?,Most efficient method for computing Singular Value Decomposition of a triangular matrix?,,There are several methods available for computing SVD of a general matrix. I am interested in knowing about the best approach which could be used for computing SVD of an upper triangular matrix. Please suggest me an algorithm which could be optimized for this special case of matrices.,There are several methods available for computing SVD of a general matrix. I am interested in knowing about the best approach which could be used for computing SVD of an upper triangular matrix. Please suggest me an algorithm which could be optimized for this special case of matrices.,,"['linear-algebra', 'matrices', 'algorithms', 'matrix-decomposition', 'svd']"
22,Geometric Interpretation of Determinant of Transpose,Geometric Interpretation of Determinant of Transpose,,"Below are two well-known statements regarding the determinant function: When $A$ is a square matrix, $\det(A)$ is the signed volume of the parallelepiped whose edges are columns of $A$. When $A$ is a square matrix, $\det(A) = \det(A^T)$. It appears to me that 1 is how $\det$ got invented in the first place. The multilinearity and alternating property are natural consequences. Cramer's rule can also be easily understood geometrically. However, 1 does not immediately yield 2. Of course the proof of 2 can be done algebraically, and it's not wrong to say that the algebra involved does come from 1. My question is, can 2 be viewed more geometrically? Can it feed us back a better geometric interpretation? [How would one visualize the row vectors of $A$ from the column vectors? And how would one think, geometrically, about the equality of the signed volume spanned by two visually different parallelepipeds?] Edit: If you are going to use multiplicativity of the determinant, please provide a geometric intuition to that too. Keep reading to see what I mean. Here is one of my failed attempts. I introduce a different interpretation: $\det(A)$ is the ratio of the volume of an arbitrary parallelepiped changed by applying $A^T$ (left multiplication), viewed as a linear transformation. The reason I say applying $A^T$ instead of applying $A$ is that I want the transpose to appear somewhere, and coincidentally the computation of $A^T u$ can be thought of geometrically in terms of columns of $A$: $A^T u$ is the vector whose components are dot products of columns of $A$ with $u$. (I'll take for granted the validity of the word arbitrary in 3 for the moment.) What is left to argue is the equality of 1 and 3, which I have not been able to do geometrically. Is This a Duplicate Question? In a sense yes, and in a sense no. I did participate in that question a long time ago. There was no satisfactory answer. The accepted answer does not say what a ""really geometric"" interpretation of (*) should be. Yes you can always do certain row and column operations to put a matrix into a diagonal form without affecting the determinant (with an exception of a possible sign flip), and by induction, one can prove $\det(A) = \det(A^T)$. This, however, seems to me more algebraic than geometric even though each step can be thought of as geometric. I am aware that the property of an interpretation being ""geometric"" or ""algebraic"" is not a rigorous notion and is quite subjective. Some may say that this is as far as ""geometry"" can assist us. I just want to know if that is really the case. That is why I tag this as a soft question. I also secretly hope that maybe one such interpretation would serve as a simple explanation of Cauchy-Binet formula.","Below are two well-known statements regarding the determinant function: When $A$ is a square matrix, $\det(A)$ is the signed volume of the parallelepiped whose edges are columns of $A$. When $A$ is a square matrix, $\det(A) = \det(A^T)$. It appears to me that 1 is how $\det$ got invented in the first place. The multilinearity and alternating property are natural consequences. Cramer's rule can also be easily understood geometrically. However, 1 does not immediately yield 2. Of course the proof of 2 can be done algebraically, and it's not wrong to say that the algebra involved does come from 1. My question is, can 2 be viewed more geometrically? Can it feed us back a better geometric interpretation? [How would one visualize the row vectors of $A$ from the column vectors? And how would one think, geometrically, about the equality of the signed volume spanned by two visually different parallelepipeds?] Edit: If you are going to use multiplicativity of the determinant, please provide a geometric intuition to that too. Keep reading to see what I mean. Here is one of my failed attempts. I introduce a different interpretation: $\det(A)$ is the ratio of the volume of an arbitrary parallelepiped changed by applying $A^T$ (left multiplication), viewed as a linear transformation. The reason I say applying $A^T$ instead of applying $A$ is that I want the transpose to appear somewhere, and coincidentally the computation of $A^T u$ can be thought of geometrically in terms of columns of $A$: $A^T u$ is the vector whose components are dot products of columns of $A$ with $u$. (I'll take for granted the validity of the word arbitrary in 3 for the moment.) What is left to argue is the equality of 1 and 3, which I have not been able to do geometrically. Is This a Duplicate Question? In a sense yes, and in a sense no. I did participate in that question a long time ago. There was no satisfactory answer. The accepted answer does not say what a ""really geometric"" interpretation of (*) should be. Yes you can always do certain row and column operations to put a matrix into a diagonal form without affecting the determinant (with an exception of a possible sign flip), and by induction, one can prove $\det(A) = \det(A^T)$. This, however, seems to me more algebraic than geometric even though each step can be thought of as geometric. I am aware that the property of an interpretation being ""geometric"" or ""algebraic"" is not a rigorous notion and is quite subjective. Some may say that this is as far as ""geometry"" can assist us. I just want to know if that is really the case. That is why I tag this as a soft question. I also secretly hope that maybe one such interpretation would serve as a simple explanation of Cauchy-Binet formula.",,"['linear-algebra', 'geometry', 'soft-question', 'determinant', 'exterior-algebra']"
23,Trying to understand polar coordinate vectors,Trying to understand polar coordinate vectors,,"I'm trying to understand what the unit polar coordinate vectors (I'll denote them $\hat r$ and $\hat \phi$ ) are and if they form a basis for $\Bbb R^2$ . So from what I understand, $\hat r$ points radially from the center and $\hat {\theta}$ points orthogonally to $\hat r$ , as in the picture below: Then a vector $\vec v$ could be represented by a linear combination of $\vec v$ , i.e. $\vec v=a \hat r + b \hat \phi$ .  My question is, if $\{\hat r,\hat \phi\}$ is a basis of $\Bbb R^2$ , then knowing $(a,b)$ should completely specify any point, right?  But I don't understand how either $\hat r$ or $\hat \phi$ specify the angle that the vector makes with the positive $x$ -axis (or whatever that reference line is called in polar coordinates).  I can see how, if you know the Cartesian representation of $\hat r$ or $\hat \phi$ , you can then determine the other and thus specify any point, but can you specify any point in the plane without a conversion to Cartesian first?  If not, why are $\hat r$ and $\hat \phi$ even useful? Edit : Muphrid does a good job of explaining why trying to think of $\hat r$ and $\hat \phi$ as basis vectors of $\Bbb R^2$ isn't the best way to use them.  But in case anyone who reads this is interested in where they come from, how to use them, and how to derive the relations for the grad, div, and curl, I found this pretty good set of lecture notes .","I'm trying to understand what the unit polar coordinate vectors (I'll denote them and ) are and if they form a basis for . So from what I understand, points radially from the center and points orthogonally to , as in the picture below: Then a vector could be represented by a linear combination of , i.e. .  My question is, if is a basis of , then knowing should completely specify any point, right?  But I don't understand how either or specify the angle that the vector makes with the positive -axis (or whatever that reference line is called in polar coordinates).  I can see how, if you know the Cartesian representation of or , you can then determine the other and thus specify any point, but can you specify any point in the plane without a conversion to Cartesian first?  If not, why are and even useful? Edit : Muphrid does a good job of explaining why trying to think of and as basis vectors of isn't the best way to use them.  But in case anyone who reads this is interested in where they come from, how to use them, and how to derive the relations for the grad, div, and curl, I found this pretty good set of lecture notes .","\hat r \hat \phi \Bbb R^2 \hat r \hat {\theta} \hat r \vec v \vec v \vec v=a \hat r + b \hat \phi \{\hat r,\hat \phi\} \Bbb R^2 (a,b) \hat r \hat \phi x \hat r \hat \phi \hat r \hat \phi \hat r \hat \phi \Bbb R^2","['linear-algebra', 'coordinate-systems']"
24,Truly intuitive geometric interpretation for the transpose of a square matrix,Truly intuitive geometric interpretation for the transpose of a square matrix,,"I'm looking for an easily understandable interpretation for a transpose of a square matrix A. An intuitive visual demonstration, how $A^{T}$ relates to A. I want to be able to instantly visualize in my mind what I'm doing to the space when transposing the vectors of a matrix. From experience, understanding linear algebra concepts in two dimensions is often enough to understand concepts in any higher dimension, so an explanation for two dimensional spaces should be enough I think. All explanations I found so far were not intuitive enough, as I want to be able to instantly imagine (and draw) how $A^{T}$ looks like given A. I'm not a mathematician btw. Here is what I found so far (but not intuitive enough for me) (Ax)$\cdot$y=$(Ax)^{T}$y=$x^{T}A^{T}$y=x$\cdot$$A^{T}$y As far I understand dot product is a projection (x onto y, y onto x, both interpretations have the same result) followed by a scaling by the length of the other vector. This would mean that mapping x into space A and projecting y onto the result is the same as mapping y into the space of $A^{T}$, then projecting the unmapped x into $A^{T}$y So $A^{T}$ is the specific space B for any pair of vectors (x,y) such that Ax$\cdot$y=x$\cdot$By This doesn't tell me instantly how $A^{T}$ drawn as vectors would look like based on A drawn as vectors. ""reassigning dimensions"" This one is hard to explain so let me do this with a drawing: parallel projections This explanation is much more visual, but far too messy to do it in my head instantly. There are also multiple ways I could have rotated and arranged the vectors around the result $A^{T}$ which is represented in the middle. Also, it doesn't feel like it makes me truly understand the transposing of matrices, especially in higher dimensions. some kind of weird rotation Symmetrical matrices can be decomposed into a rotation, scaling along eigenvectors $\Lambda$ and a rotation back A=R$\Lambda$$R^{T}$ So in this specific case, the transpose is a rotation in the opposite direction of the original. I don't know how to generalize that into arbitrary matrices. I'm wildly guessing that if A is not symmetric any more, $R^{T}$ must also include some additional operations besides rotation. Can anyone help me to find a way to easily and instantly imagine/draw how $A^{T}$ looks like given A in two dimensional space? (In a way of understanding that is generalizable into higher dimensions) Edit 1: While working on the problem I was curious to see what B in $BA=A^{T}$ looks like. B would describe what needs to be done to A in order to geometrically transpose it. My temporary result looks interesting but I'm still trying to bring it to an interpretable form. If we assume the following indexing order $$A=         \begin{bmatrix}         a_{11} & a_{12} \\         a_{21} & a_{22} \\         \end{bmatrix} $$ and $det(A)\neq0$ then $$B=\frac{1}{det(A)}         \begin{bmatrix}         a_{11} a_{22} - a_{21}^2 & a_{11} (a_{21} - a_{12})  \\         a_{22} (a_{12} - a_{21}) & a_{11} a_{22} - a_{12}^2 \\         \end{bmatrix} $$ What's visible on the first sight is that $\frac{1}{det(A)}$ causes scaling such that the area becomes exactly 1 (before applying the actual matrix). B must also preserve the area as $det(A^{T})=det(A)$. It means that the matrix $B'=\begin{bmatrix} a_{11} a_{22} - a_{21}^2 & a_{11} (a_{21} - a_{12})  \\ a_{22} (a_{12} - a_{21}) & a_{11} a_{22} - a_{12}^2 \\ \end{bmatrix}$ squares the area while transposing. Edit 2: The same matrix can be written as $B'=\begin{bmatrix} \begin{bmatrix} a_{11} & a_{21} \\ \end{bmatrix}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  \begin{bmatrix} a_{11} & a_{21} \\ \end{bmatrix}   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ \begin{bmatrix} a_{21} & a_{22} \\ \end{bmatrix}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  \begin{bmatrix} a_{12} & a_{22} \\ \end{bmatrix}   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}$ Which is $B'=\begin{bmatrix} a_{1}^{T} \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{1}^{T}  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ a_{2}^{T}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{2}^{T}    \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}= \begin{bmatrix} a_{1}\cdot \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{1}\cdot  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ a_{2}\cdot  \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{2}\cdot   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}$ I find the vectors $c_{1}=\begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}$ and $c_{2}=\begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}$ interesting. When I draw them it looks like I only need to rotate each by 90 degress in different directions to end up with the transpose column vectors. Edit 3: Maybe I fool myself, but I think I'm getting closer. The column space $C= \begin{bmatrix} c_{1}  & c_{2}  \\ \end{bmatrix} = \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix}$ is related to $A^{-1}$ because: $AC=\begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} \cdot \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix} = \begin{bmatrix} det(A)  & 0  \\ 0 & det(A) \\ \end{bmatrix}  =det(A) I$ So $C=A^{-1}det(A)$ B' can be written as well like this: $B'=\begin{bmatrix} \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix} \end{bmatrix} \\ \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \end{bmatrix} \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  c_{1} \end{bmatrix} \\ \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  c_{2} \end{bmatrix} \end{bmatrix}$ or like this $B'=\begin{bmatrix} \begin{bmatrix} a_{11}  & a_{21}  \\ \end{bmatrix}   & \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix}  \\ \begin{bmatrix} a_{21}  & a_{22}  \\ \end{bmatrix} & \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} a_1^{T} & \begin{bmatrix} c_{1}  & c_{2} \\ \end{bmatrix}  \\ a_2^{T} & \begin{bmatrix} c_{1}  & c_{2} \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} a_1^{T}C  \\ a_2^{T}C \\ \end{bmatrix} = det(A) \begin{bmatrix} a_1^{T}A^{-1} \\ a_2^{T}A^{-1} \\ \end{bmatrix}$ Therefore for $BA=A^{T}$ we have $B=\begin{bmatrix} a_1^{T}A^{-1} \\ a_2^{T}A^{-1} \\ \end{bmatrix}$ Edit 4: I think I will post my own answer soon. Going down the path of $A^{-1}$ had the idea that one can exploit the symmetry of of $AA^{T}$. Symmetry means that $AA^{T}$ decomposes nicer: $AA^{T} = R_{AA^{T}} \Lambda_{AA^{T}} (R^{-1})_{AA^{T}}$ Now if you multiply both sides with $A^{-1}$ you'll get $A^{T} = A^{-1} R_{AA^{T}} \Lambda_{AA^{T}} (R^{-1})_{AA^{T}}$ When I do an example with numbers I can also see that in my example $R_{AA^{T}} = (R^{-1})_{AA^{T}}$ $R_{AA^{T}}$ mirrors the space along y axis and then rotates by some angle $\alpha$ So my suspicion right now is: $A^{T}=A^{-1} R_{AA^{T}} \Lambda_{AA^{T}} R_{AA^{T}}$ Now if I define $R_{AA^{T}}^{'} = \begin{bmatrix} cos \alpha & -sin \alpha \\ sin \alpha & cos \alpha \\ \end{bmatrix}$ to get the mirroring out of the matrix $R_{AA^{T}}$ then I get $A^{T}=A^{-1}  R_{AA^{T}}^{'}  \begin{bmatrix} -1 & 0 \\ 0 & 1 \\ \end{bmatrix} \Lambda_{AA^{T}}  R_{AA^{T}}^{'} \begin{bmatrix} -1 & 0 \\ 0 & 1 \\ \end{bmatrix} $ So generally $A^{T}=A^{-1} R_{\alpha} M_y \Lambda R_{\alpha} M_y$ With $M_y$ being the mirroring along the y axis, $R_{\alpha}$ some counter-clockwise rotation by $\alpha$ and $\Lambda$ some scaling","I'm looking for an easily understandable interpretation for a transpose of a square matrix A. An intuitive visual demonstration, how $A^{T}$ relates to A. I want to be able to instantly visualize in my mind what I'm doing to the space when transposing the vectors of a matrix. From experience, understanding linear algebra concepts in two dimensions is often enough to understand concepts in any higher dimension, so an explanation for two dimensional spaces should be enough I think. All explanations I found so far were not intuitive enough, as I want to be able to instantly imagine (and draw) how $A^{T}$ looks like given A. I'm not a mathematician btw. Here is what I found so far (but not intuitive enough for me) (Ax)$\cdot$y=$(Ax)^{T}$y=$x^{T}A^{T}$y=x$\cdot$$A^{T}$y As far I understand dot product is a projection (x onto y, y onto x, both interpretations have the same result) followed by a scaling by the length of the other vector. This would mean that mapping x into space A and projecting y onto the result is the same as mapping y into the space of $A^{T}$, then projecting the unmapped x into $A^{T}$y So $A^{T}$ is the specific space B for any pair of vectors (x,y) such that Ax$\cdot$y=x$\cdot$By This doesn't tell me instantly how $A^{T}$ drawn as vectors would look like based on A drawn as vectors. ""reassigning dimensions"" This one is hard to explain so let me do this with a drawing: parallel projections This explanation is much more visual, but far too messy to do it in my head instantly. There are also multiple ways I could have rotated and arranged the vectors around the result $A^{T}$ which is represented in the middle. Also, it doesn't feel like it makes me truly understand the transposing of matrices, especially in higher dimensions. some kind of weird rotation Symmetrical matrices can be decomposed into a rotation, scaling along eigenvectors $\Lambda$ and a rotation back A=R$\Lambda$$R^{T}$ So in this specific case, the transpose is a rotation in the opposite direction of the original. I don't know how to generalize that into arbitrary matrices. I'm wildly guessing that if A is not symmetric any more, $R^{T}$ must also include some additional operations besides rotation. Can anyone help me to find a way to easily and instantly imagine/draw how $A^{T}$ looks like given A in two dimensional space? (In a way of understanding that is generalizable into higher dimensions) Edit 1: While working on the problem I was curious to see what B in $BA=A^{T}$ looks like. B would describe what needs to be done to A in order to geometrically transpose it. My temporary result looks interesting but I'm still trying to bring it to an interpretable form. If we assume the following indexing order $$A=         \begin{bmatrix}         a_{11} & a_{12} \\         a_{21} & a_{22} \\         \end{bmatrix} $$ and $det(A)\neq0$ then $$B=\frac{1}{det(A)}         \begin{bmatrix}         a_{11} a_{22} - a_{21}^2 & a_{11} (a_{21} - a_{12})  \\         a_{22} (a_{12} - a_{21}) & a_{11} a_{22} - a_{12}^2 \\         \end{bmatrix} $$ What's visible on the first sight is that $\frac{1}{det(A)}$ causes scaling such that the area becomes exactly 1 (before applying the actual matrix). B must also preserve the area as $det(A^{T})=det(A)$. It means that the matrix $B'=\begin{bmatrix} a_{11} a_{22} - a_{21}^2 & a_{11} (a_{21} - a_{12})  \\ a_{22} (a_{12} - a_{21}) & a_{11} a_{22} - a_{12}^2 \\ \end{bmatrix}$ squares the area while transposing. Edit 2: The same matrix can be written as $B'=\begin{bmatrix} \begin{bmatrix} a_{11} & a_{21} \\ \end{bmatrix}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  \begin{bmatrix} a_{11} & a_{21} \\ \end{bmatrix}   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ \begin{bmatrix} a_{21} & a_{22} \\ \end{bmatrix}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  \begin{bmatrix} a_{12} & a_{22} \\ \end{bmatrix}   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}$ Which is $B'=\begin{bmatrix} a_{1}^{T} \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{1}^{T}  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ a_{2}^{T}   \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{2}^{T}    \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}= \begin{bmatrix} a_{1}\cdot \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{1}\cdot  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}  \\ a_{2}\cdot  \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}  &  a_{2}\cdot   \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \\ \end{bmatrix}$ I find the vectors $c_{1}=\begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix}$ and $c_{2}=\begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix}$ interesting. When I draw them it looks like I only need to rotate each by 90 degress in different directions to end up with the transpose column vectors. Edit 3: Maybe I fool myself, but I think I'm getting closer. The column space $C= \begin{bmatrix} c_{1}  & c_{2}  \\ \end{bmatrix} = \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix}$ is related to $A^{-1}$ because: $AC=\begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} \cdot \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix} = \begin{bmatrix} det(A)  & 0  \\ 0 & det(A) \\ \end{bmatrix}  =det(A) I$ So $C=A^{-1}det(A)$ B' can be written as well like this: $B'=\begin{bmatrix} \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  \begin{bmatrix} a_{22} \\ -a_{21} \\ \end{bmatrix} \end{bmatrix} \\ \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  \begin{bmatrix} -a_{12} \\ a_{11} \\ \end{bmatrix} \end{bmatrix} \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  c_{1} \end{bmatrix} \\ \begin{bmatrix} \begin{bmatrix} a_{11}  & a_{12}  \\ a_{21} & a_{22} \\ \end{bmatrix} &  c_{2} \end{bmatrix} \end{bmatrix}$ or like this $B'=\begin{bmatrix} \begin{bmatrix} a_{11}  & a_{21}  \\ \end{bmatrix}   & \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix}  \\ \begin{bmatrix} a_{21}  & a_{22}  \\ \end{bmatrix} & \begin{bmatrix} a_{22}  & -a_{12}  \\ -a_{21} & a_{11} \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} a_1^{T} & \begin{bmatrix} c_{1}  & c_{2} \\ \end{bmatrix}  \\ a_2^{T} & \begin{bmatrix} c_{1}  & c_{2} \\ \end{bmatrix} \\ \end{bmatrix} = \begin{bmatrix} a_1^{T}C  \\ a_2^{T}C \\ \end{bmatrix} = det(A) \begin{bmatrix} a_1^{T}A^{-1} \\ a_2^{T}A^{-1} \\ \end{bmatrix}$ Therefore for $BA=A^{T}$ we have $B=\begin{bmatrix} a_1^{T}A^{-1} \\ a_2^{T}A^{-1} \\ \end{bmatrix}$ Edit 4: I think I will post my own answer soon. Going down the path of $A^{-1}$ had the idea that one can exploit the symmetry of of $AA^{T}$. Symmetry means that $AA^{T}$ decomposes nicer: $AA^{T} = R_{AA^{T}} \Lambda_{AA^{T}} (R^{-1})_{AA^{T}}$ Now if you multiply both sides with $A^{-1}$ you'll get $A^{T} = A^{-1} R_{AA^{T}} \Lambda_{AA^{T}} (R^{-1})_{AA^{T}}$ When I do an example with numbers I can also see that in my example $R_{AA^{T}} = (R^{-1})_{AA^{T}}$ $R_{AA^{T}}$ mirrors the space along y axis and then rotates by some angle $\alpha$ So my suspicion right now is: $A^{T}=A^{-1} R_{AA^{T}} \Lambda_{AA^{T}} R_{AA^{T}}$ Now if I define $R_{AA^{T}}^{'} = \begin{bmatrix} cos \alpha & -sin \alpha \\ sin \alpha & cos \alpha \\ \end{bmatrix}$ to get the mirroring out of the matrix $R_{AA^{T}}$ then I get $A^{T}=A^{-1}  R_{AA^{T}}^{'}  \begin{bmatrix} -1 & 0 \\ 0 & 1 \\ \end{bmatrix} \Lambda_{AA^{T}}  R_{AA^{T}}^{'} \begin{bmatrix} -1 & 0 \\ 0 & 1 \\ \end{bmatrix} $ So generally $A^{T}=A^{-1} R_{\alpha} M_y \Lambda R_{\alpha} M_y$ With $M_y$ being the mirroring along the y axis, $R_{\alpha}$ some counter-clockwise rotation by $\alpha$ and $\Lambda$ some scaling",,"['linear-algebra', 'matrices', 'vector-spaces', 'transpose', 'geometric-interpretation']"
25,Families of Idempotent $3\times 3$ Matrices,Families of Idempotent  Matrices,3\times 3,"I did the following analysis for $2\times2$ real idempotent (i.e. $A^2=A$) matrices: $$ \begin{bmatrix}a&b\\c&d\end{bmatrix}^2=\begin{bmatrix}a^2+bc&(a+d)b\\(a+d)c&bc+d^2\end{bmatrix}=\begin{bmatrix}a&b\\c&d\end{bmatrix} $$ So in particular we have $(a+d)c=c$ and $(a+d)b=b$ so if either $b$ or $c$ is nonzero we have $a+d=1$. We also see that $a$ and $d$ both satisfy the equation $x^2+bc=x\iff x^2-x+bc=0$ which is a quadratic equation having solutions $$ x=\frac{1\pm\sqrt{1-4bc}}{2}=0.5\pm\sqrt{0.25-bc} $$ But this is only possible if $bc\leq 0.25$ for otherwise the above expression is not real. This gives us the following cases: CASE 1: If $b,c=0$ we have $x\in\{0,1\}$ and since $a+d=1$ is unnecessary we have four possibilities: $(a,d)\in\{(0,0),(1,0),(0,1),(1,1)\}$. CASE 2: If $bc=0.25$ we have $x=0.5$ so $a=d=0.5$. CASE 3: If $bc<0.25$ yet $(b,c)\neq(0,0)$ we have $x\in L=\{0.5-\sqrt{0.25-bc},0.5+\sqrt{0.25-bc}\}$ and to have $a+d=1$ we must have $\{a,d\}=L$ so that if $a$ is one solution, then $d$ is forced to be the other solution. Or the other way around. The cases can be illustrated via the following diagram graphing the hyperbola $xy=0.25$ corresponding to CASE 2 , the area $xy<0.25$ corresponding to CASE 3 , and the point $(0.0)$ corresponding to CASE 1 : The blue bands show the graphs of $xy=k$ for $k=0.05$ to $0.20$ and the cyan bands show $xy=k$ for $k=-0.05,-0.10,...$ For instance one could choose $(b,c)=(3.75,-1)$ so that $\sqrt{0.25-bc}=2$ thus rendering $x=0.5\pm 2=-1.5$ and $2.5$ and form the matrix $$ A=\begin{bmatrix}-1.5&3.75\\-1&2.5\end{bmatrix} $$ which will then be idempotent, as an example of CASE 3 . QUESTIONs: Can similar descriptions be derived for $3\times 3$ matrices? Is this a well known description of idempotent $2\times 2$ matrices?","I did the following analysis for $2\times2$ real idempotent (i.e. $A^2=A$) matrices: $$ \begin{bmatrix}a&b\\c&d\end{bmatrix}^2=\begin{bmatrix}a^2+bc&(a+d)b\\(a+d)c&bc+d^2\end{bmatrix}=\begin{bmatrix}a&b\\c&d\end{bmatrix} $$ So in particular we have $(a+d)c=c$ and $(a+d)b=b$ so if either $b$ or $c$ is nonzero we have $a+d=1$. We also see that $a$ and $d$ both satisfy the equation $x^2+bc=x\iff x^2-x+bc=0$ which is a quadratic equation having solutions $$ x=\frac{1\pm\sqrt{1-4bc}}{2}=0.5\pm\sqrt{0.25-bc} $$ But this is only possible if $bc\leq 0.25$ for otherwise the above expression is not real. This gives us the following cases: CASE 1: If $b,c=0$ we have $x\in\{0,1\}$ and since $a+d=1$ is unnecessary we have four possibilities: $(a,d)\in\{(0,0),(1,0),(0,1),(1,1)\}$. CASE 2: If $bc=0.25$ we have $x=0.5$ so $a=d=0.5$. CASE 3: If $bc<0.25$ yet $(b,c)\neq(0,0)$ we have $x\in L=\{0.5-\sqrt{0.25-bc},0.5+\sqrt{0.25-bc}\}$ and to have $a+d=1$ we must have $\{a,d\}=L$ so that if $a$ is one solution, then $d$ is forced to be the other solution. Or the other way around. The cases can be illustrated via the following diagram graphing the hyperbola $xy=0.25$ corresponding to CASE 2 , the area $xy<0.25$ corresponding to CASE 3 , and the point $(0.0)$ corresponding to CASE 1 : The blue bands show the graphs of $xy=k$ for $k=0.05$ to $0.20$ and the cyan bands show $xy=k$ for $k=-0.05,-0.10,...$ For instance one could choose $(b,c)=(3.75,-1)$ so that $\sqrt{0.25-bc}=2$ thus rendering $x=0.5\pm 2=-1.5$ and $2.5$ and form the matrix $$ A=\begin{bmatrix}-1.5&3.75\\-1&2.5\end{bmatrix} $$ which will then be idempotent, as an example of CASE 3 . QUESTIONs: Can similar descriptions be derived for $3\times 3$ matrices? Is this a well known description of idempotent $2\times 2$ matrices?",,"['linear-algebra', 'idempotents']"
26,Blockwise Moore-Penrose pseudoinverse?,Blockwise Moore-Penrose pseudoinverse?,,"There exists a convenient formula for computing the inverse of a block matrix consisting of 4 matrices $\mathbf{A, B, C, D}$ $ \begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix} ^{-1}$ the inverse can be written as a function of $A^{-1}$ and $(A-B D^{-1}C)^{-1}$ ( wikipedia ) $\begin{bmatrix} \mathbf{A}^{-1}+\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \end{bmatrix}$ I wonder if a similar formula exists for the pseudo-inverse of non-invertible block matrices.","There exists a convenient formula for computing the inverse of a block matrix consisting of 4 matrices $\mathbf{A, B, C, D}$ $ \begin{bmatrix}\mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D}\end{bmatrix} ^{-1}$ the inverse can be written as a function of $A^{-1}$ and $(A-B D^{-1}C)^{-1}$ ( wikipedia ) $\begin{bmatrix} \mathbf{A}^{-1}+\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & -\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \\ -(\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1}\mathbf{CA}^{-1} & (\mathbf{D}-\mathbf{CA}^{-1}\mathbf{B})^{-1} \end{bmatrix}$ I wonder if a similar formula exists for the pseudo-inverse of non-invertible block matrices.",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
27,Can a continuous map $S^2 \rightarrow S^2$ preserve orthogonality without being an isometry?,Can a continuous map  preserve orthogonality without being an isometry?,S^2 \rightarrow S^2,"Suppose I have a map $\phi: S^2 \rightarrow S^2$ and I know that a) $\phi$ is continuous and bijective b) If $a$ and $b$ subtend an angle of $\pi / 2$ at the center of the sphere, then so do $\phi(a)$ and $\phi(b)$. Does it follow that $\phi$ is an isometry of the sphere? If yes, can you sketch a proof? If no, can you furnish an example of a non-isometry $\phi$ satisfying the above? (Also, in either case, is the requirement that $\phi$ be bijective redundant?)","Suppose I have a map $\phi: S^2 \rightarrow S^2$ and I know that a) $\phi$ is continuous and bijective b) If $a$ and $b$ subtend an angle of $\pi / 2$ at the center of the sphere, then so do $\phi(a)$ and $\phi(b)$. Does it follow that $\phi$ is an isometry of the sphere? If yes, can you sketch a proof? If no, can you furnish an example of a non-isometry $\phi$ satisfying the above? (Also, in either case, is the requirement that $\phi$ be bijective redundant?)",,"['linear-algebra', 'riemannian-geometry', 'projective-geometry']"
28,Intuition behind spectrum of an operator,Intuition behind spectrum of an operator,,"I am trying to understand better the concept of the spectrum of an operator, and it gives me some troubles, I would like to know if there is an intuitive idea behind this, for example, a part of the spectrum are the eigenvalues (I think this is the point spectrum), and they have a very strong ""geometric"" interpretation. But, what happens with the elements in the continuous spectrum or the residual spectrum? And we can also define something like the spectral radius ( $r_\sigma (T)=\sup \{|\lambda|\colon \lambda \in \sigma (T) \}$ ), all this stuff can be somehow interpreted? Or how should I approach them to understand them in a better way? Also I am not sure how to ""think"" the spectrum when I see linear transformations related to matrices. If you can help me to see this, it would be very helpful. Thanks.","I am trying to understand better the concept of the spectrum of an operator, and it gives me some troubles, I would like to know if there is an intuitive idea behind this, for example, a part of the spectrum are the eigenvalues (I think this is the point spectrum), and they have a very strong ""geometric"" interpretation. But, what happens with the elements in the continuous spectrum or the residual spectrum? And we can also define something like the spectral radius ( ), all this stuff can be somehow interpreted? Or how should I approach them to understand them in a better way? Also I am not sure how to ""think"" the spectrum when I see linear transformations related to matrices. If you can help me to see this, it would be very helpful. Thanks.",r_\sigma (T)=\sup \{|\lambda|\colon \lambda \in \sigma (T) \},"['linear-algebra', 'functional-analysis', 'soft-question', 'intuition', 'spectral-theory']"
29,Relation between left and right eigenvectors corresponding to the same eigenvalue,Relation between left and right eigenvectors corresponding to the same eigenvalue,,"I have a general question on how the left eigenvectors and right eigenvectors of a matrix are related to each other. Background. It is easy to see that the characteristic polynomial of a $A$ and $A^\top$ are the same, hence the ""left"" and ""right"" eigenvalues of $A$ are the same. Is there any geometric reason on why this should happen? And moreover, why there should be any relations between the left and right eigenvectors corresponding to the same eigenvalue? To be more clear, I can prove the following: Observation. Let $A \in \mathbb{C}^{n\times n}$ have $n$ distinct eigenvalues. Then for an eigenvalue $\lambda$ and corresponding left eigenvector $u^\top$ and right eigenvector $v$, we have $u^\top v \neq 0$. Proof. Let $J$ be the Jordan canonical form of $A$. Since all the eigenvalues of $A$ are simple, $J$ is diagonal. Let $A = SJS^{-1}$ for some invertible matrix $S$. Observe that for an eigenvalue $\lambda$ there is an $1 \leq i \leq n$ such that the $i$-th column of $S$, $s^i$, is a right eigenvector of $A$ for the eigenvalue $\lambda$, and the $i$-th row of $S^{-1}$, ${s_i}^\top$, is a left eigenvector of $A$ for the eigenvalue $\lambda$. Since $S^{-1} S = I$ we have ${s_i}^\top s^i = 1$. This implies $u^\top v \neq 0$. Note that this need not be true in general. For example when there isn't a full set of eigenvectors, like in $$\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{bmatrix}.$$  So, I want to make a claim as following: Claim. If $\lambda$ is an eigenvalue of $A$ where its geometric multiplicity is equal to its algebraic multiplicity, then there are left and right eigenvectors of $A$ corresponding to $\lambda$, respectively $u^\top$ and $v$, such that $u^\top v \neq 0$. Note that the claim can't be true for all left and right eigenvectors, instead of there is . For example consider the identity matrix., So, my questions are: Questions. Is there any geometric reasons that eigenvalues of $A$ and $A^\top$ are equal? Is there any intuitive way to see why the observation above holds? Is the claim above true? -- Owen Biesel in their comment to this question mentions that left eigenvectors are perpendicular to hyperplanes that are preserved under left multiplication. In that sense, that would mean $u^\top$ and $v$ are perpendicular if $v$ is in the hyperplane perpendicular to $u^\top$. But I can't quite make a connection to prove what I want.","I have a general question on how the left eigenvectors and right eigenvectors of a matrix are related to each other. Background. It is easy to see that the characteristic polynomial of a $A$ and $A^\top$ are the same, hence the ""left"" and ""right"" eigenvalues of $A$ are the same. Is there any geometric reason on why this should happen? And moreover, why there should be any relations between the left and right eigenvectors corresponding to the same eigenvalue? To be more clear, I can prove the following: Observation. Let $A \in \mathbb{C}^{n\times n}$ have $n$ distinct eigenvalues. Then for an eigenvalue $\lambda$ and corresponding left eigenvector $u^\top$ and right eigenvector $v$, we have $u^\top v \neq 0$. Proof. Let $J$ be the Jordan canonical form of $A$. Since all the eigenvalues of $A$ are simple, $J$ is diagonal. Let $A = SJS^{-1}$ for some invertible matrix $S$. Observe that for an eigenvalue $\lambda$ there is an $1 \leq i \leq n$ such that the $i$-th column of $S$, $s^i$, is a right eigenvector of $A$ for the eigenvalue $\lambda$, and the $i$-th row of $S^{-1}$, ${s_i}^\top$, is a left eigenvector of $A$ for the eigenvalue $\lambda$. Since $S^{-1} S = I$ we have ${s_i}^\top s^i = 1$. This implies $u^\top v \neq 0$. Note that this need not be true in general. For example when there isn't a full set of eigenvectors, like in $$\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{bmatrix}.$$  So, I want to make a claim as following: Claim. If $\lambda$ is an eigenvalue of $A$ where its geometric multiplicity is equal to its algebraic multiplicity, then there are left and right eigenvectors of $A$ corresponding to $\lambda$, respectively $u^\top$ and $v$, such that $u^\top v \neq 0$. Note that the claim can't be true for all left and right eigenvectors, instead of there is . For example consider the identity matrix., So, my questions are: Questions. Is there any geometric reasons that eigenvalues of $A$ and $A^\top$ are equal? Is there any intuitive way to see why the observation above holds? Is the claim above true? -- Owen Biesel in their comment to this question mentions that left eigenvectors are perpendicular to hyperplanes that are preserved under left multiplication. In that sense, that would mean $u^\top$ and $v$ are perpendicular if $v$ is in the hyperplane perpendicular to $u^\top$. But I can't quite make a connection to prove what I want.",,"['linear-algebra', 'soft-question', 'eigenvalues-eigenvectors']"
30,"Matrix function converges, how about the eigenvalues?","Matrix function converges, how about the eigenvalues?",,"Suppose I have a matrix function $A(t)$ with $$\lVert A(t) - B\rVert \le ct^\alpha$$ in some matrix norm (this will work for any norm, I guess). So, in a sense $A(t)\rightarrow B$ for $t\rightarrow 0$ in $\mathcal{O}(t^\alpha)$. Plus, we have $A(0) = B$. I happen to know the eigenvalues of $B$, but I don't know a thing about the eigenvalues of $A(t)$. Plus, $A(t)$ does not have any favorable structure, in particular, no symmetry. So, what can you say about the eigenvalues of $A(t)$? In particular: What about the spectral radius of $\lambda_{A(t)}$? Does it converge to the spectral radius of $B$? Do we have $\lambda_{A(t)}\rightarrow\lambda_B$ for $t\rightarrow 0$ for all eigenvalues $\lambda_{A(t)}$ of $A(t)$? And finally: is the speed of convergence $\mathcal{O}(t^\alpha)$ the same for the eigenvalues/spectral radius as for the matrix function? The last question is actually the most important one. If the eigenvalues of $B$ are all zero, the eigenvalues as well as the spectral radius of $A(t)$ would go to zero as $\mathcal{O}(t^\alpha)$... Any help would be appreciated, incl. references to (standard?) textbooks or papers on this matter. Maybe there is a counterexample? So far, in all numerical examples I have seen/done, all properties above do hold. Edit : To provide a bit more background: The matrices $A(t)$ are iteration matrices which depend on a time-step size $t$. They are not this  ugly, but showing convergence of this iteration has proven to be rather difficult. In the simplest case, they look like $$A(t) = (I-tQ_1)^{-1}(t(Q_2-Q_1)+B)$$ with identity matrix $I$ and some matrices $Q_1,Q_2$, which do not have any particular structure we were able to exploit so far. Now, if I can make that conclusion about the spectral radius as described above, I can state that the spectral radius is smaller than 1, i.e. the iteration converges, if the time-step size $t$ is small enough. Edit: Does this answer help? Also, this question might be related to perturbation theory for eigenvalue problems (with non-symmetric matrices, though, and $B$ is not diagonalizable).","Suppose I have a matrix function $A(t)$ with $$\lVert A(t) - B\rVert \le ct^\alpha$$ in some matrix norm (this will work for any norm, I guess). So, in a sense $A(t)\rightarrow B$ for $t\rightarrow 0$ in $\mathcal{O}(t^\alpha)$. Plus, we have $A(0) = B$. I happen to know the eigenvalues of $B$, but I don't know a thing about the eigenvalues of $A(t)$. Plus, $A(t)$ does not have any favorable structure, in particular, no symmetry. So, what can you say about the eigenvalues of $A(t)$? In particular: What about the spectral radius of $\lambda_{A(t)}$? Does it converge to the spectral radius of $B$? Do we have $\lambda_{A(t)}\rightarrow\lambda_B$ for $t\rightarrow 0$ for all eigenvalues $\lambda_{A(t)}$ of $A(t)$? And finally: is the speed of convergence $\mathcal{O}(t^\alpha)$ the same for the eigenvalues/spectral radius as for the matrix function? The last question is actually the most important one. If the eigenvalues of $B$ are all zero, the eigenvalues as well as the spectral radius of $A(t)$ would go to zero as $\mathcal{O}(t^\alpha)$... Any help would be appreciated, incl. references to (standard?) textbooks or papers on this matter. Maybe there is a counterexample? So far, in all numerical examples I have seen/done, all properties above do hold. Edit : To provide a bit more background: The matrices $A(t)$ are iteration matrices which depend on a time-step size $t$. They are not this  ugly, but showing convergence of this iteration has proven to be rather difficult. In the simplest case, they look like $$A(t) = (I-tQ_1)^{-1}(t(Q_2-Q_1)+B)$$ with identity matrix $I$ and some matrices $Q_1,Q_2$, which do not have any particular structure we were able to exploit so far. Now, if I can make that conclusion about the spectral radius as described above, I can state that the spectral radius is smaller than 1, i.e. the iteration converges, if the time-step size $t$ is small enough. Edit: Does this answer help? Also, this question might be related to perturbation theory for eigenvalue problems (with non-symmetric matrices, though, and $B$ is not diagonalizable).",,"['linear-algebra', 'matrices', 'convergence-divergence', 'eigenvalues-eigenvectors', 'perturbation-theory']"
31,Do these matrix rings have non-zero elements that are neither units nor zero divisors?,Do these matrix rings have non-zero elements that are neither units nor zero divisors?,,"Let $R$ be a commutative ring (with $1$ ) and $R^{n \times n}$ be the ring of $n \times n$ matrices with entries in $R$ . In addition, suppose that $R$ is a ring in which every non-zero element is either a zero divisor or a unit [For example: take any finite ring or any field.] My question: Is every non-zero element of $R^{n \times n}$ a zero divisor or a unit as well? We know that if $A \in R^{n \times n}$ , then $AC=CA=\mathrm{det}(A)I_n$ where $C$ is the classical adjoint of $A$ and $I_n$ is the identity matrix. This means that if $\mathrm{det}(A)$ is a unit of $R$ , then $A$ is a unit of $R^{n \times n}$ (since $A^{-1}=(\mathrm{det}(A))^{-1}C$ ). Also, the converse holds, if $A$ is a unit of $R^{n \times n}$ , then $\mathrm{det}(A)$ is a unit. I would like to know if one can show $0 \not= A \in R^{n \times n}$ is a zero divisor if $\mathrm{det}(A)$ is zero or a zero divisor. Things to consider: 1) This is true when $R=\mathbb{F}$ a field. Since over a field (no zero divisors) and if $\mathrm{det}(A)=0$ then $Ax=0$ has a non-trivial solution and so $B=[x|0|\cdots|0]$ gives us a right zero divisor $AB=0$ . 2) You can't use the classical adjoint to construct a zero divisor since it can be zero even when $A$ is not zero. For example: $$A=\begin{bmatrix} 1 & 1 & 1 \\ 0  & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \qquad \mathrm{implies} \qquad \mathrm{classical\;adjoint} = 0 $$ (All $2 \times 2$ sub-determinants are zero.) 3) This is true when $R$ is finite (since $R^{n \times n}$ would be finite as well). 4) Of course the assumption that every non-zero element of $R$ is either a zero divisor or unit is necessary since otherwise take a non-zero, non-zero divisor, non-unit element $r$ and construct the diagonal matrix $D = \mathrm{diag}(r,1,\dots,1)$ (this is non-zero, not a zero divisor, and is not a unit). Edit: Not totally unrelated... https://mathoverflow.net/questions/42647/rings-in-which-every-non-unit-is-a-zero-divisor Edit: One more thing to consider... 5) This is definitely true when $n=1$ and $n=2$ . It is true for $n=1$ by assumption on $R$ . To see that $n=2$ is true notice that the classical adjoint contains the same same elements as that of $A$ (or negations): $$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \qquad \Longrightarrow \qquad \mathrm{classical\;adjoint} = C = \begin{bmatrix} a_{22} & -a_{12} \\ -a_{21} & a_{11} \end{bmatrix} $$ Thus if $\mathrm{det}(A)b=0$ for some $b \not=0$ , then either $bC=0$ so that all of the entries of both $A$ and $C$ are annihilated by $b$ so that $A(bI_2)=0$ or $bC \not=0$ and so $A(Cb)=\mathrm{det}(A)bI_2 =0I_2=0$ . Thus $A$ is a zero divisor.","Let be a commutative ring (with ) and be the ring of matrices with entries in . In addition, suppose that is a ring in which every non-zero element is either a zero divisor or a unit [For example: take any finite ring or any field.] My question: Is every non-zero element of a zero divisor or a unit as well? We know that if , then where is the classical adjoint of and is the identity matrix. This means that if is a unit of , then is a unit of (since ). Also, the converse holds, if is a unit of , then is a unit. I would like to know if one can show is a zero divisor if is zero or a zero divisor. Things to consider: 1) This is true when a field. Since over a field (no zero divisors) and if then has a non-trivial solution and so gives us a right zero divisor . 2) You can't use the classical adjoint to construct a zero divisor since it can be zero even when is not zero. For example: (All sub-determinants are zero.) 3) This is true when is finite (since would be finite as well). 4) Of course the assumption that every non-zero element of is either a zero divisor or unit is necessary since otherwise take a non-zero, non-zero divisor, non-unit element and construct the diagonal matrix (this is non-zero, not a zero divisor, and is not a unit). Edit: Not totally unrelated... https://mathoverflow.net/questions/42647/rings-in-which-every-non-unit-is-a-zero-divisor Edit: One more thing to consider... 5) This is definitely true when and . It is true for by assumption on . To see that is true notice that the classical adjoint contains the same same elements as that of (or negations): Thus if for some , then either so that all of the entries of both and are annihilated by so that or and so . Thus is a zero divisor.","R 1 R^{n \times n} n \times n R R R^{n \times n} A \in R^{n \times n} AC=CA=\mathrm{det}(A)I_n C A I_n \mathrm{det}(A) R A R^{n \times n} A^{-1}=(\mathrm{det}(A))^{-1}C A R^{n \times n} \mathrm{det}(A) 0 \not= A \in R^{n \times n} \mathrm{det}(A) R=\mathbb{F} \mathrm{det}(A)=0 Ax=0 B=[x|0|\cdots|0] AB=0 A A=\begin{bmatrix} 1 & 1 & 1 \\ 0  & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \qquad \mathrm{implies} \qquad \mathrm{classical\;adjoint} = 0  2 \times 2 R R^{n \times n} R r D = \mathrm{diag}(r,1,\dots,1) n=1 n=2 n=1 R n=2 A  A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \qquad \Longrightarrow \qquad \mathrm{classical\;adjoint} = C = \begin{bmatrix} a_{22} & -a_{12} \\ -a_{21} & a_{11} \end{bmatrix}  \mathrm{det}(A)b=0 b \not=0 bC=0 A C b A(bI_2)=0 bC \not=0 A(Cb)=\mathrm{det}(A)bI_2 =0I_2=0 A","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
32,Circles generated by three-fold iterations $f(x)=\frac{1}{1-x}$,Circles generated by three-fold iterations,f(x)=\frac{1}{1-x},"I came across a weird property of the function $f(x)=\dfrac{1}{(1-x)}$ Observe the following: $$f(x) = \frac{1}{(1-x)}, \quad\quad f^2(x) = f(f(x)) = \frac{(x-1)}{x}, \quad\quad f^3(x) = f(f(f(x))) = x$$ ultimately implying that $f^2(x)=f^{-1}(x)$ . (Mini question: Do you know of any other functions $g(x)$ where $g \circ (g \circ g(x)) = g^3(x)=x$ aside from $f(x)$ and aside from the trivial case where $g(x)=x$ ? I was pretty shocked when I noticed this pattern with $f(x)$ .) Anyway, notice for every $x$ , there is a set of triplets generated by repeatedly applying the function $f(x)$ . Specifically $\langle x\rangle =\{x,f(x),f^{-1}(x)\}=\{x,\frac{1}{(1-x)},\frac{(x-1)}{x}\}$ For an illustrative example let $x=2$ , so then $\langle 2\rangle=\{2, -1, \frac{1}{2}\}$ . See now that this can be thought of as 3 points on the graph of the function $f(x)$ , where Point $A$ : $x \mapsto f(x)$ Point $B$ : $f(x) \mapsto f^{2}(x)=f^{-1}(x)$ Point $C$ : $f^{-1}(x) \mapsto x$ Explicitly, still using $x=2$ as the example: Point $A$ : $(x, f(x)) = (2,-1)$ Point $B$ : $(f(x), f^{-1}(x)) = (-1,\frac{1}{2})$ Point $C$ : $(f^{-1}(x),x) = (\frac{1}{2},2)$ OK so now my question! Since 3 points uniquely define a circle, I'd like to know if we can derive a closed-form function $r(x)$ that calculates the radius of circle $R$ , where circle $R$ is the circle uniquely defined by the 3 points $A$ , $B$ and $C$ generated by $\langle x\rangle$ . Continuing the example where $x=2$ , circle $R$ has center at Point $R=(\frac{3}{4},\frac{1}{4})$ (i.e. the circumcenter of points $A$ , $B$ and $C$ ). The radius of circle $R$ is then simply: $$|\overline{AR}|=\sqrt{{\left(2-\frac{3}{4}\right)}^2+{\left(-1-\frac{1}{4}\right)}^2}= \frac{5\sqrt{2}}{4}.$$ So evaluating $r(x)$ at $x=2$ gives us $r(2)=\dfrac{5\sqrt{2}}{4}\approx1.76777$ . Another cool example to consider is $x=\phi$ , where $\phi=\dfrac{1+\sqrt{5}}{2}\approx1.61803$ (the Golden Ratio). Some cool characteristics that make $\phi$ unique among all numbers are: $$\phi-1=\frac{1}{\phi}\quad\text{and}\quad \phi+1=\phi^2$$ You can calculate this on your own, but applying $f(x)$ on $x=\phi$ repeatedly results in $\langle\phi\rangle=\{\phi,-\phi,\frac{1}{\phi^{2}}\}$ . With the help of Wolfram Alpha, I was able to calculate $r(\phi)\approx1.93649$ (Circumcenter: https://tinyurl.com/y59trfn5 | Radius: https://tinyurl.com/y6jxs9sn ) Calculating the circumcenter seems to be the biggest issue, but maybe there's a cleaner way with the help of linear algebra? I was reading that there's a way to calculate the formula of a circle using matrices and determinants, but that seemed too complex for this. Maybe circles and triangles aren't the way to approach this at all -- I'd be happy to take suggestions and hear your thoughts! Just some last conceptual thoughts... 1) $r(x)$ should always be positive (i.e. there is no $x$ where $r(x)$ is $0$ or negative), and therefore somewhere hit some positive minimum value for $r(x)$ (assuming/implying that $r(x)$ is smooth and differentiable on the interval $x \in (-\infty,1)\cup(1,+\infty)$ ). 2) $\lim\limits_{x \to 1^-}r(x)=+\infty$ and $\lim\limits_{x \to 1^+}r(x)=+\infty$ 3) $\lim\limits_{x \to -\infty}r(x)=+\infty$ and $\lim\limits_{x \to +\infty}r(x)=+\infty$ 4) $r(x)$ is NOT symmetric around $x=1$ . Just as a quick check, $r(3)\approx2.12459$ and $r(-1)\approx1.76777$ 5) $r(x)$ is actual VERY NOISY as a function since for any 1 value of $r(x)$ , there are at least 3 unique variables that result in that value (i.e. all $x \in \langle x\rangle$ )(e.g. $r(2)=r(-1)=r(\frac{1}{2})\approx1.76777$ ) That last point makes me feel there's no true closed-form function for $r(x)$ . Regardless, I'd be really curious to find out what the minimum radius is... (placing \$1 on $r(x)$ for $x \in \langle\frac{\pi^2}{4}\rangle$ !)","I came across a weird property of the function Observe the following: ultimately implying that . (Mini question: Do you know of any other functions where aside from and aside from the trivial case where ? I was pretty shocked when I noticed this pattern with .) Anyway, notice for every , there is a set of triplets generated by repeatedly applying the function . Specifically For an illustrative example let , so then . See now that this can be thought of as 3 points on the graph of the function , where Point : Point : Point : Explicitly, still using as the example: Point : Point : Point : OK so now my question! Since 3 points uniquely define a circle, I'd like to know if we can derive a closed-form function that calculates the radius of circle , where circle is the circle uniquely defined by the 3 points , and generated by . Continuing the example where , circle has center at Point (i.e. the circumcenter of points , and ). The radius of circle is then simply: So evaluating at gives us . Another cool example to consider is , where (the Golden Ratio). Some cool characteristics that make unique among all numbers are: You can calculate this on your own, but applying on repeatedly results in . With the help of Wolfram Alpha, I was able to calculate (Circumcenter: https://tinyurl.com/y59trfn5 | Radius: https://tinyurl.com/y6jxs9sn ) Calculating the circumcenter seems to be the biggest issue, but maybe there's a cleaner way with the help of linear algebra? I was reading that there's a way to calculate the formula of a circle using matrices and determinants, but that seemed too complex for this. Maybe circles and triangles aren't the way to approach this at all -- I'd be happy to take suggestions and hear your thoughts! Just some last conceptual thoughts... 1) should always be positive (i.e. there is no where is or negative), and therefore somewhere hit some positive minimum value for (assuming/implying that is smooth and differentiable on the interval ). 2) and 3) and 4) is NOT symmetric around . Just as a quick check, and 5) is actual VERY NOISY as a function since for any 1 value of , there are at least 3 unique variables that result in that value (i.e. all )(e.g. ) That last point makes me feel there's no true closed-form function for . Regardless, I'd be really curious to find out what the minimum radius is... (placing \$1 on for !)","f(x)=\dfrac{1}{(1-x)} f(x) = \frac{1}{(1-x)}, \quad\quad f^2(x) = f(f(x)) = \frac{(x-1)}{x}, \quad\quad
f^3(x) = f(f(f(x))) = x f^2(x)=f^{-1}(x) g(x) g \circ (g \circ g(x)) = g^3(x)=x f(x) g(x)=x f(x) x f(x) \langle x\rangle =\{x,f(x),f^{-1}(x)\}=\{x,\frac{1}{(1-x)},\frac{(x-1)}{x}\} x=2 \langle 2\rangle=\{2, -1, \frac{1}{2}\} f(x) A x \mapsto f(x) B f(x) \mapsto f^{2}(x)=f^{-1}(x) C f^{-1}(x) \mapsto x x=2 A (x, f(x)) = (2,-1) B (f(x), f^{-1}(x)) = (-1,\frac{1}{2}) C (f^{-1}(x),x) = (\frac{1}{2},2) r(x) R R A B C \langle x\rangle x=2 R R=(\frac{3}{4},\frac{1}{4}) A B C R |\overline{AR}|=\sqrt{{\left(2-\frac{3}{4}\right)}^2+{\left(-1-\frac{1}{4}\right)}^2}= \frac{5\sqrt{2}}{4}. r(x) x=2 r(2)=\dfrac{5\sqrt{2}}{4}\approx1.76777 x=\phi \phi=\dfrac{1+\sqrt{5}}{2}\approx1.61803 \phi \phi-1=\frac{1}{\phi}\quad\text{and}\quad \phi+1=\phi^2 f(x) x=\phi \langle\phi\rangle=\{\phi,-\phi,\frac{1}{\phi^{2}}\} r(\phi)\approx1.93649 r(x) x r(x) 0 r(x) r(x) x \in (-\infty,1)\cup(1,+\infty) \lim\limits_{x \to 1^-}r(x)=+\infty \lim\limits_{x \to 1^+}r(x)=+\infty \lim\limits_{x \to -\infty}r(x)=+\infty \lim\limits_{x \to +\infty}r(x)=+\infty r(x) x=1 r(3)\approx2.12459 r(-1)\approx1.76777 r(x) r(x) x \in \langle x\rangle r(2)=r(-1)=r(\frac{1}{2})\approx1.76777 r(x) r(x) x \in \langle\frac{\pi^2}{4}\rangle","['linear-algebra', 'algebra-precalculus', 'geometry', 'numerical-methods', 'circles']"
33,Are inner product-preserving maps always linear?,Are inner product-preserving maps always linear?,,"Let $E,F$ be Pre-Hilbert spaces and $T: E \rightarrow F$ be a map that preserves the inner product, that is $\langle Tu , Tv \rangle = \langle u , v \rangle$ for all $u,v \in E$ . Must it be true that $T$ is linear? If $T$ is surjective one has $$\langle T(\lambda u+v), Tw\rangle = \langle \lambda u + v, w \rangle = \lambda \langle u, w \rangle + \langle v, w \rangle = \langle \lambda Tu, Tw \rangle + \langle Tv, Tw \rangle  \iff \langle T(\lambda u + v) - \lambda T u - Tv, Tw\rangle = 0$$ Now since $T$ is surjective one can choose $Tw$ to be $ T(\lambda u + v) - \lambda T u + Tv$ , and by positive definiteness the linearity follows. Can this somehow be extended if $T$ isn't surjective?","Let be Pre-Hilbert spaces and be a map that preserves the inner product, that is for all . Must it be true that is linear? If is surjective one has Now since is surjective one can choose to be , and by positive definiteness the linearity follows. Can this somehow be extended if isn't surjective?","E,F T: E \rightarrow F \langle Tu , Tv \rangle = \langle u , v \rangle u,v \in E T T \langle T(\lambda u+v), Tw\rangle = \langle \lambda u + v, w \rangle = \lambda \langle u, w \rangle + \langle v, w \rangle = \langle \lambda Tu, Tw \rangle + \langle Tv, Tw \rangle  \iff \langle T(\lambda u + v) - \lambda T u - Tv, Tw\rangle = 0 T Tw  T(\lambda u + v) - \lambda T u + Tv T","['linear-algebra', 'functional-analysis', 'linear-transformations', 'isometry']"
34,Cayley-Hamilton...,Cayley-Hamilton...,,"Say $A$ is a square matrix over an algebraically closed field. Say $m$ is the minimal polynomial and $p$ is the characteristic polynomial. Of course C-H implies that $m|p$ . Conversely, if we can show $m|p$ then C-H follows; the question is whether one can give a ""simple"", ""elementary"" or ""straightforward"" proof that $m|p$ . Note. What I really want is a proof such that I feel I actually understand the whole thing. Hence in particular no Jordan form allowed. Edit. An Answer has appeared that shows $m|p$ in a very simple way - simply demolishes what I wrote below. Edit. When I posted this is was an honest question that I didn't know the answer to. I think I got it; if anyone wants to say they believe the argument below (or not) that would be great. First, it's clear that linear factors of $m$ must divide $p$ : If $m(\lambda)=0$ then $p(\lambda)=0$ . Because $m(t)=(t-\lambda)r(t)$ , so $(A-\lambda)r(A)=0$ . Minimality of $m$ shows that $r(A)\ne0$ , hence $A-\lambda$ is not invertible, hence $p(\lambda)=0$ . If we could show that $(t-\lambda)^k|m$ implies $(t-\lambda)^k|p$ we'd be set. Some possible progress on that, first restricted to a simple special case: If $t^2|m(t)$ then $\dim(\ker(A^2))\ge 2$ . Proof: Say $X=K^n$ is the underlying vector space. Say $m(t)=t^2q(t)$ .  Let $$Y=q(A)X,$$ $$B=A|_Y.$$ Then $Y\subset\ker(A^2)$ . Say $d=\dim(Y)$ . Now $B^2=0$ , and it follows easily that $B^d=0$ . But $B\ne0$ , hence $d\ge2$ . Similarly If $(t-\lambda)^k|m$ then $\dim(\ker(A-\lambda)^k)\ge k$ . So we only need If $\dim(\ker(A-\lambda)^k)\ge k$ then $(t-\lambda)^k|p$ . Which I gather is true, but only by hearsay; I'm sort of missing what it ""really means"" to say $t^2|p$ . Wait , I think I got it. Say $$m(t)=(t-\lambda)^kq(t),$$ $$q(\lambda)\ne0.$$ The ""kernel lemma"" shows that $$X=\ker((A-\lambda)^k)\oplus\ker(q(A))=X_1\oplus X_2.$$ Each $X_j$ is $A$ -invariant, so we can define $$B_j=A|_{X_j}.$$ Since similar matrices have the same determinant we can use any basis we like  in calculating the determinant $p(t)$ ; if we use a basis compatible with the decomposition $X=X_1\oplus X_2$ it's clear that $$p_A=p_{B_1}p_{B_2},$$ so we need only show that $$p_{{B_1}}(t)=(t-\lambda)^k.$$ In fact it's actually enough to show $(t-\lambda)^k|p_{B_1}$ ,  and that's clear: Lemma. If $B$ is a $d\times d$ nilpotent matrix then $p_B(t)=t^d$ . Proof: We're still assuming $K$ is algebraically closed; $B$ cannot have a non-zero  eigenvalue. So if $d=\dim(\ker((A-\lambda)^k)$ then $$p_{B_1}(t)=(t-\lambda)^d;$$ we've already shown that $d\ge k$ , so $(t-\lambda)^k|p$ . Hmm. Maybe that doesn't look all that simple. It's nonetheless the sort of thing I wanted, because I can give a one-line summary making it at least comprehensible : One-line summary: Since $m$ splits, the kernel lemma (a simple consequence of the fact that $K[t]$ is a PID) shows that $A$ is the direct sum of operators $B_j$ such that $B_j-\lambda_j$ is nilpotent. So it's enough to prove C-H for nilpotent operators, which is not hard .","Say is a square matrix over an algebraically closed field. Say is the minimal polynomial and is the characteristic polynomial. Of course C-H implies that . Conversely, if we can show then C-H follows; the question is whether one can give a ""simple"", ""elementary"" or ""straightforward"" proof that . Note. What I really want is a proof such that I feel I actually understand the whole thing. Hence in particular no Jordan form allowed. Edit. An Answer has appeared that shows in a very simple way - simply demolishes what I wrote below. Edit. When I posted this is was an honest question that I didn't know the answer to. I think I got it; if anyone wants to say they believe the argument below (or not) that would be great. First, it's clear that linear factors of must divide : If then . Because , so . Minimality of shows that , hence is not invertible, hence . If we could show that implies we'd be set. Some possible progress on that, first restricted to a simple special case: If then . Proof: Say is the underlying vector space. Say .  Let Then . Say . Now , and it follows easily that . But , hence . Similarly If then . So we only need If then . Which I gather is true, but only by hearsay; I'm sort of missing what it ""really means"" to say . Wait , I think I got it. Say The ""kernel lemma"" shows that Each is -invariant, so we can define Since similar matrices have the same determinant we can use any basis we like  in calculating the determinant ; if we use a basis compatible with the decomposition it's clear that so we need only show that In fact it's actually enough to show ,  and that's clear: Lemma. If is a nilpotent matrix then . Proof: We're still assuming is algebraically closed; cannot have a non-zero  eigenvalue. So if then we've already shown that , so . Hmm. Maybe that doesn't look all that simple. It's nonetheless the sort of thing I wanted, because I can give a one-line summary making it at least comprehensible : One-line summary: Since splits, the kernel lemma (a simple consequence of the fact that is a PID) shows that is the direct sum of operators such that is nilpotent. So it's enough to prove C-H for nilpotent operators, which is not hard .","A m p m|p m|p m|p m|p m p m(\lambda)=0 p(\lambda)=0 m(t)=(t-\lambda)r(t) (A-\lambda)r(A)=0 m r(A)\ne0 A-\lambda p(\lambda)=0 (t-\lambda)^k|m (t-\lambda)^k|p t^2|m(t) \dim(\ker(A^2))\ge 2 X=K^n m(t)=t^2q(t) Y=q(A)X, B=A|_Y. Y\subset\ker(A^2) d=\dim(Y) B^2=0 B^d=0 B\ne0 d\ge2 (t-\lambda)^k|m \dim(\ker(A-\lambda)^k)\ge k \dim(\ker(A-\lambda)^k)\ge k (t-\lambda)^k|p t^2|p m(t)=(t-\lambda)^kq(t), q(\lambda)\ne0. X=\ker((A-\lambda)^k)\oplus\ker(q(A))=X_1\oplus X_2. X_j A B_j=A|_{X_j}. p(t) X=X_1\oplus X_2 p_A=p_{B_1}p_{B_2}, p_{{B_1}}(t)=(t-\lambda)^k. (t-\lambda)^k|p_{B_1} B d\times d p_B(t)=t^d K B d=\dim(\ker((A-\lambda)^k) p_{B_1}(t)=(t-\lambda)^d; d\ge k (t-\lambda)^k|p m K[t] A B_j B_j-\lambda_j",['linear-algebra']
35,Connection between rank and positive definiteness,Connection between rank and positive definiteness,,"I would like to know, is there a connection between the rank of a matrix and whether it is positive definite? Specifically, if I can prove that a matrix is not full rank, then can I say that it is not positive definite? If so, why? Thanks a lot for your help.","I would like to know, is there a connection between the rank of a matrix and whether it is positive definite? Specifically, if I can prove that a matrix is not full rank, then can I say that it is not positive definite? If so, why? Thanks a lot for your help.",,"['linear-algebra', 'matrices', 'matrix-rank', 'positive-definite']"
36,Detecting symmetric matrices of the form (low-rank + diagonal matrix),Detecting symmetric matrices of the form (low-rank + diagonal matrix),,"Let $\Sigma$ be a symmetric positive definite matrix of dimensions $n \times n$. Is there a numerically robust way of checking whether it can be decomposed as $\Sigma = \mathcal{D} + v^t.v$ where $v$ has dimensions $r \times n$ with $r < n-1$ and $\mathcal{D}$ is diagonal with positive elements? So far, given $\Sigma$, I am checking for minimal $k$ for which positive solutions for diagonal matrix of $\mathrm{rank}(\Sigma -\mathcal{D}) = k$ exist. I have to add, that even when the solution does exist, it may be non-unique. Example: $$ \Sigma = \left( \begin{array}{cccc}  6 & 6 & 7 & 0 \\  6 & 11 & 12 & -3 \\  7 & 12 & 20 & -6 \\  0 & -3 & -6 & 9 \\ \end{array}  \right) \, \text{ where } \mathcal{D} = \left( \begin{array}{cccc}  1 & 0 & 0 & 0 \\  0 & 2 & 0 & 0 \\  0 & 0 & 3 & 0 \\  0 & 0 & 0 & 4 \\ \end{array} \right) \text{  and  } v^t = \left( \begin{array}{cc}  2 & 1 \\  3 & 0 \\  4 & -1 \\  -1 & 2 \\ \end{array} \right) $$ $\Sigma$ admits a different representation as well, in fact it is one of 1-parametric family: $$  \mathcal{D} = \mathrm{diag}\left(\frac{31}{39}, \frac{403}{203}, \frac{79}{29}, \frac{84}{19} \right) \text{ and } v^t = \left( \begin{array}{cc}  0 & 679 \\  -78 & 696 \\  -156 & 812 \\  97 & 0 \\ \end{array} \right) \cdot \left( \begin{array}{cc}  \sqrt{2522} & 0 \\  0 & 2 \sqrt{19691} \\ \end{array} \right)^{-1} $$ I have two questions. Why is there a parametric family of solutions ? Can a solution be found by methods of linear algebra ? Thank you","Let $\Sigma$ be a symmetric positive definite matrix of dimensions $n \times n$. Is there a numerically robust way of checking whether it can be decomposed as $\Sigma = \mathcal{D} + v^t.v$ where $v$ has dimensions $r \times n$ with $r < n-1$ and $\mathcal{D}$ is diagonal with positive elements? So far, given $\Sigma$, I am checking for minimal $k$ for which positive solutions for diagonal matrix of $\mathrm{rank}(\Sigma -\mathcal{D}) = k$ exist. I have to add, that even when the solution does exist, it may be non-unique. Example: $$ \Sigma = \left( \begin{array}{cccc}  6 & 6 & 7 & 0 \\  6 & 11 & 12 & -3 \\  7 & 12 & 20 & -6 \\  0 & -3 & -6 & 9 \\ \end{array}  \right) \, \text{ where } \mathcal{D} = \left( \begin{array}{cccc}  1 & 0 & 0 & 0 \\  0 & 2 & 0 & 0 \\  0 & 0 & 3 & 0 \\  0 & 0 & 0 & 4 \\ \end{array} \right) \text{  and  } v^t = \left( \begin{array}{cc}  2 & 1 \\  3 & 0 \\  4 & -1 \\  -1 & 2 \\ \end{array} \right) $$ $\Sigma$ admits a different representation as well, in fact it is one of 1-parametric family: $$  \mathcal{D} = \mathrm{diag}\left(\frac{31}{39}, \frac{403}{203}, \frac{79}{29}, \frac{84}{19} \right) \text{ and } v^t = \left( \begin{array}{cc}  0 & 679 \\  -78 & 696 \\  -156 & 812 \\  97 & 0 \\ \end{array} \right) \cdot \left( \begin{array}{cc}  \sqrt{2522} & 0 \\  0 & 2 \sqrt{19691} \\ \end{array} \right)^{-1} $$ I have two questions. Why is there a parametric family of solutions ? Can a solution be found by methods of linear algebra ? Thank you",,"['linear-algebra', 'matrices']"
37,Is there a fundamental problem with extending matrix concepts to tensors?,Is there a fundamental problem with extending matrix concepts to tensors?,,"We are familiar with the theory of matrices, more specifically their eigen-theorems and associated decompositions. Indeed singular value decomposition generalizes the spectral theorem for arbitrary matrices, not just square ones. Now it only seems natural to extend this idea of 2 dimensional array of numbers to higher dimensions, i.e. tensors. But as soon as we do this, everything breaks down. For example even the notion of rank of matrix (which we all agree to be minimum of either column rank or row rank for a matrix) seems to be conflated when it comes to tensors. The Wikipedia page seems to use degree , order and rank of a tensor synonymously (understandably due to different terminology used in different fields, but somewhat annoying nevertheless). ""The order (also degree or rank) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array."" Also for example, the very familiar concept of eigenvalues or vectors also flies out the window (though people have defined them for super-symmetric tensors).  So my question is this: What is the fundamental reason why the ""nice"" theorems we have in the matrix case do not extend to the case of tensors? I can think of a couple: Tensors exhibit the phenomenon of rank jumping as well as field dependence ; which would imply the usual rules of analysis need to be re-examined when dealing with them. A large class of matrices are groups, so tools from abstract algebra are available to deal with them. Matrices can be viewed as operators from one space to another unambiguously, whereas viewing a tensor as an operator between spaces can get confusing very quickly. I know there are extensions to SVD for tensors, for example Tucker decomposition , HOSVD, etc; so I am not claiming it can't be done. I also understand (somewhat) that mathematicians prefer to study tensors abstractly or using differential geometry and forms. I am just curious as to why the results generalizing the concepts form the matrix case are many ; what is the underlying cause for a lack of unifying framework. The above reasons seem valid roadblocks, but do they hint at something more fundamental?","We are familiar with the theory of matrices, more specifically their eigen-theorems and associated decompositions. Indeed singular value decomposition generalizes the spectral theorem for arbitrary matrices, not just square ones. Now it only seems natural to extend this idea of 2 dimensional array of numbers to higher dimensions, i.e. tensors. But as soon as we do this, everything breaks down. For example even the notion of rank of matrix (which we all agree to be minimum of either column rank or row rank for a matrix) seems to be conflated when it comes to tensors. The Wikipedia page seems to use degree , order and rank of a tensor synonymously (understandably due to different terminology used in different fields, but somewhat annoying nevertheless). ""The order (also degree or rank) of a tensor is the dimensionality of the array needed to represent it, or equivalently, the number of indices needed to label a component of that array."" Also for example, the very familiar concept of eigenvalues or vectors also flies out the window (though people have defined them for super-symmetric tensors).  So my question is this: What is the fundamental reason why the ""nice"" theorems we have in the matrix case do not extend to the case of tensors? I can think of a couple: Tensors exhibit the phenomenon of rank jumping as well as field dependence ; which would imply the usual rules of analysis need to be re-examined when dealing with them. A large class of matrices are groups, so tools from abstract algebra are available to deal with them. Matrices can be viewed as operators from one space to another unambiguously, whereas viewing a tensor as an operator between spaces can get confusing very quickly. I know there are extensions to SVD for tensors, for example Tucker decomposition , HOSVD, etc; so I am not claiming it can't be done. I also understand (somewhat) that mathematicians prefer to study tensors abstractly or using differential geometry and forms. I am just curious as to why the results generalizing the concepts form the matrix case are many ; what is the underlying cause for a lack of unifying framework. The above reasons seem valid roadblocks, but do they hint at something more fundamental?",,"['linear-algebra', 'matrices', 'tensors']"
38,Is the least squares solution to an overdetermined system a triangle center?,Is the least squares solution to an overdetermined system a triangle center?,,"My question relates to a simple case of the least squares problem. For example, take the system of equations  \begin{align*} 2x- y &= 2, \\ x + 2y &= 1, \\ x+y &= 4, \end{align*} which is clearly overdetermined. Using a least squares approach, we can solve the system $$A^TA\mathbf{x^*} = A^T\mathbf{b},$$ where $\mathbf{x^*}$ minimises $\|\ A\mathbf{x} -\mathbf{b} \ \|$, and  $$A = \begin{bmatrix} 2 & -1\\ 1 & 2 \\ 1 & 1 \end{bmatrix} \ , \quad  \mathbf{b}=  \begin{bmatrix} 2 \\ 1 \\ 4 \end{bmatrix} \quad \mbox{and} \quad \mathbf{x}=  \begin{bmatrix} x \\ y \end{bmatrix},$$ to obtain an approximation for the solution as $\left(\frac{10}{7}, \frac{3}{7} \right)$. If you plot the three straight lines that constitute the system of equations, they intersect at three points that form the vertices of a triangle. My question is - does the solution always fall within this triangle (as in this example), and if so, is this point a (non-traditional) triangle center?","My question relates to a simple case of the least squares problem. For example, take the system of equations  \begin{align*} 2x- y &= 2, \\ x + 2y &= 1, \\ x+y &= 4, \end{align*} which is clearly overdetermined. Using a least squares approach, we can solve the system $$A^TA\mathbf{x^*} = A^T\mathbf{b},$$ where $\mathbf{x^*}$ minimises $\|\ A\mathbf{x} -\mathbf{b} \ \|$, and  $$A = \begin{bmatrix} 2 & -1\\ 1 & 2 \\ 1 & 1 \end{bmatrix} \ , \quad  \mathbf{b}=  \begin{bmatrix} 2 \\ 1 \\ 4 \end{bmatrix} \quad \mbox{and} \quad \mathbf{x}=  \begin{bmatrix} x \\ y \end{bmatrix},$$ to obtain an approximation for the solution as $\left(\frac{10}{7}, \frac{3}{7} \right)$. If you plot the three straight lines that constitute the system of equations, they intersect at three points that form the vertices of a triangle. My question is - does the solution always fall within this triangle (as in this example), and if so, is this point a (non-traditional) triangle center?",,"['linear-algebra', 'geometry']"
39,Characteristic Polynomial of Restriction to Invariant Subspace Divides Characteristic Polynomial,Characteristic Polynomial of Restriction to Invariant Subspace Divides Characteristic Polynomial,,"I am interested in finding a proof of the following property that does not make reference to bases, and ideally doesn't use facts about determinants that depend on the block structure of a matrix. Let $T \in L(V,V)$ be a linear operator on a finite-dimensional space $V$.  Suppose $W \preccurlyeq V$ is a $T$-invariant subspace, that is, $T(W) \subset W$.  Consider the restriction $T_W \in L(W,W)$ of $T$ to $W$.  Then the characteristic polynomial of $T_W$ divides the characteristic polynomial of $T$. Let $p,p_W$ be the characteristic polynomials and $m,m_W$ be the minimal polynomials.  It is easy to show ""algebraically"" that $m_W \mid m$ since $m$ annihilates $T_W$, so must be a multiple of the monic generator $m_W$.  However, the only proofs I have seen that $p_W \mid p$ make use of basis expansions: Let $\mathcal{B}=\{ v_1,\dots,v_n \}$ be a basis for $V$ such that $\mathcal{B}'=\{ v_1, \dots, v_r \}$ form a basis for $W$. The matrix of $T$ with respect to $\mathcal{B}$ has the following block form, where $A \in F^{r \times r}$ is the matrix of $T_W$ with respect to $\mathcal{B'}$, $$[T]_{\mathcal{B}} = \begin{bmatrix} A & B \\ & C \end{bmatrix} \implies xI - [T]_{\mathcal{B}} = \begin{bmatrix} xI - A & B \\ & xI-C \end{bmatrix}$$ Then $p = \det(xI - [T]_\mathcal{B}) = \det(xI-A)\det(xI-C)$ is a multiple of $p_W = \det(xI-A)$. The use of basis expansions and block matrices leaves something to be desired. Is there a ""matrix-free"" way to prove this? Assume we know about Cayley-Hamilton, if it helps.","I am interested in finding a proof of the following property that does not make reference to bases, and ideally doesn't use facts about determinants that depend on the block structure of a matrix. Let $T \in L(V,V)$ be a linear operator on a finite-dimensional space $V$.  Suppose $W \preccurlyeq V$ is a $T$-invariant subspace, that is, $T(W) \subset W$.  Consider the restriction $T_W \in L(W,W)$ of $T$ to $W$.  Then the characteristic polynomial of $T_W$ divides the characteristic polynomial of $T$. Let $p,p_W$ be the characteristic polynomials and $m,m_W$ be the minimal polynomials.  It is easy to show ""algebraically"" that $m_W \mid m$ since $m$ annihilates $T_W$, so must be a multiple of the monic generator $m_W$.  However, the only proofs I have seen that $p_W \mid p$ make use of basis expansions: Let $\mathcal{B}=\{ v_1,\dots,v_n \}$ be a basis for $V$ such that $\mathcal{B}'=\{ v_1, \dots, v_r \}$ form a basis for $W$. The matrix of $T$ with respect to $\mathcal{B}$ has the following block form, where $A \in F^{r \times r}$ is the matrix of $T_W$ with respect to $\mathcal{B'}$, $$[T]_{\mathcal{B}} = \begin{bmatrix} A & B \\ & C \end{bmatrix} \implies xI - [T]_{\mathcal{B}} = \begin{bmatrix} xI - A & B \\ & xI-C \end{bmatrix}$$ Then $p = \det(xI - [T]_\mathcal{B}) = \det(xI-A)\det(xI-C)$ is a multiple of $p_W = \det(xI-A)$. The use of basis expansions and block matrices leaves something to be desired. Is there a ""matrix-free"" way to prove this? Assume we know about Cayley-Hamilton, if it helps.",,"['linear-algebra', 'matrices', 'determinant', 'alternative-proof', 'minimal-polynomials']"
40,Introduction to Proof via Linear Algebra,Introduction to Proof via Linear Algebra,,"Many universities offer a transition course from computational courses like Calculus to proof-oriented courses like Abstract Algebra. Such courses often go by a name like ""Introduction to Proof"" or ""Transition to Higher Mathematics"". They typically contain an introduction to first-order logic (conditionals, conjunctions, negations, quantifiers, etc.) as well as various methods of proof (contradiction, induction, etc.). I'm hoping to find a text for a first course in linear algebra that fills the role of a ""transition course"" by deliberately incorporating first-order logic and proof techniques as part of the instruction. The text should be accessible to students with two semesters of Calculus (roughly the basics of single-variable differentiation, integration, and infinite series). In particular, the overwhelming majority of students will have never written a formal proof and will have extremely limited exposure to logic and set theory. Ideally, the author would discuss these topics just as they are needed in the treatment of linear algebra (as opposed to supposing the reader is familiar with them already). For example, the author might have a digression on proof by contradiction just prior to using it in some proof about linear independence. Less ideal (but still acceptable) would be a text that at the very least makes use of all the relevant ideas from first-order logic and proof techniques that one expects from a transition course. Hopefully, the progression of such a text would be such that the instructor could use a supplemental text to discuss, say, proof by contradiction just as it is about to make its first appearance in the text.","Many universities offer a transition course from computational courses like Calculus to proof-oriented courses like Abstract Algebra. Such courses often go by a name like ""Introduction to Proof"" or ""Transition to Higher Mathematics"". They typically contain an introduction to first-order logic (conditionals, conjunctions, negations, quantifiers, etc.) as well as various methods of proof (contradiction, induction, etc.). I'm hoping to find a text for a first course in linear algebra that fills the role of a ""transition course"" by deliberately incorporating first-order logic and proof techniques as part of the instruction. The text should be accessible to students with two semesters of Calculus (roughly the basics of single-variable differentiation, integration, and infinite series). In particular, the overwhelming majority of students will have never written a formal proof and will have extremely limited exposure to logic and set theory. Ideally, the author would discuss these topics just as they are needed in the treatment of linear algebra (as opposed to supposing the reader is familiar with them already). For example, the author might have a digression on proof by contradiction just prior to using it in some proof about linear independence. Less ideal (but still acceptable) would be a text that at the very least makes use of all the relevant ideas from first-order logic and proof techniques that one expects from a transition course. Hopefully, the progression of such a text would be such that the instructor could use a supplemental text to discuss, say, proof by contradiction just as it is about to make its first appearance in the text.",,"['linear-algebra', 'reference-request', 'proof-writing', 'education', 'book-recommendation']"
41,How many matrices with integer eigenvalues are there?,How many matrices with integer eigenvalues are there?,,"Let $m,n \in \mathbb N$. How many $m \times m$ matrices with integer entries from $-n$ to $n$ have the property that all eigenvalues (possibly multiple) are integers? The following table calculated with PARI shows the values for $m = 2$ and $n = 1,\dots,20$: 1  55  2  317  3  963  4  2301  5  4315  6  7793  7  12047  8  18449  9  26527  10  37325  11  48683  12  66149  13  82547  14  104713  15  131247  16  162297  17  191599  18  233813  19  270939  20  324045 For $m = 3$, I only know the values for $n = 1,2$: 1 6417   2 260353   3 2570569 Brute force method is soon not feasible. A formula depending on $m$ and $n$ would be nice.","Let $m,n \in \mathbb N$. How many $m \times m$ matrices with integer entries from $-n$ to $n$ have the property that all eigenvalues (possibly multiple) are integers? The following table calculated with PARI shows the values for $m = 2$ and $n = 1,\dots,20$: 1  55  2  317  3  963  4  2301  5  4315  6  7793  7  12047  8  18449  9  26527  10  37325  11  48683  12  66149  13  82547  14  104713  15  131247  16  162297  17  191599  18  233813  19  270939  20  324045 For $m = 3$, I only know the values for $n = 1,2$: 1 6417   2 260353   3 2570569 Brute force method is soon not feasible. A formula depending on $m$ and $n$ would be nice.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
42,Using the Arnoldi Iteration to find the k largest eigenvalues of a matrix,Using the Arnoldi Iteration to find the k largest eigenvalues of a matrix,,"I'm trying to obtain a general understanding of this algorithm which determines the k-largest eigenvalues of a matrix  $A\in \mathbb{R}^{n\times n}$. How I see it: power iteration : take random starting vector $b \in \mathbb{R}^{1\times n}$ find $K_{n} = \begin{bmatrix}b & Ab & A^{2}b & \cdots & A^{n-1}b \end{bmatrix}.$ find orthogonal basis $Q_n$ of $K_{n}$ using Gramm-Schmidt (Numerically unstable) n-th column vector of $Q_n$ is an approximation of n-th eigenvector of $A$ and corresponds to n-th largest eigenvalue $\lambda_n$ of $A$ Arnoldi Iteration : Is a numerically stable implementation of power iteration. take random starting vector $b \in \mathbb{R}^{1\times n}$ find first $q_1..q_n$ arnoldi vectors to form $Q_n$ $Q_n$ is an orthonormal basis of $K_n$ numerically stable Gramm-schmidt process is used determine Hessenberg Matrix $H_n=Q_n^*AQ_n$ solve eig($H_n$) which is now simple because $H_n$ is a Hessenberg matrix, upper triangular, we can use the QR algorithm Is this the general gist of it? A good, reliable link would already be great. Any help would be greatly appreciated. Thank you.","I'm trying to obtain a general understanding of this algorithm which determines the k-largest eigenvalues of a matrix  $A\in \mathbb{R}^{n\times n}$. How I see it: power iteration : take random starting vector $b \in \mathbb{R}^{1\times n}$ find $K_{n} = \begin{bmatrix}b & Ab & A^{2}b & \cdots & A^{n-1}b \end{bmatrix}.$ find orthogonal basis $Q_n$ of $K_{n}$ using Gramm-Schmidt (Numerically unstable) n-th column vector of $Q_n$ is an approximation of n-th eigenvector of $A$ and corresponds to n-th largest eigenvalue $\lambda_n$ of $A$ Arnoldi Iteration : Is a numerically stable implementation of power iteration. take random starting vector $b \in \mathbb{R}^{1\times n}$ find first $q_1..q_n$ arnoldi vectors to form $Q_n$ $Q_n$ is an orthonormal basis of $K_n$ numerically stable Gramm-schmidt process is used determine Hessenberg Matrix $H_n=Q_n^*AQ_n$ solve eig($H_n$) which is now simple because $H_n$ is a Hessenberg matrix, upper triangular, we can use the QR algorithm Is this the general gist of it? A good, reliable link would already be great. Any help would be greatly appreciated. Thank you.",,"['linear-algebra', 'matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
43,Least-squares solution to system of equations of $4 \times 4$ matrices with $2$ unknown matrices,Least-squares solution to system of equations of  matrices with  unknown matrices,4 \times 4 2,"This question is in the context of a robotics problem. The goal is to track a robot using both its onboard odometry system and a VR system (HTC Vive Pro) using a VR controller mounted to the robot. What is known is the transformation between odometry origin and the robot (measurements $A_n$ ) and between the VR origin and the controller (measurements $C_n$ ). Ignoring both systems' inaccuracies for now, we can also assume the robot's pose according to both systems is identical ( $I$ ). Driving around will result in many pairs of measurements ( $A_n$ , $C_n$ ). What is unknown is the fixed transformation $B$ between the two coordinate systems and the fixed transformation $D$ between the robot's origin and the VR controller. All transformations involved are proper rigid. The chain of transformation looks as follows. This leaves us with an equation system of $4 \times 4$ homogeneous transformation matrices $$A_n \cdot B \cdot C_n \cdot D = I$$ with $I$ being the $4 \times 4$ identity matrix and $(A_n, C_n)$ being (many) pairs of measurements. I am looking for the optimal (least-squares, I suppose) solution for $B$ and $D$ , so that the equation holds approximately true for all pairs of measurements.","This question is in the context of a robotics problem. The goal is to track a robot using both its onboard odometry system and a VR system (HTC Vive Pro) using a VR controller mounted to the robot. What is known is the transformation between odometry origin and the robot (measurements ) and between the VR origin and the controller (measurements ). Ignoring both systems' inaccuracies for now, we can also assume the robot's pose according to both systems is identical ( ). Driving around will result in many pairs of measurements ( , ). What is unknown is the fixed transformation between the two coordinate systems and the fixed transformation between the robot's origin and the VR controller. All transformations involved are proper rigid. The chain of transformation looks as follows. This leaves us with an equation system of homogeneous transformation matrices with being the identity matrix and being (many) pairs of measurements. I am looking for the optimal (least-squares, I suppose) solution for and , so that the equation holds approximately true for all pairs of measurements.","A_n C_n I A_n C_n B D 4 \times 4 A_n \cdot B \cdot C_n \cdot D = I I 4 \times 4 (A_n, C_n) B D","['linear-algebra', 'matrices', 'optimization', 'least-squares', 'robotics']"
44,Why are there multiple Jordan Blocks corresponding to the same eigenvalue?,Why are there multiple Jordan Blocks corresponding to the same eigenvalue?,,"Though the title seems clear enough, I'd like to start with a discussion of how I personally came to derive the Jordan Normal Form, because my question is very specific to the details of my derivation. Notation To start, let $X$ be a finite dimensional vector space, $L(X)$ be the space of linear operators on $X$, and $A\in L(X)$. Let $\sigma(A) = \{\lambda_1,\ \cdots,\ \lambda_k\}$ be the spectrum of $A$. Now, we define $d(\lambda)$ to be the geometric multiplicity of $\lambda$ $m(\lambda)$ to be the algebraic multiplicity of $\lambda$ Next, we denote the $k$th generalized eigenspace of $\lambda$ by $$ \text{N}_k(\lambda) = \text{Ker}(A-\lambda I)^k $$ and finally, we let $$ \text{N}(\lambda) = N_{n(\lambda)}(\lambda)\qquad n(\lambda)=\min\{k\in\mathbb{N}\ |\ \text{N}_k(\lambda)=N_{k+1}(\lambda)\} $$ we note that it can be shown that $n(\lambda) = m(\lambda)$, and so the notation $n(\lambda)$ won't really be used. We will also let $\sum_\lambda$, $\prod_\lambda$, etc. represent the sum/product/etc. over distinct eigenvalues of $A$. Fundamentals First off, it is known that we can decompose $X$ as $$ X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k) $$ Hence $\sum_{\lambda} \dim\ \text{N}(\lambda) = \dim X$. Also, from the characteristic polynomial of $A$, the sum of the algebraic multiplicities of the eigenvalues must equal the degree of the polynomial, which is $\dim X$. Thus $$ \sum_\lambda\dim\ \text{N}(\lambda) = \sum_\lambda m(\lambda) = \dim X $$ Going in a different direction, we present the following theorem: Theorem: If $B\in L(X)$ is nilpotent of order $n$, and $S\subset X\backslash\text{Ker} B^{n-1}$ is linearly independent, then   $$ \bigcup_{x\in S}\{x,\ Bx,\ B^2x,\ \cdots,\ B^{n-1}x\} $$   is linearly independent. Proof: We will show the case for $|S|=2$, and the general case follows the same format. Suppose $S = \{x,\ y\}$, and $$ \sum_{k=0}^{n-1} a_k B^kx_1 + \sum_{k=0}^{n-1}b_k B^kx_2 = 0 $$ applying $B^{n-1}$ to both sides gives $$ B^{n-1}\left(\sum_{k=0}^{n-1}a_kB^kx_1+b_kB^kx_2\right) = a_0B^{n-1}x_1+b_0B^{n-1}x_2 = B^{n-1}(a_0x_1+b_0x_2) = 0 $$ so $a_0x_1 + b_0x_2\in\text{Ker}B^{n-1}$. However, since $\text{Ker}B^{n-1}$ is a subspace of $X$, we can decompose $X$ as $X = \text{Ker}B^{n-1}\oplus Z$ for some vector space $Z$, for which $\{x_1,\ x_2\}\subset Z\backslash\{0\}$. Since $Z$ is a subspace, $a_0x_1+b_0x_2\in Z$. To say that $a_0x_1+b_0x_2\in \text{Ker}B^{n-1}\cap Z$ is equivalent to saying $a_0x_1+b_0x_2 = 0$. By linear independence of $S$, $a_0=b_0=0$. This process can be repeated to get $a_j=b_j=0$ for all $j$. $\blacksquare$ Now, take $x\in \text{N}(\lambda)\backslash \text{N}_{m(\lambda)-1}(\lambda)$. Note that $B_\lambda = (A - \lambda I)|_{\text{N}(\lambda)}$ (that is, $A - \lambda I$ restricted to $\text{N}(\lambda)$) is nilpotent of order $m(\lambda)$. Hence $\{x,\ B_\lambda x,\ \cdots,\ B_\lambda^{m(\lambda)-1}x\}$ is linearly independent, and it's span is a subspace of $\text{N}(\lambda)$. Hence $\dim \text{N}(\lambda) \ge m(\lambda)$. If we suppose that $\dim\text{N}(\lambda) > m(\lambda)$ for at least one $\lambda\in\sigma(A)$, then we contradict the fact that $\sum_\lambda\dim\text{N}(\lambda) = \dim X$, and so we conclude that $m(\lambda) = \dim\text{N}(\lambda)$. Alright, so far so good I hope... Jordan Normal Form By the above arguments, we conclude that $\text{Span}\{x,\ \cdots,\ B^{m(\lambda)-1}x\} = \text{N}(\lambda)$. Hence, if we let $e_0(\lambda)\in N(\lambda)\backslash N_{m(\lambda)-1}(\lambda)$, and $e_k(\lambda)=(A-\lambda I)^k e_0(\lambda)$, then $$ \text{Span}\left(\bigcup_{\lambda}\bigcup_{k=0}^{m(\lambda)-1}\{e_k(\lambda)\}\right) = X $$ Since $X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k)$, and each $\text{N}(\lambda_k)$ is $A$-invariant (that is $A(\text{N}(\lambda_k))\subseteq \text{N}(\lambda_k)$), it follows that if we have bases for each $N(\lambda_i)$, then we can get the following matrix representation of $A$ wrt the union of these bases: $$ A = \left[\begin{matrix} A|_{\text{N}(\lambda_1)} & O & \cdots & \vdots \\ O & A|_{\text{N}(\lambda_2)} & \cdots & \vdots \\ \vdots & \vdots & \ddots & \vdots \\ \cdots & \cdots & \cdots & A|_{\text{N}(\lambda_k)} \end{matrix}\right] $$ where $A|_{\text{N}(\lambda_i)}$ is the matrix representation of $A$ restricted to $\text{N}(\lambda_i)$ wrt the basis of $\text{N}(\lambda_i)$. Above, we demonstrated that $\{e_{m(\lambda)-1}(\lambda),\ \cdots,\ e_1(\lambda)\}$ is a basis for $\text{N}(\lambda)$. We can find a matrix representation for $A|_{\text{N}(\lambda_i)}$ by noting that $$ Ae_k(\lambda) = A(A-\lambda I)^ke_1(\lambda) = (A-\lambda I)^{k+1}e_1(\lambda) + \lambda(A-\lambda I)^ke_1(\lambda) \\ Ae_k(\lambda) = e_{k+1}(\lambda)+\lambda e_k(\lambda) \\ Ae_{m(\lambda)-1}(\lambda) = \lambda e_{m(\lambda)-1}(\lambda) $$ and so $$ A|_{N(\lambda)} = \left[\begin{matrix} \lambda & 1 & 0 & \cdots & 0 \\ 0 & \lambda & 1 & \cdots & 0 \\ 0 & 0 & \lambda & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & 1 \\ 0 & 0 & 0 & \cdots & \lambda \end{matrix}\right] $$ These $A|_{N(\lambda)}$ are the Jordan Blocks , and the matrix representation of $A$ above is the Jordan Normal Form . Main Question I'm pretty content with this derivation, nothing seems confusing or out of place or contradictory or nonrigorous, at least at a surface level. I would not be asking this question if I didn't go to the Wikipedia page on the Jordan Normal Form and see this line: The number of Jordan blocks corresponding to $\lambda$ of size at least $j$ is $\dim \text{Ker}(A - \lambda I)^j - \dim \text{Ker}(A - \lambda I)^{j-1}$. My ""derivation"" doesn't account for the fact that there can be multiple Jordan Blocks corresponding to the same eigenvalue . So, in the broadest sense possible, why? What don't I account for? My idea was that I ""assumed"" that $\text{Span}\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\} = \text{N}(\lambda)$. If there are more elements in the basis for $\text{N}(\lambda)$ than this, then there are more Jordan blocks. But if $\text{N}(\lambda)>m(\lambda)$, then the decomposition of $X$ into the direct sum of generalized eigenspaces fails, since the dimensions don't add up. My only other guess is that $\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\}$ can be ""broken down"" in some sense into the union of smaller bases which then produce more Jordan blocks, but I can't quite see where to go with that. Any help would be appreciated. Thank you for your time!","Though the title seems clear enough, I'd like to start with a discussion of how I personally came to derive the Jordan Normal Form, because my question is very specific to the details of my derivation. Notation To start, let $X$ be a finite dimensional vector space, $L(X)$ be the space of linear operators on $X$, and $A\in L(X)$. Let $\sigma(A) = \{\lambda_1,\ \cdots,\ \lambda_k\}$ be the spectrum of $A$. Now, we define $d(\lambda)$ to be the geometric multiplicity of $\lambda$ $m(\lambda)$ to be the algebraic multiplicity of $\lambda$ Next, we denote the $k$th generalized eigenspace of $\lambda$ by $$ \text{N}_k(\lambda) = \text{Ker}(A-\lambda I)^k $$ and finally, we let $$ \text{N}(\lambda) = N_{n(\lambda)}(\lambda)\qquad n(\lambda)=\min\{k\in\mathbb{N}\ |\ \text{N}_k(\lambda)=N_{k+1}(\lambda)\} $$ we note that it can be shown that $n(\lambda) = m(\lambda)$, and so the notation $n(\lambda)$ won't really be used. We will also let $\sum_\lambda$, $\prod_\lambda$, etc. represent the sum/product/etc. over distinct eigenvalues of $A$. Fundamentals First off, it is known that we can decompose $X$ as $$ X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k) $$ Hence $\sum_{\lambda} \dim\ \text{N}(\lambda) = \dim X$. Also, from the characteristic polynomial of $A$, the sum of the algebraic multiplicities of the eigenvalues must equal the degree of the polynomial, which is $\dim X$. Thus $$ \sum_\lambda\dim\ \text{N}(\lambda) = \sum_\lambda m(\lambda) = \dim X $$ Going in a different direction, we present the following theorem: Theorem: If $B\in L(X)$ is nilpotent of order $n$, and $S\subset X\backslash\text{Ker} B^{n-1}$ is linearly independent, then   $$ \bigcup_{x\in S}\{x,\ Bx,\ B^2x,\ \cdots,\ B^{n-1}x\} $$   is linearly independent. Proof: We will show the case for $|S|=2$, and the general case follows the same format. Suppose $S = \{x,\ y\}$, and $$ \sum_{k=0}^{n-1} a_k B^kx_1 + \sum_{k=0}^{n-1}b_k B^kx_2 = 0 $$ applying $B^{n-1}$ to both sides gives $$ B^{n-1}\left(\sum_{k=0}^{n-1}a_kB^kx_1+b_kB^kx_2\right) = a_0B^{n-1}x_1+b_0B^{n-1}x_2 = B^{n-1}(a_0x_1+b_0x_2) = 0 $$ so $a_0x_1 + b_0x_2\in\text{Ker}B^{n-1}$. However, since $\text{Ker}B^{n-1}$ is a subspace of $X$, we can decompose $X$ as $X = \text{Ker}B^{n-1}\oplus Z$ for some vector space $Z$, for which $\{x_1,\ x_2\}\subset Z\backslash\{0\}$. Since $Z$ is a subspace, $a_0x_1+b_0x_2\in Z$. To say that $a_0x_1+b_0x_2\in \text{Ker}B^{n-1}\cap Z$ is equivalent to saying $a_0x_1+b_0x_2 = 0$. By linear independence of $S$, $a_0=b_0=0$. This process can be repeated to get $a_j=b_j=0$ for all $j$. $\blacksquare$ Now, take $x\in \text{N}(\lambda)\backslash \text{N}_{m(\lambda)-1}(\lambda)$. Note that $B_\lambda = (A - \lambda I)|_{\text{N}(\lambda)}$ (that is, $A - \lambda I$ restricted to $\text{N}(\lambda)$) is nilpotent of order $m(\lambda)$. Hence $\{x,\ B_\lambda x,\ \cdots,\ B_\lambda^{m(\lambda)-1}x\}$ is linearly independent, and it's span is a subspace of $\text{N}(\lambda)$. Hence $\dim \text{N}(\lambda) \ge m(\lambda)$. If we suppose that $\dim\text{N}(\lambda) > m(\lambda)$ for at least one $\lambda\in\sigma(A)$, then we contradict the fact that $\sum_\lambda\dim\text{N}(\lambda) = \dim X$, and so we conclude that $m(\lambda) = \dim\text{N}(\lambda)$. Alright, so far so good I hope... Jordan Normal Form By the above arguments, we conclude that $\text{Span}\{x,\ \cdots,\ B^{m(\lambda)-1}x\} = \text{N}(\lambda)$. Hence, if we let $e_0(\lambda)\in N(\lambda)\backslash N_{m(\lambda)-1}(\lambda)$, and $e_k(\lambda)=(A-\lambda I)^k e_0(\lambda)$, then $$ \text{Span}\left(\bigcup_{\lambda}\bigcup_{k=0}^{m(\lambda)-1}\{e_k(\lambda)\}\right) = X $$ Since $X = \text{N}(\lambda_1)\oplus\cdots\oplus\text{N}(\lambda_k)$, and each $\text{N}(\lambda_k)$ is $A$-invariant (that is $A(\text{N}(\lambda_k))\subseteq \text{N}(\lambda_k)$), it follows that if we have bases for each $N(\lambda_i)$, then we can get the following matrix representation of $A$ wrt the union of these bases: $$ A = \left[\begin{matrix} A|_{\text{N}(\lambda_1)} & O & \cdots & \vdots \\ O & A|_{\text{N}(\lambda_2)} & \cdots & \vdots \\ \vdots & \vdots & \ddots & \vdots \\ \cdots & \cdots & \cdots & A|_{\text{N}(\lambda_k)} \end{matrix}\right] $$ where $A|_{\text{N}(\lambda_i)}$ is the matrix representation of $A$ restricted to $\text{N}(\lambda_i)$ wrt the basis of $\text{N}(\lambda_i)$. Above, we demonstrated that $\{e_{m(\lambda)-1}(\lambda),\ \cdots,\ e_1(\lambda)\}$ is a basis for $\text{N}(\lambda)$. We can find a matrix representation for $A|_{\text{N}(\lambda_i)}$ by noting that $$ Ae_k(\lambda) = A(A-\lambda I)^ke_1(\lambda) = (A-\lambda I)^{k+1}e_1(\lambda) + \lambda(A-\lambda I)^ke_1(\lambda) \\ Ae_k(\lambda) = e_{k+1}(\lambda)+\lambda e_k(\lambda) \\ Ae_{m(\lambda)-1}(\lambda) = \lambda e_{m(\lambda)-1}(\lambda) $$ and so $$ A|_{N(\lambda)} = \left[\begin{matrix} \lambda & 1 & 0 & \cdots & 0 \\ 0 & \lambda & 1 & \cdots & 0 \\ 0 & 0 & \lambda & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & 1 \\ 0 & 0 & 0 & \cdots & \lambda \end{matrix}\right] $$ These $A|_{N(\lambda)}$ are the Jordan Blocks , and the matrix representation of $A$ above is the Jordan Normal Form . Main Question I'm pretty content with this derivation, nothing seems confusing or out of place or contradictory or nonrigorous, at least at a surface level. I would not be asking this question if I didn't go to the Wikipedia page on the Jordan Normal Form and see this line: The number of Jordan blocks corresponding to $\lambda$ of size at least $j$ is $\dim \text{Ker}(A - \lambda I)^j - \dim \text{Ker}(A - \lambda I)^{j-1}$. My ""derivation"" doesn't account for the fact that there can be multiple Jordan Blocks corresponding to the same eigenvalue . So, in the broadest sense possible, why? What don't I account for? My idea was that I ""assumed"" that $\text{Span}\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\} = \text{N}(\lambda)$. If there are more elements in the basis for $\text{N}(\lambda)$ than this, then there are more Jordan blocks. But if $\text{N}(\lambda)>m(\lambda)$, then the decomposition of $X$ into the direct sum of generalized eigenspaces fails, since the dimensions don't add up. My only other guess is that $\{x,\ \cdots,\ \text{B}_\lambda^{m(\lambda)-1}x\}$ can be ""broken down"" in some sense into the union of smaller bases which then produce more Jordan blocks, but I can't quite see where to go with that. Any help would be appreciated. Thank you for your time!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
45,Proving that every vector space has a norm.,Proving that every vector space has a norm.,,"I am trying to prove that every vector space $X$ has a norm. I have some silly questions, but it's better to ask them now instead of later. I think I'm having a bit of trouble getting intuition about basis in infinite dimensional spaces. Fix a Hamel basis ${\cal B} = ({\bf e}_i)_{i \in I}$. Then for all ${\bf x} \in X$, we write: $${\bf x} = \sum_{i \in F}a_i{\bf e}_i,$$ for some $F \subset I$ finite. I understand that this combination is unique in the sense that: $$\sum_{i \in F_1}a_i{\bf e}_i = \sum_{i \in F_2}b_i{\bf e}_i,$$ for some $F_1,F_2 \subset I$ finite, implies that $a_i = b_i$ for all $i \in F_1 \cap F_2$ and $a_i = 0$ for all $i \in F_1 \setminus F_2$, and $b_i = 0$ for all $i \in F_2 \setminus F_1$. Does this mean that $F_1 = F_2$? Assuming that yes, although I'm not sure, the idea would be to define some kind of max norm, which would be well-defined: $$\|{\bf x}\| = \max\{|a_i| \mid i \in F  \}.$$ This idea seems good, it even showed up in another answer . The properties $\|{\bf x}\| \geq 0$ for all ${\bf x}\in X$, $\|{\bf x}\| = 0 \implies {\bf x}=0$ and $\|\lambda{\bf x}\| = |\lambda|\|{\bf x}\|$ are all clear. I'm having trouble getting the triangle inequality. How to make a sum here is a bit confuse to me. If ${\bf x},{\bf y}\in X$, then there are $F_1,F_2 \subset I$ finite such that: $${\bf x} = \sum_{i \in F_1}a_i{\bf e}_i, \quad \text{and}\quad {\bf y}=\sum_{i\in F_2}b_i{\bf e}_i,$$ so that $${\bf x}+{\bf y} = \sum_{i \in F_1 \setminus F_2}x_i{\bf e}_i + \sum_{i\in F_1 \cap F_2}(a_i+b_i){\bf e}_i + \sum_{i \in F_2 \setminus F_1}b_i{\bf e}_i.$$ I wanted to write this as $\sum_{i \in \text{ something}}c_i{\bf e}_i,$ but the only thing I could think of was: $$\sum_{i \in F_1 \cup F_2}c_i{\bf e}_i, \quad c_i = \begin{cases} a_i, \text{ if }i \in F_1 \setminus F_2 \\ a_i+b_i, \text{ if }i \in F_1 \cap F_2 \\ b_i, \text{ if }i \in F_2 \setminus F_1\end{cases}$$ But again: Is this combination unique in the sense that the only possible combination for the vector ${\bf x}+{\bf y}$ will be indexed by $F_1\cup F_2$? I think I am overcomplicating things. I can get the triangle inequality with this, it seems, but things don't look well-defined enough for me. Can someone address these questions and give me a small explanation about it? Thanks. Edit: to confirm what I understood from gerw's answer: since the combination is unique, if I write $${\bf x}=∑_{i∈F_1}a_i{\bf e}_i=∑_{i∈F_2}b_i{\bf e}_i,$$ for $F_1,F_2⊂I$, finite sets, then: $\max\{|a_i|∣i∈F_1\}=\max\{|b_i|∣i∈F_2\}$, and this ensures that $\|\cdot\|$ is well defined, right?","I am trying to prove that every vector space $X$ has a norm. I have some silly questions, but it's better to ask them now instead of later. I think I'm having a bit of trouble getting intuition about basis in infinite dimensional spaces. Fix a Hamel basis ${\cal B} = ({\bf e}_i)_{i \in I}$. Then for all ${\bf x} \in X$, we write: $${\bf x} = \sum_{i \in F}a_i{\bf e}_i,$$ for some $F \subset I$ finite. I understand that this combination is unique in the sense that: $$\sum_{i \in F_1}a_i{\bf e}_i = \sum_{i \in F_2}b_i{\bf e}_i,$$ for some $F_1,F_2 \subset I$ finite, implies that $a_i = b_i$ for all $i \in F_1 \cap F_2$ and $a_i = 0$ for all $i \in F_1 \setminus F_2$, and $b_i = 0$ for all $i \in F_2 \setminus F_1$. Does this mean that $F_1 = F_2$? Assuming that yes, although I'm not sure, the idea would be to define some kind of max norm, which would be well-defined: $$\|{\bf x}\| = \max\{|a_i| \mid i \in F  \}.$$ This idea seems good, it even showed up in another answer . The properties $\|{\bf x}\| \geq 0$ for all ${\bf x}\in X$, $\|{\bf x}\| = 0 \implies {\bf x}=0$ and $\|\lambda{\bf x}\| = |\lambda|\|{\bf x}\|$ are all clear. I'm having trouble getting the triangle inequality. How to make a sum here is a bit confuse to me. If ${\bf x},{\bf y}\in X$, then there are $F_1,F_2 \subset I$ finite such that: $${\bf x} = \sum_{i \in F_1}a_i{\bf e}_i, \quad \text{and}\quad {\bf y}=\sum_{i\in F_2}b_i{\bf e}_i,$$ so that $${\bf x}+{\bf y} = \sum_{i \in F_1 \setminus F_2}x_i{\bf e}_i + \sum_{i\in F_1 \cap F_2}(a_i+b_i){\bf e}_i + \sum_{i \in F_2 \setminus F_1}b_i{\bf e}_i.$$ I wanted to write this as $\sum_{i \in \text{ something}}c_i{\bf e}_i,$ but the only thing I could think of was: $$\sum_{i \in F_1 \cup F_2}c_i{\bf e}_i, \quad c_i = \begin{cases} a_i, \text{ if }i \in F_1 \setminus F_2 \\ a_i+b_i, \text{ if }i \in F_1 \cap F_2 \\ b_i, \text{ if }i \in F_2 \setminus F_1\end{cases}$$ But again: Is this combination unique in the sense that the only possible combination for the vector ${\bf x}+{\bf y}$ will be indexed by $F_1\cup F_2$? I think I am overcomplicating things. I can get the triangle inequality with this, it seems, but things don't look well-defined enough for me. Can someone address these questions and give me a small explanation about it? Thanks. Edit: to confirm what I understood from gerw's answer: since the combination is unique, if I write $${\bf x}=∑_{i∈F_1}a_i{\bf e}_i=∑_{i∈F_2}b_i{\bf e}_i,$$ for $F_1,F_2⊂I$, finite sets, then: $\max\{|a_i|∣i∈F_1\}=\max\{|b_i|∣i∈F_2\}$, and this ensures that $\|\cdot\|$ is well defined, right?",,"['linear-algebra', 'functional-analysis', 'vector-spaces']"
46,Inscrutable proof in Humphrey's book on Lie algebras and representations,Inscrutable proof in Humphrey's book on Lie algebras and representations,,"This is a question pertaining to Humphrey's Introduction to Lie Algebras and Representation Theory Is there an explanation of the lemma in §4.3-Cartan's Criterion ? I understand the proof given there but I fail to understand how anybody could have ever devised it or had the guts to prove such a strange statement... Lemma : Let $k$ be an algebraically closed field of characteristic $0$. Let $V$ be a finite dimensional vector space over $k$, and $A\subset B\subset \mathrm{End}(V)$ two subspaces. Let $M$ be the set of endomorphisms $x$ of $V$ such that $[x,B]\subset A$. Suppose $x\in M$ is such that $\forall y\in M, \mathrm{Tr}(xy)=0$. Then, $x$ is nilpotent. The proof uses the diagonalisable$+$nilpotent decomposition, and goes on to show that all eigenvalues of $x$ are $=0$ by showing that the $\mathbb{Q}$ subspace of $k$ they generate has only the $0$ linear functional. Added: (t.b.) here's the page from Google books for those without access:","This is a question pertaining to Humphrey's Introduction to Lie Algebras and Representation Theory Is there an explanation of the lemma in §4.3-Cartan's Criterion ? I understand the proof given there but I fail to understand how anybody could have ever devised it or had the guts to prove such a strange statement... Lemma : Let $k$ be an algebraically closed field of characteristic $0$. Let $V$ be a finite dimensional vector space over $k$, and $A\subset B\subset \mathrm{End}(V)$ two subspaces. Let $M$ be the set of endomorphisms $x$ of $V$ such that $[x,B]\subset A$. Suppose $x\in M$ is such that $\forall y\in M, \mathrm{Tr}(xy)=0$. Then, $x$ is nilpotent. The proof uses the diagonalisable$+$nilpotent decomposition, and goes on to show that all eigenvalues of $x$ are $=0$ by showing that the $\mathbb{Q}$ subspace of $k$ they generate has only the $0$ linear functional. Added: (t.b.) here's the page from Google books for those without access:",,"['linear-algebra', 'lie-algebras']"
47,pi approximation with Newton's method to an arbitrary rate of convergence,pi approximation with Newton's method to an arbitrary rate of convergence,,"If $a_1$ to $a_3$ is the solution of this linear system of equations $$\left(\begin{array}{rrr|r} -1&2&-3& -1\\ 1&-8&27&0 \\ -1 & 32 & -243 & 0 \end{array}\right)$$ then $f_3(x) = a_1\sin(x)+ a_2 \sin(2x) + a_3 \sin(3x)$ defines a function with $f_3(\pi) = 0$ and the convergence rate of Newton's methods should locally be of 6th order since $f_3'(\pi) = -1$ and the 2nd to 6th derivatives vanish at $\pi$ (That's how the system of equations is constructed). As far as I know from my class on numerical methods, this is sufficient for the rate of convergence. Similarly, one can extend the defining system of equations above to construct functions which imply an arbitrary rate of convergence for Newton's method by computing coefficients $a_1$ to $a_k$ with $f_k(\pi) = 0$ , $f_k'(\pi) = -1$ and $f_k^{(i)}(\pi) = 0$ for $2 \leq i \leq 2k$ . The convergence rate of Newton's method is therfore of the order $2k$ . Numerical calculations in Matlab confirm these results. By plotting the functions $f_k$ one can observe that the function seems to converge pointwise to $g(x) = \pi -x $ for $x \in (0, 2\pi)$ . If this is true, how can one prove this? I'm not familiar with functions definied by solutions of linear systems of equations and have never seen something like this before. I tried to use the Taylor expansion of $|f_k(x)| = |\frac{1}{(2k+1)!} \cdot f^{(2k+1)}(\xi)| \gtrapprox a_k\frac{k^{2k+1}}{(2k+1)!} (x-\pi)^{2k+1}$ . But $\frac{k^{2k+1}}{(2k+1)!}$ diverges when $k$ approches $\infty$ , so I either need an estimation of $a_k$ or another approach to show pointwise convergence. Is there any material about functions defined by linear system of equations or closely related topics (preferably undergraduate level)? Did anyone, by chance, have a smiliar idea to this and/or is there any further material available on this topic? Is it connected to the Fourier series? Another question I asked myself when I discovered these functions was if it is connected to the Fourier series of $\pi-x$ (extended periodically beyond $[0, 2\pi]$ ) because it is a linear combination of sine functions with integer factors in the argument. I remember that if there exits a uniformly convergent series of the form $\sum_{k= -\infty}^\infty c_k e^{ikx}$ it must be the Fourier series. We can write $f_k = \sum_{i= 0}^k a_{ki}\sin(ix)$ , so there can't be any coefficients $a_{\infty i}$ such that the sum defined by $f_\infty$ converges uniformly to $\pi-x$ because that would imply the uniform convergence of the Fourier Series of $\pi-x$ , which it does not, if I remember correctly. Can one define something like $a_{\infty i}$ in a reasonable way? And if so, are they the Fourier coefficients?I have no idea how to start here. I have found some results about infinite systems of linear equations but nothing directly applicable to my problem. Thanks in advance for any answers.","If to is the solution of this linear system of equations then defines a function with and the convergence rate of Newton's methods should locally be of 6th order since and the 2nd to 6th derivatives vanish at (That's how the system of equations is constructed). As far as I know from my class on numerical methods, this is sufficient for the rate of convergence. Similarly, one can extend the defining system of equations above to construct functions which imply an arbitrary rate of convergence for Newton's method by computing coefficients to with , and for . The convergence rate of Newton's method is therfore of the order . Numerical calculations in Matlab confirm these results. By plotting the functions one can observe that the function seems to converge pointwise to for . If this is true, how can one prove this? I'm not familiar with functions definied by solutions of linear systems of equations and have never seen something like this before. I tried to use the Taylor expansion of . But diverges when approches , so I either need an estimation of or another approach to show pointwise convergence. Is there any material about functions defined by linear system of equations or closely related topics (preferably undergraduate level)? Did anyone, by chance, have a smiliar idea to this and/or is there any further material available on this topic? Is it connected to the Fourier series? Another question I asked myself when I discovered these functions was if it is connected to the Fourier series of (extended periodically beyond ) because it is a linear combination of sine functions with integer factors in the argument. I remember that if there exits a uniformly convergent series of the form it must be the Fourier series. We can write , so there can't be any coefficients such that the sum defined by converges uniformly to because that would imply the uniform convergence of the Fourier Series of , which it does not, if I remember correctly. Can one define something like in a reasonable way? And if so, are they the Fourier coefficients?I have no idea how to start here. I have found some results about infinite systems of linear equations but nothing directly applicable to my problem. Thanks in advance for any answers.","a_1 a_3 \left(\begin{array}{rrr|r}
-1&2&-3& -1\\
1&-8&27&0 \\
-1 & 32 & -243 & 0
\end{array}\right) f_3(x) = a_1\sin(x)+ a_2 \sin(2x) + a_3 \sin(3x) f_3(\pi) = 0 f_3'(\pi) = -1 \pi a_1 a_k f_k(\pi) = 0 f_k'(\pi) = -1 f_k^{(i)}(\pi) = 0 2 \leq i \leq 2k 2k f_k g(x) = \pi -x  x \in (0, 2\pi) |f_k(x)| = |\frac{1}{(2k+1)!} \cdot f^{(2k+1)}(\xi)| \gtrapprox a_k\frac{k^{2k+1}}{(2k+1)!} (x-\pi)^{2k+1} \frac{k^{2k+1}}{(2k+1)!} k \infty a_k \pi-x [0, 2\pi] \sum_{k= -\infty}^\infty c_k e^{ikx} f_k = \sum_{i= 0}^k a_{ki}\sin(ix) a_{\infty i} f_\infty \pi-x \pi-x a_{\infty i}","['linear-algebra', 'numerical-methods', 'matrix-equations']"
48,A Matrix With Eigenvalues Equal to The Golden Ratio and the Golden Conjugate,A Matrix With Eigenvalues Equal to The Golden Ratio and the Golden Conjugate,,"Question : Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{i,j}=1$ if $i+j$ is a perfect power; otherwise equals to   $0$.  Can we determine the numbers $n$ such that characteristic polynomial of $A(n)$ has at least two real roots: one equal to the golden ratio and the other equal to the golden conjugate ? For an examlple consider $$A(8)= \text{ }\begin{pmatrix} 0&0&1&0&0&0&1&1\\ 0&1&0&0&0&1&1&0\\ 1&0&0&0&1&1&0&0\\ 0&0&0&1&1&0&0&0\\ 0&0&1&1&0&0&0&0\\ 0&1&1&0&0&0&0&0\\ 1&1&0&0&0&0&0&0\\ 1&0&0&0&0&0&0&1\\ \end{pmatrix}$$ I denote the characteristic polynomial of $A(n)$ by $\chi_{A(n)}(X)$. So for example $$\chi_{A(8)}(X)=X^8-3X^7-5X^6+19X^5+2X^4-31X^3+X^2+10X-3$$ I use the standard notation $\varphi$ to denote the golden ratio and it is equal to ${1+\sqrt{5}\above 1.5pt 2}$. The ""Golden Conjugate"" is written as $-{1\above 1.5 pt \varphi}$ and is equal to ${1-\sqrt{5}\above 1.5pt 2}$. I use $\lambda$ to denote an eigenvalue of $\chi_{A(n)}(X)$. In the example above the characteristic polynomial $\chi_{A(8)}(X)$ has two eigenvalues $\lambda_1=-{1\above 1.5 pt \varphi}$ and $\lambda_2=\varphi$. In particular $\chi_{A(8)}(X)$ has exactly two roots: one equal to the golden ratio and the other equal to the golden conjugate. Below is a tabulation of data for small values of $n$. Calculations were performed in WOLFRAM ALPHA. The calculations should be correct but errors are inevitable. Only real eigenvalues are listed: \begin{array}{| l | l | l | l |l|} \hline n & \text{characteristic polynomial} &\text{eigenvalues}\\ \hline 2 & X^2-X &  (0,1)\\  3 & -X^3+X^2+X-1 & (-1,1,1)\\  4 & X^4-2X^3+2X-1 & (-1,1,1,1)\\  5 & -X^5+2X^4+2X^3-5X^2+X+1 &(-\lambda_1,\lambda_2,1,1,\lambda_3)\\  \color{blue}{6} & \color{blue}{X^6-2X^5-4X^4+8X^3+2X^2-4X-1} & \color{blue}{(1,-{1\above 1.5 pt \varphi},\varphi,\lambda_1,\lambda_2,\lambda_3)}\\  7 & -X^7-2X^6+6X^5-11X^4-9X^3+15X^2+2X-4 & (1,\lambda_1,\lambda_2,\lambda_3,\lambda_4,\lambda_5,\lambda_6) \\  \color{blue}{8} & \color{blue}{X^8-3X^7-5X^6+19X^5+2X^4-31X^3+X^2+10X-3} & \color{blue}{(-{1\above 1.5 pt \varphi},\varphi,\lambda_1,\lambda_2,\lambda_3,\lambda_4,\lambda_5,\lambda_6)}\\  \color{blue}{9} & \color{blue}{-X^9+3X^8+6X^7-22X^6-5X^5+45X^4-12X^3-22X^2+5X+3} & \color{blue}{(\lambda_1,\lambda_2,\lambda_3,-\varphi,\varphi,1,-{1\above 1.5 pt \varphi},1-\varphi,\lambda_4 )}\\      \hline     \end{array} Surely $n=6,8,9$ are shown to answer the question. I am certain that if $A(n)$ has $\varphi$ as a eigenvalue then it must also have $-{1\above 1.5 pt \varphi}$ as an eigenvalue. The motivation here is an understanding of the matrix $A(n)$. Edit 1 : Correction to tabulated eigenvalues when n=6. There are actually 6 real eigenvalues and I listed only 3. Three of the eigenvalues can be written : $1$, $\varphi$ and $1-\varphi$. The other three however have exact forms that can be written with $i=\sqrt{-1}$. $$\lambda_1 = {\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 3^{2/3}} + {4 \above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ $$\lambda_2 = {\bigg(1-i\sqrt{3}\bigg)\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 2\times 3^{2/3}} - {2(1+i\sqrt{3})\above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ $$\lambda_3 = -{\bigg(1+i\sqrt{3}\bigg)\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 2\times 3^{2/3}} - {2(1-i\sqrt{3})\above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ Numerically the eigenvalues $\lambda_1,\lambda_2$ and $\lambda_3$ are real - in particular the imaginary terms cancel out. Explicitly $\lambda_1\approx 2.11491\ldots$, $\lambda_2\approx -.618034\ldots$,$\lambda_3\approx -.254102\ldots$ Edit 2 : $A(n)$ never has complex eigenvalues for any $n$. I included the missing eigenvalues in the table.  Also I corrected errors in the table for $n=5$. None of the corrections change the result/conjecture of the problem. Namely that $6,8$ and $9$ are the only integers such that $A(n)$ has a eigenvalues equal to $\varphi$ or $-{1\above 1.5 pt \varphi}$. For $n=5$ the corrected values are numerically $\lambda_1 \approx 1.87939\ldots$, $\lambda_2\approx -1.53209\ldots$,$\lambda_3=1$,$\lambda_4=$, $\lambda_5 \approx -0.347296\ldots$ These values have exact forms similar to the radicals shown in Edit 1 above For $n=7$ the $6$ missing values are numerically $\lambda_1 \approx 2.38839\ldots$, $\lambda_2\approx -1.9041\ldots$,$\lambda_3\approx 1.77217\ldots$,$\lambda_4\approx -1.33388\ldots$, $\lambda_5 \approx 0.0.64993\ldots$,$\lambda_6 \approx -0.572499 \ldots$. For $n=8$ the $6$ missing values are numerically $\lambda_1 \approx 2.48767\ldots$, $\lambda_2\approx -1.98486\ldots$,$\lambda_3\approx 1.77268\ldots$,$\lambda_4\approx -1.39899\ldots$, $\lambda_5 \approx 0.827404\ldots$,$\lambda_6 \approx 0.296099 \ldots$. For $n=9$ the $4$ missing values are numerically $\lambda_1 \approx 2.54926\ldots$, $\lambda_2\approx -2.03439\ldots$,$\lambda_3\approx 1.80552\ldots$,$\lambda_4\approx 0.320385\ldots$ These values have exact forms similar to the radicals shown in Edit 1 above. Conjecture: $6,8$ or $9$ are the only numbers $n$ such that characteristic polynomial of $A(n)$ has eigenvalues: one equal to the   golden ratio and another equal to the golden conjugate.","Question : Let $A(n)$ be a finite square $n \times n$ matrix with entries $a_{i,j}=1$ if $i+j$ is a perfect power; otherwise equals to   $0$.  Can we determine the numbers $n$ such that characteristic polynomial of $A(n)$ has at least two real roots: one equal to the golden ratio and the other equal to the golden conjugate ? For an examlple consider $$A(8)= \text{ }\begin{pmatrix} 0&0&1&0&0&0&1&1\\ 0&1&0&0&0&1&1&0\\ 1&0&0&0&1&1&0&0\\ 0&0&0&1&1&0&0&0\\ 0&0&1&1&0&0&0&0\\ 0&1&1&0&0&0&0&0\\ 1&1&0&0&0&0&0&0\\ 1&0&0&0&0&0&0&1\\ \end{pmatrix}$$ I denote the characteristic polynomial of $A(n)$ by $\chi_{A(n)}(X)$. So for example $$\chi_{A(8)}(X)=X^8-3X^7-5X^6+19X^5+2X^4-31X^3+X^2+10X-3$$ I use the standard notation $\varphi$ to denote the golden ratio and it is equal to ${1+\sqrt{5}\above 1.5pt 2}$. The ""Golden Conjugate"" is written as $-{1\above 1.5 pt \varphi}$ and is equal to ${1-\sqrt{5}\above 1.5pt 2}$. I use $\lambda$ to denote an eigenvalue of $\chi_{A(n)}(X)$. In the example above the characteristic polynomial $\chi_{A(8)}(X)$ has two eigenvalues $\lambda_1=-{1\above 1.5 pt \varphi}$ and $\lambda_2=\varphi$. In particular $\chi_{A(8)}(X)$ has exactly two roots: one equal to the golden ratio and the other equal to the golden conjugate. Below is a tabulation of data for small values of $n$. Calculations were performed in WOLFRAM ALPHA. The calculations should be correct but errors are inevitable. Only real eigenvalues are listed: \begin{array}{| l | l | l | l |l|} \hline n & \text{characteristic polynomial} &\text{eigenvalues}\\ \hline 2 & X^2-X &  (0,1)\\  3 & -X^3+X^2+X-1 & (-1,1,1)\\  4 & X^4-2X^3+2X-1 & (-1,1,1,1)\\  5 & -X^5+2X^4+2X^3-5X^2+X+1 &(-\lambda_1,\lambda_2,1,1,\lambda_3)\\  \color{blue}{6} & \color{blue}{X^6-2X^5-4X^4+8X^3+2X^2-4X-1} & \color{blue}{(1,-{1\above 1.5 pt \varphi},\varphi,\lambda_1,\lambda_2,\lambda_3)}\\  7 & -X^7-2X^6+6X^5-11X^4-9X^3+15X^2+2X-4 & (1,\lambda_1,\lambda_2,\lambda_3,\lambda_4,\lambda_5,\lambda_6) \\  \color{blue}{8} & \color{blue}{X^8-3X^7-5X^6+19X^5+2X^4-31X^3+X^2+10X-3} & \color{blue}{(-{1\above 1.5 pt \varphi},\varphi,\lambda_1,\lambda_2,\lambda_3,\lambda_4,\lambda_5,\lambda_6)}\\  \color{blue}{9} & \color{blue}{-X^9+3X^8+6X^7-22X^6-5X^5+45X^4-12X^3-22X^2+5X+3} & \color{blue}{(\lambda_1,\lambda_2,\lambda_3,-\varphi,\varphi,1,-{1\above 1.5 pt \varphi},1-\varphi,\lambda_4 )}\\      \hline     \end{array} Surely $n=6,8,9$ are shown to answer the question. I am certain that if $A(n)$ has $\varphi$ as a eigenvalue then it must also have $-{1\above 1.5 pt \varphi}$ as an eigenvalue. The motivation here is an understanding of the matrix $A(n)$. Edit 1 : Correction to tabulated eigenvalues when n=6. There are actually 6 real eigenvalues and I listed only 3. Three of the eigenvalues can be written : $1$, $\varphi$ and $1-\varphi$. The other three however have exact forms that can be written with $i=\sqrt{-1}$. $$\lambda_1 = {\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 3^{2/3}} + {4 \above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ $$\lambda_2 = {\bigg(1-i\sqrt{3}\bigg)\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 2\times 3^{2/3}} - {2(1+i\sqrt{3})\above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ $$\lambda_3 = -{\bigg(1+i\sqrt{3}\bigg)\sqrt[3]{\frac{1}{2}\bigg(9 + i \sqrt{687}\bigg)}\above 1.5 pt 2\times 3^{2/3}} - {2(1-i\sqrt{3})\above 1.5pt {\sqrt[3]{\frac{3}{2} \bigg (9 + i \sqrt{687}\bigg)}}}$$ Numerically the eigenvalues $\lambda_1,\lambda_2$ and $\lambda_3$ are real - in particular the imaginary terms cancel out. Explicitly $\lambda_1\approx 2.11491\ldots$, $\lambda_2\approx -.618034\ldots$,$\lambda_3\approx -.254102\ldots$ Edit 2 : $A(n)$ never has complex eigenvalues for any $n$. I included the missing eigenvalues in the table.  Also I corrected errors in the table for $n=5$. None of the corrections change the result/conjecture of the problem. Namely that $6,8$ and $9$ are the only integers such that $A(n)$ has a eigenvalues equal to $\varphi$ or $-{1\above 1.5 pt \varphi}$. For $n=5$ the corrected values are numerically $\lambda_1 \approx 1.87939\ldots$, $\lambda_2\approx -1.53209\ldots$,$\lambda_3=1$,$\lambda_4=$, $\lambda_5 \approx -0.347296\ldots$ These values have exact forms similar to the radicals shown in Edit 1 above For $n=7$ the $6$ missing values are numerically $\lambda_1 \approx 2.38839\ldots$, $\lambda_2\approx -1.9041\ldots$,$\lambda_3\approx 1.77217\ldots$,$\lambda_4\approx -1.33388\ldots$, $\lambda_5 \approx 0.0.64993\ldots$,$\lambda_6 \approx -0.572499 \ldots$. For $n=8$ the $6$ missing values are numerically $\lambda_1 \approx 2.48767\ldots$, $\lambda_2\approx -1.98486\ldots$,$\lambda_3\approx 1.77268\ldots$,$\lambda_4\approx -1.39899\ldots$, $\lambda_5 \approx 0.827404\ldots$,$\lambda_6 \approx 0.296099 \ldots$. For $n=9$ the $4$ missing values are numerically $\lambda_1 \approx 2.54926\ldots$, $\lambda_2\approx -2.03439\ldots$,$\lambda_3\approx 1.80552\ldots$,$\lambda_4\approx 0.320385\ldots$ These values have exact forms similar to the radicals shown in Edit 1 above. Conjecture: $6,8$ or $9$ are the only numbers $n$ such that characteristic polynomial of $A(n)$ has eigenvalues: one equal to the   golden ratio and another equal to the golden conjugate.",,"['linear-algebra', 'abstract-algebra', 'eigenvalues-eigenvectors', 'conjectures', 'golden-ratio']"
49,What's the intuition of the transpose of a matrix? [duplicate],What's the intuition of the transpose of a matrix? [duplicate],,"This question already has answers here : What is the geometric interpretation of the transpose? (5 answers) Closed 9 years ago . I know the transpose is to swap the columns and rows of a matrix. And $A^T$$A$ is a symmetric matrix which elements are the inner product of each column of $A$. But I didn't understand the intuition of transpose. Suppose $A_{m \times n}$, and A transform a vector from $\Bbb R^n$ to $\Bbb R^m$. But $A^T$ transform a vector from $\Bbb R^m$ to $\Bbb R^n$. What's the relationship between them? Could anyone please explain the relationship between $A^T$,$A$,the inner product and symmetric matrix. I think there would be a intuition explaination.","This question already has answers here : What is the geometric interpretation of the transpose? (5 answers) Closed 9 years ago . I know the transpose is to swap the columns and rows of a matrix. And $A^T$$A$ is a symmetric matrix which elements are the inner product of each column of $A$. But I didn't understand the intuition of transpose. Suppose $A_{m \times n}$, and A transform a vector from $\Bbb R^n$ to $\Bbb R^m$. But $A^T$ transform a vector from $\Bbb R^m$ to $\Bbb R^n$. What's the relationship between them? Could anyone please explain the relationship between $A^T$,$A$,the inner product and symmetric matrix. I think there would be a intuition explaination.",,"['linear-algebra', 'intuition', 'symmetry']"
50,Is every convex-linear map an affine map?,Is every convex-linear map an affine map?,,"Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is convex-linear if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in [0,1]$. Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is affine if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in \mathbb{R}$. From the definition, it seems that the requirement of being convex-linear is weaker than the requirement of being affine. However, I can't think of an example of a map which is convex-linear but not affine, but I also can't prove that convex-linearity implies affinity. Can someone show me an example of a convex-linear map which is not affine? Or tell me how to prove that every convex-linear map is affine? Or give me an appropriate reference? EDIT: With the intuition of Qiaochu Yuan's comment in mind, I've come up with the following proof: Claim: Every convex-linear map is affine. Proof: Let $f$ be convex-linear. For $\lambda \in [0, 1]$, We have that $f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$. For $\lambda \notin [0,1]$, we can assume without loss of generality that $\lambda < 0$ (in the other case where $\lambda > 1$, we can interchange the role of $x$ and $y$). We can write \begin{align} f(y) = f\left( \underbrace{\frac{1}{1-\lambda}}_{\in [0,1]}(\lambda x + (1-\lambda) y) + \left( 1 - \frac{1}{1-\lambda} \right) x \right). \label{bla} \end{align} By the convex-linearity of $f$, this reduces to \begin{align} &f(y) = \frac{1}{1-\lambda} f(\lambda x + (1-\lambda) y) + \left( 1-\frac{1}{1-\lambda} \right) f(x) \end{align} which in turn can be reduced to \begin{align} f(\lambda x + (1-\lambda x)) = \lambda f(x) + (1-\lambda) f(y). \end{align}","Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is convex-linear if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in [0,1]$. Let's say that a map $f: V \rightarrow W$ between finite-dimensional real vector spaces is affine if $f(\lambda x + (1-\lambda)y) = \lambda f(x) + (1-\lambda)f(y)$ for all $\lambda \in \mathbb{R}$. From the definition, it seems that the requirement of being convex-linear is weaker than the requirement of being affine. However, I can't think of an example of a map which is convex-linear but not affine, but I also can't prove that convex-linearity implies affinity. Can someone show me an example of a convex-linear map which is not affine? Or tell me how to prove that every convex-linear map is affine? Or give me an appropriate reference? EDIT: With the intuition of Qiaochu Yuan's comment in mind, I've come up with the following proof: Claim: Every convex-linear map is affine. Proof: Let $f$ be convex-linear. For $\lambda \in [0, 1]$, We have that $f(\lambda x + (1-\lambda) y) = \lambda f(x) + (1-\lambda) f(y)$. For $\lambda \notin [0,1]$, we can assume without loss of generality that $\lambda < 0$ (in the other case where $\lambda > 1$, we can interchange the role of $x$ and $y$). We can write \begin{align} f(y) = f\left( \underbrace{\frac{1}{1-\lambda}}_{\in [0,1]}(\lambda x + (1-\lambda) y) + \left( 1 - \frac{1}{1-\lambda} \right) x \right). \label{bla} \end{align} By the convex-linearity of $f$, this reduces to \begin{align} &f(y) = \frac{1}{1-\lambda} f(\lambda x + (1-\lambda) y) + \left( 1-\frac{1}{1-\lambda} \right) f(x) \end{align} which in turn can be reduced to \begin{align} f(\lambda x + (1-\lambda x)) = \lambda f(x) + (1-\lambda) f(y). \end{align}",,"['linear-algebra', 'convex-analysis', 'affine-geometry']"
51,Tensor Product and Physics,Tensor Product and Physics,,"During lecture, my abstract algebra professor said that the exactness of the tensor product is ""absolutely essential"" to the existence of physical phenomena such as black holes and the big bang. Is it more or less directly related to the existence of such phenomena or is it a big stretch to make such a conclusion? If the former, what is the connection?","During lecture, my abstract algebra professor said that the exactness of the tensor product is ""absolutely essential"" to the existence of physical phenomena such as black holes and the big bang. Is it more or less directly related to the existence of such phenomena or is it a big stretch to make such a conclusion? If the former, what is the connection?",,"['linear-algebra', 'category-theory', 'mathematical-physics', 'tensor-products']"
52,"$\text{SL}(2, \mathbb{F}_q)$, for which characters is the $G$-representation irreducible?",", for which characters is the -representation irreducible?","\text{SL}(2, \mathbb{F}_q) G","Followup to here . Let $\mathbb{F}$ be a finite field with $q$ elements, and let $G = \text{SL}_2(\mathbb{F})$. The group $G$ acts linearly on the $2$-dimensional vector space $\mathbb{F}^2$ and fixes the origin $0$. Hence, $G$ acts on the set $X := \mathbb{F}^2 \setminus \{0\}$, the complement of the origin. For any group homomorphism $\chi: \mathbb{F}^\times \to S^1 \subset \mathbb{C}^\times$, in $\mathbb{C}\{X\}$, we define a subspace$$\mathbb{C}\{X\}^\chi := \{f \in \mathbb{C}\{X\} : f(z \cdot x) = \chi(z) \cdot f(x), \text{ for all }z \in \mathbb{F}^\times\}.$$We know that there is a vector space direct sum decomposition$$\mathbb{C}\{X\} = \oplus_{\chi \in \widehat{H}} \mathbb{C}\{X\}^\chi,$$where we put $H := \mathbb{F}^\times$. My question is as follows. For which $\chi \in \widehat{H}$ is the $G$-representation $\mathbb{C}\{X\}^\chi$ irreducible?","Followup to here . Let $\mathbb{F}$ be a finite field with $q$ elements, and let $G = \text{SL}_2(\mathbb{F})$. The group $G$ acts linearly on the $2$-dimensional vector space $\mathbb{F}^2$ and fixes the origin $0$. Hence, $G$ acts on the set $X := \mathbb{F}^2 \setminus \{0\}$, the complement of the origin. For any group homomorphism $\chi: \mathbb{F}^\times \to S^1 \subset \mathbb{C}^\times$, in $\mathbb{C}\{X\}$, we define a subspace$$\mathbb{C}\{X\}^\chi := \{f \in \mathbb{C}\{X\} : f(z \cdot x) = \chi(z) \cdot f(x), \text{ for all }z \in \mathbb{F}^\times\}.$$We know that there is a vector space direct sum decomposition$$\mathbb{C}\{X\} = \oplus_{\chi \in \widehat{H}} \mathbb{C}\{X\}^\chi,$$where we put $H := \mathbb{F}^\times$. My question is as follows. For which $\chi \in \widehat{H}$ is the $G$-representation $\mathbb{C}\{X\}^\chi$ irreducible?",,"['linear-algebra', 'abstract-algebra']"
53,"X,AX have no common eigenvalues","X,AX have no common eigenvalues",,"Let $A\in M_n(\mathbb{C})$ s.t. $A\not= I_n$. Show that there is $X\in M_n(\mathbb{C})$ s.t. $X$ and $AX$ have no common eigenvalues. Perhaps it is easy to prove, but I don't know how to do. Comment. The proof is easy when $1\notin spectrum(A)$. Moreover, if the asked result is true, then we can show that $\{X;X,AX \;\;\text{have no common eigenvalues}\}$ is Zariski open dense in $M_n(\mathbb{C})$.","Let $A\in M_n(\mathbb{C})$ s.t. $A\not= I_n$. Show that there is $X\in M_n(\mathbb{C})$ s.t. $X$ and $AX$ have no common eigenvalues. Perhaps it is easy to prove, but I don't know how to do. Comment. The proof is easy when $1\notin spectrum(A)$. Moreover, if the asked result is true, then we can show that $\{X;X,AX \;\;\text{have no common eigenvalues}\}$ is Zariski open dense in $M_n(\mathbb{C})$.",,['linear-algebra']
54,Does there exist a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces?,Does there exist a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces?,,"Let $V_1, V_2, \dots, V_n$ be a collection of vector subspaces in $\mathbb R^n$ . For each $j=1, \dots, n$ , $\dim(V_j) = m$ with $2 \le m < n$ . We also have the condition: for any collection of $\lceil{\frac n m}\rceil$ vector spaces from $\{V_1, \dots, V_n\}$ , then $V_{k_1} + \dots + V_{k_{\lceil \frac n m \rceil}} = \mathbb R^n$ .  Suppose we construct a basis $U = \{u_1, \dots, u_n\}$ of $\mathbb R^n$ in the manner: $u_j \in V_j$ for each $j$ . Now suppose we construct another basis $W = \{w_1, \dots, w_n\}$ in the same manner, i.e., $w_j \in V_j$ for each $j$ . I am wondering whether $U$ is connected with $W$ in the sense: there is a path $\gamma = \gamma_1 \times \gamma_2 \times \dots \times \gamma_n$ , where each $\gamma_j: [0,1] \to V_j$ is a continuous path connecting $v_j$ and $w_j$ in $V_j$ and for each $t$ : $\gamma(t)$ forms a basis for $\mathbb R^n$ . We assume the basis $\{v_j\}$ and $\{w_j\}$ have the same orientation. The basis can be identified by $GL_n(\mathbb R)_+$ or $GL_n(\mathbb R)_-$ and we know they are connected. But is there a way to guarantee on the path, each column vector only varies in the corresponding subspace? I asked a similar question here Constructing a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces . The conditions are stronger here. An example of $V_1, \dots, V_n$ : suppose $n=5$ , $m=2$ . The construction I have in mind is: $V_i = \text{span} ( (1, a_i, 0, 0, 0), (0, 0, 1, a_i, a_i^2))$ . As long as $a_1 \neq \dots \neq a_5 \neq 0$ , any three subspace would span $\mathbb R^5$ . For other cases, we can use similar idea.","Let be a collection of vector subspaces in . For each , with . We also have the condition: for any collection of vector spaces from , then .  Suppose we construct a basis of in the manner: for each . Now suppose we construct another basis in the same manner, i.e., for each . I am wondering whether is connected with in the sense: there is a path , where each is a continuous path connecting and in and for each : forms a basis for . We assume the basis and have the same orientation. The basis can be identified by or and we know they are connected. But is there a way to guarantee on the path, each column vector only varies in the corresponding subspace? I asked a similar question here Constructing a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces . The conditions are stronger here. An example of : suppose , . The construction I have in mind is: . As long as , any three subspace would span . For other cases, we can use similar idea.","V_1, V_2, \dots, V_n \mathbb R^n j=1, \dots, n \dim(V_j) = m 2 \le m < n \lceil{\frac n m}\rceil \{V_1, \dots, V_n\} V_{k_1} + \dots + V_{k_{\lceil \frac n m \rceil}} = \mathbb R^n U = \{u_1, \dots, u_n\} \mathbb R^n u_j \in V_j j W = \{w_1, \dots, w_n\} w_j \in V_j j U W \gamma = \gamma_1 \times \gamma_2 \times \dots \times \gamma_n \gamma_j: [0,1] \to V_j v_j w_j V_j t \gamma(t) \mathbb R^n \{v_j\} \{w_j\} GL_n(\mathbb R)_+ GL_n(\mathbb R)_- V_1, \dots, V_n n=5 m=2 V_i = \text{span} ( (1, a_i, 0, 0, 0), (0, 0, 1, a_i, a_i^2)) a_1 \neq \dots \neq a_5 \neq 0 \mathbb R^5","['linear-algebra', 'general-topology', 'path-connected']"
55,Finding the ratio between two $8$-dimensional volumes,Finding the ratio between two -dimensional volumes,8,"EDIT: At this point, geometric interpretations of conditions 2-4 would qualify as an answer. This can include symmetries of the region. I have a real $3 \times 3$ matrix $A$ with entries $a_{ij},$ and I want to find out how much of the unit $9$-ball the $9$-dimensional volume of the region in which $A$ is stable (meaning all its eigenvalues have negative real part) takes up. However, since this region is a cone, it suffices to look at how much of the surface of the $9$-ball the region takes up, so we can look at $\sum_{i,j=1}^3 a_{ij}^2=1.$ We can prove (see below) that $A$ is stable iff $\mathrm{tr}(A)<0$ $\det(A)<0$ $\mathrm{tr}^3(A)<\mathrm{tr}(A^3)$ are all met. To be more precise, if we let $\mathcal{C}$ denote the set of all $3 \times 3$ matrices satisfying all four of the above conditions and let $\mathcal{S}^8$ denote the $8$-sphere (i.e. the surface of the unit $9$-ball), I'd like to find  $$R=\frac{\mathrm{vol}(\mathcal{C})}{\mathrm{vol}(\mathcal{S}^8)}.$$ But how would I set up the integrals for actually finding this? Or, better than messing with integrals, can we perhaps infer this ratio from looking at the symmetries of conditions 2-4? From simulations, $R\sim 0.1045.$ Any help is much appreciated. For instance, what would the geometric interpretations of conditions 2-4 be? Proof: (Not necessary reading) The (monic) characteristic polynomial for $A$ is  $$p_3(z)=z^3+c_2z^2+c_1z+c_0,$$ where \begin{align} c_0 &= -\det A\\ c_1 &= \frac{1}{2}\left[\mathrm{tr}^2(A)-\mathrm{tr}(A^2)\right]\\ c_2 &= -\mathrm{tr}A.\\ \end{align} $A$ is stable iff the characteristic polynomial satisfies The Hurwitz Stability Criterion . Written out for the present case, it reduces to \begin{align} \Delta_1&=c_2         && \mkern-18mu \mkern-18mu >0 \\ \Delta_2&=c_1c_2-c_0  && \mkern-18mu \mkern-18mu >0 \\ \Delta_3&=c_0\Delta_2 && \mkern-18mu \mkern-18mu >0, \end{align} of which the two first can be written as  $\mathrm{tr}(A)<0$ and $\mathrm{tr}^3(A)<\mathrm{tr}(A^3).$ Now, we'd like to reduce the third inequality. The idea is that we know that $\det(A)<0$ is necessary for $A$ being stable (since the determinant of a matrix is the product of its eigenvalues). This means we can divide with $c_0$ without changing the inequality, at which point the third inequality reduces to the second. Note that we're ignoring singular matrices, since their contribution to the volume is zero anyway.  $\Box$","EDIT: At this point, geometric interpretations of conditions 2-4 would qualify as an answer. This can include symmetries of the region. I have a real $3 \times 3$ matrix $A$ with entries $a_{ij},$ and I want to find out how much of the unit $9$-ball the $9$-dimensional volume of the region in which $A$ is stable (meaning all its eigenvalues have negative real part) takes up. However, since this region is a cone, it suffices to look at how much of the surface of the $9$-ball the region takes up, so we can look at $\sum_{i,j=1}^3 a_{ij}^2=1.$ We can prove (see below) that $A$ is stable iff $\mathrm{tr}(A)<0$ $\det(A)<0$ $\mathrm{tr}^3(A)<\mathrm{tr}(A^3)$ are all met. To be more precise, if we let $\mathcal{C}$ denote the set of all $3 \times 3$ matrices satisfying all four of the above conditions and let $\mathcal{S}^8$ denote the $8$-sphere (i.e. the surface of the unit $9$-ball), I'd like to find  $$R=\frac{\mathrm{vol}(\mathcal{C})}{\mathrm{vol}(\mathcal{S}^8)}.$$ But how would I set up the integrals for actually finding this? Or, better than messing with integrals, can we perhaps infer this ratio from looking at the symmetries of conditions 2-4? From simulations, $R\sim 0.1045.$ Any help is much appreciated. For instance, what would the geometric interpretations of conditions 2-4 be? Proof: (Not necessary reading) The (monic) characteristic polynomial for $A$ is  $$p_3(z)=z^3+c_2z^2+c_1z+c_0,$$ where \begin{align} c_0 &= -\det A\\ c_1 &= \frac{1}{2}\left[\mathrm{tr}^2(A)-\mathrm{tr}(A^2)\right]\\ c_2 &= -\mathrm{tr}A.\\ \end{align} $A$ is stable iff the characteristic polynomial satisfies The Hurwitz Stability Criterion . Written out for the present case, it reduces to \begin{align} \Delta_1&=c_2         && \mkern-18mu \mkern-18mu >0 \\ \Delta_2&=c_1c_2-c_0  && \mkern-18mu \mkern-18mu >0 \\ \Delta_3&=c_0\Delta_2 && \mkern-18mu \mkern-18mu >0, \end{align} of which the two first can be written as  $\mathrm{tr}(A)<0$ and $\mathrm{tr}^3(A)<\mathrm{tr}(A^3).$ Now, we'd like to reduce the third inequality. The idea is that we know that $\det(A)<0$ is necessary for $A$ being stable (since the determinant of a matrix is the product of its eigenvalues). This means we can divide with $c_0$ without changing the inequality, at which point the third inequality reduces to the second. Note that we're ignoring singular matrices, since their contribution to the volume is zero anyway.  $\Box$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'volume', 'control-theory']"
56,Linear functional equation,Linear functional equation,,"During my mathematical musings I encountered the following functional equation : denote by $L$ the set of all functions ${\mathbb Z}^2 \to {\mathbb C}$ satisfying $$ \begin{array}{cl} &f(x+a,y+b)+f(x+b,y+c)+f(x+c,y+a) \\ =& f(x+a,y+c)+f(x+b,y+a)+f(x+c,y+b)  \end{array}\tag{1} $$ for any $x,y,a,b,c\in{\mathbb Z}$. The following three classes of functions are clearly solutions : $$ f(x,y)=g(x), \ f(x,y)=h(y), \ f(x,y)=i(x+y) \tag{2} $$ where $g,h,i$ are arbitrary functions ${\mathbb Z}\to {\mathbb C}$. Are there other solutions besides linear combinations of those ?","During my mathematical musings I encountered the following functional equation : denote by $L$ the set of all functions ${\mathbb Z}^2 \to {\mathbb C}$ satisfying $$ \begin{array}{cl} &f(x+a,y+b)+f(x+b,y+c)+f(x+c,y+a) \\ =& f(x+a,y+c)+f(x+b,y+a)+f(x+c,y+b)  \end{array}\tag{1} $$ for any $x,y,a,b,c\in{\mathbb Z}$. The following three classes of functions are clearly solutions : $$ f(x,y)=g(x), \ f(x,y)=h(y), \ f(x,y)=i(x+y) \tag{2} $$ where $g,h,i$ are arbitrary functions ${\mathbb Z}\to {\mathbb C}$. Are there other solutions besides linear combinations of those ?",,['linear-algebra']
57,"Tensor Product is associative, distributive, not commutative.","Tensor Product is associative, distributive, not commutative.",,"Tensor Product is associative, distributive, not commutative. Here is my attempt to show tensor product is associative, is it legit? If $T$ is a $p$-tensor and $S$ a $q$ tensor, then $T \otimes S$ is a $p+q$ tensor: $$T \otimes S(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}) = T(v_1, \ldots, v_p) \cdot S(v_{p+1}, \ldots, v_{p+q}).$$ Now consider a third tensor, a $r$-tensor $U$. Then  \begin{eqnarray*} & & (T \otimes S ) \otimes U (v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}, v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &(T \otimes S (v_1, \ldots, v_{p+q})) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &T (v_1, \ldots, v_{p}) \cdot S(v_{p+1}, \ldots, v_{p+q}) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &T (v_1, \ldots, v_{p}) \cdot (S(v_{p+1}, \ldots, v_{p+q}) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r}))\\ &= &T (v_1, \ldots, v_{p}) \cdot (S \otimes U (v_{p+1}, \ldots, v_{p+q+r}))\\ &= &T \otimes (S \otimes U)(v_1, \ldots, v_{p+q+r}). \end{eqnarray*} Distributivity - is this proof legit? First note that tensor product distribute over addition. If $T$ is a $p$-tensor and $S, U$ a $q$ tensor. Then I need to show that $$ T \otimes (S + U) = (T \otimes U) + (S \otimes U).$$ Then \begin{eqnarray*} &&T \otimes (S + U)(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q})\\ & =& T (v_1, \ldots, v_p) \cdot (S + U)(v_{p+1}, \ldots, v_{p+q})\\ & =& T (v_1, \ldots, v_p) \cdot (S (v_{p+1}, \ldots, v_{p+q}) + U(v_{p+1}, \ldots, v_{p+q}))\\ & =& T (v_1, \ldots, v_p) \cdot S (v_{p+1}, \ldots, v_{p+q}) +  T(v_1, \ldots, v_p) \cdot U(v_{p+1}, \ldots, v_{p+q})\\ & =& (T \otimes S) + (T \otimes U) \end{eqnarray*} It is not commutative - is this legit? \begin{eqnarray*} & & T \otimes S(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q})  = T(v_1, \ldots, v_p) \cdot S(v_{p+1}, \ldots, v_{p+q})\\ &= &S(v_{p+1}, \ldots, v_{p+q}) \cdot T(v_1, \ldots, v_p) = S \otimes T(v_{p+1}, \ldots, v_{p+q}, v_{1}, \ldots, v_{p})\\ & \neq & S \otimes T(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}). \end{eqnarray*}","Tensor Product is associative, distributive, not commutative. Here is my attempt to show tensor product is associative, is it legit? If $T$ is a $p$-tensor and $S$ a $q$ tensor, then $T \otimes S$ is a $p+q$ tensor: $$T \otimes S(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}) = T(v_1, \ldots, v_p) \cdot S(v_{p+1}, \ldots, v_{p+q}).$$ Now consider a third tensor, a $r$-tensor $U$. Then  \begin{eqnarray*} & & (T \otimes S ) \otimes U (v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}, v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &(T \otimes S (v_1, \ldots, v_{p+q})) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &T (v_1, \ldots, v_{p}) \cdot S(v_{p+1}, \ldots, v_{p+q}) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r})\\ &= &T (v_1, \ldots, v_{p}) \cdot (S(v_{p+1}, \ldots, v_{p+q}) \cdot U(v_{p+q+1}, \ldots, v_{p+q+r}))\\ &= &T (v_1, \ldots, v_{p}) \cdot (S \otimes U (v_{p+1}, \ldots, v_{p+q+r}))\\ &= &T \otimes (S \otimes U)(v_1, \ldots, v_{p+q+r}). \end{eqnarray*} Distributivity - is this proof legit? First note that tensor product distribute over addition. If $T$ is a $p$-tensor and $S, U$ a $q$ tensor. Then I need to show that $$ T \otimes (S + U) = (T \otimes U) + (S \otimes U).$$ Then \begin{eqnarray*} &&T \otimes (S + U)(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q})\\ & =& T (v_1, \ldots, v_p) \cdot (S + U)(v_{p+1}, \ldots, v_{p+q})\\ & =& T (v_1, \ldots, v_p) \cdot (S (v_{p+1}, \ldots, v_{p+q}) + U(v_{p+1}, \ldots, v_{p+q}))\\ & =& T (v_1, \ldots, v_p) \cdot S (v_{p+1}, \ldots, v_{p+q}) +  T(v_1, \ldots, v_p) \cdot U(v_{p+1}, \ldots, v_{p+q})\\ & =& (T \otimes S) + (T \otimes U) \end{eqnarray*} It is not commutative - is this legit? \begin{eqnarray*} & & T \otimes S(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q})  = T(v_1, \ldots, v_p) \cdot S(v_{p+1}, \ldots, v_{p+q})\\ &= &S(v_{p+1}, \ldots, v_{p+q}) \cdot T(v_1, \ldots, v_p) = S \otimes T(v_{p+1}, \ldots, v_{p+q}, v_{1}, \ldots, v_{p})\\ & \neq & S \otimes T(v_1, \ldots, v_p, v_{p+1}, \ldots, v_{p+q}). \end{eqnarray*}",,"['linear-algebra', 'tensor-products']"
58,Inverse of Toeplitz Matrix Property,Inverse of Toeplitz Matrix Property,,"Sorry if this question has been asked already but I didn't find it.  Given a symmetric Toeplitz matrix of the form $$\left[\begin{array}{llll} a_0 & a_1 & \dots & a_n\\ a_1 & a_0 & \dots & a_{n-1}\\ \vdots& & & \\ a_n & a_{n-1} & \dots & a_0\\ \end{array} \right]$$ Suppose we have the relation $a_0\geq a_1\geq\dots\geq a_n$.  A simple $3 \times 3$ example will show you that the inverse need not have this property (even in terms of absolute value).  But does it at least have the property that away from the main diagonal, the difference decreases?  That is, do we have $|a_1-a_0|\geq|a_2-a_1|\geq\dots\geq|a_n-a_{n-1}|$? After some experimenting, it seems like we might be able to expect this, but I am wondering if this is known already as I know very little about Toeplitz forms?  A followup question I have is if this is true, then is it ""easy"" to see that it should remain true for bi-infinite Toeplitz matrices of the same form? The actual matrix I am interested in, for what it's worth, is $A_\lambda = (e^{-\lambda|j-k|^2})_{j,k\in\mathbb{Z}}$.","Sorry if this question has been asked already but I didn't find it.  Given a symmetric Toeplitz matrix of the form $$\left[\begin{array}{llll} a_0 & a_1 & \dots & a_n\\ a_1 & a_0 & \dots & a_{n-1}\\ \vdots& & & \\ a_n & a_{n-1} & \dots & a_0\\ \end{array} \right]$$ Suppose we have the relation $a_0\geq a_1\geq\dots\geq a_n$.  A simple $3 \times 3$ example will show you that the inverse need not have this property (even in terms of absolute value).  But does it at least have the property that away from the main diagonal, the difference decreases?  That is, do we have $|a_1-a_0|\geq|a_2-a_1|\geq\dots\geq|a_n-a_{n-1}|$? After some experimenting, it seems like we might be able to expect this, but I am wondering if this is known already as I know very little about Toeplitz forms?  A followup question I have is if this is true, then is it ""easy"" to see that it should remain true for bi-infinite Toeplitz matrices of the same form? The actual matrix I am interested in, for what it's worth, is $A_\lambda = (e^{-\lambda|j-k|^2})_{j,k\in\mathbb{Z}}$.",,"['linear-algebra', 'matrices', 'operator-theory', 'toeplitz-matrices']"
59,Why the determinant of a matrix with the sum of each row's elements equal 0 is 0?,Why the determinant of a matrix with the sum of each row's elements equal 0 is 0?,,"I'm trying to understand the proof of a problem, but I'm stuck. In my book they consider that if all lines of a matrix has sum 0 then it's determinant is also 0. I checked some random examples and it's true, but I couldn't proof it. Could you help me?","I'm trying to understand the proof of a problem, but I'm stuck. In my book they consider that if all lines of a matrix has sum 0 then it's determinant is also 0. I checked some random examples and it's true, but I couldn't proof it. Could you help me?",,"['linear-algebra', 'matrices']"
60,Find large power of a non-diagonalisable matrix,Find large power of a non-diagonalisable matrix,,"If $A = \begin{bmatrix}1 & 0 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}$, then find $A^{30}$. The problem here is that it has only two eigenvectors, $\begin{bmatrix}0\\1\\1\end{bmatrix}$ corresponding to eigenvalue $1$ and $\begin{bmatrix}0\\1\\-1\end{bmatrix}$ corresponding to eigenvalue $-1$. So, it is not diagonalizable. Is there any other way to compute the power?","If $A = \begin{bmatrix}1 & 0 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}$, then find $A^{30}$. The problem here is that it has only two eigenvectors, $\begin{bmatrix}0\\1\\1\end{bmatrix}$ corresponding to eigenvalue $1$ and $\begin{bmatrix}0\\1\\-1\end{bmatrix}$ corresponding to eigenvalue $-1$. So, it is not diagonalizable. Is there any other way to compute the power?",,"['linear-algebra', 'matrices', 'exponentiation', 'diagonalization']"
61,Are these square matrices always diagonalisable?,Are these square matrices always diagonalisable?,,"When trying to solve a physics problem on decoupling a system of ODEs, I found myself needing to address the following problem: Let $A_n\in M_n(\mathbb R)$ be the matrix with all $1$ s above its main diagonal, all $-1$ s below its diagonal, and $0$ s everywhere else. Is $A_n$ always diagonalisable? If so, what is its diagonalisation (equivalently: what are its eigenvalues and corresponding eigenvectors)? For example, $$A_3=\begin{bmatrix}0&1&0\\-1&0&1\\0&-1&0\end{bmatrix},\quad A_5=\begin{bmatrix}0&1&0&0&0\\-1&0&1&0&0\\0&-1&0&1&0\\0&0&-1&0&1\\0&0&0&-1&0\end{bmatrix}.$$ Assuming my code is correct, Mathematica has been able to verify that $A_n$ is always diagonalisable up to $n=1000$ . If we use $\chi_n(t)\in\mathbb Z[t]$ to denote the characteristic polynomial of $A_n$ , a straightforward evaluation also shows that $$\chi_n(t)=-t\chi_{n-1}(t)+\chi_{n-2}(t)\tag{1}$$ for all $n\geq4$ . Furthermore, note that $A_n=-A_n^t$ so that, in the case where the dimension is even, $$\det(A_{2n}-\lambda I)=\det(A_{2n}^t-\lambda I)=\det(-A_{2n}-\lambda I)=\det(A_{2n}+\lambda I).$$ This implies that whenever $\lambda$ is an eigenvalue of $A_{2n}$ , so is $-\lambda$ . In other words, $\chi_{2n}(t)$ is always of the form $(t^2-\lambda _1^2)(t^2-\lambda_2^2)\dotsm(t^2-\lambda_n^2)$ for some $\lambda_i$ . And this is where I am stuck. In order for $A_n$ to be diagonalisable, we must have that all the eigenvalues are distinct, but trying to use the recurrence $(1)$ and strong induction, or trying to use the formula for the even case have not helped at all. It seems like the most probable line of attack would be to somehow show that $$\chi_{2n}'(t)=2t\sum_{k=1}^n\frac{\chi_{2n}(t)}{t^2-\lambda_k^2}$$ never shares a common zero with $\chi_{2n}$ (which would resolve the even case), though I don't see how to make this work. Note: I do not have any clue how to actually find the eigenvalues/eigenvectors even in the case where the $A_n$ are diagonalisable. As such even if someone cannot answer the second part of the question, but can prove that the $A_n$ are diagonalisable, I would appreciate that as an answer as well. Above I tried to look at the special case where the dimension is even, though of course the proof for all odd and even $n$ is more valuable. Even if this is not possible, for my purposes I just need an unbounded subset $S\subseteq\mathbb Z$ for which the conclusion is proven for $n\in S$ , so any such approach is welcome too. Thank you in advance!","When trying to solve a physics problem on decoupling a system of ODEs, I found myself needing to address the following problem: Let be the matrix with all s above its main diagonal, all s below its diagonal, and s everywhere else. Is always diagonalisable? If so, what is its diagonalisation (equivalently: what are its eigenvalues and corresponding eigenvectors)? For example, Assuming my code is correct, Mathematica has been able to verify that is always diagonalisable up to . If we use to denote the characteristic polynomial of , a straightforward evaluation also shows that for all . Furthermore, note that so that, in the case where the dimension is even, This implies that whenever is an eigenvalue of , so is . In other words, is always of the form for some . And this is where I am stuck. In order for to be diagonalisable, we must have that all the eigenvalues are distinct, but trying to use the recurrence and strong induction, or trying to use the formula for the even case have not helped at all. It seems like the most probable line of attack would be to somehow show that never shares a common zero with (which would resolve the even case), though I don't see how to make this work. Note: I do not have any clue how to actually find the eigenvalues/eigenvectors even in the case where the are diagonalisable. As such even if someone cannot answer the second part of the question, but can prove that the are diagonalisable, I would appreciate that as an answer as well. Above I tried to look at the special case where the dimension is even, though of course the proof for all odd and even is more valuable. Even if this is not possible, for my purposes I just need an unbounded subset for which the conclusion is proven for , so any such approach is welcome too. Thank you in advance!","A_n\in M_n(\mathbb R) 1 -1 0 A_n A_3=\begin{bmatrix}0&1&0\\-1&0&1\\0&-1&0\end{bmatrix},\quad A_5=\begin{bmatrix}0&1&0&0&0\\-1&0&1&0&0\\0&-1&0&1&0\\0&0&-1&0&1\\0&0&0&-1&0\end{bmatrix}. A_n n=1000 \chi_n(t)\in\mathbb Z[t] A_n \chi_n(t)=-t\chi_{n-1}(t)+\chi_{n-2}(t)\tag{1} n\geq4 A_n=-A_n^t \det(A_{2n}-\lambda I)=\det(A_{2n}^t-\lambda I)=\det(-A_{2n}-\lambda I)=\det(A_{2n}+\lambda I). \lambda A_{2n} -\lambda \chi_{2n}(t) (t^2-\lambda _1^2)(t^2-\lambda_2^2)\dotsm(t^2-\lambda_n^2) \lambda_i A_n (1) \chi_{2n}'(t)=2t\sum_{k=1}^n\frac{\chi_{2n}(t)}{t^2-\lambda_k^2} \chi_{2n} A_n A_n n S\subseteq\mathbb Z n\in S","['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization', 'tridiagonal-matrices', 'toeplitz-matrices']"
62,How many ways are there to prove Cayley-Hamilton Theorem?,How many ways are there to prove Cayley-Hamilton Theorem?,,"I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?","I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?",,"['linear-algebra', 'abstract-algebra']"
63,Calculate minimal polynomial of a matrix,Calculate minimal polynomial of a matrix,,\begin{bmatrix}0&1&0&1\\1&0&1&0\\0&1&0&1\\1&0&1&0\end{bmatrix} I have calculated characteristic polynomial as $x^2(x^2-4)$ but I don't know what is minimal polynomial please solve,\begin{bmatrix}0&1&0&1\\1&0&1&0\\0&1&0&1\\1&0&1&0\end{bmatrix} I have calculated characteristic polynomial as $x^2(x^2-4)$ but I don't know what is minimal polynomial please solve,,"['linear-algebra', 'matrices', 'minimal-polynomials']"
64,Why can the determinant be assumed to be 0?,Why can the determinant be assumed to be 0?,,"I'm trying to work through how to calculate eigenvalues and eigenvectors. I start with $$Ax=\lambda x$$ Where $A$ is a $p \times p$ matrix, $\lambda$ is the eigenvalue and $x$ is the eigenvector. This is the same as: $$Ax=I\lambda x$$ $$Ax-I\lambda x=0$$ $$(A-I\lambda) x=0$$ We define the matrix $A$ as a $2 \times 2$ matrix: $\begin{bmatrix}4 & -2\\-3 & 6\end{bmatrix}$ Thus this -$I\lambda$ equals $\begin{bmatrix}4-\lambda & -2\\-3 & 6-\lambda\end{bmatrix}$ $$Det(A-I\lambda)=(4-\lambda(6-\lambda)-(-3)*-2)$$ $$Det(A-I\lambda)=24-10\lambda +\lambda^2 -6$$ $$Det(A-I\lambda)=18 - 10\lambda + \lambda^2 $$ Then, out of the blue my textbook claims that $$0=30 - 10\lambda + \lambda^2 $$ How do I justify setting the determinant to $0$? (I do ""not"" have an advanced knowledge in linear algebraic analysis, I only know how the determinant is used to calculate the inverse matrix)","I'm trying to work through how to calculate eigenvalues and eigenvectors. I start with $$Ax=\lambda x$$ Where $A$ is a $p \times p$ matrix, $\lambda$ is the eigenvalue and $x$ is the eigenvector. This is the same as: $$Ax=I\lambda x$$ $$Ax-I\lambda x=0$$ $$(A-I\lambda) x=0$$ We define the matrix $A$ as a $2 \times 2$ matrix: $\begin{bmatrix}4 & -2\\-3 & 6\end{bmatrix}$ Thus this -$I\lambda$ equals $\begin{bmatrix}4-\lambda & -2\\-3 & 6-\lambda\end{bmatrix}$ $$Det(A-I\lambda)=(4-\lambda(6-\lambda)-(-3)*-2)$$ $$Det(A-I\lambda)=24-10\lambda +\lambda^2 -6$$ $$Det(A-I\lambda)=18 - 10\lambda + \lambda^2 $$ Then, out of the blue my textbook claims that $$0=30 - 10\lambda + \lambda^2 $$ How do I justify setting the determinant to $0$? (I do ""not"" have an advanced knowledge in linear algebraic analysis, I only know how the determinant is used to calculate the inverse matrix)",,"['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'determinant']"
65,Any Nilpotent Matrix is not Diagonalizable,Any Nilpotent Matrix is not Diagonalizable,,"I'm trying to go about the proof that any matrix that is nilpotent (i.e. $\exists N \in\Bbb N. A^N = \mathbf{0}$ ) cannot be diagonalizable. I believe that the best way to go about this is by showing that a given eigenvalue's geometric multiplicity is not the same as its algebraic multiplicity. However, I am having some difficulty figuring out what aspect of nilpotency might help me with this calculation. I can see that if $A^N = \mathbf{0}$ for some $N \in\Bbb N,$ I think it may be reasonable to prove that the only eigenvectors we get from $A$ are the $\mathbf{0}$ vector, although I am not entirely sure if this is the appropriate way to prove the above statement, given that it doesn't necessarily pertain to multiplicity but more of the mechanics of finding eigenvectors and eigenvalues of a matrix. Any recommendations on this problem?","I'm trying to go about the proof that any matrix that is nilpotent (i.e. ) cannot be diagonalizable. I believe that the best way to go about this is by showing that a given eigenvalue's geometric multiplicity is not the same as its algebraic multiplicity. However, I am having some difficulty figuring out what aspect of nilpotency might help me with this calculation. I can see that if for some I think it may be reasonable to prove that the only eigenvectors we get from are the vector, although I am not entirely sure if this is the appropriate way to prove the above statement, given that it doesn't necessarily pertain to multiplicity but more of the mechanics of finding eigenvectors and eigenvalues of a matrix. Any recommendations on this problem?","\exists N \in\Bbb N. A^N = \mathbf{0} A^N = \mathbf{0} N \in\Bbb N, A \mathbf{0}","['linear-algebra', 'matrices', 'diagonalization']"
66,Must an injective or surjective map in an infinite dimensional vector space be a bijection?,Must an injective or surjective map in an infinite dimensional vector space be a bijection?,,"If we have some finite dimensional vector space V and linear transformation $F: V\to V$, and we know that F is injective, we immediately know that it is also bijective (same goes if we know that F is surjective). I'm curious if a same rule applies if V is infinite dimensional vector space, and we know that F is injective/surjective, does it again immediately imply that F is also bijective (intuitively I think it does)?","If we have some finite dimensional vector space V and linear transformation $F: V\to V$, and we know that F is injective, we immediately know that it is also bijective (same goes if we know that F is surjective). I'm curious if a same rule applies if V is infinite dimensional vector space, and we know that F is injective/surjective, does it again immediately imply that F is also bijective (intuitively I think it does)?",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'examples-counterexamples']"
67,Is matrix $A^TA$ always symmetric?,Is matrix  always symmetric?,A^TA,"Through experience, I've seen that the following statement holds true: "" $A^TA$ is always a symmetric matrix?"", where $A$ is any matrix. However can this statement be proven/falsified?","Through experience, I've seen that the following statement holds true: "" is always a symmetric matrix?"", where is any matrix. However can this statement be proven/falsified?",A^TA A,"['linear-algebra', 'matrices', 'transpose']"
68,"Why is it called ""Orthogonal Projection""? Why not just ""Projection""?","Why is it called ""Orthogonal Projection""? Why not just ""Projection""?",,"Right now, we are learning decomposing vectors, but something I don't understand is the names given to this stuff For instance, in the text, the parallel component of y is said to be the orthogonal projection of y onto u . This makes no sense to me. Why is the word ""orthogonal"" even in there in the first place? I think I understand why they use ""orthogonal"" for z, but it makes no sense to me when they could just call it ""orthogonal""","Right now, we are learning decomposing vectors, but something I don't understand is the names given to this stuff For instance, in the text, the parallel component of y is said to be the orthogonal projection of y onto u . This makes no sense to me. Why is the word ""orthogonal"" even in there in the first place? I think I understand why they use ""orthogonal"" for z, but it makes no sense to me when they could just call it ""orthogonal""",,['linear-algebra']
69,Connection between cross product and determinant,Connection between cross product and determinant,,"When I calculate a cross product of two vectors in Cartesian coordinates, I calculate something that seems like the determinant of a 2x2 matrix. Is there any connection between the determinant and the cross product?","When I calculate a cross product of two vectors in Cartesian coordinates, I calculate something that seems like the determinant of a 2x2 matrix. Is there any connection between the determinant and the cross product?",,"['linear-algebra', 'matrices', 'determinant', 'cross-product']"
70,"Determinant of a specific circulant matrix, $A_n$","Determinant of a specific circulant matrix,",A_n,"Let $$A_2 = \left[ \begin{array}{cc} 0 & 1\\ 1 & 0 \end{array}\right]$$ $$A_3 = \left[ \begin{array}{ccc} 0 & 1 & 1\\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{array}\right]$$ $$A_4 = \left[ \begin{array}{cccc} 0 & 1 & 1 & 1\\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 0\end{array}\right]$$ and so on for $A_n$. I was asked to calculate the determinant for $A_1, A_2, A_3, A_4$ and then guess about the determinant for $A_n$ in general. Of course the pattern is clear that $$ \det A_n = (n-1)(-1)^{n-1} $$ but I was wondering as to what the proof of this is. I tried to be clever with cofactor expansions but I couldn't get anywhere. Could someone explain it to me please?","Let $$A_2 = \left[ \begin{array}{cc} 0 & 1\\ 1 & 0 \end{array}\right]$$ $$A_3 = \left[ \begin{array}{ccc} 0 & 1 & 1\\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{array}\right]$$ $$A_4 = \left[ \begin{array}{cccc} 0 & 1 & 1 & 1\\ 1 & 0 & 1 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 1 & 1 & 0\end{array}\right]$$ and so on for $A_n$. I was asked to calculate the determinant for $A_1, A_2, A_3, A_4$ and then guess about the determinant for $A_n$ in general. Of course the pattern is clear that $$ \det A_n = (n-1)(-1)^{n-1} $$ but I was wondering as to what the proof of this is. I tried to be clever with cofactor expansions but I couldn't get anywhere. Could someone explain it to me please?",,"['linear-algebra', 'matrices', 'determinant', 'circulant-matrices']"
71,What is the difference between linearly and affinely independent vectors?,What is the difference between linearly and affinely independent vectors?,,What is the difference between linearly and affinely independent vectors? Why does affine independence not imply linear independence necessarily? Can someone explain using an example?,What is the difference between linearly and affinely independent vectors? Why does affine independence not imply linear independence necessarily? Can someone explain using an example?,,"['linear-algebra', 'vector-spaces', 'vectors', 'convex-geometry', 'affine-geometry']"
72,How to follow matrix operations in proofs?,How to follow matrix operations in proofs?,,"I'm a software engineer trying to learn linear algebra and feel like I'm having a hard time following matrix computations. For example, this is a part of the least squared method for linear model: $$\sum\limits_{i=1}^n ||\mathbf\theta^T\mathbf x_i-y_i||^2=(\mathbf{X\theta}-\mathbf y)^T(\mathbf{X\theta}-\mathbf y).$$ How do we jump from the first line, where there's a lot going on like Sigma $i=1\to n$, norm squared, $x_i$, $y_i$, etc., to the second line where those are wrapped nicely in that matrix representation with transpose thing? I know that can arrive at the second line if I carefully write down, try playing with concrete matrices, and I'm very slow with this. Is there any other way to reason, or visualize it? How do mathematicians tackle this kind of thing? Or everyone's kind of struggle with it privately too?","I'm a software engineer trying to learn linear algebra and feel like I'm having a hard time following matrix computations. For example, this is a part of the least squared method for linear model: $$\sum\limits_{i=1}^n ||\mathbf\theta^T\mathbf x_i-y_i||^2=(\mathbf{X\theta}-\mathbf y)^T(\mathbf{X\theta}-\mathbf y).$$ How do we jump from the first line, where there's a lot going on like Sigma $i=1\to n$, norm squared, $x_i$, $y_i$, etc., to the second line where those are wrapped nicely in that matrix representation with transpose thing? I know that can arrive at the second line if I carefully write down, try playing with concrete matrices, and I'm very slow with this. Is there any other way to reason, or visualize it? How do mathematicians tackle this kind of thing? Or everyone's kind of struggle with it privately too?",,"['linear-algebra', 'matrices', 'matrix-equations']"
73,Matrix multiplication notation,Matrix multiplication notation,,"This is from my textbook: If $A=(a_{ij})\in M_{mn}(\Bbb F), B=(b_{ij})\in M_{np}(\Bbb F)$ then $C=A\times B=(c_{ij})\in M_{mp}(\Bbb F)$.   $c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}$ where $i=1,...m, j=1,...p$ I know how to multiply matrices but I don't understand this notation : $c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}$ Can someone explain what that represents by giving me an example? And how did we get that formula?","This is from my textbook: If $A=(a_{ij})\in M_{mn}(\Bbb F), B=(b_{ij})\in M_{np}(\Bbb F)$ then $C=A\times B=(c_{ij})\in M_{mp}(\Bbb F)$.   $c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}$ where $i=1,...m, j=1,...p$ I know how to multiply matrices but I don't understand this notation : $c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}$ Can someone explain what that represents by giving me an example? And how did we get that formula?",,"['linear-algebra', 'matrices', 'notation']"
74,Is $\det(AB) =\det(BA)$ [closed],Is  [closed],\det(AB) =\det(BA),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $A, B$ be square matrices. I am having trouble proving if $$ \det(AB) = \det(BA) $$ is right or wrong. Can you please point me to the right direction?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let be square matrices. I am having trouble proving if is right or wrong. Can you please point me to the right direction?","A, B  \det(AB) = \det(BA) ","['linear-algebra', 'matrices', 'determinant']"
75,Largest eigenvalue of a real symmetric matrix,Largest eigenvalue of a real symmetric matrix,,"If $\lambda$ is the largest eigenvalue of a real symmetric $n \times n$ matrix $H$, how can I show that: $$\forall v \in \mathbb{R^n}, ||v||=1 \implies v^tHv\leq \lambda$$ Thank you.","If $\lambda$ is the largest eigenvalue of a real symmetric $n \times n$ matrix $H$, how can I show that: $$\forall v \in \mathbb{R^n}, ||v||=1 \implies v^tHv\leq \lambda$$ Thank you.",,[]
76,Are all symmetric matrices ​invertible?,Are all symmetric matrices ​invertible?,,"Is any symmetric matrix ​​invertible? I'm trying to prove this theoretical question, but I don't know what I need to do. I apologize for the simple question, but I'm in doubt and need clarification.","Is any symmetric matrix ​​invertible? I'm trying to prove this theoretical question, but I don't know what I need to do. I apologize for the simple question, but I'm in doubt and need clarification.",,['linear-algebra']
77,What exactly is standard basis?,What exactly is standard basis?,,I am confused about the difference between coordinates and basis. My confusion is following: Let $e_i$ denote the standard basis and $v_i$ denote a non-standard basis of a finite $n$-dimensional vector space $V$. Then $e_i = (\delta_{ij})$ (all entries zero except $i$-th). But: the coordinates of $v_i$  with respect to basis $v_i$ also are $(\delta_{ij})$. Now the definition of standard basis becomes circular: it is supposed to be the basis with vectors $(\delta_{ij})$ but every basis vector has these coordinates with respect to itself. So what exactly is standard basis?,I am confused about the difference between coordinates and basis. My confusion is following: Let $e_i$ denote the standard basis and $v_i$ denote a non-standard basis of a finite $n$-dimensional vector space $V$. Then $e_i = (\delta_{ij})$ (all entries zero except $i$-th). But: the coordinates of $v_i$  with respect to basis $v_i$ also are $(\delta_{ij})$. Now the definition of standard basis becomes circular: it is supposed to be the basis with vectors $(\delta_{ij})$ but every basis vector has these coordinates with respect to itself. So what exactly is standard basis?,,['linear-algebra']
78,Is the inverse of a linear transformation linear as well?,Is the inverse of a linear transformation linear as well?,,"A question from the field of Linear Algebra. If I have a linear transformation $T$ that is one-to-one and onto, would that mean that the $T^{-1}$ will also be linear? If so, is there any general proof for it? Thanks","A question from the field of Linear Algebra. If I have a linear transformation $T$ that is one-to-one and onto, would that mean that the $T^{-1}$ will also be linear? If so, is there any general proof for it? Thanks",,['linear-algebra']
79,Are the matrix products $AB$ and $BA$ similar?,Are the matrix products  and  similar?,AB BA,"Given two matrices $A,B.$ On what conditions does $AB \sim BA$ hold?","Given two matrices $A,B.$ On what conditions does $AB \sim BA$ hold?",,"['linear-algebra', 'matrices']"
80,How to show that the nth power of a $n \times n$ nilpotent matrix equals to zero $A^n=0$,How to show that the nth power of a  nilpotent matrix equals to zero,n \times n A^n=0,"$A$ is a $n\times n$ matrix such that $ A^m = 0 $ for some positive integer $m$ .   Show that $A^n = 0$ . My attempt: For $n > m$ , it's obvious since matrix multiplication is associative. For $n < m$ , $A^n\times A^{m-n} = 0$ ; not sure what to do next. Also I know that $\det A = 0$ .","is a matrix such that for some positive integer .   Show that . My attempt: For , it's obvious since matrix multiplication is associative. For , ; not sure what to do next. Also I know that .",A n\times n  A^m = 0  m A^n = 0 n > m n < m A^n\times A^{m-n} = 0 \det A = 0,"['linear-algebra', 'matrices', 'nilpotence']"
81,Find all matrices that commute with $\left(\begin{smallmatrix}2&3\\1&4\end{smallmatrix}\right)$,Find all matrices that commute with,\left(\begin{smallmatrix}2&3\\1&4\end{smallmatrix}\right),"Find all $2\times 2$ matrices that commute with $$\left( \begin{array}{cc} 2 & 3 \\ 1 & 4 \end{array} \right)$$ My progress: I know that a square matrix commutes with itself, the identity matrix of that order, the null matrix of that order and any scalar matrix of that order. The answer has been given as: $$\left( \begin{array}{cc} m & 3n \\ n & m+2n \end{array} \right)$$ I don't understand how they're getting that form. Can someone please explain?","Find all $2\times 2$ matrices that commute with $$\left( \begin{array}{cc} 2 & 3 \\ 1 & 4 \end{array} \right)$$ My progress: I know that a square matrix commutes with itself, the identity matrix of that order, the null matrix of that order and any scalar matrix of that order. The answer has been given as: $$\left( \begin{array}{cc} m & 3n \\ n & m+2n \end{array} \right)$$ I don't understand how they're getting that form. Can someone please explain?",,"['linear-algebra', 'matrices']"
82,Find the characteristic polynomial of the inverse of a matrix,Find the characteristic polynomial of the inverse of a matrix,,"Given the characteristic polynomial $\chi_A$ of an invertible matrix $A$, I'm to find $\chi_{A^{-1}}$. I can see that this is theoretically possible. $\chi_A$ uniquely determines the similarity class of $A$, which uniquely determines the similarity class of $A^{-1}$, which uniquely determines $\chi_{A^{-1}}$. Calculating the coefficients of $\chi_{A^{-1}}$ explicitly and then relating them to the coefficients of $\chi_A$ seems unfeasible. I thought about calculating the factors instead, which could be easier since I at least have some idea what the linear factors $\chi_{A^{-1}}$ are (since I can see how to get the eigenvalues of $A^{-1}$ from those of $A$), but that doesn't help me with potential higher-degree irreducibles or repeated linear factors. Also, we're not supposed to know the eigenvalues of $A^{-1}$ , at least I don't think so, since calculating them is the next question. Any hints?","Given the characteristic polynomial $\chi_A$ of an invertible matrix $A$, I'm to find $\chi_{A^{-1}}$. I can see that this is theoretically possible. $\chi_A$ uniquely determines the similarity class of $A$, which uniquely determines the similarity class of $A^{-1}$, which uniquely determines $\chi_{A^{-1}}$. Calculating the coefficients of $\chi_{A^{-1}}$ explicitly and then relating them to the coefficients of $\chi_A$ seems unfeasible. I thought about calculating the factors instead, which could be easier since I at least have some idea what the linear factors $\chi_{A^{-1}}$ are (since I can see how to get the eigenvalues of $A^{-1}$ from those of $A$), but that doesn't help me with potential higher-degree irreducibles or repeated linear factors. Also, we're not supposed to know the eigenvalues of $A^{-1}$ , at least I don't think so, since calculating them is the next question. Any hints?",,"['linear-algebra', 'matrices', 'characteristic-polynomial']"
83,If a Matrix Has Only Zero as an Eigen-Value Then It Is Nilpotent,If a Matrix Has Only Zero as an Eigen-Value Then It Is Nilpotent,,Prove that a matrix with only zero eigenvalues must be nilpotent. How will I be able to prove this?,Prove that a matrix with only zero eigenvalues must be nilpotent. How will I be able to prove this?,,"['linear-algebra', 'matrices']"
84,Inverse of the matrix,Inverse of the matrix,,"$$U(a,b)=\left(\begin{matrix}a&b&b&b\\b&a&b&b\\b&b&a&b\\b&b&b&a\end{matrix}\right)$$ Is there an easy way to find the inverse of $U(1,2)$ , a trick to solve this problem easy?(its part of an exam with answers)","$$U(a,b)=\left(\begin{matrix}a&b&b&b\\b&a&b&b\\b&b&a&b\\b&b&b&a\end{matrix}\right)$$ Is there an easy way to find the inverse of $U(1,2)$ , a trick to solve this problem easy?(its part of an exam with answers)",,"['linear-algebra', 'matrices', 'inverse']"
85,I need an intuitive explanation of eigenvalues and eigenvectors,I need an intuitive explanation of eigenvalues and eigenvectors,,"Been browsing around here for quite a while, and finally took the plunge and signed up. I've started my mathematics major, and am taking a course in Linear Algebra. While I seem to be doing rather well in all the topics covered, such as vectors and matrix manipulation, I am having some trouble understanding the meaning of eigenvalues and eigenvectors. For the life of me, I just cannot wrap my head around any textbook explanation (and I've tried three!), and Google so far just hasn't helped me at all. All I see are problem questions, but no real explanation (and even then most of those are hard to grasp). Could someone be so kind and provide a layman explanation of these terms, and work me through an example? Nothing too hard, seeing as I'm a first year student. Thank you, and I look forward to spending even more time on here!","Been browsing around here for quite a while, and finally took the plunge and signed up. I've started my mathematics major, and am taking a course in Linear Algebra. While I seem to be doing rather well in all the topics covered, such as vectors and matrix manipulation, I am having some trouble understanding the meaning of eigenvalues and eigenvectors. For the life of me, I just cannot wrap my head around any textbook explanation (and I've tried three!), and Google so far just hasn't helped me at all. All I see are problem questions, but no real explanation (and even then most of those are hard to grasp). Could someone be so kind and provide a layman explanation of these terms, and work me through an example? Nothing too hard, seeing as I'm a first year student. Thank you, and I look forward to spending even more time on here!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'intuition']"
86,Eigenvalues of a sum of rank-one matrices,Eigenvalues of a sum of rank-one matrices,,"How can I find the eigenvalues of the sum of rank-one matrices $vv^T + ww^T$? I know that the eigenvalues of each term is $v^Tv$ and $w^Tw$, respectively.","How can I find the eigenvalues of the sum of rank-one matrices $vv^T + ww^T$? I know that the eigenvalues of each term is $v^Tv$ and $w^Tw$, respectively.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
87,What is the most efficient way to find the inverse of large matrix?,What is the most efficient way to find the inverse of large matrix?,,"Let $A$ be a large square $(n+1) \times (n+1)$ invertible matrix, where $n \approx 1000$ . $$A = \begin{bmatrix} -1 & 0 & 0 &\cdots & 0 & a_0\\ 1 & -1 & 0 &\cdots & 0 & a_1\\ 0 & 1 & -1 &\cdots & 0 & a_2\\ \vdots & \vdots & \vdots &\ddots & \vdots & \vdots\\ 0 & 0 & 0 &\cdots & -1 & a_{n-1}\\ 0 & 0 & 0 &\cdots & 1 & a_n-1\\ \end{bmatrix}$$ What is the most efficient way to find its inverse or solve its linear equation? Matrix $A$ is the result of a subtraction of a matrix with the identity matrix. I try to solve this to find the result of the series of a matrix and apparently Gaussian elimination method was not efficient enough.","Let be a large square invertible matrix, where . What is the most efficient way to find its inverse or solve its linear equation? Matrix is the result of a subtraction of a matrix with the identity matrix. I try to solve this to find the result of the series of a matrix and apparently Gaussian elimination method was not efficient enough.","A (n+1) \times (n+1) n \approx 1000 A = \begin{bmatrix}
-1 & 0 & 0 &\cdots & 0 & a_0\\
1 & -1 & 0 &\cdots & 0 & a_1\\
0 & 1 & -1 &\cdots & 0 & a_2\\
\vdots & \vdots & \vdots &\ddots & \vdots & \vdots\\
0 & 0 & 0 &\cdots & -1 & a_{n-1}\\
0 & 0 & 0 &\cdots & 1 & a_n-1\\
\end{bmatrix} A","['linear-algebra', 'matrices', 'systems-of-equations', 'inverse', 'numerical-linear-algebra']"
88,If $A$ is a complex matrix of size $n$ of finite order then is $A$ diagonalizable ?,If  is a complex matrix of size  of finite order then is  diagonalizable ?,A n A,"Let $A$ be a complex matrix of size $n$ if for some positive integer $k$ , $A^k=I_n$ , then is $A$ diagonalizable ?","Let $A$ be a complex matrix of size $n$ if for some positive integer $k$ , $A^k=I_n$ , then is $A$ diagonalizable ?",,"['linear-algebra', 'matrices']"
89,How do I show that a matrix is injective?,How do I show that a matrix is injective?,,"I need to determine whether this matrix is injective  \begin{pmatrix} 2 & 0 & 4\\ 0 & 3 & 0\\ 1 & 7 & 2 \end{pmatrix} Using gaussian elimination, this is what I have done: \begin{pmatrix} 2 & 0 & 4 &|& 0\\ 0 & 3 & 0 &|& 0\\ 1 & 7 & 2 &|& 0 \end{pmatrix} Divide row1 by 2, and then minus row3 by values of row1: \begin{pmatrix} 1 & 0 & 2 &|& 0\\ 0 & 3 & 0 &|& 0\\ 0 & 7 & 0 &|& 0 \end{pmatrix} Divide row 2 by 3, divide row 3 by 7 and minus row 3 by row2: \begin{pmatrix} 1 & 0 & 2 &|& 0\\ 0 & 1 & 0 &|& 0\\ 0 & 0 & 0 &|& 0 \end{pmatrix} Am I doing this correctly? How do I show that the matrix is (not) injective? I was thinking along the lines of ""$x + z \ne 0$.""","I need to determine whether this matrix is injective  \begin{pmatrix} 2 & 0 & 4\\ 0 & 3 & 0\\ 1 & 7 & 2 \end{pmatrix} Using gaussian elimination, this is what I have done: \begin{pmatrix} 2 & 0 & 4 &|& 0\\ 0 & 3 & 0 &|& 0\\ 1 & 7 & 2 &|& 0 \end{pmatrix} Divide row1 by 2, and then minus row3 by values of row1: \begin{pmatrix} 1 & 0 & 2 &|& 0\\ 0 & 3 & 0 &|& 0\\ 0 & 7 & 0 &|& 0 \end{pmatrix} Divide row 2 by 3, divide row 3 by 7 and minus row 3 by row2: \begin{pmatrix} 1 & 0 & 2 &|& 0\\ 0 & 1 & 0 &|& 0\\ 0 & 0 & 0 &|& 0 \end{pmatrix} Am I doing this correctly? How do I show that the matrix is (not) injective? I was thinking along the lines of ""$x + z \ne 0$.""",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
90,Eigenvectors of a matrix and its inverse,Eigenvectors of a matrix and its inverse,,"Show that an $n\times{n}$ invertible matrix A has the same eigenvectors as its inverse. I can recall that the definition of a matrix and its inverse, together with the equation for the eigenvector $x$. But this proof I am not getting a concept to deal with it. $(A-\lambda{I})x=0$ $(A^{-1}-\lambda{I})x=0$ Thank you!","Show that an $n\times{n}$ invertible matrix A has the same eigenvectors as its inverse. I can recall that the definition of a matrix and its inverse, together with the equation for the eigenvector $x$. But this proof I am not getting a concept to deal with it. $(A-\lambda{I})x=0$ $(A^{-1}-\lambda{I})x=0$ Thank you!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
91,Finding the Exponential of a Matrix that is not Diagonalizable,Finding the Exponential of a Matrix that is not Diagonalizable,,"Consider the $3 \times 3$ matrix $$A = \begin{pmatrix}  1 & 1 & 2 \\  0 & 1 & -4 \\  0 & 0 & 1   \end{pmatrix}.$$ I am trying to find $e^{At}$. The only tool I have to find the exponential of a matrix is to diagonalize it. $A$'s eigenvalue is 1. Therefore, $A$ is not diagonalizable. How does one find the exponential of a non-diagonalizable matrix? My attempt: Write $\begin{pmatrix}  1 & 1 & 2 \\  0 & 1 & -4 \\  0 & 0 & 1   \end{pmatrix} = M  + N$, with $M = \begin{pmatrix}  1 & 0 & 0 \\  0 & 1 & 0 \\  0 & 0 & 1   \end{pmatrix}$ and $N = \begin{pmatrix}  0 & 1 & 2 \\  0 & 0 & -4 \\  0 & 0 & 0   \end{pmatrix}$. We have $N^3 = 0$, and therefore $\forall x > 3$, $N^x = 0$.  Thus: $$\begin{aligned} e^{At} &= e^{(M+N)t} = e^{Mt} e^{Nt} \\ &= \begin{pmatrix}  e^t & 0 & 0 \\  0 & e^t & 0 \\  0 & 0 & e^t   \end{pmatrix} \left(I + \begin{pmatrix}  0 & t & 2t \\  0 & 0 & -4t \\  0 & 0 & 0   \end{pmatrix}+\begin{pmatrix}  0 & 0 & -2t^2 \\  0 & 0 & 0 \\  0 & 0 & 0   \end{pmatrix}\right) \\ &= e^t \begin{pmatrix}  1 & t & 2t \\  0 & 1 & -4t \\  0 & 0 & 1   \end{pmatrix} \\ &= \begin{pmatrix}  e^t & te^t & 2t(1-t)e^t \\  0 & e^t & -4te^t \\  0 & 0 & e^t   \end{pmatrix}. \end{aligned}$$ Is that the right answer?","Consider the $3 \times 3$ matrix $$A = \begin{pmatrix}  1 & 1 & 2 \\  0 & 1 & -4 \\  0 & 0 & 1   \end{pmatrix}.$$ I am trying to find $e^{At}$. The only tool I have to find the exponential of a matrix is to diagonalize it. $A$'s eigenvalue is 1. Therefore, $A$ is not diagonalizable. How does one find the exponential of a non-diagonalizable matrix? My attempt: Write $\begin{pmatrix}  1 & 1 & 2 \\  0 & 1 & -4 \\  0 & 0 & 1   \end{pmatrix} = M  + N$, with $M = \begin{pmatrix}  1 & 0 & 0 \\  0 & 1 & 0 \\  0 & 0 & 1   \end{pmatrix}$ and $N = \begin{pmatrix}  0 & 1 & 2 \\  0 & 0 & -4 \\  0 & 0 & 0   \end{pmatrix}$. We have $N^3 = 0$, and therefore $\forall x > 3$, $N^x = 0$.  Thus: $$\begin{aligned} e^{At} &= e^{(M+N)t} = e^{Mt} e^{Nt} \\ &= \begin{pmatrix}  e^t & 0 & 0 \\  0 & e^t & 0 \\  0 & 0 & e^t   \end{pmatrix} \left(I + \begin{pmatrix}  0 & t & 2t \\  0 & 0 & -4t \\  0 & 0 & 0   \end{pmatrix}+\begin{pmatrix}  0 & 0 & -2t^2 \\  0 & 0 & 0 \\  0 & 0 & 0   \end{pmatrix}\right) \\ &= e^t \begin{pmatrix}  1 & t & 2t \\  0 & 1 & -4t \\  0 & 0 & 1   \end{pmatrix} \\ &= \begin{pmatrix}  e^t & te^t & 2t(1-t)e^t \\  0 & e^t & -4te^t \\  0 & 0 & e^t   \end{pmatrix}. \end{aligned}$$ Is that the right answer?",,"['linear-algebra', 'matrices', 'analysis']"
92,What is the dimension of this Grassmannian?,What is the dimension of this Grassmannian?,,Why is  $2\times 3$ the dimension of $Gr_2(\mathbb{R}^5)$? and can one use the dimensions of Lie groups to derive this dimension? Note: $Gr_2(\mathbb{R}^5)$ denotes the Grassmannian of all $2$-dimensional subspaces of $\mathbb{R}^5$.,Why is  $2\times 3$ the dimension of $Gr_2(\mathbb{R}^5)$? and can one use the dimensions of Lie groups to derive this dimension? Note: $Gr_2(\mathbb{R}^5)$ denotes the Grassmannian of all $2$-dimensional subspaces of $\mathbb{R}^5$.,,"['linear-algebra', 'matrices', 'algebraic-geometry', 'differential-geometry', 'lie-groups']"
93,Does the multiplicative identity have to be 1?,Does the multiplicative identity have to be 1?,,"I am just starting out with vector spaces and I am having a hard time understanding them. One of the requirements states that $1\mathbf{v}=\mathbf{v}$ where $1$ is the multiplicative identity . Does 1 have to be the identity? Or is that whatever is the multiplicative identity is labelled 1? I ran across a question where $a(x,y)$ was defined as $(a x/3, a y/3)$. It was not specified but I guess the question also implied that scalar a belongs to real numbers.  Why has 1 got to be the identity here? Why can't I define 3 to be the identity here? Edit : I realise that I didn't describe the question in enough detail.  $\mathbf{v}$ here is the set of all ordered pairs $(x,y)$ where $x, y \in\mathbb{R}$. Addition is defined as $(x_1,y_1) + (x_2,y_2) = (2x_1-3x_2,y_1-y_2)$.  Now addition obviously violates many axioms but I am interested in scalar multiplication, which is defined as given above. The question is a simple example question asking us to list down all axioms violated.","I am just starting out with vector spaces and I am having a hard time understanding them. One of the requirements states that $1\mathbf{v}=\mathbf{v}$ where $1$ is the multiplicative identity . Does 1 have to be the identity? Or is that whatever is the multiplicative identity is labelled 1? I ran across a question where $a(x,y)$ was defined as $(a x/3, a y/3)$. It was not specified but I guess the question also implied that scalar a belongs to real numbers.  Why has 1 got to be the identity here? Why can't I define 3 to be the identity here? Edit : I realise that I didn't describe the question in enough detail.  $\mathbf{v}$ here is the set of all ordered pairs $(x,y)$ where $x, y \in\mathbb{R}$. Addition is defined as $(x_1,y_1) + (x_2,y_2) = (2x_1-3x_2,y_1-y_2)$.  Now addition obviously violates many axioms but I am interested in scalar multiplication, which is defined as given above. The question is a simple example question asking us to list down all axioms violated.",,['linear-algebra']
94,What is the difference between Eigenvectors and the Kernel or Null Space of a matrix?,What is the difference between Eigenvectors and the Kernel or Null Space of a matrix?,,"I am just wondering what is the difference between Eigenvectors and the Kernel or Null Space of a matrix? The kernel for matrix A is x where, Ax = 0 Isn't that what Eigenvectors are too?","I am just wondering what is the difference between Eigenvectors and the Kernel or Null Space of a matrix? The kernel for matrix A is x where, Ax = 0 Isn't that what Eigenvectors are too?",,['linear-algebra']
95,Prove: Square Matrix Can Be Written As A Sum Of A Symmetric And Skew-Symmetric Matrices,Prove: Square Matrix Can Be Written As A Sum Of A Symmetric And Skew-Symmetric Matrices,,Let $C^{n \times n}$ be a square matrix. Prove that $$C=\frac{1}{2}(C+C^T)+\frac{1}{2}(C-C^T)$$ What I have manage so far is: a. Let $S$ be a Symmetric Matrix so $S=C+C^T$ b. Let $N$ be a Skew-Symmetric Matrix so $N=C-C^T$ Proof: $S^t=[C+C^T]^T=C^T+C=S$ $N^t=[C-C^T]^T=-C^T+C=-N$,Let $C^{n \times n}$ be a square matrix. Prove that $$C=\frac{1}{2}(C+C^T)+\frac{1}{2}(C-C^T)$$ What I have manage so far is: a. Let $S$ be a Symmetric Matrix so $S=C+C^T$ b. Let $N$ be a Skew-Symmetric Matrix so $N=C-C^T$ Proof: $S^t=[C+C^T]^T=C^T+C=S$ $N^t=[C-C^T]^T=-C^T+C=-N$,,['linear-algebra']
96,Why is this matrix product diagonalizable?,Why is this matrix product diagonalizable?,,"Someone can help me with this following problem? Let $A,B$ be symmetric matrices. If $A$ is positive definite, then   $AB$ is diagonalizable. Thanks! P.S. The matrices are over $\mathbb{R}$","Someone can help me with this following problem? Let $A,B$ be symmetric matrices. If $A$ is positive definite, then   $AB$ is diagonalizable. Thanks! P.S. The matrices are over $\mathbb{R}$",,"['linear-algebra', 'matrices']"
97,Span of an empty set is the zero vector,Span of an empty set is the zero vector,,"I am reading Nering's book on Linear Algebra and in the section on vector spaces he makes the comment, ""We also agree that the empty set spans the set consisting of the zero vector alone"". Is Nering defining the span of the empty set to be the set containing the zero vector or is this something you can prove from the definition of span? I sense it is the latter, but the proof seems a bit tricky since you would be saying that {0} = Span of the indexed set of vectors in the empty set. But since the empty set has no vectors, it is not clear to me what its span would be.","I am reading Nering's book on Linear Algebra and in the section on vector spaces he makes the comment, ""We also agree that the empty set spans the set consisting of the zero vector alone"". Is Nering defining the span of the empty set to be the set containing the zero vector or is this something you can prove from the definition of span? I sense it is the latter, but the proof seems a bit tricky since you would be saying that {0} = Span of the indexed set of vectors in the empty set. But since the empty set has no vectors, it is not clear to me what its span would be.",,"['linear-algebra', 'matrices', 'vector-spaces', 'span']"
98,What is the motivation behind the definition of the matrix sum of a graph?,What is the motivation behind the definition of the matrix sum of a graph?,,"I was studying graph theory using the textbook Combinatorics and Graph Theory by Harris, Hirst, and Mossinghoff. In section 1.2, the matrix sum is defined as follows: Given a graph $\mathscr{G}$ of order $n$ with adjacency matrix $A$ , and given a positive integer $k$ , define the matrix sum $S_k$ to be $$S_k = \mathbb{I} + A + A^2 + \ldots + A^k,$$ where $\mathbb{I}$ is the $n \times n$ identity matrix. As I learned, this sum helps us with better methods to find the eccentricity, radius, and diameter of an undirected graph: for example, if $k$ is the smallest positive integer such that row $j$ of $S_k$ contains no zeros, then $\mathrm{ecc}(v_j) = k$ . What I don't quite understand yet is why exactly it makes sense to define this sum this way. I understand that this questions is a bit imprecise, but could someone motivate the reason why this definition exists this way?","I was studying graph theory using the textbook Combinatorics and Graph Theory by Harris, Hirst, and Mossinghoff. In section 1.2, the matrix sum is defined as follows: Given a graph of order with adjacency matrix , and given a positive integer , define the matrix sum to be where is the identity matrix. As I learned, this sum helps us with better methods to find the eccentricity, radius, and diameter of an undirected graph: for example, if is the smallest positive integer such that row of contains no zeros, then . What I don't quite understand yet is why exactly it makes sense to define this sum this way. I understand that this questions is a bit imprecise, but could someone motivate the reason why this definition exists this way?","\mathscr{G} n A k S_k S_k = \mathbb{I} + A + A^2 + \ldots + A^k, \mathbb{I} n \times n k j S_k \mathrm{ecc}(v_j) = k","['linear-algebra', 'combinatorics', 'graph-theory']"
99,Invertible matrix of non-square matrix?,Invertible matrix of non-square matrix?,,Is a matrix invertible only when it is a square matrix? What about a matrix of the order $m \cdot n$ with $m \gt n$ and such that it is row-equivalent to a row-reduced echelon matrix with more non-zero rows than columns? What is the motivation behind the concepts of left and right inverse? Is it only useful when dealing with non-square matrix?,Is a matrix invertible only when it is a square matrix? What about a matrix of the order $m \cdot n$ with $m \gt n$ and such that it is row-equivalent to a row-reduced echelon matrix with more non-zero rows than columns? What is the motivation behind the concepts of left and right inverse? Is it only useful when dealing with non-square matrix?,,"['linear-algebra', 'matrices', 'inverse']"
