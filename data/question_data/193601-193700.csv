,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Calculating the ""nth term"" in terms of taylor series coefficients","Calculating the ""nth term"" in terms of taylor series coefficients",,"I've got a sequence that's defined by the coefficients of a Taylor series (of a function) centred at x=0. The function in question is : $$f(x) = \frac{1}{(1-x)^3(1-x^3)^2}$$ So, taking the nth order derivative (at x=0) and dividing by n! leaves us with $$1 + 3 x + 6 x^2 + 12 x^3 + 21 x^4 + 33 x^5...$$ Cool, so I can work out the first few terms easily. How about if I wanted the 1000th term, for example? Is there an easier pattern or do I really have to compute (and sub zero into the derivative) : $$\frac{\frac{d^{1000}}{dx^{1000}}(f(x))}{1000!}$$ I failed to find a pattern with the coefficients that gets me around this calculation, so am I missing the pattern or is this problem ""impossible"" to solve this way? I noticed that because we need to sub in zero, any term that involves x on the numerator of any part of the expansion is also 0, but it doesn't get around calculating the derivative in the first place.","I've got a sequence that's defined by the coefficients of a Taylor series (of a function) centred at x=0. The function in question is : So, taking the nth order derivative (at x=0) and dividing by n! leaves us with Cool, so I can work out the first few terms easily. How about if I wanted the 1000th term, for example? Is there an easier pattern or do I really have to compute (and sub zero into the derivative) : I failed to find a pattern with the coefficients that gets me around this calculation, so am I missing the pattern or is this problem ""impossible"" to solve this way? I noticed that because we need to sub in zero, any term that involves x on the numerator of any part of the expansion is also 0, but it doesn't get around calculating the derivative in the first place.",f(x) = \frac{1}{(1-x)^3(1-x^3)^2} 1 + 3 x + 6 x^2 + 12 x^3 + 21 x^4 + 33 x^5... \frac{\frac{d^{1000}}{dx^{1000}}(f(x))}{1000!},"['calculus', 'derivatives', 'taylor-expansion']"
1,Relationship Between a Parabola and the Normal Distribution,Relationship Between a Parabola and the Normal Distribution,,"In this video ( https://www.youtube.com/watch?v=m62I5_ow3O8 ), at 4:05 the author talks about taking the (2nd Order) Taylor Expansion of the Likelihood Function (in statistics) for some ""model parameter"". Naturally, the 2nd Order Taylor Expansion will be a ""parabola shaped function"". Based on this 2nd Order Expansion, the author is interested evaluating the ""value of the model parameter"" that brings the value of the (2nd Order Expanded) Likelihood Function to $0$ . The author states that instead of directly taking the 2nd Order Expansion of the Likelihood Function, it is more advantageous to take the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function ( I didn't quite understand the reason behind this, why exactly is it advantageous? ). However, the author states that the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function will no longer be shaped like a parabola - but rather, take the form of a Gaussian Distribution (i.e. an ""inverted parabola""). In general terms, I am trying to understand the mathematical logic behind this. Suppose I have some arbitrary 2nd Order Function : $f(x) = x^2$ . If I take the second derivative of the of the negative logarithm of this function, I get the following result: Second derivative: $-\log(x^2) = \frac{2}{x^2}$ If I were to plot this function: I can see that the ""Red Function"" somewhat resembles a ""bell curved shape"" normal distribution. My Question: Is my understanding of the above correct? And is there any reason as to why the negative logarithm of the 2nd order expansion is ""more advantageous"" compared to just the 2nd order expansion? Thanks!","In this video ( https://www.youtube.com/watch?v=m62I5_ow3O8 ), at 4:05 the author talks about taking the (2nd Order) Taylor Expansion of the Likelihood Function (in statistics) for some ""model parameter"". Naturally, the 2nd Order Taylor Expansion will be a ""parabola shaped function"". Based on this 2nd Order Expansion, the author is interested evaluating the ""value of the model parameter"" that brings the value of the (2nd Order Expanded) Likelihood Function to . The author states that instead of directly taking the 2nd Order Expansion of the Likelihood Function, it is more advantageous to take the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function ( I didn't quite understand the reason behind this, why exactly is it advantageous? ). However, the author states that the 2nd Order Expansion of the (negative) Logarithm of the Likelihood Function will no longer be shaped like a parabola - but rather, take the form of a Gaussian Distribution (i.e. an ""inverted parabola""). In general terms, I am trying to understand the mathematical logic behind this. Suppose I have some arbitrary 2nd Order Function : . If I take the second derivative of the of the negative logarithm of this function, I get the following result: Second derivative: If I were to plot this function: I can see that the ""Red Function"" somewhat resembles a ""bell curved shape"" normal distribution. My Question: Is my understanding of the above correct? And is there any reason as to why the negative logarithm of the 2nd order expansion is ""more advantageous"" compared to just the 2nd order expansion? Thanks!",0 f(x) = x^2 -\log(x^2) = \frac{2}{x^2},"['probability', 'derivatives', 'logarithms', 'graphing-functions']"
2,Radon-Nikodym derivative and derivative of functions of real variable,Radon-Nikodym derivative and derivative of functions of real variable,,"I have read Radon–Nikodym derivative and ""normal"" derivative but I am still quite confused. Here are the two kinds of derivative and I am looking for the analog: The derivative of a function of real variables measures the change of function value w.r.t. its input. The $\mu$ -integrable function $f=d\nu/d\mu$ is called the Radon-Nikodym derivative whenever $\nu\ll\mu$ . My idea is that the absolute continuity gives us a hint of the change of value $\nu$ w.r.t. $\mu$ . Is it correct?","I have read Radon–Nikodym derivative and ""normal"" derivative but I am still quite confused. Here are the two kinds of derivative and I am looking for the analog: The derivative of a function of real variables measures the change of function value w.r.t. its input. The -integrable function is called the Radon-Nikodym derivative whenever . My idea is that the absolute continuity gives us a hint of the change of value w.r.t. . Is it correct?",\mu f=d\nu/d\mu \nu\ll\mu \nu \mu,"['measure-theory', 'derivatives', 'radon-nikodym']"
3,$i = \frac{dq}{dt}$ implies $\Delta q = i \Delta t$? Incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?,implies ? Incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?,i = \frac{dq}{dt} \Delta q = i \Delta t,"I am reading an electrical engineering textbook that states that the relationship between current $i$ , charge $q$ , and time $t$ is $$i = \dfrac{dq}{dt} \tag{1}$$ Based on this, the authors then state that $$\Delta q = i \Delta t \tag{2}$$ This set off alarm bells in my head. Now, (2) may actually be true, but using (1) as some kind of implication for (2) just seems like incorrect mathematics. If I had to fill in the blanks of the authors' thinking, it seems to me that they were likely rationalising this through the derivation of the derivative $$\dfrac{df}{dx} = \lim_{\Delta x \to 0} \dfrac{f(x + \Delta x) - f(x)}{\Delta x} = \lim_{h \to 0} \dfrac{f(x + h) - f(x)}{h} = \lim_{\Delta x \to 0} \dfrac{\Delta y}{\Delta x}$$ But, nonetheless, I don't see how, mathematically, $i = \dfrac{dq}{dt}$ implies $\Delta q = i \Delta t$ . Am I mistaken here, or is this actually incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?","I am reading an electrical engineering textbook that states that the relationship between current , charge , and time is Based on this, the authors then state that This set off alarm bells in my head. Now, (2) may actually be true, but using (1) as some kind of implication for (2) just seems like incorrect mathematics. If I had to fill in the blanks of the authors' thinking, it seems to me that they were likely rationalising this through the derivation of the derivative But, nonetheless, I don't see how, mathematically, implies . Am I mistaken here, or is this actually incorrect mathematics used as some kind of hand-wavy justification for an engineering equation?",i q t i = \dfrac{dq}{dt} \tag{1} \Delta q = i \Delta t \tag{2} \dfrac{df}{dx} = \lim_{\Delta x \to 0} \dfrac{f(x + \Delta x) - f(x)}{\Delta x} = \lim_{h \to 0} \dfrac{f(x + h) - f(x)}{h} = \lim_{\Delta x \to 0} \dfrac{\Delta y}{\Delta x} i = \dfrac{dq}{dt} \Delta q = i \Delta t,"['derivatives', 'physics', 'electromagnetism']"
4,"Find area bounded by given curves. $y=\arcsin x,\;y=\arccos x,\,y=0 $",Find area bounded by given curves.,"y=\arcsin x,\;y=\arccos x,\,y=0 ","Find area bounded by given curves. $$y=\arcsin x,y=\arccos x,y=0 $$ So we need to find that shaded area.Intersection point is $\frac{\pi}{4}$ So we can write $\int_{0}^{\frac{\pi}{4}}\arcsin x + \int_{\frac{\pi}{4}}^{1} \arccos x$ and after calculation get $\frac{\pi}{4}(\arcsin(\frac{\pi}{4})-\arccos(\frac{\pi}{4}))$ but answer is $\sqrt2 - 1$",Find area bounded by given curves. So we need to find that shaded area.Intersection point is So we can write and after calculation get but answer is,"y=\arcsin x,y=\arccos x,y=0  \frac{\pi}{4} \int_{0}^{\frac{\pi}{4}}\arcsin x + \int_{\frac{\pi}{4}}^{1} \arccos x \frac{\pi}{4}(\arcsin(\frac{\pi}{4})-\arccos(\frac{\pi}{4})) \sqrt2 - 1","['real-analysis', 'calculus', 'integration', 'derivatives', 'definite-integrals']"
5,Difference between two methods,Difference between two methods,,"$f(x)=\begin{cases}        x^2 \sin (\frac{1}{x}) & x\neq 0 \\       \ 0 & x=0      \end{cases} \ $ Check Differentiability at $x$ = $0$ My Approach: $f'(0)=\lim_{x\to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x\to 0}\frac{x^2\sin(\frac{1}{x})}{x}=\lim_{x\to0} x\sin\frac{1}{x}$ . this limit is defined, so indeed the function is differentiable at zero. Second Approach If i derivate functions directly and calculate $f'(0^+)$ and $f'(0^-)$ . $f'(x)=x^2 \cdot \cos(\frac{1}{x}) \cdot (\frac{-1}{x^2}))+2x\; \cdot \sin (\frac{1}{x})$ So $f'(x)=\cos(\frac{1}{x})$ which lies in  [-1,1] for $x$ approaching to $0$ . Hence Limit does not exist. So function must be non-differentiable. Why second method give false result when i directly differentiate. For all other question except in question when function is oscillating i get same result by both method","Check Differentiability at = My Approach: . this limit is defined, so indeed the function is differentiable at zero. Second Approach If i derivate functions directly and calculate and . So which lies in  [-1,1] for approaching to . Hence Limit does not exist. So function must be non-differentiable. Why second method give false result when i directly differentiate. For all other question except in question when function is oscillating i get same result by both method","f(x)=\begin{cases} 
      x^2 \sin (\frac{1}{x}) & x\neq 0 \\
      \ 0 & x=0  
   \end{cases}
\  x 0 f'(0)=\lim_{x\to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x\to 0}\frac{x^2\sin(\frac{1}{x})}{x}=\lim_{x\to0} x\sin\frac{1}{x} f'(0^+) f'(0^-) f'(x)=x^2 \cdot \cos(\frac{1}{x}) \cdot (\frac{-1}{x^2}))+2x\; \cdot \sin (\frac{1}{x}) f'(x)=\cos(\frac{1}{x}) x 0","['limits', 'derivatives']"
6,How do you find the global maximum or minimum of an unbounded function?,How do you find the global maximum or minimum of an unbounded function?,,"Take for example: $f(x) = \frac{x^4}{4} - 2x^3 + \frac{11x^2}{2}-6x + 2$ , for all $x \in R$ The critical points are 1,2, and 3. 1 and 3 are local minimums and 2 is a local maximum from 2nd derivative test. I can draw a graph and look at the intervals in between and on the sides of the critical points to deduce the graph shape: I found out that x = 2 is a global min and that there is no global max. The problem with this method is that the more critical points there are the more I have to crunch numbers in my calculator to see how the graph is shaped around critical points. Is there another way to reach the same conclusion that avoids this graphical way of doing things? For instance could I take the limit and deduce something from that, or maybe a more rigorous way?","Take for example: , for all The critical points are 1,2, and 3. 1 and 3 are local minimums and 2 is a local maximum from 2nd derivative test. I can draw a graph and look at the intervals in between and on the sides of the critical points to deduce the graph shape: I found out that x = 2 is a global min and that there is no global max. The problem with this method is that the more critical points there are the more I have to crunch numbers in my calculator to see how the graph is shaped around critical points. Is there another way to reach the same conclusion that avoids this graphical way of doing things? For instance could I take the limit and deduce something from that, or maybe a more rigorous way?",f(x) = \frac{x^4}{4} - 2x^3 + \frac{11x^2}{2}-6x + 2 x \in R,"['calculus', 'derivatives', 'maxima-minima']"
7,"Definition of piecewise differentiable curve in do Carmo: Riemannian Geometry; onesided derivative $\lim_{t\to a^+} c'(t)$ of a curve $c:[a,b]\to M$",Definition of piecewise differentiable curve in do Carmo: Riemannian Geometry; onesided derivative  of a curve,"\lim_{t\to a^+} c'(t) c:[a,b]\to M","Do Carmo defines a piecewise differential curve in the following manner: A piecewise differentiable curve is a continuous mapping $c:[a,b]\to M$ of a closed interval $[a,b]\subset \mathbb{R}$ into $M$ satisfying the following condition: there exists a partition $$a=t_0<t_1<\dots <t_{k-1}=b$$ of $[a,b]$ such that the restrictions $c\rvert_{[t_i, t_{i+1}]},i=0,\dots,k-1,$ are differentiable. We say that $c$ joins the points $c(a)$ and $c(b)$ . $c(t_i)$ is called a vertex of $c$ , and the angle formed by $\lim_{t\to t_i^+} c'(t)$ with $\lim_{t\to t_i^-} c'(t)$ is called the vertex angle at $c(t_i)$ ; here $\lim_{t\to t_i^+}$ ( $\lim_{t\to t_i^-})$ signifies that $t$ approaches $t_i$ trough values above (below) that of $t_i$ . As far as I know, do Carmo hasn't defined what is meant by differentiability of a curve $c: [t_i, t_{i+1}]\to M$ on a closed set. If I understand the answer given by John B in On the definition of piecewise differentiable curves correctly, it means that $c$ , as a curve on $(t_i, t_{i+1})$ , is differentiable and that the one-sided derivatives exist at the endpoints $t_i$ and $t_{i+1}$ . However, how would one go about calculating these derivatives or defining their existence? We have $$c'(t) f = (f \circ c)'(t)=\frac{d}{dt} (f \circ c)(t) = \lim_{h\to 0} \frac{f(c(t+h))-f(c(t))}{h}\quad \forall f\in C^{\infty}(M), t\in (t_i,t_{i+1}).$$ Does that mean that $\lim_{t\to t_i^+} c'(t)$ exists if $$\lim_{h\to 0^+} \frac{f(c(t_i^++h))-f(c(t_i^+))}{h}$$ exists for all $f\in C^{\infty}(M)$ ? This doesn't seem correct to me since a definition without the use of $f$ would be preferable. John B also mentioned that the differentiability defined by the existence of the one-sided derivatives is equivalent to the fact that there exists a differentiable extension of $c$ which is defined on a larger open interval. Could someone provide me with a reference for this fact?","Do Carmo defines a piecewise differential curve in the following manner: A piecewise differentiable curve is a continuous mapping of a closed interval into satisfying the following condition: there exists a partition of such that the restrictions are differentiable. We say that joins the points and . is called a vertex of , and the angle formed by with is called the vertex angle at ; here ( signifies that approaches trough values above (below) that of . As far as I know, do Carmo hasn't defined what is meant by differentiability of a curve on a closed set. If I understand the answer given by John B in On the definition of piecewise differentiable curves correctly, it means that , as a curve on , is differentiable and that the one-sided derivatives exist at the endpoints and . However, how would one go about calculating these derivatives or defining their existence? We have Does that mean that exists if exists for all ? This doesn't seem correct to me since a definition without the use of would be preferable. John B also mentioned that the differentiability defined by the existence of the one-sided derivatives is equivalent to the fact that there exists a differentiable extension of which is defined on a larger open interval. Could someone provide me with a reference for this fact?","c:[a,b]\to M [a,b]\subset \mathbb{R} M a=t_0<t_1<\dots <t_{k-1}=b [a,b] c\rvert_{[t_i, t_{i+1}]},i=0,\dots,k-1, c c(a) c(b) c(t_i) c \lim_{t\to t_i^+} c'(t) \lim_{t\to t_i^-} c'(t) c(t_i) \lim_{t\to t_i^+} \lim_{t\to t_i^-}) t t_i t_i c: [t_i, t_{i+1}]\to M c (t_i, t_{i+1}) t_i t_{i+1} c'(t) f = (f \circ c)'(t)=\frac{d}{dt} (f \circ c)(t) = \lim_{h\to 0} \frac{f(c(t+h))-f(c(t))}{h}\quad \forall f\in C^{\infty}(M), t\in (t_i,t_{i+1}). \lim_{t\to t_i^+} c'(t) \lim_{h\to 0^+} \frac{f(c(t_i^++h))-f(c(t_i^+))}{h} f\in C^{\infty}(M) f c","['derivatives', 'differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds']"
8,Prove that the polynomial has exactly 2 real roots by IVT or Rolle's Theorem,Prove that the polynomial has exactly 2 real roots by IVT or Rolle's Theorem,,"I'm trying to prove this by IVT or Rolle's Theorem. Usually if it would say ""Prove it has only 1 real root"", I would assume it had 2 roots, take the derivative and if it was $≠0$ , then I would prove that it has only 1 root, so Rolle's Theorem. Im kind of stuck on this one, because I'm not able to suppose anything and then give a counterexample: $6x^4-7x+1=0$ Any tip or help would be much appreciated.","I'm trying to prove this by IVT or Rolle's Theorem. Usually if it would say ""Prove it has only 1 real root"", I would assume it had 2 roots, take the derivative and if it was , then I would prove that it has only 1 root, so Rolle's Theorem. Im kind of stuck on this one, because I'm not able to suppose anything and then give a counterexample: Any tip or help would be much appreciated.",≠0 6x^4-7x+1=0,"['derivatives', 'rolles-theorem']"
9,If $f(x) = \log x$ then what will be the value of $f'(\log x)$?,If  then what will be the value of ?,f(x) = \log x f'(\log x),"If $f(x) = \log x$ then what will be the value of $f'(\log x)$ ? Note: I mentioned the question as it is was provided in my book and I, however, consider $f'(x)$ to be derivative with respect to $x$ The answer to the question according to chain rule should be $$f'(\log x) = \frac{1}{\log x} \cdot \frac{1}{x}$$ according to me, however my book states that its answer is $\frac{1}{\log x}$ essentially the same as calculating the derivative and placing $\log x$ in place of $x$ . Now which of these methods is the correct way to evaluate this and what wrong assumption is being considered in the wrong answer?","If then what will be the value of ? Note: I mentioned the question as it is was provided in my book and I, however, consider to be derivative with respect to The answer to the question according to chain rule should be according to me, however my book states that its answer is essentially the same as calculating the derivative and placing in place of . Now which of these methods is the correct way to evaluate this and what wrong assumption is being considered in the wrong answer?",f(x) = \log x f'(\log x) f'(x) x f'(\log x) = \frac{1}{\log x} \cdot \frac{1}{x} \frac{1}{\log x} \log x x,"['calculus', 'derivatives']"
10,A question about notation in matrix calculus: $\dfrac{\partial Ax}{\partial x}=A^T$ or $ \dfrac{\partial (Ax)^T}{\partial x}=A^T$?,A question about notation in matrix calculus:  or ?,\dfrac{\partial Ax}{\partial x}=A^T  \dfrac{\partial (Ax)^T}{\partial x}=A^T,"I was looking for an explanation of the derivation of quadratic forms in matrix calculus, and during my search I came across two different, seemingly contradictory, identities. On the one hand, some derivation used $\dfrac{\partial \mathbf{Ax}}{\partial\mathbf{x}}=\mathbf{A^T}$ , which implies that $\dfrac{\partial \mathbf{x}}{\partial\mathbf{x}}=\mathbf{I}$ . On the other hand, my book on matrix algebra for example states that $ \dfrac{\partial \mathbf{(Ax)^T}}{\partial\mathbf{x}}=\dfrac{\partial \mathbf{x^TA^T}}{\partial\mathbf{x}}=\mathbf{A^T}$ , which is quite the opposite of what I found elsewhere. I can follow the derivation in both cases, but I would like to understand the deeper context, and why there are different approaches? Which of the two notations is the most common?","I was looking for an explanation of the derivation of quadratic forms in matrix calculus, and during my search I came across two different, seemingly contradictory, identities. On the one hand, some derivation used , which implies that . On the other hand, my book on matrix algebra for example states that , which is quite the opposite of what I found elsewhere. I can follow the derivation in both cases, but I would like to understand the deeper context, and why there are different approaches? Which of the two notations is the most common?",\dfrac{\partial \mathbf{Ax}}{\partial\mathbf{x}}=\mathbf{A^T} \dfrac{\partial \mathbf{x}}{\partial\mathbf{x}}=\mathbf{I}  \dfrac{\partial \mathbf{(Ax)^T}}{\partial\mathbf{x}}=\dfrac{\partial \mathbf{x^TA^T}}{\partial\mathbf{x}}=\mathbf{A^T},"['linear-algebra', 'derivatives', 'matrix-calculus', 'quadratic-forms']"
11,Show that $\Gamma^{(n)}(z) = \int_0^\infty t^{z-1}(\log(t))^ne^{-t}dt$,Show that,\Gamma^{(n)}(z) = \int_0^\infty t^{z-1}(\log(t))^ne^{-t}dt,"So, I need to show that the n-th derivative of Gamma function is equal to: \begin{equation} \Gamma^{(n)}(z) = \int_0^\infty t^{z-1}(\log(t))^ne^{-t}dt \end{equation} I already know that: \begin{equation} \Gamma(z) = \lim_{n \to \infty}\int_0^n t^{z-1}(1-\frac{t}{n})^n dt = \lim_{n \to \infty} \gamma_n(z) \end{equation} I also see that: \begin{equation} \gamma_n'(z_0) = \lim_{z \to z_0} \int_0^n (1-\frac{t}{n})^n \cdot \frac{t^{z-1} - t^{z_0-1}}{z - z_0} dt \end{equation} Now, if I could show that \begin{equation} \frac{t^{z_n-1} - t^{z_0-1}}{z_n - z_0} \to t^{z_0 - 1}\log(t)\textrm{ uniformly, as } n \to \infty  \; (z_n \to z_0) \; \; \; \; \; \;(1) \end{equation} then I would be able to pull the limit to the inside of the integral and get: \begin{equation} \gamma_n'(z_0) = \int_0^n t^{z-1}\log(t)(1-\frac{t}{n})^n dt \end{equation} From which I see how I can get $\Gamma'(z_0) = \int_0^\infty t^{z-1}\log(t)e^{-t}dt$ . However, I don't know how to prove $(1)$ . Could someone please help me?","So, I need to show that the n-th derivative of Gamma function is equal to: I already know that: I also see that: Now, if I could show that then I would be able to pull the limit to the inside of the integral and get: From which I see how I can get . However, I don't know how to prove . Could someone please help me?","\begin{equation}
\Gamma^{(n)}(z) = \int_0^\infty t^{z-1}(\log(t))^ne^{-t}dt
\end{equation} \begin{equation}
\Gamma(z) = \lim_{n \to \infty}\int_0^n t^{z-1}(1-\frac{t}{n})^n dt = \lim_{n \to \infty} \gamma_n(z)
\end{equation} \begin{equation}
\gamma_n'(z_0) = \lim_{z \to z_0} \int_0^n (1-\frac{t}{n})^n \cdot \frac{t^{z-1} - t^{z_0-1}}{z - z_0} dt
\end{equation} \begin{equation}
\frac{t^{z_n-1} - t^{z_0-1}}{z_n - z_0} \to t^{z_0 - 1}\log(t)\textrm{ uniformly, as } n \to \infty  \; (z_n \to z_0) \; \; \; \; \; \;(1)
\end{equation} \begin{equation}
\gamma_n'(z_0) = \int_0^n t^{z-1}\log(t)(1-\frac{t}{n})^n dt
\end{equation} \Gamma'(z_0) = \int_0^\infty t^{z-1}\log(t)e^{-t}dt (1)","['real-analysis', 'calculus', 'derivatives', 'gamma-function']"
12,Integration of $\sin^{-1} (x)$ using Lagrange notation,Integration of  using Lagrange notation,\sin^{-1} (x),"I would like to find the anti-derivative of $\sin^{-1} (x)$ using Lagrange notation. For derivation, Lagrange uses $f'(x)$ , $f''(x)$ , etc. However, for anti-derivation he uses $\int$ (elongated $S$ ) symbol and $f(x)$ in it. We would like to compute the derivative of $\sin^{-1} (x)$ . Let $y = \sin^{-1} (x)$ . Here $y$ is the function of $x$ , so $f(x) = \sin^{-1}  (x)$ . Now, $\sin (y) = x$ . then, we can also write it as $\sin [f(x)] = x$ , since $y = f(x)$ . Then, differentiating using the chain rule, $$\cos[f(x)] f'(x) = 1.$$ $$f'(x) = 1/\cos[f(x)].$$ Using trigonometry, $$\sin^2 [f(x)] + \cos^2 [f(x)] = 1,$$ $$\cos[f(x)] = \sqrt{ 1 - \sin^2[f(x)] }.$$ Which is equivalent to $\sqrt{1 - x^2}$ . So $f'(x) = 1 / \sqrt{1 - x^2}$ . Here $f'(x)$ means the derivative of function of $x$ w.r.t $x$ , and $y$ is a function of $x$ . So by Leibniz notation, we can also write $dy/dx = f'(x)$ , here $dy/dx$ also means the derivative of a function of $x$ w.r.t $x$ since $y$ is a function of $x$ . Similarly, how can I find the anti-derivative of $\sin^{-1} (x)$ using Lagrange notation, using as few symbols as possible; I don't want to treat $dy/dx$ like a ratio.","I would like to find the anti-derivative of using Lagrange notation. For derivation, Lagrange uses , , etc. However, for anti-derivation he uses (elongated ) symbol and in it. We would like to compute the derivative of . Let . Here is the function of , so . Now, . then, we can also write it as , since . Then, differentiating using the chain rule, Using trigonometry, Which is equivalent to . So . Here means the derivative of function of w.r.t , and is a function of . So by Leibniz notation, we can also write , here also means the derivative of a function of w.r.t since is a function of . Similarly, how can I find the anti-derivative of using Lagrange notation, using as few symbols as possible; I don't want to treat like a ratio.","\sin^{-1} (x) f'(x) f''(x) \int S f(x) \sin^{-1} (x) y = \sin^{-1} (x) y x f(x) = \sin^{-1}  (x) \sin (y) = x \sin [f(x)] = x y = f(x) \cos[f(x)] f'(x) = 1. f'(x) = 1/\cos[f(x)]. \sin^2 [f(x)] + \cos^2 [f(x)] = 1, \cos[f(x)] = \sqrt{ 1 - \sin^2[f(x)] }. \sqrt{1 - x^2} f'(x) = 1 / \sqrt{1 - x^2} f'(x) x x y x dy/dx = f'(x) dy/dx x x y x \sin^{-1} (x) dy/dx","['calculus', 'integration', 'derivatives', 'notation']"
13,How do I apply some extended sketching techniques?,How do I apply some extended sketching techniques?,,"One of the exam papers I had asked me to sketch $\cos{\theta}=\frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha$ with $\theta$ and $\alpha$ as the axises and for $0<\alpha<2\pi$ . I've tried converting $\frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha$ into a single trig term which was $\sqrt{\frac{5}{8}}\cos(\alpha-\arctan(2))$ , using this I've found the maxima to be at $\alpha=\arctan(2)$ and $\alpha=\arctan(2)+\pi$ for the domain but that's as far as I can go. What else do I need to find to sketch this graph?","One of the exam papers I had asked me to sketch with and as the axises and for . I've tried converting into a single trig term which was , using this I've found the maxima to be at and for the domain but that's as far as I can go. What else do I need to find to sketch this graph?",\cos{\theta}=\frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha \theta \alpha 0<\alpha<2\pi \frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha \sqrt{\frac{5}{8}}\cos(\alpha-\arctan(2)) \alpha=\arctan(2) \alpha=\arctan(2)+\pi,"['derivatives', 'trigonometry', 'graphing-functions']"
14,Calculate the derivative of $\left[{\sqrt{\left(1+\tan\left(x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right]$ when $x=0$,Calculate the derivative of  when,\left[{\sqrt{\left(1+\tan\left(x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right] x=0,"$\cssId{diff-var-order-mathjax}{\tfrac{\mathrm{d}}{\mathrm{d}x}}\left[{\sqrt{\left(1+\tan\left(x\right)\right)\left(1+\tan\left(2x\right)\right)\left(1+\tan\left(4x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right]=\:?$ I have tried writing  it with sin and cos, tried some formulas for ${\sin\left(2x\right)}$ , ${\cos\left(2x\right)}$ and ${\tan\left(2x\right)}$ but I have had no meaningful results. Maybe the derivative when $x=0$ can be found without calculating it? I'm guessing it can be calculated pretty easily but I'm missing the answer. Any help would be greatly appreciated.","I have tried writing  it with sin and cos, tried some formulas for , and but I have had no meaningful results. Maybe the derivative when can be found without calculating it? I'm guessing it can be calculated pretty easily but I'm missing the answer. Any help would be greatly appreciated.",\cssId{diff-var-order-mathjax}{\tfrac{\mathrm{d}}{\mathrm{d}x}}\left[{\sqrt{\left(1+\tan\left(x\right)\right)\left(1+\tan\left(2x\right)\right)\left(1+\tan\left(4x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right]=\:? {\sin\left(2x\right)} {\cos\left(2x\right)} {\tan\left(2x\right)} x=0,"['real-analysis', 'derivatives']"
15,Is a Hölder continuous function differentiable almost everywhere?,Is a Hölder continuous function differentiable almost everywhere?,,As per the title. I know that a Lipschitz continuous function is differentiable almost everywhere (see the Rademacher Theorem ). I was wondering if something similar was true for Hölder continuous functions.,As per the title. I know that a Lipschitz continuous function is differentiable almost everywhere (see the Rademacher Theorem ). I was wondering if something similar was true for Hölder continuous functions.,,"['analysis', 'derivatives', 'holder-spaces']"
16,Induction for $f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x)$ [duplicate],Induction for  [duplicate],f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x),"This question already has an answer here : How to show via induction the product rule for derivatives? (1 answer) Closed 3 years ago . For $f(x) = p(x)g(x)$ , By writing them out repeatedly, I am guessing that $f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x)$ (where $f^{(k)}(x)$ : $k$ -th derivative of $f(x)$ ). I tried proving this by mathematical induction, but am stuck on how to proceed from the induction hypothesis. how can I prove by mathematical induction that $f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x)$ ? Please help! Thank you in advance.","This question already has an answer here : How to show via induction the product rule for derivatives? (1 answer) Closed 3 years ago . For , By writing them out repeatedly, I am guessing that (where : -th derivative of ). I tried proving this by mathematical induction, but am stuck on how to proceed from the induction hypothesis. how can I prove by mathematical induction that ? Please help! Thank you in advance.",f(x) = p(x)g(x) f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x) f^{(k)}(x) k f(x) f^{(k)}(x) = \sum_{i=0}^{k} {k \choose i} p^{(k-i)}(x) g^{(i)}(x),"['derivatives', 'induction']"
17,"If $\sin x=\frac{2 t}{1+t^{2}}$ and $\cot y=\frac{1-t^{2}}{2 t}$, then the value of $\frac{d^{2} x}{d y^{2}}$","If  and , then the value of",\sin x=\frac{2 t}{1+t^{2}} \cot y=\frac{1-t^{2}}{2 t} \frac{d^{2} x}{d y^{2}},"If $\sin x=\frac{2 t}{1+t^{2}}$ and $\cot y=\frac{1-t^{2}}{2 t}$ , then the value of $\frac{d^{2} x}{d y^{2}}$ If we take $t=\tan \theta$ then $x=2\theta$ and for $y$ ,I am getting two values as follows , if we apply identity $\tan 2 A=\frac{2 \tan A}{1-\tan ^{2} A}$ then we get $y=2\theta$ and if we take $\tan \theta $ common from $\dfrac{1-\tan^2 \theta}{2\tan\theta}$ and simplify then we get $y= \dfrac{\pi}{4} + \theta$ ? how we are getting two values of $y$ ? thankyou","If and , then the value of If we take then and for ,I am getting two values as follows , if we apply identity then we get and if we take common from and simplify then we get ? how we are getting two values of ? thankyou",\sin x=\frac{2 t}{1+t^{2}} \cot y=\frac{1-t^{2}}{2 t} \frac{d^{2} x}{d y^{2}} t=\tan \theta x=2\theta y \tan 2 A=\frac{2 \tan A}{1-\tan ^{2} A} y=2\theta \tan \theta  \dfrac{1-\tan^2 \theta}{2\tan\theta} y= \dfrac{\pi}{4} + \theta y,"['calculus', 'derivatives']"
18,Second derivative near a minimum positive?,Second derivative near a minimum positive?,,"If I have a smooth real valued non-constant function $f$ with isolated minimum at $x_M$ does that mean that close to $x_M$ its second derivative must be positive? I.e. must there exist $a > 0$ so that $$ \begin{align} f''(x_M + \varepsilon) > 0 \\ f''(x_M - \varepsilon) > 0 \end{align} $$ for all $\varepsilon \in (0, a]?$ If not can someone give an example?",If I have a smooth real valued non-constant function with isolated minimum at does that mean that close to its second derivative must be positive? I.e. must there exist so that for all If not can someone give an example?,"f x_M x_M a > 0 
\begin{align}
f''(x_M + \varepsilon) > 0 \\
f''(x_M - \varepsilon) > 0
\end{align}
 \varepsilon \in (0, a]?","['real-analysis', 'derivatives', 'optimization', 'maxima-minima']"
19,$n$-th Derivative $\frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|}$ via Recursive Product Rule,-th Derivative  via Recursive Product Rule,n \frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|},"Let $g(x) = x^{-\frac{1}{2}}$ and $f(x) = e^{-\sqrt{x} |\omega|}$ . I am trying to find an expression for the $M$ -th derivative of their product: \begin{align}     \frac{d^M}{dx^M} \left[ f(x) g(x) \right] = \sum_{k=0}^{M} \binom{M}{k} f^{(M-k)}(x) g^{(k)}(x) \end{align} where $M$ is an even integer and $\omega \in \mathbb{R}$ is a constant parameter. The derivative for $g(x)$ is easy: \begin{align}     \frac{d^{k}}{d x^{k}} g(x) = \frac{(2k-1)!!}{2} x^{-\frac{2k+1}{2}} \end{align} However, for $f(x)$ , it is not so easy, and this is where I get stuck. For the $M$ -th derivative of $f(x)$ , Mathematica gives me: M! DifferenceRoot[    Function[{\[FormalY], \[FormalN]}, {(2 + 6 \[FormalN] +            4 \[FormalN]^2) \[FormalY][1 + \[FormalN]] +         4 (1 + \[FormalN]) (2 + \[FormalN]) x \[FormalY][          2 + \[FormalN]] - \[FormalY][\[FormalN]] Abs[w]^2 ==        0, \[FormalY][0] ==        E^(-Sqrt[x] Abs[w]), \[FormalY][1] == -((        E^(-Sqrt[x] Abs[w]) Abs[w])/(2 Sqrt[x]))}]][M] But I have no idea what any of that means, and their site is not very helpful... It seems that the solution will involve some kind of recursive application of the product rule, however, I can't seem to find a pattern after I do a handful of layers... Question: What is $\frac{d^{n}}{d x^{n}} f(x) = \frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|}$ when $n$ is an even integer?","Let and . I am trying to find an expression for the -th derivative of their product: where is an even integer and is a constant parameter. The derivative for is easy: However, for , it is not so easy, and this is where I get stuck. For the -th derivative of , Mathematica gives me: M! DifferenceRoot[    Function[{\[FormalY], \[FormalN]}, {(2 + 6 \[FormalN] +            4 \[FormalN]^2) \[FormalY][1 + \[FormalN]] +         4 (1 + \[FormalN]) (2 + \[FormalN]) x \[FormalY][          2 + \[FormalN]] - \[FormalY][\[FormalN]] Abs[w]^2 ==        0, \[FormalY][0] ==        E^(-Sqrt[x] Abs[w]), \[FormalY][1] == -((        E^(-Sqrt[x] Abs[w]) Abs[w])/(2 Sqrt[x]))}]][M] But I have no idea what any of that means, and their site is not very helpful... It seems that the solution will involve some kind of recursive application of the product rule, however, I can't seem to find a pattern after I do a handful of layers... Question: What is when is an even integer?","g(x) = x^{-\frac{1}{2}} f(x) = e^{-\sqrt{x} |\omega|} M \begin{align}
    \frac{d^M}{dx^M} \left[ f(x) g(x) \right] = \sum_{k=0}^{M} \binom{M}{k} f^{(M-k)}(x) g^{(k)}(x)
\end{align} M \omega \in \mathbb{R} g(x) \begin{align}
    \frac{d^{k}}{d x^{k}} g(x) = \frac{(2k-1)!!}{2} x^{-\frac{2k+1}{2}}
\end{align} f(x) M f(x) \frac{d^{n}}{d x^{n}} f(x) = \frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|} n","['calculus', 'derivatives', 'exponential-function', 'partial-derivative', 'radicals']"
20,Simplifaction of Negative Fractional Exponent for Derivatives,Simplifaction of Negative Fractional Exponent for Derivatives,,"I'm self learning calculus and I ran across this question during my practice. The question asks to differentiate the following equation: $$\sqrt{x^3 + \csc(x)}$$ Now, I was able to do the following... $$(1/2(x^3 + \csc(x)))^{-1/2}\cdot\cot(x)\cdot-\csc(x) + 3x^2$$ Basically, I applied the chain rule However, when I looked at the solution, I was only partially correct. My numerator was right, but the denominator was wrong. The solution provided was $$\frac{3x^2-\cot(x)\csc(x)}  {2 (\csc(x) + x^3)^{1/2}}$$ I am confused as to how the ""two"" comes into the denominator. Doesn't $1/2$ as an exponent imply a regular square root? The negative in the exponent makes it a fraction and the $1/2$ just makes it a regular square root . Thus, I'm thinking there is no reason for the ""2"" to be there at all, becuase it's a regular square root. Hoping someone can clear this up. Thanks","I'm self learning calculus and I ran across this question during my practice. The question asks to differentiate the following equation: Now, I was able to do the following... Basically, I applied the chain rule However, when I looked at the solution, I was only partially correct. My numerator was right, but the denominator was wrong. The solution provided was I am confused as to how the ""two"" comes into the denominator. Doesn't as an exponent imply a regular square root? The negative in the exponent makes it a fraction and the just makes it a regular square root . Thus, I'm thinking there is no reason for the ""2"" to be there at all, becuase it's a regular square root. Hoping someone can clear this up. Thanks",\sqrt{x^3 + \csc(x)} (1/2(x^3 + \csc(x)))^{-1/2}\cdot\cot(x)\cdot-\csc(x) + 3x^2 \frac{3x^2-\cot(x)\csc(x)}  {2 (\csc(x) + x^3)^{1/2}} 1/2 1/2,"['calculus', 'derivatives']"
21,Partial derivative vs Total derivative,Partial derivative vs Total derivative,,"Are partial derivative and total derivative different for a system with independent variables? The term $\frac{df(x,y)}{dx} = \frac{\partial f(x,y)}{\partial x}+\frac{\partial f(x,y)}{\partial y}\frac{dy}{dx}$ . But as $y$ and $x$ are independent, so $\frac{dy}{dx} = 0$ . So, how are two different?","Are partial derivative and total derivative different for a system with independent variables? The term . But as and are independent, so . So, how are two different?","\frac{df(x,y)}{dx} = \frac{\partial f(x,y)}{\partial x}+\frac{\partial f(x,y)}{\partial y}\frac{dy}{dx} y x \frac{dy}{dx} = 0","['derivatives', 'partial-derivative']"
22,How to find all polynomials that follows the equation $P(P'(x)) = P'(P(x))$,How to find all polynomials that follows the equation,P(P'(x)) = P'(P(x)),I want to find all polynomials that has the following property $$P(P'(x)) = P'(P(x))$$ where P(x) is a polynomial. Can you please tell me a way to solve this problem?,I want to find all polynomials that has the following property where P(x) is a polynomial. Can you please tell me a way to solve this problem?,P(P'(x)) = P'(P(x)),"['derivatives', 'polynomials']"
23,Derivative of the Determinant of a Matrix,Derivative of the Determinant of a Matrix,,"I want to compute the derivative of the determinant of a matrix. This seems to be relatively straightforward for the first derivative using e.g., Jacobi's formula . $$\frac{d}{dt}\det A(t)=\mathrm{tr}\, \left(\mathrm{adj}(A(t)) \frac{dA(t)}{dt} \right)$$ Let us assume we have the following matrix $A(t)$ : $$A(t) = \begin{bmatrix} -8 \cdot t^3 - 3 \cdot t^2 & 2 \cdot t^3 + 4 \cdot t^2  \\ 6 \cdot t^3 + 4 \cdot t^2 & -8 \cdot t^3 \\ \end{bmatrix}$$ Thus, if I have applied Jacobi's formula correctly, the result of the first derivative should be as follows: $$\frac{d}{dt}\det  A(t) = 312 \cdot t^5 - 40 \cdot t^4 - 64 \cdot t^3 $$ In a related post , there was an attempt to use Jacobi's formula for the second derivative. This is an interesting case in itself. My results for the second derivative are as follows: $$\frac{d^2}{dt^2}det\ A(t)= 1560 \cdot t^4 - 160 \cdot t^3 - 192 \cdot t^2$$ I am not overly confident with the matrix calculus and the correct application of Jacobi's formula. Hence, I would be grateful if someone could check if my obtained results are correct? Thanks!","I want to compute the derivative of the determinant of a matrix. This seems to be relatively straightforward for the first derivative using e.g., Jacobi's formula . Let us assume we have the following matrix : Thus, if I have applied Jacobi's formula correctly, the result of the first derivative should be as follows: In a related post , there was an attempt to use Jacobi's formula for the second derivative. This is an interesting case in itself. My results for the second derivative are as follows: I am not overly confident with the matrix calculus and the correct application of Jacobi's formula. Hence, I would be grateful if someone could check if my obtained results are correct? Thanks!","\frac{d}{dt}\det A(t)=\mathrm{tr}\, \left(\mathrm{adj}(A(t)) \frac{dA(t)}{dt} \right) A(t) A(t) = \begin{bmatrix}
-8 \cdot t^3 - 3 \cdot t^2 & 2 \cdot t^3 + 4 \cdot t^2  \\
6 \cdot t^3 + 4 \cdot t^2 & -8 \cdot t^3 \\
\end{bmatrix} \frac{d}{dt}\det  A(t) = 312 \cdot t^5 - 40 \cdot t^4 - 64 \cdot t^3  \frac{d^2}{dt^2}det\ A(t)= 1560 \cdot t^4 - 160 \cdot t^3 - 192 \cdot t^2","['derivatives', 'determinant', 'matrix-calculus']"
24,simplify fraction (stone drops off a cliff),simplify fraction (stone drops off a cliff),,How do I get from $$ \frac {dt} {T} = \frac {dx} {gt} \sqrt { \frac {g} {2h} } $$ to $$ \frac {dt} {T} = \frac {1} {2 \sqrt {hx} } dx $$ where $x(t) = \frac {1} {2} gt^2 $ and $ T = \sqrt { \frac {2h} {g}}$ . I'm currently struggling with Griffiths' Introduction to Quantum Mechanics . This is from a worked example on p.11-12 of the 2nd edition.,How do I get from to where and . I'm currently struggling with Griffiths' Introduction to Quantum Mechanics . This is from a worked example on p.11-12 of the 2nd edition., \frac {dt} {T} = \frac {dx} {gt} \sqrt { \frac {g} {2h} }   \frac {dt} {T} = \frac {1} {2 \sqrt {hx} } dx  x(t) = \frac {1} {2} gt^2   T = \sqrt { \frac {2h} {g}},['derivatives']
25,Evaluate $\lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin x}-1}{\sinh x\cos \left( \sin \left( x \right) -1 \right)}$,Evaluate,\lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin x}-1}{\sinh x\cos \left( \sin \left( x \right) -1 \right)},"Evaluate the limit $$ \lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin  x}-1}{\sinh x(\cos \left( \sin \left( x \right)  \right)-1)} $$ My Attempt: I tried to use L'Hôpital's rule to evalute it, however I found that the $1$ st and $2$ nd derivative of the numerator is $0$ at $x=0$ , and the $3$ rd derivative is very complicated. And the method of Taylor's Series is too complicated here. So, my question is , is there any easier way to evaluate this limit? The desired answer is $-1$ .","Evaluate the limit My Attempt: I tried to use L'Hôpital's rule to evalute it, however I found that the st and nd derivative of the numerator is at , and the rd derivative is very complicated. And the method of Taylor's Series is too complicated here. So, my question is , is there any easier way to evaluate this limit? The desired answer is .","
\lim_{x\rightarrow 0} \frac{\left( \cosh x \right) ^{\sin  x}-1}{\sinh x(\cos \left( \sin \left( x \right)  \right)-1)}
 1 2 0 x=0 3 -1","['calculus', 'limits', 'derivatives', 'taylor-expansion']"
26,Some confusion about the proof for Darboux's Theorem,Some confusion about the proof for Darboux's Theorem,,"I'm having some confusion in the proof of Darboux's Theorem. It appears similar questions have been asked before, but I'm still confused by the replies, so I thought I would ask my own. Here is my proof. Let $g(x) = f(x) - \gamma x$ Assume $f'(a) < f'(b)$ w.l.o.g. We know $f'(a) < \gamma < f'(b)$ by hypothesis. So, $f'(a) - \gamma = g'(a) < 0$ and $f'(b) - \gamma = g'(b) > 0$ Since $g'(a) < 0$ and $g'(b) > 0$ , (opposite signs) we know $\exists c$ such that $g'(c) = 0$ That step right there is my confusion. I am basically using the IVT to claim there is a value in between. However, to use the IVT, the function has to be continuous. That is not an assumption in the problem, only that $f$ is continuous. I've found this question asked a couple of times, but the common reply seems to be that the derivative need not be continuous to have the intermediate value property because of Darboux's Theorem. But I am trying to prove Darboux's Theorem! So while I believe that fact, I can't use the theorem within its proof. I cannot seem to justify that step in the event that $g'$ is discontinuous. I have been told there is another version of the proof combining the MVT and IVT. However, I've found it online in a few places, and I'm having a hard time following it. So I am trying to figure out how to do it this way since I don't understand the other way. Can someone explain to me why I can use the IVT without the derivative being continnuous?","I'm having some confusion in the proof of Darboux's Theorem. It appears similar questions have been asked before, but I'm still confused by the replies, so I thought I would ask my own. Here is my proof. Let Assume w.l.o.g. We know by hypothesis. So, and Since and , (opposite signs) we know such that That step right there is my confusion. I am basically using the IVT to claim there is a value in between. However, to use the IVT, the function has to be continuous. That is not an assumption in the problem, only that is continuous. I've found this question asked a couple of times, but the common reply seems to be that the derivative need not be continuous to have the intermediate value property because of Darboux's Theorem. But I am trying to prove Darboux's Theorem! So while I believe that fact, I can't use the theorem within its proof. I cannot seem to justify that step in the event that is discontinuous. I have been told there is another version of the proof combining the MVT and IVT. However, I've found it online in a few places, and I'm having a hard time following it. So I am trying to figure out how to do it this way since I don't understand the other way. Can someone explain to me why I can use the IVT without the derivative being continnuous?",g(x) = f(x) - \gamma x f'(a) < f'(b) f'(a) < \gamma < f'(b) f'(a) - \gamma = g'(a) < 0 f'(b) - \gamma = g'(b) > 0 g'(a) < 0 g'(b) > 0 \exists c g'(c) = 0 f g',"['real-analysis', 'derivatives', 'continuity', 'proof-explanation']"
27,Proving $f^{(n)} (0)$ s unbounded where $f(x)= \tan^{-1} x$,Proving  s unbounded where,f^{(n)} (0) f(x)= \tan^{-1} x,"The following question was asked in real analysis quiz and I am looking for an more elegant solution. Question :Let $f(x)= \tan^{-1}x $ for all $x\in \mathbb{R}$ . Prove that sequence $\{f^{(n)}(0)\}$ is unbounded. There is a solution given by differentiating $n$ times the expression $(1+x^2) f(x)$ and then using the recurrence relation $(1+x^2) f^{(n+1)} (x)+ 2nx f^{(n)} (x) + n(n-1) f^{(n-1)} (x)=0$ and $f'(0)=1$ . But is there a more elegant solution using any other concepts of other branches of pure mathematics or analysis (real, complex, functional). I thought I should ask here as I don't wanted to use that method although that is clear to me!","The following question was asked in real analysis quiz and I am looking for an more elegant solution. Question :Let for all . Prove that sequence is unbounded. There is a solution given by differentiating times the expression and then using the recurrence relation and . But is there a more elegant solution using any other concepts of other branches of pure mathematics or analysis (real, complex, functional). I thought I should ask here as I don't wanted to use that method although that is clear to me!",f(x)= \tan^{-1}x  x\in \mathbb{R} \{f^{(n)}(0)\} n (1+x^2) f(x) (1+x^2) f^{(n+1)} (x)+ 2nx f^{(n)} (x) + n(n-1) f^{(n-1)} (x)=0 f'(0)=1,['real-analysis']
28,Prove that $\arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x))$ for $0\leq x <1$,Prove that  for,\arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x)) 0\leq x <1,"It's inspired by a general result proposed in a book (Dictionary of inequalities second edition by Peter Bullen p.101) related to composition of  function : In fact we have $x\in[0,1)$ : $$\arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x))\quad (1)$$ To prove it I have introduced a function : $$f(x)=\arctan(\operatorname{arctanh}(x))-\operatorname{arctanh}(\arctan(x))$$ We differentiate to get: $$f'(x)= -\frac{1}{((x^2-1)(\operatorname{arctanh}^2(x)+1))}+\frac{1}{((x^2+1)(\arctan^2(x)-1))}$$ And then I use : Let $0\leq x <1$ then we have : $$\operatorname{arctanh}(x)\geq x \geq \arctan(x)$$ Then I cannot conclude directly . Question : How to prove $(1)$ ? Thanks in advance !",It's inspired by a general result proposed in a book (Dictionary of inequalities second edition by Peter Bullen p.101) related to composition of  function : In fact we have : To prove it I have introduced a function : We differentiate to get: And then I use : Let then we have : Then I cannot conclude directly . Question : How to prove ? Thanks in advance !,"x\in[0,1) \arctan(\operatorname{arctanh}(x))\geq \operatorname{arctanh}(\arctan(x))\quad (1) f(x)=\arctan(\operatorname{arctanh}(x))-\operatorname{arctanh}(\arctan(x)) f'(x)= -\frac{1}{((x^2-1)(\operatorname{arctanh}^2(x)+1))}+\frac{1}{((x^2+1)(\arctan^2(x)-1))} 0\leq x <1 \operatorname{arctanh}(x)\geq x \geq \arctan(x) (1)","['derivatives', 'trigonometry', 'inequality', 'hyperbolic-functions']"
29,Find $\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right )$ without using the chain rule. [closed],Find  without using the chain rule. [closed],\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right ),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $$\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right )$$ I can't for the life of me differentiate this function while only using Trig Identities, Basic differentiation rules, and Limits (no L'Hopital, either.)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I can't for the life of me differentiate this function while only using Trig Identities, Basic differentiation rules, and Limits (no L'Hopital, either.)",\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right ),"['calculus', 'derivatives']"
30,I'm asked to differentiate this $\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2}$ but I barely understand the notation.,I'm asked to differentiate this  but I barely understand the notation.,\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2},"I'm asked to differentiate $$\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2},$$ according to $w^{j'}_{k}$ which is the $k$ th weight of the vector of weight of $j'$ . It seems that $\| W^{j'} \|^{2}_{2}$ stands for the the $L2$ norm squared. This seems to indicate that we have: $$\| W^{j'} \|^{2}_{2} = w^{2}_1 + w^{2}_2 + ... + w^{2}_k$$ Then to my understanding $\sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2}$ seems to indicate that we have: $$\sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2} = (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=1} + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=2} + ... + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=m}$$ I have absolutely no clue how to derivate this formula. Do I use the sum rule to get this? $$\dfrac{\partial }{\partial w^{j'}_{k}} =\dfrac{C}{2} (2w_k)_{j'=1} + (2w_k)_{j'=2}  + ... + (2w_k)_{j'=m}$$ Edit: This is from my machine learning course. This formula represents the regularization term we add to the loss function of an SVM (Support vector machines) in order to minimize the objective function. Here $W$ is a vector containing scalars $w$ .",I'm asked to differentiate according to which is the th weight of the vector of weight of . It seems that stands for the the norm squared. This seems to indicate that we have: Then to my understanding seems to indicate that we have: I have absolutely no clue how to derivate this formula. Do I use the sum rule to get this? Edit: This is from my machine learning course. This formula represents the regularization term we add to the loss function of an SVM (Support vector machines) in order to minimize the objective function. Here is a vector containing scalars .,"\dfrac{C}{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2}, w^{j'}_{k} k j' \| W^{j'} \|^{2}_{2} L2 \| W^{j'} \|^{2}_{2} = w^{2}_1 + w^{2}_2 + ... + w^{2}_k \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2} \sum^{m}_{j' = 1} \| W^{j'} \|^{2}_{2} = (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=1} + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=2} + ... + (w^{2}_1 + w^{2}_2 + ... + w^{2}_k)_{j'=m} \dfrac{\partial }{\partial w^{j'}_{k}} =\dfrac{C}{2} (2w_k)_{j'=1} + (2w_k)_{j'=2}  + ... + (2w_k)_{j'=m} W w","['calculus', 'linear-algebra', 'derivatives', 'normed-spaces', 'machine-learning']"
31,Proof of $(x^n)' = nx^{n-1}$ using the natural log and the chain rule,Proof of  using the natural log and the chain rule,(x^n)' = nx^{n-1},"Let's say $y=x^n$ is defined for all real number $x$ , including $0$ . I tried out the following proof of the statement $(x^n)' = nx^{n-1}$ . $$y = x^n$$ $$\ln y = n \ln x$$ $$\frac{1}{y} \times y' = n \times \frac{1}{x}$$ $$y' = n \times \frac{y}{x} = n \times \frac{x^n}{x} = nx^{n-1}$$ However on the third line there appears the terms $1/y$ and especially $1/x$ which make the proof invalid for any case where $x = 0$ or $y = 0$ (which always exists in the case of $y=x^n$ ). Is there any way to complement this to make proof complete?","Let's say is defined for all real number , including . I tried out the following proof of the statement . However on the third line there appears the terms and especially which make the proof invalid for any case where or (which always exists in the case of ). Is there any way to complement this to make proof complete?",y=x^n x 0 (x^n)' = nx^{n-1} y = x^n \ln y = n \ln x \frac{1}{y} \times y' = n \times \frac{1}{x} y' = n \times \frac{y}{x} = n \times \frac{x^n}{x} = nx^{n-1} 1/y 1/x x = 0 y = 0 y=x^n,"['calculus', 'derivatives', 'logarithms']"
32,Differentiation: $y = 9x + \frac{3}{x}$,Differentiation:,y = 9x + \frac{3}{x},"Differentiate $y = 9x + \frac{3}{x}$ I think the first step is to turn $\frac{3}{x}$ into a more ""friendly"" format, so $x$ to the power of something maybe? How do I get $x$ with an index from $\frac{3}{x}$ ?","Differentiate I think the first step is to turn into a more ""friendly"" format, so to the power of something maybe? How do I get with an index from ?",y = 9x + \frac{3}{x} \frac{3}{x} x x \frac{3}{x},"['calculus', 'derivatives']"
33,Time derivative of heat semigroup.,Time derivative of heat semigroup.,,"Imagine we have the heat semigroup $\{P_t\}_{0\leq t\leq T}$ and remember that we have $$\frac{d}{dt} (P_t\varphi)(x)=\frac 1 2 (P_t\varphi '')(x).$$ I want to calculate following time derivative $$\frac{d}{dt} P_t\varphi (\int_0^t f(u)du).$$ The problem is that the argument is also time dependent, and honestly I am having some difficulties to see why this should equal $$\frac 1 2 (P_t\varphi '')(\int_0^t f(u)du)+P_t\varphi'(\int_0^t f(u)du)f(t)$$ This seems pretty much a product rule for derivatives, but this is not a product but the action of an operator. I apologize if this is rather trivial but my brain seems to refuse to understand it. I would appreciate any help. EDIT:  Could this be due the fact that $P_t$ is a linear operator, and an application of Riesz Theorem?","Imagine we have the heat semigroup and remember that we have I want to calculate following time derivative The problem is that the argument is also time dependent, and honestly I am having some difficulties to see why this should equal This seems pretty much a product rule for derivatives, but this is not a product but the action of an operator. I apologize if this is rather trivial but my brain seems to refuse to understand it. I would appreciate any help. EDIT:  Could this be due the fact that is a linear operator, and an application of Riesz Theorem?",\{P_t\}_{0\leq t\leq T} \frac{d}{dt} (P_t\varphi)(x)=\frac 1 2 (P_t\varphi '')(x). \frac{d}{dt} P_t\varphi (\int_0^t f(u)du). \frac 1 2 (P_t\varphi '')(\int_0^t f(u)du)+P_t\varphi'(\int_0^t f(u)du)f(t) P_t,"['functional-analysis', 'derivatives', 'heat-equation', 'semigroup-of-operators']"
34,Find the maximum value of $x^2y$ given constraints,Find the maximum value of  given constraints,x^2y,Find the maximum value of $${ x }^{ 2 }y$$ subject to the constraint $$x+y+\sqrt { 2{ x }^{ 2 }+2xy+3{ y }^{ 2 } } =k$$ where k is a constant. I tried it by substituting value of x and then differentiating w.r.t $x$ but not able to proceed further.,Find the maximum value of subject to the constraint where k is a constant. I tried it by substituting value of x and then differentiating w.r.t but not able to proceed further.,{ x }^{ 2 }y x+y+\sqrt { 2{ x }^{ 2 }+2xy+3{ y }^{ 2 } } =k x,"['derivatives', 'optimization', 'constraints']"
35,closed form expression for the summation $\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t$,closed form expression for the summation,\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t,"I’m trying to find a closed form expression for this summation. $\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t$ Where k and t are given nonnegative integers. According to ( Checking Jaynes' formula 6.108 for $\sum\limits_{m=0}^\infty{m+a \choose m} m^nx^m$ ) the answer is ${(x\frac{d}{dx})}^t\frac{1}{{(1-x)}^{k+1}}$ And based on ( What's the property of this series? Is it special? Coefficients of $\left(x\frac{d}{dx}\right)^n f(x) $ ) we can write $\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t=\left(x\frac{d}{dx}\right)^t\frac{1}{\left(1-x\right)^{k+1}}=\sum_{r=1}^{t}{S\left(t,r\right)x^r\frac{d^r}{dx^r}\frac{1}{\left(1-x\right)^{k+1}}} =\sum_{r=1}^{t}{S\left(t,r\right)x^r\frac{\left(k+1\right)\left(k+2\right)\ldots\left(k+r\right)}{\left(1-x\right)^{k+r+1}}} =\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{(k+1)(k+2)...(k+r)\ S\left(t,r\right)\ {(\frac{x}{1-x})}^r}$ Where $S(t,r)$ is the Stirling number of the second kind. This can be written in two ways. But none of them seems simpler to me. $=\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\frac{(k+r)!}{k!}\ S\left(t,r\right)\ {(\frac{x}{1-x})}^r}$ Or since it includes a falling factorial ( $\left(k+r\right)_{\bar{r}}$ ), we can write $=\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\sum_{u=0}^{r}{S\left(t,r\right)\left(\frac{x}{1-x}\right)^rs\left(r,u\right)\left(k+r\right)^u}\ }$ $=\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\sum_{u=1}^{r}{S\left(t,r\right)\left(\frac{x}{1-x}\right)^rs\left(r,u\right)\left(k+r\right)^u}\ }$ $=\frac{1}{\left(1-x\right)^{k+1}}\sum_{u=1}^{t}{\sum_{r=u}^{t}{S\left(t,r\right)s\left(r,u\right)\left(\frac{x}{1-x}\right)^r\left(k+r\right)^u}\ } $ Where $s(r,u)$ is the signed Stirling number of the first kind (Remember $\sum_{r=u}^{t}{S\left(t,r\right)s\left(r,u\right)=\delta_{tu}}$ but this one is more complicated). The question has not been solved yet (finding a closed form expression for this summation. $\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t$ ). Can you help me? (I wrote what I've tried since it may inspire you).","I’m trying to find a closed form expression for this summation. Where k and t are given nonnegative integers. According to ( Checking Jaynes' formula 6.108 for $\sum\limits_{m=0}^\infty{m+a \choose m} m^nx^m$ ) the answer is And based on ( What's the property of this series? Is it special? Coefficients of $\left(x\frac{d}{dx}\right)^n f(x) $ ) we can write Where is the Stirling number of the second kind. This can be written in two ways. But none of them seems simpler to me. Or since it includes a falling factorial ( ), we can write Where is the signed Stirling number of the first kind (Remember but this one is more complicated). The question has not been solved yet (finding a closed form expression for this summation. ). Can you help me? (I wrote what I've tried since it may inspire you).","\sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t {(x\frac{d}{dx})}^t\frac{1}{{(1-x)}^{k+1}} \sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t=\left(x\frac{d}{dx}\right)^t\frac{1}{\left(1-x\right)^{k+1}}=\sum_{r=1}^{t}{S\left(t,r\right)x^r\frac{d^r}{dx^r}\frac{1}{\left(1-x\right)^{k+1}}}
=\sum_{r=1}^{t}{S\left(t,r\right)x^r\frac{\left(k+1\right)\left(k+2\right)\ldots\left(k+r\right)}{\left(1-x\right)^{k+r+1}}}
=\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{(k+1)(k+2)...(k+r)\ S\left(t,r\right)\ {(\frac{x}{1-x})}^r} S(t,r) =\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\frac{(k+r)!}{k!}\ S\left(t,r\right)\ {(\frac{x}{1-x})}^r} \left(k+r\right)_{\bar{r}} =\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\sum_{u=0}^{r}{S\left(t,r\right)\left(\frac{x}{1-x}\right)^rs\left(r,u\right)\left(k+r\right)^u}\ } =\frac{1}{\left(1-x\right)^{k+1}}\sum_{r=1}^{t}{\sum_{u=1}^{r}{S\left(t,r\right)\left(\frac{x}{1-x}\right)^rs\left(r,u\right)\left(k+r\right)^u}\ } =\frac{1}{\left(1-x\right)^{k+1}}\sum_{u=1}^{t}{\sum_{r=u}^{t}{S\left(t,r\right)s\left(r,u\right)\left(\frac{x}{1-x}\right)^r\left(k+r\right)^u}\ }
 s(r,u) \sum_{r=u}^{t}{S\left(t,r\right)s\left(r,u\right)=\delta_{tu}} \sum_{i=0}^{\infty}\binom{i+k}{i}x^ii^t","['sequences-and-series', 'derivatives', 'binomial-coefficients', 'stirling-numbers', 'analytic-combinatorics']"
36,Why am I getting derivative of $y = 1/x$ function as $0$?,Why am I getting derivative of  function as ?,y = 1/x 0,"I was finding the derivative of the function: $y = 1/x$ . I did the followed steps: \begin{align*} \frac{\frac{1}{x+dx} - \frac{1}{x}}{dx} &=\left(\frac{1}{x+dx} - \frac{1}{x} \right) \frac{1}{dx} \\ &= \frac{1}{x dx + (dx)^2} - \frac{1}{x dx}. \end{align*} Since, $(dx)^2$ would be extremely small, I removed it, so $$\frac{1}{x dx} - \frac{1}{x dx}$$ which is equal to zero. why am I getting the derivative of $y = 1/x$ as $0$ ?","I was finding the derivative of the function: . I did the followed steps: Since, would be extremely small, I removed it, so which is equal to zero. why am I getting the derivative of as ?","y = 1/x \begin{align*}
\frac{\frac{1}{x+dx} - \frac{1}{x}}{dx} &=\left(\frac{1}{x+dx} - \frac{1}{x} \right) \frac{1}{dx} \\
&= \frac{1}{x dx + (dx)^2} - \frac{1}{x dx}.
\end{align*} (dx)^2 \frac{1}{x dx} - \frac{1}{x dx} y = 1/x 0","['calculus', 'derivatives']"
37,Geometric proof of chain rule with the derivative of $\sin(2x)$,Geometric proof of chain rule with the derivative of,\sin(2x),"I'm following this post https://math.stackexchange.com/a/2169/612996 as my example and I've figured out how it works for $\sin(\theta)$ , During my first try: I keep on missing the factor of $2$ when it's $\sin(2\theta)$ . I always get $\cos(2\theta)$ In my work, I've made the angle $2\theta$ and then changed everything that has $\theta$ to $2\theta$ , but I've kept the increase in the angle $\Delta\theta$ . I think I could reverse-engineer the answer so I use $\Delta 2 \theta$ and get the right answer, but I don't know why that should work and not $\Delta\theta$ since isn't it just a small increase in the angle anyway? (Or is it a small increase that is relative to the angle, and that's why there's a 2?) During my second try: I understand the algebraic way of doing the chain rule but I want to do it with some sort of geometric intuition.","I'm following this post https://math.stackexchange.com/a/2169/612996 as my example and I've figured out how it works for , During my first try: I keep on missing the factor of when it's . I always get In my work, I've made the angle and then changed everything that has to , but I've kept the increase in the angle . I think I could reverse-engineer the answer so I use and get the right answer, but I don't know why that should work and not since isn't it just a small increase in the angle anyway? (Or is it a small increase that is relative to the angle, and that's why there's a 2?) During my second try: I understand the algebraic way of doing the chain rule but I want to do it with some sort of geometric intuition.",\sin(\theta) 2 \sin(2\theta) \cos(2\theta) 2\theta \theta 2\theta \Delta\theta \Delta 2 \theta \Delta\theta,"['calculus', 'derivatives']"
38,Guided Integration Question,Guided Integration Question,,"Part 1 is easy, but struggling to see how it helps with Part 2. Part 1 Find $\frac{dy}{dx}$ if $y=xe^{-x}$ . Part 2 Hence show that $\int xe^{-x}dx=-xe^{-x}-e^{-x}+c$ .","Part 1 is easy, but struggling to see how it helps with Part 2. Part 1 Find if . Part 2 Hence show that .",\frac{dy}{dx} y=xe^{-x} \int xe^{-x}dx=-xe^{-x}-e^{-x}+c,"['integration', 'derivatives']"
39,The derivative of $f(x)=\frac{3 \sin x}{2+\cos x}$,The derivative of,f(x)=\frac{3 \sin x}{2+\cos x},"My solution: The background $$\begin{align} &3\frac{d}{dx}\left(\frac{\sin(x)}{2+\cos(x)}\right)\\ &=3\frac{\frac{d}{dx}(\sin(x))(2+\cos(x))-\frac{d}{dx}(2+\cos(x))\sin(x)}{(2+\cos(x))^2}\\ &=3\frac{\cos(x)(2+\cos(x))-(-\sin(x))\sin(x)}{(2+\cos(x))^2}\\ &=\frac{3+6\cos(x)}{(2+\cos(x))^2}\end{align}$$ However, I plugged $f(x)=\dfrac{3 \sin x}{2+\cos x}$ into the derivative calculator in wolfram alpha and received the following calculation: $$\frac{3\left(\sin^2(x)+\cos^2(x)+2\cos(x)\right)}{(2+\cos(x))^2}$$ Is my solution incorrect and the one from wolfram alpha correct? If so, where did I go wrong?","My solution: The background However, I plugged into the derivative calculator in wolfram alpha and received the following calculation: Is my solution incorrect and the one from wolfram alpha correct? If so, where did I go wrong?","\begin{align} &3\frac{d}{dx}\left(\frac{\sin(x)}{2+\cos(x)}\right)\\
&=3\frac{\frac{d}{dx}(\sin(x))(2+\cos(x))-\frac{d}{dx}(2+\cos(x))\sin(x)}{(2+\cos(x))^2}\\
&=3\frac{\cos(x)(2+\cos(x))-(-\sin(x))\sin(x)}{(2+\cos(x))^2}\\
&=\frac{3+6\cos(x)}{(2+\cos(x))^2}\end{align} f(x)=\dfrac{3 \sin x}{2+\cos x} \frac{3\left(\sin^2(x)+\cos^2(x)+2\cos(x)\right)}{(2+\cos(x))^2}","['calculus', 'derivatives', 'trigonometry']"
40,Approximation of a function’s derivative,Approximation of a function’s derivative,,I came across this result when I was tinkering with some summations and integrals. Any ideas if it could be useful? $\frac{d}{dx}f(x)\approx f(x+\frac{1}{2})-f(x-\frac{1}{2})$,I came across this result when I was tinkering with some summations and integrals. Any ideas if it could be useful?,\frac{d}{dx}f(x)\approx f(x+\frac{1}{2})-f(x-\frac{1}{2}),['calculus']
41,$f(2-x)=f(2+x)$ and $f'(1/2)=0=f'(1)$. Find minimum number of roots of $f''(x)=0$.,and . Find minimum number of roots of .,f(2-x)=f(2+x) f'(1/2)=0=f'(1) f''(x)=0,"Let $f$ be a non constant twice differentiable function satisfying $f(2-x)=f(2+x)$ and $f'(1/2)=0=f'(1)$ . Find the minimum number of roots of $f''(x)=0$ in the interval $(0,4)$ . Answer: $4$ I managed to rewrite the given equation as $f(x)=f(4-x)$ . Using this, I obtained $$f'(x)+f'(4-x)=0$$ And $$f''(x)=f''(4-x)$$ So it suffices to show that $f''(x)=0$ has at least $2$ roots in the interval $(0,2)$ but I'm unsure what's gonna happen at $x=2$ here. Also, I have $f'(7/2)=0=f'(3)$ from the given info and the first equation I obtained, but I don't know how to use this. Any help would be great.","Let be a non constant twice differentiable function satisfying and . Find the minimum number of roots of in the interval . Answer: I managed to rewrite the given equation as . Using this, I obtained And So it suffices to show that has at least roots in the interval but I'm unsure what's gonna happen at here. Also, I have from the given info and the first equation I obtained, but I don't know how to use this. Any help would be great.","f f(2-x)=f(2+x) f'(1/2)=0=f'(1) f''(x)=0 (0,4) 4 f(x)=f(4-x) f'(x)+f'(4-x)=0 f''(x)=f''(4-x) f''(x)=0 2 (0,2) x=2 f'(7/2)=0=f'(3)","['calculus', 'derivatives']"
42,"$f(0)=f(1)=0$, $f(x)=\frac{f(x+h)+f(x-h)}{2}$ implies $f(x)=0$ for $[0, 1]$",",  implies  for","f(0)=f(1)=0 f(x)=\frac{f(x+h)+f(x-h)}{2} f(x)=0 [0, 1]","Question: Suppose $f$ is continuous on $[0, 1]$ with $f(0)=f(1)=0$ . For $\forall x\in (0, 1)$ , there $\exists h>0$ with $0\le x-h<x<x+h\le1$ such that $f(x)=\frac{f(x+h)+f(x-h)}{2}$ . Show that $\forall x\in(0, 1), f(x)=0$ . I tried to prove that $f$ is differentiable on $(0, 1)$ using the fact that $\frac{f(x+h)-f(x)}{h}=\frac{f(x)-f(x-h)}{h}$ , but I realized that not all $h$ holds the equation, but there exists a particular $h$ in every $x$ . So, this is not a proper approach. I also thought about the concavity of $f$ . Since $\forall x\in(0, 1),\exists h>0$ with $0\le x-h<x<x+h\le1$ such that $$f(x)=f\left(\frac{x-h}{2}+\frac{x+h}{2}\right)\ge {1\over2}f\left(x-h\right)+{1\over2}f\left(x+h\right)$$ , which implies $f$ is concave downward, and $$f(x)=f\left(\frac{x-h}{2}+\frac{x+h}{2}\right)\le {1\over2}f\left(x-h\right)+{1\over2}f\left(x+h\right)$$ , which implies $f$ is concave upward. Two facts might imply that $f$ is constant, which in turn $\forall x\in[0, 1], f(x)=0$ since $f(0)=f(1)=0$ . Is this approach correct? I thought it has to be more precise, so I wanted to use the second derivative. But I actually failed to prove that $f$ is differentiable. Could you please give me some ideas about the question? Thanks a lot.","Question: Suppose is continuous on with . For , there with such that . Show that . I tried to prove that is differentiable on using the fact that , but I realized that not all holds the equation, but there exists a particular in every . So, this is not a proper approach. I also thought about the concavity of . Since with such that , which implies is concave downward, and , which implies is concave upward. Two facts might imply that is constant, which in turn since . Is this approach correct? I thought it has to be more precise, so I wanted to use the second derivative. But I actually failed to prove that is differentiable. Could you please give me some ideas about the question? Thanks a lot.","f [0, 1] f(0)=f(1)=0 \forall x\in (0, 1) \exists h>0 0\le x-h<x<x+h\le1 f(x)=\frac{f(x+h)+f(x-h)}{2} \forall x\in(0, 1), f(x)=0 f (0, 1) \frac{f(x+h)-f(x)}{h}=\frac{f(x)-f(x-h)}{h} h h x f \forall x\in(0, 1),\exists h>0 0\le x-h<x<x+h\le1 f(x)=f\left(\frac{x-h}{2}+\frac{x+h}{2}\right)\ge {1\over2}f\left(x-h\right)+{1\over2}f\left(x+h\right) f f(x)=f\left(\frac{x-h}{2}+\frac{x+h}{2}\right)\le {1\over2}f\left(x-h\right)+{1\over2}f\left(x+h\right) f f \forall x\in[0, 1], f(x)=0 f(0)=f(1)=0 f","['calculus', 'derivatives', 'jensen-inequality']"
43,Proving $\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y)$ using differentiation,Proving  using differentiation,\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y),While proving $\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y)$ by this $$\operatorname{sin}(x+y)=\operatorname{sin}(x)\operatorname{cos}(y)+\operatorname{cos}(x)\operatorname{sin}(y) \\ \text{differentiating both sides w.r.t } x \\ \operatorname{cos}(x+y) \left(1+\frac{dy}{dx}\right)=(\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y))\left(1+\frac{dy}{dx}\right)\\ \text{for $\frac{dy}{dx} \neq -1$}\\\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y) $$ Now I am confused what happens when $\frac{dy}{dx} = -1$,While proving by this Now I am confused what happens when,\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y) \operatorname{sin}(x+y)=\operatorname{sin}(x)\operatorname{cos}(y)+\operatorname{cos}(x)\operatorname{sin}(y) \\ \text{differentiating both sides w.r.t } x \\ \operatorname{cos}(x+y) \left(1+\frac{dy}{dx}\right)=(\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y))\left(1+\frac{dy}{dx}\right)\\ \text{for \frac{dy}{dx} \neq -1}\\\operatorname{cos}(x+y)=\operatorname{cos}(x)\operatorname{cos}(y)-\operatorname{sin}(x)\operatorname{sin}(y)  \frac{dy}{dx} = -1,"['derivatives', 'trigonometry', 'almost-everywhere']"
44,General question about derivatives.,General question about derivatives.,,"Consider a differentiable function $f:\mathbb{R} \rightarrow \mathbb{R}$ . The derivative of the function at any point can be written as: \begin{align*} f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \end{align*} Suppose we have a constant $c > 0$ , is it true that: \begin{align*} f'(x) = \lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} \hspace{3ex}? \end{align*} Since when dividing $h$ by some constant in the numerator, it will still become arbitrarily small. Or does it follow that: \begin{align*} \lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} = \frac{1}{c}\lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{\frac{h}{c}} = \frac{f'(x)}{c} \hspace{3ex} ?  \end{align*} I think that the second case is correct, but I still wanted to be 100% sure.","Consider a differentiable function . The derivative of the function at any point can be written as: Suppose we have a constant , is it true that: Since when dividing by some constant in the numerator, it will still become arbitrarily small. Or does it follow that: I think that the second case is correct, but I still wanted to be 100% sure.","f:\mathbb{R} \rightarrow \mathbb{R} \begin{align*}
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{align*} c > 0 \begin{align*}
f'(x) = \lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} \hspace{3ex}?
\end{align*} h \begin{align*}
\lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{h} = \frac{1}{c}\lim_{h \to 0} \frac{f(x+\frac{h}{c}) - f(x)}{\frac{h}{c}} = \frac{f'(x)}{c} \hspace{3ex} ? 
\end{align*}","['limits', 'derivatives']"
45,Modified Newton method and contraction principle,Modified Newton method and contraction principle,,"I am studying Newton's method modified by the book Zorich, Mathematical analysis II, page 39,40: It seems to me, if I make no mistakes, that there is a problem in the derivative of $ A (x) $ . The author says that $ | A '(x) | = | [f' (x_0)] ^ {- 1} \cdot f '(x) | $ , while I would say that: $$ | A '(x) | = | 1- [f' (x_0)] ^ {- 1} \cdot f '(x) | $$ Am I wrong?","I am studying Newton's method modified by the book Zorich, Mathematical analysis II, page 39,40: It seems to me, if I make no mistakes, that there is a problem in the derivative of . The author says that , while I would say that: Am I wrong?", A (x)   | A '(x) | = | [f' (x_0)] ^ {- 1} \cdot f '(x) |   | A '(x) | = | 1- [f' (x_0)] ^ {- 1} \cdot f '(x) | ,"['analysis', 'derivatives', 'numerical-methods', 'newton-raphson']"
46,Is the function $(1+x)^{\frac{1}{x}}$ differentiable at $x=0$?,Is the function  differentiable at ?,(1+x)^{\frac{1}{x}} x=0,Is the function $(1+x)^{\frac{1}{x}}$ differentiable at $x=0$ ? Would the expression below denote its derivative? $$  \lim_{x \to 0}  \lim_{h \to 0 } \frac{ (1+x+h)^{\frac{1}{x+h}} - e}{h}$$,Is the function differentiable at ? Would the expression below denote its derivative?,(1+x)^{\frac{1}{x}} x=0   \lim_{x \to 0}  \lim_{h \to 0 } \frac{ (1+x+h)^{\frac{1}{x+h}} - e}{h},"['calculus', 'derivatives']"
47,Derivative of Integral (Fundamental Theorem of Calculus),Derivative of Integral (Fundamental Theorem of Calculus),,"Question : Suppose $F(x) = \int^{x^2}_0 \frac{1}{\cos t} dt$ . Find the derivative of $F(x)$ over the region $x\in[0, \frac{\pi}{4}]$ for which it is continuous. Attempt: So I know how to do the following computations: $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \int^{x^2}_0 \sec(t)dt$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = [\ln|\tan(t)+\sec(t)|]^{x^2}_0$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln|\tan(0)+\sec(0)|$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln1$$ $$\int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)|$$ Note: To find $\int \sec(t) dt$ I used the method of substitution found here . And then using the chain rule to differentiate ( $u=|\tan(x^2)+\sec(x^2)|$ so $F(x)=\ln u$ ), we get $F'(x)=\frac{2x}{\cos (x^2)} = 2x\sec (x^2)$ for $x\in[0, \frac{\pi}{4}]$ . However, this question was given in a real analysis course and I believe I need to use the Fundamental Theorem of Calculus (which is fine considering $F$ is continuous for $x\in[0, \frac{\pi}{4}]$ as stated in the question so not required to prove). I am familiar with two different (but equivalent) definitions of the FTC; this one and this one , but am unsure of which one to use. Any help would be greatly appreciated.","Question : Suppose . Find the derivative of over the region for which it is continuous. Attempt: So I know how to do the following computations: Note: To find I used the method of substitution found here . And then using the chain rule to differentiate ( so ), we get for . However, this question was given in a real analysis course and I believe I need to use the Fundamental Theorem of Calculus (which is fine considering is continuous for as stated in the question so not required to prove). I am familiar with two different (but equivalent) definitions of the FTC; this one and this one , but am unsure of which one to use. Any help would be greatly appreciated.","F(x) = \int^{x^2}_0 \frac{1}{\cos t} dt F(x) x\in[0, \frac{\pi}{4}] \int^{x^2}_0 \frac{1}{\cos(t)} dt = \int^{x^2}_0 \sec(t)dt \int^{x^2}_0 \frac{1}{\cos(t)} dt = [\ln|\tan(t)+\sec(t)|]^{x^2}_0 \int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln|\tan(0)+\sec(0)| \int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| - \ln1 \int^{x^2}_0 \frac{1}{\cos(t)} dt = \ln|\tan(x^2)+\sec(x^2)| \int \sec(t) dt u=|\tan(x^2)+\sec(x^2)| F(x)=\ln u F'(x)=\frac{2x}{\cos (x^2)} = 2x\sec (x^2) x\in[0, \frac{\pi}{4}] F x\in[0, \frac{\pi}{4}]","['real-analysis', 'calculus', 'integration', 'derivatives', 'continuity']"
48,"Is $f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases}$ differentiable at 0?",Is  differentiable at 0?,"f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases}","I have the function $f:\mathbb{[0,1]\to R}$ defined by $$f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases}$$ and I must show whether or not $f$ is differentiable at $0$ . My first idea was to see if it was discontinuous at $0$ , however I found that it was in fact continuous, so now I know that $\underset{x\to 0}{\lim} f(x)=0$ but this doesn't help me to see if it is differentiable at $0$ . I know I must show whether the limit $$\underset{x\to 0}{\lim} \frac{f(x)}{x}$$ exists but I have no idea where to begin with this as my intuition would be to check both sides of the limit, however, $f$ is not well defined for $x<0$ so I can't do that. Is there something obvious I'm missing? This has been bugging me for a while now.","I have the function defined by and I must show whether or not is differentiable at . My first idea was to see if it was discontinuous at , however I found that it was in fact continuous, so now I know that but this doesn't help me to see if it is differentiable at . I know I must show whether the limit exists but I have no idea where to begin with this as my intuition would be to check both sides of the limit, however, is not well defined for so I can't do that. Is there something obvious I'm missing? This has been bugging me for a while now.","f:\mathbb{[0,1]\to R} f(x)=\begin{cases} \frac{1}{n} & x\in(\frac{1}{n+1},\frac{1}{n}]\\ 0 & x=0 \end{cases} f 0 0 \underset{x\to 0}{\lim} f(x)=0 0 \underset{x\to 0}{\lim} \frac{f(x)}{x} f x<0","['real-analysis', 'limits', 'analysis', 'derivatives']"
49,"Prove f(x) =0 for all x belonging to [0,1]","Prove f(x) =0 for all x belonging to [0,1]",,"Suppose f : [0, 1] → R is differentiable and f (0) = 0. Suppose $|f′(x)| ≤ |f(x)|^2$ , $∀x ∈ [0, 1]$ . Prove that $f(x) = 0, ∀x ∈ [0,1]$ . Since f is differentiable, then $f$ is continuous, then if such a function is continuous, then for any given $\varepsilon > 0$ and $x \in [a, b]$ , there exists a $\delta > 0$ such that $|x-y| < \delta \implies |f(x) - f(y)|< \varepsilon$ ...I am not too sure how to continue from here.","Suppose f : [0, 1] → R is differentiable and f (0) = 0. Suppose , . Prove that . Since f is differentiable, then is continuous, then if such a function is continuous, then for any given and , there exists a such that ...I am not too sure how to continue from here.","|f′(x)| ≤ |f(x)|^2 ∀x ∈ [0, 1] f(x) = 0, ∀x ∈ [0,1] f \varepsilon > 0 x \in [a, b] \delta > 0 |x-y| < \delta \implies |f(x) - f(y)|< \varepsilon","['real-analysis', 'derivatives']"
50,Prove that $(\frac{\pi}{3})^{\frac{3}{\pi}}+(\frac{3}{\pi})<2$,Prove that,(\frac{\pi}{3})^{\frac{3}{\pi}}+(\frac{3}{\pi})<2,"Prove that : $$\left(\frac{\pi}{3}\right)^{\frac{3}{\pi}}+\frac{3}{\pi}<2$$ Straightforward proof : Since the function $f(x)=(x)^{\frac{1}{x}}+\frac{1}{x}$ is decreasing on $\left[1,\frac{\pi}{3}\right]$ We get : $$f\left(\frac{\pi}{3}\right)=\left(\frac{\pi}{3}\right)^{\frac{3}{\pi}}+\frac{3}{\pi}<f(1)=2$$ So we get an almost integer easily . My questions : Have you an alternative proof ? Can we find other almost integer with this way? Thanks a lot for all your contributions .",Prove that : Straightforward proof : Since the function is decreasing on We get : So we get an almost integer easily . My questions : Have you an alternative proof ? Can we find other almost integer with this way? Thanks a lot for all your contributions .,"\left(\frac{\pi}{3}\right)^{\frac{3}{\pi}}+\frac{3}{\pi}<2 f(x)=(x)^{\frac{1}{x}}+\frac{1}{x} \left[1,\frac{\pi}{3}\right] f\left(\frac{\pi}{3}\right)=\left(\frac{\pi}{3}\right)^{\frac{3}{\pi}}+\frac{3}{\pi}<f(1)=2","['derivatives', 'inequality', 'alternative-proof', 'pi']"
51,Derivative Method of Proving of $\sqrt{x}>\ln x$,Derivative Method of Proving of,\sqrt{x}>\ln x,"I have a question about a proof I read, linked here , a proof that $\sqrt{x}>\ln x$ . The second answerer considers the derivative of the function, $f\left(x\right)=\sqrt{x}-\ln x$ . Now the derivative is negative for $0\leq x<4$ . Does this not mean, that $\sqrt{x}<\ln x$ on that interval? I.e. The derivative being negative suggests that the function is decreasing? Why is the global minimum analysed? I must be missing something important!","I have a question about a proof I read, linked here , a proof that . The second answerer considers the derivative of the function, . Now the derivative is negative for . Does this not mean, that on that interval? I.e. The derivative being negative suggests that the function is decreasing? Why is the global minimum analysed? I must be missing something important!",\sqrt{x}>\ln x f\left(x\right)=\sqrt{x}-\ln x 0\leq x<4 \sqrt{x}<\ln x,"['calculus', 'derivatives']"
52,Intuition for $\overline{z}$ not being differentiable in the complex plane,Intuition for  not being differentiable in the complex plane,\overline{z},"I am trying to get some intuition for the meaning of a complex derivative. When talking about real numbers, the function $f(x)=|x|$ is not differentiable at $x=0$ , since there is a ""sharp corner"" there, i.e., the limits from right and left are not the same, hence the function is not smooth. The math is similar when talking about complex numbers: for $f(z)=\overline{z}$ , for any $z_{0}\in\mathbb{C}$ , denoting $z-z_0=re^{i\theta}$ , we see that $\frac{\overline{z-z_{0}}}{z-z_0}=e^{-i2\theta}$ can have any value in $[-1,1]$ no matter how close to $z_0$ we approach.  However, it is somewhat unintuitive for me that $\overline{z}$ is nowhere differentiabe in $\mathbb{C}$ . Specifically: The complex conjugate is just the number $z$ reflected across the $x$ -axis. What makes this kind of reflection impossible to differentiate, while the similar reflection $f(z)=-z$ is differentiable everywhere in the complex plane? Is there a ""sharp corner"" in some essence in the function $f(z)=\overline{z}$ , similar to the one in $f(x)=|z|$ in $\mathbb{R}$ ? Separating the function to its action on the real and imaginary parts, we see that $f(x+yi)=u(x,y)+iv(x,y)$ where $u(x,y)=x$ and $v(x,y)=-y$ . Both $u$ and $v$ feel ""smooth"", how is it that $f$ isn't? Some graphical or other intuitive explanations will be helpful. Thank you.","I am trying to get some intuition for the meaning of a complex derivative. When talking about real numbers, the function is not differentiable at , since there is a ""sharp corner"" there, i.e., the limits from right and left are not the same, hence the function is not smooth. The math is similar when talking about complex numbers: for , for any , denoting , we see that can have any value in no matter how close to we approach.  However, it is somewhat unintuitive for me that is nowhere differentiabe in . Specifically: The complex conjugate is just the number reflected across the -axis. What makes this kind of reflection impossible to differentiate, while the similar reflection is differentiable everywhere in the complex plane? Is there a ""sharp corner"" in some essence in the function , similar to the one in in ? Separating the function to its action on the real and imaginary parts, we see that where and . Both and feel ""smooth"", how is it that isn't? Some graphical or other intuitive explanations will be helpful. Thank you.","f(x)=|x| x=0 f(z)=\overline{z} z_{0}\in\mathbb{C} z-z_0=re^{i\theta} \frac{\overline{z-z_{0}}}{z-z_0}=e^{-i2\theta} [-1,1] z_0 \overline{z} \mathbb{C} z x f(z)=-z f(z)=\overline{z} f(x)=|z| \mathbb{R} f(x+yi)=u(x,y)+iv(x,y) u(x,y)=x v(x,y)=-y u v f","['complex-analysis', 'derivatives', 'intuition']"
53,Show that the stereographic projection is an homeomorphism,Show that the stereographic projection is an homeomorphism,,"I want to show that $f: \Bbb{R}^2 \to \Bbb{R}^3$ defined by $$f(x,y) = \left(\frac{2x}{1+x^2+y^2},\frac{2y}{1+x^2+y^2},\frac{1-(x^2+y^2)}{1+x^2+y^2}\right)$$ is a parameterization. My definition of parameterization is immersion that is a homeomorphism over its image. I can show that $Df$ is injective when $x,y \neq 0$ or only one of them is $0$ . My problem is when $x = y = 0$ . In the case of $x = y = 0$ , seems that $Df$ is not injective so, should I consider $\Bbb{R}^2 - \{(0,0)\}$ instead of $\Bbb{R}^2$ ? I'm not sure about that, because I think that in this case, we must find an homeomorphism between $\Bbb{R}^2$ and $S^2 - \{(0,0,1)\}$ . I appreciate any help. EDIT. I found $$Df(x,y) = \frac{2}{(1+x^2+y^2)^2}\left(\begin{array}{ccc} 1+y^2-x^2 & 1+x^2-y^2\\ -2xy & -2xy\\ -2x & -2y \end{array}\right).$$ If, $x = y = 0$ , then I cannot conclude that $Df$ is injective.","I want to show that defined by is a parameterization. My definition of parameterization is immersion that is a homeomorphism over its image. I can show that is injective when or only one of them is . My problem is when . In the case of , seems that is not injective so, should I consider instead of ? I'm not sure about that, because I think that in this case, we must find an homeomorphism between and . I appreciate any help. EDIT. I found If, , then I cannot conclude that is injective.","f: \Bbb{R}^2 \to \Bbb{R}^3 f(x,y) = \left(\frac{2x}{1+x^2+y^2},\frac{2y}{1+x^2+y^2},\frac{1-(x^2+y^2)}{1+x^2+y^2}\right) Df x,y \neq 0 0 x = y = 0 x = y = 0 Df \Bbb{R}^2 - \{(0,0)\} \Bbb{R}^2 \Bbb{R}^2 S^2 - \{(0,0,1)\} Df(x,y) = \frac{2}{(1+x^2+y^2)^2}\left(\begin{array}{ccc}
1+y^2-x^2 & 1+x^2-y^2\\
-2xy & -2xy\\
-2x & -2y
\end{array}\right). x = y = 0 Df","['real-analysis', 'derivatives', 'differential-geometry']"
54,Show that there exists a number c with a certain condition,Show that there exists a number c with a certain condition,,"$f:[-1,1]\rightarrow\Bbb{R} $ $f(-1)=f(1)=0$ , both $f$ and $f'$ are differeniable, $a\in(-1,1)$ . Show that $\exists_{c\in(-1,1)}(f''(c)=\dfrac{2f(a)}{a^2-1})$ . By Lagrange's middle point theorem, we have that: $\exists_{x_{1}\in(-1,a)}\space f'(x_1)=\dfrac{f(a)-f(-1)}{a+1}=\dfrac{f(a)}{a+1}$ $\exists_{x_{2}\in(-1,a)}\space f'(x_2)=\dfrac{f(a)-f(1)}{a-1}=\dfrac{f(a)}{a-1}$ . We know that $f'$ is differentiable so we can apply the same theorem again and obtain $\exists_{c\in(x_1,x_2)}\space f''(c)=\dfrac{f'(x_1)-f'(x_2)}{x_1-x_2}=\dfrac{\dfrac{f(a)}{a+1}-\dfrac{f(a)}{a-1}}{x_1-x_2}=\dfrac{2f(a)}{a^2-1}\dfrac{1}{x_2-x_1}$ . I thought this might be a mistake by the author and tried to find a counterexample but the thesis does seem to be correct. It's so close to the answer but I have no clue how to take it from here or if this approach is even salvageable. Why would it be so that we can always find such $x_1$ and $x_2$ that are separated by exactly $1$ ?",", both and are differeniable, . Show that . By Lagrange's middle point theorem, we have that: . We know that is differentiable so we can apply the same theorem again and obtain . I thought this might be a mistake by the author and tried to find a counterexample but the thesis does seem to be correct. It's so close to the answer but I have no clue how to take it from here or if this approach is even salvageable. Why would it be so that we can always find such and that are separated by exactly ?","f:[-1,1]\rightarrow\Bbb{R}  f(-1)=f(1)=0 f f' a\in(-1,1) \exists_{c\in(-1,1)}(f''(c)=\dfrac{2f(a)}{a^2-1}) \exists_{x_{1}\in(-1,a)}\space f'(x_1)=\dfrac{f(a)-f(-1)}{a+1}=\dfrac{f(a)}{a+1} \exists_{x_{2}\in(-1,a)}\space f'(x_2)=\dfrac{f(a)-f(1)}{a-1}=\dfrac{f(a)}{a-1} f' \exists_{c\in(x_1,x_2)}\space f''(c)=\dfrac{f'(x_1)-f'(x_2)}{x_1-x_2}=\dfrac{\dfrac{f(a)}{a+1}-\dfrac{f(a)}{a-1}}{x_1-x_2}=\dfrac{2f(a)}{a^2-1}\dfrac{1}{x_2-x_1} x_1 x_2 1","['calculus', 'derivatives']"
55,"How to calculate the tangent line of $\ln(x)$ through $(2,6)$ by hand?",How to calculate the tangent line of  through  by hand?,"\ln(x) (2,6)","I came up with my own question for a tutor student, but now I am stuck myself. The exercise was to calculate the equation of a tangent-line of $f(x) = \ln(x)$ , which goes through the point $(2,6)$ . I'm trying to solve the problem by hand, but the equations I get to solve the problem aren't solveable with the lineair algebra I'm aware of. Is this right, or am I missing something? Representation of the problem, where the green line represents $\ln(x), A = (2,6)$ and the black line is the unknown which should be calculated:","I came up with my own question for a tutor student, but now I am stuck myself. The exercise was to calculate the equation of a tangent-line of , which goes through the point . I'm trying to solve the problem by hand, but the equations I get to solve the problem aren't solveable with the lineair algebra I'm aware of. Is this right, or am I missing something? Representation of the problem, where the green line represents and the black line is the unknown which should be calculated:","f(x) = \ln(x) (2,6) \ln(x), A = (2,6)","['derivatives', 'logarithms', 'tangent-line']"
56,Evaluate using differentiation under the sign of integration: $\int_{0}^{\pi} \frac {\ln (1+a\cos (x))}{\cos (x)} dx$,Evaluate using differentiation under the sign of integration:,\int_{0}^{\pi} \frac {\ln (1+a\cos (x))}{\cos (x)} dx,Evaluate by using the rule of differentiation under the sign of integration $\int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx}$ . My Attempt: Given integral is $\int_{0}^{\pi} \dfrac {\ln(1+a\cos (x))}{\cos (x)}$ . Here $a$ is the parameter so let $$F(a)=\int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx}$$ Differentiating both sides w.r.t $a$ $$\dfrac {dF(a)}{da} = \dfrac {d}{da} \int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx}$$ By Leibnitz Theorem: $$\dfrac {dF(a)}{da} = \int_{0}^{\pi} \dfrac {1}{1+a\cos (x)} \times \dfrac {1}{\cos (x)} \times \cos (x)   \textrm {dx}$$ $$\dfrac {dF(a)}{da}=\int_{0}^{\pi} \dfrac {dx}{1+a\cos (x)} \textrm {dx}$$ Now writing $\cos (x)= \dfrac {1-\tan^{2} (\dfrac {x}{2})}{1+\tan^2 (\dfrac {x}{2})}$ and proceeding with integration becomes quite cumbersome to carry on. Is there any way to simplify with some easy steps?,Evaluate by using the rule of differentiation under the sign of integration . My Attempt: Given integral is . Here is the parameter so let Differentiating both sides w.r.t By Leibnitz Theorem: Now writing and proceeding with integration becomes quite cumbersome to carry on. Is there any way to simplify with some easy steps?,"\int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx} \int_{0}^{\pi} \dfrac {\ln(1+a\cos (x))}{\cos (x)} a F(a)=\int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx} a \dfrac {dF(a)}{da} = \dfrac {d}{da} \int_{0}^{\pi} \dfrac {\ln (1+a\cos (x))}{\cos (x)} \textrm {dx} \dfrac {dF(a)}{da} = \int_{0}^{\pi} \dfrac {1}{1+a\cos (x)} \times \dfrac {1}{\cos (x)} \times \cos (x) 
 \textrm {dx} \dfrac {dF(a)}{da}=\int_{0}^{\pi} \dfrac {dx}{1+a\cos (x)} \textrm {dx} \cos (x)= \dfrac {1-\tan^{2} (\dfrac {x}{2})}{1+\tan^2 (\dfrac {x}{2})}","['calculus', 'integration', 'derivatives', 'definite-integrals']"
57,Primitive and derivative of little $o$,Primitive and derivative of little,o,"For $x\in {\rm I\!R}$ in a neighborhood of $0$ and $n>1$ , we define the function $f(x) = o(x^n)$ and one of its primitives $F(x) = \int_0^x f(t)dt$ . Is it true that: $F(x) = o(x^{n+1})$ $f'(x) = o(x^{n-1})$ How to prove it? Here are my attempts: I need to prove that: $$ \lim_{x\rightarrow 0} \frac{F(x)}{x^{n+1}} = 0 $$ I tried to develop the fraction : $$ \frac{F(x)}{x^{n+1}} = \int_0^x \frac{f(t)}{x^{n+1}}dt $$ But I couldn't go further... A related question seems to add a condition to get this true but I coundn't really understand the comments. I would use L'Hôpital's rule : $$ \lim_{x\rightarrow 0} \frac{f'(x)}{x^{n-1}} = \lim_{x\rightarrow 0} \frac{f(x)}{\frac{x^n}{n}} = 0  $$ Is it correct ?","For in a neighborhood of and , we define the function and one of its primitives . Is it true that: How to prove it? Here are my attempts: I need to prove that: I tried to develop the fraction : But I couldn't go further... A related question seems to add a condition to get this true but I coundn't really understand the comments. I would use L'Hôpital's rule : Is it correct ?","x\in {\rm I\!R} 0 n>1 f(x) = o(x^n) F(x) = \int_0^x f(t)dt F(x) = o(x^{n+1}) f'(x) = o(x^{n-1}) 
\lim_{x\rightarrow 0} \frac{F(x)}{x^{n+1}} = 0
 
\frac{F(x)}{x^{n+1}} = \int_0^x \frac{f(t)}{x^{n+1}}dt
 
\lim_{x\rightarrow 0} \frac{f'(x)}{x^{n-1}} = \lim_{x\rightarrow 0} \frac{f(x)}{\frac{x^n}{n}} = 0 
","['real-analysis', 'integration', 'derivatives', 'asymptotics']"
58,Set of values of $x$ for which $1+\log x<x$,Set of values of  for which,x 1+\log x<x,"Find the set of values of $x$ for which $$1+\log x<x$$ $$ x>0\\ f(x)=\log x+1-x<0\\ f(1)=0\\ f'(x)=\frac{1}{x}-1\\ x>1\implies f'(x)<0\\ 0<x<1\implies f'(x)>0\\ \implies x\in(1,\infty) $$ But, my reference gives the solution $x\in(0,1)\cup(1,\infty)$ , why am I missing the additional domain ?","Find the set of values of for which But, my reference gives the solution , why am I missing the additional domain ?","x 1+\log x<x 
x>0\\
f(x)=\log x+1-x<0\\
f(1)=0\\
f'(x)=\frac{1}{x}-1\\
x>1\implies f'(x)<0\\
0<x<1\implies f'(x)>0\\
\implies x\in(1,\infty)
 x\in(0,1)\cup(1,\infty)","['derivatives', 'inequality', 'logarithms']"
59,Gradient of $\mathcal{L}(W) = -\frac{n}{2}\left\{d\ln(2\pi)+\ln|C|+\mbox{Tr}(C^{-1}S)\right\}$ w.r.t. $W$,Gradient of  w.r.t.,\mathcal{L}(W) = -\frac{n}{2}\left\{d\ln(2\pi)+\ln|C|+\mbox{Tr}(C^{-1}S)\right\} W,"I have the following function $\mathcal{L}(W)$ and I want to find the gradient with respect to $W$ , but I'm struggling with the matrix operations and derivations. $$\mathcal{L}(W) := -\frac{n}{2}\left\{ d\ln(2\pi) + \ln|C| + \mbox{Tr}(C^{-1}S) \right\}$$ where $C := WW^T + \sigma^2I$ . You can consider $S$ , which is positive definite, as constant. The gradient should be $$\nabla_W \mathcal{L}(W) = -n \left( C^{-1} S C^{-1} W - C^{-1} W \right)$$ but I cannot understand the steps that lead to it. If someone is interested, this is the probabilistic PCA, and you can find more information here .","I have the following function and I want to find the gradient with respect to , but I'm struggling with the matrix operations and derivations. where . You can consider , which is positive definite, as constant. The gradient should be but I cannot understand the steps that lead to it. If someone is interested, this is the probabilistic PCA, and you can find more information here .",\mathcal{L}(W) W \mathcal{L}(W) := -\frac{n}{2}\left\{ d\ln(2\pi) + \ln|C| + \mbox{Tr}(C^{-1}S) \right\} C := WW^T + \sigma^2I S \nabla_W \mathcal{L}(W) = -n \left( C^{-1} S C^{-1} W - C^{-1} W \right),"['matrices', 'derivatives', 'matrix-calculus']"
60,Calculus application of rate of change problem,Calculus application of rate of change problem,,"I am not exactly sure how to go about solving the following question. I made a drawing to help visualize the problem and thought about assuming just for the sake of this problem that one car is stationary while the other is driving, so I will be able to fit the two cars onto the same drawing, and thus make the problem somewhat more clear. However, I am really lost, and I don't really know how to approach this question as I find there's too much information that needs to be used, which is quite overwhelming. Here's the question: Two cars travel on parallel “north-south” roads that are 2km apart from each other. The first car is traveling north on road #1 at a speed of 20km/h. While the second car is traveling south at a speed of 60km/h. How fast is the distance between them changing, when the distance between them is 8km? Furthermore, are there any general strategies that can be applied to questions of such nature? What would be an effective way to become comfortable with solving such questions. Any help would be greatly appreciated!","I am not exactly sure how to go about solving the following question. I made a drawing to help visualize the problem and thought about assuming just for the sake of this problem that one car is stationary while the other is driving, so I will be able to fit the two cars onto the same drawing, and thus make the problem somewhat more clear. However, I am really lost, and I don't really know how to approach this question as I find there's too much information that needs to be used, which is quite overwhelming. Here's the question: Two cars travel on parallel “north-south” roads that are 2km apart from each other. The first car is traveling north on road #1 at a speed of 20km/h. While the second car is traveling south at a speed of 60km/h. How fast is the distance between them changing, when the distance between them is 8km? Furthermore, are there any general strategies that can be applied to questions of such nature? What would be an effective way to become comfortable with solving such questions. Any help would be greatly appreciated!",,"['calculus', 'derivatives']"
61,The differentiability of absolute value of a function implies that the function is differentiable?,The differentiability of absolute value of a function implies that the function is differentiable?,,"I.e., if |f| is differentiable at a point $a$ , is $f$ differentiable at $a$ too? Obviously this will not work for a piecewise function $f$ whose absolute value is continuous, but I can't figure out the sufficient conditions for $f$ to be differentiable. It seems like continuity of $f$ at $a$ should force it to be differentiable at $a$ , but I don't see any algebraic manipulations in $\lim_{h\to0}\frac{f(a+h)-f(a)}{h}$ to make use of the continuity limit, that $\lim_{h\to 0}f(a+h)=f(a)$ .","I.e., if |f| is differentiable at a point , is differentiable at too? Obviously this will not work for a piecewise function whose absolute value is continuous, but I can't figure out the sufficient conditions for to be differentiable. It seems like continuity of at should force it to be differentiable at , but I don't see any algebraic manipulations in to make use of the continuity limit, that .",a f a f f f a a \lim_{h\to0}\frac{f(a+h)-f(a)}{h} \lim_{h\to 0}f(a+h)=f(a),"['analysis', 'derivatives']"
62,How to further expand $\text{grad} \left( \vec{a} \cdot\vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot\vec{b} \right )$?,How to further expand ?,\text{grad} \left( \vec{a} \cdot\vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot\vec{b} \right ),"With $\vec{a}, \vec{b}: \mathbb{R}^3 \to \mathbb{R}^3$ vector fields: I want to expand $\text{grad} \left( \vec{a} \cdot \vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot \vec{b} \right )$ . So I started with: $\left [\vec{\nabla}\left (\vec{a} \cdot \vec{b} \right )  \right ]_i = \partial_i\left (a_j b_j \right ) \overset{\text{Product rule}}{=} \left ( \partial_i a_j \right ) b_j + a_j \left( \partial_i b_j \right )$ But where to go from there? In the end I'm supposed to arrive at: $\text{grad} \left( \vec{a} \cdot \vec{b} \right ) =\vec{a} \times \left(\vec{\nabla} \times \vec{b}\right) + \vec{b} \times \left(\vec{\nabla} \times \vec{a}\right) + \left(\vec{b} \cdot\vec{\nabla}\right) \vec{a} + \left(\vec{a} \cdot\vec{\nabla} \right) \vec{b}$",With vector fields: I want to expand . So I started with: But where to go from there? In the end I'm supposed to arrive at:,"\vec{a}, \vec{b}: \mathbb{R}^3 \to \mathbb{R}^3 \text{grad} \left( \vec{a} \cdot \vec{b} \right ) = \vec{\nabla} \left (\vec{a} \cdot \vec{b} \right ) \left [\vec{\nabla}\left (\vec{a} \cdot \vec{b} \right )  \right ]_i = \partial_i\left (a_j b_j \right ) \overset{\text{Product rule}}{=} \left ( \partial_i a_j \right ) b_j + a_j \left( \partial_i b_j \right ) \text{grad} \left( \vec{a} \cdot \vec{b} \right ) =\vec{a} \times \left(\vec{\nabla} \times \vec{b}\right) + \vec{b} \times \left(\vec{\nabla} \times \vec{a}\right) + \left(\vec{b} \cdot\vec{\nabla}\right) \vec{a} + \left(\vec{a} \cdot\vec{\nabla} \right) \vec{b}","['derivatives', 'notation', 'vector-fields']"
63,How do I prove if the following functions are differentiable at the given value?,How do I prove if the following functions are differentiable at the given value?,,I have been stumped on the attached question for a while. What would I have to do in order to prove if the functions are differentiable at the given value? I'm only stuck on how to start the problem.,I have been stumped on the attached question for a while. What would I have to do in order to prove if the functions are differentiable at the given value? I'm only stuck on how to start the problem.,,"['calculus', 'limits', 'derivatives', 'limits-without-lhopital']"
64,Derivative of Var($\alpha X + (1 - \alpha)Y$) w.r.t. $\alpha$?,Derivative of Var() w.r.t. ?,\alpha X + (1 - \alpha)Y \alpha,It's supposed to result as $   2\alpha \sigma_x^2 - 2\sigma_Y^2 + 2\alpha\sigma_Y^2 + 2\sigma_{XY}-4\alpha\sigma_{XY} $ But I don't remember the rules of Var() algebra.,It's supposed to result as But I don't remember the rules of Var() algebra.,"
  2\alpha \sigma_x^2 - 2\sigma_Y^2 + 2\alpha\sigma_Y^2 + 2\sigma_{XY}-4\alpha\sigma_{XY}
","['derivatives', 'variance']"
65,How to Differentiate two equations to find Maximum Values,How to Differentiate two equations to find Maximum Values,,"I am stuck on this Differentiation problem, any help would be great! If $A=xy$ and $x+5y=20$ find the maximum value of $A$ and the values of $x$ and $y$ for which this maximum value occurs","I am stuck on this Differentiation problem, any help would be great! If and find the maximum value of and the values of and for which this maximum value occurs",A=xy x+5y=20 A x y,"['calculus', 'derivatives']"
66,Why does $|f(x)| \leq x^2 \implies$ $f$ is differentiable in $0$?,Why does   is differentiable in ?,|f(x)| \leq x^2 \implies f 0,Why does $|f(x)|\leq x^2 \implies$ $f$ is differentiable in $0$ ? Here's my proof:,Why does is differentiable in ? Here's my proof:,|f(x)|\leq x^2 \implies f 0,"['real-analysis', 'derivatives']"
67,Easy way to compute this limit,Easy way to compute this limit,,How to easily compute the following limit without l'Hospital : $$\lim_{x\to 0} \frac{\cos(x)+\cos(2x)-2}{\cos(3x)+\sin(x)-1}$$ Ps: Is it possible to use the derivative of a function ?,How to easily compute the following limit without l'Hospital : Ps: Is it possible to use the derivative of a function ?,\lim_{x\to 0} \frac{\cos(x)+\cos(2x)-2}{\cos(3x)+\sin(x)-1},['limits']
68,Differentiating $\frac{dy}{dx}=3x+2y+xy$?,Differentiating ?,\frac{dy}{dx}=3x+2y+xy,"What would $y''$ be of $\frac{dy}{dx}=3x+2y+xy$ ? In other words, what is the result ( $y''$ ) if I differentiate $y'=3x+2y+xy$ ? Can I do the following: Set $$f(x,y) = 3x+2y+xy$$ then $$f'(x,y) = \frac{df}{dx} + \frac{df}{dy}\frac{dy}{dx}$$ $$ f'(x,y) = (3 + y)+(2+x)(y')$$ $$ f'(x,y) = 3 + y + 2y'+ xy'$$ ?","What would be of ? In other words, what is the result ( ) if I differentiate ? Can I do the following: Set then ?","y'' \frac{dy}{dx}=3x+2y+xy y'' y'=3x+2y+xy f(x,y) = 3x+2y+xy f'(x,y) = \frac{df}{dx} + \frac{df}{dy}\frac{dy}{dx}  f'(x,y) = (3 + y)+(2+x)(y')  f'(x,y) = 3 + y + 2y'+ xy'","['calculus', 'derivatives']"
69,"For sufficiently small arguments, why does change of function has the same sign as differential?","For sufficiently small arguments, why does change of function has the same sign as differential?",,"From I. M. Gelfand, S. V. Fomin - Calculus of Variations (2000) page 13: ""A necessary condition for the differentiable functional $J[y]$ to have an extremum for y = y_0 is that its variation vanishes for $y = y_0$ , i.e., that $\delta J[h] = 0$ for $y = y_0$ and all admissible $h$ ."" He proceeds proving the following theorem by the following argument. ""To be explicit, suppose $J[y]$ has a minimum for $y = y_0$ . According to the definition of the variation $\delta J[h]$ , we have $\Delta J [h] = \delta J[h] + \epsilon ||h||$ , where $\epsilon \to 0$ as $||h|| \to 0$ . Thus, for sufficiently small $||h||$ the sign of $\delta J[h]$ will be the same as the sign of $\Delta J[h]$ . "" I have trouble understanding the last statement. I understand that for $J[y]$ to have a minimum we have by definition that there is some $\delta>0$ such that for all $||h|| < \delta$ we have $\Delta J[h] \geq 0$ . So I would like to find $\delta_2 > 0$ that would guarantee me that $\delta J[h] \geq 0$ for all $||h|| <$ min $(\delta_1, \delta_2)$ . Unfortunately, I don't see how. I feel like we have to assume continuity of $J[y]$ but I am not positive. Any help or suggestions are appreciated!","From I. M. Gelfand, S. V. Fomin - Calculus of Variations (2000) page 13: ""A necessary condition for the differentiable functional to have an extremum for y = y_0 is that its variation vanishes for , i.e., that for and all admissible ."" He proceeds proving the following theorem by the following argument. ""To be explicit, suppose has a minimum for . According to the definition of the variation , we have , where as . Thus, for sufficiently small the sign of will be the same as the sign of . "" I have trouble understanding the last statement. I understand that for to have a minimum we have by definition that there is some such that for all we have . So I would like to find that would guarantee me that for all min . Unfortunately, I don't see how. I feel like we have to assume continuity of but I am not positive. Any help or suggestions are appreciated!","J[y] y = y_0 \delta J[h] = 0 y = y_0 h J[y] y = y_0 \delta J[h] \Delta J [h] = \delta J[h] + \epsilon ||h|| \epsilon \to 0 ||h|| \to 0 ||h|| \delta J[h] \Delta J[h] J[y] \delta>0 ||h|| < \delta \Delta J[h] \geq 0 \delta_2 > 0 \delta J[h] \geq 0 ||h|| < (\delta_1, \delta_2) J[y]","['limits', 'derivatives']"
70,"Suppose $f(x)$ has continuous second-order derivative over $(a,+\infty)$, and $f(x)>0$,$f''(x)\leq 0$. Prove $f'(x)\geq 0$.","Suppose  has continuous second-order derivative over , and ,. Prove .","f(x) (a,+\infty) f(x)>0 f''(x)\leq 0 f'(x)\geq 0","Suppose $f(x)$ has continuous second-order derivative over $(a,+\infty)$ , and $f(x)>0$ , $f''(x)\leq 0$ . Prove $f'(x)\geq 0$ . Proof Consider proving by contradiction. If the conclusion does not hold, then $$ \exists x_0 \in (a,+\infty):f'(x_0)<0.$$ Since $f''(x)\leq 0$ , $f'(x)$ is nonincreasing. Therefore $$\forall x \in [x_0,+\infty):f'(x)\leq f'(x_0)<0.$$ One can claim that $f'(x)$ has a limit as $x \to +\infty$ , which is either a finite negtive number or the negative infinity.Thus, by L'Hôpital's rule, $$\lim_{x \to +\infty}\frac{f(x)}{x}=\lim_{x \to +\infty}f'(x)<0,$$ which contradicts, just noticing that $f(x)/x$ is positive with a sufficiently large $x$ , according to the assumption condition. Please correct me if I'm wrong!","Suppose has continuous second-order derivative over , and , . Prove . Proof Consider proving by contradiction. If the conclusion does not hold, then Since , is nonincreasing. Therefore One can claim that has a limit as , which is either a finite negtive number or the negative infinity.Thus, by L'Hôpital's rule, which contradicts, just noticing that is positive with a sufficiently large , according to the assumption condition. Please correct me if I'm wrong!","f(x) (a,+\infty) f(x)>0 f''(x)\leq 0 f'(x)\geq 0  \exists x_0 \in (a,+\infty):f'(x_0)<0. f''(x)\leq 0 f'(x) \forall x \in [x_0,+\infty):f'(x)\leq f'(x_0)<0. f'(x) x \to +\infty \lim_{x \to +\infty}\frac{f(x)}{x}=\lim_{x \to +\infty}f'(x)<0, f(x)/x x","['calculus', 'proof-verification', 'derivatives']"
71,Derivative of $f(x)=\int_{x}^{\sqrt {x^2+1}} \sin (t^2) dt$,Derivative of,f(x)=\int_{x}^{\sqrt {x^2+1}} \sin (t^2) dt,Derivative of $f(x)=\int_{x}^{\sqrt {x^2+1}} \sin (t^2) dt$ Firstly I wanted to calculate $\int \sin (t^2) dt$ and then use $x$ and $\sqrt {x^2+1}$ . But this antiderivative not exist so how can I do this? Is this function at all possible to count?,Derivative of Firstly I wanted to calculate and then use and . But this antiderivative not exist so how can I do this? Is this function at all possible to count?,f(x)=\int_{x}^{\sqrt {x^2+1}} \sin (t^2) dt \int \sin (t^2) dt x \sqrt {x^2+1},"['real-analysis', 'calculus', 'derivatives']"
72,Derivative of a quadratic form — how to derive it? [duplicate],Derivative of a quadratic form — how to derive it? [duplicate],,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I want to know how $$\frac{\delta(x^TAx)}{\delta(x)}=2Ax$$ I think here's what happens (please correct me where wrong): (by: the rule for matrix derivative) $$ \frac{\delta(x^TAx)}{\delta(w)}=x^T(A^T+A) $$ (by: I assume we can transpose matrices whenever we need to?) $$ x^T(A^T+A) = x^T(A^T+A^T) = x^T(2A^T) $$ (by: transpose property) $$ x^T(2A^T) = (x^T2A^T)^T = 2Ax $$ Is that how this happens?,This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I want to know how I think here's what happens (please correct me where wrong): (by: the rule for matrix derivative) (by: I assume we can transpose matrices whenever we need to?) (by: transpose property) Is that how this happens?,"\frac{\delta(x^TAx)}{\delta(x)}=2Ax 
\frac{\delta(x^TAx)}{\delta(w)}=x^T(A^T+A)
 
x^T(A^T+A) = x^T(A^T+A^T) = x^T(2A^T)
 
x^T(2A^T) = (x^T2A^T)^T = 2Ax
","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'quadratic-forms']"
73,Evaluating a limit of $xf(2)$ - $2f(x)/x-2$ as $x$ approaches $2$,Evaluating a limit of  -  as  approaches,xf(2) 2f(x)/x-2 x 2,"I was doing exercises on limit and I came across this question: find the limit of ( $xf(2)$ - $2f(x)$ ) $/$ ( $x-2$ ) as $x$ approaches 2 given $f(2) = 7$ and $f'(2) = 5$ . so I proceeded this way: as $ x $ approaches 2, $ xf(2) $ approaches $ 2f(2) $ . Therefore, $xf(2)$ - $2f(x)$ $/$ $x-2$ $ = $ $ 2f(2)$ - $2f(x)$ $/$ $x-2$ Factoring $2$ and since $ lim f(x) - f(a)/x-a $ is $f'(x)$ The limit evaluates to $2f'(x)$ Is the way I proceeded the right way? How do you do similar questions like: $ lim$ $ nf(x) - f(a)/x-a $ or $ lim f(x) - nf(a)/x-a $","I was doing exercises on limit and I came across this question: find the limit of ( - ) ( ) as approaches 2 given and . so I proceeded this way: as approaches 2, approaches . Therefore, - - Factoring and since is The limit evaluates to Is the way I proceeded the right way? How do you do similar questions like: or",xf(2) 2f(x) / x-2 x f(2) = 7 f'(2) = 5  x   xf(2)   2f(2)  xf(2) 2f(x) / x-2  =   2f(2) 2f(x) / x-2 2  lim f(x) - f(a)/x-a  f'(x) 2f'(x)  lim  nf(x) - f(a)/x-a   lim f(x) - nf(a)/x-a ,"['limits', 'derivatives']"
74,Using chain rule to calculate Fréchet derivative of $F(X) = \det(A^T (I - X) A)$,Using chain rule to calculate Fréchet derivative of,F(X) = \det(A^T (I - X) A),"Let $\mathbb{M}^n$ be the set of real $n \times n$ matrices, and let $A$ be a fixed real $n \times n$ matrix. Define the function $F: \mathbb{M}^n \rightarrow \mathbb{R}$ by $$ F(X) = \det( A^T (I-X) A) $$ What is the Fréchet derivative (total derivative) of $F$ at a matrix $X$ ? I am having difficulty working it out explicitly. I know that we can apply a chain rule, as $F = G \circ H$ where $H(X) = A^T(I-X)A$ and $G(Y) = \det(Y)$ . Yet, I am confused how to apply the chain rule here. I am looking for an expression for the Fréchet derivative and a derivation would be wonderful as well.","Let be the set of real matrices, and let be a fixed real matrix. Define the function by What is the Fréchet derivative (total derivative) of at a matrix ? I am having difficulty working it out explicitly. I know that we can apply a chain rule, as where and . Yet, I am confused how to apply the chain rule here. I am looking for an expression for the Fréchet derivative and a derivation would be wonderful as well.",\mathbb{M}^n n \times n A n \times n F: \mathbb{M}^n \rightarrow \mathbb{R}  F(X) = \det( A^T (I-X) A)  F X F = G \circ H H(X) = A^T(I-X)A G(Y) = \det(Y),"['derivatives', 'matrix-calculus', 'frechet-derivative']"
75,Generating function of binomial coefficients,Generating function of binomial coefficients,,"We want to evaluate the sum $$\sum_{L=0}^{\infty}\frac{1}{2}L(L+1)x^L$$ From this set of notes (page 2, equation 8) we find the formula $$\sum_{n=0}^{\infty}\binom{n}{k}y^n = \frac{y^n}{(1-y)^{n+1}}$$ which suggests that I can do $$\frac{1}{x}\sum_{L=0}^{\infty}\binom{L+1}{2}x^{L+1} = \frac{x^L}{(1-x)^{L+2}}\tag{1}\label{eqn1}$$ since $$\frac{1}{2}L(L+1) = \frac{(L+1)!}{2!((L+1)-2)!}$$ But if I do \begin{aligned} \sum_{L=0}^{\infty}\frac{1}{2}L(L+1)x^L & = \sum_{L=0}^{\infty}\frac{L}{2}\frac{d}{dx}x^{L+1} \\  & = \sum_{L=0}^{\infty} \frac{1}{2} \frac{d}{dx} \left[(L+2)x^{L+1} - 2x^{L+1}\right] \\ & = \frac{1}{2} \frac{d^2}{dx^2} \sum_{L=0}^{\infty} x^{L+2} - \frac{d}{dx} \sum_{L=0}^{\infty} x^{L+1} \\ & = \frac{1}{2} \frac{d^2}{dx^2} \frac{x^2}{1-x} - \frac{d}{dx} \frac{x}{1-x} \\ & = \frac{1}{2} \frac{d}{dx} \left[2x(1-x)^{-1} + x^2(1-x)^{-2}\right] - \left[(1-x)^{-1} + x(1-x)^{-2}\right] \\ & = \frac{1}{2} \frac{d}{dx} \frac{2x-x^2}{(1-x)^2} - \frac{1}{(1-x)^2} \\ & = \frac{1}{2} \frac{d}{dx} \left[\frac{x}{(1-x)^2} + \frac{x}{1-x} \right] - \frac{1}{(1-x)^2} \\ & = \frac{1}{2} \left[(1-x)^{-2} + 2x(1-x)^{-3} \right] \\ & = \frac{1+x}{2(1-x)^3} \end{aligned} which is nowhere close to (1). Where did I go wrong? All help is welcome.","We want to evaluate the sum From this set of notes (page 2, equation 8) we find the formula which suggests that I can do since But if I do which is nowhere close to (1). Where did I go wrong? All help is welcome.","\sum_{L=0}^{\infty}\frac{1}{2}L(L+1)x^L \sum_{n=0}^{\infty}\binom{n}{k}y^n = \frac{y^n}{(1-y)^{n+1}} \frac{1}{x}\sum_{L=0}^{\infty}\binom{L+1}{2}x^{L+1} = \frac{x^L}{(1-x)^{L+2}}\tag{1}\label{eqn1} \frac{1}{2}L(L+1) = \frac{(L+1)!}{2!((L+1)-2)!} \begin{aligned}
\sum_{L=0}^{\infty}\frac{1}{2}L(L+1)x^L & = \sum_{L=0}^{\infty}\frac{L}{2}\frac{d}{dx}x^{L+1} \\ 
& = \sum_{L=0}^{\infty} \frac{1}{2} \frac{d}{dx} \left[(L+2)x^{L+1} - 2x^{L+1}\right] \\
& = \frac{1}{2} \frac{d^2}{dx^2} \sum_{L=0}^{\infty} x^{L+2} - \frac{d}{dx} \sum_{L=0}^{\infty} x^{L+1} \\
& = \frac{1}{2} \frac{d^2}{dx^2} \frac{x^2}{1-x} - \frac{d}{dx} \frac{x}{1-x} \\
& = \frac{1}{2} \frac{d}{dx} \left[2x(1-x)^{-1} + x^2(1-x)^{-2}\right] - \left[(1-x)^{-1} + x(1-x)^{-2}\right] \\
& = \frac{1}{2} \frac{d}{dx} \frac{2x-x^2}{(1-x)^2} - \frac{1}{(1-x)^2} \\
& = \frac{1}{2} \frac{d}{dx} \left[\frac{x}{(1-x)^2} + \frac{x}{1-x} \right] - \frac{1}{(1-x)^2} \\
& = \frac{1}{2} \left[(1-x)^{-2} + 2x(1-x)^{-3} \right] \\
& = \frac{1+x}{2(1-x)^3}
\end{aligned}","['sequences-and-series', 'derivatives', 'binomial-coefficients']"
76,A formula for solving differential equations of the form $\frac{du}{dt}= Au$?,A formula for solving differential equations of the form ?,\frac{du}{dt}= Au,"Notation $\lambda_{1,1}, \lambda_{2,2}, \ldots, \lambda_{n,n}$ are all the eigenvalues of $A_n$ . $x_1, x_2, \ldots, x_n$ are all the eigen vectors of $A_n$ . $\Lambda_n = \text{diag}\left(\lambda_{1,1}, \lambda_{2,2}, \ldots, \lambda_{n,n}\right)$ . $S = \left[\begin{array}{cccc}  |   & |   &        & | \\  x_1 & x_2 & \cdots & x_n \\  |   & |   &        & | \end{array}\right]$ . Question q1: Is the solution to $\frac{du}{dt} = Au$ , given $u(0)$ , correctly described by... $$ u(t) = \sum_{k = 1}^{n} \left\{\mathrm{e}^{\lambda_{k,k}t}c_kx_k \right\} \style{font-family:inherit}{\text{ s.t. }} Sc = u(0)\tag{$\style{font-family:inherit}{\text{e1}}$}\label{eq1} $$ Example Given that... $A_5 = \text{ones}\left(5\right)$ . $u(0) = \left[\begin{array}{ccccc}  0 & 1 & 1 & 1 & 2 \end{array}\right]^T$ . We determine that... $\Lambda = \left[ \begin{array}{ccccc}  5  & 0  & 0  & 0  & 0 \\  0  & 0  & 0  & 0  & 0 \\  0  & 0  & 0  & 0  & 0 \\  0  & 0  & 0  & 0  & 0 \\  0  & 0  & 0  & 0  & 0 \end{array}\right]$ . $S = \left[ \begin{array}{ccccc}  1 & -1 & -1 & -1 & -1 \\  1  & 0  & 0  & 0  & 1 \\  1  & 0  & 0  & 1  & 0 \\  1  & 0  & 1  & 0  & 0 \\  1  & 1  & 0  & 0  & 0 \end{array}\right]$ . $Sc = u(0) \Longrightarrow c = \left[\begin{array}{ccccc}  1 & 1 & 0 & 0 & 0 \end{array}\right]^T$ . Using $\ref{eq1}$ ... $$ u(t) = \sum_{k = 1}^{5} \left\{\mathrm{e}^{\lambda_{k,k}t}c_kx_k \right\}\\ \Longrightarrow u(t) = \mathrm{e}^{(5)t}(1)x_1 + \mathrm{e}^{(0)t}(1)x_2 + \mathrm{e}^{(0)t}(0)x_3 + \mathrm{e}^{(0)t}(0)x_4 + \mathrm{e}^{(0)t}(0)x_5\\ \Longrightarrow u(t) = \mathrm{e}^{5t}\left[\begin{array}{c}  1 \\  1 \\  1 \\  1 \\  1 \end{array}\right] + \left[\begin{array}{c}  -1 \\  0 \\  0 \\  0 \\  1 \end{array}\right] $$ Additional Questions q2: If your answer to q1 was yes, is there a way to express $\ref{eq1}$ more compactly? q3: Please re-answer q1 but wrt the following (attempt @ answering q2)... $$ u(t) = \frac{S\text{e}^{\Lambda t}\text{adj}\left(S\right)u(0)}{|S|}\tag{$\style{font-family:inherit}{\text{e2}}$}\label{eq2} $$ I verified that e1 & e2 yield the same answer to my example problem (but they might not work for others?), using Mathematica... adj[m_] :=      Map[Reverse, Minors[Transpose[m], Length[m] - 1], {0, 1}] *        Table[(-1)^(i + j), {i, Length[m]}, {j, Length[m]}];  A = ConstantArray[1,{5,5}]; \[CapitalLambda] = DiagonalMatrix[Eigenvalues[A]]; S = Transpose[Eigenvectors[A]]; u0 = {0,1,1,1,2}; c = LinearSolve[S,u0];  (* Answer w/ method 1 *) a1 = Sum[Exp[Eigenvalues[A][[k]]]*c[[k]]*Eigenvectors[A][[k]],{k,1,Length[S]}];  (* Answer w/ method 2 *) a2 = (S.MatrixExp[\[CapitalLambda]*t].adj[S].u0)/Det[S];  Print[""A = "", A//MatrixForm]; Print[""\[CapitalLambda] = "", \[CapitalLambda]//MatrixForm]; Print[""S = "", S//MatrixForm]; Print[""u(0) = "", u0//MatrixForm]; Print[""c = "", c//MatrixForm]; Print[""a1: u(t) = "", a1//MatrixForm]; Print[""a2: u(t) = "", a2//MatrixForm];","Notation are all the eigenvalues of . are all the eigen vectors of . . . Question q1: Is the solution to , given , correctly described by... Example Given that... . . We determine that... . . . Using ... Additional Questions q2: If your answer to q1 was yes, is there a way to express more compactly? q3: Please re-answer q1 but wrt the following (attempt @ answering q2)... I verified that e1 & e2 yield the same answer to my example problem (but they might not work for others?), using Mathematica... adj[m_] :=      Map[Reverse, Minors[Transpose[m], Length[m] - 1], {0, 1}] *        Table[(-1)^(i + j), {i, Length[m]}, {j, Length[m]}];  A = ConstantArray[1,{5,5}]; \[CapitalLambda] = DiagonalMatrix[Eigenvalues[A]]; S = Transpose[Eigenvectors[A]]; u0 = {0,1,1,1,2}; c = LinearSolve[S,u0];  (* Answer w/ method 1 *) a1 = Sum[Exp[Eigenvalues[A][[k]]]*c[[k]]*Eigenvectors[A][[k]],{k,1,Length[S]}];  (* Answer w/ method 2 *) a2 = (S.MatrixExp[\[CapitalLambda]*t].adj[S].u0)/Det[S];  Print[""A = "", A//MatrixForm]; Print[""\[CapitalLambda] = "", \[CapitalLambda]//MatrixForm]; Print[""S = "", S//MatrixForm]; Print[""u(0) = "", u0//MatrixForm]; Print[""c = "", c//MatrixForm]; Print[""a1: u(t) = "", a1//MatrixForm]; Print[""a2: u(t) = "", a2//MatrixForm];","\lambda_{1,1}, \lambda_{2,2}, \ldots, \lambda_{n,n} A_n x_1, x_2, \ldots, x_n A_n \Lambda_n = \text{diag}\left(\lambda_{1,1}, \lambda_{2,2}, \ldots, \lambda_{n,n}\right) S = \left[\begin{array}{cccc}
 |   & |   &        & | \\
 x_1 & x_2 & \cdots & x_n \\
 |   & |   &        & |
\end{array}\right] \frac{du}{dt} = Au u(0) 
u(t) = \sum_{k = 1}^{n} \left\{\mathrm{e}^{\lambda_{k,k}t}c_kx_k \right\} \style{font-family:inherit}{\text{ s.t. }} Sc = u(0)\tag{\style{font-family:inherit}{\text{e1}}}\label{eq1}
 A_5 = \text{ones}\left(5\right) u(0) = \left[\begin{array}{ccccc}
 0 & 1 & 1 & 1 & 2
\end{array}\right]^T \Lambda = \left[ \begin{array}{ccccc}
 5  & 0  & 0  & 0  & 0 \\
 0  & 0  & 0  & 0  & 0 \\
 0  & 0  & 0  & 0  & 0 \\
 0  & 0  & 0  & 0  & 0 \\
 0  & 0  & 0  & 0  & 0
\end{array}\right] S = \left[ \begin{array}{ccccc}
 1 & -1 & -1 & -1 & -1 \\
 1  & 0  & 0  & 0  & 1 \\
 1  & 0  & 0  & 1  & 0 \\
 1  & 0  & 1  & 0  & 0 \\
 1  & 1  & 0  & 0  & 0
\end{array}\right] Sc = u(0) \Longrightarrow c = \left[\begin{array}{ccccc}
 1 & 1 & 0 & 0 & 0
\end{array}\right]^T \ref{eq1} 
u(t) = \sum_{k = 1}^{5} \left\{\mathrm{e}^{\lambda_{k,k}t}c_kx_k \right\}\\
\Longrightarrow u(t) = \mathrm{e}^{(5)t}(1)x_1 + \mathrm{e}^{(0)t}(1)x_2 + \mathrm{e}^{(0)t}(0)x_3 + \mathrm{e}^{(0)t}(0)x_4 + \mathrm{e}^{(0)t}(0)x_5\\
\Longrightarrow u(t) = \mathrm{e}^{5t}\left[\begin{array}{c}
 1 \\
 1 \\
 1 \\
 1 \\
 1
\end{array}\right] + \left[\begin{array}{c}
 -1 \\
 0 \\
 0 \\
 0 \\
 1
\end{array}\right]
 \ref{eq1} 
u(t) = \frac{S\text{e}^{\Lambda t}\text{adj}\left(S\right)u(0)}{|S|}\tag{\style{font-family:inherit}{\text{e2}}}\label{eq2}
","['linear-algebra', 'derivatives', 'eigenvalues-eigenvectors', 'notation']"
77,"Find all values of $c$ for the Mean Value Theorem's $f'(c)$ of the function $f(x) = 2x^3-6x^2-90x+6$ of the range $[-5, 8]$",Find all values of  for the Mean Value Theorem's  of the function  of the range,"c f'(c) f(x) = 2x^3-6x^2-90x+6 [-5, 8]","I need to find all values of $c$ for the $f'(c)$ of the function $f(x) = 2x^3-6x^2-90x+6$ of the range $[-5, 8]$ such that $f'(c) = -10$ . I have already found that $f'(c) = \frac{-130}{13}=-10$ . I thought I could find the values of $c$ by differentiating $f(x)$ which becomes $6x^2-12x-90$ , and applying the Mean Value Theorem $f'(c) = \frac{f(b)-f(a)}{b - a}$ again. $f(b) = 6(-5)^2-12(-5)-90 = 150+60-90 = 120$ $f(a) = 6(8)^2-12(8)-90 = 384-186 = 198$ $f'(c) = \frac{120-198}{-5-8} = \frac{78}{13} = 6$ Then I took $6x^2-12x-90$ again, replaced $x$ with $c$ , and set it equal to $6$ before solving for $c$ . $6c^2-12c-90 = 6  \to 6(c^2-2c-16) = 0$ $\frac{2 \pm \sqrt{2^2-4(1)(-17)}}{2(1)} = \frac{2 \pm \sqrt{74}}{2}$ or $c = \frac{2 - \sqrt{74}}{2}, \frac{2 + \sqrt{74}}{2}$ But this answer is wrong. How can I find the values for $c$ here?","I need to find all values of for the of the function of the range such that . I have already found that . I thought I could find the values of by differentiating which becomes , and applying the Mean Value Theorem again. Then I took again, replaced with , and set it equal to before solving for . or But this answer is wrong. How can I find the values for here?","c f'(c) f(x) = 2x^3-6x^2-90x+6 [-5, 8] f'(c) = -10 f'(c) = \frac{-130}{13}=-10 c f(x) 6x^2-12x-90 f'(c) = \frac{f(b)-f(a)}{b - a} f(b) = 6(-5)^2-12(-5)-90 = 150+60-90 = 120 f(a) = 6(8)^2-12(8)-90 = 384-186 = 198 f'(c) = \frac{120-198}{-5-8} = \frac{78}{13} = 6 6x^2-12x-90 x c 6 c 6c^2-12c-90 = 6  \to 6(c^2-2c-16) = 0 \frac{2 \pm \sqrt{2^2-4(1)(-17)}}{2(1)} = \frac{2 \pm \sqrt{74}}{2} c = \frac{2 - \sqrt{74}}{2}, \frac{2 + \sqrt{74}}{2} c","['calculus', 'derivatives']"
78,$\| X'(t) \| = (\| X(t) \|)'$,,\| X'(t) \| = (\| X(t) \|)',Let $X : \mathbb{R} \to \mathbb{R}^n$ be a $C^1$ function. Let $\| .\|$ be the norm : $\| v \| = \max_{1 \leq i \leq N} \mid v_i \mid$ . Then is it true that : $$\| X'(t) \|  = (\| X(t) \|)'$$ ? I am wondering if in general if I have any function $f : \mathbb{R}^n \to \mathbb{R}^p$ and a norm $N$ on a : $\mathbb{R}^p$ then is it always possible to invert the norm and the differential operator or the norm and in the integral? Thank you.,Let be a function. Let be the norm : . Then is it true that : ? I am wondering if in general if I have any function and a norm on a : then is it always possible to invert the norm and the differential operator or the norm and in the integral? Thank you.,X : \mathbb{R} \to \mathbb{R}^n C^1 \| .\| \| v \| = \max_{1 \leq i \leq N} \mid v_i \mid \| X'(t) \|  = (\| X(t) \|)' f : \mathbb{R}^n \to \mathbb{R}^p N \mathbb{R}^p,"['real-analysis', 'derivatives', 'normed-spaces']"
79,How to convert a rate involving radians to something that can be applied to a straight direction in a related rates problem.,How to convert a rate involving radians to something that can be applied to a straight direction in a related rates problem.,,"I can do related rates problems a little bit, but I've been given one that requires me to use a rate of $\frac{-\pi}{6}$ radians per second to figure out how fast a plane is going. Since I assume that plane is moving in a straight line, I'm not sure how to proceed. I think the answer might be to find out how fast the given rate is moving in terms of horizontal speed, and then apply implicit differentiation on that to find the answer, but I'm not sure. I could use the angular velocity formula, $\omega = \frac{\theta}{t}$ , but I'm worried that's not the right way since it gives an average. How can I convert a rate with radians to a rate involving purely horizontal/straight movement? Problem Text: A plane flies horizontally at an altitude of 5 km and passes directly over a tracking telescope on the ground. When the angle of elevation is π/3, this angle is decreasing at the rate of π/6 radians per minute. How fast is the plane traveling at that time? I have worked through it, and I've discovered that the plane, the telescope, and the distance of the plane from the ground can be formed into a right triangle. Since, $\frac{\pi}{3}$ radians is $60^\circ$ and a right triangle has a $90^\circ$ corner too, that means the the last corner must also be $60^\circ$ . Using the Law of Sines, I have found the hypotenuse is $\frac{10\sqrt{3}}{3}$ kilometers long and the other two edges are both $5$ kilometers. I haven't been able to get further than that.","I can do related rates problems a little bit, but I've been given one that requires me to use a rate of radians per second to figure out how fast a plane is going. Since I assume that plane is moving in a straight line, I'm not sure how to proceed. I think the answer might be to find out how fast the given rate is moving in terms of horizontal speed, and then apply implicit differentiation on that to find the answer, but I'm not sure. I could use the angular velocity formula, , but I'm worried that's not the right way since it gives an average. How can I convert a rate with radians to a rate involving purely horizontal/straight movement? Problem Text: A plane flies horizontally at an altitude of 5 km and passes directly over a tracking telescope on the ground. When the angle of elevation is π/3, this angle is decreasing at the rate of π/6 radians per minute. How fast is the plane traveling at that time? I have worked through it, and I've discovered that the plane, the telescope, and the distance of the plane from the ground can be formed into a right triangle. Since, radians is and a right triangle has a corner too, that means the the last corner must also be . Using the Law of Sines, I have found the hypotenuse is kilometers long and the other two edges are both kilometers. I haven't been able to get further than that.",\frac{-\pi}{6} \omega = \frac{\theta}{t} \frac{\pi}{3} 60^\circ 90^\circ 60^\circ \frac{10\sqrt{3}}{3} 5,"['calculus', 'derivatives', 'implicit-differentiation', 'word-problem', 'related-rates']"
80,Is it possible to find $n^{th}$ derivative of $\frac{1}{e^{ax}+b}$?,Is it possible to find  derivative of ?,n^{th} \frac{1}{e^{ax}+b},"Is it possible to find $n^{th}$ derivative of $\frac{1}{e^{ax}+b}$ ? I am introduced to $n^{th}$ differentiation and leibniz theorem as a first year undergraduate. But what i observe is there are some explicit formula for the $n^{th}$ derivative of $\sin x$ , $\cos x$ , $\log(ax+b)$ , $\frac{1}{ax+b}$ , etc but why there isn't any method given for $\sec x$ , $\csc x$ etc. If the highlighted question can be answered then it is possible to find the $n^{th}$ derivative of secx. How ? $$y=\sec x=\frac{1}{\cos x}=\frac{2 e^{ix}}{e^{2ix}+1}$$ Now one can use lebniz theorem of $n^{th}$ differentiation to find the n derivative of product of two functions whose general derivative is known. Now here $e^{ix}$ whose derivative can be found easily. The latter part $\frac{1}{e^{2ix+1}}$ is not known.","Is it possible to find derivative of ? I am introduced to differentiation and leibniz theorem as a first year undergraduate. But what i observe is there are some explicit formula for the derivative of , , , , etc but why there isn't any method given for , etc. If the highlighted question can be answered then it is possible to find the derivative of secx. How ? Now one can use lebniz theorem of differentiation to find the n derivative of product of two functions whose general derivative is known. Now here whose derivative can be found easily. The latter part is not known.",n^{th} \frac{1}{e^{ax}+b} n^{th} n^{th} \sin x \cos x \log(ax+b) \frac{1}{ax+b} \sec x \csc x n^{th} y=\sec x=\frac{1}{\cos x}=\frac{2 e^{ix}}{e^{2ix}+1} n^{th} e^{ix} \frac{1}{e^{2ix+1}},"['calculus', 'algebra-precalculus', 'derivatives']"
81,"General expression for $n$-th derivative of $x^{\alpha}$, $\alpha > 0$.","General expression for -th derivative of , .",n x^{\alpha} \alpha > 0,"Assume $\alpha > 0$ is fixed, but not necessarily integer. I'm looking for a general expression of $$A(n)=\frac{d^n}{d x^n} x^{\alpha} \tag{1}$$ If I'm not messing anything up, with $\alpha > n$ , $A(n)=(\alpha)_{n}~x^{\alpha - n} = \frac{\Gamma(\alpha +1)}{\Gamma(n + 1)}x^{\alpha-n}$ . On the other hand, $\alpha < n$ , it seems that at a certain point $t$ , such that $\alpha - t \geq 0 \geq \alpha - t - 1$ , a $(-1)^j$ term pops up for the remaining $n-t$ terms.  Here for me it's not clear whether we can still use a similar expression through $\Gamma$ functions, and how would it look like? My question: is there a general expression of (1) for any $\alpha$ and $n$ ? Sorry if this is a trivial question. I think I could work out the expressions for all 3 cases (the two mentioned and one with $\alpha$ being an integer), but it somehow feels like there should be a general expression, something involving $\Gamma(n - \alpha ) / \Gamma(n - 1)$ , but can't seem to reach it.","Assume is fixed, but not necessarily integer. I'm looking for a general expression of If I'm not messing anything up, with , . On the other hand, , it seems that at a certain point , such that , a term pops up for the remaining terms.  Here for me it's not clear whether we can still use a similar expression through functions, and how would it look like? My question: is there a general expression of (1) for any and ? Sorry if this is a trivial question. I think I could work out the expressions for all 3 cases (the two mentioned and one with being an integer), but it somehow feels like there should be a general expression, something involving , but can't seem to reach it.",\alpha > 0 A(n)=\frac{d^n}{d x^n} x^{\alpha} \tag{1} \alpha > n A(n)=(\alpha)_{n}~x^{\alpha - n} = \frac{\Gamma(\alpha +1)}{\Gamma(n + 1)}x^{\alpha-n} \alpha < n t \alpha - t \geq 0 \geq \alpha - t - 1 (-1)^j n-t \Gamma \alpha n \alpha \Gamma(n - \alpha ) / \Gamma(n - 1),"['real-analysis', 'calculus', 'derivatives']"
82,why the answers are difference when using FTC1 and integrate then take derivatives?,why the answers are difference when using FTC1 and integrate then take derivatives?,,"for example, the derivatives of $\int_1^x\cos4(x+t)\,\mathrm dt$ using FTC1, the answer is $2\cos(8x)$ but integrate it and then take derivatives, the answer is $2\cos(8x) - \cos(4(x+1))$ . Why? Thanks Seems I can rewrite the equation using trig identity.... $cos4(x+t) = cos(4x+4t) = cos(4x)cos(4t) - sin(4x)sin(4t) $ then I can apply FTC1 directly, $ \int_1^x cos4(x+t)dt$ = cos(4x) $\int_1^x cos(4t)dt - sin(4x)$ $\int_1^x sin(4t)dt$ $ = 2cos(8x) $ but $2cos(8x) $ looks different to 2cos(8x)−cos(4(x+1))","for example, the derivatives of using FTC1, the answer is but integrate it and then take derivatives, the answer is . Why? Thanks Seems I can rewrite the equation using trig identity.... then I can apply FTC1 directly, = cos(4x) but looks different to 2cos(8x)−cos(4(x+1))","\int_1^x\cos4(x+t)\,\mathrm dt 2\cos(8x) 2\cos(8x) - \cos(4(x+1)) cos4(x+t) = cos(4x+4t) = cos(4x)cos(4t) - sin(4x)sin(4t)   \int_1^x cos4(x+t)dt \int_1^x cos(4t)dt - sin(4x) \int_1^x sin(4t)dt  = 2cos(8x)  2cos(8x) ","['calculus', 'integration', 'derivatives']"
83,Derivative with respect to vectorized inverse Kronecker product,Derivative with respect to vectorized inverse Kronecker product,,"I am trying to derive the gradient of a function I wish to optimize, and wish to obtain the following derivative: $$ \frac{\partial}{\partial \pmb{x}} \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1} \pmb{y} $$ with $\pmb{x} = \mathrm{vec}(\pmb{X})$ , $\pmb{X}$ being a square asymetric matrix and $\pmb{y}$ a vector that is not a function of $\pmb{x}$ , and $\otimes$ the Kronecker product. My thought was to first write: $$ \left( \pmb{y}^{\top} \otimes \pmb{I} \right)   \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right)  $$ next to let $\pmb{f} = \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right)$ and then to express the differential of $\pmb{f}$ . I got to: $$ d\pmb{f} = \left(\left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) \left( \mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right)  + \mathrm{vec}\left( \pmb{X} \otimes (d\pmb{X})\right) \right)  $$ in which $-\top$ is short for the transpose of an inverse. This seems close to the answer, but not quite there yet. I guess I am getting lost in trying to express $\mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right)$ in terms of $d\pmb{x}$ . Edit: continuing this, I recognized there must be some permutation matrix $\pmb{P}$ such that: $$ \pmb{P}\mathrm{vec}( (d\pmb{x})\pmb{x}^{\top} ) = \mathrm{vec}((d\pmb{X})  \otimes \pmb{X}) $$ which I can use to further derive: $$ \begin{align} d\pmb{f} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)\pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right)d\pmb{b} \\ \frac{\partial \pmb{f}}{\partial \pmb{b}} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)  \pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right). \end{align} $$ Which seems plausible. Thus, all that seems to be needed is an expression for $\pmb{P}$ . I guess that will take a similar form as this answer , but I am not sure about it.","I am trying to derive the gradient of a function I wish to optimize, and wish to obtain the following derivative: with , being a square asymetric matrix and a vector that is not a function of , and the Kronecker product. My thought was to first write: next to let and then to express the differential of . I got to: in which is short for the transpose of an inverse. This seems close to the answer, but not quite there yet. I guess I am getting lost in trying to express in terms of . Edit: continuing this, I recognized there must be some permutation matrix such that: which I can use to further derive: Which seems plausible. Thus, all that seems to be needed is an expression for . I guess that will take a similar form as this answer , but I am not sure about it.","
\frac{\partial}{\partial \pmb{x}} \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1} \pmb{y}
 \pmb{x} = \mathrm{vec}(\pmb{X}) \pmb{X} \pmb{y} \pmb{x} \otimes 
\left( \pmb{y}^{\top} \otimes \pmb{I} \right)   \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) 
 \pmb{f} = \mathrm{vec}\left( \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) \pmb{f} 
d\pmb{f} = \left(\left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{X} \otimes \pmb{X} \right)^{-1}\right) \left( \mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right)  + \mathrm{vec}\left( \pmb{X} \otimes (d\pmb{X})\right) \right) 
 -\top \mathrm{vec}\left( (d\pmb{X}) \otimes \pmb{X} \right) d\pmb{x} \pmb{P} 
\pmb{P}\mathrm{vec}( (d\pmb{x})\pmb{x}^{\top} ) = \mathrm{vec}((d\pmb{X})  \otimes \pmb{X})
 
\begin{align}
d\pmb{f} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)\pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right)d\pmb{b} \\
\frac{\partial \pmb{f}}{\partial \pmb{b}} &= \left(\left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-\top} \otimes \left(\pmb{I} - \pmb{B} \otimes \pmb{B} \right)^{-1}\right)  \pmb{P}\left((\pmb{b} \otimes \pmb{I}) + (\pmb{I} \otimes \pmb{b})\right).
\end{align}
 \pmb{P}","['derivatives', 'matrix-calculus', 'differential', 'kronecker-product', 'vectorization']"
84,Proof of L'Hospital's Rule on Zorich book: an unclear step.,Proof of L'Hospital's Rule on Zorich book: an unclear step.,,"The proof of L'Hospital's rule on Zorich, Mathematical Analysis I, chapter 5.4, pag. 251, starts with: if $g'(x)\neq 0$ we conclude on the basis of Rolle's theorem that $g(x)$ is strictly monotonic on $(a,b)$ . In the hypothesis we have only that $g:(a,b)\to\mathbb{R}$ is a differentiable function on $(a,b)$ . So my question: we don't know if $g'(x)$ is continuous on $(a,b)$ or not. So, $g'(x)$ could jump from negative values to positive values without passing through $0$ , in theory. Consequently I think that, to conclude that $g(x)$ is strictly monotonic on $(a,b)$ when $g'(x) \neq 0 \;\;\forall x \in (a,b)$ , we have to say: let's assume that $\exists x_1,x_2\in (a,b)$ such that $g'(x_1)>0$ and $g'(x_2)<0$ ; then for Darboux property of derivative we would have that $g'(x)$ assumes all values between $g'(x_1)$ and $g'(x_2)$ , and so even $0$ , that is a contradiction for the hypothesis that $g'(x) \neq 0 \;\;\forall x \in (a,b)$ . Consequently $g'(x)$ has a definite sign that is the same $\forall x \in (a,b)$ and so $g(x)$ is strictly monotonic. Is it right?","The proof of L'Hospital's rule on Zorich, Mathematical Analysis I, chapter 5.4, pag. 251, starts with: if we conclude on the basis of Rolle's theorem that is strictly monotonic on . In the hypothesis we have only that is a differentiable function on . So my question: we don't know if is continuous on or not. So, could jump from negative values to positive values without passing through , in theory. Consequently I think that, to conclude that is strictly monotonic on when , we have to say: let's assume that such that and ; then for Darboux property of derivative we would have that assumes all values between and , and so even , that is a contradiction for the hypothesis that . Consequently has a definite sign that is the same and so is strictly monotonic. Is it right?","g'(x)\neq 0 g(x) (a,b) g:(a,b)\to\mathbb{R} (a,b) g'(x) (a,b) g'(x) 0 g(x) (a,b) g'(x) \neq 0 \;\;\forall x \in (a,b) \exists x_1,x_2\in (a,b) g'(x_1)>0 g'(x_2)<0 g'(x) g'(x_1) g'(x_2) 0 g'(x) \neq 0 \;\;\forall x \in (a,b) g'(x) \forall x \in (a,b) g(x)","['real-analysis', 'derivatives']"
85,Prove $\lim\limits_{x\to \infty}f'(x)= c \implies \lim\limits_{x\to \infty}\frac{f(x)}{x}\to c$,Prove,\lim\limits_{x\to \infty}f'(x)= c \implies \lim\limits_{x\to \infty}\frac{f(x)}{x}\to c,"Suppose $f\in C^1(\Bbb R), f:\Bbb R \to \Bbb R$ and $\lim\limits_{x\to \infty}f'(x)= c$ for some constant $c$ . How could we show $\frac{f(x)}{x}\to c$ as $x\to \infty$ also? L'Hopital's rule seems to give this, but obviously cannot be applied as we don't know whether $f(x)\to \infty$ as $x\to \infty$ . Also for my own curiosity, does the (presumably correct) forwards implication apply backwards? As in, does $\frac{f(x)}{x}\to c \Rightarrow f'(x)\to c$ as $x\to \infty$ also?","Suppose and for some constant . How could we show as also? L'Hopital's rule seems to give this, but obviously cannot be applied as we don't know whether as . Also for my own curiosity, does the (presumably correct) forwards implication apply backwards? As in, does as also?","f\in C^1(\Bbb R), f:\Bbb R \to \Bbb R \lim\limits_{x\to \infty}f'(x)= c c \frac{f(x)}{x}\to c x\to \infty f(x)\to \infty x\to \infty \frac{f(x)}{x}\to c \Rightarrow f'(x)\to c x\to \infty","['real-analysis', 'limits', 'derivatives']"
86,"If $x = a \cos t^3 , y = b \sin t^3$ then what is $d^3y/dx^3$?",If  then what is ?,"x = a \cos t^3 , y = b \sin t^3 d^3y/dx^3","If $ x = a \cos t^3 $ , $ y = b \sin t^3 $ , then what is $ \frac{d^3y}{dx^3} $ ?  I tried doing this problem by dividing $ \frac{d^3y}{dt^3} $ by $ \frac{d^3x}{dt^3} $ and got $ \frac{b}{a} $ .  However my book says the third derivative doesn't exist. Why is this so?","If , , then what is ?  I tried doing this problem by dividing by and got .  However my book says the third derivative doesn't exist. Why is this so?", x = a \cos t^3   y = b \sin t^3   \frac{d^3y}{dx^3}   \frac{d^3y}{dt^3}   \frac{d^3x}{dt^3}   \frac{b}{a} ,"['calculus', 'derivatives', 'trigonometry']"
87,Derivative of matrix w.r.t. its own vectorized version,Derivative of matrix w.r.t. its own vectorized version,,"I am unable to find what would be the derivative of a $m \times m$ real matrix $A$ with respect to $(\mathrm{vec}(A))^T$ (where $T$ is transpose and $\mathrm{vec}$ stacks the columns) without using tensors (i.e. remaining in 2d notation). I assume it would involve the Kronecker product, but is there a straightforward answer, or a convention?","I am unable to find what would be the derivative of a real matrix with respect to (where is transpose and stacks the columns) without using tensors (i.e. remaining in 2d notation). I assume it would involve the Kronecker product, but is there a straightforward answer, or a convention?",m \times m A (\mathrm{vec}(A))^T T \mathrm{vec},"['derivatives', 'matrix-calculus', 'vectorization']"
88,Verification: Investigation of a linear map on surjectivity and injectivity,Verification: Investigation of a linear map on surjectivity and injectivity,,"The linear map I'm investigating is defined like this: $l_2: P_3(\mathbb{R}) \rightarrow \mathbb{R}, l_2(p):= p'(1)$ And these are my calculations: Injectivity: $Let \,\, p,q \in  P_3(\mathbb{R}), \,\, p(x) := 2x^3 \,\, and \,\, q(x):= x^3+x^2+x$ $\Rightarrow l_2(p) = l_2(q) = 6 \,\, but \,\, p \neq q \Rightarrow l_2 \,\, is \,\, not \,\, injective.$ Surjectivity: $Let \,\, p \in P_3(\mathbb{R}), \,\, p(x):= ax^3+bx^2+cx+d \,\, and \,\, e \in \mathbb{R}$ Then $l_2(p) = 3a+2b+c$ $3a+2b+c = e \,\, so \,\, for \,\, example \,\, a=\frac{e}{3}; b=\frac{e}{2} \,\, and \,\, c=e \,\, would\,\, be \,\,one\,\, possible\,\, solution\,\, s.t\,\, l_2(p) = -e.$ $\Rightarrow l_2 \,\, is \,\, surjective.$",The linear map I'm investigating is defined like this: And these are my calculations: Injectivity: Surjectivity: Then,"l_2: P_3(\mathbb{R}) \rightarrow \mathbb{R}, l_2(p):= p'(1) Let \,\, p,q \in  P_3(\mathbb{R}), \,\, p(x) := 2x^3 \,\, and \,\, q(x):= x^3+x^2+x \Rightarrow l_2(p) = l_2(q) = 6 \,\, but \,\, p \neq q \Rightarrow l_2 \,\, is \,\, not \,\, injective. Let \,\, p \in P_3(\mathbb{R}), \,\, p(x):= ax^3+bx^2+cx+d \,\, and \,\, e \in \mathbb{R} l_2(p) = 3a+2b+c 3a+2b+c = e \,\, so \,\, for \,\, example \,\, a=\frac{e}{3}; b=\frac{e}{2} \,\, and \,\, c=e \,\, would\,\, be \,\,one\,\, possible\,\, solution\,\, s.t\,\, l_2(p) = -e. \Rightarrow l_2 \,\, is \,\, surjective.","['linear-algebra', 'derivatives', 'proof-verification', 'linear-transformations']"
89,"Does $|f^\prime|<1$ imply that $\forall_{x,y}|f(x)-f(y)|<|x-y|$?",Does  imply that ?,"|f^\prime|<1 \forall_{x,y}|f(x)-f(y)|<|x-y|","I have a task that I think reduces to proving that $f$ is a contraction mapping. We know that $\forall_x|f^\prime(x)|<1$ . Therefore if I could prove that $|f^\prime|<1\implies|f(x)-f(y)|<|x-y|$ then I think the task would be solved. I feel this property does hold and that it is also somewhat obvious. Unfortunately, saying that something is obvious is obviously not a valid proof. I have feeling proving this belongs to an elementary course on analysis, but it is somehow surprising how much have I forgotten from this course... How to prove this property and does it even hold?","I have a task that I think reduces to proving that is a contraction mapping. We know that . Therefore if I could prove that then I think the task would be solved. I feel this property does hold and that it is also somewhat obvious. Unfortunately, saying that something is obvious is obviously not a valid proof. I have feeling proving this belongs to an elementary course on analysis, but it is somehow surprising how much have I forgotten from this course... How to prove this property and does it even hold?",f \forall_x|f^\prime(x)|<1 |f^\prime|<1\implies|f(x)-f(y)|<|x-y|,"['real-analysis', 'derivatives', 'normed-spaces']"
90,Why needed Open disk in domain?,Why needed Open disk in domain?,,"Why is in above theorem, it is assumed that $D$ an open disk? Is it to make sure that we can surely apply mean value Theorem, ie, walking parallel to x or y axis, we are not moving out of domain? Can a convex open set do that job? Also, this may sound silly, but why open disk? Is it so that derivative is defined without fuss, that is, without considering boundary of that disk??","Why is in above theorem, it is assumed that an open disk? Is it to make sure that we can surely apply mean value Theorem, ie, walking parallel to x or y axis, we are not moving out of domain? Can a convex open set do that job? Also, this may sound silly, but why open disk? Is it so that derivative is defined without fuss, that is, without considering boundary of that disk??",D,"['complex-analysis', 'derivatives', 'holomorphic-functions']"
91,Need help understanding differential of function,Need help understanding differential of function,,"I have encountered the term differential/pushforward many times in the literature, although I cannot seem to understand just what is meant by it. I still cannot seem to understand the definition of the differential of a multivalued multivariable function $ f : \mathbb{R}^n \to \mathbb{R}^m $ and its generalization to differentiable manifolds. I have seen many definitions of the differential, particularly those for tangent vectors on manifolds and the definition with derivations of functions and one involving the Jacobian matrix, but I cannot understand this or any of them, thus I also cannot understand just what is meant by tangent space on manifolds. What does the differential ""really mean"" and how to use it? In particular, how is the differential/pushforward related to derivations of functions? Could someone please explain the differential to me and possibly using it to define tangent spaces on manifolds. I am frustrated as I have never understood the differential and lack the proper understanding in tangent spaces. All help is appreciated.","I have encountered the term differential/pushforward many times in the literature, although I cannot seem to understand just what is meant by it. I still cannot seem to understand the definition of the differential of a multivalued multivariable function and its generalization to differentiable manifolds. I have seen many definitions of the differential, particularly those for tangent vectors on manifolds and the definition with derivations of functions and one involving the Jacobian matrix, but I cannot understand this or any of them, thus I also cannot understand just what is meant by tangent space on manifolds. What does the differential ""really mean"" and how to use it? In particular, how is the differential/pushforward related to derivations of functions? Could someone please explain the differential to me and possibly using it to define tangent spaces on manifolds. I am frustrated as I have never understood the differential and lack the proper understanding in tangent spaces. All help is appreciated.", f : \mathbb{R}^n \to \mathbb{R}^m ,"['derivatives', 'differential-geometry', 'manifolds', 'differential', 'tangent-spaces']"
92,Taylor series of $\ln\frac{1+x}{1-x}$ [duplicate],Taylor series of  [duplicate],\ln\frac{1+x}{1-x},"This question already has an answer here : Taylor Expansion $\log(\frac{1+z}{1-z})$ (1 answer) Closed 5 years ago . Let $f(x)=\ln\frac{1+x}{1-x}$ for $x$ in $(-1,1)$ . Calculate the Taylor series of $f$ at $x_0=0$ I determined some derivatives: $f'(x)=\frac{2}{1-x^2}$ ; $f''(x)=\frac{4x}{(1-x^2)^2}$ ; $f^{(3)}(x)=\frac{4(3x^2+1)}{(1-x^2)^3}$ ; $f^{(4)}(x)=\frac{48x(x^2+1)}{(1-x^2)^4}$ ; $f^{(5)}(x)=\frac{48(5x^2+10x^2+1)}{(1-x^2)^5}$ and their values at $x_0=0$ : $f(0)=0$ ; $f'(0)=2$ ; $f''(0)=0$ ; $f^{(3)}(0)=4=2^2$ $f^{(4)}(0)=0$ ; $f^{(5)}(0)=48=2^4.3$ ; $f^{(7)}(0)=1440=2^5.3^2.5$ I can just see that for $n$ even, $f^{(n)}(0)=0$ , but how can I generalize the entire series?","This question already has an answer here : Taylor Expansion $\log(\frac{1+z}{1-z})$ (1 answer) Closed 5 years ago . Let for in . Calculate the Taylor series of at I determined some derivatives: ; ; ; ; and their values at : ; ; ; ; ; I can just see that for even, , but how can I generalize the entire series?","f(x)=\ln\frac{1+x}{1-x} x (-1,1) f x_0=0 f'(x)=\frac{2}{1-x^2} f''(x)=\frac{4x}{(1-x^2)^2} f^{(3)}(x)=\frac{4(3x^2+1)}{(1-x^2)^3} f^{(4)}(x)=\frac{48x(x^2+1)}{(1-x^2)^4} f^{(5)}(x)=\frac{48(5x^2+10x^2+1)}{(1-x^2)^5} x_0=0 f(0)=0 f'(0)=2 f''(0)=0 f^{(3)}(0)=4=2^2 f^{(4)}(0)=0 f^{(5)}(0)=48=2^4.3 f^{(7)}(0)=1440=2^5.3^2.5 n f^{(n)}(0)=0","['real-analysis', 'sequences-and-series', 'derivatives', 'taylor-expansion']"
93,Is there a way to write the convolution form of a derivative?,Is there a way to write the convolution form of a derivative?,,"In image filtering and computer simulations, derivatives can be applied to a image by performing a convolution of the image/data with an appropriate kernel matrix. For example, in 1D this could be $ k = [-1,0,1]/2 $ This form makes sense for discrete, evenly spaced data. How would you express this in a general way? I'm thinking of something of the form: $\frac{df}{dx} = f * k$ $ k = ?$ Does this make sense?","In image filtering and computer simulations, derivatives can be applied to a image by performing a convolution of the image/data with an appropriate kernel matrix. For example, in 1D this could be This form makes sense for discrete, evenly spaced data. How would you express this in a general way? I'm thinking of something of the form: Does this make sense?"," k = [-1,0,1]/2  \frac{df}{dx} = f * k  k = ?","['calculus', 'derivatives', 'convolution', 'image-processing']"
94,What is the relationship between the determinant and the derivative of a linear map?,What is the relationship between the determinant and the derivative of a linear map?,,"I know determinants tell you the oriented volume of the parallelepiped after the linear transformation, but if you define the derivative as I do below then it seems equivalent, at least in terms of the “slope” of the linear transformation. Or at least I hope someone can answer what exactly it is I’m defining below. It gives the derivative in single variable but doesn’t give best linear approx. in multivariate case. Is it a kind of directional derivative? The typical definition of a derivative is $$f'(x) \approx \frac{f(x+dx) - f(x)}{dx}$$ where $dx$ is an infinitesimal (appealing to nonstandard analysis here for simplicity). This is usually motivated as the slope of the tangent line to a point at $f(x)$ , but the more general formulation of the derivative is the ratio of oriented intervals: Let $I_{ab}$ be an oriented interval in the domain of a function $f$ , in most cases we will want this to be an infinitesimal interval so an interval of $[a, a+dx]$ . More generally an oriented interval is an oriented n-dimensional hypercube/hyper-rectangle/parallelotope, if $f: \mathbb{R} \rightarrow \mathbb{R}$ then $I_{ab}$ is the interval $[a,b]$ where $a,b \in \mathbb{R}$ , so it is a 1-dimensional oriented hypercube, such that $ab = -ba$ (suppresed $I$ notation, treating the interval algebraically like a simplicial complex). The magnitude of the coefficient to the interval is the size of the interval (i.e. the volume of the interval hypercube) and the sign of the coefficient is the orientation, so we start with a positive unit interval $I_{ab}$ and see how the function changes the interval size and orientation. Then the derivative of a function $f: \mathbb{R}\rightarrow \mathbb{R}$ is: $$ f'(x) \approx \frac{f(I_{ab})}{I_{ab}} $$ For example, if $f(x) = -2x$ then $f(I_{ab}) = 2I_{ba} = -2I_{ab}$ and hence $f'(x) = \frac{-2I_{ab}}{I_{ab}} = -2$ . So more generally for a function $f: \mathbb{R}^M \rightarrow \mathbb{R}^N, f' = \frac{f(I^N)}{I^M}$ where $I^N$ refers to an N-dimensional oriented interval. For a 2x2 matrix, representing a linear map from $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ , the interval is an oriented 2-dimensional hypercube (a square), with corners denoted $abcd$ , read clockwise for a positive orientation, such that $abcd = -adcb$ . Consider the linear map $F: \begin{bmatrix}-2&0\\0&2 \end{bmatrix}$ , which simply inverts one orthonormal basis and scales by 2. We know the determinant is -4.  Label an arbitrary 2-hypercube with corner points $abcd$ read in clockwise orientation. This linear map will map this interval such that $F(I_{abcd}) = 4I_{adcb} = -4I_{abcd}$ (suppresing $I$ for easier reading, $F(abcd) = 4adcb = -4abcd$ , since the map flips the originally clockwise orientation of the corners to counter-clockwise, hence negative sign. Now we calculate $F' = \frac{-4I_{abcd}}{I_{abcd}} = -4$ , the same as the determinant. In the general case, $f(I^N)$ may map an oriented unit hyper-cube to an arbitrary N-parallelotope, but it is still a kind of interval with orientation which gives the sign of the determinant. I already knew determinants tell you the volume of the parallelepiped after the mapping but in the way I've defined a derivative here, in what sense is the determinant any different than the slope of the derivative of the linear map represented by the matrix? The determinant seems analogous to the derivative to me. The derivative tells you how much an (infinitesimal) interval is scaled (magnitude of derivative) and if its orientation changes (sign of derivative), which is exactly what the determinant is telling you about a linear map.","I know determinants tell you the oriented volume of the parallelepiped after the linear transformation, but if you define the derivative as I do below then it seems equivalent, at least in terms of the “slope” of the linear transformation. Or at least I hope someone can answer what exactly it is I’m defining below. It gives the derivative in single variable but doesn’t give best linear approx. in multivariate case. Is it a kind of directional derivative? The typical definition of a derivative is where is an infinitesimal (appealing to nonstandard analysis here for simplicity). This is usually motivated as the slope of the tangent line to a point at , but the more general formulation of the derivative is the ratio of oriented intervals: Let be an oriented interval in the domain of a function , in most cases we will want this to be an infinitesimal interval so an interval of . More generally an oriented interval is an oriented n-dimensional hypercube/hyper-rectangle/parallelotope, if then is the interval where , so it is a 1-dimensional oriented hypercube, such that (suppresed notation, treating the interval algebraically like a simplicial complex). The magnitude of the coefficient to the interval is the size of the interval (i.e. the volume of the interval hypercube) and the sign of the coefficient is the orientation, so we start with a positive unit interval and see how the function changes the interval size and orientation. Then the derivative of a function is: For example, if then and hence . So more generally for a function where refers to an N-dimensional oriented interval. For a 2x2 matrix, representing a linear map from , the interval is an oriented 2-dimensional hypercube (a square), with corners denoted , read clockwise for a positive orientation, such that . Consider the linear map , which simply inverts one orthonormal basis and scales by 2. We know the determinant is -4.  Label an arbitrary 2-hypercube with corner points read in clockwise orientation. This linear map will map this interval such that (suppresing for easier reading, , since the map flips the originally clockwise orientation of the corners to counter-clockwise, hence negative sign. Now we calculate , the same as the determinant. In the general case, may map an oriented unit hyper-cube to an arbitrary N-parallelotope, but it is still a kind of interval with orientation which gives the sign of the determinant. I already knew determinants tell you the volume of the parallelepiped after the mapping but in the way I've defined a derivative here, in what sense is the determinant any different than the slope of the derivative of the linear map represented by the matrix? The determinant seems analogous to the derivative to me. The derivative tells you how much an (infinitesimal) interval is scaled (magnitude of derivative) and if its orientation changes (sign of derivative), which is exactly what the determinant is telling you about a linear map.","f'(x) \approx \frac{f(x+dx) - f(x)}{dx} dx f(x) I_{ab} f [a, a+dx] f: \mathbb{R} \rightarrow \mathbb{R} I_{ab} [a,b] a,b \in \mathbb{R} ab = -ba I I_{ab} f: \mathbb{R}\rightarrow \mathbb{R}  f'(x) \approx \frac{f(I_{ab})}{I_{ab}}  f(x) = -2x f(I_{ab}) = 2I_{ba} = -2I_{ab} f'(x) = \frac{-2I_{ab}}{I_{ab}} = -2 f: \mathbb{R}^M \rightarrow \mathbb{R}^N, f' = \frac{f(I^N)}{I^M} I^N g: \mathbb{R}^2 \rightarrow \mathbb{R}^2 abcd abcd = -adcb F: \begin{bmatrix}-2&0\\0&2 \end{bmatrix} abcd F(I_{abcd}) = 4I_{adcb} = -4I_{abcd} I F(abcd) = 4adcb = -4abcd F' = \frac{-4I_{abcd}}{I_{abcd}} = -4 f(I^N)","['calculus', 'linear-algebra', 'derivatives', 'linear-transformations', 'determinant']"
95,Finding derivative of $f(x)$ where $f(xy) = f(x) + f(y)$ - without change of variable,Finding derivative of  where  - without change of variable,f(x) f(xy) = f(x) + f(y),"Let $f(x)$ be a function $(0,\infty) \to R$ and for every $x,y$ in the domain we have: $$f(xy) = f(x) + f(y)$$ It is like logarithm but we don't know the exact form of the function. we know it is differentiable at x=1. Now we want to show it is differentiable at its domain and its derivative is $f'(x) = \frac{1}{x} f'(1)$ . Solution: I can find that $f(1) = 0$ and $f(x/y) = f(x) - f(y)$ So we have: $f'(1) = \lim_{h\to 0} \frac{f(1+h) - f(1)}{h} = \lim_{h\to 0}\frac{f(1+h)}{h}  $ so $f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h\to 0} \frac{f(\frac{x+h}{x})}{h}  = \lim_{h\to 0} \frac{f(1 + h/x)}{h}$ . I know I can solve it with a simple change of variables $h \to 0 ~~~\rightarrow~~~~ h/x \to 0$ , But I want to know is there any way to solve this without changing variable? Maybe using definition of limit?","Let be a function and for every in the domain we have: It is like logarithm but we don't know the exact form of the function. we know it is differentiable at x=1. Now we want to show it is differentiable at its domain and its derivative is . Solution: I can find that and So we have: so . I know I can solve it with a simple change of variables , But I want to know is there any way to solve this without changing variable? Maybe using definition of limit?","f(x) (0,\infty) \to R x,y f(xy) = f(x) + f(y) f'(x) = \frac{1}{x} f'(1) f(1) = 0 f(x/y) = f(x) - f(y) f'(1) = \lim_{h\to 0} \frac{f(1+h) - f(1)}{h} = \lim_{h\to 0}\frac{f(1+h)}{h}   f'(x) = \lim_{h\to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h\to 0} \frac{f(\frac{x+h}{x})}{h}  = \lim_{h\to 0} \frac{f(1 + h/x)}{h} h \to 0 ~~~\rightarrow~~~~ h/x \to 0","['limits', 'derivatives', 'logarithms', 'change-of-variable']"
96,If $F(x)=\frac{x^4-3}{x^4+1}$ is a primitive of $f(x)$ find $\int_{0}^{1} xf(x) dx$,If  is a primitive of  find,F(x)=\frac{x^4-3}{x^4+1} f(x) \int_{0}^{1} xf(x) dx,Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function. If $F(x)=\frac{x^4-3}{x^4+1}$ is a primitive of $f(x)$ find $\int_{0}^{1} xf(x) dx$ I literally have no idea how to integrate this. I tried integrating by parts (and finding the derivative of $F(x)$ ) but I end up getting a even worse integral... The correct answer apparently is $-3$ .,Let be a differentiable function. If is a primitive of find I literally have no idea how to integrate this. I tried integrating by parts (and finding the derivative of ) but I end up getting a even worse integral... The correct answer apparently is .,f:\mathbb{R}\to\mathbb{R} F(x)=\frac{x^4-3}{x^4+1} f(x) \int_{0}^{1} xf(x) dx F(x) -3,"['calculus', 'real-analysis', 'integration', 'derivatives', 'definite-integrals']"
97,First Derivative Test for inflection points,First Derivative Test for inflection points,,"Mathworld, ""First Derivative Test"" states: Suppose $f(x)$ is continuous at a stationary point $x_0$ . ... If $f'(x)$ has the same sign on an open interval extending left from $x_0$ and on an open interval extending right from $x_0$ , then $f(x)$ has an inflection point at $x_0$ . I am however having trouble proving the above claim and I suspect that without more conditions, it is false. My questions: If the above claim is true, how do I prove it? If false, how do I strengthen the assumptions so that it becomes true?","Mathworld, ""First Derivative Test"" states: Suppose is continuous at a stationary point . ... If has the same sign on an open interval extending left from and on an open interval extending right from , then has an inflection point at . I am however having trouble proving the above claim and I suspect that without more conditions, it is false. My questions: If the above claim is true, how do I prove it? If false, how do I strengthen the assumptions so that it becomes true?",f(x) x_0 f'(x) x_0 x_0 f(x) x_0,['real-analysis']
98,"Prove $f(x)=x^n(1-x)\lt \frac{1}{ne}$ for all $n\in \mathbb{N},x\in(0,1)$",Prove  for all,"f(x)=x^n(1-x)\lt \frac{1}{ne} n\in \mathbb{N},x\in(0,1)","Prove $f(x)=x^n(1-x)\lt \frac{1}{ne}$ for all $n\in \mathbb{N},x\in(0,1)$ . My try: $f'(x)=nx^{n-1}-(n+1)x^n=x^{n-1}(n-(n+1)x)=0\Rightarrow x=\frac{n}{n+1} $ , hence $\max_{x\in(0,1)} f(x)=f(\frac{n}{n+1})=(1-\frac{1}{n+1})^n\cdot\frac{1}{n+1}$ . I know $(1-\frac{1}{n+1})^n\to e^{-1}$ . But I don't know how to prove $(1-\frac{1}{n+1})^n\lt \frac{1}{e}$ .","Prove for all . My try: , hence . I know . But I don't know how to prove .","f(x)=x^n(1-x)\lt \frac{1}{ne} n\in \mathbb{N},x\in(0,1) f'(x)=nx^{n-1}-(n+1)x^n=x^{n-1}(n-(n+1)x)=0\Rightarrow x=\frac{n}{n+1}  \max_{x\in(0,1)} f(x)=f(\frac{n}{n+1})=(1-\frac{1}{n+1})^n\cdot\frac{1}{n+1} (1-\frac{1}{n+1})^n\to e^{-1} (1-\frac{1}{n+1})^n\lt \frac{1}{e}","['calculus', 'derivatives', 'maxima-minima']"
99,Why is there no derivative in an absolute value function?,Why is there no derivative in an absolute value function?,,"For example the function $f(x)=|x|$ , the graphic will be something like the letter ""V"". The function is continuous in $x=0$ . However, the slope is different for $x < 0$ than for $x>0$ , just like in quadratic functions, so I don’t see how that explains it. Can’t I draw a tangent line to the graph in $x=0$ which coincides with the $x$ axis? If i am able to do that, why isn’t the derivative (in $x=0$ ) equal to zero as in quadratic functions?","For example the function , the graphic will be something like the letter ""V"". The function is continuous in . However, the slope is different for than for , just like in quadratic functions, so I don’t see how that explains it. Can’t I draw a tangent line to the graph in which coincides with the axis? If i am able to do that, why isn’t the derivative (in ) equal to zero as in quadratic functions?",f(x)=|x| x=0 x < 0 x>0 x=0 x x=0,"['calculus', 'derivatives']"
