,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A question about prime elements in integral domains,A question about prime elements in integral domains,,"I have to show the following: Let $p \in R\setminus\{0\}$ then: $p$ is prime element in $R$ if and only if $(p)$ is a prime ideal in $R$. I have real problems doing so. I tried the following: $\Rightarrow$ let $p$ be a prime element in $R$, then we know that if $p \mid ab$ then $p\mid a$ or $p\mid b$. Also, we know that $$(p) = Rp = \{ rp \mid r \in R\},$$ then we know that $(p)$ is prime ideal because $rp \in (p) \Rightarrow p \in (p)$. Now how do i show that $(p)\neq R$. Also, the last step feels strange, as this seems to imply that $(a)$ is prime ideal for any $a\in R$ if $(a) \neq R$? with the $\Leftarrow$ direction I do not know how to start, could I get any hints? Thanks!","I have to show the following: Let $p \in R\setminus\{0\}$ then: $p$ is prime element in $R$ if and only if $(p)$ is a prime ideal in $R$. I have real problems doing so. I tried the following: $\Rightarrow$ let $p$ be a prime element in $R$, then we know that if $p \mid ab$ then $p\mid a$ or $p\mid b$. Also, we know that $$(p) = Rp = \{ rp \mid r \in R\},$$ then we know that $(p)$ is prime ideal because $rp \in (p) \Rightarrow p \in (p)$. Now how do i show that $(p)\neq R$. Also, the last step feels strange, as this seems to imply that $(a)$ is prime ideal for any $a\in R$ if $(a) \neq R$? with the $\Leftarrow$ direction I do not know how to start, could I get any hints? Thanks!",,['abstract-algebra']
1,Invariant under transformation $i\mapsto -i$ implies real?,Invariant under transformation  implies real?,i\mapsto -i,"When one has an expression in terms of $i$, one can send $i$ to $-i$ and, if the expression remains unchanged, one can conclude that the expression is, in fact, real. Analogous statements hold for expressions involving radicals. Why is this? One admittedly trivial example is the expression $$\frac{1}{x-i}+\frac{1}{x+i} .$$","When one has an expression in terms of $i$, one can send $i$ to $-i$ and, if the expression remains unchanged, one can conclude that the expression is, in fact, real. Analogous statements hold for expressions involving radicals. Why is this? One admittedly trivial example is the expression $$\frac{1}{x-i}+\frac{1}{x+i} .$$",,"['abstract-algebra', 'complex-numbers', 'galois-theory']"
2,Proving uniqueness in the structure theorem for finitely generated modules over a principal ideal domain,Proving uniqueness in the structure theorem for finitely generated modules over a principal ideal domain,,"In an introductory algebra course, one proves the Structure theorem for finitely generated modules over a principal ideal domain. http://en.wikipedia.org/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain In every book that I have looked in, the uniqueness part of the proof is a mess. Can anyone give me a reference for a nicer proof of the uniqueness part? I am especially interested in a proof of uniqueness when the underlying ring is the ring of polynomials in one variable over a field.","In an introductory algebra course, one proves the Structure theorem for finitely generated modules over a principal ideal domain. http://en.wikipedia.org/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain In every book that I have looked in, the uniqueness part of the proof is a mess. Can anyone give me a reference for a nicer proof of the uniqueness part? I am especially interested in a proof of uniqueness when the underlying ring is the ring of polynomials in one variable over a field.",,"['abstract-algebra', 'reference-request']"
3,Is there a perfect group in which not every element is a commutator?,Is there a perfect group in which not every element is a commutator?,,"Is there a perfect group in which not every element is a commutator? By a well-known fact,  it must have order at least $96.$ By Ore's conjecture (now a theorem), it must be infinite or non-simple.","Is there a perfect group in which not every element is a commutator? By a well-known fact,  it must have order at least By Ore's conjecture (now a theorem), it must be infinite or non-simple.",96.,"['abstract-algebra', 'group-theory', 'conjectures']"
4,"How do I find $S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots$ by complex numbers, where $p \in N$?","How do I find  by complex numbers, where ?",S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots p \in N,"In my post on Quora , I had expressed the three series in terms of sine and cosine: $$1+\frac{x^{3}}{3 !}+\frac{x^{6}}{6 !}+\frac{x^{9}}{9!}+\cdots \cdot=\frac{1}{3}\left[e^{x}+2 e^{-\frac{x}{2}} \cos \left(\frac{\sqrt{3}}{2} x\right)\right]\\ \frac{x^{2}}{2 !}+\frac{x^{5}}{5 !}+\frac{x^{8}}{8 !}+\cdots=\frac{1}{3}\left[e^{x}-e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)+\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right]\\ x+\frac{x^{4}}{4 !}+\frac{x^{7}}{7 !}+\cdots= \frac{1}{3}\left[e^{x}+e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)-\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right].$$ They all have the same “length” $3$ . I then think that the proof can be extended for a general series $S(p)$ with an arbitrary length $p$ and try to prove similarly that for any natunal number $p$ , $$ S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots=\frac{1}{p}\left(e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{n-1} x}\right) \tag*{(*)}  $$ where $\omega$ is the complex $p$ -th root of unity satisfying $$1+\omega+\omega^{2}+\cdots+\omega^{p-1}=0.$$ My question is how to prove (*) and express , in terms of sine and cosine, the series $$e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{p-1} x}$$","In my post on Quora , I had expressed the three series in terms of sine and cosine: They all have the same “length” . I then think that the proof can be extended for a general series with an arbitrary length and try to prove similarly that for any natunal number , where is the complex -th root of unity satisfying My question is how to prove (*) and express , in terms of sine and cosine, the series","1+\frac{x^{3}}{3 !}+\frac{x^{6}}{6 !}+\frac{x^{9}}{9!}+\cdots \cdot=\frac{1}{3}\left[e^{x}+2 e^{-\frac{x}{2}} \cos \left(\frac{\sqrt{3}}{2} x\right)\right]\\
\frac{x^{2}}{2 !}+\frac{x^{5}}{5 !}+\frac{x^{8}}{8 !}+\cdots=\frac{1}{3}\left[e^{x}-e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)+\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right]\\
x+\frac{x^{4}}{4 !}+\frac{x^{7}}{7 !}+\cdots=
\frac{1}{3}\left[e^{x}+e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)-\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right]. 3 S(p) p p 
S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots=\frac{1}{p}\left(e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{n-1} x}\right) \tag*{(*)} 
 \omega p 1+\omega+\omega^{2}+\cdots+\omega^{p-1}=0. e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{p-1} x}","['abstract-algebra', 'sequences-and-series', 'trigonometry', 'complex-numbers']"
5,Structures that are reciprocally homomorphic images but are not isomorphic,Structures that are reciprocally homomorphic images but are not isomorphic,,"Consider two algebras $\mathscr{A}=\langle A,O_i\rangle$ and $\mathscr{B}=\langle B,P_i\rangle$ with the same type. Suppose that there is a homomorphism from $A$ onto $B$ , and a homomorphism from $B$ onto $A$ . Clearly (edit: not really ), $|A|=|B|$ . It is easy to prove that if the algebras are finite, they must be isomorphic, but can we conclude the same thing for infinite algebras? I can find a counterexample for the same question about relational structures (sets equipped with relations) but my counterexample uses a one-many relation (the structures are $\big\langle\mathbb{N},\{\langle1,1\rangle,\langle1,2\rangle\}\big\rangle$ and $\big\langle\mathbb{N},\{\langle1,1\rangle\}\big\rangle$ ).","Consider two algebras and with the same type. Suppose that there is a homomorphism from onto , and a homomorphism from onto . Clearly (edit: not really ), . It is easy to prove that if the algebras are finite, they must be isomorphic, but can we conclude the same thing for infinite algebras? I can find a counterexample for the same question about relational structures (sets equipped with relations) but my counterexample uses a one-many relation (the structures are and ).","\mathscr{A}=\langle A,O_i\rangle \mathscr{B}=\langle B,P_i\rangle A B B A |A|=|B| \big\langle\mathbb{N},\{\langle1,1\rangle,\langle1,2\rangle\}\big\rangle \big\langle\mathbb{N},\{\langle1,1\rangle\}\big\rangle","['abstract-algebra', 'model-theory', 'universal-algebra']"
6,Places of an algebraic number field,Places of an algebraic number field,,"I have been doing my best to learn the rudiments of class field theory via the formulation in terms of ideals. The $\textbf{ray class group}$ of an algebraic number field $K$ with respect to $\mathfrak{m}$ is defined as: $$ \textrm{Cl}_{K}^{\mathfrak{m}}:=J^{\mathfrak{m}}/P^{\mathfrak{m}}.$$ I was initially under the erroneous impression that the ray class group is defined in terms of any ideal $\mathfrak{m} \subset \mathcal{O}_K$ , so that $J^{\mathfrak{m}}$ is the group of ideals in $\mathcal{O}_K$ coprime to $\mathfrak{m}$ and $P^{\mathfrak{m}}$ is the normal subgroup of prime ideals congruent to $1\ (\textrm{mod}\ \mathfrak{m})$ . $\textrm{Cl}_{K}^{\mathfrak{m}}$ would then be a generalisation of the ideal class group - coinciding with the latter when $\mathfrak{m}=1$ . After further elucidation, it turns out that the ray class group is actually defined in terms of something called a $\textbf{modulus}$ $\mathfrak{m}$ of $K$ , defined as: $$ \mathfrak{m} := \prod_{\mathfrak{p}}\mathfrak{p}^{\nu(\mathfrak{p})} \quad\textrm{where}\ \nu(\mathfrak{p}) \neq 0\ \textrm{for finitely many}\ \mathfrak{p}. \tag{$\dagger$} $$ When I first saw ( $\dagger$ ), I thought to myself: Fair enough; the product is taken over all prime ideals $\mathfrak{p} \subset \mathcal{O}_K$ , so $\mathfrak{m}$ is just some ideal in $\mathcal{O}_K$ , and ""modulus"" is just fancy nomenclature. But upon closer inspection, it turns out that the modulus is not defined as a product of prime ideals, but rather in terms of something called the $\textbf{places}$ of $K$ . Thus I set out to find out exactly what is meant by the ""places"" of a number field $K$ . And in my quest to understand the notion of places, I have managed to find places in the literature where places are mentioned, but no comprehensive treatment or even a definition. As for the online resources, I have found them wanting in intelligibility and consistency. Most resources I have consulted treat something called Archimedean places, which they relate to the real and complex embeddings of the number field in question. Hence my questions - which may be somewhat soft - are the following: 1) Why is the ray class group defined in terms of these moduli, as opposed to ideals. $\textit{E.g.}$ if we defined it in terms of an ideal $\mathfrak{m}$ , would it still be true that every ray class group had a corresponding ray class field, $\textit{i.e.}$ a normal extension $E$ of $K$ , so that $\textrm{Cl}_{K}^{\mathfrak{m}} \cong \textrm{Gal}(E/K)$ . That is to say: Would we still have the Takagi existence theorem? 2) Where can I find a really thorough treatment of the notion of a place of a number field? 3) As with most things in mathematics, I believe that it is most instructive to confront a few concrete cases before one gives an abstract definition. So can we provide concrete instances of ""places"" of $\mathbb{Q}$ , of $\mathbb{Q}(i)$ or of $\mathbb{Q}(\sqrt{2})$ ?","I have been doing my best to learn the rudiments of class field theory via the formulation in terms of ideals. The of an algebraic number field with respect to is defined as: I was initially under the erroneous impression that the ray class group is defined in terms of any ideal , so that is the group of ideals in coprime to and is the normal subgroup of prime ideals congruent to . would then be a generalisation of the ideal class group - coinciding with the latter when . After further elucidation, it turns out that the ray class group is actually defined in terms of something called a of , defined as: When I first saw ( ), I thought to myself: Fair enough; the product is taken over all prime ideals , so is just some ideal in , and ""modulus"" is just fancy nomenclature. But upon closer inspection, it turns out that the modulus is not defined as a product of prime ideals, but rather in terms of something called the of . Thus I set out to find out exactly what is meant by the ""places"" of a number field . And in my quest to understand the notion of places, I have managed to find places in the literature where places are mentioned, but no comprehensive treatment or even a definition. As for the online resources, I have found them wanting in intelligibility and consistency. Most resources I have consulted treat something called Archimedean places, which they relate to the real and complex embeddings of the number field in question. Hence my questions - which may be somewhat soft - are the following: 1) Why is the ray class group defined in terms of these moduli, as opposed to ideals. if we defined it in terms of an ideal , would it still be true that every ray class group had a corresponding ray class field, a normal extension of , so that . That is to say: Would we still have the Takagi existence theorem? 2) Where can I find a really thorough treatment of the notion of a place of a number field? 3) As with most things in mathematics, I believe that it is most instructive to confront a few concrete cases before one gives an abstract definition. So can we provide concrete instances of ""places"" of , of or of ?","\textbf{ray class group} K \mathfrak{m}  \textrm{Cl}_{K}^{\mathfrak{m}}:=J^{\mathfrak{m}}/P^{\mathfrak{m}}. \mathfrak{m} \subset \mathcal{O}_K J^{\mathfrak{m}} \mathcal{O}_K \mathfrak{m} P^{\mathfrak{m}} 1\ (\textrm{mod}\ \mathfrak{m}) \textrm{Cl}_{K}^{\mathfrak{m}} \mathfrak{m}=1 \textbf{modulus} \mathfrak{m} K 
\mathfrak{m} := \prod_{\mathfrak{p}}\mathfrak{p}^{\nu(\mathfrak{p})} \quad\textrm{where}\ \nu(\mathfrak{p}) \neq 0\ \textrm{for finitely many}\ \mathfrak{p}. \tag{\dagger}
 \dagger \mathfrak{p} \subset \mathcal{O}_K \mathfrak{m} \mathcal{O}_K \textbf{places} K K \textit{E.g.} \mathfrak{m} \textit{i.e.} E K \textrm{Cl}_{K}^{\mathfrak{m}} \cong \textrm{Gal}(E/K) \mathbb{Q} \mathbb{Q}(i) \mathbb{Q}(\sqrt{2})","['abstract-algebra', 'ring-theory', 'algebraic-number-theory', 'ideals', 'class-field-theory']"
7,Dual of a Hopf algebra,Dual of a Hopf algebra,,"Given is a Hopf algebra $(H,m,\eta, \Delta, \epsilon, S)$ . We know that there is a dual notion of it, called the dual Hopf algebra on $H^{*}$ as a vector space. It has the natural structure of a Hopf algebra. We know that the finite-dimensional algebra $(H,m, \eta)$ has the structure of a coalgebra, given by the maps: $m^{*}: H^{*} \rightarrow (H \otimes H)^{*}\cong H^{*} \otimes H^{*}, m^{*}(f)(a \otimes b)=f(ab),$ $u^{*}: H^{*} \rightarrow \mathbb{K}$ , $u^{*}(f)=f(1_{H})$ , for any $f \in H^{*}$ and $a,b \in H$ . On the other side the coalgebra $(H, \Delta, \epsilon)$ has the structure of an algebra, this is true even if $H$ is of infinite dimension. Its structural maps should be the following: $\Delta^{*}: H^{*} \otimes H^{*} \rightarrow H^{*}$ , $\Delta^{*}(f \otimes g)(\Delta(h))=f(h_{(1)})g(h_{(2)}),$ $\epsilon^{*}=\epsilon(h)$ , for any $f,g \in H^{*}$ and $h \in H$ . My question is how to define the dual notion of the antipode? How would the precise assignment look like? Thank you in advance for your help!","Given is a Hopf algebra . We know that there is a dual notion of it, called the dual Hopf algebra on as a vector space. It has the natural structure of a Hopf algebra. We know that the finite-dimensional algebra has the structure of a coalgebra, given by the maps: , , for any and . On the other side the coalgebra has the structure of an algebra, this is true even if is of infinite dimension. Its structural maps should be the following: , , for any and . My question is how to define the dual notion of the antipode? How would the precise assignment look like? Thank you in advance for your help!","(H,m,\eta, \Delta, \epsilon, S) H^{*} (H,m, \eta) m^{*}: H^{*} \rightarrow (H \otimes H)^{*}\cong H^{*} \otimes H^{*}, m^{*}(f)(a \otimes b)=f(ab), u^{*}: H^{*} \rightarrow \mathbb{K} u^{*}(f)=f(1_{H}) f \in H^{*} a,b \in H (H, \Delta, \epsilon) H \Delta^{*}: H^{*} \otimes H^{*} \rightarrow H^{*} \Delta^{*}(f \otimes g)(\Delta(h))=f(h_{(1)})g(h_{(2)}), \epsilon^{*}=\epsilon(h) f,g \in H^{*} h \in H","['abstract-algebra', 'representation-theory', 'hopf-algebras', 'quantum-groups']"
8,Does every finite non-trivial complete group have even order?,Does every finite non-trivial complete group have even order?,,"Does every finite non-trivial complete group have even order? I checked three well known classes of complete groups, and this statement is true for them all: 1) Symmetric groups: All symmetric groups have even order (a well known fact) 2) Automorphism groups of non-abelian simple groups: All non-abelian simple groups are of even order by Feit-Thompson theorem. Thus they have elements of order 2. And, as all non-abelian simple groups are centreless, this element is not in its centre. Thus, the conjugation by it is an automorphism of order 2. That means, that all automorphism groups of non-abelian simple groups have even order. 3) Holomorphs of cyclic groups of odd order (here is the proof, why they are complete: Is the statement that $ \operatorname{Aut}( \operatorname{Hol}(Z_n)) \cong \operatorname{Hol}(Z_n)$ true for every odd $n$? ): All cyclic groups are abelian and thus all cyclic groups of even order have automorphism of order 2, that maps all their elements to their inverse. Thus both their automorphism group and their holomorph are of even order. However, I do not know, how to prove this statement in general. Any help will be appreciated.","Does every finite non-trivial complete group have even order? I checked three well known classes of complete groups, and this statement is true for them all: 1) Symmetric groups: All symmetric groups have even order (a well known fact) 2) Automorphism groups of non-abelian simple groups: All non-abelian simple groups are of even order by Feit-Thompson theorem. Thus they have elements of order 2. And, as all non-abelian simple groups are centreless, this element is not in its centre. Thus, the conjugation by it is an automorphism of order 2. That means, that all automorphism groups of non-abelian simple groups have even order. 3) Holomorphs of cyclic groups of odd order (here is the proof, why they are complete: Is the statement that $ \operatorname{Aut}( \operatorname{Hol}(Z_n)) \cong \operatorname{Hol}(Z_n)$ true for every odd $n$? ): All cyclic groups are abelian and thus all cyclic groups of even order have automorphism of order 2, that maps all their elements to their inverse. Thus both their automorphism group and their holomorph are of even order. However, I do not know, how to prove this statement in general. Any help will be appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'automorphism-group', 'complete-groups']"
9,Show that the Euler characteristic of a chain complex is equal to the Euler characteristic of its homology,Show that the Euler characteristic of a chain complex is equal to the Euler characteristic of its homology,,"Let $C_*$ be a chain complex such that each $C_i$ is a torsion-free, finite-range abelian group with $C_i=0$ for all $i<0$. Suppose that $C_i=0$ for all $i$ is large enough. The Euler characteristic of $C_*$ chain complex is defined as $$\chi(C_*)=\sum_{i\geq 0}(-1)^iRank(C_i)$$ Prove that $$\chi(C_*)=\sum_{i\geq 0}(-1)^iRank(H_i(C_*))$$ I have to prove that $\sum_{i\geq 0}(-1)^iRank(C_i)=\sum_{i\geq 0}(-1)^iRank(H_i(C_*))$, I think that one way to do this is by showing that $Rank(C_i)=Rank(H_i(C_*))$ but I do not know if this is true in general, in a nutshell, I want to show that the cardinality of the base of any $C_i$ is the same as the cardinality of the basis of the corresponding homology, how can I do this? Thank you","Let $C_*$ be a chain complex such that each $C_i$ is a torsion-free, finite-range abelian group with $C_i=0$ for all $i<0$. Suppose that $C_i=0$ for all $i$ is large enough. The Euler characteristic of $C_*$ chain complex is defined as $$\chi(C_*)=\sum_{i\geq 0}(-1)^iRank(C_i)$$ Prove that $$\chi(C_*)=\sum_{i\geq 0}(-1)^iRank(H_i(C_*))$$ I have to prove that $\sum_{i\geq 0}(-1)^iRank(C_i)=\sum_{i\geq 0}(-1)^iRank(H_i(C_*))$, I think that one way to do this is by showing that $Rank(C_i)=Rank(H_i(C_*))$ but I do not know if this is true in general, in a nutshell, I want to show that the cardinality of the base of any $C_i$ is the same as the cardinality of the basis of the corresponding homology, how can I do this? Thank you",,"['abstract-algebra', 'general-topology', 'algebraic-topology', 'homological-algebra', 'homotopy-theory']"
10,An $n$ degree polynomial with more than $n$ roots?,An  degree polynomial with more than  roots?,n n,"I know that a polynomial of degree $n$ over a field has at most $n$ roots (even counted with multiplicity). My question is how this works with polynomials over integral domains. For example, $f(x) = x^2 + x + 1 \in \mathbb{Z}_{4}[x]$ has no roots. $f(x) = x^3 + x - 1\in \mathbb{Z}_9[x]$ does not have more than $3$ roots. From just playing with example like these it seems that even an $n$ degree polynomial over an integral domain has at most $n$ roots. Is that correct?","I know that a polynomial of degree $n$ over a field has at most $n$ roots (even counted with multiplicity). My question is how this works with polynomials over integral domains. For example, $f(x) = x^2 + x + 1 \in \mathbb{Z}_{4}[x]$ has no roots. $f(x) = x^3 + x - 1\in \mathbb{Z}_9[x]$ does not have more than $3$ roots. From just playing with example like these it seems that even an $n$ degree polynomial over an integral domain has at most $n$ roots. Is that correct?",,"['abstract-algebra', 'polynomials', 'roots', 'integral-domain']"
11,Ideal Generated by a Finite Number of Polynomials?,Ideal Generated by a Finite Number of Polynomials?,,"Background I have been confused about a particular definition in the textbook for my abstract algebra class, Ideals, Varieties, and Algorithms by Cox, Little, and O'Shea.  It is frustrating because I feel that I have a partial grasp on what the definition is trying to say, but when it comes down to it I simply find my confused.  So, instead of suffering by myself for any longer with this definition, I have decided to turn to you lovely folks to help me understand this seemingly simple concept. What I Understand First off, I understand (at least in the context of this book) what an ideal is.  The definition my book gives for an ideal is Definition. A subset $I\subseteq k[x_1,..., x_n]$ is an ideal if it satisfies: (i) $0\in I$ . (ii) If $f,g\in I$ , then $f+g\in I$ . (iii)  If $f\in I$ and $h\in k[x_1,...,x_n]$ , then $hf\in I$ . I find this to be a simple, easy to understand definition.  My problem, however, arises a few lines later when they define an ideal generated by a finite number of polynomials. What I Don't Understand And now, I give you the definition that has been causing me an incredible amount of confusion and frustration. Definition. Let $f_1,...,f_s$ be polynomials in $k[x_1,...,x_n]$ .  Then we set $$\langle f_1,...,f_s \rangle=\Big\lbrace \sum_{i=1}^s h_if_i \ | \ h_1,...,h_s\in k[x_1,...,x_n] \Big\rbrace.$$ I know what you are thinking. How does he not understand this ?  I wish I knew the answer to that question, but in the meantime, can someone please help me visualize what this set looks like?  I understand that $\langle f_1,...,f_s \rangle$ is an ideal, but I don't understand its structure, if that makes sense.  In other words, I can't visualize this definition in a way that makes sense to me.  The authors do make a slightly helpful note, saying that ""we can think of $\langle f_1,...,f_s \rangle$ as consisting of all 'polynomial consequences' of the equations $f_1=f_2=...=f_s=0$ ."" To elaborate a little more on my confusion, what I'm asking for is a less ""compact"" definition.  When I read this definition, for whatever reason the only thing I can come up with is $$f_1h_1+f_2h_2+...+f_sh_s.$$ But that doesn't make sense, because $\langle f_1,...,f_s \rangle$ is supposed to generate a set , not just a single polynomial. As always, thank you all for your time.  If you find this to be a stupid or silly question, then I'm sorry to have disappointed you -- I'm a slow learner, and I get hung up on stupid things sometimes. Oh, and Happy Halloween!","Background I have been confused about a particular definition in the textbook for my abstract algebra class, Ideals, Varieties, and Algorithms by Cox, Little, and O'Shea.  It is frustrating because I feel that I have a partial grasp on what the definition is trying to say, but when it comes down to it I simply find my confused.  So, instead of suffering by myself for any longer with this definition, I have decided to turn to you lovely folks to help me understand this seemingly simple concept. What I Understand First off, I understand (at least in the context of this book) what an ideal is.  The definition my book gives for an ideal is Definition. A subset is an ideal if it satisfies: (i) . (ii) If , then . (iii)  If and , then . I find this to be a simple, easy to understand definition.  My problem, however, arises a few lines later when they define an ideal generated by a finite number of polynomials. What I Don't Understand And now, I give you the definition that has been causing me an incredible amount of confusion and frustration. Definition. Let be polynomials in .  Then we set I know what you are thinking. How does he not understand this ?  I wish I knew the answer to that question, but in the meantime, can someone please help me visualize what this set looks like?  I understand that is an ideal, but I don't understand its structure, if that makes sense.  In other words, I can't visualize this definition in a way that makes sense to me.  The authors do make a slightly helpful note, saying that ""we can think of as consisting of all 'polynomial consequences' of the equations ."" To elaborate a little more on my confusion, what I'm asking for is a less ""compact"" definition.  When I read this definition, for whatever reason the only thing I can come up with is But that doesn't make sense, because is supposed to generate a set , not just a single polynomial. As always, thank you all for your time.  If you find this to be a stupid or silly question, then I'm sorry to have disappointed you -- I'm a slow learner, and I get hung up on stupid things sometimes. Oh, and Happy Halloween!","I\subseteq k[x_1,..., x_n] 0\in I f,g\in I f+g\in I f\in I h\in k[x_1,...,x_n] hf\in I f_1,...,f_s k[x_1,...,x_n] \langle f_1,...,f_s \rangle=\Big\lbrace \sum_{i=1}^s h_if_i \ | \ h_1,...,h_s\in k[x_1,...,x_n] \Big\rbrace. \langle f_1,...,f_s \rangle \langle f_1,...,f_s \rangle f_1=f_2=...=f_s=0 f_1h_1+f_2h_2+...+f_sh_s. \langle f_1,...,f_s \rangle","['abstract-algebra', 'definition', 'ideals']"
12,Is $1-\zeta_n$ a unit in every ring where $n$ is invertible?,Is  a unit in every ring where  is invertible?,1-\zeta_n n,"Let $R$ be a commutative ring where $n\ge 2$ is invertible and containing a primitive $n$th root of 1, called $\zeta_n$, satisfying $\zeta_n^n = 1$ and $\zeta_n^k\ne 1$ for any $1\le k\le n$. Is $1-\zeta_n$ invertible on $R$? Thanks to Hurkyl's excellent point, one can consider the counterexample $k[t]/(t^2-1)$, where $t$ is a primitive square root of 1, but $t-1$ is a zero divisor, hence not a unit. In this example, my impression is that you get two connected components of $\text{Spec }k[t]/(t^2-1)$, where on one of them $t = 1$, and on the other $t = -1$. Thus, we have the follow up question: If $R$ is a local ring, must $1-\zeta_n$ be invertible on $R$? Also I don't really understand why this was put on hold. The question was apparently clear enough to garner short and yet valuable answers.","Let $R$ be a commutative ring where $n\ge 2$ is invertible and containing a primitive $n$th root of 1, called $\zeta_n$, satisfying $\zeta_n^n = 1$ and $\zeta_n^k\ne 1$ for any $1\le k\le n$. Is $1-\zeta_n$ invertible on $R$? Thanks to Hurkyl's excellent point, one can consider the counterexample $k[t]/(t^2-1)$, where $t$ is a primitive square root of 1, but $t-1$ is a zero divisor, hence not a unit. In this example, my impression is that you get two connected components of $\text{Spec }k[t]/(t^2-1)$, where on one of them $t = 1$, and on the other $t = -1$. Thus, we have the follow up question: If $R$ is a local ring, must $1-\zeta_n$ be invertible on $R$? Also I don't really understand why this was put on hold. The question was apparently clear enough to garner short and yet valuable answers.",,[]
13,is equivalence classes the same thing as cosets?,is equivalence classes the same thing as cosets?,,"In abstract algebra (modern algebra), is the equivalence classes the same thing as cosets?  In the lecture notes that I have, it seems as though they are but is it a universal rule for the equivalence classes to mean the same thing cosets? or is the equivalence classes a subset of some set that equivalence relation holds ie (reflexivity, symmetric and transitivity)?","In abstract algebra (modern algebra), is the equivalence classes the same thing as cosets?  In the lecture notes that I have, it seems as though they are but is it a universal rule for the equivalence classes to mean the same thing cosets? or is the equivalence classes a subset of some set that equivalence relation holds ie (reflexivity, symmetric and transitivity)?",,['abstract-algebra']
14,"If every element of R has a (multiplicative) inverse, then $R = \{0\}$","If every element of R has a (multiplicative) inverse, then",R = \{0\},"I am struggling to understand why is this the case. I need to prove this, but I don't understand how it's true. For example, if every non-zero element of $R$ has a multiplicative inverse, then it's a field.  So how does $R=\{0\}$? Thanks you for your time :)","I am struggling to understand why is this the case. I need to prove this, but I don't understand how it's true. For example, if every non-zero element of $R$ has a multiplicative inverse, then it's a field.  So how does $R=\{0\}$? Thanks you for your time :)",,"['abstract-algebra', 'group-theory', 'ring-theory', 'field-theory']"
15,"If every polynomial of degree $2$ and odd degree has a root in $k[x]$, then $k$ is algebraically closed.","If every polynomial of degree  and odd degree has a root in , then  is algebraically closed.",2 k[x] k,"Let $k$ be a field of characteristic zero. Assume that every polynomial in $k[X]$ of odd degree and every polynomial in $k[X]$ of degree two has a root in $k$. Show that $k$ is algebraically closed. What we need to show is that these two assumptions implies that given $f\in k[X]$ with deg $f = 2^nm,$ $m$ an odd integer, $f$ has a root in $k[X]$. We probably want to use induction on $n$. The case $n=0$ is good by assumption. Now assume $n>0$, and that $f$ is irreducible. And I don't know what to do after this point.","Let $k$ be a field of characteristic zero. Assume that every polynomial in $k[X]$ of odd degree and every polynomial in $k[X]$ of degree two has a root in $k$. Show that $k$ is algebraically closed. What we need to show is that these two assumptions implies that given $f\in k[X]$ with deg $f = 2^nm,$ $m$ an odd integer, $f$ has a root in $k[X]$. We probably want to use induction on $n$. The case $n=0$ is good by assumption. Now assume $n>0$, and that $f$ is irreducible. And I don't know what to do after this point.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
16,When is $(-1+\sqrt{5})^x$ a rational number?,When is  a rational number?,(-1+\sqrt{5})^x,"When is $(-1+\sqrt{5})^x$ a rational number for some integer $x$? We know that $(-1+\sqrt{5})^x = A+B\sqrt{5}$ since $5$ is a prime for $x > 0$, but how do I determine when the $B$ term is $0$? Similarly when $x$ is negative.","When is $(-1+\sqrt{5})^x$ a rational number for some integer $x$? We know that $(-1+\sqrt{5})^x = A+B\sqrt{5}$ since $5$ is a prime for $x > 0$, but how do I determine when the $B$ term is $0$? Similarly when $x$ is negative.",,"['abstract-algebra', 'algebra-precalculus', 'number-theory']"
17,Associativity of $x*y := \frac{xy}{x+y+1}$,Associativity of,x*y := \frac{xy}{x+y+1},"In order to show something is associative one must show that $(x*y)*z$ = $x*(y*z)$. I want to show that $x * y = \frac{xy}{x+y+1}$ is associative. This is for self-study (I'm learning algebra over the summer) and need help finishing the proof. Below are my (hopefully not incorrect) steps, 1) $x*(y*z)$ = $x * \left(\frac{yz}{y+z+1}\right)$ Applying the binary operation * I obtain 2) $\frac{x\left(\frac{yz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ Here is where my first question comes: Question 1: Do I multiple x (in numerator) using the definition of *, or do I multiply x the ""normal"" way? If I multiply x the ""normal"" way, then why do I not use the * definition recursively for each application of *? Assuming I do multiplication the ""normal"" way I obtain, 3) $\frac{\left(\frac{xyz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ I know in order to add fractions they must have the same denominators. This means the denominator for each term needs to be y+z+1. So this brings me to 4) $\frac{\left(\frac{xyz}{y+z+1}\right)}{{y+z+1}(x+\frac{yz}{y+z+1}+ 1)}$ Here is where I become and more uncertain... Question 2: Again, do I use the * definition of multiplication or do I use the ""normal"" definition of multiplication for multiplying the y+z+1 across and why? Again, assuming I use ""normal"" multiplication I obtain 5)  $\frac{\left(\frac{xyz}{y+z+1}\right)}{\frac{xy+xz+x+yz+y+z+1)}{y+z+1}}$ From here I'm suck and not sure how to reduce this any further. Question 3: How do I proceed from step 5) to the conclusion? I believe once I understand this, I will be able to show $(x*y)*z$,I would like to see the rest of the proof from where I am stuck. Thank you very much in advance for the help. Disclaimer: A similar question has been asked & answered in this thread , but I have different questions than the ones they posed.","In order to show something is associative one must show that $(x*y)*z$ = $x*(y*z)$. I want to show that $x * y = \frac{xy}{x+y+1}$ is associative. This is for self-study (I'm learning algebra over the summer) and need help finishing the proof. Below are my (hopefully not incorrect) steps, 1) $x*(y*z)$ = $x * \left(\frac{yz}{y+z+1}\right)$ Applying the binary operation * I obtain 2) $\frac{x\left(\frac{yz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ Here is where my first question comes: Question 1: Do I multiple x (in numerator) using the definition of *, or do I multiply x the ""normal"" way? If I multiply x the ""normal"" way, then why do I not use the * definition recursively for each application of *? Assuming I do multiplication the ""normal"" way I obtain, 3) $\frac{\left(\frac{xyz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ I know in order to add fractions they must have the same denominators. This means the denominator for each term needs to be y+z+1. So this brings me to 4) $\frac{\left(\frac{xyz}{y+z+1}\right)}{{y+z+1}(x+\frac{yz}{y+z+1}+ 1)}$ Here is where I become and more uncertain... Question 2: Again, do I use the * definition of multiplication or do I use the ""normal"" definition of multiplication for multiplying the y+z+1 across and why? Again, assuming I use ""normal"" multiplication I obtain 5)  $\frac{\left(\frac{xyz}{y+z+1}\right)}{\frac{xy+xz+x+yz+y+z+1)}{y+z+1}}$ From here I'm suck and not sure how to reduce this any further. Question 3: How do I proceed from step 5) to the conclusion? I believe once I understand this, I will be able to show $(x*y)*z$,I would like to see the rest of the proof from where I am stuck. Thank you very much in advance for the help. Disclaimer: A similar question has been asked & answered in this thread , but I have different questions than the ones they posed.",,"['abstract-algebra', 'fractions', 'binary-operations']"
18,"Equality on pg. 40 of Humphreys's Lie Algebras, $\kappa(t_\lambda,t_\mu)=\sum_{\alpha\in\Phi}\alpha(t_\lambda)\alpha(t_\mu)$?","Equality on pg. 40 of Humphreys's Lie Algebras, ?","\kappa(t_\lambda,t_\mu)=\sum_{\alpha\in\Phi}\alpha(t_\lambda)\alpha(t_\mu)","I don't understand part of an equality on page 40 of Humphreys's book on Lie Algebras. Suppose $L$ is a semi-simple Lie algebra over an algebraically closed field of characteristic $0$, and $H$ a maximal toral subalgebra. Let $\kappa$ denote the Killing form, so for each $\phi\in H^\ast$, there is a unique $t_\phi\in H$ such that $\phi(h)=\kappa(t_\phi,h)$ for all $h\in H$. The equation says for any $\lambda,\mu\in H^\ast$,  $$ \kappa(t_\lambda,t_\mu)=\sum\alpha(t_\lambda)\alpha(t_\mu) $$ where the sum is over the set of roots $\alpha\in\Phi$. By definition of $t_\lambda$, $\kappa(t_\lambda,t_\mu)=\lambda(t_\mu)$, and since the roots span $H^\ast$, I can write $\lambda=\sum_{\alpha\in\Phi}c_\alpha\alpha$ for coefficients $c_\alpha$. Then $$ \lambda(t_\mu)=\sum_{\alpha\in\Phi}c_\alpha\alpha(t_\mu) $$ so this would work out if $c_\alpha=\alpha(t_\lambda)$. Is this true, or is there another way to see this?","I don't understand part of an equality on page 40 of Humphreys's book on Lie Algebras. Suppose $L$ is a semi-simple Lie algebra over an algebraically closed field of characteristic $0$, and $H$ a maximal toral subalgebra. Let $\kappa$ denote the Killing form, so for each $\phi\in H^\ast$, there is a unique $t_\phi\in H$ such that $\phi(h)=\kappa(t_\phi,h)$ for all $h\in H$. The equation says for any $\lambda,\mu\in H^\ast$,  $$ \kappa(t_\lambda,t_\mu)=\sum\alpha(t_\lambda)\alpha(t_\mu) $$ where the sum is over the set of roots $\alpha\in\Phi$. By definition of $t_\lambda$, $\kappa(t_\lambda,t_\mu)=\lambda(t_\mu)$, and since the roots span $H^\ast$, I can write $\lambda=\sum_{\alpha\in\Phi}c_\alpha\alpha$ for coefficients $c_\alpha$. Then $$ \lambda(t_\mu)=\sum_{\alpha\in\Phi}c_\alpha\alpha(t_\mu) $$ so this would work out if $c_\alpha=\alpha(t_\lambda)$. Is this true, or is there another way to see this?",,"['abstract-algebra', 'representation-theory', 'lie-algebras']"
19,is a number field by definition a subfield of $ \mathbb C $?,is a number field by definition a subfield of ?, \mathbb C ,"I have seen that some authors are defining the number field as a subfield of $ \mathbb C$ which is a finite extension of the rational numbers $ \mathbb Q $ , while some others without referring to complex numbers $ \mathbb C$ . I think we don't need $ K$ to be a subfield of $ \mathbb C$ in the definition. So, my question is the follwing: Is it neceserily to define $K$ as a subfield of $ \mathbb C$ or not ? And if no why ? Is it true that if we omit this in the definition, that then $K$ will turn out to be a subfield of $ \mathbb C$ ? I came up with this question, when I saw that in order to define infinite primes in a number field then these are determined by the embeddings $ K \to \mathbb C $ Any idea would be really appreciated. Thank you in advance.","I have seen that some authors are defining the number field as a subfield of which is a finite extension of the rational numbers , while some others without referring to complex numbers . I think we don't need to be a subfield of in the definition. So, my question is the follwing: Is it neceserily to define as a subfield of or not ? And if no why ? Is it true that if we omit this in the definition, that then will turn out to be a subfield of ? I came up with this question, when I saw that in order to define infinite primes in a number field then these are determined by the embeddings Any idea would be really appreciated. Thank you in advance.", \mathbb C  \mathbb Q   \mathbb C  K  \mathbb C K  \mathbb C K  \mathbb C  K \to \mathbb C ,"['abstract-algebra', 'commutative-algebra', 'field-theory', 'algebraic-number-theory']"
20,$\mathrm{Aut}(D_4)$ is isomorphic to $D_4$,is isomorphic to,\mathrm{Aut}(D_4) D_4,"Problem statement : I need to find out if $\mathrm{Aut}(D_4)$ is isomorphic to $D_4$ and explain my answer. I already know that it is isomorphic, so now all I need to do is to prove it. I assume that first we need to look where it sends $s$ and $t$ and it must send them to elements that satisfy the same relations() And then need to show that those two are conjugation by some element in $D_4$(?) Any help is appreciated. I was hoping for a duplicate post, but couldn't find it. Thank you!","Problem statement : I need to find out if $\mathrm{Aut}(D_4)$ is isomorphic to $D_4$ and explain my answer. I already know that it is isomorphic, so now all I need to do is to prove it. I assume that first we need to look where it sends $s$ and $t$ and it must send them to elements that satisfy the same relations() And then need to show that those two are conjugation by some element in $D_4$(?) Any help is appreciated. I was hoping for a duplicate post, but couldn't find it. Thank you!",,"['abstract-algebra', 'group-theory', 'group-isomorphism', 'dihedral-groups']"
21,Groups of order 24.,Groups of order 24.,,"I supposed $n_3=4$ and $n_2=3$ , and then I made $G$ act by conjugation on $\text{Syl}_3 (G)$ . I want to show that $G\cong S_4$ (looking at all order 24 groups here I saw the only one that has $n_3=|\text{Syl}_3(G)|=4$ and $n_2=|\text{Syl}_2(G)|=3$ is $S_4$ ) so I want to show that the kernel of this conjugation action is trivial. So this action defines a representation $\varphi:G\rightarrow S_4$ . I was thinking of using the order of $G/\ker(\varphi)$ but this didn't solve anything. So I thought about this kernel as the set that normalizes all three Sylow $3-$ subgroups, the intersection of the normalizers. If we say $\text{Syl}_3 (G)=\left\{P_1,P_2,P_3\right\}$ ,then I had, by the Orbit-stabilizer theorem, that the order of the stabilizers is $6$ .  I saw that in $S_4$ any two of these normalizers intersect in two elements, but when I intersect all of them the intersection is trivial. I want to prove that. So I was doing the following. By Lagrange, the intersection of all, has order $1, 2, 3$ or $6$ . It can't have order $3$ , it'd be a normal Sylow $3-$ subgroup and that's impossible. So it has order $1,2$ or $6$ , and I want to discard $2,6$ . How could I show that the normalizers are different (This would discard $6$ )? And how could I show that the intersection of all of them has more than two elements? I was thinking about saying that, if the intersection had order $2$ , then there would be a normal subgroup of order $2$ , and then a subgroup of order $6$ isomorphic to $\mathbb{Z}_2\rtimes_\phi \mathbb{Z}_3\cong \mathbb{Z}_6$ . Could I get somewhere this way? And how could I do the $2$ and $6$ parts?","I supposed and , and then I made act by conjugation on . I want to show that (looking at all order 24 groups here I saw the only one that has and is ) so I want to show that the kernel of this conjugation action is trivial. So this action defines a representation . I was thinking of using the order of but this didn't solve anything. So I thought about this kernel as the set that normalizes all three Sylow subgroups, the intersection of the normalizers. If we say ,then I had, by the Orbit-stabilizer theorem, that the order of the stabilizers is .  I saw that in any two of these normalizers intersect in two elements, but when I intersect all of them the intersection is trivial. I want to prove that. So I was doing the following. By Lagrange, the intersection of all, has order or . It can't have order , it'd be a normal Sylow subgroup and that's impossible. So it has order or , and I want to discard . How could I show that the normalizers are different (This would discard )? And how could I show that the intersection of all of them has more than two elements? I was thinking about saying that, if the intersection had order , then there would be a normal subgroup of order , and then a subgroup of order isomorphic to . Could I get somewhere this way? And how could I do the and parts?","n_3=4 n_2=3 G \text{Syl}_3 (G) G\cong S_4 n_3=|\text{Syl}_3(G)|=4 n_2=|\text{Syl}_2(G)|=3 S_4 \varphi:G\rightarrow S_4 G/\ker(\varphi) 3- \text{Syl}_3 (G)=\left\{P_1,P_2,P_3\right\} 6 S_4 1, 2, 3 6 3 3- 1,2 6 2,6 6 2 2 6 \mathbb{Z}_2\rtimes_\phi \mathbb{Z}_3\cong \mathbb{Z}_6 2 6","['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
22,"Problem on the number of generators of some ideals in $k[x,y,z]$ [closed]",Problem on the number of generators of some ideals in  [closed],"k[x,y,z]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have got stuck with two generator problems: The ideal $(zx,xy,yz)$ can't be generated by $2$ elements. The ideal $(xz-y^2,yz-x^3,z^2-xy)$ can't be generated by $2$ elements. Here the ring is $k[x,y,z]$. Thanks in advance for any help.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I have got stuck with two generator problems: The ideal $(zx,xy,yz)$ can't be generated by $2$ elements. The ideal $(xz-y^2,yz-x^3,z^2-xy)$ can't be generated by $2$ elements. Here the ring is $k[x,y,z]$. Thanks in advance for any help.",,"['abstract-algebra', 'commutative-algebra', 'ideals']"
23,Simple $M_n(D)$-module with $D$ a division ring,Simple -module with  a division ring,M_n(D) D,Define $D$ to be a division algebra over a field $k$ and $R=M_n(D)$ the $n\times n$ matrix ring over $D$. A simple $R$-module $M$ is the quotient of $R$. I can write $R=\bigoplus_j I_j$ where $I_j$ is the subring of $R$ all of whose columns except the $j$-th are zero. Is it true that the induced map $R\to M$ induces an isomorphism between $M$ and some $I_j$? Many thanks!,Define $D$ to be a division algebra over a field $k$ and $R=M_n(D)$ the $n\times n$ matrix ring over $D$. A simple $R$-module $M$ is the quotient of $R$. I can write $R=\bigoplus_j I_j$ where $I_j$ is the subring of $R$ all of whose columns except the $j$-th are zero. Is it true that the induced map $R\to M$ induces an isomorphism between $M$ and some $I_j$? Many thanks!,,"['abstract-algebra', 'ring-theory', 'modules', 'division-algebras']"
24,What is the interpretation of $a \equiv b$ mod $H$ in group theory?,What is the interpretation of  mod  in group theory?,a \equiv b H,"I.N. Herstein has defined: Let $G$ be a group, $H$ a subgroup of $G$; for $a,b \in G$ we say $a$ is congruent to $b \mod H$, written as $a \equiv b  \mod H$ if $ab^{-1} \in H$. Let $G$ be a group, $H$ a subgroup of $G$; for $a,b \in G$ we say $a$ is congruent to $b \mod H$, written as $a \equiv b  \mod H$ if $ab^{-1} \in H$. When dealing with integers $a \equiv b  \mod n$ means $n \mid (a-b)$, what is the meaning of $a \equiv b  \mod H$ in context of group theory? And why and how has $ab^{-1} \in H$ been used as a condition to define this relation?","I.N. Herstein has defined: Let $G$ be a group, $H$ a subgroup of $G$; for $a,b \in G$ we say $a$ is congruent to $b \mod H$, written as $a \equiv b  \mod H$ if $ab^{-1} \in H$. Let $G$ be a group, $H$ a subgroup of $G$; for $a,b \in G$ we say $a$ is congruent to $b \mod H$, written as $a \equiv b  \mod H$ if $ab^{-1} \in H$. When dealing with integers $a \equiv b  \mod n$ means $n \mid (a-b)$, what is the meaning of $a \equiv b  \mod H$ in context of group theory? And why and how has $ab^{-1} \in H$ been used as a condition to define this relation?",,"['abstract-algebra', 'group-theory']"
25,Characteristic subgroups $\phi(H) \subseteq H$,Characteristic subgroups,\phi(H) \subseteq H,"My book uses this definition: Let $G$ be a group. A subgroup $H$ of $G$ is called a characteristic subgroup if $\phi(H) \subseteq H$ for all $\phi \in \operatorname{Aut}(G)$. But after some googling around, it seems that the definition for a characteristic subgroup involves equality $\phi(H) = H$. Does  $\phi(H) \subseteq H$ $\Rightarrow$ $\phi(H) = H$ ??? I tried to multiply by $\phi ^{-1}$ to get $H \subseteq \phi^{-1}(H)$ but I'm not sure if I am allowed to that and I'm even more unsure if $\phi(H) \subseteq H$ and $H\subseteq \phi^{-1}(H)$ $\Rightarrow$ $\phi(H) = H$. Any mathematical wisdom? Thank you.","My book uses this definition: Let $G$ be a group. A subgroup $H$ of $G$ is called a characteristic subgroup if $\phi(H) \subseteq H$ for all $\phi \in \operatorname{Aut}(G)$. But after some googling around, it seems that the definition for a characteristic subgroup involves equality $\phi(H) = H$. Does  $\phi(H) \subseteq H$ $\Rightarrow$ $\phi(H) = H$ ??? I tried to multiply by $\phi ^{-1}$ to get $H \subseteq \phi^{-1}(H)$ but I'm not sure if I am allowed to that and I'm even more unsure if $\phi(H) \subseteq H$ and $H\subseteq \phi^{-1}(H)$ $\Rightarrow$ $\phi(H) = H$. Any mathematical wisdom? Thank you.",,"['abstract-algebra', 'group-theory']"
26,Why is the reciprocal of an $n$-th root of unity its complex conjugate?,Why is the reciprocal of an -th root of unity its complex conjugate?,n,"As stated in the Wikipedia article on roots of unity, the reciprocal of an $n$-th root of unity is its complex conjugate. They provide the following proof of this statement: Let $z\in\mathbb{C}$ be a $n$-th root of unity $\Rightarrow$ $$\frac{1}{z}=z^{-1}=z^nz^{-1}=z^{n-1}=\overline{z}$$ I'm unable to figure out why the last equal sign holds.","As stated in the Wikipedia article on roots of unity, the reciprocal of an $n$-th root of unity is its complex conjugate. They provide the following proof of this statement: Let $z\in\mathbb{C}$ be a $n$-th root of unity $\Rightarrow$ $$\frac{1}{z}=z^{-1}=z^nz^{-1}=z^{n-1}=\overline{z}$$ I'm unable to figure out why the last equal sign holds.",,"['abstract-algebra', 'complex-numbers', 'roots-of-unity']"
27,Degree of closure of $\mathbb{Q}_p$,Degree of closure of,\mathbb{Q}_p,"In order to prove that algebraic closure of $\mathbb{Q}_p$ is infinite, I took the polynomial $x^n-p$ with $n>1$ over $\mathbb{Q}_p$ to show that this eqaution has no solution for infinite cases to get the result but I want to make one part of the proof precise. It's clear that $x^2-p$ has no solution in the field so we adjoin $\sqrt{p}$ to get $\mathbb{Q}_p(\sqrt{p})$. For higher powers of $n$, clearly $p^{1/n}$ is not contained in $\mathbb{Q}_p$ but how do we show that $p^{1/n}$ is not contained in the extensions with smaller $n$? For instance how do we show that $\sqrt[3]{p}\notin \mathbb{Q}_p(\sqrt{p})$? Addendum: Reading Prof. Conrad's comment is helpful for the discussion.","In order to prove that algebraic closure of $\mathbb{Q}_p$ is infinite, I took the polynomial $x^n-p$ with $n>1$ over $\mathbb{Q}_p$ to show that this eqaution has no solution for infinite cases to get the result but I want to make one part of the proof precise. It's clear that $x^2-p$ has no solution in the field so we adjoin $\sqrt{p}$ to get $\mathbb{Q}_p(\sqrt{p})$. For higher powers of $n$, clearly $p^{1/n}$ is not contained in $\mathbb{Q}_p$ but how do we show that $p^{1/n}$ is not contained in the extensions with smaller $n$? For instance how do we show that $\sqrt[3]{p}\notin \mathbb{Q}_p(\sqrt{p})$? Addendum: Reading Prof. Conrad's comment is helpful for the discussion.",,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
28,Explicitly computing the isomorphism class of the tensor product of two finite abelian groups,Explicitly computing the isomorphism class of the tensor product of two finite abelian groups,,"How do I compute the isomorphism class of $A\otimes_\mathbb{Z} B$, where $A$ and $B$ are abelian of finite order? I can do this for a few examples, but I am unsure of how to proceed in the general case. Specifically, I am interested in the cases where... $|A|$ and $|B|$ are coprime $\pi(|A|)\cap\pi(|B|)=1$  (where $\pi(n)$ denotes the set of prime divisors of $n$) all Sylow subgroups of $A$ and $B$ are elementary abelian Is there a way of seeing such results intuitively?","How do I compute the isomorphism class of $A\otimes_\mathbb{Z} B$, where $A$ and $B$ are abelian of finite order? I can do this for a few examples, but I am unsure of how to proceed in the general case. Specifically, I am interested in the cases where... $|A|$ and $|B|$ are coprime $\pi(|A|)\cap\pi(|B|)=1$  (where $\pi(n)$ denotes the set of prime divisors of $n$) all Sylow subgroups of $A$ and $B$ are elementary abelian Is there a way of seeing such results intuitively?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups', 'tensor-products']"
29,Sylow $p$ subgroups of $S_{2p}$ and $S_{p^2}$ - Dummit foote - $4.5.45; 4.5.46$,Sylow  subgroups of  and  - Dummit foote -,p S_{2p} S_{p^2} 4.5.45; 4.5.46,"Question is to find Sylow $p$ subgroups of $S_{2p}$ for odd prime $p$ and show that this is an abelian group of order $p^2$ Sylow $p$ subgroups of  $S_{p^2}$ for odd prime $p$ and show that this is a non abelian group of order $p^{p+1}$ what I have done so far is : $|S_{2p}|=(2p)!=(2p)(2p-1)\cdots (2p-p)\cdots 4\cdot3\cdot2\cdot1$ with out giving proper justification I would say that $p$ occurs only twice in factorization of $(2p)!$.. $p$ occurs first time in $2p$ and second time in $(2p-p)=p$.. (I do not know how to make above justification look better) So, sylow subgroup of $S_{2p}$ is of order $p^2$.. I was first thought of seeing for some example May be something like $S_6=S_{2.3}$ Order of sylow $3$ subgroup of $S_6$ is $9$ only possibilities for orders of elements are $3$.. If i take some two arbitrary elements of order $3$ there is a possibility that they do not end up nicely i mean some thing like $$(123)(124)=(13)(42)$$ would happen.. I would take two elements of order $3$ and then end up with an element of order $2$ which would be a problem as a group of order $9$ can not have an element of order $2$ So, I would make sure that those elements do not create any mess and commutes nicely and the choice that comes to my mind is $\{(123),(456)\}$ and the group generated by them is of order $9$ $$\{\langle(123),(456)\rangle\}=\{(123)^i(456)^j : 0\leq i,j\leq 2\}$$ I would like to repeat the same in terms of $S_{2p}$ I would consider first half of $\{1,2,3,\cdots,2p-2,2p-1,2p\}$ as one cycle and second half as another cycle. I mean $\{(123\cdots,p-1,p);(p+1,p+2,\cdots,2p)\}$ These two cycles commutes nicely and do not create any mess giving rise to a subgroup of order $p^2$ namely : $$\{\langle(123\cdots,p-1,p);(p+1,p+2,\cdots,2p)\rangle\}$$ $$=\{(123\cdots,p-1,p)^i(p+1,p+2,\cdots,2p)^j : 0\leq i,j\leq p-1\}$$ So, I guess this is good enough.. $|S_{p^2}|=(p^2)!=(p^2)(p^2-1)\cdots (p^2-p)\cdots (p^2-2p)\cdots (p^2-(p-1)p) \cdots 4\cdot3\cdot2\cdot 1$ i.e., $|S_{p^2}|=p^2\cdot p(p-1)\cdot p(p-2)\cdots p(1)m$ where $p\nmid m$ i.e., we have $p^2\cdot p^{p-1}=p{p+1}$ number of $p$ in factorization of $(p^2)!$ So, now the order is known and we need to find the subgroup... I would repeat the same idea as of $S_{2p}$ For the same reason as that of $S_{2p}$ I would take for $S_{9}=S_{3^2}$the set $\{(123),(456),(789)\}$. But then $\{\langle (123),(456),(789)\rangle\}=\{(123)^i(456)^j(789)^k : 0\leq i,j,k\leq 2\}$ is giving me a subgroup of order $27$ where as i need a group of order $81$ (which is the order of sylow $3$ subgroup of $S_{9}$ i.e.$3^{3+1}$) so, It sounds some thing like I should take an element of order $3$ which behaves nicely and does not create much problem and give a subgroup of order $81$ I am sure that any $3$ sylow subgroup of $S_9$ would have a copy isomorphic to  $\{\langle (123),(456),(789)\rangle\}$ so only problem is i am not sure how to extend this to a subgroup of order $81$ More generally for $S_{p^2}$ I would consider $$\{\langle(123\cdots p)(p+1\cdots 2p)\cdots (p^2-p+1\cdots p^2)\rangle\}$$ This would give a subgroup of order $p^p$ but then i need a subgroup of order $p^{p+1}$. I would be thankful if some one can help me to see what would be the obvious element that one should multiply this with :O Please help me to clear this. Thank you.","Question is to find Sylow $p$ subgroups of $S_{2p}$ for odd prime $p$ and show that this is an abelian group of order $p^2$ Sylow $p$ subgroups of  $S_{p^2}$ for odd prime $p$ and show that this is a non abelian group of order $p^{p+1}$ what I have done so far is : $|S_{2p}|=(2p)!=(2p)(2p-1)\cdots (2p-p)\cdots 4\cdot3\cdot2\cdot1$ with out giving proper justification I would say that $p$ occurs only twice in factorization of $(2p)!$.. $p$ occurs first time in $2p$ and second time in $(2p-p)=p$.. (I do not know how to make above justification look better) So, sylow subgroup of $S_{2p}$ is of order $p^2$.. I was first thought of seeing for some example May be something like $S_6=S_{2.3}$ Order of sylow $3$ subgroup of $S_6$ is $9$ only possibilities for orders of elements are $3$.. If i take some two arbitrary elements of order $3$ there is a possibility that they do not end up nicely i mean some thing like $$(123)(124)=(13)(42)$$ would happen.. I would take two elements of order $3$ and then end up with an element of order $2$ which would be a problem as a group of order $9$ can not have an element of order $2$ So, I would make sure that those elements do not create any mess and commutes nicely and the choice that comes to my mind is $\{(123),(456)\}$ and the group generated by them is of order $9$ $$\{\langle(123),(456)\rangle\}=\{(123)^i(456)^j : 0\leq i,j\leq 2\}$$ I would like to repeat the same in terms of $S_{2p}$ I would consider first half of $\{1,2,3,\cdots,2p-2,2p-1,2p\}$ as one cycle and second half as another cycle. I mean $\{(123\cdots,p-1,p);(p+1,p+2,\cdots,2p)\}$ These two cycles commutes nicely and do not create any mess giving rise to a subgroup of order $p^2$ namely : $$\{\langle(123\cdots,p-1,p);(p+1,p+2,\cdots,2p)\rangle\}$$ $$=\{(123\cdots,p-1,p)^i(p+1,p+2,\cdots,2p)^j : 0\leq i,j\leq p-1\}$$ So, I guess this is good enough.. $|S_{p^2}|=(p^2)!=(p^2)(p^2-1)\cdots (p^2-p)\cdots (p^2-2p)\cdots (p^2-(p-1)p) \cdots 4\cdot3\cdot2\cdot 1$ i.e., $|S_{p^2}|=p^2\cdot p(p-1)\cdot p(p-2)\cdots p(1)m$ where $p\nmid m$ i.e., we have $p^2\cdot p^{p-1}=p{p+1}$ number of $p$ in factorization of $(p^2)!$ So, now the order is known and we need to find the subgroup... I would repeat the same idea as of $S_{2p}$ For the same reason as that of $S_{2p}$ I would take for $S_{9}=S_{3^2}$the set $\{(123),(456),(789)\}$. But then $\{\langle (123),(456),(789)\rangle\}=\{(123)^i(456)^j(789)^k : 0\leq i,j,k\leq 2\}$ is giving me a subgroup of order $27$ where as i need a group of order $81$ (which is the order of sylow $3$ subgroup of $S_{9}$ i.e.$3^{3+1}$) so, It sounds some thing like I should take an element of order $3$ which behaves nicely and does not create much problem and give a subgroup of order $81$ I am sure that any $3$ sylow subgroup of $S_9$ would have a copy isomorphic to  $\{\langle (123),(456),(789)\rangle\}$ so only problem is i am not sure how to extend this to a subgroup of order $81$ More generally for $S_{p^2}$ I would consider $$\{\langle(123\cdots p)(p+1\cdots 2p)\cdots (p^2-p+1\cdots p^2)\rangle\}$$ This would give a subgroup of order $p^p$ but then i need a subgroup of order $p^{p+1}$. I would be thankful if some one can help me to see what would be the obvious element that one should multiply this with :O Please help me to clear this. Thank you.",,"['abstract-algebra', 'group-theory']"
30,Galois Group of $x^5+ 5x^3 + 5x + 1$.,Galois Group of .,x^5+ 5x^3 + 5x + 1,"I've been asked to determine the Galois Group of $x^5+  5x^3 + 5x + 1$. This is what I know so far. 1) The polynomial is irreducible. 2) Its discriminant is $78125=5^7$ Since the discriminant is not a square, the Galois group should be $S_5$ or $F_{20}$. How can I know? In case it was $F_{20}$, is it solvable? Thanks in advance","I've been asked to determine the Galois Group of $x^5+  5x^3 + 5x + 1$. This is what I know so far. 1) The polynomial is irreducible. 2) Its discriminant is $78125=5^7$ Since the discriminant is not a square, the Galois group should be $S_5$ or $F_{20}$. How can I know? In case it was $F_{20}$, is it solvable? Thanks in advance",,"['abstract-algebra', 'galois-theory']"
31,What is the characteristic of a ring that does not contain an identity element?,What is the characteristic of a ring that does not contain an identity element?,,Does a ring without an identity element even have a characteristic?,Does a ring without an identity element even have a characteristic?,,['abstract-algebra']
32,At most one subgroup of every order dividing $\lvert G\rvert$ implies $G$ cyclic [closed],At most one subgroup of every order dividing  implies  cyclic [closed],\lvert G\rvert G,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Suppose we have a finite group $G$ of finite order $n$. For every $d\mid n$, $G$ has at most one subgroup of order $d$. Show that $G$ is cyclic.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Suppose we have a finite group $G$ of finite order $n$. For every $d\mid n$, $G$ has at most one subgroup of order $d$. Show that $G$ is cyclic.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups']"
33,"In a slice category C/A of a category C over a given object A, What is the role of the identity morphism of A in C with respect to C/A","In a slice category C/A of a category C over a given object A, What is the role of the identity morphism of A in C with respect to C/A",,"In a slice category $C/A$ of a category $C$ over a given object $A$, what is the role of  the $C$ identity morphism, $A\to A$ ($1_A$), in $C/A$, particularly with respect to composition? I understand that as an arrow targeting $A$, it is an object of $C/A$.  However, in $C$, $1_A$ is not practically composable, because it is an identity morphism and $(X\to A) \circ 1_A$ is just reduced to $X\to A$.  This doesn't seem like it would necesarily be the case in $C/A$, because in $C/A$, $1_A$ is not the identity morphism (the commutative diagram that represents $1_A\to 1_A$ should play this role). It seems possible that $1_A\to 1_A$ could be reduced to $1_A$, but I don't know if this is the case and how I would validate it.  If it isn't the case, then it would seem that for every other object, $Z\to A$ in $C/A$, at least two morphisms would exist: #1, $(Z\to A) \to (Z\to A)$, (the identity morphism in $C/A$); and, #2, $(Z\to A)\to (A\to A)$. I haven't found such an explanation in the books/articles I've read, which leads me to believe I am misunderstanding something fundamental about how slice categories are derived, but without an authoritative reference, I can't be sure.","In a slice category $C/A$ of a category $C$ over a given object $A$, what is the role of  the $C$ identity morphism, $A\to A$ ($1_A$), in $C/A$, particularly with respect to composition? I understand that as an arrow targeting $A$, it is an object of $C/A$.  However, in $C$, $1_A$ is not practically composable, because it is an identity morphism and $(X\to A) \circ 1_A$ is just reduced to $X\to A$.  This doesn't seem like it would necesarily be the case in $C/A$, because in $C/A$, $1_A$ is not the identity morphism (the commutative diagram that represents $1_A\to 1_A$ should play this role). It seems possible that $1_A\to 1_A$ could be reduced to $1_A$, but I don't know if this is the case and how I would validate it.  If it isn't the case, then it would seem that for every other object, $Z\to A$ in $C/A$, at least two morphisms would exist: #1, $(Z\to A) \to (Z\to A)$, (the identity morphism in $C/A$); and, #2, $(Z\to A)\to (A\to A)$. I haven't found such an explanation in the books/articles I've read, which leads me to believe I am misunderstanding something fundamental about how slice categories are derived, but without an authoritative reference, I can't be sure.",,"['abstract-algebra', 'category-theory', 'universal-algebra']"
34,"If $a$ and $b$ commute and $\text{gcd}\left(\text{ord}(a),\text{ord}(b)\right)=1$, then $\text{ord}(ab)=\text{ord}(a)\text{ord}(b)$.","If  and  commute and , then .","a b \text{gcd}\left(\text{ord}(a),\text{ord}(b)\right)=1 \text{ord}(ab)=\text{ord}(a)\text{ord}(b)","Prove if $\operatorname{ord}(a)=m$ , $\operatorname{ord}(b)=n$ , and $\operatorname{gcd}(m,n)=1$ , then $\operatorname{ord}(ab)=mn$ . I was reading this and was thinking how this proof would look like. I tried to do it and am not sure if this is correct. Here is what I did: If $a$ and $b$ commute then $(ab)^{mn} = a^{mn} * b^{mn} = (a^m)^n * (b^n)^m = 1 * 1 = 1.$ So $ord(ab) | mn$ . Now, take $k = \operatorname{ord}(ab) = m^\prime * n^\prime$ where m' is relatively prime with $n$ and $n'$ is relatively prime with $m$ . By the result above $m' | m$ and $n' | n$ . Now we have $((ab)^k)^{m/m'} = 1$ since $(ab)^k=1$ . But on the other hand, by the commutativity: $$((ab)^k)^(m/m') =$$ $$((ab)^(m' * n'))^(m/m') =$$ $$ a^{n'm} * b^{n'm} = $$ $$(a^m)^{n'} * b^{n'm'}=$$ $$b^{n'm'} = 1$$ This implies that $n' * m'$ is divisible by $n$ . But $m'$ is relatively prime with $n$ , so we must have $n' = n.$ By symmetry, $m' = m$ . So $ord(ab) = mn$ . Just to say this again, I want to prove if $a$ and $b$ commute and $m$ and $n$ are relatively prime then $$ord(ab)=mn.$$ The comments are vague. I guess this must be the answer then and there probably is not another way to do this.","Prove if , , and , then . I was reading this and was thinking how this proof would look like. I tried to do it and am not sure if this is correct. Here is what I did: If and commute then So . Now, take where m' is relatively prime with and is relatively prime with . By the result above and . Now we have since . But on the other hand, by the commutativity: This implies that is divisible by . But is relatively prime with , so we must have By symmetry, . So . Just to say this again, I want to prove if and commute and and are relatively prime then The comments are vague. I guess this must be the answer then and there probably is not another way to do this.","\operatorname{ord}(a)=m \operatorname{ord}(b)=n \operatorname{gcd}(m,n)=1 \operatorname{ord}(ab)=mn a b (ab)^{mn} = a^{mn} * b^{mn} = (a^m)^n * (b^n)^m = 1 * 1 = 1. ord(ab) | mn k = \operatorname{ord}(ab) = m^\prime * n^\prime n n' m m' | m n' | n ((ab)^k)^{m/m'} = 1 (ab)^k=1 ((ab)^k)^(m/m') = ((ab)^(m' * n'))^(m/m') =  a^{n'm} * b^{n'm} =  (a^m)^{n'} * b^{n'm'}= b^{n'm'} = 1 n' * m' n m' n n' = n. m' = m ord(ab) = mn a b m n ord(ab)=mn.","['abstract-algebra', 'group-theory', 'number-theory']"
35,Does $A\!\leq\!M$ and $B\!\leq\!N$ imply $A\!\otimes_R\!B\hookrightarrow M\!\otimes_R\!N$? (tensor product of modules),Does  and  imply ? (tensor product of modules),A\!\leq\!M B\!\leq\!N A\!\otimes_R\!B\hookrightarrow M\!\otimes_R\!N,"Let $R$ be a commutative unital ring. What would be an example of a $R$-modules $M,N$ with submodules $A,B$, such that there does not exist an embedding of $R$-modules $$A\!\otimes_R\!B\hookrightarrow M\!\otimes_R\!N.$$ I've tried with finitely generated $\mathbb{Z}$-modules, by searching for $M$ and $N$ with $M\!\otimes\!N \cong 0$, but then every time also $A\!\otimes\!B \cong 0$ happens in my attempts.","Let $R$ be a commutative unital ring. What would be an example of a $R$-modules $M,N$ with submodules $A,B$, such that there does not exist an embedding of $R$-modules $$A\!\otimes_R\!B\hookrightarrow M\!\otimes_R\!N.$$ I've tried with finitely generated $\mathbb{Z}$-modules, by searching for $M$ and $N$ with $M\!\otimes\!N \cong 0$, but then every time also $A\!\otimes\!B \cong 0$ happens in my attempts.",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra', 'tensor-products']"
36,"Let $A,B$ be two subsets of a finite group $G$. If $|A|+|B|>|G|$, show that $G=AB$","Let  be two subsets of a finite group . If , show that","A,B G |A|+|B|>|G| G=AB","Let $A,B$ be two subsets of a finite group $G$. If $|A|+|B|>|G|$, show that $G=AB$. My attempt is : Since $|A|+|B|>|G|$, there exists one common element in both sets $A$ and $B$, say $g$. Then since $G$ is a group, by closure, $g^2 \in G$, which implies that $G \subset AB$. Let $a \in A$, $b \in B$. Then I get stuck at proving another inclusion.","Let $A,B$ be two subsets of a finite group $G$. If $|A|+|B|>|G|$, show that $G=AB$. My attempt is : Since $|A|+|B|>|G|$, there exists one common element in both sets $A$ and $B$, say $g$. Then since $G$ is a group, by closure, $g^2 \in G$, which implies that $G \subset AB$. Let $a \in A$, $b \in B$. Then I get stuck at proving another inclusion.",,['abstract-algebra']
37,Is the tensor product of two torsion-free modules always non-zero?,Is the tensor product of two torsion-free modules always non-zero?,,"Let $R$ be a commutative domain and let $M$ and $N$ be torsion-free $R$ modules. I would like to know whether or not $M\otimes_{R}{N}$ is always non-zero. Now, I know this is true in the finitely generated case; in fact we have something stronger, since for finitely generated modules $M$ and $N$ , $M\otimes_{R}{N}=\{0\}$ if and only if $\operatorname{Ann}{M}+\operatorname{Ann}{N}=R$ . Back to the general non finitely generated case. Take $m\in{M}$ and $n\in{N}$ , both non-zero. Then in particular (using the result stated above or just by the fact that we are tensoring free modules), $m\otimes{n}$ is non-zero as an element of $R{m}\otimes_{R}{R{n}}$ . Consider now the short exact sequence $0\rightarrow{Rn}\rightarrow{N}\rightarrow{N/Rn}\rightarrow{0}$ Let's now tensor this sequence by the module $mR$ . Since $M$ is torsion-free, this means we are tensoring by a free (hence flat) module, and so we get a short exact sequence $0\rightarrow{mR\otimes_{R}Rn}\rightarrow{mR\otimes_{R}{N}}\rightarrow{mR\otimes_{R}{N/Rn}}\rightarrow{0}$ and so we deduce that since $m\otimes{n}$ is non-zero in $mR\otimes_{R}Rn$ and this module embeds in $mR\otimes_{R}{N}$ , that $m\otimes{n}$ is non-zero as an element of $mR\otimes_{R}{N}$ . This is as far as I've managed to get. A couple of points to note: If I was working over a semiheriditary domain, then modules are flat if and only if they are torsion-free, and so I believe the result should always hold in this case. Thus if a counter example to my question exists, we must work over domains in which every there are non-projective finitely generated ideals. Since I can't find a statement, never mind a proof, of this result anywhere, I feel it's probably false. However, intuitively (to me at least), it should be true, for it feels like whenever I've seen an example about how to show some element $m\otimes{n}$ of a tensor product is zero, it usually involves some sort of argument with the bilinearity properties of the tensor and the torsion properties of the module. I'd very much appreciate if someone could help me work towards finding a proof or counterexample to this claim.","Let be a commutative domain and let and be torsion-free modules. I would like to know whether or not is always non-zero. Now, I know this is true in the finitely generated case; in fact we have something stronger, since for finitely generated modules and , if and only if . Back to the general non finitely generated case. Take and , both non-zero. Then in particular (using the result stated above or just by the fact that we are tensoring free modules), is non-zero as an element of . Consider now the short exact sequence Let's now tensor this sequence by the module . Since is torsion-free, this means we are tensoring by a free (hence flat) module, and so we get a short exact sequence and so we deduce that since is non-zero in and this module embeds in , that is non-zero as an element of . This is as far as I've managed to get. A couple of points to note: If I was working over a semiheriditary domain, then modules are flat if and only if they are torsion-free, and so I believe the result should always hold in this case. Thus if a counter example to my question exists, we must work over domains in which every there are non-projective finitely generated ideals. Since I can't find a statement, never mind a proof, of this result anywhere, I feel it's probably false. However, intuitively (to me at least), it should be true, for it feels like whenever I've seen an example about how to show some element of a tensor product is zero, it usually involves some sort of argument with the bilinearity properties of the tensor and the torsion properties of the module. I'd very much appreciate if someone could help me work towards finding a proof or counterexample to this claim.",R M N R M\otimes_{R}{N} M N M\otimes_{R}{N}=\{0\} \operatorname{Ann}{M}+\operatorname{Ann}{N}=R m\in{M} n\in{N} m\otimes{n} R{m}\otimes_{R}{R{n}} 0\rightarrow{Rn}\rightarrow{N}\rightarrow{N/Rn}\rightarrow{0} mR M 0\rightarrow{mR\otimes_{R}Rn}\rightarrow{mR\otimes_{R}{N}}\rightarrow{mR\otimes_{R}{N/Rn}}\rightarrow{0} m\otimes{n} mR\otimes_{R}Rn mR\otimes_{R}{N} m\otimes{n} mR\otimes_{R}{N} m\otimes{n},"['abstract-algebra', 'homological-algebra', 'tensor-products']"
38,First Isomorphism Theorem,First Isomorphism Theorem,,"Theorem: Let $G$ and $G'$ be groups  and let $f:G\to G'$ be a group homomorphism. Then $G/\textrm{ker}\, f  \cong\textrm{im}\, f$. My question is how to understand this theorem  intuitively.","Theorem: Let $G$ and $G'$ be groups  and let $f:G\to G'$ be a group homomorphism. Then $G/\textrm{ker}\, f  \cong\textrm{im}\, f$. My question is how to understand this theorem  intuitively.",,"['abstract-algebra', 'group-theory']"
39,Stabilizer of a point and orbit of a point,Stabilizer of a point and orbit of a point,,"I really need help with this topic I have an exam tomorrow and am trying to get this stuff in my head. But the book is not explaining me these two topics properly. It gives me the definition of a stabilizer at a point where $\mathrm {Stab}_G (i) = \{\phi \in G \mid \phi(i) = i\}$, and where $\mathrm{Orb}_G (i) = \{\phi(i) \mid \phi \in G\}$. I do not know how to calculate the stabilizer nor the orbit for this. I am also given an example Let $G = \{ (1), (132)(465)(78), (132)(465), (123)(456), (123)(456)(78), (78)\}$ and then $\mathrm{Orb}_G (1) = \{1, 3, 2\}$, $\mathrm{Orb}_G (2) = \{2, 1, 3\}$, $\mathrm{Orb}_G (4) = \{4, 6, 5\}$, and $\mathrm{Orb}_G (7) = \{7, 8\}$. also $\mathrm{Stab}_G (1) = \{(1), (78)\},\\ \mathrm{Stab}_G (2) = \{(1), (78)\},\\ \mathrm{Stab}_G (3) = \{(1), (78)\},\text {and}\\ \mathrm{Stab}_G (7) = \{(1), (132)(465), (123)(456)\}.$ If someone could PLEASE go step by step in how this example was solved it would be really helpful. Thank you","I really need help with this topic I have an exam tomorrow and am trying to get this stuff in my head. But the book is not explaining me these two topics properly. It gives me the definition of a stabilizer at a point where $\mathrm {Stab}_G (i) = \{\phi \in G \mid \phi(i) = i\}$, and where $\mathrm{Orb}_G (i) = \{\phi(i) \mid \phi \in G\}$. I do not know how to calculate the stabilizer nor the orbit for this. I am also given an example Let $G = \{ (1), (132)(465)(78), (132)(465), (123)(456), (123)(456)(78), (78)\}$ and then $\mathrm{Orb}_G (1) = \{1, 3, 2\}$, $\mathrm{Orb}_G (2) = \{2, 1, 3\}$, $\mathrm{Orb}_G (4) = \{4, 6, 5\}$, and $\mathrm{Orb}_G (7) = \{7, 8\}$. also $\mathrm{Stab}_G (1) = \{(1), (78)\},\\ \mathrm{Stab}_G (2) = \{(1), (78)\},\\ \mathrm{Stab}_G (3) = \{(1), (78)\},\text {and}\\ \mathrm{Stab}_G (7) = \{(1), (132)(465), (123)(456)\}.$ If someone could PLEASE go step by step in how this example was solved it would be really helpful. Thank you",,"['abstract-algebra', 'group-theory']"
40,Show $\mathbb{Z}[\sqrt{2}]$ is a PID,Show  is a PID,\mathbb{Z}[\sqrt{2}],"I understand how to show something isn't a PID (namely by constructing a counterexample), and I think I understand the proof that $\mathbb{Z}$ is a PID, but I'm not sure how to modify it so that I can show $\mathbb{Z}[\sqrt{2}]$ is a PID. I'm given a hint to let $d(a+b\sqrt{2}):=|a^2-2b^2|$, but I'm not sure how to use a norm to prove that all ideals are principal.","I understand how to show something isn't a PID (namely by constructing a counterexample), and I think I understand the proof that $\mathbb{Z}$ is a PID, but I'm not sure how to modify it so that I can show $\mathbb{Z}[\sqrt{2}]$ is a PID. I'm given a hint to let $d(a+b\sqrt{2}):=|a^2-2b^2|$, but I'm not sure how to use a norm to prove that all ideals are principal.",,"['abstract-algebra', 'principal-ideal-domains']"
41,Multiple faithful representation?,Multiple faithful representation?,,"I came across the following paragraph on pp.56 of Marshall Hall’s “The theory of groups” 1959 ed. ... the non-Abelian group of order 6 may be faithfully represented as a transitive permutation group on three letters and also on six letters ... My question is, In general, is there a theory about this? That is, given a non-Abelian group of order n, on what conditions it can be faithfully represented by different transitive permutation groups? If the answer to the above question is yes, how do we call it? Multiple faithful representation? or something else? Thanks in advance.","I came across the following paragraph on pp.56 of Marshall Hall’s “The theory of groups” 1959 ed. ... the non-Abelian group of order 6 may be faithfully represented as a transitive permutation group on three letters and also on six letters ... My question is, In general, is there a theory about this? That is, given a non-Abelian group of order n, on what conditions it can be faithfully represented by different transitive permutation groups? If the answer to the above question is yes, how do we call it? Multiple faithful representation? or something else? Thanks in advance.",,"['abstract-algebra', 'group-theory']"
42,What happens when the order of two finite subgroups are relatively prime?,What happens when the order of two finite subgroups are relatively prime?,,"I'm returning from an exam on group-theory and there were 2 questions I couldn't solve (and still can't), so I'm asking here for any hint you could possibly give. Let G be a group and H and K subgroups such that $|H| = n$, $|K| = m$ and $gcd(n, m) = 1$. Show that $H \cap K = \{e\}$. I wish I could show you some of my attempts before hand, but they're all rubbish that didn't get me anywhere. Essentially, the only (and last) thing I remembered and thought it could be useful was to see if if H and K are partitions of G. I think I've read something similar somewhere but can't recall where, so, am uncertain about it. The other question I couldn't solve is, I think, related to this, so I shall try it once I understand this one. Thanks for taking the time to read! Any tip is appreciated.","I'm returning from an exam on group-theory and there were 2 questions I couldn't solve (and still can't), so I'm asking here for any hint you could possibly give. Let G be a group and H and K subgroups such that $|H| = n$, $|K| = m$ and $gcd(n, m) = 1$. Show that $H \cap K = \{e\}$. I wish I could show you some of my attempts before hand, but they're all rubbish that didn't get me anywhere. Essentially, the only (and last) thing I remembered and thought it could be useful was to see if if H and K are partitions of G. I think I've read something similar somewhere but can't recall where, so, am uncertain about it. The other question I couldn't solve is, I think, related to this, so I shall try it once I understand this one. Thanks for taking the time to read! Any tip is appreciated.",,"['abstract-algebra', 'group-theory']"
43,Supplement for Jacobson's Basic Algebra I,Supplement for Jacobson's Basic Algebra I,,"I am going to be taking a first course in abstract algebra this fall and we are using Jacobson's Basic Algebra I for the course text. It looks like a good textbook, but it is lacking a lot of motivation (I am new to higher math), so I was wondering if there are other books to supplement my studies. Particularly historical or geometric motivations; Something to give me an idea about why we are learning about ideals, modules, etc. so I can ""picture"" what we are learning. Thanks for any input!","I am going to be taking a first course in abstract algebra this fall and we are using Jacobson's Basic Algebra I for the course text. It looks like a good textbook, but it is lacking a lot of motivation (I am new to higher math), so I was wondering if there are other books to supplement my studies. Particularly historical or geometric motivations; Something to give me an idea about why we are learning about ideals, modules, etc. so I can ""picture"" what we are learning. Thanks for any input!",,['abstract-algebra']
44,Abelian group is free iff projective,Abelian group is free iff projective,,"It is well-known that an abelian group is free iff it is projective, which can be easily extended to modules over PID. The standard proof requires the use of Zorn's Lemma/Well-ordering principle. Are there ways of proving this without axiom of choice or anything equivalent? Any hints or proof outlines will be appreciated. Any tools from introductory homological algebra and commutative algebra can be freely used. Thanks in advance.","It is well-known that an abelian group is free iff it is projective, which can be easily extended to modules over PID. The standard proof requires the use of Zorn's Lemma/Well-ordering principle. Are there ways of proving this without axiom of choice or anything equivalent? Any hints or proof outlines will be appreciated. Any tools from introductory homological algebra and commutative algebra can be freely used. Thanks in advance.",,"['abstract-algebra', 'commutative-algebra']"
45,Does the center of a perfect group not contain all elements of prime order?,Does the center of a perfect group not contain all elements of prime order?,,Let $G$ be a finite perfect group (i.e. $G=G'$ ) and $Z(G)$ be its center. I don't know whether this statement is correct: There exists an element $x$ of prime order such that $x\notin Z(G)$ . A quick check on CFSG gives that this holds for every (quasi-)simple group. But what if $G$ is a general finite perfect group? Or is there any further descriptions on the center of perfect groups? Another description on this question is (also I don't know if this holds): Let $H$ be a center-less (insoluble) group (i.e. $Z(H)=1$ ). Then there always exists a prime divisor $p$ of $|H|$ such that the $p$ -part of the Schur multiplicator of $H$ is trivial. Is there any result on both?,Let be a finite perfect group (i.e. ) and be its center. I don't know whether this statement is correct: There exists an element of prime order such that . A quick check on CFSG gives that this holds for every (quasi-)simple group. But what if is a general finite perfect group? Or is there any further descriptions on the center of perfect groups? Another description on this question is (also I don't know if this holds): Let be a center-less (insoluble) group (i.e. ). Then there always exists a prime divisor of such that the -part of the Schur multiplicator of is trivial. Is there any result on both?,G G=G' Z(G) x x\notin Z(G) G H Z(H)=1 p |H| p H,"['abstract-algebra', 'group-theory', 'reference-request', 'finite-groups']"
46,When does the underlying map of a polynomial induce a permutation on $\mathbb{Z}/p\mathbb{Z}$?,When does the underlying map of a polynomial induce a permutation on ?,\mathbb{Z}/p\mathbb{Z},"For example, the underlying function of the polynomial $$f(x)=4x^2-3x^7$$ induces a permutation on $\mathbb{Z}/11\mathbb{Z}$ , though I only know the proof by ""brutal force"" (is there a cleverer proof?). In general, is there a criterion determining if a polynomial $f(x)$ induces a permutation on $\mathbb{Z}/p\mathbb{Z}$ ? Edit : I am not even sure if it matters whether $p$ is prime or not but I will tag this question ""finite fields"" for now. Does it matter?","For example, the underlying function of the polynomial induces a permutation on , though I only know the proof by ""brutal force"" (is there a cleverer proof?). In general, is there a criterion determining if a polynomial induces a permutation on ? Edit : I am not even sure if it matters whether is prime or not but I will tag this question ""finite fields"" for now. Does it matter?",f(x)=4x^2-3x^7 \mathbb{Z}/11\mathbb{Z} f(x) \mathbb{Z}/p\mathbb{Z} p,"['abstract-algebra', 'group-theory', 'polynomials', 'permutations', 'finite-fields']"
47,Existence of homomorphisms between finite fields,Existence of homomorphisms between finite fields,,"Let $F$ and $E$ be the fields of order $8$ and $32$ respectively. Construct a ring homomorphism $F\to E$ or prove that one cannot exist. Any element $x$ of $F$ satisfies $x^8=x$ and any nonzero element $x\in F$ satisfies $x^7=1$. Let $f:F\to E$ be a homomorphism. Then $1=f(1)=f(x^7)=f(x)^7$ for all nonzero $x\in F$. Also $0=f(0)=f(2)=f(1+1)=f(1)+f(1)=0$ (here $2=1+1\in F$). Now on the one hand, $f(2^7)=f(2)^7=1$. On the other hand, $f(2^7)$ is $f$ applied to $2+\dots+2$ ($2^6$ terms), so $f(2^7)=f(2\cdot 2^6)=f(2)+\dots+f(2) \text{ ($2^6$ terms) }=0+\dots+0=0$. This is a contradiction. Is this reasoning correct? Are there other ways to solve this? In general, for which finite fields there exists a homomorphism between them? As I pointed out in the comments, the above argument is incorrect. How do I solve the problem then?","Let $F$ and $E$ be the fields of order $8$ and $32$ respectively. Construct a ring homomorphism $F\to E$ or prove that one cannot exist. Any element $x$ of $F$ satisfies $x^8=x$ and any nonzero element $x\in F$ satisfies $x^7=1$. Let $f:F\to E$ be a homomorphism. Then $1=f(1)=f(x^7)=f(x)^7$ for all nonzero $x\in F$. Also $0=f(0)=f(2)=f(1+1)=f(1)+f(1)=0$ (here $2=1+1\in F$). Now on the one hand, $f(2^7)=f(2)^7=1$. On the other hand, $f(2^7)$ is $f$ applied to $2+\dots+2$ ($2^6$ terms), so $f(2^7)=f(2\cdot 2^6)=f(2)+\dots+f(2) \text{ ($2^6$ terms) }=0+\dots+0=0$. This is a contradiction. Is this reasoning correct? Are there other ways to solve this? In general, for which finite fields there exists a homomorphism between them? As I pointed out in the comments, the above argument is incorrect. How do I solve the problem then?",,"['abstract-algebra', 'ring-theory', 'field-theory', 'finite-fields', 'ring-homomorphism']"
48,Is $\mathbb Q(\zeta_6)=\mathbb {Q}(\zeta_3)$?,Is ?,\mathbb Q(\zeta_6)=\mathbb {Q}(\zeta_3),"I got myself confused over the following: We have $$\mathbb Q(\zeta_3)=\mathbb Q(\exp(2\pi i/3))=\mathbb Q\left(\cos\frac{2\pi}{3}+i\sin\frac{2\pi}{3}\right)=\mathbb Q\left(-\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3),$$ but also $$\mathbb Q(\zeta_6)=\mathbb Q(\exp(2\pi i/6))=\mathbb Q\left(\cos\frac{2\pi}{6}+i\sin\frac{2\pi}{6}\right)=\mathbb Q\left(\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3).$$ So the fields are absolutely identical? $\Phi_6$ splits in $\mathbb Q (\zeta_3 )$ and vice versa?","I got myself confused over the following: We have $$\mathbb Q(\zeta_3)=\mathbb Q(\exp(2\pi i/3))=\mathbb Q\left(\cos\frac{2\pi}{3}+i\sin\frac{2\pi}{3}\right)=\mathbb Q\left(-\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3),$$ but also $$\mathbb Q(\zeta_6)=\mathbb Q(\exp(2\pi i/6))=\mathbb Q\left(\cos\frac{2\pi}{6}+i\sin\frac{2\pi}{6}\right)=\mathbb Q\left(\frac{1}{2}+\frac{i\sqrt 3}{2}\right)=\mathbb Q(i\sqrt 3).$$ So the fields are absolutely identical? $\Phi_6$ splits in $\mathbb Q (\zeta_3 )$ and vice versa?",,['abstract-algebra']
49,Commutative ring category is not additive category,Commutative ring category is not additive category,,"I was told that commutative ring category is not additive category all the time and it is thus not abelian category. A category is additive if $Hom(A,B)$ is abelian group for all objects $A,B$ and morphisms satisfy distributive laws, it has zero objects and it has finite coproduct and products. It seems commutative ring satisfies most of the requirement except $Hom(A,B)$ being abelian group. $Hom(A,B)$ is not abelian group by taking any $f\in Hom(A,B)$. If $f$ has inverse $g$, I can check $g(a_1a_2)=f(a_1a_2)$ which never cancels out with $f(a_1a_2)$. It definitely satisfies distributive laws of morphisms.(Wrong. Distributive law fails due to not fixing $1$.) Zero object is 0 ring as identity element is $0$ in 0 ring. Coproduct is tensor and product is direct sum. Does commutative ring category fail only $Hom(A,B)$ being abelian group requirement?","I was told that commutative ring category is not additive category all the time and it is thus not abelian category. A category is additive if $Hom(A,B)$ is abelian group for all objects $A,B$ and morphisms satisfy distributive laws, it has zero objects and it has finite coproduct and products. It seems commutative ring satisfies most of the requirement except $Hom(A,B)$ being abelian group. $Hom(A,B)$ is not abelian group by taking any $f\in Hom(A,B)$. If $f$ has inverse $g$, I can check $g(a_1a_2)=f(a_1a_2)$ which never cancels out with $f(a_1a_2)$. It definitely satisfies distributive laws of morphisms.(Wrong. Distributive law fails due to not fixing $1$.) Zero object is 0 ring as identity element is $0$ in 0 ring. Coproduct is tensor and product is direct sum. Does commutative ring category fail only $Hom(A,B)$ being abelian group requirement?",,"['abstract-algebra', 'category-theory', 'abelian-categories', 'additive-categories']"
50,Computing $(3^{999^{100}} + 7^{960^{961}}) \bmod 225 $,Computing,(3^{999^{100}} + 7^{960^{961}}) \bmod 225 ,"Compute $$(3^{999^{100}} + 7^{960^{961}}) \bmod 225. $$ I first computed the left expression, and eventually found it, but it took me like an hour, so I was wondering if there is no faster way. First I wrote $$3^{999^{100}} \bmod 225 = (3^{999} \bmod 225)^{100} \bmod 225. $$ I noticed that $225 = 3 \cdot 75 = 3 \cdot (3 \cdot 25)$. So I tried working on the system $$ \begin{cases} 3^{999^{100}} \bmod 25 = 0 \\ (27^{333} \bmod 25)^{100} \bmod 25 \end{cases}$$ and then using Chinese Remainder theorem. For the last equation, We have $27^{333} \equiv 2^{333} \equiv (8^{111} \bmod 25)^{100} \bmod 25. $ To compute $(8^{111} \bmod 25)$, I just kept squaring, computing $8^2, 8^4, 8^8$ etc, and taking the modulo each time. Eventually I found $8^{111} \equiv 17 \bmod 25$. Then I needed to find $$17^{100} \bmod 25 \equiv (17^4 \cdot 17^{25} ) \bmod 25. $$ By squaring again I found the answer as $22 \bmod 25$. So I used the Chinese Remainder theorem to find the solution of $$ \begin{cases} x \bmod 25 = 22 \\ x \bmod 3 = 0 \end{cases}$$ which gave me $x = 72$. So I managed to find $$3^{999^{100}} \bmod 225 = 72 \bmod 225. $$ Now, I was wondering if there is no better trick/faster way to find the correct answer by hand (no calculator), since I'm not looking forward to doing another hour of computation to find the second expression. Thank you in advance. Later addition: With the help of Arthurs hints: We have $225 = 9 \cdot 25$. Hence for the first expression I have the system $$ \begin{cases} 3^{999^{100}} \bmod 9 = 0 \\ 3^{999^{100}} \bmod 25 = a. \end{cases}. $$ I want to find $a$ now. I have $\phi(25) = 20$, so $3^{20} \bmod 25 = 1 \bmod 25$ by Euler. Now I wanted to write the exponent $999^{100}$ as a multiple of $20$. I have $3^{(20 \cdot (50-1))^{100}} = 3^{20^{100} \cdot (50-1)^{100}} = (3^{20})^{(20^{99} \cdot (50-1)^{100})}$. So taking the modulo would give me $1$?","Compute $$(3^{999^{100}} + 7^{960^{961}}) \bmod 225. $$ I first computed the left expression, and eventually found it, but it took me like an hour, so I was wondering if there is no faster way. First I wrote $$3^{999^{100}} \bmod 225 = (3^{999} \bmod 225)^{100} \bmod 225. $$ I noticed that $225 = 3 \cdot 75 = 3 \cdot (3 \cdot 25)$. So I tried working on the system $$ \begin{cases} 3^{999^{100}} \bmod 25 = 0 \\ (27^{333} \bmod 25)^{100} \bmod 25 \end{cases}$$ and then using Chinese Remainder theorem. For the last equation, We have $27^{333} \equiv 2^{333} \equiv (8^{111} \bmod 25)^{100} \bmod 25. $ To compute $(8^{111} \bmod 25)$, I just kept squaring, computing $8^2, 8^4, 8^8$ etc, and taking the modulo each time. Eventually I found $8^{111} \equiv 17 \bmod 25$. Then I needed to find $$17^{100} \bmod 25 \equiv (17^4 \cdot 17^{25} ) \bmod 25. $$ By squaring again I found the answer as $22 \bmod 25$. So I used the Chinese Remainder theorem to find the solution of $$ \begin{cases} x \bmod 25 = 22 \\ x \bmod 3 = 0 \end{cases}$$ which gave me $x = 72$. So I managed to find $$3^{999^{100}} \bmod 225 = 72 \bmod 225. $$ Now, I was wondering if there is no better trick/faster way to find the correct answer by hand (no calculator), since I'm not looking forward to doing another hour of computation to find the second expression. Thank you in advance. Later addition: With the help of Arthurs hints: We have $225 = 9 \cdot 25$. Hence for the first expression I have the system $$ \begin{cases} 3^{999^{100}} \bmod 9 = 0 \\ 3^{999^{100}} \bmod 25 = a. \end{cases}. $$ I want to find $a$ now. I have $\phi(25) = 20$, so $3^{20} \bmod 25 = 1 \bmod 25$ by Euler. Now I wanted to write the exponent $999^{100}$ as a multiple of $20$. I have $3^{(20 \cdot (50-1))^{100}} = 3^{20^{100} \cdot (50-1)^{100}} = (3^{20})^{(20^{99} \cdot (50-1)^{100})}$. So taking the modulo would give me $1$?",,"['abstract-algebra', 'elementary-number-theory']"
51,Between complex numbers and quaternions?,Between complex numbers and quaternions?,,"Complex numbers are $a+ b i $; Quaternions are $a + b i + c j + d k $. So, do there exist numbers like $a + b i + c j$? Here $a$, $b$, $c$, $d$ are all real. Did Hamilton consider such a case?","Complex numbers are $a+ b i $; Quaternions are $a + b i + c j + d k $. So, do there exist numbers like $a + b i + c j$? Here $a$, $b$, $c$, $d$ are all real. Did Hamilton consider such a case?",,['abstract-algebra']
52,The order of group $G$ is odd. Prove the mapping $f:G\to G$ by $f(x) = x^2$ is injective.,The order of group  is odd. Prove the mapping  by  is injective.,G f:G\to G f(x) = x^2,"The order of group $G$ is odd. Prove the mapping $f:G\to G$ by $f(x) = x^2$ is injective For what it is worth this is what I have tried. Assume $x,y \in G$,  $f(x) = f(y)$. We want to show $x = y$. Case (i) Either $x$ or $y$ is $e$ (the identity element of $G$). Then w.l.o.g. let $x = e$. So $x^2 = y^2$.  $e^2 = e = y^2$.  So $y^{-1} = y$. So $y = e = x$ and we are done or the order of $y$ is $2$ which is a contradiction since $|G|$ is odd. Case (ii) Neither $x$ nor $y$ is $e$.  $x^2 = y^2$. Now I am out of gas.  I stared at some Cayley tables of $Z_n$ under modular addition for odd $n$.  Sure enough the elements on the diagonal are the group elements.  I cannot even think of any other examples of groups of odd order.","The order of group $G$ is odd. Prove the mapping $f:G\to G$ by $f(x) = x^2$ is injective For what it is worth this is what I have tried. Assume $x,y \in G$,  $f(x) = f(y)$. We want to show $x = y$. Case (i) Either $x$ or $y$ is $e$ (the identity element of $G$). Then w.l.o.g. let $x = e$. So $x^2 = y^2$.  $e^2 = e = y^2$.  So $y^{-1} = y$. So $y = e = x$ and we are done or the order of $y$ is $2$ which is a contradiction since $|G|$ is odd. Case (ii) Neither $x$ nor $y$ is $e$.  $x^2 = y^2$. Now I am out of gas.  I stared at some Cayley tables of $Z_n$ under modular addition for odd $n$.  Sure enough the elements on the diagonal are the group elements.  I cannot even think of any other examples of groups of odd order.",,"['abstract-algebra', 'group-theory']"
53,"Why is $(X,Y)$ not a projective $\mathbb{Z}[X,Y]$-module?",Why is  not a projective -module?,"(X,Y) \mathbb{Z}[X,Y]","This was an exam problem I had which stumped me. The question was to prove that the ideal generated by $X$ and $Y$ in $\mathbb{Z}[X,Y]$ is not a projective $\mathbb{Z}[X,Y]$-module. I was trying to exhibit an exact sequence with fourth term $(X,Y)$  which did not split, but hit a dead end.","This was an exam problem I had which stumped me. The question was to prove that the ideal generated by $X$ and $Y$ in $\mathbb{Z}[X,Y]$ is not a projective $\mathbb{Z}[X,Y]$-module. I was trying to exhibit an exact sequence with fourth term $(X,Y)$  which did not split, but hit a dead end.",,"['abstract-algebra', 'projective-module']"
54,Number of elements in the ring $\mathbb Z [i]/\langle 2+2i\rangle$,Number of elements in the ring,\mathbb Z [i]/\langle 2+2i\rangle,"The question is : Show that $I=\langle 2+2 i\rangle$ is not a prime ideal of $\mathbb Z[i]$. Also find the number of elements in $\mathbb Z[i]/I$ and its characteristic. My try: I started with elements $2, 1+i \in \mathbb Z[i]$. Clearly, the product is $0$ in the coset representation. The part where i am struck at is the number of elements and the characteristic. Well, $2+2i+\langle 2+2i\rangle=0+\langle 2+2i\rangle$ Can we directly write $i^2=-1$ in the coset representation?","The question is : Show that $I=\langle 2+2 i\rangle$ is not a prime ideal of $\mathbb Z[i]$. Also find the number of elements in $\mathbb Z[i]/I$ and its characteristic. My try: I started with elements $2, 1+i \in \mathbb Z[i]$. Clearly, the product is $0$ in the coset representation. The part where i am struck at is the number of elements and the characteristic. Well, $2+2i+\langle 2+2i\rangle=0+\langle 2+2i\rangle$ Can we directly write $i^2=-1$ in the coset representation?",,"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals', 'gaussian-integers']"
55,Are there always nontrivial primitive elements in a Hopf algebra?,Are there always nontrivial primitive elements in a Hopf algebra?,,"Let $k$ be an algebraically closed field of arbitrary characteristic.  Let $H$ be a Hopf algebra over $k$.  We say $x\in H$ is a primitive element if $\Delta(x)=1\otimes x+x\otimes 1$, where $\Delta$ is the comultiplication in $H$.  The set of primitive elements in $H$, denoted $P(H)$, is a Lie subalgebra, with bracket given by the commutator. Is $P(H)$ always nontrivial?  If not, what is an example of a Hopf algebra with $P(H)=0$?  What if I require $H$ to be finite-dimensional, or finite-dimensional and local?  Do we have examples of such Hopf algebras with no nontrivial primitive elements?","Let $k$ be an algebraically closed field of arbitrary characteristic.  Let $H$ be a Hopf algebra over $k$.  We say $x\in H$ is a primitive element if $\Delta(x)=1\otimes x+x\otimes 1$, where $\Delta$ is the comultiplication in $H$.  The set of primitive elements in $H$, denoted $P(H)$, is a Lie subalgebra, with bracket given by the commutator. Is $P(H)$ always nontrivial?  If not, what is an example of a Hopf algebra with $P(H)=0$?  What if I require $H$ to be finite-dimensional, or finite-dimensional and local?  Do we have examples of such Hopf algebras with no nontrivial primitive elements?",,"['abstract-algebra', 'lie-algebras', 'hopf-algebras', 'coalgebras']"
56,What are some properties that imply that a group must be the trivial group?,What are some properties that imply that a group must be the trivial group?,,"In the problem posed in this question of mine we want to show that a particular group is both perfect and solvable, and therefore trivial, and this turns out to be useful in proving the result. What other combinations of properties required of a group imply that it must be isomorphic to the trivial group?","In the problem posed in this question of mine we want to show that a particular group is both perfect and solvable, and therefore trivial, and this turns out to be useful in proving the result. What other combinations of properties required of a group imply that it must be isomorphic to the trivial group?",,"['abstract-algebra', 'group-theory', 'soft-question', 'big-list']"
57,"if $A^\times $ is a commutative group, does $A$ necessarily be a commutative ring?","if  is a commutative group, does  necessarily be a commutative ring?",A^\times  A,"Let $A$ be a ring and $A^\times$ be the collection of unit elements of $A$. If $A$ is a commutative ring, then $A^\times$ is a commutative group. Conversely, if $A^\times $ is a commutative group, does $A$ necessarily be a commutative ring? Is there any counterexample?","Let $A$ be a ring and $A^\times$ be the collection of unit elements of $A$. If $A$ is a commutative ring, then $A^\times$ is a commutative group. Conversely, if $A^\times $ is a commutative group, does $A$ necessarily be a commutative ring? Is there any counterexample?",,"['abstract-algebra', 'group-theory', 'commutative-algebra', 'ring-theory', 'noncommutative-algebra']"
58,Polynomial ring with uncountable indeterminates,Polynomial ring with uncountable indeterminates,,"In Rotman's Advanced Modern Algebra second edition (2010), on page 883 (or on page 905 in its first edition (2002)), in the proof of the existence of localization of a commutative ring $R$ on its multiplicative subset $S$, he writes: ""Let $X=(x_{s})_{s\in S}$ be an indexed set with $x_{s}\mapsto s$ a bijection $X \rightarrow S$, and let $R[X]$ be the polynomial ring over R with indeterminates $X$."" However in his definition of formal power series over $R$ he comments: ""To determine when two formal power series are equal, let us recognize that a sequence $\sigma$ is really a function $\sigma:\mathbb{N} \rightarrow R$, where $\mathbb{N}$ is the set of  natural numbers, with $\sigma(i) = s_{i}$ for all $i \geq 0$."" So I want to ask if $S$ is uncountable, then is it still legitimate that he defines $R[X]$ in this way?","In Rotman's Advanced Modern Algebra second edition (2010), on page 883 (or on page 905 in its first edition (2002)), in the proof of the existence of localization of a commutative ring $R$ on its multiplicative subset $S$, he writes: ""Let $X=(x_{s})_{s\in S}$ be an indexed set with $x_{s}\mapsto s$ a bijection $X \rightarrow S$, and let $R[X]$ be the polynomial ring over R with indeterminates $X$."" However in his definition of formal power series over $R$ he comments: ""To determine when two formal power series are equal, let us recognize that a sequence $\sigma$ is really a function $\sigma:\mathbb{N} \rightarrow R$, where $\mathbb{N}$ is the set of  natural numbers, with $\sigma(i) = s_{i}$ for all $i \geq 0$."" So I want to ask if $S$ is uncountable, then is it still legitimate that he defines $R[X]$ in this way?",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
59,inverse limit isomorphisms,inverse limit isomorphisms,,"Let $(I,\leq )$, $(J,\leq )$ be partially ordered sets and $G_{ij}$ be a set of groups indexed by pairs $(i,j)\in I\times J$.The product of the partially ordered sets $(I,\leq )$, $(J,\leq )$ is $I\times J$ with the partial order $(i,j)\leq (i',j')$ iff $i\leq i'$ and $j\leq j'$. Suppose that for all $(i,j)\leq (i',j')$ we are given homomorphisms $f_{ij,i'j'}:G_{i'j'}\rightarrow G_{ij}$ satisfying the usual transitivity conditions. How to prove that there are canonical isomorphisms : $\varprojlim_{i}\varprojlim_{j}G_{ij}\sim \varprojlim_{(i,j)}G_{ij}\sim \varprojlim_{j}\varprojlim_{i}G_{ij}$? Thank you.","Let $(I,\leq )$, $(J,\leq )$ be partially ordered sets and $G_{ij}$ be a set of groups indexed by pairs $(i,j)\in I\times J$.The product of the partially ordered sets $(I,\leq )$, $(J,\leq )$ is $I\times J$ with the partial order $(i,j)\leq (i',j')$ iff $i\leq i'$ and $j\leq j'$. Suppose that for all $(i,j)\leq (i',j')$ we are given homomorphisms $f_{ij,i'j'}:G_{i'j'}\rightarrow G_{ij}$ satisfying the usual transitivity conditions. How to prove that there are canonical isomorphisms : $\varprojlim_{i}\varprojlim_{j}G_{ij}\sim \varprojlim_{(i,j)}G_{ij}\sim \varprojlim_{j}\varprojlim_{i}G_{ij}$? Thank you.",,"['abstract-algebra', 'category-theory']"
60,"An abelian tower of a finite group admits a cyclic refinement — Proposition I.3.1, Lang's 'Algebra'","An abelian tower of a finite group admits a cyclic refinement — Proposition I.3.1, Lang's 'Algebra'",,"Recently I've been looking through Lang's Algebra , and I encountered a problem in the proof of Proposition 3.1 in Chapter I Groups. Proposition 3.1. Let $G$ be a finite group. An abelian tower of $G$ admits a cyclic refinement. Let $G$ be a finite solvable group. Then $G$ admits a cyclic tower, whose last element is $\{e\}$ . Proof. The second assertion is an immediate consequence of the first, and it clearly suffices to prove that if $G$ is finite, abelian, then $G$ admits a cyclic tower. We use induction on the order of $G$ . Let $x$ be an element of $G$ . We may assume that $x \neq e$ . Let $X$ be the cyclic group generated by $x$ . Let $G' = G/X$ . By induction, we can find a cyclic tower in $G'$ , and its inverse image is a cyclic tower in $G$ whose last element is $X$ . If we refine this tower by inserting $\{e\}$ at the end, we obtain the desired cyclic tower. I don't understand why it suffices to prove that if $G$ is finite, abelian, then $G$ admits a cyclic tower. In the statement of the proposition $G$ is not assumed to be an abelian group. Moroever, even assuming that we do prove that if $G$ is finite, abelian, then $G$ admits a cyclic tower, I don't see how can we use this in proving Proposition 3.1. Maybe this question is very easy, but currently I can't understand it. Any help would be appreciated.","Recently I've been looking through Lang's Algebra , and I encountered a problem in the proof of Proposition 3.1 in Chapter I Groups. Proposition 3.1. Let be a finite group. An abelian tower of admits a cyclic refinement. Let be a finite solvable group. Then admits a cyclic tower, whose last element is . Proof. The second assertion is an immediate consequence of the first, and it clearly suffices to prove that if is finite, abelian, then admits a cyclic tower. We use induction on the order of . Let be an element of . We may assume that . Let be the cyclic group generated by . Let . By induction, we can find a cyclic tower in , and its inverse image is a cyclic tower in whose last element is . If we refine this tower by inserting at the end, we obtain the desired cyclic tower. I don't understand why it suffices to prove that if is finite, abelian, then admits a cyclic tower. In the statement of the proposition is not assumed to be an abelian group. Moroever, even assuming that we do prove that if is finite, abelian, then admits a cyclic tower, I don't see how can we use this in proving Proposition 3.1. Maybe this question is very easy, but currently I can't understand it. Any help would be appreciated.",G G G G \{e\} G G G x G x \neq e X x G' = G/X G' G X \{e\} G G G G G,"['abstract-algebra', 'group-theory', 'abelian-groups', 'cyclic-groups', 'solvable-groups']"
61,Product of a non-unit with any other element,Product of a non-unit with any other element,,"In any ring, is the product of a non-unit with any other element necessarily a non-unit?","In any ring, is the product of a non-unit with any other element necessarily a non-unit?",,"['abstract-algebra', 'ring-theory']"
62,$x\otimes 1\neq 1\otimes x$,,x\otimes 1\neq 1\otimes x,"In Bourbaki, Algèbre 5, section 5, one has $A$ and $B$ two $K$ -algebras in an extension $\Omega$ of $K$ . It is said that if the morphism $A\otimes_K B\to \Omega$ is injective then $A\cap B=K$ . I see the reason: if not there would exist $x\in A\cap B\setminus K$ so that $x\otimes 1=1\otimes x$ which is false. But why $1\otimes x\neq x\otimes 1$ for $x\notin K$ ?","In Bourbaki, Algèbre 5, section 5, one has and two -algebras in an extension of . It is said that if the morphism is injective then . I see the reason: if not there would exist so that which is false. But why for ?",A B K \Omega K A\otimes_K B\to \Omega A\cap B=K x\in A\cap B\setminus K x\otimes 1=1\otimes x 1\otimes x\neq x\otimes 1 x\notin K,"['abstract-algebra', 'tensor-products', 'extension-field']"
63,Decomposition of polynomial into irreducible polynomials,Decomposition of polynomial into irreducible polynomials,,I'm preparing to my algebra exam. And I have problem and I have no idea how to solve it. Given polynomial   $$x^4+4x^3+4x^2+1.$$   The task is find expansion of the polynomial as a product of irreducible polynomials in $\mathbb{R}$. I will be happy if you show me the way how to solve such problems,I'm preparing to my algebra exam. And I have problem and I have no idea how to solve it. Given polynomial   $$x^4+4x^3+4x^2+1.$$   The task is find expansion of the polynomial as a product of irreducible polynomials in $\mathbb{R}$. I will be happy if you show me the way how to solve such problems,,"['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
64,"If ideal quotients of a ring are isomorphic, are these ideals isomorphic?","If ideal quotients of a ring are isomorphic, are these ideals isomorphic?",,"Suppose that $R$ is a ring, $I$ and $J$ are ideals in $R,$ and $R/I\cong R/J$ as rings. When does $I\cong J$ as $R$-modules hold?","Suppose that $R$ is a ring, $I$ and $J$ are ideals in $R,$ and $R/I\cong R/J$ as rings. When does $I\cong J$ as $R$-modules hold?",,['abstract-algebra']
65,Rings' second isomorphism theorem,Rings' second isomorphism theorem,,"I am thinking about the proof of the second isomorphism theorem, and something isn't very clear to me. Let $R$ be a ring, $S\subset R$ a subring and $I\subset R$ an ideal. We have the natural homomorphism $f:R\rightarrow R/I$. The theorem states that $\mathrm{Im}(S)=(S+I)/I$. My question is why not simply $\mathrm{Im}(S)=S/I$? I understand that it is not true (for starters, $S/I$ need not be a subring), but I cannot explain that to my self in a convincing way.","I am thinking about the proof of the second isomorphism theorem, and something isn't very clear to me. Let $R$ be a ring, $S\subset R$ a subring and $I\subset R$ an ideal. We have the natural homomorphism $f:R\rightarrow R/I$. The theorem states that $\mathrm{Im}(S)=(S+I)/I$. My question is why not simply $\mathrm{Im}(S)=S/I$? I understand that it is not true (for starters, $S/I$ need not be a subring), but I cannot explain that to my self in a convincing way.",,"['abstract-algebra', 'ring-theory', 'soft-question']"
66,Variety generated by finite fields,Variety generated by finite fields,,"Let $K_1,\dotsc,K_n$ be finite fields and let $V$ be the variety of rings, generated by the $K_i$ (rings aren't necessarily unital). I want to figure out what $V$ looks like. By a theorem of Tarski, elements of $V$ are the quotients of subrings of direct products of (possibly infinite) families of the $K_i$. But what are these rings exactly? One thing we can figure out are the possible characteristics of the elements of $V$. Since taking subrings and quotients decreases the characteristic and the characteristic of the product is the least common multiple of the characteristics of the factors, any ring in $V$ has a characteristic which is a squarefree integer, whose prime factors are among the characteristics of the $K_i$. On the other hand, not every such ring lives in $V$. For example, the multiplicative semigroup of any ring in $V$ must have finite exponent (since this is true for the products of the $K_i$), which means that things like polynomial rings over $K_i$ can't appear in $V$. I tried looking at the simplest case where $V$ is generated by $\mathbb{Z}_2$, but I can't really picture what's going on. I have a feeling that in this case $V$ will be the class of Boolean rings, but I'm not even sure how to show this.","Let $K_1,\dotsc,K_n$ be finite fields and let $V$ be the variety of rings, generated by the $K_i$ (rings aren't necessarily unital). I want to figure out what $V$ looks like. By a theorem of Tarski, elements of $V$ are the quotients of subrings of direct products of (possibly infinite) families of the $K_i$. But what are these rings exactly? One thing we can figure out are the possible characteristics of the elements of $V$. Since taking subrings and quotients decreases the characteristic and the characteristic of the product is the least common multiple of the characteristics of the factors, any ring in $V$ has a characteristic which is a squarefree integer, whose prime factors are among the characteristics of the $K_i$. On the other hand, not every such ring lives in $V$. For example, the multiplicative semigroup of any ring in $V$ must have finite exponent (since this is true for the products of the $K_i$), which means that things like polynomial rings over $K_i$ can't appear in $V$. I tried looking at the simplest case where $V$ is generated by $\mathbb{Z}_2$, but I can't really picture what's going on. I have a feeling that in this case $V$ will be the class of Boolean rings, but I'm not even sure how to show this.",,"['abstract-algebra', 'universal-algebra']"
67,Intersection of subgroups of orders 3 and 5 is the identity,Intersection of subgroups of orders 3 and 5 is the identity,,"This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is from assignment 5. Let $H, K$ be subgroups of a group $G$ of orders 3,5 respectively. Prove that $H\cap K=\{1\}$. The order of each element of $H$ must divide 3. Since the identity element is the only element with order 1, every other element in $H$ has order 3. Similar reasoning shows every nonidentity element of $K$ has order 5. Since $H$ and $K$ are both subgroups of $G$ they share the same identity element. Therefore, $H\cap K =\{1\}$. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.","This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is from assignment 5. Let $H, K$ be subgroups of a group $G$ of orders 3,5 respectively. Prove that $H\cap K=\{1\}$. The order of each element of $H$ must divide 3. Since the identity element is the only element with order 1, every other element in $H$ has order 3. Similar reasoning shows every nonidentity element of $K$ has order 5. Since $H$ and $K$ are both subgroups of $G$ they share the same identity element. Therefore, $H\cap K =\{1\}$. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.",,"['abstract-algebra', 'group-theory']"
68,Terminology: A homomorphism factors,Terminology: A homomorphism factors,,"I've never heard the term, a homomorphism ""factors"" before, and it's on my current assignment, so I was hoping someone could explain. The problem: Let $\pi:G\rightarrow G/G'$ be the canonical homomorphism and let $A$ be an abelian group. Show that every group homomorphism $\phi:G\rightarrow A$ factors as $\phi = \phi'\circ\pi$ where $\phi':G/G'\rightarrow A/A'$ is the induced group homomorphism. (Where $G'$ is the commutator subgroup of $G$.) So far, what I've poked around with... As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e_A\}$ and $A/A' \cong A$. Thus if we let $\phi:G\to A$ be a group homomorphism, for every $g,h\in G$, we must have $\phi(g)\phi(h) = \phi(gh)$. Now, as $A$ is abelian, we must also have $\phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h) = e_A$. But using the fact that $\phi$ is a homomorphism again, we have $\phi(g^{-1}h^{-1}gh) = e_A$, and hence, every element of $G'$ is mapped by $\phi$ to $e_A$. Thus, if we have any element $g\in G$, $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)A'$, which is simply $\{\phi(g)\}$. Is this what the question was asking for? Thanks! Edit: New version, As it's always a good idea, we start by showing the map $\phi':G/G'\to A/A'$ given by $\phi'(hG') = \phi(h)A'$ is well defined. For any $g,h\in G$ we note $\phi(g^{-1}h^{-1}gh) = \phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h)\in A'$. So $\phi(G')\subset A'$. Now, if we have two elements $h_1,h_2\in G$ such that $[h_1] = [h_2]$, then by definition, we have $h_1 = h_2c$ for some $c\in G'$. So $\phi(h_1) = \phi(h_2c) = \phi(h_2)\phi(c)\in \phi(h_2)A'$. Hence $[\phi(h_1)] = [\phi(h_2)]$ in $A/A'$, and $\phi'$ is well defined. To see that $\phi'$ is indeed a group homomorphism, let $g,h\in G$. Then  \begin{align*}  \phi'([g][h]) &= \phi'([gh])\newline                &= \phi(gh)A'\newline                &= \phi(g)\phi(h)A'\newline                &= (\phi(g)A')(\phi(h)A') = \phi'([g])\phi'([h]). \end{align*} As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e\}$ and $A/A' \cong A$. So although $\phi'$ maps to $A/A'$, we may simply say $\phi'$ maps to $A$ in the obvious way, so from here we say $\phi'([h]) = \phi(h)$ for any $h\in G$. Given this, for any element $g\in G$, we have $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)$, and so any group homomorphism $\phi:G\to A$ factors as $\phi'\circ\pi$.","I've never heard the term, a homomorphism ""factors"" before, and it's on my current assignment, so I was hoping someone could explain. The problem: Let $\pi:G\rightarrow G/G'$ be the canonical homomorphism and let $A$ be an abelian group. Show that every group homomorphism $\phi:G\rightarrow A$ factors as $\phi = \phi'\circ\pi$ where $\phi':G/G'\rightarrow A/A'$ is the induced group homomorphism. (Where $G'$ is the commutator subgroup of $G$.) So far, what I've poked around with... As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e_A\}$ and $A/A' \cong A$. Thus if we let $\phi:G\to A$ be a group homomorphism, for every $g,h\in G$, we must have $\phi(g)\phi(h) = \phi(gh)$. Now, as $A$ is abelian, we must also have $\phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h) = e_A$. But using the fact that $\phi$ is a homomorphism again, we have $\phi(g^{-1}h^{-1}gh) = e_A$, and hence, every element of $G'$ is mapped by $\phi$ to $e_A$. Thus, if we have any element $g\in G$, $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)A'$, which is simply $\{\phi(g)\}$. Is this what the question was asking for? Thanks! Edit: New version, As it's always a good idea, we start by showing the map $\phi':G/G'\to A/A'$ given by $\phi'(hG') = \phi(h)A'$ is well defined. For any $g,h\in G$ we note $\phi(g^{-1}h^{-1}gh) = \phi(g)^{-1}\phi(h)^{-1}\phi(g)\phi(h)\in A'$. So $\phi(G')\subset A'$. Now, if we have two elements $h_1,h_2\in G$ such that $[h_1] = [h_2]$, then by definition, we have $h_1 = h_2c$ for some $c\in G'$. So $\phi(h_1) = \phi(h_2c) = \phi(h_2)\phi(c)\in \phi(h_2)A'$. Hence $[\phi(h_1)] = [\phi(h_2)]$ in $A/A'$, and $\phi'$ is well defined. To see that $\phi'$ is indeed a group homomorphism, let $g,h\in G$. Then  \begin{align*}  \phi'([g][h]) &= \phi'([gh])\newline                &= \phi(gh)A'\newline                &= \phi(g)\phi(h)A'\newline                &= (\phi(g)A')(\phi(h)A') = \phi'([g])\phi'([h]). \end{align*} As $A$ is abelian, we note for any elements $a_1,a_2\in A$, $a_1^{-1}a_2^{-1}a_1a_2 = e_A$. So $A' = \{e\}$ and $A/A' \cong A$. So although $\phi'$ maps to $A/A'$, we may simply say $\phi'$ maps to $A$ in the obvious way, so from here we say $\phi'([h]) = \phi(h)$ for any $h\in G$. Given this, for any element $g\in G$, we have $\phi'\circ\pi(g) = \phi'(gG') = \phi(g)$, and so any group homomorphism $\phi:G\to A$ factors as $\phi'\circ\pi$.",,"['abstract-algebra', 'group-theory']"
69,Induced homomorphism by passing to the quotient,Induced homomorphism by passing to the quotient,,Let $G_1$ and $G_2$ be two abelian groups with respective subgroups $N_1$ and $N_2$. and let  $f:G_1 \to G_2$ be a homomorphism. Under what conditions is  the induced map $f':G_1/N_1 \to G_2/N_2$ well defined? I think the only thing to verify is that given any $g_1\in N_1$ we must have $f(g_1)\in N_2$. is there any other condition?,Let $G_1$ and $G_2$ be two abelian groups with respective subgroups $N_1$ and $N_2$. and let  $f:G_1 \to G_2$ be a homomorphism. Under what conditions is  the induced map $f':G_1/N_1 \to G_2/N_2$ well defined? I think the only thing to verify is that given any $g_1\in N_1$ we must have $f(g_1)\in N_2$. is there any other condition?,,"['abstract-algebra', 'group-theory']"
70,On the isomorphism $\mathbb{R} \cong \mathbb{R}^2$,On the isomorphism,\mathbb{R} \cong \mathbb{R}^2,"It is a well known pathological counterexample in group theory that (assuming the axiom of choice), we have an isomorphism $$\mathbb{R} \cong \mathbb{R}^2,$$ where these are additive groups. This somewhat unsettling result generally is shown by appealing to fact that both of these groups, when viewed as $\mathbb{Q}$ -vector spaces, have the same dimension. Naturally, showing this relies on the axiom of choice. As with most results coming from the axiom of choice, it is strongly insinuated that one cannot explicitly construct an isomorphism $\phi : \mathbb{R} \rightarrow \mathbb{R}^2$ . This is very believable, but I'm wondering if it has actually been shown that this fact is equivalent to choice (i.e. without choice, the two are not isomorphic). If so, what would a proof of that look like? All comments are appreciated.","It is a well known pathological counterexample in group theory that (assuming the axiom of choice), we have an isomorphism where these are additive groups. This somewhat unsettling result generally is shown by appealing to fact that both of these groups, when viewed as -vector spaces, have the same dimension. Naturally, showing this relies on the axiom of choice. As with most results coming from the axiom of choice, it is strongly insinuated that one cannot explicitly construct an isomorphism . This is very believable, but I'm wondering if it has actually been shown that this fact is equivalent to choice (i.e. without choice, the two are not isomorphic). If so, what would a proof of that look like? All comments are appreciated.","\mathbb{R} \cong \mathbb{R}^2, \mathbb{Q} \phi : \mathbb{R} \rightarrow \mathbb{R}^2","['abstract-algebra', 'group-theory', 'axiom-of-choice']"
71,Which lattices can be represented as the lattice of closed sets in a topological space?,Which lattices can be represented as the lattice of closed sets in a topological space?,,"I sometimes come across families of sets that are closed under arbitrary intersection , but have finite joins that are not unions, e.g. ideals deductively closed sets in some deductive system filters We can think of these things as lattices $\Lambda$ in the language $(\land, \lor)$ , with an additional second-order constraint promising us that infinite meets make sense. I'm curious under what circumstances we can represent one of these lattices as the lattice of closed sets on a topological space $(X, \tau)$ . I've tried a lot of things to try to build myself a point-set representation of an arbitrary (distributive) lattice (with a global maximum and minimum). Intuitively, filters and especially ultrafilters act a lot like element-elements in a family of sets. So, I tried making my point set $X$ the ultrafilters in $\Lambda$ (and declare the representation of a lattice element $a$ to be the set of ultrafilters containing it), since that respects $\land$ and $\lor$ but I immediately ran into problems. If we take the lattice of closed sets of the Sierpiński space , $\{ \{a, b\}, \{b\}, \varnothing \}$ , there's only one ultrafilter, so it collapses down to the two-element Boolean algebra. I also tried taking my lattice $\Lambda$ and adding a new formal operation the formal relative complement , $a \setminus b$ , to try to build myself a larger Boolean lattice for $\Lambda$ to embed into, and then take ultrafilters there. My intuition is breaking down though and I remember from looking at Boolean algebras previously that they can have few or no atoms or otherwise be really badly behaved. So, at this point, I'm taking a step and wondering which lattices can be represented as the lattices of closed sets in topological spaces in the first place.","I sometimes come across families of sets that are closed under arbitrary intersection , but have finite joins that are not unions, e.g. ideals deductively closed sets in some deductive system filters We can think of these things as lattices in the language , with an additional second-order constraint promising us that infinite meets make sense. I'm curious under what circumstances we can represent one of these lattices as the lattice of closed sets on a topological space . I've tried a lot of things to try to build myself a point-set representation of an arbitrary (distributive) lattice (with a global maximum and minimum). Intuitively, filters and especially ultrafilters act a lot like element-elements in a family of sets. So, I tried making my point set the ultrafilters in (and declare the representation of a lattice element to be the set of ultrafilters containing it), since that respects and but I immediately ran into problems. If we take the lattice of closed sets of the Sierpiński space , , there's only one ultrafilter, so it collapses down to the two-element Boolean algebra. I also tried taking my lattice and adding a new formal operation the formal relative complement , , to try to build myself a larger Boolean lattice for to embed into, and then take ultrafilters there. My intuition is breaking down though and I remember from looking at Boolean algebras previously that they can have few or no atoms or otherwise be really badly behaved. So, at this point, I'm taking a step and wondering which lattices can be represented as the lattices of closed sets in topological spaces in the first place.","\Lambda (\land, \lor) (X, \tau) X \Lambda a \land \lor \{ \{a, b\}, \{b\}, \varnothing \} \Lambda a \setminus b \Lambda","['abstract-algebra', 'general-topology']"
72,Subring of a commutative ring with unity implies ring is an integral domain,Subring of a commutative ring with unity implies ring is an integral domain,,"Let $R$ be a commutative ring with unity. If a subring $S$ of $R$ is an integral domain containing the unity of $R$ (that isn’t $\{0,1\}$ ), does this imply that $R$ is too an integral domain? I tried to find a counterexample but I did not find one: I figured maybe the subring of $Z_{10}$ , $(2)$ , would work since I figured it would be isomorphic to $Z_5$ (and thus an integral domain), but then I realized it has no multiplicative identity so that’s a no. Any help would be appreciated.","Let be a commutative ring with unity. If a subring of is an integral domain containing the unity of (that isn’t ), does this imply that is too an integral domain? I tried to find a counterexample but I did not find one: I figured maybe the subring of , , would work since I figured it would be isomorphic to (and thus an integral domain), but then I realized it has no multiplicative identity so that’s a no. Any help would be appreciated.","R S R R \{0,1\} R Z_{10} (2) Z_5","['abstract-algebra', 'ring-theory', 'integral-domain']"
73,When does a system of polynomial equations have infinitely many solutions?,When does a system of polynomial equations have infinitely many solutions?,,"It seems intuitively correct to say that a system of polynomial equations has finitely many solutions if there are as many equations as there are variables in the system. However, how would you prove that a system of polynomial equations with fewer equations than variables has infinitely many solutions (corresponding to the ""under-determined"" case in linear systems)? How would you characterize when a subset of equations ""coincide"" (you can combine several to get another, like linear dependence in linear systems)? For context, I'm in the middle of an abstract algebra course, and we've just finished Chapter 7 of Dummit & Foote (though this question isn't yet directly related to the course, that's just my background knowledge).","It seems intuitively correct to say that a system of polynomial equations has finitely many solutions if there are as many equations as there are variables in the system. However, how would you prove that a system of polynomial equations with fewer equations than variables has infinitely many solutions (corresponding to the ""under-determined"" case in linear systems)? How would you characterize when a subset of equations ""coincide"" (you can combine several to get another, like linear dependence in linear systems)? For context, I'm in the middle of an abstract algebra course, and we've just finished Chapter 7 of Dummit & Foote (though this question isn't yet directly related to the course, that's just my background knowledge).",,"['abstract-algebra', 'algebraic-geometry', 'polynomials', 'systems-of-equations', 'computer-algebra-systems']"
74,Defining loops: why is divisibility and identitiy implying invertibility?,Defining loops: why is divisibility and identitiy implying invertibility?,,"Wikipedia contains the following figure (to be found, e.g. here ) in order to visualize the relations between several algebraic structures. I highlighted a part that I find especially interesting. It seems to suggest that a loop can be defined either as a magma with identity and invertibility, or as a magma with identity and divisibility. It is easy to see that from 1. follows 2. But I have trouble proving the converse. Here is a proof for the case that the loop is finite: By divisibility $xa=xb\implies a=b$ for all $x,a,b\in L$ . Thus, the map $a\mapsto xa$ is injective. If the loop is finite, the map is also surjective. Thus, there is a $x'\in L$ with $xx'=e$ . This $x'$ is therefore the inverse of $x$ . But what if $L$ is not finite?","Wikipedia contains the following figure (to be found, e.g. here ) in order to visualize the relations between several algebraic structures. I highlighted a part that I find especially interesting. It seems to suggest that a loop can be defined either as a magma with identity and invertibility, or as a magma with identity and divisibility. It is easy to see that from 1. follows 2. But I have trouble proving the converse. Here is a proof for the case that the loop is finite: By divisibility for all . Thus, the map is injective. If the loop is finite, the map is also surjective. Thus, there is a with . This is therefore the inverse of . But what if is not finite?","xa=xb\implies a=b x,a,b\in L a\mapsto xa x'\in L xx'=e x' x L","['abstract-algebra', 'group-theory', 'inverse', 'magma', 'quasigroups']"
75,Determine if a number is algebraic.,Determine if a number is algebraic.,,"I recall there is an algorithm involving lattices that would tell me if a given number(approximation) is an algebraic number or not, along with the exact algebraic form of the number. Is there any tool (Gap, Mathematica, python package, etc) that has an implementation of this? I have numbers which I can approximate to any degree of accuracy I want and I would like to know if they are algebraic or not.","I recall there is an algorithm involving lattices that would tell me if a given number(approximation) is an algebraic number or not, along with the exact algebraic form of the number. Is there any tool (Gap, Mathematica, python package, etc) that has an implementation of this? I have numbers which I can approximate to any degree of accuracy I want and I would like to know if they are algebraic or not.",,"['abstract-algebra', 'algebraic-number-theory', 'mathematica', 'gap', 'python']"
76,coset of a group,coset of a group,,Assume that $G$ is an abelian group and $A$ is a subset of $G$ such that $|A+A|=|A|$ then $A$ is a coset of a subgroup. I tried that if $0  \in  A$ then $A+A =A$ but I can't go any further. Could someone help me? sorry if my English isn't well.,Assume that is an abelian group and is a subset of such that then is a coset of a subgroup. I tried that if then but I can't go any further. Could someone help me? sorry if my English isn't well.,G A G |A+A|=|A| A 0  \in  A A+A =A,"['abstract-algebra', 'group-theory']"
77,Program to find intersection of subgroups of free groups,Program to find intersection of subgroups of free groups,,"As the title says, I am working on examples for a research project I'm doing, and I need a way to efficiently calculate the intersection of subgroups of a free group (say, of rank 2). Are there any computer programs to do this, or any papers explaining how such a program could be written?","As the title says, I am working on examples for a research project I'm doing, and I need a way to efficiently calculate the intersection of subgroups of a free group (say, of rank 2). Are there any computer programs to do this, or any papers explaining how such a program could be written?",,"['abstract-algebra', 'group-theory', 'free-groups', 'geometric-group-theory', 'computational-algebra']"
78,Subgroup of a finite group $H$ and $G$,Subgroup of a finite group  and,H G,"I regret to admit that this has been confusing me for much longer than I would like. I just can't wrap my head around some parts of this question and I'd like some guidance. Let $G$ be a finite group, and let $S$ be a nonempty subset of $G$ . Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$ . (HINT: It remains to prove that $S$ contains $e$ and is closed with respect to inverses. Let $S$ = { $a_1$ ... $a_n$ }. If $a_i$ $∈$ $S$ , consider the distinct elements $a_ia_1$ , $a_ia_2$ , $...$ $a_ia_n$ I think if I can understand one tiny segment then I can complete my proof. I know I need to show $e$ $∈$ $S$ and that $S$ is closed w.r.t inverses, but I can't get across a tiny step in proving $e$ $∈$ $S$ . The proof I'm trying to figure out goes something like: Since $S$ is finite and closed under the group operation, then we have $a_1$ = $a_1a_k$ for some $a_k$ $∈$ $S$ . Then from this we can find that $a_k$ = $e$ . My question is how we know that we have $a_1$ = $a_1a_k$ for some $a_k$ $∈$ $S$ . I have no idea why this would be the case. I've also seen another post that used a map from $S$ $\rightarrow$ $S$ defined by left multiplication by $a_1$ . The map is one-to-one and since $S$ is finite, it is injective as well. I understand this, but then the poster goes on to say that this somehow shows that we have $a_1s$ = $a_1$ for some $s$ $∈$ $S$ . I have no idea why this would be the case either. If I can figure this out I think I can do the rest of the proof. Thanks in advance.","I regret to admit that this has been confusing me for much longer than I would like. I just can't wrap my head around some parts of this question and I'd like some guidance. Let be a finite group, and let be a nonempty subset of . Suppose is closed with respect to multiplication. Prove that is a subgroup of . (HINT: It remains to prove that contains and is closed with respect to inverses. Let = { ... }. If , consider the distinct elements , , I think if I can understand one tiny segment then I can complete my proof. I know I need to show and that is closed w.r.t inverses, but I can't get across a tiny step in proving . The proof I'm trying to figure out goes something like: Since is finite and closed under the group operation, then we have = for some . Then from this we can find that = . My question is how we know that we have = for some . I have no idea why this would be the case. I've also seen another post that used a map from defined by left multiplication by . The map is one-to-one and since is finite, it is injective as well. I understand this, but then the poster goes on to say that this somehow shows that we have = for some . I have no idea why this would be the case either. If I can figure this out I think I can do the rest of the proof. Thanks in advance.",G S G S S G S e S a_1 a_n a_i ∈ S a_ia_1 a_ia_2 ... a_ia_n e ∈ S S e ∈ S S a_1 a_1a_k a_k ∈ S a_k e a_1 a_1a_k a_k ∈ S S \rightarrow S a_1 S a_1s a_1 s ∈ S,"['abstract-algebra', 'group-theory', 'elementary-set-theory', 'finite-groups']"
79,$\Lambda = \varprojlim\Lambda_n$ (ring of symmetric functions),(ring of symmetric functions),\Lambda = \varprojlim\Lambda_n,"This question is related to this other question. When understanding how it is defined the ring of symmetric functions, I can not see why is so much important to take the inverse limit in the category of graded rings. MY WORK Consider $\Lambda$ to be the ring of symmetric functions. $\Lambda_n$ to be the symmetric polynomials in $n$ independent variables. Moreover, I know that in the category of rings, the objects are rings and the arrows are ring homomorphisms. I also know that in the category of graded rings, the objects are rings and the arrows are graded rings homomorphisms. I.e. if $f:R\to S$ is a ring homomorphisms. A graded ring homomorphisms is $f$ such that $f(R)\subseteq S$ . Then, in the category of graded rings, $$\Lambda = \varprojlim\Lambda_n = \left\{a \in \prod_{i\in I}\Lambda_i \mathrel{\Bigg|}  \forall i \leq j:  f_{i,j}(a_j)=a_i \right\}$$ In the category of rings, $$\Lambda ^* = \varprojlim\Lambda_n = \left\{a \in \prod_{i\in I}\Lambda_i \mathrel{\Bigg|} \forall i \leq j: f_{i,j}(a_j)=a_i  \right\}$$ And $\Lambda \subset \Lambda^*$ (my teacher told me). But I can not see what makes the difference in considering the inverse limit in these two different categories. I can not see how it affects the arrows in the categories to these sets. Any help?","This question is related to this other question. When understanding how it is defined the ring of symmetric functions, I can not see why is so much important to take the inverse limit in the category of graded rings. MY WORK Consider to be the ring of symmetric functions. to be the symmetric polynomials in independent variables. Moreover, I know that in the category of rings, the objects are rings and the arrows are ring homomorphisms. I also know that in the category of graded rings, the objects are rings and the arrows are graded rings homomorphisms. I.e. if is a ring homomorphisms. A graded ring homomorphisms is such that . Then, in the category of graded rings, In the category of rings, And (my teacher told me). But I can not see what makes the difference in considering the inverse limit in these two different categories. I can not see how it affects the arrows in the categories to these sets. Any help?","\Lambda \Lambda_n n f:R\to S f f(R)\subseteq S \Lambda = \varprojlim\Lambda_n = \left\{a \in \prod_{i\in I}\Lambda_i \mathrel{\Bigg|}  \forall i \leq j:  f_{i,j}(a_j)=a_i \right\} \Lambda ^* = \varprojlim\Lambda_n = \left\{a \in \prod_{i\in I}\Lambda_i \mathrel{\Bigg|} \forall i \leq j: f_{i,j}(a_j)=a_i  \right\} \Lambda \subset \Lambda^*","['abstract-algebra', 'symmetric-polynomials', 'limits-colimits', 'symmetric-functions']"
80,List of small rings,List of small rings,,"Where can I find a list of small rings? (like the one Wikipedia has for groups ) (obviously I  don't expect it to be as comprehensive given how many rings there can be for certain orders, but if there's a list out there that at least covers a fair few of the easier sizes of a ring to deal with, that'd be great)","Where can I find a list of small rings? (like the one Wikipedia has for groups ) (obviously I  don't expect it to be as comprehensive given how many rings there can be for certain orders, but if there's a list out there that at least covers a fair few of the easier sizes of a ring to deal with, that'd be great)",,"['abstract-algebra', 'reference-request', 'ring-theory']"
81,Is there a simple abelian group $G$ with infinite order?,Is there a simple abelian group  with infinite order?,G,"I am reading ""An Introduction to Algebraic Systems"" by Kazuo Matsuzaka. There is the following problem in this book: On p.80 Problem 8: Show that a simple abelian group $G \neq \{e\}$ is a cyclic group whose order is prime. Did the author intend the following problem? If this problem were the following, I could solve it: Problem 8': Show that a simple finite abelian group $G \neq \{e\}$ is a cyclic group whose order is prime. Proof: Because $G \neq \{e\}$ , there is an element $g \in G$ which is not equal to $e$ . $H := \{g^i | i \in \mathbb{Z}\}$ is a subgroup of $G$ and $G$ is abelian. So $H$ is a normal subgroup of $G$ . And $G$ is simple. And $H \ni g \ne e$ . So $H = G$ . Let $n := \#H = \#G$ . Then, $n$ is prime. If $n$ is not prime, then we can write $n = d d'$ , $1 < d < n$ , $1 < d' < n$ . Then $H' := \{(g^d)^i | i \in \mathbb{Z}\}$ is a subgroup of $G$ whose order is $d'$ . So, $H'$ is neither $\{e\}$ nor $G$ . Becasue $G$ is abelian, $H'$ is a normal subgroup of $G$ . And $G$ is simple. This is a contradiction.","I am reading ""An Introduction to Algebraic Systems"" by Kazuo Matsuzaka. There is the following problem in this book: On p.80 Problem 8: Show that a simple abelian group is a cyclic group whose order is prime. Did the author intend the following problem? If this problem were the following, I could solve it: Problem 8': Show that a simple finite abelian group is a cyclic group whose order is prime. Proof: Because , there is an element which is not equal to . is a subgroup of and is abelian. So is a normal subgroup of . And is simple. And . So . Let . Then, is prime. If is not prime, then we can write , , . Then is a subgroup of whose order is . So, is neither nor . Becasue is abelian, is a normal subgroup of . And is simple. This is a contradiction.",G \neq \{e\} G \neq \{e\} G \neq \{e\} g \in G e H := \{g^i | i \in \mathbb{Z}\} G G H G G H \ni g \ne e H = G n := \#H = \#G n n n = d d' 1 < d < n 1 < d' < n H' := \{(g^d)^i | i \in \mathbb{Z}\} G d' H' \{e\} G G H' G G,"['abstract-algebra', 'group-theory', 'simple-groups']"
82,"Every group homomorphism from $(\mathbb{Q}, +)$ to $(\mathbb{Q}, \times)$ is the trivial map.",Every group homomorphism from  to  is the trivial map.,"(\mathbb{Q}, +) (\mathbb{Q}, \times)","How do you show that every group homomorphism from $(\mathbb{Q}, +)$ to $(\mathbb{Q}, \times)$ is the trivial map? I am trying to use proof by contradiction by assuming there is an element $\frac{a}{b}$ such that some homomorphism $\phi$ has $\phi(\frac{a}{b}) = \frac{c}{d} \ne 1$ . But I cannot seem to deduce any contradictions from here. Maybe using direct proof is a better approach? Any help is appreciated.",How do you show that every group homomorphism from to is the trivial map? I am trying to use proof by contradiction by assuming there is an element such that some homomorphism has . But I cannot seem to deduce any contradictions from here. Maybe using direct proof is a better approach? Any help is appreciated.,"(\mathbb{Q}, +) (\mathbb{Q}, \times) \frac{a}{b} \phi \phi(\frac{a}{b}) = \frac{c}{d} \ne 1","['abstract-algebra', 'group-theory']"
83,Concrete cases where $YX=qXY$,Concrete cases where,YX=qXY,"I was reading Kassel on the quantum plane and he defines an $R$-point on this plane as a pair of $X$, and $Y$ elements of the non commutative algebra $R$ such that $$YX=qXY,$$ with $q$ invertible. Can anyone give me a concrete example of such algebra $R$? Is there a matrix algebra that could fit this example? Thank you in advance Edit. I found that if we take R as the Heisenberg Algebra then $$X=\left(\begin{array}{ccc} 0 & a & b\\ 0 & 0 & 1\\ 0 & 0 & 0 \end{array}\right),\,Y=\left(\begin{array}{ccc} 0 & a & c\\ 0 & 0 & 1/q\\ 0 & 0 & 0 \end{array}\right),$$ is an $R$-point on the quantum plane. If you have any other concrete example, please write :)","I was reading Kassel on the quantum plane and he defines an $R$-point on this plane as a pair of $X$, and $Y$ elements of the non commutative algebra $R$ such that $$YX=qXY,$$ with $q$ invertible. Can anyone give me a concrete example of such algebra $R$? Is there a matrix algebra that could fit this example? Thank you in advance Edit. I found that if we take R as the Heisenberg Algebra then $$X=\left(\begin{array}{ccc} 0 & a & b\\ 0 & 0 & 1\\ 0 & 0 & 0 \end{array}\right),\,Y=\left(\begin{array}{ccc} 0 & a & c\\ 0 & 0 & 1/q\\ 0 & 0 & 0 \end{array}\right),$$ is an $R$-point on the quantum plane. If you have any other concrete example, please write :)",,"['abstract-algebra', 'matrices', 'hopf-algebras', 'quantum-groups']"
84,"Inverse of a matrix belonging to $GL(2,\mathbb{Z}_{11})$",Inverse of a matrix belonging to,"GL(2,\mathbb{Z}_{11})","$GL(2,\mathbb{Z}_{11})$ is a general linear group whose elements are $2\times 2$ matrices where every element of matrices belongs to the set $\mathbb{Z}_{11}$ (so modulo 11 arithmetic would take place) and determinant should non-zero for matrices. So I have been given a matrix  $$ A = \begin{pmatrix} 2 & 6 \\  3 & 5  \end{pmatrix}, $$ where A belongs to $GL(2,\mathbb{Z}_{11})$ and I have to find it's inverse. So what confused me is the way the inverse would be solved ? What is process to find it's inverse ?","$GL(2,\mathbb{Z}_{11})$ is a general linear group whose elements are $2\times 2$ matrices where every element of matrices belongs to the set $\mathbb{Z}_{11}$ (so modulo 11 arithmetic would take place) and determinant should non-zero for matrices. So I have been given a matrix  $$ A = \begin{pmatrix} 2 & 6 \\  3 & 5  \end{pmatrix}, $$ where A belongs to $GL(2,\mathbb{Z}_{11})$ and I have to find it's inverse. So what confused me is the way the inverse would be solved ? What is process to find it's inverse ?",,"['abstract-algebra', 'group-theory']"
85,$X^4-4X^2-1$ irreducible over $\mathbb{Q}[X]$,irreducible over,X^4-4X^2-1 \mathbb{Q}[X],"I want to show, that $X^4-4X^2-1$ is irreducible over $\mathbb{Q}[X]$. Since there are no roots in $\mathbb{Q}$ it has to be: $(X^4-4X^2-1)=(X^2+aX+b)(X^2+cX+d)$ Comparision of the coefficients shows that this can not hold over $\mathbb{Q}$. But I am searching for an easier way to show this which uses less calculation. I tried this: $X^4-4X^2-1=X^4-4X^2+4-5=(X^2-2)^2-5=(X^2-2-\sqrt{5})(X^2-2+\sqrt{5})$ Which is obviously not in $\mathbb{Q}[X]$ anymore. But would this calculation be enough to show, that $X^4-4X^2-1$ is irreducible. How do I know, that this is the only possible way to factor it? Thanks in advance.","I want to show, that $X^4-4X^2-1$ is irreducible over $\mathbb{Q}[X]$. Since there are no roots in $\mathbb{Q}$ it has to be: $(X^4-4X^2-1)=(X^2+aX+b)(X^2+cX+d)$ Comparision of the coefficients shows that this can not hold over $\mathbb{Q}$. But I am searching for an easier way to show this which uses less calculation. I tried this: $X^4-4X^2-1=X^4-4X^2+4-5=(X^2-2)^2-5=(X^2-2-\sqrt{5})(X^2-2+\sqrt{5})$ Which is obviously not in $\mathbb{Q}[X]$ anymore. But would this calculation be enough to show, that $X^4-4X^2-1$ is irreducible. How do I know, that this is the only possible way to factor it? Thanks in advance.",,"['abstract-algebra', 'polynomials', 'field-theory', 'irreducible-polynomials']"
86,"Let $F$ be a field of characteristic not $2$, and let $K$ be an extension of $F$ with $[K: F] = 2$. Show that $K = F (\sqrt{a})$ for some $a\in F$","Let  be a field of characteristic not , and let  be an extension of  with . Show that  for some",F 2 K F [K: F] = 2 K = F (\sqrt{a}) a\in F,"Let $F$ be a field of characteristic not $2$, and let $K$ be an extension of $F$ with $[K: F] = 2$. Show that $K = F (\sqrt{a})$ for some $a\in F$; that is, show that $K = F(\alpha)$ with $\alpha^2= a\in F$. Moreover, show that $K$ is Galois over $F$. I do not know how to try the first part, could someone help me please? For the second part I am using the following theorem: Let $K$ be a finite extension of $F$. Then $K/F$ is Galois if and only if $|Gal(K/F)| = [K:F]$ Because $Gal(K/F)=\left \{ id,\sigma  \right \}  $ where $\sigma$ is such that $\sigma(\sqrt{a})=-\sqrt{a}$, then $K$ is Galois over $F$.","Let $F$ be a field of characteristic not $2$, and let $K$ be an extension of $F$ with $[K: F] = 2$. Show that $K = F (\sqrt{a})$ for some $a\in F$; that is, show that $K = F(\alpha)$ with $\alpha^2= a\in F$. Moreover, show that $K$ is Galois over $F$. I do not know how to try the first part, could someone help me please? For the second part I am using the following theorem: Let $K$ be a finite extension of $F$. Then $K/F$ is Galois if and only if $|Gal(K/F)| = [K:F]$ Because $Gal(K/F)=\left \{ id,\sigma  \right \}  $ where $\sigma$ is such that $\sigma(\sqrt{a})=-\sqrt{a}$, then $K$ is Galois over $F$.",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
87,"Why is the arbitrary sum, but not the arbitrary intersection, of ideals an ideal?","Why is the arbitrary sum, but not the arbitrary intersection, of ideals an ideal?",,"Two Questions: What is the definition of an arbitrary sum of ideals? (I.e. arbitrarily infinitely many) Why is the arbitrary sum of ideals an ideal, but not the arbitrary intersection of ideals? My progress so far: My textbook doesn't have a definition, and I haven't been able to find the definition using Google so far. I have two guesses (i) it is the set of all infinite sums, or (ii) the same as the previous except with the additional condition that cofinitely many summands in each sum be equal to zero. The former seems like the ""natural"" extension of the definition of sum to an arbitrary index set of ideals, but the latter seems like the ""convenient"" definition, and I'm not sure which goal (naturalness or convenience) should take priority. I would expect that the arbitrary intersection of ideals is an ideal, because the ideal generated by any set is the arbitrary intersection of all ideals containing that set, and the radical of an ideal is the arbitrary intersection of all prime ideals containing the ideal. Meanwhile, it isn't clear to me that the infinite sum of elements of a ring should even be defined. Usually one has that the product of ideals is included in the intersection, and that usually only the finite product of ideals is an ideal, so in the exceptional cases where the product of ideals equals the intersection of ideals, I can somewhat see how the intersection wouldn't be an ideal. But there are at least two problems with this: if only the finite product of ideals is an ideal, then why isn't the same true for sums of ideals? Why is the multiplication operation in the ring being given a privileged position compared to the addition operation? Second, if the product of ideals equals the intersection of ideals in a certain ring, then why doesn't that just allow one to define arbitrary products of ideals, instead of making it impossible to define anything besides finite intersections of ideals? It seems clear to me that I am fundamentally misunderstanding some aspect of the operations (addition, multiplication, intersections) of ideals. EDIT: http://planetmath.org/sumofideals Planetmath says that ""the sum of ideals is the smallest ideal of the ring containing all of those ideals"". So I guess my question could be thought of as -- why isn't the intersection of ideals the smallest ideal of the ring containing all of those ideals? For every other object I know of which is closed under intersections, it is the intersection which has this property. Do the sum and intersection coincide in some sense? Note: This question is so I can show that the Zariski topology on Spec(R) for a commutative ring with unit is a topology -- I have already shown that finite unions and intersections of Zariski closed sets are Zariski closed, see here , so now the question is how to proceed to the general case. In the finite case, the union of Zariski closed sets corresponds to the intersection of ideals, while the intersection of Zariski closed sets corresponds to the sum of ideals. I would expect the intersection of ideals to be arbitrarily extensible, and the sums to be only finitely defined, but that would make the Zariski closed sets the open sets of a topology, which is clearly incorrect.","Two Questions: What is the definition of an arbitrary sum of ideals? (I.e. arbitrarily infinitely many) Why is the arbitrary sum of ideals an ideal, but not the arbitrary intersection of ideals? My progress so far: My textbook doesn't have a definition, and I haven't been able to find the definition using Google so far. I have two guesses (i) it is the set of all infinite sums, or (ii) the same as the previous except with the additional condition that cofinitely many summands in each sum be equal to zero. The former seems like the ""natural"" extension of the definition of sum to an arbitrary index set of ideals, but the latter seems like the ""convenient"" definition, and I'm not sure which goal (naturalness or convenience) should take priority. I would expect that the arbitrary intersection of ideals is an ideal, because the ideal generated by any set is the arbitrary intersection of all ideals containing that set, and the radical of an ideal is the arbitrary intersection of all prime ideals containing the ideal. Meanwhile, it isn't clear to me that the infinite sum of elements of a ring should even be defined. Usually one has that the product of ideals is included in the intersection, and that usually only the finite product of ideals is an ideal, so in the exceptional cases where the product of ideals equals the intersection of ideals, I can somewhat see how the intersection wouldn't be an ideal. But there are at least two problems with this: if only the finite product of ideals is an ideal, then why isn't the same true for sums of ideals? Why is the multiplication operation in the ring being given a privileged position compared to the addition operation? Second, if the product of ideals equals the intersection of ideals in a certain ring, then why doesn't that just allow one to define arbitrary products of ideals, instead of making it impossible to define anything besides finite intersections of ideals? It seems clear to me that I am fundamentally misunderstanding some aspect of the operations (addition, multiplication, intersections) of ideals. EDIT: http://planetmath.org/sumofideals Planetmath says that ""the sum of ideals is the smallest ideal of the ring containing all of those ideals"". So I guess my question could be thought of as -- why isn't the intersection of ideals the smallest ideal of the ring containing all of those ideals? For every other object I know of which is closed under intersections, it is the intersection which has this property. Do the sum and intersection coincide in some sense? Note: This question is so I can show that the Zariski topology on Spec(R) for a commutative ring with unit is a topology -- I have already shown that finite unions and intersections of Zariski closed sets are Zariski closed, see here , so now the question is how to proceed to the general case. In the finite case, the union of Zariski closed sets corresponds to the intersection of ideals, while the intersection of Zariski closed sets corresponds to the sum of ideals. I would expect the intersection of ideals to be arbitrarily extensible, and the sums to be only finitely defined, but that would make the Zariski closed sets the open sets of a topology, which is clearly incorrect.",,"['abstract-algebra', 'commutative-algebra', 'definition', 'ideals']"
88,Show that $S$ is a group if and only if $aS=S=Sa$.,Show that  is a group if and only if .,S aS=S=Sa,"Let $S$ be a semigroup.  Show that $S$ is a group if and only if $aS=S=Sa$ for all $a\in S$. Since it is if and only statement, we have to show that if $S$ is a group then $ aS=S=Sa$, which I already know how to do. The other part: if you have $aS=S=Sa$, then prove that $S$ is a group. (NB: See the comments for the original thoughts on the problem.)","Let $S$ be a semigroup.  Show that $S$ is a group if and only if $aS=S=Sa$ for all $a\in S$. Since it is if and only statement, we have to show that if $S$ is a group then $ aS=S=Sa$, which I already know how to do. The other part: if you have $aS=S=Sa$, then prove that $S$ is a group. (NB: See the comments for the original thoughts on the problem.)",,"['abstract-algebra', 'group-theory', 'semigroups']"
89,Forming a polynomial to show that $\cos\left(\frac{2\pi}{17}\right)$ is algebraic,Forming a polynomial to show that  is algebraic,\cos\left(\frac{2\pi}{17}\right),I know if $x=e^{\frac{2\pi i}{17}}$ then $x^{17}=1$ and $\Re(x)=\cos\left(\frac{2\pi}{17}\right)$. But how do I form a polynomial which has root $\cos\left(\frac{2\pi}{17}\right)$. I know you can consider de Moivre's theorem and expand the LHS using binomial theorem but that will take a long time.,I know if $x=e^{\frac{2\pi i}{17}}$ then $x^{17}=1$ and $\Re(x)=\cos\left(\frac{2\pi}{17}\right)$. But how do I form a polynomial which has root $\cos\left(\frac{2\pi}{17}\right)$. I know you can consider de Moivre's theorem and expand the LHS using binomial theorem but that will take a long time.,,"['abstract-algebra', 'number-theory']"
90,"Uniqueness of elements in Klein Four and ""Klein Five"" group","Uniqueness of elements in Klein Four and ""Klein Five"" group",,"I had a question about the uniqueness of group elements. Let the Klein Four group be defined as the group generated by the elements ${1,a,b,c}$ such that $a^2=b^2=c^2=1$ and $ab=c$, $bc=a$, $ca=b$, and $1$ is the identity element. Let the ""Klein Five"" group be defined as the group generated by the elements ${1,a,b,c,d}$ such that $a^2=b^2=c^2=d^2=1$ and $ab=c$, $bc=d$, $cd=a$, $da=b$, and $1$ is the identity element. If I manipulate the symbols of the ""Klein Five"" group I defined above, I can show every element is equivalent to the identity. From $ab=c=ad$ I can see $a$ is the identity, from $bc=d=ba$ I can see $b$ is the identity, and so on. This gives that $a=b=c=d=1$. In some sense, this group doesn't seem to exist. I can't make a group such that $a \neq b \neq c \neq d \neq 1$ with the constraints above. How do I know that the same isn't true of the Klein Four group? How do I know there isn't some set of constraints that makes the group ""non-existent"" for unique elements in the same way as the ""Klein Five"" group? Any help would be appreciated!","I had a question about the uniqueness of group elements. Let the Klein Four group be defined as the group generated by the elements ${1,a,b,c}$ such that $a^2=b^2=c^2=1$ and $ab=c$, $bc=a$, $ca=b$, and $1$ is the identity element. Let the ""Klein Five"" group be defined as the group generated by the elements ${1,a,b,c,d}$ such that $a^2=b^2=c^2=d^2=1$ and $ab=c$, $bc=d$, $cd=a$, $da=b$, and $1$ is the identity element. If I manipulate the symbols of the ""Klein Five"" group I defined above, I can show every element is equivalent to the identity. From $ab=c=ad$ I can see $a$ is the identity, from $bc=d=ba$ I can see $b$ is the identity, and so on. This gives that $a=b=c=d=1$. In some sense, this group doesn't seem to exist. I can't make a group such that $a \neq b \neq c \neq d \neq 1$ with the constraints above. How do I know that the same isn't true of the Klein Four group? How do I know there isn't some set of constraints that makes the group ""non-existent"" for unique elements in the same way as the ""Klein Five"" group? Any help would be appreciated!",,"['abstract-algebra', 'group-theory']"
91,"Is ideal an ""anti-field""?","Is ideal an ""anti-field""?",,"I am comparing theorems on normal subgroup and ideal from Fraleigh 's, and come to this strange intuition. I hope my conclusion does not screw up, I hope I won't get ridiculed: Theorem 15.18: $M$is a maximal normal subgroup of $G$ if and only if $G/M$ is simple. To me this theorem makes sense because in $G/M$, the $M$ has been ""collapsed"" into either $0$ or $e$ (borrowing from Fraleigh 's term.) In other words, the maximal normal subgroup $M$ has been ""modded out"" of $G$ such that all that is left is a simple group. Having said that, let's us now go to the second theorem: Theorem 27.9: (Analogue of Theorem 15.18) Let $R$ be a commutative ring with unity. Then $M$ is a maximal ideal of $R$ if and only if $R/M$ is a field. Since $R$ becomes a field only after it is ""modded out"" of the ideal $M$, may I thus conclude that ideal can intuitively be seen as an ""anti-field,"" meaning that each and every element of an ideal does not have multiplicative inverse, whereas each and every element of field has multiplicative inverse? Thank you for your time and effort. POST SCRIPT : I found another theorem of similar flavor: An ideal $I$ of $R$ is prime if and only if $R/I$ is an integral domain. In similar vein, may I conclude that each element of prime ideal has zero divisor? This 4-year-old posting here strikes me as relevant to my conclusion. Thanks again.","I am comparing theorems on normal subgroup and ideal from Fraleigh 's, and come to this strange intuition. I hope my conclusion does not screw up, I hope I won't get ridiculed: Theorem 15.18: $M$is a maximal normal subgroup of $G$ if and only if $G/M$ is simple. To me this theorem makes sense because in $G/M$, the $M$ has been ""collapsed"" into either $0$ or $e$ (borrowing from Fraleigh 's term.) In other words, the maximal normal subgroup $M$ has been ""modded out"" of $G$ such that all that is left is a simple group. Having said that, let's us now go to the second theorem: Theorem 27.9: (Analogue of Theorem 15.18) Let $R$ be a commutative ring with unity. Then $M$ is a maximal ideal of $R$ if and only if $R/M$ is a field. Since $R$ becomes a field only after it is ""modded out"" of the ideal $M$, may I thus conclude that ideal can intuitively be seen as an ""anti-field,"" meaning that each and every element of an ideal does not have multiplicative inverse, whereas each and every element of field has multiplicative inverse? Thank you for your time and effort. POST SCRIPT : I found another theorem of similar flavor: An ideal $I$ of $R$ is prime if and only if $R/I$ is an integral domain. In similar vein, may I conclude that each element of prime ideal has zero divisor? This 4-year-old posting here strikes me as relevant to my conclusion. Thanks again.",,"['abstract-algebra', 'ring-theory']"
92,Proof that $\Bbb R/\Bbb Z$ is isomorphic to $S^1$,Proof that  is isomorphic to,\Bbb R/\Bbb Z S^1,"I just learned the first isomorphism theorem for groups and now I need to prove that: $$\Bbb R/\Bbb Z \cong S^1$$ In other words, the quotient group of $\Bbb Z$ in $\Bbb R$ is isomorphic to the $S^1$ (the group of all permutations of $1$ elements, I guess). I'll write what I understand about the first isomorphism theorem for groups: If we have $\phi: G\to G'$ as an homomorphsim, then the quotient group $$G/\ker(G)\cong G'$$ and the isomorphism if given by $\phi'$ which I forgot exactly how it's given. So, if I want to prove that $$\Bbb R/\Bbb Z  \cong S^1$$ using the first isomorphism theorem, I would need to somehow get $\Bbb Z$ to be the kernel of a homomorphism between $\Bbb R$ and $S_1$? Because the exercise does not give any information about which is the homomorphism between the two sets, so I'm thinking about creating one. Am I doing the rigth thing?","I just learned the first isomorphism theorem for groups and now I need to prove that: $$\Bbb R/\Bbb Z \cong S^1$$ In other words, the quotient group of $\Bbb Z$ in $\Bbb R$ is isomorphic to the $S^1$ (the group of all permutations of $1$ elements, I guess). I'll write what I understand about the first isomorphism theorem for groups: If we have $\phi: G\to G'$ as an homomorphsim, then the quotient group $$G/\ker(G)\cong G'$$ and the isomorphism if given by $\phi'$ which I forgot exactly how it's given. So, if I want to prove that $$\Bbb R/\Bbb Z  \cong S^1$$ using the first isomorphism theorem, I would need to somehow get $\Bbb Z$ to be the kernel of a homomorphism between $\Bbb R$ and $S_1$? Because the exercise does not give any information about which is the homomorphism between the two sets, so I'm thinking about creating one. Am I doing the rigth thing?",,"['abstract-algebra', 'group-theory']"
93,"Proof about finitely generated torsion-free R-module M is free, where R is a PID","Proof about finitely generated torsion-free R-module M is free, where R is a PID",,"Proof about finitely generated torsion-free $R$-module $M$ is free, where $R$ is a PID. Can I prove by the following way? Proof by induction on $n$, where $M=\langle v_1,...,v_n\rangle$. If $n=1$, then $M$ is cyclic, and it's easy to see $M\cong R$, thus $M$ is free. For the inductive step, let $M=\langle v_1,...,v_{n+1}\rangle$ and define $S=\langle v_{n+1}\rangle$. And as M is torsion free thus its submodule M/S is torsion free. And clearly M/S is generated by n elements. Thus by inductive hypothesis, M/S is free. Thus M/S is a projective module. Thus by the sequence: 0$\to S \to M \to M/S \to 0$, we can get $M\cong S\oplus M/S$. And we know S is free, thus M is free. Is there a mistake in my proof? Thank you!","Proof about finitely generated torsion-free $R$-module $M$ is free, where $R$ is a PID. Can I prove by the following way? Proof by induction on $n$, where $M=\langle v_1,...,v_n\rangle$. If $n=1$, then $M$ is cyclic, and it's easy to see $M\cong R$, thus $M$ is free. For the inductive step, let $M=\langle v_1,...,v_{n+1}\rangle$ and define $S=\langle v_{n+1}\rangle$. And as M is torsion free thus its submodule M/S is torsion free. And clearly M/S is generated by n elements. Thus by inductive hypothesis, M/S is free. Thus M/S is a projective module. Thus by the sequence: 0$\to S \to M \to M/S \to 0$, we can get $M\cong S\oplus M/S$. And we know S is free, thus M is free. Is there a mistake in my proof? Thank you!",,"['abstract-algebra', 'modules', 'principal-ideal-domains']"
94,"In a ring: $\, x^6 = x\,$ for all $x\ \Rightarrow\, x^2 = x\,$ for all $x$",In a ring:  for all  for all,"\, x^6 = x\, x\ \Rightarrow\, x^2 = x\, x","In a ring: $\, x^6 = x\,$ for all $x\ \Rightarrow\, x^2 = x\,$ for all $x$ I found a short and interesting problem: Given a ring $(R, +, \cdot)$ and knowing that $x ^ 6 = x\ (\forall x\in R)$ , prove that $x ^ 2 = x\ (\forall x \in R)$ . While it is short, I cannot figure out how to solve it. If it would be the reverse, then the solution were simple: $(x ^ 2) ^ 3 = x$ . Given this information, can be the problem be solved? If so, which is the simplest way to solve it?","In a ring: for all for all I found a short and interesting problem: Given a ring and knowing that , prove that . While it is short, I cannot figure out how to solve it. If it would be the reverse, then the solution were simple: . Given this information, can be the problem be solved? If so, which is the simplest way to solve it?","\, x^6 = x\, x\ \Rightarrow\, x^2 = x\, x (R, +, \cdot) x ^ 6 = x\ (\forall x\in R) x ^ 2 = x\ (\forall x \in R) (x ^ 2) ^ 3 = x","['abstract-algebra', 'ring-theory']"
95,Cyclic group Zp [duplicate],Cyclic group Zp [duplicate],,"This question already has answers here : Is $\mathbb Z _p^*=\{ 1, 2, 3, ... , p-1 \}$ a cyclic group? (6 answers) Closed 9 years ago . How to show if $p$ is prime, then group $Z_{p}^{*}$ is cyclic. Tip Let $g$ and $h$ of a commutative group $G$ have orders $n$ and $m$ respectively. There exists and element $x \in G$ of order $LCM (n,m)$ In any field $\mathbb{K}$ a polynomial $f \in \mathbb{K} [x]$ of degree $n$ has at most $n$ different roots. I have no idea.","This question already has answers here : Is $\mathbb Z _p^*=\{ 1, 2, 3, ... , p-1 \}$ a cyclic group? (6 answers) Closed 9 years ago . How to show if $p$ is prime, then group $Z_{p}^{*}$ is cyclic. Tip Let $g$ and $h$ of a commutative group $G$ have orders $n$ and $m$ respectively. There exists and element $x \in G$ of order $LCM (n,m)$ In any field $\mathbb{K}$ a polynomial $f \in \mathbb{K} [x]$ of degree $n$ has at most $n$ different roots. I have no idea.",,['abstract-algebra']
96,Difference between generators and basis,Difference between generators and basis,,"What is the difference between the terms ""generator set"" and ""basis""?  Don't they both just mean a set of objects that you can use to obtain all of the objects in a larger set under some operations?  Is a basis just a particular kind of generator set or are they completely distinct ideas?","What is the difference between the terms ""generator set"" and ""basis""?  Don't they both just mean a set of objects that you can use to obtain all of the objects in a larger set under some operations?  Is a basis just a particular kind of generator set or are they completely distinct ideas?",,"['abstract-algebra', 'definition']"
97,Prove that $G$ is abelian iff $\varphi(g) = g^2$ is a homomorphism,Prove that  is abelian iff  is a homomorphism,G \varphi(g) = g^2,"I'm working on the following problem: Let $G$ be a group. Prove that $G$ is abelian if and only if   $\varphi(g) = g^2$ is a homomorphism. My solution : First assume that $G$ is an abelian group and let $g, h \in G$. Observe that $\varphi(gh) = (gh)^2 = (gh)(gh) = g^2h^2 = \varphi(g)\varphi(h)$. Thus, $\varphi$ is a homomorphism. I'm having trouble completing the proof in the reverse direction. Assume that $\varphi$ is a homomorphism. We then know that $\varphi(gh) = \varphi(g)\varphi(h)$ and $\varphi(hg) = \varphi(h)\varphi(g)$. However, I don't see a way to use this to show that $gh = hg$. Could anyone lend a helping hand?","I'm working on the following problem: Let $G$ be a group. Prove that $G$ is abelian if and only if   $\varphi(g) = g^2$ is a homomorphism. My solution : First assume that $G$ is an abelian group and let $g, h \in G$. Observe that $\varphi(gh) = (gh)^2 = (gh)(gh) = g^2h^2 = \varphi(g)\varphi(h)$. Thus, $\varphi$ is a homomorphism. I'm having trouble completing the proof in the reverse direction. Assume that $\varphi$ is a homomorphism. We then know that $\varphi(gh) = \varphi(g)\varphi(h)$ and $\varphi(hg) = \varphi(h)\varphi(g)$. However, I don't see a way to use this to show that $gh = hg$. Could anyone lend a helping hand?",,"['abstract-algebra', 'group-theory']"
98,Suppose $G$ is a group with exactly $8$ elements of order $3$. How many subgroups of order $3$ does $G$ have?,Suppose  is a group with exactly  elements of order . How many subgroups of order  does  have?,G 8 3 3 G,Suppose $G$ is a group with exactly $8$ elements of order $3$. How many subgroups of order $3$ does $G$ have?,Suppose $G$ is a group with exactly $8$ elements of order $3$. How many subgroups of order $3$ does $G$ have?,,"['abstract-algebra', 'group-theory', 'finite-groups']"
99,Herstein Question: $G^{i}$ normal in $G$?,Herstein Question:  normal in ?,G^{i} G,"I just wanted to ask a quick question. I'm going over the second edition of I.N. Herstein's topics in algebra and one of his exercises asks the reader to prove that each $G^{i} $ is a normal subgroup of G where $G^{i}$ is the $i^{th}$ commutator group. Now I think I was able to prove $G^{i}$ is normal in $G^{i+1}$, where $G^{i}$ is the commutator group of $G^{i+1}$: Let $g \in G^{i+1}, h \in G^{i}$. Every $h \in G^{i+1}$ since $h=g_{1}^{-1}g_{2}^{-1}g_{1}g_{2} \in G^{i+1}$ since $G^{i+1}$ is a group. So $(gh_{1}g^{-1}h_{1}^{-1} \in G^{i}) \Rightarrow (gh_{1}g^{-1}h_{1}^{-1}=h_{2}) \Rightarrow (gh_{1}=h_{2}h_{1}g) \Rightarrow (gh_{1}=h_{3}g)$ So $G^{i}$ is normal in $G^{i+1}$. I'm seeing how this relates to solvable groups through composition series. However it doesn't seem necessary to have every $G^{i}$ normal to the whole group G. We only really need $G^{i}$ normal in $G^{i+1}$. Is it absolutely necessary I prove the additional property of its normality in G? I suppose for the sake of curiosity I might come back to prove the proposition but, for now, can I move on without being delayed by it? I'd like to get to the more exciting stuff. Also is my proof correct? Thanks in advance!!","I just wanted to ask a quick question. I'm going over the second edition of I.N. Herstein's topics in algebra and one of his exercises asks the reader to prove that each $G^{i} $ is a normal subgroup of G where $G^{i}$ is the $i^{th}$ commutator group. Now I think I was able to prove $G^{i}$ is normal in $G^{i+1}$, where $G^{i}$ is the commutator group of $G^{i+1}$: Let $g \in G^{i+1}, h \in G^{i}$. Every $h \in G^{i+1}$ since $h=g_{1}^{-1}g_{2}^{-1}g_{1}g_{2} \in G^{i+1}$ since $G^{i+1}$ is a group. So $(gh_{1}g^{-1}h_{1}^{-1} \in G^{i}) \Rightarrow (gh_{1}g^{-1}h_{1}^{-1}=h_{2}) \Rightarrow (gh_{1}=h_{2}h_{1}g) \Rightarrow (gh_{1}=h_{3}g)$ So $G^{i}$ is normal in $G^{i+1}$. I'm seeing how this relates to solvable groups through composition series. However it doesn't seem necessary to have every $G^{i}$ normal to the whole group G. We only really need $G^{i}$ normal in $G^{i+1}$. Is it absolutely necessary I prove the additional property of its normality in G? I suppose for the sake of curiosity I might come back to prove the proposition but, for now, can I move on without being delayed by it? I'd like to get to the more exciting stuff. Also is my proof correct? Thanks in advance!!",,"['abstract-algebra', 'group-theory', 'proof-verification']"
