,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find the sum of the series $\sum \frac{1}{n(n+1)(n+2)}$,Find the sum of the series,\sum \frac{1}{n(n+1)(n+2)},I got this question in my maths paper Test the condition for convergence of $$\sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)}$$   and find the sum if it exists. I managed to show that the series converges but I was unable to find the sum. Any help/hint will go a long way. Thank you.,I got this question in my maths paper Test the condition for convergence of $$\sum_{n=1}^\infty \frac{1}{n(n+1)(n+2)}$$   and find the sum if it exists. I managed to show that the series converges but I was unable to find the sum. Any help/hint will go a long way. Thank you.,,['sequences-and-series']
1,Calculating $\lim_{n\to\infty}\sqrt{n}\sin(\sin...(\sin(x)..)$,Calculating,\lim_{n\to\infty}\sqrt{n}\sin(\sin...(\sin(x)..),"I was asked today by a friend to calculate a limit and I am having trouble with the question. Denote $\sin_{1}:=\sin$ and for $n>1$ define $\sin_{n}=\sin(\sin_{n-1})$. Calculate $\lim_{n\to\infty}\sqrt{n}\sin_{n}(x)$ for $x\in\mathbb{R}$ (the answer should be a function of $x$ ). My thoughts: It is sufficient to find the limit for $x\in[0,2\pi]$ , and it is easy to find the limit at $0,2\pi$ so we need to find the limit for $x\in(0,2\pi)$. If $[a,b]\subset(0,\pi)$ or $[a,b]\subset(\pi,2\pi)$ we have it that then $$\max_{x\in[a,b]}|\sin'(x)|=\max_{x\in[a,b]}|\cos(x)|<\lambda\leq1$$ hence the map $\sin(x)$ is a contracting map. We know there is a unique fixed-point but since $0$ is such a point I deduce that for any $x\in(0,2\pi)$ s.t $x\neq\pi$ we have it that $$\lim_{n\to\infty}\sin_{n}(x)=0$$ So I have a limit of the form ""$0\cdot\infty$"" and I can't figure out any way on how to tackle it. Can someone please suggest a way to find that limit ? Note: I am unsure about the tags, please change them if you see fit.","I was asked today by a friend to calculate a limit and I am having trouble with the question. Denote $\sin_{1}:=\sin$ and for $n>1$ define $\sin_{n}=\sin(\sin_{n-1})$. Calculate $\lim_{n\to\infty}\sqrt{n}\sin_{n}(x)$ for $x\in\mathbb{R}$ (the answer should be a function of $x$ ). My thoughts: It is sufficient to find the limit for $x\in[0,2\pi]$ , and it is easy to find the limit at $0,2\pi$ so we need to find the limit for $x\in(0,2\pi)$. If $[a,b]\subset(0,\pi)$ or $[a,b]\subset(\pi,2\pi)$ we have it that then $$\max_{x\in[a,b]}|\sin'(x)|=\max_{x\in[a,b]}|\cos(x)|<\lambda\leq1$$ hence the map $\sin(x)$ is a contracting map. We know there is a unique fixed-point but since $0$ is such a point I deduce that for any $x\in(0,2\pi)$ s.t $x\neq\pi$ we have it that $$\lim_{n\to\infty}\sin_{n}(x)=0$$ So I have a limit of the form ""$0\cdot\infty$"" and I can't figure out any way on how to tackle it. Can someone please suggest a way to find that limit ? Note: I am unsure about the tags, please change them if you see fit.",,"['real-analysis', 'limits', 'numerical-methods']"
2,Closed form for this $\text{I}(u)=\int_u^\infty\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x$ integral? (spot my mistake),Closed form for this  integral? (spot my mistake),\text{I}(u)=\int_u^\infty\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x,"Solving this question , I came up with this (but mathematica gives me a different answer, also when I use a value for $u$) can someone spot my mistake? My work: $$\text{I}(u)=\int_u^\infty\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x=\lim_{n\to\infty}\int_u^n\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x=$$ Substitute $s=\sqrt{x}$ and $\text{d}s=\frac{1}{2\sqrt{x}}\space\text{d}x$. This gives a new lower bound $s=\sqrt{u}$ and upper bound $s=\sqrt{n}$: $$2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\frac{s^2}{s^4-2s^2+2}\space\text{d}s=2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\frac{s^2}{\left(s^2-(1+i)\right)\left(s^2-(1-i)\right)}\space\text{d}s=$$ Use partial fractions, I set: $$\text{z}=\frac{1+i}{2}\to\overline{\text{z}}=\frac{1-i}{2}$$ $$\text{q}=1+i\to\overline{\text{q}}=1-i$$ $$2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\left[\frac{\text{z}}{s^2-\overline{\text{q}}}+\frac{\overline{\text{z}}}{s^2-\text{q}}\right]\space\text{d}s=$$ $$2\lim_{n\to\infty}\left[\text{z}\cdot\int_{\sqrt{u}}^{\sqrt{n}}\frac{1}{s^2-\overline{\text{q}}}\space\text{d}s+\overline{\text{z}}\cdot\int_{\sqrt{u}}^{\sqrt{n}}\frac{1}{s^2-\text{q}}\space\text{d}s\right]=$$ Substitute, for the left integral $t=\frac{s}{\sqrt{\overline{\text{q}}}}$ and $\text{d}t=\frac{1}{\sqrt{\overline{\text{q}}}}\space\text{d}s$. This gives a new lower bound $t=\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}$ and upper bound $t=\frac{\sqrt{n}}{\sqrt{\overline{\text{q}}}}$: Substitute, for the right integral $v=\frac{s}{\sqrt{\text{q}}}$ and $\text{d}v=\frac{1}{\sqrt{\text{q}}}\space\text{d}s$. This gives a new lower bound $v=\frac{\sqrt{u}}{\sqrt{\text{q}}}$ and upper bound $v=\frac{\sqrt{n}}{\sqrt{\text{q}}}$: $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\int_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\frac{1}{1-t^2}\space\text{d}t+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\int_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\frac{1}{1-v^2}\space\text{d}v\right]=$$ Now, use: $$\int\frac{1}{1-x^2}\space\text{d}x=\frac{\ln\left|\frac{x+1}{x-1}\right|}{2}+\text{C}$$ $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left[\frac{\ln\left|\frac{t+1}{t-1}\right|}{2}\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left[\frac{\ln\left|\frac{v+1}{v-1}\right|}{2}\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{2\sqrt{\overline{\text{q}}}}\cdot\left[\ln\left|\frac{t+1}{t-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{2\sqrt{\text{q}}}\cdot\left[\ln\left|\frac{v+1}{v-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left[\ln\left|\frac{t+1}{t-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left[\ln\left|\frac{v+1}{v-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)\right]=$$ Set: $$\text{s}=\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\to\overline{\text{s}}=\frac{\overline{\text{z}}}{\sqrt{\text{q}}}$$ $$-\lim_{n\to\infty}\left[\text{s}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)+\overline{\text{s}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)\right]=$$ Assume $u\space\wedge\space n\in\mathbb{R}^+$: $$\text{A}=\left|\frac{\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}+1}{\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}-1}\right|=\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|=\frac{\sqrt{\sqrt{2}+\sqrt{u}\cdot\sqrt{2(1+\sqrt{2})}+u}}{\sqrt{\sqrt{2}-\sqrt{u}\cdot\sqrt{2(1+\sqrt{2})}+u}}$$ $$\text{B}=\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|=\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|=\frac{\sqrt{\sqrt{2}+\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}{\sqrt{\sqrt{2}-\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}$$ $$-\lim_{n\to\infty}\left[\text{s}\cdot\left(\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right)+\overline{\text{s}}\cdot\left(\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right)\right]=-\left(\text{s}+\overline{\text{s}}\right)\lim_{n\to\infty}\left[\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right]=$$ Notice: $$\lim_{n\to\infty}\frac{\sqrt{\sqrt{2}+\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}{\sqrt{\sqrt{2}-\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}=1$$ $$\text{s}+\overline{\text{s}}=2\Re[\text{s}]=\sqrt{\frac{1}{\sqrt{2}}-\frac{1}{2}}$$ $$\left(\text{s}+\overline{\text{s}}\right)\ln\left(\text{A}\right)=\sqrt{\frac{1}{\sqrt{2}}-\frac{1}{2}}\cdot\ln\left(\text{A}\right)$$","Solving this question , I came up with this (but mathematica gives me a different answer, also when I use a value for $u$) can someone spot my mistake? My work: $$\text{I}(u)=\int_u^\infty\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x=\lim_{n\to\infty}\int_u^n\frac{\sqrt{x}}{x^2-2x+2}\space\text{d}x=$$ Substitute $s=\sqrt{x}$ and $\text{d}s=\frac{1}{2\sqrt{x}}\space\text{d}x$. This gives a new lower bound $s=\sqrt{u}$ and upper bound $s=\sqrt{n}$: $$2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\frac{s^2}{s^4-2s^2+2}\space\text{d}s=2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\frac{s^2}{\left(s^2-(1+i)\right)\left(s^2-(1-i)\right)}\space\text{d}s=$$ Use partial fractions, I set: $$\text{z}=\frac{1+i}{2}\to\overline{\text{z}}=\frac{1-i}{2}$$ $$\text{q}=1+i\to\overline{\text{q}}=1-i$$ $$2\lim_{n\to\infty}\int_{\sqrt{u}}^{\sqrt{n}}\left[\frac{\text{z}}{s^2-\overline{\text{q}}}+\frac{\overline{\text{z}}}{s^2-\text{q}}\right]\space\text{d}s=$$ $$2\lim_{n\to\infty}\left[\text{z}\cdot\int_{\sqrt{u}}^{\sqrt{n}}\frac{1}{s^2-\overline{\text{q}}}\space\text{d}s+\overline{\text{z}}\cdot\int_{\sqrt{u}}^{\sqrt{n}}\frac{1}{s^2-\text{q}}\space\text{d}s\right]=$$ Substitute, for the left integral $t=\frac{s}{\sqrt{\overline{\text{q}}}}$ and $\text{d}t=\frac{1}{\sqrt{\overline{\text{q}}}}\space\text{d}s$. This gives a new lower bound $t=\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}$ and upper bound $t=\frac{\sqrt{n}}{\sqrt{\overline{\text{q}}}}$: Substitute, for the right integral $v=\frac{s}{\sqrt{\text{q}}}$ and $\text{d}v=\frac{1}{\sqrt{\text{q}}}\space\text{d}s$. This gives a new lower bound $v=\frac{\sqrt{u}}{\sqrt{\text{q}}}$ and upper bound $v=\frac{\sqrt{n}}{\sqrt{\text{q}}}$: $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\int_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\frac{1}{1-t^2}\space\text{d}t+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\int_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\frac{1}{1-v^2}\space\text{d}v\right]=$$ Now, use: $$\int\frac{1}{1-x^2}\space\text{d}x=\frac{\ln\left|\frac{x+1}{x-1}\right|}{2}+\text{C}$$ $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left[\frac{\ln\left|\frac{t+1}{t-1}\right|}{2}\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left[\frac{\ln\left|\frac{v+1}{v-1}\right|}{2}\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-2\lim_{n\to\infty}\left[\frac{\text{z}}{2\sqrt{\overline{\text{q}}}}\cdot\left[\ln\left|\frac{t+1}{t-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{2\sqrt{\text{q}}}\cdot\left[\ln\left|\frac{v+1}{v-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left[\ln\left|\frac{t+1}{t-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left[\ln\left|\frac{v+1}{v-1}\right|\right]_{\frac{\sqrt{u}}{\sqrt{\text{q}}}}^{\frac{\sqrt{n}}{\sqrt{\text{q}}}}\right]=$$ $$-\lim_{n\to\infty}\left[\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)+\frac{\overline{\text{z}}}{\sqrt{\text{q}}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)\right]=$$ Set: $$\text{s}=\frac{\text{z}}{\sqrt{\overline{\text{q}}}}\to\overline{\text{s}}=\frac{\overline{\text{z}}}{\sqrt{\text{q}}}$$ $$-\lim_{n\to\infty}\left[\text{s}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)+\overline{\text{s}}\cdot\left(\ln\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|-\ln\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|\right)\right]=$$ Assume $u\space\wedge\space n\in\mathbb{R}^+$: $$\text{A}=\left|\frac{\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}+1}{\frac{\sqrt{u}}{\sqrt{\overline{\text{q}}}}-1}\right|=\left|\frac{\frac{\sqrt{u}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{u}}{\sqrt{\text{q}}}-1}\right|=\frac{\sqrt{\sqrt{2}+\sqrt{u}\cdot\sqrt{2(1+\sqrt{2})}+u}}{\sqrt{\sqrt{2}-\sqrt{u}\cdot\sqrt{2(1+\sqrt{2})}+u}}$$ $$\text{B}=\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|=\left|\frac{\frac{\sqrt{n}}{\sqrt{\text{q}}}+1}{\frac{\sqrt{n}}{\sqrt{\text{q}}}-1}\right|=\frac{\sqrt{\sqrt{2}+\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}{\sqrt{\sqrt{2}-\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}$$ $$-\lim_{n\to\infty}\left[\text{s}\cdot\left(\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right)+\overline{\text{s}}\cdot\left(\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right)\right]=-\left(\text{s}+\overline{\text{s}}\right)\lim_{n\to\infty}\left[\ln\left(\text{B}\right)-\ln\left(\text{A}\right)\right]=$$ Notice: $$\lim_{n\to\infty}\frac{\sqrt{\sqrt{2}+\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}{\sqrt{\sqrt{2}-\sqrt{n}\cdot\sqrt{2(1+\sqrt{2})}+n}}=1$$ $$\text{s}+\overline{\text{s}}=2\Re[\text{s}]=\sqrt{\frac{1}{\sqrt{2}}-\frac{1}{2}}$$ $$\left(\text{s}+\overline{\text{s}}\right)\ln\left(\text{A}\right)=\sqrt{\frac{1}{\sqrt{2}}-\frac{1}{2}}\cdot\ln\left(\text{A}\right)$$",,"['integration', 'limits', 'definite-integrals']"
3,Find the limit of a definite integral,Find the limit of a definite integral,,"A definite integral is defined as $$I(v,\theta)=\int_0^{\pi} e^{v[\cos(\theta-\phi)-1]}\sqrt{\dfrac{v \sin\phi}{\sin\theta}}d\phi$$ My question is how to show that  $$\lim_{v\to \infty} I(v, \theta)=\sqrt{2\pi}$$ for any given $\theta \in (0, \pi)$. My understanding is like this. Since $v$ goes to infinity, the main contribution of the integral comes from those $\phi$ values which are close to $\theta$ (because of the exponential term). I tried the Taylor expansion of $\phi$ around $\theta$, for example, \begin{align*}  \cos (\theta -\phi) &\approx 1-\dfrac{(\theta-\phi)^2}{2!} + \dfrac{(\theta-\phi)^4}{4!} + \cdots \\ \sin \phi & \approx \sin \theta + \cos\theta(\phi-\theta) - \dfrac{\sin \theta}{2}(\phi-\theta)^2 + \cdots \end{align*} but still can't go far. I don't know if I am on the right track. Please help.","A definite integral is defined as $$I(v,\theta)=\int_0^{\pi} e^{v[\cos(\theta-\phi)-1]}\sqrt{\dfrac{v \sin\phi}{\sin\theta}}d\phi$$ My question is how to show that  $$\lim_{v\to \infty} I(v, \theta)=\sqrt{2\pi}$$ for any given $\theta \in (0, \pi)$. My understanding is like this. Since $v$ goes to infinity, the main contribution of the integral comes from those $\phi$ values which are close to $\theta$ (because of the exponential term). I tried the Taylor expansion of $\phi$ around $\theta$, for example, \begin{align*}  \cos (\theta -\phi) &\approx 1-\dfrac{(\theta-\phi)^2}{2!} + \dfrac{(\theta-\phi)^4}{4!} + \cdots \\ \sin \phi & \approx \sin \theta + \cos\theta(\phi-\theta) - \dfrac{\sin \theta}{2}(\phi-\theta)^2 + \cdots \end{align*} but still can't go far. I don't know if I am on the right track. Please help.",,"['limits', 'definite-integrals', 'taylor-expansion']"
4,Pass to the limit under the sign of integral,Pass to the limit under the sign of integral,,"I need to show that this integral converges to its limit, showing this only for a subsequence is also good enough for  me. Consider $w_k \rightarrow w $ in $L^2$, $u_k \rightarrow u $ weakly in $H^1_0$, strongly in $L^2$ (passing to a subsequence we can say it converges also almost everywhere), $\Omega$ a bounded and smooth domain in $\mathbb R^3$. $a \in C^0 (\mathbb R) $ s.t. $0 < \lambda \leq a(s) \leq \Lambda  \ \forall s \in \mathbb R$ $$\int_\Omega a(w_k) \nabla u_k  \nabla v  \rightarrow \int_\Omega a(w) \nabla u  \nabla v  ?$$ I tried splitting it as: $$\int_\Omega a(w_k) \nabla u_k  \nabla v = \int_\Omega a(w_k) (\nabla u_k - \nabla u) \nabla v \ + \ \int_\Omega a(w_k) \nabla u \nabla v $$ So my problem is: $$\int_\Omega a(w_k) (\nabla u_k - \nabla u) \nabla v \rightarrow 0 ? $$","I need to show that this integral converges to its limit, showing this only for a subsequence is also good enough for  me. Consider $w_k \rightarrow w $ in $L^2$, $u_k \rightarrow u $ weakly in $H^1_0$, strongly in $L^2$ (passing to a subsequence we can say it converges also almost everywhere), $\Omega$ a bounded and smooth domain in $\mathbb R^3$. $a \in C^0 (\mathbb R) $ s.t. $0 < \lambda \leq a(s) \leq \Lambda  \ \forall s \in \mathbb R$ $$\int_\Omega a(w_k) \nabla u_k  \nabla v  \rightarrow \int_\Omega a(w) \nabla u  \nabla v  ?$$ I tried splitting it as: $$\int_\Omega a(w_k) \nabla u_k  \nabla v = \int_\Omega a(w_k) (\nabla u_k - \nabla u) \nabla v \ + \ \int_\Omega a(w_k) \nabla u \nabla v $$ So my problem is: $$\int_\Omega a(w_k) (\nabla u_k - \nabla u) \nabla v \rightarrow 0 ? $$",,"['real-analysis', 'limits', 'convergence-divergence', 'weak-convergence']"
5,Is this limit correctly computed?,Is this limit correctly computed?,,"Here is the function: \begin{align*} u(x,y,z)={}&\int_0^1\frac{\tau}{\sqrt{x^2+y^2+(z-\tau)^2}}\mathrm{d}\tau={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-{} \\ &{}+z\log\left|\frac{z-1+\sqrt{x^2+y^2+(z-1)^2}}{z+\sqrt{x^2+y^2+z^2}}\right|, \end{align*} where the last equality holds outside the $z$ axis, whereas on the $z$ axis the expression is just the first two terms, outside $\{0\}\times\{0\}\times[0,1]$, of course. I am now pretty convinced I calculated this correctly, and Wolfram agrees if I take the absolute values out, but since they are in a log when I differentiate they just go away, so it's like confirming in any case. I state this because my Higher Analysis course notes have different signs in various points, and the same goes for this pdf . Now I want to take the limit for $z\to0^+$ along surfaces of the form $(e^{-\frac\alpha{2z}}\cos\theta,e^{-\frac\alpha{2z}}\sin\theta,z)$. This is because I want to test its continuity at the origin. The two square roots converge to 1 and 0 respectively, since both $z$ and $e^{\frac\alpha{2z}}$ go to 0. I'm then left with the log, which I split into a sum of logs -- well yeah a difference since I have a quotient inside it. So here I go: \begin{multline*} \lim_{z\to0^+}\left[-z\log\left(z-1+\sqrt{e^{-\frac{\alpha}{z}}+(z-1)^2}\right)+z\log\left(z+\sqrt{e^{-\frac{\alpha}{z}}+z^2}\right)\right]={} \\ =\lim_{z\to0^+}z\left[-\log\left(z-1+\sqrt{1+z^2-2z+e^{-\frac{\alpha}{z}}}\right)+\log\left(z+e^{-\frac{\alpha}{2z}}\sqrt{1+\frac{z^2}{e^{-\frac{\alpha}{z}}}}\right)\right]={} \\ \lim_{z\to0^+}z\left[-\log\left(z-1+1+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2}+o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right.+{} \\ {}+\left.\log e^{-\frac{\alpha}{2z}}+\log\left(\frac{z}{e^{-\frac{\alpha}{2z}}}+1+\frac{z^2}{2e^{\frac{\alpha}{2z}}}+o\left(\frac{z^2}{e^{-\frac{\alpha}{2z}}}\right)\right)\right]={} {}=\lim_{z\to0^+}z\left[-\frac{\alpha}{2z}-\log\left(z+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2}+o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right.+{} \\ {}+\left.\underbrace{\log\left(\frac{z}{e^{-\frac{\alpha}{2z}}}+1+\frac{z^2}{2e^{\frac{\alpha}{2z}}}+o\left(\frac{z^2}{e^{-\frac{\alpha}{2z}}}\right)\right)}_{\text{Goes to zero since the argument is $1+o(1)$.}}\right]={} {}=\lim_{z\to0^+}z\left[-\frac{\alpha}{2z}-\log e^{-\frac{\alpha}{z}}-\log\left(\frac{z}{e^{-\frac{\alpha}{z}}}+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2e^{-\frac{\alpha}{z}}}+\frac{1}{e^{-\frac{\alpha}{z}}}o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right]. \end{multline*} The first two terms give me a $\frac\alpha2$, and the rest should have a finite limit ($-\log\frac12$). So I would get the limit is $1+\frac\alpha2$. But I have two things giving me doubts: An older version of the notes and the pdf give this limit as $1+\alpha$, and the newer version (linked above) of the notes gives $\alpha$; I'm suspicious about that last small-o with the exponential under it: it does go to zero right? So is this all correct or is there something not quite right about it? Update I just realized that: No, that thing in general doesn't tend to zero, since if we multiply and divide by the small-$o$'s argument it becomes an infinitesimal fraction times $\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{e^{-\frac{\alpha}{z}}}$ which goes to infinity; The small-$o$ in the other logarithm has an infinite as its argument. So I will start over. Now the expansion for the first logarithm is a huge mess, so I calculated a few terms with Wolfram ( 1 and 2 ) and, admitting I didn't mis-sum those messes, it should collapse into $\frac{7}{16}z^5+o(z^5)$, hence: \begin{multline*} \lim_{z\to0^+}\left[-z\log\left(z-1+\sqrt{1+e^{-\frac{\alpha}{z}}+z^2-2z}\right)+z\log\left(z+\sqrt{e^{-\frac{\alpha}{z}}+z^2}\right)\right]={} \\ {}=\lim_{z\to0^+}z\left[-\log\left(\frac{7}{16}z^5+o(z^5)\right)+\log z+\log\left(1+\sqrt{\frac{e^{-\frac{\alpha}{z}}}{z^2}+1}\right)\right]={} \\ {}=\lim_{z\to0^+}z\left[-\log\frac{7z^5}{16}-\log\left(1+o(1)\right)+\log z+\log\left(1+1+\frac{e^{-\frac{\alpha}{z}}}{2z^2}\right)\right]={} \\ {}=\lim_{z\to0^+}\left[-z\log\frac{7}{16}-5z\log z-z\log(1+o(1))+z\log z+z\log(2+o(1))\right]=0. \end{multline*} Is this huge mess right now? So my limit is 1 from the square roots plus 0 from the logarithms, thus 1. No $\alpha$. So it seems the whole thing in the notes is completely falling apart, because the proof that $u$ has no limit at the origin is now a non-proof, and even discontinuity with that $u(0,0,0)=2$ claimed by the notes is torn apart by the simple remark that: $$u(0,0,0)=\int_0^1\frac{\tau}{\sqrt{0^2+0^2+(0-\tau)^2}}\mathrm{d}\tau=\int_0^2\frac{\tau}{|\tau|}\mathrm{d}\tau=\int_0^1\operatorname{sgn}\tau\mathrm{d}\tau=\int_0^1\mathrm{d}\tau=1.$$ So am I making any mistake in the above or are the notes full of errors? Extras: computing the ugly integral First of all I will get rid of the $z$ axis so as to always have a strictly positive denominator. It is easy to verify that for $z\in(0,1]$ $u(0,0,z)$ gives a diverging integral. After all, the denominator has a zero in a place ($\tau=z$) where the numerator doesn't, and it's an absolute-value type of zero, so order one, so non-integrable. Let's see the origin. Wait, I've already done it above. So for $z\notin[0,1]$ we proceed as follows: \begin{align*} u(0,0,z)={}&\int_0^1\frac{\tau}{\sqrt{0^2+0^2+(z-\tau)^2}}\mathrm{d}\tau=\int_0^1\frac{\tau}{|z-\tau|}\mathrm{d}\tau={} \\ {}={}&\begin{cases} \int_0^1\frac{\tau}{\tau-z} & z<0 \\ -\int_0^1\frac{\tau}{\tau-z} & z>1 \\ \end{cases}=\begin{cases} \int_0^1\frac{\tau-z}{\tau-z}\mathrm{d}\tau+z\cdot\int_0^1\frac{1}{\tau-z}\mathrm{d}\tau & z<0 \\ -\int_0^1\frac{\tau-z}{\tau-z}\mathrm{d}\tau-z\cdot\int_0^1\frac{1}{\tau-z}\mathrm{d}\tau & z>1 \end{cases}={} \\ {}={}&\begin{cases} 1+z\log|\tau-z|\Big|_0^1 & z<0 \\ -1-z\log|\tau-z|\Big|_0^1 & z>1 \end{cases}=\begin{cases} 1+z\log(1-z)-z\log(-z) & z<0 \\ -1-z\log(z-1)+z\log(z) & z>1 \\ \end{cases}={} \\ {}={}&\begin{cases} 1+z\log(1-\frac1z) & z<0 \\ -1-z\log(1-\frac1z) & z>1 \\ \end{cases}=\operatorname{sgn}(1-z)\cdot\left(1+z\log\left(1-\frac1z\right)\right). \end{align*} Let us now get out of the $z$ axis: \begin{align*} u(x,y,z)={}&\int_0^1\frac{\tau}{\sqrt{x^2+y^2+(\tau-z)^2}}\mathrm{d\tau}={} \\ {}={}&\int_0^1\frac{\tau-z}{\sqrt{x^2+y^2+(\tau-z)^2}}+z\cdot\int_0^1\frac{1}{\sqrt{x^2+y^2+(\tau-z)^2}}\mathrm{d}\tau={} \\ {}\underset{\substack{\Big| \\ t=\frac{z-\tau}{\sqrt{x^2+y^2}}}}{=}{\hspace{-.6cm}}&\sqrt{x^2+y^2+(\tau-z)^2}\Big|_0^1-z\cdot\int_{\frac{z}{\sqrt{x^2+y^2}}}^{\frac{z-1}{\sqrt{x^2+y^2}}}\frac{1}{\sqrt{1+t^2}}\frac{1}{\sqrt{x^2+y^2}}\sqrt{x^2+y^2}\mathrm{d}t={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-z\log(t+\sqrt{1+t^2})\Big|_{\frac{z}{\sqrt{x^2+y^2}}}^{\frac{z-1}{\sqrt{x^2+y^2}}}={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-{} \\ &{}+z\log\left(\frac{z-1}{\sqrt{x^2+y^2}}+\sqrt{1+\frac{(z-1)^2}{x^2+y^2}}\right)+{} \\ &{}+z\log\left(\frac{z}{\sqrt{x^2+y^2}}+\sqrt{1+\frac{z^2}{x^2+y^2}}\right)={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-z\log\frac{z-1+\sqrt{x^2+y^2+(z-1)^2}}{z+\sqrt{x^2+y^2+z^2}}. \end{align*} Interestingly enough, if I substitute $t=\frac{\tau-z}{\sqrt{x^2+y^2}}$ up there, I get the pdf's expression, the notes' expression having an extra term and a wrong log argument, and the limit above is much easier. But do those things actually differ by a constant? Naturally! Here is the difference: \begin{multline*} \log(\tau-z+\sqrt{x^2+y^2+(\tau-z)^2})-(-\log(z-\tau+\sqrt{x^2+y^2+(z-\tau)^2}))={} \\ {}=\log((\tau-z+\sqrt{x^2+y^2+(\tau-z)^2})(z-\tau+\sqrt{x^2+y^2+(z-\tau)^2})))={} \\ {}=\log(x^2+y^2+(z-\tau)^2-(z-\tau)^2)=\log(x^2+y^2+z^2), \end{multline*} which is constant in $\tau$!","Here is the function: \begin{align*} u(x,y,z)={}&\int_0^1\frac{\tau}{\sqrt{x^2+y^2+(z-\tau)^2}}\mathrm{d}\tau={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-{} \\ &{}+z\log\left|\frac{z-1+\sqrt{x^2+y^2+(z-1)^2}}{z+\sqrt{x^2+y^2+z^2}}\right|, \end{align*} where the last equality holds outside the $z$ axis, whereas on the $z$ axis the expression is just the first two terms, outside $\{0\}\times\{0\}\times[0,1]$, of course. I am now pretty convinced I calculated this correctly, and Wolfram agrees if I take the absolute values out, but since they are in a log when I differentiate they just go away, so it's like confirming in any case. I state this because my Higher Analysis course notes have different signs in various points, and the same goes for this pdf . Now I want to take the limit for $z\to0^+$ along surfaces of the form $(e^{-\frac\alpha{2z}}\cos\theta,e^{-\frac\alpha{2z}}\sin\theta,z)$. This is because I want to test its continuity at the origin. The two square roots converge to 1 and 0 respectively, since both $z$ and $e^{\frac\alpha{2z}}$ go to 0. I'm then left with the log, which I split into a sum of logs -- well yeah a difference since I have a quotient inside it. So here I go: \begin{multline*} \lim_{z\to0^+}\left[-z\log\left(z-1+\sqrt{e^{-\frac{\alpha}{z}}+(z-1)^2}\right)+z\log\left(z+\sqrt{e^{-\frac{\alpha}{z}}+z^2}\right)\right]={} \\ =\lim_{z\to0^+}z\left[-\log\left(z-1+\sqrt{1+z^2-2z+e^{-\frac{\alpha}{z}}}\right)+\log\left(z+e^{-\frac{\alpha}{2z}}\sqrt{1+\frac{z^2}{e^{-\frac{\alpha}{z}}}}\right)\right]={} \\ \lim_{z\to0^+}z\left[-\log\left(z-1+1+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2}+o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right.+{} \\ {}+\left.\log e^{-\frac{\alpha}{2z}}+\log\left(\frac{z}{e^{-\frac{\alpha}{2z}}}+1+\frac{z^2}{2e^{\frac{\alpha}{2z}}}+o\left(\frac{z^2}{e^{-\frac{\alpha}{2z}}}\right)\right)\right]={} {}=\lim_{z\to0^+}z\left[-\frac{\alpha}{2z}-\log\left(z+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2}+o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right.+{} \\ {}+\left.\underbrace{\log\left(\frac{z}{e^{-\frac{\alpha}{2z}}}+1+\frac{z^2}{2e^{\frac{\alpha}{2z}}}+o\left(\frac{z^2}{e^{-\frac{\alpha}{2z}}}\right)\right)}_{\text{Goes to zero since the argument is $1+o(1)$.}}\right]={} {}=\lim_{z\to0^+}z\left[-\frac{\alpha}{2z}-\log e^{-\frac{\alpha}{z}}-\log\left(\frac{z}{e^{-\frac{\alpha}{z}}}+\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{2e^{-\frac{\alpha}{z}}}+\frac{1}{e^{-\frac{\alpha}{z}}}o\left(z^2-2z+e^{-\frac{\alpha}{z}}\right)\right)\right]. \end{multline*} The first two terms give me a $\frac\alpha2$, and the rest should have a finite limit ($-\log\frac12$). So I would get the limit is $1+\frac\alpha2$. But I have two things giving me doubts: An older version of the notes and the pdf give this limit as $1+\alpha$, and the newer version (linked above) of the notes gives $\alpha$; I'm suspicious about that last small-o with the exponential under it: it does go to zero right? So is this all correct or is there something not quite right about it? Update I just realized that: No, that thing in general doesn't tend to zero, since if we multiply and divide by the small-$o$'s argument it becomes an infinitesimal fraction times $\frac{z^2-2z+e^{-\frac{\alpha}{z}}}{e^{-\frac{\alpha}{z}}}$ which goes to infinity; The small-$o$ in the other logarithm has an infinite as its argument. So I will start over. Now the expansion for the first logarithm is a huge mess, so I calculated a few terms with Wolfram ( 1 and 2 ) and, admitting I didn't mis-sum those messes, it should collapse into $\frac{7}{16}z^5+o(z^5)$, hence: \begin{multline*} \lim_{z\to0^+}\left[-z\log\left(z-1+\sqrt{1+e^{-\frac{\alpha}{z}}+z^2-2z}\right)+z\log\left(z+\sqrt{e^{-\frac{\alpha}{z}}+z^2}\right)\right]={} \\ {}=\lim_{z\to0^+}z\left[-\log\left(\frac{7}{16}z^5+o(z^5)\right)+\log z+\log\left(1+\sqrt{\frac{e^{-\frac{\alpha}{z}}}{z^2}+1}\right)\right]={} \\ {}=\lim_{z\to0^+}z\left[-\log\frac{7z^5}{16}-\log\left(1+o(1)\right)+\log z+\log\left(1+1+\frac{e^{-\frac{\alpha}{z}}}{2z^2}\right)\right]={} \\ {}=\lim_{z\to0^+}\left[-z\log\frac{7}{16}-5z\log z-z\log(1+o(1))+z\log z+z\log(2+o(1))\right]=0. \end{multline*} Is this huge mess right now? So my limit is 1 from the square roots plus 0 from the logarithms, thus 1. No $\alpha$. So it seems the whole thing in the notes is completely falling apart, because the proof that $u$ has no limit at the origin is now a non-proof, and even discontinuity with that $u(0,0,0)=2$ claimed by the notes is torn apart by the simple remark that: $$u(0,0,0)=\int_0^1\frac{\tau}{\sqrt{0^2+0^2+(0-\tau)^2}}\mathrm{d}\tau=\int_0^2\frac{\tau}{|\tau|}\mathrm{d}\tau=\int_0^1\operatorname{sgn}\tau\mathrm{d}\tau=\int_0^1\mathrm{d}\tau=1.$$ So am I making any mistake in the above or are the notes full of errors? Extras: computing the ugly integral First of all I will get rid of the $z$ axis so as to always have a strictly positive denominator. It is easy to verify that for $z\in(0,1]$ $u(0,0,z)$ gives a diverging integral. After all, the denominator has a zero in a place ($\tau=z$) where the numerator doesn't, and it's an absolute-value type of zero, so order one, so non-integrable. Let's see the origin. Wait, I've already done it above. So for $z\notin[0,1]$ we proceed as follows: \begin{align*} u(0,0,z)={}&\int_0^1\frac{\tau}{\sqrt{0^2+0^2+(z-\tau)^2}}\mathrm{d}\tau=\int_0^1\frac{\tau}{|z-\tau|}\mathrm{d}\tau={} \\ {}={}&\begin{cases} \int_0^1\frac{\tau}{\tau-z} & z<0 \\ -\int_0^1\frac{\tau}{\tau-z} & z>1 \\ \end{cases}=\begin{cases} \int_0^1\frac{\tau-z}{\tau-z}\mathrm{d}\tau+z\cdot\int_0^1\frac{1}{\tau-z}\mathrm{d}\tau & z<0 \\ -\int_0^1\frac{\tau-z}{\tau-z}\mathrm{d}\tau-z\cdot\int_0^1\frac{1}{\tau-z}\mathrm{d}\tau & z>1 \end{cases}={} \\ {}={}&\begin{cases} 1+z\log|\tau-z|\Big|_0^1 & z<0 \\ -1-z\log|\tau-z|\Big|_0^1 & z>1 \end{cases}=\begin{cases} 1+z\log(1-z)-z\log(-z) & z<0 \\ -1-z\log(z-1)+z\log(z) & z>1 \\ \end{cases}={} \\ {}={}&\begin{cases} 1+z\log(1-\frac1z) & z<0 \\ -1-z\log(1-\frac1z) & z>1 \\ \end{cases}=\operatorname{sgn}(1-z)\cdot\left(1+z\log\left(1-\frac1z\right)\right). \end{align*} Let us now get out of the $z$ axis: \begin{align*} u(x,y,z)={}&\int_0^1\frac{\tau}{\sqrt{x^2+y^2+(\tau-z)^2}}\mathrm{d\tau}={} \\ {}={}&\int_0^1\frac{\tau-z}{\sqrt{x^2+y^2+(\tau-z)^2}}+z\cdot\int_0^1\frac{1}{\sqrt{x^2+y^2+(\tau-z)^2}}\mathrm{d}\tau={} \\ {}\underset{\substack{\Big| \\ t=\frac{z-\tau}{\sqrt{x^2+y^2}}}}{=}{\hspace{-.6cm}}&\sqrt{x^2+y^2+(\tau-z)^2}\Big|_0^1-z\cdot\int_{\frac{z}{\sqrt{x^2+y^2}}}^{\frac{z-1}{\sqrt{x^2+y^2}}}\frac{1}{\sqrt{1+t^2}}\frac{1}{\sqrt{x^2+y^2}}\sqrt{x^2+y^2}\mathrm{d}t={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-z\log(t+\sqrt{1+t^2})\Big|_{\frac{z}{\sqrt{x^2+y^2}}}^{\frac{z-1}{\sqrt{x^2+y^2}}}={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-{} \\ &{}+z\log\left(\frac{z-1}{\sqrt{x^2+y^2}}+\sqrt{1+\frac{(z-1)^2}{x^2+y^2}}\right)+{} \\ &{}+z\log\left(\frac{z}{\sqrt{x^2+y^2}}+\sqrt{1+\frac{z^2}{x^2+y^2}}\right)={} \\ {}={}&\sqrt{x^2+y^2+(z-1)^2}-\sqrt{x^2+y^2+z^2}-z\log\frac{z-1+\sqrt{x^2+y^2+(z-1)^2}}{z+\sqrt{x^2+y^2+z^2}}. \end{align*} Interestingly enough, if I substitute $t=\frac{\tau-z}{\sqrt{x^2+y^2}}$ up there, I get the pdf's expression, the notes' expression having an extra term and a wrong log argument, and the limit above is much easier. But do those things actually differ by a constant? Naturally! Here is the difference: \begin{multline*} \log(\tau-z+\sqrt{x^2+y^2+(\tau-z)^2})-(-\log(z-\tau+\sqrt{x^2+y^2+(z-\tau)^2}))={} \\ {}=\log((\tau-z+\sqrt{x^2+y^2+(\tau-z)^2})(z-\tau+\sqrt{x^2+y^2+(z-\tau)^2})))={} \\ {}=\log(x^2+y^2+(z-\tau)^2-(z-\tau)^2)=\log(x^2+y^2+z^2), \end{multline*} which is constant in $\tau$!",,"['calculus', 'limits']"
6,"Does ""the functions agree at infinity"" mean anything?","Does ""the functions agree at infinity"" mean anything?",,"I want a way to describe how two continuous functions $f,g \colon (X-x) \to Y$ might ""share a limit"" at the point $x$ when unfortunately neither of $\displaystyle \lim _{y \to x}f(x)$ or $\displaystyle \lim _{y \to x}g$ happen to exist.. Let me give an example. Suppose $x = 0$ and $f,g \colon (0,1] \to \mathbb R$ are given by $f = \sin(1/x)$ and $g = \sin(1/x) +x$. Then for any prescribed $\epsilon > 0$ there is an $\delta > 0$ such that, when I restrict the functions to the ball $B(0,\delta)$ their graphs are within distance $\epsilon$ of each other. However in this case it's a lot easier. We can just take the difference of the functions $(f-g)(x) =x$ and say that function tends to zero at $x$. But this relies on how the real line has an additive structure. If we replace the codomain with something more exotic this definition might not apply. So how might I formalise this notion of ""agreeing at infinity"" for more general topological spaces $Y$?","I want a way to describe how two continuous functions $f,g \colon (X-x) \to Y$ might ""share a limit"" at the point $x$ when unfortunately neither of $\displaystyle \lim _{y \to x}f(x)$ or $\displaystyle \lim _{y \to x}g$ happen to exist.. Let me give an example. Suppose $x = 0$ and $f,g \colon (0,1] \to \mathbb R$ are given by $f = \sin(1/x)$ and $g = \sin(1/x) +x$. Then for any prescribed $\epsilon > 0$ there is an $\delta > 0$ such that, when I restrict the functions to the ball $B(0,\delta)$ their graphs are within distance $\epsilon$ of each other. However in this case it's a lot easier. We can just take the difference of the functions $(f-g)(x) =x$ and say that function tends to zero at $x$. But this relies on how the real line has an additive structure. If we replace the codomain with something more exotic this definition might not apply. So how might I formalise this notion of ""agreeing at infinity"" for more general topological spaces $Y$?",,"['general-topology', 'limits', 'functions', 'reference-request', 'asymptotics']"
7,"Proof Verification: ""Composition"" of limits","Proof Verification: ""Composition"" of limits",,"I actually don't even know whether the following statement is true; I'm using it as a sort of lemma to solve another problem. Suppose $f$ is a real function defined on an open interval containing $x$ and also  for all $t$ in the domain of $f$, $\lim_{y \to t} f(y)$ exists. Then $$ \lim_{y \to x} f(y) = L \Longrightarrow  \lim_{t \to x}(\lim_{y \to t} f(y)) = L$$ Is this proof fine? I'm feeling iffy about it. For every neighbourhood $N_1(L)$ we have a neighbourhood $N_2(x)$ such that the whenever $y \in N_2(x)$ and $x\neq y$, $f(y) \in N_1(L)$. Now, consider $t$ only such that $t \in N_2(x)$. Note $t\neq x$. $\lim_{y \to t} f(y) = L'$ exists, and so for any neighbourhood $N_4(L')$ we have a corresponding $N_3(t)$; we simply make this $N_3(t)$ small enough so that $N_3(t) \subset N_2(x)$. This implies $N_4(L') \subset N_1(L)$, and so $L' \in N_1(L)$ Thus, we have some neighbourhood $N_2(x)$ such that whenever $t \in N_2(x)$ and $x\neq t$, $\lim_{y \to t} f(y) = L' \in N_1(L)$ for any neighbourhood $N_1(L)$ of $L$, which proves the statement.","I actually don't even know whether the following statement is true; I'm using it as a sort of lemma to solve another problem. Suppose $f$ is a real function defined on an open interval containing $x$ and also  for all $t$ in the domain of $f$, $\lim_{y \to t} f(y)$ exists. Then $$ \lim_{y \to x} f(y) = L \Longrightarrow  \lim_{t \to x}(\lim_{y \to t} f(y)) = L$$ Is this proof fine? I'm feeling iffy about it. For every neighbourhood $N_1(L)$ we have a neighbourhood $N_2(x)$ such that the whenever $y \in N_2(x)$ and $x\neq y$, $f(y) \in N_1(L)$. Now, consider $t$ only such that $t \in N_2(x)$. Note $t\neq x$. $\lim_{y \to t} f(y) = L'$ exists, and so for any neighbourhood $N_4(L')$ we have a corresponding $N_3(t)$; we simply make this $N_3(t)$ small enough so that $N_3(t) \subset N_2(x)$. This implies $N_4(L') \subset N_1(L)$, and so $L' \in N_1(L)$ Thus, we have some neighbourhood $N_2(x)$ such that whenever $t \in N_2(x)$ and $x\neq t$, $\lim_{y \to t} f(y) = L' \in N_1(L)$ for any neighbourhood $N_1(L)$ of $L$, which proves the statement.",,"['real-analysis', 'limits', 'proof-verification']"
8,Indetermination in limit of integral $\lim_{x \rightarrow 0}(\int_0^{\sin x}e^{xt^{2}}dt \big/ \int _0^{\tan x}e^{-xt^{2}}dt)$,Indetermination in limit of integral,\lim_{x \rightarrow 0}(\int_0^{\sin x}e^{xt^{2}}dt \big/ \int _0^{\tan x}e^{-xt^{2}}dt),"I would like to evaluate the following limit $$\lim_{x \rightarrow 0}\frac{\int \limits_0^{\sin x}e^{xt^{2}}dt}{\int \limits_0^{\tan x}e^{-xt^{2}}dt}$$ In order to use L'Hospital's rule I obviously need derivatives with respect to $x$. With this including x as a parameter in the upper integration limits, I can reduce the limit to $$\lim_{x \rightarrow 0}\frac{e^{x \sin^{2} x}\cos x+\int \limits_0^{\sin x}t^{2}e^{xt^{2}}dt}{e^{-x\tan^{2}x}\cos^{-2} x+\int \limits_0^{\tan x}(-t^{2})e^{-xt^{2}}dt}$$ I'm not sure if I'm approaching this in the correct way; so my questions are Am I using L'Hospital's rule correctly in the above? If so How should I proceed from now on? I feel a little stuck. Any help is greatly appreciated.","I would like to evaluate the following limit $$\lim_{x \rightarrow 0}\frac{\int \limits_0^{\sin x}e^{xt^{2}}dt}{\int \limits_0^{\tan x}e^{-xt^{2}}dt}$$ In order to use L'Hospital's rule I obviously need derivatives with respect to $x$. With this including x as a parameter in the upper integration limits, I can reduce the limit to $$\lim_{x \rightarrow 0}\frac{e^{x \sin^{2} x}\cos x+\int \limits_0^{\sin x}t^{2}e^{xt^{2}}dt}{e^{-x\tan^{2}x}\cos^{-2} x+\int \limits_0^{\tan x}(-t^{2})e^{-xt^{2}}dt}$$ I'm not sure if I'm approaching this in the correct way; so my questions are Am I using L'Hospital's rule correctly in the above? If so How should I proceed from now on? I feel a little stuck. Any help is greatly appreciated.",,"['calculus', 'integration']"
9,Find $\lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{2k-1}{2^k}$ [duplicate],Find  [duplicate],\lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{2k-1}{2^k},This question already has answers here : What is the sum of $\sum\limits_{i=1}^{n}ip^i$? (4 answers) Why $\sum_{k=1}^{\infty} \frac{k}{2^k} = 2$? [duplicate] (12 answers) Closed 8 years ago . What is the method finding the closed form of $\displaystyle\sum_{k=1}^{n}\frac{2k-1}{2^k}$?,This question already has answers here : What is the sum of $\sum\limits_{i=1}^{n}ip^i$? (4 answers) Why $\sum_{k=1}^{\infty} \frac{k}{2^k} = 2$? [duplicate] (12 answers) Closed 8 years ago . What is the method finding the closed form of $\displaystyle\sum_{k=1}^{n}\frac{2k-1}{2^k}$?,,"['calculus', 'sequences-and-series', 'limits']"
10,"Limits: ""does not exist"" vs ""cannot be evaluated""","Limits: ""does not exist"" vs ""cannot be evaluated""",,"Assuming we have a limit which doesn't exist, i.e. $$\lim_{x\rightarrow x_0}{f(x)} \not{\exists}$$ Is the above wording and notation mathematically equivalent to saying ""The limit cannot be evaluated""? Of course, a limit that doesn't exist can't be evaluated. But can there be a limit that exists and yet cannot be evaluated?","Assuming we have a limit which doesn't exist, i.e. $$\lim_{x\rightarrow x_0}{f(x)} \not{\exists}$$ Is the above wording and notation mathematically equivalent to saying ""The limit cannot be evaluated""? Of course, a limit that doesn't exist can't be evaluated. But can there be a limit that exists and yet cannot be evaluated?",,"['limits', 'notation']"
11,Iterated Sine... Further expansion,Iterated Sine... Further expansion,,We all can find the classical approximation of the iterated sine : $$\text{Let } \sin^n(x) = \sin(\sin(\sin(.... \sin(x))))\:\:n\:\text{times.} \\ $$ Then: $$\sin^n(x) = \sqrt{\frac3n} + o\left(\frac 1{\sqrt n}\right)$$ And a further approximation here gives : $$\sin^n(x) = \sqrt{\frac3n} - \frac {3\sqrt3\:\ \ln(n)}{10\:n \sqrt n} + o\left(\frac {\ln(n)}{n\:\sqrt n}\right) $$ which is still independent from the initial value $x$ (assumed to be between 0 and π/2 for simplicity). I am using the method described in this paper to push it further : $$\frac1{sin^2(u_n)} - \frac1{u_n^2} = \frac13 + \frac{u_n^2}{15} + \frac{u_n^4}{189} + \frac{2\:u_n^4}{189} +o(u_n^5)  $$ Now we can replace $u_n^2$ by its approximation $ \frac n3+ \frac{\ln(n)}5 + o(\ln(n))$ and sum from 0 to n (with $u_0$ = x ) Which gives me : $$\frac1{\sin^n(x)^2}- \frac1{x^2} = \frac n3 + \frac{\ln (n)}5 + constant + o(\frac 1n)$$ and finally  $$\sin^n(x) = {\bigg[\frac n3 + \frac{\ln (n)}5 + \frac1{x^2} + constant + o(\frac 1n)\:\:\bigg]} ^{-1/2}$$ Where x appears after the term in $\ln(n)$ and fades out as $n^{-3/2}$. The constant involves Euler constant and other limits of convergent series. Q: Is this wrong ? Did I miss something ? Has anybody pushed the approximation of the iterated sine further ?,We all can find the classical approximation of the iterated sine : $$\text{Let } \sin^n(x) = \sin(\sin(\sin(.... \sin(x))))\:\:n\:\text{times.} \\ $$ Then: $$\sin^n(x) = \sqrt{\frac3n} + o\left(\frac 1{\sqrt n}\right)$$ And a further approximation here gives : $$\sin^n(x) = \sqrt{\frac3n} - \frac {3\sqrt3\:\ \ln(n)}{10\:n \sqrt n} + o\left(\frac {\ln(n)}{n\:\sqrt n}\right) $$ which is still independent from the initial value $x$ (assumed to be between 0 and π/2 for simplicity). I am using the method described in this paper to push it further : $$\frac1{sin^2(u_n)} - \frac1{u_n^2} = \frac13 + \frac{u_n^2}{15} + \frac{u_n^4}{189} + \frac{2\:u_n^4}{189} +o(u_n^5)  $$ Now we can replace $u_n^2$ by its approximation $ \frac n3+ \frac{\ln(n)}5 + o(\ln(n))$ and sum from 0 to n (with $u_0$ = x ) Which gives me : $$\frac1{\sin^n(x)^2}- \frac1{x^2} = \frac n3 + \frac{\ln (n)}5 + constant + o(\frac 1n)$$ and finally  $$\sin^n(x) = {\bigg[\frac n3 + \frac{\ln (n)}5 + \frac1{x^2} + constant + o(\frac 1n)\:\:\bigg]} ^{-1/2}$$ Where x appears after the term in $\ln(n)$ and fades out as $n^{-3/2}$. The constant involves Euler constant and other limits of convergent series. Q: Is this wrong ? Did I miss something ? Has anybody pushed the approximation of the iterated sine further ?,,"['calculus', 'sequences-and-series', 'limits', 'functions']"
12,Evaluate $\lim_{n \rightarrow \infty} \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$ [duplicate],Evaluate  [duplicate],\lim_{n \rightarrow \infty} \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n},This question already has answers here : How to prove that $\lim \frac{1}{n} \sqrt[n]{(n+1)(n+2)... 2n} = \frac{4}{e}$ (5 answers) Closed 4 years ago . Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ Attempt: Let $$y=\lim_{n \rightarrow \infty} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ $$\implies \log y = \lim_{n \rightarrow \infty} \dfrac {1} {n} [\log (n+1) +\cdots+log(n+n)-log(n)] $$ How do I move forward? Thank you very much for your help.,This question already has answers here : How to prove that $\lim \frac{1}{n} \sqrt[n]{(n+1)(n+2)... 2n} = \frac{4}{e}$ (5 answers) Closed 4 years ago . Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ Attempt: Let $$y=\lim_{n \rightarrow \infty} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ $$\implies \log y = \lim_{n \rightarrow \infty} \dfrac {1} {n} [\log (n+1) +\cdots+log(n+n)-log(n)] $$ How do I move forward? Thank you very much for your help.,,"['calculus', 'real-analysis', 'sequences-and-series']"
13,Limit with the Appell F1 function,Limit with the Appell F1 function,,"While attempting to solve this problem I ran into a nasty limit. Mathematica claims that the indefinite integral $$\int\left[\frac{1}{\sqrt{a+b}(z^2)^{n/4}}-\frac{1}{\sqrt{a+b\cos^2\theta}(R^2+z^2)^{n/4}}\right]\,dz $$ where $$\cos\theta = \frac{z}{\sqrt{z^2+R^2}}$$ is equal to $$-\frac{z \left(R^2+z^2\right)^{-\frac{n}{4}} \left(\frac{z^2}{R^2}+1\right)^{\frac{n+2}{4}} \sqrt{\frac{a \left(R^2+z^2\right)+b z^2}{R^2+z^2}}}{a (a+b) \sqrt{\frac{z^2 (a+b)}{a R^2}+1}}-\frac{2 z \left(z^2\right)^{-\frac{n}{4}}}{(n-2) \sqrt{a+b}}\cdot\left[b F_1\left(\frac{1}{2};\frac{n+2}{4},\frac{1}{2};\frac{3}{2};-\frac{z^2}{R^2},-\frac{(a+b) z^2}{a R^2}\right)+a F_1\left(\frac{1}{2};\frac{n+2}{4},-\frac{1}{2};\frac{3}{2};-\frac{z^2}{R^2},-\frac{(a+b) z^2}{a R^2}\right)\right]$$ With the restrictions $a>0$, $b>0$, $0<n<2$ and $R\ll 1$. We want the definite integral from $-\infty $ to $\infty$ and because the function is even we can take the difference of the limits at infinity and $0$. Mathematica evaluates to limit at $0$ to be equal to zero but isn't able to evaluate the upper limit. In an effort to make the arguments of the Appell functions converge to something I applied the following transformation ( citation ) $$F_1\left(a;b_1,b_2;c;z_1,z_2\right)\to\left(1-z_1\right){}^{-b_1} \left(1-z_2\right){}^{-b_2} F_1\left(c-a;b_1,b_2;c;\frac{z_1}{z_1-1},\frac{z_2}{z_2-1}\right)$$ Which simplifies to $$\lim_{z\to\infty}z\left(\frac{\left(R^2+z^2\right)^{-\frac{n}{4}} \sqrt{a+\frac{b z^2}{R^2+z^2}}}{(a+b) \left(z^2 (a+b)+a R^2\right)}+\frac{2 \left(z^2\right)^{-\frac{n}{4}}}{(n-2) \sqrt{a+b}}\left[b R^2 F_1\left(1;\frac{n+2}{4},\frac{1}{2};\frac{3}{2};\frac{z^2}{R^2+z^2},\frac{(a+b) z^2}{a R^2+(a+b) z^2}\right)+\left(z^2 (a+b)+a R^2\right) F_1\left(1;\frac{n+2}{4},-\frac{1}{2};\frac{3}{2};\frac{z^2}{R^2+z^2},\frac{(a+b) z^2}{a R^2+(a+b) z^2}\right)\right]\right)$$ The arguments now converge to 1, but mathematica claims that the limit is complex infinity which I know not to be true because the integral converges for the special cases that I have tried numerically. Where is my mistake? The transformation seems valid because the only restriction is that $$z_1, z_2\not\in(1, \infty)$$ which is clearly the case. Do I need more restrictions to ensure convergence? Edit: Using the comment from Pierpaolo Vivo the limit in question becomes $$\lim_{\zeta\to\infty}\left[\frac{\zeta  \sqrt{\frac{\zeta ^2}{\alpha ^2}+1}}{\sqrt{\alpha ^2+\zeta ^2}}+\frac{2 \zeta ^{1-\frac{n}{2}}}{n-2}F_1\left(\frac{1}{2};\frac{n-2}{4},\frac{1}{2};\frac{3}{2};-\zeta ^2,-\frac{\zeta ^2}{\alpha ^2}\right)\right]$$ When I apply the same transformation as above we obtain $$F_1\left(\frac{1}{2};\frac{n-2}{4},\frac{1}{2};\frac{3}{2};-\zeta ^2,-\frac{\zeta ^2}{\alpha ^2}\right)=\frac{\left(\zeta ^2+1\right)^{\frac{1}{2}-\frac{n}{4}}}{\sqrt{\frac{\zeta ^2}{\alpha ^2}+1}}F_1\left(1;\frac{n-2}{4},\frac{1}{2};\frac{3}{2};\frac{\zeta ^2}{\zeta ^2+1},\frac{\zeta ^2}{\alpha ^2+\zeta ^2}\right)$$ The limit now becomes $$\lim_{\zeta\to\infty}\left[\zeta  \left(\frac{\left(\zeta ^2+1\right)^{\frac{1}{2}-\frac{n}{4}} F_1\left(1;\frac{n-2}{4},\frac{1}{2};\frac{3}{2};\frac{\zeta ^2}{\zeta ^2+1},\frac{\zeta ^2}{\alpha ^2+\zeta ^2}\right)}{\sqrt{\alpha ^2+\zeta ^2}}+\frac{2 \zeta ^{-\frac{n}{2}}}{n-2}\right)\right]$$ Now mathematica evaluates the limit to be $0$ at both end points..... I feel like there has to be something flawed with these transformations but all the given criteria are met. What's going on here?","While attempting to solve this problem I ran into a nasty limit. Mathematica claims that the indefinite integral $$\int\left[\frac{1}{\sqrt{a+b}(z^2)^{n/4}}-\frac{1}{\sqrt{a+b\cos^2\theta}(R^2+z^2)^{n/4}}\right]\,dz $$ where $$\cos\theta = \frac{z}{\sqrt{z^2+R^2}}$$ is equal to $$-\frac{z \left(R^2+z^2\right)^{-\frac{n}{4}} \left(\frac{z^2}{R^2}+1\right)^{\frac{n+2}{4}} \sqrt{\frac{a \left(R^2+z^2\right)+b z^2}{R^2+z^2}}}{a (a+b) \sqrt{\frac{z^2 (a+b)}{a R^2}+1}}-\frac{2 z \left(z^2\right)^{-\frac{n}{4}}}{(n-2) \sqrt{a+b}}\cdot\left[b F_1\left(\frac{1}{2};\frac{n+2}{4},\frac{1}{2};\frac{3}{2};-\frac{z^2}{R^2},-\frac{(a+b) z^2}{a R^2}\right)+a F_1\left(\frac{1}{2};\frac{n+2}{4},-\frac{1}{2};\frac{3}{2};-\frac{z^2}{R^2},-\frac{(a+b) z^2}{a R^2}\right)\right]$$ With the restrictions $a>0$, $b>0$, $0<n<2$ and $R\ll 1$. We want the definite integral from $-\infty $ to $\infty$ and because the function is even we can take the difference of the limits at infinity and $0$. Mathematica evaluates to limit at $0$ to be equal to zero but isn't able to evaluate the upper limit. In an effort to make the arguments of the Appell functions converge to something I applied the following transformation ( citation ) $$F_1\left(a;b_1,b_2;c;z_1,z_2\right)\to\left(1-z_1\right){}^{-b_1} \left(1-z_2\right){}^{-b_2} F_1\left(c-a;b_1,b_2;c;\frac{z_1}{z_1-1},\frac{z_2}{z_2-1}\right)$$ Which simplifies to $$\lim_{z\to\infty}z\left(\frac{\left(R^2+z^2\right)^{-\frac{n}{4}} \sqrt{a+\frac{b z^2}{R^2+z^2}}}{(a+b) \left(z^2 (a+b)+a R^2\right)}+\frac{2 \left(z^2\right)^{-\frac{n}{4}}}{(n-2) \sqrt{a+b}}\left[b R^2 F_1\left(1;\frac{n+2}{4},\frac{1}{2};\frac{3}{2};\frac{z^2}{R^2+z^2},\frac{(a+b) z^2}{a R^2+(a+b) z^2}\right)+\left(z^2 (a+b)+a R^2\right) F_1\left(1;\frac{n+2}{4},-\frac{1}{2};\frac{3}{2};\frac{z^2}{R^2+z^2},\frac{(a+b) z^2}{a R^2+(a+b) z^2}\right)\right]\right)$$ The arguments now converge to 1, but mathematica claims that the limit is complex infinity which I know not to be true because the integral converges for the special cases that I have tried numerically. Where is my mistake? The transformation seems valid because the only restriction is that $$z_1, z_2\not\in(1, \infty)$$ which is clearly the case. Do I need more restrictions to ensure convergence? Edit: Using the comment from Pierpaolo Vivo the limit in question becomes $$\lim_{\zeta\to\infty}\left[\frac{\zeta  \sqrt{\frac{\zeta ^2}{\alpha ^2}+1}}{\sqrt{\alpha ^2+\zeta ^2}}+\frac{2 \zeta ^{1-\frac{n}{2}}}{n-2}F_1\left(\frac{1}{2};\frac{n-2}{4},\frac{1}{2};\frac{3}{2};-\zeta ^2,-\frac{\zeta ^2}{\alpha ^2}\right)\right]$$ When I apply the same transformation as above we obtain $$F_1\left(\frac{1}{2};\frac{n-2}{4},\frac{1}{2};\frac{3}{2};-\zeta ^2,-\frac{\zeta ^2}{\alpha ^2}\right)=\frac{\left(\zeta ^2+1\right)^{\frac{1}{2}-\frac{n}{4}}}{\sqrt{\frac{\zeta ^2}{\alpha ^2}+1}}F_1\left(1;\frac{n-2}{4},\frac{1}{2};\frac{3}{2};\frac{\zeta ^2}{\zeta ^2+1},\frac{\zeta ^2}{\alpha ^2+\zeta ^2}\right)$$ The limit now becomes $$\lim_{\zeta\to\infty}\left[\zeta  \left(\frac{\left(\zeta ^2+1\right)^{\frac{1}{2}-\frac{n}{4}} F_1\left(1;\frac{n-2}{4},\frac{1}{2};\frac{3}{2};\frac{\zeta ^2}{\zeta ^2+1},\frac{\zeta ^2}{\alpha ^2+\zeta ^2}\right)}{\sqrt{\alpha ^2+\zeta ^2}}+\frac{2 \zeta ^{-\frac{n}{2}}}{n-2}\right)\right]$$ Now mathematica evaluates the limit to be $0$ at both end points..... I feel like there has to be something flawed with these transformations but all the given criteria are met. What's going on here?",,"['calculus', 'integration', 'limits', 'definite-integrals', 'special-functions']"
14,Prove that $\lim_\limits{x\to 2}f(x)=3.$ [duplicate],Prove that  [duplicate],\lim_\limits{x\to 2}f(x)=3.,"This question already has answers here : Nowhere continuous function limit (3 answers) Proof that the Dirichlet function is discontinuous (4 answers) Closed 8 years ago . Let: $$f(x) = \begin{cases} 5-x,  & \text{if $x$ is irrational.} \\[2ex] 1+x, & \text{if $x$ is rational.} \end{cases}$$ Prove that $\lim_\limits{x\to 2}f(x)=3.$ Prove that $\lim_\limits{x\to 0}f(x) $ doesn't exist. Solution Attempt: I want to use the definition of continuity. by saying that for each each $\epsilon>0$ there is a $\delta>0$ s.t. $|x-2|<\delta$  $\Rightarrow$ $|f(x)-f(2)|<\epsilon$. and somehow I thought about using the qualities of $|f(x)|$ and try to get that an equation that might help me in the proof. but got stuck there. any help?","This question already has answers here : Nowhere continuous function limit (3 answers) Proof that the Dirichlet function is discontinuous (4 answers) Closed 8 years ago . Let: $$f(x) = \begin{cases} 5-x,  & \text{if $x$ is irrational.} \\[2ex] 1+x, & \text{if $x$ is rational.} \end{cases}$$ Prove that $\lim_\limits{x\to 2}f(x)=3.$ Prove that $\lim_\limits{x\to 0}f(x) $ doesn't exist. Solution Attempt: I want to use the definition of continuity. by saying that for each each $\epsilon>0$ there is a $\delta>0$ s.t. $|x-2|<\delta$  $\Rightarrow$ $|f(x)-f(2)|<\epsilon$. and somehow I thought about using the qualities of $|f(x)|$ and try to get that an equation that might help me in the proof. but got stuck there. any help?",,"['calculus', 'limits', 'continuity']"
15,"Computing the limit of this integral,","Computing the limit of this integral,",,"This is Part 6 (last part) of a problem statement of an old comprehensive exam question that I am working on. It asks to evaluate $$\lim_{r_0 \to 0} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ where $f(x,y)$ is a bounded and continuous function of $x$ and $y$. From a previous part of this problem, I computed the limit of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$, which is equal to $0$ for all $(x,y)\ne(0,0)$, and equal to $\infty$ at $(x,y) = (0,0)$. Also, I computed the integral of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$ inside the disk of radius $R$, centered at $(0,0)$, and got $\large \frac{kR^2}{R^2-r_0^2}$. So, now we see that the integral of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$ $\to$ $k$, as $r_0 \to 0$.  I.e., we get a bump of $k$ in a neighborhood of $(0,0)$. Now, back to the above limit that I want to evaluate.  A simple check of the integrand shows that we can apply the Dominated Convergence Theorem for $(x,y)\ne(0,0)$.  Then we have that $$\lim_{r_0 \to 0} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy =  \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\lim_{r_0 \to 0}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}0dxdy = 0 $$ for all $(x,y) \ne (0,0)$.  The limit then reduces to just evaluating $$\lim_{r_0 \to 0} \int \int_{B_\delta (0,0)} \frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ But I am stuck here and don't know how to show it.  I think I need to somehow use the continuity and boundedness of $f$. A student solution claims the answer is $f(0,0)k$, but I do not know how to arrive at this number yet. Any ideas are welcome. Thanks,","This is Part 6 (last part) of a problem statement of an old comprehensive exam question that I am working on. It asks to evaluate $$\lim_{r_0 \to 0} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ where $f(x,y)$ is a bounded and continuous function of $x$ and $y$. From a previous part of this problem, I computed the limit of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$, which is equal to $0$ for all $(x,y)\ne(0,0)$, and equal to $\infty$ at $(x,y) = (0,0)$. Also, I computed the integral of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$ inside the disk of radius $R$, centered at $(0,0)$, and got $\large \frac{kR^2}{R^2-r_0^2}$. So, now we see that the integral of $\large \frac{k(r_0)^2}{\pi(x^2+y^2+r_0^2)}$ $\to$ $k$, as $r_0 \to 0$.  I.e., we get a bump of $k$ in a neighborhood of $(0,0)$. Now, back to the above limit that I want to evaluate.  A simple check of the integrand shows that we can apply the Dominated Convergence Theorem for $(x,y)\ne(0,0)$.  Then we have that $$\lim_{r_0 \to 0} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy =  \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}\lim_{r_0 \to 0}\frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ $$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}0dxdy = 0 $$ for all $(x,y) \ne (0,0)$.  The limit then reduces to just evaluating $$\lim_{r_0 \to 0} \int \int_{B_\delta (0,0)} \frac{f(x,y)k(r_0)^2}{\pi(x^2+y^2+r_0^2)}dxdy$$ But I am stuck here and don't know how to show it.  I think I need to somehow use the continuity and boundedness of $f$. A student solution claims the answer is $f(0,0)k$, but I do not know how to arrive at this number yet. Any ideas are welcome. Thanks,",,"['calculus', 'real-analysis', 'integration', 'limits', 'dirac-delta']"
16,How to prove that $\lim_{n \to\infty} \frac{(2n-1)!!}{(2n)!!}=0$,How to prove that,\lim_{n \to\infty} \frac{(2n-1)!!}{(2n)!!}=0,"So guys, how can I evaluate and prove that $$\lim_{n \to\infty} \frac{(2n-1)!!}{(2n)!!}=0.$$ Any ideas are welcomed. $n!!$ is the double factorial, as explained in this wolfram post.","So guys, how can I evaluate and prove that $$\lim_{n \to\infty} \frac{(2n-1)!!}{(2n)!!}=0.$$ Any ideas are welcomed. $n!!$ is the double factorial, as explained in this wolfram post.",,"['calculus', 'limits', 'factorial']"
17,Limit with fractional part and greatest integer part,Limit with fractional part and greatest integer part,,"Find $$\lim_{x\rightarrow 2-} \{x+(x-[x]^2)\}$$ For $x\to 2-$, $[x]=1$, i.e $[x]^2=1$, so $\lim_{x\rightarrow 2-} \{x+(x-1)\}=\lim_{x\rightarrow 2-} \{2x-1\}=3.$ I don't know whether $\{\}$ symbolizes fractional part=$x-[x]$ or not for this particular question. I have assumed  $\{\}$ not as fractional part and solved. Please check my solution is correct or not. In case $\{\}$ symbolizes fractional part=$x-[x]$, then what will be the answer?","Find $$\lim_{x\rightarrow 2-} \{x+(x-[x]^2)\}$$ For $x\to 2-$, $[x]=1$, i.e $[x]^2=1$, so $\lim_{x\rightarrow 2-} \{x+(x-1)\}=\lim_{x\rightarrow 2-} \{2x-1\}=3.$ I don't know whether $\{\}$ symbolizes fractional part=$x-[x]$ or not for this particular question. I have assumed  $\{\}$ not as fractional part and solved. Please check my solution is correct or not. In case $\{\}$ symbolizes fractional part=$x-[x]$, then what will be the answer?",,"['calculus', 'limits']"
18,Is this a valid technique when calculating limits,Is this a valid technique when calculating limits,,"I recently came across the limit \begin{equation} \lim_{x \to 0} \frac{\sin(\tan^2 x)}{1 - \cos x} \end{equation} Now I know this limit can be evaluated using the half angle identity or by l'Hopital's rule but I recently tried a method that yielded the correct answer but I don't know if the method is always correct or if it's even mathematically valid. It goes as follows. \begin{equation} \lim_{x \to 0} \frac{\sin(\tan^2x)}{1 - \cos x} = \lim_{x \to 0} \frac{\sin(\sec^2 x - 1)}{1 - \cos x} \end{equation} at this point I realized I can't simply substitute $x = 0$. My intuition behind this was that although $\sec^2(0) = \sec(0)$, the functions grow at different rates, so when used in a ratio, the $\sec^2x$ answer would be greater than the $\sec x$ answer. So I made the substitution $\lim_{x \to 0} \sec x = \lim_{h \to 0} 1 + h$. This would change the limit to \begin{align} \lim_{h \to 0} \frac{\sin((1 + h)^2 - 1)}{1 - \frac{1}{1 + h}} &= \lim_{h \to 0} \frac{(1 + h)\sin(h^2 + 2h)}{h}\\ &= \lim_{h \to 0} \frac{(2 + h)(1 + h)\sin(h^2 + 2h)}{h^2 + 2h}\\ &= \lim_{h \to 0} (2 + h)(1 + h)\frac{\sin(h^2 + 2h)}{h^2 + 2h} = 2 \end{align} So is this method vlid, or was I just lucky and it happend to work for this case but would fail for most others? I originally had $h$ as an infinitesimal when I first tried this.","I recently came across the limit \begin{equation} \lim_{x \to 0} \frac{\sin(\tan^2 x)}{1 - \cos x} \end{equation} Now I know this limit can be evaluated using the half angle identity or by l'Hopital's rule but I recently tried a method that yielded the correct answer but I don't know if the method is always correct or if it's even mathematically valid. It goes as follows. \begin{equation} \lim_{x \to 0} \frac{\sin(\tan^2x)}{1 - \cos x} = \lim_{x \to 0} \frac{\sin(\sec^2 x - 1)}{1 - \cos x} \end{equation} at this point I realized I can't simply substitute $x = 0$. My intuition behind this was that although $\sec^2(0) = \sec(0)$, the functions grow at different rates, so when used in a ratio, the $\sec^2x$ answer would be greater than the $\sec x$ answer. So I made the substitution $\lim_{x \to 0} \sec x = \lim_{h \to 0} 1 + h$. This would change the limit to \begin{align} \lim_{h \to 0} \frac{\sin((1 + h)^2 - 1)}{1 - \frac{1}{1 + h}} &= \lim_{h \to 0} \frac{(1 + h)\sin(h^2 + 2h)}{h}\\ &= \lim_{h \to 0} \frac{(2 + h)(1 + h)\sin(h^2 + 2h)}{h^2 + 2h}\\ &= \lim_{h \to 0} (2 + h)(1 + h)\frac{\sin(h^2 + 2h)}{h^2 + 2h} = 2 \end{align} So is this method vlid, or was I just lucky and it happend to work for this case but would fail for most others? I originally had $h$ as an infinitesimal when I first tried this.",,"['limits', 'limits-without-lhopital']"
19,Evaluate $\lim_\limits{x \to 0}\frac{x}{\sqrt[n]{1+ax} \cdot \sqrt[k]{1+bx} -1}$,Evaluate,\lim_\limits{x \to 0}\frac{x}{\sqrt[n]{1+ax} \cdot \sqrt[k]{1+bx} -1},"For all $n,k \in N a,b > 0$ $$\lim_{x \to 0}\frac{x}{\sqrt[n]{1+ax} \cdot \sqrt[k]{1+bx} -1} = \lim_{x \to 0}\frac{x}{(1+ \frac{ax}{n})(1+ \frac{bx}{k})- 1}= \lim_{x \to 0}\frac{x}{x(\frac{a}{n} + \frac{b}{k}) + \frac{ab}{nk}x^2} = \lim_{x \to 0}\frac{1}{\frac{a}{n} + \frac{b}{k}} = \frac{nk}{ka+nb}$$  Am I right?","For all $n,k \in N a,b > 0$ $$\lim_{x \to 0}\frac{x}{\sqrt[n]{1+ax} \cdot \sqrt[k]{1+bx} -1} = \lim_{x \to 0}\frac{x}{(1+ \frac{ax}{n})(1+ \frac{bx}{k})- 1}= \lim_{x \to 0}\frac{x}{x(\frac{a}{n} + \frac{b}{k}) + \frac{ab}{nk}x^2} = \lim_{x \to 0}\frac{1}{\frac{a}{n} + \frac{b}{k}} = \frac{nk}{ka+nb}$$  Am I right?",,['limits']
20,"Find $\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}! $ where $x_n=x_{n-1}+x_{n-2},x_1=1,x_2=2$",Find  where,"\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}!  x_n=x_{n-1}+x_{n-2},x_1=1,x_2=2","Find $\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}! $ where $x_n=x_{n-1}+x_{n-2} ,(n>2),x_1=1,x_2=2$ $x_n=x_{n-1}+x_{n-2}$ $x_{n+1}=x_{n}+x_{n-1}$ From the first recurrence relation, $$x_n=\frac{3+\sqrt{5}}{5+\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^n+\frac{3-\sqrt{5}}{5-\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^n$$ From the second recurrence relation, $$x_{n+1}=c_2({x_1}^{'})^{n+1}+c_3({x_2}^{'})^{n+1}$$ where  $${x_2}^{'}=\frac{1+\sqrt{5}}{2},{x_3}^{'}=\frac{1-\sqrt{5}}{2}$$  $$c_2=\frac{x_3-{x_3}^{'}x_2}{{x_2}^{'}({x_2}^{'}-{x_3}^{'})}=\frac{2(2+\sqrt{5})}{5+\sqrt{5}}$$ $$c_3=-\frac{x_3-{x_2}^{'}x_2}{{x_3}^{'}({x_2}^{'}-{x_3}^{'})}=\frac{2(2-\sqrt{5})}{5-\sqrt{5}}$$ gives  $$x_{n+1}=\frac{2(2+\sqrt{5})}{5+\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{n+1}+\frac{2(2-\sqrt{5})}{5-\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{n+1}$$ This gives $$\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}! =\frac{2(2+\sqrt{5})}{3+\sqrt{5}}!$$ Is this correct?","Find $\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}! $ where $x_n=x_{n-1}+x_{n-2} ,(n>2),x_1=1,x_2=2$ $x_n=x_{n-1}+x_{n-2}$ $x_{n+1}=x_{n}+x_{n-1}$ From the first recurrence relation, $$x_n=\frac{3+\sqrt{5}}{5+\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^n+\frac{3-\sqrt{5}}{5-\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^n$$ From the second recurrence relation, $$x_{n+1}=c_2({x_1}^{'})^{n+1}+c_3({x_2}^{'})^{n+1}$$ where  $${x_2}^{'}=\frac{1+\sqrt{5}}{2},{x_3}^{'}=\frac{1-\sqrt{5}}{2}$$  $$c_2=\frac{x_3-{x_3}^{'}x_2}{{x_2}^{'}({x_2}^{'}-{x_3}^{'})}=\frac{2(2+\sqrt{5})}{5+\sqrt{5}}$$ $$c_3=-\frac{x_3-{x_2}^{'}x_2}{{x_3}^{'}({x_2}^{'}-{x_3}^{'})}=\frac{2(2-\sqrt{5})}{5-\sqrt{5}}$$ gives  $$x_{n+1}=\frac{2(2+\sqrt{5})}{5+\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{n+1}+\frac{2(2-\sqrt{5})}{5-\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{n+1}$$ This gives $$\lim\limits_{n\to\infty}\frac{x_{n+1}}{x_n}! =\frac{2(2+\sqrt{5})}{3+\sqrt{5}}!$$ Is this correct?",,"['limits', 'proof-verification', 'recurrence-relations']"
21,"Prove $x^2$ is continuous on the interval $[0,1]$",Prove  is continuous on the interval,"x^2 [0,1]","I realize this may be a simple question, but I am having trouble proving proving this, mostly with selecting a suitable delta. My attempt so far: $x^2$ is continuous on $[0,1]$ if $\forall\epsilon>0,\exists\delta>0$ such that $$0<\vert x-a\vert<\delta\,x\in [0,1]\Rightarrow\vert x^2-a^2\vert<\epsilon$$ for all $a\in [0,1]$. Since $a$ and $x$ are in $[0,1]$, the maximum value of $a+x$ is 2. Take $\delta=\min\{1,\frac{\epsilon}{2}\}$. Then $$0<\vert x-a\vert<\delta\Rightarrow\vert x^2-a^2\vert=\vert x-a\vert\vert x+a\vert<2\vert x-a\vert=2\delta\leq\epsilon$$ I think my logic makes sense but I am unsure about my step involving $\delta$. Is this a correct proof? If not, how would I improve it?","I realize this may be a simple question, but I am having trouble proving proving this, mostly with selecting a suitable delta. My attempt so far: $x^2$ is continuous on $[0,1]$ if $\forall\epsilon>0,\exists\delta>0$ such that $$0<\vert x-a\vert<\delta\,x\in [0,1]\Rightarrow\vert x^2-a^2\vert<\epsilon$$ for all $a\in [0,1]$. Since $a$ and $x$ are in $[0,1]$, the maximum value of $a+x$ is 2. Take $\delta=\min\{1,\frac{\epsilon}{2}\}$. Then $$0<\vert x-a\vert<\delta\Rightarrow\vert x^2-a^2\vert=\vert x-a\vert\vert x+a\vert<2\vert x-a\vert=2\delta\leq\epsilon$$ I think my logic makes sense but I am unsure about my step involving $\delta$. Is this a correct proof? If not, how would I improve it?",,"['real-analysis', 'limits', 'proof-verification', 'continuity']"
22,"Converting a series to a Riemann sum,","Converting a series to a Riemann sum,",,"I am manipulating a series and have gotten this far: $$ \lim_{n\to\infty} \sum_{m=1}^n \frac {(\frac{m}{n})^{p-1}}{1+ (\frac{m}{n})^p} \frac{1}{n}$$ I want to now say that this is a Riemann sum, which corresponds to the Riemann integral $$ \int_0^1 \frac {x^{p-1}}{1+x^p}dx$$ which evaluates to ln(2)/p -- the answer I am looking for. Have I converted to the integral correctly? My concerns are:  I just relabeled the discrete variable ($m/n$) as a continuous variable $x$.  Also I converted the $1/n$ interval length to $dx$ -- this part I am ok with, just from checking the definition of Riemann integral and knowing that I have to let mesh($p$) = supremum of sub-interval lengths go to zero. So, I sort of did it blindly but I would like to know what are the rules that I may have missed?  E.g., does the variable ($m/n$) also have some size limits to adhere to?  Does ($m/n$) have to be small? Thanks, EDIT: The main concern really is:  in past problems, I converted to Riemann integrals that did not correspond to the Riemann sums that I was working with.  An MSE contributor gave me the heads up -- and showed that my corresponding ""Riemann integrals"" actually had values different from the sums.  But I used the same ""rules"" as I did with this current problem statement.  Apparently, it was incorrect.  So, I just want to know whether I have converted this sum to an integral  correctly.","I am manipulating a series and have gotten this far: $$ \lim_{n\to\infty} \sum_{m=1}^n \frac {(\frac{m}{n})^{p-1}}{1+ (\frac{m}{n})^p} \frac{1}{n}$$ I want to now say that this is a Riemann sum, which corresponds to the Riemann integral $$ \int_0^1 \frac {x^{p-1}}{1+x^p}dx$$ which evaluates to ln(2)/p -- the answer I am looking for. Have I converted to the integral correctly? My concerns are:  I just relabeled the discrete variable ($m/n$) as a continuous variable $x$.  Also I converted the $1/n$ interval length to $dx$ -- this part I am ok with, just from checking the definition of Riemann integral and knowing that I have to let mesh($p$) = supremum of sub-interval lengths go to zero. So, I sort of did it blindly but I would like to know what are the rules that I may have missed?  E.g., does the variable ($m/n$) also have some size limits to adhere to?  Does ($m/n$) have to be small? Thanks, EDIT: The main concern really is:  in past problems, I converted to Riemann integrals that did not correspond to the Riemann sums that I was working with.  An MSE contributor gave me the heads up -- and showed that my corresponding ""Riemann integrals"" actually had values different from the sums.  But I used the same ""rules"" as I did with this current problem statement.  Apparently, it was incorrect.  So, I just want to know whether I have converted this sum to an integral  correctly.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'riemann-sum']"
23,Limit and Taylor Series when non-differentiable,Limit and Taylor Series when non-differentiable,,"I am stuck on a problem similar to this one.  Define $$f(\theta,y)=\frac{g(\theta y)}{\int_0^1 g(\theta x)dx}$$ with $g(0)=0$ and $\lim_{t \to 0}g'(t)=+\infty$. I am interested in $\lim_{\theta \to 0}f(\theta,y)$. If $g'(0)$ were real, I would write: $$g(\theta y)=\theta y g'(0)+o(\theta)$$ and $$\int_0^1 g(\theta x)dx=\theta\int_0^1 xg'(0)dx+o(\theta)=\frac{1}{2} \theta g'(0) + o(\theta)$$ Then, for $g'(0)\ne0$, one can show that $$f(\theta,y)=\frac{2y+o(1)}{1+o(1)}$$  and $\lim_{\theta \to 0}f(\theta,y)=2y$. However, I think it is wrong to write these equations when $\lim_{t \to 0}g'(t)=+\infty$.  Can someone give me some insights on how to prove the result in this case? A result just showing that the limit is real would be enough.","I am stuck on a problem similar to this one.  Define $$f(\theta,y)=\frac{g(\theta y)}{\int_0^1 g(\theta x)dx}$$ with $g(0)=0$ and $\lim_{t \to 0}g'(t)=+\infty$. I am interested in $\lim_{\theta \to 0}f(\theta,y)$. If $g'(0)$ were real, I would write: $$g(\theta y)=\theta y g'(0)+o(\theta)$$ and $$\int_0^1 g(\theta x)dx=\theta\int_0^1 xg'(0)dx+o(\theta)=\frac{1}{2} \theta g'(0) + o(\theta)$$ Then, for $g'(0)\ne0$, one can show that $$f(\theta,y)=\frac{2y+o(1)}{1+o(1)}$$  and $\lim_{\theta \to 0}f(\theta,y)=2y$. However, I think it is wrong to write these equations when $\lim_{t \to 0}g'(t)=+\infty$.  Can someone give me some insights on how to prove the result in this case? A result just showing that the limit is real would be enough.",,"['limits', 'derivatives', 'taylor-expansion']"
24,How to calculate the limit of a recursively defined sequence?,How to calculate the limit of a recursively defined sequence?,,"My attempt: (i) For r=1, $x_n$ = n+1 $\iff$ $\frac{n+1}{1+1/n}$ = $x_{n-1}$ $\iff$ $\frac{n(n+1)}{n+1}$ = $x_{n-1}$ $\iff$ n = $x_{n-1}$ But I don't know how to use what I have shown so far to show that $x_n$ = n+1 (ii) $$\lim_{n\to\infty} x_n = (r+0) \ell = r\ell $$ I know this is the wrong answer, but I'm not sure what else to try.","My attempt: (i) For r=1, $x_n$ = n+1 $\iff$ $\frac{n+1}{1+1/n}$ = $x_{n-1}$ $\iff$ $\frac{n(n+1)}{n+1}$ = $x_{n-1}$ $\iff$ n = $x_{n-1}$ But I don't know how to use what I have shown so far to show that $x_n$ = n+1 (ii) $$\lim_{n\to\infty} x_n = (r+0) \ell = r\ell $$ I know this is the wrong answer, but I'm not sure what else to try.",,"['limits', 'recursion']"
25,"Trying to prove that a function got no limit at $(0,0)$",Trying to prove that a function got no limit at,"(0,0)","Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$,   defined by: $$ f(x,y)=\begin{cases} 1 & y=x^{2}\\ 0 & \text{otherwise} \end{cases} $$ How can I show that this function got no limit at $(0,0)$? Can I define a sequence $x_n = \frac{1}{n}$ and get: $\lim_{n\to\infty}f(x_n,(x_n)^2)=1$ so the limit is $1$, but if I will take $\lim_{n\to\infty} f(x_n,0) = 0$ ? Is this a valid proof? Thank you!","Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$,   defined by: $$ f(x,y)=\begin{cases} 1 & y=x^{2}\\ 0 & \text{otherwise} \end{cases} $$ How can I show that this function got no limit at $(0,0)$? Can I define a sequence $x_n = \frac{1}{n}$ and get: $\lim_{n\to\infty}f(x_n,(x_n)^2)=1$ so the limit is $1$, but if I will take $\lim_{n\to\infty} f(x_n,0) = 0$ ? Is this a valid proof? Thank you!",,"['limits', 'multivariable-calculus', 'continuity']"
26,How does $\ln(x)$ blow up at $0$ and $\infty$.,How does  blow up at  and .,\ln(x) 0 \infty,"In general: How do I figure out how fast a function blows up at a certain point or infinity? How fast does $\ln x$ blow up at $0$? Does it blow up as fast as $1/x$, $1/x^2$, or maybe faster than any $1/x^n$? How do I answer such a question about other types of functions? How do I answer such a question for infinity? I know that $\ln x$ blows up very slowly at infinity, slower than any $x^n, n>0$. How do I justify this information? Edit: After some thinking I realized that lnx is e^x flipped about y=x. This means lnx is asymptotic to the -y axis as e^-x is asymptotic to +x axis. This is an intuitive justification of lnx blowing up slower than any x^-n, n>0","In general: How do I figure out how fast a function blows up at a certain point or infinity? How fast does $\ln x$ blow up at $0$? Does it blow up as fast as $1/x$, $1/x^2$, or maybe faster than any $1/x^n$? How do I answer such a question about other types of functions? How do I answer such a question for infinity? I know that $\ln x$ blows up very slowly at infinity, slower than any $x^n, n>0$. How do I justify this information? Edit: After some thinking I realized that lnx is e^x flipped about y=x. This means lnx is asymptotic to the -y axis as e^-x is asymptotic to +x axis. This is an intuitive justification of lnx blowing up slower than any x^-n, n>0",,"['limits', 'logarithms', 'infinity']"
27,How do I find the finite limits of this infinite product?,How do I find the finite limits of this infinite product?,,"What is... $$\lim_{\omega \to \infty}  \left( {1 \over {a^{\omega}}} \cdot  \prod_{N=1}^{\omega} (1+e^{b \cdot c^{-N}}) \right)$$ I'd like closed form solutions, and in this case that means any solution. So you don't need to concern yourself over ""elementary"" solutions. (although I'd prefer that). As I discuss below, the values that are infinite or zero are already known, I'd like expressions for finite values only... What I know: I made a bounty on this question. I got a proof that showed that the limit of the above equation is infinite for $1 \lt a=c \lt  2$ and 0 for $a=c \gt 2$. So knowing the limit is volatile, I've made a generalization to find the analytical methods to get the finite values from this equation. My attempt: I have absolutely no clue except for the case of $a=2 \ $, $b=1$ and  $c=2$, which is just a horrible coincidence, considering that its the only finite non zero limit preserved in the proof given in the link, which does generalize to any b... Create a line integral over the unit line evaluated with a uniform measure... $$\int_L e^x d \mu=\int_{L/2} e^x \ d\mu+\int_{L/2} e^{x+1/2} \ d\mu$$ This identity should be evident by self-similarity. Prepare for recursion... $$\int_L e^x d \mu=\int_{L/2} e^x+e^{x+1/2} \ d\mu=(1+e^{1/2}) \cdot \int_{L/2} e^x \ d\mu$$ $$\Rightarrow \int_L e^x d \mu=(1+e^{1/2}) \cdot \left( \int_{L/4} e^x \ d\mu+\int_{L/4} e^{x+1/4} \ d\mu \right)$$ $$\Rightarrow \int_L e^x d \mu=(1+e^{1/2}) \cdot (1+e^{1/4}) \cdot \left( \int_{L/4} e^x \ d\mu \right)$$ It wouldn't be hard to prove by induction then that... $$\Rightarrow \int_L e^x d \mu=\lim_{\omega \to \infty}  \left(\prod_{N=1}^{\omega} (1+e^{2^{-N}}) \cdot \int_{L/{2^{\omega}}} e^x \ d\mu \right)$$ Yet we know what the left hand side equals, since it can be evaluated as a definite integral, also we know what the integral on the right equals. Since the measure is uniform and the number of values x will be allowed to take on the interval decreases to just the value, namely $0$... $$e-1= \lim_{\omega \to \infty}  \left( {1 \over {2^{\omega}}} \cdot \prod_{N=1}^{\omega} (1+e^{2^{-N}}) \right)$$ Motivation: Getting an answer will allow me to derive methods to integrate a function like $e^x$ over fractals. As I hinted at above, its easy to find the numerical solution to the above limit. For instance for $a=2 \ $ , $b=2$ , $c=3$, the limit is $1.753$...","What is... $$\lim_{\omega \to \infty}  \left( {1 \over {a^{\omega}}} \cdot  \prod_{N=1}^{\omega} (1+e^{b \cdot c^{-N}}) \right)$$ I'd like closed form solutions, and in this case that means any solution. So you don't need to concern yourself over ""elementary"" solutions. (although I'd prefer that). As I discuss below, the values that are infinite or zero are already known, I'd like expressions for finite values only... What I know: I made a bounty on this question. I got a proof that showed that the limit of the above equation is infinite for $1 \lt a=c \lt  2$ and 0 for $a=c \gt 2$. So knowing the limit is volatile, I've made a generalization to find the analytical methods to get the finite values from this equation. My attempt: I have absolutely no clue except for the case of $a=2 \ $, $b=1$ and  $c=2$, which is just a horrible coincidence, considering that its the only finite non zero limit preserved in the proof given in the link, which does generalize to any b... Create a line integral over the unit line evaluated with a uniform measure... $$\int_L e^x d \mu=\int_{L/2} e^x \ d\mu+\int_{L/2} e^{x+1/2} \ d\mu$$ This identity should be evident by self-similarity. Prepare for recursion... $$\int_L e^x d \mu=\int_{L/2} e^x+e^{x+1/2} \ d\mu=(1+e^{1/2}) \cdot \int_{L/2} e^x \ d\mu$$ $$\Rightarrow \int_L e^x d \mu=(1+e^{1/2}) \cdot \left( \int_{L/4} e^x \ d\mu+\int_{L/4} e^{x+1/4} \ d\mu \right)$$ $$\Rightarrow \int_L e^x d \mu=(1+e^{1/2}) \cdot (1+e^{1/4}) \cdot \left( \int_{L/4} e^x \ d\mu \right)$$ It wouldn't be hard to prove by induction then that... $$\Rightarrow \int_L e^x d \mu=\lim_{\omega \to \infty}  \left(\prod_{N=1}^{\omega} (1+e^{2^{-N}}) \cdot \int_{L/{2^{\omega}}} e^x \ d\mu \right)$$ Yet we know what the left hand side equals, since it can be evaluated as a definite integral, also we know what the integral on the right equals. Since the measure is uniform and the number of values x will be allowed to take on the interval decreases to just the value, namely $0$... $$e-1= \lim_{\omega \to \infty}  \left( {1 \over {2^{\omega}}} \cdot \prod_{N=1}^{\omega} (1+e^{2^{-N}}) \right)$$ Motivation: Getting an answer will allow me to derive methods to integrate a function like $e^x$ over fractals. As I hinted at above, its easy to find the numerical solution to the above limit. For instance for $a=2 \ $ , $b=2$ , $c=3$, the limit is $1.753$...",,"['limits', 'closed-form', 'infinite-product']"
28,Taylor expansion of a power function,Taylor expansion of a power function,,"I was wondering about Taylor expansions of functions of the form $x^p$, where p is a real number, about $x = 0$. It seems clear how to do it about any other point, but what happens to the series as I approach 0? What specifically does ""break down"" in the expansion? As a simple example, I was looking at $y(x) = (1-x^3)^{1/3}$, where the cube root has the usual property that $(-x)^{1/3} = -x$. When I look at the point $x=1$ and the first derivative there, I get division by zero. The function for $x = 1+\epsilon$ looks like $-3^{1/3} \epsilon^{1/3}$, hence I get $-3^{-2/3} \epsilon^{-2/3}$ for the derivative. How to deal with that? Thanks for your help. SSF","I was wondering about Taylor expansions of functions of the form $x^p$, where p is a real number, about $x = 0$. It seems clear how to do it about any other point, but what happens to the series as I approach 0? What specifically does ""break down"" in the expansion? As a simple example, I was looking at $y(x) = (1-x^3)^{1/3}$, where the cube root has the usual property that $(-x)^{1/3} = -x$. When I look at the point $x=1$ and the first derivative there, I get division by zero. The function for $x = 1+\epsilon$ looks like $-3^{1/3} \epsilon^{1/3}$, hence I get $-3^{-2/3} \epsilon^{-2/3}$ for the derivative. How to deal with that? Thanks for your help. SSF",,"['limits', 'derivatives', 'polynomials', 'taylor-expansion']"
29,Limit doesn't exist,Limit doesn't exist,,"I know for a limit to exist on a LH/RH function they must be equal.  So to prove that the limit doesn't exist in this situation can I just do what I have done? Edit, I think I need to make it more like this which is by the 3.14 Theorem????","I know for a limit to exist on a LH/RH function they must be equal.  So to prove that the limit doesn't exist in this situation can I just do what I have done? Edit, I think I need to make it more like this which is by the 3.14 Theorem????",,['limits']
30,Proof of L'Hopital's Rule on infinity,Proof of L'Hopital's Rule on infinity,,"Suppose $f,g: (a,+\infty) \rightarrow \mathbb{R}$ are continuous and differentiable with $f(x) \rightarrow 0$ and $g(x) \rightarrow 0$ as $x \rightarrow \infty$. If $g'(x) \neq 0$ on $(a, +\infty$ and $\frac{f'(x)}{g'(x)} \rightarrow l$ as $x \rightarrow \infty$, then $\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)}=l$. Prove this theorem by applying L'Hopital's Rule (in the case where $x \rightarrow a$) to $f(1/x)/g(1/x)$. I know that someone has already asked this question However, I am still confused by the explanation given. Specifically, could someone answer two questions: (1) are we dealing with a direct application of the LHR, i.e. we alter the representation of our functions to allow for a direct application, and (2) why is that $\lim_{x \rightarrow 0}f(1/x)=\lim_{u\rightarrow \infty}f(u)=0$","Suppose $f,g: (a,+\infty) \rightarrow \mathbb{R}$ are continuous and differentiable with $f(x) \rightarrow 0$ and $g(x) \rightarrow 0$ as $x \rightarrow \infty$. If $g'(x) \neq 0$ on $(a, +\infty$ and $\frac{f'(x)}{g'(x)} \rightarrow l$ as $x \rightarrow \infty$, then $\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)}=l$. Prove this theorem by applying L'Hopital's Rule (in the case where $x \rightarrow a$) to $f(1/x)/g(1/x)$. I know that someone has already asked this question However, I am still confused by the explanation given. Specifically, could someone answer two questions: (1) are we dealing with a direct application of the LHR, i.e. we alter the representation of our functions to allow for a direct application, and (2) why is that $\lim_{x \rightarrow 0}f(1/x)=\lim_{u\rightarrow \infty}f(u)=0$",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
31,Can we always exploit Taylor's approximation in order to avoid using l'Hopital rule?,Can we always exploit Taylor's approximation in order to avoid using l'Hopital rule?,,"We are given two  functions, namely: $f, g: U^\circ\subset \mathbb{R}\to \mathbb{R}$ where $0 \in \overline{U}$. Suppose you want to compute the limit  $$\lim_{x \to 0} \frac{f(x)}{g(x)}$$ and that $\lim_{x \to 0} f(x) = \lim_{x\to 0} g(x) = 0$.  In addition, suppose that l'Hopital's theorem hypothesis are satisfied and that exploiting l'Hopital rule we found that $$\lim_{x \to 0} \frac{f'(x)}{g'(x)} = L \in \{\mathbb{R}, \pm\infty\}.$$ Can the same result be obtained using Taylor's approximation? And if we consider instead $\lim_{x \to +\infty} \frac{f(x)}{g(x)}$ anything changes? My speculations and question's motivation follow. In order  to apply Taylor's approximation one has to choose a function, an approximation point and an order of approximation. Taylor's formula gives us two objects: a polynomial and a reminder. But the only thing we know about the reminder is its behavior near the approximation point. So in order to apply Taylor to the functions $f,g$ we have to  choose  0 as approximation point (otherwise how can we get rid of the reminder?). It follows that $f,g$  have to be defined in $0$. This can be problematic, for example if $f(x) = \log(x) $ and $g(x) = x^{-1}$ we can apply l'Hopital's rule and get to $$ \frac{-x^2}{x} \to 0.$$ But we are unable to expand $f$ or $g$ near $0$ as them are not defined in $0$. So it seems to me that Taylor  method alone cannot cover all cases of interest. But I must  be wrong since I've read Why are people so interested in finding limits without l'Hôpital's rule? and I figured out that many people rely only on Taylor's method (which is a beautiful thing in my opinion) and that in some universities L'Hopital's rule use is even discouraged.","We are given two  functions, namely: $f, g: U^\circ\subset \mathbb{R}\to \mathbb{R}$ where $0 \in \overline{U}$. Suppose you want to compute the limit  $$\lim_{x \to 0} \frac{f(x)}{g(x)}$$ and that $\lim_{x \to 0} f(x) = \lim_{x\to 0} g(x) = 0$.  In addition, suppose that l'Hopital's theorem hypothesis are satisfied and that exploiting l'Hopital rule we found that $$\lim_{x \to 0} \frac{f'(x)}{g'(x)} = L \in \{\mathbb{R}, \pm\infty\}.$$ Can the same result be obtained using Taylor's approximation? And if we consider instead $\lim_{x \to +\infty} \frac{f(x)}{g(x)}$ anything changes? My speculations and question's motivation follow. In order  to apply Taylor's approximation one has to choose a function, an approximation point and an order of approximation. Taylor's formula gives us two objects: a polynomial and a reminder. But the only thing we know about the reminder is its behavior near the approximation point. So in order to apply Taylor to the functions $f,g$ we have to  choose  0 as approximation point (otherwise how can we get rid of the reminder?). It follows that $f,g$  have to be defined in $0$. This can be problematic, for example if $f(x) = \log(x) $ and $g(x) = x^{-1}$ we can apply l'Hopital's rule and get to $$ \frac{-x^2}{x} \to 0.$$ But we are unable to expand $f$ or $g$ near $0$ as them are not defined in $0$. So it seems to me that Taylor  method alone cannot cover all cases of interest. But I must  be wrong since I've read Why are people so interested in finding limits without l'Hôpital's rule? and I figured out that many people rely only on Taylor's method (which is a beautiful thing in my opinion) and that in some universities L'Hopital's rule use is even discouraged.",,"['limits', 'taylor-expansion', 'limits-without-lhopital']"
32,"Finding $\lim_{x\to0}\int_0^1\frac{xf(t)}{x^2+t^2}\,dt$",Finding,"\lim_{x\to0}\int_0^1\frac{xf(t)}{x^2+t^2}\,dt","Let $f$ be a continuous function and $f:[0,1]\to\mathbb R$ . Find $$\lim_{x\to0}\int_0^1\frac{xf(t)}{x^2+t^2}\,dt$$ I am finding that the limit does not exist. But the question is stated in a way to enable one to think that the limit actually exists . But I don't think the limit exists, the reason being simple: By Mean Value Theorem we can write the integral as $$xf(c)\int_0^1\frac{dt}{x^2+t^2}=f(c)\tan^{-1}\left(\frac{1}{x}\right)$$ Consider a positive subsequence of $x_n\to0$ for which $f(c)\tan^{-1}(\frac{1}{x})\to f(c)\frac{\pi}{2}$ and for a negative subsequence of $x_n\to0$ we will have $f(c)\tan^{-1}(\frac{1}{x})\to-f(c)\frac{\pi}{2}$ . So the limit does not exist. So is it a situation of mis-statement of a question or am I doing something wrong? EDIT: I forgot to mention that in my solution, $c\in(0,1)$","Let be a continuous function and . Find I am finding that the limit does not exist. But the question is stated in a way to enable one to think that the limit actually exists . But I don't think the limit exists, the reason being simple: By Mean Value Theorem we can write the integral as Consider a positive subsequence of for which and for a negative subsequence of we will have . So the limit does not exist. So is it a situation of mis-statement of a question or am I doing something wrong? EDIT: I forgot to mention that in my solution,","f f:[0,1]\to\mathbb R \lim_{x\to0}\int_0^1\frac{xf(t)}{x^2+t^2}\,dt xf(c)\int_0^1\frac{dt}{x^2+t^2}=f(c)\tan^{-1}\left(\frac{1}{x}\right) x_n\to0 f(c)\tan^{-1}(\frac{1}{x})\to f(c)\frac{\pi}{2} x_n\to0 f(c)\tan^{-1}(\frac{1}{x})\to-f(c)\frac{\pi}{2} c\in(0,1)","['real-analysis', 'integration', 'limits', 'definite-integrals', 'limits-without-lhopital']"
33,Finding the limit of a sequence of sequences,Finding the limit of a sequence of sequences,,"Take any $\bar{r} \in \mathbb{R}$ with $\bar{r}>0$. Assume that $f : \mathbb{R} \rightarrow \mathbb{R} $ is continuous. Assume that for all $r\in [0,\bar{r})$, there exists a strictly decreasing real sequence $\{t(r)^n\}_{n=1}^{\infty}$ such that $t(r)^1 = r$, and for all $n \in \mathbb{N}\backslash\{1\}$, $ 0 < t(r)^n < r, \qquad$    (1) and $f\big(t(r)^n\big) - f\big(t(r)^{n-1}\big) \geq 0. \qquad$    (2) Question : is it possible to have $f(0) < f(\bar{r})$ or does it necessarily induce a contradiction? What I have done so far: I have tried to work by contradiction, assuming that $f(0) < f(\bar{r})$ holds. Because $t(\bar{r})^n$ is bounded by (1), it has a converging subsequence, say $t(\bar{r})^{m(n)} \rightarrow r^*_1$. We know from (1) and the fact that the sequence is strictly decreasing that $0 \leq r^*_1 < \bar{r}$. Using (2) repeatedly together with the continuity of $f$, we get $f(r^*_1) - f(\bar{r}) \geq 0$. Then by the contradiction assumption we obtain $f(r^*_1) > f(0)$. If $r^*_1 = 0$, we reached a contradiction and we are done. Otherwise, we can repeat the process again and again, getting $0 \leq r^*_2 < r^*_1$, $0 \leq r^*_3 < r^*_2$, ... But I do not see why we would necessarily reach some $r^*_{n}$ such that  $r^*_{n}=0$. Is it the case? Are their counterexamples?","Take any $\bar{r} \in \mathbb{R}$ with $\bar{r}>0$. Assume that $f : \mathbb{R} \rightarrow \mathbb{R} $ is continuous. Assume that for all $r\in [0,\bar{r})$, there exists a strictly decreasing real sequence $\{t(r)^n\}_{n=1}^{\infty}$ such that $t(r)^1 = r$, and for all $n \in \mathbb{N}\backslash\{1\}$, $ 0 < t(r)^n < r, \qquad$    (1) and $f\big(t(r)^n\big) - f\big(t(r)^{n-1}\big) \geq 0. \qquad$    (2) Question : is it possible to have $f(0) < f(\bar{r})$ or does it necessarily induce a contradiction? What I have done so far: I have tried to work by contradiction, assuming that $f(0) < f(\bar{r})$ holds. Because $t(\bar{r})^n$ is bounded by (1), it has a converging subsequence, say $t(\bar{r})^{m(n)} \rightarrow r^*_1$. We know from (1) and the fact that the sequence is strictly decreasing that $0 \leq r^*_1 < \bar{r}$. Using (2) repeatedly together with the continuity of $f$, we get $f(r^*_1) - f(\bar{r}) \geq 0$. Then by the contradiction assumption we obtain $f(r^*_1) > f(0)$. If $r^*_1 = 0$, we reached a contradiction and we are done. Otherwise, we can repeat the process again and again, getting $0 \leq r^*_2 < r^*_1$, $0 \leq r^*_3 < r^*_2$, ... But I do not see why we would necessarily reach some $r^*_{n}$ such that  $r^*_{n}=0$. Is it the case? Are their counterexamples?",,"['real-analysis', 'sequences-and-series', 'limits', 'continuity']"
34,Are all limits solvable without L'Hôpital Rule or Series Expansion,Are all limits solvable without L'Hôpital Rule or Series Expansion,,"Is it always possible to find the limit of a function without using L'Hôpital Rule or Series Expansion ? For example, $$\lim_{x\to0}\frac{\tan x-x}{x^3}$$ $$\lim_{x\to0}\frac{\sin x-x}{x^3}$$ $$\lim_{x\to0}\frac{\ln(1+x)-x}{x^2}$$ $$\lim_{x\to0}\frac{e^x-x-1}{x^2}$$ $$\lim_{x\to0}\frac{\sin^{-1}x-x}{x^3}$$ $$\lim_{x\to0}\frac{\tan^{-1}x-x}{x^3}$$","Is it always possible to find the limit of a function without using L'Hôpital Rule or Series Expansion ? For example, $$\lim_{x\to0}\frac{\tan x-x}{x^3}$$ $$\lim_{x\to0}\frac{\sin x-x}{x^3}$$ $$\lim_{x\to0}\frac{\ln(1+x)-x}{x^2}$$ $$\lim_{x\to0}\frac{e^x-x-1}{x^2}$$ $$\lim_{x\to0}\frac{\sin^{-1}x-x}{x^3}$$ $$\lim_{x\to0}\frac{\tan^{-1}x-x}{x^3}$$",,"['calculus', 'limits', 'limits-without-lhopital']"
35,Problem 3.14(e) in Baby Rudin,Problem 3.14(e) in Baby Rudin,,"If $\{s_n\}$ be a sequence of complex numbers, define its arithmetic mean $\sigma_n$ by  $$\sigma_n \colon= \frac{s_0 + s_1 \cdots + s_n}{n+1} \, \, (n = 0, 1, 2, \ldots). $$ Put $a_n = s_n - s_{n-1}$ for $n \geq 1$. Assume $M < +\infty$, $\vert n a_n \vert \leq M$ for all $n$, and $\lim \sigma_n = \sigma$. Prove that $\lim s_n = \sigma$. I haven't been able to fill in the proofs in the steps of the outline suggested by Rudin. Can anyone please help me out with coming up with a solution to this problem?","If $\{s_n\}$ be a sequence of complex numbers, define its arithmetic mean $\sigma_n$ by  $$\sigma_n \colon= \frac{s_0 + s_1 \cdots + s_n}{n+1} \, \, (n = 0, 1, 2, \ldots). $$ Put $a_n = s_n - s_{n-1}$ for $n \geq 1$. Assume $M < +\infty$, $\vert n a_n \vert \leq M$ for all $n$, and $\lim \sigma_n = \sigma$. Prove that $\lim s_n = \sigma$. I haven't been able to fill in the proofs in the steps of the outline suggested by Rudin. Can anyone please help me out with coming up with a solution to this problem?",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'convergence-divergence']"
36,Is it true that if $\lim a_n = g$ then $\lim |a_n| = |g|$?,Is it true that if  then ?,\lim a_n = g \lim |a_n| = |g|,The question is in the title. I'm trying to prove that if $\lim_{n\rightarrow\infty} a_n = g$ then $\lim_{n\rightarrow\infty} |a_n| = |g|$. Is the following proof correct? If $\lim_{n\rightarrow\infty} a_n = g$ that means that for all $\epsilon > 0$ we have $|a_n - g| < \epsilon$ for sufficiently large $n$. But it is also true that $||a_n|-|g||\leq|a_n-g|<\epsilon$ which means that $\lim_{n\rightarrow\infty} |a_n| = |g|$.,The question is in the title. I'm trying to prove that if $\lim_{n\rightarrow\infty} a_n = g$ then $\lim_{n\rightarrow\infty} |a_n| = |g|$. Is the following proof correct? If $\lim_{n\rightarrow\infty} a_n = g$ that means that for all $\epsilon > 0$ we have $|a_n - g| < \epsilon$ for sufficiently large $n$. But it is also true that $||a_n|-|g||\leq|a_n-g|<\epsilon$ which means that $\lim_{n\rightarrow\infty} |a_n| = |g|$.,,"['calculus', 'sequences-and-series', 'limits', 'proof-verification']"
37,How to solve limits?,How to solve limits?,,"The above limit was solved by making a seemingly arbitrary substitution. The previous limit was solved by making a linear substitution $y=mx$ . Which again seemed a bit out of the blue. For another question, my book somehow came to the conclusion that the limit exists and that we should be trying to prove this (again, no explanation was given as to why they were trying to prove the limit existed this time). They then somehow came to the conclusion that a polar coordinate substitution might help along with the Squeeze theorem. When given a limit, my book keeps using all these different methods from all these different areas of math- most of which are very non-obvious. So my question(s) boils down to: a) When given a limit, what's a good way to get ""a hunch"" if the limit exists or not? I don't want to waste 15 minutes trying to prove a limit that doesn't exist. b) If I believe the limit exists, what's a good way to approach the problem and generate ideas on how to prove it? c) If I believe the limit doesn't exist, what's a good way to approach the problem and generate ideas on how to prove it? These questions obviously don't have deterministic answers that always work, I'm just looking for something to get past the initial ""What the hell do I do?!?!"". Most of the math I've done so far has been pretty mechanical (keep trying methods from your toolbox until one finally works), so these limits are pretty intimating.","The above limit was solved by making a seemingly arbitrary substitution. The previous limit was solved by making a linear substitution . Which again seemed a bit out of the blue. For another question, my book somehow came to the conclusion that the limit exists and that we should be trying to prove this (again, no explanation was given as to why they were trying to prove the limit existed this time). They then somehow came to the conclusion that a polar coordinate substitution might help along with the Squeeze theorem. When given a limit, my book keeps using all these different methods from all these different areas of math- most of which are very non-obvious. So my question(s) boils down to: a) When given a limit, what's a good way to get ""a hunch"" if the limit exists or not? I don't want to waste 15 minutes trying to prove a limit that doesn't exist. b) If I believe the limit exists, what's a good way to approach the problem and generate ideas on how to prove it? c) If I believe the limit doesn't exist, what's a good way to approach the problem and generate ideas on how to prove it? These questions obviously don't have deterministic answers that always work, I'm just looking for something to get past the initial ""What the hell do I do?!?!"". Most of the math I've done so far has been pretty mechanical (keep trying methods from your toolbox until one finally works), so these limits are pretty intimating.",y=mx,"['limits', 'multivariable-calculus']"
38,Question about the definition of $\limsup$ and $\liminf$ on real valued functions,Question about the definition of  and  on real valued functions,\limsup \liminf,"I understood the definition of $\liminf$ and $\limsup$ for sequences well. But there is a little bit of confusion when it comes to functions. If I did learn it correctly, the definition of $\limsup_{x \to l} f(x)$ is given as the following: Let $A(\delta)$ a set with $A(\delta) = \left\{f(x): 0<| x-l | < \delta \right\}$ for each $\delta > 0$. Then it is $\limsup_{x \to l} f(x) = \lim_{\delta \to 0^{-}} \sup A(\delta)$. For a regular limit $\lim_{x \to l}f(x)$ it is not important whether $f$ is defined at $l$ or not. For example let $f(x)$ be a well behaved, continuous function except only at $l$ where it makes a pointwise ""jump"". Then it is $\lim_{x \to l}f(x) = a$ where $a$ is the value which makes $f$ completely continuous if it were $f(l)=a$. My question is whether the same is valid for $\limsup$ and $\liminf$. Let's say that we calculate $\limsup_{x \to l} f(x)$ for the same $f(x)$ which makes a jump at $l$. Since the set $A(\delta)$ at the definition does not cover $0$, $f(l)$ is not included in $A(\delta)$. But since we take the limit of $\sup A(\delta)$s as $\delta$ approaches to $0$, $\limsup$ again becomes $a$ which is equal to $\lim_{x \to l}f(x)$. Is this logic correct here?","I understood the definition of $\liminf$ and $\limsup$ for sequences well. But there is a little bit of confusion when it comes to functions. If I did learn it correctly, the definition of $\limsup_{x \to l} f(x)$ is given as the following: Let $A(\delta)$ a set with $A(\delta) = \left\{f(x): 0<| x-l | < \delta \right\}$ for each $\delta > 0$. Then it is $\limsup_{x \to l} f(x) = \lim_{\delta \to 0^{-}} \sup A(\delta)$. For a regular limit $\lim_{x \to l}f(x)$ it is not important whether $f$ is defined at $l$ or not. For example let $f(x)$ be a well behaved, continuous function except only at $l$ where it makes a pointwise ""jump"". Then it is $\lim_{x \to l}f(x) = a$ where $a$ is the value which makes $f$ completely continuous if it were $f(l)=a$. My question is whether the same is valid for $\limsup$ and $\liminf$. Let's say that we calculate $\limsup_{x \to l} f(x)$ for the same $f(x)$ which makes a jump at $l$. Since the set $A(\delta)$ at the definition does not cover $0$, $f(l)$ is not included in $A(\delta)$. But since we take the limit of $\sup A(\delta)$s as $\delta$ approaches to $0$, $\limsup$ again becomes $a$ which is equal to $\lim_{x \to l}f(x)$. Is this logic correct here?",,"['real-analysis', 'limits', 'limsup-and-liminf']"
39,Is it possible to calculate the limit $\lim\limits_{n \to \infty} \frac{a_n}{n}$ of the conditions?,Is it possible to calculate the limit  of the conditions?,\lim\limits_{n \to \infty} \frac{a_n}{n},Is it possible to calculate the limit  $\displaystyle  \lim\limits_{n \to \infty} \frac{a_n}{n}$ of the conditions? $ a_n>0$ for $ {n \geq 1}$ and $ a_{n-1} \leq \left(a_{n+2} - a_n \right)^n \leq a_{n+1}$.,Is it possible to calculate the limit  $\displaystyle  \lim\limits_{n \to \infty} \frac{a_n}{n}$ of the conditions? $ a_n>0$ for $ {n \geq 1}$ and $ a_{n-1} \leq \left(a_{n+2} - a_n \right)^n \leq a_{n+1}$.,,"['analysis', 'limits']"
40,Is the sequences$\{S_n\}$ convergent? [duplicate],Is the sequences convergent? [duplicate],\{S_n\},"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Partial sums of exponential series (2 answers) Closed 9 years ago . Let $$S_n=e^{-n}\sum_{k=0}^n\frac{n^k}{k!}$$ Is the sequences$\{S_n\}$ convergent? The following is my answer,but this is not correct. please give some hints. For all $x\in\mathbb{R}$, $$\lim_{n\rightarrow\infty}\sum_{k=0}^n\frac{x^k}{k!}=e^x.$$ then $$\lim_{n\rightarrow\infty}e^{-n}\sum_{k=0}^n\frac{n^k}{k!}=1.$$","This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Partial sums of exponential series (2 answers) Closed 9 years ago . Let $$S_n=e^{-n}\sum_{k=0}^n\frac{n^k}{k!}$$ Is the sequences$\{S_n\}$ convergent? The following is my answer,but this is not correct. please give some hints. For all $x\in\mathbb{R}$, $$\lim_{n\rightarrow\infty}\sum_{k=0}^n\frac{x^k}{k!}=e^x.$$ then $$\lim_{n\rightarrow\infty}e^{-n}\sum_{k=0}^n\frac{n^k}{k!}=1.$$",,"['sequences-and-series', 'limits']"
41,"""Closed"" form for $\sum \frac{1}{n^n}$","""Closed"" form for",\sum \frac{1}{n^n},"Earlier today, I was talking with my friend about some ""cool"" infinite series and the value they converge to like the Basel problem, Madhava-Leibniz formula for $\pi/4, \log 2$ and similar alternating series etc. One series that popped into our discussion was $\sum\limits_{n=1}^{\infty} \frac{1}{n^n}$. Proving the convergence of this series is trivial but finding the value to which converges has defied me so far. Mathematica says this series converges to $\approx 1.29129$. I tried Googling about this series and found very little information about this series (which is actually surprising since the series looks cool enough to arise in some context). We were joking that it should have something to do with $\pi,e,\phi,\gamma$ or at the least it must be a transcendental number :-). My questions are: What does this series converge to? Does this series arise in any context and are there interesting trivia to be known about this series? I am actually slightly puzzled that I have not been able to find much about this series on the Internet. (At least my Google search did not yield any interesting results).","Earlier today, I was talking with my friend about some ""cool"" infinite series and the value they converge to like the Basel problem, Madhava-Leibniz formula for $\pi/4, \log 2$ and similar alternating series etc. One series that popped into our discussion was $\sum\limits_{n=1}^{\infty} \frac{1}{n^n}$. Proving the convergence of this series is trivial but finding the value to which converges has defied me so far. Mathematica says this series converges to $\approx 1.29129$. I tried Googling about this series and found very little information about this series (which is actually surprising since the series looks cool enough to arise in some context). We were joking that it should have something to do with $\pi,e,\phi,\gamma$ or at the least it must be a transcendental number :-). My questions are: What does this series converge to? Does this series arise in any context and are there interesting trivia to be known about this series? I am actually slightly puzzled that I have not been able to find much about this series on the Internet. (At least my Google search did not yield any interesting results).",,[]
42,How prove this nice limit $\lim\limits_{n\to\infty}\frac{a_{n}}{n}=\frac{12}{\log{432}}$,How prove this nice limit,\lim\limits_{n\to\infty}\frac{a_{n}}{n}=\frac{12}{\log{432}},"Nice problem: Let $a_{0}=1$ and $$a_{n}=a_{\left\lfloor n/2\right\rfloor}+a_{\left\lfloor n/3 \right\rfloor}+a_{\left\lfloor n/6\right\rfloor}.$$  Show that $$\lim_{n\to\infty}\dfrac{a_{n}}{n}=\dfrac{12}{\log{432}},$$ where $\lfloor x \rfloor$ is the largest integer not greater than $x$. It is said this problem was created by Paul Erdős , and I can't find this problem's solution, does anyone have any nice methods? Thank you.","Nice problem: Let $a_{0}=1$ and $$a_{n}=a_{\left\lfloor n/2\right\rfloor}+a_{\left\lfloor n/3 \right\rfloor}+a_{\left\lfloor n/6\right\rfloor}.$$  Show that $$\lim_{n\to\infty}\dfrac{a_{n}}{n}=\dfrac{12}{\log{432}},$$ where $\lfloor x \rfloor$ is the largest integer not greater than $x$. It is said this problem was created by Paul Erdős , and I can't find this problem's solution, does anyone have any nice methods? Thank you.",,"['sequences-and-series', 'limits', 'recurrence-relations']"
43,Rudin's Principle of Mathematical Analysis Problem,Rudin's Principle of Mathematical Analysis Problem,,"If ${(s_n)}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n = \frac{s_0 +s_1 + ... + s_n}{n+1}$$ Assume that $M < \infty, |na_n| \leq M$ for all n and  lim ${\sigma_n = \sigma}$. Prove that lim $s_n = \sigma$. The solution given is If $m <n$, then $$s_n - \sigma_n = \frac{m+1}{n-m}(\sigma_n -\sigma_m) + \frac{1}{n-m}\sum_{i=m+1}^n (s_n -s_i)$$ For these i, $$|s_n - s_i|\leq\frac{(n-1)M}{i+1}\leq\frac{(n-m-1)M}{m+2}$$ Fix $\epsilon>1$ and associate with each n the integer m that satisfies $$m\leq\frac{n-\epsilon}{1+\epsilon}<m+1$$ Then $(m+1)/(n-m) \leq \frac{1}{\epsilon}$ and $|s_n -s_i|<M\epsilon$. Since $\epsilon$ was arbitrary, lim $s_n = \sigma$. My problems is: *Why we can choose an upper bound for n?","If ${(s_n)}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n = \frac{s_0 +s_1 + ... + s_n}{n+1}$$ Assume that $M < \infty, |na_n| \leq M$ for all n and  lim ${\sigma_n = \sigma}$. Prove that lim $s_n = \sigma$. The solution given is If $m <n$, then $$s_n - \sigma_n = \frac{m+1}{n-m}(\sigma_n -\sigma_m) + \frac{1}{n-m}\sum_{i=m+1}^n (s_n -s_i)$$ For these i, $$|s_n - s_i|\leq\frac{(n-1)M}{i+1}\leq\frac{(n-m-1)M}{m+2}$$ Fix $\epsilon>1$ and associate with each n the integer m that satisfies $$m\leq\frac{n-\epsilon}{1+\epsilon}<m+1$$ Then $(m+1)/(n-m) \leq \frac{1}{\epsilon}$ and $|s_n -s_i|<M\epsilon$. Since $\epsilon$ was arbitrary, lim $s_n = \sigma$. My problems is: *Why we can choose an upper bound for n?",,"['real-analysis', 'sequences-and-series', 'limits']"
44,Zero arithmetic mean: bound on Abel sum?,Zero arithmetic mean: bound on Abel sum?,,"Let $(a_0, a_1, a_2, \ldots)$ be a bounded sequence in $\mathbb{R}$ with arithmetic means \begin{equation} \frac{a_0 + a_1 + \cdots + a_{n-1}}{n} \end{equation} converging to zero as $n \to \infty$. Is the following statement true: \begin{equation} \liminf_{x \to 1^-} \sum_{n=0}^{\infty} a_n x^n \geq 0 \quad \text{or} \quad \liminf_{x \to 1^-} \sum_{n=0}^{\infty} (-a_n) x^n \geq 0 \quad \text{(or both)}. \end{equation} What I tried: for nonzero means, the partial sums are bounded away from zero for sufficiently large $n$, giving corresponding estimates on the lower/upper Abelian sum. But for the zero mean case, I couldn't figure it out. Tried finding this in Hardy's Divergent Series as well.","Let $(a_0, a_1, a_2, \ldots)$ be a bounded sequence in $\mathbb{R}$ with arithmetic means \begin{equation} \frac{a_0 + a_1 + \cdots + a_{n-1}}{n} \end{equation} converging to zero as $n \to \infty$. Is the following statement true: \begin{equation} \liminf_{x \to 1^-} \sum_{n=0}^{\infty} a_n x^n \geq 0 \quad \text{or} \quad \liminf_{x \to 1^-} \sum_{n=0}^{\infty} (-a_n) x^n \geq 0 \quad \text{(or both)}. \end{equation} What I tried: for nonzero means, the partial sums are bounded away from zero for sufficiently large $n$, giving corresponding estimates on the lower/upper Abelian sum. But for the zero mean case, I couldn't figure it out. Tried finding this in Hardy's Divergent Series as well.",,"['real-analysis', 'sequences-and-series', 'limits', 'divergent-series']"
45,Prove that $\lim_{\substack{b\to\infty \\ a\to0+}}\int_a^b\frac{\hat{f}(\xi)}\xi d\xi=-\pi i\int_0^\infty f(x)dx$,Prove that,\lim_{\substack{b\to\infty \\ a\to0+}}\int_a^b\frac{\hat{f}(\xi)}\xi d\xi=-\pi i\int_0^\infty f(x)dx,"Suppose $f\in L^1(\mathbb{R})$ and that $f$ is odd. Prove that $$\lim_{\substack{b\to\infty \\ a\to0+}}\int_a^b\frac{\hat{f}(\xi)}\xi d\xi=-\pi i\int_0^\infty f(x)dx$$ Here $\hat{f}$ denotes the Fourier transform of $f$. I showed that if $f$ is odd, then $\hat{f}(\xi)=2i\int_0^\infty f(x)\sin(\xi x)dx$. Also, I'm thinking of using this formula: $$\int \hat{f}(\xi)g(\xi)d\xi=\int f(x)\hat{g}(x)dx$$ for $f,g\in L^1(\mathbb{R})$ (Actually I can't use the formula since $\frac{1}{x}$ is not in $L^1$) Also, this inequality may help: $$\left|\int_0^b\frac{\sin ax}x dx\right|\le\int_0^\pi\frac{\sin x}xdx$$","Suppose $f\in L^1(\mathbb{R})$ and that $f$ is odd. Prove that $$\lim_{\substack{b\to\infty \\ a\to0+}}\int_a^b\frac{\hat{f}(\xi)}\xi d\xi=-\pi i\int_0^\infty f(x)dx$$ Here $\hat{f}$ denotes the Fourier transform of $f$. I showed that if $f$ is odd, then $\hat{f}(\xi)=2i\int_0^\infty f(x)\sin(\xi x)dx$. Also, I'm thinking of using this formula: $$\int \hat{f}(\xi)g(\xi)d\xi=\int f(x)\hat{g}(x)dx$$ for $f,g\in L^1(\mathbb{R})$ (Actually I can't use the formula since $\frac{1}{x}$ is not in $L^1$) Also, this inequality may help: $$\left|\int_0^b\frac{\sin ax}x dx\right|\le\int_0^\pi\frac{\sin x}xdx$$",,"['real-analysis', 'integration', 'analysis', 'limits', 'fourier-analysis']"
46,Sequence Limit Problem: If $0 \leq x_{m+n} \leq x_n + x_m$ then limit of $x_n/n$ exists,Sequence Limit Problem: If  then limit of  exists,0 \leq x_{m+n} \leq x_n + x_m x_n/n,"If the sequence $\{x_n\}$ satisfies the property that $0 \leq x_{m+n} \leq x_n + x_m$ for all $n$, $m \in \mathbb{N}$ , show that the limit of the sequence $\left\{\frac{x_n}{n}\right\}_n$ exists. Provide an example of such a sequence. I can show that it is bounded, but I don't know how to show that it is monotone increasing (I think it is monotone increasing, not sure).","If the sequence $\{x_n\}$ satisfies the property that $0 \leq x_{m+n} \leq x_n + x_m$ for all $n$, $m \in \mathbb{N}$ , show that the limit of the sequence $\left\{\frac{x_n}{n}\right\}_n$ exists. Provide an example of such a sequence. I can show that it is bounded, but I don't know how to show that it is monotone increasing (I think it is monotone increasing, not sure).",,"['real-analysis', 'sequences-and-series']"
47,Limits involving factorials $\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$,Limits involving factorials,\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}},"I am trying to calculate the following limit $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$$ where $k$ can be any number between $0$ and $N$ . I thought of the following: If I take the logarithm of the expression then I get: $$\lim_{N\to\infty} \left(\log(N!)-\log((N-k)!)-k\log N\right)$$ Using the Stirling formula this can be approximated as: $$\lim_{N\to\infty} \left(N\log(N)-(N-k)\log(N-k)-k\log N\right)$$ Now there are two cases: If $k$ is $N$ , then the second term vanishes and the remaining terms cancel. If $k$ is smaller than $N$ , then I can drop the $k$ inside the second logarithm and all the terms cancel. So the limit $$\lim_{N\to\infty} \log\left(\frac{N!}{(N-k)!N^{k}}\right)=0$$ Which means that: $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}=1$$ I don't know if this is mathematically rigorous. Would like some help. Thanks","I am trying to calculate the following limit where can be any number between and . I thought of the following: If I take the logarithm of the expression then I get: Using the Stirling formula this can be approximated as: Now there are two cases: If is , then the second term vanishes and the remaining terms cancel. If is smaller than , then I can drop the inside the second logarithm and all the terms cancel. So the limit Which means that: I don't know if this is mathematically rigorous. Would like some help. Thanks",\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}} k 0 N \lim_{N\to\infty} \left(\log(N!)-\log((N-k)!)-k\log N\right) \lim_{N\to\infty} \left(N\log(N)-(N-k)\log(N-k)-k\log N\right) k N k N k \lim_{N\to\infty} \log\left(\frac{N!}{(N-k)!N^{k}}\right)=0 \lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}=1,"['calculus', 'limits', 'factorial']"
48,Continuity on open and closed intervals,Continuity on open and closed intervals,,"I will be taking Calculus I soon, and I just want to make sure I understand some concepts correctly. So far, reading my book for Calculus I, I've encountered the definition of continuity as being defined on a closed interval $[a,b]$. This means that it exists at every single point in the interval including the endpoints, (meaning, that it can be approached from $a^+$, from $a^-$, from $b^+$, and from $b^-$, i.e., from both sides), correct? A polynomial would be such an example. I looked through my book but there is no definition of continuity on an open interval $(a,b)$. Continuity on an open interval just means that it's approachable from only one side, either $a^-$ or $a^+$, or $b^+$ or $b^-$, but not only one of those, correct? Is such a property the same thing as a one-sided limit? So, for example, ln would be an example of such a continuous property, since it's only defined for the positive numbers and thus can only be approached from one side? So, then, does that mean that only a continuous function on a closed interval $[a,b]$ can attain a maximum/minimum value? A closed-open interval would also attain a maximum or a minimum but not both, such as $[a,b)$, correct? sorry if these are too elementary for this website...","I will be taking Calculus I soon, and I just want to make sure I understand some concepts correctly. So far, reading my book for Calculus I, I've encountered the definition of continuity as being defined on a closed interval $[a,b]$. This means that it exists at every single point in the interval including the endpoints, (meaning, that it can be approached from $a^+$, from $a^-$, from $b^+$, and from $b^-$, i.e., from both sides), correct? A polynomial would be such an example. I looked through my book but there is no definition of continuity on an open interval $(a,b)$. Continuity on an open interval just means that it's approachable from only one side, either $a^-$ or $a^+$, or $b^+$ or $b^-$, but not only one of those, correct? Is such a property the same thing as a one-sided limit? So, for example, ln would be an example of such a continuous property, since it's only defined for the positive numbers and thus can only be approached from one side? So, then, does that mean that only a continuous function on a closed interval $[a,b]$ can attain a maximum/minimum value? A closed-open interval would also attain a maximum or a minimum but not both, such as $[a,b)$, correct? sorry if these are too elementary for this website...",,"['calculus', 'limits', 'continuity', 'definition']"
49,Nested Radicals: $\sqrt{a+\sqrt{2a+\sqrt{3a+\ldots}}}$,Nested Radicals:,\sqrt{a+\sqrt{2a+\sqrt{3a+\ldots}}},Let $a>0$ . How we can find the limit of : $$\sqrt{a+\sqrt{2a+\sqrt{3a+\ldots}}}$$ Thanks in advance for your help,Let $a>0$ . How we can find the limit of : $$\sqrt{a+\sqrt{2a+\sqrt{3a+\ldots}}}$$ Thanks in advance for your help,,"['calculus', 'limits']"
50,Show that $f$ is uniformly continuous if limit exists,Show that  is uniformly continuous if limit exists,f,"Let $f(x)$ be continuous on $(0,1]$. Show that $f$ is uniformly continuous IFF $\displaystyle \lim_{x\to0^+} f(x)$ exists. Thoughts: Backward Proof: Let another function $\overline f(x)$ be continuous on $[0,1]$ which is equal to $f(x)$ plus the limit point. Thus the limit exists and it is uniformly continuous. So, as $f(x)$. Forward Proof: I Don't really have any idea...?? Please help guys","Let $f(x)$ be continuous on $(0,1]$. Show that $f$ is uniformly continuous IFF $\displaystyle \lim_{x\to0^+} f(x)$ exists. Thoughts: Backward Proof: Let another function $\overline f(x)$ be continuous on $[0,1]$ which is equal to $f(x)$ plus the limit point. Thus the limit exists and it is uniformly continuous. So, as $f(x)$. Forward Proof: I Don't really have any idea...?? Please help guys",,"['real-analysis', 'general-topology', 'continuity']"
51,interchange limits (not a sequence of function),interchange limits (not a sequence of function),,"I am aware of a theorem in analysis regarding the result that $$\lim_{t\rightarrow x} \lim_{n\rightarrow \infty} f_n(t) = \lim_{n\rightarrow \infty} \lim_{x\rightarrow t} f_n(t).$$ My question is about interchanging two limit operators as well but it does not involve a sequence of functions. Here goes: I have a continuous function $C(uw_1,uw_2)$. One research paper I'm reading makes use of the interchange of the derivative and the limit. (I am aware that there is a theorem for switching derivatives and limits which is an analogue to the above but I do recognize that the derivative can be expressed as a limit.) $$\frac{\partial}{\partial w_j} \lim_{u\rightarrow 0} \frac{C(uw_1,uw_2)}{u} = \lim_{u\rightarrow 0} \frac{\partial}{\partial w_j} \frac{C(uw_1,uw_2)}{u}.$$ I am looking for a rationale behind this. I thought of considering $\dfrac{C(uw_1,uw_2)}{u}$ as a constant sequence of functions in order to apply theorems from analysis but I am unsure about how to proceed. On the other hand, I am well aware that if $g(j,k)$ is increasing with respect to $j$ and $k$, switching the order of limits is possible as this can be thought of switching the order of summation. But I am unsure if the function I have at hand is increasing with respect to $u$ as $u \rightarrow 0$.","I am aware of a theorem in analysis regarding the result that $$\lim_{t\rightarrow x} \lim_{n\rightarrow \infty} f_n(t) = \lim_{n\rightarrow \infty} \lim_{x\rightarrow t} f_n(t).$$ My question is about interchanging two limit operators as well but it does not involve a sequence of functions. Here goes: I have a continuous function $C(uw_1,uw_2)$. One research paper I'm reading makes use of the interchange of the derivative and the limit. (I am aware that there is a theorem for switching derivatives and limits which is an analogue to the above but I do recognize that the derivative can be expressed as a limit.) $$\frac{\partial}{\partial w_j} \lim_{u\rightarrow 0} \frac{C(uw_1,uw_2)}{u} = \lim_{u\rightarrow 0} \frac{\partial}{\partial w_j} \frac{C(uw_1,uw_2)}{u}.$$ I am looking for a rationale behind this. I thought of considering $\dfrac{C(uw_1,uw_2)}{u}$ as a constant sequence of functions in order to apply theorems from analysis but I am unsure about how to proceed. On the other hand, I am well aware that if $g(j,k)$ is increasing with respect to $j$ and $k$, switching the order of limits is possible as this can be thought of switching the order of summation. But I am unsure if the function I have at hand is increasing with respect to $u$ as $u \rightarrow 0$.",,"['real-analysis', 'analysis', 'limits']"
52,Fourier Coefficients of Complex Measure,Fourier Coefficients of Complex Measure,,"For my homework I am trying to prove the following: Suppose $\mu$ is a complex Borel measure on $[0,2\pi)$, and define the Fourier coefficients of $\mu$ by $\hat{\mu}(n)=\displaystyle\int e^{-int}d\mu(t)$ for $n=0, \pm 1, \pm 2, \ldots$. Assume that $\hat{\mu}(n)\to 0$ as $n\to +\infty$ and prove that $\hat{\mu}\to 0$ as $n \to -\infty$. I'm not sure how to get started, can someone provide a hint?","For my homework I am trying to prove the following: Suppose $\mu$ is a complex Borel measure on $[0,2\pi)$, and define the Fourier coefficients of $\mu$ by $\hat{\mu}(n)=\displaystyle\int e^{-int}d\mu(t)$ for $n=0, \pm 1, \pm 2, \ldots$. Assume that $\hat{\mu}(n)\to 0$ as $n\to +\infty$ and prove that $\hat{\mu}\to 0$ as $n \to -\infty$. I'm not sure how to get started, can someone provide a hint?",,"['real-analysis', 'measure-theory', 'limits', 'fourier-analysis']"
53,The Limit of the Following Derivative,The Limit of the Following Derivative,,"Suppose you have two functions $F$ and $G$ with the following properties. $G(0)=F(0)=0, G'>0, G''<0, F'>0, F''<0 $ and also $\lim_{x\to0} F'(x)=\infty, \lim_{x\to\infty} F'(x)=0, \lim_{x\to0} G'(x)=0, \lim_{x\to\infty} G'(x)=\infty$ Suppose that $s$ is defined such that: $\frac{F'(S)}{G'(S)}=ab$  (1) where $a>0$ and $b\in[0,1]$ (Note, I'm not sure these conditions on a,b matter so much) Obviously, the above equation yields a solution for $S$ such that $S=S(a,b)$ I have to find $\lim_{a\to\infty} \frac{\partial S}{\partial b}$ and $\lim_{a\to 0} \frac{\partial S}{\partial b}$ My answer so far: From (1), using implicit differentiation, I get: $\frac{F''(S)G'(S)-F''(S)G'(S)}{(C'(S))^2}\frac{\partial S}{\partial b}=a$  $\implies \frac{\partial S}{\partial b}= \frac{a(G'(S))^2}{F''(S)G'(S)-G''(S)F'(S)}$ (2) Also from (1), as $a\to 0, S\to \infty $ However, there are no assumptions made on $F''(x)G'(x)$ so I'm not sure how to continue since this could go to 0 or infinity. Do I need to split the problem into cases? Edit: Ok I have extra assumptions: Let $F''(x)G'(x)\to 0$ and $G''(x)F'(x)\to 0$  as $x \to \infty$","Suppose you have two functions $F$ and $G$ with the following properties. $G(0)=F(0)=0, G'>0, G''<0, F'>0, F''<0 $ and also $\lim_{x\to0} F'(x)=\infty, \lim_{x\to\infty} F'(x)=0, \lim_{x\to0} G'(x)=0, \lim_{x\to\infty} G'(x)=\infty$ Suppose that $s$ is defined such that: $\frac{F'(S)}{G'(S)}=ab$  (1) where $a>0$ and $b\in[0,1]$ (Note, I'm not sure these conditions on a,b matter so much) Obviously, the above equation yields a solution for $S$ such that $S=S(a,b)$ I have to find $\lim_{a\to\infty} \frac{\partial S}{\partial b}$ and $\lim_{a\to 0} \frac{\partial S}{\partial b}$ My answer so far: From (1), using implicit differentiation, I get: $\frac{F''(S)G'(S)-F''(S)G'(S)}{(C'(S))^2}\frac{\partial S}{\partial b}=a$  $\implies \frac{\partial S}{\partial b}= \frac{a(G'(S))^2}{F''(S)G'(S)-G''(S)F'(S)}$ (2) Also from (1), as $a\to 0, S\to \infty $ However, there are no assumptions made on $F''(x)G'(x)$ so I'm not sure how to continue since this could go to 0 or infinity. Do I need to split the problem into cases? Edit: Ok I have extra assumptions: Let $F''(x)G'(x)\to 0$ and $G''(x)F'(x)\to 0$  as $x \to \infty$",,"['calculus', 'real-analysis', 'limits']"
54,"When are sums and integrals ""identical"" in form?","When are sums and integrals ""identical"" in form?",,"In the answer to this question Eric Näslund showed that logarithms can be written as the following limit of a sum: $$\displaystyle \log(x) = \lim_{k\to \infty } \, \sum\limits_{n=k}^{x k} \frac{1}{n}$$ Changing the symbol for the sum $\sum$ into an integral $\int$ with the same limits as in the sum, one still gets logarithms: $$\displaystyle \log(x) = \lim_{k\to \infty } \, \int\limits_k^{x k} \frac{1}{n} \, dn$$ or at least in Mathematica, so I guess it is true. Are there other sums and integrals such that the function to be integrated or summed is the same, as well as the integration and summation limits, that give the same result? This provided that integrals or sums giving zero as result are not considered. As a Mathematica program this is: Clear[n, k, x]; Table[Limit[Integrate[1/n, {n, k, x*k}], k -> Infinity], {x, 1, 12}] Table[Limit[Sum[1/n, {n, k, x*k}], k -> Infinity], {x, 1, 12}] which gives the output: {0, Log[2], Log[3], Log[4], Log[5], Log[6], Log[7], Log[8], Log[9], Log[10], Log[11], Log[12]} in both cases. Edit 7.4.2013: I now realized that the integral is not dependent on that the limit goes to infinity. Actually any value of k will give the same logarithm. A small Mathematica program to illustrate this: Table[Integrate[1/n, {n, k, 2*k}], {k, 1, 12}] which gives: {Log[2], Log[2], Log[2], Log[2], Log[2], Log[2], Log[2],  Log[2], Log[2], Log[2], Log[2], Log[2]}","In the answer to this question Eric Näslund showed that logarithms can be written as the following limit of a sum: $$\displaystyle \log(x) = \lim_{k\to \infty } \, \sum\limits_{n=k}^{x k} \frac{1}{n}$$ Changing the symbol for the sum $\sum$ into an integral $\int$ with the same limits as in the sum, one still gets logarithms: $$\displaystyle \log(x) = \lim_{k\to \infty } \, \int\limits_k^{x k} \frac{1}{n} \, dn$$ or at least in Mathematica, so I guess it is true. Are there other sums and integrals such that the function to be integrated or summed is the same, as well as the integration and summation limits, that give the same result? This provided that integrals or sums giving zero as result are not considered. As a Mathematica program this is: Clear[n, k, x]; Table[Limit[Integrate[1/n, {n, k, x*k}], k -> Infinity], {x, 1, 12}] Table[Limit[Sum[1/n, {n, k, x*k}], k -> Infinity], {x, 1, 12}] which gives the output: {0, Log[2], Log[3], Log[4], Log[5], Log[6], Log[7], Log[8], Log[9], Log[10], Log[11], Log[12]} in both cases. Edit 7.4.2013: I now realized that the integral is not dependent on that the limit goes to infinity. Actually any value of k will give the same logarithm. A small Mathematica program to illustrate this: Table[Integrate[1/n, {n, k, 2*k}], {k, 1, 12}] which gives: {Log[2], Log[2], Log[2], Log[2], Log[2], Log[2], Log[2],  Log[2], Log[2], Log[2], Log[2], Log[2]}",,"['calculus', 'integration', 'limits']"
55,What is the limit of the quotient of Stieltjes Constants? [closed],What is the limit of the quotient of Stieltjes Constants? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is the limit : $\displaystyle\lim_{n\to \infty }\frac{\gamma_{n-1} }{\gamma_{n}} $ Here $\gamma_{n} $ is the $n$-th Stieltjes Constant .","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question What is the limit : $\displaystyle\lim_{n\to \infty }\frac{\gamma_{n-1} }{\gamma_{n}} $ Here $\gamma_{n} $ is the $n$-th Stieltjes Constant .",,"['functions', 'limits']"
56,Interchanging the order of limits,Interchanging the order of limits,,Would you advise me on the references of Pringsheim Convergence about interchanging the order of limits? Where can I find the most general statement?,Would you advise me on the references of Pringsheim Convergence about interchanging the order of limits? Where can I find the most general statement?,,"['real-analysis', 'reference-request', 'limits', 'convergence-divergence']"
57,Asymptotic vlaue of $ f(n)=\sum_{i=0}^n\lfloor \sqrt{i}\rfloor\binom{n}{i} $,Asymptotic vlaue of, f(n)=\sum_{i=0}^n\lfloor \sqrt{i}\rfloor\binom{n}{i} ,Inspired by this question I tried to find an asymptotic formula for $$ f(n)=\sum_{i=0}^n\lfloor \sqrt{i}\rfloor\binom{n}{i} $$ With the observation: $$ f(n)=\sum_{i=0}^n\frac{\lfloor \sqrt{i}\rfloor+\lfloor \sqrt{n-i}\rfloor}{2}\binom{n}{i} $$ And the a bit naive approximation  $$ \frac{\lfloor \sqrt{i}\rfloor+\lfloor \sqrt{n-i}\rfloor}{2}=\frac{1}{n}\sum_{i=1}^{n}{\lfloor \sqrt{i}\rfloor}=\frac{n\lfloor \sqrt{n}\rfloor-\frac{1}{6}\lfloor \sqrt{n}\rfloor\left(\lfloor \sqrt{n}\rfloor-1\right)\left(2\lfloor \sqrt{n}\rfloor+5\right)}{n}\approx\frac{2}{3}\sqrt{n} $$ We obtain $f(n)\approx\frac{2}{3}\sqrt{n}2^n=:g(n)$. The approximation doesn't seem that bad; I computed with wolfram alpha: $$ $$ \begin{array}{c|c|c|c} n & f(n) & g(n) & f(n)/g(n) \\ \hline 1 & 1 & 1.333 & 0.75 \\ 2 & 3 & 3.771 & 0.795 \\ 3 & 7 & 9.238 & 0.758 \\ 4 & 16 & 21.333 & 0.75 \\ 5 & 37 & 47.703 & 0.776 \\ 10 & 1882 & 2158.782 & 0.872 \\ 50 & 5.128\cdot10^{15} & 5.308\cdot10^{15} & 0.996 \\ 100 & 8.391\cdot10^{30} & 8.451\cdot10^{30} & 0.993 \\ 200 & 1.531\cdot10^{61} & 1.515\cdot10^{61} & 1.011 \\ 350 & 2.929\cdot10^{106} & 2.86\cdot10^{106} & 1.024 \\ \end{array} $$ $$ The error term fluctuates a bit so it seems natural to ask: Is it true that: $$ \lim_{n\to\infty}{\frac{f(n)}{g(n)}}=1 $$,Inspired by this question I tried to find an asymptotic formula for $$ f(n)=\sum_{i=0}^n\lfloor \sqrt{i}\rfloor\binom{n}{i} $$ With the observation: $$ f(n)=\sum_{i=0}^n\frac{\lfloor \sqrt{i}\rfloor+\lfloor \sqrt{n-i}\rfloor}{2}\binom{n}{i} $$ And the a bit naive approximation  $$ \frac{\lfloor \sqrt{i}\rfloor+\lfloor \sqrt{n-i}\rfloor}{2}=\frac{1}{n}\sum_{i=1}^{n}{\lfloor \sqrt{i}\rfloor}=\frac{n\lfloor \sqrt{n}\rfloor-\frac{1}{6}\lfloor \sqrt{n}\rfloor\left(\lfloor \sqrt{n}\rfloor-1\right)\left(2\lfloor \sqrt{n}\rfloor+5\right)}{n}\approx\frac{2}{3}\sqrt{n} $$ We obtain $f(n)\approx\frac{2}{3}\sqrt{n}2^n=:g(n)$. The approximation doesn't seem that bad; I computed with wolfram alpha: $$ $$ \begin{array}{c|c|c|c} n & f(n) & g(n) & f(n)/g(n) \\ \hline 1 & 1 & 1.333 & 0.75 \\ 2 & 3 & 3.771 & 0.795 \\ 3 & 7 & 9.238 & 0.758 \\ 4 & 16 & 21.333 & 0.75 \\ 5 & 37 & 47.703 & 0.776 \\ 10 & 1882 & 2158.782 & 0.872 \\ 50 & 5.128\cdot10^{15} & 5.308\cdot10^{15} & 0.996 \\ 100 & 8.391\cdot10^{30} & 8.451\cdot10^{30} & 0.993 \\ 200 & 1.531\cdot10^{61} & 1.515\cdot10^{61} & 1.011 \\ 350 & 2.929\cdot10^{106} & 2.86\cdot10^{106} & 1.024 \\ \end{array} $$ $$ The error term fluctuates a bit so it seems natural to ask: Is it true that: $$ \lim_{n\to\infty}{\frac{f(n)}{g(n)}}=1 $$,,"['sequences-and-series', 'limits', 'asymptotics', 'approximation']"
58,Why is it ok to factor an equation with no limit so it has a limit?,Why is it ok to factor an equation with no limit so it has a limit?,,"I'm just starting out in calculus, so please bear with me if this is not a sensible question. In the book I'm reading, the author gives the example of the problem of finding the limit of $\lim\limits_{x\to 5}(\frac{x^2 - 25}{x-5})$, because if you substitute in $x=5$, you get a denominator of $0$, so the output of the function at $x=5$ is undefined. He then goes on to demonstrate how by factoring this equation to $\lim\limits_{x\to 5}(x+5)$, you can now plug in $x=5$ to get $\lim\limits_{x\to 5}(x+5) = 10$. But is not the fact that 'at $x=5$ the function is undefined' an integral part of the original function? By factoring it, have you not added to the domain of the original function and therefore created a different function? So now you have the limit of a different function?","I'm just starting out in calculus, so please bear with me if this is not a sensible question. In the book I'm reading, the author gives the example of the problem of finding the limit of $\lim\limits_{x\to 5}(\frac{x^2 - 25}{x-5})$, because if you substitute in $x=5$, you get a denominator of $0$, so the output of the function at $x=5$ is undefined. He then goes on to demonstrate how by factoring this equation to $\lim\limits_{x\to 5}(x+5)$, you can now plug in $x=5$ to get $\lim\limits_{x\to 5}(x+5) = 10$. But is not the fact that 'at $x=5$ the function is undefined' an integral part of the original function? By factoring it, have you not added to the domain of the original function and therefore created a different function? So now you have the limit of a different function?",,['limits']
59,Limit of cosines product,Limit of cosines product,,"Let $\displaystyle P_n:=\prod_{k=1}^n \cos\frac{\pi}{k+2}$. Evaluate $\displaystyle\lim_{n\to \infty} P_n$. I've only shown that the limit is positive. Let $\vartheta_k:=\pi/(k+2)$. We have $\log P_n=\sum_{k=1}^n \log\cos\vartheta_k$. Now,  $$\log\cos\vartheta_k=-\frac{1}{2}\log(1+\tan^2\vartheta_k)>-\frac{1}{2}\tan^2\vartheta_k=-\frac{1}{2}\frac{\sin^2\vartheta_k}{\cos^2\vartheta_k}>-2\vartheta_k^2.$$ Therefore we can write that $$\log P_n>-2\pi^2\sum_{k=1}^n \frac{1}{(k+2)^2}>-2\pi^2\int_2^{\infty}\frac{dx}{x^2}=-\pi^2$$ i.e. $P_n>e^{-\pi^2}$.","Let $\displaystyle P_n:=\prod_{k=1}^n \cos\frac{\pi}{k+2}$. Evaluate $\displaystyle\lim_{n\to \infty} P_n$. I've only shown that the limit is positive. Let $\vartheta_k:=\pi/(k+2)$. We have $\log P_n=\sum_{k=1}^n \log\cos\vartheta_k$. Now,  $$\log\cos\vartheta_k=-\frac{1}{2}\log(1+\tan^2\vartheta_k)>-\frac{1}{2}\tan^2\vartheta_k=-\frac{1}{2}\frac{\sin^2\vartheta_k}{\cos^2\vartheta_k}>-2\vartheta_k^2.$$ Therefore we can write that $$\log P_n>-2\pi^2\sum_{k=1}^n \frac{1}{(k+2)^2}>-2\pi^2\int_2^{\infty}\frac{dx}{x^2}=-\pi^2$$ i.e. $P_n>e^{-\pi^2}$.",,"['calculus', 'limits']"
60,Why do we need $x_0$ to be a cluster point if we take take the limit $\lim_{x \to x_0} f(x)$?,Why do we need  to be a cluster point if we take take the limit ?,x_0 \lim_{x \to x_0} f(x),"We did limits of functions recently and I am wondering why we always required that $x_0$ is a cluster point of the domain. Why would taking the limit not work if $x_0$ is not a cluster point? Our definition of a limit of a function is Let $D \subseteq \mathbb R$ be a subset, $x_0$ a cluster point of $D$ and $f: D \to \mathbb R$ a function. We say $f$ converges to $L \in \mathbb R$ and write $\lim_{x \to x_0} f(x) = L$ $\iff \forall \varepsilon \gt 0 \, \exists \delta \gt 0 \, \forall x \in D \setminus \{x_0\}: |x-x_0| \lt \delta \implies |f(x)-L| \lt \varepsilon$ Our definition of a cluster point is Let $D \subseteq \mathbb R$ be a subset and $x_0\in \mathbb {R}$. We say $x_0$ is a cluster point of $D$ $\iff$ for every $\delta \gt 0$ we have $D \cap (x_0-\delta, x_0-\delta) \setminus \{x_0\} \neq \emptyset$","We did limits of functions recently and I am wondering why we always required that $x_0$ is a cluster point of the domain. Why would taking the limit not work if $x_0$ is not a cluster point? Our definition of a limit of a function is Let $D \subseteq \mathbb R$ be a subset, $x_0$ a cluster point of $D$ and $f: D \to \mathbb R$ a function. We say $f$ converges to $L \in \mathbb R$ and write $\lim_{x \to x_0} f(x) = L$ $\iff \forall \varepsilon \gt 0 \, \exists \delta \gt 0 \, \forall x \in D \setminus \{x_0\}: |x-x_0| \lt \delta \implies |f(x)-L| \lt \varepsilon$ Our definition of a cluster point is Let $D \subseteq \mathbb R$ be a subset and $x_0\in \mathbb {R}$. We say $x_0$ is a cluster point of $D$ $\iff$ for every $\delta \gt 0$ we have $D \cap (x_0-\delta, x_0-\delta) \setminus \{x_0\} \neq \emptyset$",,"['real-analysis', 'limits']"
61,Asymptotic behaviour of the number of sign changes in the sequence $\cos n\alpha$,Asymptotic behaviour of the number of sign changes in the sequence,\cos n\alpha,Let $0$ $\leq$ $\alpha$ $\leq$ $\pi$. Denote by $V_n$$(\alpha)$ the number of sign changes in the sequence ${u_n}$ where $u_n$ $=$ $\displaystyle \cos n\alpha$. Then find the limit of the sequence $\frac{V_n(\alpha)}{n}$ as $n$ $\to$ $\infty$. I am not getting any idea as to how to proceed because the sign changes is irregular. Any help is appreciated.,Let $0$ $\leq$ $\alpha$ $\leq$ $\pi$. Denote by $V_n$$(\alpha)$ the number of sign changes in the sequence ${u_n}$ where $u_n$ $=$ $\displaystyle \cos n\alpha$. Then find the limit of the sequence $\frac{V_n(\alpha)}{n}$ as $n$ $\to$ $\infty$. I am not getting any idea as to how to proceed because the sign changes is irregular. Any help is appreciated.,,"['limits', 'asymptotics']"
62,Consider the set $A_N$ of all fractions $\left(\frac{3}{2}\right)^n \pmod{1}$ for $n\le N.$ Prove that $\min(A_N)→0$ as $N→∞.$,Consider the set  of all fractions  for  Prove that  as,A_N \left(\frac{3}{2}\right)^n \pmod{1} n\le N. \min(A_N)→0 N→∞.,"This is related to the well-known unsolved problem in number theory that concerns the distribution of $(3/2)^n \pmod{1}$ . This sequence is believed to be uniformly distributed. Has this simpler problem been proven before? I think that it may be done by a simple proof by contradiction, but my main concern is if it has been done before. The set $A_7$ is $\{1/2, 1/4, 3/8, 1/16, 19/32, 25/64, 11/128\}.$ It seems very intuitive that the lower limit is 0. To clarify, this question applies to n, where n is a natural number.","This is related to the well-known unsolved problem in number theory that concerns the distribution of . This sequence is believed to be uniformly distributed. Has this simpler problem been proven before? I think that it may be done by a simple proof by contradiction, but my main concern is if it has been done before. The set is It seems very intuitive that the lower limit is 0. To clarify, this question applies to n, where n is a natural number.","(3/2)^n \pmod{1} A_7 \{1/2, 1/4, 3/8, 1/16, 19/32, 25/64, 11/128\}.","['limits', 'discrete-mathematics', 'uniform-distribution', 'fractional-part', 'modular-function']"
63,Can a continuous function be split into sum of continuous and discontinuous function? [closed],Can a continuous function be split into sum of continuous and discontinuous function? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question Suppose $a$ and $b$ are functions of $x$ . When $$ \lim_{x \to +\infty} a = c\quad\text{and}\quad \lim_{x \to +\infty} b\text{ does not exist ?} $$ Is it guaranteed that $$ \lim_{x \to +\infty} a + b\text{ does not exist} $$ Sequence version","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question Suppose and are functions of . When Is it guaranteed that Sequence version","a b x 
\lim_{x \to +\infty} a = c\quad\text{and}\quad
\lim_{x \to +\infty} b\text{ does not exist ?}
 
\lim_{x \to +\infty} a + b\text{ does not exist}
",['limits']
64,Evaluate $\lim\limits_{n\to \infty}\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{6n}$,Evaluate,\lim\limits_{n\to \infty}\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{6n},Show that $$\lim_{n\to \infty}\left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{6n}\right)=\log 6$$ Here I need to use the definition of integral but I  faced problem in range . Please help.,Show that $$\lim_{n\to \infty}\left(\frac{1}{n+1}+\frac{1}{n+2}+\cdots+\frac{1}{6n}\right)=\log 6$$ Here I need to use the definition of integral but I  faced problem in range . Please help.,,"['integration', 'limits']"
65,How to calculate $\lim_{x \to \infty} \left ( \frac{x+2}{x} \right )^{x}=e^{2}$,How to calculate,\lim_{x \to \infty} \left ( \frac{x+2}{x} \right )^{x}=e^{2},I know from an online calculator http://www.numberempire.com/derivatives.php that  $\lim_{x \to \infty} \left ( \frac{x+2}{x} \right )^{x}=e^{2}$.  How do you calculate this step by step?,I know from an online calculator http://www.numberempire.com/derivatives.php that  $\lim_{x \to \infty} \left ( \frac{x+2}{x} \right )^{x}=e^{2}$.  How do you calculate this step by step?,,['calculus']
66,Question on limit: $\lim_{x\to 0}\large \frac{\sin^2{x^{2}}}{x^{2}}$,Question on limit:,\lim_{x\to 0}\large \frac{\sin^2{x^{2}}}{x^{2}},How would I solve the following trig equations? $$\lim_{x\to 0}\frac{\sin^2{x^{2}}}{x^{2}}$$ I am thinking the limit would be zero but I am not sure.,How would I solve the following trig equations? $$\lim_{x\to 0}\frac{\sin^2{x^{2}}}{x^{2}}$$ I am thinking the limit would be zero but I am not sure.,,"['calculus', 'limits']"
67,How do I get rid of the radical in the denominator of $\lim\limits_{x \to -9} \frac{x+9}{\sqrt{x+9}}$?,How do I get rid of the radical in the denominator of ?,\lim\limits_{x \to -9} \frac{x+9}{\sqrt{x+9}},$\lim\limits_{x \to -9} \frac{x+9}{\sqrt{x+9}}$ I know the answer is $0$ from looking at the graph but I want to know how to solve this algebraically. I tried to use L'hopital's rule but that doesn't get rid of the radical in the denominator. Then I tried to multiply the top and bottom by $\sqrt{x+9}$ and after trying to cancel stuff out I still ended up with an $x+9$ in the denominator. The I tried to split up the limit by taking the limit of the numerator and putting that over the limit of the denominator and then put the limit in the denominator inside the radical but I would still get $0$ in the denominator. What method am I suppose to use to solve this?,$\lim\limits_{x \to -9} \frac{x+9}{\sqrt{x+9}}$ I know the answer is $0$ from looking at the graph but I want to know how to solve this algebraically. I tried to use L'hopital's rule but that doesn't get rid of the radical in the denominator. Then I tried to multiply the top and bottom by $\sqrt{x+9}$ and after trying to cancel stuff out I still ended up with an $x+9$ in the denominator. The I tried to split up the limit by taking the limit of the numerator and putting that over the limit of the denominator and then put the limit in the denominator inside the radical but I would still get $0$ in the denominator. What method am I suppose to use to solve this?,,"['calculus', 'limits']"
68,I want to find $\lim\limits_{x\to 5}\frac{2^x-2^5}{x-5}$ without using l'Hopital's rule,I want to find  without using l'Hopital's rule,\lim\limits_{x\to 5}\frac{2^x-2^5}{x-5},I want to solve this limit without using L'Hopital's rule: $$\lim_{x\to 5}\frac{2^x-2^5}{x-5}.$$ And thanks.,I want to solve this limit without using L'Hopital's rule: $$\lim_{x\to 5}\frac{2^x-2^5}{x-5}.$$ And thanks.,,['calculus']
69,Find $\lim_{n\to\infty}\frac{a^n}{n!}$ [duplicate],Find  [duplicate],\lim_{n\to\infty}\frac{a^n}{n!},"This question already has answers here : Calculus - Prove $\lim_{n \to \infty} \frac{x^n}{n!}=0$ [duplicate] (2 answers) Closed 6 years ago . First I tried to use integration: $$y=\lim_{n\to\infty}\frac{a^n}{n!}=\lim_{n\to\infty}\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{n}$$ $$\log y=\lim_{n\to\infty}\sum_{r=1}^n\log\frac{a}{r}$$ But I could not express it as a riemann integral . Now I am thinking about sandwich theorem. $$\frac{a}{n!}=\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{t} \cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}=\frac{a}{t!}\cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}$$ Since $\frac{a}{t+1}>\frac{a}{t+2}>\frac{a}{t+1}>\cdots>\frac{a}{n}$ $$\frac{a^n}{n!}<\frac{a^t}{t!}\cdot\big(\frac{a}{t+1}\big)^{n-t}$$ since $\frac{a}{t+1}<1$, $$\lim_{n\to\infty}\big(\frac{a}{t+1}\big)^{n-t}=0$$ Hence, $$\lim_{n\to\infty}\frac{a^t}{t!}\big(\frac{a}{t+1}\big)^{n-t}=0$$ And by using sandwich theorem, $y=0$. Is this correct?","This question already has answers here : Calculus - Prove $\lim_{n \to \infty} \frac{x^n}{n!}=0$ [duplicate] (2 answers) Closed 6 years ago . First I tried to use integration: $$y=\lim_{n\to\infty}\frac{a^n}{n!}=\lim_{n\to\infty}\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{n}$$ $$\log y=\lim_{n\to\infty}\sum_{r=1}^n\log\frac{a}{r}$$ But I could not express it as a riemann integral . Now I am thinking about sandwich theorem. $$\frac{a}{n!}=\frac{a}{1}\cdot\frac{a}{2}\cdot\frac{a}{3}\cdots\frac{a}{t} \cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}=\frac{a}{t!}\cdot\frac{a}{t+1}\cdot\frac{a}{t+2}\cdots\frac{a}{n}$$ Since $\frac{a}{t+1}>\frac{a}{t+2}>\frac{a}{t+1}>\cdots>\frac{a}{n}$ $$\frac{a^n}{n!}<\frac{a^t}{t!}\cdot\big(\frac{a}{t+1}\big)^{n-t}$$ since $\frac{a}{t+1}<1$, $$\lim_{n\to\infty}\big(\frac{a}{t+1}\big)^{n-t}=0$$ Hence, $$\lim_{n\to\infty}\frac{a^t}{t!}\big(\frac{a}{t+1}\big)^{n-t}=0$$ And by using sandwich theorem, $y=0$. Is this correct?",,"['calculus', 'limits']"
70,Finding the limit of $ \lim_{x\to0} \frac {\sqrt[3]{1+x} - 1} {x} $,Finding the limit of, \lim_{x\to0} \frac {\sqrt[3]{1+x} - 1} {x} ,$$ \lim_{x\to0} \frac {\sqrt[3]{1+x} - 1} {x} $$ I think I'm missing out on something. Is there a concept to factor $\sqrt[3] {1+x}$? Thanks.,$$ \lim_{x\to0} \frac {\sqrt[3]{1+x} - 1} {x} $$ I think I'm missing out on something. Is there a concept to factor $\sqrt[3] {1+x}$? Thanks.,,"['real-analysis', 'calculus', 'limits']"
71,What is the result of $\lim_{x\to0_+} \frac{e^x - x e^x - 1}{\left(e^x - 1 \right)^2}$ without L'Hôpital's rule. [closed],What is the result of  without L'Hôpital's rule. [closed],\lim_{x\to0_+} \frac{e^x - x e^x - 1}{\left(e^x - 1 \right)^2},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have the limit $$ \lim_{x\to0_+} \frac{e^x - x e^x - 1}{\left(e^x - 1 \right)^2} $$ which I need to compute without L'Hôpital's rule. (The result is $-\frac{1}{2}$ with L'Hôpital's rule). Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have the limit $$ \lim_{x\to0_+} \frac{e^x - x e^x - 1}{\left(e^x - 1 \right)^2} $$ which I need to compute without L'Hôpital's rule. (The result is $-\frac{1}{2}$ with L'Hôpital's rule). Thanks.",,"['calculus', 'limits', 'limits-without-lhopital']"
72,Finding $\lim_{n\to\infty}\sqrt[n]{3^n + 4^n}$ [closed],Finding  [closed],\lim_{n\to\infty}\sqrt[n]{3^n + 4^n},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Evaluate the limit $$\lim_{n\to\infty}\sqrt[\large n]{3^n + 4^n}.$$ Sorry, I'm not really sure how to find this limit. Can someone share a clever trick for this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Evaluate the limit $$\lim_{n\to\infty}\sqrt[\large n]{3^n + 4^n}.$$ Sorry, I'm not really sure how to find this limit. Can someone share a clever trick for this?",,"['calculus', 'limits']"
73,Is the limit not infinity?,Is the limit not infinity?,,Is the limit of this not infinity? No matter what the value of p is? Or is there a way to simplify that fraction? $$\lim_{k \to \infty} 2^{p}\left(\frac{k}{k+1}\right)^k$$,Is the limit of this not infinity? No matter what the value of p is? Or is there a way to simplify that fraction? $$\lim_{k \to \infty} 2^{p}\left(\frac{k}{k+1}\right)^k$$,,"['calculus', 'limits']"
74,Another difficultl limit I need help with,Another difficultl limit I need help with,,"Can someone help me calculate: $$  \lim_{x \to \infty} \frac {xe^{x/2}}{1+e^x}\quad?$$ Using l'Hospital doesn't help, but I can't figure out how to do it with Taylor polynomial... it doesn't give me anything! Help anyone? Thanks!","Can someone help me calculate: $$  \lim_{x \to \infty} \frac {xe^{x/2}}{1+e^x}\quad?$$ Using l'Hospital doesn't help, but I can't figure out how to do it with Taylor polynomial... it doesn't give me anything! Help anyone? Thanks!",,"['calculus', 'limits']"
75,$\sum a_{n}$ converges but $\sum a_{n}^2 $ diverges?,converges but  diverges?,\sum a_{n} \sum a_{n}^2 ,I have to give an example of a convergent series $\sum a_{n}$ for which $\sum a_{n}^2 $ diverges. I think that such a series cannot exist because if $\sum a_{n}$ converges absolutely then $\sum a_{n}^2 $ will always converge right?,I have to give an example of a convergent series for which diverges. I think that such a series cannot exist because if converges absolutely then will always converge right?,\sum a_{n} \sum a_{n}^2  \sum a_{n} \sum a_{n}^2 ,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
76,How can we define the limit of a constant function?,How can we define the limit of a constant function?,,"Wikipedia says: In mathematics, a limit is the value that a function(or sequence) ""approaches"" as the input (or index) ""approaches"" some value. What if the function was a constant?! A constant function will not approach anything, so, how would we define the limit of a constant function?","Wikipedia says: In mathematics, a limit is the value that a function(or sequence) ""approaches"" as the input (or index) ""approaches"" some value. What if the function was a constant?! A constant function will not approach anything, so, how would we define the limit of a constant function?",,"['calculus', 'limits']"
77,"Determine $\lim\limits_{x\to 0, x\neq 0}\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{1}{2}$ without using L'Hospital",Determine  without using L'Hospital,"\lim\limits_{x\to 0, x\neq 0}\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{1}{2}","How to prove that $$\lim\limits_{x\to 0, x\neq 0}\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{1}{2}$$ without using L'Hospital? Using L'Hospital, it's quite easy. But without, I don't get this. I tried different approaches, for example writing $$e^{\sin(x)}=\sum\limits_{k=0}^\infty\frac{\sin(x)^k}{k!}$$ and $$\sin(2x)=2\sin(x)\cos(x)$$ and get $$\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{\sin(x)+\sum\limits_{k=2}^\infty\frac{\sin(x)^k}{k!} }{2\sin(x)\cos(x)}$$ but it seems to be unrewarding. How can I calculate the limit instead? Any advice will be appreciated.","How to prove that $$\lim\limits_{x\to 0, x\neq 0}\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{1}{2}$$ without using L'Hospital? Using L'Hospital, it's quite easy. But without, I don't get this. I tried different approaches, for example writing $$e^{\sin(x)}=\sum\limits_{k=0}^\infty\frac{\sin(x)^k}{k!}$$ and $$\sin(2x)=2\sin(x)\cos(x)$$ and get $$\frac{e^{\sin(x)}-1}{\sin(2x)}=\frac{\sin(x)+\sum\limits_{k=2}^\infty\frac{\sin(x)^k}{k!} }{2\sin(x)\cos(x)}$$ but it seems to be unrewarding. How can I calculate the limit instead? Any advice will be appreciated.",,['calculus']
78,Simple limit calculation $\lim_{n\to\infty} \sqrt{n(n + 1)}/(n+1)$,Simple limit calculation,\lim_{n\to\infty} \sqrt{n(n + 1)}/(n+1),"Why is this limit equals 1? $$\eqalign{   & \mathop {\lim }\limits_{n \to \infty } {{\sqrt n \cdot\sqrt {n + 1} } \over {n + 1}} = 1  \cr    &  \cr} $$ I tried dividing by n, but it gives 0/0, which isn't so great..","Why is this limit equals 1? $$\eqalign{   & \mathop {\lim }\limits_{n \to \infty } {{\sqrt n \cdot\sqrt {n + 1} } \over {n + 1}} = 1  \cr    &  \cr} $$ I tried dividing by n, but it gives 0/0, which isn't so great..",,"['calculus', 'limits']"
79,"Prove that $\lim\limits_{(x,y) \to (0,0)} \frac{{x{y^2}}}{{{x^2} + {y^4}}} = 0$ [duplicate]",Prove that  [duplicate],"\lim\limits_{(x,y) \to (0,0)} \frac{{x{y^2}}}{{{x^2} + {y^4}}} = 0","This question already has answers here : Computing A Multivariable Limit: $\lim_{(x,y) \to (0,0)}\frac{2x^2y}{x^4 + y^2}.$ (6 answers) Closed 6 years ago . $$\lim_{(x,y) \to (0,0)} \frac{{x{y^2}}}{{{x^2} + {y^4}}} = 0$$ Please, Anyone could suggest me some way for this?. Thanks.","This question already has answers here : Computing A Multivariable Limit: $\lim_{(x,y) \to (0,0)}\frac{2x^2y}{x^4 + y^2}.$ (6 answers) Closed 6 years ago . $$\lim_{(x,y) \to (0,0)} \frac{{x{y^2}}}{{{x^2} + {y^4}}} = 0$$ Please, Anyone could suggest me some way for this?. Thanks.",,"['real-analysis', 'limits', 'multivariable-calculus']"
80,Is $\sqrt{2\sqrt[3]{3!\sqrt[4]{4!\sqrt[5]{5!..}}}}=2\log \pi$,Is,\sqrt{2\sqrt[3]{3!\sqrt[4]{4!\sqrt[5]{5!..}}}}=2\log \pi,Is  $$\sqrt{2\sqrt[3]{3!\sqrt[4]{4!\sqrt[5]{5!..}}}}=2\log \pi$$ Can anyone help me to know the way of proving above if it is true?,Is  $$\sqrt{2\sqrt[3]{3!\sqrt[4]{4!\sqrt[5]{5!..}}}}=2\log \pi$$ Can anyone help me to know the way of proving above if it is true?,,"['real-analysis', 'sequences-and-series', 'limits', 'pi', 'nested-radicals']"
81,Limit of a 0/0 function,Limit of a 0/0 function,,"Let's say we have a function, for example, $$ f(x) = \frac{x-1}{x^2+2x-3}, $$ and we want to now what is  $$ \lim_{x \to 1} f(x). $$ The result is $\frac{1}{4}$. So there exists a limit as $x \to 1$. My teacher says that the limit at $x=1$ doesn't exist. How is that? I don't understand it. We know that a limit exists when the one sided limits are the same result. Thank you!","Let's say we have a function, for example, $$ f(x) = \frac{x-1}{x^2+2x-3}, $$ and we want to now what is  $$ \lim_{x \to 1} f(x). $$ The result is $\frac{1}{4}$. So there exists a limit as $x \to 1$. My teacher says that the limit at $x=1$ doesn't exist. How is that? I don't understand it. We know that a limit exists when the one sided limits are the same result. Thank you!",,"['calculus', 'limits', 'functions']"
82,Evaluate $\lim_{x\rightarrow 0} \frac{\sin x}{x + \tan x} $ without L'Hopital,Evaluate  without L'Hopital,\lim_{x\rightarrow 0} \frac{\sin x}{x + \tan x} ,"I need help finding the the following limit: $$\lim_{x\rightarrow 0} \frac{\sin x}{x + \tan x} $$ I tried to simplify to: $$ \lim_{x\rightarrow 0} \frac{\sin x \cos x}{x\cos x+\sin x} $$ but I don't know where to go from there. I think, at some point, you have to use the fact that $\lim_{x\rightarrow 0} \frac{\sin x}{x} = 1$. Any help would be appreciated. Thanks!","I need help finding the the following limit: $$\lim_{x\rightarrow 0} \frac{\sin x}{x + \tan x} $$ I tried to simplify to: $$ \lim_{x\rightarrow 0} \frac{\sin x \cos x}{x\cos x+\sin x} $$ but I don't know where to go from there. I think, at some point, you have to use the fact that $\lim_{x\rightarrow 0} \frac{\sin x}{x} = 1$. Any help would be appreciated. Thanks!",,"['calculus', 'limits']"
83,"Determine $a,b$ such that $y= \begin{cases} 2x^2+x+1 & x\leq 0 \\ ax+b & x > 0 \end{cases} $ is differentiable at $0$",Determine  such that  is differentiable at,"a,b y= \begin{cases} 2x^2+x+1 & x\leq 0 \\ ax+b & x > 0 \end{cases}  0","Let $$y= \begin{cases}        2x^2+x+1 & x\leq 0 \\       ax+b & x >  0     \end{cases} $$ Determine $a,b$ such that $y$ has a derivative at $0$. So, I used the definition of the derivative: $$\lim_{x \to 0^+} \frac{y(x)-y(0)}{x} = \lim_{x \to 0}\frac{ax+b-1}{x}$$ but the limit is $+\infty$. What am I doing wrong?","Let $$y= \begin{cases}        2x^2+x+1 & x\leq 0 \\       ax+b & x >  0     \end{cases} $$ Determine $a,b$ such that $y$ has a derivative at $0$. So, I used the definition of the derivative: $$\lim_{x \to 0^+} \frac{y(x)-y(0)}{x} = \lim_{x \to 0}\frac{ax+b-1}{x}$$ but the limit is $+\infty$. What am I doing wrong?",,"['calculus', 'limits', 'derivatives']"
84,How to find $\lim _{ n\to \infty } \frac { ({ n!) }^{ 1\over n } }{ n } $? [duplicate],How to find ? [duplicate],\lim _{ n\to \infty } \frac { ({ n!) }^{ 1\over n } }{ n } ,"This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 9 years ago . How to find $\lim _{ n\to \infty  } \frac { ({ n!) }^{ 1\over n } }{ n } $ ? I tried taking using logarithm to bring the expression to sum form and then tried L Hospital's Rule.But its not working.Please help! This is what wolfram alpha is showing,but its not providing the steps! BTW if someone can tell me a method without using integration, I'd love to know!","This question already has answers here : Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ (11 answers) Closed 9 years ago . How to find $\lim _{ n\to \infty  } \frac { ({ n!) }^{ 1\over n } }{ n } $ ? I tried taking using logarithm to bring the expression to sum form and then tried L Hospital's Rule.But its not working.Please help! This is what wolfram alpha is showing,but its not providing the steps! BTW if someone can tell me a method without using integration, I'd love to know!",,['limits']
85,Calculate the limit : $\lim_{x \to 0}\frac{x-\sin{x}}{x^3}$ WITHOUT using L'Hopital's rule [duplicate],Calculate the limit :  WITHOUT using L'Hopital's rule [duplicate],\lim_{x \to 0}\frac{x-\sin{x}}{x^3},"This question already has an answer here : Solve $\lim_{x\to 0} \frac{\sin x-x}{x^3}$ [duplicate] (1 answer) Closed 9 years ago . I was given a task to find  $$\lim_{x\to0}\frac{x-\sin{x}}{x^3}$$ at my school today. I thought it was an easy problem and started differentiating denominator and numerator to calculate the limit but the teacher then said we aren't allowed to use  L'Hopital's rule, but to ""play around"" with known limits and limit definition. I got stuck here since I can't really think of a way to do this, and according to my teacher, there are at least 4 ways. A subtle hint would be enough. Thanks.","This question already has an answer here : Solve $\lim_{x\to 0} \frac{\sin x-x}{x^3}$ [duplicate] (1 answer) Closed 9 years ago . I was given a task to find  $$\lim_{x\to0}\frac{x-\sin{x}}{x^3}$$ at my school today. I thought it was an easy problem and started differentiating denominator and numerator to calculate the limit but the teacher then said we aren't allowed to use  L'Hopital's rule, but to ""play around"" with known limits and limit definition. I got stuck here since I can't really think of a way to do this, and according to my teacher, there are at least 4 ways. A subtle hint would be enough. Thanks.",,"['calculus', 'analysis', 'limits', 'limits-without-lhopital']"
86,Limits of square root [closed],Limits of square root [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question $$\lim_{x\to\infty}\left(\sqrt{x+\sqrt{x+\sqrt{x + \sqrt x} }}-\sqrt x\right) $$ (original screenshot) Compute the limit Can you please help me out with this limit problem","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question $$\lim_{x\to\infty}\left(\sqrt{x+\sqrt{x+\sqrt{x + \sqrt x} }}-\sqrt x\right) $$ (original screenshot) Compute the limit Can you please help me out with this limit problem",,"['analysis', 'limits', 'arithmetic', 'radicals']"
87,Prove $\lim_{x \to 1} \frac{x^b - 1}{x - 1} = b$,Prove,\lim_{x \to 1} \frac{x^b - 1}{x - 1} = b,"Prove that $$   \lim_{x \to 1} \frac{x^b - 1}{x - 1} = b $$ (No L'Hospital's rule, or series) I'm not sure how to go about this. I have that $x^b = e^{b \ln(x)}$, which gives $$   \lim_{x \to 1} \frac{x^b - 1}{x - 1} =    \lim_{x \to 1} \frac{e^{b \ln(x)}- 1}{x - 1} $$ But this doesn't (seem to) do much. I also have that $1 - \frac{1}{x} < \ln(x) < x - 1$.","Prove that $$   \lim_{x \to 1} \frac{x^b - 1}{x - 1} = b $$ (No L'Hospital's rule, or series) I'm not sure how to go about this. I have that $x^b = e^{b \ln(x)}$, which gives $$   \lim_{x \to 1} \frac{x^b - 1}{x - 1} =    \lim_{x \to 1} \frac{e^{b \ln(x)}- 1}{x - 1} $$ But this doesn't (seem to) do much. I also have that $1 - \frac{1}{x} < \ln(x) < x - 1$.",,"['calculus', 'real-analysis', 'limits']"
88,Why does this limit approach $e$? [duplicate],Why does this limit approach ? [duplicate],e,This question already has answers here : Why isn't $\lim \limits_{x\to\infty}\left(1+\frac{1}{x}\right)^{x}$ equal to $1$? (10 answers) Closed 8 years ago . Why does  $$(1+1/k)^{k}\rightarrow e\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$ why does it not approach $$(1+1/k)^{k}\rightarrow (1+0)^k=1\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$,This question already has answers here : Why isn't $\lim \limits_{x\to\infty}\left(1+\frac{1}{x}\right)^{x}$ equal to $1$? (10 answers) Closed 8 years ago . Why does  $$(1+1/k)^{k}\rightarrow e\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$ why does it not approach $$(1+1/k)^{k}\rightarrow (1+0)^k=1\hspace{0.5cm}\text{as}\hspace{0.5cm}k\rightarrow\infty$$,,"['calculus', 'limits']"
89,"How to factorise $x^4 - 3x^3 + 2$, so as to compute the limit of a quotient?","How to factorise , so as to compute the limit of a quotient?",x^4 - 3x^3 + 2,"Question: Find the limit: $$\lim_{x \to 1}\frac{x^4 - 3x^3 + 2}{x^3 -5x^2+3x+1}$$ The denominator can be simplified to: $$(x-1)(x^2+x)$$ However, I am unable to factor the numerator in a proper manner (so that $(x-1)$ will cancel out) I know upon graphing that the limit is $5\over4$. What should I do here? Note: To be done without the use of L'Hospital Rule","Question: Find the limit: $$\lim_{x \to 1}\frac{x^4 - 3x^3 + 2}{x^3 -5x^2+3x+1}$$ The denominator can be simplified to: $$(x-1)(x^2+x)$$ However, I am unable to factor the numerator in a proper manner (so that $(x-1)$ will cancel out) I know upon graphing that the limit is $5\over4$. What should I do here? Note: To be done without the use of L'Hospital Rule",,"['calculus', 'limits', 'limits-without-lhopital']"
90,Evaluate: $\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta}{\theta - \frac {\pi}{4}}$,Evaluate:,\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta}{\theta - \frac {\pi}{4}},Evaluate: $\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta}{\theta - \dfrac {\pi}{4}}$. My Attempt: \begin{align} \lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta }{\theta - \dfrac {\pi}{4}} &=\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \cos \dfrac {\pi}{4} + \sin \dfrac {\pi}{4} - \sin \theta}{\theta - \dfrac {\pi}{4}} \\ &=\lim_{\theta \to \frac {\pi}{4}} \dfrac {2\sin \dfrac {\pi-4\theta }{8}\cos \dfrac {\pi+4\theta}{8} - 2\sin \dfrac {4\theta + \pi}{8}\sin \dfrac {4\theta -\pi}{8}}{\theta - \dfrac {\pi}{4}}. \end{align} How do I proceed?,Evaluate: $\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta}{\theta - \dfrac {\pi}{4}}$. My Attempt: \begin{align} \lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \sin \theta }{\theta - \dfrac {\pi}{4}} &=\lim_{\theta \to \frac {\pi}{4}} \dfrac {\cos \theta - \cos \dfrac {\pi}{4} + \sin \dfrac {\pi}{4} - \sin \theta}{\theta - \dfrac {\pi}{4}} \\ &=\lim_{\theta \to \frac {\pi}{4}} \dfrac {2\sin \dfrac {\pi-4\theta }{8}\cos \dfrac {\pi+4\theta}{8} - 2\sin \dfrac {4\theta + \pi}{8}\sin \dfrac {4\theta -\pi}{8}}{\theta - \dfrac {\pi}{4}}. \end{align} How do I proceed?,,"['calculus', 'limits', 'trigonometry', 'fractions', 'limits-without-lhopital']"
91,How to find $\lim_{x \to \frac{\pi}{6}}\frac{\sin(x)-\frac{1}{2}}{x-\frac{\pi}{6}}$ without using L'Hospital's Rule?,How to find  without using L'Hospital's Rule?,\lim_{x \to \frac{\pi}{6}}\frac{\sin(x)-\frac{1}{2}}{x-\frac{\pi}{6}},"I have to find $$\lim_{x \to \frac{\pi}{6}}\frac{\sin(x)-\frac{1}{2}}{x-\frac{\pi}{6}}$$ We are not allowed to use L'Hospital's rule, any suggestions would be beneficial! I have tried multiplying by the conjugate (both the numerator and the denominator). I have tried using trig substitution.","I have to find $$\lim_{x \to \frac{\pi}{6}}\frac{\sin(x)-\frac{1}{2}}{x-\frac{\pi}{6}}$$ We are not allowed to use L'Hospital's rule, any suggestions would be beneficial! I have tried multiplying by the conjugate (both the numerator and the denominator). I have tried using trig substitution.",,"['calculus', 'limits', 'limits-without-lhopital']"
92,How do I prove that $\lim_{n\to\infty} n\sqrt{2-2\cos(\frac{2\pi}{n})}=2\pi$,How do I prove that,\lim_{n\to\infty} n\sqrt{2-2\cos(\frac{2\pi}{n})}=2\pi,I have to prove that $$ \lim_{n\to\infty} n\sqrt{2-2\cos\left(\frac{2\pi}{n}\right)}=2\pi $$ It's geometrically obvious since it is the limit of the $n$-gons inside a unit circle. But how do I prove it?,I have to prove that $$ \lim_{n\to\infty} n\sqrt{2-2\cos\left(\frac{2\pi}{n}\right)}=2\pi $$ It's geometrically obvious since it is the limit of the $n$-gons inside a unit circle. But how do I prove it?,,['limits']
93,finding the limit to $(3^n+5^n)^{\frac{1}{n}}$,finding the limit to,(3^n+5^n)^{\frac{1}{n}},"I was wondering if one can find the limit to the sequence $\{a_n\}$, where: $$\large a_n=(3^n+5^n)^{\large\frac{1}{n}}$$ Without the use of a calculator.","I was wondering if one can find the limit to the sequence $\{a_n\}$, where: $$\large a_n=(3^n+5^n)^{\large\frac{1}{n}}$$ Without the use of a calculator.",,"['calculus', 'sequences-and-series', 'limits']"
94,$0/0$ limit question,limit question,0/0,"I have an examples book with a limit exercise that I can't understand. The limit in question is: $$f(x,y)=\frac{x}{x+y}$$ with $x\ne-y$; $$\lim_{(x,y)\to(0,0)} f(x,y)$$ And then to solve it, it goes: $$\lim_{(x,y)\to(0,0)} f(x,y) = \lim_{x\to0} f(x,mx) =\lim_{x\to 0}\frac{x}{x+mx}=\frac{1}{1+m}.$$ Can you help me understand that? Thanks, UPDATE : Ok, just to make sure that I got it right. I have a very similar test exercise with $4$ different options. The following limit $$\lim_{(x,y)\to(0,0)}\frac{-x^3+3xy^2}{x^2+y^2}$$ equals: A. $0$ B. $- \infty$ C. Doesn't exist D. $ +\infty$ My doubt is: if I consider it normally I'd say that it doesn't exist, but if I solve it using the same approach (i.e. $y=mx$) then the limit equals $0$. Which one is the right answer?","I have an examples book with a limit exercise that I can't understand. The limit in question is: $$f(x,y)=\frac{x}{x+y}$$ with $x\ne-y$; $$\lim_{(x,y)\to(0,0)} f(x,y)$$ And then to solve it, it goes: $$\lim_{(x,y)\to(0,0)} f(x,y) = \lim_{x\to0} f(x,mx) =\lim_{x\to 0}\frac{x}{x+mx}=\frac{1}{1+m}.$$ Can you help me understand that? Thanks, UPDATE : Ok, just to make sure that I got it right. I have a very similar test exercise with $4$ different options. The following limit $$\lim_{(x,y)\to(0,0)}\frac{-x^3+3xy^2}{x^2+y^2}$$ equals: A. $0$ B. $- \infty$ C. Doesn't exist D. $ +\infty$ My doubt is: if I consider it normally I'd say that it doesn't exist, but if I solve it using the same approach (i.e. $y=mx$) then the limit equals $0$. Which one is the right answer?",,"['calculus', 'limits']"
95,Variations of $\lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n$,Variations of,\lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n,"I understand that  $$ \lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n = e $$ However, how would $ \lim_{n\rightarrow \infty} \left(1 + \frac{1}{3n}\right)^{5n} $ be simplified. The $5$ can be moved outside the limit: $$ {\lim_{n\rightarrow \infty} \left[\left(1 + \frac{1}{3n}\right)^{n}\right]^5} $$ But how can you simplify it to the form $e^{\frac{m}{n}}$? The answer is $ e^{\frac{5}{3}} $, but could someone help me understand the methodology?","I understand that  $$ \lim_{n\rightarrow \infty} \left(1 + \frac{1}{n}\right)^n = e $$ However, how would $ \lim_{n\rightarrow \infty} \left(1 + \frac{1}{3n}\right)^{5n} $ be simplified. The $5$ can be moved outside the limit: $$ {\lim_{n\rightarrow \infty} \left[\left(1 + \frac{1}{3n}\right)^{n}\right]^5} $$ But how can you simplify it to the form $e^{\frac{m}{n}}$? The answer is $ e^{\frac{5}{3}} $, but could someone help me understand the methodology?",,"['calculus', 'limits']"
96,"What is $\lim_{p \to 0} \left(\int_0^1 (1+x)^p \, dx\right)^{1/p}$?",What is ?,"\lim_{p \to 0} \left(\int_0^1 (1+x)^p \, dx\right)^{1/p}","What is $$\lim_{p \to 0} \left(\int_0^1 (1+x)^p \, dx\right)^{1/p}\text{ ?}$$ I used binomial to get value as $2^p-1$ so limit becomes $$(2^p-1)^{1/p}.$$ But I can't go any further.","What is $$\lim_{p \to 0} \left(\int_0^1 (1+x)^p \, dx\right)^{1/p}\text{ ?}$$ I used binomial to get value as $2^p-1$ so limit becomes $$(2^p-1)^{1/p}.$$ But I can't go any further.",,"['limits', 'definite-integrals', 'limits-without-lhopital']"
97,Calculate limit without L'Hopital's rule,Calculate limit without L'Hopital's rule,,"I have to calculate this limit whitout using L'Hopital's rule or Taylor polynomials: $$\lim_{ x\to \pi/4 } \frac{1 - \tan(x)}{x-\frac{\pi}{4}}$$ I know how to make it using L'Hopital and that the result is $-2$ ,but I'm getting nowhere when I try without it. Any advice?","I have to calculate this limit whitout using L'Hopital's rule or Taylor polynomials: $$\lim_{ x\to \pi/4 } \frac{1 - \tan(x)}{x-\frac{\pi}{4}}$$ I know how to make it using L'Hopital and that the result is $-2$ ,but I'm getting nowhere when I try without it. Any advice?",,"['calculus', 'limits', 'limits-without-lhopital']"
98,Limit of square root without L'Hopital's rule.,Limit of square root without L'Hopital's rule.,,How might one go about taking the following limit without using L'Hopital's rule? I am stumped: $$\lim_{x \to \infty} \sqrt{x^2 + x} - x$$,How might one go about taking the following limit without using L'Hopital's rule? I am stumped: $$\lim_{x \to \infty} \sqrt{x^2 + x} - x$$,,"['analysis', 'limits']"
99,Find $\lim \limits_{x\to 0}{\sin{42x} \over \sin{6x}-\sin{7x}}$,Find,\lim \limits_{x\to 0}{\sin{42x} \over \sin{6x}-\sin{7x}},"I want to find $$\lim \limits_{x\to 0}{\sin{42x} \over \sin{6x}-\sin{7x}}$$ without resorting to L'Hôpital's rule. Numerically, this computes as $-42$. My idea is to examine two cases: $x>0$ and $x<0$ and use ${\sin{42x} \over \sin{6x}}\to 7$ and ${\sin{42x} \over \sin{7x}}\to 6$. I can't find the appropriate inequalities to use the squeeze theorem, though. Do you have suggestions?","I want to find $$\lim \limits_{x\to 0}{\sin{42x} \over \sin{6x}-\sin{7x}}$$ without resorting to L'Hôpital's rule. Numerically, this computes as $-42$. My idea is to examine two cases: $x>0$ and $x<0$ and use ${\sin{42x} \over \sin{6x}}\to 7$ and ${\sin{42x} \over \sin{7x}}\to 6$. I can't find the appropriate inequalities to use the squeeze theorem, though. Do you have suggestions?",,"['real-analysis', 'limits', 'limits-without-lhopital']"
