,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving the Borel-Cantelli Lemma,Proving the Borel-Cantelli Lemma,,"Let $\{{E_k}\}^{\infty}_{k=1}$ be a countable family of measurable subsets of $\mathbb{R}^d$ and that \begin{equation} % Equation (1) \sum^{\infty}_{k=1}m(E_k)<\infty \end{equation} Let \begin{align*} E&=\{x\in \mathbb{R}^d:x\in E_k, \text{ for infinitely many $k$ }\} \\ &= \underset{k\rightarrow \infty}{\lim \sup}(E_k).\\ \end{align*} (a) Show that $E$ is measurable (b) Prove $m(E)=0.$ My Proof Attempt: Proof . Let the assumptions be as above. We will prove part (a) by showing that \begin{equation*} E=\cap^{\infty}_{n=1}\cup_{k\geq n}E_k.  \end{equation*} Hence, E would be measurable, since for every fixed $n$ , $\cup_{k\geq n}E_k$ is measurable since it is a countable union of measurable sets. Then $\cap^{\infty}_{n=1}\cup_{k\geq n}E_k$ is the countable intersection of measurable sets. From here, we shall denote $\cup_{k\geq n}E_k$ as $S_n$ .  Let $x\in \cap^{\infty}_{n=1}S_n$ .  Then $x\in S_n$ for every $n\in \mathbb{N}$ . Hence, $x$ must be in $E_k$ for infinitely many $k$ , otherwise there would exist an $N\in \mathbb{N}$ such that $x\notin S_N$ . Leaving $x$ out of the intersection. Thus, $\cap^{\infty}_{n=1}S_n\subset E$ . Conversely, let $x\in E.$ Then $x\in E_k$ for infinitely many $k$ . Therefore, $\forall n\in \mathbb{N}$ , $x\in S_n$ . Otherwise, $\exists N\in \mathbb{N}$ such that $x\notin S_N$ . Which would imply that $x\in E_k$ for only $k$ up to $N$ , i.e. finitely many. A contradiction. Therefore, $x\in \cap^{\infty}_{n=1}S_n$ . Hence, they contain one another and equality holds. This proves part (1). Now for part (b). Fix $\epsilon>0$ . We need to show that there exists $N\in \mathbb{N}$ such that \begin{equation*} m(S_N)\leq \epsilon \end{equation*} Then since $E\subset S_N$ , monotonicity of the measure would imply that $m(E)\leq \epsilon$ . Hence, proving our desired conclusion as we let $\epsilon \rightarrow 0$ . Since $\sum^{\infty}_{k=1}m(E_k)<\infty$ , there exists $N\in \mathbb{N}$ such that \begin{equation*} \left| \sum^{\infty}_{k=N}m(E_k)\right |\leq \epsilon \end{equation*} By definition, \begin{equation*} m(S_N)=m(\cup_{k\geq N}E_k)=\sum^{\infty}_{k=N}m(E_k) \end{equation*} Thus, $m(S_N)\leq \epsilon$ . This completes our proof. Any corrections of the proof, or comments on style of the proof are welcome and appreciated. Thank you all for your time.","Let be a countable family of measurable subsets of and that Let (a) Show that is measurable (b) Prove My Proof Attempt: Proof . Let the assumptions be as above. We will prove part (a) by showing that Hence, E would be measurable, since for every fixed , is measurable since it is a countable union of measurable sets. Then is the countable intersection of measurable sets. From here, we shall denote as .  Let .  Then for every . Hence, must be in for infinitely many , otherwise there would exist an such that . Leaving out of the intersection. Thus, . Conversely, let Then for infinitely many . Therefore, , . Otherwise, such that . Which would imply that for only up to , i.e. finitely many. A contradiction. Therefore, . Hence, they contain one another and equality holds. This proves part (1). Now for part (b). Fix . We need to show that there exists such that Then since , monotonicity of the measure would imply that . Hence, proving our desired conclusion as we let . Since , there exists such that By definition, Thus, . This completes our proof. Any corrections of the proof, or comments on style of the proof are welcome and appreciated. Thank you all for your time.","\{{E_k}\}^{\infty}_{k=1} \mathbb{R}^d \begin{equation} % Equation (1)
\sum^{\infty}_{k=1}m(E_k)<\infty
\end{equation} \begin{align*}
E&=\{x\in \mathbb{R}^d:x\in E_k, \text{ for infinitely many k }\} \\
&= \underset{k\rightarrow \infty}{\lim \sup}(E_k).\\
\end{align*} E m(E)=0. \begin{equation*}
E=\cap^{\infty}_{n=1}\cup_{k\geq n}E_k. 
\end{equation*} n \cup_{k\geq n}E_k \cap^{\infty}_{n=1}\cup_{k\geq n}E_k \cup_{k\geq n}E_k S_n x\in \cap^{\infty}_{n=1}S_n x\in S_n n\in \mathbb{N} x E_k k N\in \mathbb{N} x\notin S_N x \cap^{\infty}_{n=1}S_n\subset E x\in E. x\in E_k k \forall n\in \mathbb{N} x\in S_n \exists N\in \mathbb{N} x\notin S_N x\in E_k k N x\in \cap^{\infty}_{n=1}S_n \epsilon>0 N\in \mathbb{N} \begin{equation*}
m(S_N)\leq \epsilon
\end{equation*} E\subset S_N m(E)\leq \epsilon \epsilon \rightarrow 0 \sum^{\infty}_{k=1}m(E_k)<\infty N\in \mathbb{N} \begin{equation*}
\left| \sum^{\infty}_{k=N}m(E_k)\right |\leq \epsilon
\end{equation*} \begin{equation*}
m(S_N)=m(\cup_{k\geq N}E_k)=\sum^{\infty}_{k=N}m(E_k)
\end{equation*} m(S_N)\leq \epsilon","['real-analysis', 'measure-theory', 'proof-verification']"
1,Show that $\sum_{n=1}^\infty \ \frac{x}{n(1+nx^2)}$ converges uniformly on $R$.,Show that  converges uniformly on .,\sum_{n=1}^\infty \ \frac{x}{n(1+nx^2)} R,"I need help with this problem: Show that $$\sum_{n=1}^\infty \ \frac{x}{n(1+nx^2)}$$ converges uniformly on $R$ . So, I tried using the Weierstrass $M$ -test. This is what I did: $f_n(x)=\frac{x}{n(1+nx^2)}$ so by the $M$ -test $|f_n(x)|\leq M_n$ , so $\left|\frac{x}{n(1+nx^2)}\right|\leq \left|\frac{x}{n(nx^2)}\right|=\left|\frac{x}{n^2x^2}\right|\leq \frac{1}{n^2}$ , and $\frac{1}{n^2}$ converges, so the series converges uniformly on $R$ . Is what I'm doing right? Because the assistant teacher said that we can't use the $M$ -test here, why? If I'm wrong, how can I solve this correctly?","I need help with this problem: Show that converges uniformly on . So, I tried using the Weierstrass -test. This is what I did: so by the -test , so , and converges, so the series converges uniformly on . Is what I'm doing right? Because the assistant teacher said that we can't use the -test here, why? If I'm wrong, how can I solve this correctly?",\sum_{n=1}^\infty \ \frac{x}{n(1+nx^2)} R M f_n(x)=\frac{x}{n(1+nx^2)} M |f_n(x)|\leq M_n \left|\frac{x}{n(1+nx^2)}\right|\leq \left|\frac{x}{n(nx^2)}\right|=\left|\frac{x}{n^2x^2}\right|\leq \frac{1}{n^2} \frac{1}{n^2} R M,"['real-analysis', 'calculus', 'sequences-and-series', 'uniform-convergence']"
2,Integrate $\int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx$,Integrate,\int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx,I had to integrate the following integral: \begin{equation} \int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx \end{equation} but I can't find a suitable substitution to find a solution. Nothing I try works out and only seems to make it more complicated. Does anyone have an idea as to how to solve this? I also tried to get help from WolframAlpha but it just says that there is no step-by-step solution available. The sollution by wolfram alpha is: \begin{equation} \int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx = \frac{x\cos(x)}{x+\cos(x)} + c \end{equation},I had to integrate the following integral: but I can't find a suitable substitution to find a solution. Nothing I try works out and only seems to make it more complicated. Does anyone have an idea as to how to solve this? I also tried to get help from WolframAlpha but it just says that there is no step-by-step solution available. The sollution by wolfram alpha is:,"\begin{equation}
\int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx
\end{equation} \begin{equation}
\int\frac{\cos^2(x)-x^2\sin(x)}{(x+\cos(x))^2}dx = \frac{x\cos(x)}{x+\cos(x)} + c
\end{equation}","['real-analysis', 'integration']"
3,"Let $f$ be integrable on $[a,b]$ and suppose for each integrable function $g$ defined on $[a,b]$, $\int^{b}_afg=0$, then $f(x)=0,\forall x\in[a,b]$","Let  be integrable on  and suppose for each integrable function  defined on , , then","f [a,b] g [a,b] \int^{b}_afg=0 f(x)=0,\forall x\in[a,b]","I do not think this is true, but at the same time I am not sure. I know that if we assume that f is continuous instead of integrable then this statement is true. I just do not know how to provide a counterexample if it is false to show that this is wrong. Integrable does not imply continuity I know that much.","I do not think this is true, but at the same time I am not sure. I know that if we assume that f is continuous instead of integrable then this statement is true. I just do not know how to provide a counterexample if it is false to show that this is wrong. Integrable does not imply continuity I know that much.",,"['real-analysis', 'integration', 'continuity']"
4,Inequality $(1+x^k)^{k+1}\geq (1+x^{k+1})^k$,Inequality,(1+x^k)^{k+1}\geq (1+x^{k+1})^k,"Let $k$ be a positive integer and $x$ a positive real number. Prove that $(1+x^k)^{k+1}\geq (1+x^{k+1})^k$. This looks similar to Bernoulli's inequality. If we write $X=x^k$, the inequality is equivalent to $(1+X)^{\frac{k+1}{k}}\geq 1+X^{\frac{k+1}{k}}$, but this is not exactly in the form where we can apply Bernoulli.","Let $k$ be a positive integer and $x$ a positive real number. Prove that $(1+x^k)^{k+1}\geq (1+x^{k+1})^k$. This looks similar to Bernoulli's inequality. If we write $X=x^k$, the inequality is equivalent to $(1+X)^{\frac{k+1}{k}}\geq 1+X^{\frac{k+1}{k}}$, but this is not exactly in the form where we can apply Bernoulli.",,"['real-analysis', 'algebra-precalculus', 'inequality', 'karamata-inequality']"
5,Is there a countably additive set function $m$ on $\mathscr{P}(\mathbb{R})$ such that $m(I) = l(I)$ for every interval $I$?,Is there a countably additive set function  on  such that  for every interval ?,m \mathscr{P}(\mathbb{R}) m(I) = l(I) I,"In the introduction to Chapter 3 (Lebesgue Measure) of H. L. Royden's Real Analysis , he motivates the definition of a measure by saying that we would like to construct a set function $m$ on a collection $\mathscr{M}$ of subsets of $\mathbb{R}$ that assigns a non-negative extended real number to each set $E$ in $\mathscr{M}$ in the following way. Ideally, we should like $m$ to have the following properties: $mE$ is defined for each set $E$ of real numbers; that is, $\mathscr{M} = \mathscr{P}(\mathbb{R})$. For an interval $I$, $mI = l(I)$, where $l(I)$ denotes the length of the interval. If $\{ E_n \}$ is a sequence of disjoint sets (for which $m$ is defined), $m(\cup_n E_n) = \sum_n E_n$. $m$ is translation invariant; that is, if $E$ is a set for which $m$ is defined and if $E + y$ is the set $\{ x + y : x \in E \}$, obtained by replacing each point $x$ in $E$ by the point $x + y$, then $$m(E+y)=mE.$$ He goes on to say that Unfortunately, . . . it is not known whether there is a set function satisfying the first three properties. Does this mean that this is an open problem in measure theory? Has there been any progress on this problem since the textbook was published? Royden does mention in a footnote that assuming the Continuum Hypothesis, one can show that no such function exists. This question asks for details regarding this case.","In the introduction to Chapter 3 (Lebesgue Measure) of H. L. Royden's Real Analysis , he motivates the definition of a measure by saying that we would like to construct a set function $m$ on a collection $\mathscr{M}$ of subsets of $\mathbb{R}$ that assigns a non-negative extended real number to each set $E$ in $\mathscr{M}$ in the following way. Ideally, we should like $m$ to have the following properties: $mE$ is defined for each set $E$ of real numbers; that is, $\mathscr{M} = \mathscr{P}(\mathbb{R})$. For an interval $I$, $mI = l(I)$, where $l(I)$ denotes the length of the interval. If $\{ E_n \}$ is a sequence of disjoint sets (for which $m$ is defined), $m(\cup_n E_n) = \sum_n E_n$. $m$ is translation invariant; that is, if $E$ is a set for which $m$ is defined and if $E + y$ is the set $\{ x + y : x \in E \}$, obtained by replacing each point $x$ in $E$ by the point $x + y$, then $$m(E+y)=mE.$$ He goes on to say that Unfortunately, . . . it is not known whether there is a set function satisfying the first three properties. Does this mean that this is an open problem in measure theory? Has there been any progress on this problem since the textbook was published? Royden does mention in a footnote that assuming the Continuum Hypothesis, one can show that no such function exists. This question asks for details regarding this case.",,['real-analysis']
6,"What is the difference between lemma, axiom, definition, corollary, etc?","What is the difference between lemma, axiom, definition, corollary, etc?",,"In learning basic analysis I am encountering a lot of language that seems somewhat tough to define: Axiom, proposition, definition, lemma, theorem, law, corollary Are there clear separations in definitions between all these things or is there a lot of overlap? Or is it sometimes a judgment call? How do most people use these words?","In learning basic analysis I am encountering a lot of language that seems somewhat tough to define: Axiom, proposition, definition, lemma, theorem, law, corollary Are there clear separations in definitions between all these things or is there a lot of overlap? Or is it sometimes a judgment call? How do most people use these words?",,"['real-analysis', 'proof-writing', 'soft-question', 'definition']"
7,Nonexistence of a homeomorphism between a open set and the unit sphere,Nonexistence of a homeomorphism between a open set and the unit sphere,,"Let $U\subset\mathbb{R^n}$ be a open set and $\mathbb{S^n}$ the unit sphere of $\mathbb{R^{n+1}}$(i.e. $\mathbb{S^n}=\{x\in\mathbb{R^{n+1}}:||x||=1\}$). How can I show that there's no homeomorphism between $U$ and $\mathbb{S^n}$? My progress: As image of connected set over a continuous function is connected and $\mathbb{S^n}$ is connected, if there was such homeomorphism, then $U$ would be connected as well. Now, using the fact that all connected open sets of $\mathbb{R^n}$ is homeomorphic to $\mathbb{R^n}$, it suffice to show that there's no homeomorphism between $\mathbb{R^n}$ and $\mathbb{S^n}$. I know that for all $p\in \mathbb{S^n}$, $\mathbb{S^n}- \{p\}$ is homeomorphic to $\mathbb{R^n}$. Any help would be much appreciated!","Let $U\subset\mathbb{R^n}$ be a open set and $\mathbb{S^n}$ the unit sphere of $\mathbb{R^{n+1}}$(i.e. $\mathbb{S^n}=\{x\in\mathbb{R^{n+1}}:||x||=1\}$). How can I show that there's no homeomorphism between $U$ and $\mathbb{S^n}$? My progress: As image of connected set over a continuous function is connected and $\mathbb{S^n}$ is connected, if there was such homeomorphism, then $U$ would be connected as well. Now, using the fact that all connected open sets of $\mathbb{R^n}$ is homeomorphic to $\mathbb{R^n}$, it suffice to show that there's no homeomorphism between $\mathbb{R^n}$ and $\mathbb{S^n}$. I know that for all $p\in \mathbb{S^n}$, $\mathbb{S^n}- \{p\}$ is homeomorphic to $\mathbb{R^n}$. Any help would be much appreciated!",,"['real-analysis', 'general-topology', 'spheres']"
8,Orthogonality and norm of Hermite polynomials,Orthogonality and norm of Hermite polynomials,,"I wish to prove that the Hermite polynomials defined as $$H_n(x) := (-1)^n e^{x^2} D^n(e^{-x^2})$$ are orthogonal  wrt the inner product $$\langle f,g\rangle = \int_{\mathbb{R}} e^{-x^2} {f(x)}\overline{g(x)} dx $$ and that $||H_n||^2=n!2^n\sqrt{\pi}$. Not much else to do than compute $\langle H_m, H_n \rangle$. The integral becomes $$(-1)^{n+m} \int_{\mathbb{R}} e^{x^2}D^m(e^{-x^2})D^{n}(e ^{-x^2}) dx $$ Let's look at the derivatives of $e^{-x^2}$: $$ \begin{array}{|} \hline n &&D^n(e^{-x^2}) \\ \hline 1 &&-2xe^{-x^2} \\ \hline2 &&-2e^{-x^2}+4x^2e^{-x^2} \\\hline3 &&4xe^{-x^2}+8xe^{-x^2}-8x^3e^{-x^2}\\ \vdots && \vdots\end{array}$$ It is obvious $D^{n}(e^{-x^2}) = P_n(x)e^{-x^2}$ for a polynomial $P_n$ of degree $n$ where the leading coefficient in $P_n$ is $(-1)^n2^n$. It should also be apparant that $P_n$ only contains terms $\{c_kx^k |k \equiv n \mod 2, 0\leq k\leq n, c_k\neq 0\}$. However, I am struggling to find the rest of the coefficients of $P_n$ under closed form. So, I am looking for a way to compute this integral without having to find the coeffiecients. Alas, I still haven't found what I am looking for. Any ideas?","I wish to prove that the Hermite polynomials defined as $$H_n(x) := (-1)^n e^{x^2} D^n(e^{-x^2})$$ are orthogonal  wrt the inner product $$\langle f,g\rangle = \int_{\mathbb{R}} e^{-x^2} {f(x)}\overline{g(x)} dx $$ and that $||H_n||^2=n!2^n\sqrt{\pi}$. Not much else to do than compute $\langle H_m, H_n \rangle$. The integral becomes $$(-1)^{n+m} \int_{\mathbb{R}} e^{x^2}D^m(e^{-x^2})D^{n}(e ^{-x^2}) dx $$ Let's look at the derivatives of $e^{-x^2}$: $$ \begin{array}{|} \hline n &&D^n(e^{-x^2}) \\ \hline 1 &&-2xe^{-x^2} \\ \hline2 &&-2e^{-x^2}+4x^2e^{-x^2} \\\hline3 &&4xe^{-x^2}+8xe^{-x^2}-8x^3e^{-x^2}\\ \vdots && \vdots\end{array}$$ It is obvious $D^{n}(e^{-x^2}) = P_n(x)e^{-x^2}$ for a polynomial $P_n$ of degree $n$ where the leading coefficient in $P_n$ is $(-1)^n2^n$. It should also be apparant that $P_n$ only contains terms $\{c_kx^k |k \equiv n \mod 2, 0\leq k\leq n, c_k\neq 0\}$. However, I am struggling to find the rest of the coefficients of $P_n$ under closed form. So, I am looking for a way to compute this integral without having to find the coeffiecients. Alas, I still haven't found what I am looking for. Any ideas?",,"['real-analysis', 'orthogonal-polynomials', 'hermite-polynomials']"
9,$a = e_1\wedge e_2 + e_3\wedge e_4$ is not decomposable,is not decomposable,a = e_1\wedge e_2 + e_3\wedge e_4,"I'm reading about exterior algebra and wikipedia gives this example of a non decomposable 2 vector: $a = e_1 \wedge e_2 + e_3\wedge e_4$ But it does not tell the reason. I know that decomposable means that it cannot be written as a product, but why? Wikipedia: https://en.m.wikipedia.org/wiki/Exterior_algebra","I'm reading about exterior algebra and wikipedia gives this example of a non decomposable 2 vector: $a = e_1 \wedge e_2 + e_3\wedge e_4$ But it does not tell the reason. I know that decomposable means that it cannot be written as a product, but why? Wikipedia: https://en.m.wikipedia.org/wiki/Exterior_algebra",,"['real-analysis', 'linear-algebra', 'abstract-algebra']"
10,Show that sup $f_n$ and inf $f_n$ are measurable if {$f_n$} is a sequence of measurable functions,Show that sup  and inf  are measurable if {} is a sequence of measurable functions,f_n f_n f_n,"Let {$f_n$} be a sequence of measurable functions E -> $\mathbb{R} \cup \infty$. Show that the functions sup$f_n$ and inf$f_n$ are measurable. I've started by defining f(x) = sup{$f_n(x) | n \in \mathbb{N}$}, but I don't know what to do from here.  I've seen a bunch of proofs online, but they all seem to just state $$ \left\{ x \mid g(x) > a \right\} = \bigcup_{n=1}^{\infty} \left\{ x \mid f_n (x) > a \right\} $$ How are those equivalent to f(x)? We have two definitions for a function f being measurable and they are i) the preimage of every open set is measurable and preimages of $\infty$ and -$\infty$ are measurable. ii) preimage of every interval of the form (c, $\infty$] I think here using (ii) makes more sense","Let {$f_n$} be a sequence of measurable functions E -> $\mathbb{R} \cup \infty$. Show that the functions sup$f_n$ and inf$f_n$ are measurable. I've started by defining f(x) = sup{$f_n(x) | n \in \mathbb{N}$}, but I don't know what to do from here.  I've seen a bunch of proofs online, but they all seem to just state $$ \left\{ x \mid g(x) > a \right\} = \bigcup_{n=1}^{\infty} \left\{ x \mid f_n (x) > a \right\} $$ How are those equivalent to f(x)? We have two definitions for a function f being measurable and they are i) the preimage of every open set is measurable and preimages of $\infty$ and -$\infty$ are measurable. ii) preimage of every interval of the form (c, $\infty$] I think here using (ii) makes more sense",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
11,Prove that the set $\Big\{ 1/(n+1): n \in \mathbb{N} \Big\} \cup \big\{ 0 \big\} $ is closed.,Prove that the set  is closed.,\Big\{ 1/(n+1): n \in \mathbb{N} \Big\} \cup \big\{ 0 \big\} ,"Prove that the set $\Big\{ 1/(n+1): n \in \mathbb{N} \Big\} \cup \big\{ 0 \big\} $ is closed By definition of closed, I know that you have to show that the complement of the set is open. But I don't know how to take its complement. How should I do it? Or are there other ways to show that a set is open?","Prove that the set $\Big\{ 1/(n+1): n \in \mathbb{N} \Big\} \cup \big\{ 0 \big\} $ is closed By definition of closed, I know that you have to show that the complement of the set is open. But I don't know how to take its complement. How should I do it? Or are there other ways to show that a set is open?",,"['real-analysis', 'general-topology', 'elementary-set-theory']"
12,a problem in functional analysis or operator theory,a problem in functional analysis or operator theory,,"I need a small hint for solving this problem please. Problem: Consider the operator $ T: C([ 0,1 ])\to C([ 0,1 ]) $ which is defined as follows  $$ T(u)(x)=\int_{0}^{x^{2}}u(\sqrt s)ds. $$ Prove that for each $\lambda\in \mathbb{R},$ there does not exist  $ u\in C([ 0,1 ]) $ different from null function, such that $ Tu=\lambda u. $ What I have done: I tried to test the proposition with some examples. For instance, I put  $ u(x)=x $  or  $ u(x)=2x^{2}. $ And I found it without having any problems which contradicts with the above. Please let me know if there is something which is helpful.","I need a small hint for solving this problem please. Problem: Consider the operator $ T: C([ 0,1 ])\to C([ 0,1 ]) $ which is defined as follows  $$ T(u)(x)=\int_{0}^{x^{2}}u(\sqrt s)ds. $$ Prove that for each $\lambda\in \mathbb{R},$ there does not exist  $ u\in C([ 0,1 ]) $ different from null function, such that $ Tu=\lambda u. $ What I have done: I tried to test the proposition with some examples. For instance, I put  $ u(x)=x $  or  $ u(x)=2x^{2}. $ And I found it without having any problems which contradicts with the above. Please let me know if there is something which is helpful.",,"['real-analysis', 'functional-analysis', 'analysis', 'operator-theory']"
13,"Is the closure of a discrete subspace of $[0,1]$ necessarily countable?",Is the closure of a discrete subspace of  necessarily countable?,"[0,1]","Question is in the title. According to this , every discrete subspace of $[0,1]$ must be countable.  But what about its closure?","Question is in the title. According to this , every discrete subspace of $[0,1]$ must be countable.  But what about its closure?",,"['real-analysis', 'general-topology']"
14,"$G$ be a non-measurable subgroup of $(\mathbb R,+)$ ; $I$ be a bounded interval , then $m^*(G \cap I)=m^*(I)$?","be a non-measurable subgroup of  ;  be a bounded interval , then ?","G (\mathbb R,+) I m^*(G \cap I)=m^*(I)","Let $G$ be a non-measurable subgroup of $(\mathbb R,+)$ , $I$ be a bounded interval , then is it true that $m^*(G \cap I)=m^*(I)(=|I|)$ ? where $m^*$ denotes the Lebesgue outer measure","Let $G$ be a non-measurable subgroup of $(\mathbb R,+)$ , $I$ be a bounded interval , then is it true that $m^*(G \cap I)=m^*(I)(=|I|)$ ? where $m^*$ denotes the Lebesgue outer measure",,"['real-analysis', 'group-theory']"
15,"How to justify differentiation under the integral in calculation of $ \int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx $?",How to justify differentiation under the integral in calculation of ?," \int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx ","In this post the author uses differentiation under the integral sign to evaluate the integral $$ \int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx $$ by setting up the function $$ F(a) = \int_0^1\frac{\log(1+ax)}{1+x^2}\,\mathrm dx, $$ and computing $F'(a) = \int_0^1\frac{\partial}{\partial a}\big[\frac{\log(1+ax)}{1+x^2}\big]\,\mathrm dx$. My question is how do we formally justify the use of differentiation under the integral sign here? Some thoughts. Let's put $f(x,a) = \frac{\log(1+ax)}{1+x^2}$ for $x\in[0,1]$ and $a$ in some domain $A\subseteq\mathbf R$. I know that sufficient conditions for this procedure are that $f(x,a)$ and $\frac{\partial f}{\partial a}(x,a)$ are continuous on $[0,1]\times A$, that $F(a)$ exists for each $x\in[0,1]$, and that for each $a\in A$, $\big|\frac{\partial f}{\partial a}(x,a)\big|\le g(x)$ where $\int_0^1g(x)\,\mathrm dx<\infty$. I'm not sure how to use these ideas to put together a proof, and maybe we don't need each of these ideas to do it. So how do we justify this?","In this post the author uses differentiation under the integral sign to evaluate the integral $$ \int_0^1\frac{\log(1+x)}{1+x^2}\,\mathrm dx $$ by setting up the function $$ F(a) = \int_0^1\frac{\log(1+ax)}{1+x^2}\,\mathrm dx, $$ and computing $F'(a) = \int_0^1\frac{\partial}{\partial a}\big[\frac{\log(1+ax)}{1+x^2}\big]\,\mathrm dx$. My question is how do we formally justify the use of differentiation under the integral sign here? Some thoughts. Let's put $f(x,a) = \frac{\log(1+ax)}{1+x^2}$ for $x\in[0,1]$ and $a$ in some domain $A\subseteq\mathbf R$. I know that sufficient conditions for this procedure are that $f(x,a)$ and $\frac{\partial f}{\partial a}(x,a)$ are continuous on $[0,1]\times A$, that $F(a)$ exists for each $x\in[0,1]$, and that for each $a\in A$, $\big|\frac{\partial f}{\partial a}(x,a)\big|\le g(x)$ where $\int_0^1g(x)\,\mathrm dx<\infty$. I'm not sure how to use these ideas to put together a proof, and maybe we don't need each of these ideas to do it. So how do we justify this?",,"['real-analysis', 'integration']"
16,"Prove that if $f$ is uniformly continuous on $[a,b]$ and $f$ is uniformly continuous on $[b,c]$, then $f$ is uniformly continuous on $[a,c]$.","Prove that if  is uniformly continuous on  and  is uniformly continuous on , then  is uniformly continuous on .","f [a,b] f [b,c] f [a,c]","Prove that if $f$ is uniformly continuous on $[a,b]$ and $f$ is uniformly continuous on $[b,c]$, then $f$ is uniformly continuous on $[a,c]$. My attempt: Let $\epsilon >0$ be given. Since $f$ is uniformly continuous on $[a,b]$ there $\exists \delta_1 >0$ such that if $x,y \in [a,b]$ and $|x-y|<\delta_1$, then $|f(x)-f(y)|<\epsilon$. Since  $f$ is uniformly continuous on $[b,c]$ there $\exists \delta_2 >0$ such that if $x,y \in b,c]$ and $|x-y|<\delta_2$, then $|f(x)-f(y)|<\epsilon$. Now to show $f$ is continuous on $[a,c]$ how would i show this. Do i sort of add the two above relations?","Prove that if $f$ is uniformly continuous on $[a,b]$ and $f$ is uniformly continuous on $[b,c]$, then $f$ is uniformly continuous on $[a,c]$. My attempt: Let $\epsilon >0$ be given. Since $f$ is uniformly continuous on $[a,b]$ there $\exists \delta_1 >0$ such that if $x,y \in [a,b]$ and $|x-y|<\delta_1$, then $|f(x)-f(y)|<\epsilon$. Since  $f$ is uniformly continuous on $[b,c]$ there $\exists \delta_2 >0$ such that if $x,y \in b,c]$ and $|x-y|<\delta_2$, then $|f(x)-f(y)|<\epsilon$. Now to show $f$ is continuous on $[a,c]$ how would i show this. Do i sort of add the two above relations?",,"['calculus', 'real-analysis', 'continuity', 'epsilon-delta', 'uniform-continuity']"
17,Spectrum of the inverse operator?,Spectrum of the inverse operator?,,How can you prove that the spectrum of the inverse of an operator $A^{-1}$ is given by all $\frac{1}{\lambda}$ for all $\lambda \in \sigma(A)\backslash \{0\}$?,How can you prove that the spectrum of the inverse of an operator $A^{-1}$ is given by all $\frac{1}{\lambda}$ for all $\lambda \in \sigma(A)\backslash \{0\}$?,,['real-analysis']
18,Show that $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ converges uniformly to $0$,Show that  converges uniformly to,f_n(x) = \frac{\sin(nx)}{\sqrt{n}} 0,"I need to show that $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ converges uniformly to $0$. My book has a theorem that says that if $|f_n(x)|<a_n$ and $\sum |a_n|$ converges, then $f_n$ converges uniformly. But $\sum \frac{1}{\sqrt{n}}$ doesn't converge, so I don't know how to do it. Should it be done by definition? $$\left|\frac{\sin(nx)}{\sqrt{n}}\right|\le \left|\frac{1}{\sqrt{n}}\right|$$ if I take $n_0 = \frac{1}{\epsilon^2}$, then $n>n_0\implies n>\frac{1}{\epsilon^2}\implies \sqrt{n}>\frac{1}{\epsilon}\implies \frac{1}{\sqrt{n}}<\epsilon$ therefore it converges uniformly. Now, I need to also show that $f'_n$ diverges in every point of the interval $[0,1]$. The derivative is: $$\frac{n\cos nx}{\sqrt{n}}$$ should I just say that this limit goes to infinity because $n$ grows faster than $\sqrt{n}$?","I need to show that $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ converges uniformly to $0$. My book has a theorem that says that if $|f_n(x)|<a_n$ and $\sum |a_n|$ converges, then $f_n$ converges uniformly. But $\sum \frac{1}{\sqrt{n}}$ doesn't converge, so I don't know how to do it. Should it be done by definition? $$\left|\frac{\sin(nx)}{\sqrt{n}}\right|\le \left|\frac{1}{\sqrt{n}}\right|$$ if I take $n_0 = \frac{1}{\epsilon^2}$, then $n>n_0\implies n>\frac{1}{\epsilon^2}\implies \sqrt{n}>\frac{1}{\epsilon}\implies \frac{1}{\sqrt{n}}<\epsilon$ therefore it converges uniformly. Now, I need to also show that $f'_n$ diverges in every point of the interval $[0,1]$. The derivative is: $$\frac{n\cos nx}{\sqrt{n}}$$ should I just say that this limit goes to infinity because $n$ grows faster than $\sqrt{n}$?",,"['calculus', 'real-analysis', 'sequences-and-series']"
19,Showing that a function $f$ has a unique fixed point in a metric space. [duplicate],Showing that a function  has a unique fixed point in a metric space. [duplicate],f,"This question already has an answer here : To show that $f$ has a fixed point, that is, there exists $x_0 \in X$ such that $f (x_0) = x_o$. (1 answer) Closed 2 years ago . Let $(X, d)$ be a compact metric space, and suppose $f : X → X$ satisfies    $$d(f(x), f(y)) < d(x, y)$$   for all $x \neq y \in X$. Show that f has a unique fixed point. All I've gotten it so far is that we need to somehow use another function $g(x)=(x,f(x))$. Thanks","This question already has an answer here : To show that $f$ has a fixed point, that is, there exists $x_0 \in X$ such that $f (x_0) = x_o$. (1 answer) Closed 2 years ago . Let $(X, d)$ be a compact metric space, and suppose $f : X → X$ satisfies    $$d(f(x), f(y)) < d(x, y)$$   for all $x \neq y \in X$. Show that f has a unique fixed point. All I've gotten it so far is that we need to somehow use another function $g(x)=(x,f(x))$. Thanks",,"['real-analysis', 'analysis', 'metric-spaces', 'fixed-point-theorems', 'banach-fixed-point']"
20,What is the reasoning behind why the ratio test works?,What is the reasoning behind why the ratio test works?,,"The ratio test says that, if we have the positive series $$\sum_{n=1}^{\infty} a_n,$$ such that $\lim_{n \to \infty} \dfrac{a_{n+1}}{a_n} = L$ , then (1) if $L < 1$ , then $\sum_{n=1}^{\infty}a_n$ is absolutely convergent; (2) if $L > 1$ , then $\sum_{n=1}^{\infty}a_n$ is divergent; (3) if $L = 1$ , then the ratio test gives no information. I want to understand the reasons behind why and how this works, rather than just memorising formulae. I have attempted to reason about this theorem myself. It can be seen that we're taking the ratio between the second term in the series, $a_{n+1}$ , and the first term in the series, $a_n$ . This is exactly how one finds the common ratio in a geometric sequence ( $r = \dfrac{a_{n+1}}{a_n} )$ . We then take the limit of this ratio, which I assume is to find the relative rate of change between $a_{n+1}$ and $a_n$ . In which case, it is more effective to take the absolute value of the limit, $\lim_{n \to \infty} \begin{vmatrix}{ \dfrac{a_{n+1}}{a_n} }\end{vmatrix} = L$ . This seems analagous to how the limit comparison test theorem works. Therefore, I presume that, if $L < 1$ , then this implies that $a_n$ has a greater rate of change than $a_{n+1}$ , which implies that each successive term in the series is getting smaller, and so as we go to infinity, each successive term is converging towards $0$ . As such, the series should converge to some value. Analogously, I presume that, if $L > 1$ , then this implies that $a_{n+1}$ has a greater rate of change than $a_n$ , which implies that each successive term in the series is getting larger, and so as we go to infinity, the terms diverge towards infinity. Is this reasoning correct? If anything is incorrect, then please clarify why it is incorrect, and what the correct reasoning is.","The ratio test says that, if we have the positive series such that , then (1) if , then is absolutely convergent; (2) if , then is divergent; (3) if , then the ratio test gives no information. I want to understand the reasons behind why and how this works, rather than just memorising formulae. I have attempted to reason about this theorem myself. It can be seen that we're taking the ratio between the second term in the series, , and the first term in the series, . This is exactly how one finds the common ratio in a geometric sequence ( . We then take the limit of this ratio, which I assume is to find the relative rate of change between and . In which case, it is more effective to take the absolute value of the limit, . This seems analagous to how the limit comparison test theorem works. Therefore, I presume that, if , then this implies that has a greater rate of change than , which implies that each successive term in the series is getting smaller, and so as we go to infinity, each successive term is converging towards . As such, the series should converge to some value. Analogously, I presume that, if , then this implies that has a greater rate of change than , which implies that each successive term in the series is getting larger, and so as we go to infinity, the terms diverge towards infinity. Is this reasoning correct? If anything is incorrect, then please clarify why it is incorrect, and what the correct reasoning is.","\sum_{n=1}^{\infty} a_n, \lim_{n \to \infty} \dfrac{a_{n+1}}{a_n} = L L < 1 \sum_{n=1}^{\infty}a_n L > 1 \sum_{n=1}^{\infty}a_n L = 1 a_{n+1} a_n r = \dfrac{a_{n+1}}{a_n} ) a_{n+1} a_n \lim_{n \to \infty} \begin{vmatrix}{ \dfrac{a_{n+1}}{a_n} }\end{vmatrix} = L L < 1 a_n a_{n+1} 0 L > 1 a_{n+1} a_n","['real-analysis', 'sequences-and-series', 'intuition']"
21,"From $C[0, 1]$ to $L^\infty [0,1]$",From  to,"C[0, 1] L^\infty [0,1]","I am dealing with the following exercise: Let $u_n$ bounded in $L^\infty[0,1]$ such that, for any continuous function $f: [0,1]\times R$ to $R$ $$\lim_n \int_0^1 f(x, u_n(x))=\int_0^1 f(x, u(x)).$$ Prove that $u_n$ converges to $u$ in $L^1.$ There is a hint saying to prove it first for $u\in C^0$. I succeeded in proving it for $u$ continuous, choosing $f(x, y)=|y- u(x)|$, but I don't know how to get to $L^\infty$. My idea was to approximate $u$ in $L^1$ with continuous functions, (while continuous functions are not dense in $L^\infty$, right?), but I can't see how to work out details. Thank you in advance.","I am dealing with the following exercise: Let $u_n$ bounded in $L^\infty[0,1]$ such that, for any continuous function $f: [0,1]\times R$ to $R$ $$\lim_n \int_0^1 f(x, u_n(x))=\int_0^1 f(x, u(x)).$$ Prove that $u_n$ converges to $u$ in $L^1.$ There is a hint saying to prove it first for $u\in C^0$. I succeeded in proving it for $u$ continuous, choosing $f(x, y)=|y- u(x)|$, but I don't know how to get to $L^\infty$. My idea was to approximate $u$ in $L^1$ with continuous functions, (while continuous functions are not dense in $L^\infty$, right?), but I can't see how to work out details. Thank you in advance.",,"['real-analysis', 'functional-analysis', 'convergence-divergence']"
22,Another definition of a Holder norm,Another definition of a Holder norm,,"$\def\R{\mathbb{R}}$ Consider a space $C^{1,\beta}$ of all continuous differentiable functions $f\colon\R\to\R$ such that their derivative $f'$ is Holder continuous with exponent $\beta$. A standard way to define a norm on this space is to put $$ \|f\|_{1,\beta}:=\sup_{x\in\R} |f(x)|+\sup_{x\in\R} |f'(x)|+\sup_{x\neq y}\frac{|f'(x)-f'(y)|}{|x-y|^\beta}. $$ My question is why do we need here the middle  term $\sup_{x\in\R} |f'(x)|$? If we just put $$ \|f\|:=\sup_{x\in\R} |f(x)|+\sup_{x\neq y}\frac{|f'(x)-f'(y)|}{|x-y|^\beta}. $$ would this be a norm? Would the space equipped with this norm be a Banach space? Would this norm be equivalent to the standard norm $\|\cdot\|_{1,\beta}$? Thanks!","$\def\R{\mathbb{R}}$ Consider a space $C^{1,\beta}$ of all continuous differentiable functions $f\colon\R\to\R$ such that their derivative $f'$ is Holder continuous with exponent $\beta$. A standard way to define a norm on this space is to put $$ \|f\|_{1,\beta}:=\sup_{x\in\R} |f(x)|+\sup_{x\in\R} |f'(x)|+\sup_{x\neq y}\frac{|f'(x)-f'(y)|}{|x-y|^\beta}. $$ My question is why do we need here the middle  term $\sup_{x\in\R} |f'(x)|$? If we just put $$ \|f\|:=\sup_{x\in\R} |f(x)|+\sup_{x\neq y}\frac{|f'(x)-f'(y)|}{|x-y|^\beta}. $$ would this be a norm? Would the space equipped with this norm be a Banach space? Would this norm be equivalent to the standard norm $\|\cdot\|_{1,\beta}$? Thanks!",,"['real-analysis', 'functional-analysis', 'normed-spaces', 'holder-spaces']"
23,"If $\lim_{n\to\infty} x_n =0$, then $\{f(x_n)\}$ is Cauchy?","If , then  is Cauchy?",\lim_{n\to\infty} x_n =0 \{f(x_n)\},"For each positive integer $n$, let $x_n$ be a real number in $\left(0,\frac{1}{n}\right)$. Is the following true? If $f$ is a continuous real-valued function defined on $(0,1)$, then $\{f(x_n)\}_{n=1}^\infty$ is a Cauchy sequence. I can't see why this is wrong. My quick thought was that, if $x_n$ is Cauchy and $f$ is continuous, then $f(x_n)$ is Cauchy as well. I am very sure this is true for compact domain. Is it something going wrong with $x=0$? Could someone give a counterexample?","For each positive integer $n$, let $x_n$ be a real number in $\left(0,\frac{1}{n}\right)$. Is the following true? If $f$ is a continuous real-valued function defined on $(0,1)$, then $\{f(x_n)\}_{n=1}^\infty$ is a Cauchy sequence. I can't see why this is wrong. My quick thought was that, if $x_n$ is Cauchy and $f$ is continuous, then $f(x_n)$ is Cauchy as well. I am very sure this is true for compact domain. Is it something going wrong with $x=0$? Could someone give a counterexample?",,"['real-analysis', 'gre-exam']"
24,"The Jordan Decomposition Theorem, Folland","The Jordan Decomposition Theorem, Folland",,"The Jordan Decomposition Theorem - If $\nu$ is a signed measure, there exists unique positive measures $\nu^+$ and $\nu^-$ such that $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Attempted proof - Let $X = P\cup N$ be a Hahn decomposition for $\nu$ where $P$ and $N$ are positive and negative sets respectively.. Then let us define the positive and negative measure as such $$\nu^{+}(E) := \nu(E\cap P) \ \ \ \nu^{-}(E) := - \nu(E\cap N)$$ then,  \begin{align*} \nu^+(E) - \nu^-(E) &= \nu(E\cap P) + \nu(E\cap N)\\ &= \nu(E) \end{align*} So we have $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Now I believe to complete this proof we need to show uniqueness. As in assuming we have $\nu = \tilde{\nu^{+}} - \tilde{\nu^{-}}$ is another such pair and we have $\tilde{\nu^{+}}\perp\tilde{\nu^{-}}$ we can find $\tilde{N}$ and $\tilde{P}$ such that $X = \tilde{P}\cup\tilde{N}$ and we need to check that $\tilde{P}$ is positive and $\tilde{N}$ is negative (not sure how to do that yet). Then we need to show that $\tilde{\nu^{+}} = \nu^+$ and $\tilde{\nu^{-}} = \nu^-$ again I am not sure how to do that either yet. Any suggestions is greatly appreciated.","The Jordan Decomposition Theorem - If $\nu$ is a signed measure, there exists unique positive measures $\nu^+$ and $\nu^-$ such that $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Attempted proof - Let $X = P\cup N$ be a Hahn decomposition for $\nu$ where $P$ and $N$ are positive and negative sets respectively.. Then let us define the positive and negative measure as such $$\nu^{+}(E) := \nu(E\cap P) \ \ \ \nu^{-}(E) := - \nu(E\cap N)$$ then,  \begin{align*} \nu^+(E) - \nu^-(E) &= \nu(E\cap P) + \nu(E\cap N)\\ &= \nu(E) \end{align*} So we have $\nu = \nu^+ - \nu^-$ and $\nu^+\perp \nu^-$. Now I believe to complete this proof we need to show uniqueness. As in assuming we have $\nu = \tilde{\nu^{+}} - \tilde{\nu^{-}}$ is another such pair and we have $\tilde{\nu^{+}}\perp\tilde{\nu^{-}}$ we can find $\tilde{N}$ and $\tilde{P}$ such that $X = \tilde{P}\cup\tilde{N}$ and we need to check that $\tilde{P}$ is positive and $\tilde{N}$ is negative (not sure how to do that yet). Then we need to show that $\tilde{\nu^{+}} = \nu^+$ and $\tilde{\nu^{-}} = \nu^-$ again I am not sure how to do that either yet. Any suggestions is greatly appreciated.",,"['real-analysis', 'measure-theory']"
25,"Introduction to Measure Theory, length of an Interval.","Introduction to Measure Theory, length of an Interval.",,"Let $I$ be an interval on real line, (in the form of $(a, b)$,$[a,b)$,$(a,b]$ or $[a,b]$, where $a,b \in \mathbb R$) and $|I|:= b-a$. Prove that $$ |I| = \lim_{N\to\infty}\frac{\#\left(I\cap\tfrac{\mathbb Z}{N}\right)}{N}, $$ where $\tfrac{\mathbb Z}{N} := \left\{\tfrac{n}{N} \;\middle|\; n\in \mathbb Z \right\}$ and $\#$ denotes the cardinality of a finite set. This is an observation Terence Tao used at the beginning in his An Introduction to Measure Theory. My idea is to express $\#\left(I\cap\tfrac{\mathbb Z}{N}\right)$ in terms of $N$ with coefficient $(b-a)$ and some other neglectable terms, but I am struggling on how to do it when $I = (a,b)$ where $a,b$ are irrational. Great thanks!","Let $I$ be an interval on real line, (in the form of $(a, b)$,$[a,b)$,$(a,b]$ or $[a,b]$, where $a,b \in \mathbb R$) and $|I|:= b-a$. Prove that $$ |I| = \lim_{N\to\infty}\frac{\#\left(I\cap\tfrac{\mathbb Z}{N}\right)}{N}, $$ where $\tfrac{\mathbb Z}{N} := \left\{\tfrac{n}{N} \;\middle|\; n\in \mathbb Z \right\}$ and $\#$ denotes the cardinality of a finite set. This is an observation Terence Tao used at the beginning in his An Introduction to Measure Theory. My idea is to express $\#\left(I\cap\tfrac{\mathbb Z}{N}\right)$ in terms of $N$ with coefficient $(b-a)$ and some other neglectable terms, but I am struggling on how to do it when $I = (a,b)$ where $a,b$ are irrational. Great thanks!",,"['real-analysis', 'limits', 'measure-theory']"
26,Proving by induction that $n! < (\frac{n+1}{2})^n$,Proving by induction that,n! < (\frac{n+1}{2})^n,"As an analysis homework I have to prove by induction that $n! < (\frac{n+1}{2})^n : (2 \le n \in\mathbb{N})$ For $n = 2$ this is trivial, but for $n+1$ no matter how I transform the equation I can't seem to get $(\frac{n+2}{2})^{n+1}$ on the right-hand side. I'm sure that this is an easy homework and I'm missing something blatantly obvious but the closest I got was $(n+1)! < \frac{(n+2)^{n+1}}{2^{n}}$. Any help is greatly appreciated.","As an analysis homework I have to prove by induction that $n! < (\frac{n+1}{2})^n : (2 \le n \in\mathbb{N})$ For $n = 2$ this is trivial, but for $n+1$ no matter how I transform the equation I can't seem to get $(\frac{n+2}{2})^{n+1}$ on the right-hand side. I'm sure that this is an easy homework and I'm missing something blatantly obvious but the closest I got was $(n+1)! < \frac{(n+2)^{n+1}}{2^{n}}$. Any help is greatly appreciated.",,"['real-analysis', 'inequality', 'induction', 'factorial']"
27,Direct proof of Bolzano-Weierstrass using Axiom of Completeness,Direct proof of Bolzano-Weierstrass using Axiom of Completeness,,"The author of my intro analysis text has an exercise to give a proof of Bolzano-Weierstrass using axiom of completeness directly. Let $(a_n)$ be a bounded sequence, and define the set $$S=\{x \in \mathbb{R} : x<a_n \text{ for infinitely many terms } a_n\}.$$ Show that there exists a subsequence $a_{n_k}$ converging to $s = \sup S$. I feel I am close. I know that for any $\epsilon > 0$, there must be infinitely many $a_n$ such that $\sup S - \epsilon < a_n < \sup S + \epsilon$. (If there were only finitely many $a_n$ in that interval, then $\sup S + \frac{\epsilon}{2} \in S$, contradicting $\sup S$ as an upper bound.) However, I don't know how to pinpoint a single subsequence $(a_{n_k})$ such that all such elements with $k \geq \text{ some } N$ are in this interval for all $\epsilon$.","The author of my intro analysis text has an exercise to give a proof of Bolzano-Weierstrass using axiom of completeness directly. Let $(a_n)$ be a bounded sequence, and define the set $$S=\{x \in \mathbb{R} : x<a_n \text{ for infinitely many terms } a_n\}.$$ Show that there exists a subsequence $a_{n_k}$ converging to $s = \sup S$. I feel I am close. I know that for any $\epsilon > 0$, there must be infinitely many $a_n$ such that $\sup S - \epsilon < a_n < \sup S + \epsilon$. (If there were only finitely many $a_n$ in that interval, then $\sup S + \frac{\epsilon}{2} \in S$, contradicting $\sup S$ as an upper bound.) However, I don't know how to pinpoint a single subsequence $(a_{n_k})$ such that all such elements with $k \geq \text{ some } N$ are in this interval for all $\epsilon$.",,['real-analysis']
28,"Find $(a,b)$ such that $\lim\limits_{x\to 0}\frac{f(x)}{x}=1$ implies $\lim\limits_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1$",Find  such that  implies,"(a,b) \lim\limits_{x\to 0}\frac{f(x)}{x}=1 \lim\limits_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1","Let $f$ denote any function such that $\lim\limits_{x\to 0}\frac{f(x)}{x}=1$. Find the value of $a$ and $b$, assuming that $\lim\limits_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1$. My  attempt: $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1$ $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{x^3\frac{(f(x))^3}{x^3}}=1$ $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{x^3}=1$ $\lim_{x\to 0}\frac{1+a\cos x-b\frac{\sin x}{x}}{x^2}=1$ $\lim_{x\to 0}\frac{1+a\cos x-b}{x^2}=1$ As denominator is tending to zero,so numerato will also tend to zero. $1+a-b=0............................(1)$ Applying L Hospital rule, $\lim_{x\to 0}\frac{-a\sin x}{2x}=1$ So $a=-2$ and $b=-1$ But the answer given in my book is $a=\frac{-5}{2}$ and $b=-\frac{3}{2}$. I do not understand where have i gone wrong?","Let $f$ denote any function such that $\lim\limits_{x\to 0}\frac{f(x)}{x}=1$. Find the value of $a$ and $b$, assuming that $\lim\limits_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1$. My  attempt: $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{(f(x))^3}=1$ $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{x^3\frac{(f(x))^3}{x^3}}=1$ $\lim_{x\to 0}\frac{x(1+a\cos x)-b\sin x}{x^3}=1$ $\lim_{x\to 0}\frac{1+a\cos x-b\frac{\sin x}{x}}{x^2}=1$ $\lim_{x\to 0}\frac{1+a\cos x-b}{x^2}=1$ As denominator is tending to zero,so numerato will also tend to zero. $1+a-b=0............................(1)$ Applying L Hospital rule, $\lim_{x\to 0}\frac{-a\sin x}{2x}=1$ So $a=-2$ and $b=-1$ But the answer given in my book is $a=\frac{-5}{2}$ and $b=-\frac{3}{2}$. I do not understand where have i gone wrong?",,"['real-analysis', 'limits']"
29,Maximum Baire class of a Riemann integrable function,Maximum Baire class of a Riemann integrable function,,"In this answer (see example 1), Andrés Caicedo gives an example of a function $f : [0,1] \to \mathbb{R}$ which is Riemann integrable and is (strictly) of second Baire class. Are there Riemann integrable functions of higher Baire class?  Arbitrarily high?  Are there Riemann integrable functions which are not in any Baire class? To be precise, recall that for an ordinal $\alpha$, the Baire class $B_{\alpha}$ is defined inductively as follows.  $B_0$ is the set of all continuous functions $f : [0,1] \to \mathbb{R}$.  Then, once $B_\beta$ are constructed for all $\beta < \alpha$, we say $f \in B_\alpha$ if there exists a sequence $f_n \in \bigcup_{\beta < \alpha} B_\beta$ with $f_n \to f$ pointwise.  Note that this stabilizes at $\alpha = \omega_1$. If $\mathcal{R}$ is the set of all Riemann integrable functions, my question is: do we have $\mathcal{R} \subset B_\alpha$ for some $\alpha$?  If so, what is the least such $\alpha$?  If not, what is the least $\alpha$ such that $\mathcal{R} \cap B_{\omega_1} \subset B_\alpha$? Of course, it may be helpful to recall that $f$ is Riemann integrable iff the set of discontinuities of $f$ has Lebesgue measure zero.","In this answer (see example 1), Andrés Caicedo gives an example of a function $f : [0,1] \to \mathbb{R}$ which is Riemann integrable and is (strictly) of second Baire class. Are there Riemann integrable functions of higher Baire class?  Arbitrarily high?  Are there Riemann integrable functions which are not in any Baire class? To be precise, recall that for an ordinal $\alpha$, the Baire class $B_{\alpha}$ is defined inductively as follows.  $B_0$ is the set of all continuous functions $f : [0,1] \to \mathbb{R}$.  Then, once $B_\beta$ are constructed for all $\beta < \alpha$, we say $f \in B_\alpha$ if there exists a sequence $f_n \in \bigcup_{\beta < \alpha} B_\beta$ with $f_n \to f$ pointwise.  Note that this stabilizes at $\alpha = \omega_1$. If $\mathcal{R}$ is the set of all Riemann integrable functions, my question is: do we have $\mathcal{R} \subset B_\alpha$ for some $\alpha$?  If so, what is the least such $\alpha$?  If not, what is the least $\alpha$ such that $\mathcal{R} \cap B_{\omega_1} \subset B_\alpha$? Of course, it may be helpful to recall that $f$ is Riemann integrable iff the set of discontinuities of $f$ has Lebesgue measure zero.",,"['real-analysis', 'measure-theory', 'descriptive-set-theory', 'riemann-integration']"
30,"How discontinuous does a $[a,b] \to (c,d)$ bijection have to be?",How discontinuous does a  bijection have to be?,"[a,b] \to (c,d)","I know that bijection from $[a,b]$ to $(c,d)$ can't be continuous, but I'm wondering if such a function could exist if it was discontinuous at just countable points, and if this isn't possible, why?","I know that bijection from $[a,b]$ to $(c,d)$ can't be continuous, but I'm wondering if such a function could exist if it was discontinuous at just countable points, and if this isn't possible, why?",,['real-analysis']
31,"Closed form of $\ln^n \tan x\, dx$",Closed form of,"\ln^n \tan x\, dx","Here is an integral I am really stuck at. I am pretty sure that a general closed form of the integral: $$\mathcal{J}=\int_0^{\pi/2} \ln^n \tan x\, {\rm d}x, \;\; n \in \mathbb{N}$$ exists. Well if $n$ is odd , then the integral is obviously zero due to symmetry. On the contrary if $n$ is even then the closed form I seek must contain the beta dirichlet function however I am unable to reach it. Setting $m=2n$ then: $$\int_{0}^{\pi/2}\ln^m \tan x\, {\rm d}x=\int_{0}^{\infty}\frac{\ln^m u}{u^2+1}\, {\rm d}u= 2\int_{0}^{1}\frac{\ln^m u}{u^2+1}\, {\rm d}u$$ If we expand the denominator in a Taylor series, namely $1+x^2=\sum \limits_{n=0}^{\infty} (-1)^n x^n$ then the last integral is written as: $$2\int_{0}^{1}\ln^m x \sum_{n=0}^{\infty}(-1)^n  x^n \, {\rm d}x = 2\sum_{n=0}^{\infty}(-1)^n \int_{0}^{1}x^n \ln^m x \, {\rm d}x = 2 \sum_{n=0}^{\infty}\frac{(-1)^n (-1)^m m!}{\left ( n+1 \right )^{m+1}}= 2 (-1)^m m! \sum_{n=0}^{\infty}\frac{(-1)^n}{\left ( n+1 \right )^{m+1}}$$ Apparently there is something wrong here. I used the result $$\int_{0}^{1}x^m \ln^n x \, {\rm d}x = \frac{(-1)^n n!}{\left ( m+1 \right )^{n+1}}$$ as presented here . Edit/ Update: A conjecture of mine is that the closed form actually is: $$\int_0^{\pi/2} \ln^{m} \tan x \, {\rm d}x=2m! \beta(m+1), \;\; m \;\;{\rm even}$$ For $m=2$ matches the result $\displaystyle \int_0^{\pi/2} \ln^2 \tan x\, {\rm d}x= \frac{\pi^3}{8}$.","Here is an integral I am really stuck at. I am pretty sure that a general closed form of the integral: $$\mathcal{J}=\int_0^{\pi/2} \ln^n \tan x\, {\rm d}x, \;\; n \in \mathbb{N}$$ exists. Well if $n$ is odd , then the integral is obviously zero due to symmetry. On the contrary if $n$ is even then the closed form I seek must contain the beta dirichlet function however I am unable to reach it. Setting $m=2n$ then: $$\int_{0}^{\pi/2}\ln^m \tan x\, {\rm d}x=\int_{0}^{\infty}\frac{\ln^m u}{u^2+1}\, {\rm d}u= 2\int_{0}^{1}\frac{\ln^m u}{u^2+1}\, {\rm d}u$$ If we expand the denominator in a Taylor series, namely $1+x^2=\sum \limits_{n=0}^{\infty} (-1)^n x^n$ then the last integral is written as: $$2\int_{0}^{1}\ln^m x \sum_{n=0}^{\infty}(-1)^n  x^n \, {\rm d}x = 2\sum_{n=0}^{\infty}(-1)^n \int_{0}^{1}x^n \ln^m x \, {\rm d}x = 2 \sum_{n=0}^{\infty}\frac{(-1)^n (-1)^m m!}{\left ( n+1 \right )^{m+1}}= 2 (-1)^m m! \sum_{n=0}^{\infty}\frac{(-1)^n}{\left ( n+1 \right )^{m+1}}$$ Apparently there is something wrong here. I used the result $$\int_{0}^{1}x^m \ln^n x \, {\rm d}x = \frac{(-1)^n n!}{\left ( m+1 \right )^{n+1}}$$ as presented here . Edit/ Update: A conjecture of mine is that the closed form actually is: $$\int_0^{\pi/2} \ln^{m} \tan x \, {\rm d}x=2m! \beta(m+1), \;\; m \;\;{\rm even}$$ For $m=2$ matches the result $\displaystyle \int_0^{\pi/2} \ln^2 \tan x\, {\rm d}x= \frac{\pi^3}{8}$.",,"['real-analysis', 'improper-integrals', 'special-functions']"
32,"A particular integral: $\int_{-\infty}^{+\infty}\frac{\sin(\pi x)}{\prod_{k=-n}^{n}(x-k)}\,dx$",A particular integral:,"\int_{-\infty}^{+\infty}\frac{\sin(\pi x)}{\prod_{k=-n}^{n}(x-k)}\,dx","I have to show summability, then compute the following integral: $$\int\limits_{-\infty}^{+\infty} \frac{\sin(\pi\,x)}{\prod_{k = - n}^n (x - k)}\,dx = \frac{(-4)^n}{(2\,n)!}\,\pi $$ for every $n\in \mathbb{N}$. Is it possible to avoid the residue theorem? I would be happy with a real-analytic solution, if possible. Thanks in advance.","I have to show summability, then compute the following integral: $$\int\limits_{-\infty}^{+\infty} \frac{\sin(\pi\,x)}{\prod_{k = - n}^n (x - k)}\,dx = \frac{(-4)^n}{(2\,n)!}\,\pi $$ for every $n\in \mathbb{N}$. Is it possible to avoid the residue theorem? I would be happy with a real-analytic solution, if possible. Thanks in advance.",,"['real-analysis', 'integration', 'complex-analysis', 'improper-integrals', 'residue-calculus']"
33,"If $f_n \to f$ uniformly and $f$ is continuous, does that imply $f_n$ is continuous?","If  uniformly and  is continuous, does that imply  is continuous?",f_n \to f f f_n,"I have a theorem in my book which says if $(f_n)$ is a sequence of functions uniformly converging on $A$ to $f$, and is continuous at some point $c \in A$, then $f$ is also continuous at this point $c$. Does this also hold for the case in the title of this question?","I have a theorem in my book which says if $(f_n)$ is a sequence of functions uniformly converging on $A$ to $f$, and is continuous at some point $c \in A$, then $f$ is also continuous at this point $c$. Does this also hold for the case in the title of this question?",,"['real-analysis', 'continuity', 'uniform-convergence']"
34,Roots of a power series in an interval,Roots of a power series in an interval,,"Let $ a_0 + \frac{a_1}{2} + \frac{a_2}{3} + \cdots + \frac{a_n}{n+1} = 0 $ Prove that $ a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n = 0 $ has real roots into the interval $ (0,1) $ I found this problem in a real analysis course notes, but I don't even know how to attack the problem. I tried to affirm that all coefficients are zero, but that is cleary not true, we have many cases when the result is 0 but $ a_i \ne 0$ for some $i$. I have tried derive/integrate, isolate and substitute some coefficients ($ a_0 $ and $a_n $ where my favorite candidates). Work with factorials (and derivatives and factorials) but could not find a way to prove. I have many pages of useless scratches. Any tips are welcome.","Let $ a_0 + \frac{a_1}{2} + \frac{a_2}{3} + \cdots + \frac{a_n}{n+1} = 0 $ Prove that $ a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n = 0 $ has real roots into the interval $ (0,1) $ I found this problem in a real analysis course notes, but I don't even know how to attack the problem. I tried to affirm that all coefficients are zero, but that is cleary not true, we have many cases when the result is 0 but $ a_i \ne 0$ for some $i$. I have tried derive/integrate, isolate and substitute some coefficients ($ a_0 $ and $a_n $ where my favorite candidates). Work with factorials (and derivatives and factorials) but could not find a way to prove. I have many pages of useless scratches. Any tips are welcome.",,['real-analysis']
35,Equivalence Relation between Derivative Being Odd and Function Being Even,Equivalence Relation between Derivative Being Odd and Function Being Even,,"In the exercise, I am required to prove that $f'$ is odd $\iff$ $f$ is even Moving from right to left was pretty trivial, however, I couldn't move from left to right. Note that we can only use very basic things like the definition of the derivative and rules of differentiation (No mean value theorem or any other theorems.) I cannot even see the left implying the right, but I hope I will see it after you guys explain how. Thanks in advance.","In the exercise, I am required to prove that $f'$ is odd $\iff$ $f$ is even Moving from right to left was pretty trivial, however, I couldn't move from left to right. Note that we can only use very basic things like the definition of the derivative and rules of differentiation (No mean value theorem or any other theorems.) I cannot even see the left implying the right, but I hope I will see it after you guys explain how. Thanks in advance.",,"['real-analysis', 'analysis', 'self-learning']"
36,Construction bump function with positive Fourier transform,Construction bump function with positive Fourier transform,,"I am looking for the construction of a smooth bump function, $f$, mapping the real line to itself which has two special properties: (1) $f$ is constant on some interval in its support (for instance $f=1$ on $(-1,1)$ and $f=0$ outside $(-2,2)$.) (2) The Fourier transform of $f$ is non-negative and rapidly decaying. Thank you","I am looking for the construction of a smooth bump function, $f$, mapping the real line to itself which has two special properties: (1) $f$ is constant on some interval in its support (for instance $f=1$ on $(-1,1)$ and $f=0$ outside $(-2,2)$.) (2) The Fourier transform of $f$ is non-negative and rapidly decaying. Thank you",,"['real-analysis', 'partial-differential-equations', 'fourier-analysis']"
37,How to calculate $\lim_{x \to0} \dfrac{f(x)-f(\ln(1+x))}{x^{3}}$,How to calculate,\lim_{x \to0} \dfrac{f(x)-f(\ln(1+x))}{x^{3}},"$f$ is a differntiable function on $[-1,1]$ and doubly differentiable  on $x=0$ and $f^{'}(0)=0,f^{""}(0)=4$. How to calculate $$\lim_{x \to0} \dfrac{f(x)-f\big(\ln(1+x)\big)}{x^{3}}. $$ I have tried the L'Hosptial's rule, but get trouble with the form of  $$\lim_{x \to 0}f^{""}(x)-\dfrac {f^{""}\big(\ln(1+x)\big)}{(1+x)^2},$$ it may not exist.","$f$ is a differntiable function on $[-1,1]$ and doubly differentiable  on $x=0$ and $f^{'}(0)=0,f^{""}(0)=4$. How to calculate $$\lim_{x \to0} \dfrac{f(x)-f\big(\ln(1+x)\big)}{x^{3}}. $$ I have tried the L'Hosptial's rule, but get trouble with the form of  $$\lim_{x \to 0}f^{""}(x)-\dfrac {f^{""}\big(\ln(1+x)\big)}{(1+x)^2},$$ it may not exist.",,"['calculus', 'real-analysis', 'complex-analysis', 'limits', 'logarithms']"
38,What can $\delta$ depend on and not on in $\delta-\epsilon$ formulation of limit?,What can  depend on and not on in  formulation of limit?,\delta \delta-\epsilon,"in the  delta-epsilon formulation for $\lim_{n\to \infty} x_n = L$ , $\forall \epsilon >0, \exists n_0 \in \mathbb N,$ $$ |x_n-L| \leq \epsilon, \ \forall n > n_0$$ $n_0$ depends on $\epsilon$ . What else can it depend on? Can $n_0$ depend on $x_n$ ? What  can it not depend on? Thanks.","in the  delta-epsilon formulation for , depends on . What else can it depend on? Can depend on ? What  can it not depend on? Thanks.","\lim_{n\to \infty} x_n = L \forall \epsilon >0, \exists n_0 \in \mathbb N,  |x_n-L| \leq \epsilon, \ \forall n > n_0 n_0 \epsilon n_0 x_n","['real-analysis', 'sequences-and-series', 'limits']"
39,Evaluating a limit using L'Hôpital's rule,Evaluating a limit using L'Hôpital's rule,,"I know that it can be also evaluate using Taylor expansion, but I am intentionally want to solve it using L'Hôpital's rule: $$ \lim\limits_{x\to 0} \frac{\sin x}{x}^{\frac{1}{1-\cos x}} =  \lim\limits_{x\to 0}\exp\left( \frac{\ln(\frac{\sin x}{x})}{1-\cos x} \right)$$ Now, from continuity and L'hopital Rule:  $$\lim\limits_{x\to 0} \frac{\ln(\frac{\sin x}{x})}{1-\cos x} =  \lim\limits_{x\to 0} \frac{\frac{x}{\sin x}\cdot\frac{x\cos x - \sin x}{x^2}}{\sin x} =  \lim\limits_{x\to 0}\frac{\frac{x\cos x - \sin x}{x\sin x}}{\sin x}$$ This is where I got stuck. If I'm not mistaken the limit is $-\frac{1}{3}$ so the orginial one is $e^{-\frac{1}{3}}$ What should I do different (Or what's is wrong with my calculation?) Thanks","I know that it can be also evaluate using Taylor expansion, but I am intentionally want to solve it using L'Hôpital's rule: $$ \lim\limits_{x\to 0} \frac{\sin x}{x}^{\frac{1}{1-\cos x}} =  \lim\limits_{x\to 0}\exp\left( \frac{\ln(\frac{\sin x}{x})}{1-\cos x} \right)$$ Now, from continuity and L'hopital Rule:  $$\lim\limits_{x\to 0} \frac{\ln(\frac{\sin x}{x})}{1-\cos x} =  \lim\limits_{x\to 0} \frac{\frac{x}{\sin x}\cdot\frac{x\cos x - \sin x}{x^2}}{\sin x} =  \lim\limits_{x\to 0}\frac{\frac{x\cos x - \sin x}{x\sin x}}{\sin x}$$ This is where I got stuck. If I'm not mistaken the limit is $-\frac{1}{3}$ so the orginial one is $e^{-\frac{1}{3}}$ What should I do different (Or what's is wrong with my calculation?) Thanks",,"['calculus', 'real-analysis', 'limits']"
40,Where can I find SOLUTIONS to real analysis problems? [closed],Where can I find SOLUTIONS to real analysis problems? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I'm specifically interested in problem sets in Real Analysis that have solutions .  I have a few books on it, but I'd like to compare my solutions with some given answers in a lot of cases to ensure that I've mastered the material as much as possible. I've searched Real Analysis books on this site, and came up with this question. I'm really wondering about books (maybe teacher's manuals), PDFs, or other resources that have solutions to questions.  I'm self-studying, and I do have some good books that explain things fairly well, but I haven't yet been satisfied that I've mastered the material well enough.  Thus my want for solutions, besides just having problems without solutions. For example, this PDF has some solutions, but I'm wondering if I can find more books, etc.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I'm specifically interested in problem sets in Real Analysis that have solutions .  I have a few books on it, but I'd like to compare my solutions with some given answers in a lot of cases to ensure that I've mastered the material as much as possible. I've searched Real Analysis books on this site, and came up with this question. I'm really wondering about books (maybe teacher's manuals), PDFs, or other resources that have solutions to questions.  I'm self-studying, and I do have some good books that explain things fairly well, but I haven't yet been satisfied that I've mastered the material well enough.  Thus my want for solutions, besides just having problems without solutions. For example, this PDF has some solutions, but I'm wondering if I can find more books, etc.",,"['real-analysis', 'reference-request']"
41,Lipschitz-continuous $f(x)=x^2\cdot \sin\left(\frac{1}{x}\right)$,Lipschitz-continuous,f(x)=x^2\cdot \sin\left(\frac{1}{x}\right),"How to prove that $f$ is globally Lipschitz-continuous $$ f:\mathbb{R}\longrightarrow \mathbb{R}$$ $$ f(x) = \left\{    \begin{array}{c l}     x^2\cdot \sin\left(\frac{1}{x}\right) & ,\quad x\neq0\\     0 & ,\quad x=0   \end{array} \right.$$ Any hints would be appreciated.","How to prove that $f$ is globally Lipschitz-continuous $$ f:\mathbb{R}\longrightarrow \mathbb{R}$$ $$ f(x) = \left\{    \begin{array}{c l}     x^2\cdot \sin\left(\frac{1}{x}\right) & ,\quad x\neq0\\     0 & ,\quad x=0   \end{array} \right.$$ Any hints would be appreciated.",,"['real-analysis', 'lipschitz-functions', 'holder-spaces']"
42,Prove that there doesn't exist any function $f:\mathbb R\to \mathbb R$ that is continuous only at the rational points.,Prove that there doesn't exist any function  that is continuous only at the rational points.,f:\mathbb R\to \mathbb R,"Prove that there doesn't exist any function $f:\mathbb R \to \mathbb R$ that is continuous only at the rational points. Suggestion: For every $n \in \mathbb N$ , consider the set $$U_n=\{x \in \mathbb R : \exists U \subset \mathbb R \,\text{open, with}\, x \in U, {\rm diam}(f(U))<1/n\}.$$ I am supposed to prove this statement using the Baire Category theorem. I am not sure but I think that the suggestion points towards trying to express $\mathbb R$ as the union of the sets $U_n$ . If I could prove that any $U_n$ is a nowhere dense set and I affirm $\mathbb R=\bigcup_{n \in \mathbb N} U_n$ , since the Baire Category theorem says that the interior of a countable union of nowhere dense sets is empty, I would get to an absurd. I have two problems: what does this have to do with the fact that there can't be any function $f$ continuous only at rational points? How can I assure that every $x \in \mathbb R$ is in some $U_n$ ? Moreover, is there any non-empty $U_n$ ?","Prove that there doesn't exist any function that is continuous only at the rational points. Suggestion: For every , consider the set I am supposed to prove this statement using the Baire Category theorem. I am not sure but I think that the suggestion points towards trying to express as the union of the sets . If I could prove that any is a nowhere dense set and I affirm , since the Baire Category theorem says that the interior of a countable union of nowhere dense sets is empty, I would get to an absurd. I have two problems: what does this have to do with the fact that there can't be any function continuous only at rational points? How can I assure that every is in some ? Moreover, is there any non-empty ?","f:\mathbb R \to \mathbb R n \in \mathbb N U_n=\{x \in \mathbb R : \exists U \subset \mathbb R \,\text{open, with}\, x \in U, {\rm diam}(f(U))<1/n\}. \mathbb R U_n U_n \mathbb R=\bigcup_{n \in \mathbb N} U_n f x \in \mathbb R U_n U_n","['real-analysis', 'analysis', 'metric-spaces', 'baire-category']"
43,Locally Lipschitz does not imply $C^1$,Locally Lipschitz does not imply,C^1,"Let $A$ be open in $\mathbb{R}^m$; let $g:A\rightarrow\mathbb{R}^n$. If $S\subseteq A$, we say that $S$ satisfies the Lipschitz condition on $S$ if the function $\lambda(x,y)=|g(x)-g(y)|/|x-y|$ is bounded for $x\neq y\in S$. We say that $g$ is locally Lipschitz if each point of $A$ has a neighborhood on which $g$ satisfies the Lipschitz condition. Show that if $g$ is locally Lipschitz, then $g$ is not necessarily of class $C^1$. I've thought about functions $f:\mathbb{R}\rightarrow\mathbb{R}$. Functions like $f(x)=x^a$ for $a\geq 1$ are locally Lipschitz, but they're also continuously differentiable, so don't quite work.","Let $A$ be open in $\mathbb{R}^m$; let $g:A\rightarrow\mathbb{R}^n$. If $S\subseteq A$, we say that $S$ satisfies the Lipschitz condition on $S$ if the function $\lambda(x,y)=|g(x)-g(y)|/|x-y|$ is bounded for $x\neq y\in S$. We say that $g$ is locally Lipschitz if each point of $A$ has a neighborhood on which $g$ satisfies the Lipschitz condition. Show that if $g$ is locally Lipschitz, then $g$ is not necessarily of class $C^1$. I've thought about functions $f:\mathbb{R}\rightarrow\mathbb{R}$. Functions like $f(x)=x^a$ for $a\geq 1$ are locally Lipschitz, but they're also continuously differentiable, so don't quite work.",,['real-analysis']
44,Does extreme value theorem hold when continuous is replaced with bounded?,Does extreme value theorem hold when continuous is replaced with bounded?,,"The extreme value theorem says that if the domain of a 'continuous' function is compact then both the max and min of the function lies in the domain set. My question is: can the 'continuity' be replaced by 'bounded' function keeping the domain compact and the theorem continue to hold? I haven't seen a proof, but if not, I'd be interested in seeing a counterexample.","The extreme value theorem says that if the domain of a 'continuous' function is compact then both the max and min of the function lies in the domain set. My question is: can the 'continuity' be replaced by 'bounded' function keeping the domain compact and the theorem continue to hold? I haven't seen a proof, but if not, I'd be interested in seeing a counterexample.",,"['real-analysis', 'continuity', 'compactness', 'bounded-variation']"
45,space of riemann integrable functions not complete,space of riemann integrable functions not complete,,"Define norm as $\int |f|$ (Riemann integral) on  $\mathcal R^1[0,1]$, the space of riemann integrable functions on $[0,1]$ with identification $f=g$ iff $\int |f-g|=0$. Let $\{ r_1,r_2,\cdots \}$ be the rationals in $[0,1]$, and let $f_n=1_{\{r_1,\cdots,r_n\}}$. Then $f_n$ is a cauchy sequence in $\mathcal R^1[0,1]$. I want to show that there is no $f\in \mathcal R^1[0,1]$ such that $f_n$ converges to $f$ in norm. How can I show it? Obviously the pointwise limit $f=1_\mathbb{Q}$ is not contained in $\mathcal R^1[0,1]$, but can I use this fact? I think that there can be other candidates, since convergence in norm and pointwise convergence are different.","Define norm as $\int |f|$ (Riemann integral) on  $\mathcal R^1[0,1]$, the space of riemann integrable functions on $[0,1]$ with identification $f=g$ iff $\int |f-g|=0$. Let $\{ r_1,r_2,\cdots \}$ be the rationals in $[0,1]$, and let $f_n=1_{\{r_1,\cdots,r_n\}}$. Then $f_n$ is a cauchy sequence in $\mathcal R^1[0,1]$. I want to show that there is no $f\in \mathcal R^1[0,1]$ such that $f_n$ converges to $f$ in norm. How can I show it? Obviously the pointwise limit $f=1_\mathbb{Q}$ is not contained in $\mathcal R^1[0,1]$, but can I use this fact? I think that there can be other candidates, since convergence in norm and pointwise convergence are different.",,['real-analysis']
46,Show that function is strictly monotone increasing,Show that function is strictly monotone increasing,,"I want to show that $$ f(x)=\dfrac{x-\sin(x)}{1-\cos(x)} $$ is strictly increasing in $(0,2 \pi) $. Unforunately, this is not that easy for me , as the derivative is not very manageable and trigonometric identities appear to be not very helpful, too.$$$$ Are there any further ways to do this?","I want to show that $$ f(x)=\dfrac{x-\sin(x)}{1-\cos(x)} $$ is strictly increasing in $(0,2 \pi) $. Unforunately, this is not that easy for me , as the derivative is not very manageable and trigonometric identities appear to be not very helpful, too.$$$$ Are there any further ways to do this?",,"['calculus', 'real-analysis']"
47,Is this Function Continuous at the origin?,Is this Function Continuous at the origin?,,Define $f:\mathbb{R}\rightarrow\mathbb{R}$ by $$f(x)=-2\sum_{n=1}^\infty xe^{-n^2x^2}$$ Is $f$ continuous at the origin? I think that $f$ is not continuous at the origin. Any help is appreciated.,Define $f:\mathbb{R}\rightarrow\mathbb{R}$ by $$f(x)=-2\sum_{n=1}^\infty xe^{-n^2x^2}$$ Is $f$ continuous at the origin? I think that $f$ is not continuous at the origin. Any help is appreciated.,,"['real-analysis', 'continuity']"
48,What's the justification for defining the (definite) integral using a countably infinite sum?,What's the justification for defining the (definite) integral using a countably infinite sum?,,"To compute the area under a continuous function $f$ from $\Bbb{R\to R}$ in the interval $[a,b]$, Riemannian integration is often employed that could defined as $$\lim_{N \to \infty} \sum_{i=0}^{N-1} \frac{(b-a)}{N} f\left(a+i\frac{b-a}{N}\right)$$ The intuition given behind this is to divide the interval into smaller and smaller pieces and 'sample' the function in each interval to compute the areas. However, no matter how large N gets, we will still be making only a finite sum. In the limit as $N\to\infty$ then, we will be sampling a countable infinity of points from the function. But the function's domain is $\Bbb R$, which is uncountably infinite . So, intuitively, how do we expect to sample it faithfully with only a countable infinity of samples? Since there is no bijection from $\Bbb N$ to $\Bbb R$, it seems that we will always 'miss' some points of f, which could contribute to the area.","To compute the area under a continuous function $f$ from $\Bbb{R\to R}$ in the interval $[a,b]$, Riemannian integration is often employed that could defined as $$\lim_{N \to \infty} \sum_{i=0}^{N-1} \frac{(b-a)}{N} f\left(a+i\frac{b-a}{N}\right)$$ The intuition given behind this is to divide the interval into smaller and smaller pieces and 'sample' the function in each interval to compute the areas. However, no matter how large N gets, we will still be making only a finite sum. In the limit as $N\to\infty$ then, we will be sampling a countable infinity of points from the function. But the function's domain is $\Bbb R$, which is uncountably infinite . So, intuitively, how do we expect to sample it faithfully with only a countable infinity of samples? Since there is no bijection from $\Bbb N$ to $\Bbb R$, it seems that we will always 'miss' some points of f, which could contribute to the area.",,"['calculus', 'real-analysis', 'integration']"
49,Rudin Theorem 1.21,Rudin Theorem 1.21,,"The Theorem 1.21 at page 10 of Rudin's Book states that For every real $x>0$  and every integer $n>0$ there is one and only one positive real $y$ such that $y^n=x.$ This number $y$ is written $\sqrt[n]{x}$ or $x^{1/n}$. I do not understand the first sentence of the proof, which states that That there is at most one such $y$ is clear, since $0<y_1<y_2$ implies $y_1^n<y_2^n.$ Why is it clear? I will appreciate any answers.","The Theorem 1.21 at page 10 of Rudin's Book states that For every real $x>0$  and every integer $n>0$ there is one and only one positive real $y$ such that $y^n=x.$ This number $y$ is written $\sqrt[n]{x}$ or $x^{1/n}$. I do not understand the first sentence of the proof, which states that That there is at most one such $y$ is clear, since $0<y_1<y_2$ implies $y_1^n<y_2^n.$ Why is it clear? I will appreciate any answers.",,['real-analysis']
50,Metrics on Euclidean spaces,Metrics on Euclidean spaces,,"I vaguely remember a theorem that says that any two metrics on the Euclidean space $\mathbb{R}^n$ are equivalent in some sense, but probably not in the sense of metric equivalence: two metrics $d_1$ and $d_2$ are said to be metrically equivalent if there are positive numbers $c_1$ and $c_2$ such that $c_1 d_1(x,y) \le d_2(x,y) \le c_2 d_1(x,y)$, $\forall$ $x,y \in \mathbb{R}^n$ Can somebody confirm which kind of equivalence there might be? Thanks.","I vaguely remember a theorem that says that any two metrics on the Euclidean space $\mathbb{R}^n$ are equivalent in some sense, but probably not in the sense of metric equivalence: two metrics $d_1$ and $d_2$ are said to be metrically equivalent if there are positive numbers $c_1$ and $c_2$ such that $c_1 d_1(x,y) \le d_2(x,y) \le c_2 d_1(x,y)$, $\forall$ $x,y \in \mathbb{R}^n$ Can somebody confirm which kind of equivalence there might be? Thanks.",,['real-analysis']
51,Unnecessary condition of Lebesgue's monotone convergence theorem?,Unnecessary condition of Lebesgue's monotone convergence theorem?,,"Rudin RCA p.21 Let $(X,\Sigma,\mu)$ be a measure space and $\{f_n\}$ be a sequence of measurable functions on $X$ and suppose that (a) $0\leq f_1\leq f_2\leq\cdots\leq\infty$ for every $x\in X,$ (b) $f_n\rightarrow f$ pointwise on $X$. Then $\int_X f_n d\mu \rightarrow \int_X f d\mu$. ============== I followed the proof, but cannot see why the condition (b) is essential. Every monotonic sequence in extended real system has a limit as a supremum or infimum, so I think the condition (a) implies the condition (b). Am I wrong? How?","Rudin RCA p.21 Let $(X,\Sigma,\mu)$ be a measure space and $\{f_n\}$ be a sequence of measurable functions on $X$ and suppose that (a) $0\leq f_1\leq f_2\leq\cdots\leq\infty$ for every $x\in X,$ (b) $f_n\rightarrow f$ pointwise on $X$. Then $\int_X f_n d\mu \rightarrow \int_X f d\mu$. ============== I followed the proof, but cannot see why the condition (b) is essential. Every monotonic sequence in extended real system has a limit as a supremum or infimum, so I think the condition (a) implies the condition (b). Am I wrong? How?",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'convergence-divergence']"
52,root test: why $\lim\sup$?,root test: why ?,\lim\sup,We just had the root test in class: $\sum_{n=1}^\infty a_n$ (in $\mathbb R)$ converges if $\lim\limits_{n\rightarrow\infty}\sup\sqrt[n]{|a_n|}<1$ Why is it important to take the $\lim\sup$ and not taking just $\lim$? Any examples? I've considered some series but with none of them I had a problem of taking $\lim$ instead of $\lim\sup$.,We just had the root test in class: $\sum_{n=1}^\infty a_n$ (in $\mathbb R)$ converges if $\lim\limits_{n\rightarrow\infty}\sup\sqrt[n]{|a_n|}<1$ Why is it important to take the $\lim\sup$ and not taking just $\lim$? Any examples? I've considered some series but with none of them I had a problem of taking $\lim$ instead of $\lim\sup$.,,['real-analysis']
53,Prove $\sum_{i=1}^{n}\frac{x_{i}^3}{x_{i+1}^2}\geq \sum_{i=1}^{n}\frac{x_{i}^2}{x_{i+1}}$,Prove,\sum_{i=1}^{n}\frac{x_{i}^3}{x_{i+1}^2}\geq \sum_{i=1}^{n}\frac{x_{i}^2}{x_{i+1}},"As in the topic, my question is to prove $$\sum_{i=1}^{n}\frac{x_{i}^3}{x_{i+1}^2}\geq \sum_{i=1}^{n}\frac{x_{i}^2}{x_{i+1}}$$We know that $x_{n+1}=x_{1}$ and $x_{1},x_{2}, x_{3},..., x_{n}\in \mathbb{R}_{+}$. As I suspect that it would be to cool if all $x_{i}$ were equal, and question is marked as tough one, after a few hours my brain stopped without producing anything reasonable so I ask you for hints how to move it. Everything will be appreciated. Thanks in advance.","As in the topic, my question is to prove $$\sum_{i=1}^{n}\frac{x_{i}^3}{x_{i+1}^2}\geq \sum_{i=1}^{n}\frac{x_{i}^2}{x_{i+1}}$$We know that $x_{n+1}=x_{1}$ and $x_{1},x_{2}, x_{3},..., x_{n}\in \mathbb{R}_{+}$. As I suspect that it would be to cool if all $x_{i}$ were equal, and question is marked as tough one, after a few hours my brain stopped without producing anything reasonable so I ask you for hints how to move it. Everything will be appreciated. Thanks in advance.",,"['real-analysis', 'limits']"
54,Name for the real numbers between $0$ and $1$,Name for the real numbers between  and,0 1,"I see this class of numbers all the time, so I was wondering if there was a special name for it. How to refer to a number $n$ in $\Bbb R$, such that $0<n<1$?","I see this class of numbers all the time, so I was wondering if there was a special name for it. How to refer to a number $n$ in $\Bbb R$, such that $0<n<1$?",,"['real-analysis', 'notation', 'terminology']"
55,Show that a function that is locally increasing is increasing?,Show that a function that is locally increasing is increasing?,,"A function $f : \mathbb{R} \to \mathbb{R}$ is locally increasing at a point $x$ if there is a $\delta > 0$ such that $f(s) < f(x) < f(t)$ whenever $x-\delta < s < x < t < x+\delta$. Show that a function that is locally increasing at every point in $\mathbb{R}$ must be increasing, i.e., $f(x) < f(y)$ for all $x < y$.","A function $f : \mathbb{R} \to \mathbb{R}$ is locally increasing at a point $x$ if there is a $\delta > 0$ such that $f(s) < f(x) < f(t)$ whenever $x-\delta < s < x < t < x+\delta$. Show that a function that is locally increasing at every point in $\mathbb{R}$ must be increasing, i.e., $f(x) < f(y)$ for all $x < y$.",,"['real-analysis', 'analysis']"
56,Is this an example of a metric space?,Is this an example of a metric space?,,"If $X=\mathbb{R}^2$. For $x=(x_1,x_2)$, $y=(y_1,y_2)$ define $$d_{1/2}(x,y)=\left(|x_1-y_1|^{1/2}+|x_2-y_2|^{1/2}\right)^2\;.$$ Prove or disprove $(X,d_{1/2})$ is a metric space. Attempt at a solution: After multiplying it out, it seems to boil down to $2|x_1-y_1|^{1/2}|x_2-y_2|^{1/2}$ satisfying the triangle inequality. However, I can't seems to prove or disprove this one way or another. Intution, however, is leading me towards the fact that it is in fact not a metric space.","If $X=\mathbb{R}^2$. For $x=(x_1,x_2)$, $y=(y_1,y_2)$ define $$d_{1/2}(x,y)=\left(|x_1-y_1|^{1/2}+|x_2-y_2|^{1/2}\right)^2\;.$$ Prove or disprove $(X,d_{1/2})$ is a metric space. Attempt at a solution: After multiplying it out, it seems to boil down to $2|x_1-y_1|^{1/2}|x_2-y_2|^{1/2}$ satisfying the triangle inequality. However, I can't seems to prove or disprove this one way or another. Intution, however, is leading me towards the fact that it is in fact not a metric space.",,"['real-analysis', 'general-topology']"
57,Inequality involving the regularized gamma function,Inequality involving the regularized gamma function,,"Prove that $$Q(x,\ln 2) := \frac{\int_{\ln 2}^{\infty} t^{x-1} e^{-t} dt}{\int_{0}^{\infty} t^{x-1} e^{-t} dt} \geqslant 1 - 2^{-x}$$ for all $x\geqslant 1$. ($Q$ is the regularized gamma function.)","Prove that $$Q(x,\ln 2) := \frac{\int_{\ln 2}^{\infty} t^{x-1} e^{-t} dt}{\int_{0}^{\infty} t^{x-1} e^{-t} dt} \geqslant 1 - 2^{-x}$$ for all $x\geqslant 1$. ($Q$ is the regularized gamma function.)",,"['real-analysis', 'inequality', 'special-functions', 'gamma-function']"
58,Proving that the unit ball in $\ell^2(\mathbb{N})$ is non-compact,Proving that the unit ball in  is non-compact,\ell^2(\mathbb{N}),"So on my homework it says that to prove the unit ball in $\ell^2(\mathbb{N})$ is non-compact, it suffices to find countably many elements $x_n$ of $\ell^2(\mathbb{N})$ with $\lVert x_n\rVert \leq \frac{1}{2}$ such that $\lVert x_n - x_m\rVert \geq \delta$ for some $\delta \in (0, \frac{1}{2})$ and $n \neq m$.  Why is that? Thanks","So on my homework it says that to prove the unit ball in $\ell^2(\mathbb{N})$ is non-compact, it suffices to find countably many elements $x_n$ of $\ell^2(\mathbb{N})$ with $\lVert x_n\rVert \leq \frac{1}{2}$ such that $\lVert x_n - x_m\rVert \geq \delta$ for some $\delta \in (0, \frac{1}{2})$ and $n \neq m$.  Why is that? Thanks",,"['real-analysis', 'functional-analysis', 'compactness', 'lp-spaces']"
59,"Using the Taylor expansion for ${(1+x)}^{-1/2}$, evaluate $\sum_{n=0}^\infty \binom{2n}{n} a^n$","Using the Taylor expansion for , evaluate",{(1+x)}^{-1/2} \sum_{n=0}^\infty \binom{2n}{n} a^n,"Using the Taylor expansion for $${(1+x)}^{-1/2}$$ we have $${(1+x)}^{-1/2}= \sum_{n=0}^\infty \binom{-1/2}{n} (x^n)$$ for $|x|<1$. But if $|a| <1$, how can we use the above fact to find $$\sum_{n=0}^\infty \binom{2n}{n} a^n?$$ Thanks! Help much appreciated.","Using the Taylor expansion for $${(1+x)}^{-1/2}$$ we have $${(1+x)}^{-1/2}= \sum_{n=0}^\infty \binom{-1/2}{n} (x^n)$$ for $|x|<1$. But if $|a| <1$, how can we use the above fact to find $$\sum_{n=0}^\infty \binom{2n}{n} a^n?$$ Thanks! Help much appreciated.",,"['real-analysis', 'binomial-coefficients', 'taylor-expansion']"
60,Subadditivity of the limit superior,Subadditivity of the limit superior,,$$ \limsup \left(f(h)+g(h)\right) \leq \limsup f(h)+ \limsup g(h).$$ How can we prove this? Any help would be appreciated.,$$ \limsup \left(f(h)+g(h)\right) \leq \limsup f(h)+ \limsup g(h).$$ How can we prove this? Any help would be appreciated.,,"['real-analysis', 'inequality', 'limsup-and-liminf']"
61,Examples of continuous functions that are monotone along all lines,Examples of continuous functions that are monotone along all lines,,"I am looking for different examples (or even a complete characterization if this is possible) of continuous functions that are monotone along all lines. By that I mean functions $f\colon X\to\mathbb{R}$ such that for all $x,y\in X$ the function $g_{x,y}:[0,1]\to\mathbb{R}$ , $\alpha\mapsto f(\alpha x + (1-\alpha)y)$ is monotone. $X\subset\mathbb{R}^n$ should be convex so that everything is well defined. If $n=1$ and $X$ is an interval then obviously these are just the the usual monotone functions. Also for general $n$ , every affine function is monoton allong all lines since $f(\alpha x + (1-\alpha)y)=\alpha f(x) + (1-\alpha) f(y)$ . What other examples of functions are there satisfying this property? Note that there are easy examples of functions $\big(\text{ like }f(x)=\Vert x\Vert^2\big)$ such that if you fix a certain $x$ (here $x=0$ ), then $g_{x,y}$ is monotone for all $y$ . However $f$ is still not monotone allong all lines. Also this choice of $f$ shows that there seems to be no easy criterion, like looking at partial derivatives for example.","I am looking for different examples (or even a complete characterization if this is possible) of continuous functions that are monotone along all lines. By that I mean functions such that for all the function , is monotone. should be convex so that everything is well defined. If and is an interval then obviously these are just the the usual monotone functions. Also for general , every affine function is monoton allong all lines since . What other examples of functions are there satisfying this property? Note that there are easy examples of functions such that if you fix a certain (here ), then is monotone for all . However is still not monotone allong all lines. Also this choice of shows that there seems to be no easy criterion, like looking at partial derivatives for example.","f\colon X\to\mathbb{R} x,y\in X g_{x,y}:[0,1]\to\mathbb{R} \alpha\mapsto f(\alpha x + (1-\alpha)y) X\subset\mathbb{R}^n n=1 X n f(\alpha x + (1-\alpha)y)=\alpha f(x) + (1-\alpha) f(y) \big(\text{ like }f(x)=\Vert x\Vert^2\big) x x=0 g_{x,y} y f f","['real-analysis', 'analysis', 'functions', 'convex-analysis', 'examples-counterexamples']"
62,Klenke's proof of Slutzky's Theorem,Klenke's proof of Slutzky's Theorem,,"In Klenke's book on probability he states Slutzky's theorem as: Let $X, X_1, X_2, \ldots$ and $Y_1, Y_2\ldots$ be random varaibles with values in $E$ . Assume $X_n \xrightarrow{\mathcal{D}} X$ and $d(X_n, Y_n) \xrightarrow{n\rightarrow \infty} 0$ in probability. Then $Y_n \xrightarrow{\mathcal{D}} X$ . Here $E$ is a metric space with metric $d$ , and $\xrightarrow{\mathcal{D}}$ means convergence in disribution, i.e. $X_n \xrightarrow{\mathcal{D}} X$ if the distributions $\mu_{X_n}$ of the $X_n$ converge weakly to the distribution $\mu_X$ of $X$ . By weak convergence he means $$\int f \mu_{X_n} \rightarrow \int f \mu_X$$ for all continuous and bounded functions $f$ . His proof is as follows: Let $f: E \rightarrow \mathbb{R}$ be bounded and Lipschitz continuous with constant $K$ . Then $$|f(x) - f(y)| \leq Kd(x,y) \wedge 2\|f\|_{\infty} \quad \text{for all } x, y \in E.$$ Dominated convergence yields $\limsup_{n \rightarrow \infty} \mathbf{E}[|f(X_n) - f(Y_n)|] = 0.$ Hence we have $$\limsup_{n\rightarrow \infty} |\mathbf{E}[f(Y_n)] - \mathbf{E}[f(X)]| \\ \leq \limsup_{n\rightarrow \infty} |\mathbf{E}[f(X)] - \mathbf{E}[f(X_n)]| + \limsup_{n\rightarrow \infty} |\mathbf{E}[f(X_n)] - \mathbf{E}[f(Y_n)]| = 0.$$ I have a few questions about this proof: Why are we only considering Lipschitz continuous functions? By the definition of weak convergence shouldn't we consider continuous and bounded functions instead? What is the point of showing $|f(x) - f(y)| \leq Kd(x,y) \wedge 2\|f\|_{\infty}$ ? It appears we do not use it in the inequalities beneath it. Why is he using $\limsup$ and not $\lim$ for the dominated convergence theorem? I thought this theorem only applies to $\lim$ . The above proof suggests there is a relationship between $X_n \xrightarrow{\mathcal{D}} X$ and $\limsup_{n\rightarrow \infty} |\mathbf{E}[f(X)] - \mathbf{E}[f(X_n)]| = 0$ . What is this relationship?","In Klenke's book on probability he states Slutzky's theorem as: Let and be random varaibles with values in . Assume and in probability. Then . Here is a metric space with metric , and means convergence in disribution, i.e. if the distributions of the converge weakly to the distribution of . By weak convergence he means for all continuous and bounded functions . His proof is as follows: Let be bounded and Lipschitz continuous with constant . Then Dominated convergence yields Hence we have I have a few questions about this proof: Why are we only considering Lipschitz continuous functions? By the definition of weak convergence shouldn't we consider continuous and bounded functions instead? What is the point of showing ? It appears we do not use it in the inequalities beneath it. Why is he using and not for the dominated convergence theorem? I thought this theorem only applies to . The above proof suggests there is a relationship between and . What is this relationship?","X, X_1, X_2, \ldots Y_1, Y_2\ldots E X_n \xrightarrow{\mathcal{D}} X d(X_n, Y_n) \xrightarrow{n\rightarrow \infty} 0 Y_n \xrightarrow{\mathcal{D}} X E d \xrightarrow{\mathcal{D}} X_n \xrightarrow{\mathcal{D}} X \mu_{X_n} X_n \mu_X X \int f \mu_{X_n} \rightarrow \int f \mu_X f f: E \rightarrow \mathbb{R} K |f(x) - f(y)| \leq Kd(x,y) \wedge 2\|f\|_{\infty} \quad \text{for all } x, y \in E. \limsup_{n \rightarrow \infty} \mathbf{E}[|f(X_n) - f(Y_n)|] = 0. \limsup_{n\rightarrow \infty} |\mathbf{E}[f(Y_n)] - \mathbf{E}[f(X)]| \\ \leq \limsup_{n\rightarrow \infty} |\mathbf{E}[f(X)] - \mathbf{E}[f(X_n)]| + \limsup_{n\rightarrow \infty} |\mathbf{E}[f(X_n)] - \mathbf{E}[f(Y_n)]| = 0. |f(x) - f(y)| \leq Kd(x,y) \wedge 2\|f\|_{\infty} \limsup \lim \lim X_n \xrightarrow{\mathcal{D}} X \limsup_{n\rightarrow \infty} |\mathbf{E}[f(X)] - \mathbf{E}[f(X_n)]| = 0","['real-analysis', 'probability-theory', 'measure-theory', 'convergence-divergence', 'probability-limit-theorems']"
63,"Let $0<\alpha\leq\frac{\pi^2}{6}.\ $ Does $\exists\ A\subset\mathbb{N}$ and $f:A\to\{-1,1\}$ such that $\sum_{n\in A} \frac{f(n)}{n^2}=\alpha?$",Let  Does  and  such that,"0<\alpha\leq\frac{\pi^2}{6}.\  \exists\ A\subset\mathbb{N} f:A\to\{-1,1\} \sum_{n\in A} \frac{f(n)}{n^2}=\alpha?","If we let $0<\alpha\leq \frac{\pi^2}{6},\ $ then it is not always true that $\exists\ A\subset \mathbb{N}$ such that $\displaystyle\sum_{n\in A} \frac{1}{n^2} = \alpha.\ $ To see this, consider the fact that $\ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \approx 1.645,\ $ and $\displaystyle\sum_{n=2}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}-1 \approx 0.645,\ $ and so there is no $\ A\subset \mathbb{N}$ such that $\displaystyle\sum_{n\in A} \frac{1}{n^2} = 0.9,\ $ which is $<1$ but $>0.645.$ This prompted me to ask the following: Let $0<\alpha\leq \frac{\pi^2}{6}.\ $ Does $\exists\ A\subset \mathbb{N}$ and $f:A\to \{-1,1\}$ such that $\displaystyle\sum_{n\in A} \frac{f(n)}{n^2} = \alpha?$","If we let then it is not always true that such that To see this, consider the fact that and and so there is no such that which is but This prompted me to ask the following: Let Does and such that","0<\alpha\leq \frac{\pi^2}{6},\  \exists\ A\subset \mathbb{N} \displaystyle\sum_{n\in A} \frac{1}{n^2} = \alpha.\  \ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \approx 1.645,\  \displaystyle\sum_{n=2}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}-1 \approx 0.645,\  \ A\subset \mathbb{N} \displaystyle\sum_{n\in A} \frac{1}{n^2} = 0.9,\  <1 >0.645. 0<\alpha\leq \frac{\pi^2}{6}.\  \exists\ A\subset \mathbb{N} f:A\to \{-1,1\} \displaystyle\sum_{n\in A} \frac{f(n)}{n^2} = \alpha?","['real-analysis', 'sequences-and-series', 'examples-counterexamples', 'problem-solving']"
64,"Does the series $\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} } $ converges uniformly on $\left [ 0,\infty \right )$?",Does the series  converges uniformly on ?,"\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} }  \left [ 0,\infty \right )","Does the series $\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} } $ converges uniformly on $\left [ 0,\infty  \right )$ ? Prove or disprove it. My first attempt is try to use Weierstrass M-test: $$\left |\frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} }  \right | =\frac{1}{n}e^{-\frac{x}{n} } =\frac{1}{ne^{\frac{x}{n}} }< \frac{1}{ne^{\frac{0}{n}} }=\frac{1}{n}......(1)    $$ but unfortunately $\sum_{1}^{\infty } \frac{1}{n} $ is divergent, so (1) tell me nothing. My second attempt is try to use Dirichlet test: $$\left | \sum_{n=1}^{k} \left ( -1 \right ) ^n \right | < 2 \qquad \forall  x\in \left [0,\infty   \right ) ...\space this \space condition \space is \space OK$$ Next assume $b(n)=\frac{e^{\frac{-x}{n}}}{n} $ , then... $$b'{(n)} =\frac{ \left (e^{-\frac{x}{n}}\times \frac{x}{n^2}\times n  \right ) - \left ( e^{-\frac{x}{n}}\times1 \right ) }{n^2}= \frac{e^{-\frac{x}{n}}\times \left ( \frac{x}{n}-1  \right )  }{n^2}$$ sadly, it seems not to decrease monotonically to zero, so this condition has failed. What should I do in order to test the uniform convergence of this series on $\left [ 0,\infty  \right )$ ?","Does the series converges uniformly on ? Prove or disprove it. My first attempt is try to use Weierstrass M-test: but unfortunately is divergent, so (1) tell me nothing. My second attempt is try to use Dirichlet test: Next assume , then... sadly, it seems not to decrease monotonically to zero, so this condition has failed. What should I do in order to test the uniform convergence of this series on ?","\sum_{1}^{\infty } \frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} }  \left [ 0,\infty  \right ) \left |\frac{\left ( -1 \right )^{n} }{n}e^{-\frac{x}{n} }  \right | =\frac{1}{n}e^{-\frac{x}{n} } =\frac{1}{ne^{\frac{x}{n}} }< \frac{1}{ne^{\frac{0}{n}} }=\frac{1}{n}......(1)     \sum_{1}^{\infty } \frac{1}{n}  \left | \sum_{n=1}^{k} \left ( -1 \right ) ^n \right | < 2 \qquad \forall  x\in \left [0,\infty   \right ) ...\space this \space condition \space is \space OK b(n)=\frac{e^{\frac{-x}{n}}}{n}  b'{(n)} =\frac{ \left (e^{-\frac{x}{n}}\times \frac{x}{n^2}\times n  \right ) - \left ( e^{-\frac{x}{n}}\times1 \right ) }{n^2}= \frac{e^{-\frac{x}{n}}\times \left ( \frac{x}{n}-1  \right )  }{n^2} \left [ 0,\infty  \right )","['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
65,Closed form for zeros of a function,Closed form for zeros of a function,,"I need a closed form for the zeros of $$f(x)=2\sin\left(\frac{\pi}{6}-\frac{\sqrt{3} x}{2} \right)-e^{-\frac{3x}{2}} $$ Putting $x=0$ , we see that $f(0)=0$ . For the closed form of the remaining zeros we use the series expansion of $f(x)$ about $x=0$ and finally ""reversion of series with nth term"" ( see here ) to get a solution $x=x_i$ , $i=1,2,3...$ $$f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!} x^n $$ where $f^{(0)}(0)=f(0)=0$ . So we have $$f^{(n)}(x)=\left(-\frac{\sqrt{3}}{2}\right)^n\sin\left(-\frac{\sqrt{3}x}{2}+\frac{\pi}{6}+\frac{n\pi}{2}\right)-\left(-\frac{3}{2}\right)^n e^{-\frac{3x}{2}} $$ $$f^{(n)}(0)=\left(-\frac{\sqrt{3}}{2}\right)^n\left[\sin\left(\frac{\pi}{6}+\frac{n\pi}{2}\right)-(\sqrt{3})^n\right] \tag{1}$$ Now we discuss two cases: Case $1$ : $n$ is even or $n=2m$ where $m\in \mathbb{N}\cup \{0\}$ $$f^{(n)}(0)=\left(-\frac{\sqrt{3}}{2}\right)^{2m}\left[\sin\left(\frac{\pi}{6}+m\pi\right)-(\sqrt{3})^{2m}\right] $$ $$f^{(n)}(0)=\left(\frac{\sqrt{3}}{2}\right)^{2m}\left[\frac{(-1)^m}{2}-(\sqrt{3})^{2m}\right] $$ $$f^{(n)}(0)=\left(\frac{3}{4}\right)^{m}\left[\frac{(-1)^m}{2}-3^{m}\right] \tag{2}$$ Case $2$ : $n$ is odd or $n=2k+1$ where $k\in \mathbb{N}\cup\{0\}$ $$f^{(n)}(0)=-\left(\frac{\sqrt{3}}{2}\right)^{2k+1}\left[\sin\left(\frac{\pi}{6}+\frac{(2k+1)\pi}{2}\right)-(\sqrt{3})^{2k+1}\right] $$ Now we have $\sin\left(\frac{\pi}{6}+\frac{(2k+1)\pi}{2}\right)=\frac{\sqrt{3}}{2}(-1)^k$ $$f^{(n)}(0)=-\left(\frac{\sqrt{3}}{2}\right)^{2k+1}\left[\frac{\sqrt{3}}{2}(-1)^k-(\sqrt{3})^{2k+1}\right] $$ $$f^{(n)}(0)=-\left(\frac{3}{2}\right)\left(\frac{3}{4}\right)^{k}\left[\frac{(-1)^k}{2}-3^{k}\right] \tag{3}$$ $$f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}x^n=\sum_{m=0}^\infty\frac{f^{(2m)}(0)}{(2m)!}x^{2m}+\sum_{k=0}^\infty\frac{f^{(2k+1)}(0)}{(2k+1)!}x^{2k+1} $$ So by $(2)$ and $(3)$ $$f(x)=\sum_{m=0}^\infty\frac{\left(\frac{3}{4}\right)^{m}\left(\frac{(-1)^m}{2}-3^{m}\right)}{(2m)!}x^{2m}-\frac{3}{2}\sum_{k=0}^\infty\frac{\left(\frac{3}{4}\right)^{k}\left(\frac{(-1)^k}{2}-3^{k}\right)}{(2k+1)!}x^{2k+1} $$ Edit I tried using Langrange inversion theorem but was unable to simply it further. Any help would be appreciated. Thank you.","I need a closed form for the zeros of Putting , we see that . For the closed form of the remaining zeros we use the series expansion of about and finally ""reversion of series with nth term"" ( see here ) to get a solution , where . So we have Now we discuss two cases: Case : is even or where Case : is odd or where Now we have So by and Edit I tried using Langrange inversion theorem but was unable to simply it further. Any help would be appreciated. Thank you.","f(x)=2\sin\left(\frac{\pi}{6}-\frac{\sqrt{3} x}{2} \right)-e^{-\frac{3x}{2}}  x=0 f(0)=0 f(x) x=0 x=x_i i=1,2,3... f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!} x^n  f^{(0)}(0)=f(0)=0 f^{(n)}(x)=\left(-\frac{\sqrt{3}}{2}\right)^n\sin\left(-\frac{\sqrt{3}x}{2}+\frac{\pi}{6}+\frac{n\pi}{2}\right)-\left(-\frac{3}{2}\right)^n e^{-\frac{3x}{2}}  f^{(n)}(0)=\left(-\frac{\sqrt{3}}{2}\right)^n\left[\sin\left(\frac{\pi}{6}+\frac{n\pi}{2}\right)-(\sqrt{3})^n\right] \tag{1} 1 n n=2m m\in \mathbb{N}\cup \{0\} f^{(n)}(0)=\left(-\frac{\sqrt{3}}{2}\right)^{2m}\left[\sin\left(\frac{\pi}{6}+m\pi\right)-(\sqrt{3})^{2m}\right]  f^{(n)}(0)=\left(\frac{\sqrt{3}}{2}\right)^{2m}\left[\frac{(-1)^m}{2}-(\sqrt{3})^{2m}\right]  f^{(n)}(0)=\left(\frac{3}{4}\right)^{m}\left[\frac{(-1)^m}{2}-3^{m}\right] \tag{2} 2 n n=2k+1 k\in \mathbb{N}\cup\{0\} f^{(n)}(0)=-\left(\frac{\sqrt{3}}{2}\right)^{2k+1}\left[\sin\left(\frac{\pi}{6}+\frac{(2k+1)\pi}{2}\right)-(\sqrt{3})^{2k+1}\right]  \sin\left(\frac{\pi}{6}+\frac{(2k+1)\pi}{2}\right)=\frac{\sqrt{3}}{2}(-1)^k f^{(n)}(0)=-\left(\frac{\sqrt{3}}{2}\right)^{2k+1}\left[\frac{\sqrt{3}}{2}(-1)^k-(\sqrt{3})^{2k+1}\right]  f^{(n)}(0)=-\left(\frac{3}{2}\right)\left(\frac{3}{4}\right)^{k}\left[\frac{(-1)^k}{2}-3^{k}\right] \tag{3} f(x)=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}x^n=\sum_{m=0}^\infty\frac{f^{(2m)}(0)}{(2m)!}x^{2m}+\sum_{k=0}^\infty\frac{f^{(2k+1)}(0)}{(2k+1)!}x^{2k+1}  (2) (3) f(x)=\sum_{m=0}^\infty\frac{\left(\frac{3}{4}\right)^{m}\left(\frac{(-1)^m}{2}-3^{m}\right)}{(2m)!}x^{2m}-\frac{3}{2}\sum_{k=0}^\infty\frac{\left(\frac{3}{4}\right)^{k}\left(\frac{(-1)^k}{2}-3^{k}\right)}{(2k+1)!}x^{2k+1} ","['real-analysis', 'calculus', 'analysis', 'inverse', 'transcendental-equations']"
66,"If an invertible function approaches infinity at infinity, does its inverse approach infinity at infinity?","If an invertible function approaches infinity at infinity, does its inverse approach infinity at infinity?",,"Prove or disprove: if $f(x)$ is defined on $ℝ$ (not necessarily continuous) and has an inverse and $\lim_{x→∞} f(x)=∞ $ then $\lim_{x→∞}f^{-1}(x)=∞ $ . I think it's true. I tried using Heine's theorem: We know that $\lim_{x→∞}f(x)=∞ $ , this means that for every sequence $x_n→∞$ we have $f(x_n)→∞$ . So if by way of contradiction we assume that $\lim_{x→∞}f^{-1}(x)≠∞ $ , this means that there exists a  sequence $y_n→∞$ such that $\lim_{n→∞}f^{-1}(y_n)≠∞ $ but now I'm stuck because it doesn't give me any information about $f(x)$ .","Prove or disprove: if is defined on (not necessarily continuous) and has an inverse and then . I think it's true. I tried using Heine's theorem: We know that , this means that for every sequence we have . So if by way of contradiction we assume that , this means that there exists a  sequence such that but now I'm stuck because it doesn't give me any information about .",f(x) ℝ \lim_{x→∞} f(x)=∞  \lim_{x→∞}f^{-1}(x)=∞  \lim_{x→∞}f(x)=∞  x_n→∞ f(x_n)→∞ \lim_{x→∞}f^{-1}(x)≠∞  y_n→∞ \lim_{n→∞}f^{-1}(y_n)≠∞  f(x),"['real-analysis', 'calculus', 'limits', 'examples-counterexamples']"
67,"Is there a function $f:[0,\infty) \to \{-1, 1\}$ such that $\int_0^\infty{f(x)\,dx}$ is well defined?",Is there a function  such that  is well defined?,"f:[0,\infty) \to \{-1, 1\} \int_0^\infty{f(x)\,dx}","I suspect it is impossible to find such an $f$ , but I can see one way it might be possible to as well. If it is possible to construct two sets $A$ and $B$ which partition the non-negative reals such that $A \cap [0,a)$ and $B \cap [0,a)$ both have measure $a/2$ for any positive real $a$ , then take $f^{-1}(1) = A$ , $f^{-1}(-1) = B$ . We would have $\int_0^a{f(x)\,dx}=0$ for all $a$ and $\lim_{a \to \infty}\int_0^a{f(x)\,dx}=0$ .","I suspect it is impossible to find such an , but I can see one way it might be possible to as well. If it is possible to construct two sets and which partition the non-negative reals such that and both have measure for any positive real , then take , . We would have for all and .","f A B A \cap [0,a) B \cap [0,a) a/2 a f^{-1}(1) = A f^{-1}(-1) = B \int_0^a{f(x)\,dx}=0 a \lim_{a \to \infty}\int_0^a{f(x)\,dx}=0","['real-analysis', 'measure-theory', 'improper-integrals', 'riemann-integration']"
68,Counter-example for Lipschitz function,Counter-example for Lipschitz function,,"Is the following statement true or false? Let $f : [0,1] \to \mathbb{R}$ be a continuous function such that $|f(x)-f(0)|\leq |x|$ for all $x\in[0,1/2]$ For all $\varepsilon >0$ there exists $C_{\varepsilon}>0$ such that $|f(x)-f(y)|\leq C_{\varepsilon}|x-y|$ for all $x, y \in [\varepsilon , 1]$ . Then $f$ is Lipschitz on $[0,1]$ . I think that the statement is false but I'm not able to produce a counter-example. Here is some intuition: Since $C_{\varepsilon}$ may go to infinity as $\varepsilon \to 0$ , then it is not useful when proving that $f$ is Lipschitz in a neighborhood of $0$ . And the condition $|f(x)-f(0)|\leq |x|$ alone is not powerful enough: take $x=1/n$ and $y=1/n+1/n^2$ , we have $|x-y|=\frac{1}{n^2}$ but $|f(x)-f(y)|\leq |f(x)-f(0)|+|f(y)-f(0)|\lesssim \frac{1}{n}$ which is too big.","Is the following statement true or false? Let be a continuous function such that for all For all there exists such that for all . Then is Lipschitz on . I think that the statement is false but I'm not able to produce a counter-example. Here is some intuition: Since may go to infinity as , then it is not useful when proving that is Lipschitz in a neighborhood of . And the condition alone is not powerful enough: take and , we have but which is too big.","f : [0,1] \to \mathbb{R} |f(x)-f(0)|\leq |x| x\in[0,1/2] \varepsilon >0 C_{\varepsilon}>0 |f(x)-f(y)|\leq C_{\varepsilon}|x-y| x, y \in [\varepsilon , 1] f [0,1] C_{\varepsilon} \varepsilon \to 0 f 0 |f(x)-f(0)|\leq |x| x=1/n y=1/n+1/n^2 |x-y|=\frac{1}{n^2} |f(x)-f(y)|\leq |f(x)-f(0)|+|f(y)-f(0)|\lesssim \frac{1}{n}","['real-analysis', 'analysis', 'lipschitz-functions']"
69,Bourbaki... caveman(?) symbol,Bourbaki... caveman(?) symbol,,"I don't know if this is the best place to ask this question, but I'm sure many of you are familiar with the Bourbaki ""dangerous bend"" symbol: The idea behind the symbol is to indicate a particularly conceptually challenging part of the text, which could potentially be skipped when reading through for the first time. Now, my question is: is there an equivalent ""caveman"" symbol? I came across this in a set of notes on analysis: The symbol appears next to all definitions, I suppose the idea is that when a concept is defined (especially in analysis) we are bound to discover something new (akin to a cave man). What is this symbol called? Is it commonly used, similarly to Bourbaki? Is there a LaTeX package for it?","I don't know if this is the best place to ask this question, but I'm sure many of you are familiar with the Bourbaki ""dangerous bend"" symbol: The idea behind the symbol is to indicate a particularly conceptually challenging part of the text, which could potentially be skipped when reading through for the first time. Now, my question is: is there an equivalent ""caveman"" symbol? I came across this in a set of notes on analysis: The symbol appears next to all definitions, I suppose the idea is that when a concept is defined (especially in analysis) we are bound to discover something new (akin to a cave man). What is this symbol called? Is it commonly used, similarly to Bourbaki? Is there a LaTeX package for it?",,"['real-analysis', 'soft-question', 'book-recommendation']"
70,Evaluate $\int_{0}^{1}K(x)^2\text{d}x -\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x$,Evaluate,\int_{0}^{1}K(x)^2\text{d}x -\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x,"Recently, I found this identity on a mathematical site(seems true): $$\int_{0}^{1}K(x)^2\text{d}x -\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x =\frac{\Gamma\left ( \frac{1}{4}  \right )^4 }{64}$$ where $K(x)=\int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t$ . A problem that resemble the above identity is here . Which has the power $3$ . But so far, I still don't really know how to relate those two integrals.","Recently, I found this identity on a mathematical site(seems true): where . A problem that resemble the above identity is here . Which has the power . But so far, I still don't really know how to relate those two integrals.","\int_{0}^{1}K(x)^2\text{d}x
-\int_{0}^{1} \frac{x\sqrt{1-x^2} }{2-x^2}K(x)^2\text{d}x
=\frac{\Gamma\left ( \frac{1}{4}  \right )^4 }{64} K(x)=\int_{0}^{1} \frac{1}{\sqrt{1-t^2}\sqrt{1-x^2t^2}  }\text{d}t 3","['real-analysis', 'integration', 'definite-integrals', 'special-functions', 'elliptic-integrals']"
71,"Can a non-zero smooth $f: [a,b] \rightarrow \mathbb{R}_{\geq 0}$ have infinitely many zeroes? [duplicate]",Can a non-zero smooth  have infinitely many zeroes? [duplicate],"f: [a,b] \rightarrow \mathbb{R}_{\geq 0}","This question already has answers here : Infinitely differentiable function with given zero set? (3 answers) Closed 2 years ago . Given a non-negative, smooth function $f: [a,b] \rightarrow \mathbb{R}_{\geq 0}$ . If there exists a sequence (of pairwise disjoint points) $x_n \in [a,b]$ such that $f(x_n)=0$ for all $n  \in \mathbb{N}$ , does it mean that $f(x)=0$ for all $x \in [a,b]$ ? Motivation: Consider smooth curves in polar coordinates that is a map $c: [0,2\pi) \rightarrow \mathbb{R}^2 , t \mapsto r(t) \cdot (\cos(t),\sin(t))$ . If we restrict this curve to a compact intervall, is it regular (i.e. $r'(t)^2 +r^2(t) \neq 0$ for all $t \in [0,2\pi)$ ) up to finitely many points or finitely many closed intervalls? This would follow, if the above conclusion holds. Thoughts: By compactness we can find (by abuse of notation) a subsequence $x_n \rightarrow x \in [a,b]$ such that $f(x)=0$ . From the taylor expansion we see that all derivatives in $x$ must vanish, that is $f^{(n)}(x)=0$ for all $n \in \mathbb{N}$ . Remarks: Smooth function on a closed intervall means there is an extension to an open intervall containing it.","This question already has answers here : Infinitely differentiable function with given zero set? (3 answers) Closed 2 years ago . Given a non-negative, smooth function . If there exists a sequence (of pairwise disjoint points) such that for all , does it mean that for all ? Motivation: Consider smooth curves in polar coordinates that is a map . If we restrict this curve to a compact intervall, is it regular (i.e. for all ) up to finitely many points or finitely many closed intervalls? This would follow, if the above conclusion holds. Thoughts: By compactness we can find (by abuse of notation) a subsequence such that . From the taylor expansion we see that all derivatives in must vanish, that is for all . Remarks: Smooth function on a closed intervall means there is an extension to an open intervall containing it.","f: [a,b] \rightarrow \mathbb{R}_{\geq 0} x_n \in [a,b] f(x_n)=0 n  \in \mathbb{N} f(x)=0 x \in [a,b] c: [0,2\pi) \rightarrow \mathbb{R}^2 , t \mapsto r(t) \cdot (\cos(t),\sin(t)) r'(t)^2 +r^2(t) \neq 0 t \in [0,2\pi) x_n \rightarrow x \in [a,b] f(x)=0 x f^{(n)}(x)=0 n \in \mathbb{N}","['real-analysis', 'smooth-functions']"
72,Do the properties $f(x)f(y)=f(x+y)$ and $f(x) \geq 1+x$ uniquely characterise the function $f(x)=e^x$?,Do the properties  and  uniquely characterise the function ?,f(x)f(y)=f(x+y) f(x) \geq 1+x f(x)=e^x,"In this post about possible definitions of the exponential function, it is mentioned that $e^x$ is the unique function $f:\Bbb{R}\mapsto\Bbb{R}$ satisfying $f(x)f(y)=f(x+y)$ $f(x)\geq 1+x$ Do these properties alone really uniquely characterise the exponential function, or do we have to impose further requirements (e.g. that $f$ be continuous)?","In this post about possible definitions of the exponential function, it is mentioned that is the unique function satisfying Do these properties alone really uniquely characterise the exponential function, or do we have to impose further requirements (e.g. that be continuous)?",e^x f:\Bbb{R}\mapsto\Bbb{R} f(x)f(y)=f(x+y) f(x)\geq 1+x f,"['real-analysis', 'calculus', 'exponential-function', 'definition']"
73,Series convergence by Gauss Test,Series convergence by Gauss Test,,"I want to try to prove that the $$\sum_{n=1}^\infty \frac{n!e^n}{n^{n+p}}$$ series convergent for $p>1.5$ with Gauss Test but failed? Gauss Test said if $\frac{a_n}{a_{n+1}}$ can be represnted as $\frac{a_n}{a_{n+1}}=\lambda+\frac{\mu}{n}+b_n$ where $\sum_{n=1}^\infty b_n$ absolutely convergent then $\lambda>1$ convergent $\lambda<1$ divergent $\lambda=1, \mu>1 $ convergent $\lambda=1, \mu\leq1 $ divergent My work : I found that $$\frac{a_n}{a_{n+1}}=\frac{1}{e}{(1+\frac{1}{n})}^{n+p}=\frac{1}{e}(1+1/n)^{n+p}=\frac{1}{e}(1+\frac{n+p}{n}+\frac{(n+p)(n+p-1)}{2}\frac{1}{n^2}+\dots)),$$ and then stopped and didnt see a continuation.",I want to try to prove that the series convergent for with Gauss Test but failed? Gauss Test said if can be represnted as where absolutely convergent then convergent divergent convergent divergent My work : I found that and then stopped and didnt see a continuation.,"\sum_{n=1}^\infty \frac{n!e^n}{n^{n+p}} p>1.5 \frac{a_n}{a_{n+1}} \frac{a_n}{a_{n+1}}=\lambda+\frac{\mu}{n}+b_n \sum_{n=1}^\infty b_n \lambda>1 \lambda<1 \lambda=1, \mu>1  \lambda=1, \mu\leq1  \frac{a_n}{a_{n+1}}=\frac{1}{e}{(1+\frac{1}{n})}^{n+p}=\frac{1}{e}(1+1/n)^{n+p}=\frac{1}{e}(1+\frac{n+p}{n}+\frac{(n+p)(n+p-1)}{2}\frac{1}{n^2}+\dots)),","['real-analysis', 'calculus']"
74,How do you prove $\pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)}$?,How do you prove ?,\pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)},"In the book Pi: A Source Book I found the following: Extract the square root of twelve times the diameter squared. This is the first term. Dividing the first term repeatedly by 3, obtain other terms: the second after one division by 3, the third after more division and so on. Divide the terms in order by the odd integers $1,\,3,\,5,\,\ldots$ ; add the odd-order terms to, and subtract the even order terms from, the preceding. The result is the circumference. That is equivalent to $$\pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)}.$$ The formula is due to an Indian mathematician Madhava of Sangamagrama . The proof of this formula should be in the treatise Yuktibhāṣā written in c. 1530 by an Indian astronomer Jyesthadeva , which I don't have access to. I've been trying to find a proof of the formula elsewhere but with no success. Maybe this could be proved from $$\arctan x=\sum_{n\ge 0}\frac{(-1)^n x^{2n+1}}{2n+1}$$ which is mentioned in Yuktibhāṣā as well, but I don't see how could that be done.","In the book Pi: A Source Book I found the following: Extract the square root of twelve times the diameter squared. This is the first term. Dividing the first term repeatedly by 3, obtain other terms: the second after one division by 3, the third after more division and so on. Divide the terms in order by the odd integers ; add the odd-order terms to, and subtract the even order terms from, the preceding. The result is the circumference. That is equivalent to The formula is due to an Indian mathematician Madhava of Sangamagrama . The proof of this formula should be in the treatise Yuktibhāṣā written in c. 1530 by an Indian astronomer Jyesthadeva , which I don't have access to. I've been trying to find a proof of the formula elsewhere but with no success. Maybe this could be proved from which is mentioned in Yuktibhāṣā as well, but I don't see how could that be done.","1,\,3,\,5,\,\ldots \pi =\sqrt{12}\sum_{n\ge 0}\frac{(-1)^n}{3^n(2n+1)}. \arctan x=\sum_{n\ge 0}\frac{(-1)^n x^{2n+1}}{2n+1}","['real-analysis', 'sequences-and-series', 'euclidean-geometry', 'pi']"
75,"Is there a real-analytic monotone function $f:(0,\infty) \to \mathbb{R}$ which vanishes at infinity, but whose derivative admits no limit?","Is there a real-analytic monotone function  which vanishes at infinity, but whose derivative admits no limit?","f:(0,\infty) \to \mathbb{R}","A function $f:\mathbb{R} \to \mathbb{R}$ is called real-analytic if for each $x_0 \in \mathbb{R}$ there exists a neighbourhood of $x_0$ where $f$ is given by a convergent power series centred at $x_0$ . Problem: Is there a real-analytic monotone function $f:(0,\infty) \to \mathbb{R}$ which vanishes at infinity, but whose derivative admits no limit as $x \to \infty$ ? We can note some weaker, but related, results. The (non-monotone) function $f(x)=x^{-1} \sin x^2$ is a real-analytic function on $(0, +\infty)$ and has the property that $\lim_{x \to +\infty} f(x) = 0$ but $\lim_{x \to + \infty} f'(x)$ fails to exist. It's not difficult to construct monotone examples if real-analyticity is weakened to merely being infinitely differentiable. The basic construction is straightforward. For each integer $n \geq 2$ , and on each interval $[n, n+1-1/n^3]$ , set $f(x)=1/n$ , and on intervals $[ n+1-1/n^3, n+1]$ the function is linear, and decreasing from $\frac{1}{n}$ to $\frac{1}{n+1}$ . This function is piecewise linear, and not smooth at the transition points, but it's trivial to smoothen this construction by utilizing appropriate variants of $\exp(1/x)$ , rather than a linear interpolation. By the mean value theorem, we have that $\sup_{x \in [n+1-1/n^3, n+1]} |f'(x)| \geq \left|\frac{\frac{1}{n+1} - \frac{1}{n}}{\frac{1}{n^3}}\right|=\frac{n^3}{n(n+1)} \xrightarrow{n \to + \infty} + \infty$ hence $\lim f'(x)$ fails to exist. However, I don't think one can use these ideas to obtain a real-analytic monotone function with the desired properties, since there's no real-analytic ""transition"" functions.","A function is called real-analytic if for each there exists a neighbourhood of where is given by a convergent power series centred at . Problem: Is there a real-analytic monotone function which vanishes at infinity, but whose derivative admits no limit as ? We can note some weaker, but related, results. The (non-monotone) function is a real-analytic function on and has the property that but fails to exist. It's not difficult to construct monotone examples if real-analyticity is weakened to merely being infinitely differentiable. The basic construction is straightforward. For each integer , and on each interval , set , and on intervals the function is linear, and decreasing from to . This function is piecewise linear, and not smooth at the transition points, but it's trivial to smoothen this construction by utilizing appropriate variants of , rather than a linear interpolation. By the mean value theorem, we have that hence fails to exist. However, I don't think one can use these ideas to obtain a real-analytic monotone function with the desired properties, since there's no real-analytic ""transition"" functions.","f:\mathbb{R} \to \mathbb{R} x_0 \in \mathbb{R} x_0 f x_0 f:(0,\infty) \to \mathbb{R} x \to \infty f(x)=x^{-1} \sin x^2 (0, +\infty) \lim_{x \to +\infty} f(x) = 0 \lim_{x \to + \infty} f'(x) n \geq 2 [n, n+1-1/n^3] f(x)=1/n [ n+1-1/n^3, n+1] \frac{1}{n} \frac{1}{n+1} \exp(1/x) \sup_{x \in [n+1-1/n^3, n+1]} |f'(x)| \geq \left|\frac{\frac{1}{n+1} - \frac{1}{n}}{\frac{1}{n^3}}\right|=\frac{n^3}{n(n+1)} \xrightarrow{n \to + \infty} + \infty \lim f'(x)","['real-analysis', 'limits', 'derivatives', 'monotone-functions']"
76,Do we need rectangles for the Riemann integral?,Do we need rectangles for the Riemann integral?,,"The basic way the one-dimensional Riemann integral is extended to multiple integrals over bounded domains $D \subset \mathbb{R}^n$ is as follows. We extend the function $f:D \to \mathbb{R}$ we want to integrate to all of $\mathbb{R}^n$ , by defining $f(x)=0$ for all $x \in \mathbb{R}^n \setminus D$ . We enclose $D$ in a hyperrectangle $H \supset D$ , say $H=[a_1, b_1] \times [a_2, b_2] \cdots \times [a_n \times b_n]$ . A partition $P$ of $H$ is an $n$ -tuple $(I_1, I_2, ..., I_n)$ where each $I_i=\{S_{i,0}, S_{i,1}, \cdots, S_{i,k_i}\}$ $(k_i \geq 1)$ is a usual one-dimensional partition of $[a_i, b_i]$ into disjoint subintervals $S_{i,j}$ . Each such partition partitions $H$ into various sub-hyperrectangles, and a Riemann sum can be defined in the usual way by choosing ""tags"" in each such sub-hyperrectangle. If there is a meaningful limit that all such Riemann sums approach, independent of the choice of ""tags"", as the maximal subrectangle hyperrarea  of the partitions approaches $0$ , we say $f$ is Riemann integrable on $D$ , and the value of the limit is the value of the integral. Problem: can this be defined without hyperrectangles? Somehow, I don't feel that they are truly needed here, and any partitions of $D$ will do. Specifically, I would propose the following. We assume that $D$ is a ""nice"", closed and bounded region (perhaps homeomorphic to the closed $n$ -ball, but the answerer can propose any reasonable definition of ""niceness""). A partition of $D$ is a finite collection of disjoint, nonempty, compact (hence Lebesgue measurable), connected sets $\{S_i\}$ whose union is $D$ , and the norm $||P||$ of $P$ is defined to be $\max_{i} \mu(S_i)$ where $\mu$ is the $n$ -dimensional Lebesgue measure. We define Riemann sums as usual, namely sums of the form $\sum_{i} f(t_i) \mu(S_i)$ , $t_i \in S_i$ , and if a limit $\ell$ is approached as $||P|| \to 0$ , independent of the choice of tags, we say $f$ is Riemann integrable on $D$ with integral $\ell$ . The basic concern I have with this, however, is that now we're working with much more general partitions, and hence there is a possibility of pathological situations. In particular, there may be functions integrable with respect to the original definition, but not integrable with respect to the second definition. Question: does this work, and is it equivalent?","The basic way the one-dimensional Riemann integral is extended to multiple integrals over bounded domains is as follows. We extend the function we want to integrate to all of , by defining for all . We enclose in a hyperrectangle , say . A partition of is an -tuple where each is a usual one-dimensional partition of into disjoint subintervals . Each such partition partitions into various sub-hyperrectangles, and a Riemann sum can be defined in the usual way by choosing ""tags"" in each such sub-hyperrectangle. If there is a meaningful limit that all such Riemann sums approach, independent of the choice of ""tags"", as the maximal subrectangle hyperrarea  of the partitions approaches , we say is Riemann integrable on , and the value of the limit is the value of the integral. Problem: can this be defined without hyperrectangles? Somehow, I don't feel that they are truly needed here, and any partitions of will do. Specifically, I would propose the following. We assume that is a ""nice"", closed and bounded region (perhaps homeomorphic to the closed -ball, but the answerer can propose any reasonable definition of ""niceness""). A partition of is a finite collection of disjoint, nonempty, compact (hence Lebesgue measurable), connected sets whose union is , and the norm of is defined to be where is the -dimensional Lebesgue measure. We define Riemann sums as usual, namely sums of the form , , and if a limit is approached as , independent of the choice of tags, we say is Riemann integrable on with integral . The basic concern I have with this, however, is that now we're working with much more general partitions, and hence there is a possibility of pathological situations. In particular, there may be functions integrable with respect to the original definition, but not integrable with respect to the second definition. Question: does this work, and is it equivalent?","D \subset \mathbb{R}^n f:D \to \mathbb{R} \mathbb{R}^n f(x)=0 x \in \mathbb{R}^n \setminus D D H \supset D H=[a_1, b_1] \times [a_2, b_2] \cdots \times [a_n \times b_n] P H n (I_1, I_2, ..., I_n) I_i=\{S_{i,0}, S_{i,1}, \cdots, S_{i,k_i}\} (k_i \geq 1) [a_i, b_i] S_{i,j} H 0 f D D D n D \{S_i\} D ||P|| P \max_{i} \mu(S_i) \mu n \sum_{i} f(t_i) \mu(S_i) t_i \in S_i \ell ||P|| \to 0 f D \ell","['real-analysis', 'riemann-integration']"
77,"Prove that $\int_0^1\sqrt{f^4(x)+(\int_0^1f(t)\, dt)^4}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx$",Prove that,"\int_0^1\sqrt{f^4(x)+(\int_0^1f(t)\, dt)^4}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx","Can someone help me prove this integrals inequality $$\int_0^1\sqrt{f^4(x)+\bigg(\int_0^1f(t)\, dt\bigg)^4}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx$$ where $f$ is a function integrable on $[0,1]$ with real values. My initial thought was that the inequality is trivial if: $$\int_0^1f(t)\, dt \leq f(x)$$ but this is not true always. Then I thought with Cauchy-Bunyakovsky-Schwarz inequality for integrals: $$\bigg(\int_0^1f(t)\, dt\bigg)^4 \leq \bigg(\int_0^1f^2(t)\, dt\bigg)^2\leq \int_0^1f^4(t)\, dt$$ but I don't know if this inequality is true: $$\int_0^1\sqrt{f^4(x)+\int_0^1f^4(t)\, dt}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx$$ It might be true, but I don't know how to prove it. I would appreciate any help.","Can someone help me prove this integrals inequality where is a function integrable on with real values. My initial thought was that the inequality is trivial if: but this is not true always. Then I thought with Cauchy-Bunyakovsky-Schwarz inequality for integrals: but I don't know if this inequality is true: It might be true, but I don't know how to prove it. I would appreciate any help.","\int_0^1\sqrt{f^4(x)+\bigg(\int_0^1f(t)\, dt\bigg)^4}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx f [0,1] \int_0^1f(t)\, dt \leq f(x) \bigg(\int_0^1f(t)\, dt\bigg)^4 \leq \bigg(\int_0^1f^2(t)\, dt\bigg)^2\leq \int_0^1f^4(t)\, dt \int_0^1\sqrt{f^4(x)+\int_0^1f^4(t)\, dt}\, dx\le \sqrt{2}\int_0^1f^2(x)\,dx","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'integral-inequality']"
78,"Closed-form expression for $F(x,y) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 (t+y)} \mathrm{d}t$?",Closed-form expression for ?,"F(x,y) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 (t+y)} \mathrm{d}t","I am considering the following function $$F(x,y) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 (t+y)} \mathrm{d}t,$$ which is well-defined for any $x > 0$ and $y \geq 0$ . Is there a hope to obtain a closed form formula with respect to $x$ and $y$ ? For instance, according to Mathematical, we have that $$F(x,0) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 t} \mathrm{d}t = \frac{\pi}{x \sqrt{x (x+2)}}.$$ Remark: To give a bit of context, the function $F$ appears when I consider the quadratic optimization problem of the form $\min_{\mathbf{x} \in \mathrm{R}^N} \lVert \mathbf{A} \mathbf{x} - \mathbf{y} \rVert_2^2 + \lambda \lVert \mathbf{x} \rVert_2^2$ and I try to understand the behavior of $\lVert \widehat{\mathbf{x}} - \mathbf{x}_0 \rVert_2^2$ with $\widehat{\mathbf{x}}$ the unique optimizer and $\mathbf{x}_0$ the vector we aim at recovering, with $\mathbf{y} = \mathbf{A} \mathbf{x}_0 + \mathbf{n} \in \mathbb{R}^M$ and $\mathbf{n}$ an i.i.d. Gaussian vector. The values $x$ and $y$ above appear as functions of $\lambda$ and $\gamma = \lim M/N$ when $N\rightarrow \infty$ when the matrix $\mathbf{A}$ is i.i.d. Gaussian and its spectrum behaves according to the Marchenko-Pastur law.","I am considering the following function which is well-defined for any and . Is there a hope to obtain a closed form formula with respect to and ? For instance, according to Mathematical, we have that Remark: To give a bit of context, the function appears when I consider the quadratic optimization problem of the form and I try to understand the behavior of with the unique optimizer and the vector we aim at recovering, with and an i.i.d. Gaussian vector. The values and above appear as functions of and when when the matrix is i.i.d. Gaussian and its spectrum behaves according to the Marchenko-Pastur law.","F(x,y) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 (t+y)} \mathrm{d}t, x > 0 y \geq 0 x y F(x,0) = \int_0^1 \frac{\sqrt{t(1-t)}}{(t+x)^2 t} \mathrm{d}t = \frac{\pi}{x \sqrt{x (x+2)}}. F \min_{\mathbf{x} \in \mathrm{R}^N} \lVert \mathbf{A} \mathbf{x} - \mathbf{y} \rVert_2^2 + \lambda \lVert \mathbf{x} \rVert_2^2 \lVert \widehat{\mathbf{x}} - \mathbf{x}_0 \rVert_2^2 \widehat{\mathbf{x}} \mathbf{x}_0 \mathbf{y} = \mathbf{A} \mathbf{x}_0 + \mathbf{n} \in \mathbb{R}^M \mathbf{n} x y \lambda \gamma = \lim M/N N\rightarrow \infty \mathbf{A}","['real-analysis', 'calculus', 'integration', 'closed-form']"
79,Hilbert spaces: Orthonormal set has a dense span iff the only vector orthogonal to it is zero.,Hilbert spaces: Orthonormal set has a dense span iff the only vector orthogonal to it is zero.,,"I am reading in Young's An introduction to Hilbert Space that: A countable orthonormal set $S$ in a Hilbert space $\mathcal{H}$ has the property $\mathcal{P}=$ {the only vector orthogonal to it is the zero vector} if and only if $S$ spans $\mathcal{H}.$ Question I am wondering if this result can be stated in this more general form: that an orthonormal system indexed by some set $I$ has property $\mathcal{P}$ if and only if its span is dense in $\mathcal{H}$ If true, can you give me a proof?","I am reading in Young's An introduction to Hilbert Space that: A countable orthonormal set in a Hilbert space has the property {the only vector orthogonal to it is the zero vector} if and only if spans Question I am wondering if this result can be stated in this more general form: that an orthonormal system indexed by some set has property if and only if its span is dense in If true, can you give me a proof?",S \mathcal{H} \mathcal{P}= S \mathcal{H}. I \mathcal{P} \mathcal{H},"['real-analysis', 'functional-analysis']"
80,Rearrangement in proof for Euler's formula,Rearrangement in proof for Euler's formula,,"In the proof for Euler's formula, we expand $e^{ix}$ as a Taylor series, rearrange the terms, factor out $i$ , and thus obtain the Taylor series for $\sin (x)$ and $\cos(x)$ . However, this rearrangement can only be done if the Taylor series for $e^{ix}$ is absolutely convergent, by the Riemann series theorem. I know how to prove that a series of real terms is absolutely convergent. However, how do you do the same for a series of complex terms, like the one obtained in the Taylor series expansion of $e^{ix}$ ?","In the proof for Euler's formula, we expand as a Taylor series, rearrange the terms, factor out , and thus obtain the Taylor series for and . However, this rearrangement can only be done if the Taylor series for is absolutely convergent, by the Riemann series theorem. I know how to prove that a series of real terms is absolutely convergent. However, how do you do the same for a series of complex terms, like the one obtained in the Taylor series expansion of ?",e^{ix} i \sin (x) \cos(x) e^{ix} e^{ix},"['real-analysis', 'complex-analysis', 'convergence-divergence', 'absolute-convergence', 'conditional-convergence']"
81,"Function $f:\mathbb{R}\to \mathbb{R}$ of class $C^\infty$ such that $f(x)\not =x, \forall x\in \mathbb{R}$ and $|f^\prime(x)|<1$",Function  of class  such that  and,"f:\mathbb{R}\to \mathbb{R} C^\infty f(x)\not =x, \forall x\in \mathbb{R} |f^\prime(x)|<1","Give an example of function $f:\mathbb{R}\to \mathbb{R}$ of class $C^\infty$ such that $f(x)\not =x, \forall x\in \mathbb{R}$ and $|f^\prime(x)|<1, \forall x\in \mathbb{R}$ . The condition $|f′(x)|<1$ is equivalent to $f$ is Liphchitzian with constant $c<1$ . If $g$ is a bounded function of class $C^\infty$ with $|g(x)|<c<1$ we know that the function $f(x)=\int_{0}^x g(t)dt$ is Lipchitzian with constant $c<1$ , of class $C^\infty$ with $|f′(x)|=|g(x)|<c<1$ . So my idea is to find a function $g$ continuous bounded by a number less than 1 so that the function $f$ defined by $f(x)=\int_0^x g(t)dt$ is the one that meets the properties of $|f′(x)|<1$ . My problem is in the condition $f(x)\not= x,\forall x\in \mathbb{R}$ .","Give an example of function of class such that and . The condition is equivalent to is Liphchitzian with constant . If is a bounded function of class with we know that the function is Lipchitzian with constant , of class with . So my idea is to find a function continuous bounded by a number less than 1 so that the function defined by is the one that meets the properties of . My problem is in the condition .","f:\mathbb{R}\to \mathbb{R} C^\infty f(x)\not =x, \forall x\in \mathbb{R} |f^\prime(x)|<1, \forall x\in \mathbb{R} |f′(x)|<1 f c<1 g C^\infty |g(x)|<c<1 f(x)=\int_{0}^x g(t)dt c<1 C^\infty |f′(x)|=|g(x)|<c<1 g f f(x)=\int_0^x g(t)dt |f′(x)|<1 f(x)\not= x,\forall x\in \mathbb{R}","['real-analysis', 'derivatives']"
82,What does it mean to integrate a complex function over a real domain?,What does it mean to integrate a complex function over a real domain?,,"In complex form, we know that the $n$ -th Fourier coefficient of a function $f$ is given by $$\int_{-\pi}^{\pi} f(\theta)e^{-in\theta} d\theta.$$ My question: What exactly does it mean to integrate the complex function $f(\theta)e^{-in\theta}$ over a real domain? How can I visualize this—in general and in this particular case?","In complex form, we know that the -th Fourier coefficient of a function is given by My question: What exactly does it mean to integrate the complex function over a real domain? How can I visualize this—in general and in this particular case?",n f \int_{-\pi}^{\pi} f(\theta)e^{-in\theta} d\theta. f(\theta)e^{-in\theta},"['real-analysis', 'integration', 'complex-numbers', 'fourier-series']"
83,Questions about the existence of a function,Questions about the existence of a function,,"Question 1: Study the existence of $C^1$ function $f : \mathbb{R} \rightarrow \mathbb{R}$ satisfying $\forall x\in\mathbb{R},\mbox{ } f\circ f'(x)=x.$ Question 2: Study the existence of  differentiable function $f : \mathbb{R} \rightarrow \mathbb{R}$ satisfing $\forall x\in\mathbb{R},\mbox{ } f\circ f'(x)=x.$ Question 3: Study the existence of $C^1$ function $f : \mathbb{R} \rightarrow \mathbb{R}$ satisfying $\forall x\in\mathbb{R},\mbox{ } f'\circ f(x)=x.$ For question 1, such a function can not exist because : f' must be injective and since f 'is continuous, f' must be strictly monotonous. For example, If we assume that f 'is strictly increasing  We can show that $\displaystyle \lim_{x\rightarrow -\infty}f'(x)=-\infty$ and $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=+\infty$ which implies that f ' is surjective . with a simple argument it shows that f is injective ( if $f(x)=f(y)$ by surjection of $ f'$ , we have $f'(a)=x $ and $f'(b)=y$ for some real $ a,b $ , thus implie $a=f(f'(a)=f(x)=f(y)=f(f'(b)=b$ so $ x=y$ ). the continuity of f proves that f est strictly monotonous. For example, if we suppose f strictly inreasing, we must have $f'>0$ . this contradicts the surjectivity of $f '$ For Question 2 , I need help","Question 1: Study the existence of function satisfying Question 2: Study the existence of  differentiable function satisfing Question 3: Study the existence of function satisfying For question 1, such a function can not exist because : f' must be injective and since f 'is continuous, f' must be strictly monotonous. For example, If we assume that f 'is strictly increasing  We can show that and which implies that f ' is surjective . with a simple argument it shows that f is injective ( if by surjection of , we have and for some real , thus implie so ). the continuity of f proves that f est strictly monotonous. For example, if we suppose f strictly inreasing, we must have . this contradicts the surjectivity of For Question 2 , I need help","C^1 f : \mathbb{R} \rightarrow \mathbb{R} \forall x\in\mathbb{R},\mbox{ } f\circ f'(x)=x. f : \mathbb{R} \rightarrow \mathbb{R} \forall x\in\mathbb{R},\mbox{ } f\circ f'(x)=x. C^1 f : \mathbb{R} \rightarrow \mathbb{R} \forall x\in\mathbb{R},\mbox{ } f'\circ f(x)=x. \displaystyle \lim_{x\rightarrow -\infty}f'(x)=-\infty \displaystyle \lim_{x\rightarrow +\infty}f'(x)=+\infty f(x)=f(y)  f' f'(a)=x  f'(b)=y  a,b  a=f(f'(a)=f(x)=f(y)=f(f'(b)=b  x=y f'>0 f '","['real-analysis', 'analysis', 'derivatives', 'functional-equations']"
84,Prove that $A_n \cap B_n \rightarrow A \cap B$,Prove that,A_n \cap B_n \rightarrow A \cap B,"If $A_n \rightarrow A$ and $B_n \rightarrow B$ are sequences of sets then is it true that $A_n \cap B_n \rightarrow A \cap B$ ? How to prove or provide a counterexample? I had thought the following possible solution, but I am not convinced about it. I tried to prove the following statements: (i) $\liminf \left(A_n \cap B_n\right) = A \cap B$ ; (ii) $\limsup \left(A_n \cap B_n\right) = A \cap B$ ; thus, we can conclude that $\lim (A_n \cap B_n)$ , i.e., $A_n \cap B_n \rightarrow A \cap B$ To prove these statements I use the following arguments: (i) $\liminf \left(A_n \cap B_n\right) = \liminf A_n \cap \liminf B_n = A \cap B$ ; (ii) $\limsup \left(A_n \cap B_n\right) \subset \limsup A_n \cap \limsup B_n = A \cap B$ ; Therefore, $A_n \cap B_n \rightarrow A \cap B$ . Obs: A similar question was asked here , but the response is not complete. Can anybody help me? Thanks!","If and are sequences of sets then is it true that ? How to prove or provide a counterexample? I had thought the following possible solution, but I am not convinced about it. I tried to prove the following statements: (i) ; (ii) ; thus, we can conclude that , i.e., To prove these statements I use the following arguments: (i) ; (ii) ; Therefore, . Obs: A similar question was asked here , but the response is not complete. Can anybody help me? Thanks!",A_n \rightarrow A B_n \rightarrow B A_n \cap B_n \rightarrow A \cap B \liminf \left(A_n \cap B_n\right) = A \cap B \limsup \left(A_n \cap B_n\right) = A \cap B \lim (A_n \cap B_n) A_n \cap B_n \rightarrow A \cap B \liminf \left(A_n \cap B_n\right) = \liminf A_n \cap \liminf B_n = A \cap B \limsup \left(A_n \cap B_n\right) \subset \limsup A_n \cap \limsup B_n = A \cap B A_n \cap B_n \rightarrow A \cap B,"['real-analysis', 'limsup-and-liminf']"
85,Bounding the absolute value of a function with an integral,Bounding the absolute value of a function with an integral,,"I am having trouble with the following problem in analysis: Suppose that $f, f^\prime \in C([0, 1])$ . Prove that for all $x \in [0, 1]$ $$ |f(x)| \leq \int_0^1 (|f(t)| + |f^\prime (t)|) dt. $$ Any pointers? I have tried writing this as a Riemann Sum (given arbitrary tagged partition) but am still not sure how to proceed.",I am having trouble with the following problem in analysis: Suppose that . Prove that for all Any pointers? I have tried writing this as a Riemann Sum (given arbitrary tagged partition) but am still not sure how to proceed.,"f, f^\prime \in C([0, 1]) x \in [0, 1] 
|f(x)| \leq \int_0^1 (|f(t)| + |f^\prime (t)|) dt.
",['real-analysis']
86,Closed form expression for the harmonic sum $\sum\limits_{n=1}^{\infty}\frac{H_{2n}}{n^2\cdot4^n}{2n \choose n}$,Closed form expression for the harmonic sum,\sum\limits_{n=1}^{\infty}\frac{H_{2n}}{n^2\cdot4^n}{2n \choose n},"I'm wondering if one could derive a closed form expression for the series $$\sum_{n=1}^{\infty}\frac{H_{2n}}{n^2\cdot4^n}{2n \choose n}$$ $$\text{With } \text{ } \text{ } \text{ }H_n=\sum_{k=1}^{n}\frac{1}{k}\text{ } \text{ } \text{} \text{ } \text{ }\text{the } n^{th} \text{ harmonic number.}$$ Now, I know series involving harmonic numbers are well suited for a summation by part (or Abel's transformation) approach, but it doesn't lead anywere here, at least not in this state. Any suggestions ?","I'm wondering if one could derive a closed form expression for the series Now, I know series involving harmonic numbers are well suited for a summation by part (or Abel's transformation) approach, but it doesn't lead anywere here, at least not in this state. Any suggestions ?",\sum_{n=1}^{\infty}\frac{H_{2n}}{n^2\cdot4^n}{2n \choose n} \text{With } \text{ } \text{ } \text{ }H_n=\sum_{k=1}^{n}\frac{1}{k}\text{ } \text{ } \text{} \text{ } \text{ }\text{the } n^{th} \text{ harmonic number.},"['real-analysis', 'sequences-and-series', 'definite-integrals', 'binomial-coefficients', 'harmonic-numbers']"
87,Way to solve $\int\limits _0^\limits\infty \frac{x^{-t}}{1+x}\ dx$,Way to solve,\int\limits _0^\limits\infty \frac{x^{-t}}{1+x}\ dx,"Is there a direct way to prove that, for $t\in (0,1)$ $$\int\limits _0^\limits\infty \dfrac{x^{-t}}{1+x}\ dx=\dfrac{\pi}{\sin \pi t}$$ Without using Mellin transform?","Is there a direct way to prove that, for Without using Mellin transform?","t\in (0,1) \int\limits _0^\limits\infty \dfrac{x^{-t}}{1+x}\ dx=\dfrac{\pi}{\sin \pi t}",['real-analysis']
88,Why is an uncountable union of null sets not necessarily a null set?,Why is an uncountable union of null sets not necessarily a null set?,,"I ran across this statement ""...for instance, an uncountable union of null sets need not be a null set (or even a measurable set)..."" while looking through Terence Tao's blog site (See the first statement of #5). Since I'm taking a course in measure theory right now, I thought it might be relevant to understand why this is true, but I really don't know where to start. (In fact, I'm not entirely sure if this is relevant to measure theory, but Dr. Tao did mention that such a union might not even be a measurable set...). Intuition told me it might be similar to why 0 $\cdot$ $\infty$ is indeterminate, but I understood that problem to be one of definitions. As such, the only thing I have been able to come up with is that I don't understand the definition(s) either ""null set"", ""uncountable"", or ""union"" precisely enough for me to grasp yet. I'm leaning towards not fully understanding the term ""uncountable,"" as my understanding of transfinites and ordinals is sketchy at best. I was wondering if anyone could define these terms; point me towards something to read, learn, or think about; or provide an example of an uncountable union of null sets not being a null set? Edit: I didn't realize null set and the empty set were different things . Thanks to everyone for the examples and definitions!","I ran across this statement ""...for instance, an uncountable union of null sets need not be a null set (or even a measurable set)..."" while looking through Terence Tao's blog site (See the first statement of #5). Since I'm taking a course in measure theory right now, I thought it might be relevant to understand why this is true, but I really don't know where to start. (In fact, I'm not entirely sure if this is relevant to measure theory, but Dr. Tao did mention that such a union might not even be a measurable set...). Intuition told me it might be similar to why 0 $\cdot$ $\infty$ is indeterminate, but I understood that problem to be one of definitions. As such, the only thing I have been able to come up with is that I don't understand the definition(s) either ""null set"", ""uncountable"", or ""union"" precisely enough for me to grasp yet. I'm leaning towards not fully understanding the term ""uncountable,"" as my understanding of transfinites and ordinals is sketchy at best. I was wondering if anyone could define these terms; point me towards something to read, learn, or think about; or provide an example of an uncountable union of null sets not being a null set? Edit: I didn't realize null set and the empty set were different things . Thanks to everyone for the examples and definitions!",,"['real-analysis', 'measure-theory', 'soft-question', 'terminology']"
89,"Does the limit of $\cos^{2n}(n)$, with $n$ a positive integer, converges as $n\to\infty$?","Does the limit of , with  a positive integer, converges as ?",\cos^{2n}(n) n n\to\infty,"I'm struggling with what it seems to be a pretty simple limit: $$\lim_{n \rightarrow \infty} \cos^{2n}(n)$$ I have arguments to believe that this limit converges to $0$ because $n \in (kπ, (k+1)π) $ and $\cos[(k\pi, (k+1)\pi)] \rightarrow 0 $ as $n$ increases. But also I believe that this limit may diverge, because you can always find an integer that is closer to a multiple of $\pi$ (by Dirichlet's approximation theorem), so you may find a subsequence that converges to 1, and so this limit diverges. Many thanks in advance!!","I'm struggling with what it seems to be a pretty simple limit: $$\lim_{n \rightarrow \infty} \cos^{2n}(n)$$ I have arguments to believe that this limit converges to $0$ because $n \in (kπ, (k+1)π) $ and $\cos[(k\pi, (k+1)\pi)] \rightarrow 0 $ as $n$ increases. But also I believe that this limit may diverge, because you can always find an integer that is closer to a multiple of $\pi$ (by Dirichlet's approximation theorem), so you may find a subsequence that converges to 1, and so this limit diverges. Many thanks in advance!!",,"['real-analysis', 'sequences-and-series', 'limits', 'irrational-numbers', 'trigonometric-series']"
90,Two sequences which have an average tend to zero,Two sequences which have an average tend to zero,,"Let $(a_n)_{n\geq 1}, (b_n)_{n\geq 1}$ be two sequence of positive real numbers such that $$ \lim_{n\to +\infty}\frac{a_1+a_2+\cdots+a_n}{n}=\lim_{n\to +\infty}\frac{b_1+b_2+\cdots+b_n}{n}=0.$$ Conjecture. For all $\epsilon>0$ , there are infinitely many values of indices $k$ such that $a_k<\epsilon$ and $b_k<\epsilon.$ I think that this is true but I can not prove it now. In the special case where $a_n = b_n$ , that is, there is only one sequence, then one can argue easily using a contradiction argument. In the general case, the hard part is to show that the same set of indices are shared by both sequences $(a_n)_{n\geq 1}, (b_n)_{n\geq 1}$ .","Let be two sequence of positive real numbers such that Conjecture. For all , there are infinitely many values of indices such that and I think that this is true but I can not prove it now. In the special case where , that is, there is only one sequence, then one can argue easily using a contradiction argument. In the general case, the hard part is to show that the same set of indices are shared by both sequences .","(a_n)_{n\geq 1}, (b_n)_{n\geq 1}  \lim_{n\to +\infty}\frac{a_1+a_2+\cdots+a_n}{n}=\lim_{n\to +\infty}\frac{b_1+b_2+\cdots+b_n}{n}=0. \epsilon>0 k a_k<\epsilon b_k<\epsilon. a_n = b_n (a_n)_{n\geq 1}, (b_n)_{n\geq 1}","['real-analysis', 'sequences-and-series', 'limits', 'conjectures']"
91,Number of elements in the set $A=\{x\in\mathbb{R}|f(x)=1\}$.,Number of elements in the set .,A=\{x\in\mathbb{R}|f(x)=1\},"Let $f:\mathbb{R}\to\mathbb{R},f(x)=3^{x^3-3x}-3^{x+1}+x^3-4x$ and $A=\{x\in\mathbb{R}|f(x)=1\}$ if $|A|=$number of elements in A, then $|A|=?$. I tried differentation and got $$f'(x)=(3x^2-3)3^{x^3-3x}\ln(3)-3^{x+1}\ln(3)+3x^2-4$$ However trying to find a maxima or minima from this expression is really hard... so then there must be some clever way too look at $f$ maybe something with the composition of $2$ functions? any hints?","Let $f:\mathbb{R}\to\mathbb{R},f(x)=3^{x^3-3x}-3^{x+1}+x^3-4x$ and $A=\{x\in\mathbb{R}|f(x)=1\}$ if $|A|=$number of elements in A, then $|A|=?$. I tried differentation and got $$f'(x)=(3x^2-3)3^{x^3-3x}\ln(3)-3^{x+1}\ln(3)+3x^2-4$$ However trying to find a maxima or minima from this expression is really hard... so then there must be some clever way too look at $f$ maybe something with the composition of $2$ functions? any hints?",,"['calculus', 'real-analysis']"
92,Relation between homeomorphism and topological equivalence,Relation between homeomorphism and topological equivalence,,"Please consider the following definitions: A: Suppose $d$ and $e$ are metrics on a set $X$. Then, $d$ and $e$ are topologically equivalent metrics if and only if the identity functions from $(X,d)$ to $(X,e)$ and from $(X,e)$ to $(X,d)$ are both continuous. B: Suppose $(X,d)$ and $(Y,e)$ are metric spaces. Then $X$ and $Y$ are said to be homeomorphic or topologically equivalent if and only if, there exists a bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; such a function is called a homeomorphism. C : Suppose $d$ and $e$ are metrics on a set $X$. If $d$ and $e$ are topologically equivalent, then certainly $(X,d)$ and $(X,e)$ are homeomorphic, because the identity function $I_{d,e}:(X,d) ]\rightarrow (X,e)$ is continuous and has a continuous inverse $I_{e,d}:(X,e) \rightarrow (X,d)$. However, the converse need not be true In definition C , it is specifically mentioned that the converse need not be true. I assume, by the word : converse, the author implies : if $(X,d)$ and $(X,e)$ are homeomorphic, then,  $d$ and $e$ are topologically equivalent . I reason this as follows: there can exist bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; but which is not the identity function. Hence, $d$ and $e$ need not be topologically equivalent even though $(X,d)$ and $(X,e)$ are homeomorphic. Is this correct? Thanks","Please consider the following definitions: A: Suppose $d$ and $e$ are metrics on a set $X$. Then, $d$ and $e$ are topologically equivalent metrics if and only if the identity functions from $(X,d)$ to $(X,e)$ and from $(X,e)$ to $(X,d)$ are both continuous. B: Suppose $(X,d)$ and $(Y,e)$ are metric spaces. Then $X$ and $Y$ are said to be homeomorphic or topologically equivalent if and only if, there exists a bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; such a function is called a homeomorphism. C : Suppose $d$ and $e$ are metrics on a set $X$. If $d$ and $e$ are topologically equivalent, then certainly $(X,d)$ and $(X,e)$ are homeomorphic, because the identity function $I_{d,e}:(X,d) ]\rightarrow (X,e)$ is continuous and has a continuous inverse $I_{e,d}:(X,e) \rightarrow (X,d)$. However, the converse need not be true In definition C , it is specifically mentioned that the converse need not be true. I assume, by the word : converse, the author implies : if $(X,d)$ and $(X,e)$ are homeomorphic, then,  $d$ and $e$ are topologically equivalent . I reason this as follows: there can exist bijective function $f:X \rightarrow Y$ that is continuous and has continuous inverse; but which is not the identity function. Hence, $d$ and $e$ need not be topologically equivalent even though $(X,d)$ and $(X,e)$ are homeomorphic. Is this correct? Thanks",,"['real-analysis', 'general-topology', 'metric-spaces']"
93,Uniform convergence of alternating series,Uniform convergence of alternating series,,"If the sequence of functions $f_{n}: X \longrightarrow \mathbb{R}$  is such that $f_{1} \geq ... \geq f_{k} \geq ...$ and $f_{n} \longrightarrow 0$ uniformly in $X$. Prove that $\displaystyle \sum(-1)^{n}f_{n}$ uniformly converges in $X$ Since $f_{n}$ is uniformly convergent, the convergence of $f$ not dependes of $x$. Thus, is the proof reduced to the Leibniz test?","If the sequence of functions $f_{n}: X \longrightarrow \mathbb{R}$  is such that $f_{1} \geq ... \geq f_{k} \geq ...$ and $f_{n} \longrightarrow 0$ uniformly in $X$. Prove that $\displaystyle \sum(-1)^{n}f_{n}$ uniformly converges in $X$ Since $f_{n}$ is uniformly convergent, the convergence of $f$ not dependes of $x$. Thus, is the proof reduced to the Leibniz test?",,"['real-analysis', 'uniform-convergence']"
94,Green’s Function for the Heat Equation,Green’s Function for the Heat Equation,,"I’m trying find the Green’s function for the Heat Equation which satisfies the condition $$\Delta G( \bar{x}, t; \bar{x},^*t^* ) - \partial_t G = \delta(\bar{x} - \bar{x}^*) \delta(t-t^*),$$ where $\bar{x}$ represents n-tuples of spacial coordinates (i.e. $x,y,z,$ e.t.c.) and $\bar{x}^*$ is a point source. Now, it’s just a matter of solving this equation. My questions are the following: $\bullet$ In this case, what would the green’s function represent physically. On Wikipedia, it says that the Green’s Function is the response to a in-homogenous source term, but if that were true then the Laplace Equation could not have a Green’s Function. $\bullet$ How would one solve the above equation by Fourier Transforms? Are Fourier Transforms generally the best way to find Green’s Functions? Also, why couldn’t you just assume $\bar{x} \neq \bar{x}^*$ and let the LHS be zero? In all the notes I’ve read on Google, no one has done this.","I’m trying find the Green’s function for the Heat Equation which satisfies the condition $$\Delta G( \bar{x}, t; \bar{x},^*t^* ) - \partial_t G = \delta(\bar{x} - \bar{x}^*) \delta(t-t^*),$$ where $\bar{x}$ represents n-tuples of spacial coordinates (i.e. $x,y,z,$ e.t.c.) and $\bar{x}^*$ is a point source. Now, it’s just a matter of solving this equation. My questions are the following: $\bullet$ In this case, what would the green’s function represent physically. On Wikipedia, it says that the Green’s Function is the response to a in-homogenous source term, but if that were true then the Laplace Equation could not have a Green’s Function. $\bullet$ How would one solve the above equation by Fourier Transforms? Are Fourier Transforms generally the best way to find Green’s Functions? Also, why couldn’t you just assume $\bar{x} \neq \bar{x}^*$ and let the LHS be zero? In all the notes I’ve read on Google, no one has done this.",,"['calculus', 'real-analysis']"
95,"If $\int_0^1 f(x)dx$ exist but $\lim_{x\to 0}f(x)$ doesn't exist, does $\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f(k/n)=\int_0^1 f(x)dx$?","If  exist but  doesn't exist, does ?",\int_0^1 f(x)dx \lim_{x\to 0}f(x) \lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f(k/n)=\int_0^1 f(x)dx,"I was wondering for something : Let $f:]0,1]\to \mathbb R$ continuous. If $\displaystyle\lim_{x\to 0}f(x)$ exist, then indeed $$\int_0^1 f(x)dx=\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f\left(\frac{k}{n}\right).$$ Now, what happen if $\int_{0^+}^1f$ exist but not $\lim_{x\to 0}f(x)$ ? Do we still have $$\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f\left(\frac{k}{n}\right)\ \ ?$$ I'm thinking for example at $f(x)=\frac{1}{\sqrt x}$.","I was wondering for something : Let $f:]0,1]\to \mathbb R$ continuous. If $\displaystyle\lim_{x\to 0}f(x)$ exist, then indeed $$\int_0^1 f(x)dx=\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f\left(\frac{k}{n}\right).$$ Now, what happen if $\int_{0^+}^1f$ exist but not $\lim_{x\to 0}f(x)$ ? Do we still have $$\lim_{n\to \infty }\frac{1}{n}\sum_{k=1}^n f\left(\frac{k}{n}\right)\ \ ?$$ I'm thinking for example at $f(x)=\frac{1}{\sqrt x}$.",,"['real-analysis', 'integration']"
96,Fourier transform of $\frac{1}{\sqrt{1 + x^2}}$,Fourier transform of,\frac{1}{\sqrt{1 + x^2}},"I have a question about Fourier transforms: What does the following question means? "" Does the Fourier transform of the function $f(x)= \frac{1}{\sqrt{1 + x^2}}$ belongs to $L_2(\mathbb{R})$?"" We know that $L_2(\mathbb{R})= \{ f: \mathbb{R} \rightarrow \mathbb{C} \mid \int_{- \infty}^{+ \infty} |f(t)|^2 dt < \infty \} $ and we know that $\hat{f}(t) = \int_{- \infty}^{+ \infty} e^{-itx} f(x) dx$. Now for to show that $\hat{f}(t) \in L_2(\mathbb{R})$, what do we need to prove? $\textbf{(1)}$ $~$ Is it enough to show that $f(x) \in L_1(\mathbb{R}) \cap L_2(\mathbb{R})$, then we have some results that shows $\hat{f}(t) \in L_2(\mathbb{R})!$ $\textbf{(2)}$ $~$ Or we have to prove $\int_{- \infty}^{+ \infty} |\hat{f}(t)|^2 dt < \infty$? If $\textbf{(2)}$ is true? Can you please help me to show that? Here linked-to result someone has found its Fourier transform, but I cannot understand it! I mostly prefer to find it by usual integration ways such as variable changes or substitution or even by inverse transform theorem! Please let me know if I am wrong about using $\textbf{(1)}$ for to prove that $\hat{f}(t) \in L_2(\mathbb{R})$? Thanks!","I have a question about Fourier transforms: What does the following question means? "" Does the Fourier transform of the function $f(x)= \frac{1}{\sqrt{1 + x^2}}$ belongs to $L_2(\mathbb{R})$?"" We know that $L_2(\mathbb{R})= \{ f: \mathbb{R} \rightarrow \mathbb{C} \mid \int_{- \infty}^{+ \infty} |f(t)|^2 dt < \infty \} $ and we know that $\hat{f}(t) = \int_{- \infty}^{+ \infty} e^{-itx} f(x) dx$. Now for to show that $\hat{f}(t) \in L_2(\mathbb{R})$, what do we need to prove? $\textbf{(1)}$ $~$ Is it enough to show that $f(x) \in L_1(\mathbb{R}) \cap L_2(\mathbb{R})$, then we have some results that shows $\hat{f}(t) \in L_2(\mathbb{R})!$ $\textbf{(2)}$ $~$ Or we have to prove $\int_{- \infty}^{+ \infty} |\hat{f}(t)|^2 dt < \infty$? If $\textbf{(2)}$ is true? Can you please help me to show that? Here linked-to result someone has found its Fourier transform, but I cannot understand it! I mostly prefer to find it by usual integration ways such as variable changes or substitution or even by inverse transform theorem! Please let me know if I am wrong about using $\textbf{(1)}$ for to prove that $\hat{f}(t) \in L_2(\mathbb{R})$? Thanks!",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'fourier-transform']"
97,"Show that if $f: [0,1]\to \mathbb{R}$ is lower semi-continuous then attains its minimum on $[0,1]$",Show that if  is lower semi-continuous then attains its minimum on,"f: [0,1]\to \mathbb{R} [0,1]","Let $f: [0,1]\to \mathbb{R}$ be a lower semi-continuous function, then $$ \liminf_{x\to a} f(x) \geq f(a), \forall a \in [0,1]$$ I have to prove that $f$ attains its minimum on $[0,1]$, that is: $\exists x_0 \in [0,1]$ such that $f(x_0) \le f(x)$, $\forall x \in [0,1]$. This is a problem from a past qualifying exam in Measure Theory. I'm trying to solve it but I do not know how I should begin with this problem. I did not understand where the Inf is taken. What means that limit?","Let $f: [0,1]\to \mathbb{R}$ be a lower semi-continuous function, then $$ \liminf_{x\to a} f(x) \geq f(a), \forall a \in [0,1]$$ I have to prove that $f$ attains its minimum on $[0,1]$, that is: $\exists x_0 \in [0,1]$ such that $f(x_0) \le f(x)$, $\forall x \in [0,1]$. This is a problem from a past qualifying exam in Measure Theory. I'm trying to solve it but I do not know how I should begin with this problem. I did not understand where the Inf is taken. What means that limit?",,"['real-analysis', 'continuity']"
98,Differentiability of the sum of $\sum_{n=1}^{\infty}\frac{\sin(nx)}{n^2}$,Differentiability of the sum of,\sum_{n=1}^{\infty}\frac{\sin(nx)}{n^2},"Consider the series $$\sum_{n=1}^{\infty}\frac{\sin(nx)}{n^2}$$ This series converges uniformly to a continuous function, by Weierstrass's test, and it is the Fourier series of its sum, I'll call $f(x)$. Is $f$ continuously differentiable in $[-\pi, \pi]$? The common criterion to determine differentiability of the sum of uniformly convergent series is by testing uniform convergence of the series of derivatives. But I don't see how it's applicable here. This question: The Fourier series $\sum_{n=1}^\infty (1/n)\cos nx$ calculates the sum of the derivatives. But is there an argument to say that $f$ is not differentiable (based on the answer there), without calculating the explicit sum?","Consider the series $$\sum_{n=1}^{\infty}\frac{\sin(nx)}{n^2}$$ This series converges uniformly to a continuous function, by Weierstrass's test, and it is the Fourier series of its sum, I'll call $f(x)$. Is $f$ continuously differentiable in $[-\pi, \pi]$? The common criterion to determine differentiability of the sum of uniformly convergent series is by testing uniform convergence of the series of derivatives. But I don't see how it's applicable here. This question: The Fourier series $\sum_{n=1}^\infty (1/n)\cos nx$ calculates the sum of the derivatives. But is there an argument to say that $f$ is not differentiable (based on the answer there), without calculating the explicit sum?",,"['real-analysis', 'fourier-series']"
99,Where am I making a mistake in following delta function integral?,Where am I making a mistake in following delta function integral?,,"If the given integral is $$\int_{-\infty}^{+\infty} dx \delta (x-x^{'})f(x)$$ The answer is $f(x')$. However if we make a transformation$$x\rightarrow\alpha x=y$$and $$x^{'}\rightarrow\alpha x^{'}=y^{'}$$ substituting this into the above equation I get$$\alpha dx= dy$$ so the integral becomes, $$\frac{1}{\alpha}\int dy \delta (\frac{y}{\alpha}-\frac{y^{'}}{\alpha})f(\frac{y}{\alpha} )$$ Hence the answer is $$\frac{1}{\alpha}f(\frac{y^{'}}{\alpha})$$ If we go back to the old variables $x$ by making an inverse transformation $$y^{'}=\alpha x^{'}$$ I am getting $$\frac{1}{\alpha}f(x^{'})$$ However the solution is not just $$f(x^{'})$$ Where am I going wrong in the following transformation ?","If the given integral is $$\int_{-\infty}^{+\infty} dx \delta (x-x^{'})f(x)$$ The answer is $f(x')$. However if we make a transformation$$x\rightarrow\alpha x=y$$and $$x^{'}\rightarrow\alpha x^{'}=y^{'}$$ substituting this into the above equation I get$$\alpha dx= dy$$ so the integral becomes, $$\frac{1}{\alpha}\int dy \delta (\frac{y}{\alpha}-\frac{y^{'}}{\alpha})f(\frac{y}{\alpha} )$$ Hence the answer is $$\frac{1}{\alpha}f(\frac{y^{'}}{\alpha})$$ If we go back to the old variables $x$ by making an inverse transformation $$y^{'}=\alpha x^{'}$$ I am getting $$\frac{1}{\alpha}f(x^{'})$$ However the solution is not just $$f(x^{'})$$ Where am I going wrong in the following transformation ?",,"['real-analysis', 'integration', 'distribution-theory', 'dirac-delta']"
