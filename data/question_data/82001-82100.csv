,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differentiation of a matrix expression with respect to a vector,Differentiation of a matrix expression with respect to a vector,,"I am interested in the expression: $$ \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right]  $$ where $\bf{p}, \bf{x} $ are $S \times 1$ vectors and $D_1,P_1$ are $S \times S$ matrices. In this case, only the entries of $P_1$ directly depend on $\bf{p}$ so I was thinking of writing: $$ \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right] = \left(D_1 +P_1 \right)^{-1} \frac{dP_1}{d\bf{p}}  \left(D_1 +P_1 \right)^{-1} \left(P_1\bf{x} \right) + \left(D_1 +P_1 \right)^{-1} \frac{dP_1\bf{x}}{d\bf{p}}. $$ I don't have much confidence that this is correct though. I don't even know if it makes total sense, since $P_1 \bf{x}$ is a vector and $\frac{dP_1}{d\bf{p}}$ is a tensor? Any help is appreciated.","I am interested in the expression: where are vectors and are matrices. In this case, only the entries of directly depend on so I was thinking of writing: I don't have much confidence that this is correct though. I don't even know if it makes total sense, since is a vector and is a tensor? Any help is appreciated."," \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right]   \bf{p}, \bf{x}  S \times 1 D_1,P_1 S \times S P_1 \bf{p}  \frac{d}{d\bf{p}} \left[\left(D_1 +P_1 \right)^{-1}\left(P_1\bf{x} \right) \right] = \left(D_1 +P_1 \right)^{-1} \frac{dP_1}{d\bf{p}}  \left(D_1 +P_1 \right)^{-1} \left(P_1\bf{x} \right) + \left(D_1 +P_1 \right)^{-1} \frac{dP_1\bf{x}}{d\bf{p}}.  P_1 \bf{x} \frac{dP_1}{d\bf{p}}","['matrices', 'derivatives', 'matrix-calculus']"
1,Let $R$ be a commutative ring with unity. $s\in U(S) \iff \det(s) \in U(R)$,Let  be a commutative ring with unity.,R s\in U(S) \iff \det(s) \in U(R),"Let $R$ be a commutative ring with $1_R$ and let $$ S = \biggl\{ \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \;\Biggm| \; a,b,c \in R \;\biggr\}. $$ If $s = \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \in S$ , is it true that $$ s \in U(S) \iff \det(s) \in U(R)? $$ My instinct says it isn't but I cant find an example so I can make my mind right.","Let be a commutative ring with and let If , is it true that My instinct says it isn't but I cant find an example so I can make my mind right.","R 1_R 
S = \biggl\{ \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \;\Biggm| \; a,b,c \in R \;\biggr\}.
 s = \begin{pmatrix} a & b \\ 0 & c \end{pmatrix} \in S 
s \in U(S) \iff \det(s) \in U(R)?
","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'multilinear-algebra']"
2,When is an invertible $n \times n$ matrix (over an arbitrary ring) similar to its inverse?,When is an invertible  matrix (over an arbitrary ring) similar to its inverse?,n \times n,I want to find necessary and sufficient conditions for an invertible $n\times n$ matrix (over an arbitrary ring) similar to its inverse. Two $n\times n$ matrices $A$ and $B$ are called similar if there exists an invertible $n\times n$ matrix $P$ such that $B=P^{-1}AP.$ I tried to find it with invertible real 2 × 2 matrices. But I did not get any results. I don't know if anyone has any results on this problem.,I want to find necessary and sufficient conditions for an invertible matrix (over an arbitrary ring) similar to its inverse. Two matrices and are called similar if there exists an invertible matrix such that I tried to find it with invertible real 2 × 2 matrices. But I did not get any results. I don't know if anyone has any results on this problem.,n\times n n\times n A B n\times n P B=P^{-1}AP.,"['linear-algebra', 'matrices', 'matrix-decomposition', 'similar-matrices']"
3,Proof of the Laplace Expansion? [closed],Proof of the Laplace Expansion? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I just learned about Laplace Expansion for determinant calculation in high-school, they taught me how to calculate minors cofactors and everything but they did not include the proof in the book. I have tried watching YouTube videos and there is a proof on Wikipedia which I cannot understand anything. My teacher says she doesn't know the proof. Is the proof too hard for a high school student?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I just learned about Laplace Expansion for determinant calculation in high-school, they taught me how to calculate minors cofactors and everything but they did not include the proof in the book. I have tried watching YouTube videos and there is a proof on Wikipedia which I cannot understand anything. My teacher says she doesn't know the proof. Is the proof too hard for a high school student?",,"['linear-algebra', 'matrices', 'determinant', 'laplace-expansion']"
4,How to obtain a certain set of matrices in GAP?,How to obtain a certain set of matrices in GAP?,,"I'd like to ask the following question: Let $A$ be a finite set of complex numbers. I would like to obtain in GAP the set $B$ of all $n\times n$ - matrices where in each row and column there is exactly one non-zero entry and this entry is from the set $A$ . Thus this set $B$ of $n\times n$ -matrices should have cardinality $m^n \cdot n!$ where $m$ is the cardinality of $A$ . Is there an easy way to obtain this set $B$ in GAP? Thank you very much. (Example: When $A$ is equal to the set $\{1\}$ , then we get for $B$ the set of $n\times n$ permutation matrices.)","I'd like to ask the following question: Let be a finite set of complex numbers. I would like to obtain in GAP the set of all - matrices where in each row and column there is exactly one non-zero entry and this entry is from the set . Thus this set of -matrices should have cardinality where is the cardinality of . Is there an easy way to obtain this set in GAP? Thank you very much. (Example: When is equal to the set , then we get for the set of permutation matrices.)",A B n\times n A B n\times n m^n \cdot n! m A B A \{1\} B n\times n,"['matrices', 'group-theory', 'finite-groups', 'gap']"
5,Infinite series of matrix product,Infinite series of matrix product,,"If $t$ is real and positive and matrix $A$ and $B$ are such that $A = \begin{pmatrix} \dfrac{t^2+1}{t} & 0 & 0 \\ 0 & t & 0 \\ 0 & 0 & 25 \end{pmatrix}$ and $B=\begin{pmatrix} \dfrac{2t}{t^2+1} & 0 & 0 \\ 0 & \dfrac{3}{t} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}$ and $X=(AB)^{-1} + (AB)^{-2} + (AB)^{-3} + \cdots\infty, Y=X^{-1}$ then which option is correct? $1.$ $\text{det}(Y)=10$ $2.$ $X\cdot \text{adj(adj}(Y)=8I$ $3.$ $\text{det}(Y)=20$ $4.$ $X\cdot \text{adj(adj}(Y)=5I$ My attempt: I managed to calculate some of the important matrices, but am stuck in a part where I have to find the determinant of a matrix that has some elements tending to infinity. As $t>0$ $$AB= \begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix}$$ The inverse of a diagonal matrix is another diagonal matrix that's diagonal elements are reciprocals of the diagonal elements of the original matrix, so $$(AB)^{-1}= \begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, (AB)^{-2}=\begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix}, (AB)^{-3}=\begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, \cdots$$ My problem is, adding these matrices results in a matrix that's diagonal elements tend to infinity, as $$X=\begin{pmatrix} 2+1/2+2+1/2+\cdots \infty & 0 & 0 \\ 0 & 3+1/3+3+1/3+\cdots \infty \\ 0 & 0 & 5+1/5+5+1/5+\cdots \infty\end{pmatrix} = \begin{pmatrix} S_2 & 0 & 0 \\ 0 & S_3 & 0\\ 0 & 0 & S_5 \end{pmatrix}$$ And $$\text{adj}(X) = \begin{pmatrix} S_3S_5 & 0 & 0 \\ 0 & S_2S_5 & 0\\ 0 & 0 & S_2S_3 \end{pmatrix}$$ I know that $Y=\dfrac{\text{adj}(X)}{|X|}$ but I can't figure out how to calculate $|X|$ . The determinant of a diagonal matrix is simply the product of the diagonal elements, but how to calculate this value when the diagonal elements tend to infinity?","If is real and positive and matrix and are such that and and then which option is correct? My attempt: I managed to calculate some of the important matrices, but am stuck in a part where I have to find the determinant of a matrix that has some elements tending to infinity. As The inverse of a diagonal matrix is another diagonal matrix that's diagonal elements are reciprocals of the diagonal elements of the original matrix, so My problem is, adding these matrices results in a matrix that's diagonal elements tend to infinity, as And I know that but I can't figure out how to calculate . The determinant of a diagonal matrix is simply the product of the diagonal elements, but how to calculate this value when the diagonal elements tend to infinity?","t A B A = \begin{pmatrix} \dfrac{t^2+1}{t} & 0 & 0 \\ 0 & t & 0 \\ 0 & 0 & 25 \end{pmatrix} B=\begin{pmatrix} \dfrac{2t}{t^2+1} & 0 & 0 \\ 0 & \dfrac{3}{t} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix} X=(AB)^{-1} + (AB)^{-2} + (AB)^{-3} + \cdots\infty, Y=X^{-1} 1. \text{det}(Y)=10 2. X\cdot \text{adj(adj}(Y)=8I 3. \text{det}(Y)=20 4. X\cdot \text{adj(adj}(Y)=5I t>0 AB= \begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix} (AB)^{-1}= \begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, (AB)^{-2}=\begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix}, (AB)^{-3}=\begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, \cdots X=\begin{pmatrix} 2+1/2+2+1/2+\cdots \infty & 0 & 0 \\ 0 & 3+1/3+3+1/3+\cdots \infty \\ 0 & 0 & 5+1/5+5+1/5+\cdots \infty\end{pmatrix} = \begin{pmatrix} S_2 & 0 & 0 \\ 0 & S_3 & 0\\ 0 & 0 & S_5 \end{pmatrix} \text{adj}(X) = \begin{pmatrix} S_3S_5 & 0 & 0 \\ 0 & S_2S_5 & 0\\ 0 & 0 & S_2S_3 \end{pmatrix} Y=\dfrac{\text{adj}(X)}{|X|} |X|","['linear-algebra', 'sequences-and-series', 'matrices']"
6,"What functions have the property $f(AB)=f(A)f(B)$ where $A,B$ are matrices?",What functions have the property  where  are matrices?,"f(AB)=f(A)f(B) A,B","There is the classic example $\det(AB)=\det(A)\det(B)$ . I am looking for other examples of this property, it is best if f maps matrices to scalars but any example would be great. Note: No need for trivial $f(A)=cA$ type functions; I am looking for something more exotic.","There is the classic example . I am looking for other examples of this property, it is best if f maps matrices to scalars but any example would be great. Note: No need for trivial type functions; I am looking for something more exotic.",\det(AB)=\det(A)\det(B) f(A)=cA,"['linear-algebra', 'matrices', 'functions']"
7,Orthogonal transformation of a set of points to positive orthant,Orthogonal transformation of a set of points to positive orthant,,"Let $x_1,x_2\ldots,x_n$ are vectors from $\mathbb{R}^d$ . Also assume that $x_i^{\top}x_j \geq 0$ for all $i,j=1,2,\ldots,n$ . I am wondering if there is an orthogonal matrix $W$ such that the entries of $Wx_i$ 's are non-negative for $i=1,2,\ldots,n$ . I can prove the statement for $d=2$ . Any ideas or counterexample for the cases when $d\geq 3$ will be very helpful.",Let are vectors from . Also assume that for all . I am wondering if there is an orthogonal matrix such that the entries of 's are non-negative for . I can prove the statement for . Any ideas or counterexample for the cases when will be very helpful.,"x_1,x_2\ldots,x_n \mathbb{R}^d x_i^{\top}x_j \geq 0 i,j=1,2,\ldots,n W Wx_i i=1,2,\ldots,n d=2 d\geq 3","['linear-algebra', 'matrices', 'vectors', 'linear-transformations', 'orthogonal-matrices']"
8,Derivative of symmetric matrix with respect to its elements?,Derivative of symmetric matrix with respect to its elements?,,"Let's say we have a symmetric matrix $g$ whose elements are $g_{ij}$ . I would like to write down the answer for the derivative of $g_{ij}$ with respect to $g_{kl}$ . Since $g$ is symmetric I would expect that $$\frac{\partial g_{ij}}{\partial g_{ij}}=\frac{\partial g_{ji}}{\partial g_{ij}}=1$$ And all other derivatives vanish. I would like to be able to write this as a result in terms of Kronecker deltas for general indices but I am getting stuck. My first guess would be to say $$\frac{\partial g_{ij}}{\partial g_{kl}}=\delta^{k}_{i}\delta^{l}_j$$ But this does not respect the symmetry of the matrix elements, so one might guess to symmetrize: $$\frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j\Big)$$ But now we have a problem because the nonzero off-diagonal derivatives equal $1/2$ and the diagonal derivatives equal $1$ . Is there anyway to resolve this? Edit : Going off of the comments so far, I tried a different form which looks wrong, but it's the only form I can think of which respects the way in which these indices transform under an e.g. orthogonal transformation: $$\frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j-\frac{3}{n}g_{ij}g^{kl}\Big)$$ Where $g^{kl}$ are the elements of $g^{-1}$ , assuming it exists, and $n$ is the dimension of the matrix. This is obtained by assuming that the right hand side is a projection matrix.","Let's say we have a symmetric matrix whose elements are . I would like to write down the answer for the derivative of with respect to . Since is symmetric I would expect that And all other derivatives vanish. I would like to be able to write this as a result in terms of Kronecker deltas for general indices but I am getting stuck. My first guess would be to say But this does not respect the symmetry of the matrix elements, so one might guess to symmetrize: But now we have a problem because the nonzero off-diagonal derivatives equal and the diagonal derivatives equal . Is there anyway to resolve this? Edit : Going off of the comments so far, I tried a different form which looks wrong, but it's the only form I can think of which respects the way in which these indices transform under an e.g. orthogonal transformation: Where are the elements of , assuming it exists, and is the dimension of the matrix. This is obtained by assuming that the right hand side is a projection matrix.",g g_{ij} g_{ij} g_{kl} g \frac{\partial g_{ij}}{\partial g_{ij}}=\frac{\partial g_{ji}}{\partial g_{ij}}=1 \frac{\partial g_{ij}}{\partial g_{kl}}=\delta^{k}_{i}\delta^{l}_j \frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j\Big) 1/2 1 \frac{\partial g_{ij}}{\partial g_{kl}}=\frac{1}{2}\Big(\delta^{k}_{i}\delta^{l}_j+\delta^{l}_{i}\delta^{k}_j-\frac{3}{n}g_{ij}g^{kl}\Big) g^{kl} g^{-1} n,"['matrices', 'derivatives', 'matrix-calculus']"
9,Confused about computing the gradient of least-squares cost,Confused about computing the gradient of least-squares cost,,"Given matrix $A \in \mathbb R^{m \times n}$ and vector $y \in \mathbb R^m$ , I want to take the gradient of the following scalar field with respect to $x\in \mathbb R^n$ . $$x \mapsto \big((Ax - y)^T(Ax - y) \big),$$ $\textbf{Attempt}.$ \begin{align} \frac{\partial}{\partial x}  \big((Ax - y)^T(Ax - y) \big) &= \frac{\partial}{\partial x} \big( (x^TA^TAx - x^TA^Ty - y^TAx+ y^Ty )\big)\\ &= \frac{\partial}{\partial x}x^TA^TAx - \frac{\partial}{\partial x}x^TA^Ty - \frac{\partial}{\partial x}y^TAx+ \frac{\partial}{\partial x}y^Ty  \\ &= 2 A^TAx - A^Ty - y^TA\qquad\,\,\mathbf{(1*)}\\ &= 2 A^TAx - 2A^Ty. \qquad\qquad\,\mathbf{(2*)}\\ \end{align} $\textbf{Question}.$ There are two expressions above marked by $(*)$ . I don't understand the justification in going from $(1*)$ to $(2*)$ (in fact, the dimensions don't make sense...), which makes me think that there is a mistake in $(1*)$ . Can someone explain the basics involved in these matrix manipulations?","Given matrix and vector , I want to take the gradient of the following scalar field with respect to . There are two expressions above marked by . I don't understand the justification in going from to (in fact, the dimensions don't make sense...), which makes me think that there is a mistake in . Can someone explain the basics involved in these matrix manipulations?","A \in \mathbb R^{m \times n} y \in \mathbb R^m x\in \mathbb R^n x \mapsto \big((Ax - y)^T(Ax - y) \big), \textbf{Attempt}. \begin{align}
\frac{\partial}{\partial x}  \big((Ax - y)^T(Ax - y) \big)
&= \frac{\partial}{\partial x} \big( (x^TA^TAx - x^TA^Ty - y^TAx+ y^Ty )\big)\\
&= \frac{\partial}{\partial x}x^TA^TAx - \frac{\partial}{\partial x}x^TA^Ty - \frac{\partial}{\partial x}y^TAx+ \frac{\partial}{\partial x}y^Ty  \\
&= 2 A^TAx - A^Ty - y^TA\qquad\,\,\mathbf{(1*)}\\
&= 2 A^TAx - 2A^Ty. \qquad\qquad\,\mathbf{(2*)}\\
\end{align} \textbf{Question}. (*) (1*) (2*) (1*)","['matrices', 'multivariable-calculus', 'least-squares', 'scalar-fields']"
10,Proving a series of matrices to be positive,Proving a series of matrices to be positive,,"Assume that $A \geq 0$ , $(I-A)$ is invertible. I have shown that if we have a sequence $$ S_n = I + A + A^2 + \cdots $$ that $$(I-A)S_n = I - S^{n+1} = I$$ as $n \rightarrow \infty$ . How can I now show that $S_{n \rightarrow \infty} \geq 0$ ? ""Positive"" here means that all entries of the matrix are $\geq 0$ .","Assume that , is invertible. I have shown that if we have a sequence that as . How can I now show that ? ""Positive"" here means that all entries of the matrix are .",A \geq 0 (I-A)  S_n = I + A + A^2 + \cdots  (I-A)S_n = I - S^{n+1} = I n \rightarrow \infty S_{n \rightarrow \infty} \geq 0 \geq 0,"['sequences-and-series', 'matrices', 'matrix-calculus', 'positive-matrices']"
11,Convergence of $A^n v$ for a matrix $A$ and a vector $v$,Convergence of  for a matrix  and a vector,A^n v A v,"In this question, Martin Sleziak shows that for a complex square matrix $A$ , we have $$\lim A^n = 0 $$ if and only if the spectral radius of $A$ is less than $1$ . I was wondering what happens if we are interested only in the convergence with respect to a given vector. To be more specific, suppose now that all eigenvalues of $A$ have modulus $\ge 1$ and let v be some given vector such that $$\lim A^n v = 0. $$ Does it imply that $ v = 0 $ ? I suspect the answer is yes, but I don't know how to deal with the case where two distinct eigenvalues of $A$ have the same modulus. Edit: The comments encouraged me to add more details. Let $J$ be the Jordan form of $A$ and $P$ such that $A=PJP^{-1}$ . Then $$A^n = P\begin{pmatrix} J_1^n & \;     & \; \\ \;  & \ddots & \; \\  \;  & \;     & J_m^n \end{pmatrix}P^{-1} $$ where $J_i$ is a Jordan block of size $k_i\times k_i$ , so that $$ J_i^n = \begin{pmatrix} \lambda_i^n & \binom{n}{1}\lambda_i^{n-1} & \binom{n}{2}\lambda_i^{n-2} & \cdots & \cdots & \binom{n}{k_i-1}\lambda_i^{n-k_i+1} \\  & \lambda_i^n & \binom{n}{1}\lambda_i^{n-1} & \cdots & \cdots & \binom{n}{k_i-2}\lambda_i^{n-k_i+2} \\  &  & \ddots & \ddots & \vdots & \vdots\\  &  & & \ddots & \ddots & \vdots\\  &  & &  & \lambda_i^n & \binom{n}{1}\lambda_i^{n-1}\\  &  &  &  &  & \lambda_i^n \end{pmatrix}. $$ Thus every entry of $A^n v$ has the form $$\tag{1}\label{sum}\sum_{i=1}^m p_i(n)\lambda_i^n$$ where $p_i$ is a complex polynomial such that $\deg p_i< k_i$ for $i=1,\dots,m$ . So essentially what I'm asking is this: Suppose $|\lambda_i|\ge1$ for $i=1,\dots,m$ and that \eqref{sum} converges to $0$ (when $n\to\infty$ ). Does it imply that the sequence \eqref{sum} is identically zero?","In this question, Martin Sleziak shows that for a complex square matrix , we have if and only if the spectral radius of is less than . I was wondering what happens if we are interested only in the convergence with respect to a given vector. To be more specific, suppose now that all eigenvalues of have modulus and let v be some given vector such that Does it imply that ? I suspect the answer is yes, but I don't know how to deal with the case where two distinct eigenvalues of have the same modulus. Edit: The comments encouraged me to add more details. Let be the Jordan form of and such that . Then where is a Jordan block of size , so that Thus every entry of has the form where is a complex polynomial such that for . So essentially what I'm asking is this: Suppose for and that \eqref{sum} converges to (when ). Does it imply that the sequence \eqref{sum} is identically zero?","A \lim A^n = 0  A 1 A \ge 1 \lim A^n v = 0.   v = 0  A J A P A=PJP^{-1} A^n =
P\begin{pmatrix}
J_1^n & \;     & \; \\
\;  & \ddots & \; \\ 
\;  & \;     & J_m^n
\end{pmatrix}P^{-1}  J_i k_i\times k_i  J_i^n = \begin{pmatrix}
\lambda_i^n & \binom{n}{1}\lambda_i^{n-1} & \binom{n}{2}\lambda_i^{n-2} & \cdots & \cdots & \binom{n}{k_i-1}\lambda_i^{n-k_i+1} \\
 & \lambda_i^n & \binom{n}{1}\lambda_i^{n-1} & \cdots & \cdots & \binom{n}{k_i-2}\lambda_i^{n-k_i+2} \\
 &  & \ddots & \ddots & \vdots & \vdots\\
 &  & & \ddots & \ddots & \vdots\\
 &  & &  & \lambda_i^n & \binom{n}{1}\lambda_i^{n-1}\\
 &  &  &  &  & \lambda_i^n
\end{pmatrix}.  A^n v \tag{1}\label{sum}\sum_{i=1}^m p_i(n)\lambda_i^n p_i \deg p_i< k_i i=1,\dots,m |\lambda_i|\ge1 i=1,\dots,m 0 n\to\infty","['linear-algebra', 'matrices', 'convergence-divergence']"
12,Why $\|(A + cI)^{-1}x\|\leq \frac{\|x\|}{\lambda_{\min}(A)}$,Why,\|(A + cI)^{-1}x\|\leq \frac{\|x\|}{\lambda_{\min}(A)},"Given $A\succ 0$ (positive-definite) and $c>0$ , I am trying to show $$\|(A + cI)^{-1}x\|\leq \frac{\|x\|}{\lambda_{\min}(A)} \tag{1}$$ using information like this but without success so far. Could you please help to prove it? Any help is welcome. EDIT: This inequality is an abstract form of the inequality $$\left\| \left[f''(x_k) - r_M(x_k)\frac{M}{2}I \right]^{-1}f'(x_k)\right\| \leq \frac{\|f'(x_k)\|}{\lambda_{min}\left(f''(x_k)\right)}\tag{2}$$ appeared here (Theorem 3). Here is the part of the proof","Given (positive-definite) and , I am trying to show using information like this but without success so far. Could you please help to prove it? Any help is welcome. EDIT: This inequality is an abstract form of the inequality appeared here (Theorem 3). Here is the part of the proof",A\succ 0 c>0 \|(A + cI)^{-1}x\|\leq \frac{\|x\|}{\lambda_{\min}(A)} \tag{1} \left\| \left[f''(x_k) - r_M(x_k)\frac{M}{2}I \right]^{-1}f'(x_k)\right\| \leq \frac{\|f'(x_k)\|}{\lambda_{min}\left(f''(x_k)\right)}\tag{2},"['matrices', 'inequality', 'normed-spaces', 'inverse', 'upper-lower-bounds']"
13,Simplifying matrix equation $A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1}$,Simplifying matrix equation,A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1},"I want to simplify the inverse of a $2\times 2$ block matrix: $$ \begin{bmatrix}  A_{n\times n} & B^T_{n\times m}\\ B_{m\times n} & 0_{m\times m} \end{bmatrix} $$ $A_{n\times n}$ is square and invertible (and possibly positive definite), and $B_{m\times n} (m < n) $ is not square but full row rank. Using this eq. (2.2), the inverse can be written as: $$ \begin{bmatrix}  A & B^T\\ B & 0 \end{bmatrix} ^{-1}=  \begin{bmatrix} A^{-1}-A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1}  &  A^{-1}B^T(BA^{-1}B^T)^{-1} \\ (BA^{-1}B^T)^{-1}BA^{-1} & -(BA^{-1}B^T)^{-1} \end{bmatrix} $$ I want to see is there a way to simplify this matrix? For example, can we simplify this term $A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1}$ , or other terms?","I want to simplify the inverse of a block matrix: is square and invertible (and possibly positive definite), and is not square but full row rank. Using this eq. (2.2), the inverse can be written as: I want to see is there a way to simplify this matrix? For example, can we simplify this term , or other terms?","2\times 2 
\begin{bmatrix} 
A_{n\times n} & B^T_{n\times m}\\
B_{m\times n} & 0_{m\times m}
\end{bmatrix}
 A_{n\times n} B_{m\times n} (m < n)  
\begin{bmatrix} 
A & B^T\\
B & 0
\end{bmatrix} ^{-1}= 
\begin{bmatrix}
A^{-1}-A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1}  &  A^{-1}B^T(BA^{-1}B^T)^{-1} \\
(BA^{-1}B^T)^{-1}BA^{-1} & -(BA^{-1}B^T)^{-1}
\end{bmatrix}
 A^{-1}B^T(BA^{-1}B^T)^{-1}BA^{-1}","['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
14,Is there a link between two magic squares with the same constant?,Is there a link between two magic squares with the same constant?,,"For instance we consider the magic squares of order $3$ with the constant $15$ . We can find : \begin{array}{ | l | c | r | }      \hline      8 & 3 & 4 \\ \hline      1 & 5 & 9 \\ \hline      6 & 7 & 2 \\      \hline     \end{array} and \begin{array}{ | l | c | r | }      \hline      -2 & 8 & 9 \\ \hline      16 & 5 & -6 \\ \hline      1 & 2 & 12 \\      \hline     \end{array} Do we have a tool to transform the first matrix into the second ? I mean that does the invariant (same constant) is sufficient to classify all the magic squares of a given constant under a group's action ? I know that of it was the same numbers for the two matrices, there exists an action from the group $\mathcal{D}_4$ on the set of magic squares of order $3$ . Thanks in advance !","For instance we consider the magic squares of order with the constant . We can find : and Do we have a tool to transform the first matrix into the second ? I mean that does the invariant (same constant) is sufficient to classify all the magic squares of a given constant under a group's action ? I know that of it was the same numbers for the two matrices, there exists an action from the group on the set of magic squares of order . Thanks in advance !","3 15 \begin{array}{ | l | c | r | }
     \hline
     8 & 3 & 4 \\ \hline
     1 & 5 & 9 \\ \hline
     6 & 7 & 2 \\
     \hline
    \end{array} \begin{array}{ | l | c | r | }
     \hline
     -2 & 8 & 9 \\ \hline
     16 & 5 & -6 \\ \hline
     1 & 2 & 12 \\
     \hline
    \end{array} \mathcal{D}_4 3","['matrices', 'group-actions', 'invariance', 'magic-square']"
15,Derivative of submatrix with respect to the whole block matrix,Derivative of submatrix with respect to the whole block matrix,,"I am reading a research paper and getting stucked with how they derived a formula. Suppose that we have the following block matrix $$\underset{d \times d}{\boldsymbol{A}} = \begin{bmatrix} \underset{q \times q}{\boldsymbol{A}_{11}} & \underset{q \times (d - q)}{\boldsymbol{A}_{12}} \\ \underset{(d - q) \times q}{\boldsymbol{A}_{21}} & \underset{(d - q) \times (d - q)}{\boldsymbol{A}_{22}} \end{bmatrix}$$ The formula involves taking the derivative of a quantity that involves $\boldsymbol{A}_{22}$ with respect to $\boldsymbol{A}$ . In particular, that quantity is $$\text{trace} (\boldsymbol{A}_{22} \boldsymbol{B}),$$ where $\boldsymbol{B}$ is matrix with dimension $(d - q) \times (d - q)$ . We need to take the following derivative $$\frac{\partial }{\partial \boldsymbol{A}} \text{trace} (\boldsymbol{A}_{22} \boldsymbol{B}).$$ In the context of my paper, $\boldsymbol{A}$ is a symmetric matrix, but if possible, I would assume $\boldsymbol{A}$ is a square matrix. Please help me if you have an idea. Thank you so much.","I am reading a research paper and getting stucked with how they derived a formula. Suppose that we have the following block matrix The formula involves taking the derivative of a quantity that involves with respect to . In particular, that quantity is where is matrix with dimension . We need to take the following derivative In the context of my paper, is a symmetric matrix, but if possible, I would assume is a square matrix. Please help me if you have an idea. Thank you so much.","\underset{d \times d}{\boldsymbol{A}} = \begin{bmatrix} \underset{q \times q}{\boldsymbol{A}_{11}} & \underset{q \times (d - q)}{\boldsymbol{A}_{12}} \\ \underset{(d - q) \times q}{\boldsymbol{A}_{21}} & \underset{(d - q) \times (d - q)}{\boldsymbol{A}_{22}} \end{bmatrix} \boldsymbol{A}_{22} \boldsymbol{A} \text{trace} (\boldsymbol{A}_{22} \boldsymbol{B}), \boldsymbol{B} (d - q) \times (d - q) \frac{\partial }{\partial \boldsymbol{A}} \text{trace} (\boldsymbol{A}_{22} \boldsymbol{B}). \boldsymbol{A} \boldsymbol{A}","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
16,"Proof Verification: Prove that $AB-BA=A$ is false, given $A$ is invertible","Proof Verification: Prove that  is false, given  is invertible",AB-BA=A A,"Steps taken: Multiply both sides by $A^{-1}$ , given $A$ is invertible. $A^{-1}AB - A^{-1}BA = A^{-1}A $ Thus, $B - A^{-1}BA = I$ Take the trace of both sides. $\operatorname{Tr}(B-A^{-1}BA) = \operatorname{Tr}(I)$ Utilize the linear mapping property of trace. $\operatorname{Tr}(B)-\operatorname{tr}(A^{-1}BA) = \operatorname{Tr}(I)$ Utilize the cyclic property of trace. $\operatorname{Tr}(B)-\operatorname{tr}(BAA^{-1}) = \operatorname{Tr}(I)$ $\operatorname{Tr}(B)-\operatorname{tr}(BI) = \operatorname{Tr}(I)$ $\operatorname{Tr}(B)-\operatorname{tr}(B) = \operatorname{Tr}(I)$ Reaching a contradiction $0 = \operatorname{Tr}(I)$ Is my usage of the trace properties correct?","Steps taken: Multiply both sides by , given is invertible. Thus, Take the trace of both sides. Utilize the linear mapping property of trace. Utilize the cyclic property of trace. Reaching a contradiction Is my usage of the trace properties correct?",A^{-1} A A^{-1}AB - A^{-1}BA = A^{-1}A  B - A^{-1}BA = I \operatorname{Tr}(B-A^{-1}BA) = \operatorname{Tr}(I) \operatorname{Tr}(B)-\operatorname{tr}(A^{-1}BA) = \operatorname{Tr}(I) \operatorname{Tr}(B)-\operatorname{tr}(BAA^{-1}) = \operatorname{Tr}(I) \operatorname{Tr}(B)-\operatorname{tr}(BI) = \operatorname{Tr}(I) \operatorname{Tr}(B)-\operatorname{tr}(B) = \operatorname{Tr}(I) 0 = \operatorname{Tr}(I),"['linear-algebra', 'matrices', 'solution-verification', 'matrix-equations']"
17,"Given the minimal polynomial of a matrix $A^2$, what could the minimal polynomial of $A$ be?","Given the minimal polynomial of a matrix , what could the minimal polynomial of  be?",A^2 A,"It is given that the minimal polynomial of $A^2$ is $\phi_{A^2}(x) = (x-1)^2$ , where $A$ is a complex $4\times4$ matrix. The question is, what are the possible minimal polynomials for $A$ ? From the given that $\phi_{A^2}(x) = (x-1)^2$ , I can derive several things. First off, I know that the characteristic polynomial $f_{A^2}$ of $A^2$ must have the same irreducible factors, which implies that the only eigenvalue of $A^2$ is $1$ . Secondly, it is easily seen that $(A^2 -I)^2 = O$ and $A^2 - I \neq O$ . Therefore, the matrix $A^2 - I$ is nilpotent with index 2. This implies that the invariant system is either $\{2,1,1\}$ or $\{2,2\}$ . Putting this together, there are two possible Jordan forms of $A^2$ : $$ J_{A^2,1} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix} \quad\text{or}\quad J_{A^2,2} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1\end{bmatrix} $$ Great, all these things I know about $A^2$ ... but how does any of it lead to information about the minimal polynomial of $A$ ? Or just any information about $A$ at all? I'm kind of stuck on this.","It is given that the minimal polynomial of is , where is a complex matrix. The question is, what are the possible minimal polynomials for ? From the given that , I can derive several things. First off, I know that the characteristic polynomial of must have the same irreducible factors, which implies that the only eigenvalue of is . Secondly, it is easily seen that and . Therefore, the matrix is nilpotent with index 2. This implies that the invariant system is either or . Putting this together, there are two possible Jordan forms of : Great, all these things I know about ... but how does any of it lead to information about the minimal polynomial of ? Or just any information about at all? I'm kind of stuck on this.","A^2 \phi_{A^2}(x) = (x-1)^2 A 4\times4 A \phi_{A^2}(x) = (x-1)^2 f_{A^2} A^2 A^2 1 (A^2 -I)^2 = O A^2 - I \neq O A^2 - I \{2,1,1\} \{2,2\} A^2  J_{A^2,1} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmatrix} \quad\text{or}\quad J_{A^2,2} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 1 & 1\end{bmatrix}  A^2 A A","['linear-algebra', 'abstract-algebra', 'matrices', 'minimal-polynomials']"
18,"Quaternion's multiplicative group as a subgroup of $\mathrm{GL}(4,\mathbb{R})$",Quaternion's multiplicative group as a subgroup of,"\mathrm{GL}(4,\mathbb{R})","Thinking about $\mathbb{C}$ : One can view the multiplicative group of complex numbers to be the matrices in $\mathrm{GL}(2,\mathbb{R})$ such that commutes with the matrix $I=\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$ a quick calculation shows that those are the matrix of the form $\begin{pmatrix} a & b \\ -b & a \end{pmatrix}$ . One can also view the quaternion's multiplicative group as the matrices in $\mathrm{GL}(2,\mathbb{C})$ that satisfies $IX=\bar{X}I$ the same calculation as above shows that those are the matrices of the form $\begin{pmatrix} a & b \\ -\bar{b} & \bar{a} \end{pmatrix}$ . How can i find a matrix that indentifies by a law of the form above the multiplicative group of $\mathbb{H}$ in $\mathrm{GL}(4;\mathbb{R})$ ? My attempts: I've tryed to use the fact that in the matrix representation of complex numbers the conjugation is the transpose operation together with the properties of block matrix under transposition and substituing in the complex matrices the 2x2 blocks that corresponds to the complexes entries, but i didn't succeded. If someone wants i can give more datails of my attempt. The candidate matrix was a the real-blocks matrix \begin{pmatrix} 0 & -I \\ I & 0 \end{pmatrix} but i am not sure it is enought. Any hint or suggestion please?","Thinking about : One can view the multiplicative group of complex numbers to be the matrices in such that commutes with the matrix a quick calculation shows that those are the matrix of the form . One can also view the quaternion's multiplicative group as the matrices in that satisfies the same calculation as above shows that those are the matrices of the form . How can i find a matrix that indentifies by a law of the form above the multiplicative group of in ? My attempts: I've tryed to use the fact that in the matrix representation of complex numbers the conjugation is the transpose operation together with the properties of block matrix under transposition and substituing in the complex matrices the 2x2 blocks that corresponds to the complexes entries, but i didn't succeded. If someone wants i can give more datails of my attempt. The candidate matrix was a the real-blocks matrix but i am not sure it is enought. Any hint or suggestion please?","\mathbb{C} \mathrm{GL}(2,\mathbb{R}) I=\begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} a & b \\ -b & a \end{pmatrix} \mathrm{GL}(2,\mathbb{C}) IX=\bar{X}I \begin{pmatrix} a & b \\ -\bar{b} & \bar{a} \end{pmatrix} \mathbb{H} \mathrm{GL}(4;\mathbb{R}) \begin{pmatrix} 0 & -I \\ I & 0 \end{pmatrix}","['linear-algebra', 'abstract-algebra', 'matrices', 'complex-numbers', 'quaternions']"
19,solutions to quadratic matrix equation $XFX - F^{-1}X^{-1}F = 0$,solutions to quadratic matrix equation,XFX - F^{-1}X^{-1}F = 0,"What is the solution for the matrix $X$ in the following quadratic matrix equation? $$ XFX - F^{-1}X^{-1}F = 0 $$ where $F$ is a $N \times N$ discrete Fourier transform (DFT) matrix, so it's unitary ( $F^{-1} = F^\dagger$ ). Based on similar quadratic matrix equations it seems that $X$ would be expressed in terms of fractional powers of $F$ . I have access to the eigenvectors of $F$ and hence all fractional powers of $F$ thanks to the discrete fractional Fourier transform (DFRT) in Candan et al. (2000). Found an almost trivial solution $X = F^{-1/3}$ by guessing and checking. Substituting $X = F^{-1/3}$ into the equation results in $F^{1/3} - F^{1/3} = 0$ which is true hence $X = F^{-1/3}$ is a valid solution. Are there other nontrivial solutions? $F^{-1/3 + 4n}$ for integer $n$ is another obvious one due to the periodicity of the fractional Fourier transform.","What is the solution for the matrix in the following quadratic matrix equation? where is a discrete Fourier transform (DFT) matrix, so it's unitary ( ). Based on similar quadratic matrix equations it seems that would be expressed in terms of fractional powers of . I have access to the eigenvectors of and hence all fractional powers of thanks to the discrete fractional Fourier transform (DFRT) in Candan et al. (2000). Found an almost trivial solution by guessing and checking. Substituting into the equation results in which is true hence is a valid solution. Are there other nontrivial solutions? for integer is another obvious one due to the periodicity of the fractional Fourier transform.",X  XFX - F^{-1}X^{-1}F = 0  F N \times N F^{-1} = F^\dagger X F F F X = F^{-1/3} X = F^{-1/3} F^{1/3} - F^{1/3} = 0 X = F^{-1/3} F^{-1/3 + 4n} n,"['matrices', 'fourier-analysis', 'matrix-equations']"
20,Orthogonal idempotents in $M_{4\times 4}(\mathbb{C})$.,Orthogonal idempotents in .,M_{4\times 4}(\mathbb{C}),"If $e_1,\cdots,e_n \in M_{4\times 4}(\mathbb{C})$ are $n$ distinct, nonzero, $4\times 4$ matrices with complex entries that satisfy $e_ie_j = e_je_i = 0$ and $e_i^2 = e_i$ for all $1\leq i \leq n$ , then I want to show that $n \leq 4$ . My initial instinct was to look at the Jordan Canonical forms of the matrices $e_i$ , and note that the minimal polynomial of $e_i$ must divide $x^2-x$ , and hence must be $x,(x-1)$ or $x(x-1)$ . The minimal polynomial can't be $x$ (since the $e_i$ 's were assumed nonzero) and it can't be $x-1$ , since then $e_i$ is the identity matrix which is only orthogonal to the zero matrix. Thus, then minimal polynomial of $e_i$ is $x(x-1)$ , and the invariant factor decomposition of $e_i$ must be one of \begin{align*} &(x,x,x(x-1)) &(x-1,x-1,x(x-1))& &(x(x-1),x(x-1)). \end{align*} However, at this point I realized that this method does not seem to be on the right track, since  there is no reason why the $e_i$ 's shouldn't be similar, so knowing their JCF doesn't seem to help me reason why the total number of such matrices should be less than $4$ . Any thoughts or hints would be greatly appreciated.","If are distinct, nonzero, matrices with complex entries that satisfy and for all , then I want to show that . My initial instinct was to look at the Jordan Canonical forms of the matrices , and note that the minimal polynomial of must divide , and hence must be or . The minimal polynomial can't be (since the 's were assumed nonzero) and it can't be , since then is the identity matrix which is only orthogonal to the zero matrix. Thus, then minimal polynomial of is , and the invariant factor decomposition of must be one of However, at this point I realized that this method does not seem to be on the right track, since  there is no reason why the 's shouldn't be similar, so knowing their JCF doesn't seem to help me reason why the total number of such matrices should be less than . Any thoughts or hints would be greatly appreciated.","e_1,\cdots,e_n \in M_{4\times 4}(\mathbb{C}) n 4\times 4 e_ie_j = e_je_i = 0 e_i^2 = e_i 1\leq i \leq n n \leq 4 e_i e_i x^2-x x,(x-1) x(x-1) x e_i x-1 e_i e_i x(x-1) e_i \begin{align*}
&(x,x,x(x-1)) &(x-1,x-1,x(x-1))& &(x(x-1),x(x-1)).
\end{align*} e_i 4","['linear-algebra', 'matrices', 'idempotents']"
21,Show that rank($A^{n+1}$) = rank($A^n$) [duplicate],Show that rank() = rank() [duplicate],A^{n+1} A^n,"This question already has answers here : Given a square matrix A of order n, prove $\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$ (3 answers) Closed 3 years ago . Suppose $A$ is a $n \times n$ matrix i.e. $A \in \mathbb{C}^{n \times n}$ , prove that rank( $A^{n+1}$ ) = rank( $A^n$ ). In other words, I need to prove that their range spaces or null spaces are equal. If it helps, $A$ is a singular matrix. Note that, I don't want to use Jordan blocks to prove this. Is it possible to prove this without using Jordan form? I can use Schur's triangularization theorem. Also, it's not known if A is diagonalizable.","This question already has answers here : Given a square matrix A of order n, prove $\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$ (3 answers) Closed 3 years ago . Suppose is a matrix i.e. , prove that rank( ) = rank( ). In other words, I need to prove that their range spaces or null spaces are equal. If it helps, is a singular matrix. Note that, I don't want to use Jordan blocks to prove this. Is it possible to prove this without using Jordan form? I can use Schur's triangularization theorem. Also, it's not known if A is diagonalizable.",A n \times n A \in \mathbb{C}^{n \times n} A^{n+1} A^n A,"['linear-algebra', 'matrices', 'matrix-rank']"
22,Jacobian of non-square matrices,Jacobian of non-square matrices,,"Let $m>n$ and $L\colon \mathbb{R}^m \to \mathbb{R}^n$ be a linear map(=matrix). The "" $n$ -dimensional Jacobian"" $$ J^n(L) = \sqrt{\det(LL^t)}  $$ is of geometric significance! Let's say it makes the co-area formula possible. Suppose $C\colon \mathbb{R}^m \to \mathbb{R}^m$ is an invertible linear map (think of it as a change of coordinates). I am interested in the precise formula relating $J^n(L)$ and $J^n(L \circ C)$ . For simplicity assume $L$ is surjective, i.e. it has rank $n$ . Denote by $C^{ker}$ the restriction of $C$ to the $(m-n)$ -dimensional $(\ker L\circ C)$ . So, $C^{ker}\colon (\ker L\circ C) \to \ker L$ can be seen as a map $\mathbb{R}^{m-n} \to \mathbb{R}^{m-n}$ . I think the answer should be: $$ J^n(L) = \frac{|\det C^{ker}|}{|\det C|}\, \cdot J^n(L \circ C)\, , $$ Because for certain $C$ I do have a proof, albeit quite a tricky one. Questions: Is the claimed identity true? If not, what is the right formula? How does one prove it?","Let and be a linear map(=matrix). The "" -dimensional Jacobian"" is of geometric significance! Let's say it makes the co-area formula possible. Suppose is an invertible linear map (think of it as a change of coordinates). I am interested in the precise formula relating and . For simplicity assume is surjective, i.e. it has rank . Denote by the restriction of to the -dimensional . So, can be seen as a map . I think the answer should be: Because for certain I do have a proof, albeit quite a tricky one. Questions: Is the claimed identity true? If not, what is the right formula? How does one prove it?","m>n L\colon \mathbb{R}^m \to \mathbb{R}^n n 
J^n(L) = \sqrt{\det(LL^t)} 
 C\colon \mathbb{R}^m \to \mathbb{R}^m J^n(L) J^n(L \circ C) L n C^{ker} C (m-n) (\ker L\circ C) C^{ker}\colon (\ker L\circ C) \to \ker L \mathbb{R}^{m-n} \to \mathbb{R}^{m-n} 
J^n(L) = \frac{|\det C^{ker}|}{|\det C|}\, \cdot J^n(L \circ C)\, ,
 C","['linear-algebra', 'matrices', 'geometry', 'multivariable-calculus', 'determinant']"
23,Show that the rank of a symmetric matrix is the maximum order of a principal sub-matrix which is invertible,Show that the rank of a symmetric matrix is the maximum order of a principal sub-matrix which is invertible,,"The question is this- Show that the rank of a symmetric matrix is the maximum order of a principal sub-matrix which is invertible. I can show that there cannot exist a sub-matrix with rank more than the actual rank of the matrix. But I cannot show the other way around, i.e when the rank of the actual matrix is $r$ , then there exist a principal sub-matrix of the same rank (though I can prove that there exist a sub-matrix with rank $r$ ). I was thinking like this: if the rank of the matrix is $r$ , we can find r linearly independent rows of the matrix, say $a_1, a_2, ..., a_r$ -th rows are linearly independent. Then the corresposding columns $a_1^t, a_2^t, ..., a_r^t$ are also linearly independent. But how to show that the submatrix that they produce is of rank r? I hope my question is clear. Any hints or help would be highly appreciated.","The question is this- Show that the rank of a symmetric matrix is the maximum order of a principal sub-matrix which is invertible. I can show that there cannot exist a sub-matrix with rank more than the actual rank of the matrix. But I cannot show the other way around, i.e when the rank of the actual matrix is , then there exist a principal sub-matrix of the same rank (though I can prove that there exist a sub-matrix with rank ). I was thinking like this: if the rank of the matrix is , we can find r linearly independent rows of the matrix, say -th rows are linearly independent. Then the corresposding columns are also linearly independent. But how to show that the submatrix that they produce is of rank r? I hope my question is clear. Any hints or help would be highly appreciated.","r r r a_1, a_2, ..., a_r a_1^t, a_2^t, ..., a_r^t","['linear-algebra', 'matrices', 'symmetric-matrices']"
24,Foiling vectors with a matrix term,Foiling vectors with a matrix term,,"Please excuse the simple question, but I can't seem to find anything relevant online. I have a term of the form: $$(\mathbf x-\mathbf y)^\top A(\mathbf x-\mathbf y)$$ and I can't figure out how to multiply these out to separate the terms as with FOILing with scalar variables as the matrix term is throwing me off. Is there any way to do that?","Please excuse the simple question, but I can't seem to find anything relevant online. I have a term of the form: and I can't figure out how to multiply these out to separate the terms as with FOILing with scalar variables as the matrix term is throwing me off. Is there any way to do that?",(\mathbf x-\mathbf y)^\top A(\mathbf x-\mathbf y),"['linear-algebra', 'matrices', 'vectors']"
25,Solving a least square problems where the vectors are known and the unknown is the matrix,Solving a least square problems where the vectors are known and the unknown is the matrix,,"I'm trying to solve a least-square problem of the form $Fc \approx d$ where $c$ and $d$ are known $3\times24$ matrices and $F$ is an unknown $3\times3$ matrix. I was asked to try and solve a different problem in order to find $F$ : reinterpreting $F$ as a $9\times1$ vector $x$ and solving $Ax = b$ , but I have no idea what $A$ and $b$ should represent, only that they'd be found from $c$ and $d$ . Does anyone have an idea as to how I could do this? Thanks in advance","I'm trying to solve a least-square problem of the form where and are known matrices and is an unknown matrix. I was asked to try and solve a different problem in order to find : reinterpreting as a vector and solving , but I have no idea what and should represent, only that they'd be found from and . Does anyone have an idea as to how I could do this? Thanks in advance",Fc \approx d c d 3\times24 F 3\times3 F F 9\times1 x Ax = b A b c d,"['linear-algebra', 'matrices', 'least-squares']"
26,Does any undirected graph admits a doubly stochastic matrix?,Does any undirected graph admits a doubly stochastic matrix?,,"Given an undirected graph, can we always find a way to assign weights to every edge so that the adjacent matrix is double stochastic matrix? (edit: We allow the matrix has positive element on (i,i), so strictly speaking, it is not an adjacent matrix) The background is about distributed optimization. We can see each node as a computer, and they want to exchange information(like gradients or parameters) via edges in the connected graph to solve an optimization problem, for example, a machine learning problem. So, there is a weight matrix W, whose (i,j) element is the weight of information sent from node i to node j. Because node i can use its own information, so diagonal elements of W should be positive. The question is can we always find a doubly stochastic matrix W.","Given an undirected graph, can we always find a way to assign weights to every edge so that the adjacent matrix is double stochastic matrix? (edit: We allow the matrix has positive element on (i,i), so strictly speaking, it is not an adjacent matrix) The background is about distributed optimization. We can see each node as a computer, and they want to exchange information(like gradients or parameters) via edges in the connected graph to solve an optimization problem, for example, a machine learning problem. So, there is a weight matrix W, whose (i,j) element is the weight of information sent from node i to node j. Because node i can use its own information, so diagonal elements of W should be positive. The question is can we always find a doubly stochastic matrix W.",,"['matrices', 'graph-theory']"
27,Largest Singular Value of Triangular Matrix,Largest Singular Value of Triangular Matrix,,"Let $R\in\mathbb{R}^{d\times d}$ be an upper triangular matrix. The eigenvalues of $R$ is then its diagonal entries, that is, $(R_{ii}, e_i)$ is an eigenpair of $R$ since $Re_i=R_{ii}e_i$ . Is it possible to bound the largest singular value $\sigma_\max (R)=\sqrt{\lambda_\max (RR^T)}$ by the diagonal entries of $R$ ? Weyl's inequality tells us $\lambda_\max \le \sigma_\max$ which is the wrong direction. But maybe we can get something like $\sigma_\max < \lambda_\max^2$ assuming $\lambda_\max>1$ ? Or can the largest singular value be arbitrarily larger?","Let be an upper triangular matrix. The eigenvalues of is then its diagonal entries, that is, is an eigenpair of since . Is it possible to bound the largest singular value by the diagonal entries of ? Weyl's inequality tells us which is the wrong direction. But maybe we can get something like assuming ? Or can the largest singular value be arbitrarily larger?","R\in\mathbb{R}^{d\times d} R (R_{ii}, e_i) R Re_i=R_{ii}e_i \sigma_\max (R)=\sqrt{\lambda_\max (RR^T)} R \lambda_\max \le \sigma_\max \sigma_\max < \lambda_\max^2 \lambda_\max>1","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'singular-values']"
28,Eigenvalues of $A^{2018}$,Eigenvalues of,A^{2018},"Find the eigenvalues and eigenvectors of $A^{2018}$ . $$ A=\begin{bmatrix} 1 & 3 & 4\\ 3 & 1 & 4\\ 0 & 0 & 4\end{bmatrix} $$ My solution: First, by substracting first row times three from second row we get: $$ A\approx \begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix} $$ We achieved the upper triangular matrix so the characteristic polynomial is: $$ \chi_{A^{2018}}(\lambda)=det (\begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix}^{2018}-\lambda I)=(1^{2018}-\lambda)((-8)^{2018}-\lambda)(4^{2018}-\lambda) $$ Therefore the set of eigevalues is $\{1,4^{2018},8^{2018},\}$ . Please verify if this the correct solution, and in case it isn't, help me find the correct one.","Find the eigenvalues and eigenvectors of . My solution: First, by substracting first row times three from second row we get: We achieved the upper triangular matrix so the characteristic polynomial is: Therefore the set of eigevalues is . Please verify if this the correct solution, and in case it isn't, help me find the correct one.","A^{2018} 
A=\begin{bmatrix} 1 & 3 & 4\\ 3 & 1 & 4\\ 0 & 0 & 4\end{bmatrix}
 
A\approx \begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix}
 
\chi_{A^{2018}}(\lambda)=det (\begin{bmatrix} 1 & 3 & 4\\ 0 & -8 & -8\\ 0 & 0 & 4\end{bmatrix}^{2018}-\lambda I)=(1^{2018}-\lambda)((-8)^{2018}-\lambda)(4^{2018}-\lambda)
 \{1,4^{2018},8^{2018},\}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
29,Is there a better/easier way to solve this matrix?,Is there a better/easier way to solve this matrix?,,\begin{equation*} \begin{bmatrix} 4 & -1 & -1 & 0 &|&30 \\ -1 & 4 & 0 & -1&|&60 \\ -1 & 0 & 4 & -1&|&40 \\ 0 & -1 & -1 & 4&|&70 \end{bmatrix} \end{equation*} What's the best way to solve the matrix above? There's a clear pattern of the diagonal 4's and 0's and the -1's so I feel like there has to be a better way of doing things rather than using scaling and row reduction. If I do those methods I end up with messy fractions. My Step 1: New Row 2 = (1/4)Row 1 + Row 2 Even at step 1 I can tell the whole thing will be messy with fractions. Is there a better way to solve this matrix? Or am I doing it wrong? Thanks.,What's the best way to solve the matrix above? There's a clear pattern of the diagonal 4's and 0's and the -1's so I feel like there has to be a better way of doing things rather than using scaling and row reduction. If I do those methods I end up with messy fractions. My Step 1: New Row 2 = (1/4)Row 1 + Row 2 Even at step 1 I can tell the whole thing will be messy with fractions. Is there a better way to solve this matrix? Or am I doing it wrong? Thanks.,"\begin{equation*}
\begin{bmatrix}
4 & -1 & -1 & 0 &|&30 \\
-1 & 4 & 0 & -1&|&60 \\
-1 & 0 & 4 & -1&|&40 \\
0 & -1 & -1 & 4&|&70
\end{bmatrix}
\end{equation*}","['calculus', 'matrices', 'matrix-equations', 'matrix-calculus']"
30,"Proof of inner product property: $\langle Ax,x \rangle = \langle x,A^Tx \rangle$ [duplicate]",Proof of inner product property:  [duplicate],"\langle Ax,x \rangle = \langle x,A^Tx \rangle","This question already has an answer here : Property of the conjugate transpose matrix with inner product (1 answer) Closed 3 years ago . $A$ is a matrix in $\mathbb{R}^{m \times n}$ and $x \in \mathbb{R}^n$ . I want to prove that: $$\langle Ax, x \rangle = \langle x , A^T x \rangle $$ Having $A^T$ as the transpose of the matrix $A$ . Just realized I use this property so much, yet I don't known how to prove it.","This question already has an answer here : Property of the conjugate transpose matrix with inner product (1 answer) Closed 3 years ago . is a matrix in and . I want to prove that: Having as the transpose of the matrix . Just realized I use this property so much, yet I don't known how to prove it.","A \mathbb{R}^{m \times n} x \in \mathbb{R}^n \langle Ax, x \rangle = \langle x , A^T x \rangle  A^T A","['linear-algebra', 'matrices', 'inner-products']"
31,Eigenvalues and null space,Eigenvalues and null space,,"I would like to understand the relationships between the null space and the eigenvalues of a matrix better. First of all, we know that an $n \times n$ matrix will have $n$ eigenvalues, though the eigenvalues can be complex and repeated. Next, we know that if $A$ has the eigenvalue 0, then the corresponding eigenvector is in the null space $N(A)$ , since $A\textbf{x}=0\textbf{x}=\textbf{0}$ . This implies that all eigenvectors that correspond to the eigenvalue 0 exactly span $N(A)$ . Using the above-mentioned two conclusions, and assume we have an $n \times n$ matrix with rank $r$ , now we know the dimension of the null space is $n-r$ . From this, can we conclude that there will be at least $n-r$ eigenvalues that equal to 0? and exact $n-r$ independent eigenvectors to span the null space?","I would like to understand the relationships between the null space and the eigenvalues of a matrix better. First of all, we know that an matrix will have eigenvalues, though the eigenvalues can be complex and repeated. Next, we know that if has the eigenvalue 0, then the corresponding eigenvector is in the null space , since . This implies that all eigenvectors that correspond to the eigenvalue 0 exactly span . Using the above-mentioned two conclusions, and assume we have an matrix with rank , now we know the dimension of the null space is . From this, can we conclude that there will be at least eigenvalues that equal to 0? and exact independent eigenvectors to span the null space?",n \times n n A N(A) A\textbf{x}=0\textbf{x}=\textbf{0} N(A) n \times n r n-r n-r n-r,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
32,Matrix differential of a trace with Hadamard product,Matrix differential of a trace with Hadamard product,,"I'm encountering difficulties taking the differential of the following matrix expression with respect to $S$ : $\text{logdet}(S) + \text{Tr}[C(D\odot((AS^{-1/2}B)(AS^{-1/2}B)^{T}))]$ $C$ and $D$ are symmetric and $S$ is diagonal so I mean taking the element-wise inverse of the element-wise square-root by the notation $S^{-1/2}$ . From Matrix CookBook, I know that the first term leads to $\text{Tr}(S^{-1}dS)$ and I know that I can apply the differential of the expression inside the trace term but I'm struggle with the computation of the differential because of the quadratic form coupled with the Hadamard product. I have tried to rewrite the expression by means of Hadamard and Frobenius products (that are commutative)...without success. Then, my goal is to find the ""roots"" of the derivative with respect to $S$ . Given the form of the expression, my intuition is that I will obtain a fixed-point expression (in the sense that it is not possible to obtain a closed-form expression in the form $\hat{S}=$ something that doesn't depend on $S$ ), but it's not a problem, I will solve it numerically. Can you help me ? Thank you in advance.","I'm encountering difficulties taking the differential of the following matrix expression with respect to : and are symmetric and is diagonal so I mean taking the element-wise inverse of the element-wise square-root by the notation . From Matrix CookBook, I know that the first term leads to and I know that I can apply the differential of the expression inside the trace term but I'm struggle with the computation of the differential because of the quadratic form coupled with the Hadamard product. I have tried to rewrite the expression by means of Hadamard and Frobenius products (that are commutative)...without success. Then, my goal is to find the ""roots"" of the derivative with respect to . Given the form of the expression, my intuition is that I will obtain a fixed-point expression (in the sense that it is not possible to obtain a closed-form expression in the form something that doesn't depend on ), but it's not a problem, I will solve it numerically. Can you help me ? Thank you in advance.",S \text{logdet}(S) + \text{Tr}[C(D\odot((AS^{-1/2}B)(AS^{-1/2}B)^{T}))] C D S S^{-1/2} \text{Tr}(S^{-1}dS) S \hat{S}= S,"['matrices', 'derivatives', 'matrix-calculus', 'differential', 'hadamard-product']"
33,Show that the determinant of a matrix is nonzero [duplicate],Show that the determinant of a matrix is nonzero [duplicate],,"This question already has answers here : Show determinant of matrix is non-zero (5 answers) Closed 3 years ago . Suppose $u,v,w \in \mathbb{Q}$ with $u,v,w \neq 0$ . Show that the determinant of the following matrix is nonzero. $$M = \begin{bmatrix} u & 2w & 2v \\ v & u & 2w \\ w & v & u \end{bmatrix}$$ Hint: Argue by contradiction, reduce to the case when $u,v,w$ are integers and use some number theory over $\mathbb{Z}$ . I know that the determinant is given by $$ \det M = u^{3} + 2v^{3} + 4w^{3} - 6 \,u\,v\,w $$ It is unclear to me how to utilize the hint. How to convert the problem over $\mathbb{Z}$ ?","This question already has answers here : Show determinant of matrix is non-zero (5 answers) Closed 3 years ago . Suppose with . Show that the determinant of the following matrix is nonzero. Hint: Argue by contradiction, reduce to the case when are integers and use some number theory over . I know that the determinant is given by It is unclear to me how to utilize the hint. How to convert the problem over ?","u,v,w \in \mathbb{Q} u,v,w \neq 0 M = \begin{bmatrix} u & 2w & 2v \\ v & u & 2w \\ w & v & u \end{bmatrix} u,v,w \mathbb{Z}  \det M = u^{3} + 2v^{3} + 4w^{3} - 6 \,u\,v\,w  \mathbb{Z}","['matrices', 'number-theory', 'determinant']"
34,"Why did we call a row operation ""elementary""?","Why did we call a row operation ""elementary""?",,"Why we called the three actions of row operation ""elementary""? Is there a thing called ""advanced"" or ""complicated"" row operation? I've seen the word ""non-elementary"" row operation is used to describe things like $R_1-R_2$ , which is not written in the conventional $-1R_2 + R_1$ . Is this usage correct? In particular, what should $R_1-R_2$ be called? \begin{align*}     &\text{a) A non-elementary row operation} \\     &\text{b) An elementary row operation} \\     &\text{c) Just a row operation} \\     &\text{d) It is not a row operation} \end{align*}","Why we called the three actions of row operation ""elementary""? Is there a thing called ""advanced"" or ""complicated"" row operation? I've seen the word ""non-elementary"" row operation is used to describe things like , which is not written in the conventional . Is this usage correct? In particular, what should be called?","R_1-R_2 -1R_2 + R_1 R_1-R_2 \begin{align*}
    &\text{a) A non-elementary row operation} \\
    &\text{b) An elementary row operation} \\
    &\text{c) Just a row operation} \\
    &\text{d) It is not a row operation}
\end{align*}","['linear-algebra', 'matrices', 'terminology']"
35,A question regarding matrices and minimal polynomials.,A question regarding matrices and minimal polynomials.,,"I am trying to answer the following question: Let $A$ be a $3\times 3$ real matrix and $A^4=I$ and $A\neq \pm I$ ,then does it imply $A^2+I=O$ ? Attempt: Since $A$ is $3\times 3$ real matrix,it has at least one eigenvalue $c\in \mathbb R$ .Then $(x-c)|m(x)$ ,where $m$ is the minimal polynomial of $A$ .Now, $A^2+I=O$ implies $x^2+1$ annihilates $A$ and hence $m(x)| x^2+1$ but then $(x-c)$ must be a factor of $(x^2+1)$ and hence $c$ must be a root of $x^2+1$ which is not possible as $c\in \mathbb R$ . Is this solution alright?","I am trying to answer the following question: Let be a real matrix and and ,then does it imply ? Attempt: Since is real matrix,it has at least one eigenvalue .Then ,where is the minimal polynomial of .Now, implies annihilates and hence but then must be a factor of and hence must be a root of which is not possible as . Is this solution alright?",A 3\times 3 A^4=I A\neq \pm I A^2+I=O A 3\times 3 c\in \mathbb R (x-c)|m(x) m A A^2+I=O x^2+1 A m(x)| x^2+1 (x-c) (x^2+1) c x^2+1 c\in \mathbb R,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'solution-verification', 'minimal-polynomials']"
36,"If $A$ is a rank $1$ matrix, then $A^2= \operatorname{Tr}A \cdot A$","If  is a rank  matrix, then",A 1 A^2= \operatorname{Tr}A \cdot A,"I am posting this question because I want to know if my proof is correct (I know that the result holds for $\mathbb{K}=\mathbb{C}$ and I see no reason why it wouldn't work for an arbitrary field, but I just want to be sure). Claim : Let $A \in \mathcal{M}_n(\mathbb{K})$ ( $\mathbb{K}$ is a field, $n\in \mathbb{N}, n\ge 2$ ) such that $\operatorname{rank}A=1$ . Then we have that $A^2=\operatorname{Tr}A\cdot A$ . Proof : Since $\operatorname{rank}A=1$ , $A's$ lines are proportional i.e. $A= \begin{pmatrix} b_1c_1 & b_1c_2 &...& b_1c_n\\ b_2c_1 & b_2c_2 &...& b_2c_n\\ ... & ... & ...& ...\\ b_nc_1 & b_nc_2 &...& b_nc_n\\ \end{pmatrix}=\begin{pmatrix} b_1 & 0 &...& 0\\ b_2 & 0 &...& 0\\ ... & ... & ...& ...\\ b_n & 0 &...& 0\\ \end{pmatrix}\cdot \begin{pmatrix} c_1 & c_2 &...& c_n\\ 0 & 0 &...& 0\\ ... & ... & ...& ...\\ 0 & 0 &...& 0\\ \end{pmatrix}.$ Let $B:=\begin{pmatrix} b_1 & 0 &...& 0\\ b_2 & 0 &...& 0\\ ... & ... & ...& ...\\ b_n & 0 &...& 0\\ \end{pmatrix}$ and $C:=\begin{pmatrix} c_1 & c_2 &...& c_n\\ 0 & 0 &...& 0\\ ... & ... & ...& ...\\ 0 & 0 &...& 0\\ \end{pmatrix}$ . We have that $A^2=B(CB)C=\operatorname{Tr}A\cdot BC=\operatorname{Tr}A\cdot A$ and we are done.","I am posting this question because I want to know if my proof is correct (I know that the result holds for and I see no reason why it wouldn't work for an arbitrary field, but I just want to be sure). Claim : Let ( is a field, ) such that . Then we have that . Proof : Since , lines are proportional i.e. Let and . We have that and we are done.","\mathbb{K}=\mathbb{C} A \in \mathcal{M}_n(\mathbb{K}) \mathbb{K} n\in \mathbb{N}, n\ge 2 \operatorname{rank}A=1 A^2=\operatorname{Tr}A\cdot A \operatorname{rank}A=1 A's A= \begin{pmatrix}
b_1c_1 & b_1c_2 &...& b_1c_n\\
b_2c_1 & b_2c_2 &...& b_2c_n\\
... & ... & ...& ...\\
b_nc_1 & b_nc_2 &...& b_nc_n\\
\end{pmatrix}=\begin{pmatrix}
b_1 & 0 &...& 0\\
b_2 & 0 &...& 0\\
... & ... & ...& ...\\
b_n & 0 &...& 0\\
\end{pmatrix}\cdot \begin{pmatrix}
c_1 & c_2 &...& c_n\\
0 & 0 &...& 0\\
... & ... & ...& ...\\
0 & 0 &...& 0\\
\end{pmatrix}. B:=\begin{pmatrix}
b_1 & 0 &...& 0\\
b_2 & 0 &...& 0\\
... & ... & ...& ...\\
b_n & 0 &...& 0\\
\end{pmatrix} C:=\begin{pmatrix}
c_1 & c_2 &...& c_n\\
0 & 0 &...& 0\\
... & ... & ...& ...\\
0 & 0 &...& 0\\
\end{pmatrix} A^2=B(CB)C=\operatorname{Tr}A\cdot BC=\operatorname{Tr}A\cdot A","['linear-algebra', 'matrices', 'solution-verification']"
37,The signature of the tensor product of skew-symmetric non-singular matrices,The signature of the tensor product of skew-symmetric non-singular matrices,,"Let $X$ be a non-singular (real) symmetric matrix which is the tensor product of two (real) $n\times n$ skew symmetric non-singular matrices, i.e. $X=A\otimes B$ . Then how to see the number of negative eigenvalues of $X$ equals to the number of positive eigenvalues of $X$ . What I know is the eigenvalues of $X$ are the pairwise products of the eigenvalues of $A$ and $B$ and the eigenvalues of $X$ are purely imaginary. I wonder if it suffices to show the claim? If so, is that possible to show me a proof?","Let be a non-singular (real) symmetric matrix which is the tensor product of two (real) skew symmetric non-singular matrices, i.e. . Then how to see the number of negative eigenvalues of equals to the number of positive eigenvalues of . What I know is the eigenvalues of are the pairwise products of the eigenvalues of and and the eigenvalues of are purely imaginary. I wonder if it suffices to show the claim? If so, is that possible to show me a proof?",X n\times n X=A\otimes B X X X A B X,"['linear-algebra', 'matrices']"
38,Find the matrix $A^{15}$.,Find the matrix .,A^{15},"Let $I= \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ \end{pmatrix}$ and $O=\begin{pmatrix} 0 & 0 \\ 0 & 0 \\ \end{pmatrix}$ . 1.Let $A=\begin{pmatrix} 1 & 3 \\ 3 & 5 \\ \end{pmatrix}$ and $ B=\begin{pmatrix} x & 3 \\ 3 & 6 \\  \end{pmatrix}$ . Find the value of $x$ which satisfies $AB=BA$ . $AB=\begin{pmatrix} 1 & 3 \\ 3 & 5 \\ \end{pmatrix}\cdot\begin{pmatrix} x & 3 \\ 3 & 6 \\ \end{pmatrix}=\begin{pmatrix} x+9 & 21 \\ 3x+15 & 39 \\ \end{pmatrix}$ $BA=\begin{pmatrix} x & 3 \\ 3 & 6 \\ \end{pmatrix}\cdot\begin{pmatrix} 1 & 3 \\ 3 & 5 \\ \end{pmatrix}=\begin{pmatrix} x+9 & 3x+15 \\ 21 & 39 \\ \end{pmatrix}$ .So that we get $3x+15=21 \Rightarrow x=2$ 2.Let $A=\begin{pmatrix} 1 & 2 \\ 2 & 4 \\ \end{pmatrix}$ and $ B=\begin{pmatrix} -2 & x \\ 4 & y \\ \end{pmatrix}$ .Find the values of x and y which satify $BA=O$ . $BA=\begin{pmatrix} -2 & x \\ 4 & y \\ \end{pmatrix}\cdot\begin{pmatrix} 1 & 2 \\ 2 & 4 \\ \end{pmatrix}=\begin{pmatrix} -2+2x & 0 \\ 4+2y & 8+4y \\ \end{pmatrix}=\begin{pmatrix} 0 & 0 \\ 0 & 0 \\ \end{pmatrix}$ S0 that we can get $x=1,y=-2$ . 3.Let $A$ satisfying $A^2=A-I$ . Find $A^{15}$ . Please help to show me about this. Thank you in advance!",Let and . 1.Let and . Find the value of which satisfies . .So that we get 2.Let and .Find the values of x and y which satify . S0 that we can get . 3.Let satisfying . Find . Please help to show me about this. Thank you in advance!,"I=
\begin{pmatrix}
1 & 0 \\
0 & 1 \\
\end{pmatrix} O=\begin{pmatrix}
0 & 0 \\
0 & 0 \\
\end{pmatrix} A=\begin{pmatrix}
1 & 3 \\
3 & 5 \\
\end{pmatrix}  B=\begin{pmatrix}
x & 3 \\
3 & 6 \\ 
\end{pmatrix} x AB=BA AB=\begin{pmatrix}
1 & 3 \\
3 & 5 \\
\end{pmatrix}\cdot\begin{pmatrix}
x & 3 \\
3 & 6 \\
\end{pmatrix}=\begin{pmatrix}
x+9 & 21 \\
3x+15 & 39 \\
\end{pmatrix} BA=\begin{pmatrix}
x & 3 \\
3 & 6 \\
\end{pmatrix}\cdot\begin{pmatrix}
1 & 3 \\
3 & 5 \\
\end{pmatrix}=\begin{pmatrix}
x+9 & 3x+15 \\
21 & 39 \\
\end{pmatrix} 3x+15=21 \Rightarrow x=2 A=\begin{pmatrix}
1 & 2 \\
2 & 4 \\
\end{pmatrix}  B=\begin{pmatrix}
-2 & x \\
4 & y \\
\end{pmatrix} BA=O BA=\begin{pmatrix}
-2 & x \\
4 & y \\
\end{pmatrix}\cdot\begin{pmatrix}
1 & 2 \\
2 & 4 \\
\end{pmatrix}=\begin{pmatrix}
-2+2x & 0 \\
4+2y & 8+4y \\
\end{pmatrix}=\begin{pmatrix}
0 & 0 \\
0 & 0 \\
\end{pmatrix} x=1,y=-2 A A^2=A-I A^{15}",['matrices']
39,Is there a simple proof that a non-invertible matrix reduces to give a zero row?,Is there a simple proof that a non-invertible matrix reduces to give a zero row?,,"Let $A$ be a square matrix that is non-invertible. I was wondering if there is a simple proof that we can apply elementary row operations to get a zero row. (For a matrix $C$ to be invertible, I mean there is $B$ such that $CB = BC = I$ .) I can prove this using elementary column operations but I would like a more direct proof that doesn’t appeal to column operations or the fact that row rank equals column rank, or anything to do with transposes, or the existence of RREF, or determinants, etc. The difficulty seems to be that elementary row operations are applied on the row space, whereas invertibility is sort of defined in terms of the column space. You could also use (but it is preferred not to) facts like: A matrix $C$ being invertible is equivalent to null space of $C$ being zero (i.e. injective) is equivalent to $C$ being surjective.","Let be a square matrix that is non-invertible. I was wondering if there is a simple proof that we can apply elementary row operations to get a zero row. (For a matrix to be invertible, I mean there is such that .) I can prove this using elementary column operations but I would like a more direct proof that doesn’t appeal to column operations or the fact that row rank equals column rank, or anything to do with transposes, or the existence of RREF, or determinants, etc. The difficulty seems to be that elementary row operations are applied on the row space, whereas invertibility is sort of defined in terms of the column space. You could also use (but it is preferred not to) facts like: A matrix being invertible is equivalent to null space of being zero (i.e. injective) is equivalent to being surjective.",A C B CB = BC = I C C C,"['linear-algebra', 'matrices', 'gaussian-elimination']"
40,Is there any easy way to calculate the value of this determinant?,Is there any easy way to calculate the value of this determinant?,,\begin{vmatrix} 0 & 3 & 1 & 2 & 10! & e^{-7}\\  1 & 2 & -1 & 2 & \sqrt{2} & 2 \\  -1 & -2 & 3 & -3 & 1 & -\frac{1}{5} \\  -2 & -1 & 3 & 2 & -2 & -9 \\ 0 & 0 & 0 & 0 & 4 & 2 \\ 0 & 0 & 0 & 0 & 1 & 1 \\ \end{vmatrix} I still can't see any easy way to compute the determinant above I would appreciate any kind of help. Thanks.,I still can't see any easy way to compute the determinant above I would appreciate any kind of help. Thanks.,"\begin{vmatrix}
0 & 3 & 1 & 2 & 10! & e^{-7}\\ 
1 & 2 & -1 & 2 & \sqrt{2} & 2 \\ 
-1 & -2 & 3 & -3 & 1 & -\frac{1}{5} \\ 
-2 & -1 & 3 & 2 & -2 & -9 \\
0 & 0 & 0 & 0 & 4 & 2 \\
0 & 0 & 0 & 0 & 1 & 1 \\
\end{vmatrix}","['linear-algebra', 'matrices', 'determinant']"
41,Matrix of the linear transformation $T : \{0\} \to \{0\}$,Matrix of the linear transformation,T : \{0\} \to \{0\},"I am kindly asking for someone to clarify the following doubt. Is there a matrix for a linear transformation $T : \{0\} \to \{0\}$ ? I can generalize this question to the following. Is there a matrix for a linear transformation mapping the trivial vector space to another vector space (or vice-versa)? I believe there isn't since a matrix (as Axler defines it in the 3rd edition of Linear Algebra Done Right ) is defined for some positive integers $m$ and $n.$ Specifically, the integer $m$ specifies the dimension of the vector space in the domain, and the integer $n$ specifies the dimension of the codomain. As the dimension of the trivial vector space is $0,$ I believe there is no matrix for such linear transformation. Thank you.","I am kindly asking for someone to clarify the following doubt. Is there a matrix for a linear transformation ? I can generalize this question to the following. Is there a matrix for a linear transformation mapping the trivial vector space to another vector space (or vice-versa)? I believe there isn't since a matrix (as Axler defines it in the 3rd edition of Linear Algebra Done Right ) is defined for some positive integers and Specifically, the integer specifies the dimension of the vector space in the domain, and the integer specifies the dimension of the codomain. As the dimension of the trivial vector space is I believe there is no matrix for such linear transformation. Thank you.","T : \{0\} \to \{0\} m n. m n 0,","['linear-algebra', 'matrices', 'linear-transformations']"
42,Pseudo determinant of product of two square matrices,Pseudo determinant of product of two square matrices,,Let $A$ and $B$ be square symmetric matrices. $A$ is singular and $B$ is non-singular. Is there a way to decompose: $Det(AB)$ in terms of $Det(A)$ and $det(B)$ . $Det(.)$ refers to the pseudo determinant and $det(.)$ refers to the usual determinant of a square non-singular matrix.,Let and be square symmetric matrices. is singular and is non-singular. Is there a way to decompose: in terms of and . refers to the pseudo determinant and refers to the usual determinant of a square non-singular matrix.,A B A B Det(AB) Det(A) det(B) Det(.) det(.),"['linear-algebra', 'matrices', 'determinant']"
43,The matrix norm of the identity matrix disturbed by a small matrix,The matrix norm of the identity matrix disturbed by a small matrix,,"So I am consider the norm of matrix $\|I-C\|_2$ , where $C$ is a positive definite matrix with a very small norm, what can we say about $\|I-C\|_2$ ? Like, is it smaller than $1$ ? Or can I express it w.r.t $C$ ? Thank you in advance!","So I am consider the norm of matrix , where is a positive definite matrix with a very small norm, what can we say about ? Like, is it smaller than ? Or can I express it w.r.t ? Thank you in advance!",\|I-C\|_2 C \|I-C\|_2 1 C,"['linear-algebra', 'matrices', 'symmetric-matrices', 'matrix-norms', 'spectral-norm']"
44,Matrix of the differentiation operation [duplicate],Matrix of the differentiation operation [duplicate],,"This question already has an answer here : How do you write a differential operator as a matrix? (1 answer) Closed 4 years ago . Exercise: Find the matrix of the derivative operation $D$ related to the base $\{1, t, t^2,..., t^n\}$ $$D: \mathcal P_{n} \to \mathcal P_{n}$$ I found a possible solution to this exercise, given that $D(t^k)=kt^{k-1}$ $$ \begin{equation*} D_{n+1,n+1} =  \begin{pmatrix} 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 2 & \cdots & 0 \\ \vdots  & \vdots  & \vdots &\ddots & \vdots  \\ 0 & 0 & 0 &\cdots & n \\  0 & 0 & 0 &\cdots & 0 \\ \end{pmatrix} \end{equation*}$$ Nevertheless, it doesn't convince me at all, because when multiplying the matrix with the vectors in $\mathcal P_{n}$ , the exponent remains the same. Is this solution correct?","This question already has an answer here : How do you write a differential operator as a matrix? (1 answer) Closed 4 years ago . Exercise: Find the matrix of the derivative operation related to the base I found a possible solution to this exercise, given that Nevertheless, it doesn't convince me at all, because when multiplying the matrix with the vectors in , the exponent remains the same. Is this solution correct?","D \{1, t, t^2,..., t^n\} D: \mathcal P_{n} \to \mathcal P_{n} D(t^k)=kt^{k-1}  \begin{equation*}
D_{n+1,n+1} = 
\begin{pmatrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 2 & \cdots & 0 \\
\vdots  & \vdots  & \vdots &\ddots & \vdots  \\
0 & 0 & 0 &\cdots & n \\
 0 & 0 & 0 &\cdots & 0 \\
\end{pmatrix}
\end{equation*} \mathcal P_{n}","['linear-algebra', 'matrices', 'linear-transformations']"
45,Taylor expansion of a function of a symmetric matrix,Taylor expansion of a function of a symmetric matrix,,"First of all let me tell you that the answer to this question is likely to confirm a not-so-minor error in a very popular (and excellent) textbook on optimization, as you'll see below. Background Suppose that we have a real-valued function $f(X)$ whose domain is the set of $n\times n$ nonsingular symmetric matrices. Clearly, $X$ does not have $n^2$ independent variables; it has $n(n+1)/2$ independent variables as it's symmetric. As is well known, an important use of Taylor expansion is to find the derivative of a function by finding the optimal first-order approximation. That is, if one can find a matrix $D \in \mathbb{R}^{n\times n}$ that is a function of $X$ and satisfies $$f(X+V) = f(X) + \langle D, V \rangle + \text{h.o.t.},  $$ where $\text{h.o.t.}$ stands for higher-order terms and $\langle \cdot, \cdot \rangle$ is inner product, then the matrix $D$ is the derivative of $f$ w.r.t. $X$ . Question Now my question is: What is the right inner product $\langle \cdot, \cdot \rangle$ to use here if the matrix is symmetric? I know that if the entries of $X$ were independent (i.e., not symmetric), then the $\text{trace}$ operator would be the correct inner product. But I suspect that this is not true in general for a symmetric matrix. More specifically, my guess is that even if the $\text{trace}$ operator would lead to the correct expansion in the equation above, the $D$ matrix that comes as a result won't give the correct derivative. Here is why I think this is the case. A while ago, I asked a question about the derivative of the $\log\det X$ function, because I suspected that the formula in the book Convex Optimization of Boyd & Vandenberghe is wrong. The formula indeed seems to be wrong as the accepted answer made it clear. I tried to understand what went wrong in the proof in the Convex Optimization book. The approach that is used in the book is precisely the approach that I outlined above in Background. The authors show that the first-order Taylor approximation of $f(X)=\log\det X$ for symmetric $X$ is $$ f(X+V) \approx f(X)+\text{trace}(X^{-1}V). $$ The authors prove this approximation by using decomposition specific to symmetric matrices (proof in Appenix A.4.1; book is publicly available ). Now this approximation is correct but $X^{-1}$ is not the correct derivative of $\log\det X$ for symmetric $X$ ; the correct derivative is $2X^{-1}-\text{diag}(\text{diag}(X^{-1}))$ . Interestingly, the same approximation  in the formula above holds for nonsymmetric invertible matrices too (can be shown with SVD decomposition), and in this case it does give the right derivative because the derivative of $\log\det X$ is indeed $X^{-T}$ for a matrix with $n^2$ independent entries. Therefore I suspect that $\text{trace}$ is not the right inner product $\langle \cdot, \cdot \rangle$ for symmetric matrices, as it ignores the fact that the entries of $X$ are not independent. Can anyone shed light on this question? Added: A simpler question Based on a comment, I understand that the general answer to my question may be difficult, so let me ask a simpler question. The answer to this question may be sufficient to show what went wrong in the proof in the Convex Optimization book. Suppose $g(X)$ is a function $g: \mathbb{R}^{n\times n} \to \mathbb R$ . Is it true that the first-order Taylor approximaton with trace as inner product, i.e., $$g(X+V) \approx g(X) + \text{trace}\left( \nabla g (X)^T V \right), $$ implicitly assumes that the entries of $X$ are independent? In other words, is it true that this approximation may not hold if entries of $X$ are not independent (e.g., if $X$ is symmetric)?","First of all let me tell you that the answer to this question is likely to confirm a not-so-minor error in a very popular (and excellent) textbook on optimization, as you'll see below. Background Suppose that we have a real-valued function whose domain is the set of nonsingular symmetric matrices. Clearly, does not have independent variables; it has independent variables as it's symmetric. As is well known, an important use of Taylor expansion is to find the derivative of a function by finding the optimal first-order approximation. That is, if one can find a matrix that is a function of and satisfies where stands for higher-order terms and is inner product, then the matrix is the derivative of w.r.t. . Question Now my question is: What is the right inner product to use here if the matrix is symmetric? I know that if the entries of were independent (i.e., not symmetric), then the operator would be the correct inner product. But I suspect that this is not true in general for a symmetric matrix. More specifically, my guess is that even if the operator would lead to the correct expansion in the equation above, the matrix that comes as a result won't give the correct derivative. Here is why I think this is the case. A while ago, I asked a question about the derivative of the function, because I suspected that the formula in the book Convex Optimization of Boyd & Vandenberghe is wrong. The formula indeed seems to be wrong as the accepted answer made it clear. I tried to understand what went wrong in the proof in the Convex Optimization book. The approach that is used in the book is precisely the approach that I outlined above in Background. The authors show that the first-order Taylor approximation of for symmetric is The authors prove this approximation by using decomposition specific to symmetric matrices (proof in Appenix A.4.1; book is publicly available ). Now this approximation is correct but is not the correct derivative of for symmetric ; the correct derivative is . Interestingly, the same approximation  in the formula above holds for nonsymmetric invertible matrices too (can be shown with SVD decomposition), and in this case it does give the right derivative because the derivative of is indeed for a matrix with independent entries. Therefore I suspect that is not the right inner product for symmetric matrices, as it ignores the fact that the entries of are not independent. Can anyone shed light on this question? Added: A simpler question Based on a comment, I understand that the general answer to my question may be difficult, so let me ask a simpler question. The answer to this question may be sufficient to show what went wrong in the proof in the Convex Optimization book. Suppose is a function . Is it true that the first-order Taylor approximaton with trace as inner product, i.e., implicitly assumes that the entries of are independent? In other words, is it true that this approximation may not hold if entries of are not independent (e.g., if is symmetric)?","f(X) n\times n X n^2 n(n+1)/2 D \in \mathbb{R}^{n\times n} X f(X+V) = f(X) + \langle D, V \rangle + \text{h.o.t.},   \text{h.o.t.} \langle \cdot, \cdot \rangle D f X \langle \cdot, \cdot \rangle X \text{trace} \text{trace} D \log\det X f(X)=\log\det X X  f(X+V) \approx f(X)+\text{trace}(X^{-1}V).  X^{-1} \log\det X X 2X^{-1}-\text{diag}(\text{diag}(X^{-1})) \log\det X X^{-T} n^2 \text{trace} \langle \cdot, \cdot \rangle X g(X) g: \mathbb{R}^{n\times n} \to \mathbb R g(X+V) \approx g(X) + \text{trace}\left( \nabla g (X)^T V \right),  X X X","['linear-algebra', 'matrices', 'taylor-expansion', 'matrix-calculus']"
46,Is The Composition of Two Linear Transformations Invertible,Is The Composition of Two Linear Transformations Invertible,,"Assume we have two linear compositions $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ and $S: \mathbb{R}^n \rightarrow \mathbb{R}^n$ . How would I go about proving that $S \circ T$ ( Composition of $S$ and $T$ ) is an invertible linear transformation T(x1,x2)=(x1-x2,3x1-2x2) S(x1,x2)=(2x1+3x2,-x1+x2)","Assume we have two linear compositions and . How would I go about proving that ( Composition of and ) is an invertible linear transformation T(x1,x2)=(x1-x2,3x1-2x2) S(x1,x2)=(2x1+3x2,-x1+x2)",T: \mathbb{R}^n \rightarrow \mathbb{R}^n S: \mathbb{R}^n \rightarrow \mathbb{R}^n S \circ T S T,"['linear-algebra', 'matrices', 'linear-transformations']"
47,Is it possible to write a discretized differentiation operator as a Kronecker sum?,Is it possible to write a discretized differentiation operator as a Kronecker sum?,,"The following is a question that was posted and then deleted . Since I found the question interesting, I am posting it anew. I have this differential operator $$L=\begin{bmatrix} 0 & -\partial_x  \\ -\partial_x & 0  \end{bmatrix}$$ and I have to discretize (say with 2nd order finite differences). Let's call $A$ the matrix that discretize $\partial_x$ . Then, the discretization of $L$ will result in a block diagonal  matrix like $$\begin{bmatrix} \mathcal{O} & -A \\ -A & \mathcal{O}  \end{bmatrix}$$ Of course, I could diagonalize, but my question is: ""is it possible to write this as a Kronecker sum ?""  Something like $I \otimes A + A \otimes I$ ? The zeros along the diagonal in some sense don't look so promising, but I really don't know how to even disprove this.","The following is a question that was posted and then deleted . Since I found the question interesting, I am posting it anew. I have this differential operator and I have to discretize (say with 2nd order finite differences). Let's call the matrix that discretize . Then, the discretization of will result in a block diagonal  matrix like Of course, I could diagonalize, but my question is: ""is it possible to write this as a Kronecker sum ?""  Something like ? The zeros along the diagonal in some sense don't look so promising, but I really don't know how to even disprove this.","L=\begin{bmatrix}
0 & -\partial_x  \\
-\partial_x & 0 
\end{bmatrix} A \partial_x L \begin{bmatrix}
\mathcal{O} & -A \\
-A & \mathcal{O} 
\end{bmatrix} I \otimes A + A \otimes I","['real-analysis', 'linear-algebra', 'matrices', 'ordinary-differential-equations', 'partial-differential-equations']"
48,Is matrix notation with dots acceptable for papers? What are some alternatives?,Is matrix notation with dots acceptable for papers? What are some alternatives?,,"I am very used to this kind notation when dealing with matrices with arbitrary dimensions: $$ X_P^T\Lambda X_P= \begin{bmatrix} 	x_1 & x_2, &\dots, &x_n\\ \end{bmatrix} \begin{bmatrix} \lambda_1 & 0 & \dots & 0\\ 0 & \lambda_2 & \dots & 0\\ \vdots & \vdots &\ddots \\ 0 & 0 & \dots & \lambda_n\\ \end{bmatrix} \begin{bmatrix} 	x_1 \\  	x_2 \\  	\vdots\\ 	x_n\\ \end{bmatrix} $$ I like it because it's nice and directly conveys the matrix representation. However, someone mentioned to me that these representations are good for notes and teaching materials but not for publications. In this specific example the matrix is a diagonal matrix, but I am asking more in general, if you have to convey an arbitrary matrix with any dimensions and any entries, how do you convey the pattern in the matrix without doing dot notation?","I am very used to this kind notation when dealing with matrices with arbitrary dimensions: I like it because it's nice and directly conveys the matrix representation. However, someone mentioned to me that these representations are good for notes and teaching materials but not for publications. In this specific example the matrix is a diagonal matrix, but I am asking more in general, if you have to convey an arbitrary matrix with any dimensions and any entries, how do you convey the pattern in the matrix without doing dot notation?","
X_P^T\Lambda X_P=
\begin{bmatrix}
	x_1 & x_2, &\dots, &x_n\\
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0\\
0 & \lambda_2 & \dots & 0\\
\vdots & \vdots &\ddots \\
0 & 0 & \dots & \lambda_n\\
\end{bmatrix}
\begin{bmatrix}
	x_1 \\ 
	x_2 \\ 
	\vdots\\
	x_n\\
\end{bmatrix}
","['linear-algebra', 'matrices', 'notation']"
49,"If the matrix rings over two Rings of the same size are isomorphic, then the scalar rings are isomorphic","If the matrix rings over two Rings of the same size are isomorphic, then the scalar rings are isomorphic",,"Let $R$ and $R'$ be rings (with 1 but no further assumptions) and $n \in \mathbb{N}$ . Does the following implication hold? If $M_n(R) \simeq M_n(R')$ then $R \simeq R'$ . If the rings are commutative then it follows from considering the centers of $M_n(R)$ and $M_n(R')$ . If the rings are division it also holds and even stronger, it holds even if the matrices are not of the same size. I thought about considering the embeddings of $R$ in $M_n(R)$ as diagonal matrices, but since I do not want to asume anything about the isomorphism I was not able to conclude that these subrings of matrices in $M_n(R)$ and $M_n(R')$ are isomorphic. I hope someone can help me or with a proof, an idea or a counterexample. Thank you.","Let and be rings (with 1 but no further assumptions) and . Does the following implication hold? If then . If the rings are commutative then it follows from considering the centers of and . If the rings are division it also holds and even stronger, it holds even if the matrices are not of the same size. I thought about considering the embeddings of in as diagonal matrices, but since I do not want to asume anything about the isomorphism I was not able to conclude that these subrings of matrices in and are isomorphic. I hope someone can help me or with a proof, an idea or a counterexample. Thank you.",R R' n \in \mathbb{N} M_n(R) \simeq M_n(R') R \simeq R' M_n(R) M_n(R') R M_n(R) M_n(R) M_n(R'),"['abstract-algebra', 'matrices', 'ring-theory']"
50,Does $B^{-1}-A^{-1}$ has r positive eigenvalues when A-B has r positive eigenvalues?,Does  has r positive eigenvalues when A-B has r positive eigenvalues?,B^{-1}-A^{-1},"Assume that A and B are 2 positive definite matrices of $n\times n$ . As is known, $A-B>0$ implies $B^{-1}-A^{-1} >0$ . That is to say, $B^{-1}-A^{-1}$ has n positive eigenvalues  when A-B has n positive eigenvalues. My question is :Assume that A and B are 2 symmetric matrices of $n\times n$ and B is positive definite and A is invertible.  Does $B^{-1}-A^{-1}$ has r positive eigenvalues  when A-B has r positive eigenvalues ?( $r\in \{1,2,\ldots,n\}$ )","Assume that A and B are 2 positive definite matrices of . As is known, implies . That is to say, has n positive eigenvalues  when A-B has n positive eigenvalues. My question is :Assume that A and B are 2 symmetric matrices of and B is positive definite and A is invertible.  Does has r positive eigenvalues  when A-B has r positive eigenvalues ?( )","n\times n A-B>0 B^{-1}-A^{-1} >0 B^{-1}-A^{-1} n\times n B^{-1}-A^{-1} r\in \{1,2,\ldots,n\}","['linear-algebra', 'matrices']"
51,$X^TX$ not full rank when $X$ is full rank?,not full rank when  is full rank?,X^TX X,"$X$ is an $n \times m$ matrix, where $n \geq m$ and $\mbox{rank}(X) = m$ . Is it possible for $X^TX$ to not be full rank? If $X$ can only be square I could easily prove this with $$ \det(X^TX) = \det(X^T)\det(X) = \det(X)^2 > 0 $$","is an matrix, where and . Is it possible for to not be full rank? If can only be square I could easily prove this with","X n \times m n \geq m \mbox{rank}(X) = m X^TX X 
\det(X^TX) = \det(X^T)\det(X) = \det(X)^2 > 0
","['linear-algebra', 'matrices', 'matrix-rank']"
52,"If a matrix is positive-semidefinite, is Hermitian, and has a trace of 1, is it idempotent?","If a matrix is positive-semidefinite, is Hermitian, and has a trace of 1, is it idempotent?",,"I have a matrix $A^{n\times n}$ that is positive semi-def. and Hermitian. Also, $Tr(A) = 1$ . I want to show that $A$ is idempotent. Are these properties enough? If so, how would one show this? If not, what else might I need? Thanks!","I have a matrix that is positive semi-def. and Hermitian. Also, . I want to show that is idempotent. Are these properties enough? If so, how would one show this? If not, what else might I need? Thanks!",A^{n\times n} Tr(A) = 1 A,"['linear-algebra', 'matrices', 'positive-semidefinite', 'idempotents', 'hermitian-matrices']"
53,Orbit of a vector under the action of a $2\times 2$ matrix,Orbit of a vector under the action of a  matrix,2\times 2,"Consider the following matrix $A=\pmatrix{0 &1\\ -1&\frac{1}{3}}$ and put $e_1:=(1,0)^t$ . How can I find the elements of the orbit $A^ne_1$ , for $n\in\mathbb N$ ? It seems to me that $A$ has infinite order in $SL_2(\mathbb R)$ and moreover I don't see any closed formula for the powers of $A$ .","Consider the following matrix and put . How can I find the elements of the orbit , for ? It seems to me that has infinite order in and moreover I don't see any closed formula for the powers of .","A=\pmatrix{0 &1\\
-1&\frac{1}{3}} e_1:=(1,0)^t A^ne_1 n\in\mathbb N A SL_2(\mathbb R) A","['linear-algebra', 'matrices', 'geometry']"
54,Are these two notions of convolution the same somehow?,Are these two notions of convolution the same somehow?,,"Given a (locally finite) poset $(P,\leq)$ we can work with its incidence algebra , which is the $\mathbb{C}$ -algebra with a basis element for each interval $[x,y] = \{z ~|~ x \leq z \leq y\}$ . The multiplication is given by ""convolution"", where $$ (\alpha \ast \beta)([x,y]) = \sum_{z \in [x,y]} \alpha([x,z]) \beta([z,y]) $$ Notice the incidence algebra is really a matrix algebra in disguise. If we look at matrices with rows and columns indexed by $P$ , with usual matrix multiplication, then the subalgebra of matrices $A$ satisfying $A_{xy} = 0$ whenever $x \not \leq y$ is exactly the incidence algebra. So ""convolution"" is really matrix multiplication. The other place one commonly sees convolution is on functions. Here we have two complex measurable functions $f$ and $g$ and we define $$ (f \ast g)(x) = \int f(x) g(s-x) d \mu(s) $$ In the case of a discrete measure, this becomes $$ (f \ast g)(n) = \sum_m f(n) g(m-n) $$ Now this, at least initially, doesn't look very much like matrix multiplication. We can give it the right number of variables by working with a 2-dimensional convolution : $$ (f \ast \ast g)(x,y) = \sum_m \sum_n f(x,y) g(m-x,n-y) $$ This still doesn't look much like convolution of poset algebras, though. There is a way to compute a convolution by working with matrices, using a Toeplitz matrix , but it doesn't seem to line up with the question I'm asking, and completely saturates the google results for anything to do with convolution and matrix multiplication. Is there a way to see convolution in an incidence algebra and convolution of functions as the same   thing? If not, why are they named this way? They don't even seem superficially similar to me. Thanks in advance!","Given a (locally finite) poset we can work with its incidence algebra , which is the -algebra with a basis element for each interval . The multiplication is given by ""convolution"", where Notice the incidence algebra is really a matrix algebra in disguise. If we look at matrices with rows and columns indexed by , with usual matrix multiplication, then the subalgebra of matrices satisfying whenever is exactly the incidence algebra. So ""convolution"" is really matrix multiplication. The other place one commonly sees convolution is on functions. Here we have two complex measurable functions and and we define In the case of a discrete measure, this becomes Now this, at least initially, doesn't look very much like matrix multiplication. We can give it the right number of variables by working with a 2-dimensional convolution : This still doesn't look much like convolution of poset algebras, though. There is a way to compute a convolution by working with matrices, using a Toeplitz matrix , but it doesn't seem to line up with the question I'm asking, and completely saturates the google results for anything to do with convolution and matrix multiplication. Is there a way to see convolution in an incidence algebra and convolution of functions as the same   thing? If not, why are they named this way? They don't even seem superficially similar to me. Thanks in advance!","(P,\leq) \mathbb{C} [x,y] = \{z ~|~ x \leq z \leq y\} 
(\alpha \ast \beta)([x,y]) = \sum_{z \in [x,y]} \alpha([x,z]) \beta([z,y])
 P A A_{xy} = 0 x \not \leq y f g 
(f \ast g)(x) = \int f(x) g(s-x) d \mu(s)
 
(f \ast g)(n) = \sum_m f(n) g(m-n)
 
(f \ast \ast g)(x,y) = \sum_m \sum_n f(x,y) g(m-x,n-y)
","['matrices', 'soft-question', 'order-theory', 'convolution']"
55,Completing the square in matrix form,Completing the square in matrix form,,"I am having trouble understanding how to complete the square in matrix form. I can't find any source online for a clear, final equation for that. However, I would also like to grasp the intuition behind it.  Thanks. To give an example: $$ x'Mx-2b'x=(x−M^{−1}b)′M(x−M^{−1}b)−b′M^{−1}b $$ How do I get that solution?","I am having trouble understanding how to complete the square in matrix form. I can't find any source online for a clear, final equation for that. However, I would also like to grasp the intuition behind it.  Thanks. To give an example: How do I get that solution?", x'Mx-2b'x=(x−M^{−1}b)′M(x−M^{−1}b)−b′M^{−1}b ,"['linear-algebra', 'matrices']"
56,Is the set of symmetric positive semi-definite matrices a smooth manifold with boundary,Is the set of symmetric positive semi-definite matrices a smooth manifold with boundary,,"Let $P_n(\mathbb{R}):=\left\{ A \in \operatorname{Mat}_{n\times n}(\mathbb{R}):\, A=A^T \mbox{ and }  (\forall i)\, \lambda_i^A \geq 0 \right\}$ , where $\left\{\lambda_i^A\right\}_i$ are the eigenvalues of $A$ .  Is this a manifold with boundary which can be decomposed as: $$ P_n(\mathbb{R}) = P_n^+(\mathbb{R}) \cup Sym_n^0(\mathbb{R}), $$ where $Sym_n^0(\mathbb{R})$ is the set of symmetric $n\times n$ matrices satisfying $x^TAx=0$ for some non-zero vector and $P_n^+(\mathbb{R})$ is the set of symmetric positive-definite matrices with real entries? I know that $P_n^+(\mathbb{R})$ is a smooth manifold since I can explicitly write down its global chart using the matrix exponential map and (a slight variant of) the vectorization operation .  However, I'm not sure if $Sym_n^0(\mathbb{R})$ is of dimension $\frac{n(n+1)}{2} -1$ .","Let , where are the eigenvalues of .  Is this a manifold with boundary which can be decomposed as: where is the set of symmetric matrices satisfying for some non-zero vector and is the set of symmetric positive-definite matrices with real entries? I know that is a smooth manifold since I can explicitly write down its global chart using the matrix exponential map and (a slight variant of) the vectorization operation .  However, I'm not sure if is of dimension .","P_n(\mathbb{R}):=\left\{
A \in \operatorname{Mat}_{n\times n}(\mathbb{R}):\,
A=A^T \mbox{ and }  (\forall i)\,
\lambda_i^A \geq 0
\right\} \left\{\lambda_i^A\right\}_i A 
P_n(\mathbb{R}) = P_n^+(\mathbb{R}) \cup Sym_n^0(\mathbb{R}),
 Sym_n^0(\mathbb{R}) n\times n x^TAx=0 P_n^+(\mathbb{R}) P_n^+(\mathbb{R}) Sym_n^0(\mathbb{R}) \frac{n(n+1)}{2} -1","['matrices', 'differential-geometry', 'manifolds', 'geometric-topology', 'manifolds-with-boundary']"
57,Characterization of Graph Laplacians,Characterization of Graph Laplacians,,"It is known that every graph Laplacian (of a simple undirected graph) is a positive semi-definite matrix.  However, is every positive definite matrix a graph Laplacian?","It is known that every graph Laplacian (of a simple undirected graph) is a positive semi-definite matrix.  However, is every positive definite matrix a graph Laplacian?",,"['matrices', 'graph-theory']"
58,Checking if given matrix is perfect square of another matrix with real entries,Checking if given matrix is perfect square of another matrix with real entries,,"The question is from JEE Advanced(2017), where it asks to identify matrices which are the square of a matrix with real entries: I first found out the determinant of all the matrices. Options (A & B) both have negative determinant value and hence can't be expressed as square of a matrix with real entries. For explanation let any of the two be called as $A$ . Given they are square of another matrix(let $B$ ) so $B^2=A$ . Taking the determinant on both sides, I get $|B|^2=|A|$ and since $|A|$ is negative, I get $|B|^2<0$ . So $B$ can't have all real entries. Option C is $I$ whose square is $I$ or vice versa. I am having trouble with option D . Since it's determinant is also positive and I can't find a simple matrix which when squared gives that option. I have talked to my teacher . He said there is a method to find square root of a matrix but that's far beyond our level. I am a High school student studying in grade 12. So please, if possible, give a simplified hint/answer. Thanks in advance!","The question is from JEE Advanced(2017), where it asks to identify matrices which are the square of a matrix with real entries: I first found out the determinant of all the matrices. Options (A & B) both have negative determinant value and hence can't be expressed as square of a matrix with real entries. For explanation let any of the two be called as . Given they are square of another matrix(let ) so . Taking the determinant on both sides, I get and since is negative, I get . So can't have all real entries. Option C is whose square is or vice versa. I am having trouble with option D . Since it's determinant is also positive and I can't find a simple matrix which when squared gives that option. I have talked to my teacher . He said there is a method to find square root of a matrix but that's far beyond our level. I am a High school student studying in grade 12. So please, if possible, give a simplified hint/answer. Thanks in advance!",A B B^2=A |B|^2=|A| |A| |B|^2<0 B I I,"['matrices', 'contest-math', 'determinant']"
59,Is there a linear mapping $L$ that is not a scalar multiple of the identity?,Is there a linear mapping  that is not a scalar multiple of the identity?,L,"Let $V$ be an $n$ -dimensional vector space. Can we find a linear map $A : V\to V$ with $n+1$ eigenvectors, any $n$ of which are linearly independent, which is not a scalar multiple of the identity? Here's a solution I found. The answer is no. Let the $n+1$ corresponding eigenvalues have sum $k$ . Given any eigenvector with eigenvalue $\lambda,$ the remaining $n$ eigenvectors are linearly independent, so they form a basis. The basis formed by the remaining $n$ eigenvectors diagonalizes the matrix of the linear transformation. The trace of the resulting matrix is $k-\lambda$ . Since trace is independent of the choice of basis, all eigenvalues are equal to lambda. Hence $A$ is a scalar matrix (i.e. a scalar multiple of the identity). I still can't convince myself that $A$ is a scalar matrix. What if $A$ is a matrix such that it that satisfies the given conditions and before it is diagonalized by the basis, it is not a scalar multiple of the identity?","Let be an -dimensional vector space. Can we find a linear map with eigenvectors, any of which are linearly independent, which is not a scalar multiple of the identity? Here's a solution I found. The answer is no. Let the corresponding eigenvalues have sum . Given any eigenvector with eigenvalue the remaining eigenvectors are linearly independent, so they form a basis. The basis formed by the remaining eigenvectors diagonalizes the matrix of the linear transformation. The trace of the resulting matrix is . Since trace is independent of the choice of basis, all eigenvalues are equal to lambda. Hence is a scalar matrix (i.e. a scalar multiple of the identity). I still can't convince myself that is a scalar matrix. What if is a matrix such that it that satisfies the given conditions and before it is diagonalized by the basis, it is not a scalar multiple of the identity?","V n A : V\to V n+1 n n+1 k \lambda, n n k-\lambda A A A",['linear-algebra']
60,Determinant of the $n$-th order calculated with a combination of methods-verification,Determinant of the -th order calculated with a combination of methods-verification,n,"$A\in M_n(\mathbb F)$ $$\det{A}=\begin{vmatrix} -1  &\;1&\;1&...&\;1&\;1&\;1 \\ -2 &-1&\;0&\ldots&\;0&\;0&\;1\\ -2&\;0&-1&...&\;0&\;0&\;1\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\-2&0&\;0&\ldots&-1&\;0&\;1\\-2&\;0&\;0&\ldots&\;0&-1&\;1\\-2&-2&-2&\ldots&-2&-2&-1  \end{vmatrix}=?$$ My work: I subtracted $\text{the last ($n$-th) row}$ so as to free the terrain for the LaPlace transform to the $\text{first column}$ . I got: $$\begin{vmatrix} -1 &\;1&\;1&\ldots&\;1&\;1&\;1 \\ 0 &1&\;2&\ldots&\;2&\;2&\;2\\ 0&\;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\0&2&\;2&\ldots&\;1&\;2&\;2\\\;0&\;2&\;2&\ldots&\;2&\;1&\;2\\-2&-2&-2&\ldots&-2&-2&-1 \end{vmatrix}$$ Then I have two sumands: $$-1\cdot\begin{vmatrix} \;1&\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\\-2&-2&\ldots&-2&-2&-1 \end{vmatrix}+(-1)^{n+1}\cdot(-2)\cdot\begin{vmatrix} \;1&\;1&\ldots&\;1&\;1&\;1 \\ 1&\;\;2&\ldots&\;2&\;2&\;2\\ \;2&1&...&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\end{vmatrix}$$ $$=\begin{vmatrix} \;1&\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\\\;2&\;2&\ldots&\;2&\;2&\;1 \end{vmatrix}+2\cdot(-1)^{n}\cdot\begin{vmatrix} \;1&\;1&\ldots&\;1&\;1&\;1 \\ 1&\;\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\end{vmatrix}$$ I applied a formula derived earlier to the first sumand (where, instead of 1's on the main diagonal, there are parameters $a_k$ and x, whenever $i\ne j$ - under and above the diagonal): It looked like this: $$\color{blue}{\begin{vmatrix} \;a_1&\;x&\ldots&\;x&\;x&\;x\\ \;x&a_2&\ldots&\;x&\;x&\;x\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;x&\;x&\ldots&\;a_{n-2}&\;x&\;x\\\;x&\;x&\ldots&\;x&\;a_{n-1}&\;x\\\;x&\;x&\ldots&\;x&\;x&\;a_n\end{vmatrix} }$$ After subtracting the $\text{first row}$ from the rest of them: $$\color{blue}{\begin{vmatrix}\;a_1&\;x&\ldots&\;x&\;x&\;x\\ \;x-a_1&a_2-x&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;x-a_1&\;0&\ldots&\;a_{n-2}-x&\;0&\;0\\\;x-a_1&\;0&\ldots&\;0&\;a_{n-1}-x&\;0\\\;x-a_1&\;0&...&\;0&\;0&\;a_n-x\end{vmatrix}}$$ After knocking out the factor $a_j-x$ from every column: $$\color{blue}{\prod_{j=1}^{n} (a_j-x)\cdot\begin{vmatrix} \;\frac{a_1}{a_1-x}&\;\frac{x}{a_2-x}&\ldots&\;\frac{x}{a_{n-2}-x}&\;\frac{x}{a_{n-1}-x}&\;\frac{x}{a_n-x}\\ -1&1&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\-1&\;0&\ldots&\;1&\;0&\;0\\-1&\;0&\ldots&\;0&\;1&\;0\\-1&\;0&\ldots&\;0&\;0&\;1\end{vmatrix}}$$ After adding each column to the $\text{first}$ column we get the element: $$\frac{a_1}{a_1-x}+\sum_{j=2}^{n}\frac{x}{a_j-x}=\frac{a_1-x}{a_1-x} +\frac{x}{a_1-x}+x\sum_{j=2}^{n}\frac{1}{a_j-x}=1+x\sum_{j=1}^{n}\frac{1}{a_j-x}$$ on the position $1,1$ and $I_{n-1}$ inside the matrix. $$\color{blue}{\implies\det{X}=\prod_{j=1}^{n} (a_j-x)\;\cdot\;\left(1+x\sum_{j=1}^{n}\frac{1}{a_j-x}\right)}$$ In the task above, when I plugged $1,2$ and $(n-1)$ into the formula I got (for the first sumand): $$\prod_{k=1}^{n-1}(-1)\;\cdot\;(1-2(n-1))=(-1)^{n-1}(3-2n)$$ The second summand was the result of a transformation into $\text{lower triangular matrix}$ after subtracting each column from the next one: $$\begin{vmatrix} \;1&\;0&\ldots&\;0&\;0&\;0 \\ 1&\;\;1&\ldots&\;0&\;0&\;0\\ \;2&-1&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;0&\ldots&-1&\;1&\;0\\\;2&\;0&\ldots&\;0&-1&1\end{vmatrix}$$ The $\text{product of the diagonal}$ is $1$ . My final answer is (thanks to users in comments who noticed the arithmetic mistakes): $$\det A=(-1)^{n-1}(3-2n)+2\cdot(-1)^n=(2n-3)\cdot(-1)^n+2\cdot(-1)^n$$ $$\det A=(-1)^n(2n-3+2)=(-1)^n(2n-1)$$","My work: I subtracted so as to free the terrain for the LaPlace transform to the . I got: Then I have two sumands: I applied a formula derived earlier to the first sumand (where, instead of 1's on the main diagonal, there are parameters and x, whenever - under and above the diagonal): It looked like this: After subtracting the from the rest of them: After knocking out the factor from every column: After adding each column to the column we get the element: on the position and inside the matrix. In the task above, when I plugged and into the formula I got (for the first sumand): The second summand was the result of a transformation into after subtracting each column from the next one: The is . My final answer is (thanks to users in comments who noticed the arithmetic mistakes):","A\in M_n(\mathbb F) \det{A}=\begin{vmatrix} -1
 &\;1&\;1&...&\;1&\;1&\;1 \\ -2 &-1&\;0&\ldots&\;0&\;0&\;1\\
-2&\;0&-1&...&\;0&\;0&\;1\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\-2&0&\;0&\ldots&-1&\;0&\;1\\-2&\;0&\;0&\ldots&\;0&-1&\;1\\-2&-2&-2&\ldots&-2&-2&-1
 \end{vmatrix}=? \text{the last (n-th) row} \text{first column} \begin{vmatrix} -1 &\;1&\;1&\ldots&\;1&\;1&\;1 \\ 0 &1&\;2&\ldots&\;2&\;2&\;2\\ 0&\;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\0&2&\;2&\ldots&\;1&\;2&\;2\\\;0&\;2&\;2&\ldots&\;2&\;1&\;2\\-2&-2&-2&\ldots&-2&-2&-1 \end{vmatrix} -1\cdot\begin{vmatrix} \;1&\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\\-2&-2&\ldots&-2&-2&-1 \end{vmatrix}+(-1)^{n+1}\cdot(-2)\cdot\begin{vmatrix} \;1&\;1&\ldots&\;1&\;1&\;1 \\ 1&\;\;2&\ldots&\;2&\;2&\;2\\ \;2&1&...&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\end{vmatrix} =\begin{vmatrix} \;1&\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\\\;2&\;2&\ldots&\;2&\;2&\;1 \end{vmatrix}+2\cdot(-1)^{n}\cdot\begin{vmatrix} \;1&\;1&\ldots&\;1&\;1&\;1 \\ 1&\;\;2&\ldots&\;2&\;2&\;2\\ \;2&1&\ldots&\;2&\;2&\;2\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;2&\ldots&\;1&\;2&\;2\\\;2&\;2&\ldots&\;2&\;1&\;2\end{vmatrix} a_k i\ne j \color{blue}{\begin{vmatrix} \;a_1&\;x&\ldots&\;x&\;x&\;x\\ \;x&a_2&\ldots&\;x&\;x&\;x\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;x&\;x&\ldots&\;a_{n-2}&\;x&\;x\\\;x&\;x&\ldots&\;x&\;a_{n-1}&\;x\\\;x&\;x&\ldots&\;x&\;x&\;a_n\end{vmatrix} } \text{first row} \color{blue}{\begin{vmatrix}\;a_1&\;x&\ldots&\;x&\;x&\;x\\ \;x-a_1&a_2-x&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;x-a_1&\;0&\ldots&\;a_{n-2}-x&\;0&\;0\\\;x-a_1&\;0&\ldots&\;0&\;a_{n-1}-x&\;0\\\;x-a_1&\;0&...&\;0&\;0&\;a_n-x\end{vmatrix}} a_j-x \color{blue}{\prod_{j=1}^{n} (a_j-x)\cdot\begin{vmatrix} \;\frac{a_1}{a_1-x}&\;\frac{x}{a_2-x}&\ldots&\;\frac{x}{a_{n-2}-x}&\;\frac{x}{a_{n-1}-x}&\;\frac{x}{a_n-x}\\ -1&1&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\-1&\;0&\ldots&\;1&\;0&\;0\\-1&\;0&\ldots&\;0&\;1&\;0\\-1&\;0&\ldots&\;0&\;0&\;1\end{vmatrix}} \text{first} \frac{a_1}{a_1-x}+\sum_{j=2}^{n}\frac{x}{a_j-x}=\frac{a_1-x}{a_1-x} +\frac{x}{a_1-x}+x\sum_{j=2}^{n}\frac{1}{a_j-x}=1+x\sum_{j=1}^{n}\frac{1}{a_j-x} 1,1 I_{n-1} \color{blue}{\implies\det{X}=\prod_{j=1}^{n} (a_j-x)\;\cdot\;\left(1+x\sum_{j=1}^{n}\frac{1}{a_j-x}\right)} 1,2 (n-1) \prod_{k=1}^{n-1}(-1)\;\cdot\;(1-2(n-1))=(-1)^{n-1}(3-2n) \text{lower triangular matrix} \begin{vmatrix} \;1&\;0&\ldots&\;0&\;0&\;0 \\ 1&\;\;1&\ldots&\;0&\;0&\;0\\ \;2&-1&\ldots&\;0&\;0&\;0\\\vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\\;2&\;0&\ldots&-1&\;1&\;0\\\;2&\;0&\ldots&\;0&-1&1\end{vmatrix} \text{product of the diagonal} 1 \det A=(-1)^{n-1}(3-2n)+2\cdot(-1)^n=(2n-3)\cdot(-1)^n+2\cdot(-1)^n \det A=(-1)^n(2n-3+2)=(-1)^n(2n-1)","['linear-algebra', 'matrices', 'determinant', 'matrix-calculus']"
61,"Prove that for local rings $R,S$, $M_m(R)\cong M_n(S)\implies R\cong S$ and $m=n$","Prove that for local rings ,  and","R,S M_m(R)\cong M_n(S)\implies R\cong S m=n","Prove that for local rings (not necessarily commutative, and local means the nonunits form a two-sided ideal) $R,S$ , $M_m(R)\cong M_n(S)\implies R\cong S$ and $m=n$ . My first attempt was to consider the natural embeddings $R\to M_m(R),\ S\to M_n(S)$ that sends, e.g. $r\in R$ to the scalar matrix $rI$ and $s\in S$ to $sI$ , and then maybe restrict the isomorphism $M_m(R)\cong M_n(S)$ to the scalar matrices. The problem is I can't prove that this isomorphism maps scalar matrices to scalar matrices. I am sensing there is something wrong with my attempt. After all, I didn't even use $R,S$ being local. Am I on the right track? How should I use the assumption that they are local rings? Related: The same question without assuming $R,S$ local. Commutative case","Prove that for local rings (not necessarily commutative, and local means the nonunits form a two-sided ideal) , and . My first attempt was to consider the natural embeddings that sends, e.g. to the scalar matrix and to , and then maybe restrict the isomorphism to the scalar matrices. The problem is I can't prove that this isomorphism maps scalar matrices to scalar matrices. I am sensing there is something wrong with my attempt. After all, I didn't even use being local. Am I on the right track? How should I use the assumption that they are local rings? Related: The same question without assuming local. Commutative case","R,S M_m(R)\cong M_n(S)\implies R\cong S m=n R\to M_m(R),\ S\to M_n(S) r\in R rI s\in S sI M_m(R)\cong M_n(S) R,S R,S","['abstract-algebra', 'matrices', 'noncommutative-algebra', 'local-rings']"
62,Simple formula for determinant of circulant matrix built on an arithmetic sequence.,Simple formula for determinant of circulant matrix built on an arithmetic sequence.,,"Let $a$ be an arithmetic sequence: $$a_i=a_1+\lambda(i-1),\tag1$$ and consider a $n\times n$ circulant matrix $M_{n}(a)$ ""built"" on rotational shifts of the sequence $a$ , i.e. with elements: $$M_{ij}=a_{1+(j−i)\operatorname{mod}n}.\tag2$$ Prove: $$ \det M_n(a)=\frac{a_1+a_n}2(-n\lambda)^{(n-1)}.\tag3 $$ An example: $$ \det\begin{pmatrix} 1&2&3&4&5\\ 5&1&2&3&4\\ 4&5&1&2&3\\ 3&4&5&1&2\\ 2&3&4&5&1\\ \end{pmatrix}=\frac{1+5}2(-5)^4=1875 $$","Let be an arithmetic sequence: and consider a circulant matrix ""built"" on rotational shifts of the sequence , i.e. with elements: Prove: An example:","a a_i=a_1+\lambda(i-1),\tag1 n\times n M_{n}(a) a M_{ij}=a_{1+(j−i)\operatorname{mod}n}.\tag2 
\det M_n(a)=\frac{a_1+a_n}2(-n\lambda)^{(n-1)}.\tag3
 
\det\begin{pmatrix}
1&2&3&4&5\\
5&1&2&3&4\\
4&5&1&2&3\\
3&4&5&1&2\\
2&3&4&5&1\\
\end{pmatrix}=\frac{1+5}2(-5)^4=1875
","['linear-algebra', 'matrices', 'determinant', 'arithmetic-progressions', 'circulant-matrices']"
63,What are all the possibilities of $A$ s.t. $\det(A)=k$?,What are all the possibilities of  s.t. ?,A \det(A)=k,"Suppose we have $A \in M_3(\Bbb N\cup\{0\})$ s.t. sum of the elements of each row is $k $ for some fixed $k\in \Bbb N\cup\{0\}$ . What are all the possibilities of $A$ s.t. $\det(A)=k$ ? We can start from $k=0$ here we have to have the matrix to be zero. For $k=2$ , I am getting $$     \begin{pmatrix}     1 & 1 & 0 \\     0 & 1 & 1 \\     1 & 0 & 1 \\     \end{pmatrix} $$ as one such matrix then what are the other possibilities and can you give me a general question. Then what about $k=1,3$ And so on for any $k$ can we give a general structure? Then this question can be extended to $A\in M_4(\Bbb N \cup{0})$ . By the way, what I want is if someone can give me some of the partial answers as the general answer might be too strong to expect!","Suppose we have s.t. sum of the elements of each row is for some fixed . What are all the possibilities of s.t. ? We can start from here we have to have the matrix to be zero. For , I am getting as one such matrix then what are the other possibilities and can you give me a general question. Then what about And so on for any can we give a general structure? Then this question can be extended to . By the way, what I want is if someone can give me some of the partial answers as the general answer might be too strong to expect!","A \in M_3(\Bbb N\cup\{0\}) k  k\in \Bbb N\cup\{0\} A \det(A)=k k=0 k=2 
    \begin{pmatrix}
    1 & 1 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    \end{pmatrix}
 k=1,3 k A\in M_4(\Bbb N \cup{0})","['combinatorics', 'matrices', 'discrete-mathematics', 'contest-math', 'determinant']"
64,"Let $v,w \in R^n$ with $||u||=||w||=1$. Prove that there exists orthogonal matrix $A$ such that $Av=w$.",Let  with . Prove that there exists orthogonal matrix  such that .,"v,w \in R^n ||u||=||w||=1 A Av=w","Let $v,w \in \mathbb R^n$ with $\|u\|=\|w\|=1$ . Prove that there exists   orthogonal matrix $A$ such that $Av=w$ . Also prove that $A$ can be   chosen such that $\det(A)=1$ Here, $u\neq 0$ and $w\neq 0$ so I can extend these to get $B_1 = \{u,x2,x3\dots x_n\}$ and $B_2=\{w,y2\dots y_n\}$ both orthonormal basis of $R^n$ and define $T:R^n \to R^n $ as $T(u)=w$ and $T(x_i)=y_i \;\; \forall i\leq i \leq n$ here will $A=[T]_{B_1}^{B2}$ satisfy the given conditions? I can't think of another way to solve this. please help","Let with . Prove that there exists   orthogonal matrix such that . Also prove that can be   chosen such that Here, and so I can extend these to get and both orthonormal basis of and define as and here will satisfy the given conditions? I can't think of another way to solve this. please help","v,w \in \mathbb R^n \|u\|=\|w\|=1 A Av=w A \det(A)=1 u\neq 0 w\neq 0 B_1 = \{u,x2,x3\dots x_n\} B_2=\{w,y2\dots y_n\} R^n T:R^n \to R^n  T(u)=w T(x_i)=y_i \;\; \forall i\leq i \leq n A=[T]_{B_1}^{B2}","['linear-algebra', 'matrices', 'matrix-equations', 'orthogonal-matrices']"
65,Proving trace is independent of basis using tensor product,Proving trace is independent of basis using tensor product,,"This is a question that has been deleted ( see here if you have sufficient reputation), but which I would like to ask again since it seems interesting.  I have an answer of my own that I will post when I have the time, but I am interested in seeing other approaches as well. The question: For $V,W$ vector spaces with $\dim V = n < \infty$ , we let $A \in Hom(V, V \otimes W)$ . Given a basis $\{v_j\}_{j=1}^{n}$ of $V$ let $w_{ij} \in W$ be defined by $Av_j = \sum_{i=1}^n v_i \otimes w_{ij}$ and let $Trace(A) = \sum_{i=1}^n w_{ii}$ . I'm asked to prove that this definition is independent of choice of basis. As is pointed out in the original post, it is notable that there is no straightforward way to compute the $w_{ij}$ , let alone compute how they would change under a change of basis.","This is a question that has been deleted ( see here if you have sufficient reputation), but which I would like to ask again since it seems interesting.  I have an answer of my own that I will post when I have the time, but I am interested in seeing other approaches as well. The question: For vector spaces with , we let . Given a basis of let be defined by and let . I'm asked to prove that this definition is independent of choice of basis. As is pointed out in the original post, it is notable that there is no straightforward way to compute the , let alone compute how they would change under a change of basis.","V,W \dim V = n < \infty A \in Hom(V, V \otimes W) \{v_j\}_{j=1}^{n} V w_{ij} \in W Av_j = \sum_{i=1}^n v_i \otimes w_{ij} Trace(A) = \sum_{i=1}^n w_{ii} w_{ij}","['linear-algebra', 'abstract-algebra', 'matrices', 'tensor-products']"
66,Prove that $\det A$ does not exceed $1$,Prove that  does not exceed,\det A 1,"Let $A =(a_{ij}) ∈  M_n(\Bbb R)$ be a matrix with nonnegative entries such that the sum of the entries in each row does not exceed $1$ . Prove that $|\det A|$ does not exceed $1$ too. This is one of my exercise,I tried to induction,but i'm stuck.  Help me please! Thanks","Let be a matrix with nonnegative entries such that the sum of the entries in each row does not exceed . Prove that does not exceed too. This is one of my exercise,I tried to induction,but i'm stuck.  Help me please! Thanks",A =(a_{ij}) ∈  M_n(\Bbb R) 1 |\det A| 1,"['linear-algebra', 'matrices', 'determinant']"
67,Do either of $c\cdot I_n-A$ or $A-c\cdot I_n$ have a name?,Do either of  or  have a name?,c\cdot I_n-A A-c\cdot I_n,"Let $A$ be an $n\times n$ matrix and let $c$ be a scalar. The matrices $c\cdot I_n-A$ and $A-c\cdot I_n$ are used to say interesting things about $A$ . Do either of $c\cdot I_n-A$ and $A-c\cdot I_n$ have a name? It seems like calling $A-c\cdot I_n$ something like the ""shift of $A$ by a factor of $c$ "" makes sense, but ""shift matrices"" usually refer to binary matrices whose only nonzero entries are either on the superdiagonal or subdiagonal. I suppose it's also worth asking if either of $c\cdot I-T$ or $T-c\cdot I$ have a name if $T:V\to V$ is a linear endomorphism of a vector space $V$ and $I:V\to V$ is the identity.","Let be an matrix and let be a scalar. The matrices and are used to say interesting things about . Do either of and have a name? It seems like calling something like the ""shift of by a factor of "" makes sense, but ""shift matrices"" usually refer to binary matrices whose only nonzero entries are either on the superdiagonal or subdiagonal. I suppose it's also worth asking if either of or have a name if is a linear endomorphism of a vector space and is the identity.",A n\times n c c\cdot I_n-A A-c\cdot I_n A c\cdot I_n-A A-c\cdot I_n A-c\cdot I_n A c c\cdot I-T T-c\cdot I T:V\to V V I:V\to V,"['linear-algebra', 'matrices', 'notation', 'terminology']"
68,Which is the importance of Young’s tableaux in mathematics?,Which is the importance of Young’s tableaux in mathematics?,,"I don’t know much about combinatorics, I’m just getting started on this. I want to know, why Young’s tableaux are important? and why it is important to relate them to matrices?  Thank you very much.","I don’t know much about combinatorics, I’m just getting started on this. I want to know, why Young’s tableaux are important? and why it is important to relate them to matrices?  Thank you very much.",,"['combinatorics', 'matrices', 'young-tableaux']"
69,Solution of $AX+XA=B$ through eigenvectors of $A$,Solution of  through eigenvectors of,AX+XA=B A,I see that Matlab uses the spectral decomposition to solve the continuous Lyapunov equation $$AX+XA=B$$ The formula they use for positive definite $A$ with matrix of eigenvectors $U$ and column vector of eigenvalues $s$ is as follows $$U \left( \frac{U' BU}{s + s'} \right) U'$$ (Addition of row and column is done with NumPy broadcasting rules) Any suggestions on how to derive this?,I see that Matlab uses the spectral decomposition to solve the continuous Lyapunov equation The formula they use for positive definite with matrix of eigenvectors and column vector of eigenvalues is as follows (Addition of row and column is done with NumPy broadcasting rules) Any suggestions on how to derive this?,AX+XA=B A U s U \left( \frac{U' BU}{s + s'} \right) U',"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations']"
70,"Diagonal of product of matrices, expression with inner product","Diagonal of product of matrices, expression with inner product",,"I have two matrices $\mathbf{A}$ and $\mathbf{B}$ . They are full and not square, though their dimensions implies that their product is : $$ \dim (\mathbf{A}) = (n,p) \quad \text{and} \quad \dim (\mathbf{B}) = (p,n) \implies \dim (\mathbf{A}\mathbf{B})=(n,n) $$ Now, I am interested in calculating the diagonal terms of this product, but without having to calculate all the other terms in the process (this will be in a code where $n$ and $p$ are very large). I wrote it on a simple example and ended up with : $$ diag(\mathbf{A}\mathbf{B}) = \sum Col\left(\mathbf{A} \cdot \mathbf{B}^\mathsf{T} \right) $$ with $\cdot$ the inner product between the two matrices, and $\sum Col$ the sum on the columns of the matrix. Sorry if my formalism is bad, I wrote it in a coding style, so please feel free to correct me. I tested it on my code and it matches, but : How could I demonstrate that properly? Is there a theorem or a property for that? I failed to find it on the internet... Thanks for your time!","I have two matrices and . They are full and not square, though their dimensions implies that their product is : Now, I am interested in calculating the diagonal terms of this product, but without having to calculate all the other terms in the process (this will be in a code where and are very large). I wrote it on a simple example and ended up with : with the inner product between the two matrices, and the sum on the columns of the matrix. Sorry if my formalism is bad, I wrote it in a coding style, so please feel free to correct me. I tested it on my code and it matches, but : How could I demonstrate that properly? Is there a theorem or a property for that? I failed to find it on the internet... Thanks for your time!","\mathbf{A} \mathbf{B} 
\dim (\mathbf{A}) = (n,p) \quad \text{and} \quad \dim (\mathbf{B}) = (p,n) \implies \dim (\mathbf{A}\mathbf{B})=(n,n)
 n p 
diag(\mathbf{A}\mathbf{B}) = \sum Col\left(\mathbf{A} \cdot \mathbf{B}^\mathsf{T} \right)
 \cdot \sum Col","['linear-algebra', 'matrices']"
71,"Find the reflection of the point $(4,-13)$ in the line $5x+y+6=0$",Find the reflection of the point  in the line,"(4,-13) 5x+y+6=0","Find The image(or reflection) of the point $(4,-13)$ in the line $5x+y+6=0$ Method 1 $$ y+13=\frac{1}{5}(x-4)\implies x-5y-69=0\quad\&\quad 5x+y+6=0\implies (3/2,-27/2)\\ (3/2,-27/2)=(\frac{x+4}{2},\frac{y-13}{2})\implies(x,y)=(-1,-14) $$ Method 2 $m=\tan\theta=-5$ Ref $(\theta)$ = $\begin{bmatrix} \cos(2\theta) & \sin(2\theta) \\ \sin(2\theta) & -\cos(2\theta) \end{bmatrix}$ $$ \cos2\theta=\frac{1-\tan^2\theta}{1+\tan^2\theta}=\frac{1-25}{1+25}=\frac{-24}{26}=\frac{-12}{13}\\ \sin2\theta=\frac{2\tan\theta}{1+\tan^2\theta}=\frac{-10}{26}=\frac{-5}{13}\\ Ref(\theta)\begin{bmatrix}4\\-13\end{bmatrix}=\begin{bmatrix} \cos(2\theta) & \sin(2\theta) \\ \sin(2\theta) & -\cos(2\theta) \end{bmatrix}\begin{bmatrix}4\\-13\end{bmatrix}=\begin{bmatrix} \dfrac{-12}{13} & \dfrac{-5}{13} \\ \dfrac{-5}{13} & \dfrac{12}{13} \end{bmatrix}\begin{bmatrix}4\\-13\end{bmatrix}\\ =\frac{1}{13}\begin{bmatrix}-48+65\\-20-156\end{bmatrix}=\frac{1}{13}\begin{bmatrix}17\\-176\end{bmatrix} $$ Why am I not getting the required solution in Method two using matrix method ? Thanx @ganeshie8 for the remarks, so in that case how do I find the operator for reflection of a point over the line not passing through the origin ?","Find The image(or reflection) of the point in the line Method 1 Method 2 Ref = Why am I not getting the required solution in Method two using matrix method ? Thanx @ganeshie8 for the remarks, so in that case how do I find the operator for reflection of a point over the line not passing through the origin ?","(4,-13) 5x+y+6=0 
y+13=\frac{1}{5}(x-4)\implies x-5y-69=0\quad\&\quad 5x+y+6=0\implies (3/2,-27/2)\\
(3/2,-27/2)=(\frac{x+4}{2},\frac{y-13}{2})\implies(x,y)=(-1,-14)
 m=\tan\theta=-5 (\theta) \begin{bmatrix}
\cos(2\theta) & \sin(2\theta) \\ \sin(2\theta) & -\cos(2\theta)
\end{bmatrix} 
\cos2\theta=\frac{1-\tan^2\theta}{1+\tan^2\theta}=\frac{1-25}{1+25}=\frac{-24}{26}=\frac{-12}{13}\\
\sin2\theta=\frac{2\tan\theta}{1+\tan^2\theta}=\frac{-10}{26}=\frac{-5}{13}\\
Ref(\theta)\begin{bmatrix}4\\-13\end{bmatrix}=\begin{bmatrix}
\cos(2\theta) & \sin(2\theta) \\ \sin(2\theta) & -\cos(2\theta)
\end{bmatrix}\begin{bmatrix}4\\-13\end{bmatrix}=\begin{bmatrix}
\dfrac{-12}{13} & \dfrac{-5}{13} \\ \dfrac{-5}{13} & \dfrac{12}{13}
\end{bmatrix}\begin{bmatrix}4\\-13\end{bmatrix}\\
=\frac{1}{13}\begin{bmatrix}-48+65\\-20-156\end{bmatrix}=\frac{1}{13}\begin{bmatrix}17\\-176\end{bmatrix}
","['linear-algebra', 'matrices', 'geometry', 'unitary-matrices']"
72,Does $A\geq (\mathrm{tr}(A^{-1}))^{-1}I$ hold for symmetric positive-definite matrix $A$?,Does  hold for symmetric positive-definite matrix ?,A\geq (\mathrm{tr}(A^{-1}))^{-1}I A,"Somewhere in my reading, it seems that the following inequality holds for  every symmetric positive definite matrix $A$ , $$A\geq \big(\mathrm{tr}(A^{-1})\big)^{-1}I,$$ where $I$ is the identity matrix. Is this true? Thank you!","Somewhere in my reading, it seems that the following inequality holds for  every symmetric positive definite matrix , where is the identity matrix. Is this true? Thank you!","A A\geq \big(\mathrm{tr}(A^{-1})\big)^{-1}I, I","['matrices', 'inequality', 'trace', 'positive-definite', 'symmetric-matrices']"
73,Decompose projection matrix into a matrix and its pseudoinverse,Decompose projection matrix into a matrix and its pseudoinverse,,"While researching regression, I encountered the situation where I am trying to reconstruct a data matrix $X \in \mathbb{R}^{n\times p}$ from the $n \times n$ hat matrix $H = X(X'X)^{-1}X' = XX^{+}$ (where $A^{+}$ is the Moore-Penrose inverse of $A$ ). Note that I only have access to the hat matrix, not the original data $X$ . So I have only $H$ , but I do know the dimensions ( $n$ and $p$ ) of the $X$ matrix, and I know that it is full column rank (i.e., $n > p$ and $\text{rk}(X) = p$ ). I know about $H$ that it is symmetric, idempotent, and positive semidefinite (it is a projection matrix). Is there a way to decompose the matrix $H \to XX^{+}$ in this context? Any approximate solutions where I could define the approximation error would also be very much appreciated. Some steps I already went through: $$H = XX^+$$ $$HX = X$$ $$(I_n - H)X = 0$$ and from there I realised that $X$ has something to do with the nullspace of $(I_n - H)$ .","While researching regression, I encountered the situation where I am trying to reconstruct a data matrix from the hat matrix (where is the Moore-Penrose inverse of ). Note that I only have access to the hat matrix, not the original data . So I have only , but I do know the dimensions ( and ) of the matrix, and I know that it is full column rank (i.e., and ). I know about that it is symmetric, idempotent, and positive semidefinite (it is a projection matrix). Is there a way to decompose the matrix in this context? Any approximate solutions where I could define the approximation error would also be very much appreciated. Some steps I already went through: and from there I realised that has something to do with the nullspace of .",X \in \mathbb{R}^{n\times p} n \times n H = X(X'X)^{-1}X' = XX^{+} A^{+} A X H n p X n > p \text{rk}(X) = p H H \to XX^{+} H = XX^+ HX = X (I_n - H)X = 0 X (I_n - H),"['matrices', 'matrix-decomposition', 'projection-matrices']"
74,"Do non-symmetric ""strongly positive definite"" matrices have unique positive definite square roots?","Do non-symmetric ""strongly positive definite"" matrices have unique positive definite square roots?",,"It is well known that if $A$ is a symmetric positive definite matrix, then it has a unique square root which is positive definite. My question is whether this result extends to a strongly positive definite nonsymmetric matrix. More precisely, let $A$ be a real nonsymmetric $n\times n$ matrix, which satisfies the following strong positive definite condition: there exists $a>0$ such that for each $x\in\mathbb R^n$ , the estimate $$ \langle Ax, x\rangle\geq a|x|^2 $$ holds. Is it true then that there exists a unique (edit: strongly positive definite) matrix $B$ such that $B^2=A$ ? I would be very interested in knowing the answer to this result, and reference to a proof. Thanks!","It is well known that if is a symmetric positive definite matrix, then it has a unique square root which is positive definite. My question is whether this result extends to a strongly positive definite nonsymmetric matrix. More precisely, let be a real nonsymmetric matrix, which satisfies the following strong positive definite condition: there exists such that for each , the estimate holds. Is it true then that there exists a unique (edit: strongly positive definite) matrix such that ? I would be very interested in knowing the answer to this result, and reference to a proof. Thanks!","A A n\times n a>0 x\in\mathbb R^n 
\langle Ax, x\rangle\geq a|x|^2
 B B^2=A","['linear-algebra', 'matrices']"
75,Upper Triangular and Diagonal Matrices,Upper Triangular and Diagonal Matrices,,"I'm trying to understand the difference between Upper Triangular and Diagonal Matrices for square matrices. There is a theorem that says every matrix can be made into an upper triangular matrix. Now if my matrix as the same number of eigenvectors as its dimension then I can make a Diagonal matrix by changing the basis to the eigenspace. So in this case, why will I ever prefer to make a matrix Upper Triangular(and not diagonal)  when I can make it diagonal?","I'm trying to understand the difference between Upper Triangular and Diagonal Matrices for square matrices. There is a theorem that says every matrix can be made into an upper triangular matrix. Now if my matrix as the same number of eigenvectors as its dimension then I can make a Diagonal matrix by changing the basis to the eigenspace. So in this case, why will I ever prefer to make a matrix Upper Triangular(and not diagonal)  when I can make it diagonal?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'diagonalization']"
76,"The proof of SVD solver formula, $C^T C = V \Sigma^T \Sigma V^T$","The proof of SVD solver formula,",C^T C = V \Sigma^T \Sigma V^T,"This formula could be used to compute matrix multiplication transpose $${\displaystyle (\mathbf {AB} )^{\mathsf {T}}=\mathbf {B} ^{\mathsf {T}}\mathbf {A} ^{\mathsf {T}}}$$ This formula is used to elaborate matrix multiplication associativity $${\displaystyle (\mathbf {AB} )\mathbf {C} =\mathbf {A} (\mathbf {BC} )}$$ This formula is used to compute svd of a matrix. $$C = U \Sigma V^T$$ this MIT course puts them together and gives (equation_1) $$C^T C = V \Sigma^T \Sigma V^T \tag 1$$ what is the detailed proof for equation_1? first, what is the detailed procedure of $C^TC = (U \Sigma V^T)^TC$ how can I transfer this to $U^TU$ ? $$C^T = V \Sigma^T U^T$$","This formula could be used to compute matrix multiplication transpose This formula is used to elaborate matrix multiplication associativity This formula is used to compute svd of a matrix. this MIT course puts them together and gives (equation_1) what is the detailed proof for equation_1? first, what is the detailed procedure of how can I transfer this to ?",{\displaystyle (\mathbf {AB} )^{\mathsf {T}}=\mathbf {B} ^{\mathsf {T}}\mathbf {A} ^{\mathsf {T}}} {\displaystyle (\mathbf {AB} )\mathbf {C} =\mathbf {A} (\mathbf {BC} )} C = U \Sigma V^T C^T C = V \Sigma^T \Sigma V^T \tag 1 C^TC = (U \Sigma V^T)^TC U^TU C^T = V \Sigma^T U^T,"['linear-algebra', 'matrices', 'svd']"
77,Why does every $ m \times n$ matrix of rank $r$ reduce to $(m \times r)$ times $(r \times n)$?,Why does every  matrix of rank  reduce to  times ?, m \times n r (m \times r) (r \times n),"How can I prove the following statement? Every $m \times n$ matrix of rank $r$ reduces to $(m \times r)$ times $(r \times n)$ : $A = $ (pivot columns of $A$ ) (first $r$ rows of $R$ ) = (COL)(ROW). [ Source: Gilbert Strang, Introduction to Linear Algebra , question $56$ , section $3.2$ . ] I think that the $R$ is reduced row echelon form. I believe there is a brief elegant proof.","How can I prove the following statement? Every matrix of rank reduces to times : (pivot columns of ) (first rows of ) = (COL)(ROW). [ Source: Gilbert Strang, Introduction to Linear Algebra , question , section . ] I think that the is reduced row echelon form. I believe there is a brief elegant proof.",m \times n r (m \times r) (r \times n) A =  A r R 56 3.2 R,"['linear-algebra', 'matrices', 'matrix-decomposition', 'matrix-rank']"
78,"If $A^{2016} = I_n$, show that $A^{576} - A^{288} + I_n$ is invertible, and calculate it's inverse in terms of $A$.","If , show that  is invertible, and calculate it's inverse in terms of .",A^{2016} = I_n A^{576} - A^{288} + I_n A,"Let $A$ be a real valued $n \times n$ matrix,where $n \geq 2$ , such that $A^{2016} =I_n.$ Show that the matrix $B = A^{576} - A^{288} + I_n$ is invertible, and calculate it's inverse in terms of $A$ . Well, I was able to prove that matrix is invertible, however it is a bit long-winded. Call $p(x) = x^{576} - x^{288} + 1$ . The eigenvalues of $B$ are of the form $p(\lambda)$ , where $\lambda \in Spec A \subset U_{2018},$ the 2018th roots of unity. We want to show that $B$ can't have a zero eigenvalue; That is, no $2018$ th root of unity is a root of $p(x)$ . Since $p(x) = \left( x^{288} - e^\frac{2\pi}{5} \right) \left( x^{288} - e^\frac{8\pi}{5} \right)$ , it's enough to show that there is no k such that $$2016\left(\frac{2\pi}{5\cdot288}+\frac{2k\pi}{288}\right) = 2l\pi$$ or $$2016\left(\frac{8\pi}{5\cdot288}+\frac{2k\pi}{288}\right) = 2l\pi,$$ where $l$ is another integer. This follows from the fact that $5$ does not divide $2016$ . However, I have no idea how to find $B$ in terms of $A$ . I think that there is a way to show that $B$ is invertible which also gives an expression for $B$ , but I can't figure that out. Any ideas?","Let be a real valued matrix,where , such that Show that the matrix is invertible, and calculate it's inverse in terms of . Well, I was able to prove that matrix is invertible, however it is a bit long-winded. Call . The eigenvalues of are of the form , where the 2018th roots of unity. We want to show that can't have a zero eigenvalue; That is, no th root of unity is a root of . Since , it's enough to show that there is no k such that or where is another integer. This follows from the fact that does not divide . However, I have no idea how to find in terms of . I think that there is a way to show that is invertible which also gives an expression for , but I can't figure that out. Any ideas?","A n \times n n \geq 2 A^{2016} =I_n. B = A^{576} - A^{288} + I_n A p(x) = x^{576} - x^{288} + 1 B p(\lambda) \lambda \in Spec A \subset U_{2018}, B 2018 p(x) p(x) = \left( x^{288} - e^\frac{2\pi}{5} \right) \left( x^{288} - e^\frac{8\pi}{5} \right) 2016\left(\frac{2\pi}{5\cdot288}+\frac{2k\pi}{288}\right) = 2l\pi 2016\left(\frac{8\pi}{5\cdot288}+\frac{2k\pi}{288}\right) = 2l\pi, l 5 2016 B A B B","['matrices', 'roots-of-unity']"
79,Discrete-time LQR and solutions via LMI,Discrete-time LQR and solutions via LMI,,"Having a infinite horizon discrete-time LQR problem $J^* = \min_u \ \sum_{k=0}^{\infty} x^\top_k Q x_k +u^\top_kRu_k$ subject to $x_{k+1}= Ax_k+Bu_k, \quad x(0)=x_0$ . With some algebra manipulations, and setting $J^*=x_kPx_k$ , with $P=P^\top\succ 0$ the following LMI is obtained: $J^* = \begin{bmatrix} u_k\\x_k \end{bmatrix}^\top \begin{bmatrix} B^\top PB+R & B^\top PA\\A^\top PB & A^\top PA +Q \end{bmatrix} \begin{bmatrix} u_k\\x_k \end{bmatrix} \prec 0$ Taking the Schur complement, the resulting state feedback controller $u_k=Kx_k$ is $K = -(R+B^\top PB)^{-1}B^\top PA$ where P is the solution of the Riccati equation $P = Q + A^\top PA - A^\top PB(R+B^\top PB)^{-1}B^\top PA$ I implemented an example in Matlab and compared the solutions obtained using the command dlqr and the LMI solved with Yalmip, but the values of the obtained (P,K) are not the same. the code is: A=[1.1 -0.3; 1 0]; B=[1;0]; Q=eye(2,2); R=1;  %% inf horizon [K,S,e] = dlqr(A,B,Q,R)  %% LMI solver: P = sdpvar(2,2); F1 = [A'*P*A + Q,A'*P*B; B'*P*A, R+B'*P*B ]; F = [P>=0, F1<=0]; optimize (F,P); Pfeasible=value(P); Kfeasible = -inv(R+B'*Pfeasible*B)*B'*Pfeasible*A Do you have an idea why? is it due to the solver?","Having a infinite horizon discrete-time LQR problem subject to . With some algebra manipulations, and setting , with the following LMI is obtained: Taking the Schur complement, the resulting state feedback controller is where P is the solution of the Riccati equation I implemented an example in Matlab and compared the solutions obtained using the command dlqr and the LMI solved with Yalmip, but the values of the obtained (P,K) are not the same. the code is: A=[1.1 -0.3; 1 0]; B=[1;0]; Q=eye(2,2); R=1;  %% inf horizon [K,S,e] = dlqr(A,B,Q,R)  %% LMI solver: P = sdpvar(2,2); F1 = [A'*P*A + Q,A'*P*B; B'*P*A, R+B'*P*B ]; F = [P>=0, F1<=0]; optimize (F,P); Pfeasible=value(P); Kfeasible = -inv(R+B'*Pfeasible*B)*B'*Pfeasible*A Do you have an idea why? is it due to the solver?","J^* = \min_u \ \sum_{k=0}^{\infty} x^\top_k Q x_k +u^\top_kRu_k x_{k+1}= Ax_k+Bu_k, \quad x(0)=x_0 J^*=x_kPx_k P=P^\top\succ 0 J^* = \begin{bmatrix}
u_k\\x_k
\end{bmatrix}^\top \begin{bmatrix}
B^\top PB+R & B^\top PA\\A^\top PB & A^\top PA +Q
\end{bmatrix} \begin{bmatrix}
u_k\\x_k
\end{bmatrix} \prec 0 u_k=Kx_k K = -(R+B^\top PB)^{-1}B^\top PA P = Q + A^\top PA - A^\top PB(R+B^\top PB)^{-1}B^\top PA","['matrices', 'convex-optimization', 'matrix-equations', 'linear-matrix-inequality', 'schur-complement']"
80,Is multiplying the matrix by its conjugate-transpose and divide by Frobenius norm something special for the matrix itself,Is multiplying the matrix by its conjugate-transpose and divide by Frobenius norm something special for the matrix itself,,"Assuming a complex matrix $A$ with $n$ x $m$ dimension. What does mean multiplying $A$ with its conjugate transpose and divide by Frobenius norm? ( $AA^H$ / $||A||_F$ ). I'm asking that question because I noticed something about that. suppose that I have $y = hA$ , where $h$ is a matrix of dimension $n$ x $m$ too. the results of $A^Hy / ||A||_F$ has always the same sign of $y$ . for that I asked Is that something special for any complex matrix $A$ ?","Assuming a complex matrix with x dimension. What does mean multiplying with its conjugate transpose and divide by Frobenius norm? ( / ). I'm asking that question because I noticed something about that. suppose that I have , where is a matrix of dimension x too. the results of has always the same sign of . for that I asked Is that something special for any complex matrix ?",A n m A AA^H ||A||_F y = hA h n m A^Hy / ||A||_F y A,"['linear-algebra', 'matrices', 'complex-numbers']"
81,Finding a limit of a matrix raised to the $n$-th power,Finding a limit of a matrix raised to the -th power,n,"There is a stochastic matrix given $$\mathbb{P} = \begin{bmatrix} 0&\frac{1}{2}&\frac{1}{2}&0 \\ 0&1&0&0 \\ \frac{1}{2}&0&0&\frac{1}{2} \\ \frac{1}{3}&0&\frac{1}{3}&\frac{1}{3} \end{bmatrix}$$ I am to find $\mathbb{P}^n$ . I know a theorem which says that if $\exists n$ $[p_{i,j}^n] >0$ then the limit exists and can be easily calculated (of course by $[p_{i,j}^n]$ I mean elements of $\mathbb{P}^n$ ). In my case however $[p_{2,1}^n] = [p_{2,3}^n] = [p_{2,4}^n] =0, \forall n$ . How can I solve this problem? Is there another theorem?",There is a stochastic matrix given I am to find . I know a theorem which says that if then the limit exists and can be easily calculated (of course by I mean elements of ). In my case however . How can I solve this problem? Is there another theorem?,"\mathbb{P} = \begin{bmatrix} 0&\frac{1}{2}&\frac{1}{2}&0 \\ 0&1&0&0 \\ \frac{1}{2}&0&0&\frac{1}{2} \\ \frac{1}{3}&0&\frac{1}{3}&\frac{1}{3} \end{bmatrix} \mathbb{P}^n \exists n [p_{i,j}^n] >0 [p_{i,j}^n] \mathbb{P}^n [p_{2,1}^n] = [p_{2,3}^n] = [p_{2,4}^n] =0, \forall n","['real-analysis', 'matrices', 'stochastic-processes']"
82,Are two isomorphic finite subgroups of $SO(4)$ conjugate?,Are two isomorphic finite subgroups of  conjugate?,SO(4),"Let $A,B$ be two finite subgroups of $SO(4)$ such that $A$ and $B$ are isomorphic as abstract groups. Can we find a $g \in SO(4)$ such that $$ gAg^{-1}=B? $$ If it is the case, does the same conclusion hold for $SO(n)$ ?","Let be two finite subgroups of such that and are isomorphic as abstract groups. Can we find a such that If it is the case, does the same conclusion hold for ?","A,B SO(4) A B g \in SO(4) 
gAg^{-1}=B?
 SO(n)","['matrices', 'group-theory', 'finite-groups', 'orthogonal-matrices']"
83,How to prove/show this actually defines a homomoprhism,How to prove/show this actually defines a homomoprhism,,"We define the  homomorphism $f: \text{SL}_2(\mathbb Z / 2 \mathbb Z) \to \text{SL}_2(\mathbb Z / 2 \mathbb Z)$ that maps the generators to: $ \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix} \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ $ \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \to \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$ How do we know this is a homorphism? In a previous exercise I explored that these two elements generate the group, the first has order 3, the second has order 2. Essentially this maps any power of the first matrix to the identity, and any power of the second to a power of the matrix $\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$ is it as simple as: $$f\left(\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^{m \bmod 3}\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}^{n \bmod 2}\right)=\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}^{n \bmod 2}$$ But since the matrices don't commute I find it hard to prove this is a homomorphism ( $f(AB)=f(A) f(B))$ . Essentially I want to prove that: $f$ defined by: $$\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix},\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^2= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^3 =\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$$ $$\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix},\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}    \to \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} $$ is a homomorphism.","We define the  homomorphism that maps the generators to: How do we know this is a homorphism? In a previous exercise I explored that these two elements generate the group, the first has order 3, the second has order 2. Essentially this maps any power of the first matrix to the identity, and any power of the second to a power of the matrix is it as simple as: But since the matrices don't commute I find it hard to prove this is a homomorphism ( . Essentially I want to prove that: defined by: is a homomorphism.","f: \text{SL}_2(\mathbb Z / 2 \mathbb Z) \to \text{SL}_2(\mathbb Z / 2 \mathbb Z)  \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix} \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}  \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \to \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} f\left(\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^{m \bmod 3}\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}^{n \bmod 2}\right)=\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}^{n \bmod 2} f(AB)=f(A) f(B)) f \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix},\begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^2= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^3 =\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix},\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}, \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix}    \to \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} ","['abstract-algebra', 'matrices']"
84,Find $Tr(B)+Tr(C)$,Find,Tr(B)+Tr(C),"If $B,C$ are $2 \times 2 $ matrices with integer entries such that: $$\begin{bmatrix} -1 &1 \\   0& 2 \end{bmatrix}=B^3+C^3$$ Find value of $Tr(B)+Tr(C)$ My try: Taking trace on both sides we get: $$Tr(B^3)+Tr(C^3)=1$$ Any clue from here?",If are matrices with integer entries such that: Find value of My try: Taking trace on both sides we get: Any clue from here?,"B,C 2 \times 2  \begin{bmatrix}
-1 &1 \\ 
 0& 2
\end{bmatrix}=B^3+C^3 Tr(B)+Tr(C) Tr(B^3)+Tr(C^3)=1","['matrices', 'determinant', 'trace']"
85,"How does the high school approach to classifying linear systems generalize to more variables, and why does it work?","How does the high school approach to classifying linear systems generalize to more variables, and why does it work?",,"If we have two linear equations in two variables, there may be $0$ , $1$ or infinitely-many solutions. The standard approach to working out which case we're dealing with is probably to use Gaussian elimination. However, in high schools around here, they don't teach Gaussian elimination, and instead expect students to use (something like) the following result: Theorem. Consider $a,b,c,d,p,q \in \mathbb{R}$ . Let $\Gamma(x,y)$ denote the following condition. $$ax+by = p$$ $$cx+dy = q$$ Define $\tilde{\Gamma} := \{(x,y) \in \mathbb{R}^2 : \Gamma(x,y)\}.$ Then: $\tilde{\Gamma}$ has exactly one element if and only if $ad\neq bc$ . $\tilde{\Gamma}$ has infinitely many elements if and only if $ad = bc$ and $bq = dp.$ $\tilde{\Gamma}$ has no elements if and only if $ad = bc$ and $bq \neq dp.$ (*I haven't actually proved this, and some nonzeroness assumptions might be necessary to make it fully technically correct.) What seems to be happening, in particular, is that minors play a role in the number of solutions. By a $k$ -minor of a matrix $M$ , I mean a determinant of a $k \times k$ submatrix of $M$ . What seems to be going on is that we have to look at the $2$ -minors of the ""augmented matrix"" $$   \left[\begin{array}{rr|r}     a & b & p \\     c & d & q \\   \end{array}\right] $$ or, perhaps more precisely, the $2$ -minors of $$   \left[\begin{array}{rrr}     a & b & -p \\     c & d & -q \\   \end{array}\right] $$ which, due to multilinearity of the determinant in the columns of the matrices to which it can be applied, has the same $2$ -minors up to a change in sign. However, I don't really get what's going on here. I get that we can move to projective space by homogenizing the linear equations above to obtain $$ax+by -pz = 0$$ $$cx+dy-qz=0$$ This seems to have something do with it. I also get that there's a way to determine the rank of a matrix using its minors, and thus the nullity using the rank-nullity theorem. But this isn't enough to really understand what's going on. Question. How does this classification generalize to more variables and/or more equations, and does projective space have something to do with it? Why does it work, and what's really going on here? I put in some partially-related tags to get input from a wider audience, but feel free to remove them if you think they're a bad fit.","If we have two linear equations in two variables, there may be , or infinitely-many solutions. The standard approach to working out which case we're dealing with is probably to use Gaussian elimination. However, in high schools around here, they don't teach Gaussian elimination, and instead expect students to use (something like) the following result: Theorem. Consider . Let denote the following condition. Define Then: has exactly one element if and only if . has infinitely many elements if and only if and has no elements if and only if and (*I haven't actually proved this, and some nonzeroness assumptions might be necessary to make it fully technically correct.) What seems to be happening, in particular, is that minors play a role in the number of solutions. By a -minor of a matrix , I mean a determinant of a submatrix of . What seems to be going on is that we have to look at the -minors of the ""augmented matrix"" or, perhaps more precisely, the -minors of which, due to multilinearity of the determinant in the columns of the matrices to which it can be applied, has the same -minors up to a change in sign. However, I don't really get what's going on here. I get that we can move to projective space by homogenizing the linear equations above to obtain This seems to have something do with it. I also get that there's a way to determine the rank of a matrix using its minors, and thus the nullity using the rank-nullity theorem. But this isn't enough to really understand what's going on. Question. How does this classification generalize to more variables and/or more equations, and does projective space have something to do with it? Why does it work, and what's really going on here? I put in some partially-related tags to get input from a wider audience, but feel free to remove them if you think they're a bad fit.","0 1 a,b,c,d,p,q \in \mathbb{R} \Gamma(x,y) ax+by = p cx+dy = q \tilde{\Gamma} := \{(x,y) \in \mathbb{R}^2 : \Gamma(x,y)\}. \tilde{\Gamma} ad\neq bc \tilde{\Gamma} ad = bc bq = dp. \tilde{\Gamma} ad = bc bq \neq dp. k M k \times k M 2 
  \left[\begin{array}{rr|r}
    a & b & p \\
    c & d & q \\
  \end{array}\right]
 2 
  \left[\begin{array}{rrr}
    a & b & -p \\
    c & d & -q \\
  \end{array}\right]
 2 ax+by -pz = 0 cx+dy-qz=0","['linear-algebra', 'matrices', 'algebraic-geometry', 'representation-theory', 'projective-space']"
86,"Proof $\langle v,w\rangle=0\implies \langle Av,Aw\rangle=0$, given $A\in F^{m\times n}, \operatorname{rank}(A)=n$.","Proof , given .","\langle v,w\rangle=0\implies \langle Av,Aw\rangle=0 A\in F^{m\times n}, \operatorname{rank}(A)=n","Given two vectors $v,w$ orthogonal, and a matrix $A$ which has orthogonal columns, how to prove that $\langle Av,Aw \rangle=0$ , i.e. their image is orthogonal?","Given two vectors orthogonal, and a matrix which has orthogonal columns, how to prove that , i.e. their image is orthogonal?","v,w A \langle Av,Aw \rangle=0","['linear-algebra', 'matrices', 'inner-products', 'orthogonality']"
87,"$ Av=\lambda v \implies\ A^*v=\overline \lambda v $, Is this true only for normal operators?",", Is this true only for normal operators?", Av=\lambda v \implies\ A^*v=\overline \lambda v ,"I know that is true for normal operators ( matrices). But isnt the following proof independant of normality. $$\langle A^\ast v, v\rangle = \langle v, Av\rangle = \langle v, \lambda v\rangle = \langle \overline{\lambda} v,v\rangle $$ So we can conclude $ A^\ast v=\overline{\lambda} $ , and we didnt use normality anywhere. If it is true that normality isnt needed for this then also normality isnt needed for orthogonal eigenvectors either. Where am i making a mistake ? @Omnomnomnom is the proof of $ (AB)^\ast=B^\ast A^\ast $ that goes like this $$ \langle (AB) v, v\rangle= \langle v, (AB)^\ast v\rangle $$ $$ \langle (AB) v, v\rangle= \langle Bv, A^\ast v\rangle= \langle v, B^\ast A^\ast v\rangle$$ also wrong becuase we cant divide by vectors                       in inner product","I know that is true for normal operators ( matrices). But isnt the following proof independant of normality. So we can conclude , and we didnt use normality anywhere. If it is true that normality isnt needed for this then also normality isnt needed for orthogonal eigenvectors either. Where am i making a mistake ? @Omnomnomnom is the proof of that goes like this also wrong becuase we cant divide by vectors                       in inner product","\langle A^\ast v, v\rangle = \langle v, Av\rangle = \langle v, \lambda v\rangle = \langle \overline{\lambda} v,v\rangle   A^\ast v=\overline{\lambda}   (AB)^\ast=B^\ast A^\ast   \langle (AB) v, v\rangle= \langle v, (AB)^\ast v\rangle   \langle (AB) v, v\rangle= \langle Bv, A^\ast v\rangle= \langle v, B^\ast A^\ast v\rangle","['linear-algebra', 'matrices', 'proof-verification']"
88,Give the standard matrix of the projection $T:\Bbb R^3 \to \Bbb R^3$ that projects a vector on the plane $x+y+z=0$,Give the standard matrix of the projection  that projects a vector on the plane,T:\Bbb R^3 \to \Bbb R^3 x+y+z=0,"I'm trying to figure this one out guys with no luck. Give the standard matrix of the projection $$ T:\Bbb R^3 \to \Bbb R^3 $$ that projects a vector on the plane $x+y+z=0.$ I tried to make a basis $B$ , for instance $(-1,0,1)$ and $(0,1,-1)$ then make them orthogonal using Gram-Schmidt, then make the projection $$\frac{(x,y,z) \cdot (-1,0,1)}{(-1,0,1)\cdot(-1,0,1)} + \frac{(x,y,z) \cdot (0,1,-1)}{(0,1,-1)\cdot(0,1,-1)}$$ After this I'm lost I'm taking a linear algebra course, so an answer in that field would be much appreciated prefferably using the method I used. Thanks in advance!","I'm trying to figure this one out guys with no luck. Give the standard matrix of the projection that projects a vector on the plane I tried to make a basis , for instance and then make them orthogonal using Gram-Schmidt, then make the projection After this I'm lost I'm taking a linear algebra course, so an answer in that field would be much appreciated prefferably using the method I used. Thanks in advance!","
T:\Bbb R^3 \to \Bbb R^3
 x+y+z=0. B (-1,0,1) (0,1,-1) \frac{(x,y,z) \cdot (-1,0,1)}{(-1,0,1)\cdot(-1,0,1)} + \frac{(x,y,z) \cdot (0,1,-1)}{(0,1,-1)\cdot(0,1,-1)}","['matrices', 'projection']"
89,Matrix quadratic form expansion question,Matrix quadratic form expansion question,,"I'm trying to do a question and within it, I need to expand a matrix quadratic form: $\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x})$ In my working out, I think that the following is correct: $$ \begin{align} \frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) & = \frac{1}{2}(\vec{y}^{T} - \vec{x}^{T}) \Sigma (\vec{y} - \vec{x}) \\ & = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \frac{1}{2} \vec{y}^{T}\Sigma\vec{x} - \frac{1}{2} \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x} \end{align} $$ However, in the answers, it says that the answer is $\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \vec{y}^{T}\Sigma\vec{x} - \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x}$ so the middle two cross product terms do not have a half multiplied to them. Can anyone explain this? Or are the answers wrong? Thanks in advance!","I'm trying to do a question and within it, I need to expand a matrix quadratic form: In my working out, I think that the following is correct: However, in the answers, it says that the answer is so the middle two cross product terms do not have a half multiplied to them. Can anyone explain this? Or are the answers wrong? Thanks in advance!","\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) 
\begin{align}
\frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) & = \frac{1}{2}(\vec{y}^{T} - \vec{x}^{T}) \Sigma (\vec{y} - \vec{x}) \\
& = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \frac{1}{2} \vec{y}^{T}\Sigma\vec{x} - \frac{1}{2} \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x}
\end{align}
 \frac{1}{2}(\vec{y} - \vec{x})^{T} \Sigma (\vec{y} - \vec{x}) = \frac{1}{2} \vec{y}^{T}\Sigma\vec{y} - \vec{y}^{T}\Sigma\vec{x} - \vec{x}^{T}\Sigma\vec{y} + \frac{1}{2}\vec{x}^{T}\Sigma\vec{x}","['matrices', 'matrix-equations']"
90,"Let $A$ and $B$ be matrixes. If $BA=B$ and $Rank\space A = Rank\space B$, prove $A^2=A$","Let  and  be matrixes. If  and , prove",A B BA=B Rank\space A = Rank\space B A^2=A,"Let $A$ and $B$ be matrixes. If $BA=B$ and $Rank\space A = Rank\space B$ , prove $A^2=A$ Ok, so I can see that: $ABA=AB$ $AABA=AAB$ $AABAA=AABA$ $A^2BA^2=A^2BA$ but I don't know how to keep following. Any hint? Also how would I apply the Rank thing? EDIT: I messed up ABA implies B is regular which I don't know.","Let and be matrixes. If and , prove Ok, so I can see that: but I don't know how to keep following. Any hint? Also how would I apply the Rank thing? EDIT: I messed up ABA implies B is regular which I don't know.",A B BA=B Rank\space A = Rank\space B A^2=A ABA=AB AABA=AAB AABAA=AABA A^2BA^2=A^2BA,"['linear-algebra', 'matrices']"
91,"Prove if the product of $k$ matrices $A_1$ ... $A_k$ is nonsingular, then each matrix $A_i$ is nonsingular.","Prove if the product of  matrices  ...  is nonsingular, then each matrix  is nonsingular.",k A_1 A_k A_i,"I'm having trouble proving this without using determinants. I know how to prove it with the product of just two matrices, but I'm not sure how to generalize this to a product of k matrices. Is there a way to do this proof without determinants? To clarify the question, each matrix is an $n$ by $n$ matrix. For example, I know that if the determinant of the product is nonzero, then all of the determinants of the individual matrices must also be nonzero. I also know that if a product of two matrices $A$ and $B$ , $AB$ is nonsingular, then there exists a matrix $C$ so that $C(AB) = I$ and $(AB)C = I$ , and so $(CA)B = I$ and $A(BC) = I$ , so $A$ and $B$ are both invertible, and thus nonsingular. I'm looking for a way to generalize this. Or just any other way to prove this without determinants. Thanks so much!","I'm having trouble proving this without using determinants. I know how to prove it with the product of just two matrices, but I'm not sure how to generalize this to a product of k matrices. Is there a way to do this proof without determinants? To clarify the question, each matrix is an by matrix. For example, I know that if the determinant of the product is nonzero, then all of the determinants of the individual matrices must also be nonzero. I also know that if a product of two matrices and , is nonsingular, then there exists a matrix so that and , and so and , so and are both invertible, and thus nonsingular. I'm looking for a way to generalize this. Or just any other way to prove this without determinants. Thanks so much!",n n A B AB C C(AB) = I (AB)C = I (CA)B = I A(BC) = I A B,"['linear-algebra', 'matrices']"
92,Can $(AA^T)^{-1}A$ be simplified? [closed],Can  be simplified? [closed],(AA^T)^{-1}A,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Given an $m\times n$ matrix $A$ , where $m<n$ , can the expression $(AA^T)^{-1}A$ be simplified?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Given an matrix , where , can the expression be simplified?",m\times n A m<n (AA^T)^{-1}A,"['linear-algebra', 'matrices']"
93,"Given a matrix $A(x)$, prove that $A(1)*A(2)*...*A(2017)=A(1009^2)$","Given a matrix , prove that",A(x) A(1)*A(2)*...*A(2017)=A(1009^2),"I am given the matrix $A=\begin{bmatrix}1&x&0\\0&1&0\\0&0&30^x\end{bmatrix}$ and I have to prove: $A(1)*A(2)*...*A(2017)=A(1009^2)$ From previous sub-points of the problem I have proved that $A(x)*A(y)=A(x+y)$ (1) for any x,y from $R$ . Also, I have proved (with mathematical induction) that $A^k=\begin{bmatrix}1&k*x&0\\0&1&0\\0&0&30^{k*x}\end{bmatrix}$ (2), but I can't see how  to use these to prove that $A(1)*A(2)*...*A(2017)=A(1009^2)$ . If I use (1), wouldn't $A(1)*A(2)*...*A(2017)=A(1+2+3+...+2017)$ ? Making the answer $A(2017*1009)$ ? Or am I using the proof (1) wrongly? How should I approach this problem?","I am given the matrix and I have to prove: From previous sub-points of the problem I have proved that (1) for any x,y from . Also, I have proved (with mathematical induction) that (2), but I can't see how  to use these to prove that . If I use (1), wouldn't ? Making the answer ? Or am I using the proof (1) wrongly? How should I approach this problem?",A=\begin{bmatrix}1&x&0\\0&1&0\\0&0&30^x\end{bmatrix} A(1)*A(2)*...*A(2017)=A(1009^2) A(x)*A(y)=A(x+y) R A^k=\begin{bmatrix}1&k*x&0\\0&1&0\\0&0&30^{k*x}\end{bmatrix} A(1)*A(2)*...*A(2017)=A(1009^2) A(1)*A(2)*...*A(2017)=A(1+2+3+...+2017) A(2017*1009),['matrices']
94,(non)Zero elements in an SPD matrix A lead to (non)zero elements in L where L is the Cholesky decomposition matrix.,(non)Zero elements in an SPD matrix A lead to (non)zero elements in L where L is the Cholesky decomposition matrix.,,"Given an SPD matrix $A$ and Cholesky decomposition $A = LL^T$ . Prove that $A(i,j) = 0$ means that $L(i,j) = 0$ and that $A(i,j) \neq 0$ means that $L(i,j) \neq 0$ I've been messing around with the formulas that determine the elements of the L matrix for too long now. I thought about proving this through induction and through a contradiction but nothing worked. Any help is appreciated.",Given an SPD matrix and Cholesky decomposition . Prove that means that and that means that I've been messing around with the formulas that determine the elements of the L matrix for too long now. I thought about proving this through induction and through a contradiction but nothing worked. Any help is appreciated.,"A A = LL^T A(i,j) = 0 L(i,j) = 0 A(i,j) \neq 0 L(i,j) \neq 0","['linear-algebra', 'matrices', 'matrix-equations']"
95,Finding the best rank-one approximation of the matrix $\bf A$,Finding the best rank-one approximation of the matrix,\bf A,"I have computed the singular value decomposition (SVD) of the following matrix $A$ . $$ {\bf A} := \begin{bmatrix}1&2\\0&1\\-1&0\\\end{bmatrix} = \underbrace{\left[\begin{matrix}0 & \sqrt{\frac 5 6} & \frac{1}{\sqrt 6} \\ \frac{1}{\sqrt 5} & \sqrt{\frac 2 {15}} & -\sqrt{\frac 2 3} \\ \frac{2}{\sqrt 5} & -\frac{1}{\sqrt 30} & \frac{1}{\sqrt 6}\end{matrix}\right]}_{=: {\bf U}} \underbrace{\left[\begin{matrix}1 & 0 \\ 0 & \sqrt 6 \\ 0 & 0\end{matrix}\right]}_{=: {\bf \Sigma}} \underbrace{\left[\begin{matrix}-\frac{2}{\sqrt 5} & \frac{1}{\sqrt 5} \\ \frac{1}{\sqrt 5} & \frac{2}{\sqrt 5}\end{matrix}\right]^\top}_{=: {\bf V}^\top} $$ The singular value decomposition can be used to obtain the best rank $p\leq r $ approximation of a matrix $A$ , by only keeping the first $p$ terms. Find the best rank-one approximation of the matrix $\bf A$ . How can I find the best rank-one approximation of the matrix $\bf A$ ?","I have computed the singular value decomposition (SVD) of the following matrix . The singular value decomposition can be used to obtain the best rank approximation of a matrix , by only keeping the first terms. Find the best rank-one approximation of the matrix . How can I find the best rank-one approximation of the matrix ?",A  {\bf A} := \begin{bmatrix}1&2\\0&1\\-1&0\\\end{bmatrix} = \underbrace{\left[\begin{matrix}0 & \sqrt{\frac 5 6} & \frac{1}{\sqrt 6} \\ \frac{1}{\sqrt 5} & \sqrt{\frac 2 {15}} & -\sqrt{\frac 2 3} \\ \frac{2}{\sqrt 5} & -\frac{1}{\sqrt 30} & \frac{1}{\sqrt 6}\end{matrix}\right]}_{=: {\bf U}} \underbrace{\left[\begin{matrix}1 & 0 \\ 0 & \sqrt 6 \\ 0 & 0\end{matrix}\right]}_{=: {\bf \Sigma}} \underbrace{\left[\begin{matrix}-\frac{2}{\sqrt 5} & \frac{1}{\sqrt 5} \\ \frac{1}{\sqrt 5} & \frac{2}{\sqrt 5}\end{matrix}\right]^\top}_{=: {\bf V}^\top}  p\leq r  A p \bf A \bf A,"['matrices', 'optimization', 'matrix-decomposition', 'svd', 'rank-1-matrices']"
96,$a_1^k+\cdots+a_n^k=0$ for all $1\leq k\leq n$,for all,a_1^k+\cdots+a_n^k=0 1\leq k\leq n,"Prove that for $a_i\in F$ where $F$ is an algebraically closed field (Edit:with characteristic $0$ ), say $\mathbb C$ , if $a_1^k+\cdots+a_n^k=0$ for all $1\leq k\leq n$ , then $a_1=\cdots=a_n=0$ . For now the answer I know either use Newton's identity or Vandermonde determinant, which both seem overkill. I think maybe we could argue that the equations are linearly independent so it has at most one solution, or use induction somehow. Any thoughts?","Prove that for where is an algebraically closed field (Edit:with characteristic ), say , if for all , then . For now the answer I know either use Newton's identity or Vandermonde determinant, which both seem overkill. I think maybe we could argue that the equations are linearly independent so it has at most one solution, or use induction somehow. Any thoughts?",a_i\in F F 0 \mathbb C a_1^k+\cdots+a_n^k=0 1\leq k\leq n a_1=\cdots=a_n=0,"['linear-algebra', 'abstract-algebra', 'matrices']"
97,How to optimize objective in the Grassmann manifold?,How to optimize objective in the Grassmann manifold?,,"For Stiefel manifold, it contains all the orthogonal column matrices $$St(d,M) = \{X \in R^{M \times d} | X^TX = I\}$$ For Grassmann manifold, it is $$Gr(d,M) = \{col(X), X \in R^{M \times d}\}$$ For an objective problem $$ \min_{X \in St(d,M)} f(X)$$ can we solve above optimization using $$ \min_{X \in Gr(d,M)} f(X)$$ Is above transform legal? or $f(X)$ should satisfy some conditions?","For Stiefel manifold, it contains all the orthogonal column matrices For Grassmann manifold, it is For an objective problem can we solve above optimization using Is above transform legal? or should satisfy some conditions?","St(d,M) = \{X \in R^{M \times d} | X^TX = I\} Gr(d,M) = \{col(X), X \in R^{M \times d}\}  \min_{X \in St(d,M)} f(X)  \min_{X \in Gr(d,M)} f(X) f(X)","['linear-algebra', 'matrices', 'grassmannian', 'stiefel-manifolds']"
98,Explicit formula of exponential of companion matrix,Explicit formula of exponential of companion matrix,,"Let $$A=\begin{bmatrix} a_k & a_{k-1} & a_{k-2} & \cdots & a_2 & a_1 \\ 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots &\\0 & 0 & 0 & \cdots & 1 & 0\end{bmatrix}$$ be a $k \times k$ matrix of { $a_k$ } on a commutative ring. Find the explicit expression of the last row of $A^n$ in terms of { $a_k$ }, where $k\le n$ .","Let be a matrix of { } on a commutative ring. Find the explicit expression of the last row of in terms of { }, where .",A=\begin{bmatrix} a_k & a_{k-1} & a_{k-2} & \cdots & a_2 & a_1 \\ 1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 1 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots &\\0 & 0 & 0 & \cdots & 1 & 0\end{bmatrix} k \times k a_k A^n a_k k\le n,"['linear-algebra', 'matrices', 'matrix-exponential', 'companion-matrices']"
99,Inverse of matrix of ones minus identity matrix,Inverse of matrix of ones minus identity matrix,,"What is the solution for $$A^{-1}(n) = (\mathbf{1}_{n} - I_{n})^{-1},$$ where $\mathbf{1}_{n}$ is the $n\times n$ matrix of ones and $I_{n}$ is the $n\times n$ identity matrix. Numerical Examples Suggest $$ A^{-1}(n) = \begin{bmatrix} \frac{-n+2}{n-1} & \frac{1}{n-1} & \frac{1}{n-1} & ... \\ \frac{1}{n-1} & \frac{-n+2}{n-1} & \frac{1}{n-1} & ... \\ \frac{1}{n-1} & \frac{1}{n-1} & \frac{-n+2}{n-1} & ... \\ \vdots & \vdots & \vdots & \frac{-n+2}{n-1} \\ \end{bmatrix} $$ For similar questions on determinants, see How to calculate the determinant of all-ones matrix minus the identity? Why is the determinant of the all one matrix minus the identity matrix n-1? How to calculate the determinant of all-ones matrix minus the identity? Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ Determinant of a specific circulant matrix, $A_n$","What is the solution for where is the matrix of ones and is the identity matrix. Numerical Examples Suggest For similar questions on determinants, see How to calculate the determinant of all-ones matrix minus the identity? Why is the determinant of the all one matrix minus the identity matrix n-1? How to calculate the determinant of all-ones matrix minus the identity? Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ Determinant of a specific circulant matrix, $A_n$","A^{-1}(n) = (\mathbf{1}_{n} - I_{n})^{-1}, \mathbf{1}_{n} n\times n I_{n} n\times n 
A^{-1}(n) = \begin{bmatrix}
\frac{-n+2}{n-1} & \frac{1}{n-1} & \frac{1}{n-1} & ... \\
\frac{1}{n-1} & \frac{-n+2}{n-1} & \frac{1}{n-1} & ... \\
\frac{1}{n-1} & \frac{1}{n-1} & \frac{-n+2}{n-1} & ... \\
\vdots & \vdots & \vdots & \frac{-n+2}{n-1} \\
\end{bmatrix}
","['matrices', 'inverse']"
