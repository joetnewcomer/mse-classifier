,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Anybody know a proof of $\prod_{n=1}^\infty\cos(x/2^n)=\sin x/x$. [duplicate],Anybody know a proof of . [duplicate],\prod_{n=1}^\infty\cos(x/2^n)=\sin x/x,"This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 2 years ago . This is actually an exercise from Apostol's Mathematical Analysis. Ch. 8 Ex 42. which asks to find all real values $x$ for which $\prod_{n=1}^\infty \cos\left(\large\frac{x}{2^n}\right)$ converges. I've shown that the product converges for all $x$ . The problem then asks to find what values the product converges to. By playing around with Wolfram Alpha, I found that $$\large\prod_{n=1}^\infty\cos\left(\frac{x}{2^n}\right)=\frac{\sin (x)}{x}.$$ I can't figure out how to prove this.","This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 2 years ago . This is actually an exercise from Apostol's Mathematical Analysis. Ch. 8 Ex 42. which asks to find all real values for which converges. I've shown that the product converges for all . The problem then asks to find what values the product converges to. By playing around with Wolfram Alpha, I found that I can't figure out how to prove this.",x \prod_{n=1}^\infty \cos\left(\large\frac{x}{2^n}\right) x \large\prod_{n=1}^\infty\cos\left(\frac{x}{2^n}\right)=\frac{\sin (x)}{x}.,"['real-analysis', 'infinite-product']"
1,"The closed form of $\lim_{x\to\frac{4}{3}}\frac{\partial}{\partial x}\left[\,_2{\rm{F}}_1\left(\frac{1}{3},1;x;-1\right)\right]$",The closed form of,"\lim_{x\to\frac{4}{3}}\frac{\partial}{\partial x}\left[\,_2{\rm{F}}_1\left(\frac{1}{3},1;x;-1\right)\right]","Do you think the following limit might have a closed form? Some hints or clues? $$\lim_{x\to\frac{4}{3}}\frac{\partial}{\partial x}\left[\,_2{\rm{F}}_1\left(\frac{1}{3},1;x;-1\right)\right]$$","Do you think the following limit might have a closed form? Some hints or clues? $$\lim_{x\to\frac{4}{3}}\frac{\partial}{\partial x}\left[\,_2{\rm{F}}_1\left(\frac{1}{3},1;x;-1\right)\right]$$",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'hypergeometric-function']"
2,Absolute convergence to a rational number,Absolute convergence to a rational number,,Let's recall the not so popular/familiar form of completeness of real numbers : Theorem : Absolute convergence of a series implies its convergence. Since $\mathbb{Q} $ is not complete there should exist a series $\sum_{n=1}^{\infty} u_n$ with rational terms such that $\sum_{n=1}^{\infty} |u_n|$ converges to a rational number and $\sum_{n=1}^{\infty}u_n$ converges to an irrational number. I could not think of an obvious example of such a series. Please provide one such example.,Let's recall the not so popular/familiar form of completeness of real numbers : Theorem : Absolute convergence of a series implies its convergence. Since is not complete there should exist a series with rational terms such that converges to a rational number and converges to an irrational number. I could not think of an obvious example of such a series. Please provide one such example.,\mathbb{Q}  \sum_{n=1}^{\infty} u_n \sum_{n=1}^{\infty} |u_n| \sum_{n=1}^{\infty}u_n,"['real-analysis', 'sequences-and-series']"
3,How to prove $\lim \limits_{x \to 1^-} \sum\limits_{n=0}^\infty (-1)^nx^{n^2} = \frac{1}{2} \ $?,How to prove ?,\lim \limits_{x \to 1^-} \sum\limits_{n=0}^\infty (-1)^nx^{n^2} = \frac{1}{2} \ ,"How to prove $$\lim \limits_{x \to 1^-} \displaystyle \sum_{n=0}^\infty (-1)^nx^{n^2} = \frac{1}{2}\,?$$ The power $n^2$ is problematic. Can we bring this back to the study of usual power series? I do not really have any idea for the moment.",How to prove The power is problematic. Can we bring this back to the study of usual power series? I do not really have any idea for the moment.,"\lim \limits_{x \to 1^-} \displaystyle \sum_{n=0}^\infty (-1)^nx^{n^2} = \frac{1}{2}\,? n^2",['real-analysis']
4,Does the series $\sum_{n=1}^{\infty}{\frac{\sin^2(\sqrt{n})}{n}}$ converge?,Does the series  converge?,\sum_{n=1}^{\infty}{\frac{\sin^2(\sqrt{n})}{n}},"Does the following series converge?    $$\sum_{n=1}^{\infty}{\frac{\sin^2(\sqrt{n})}{n}}$$ It shouldn't, but I have no idea how to prove it. I was wondering about Integral Criterion , but the assumptions are not satisfied. Or perhaps Dirichlet test would help, but then it should be shown that $\sum_{k=1}^n(\sin^2(\sqrt{k}))$ is bounded.","Does the following series converge?    $$\sum_{n=1}^{\infty}{\frac{\sin^2(\sqrt{n})}{n}}$$ It shouldn't, but I have no idea how to prove it. I was wondering about Integral Criterion , but the assumptions are not satisfied. Or perhaps Dirichlet test would help, but then it should be shown that $\sum_{k=1}^n(\sin^2(\sqrt{k}))$ is bounded.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
5,Does bounded variation imply boundedness,Does bounded variation imply boundedness,,"Using the standard definition  $$||f||_{TV} := \sup_{x_0<\cdots<x_n}\sum_{i=1}^{n} |f(x_i) - f(x_{i-1})|.$$ 1.When the domain is a bounded interval $[a,b]$, the statement holds. 2.When the domain is $\mathbb{R}$ and the function is monotone, the statement holds both ways (if and only if). But what about in general? My guess is true, and here is my arguement: If $||f||_{TV} < \infty$, then $f$ only has jump discontinuities, so we can bound $|f|\leq g$ where $g$ is monotone. By this construction, $||g||_{TV}\leq ||f||_{TV}<\infty$. By (2), we know $g$ is bounded, thus $f$ is bounded. Edit: I forgot, I probably need to impose a limit behavior at $\pm \infty$ for the function $g$. would $\limsup_{x\rightarrow \infty} g(x) - |f(x)| = 0$ be enough? is this correct and thank you for your help!","Using the standard definition  $$||f||_{TV} := \sup_{x_0<\cdots<x_n}\sum_{i=1}^{n} |f(x_i) - f(x_{i-1})|.$$ 1.When the domain is a bounded interval $[a,b]$, the statement holds. 2.When the domain is $\mathbb{R}$ and the function is monotone, the statement holds both ways (if and only if). But what about in general? My guess is true, and here is my arguement: If $||f||_{TV} < \infty$, then $f$ only has jump discontinuities, so we can bound $|f|\leq g$ where $g$ is monotone. By this construction, $||g||_{TV}\leq ||f||_{TV}<\infty$. By (2), we know $g$ is bounded, thus $f$ is bounded. Edit: I forgot, I probably need to impose a limit behavior at $\pm \infty$ for the function $g$. would $\limsup_{x\rightarrow \infty} g(x) - |f(x)| = 0$ be enough? is this correct and thank you for your help!",,"['real-analysis', 'bounded-variation']"
6,Real Analysis Book Choice,Real Analysis Book Choice,,"I am currently planning to get a book on Real Analysis for self  studying before diving into my 4th year real analysis course.  The standard textbook for my 4th year course is Stein's Measure,  but I do not like much about abstract measure introduced near the end.  Perhaps because I am currently taking 3rd year real analysis course  in the level of Pugh with some other additional materials. Anyway, I am considering one of the followings:  Folland - Real Analysis,   Bruckner, Bruckner, Thomson - Real Analysis,   Yeh - Real Analysis,   Kantorovitz - Introduction to Modern Analysis  (and maybe Cohn - Measure Theory) (Note: Royden is omitted because I am waiting for 2nd printing  and waiting so that I can get it cheap from some website  (like abebooks), so 12 pages of erratas are all fixed) Which book do you think is most suitable for self-study? (My 4th year course is cross-listed, meaning it is equivalent to first year graduate real analysis course)","I am currently planning to get a book on Real Analysis for self  studying before diving into my 4th year real analysis course.  The standard textbook for my 4th year course is Stein's Measure,  but I do not like much about abstract measure introduced near the end.  Perhaps because I am currently taking 3rd year real analysis course  in the level of Pugh with some other additional materials. Anyway, I am considering one of the followings:  Folland - Real Analysis,   Bruckner, Bruckner, Thomson - Real Analysis,   Yeh - Real Analysis,   Kantorovitz - Introduction to Modern Analysis  (and maybe Cohn - Measure Theory) (Note: Royden is omitted because I am waiting for 2nd printing  and waiting so that I can get it cheap from some website  (like abebooks), so 12 pages of erratas are all fixed) Which book do you think is most suitable for self-study? (My 4th year course is cross-listed, meaning it is equivalent to first year graduate real analysis course)",,"['real-analysis', 'measure-theory', 'reference-request', 'book-recommendation']"
7,Interesting Proofs about Metric Spaces?,Interesting Proofs about Metric Spaces?,,"I'm currently working through the book Introduction to Topology by Bert Mendelson, and I've finished all of the exercises provided at the end of the section that I have just completed, but I would like some more to try. I've just finished learning about metric spaces, continuity, and open balls about points in metric spaces. Just for a bit of context, some of the proofs that I have done include: If $(X,d)$ is a metric space and $a\in X$, for each $\delta \gt 0$, the open ball $B(a; \delta)$ is a neighborhood of each of its points. Let $f:(X,d)\to (Y,d')$, $a\in X$, and let $\beta_{f(a)}$ be a basis for the neighborhood system at $f(a)$. Prove that $f$ is continuous at $a$ iff $f^{-1}(N)$ is a neighborhood of $a$ for each $N \in \beta_{f(a)}$. If $a$ and $b$ are distinct points of a metric space $X$, prove that there exist neighborhoods $N_a$ and $N_b$ of $a$ and $b$ respectively such that $N_a \cap N_b=\varnothing$. If $(X,d)$ is a metric space containing $a$ and $b$, and $\delta+\eta \lt d(a,b)$, then $B(a;\delta)\cap B(b;\eta)=\varnothing$. Can anybody give me any other (perhaps slightly more challenging) proofs to do about these topics? I would like to practice some more with them, but I'm not very good about forming true conjectures to prove. Thanks!","I'm currently working through the book Introduction to Topology by Bert Mendelson, and I've finished all of the exercises provided at the end of the section that I have just completed, but I would like some more to try. I've just finished learning about metric spaces, continuity, and open balls about points in metric spaces. Just for a bit of context, some of the proofs that I have done include: If $(X,d)$ is a metric space and $a\in X$, for each $\delta \gt 0$, the open ball $B(a; \delta)$ is a neighborhood of each of its points. Let $f:(X,d)\to (Y,d')$, $a\in X$, and let $\beta_{f(a)}$ be a basis for the neighborhood system at $f(a)$. Prove that $f$ is continuous at $a$ iff $f^{-1}(N)$ is a neighborhood of $a$ for each $N \in \beta_{f(a)}$. If $a$ and $b$ are distinct points of a metric space $X$, prove that there exist neighborhoods $N_a$ and $N_b$ of $a$ and $b$ respectively such that $N_a \cap N_b=\varnothing$. If $(X,d)$ is a metric space containing $a$ and $b$, and $\delta+\eta \lt d(a,b)$, then $B(a;\delta)\cap B(b;\eta)=\varnothing$. Can anybody give me any other (perhaps slightly more challenging) proofs to do about these topics? I would like to practice some more with them, but I'm not very good about forming true conjectures to prove. Thanks!",,"['real-analysis', 'general-topology', 'metric-spaces', 'soft-question']"
8,Evalute the series $\sum_{n=0}^\infty \frac{(-1)^n}{n^2+1}$,Evalute the series,\sum_{n=0}^\infty \frac{(-1)^n}{n^2+1},"I want to find the value to which this series converges $$\sum_{n=0}^\infty \frac{(-1)^n}{n^2+1}$$ I tried looking at the sequence of partial sums $$S_k = \sum_{n=0}^k \frac{(-1)^n}{n^2+1}$$ and I noticed that $$\frac{-1}{n^2+1} \leq \frac{(-1)^n}{n^2+1} \leq \frac{1}{n^2 +1}$$ and so I think that by the squeeze rule I can see ( I could have just noticed it by logic, but okay) that the terms converge to zero. How do I find the value of the original series though? I could only show that it converged","I want to find the value to which this series converges $$\sum_{n=0}^\infty \frac{(-1)^n}{n^2+1}$$ I tried looking at the sequence of partial sums $$S_k = \sum_{n=0}^k \frac{(-1)^n}{n^2+1}$$ and I noticed that $$\frac{-1}{n^2+1} \leq \frac{(-1)^n}{n^2+1} \leq \frac{1}{n^2 +1}$$ and so I think that by the squeeze rule I can see ( I could have just noticed it by logic, but okay) that the terms converge to zero. How do I find the value of the original series though? I could only show that it converged",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
9,Evaluate $\lim_{n\to\infty}nI_n$ with $I_n=\int_0^1\frac{x^n}{x^2+3x+2}dx$,Evaluate  with,\lim_{n\to\infty}nI_n I_n=\int_0^1\frac{x^n}{x^2+3x+2}dx,"We have to evaluate: $$\lim_{n\to\infty}nI_n$$ with $$I_n=\int_0^1\frac{x^n}{x^2+3x+2}\:dx.$$ There is an elegant way to solve this problem? Here is all my steps: My first ideea was to find a recurrence relation such that: $$I_{n+2}+3I_{n+1}+2I_n=\frac{1}{n+1},\forall x\in\mathbb{N}$$ Next step I show that $\forall x\in[0,1]\Rightarrow I_{n}\ge I_{n+1}\ge I_{n+2}$ Therefore it involving that: $$6I_n\ge 4I_{n+1}+2I_n\ge\frac{1}{n+1},\forall x\in\mathbb{N}$$ As I said above $$6I_{n+2}\leq 4I_{n+2}+2I_n\leq\frac{1}{n+1}$$  $\Rightarrow \frac{n}{6(n+1)}\leq nI_n\leq\frac{n}{6(n-1)},\forall x\in\mathbb{N}$ Therefore by squeeze thereom: $$nI_n\to\frac{1}{6}\:as\:n\to\infty$$","We have to evaluate: $$\lim_{n\to\infty}nI_n$$ with $$I_n=\int_0^1\frac{x^n}{x^2+3x+2}\:dx.$$ There is an elegant way to solve this problem? Here is all my steps: My first ideea was to find a recurrence relation such that: $$I_{n+2}+3I_{n+1}+2I_n=\frac{1}{n+1},\forall x\in\mathbb{N}$$ Next step I show that $\forall x\in[0,1]\Rightarrow I_{n}\ge I_{n+1}\ge I_{n+2}$ Therefore it involving that: $$6I_n\ge 4I_{n+1}+2I_n\ge\frac{1}{n+1},\forall x\in\mathbb{N}$$ As I said above $$6I_{n+2}\leq 4I_{n+2}+2I_n\leq\frac{1}{n+1}$$  $\Rightarrow \frac{n}{6(n+1)}\leq nI_n\leq\frac{n}{6(n-1)},\forall x\in\mathbb{N}$ Therefore by squeeze thereom: $$nI_n\to\frac{1}{6}\:as\:n\to\infty$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
10,Proof that if $f$ is integrable then also $f^2$ is integrable,Proof that if  is integrable then also  is integrable,f f^2,"Prove this: Let $f :[a,b] \to \mathbb{R}$ be a bounded and integrable function. Show that $f^2$ is integrable too. I'm in trouble with this. Can anyone show how to do it?","Prove this: Let $f :[a,b] \to \mathbb{R}$ be a bounded and integrable function. Show that $f^2$ is integrable too. I'm in trouble with this. Can anyone show how to do it?",,"['real-analysis', 'integration']"
11,Integral $\int_0^{\pi/2} \frac{\sin^3 x\log \sin x}{\sqrt{1+\sin^2 x}}dx=\frac{\ln 2 -1}{4}$,Integral,\int_0^{\pi/2} \frac{\sin^3 x\log \sin x}{\sqrt{1+\sin^2 x}}dx=\frac{\ln 2 -1}{4},"Hi I am trying to prove$$ I:=\int_0^{\pi/2} \frac{\sin^3 x\log \sin x}{\sqrt{1+\sin^2 x}}dx=\frac{\ln 2 -1}{4}. $$ Thanks. I am possibly trying to simplify this to obtain something like $2\int_0^{\pi/2} \log \sin x\, dx=-\pi \ln 2 $ since this is easily integrable.  However when I try to simplify the terms  $$ \frac{\sin^3 x}{\sqrt {1+\sin^2 x}} $$ I obtain a more complicated integrand.  I am not sure how else to go about this one.  I was trying to possibly write  $$ I(a)=\int_0^{\pi/2} \frac{\sin^3 a x\log \sin x}{\sqrt{1+\sin^2 x}}dx,\quad I'(a)=\int_0^{\pi/2} \frac{\partial}{\partial a}\left(\frac{\sin^3 ax\log \sin x}{\sqrt{1+\sin^2 x}}\right)\, dx, $$ but this didn't simplify anything for me. I also tried the substitution $y=\sin^2 x$, but couldn't manage to get an integral because of the $\sin 2x$ from the derivative.","Hi I am trying to prove$$ I:=\int_0^{\pi/2} \frac{\sin^3 x\log \sin x}{\sqrt{1+\sin^2 x}}dx=\frac{\ln 2 -1}{4}. $$ Thanks. I am possibly trying to simplify this to obtain something like $2\int_0^{\pi/2} \log \sin x\, dx=-\pi \ln 2 $ since this is easily integrable.  However when I try to simplify the terms  $$ \frac{\sin^3 x}{\sqrt {1+\sin^2 x}} $$ I obtain a more complicated integrand.  I am not sure how else to go about this one.  I was trying to possibly write  $$ I(a)=\int_0^{\pi/2} \frac{\sin^3 a x\log \sin x}{\sqrt{1+\sin^2 x}}dx,\quad I'(a)=\int_0^{\pi/2} \frac{\partial}{\partial a}\left(\frac{\sin^3 ax\log \sin x}{\sqrt{1+\sin^2 x}}\right)\, dx, $$ but this didn't simplify anything for me. I also tried the substitution $y=\sin^2 x$, but couldn't manage to get an integral because of the $\sin 2x$ from the derivative.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
12,"Function that decays faster than any polynomial, but not in the Schwartz space?","Function that decays faster than any polynomial, but not in the Schwartz space?",,"Motivated by the very restrictive condition imposed in the definition of the Schwartz space, I was wondering about the following question. Is there a $C^\infty$ function that decays faster than any polynomial, but   whose derivatives do not? That is, we would like $|x^n f(x)|$ to be bounded for all $n$ and $x$, but for $|x^n f^{(k)}(x)|$ to be unbounded for all $n$ and $k>0$ as $x$ ranges over the reals. Unfortunately, I only really know one rapidly decaying function (the exponential), and it doesn't work here. Maybe if we tack on some oscillation, like $\sin(x^2)$, that would help?","Motivated by the very restrictive condition imposed in the definition of the Schwartz space, I was wondering about the following question. Is there a $C^\infty$ function that decays faster than any polynomial, but   whose derivatives do not? That is, we would like $|x^n f(x)|$ to be bounded for all $n$ and $x$, but for $|x^n f^{(k)}(x)|$ to be unbounded for all $n$ and $k>0$ as $x$ ranges over the reals. Unfortunately, I only really know one rapidly decaying function (the exponential), and it doesn't work here. Maybe if we tack on some oscillation, like $\sin(x^2)$, that would help?",,"['real-analysis', 'schwartz-space']"
13,"composition of uniformly convergence sequence with continuous function, is uniformly convergence?","composition of uniformly convergence sequence with continuous function, is uniformly convergence?",,"Let $(f_n)$ be a series of functions in $C[0,1]$ that uniformly converge to a continuous function $f\in C[0,1]$. a. Let $g: [0,1]\to [0,1]$ be a continuous function. Is it true that $f_n\circ g$ uniformly converges to $f\circ g$? b. If no, what about the special case $g(x)=x^2$? It would also be differentiable, have bounded derivative, etc. About my thoughts: Intuitively I find it logical that a. would be true, and intuitively I'd guess that in order to prove such claim I'd have to go directly to the $\epsilon-\delta$ definition.","Let $(f_n)$ be a series of functions in $C[0,1]$ that uniformly converge to a continuous function $f\in C[0,1]$. a. Let $g: [0,1]\to [0,1]$ be a continuous function. Is it true that $f_n\circ g$ uniformly converges to $f\circ g$? b. If no, what about the special case $g(x)=x^2$? It would also be differentiable, have bounded derivative, etc. About my thoughts: Intuitively I find it logical that a. would be true, and intuitively I'd guess that in order to prove such claim I'd have to go directly to the $\epsilon-\delta$ definition.",,"['real-analysis', 'functions']"
14,How to solve $\displaystyle\lim_{n\to\infty}\int_0^3\underbrace{\sin(\frac{\pi}{3}\sin(\frac{\pi}{3}...\sin(\frac{\pi}{3} x)...))}_\text{n sines}dx$?,How to solve ?,\displaystyle\lim_{n\to\infty}\int_0^3\underbrace{\sin(\frac{\pi}{3}\sin(\frac{\pi}{3}...\sin(\frac{\pi}{3} x)...))}_\text{n sines}dx,"I saw this problem: Find $\lim\limits_{n \to \infty }\int_0^3 \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin \left(\frac{\pi}{3}  \dots \sin\left(\frac{\pi}{3} x\right) \dots \right)\right)\right)}_\text{n times of sines}dx$ I tried to solve it, but I have no idea if my solution is correct or wrong. My attempt: $$I_n:=\int_0^3 \underbrace{\sin\left(\frac\pi3 \sin\left(\frac\pi3\dots(\frac{\pi}{3}\sin\left(\frac\pi3x\right) \dots \right)\right)}_\text{n sines} dx        $$ Let $x=\frac{3}{\pi }t$ , then $$I_n =\frac{3}{\pi}\int_0^\pi \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\frac{\pi}{3}\sin\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt$$ $$=\frac{6}{\pi}\int_0^{\frac{\pi}{2}}\underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\sin\frac{\pi}{3}\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt$$ For all $t \in (0, \frac{\pi}{2})$ define $f_0(t) =t$ and $f_n(t)= \sin (\frac{\pi}{3}f_{n-1}(t))$ . If $\sin(t)= \frac{1}{2}$ , then $f_n(t) = \frac{1}{2} \ \forall n \in \mathbb{N}$ .  If $\sin(t) > \frac{1}{2}= \frac{1}{2} +\varepsilon $ for some $\varepsilon >0 $ , then $\sin(\frac{\pi}{3} \sin(t))= \sin (\frac{\pi}{6}+\frac{\pi}{3} \varepsilon) = \frac{1}{2} \cos(\frac{\pi}{3} \varepsilon) +\frac{\sqrt{3}}{2} \sin(\frac{\pi}{3}  \varepsilon) < \frac{1}{2}+\frac{\pi \varepsilon }{2\sqrt{3}} < \frac{1}{2}+ \varepsilon $ which means that the sequence is monotone decreasing bounded below by $\frac{1}{2}$ (This is the same way to prove that the sequence of all $t$ st $\sin(t) <\frac{1}{2}$ is increasing and bounded above by $\frac{1}{2}$ .), So by monotone convergence theorem the limit exist and $f(x)=\lim\limits_{n \to \infty} f_n(x)$ since $f_n(x) =\sin(\frac{\pi}{3} f_{n-1}(x)) $ $f(x) =\sin(\frac{\pi}{3} f(x)) $ then  it is easy to guess that $f(x)= \frac{1}{2}$ the limit is $\frac{1}{2}$ then $f_n(t) \to \frac{1}{2}\  \forall t $ the last part is to prove that the sequence $f_n$ is uniformly convergent and this can be shown by Dini’s Theorem:- Suppose that $f_n$ is a monotone sequence of continuous functions on $I := [a, b]$ that converges on $I$ to a continuous function $f$ . Then the convergence of the sequence is uniform. The sequences $f_n(x)$ is monotone increasing when $x\in (0, \frac{\pi}{6})$ , and monotone increasing if $x\in (\frac{\pi}{6},\frac{\pi}{2})$ , then $$ \lim\limits_{n \to \infty} I_n =\frac{6}{\pi} \lim\limits_{n \to \infty} \int_0 ^{\frac{\pi}{2}} f_n(x)dx= \frac{6}{\pi}  \lim\limits_{n \to \infty}\left(  \int_0 ^{\frac{\pi}{6}} f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} f_n(x)dx \right) $$ $$= \frac{6}{\pi}   \left(  \int_0 ^{\frac{\pi}{6}} \lim\limits_{n \to \infty}f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} \lim\limits_{n \to \infty}f_n(x)dx \right) =\frac{6}{\pi}   \left(\frac{1}{2}\left( \frac{\pi}{2} \right) \right)=\frac{3}{2}$$ Is my solution correct? If it is not, where did I make a mistake and how can I solve this integral? What are other ways to solve this?","I saw this problem: Find I tried to solve it, but I have no idea if my solution is correct or wrong. My attempt: Let , then For all define and . If , then .  If for some , then which means that the sequence is monotone decreasing bounded below by (This is the same way to prove that the sequence of all st is increasing and bounded above by .), So by monotone convergence theorem the limit exist and since then  it is easy to guess that the limit is then the last part is to prove that the sequence is uniformly convergent and this can be shown by Dini’s Theorem:- Suppose that is a monotone sequence of continuous functions on that converges on to a continuous function . Then the convergence of the sequence is uniform. The sequences is monotone increasing when , and monotone increasing if , then Is my solution correct? If it is not, where did I make a mistake and how can I solve this integral? What are other ways to solve this?","\lim\limits_{n \to \infty }\int_0^3 \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin \left(\frac{\pi}{3}  \dots \sin\left(\frac{\pi}{3} x\right) \dots \right)\right)\right)}_\text{n times of sines}dx I_n:=\int_0^3 \underbrace{\sin\left(\frac\pi3 \sin\left(\frac\pi3\dots(\frac{\pi}{3}\sin\left(\frac\pi3x\right) \dots \right)\right)}_\text{n sines} dx         x=\frac{3}{\pi }t I_n =\frac{3}{\pi}\int_0^\pi \underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\frac{\pi}{3}\sin\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt =\frac{6}{\pi}\int_0^{\frac{\pi}{2}}\underbrace{\sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \sin\left(\frac{\pi}{3} \dots \frac{\pi}{3}\sin\left(\sin\frac{\pi}{3}\left(t\right)\right) \dots \right)\right)\right)}_\text{n sines}dt t \in (0, \frac{\pi}{2}) f_0(t) =t f_n(t)= \sin (\frac{\pi}{3}f_{n-1}(t)) \sin(t)= \frac{1}{2} f_n(t) = \frac{1}{2} \ \forall n \in \mathbb{N} \sin(t) > \frac{1}{2}= \frac{1}{2} +\varepsilon  \varepsilon >0  \sin(\frac{\pi}{3} \sin(t))= \sin (\frac{\pi}{6}+\frac{\pi}{3} \varepsilon) = \frac{1}{2} \cos(\frac{\pi}{3} \varepsilon) +\frac{\sqrt{3}}{2} \sin(\frac{\pi}{3}  \varepsilon) < \frac{1}{2}+\frac{\pi \varepsilon }{2\sqrt{3}} < \frac{1}{2}+ \varepsilon  \frac{1}{2} t \sin(t) <\frac{1}{2} \frac{1}{2} f(x)=\lim\limits_{n \to \infty} f_n(x) f_n(x) =\sin(\frac{\pi}{3} f_{n-1}(x))  f(x) =\sin(\frac{\pi}{3} f(x))  f(x)= \frac{1}{2} \frac{1}{2} f_n(t) \to \frac{1}{2}\  \forall t  f_n f_n I := [a, b] I f f_n(x) x\in (0, \frac{\pi}{6}) x\in (\frac{\pi}{6},\frac{\pi}{2})  \lim\limits_{n \to \infty} I_n =\frac{6}{\pi} \lim\limits_{n \to \infty} \int_0 ^{\frac{\pi}{2}} f_n(x)dx= \frac{6}{\pi}
 \lim\limits_{n \to \infty}\left(  \int_0 ^{\frac{\pi}{6}} f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} f_n(x)dx \right)  = \frac{6}{\pi} 
 \left(  \int_0 ^{\frac{\pi}{6}} \lim\limits_{n \to \infty}f_n(x)dx+\int_{\frac{\pi}{6}} ^{\frac{\pi}{2}} \lim\limits_{n \to \infty}f_n(x)dx \right) =\frac{6}{\pi} 
 \left(\frac{1}{2}\left( \frac{\pi}{2} \right) \right)=\frac{3}{2}","['real-analysis', 'calculus', 'integration', 'limits', 'definite-integrals']"
15,Nonstandard infinite / hyperfinite sum in IST,Nonstandard infinite / hyperfinite sum in IST,,"TLDR: If anyone could provide a detailed proof that a sum indexed by an unlimited hypernatural number is well-defined using the axioms of IST , I would greatly appreciate it. I am studying Nelson's ""Radically Elementary Probability Theory"" and I am having some trouble. Namely, after introducing the axioms of IST, Nelson proves Theorem 5.3 where the expression $$ \sum_{i=1}^n x_i  $$ arises (in the statement of the theorem), and he remarks afterwards that ""there is no requirement that $n$ be limited"". However, this is quite confusing to me, as so far he hasn't defined what an unlimited sum would mean. I have seen posts like this one: How are infinite sums in nonstandard analysis defined? that talk about this same issue, but these posts do not specify which framework of NSA they are working in, and, from the references to Keisler's book in the comments, I think they are working in a different framework than Nelson's. With that said, the above post does mention the transfer principle, which I know is also an axiom of IST. What troubles me, however, is that the specifics of the justification are glossed over, and all of the interpretations I can come up with are disturbing. These are some interpretations I came up with (I will use $\mathbb{N}$ to denote the standard naturals and $\mathbb{N}^*$ to denote the hyperrnaturals; though $\mathbb{N}$ is not a set, this is ok because in the nonstandard setting we will only be using the notation $n \in \mathbb{N}$ as a shorthand for "" $n$ is standard""): We define a function $S: \mathbb{N} \to \mathbb{R}^*$ that is the partial sum function with respect to a priorly fixed hyperreal sequence $(a_n)_{n \in \mathbb{N}^*}$ . Then $S$ is defined for every $n \in \mathbb{N}$ , and (I'm not sure if this is correct but I'm guessing so) extends to some function defined on $\mathbb{N}^*$ . You begin in the classical universe and fix a sequence $(a_n)_{n \in \mathbb{N}}$ . Define its corresponding partial sum function $S$ . Then $(a_n)_{n \in \mathbb{N}}$ admits a (unique?) extension to the sequence $(a_n)_{n \in \mathbb{N}^*}$ after applying transfer. Then $S$ also admits a corresponding extension $S^*$ . You begin in the classical universe and define a function $S: \mathbb{N} \times \{\mathrm{all} \ \mathrm{real} \ \mathrm{sequences} \} \to \mathbb{R}$ and then take its transfer. The reasons I find these disturbing/problematic are as follows: You need to fix a sequence indexed by the hyper naturals in order to define a function from the classical naturals to the hyperreals. There is a mixing of universes all over the place here and that seems very wrong. One sequence indexed by the naturals (e.g., $(0)_{n \in \mathbb{N}}$ ) can easily be extended to different sequences indexed by the hypernaturals (e.g., $(0)_{n \in \mathbb{N}^*}$ and $(\delta_{n, \nu})_{n \in \mathbb{N}^*}$ where $\delta_{n, \nu}$ is the Kronecker $\delta$ with $\nu$ an unlimited hypernatural). This would mean the extension of $S$ cannot possibly be unique, and we have to choose an extension that fits the sequence whose partial sums we are trying to determine exist. You are defining a function with a set of sets as a domain, and as far as I vaguely understand, this is not allowed in ""first-order logic"" (I don't know what this means besides that it is some restriction on what quantifiers you can use), and the transfer principle is (or should?) only work for first-order statements. P.S. I am studying this book because I am interested in studying Herzberg's book on Stochastic calculus (which also glosses over what this sum would mean). I am not so much interested in logic/model theory, but the existence of this sum is crucial to the ability to define probability measures point wise on infinitesimal meshes, and so I would really like to know why we can do this.","TLDR: If anyone could provide a detailed proof that a sum indexed by an unlimited hypernatural number is well-defined using the axioms of IST , I would greatly appreciate it. I am studying Nelson's ""Radically Elementary Probability Theory"" and I am having some trouble. Namely, after introducing the axioms of IST, Nelson proves Theorem 5.3 where the expression arises (in the statement of the theorem), and he remarks afterwards that ""there is no requirement that be limited"". However, this is quite confusing to me, as so far he hasn't defined what an unlimited sum would mean. I have seen posts like this one: How are infinite sums in nonstandard analysis defined? that talk about this same issue, but these posts do not specify which framework of NSA they are working in, and, from the references to Keisler's book in the comments, I think they are working in a different framework than Nelson's. With that said, the above post does mention the transfer principle, which I know is also an axiom of IST. What troubles me, however, is that the specifics of the justification are glossed over, and all of the interpretations I can come up with are disturbing. These are some interpretations I came up with (I will use to denote the standard naturals and to denote the hyperrnaturals; though is not a set, this is ok because in the nonstandard setting we will only be using the notation as a shorthand for "" is standard""): We define a function that is the partial sum function with respect to a priorly fixed hyperreal sequence . Then is defined for every , and (I'm not sure if this is correct but I'm guessing so) extends to some function defined on . You begin in the classical universe and fix a sequence . Define its corresponding partial sum function . Then admits a (unique?) extension to the sequence after applying transfer. Then also admits a corresponding extension . You begin in the classical universe and define a function and then take its transfer. The reasons I find these disturbing/problematic are as follows: You need to fix a sequence indexed by the hyper naturals in order to define a function from the classical naturals to the hyperreals. There is a mixing of universes all over the place here and that seems very wrong. One sequence indexed by the naturals (e.g., ) can easily be extended to different sequences indexed by the hypernaturals (e.g., and where is the Kronecker with an unlimited hypernatural). This would mean the extension of cannot possibly be unique, and we have to choose an extension that fits the sequence whose partial sums we are trying to determine exist. You are defining a function with a set of sets as a domain, and as far as I vaguely understand, this is not allowed in ""first-order logic"" (I don't know what this means besides that it is some restriction on what quantifiers you can use), and the transfer principle is (or should?) only work for first-order statements. P.S. I am studying this book because I am interested in studying Herzberg's book on Stochastic calculus (which also glosses over what this sum would mean). I am not so much interested in logic/model theory, but the existence of this sum is crucial to the ability to define probability measures point wise on infinitesimal meshes, and so I would really like to know why we can do this.","
\sum_{i=1}^n x_i 
 n \mathbb{N} \mathbb{N}^* \mathbb{N} n \in \mathbb{N} n S: \mathbb{N} \to \mathbb{R}^* (a_n)_{n \in \mathbb{N}^*} S n \in \mathbb{N} \mathbb{N}^* (a_n)_{n \in \mathbb{N}} S (a_n)_{n \in \mathbb{N}} (a_n)_{n \in \mathbb{N}^*} S S^* S: \mathbb{N} \times \{\mathrm{all} \ \mathrm{real} \ \mathrm{sequences} \} \to \mathbb{R} (0)_{n \in \mathbb{N}} (0)_{n \in \mathbb{N}^*} (\delta_{n, \nu})_{n \in \mathbb{N}^*} \delta_{n, \nu} \delta \nu S","['real-analysis', 'sequences-and-series', 'logic', 'first-order-logic', 'nonstandard-analysis']"
16,Finding $\displaystyle \lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt$,Finding,\displaystyle \lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt,"To find the limit $$\lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt,$$ I attempted to use Laplace's method . We can express the given integral as $$\int_0^1 \exp \left(n \ln \left(\frac{\sin x}{x}\right)\right) dx$$ However, I encountered an issue with this approach. In Laplace's method, we require the function $f(x)=\ln \left(\frac{\sin x}{x}\right)$ to be twice differentiable. Additionally, the global maximum of $f(x)$ within the integration range must be unique and not located at the boundary points. Unfortunately, in this case, the maximum occurs at the boundary point $0$ , which prevents me from applying Laplace's method as intended. I have been thinking about this problem for quite some time, but I haven't been able to come up with any promising ideas. I would really appreciate any guidance or suggestions. Thank you in advance.","To find the limit I attempted to use Laplace's method . We can express the given integral as However, I encountered an issue with this approach. In Laplace's method, we require the function to be twice differentiable. Additionally, the global maximum of within the integration range must be unique and not located at the boundary points. Unfortunately, in this case, the maximum occurs at the boundary point , which prevents me from applying Laplace's method as intended. I have been thinking about this problem for quite some time, but I haven't been able to come up with any promising ideas. I would really appreciate any guidance or suggestions. Thank you in advance.","\lim_{n \rightarrow \infty} \sqrt{n} \int_0^1 \left(\frac{\sin t}{t}\right)^n dt, \int_0^1 \exp \left(n \ln \left(\frac{\sin x}{x}\right)\right) dx f(x)=\ln \left(\frac{\sin x}{x}\right) f(x) 0","['real-analysis', 'integration', 'asymptotics']"
17,"Need Help : Proving polynomials are continuous, without circular reasoning","Need Help : Proving polynomials are continuous, without circular reasoning",,"I know there are a lot of answers regarding continuity of polynomials. But, this question is different. We need to have $ \lim_{x\to a} {x^n} = a^n$ , $n \in N$ , to be able to prove that polynomials are continuous. This fact is derived from the product rule (or may be it can't be, which is my question). The product rule is proved using square roots, so it assumes the existence of square roots.  The fact that For every non negative number $x$ , it's $n^{th}$ root exists, i.e. $x^n$ is invertible, assumes the continuity of $x^n$ because this is proven using Intermediate value Theorem. Bam - Circular reasoning ! Or am I Wrong ?  Here's the only proof of product rule which I know : Assume $ \lim_{x\to a} {f(x)} = L$ and $ \lim_{x\to a} {g(x)} = K$ Let $ϵ > 0$ be any positive number Hence, $∃\delta_1> 0 ∶ 0<|x-a|<δ_1⟹|f(x)-L|<\sqrt{\epsilon}$ And $∃δ_2>0 ∶ 0<|x-a|<δ_2 ⟹ |g(x)-K|<\sqrt{\epsilon}$ Let $δ=\min\{δ_1,δ_2\}$ Hence, $0<|x-a|<δ$ ⟹ $|(f(x)-L)(g(x)-K)-0|<\sqrt{\epsilon} \sqrt{\epsilon} = ϵ$ Hence, $\lim_{x \to a} {(f(x)-L)(g(x)-K)} = 0$ ⟹ $\lim_{x \to a} {(f(x)g(x)-Kf(x)-Lg(x)+KL)} = 0$ And then the result follows.  Even if we use $\epsilon$ in place of $√ϵ$ , we end up with $0<|x-a|<δ⟹|(f(x)-L)(g(x)-K)-0|<\epsilon\cdot \epsilon = ϵ^2$ , and then we have to prove that the range of $\epsilon^2$ is $[0,\infty]$ , which amounts to proving that for each number in $[0,\infty]$ , a corresponding square root exists.","I know there are a lot of answers regarding continuity of polynomials. But, this question is different. We need to have , , to be able to prove that polynomials are continuous. This fact is derived from the product rule (or may be it can't be, which is my question). The product rule is proved using square roots, so it assumes the existence of square roots.  The fact that For every non negative number , it's root exists, i.e. is invertible, assumes the continuity of because this is proven using Intermediate value Theorem. Bam - Circular reasoning ! Or am I Wrong ?  Here's the only proof of product rule which I know : Assume and Let be any positive number Hence, And Let Hence, ⟹ Hence, ⟹ And then the result follows.  Even if we use in place of , we end up with , and then we have to prove that the range of is , which amounts to proving that for each number in , a corresponding square root exists."," \lim_{x\to a} {x^n} = a^n n \in N x n^{th} x^n x^n  \lim_{x\to a} {f(x)} = L  \lim_{x\to a} {g(x)} = K ϵ > 0 ∃\delta_1> 0 ∶ 0<|x-a|<δ_1⟹|f(x)-L|<\sqrt{\epsilon} ∃δ_2>0 ∶ 0<|x-a|<δ_2 ⟹ |g(x)-K|<\sqrt{\epsilon} δ=\min\{δ_1,δ_2\} 0<|x-a|<δ |(f(x)-L)(g(x)-K)-0|<\sqrt{\epsilon} \sqrt{\epsilon} = ϵ \lim_{x \to a} {(f(x)-L)(g(x)-K)} = 0 \lim_{x \to a} {(f(x)g(x)-Kf(x)-Lg(x)+KL)} = 0 \epsilon √ϵ 0<|x-a|<δ⟹|(f(x)-L)(g(x)-K)-0|<\epsilon\cdot \epsilon = ϵ^2 \epsilon^2 [0,\infty] [0,\infty]","['real-analysis', 'limits']"
18,What is the correct solution for the limit $\lim_{x\to\infty} x \sin\frac1x$?,What is the correct solution for the limit ?,\lim_{x\to\infty} x \sin\frac1x,"$$\lim_{x\to\infty} x \sin\left(\frac{1}{x}\right) = ?$$ Not a long ago I saw this function, and I was curious, what limit it has, when $x$ approaches $\infty$? Some of my friends said fast that it must approach $\infty$, since $\sin$ is a bounded function, and $x$ goes to infinity, therefore infinity * bounded must be infinity. Some others said that $\sin(\frac{1}{x})$ is $0$, since $\frac{1}{x}$ is $0$, when $x \rightarrow \infty$. So, the first possible solution should be $\infty$, but here is an other one. Let $y=\frac{1}{x}$. If $x \rightarrow \infty$, then $y \rightarrow 0$. Using that: $$\lim_{x\to\infty} x \sin\left(\frac{1}{x}\right) = \lim_{y\to 0} \frac {1}{y} \sin(y) = \lim_{y\to 0} \frac{\sin(y)}{y}  = 1$$ Here is a proof that, $\lim_{y\to 0} \frac{\sin(y)}{y}  = 1$: Proof So here we have $2$ completely different solutions for the same task, which both seem ""logical"". Is any of them correct, or if not, what should be the solution? Is this convergent, or divergent? Any help appreciated!","$$\lim_{x\to\infty} x \sin\left(\frac{1}{x}\right) = ?$$ Not a long ago I saw this function, and I was curious, what limit it has, when $x$ approaches $\infty$? Some of my friends said fast that it must approach $\infty$, since $\sin$ is a bounded function, and $x$ goes to infinity, therefore infinity * bounded must be infinity. Some others said that $\sin(\frac{1}{x})$ is $0$, since $\frac{1}{x}$ is $0$, when $x \rightarrow \infty$. So, the first possible solution should be $\infty$, but here is an other one. Let $y=\frac{1}{x}$. If $x \rightarrow \infty$, then $y \rightarrow 0$. Using that: $$\lim_{x\to\infty} x \sin\left(\frac{1}{x}\right) = \lim_{y\to 0} \frac {1}{y} \sin(y) = \lim_{y\to 0} \frac{\sin(y)}{y}  = 1$$ Here is a proof that, $\lim_{y\to 0} \frac{\sin(y)}{y}  = 1$: Proof So here we have $2$ completely different solutions for the same task, which both seem ""logical"". Is any of them correct, or if not, what should be the solution? Is this convergent, or divergent? Any help appreciated!",,"['real-analysis', 'limits', 'trigonometry']"
19,"Why isn't $f(x) = x\cos\frac{\pi}{x}$ differentiable at $x=0$, and how do we foresee it?","Why isn't  differentiable at , and how do we foresee it?",f(x) = x\cos\frac{\pi}{x} x=0,"Consider $$f(x)=\begin{cases} x\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\  0 & \text{for} \ x=0. \end{cases} $$ Its difference quotient $\frac{\Delta\left(f(x)\right)}{\Delta(x)}$ approaches $\cos\frac{\pi}{h}$ as $x$ gets closer to $0$, and thus $f$ is not differentiable in the origin because $\lim\limits_{h\to0}\cos\frac{\pi}{h}$ does not exist. This is the plot of $y=x \cos \frac{\pi}{x}$: But here's how my book goes on: Examining the figure we can foresee that the tangent line in a generic point $P$ of the graph doesn't tend to any limiting position as $P$ tends to the origin along the curve itself. One may think this happens because the graph of the function completes infinitely many oscillations in any neighbourhood of the origin. In fact, no: indeed the function thus defined: $$g(x)=\begin{cases} x^2\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\  0 & \text{for} \ x=0 \end{cases} $$   has a graph that completes infinitely many oscillations in any neighbourhood of the origin, but, as you can verify, it is differentiable at $x=0$ and we have $g'(0)=0$. This is the plot of $y=x^2 \cos \frac{\pi}{x}$: So, I have two questions related to what I quoted from the book: how do we foresee the non-differentiability of $f$, given that, correctly, the infinitude of the oscillations is not an argument for it? And then, why isn't $f$ differentiable, instead of $g$? I shall emphasise that I know that, simply, the limit as $h\to 0$ of the difference ratio of $f$ doesn't exist, while that of $g$ does, but I've been wondering about an other kind of reason after reading that excerpt. Or is my book wrong in mentioning other reasons?","Consider $$f(x)=\begin{cases} x\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\  0 & \text{for} \ x=0. \end{cases} $$ Its difference quotient $\frac{\Delta\left(f(x)\right)}{\Delta(x)}$ approaches $\cos\frac{\pi}{h}$ as $x$ gets closer to $0$, and thus $f$ is not differentiable in the origin because $\lim\limits_{h\to0}\cos\frac{\pi}{h}$ does not exist. This is the plot of $y=x \cos \frac{\pi}{x}$: But here's how my book goes on: Examining the figure we can foresee that the tangent line in a generic point $P$ of the graph doesn't tend to any limiting position as $P$ tends to the origin along the curve itself. One may think this happens because the graph of the function completes infinitely many oscillations in any neighbourhood of the origin. In fact, no: indeed the function thus defined: $$g(x)=\begin{cases} x^2\cos\frac{\pi}{x} & \text{for} \ x\ne0 \\  0 & \text{for} \ x=0 \end{cases} $$   has a graph that completes infinitely many oscillations in any neighbourhood of the origin, but, as you can verify, it is differentiable at $x=0$ and we have $g'(0)=0$. This is the plot of $y=x^2 \cos \frac{\pi}{x}$: So, I have two questions related to what I quoted from the book: how do we foresee the non-differentiability of $f$, given that, correctly, the infinitude of the oscillations is not an argument for it? And then, why isn't $f$ differentiable, instead of $g$? I shall emphasise that I know that, simply, the limit as $h\to 0$ of the difference ratio of $f$ doesn't exist, while that of $g$ does, but I've been wondering about an other kind of reason after reading that excerpt. Or is my book wrong in mentioning other reasons?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
20,"Why $ \int^{+\infty}_{-\infty} x \, dx \neq 0 $",Why," \int^{+\infty}_{-\infty} x \, dx \neq 0 ","We are going over improper integrals and tests for convergence in my Calc II course. During a lecture, my professor warned us to take caution when taking integrals from negative infinity to infinity. His specific example was: $$ \int^{+\infty}_{-\infty} x \, dx $$ I understand that the function is definitely not convergent, but my current intuition would be that the integral from $-\infty \rightarrow 0$ would be equal to $-\infty$, and likewise the part from $0 \rightarrow \infty$ would be $+\infty$, so one could think that the negative and positive parts would cancel out, rendering the integral to equal $0$. But, the prof made it very clear that this is not the case, and that we will learn how to deal with ""nasty"" integrals like this in future courses on real analysis. I have been trying to find a fairly simple explanation that a first year undergrad like myself would actually understand, but no luck this far. I still kind of do not accept this as being not equal to zero, as both of the parts grow as $O(x^{-1})$, so I can't understand why they do not cancel out. Any insight on this would be appreciated. NB: I understand that if one blindly follows the rule that both must converge for the overall integral to converge. But, I am the kind of person that always wonders why something is.","We are going over improper integrals and tests for convergence in my Calc II course. During a lecture, my professor warned us to take caution when taking integrals from negative infinity to infinity. His specific example was: $$ \int^{+\infty}_{-\infty} x \, dx $$ I understand that the function is definitely not convergent, but my current intuition would be that the integral from $-\infty \rightarrow 0$ would be equal to $-\infty$, and likewise the part from $0 \rightarrow \infty$ would be $+\infty$, so one could think that the negative and positive parts would cancel out, rendering the integral to equal $0$. But, the prof made it very clear that this is not the case, and that we will learn how to deal with ""nasty"" integrals like this in future courses on real analysis. I have been trying to find a fairly simple explanation that a first year undergrad like myself would actually understand, but no luck this far. I still kind of do not accept this as being not equal to zero, as both of the parts grow as $O(x^{-1})$, so I can't understand why they do not cancel out. Any insight on this would be appreciated. NB: I understand that if one blindly follows the rule that both must converge for the overall integral to converge. But, I am the kind of person that always wonders why something is.",,"['calculus', 'real-analysis', 'convergence-divergence', 'improper-integrals']"
21,Counterexample to the chain-rule,Counterexample to the chain-rule,,"I made the following observation Let $f(t):=\left(\begin{matrix} 0 &e^{it} \\ e^{-it} & 0 \end{matrix}\right),$ then $f(t)^2= \operatorname{id}$. Thus, we have $\frac{d}{dt}f(t)^2= \frac{d}{dt}\operatorname{id}=0.$ On the other hand yields the chain-rule $$\frac{d}{dt}f(t)^2= 2 f(t)f'(t)=0.$$ However, $f(t)$ and $f'(t)$ are both matrices with full-rank. So something is very wrong here, no? Can anybody explain to me what just happened?","I made the following observation Let $f(t):=\left(\begin{matrix} 0 &e^{it} \\ e^{-it} & 0 \end{matrix}\right),$ then $f(t)^2= \operatorname{id}$. Thus, we have $\frac{d}{dt}f(t)^2= \frac{d}{dt}\operatorname{id}=0.$ On the other hand yields the chain-rule $$\frac{d}{dt}f(t)^2= 2 f(t)f'(t)=0.$$ However, $f(t)$ and $f'(t)$ are both matrices with full-rank. So something is very wrong here, no? Can anybody explain to me what just happened?",,"['calculus', 'real-analysis']"
22,Example of a continuous function with a discontinuous inverse,Example of a continuous function with a discontinuous inverse,,What is an example of a function $f: \Bbb R^n \rightarrow \Bbb R^m$ such that $f$ is continuous and injective but that $f^{-1}$ is not continuous. Our professor teased us with the notion but I haven't been able to think of such a function.,What is an example of a function $f: \Bbb R^n \rightarrow \Bbb R^m$ such that $f$ is continuous and injective but that $f^{-1}$ is not continuous. Our professor teased us with the notion but I haven't been able to think of such a function.,,"['real-analysis', 'continuity', 'examples-counterexamples', 'inverse']"
23,Book Recommendations and Proofs for a First Course in Real Analysis,Book Recommendations and Proofs for a First Course in Real Analysis,,I am taking real analysis in university. I find that it is difficult to prove some certain questions. What I want to ask is: How do we come out with a proof? Do we use some intuitive idea first and then write it down formally? What books do you recommended for an undergraduate who is studying real analysis? Are there any books which explain the motivation of theorems?,I am taking real analysis in university. I find that it is difficult to prove some certain questions. What I want to ask is: How do we come out with a proof? Do we use some intuitive idea first and then write it down formally? What books do you recommended for an undergraduate who is studying real analysis? Are there any books which explain the motivation of theorems?,,"['real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
24,Set of points of continuity are $G_{\delta}$,Set of points of continuity are,G_{\delta},"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a function.  Show that the points at which $f$ is continuous is a $G_{\delta}$ set. $$A_n = \{ x \in \mathbb{R} | x \in B(x,r) \text{ open }, f(x'')-f(x')<\frac{1}{n}, \forall x',x'' \in B(x)\}$$ I saw that this proof was already on here, but I wanted to confirm and flesh out more details. ""$\Rightarrow$"" If f is continuous at $x$, then $f(x'')-f(x')<\frac{1}{n}$ for $x'',x' \in B(x, r_{n})$.  That is, there is a ball of radius $r$ where $r$ depends on $n$.  Then $x \in A_n$ and thus $x \in \cap A_n$. ""$\Leftarrow$"" If $x \in \cap A_n$, then there is an $\epsilon > 0$ and a $\delta > 0$ such that $x' , x'' \in B(x, \delta_n)$ for all $n$ and $$|f(x'')-f(x')|<\epsilon.$$ Take $\epsilon = \frac{1}{n}$.","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a function.  Show that the points at which $f$ is continuous is a $G_{\delta}$ set. $$A_n = \{ x \in \mathbb{R} | x \in B(x,r) \text{ open }, f(x'')-f(x')<\frac{1}{n}, \forall x',x'' \in B(x)\}$$ I saw that this proof was already on here, but I wanted to confirm and flesh out more details. ""$\Rightarrow$"" If f is continuous at $x$, then $f(x'')-f(x')<\frac{1}{n}$ for $x'',x' \in B(x, r_{n})$.  That is, there is a ball of radius $r$ where $r$ depends on $n$.  Then $x \in A_n$ and thus $x \in \cap A_n$. ""$\Leftarrow$"" If $x \in \cap A_n$, then there is an $\epsilon > 0$ and a $\delta > 0$ such that $x' , x'' \in B(x, \delta_n)$ for all $n$ and $$|f(x'')-f(x')|<\epsilon.$$ Take $\epsilon = \frac{1}{n}$.",,"['real-analysis', 'measure-theory']"
25,Construct a monotone function which has countably many discontinuities,Construct a monotone function which has countably many discontinuities,,"I read in a textbook, which had seemed to have other dubious errors, that one may construct a monotone function with discontinuities at every point in a countable set $C \subset [a,b]$ by enumerating the points as $c_1, c_2, \dots$ and defining $f(x) = \sum_{c_n < x}2^{-n}$. However, if seems that if we let $[a,b] = [0,1], C = \mathbb{Q} \cap [0,1]$, then $f(x)$ is constant $1$ everywhere except 0, an apparent counterexample. So my question is: how does one construct a monotonic function which has discontinuities precisely on a countable set $C$? Further, are there any relatively easy-to-visualize constructions?","I read in a textbook, which had seemed to have other dubious errors, that one may construct a monotone function with discontinuities at every point in a countable set $C \subset [a,b]$ by enumerating the points as $c_1, c_2, \dots$ and defining $f(x) = \sum_{c_n < x}2^{-n}$. However, if seems that if we let $[a,b] = [0,1], C = \mathbb{Q} \cap [0,1]$, then $f(x)$ is constant $1$ everywhere except 0, an apparent counterexample. So my question is: how does one construct a monotonic function which has discontinuities precisely on a countable set $C$? Further, are there any relatively easy-to-visualize constructions?",,"['real-analysis', 'functions', 'continuity', 'examples-counterexamples']"
26,"If $x_n \to 0 \pmod{a}$ for every $a>0$, does it follow that $x_n \to 0$?","If  for every , does it follow that ?",x_n \to 0 \pmod{a} a>0 x_n \to 0,"Let $x_n$ be a real-valued sequence such that $x_n \to 0\pmod{a}$ for every real $a>0$ : does it follow that $x_n \to 0$ ? Just so there is no ambiguity in the meaning of the question the hypothesis is that $$ \forall a>0 \; \forall \varepsilon>0 \; \exists n_0\in\mathbb{N} \; \forall n\geq n_0 \; (x_n \in \mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z})$$ where $$\mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z} := \{x\in\mathbb{R} \; : \; \exists u\in\mathbb{R}\; \exists k\in\mathbb{Z} \; (|u|<\varepsilon \; \land \; x=u+ka)\}$$ ( Edit: The question was initially posed with the notation “ $A$ ”: I changed this to $a$ to avoid a notation clash with the accepted answer which uses $A$ for something else (essentially the set of those $a$ ).) Addendum: The question has now been answered satisfactorily, but for completeness of MSE, let me still add some remarks that might help explain both whence the question is coming from and what the answer tells us. Suppose we're given a single $a>0$ , say $a>1$ for definiteness, and we want to find a sequence $x_n$ such that $x_n \to 0 \pmod{1}$ and $x_n \to 0 \pmod{a}$ yet $x_n \to +\infty$ in $\mathbb{R}$ ; in fact, let's even demand $x_n \equiv 0 \pmod{1}$ (i.e., $x_n\in\mathbb{Z}$ ).  Here's how one can do it: if $a$ is rational it's obvious (just take $x_n$ to be ever larger integers that are also multiples of $a$ ); and if $a$ is irrational, let $p_n/q_n$ be the convergents of its continued fraction: they satisfy $|q_n a-p_n| < \frac{1}{q_n}$ , which means that $p_n$ is both an integer, and also ever closer to $0$ mod $a$ , so we have $p_n \equiv 0 \pmod{a}$ and $p_n \to 0 \pmod{a}$ , yet $p_n \to +\infty$ in $\mathbb{R}$ , as promised. ∎ (For example, the sequence 1, 3, 7, 17, 41, 99, etc. tends to $0$ mod $\sqrt{2}$ , and is, of course, identically $0$ mod $1$ .) I suspect (thought I didn't check the details) that standard results on simultaneous Diophantine approximation (maybe applied to the $1/a$ : my $a$ is in some sense “backwards” in the question) will similarly give us a sequence $x_n$ tending to $+\infty$ in $\mathbb{R}$ but tending to $0$ mod finitely many $a$ 's (and exactly $0$ mod one).  The question was whether this “finitely many $a$ 's” can be strengthened to “all $a>0$ ”, and the accepted answer shows that it's not even possible for a dense $G_\delta$ of $a$ .","Let be a real-valued sequence such that for every real : does it follow that ? Just so there is no ambiguity in the meaning of the question the hypothesis is that where ( Edit: The question was initially posed with the notation “ ”: I changed this to to avoid a notation clash with the accepted answer which uses for something else (essentially the set of those ).) Addendum: The question has now been answered satisfactorily, but for completeness of MSE, let me still add some remarks that might help explain both whence the question is coming from and what the answer tells us. Suppose we're given a single , say for definiteness, and we want to find a sequence such that and yet in ; in fact, let's even demand (i.e., ).  Here's how one can do it: if is rational it's obvious (just take to be ever larger integers that are also multiples of ); and if is irrational, let be the convergents of its continued fraction: they satisfy , which means that is both an integer, and also ever closer to mod , so we have and , yet in , as promised. ∎ (For example, the sequence 1, 3, 7, 17, 41, 99, etc. tends to mod , and is, of course, identically mod .) I suspect (thought I didn't check the details) that standard results on simultaneous Diophantine approximation (maybe applied to the : my is in some sense “backwards” in the question) will similarly give us a sequence tending to in but tending to mod finitely many 's (and exactly mod one).  The question was whether this “finitely many 's” can be strengthened to “all ”, and the accepted answer shows that it's not even possible for a dense of .","x_n x_n \to 0\pmod{a} a>0 x_n \to 0  \forall a>0 \; \forall \varepsilon>0 \; \exists n_0\in\mathbb{N} \; \forall n\geq n_0 \; (x_n \in \mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z}) \mathopen]-\varepsilon,+\varepsilon\mathclose[+a\mathbb{Z} := \{x\in\mathbb{R} \; : \; \exists u\in\mathbb{R}\; \exists k\in\mathbb{Z} \; (|u|<\varepsilon \; \land \; x=u+ka)\} A a A a a>0 a>1 x_n x_n \to 0 \pmod{1} x_n \to 0 \pmod{a} x_n \to +\infty \mathbb{R} x_n \equiv 0 \pmod{1} x_n\in\mathbb{Z} a x_n a a p_n/q_n |q_n a-p_n| < \frac{1}{q_n} p_n 0 a p_n \equiv 0 \pmod{a} p_n \to 0 \pmod{a} p_n \to +\infty \mathbb{R} 0 \sqrt{2} 0 1 1/a a x_n +\infty \mathbb{R} 0 a 0 a a>0 G_\delta a","['real-analysis', 'sequences-and-series', 'general-topology']"
27,How to evaluate $\lim_{x\to 0} \frac{x^2\sin {\frac{1}{x}}}{\sin x}$,How to evaluate,\lim_{x\to 0} \frac{x^2\sin {\frac{1}{x}}}{\sin x},"Find the value of $\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$. Below I am showing my attempt at the question: $x^2\sin {\dfrac{1}{x}}\to 0$ as $x\to 0$ since $\sin {\dfrac{1}{x}}$ is bounded in a neighbourhood of $0$ and $x^2\to 0$ as $x\to 0$. Hence , we have a $\dfrac{0}{0}$ form. By L'Hospital's Rule ,$\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$ reduces to  $\dfrac{2x\sin {\dfrac{1}{x}}-\cos{\dfrac{1}{x}}}{\cos x}$ whose limit can't be computed at $x=0$ since $\lim _{x\to 0} \cos{\dfrac{1}{x}}$ does not exist. How can I evaluate this correctly? Will the answer be limit does not exist ? Do excuse me if I am unable to post a good question as this is my first question on MSE","Find the value of $\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$. Below I am showing my attempt at the question: $x^2\sin {\dfrac{1}{x}}\to 0$ as $x\to 0$ since $\sin {\dfrac{1}{x}}$ is bounded in a neighbourhood of $0$ and $x^2\to 0$ as $x\to 0$. Hence , we have a $\dfrac{0}{0}$ form. By L'Hospital's Rule ,$\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$ reduces to  $\dfrac{2x\sin {\dfrac{1}{x}}-\cos{\dfrac{1}{x}}}{\cos x}$ whose limit can't be computed at $x=0$ since $\lim _{x\to 0} \cos{\dfrac{1}{x}}$ does not exist. How can I evaluate this correctly? Will the answer be limit does not exist ? Do excuse me if I am unable to post a good question as this is my first question on MSE",,"['real-analysis', 'limits', 'trigonometry']"
28,Function which takes every value uncountably often,Function which takes every value uncountably often,,"Is there a function such that for all $y\in \mathbb{R}$ there exist uncountably many $x\in\mathbb{R}$ with $f(x)=y$? A function for which countably many $x$ exist is for example $\tan$, but I fail to see how to take this a step further. So any help is highly appreciated.","Is there a function such that for all $y\in \mathbb{R}$ there exist uncountably many $x\in\mathbb{R}$ with $f(x)=y$? A function for which countably many $x$ exist is for example $\tan$, but I fail to see how to take this a step further. So any help is highly appreciated.",,"['real-analysis', 'analysis', 'elementary-set-theory']"
29,Evaluate $\sum\limits_{n=1}^{\infty} \frac{2^n}{1+2^{2^n}}$,Evaluate,\sum\limits_{n=1}^{\infty} \frac{2^n}{1+2^{2^n}},How to evaluate the infinite series: $$\sum\limits_{n=1}^{\infty} \frac{2^n}{1+2^{2^n}}$$,How to evaluate the infinite series: $$\sum\limits_{n=1}^{\infty} \frac{2^n}{1+2^{2^n}}$$,,"['real-analysis', 'sequences-and-series']"
30,Example of continuous but not absolutely continuous strictly increasing function,Example of continuous but not absolutely continuous strictly increasing function,,"Could one give an example of a strictly increasing, continuous but not absolutely continuous, function on $[0,1]$ into $[0,1]$ or on $[0,1)$ into $R$ or any of the related combinations of 1-d domain and range?","Could one give an example of a strictly increasing, continuous but not absolutely continuous, function on $[0,1]$ into $[0,1]$ or on $[0,1)$ into $R$ or any of the related combinations of 1-d domain and range?",,"['real-analysis', 'functions', 'examples-counterexamples', 'absolute-continuity']"
31,Divergence of $\sum\limits_{n=1}^{\infty} \frac{\cos(\log(n))}{n}$,Divergence of,\sum\limits_{n=1}^{\infty} \frac{\cos(\log(n))}{n},I'm struggling to prove that $$\sum \limits_{n=1}^{\infty} \frac{\cos(\log(n))}{n}$$ diverges. Does anyone have any idea how to prove it? Breaking it into smaller pieces din't work. Maybe I should bound it with another series? But how?,I'm struggling to prove that $$\sum \limits_{n=1}^{\infty} \frac{\cos(\log(n))}{n}$$ diverges. Does anyone have any idea how to prove it? Breaking it into smaller pieces din't work. Maybe I should bound it with another series? But how?,,"['real-analysis', 'sequences-and-series', 'limits', 'divergent-series']"
32,Proving the properties of polynomial functions without using calculus,Proving the properties of polynomial functions without using calculus,,"I have a little project of mine, where I'm basically trying to prove most of the properties of polynomial functions without using any calculus whatsoever. I've been able to show, using the completeness of the real numbers, that if a polynomial function $f: \mathbf{R} \rightarrow \mathbf{R}$ is such that $f(a) f(b) < 0$ for some reals $a < b$ , then there exists a real number $c \in (a, b)$ such that $f(c) = 0$ . Then, I defined the concept of the derivative polynomial: Let $f: \mathbf{R} \rightarrow \mathbf{R}$ the polynomial function given by $\displaystyle f(x) = \sum_{k=0}^n a_k x^k$ for all $x \in \mathbf{R}$ . We define the derivative of $f$ as the polynomial function $f' : \mathbf{R} \rightarrow \mathbf{R}$ given by $\displaystyle f'(x) = \sum_{k=1}^n k a_k x^{k-1}$ for all $x \in \mathbf{R}$ . I am trying to show, by elementary means, that if $a < b$ and $f'(x) > 0$ for all $x \in (a, b)$ , then $f$ is increasing on $(a, b)$ , but I have no idea on how to do that.","I have a little project of mine, where I'm basically trying to prove most of the properties of polynomial functions without using any calculus whatsoever. I've been able to show, using the completeness of the real numbers, that if a polynomial function is such that for some reals , then there exists a real number such that . Then, I defined the concept of the derivative polynomial: Let the polynomial function given by for all . We define the derivative of as the polynomial function given by for all . I am trying to show, by elementary means, that if and for all , then is increasing on , but I have no idea on how to do that.","f: \mathbf{R} \rightarrow \mathbf{R} f(a) f(b) < 0 a < b c \in (a, b) f(c) = 0 f: \mathbf{R} \rightarrow \mathbf{R} \displaystyle f(x) = \sum_{k=0}^n a_k x^k x \in \mathbf{R} f f' : \mathbf{R} \rightarrow \mathbf{R} \displaystyle f'(x) = \sum_{k=1}^n k a_k x^{k-1} x \in \mathbf{R} a < b f'(x) > 0 x \in (a, b) f (a, b)","['real-analysis', 'derivatives', 'polynomials', 'real-numbers']"
33,Prove subgaussian norm of sugaurssian random variables is a norm,Prove subgaussian norm of sugaurssian random variables is a norm,,I know how to prove the zero and scaling property of norm. However I'm stuck on proving triangle inequality. The definition of norm of sub-Gaussian random variable is. Sub-Gaussian random variable is such norm exists. $$\|X\|_{\psi_2}=\inf\{t>0:E e^{-\frac{X^2}{t^2}}\}$$,I know how to prove the zero and scaling property of norm. However I'm stuck on proving triangle inequality. The definition of norm of sub-Gaussian random variable is. Sub-Gaussian random variable is such norm exists. $$\|X\|_{\psi_2}=\inf\{t>0:E e^{-\frac{X^2}{t^2}}\}$$,,"['real-analysis', 'probability', 'functional-analysis']"
34,Minimize $\min_{f\in E}\left(\int_0^1f(x) dx\right)$,Minimize,\min_{f\in E}\left(\int_0^1f(x) dx\right),"Inspired by this question , I pose this following problem. Let $E$ be the set of all nonnegative continuous functions $f:[0,1]\to \mathbb{R}$ such that $$f(x)\,f(y)\ge |x-y|\qquad\forall\{x,y\}\subset [0,1]$$ Find $$\min_{f\in E}\left(\int_0^1f(x) \,dx\right)$$","Inspired by this question , I pose this following problem. Let be the set of all nonnegative continuous functions such that Find","E f:[0,1]\to \mathbb{R} f(x)\,f(y)\ge |x-y|\qquad\forall\{x,y\}\subset [0,1] \min_{f\in E}\left(\int_0^1f(x) \,dx\right)","['calculus', 'real-analysis', 'inequality', 'optimization', 'contest-math']"
35,Is the following limit finite ....?,Is the following limit finite ....?,,"I would like to see some clue for the following problem: Let $a_1=1$ and $a_n=1+\frac{1}{a_1}+\cdots+\frac{1}{a_{n-1}}$, $n>1$. Find $$ \lim_{n\to\infty}\left(a_n-\sqrt{2n}\right). $$","I would like to see some clue for the following problem: Let $a_1=1$ and $a_n=1+\frac{1}{a_1}+\cdots+\frac{1}{a_{n-1}}$, $n>1$. Find $$ \lim_{n\to\infty}\left(a_n-\sqrt{2n}\right). $$",,"['real-analysis', 'sequences-and-series', 'asymptotics']"
36,Are there sets of zero measure and full Hausdorff dimension?,Are there sets of zero measure and full Hausdorff dimension?,,"I would like to ask the following: Are there ""many"" sets, say in the interval $[0,1]$, with zero Lebesgue measure but with Hausdorff dimension $1$? The motivation for this question is the dichotomy between measure and category. There are certainly dense sets with zero Lebesgue measure. But a dense set need not have positive Hausdorff dimension (for example, the rationals are dense but have zero Hausdorff dimension). Honestly, I would already be satisfied with an answer to the following question: Is there any set in $[0,1]$ with zero Lebesgue measure but with Hausdorff dimension $1$?","I would like to ask the following: Are there ""many"" sets, say in the interval $[0,1]$, with zero Lebesgue measure but with Hausdorff dimension $1$? The motivation for this question is the dichotomy between measure and category. There are certainly dense sets with zero Lebesgue measure. But a dense set need not have positive Hausdorff dimension (for example, the rationals are dense but have zero Hausdorff dimension). Honestly, I would already be satisfied with an answer to the following question: Is there any set in $[0,1]$ with zero Lebesgue measure but with Hausdorff dimension $1$?",,"['real-analysis', 'measure-theory']"
37,Differences between real and complex analysis?,Differences between real and complex analysis?,,"To start with, real analysis deals with numbers along the (one dimensional) number line, while complex analysis deals with numbers along two dimensions, real and imaginary, Cartesian style. Could this be what causes some of the differences mentioned below? For instance real analysis deals with sequences of numbers, while complex analysis deals with series, which is the say, the sums of terms of sequences. Real analysis seems to be about limits and accumulation points and oriented toward differentiation, while complex analysis deals with with curve integrals and radii of convergence, and ""poles,"" lending itself more to integration. Is complex analysis to real analysis what polar coordinates are to linear coordinates, and does this help explain the differences in their orientations?","To start with, real analysis deals with numbers along the (one dimensional) number line, while complex analysis deals with numbers along two dimensions, real and imaginary, Cartesian style. Could this be what causes some of the differences mentioned below? For instance real analysis deals with sequences of numbers, while complex analysis deals with series, which is the say, the sums of terms of sequences. Real analysis seems to be about limits and accumulation points and oriented toward differentiation, while complex analysis deals with with curve integrals and radii of convergence, and ""poles,"" lending itself more to integration. Is complex analysis to real analysis what polar coordinates are to linear coordinates, and does this help explain the differences in their orientations?",,"['real-analysis', 'complex-analysis']"
38,Unconventional (but instructive) proofs of basic theorems of calculus,Unconventional (but instructive) proofs of basic theorems of calculus,,"Inspired by this questions asked on MathOverflow , I would like to ask if you know some ""sophisticated"" proofs of the basic theorems in a calculus course (that is, the ones that you can find, for instance, in Spivak's Calculus ). In this case, by ""sophisticated"", I do not mean awfully complicated, but unexpected, extremely clever and unconventional (and hopefully instructive), either because they use concepts from other areas of mathematics or because they enlighten a theorem by tackling it from a non-obvious and by no means standard(ized) perspective.","Inspired by this questions asked on MathOverflow , I would like to ask if you know some ""sophisticated"" proofs of the basic theorems in a calculus course (that is, the ones that you can find, for instance, in Spivak's Calculus ). In this case, by ""sophisticated"", I do not mean awfully complicated, but unexpected, extremely clever and unconventional (and hopefully instructive), either because they use concepts from other areas of mathematics or because they enlighten a theorem by tackling it from a non-obvious and by no means standard(ized) perspective.",,"['calculus', 'real-analysis', 'examples-counterexamples', 'big-list']"
39,Archimedean property concept,Archimedean property concept,,"I want to know what the ""big deal"" about the Archimedean property is. Abbott states it is an important fact about how $\Bbb Q$ fits inside $\Bbb R.$ First, I want to know if the following statements are true: The Archimedean property states that $\Bbb N$ isn't bounded above--some natural number can be found such that it is greater than some specified real number. The Archimedean property also states that there is some rational $\frac1n, n \in\Bbb N$ such that it is less than some specified real number. Secondly, what do the above statements imply about the connection between $\Bbb Q$ and $\Bbb R?$ Does it imply that $\Bbb R$ fills the gaps of $\Bbb Q$ and $\Bbb N;$ is it the proof for $\Bbb R$ completing $\Bbb Q?$ Lastly, what insights have you obtained from the Archimedean property? I am sorry if some of my questions are unclear.","I want to know what the ""big deal"" about the Archimedean property is. Abbott states it is an important fact about how $\Bbb Q$ fits inside $\Bbb R.$ First, I want to know if the following statements are true: The Archimedean property states that $\Bbb N$ isn't bounded above--some natural number can be found such that it is greater than some specified real number. The Archimedean property also states that there is some rational $\frac1n, n \in\Bbb N$ such that it is less than some specified real number. Secondly, what do the above statements imply about the connection between $\Bbb Q$ and $\Bbb R?$ Does it imply that $\Bbb R$ fills the gaps of $\Bbb Q$ and $\Bbb N;$ is it the proof for $\Bbb R$ completing $\Bbb Q?$ Lastly, what insights have you obtained from the Archimedean property? I am sorry if some of my questions are unclear.",,['real-analysis']
40,integral computed with respect to a sub-$\sigma$-algebra,integral computed with respect to a sub--algebra,\sigma,"Let $\mathcal M_0$ be a $\sigma$-algebra that is contained in a $\sigma$-algebra $\mathcal M$ of subsets of a set $X$, $\mu$ a measure on $\mathcal M$ and $\mu_0$ the restriction of $\mu$ to $\mathcal M_0$. Let $f$ be a nonnegative real-valued function that is measurable with respect to $\mathcal M_0$ (and hence with respect to $\mathcal M$ as well). Then the set of all nonnegative simple functions $\phi\le f$ measurable with respect to $\mathcal M_0$ is a subset of the set of all nonnegative simple functions $\psi\le f$ measurable with respect to $\mathcal M$, and so $$ \int_Xf\,d\mu_0\le\int_Xf\,d\mu. $$ Can this inequality be strict?","Let $\mathcal M_0$ be a $\sigma$-algebra that is contained in a $\sigma$-algebra $\mathcal M$ of subsets of a set $X$, $\mu$ a measure on $\mathcal M$ and $\mu_0$ the restriction of $\mu$ to $\mathcal M_0$. Let $f$ be a nonnegative real-valued function that is measurable with respect to $\mathcal M_0$ (and hence with respect to $\mathcal M$ as well). Then the set of all nonnegative simple functions $\phi\le f$ measurable with respect to $\mathcal M_0$ is a subset of the set of all nonnegative simple functions $\psi\le f$ measurable with respect to $\mathcal M$, and so $$ \int_Xf\,d\mu_0\le\int_Xf\,d\mu. $$ Can this inequality be strict?",,"['real-analysis', 'measure-theory']"
41,pointwise convergence and boundedness in norm imply weak convergence,pointwise convergence and boundedness in norm imply weak convergence,,"I am contemplating over the following exercise (in which $E=[0,1]$): Let $f_n$ be a sequence of functions in $L^p(E)$, $1<p<\infty$, which converge almost everywhere to a function $f$ in $L^p(E)$, and suppose that there is a constant $M$ such that $\|f_n\|_p\le M$ for all $n$. Then for each function $g$ in $L^q(E)$ (with $\frac1p+\frac1q=1$), we have    $$\int_E fg=\lim_{n\to\infty}\int f_n g.$$ If measure of $E$ is finite, one can make use of Egoroff's theorem and find a set $A$ of small enough measure such that $f_n$ converges uniformly to $f$ on $E\setminus A$, and $\int_A|g|^q<\epsilon'^q$. Thus the difference  $$ \left|\int_E fg - \int_E f_ng\right|\le\int_E |(f-f_n)g|=\int_A |(f-f_n)g|+\int_{E\setminus A}|(f-f_n)g| $$ $$ \le\text{[by Holder]}\le 2M\epsilon'+\|f-f_n\|_p\cdot\|g\|_q $$ can be made less than any desired $\epsilon$ for big enough $n$. My question is: does this result also hold true if the measure of $E$ is infinite?","I am contemplating over the following exercise (in which $E=[0,1]$): Let $f_n$ be a sequence of functions in $L^p(E)$, $1<p<\infty$, which converge almost everywhere to a function $f$ in $L^p(E)$, and suppose that there is a constant $M$ such that $\|f_n\|_p\le M$ for all $n$. Then for each function $g$ in $L^q(E)$ (with $\frac1p+\frac1q=1$), we have    $$\int_E fg=\lim_{n\to\infty}\int f_n g.$$ If measure of $E$ is finite, one can make use of Egoroff's theorem and find a set $A$ of small enough measure such that $f_n$ converges uniformly to $f$ on $E\setminus A$, and $\int_A|g|^q<\epsilon'^q$. Thus the difference  $$ \left|\int_E fg - \int_E f_ng\right|\le\int_E |(f-f_n)g|=\int_A |(f-f_n)g|+\int_{E\setminus A}|(f-f_n)g| $$ $$ \le\text{[by Holder]}\le 2M\epsilon'+\|f-f_n\|_p\cdot\|g\|_q $$ can be made less than any desired $\epsilon$ for big enough $n$. My question is: does this result also hold true if the measure of $E$ is infinite?",,"['real-analysis', 'lebesgue-integral']"
42,"Finding $\lim\limits_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n}$ for continuous $f:[0,1]\to[0,\infty)$ [duplicate]",Finding  for continuous  [duplicate],"\lim\limits_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n} f:[0,1]\to[0,\infty)","This question already has answers here : Limit of $L^p$ norm (4 answers) limit of Holder norms: $\sup\limits_{x\in [a,b]} f(x) = \lim\limits_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$ [duplicate] (1 answer) Closed 11 years ago . Find $$\lim_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n}$$if $f:[0,1]\rightarrow(0,\infty)$ is a continuous function. My attempt: Say $f(x)$ has a max. value $M$. Then $$\left(\int_0^1(f(x))^ndx\right)^\frac{1}{n}\leq\left(\int_0^1M^ndx\right)^\frac{1}{n} =M$$ I cannot figure out what to do next.","This question already has answers here : Limit of $L^p$ norm (4 answers) limit of Holder norms: $\sup\limits_{x\in [a,b]} f(x) = \lim\limits_{n\rightarrow\infty} \left(\int_a^b (f(x))^n \;dx\right)^{\frac{1}{n}}$ [duplicate] (1 answer) Closed 11 years ago . Find $$\lim_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n}$$if $f:[0,1]\rightarrow(0,\infty)$ is a continuous function. My attempt: Say $f(x)$ has a max. value $M$. Then $$\left(\int_0^1(f(x))^ndx\right)^\frac{1}{n}\leq\left(\int_0^1M^ndx\right)^\frac{1}{n} =M$$ I cannot figure out what to do next.",,['real-analysis']
43,If series $\sum a_n$ is convergent with positive terms does $\sum \sin a_n$ also converge?,If series  is convergent with positive terms does  also converge?,\sum a_n \sum \sin a_n,"If $\{a_n\}$ is a sequence of positive terms such that the series $$\sum_{n=1}^\infty a_n$$ coverges, does the series $$\sum_{n=1}^\infty \sin a_n$$ also converge? I believe that limit comparison test is necessary but I'm not sure how to use it here","If $\{a_n\}$ is a sequence of positive terms such that the series $$\sum_{n=1}^\infty a_n$$ coverges, does the series $$\sum_{n=1}^\infty \sin a_n$$ also converge? I believe that limit comparison test is necessary but I'm not sure how to use it here",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
44,Moving a limit inside an infinite sum,Moving a limit inside an infinite sum,,Is uniform convergence justification for moving a limit inside an infinite sum? I'm trying to evaluate $$ \lim_{n \to \infty} n \int_{0}^{1} \ln(1+x^{n}) \ dx . $$ I found that it equals $$\displaystyle\lim_{n \to \infty} \sum_{k=1}^{\infty} (-1)^{k+1} \frac{n}{k+k^2n}.$$ Can I move the limit inside the infinite sum and conclude $$ \lim_{n \to \infty} n \int_{0}^{1} \ln(1+x^{n}) \ dx = \sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k^{2}} = \frac{\pi^{2}}{12}?$$,Is uniform convergence justification for moving a limit inside an infinite sum? I'm trying to evaluate $$ \lim_{n \to \infty} n \int_{0}^{1} \ln(1+x^{n}) \ dx . $$ I found that it equals $$\displaystyle\lim_{n \to \infty} \sum_{k=1}^{\infty} (-1)^{k+1} \frac{n}{k+k^2n}.$$ Can I move the limit inside the infinite sum and conclude $$ \lim_{n \to \infty} n \int_{0}^{1} \ln(1+x^{n}) \ dx = \sum_{k=1}^{\infty} \frac{(-1)^{k+1}}{k^{2}} = \frac{\pi^{2}}{12}?$$,,"['calculus', 'real-analysis', 'sequences-and-series']"
45,Drawing by lifting pencil from paper can still beget continuous function.,Drawing by lifting pencil from paper can still beget continuous function.,,"From page 105 of the 1994 edition of Spivak's Calculus: A continuous function is sometimes described, intuitively, as one   whose graph can be drawn without lifting your pencil from the paper.   Consideration of the continuous function    $$ f(x) = \begin{cases} x \sin \frac 1x, & \text{if }x\neq 0 \\ x, & \text{if }x=0 \end{cases} $$   shows that this description is a little too   optimistic. What does Spivak mean? $f(x)$ can be drawn without lifting the pen, can't it? http://www.wolframalpha.com/input/?i=x+sin+(1%2Fx) (On the other hand, the function $x \mapsto x$ with domain $\mathbb R -\{0\}$ is clearly continuous but can't be drawn without lifting the pen.)","From page 105 of the 1994 edition of Spivak's Calculus: A continuous function is sometimes described, intuitively, as one   whose graph can be drawn without lifting your pencil from the paper.   Consideration of the continuous function    $$ f(x) = \begin{cases} x \sin \frac 1x, & \text{if }x\neq 0 \\ x, & \text{if }x=0 \end{cases} $$   shows that this description is a little too   optimistic. What does Spivak mean? $f(x)$ can be drawn without lifting the pen, can't it? http://www.wolframalpha.com/input/?i=x+sin+(1%2Fx) (On the other hand, the function $x \mapsto x$ with domain $\mathbb R -\{0\}$ is clearly continuous but can't be drawn without lifting the pen.)",,"['real-analysis', 'functions', 'continuity']"
46,"$\epsilon$-$\delta$ proof that $f(x) = x \sin(1/x)$, $x \ne 0$, is continuous","- proof that , , is continuous",\epsilon \delta f(x) = x \sin(1/x) x \ne 0,"I'm doing an exercise that asks me to prove that $f$ is continuous using a $\epsilon$-$\delta$ proof. I have that $$   f(x) = \begin{cases} x\cdot \sin \frac1x,&x\neq 0 \\ 0,&x = 0 \end{cases} $$ I've already managed to show this property for $x=0$. How can I show it for $x \ne 0$, also using a $\epsilon$-$\delta$ proof? Thank you very much.","I'm doing an exercise that asks me to prove that $f$ is continuous using a $\epsilon$-$\delta$ proof. I have that $$   f(x) = \begin{cases} x\cdot \sin \frac1x,&x\neq 0 \\ 0,&x = 0 \end{cases} $$ I've already managed to show this property for $x=0$. How can I show it for $x \ne 0$, also using a $\epsilon$-$\delta$ proof? Thank you very much.",,"['real-analysis', 'calculus', 'limits', 'continuity', 'epsilon-delta']"
47,Dedekind Cuts versus Cauchy Sequences,Dedekind Cuts versus Cauchy Sequences,,Are there any advantages or disadvantages in defining a real number in the following ways: Definition 1 A real number is an object of the form $\lim\limits_{n \to \infty} a_n$ where $(a_n)_{n}^{\infty}$ is a cauchy sequence of rational numbers. Definition 2 A real number is a cut in $\mathbb{Q}$.,Are there any advantages or disadvantages in defining a real number in the following ways: Definition 1 A real number is an object of the form $\lim\limits_{n \to \infty} a_n$ where $(a_n)_{n}^{\infty}$ is a cauchy sequence of rational numbers. Definition 2 A real number is a cut in $\mathbb{Q}$.,,['real-analysis']
48,Asymptotic behavior of a sequence of integrals of non-analytic functions,Asymptotic behavior of a sequence of integrals of non-analytic functions,,"I am interested in the asymptotic behavior of the sequence $(I_n)_{n=1}^\infty$ of integrals $$I_n=\int_0^1(1-x)^ne^{-1/x}\,dx.$$ Motivation: It is straightforward to show that, for any $f\in C[0,1]$ with $f(0)\neq0$ , $$\int_0^1(1-x)^nf(x)\,dx\sim\frac{f(0)}{n}.$$ (One method is to approximate $f$ by a $C^1$ function, then integrate by parts.) In fact, using essentially the same argument, one can show that for any $f\in C^\infty[0,1]$ , if there is a $k\in\mathbb{N}$ such that $f^{(k)}(0)\neq0$ and $f(0),f'(0),\ldots,f^{(k-1)}(0)=0$ , then $$\int_0^1(1-x)^nf(x)\,dx\sim\frac{f^{(k)}(0)}{n^{k+1}}.$$ So what about smooth functions which have every derivative at $0$ equal to zero? Such functions are either identically zero (boring) or non-analytic, so appeals to Taylor series expansions will not help to analyze them. The choice $f(x)=e^{-1/x}$ for $x\in(0,1]$ and $f(0)=0$ is one of the classical examples of a smooth but non-analytic function. The integration by parts argument doesn't seem as helpful here - we can use the formula $$\frac{d^k}{dx^k}e^{-1/x}=e^{-1/x}\sum_{\ell=1}^k(-1)^{k+\ell}\binom{k}{\ell}\binom{k-1}{\ell-1}x^{-(k+\ell)}$$ (proved here ) to rewrite the integral as $$I_n=\frac{k!}{(n+k)!}\sum_{\ell=1}^k(-1)^{k+\ell}\binom{k}{\ell}\binom{k-1}{\ell-1}\int_0^1(1-x)^{n+k}\frac{e^{-1/x}}{x^{k+\ell}}\,dx,$$ but this doesn't seem to make things any easier, except to see that the sequence $(I_n)_{n=1}^\infty$ must converge to zero faster than $n^k$ for any $k\in\mathbb{N}$ (as expected). Any ideas for how to attack this?","I am interested in the asymptotic behavior of the sequence of integrals Motivation: It is straightforward to show that, for any with , (One method is to approximate by a function, then integrate by parts.) In fact, using essentially the same argument, one can show that for any , if there is a such that and , then So what about smooth functions which have every derivative at equal to zero? Such functions are either identically zero (boring) or non-analytic, so appeals to Taylor series expansions will not help to analyze them. The choice for and is one of the classical examples of a smooth but non-analytic function. The integration by parts argument doesn't seem as helpful here - we can use the formula (proved here ) to rewrite the integral as but this doesn't seem to make things any easier, except to see that the sequence must converge to zero faster than for any (as expected). Any ideas for how to attack this?","(I_n)_{n=1}^\infty I_n=\int_0^1(1-x)^ne^{-1/x}\,dx. f\in C[0,1] f(0)\neq0 \int_0^1(1-x)^nf(x)\,dx\sim\frac{f(0)}{n}. f C^1 f\in C^\infty[0,1] k\in\mathbb{N} f^{(k)}(0)\neq0 f(0),f'(0),\ldots,f^{(k-1)}(0)=0 \int_0^1(1-x)^nf(x)\,dx\sim\frac{f^{(k)}(0)}{n^{k+1}}. 0 f(x)=e^{-1/x} x\in(0,1] f(0)=0 \frac{d^k}{dx^k}e^{-1/x}=e^{-1/x}\sum_{\ell=1}^k(-1)^{k+\ell}\binom{k}{\ell}\binom{k-1}{\ell-1}x^{-(k+\ell)} I_n=\frac{k!}{(n+k)!}\sum_{\ell=1}^k(-1)^{k+\ell}\binom{k}{\ell}\binom{k-1}{\ell-1}\int_0^1(1-x)^{n+k}\frac{e^{-1/x}}{x^{k+\ell}}\,dx, (I_n)_{n=1}^\infty n^k k\in\mathbb{N}","['real-analysis', 'sequences-and-series', 'definite-integrals', 'asymptotics']"
49,Challenging problem: Find $a$ where $\int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0$.,Challenging problem: Find  where .,a \int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0,"What is the value of $a\in\mathbb{R}$ that makes the following integral true $$\int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0\,?$$ This question was proposed by my friend Khalef Ruhemi and I have no idea how to approach it but all I tried is setting $x=\tan\theta$ and I don't know how to continue after that. Also I noticed that the integrand is an even function and again I don't know how to make use of this fact. Any help would be much appreciated.",What is the value of that makes the following integral true This question was proposed by my friend Khalef Ruhemi and I have no idea how to approach it but all I tried is setting and I don't know how to continue after that. Also I noticed that the integrand is an even function and again I don't know how to make use of this fact. Any help would be much appreciated.,"a\in\mathbb{R} \int_0^\infty \frac{\cos(ax)\ln(1+x^2)}{\sqrt{1+x^2}}dx=0\,? x=\tan\theta","['real-analysis', 'calculus', 'integration', 'improper-integrals']"
50,Set of discontinuity of monotone function is countable,Set of discontinuity of monotone function is countable,,"The result I am trying to prove is the following: Let $F:\Bbb R\to\Bbb R$ be increasing. Then the set of points, at which it is discontinuous, is countable. I have been reading this form Folland's Real Analysis and he proves this by considering the sum $\sum_{|x|<N}[F(x^+)-F(x^-)]$ which has to be finite by some sort of telescoping sum. Now, I am not sure how he defines this sum. I am assuming he does this by using nets (let me know if there is another way to look at this), but he doesn't introduce the concept of nets until the next chapter. So I was looking for some other ways to prove this result and I found the following result (from Stein-Shakarchi ) which is quite similar but assumes $F$ to be bounded. A bounded increasing function $F$ on $[a,b]$ has at most countably many discontinuities. This is shown by using the density of rationals in $\Bbb R$ , i.e. $\exists r_x\in\Bbb Q$ s. t. $F(x^-)<r_x<F(x^+)$ corresponding to each point of discontinuity $x$ . Now I feel like there shouldn't be a problem in doing the exact same thing for unbounded increasing function. Is this right? Secondly how does this proof relate to the first one? I feel like there isn't much of a difference between the two proofs. To be precise I think that the previous sum is well defined/finite because of this countability of discontinuous points. What do you think about this way of thinking? Edit. Here is the Folland's proof in more detail: Since $F$ is increasing, the intervals $(F(x^-),F(x^+))$ are disjoint for all $\forall x\in\Bbb R$ . Moreover, $\forall|x| < N$ such intervals are contained in $(F(-N),F(N^+))$ and so $$\sum_{|x|<N}[F(x^+)-F(x^-)] \leqslant F(N) - F(-N) < \infty. $$ Hence the set $\{ x \in (-N,N) : F(x^+) \neq F(x^-)\}$ is countable.","The result I am trying to prove is the following: Let be increasing. Then the set of points, at which it is discontinuous, is countable. I have been reading this form Folland's Real Analysis and he proves this by considering the sum which has to be finite by some sort of telescoping sum. Now, I am not sure how he defines this sum. I am assuming he does this by using nets (let me know if there is another way to look at this), but he doesn't introduce the concept of nets until the next chapter. So I was looking for some other ways to prove this result and I found the following result (from Stein-Shakarchi ) which is quite similar but assumes to be bounded. A bounded increasing function on has at most countably many discontinuities. This is shown by using the density of rationals in , i.e. s. t. corresponding to each point of discontinuity . Now I feel like there shouldn't be a problem in doing the exact same thing for unbounded increasing function. Is this right? Secondly how does this proof relate to the first one? I feel like there isn't much of a difference between the two proofs. To be precise I think that the previous sum is well defined/finite because of this countability of discontinuous points. What do you think about this way of thinking? Edit. Here is the Folland's proof in more detail: Since is increasing, the intervals are disjoint for all . Moreover, such intervals are contained in and so Hence the set is countable.","F:\Bbb R\to\Bbb R \sum_{|x|<N}[F(x^+)-F(x^-)] F F [a,b] \Bbb R \exists r_x\in\Bbb Q F(x^-)<r_x<F(x^+) x F (F(x^-),F(x^+)) \forall x\in\Bbb R \forall|x| < N (F(-N),F(N^+)) \sum_{|x|<N}[F(x^+)-F(x^-)] \leqslant F(N) - F(-N) < \infty.  \{ x \in (-N,N) : F(x^+) \neq F(x^-)\}","['real-analysis', 'measure-theory', 'lebesgue-measure']"
51,Show that $45<x_{1000}<45.1$,Show that,45<x_{1000}<45.1,"If $x_0 = 5$ and $x_{n+1} = x_n + \frac {1}{x_n},$ show that $45<x_{1000}<45.1$ This problem is taken from the list submitted for the $1975$ Canadian Mathematics Olympiad (but not used on the actual exam). SOURCE : CRUX (Page Number 3 ; Question Number 162) I tried writing out the first few terms : $x_1 = 5+ \frac{1}{5} $ $x_2 = \big(5+\frac{1}{5}\big) + \big(5+\frac{1}{5}\big)^{-1} = \frac{x_0^2 + 1}{x_0} +  \frac{x_0}{x_0^2 + 1} = \frac{(x_0^2 + 1)^2+x_0^2}{x_0(x_0^2+1)}$ $x_3 = \frac{(x_0^2 + 1)^2+x_0^2}{x_0(x_0^2+1)} + \frac{x_0(x_0^2+1)}{(x_0^2 + 1)^2+x_0^2} = Messy$ I tried a lot but could not find any general formula for the $n$ th term. Does there even exist any? Also it is clear that $\big(x_n + \frac{1}{x_n}\big)$ is an increasing function. So I think sequence diverges, but how can the $1000th$ term be calculated or aprroximated? Any help would be gratefully acknowledged :).","If and show that This problem is taken from the list submitted for the Canadian Mathematics Olympiad (but not used on the actual exam). SOURCE : CRUX (Page Number 3 ; Question Number 162) I tried writing out the first few terms : I tried a lot but could not find any general formula for the th term. Does there even exist any? Also it is clear that is an increasing function. So I think sequence diverges, but how can the term be calculated or aprroximated? Any help would be gratefully acknowledged :).","x_0 = 5 x_{n+1} = x_n + \frac {1}{x_n}, 45<x_{1000}<45.1 1975 x_1 = 5+ \frac{1}{5}  x_2 = \big(5+\frac{1}{5}\big) + \big(5+\frac{1}{5}\big)^{-1} = \frac{x_0^2 + 1}{x_0} +  \frac{x_0}{x_0^2 + 1} = \frac{(x_0^2 + 1)^2+x_0^2}{x_0(x_0^2+1)} x_3 = \frac{(x_0^2 + 1)^2+x_0^2}{x_0(x_0^2+1)} + \frac{x_0(x_0^2+1)}{(x_0^2 + 1)^2+x_0^2} = Messy n \big(x_n + \frac{1}{x_n}\big) 1000th","['real-analysis', 'sequences-and-series']"
52,Convolution and multiplication of polynomials is the same?,Convolution and multiplication of polynomials is the same?,,Give two polynomials with finite degree - I wonder if a multiplication of both polynomials is the same as the continous convolution of both polynomials? Can anybody shed some light on this?,Give two polynomials with finite degree - I wonder if a multiplication of both polynomials is the same as the continous convolution of both polynomials? Can anybody shed some light on this?,,"['calculus', 'real-analysis', 'algebra-precalculus']"
53,Prove that $\lim_{h \to 0}\frac{1}{h}\int_0^h{\cos{\frac{1}{t}}dt} = 0$,Prove that,\lim_{h \to 0}\frac{1}{h}\int_0^h{\cos{\frac{1}{t}}dt} = 0,I'm trying to prove that $$\lim_{h \to 0}\frac{1}{h}\int_0^h{f(t)dt} = 0$$ where $$f(t) = \begin{cases}\cos{\frac{1}{t}} &\text{ if } t \neq 0\\ 0&\text{otherwise}\end{cases}.$$ Can someone give me a hint where to start? Darboux sums somehow seem to lead me nowhere. NOTE: I cannot assume that $f$ has an antiderivative $F$.,I'm trying to prove that $$\lim_{h \to 0}\frac{1}{h}\int_0^h{f(t)dt} = 0$$ where $$f(t) = \begin{cases}\cos{\frac{1}{t}} &\text{ if } t \neq 0\\ 0&\text{otherwise}\end{cases}.$$ Can someone give me a hint where to start? Darboux sums somehow seem to lead me nowhere. NOTE: I cannot assume that $f$ has an antiderivative $F$.,,"['calculus', 'real-analysis', 'integration', 'limits']"
54,Evaluating $\int_0^{\infty} \log(\sin^2(x))\left(1-x\operatorname{arccot}(x)\right) \ dx$,Evaluating,\int_0^{\infty} \log(\sin^2(x))\left(1-x\operatorname{arccot}(x)\right) \ dx,"One of the ways to compute the integral $$\int_0^{\infty} \log(\sin^2(x))\left(1-x\operatorname{arccot}(x)\right) \ dx=\frac{\pi}{4}\left(\operatorname{Li_3}(e^{-2})+2\operatorname{Li_2}(e^{-2})-2\log(2)-\zeta(3)\right)$$ is to make use of the series of $\log(\sin(x))$, but the result I got after doing that wasn't that friendly. Is it possible to find a neat way of evaluating the integral?","One of the ways to compute the integral $$\int_0^{\infty} \log(\sin^2(x))\left(1-x\operatorname{arccot}(x)\right) \ dx=\frac{\pi}{4}\left(\operatorname{Li_3}(e^{-2})+2\operatorname{Li_2}(e^{-2})-2\log(2)-\zeta(3)\right)$$ is to make use of the series of $\log(\sin(x))$, but the result I got after doing that wasn't that friendly. Is it possible to find a neat way of evaluating the integral?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
55,Hilbert cube is compact,Hilbert cube is compact,,"Let $\{u_n\}_{n\in \mathbb N}$ be an orthonormal set in $H$ (Hilbert space). How prove that the set $\displaystyle Q=\{x\in H :\ x=\sum_{i=1}^{\infty}{c_nu_n}, \ \mbox{where} |c_n|\leq\frac{1}{n} \}$ is compact? $H$ is a Hilbert space over the complex numbers, $c_n\in\mathbb C$ Thanks for any suggestion.","Let $\{u_n\}_{n\in \mathbb N}$ be an orthonormal set in $H$ (Hilbert space). How prove that the set $\displaystyle Q=\{x\in H :\ x=\sum_{i=1}^{\infty}{c_nu_n}, \ \mbox{where} |c_n|\leq\frac{1}{n} \}$ is compact? $H$ is a Hilbert space over the complex numbers, $c_n\in\mathbb C$ Thanks for any suggestion.",,"['real-analysis', 'hilbert-spaces', 'compactness', 'lp-spaces']"
56,Iterated Limits Schizophrenia,Iterated Limits Schizophrenia,,"Consider the functions $g_n(x)$, with $n\in\mathbb{N}$, $n \ge 1$ and $x\in\mathbb{R}$, defined as follows: $$    g_n(x) = \begin{cases}             2n^2x & \text{if }0 \le x < 1/(2n) \\             2n - 2 n^2 x & \text{if } 1/(2n) \le x < 1/n \\             0 & \text{everywhere else} \end{cases} $$ Standard mathematics argument : These functions are triangular, and they all disappear outside of $[0,1]$, so I can compute $\int_0^1 g_n(x) \, dx = 1$ for every $n$. For every $x$, $\lim_{n \rightarrow \infty} g_n(x) = 0$. So the limit and the integration can't be interchanged. Here is an animated picture of the functions: The function $g_n(x)$ becomes a sharp peak at $x=0$ for $n \rightarrow \infty$ and, geometrically, it certainly does not disappear or becomes zero. Instead, it seems that we get, in the end, what physicists know as a delta function. Informally: $$    \delta(x) = \begin{cases}             0 & \text{for } x \ne 0 \\             \infty & \text{for } x = 0                \end{cases} \qquad ; \qquad    \int_{-\infty}^{+\infty} \delta(x) \, dx = 1 $$ Whatever definition might be the ""rigorous"" one, a delta function , roughly speaking, is just a very large peak near $x = 0$ with area normed to $1$. Furthermore, it is typical that the following function, triangular as well, is indeed supposed to converge to the delta function - instead of becoming zero - for $n \rightarrow \infty$ and nobody has any doubt about it. $$    D_n(x) = \begin{cases}             n^2x + n & \text{if } -1/n \le x \le 0 \\             n - n^2x & \text{if } 0 \le x \le +1/n \\             0 & \text{everywhere else} \end{cases} $$ The only thing that distinguishes $g_n(x)$ from $D_n(x)$ is that the maximum of the former is shifted an infinitesimal distance $\lim_{n \rightarrow \infty} 1/(2n)$ with respect to the maximum of the latter at $x=0$. So it's easy to see that these functions become one and the same for $n \rightarrow \infty$: $$   \lim_{n \rightarrow \infty} g_n(x) =   \lim_{n \rightarrow \infty} D_n(x) = \delta(x) $$ Therefore, in the end, we have two arguments that, unfortunately, also lead to different outcomes for the iterated limit. According to standard mathematics, the iterated limits do not commute:     $$\int_0^1 \left[ \lim_{n \rightarrow \infty} g_n(x) \right] \, dx = 0 \qquad \text{and} \qquad \lim_{n \rightarrow \infty} \left[ \int_0^1 g_n(x) \, dx \right] = 1 $$ According to this physicist, the iterated limits do commute :     $$\int_0^1 \left[ \lim_{n \rightarrow \infty} g_n(x)  \right] \, dx = 1 \qquad \text{and} \qquad \lim_{n \rightarrow \infty} \left[ \int_0^1 g_n(x) \, dx  \right] = 1 $$ And the question is, of course the following. Is it possible to escape from this apparent paradox? How then?","Consider the functions $g_n(x)$, with $n\in\mathbb{N}$, $n \ge 1$ and $x\in\mathbb{R}$, defined as follows: $$    g_n(x) = \begin{cases}             2n^2x & \text{if }0 \le x < 1/(2n) \\             2n - 2 n^2 x & \text{if } 1/(2n) \le x < 1/n \\             0 & \text{everywhere else} \end{cases} $$ Standard mathematics argument : These functions are triangular, and they all disappear outside of $[0,1]$, so I can compute $\int_0^1 g_n(x) \, dx = 1$ for every $n$. For every $x$, $\lim_{n \rightarrow \infty} g_n(x) = 0$. So the limit and the integration can't be interchanged. Here is an animated picture of the functions: The function $g_n(x)$ becomes a sharp peak at $x=0$ for $n \rightarrow \infty$ and, geometrically, it certainly does not disappear or becomes zero. Instead, it seems that we get, in the end, what physicists know as a delta function. Informally: $$    \delta(x) = \begin{cases}             0 & \text{for } x \ne 0 \\             \infty & \text{for } x = 0                \end{cases} \qquad ; \qquad    \int_{-\infty}^{+\infty} \delta(x) \, dx = 1 $$ Whatever definition might be the ""rigorous"" one, a delta function , roughly speaking, is just a very large peak near $x = 0$ with area normed to $1$. Furthermore, it is typical that the following function, triangular as well, is indeed supposed to converge to the delta function - instead of becoming zero - for $n \rightarrow \infty$ and nobody has any doubt about it. $$    D_n(x) = \begin{cases}             n^2x + n & \text{if } -1/n \le x \le 0 \\             n - n^2x & \text{if } 0 \le x \le +1/n \\             0 & \text{everywhere else} \end{cases} $$ The only thing that distinguishes $g_n(x)$ from $D_n(x)$ is that the maximum of the former is shifted an infinitesimal distance $\lim_{n \rightarrow \infty} 1/(2n)$ with respect to the maximum of the latter at $x=0$. So it's easy to see that these functions become one and the same for $n \rightarrow \infty$: $$   \lim_{n \rightarrow \infty} g_n(x) =   \lim_{n \rightarrow \infty} D_n(x) = \delta(x) $$ Therefore, in the end, we have two arguments that, unfortunately, also lead to different outcomes for the iterated limit. According to standard mathematics, the iterated limits do not commute:     $$\int_0^1 \left[ \lim_{n \rightarrow \infty} g_n(x) \right] \, dx = 0 \qquad \text{and} \qquad \lim_{n \rightarrow \infty} \left[ \int_0^1 g_n(x) \, dx \right] = 1 $$ According to this physicist, the iterated limits do commute :     $$\int_0^1 \left[ \lim_{n \rightarrow \infty} g_n(x)  \right] \, dx = 1 \qquad \text{and} \qquad \lim_{n \rightarrow \infty} \left[ \int_0^1 g_n(x) \, dx  \right] = 1 $$ And the question is, of course the following. Is it possible to escape from this apparent paradox? How then?",,"['real-analysis', 'limits', 'distribution-theory']"
57,Absolutely continuous maps measurable sets to measurable sets,Absolutely continuous maps measurable sets to measurable sets,,"Show that if $f:\mathbb{R} \rightarrow \mathbb{R}$ is absolutely continuous, then $f$ maps measurable sets to measurable sets. Any ideas on how to do this?","Show that if $f:\mathbb{R} \rightarrow \mathbb{R}$ is absolutely continuous, then $f$ maps measurable sets to measurable sets. Any ideas on how to do this?",,"['real-analysis', 'measure-theory', 'measurable-sets']"
58,Irrational and rational sequence proof,Irrational and rational sequence proof,,Show that every irrational number in $\mathbb{R}$ is the limit of a   sequence of rational numbers. Every rational number in $\mathbb{R}$ is   the limit of a sequence of irrational numbers. How can I prove this?,Show that every irrational number in $\mathbb{R}$ is the limit of a   sequence of rational numbers. Every rational number in $\mathbb{R}$ is   the limit of a sequence of irrational numbers. How can I prove this?,,"['real-analysis', 'irrational-numbers']"
59,Is a monotone differentiable function continuously differentiable?,Is a monotone differentiable function continuously differentiable?,,"If $f:\mathbb{R}^+\to\mathbb{R}$ is monotonic and differentiable, does it follow that $f$ is continuously differentiable? (This question arose from discussion here: problem on continuous and differentiable function )","If $f:\mathbb{R}^+\to\mathbb{R}$ is monotonic and differentiable, does it follow that $f$ is continuously differentiable? (This question arose from discussion here: problem on continuous and differentiable function )",,['real-analysis']
60,Computing: $\lim\limits_{n\to\infty}\left(\prod\limits_{k=1}^{n} \binom{n}{k}\right)^\frac{1}{n}$,Computing:,\lim\limits_{n\to\infty}\left(\prod\limits_{k=1}^{n} \binom{n}{k}\right)^\frac{1}{n},I try to compute the following limit: $$\lim_{n\to\infty}\left(\prod_{k=1}^{n} \binom{n}{k}\right)^\frac{1}{n}$$ I'm interested in finding some reasonable ways of solving the limit. I don't find any easy approach. Any hint/suggestion is very welcome.,I try to compute the following limit: $$\lim_{n\to\infty}\left(\prod_{k=1}^{n} \binom{n}{k}\right)^\frac{1}{n}$$ I'm interested in finding some reasonable ways of solving the limit. I don't find any easy approach. Any hint/suggestion is very welcome.,,"['real-analysis', 'limits', 'binomial-coefficients']"
61,"How can i find ${I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}$",How can i find,"{I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}","Question:- Find ${I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}$ Recently I asked a similar question $\int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})}$ Below the above question a note is  written as follows: 'The reader should evaluate the family of integrals ${I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N}$ .The computation of the first few special values indicates an interesting arithmetic structure of the answer.’ I don't know how to tackle ${I_{n}}$ till now. Edit:- Here is the link for above document http://emmy.uprrp.edu/lmedina/papers/part27/final27.pdf I have searched all the references mentioned in above document but found nothing related to above integral.We might have to look values for different values of $n$ instead of finding integral for general $n$ .",Question:- Find Recently I asked a similar question Below the above question a note is  written as follows: 'The reader should evaluate the family of integrals .The computation of the first few special values indicates an interesting arithmetic structure of the answer.’ I don't know how to tackle till now. Edit:- Here is the link for above document http://emmy.uprrp.edu/lmedina/papers/part27/final27.pdf I have searched all the references mentioned in above document but found nothing related to above integral.We might have to look values for different values of instead of finding integral for general .,"{I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N} \int_{0}^{1}\frac {x^2\ln x}{{(1-x^2)}{(1+x^4)}}dx=\frac{-π^2}{16(2+\sqrt{2})} {I_{n}=\int_{0}^{1}\frac {x^{2n}\ln x}{{(1-x^2)}{(1+x^4)^n}}dx{,n} \in N} {I_{n}} n n","['real-analysis', 'calculus', 'integration', 'complex-analysis', 'definite-integrals']"
62,Differentiating the binomial coefficient,Differentiating the binomial coefficient,,"I took a lecture in combinatorics this semester and the professor did the following step in a proof:  He showed that function $f: x \mapsto \binom{x}{r}$ is convex for $x > r - 1$ (in order to use Jensen's inequality on $f$) and did this in the following way: ""By the product-rule we have $$f''(x) = \frac{2}{r!} \sum_{0 \leq i < j \leq r - 1} \prod_{l = 0}^{r - 1} ( x - l) \frac{1}{(x - i ) (x - j)} \geq 0$$ for all $ x > r - 1$."" I am a bit confused on his definition: How would one extend the binomial coefficient to $x \notin \mathbf{N}$? I first thought about piecewise linear interpolation, but then I can't differentiate it. I also thought of plugging in the Gamma-function for the factorials, but I doubt that this is the definition he used here. Can anyone explain to me what's happening here? Thanks!","I took a lecture in combinatorics this semester and the professor did the following step in a proof:  He showed that function $f: x \mapsto \binom{x}{r}$ is convex for $x > r - 1$ (in order to use Jensen's inequality on $f$) and did this in the following way: ""By the product-rule we have $$f''(x) = \frac{2}{r!} \sum_{0 \leq i < j \leq r - 1} \prod_{l = 0}^{r - 1} ( x - l) \frac{1}{(x - i ) (x - j)} \geq 0$$ for all $ x > r - 1$."" I am a bit confused on his definition: How would one extend the binomial coefficient to $x \notin \mathbf{N}$? I first thought about piecewise linear interpolation, but then I can't differentiate it. I also thought of plugging in the Gamma-function for the factorials, but I doubt that this is the definition he used here. Can anyone explain to me what's happening here? Thanks!",,"['real-analysis', 'combinatorics', 'derivatives', 'binomial-coefficients']"
63,Laplacian of a radial function,Laplacian of a radial function,,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be a radial function, i.e. $f(x)=f(r)$ with $r:=\left\|x\right\|_2$. As stated at Wikipedia $$\Delta f=\frac{1}{r^{n-1}}\frac{d}{dr}(r^{n-1}f')$$ What's the most elegant way to prove this fact?","Let $f:\mathbb{R}^n\to\mathbb{R}$ be a radial function, i.e. $f(x)=f(r)$ with $r:=\left\|x\right\|_2$. As stated at Wikipedia $$\Delta f=\frac{1}{r^{n-1}}\frac{d}{dr}(r^{n-1}f')$$ What's the most elegant way to prove this fact?",,"['real-analysis', 'analysis', 'derivatives', 'laplacian']"
64,Passing from induction to $\infty$,Passing from induction to,\infty,"Somehow, the operation of passing to the limit after I have shown that something is true by induction for each natural number $n$ troubles me each time. I know there are instances where one cannot deduce a statement is true for $\infty$ if it holds for each $n \in \mathbb{N}$, and sometimes one can. Here is one instance where I am not sure whether my argument is solid: Suppose $X$ is a Banach space with norm $\| \cdot \|$. I would like to show that the triangle inequality for finite linear combinations extends to series. So, by induction and by properties of the norm we have $$\begin{equation} \bigg\|\sum^n_{k = 1} a_k \bigg\| \;\;\leq \;\;\sum_{k = 1}^n \|a_k\| \end{equation}$$ holds for each natural number $n$. I would now argue that I can immediately pass to the limit and deduce directly that $$\begin{equation} \bigg\|\sum^\infty_{k = 1} a_k\bigg\| \;\;\leq \;\;\sum_{k = 1}^\infty \|a_k\| \end{equation} $$ because of a property of sequences of real numbers, which says that if $(c_n)_{n \in \mathbb{N}}$ and $(d_n)_{n \in \mathbb{N}}$ are sequences of real numbers such that \begin{equation} 0 \leq c_n \ \leq d_n \quad \text{ for all } n \in \mathbb{N}. \end{equation} Then it follows that \begin{equation} \lim_{n \to \infty} c_n \leq \lim_{n \to \infty} d_n \end{equation} (I can use this by taking $c_n = \| \sum_{k = 1}^n a_k \|$ and $d_n = \sum_{k = 1}^n \|a_k\|$) Is this reasoning correct ? I am not sure .. For example, one of the issues I have with my argument is the following: When I replace $n$ by $\infty$ in the expression $\|\sum_{k = 1}^n a_k \|$ then I might make a statement that is ill-defined, because $\|\sum_{k = 1}^\infty a_k \|$ might not exist, whereas the expression $\sum_{k = 1}^\infty \|a_k\|$ always has a value in $[0,\infty]$ (since $s_n = \sum_{k = 1}^n \|a_k\|$ is a sequence that is monotone). Is this an issue? Or is the statement $\|\sum_{k = 1}^\infty a_k \| \leq \sum_{k = 1}^\infty \|a_k\|$ simply vacuously true in this case? If it is an issue, how can I rectify the argument?","Somehow, the operation of passing to the limit after I have shown that something is true by induction for each natural number $n$ troubles me each time. I know there are instances where one cannot deduce a statement is true for $\infty$ if it holds for each $n \in \mathbb{N}$, and sometimes one can. Here is one instance where I am not sure whether my argument is solid: Suppose $X$ is a Banach space with norm $\| \cdot \|$. I would like to show that the triangle inequality for finite linear combinations extends to series. So, by induction and by properties of the norm we have $$\begin{equation} \bigg\|\sum^n_{k = 1} a_k \bigg\| \;\;\leq \;\;\sum_{k = 1}^n \|a_k\| \end{equation}$$ holds for each natural number $n$. I would now argue that I can immediately pass to the limit and deduce directly that $$\begin{equation} \bigg\|\sum^\infty_{k = 1} a_k\bigg\| \;\;\leq \;\;\sum_{k = 1}^\infty \|a_k\| \end{equation} $$ because of a property of sequences of real numbers, which says that if $(c_n)_{n \in \mathbb{N}}$ and $(d_n)_{n \in \mathbb{N}}$ are sequences of real numbers such that \begin{equation} 0 \leq c_n \ \leq d_n \quad \text{ for all } n \in \mathbb{N}. \end{equation} Then it follows that \begin{equation} \lim_{n \to \infty} c_n \leq \lim_{n \to \infty} d_n \end{equation} (I can use this by taking $c_n = \| \sum_{k = 1}^n a_k \|$ and $d_n = \sum_{k = 1}^n \|a_k\|$) Is this reasoning correct ? I am not sure .. For example, one of the issues I have with my argument is the following: When I replace $n$ by $\infty$ in the expression $\|\sum_{k = 1}^n a_k \|$ then I might make a statement that is ill-defined, because $\|\sum_{k = 1}^\infty a_k \|$ might not exist, whereas the expression $\sum_{k = 1}^\infty \|a_k\|$ always has a value in $[0,\infty]$ (since $s_n = \sum_{k = 1}^n \|a_k\|$ is a sequence that is monotone). Is this an issue? Or is the statement $\|\sum_{k = 1}^\infty a_k \| \leq \sum_{k = 1}^\infty \|a_k\|$ simply vacuously true in this case? If it is an issue, how can I rectify the argument?",,"['real-analysis', 'sequences-and-series', 'normed-spaces']"
65,Prove the definitions of $e$ to be equivalent,Prove the definitions of  to be equivalent,e,"How to prove that the following definitions of $e^x$ are equivalent, with as simple tools as possible and without any knowledge of $e$ or logarithms ? $$\sum_{n=0}^{\infty} \frac{x^n}{n!}=\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n$$ Also preferably, prove that this is $a^x$ for some real number $a>0$.","How to prove that the following definitions of $e^x$ are equivalent, with as simple tools as possible and without any knowledge of $e$ or logarithms ? $$\sum_{n=0}^{\infty} \frac{x^n}{n!}=\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n$$ Also preferably, prove that this is $a^x$ for some real number $a>0$.",,"['real-analysis', 'calculus', 'limits', 'exponential-function', 'eulers-number-e']"
66,Weierstrass approximation does not hold on the entire Real Line,Weierstrass approximation does not hold on the entire Real Line,,"This is a question from Bergman's companion to Rudin. a) Show that the only polynomials which are bounded as functions $\mathbb{R} \rightarrow \mathbb{R}$ are constant functions. (I can do this) Also done here b)Deduce that if a sequence of polynomials $P_n:\mathbb{R} \rightarrow \mathbb{R}$ converges uniformly on $\mathbb{R}$ to $f$ then  $f$ is a polynomial. I figure that the uniform convergence implies at some point (for large n) the polynomials must have the same highest power because otherwise large values of $\mathbb{R}$ would destroy any hope of uniform convergence. Then eventually the second highest power must be equal as well by a similar argument...Then I guess you could make a similar argument for the co-efficients by plugging in large values of x, the difference in each co-efficient must be quite small in order to maintain the uniform convergence. I would like some help understanding if/why this means that the limit actually is a polynomial.","This is a question from Bergman's companion to Rudin. a) Show that the only polynomials which are bounded as functions $\mathbb{R} \rightarrow \mathbb{R}$ are constant functions. (I can do this) Also done here b)Deduce that if a sequence of polynomials $P_n:\mathbb{R} \rightarrow \mathbb{R}$ converges uniformly on $\mathbb{R}$ to $f$ then  $f$ is a polynomial. I figure that the uniform convergence implies at some point (for large n) the polynomials must have the same highest power because otherwise large values of $\mathbb{R}$ would destroy any hope of uniform convergence. Then eventually the second highest power must be equal as well by a similar argument...Then I guess you could make a similar argument for the co-efficients by plugging in large values of x, the difference in each co-efficient must be quite small in order to maintain the uniform convergence. I would like some help understanding if/why this means that the limit actually is a polynomial.",,['real-analysis']
67,Proving and deriving a Gamma function,Proving and deriving a Gamma function,,I'm having a hard time trying to prove this Gamma function and trying to derive the duplication formula: a.) Prove that $$\frac{\Gamma (p)\Gamma (p)}{\Gamma (2p)} = 2\int_0^{1/2}x^{p-1}(1-x)^{p-1}\mathrm dx$$ b.) Make a suitable change of variable (a) and derive the duplication formula for the Gamma function: $$\Gamma (2p)\Gamma\left(\frac12\right) = 2^{2p-1}\Gamma (p)\Gamma\left(p-\frac12\right)$$,I'm having a hard time trying to prove this Gamma function and trying to derive the duplication formula: a.) Prove that $$\frac{\Gamma (p)\Gamma (p)}{\Gamma (2p)} = 2\int_0^{1/2}x^{p-1}(1-x)^{p-1}\mathrm dx$$ b.) Make a suitable change of variable (a) and derive the duplication formula for the Gamma function: $$\Gamma (2p)\Gamma\left(\frac12\right) = 2^{2p-1}\Gamma (p)\Gamma\left(p-\frac12\right)$$,,"['calculus', 'real-analysis', 'special-functions', 'gamma-function']"
68,Some basics of Sobolev spaces,Some basics of Sobolev spaces,,"Let $W^{m,p}(\Omega) = \{ f \in L^p(\Omega): D^\alpha f \in L^p(\Omega) \text{ for multi-indices } |\alpha| \leq m\}$, where $D$ denotes the weak derivative. Let $W_0^{m,p}$ denote the closure of $C_c^\infty(\Omega)$ in $W^{m,p}(\Omega)$. Why is it true that $W_0^{m,p}(\mathbb{R}^d) = W^{m,p}(\mathbb{R}^d)$, but in general $W_0^{m,p}(\Omega) \subsetneq W^{m,p}(\Omega)$? I am trying to understand why there is a need to consider $W_0^{m,p}(\mathbb{R}^d)$. I'm guessing it's because the elements in $W^{m,p}(\Omega)$ can get really messy, but I don't have very good intuition about both spaces.","Let $W^{m,p}(\Omega) = \{ f \in L^p(\Omega): D^\alpha f \in L^p(\Omega) \text{ for multi-indices } |\alpha| \leq m\}$, where $D$ denotes the weak derivative. Let $W_0^{m,p}$ denote the closure of $C_c^\infty(\Omega)$ in $W^{m,p}(\Omega)$. Why is it true that $W_0^{m,p}(\mathbb{R}^d) = W^{m,p}(\mathbb{R}^d)$, but in general $W_0^{m,p}(\Omega) \subsetneq W^{m,p}(\Omega)$? I am trying to understand why there is a need to consider $W_0^{m,p}(\mathbb{R}^d)$. I'm guessing it's because the elements in $W^{m,p}(\Omega)$ can get really messy, but I don't have very good intuition about both spaces.",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
69,Functional equation $f(px)+p=[f(x)]^2$,Functional equation,f(px)+p=[f(x)]^2,"Let $p\in\mathbb{N}$ and $p>1$ . Consider the functional equation $$f(px)+p=[f(x)]^2$$ I need to find all functions $f:\mathbb{R}\to\mathbb{R}$ that is continuous at $0$ and satisfies above functional equation for all $x\in\mathbb{R}$ . For $p=2$ , it can be showed that there exists a solution, the function $f:\mathbb{R}\to\mathbb{R}$ defined by $f(x)=2\cos(\alpha x)$ where $\alpha\in\mathbb{R}\cup i\mathbb{R}$ is arbitrary. How can we find solutions for $p>2$ ? It possible to prove the existence since there exists constant solutions.","Let and . Consider the functional equation I need to find all functions that is continuous at and satisfies above functional equation for all . For , it can be showed that there exists a solution, the function defined by where is arbitrary. How can we find solutions for ? It possible to prove the existence since there exists constant solutions.",p\in\mathbb{N} p>1 f(px)+p=[f(x)]^2 f:\mathbb{R}\to\mathbb{R} 0 x\in\mathbb{R} p=2 f:\mathbb{R}\to\mathbb{R} f(x)=2\cos(\alpha x) \alpha\in\mathbb{R}\cup i\mathbb{R} p>2,"['real-analysis', 'functions', 'continuity', 'functional-equations', 'nonlinear-analysis']"
70,Find : $\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$,Find :,\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}},Find : $$\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$$ My attempt : i don't know is correct or no! I use this rule : $$\lim\limits_{n\to +\infty}(f(x))^{g(x)}=1^{\infty}$$ Then : $$\lim\limits_{n\to +\infty}(f(x))^{g(x)}=\lim\limits_{n\to +\infty}e^{g(x)(f(x)-1)}$$ So : $$\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}}$$ $$=\lim\limits_{n\to +\infty}\frac{e^{\frac{n^{4}}{n^{3}}}}{e^{\frac{(n+1)^{4}}{(n+1)^{3}}}}$$ $$=\lim\limits_{n\to +\infty}\frac{e^{n}}{e^{n+1}}$$ $$=\frac{1}{e}$$ Is my approach wrong ? is this called partial limit calculation ?,Find : My attempt : i don't know is correct or no! I use this rule : Then : So : Is my approach wrong ? is this called partial limit calculation ?,\lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}} \lim\limits_{n\to +\infty}(f(x))^{g(x)}=1^{\infty} \lim\limits_{n\to +\infty}(f(x))^{g(x)}=\lim\limits_{n\to +\infty}e^{g(x)(f(x)-1)} \lim\limits_{n\to +\infty}\frac{\left(1+\frac{1}{n^3}\right)^{n^4}}{\left(1+\frac{1}{(n+1)^3}\right)^{(n+1)^4}} =\lim\limits_{n\to +\infty}\frac{e^{\frac{n^{4}}{n^{3}}}}{e^{\frac{(n+1)^{4}}{(n+1)^{3}}}} =\lim\limits_{n\to +\infty}\frac{e^{n}}{e^{n+1}} =\frac{1}{e},"['real-analysis', 'calculus', 'limits']"
71,How to solve an equation of the form $f(x)=f(a)$ for a fixed real a.,How to solve an equation of the form  for a fixed real a.,f(x)=f(a),"I got stuck on this question: find all solutions $x$ for $a\in R$: $$\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{(a^2-a+1)^3}{a^2(a-1)^2}$$ I see that if we simplify we get: $$\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{[(x-{\frac 12})^2+{\frac 34}]^3}{[(x-{\frac 12})^2-{\frac 14}]^2}$$ From the expression $(x-{\frac 12})^2$, I see that if $x=x_1$ is a solution, then $x=1-x_1$ is also a solution. But in the solution to this exercise, it was stated that $x=\frac{1}{x_1}$ must also be a solution, and I don't see how. [EDIT] Ok, thx for the help guys. What do you think of this solution (doesn't involve any above precalculus math, and needs no long calculations)? From the above we know that if $x_1=a$ is a solution, then $x_2=1-a$ is also a solution. Also, from here: $$\require{cancel}\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{\cancel{x^3}(x+{\frac 1x}-1)^3}{\cancel{x^3}(x+{\frac 1x}-2)}$$ in the expression $x+{\frac 1x}$ we see that if $x=x_1$ is a solution, then $x=\frac{1}{x_1}$ is also a solution, so $x_3=\frac{1}{a}$. With these two rules we can now keep generating roots until we have 6 total. If $x=x_2$ is a solution, then $x=\frac{1}{x_2}$ is also a solution, so $x_4=\frac{1}{1-a}$. If $x=x_3$ is a solution, then $x=1-x_3$ is also a solution, so $x_5=\frac{a-1}{a}$. Finally, if $x=x_5$ is a solution, then $x=\frac{1}{x_5}$ is also a solution, so $x_6=\frac{a}{a-1}$ The 6 obtained values are distinct, so they cover all the roots. [EDIT2] I guess this is answered. No sure whose particular answer to actually select as the right one since they're all correct, so I'll just leave it like this.","I got stuck on this question: find all solutions $x$ for $a\in R$: $$\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{(a^2-a+1)^3}{a^2(a-1)^2}$$ I see that if we simplify we get: $$\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{[(x-{\frac 12})^2+{\frac 34}]^3}{[(x-{\frac 12})^2-{\frac 14}]^2}$$ From the expression $(x-{\frac 12})^2$, I see that if $x=x_1$ is a solution, then $x=1-x_1$ is also a solution. But in the solution to this exercise, it was stated that $x=\frac{1}{x_1}$ must also be a solution, and I don't see how. [EDIT] Ok, thx for the help guys. What do you think of this solution (doesn't involve any above precalculus math, and needs no long calculations)? From the above we know that if $x_1=a$ is a solution, then $x_2=1-a$ is also a solution. Also, from here: $$\require{cancel}\frac{(x^2-x+1)^3}{x^2(x-1)^2}=\frac{\cancel{x^3}(x+{\frac 1x}-1)^3}{\cancel{x^3}(x+{\frac 1x}-2)}$$ in the expression $x+{\frac 1x}$ we see that if $x=x_1$ is a solution, then $x=\frac{1}{x_1}$ is also a solution, so $x_3=\frac{1}{a}$. With these two rules we can now keep generating roots until we have 6 total. If $x=x_2$ is a solution, then $x=\frac{1}{x_2}$ is also a solution, so $x_4=\frac{1}{1-a}$. If $x=x_3$ is a solution, then $x=1-x_3$ is also a solution, so $x_5=\frac{a-1}{a}$. Finally, if $x=x_5$ is a solution, then $x=\frac{1}{x_5}$ is also a solution, so $x_6=\frac{a}{a-1}$ The 6 obtained values are distinct, so they cover all the roots. [EDIT2] I guess this is answered. No sure whose particular answer to actually select as the right one since they're all correct, so I'll just leave it like this.",,['real-analysis']
72,Negation of definition of continuity,Negation of definition of continuity,,"This should be a very easy question but it might just be that I'm confusing myself. So we have the definition of a function $f$ on $S$ being continuous at $x_0$: For any $\epsilon$>0, there exists $\delta>0$ such that: whenever $|x-x_0|<\delta$, we have $|f(x)-f(x_0)|<\epsilon$ And I assume the negation is There exists $\epsilon$>0 such that for all $\delta>0$, $|x-x_0|<\delta$ yet $|f(x)-f(x_0)|\ge \epsilon$. Now I want to show that the function $f(x)=\sin(\frac{1}{x})$ together with $f(0)=0$ cannot be made into a continuous function at $x=0$. So I need to show that there exists $\epsilon>0$ such that for all $\delta>0$, $|x|<\delta$ yet $|f(x)|\ge\epsilon$. Let $\epsilon = \frac{1}{2}$. Then no matter what $\delta$ we choose, let $|x|<\frac{1}{2}$. It is certainly possible that $|f(x)|\ge \frac{1}{2}$, because, well, $\frac{1}{x}$ can really take on arbitrarily large value as $x$ is small. Now, what confuses me is that, as $x$ gets small, $f(x)$ can certainly be greater than $\frac{1}{2}$ for infinitely many times, but it will be less than that infinitely many times, too. But I suppose it doesn't really matter. So I think there's something wrong with my negation but I couldn't figure out where. Update: The correct version can be found here . Watch for Lemma 4.6","This should be a very easy question but it might just be that I'm confusing myself. So we have the definition of a function $f$ on $S$ being continuous at $x_0$: For any $\epsilon$>0, there exists $\delta>0$ such that: whenever $|x-x_0|<\delta$, we have $|f(x)-f(x_0)|<\epsilon$ And I assume the negation is There exists $\epsilon$>0 such that for all $\delta>0$, $|x-x_0|<\delta$ yet $|f(x)-f(x_0)|\ge \epsilon$. Now I want to show that the function $f(x)=\sin(\frac{1}{x})$ together with $f(0)=0$ cannot be made into a continuous function at $x=0$. So I need to show that there exists $\epsilon>0$ such that for all $\delta>0$, $|x|<\delta$ yet $|f(x)|\ge\epsilon$. Let $\epsilon = \frac{1}{2}$. Then no matter what $\delta$ we choose, let $|x|<\frac{1}{2}$. It is certainly possible that $|f(x)|\ge \frac{1}{2}$, because, well, $\frac{1}{x}$ can really take on arbitrarily large value as $x$ is small. Now, what confuses me is that, as $x$ gets small, $f(x)$ can certainly be greater than $\frac{1}{2}$ for infinitely many times, but it will be less than that infinitely many times, too. But I suppose it doesn't really matter. So I think there's something wrong with my negation but I couldn't figure out where. Update: The correct version can be found here . Watch for Lemma 4.6",,"['real-analysis', 'logic']"
73,Can we construct a function that has uncountable many jump discontinuities?,Can we construct a function that has uncountable many jump discontinuities?,,"I know that Dirichlet function has uncountable many discontinuities. I think they are removable, because the discontinuities can be removed by redefining the function values of the rational numbers as 0. So Dirichlet function is a function that has uncountable many removable discontinuities, then my question is can we construct a function with uncountable  many jump discontinuities? If not, how do we prove it is impossible? Thank you. An odd but similar question is can we have a function that has uncountable many infinite discontinuities?","I know that Dirichlet function has uncountable many discontinuities. I think they are removable, because the discontinuities can be removed by redefining the function values of the rational numbers as 0. So Dirichlet function is a function that has uncountable many removable discontinuities, then my question is can we construct a function with uncountable  many jump discontinuities? If not, how do we prove it is impossible? Thank you. An odd but similar question is can we have a function that has uncountable many infinite discontinuities?",,['real-analysis']
74,"$f:\mathbb R\to\mathbb R$ continuous function. Which of the following sets can not be image of $(0,1]$ under $f$?",continuous function. Which of the following sets can not be image of  under ?,"f:\mathbb R\to\mathbb R (0,1] f","Let $f:\mathbb R\to\mathbb R$ continuous function. Which of the following sets can not be image of $(0,1]$ under $f$? A. $\{0\}$. B. $(0,1)$. C.$[0,1)$. D.$[0,1]$. My effort: Continuous image of connected set connected. $(0,1]$ is connected and remove $1$ from the set left the set connected...but removing any point from $(0,1)$ make it disconnected.....I am not sure though","Let $f:\mathbb R\to\mathbb R$ continuous function. Which of the following sets can not be image of $(0,1]$ under $f$? A. $\{0\}$. B. $(0,1)$. C.$[0,1)$. D.$[0,1]$. My effort: Continuous image of connected set connected. $(0,1]$ is connected and remove $1$ from the set left the set connected...but removing any point from $(0,1)$ make it disconnected.....I am not sure though",,"['real-analysis', 'general-topology', 'continuity']"
75,$f(0)=f'(0)=f'(1)=0$ and $f(1)=1$ implies $\max|f''|\geq 4$,and  implies,f(0)=f'(0)=f'(1)=0 f(1)=1 \max|f''|\geq 4,"Let $f\in C^2(\mathbb [0,1],\mathbb [0,1])$ such that $f(0)=f'(0)=f'(1)=0$ and $f(1)=1$ Prove that $\max_{[0,1]}|f''|\geq 4$ Progress Applying Cauchy mean value theorem three times proves the existence of $\xi\in (0,1)$ such that $f'(\xi)=1$ $\eta\in(\xi,1)$ such that $\displaystyle f''(\eta)=\frac{1}{\xi-1} <0$ $\beta\in(0,\xi)$ such that $\displaystyle f''(\beta)=\frac{1}{\xi}>0$ If $\displaystyle \xi\leq \frac{1}{4}$ or $\displaystyle \xi\geq \frac{3}{4}$ , we're done. What about other cases ? I haven't used the continuity of $f''$ yet...","Let such that and Prove that Progress Applying Cauchy mean value theorem three times proves the existence of such that such that such that If or , we're done. What about other cases ? I haven't used the continuity of yet...","f\in C^2(\mathbb [0,1],\mathbb [0,1]) f(0)=f'(0)=f'(1)=0 f(1)=1 \max_{[0,1]}|f''|\geq 4 \xi\in (0,1) f'(\xi)=1 \eta\in(\xi,1) \displaystyle f''(\eta)=\frac{1}{\xi-1} <0 \beta\in(0,\xi) \displaystyle f''(\beta)=\frac{1}{\xi}>0 \displaystyle \xi\leq \frac{1}{4} \displaystyle \xi\geq \frac{3}{4} f''","['calculus', 'real-analysis']"
76,"Is the Cantor set a subset of rational numbers, and is it countable or uncountable?","Is the Cantor set a subset of rational numbers, and is it countable or uncountable?",,"In Chapter 2 of Rudin's Priniciples of Mathematical Analysis , Rudin takes the Cantor set as an example of a perfect set in $\mathbb{R}^1$ which contains no segment. Here's the construction of the Cantor set and the proof: 2.44 The Cantor set The set which we are now going to construct shows   that there exist perfect sets in $\mathbb{R}^1$ which contain no segment. Let $E_0$ be the interval $[0, 1]$. Remove the segment $(\frac13,\frac23)$, and let $E_1$ be   the union of the intervals   $$[0,\frac13], [\frac23,1].$$   Remove the middle thirds of these intervals, and let $E_2$ be the union of the   intervals   $$[0,\frac19], [\frac29,\frac39], [\frac69,\frac79],[\frac89,1]$$   Continuing in this way, we obtain a sequence of compact sets $E_n$, such that (a) $E_1\supset E_2  \supset E_3 \dots $; (b) $E_n$ is the union of $2^n$ intervals, each of length $3^{-n}$. The set   $$P=\bigcap_{n=1}^\infty E_n$$   is called the Cantor set . $P$ is clearly compact, and Theorem 2.36 shows that $P$   is not empty. No segment of the form   $$\left(\frac{3k+1}{3^m},\frac{3k+2}{3^m}\right)\tag{24},$$   where $k$ and $m$ are positive integers, has a point in common with $P$.   Since every segment $(\alpha,\beta)$ contains a segment of the form (24), if   $$3^{-m}<\frac{\beta-\alpha}6,$$   $P$ contains no segment. To show that $P$ is perfect, it is enough to show that $P$ contains no isolated   point. Let $x \in P$, and let $S$ be any segment containing $x$. Let $I_n$ be that interval   of $E_n$ which contains $x$. Choose $n$ large enough, so that $I_n\subset S$. Let $x_n$ be an   endpoint of $I_n$, such that $x_n\ne x$. It follows from the construction of $P$ that $x_n\in P$. Hence $x$ is a limit point   of $P$, and $P$ is perfect. One of the most interesting properties of the Cantor set is that it provides   us with an example of an uncountable set of measure zero (the concept of   measure will be discussed in Chap. 11). I can follow the proof with some effort, but in the end of this section Rudin claims that the Cantor set is an example of an uncountable set of measure zero. How can the Cantor set be uncountable? Corollary of Theorem 2.13 shows the set of all rational numbers is countable. Theorem 2.8 shows that every infinite subset of a countable set is countable. The elements in the Cantor set are the end points of all the intervals in $E_n$, it follows from the construction of the Cantor set that these end points are all rational numbers. Hence $P$ is a subset of the rational numbers and countable. Is there anything wrong with my reasoning here?","In Chapter 2 of Rudin's Priniciples of Mathematical Analysis , Rudin takes the Cantor set as an example of a perfect set in $\mathbb{R}^1$ which contains no segment. Here's the construction of the Cantor set and the proof: 2.44 The Cantor set The set which we are now going to construct shows   that there exist perfect sets in $\mathbb{R}^1$ which contain no segment. Let $E_0$ be the interval $[0, 1]$. Remove the segment $(\frac13,\frac23)$, and let $E_1$ be   the union of the intervals   $$[0,\frac13], [\frac23,1].$$   Remove the middle thirds of these intervals, and let $E_2$ be the union of the   intervals   $$[0,\frac19], [\frac29,\frac39], [\frac69,\frac79],[\frac89,1]$$   Continuing in this way, we obtain a sequence of compact sets $E_n$, such that (a) $E_1\supset E_2  \supset E_3 \dots $; (b) $E_n$ is the union of $2^n$ intervals, each of length $3^{-n}$. The set   $$P=\bigcap_{n=1}^\infty E_n$$   is called the Cantor set . $P$ is clearly compact, and Theorem 2.36 shows that $P$   is not empty. No segment of the form   $$\left(\frac{3k+1}{3^m},\frac{3k+2}{3^m}\right)\tag{24},$$   where $k$ and $m$ are positive integers, has a point in common with $P$.   Since every segment $(\alpha,\beta)$ contains a segment of the form (24), if   $$3^{-m}<\frac{\beta-\alpha}6,$$   $P$ contains no segment. To show that $P$ is perfect, it is enough to show that $P$ contains no isolated   point. Let $x \in P$, and let $S$ be any segment containing $x$. Let $I_n$ be that interval   of $E_n$ which contains $x$. Choose $n$ large enough, so that $I_n\subset S$. Let $x_n$ be an   endpoint of $I_n$, such that $x_n\ne x$. It follows from the construction of $P$ that $x_n\in P$. Hence $x$ is a limit point   of $P$, and $P$ is perfect. One of the most interesting properties of the Cantor set is that it provides   us with an example of an uncountable set of measure zero (the concept of   measure will be discussed in Chap. 11). I can follow the proof with some effort, but in the end of this section Rudin claims that the Cantor set is an example of an uncountable set of measure zero. How can the Cantor set be uncountable? Corollary of Theorem 2.13 shows the set of all rational numbers is countable. Theorem 2.8 shows that every infinite subset of a countable set is countable. The elements in the Cantor set are the end points of all the intervals in $E_n$, it follows from the construction of the Cantor set that these end points are all rational numbers. Hence $P$ is a subset of the rational numbers and countable. Is there anything wrong with my reasoning here?",,"['real-analysis', 'cantor-set']"
77,Strong convergence of operators,Strong convergence of operators,,"I'm working through the functional analysis book by Milman, Eidelman, and Tsolomitis, and I have a question. The book states a lemma that I'm a bit confused about: A sequence of operators $T_n\in L(X, Y)$ (here, $X$ and $Y$ are Banach Spaces) converges strongly to an operator $T\in L(X,Y)$ if and only if (i) the sequence $\{T_n x\}$ converges for any $x$ in a dense subset $M\subset X$; (ii) there exists $C>0$ such that $\| T_n\|\leq C$. My question is whether or not $M$ has to be a linear subspace. I think it does. The reason I think so is that the proof of the theorem says that we first define an operator $T_0$ by $$ T_0 x:= \lim_{n\to \infty} T_n x. $$ By the assumption that $T_n x$ converges on $M$, we have that the domain of an operator is $M$. Hence, $M$ must be a linear space. The proof then goes on to define an extension of $T_0$ by $$ Ty:= \lim_{n\to \infty} T_0 x_n, $$ where $x_n\to y$. Here we are using the density of $M$ and the boundedness of $T_0$ which is inherited from $\{T_n\}$. Even here though, to prove uniqueness of this limit under any sequence converging to $y$ we need the fact that $M$ is a subspace. The reason is that, if $\{z_n\}$ is any other sequence in $M$ converging to $y$, we have to consider the expression $$ \|T_0 x_n -T_0 z_n\|=\|T_0(x_n-z_n)\|\leq \|T_0\|\cdot \|x_n-z_n\|\to 0$$ since $\{x_n\}$ and $\{z_n\}$ both converge to $y$. We must have that the vector $x_n-z_n$ is in the domain of $T_0$ (i.e. $M$). Anyways, I'm pretty sure that our dense subset $M$ must be a dense subspace for this to work. Can someone please tell me if this is correct. I'm 98% sure this is a typo in the statement of the lemma.","I'm working through the functional analysis book by Milman, Eidelman, and Tsolomitis, and I have a question. The book states a lemma that I'm a bit confused about: A sequence of operators $T_n\in L(X, Y)$ (here, $X$ and $Y$ are Banach Spaces) converges strongly to an operator $T\in L(X,Y)$ if and only if (i) the sequence $\{T_n x\}$ converges for any $x$ in a dense subset $M\subset X$; (ii) there exists $C>0$ such that $\| T_n\|\leq C$. My question is whether or not $M$ has to be a linear subspace. I think it does. The reason I think so is that the proof of the theorem says that we first define an operator $T_0$ by $$ T_0 x:= \lim_{n\to \infty} T_n x. $$ By the assumption that $T_n x$ converges on $M$, we have that the domain of an operator is $M$. Hence, $M$ must be a linear space. The proof then goes on to define an extension of $T_0$ by $$ Ty:= \lim_{n\to \infty} T_0 x_n, $$ where $x_n\to y$. Here we are using the density of $M$ and the boundedness of $T_0$ which is inherited from $\{T_n\}$. Even here though, to prove uniqueness of this limit under any sequence converging to $y$ we need the fact that $M$ is a subspace. The reason is that, if $\{z_n\}$ is any other sequence in $M$ converging to $y$, we have to consider the expression $$ \|T_0 x_n -T_0 z_n\|=\|T_0(x_n-z_n)\|\leq \|T_0\|\cdot \|x_n-z_n\|\to 0$$ since $\{x_n\}$ and $\{z_n\}$ both converge to $y$. We must have that the vector $x_n-z_n$ is in the domain of $T_0$ (i.e. $M$). Anyways, I'm pretty sure that our dense subset $M$ must be a dense subspace for this to work. Can someone please tell me if this is correct. I'm 98% sure this is a typo in the statement of the lemma.",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'banach-spaces']"
78,Is the set of discontinuity of $f$ countable?,Is the set of discontinuity of  countable?,f,"Suppose $f:[0,1]\rightarrow\mathbb{R}$ is a bounded function satisfying: for each $c\in [0,1]$ there exist the limits $\lim_{x\rightarrow c^+}f(x)$ and $\lim_{x\rightarrow c^-}f(x)$. Is true that the set of discontinuity of $f$ is countable?","Suppose $f:[0,1]\rightarrow\mathbb{R}$ is a bounded function satisfying: for each $c\in [0,1]$ there exist the limits $\lim_{x\rightarrow c^+}f(x)$ and $\lim_{x\rightarrow c^-}f(x)$. Is true that the set of discontinuity of $f$ is countable?",,['real-analysis']
79,What is the difference between Lipschitz and general uniform continuity?,What is the difference between Lipschitz and general uniform continuity?,,"Showing that Lipschitz continuity is a subset of uniform continuity isn't hard. Let's assume that it is true that $|f(x_1) - f(x_2)| \leq K(x_1-x_2)$ given an arbitrary value $\epsilon > 0$ let's define $\delta = \epsilon / K$ Then trivially if $|x_1 - x_2| \leq \delta$ : $|f(x_1) - f(x_2)| \leq K\cdot|x_1 - x_2| \leq K\cdot\delta \leq K\cdot\epsilon/K \leq \epsilon$ . Which satisfies the uniform continuity property that for every $\epsilon$ there is a $\delta$ such that $|x_1-x_2| \leq \delta \implies |f(x_1) - f(x_2)| \leq \epsilon$ However, I am having a hard time trying to understand why not all uniformly continuous functions obey this property. I would appreciate a general explanation with at least one example of a non  Lipschitz continous function that is uniformly continuous. Edit: Although I admit I had not found the answer that is linked. That answer, although very good, still does not suffice. The author of the question provided multiple functions that are not Lipshitz continuous, but did not prove why they are not Lipshitz continuous (although he did prove that they are uniformly continuous). My issue is not an absence of an example, but rather a lack of intuition to understand the difference between both.","Showing that Lipschitz continuity is a subset of uniform continuity isn't hard. Let's assume that it is true that given an arbitrary value let's define Then trivially if : . Which satisfies the uniform continuity property that for every there is a such that However, I am having a hard time trying to understand why not all uniformly continuous functions obey this property. I would appreciate a general explanation with at least one example of a non  Lipschitz continous function that is uniformly continuous. Edit: Although I admit I had not found the answer that is linked. That answer, although very good, still does not suffice. The author of the question provided multiple functions that are not Lipshitz continuous, but did not prove why they are not Lipshitz continuous (although he did prove that they are uniformly continuous). My issue is not an absence of an example, but rather a lack of intuition to understand the difference between both.",|f(x_1) - f(x_2)| \leq K(x_1-x_2) \epsilon > 0 \delta = \epsilon / K |x_1 - x_2| \leq \delta |f(x_1) - f(x_2)| \leq K\cdot|x_1 - x_2| \leq K\cdot\delta \leq K\cdot\epsilon/K \leq \epsilon \epsilon \delta |x_1-x_2| \leq \delta \implies |f(x_1) - f(x_2)| \leq \epsilon,"['real-analysis', 'metric-spaces']"
80,"Are ""most"" sets in $\mathbb R$ neither open nor closed?","Are ""most"" sets in  neither open nor closed?",\mathbb R,"It seems intuitive to believe that most subsets of $\mathbb R$ are neither open nor closed. For instance, if we consider the collection of all (open, closed, half-closed/open) intervals, then one can probably make precise the notion that ""half"" of all intervals in this collection are neither open nor closed. (Whether this will amount to a reasonable definition of what it means for most subsets to be neither open nor closed might be up for debate.) If this intuition is correct, is there a way to formalise it? If not, how would we formalise its being wrong? To be clear, I am happy for a fairly broad interpretation of the term ""most"". Natural interpretations include but are not limited to : Measure-theoretic (e.g. is there a natural measure on (a $\sigma$-algebra on) the power set of $\mathbb R$ that assigns negligible measure to $\tau$?) Topological (e.g. is there a natural topology on the power set of $\mathbb R$ where $\tau$ is meagre, or even nowhere dense?) Set-theoretic (e.g. does the power set of $\mathbb R$ have larger cardinality than $\tau$?) Here, $\tau$ is (obviously) the Euclidean topology. Actually, that last version of the question in parentheses might have the easiest answer: Let $\mathcal B$ be the Borel sets on $\mathbb R$. We have that $|\tau| \le | \mathcal B | = | \mathbb R | < \left| 2^{\mathbb R} \right|$. (For details on the equality, see here . For a much simpler proof, see this answer .) Are there alternative ways to make this precise?","It seems intuitive to believe that most subsets of $\mathbb R$ are neither open nor closed. For instance, if we consider the collection of all (open, closed, half-closed/open) intervals, then one can probably make precise the notion that ""half"" of all intervals in this collection are neither open nor closed. (Whether this will amount to a reasonable definition of what it means for most subsets to be neither open nor closed might be up for debate.) If this intuition is correct, is there a way to formalise it? If not, how would we formalise its being wrong? To be clear, I am happy for a fairly broad interpretation of the term ""most"". Natural interpretations include but are not limited to : Measure-theoretic (e.g. is there a natural measure on (a $\sigma$-algebra on) the power set of $\mathbb R$ that assigns negligible measure to $\tau$?) Topological (e.g. is there a natural topology on the power set of $\mathbb R$ where $\tau$ is meagre, or even nowhere dense?) Set-theoretic (e.g. does the power set of $\mathbb R$ have larger cardinality than $\tau$?) Here, $\tau$ is (obviously) the Euclidean topology. Actually, that last version of the question in parentheses might have the easiest answer: Let $\mathcal B$ be the Borel sets on $\mathbb R$. We have that $|\tau| \le | \mathcal B | = | \mathbb R | < \left| 2^{\mathbb R} \right|$. (For details on the equality, see here . For a much simpler proof, see this answer .) Are there alternative ways to make this precise?",,"['real-analysis', 'general-topology', 'measure-theory', 'real-numbers', 'descriptive-set-theory']"
81,Can an open set contain its supremum?,Can an open set contain its supremum?,,"I say the answer is no: By definition,let $A \subset \mathbb{R} $ and let $s$ be an upperbound for $A$. Then, $s=sup A$ if and only if, $\forall \epsilon>0$, $\exists a \in A$ satisfying $s-\epsilon < a$ With this in mind, and I'm having trouble stating it formally, let's assume that $A \subset \mathbb{R}$ is open, as the question states. For a contradiction, let's say that $s \in A$. Let $b$ be an upperbound for $A$. Since $s = sup A$, for an arbitrary $\epsilon$, $s + \epsilon \leq b$, but $b \notin A$. Where I am going with this is that $V_\epsilon (s)$ is not  contained in $A$, which contradicts the fact that $A$ is an open set. How is my logic and how can I improve on that? Thanks!","I say the answer is no: By definition,let $A \subset \mathbb{R} $ and let $s$ be an upperbound for $A$. Then, $s=sup A$ if and only if, $\forall \epsilon>0$, $\exists a \in A$ satisfying $s-\epsilon < a$ With this in mind, and I'm having trouble stating it formally, let's assume that $A \subset \mathbb{R}$ is open, as the question states. For a contradiction, let's say that $s \in A$. Let $b$ be an upperbound for $A$. Since $s = sup A$, for an arbitrary $\epsilon$, $s + \epsilon \leq b$, but $b \notin A$. Where I am going with this is that $V_\epsilon (s)$ is not  contained in $A$, which contradicts the fact that $A$ is an open set. How is my logic and how can I improve on that? Thanks!",,"['real-analysis', 'general-topology']"
82,Probabilistic Proof of $\prod\limits_{i=1}^\infty\cos\left(\frac t{2^i}\right)=\frac{\sin t}t$ [duplicate],Probabilistic Proof of  [duplicate],\prod\limits_{i=1}^\infty\cos\left(\frac t{2^i}\right)=\frac{\sin t}t,This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 6 years ago . Using probability methods (characteristic function?) prove  $$\prod_{i=1}^\infty\cos\left(\frac t{2^i}\right)=\frac{\sin t}t$$ I know what is characteristic function but I have no idea how use it in this task. I will grateful for help.,This question already has answers here : Finding the limit $\lim \limits_{n \to \infty}\ (\cos \frac x 2 \cdot\cos \frac x 4\cdot \cos \frac x 8\cdots \cos \frac x {2^n}) $ (3 answers) Closed 6 years ago . Using probability methods (characteristic function?) prove  $$\prod_{i=1}^\infty\cos\left(\frac t{2^i}\right)=\frac{\sin t}t$$ I know what is characteristic function but I have no idea how use it in this task. I will grateful for help.,,"['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'trigonometry']"
83,Need help with $\int_0^1 \frac{\ln(1+x^2)}{1+x} dx$ [duplicate],Need help with  [duplicate],\int_0^1 \frac{\ln(1+x^2)}{1+x} dx,"This question already has an answer here : integral of $\int_{0}^{1}\frac{\ln(x^{2}+1)}{x+1}dx$ using contour integration? (1 answer) Closed 5 years ago . Can the definite integral $$\int_0^1\dfrac{\ln(1+x^2)}{1+x}dx$$ be evaluated using the technique of “differentiation under integral sign”. I don't want a complete solution, just the parameter would do. PS: An alternative approach (preferably simple) would also be welcome as long as it doesn't involve contour integration.","This question already has an answer here : integral of $\int_{0}^{1}\frac{\ln(x^{2}+1)}{x+1}dx$ using contour integration? (1 answer) Closed 5 years ago . Can the definite integral $$\int_0^1\dfrac{\ln(1+x^2)}{1+x}dx$$ be evaluated using the technique of “differentiation under integral sign”. I don't want a complete solution, just the parameter would do. PS: An alternative approach (preferably simple) would also be welcome as long as it doesn't involve contour integration.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
84,Is there any handwavy argument that shows that $\int_{-\infty}^{\infty} e^{-ikx} dk = 2\pi \delta(x)$?,Is there any handwavy argument that shows that ?,\int_{-\infty}^{\infty} e^{-ikx} dk = 2\pi \delta(x),It should not be a good argument but rather a short one and one that convinces a physicist ( so no need for mathematical rigor ) that shows that $\int_{-\infty}^{\infty} e^{-ikx} dk = 2\pi \delta(x)$ holds? It should only refer to basic calculus (especially no fourier transform ) since I am supposed to give a proof of a related relationship about fourier transforms on a physics homework sheet.,It should not be a good argument but rather a short one and one that convinces a physicist ( so no need for mathematical rigor ) that shows that $\int_{-\infty}^{\infty} e^{-ikx} dk = 2\pi \delta(x)$ holds? It should only refer to basic calculus (especially no fourier transform ) since I am supposed to give a proof of a related relationship about fourier transforms on a physics homework sheet.,,"['calculus', 'real-analysis']"
85,How to prove $C_1 \|x\|_\infty \leq \|x\| \leq C_2 \|x\|_\infty$?,How to prove ?,C_1 \|x\|_\infty \leq \|x\| \leq C_2 \|x\|_\infty,"I want to prove the following theorem (no idea whether it has a name): Let $V = \mathbb{R}^n$ or $\mathbb{C}^n$ and $\|\cdot\|$ be a norm on $V$. Then, there exist $C_1, C_2 > 0$ such that for all $x \in V$: $$C_1 \|x\|_\infty \leq \|x\| \leq C_2 \|x\|_\infty$$ I first let $x \neq 0$ (otherwise it would be trivial). Then, I divided by $\|x\|_\infty$ and normalized the vector $x$ such that $\|x\|_\infty = 1$. That left me with $$C_1 \leq \|x\| \leq C_2$$ but I don't see how this could help me or how I could possibly limit an unknown norm. How can I proceed? Or is this the wrong way anyway? Thanks for any answers.","I want to prove the following theorem (no idea whether it has a name): Let $V = \mathbb{R}^n$ or $\mathbb{C}^n$ and $\|\cdot\|$ be a norm on $V$. Then, there exist $C_1, C_2 > 0$ such that for all $x \in V$: $$C_1 \|x\|_\infty \leq \|x\| \leq C_2 \|x\|_\infty$$ I first let $x \neq 0$ (otherwise it would be trivial). Then, I divided by $\|x\|_\infty$ and normalized the vector $x$ such that $\|x\|_\infty = 1$. That left me with $$C_1 \leq \|x\| \leq C_2$$ but I don't see how this could help me or how I could possibly limit an unknown norm. How can I proceed? Or is this the wrong way anyway? Thanks for any answers.",,"['real-analysis', 'vector-spaces', 'normed-spaces']"
86,"Computing an indefinite integral: $\int \frac{2n!\sin x + x^n }{e^x + \sin x + \cos x + P_n (x)}\, dx $",Computing an indefinite integral:,"\int \frac{2n!\sin x + x^n }{e^x + \sin x + \cos x + P_n (x)}\, dx ","Let $\displaystyle  P_n (x) = 1 + \frac{x}{1!} + \frac{x^2 }{2!} + \cdots + \frac{x^n }{n!} \ $ and $$ I(x) = \int \frac{2n!\sin x + x^n }{e^x  + \sin x + \cos x + P_n (x)}\, dx $$ (where $\ n \to \infty  \ $). This problem is REALLY frustrating to me at the moment, it's 6 AM here and I've been trying to sort it out since 4:30 AM. First of all, I don't get the use of the $P_n(x)$ notation,  isn't that just $e^x$ ? Anyhow...None of my approaches yielded any useful results, so I'm reaching out to you. Can someone suggest anything, at all ? It would be much appreciated, thanks a lot! EDIT : I managed to solve part of it, I'm now stuck with $ I(x) = n!(x - \log [2e^x  + \sin (x) + \cos (x)] + \int {\frac{{x^n }}{{e^x  + \sin x + \cos x + P_n (x)}}} dx $. Can't really figure out if this is much better, but that's all I could get up until this point.","Let $\displaystyle  P_n (x) = 1 + \frac{x}{1!} + \frac{x^2 }{2!} + \cdots + \frac{x^n }{n!} \ $ and $$ I(x) = \int \frac{2n!\sin x + x^n }{e^x  + \sin x + \cos x + P_n (x)}\, dx $$ (where $\ n \to \infty  \ $). This problem is REALLY frustrating to me at the moment, it's 6 AM here and I've been trying to sort it out since 4:30 AM. First of all, I don't get the use of the $P_n(x)$ notation,  isn't that just $e^x$ ? Anyhow...None of my approaches yielded any useful results, so I'm reaching out to you. Can someone suggest anything, at all ? It would be much appreciated, thanks a lot! EDIT : I managed to solve part of it, I'm now stuck with $ I(x) = n!(x - \log [2e^x  + \sin (x) + \cos (x)] + \int {\frac{{x^n }}{{e^x  + \sin x + \cos x + P_n (x)}}} dx $. Can't really figure out if this is much better, but that's all I could get up until this point.",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals', 'closed-form']"
87,A question about a mathematical analysis book,A question about a mathematical analysis book,,"I am a newcomer to Analysis. All knowledge I know about ""Analysis"" are differentials,limit and integration (basically, what we have been taught in high school) I am studying Principles of Mathematical Analysis by Walter Rudin, and I must say that this is, by far, the most difficult book I have ever touched. Even my Ph.D Microeconomic Theory by Mas Colell is not as hard as this book. I struggled a lot with Rudin,trying to draw some pictures about open sets, closed sets,perfect sets,..., but I still cannot grasp the ""gist"" of Analysis. So, I realize that Rudin may not be good enough for self-study due to its superb terseness and decide to switch to other Analysis's book in order to get a good picture of what Real Analysis is. Rudin lacks of ability to do that since it assumes the reader must know some topology to a degree. Hence, I am looking for a good substitution of Rudin and have seen on Amazon that there is a book called ""Mathematical Analysis I"" and ""Mathematical Analysis II"" written by Zorich that can cover Real Analysis that a math major undergraduate needs to know. Is there anyone know how good those books are so I can prepare a very great Analysis courses for myself? I will definitely read Rudin again, but now I think I need some books as great as Spivak's Calculus book so that I can understand Analysis in a very good way. I know Apostol's is a good book, but I want to learn a book that can cover as much material as it can in the shortest time so that I can reach Royden's later on. I thank you very much for your advice. EDIT 1 : So, after having read all of your advice (and Mr.Dave's), I WANT TO ""invent""  a way to read and understand Rudin's Analysis. Since Rudin is now causing me difficulty in understanding concepts in chapter 2's topology(perfect sets,open sets, closed set, closure sets, cantor sets,connected sets), I have 3 choices: Spivak's Calculus => Apostol's => Rudin's => Royden's Spivak's Calculus => Zorich (I+II) => Rudin's => Royden's Spivak's Calculus => Munkres's Introduction to Topology => Rudin's => Royden's Which one do you think it is the most appropriate way? I thank you for your advice. EDIT 2 : I thank you very much for your advice, Mr.Dave. However, I have a big trouble with topology in the very chapter 2 of Rudin's Analysis. After reading Spivak's, does my trouble go away? Because after reading materials in chapter 2, I have A VERY VAGUE understanding of what a limit point, perfect sets,infinite,finite,countable,uncountable, open sets,closed sets,...etc. Topology is really really difficult.","I am a newcomer to Analysis. All knowledge I know about ""Analysis"" are differentials,limit and integration (basically, what we have been taught in high school) I am studying Principles of Mathematical Analysis by Walter Rudin, and I must say that this is, by far, the most difficult book I have ever touched. Even my Ph.D Microeconomic Theory by Mas Colell is not as hard as this book. I struggled a lot with Rudin,trying to draw some pictures about open sets, closed sets,perfect sets,..., but I still cannot grasp the ""gist"" of Analysis. So, I realize that Rudin may not be good enough for self-study due to its superb terseness and decide to switch to other Analysis's book in order to get a good picture of what Real Analysis is. Rudin lacks of ability to do that since it assumes the reader must know some topology to a degree. Hence, I am looking for a good substitution of Rudin and have seen on Amazon that there is a book called ""Mathematical Analysis I"" and ""Mathematical Analysis II"" written by Zorich that can cover Real Analysis that a math major undergraduate needs to know. Is there anyone know how good those books are so I can prepare a very great Analysis courses for myself? I will definitely read Rudin again, but now I think I need some books as great as Spivak's Calculus book so that I can understand Analysis in a very good way. I know Apostol's is a good book, but I want to learn a book that can cover as much material as it can in the shortest time so that I can reach Royden's later on. I thank you very much for your advice. EDIT 1 : So, after having read all of your advice (and Mr.Dave's), I WANT TO ""invent""  a way to read and understand Rudin's Analysis. Since Rudin is now causing me difficulty in understanding concepts in chapter 2's topology(perfect sets,open sets, closed set, closure sets, cantor sets,connected sets), I have 3 choices: Spivak's Calculus => Apostol's => Rudin's => Royden's Spivak's Calculus => Zorich (I+II) => Rudin's => Royden's Spivak's Calculus => Munkres's Introduction to Topology => Rudin's => Royden's Which one do you think it is the most appropriate way? I thank you for your advice. EDIT 2 : I thank you very much for your advice, Mr.Dave. However, I have a big trouble with topology in the very chapter 2 of Rudin's Analysis. After reading Spivak's, does my trouble go away? Because after reading materials in chapter 2, I have A VERY VAGUE understanding of what a limit point, perfect sets,infinite,finite,countable,uncountable, open sets,closed sets,...etc. Topology is really really difficult.",,"['calculus', 'real-analysis', 'analysis', 'reference-request', 'education']"
88,"If $x_1=5$, $x_{n+1}=x_n^2-2$, find $\lim x_{n+1}/(x_1\cdots x_n)$","If , , find",x_1=5 x_{n+1}=x_n^2-2 \lim x_{n+1}/(x_1\cdots x_n),"If $$\left\{x_{n}\right\}\mid x_{1}=5,x_{n+1}=x_{n}^{2}-2,\forall n\geq 1$$ find $$\lim_{n\to\infty}\frac{x_{n+1}}{x_{1}x_{2}\cdots x_{n}}.$$ If someone could help me out with tags, it'd be lovely. I think this is calculus and real-analysis, but I'm not sure--I had the problem scribbled down on a post-it, and I forget where it's from.","If $$\left\{x_{n}\right\}\mid x_{1}=5,x_{n+1}=x_{n}^{2}-2,\forall n\geq 1$$ find $$\lim_{n\to\infty}\frac{x_{n+1}}{x_{1}x_{2}\cdots x_{n}}.$$ If someone could help me out with tags, it'd be lovely. I think this is calculus and real-analysis, but I'm not sure--I had the problem scribbled down on a post-it, and I forget where it's from.",,"['real-analysis', 'sequences-and-series', 'limits']"
89,"Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property?","Equivalence of Rolle's theorem, the mean value theorem, and the least upper bound property?",,"How to show that Rolle's theorem, the Mean Value Theorem are equivalent to the least upper bound property? I'm thinking of starting like this: Let F be an ordered field that does not satisfy the least upper bound property, and then deduce that F does not satisfy either Rolle's or MVT. But I'm not sure how to continue, any help please? thanks!","How to show that Rolle's theorem, the Mean Value Theorem are equivalent to the least upper bound property? I'm thinking of starting like this: Let F be an ordered field that does not satisfy the least upper bound property, and then deduce that F does not satisfy either Rolle's or MVT. But I'm not sure how to continue, any help please? thanks!",,['real-analysis']
90,Does a continuous and 1-1 function map Borel sets to Borel sets?,Does a continuous and 1-1 function map Borel sets to Borel sets?,,"Suppose $f: \mathbb{R} \to \mathbb{R}$ is a continuous function which is 1-1, then does $f$ map Borel sets onto Borel sets?","Suppose $f: \mathbb{R} \to \mathbb{R}$ is a continuous function which is 1-1, then does $f$ map Borel sets onto Borel sets?",,['real-analysis']
91,Definite integral - possible evaluation using real methods?,Definite integral - possible evaluation using real methods?,,"The book ""inside interesting integrals"" gives the following exercise for the chapter about contour integration and the residue theorem: $$\int_{0}^\infty \frac{e^{\cos x}\sin(\sin x)}{x}dx=\space\space ?$$ This can be solved using the function $$f(z)=\frac{\exp(e^{iz})}{z}$$ on a quarter-circular contour, and is pretty straightforward. The answer turns out to be $$\frac{\pi}{2}(e-1)$$ However, in the book, the author makes the following comment: Edward Copson (1901-1980), who was professor of mathematics at the University of St. Andrews in Scotland, wrote ""A definite integral which can be evaluated using Cauchy's method of residues can always be evaluated by other means, though generally not so simply."" Here's an example of what Copson meant, an integral attributed to the great Cauchy himself. It is easily done with contour integration, but would (I think) otherwise be pretty darn tough. Does anyone know how to evaluate this integral using real methods?","The book ""inside interesting integrals"" gives the following exercise for the chapter about contour integration and the residue theorem: $$\int_{0}^\infty \frac{e^{\cos x}\sin(\sin x)}{x}dx=\space\space ?$$ This can be solved using the function $$f(z)=\frac{\exp(e^{iz})}{z}$$ on a quarter-circular contour, and is pretty straightforward. The answer turns out to be $$\frac{\pi}{2}(e-1)$$ However, in the book, the author makes the following comment: Edward Copson (1901-1980), who was professor of mathematics at the University of St. Andrews in Scotland, wrote ""A definite integral which can be evaluated using Cauchy's method of residues can always be evaluated by other means, though generally not so simply."" Here's an example of what Copson meant, an integral attributed to the great Cauchy himself. It is easily done with contour integration, but would (I think) otherwise be pretty darn tough. Does anyone know how to evaluate this integral using real methods?",,"['real-analysis', 'complex-analysis', 'definite-integrals', 'improper-integrals', 'residue-calculus']"
92,"What is wrong with the ""proof"" for $\ln(2) =\frac{1}{2}\ln(2)$?","What is wrong with the ""proof"" for ?",\ln(2) =\frac{1}{2}\ln(2),"I have got a question which is as follows: Is  $\ln(2)=\frac{1}{2}\ln(2)$?? The following argument seems suggesting that the answer is yes: We have the series $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$   which has a mathematically determined value $\ln(2)=0.693$. Now, let's do some rearrangement: $$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......$$    $$  (1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}.......$$   $$\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\frac{1}{10}-\frac{1}{12}......$$    $$\frac{1}{2}(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......)$$ $$\frac{1}{2}\ln(2).$$ I know that mathematics can't be wrong, and I have done something wrong. But here is my question : where does the argument above go wrong?","I have got a question which is as follows: Is  $\ln(2)=\frac{1}{2}\ln(2)$?? The following argument seems suggesting that the answer is yes: We have the series $1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots$   which has a mathematically determined value $\ln(2)=0.693$. Now, let's do some rearrangement: $$1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......$$    $$  (1-\frac{1}{2})-\frac{1}{4}+(\frac{1}{3}-\frac{1}{6})-\frac{1}{8}+(\frac{1}{5}-\frac{1}{10})-\frac{1}{12}.......$$   $$\frac{1}{2}-\frac{1}{4}+\frac{1}{6}-\frac{1}{8}+\frac{1}{10}-\frac{1}{12}......$$    $$\frac{1}{2}(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\frac{1}{5}-\frac{1}{6}+\frac{1}{7}-\frac{1}{8}+\frac{1}{9}-\frac{1}{10}+\frac{1}{11}-\frac{1}{12}......)$$ $$\frac{1}{2}\ln(2).$$ I know that mathematics can't be wrong, and I have done something wrong. But here is my question : where does the argument above go wrong?",,"['real-analysis', 'sequences-and-series', 'fake-proofs']"
93,Taylor expansion of composite function,Taylor expansion of composite function,,"My confusion is slightly related to this question . Suppose we have two nice functions $f(x)$ and $g(x)$, how do we find Taylor series of $f(g(x))$? To be more concrete, consider $f(x^2)$. In this case, we can regard it as $f(g(x))$ where $g(x) = x^2$. One way to find the Taylor series around $1$ is just $$f(x^2)=f(1)+f'(1)(x^2-1)+\frac{1}{2}f''(1)(x^2 - 1)^2+\cdots$$ However, what if I do this? $$f(x^2)=f(g(x))=f(1)+\color{blue}{\frac{d}{dg}f(g(x))\cdot\frac{d}{dx}g(x)\biggr|_{x=1}}(x-1)+\frac{1}{2}\frac{d^2}{d^2 x}f(g(x))\biggr|_{x=1}(x-1)^2+\cdots$$ This ends up like $$f(x^2) = f(1)+2f'(1)(x-1)+\cdots$$ Which looks different from what you should get. I am confused because I thought both methods are valid and they should agree (although I've always been using the first one, regarding $x^2$ as a ""number"" instead of a function).","My confusion is slightly related to this question . Suppose we have two nice functions $f(x)$ and $g(x)$, how do we find Taylor series of $f(g(x))$? To be more concrete, consider $f(x^2)$. In this case, we can regard it as $f(g(x))$ where $g(x) = x^2$. One way to find the Taylor series around $1$ is just $$f(x^2)=f(1)+f'(1)(x^2-1)+\frac{1}{2}f''(1)(x^2 - 1)^2+\cdots$$ However, what if I do this? $$f(x^2)=f(g(x))=f(1)+\color{blue}{\frac{d}{dg}f(g(x))\cdot\frac{d}{dx}g(x)\biggr|_{x=1}}(x-1)+\frac{1}{2}\frac{d^2}{d^2 x}f(g(x))\biggr|_{x=1}(x-1)^2+\cdots$$ This ends up like $$f(x^2) = f(1)+2f'(1)(x-1)+\cdots$$ Which looks different from what you should get. I am confused because I thought both methods are valid and they should agree (although I've always been using the first one, regarding $x^2$ as a ""number"" instead of a function).",,"['real-analysis', 'sequences-and-series']"
94,Why does a Borel measurable function imply its Lebesgue measure?,Why does a Borel measurable function imply its Lebesgue measure?,,"Borel measurable defined as: $f: D \rightarrow\mathbb R$ is Borel measurable if $D$ is a Borel set and for each $a\in\mathbb R$, the set $\{x\in D: f(x) > a\}$ is a Borel set. Definition of Lebesgue measurable function is: Given a function $f: D\rightarrow\mathbb R$, defined on some domain $D$, we say that $f$ is Lebesgue measurable if $D$ is measurable and for each $a\in\mathbb R$, the set $\{x\in D: f(x) > a\}$ is measurable. Intuitively, I think Lebesgue measure function is essentially a function with both input and output(or say domain and range) being Lebesgue measurable sets. Since preimage of a Borel set is another Borel set as well under Lebesgue measurable function and Borel measurable function asks for a Borel set, a Lebesgue measurable set, as the image and with the domain is a Borel set as well, I can claim that a Borel measurable function is Lebesgue measurable. I'm not sure whether my idea is correct especially which one of them, the range set and domain set, is required being Lebesgue measurable first? Hope some can help me correct it or offer me with better explains or proofs. Appreciate much^_^ update: Range being measurable should be first.","Borel measurable defined as: $f: D \rightarrow\mathbb R$ is Borel measurable if $D$ is a Borel set and for each $a\in\mathbb R$, the set $\{x\in D: f(x) > a\}$ is a Borel set. Definition of Lebesgue measurable function is: Given a function $f: D\rightarrow\mathbb R$, defined on some domain $D$, we say that $f$ is Lebesgue measurable if $D$ is measurable and for each $a\in\mathbb R$, the set $\{x\in D: f(x) > a\}$ is measurable. Intuitively, I think Lebesgue measure function is essentially a function with both input and output(or say domain and range) being Lebesgue measurable sets. Since preimage of a Borel set is another Borel set as well under Lebesgue measurable function and Borel measurable function asks for a Borel set, a Lebesgue measurable set, as the image and with the domain is a Borel set as well, I can claim that a Borel measurable function is Lebesgue measurable. I'm not sure whether my idea is correct especially which one of them, the range set and domain set, is required being Lebesgue measurable first? Hope some can help me correct it or offer me with better explains or proofs. Appreciate much^_^ update: Range being measurable should be first.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
95,A sequence converges if and only if every subsequence converges?,A sequence converges if and only if every subsequence converges?,,"I want to prove this and intuitively it makes sense. But I'm having a hard time coming up with a proof. So if a sequence converges, then we have a natural number for which the distance between all terms after it and the limit point get arbitrarily small. So how can I show that this also holds for every subsequence (which is like a subset of a sequence)? (are subsequences always infinite?) Could I suppose that there is a subsequence that doesn't converge to that limit, and find a contradiction? (and do the same for the other direction?)","I want to prove this and intuitively it makes sense. But I'm having a hard time coming up with a proof. So if a sequence converges, then we have a natural number for which the distance between all terms after it and the limit point get arbitrarily small. So how can I show that this also holds for every subsequence (which is like a subset of a sequence)? (are subsequences always infinite?) Could I suppose that there is a subsequence that doesn't converge to that limit, and find a contradiction? (and do the same for the other direction?)",,"['real-analysis', 'sequences-and-series', 'general-topology', 'convergence-divergence']"
96,Integral $\int_0^\infty \log(1+x^2)\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}=4-\pi$?,Integral ?,\int_0^\infty \log(1+x^2)\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}=4-\pi,"Hello am looking for a solution to proving this. $$ I:=\int_0^\infty \log(1+x^2)\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}=4-\pi. $$ This one is related to Integral $\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi}$ that the community together seemed to solve! I tried writing $$ I=\int_0^\infty \log(1+x^2)\frac{dx}{x^2\cosh \pi x}+\int_0^\infty \log(1+x^2)\frac{\pi x\tanh \pi x}{\cosh \pi x }\frac{dx}{x^2} $$ but is not so clear now that I have two integrals to solve.  I wasn't sure how integrating by parts would give me a clearer integral as the terms do not clean up here like the last one.  I am not sure how else Introducing something like $I(\alpha), I'(\alpha)$ in this situation did help but not much after this: $$ I(\alpha)=\int_0^\infty \log(1+\alpha x^2) \frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}, \frac{dI}{d\alpha}= $$  $$ \int_0^\infty \frac{dx}{1+\alpha x^2}\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}=\int_0^\infty \frac{dx}{(1+\alpha x^2)\cosh \pi x}+\pi\int_0^\infty \frac{dx}{1+\alpha x^2}\frac{x\tanh \pi x}{\cosh \pi x} $$   Thank you","Hello am looking for a solution to proving this. $$ I:=\int_0^\infty \log(1+x^2)\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}=4-\pi. $$ This one is related to Integral $\int_0^\infty \log(1+x^2)\frac{\cosh{\frac{\pi x}{2}}}{\sinh^2{\frac{\pi x}{2}}}\mathrm dx=2-\frac{4}{\pi}$ that the community together seemed to solve! I tried writing $$ I=\int_0^\infty \log(1+x^2)\frac{dx}{x^2\cosh \pi x}+\int_0^\infty \log(1+x^2)\frac{\pi x\tanh \pi x}{\cosh \pi x }\frac{dx}{x^2} $$ but is not so clear now that I have two integrals to solve.  I wasn't sure how integrating by parts would give me a clearer integral as the terms do not clean up here like the last one.  I am not sure how else Introducing something like $I(\alpha), I'(\alpha)$ in this situation did help but not much after this: $$ I(\alpha)=\int_0^\infty \log(1+\alpha x^2) \frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}\frac{dx}{x^2}, \frac{dI}{d\alpha}= $$  $$ \int_0^\infty \frac{dx}{1+\alpha x^2}\frac{\cosh \pi x +\pi x\sinh \pi x}{\cosh^2 \pi x}=\int_0^\infty \frac{dx}{(1+\alpha x^2)\cosh \pi x}+\pi\int_0^\infty \frac{dx}{1+\alpha x^2}\frac{x\tanh \pi x}{\cosh \pi x} $$   Thank you",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
97,Can subsequences be finite?,Can subsequences be finite?,,"Say I'm given the sequence $\{a_1,a_2,a_3,\dots\}$. Does a subsequence have to be infinite? Or can it be finite too? For example, is $\{a_1,a_2,a_3\}$ a subsequence?","Say I'm given the sequence $\{a_1,a_2,a_3,\dots\}$. Does a subsequence have to be infinite? Or can it be finite too? For example, is $\{a_1,a_2,a_3\}$ a subsequence?",,['real-analysis']
98,Fourier transform of a real function is real,Fourier transform of a real function is real,,"I was trying to find the Fourier transform of the function $$x \mapsto \frac1{x^2 - 2x  +2}$$ and I keep getting something with non-zero imaginary part. But the Fourier transform of a real function should be real, right? So I must be making a mistake? What is a proof that the Fourier transform of a real function is real?","I was trying to find the Fourier transform of the function and I keep getting something with non-zero imaginary part. But the Fourier transform of a real function should be real, right? So I must be making a mistake? What is a proof that the Fourier transform of a real function is real?",x \mapsto \frac1{x^2 - 2x  +2},"['real-analysis', 'integration', 'fourier-analysis', 'fourier-transform']"
99,Am I allowed to use distributive law for infinitely many sets?,Am I allowed to use distributive law for infinitely many sets?,,"Let $X$ be a nonempty set. A collection $S$ of subsets of $X$ is    called a semiring if it satisfies the following properties: The empty set belongs to S; that is $\emptyset \in S$. If $A,B \in S$; then $A \cap B \in S$; that is, $S$ is closed under finite intersections. The set difference of any two sets of $S$ can be written as a finite union of    pair-wise disjoint members of $S$. That is, for every $A, B \in S$; there exist   $C_1, ...,C_n \in S$ (depending on $A$ and $B$) such that $A\setminus B = \cup _{i=1}^n C_i$ and $C_i \cap C_j = \emptyset$ if $i\ne j$. Now, let $S$ be a semiring of subsets of $X$. A subset $A$ of $X$ is called a $\sigma$-set with respect to $S$ (or simply a $\sigma$-set) if there exists a disjoint sequence $\{A_n\}$ of  $S$ such that $A = \cup_{n=1}^\infty A_n$. One can show easily that for every sequence $\{A_n\}$ of $S$, the set $A = \cup_{n=1}^\infty A_n$ is a $\sigma$-set. I would like to prove that finite intersections of $\sigma$-sets is a $\sigma$-set. For this purpose, suppose $A ,B$ are $\sigma$-sets then $A = \cup_{i=1}^\infty C_i$ , $B = \cup_{j=1}^\infty D_j$. $$A\cap B=(\cup_{i=1}^\infty C_i)\cap (\cup_{j=1}^\infty D_j)$$ In this step I don't know am I allowed to use distributive law for infinitely many sets? Or the law holds only for finitely many sets? If it holds only for finitely many sets how do I prove that finite intersections of $\sigma$-sets is a $\sigma$-set? Thanks.","Let $X$ be a nonempty set. A collection $S$ of subsets of $X$ is    called a semiring if it satisfies the following properties: The empty set belongs to S; that is $\emptyset \in S$. If $A,B \in S$; then $A \cap B \in S$; that is, $S$ is closed under finite intersections. The set difference of any two sets of $S$ can be written as a finite union of    pair-wise disjoint members of $S$. That is, for every $A, B \in S$; there exist   $C_1, ...,C_n \in S$ (depending on $A$ and $B$) such that $A\setminus B = \cup _{i=1}^n C_i$ and $C_i \cap C_j = \emptyset$ if $i\ne j$. Now, let $S$ be a semiring of subsets of $X$. A subset $A$ of $X$ is called a $\sigma$-set with respect to $S$ (or simply a $\sigma$-set) if there exists a disjoint sequence $\{A_n\}$ of  $S$ such that $A = \cup_{n=1}^\infty A_n$. One can show easily that for every sequence $\{A_n\}$ of $S$, the set $A = \cup_{n=1}^\infty A_n$ is a $\sigma$-set. I would like to prove that finite intersections of $\sigma$-sets is a $\sigma$-set. For this purpose, suppose $A ,B$ are $\sigma$-sets then $A = \cup_{i=1}^\infty C_i$ , $B = \cup_{j=1}^\infty D_j$. $$A\cap B=(\cup_{i=1}^\infty C_i)\cap (\cup_{j=1}^\infty D_j)$$ In this step I don't know am I allowed to use distributive law for infinitely many sets? Or the law holds only for finitely many sets? If it holds only for finitely many sets how do I prove that finite intersections of $\sigma$-sets is a $\sigma$-set? Thanks.",,"['real-analysis', 'elementary-set-theory']"
