,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,I would like to calculate this limit: $ \lim_{n \to \infty}(n^2+1)\cdot(\ln(n^2-4)-2\ln(n)) $,I would like to calculate this limit:, \lim_{n \to \infty}(n^2+1)\cdot(\ln(n^2-4)-2\ln(n)) ,I would like to calculate this limit: $$ \lim_{n \to \infty}(n^2+1)\cdot(\ln(n^2-4)-2\ln(n)) $$ but I am a bit lost on how to tackle the logarithm. Any help would be greatly appreciated.,I would like to calculate this limit: $$ \lim_{n \to \infty}(n^2+1)\cdot(\ln(n^2-4)-2\ln(n)) $$ but I am a bit lost on how to tackle the logarithm. Any help would be greatly appreciated.,,"['calculus', 'real-analysis', 'limits', 'logarithms']"
1,Convergence of sequence of ratio of consecutive terms.,Convergence of sequence of ratio of consecutive terms.,,"I would like to prove that the following sequence of ratios $(x_n)$ where $x_n = c_n/c_{n-1}$ converges to a finite limit $L$: $$\lim_{n\to\infty} x_n = \lim_{n\to\infty}\frac{c_n}{c_{n-1}} = L$$ Where $c_n$ is defined by the recurrence relation: $c_n = c_{n-1} + \lfloor c_{n-4}/2 \rfloor$ for $n \geq 4$, with initial conditions $c_0=3, c_1=4, c_2=5, c_3=6$. I feel there should be a similar approach to proving limit of consecutive Fibonacci numbers converges, which I think might require me to solve the recurrence, but the floor function doesn't help. Some observations, verification by computer software has led me to predict that the limit tends towards $L = 1.25372495821...$, which appears to be very close to a solution to the equation: $$x^4 - x^3 - \frac{1}{2}=0$$ This would be the characteristic equation to the recurrence relation $\alpha_n = \alpha_{n-1} + \alpha_{n-4}/2$, which is very similar to the way how $c_n$ is defined. Any ideas? I exhausted most of the standard undergraduate analysis techniques for proving sequences converge, no luck though.","I would like to prove that the following sequence of ratios $(x_n)$ where $x_n = c_n/c_{n-1}$ converges to a finite limit $L$: $$\lim_{n\to\infty} x_n = \lim_{n\to\infty}\frac{c_n}{c_{n-1}} = L$$ Where $c_n$ is defined by the recurrence relation: $c_n = c_{n-1} + \lfloor c_{n-4}/2 \rfloor$ for $n \geq 4$, with initial conditions $c_0=3, c_1=4, c_2=5, c_3=6$. I feel there should be a similar approach to proving limit of consecutive Fibonacci numbers converges, which I think might require me to solve the recurrence, but the floor function doesn't help. Some observations, verification by computer software has led me to predict that the limit tends towards $L = 1.25372495821...$, which appears to be very close to a solution to the equation: $$x^4 - x^3 - \frac{1}{2}=0$$ This would be the characteristic equation to the recurrence relation $\alpha_n = \alpha_{n-1} + \alpha_{n-4}/2$, which is very similar to the way how $c_n$ is defined. Any ideas? I exhausted most of the standard undergraduate analysis techniques for proving sequences converge, no luck though.",,"['sequences-and-series', 'limits', 'recurrence-relations']"
2,proof that $x/c^x$ goes to 0,proof that  goes to 0,x/c^x,"Im trying to show that $\lim_{x\to\infty}\frac{x}{c^x}$ where $c>1$ is a constant goes to zero.  To show this i know i need to take some $\epsilon$ and find some $m$ such that $\forall x>m$, $|\frac{x}{c^x}|<\epsilon$.  However, im having trouble finding $m$. What is the usual way people go to find $m$?","Im trying to show that $\lim_{x\to\infty}\frac{x}{c^x}$ where $c>1$ is a constant goes to zero.  To show this i know i need to take some $\epsilon$ and find some $m$ such that $\forall x>m$, $|\frac{x}{c^x}|<\epsilon$.  However, im having trouble finding $m$. What is the usual way people go to find $m$?",,"['real-analysis', 'limits']"
3,Team Fortress 2: Choosing Between Two Weapons,Team Fortress 2: Choosing Between Two Weapons,,"Background : In Team Fortress 2, there are two secondary weapons that the 'Heavy' class can choose between. I am trying to figure out what the best way to compare the two weapons mathematically is. The first weapon is the default Shotgun . It shoots $1.6$ times per second, and has a clip size of $6$. For simplicity, let's assume that it does $60$ damage every time it hits. The second weapon is called the Family Business . It shoots $15\%$ faster than the regular shotgun, so it shoots $1.84$ times per second. It has a clip size of $8$, and it does $15\%$ less damage than the regular shotgun, so that means it does $51$ damage every time it hits. Finally, both weapons must be reloaded after their clips are emptied. It takes $3.5$ seconds for the regular shotgun to reload completely, and it takes $4.5$ seconds for the family business to reload completely. My Calculations (so far): The family business shoots $15\%$ faster and does $15\%$ less damage per shot. $$1.15\cdot0.85 = 0.9775$$ So, ignoring clip sizes, the family business should intuitively do about $2.25\%$ less damage than the regular shotgun while both weapons are continuously firing (ignoring reloading times). The shotgun takes $6/1.6=3.75$ seconds to empty its clip. The family business takes $8/1.84 \approx 4.35$ seconds to empty its clip. That means that if both weapons are shooting for less than or equal to $3.75$ seconds, then the family business should do $2.25\%$ less damage. My Question : Which of these two weapons has a greater average damage per second in an indefinitely prolonged battle? In other words, if $D_s(t)$ and $D_{fb}(t)$ are the damage done after $t$ seconds of continuous shooting by the shotgun and family business, respectively, then what is the value of this limit? $$\lim_{t\to\infty} \frac{D_{fb}(t)}{D_{s}(t)}$$ The answer should represent ratio of the average damage per second of the family business to that of the regular shotgun, over an indefinitely long shootout. I know the value for $t=3.75$. $$\frac{D_{fb}(3.75)}{D_{s}(3.75)}=0.9775$$ Any help is appreciated, thank you!","Background : In Team Fortress 2, there are two secondary weapons that the 'Heavy' class can choose between. I am trying to figure out what the best way to compare the two weapons mathematically is. The first weapon is the default Shotgun . It shoots $1.6$ times per second, and has a clip size of $6$. For simplicity, let's assume that it does $60$ damage every time it hits. The second weapon is called the Family Business . It shoots $15\%$ faster than the regular shotgun, so it shoots $1.84$ times per second. It has a clip size of $8$, and it does $15\%$ less damage than the regular shotgun, so that means it does $51$ damage every time it hits. Finally, both weapons must be reloaded after their clips are emptied. It takes $3.5$ seconds for the regular shotgun to reload completely, and it takes $4.5$ seconds for the family business to reload completely. My Calculations (so far): The family business shoots $15\%$ faster and does $15\%$ less damage per shot. $$1.15\cdot0.85 = 0.9775$$ So, ignoring clip sizes, the family business should intuitively do about $2.25\%$ less damage than the regular shotgun while both weapons are continuously firing (ignoring reloading times). The shotgun takes $6/1.6=3.75$ seconds to empty its clip. The family business takes $8/1.84 \approx 4.35$ seconds to empty its clip. That means that if both weapons are shooting for less than or equal to $3.75$ seconds, then the family business should do $2.25\%$ less damage. My Question : Which of these two weapons has a greater average damage per second in an indefinitely prolonged battle? In other words, if $D_s(t)$ and $D_{fb}(t)$ are the damage done after $t$ seconds of continuous shooting by the shotgun and family business, respectively, then what is the value of this limit? $$\lim_{t\to\infty} \frac{D_{fb}(t)}{D_{s}(t)}$$ The answer should represent ratio of the average damage per second of the family business to that of the regular shotgun, over an indefinitely long shootout. I know the value for $t=3.75$. $$\frac{D_{fb}(3.75)}{D_{s}(3.75)}=0.9775$$ Any help is appreciated, thank you!",,"['limits', 'optimization', 'arithmetic']"
4,"How do I prove there is no limit at $(0,0)$?",How do I prove there is no limit at ?,"(0,0)","$$f(x,y)=\frac{2^{xy}-1}{|x|+|y|}$$ I got to the conclusion that there is no limit, but I am not sure how to prove it.","$$f(x,y)=\frac{2^{xy}-1}{|x|+|y|}$$ I got to the conclusion that there is no limit, but I am not sure how to prove it.",,['limits']
5,Give me hints for evaluating this limit,Give me hints for evaluating this limit,,"Evaluate this limit: $$\lim_{n\to\infty}{\left\{\left(1+\frac{1}{n}\right)^n-\left(1+\frac{1}{n}\right)\right\}}^{-n}$$ Please give me some hints. If you provide a complete answer instead, please include a spoiler tag.","Evaluate this limit: $$\lim_{n\to\infty}{\left\{\left(1+\frac{1}{n}\right)^n-\left(1+\frac{1}{n}\right)\right\}}^{-n}$$ Please give me some hints. If you provide a complete answer instead, please include a spoiler tag.",,"['real-analysis', 'analysis', 'limits']"
6,Compute the limit of the sequence of functions of $\lim_{n\to \infty} f_n(x) = \frac{x^2}{x^2+(1-nx)^2}$,Compute the limit of the sequence of functions of,\lim_{n\to \infty} f_n(x) = \frac{x^2}{x^2+(1-nx)^2},Compute the limit of the sequence of functions of  $$ \lim_{n\to \infty} f_n(x) = \frac{x^2}{x^2+(1-nx)^2}. $$ Attempt. $$\lim_{n\to \infty} f_n(x) = \lim_{n\to \infty} \frac{x^2}{x^2+(1-nx)^2} = \lim_{n\to \infty} \frac{x^2}{x^2+n^2x^2-2nx + 1} $$ $$= \lim_{n\to \infty} \frac{x^2}{x^2+\frac{x^2}{n^2}-\frac{2x}{n} + \frac{1}{n}} = \lim_{n\to \infty} \frac{x^2}{x^2} = 1$$,Compute the limit of the sequence of functions of  $$ \lim_{n\to \infty} f_n(x) = \frac{x^2}{x^2+(1-nx)^2}. $$ Attempt. $$\lim_{n\to \infty} f_n(x) = \lim_{n\to \infty} \frac{x^2}{x^2+(1-nx)^2} = \lim_{n\to \infty} \frac{x^2}{x^2+n^2x^2-2nx + 1} $$ $$= \lim_{n\to \infty} \frac{x^2}{x^2+\frac{x^2}{n^2}-\frac{2x}{n} + \frac{1}{n}} = \lim_{n\to \infty} \frac{x^2}{x^2} = 1$$,,"['sequences-and-series', 'limits', 'proof-verification']"
7,Limit of $\lim\limits_{n\to\infty}\frac{\sum_{m=0}^n (2m+1)^k}{n^{k+1}}$,Limit of,\lim\limits_{n\to\infty}\frac{\sum_{m=0}^n (2m+1)^k}{n^{k+1}},I wanted to find the limit of: ($k \in N)$ $$\lim_{n \to \infty}{\frac{1^k+3^k+5^k+\cdots+(2n+1)^k}{n^{k+1}}}.$$ Stolz–Cesàro theorem could help but $\frac{a_n-a_{n-1}}{b_n-b_{n-1}}$ makes big mess here: $$\lim_{n \to \infty}{\frac{-0^k+1^k-2^k+3^k-4^k+5^k-6^k+\cdots-(2n)^k+(2n+1)^k}{n^{k+1}-(n-1)^{k+1}}}.$$ Is following statement true as well $$\lim_{n \to \infty} \frac{a_n}{b_n} = \lim_{n \to \infty} \frac{a_n-a_{n-2}}{b_n-b_{n-2}}$$?,I wanted to find the limit of: ($k \in N)$ $$\lim_{n \to \infty}{\frac{1^k+3^k+5^k+\cdots+(2n+1)^k}{n^{k+1}}}.$$ Stolz–Cesàro theorem could help but $\frac{a_n-a_{n-1}}{b_n-b_{n-1}}$ makes big mess here: $$\lim_{n \to \infty}{\frac{-0^k+1^k-2^k+3^k-4^k+5^k-6^k+\cdots-(2n)^k+(2n+1)^k}{n^{k+1}-(n-1)^{k+1}}}.$$ Is following statement true as well $$\lim_{n \to \infty} \frac{a_n}{b_n} = \lim_{n \to \infty} \frac{a_n-a_{n-2}}{b_n-b_{n-2}}$$?,,['limits']
8,prove equality $\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \ln a $ [duplicate],prove equality  [duplicate],\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \ln a ,This question already has answers here : The limit as $n$ approaches infinity of $n\left(a^{1/n}-1\right)$ (4 answers) Closed 4 years ago . $$\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \ln a $$ since $a > 0$ My attempt is $\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \lim_{n\to\infty} \ln e^{n(\sqrt[n]{a} - 1)} = \lim_{n\to\infty} \ln e^{(n\sqrt[n]{a} - n)} $ $\lim_{n\to\infty} \ln e^{(n\sqrt[n]{a} - n)} = \lim_{n\to\infty} \ln \frac{e^{n\sqrt[n]{a}}}{e^n} $ And I do not know what to do then.,This question already has answers here : The limit as $n$ approaches infinity of $n\left(a^{1/n}-1\right)$ (4 answers) Closed 4 years ago . $$\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \ln a $$ since $a > 0$ My attempt is $\lim_{n\to\infty}n(\sqrt[n]{a} - 1) = \lim_{n\to\infty} \ln e^{n(\sqrt[n]{a} - 1)} = \lim_{n\to\infty} \ln e^{(n\sqrt[n]{a} - n)} $ $\lim_{n\to\infty} \ln e^{(n\sqrt[n]{a} - n)} = \lim_{n\to\infty} \ln \frac{e^{n\sqrt[n]{a}}}{e^n} $ And I do not know what to do then.,,"['calculus', 'limits', 'radicals']"
9,A recursively defined sequence and a limit,A recursively defined sequence and a limit,,"Fix real numbers $ a_0 $, $ a_1 $ and define, $$ a_{n+1} = a_n +  \Big(\frac{2}{n+1} \Big) a_{n-1} \space \space \forall \space n \ge 1 $$ Show that the sequence $ \Big\{ \dfrac{a_n}{n^2} \Big\}_{n=1}^{ \infty} $ is convergent and find its limit. This question is from an old Miklos Schweitzer competition (1958, problem 7, as pointed out by @user37238). I don't have any good ideas of approaching this question. Writing the recursion as $ (n+1) (a_{n+1}-a_n)=2a_{n-1} $ gives me the impression that it is a relation between the coefficients of a power series, obtained as a solution to some differential equation, although this line of thought doesn't lead me too far.","Fix real numbers $ a_0 $, $ a_1 $ and define, $$ a_{n+1} = a_n +  \Big(\frac{2}{n+1} \Big) a_{n-1} \space \space \forall \space n \ge 1 $$ Show that the sequence $ \Big\{ \dfrac{a_n}{n^2} \Big\}_{n=1}^{ \infty} $ is convergent and find its limit. This question is from an old Miklos Schweitzer competition (1958, problem 7, as pointed out by @user37238). I don't have any good ideas of approaching this question. Writing the recursion as $ (n+1) (a_{n+1}-a_n)=2a_{n-1} $ gives me the impression that it is a relation between the coefficients of a power series, obtained as a solution to some differential equation, although this line of thought doesn't lead me too far.",,['sequences-and-series']
10,"Help me show that the limit $\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$ does not exist",Help me show that the limit  does not exist,"\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}","Show that the limit $\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$   does not exist $$\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$$ Divide by $y^2$: $$\lim_{(x,y)\to (0,0)}\frac{2e^x}{\frac{x^2}{y^2}+1}$$ $$=\frac{2(1)}{\frac{0}{0}+1}$$ Since $\frac{0}{0}$ is undefined. This limit does not exist. I am not satisfied with my proof. Makes me think that what I just did was simple step, and not acceptable university level mathematics. Any comments on my proof?","Show that the limit $\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$   does not exist $$\lim_{(x,y)\to (0,0)}\frac{2e^x y^2}{x^2+y^2}$$ Divide by $y^2$: $$\lim_{(x,y)\to (0,0)}\frac{2e^x}{\frac{x^2}{y^2}+1}$$ $$=\frac{2(1)}{\frac{0}{0}+1}$$ Since $\frac{0}{0}$ is undefined. This limit does not exist. I am not satisfied with my proof. Makes me think that what I just did was simple step, and not acceptable university level mathematics. Any comments on my proof?",,"['calculus', 'limits']"
11,Evaluation of $\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}$,Evaluation of,\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!},"I have problems evaluating the following limit: $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}$$ What causes problems in particular is that I am unsure how to behave when there is a sum which becomes a series. I am aware that I need to evaluate all limits at the same time. So, I am not sure if I can use any knowledge about series (e.g. radius of convergence, which should give me the ratio of the coefficents of the series so that I can probably write down the limit) to get more information because it will only be a series when taking the limit. Using Sterling is in question was not that helpful either, because I didn't figure out any way of properly arranging the terms. I need to show that it is only $\approx \exp(x/2)$ which would be enough. Any help would be appreciated!","I have problems evaluating the following limit: $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}$$ What causes problems in particular is that I am unsure how to behave when there is a sum which becomes a series. I am aware that I need to evaluate all limits at the same time. So, I am not sure if I can use any knowledge about series (e.g. radius of convergence, which should give me the ratio of the coefficents of the series so that I can probably write down the limit) to get more information because it will only be a series when taking the limit. Using Sterling is in question was not that helpful either, because I didn't figure out any way of properly arranging the terms. I need to show that it is only $\approx \exp(x/2)$ which would be enough. Any help would be appreciated!",,"['sequences-and-series', 'limits', 'factorial']"
12,Show $g(x) = \frac{f(x)-f(x_0)}{x-x_0}$ is continuous,Show  is continuous,g(x) = \frac{f(x)-f(x_0)}{x-x_0},"Suppose that $f:(a,b) \to \mathbb{R}$ is continuous on $(a,b)$ and differentiable at $x_0 \in (a,b)$. Define $g(x) = \frac{f(x)-f(x_0)}{x-x_0}$ for $x \in (a,b)\setminus \{x_0\}$ and $g(x_0)=f'(x_0)$. Prove that $g$ is continuous on $(a,b)$. Here is how I have attempted it thus far: Assume we have $f:(a,b)\to \mathbb{R}$ which is continuous on its domain and differentiable at $x_0 \in (a,b)$. Then, we know $lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}=L$ or, in other words, that our limit at $x_0$ exists. We wish to show that $g:(a,b)\to \mathbb{R}$ is continuous on its domain. Since $f$ is continuous by assumption and, clearly, $x-x_0$ is continuous the only point we need concern ourselves with is $x_0$. Everywhere else we have the difference and quotient of continuous functions where the denominator is non-zero which is also continuous. Since $g(x_0)=f'(x)$ by definition we select $\delta>0$ such that $|x-x_0|<\delta$ and given $\varepsilon>0$ we have $\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x)\right|=0<\varepsilon$ I am unsure of how to pick our $\delta$ but I am fairly certain all the ideas here are correct. Thanks!","Suppose that $f:(a,b) \to \mathbb{R}$ is continuous on $(a,b)$ and differentiable at $x_0 \in (a,b)$. Define $g(x) = \frac{f(x)-f(x_0)}{x-x_0}$ for $x \in (a,b)\setminus \{x_0\}$ and $g(x_0)=f'(x_0)$. Prove that $g$ is continuous on $(a,b)$. Here is how I have attempted it thus far: Assume we have $f:(a,b)\to \mathbb{R}$ which is continuous on its domain and differentiable at $x_0 \in (a,b)$. Then, we know $lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}=L$ or, in other words, that our limit at $x_0$ exists. We wish to show that $g:(a,b)\to \mathbb{R}$ is continuous on its domain. Since $f$ is continuous by assumption and, clearly, $x-x_0$ is continuous the only point we need concern ourselves with is $x_0$. Everywhere else we have the difference and quotient of continuous functions where the denominator is non-zero which is also continuous. Since $g(x_0)=f'(x)$ by definition we select $\delta>0$ such that $|x-x_0|<\delta$ and given $\varepsilon>0$ we have $\left|\frac{f(x)-f(x_0)}{x-x_0}-f'(x)\right|=0<\varepsilon$ I am unsure of how to pick our $\delta$ but I am fairly certain all the ideas here are correct. Thanks!",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
13,"A limit about $a_1=1,a_{n+1}=a_n+[\sqrt{a_n}]$",A limit about,"a_1=1,a_{n+1}=a_n+[\sqrt{a_n}]","Let the sequence $\{a_n\}$ satisfy $$a_1=1,a_{n+1}=a_n+[\sqrt{a_n}]\quad(n\geq1),$$ where $[x]$ is the integer part of $x$. Find the limit $$\lim\limits_{n\to\infty}\frac{a_n}{n^2}$$. Add: By the Stolz formula, we have  \begin{align*} &\mathop {\lim }\limits_{n \to \infty } \frac{{{a_n}}}{{{n^2}}} = \mathop {\lim }\limits_{n \to \infty } \frac{{{a_{n + 1}} - {a_n}}}{{2n + 1}} = \mathop {\lim }\limits_{n \to \infty } \frac{{\left[ {\sqrt {{a_n}} } \right]}}{{2n + 1}} = \frac{1}{2}\mathop {\lim }\limits_{n \to \infty } \left( {\left[ {\sqrt {{a_{n + 1}}} } \right] - \left[ {\sqrt {{a_n}} } \right]} \right)\\  = &\frac{1}{2}\mathop {\lim }\limits_{n \to \infty } \left( {\left[ {\sqrt {{a_n} + \left[ {\sqrt {{a_n}} } \right]} } \right] - \left[ {\sqrt {{a_n}} } \right]} \right) = \frac{1}{2}\mathop {\lim }\limits_{x \to \infty } \left( {\left[ {\sqrt {x + \left[ x \right]} } \right] - \left[ {\sqrt x } \right]} \right).\end{align*} But it seems no use!","Let the sequence $\{a_n\}$ satisfy $$a_1=1,a_{n+1}=a_n+[\sqrt{a_n}]\quad(n\geq1),$$ where $[x]$ is the integer part of $x$. Find the limit $$\lim\limits_{n\to\infty}\frac{a_n}{n^2}$$. Add: By the Stolz formula, we have  \begin{align*} &\mathop {\lim }\limits_{n \to \infty } \frac{{{a_n}}}{{{n^2}}} = \mathop {\lim }\limits_{n \to \infty } \frac{{{a_{n + 1}} - {a_n}}}{{2n + 1}} = \mathop {\lim }\limits_{n \to \infty } \frac{{\left[ {\sqrt {{a_n}} } \right]}}{{2n + 1}} = \frac{1}{2}\mathop {\lim }\limits_{n \to \infty } \left( {\left[ {\sqrt {{a_{n + 1}}} } \right] - \left[ {\sqrt {{a_n}} } \right]} \right)\\  = &\frac{1}{2}\mathop {\lim }\limits_{n \to \infty } \left( {\left[ {\sqrt {{a_n} + \left[ {\sqrt {{a_n}} } \right]} } \right] - \left[ {\sqrt {{a_n}} } \right]} \right) = \frac{1}{2}\mathop {\lim }\limits_{x \to \infty } \left( {\left[ {\sqrt {x + \left[ x \right]} } \right] - \left[ {\sqrt x } \right]} \right).\end{align*} But it seems no use!",,"['calculus', 'real-analysis', 'analysis', 'limits']"
14,Finishing my $\epsilon - \delta$ proof,Finishing my  proof,\epsilon - \delta,"Hi fellas I have this limit: $$lim_{x\to-1}\dfrac{x^2-1}{x^2+x}=2$$ $Dem:$ Let $\epsilon >0$ random but fixed then: $$|\dfrac{x^2-1}{x^2+x}-2|=|\dfrac{x^2-1-2x^2-x}{x^2+x}|=|\dfrac{-x^2-2x-1}{x^2+x}|=|\dfrac{(x+1)^2}{x(x+1)}|=|\dfrac{x+1}{x}|<M|x-1|<M(\dfrac{\epsilon}{M})=\epsilon$$ So I have to choose M such that $|\dfrac{1}{x}|<M$ Let's suppose that $|x+1|<c$ for some $c$. (My problem is that I don't know how to choose this $c$) So $|x-1|<c \Rightarrow -c<x-1<c$ and so $-c-1<x<c-1$. The problem is that for a little $c<1$, $-c-1$ and $c-1 $ are both negative and for a bigger c, $-c-1$ is always negative and so I can't find my beloved $M$.","Hi fellas I have this limit: $$lim_{x\to-1}\dfrac{x^2-1}{x^2+x}=2$$ $Dem:$ Let $\epsilon >0$ random but fixed then: $$|\dfrac{x^2-1}{x^2+x}-2|=|\dfrac{x^2-1-2x^2-x}{x^2+x}|=|\dfrac{-x^2-2x-1}{x^2+x}|=|\dfrac{(x+1)^2}{x(x+1)}|=|\dfrac{x+1}{x}|<M|x-1|<M(\dfrac{\epsilon}{M})=\epsilon$$ So I have to choose M such that $|\dfrac{1}{x}|<M$ Let's suppose that $|x+1|<c$ for some $c$. (My problem is that I don't know how to choose this $c$) So $|x-1|<c \Rightarrow -c<x-1<c$ and so $-c-1<x<c-1$. The problem is that for a little $c<1$, $-c-1$ and $c-1 $ are both negative and for a bigger c, $-c-1$ is always negative and so I can't find my beloved $M$.",,"['calculus', 'limits', 'epsilon-delta']"
15,Multi-Variable Limit Problem,Multi-Variable Limit Problem,,"I am having difficulty showing that the limit of  $$f(x,y) = \frac{x^2+y^2\sin(x)}{x+y}$$ as (x,y) goes to (0,0). does not exist. All the usual tricks of computing the limit along $(0,x)$ and $(0,y)$ and $(x,mx)$ are giving me limits of $0$. Any hints or tips would be appreciated.","I am having difficulty showing that the limit of  $$f(x,y) = \frac{x^2+y^2\sin(x)}{x+y}$$ as (x,y) goes to (0,0). does not exist. All the usual tricks of computing the limit along $(0,x)$ and $(0,y)$ and $(x,mx)$ are giving me limits of $0$. Any hints or tips would be appreciated.",,"['limits', 'multivariable-calculus']"
16,$f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}}$,,f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}},"Let $$f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}}$$ and $$\phi(x)=\sqrt{x^2+\sqrt{x^2+\sqrt{x^2+\cdots}}}$$ Then find $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}.$ I tried to solve but got stuck. I simplified $f(x)=\frac{1\pm\sqrt{5-4\cos x}}{2}$ and $\phi(x)=\frac{1\pm\sqrt{1+4x^2}}{2}$ but when i calculated $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}=\frac{1\mp\sqrt6}{1\mp1}$ ,but the answer given in the book is $\frac{1}{2}$ Where have i gone wrong?Please help me.","Let $$f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}}$$ and $$\phi(x)=\sqrt{x^2+\sqrt{x^2+\sqrt{x^2+\cdots}}}$$ Then find $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}.$ I tried to solve but got stuck. I simplified $f(x)=\frac{1\pm\sqrt{5-4\cos x}}{2}$ and $\phi(x)=\frac{1\pm\sqrt{1+4x^2}}{2}$ but when i calculated $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}=\frac{1\mp\sqrt6}{1\mp1}$ ,but the answer given in the book is $\frac{1}{2}$ Where have i gone wrong?Please help me.",,"['limits', 'functions']"
17,Evaluate limit without L'hopital,Evaluate limit without L'hopital,,"$$\lim_{x\to2}\dfrac{\sqrt{x^2+1}-\sqrt{2x+1}}{\sqrt{x^3-x^2}-\sqrt{x+2}}$$ Please help me evaluate this limit. I have tried rationalising it but it just can't work, I keep ending up with $0$ at the denominator... Thanks!","$$\lim_{x\to2}\dfrac{\sqrt{x^2+1}-\sqrt{2x+1}}{\sqrt{x^3-x^2}-\sqrt{x+2}}$$ Please help me evaluate this limit. I have tried rationalising it but it just can't work, I keep ending up with $0$ at the denominator... Thanks!",,"['calculus', 'limits']"
18,What is the value of $\lim_{h\to 0}\frac{1/h-a}{h}$,What is the value of,\lim_{h\to 0}\frac{1/h-a}{h},"What is the value of $\lim_{h\to 0}\frac{\frac{1}{h}-a}{h}$? I believe it does not exist, but how can this be proven? If it exists, how would you solve it? Also, in general, how would you approach a limit like this?","What is the value of $\lim_{h\to 0}\frac{\frac{1}{h}-a}{h}$? I believe it does not exist, but how can this be proven? If it exists, how would you solve it? Also, in general, how would you approach a limit like this?",,['limits']
19,Seeking for a hint to a limit question $\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0$,Seeking for a hint to a limit question,\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0,"Assume ${a_n}$ is a positive, strictly increasing sequence, while $a_{n+1}-a_n$ is bounded. Prove: for every real number $\alpha \in (0,1)$, $$\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0.$$ I'm wondering how can I associate $a_{n+1}-a_n$ with $a_{n+1}^{\alpha}-a_n^{\alpha}$.","Assume ${a_n}$ is a positive, strictly increasing sequence, while $a_{n+1}-a_n$ is bounded. Prove: for every real number $\alpha \in (0,1)$, $$\lim_{n \to \infty}(a_{n+1}^{\alpha}-a_n^{\alpha})=0.$$ I'm wondering how can I associate $a_{n+1}-a_n$ with $a_{n+1}^{\alpha}-a_n^{\alpha}$.",,"['real-analysis', 'sequences-and-series', 'limits']"
20,Convergence of sequence of real numbers and (sub)subsequence,Convergence of sequence of real numbers and (sub)subsequence,,"Let $(x_n)_{n\in \mathbb{N}}$ be a sequence of real numbers. A well known result is: The sequence $(x_n)_{n\in \mathbb{N}}$ converges to $a\in \mathbb{R}$ if and only if every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ converges to $a\in \mathbb{R}$. Now, I'm working on an exercise which essentialy asks to prove the following: The sequence $(x_n)_{n\in \mathbb{N}}$ converges to $a\in \mathbb{R}$ if and only if for every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ there is a (sub)subsequence $(x_{n_{k_j}})_{j\in \mathbb{N}}$ which converges to $a$. Now, one direction is trivial. If in fact $(x_n)_{n\in \mathbb{N}}$ converges to $a$, then by the result I've mentioned, every subsequence of $(x_{n})_{n\in \mathbb{N}}$ converges to $a$. Let then $(x_{n_k})_{k\in \mathbb{N}}$ be an arbitrary subsequence. In that case, we know that it is a subsequence of itself, and hence we've found a (sub)subsequence which converges to $a$. The other way around seems quite complicated for me. The reason is as follows: I must assume that every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ has a (sub)subsequence which converges to $a$ and the proof that the original one converges to $a$. My problem is exactly that "" a "". If it had all (sub)subsequences converging to $a$, then by the result i've mentionted, the subsequence $(x_{n_k})_{k\in \mathbb{N}}$ would converge to $a$. In that case, we would have every subsequence converging to $a$ and the result would follow. My idea to prove the claim was then: First remember that a subsequence is the restriction of the original one $x: \mathbb{N}\to \mathbb{R}$ to an infinite subset $\mathbb{N}'\subset \mathbb{N}$. In that case a (sub)subsequence is the restriction of $\tilde{x} = x|\mathbb{N}'$ to an infinite subset $\mathbb{N}''\subset \mathbb{N}$. But since $\tilde{x}$ is the restriction of $x$ to $\mathbb{N}'$ and since $\mathbb{N}''\subset \mathbb{N}'$, the restriction of $\tilde{x}$ to $\mathbb{N}''$ is itself the restriction of $x$ to $\mathbb{N}''$. In other words, every (sub)subsequence can be viewed as a subsequence of the original one. Considering this, by hypothesis every subsequence has a (sub)subsequence which converges to $a$. But by this argument, this means that we can replace every subsequence with a convergent one. And then we can use the result I stated. Is my idea correct? Is this really the way to prove the claim? My only doubt is that I need every subsequence converging to $a$ to use the result. In this case I can't see how I can argue that this implies every subsequence converges to $a$. How can I finish this proof?","Let $(x_n)_{n\in \mathbb{N}}$ be a sequence of real numbers. A well known result is: The sequence $(x_n)_{n\in \mathbb{N}}$ converges to $a\in \mathbb{R}$ if and only if every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ converges to $a\in \mathbb{R}$. Now, I'm working on an exercise which essentialy asks to prove the following: The sequence $(x_n)_{n\in \mathbb{N}}$ converges to $a\in \mathbb{R}$ if and only if for every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ there is a (sub)subsequence $(x_{n_{k_j}})_{j\in \mathbb{N}}$ which converges to $a$. Now, one direction is trivial. If in fact $(x_n)_{n\in \mathbb{N}}$ converges to $a$, then by the result I've mentioned, every subsequence of $(x_{n})_{n\in \mathbb{N}}$ converges to $a$. Let then $(x_{n_k})_{k\in \mathbb{N}}$ be an arbitrary subsequence. In that case, we know that it is a subsequence of itself, and hence we've found a (sub)subsequence which converges to $a$. The other way around seems quite complicated for me. The reason is as follows: I must assume that every subsequence $(x_{n_k})_{k\in \mathbb{N}}$ has a (sub)subsequence which converges to $a$ and the proof that the original one converges to $a$. My problem is exactly that "" a "". If it had all (sub)subsequences converging to $a$, then by the result i've mentionted, the subsequence $(x_{n_k})_{k\in \mathbb{N}}$ would converge to $a$. In that case, we would have every subsequence converging to $a$ and the result would follow. My idea to prove the claim was then: First remember that a subsequence is the restriction of the original one $x: \mathbb{N}\to \mathbb{R}$ to an infinite subset $\mathbb{N}'\subset \mathbb{N}$. In that case a (sub)subsequence is the restriction of $\tilde{x} = x|\mathbb{N}'$ to an infinite subset $\mathbb{N}''\subset \mathbb{N}$. But since $\tilde{x}$ is the restriction of $x$ to $\mathbb{N}'$ and since $\mathbb{N}''\subset \mathbb{N}'$, the restriction of $\tilde{x}$ to $\mathbb{N}''$ is itself the restriction of $x$ to $\mathbb{N}''$. In other words, every (sub)subsequence can be viewed as a subsequence of the original one. Considering this, by hypothesis every subsequence has a (sub)subsequence which converges to $a$. But by this argument, this means that we can replace every subsequence with a convergent one. And then we can use the result I stated. Is my idea correct? Is this really the way to prove the claim? My only doubt is that I need every subsequence converging to $a$ to use the result. In this case I can't see how I can argue that this implies every subsequence converges to $a$. How can I finish this proof?",,"['real-analysis', 'sequences-and-series', 'limits', 'metric-spaces']"
21,"How I can evaluate $\lim_{(x,y) \rightarrow (0,0)} xy(\frac{1+xy}{x^3+y^3})^{1/3}$",How I can evaluate,"\lim_{(x,y) \rightarrow (0,0)} xy(\frac{1+xy}{x^3+y^3})^{1/3}","I don't have idea how I can evaluate this double limit  $$\lim_{(x,y) \rightarrow (0,0)} xy \left(\frac{1+xy}{x^3+y^3} \right) ^{1/3}$$ could you help me please! I try prove that $f$ is continuous: $f(x,y)=xy \left(\frac{1+xy}{x^3+y^3} \right) ^{1/3}$ if $x\not=-y$ and $f(x,y)=0$ if $x=-y$","I don't have idea how I can evaluate this double limit  $$\lim_{(x,y) \rightarrow (0,0)} xy \left(\frac{1+xy}{x^3+y^3} \right) ^{1/3}$$ could you help me please! I try prove that $f$ is continuous: $f(x,y)=xy \left(\frac{1+xy}{x^3+y^3} \right) ^{1/3}$ if $x\not=-y$ and $f(x,y)=0$ if $x=-y$",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus']"
22,Finding the Limit of Binomial CDF,Finding the Limit of Binomial CDF,,"Let $X\sim \mathrm{Binom}(n,p)$. Then its CDF is $$F_X(x)=\sum_{k=0}^{\lfloor x\rfloor}\binom{n}{k}p^k(1-p)^{n-k}.$$ I am trying to determine the limit of $F_X(x)$ with respect to $n$, with $p$ remain fixed: $$\lim_{n\to\infty}F_X(x)=\lim_{n\to\infty}\sum_{k=0}^{\lfloor x\rfloor}\binom{n}{k}p^k(1-p)^{n-k}.$$ The first thing that came up is de Moivre–Laplace theorem, that is $\frac{X-np}{\sqrt{np(1-p)}}\xrightarrow{D}N(0,1)$. Can I conclude from that fact then ""$X\xrightarrow{D} N(np, np(1-p))$"" (not really since $np$ explodes), so  $$\lim_{n\to\infty}F_X(x)=\lim_{n\to\infty}\left[\frac{1}{2}+\frac{1}{2}\mathscr{E}\mathrm{rf}\left(\frac{x-np}{\sqrt{2np(1-p)}}\right)\right]?$$ If not, how should I continue?","Let $X\sim \mathrm{Binom}(n,p)$. Then its CDF is $$F_X(x)=\sum_{k=0}^{\lfloor x\rfloor}\binom{n}{k}p^k(1-p)^{n-k}.$$ I am trying to determine the limit of $F_X(x)$ with respect to $n$, with $p$ remain fixed: $$\lim_{n\to\infty}F_X(x)=\lim_{n\to\infty}\sum_{k=0}^{\lfloor x\rfloor}\binom{n}{k}p^k(1-p)^{n-k}.$$ The first thing that came up is de Moivre–Laplace theorem, that is $\frac{X-np}{\sqrt{np(1-p)}}\xrightarrow{D}N(0,1)$. Can I conclude from that fact then ""$X\xrightarrow{D} N(np, np(1-p))$"" (not really since $np$ explodes), so  $$\lim_{n\to\infty}F_X(x)=\lim_{n\to\infty}\left[\frac{1}{2}+\frac{1}{2}\mathscr{E}\mathrm{rf}\left(\frac{x-np}{\sqrt{2np(1-p)}}\right)\right]?$$ If not, how should I continue?",,"['limits', 'central-limit-theorem', 'probability-limit-theorems', 'binomial-distribution']"
23,Question about limits using delta - epsilon definition,Question about limits using delta - epsilon definition,,"So I was trying to understand better limits using the $\epsilon$-$\delta$ definition and I decided to disprove a limit which is obviously false. Say, $\lim_{x\to 1} (x + 2) = 10$. Ok, this is false because the limit is  $3$, but using the definition: $|(x+2) - 10|< \epsilon$ (this is my epsilon) $|(x-1)| < \delta$ (this is my delta) $|x-1 -7| < \epsilon$ $|x-1|< \epsilon+7$ so it looks like if my $\delta$ is equal to $\epsilon +7$, I should get an $\epsilon$ bigger than $\delta$, since the limit is false. But if $\epsilon$ is say $20$, $\delta$ can be at most $27$, so let us pick $x$ to be $26$ $|(26-2) - 10| < 20$ $|24 - 10| < 20$ $|14| < 20$ So it works!! so my question is, where is my mistake?","So I was trying to understand better limits using the $\epsilon$-$\delta$ definition and I decided to disprove a limit which is obviously false. Say, $\lim_{x\to 1} (x + 2) = 10$. Ok, this is false because the limit is  $3$, but using the definition: $|(x+2) - 10|< \epsilon$ (this is my epsilon) $|(x-1)| < \delta$ (this is my delta) $|x-1 -7| < \epsilon$ $|x-1|< \epsilon+7$ so it looks like if my $\delta$ is equal to $\epsilon +7$, I should get an $\epsilon$ bigger than $\delta$, since the limit is false. But if $\epsilon$ is say $20$, $\delta$ can be at most $27$, so let us pick $x$ to be $26$ $|(26-2) - 10| < 20$ $|24 - 10| < 20$ $|14| < 20$ So it works!! so my question is, where is my mistake?",,"['calculus', 'limits']"
24,subadditivity for sets and monotonic function,subadditivity for sets and monotonic function,,"Let $f : \Sigma \rightarrow \mathbb{R}^+$ be a function, where $\Sigma$ is the space of finite subsets of $\mathbb{Z}^d$. Assume that, if $A_1, A_2 \subset \mathbb{Z}^d$ are two disjoint finite sets, then  $$ f(A_1 \cup A_2) \leq f(A_1) + f(A_2). $$ Assume also that, if $A_1 \subset A_2$, then $f(A_1) \leq f(A_2)$. How to prove that there exists a value $f_0 \in [0, \infty]$ such that  $$ \lim\limits_{n \rightarrow \infty} \frac{f(A_n)}{|A_n|} = f_0, $$ independently on the specific sequence $A_n \uparrow \mathbb{Z}^d$ satisfying $A_n \subset A_{n+1}$ (EDITED ASSUMPTION), where $|A|$ is the number of elements in $A$?","Let $f : \Sigma \rightarrow \mathbb{R}^+$ be a function, where $\Sigma$ is the space of finite subsets of $\mathbb{Z}^d$. Assume that, if $A_1, A_2 \subset \mathbb{Z}^d$ are two disjoint finite sets, then  $$ f(A_1 \cup A_2) \leq f(A_1) + f(A_2). $$ Assume also that, if $A_1 \subset A_2$, then $f(A_1) \leq f(A_2)$. How to prove that there exists a value $f_0 \in [0, \infty]$ such that  $$ \lim\limits_{n \rightarrow \infty} \frac{f(A_n)}{|A_n|} = f_0, $$ independently on the specific sequence $A_n \uparrow \mathbb{Z}^d$ satisfying $A_n \subset A_{n+1}$ (EDITED ASSUMPTION), where $|A|$ is the number of elements in $A$?",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
25,Is my proof correct about sequences?,Is my proof correct about sequences?,,"Suppose that $\{ a_n\}_n$ is a  sequence of  real  numbers  such  that $$ (a_{n+1}-a_n) \rightarrow a, \text{ if } \ n \rightarrow \infty. $$ Prove that $$ \frac{a_n}{n} \rightarrow a \, \text{ if } \ n \rightarrow \infty  $$ My  Proof Let $\epsilon > 0$ then there exists $n_0 \in \mathbb{N}$  such that  $$ | a_{n+1}-a_n-a |<\epsilon  \ \forall n \geq n_0 \Longrightarrow (a-\epsilon)<a_{n+1}-a_{n}<(a+\epsilon) $$ Let $k=n+1$, then  $$ (a-\epsilon)<a_k-a_{n_0}<(a+\epsilon)\\ \frac{(a-\epsilon)}{n}<\frac{a_k-a_{n_0}}{n}<\frac{(a+\epsilon)}{n} \\ (a-\epsilon)< \varliminf_{n \to \infty} \frac{a_k}{n}<(a+\epsilon) \\ (a-\epsilon)< \varlimsup_{n \to \infty} \frac{a_k}{n}<(a+\epsilon) \\ $$ Therefore $\varlimsup_{n \to \infty} =\varliminf_{n \to \infty} =a$ and $$ \lim_{n \to \infty}\frac{a_n}{n}=a$$","Suppose that $\{ a_n\}_n$ is a  sequence of  real  numbers  such  that $$ (a_{n+1}-a_n) \rightarrow a, \text{ if } \ n \rightarrow \infty. $$ Prove that $$ \frac{a_n}{n} \rightarrow a \, \text{ if } \ n \rightarrow \infty  $$ My  Proof Let $\epsilon > 0$ then there exists $n_0 \in \mathbb{N}$  such that  $$ | a_{n+1}-a_n-a |<\epsilon  \ \forall n \geq n_0 \Longrightarrow (a-\epsilon)<a_{n+1}-a_{n}<(a+\epsilon) $$ Let $k=n+1$, then  $$ (a-\epsilon)<a_k-a_{n_0}<(a+\epsilon)\\ \frac{(a-\epsilon)}{n}<\frac{a_k-a_{n_0}}{n}<\frac{(a+\epsilon)}{n} \\ (a-\epsilon)< \varliminf_{n \to \infty} \frac{a_k}{n}<(a+\epsilon) \\ (a-\epsilon)< \varlimsup_{n \to \infty} \frac{a_k}{n}<(a+\epsilon) \\ $$ Therefore $\varlimsup_{n \to \infty} =\varliminf_{n \to \infty} =a$ and $$ \lim_{n \to \infty}\frac{a_n}{n}=a$$",,"['real-analysis', 'sequences-and-series', 'limits']"
26,$\lim_{x \to \infty} \Gamma(x+1)/\Gamma(x+1+1/x^2)$,,\lim_{x \to \infty} \Gamma(x+1)/\Gamma(x+1+1/x^2),"Can someone please help me with the calculation of this limit? $$\lim_{x \to \infty} \frac{\Gamma(x+1)}{\Gamma(x+1+1/x^2)}$$ I tried wolframalpha and seems to be 1, yet there are no ""detailed steps"" as to how it reaches the conclusion (I understand there wouldn't be even if I pay, as in other cases it shows me the first ones). Thanks in advance, Sergio","Can someone please help me with the calculation of this limit? I tried wolframalpha and seems to be 1, yet there are no ""detailed steps"" as to how it reaches the conclusion (I understand there wouldn't be even if I pay, as in other cases it shows me the first ones). Thanks in advance, Sergio",\lim_{x \to \infty} \frac{\Gamma(x+1)}{\Gamma(x+1+1/x^2)},"['limits', 'gamma-function']"
27,"Improper integral: why $\int_0^1(x^2+ x^{1/3})^{-1}\,dx$ is convergent and not $\int \frac{1}{x^2}\,dx$ ???",Improper integral: why  is convergent and not  ???,"\int_0^1(x^2+ x^{1/3})^{-1}\,dx \int \frac{1}{x^2}\,dx","How do I show that $\int_0^1(x^2+ x^{1/3})^{-1}\,dx$ converges? I assume you show it on $(0,1]$. Can't seem to get my head around why this would be true.","How do I show that $\int_0^1(x^2+ x^{1/3})^{-1}\,dx$ converges? I assume you show it on $(0,1]$. Can't seem to get my head around why this would be true.",,"['limits', 'improper-integrals', 'infinity']"
28,mathematical analysis,mathematical analysis,,"Can anyone help me find this limit : Let the function $f(x)$ has continous derivative in the interval $[0,1] $and for every $ n\in{\Bbb{N}} $: $$x(n)=f(\frac1n)+f(\frac2n)+...+f(\frac{n−1}n)$$ Using the definite integral find the following limit : $$\lim_{n\to\infty}x(n+1)−x(n)=?$$ We can solve this using the mean value theorem, but I have no idea how to start solving this. Please help me.","Can anyone help me find this limit : Let the function $f(x)$ has continous derivative in the interval $[0,1] $and for every $ n\in{\Bbb{N}} $: $$x(n)=f(\frac1n)+f(\frac2n)+...+f(\frac{n−1}n)$$ Using the definite integral find the following limit : $$\lim_{n\to\infty}x(n+1)−x(n)=?$$ We can solve this using the mean value theorem, but I have no idea how to start solving this. Please help me.",,"['integration', 'analysis', 'limits']"
29,Why is this condition for the limit of a composite function necessary?,Why is this condition for the limit of a composite function necessary?,,"I'm trying to learn calculus from a textbook I picked up on Amazon. The book is Modern Calculus and Analytic Geometry by Richard A. Silverman and covers calculus 1 through 3 or 4. I like the style of the book in that it gives proofs for all the theorems, but this level of rigor is completely new to me so it has been a bit of a struggle. I want to get better at constructing proofs so I've been focusing on these types of exercises. Theorem 4.9 states that if a function g has the limit a at $x_{0}$ , and a function f has the limit A at a and g(x)$\neq$a in some deleted neighborhood of $x_{0}$ , then the composite function f(g(x)) has the limit A at $x_{0}$ . Show that the theorem fails if we drop the condition that g(x)$\neq$a in some deleted neighborhood of $x_{0}$ . When reading the statement of the theorem before I even got to this exercise, I didn't understand why this condition was necessary. If g(x)=a in some deleted neighborhood of $x_{0}$ , how does this affect the limit of f(g(x)) at $x_{0}$ ? For example, say g is just a constant function taking the value a for every x in it's domain. If g is constant then f(g) would be constant as well, say with a value of A. Certainly g(x)=a in some deleted neighborhood of $x_{0}$. I don't see why f(g($x_{0}$))=f(a)=A wouldn't imply a limit of A at $x_{0}$ I'm guessing that while the theorem seems to hold for the example above, there are certain other cases where it does not hold. If so, could you please provide an example? More generally, is there a way to know that the theorem may not hold without this condition without the need to find a counterexample?","I'm trying to learn calculus from a textbook I picked up on Amazon. The book is Modern Calculus and Analytic Geometry by Richard A. Silverman and covers calculus 1 through 3 or 4. I like the style of the book in that it gives proofs for all the theorems, but this level of rigor is completely new to me so it has been a bit of a struggle. I want to get better at constructing proofs so I've been focusing on these types of exercises. Theorem 4.9 states that if a function g has the limit a at $x_{0}$ , and a function f has the limit A at a and g(x)$\neq$a in some deleted neighborhood of $x_{0}$ , then the composite function f(g(x)) has the limit A at $x_{0}$ . Show that the theorem fails if we drop the condition that g(x)$\neq$a in some deleted neighborhood of $x_{0}$ . When reading the statement of the theorem before I even got to this exercise, I didn't understand why this condition was necessary. If g(x)=a in some deleted neighborhood of $x_{0}$ , how does this affect the limit of f(g(x)) at $x_{0}$ ? For example, say g is just a constant function taking the value a for every x in it's domain. If g is constant then f(g) would be constant as well, say with a value of A. Certainly g(x)=a in some deleted neighborhood of $x_{0}$. I don't see why f(g($x_{0}$))=f(a)=A wouldn't imply a limit of A at $x_{0}$ I'm guessing that while the theorem seems to hold for the example above, there are certain other cases where it does not hold. If so, could you please provide an example? More generally, is there a way to know that the theorem may not hold without this condition without the need to find a counterexample?",,"['calculus', 'limits']"
30,What does it mean if the derivative of a function is a constant?,What does it mean if the derivative of a function is a constant?,,"I was doing a homework problem to find the derivative of an equation and got ""7"" as the answer. I was trying to think about what it means if a derivative is a constant like that, is it just that the function is linear?","I was doing a homework problem to find the derivative of an equation and got ""7"" as the answer. I was trying to think about what it means if a derivative is a constant like that, is it just that the function is linear?",,"['calculus', 'limits', 'derivatives']"
31,Can a function which is periodically undefined have a limit as x goes to infinity?,Can a function which is periodically undefined have a limit as x goes to infinity?,,"I'm currently preparing for a calculus test. I was trying to solve the exercises of the test of last year, and one of the questions was: Give a full limit research of this function: $$f:(e,\infty) \setminus \{e^{e^{k\pi}}\mid k \in \mathbb N \} \to \mathbb R $$   $$x \mapsto \frac{\cot(\ln(\ln(x)))}{\sqrt{x-e}}$$ Now, I was wondering if it is even possible that this function has a limit as $x$ approaches $+\infty$. If I insert the function in Maple, I get $0$ as the limit for $x \to +\infty$. But I'm not really sure if that's correct since the function is periodically not defined. So my question is: Can a function that is periodically not defined have a limit for $x$ approaching infinity? (The question wasn't formulated in English, so I'm sorry if I mistranslated it)","I'm currently preparing for a calculus test. I was trying to solve the exercises of the test of last year, and one of the questions was: Give a full limit research of this function: $$f:(e,\infty) \setminus \{e^{e^{k\pi}}\mid k \in \mathbb N \} \to \mathbb R $$   $$x \mapsto \frac{\cot(\ln(\ln(x)))}{\sqrt{x-e}}$$ Now, I was wondering if it is even possible that this function has a limit as $x$ approaches $+\infty$. If I insert the function in Maple, I get $0$ as the limit for $x \to +\infty$. But I'm not really sure if that's correct since the function is periodically not defined. So my question is: Can a function that is periodically not defined have a limit for $x$ approaching infinity? (The question wasn't formulated in English, so I'm sorry if I mistranslated it)",,"['calculus', 'limits', 'functions']"
32,Finding the limit of a recursively defined sequence (recurrence relation). Specifically and generally,Finding the limit of a recursively defined sequence (recurrence relation). Specifically and generally,,"Whilst reading Goldrei's Classic Set Theory, I have come across a recursively defined sequence $a_0=0, a_1=1, a_n=\frac{1}{2}\left(a_{n-1} + a_{n-2} \right) $ The first few terms of which are: $0, 1, \frac{1}{2}, \frac{3}{4}, \frac{5}{8}, \frac{11}{16}, ... $ It is easy to show the successive terms get closer to one another $ |a_n - a_{n-1} |= | \frac{1}{2}\left(a_{n-1} + a_{n-2}\right) - a_{n-1} |$ $ = \frac{1}{2}|a_{n-1} - a_{n-2}|  $ Inducting on the ""first difference"" gives $ |a_n - a_{n-1} | = \frac{1}{2}^{n-1}|a_1 - a_0| $ $ = \frac{1}{2}^{n-1} $ So I have shown that the difference between each successive term approaches zero, i.e. the sequence approaches a limit. How do I find the limit of the recursively defined sequence? How are these types of problems (recursively defined functions) generally approached? It is: Show the sequence converges ...","Whilst reading Goldrei's Classic Set Theory, I have come across a recursively defined sequence $a_0=0, a_1=1, a_n=\frac{1}{2}\left(a_{n-1} + a_{n-2} \right) $ The first few terms of which are: $0, 1, \frac{1}{2}, \frac{3}{4}, \frac{5}{8}, \frac{11}{16}, ... $ It is easy to show the successive terms get closer to one another $ |a_n - a_{n-1} |= | \frac{1}{2}\left(a_{n-1} + a_{n-2}\right) - a_{n-1} |$ $ = \frac{1}{2}|a_{n-1} - a_{n-2}|  $ Inducting on the ""first difference"" gives $ |a_n - a_{n-1} | = \frac{1}{2}^{n-1}|a_1 - a_0| $ $ = \frac{1}{2}^{n-1} $ So I have shown that the difference between each successive term approaches zero, i.e. the sequence approaches a limit. How do I find the limit of the recursively defined sequence? How are these types of problems (recursively defined functions) generally approached? It is: Show the sequence converges ...",,"['sequences-and-series', 'limits', 'recurrence-relations', 'recursion', 'cauchy-sequences']"
33,Limit of cos function in a sequence,Limit of cos function in a sequence,,"In my assignment I have to calculate to following limit. I wanted to know if my solution is correct. Your help is appreciated: $$\lim_{n \to \infty}n\cos\frac{\pi n} {n+1} $$ Here's my solution: $$=\lim_{n \to \infty}n\cos \pi \frac{n} {n+1} $$ Since $\frac {n} {n+1}\to 1 $ and $\cos \pi \to (-1)$ we can use the ""infinity times a number"" rule, since $n \to \infty$. Therefore, the limit will be $(- \infty) $ Did I Get it right? Thanks, Alan","In my assignment I have to calculate to following limit. I wanted to know if my solution is correct. Your help is appreciated: $$\lim_{n \to \infty}n\cos\frac{\pi n} {n+1} $$ Here's my solution: $$=\lim_{n \to \infty}n\cos \pi \frac{n} {n+1} $$ Since $\frac {n} {n+1}\to 1 $ and $\cos \pi \to (-1)$ we can use the ""infinity times a number"" rule, since $n \to \infty$. Therefore, the limit will be $(- \infty) $ Did I Get it right? Thanks, Alan",,"['calculus', 'sequences-and-series', 'limits', 'trigonometry']"
34,Limit of sequence and Riemann sum,Limit of sequence and Riemann sum,,"I have to calculate $$\lim_{n \to \infty}\sum_{k=1}^{n}\frac{(k-1)^7}{n^8}$$ So, $$\lim_{n \to \infty}\sum_{k=1}^{n}\frac{(k-1)^7}{n^8} = \lim_{n \to \infty}\sum_{k=1}^{n}\frac{1}{n}(\frac{k}{n}-\frac{1}{n})^7$$ But how to get the rid of $-\frac{1}{n}$?","I have to calculate $$\lim_{n \to \infty}\sum_{k=1}^{n}\frac{(k-1)^7}{n^8}$$ So, $$\lim_{n \to \infty}\sum_{k=1}^{n}\frac{(k-1)^7}{n^8} = \lim_{n \to \infty}\sum_{k=1}^{n}\frac{1}{n}(\frac{k}{n}-\frac{1}{n})^7$$ But how to get the rid of $-\frac{1}{n}$?",,"['integration', 'sequences-and-series', 'limits']"
35,"Find $\lim\limits _{n\to \infty }a_n$ where $a_{n+1}=\frac{a_n+b_n}{2},\:b_{n+1}=\sqrt{a_n b_n}$",Find  where,"\lim\limits _{n\to \infty }a_n a_{n+1}=\frac{a_n+b_n}{2},\:b_{n+1}=\sqrt{a_n b_n}",I showed that the limit exists and that $\lim\limits_{n\to \infty }a_n=\lim\limits_{n\to \infty }b_n$. My question is how to compute the limit assuming $a_1$ and $a_2$ are given positive numbers.,I showed that the limit exists and that $\lim\limits_{n\to \infty }a_n=\lim\limits_{n\to \infty }b_n$. My question is how to compute the limit assuming $a_1$ and $a_2$ are given positive numbers.,,"['calculus', 'sequences-and-series', 'limits']"
36,Find $\lim_{n\to\ \infty}\sqrt[n]{\frac{\sum_{i=1}^p a_i^n}{p}}$,Find,\lim_{n\to\ \infty}\sqrt[n]{\frac{\sum_{i=1}^p a_i^n}{p}},"I was trying to solve a question of an entrance exam. I am having trouble in the following problem. Please help me. For positive real numbers $a_1, a_2, \ldots, a_p$ find the value of  $$\displaystyle \lim_{n\to\ \infty}\sqrt[n]{\frac{\displaystyle \sum_{i=1}^p a_i^n}{p}}$$ What I have done so far: From AM-GM inequality $\frac{\displaystyle \sum_{i=1}^p a_i^n}{p} \ge \sqrt[p]{\displaystyle \prod_{i=1}^p a_i^n} = \sqrt[\frac{n}{p}]{\displaystyle \prod_{i=1}^p a_i}$ So $\sqrt[n]{\frac{\displaystyle \sum_{i=1}^p a_i^n}{p}} \ge \sqrt[p]{\displaystyle \prod_{i=1}^p a_i}$ But then I can not find any way to proceed further. It will be very helpful for me any one provide me some help. I apologise for not showing much effort but I am really stuck. Please help me. Thnx in advance.","I was trying to solve a question of an entrance exam. I am having trouble in the following problem. Please help me. For positive real numbers $a_1, a_2, \ldots, a_p$ find the value of  $$\displaystyle \lim_{n\to\ \infty}\sqrt[n]{\frac{\displaystyle \sum_{i=1}^p a_i^n}{p}}$$ What I have done so far: From AM-GM inequality $\frac{\displaystyle \sum_{i=1}^p a_i^n}{p} \ge \sqrt[p]{\displaystyle \prod_{i=1}^p a_i^n} = \sqrt[\frac{n}{p}]{\displaystyle \prod_{i=1}^p a_i}$ So $\sqrt[n]{\frac{\displaystyle \sum_{i=1}^p a_i^n}{p}} \ge \sqrt[p]{\displaystyle \prod_{i=1}^p a_i}$ But then I can not find any way to proceed further. It will be very helpful for me any one provide me some help. I apologise for not showing much effort but I am really stuck. Please help me. Thnx in advance.",,"['real-analysis', 'analysis', 'limits', 'inequality']"
37,Trigonometric tough limit: $\lim\limits_{x\to 0} \frac{1-\cos x}{x^2}$ [duplicate],Trigonometric tough limit:  [duplicate],\lim\limits_{x\to 0} \frac{1-\cos x}{x^2},This question already has answers here : Limits of trig functions (3 answers) Closed 5 years ago . I am still struggling with this one. Can't figure it out... $$\lim_{x\to 0} \frac{1-\cos x}{x^2}$$ I tried $\cos x = \sqrt{1 - \sin^2{x}}$,This question already has answers here : Limits of trig functions (3 answers) Closed 5 years ago . I am still struggling with this one. Can't figure it out... $$\lim_{x\to 0} \frac{1-\cos x}{x^2}$$ I tried $\cos x = \sqrt{1 - \sin^2{x}}$,,"['limits', 'trigonometry']"
38,Can someone give me a counterexample to understand why this definition of limit is wrong?,Can someone give me a counterexample to understand why this definition of limit is wrong?,,"Could someone give me a counterexample to understand why this definition of $\lim_ {x\to a} f(x) = L$, does not work? $\forall \delta>0 \exists \varepsilon>0$ such that, if $0<|x−a|<\delta$, then $|f(x)−L|<\varepsilon$ I've searched in this forum and the question has already been made but I don't understand the examples and counterexamples given. I found this: $\lim_{x\to 1} \frac{1}{x} = 1$, as an example in Spivak Supplement of calculus, but I don´t understand why this example works.","Could someone give me a counterexample to understand why this definition of $\lim_ {x\to a} f(x) = L$, does not work? $\forall \delta>0 \exists \varepsilon>0$ such that, if $0<|x−a|<\delta$, then $|f(x)−L|<\varepsilon$ I've searched in this forum and the question has already been made but I don't understand the examples and counterexamples given. I found this: $\lim_{x\to 1} \frac{1}{x} = 1$, as an example in Spivak Supplement of calculus, but I don´t understand why this example works.",,"['calculus', 'limits', 'epsilon-delta']"
39,How can you find the integral of $\frac{cos(2t)}{2t^2}$ between 1 and infinity?,How can you find the integral of  between 1 and infinity?,\frac{cos(2t)}{2t^2},How can you find the integral of $\frac{\cos(2t)}{2t^2}$ between 1 and infinity? $$ I = \int\limits_1^\infty \frac{\cos(2t)}{2t^2} dt $$ My problem is that I just simply do not know how to handle this. I have tried integration by parts and some trigonometric identities...,How can you find the integral of $\frac{\cos(2t)}{2t^2}$ between 1 and infinity? $$ I = \int\limits_1^\infty \frac{\cos(2t)}{2t^2} dt $$ My problem is that I just simply do not know how to handle this. I have tried integration by parts and some trigonometric identities...,,"['integration', 'limits', 'trigonometry']"
40,Prove that $g(x):=|f(x)|$ is differentiable iff $f'(a)=0$,Prove that  is differentiable iff,g(x):=|f(x)| f'(a)=0,"Suppose that $f(a)=0$. Prove that $g(x):=|f(x)|$ is differentiable iff $f'(a)=0$ Not sure how to go about this at all. The limit definition that I am working with is $$ g'(a)=\lim_{x \rightarrow a} \frac{g(x)-g(a)}{x-a}= \lim_{x \rightarrow a} \frac{|f(x)|-|f(a)|}{x-a}= \lim_{x \rightarrow a} \frac{|f(x)|}{x-a}=\text{something} $$ On the other hands, if $f'(a)=0$ $$ f'(a)=\lim_{x \rightarrow a} \frac{f(x)-f(a)}{x-a}= \lim_{x \rightarrow a} \frac{f(x)}{x-a}=0 \quad\text{iff}\quad \lim_{x\rightarrow a} f(x)=0 $$ Then, $g'(a)=\lim\limits_{x \rightarrow a} \dfrac{|f(x)|}{x-a}=0$. My only question is how to do the other direction.","Suppose that $f(a)=0$. Prove that $g(x):=|f(x)|$ is differentiable iff $f'(a)=0$ Not sure how to go about this at all. The limit definition that I am working with is $$ g'(a)=\lim_{x \rightarrow a} \frac{g(x)-g(a)}{x-a}= \lim_{x \rightarrow a} \frac{|f(x)|-|f(a)|}{x-a}= \lim_{x \rightarrow a} \frac{|f(x)|}{x-a}=\text{something} $$ On the other hands, if $f'(a)=0$ $$ f'(a)=\lim_{x \rightarrow a} \frac{f(x)-f(a)}{x-a}= \lim_{x \rightarrow a} \frac{f(x)}{x-a}=0 \quad\text{iff}\quad \lim_{x\rightarrow a} f(x)=0 $$ Then, $g'(a)=\lim\limits_{x \rightarrow a} \dfrac{|f(x)|}{x-a}=0$. My only question is how to do the other direction.",,"['real-analysis', 'limits', 'derivatives', 'absolute-value']"
41,Using exclusively the definition of limit proof that $\lim_{x \to 0} \frac{x^3-2x+x}{\sin(x)} = -1$,Using exclusively the definition of limit proof that,\lim_{x \to 0} \frac{x^3-2x+x}{\sin(x)} = -1,"Using exclusively the definition of limit proof that  $$ \lim_{x \to 0} \frac{x^3-x}{\sin(x)} = -1 $$ I have to learn how to prove limits by the delta-epsilon definition, I know how to do basic ones (like linear functions) but not things like this. I'd like to thoroughly understand the process to get the proof working. For example: I've seen people choose an ""auxiliary $\delta_1=1$"" and then declare $\delta=\min(\delta_1,\text{some other thing})$. I have no idea why choosing a delta if allowed, or why we have to take the min when we are done. Every bit of insight is hugely appreciated.","Using exclusively the definition of limit proof that  $$ \lim_{x \to 0} \frac{x^3-x}{\sin(x)} = -1 $$ I have to learn how to prove limits by the delta-epsilon definition, I know how to do basic ones (like linear functions) but not things like this. I'd like to thoroughly understand the process to get the proof working. For example: I've seen people choose an ""auxiliary $\delta_1=1$"" and then declare $\delta=\min(\delta_1,\text{some other thing})$. I have no idea why choosing a delta if allowed, or why we have to take the min when we are done. Every bit of insight is hugely appreciated.",,"['calculus', 'limits', 'epsilon-delta']"
42,The limit as $n \to \infty$ of $\big[\prod_{k=1}^{n}(1 + \frac{k}{n})\big]^{1/n}$,The limit as  of,n \to \infty \big[\prod_{k=1}^{n}(1 + \frac{k}{n})\big]^{1/n},I think the limit should be $=1$ because $$\bigg[ \prod_{k=1}^{n}(1 + \frac{k}{n})\bigg]^{1/n} = \exp \bigg[ \frac{1}{n} \big[ \log(1 + \frac{1}{n}) + \cdots + \log2 \big] \bigg] =\\ \exp \bigg[ \frac{1}{n} \big[ o(1) + \cdots + o(1) + \log 2 \big] \bigg] = 1 + o(1)\big|_{n \to \infty}.$$ Is there an error in the above?,I think the limit should be $=1$ because $$\bigg[ \prod_{k=1}^{n}(1 + \frac{k}{n})\bigg]^{1/n} = \exp \bigg[ \frac{1}{n} \big[ \log(1 + \frac{1}{n}) + \cdots + \log2 \big] \bigg] =\\ \exp \bigg[ \frac{1}{n} \big[ o(1) + \cdots + o(1) + \log 2 \big] \bigg] = 1 + o(1)\big|_{n \to \infty}.$$ Is there an error in the above?,,"['calculus', 'sequences-and-series', 'limits']"
43,Why doesn't the limit of `h` of the derivative formula approach x?,Why doesn't the limit of `h` of the derivative formula approach x?,,"For the following graph: We derive an equation to find the derivative (or the slope at an instant point, x ) which is given by: However, we are not supposed to find the tangent for the point x + h as it approaches 0 . We have to find it as it approaches x for that will give the tangent at that point of the curve. So, why does the limit of h approach 0 ( lim h->0 ) instead of x ( lim h->x )? Thanks.","For the following graph: We derive an equation to find the derivative (or the slope at an instant point, x ) which is given by: However, we are not supposed to find the tangent for the point x + h as it approaches 0 . We have to find it as it approaches x for that will give the tangent at that point of the curve. So, why does the limit of h approach 0 ( lim h->0 ) instead of x ( lim h->x )? Thanks.",,"['calculus', 'limits', 'derivatives']"
44,Is it true that $\lim \limits_{x \to a} [l(x)-m(x)] = 0 \implies l(x)$ has the same sign of $m(x)$ for $x$ sufficiently close to $a$?,Is it true that  has the same sign of  for  sufficiently close to ?,\lim \limits_{x \to a} [l(x)-m(x)] = 0 \implies l(x) m(x) x a,"This was present in a demonstration of a theorem, however, it didn't seem obvious to me. Is this a general result? Here is the exact context where this appeared (from Spivak's Calculus book): If f is function for which $f(a) =f'(a) = f''(a) = ... = f^{n-1}(a) = 0$ and $f^{n}(a) > 0$, then $f$ will have a local minimum at $x=a$ if $n$ is even. The demonstration uses a Taylor polynomial $$P_{n,a}(x) = \frac {f^{n}(a)}{n!}(x-a)^n$$ as $$\lim\limits_{x \to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n} =0$$ We would have $$\lim\limits_{x \to a} \left [ \frac{f(x)}{(x-a)^n} - \frac{f^{(n)}(a)}{n!} \right ] = 0$$ Which implies that $\frac{f(x)}{(x-a)^n}$ has the same sign as $\frac{f^{(n)}(a)}{n!}$, for $x$ sufficiently close to $a$. As a consequence, $f(x)>0=f(a)$, because $f^{(n)}(a)>0$ and $(x-a)^n>0$, so there's a local minimum at $a$.","This was present in a demonstration of a theorem, however, it didn't seem obvious to me. Is this a general result? Here is the exact context where this appeared (from Spivak's Calculus book): If f is function for which $f(a) =f'(a) = f''(a) = ... = f^{n-1}(a) = 0$ and $f^{n}(a) > 0$, then $f$ will have a local minimum at $x=a$ if $n$ is even. The demonstration uses a Taylor polynomial $$P_{n,a}(x) = \frac {f^{n}(a)}{n!}(x-a)^n$$ as $$\lim\limits_{x \to a} \frac{f(x)-P_{n,a}(x)}{(x-a)^n} =0$$ We would have $$\lim\limits_{x \to a} \left [ \frac{f(x)}{(x-a)^n} - \frac{f^{(n)}(a)}{n!} \right ] = 0$$ Which implies that $\frac{f(x)}{(x-a)^n}$ has the same sign as $\frac{f^{(n)}(a)}{n!}$, for $x$ sufficiently close to $a$. As a consequence, $f(x)>0=f(a)$, because $f^{(n)}(a)>0$ and $(x-a)^n>0$, so there's a local minimum at $a$.",,"['calculus', 'limits', 'power-series']"
45,Inequality and integral,Inequality and integral,,"I am working on a problem and I feel like I may not completely understand that concept which I think is real important that I do. The question is in regard to showing the validity of the following inequality $\int_0^{n}x^{q}dx$ $\le 1+2^{q}+..+n^{q} \le \int_0^{n+1}x^{q}dx$ for any positive integer n and any real q $\ge 0 $ One answer, similar to one suggested by the user Tryss on this site is as follows, $$\int_0^n x^q dx = \sum_{k=1}^{n} \int_{k-1}^k x^q dx \leq \sum_{k=1}^{n} \int_{k-1}^k k^q dx  $$ Then $$\int_0^n x^q dx \leq \sum_{k=1}^{n} k^q  = 1+ 2^q + \cdots n^q $$ And we have, $$\int_0^{n+1} x^q dx = \sum_{k=0}^{n} \int_{k}^{k+1} x^q dx \geq \sum_{k=0}^{n} \int_{k}^{k+1} k^q dx  $$ i,e then, $$\int_0^{n+1} x^q dx \geq \sum_{k=0}^{n} k^q  = 1+ 2^q + \cdots n^q $$ The next part of the question that I am mostly stumped on is to find $\lim_{n\to \infty}a_n$ where $a_n= 1/{n^{q+1}}+2^{q}/n^{q+1}+…+n^{q}/n^{q+1}$. I'm sure I need to use the result of the first part of the problem but I am not sure where to begin. Here is where I am looking for help as well; I am feeling confused about the proposed solution above. I think my background is a little lacking on remembering the relation between the summation symbols and the integral symbol in general. I see clearly that we are increasing, but why choose the interval $[k,k-1]$ for example? is it just an arbitrary interval for which we can use the fact that we have an increasing so we can write $x^{q} \le k^{q}$ Then I am confused overall, about the geometric meaning and such. for example how is it true and what is the meaning of writing  $$\int_0^n x^q dx = \sum_{k=1}^{n} \int_{k-1}^k x^q dx \leq \sum_{k=1}^{n} \int_{k-1}^k k^q dx  $$ . How can I know/see/understand what this is saying is true? and vice versa for the other inequality. So overall I am looking for any help on that, and with the limit. All is very greatly appreciated and I am trying to understand! Thanks a lot in advance to anyone who is willing to help.","I am working on a problem and I feel like I may not completely understand that concept which I think is real important that I do. The question is in regard to showing the validity of the following inequality $\int_0^{n}x^{q}dx$ $\le 1+2^{q}+..+n^{q} \le \int_0^{n+1}x^{q}dx$ for any positive integer n and any real q $\ge 0 $ One answer, similar to one suggested by the user Tryss on this site is as follows, $$\int_0^n x^q dx = \sum_{k=1}^{n} \int_{k-1}^k x^q dx \leq \sum_{k=1}^{n} \int_{k-1}^k k^q dx  $$ Then $$\int_0^n x^q dx \leq \sum_{k=1}^{n} k^q  = 1+ 2^q + \cdots n^q $$ And we have, $$\int_0^{n+1} x^q dx = \sum_{k=0}^{n} \int_{k}^{k+1} x^q dx \geq \sum_{k=0}^{n} \int_{k}^{k+1} k^q dx  $$ i,e then, $$\int_0^{n+1} x^q dx \geq \sum_{k=0}^{n} k^q  = 1+ 2^q + \cdots n^q $$ The next part of the question that I am mostly stumped on is to find $\lim_{n\to \infty}a_n$ where $a_n= 1/{n^{q+1}}+2^{q}/n^{q+1}+…+n^{q}/n^{q+1}$. I'm sure I need to use the result of the first part of the problem but I am not sure where to begin. Here is where I am looking for help as well; I am feeling confused about the proposed solution above. I think my background is a little lacking on remembering the relation between the summation symbols and the integral symbol in general. I see clearly that we are increasing, but why choose the interval $[k,k-1]$ for example? is it just an arbitrary interval for which we can use the fact that we have an increasing so we can write $x^{q} \le k^{q}$ Then I am confused overall, about the geometric meaning and such. for example how is it true and what is the meaning of writing  $$\int_0^n x^q dx = \sum_{k=1}^{n} \int_{k-1}^k x^q dx \leq \sum_{k=1}^{n} \int_{k-1}^k k^q dx  $$ . How can I know/see/understand what this is saying is true? and vice versa for the other inequality. So overall I am looking for any help on that, and with the limit. All is very greatly appreciated and I am trying to understand! Thanks a lot in advance to anyone who is willing to help.",,"['calculus', 'integration', 'limits', 'inequality']"
46,Show that if $x_n\rightarrow\ x_0^{-}$ and $y_n\rightarrow\ x_0^{+}$ then $ \lim \limits_{n \to \infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0)$,Show that if  and  then,x_n\rightarrow\ x_0^{-} y_n\rightarrow\ x_0^{+}  \lim \limits_{n \to \infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0),"Let $f: \Bbb[a,b]\rightarrow\Bbb{R}$ be differentiable in a point $a<x_0<b$. Use Taylor's expansion with the remainder of $f$ around $x_0$ to show that if $ x_n\rightarrow\ x_0^{-}$, and  $ y_n\rightarrow\ x_0^{+}$ then:    $$ \lim \limits_{n \to \infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0)$$ I know that: $f(x_n)= f(x_0)+f^\prime(x_0)(x_n-x_0) + R_1 (x_n) $ and: $f(y_n)= f(x_0)+f^\prime(x_0)(y_n-x_0) + R_1 (y_n)$ Note: $R_1$ is the remainder. and then by substracting: $ \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0) + \frac{R_1(x_n)-R_1(y_n)}{x_n-y_n}$ but It seemed that I couldn't continue from here to anywhere, can someone please show the right way to do this?","Let $f: \Bbb[a,b]\rightarrow\Bbb{R}$ be differentiable in a point $a<x_0<b$. Use Taylor's expansion with the remainder of $f$ around $x_0$ to show that if $ x_n\rightarrow\ x_0^{-}$, and  $ y_n\rightarrow\ x_0^{+}$ then:    $$ \lim \limits_{n \to \infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0)$$ I know that: $f(x_n)= f(x_0)+f^\prime(x_0)(x_n-x_0) + R_1 (x_n) $ and: $f(y_n)= f(x_0)+f^\prime(x_0)(y_n-x_0) + R_1 (y_n)$ Note: $R_1$ is the remainder. and then by substracting: $ \frac{f(y_n)-f(x_n)}{y_n-x_n} = f^\prime(x_0) + \frac{R_1(x_n)-R_1(y_n)}{x_n-y_n}$ but It seemed that I couldn't continue from here to anywhere, can someone please show the right way to do this?",,"['limits', 'taylor-expansion', 'limits-without-lhopital']"
47,Limit of a function of two variables,Limit of a function of two variables,,"This is a problem occurs to me when I was trying to find the limit of a function with two variables(if only it exists). Please help. When I have to show that the limit does not exist for some function. Then by showing along two paths have two different limits I can prove it since the functions with two variables have infinite number of directions of approach to the given point. So selecting the paths, should I only take the linear ones or can I take non linear paths also? As an example,consider the limit of $\frac{xy^3}{x^2+y^6}$ as $(x,y)$ goes to $(0,0)$ Then taking along $x$ axis I have the limit $0$. Can I take $y=x^{1/3}$ which gives me the limit $1/2$ ? When I check the answer on some website it seems they haven't consider curved functions to find different limits. Please explain. Where am I wrong?","This is a problem occurs to me when I was trying to find the limit of a function with two variables(if only it exists). Please help. When I have to show that the limit does not exist for some function. Then by showing along two paths have two different limits I can prove it since the functions with two variables have infinite number of directions of approach to the given point. So selecting the paths, should I only take the linear ones or can I take non linear paths also? As an example,consider the limit of $\frac{xy^3}{x^2+y^6}$ as $(x,y)$ goes to $(0,0)$ Then taking along $x$ axis I have the limit $0$. Can I take $y=x^{1/3}$ which gives me the limit $1/2$ ? When I check the answer on some website it seems they haven't consider curved functions to find different limits. Please explain. Where am I wrong?",,"['limits', 'multivariable-calculus']"
48,Proving continuity using $\epsilon$ and $\delta$,Proving continuity using  and,\epsilon \delta,"Here is my first attempt to prove continuity using epsilon and delta: Prove that $f(x)=\sqrt{x}$ is continuous at $p=4$ . Unfortunately my book has just some answers and that isn't one of them. I would like to someone verify my proof: By definition: $p-\delta < x < p+\delta\;\;\Rightarrow\;\;f(p)-\epsilon<f(x)<f(p)+\epsilon$ Plug-in the known values: $4-\delta < x < 4+\delta$ and $\sqrt{4}-\epsilon<\sqrt{x}<\sqrt{4}+\epsilon$ Then working in the second inequality: $2-\epsilon<\sqrt{x}<2+\epsilon$ $4-4\epsilon+\epsilon^{2}<x<4+4\epsilon+\epsilon^{2}$ Then let $\epsilon>0$ and set $\delta = 4\epsilon + \epsilon^{2}$, then $|x-p|<\delta\;\;\Rightarrow\;\;|f(x)-f(p)|<\epsilon$ So $f$ is continuous at $p=4$.","Here is my first attempt to prove continuity using epsilon and delta: Prove that $f(x)=\sqrt{x}$ is continuous at $p=4$ . Unfortunately my book has just some answers and that isn't one of them. I would like to someone verify my proof: By definition: $p-\delta < x < p+\delta\;\;\Rightarrow\;\;f(p)-\epsilon<f(x)<f(p)+\epsilon$ Plug-in the known values: $4-\delta < x < 4+\delta$ and $\sqrt{4}-\epsilon<\sqrt{x}<\sqrt{4}+\epsilon$ Then working in the second inequality: $2-\epsilon<\sqrt{x}<2+\epsilon$ $4-4\epsilon+\epsilon^{2}<x<4+4\epsilon+\epsilon^{2}$ Then let $\epsilon>0$ and set $\delta = 4\epsilon + \epsilon^{2}$, then $|x-p|<\delta\;\;\Rightarrow\;\;|f(x)-f(p)|<\epsilon$ So $f$ is continuous at $p=4$.",,"['calculus', 'limits', 'proof-verification', 'continuity', 'epsilon-delta']"
49,$f_n(x)=n\sin^{2n+1}x\cos x$. Find $\lim_{n\to\infty}f_n(x)$,. Find,f_n(x)=n\sin^{2n+1}x\cos x \lim_{n\to\infty}f_n(x),"Let $f_n(x)=n\sin^{2n+1}x\cos x$. Then find the value of $\lim\limits_{n\to\infty}\displaystyle\int_0^{\pi/2}f_n(x)\;dx-\displaystyle\int_0^{\pi/2}(\lim\limits_{n\to\infty}f_n(x))\;dx$ My Thoughts: If the function is uniformly convergent, the answer should be zero. To check for uniform convergence, I need to do Weierstrass's M-Test. A ratio test says $f_n(x)$ is convergent provided $x<\pi/2$. How do I properly solve this problem ? Please help.","Let $f_n(x)=n\sin^{2n+1}x\cos x$. Then find the value of $\lim\limits_{n\to\infty}\displaystyle\int_0^{\pi/2}f_n(x)\;dx-\displaystyle\int_0^{\pi/2}(\lim\limits_{n\to\infty}f_n(x))\;dx$ My Thoughts: If the function is uniformly convergent, the answer should be zero. To check for uniform convergence, I need to do Weierstrass's M-Test. A ratio test says $f_n(x)$ is convergent provided $x<\pi/2$. How do I properly solve this problem ? Please help.",,"['calculus', 'limits', 'definite-integrals', 'uniform-convergence']"
50,Proof of a limit of a sequence,Proof of a limit of a sequence,,"I want to prove that $$\lim_{n\to\infty} \frac{2n^2+1}{n^2+3n} = 2.$$ Is the following proof valid? Proof $\left|\frac{2n^2+1}{n^2+3n} - 2\right|=\left|\frac{1-6n}{n^2+3n}\right| =\frac{6n-1}{n(n+3)} $ (because $n \in \mathbb N^+)$. $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (*)$ We have $n \ge 1 \implies 6n -1 > n + 3 \implies \frac{n+3}{n(n+3)} < \frac{6n-1}{n^2+3n}.$ Let $\epsilon > 0$ be given. Note that $ \frac{6n-1}{n^2+3n}< \epsilon \iff \frac{n+3}{n(n+3)} < \epsilon \iff n > \frac{1}{\epsilon}.$  $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (**)$ By the Archimedean Property of $\mathbb R$, $\exists N \in \mathbb N^+$ such that $N > \frac{1}{\epsilon}.$ If $n \ge N$, then $n > \frac{1}{\epsilon}$, and from $(*)$ and $(**)$ it follows that $\left|\frac{2n^2+1}{n^2+3n} - 2\right| < \epsilon$. Therefore $\lim_{n\to\infty} \frac{2n^2+1}{n^2+3n} = 2.$","I want to prove that $$\lim_{n\to\infty} \frac{2n^2+1}{n^2+3n} = 2.$$ Is the following proof valid? Proof $\left|\frac{2n^2+1}{n^2+3n} - 2\right|=\left|\frac{1-6n}{n^2+3n}\right| =\frac{6n-1}{n(n+3)} $ (because $n \in \mathbb N^+)$. $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (*)$ We have $n \ge 1 \implies 6n -1 > n + 3 \implies \frac{n+3}{n(n+3)} < \frac{6n-1}{n^2+3n}.$ Let $\epsilon > 0$ be given. Note that $ \frac{6n-1}{n^2+3n}< \epsilon \iff \frac{n+3}{n(n+3)} < \epsilon \iff n > \frac{1}{\epsilon}.$  $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (**)$ By the Archimedean Property of $\mathbb R$, $\exists N \in \mathbb N^+$ such that $N > \frac{1}{\epsilon}.$ If $n \ge N$, then $n > \frac{1}{\epsilon}$, and from $(*)$ and $(**)$ it follows that $\left|\frac{2n^2+1}{n^2+3n} - 2\right| < \epsilon$. Therefore $\lim_{n\to\infty} \frac{2n^2+1}{n^2+3n} = 2.$",,"['real-analysis', 'limits', 'proof-verification']"
51,Question about limits $\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x}$,Question about limits,\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x},How to calculate this: $$\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x}$$,How to calculate this: $$\lim_{x\to\infty}\frac{x-2}{e^{1/x}\cdot x}$$,,"['calculus', 'limits']"
52,Limit to infinity,Limit to infinity,,"If $f:\mathbb R \to \mathbb R$ is a differentiable function and $f'(x)>x^2$  for every $x<0$, find   $$\lim_{x\to -\infty} f(x).$$ I have so far observed that $f(x)$ is increasing in $(-\infty,0)$ therefore its range will be ($\lim\limits_{x\to -\infty}f(x), f(0))$ but have no idea how to continue. Maybe I should somehow apply the Mean Value Theorem.","If $f:\mathbb R \to \mathbb R$ is a differentiable function and $f'(x)>x^2$  for every $x<0$, find   $$\lim_{x\to -\infty} f(x).$$ I have so far observed that $f(x)$ is increasing in $(-\infty,0)$ therefore its range will be ($\lim\limits_{x\to -\infty}f(x), f(0))$ but have no idea how to continue. Maybe I should somehow apply the Mean Value Theorem.",,['calculus']
53,Limit Definition for Proof,Limit Definition for Proof,,"How can I use the definition of a limit to set up a proof for a statement such as: $$\lim_{n\to\infty} {f(n)} \to \infty$$ I have tried applying the standard definition, but I come out with meaningless expressions.","How can I use the definition of a limit to set up a proof for a statement such as: $$\lim_{n\to\infty} {f(n)} \to \infty$$ I have tried applying the standard definition, but I come out with meaningless expressions.",,"['real-analysis', 'limits']"
54,Why is this sum wrong?,Why is this sum wrong?,,$$\lim_{n\rightarrow\infty}\sum_{r=1}^n \sin\left(\frac{r}{n} \right)=\lim_{n\rightarrow\infty} \left[\sin\frac{1}{n}+\sin\frac{2}{n}+\cdots+\sin(1)\right]=0+0+\cdots+\sin(1)=1$$ Could anybody explain why this is wrong? I've tried to see why this doesn't work but I don't see why not. Thank you.,$$\lim_{n\rightarrow\infty}\sum_{r=1}^n \sin\left(\frac{r}{n} \right)=\lim_{n\rightarrow\infty} \left[\sin\frac{1}{n}+\sin\frac{2}{n}+\cdots+\sin(1)\right]=0+0+\cdots+\sin(1)=1$$ Could anybody explain why this is wrong? I've tried to see why this doesn't work but I don't see why not. Thank you.,,['limits']
55,Limit $\lim \limits_{x \to 2}{\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4}}$,Limit,\lim \limits_{x \to 2}{\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4}},"I try to calculate $\lim \limits_{x \to 2}{\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4}}$. So, $\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4} = \frac{(x-2)x}{(x+2)(x+\sqrt{(x-2)^2(x+1)}-2)}$ but I don't know what to do next.","I try to calculate $\lim \limits_{x \to 2}{\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4}}$. So, $\frac{\sqrt{x^3 - 3x^2 + 4}-x +2}{x^2 - 4} = \frac{(x-2)x}{(x+2)(x+\sqrt{(x-2)^2(x+1)}-2)}$ but I don't know what to do next.",,['limits']
56,Looking for the limit of a sum,Looking for the limit of a sum,,"Looking for a limit, with $1<\alpha\leq 2$, $\sigma>0$: $$\lim_{p\to \infty } \, \sum _{k=1}^p \left( \frac{1}{2 \pi  k!}\left(1+i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{1/\alpha } \left(1-i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{k/\alpha }+(-1)^k \left(1-i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{1/\alpha } \left(1+i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{k/\alpha }\right) \left(i^k K^k \sigma ^{1-k} \sec ^2\left(\frac{\pi  \alpha }{2}\right)^{-\frac{k}{\alpha }} \Gamma \left(\frac{k+\alpha -1}{\alpha }\right)\right),$$ which converges numerically. It is easy to get acceptable results with special functions for known values of $\alpha$, say with $\alpha=\frac{3}{2}$: $$-\frac{e^{\frac{K^3}{27 \sigma ^3}} \left(\sqrt[3]{2} K^2 \, _0\tilde{F}_1\left(;\frac{4}{3};\frac{K^6}{2916 \sigma ^6}\right)-18 \sigma ^2 \, _0\tilde{F}_1\left(;\frac{2}{3};\frac{K^6}{2916 \sigma ^6}\right)\right)}{9\ 2^{2/3} \sigma }$$ (where $_0\tilde{F}_1$ is the confluent hypergeometric function, $_0F_1(;a;z)=\sum _{k=0}^{\infty } \frac{z^k}{k! (a)_k}$, $(a)_k$ is the Pochhammer function: $(a)_n=a (a+1) \ldots  (a+n-1).$ )","Looking for a limit, with $1<\alpha\leq 2$, $\sigma>0$: $$\lim_{p\to \infty } \, \sum _{k=1}^p \left( \frac{1}{2 \pi  k!}\left(1+i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{1/\alpha } \left(1-i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{k/\alpha }+(-1)^k \left(1-i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{1/\alpha } \left(1+i \tan \left(\frac{\pi  \alpha }{2}\right)\right)^{k/\alpha }\right) \left(i^k K^k \sigma ^{1-k} \sec ^2\left(\frac{\pi  \alpha }{2}\right)^{-\frac{k}{\alpha }} \Gamma \left(\frac{k+\alpha -1}{\alpha }\right)\right),$$ which converges numerically. It is easy to get acceptable results with special functions for known values of $\alpha$, say with $\alpha=\frac{3}{2}$: $$-\frac{e^{\frac{K^3}{27 \sigma ^3}} \left(\sqrt[3]{2} K^2 \, _0\tilde{F}_1\left(;\frac{4}{3};\frac{K^6}{2916 \sigma ^6}\right)-18 \sigma ^2 \, _0\tilde{F}_1\left(;\frac{2}{3};\frac{K^6}{2916 \sigma ^6}\right)\right)}{9\ 2^{2/3} \sigma }$$ (where $_0\tilde{F}_1$ is the confluent hypergeometric function, $_0F_1(;a;z)=\sum _{k=0}^{\infty } \frac{z^k}{k! (a)_k}$, $(a)_k$ is the Pochhammer function: $(a)_n=a (a+1) \ldots  (a+n-1).$ )",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'pochhammer-symbol']"
57,Find this limits $\lim_{n\to\infty}n^2\bigl(n(H_{2n}-H_{n}-\ln{2})+\frac{1}{4}\bigr)$,Find this limits,\lim_{n\to\infty}n^2\bigl(n(H_{2n}-H_{n}-\ln{2})+\frac{1}{4}\bigr),Question1: Find this limits $$\lim_{n\to\infty}n^2\left(n(H_{2n}-H_{n}-\ln{2})+\dfrac{1}{4}\right)$$ where $$H_{n}=1+\dfrac{1}{2}+\dfrac{1}{3}+\cdots+\dfrac{1}{n}$$ Question 2: Can we obtain a higher asymptotic expansion? I know $$ \lim_{n\to\infty}n(H_{2n}-H_{n}-\ln{2})=-\dfrac{1}{4}$$ this following well know $$\lim_{n\to\infty}n\left(\sum_{i=1}^{n}f(\dfrac{i}{n})-\int_{0}^{1}f(x)dx\right)=\dfrac{f(1)-f(0)}{2}$$,Question1: Find this limits where Question 2: Can we obtain a higher asymptotic expansion? I know this following well know,\lim_{n\to\infty}n^2\left(n(H_{2n}-H_{n}-\ln{2})+\dfrac{1}{4}\right) H_{n}=1+\dfrac{1}{2}+\dfrac{1}{3}+\cdots+\dfrac{1}{n}  \lim_{n\to\infty}n(H_{2n}-H_{n}-\ln{2})=-\dfrac{1}{4} \lim_{n\to\infty}n\left(\sum_{i=1}^{n}f(\dfrac{i}{n})-\int_{0}^{1}f(x)dx\right)=\dfrac{f(1)-f(0)}{2},"['limits', 'asymptotics']"
58,Finding the limit $\lim\limits_{x\to 2} \frac{x^3-8}{x-2}$ when naive substitution yields a division by 0,Finding the limit  when naive substitution yields a division by 0,\lim\limits_{x\to 2} \frac{x^3-8}{x-2},I am trying to find the following. $$\lim\limits_{x\to 2} \frac{x^3-8}{x-2}$$ The book I am reading (Spivak's Calculus) provides the following answer: $$\lim\limits_{x\to 2} \frac{x^3-8}{x-2} = \lim\limits_{x\to 2} (x^2+2x+4)=12$$ I don't see how that works. Could you explain what is going on here?,I am trying to find the following. $$\lim\limits_{x\to 2} \frac{x^3-8}{x-2}$$ The book I am reading (Spivak's Calculus) provides the following answer: $$\lim\limits_{x\to 2} \frac{x^3-8}{x-2} = \lim\limits_{x\to 2} (x^2+2x+4)=12$$ I don't see how that works. Could you explain what is going on here?,,"['calculus', 'limits']"
59,Find the function,Find the function,,Find function $f: \mathbb{R} \to \mathbb{R}$ which has limit only at $0$ and $1$ I think function $f(x)=x(x-1)$ when $x\in \mathbb{Q}$ and $f(x)=-x(x-1)$ when $x \notin \mathbb{Q}$ satisfy conditions but I don't know how to prove it,Find function $f: \mathbb{R} \to \mathbb{R}$ which has limit only at $0$ and $1$ I think function $f(x)=x(x-1)$ when $x\in \mathbb{Q}$ and $f(x)=-x(x-1)$ when $x \notin \mathbb{Q}$ satisfy conditions but I don't know how to prove it,,"['calculus', 'limits', 'functions']"
60,Problems taking the limit in $\int_a^b f=\lim_{c\to a}\int_c^b f$ from definitions,Problems taking the limit in  from definitions,\int_a^b f=\lim_{c\to a}\int_c^b f,"Let $f$ be bounded on $[a,b]$ and Riemann integrable for each $c$ with $a<c<b$.  I need to show that $f$ is Riemann integrable on $[a,b]$, and $\int_a^b f=\lim_{c\to a}\int_c^b f$. My problem is that I'm not sure how to go about taking the limit on the RHS.  I can start with $\epsilon > 0 $ as always, but then by the time I've gotten done definition chasing (taking an arbitrary partitions, looking at the sup and inf in the the upper and lower sums, etc) everything has become so confusing that I can't make any headway.  How do I unravel the definitions here to get something tractable? Note: I am not trying to use any ""big theorem"" shortcuts here that just do it all in one step, I just want to prove it directly from the definitions, with minimal corner cutting. Also, because it sometimes seems relevant to mention this on this site, this is for self-study, not homework.","Let $f$ be bounded on $[a,b]$ and Riemann integrable for each $c$ with $a<c<b$.  I need to show that $f$ is Riemann integrable on $[a,b]$, and $\int_a^b f=\lim_{c\to a}\int_c^b f$. My problem is that I'm not sure how to go about taking the limit on the RHS.  I can start with $\epsilon > 0 $ as always, but then by the time I've gotten done definition chasing (taking an arbitrary partitions, looking at the sup and inf in the the upper and lower sums, etc) everything has become so confusing that I can't make any headway.  How do I unravel the definitions here to get something tractable? Note: I am not trying to use any ""big theorem"" shortcuts here that just do it all in one step, I just want to prove it directly from the definitions, with minimal corner cutting. Also, because it sometimes seems relevant to mention this on this site, this is for self-study, not homework.",,"['real-analysis', 'integration', 'limits']"
61,The limit of an expected value vs expected value of a limit in this betting game,The limit of an expected value vs expected value of a limit in this betting game,,"Setting The outcome $X$ of a slot machine takes values 1,2,or 3 with probability $p(1) = \frac{1}{2}$, $p(2) = \frac{1}{4}$, $p(3) = \frac{1}{4}$. We are given 3 for one odds, that is if we bet 1 dollar and win then we will win $3. The betting strategy is as follows: we bet $\frac{1}{2}$ of current wealth that $X = 1$, $\frac{1}{4}$ of wealth that $X = 2$, and $\frac{1}{4}$ of wealth that $X = 3$. So our winning after n rounds is $$V_n = 3 V_{n-1} p(X_n).$$ Now find $\lim_n V_{n}$. Attempted Solution So I take the expected value of $V_n$ for all n and then take the expected value to the limit: $$E[V_n] = \sum_{k=0}^n \binom{n}{k} \left(\frac{3}{4}\frac{1}{2}\right)^{n-k}\left(\frac{3}{2}\frac{1}{2}\right)^{k} = \left(\frac{9}{8}\right)^n$$ And $\left(\frac{9}{8}\right)^n \rightarrow^n \infty$. Confusion However, the counter argument to the solution is that we have taken then limit of an expectation, when in fact, I need to the expectation of the limit. And by Fatou's lemma $$\lim inf E[V_n] \ge E[\lim inf V_n]$$ So I found an upper bound that diverges and therefore cannot conclude anything about $V_n$. Please explain the subtleties here? Alternate Solution Now consider this alternate solution and please explain why it makes sense relative to Fatou's lemma. Define $\log := \log_2$. And note $$V_n = 3^n V_o  P(X_1) \cdot \ldots \cdot P(X_n)$$ So let $V_o = 1$ and we have: $$2^{\log V_n} = 2^{n \log 3 + \log V_o + \log P(X_1) + \ldots + \log P(X_n)}$$ And by LLN $\log P(X_1) + \ldots + \log P(X_n) \rightarrow^n n E[\log(X_j)] = n \left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{4} \right)$. And the rest is just algebra. But my primary concern is how does taking a limiting argument for the sum of $P(X_j)$'s sit wrt Fatou's lemma.","Setting The outcome $X$ of a slot machine takes values 1,2,or 3 with probability $p(1) = \frac{1}{2}$, $p(2) = \frac{1}{4}$, $p(3) = \frac{1}{4}$. We are given 3 for one odds, that is if we bet 1 dollar and win then we will win $3. The betting strategy is as follows: we bet $\frac{1}{2}$ of current wealth that $X = 1$, $\frac{1}{4}$ of wealth that $X = 2$, and $\frac{1}{4}$ of wealth that $X = 3$. So our winning after n rounds is $$V_n = 3 V_{n-1} p(X_n).$$ Now find $\lim_n V_{n}$. Attempted Solution So I take the expected value of $V_n$ for all n and then take the expected value to the limit: $$E[V_n] = \sum_{k=0}^n \binom{n}{k} \left(\frac{3}{4}\frac{1}{2}\right)^{n-k}\left(\frac{3}{2}\frac{1}{2}\right)^{k} = \left(\frac{9}{8}\right)^n$$ And $\left(\frac{9}{8}\right)^n \rightarrow^n \infty$. Confusion However, the counter argument to the solution is that we have taken then limit of an expectation, when in fact, I need to the expectation of the limit. And by Fatou's lemma $$\lim inf E[V_n] \ge E[\lim inf V_n]$$ So I found an upper bound that diverges and therefore cannot conclude anything about $V_n$. Please explain the subtleties here? Alternate Solution Now consider this alternate solution and please explain why it makes sense relative to Fatou's lemma. Define $\log := \log_2$. And note $$V_n = 3^n V_o  P(X_1) \cdot \ldots \cdot P(X_n)$$ So let $V_o = 1$ and we have: $$2^{\log V_n} = 2^{n \log 3 + \log V_o + \log P(X_1) + \ldots + \log P(X_n)}$$ And by LLN $\log P(X_1) + \ldots + \log P(X_n) \rightarrow^n n E[\log(X_j)] = n \left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{4} \right)$. And the rest is just algebra. But my primary concern is how does taking a limiting argument for the sum of $P(X_j)$'s sit wrt Fatou's lemma.",,"['probability', 'limits', 'probability-distributions', 'limsup-and-liminf']"
62,An idea on the Collatz problem,An idea on the Collatz problem,,"I am using the T-version of the function: $$ T(x)=\left\{\begin{array}{cl} \text{down}(x)=x/2,& \mbox{x even}\\ \quad\,\,\,\text{up}(x)=(3x+1)/2,& \mbox{x odd}\end{array}\right. $$ I will also make use of the iterating function $G(x,0)=x,G(x,1)=T(x),G(x,2)=T(T(x)),...$ The goal is to get an idea about when the T increases or decreases it's argument. Let's fix some $n\in \mathbf{N}$ constant for the moment and define $$ \mathbf{A}=\{2^n+k,k=0,1,\dots,2^n-1\} $$ ie the set of all the integers from $2^n$ to $2^{n+1}-1$. Now let's define a word to be a sequence of zeroes and ones of length $n$. Finally Let's denote by $\mathbf{W}$ the set of all distinct words. Every $x \in \mathbf{A}$has an ""address"" in $\mathbf{W}$ which we denote by $path(x)$, given by $$ path(x)=\{x,T(x),G(x,2),...G(x,n-1)\} \pmod{2} $$ Now here is the punch line: There is a $1-1$ correspondence between the set of numbers $\mathbf{A}$ and the set of words $\mathbf{W}$, given by the path function. Once we establish that we will have to deal with words instead of numbers which might prove to be more informative. How to show that? First of all note that both sets have $2^n$ elements. So if we can show that the path function is invertible we are done. Equivalently that every word $w \in \mathbf{W}$ has a unique element $a \in \mathbf{A}$ with $path(a)=w$. Equivalently $path(a)=path(b) \Rightarrow a=b$. Here is where I need help because the words are not in standard order, they come scrambled!. Take a look: For $n=1$: $\mathbf{A}=\{2,3\}$, $path(\mathbf{A})=\{0,1\}$ For $n=2$: $\mathbf{A}=\{4,5,6,7\}$, $path(\mathbf{A})=\{00,10,01,11\}$ For $n=3$: $\mathbf{A}=\{8,9,10,11,12,13,14,15\}$, $path(\mathbf{A})=\{000,101,010,110,001,100,011,111\}$ I have checked it for up to $2^8$ and it holds. No word is repeated, so I am convinced it's true. For any $x \in \mathbf{N}$ now we find the corresponding $n$ (which is basically $[\log{x}]$), the $\mathbf{A}$ set and then we find the path, so the path function is defined over all of $\mathbf{N}$. The power of the words representation: $1101001$ means go up twice, go down, go up, go down twice and go up. There are many interesting questions to ask: Can this be a non-trivial cycle? Conversely, given any word $w$, can it belong to a non-trivial cycle? Must there be more ones than zeroes in a non-trivial cycle? Is there an upper bound or lower bound on the fraction of number of ones over the number of zeroes? I guess you can say that I am asking too many questions, but the only answer I really need is this $1-1$ correspondence. I can't crack it but I bet some group theorist out there can help. Many thanks in advance.","I am using the T-version of the function: $$ T(x)=\left\{\begin{array}{cl} \text{down}(x)=x/2,& \mbox{x even}\\ \quad\,\,\,\text{up}(x)=(3x+1)/2,& \mbox{x odd}\end{array}\right. $$ I will also make use of the iterating function $G(x,0)=x,G(x,1)=T(x),G(x,2)=T(T(x)),...$ The goal is to get an idea about when the T increases or decreases it's argument. Let's fix some $n\in \mathbf{N}$ constant for the moment and define $$ \mathbf{A}=\{2^n+k,k=0,1,\dots,2^n-1\} $$ ie the set of all the integers from $2^n$ to $2^{n+1}-1$. Now let's define a word to be a sequence of zeroes and ones of length $n$. Finally Let's denote by $\mathbf{W}$ the set of all distinct words. Every $x \in \mathbf{A}$has an ""address"" in $\mathbf{W}$ which we denote by $path(x)$, given by $$ path(x)=\{x,T(x),G(x,2),...G(x,n-1)\} \pmod{2} $$ Now here is the punch line: There is a $1-1$ correspondence between the set of numbers $\mathbf{A}$ and the set of words $\mathbf{W}$, given by the path function. Once we establish that we will have to deal with words instead of numbers which might prove to be more informative. How to show that? First of all note that both sets have $2^n$ elements. So if we can show that the path function is invertible we are done. Equivalently that every word $w \in \mathbf{W}$ has a unique element $a \in \mathbf{A}$ with $path(a)=w$. Equivalently $path(a)=path(b) \Rightarrow a=b$. Here is where I need help because the words are not in standard order, they come scrambled!. Take a look: For $n=1$: $\mathbf{A}=\{2,3\}$, $path(\mathbf{A})=\{0,1\}$ For $n=2$: $\mathbf{A}=\{4,5,6,7\}$, $path(\mathbf{A})=\{00,10,01,11\}$ For $n=3$: $\mathbf{A}=\{8,9,10,11,12,13,14,15\}$, $path(\mathbf{A})=\{000,101,010,110,001,100,011,111\}$ I have checked it for up to $2^8$ and it holds. No word is repeated, so I am convinced it's true. For any $x \in \mathbf{N}$ now we find the corresponding $n$ (which is basically $[\log{x}]$), the $\mathbf{A}$ set and then we find the path, so the path function is defined over all of $\mathbf{N}$. The power of the words representation: $1101001$ means go up twice, go down, go up, go down twice and go up. There are many interesting questions to ask: Can this be a non-trivial cycle? Conversely, given any word $w$, can it belong to a non-trivial cycle? Must there be more ones than zeroes in a non-trivial cycle? Is there an upper bound or lower bound on the fraction of number of ones over the number of zeroes? I guess you can say that I am asking too many questions, but the only answer I really need is this $1-1$ correspondence. I can't crack it but I bet some group theorist out there can help. Many thanks in advance.",,"['limits', 'discrete-mathematics']"
63,Limit properties as $x\to \infty$ for functions,Limit properties as  for functions,x\to \infty,"1) Let $q\in(0,1)$ is fixed and $L$ is a finite value. Is it possible to say if $\lim_{x\to\infty}f(qx)=L$ then $\lim_{x\to\infty}f(x)=L.$ 2) And i also stack in if for all $\epsilon>0$ is it possible to say if $L-\epsilon\leq f(\frac{x}{q})$ and $f(qx)\leq L+\epsilon$ then $\lim_{x\to\infty}f(x)=L.$","1) Let $q\in(0,1)$ is fixed and $L$ is a finite value. Is it possible to say if $\lim_{x\to\infty}f(qx)=L$ then $\lim_{x\to\infty}f(x)=L.$ 2) And i also stack in if for all $\epsilon>0$ is it possible to say if $L-\epsilon\leq f(\frac{x}{q})$ and $f(qx)\leq L+\epsilon$ then $\lim_{x\to\infty}f(x)=L.$",,"['calculus', 'real-analysis', 'limits']"
64,Prove that there exists $c \in \mathbb{R}$ such that $f'(c)=0$ if $ \lim_{x\to-\infty}f(x) =\lim_{x\to\infty}f(x) = 0 $.?,Prove that there exists  such that  if .?,c \in \mathbb{R} f'(c)=0  \lim_{x\to-\infty}f(x) =\lim_{x\to\infty}f(x) = 0 ,Suppose that $f$  is differentiable on $ \mathbb{R} $ and $ \lim_{x\to-\infty}f(x) =\lim_{x\to\infty}f(x) = 0 $. Prove that there exists $c \in \mathbb{R}$ such that $f'(c)=0$ . and can I extending this claim  to $\mathbb{C}$ ?,Suppose that $f$  is differentiable on $ \mathbb{R} $ and $ \lim_{x\to-\infty}f(x) =\lim_{x\to\infty}f(x) = 0 $. Prove that there exists $c \in \mathbb{R}$ such that $f'(c)=0$ . and can I extending this claim  to $\mathbb{C}$ ?,,"['calculus', 'limits', 'maximum-principle']"
65,How to show that $\lim_{x\to \infty}f'(x)=0$,How to show that,\lim_{x\to \infty}f'(x)=0,"Let $f$ be a real-valued, bounded, twice differentiable function defined on $(0,\infty)$ with $f'(x)\ge 0$ and $f''(x)\le 0$. Show that $$\lim_{x\to \infty}f'(x)=0$$ I understand $f: (0,\infty) \rightarrow \mathbb{R}, |f(x)|<M$ and $f''$ exists. But I coun't find the way to use the facts and show the limit is zero.","Let $f$ be a real-valued, bounded, twice differentiable function defined on $(0,\infty)$ with $f'(x)\ge 0$ and $f''(x)\le 0$. Show that $$\lim_{x\to \infty}f'(x)=0$$ I understand $f: (0,\infty) \rightarrow \mathbb{R}, |f(x)|<M$ and $f''$ exists. But I coun't find the way to use the facts and show the limit is zero.",,"['real-analysis', 'limits', 'derivatives', 'self-learning']"
66,How do I prove the maximum of this function,How do I prove the maximum of this function,,"I have the function $$y = x - \sqrt{x^2 - 1}$$ which must have a maximum of $1$ at $x = 1$, as after that you're taking $x$ and subtracting something slightly smaller than $x$, tending to $0$ as $x$ tends to infinity, however its derivative of $$1 - \frac{x}{\sqrt{x^2 - 1}}$$ is undefined at $x = \pm 1$, as is its second derivative. How can I prove this function is bounded above by 1, and that the absolute value of y doesn't exceed 1 at some point 0 < x < 1?","I have the function $$y = x - \sqrt{x^2 - 1}$$ which must have a maximum of $1$ at $x = 1$, as after that you're taking $x$ and subtracting something slightly smaller than $x$, tending to $0$ as $x$ tends to infinity, however its derivative of $$1 - \frac{x}{\sqrt{x^2 - 1}}$$ is undefined at $x = \pm 1$, as is its second derivative. How can I prove this function is bounded above by 1, and that the absolute value of y doesn't exceed 1 at some point 0 < x < 1?",,['limits']
67,Prove a limit for $g(x)$ with the definition of limit only,Prove a limit for  with the definition of limit only,g(x),"We have the function $g(x) = x^3+1$ Prove, with the definition of limit only , that the limit $L=9$ is indeed the limit of the function when  $x=2$. I started with: $\lvert g(x) - L \rvert$ : $\lvert x^3+1-9 \rvert = \lvert x^3-8 \rvert = \lvert (x-2)(x^2+2x+4) \rvert < \epsilon $ From the definition of the limit we know: $ \rvert x-2 \lvert < \delta$,  so therefore: $\lvert (x-2)(x^2+2x+4) \rvert < \delta (x^2+2x+4) $ How do I proceed from here? do I need to find a condition for $ x^2+2x+4 $ ? Thanks guys","We have the function $g(x) = x^3+1$ Prove, with the definition of limit only , that the limit $L=9$ is indeed the limit of the function when  $x=2$. I started with: $\lvert g(x) - L \rvert$ : $\lvert x^3+1-9 \rvert = \lvert x^3-8 \rvert = \lvert (x-2)(x^2+2x+4) \rvert < \epsilon $ From the definition of the limit we know: $ \rvert x-2 \lvert < \delta$,  so therefore: $\lvert (x-2)(x^2+2x+4) \rvert < \delta (x^2+2x+4) $ How do I proceed from here? do I need to find a condition for $ x^2+2x+4 $ ? Thanks guys",,"['calculus', 'limits', 'epsilon-delta']"
68,How to calculate $\lim_{x \rightarrow \infty} (4x\arctan(x)-2\pi x)$,How to calculate,\lim_{x \rightarrow \infty} (4x\arctan(x)-2\pi x),"How to calculate $$\lim_{x \rightarrow \infty} (4x\arctan(x)-2\pi x)$$ ? I checked Wolframalpha and the answer is -4, shall I use some kinda standard limit? I dont get anywhere.","How to calculate $$\lim_{x \rightarrow \infty} (4x\arctan(x)-2\pi x)$$ ? I checked Wolframalpha and the answer is -4, shall I use some kinda standard limit? I dont get anywhere.",,"['calculus', 'limits']"
69,Question about $\lim_{x\to \infty}\frac{\cos(3x)}{e^{8x}}$,Question about,\lim_{x\to \infty}\frac{\cos(3x)}{e^{8x}},"$\lim_{x\to \infty}\dfrac{\cos(3x)}{e^{8x}}$ The answer is $0$. Why is the answer $0$? The top oscillates between $-1$  and $1$ and the bottom becomes huge, but since the top is oscillating, shouldn't the answer be DNE (does not exist)?","$\lim_{x\to \infty}\dfrac{\cos(3x)}{e^{8x}}$ The answer is $0$. Why is the answer $0$? The top oscillates between $-1$  and $1$ and the bottom becomes huge, but since the top is oscillating, shouldn't the answer be DNE (does not exist)?",,"['calculus', 'limits', 'infinity']"
70,What do limits of functions of the form $te^t$ have to do with l'Hopital's rule?,What do limits of functions of the form  have to do with l'Hopital's rule?,te^t,"I have an improper function that I have to integrate from some number to infinity. Once integration is done, the function is of the form $te^t$ . What I'm wondering is what does this have to do with l'Hopital's rule? From reading my book, I see the following: We know that $e^t \to 0$ as $t \to -\infty$ , and by l'Hopital's rule we have $$\lim_{t\to-\infty} te^t = \lim_{t\to-\infty} \frac{t}{e^{-t}} = \lim_{t\to-\infty} \frac{1}{-e^{-t}} = \lim_{t\to-\infty} -e^t = 0.$$ I know what l'Hopital's rule is The limit of a quotient of functions is equal to the limit of the quotient of their derivatives. I don't understand what this has to do with the explanation above. Can somebody help me understand this please?","I have an improper function that I have to integrate from some number to infinity. Once integration is done, the function is of the form . What I'm wondering is what does this have to do with l'Hopital's rule? From reading my book, I see the following: We know that as , and by l'Hopital's rule we have I know what l'Hopital's rule is The limit of a quotient of functions is equal to the limit of the quotient of their derivatives. I don't understand what this has to do with the explanation above. Can somebody help me understand this please?",te^t e^t \to 0 t \to -\infty \lim_{t\to-\infty} te^t = \lim_{t\to-\infty} \frac{t}{e^{-t}} = \lim_{t\to-\infty} \frac{1}{-e^{-t}} = \lim_{t\to-\infty} -e^t = 0.,"['integration', 'limits', 'improper-integrals']"
71,Proving $\lim_{x\to1}(x^2+3)=4$,Proving,\lim_{x\to1}(x^2+3)=4,"I'm learning about proving limits with the epsilon delta thing. Prove that $$\lim_{x\to1}(x^2+3)=4$$ Alright, so let's grab some $\epsilon > 0$ and $\delta > 0$. We want to prove that $$\left|x-1 \right| < \delta \implies \left|x^2+3-4 \right| < \epsilon$$ Let's begin with $$\left|x^2-1 \right| < \epsilon$$ $$\left|(x-1)(x+1) \right| < \epsilon$$ $$\left|(x-1)\right| \cdot\left|(x+1) \right| < \epsilon$$ Well... I have the feeling I can't simply do $$\left|(x-1)\right| < \frac{\epsilon}{\left|(x+1) \right|}$$ What do I do in this scenario?","I'm learning about proving limits with the epsilon delta thing. Prove that $$\lim_{x\to1}(x^2+3)=4$$ Alright, so let's grab some $\epsilon > 0$ and $\delta > 0$. We want to prove that $$\left|x-1 \right| < \delta \implies \left|x^2+3-4 \right| < \epsilon$$ Let's begin with $$\left|x^2-1 \right| < \epsilon$$ $$\left|(x-1)(x+1) \right| < \epsilon$$ $$\left|(x-1)\right| \cdot\left|(x+1) \right| < \epsilon$$ Well... I have the feeling I can't simply do $$\left|(x-1)\right| < \frac{\epsilon}{\left|(x+1) \right|}$$ What do I do in this scenario?",,"['calculus', 'limits']"
72,Growth rate of logarithmic function?,Growth rate of logarithmic function?,,Just curious about the growth rate of the logarithmic function: Does there exist a real number $n$ such that $lim_{x \to \infty} \frac{(ln(x))^{n}}{x}$ diverges (does not converge to $0$)? Thanks in advance!,Just curious about the growth rate of the logarithmic function: Does there exist a real number $n$ such that $lim_{x \to \infty} \frac{(ln(x))^{n}}{x}$ diverges (does not converge to $0$)? Thanks in advance!,,['limits']
73,Finding the limit $\lim_{x\to+\infty}(x-x^2\log(1+1/x))$ in a elementary way,Finding the limit  in a elementary way,\lim_{x\to+\infty}(x-x^2\log(1+1/x)),"How to find the limit $$\lim_{x\to+\infty}\left(x-x^2\log\left(1+\frac{1}{x}\right)\right)$$  in a elementary way?    I can solve with Taylor expansion, but it is placed in the beginning of my calculus book, so I should only use things like: -Main theorems involving limits, including the limits for $x\to 0$, $\lim\frac{\sin x}{x}$, $\lim\frac{e^x-1}{x}$, $\lim\frac{\log(x+1)}{x}$, $\lim\frac{(x+1)^p-1}{x}$ -$\frac{x}{1+x} \leq \log(1+x) \leq x$ or similar inequalities I cannot use derivatives, Taylor expansion, $o(x), O(x)$ and similar things. Using the inequality that I have written above and the substitution $x=\frac{1}{\sin t}$ I have been only able to prove that the limit is greater or equal than 0 and smaller or equal than 1. Any ideas?","How to find the limit $$\lim_{x\to+\infty}\left(x-x^2\log\left(1+\frac{1}{x}\right)\right)$$  in a elementary way?    I can solve with Taylor expansion, but it is placed in the beginning of my calculus book, so I should only use things like: -Main theorems involving limits, including the limits for $x\to 0$, $\lim\frac{\sin x}{x}$, $\lim\frac{e^x-1}{x}$, $\lim\frac{\log(x+1)}{x}$, $\lim\frac{(x+1)^p-1}{x}$ -$\frac{x}{1+x} \leq \log(1+x) \leq x$ or similar inequalities I cannot use derivatives, Taylor expansion, $o(x), O(x)$ and similar things. Using the inequality that I have written above and the substitution $x=\frac{1}{\sin t}$ I have been only able to prove that the limit is greater or equal than 0 and smaller or equal than 1. Any ideas?",,"['calculus', 'limits']"
74,Limit of a sequence of averages (three variables),Limit of a sequence of averages (three variables),,"Let $a_0 = 0$, $a_1 = 0$, $a_2=1$ and for $n>2$, $a_n = \dfrac{a_{n-1}+a_{n-2}+a_{n-3}}{3}$. Consider $\lim\limits_{n \to +\infty} a_n$. Using a python script I found that $a_n$ tends to $\frac{1}{2}$ as $n \to +\infty$. However I don't know how to prove this...I tried the approach from this question that I asked here a while back, but I couldn't anything like it to work because the differences between terms kept altering between positive and negative in an unpredictable fashion (i.e. -+-++-++-++-+--+--...) and the absolute value of the differences did not always decrease as $n$ increased ($|a_7-a_8| < |a_8-a_9|$). (Source: the python code, which I can include if that helps...) Another approach I tried was to use induction to try to prove the general case that if the first $k$ terms $a_0,\cdots,a_{k-1}$ are given where $a_0,\cdots,a_{k-2} = 0$ and $a_{k-1} = 1$ and for $n>k-1$, $a_n = \dfrac{\sum\limits_{i=1}^k a_{n-i}}{k}$ then that $\lim\limits_{n \to +\infty} a_n = \dfrac{2}{k+1}$ (this is just a guess after the first five or six terms), but I also made no progress there. Could anybody help point me in the right direction? How do I prove this? (The $k=3$ case is enough; proving the general case was just an idea.)","Let $a_0 = 0$, $a_1 = 0$, $a_2=1$ and for $n>2$, $a_n = \dfrac{a_{n-1}+a_{n-2}+a_{n-3}}{3}$. Consider $\lim\limits_{n \to +\infty} a_n$. Using a python script I found that $a_n$ tends to $\frac{1}{2}$ as $n \to +\infty$. However I don't know how to prove this...I tried the approach from this question that I asked here a while back, but I couldn't anything like it to work because the differences between terms kept altering between positive and negative in an unpredictable fashion (i.e. -+-++-++-++-+--+--...) and the absolute value of the differences did not always decrease as $n$ increased ($|a_7-a_8| < |a_8-a_9|$). (Source: the python code, which I can include if that helps...) Another approach I tried was to use induction to try to prove the general case that if the first $k$ terms $a_0,\cdots,a_{k-1}$ are given where $a_0,\cdots,a_{k-2} = 0$ and $a_{k-1} = 1$ and for $n>k-1$, $a_n = \dfrac{\sum\limits_{i=1}^k a_{n-i}}{k}$ then that $\lim\limits_{n \to +\infty} a_n = \dfrac{2}{k+1}$ (this is just a guess after the first five or six terms), but I also made no progress there. Could anybody help point me in the right direction? How do I prove this? (The $k=3$ case is enough; proving the general case was just an idea.)",,['sequences-and-series']
75,Using one limit to compute other [duplicate],Using one limit to compute other [duplicate],,"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 9 years ago . I've calculated $\lim_{n\to\infty}\dfrac{1^p+2^p+\cdots+n^p}{n^{p+1}}=\dfrac1{p+1}$ where $p\in\mathbb{N}$ fixed. I feel it should help me get this one $\lim_{n\to\infty}\left(\dfrac{1^p+2^p+\cdots+n^p}{n^{p}}-\dfrac{n}{p+1}\right)$, but I'm not sure how. Any hints?","This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 9 years ago . I've calculated $\lim_{n\to\infty}\dfrac{1^p+2^p+\cdots+n^p}{n^{p+1}}=\dfrac1{p+1}$ where $p\in\mathbb{N}$ fixed. I feel it should help me get this one $\lim_{n\to\infty}\left(\dfrac{1^p+2^p+\cdots+n^p}{n^{p}}-\dfrac{n}{p+1}\right)$, but I'm not sure how. Any hints?",,"['sequences-and-series', 'limits']"
76,Multivariable integral limitation proof,Multivariable integral limitation proof,,Please prove the following formula. $$ \lim_{n\to\infty}\int_0^1\cdots\int_0^1 \frac{n}{\sum_{i=1}^n x_i}dx_1\cdots dx_n = 2 $$,Please prove the following formula. $$ \lim_{n\to\infty}\int_0^1\cdots\int_0^1 \frac{n}{\sum_{i=1}^n x_i}dx_1\cdots dx_n = 2 $$,,"['calculus', 'integration', 'limits']"
77,How to evaluate limits having infinity by infinity form,How to evaluate limits having infinity by infinity form,,"After taking an improper integral $\int_0^\infty \dots $ I arrived at  $$\left({-x^2}e^{-\large\frac{x^2}{2a}}\,-2ae^{-\large\frac{x^2}{2a}}\right)\bigg|_{x=0}^{x=\infty}$$ Now I am trying to evaluate limits $x=0$ to $\infty$. The result should $2a$. How it comes?","After taking an improper integral $\int_0^\infty \dots $ I arrived at  $$\left({-x^2}e^{-\large\frac{x^2}{2a}}\,-2ae^{-\large\frac{x^2}{2a}}\right)\bigg|_{x=0}^{x=\infty}$$ Now I am trying to evaluate limits $x=0$ to $\infty$. The result should $2a$. How it comes?",,"['calculus', 'limits', 'improper-integrals']"
78,What happen to composite of infinite number of continuous functions?,What happen to composite of infinite number of continuous functions?,,We all know that a composite of continuous functions is continuous. And this holds for any $\textbf{finite}$ number of functions. My question is what happen to infinite number of functions? Is it always discontinuous? Or it can also be continuous in some cases? Any counter examples and examples where it is discontinuous and continuous for infinite number of functions? Many thanks!,We all know that a composite of continuous functions is continuous. And this holds for any $\textbf{finite}$ number of functions. My question is what happen to infinite number of functions? Is it always discontinuous? Or it can also be continuous in some cases? Any counter examples and examples where it is discontinuous and continuous for infinite number of functions? Many thanks!,,"['calculus', 'limits', 'functions', 'continuity']"
79,Two and Three Variable Limit Questions,Two and Three Variable Limit Questions,,"Find the following limits, if they exist. $$\lim_{x,y\rightarrow 0,0}\frac{x^2 + \sin^2 y}{\sqrt{x^2+y^2}}$$ I believe we're suppose to use the squeeze theorem on this first one above. Possibly utilizing the fact that sin(y) is always between -1 and 1? I know the end result is suppose to be zero, but I'm having a hard time getting there. $$\lim_{x,y,z\rightarrow 0,0,0}\frac{x^2 yz}{x^8 + y^4 + z^2}$$ I know the end result is suppose to be a DNE. I attempted to set multiple variables equal to zero to see what it would come out to, and got differing values. So, when $y,z = 0$, the limit $= 0$, when $y,z = x$, the limit $= \infty$. Since these values are different -> DNE. However I'm quite sure I'm not getting the full picture here either. If someone could go over the process and logic associated with these problems, I'd greatly appreciate it. This is NOT homework, this is test prep.","Find the following limits, if they exist. $$\lim_{x,y\rightarrow 0,0}\frac{x^2 + \sin^2 y}{\sqrt{x^2+y^2}}$$ I believe we're suppose to use the squeeze theorem on this first one above. Possibly utilizing the fact that sin(y) is always between -1 and 1? I know the end result is suppose to be zero, but I'm having a hard time getting there. $$\lim_{x,y,z\rightarrow 0,0,0}\frac{x^2 yz}{x^8 + y^4 + z^2}$$ I know the end result is suppose to be a DNE. I attempted to set multiple variables equal to zero to see what it would come out to, and got differing values. So, when $y,z = 0$, the limit $= 0$, when $y,z = x$, the limit $= \infty$. Since these values are different -> DNE. However I'm quite sure I'm not getting the full picture here either. If someone could go over the process and logic associated with these problems, I'd greatly appreciate it. This is NOT homework, this is test prep.",,"['calculus', 'limits', 'multivariable-calculus', '3d']"
80,Tough limit to evaluate,Tough limit to evaluate,,I am trying to solve this limit problem $$\lim_{x\to 1} {(1-x)(1-x^2)....(1-x^{2n})\over[(1-x)(1-x^2)....(1-x^n)]^2}$$ I am not able to figure how to to convert it to a compact form. Any tips?,I am trying to solve this limit problem $$\lim_{x\to 1} {(1-x)(1-x^2)....(1-x^{2n})\over[(1-x)(1-x^2)....(1-x^n)]^2}$$ I am not able to figure how to to convert it to a compact form. Any tips?,,['limits']
81,Proof $e^n*n!$ is an asymptote of $(n+1)^n$,Proof  is an asymptote of,e^n*n! (n+1)^n,I would like to prove $\lim_{n\to \infty}e^nn!-(n+1)^n=0$. All I have really done is show $(n+1)^n=\sum_{i=0}^n\frac{n!}{(n+1)^i(i!)(n-i)!}$,I would like to prove $\lim_{n\to \infty}e^nn!-(n+1)^n=0$. All I have really done is show $(n+1)^n=\sum_{i=0}^n\frac{n!}{(n+1)^i(i!)(n-i)!}$,,"['sequences-and-series', 'combinatorics', 'limits']"
82,Use squeeze theorem to find the limit of a non-trigonometric (rational) function,Use squeeze theorem to find the limit of a non-trigonometric (rational) function,,Use the squeeze theorem to prove $$\lim_{x \to 0} \frac {2x^3}{x+1} =0$$ The only thing I can think of is that for all $x<0$  $f(x) $will be negative and positive for all $x>0$ $$\lim_{x \to 0}-x \le \frac {2x^3}{x+1} \le x$$ and then substitute $x$ proving $\lim_{x \to 0} -x = \lim f(x) = \lim x$. Is this valid or is there a better way?,Use the squeeze theorem to prove $$\lim_{x \to 0} \frac {2x^3}{x+1} =0$$ The only thing I can think of is that for all $x<0$  $f(x) $will be negative and positive for all $x>0$ $$\lim_{x \to 0}-x \le \frac {2x^3}{x+1} \le x$$ and then substitute $x$ proving $\lim_{x \to 0} -x = \lim f(x) = \lim x$. Is this valid or is there a better way?,,"['calculus', 'limits']"
83,How to calculate this expression?(multiplication),How to calculate this expression?(multiplication),,"How do I show that for any starting $n$? (I am not really sure it is $0$, but I think it is). $$\prod_{i=n}^\infty \left[1-\frac 1 i\right]=\prod_{i=n}^\infty \left[\frac{i-1}{i}\right]=0$$ I tried taking logarighm, then I get a sum. $\sum_{i=n}^\infty \ln\left(\frac{i-1}{i}\right)$. But then I must show that this sum goes to $-\infty$ can I show that in some way?","How do I show that for any starting $n$? (I am not really sure it is $0$, but I think it is). $$\prod_{i=n}^\infty \left[1-\frac 1 i\right]=\prod_{i=n}^\infty \left[\frac{i-1}{i}\right]=0$$ I tried taking logarighm, then I get a sum. $\sum_{i=n}^\infty \ln\left(\frac{i-1}{i}\right)$. But then I must show that this sum goes to $-\infty$ can I show that in some way?",,"['calculus', 'real-analysis', 'limits', 'infinite-product']"
84,Why did Michael Spivak pick $\frac{\epsilon}{2(|m|+1)}$? Why didn't he start with arbitrary $\epsilon$?,Why did Michael Spivak pick ? Why didn't he start with arbitrary ?,\frac{\epsilon}{2(|m|+1)} \epsilon,"On page 104 in Calculus (2008 4th edition), Michael Spivak proves that limits of two functions equals the limit of the product of the functions, as follows. If $\epsilon>0$ there are $\delta_1,\delta_2>0$ such that, for all $x$ , $$\text{if }0 <|x-a|<\delta_1,\text{ then }|f(x)-l|<\min\left(1,\frac{\epsilon}{2(|m|+1)}\right),$$ $$\text{ and if }0 <|x-a|<\delta_2,\text{ then }|g(x)-m|<\frac{\epsilon}{2(|l|+1)}.$$ This is supposedly due to the fact that $\lim_{x\to a}f(x)=l$ and $\lim_{x\to a}g(x)=m$ . There are two things I don't understand about this argument. First, shouldn't you start from $\epsilon$ when making such a proof, not $\frac{\epsilon}{2(|m|+1)}$ or something like that? Second, does the first row really hold? It merely tries (if I've understood it correctly) that $$\lim_{x\to a}f(x)=l.$$ In the definition of a limit it says that for any $\epsilon$ we can find a corresponding $\delta$ . But it seems like you can't in this case. What if $\frac{\epsilon}{2(|m|+1)}>1$ ? Then we can't really know for sure if the first statement holds, since for all we know, $|f(x)-l|$ could lie between $1$ and $\frac{\epsilon}{2(|m|+1)}>1$ . How am I understanding his proof wrong?","On page 104 in Calculus (2008 4th edition), Michael Spivak proves that limits of two functions equals the limit of the product of the functions, as follows. If there are such that, for all , This is supposedly due to the fact that and . There are two things I don't understand about this argument. First, shouldn't you start from when making such a proof, not or something like that? Second, does the first row really hold? It merely tries (if I've understood it correctly) that In the definition of a limit it says that for any we can find a corresponding . But it seems like you can't in this case. What if ? Then we can't really know for sure if the first statement holds, since for all we know, could lie between and . How am I understanding his proof wrong?","\epsilon>0 \delta_1,\delta_2>0 x \text{if }0 <|x-a|<\delta_1,\text{ then }|f(x)-l|<\min\left(1,\frac{\epsilon}{2(|m|+1)}\right), \text{ and if }0 <|x-a|<\delta_2,\text{ then }|g(x)-m|<\frac{\epsilon}{2(|l|+1)}. \lim_{x\to a}f(x)=l \lim_{x\to a}g(x)=m \epsilon \frac{\epsilon}{2(|m|+1)} \lim_{x\to a}f(x)=l. \epsilon \delta \frac{\epsilon}{2(|m|+1)}>1 |f(x)-l| 1 \frac{\epsilon}{2(|m|+1)}>1","['real-analysis', 'limits']"
85,if $\lim_{n\to\infty}(4a_{n+2}-4a_{n+1}+a_{n})=2014$ prove the $\lim_{n\to\infty}a_{n}$ is exist and find the value,if  prove the  is exist and find the value,\lim_{n\to\infty}(4a_{n+2}-4a_{n+1}+a_{n})=2014 \lim_{n\to\infty}a_{n},"Let sequence $\{a_{n}\}$ such  $$\lim_{n\to\infty}(4a_{n+2}-4a_{n+1}+a_{n})=2014$$ show that $$\lim_{n\to \infty}a_{n}$$ exist and find the limit value. Now I use an ugly method to solve this. I use this follow lemma: if $$\lim_{n\to\infty}(a_{n+1}-\lambda a_{n})=a \Longleftrightarrow \lim_{n\to\infty}a_{n}=a,|\lambda|<1$$ I know this lemma proof is ugly, maybe anyone here has a simple method. Thank you.","Let sequence $\{a_{n}\}$ such  $$\lim_{n\to\infty}(4a_{n+2}-4a_{n+1}+a_{n})=2014$$ show that $$\lim_{n\to \infty}a_{n}$$ exist and find the limit value. Now I use an ugly method to solve this. I use this follow lemma: if $$\lim_{n\to\infty}(a_{n+1}-\lambda a_{n})=a \Longleftrightarrow \lim_{n\to\infty}a_{n}=a,|\lambda|<1$$ I know this lemma proof is ugly, maybe anyone here has a simple method. Thank you.",,"['analysis', 'limits']"
86,Evaluating e using limits,Evaluating e using limits,,What algebraic operations can I use on the $RHS$ to show $RHS = LHS$ $$e=\lim_{k\to\infty}\left(\frac{2+\sqrt{3+9k^2}}{3k-1}\right)^k$$,What algebraic operations can I use on the $RHS$ to show $RHS = LHS$ $$e=\lim_{k\to\infty}\left(\frac{2+\sqrt{3+9k^2}}{3k-1}\right)^k$$,,"['calculus', 'limits']"
87,"Limit of the ""productory""","Limit of the ""productory""",,"With the term ""productory"" I just mean $\Pi_{i=m}^nx_i$  but I do not know the english term. My question is: is there a limit for such an expression in the same sense as the limit of a sum is an integral, i.e. $\lim_{\delta x\rightarrow 0}\Sigma y(x)\delta x = \int y(x) dx$? Has it any application?","With the term ""productory"" I just mean $\Pi_{i=m}^nx_i$  but I do not know the english term. My question is: is there a limit for such an expression in the same sense as the limit of a sum is an integral, i.e. $\lim_{\delta x\rightarrow 0}\Sigma y(x)\delta x = \int y(x) dx$? Has it any application?",,"['limits', 'soft-question', 'products']"
88,Proving divergence of a limit,Proving divergence of a limit,,$(a_n)_{n\ge 1}$ be a sequence of positive reals such that $a_1+a_2+\cdots +a_n<n^2$ for all $n\ge 1$. Prove that  $$\displaystyle \lim_{n\to \infty}\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}\right)=\infty $$ My attempt : Suppose eventually $a_{n+1}<2n+1$ then we have our condition is satisfied but also $$\displaystyle \sum \frac{1}{a_{n+1}}>\sum \frac{1}{2n+1}$$ but the right hand side diverges. Though I am not sure if this process is correct and probably it is not but any and all help will be welcomed. Thanks in advance.,$(a_n)_{n\ge 1}$ be a sequence of positive reals such that $a_1+a_2+\cdots +a_n<n^2$ for all $n\ge 1$. Prove that  $$\displaystyle \lim_{n\to \infty}\left(\frac{1}{a_1}+\frac{1}{a_2}+\cdots+\frac{1}{a_n}\right)=\infty $$ My attempt : Suppose eventually $a_{n+1}<2n+1$ then we have our condition is satisfied but also $$\displaystyle \sum \frac{1}{a_{n+1}}>\sum \frac{1}{2n+1}$$ but the right hand side diverges. Though I am not sure if this process is correct and probably it is not but any and all help will be welcomed. Thanks in advance.,,"['sequences-and-series', 'limits']"
89,What must a function satisfy in order to say that a certain limit exists?,What must a function satisfy in order to say that a certain limit exists?,,"Suppose that $f:R_+\to R_+$ is $C^2$, and that $f(0)=f'(0)=f''(0)=0$, and that for all $x$, $\frac{xf''(x)}{f'(x)}\geq1$. From here we can safely say that $\lim\inf_{x\to0}\frac{xf''(x)}{f'(x)}\geq1$. But, under what conditions on $f$ can we say that the $\lim_{x\to0}\frac{xf''(x)}{f'(x)}$ exists and is greater than 1?","Suppose that $f:R_+\to R_+$ is $C^2$, and that $f(0)=f'(0)=f''(0)=0$, and that for all $x$, $\frac{xf''(x)}{f'(x)}\geq1$. From here we can safely say that $\lim\inf_{x\to0}\frac{xf''(x)}{f'(x)}\geq1$. But, under what conditions on $f$ can we say that the $\lim_{x\to0}\frac{xf''(x)}{f'(x)}$ exists and is greater than 1?",,"['calculus', 'real-analysis', 'limits', 'multivariable-calculus', 'limsup-and-liminf']"
90,Does this series converge ? $\sum_{n=1}^{\infty}\frac{\ln2\ln3\dots\ln(n+1)}{\ln(2+a)\ln(3+a)\dots \ln(n+1+a)}$,Does this series converge ?,\sum_{n=1}^{\infty}\frac{\ln2\ln3\dots\ln(n+1)}{\ln(2+a)\ln(3+a)\dots \ln(n+1+a)},"I want to evaluate $$\sum_{n=1}^{\infty}\frac{\ln2\ln3\dots\ln(n+1)}{\ln(2+a)\ln(3+a)\dots \ln(n+1+a)}$$ where $a>0$. I tried to see if it converges or not with Raabe Duhamel , but it gets really nasty and I'm not sure of the outcome.Any help is welcomed.","I want to evaluate $$\sum_{n=1}^{\infty}\frac{\ln2\ln3\dots\ln(n+1)}{\ln(2+a)\ln(3+a)\dots \ln(n+1+a)}$$ where $a>0$. I tried to see if it converges or not with Raabe Duhamel , but it gets really nasty and I'm not sure of the outcome.Any help is welcomed.",,"['calculus', 'limits', 'convergence-divergence']"
91,Finding limit and sum of power series,Finding limit and sum of power series,,"Have such limit: $$   \lim \limits_{n \to \infty} \dfrac{1^{15}+3^{15}+ ...+ (2n-1)^{15}}{n^{16}} $$ But the sum $$  \sum_{n=1}^\infty (2n-1)^{15} $$ diverges. I think, that the answer is 0, because it's the sum of limits where the largest is 1/n but I can't prove it (if it is actually true). Can you help me to solve this?","Have such limit: $$   \lim \limits_{n \to \infty} \dfrac{1^{15}+3^{15}+ ...+ (2n-1)^{15}}{n^{16}} $$ But the sum $$  \sum_{n=1}^\infty (2n-1)^{15} $$ diverges. I think, that the answer is 0, because it's the sum of limits where the largest is 1/n but I can't prove it (if it is actually true). Can you help me to solve this?",,"['limits', 'power-series']"
92,How find this limit $\lim_{n\to \infty} \left(\frac{(2n)!}{2^n\cdot n!}\right)^{\frac{1}{n}}\cdot \cdots$,How find this limit,\lim_{n\to \infty} \left(\frac{(2n)!}{2^n\cdot n!}\right)^{\frac{1}{n}}\cdot \cdots,"Find this limit $$\lim_{n\to\infty}\left(\dfrac{(2n)!}{2^n\cdot n!}\right)^{\frac{1}{n}}\left(\tan{\left(\dfrac{\pi\sqrt[n+1]{(n+1)!}}{4\sqrt[n]{n!}}\right)}-1\right)$$ I know we must use this  $$n!\approx\left(\dfrac{n}{e}\right)^{n}\sqrt{2n\pi}$$ so $$\dfrac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}\approx\dfrac{n+1}{n}\dfrac{\sqrt[n+1]{2(n+1)\pi}}{\sqrt[n]{n!}}\to 1,n\to \infty$$ But I can't .Thank you","Find this limit $$\lim_{n\to\infty}\left(\dfrac{(2n)!}{2^n\cdot n!}\right)^{\frac{1}{n}}\left(\tan{\left(\dfrac{\pi\sqrt[n+1]{(n+1)!}}{4\sqrt[n]{n!}}\right)}-1\right)$$ I know we must use this  $$n!\approx\left(\dfrac{n}{e}\right)^{n}\sqrt{2n\pi}$$ so $$\dfrac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}\approx\dfrac{n+1}{n}\dfrac{\sqrt[n+1]{2(n+1)\pi}}{\sqrt[n]{n!}}\to 1,n\to \infty$$ But I can't .Thank you",,['limits']
93,Sequential Criterion for Limits,Sequential Criterion for Limits,,"Prove that if for every sequence ($x_n$) in A that converges to c such that $x_n \ne c$ for all $n \in \mathbb{N}$, the sequence $(f(x_n))$ converges to L, then $lim_{x\to c} f = L$.","Prove that if for every sequence ($x_n$) in A that converges to c such that $x_n \ne c$ for all $n \in \mathbb{N}$, the sequence $(f(x_n))$ converges to L, then $lim_{x\to c} f = L$.",,"['real-analysis', 'limits']"
94,Limit of a functional,Limit of a functional,,"I'd like to find: $$ \lim_{\varepsilon\rightarrow 0}\frac{\varepsilon}{\varepsilon^2+x^2}\qquad \mbox{ in }\mathcal D'(\mathbb{R}) $$ And I started with the definition: $$ \left\langle \frac{\varepsilon}{\varepsilon^2+x^2},\varphi\right\rangle $$ After doing the integration by parts I had some problems with the support of $\varphi$. I think the result could be $0$, but I don't know how to prove it in a proper way. (during the proof I used the substitution $x=\varepsilon y$ or $y=\varepsilon x$).","I'd like to find: $$ \lim_{\varepsilon\rightarrow 0}\frac{\varepsilon}{\varepsilon^2+x^2}\qquad \mbox{ in }\mathcal D'(\mathbb{R}) $$ And I started with the definition: $$ \left\langle \frac{\varepsilon}{\varepsilon^2+x^2},\varphi\right\rangle $$ After doing the integration by parts I had some problems with the support of $\varphi$. I think the result could be $0$, but I don't know how to prove it in a proper way. (during the proof I used the substitution $x=\varepsilon y$ or $y=\varepsilon x$).",,"['sequences-and-series', 'functional-analysis', 'limits', 'distribution-theory']"
95,"Show there must be another ""partial limit""","Show there must be another ""partial limit""",,"Let $P$ the set of all partial limits of the sequence $\{a_n\}$ (partial limit = limit of a subsequence). It's given that $\{0,2\} \subseteq P$ and $\forall n\in \mathbb{N}. \left| {a_{n+1}-a_n} \right| < 1$. Show that there must be another partial limit. My try: Assuming by contradiction there are only $2$ partial limits; $\{0,2\}$. We have: $$\eqalign{   & \forall \varepsilon  > 0\exists K \in N.\forall k > K:\left| {{a_{{n_k}}} - 0} \right| < \varepsilon   \cr    & \forall \varepsilon  > 0\exists L \in N.\forall l > L:\left| {{a_{{n_l}}} - 2} \right| < \varepsilon  \cr} $$ Now, there must $k_0$ such that $k_0+1=l_0$. From the two inequalities above we have: $$2 - 2\varepsilon  < \left| {{a_{{k_0}}} - {a_{{l_0}}}} \right| < 2 + 2\varepsilon $$ Since we can choose $\varepsilon$ to be as small as we want, clearly  $$\left| {{a_{{k_0}}} - {a_{{l_0}}}} \right| > 1$$ Which is a contradiction to  $$\left| {{a_{n + 1}} - {a_n}} \right| < 1$$ Am I right? I'm not 100% percent sure of what I did here.","Let $P$ the set of all partial limits of the sequence $\{a_n\}$ (partial limit = limit of a subsequence). It's given that $\{0,2\} \subseteq P$ and $\forall n\in \mathbb{N}. \left| {a_{n+1}-a_n} \right| < 1$. Show that there must be another partial limit. My try: Assuming by contradiction there are only $2$ partial limits; $\{0,2\}$. We have: $$\eqalign{   & \forall \varepsilon  > 0\exists K \in N.\forall k > K:\left| {{a_{{n_k}}} - 0} \right| < \varepsilon   \cr    & \forall \varepsilon  > 0\exists L \in N.\forall l > L:\left| {{a_{{n_l}}} - 2} \right| < \varepsilon  \cr} $$ Now, there must $k_0$ such that $k_0+1=l_0$. From the two inequalities above we have: $$2 - 2\varepsilon  < \left| {{a_{{k_0}}} - {a_{{l_0}}}} \right| < 2 + 2\varepsilon $$ Since we can choose $\varepsilon$ to be as small as we want, clearly  $$\left| {{a_{{k_0}}} - {a_{{l_0}}}} \right| > 1$$ Which is a contradiction to  $$\left| {{a_{n + 1}} - {a_n}} \right| < 1$$ Am I right? I'm not 100% percent sure of what I did here.",,"['calculus', 'limits', 'probability-limit-theorems']"
96,Difference between $\lim_\limits{n\rightarrow\infty}\sum_\limits{k=1}^{n}a_k$ and $\sum_\limits{k=1}^{\infty}a_k$,Difference between  and,\lim_\limits{n\rightarrow\infty}\sum_\limits{k=1}^{n}a_k \sum_\limits{k=1}^{\infty}a_k,Is there any difference between $\lim_{n\rightarrow\infty}\sum_\limits{k=1}^{n}a_k$ and $\sum_\limits{k=1}^{\infty}a_k$? My example and thought: Let $a_n=n$ where $n\in\mathbb{P}$. $\mathbb{P}$ is the set of all positive integer. Then $$\lim_{n\rightarrow\infty}\sum_{k=1}^{n}a_k=\lim_{n\rightarrow\infty}\sum_{k=1}^{n}k=\lim_{n\rightarrow\infty}\frac{n(n+1)}{2}\rightarrow\infty$$ And $$\sum_{k=1}^{\infty}a_k=\sum_{k=1}^{\infty}k=-\frac{1}{12}.$$ Am I thinking right?,Is there any difference between $\lim_{n\rightarrow\infty}\sum_\limits{k=1}^{n}a_k$ and $\sum_\limits{k=1}^{\infty}a_k$? My example and thought: Let $a_n=n$ where $n\in\mathbb{P}$. $\mathbb{P}$ is the set of all positive integer. Then $$\lim_{n\rightarrow\infty}\sum_{k=1}^{n}a_k=\lim_{n\rightarrow\infty}\sum_{k=1}^{n}k=\lim_{n\rightarrow\infty}\frac{n(n+1)}{2}\rightarrow\infty$$ And $$\sum_{k=1}^{\infty}a_k=\sum_{k=1}^{\infty}k=-\frac{1}{12}.$$ Am I thinking right?,,"['limits', 'notation', 'summation']"
97,Can anyone help to get rid of this infinity-infinity?,Can anyone help to get rid of this infinity-infinity?,,How does one get rid of the infinities arising here? $$\lim_{x\to\infty}\left(\frac{\ln|x-1|}3-\frac{\ln|x+2|}3\right)$$ I really have no idea how to handle such natural logarithms.,How does one get rid of the infinities arising here? $$\lim_{x\to\infty}\left(\frac{\ln|x-1|}3-\frac{\ln|x+2|}3\right)$$ I really have no idea how to handle such natural logarithms.,,['limits']
98,Evaluate the limit $\lim\limits_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\frac1x}$,Evaluate the limit,\lim\limits_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\frac1x},"Evaluate $$ \displaystyle\lim_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}} $$ And actually I have my answer and just need someone to verify this for me since I haven't done something like this for a long time. First, to deal with the pesky $1/x$ , I take the natural log inside the limit: \begin{align} \lim_{x\to0+}\ln\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}} &= \lim_{x\to0+}\frac{1}{x}\ln\left(\frac{3^x+5^x}{2}\right)\\ &= \lim_{x\to0+}\frac{\ln(3^x+5^x)-\ln2}{x}\\ &= \lim_{x\to0+}\frac{3^x\ln3+5^x\ln5}{3^x+5^x}......L'Hopital's \;Rule\\ &=\frac{\ln3+\ln5}{2}\\ &=\frac{1}{2}\ln3+\frac{1}{2}\ln5 \end{align} And since what we calculated was the limit the of the natural log, the final answer would be $\displaystyle e^{\frac{1}{2}ln3+\frac{1}{2}ln5}=e^{\sqrt{3}+\sqrt{5}}$ . Please tell me if I did this correctly, thanks.","Evaluate And actually I have my answer and just need someone to verify this for me since I haven't done something like this for a long time. First, to deal with the pesky , I take the natural log inside the limit: And since what we calculated was the limit the of the natural log, the final answer would be . Please tell me if I did this correctly, thanks.","
\displaystyle\lim_{x\to0+}\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}}
 1/x \begin{align}
\lim_{x\to0+}\ln\left(\frac{3^x+5^x}{2}\right)^{\displaystyle\frac{1}{x}}
&= \lim_{x\to0+}\frac{1}{x}\ln\left(\frac{3^x+5^x}{2}\right)\\
&= \lim_{x\to0+}\frac{\ln(3^x+5^x)-\ln2}{x}\\
&= \lim_{x\to0+}\frac{3^x\ln3+5^x\ln5}{3^x+5^x}......L'Hopital's \;Rule\\
&=\frac{\ln3+\ln5}{2}\\
&=\frac{1}{2}\ln3+\frac{1}{2}\ln5
\end{align} \displaystyle e^{\frac{1}{2}ln3+\frac{1}{2}ln5}=e^{\sqrt{3}+\sqrt{5}}",['limits']
99,Limit of $\sqrt[x]{1+\sin x}$ with $x$ approaching to $0$,Limit of  with  approaching to,\sqrt[x]{1+\sin x} x 0,"Could anyone explain to me how to calculate the limit $\lim_{x\to0} \sqrt[x]{1+\sin x}$? I am pretty sure that the answer is $e$, but how do I arrive at that? I tried to calculate one-sided limits playing with the Squeeze Theorem: in a neighbourhood $(0,\frac{\pi}{2})$ of $0$ one has $\sin x \leq x$, hence $\sqrt[x]{1+\sin x}\leq \sqrt[x]{1+x}$, and the right-hand side tends do $e$ when $x\to 0$. But how to evaluate the expression from the bottom?","Could anyone explain to me how to calculate the limit $\lim_{x\to0} \sqrt[x]{1+\sin x}$? I am pretty sure that the answer is $e$, but how do I arrive at that? I tried to calculate one-sided limits playing with the Squeeze Theorem: in a neighbourhood $(0,\frac{\pi}{2})$ of $0$ one has $\sin x \leq x$, hence $\sqrt[x]{1+\sin x}\leq \sqrt[x]{1+x}$, and the right-hand side tends do $e$ when $x\to 0$. But how to evaluate the expression from the bottom?",,['limits']
