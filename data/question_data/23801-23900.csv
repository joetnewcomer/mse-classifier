,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is every algebraic structure of this sort embeddable in a vector space?,Is every algebraic structure of this sort embeddable in a vector space?,,"A convex space is a set $X$ equipped with a function $X \times X \times [0, 1] \to X$ , denoted as $(a, b, t) \mapsto a \overset{t}{+} b$ , such that for all $a, b, c \in X$ and $t, t' \in (0, 1)$ , the following axioms hold: (""identity"") $a \overset{0}{+} b = a$ , $a \overset{1}{+} b = b$ , and $a \overset{t}{+} a = a$ . (""associativity"") $a \overset{t}{+} (b \overset{t'}{+} c) = (a \overset{\frac{t(1-t')}{1-tt'}}{+} b) \overset{tt'}{+} c$ . (""commutativity"") $a \overset{t}{+} b = b \overset{1-t}{+} a$ . (There is a short Wikipedia entry about this type of algebraic structure. I was originally curious about it as a simple way to model real-life paint mixing, and funny enough, independently even gave it the same name.) Let's say that a convex space $X$ is embeddable in a vector space if there exists a vector space $V$ over $\mathbb{R}$ with $X \subseteq V$ and $a \overset{t}{+} b = (1-t)a + tb$ for all $a, b \in X$ and $t \in [0, 1]$ . Now my initial question was whether every convex space is embeddable in a vector space - it turns out the answer is no, and here is a counterexample: Let $X$ be the two-point set $\{x, y\}$ and define $$a \overset{t}{+} b = \begin{cases} x & \text{if ($a = b = x$) or ($a = x$ and $t = 0$) or ($b = x$ and $t = 1$)},\\ y & \text{otherwise.} \end{cases}$$ Then $X$ is a convex space that cannot be embeddable in a vector space, for then the equation $\frac{1}{2}x + \frac{1}{2}y = x \overset{\frac{1}{2}}{+} y = y$ gives $x = y$ . This motivates an additional axiom: A convex space $X$ is faithful if the function $t \mapsto a \overset{t}{+} b$ is injective for each pair of distinct $a, b \in X$ . Now, is this enough? That is to say, is every faithful convex space embeddable in a vector space ?","A convex space is a set equipped with a function , denoted as , such that for all and , the following axioms hold: (""identity"") , , and . (""associativity"") . (""commutativity"") . (There is a short Wikipedia entry about this type of algebraic structure. I was originally curious about it as a simple way to model real-life paint mixing, and funny enough, independently even gave it the same name.) Let's say that a convex space is embeddable in a vector space if there exists a vector space over with and for all and . Now my initial question was whether every convex space is embeddable in a vector space - it turns out the answer is no, and here is a counterexample: Let be the two-point set and define Then is a convex space that cannot be embeddable in a vector space, for then the equation gives . This motivates an additional axiom: A convex space is faithful if the function is injective for each pair of distinct . Now, is this enough? That is to say, is every faithful convex space embeddable in a vector space ?","X X \times X \times [0, 1] \to X (a, b, t) \mapsto a \overset{t}{+} b a, b, c \in X t, t' \in (0, 1) a \overset{0}{+} b = a a \overset{1}{+} b = b a \overset{t}{+} a = a a \overset{t}{+} (b \overset{t'}{+} c) = (a \overset{\frac{t(1-t')}{1-tt'}}{+} b) \overset{tt'}{+} c a \overset{t}{+} b = b \overset{1-t}{+} a X V \mathbb{R} X \subseteq V a \overset{t}{+} b = (1-t)a + tb a, b \in X t \in [0, 1] X \{x, y\} a \overset{t}{+} b = \begin{cases}
x & \text{if (a = b = x) or (a = x and t = 0) or (b = x and t = 1)},\\
y & \text{otherwise.}
\end{cases} X \frac{1}{2}x + \frac{1}{2}y = x \overset{\frac{1}{2}}{+} y = y x = y X t \mapsto a \overset{t}{+} b a, b \in X","['linear-algebra', 'abstract-algebra', 'vector-spaces', 'convex-analysis']"
1,"$A,B$ are real matrices, similiar in complex field, then they are similar in the real field.","are real matrices, similiar in complex field, then they are similar in the real field.","A,B","$A,B$ are real matrices, similiar in complex field, then they are similar in the real field. This is well-known. My problem is that if we know there exists an invertible complex matrix $C$ such that $C^{-1}AC=B$ , can we use $A,B,C$ to find a real matrix $D$ such that $D^{-1}AD=B$ . Let $C=C_1+C_2i$ , where $C_1$ and $C_2$ are real matrices, then $AC_1=C_1B$ and $AC_2=C_2B$ . What next?","are real matrices, similiar in complex field, then they are similar in the real field. This is well-known. My problem is that if we know there exists an invertible complex matrix such that , can we use to find a real matrix such that . Let , where and are real matrices, then and . What next?","A,B C C^{-1}AC=B A,B,C D D^{-1}AD=B C=C_1+C_2i C_1 C_2 AC_1=C_1B AC_2=C_2B","['linear-algebra', 'matrices']"
2,Smallest eigenvalue of a nearest neighbor matrix in $2$ dimensions.,Smallest eigenvalue of a nearest neighbor matrix in  dimensions.,2,"Consider a 2D square lattice with $n \times n$ lattice sites. A matrix $M_n$ of size $n^2 \times n^2$ is constructed by setting $M_{ij} = u$ (where $0 \leq u \leq 1$ ) if sites $i$ and $j$ are nearest neighbors, and all the diagonal elements $M_{ii} = 1$ . For example, with $2 \times 2$ lattice sites, we have $$M_2 = \begin{pmatrix} 1 & u & u & 0 \\ u & 1 & 0 & u \\ u & 0 & 1 & 0 \\ 0 & u & u & 1 \end{pmatrix}$$ (i.e. lattice 1 has nearest neighbors 2,3 and lattice 2 has nearest neighbors 1,4 etc...). The smallest eigenvalue of $M_2$ is $\lambda_\text{min}^{(2)} = 1-2u$ , for $M_3$ it is $\lambda_\text{min}^{(3)} = 1-2\sqrt{2}u$ , for $M_4$ it is $\lambda_\text{min}^{(4)} = 1-(1+\sqrt{5})u$ . Numerically I seem to get $\lambda_{\text{min}}^{(N)} \to 1-4u$ as $N \to \infty$ , but I am not sure how to prove it. Using the Gershgorin circle theorem, I am able to get the bound $\lambda_{\text{min}}^{(N)} \geq 1-4u$ so it seems like the matrix here saturates the lower bound. Is there a way to prove this?","Consider a 2D square lattice with lattice sites. A matrix of size is constructed by setting (where ) if sites and are nearest neighbors, and all the diagonal elements . For example, with lattice sites, we have (i.e. lattice 1 has nearest neighbors 2,3 and lattice 2 has nearest neighbors 1,4 etc...). The smallest eigenvalue of is , for it is , for it is . Numerically I seem to get as , but I am not sure how to prove it. Using the Gershgorin circle theorem, I am able to get the bound so it seems like the matrix here saturates the lower bound. Is there a way to prove this?",n \times n M_n n^2 \times n^2 M_{ij} = u 0 \leq u \leq 1 i j M_{ii} = 1 2 \times 2 M_2 = \begin{pmatrix} 1 & u & u & 0 \\ u & 1 & 0 & u \\ u & 0 & 1 & 0 \\ 0 & u & u & 1 \end{pmatrix} M_2 \lambda_\text{min}^{(2)} = 1-2u M_3 \lambda_\text{min}^{(3)} = 1-2\sqrt{2}u M_4 \lambda_\text{min}^{(4)} = 1-(1+\sqrt{5})u \lambda_{\text{min}}^{(N)} \to 1-4u N \to \infty \lambda_{\text{min}}^{(N)} \geq 1-4u,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
3,How do I get to this formula for the area of a triangle,How do I get to this formula for the area of a triangle,,"I am new here and new student to geometry. In my geometry skript there is a task: Show that the formula for the area $F$ of a triangle with sidelengths $a,b,c $ is given by $$F^2 = - \frac{1}{16} \det \begin{pmatrix} 0 & c^2 & b^2 & 1 \\ c^2 & 0 & a^2 & 1 \\ b^2 & a^2 & 0 & 1 \\ 1& 1& 1& 0 \end{pmatrix}$$ This drives me crazy because I cant see how to get there. I do know how to get to $F= \frac12 \det \begin{pmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \end{pmatrix} $ and $ F = \frac12 \det \begin{pmatrix} 1 & 1 & 1 \\ a & b & c \end{pmatrix} $ I also thought about using Heron's formula for the square of a triangle: $ F^2 = s(s-a)(s-b)(s-c) $ where $s= \frac{a+b+c}{2} $ I did multiplied it all out and can't seem to find a pattern.. Hope this question is appropriate. Maybe here is someone who has an idea :)",I am new here and new student to geometry. In my geometry skript there is a task: Show that the formula for the area of a triangle with sidelengths is given by This drives me crazy because I cant see how to get there. I do know how to get to and I also thought about using Heron's formula for the square of a triangle: where I did multiplied it all out and can't seem to find a pattern.. Hope this question is appropriate. Maybe here is someone who has an idea :),"F a,b,c  F^2 = - \frac{1}{16} \det \begin{pmatrix} 0 & c^2 & b^2 & 1 \\ c^2 & 0 & a^2 & 1 \\ b^2 & a^2 & 0 & 1 \\ 1& 1& 1& 0 \end{pmatrix} F= \frac12 \det \begin{pmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1 \end{pmatrix}   F = \frac12 \det \begin{pmatrix} 1 & 1 & 1 \\ a & b & c \end{pmatrix}   F^2 = s(s-a)(s-b)(s-c)  s= \frac{a+b+c}{2} ","['linear-algebra', 'geometry']"
4,Is “defective eigenvalue“ a common term?,Is “defective eigenvalue“ a common term?,,"Is defective eigenvalue an acceptable term that is used often? By a defective eigenvalue, I mean an eigenvalue whose geometric multiplicity is strictly less than its algebraic multiplicity. I have seen it used occasionally but not enough to be sure that it is an “official” terminology in the sense that defective matrix is definitely an official terminology.","Is defective eigenvalue an acceptable term that is used often? By a defective eigenvalue, I mean an eigenvalue whose geometric multiplicity is strictly less than its algebraic multiplicity. I have seen it used occasionally but not enough to be sure that it is an “official” terminology in the sense that defective matrix is definitely an official terminology.",,"['linear-algebra', 'terminology']"
5,Solve $A=\text{adj}(A)$ in $\mathbf{M}_n(\mathbb{R})$.,Solve  in .,A=\text{adj}(A) \mathbf{M}_n(\mathbb{R}),"Solve the equation $A=\text{adj}(A)$ in $\mathbf{M}_n(\mathbb{R})$ . Where $\text{adj}(A)$ is the adjugate of $A$ (tranpose of cofactor matrix). Here is what I know: $A^2=\det(A)I_n$ $\text{rank(A)} \lt n-1 \implies \text{rank}(\text{adj}(A)) = 0$ Here is what I have done so far: We have 3 cases: If $det(A) = 0$ Then $A^2=0$ , thus $\text{rank}(A)\leq2-1=1$ . This implies $\text{rank(A)} = \text{rank}(\text{adj}(A)) \leq 1$ So $\text{rank}(A) = 0$ , that is, $A=0$ If $det(A) > 0$ Then $\left(X-\sqrt{\det A}\right)\left(X+\sqrt{\det A}\right)$ has $A$ as a root. Thus $A$ is diagonalizable (since its minimal polynomial has simple roots) and its eigenvalues are a subset of $\{\sqrt{\det A}, -\sqrt{\det A} \}$ . Thus $A=P^{-1} \begin{bmatrix} \pm\sqrt{\det A} & &\\ & \ddots &\\ & & \pm\sqrt{\det A} \end{bmatrix} P$ Which we can verify as a solution to our equation. If $det(A) < 0$ Then $A^2=\det(A)I_n$ . Thus $B^2=-I_n$ with $B=\frac{1}{\sqrt{-\det A}}A$ How do I solve for $B$ ?","Solve the equation in . Where is the adjugate of (tranpose of cofactor matrix). Here is what I know: Here is what I have done so far: We have 3 cases: If Then , thus . This implies So , that is, If Then has as a root. Thus is diagonalizable (since its minimal polynomial has simple roots) and its eigenvalues are a subset of . Thus Which we can verify as a solution to our equation. If Then . Thus with How do I solve for ?","A=\text{adj}(A) \mathbf{M}_n(\mathbb{R}) \text{adj}(A) A A^2=\det(A)I_n \text{rank(A)} \lt n-1 \implies \text{rank}(\text{adj}(A)) = 0 det(A) = 0 A^2=0 \text{rank}(A)\leq2-1=1 \text{rank(A)} = \text{rank}(\text{adj}(A)) \leq 1 \text{rank}(A) = 0 A=0 det(A) > 0 \left(X-\sqrt{\det A}\right)\left(X+\sqrt{\det A}\right) A A \{\sqrt{\det A}, -\sqrt{\det A} \} A=P^{-1}
\begin{bmatrix}
\pm\sqrt{\det A} & &\\
& \ddots &\\
& & \pm\sqrt{\det A}
\end{bmatrix}
P det(A) < 0 A^2=\det(A)I_n B^2=-I_n B=\frac{1}{\sqrt{-\det A}}A B","['linear-algebra', 'matrices', 'determinant']"
6,When is the orbit of a vector spanning the whole space?,When is the orbit of a vector spanning the whole space?,,"Let $\rho:G\to\mathrm{GL}(V)$ be a linear representation of a finite group $G$ over a complex vector space $V$ . One may ask whether there is a point $v\in V$ so that the orbit $\rho(G)v$ spans all of $V$ . Apprently there is an answer in terms of characters: If $\chi$ is the character of $\rho$ and $\psi_1,...,\psi_m$ is an enumeration of the irreducible characters of $G$ , then such a point exists if and only if $\langle \chi,\psi_i\rangle \le \psi_i(1)$ for all $i\in\{1,...,m\}$ . I thought about this and found an involved topological argument. But this seems like a basic and well-known fact. So I wonder, what is the easiest proof that you know of? And do we need to assume some machinery or does it quickly follow from first principles?","Let be a linear representation of a finite group over a complex vector space . One may ask whether there is a point so that the orbit spans all of . Apprently there is an answer in terms of characters: If is the character of and is an enumeration of the irreducible characters of , then such a point exists if and only if for all . I thought about this and found an involved topological argument. But this seems like a basic and well-known fact. So I wonder, what is the easiest proof that you know of? And do we need to assume some machinery or does it quickly follow from first principles?","\rho:G\to\mathrm{GL}(V) G V v\in V \rho(G)v V \chi \rho \psi_1,...,\psi_m G \langle \chi,\psi_i\rangle \le \psi_i(1) i\in\{1,...,m\}","['linear-algebra', 'vector-spaces', 'linear-transformations', 'representation-theory', 'characters']"
7,Find the determinant of a particular matrix without a calculator.,Find the determinant of a particular matrix without a calculator.,,"I would like to know the trick to solving this determinant problem. Find the determinant of the matrix $$ \begin{bmatrix} 283&5&\pi&347.86\times10^{15^{83}}\\ 3136 & 56 & 5 & \cos(2.7402)\\ 6776 & 121 & 11 & 5\\ 2464 & 44 & 4 & 2 \end{bmatrix} $$ Hint: do not use a calculator. This is a problem here (section 3.2). According to it, the answer is 6. I have no idea how to do this. Obviously some of the entries in this matrix are red herrings, and I need to perform some trick. But the only manipulations I know towards computing the determinant is adding one row to another and multiplying a row by a scalar, and nothing in this matrix suggests that I do that. I've tried decomposing the matrix as follows, $$ \begin{bmatrix} 283 & 5 & \pi & 347.86\times10^{15^{83}}\\ 56^2 & 56 & 5 & \cos(2.7402)\\ 11^2\cdot 56 & 11^2 & 11 & 5\\ 4\cdot11\cdot 56 & 4\cdot 11 & 4 & 2 \end{bmatrix},$$ though I'm not sure what I accomplished (I could also note that $56 = 14\cdot 4, 4 = 2^2$ etc. and do more decomposing). The problem is not all entries in any column/row are nice, and the nicest looking ones are the first and second column (or third and fourth row). Maybe the matrix is the product of two nice ones?","I would like to know the trick to solving this determinant problem. Find the determinant of the matrix Hint: do not use a calculator. This is a problem here (section 3.2). According to it, the answer is 6. I have no idea how to do this. Obviously some of the entries in this matrix are red herrings, and I need to perform some trick. But the only manipulations I know towards computing the determinant is adding one row to another and multiplying a row by a scalar, and nothing in this matrix suggests that I do that. I've tried decomposing the matrix as follows, though I'm not sure what I accomplished (I could also note that etc. and do more decomposing). The problem is not all entries in any column/row are nice, and the nicest looking ones are the first and second column (or third and fourth row). Maybe the matrix is the product of two nice ones?","
\begin{bmatrix}
283&5&\pi&347.86\times10^{15^{83}}\\
3136 & 56 & 5 & \cos(2.7402)\\
6776 & 121 & 11 & 5\\
2464 & 44 & 4 & 2
\end{bmatrix}
 
\begin{bmatrix}
283 & 5 & \pi & 347.86\times10^{15^{83}}\\
56^2 & 56 & 5 & \cos(2.7402)\\
11^2\cdot 56 & 11^2 & 11 & 5\\
4\cdot11\cdot 56 & 4\cdot 11 & 4 & 2
\end{bmatrix}, 56 = 14\cdot 4, 4 = 2^2","['linear-algebra', 'determinant']"
8,"Objective function of complex variables, with real constraints","Objective function of complex variables, with real constraints",,"I have an objective function $L:\mathbb{C}^M \to \mathbb{R}$ that can be written: $$ L(x) = x^H A x - b^H x - x^H b + d $$ where superscript $H$ denotes the conjugate transpose. $A \in \mathbb{C}^M$ is a hermitian matrix of size $M$ (so $A^H = A$ ), $b$ is a complex vector $b \in \mathbb{C}^M$ , and $d$ is a real constant. I believe $L(x)$ is known to be convex, as it can be equivalently written as the squared euclidean norm of an affine function of $x$ (i.e. $L(x) = (Fx - g)^H (Fx - g)$ ). I'm looking to solve the following minimization problem, with or without the added constraint that the vector $x$ is real. Formally: $$ \text{minimize} \; L(x) $$ $$ \text{subject to} \; \text{Im}(x) = 0 $$ In other words, I want to find the real vector $x \in \mathbb{R}^M$ that minimizes the convex loss function, which is formally a function of complex input vector $x$ . I have a feeling this should be straightforward but I'm not sure exactly how to go about doing so. In the case that the ""reality"" constraint is quite non-trivial, it can be discarded. Discarding the constraint, I believe Wirtinger calculus can be used to solve this. Unfortunately, I'm not 100% clear on how to apply Wirtinger derivatives to actually obtain first-order conditions for a minima. For example, to solve the unconstrained problem, is it valid to write: $$ L(x) = x^H A x - 2 \text{Re}(b^H x) + d $$ $$ \nabla{L}(\hat{x}) = A\hat{x} - 2\text{Re}(b) = 0$$ $$ A \hat{x} = 2\text{Re}(b) $$ Thus obtaining the solution $\hat{x} = A^{-1} 2\text{Re}(b)$ ?","I have an objective function that can be written: where superscript denotes the conjugate transpose. is a hermitian matrix of size (so ), is a complex vector , and is a real constant. I believe is known to be convex, as it can be equivalently written as the squared euclidean norm of an affine function of (i.e. ). I'm looking to solve the following minimization problem, with or without the added constraint that the vector is real. Formally: In other words, I want to find the real vector that minimizes the convex loss function, which is formally a function of complex input vector . I have a feeling this should be straightforward but I'm not sure exactly how to go about doing so. In the case that the ""reality"" constraint is quite non-trivial, it can be discarded. Discarding the constraint, I believe Wirtinger calculus can be used to solve this. Unfortunately, I'm not 100% clear on how to apply Wirtinger derivatives to actually obtain first-order conditions for a minima. For example, to solve the unconstrained problem, is it valid to write: Thus obtaining the solution ?",L:\mathbb{C}^M \to \mathbb{R}  L(x) = x^H A x - b^H x - x^H b + d  H A \in \mathbb{C}^M M A^H = A b b \in \mathbb{C}^M d L(x) x L(x) = (Fx - g)^H (Fx - g) x  \text{minimize} \; L(x)   \text{subject to} \; \text{Im}(x) = 0  x \in \mathbb{R}^M x  L(x) = x^H A x - 2 \text{Re}(b^H x) + d   \nabla{L}(\hat{x}) = A\hat{x} - 2\text{Re}(b) = 0  A \hat{x} = 2\text{Re}(b)  \hat{x} = A^{-1} 2\text{Re}(b),"['linear-algebra', 'complex-analysis', 'convex-optimization', 'numerical-linear-algebra', 'several-complex-variables']"
9,Probability of sampling linearly independent vectors,Probability of sampling linearly independent vectors,,"Let $q$ be a prime so that $\mathbb{Z}_q$ is a field. I would like to sample $n$ vectors independently and uniformly from $\mathbb{Z}^n_q$ , to get a set $V = \{ v_i \}_{i=1}^n$ . What is the probability that $V$ is a basis for $\mathbb{Z}^n_q$ ? I had the following proof idea, by induction. The problem reduces to what is the probability of sampling $n$ linearly independent vectors, and as such, we proceed by induction on the number of sampled vectors $k$ . For $k = 1$ , we only require that we are not sampling the zero vector, and as such we have that $\Pr[\{v_1\} \text{is LI} ] = 1 - q^{-n}$ . Now let us suppose that we have $k$ linearly independent vectors $\{ v_i \}_{i=1}^k$ . We sample a new vector $v_{k+1}$ . In order for it to be linearly independent, it must be that it is not a linear combination of the others, and as such there are at most $q^k$ possible 'forbidden' values. Namely these correspond to the possible coefficients $\alpha_i$ of the   linear combinations $\sum_{i=1}^k \alpha_i v_i$ . So we have that $\Pr[\{v_i\}_{i=1}^{k+1} \text{is LI} \, | \, \{v_i\}_{i=1}^{k} \text{is LI}] \geq 1 - q^{k - n}$ . As such we should have that $$\Pr[\{v_i\}_{i=1}^n \text{is LI}] = \Pr[\{v_i\}_{i=1}^n \text{is LI} | \{v_i\}_{i=1}^{n-1} \text{is LI}] \Pr[\{v_i\}_{i=1}^{n-1} \text{is LI}] \geq \prod_{i=1}^n (1 - q^{i - n})$$ Do you think this analysis is correct? What I am also interested in is the following generalizations, on which I, unfortunately, had less success. Suppose we set $n=3k$ and that instead of sampling the vectors independently we sample three matrices $M, M_0, M_1 \in \mathbb{Z}^{3k\times k}_q$ such that each matrix is of full rank . What would be an upper bound on the probability of the columns of said matrices spanning $\mathbb{Z}_q^{3k}$ ? I have found in the literature (Lemma 8) a claim that it should be at least $1 - \frac{2k}{q}$ but have not found any good argument Let us relax the very first statement, and admit that $q$ could be composite. Does this change the argument significantly? I am interested in this result both in the vector case and in the three matrices sample case. Thank you, I hope the formatting is clear enough :) For context, I am investigating whether the construction in the linked paper would hold in nonprime order groups.","Let be a prime so that is a field. I would like to sample vectors independently and uniformly from , to get a set . What is the probability that is a basis for ? I had the following proof idea, by induction. The problem reduces to what is the probability of sampling linearly independent vectors, and as such, we proceed by induction on the number of sampled vectors . For , we only require that we are not sampling the zero vector, and as such we have that . Now let us suppose that we have linearly independent vectors . We sample a new vector . In order for it to be linearly independent, it must be that it is not a linear combination of the others, and as such there are at most possible 'forbidden' values. Namely these correspond to the possible coefficients of the   linear combinations . So we have that . As such we should have that Do you think this analysis is correct? What I am also interested in is the following generalizations, on which I, unfortunately, had less success. Suppose we set and that instead of sampling the vectors independently we sample three matrices such that each matrix is of full rank . What would be an upper bound on the probability of the columns of said matrices spanning ? I have found in the literature (Lemma 8) a claim that it should be at least but have not found any good argument Let us relax the very first statement, and admit that could be composite. Does this change the argument significantly? I am interested in this result both in the vector case and in the three matrices sample case. Thank you, I hope the formatting is clear enough :) For context, I am investigating whether the construction in the linked paper would hold in nonprime order groups.","q \mathbb{Z}_q n \mathbb{Z}^n_q V = \{ v_i \}_{i=1}^n V \mathbb{Z}^n_q n k k = 1 \Pr[\{v_1\} \text{is LI} ] = 1 - q^{-n} k \{ v_i \}_{i=1}^k v_{k+1} q^k \alpha_i \sum_{i=1}^k \alpha_i v_i \Pr[\{v_i\}_{i=1}^{k+1} \text{is LI} \, | \, \{v_i\}_{i=1}^{k} \text{is LI}] \geq 1 - q^{k - n} \Pr[\{v_i\}_{i=1}^n \text{is LI}] = \Pr[\{v_i\}_{i=1}^n \text{is LI} | \{v_i\}_{i=1}^{n-1} \text{is LI}] \Pr[\{v_i\}_{i=1}^{n-1} \text{is LI}] \geq \prod_{i=1}^n (1 - q^{i - n}) n=3k M, M_0, M_1 \in \mathbb{Z}^{3k\times k}_q \mathbb{Z}_q^{3k} 1 - \frac{2k}{q} q","['linear-algebra', 'probability', 'probability-distributions']"
10,A problem about positive definite matrices,A problem about positive definite matrices,,"Given $A\in\mathbb{R}^{n\times n}$ , show that all eigenvalues of A has negative real part if and only if for each positive definite matrix $C\in\mathbb{R}^{n\times n}$ , there exists an unique positive definite matrix $B\in\mathbb{R}^{n\times n}$ which satisfies $BA+A^TB=-C$ I thought that since $B$ and $C$ are both positive definite, then they can be simultaneously diagonalized, then I cannot go any further. Can anyone help me? Many thanks","Given , show that all eigenvalues of A has negative real part if and only if for each positive definite matrix , there exists an unique positive definite matrix which satisfies I thought that since and are both positive definite, then they can be simultaneously diagonalized, then I cannot go any further. Can anyone help me? Many thanks",A\in\mathbb{R}^{n\times n} C\in\mathbb{R}^{n\times n} B\in\mathbb{R}^{n\times n} BA+A^TB=-C B C,"['linear-algebra', 'matrices', 'positive-definite', 'linear-control', 'hurwitz-matrices']"
11,"The given matrix has three linearly independent eigenvectors, then $x+y=0$.","The given matrix has three linearly independent eigenvectors, then .",x+y=0,"The question asked is  :: If the matrix $$ A=\left(\begin{array}{lll} 0 & 0 & 1 \\ x & 1 & y \\ 1 & 0 & 0 \end{array}\right) $$ has three linearly independent eigenvectors, then show that $x+y=0$ . solving for eigenvalues from the characteristic polynomial: $$\left|\begin{matrix} 0-\lambda & 0 & 1 \\ x & 1-\lambda & y \\ 1 & 0 & 0-\lambda \end{matrix}\right| =-λ^3+λ^2+λ-1$$ $$=-(λ-1)*(λ^2-1)=-(λ-1)*(λ-1)=-(λ-1)^2*(λ+1)$$ So eigenvalues are $λ_1=1$ and $λ_2=-1$ , Independent of the values of $x$ and $y$ . Now solving for eigenvectors I got $\left(\begin{matrix} 0 \\ 1 \\ 0 \end{matrix}\right)$ and $\left(\begin{matrix} -1 \\ \frac{x-y}{2} \\ 1 \end{matrix}\right)$ From here how to show that if there are three linearly independent eigenvectors, then show that $x+y=0$ .","The question asked is  :: If the matrix has three linearly independent eigenvectors, then show that . solving for eigenvalues from the characteristic polynomial: So eigenvalues are and , Independent of the values of and . Now solving for eigenvectors I got and From here how to show that if there are three linearly independent eigenvectors, then show that .","
A=\left(\begin{array}{lll}
0 & 0 & 1 \\
x & 1 & y \\
1 & 0 & 0
\end{array}\right)
 x+y=0 \left|\begin{matrix}
0-\lambda & 0 & 1 \\
x & 1-\lambda & y \\
1 & 0 & 0-\lambda
\end{matrix}\right| =-λ^3+λ^2+λ-1 =-(λ-1)*(λ^2-1)=-(λ-1)*(λ-1)=-(λ-1)^2*(λ+1) λ_1=1 λ_2=-1 x y \left(\begin{matrix}
0 \\
1 \\
0
\end{matrix}\right) \left(\begin{matrix}
-1 \\
\frac{x-y}{2} \\
1
\end{matrix}\right) x+y=0","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
12,Algebra - Steps to rearrange a simple equation,Algebra - Steps to rearrange a simple equation,,"I have the following equation: $$\frac{1000−6n}{3+10n}=k$$ I know it can be rearranged into this, but I do not quite understand how: $$(5k+3)(10n+3)=5009$$ I know the first thing I can do is multiple both sides by $3 + 10n$ to get: $$1000 - 6n = 3k + 10kn$$ Then I can add $6n$ to both sides to get: $$ 1000 = 3k + 10kn + 6n$$ This is where my math knowledge now dies. I am unsure of what the next steps are. Could someone please explain in an idiot proof way of what needs to be done next to get the answer I am looking for?","I have the following equation: I know it can be rearranged into this, but I do not quite understand how: I know the first thing I can do is multiple both sides by to get: Then I can add to both sides to get: This is where my math knowledge now dies. I am unsure of what the next steps are. Could someone please explain in an idiot proof way of what needs to be done next to get the answer I am looking for?",\frac{1000−6n}{3+10n}=k (5k+3)(10n+3)=5009 3 + 10n 1000 - 6n = 3k + 10kn 6n  1000 = 3k + 10kn + 6n,['linear-algebra']
13,Linear transformation bounded iff its kernel is closed in infinite-dimensional Banach spaces,Linear transformation bounded iff its kernel is closed in infinite-dimensional Banach spaces,,"I am working on Problem 8, Chapter 6, in Luenberger's Optimization by Vector Space Methods . It states: ""Show that a linear transformation mapping one Banach space into another is bounded if and only if its nullspace is closed."" I am having a bit of trouble with the converse. In particular, if we let $f:X \rightarrow Y$ be a linear transformation, Luenberger doesn't assume that either $X$ or $Y$ be finite-dimensional. Do you have any idea on how to proceed? I have thought (without success) of considering the quotient space $\hat{X}/\ker f$","I am working on Problem 8, Chapter 6, in Luenberger's Optimization by Vector Space Methods . It states: ""Show that a linear transformation mapping one Banach space into another is bounded if and only if its nullspace is closed."" I am having a bit of trouble with the converse. In particular, if we let be a linear transformation, Luenberger doesn't assume that either or be finite-dimensional. Do you have any idea on how to proceed? I have thought (without success) of considering the quotient space",f:X \rightarrow Y X Y \hat{X}/\ker f,"['linear-algebra', 'general-topology', 'functional-analysis']"
14,$Q=(P+\frac12I)$ is invertible where $P$ is a square matrix with integral entries,is invertible where  is a square matrix with integral entries,Q=(P+\frac12I) P,"Let $P $ be $n×n$ matrix with integral entries and $Q=P+\frac12I$ where I denote the $n×n$ identity matrix . Then $Q$ is invertible. My attempt to prove: $Q$ is invertible iff $0$ is not an eigen value. If possible, let $0$ be eigen value. Then $\exists$ $v\in R^n$ s.t $Qv=0$ Then $Pv=-\frac12v$ . So $-\frac12$ is an eigen value of $P$ with eigen vector $v$ . So $(2x+1)$ is a divisor of the characteristics polynomail $p(x)$ (say) of $P$ which is of course a monic polynomial with integer coefficients. So $\exists g(x)$ s.t $(2x+1)g(x)=p(x)$ . I am not sure how to proceed from here because $g(x)$ may not itself be monic but the product is monic. . This may be  elementary question but please guide me if I am wrong or give a better proof.A lot of thanks for your time!!","Let be matrix with integral entries and where I denote the identity matrix . Then is invertible. My attempt to prove: is invertible iff is not an eigen value. If possible, let be eigen value. Then s.t Then . So is an eigen value of with eigen vector . So is a divisor of the characteristics polynomail (say) of which is of course a monic polynomial with integer coefficients. So s.t . I am not sure how to proceed from here because may not itself be monic but the product is monic. . This may be  elementary question but please guide me if I am wrong or give a better proof.A lot of thanks for your time!!",P  n×n Q=P+\frac12I n×n Q Q 0 0 \exists v\in R^n Qv=0 Pv=-\frac12v -\frac12 P v (2x+1) p(x) P \exists g(x) (2x+1)g(x)=p(x) g(x),"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
15,Index of a Nilpotent matrix,Index of a Nilpotent matrix,,I was wondering why there can't be a  nilpotent matrix of index greater than its no.  of rows. Like why there does not exist a nilpotent matrix of index 3 in $M_{2×2}(F)$ When I look up on the internet it says that it is related to ring theory and an element being nilpotent is $x^n = 0$ . I have a very basic understanding of rings but I know groups so I thought for $x^n=0$ n must be less than or equal to the order of group  and thus this thing. Am I right?,I was wondering why there can't be a  nilpotent matrix of index greater than its no.  of rows. Like why there does not exist a nilpotent matrix of index 3 in When I look up on the internet it says that it is related to ring theory and an element being nilpotent is . I have a very basic understanding of rings but I know groups so I thought for n must be less than or equal to the order of group  and thus this thing. Am I right?,M_{2×2}(F) x^n = 0 x^n=0,"['linear-algebra', 'matrices', 'ring-theory', 'nilpotence', 'matrix-analysis']"
16,An elegant proof for a claim about orthogonal and positive-definite matrices,An elegant proof for a claim about orthogonal and positive-definite matrices,,"Let $P$ be a real $n \times n$ symmetric positive-semidefinite matrix. Suppose that $\langle O,P \rangle = \langle I,P \rangle$ for some orthogonal matrix $O$ . Here $\langle , \rangle$ is the Euclidean (Frobenius) inner product. The assumption is equivalent to $\text{tr}(O^TP)=\text{tr}(P)$ . Then $OP=P$ holds. Since we can replace $O$ with $O^T$ the claim is equivalent to the seemingly surprising assertion $\text{tr}(OP)=\text{tr}(P) \implies OP=P$ . I have a proof, and I wonder whether there are other shorter proofs.   Is there a proof which does not require diagonalizing $P$ ? Here is my proof: By orthogonally diagonalizing $P$ , we can reduce to the case where $P=\Sigma$ is diagonal, with non-negative entries $\sigma_i$ . The assumption implies $\sum_i \sigma_i O_{ii}=\sum_i \sigma_i$ where the sums run over all the indices $i$ where $\sigma_i \neq 0$ . Since $|O_{ii}| \le 1$ this forces $O_{ii}=1$ for all these $i$ . A direct check then implies that $O\Sigma=\Sigma$ : Indeed, $O\Sigma=\Sigma$ is equivalent to $O_{ij}\sigma_j=\Sigma_{ij}$ . If $\sigma_j=0$ , then $\Sigma_{ij}=0$ so equality holds. If $\sigma_j \neq 0$ , then $O_{jj}=1$ so equality holds for $i=j$ . For $i \neq j$ , $\Sigma_{ij}=0$ (since $\Sigma$ is diagonal) and , and $O_{ij}=0$ , since whenever $O_{jj}=1$ , all the rest of the entries in the $j$ -th column of $O$ are zero. (Since $O$ is orthogonal).","Let be a real symmetric positive-semidefinite matrix. Suppose that for some orthogonal matrix . Here is the Euclidean (Frobenius) inner product. The assumption is equivalent to . Then holds. Since we can replace with the claim is equivalent to the seemingly surprising assertion . I have a proof, and I wonder whether there are other shorter proofs.   Is there a proof which does not require diagonalizing ? Here is my proof: By orthogonally diagonalizing , we can reduce to the case where is diagonal, with non-negative entries . The assumption implies where the sums run over all the indices where . Since this forces for all these . A direct check then implies that : Indeed, is equivalent to . If , then so equality holds. If , then so equality holds for . For , (since is diagonal) and , and , since whenever , all the rest of the entries in the -th column of are zero. (Since is orthogonal).","P n \times n \langle O,P \rangle = \langle I,P \rangle O \langle , \rangle \text{tr}(O^TP)=\text{tr}(P) OP=P O O^T \text{tr}(OP)=\text{tr}(P) \implies OP=P P P P=\Sigma \sigma_i \sum_i \sigma_i O_{ii}=\sum_i \sigma_i i \sigma_i \neq 0 |O_{ii}| \le 1 O_{ii}=1 i O\Sigma=\Sigma O\Sigma=\Sigma O_{ij}\sigma_j=\Sigma_{ij} \sigma_j=0 \Sigma_{ij}=0 \sigma_j \neq 0 O_{jj}=1 i=j i \neq j \Sigma_{ij}=0 \Sigma O_{ij}=0 O_{jj}=1 j O O","['linear-algebra', 'matrices', 'matrix-calculus', 'positive-semidefinite', 'orthogonal-matrices']"
17,Determining the relation between two matrices satisfying given conditions.,Determining the relation between two matrices satisfying given conditions.,,"Suppose $Ax = b$ and $Cx = b$ have the same (complete) solutions for every b (A, B, C, x are suitable matrices). Is it true that A = C? My attempt: I could just get $(A-C)x=0$ and I am unable to draw any further conclusion from this.","Suppose and have the same (complete) solutions for every b (A, B, C, x are suitable matrices). Is it true that A = C? My attempt: I could just get and I am unable to draw any further conclusion from this.",Ax = b Cx = b (A-C)x=0,"['linear-algebra', 'matrices']"
18,"What can be said about the theory of inner product spaces using only the group structure and the mapping $(u, v, w) \mapsto \langle u, v \rangle w$?",What can be said about the theory of inner product spaces using only the group structure and the mapping ?,"(u, v, w) \mapsto \langle u, v \rangle w","Consider the language $(+, -, 0, \langle - , - \rangle -)$ of abelian groups, with an additional ternary operation $\langle - , - \rangle -$ , which is intended to represent the operation of taking vectors $u, v, w$ of an inner product space and returning $\langle u, v \rangle w$ . Let $T$ be the theory consisting of all first-order sentences written in this language which are satisfied by all real inner product spaces (or maybe even ""rational inner product spaces""?). My broad question is: what can be said about $T$ ? Is it interesting, has it been studied? Two concrete questions are: Is $T$ finitely axiomatizable? What are interesting structures that model $T$ , apart from inner product spaces? (This question does not have an application in mind, it is plain curiosity.)","Consider the language of abelian groups, with an additional ternary operation , which is intended to represent the operation of taking vectors of an inner product space and returning . Let be the theory consisting of all first-order sentences written in this language which are satisfied by all real inner product spaces (or maybe even ""rational inner product spaces""?). My broad question is: what can be said about ? Is it interesting, has it been studied? Two concrete questions are: Is finitely axiomatizable? What are interesting structures that model , apart from inner product spaces? (This question does not have an application in mind, it is plain curiosity.)","(+, -, 0, \langle - , - \rangle -) \langle - , - \rangle - u, v, w \langle u, v \rangle w T T T T","['linear-algebra', 'logic', 'model-theory', 'universal-algebra']"
19,Adjoint of a nonlinear operator?,Adjoint of a nonlinear operator?,,"For any linear operator $A$ , the adjoint $A^*$ is defined as a linear operator that satisfies $$\langle v, Au\rangle = \langle A^*v, u\rangle$$ Moreover, one has that $(A^{*})^{*} = A$ (proof here ). When things are nonlinear, this seems to result in weird conclusions. Let $F$ be an operator that is not linear. Can one define the adjoint of $F$ and if yes how? If one goes along with the usual definition, then we see that $\begin{align} \langle x, F(y+z)\rangle &= \langle F^*x, y+z\rangle \\ &= \langle F^*x, y\rangle +\langle F^*x, z\rangle \\ &= \langle x, F(y) + F(z)\rangle \end{align}$ I'm not sure if this explicitly contradicts the assumption that $F$ is nonlinear but it seems to? I did assume that $(F^*)^* = F$ but the linked proof above doesn't seem to require linearity so this seems okay. In general, is it possible to define the adjoint in the sense of $\langle v, Fu\rangle = \langle F^*v, u\rangle$ for nonlinear $F$ ?","For any linear operator , the adjoint is defined as a linear operator that satisfies Moreover, one has that (proof here ). When things are nonlinear, this seems to result in weird conclusions. Let be an operator that is not linear. Can one define the adjoint of and if yes how? If one goes along with the usual definition, then we see that I'm not sure if this explicitly contradicts the assumption that is nonlinear but it seems to? I did assume that but the linked proof above doesn't seem to require linearity so this seems okay. In general, is it possible to define the adjoint in the sense of for nonlinear ?","A A^* \langle v, Au\rangle = \langle A^*v, u\rangle (A^{*})^{*} = A F F \begin{align}
\langle x, F(y+z)\rangle &= \langle F^*x, y+z\rangle \\
&= \langle F^*x, y\rangle +\langle F^*x, z\rangle \\
&= \langle x, F(y) + F(z)\rangle
\end{align} F (F^*)^* = F \langle v, Fu\rangle = \langle F^*v, u\rangle F","['linear-algebra', 'adjoint-operators']"
20,Rank of circulant matrix with $k$ ones per row,Rank of circulant matrix with  ones per row,k,"Consider the $n\times n$ matrix over the field $\mathbb F_2$ formed by creating the circulant matrix of the vector consisting of $k$ ones followed by $n-k$ zeroes. E.g., for $n=4$ and $k=2$ , the resulting matrix is $$\begin{bmatrix}1 & 1 & 0 & 0\\ 0 & 1 & 1 & 0\\0 & 0 & 1 & 1\\1 & 0&0 &1\end{bmatrix}.$$ What is the rank of this matrix? I suspect the answer is $n-d+1$ , where $d=\gcd(n,k)$ . I can prove this is an upper bound on the rank, since the following vectors are linearly independent and lie in the kernel of the matrix: $$\begin{bmatrix}1 & (i\ \textrm{zeroes}) & 1 & (d-2-i\ \textrm{zeroes}) & 1 & (i\ \textrm{zeroes}) & \cdots \end{bmatrix}$$ with $d-1$ different choices for $i$ . Is there an easy way to see that these completely describe the kernel?","Consider the matrix over the field formed by creating the circulant matrix of the vector consisting of ones followed by zeroes. E.g., for and , the resulting matrix is What is the rank of this matrix? I suspect the answer is , where . I can prove this is an upper bound on the rank, since the following vectors are linearly independent and lie in the kernel of the matrix: with different choices for . Is there an easy way to see that these completely describe the kernel?","n\times n \mathbb F_2 k n-k n=4 k=2 \begin{bmatrix}1 & 1 & 0 & 0\\ 0 & 1 & 1 & 0\\0 & 0 & 1 & 1\\1 & 0&0 &1\end{bmatrix}. n-d+1 d=\gcd(n,k) \begin{bmatrix}1 & (i\ \textrm{zeroes}) & 1 & (d-2-i\ \textrm{zeroes}) & 1 & (i\ \textrm{zeroes}) & \cdots \end{bmatrix} d-1 i","['linear-algebra', 'matrices', 'finite-fields', 'matrix-rank', 'circulant-matrices']"
21,"If $A\in {\mathbf F_2}^{n\times n}$ is symmetric, its diagonal is in the span of its columns","If  is symmetric, its diagonal is in the span of its columns",A\in {\mathbf F_2}^{n\times n},"Let $A$ be an $n\times n$ symmetric matrix with entries in $\mathbf F_2$ (the field with 2 elements, also referred to as $\mathbb Z/2\mathbb Z$ ). Prove that the diagonal of $A$ is in the span of its columns. If the diagonal is only zeros the problem is trivial, but I haven't made any significant progress in the general case...","Let be an symmetric matrix with entries in (the field with 2 elements, also referred to as ). Prove that the diagonal of is in the span of its columns. If the diagonal is only zeros the problem is trivial, but I haven't made any significant progress in the general case...",A n\times n \mathbf F_2 \mathbb Z/2\mathbb Z A,"['linear-algebra', 'matrices', 'number-theory', 'symmetric-matrices']"
22,Existence of $n-1$ dimensional invariant subspace of $V$ over $\mathbb{R}$ given characteristic polynomial has a real root.,Existence of  dimensional invariant subspace of  over  given characteristic polynomial has a real root.,n-1 V \mathbb{R},"$V$ is a finite dimensional vector space over $\mathbb{R}$ with $\dim V \ge 1$ and $\phi \in L(V, V)$ is an endomorphism. Its characteristic polynomial $w_{\phi}(\lambda)$ has a real root. Prove the existence of an $n-1$ dimensional invariant subspace for $V$ . I tried to deduce something from Jordan Canonical Form but with no effect.",is a finite dimensional vector space over with and is an endomorphism. Its characteristic polynomial has a real root. Prove the existence of an dimensional invariant subspace for . I tried to deduce something from Jordan Canonical Form but with no effect.,"V \mathbb{R} \dim V \ge 1 \phi \in L(V, V) w_{\phi}(\lambda) n-1 V","['linear-algebra', 'eigenvalues-eigenvectors', 'invariant-subspace']"
23,Taylor series of functions with matrix input,Taylor series of functions with matrix input,,"Today my professor said something interesting, replace $x$ in $f(x)$ with matrix \begin{bmatrix}x&1\\0&x\end{bmatrix} Then $$f(\begin{bmatrix}x&1\\0&x\end{bmatrix}) = f(x)\begin{bmatrix}1&0\\0&1\end{bmatrix} + f^{'}(x)\begin{bmatrix}0&-1\\1&0\end{bmatrix}$$ and asked us to find the higher order terms ( $f^{''}(x), f^{'''}(x) ....$ )  and extend it to multi variable functions. I didn't understand how he came up with this. After some googling, this seems similar to matrix exponential from lie groups.","Today my professor said something interesting, replace in with matrix Then and asked us to find the higher order terms ( )  and extend it to multi variable functions. I didn't understand how he came up with this. After some googling, this seems similar to matrix exponential from lie groups.","x f(x) \begin{bmatrix}x&1\\0&x\end{bmatrix} f(\begin{bmatrix}x&1\\0&x\end{bmatrix}) = f(x)\begin{bmatrix}1&0\\0&1\end{bmatrix} + f^{'}(x)\begin{bmatrix}0&-1\\1&0\end{bmatrix} f^{''}(x), f^{'''}(x) ....","['linear-algebra', 'matrices']"
24,Is there any relation between an eigenvector of $A$ and the eigenvector of $A^T$ with the same eigenvalue?,Is there any relation between an eigenvector of  and the eigenvector of  with the same eigenvalue?,A A^T,"Let $A$ be a square matrix over $\mathbb C$ , and let $A^T$ denote its transpose. It is not hard to see that $A$ and $A^T$ have the same set of eigenvalues , so given $Ax=\lambda x$ for some vector $x\in V$ and eigenvalue $\lambda\in\mathbb C$ , we know that there must always be some other $y\in V$ such that also $$A^T y=\lambda y.$$ We also know that, if $Ax=\lambda x$ and $A^T y=\mu y$ with $\lambda\neq \mu$ , then $\langle y^*,x\rangle=0$ , where $y^*$ denotes the vector whose elements are complex conjugate of those of $y$ , as follows from $$\langle y^*,Ax\rangle=\lambda \langle y^*,x\rangle=\mu\langle y^*,x\rangle.$$ The same argument, however, does not provide any information for the case $\mu=\lambda$ . Is there relation holding in general for such a case? More precisely, given $Ax=\lambda x$ and $A^T y=\lambda y$ , is there any general relation between $x$ and $y$ ?","Let be a square matrix over , and let denote its transpose. It is not hard to see that and have the same set of eigenvalues , so given for some vector and eigenvalue , we know that there must always be some other such that also We also know that, if and with , then , where denotes the vector whose elements are complex conjugate of those of , as follows from The same argument, however, does not provide any information for the case . Is there relation holding in general for such a case? More precisely, given and , is there any general relation between and ?","A \mathbb C A^T A A^T Ax=\lambda x x\in V \lambda\in\mathbb C y\in V A^T y=\lambda y. Ax=\lambda x A^T y=\mu y \lambda\neq \mu \langle y^*,x\rangle=0 y^* y \langle y^*,Ax\rangle=\lambda \langle y^*,x\rangle=\mu\langle y^*,x\rangle. \mu=\lambda Ax=\lambda x A^T y=\lambda y x y","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'transpose']"
25,Prove there exist a non-zero vector $v$ such that $Av=v$.,Prove there exist a non-zero vector  such that .,v Av=v,"I need to prove that if $A^t=A^{-1}$ , $\det(A)>0$ for a matrix $A_{3\times3}$ then exist a vector $v$ non-zero such that $Av=v$ . I tried taking a general matrix A, $ \left( \begin{array}{cccc}  a_{11} & a_{12} & a_{13} \\   a_{21} & a_{22} & a_{23} \\   a_{31}& a_{32} & a_{33}  \end{array} \right) $ but I don't get anything, and I tried using the definition of eigenvalue $\lambda$ and eigenvector $v$ , $\{v \in V \mid Av=\lambda v\}$ , and I did not get anything important, so, any hint is very valuable, thanks!","I need to prove that if , for a matrix then exist a vector non-zero such that . I tried taking a general matrix A, but I don't get anything, and I tried using the definition of eigenvalue and eigenvector , , and I did not get anything important, so, any hint is very valuable, thanks!","A^t=A^{-1} \det(A)>0 A_{3\times3} v Av=v 
\left( \begin{array}{cccc}
 a_{11} & a_{12} & a_{13} \\ 
 a_{21} & a_{22} & a_{23} \\
  a_{31}& a_{32} & a_{33} 
\end{array} \right)  \lambda v \{v \in V \mid Av=\lambda v\}","['linear-algebra', 'eigenvalues-eigenvectors']"
26,"Groups, Rings and Fields.","Groups, Rings and Fields.",,"I am asking for the analogy behind these structures names. Why is a ""field"" called a field? Is there an analogy between a usual ring (finger ring) and a mathematical ring?","I am asking for the analogy behind these structures names. Why is a ""field"" called a field? Is there an analogy between a usual ring (finger ring) and a mathematical ring?",,"['linear-algebra', 'abstract-algebra']"
27,Every real matrix $A$ is the linear combination of $4$ orthogonal matrices,Every real matrix  is the linear combination of  orthogonal matrices,A 4,"Question: Prove that every matrix $A\in M_n(\mathbb R)$ is the linear combination of $4$ orthogonal matrices $X, Y, Z, W$ , i.e. $A=aX+bY+cZ+dW$ for some $a,b,c,d\in\mathbb R$ . This problem is taken from a forum and this is my paraphrase. It is not obviously true. But I think the proof must invoke the singular-value decomposition (SVD) of a real matrix, but it's unclear to me what the next step is. Any idea is appreciated. Many thanks.","Question: Prove that every matrix is the linear combination of orthogonal matrices , i.e. for some . This problem is taken from a forum and this is my paraphrase. It is not obviously true. But I think the proof must invoke the singular-value decomposition (SVD) of a real matrix, but it's unclear to me what the next step is. Any idea is appreciated. Many thanks.","A\in M_n(\mathbb R) 4 X, Y, Z, W A=aX+bY+cZ+dW a,b,c,d\in\mathbb R","['linear-algebra', 'svd', 'orthogonal-matrices']"
28,Weighted Least Squares,Weighted Least Squares,,"I understand the concept of least squares but I'm not able to wrap my head around weighted least squares (the matrix form). We convert $Ax = b$ to $WAx = Wb$ . What exactly happens when we multiply the equation with $W$ ? Is the column space of A modified based on the changed equations? Also how do I find this matrix $W$ , assuming I have the given data (The probability of each observation according to a textbook example ""Linear Algebra and it's Applications"" by Gilbert Strang, page 174, question 42). Suppose you guess your professor's age, making errors $e = -2, -1, 5$ with probabilities $1/2, 1/4, 1/4$ . If the professor guesses too (or tries to remember), making errors $-1, 0, 1$ with probabilities $1/8, 6/8, 1/8$ , what weights $w_1$ and $w_2$ give the reliability of your guess and the professor's guess?","I understand the concept of least squares but I'm not able to wrap my head around weighted least squares (the matrix form). We convert to . What exactly happens when we multiply the equation with ? Is the column space of A modified based on the changed equations? Also how do I find this matrix , assuming I have the given data (The probability of each observation according to a textbook example ""Linear Algebra and it's Applications"" by Gilbert Strang, page 174, question 42). Suppose you guess your professor's age, making errors with probabilities . If the professor guesses too (or tries to remember), making errors with probabilities , what weights and give the reliability of your guess and the professor's guess?","Ax = b WAx = Wb W W e = -2, -1, 5 1/2, 1/4, 1/4 -1, 0, 1 1/8, 6/8, 1/8 w_1 w_2","['linear-algebra', 'statistics']"
29,prove a polynomial has at least 2n-1 distinct real roots,prove a polynomial has at least 2n-1 distinct real roots,,"If $P(x)$ is a real polynomial has $n$ distinct real roots in $(1,+\infty)$ . Set: $$Q(x)=(x^2+1)P(x)P'(x)+x(P^2(x)+P'^2(x))$$ How to prove $Q(x)=0$ has at least $2n-1$ distinct real roots? I think the idea is finding closed intervals that the boundary points have different signs. I think the only special points are the roots of $P(x)$ or $P'(x)$ . But if $a$ is such a point, we have $Q(a)>0$ . So i'm not sure this can work out... Can anyone show me some hints? Thanks!","If is a real polynomial has distinct real roots in . Set: How to prove has at least distinct real roots? I think the idea is finding closed intervals that the boundary points have different signs. I think the only special points are the roots of or . But if is such a point, we have . So i'm not sure this can work out... Can anyone show me some hints? Thanks!","P(x) n (1,+\infty) Q(x)=(x^2+1)P(x)P'(x)+x(P^2(x)+P'^2(x)) Q(x)=0 2n-1 P(x) P'(x) a Q(a)>0","['linear-algebra', 'polynomials']"
30,Find eigenvalues of the matrix,Find eigenvalues of the matrix,,"Find eigenvalues of the $(n+1) \times (n+1)$ -matrix $$ \left( \begin {array}{cccccccc} 0&0&0&0&0&0&-n&0\\ 0 &0&0&0&0&-(n-1)&0&1\\ 0&0&0&0&-(n-2)&0&2&0 \\ 0&0&0&\ldots&0&3&0&0\\ 0&0&-3&0&\ldots&0 &0&0\\ 0&-2&0&n-2&0&0&0&0\\ -1&0&n-1&0 &0&0&0&0\\ 0&n&0&0&0&0&0&0\end {array} \right) $$ I have tried for some small $n$ by hand and have got that for $n=3$ the eigenvalues are $1,-1,3,-3$ , for $n=4$ eigenvalues are $0,2,-2,4,-4$ . So I am conjectured than for arbitrary $n$ the eigenvalues are $n, n-2, n-4, \ldots, -n+2,-n.$ How to  prove it?","Find eigenvalues of the -matrix I have tried for some small by hand and have got that for the eigenvalues are , for eigenvalues are . So I am conjectured than for arbitrary the eigenvalues are How to  prove it?","(n+1) \times (n+1)  \left( \begin {array}{cccccccc} 0&0&0&0&0&0&-n&0\\ 0
&0&0&0&0&-(n-1)&0&1\\ 0&0&0&0&-(n-2)&0&2&0
\\ 0&0&0&\ldots&0&3&0&0\\ 0&0&-3&0&\ldots&0
&0&0\\ 0&-2&0&n-2&0&0&0&0\\ -1&0&n-1&0
&0&0&0&0\\ 0&n&0&0&0&0&0&0\end {array} \right)
 n n=3 1,-1,3,-3 n=4 0,2,-2,4,-4 n n, n-2, n-4, \ldots, -n+2,-n.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
31,Existence of a vector $v$ in $V$ such that the $T$-annihilator of $v$ is the minimal polynomial for $T$.,Existence of a vector  in  such that the -annihilator of  is the minimal polynomial for .,v V T v T,Definition: $T$ -annihilator of a vector $\alpha$ (denoted as $p_\alpha$ ) is the unique monic polynomial which generates the ideal such that $g(T)\alpha = 0$ for all $g$ in this ideal. I'm trying to prove the below statement without invoking the Cyclic Decomposition Theorem. Let $T$ be a linear operator on a finite-dimensional vector space $V$ . Then there exists a vector $v$ in $V$ such that the $T$ -annihilator of $v$ is the minimal polynomial for $T$ . Attempt: Assume that there is no such $v$ . Then every vector has a $T$ -annihilator of degree less than that of the minimal polynomial. Define a monic polynomial $h$ which is the sum of $T$ -annihilators of given basis elements. Then $h(T)v=0$ for all $v\in V$ . But this contradicts the definition of minimal polynomial since the degree of $h\lt$ the degree of the minimal polynomial. Can someone verify my argument?,Definition: -annihilator of a vector (denoted as ) is the unique monic polynomial which generates the ideal such that for all in this ideal. I'm trying to prove the below statement without invoking the Cyclic Decomposition Theorem. Let be a linear operator on a finite-dimensional vector space . Then there exists a vector in such that the -annihilator of is the minimal polynomial for . Attempt: Assume that there is no such . Then every vector has a -annihilator of degree less than that of the minimal polynomial. Define a monic polynomial which is the sum of -annihilators of given basis elements. Then for all . But this contradicts the definition of minimal polynomial since the degree of the degree of the minimal polynomial. Can someone verify my argument?,T \alpha p_\alpha g(T)\alpha = 0 g T V v V T v T v T h T h(T)v=0 v\in V h\lt,"['linear-algebra', 'solution-verification', 'linear-transformations', 'minimal-polynomials']"
32,Basis formed by hyperbolic functions,Basis formed by hyperbolic functions,,"I am currently working with separation of variables for different kinds of PDEs and one often uses here the fact that one has the sine base , i.e., $$ \left( \sin(k\pi y) \right)_{k=1}^{\infty} $$ forms a base of $L^2(0,1)$ . This also holds on discrete level, i.e., having vector $v_k \in \mathbb{R}^{n-1}$ defined entry-wise as $$ v_k = \left( \sin(\frac{k\pi}{n} j) \right)_{j=1}^{n-1} $$ then the set $\{v_k\}_{k=1}^{n-1}$ is basis of $\mathbb{R}^{n-1}$ . However, in some cases it would be much more suitable to use the hyperbolic functions - in this case the hyperbolic sine. My question is whether or not the analogue holds for hyperbolic sines as well, i.e., (A) Is the $\left( \sinh(k\pi y) \right)_{k=1}^{\infty}$ basis of some reasonable Lebesgue space on $(0,1)$ ? (B) Having vector $w_k \in \mathbb{R}^{n-1}$ defined entry-wise as $$ w_k = \left( \sinh(\frac{k\pi}{n} j) \right)_{j=1}^{n-1}, $$ is the set $\{w_k\}_{k=1}^{n-1}$ a basis of $\mathbb{R}^{n-1}$ ? Update : The computations suggest that also the vectors $w_k$ form a basis, but one that is incresingly ill-conditioned when $n$ grows. For $n=100$ , python computed that the condition number is of order $10^{130}$ . I am aware that at that point it is impossible to argue with such a result, so the question remains open. Also, I am interested only in the theoretical usage and I do not intend to use this basis (if it indeed is one in general) for computational purposes.","I am currently working with separation of variables for different kinds of PDEs and one often uses here the fact that one has the sine base , i.e., forms a base of . This also holds on discrete level, i.e., having vector defined entry-wise as then the set is basis of . However, in some cases it would be much more suitable to use the hyperbolic functions - in this case the hyperbolic sine. My question is whether or not the analogue holds for hyperbolic sines as well, i.e., (A) Is the basis of some reasonable Lebesgue space on ? (B) Having vector defined entry-wise as is the set a basis of ? Update : The computations suggest that also the vectors form a basis, but one that is incresingly ill-conditioned when grows. For , python computed that the condition number is of order . I am aware that at that point it is impossible to argue with such a result, so the question remains open. Also, I am interested only in the theoretical usage and I do not intend to use this basis (if it indeed is one in general) for computational purposes."," \left( \sin(k\pi y) \right)_{k=1}^{\infty}  L^2(0,1) v_k \in \mathbb{R}^{n-1}  v_k = \left( \sin(\frac{k\pi}{n} j) \right)_{j=1}^{n-1}  \{v_k\}_{k=1}^{n-1} \mathbb{R}^{n-1} \left( \sinh(k\pi y) \right)_{k=1}^{\infty} (0,1) w_k \in \mathbb{R}^{n-1}  w_k = \left( \sinh(\frac{k\pi}{n} j) \right)_{j=1}^{n-1},  \{w_k\}_{k=1}^{n-1} \mathbb{R}^{n-1} w_k n n=100 10^{130}","['linear-algebra', 'vector-spaces', 'hyperbolic-functions', 'change-of-basis']"
33,Trace inequality for a product of p.s.d. matrices and their pseudo inverse.,Trace inequality for a product of p.s.d. matrices and their pseudo inverse.,,"Let $A, B_i$ be positive semidefinite real matrices. Let $\dagger$ stand for the Moore-Penrose generalized inverse. I managed to prove that if $\operatorname{Ran}B_1\subseteq\operatorname{Ker}B_2$ then $$\operatorname{trace}\left((A + B_1 + B_2)^\dagger B_1 \right) \leq\operatorname{trace}\left(( A + B_1)^\dagger B_1\right) $$ Does it still hold without this assumption?",Let be positive semidefinite real matrices. Let stand for the Moore-Penrose generalized inverse. I managed to prove that if then Does it still hold without this assumption?,"A, B_i \dagger \operatorname{Ran}B_1\subseteq\operatorname{Ker}B_2 \operatorname{trace}\left((A + B_1 + B_2)^\dagger B_1 \right) \leq\operatorname{trace}\left(( A + B_1)^\dagger B_1\right) ","['linear-algebra', 'matrices', 'positive-definite', 'positive-semidefinite', 'generalized-inverse']"
34,How to verify whether or not there exists a vector $x$ such that $Ax > 0$?,How to verify whether or not there exists a vector  such that ?,x Ax > 0,"Given a matrix $A$ in $\mathbb R^{m\times n}$, if I want to know whether or not there is an $x \in \mathbb R^{n}$ such that  $$ Ax > 0, $$ (meaning all elements in $Ax$ are positive). Is there some easy way to verify whether or not such $x$ exists?","Given a matrix $A$ in $\mathbb R^{m\times n}$, if I want to know whether or not there is an $x \in \mathbb R^{n}$ such that  $$ Ax > 0, $$ (meaning all elements in $Ax$ are positive). Is there some easy way to verify whether or not such $x$ exists?",,"['linear-algebra', 'linear-programming']"
35,Counting Conjugacy Classes: $A^6 = I$,Counting Conjugacy Classes:,A^6 = I,"This question is inspired by this earlier post .  The question that I'm interested in is as follows: Let $S= \{A \in \mathbb{Q}^{k \times k} : A^6 = I \text{ and }A^n \ne I \text{ for any }0 < n < 6\}$. How many orbits under conjugation by $GL_k(\mathbb{Q})$ does $S$ contain? Per the argument that I've given in the linked post, it suffices to list representatives in rational canonical form .  Thus, it is equivalent to enumerate the multisets of irreducible polynomials $\{p_1,\dots,p_n\}$ (which are the polynomials corresponding to companion matrix blocks) each of which divides $x^6-1$, whose degrees sum to $k$, and whose lcm divides $x^6 - 1$ but no polynomial of the form $x^n - 1$ for $n < 6$. Of course, there is only one such multiset in the case of $k=2$, hence the answer to the linked question.  As $k$ grows things get trickier. For $k=5$, we have $$ \{x^2 - x + 1, x \pm 1, x \pm 1, x\pm 1\}\\ \{x^2 + x + 1, x+1, x\pm 1, x\pm 1\}\\ \{x^2 - x + 1,  x^2 \pm x + 1 ,x \pm 1\}\\ \{x^2 + x + 1, x^2 + x + 1, x+1\}\\ $$ For a total of $14$ conjugacy classes. How might one approach this problem in general?","This question is inspired by this earlier post .  The question that I'm interested in is as follows: Let $S= \{A \in \mathbb{Q}^{k \times k} : A^6 = I \text{ and }A^n \ne I \text{ for any }0 < n < 6\}$. How many orbits under conjugation by $GL_k(\mathbb{Q})$ does $S$ contain? Per the argument that I've given in the linked post, it suffices to list representatives in rational canonical form .  Thus, it is equivalent to enumerate the multisets of irreducible polynomials $\{p_1,\dots,p_n\}$ (which are the polynomials corresponding to companion matrix blocks) each of which divides $x^6-1$, whose degrees sum to $k$, and whose lcm divides $x^6 - 1$ but no polynomial of the form $x^n - 1$ for $n < 6$. Of course, there is only one such multiset in the case of $k=2$, hence the answer to the linked question.  As $k$ grows things get trickier. For $k=5$, we have $$ \{x^2 - x + 1, x \pm 1, x \pm 1, x\pm 1\}\\ \{x^2 + x + 1, x+1, x\pm 1, x\pm 1\}\\ \{x^2 - x + 1,  x^2 \pm x + 1 ,x \pm 1\}\\ \{x^2 + x + 1, x^2 + x + 1, x+1\}\\ $$ For a total of $14$ conjugacy classes. How might one approach this problem in general?",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'matrices']"
36,Operator norm of a self adjoint operator.,Operator norm of a self adjoint operator.,,"Let $T$ be a self adjoint operator on $\Bbb C^n$. Let $\lambda_1,\lambda_2, \cdots ,\lambda_n$ be the eigen values of $T$. Then show that the operator norm of $T$ i.e. $\|T\| = \mathrm {max}_j\ |\lambda_j|$. I am trying to prove it in the following way. I have just proved that $\|T\|= \mathrm {sup} \left \{|\left < Tx,x \right > |\ :\ \|x\|=1 \right \}$. Also since $T$ is self adjoint it is normal. So by Spectral Theorem for Normal Operators we have a orthonormal basis $\{X_1,X_2, \cdots ,X_n \}$ for $\Bbb C^n$ each vector of which is an eigen vector of $T$. WLOG let us assume that $\lambda_j$ be an eigen value of $T$ corresponding to the eigen vector $X_j$, for $j=1,2, \cdots ,n$. Then $|\left < TX_j,X_j \right > | = |\left <\lambda_jX_j,X_j \right > | = |\lambda_j|$ (since $\left < X_j,X_j \right >=1$) for $j=1,2, \cdots ,n$. Since $\|X_j\|=1$ for $j=1,2, \cdots ,n$ so we have $\mathrm {max}_j\ |\lambda_j| \le \|T\|$  by the above proven result. But I find difficulty to prove the reverse inequality. For the reverse I started by taking a vector $X \in \Bbb C^n$ with $\|X\|=1$.  Then there exist scalars $c_1,c_2, \cdots ,c_n \in \Bbb C$ such that $X=c_1X_1+c_2X_2+ \cdots +c_nX_n$. Since $X_j$'s are orthonormal we have $\|X\|=\sqrt {|c_1|^2+|c_2|^2+ \cdots +|c_n|^2}$. So we have $|c_1|^2+|c_2|^2+ \cdots +|c_n|^2=1$. Again by orthonormality of $X_j$'s we have $\left <TX,X \right > = |c_1|^2\lambda_1+|c_2|^2\lambda_2+ \cdots +|c_n|^2\lambda_n \le (|c_1|^2+|c_2|^2+ \cdots + |c_n|^2)(\lambda_1+\lambda_2+ \cdots +\lambda_n)=\lambda_1+\lambda_2+\cdots +\lambda_n$. Hence by triangle inequality we have $|\left <TX,X \right >| \le |\lambda_1|+|\lambda_2|+ \cdots +|\lambda_n|$. Which clearly doesn't meet my purpose. So how do I proceed to prove the reverse inequality? Please help me in this regard. Thank you very much.","Let $T$ be a self adjoint operator on $\Bbb C^n$. Let $\lambda_1,\lambda_2, \cdots ,\lambda_n$ be the eigen values of $T$. Then show that the operator norm of $T$ i.e. $\|T\| = \mathrm {max}_j\ |\lambda_j|$. I am trying to prove it in the following way. I have just proved that $\|T\|= \mathrm {sup} \left \{|\left < Tx,x \right > |\ :\ \|x\|=1 \right \}$. Also since $T$ is self adjoint it is normal. So by Spectral Theorem for Normal Operators we have a orthonormal basis $\{X_1,X_2, \cdots ,X_n \}$ for $\Bbb C^n$ each vector of which is an eigen vector of $T$. WLOG let us assume that $\lambda_j$ be an eigen value of $T$ corresponding to the eigen vector $X_j$, for $j=1,2, \cdots ,n$. Then $|\left < TX_j,X_j \right > | = |\left <\lambda_jX_j,X_j \right > | = |\lambda_j|$ (since $\left < X_j,X_j \right >=1$) for $j=1,2, \cdots ,n$. Since $\|X_j\|=1$ for $j=1,2, \cdots ,n$ so we have $\mathrm {max}_j\ |\lambda_j| \le \|T\|$  by the above proven result. But I find difficulty to prove the reverse inequality. For the reverse I started by taking a vector $X \in \Bbb C^n$ with $\|X\|=1$.  Then there exist scalars $c_1,c_2, \cdots ,c_n \in \Bbb C$ such that $X=c_1X_1+c_2X_2+ \cdots +c_nX_n$. Since $X_j$'s are orthonormal we have $\|X\|=\sqrt {|c_1|^2+|c_2|^2+ \cdots +|c_n|^2}$. So we have $|c_1|^2+|c_2|^2+ \cdots +|c_n|^2=1$. Again by orthonormality of $X_j$'s we have $\left <TX,X \right > = |c_1|^2\lambda_1+|c_2|^2\lambda_2+ \cdots +|c_n|^2\lambda_n \le (|c_1|^2+|c_2|^2+ \cdots + |c_n|^2)(\lambda_1+\lambda_2+ \cdots +\lambda_n)=\lambda_1+\lambda_2+\cdots +\lambda_n$. Hence by triangle inequality we have $|\left <TX,X \right >| \le |\lambda_1|+|\lambda_2|+ \cdots +|\lambda_n|$. Which clearly doesn't meet my purpose. So how do I proceed to prove the reverse inequality? Please help me in this regard. Thank you very much.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'operator-theory', 'spectral-theory']"
37,Inequality of determinant and product of diagonal elements of a matrix [duplicate],Inequality of determinant and product of diagonal elements of a matrix [duplicate],,"This question already has an answer here : If $M$ is positive definite, then $\operatorname{det}{(M)}\leq \prod_i m_{ii}$ (1 answer) Closed 4 years ago . Let $A$ be a symmetric, positive definite matrix. Is is true that the det $ A$ (which is the product of eigenvalues) is smaller or equal to the product of diagonal elements of $A$ ? I could not prove it, and not even sure if the inequality above is true.","This question already has an answer here : If $M$ is positive definite, then $\operatorname{det}{(M)}\leq \prod_i m_{ii}$ (1 answer) Closed 4 years ago . Let be a symmetric, positive definite matrix. Is is true that the det (which is the product of eigenvalues) is smaller or equal to the product of diagonal elements of ? I could not prove it, and not even sure if the inequality above is true.",A  A A,"['linear-algebra', 'determinant', 'positive-definite']"
38,"$\mathcal{L}(V,W)$ is infinite dimensional when $V$ is finite dimensional and $W$ is infinite dimensional.",is infinite dimensional when  is finite dimensional and  is infinite dimensional.,"\mathcal{L}(V,W) V W","In an earlier post Proof of $\mathcal{L}(V,W)$ being infinite dimensional I presented a proof for the proposition that Theorem. If $V$ is finite dimensional with $\dim V>0$ and $W$ is infinite   dimensional then the vector space $\mathcal{L}(V,W)$ is infinite   dimensional. However the proof presented there though correct was preposterously long. I present here another proof which makes use of the following theorem. I would like to know whether this new proof is correct? Theorem ($2.A.9)$ A vector space $V$ is infinite dimensional if and only if there is a sequence of vectors $v_1,v_2,v_3,v_4,....$ such that for all $n\in\mathbf{Z^+}$ the subsequence $v_1,v_2,....,v_n$ is linearly independent. Proof. Since $W$ is infinite dimensional we may invoke a sequence of vectors  $\psi:\mathbf{Z^+}\to W$ such forall $n\in\mathbf{Z^+}$ the subsequence $\psi(1),\psi(2),...,\psi(n)$ is linearly independent. Let $v_1,v_2,...,v_m$ be a basis for $V$ and consider the sequence $\phi:\mathbf{Z^+}\to\mathcal{L}(V,W)$ defined as follows $$\phi(n) = T(c_1v_1+c_2v_2+\cdot\cdot\cdot+c_mv_m) = c_1\psi(1)+c_2\psi(2)+\cdot\cdot\cdot+c_m\psi(m+n)\tag{1}$$ We now show by recourse to Mathematical-Induction that with the above definition for all $n\in\mathbf{Z^+}$ the list $\phi(1),\phi(2),...,\phi(n)$ is linearly independent. Evidently $\phi(1)$ on its own is linearly independent. Now assume that for some $k\in\mathbf{Z^+}$ the list $\phi(1),\phi(2),...,\phi(k)$ is linearly independent but the list $\phi(1),\phi(2),...,\phi(k),\phi(k+1)$ is not. Then for some $j\in\{1,2,3,...,k+1\}$ it is the case that $\phi(j)\in\operatorname{span}(\phi(1),\phi(2),...,\phi(j-1))$ but this $j\not\in\{1,2,....,k\}$ since $\phi(1),\phi(2),...,\phi(k)$ is linearly independent, consequently $j = k+1$, which implies that for some $x_1,x_2,...,x_k\in\mathbf{F}$ $$\phi(k+1)= x_1\phi(1)+x_2\phi(2)+\cdot\cdot\cdot+x_k\phi(k)$$ then applying both sides to the vector $v_m$ we have $$\phi(k+1)v_m = x_1\phi(1)v_m+x_2\phi(2)v_m+\cdot\cdot\cdot+x_k\phi(k)v_m\tag{2}$$ $$\psi(m+k+1) = x_1\psi(m+1)+x_2\psi(m+2)+\cdot\cdot\cdot+x_m\psi(m+k)\tag{3}$$ but $(3)$ implies that $\psi(m+k+1)\in\operatorname{span}(\psi(1),\psi(2),...,\psi(m+k))$ contradicting the fact that for all $n\in\mathbf{Z^+}$, $\psi(1),\psi(2),...,\psi(n)$ is linearly independent. $\blacksquare$","In an earlier post Proof of $\mathcal{L}(V,W)$ being infinite dimensional I presented a proof for the proposition that Theorem. If $V$ is finite dimensional with $\dim V>0$ and $W$ is infinite   dimensional then the vector space $\mathcal{L}(V,W)$ is infinite   dimensional. However the proof presented there though correct was preposterously long. I present here another proof which makes use of the following theorem. I would like to know whether this new proof is correct? Theorem ($2.A.9)$ A vector space $V$ is infinite dimensional if and only if there is a sequence of vectors $v_1,v_2,v_3,v_4,....$ such that for all $n\in\mathbf{Z^+}$ the subsequence $v_1,v_2,....,v_n$ is linearly independent. Proof. Since $W$ is infinite dimensional we may invoke a sequence of vectors  $\psi:\mathbf{Z^+}\to W$ such forall $n\in\mathbf{Z^+}$ the subsequence $\psi(1),\psi(2),...,\psi(n)$ is linearly independent. Let $v_1,v_2,...,v_m$ be a basis for $V$ and consider the sequence $\phi:\mathbf{Z^+}\to\mathcal{L}(V,W)$ defined as follows $$\phi(n) = T(c_1v_1+c_2v_2+\cdot\cdot\cdot+c_mv_m) = c_1\psi(1)+c_2\psi(2)+\cdot\cdot\cdot+c_m\psi(m+n)\tag{1}$$ We now show by recourse to Mathematical-Induction that with the above definition for all $n\in\mathbf{Z^+}$ the list $\phi(1),\phi(2),...,\phi(n)$ is linearly independent. Evidently $\phi(1)$ on its own is linearly independent. Now assume that for some $k\in\mathbf{Z^+}$ the list $\phi(1),\phi(2),...,\phi(k)$ is linearly independent but the list $\phi(1),\phi(2),...,\phi(k),\phi(k+1)$ is not. Then for some $j\in\{1,2,3,...,k+1\}$ it is the case that $\phi(j)\in\operatorname{span}(\phi(1),\phi(2),...,\phi(j-1))$ but this $j\not\in\{1,2,....,k\}$ since $\phi(1),\phi(2),...,\phi(k)$ is linearly independent, consequently $j = k+1$, which implies that for some $x_1,x_2,...,x_k\in\mathbf{F}$ $$\phi(k+1)= x_1\phi(1)+x_2\phi(2)+\cdot\cdot\cdot+x_k\phi(k)$$ then applying both sides to the vector $v_m$ we have $$\phi(k+1)v_m = x_1\phi(1)v_m+x_2\phi(2)v_m+\cdot\cdot\cdot+x_k\phi(k)v_m\tag{2}$$ $$\psi(m+k+1) = x_1\psi(m+1)+x_2\psi(m+2)+\cdot\cdot\cdot+x_m\psi(m+k)\tag{3}$$ but $(3)$ implies that $\psi(m+k+1)\in\operatorname{span}(\psi(1),\psi(2),...,\psi(m+k))$ contradicting the fact that for all $n\in\mathbf{Z^+}$, $\psi(1),\psi(2),...,\psi(n)$ is linearly independent. $\blacksquare$",,"['linear-algebra', 'proof-verification', 'linear-transformations']"
39,Constructing an invertible integer matrix given one column.,Constructing an invertible integer matrix given one column.,,"Given a vector $\vec{v}\in\mathbb{Z}^n$ whose entries are relatively prime, is it possible to build an invertible $n\times n$ integer matrix which has $\vec{v}$ as a column? For $n=2$ this is true, and it seems like it should be true for $n>2$, but the proof becomes much more complicated as the expression for the determinant becomes more complicated. I am more or less trying to set the determinant equal to $\pm 1$ and then determine if the corresponding equations have integer solutions. This is messy. I am wondering if there is a nicer way to prove this. The proof doesn't need to be elementary.","Given a vector $\vec{v}\in\mathbb{Z}^n$ whose entries are relatively prime, is it possible to build an invertible $n\times n$ integer matrix which has $\vec{v}$ as a column? For $n=2$ this is true, and it seems like it should be true for $n>2$, but the proof becomes much more complicated as the expression for the determinant becomes more complicated. I am more or less trying to set the determinant equal to $\pm 1$ and then determine if the corresponding equations have integer solutions. This is messy. I am wondering if there is a nicer way to prove this. The proof doesn't need to be elementary.",,"['linear-algebra', 'elementary-number-theory']"
40,A problem on rank of a matrix over two fields,A problem on rank of a matrix over two fields,,"This is a problem from Berkeley problems in mathematics. If $F$ is a subfield of $K$, and $M$ has entries in $F$, how is the row rank of $M$ over $F$ related to the row rank of $M$ over $K$? where $M$ is a n by n matrix The solution says ""If a set of rows of $M$ is linearly independent over $F$, then clearly it is also independent over K, so the rank of $M$ over $F$ is, at most, the rank of $M$ over $K$."" I have some trouble understanding this, what I thought was that if they are linearly independent over the bigger field K, they are linearly independent over F. (Because all linear combinations with scalars from F are subsumed when you are talking about linear combinations in K) However here it is the other way around","This is a problem from Berkeley problems in mathematics. If $F$ is a subfield of $K$, and $M$ has entries in $F$, how is the row rank of $M$ over $F$ related to the row rank of $M$ over $K$? where $M$ is a n by n matrix The solution says ""If a set of rows of $M$ is linearly independent over $F$, then clearly it is also independent over K, so the rank of $M$ over $F$ is, at most, the rank of $M$ over $K$."" I have some trouble understanding this, what I thought was that if they are linearly independent over the bigger field K, they are linearly independent over F. (Because all linear combinations with scalars from F are subsumed when you are talking about linear combinations in K) However here it is the other way around",,"['linear-algebra', 'matrices']"
41,When is a matrix the cosine of another matrix?,When is a matrix the cosine of another matrix?,,"Knowing that every nonsingular matrix in $M_n(\mathbb{C})$ (the set of size $n$ matrices with complex entries) is an exponential of some matrix in $M_n(\mathbb{C})$, what can be said to answer the question: For which matrices $A$ in $M_n(\mathbb{C})$, does the equation $$\cos X = A$$ have a solution $X$ in $M_n(\mathbb{C})$?","Knowing that every nonsingular matrix in $M_n(\mathbb{C})$ (the set of size $n$ matrices with complex entries) is an exponential of some matrix in $M_n(\mathbb{C})$, what can be said to answer the question: For which matrices $A$ in $M_n(\mathbb{C})$, does the equation $$\cos X = A$$ have a solution $X$ in $M_n(\mathbb{C})$?",,"['linear-algebra', 'matrices']"
42,If $M\geq0$. Why $\overline{\text{Im}(M)}=\text{Im}(M)\Leftrightarrow \text{Im}(M)=\text{Im}(M^{1/2}) $?,If . Why ?,M\geq0 \overline{\text{Im}(M)}=\text{Im}(M)\Leftrightarrow \text{Im}(M)=\text{Im}(M^{1/2}) ,"Let $E$ be a complex Hilbert space. Let $M\in \mathcal{L}(E)^+$ (i.e. $M^*=M$ and $\langle Mx\;,x\;\rangle\geq 0,\,\forall x\in E$). Why $$\overline{\operatorname{Im}(M)}=\operatorname{Im}(M)\Leftrightarrow \operatorname{Im}(M)=\operatorname{Im}(M^{1/2})\; ?$$ Thank you","Let $E$ be a complex Hilbert space. Let $M\in \mathcal{L}(E)^+$ (i.e. $M^*=M$ and $\langle Mx\;,x\;\rangle\geq 0,\,\forall x\in E$). Why $$\overline{\operatorname{Im}(M)}=\operatorname{Im}(M)\Leftrightarrow \operatorname{Im}(M)=\operatorname{Im}(M^{1/2})\; ?$$ Thank you",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras']"
43,"A sequence ${\bf x}_1,{\bf x}_2,{\bf x}_3,...$ converges geometrically in some norm, will it converge geometrically in any equivalent norm?","A sequence  converges geometrically in some norm, will it converge geometrically in any equivalent norm?","{\bf x}_1,{\bf x}_2,{\bf x}_3,...","If a sequence ${\bf x}_1,{\bf x}_2,{\bf x}_3,...$ converges geometrically to a vector ${\bf u}$ in norm $\|\cdot\|_1$, i.e. $\|{\bf x}_i - {\bf u}\|_1 \le q\|{\bf x}_{i-1} - {\bf u}\|_1$ for some $q\in (0,1)$, will it converge geometrically in any norm $\|\cdot\|_2$ that is equivalent to $\|\cdot\|_1$? Suppose $c_1\|\cdot\|_2 \le \|\cdot\|_1 \le c_2\|\cdot\|_2$ for some $0<c_1 \le c_2$. I think ${\bf x}_1,{\bf x}_2,{\bf x}_3,...$ converges to $\bf u$ in $\|\cdot\|_2$, through a simple $\epsilon$-arugment. However, I cannot see the convergence is also geometric in $\|\cdot\|_2$. If it is indeed not, can anyone help give a counterexample? Thanks!","If a sequence ${\bf x}_1,{\bf x}_2,{\bf x}_3,...$ converges geometrically to a vector ${\bf u}$ in norm $\|\cdot\|_1$, i.e. $\|{\bf x}_i - {\bf u}\|_1 \le q\|{\bf x}_{i-1} - {\bf u}\|_1$ for some $q\in (0,1)$, will it converge geometrically in any norm $\|\cdot\|_2$ that is equivalent to $\|\cdot\|_1$? Suppose $c_1\|\cdot\|_2 \le \|\cdot\|_1 \le c_2\|\cdot\|_2$ for some $0<c_1 \le c_2$. I think ${\bf x}_1,{\bf x}_2,{\bf x}_3,...$ converges to $\bf u$ in $\|\cdot\|_2$, through a simple $\epsilon$-arugment. However, I cannot see the convergence is also geometric in $\|\cdot\|_2$. If it is indeed not, can anyone help give a counterexample? Thanks!",,"['linear-algebra', 'functional-analysis', 'operator-theory']"
44,Rearrange given vectors to minimize the sum of $\ell_1$ norms of pairwise sums,Rearrange given vectors to minimize the sum of  norms of pairwise sums,\ell_1,"Let $\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n$ be real vectors with the following property $$\sum_{i=1}^{n}\mathbf{x}_i=0$$ I want to find a grouping strategy to achieve the minimum of $$\|\mathbf{x}_{i_1}+\mathbf{x}_{i_2}\|_1+\|\mathbf{x}_{i_3}+\mathbf{x}_{i_4}\|_1+\cdots+\|\mathbf{x}_{i_{n-1}}+\mathbf{x}_{i_n}\|_1$$ where $\|\cdot\|_1$ is the $\ell_1$ norm.","Let $\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n$ be real vectors with the following property $$\sum_{i=1}^{n}\mathbf{x}_i=0$$ I want to find a grouping strategy to achieve the minimum of $$\|\mathbf{x}_{i_1}+\mathbf{x}_{i_2}\|_1+\|\mathbf{x}_{i_3}+\mathbf{x}_{i_4}\|_1+\cdots+\|\mathbf{x}_{i_{n-1}}+\mathbf{x}_{i_n}\|_1$$ where $\|\cdot\|_1$ is the $\ell_1$ norm.",,"['linear-algebra', 'optimization', 'permutations', 'discrete-optimization']"
45,Proper function notation for matrix functions?,Proper function notation for matrix functions?,,"For a vector-valued function we have the notation $f:\mathbb R^n\rightarrow \mathbb R^m$. Is this also a proper notation for a matrix function? Are there any conventions? For a matrix of one variable, $t$,    \begin{align} \mathbf A(t)= \begin{bmatrix} a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t)\\ a_{12}(t) & a_{22}(t) & \cdots & a_{2n}(t)\\ \vdots &\vdots & \ddots & \vdots\\ a_{n1}(t) & a_{n2}(t) &\cdots & a_{mn}(t) \end{bmatrix}, \end{align}   is it correct to write $\mathbf A:\mathbb R \rightarrow \mathbb R^{m\times n}$? And for a matrix of several variables $\mathbf x=(x_1, \dots, x_n)$,   \begin{align} \mathbf B(\mathbf x)= \begin{bmatrix} b_{11}(\mathbf x) & b_{12}(\mathbf x) & \cdots & b_{1n}(\mathbf x)\\ b_{12}(\mathbf x) & b_{22}(\mathbf x) & \cdots & b_{2n}(\mathbf x)\\ \vdots &\vdots & \ddots & \vdots\\ b_{n1}(\mathbf x) & b_{n2}(\mathbf x) &\cdots & b_{mn}(\mathbf x) \end{bmatrix}, \end{align}   is the proper notation $\mathbf B:\mathbb R^{n} \rightarrow \mathbb R^{m \times n}$?","For a vector-valued function we have the notation $f:\mathbb R^n\rightarrow \mathbb R^m$. Is this also a proper notation for a matrix function? Are there any conventions? For a matrix of one variable, $t$,    \begin{align} \mathbf A(t)= \begin{bmatrix} a_{11}(t) & a_{12}(t) & \cdots & a_{1n}(t)\\ a_{12}(t) & a_{22}(t) & \cdots & a_{2n}(t)\\ \vdots &\vdots & \ddots & \vdots\\ a_{n1}(t) & a_{n2}(t) &\cdots & a_{mn}(t) \end{bmatrix}, \end{align}   is it correct to write $\mathbf A:\mathbb R \rightarrow \mathbb R^{m\times n}$? And for a matrix of several variables $\mathbf x=(x_1, \dots, x_n)$,   \begin{align} \mathbf B(\mathbf x)= \begin{bmatrix} b_{11}(\mathbf x) & b_{12}(\mathbf x) & \cdots & b_{1n}(\mathbf x)\\ b_{12}(\mathbf x) & b_{22}(\mathbf x) & \cdots & b_{2n}(\mathbf x)\\ \vdots &\vdots & \ddots & \vdots\\ b_{n1}(\mathbf x) & b_{n2}(\mathbf x) &\cdots & b_{mn}(\mathbf x) \end{bmatrix}, \end{align}   is the proper notation $\mathbf B:\mathbb R^{n} \rightarrow \mathbb R^{m \times n}$?",,"['linear-algebra', 'matrices', 'functions', 'multivariable-calculus', 'notation']"
46,Factorizing a rotation matrix into a product of stretch and shear matrices,Factorizing a rotation matrix into a product of stretch and shear matrices,,"In a video which shows how to rotate a picture in Microsoft Paint by any given angle (usually you can only rotate by 90 degrees in Paint), it is shown how to do a rotation by $\alpha$ doing the following operations: Horziontal skew by $\alpha$ Vertical stretch by $\frac{1}{\cos^2(\alpha)}$ Vertical skew by $-\alpha$ Horizontal and vertical stretch by $\cos(\alpha)$ So in terms of matrices, we have $\begin{pmatrix}\cos(\alpha)&\sin(\alpha)\\-\sin(\alpha)&\cos(\alpha)\end{pmatrix}=\begin{pmatrix}\cos(\alpha)&0\\ 0&\cos(\alpha) \end{pmatrix}\begin{pmatrix}1&0\\ \tan(-\alpha)&1 \end{pmatrix}\begin{pmatrix}1&0\\0& \frac{1}{\cos^2(\alpha)} \end{pmatrix}\begin{pmatrix}1&\tan(\alpha)\\0&1 \end{pmatrix}.$ Are there general factorization results of this kind (e.g. for non-rotation matrices? In which sense is this factorization unique for a rotation matrix?","In a video which shows how to rotate a picture in Microsoft Paint by any given angle (usually you can only rotate by 90 degrees in Paint), it is shown how to do a rotation by $\alpha$ doing the following operations: Horziontal skew by $\alpha$ Vertical stretch by $\frac{1}{\cos^2(\alpha)}$ Vertical skew by $-\alpha$ Horizontal and vertical stretch by $\cos(\alpha)$ So in terms of matrices, we have $\begin{pmatrix}\cos(\alpha)&\sin(\alpha)\\-\sin(\alpha)&\cos(\alpha)\end{pmatrix}=\begin{pmatrix}\cos(\alpha)&0\\ 0&\cos(\alpha) \end{pmatrix}\begin{pmatrix}1&0\\ \tan(-\alpha)&1 \end{pmatrix}\begin{pmatrix}1&0\\0& \frac{1}{\cos^2(\alpha)} \end{pmatrix}\begin{pmatrix}1&\tan(\alpha)\\0&1 \end{pmatrix}.$ Are there general factorization results of this kind (e.g. for non-rotation matrices? In which sense is this factorization unique for a rotation matrix?",,"['linear-algebra', 'trigonometry']"
47,Moore–Penrose pseudo-inverse Reference.,Moore–Penrose pseudo-inverse Reference.,,I have seen the Wikipedia page on Moore–Penrose pseudoinverse . I want to study more about generalized inverse of matrix. Can you provide some best references for this topic? (Self-Study).,I have seen the Wikipedia page on Moore–Penrose pseudoinverse . I want to study more about generalized inverse of matrix. Can you provide some best references for this topic? (Self-Study).,,"['linear-algebra', 'matrices', 'reference-request', 'soft-question']"
48,basis of sum of 2 vector spaces,basis of sum of 2 vector spaces,,"Let $V=\{(x,y,z)\epsilon \mathbb R^3: x-y=z\}$ and $W=\{(x,y,z)\epsilon \mathbb R^3: x+z=2y\}$ subspaces of $\mathbb R^3$. Show that $V+W= \mathbb R^3$. Also check if the sum is direct, $V \oplus W= \mathbb R^3$. At first i found the basis of $V$ and $W$ which are $V=<(1,1,0),(1,0,1)>$ and  $W=<(2,1,0),(-1,0,1)>$. By definition the sum is $V+W=<x_1,x_2,x_3,x_4>$ where $x_1,x_2$ is the vectors of $V$ and $x_3,x_4$ the vectors of $W$. Then, i put the vectors on a matrix and proceeded to gauss elimination and i got an equation between $x_1,x_2,x_3,x_4$ equals $0$ but i don't know how to proceed from this point. Any help will be appreciated.","Let $V=\{(x,y,z)\epsilon \mathbb R^3: x-y=z\}$ and $W=\{(x,y,z)\epsilon \mathbb R^3: x+z=2y\}$ subspaces of $\mathbb R^3$. Show that $V+W= \mathbb R^3$. Also check if the sum is direct, $V \oplus W= \mathbb R^3$. At first i found the basis of $V$ and $W$ which are $V=<(1,1,0),(1,0,1)>$ and  $W=<(2,1,0),(-1,0,1)>$. By definition the sum is $V+W=<x_1,x_2,x_3,x_4>$ where $x_1,x_2$ is the vectors of $V$ and $x_3,x_4$ the vectors of $W$. Then, i put the vectors on a matrix and proceeded to gauss elimination and i got an equation between $x_1,x_2,x_3,x_4$ equals $0$ but i don't know how to proceed from this point. Any help will be appreciated.",,"['linear-algebra', 'vectors']"
49,Leibniz formula for determinants,Leibniz formula for determinants,,"I don't fully understand the formula for Leibniz formula of determinant. It is in my book as:\ $\ \Sigma sgn(\pi )A_{1\pi (1)}...A_{n\pi (n)}$ I kind of understand that it has to do with eradicating repeated rows and the amount of rows that have been interchanged but that is about the extent of it. An example of a 3x3 matrix would be great, thanks.","I don't fully understand the formula for Leibniz formula of determinant. It is in my book as:\ $\ \Sigma sgn(\pi )A_{1\pi (1)}...A_{n\pi (n)}$ I kind of understand that it has to do with eradicating repeated rows and the amount of rows that have been interchanged but that is about the extent of it. An example of a 3x3 matrix would be great, thanks.",,"['linear-algebra', 'matrices', 'determinant']"
50,Bra-ket notation - what does $|0\rangle$ mean?,Bra-ket notation - what does  mean?,|0\rangle,"So, I've read that $|0\rangle$ does not mean the zero vector, that is just represented by $0$. So what does $|0\rangle$ and $\langle 0|$  ""equal"" in standard matrix notation? In particular, are there some general rules I can follow for the translation of bra-ket notation into standard matrix notation? I come up against problems like $\langle 0|0\rangle$ and I understand what is happening intuitively, but I have no idea how to practically carry it out (for what it's worth, I'm used to normal matrix notation and can solve problems in it). Thanks! EDIT: Perhaps I didn't make my question clear. I know that $\langle 0|0\rangle$ represents an inner product or dot product, that $\langle 0|$ represents a row vector, and that $|0\rangle$ represents a column vector; this is what I meant by understanding it intuitively. However, given the problem $\langle 0|0\rangle$ I don't know what the answer is, because I don't know how to work it out; I'm used to standard matrix notation.","So, I've read that $|0\rangle$ does not mean the zero vector, that is just represented by $0$. So what does $|0\rangle$ and $\langle 0|$  ""equal"" in standard matrix notation? In particular, are there some general rules I can follow for the translation of bra-ket notation into standard matrix notation? I come up against problems like $\langle 0|0\rangle$ and I understand what is happening intuitively, but I have no idea how to practically carry it out (for what it's worth, I'm used to normal matrix notation and can solve problems in it). Thanks! EDIT: Perhaps I didn't make my question clear. I know that $\langle 0|0\rangle$ represents an inner product or dot product, that $\langle 0|$ represents a row vector, and that $|0\rangle$ represents a column vector; this is what I meant by understanding it intuitively. However, given the problem $\langle 0|0\rangle$ I don't know what the answer is, because I don't know how to work it out; I'm used to standard matrix notation.",,"['linear-algebra', 'notation', 'quantum-mechanics']"
51,The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal,The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal,,"Anyone can help provide a proof for The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal. I found one related question Why is the largest element of symmetric, positive semidefinite matrix on the diagonal? whose answer proves a similar claim for symmetric positive definite matrix. However, I think the prove fails for a matrix with complex numbers, as the imaginary part also contributes to the magnitude.","Anyone can help provide a proof for The largest element in magnitude of a Hermitian positive definite matrix is on the diagonal. I found one related question Why is the largest element of symmetric, positive semidefinite matrix on the diagonal? whose answer proves a similar claim for symmetric positive definite matrix. However, I think the prove fails for a matrix with complex numbers, as the imaginary part also contributes to the magnitude.",,['linear-algebra']
52,"If $A$ is a symmetric positive definite matrix, then $A_{ii}(A^{-1})_{ii}\geq1$ for all $i$. Equality?","If  is a symmetric positive definite matrix, then  for all . Equality?",A A_{ii}(A^{-1})_{ii}\geq1 i,"I know that, if $A$ is an $n\times n$ symmetric positive definite matrix, then $A_{ii}(A^{-1})_{ii}\geq1$ for all $i=1,\ldots,n$. A proof is the following: we can write $A=PDP^T$ and $A^{-1}=PD^{-1}P^T$, where $D$ is diagonal and $P$ orthogonal. Then $$ A_{ii}=\sum_{l=1}^n P_{il}^2 D_{ll},\quad (A^{-1})_{ii}=\sum_{l=1}^n \frac{P_{il}^2}{D_{ll}}.$$ Then, by the Cauchy-Schwarz inequality and the fact that the rows of $P$ have norm $1$,  $$ A_{ii}(A^{-1})_{ii}=\left(\sum_{l=1}^n (P_{il}\sqrt{D_{ll}})^2\right)\left(\sum_{l=1}^n\left(\frac{P_{il}}{\sqrt{D_{ll}}}\right)^2\right)\geq \left(\sum_{l=1}^n P_{il}\sqrt{D_{ll}}\frac{P_{il}}{\sqrt{D_{ll}}}\right)^2=1.$$ My question is about when the equality holds. Looking at the above proof, we have to use the fact that there is equality in the Cauchy-Schwarz inequality if and only if the vectors are linearly dependent. So, $A_{ii}(A^{-1})_{ii}=1$ if and only if there is a $\lambda_i\in\mathbb{R}$ such that $$P_{il}\sqrt{D_{ll}}=\lambda_i \frac{P_{il}}{\sqrt{D_{ll}}}.$$ I think that, if $A_{ii}(A^{-1})_{ii}=1$ for all $i=1,\ldots,n$, then $A$ is diagonal, but I am not sure. The idea would be to prove that each row of $P$ has a component $1$ and the rest are zeros. So suppose that, for a row $i$, there are two columns $l_i$ and $k_i$ such that $P_{i,l_{i}}\neq0$ and $P_{i,k_{i}}\neq0$. Then $D_{l_i,l_i}=D_{k_i,k_i}=\lambda_i$, that is, $A$ has two eigenvalues equal. I do not know how to proceed and if this fact is important. Any ideas?","I know that, if $A$ is an $n\times n$ symmetric positive definite matrix, then $A_{ii}(A^{-1})_{ii}\geq1$ for all $i=1,\ldots,n$. A proof is the following: we can write $A=PDP^T$ and $A^{-1}=PD^{-1}P^T$, where $D$ is diagonal and $P$ orthogonal. Then $$ A_{ii}=\sum_{l=1}^n P_{il}^2 D_{ll},\quad (A^{-1})_{ii}=\sum_{l=1}^n \frac{P_{il}^2}{D_{ll}}.$$ Then, by the Cauchy-Schwarz inequality and the fact that the rows of $P$ have norm $1$,  $$ A_{ii}(A^{-1})_{ii}=\left(\sum_{l=1}^n (P_{il}\sqrt{D_{ll}})^2\right)\left(\sum_{l=1}^n\left(\frac{P_{il}}{\sqrt{D_{ll}}}\right)^2\right)\geq \left(\sum_{l=1}^n P_{il}\sqrt{D_{ll}}\frac{P_{il}}{\sqrt{D_{ll}}}\right)^2=1.$$ My question is about when the equality holds. Looking at the above proof, we have to use the fact that there is equality in the Cauchy-Schwarz inequality if and only if the vectors are linearly dependent. So, $A_{ii}(A^{-1})_{ii}=1$ if and only if there is a $\lambda_i\in\mathbb{R}$ such that $$P_{il}\sqrt{D_{ll}}=\lambda_i \frac{P_{il}}{\sqrt{D_{ll}}}.$$ I think that, if $A_{ii}(A^{-1})_{ii}=1$ for all $i=1,\ldots,n$, then $A$ is diagonal, but I am not sure. The idea would be to prove that each row of $P$ has a component $1$ and the rest are zeros. So suppose that, for a row $i$, there are two columns $l_i$ and $k_i$ such that $P_{i,l_{i}}\neq0$ and $P_{i,k_{i}}\neq0$. Then $D_{l_i,l_i}=D_{k_i,k_i}=\lambda_i$, that is, $A$ has two eigenvalues equal. I do not know how to proceed and if this fact is important. Any ideas?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
53,Connection between linearly independent vectors and projective points in general position,Connection between linearly independent vectors and projective points in general position,,"I'm trying to understand the connection between the notions of linear independence and general position . I have no background in geometry, so first I'll start with what I know and then I'll pose specific questions, please bear with me and correct me at any point. Let $q$ be a prime power, $d$ be a nonnegative integer, and $V$ be a $(d+1)$-dimensional vector space over the finite field $F_q$ with $q$ elements. For $v \in V$ denote  $$[v] = \left\{ cv \mid c \in F_q, c\neq 0 \right\}.$$ Then the collection of symbols $[v]$ can be seen as the points of  the $d$-dimensional projective space PG(d,q).  Furthermore, for a $(k+1)$-dimensional subspace $S$ of $V$, the set  $$\left\{ [s] \mid s \in S \right\}$$ is a $k$-flat of $PG(d,q)$ . From what I read in pg. 19 of these notes I assume that this definition of the notion of ""general position"" is correct: We say that $m$ points in $PG(d,q)$ are in general position if they   are not contained in any $(m-2)$-flat. So, to my understanding, the following statement is correct: The points $[v_1], \ldots, [v_m]$ of  $PG(d,q)$ are in general position   if and only if the vectors $v_1, \ldots, v_m$ are linearly   independent. My proof. The points $[v_1], \ldots, [v_m]$ are in general position iff they are not contained in any $(m-2)$-flat, which is true iff $v_1, \ldots, v_m$ are not contained in any $(m-1)$-dimensional subspace in $V$. This is the same as linear independence of $v_1, \ldots , v_m$. My questions are: Is the above statement correct? Are the preceding definitions accurate? PS. The reason for my confusion is that I've read different definitions for ""general position"" that I don't understand well, as well as discussions were people explain that general position is not equivalent to linear independence (which I thought my statement above implies). While I'm trying to understand and digest things, it would be very helpful to know if I got the above correctly.","I'm trying to understand the connection between the notions of linear independence and general position . I have no background in geometry, so first I'll start with what I know and then I'll pose specific questions, please bear with me and correct me at any point. Let $q$ be a prime power, $d$ be a nonnegative integer, and $V$ be a $(d+1)$-dimensional vector space over the finite field $F_q$ with $q$ elements. For $v \in V$ denote  $$[v] = \left\{ cv \mid c \in F_q, c\neq 0 \right\}.$$ Then the collection of symbols $[v]$ can be seen as the points of  the $d$-dimensional projective space PG(d,q).  Furthermore, for a $(k+1)$-dimensional subspace $S$ of $V$, the set  $$\left\{ [s] \mid s \in S \right\}$$ is a $k$-flat of $PG(d,q)$ . From what I read in pg. 19 of these notes I assume that this definition of the notion of ""general position"" is correct: We say that $m$ points in $PG(d,q)$ are in general position if they   are not contained in any $(m-2)$-flat. So, to my understanding, the following statement is correct: The points $[v_1], \ldots, [v_m]$ of  $PG(d,q)$ are in general position   if and only if the vectors $v_1, \ldots, v_m$ are linearly   independent. My proof. The points $[v_1], \ldots, [v_m]$ are in general position iff they are not contained in any $(m-2)$-flat, which is true iff $v_1, \ldots, v_m$ are not contained in any $(m-1)$-dimensional subspace in $V$. This is the same as linear independence of $v_1, \ldots , v_m$. My questions are: Is the above statement correct? Are the preceding definitions accurate? PS. The reason for my confusion is that I've read different definitions for ""general position"" that I don't understand well, as well as discussions were people explain that general position is not equivalent to linear independence (which I thought my statement above implies). While I'm trying to understand and digest things, it would be very helpful to know if I got the above correctly.",,"['linear-algebra', 'geometry', 'finite-fields', 'finite-geometry']"
54,"$\text{null}\,T^k\subsetneq\text{null}\,T^{k+1}$ and $\text{range}\,T^k\supsetneq\text{range}\,T^{k+1}$ for all $k\in\mathbb{N}$",and  for all,"\text{null}\,T^k\subsetneq\text{null}\,T^{k+1} \text{range}\,T^k\supsetneq\text{range}\,T^{k+1} k\in\mathbb{N}","Let $V$ be a vector space over $\mathbb{F}=\mathbb R$ or $\mathbb C$ and $T$ an operator on $V$. It is well known that $$\forall k\in\mathbb{N},\,\text{null}\,T^k\subseteq\text{null}\,T^{k+1}\,\land\,\text{range}\,T^{k+1}\supseteq\text{range}\,T^k$$ Exercise $21$ page $251$ in Sheldon Axler's Linear Algebra Done Right is: Find a vector space $W$ and $T\in\mathcal{L}(W)$ sich that $\text{null}\,T^k\subsetneq\text{null}\,T^{k+1}$ and $\text{range}\,T^k\supsetneq\text{range}\,T^{k+1}$ for every positive integer $k$. It is well known that if $\dim V=n$ is finite then $$\text{null}\,T^n=\text{null}\,T^{n+1}=\text{null}\,T^{n+2}=\cdots$$ and that $$\text{range}\,T^n=\text{range}\,T^{n+1}=\text{range}\,T^{n+2}=\cdots$$ Therefore we must choose an infinite dimensional vector space. I chose $W=\mathbb{F}^\infty$ the set of all sequences $(a_1,a_2,\cdots)$ over $\mathbb F$. Consider $\mathcal{B_c}=\{e_1,e_2,\cdots\}$ its canonical basis. When we define $T$ such as $T(a_1,a_2,\cdots)=(a_2,a_3,\cdots)$ we have $\forall k\in\mathbb{N}\backslash\{0\},\,\text{null}\,T^k=\text{span}\{e_1,\cdots,e_k\}\,\land\,\text{range}\,T^k=\mathbb F^\infty$, which satisfies only one condition. On the other hand, defining $T$ by $T(a_1,a_2,\cdots)=(0,a_1,a_2,\cdots)$ gives $\text{null}\,T^k=\{0\}\,\land\,\text{range}\,T^k=\text{span}\{e_{k+1},e_{k+2},\cdots\}$, $T$ satisfies the other. Projections are no good as $T=T^2$ and I tried some few other examples but I couldn't find a good one. The idea I kept in mind while looking for an example is that as we move on from $T^k$ to $T^{k+1}$, one vector is ""transported"" from $\text{range}\,T^k$ to  $\text{null}\,T^k$. Unfortunately, I failed to find such an operator. Could you please provide me with some examples?","Let $V$ be a vector space over $\mathbb{F}=\mathbb R$ or $\mathbb C$ and $T$ an operator on $V$. It is well known that $$\forall k\in\mathbb{N},\,\text{null}\,T^k\subseteq\text{null}\,T^{k+1}\,\land\,\text{range}\,T^{k+1}\supseteq\text{range}\,T^k$$ Exercise $21$ page $251$ in Sheldon Axler's Linear Algebra Done Right is: Find a vector space $W$ and $T\in\mathcal{L}(W)$ sich that $\text{null}\,T^k\subsetneq\text{null}\,T^{k+1}$ and $\text{range}\,T^k\supsetneq\text{range}\,T^{k+1}$ for every positive integer $k$. It is well known that if $\dim V=n$ is finite then $$\text{null}\,T^n=\text{null}\,T^{n+1}=\text{null}\,T^{n+2}=\cdots$$ and that $$\text{range}\,T^n=\text{range}\,T^{n+1}=\text{range}\,T^{n+2}=\cdots$$ Therefore we must choose an infinite dimensional vector space. I chose $W=\mathbb{F}^\infty$ the set of all sequences $(a_1,a_2,\cdots)$ over $\mathbb F$. Consider $\mathcal{B_c}=\{e_1,e_2,\cdots\}$ its canonical basis. When we define $T$ such as $T(a_1,a_2,\cdots)=(a_2,a_3,\cdots)$ we have $\forall k\in\mathbb{N}\backslash\{0\},\,\text{null}\,T^k=\text{span}\{e_1,\cdots,e_k\}\,\land\,\text{range}\,T^k=\mathbb F^\infty$, which satisfies only one condition. On the other hand, defining $T$ by $T(a_1,a_2,\cdots)=(0,a_1,a_2,\cdots)$ gives $\text{null}\,T^k=\{0\}\,\land\,\text{range}\,T^k=\text{span}\{e_{k+1},e_{k+2},\cdots\}$, $T$ satisfies the other. Projections are no good as $T=T^2$ and I tried some few other examples but I couldn't find a good one. The idea I kept in mind while looking for an example is that as we move on from $T^k$ to $T^{k+1}$, one vector is ""transported"" from $\text{range}\,T^k$ to  $\text{null}\,T^k$. Unfortunately, I failed to find such an operator. Could you please provide me with some examples?",,"['linear-algebra', 'linear-transformations']"
55,Relation between the eigenvalues of a matrix A and the eigenvalues of its hermitian and skew-hermitian parts,Relation between the eigenvalues of a matrix A and the eigenvalues of its hermitian and skew-hermitian parts,,"It is well know that every matrix $A\in M_n(\mathbb{C})$ can be decomposed in a unique way as a sum of its hermitian part $H$ and its skew-hermitian part $K$:  $$A=\frac{A+A^*}{2}+\frac{A-A^*}{2}$$ However, there is a relationship between the eigenvalues of $A$, $H$ and $K$ that I´m trying to prove without success: If the spectrums of $A, H, K$ are $\sigma(A)= \{ \lambda_1,\ldots,\lambda_n  \}$, $\sigma(H)= \{ \alpha_1,\ldots,\alpha_n \}$, $\sigma(K)= \{ \beta_1, \ldots, \beta_n  \},$ then $$\sum_{i=1}^n\mathopen|\lambda_i\mathclose|^2\leqslant \sum_{i=1}^n\mathopen|\alpha_i\mathclose|^2+ \sum_{i=1}^n\mathopen|\beta_i\mathclose|^2$$ With equality $\text{iff}$ $A$ is normal.","It is well know that every matrix $A\in M_n(\mathbb{C})$ can be decomposed in a unique way as a sum of its hermitian part $H$ and its skew-hermitian part $K$:  $$A=\frac{A+A^*}{2}+\frac{A-A^*}{2}$$ However, there is a relationship between the eigenvalues of $A$, $H$ and $K$ that I´m trying to prove without success: If the spectrums of $A, H, K$ are $\sigma(A)= \{ \lambda_1,\ldots,\lambda_n  \}$, $\sigma(H)= \{ \alpha_1,\ldots,\alpha_n \}$, $\sigma(K)= \{ \beta_1, \ldots, \beta_n  \},$ then $$\sum_{i=1}^n\mathopen|\lambda_i\mathclose|^2\leqslant \sum_{i=1}^n\mathopen|\alpha_i\mathclose|^2+ \sum_{i=1}^n\mathopen|\beta_i\mathclose|^2$$ With equality $\text{iff}$ $A$ is normal.",,"['linear-algebra', 'matrices']"
56,Extracting vector containing the elements of the main diagonal of a matrix [duplicate],Extracting vector containing the elements of the main diagonal of a matrix [duplicate],,"This question already has answers here : Mathematical expression to form a vector from diagonal elements (2 answers) Closed 7 years ago . Is there any mathematical operation that would extract the elements of the main diagonal as a vector? i.e. multiply it by certain vectors or something like that. I'm using this in the context of linear systems. In the specific case I'm looking at I have a relationship between the elements of three vectors as follows: $ \bf{a} = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \\ a_{4} \end{bmatrix} $ , $ \bf{b} = \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \\ b_{4} \end{bmatrix} $ , and $ \bf{c} = \begin{bmatrix} c_{1} \\ c_{2} \\ c_{3} \\ c_{4} \end{bmatrix} $ I also know that: $c_{i} = a_{i}b_{i} $ for $i \in [1, 4]$ Now I want to express this relationship as a vector equation. I understand that $\bf{a} \bf{b}^\top$ would give a square matrix with the elements of $\bf{c}$ on its main diagonal, but is there anyway to extract them as a vector? EDIT: Let me clarify a bit. If I multiply $\bf{a}$ by $\bf{b}^\top$ I get the following matrix: $\bf{a} \bf{b}^\top = \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} $ The elements which have been made bold are the ones I'm interested in extracting as a vector. This vector would be $\bf{c}$. If I multiply this by the all-ones vector, as some of the answers have suggested, I would get: $\bf{a} \bf{b}^\top \bf{1}= \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 \\ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 \\ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 \\ a_4b_1 + a_4b_2 + a_4b_3 + a_4b_4  \end{bmatrix}$ Which is not the vector I'm looking for (it isn't equal to $\bf{c}$). EDIT 2: Multiplying by the $\bf{1}$ vector would obviously work if all off diagonal elements become zero. So if anyone knows of a way to do that without modifying the elements of the main diagonal that would also answer my question. EDIT 3: The other question pointed out in the comments area is essentially the same and I have received similar answers but I was hoping for a simpler solution. I haven't marked it as duplicate to allow people to contribute in the future. I was hoping for a solution that would be linear in $\bf{b}$ which I would substitute in place of $\bf{c}$ into the equation I'm trying to solve. In that case $\bf{b}$ would be my only unknown and I would be able to get an algebraic solution.","This question already has answers here : Mathematical expression to form a vector from diagonal elements (2 answers) Closed 7 years ago . Is there any mathematical operation that would extract the elements of the main diagonal as a vector? i.e. multiply it by certain vectors or something like that. I'm using this in the context of linear systems. In the specific case I'm looking at I have a relationship between the elements of three vectors as follows: $ \bf{a} = \begin{bmatrix} a_{1} \\ a_{2} \\ a_{3} \\ a_{4} \end{bmatrix} $ , $ \bf{b} = \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3} \\ b_{4} \end{bmatrix} $ , and $ \bf{c} = \begin{bmatrix} c_{1} \\ c_{2} \\ c_{3} \\ c_{4} \end{bmatrix} $ I also know that: $c_{i} = a_{i}b_{i} $ for $i \in [1, 4]$ Now I want to express this relationship as a vector equation. I understand that $\bf{a} \bf{b}^\top$ would give a square matrix with the elements of $\bf{c}$ on its main diagonal, but is there anyway to extract them as a vector? EDIT: Let me clarify a bit. If I multiply $\bf{a}$ by $\bf{b}^\top$ I get the following matrix: $\bf{a} \bf{b}^\top = \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} $ The elements which have been made bold are the ones I'm interested in extracting as a vector. This vector would be $\bf{c}$. If I multiply this by the all-ones vector, as some of the answers have suggested, I would get: $\bf{a} \bf{b}^\top \bf{1}= \begin{bmatrix} \bf{a_1b_1} && a_1b_2 && a_1b_3 && a_1b_4 \\ a_2b_1 && \bf{a_2b_2} && a_2b_3 && a_2b_4 \\ a_3b_1 && a_3b_2 && \bf{a_3b_3} && a_3b_4 \\ a_4b_1 && a_4b_2 && a_4b_3 && \bf{a_4b_4}  \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 \\ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 \\ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 \\ a_4b_1 + a_4b_2 + a_4b_3 + a_4b_4  \end{bmatrix}$ Which is not the vector I'm looking for (it isn't equal to $\bf{c}$). EDIT 2: Multiplying by the $\bf{1}$ vector would obviously work if all off diagonal elements become zero. So if anyone knows of a way to do that without modifying the elements of the main diagonal that would also answer my question. EDIT 3: The other question pointed out in the comments area is essentially the same and I have received similar answers but I was hoping for a simpler solution. I haven't marked it as duplicate to allow people to contribute in the future. I was hoping for a solution that would be linear in $\bf{b}$ which I would substitute in place of $\bf{c}$ into the equation I'm trying to solve. In that case $\bf{b}$ would be my only unknown and I would be able to get an algebraic solution.",,"['linear-algebra', 'matrices', 'vectors', 'matrix-equations']"
57,Properties of a permutation matrix,Properties of a permutation matrix,,"Let $P$ be a permutation matrix, i.e. an $n \times n$ matrix consisting of $0$ and $1$ such that there is exactly one $1$ in every row and every column. I want to prove that there exists some $N > 0$ such that $P^N = I.$ I was given the recommendation that I should consider how there is only finitely many permutations. This suggests to me that I should consider the fact that if $N$ does exist, $N$ must be finite. However, I am considering going about this proof using an assumption for the sake of contradiction, such that $P^N = Q, Q \neq I$. I think the first step is proving that if $P$ is a permutation matrix, then $P^N$ is a permutation matrix for $N > 0.$ I imagine that I can do this inductivity by showing that $P^2$ is a permutation matrix. However, is this a bit of an unnecessary way to prove our lemma? Any suggestions would be appreciated.","Let $P$ be a permutation matrix, i.e. an $n \times n$ matrix consisting of $0$ and $1$ such that there is exactly one $1$ in every row and every column. I want to prove that there exists some $N > 0$ such that $P^N = I.$ I was given the recommendation that I should consider how there is only finitely many permutations. This suggests to me that I should consider the fact that if $N$ does exist, $N$ must be finite. However, I am considering going about this proof using an assumption for the sake of contradiction, such that $P^N = Q, Q \neq I$. I think the first step is proving that if $P$ is a permutation matrix, then $P^N$ is a permutation matrix for $N > 0.$ I imagine that I can do this inductivity by showing that $P^2$ is a permutation matrix. However, is this a bit of an unnecessary way to prove our lemma? Any suggestions would be appreciated.",,"['linear-algebra', 'matrices', 'permutation-matrices']"
58,Prove that if $I-BA$ is invertible then $I-AB$ is invertible [duplicate],Prove that if  is invertible then  is invertible [duplicate],I-BA I-AB,"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . Prove that if $I-BA$ is invertible then $I-AB$ is invertible. Though I have found this question already posted and  also it has some answers like Use :$(I−BA)(I+B(I−AB)^{−1}A)=I(I−BA)(I+B(I−AB)^{−1}A)=I $ I have done it like this. $I-BA$ is invertible $\implies 0$ is not an eigen value of $I-BA\implies 1$  is not an eigen value of $BA\implies 1$ is not an eigen value of $AB\implies 0$ is not an eigen value of $I-AB\implies I-AB$ is invertible. I have used the facts the $AB,BA$ have same non-zero eigen values Proof :Let $c\neq 0$ be a eigen value of $AB$ corresponding to eigen vector $\alpha$.Then $A(B\alpha)=c\alpha$ Now $(BA)(B\alpha)=B(AB\alpha)=c(B\alpha)\implies c$ is an eigen vector of $BA$ corresponding to $B\alpha$.Also $B\alpha\neq 0$ otherwise $c=0$ . Similarly every eigen value of $BA$ is an eigen value of $AB$. How to show that they have same eigen values for if $A,B$ are $n\times n$ matrices? and If $c$ is an eigen value of a matrix $M$, then $1-c$ is an eigen value of $I-M$. Please check whether my answer is correct/not.","This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . Prove that if $I-BA$ is invertible then $I-AB$ is invertible. Though I have found this question already posted and  also it has some answers like Use :$(I−BA)(I+B(I−AB)^{−1}A)=I(I−BA)(I+B(I−AB)^{−1}A)=I $ I have done it like this. $I-BA$ is invertible $\implies 0$ is not an eigen value of $I-BA\implies 1$  is not an eigen value of $BA\implies 1$ is not an eigen value of $AB\implies 0$ is not an eigen value of $I-AB\implies I-AB$ is invertible. I have used the facts the $AB,BA$ have same non-zero eigen values Proof :Let $c\neq 0$ be a eigen value of $AB$ corresponding to eigen vector $\alpha$.Then $A(B\alpha)=c\alpha$ Now $(BA)(B\alpha)=B(AB\alpha)=c(B\alpha)\implies c$ is an eigen vector of $BA$ corresponding to $B\alpha$.Also $B\alpha\neq 0$ otherwise $c=0$ . Similarly every eigen value of $BA$ is an eigen value of $AB$. How to show that they have same eigen values for if $A,B$ are $n\times n$ matrices? and If $c$ is an eigen value of a matrix $M$, then $1-c$ is an eigen value of $I-M$. Please check whether my answer is correct/not.",,"['linear-algebra', 'matrices', 'proof-verification']"
59,Vector Multiplication with Multiple Kronecker Products,Vector Multiplication with Multiple Kronecker Products,,"My question concerns matrix-vector multiplications when your matrix has Kronecker structure, which can be done faster in that case. I know how to compute this for a matrix $A = A_1 \otimes A_2$, which has two components $A_i$: $$Ax = (A_1 \otimes A_2)x = (A_1 \otimes A_2)vec(X) = vec(A_2XA_1^T)$$ where $vec(X) = x$ is the vectorization of $X$. However, I have no idea how to proceed for more components $A_i$. I can imagine doing something as follows: $$Ax = (A_1 \otimes A_2 \otimes \cdots \otimes A_n)x = vec((A_2 \otimes \cdots A_n)XA_1^T) = (I_m \otimes A_2 \otimes \cdots \otimes A_n)vec(XA_1^T)$$ which provides me again with an actual matrix-vector multiplication. I was hoping to get a large identity matrix on the left-hand side this way, but no luck. ( EDIT : I realised it is impossible to do it this way, as the matrices $X$ and $A_1^T$ do not have the same dimensions.) I tried looking it up on-line, but I mainly get the case for two components. Can anyone of you help me out? Thanks!","My question concerns matrix-vector multiplications when your matrix has Kronecker structure, which can be done faster in that case. I know how to compute this for a matrix $A = A_1 \otimes A_2$, which has two components $A_i$: $$Ax = (A_1 \otimes A_2)x = (A_1 \otimes A_2)vec(X) = vec(A_2XA_1^T)$$ where $vec(X) = x$ is the vectorization of $X$. However, I have no idea how to proceed for more components $A_i$. I can imagine doing something as follows: $$Ax = (A_1 \otimes A_2 \otimes \cdots \otimes A_n)x = vec((A_2 \otimes \cdots A_n)XA_1^T) = (I_m \otimes A_2 \otimes \cdots \otimes A_n)vec(XA_1^T)$$ which provides me again with an actual matrix-vector multiplication. I was hoping to get a large identity matrix on the left-hand side this way, but no luck. ( EDIT : I realised it is impossible to do it this way, as the matrices $X$ and $A_1^T$ do not have the same dimensions.) I tried looking it up on-line, but I mainly get the case for two components. Can anyone of you help me out? Thanks!",,"['linear-algebra', 'matrices', 'kronecker-product']"
60,How to prove the eigenvalues of this matrix $A$ come in pairs $\pm\lambda$?,How to prove the eigenvalues of this matrix  come in pairs ?,A \pm\lambda,"This is a branched question of my previous question What's the condition for a matrix $A$ ($2N\times 2N$ dimension) to have eigenvalues in pairs $\pm\lambda$? . Let's look at this $2N\times2N$ matrix $A$: $A=\left(\begin{array}{cc} B & C\\ -C^{*} & -B^{*} \end{array}\right)$, where $B$ is a hermitian matrix, $C$ is a complex symmetric matrix, and $M^\ast$ denotes the complex conjugate (but not the conjugate transpose) of a matrix $M$. If $A$ is a $2\times2$ matrix, the eigenvalues are obviously in pairs, and I also proved the $4\times4$ case by showing the determinant of $A-\lambda I$ only depends on $\lambda^{2}$. Then I use Mathematica to calculate the eigenvalues of this matrix with randomly generated matrices $B$ and $C$ for higher dimension cases, the eigenvalues also come in pairs. So I guess this matrix always has eigenvalues in pairs, but I don't know how to prove it analytically.","This is a branched question of my previous question What's the condition for a matrix $A$ ($2N\times 2N$ dimension) to have eigenvalues in pairs $\pm\lambda$? . Let's look at this $2N\times2N$ matrix $A$: $A=\left(\begin{array}{cc} B & C\\ -C^{*} & -B^{*} \end{array}\right)$, where $B$ is a hermitian matrix, $C$ is a complex symmetric matrix, and $M^\ast$ denotes the complex conjugate (but not the conjugate transpose) of a matrix $M$. If $A$ is a $2\times2$ matrix, the eigenvalues are obviously in pairs, and I also proved the $4\times4$ case by showing the determinant of $A-\lambda I$ only depends on $\lambda^{2}$. Then I use Mathematica to calculate the eigenvalues of this matrix with randomly generated matrices $B$ and $C$ for higher dimension cases, the eigenvalues also come in pairs. So I guess this matrix always has eigenvalues in pairs, but I don't know how to prove it analytically.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
61,"What is the spectral radius of $PBD$ with $P$ projection, $\|B\|_\infty=1$, and $D$ diagonal with $\|D\|_2<1$?","What is the spectral radius of  with  projection, , and  diagonal with ?",PBD P \|B\|_\infty=1 D \|D\|_2<1,"Assume we have the matrix product: $$A=PBD$$ where $P$ is a projection matrix (i.e., $P=P^2$, $P=P^\top$, and $\|P\|_2=1$), $B$ is a matrix whose infinite norm is equal to one ($\|B\|_\infty=1$), and $D$ is a diagonal matrix whose $\ell_2$-norm is less than one ($\|D\|_2<1$). Is it correct to say that the spectral radius of $A$ is less than 1 ($\rho(A)< 1$). If yes, how to prove it?","Assume we have the matrix product: $$A=PBD$$ where $P$ is a projection matrix (i.e., $P=P^2$, $P=P^\top$, and $\|P\|_2=1$), $B$ is a matrix whose infinite norm is equal to one ($\|B\|_\infty=1$), and $D$ is a diagonal matrix whose $\ell_2$-norm is less than one ($\|D\|_2<1$). Is it correct to say that the spectral radius of $A$ is less than 1 ($\rho(A)< 1$). If yes, how to prove it?",,"['linear-algebra', 'matrices']"
62,In which sense is composition a tensor product,In which sense is composition a tensor product,,"Let $\Phi\colon U\to V$ and $\Psi\colon V \to W$ be linear operators, and consider their composition  $$ \Psi\circ \Phi $$ The operation, $$\circ:\mathcal{L}(U,V)\times\mathcal{L}(V,W)\to \mathcal{L}(U,W)\\ (\Phi,\Psi)\mapsto \Psi\circ \Phi $$ is bilinear. So I expect that we can understand $\Psi$ and $\Phi$ identified (by $\iota$) within a tensor space $T$ such that $$ \Psi\circ\Phi= \iota(\Psi \otimes \Phi)\ . $$ However, I cannot figure out what $T$ should be.","Let $\Phi\colon U\to V$ and $\Psi\colon V \to W$ be linear operators, and consider their composition  $$ \Psi\circ \Phi $$ The operation, $$\circ:\mathcal{L}(U,V)\times\mathcal{L}(V,W)\to \mathcal{L}(U,W)\\ (\Phi,\Psi)\mapsto \Psi\circ \Phi $$ is bilinear. So I expect that we can understand $\Psi$ and $\Phi$ identified (by $\iota$) within a tensor space $T$ such that $$ \Psi\circ\Phi= \iota(\Psi \otimes \Phi)\ . $$ However, I cannot figure out what $T$ should be.",,"['linear-algebra', 'operator-theory', 'tensor-products', 'multilinear-algebra']"
63,Interpretation of SVD for non-square matrices.,Interpretation of SVD for non-square matrices.,,"I was reading the Wikipedia article on Singular Value Decomposition . It shows a nice visualisation where the SVD of a matrix $M = U\Sigma V^*$ allows us interpret M as a rotation $V^*$, followed by a scaling $\Sigma$, and a second and final rotation $U$. That makes perfect sense when $M$ is a square. Now, how can I interpret the decomposition of a non-square matrix? If I take as a simple example a real row matrix $M$, then $U$ always seems to be a scalar equal to 1 or -1 (which also makes total sense), and $\Sigma$ has a single non-zero entry, namely the first SV. And what does the matrix $V^*$ represent?","I was reading the Wikipedia article on Singular Value Decomposition . It shows a nice visualisation where the SVD of a matrix $M = U\Sigma V^*$ allows us interpret M as a rotation $V^*$, followed by a scaling $\Sigma$, and a second and final rotation $U$. That makes perfect sense when $M$ is a square. Now, how can I interpret the decomposition of a non-square matrix? If I take as a simple example a real row matrix $M$, then $U$ always seems to be a scalar equal to 1 or -1 (which also makes total sense), and $\Sigma$ has a single non-zero entry, namely the first SV. And what does the matrix $V^*$ represent?",,"['linear-algebra', 'svd']"
64,Why is the Jacobian matrix equal to the matrix associated to a linear transformation?,Why is the Jacobian matrix equal to the matrix associated to a linear transformation?,,"Given the linear transformation $f$, we can construct the matrix $A$ as follows: on the $i$-th column we put the vector $f(\mathbf e_i)$ where $E = (\mathbf e_1, \ldots, \mathbf e_n)$ is a basis of $\mathbb R^n$. Now, I read that for linear transformations, if we use the canonical basis of $\mathbb R^n$, the matrix $A$ is equal to $J f$ (the Jacobian matrix associated to $f$). It's indeed so for the few examples that I tried, but I cannot find a proof of this fact.","Given the linear transformation $f$, we can construct the matrix $A$ as follows: on the $i$-th column we put the vector $f(\mathbf e_i)$ where $E = (\mathbf e_1, \ldots, \mathbf e_n)$ is a basis of $\mathbb R^n$. Now, I read that for linear transformations, if we use the canonical basis of $\mathbb R^n$, the matrix $A$ is equal to $J f$ (the Jacobian matrix associated to $f$). It's indeed so for the few examples that I tried, but I cannot find a proof of this fact.",,"['linear-algebra', 'matrices']"
65,Unique Positive Definite Square Root of a Positive Definite Matrix,Unique Positive Definite Square Root of a Positive Definite Matrix,,"If $A$ be an $n\times n$ positive definite matrix, then there exists a unique positive definite matrix $B$ such that $B^2=A$. My question is how to get this $B$. What is the name of the algorithm for finding $B$?","If $A$ be an $n\times n$ positive definite matrix, then there exists a unique positive definite matrix $B$ such that $B^2=A$. My question is how to get this $B$. What is the name of the algorithm for finding $B$?",,"['linear-algebra', 'matrices', 'algorithms', 'self-learning']"
66,Proving an eigenspace is a T-Invariant subspace.,Proving an eigenspace is a T-Invariant subspace.,,"I want to know if I'm going about this proof the correct way. Problem Statement: Let $T$ be a linear operator on a vector space $V$, and let $λ$ be a scalar. The eigenspace $V^{(λ)}$ is the set of eigenvectors of $T$ with eigenvalue $λ$, together with $\textbf{0}$. Prove that $V^{(λ)}$ is a $T$-invariant subspace. So I need to show that $T(V^{(λ)})\subseteq V^{(λ)}$. Since $V^{(λ)}$ is the set of eigenvectors of the matrix $T$ corresponding to $λ$, that meants that for any $\textbf{v}\in V^{(λ)}$, we have $T\textbf{v}=λ\textbf{v}$. Clearly, $T\textbf{v}\in T(V^{(λ)})$. Then we know that for any $\textbf{v}\in V^{(λ)}$, $\text{span}(\textbf{v})\in V^{(λ)}$. Thus, $λ\textbf{v}\in V^{(λ)}$. Since $T\textbf{v}=λ\textbf{v}$, then $T\textbf{v}\in V^{(λ)}$. So then $T(V^{(λ)})\subseteq V^{(λ)}$?","I want to know if I'm going about this proof the correct way. Problem Statement: Let $T$ be a linear operator on a vector space $V$, and let $λ$ be a scalar. The eigenspace $V^{(λ)}$ is the set of eigenvectors of $T$ with eigenvalue $λ$, together with $\textbf{0}$. Prove that $V^{(λ)}$ is a $T$-invariant subspace. So I need to show that $T(V^{(λ)})\subseteq V^{(λ)}$. Since $V^{(λ)}$ is the set of eigenvectors of the matrix $T$ corresponding to $λ$, that meants that for any $\textbf{v}\in V^{(λ)}$, we have $T\textbf{v}=λ\textbf{v}$. Clearly, $T\textbf{v}\in T(V^{(λ)})$. Then we know that for any $\textbf{v}\in V^{(λ)}$, $\text{span}(\textbf{v})\in V^{(λ)}$. Thus, $λ\textbf{v}\in V^{(λ)}$. Since $T\textbf{v}=λ\textbf{v}$, then $T\textbf{v}\in V^{(λ)}$. So then $T(V^{(λ)})\subseteq V^{(λ)}$?",,"['linear-algebra', 'abstract-algebra', 'proof-verification', 'eigenvalues-eigenvectors', 'invariant-theory']"
67,Order of the group of integer orthogonal matrices,Order of the group of integer orthogonal matrices,,"Let $O_n(\mathbb Z)$ be the group of orthogonal matrices (matrices $B$ s.t. $BB^T=I$ ) with entries in $\mathbb Z$ . 1) How do I show that $O_n(\mathbb Z)$ is a finite group and find its order? 2) I need to show also that symmetric group $S_n$ is a subgroup of $O_n(\mathbb Z)$ . So it needs to satisfy associativity/identity/inverse. It is easy to see that every orthogonal matrix $A \in O(\mathbb Z)$ has an inverse, namely $A^T$ . Moreover, the product of two orthogonal matrices is orthogonal since $(AB)^T = B^T A^T$ .  If $A, B \in O_n(\mathbb Z)$ then $(AB)^T(AB) = B^T A^T AB = BIB^T = BB^T = I$ , hence $O_n(\mathbb Z)$ is closed under multiplication, since $I \in O_n(\mathbb Z)$ .","Let be the group of orthogonal matrices (matrices s.t. ) with entries in . 1) How do I show that is a finite group and find its order? 2) I need to show also that symmetric group is a subgroup of . So it needs to satisfy associativity/identity/inverse. It is easy to see that every orthogonal matrix has an inverse, namely . Moreover, the product of two orthogonal matrices is orthogonal since .  If then , hence is closed under multiplication, since .","O_n(\mathbb Z) B BB^T=I \mathbb Z O_n(\mathbb Z) S_n O_n(\mathbb Z) A \in O(\mathbb Z) A^T (AB)^T = B^T A^T A, B \in O_n(\mathbb Z) (AB)^T(AB) = B^T A^T AB = BIB^T = BB^T = I O_n(\mathbb Z) I \in O_n(\mathbb Z)","['linear-algebra', 'matrices', 'group-theory', 'finite-groups', 'orthogonal-matrices']"
68,Definition of annihilator is not clear,Definition of annihilator is not clear,,"I have a question regarding the following definition of an annihilator of a finite dimensional vector space. I think I understand the two definitions but I don't really get the link implied by the last sentence. Definition: If $A \subset V$, the annihilator of $A, A^{°}$, is the set of all $f$ in $V^*$ such that $f(a) = 0$ for all $a$ in $A$. Similarly, if $A \subset V^*$, then $$ A^{°} = \{a\in V\colon\ f(a) = 0 \text{ for all } f\in A \}. $$ If we view $V$ as $(V^{*})^{*}$, the second definition is included in the first. So now my questions are why may we view $V$ as $(V^{*})^{*}$, I know they're equivalent by the isomorphism but this doesn't mean that they're equal, does it? And how does the first definition include the second one?","I have a question regarding the following definition of an annihilator of a finite dimensional vector space. I think I understand the two definitions but I don't really get the link implied by the last sentence. Definition: If $A \subset V$, the annihilator of $A, A^{°}$, is the set of all $f$ in $V^*$ such that $f(a) = 0$ for all $a$ in $A$. Similarly, if $A \subset V^*$, then $$ A^{°} = \{a\in V\colon\ f(a) = 0 \text{ for all } f\in A \}. $$ If we view $V$ as $(V^{*})^{*}$, the second definition is included in the first. So now my questions are why may we view $V$ as $(V^{*})^{*}$, I know they're equivalent by the isomorphism but this doesn't mean that they're equal, does it? And how does the first definition include the second one?",,"['linear-algebra', 'duality-theorems']"
69,$AXB$ sort of decomposition?,sort of decomposition?,AXB,"Let $f: M_n(\mathbb{C}) \to M_n(\mathbb{C})$ be a $\mathbb{C}$-linear map (not necessarily an algebra homomorphism). Do there exist matrices $A_1, \dots, A_d \in M_n(\mathbb{C})$ and $B_1 \dots, B_d \in M_n(\mathbb{C})$ such that $$f(X) = \sum_{j = 1}^d A_jXB_j,\text{ }\forall\,X \in M_n(\mathbb{C})?$$","Let $f: M_n(\mathbb{C}) \to M_n(\mathbb{C})$ be a $\mathbb{C}$-linear map (not necessarily an algebra homomorphism). Do there exist matrices $A_1, \dots, A_d \in M_n(\mathbb{C})$ and $B_1 \dots, B_d \in M_n(\mathbb{C})$ such that $$f(X) = \sum_{j = 1}^d A_jXB_j,\text{ }\forall\,X \in M_n(\mathbb{C})?$$",,[]
70,Diagonalize the matrix A (complex numbers),Diagonalize the matrix A (complex numbers),,"Diagonalize the matrix A $A=\begin{pmatrix}1 & 2 & 4 \\3 & 5 & 2 \\2 & 6 & 1\end{pmatrix}$ So, i began the problem by finding the characteristic polynomial which was $λ^3-7λ^2-15λ-27$ using long division i got $(λ-9)(λ^2+2λ+3)$ so i used the quadratic formula and got $λ_1=-1+i\sqrt{2}$ and $λ_2=-1-i\sqrt{2}$ and $λ_3=9$ I decided to start with $λ_1$ $A-\left(-1+i\sqrt{2}\right)I=\begin{pmatrix}2-i\sqrt{2} & 2 & 4 \\3 & 6-i\sqrt{2} & 2 \\2 & 6 & 2-i\sqrt{2}\end{pmatrix}$ Now i understand how to diagonalize when i have all numbers but once i get these $i$'s in the equation it's like my brain doesn't comprehend the steps i need to take the get the diagonal form.. I would like to think i'm supposed to start by getting the inverse of this new matrix but fail to see how to do that with $i$'s involved.","Diagonalize the matrix A $A=\begin{pmatrix}1 & 2 & 4 \\3 & 5 & 2 \\2 & 6 & 1\end{pmatrix}$ So, i began the problem by finding the characteristic polynomial which was $λ^3-7λ^2-15λ-27$ using long division i got $(λ-9)(λ^2+2λ+3)$ so i used the quadratic formula and got $λ_1=-1+i\sqrt{2}$ and $λ_2=-1-i\sqrt{2}$ and $λ_3=9$ I decided to start with $λ_1$ $A-\left(-1+i\sqrt{2}\right)I=\begin{pmatrix}2-i\sqrt{2} & 2 & 4 \\3 & 6-i\sqrt{2} & 2 \\2 & 6 & 2-i\sqrt{2}\end{pmatrix}$ Now i understand how to diagonalize when i have all numbers but once i get these $i$'s in the equation it's like my brain doesn't comprehend the steps i need to take the get the diagonal form.. I would like to think i'm supposed to start by getting the inverse of this new matrix but fail to see how to do that with $i$'s involved.",,['linear-algebra']
71,Understanding of non-degeneracy and inner product,Understanding of non-degeneracy and inner product,,"My class version of the non-degeneracy definition states: Let $V$ be a vector space over a field $\Bbb F$, equipped with a symmetric bilinear form $b : V \times V → \Bbb F$ . Then $b$ is a non-degenerate bilinear form if $~∀\,u∈V,\ b(v,u)=0 \implies v=0$. I find myself confused when I try to apply it to proving all inner products are non-degenerate, and here is my confusion: Given an inner product $b$. I let $u \in V$ and assume $b(v,u)=0$ and I am trying to show $v=0$. I saw on a few texts that we are allowed to pick $v$ so that $v=u$ and thus we get $v=0$ from positivity. Why are we allowed to choose the value for $v$? I thought $v$ could just be any arbitrary vector and it doesn't have to happen to be $u$. To make it clearer, it doesn't make sense to me that $b(v,u)=0\Rightarrow v=u$ whatsoever. In my mind $v$ is fixed and does not vary with $u$. Could someone kindly explain what is wrong with my reasoning?","My class version of the non-degeneracy definition states: Let $V$ be a vector space over a field $\Bbb F$, equipped with a symmetric bilinear form $b : V \times V → \Bbb F$ . Then $b$ is a non-degenerate bilinear form if $~∀\,u∈V,\ b(v,u)=0 \implies v=0$. I find myself confused when I try to apply it to proving all inner products are non-degenerate, and here is my confusion: Given an inner product $b$. I let $u \in V$ and assume $b(v,u)=0$ and I am trying to show $v=0$. I saw on a few texts that we are allowed to pick $v$ so that $v=u$ and thus we get $v=0$ from positivity. Why are we allowed to choose the value for $v$? I thought $v$ could just be any arbitrary vector and it doesn't have to happen to be $u$. To make it clearer, it doesn't make sense to me that $b(v,u)=0\Rightarrow v=u$ whatsoever. In my mind $v$ is fixed and does not vary with $u$. Could someone kindly explain what is wrong with my reasoning?",,"['linear-algebra', 'inner-products']"
72,Name of Jordan Canonical Form in infinite dimensions?,Name of Jordan Canonical Form in infinite dimensions?,,"I tend to think of Jordan canonical form as the generalized spectrum theorem.  I read it as saying, every matrix cannot be diagonalized, but they can be ""jordanized"".  In functional, I've seen the spectral theorem again.  The situation there however is much more complex.  What does the Jordan canonical form go by in infinite dimensions, or does it really not hold there well?  Maybe just Hilbert Spaces would be easiest to consider so I can get a feeling for what is out there. Thanks!","I tend to think of Jordan canonical form as the generalized spectrum theorem.  I read it as saying, every matrix cannot be diagonalized, but they can be ""jordanized"".  In functional, I've seen the spectral theorem again.  The situation there however is much more complex.  What does the Jordan canonical form go by in infinite dimensions, or does it really not hold there well?  Maybe just Hilbert Spaces would be easiest to consider so I can get a feeling for what is out there. Thanks!",,"['linear-algebra', 'functional-analysis']"
73,What does this vector notation really mean?,What does this vector notation really mean?,,"With regard to vectors, how is this (form 1): $$\begin{bmatrix}1\\2\\-1\end{bmatrix}$$ Different to this (form 2):  $$\begin{bmatrix}1\ 2\ -1 \end{bmatrix}$$ I would think that the first set consists of magnitudes of the same variable (where simply the sum gives the magnitude), while the second refers to the coefficients of three different variables. In my book they take the magnitude of form 1 by $$\sqrt{1^2+2^2+(-1)^2}$$ That would indicate a set of vectors in different directions. So my question is; which of the two (rows or columns) give an indication of the variable or the vector direction / dimension?","With regard to vectors, how is this (form 1): $$\begin{bmatrix}1\\2\\-1\end{bmatrix}$$ Different to this (form 2):  $$\begin{bmatrix}1\ 2\ -1 \end{bmatrix}$$ I would think that the first set consists of magnitudes of the same variable (where simply the sum gives the magnitude), while the second refers to the coefficients of three different variables. In my book they take the magnitude of form 1 by $$\sqrt{1^2+2^2+(-1)^2}$$ That would indicate a set of vectors in different directions. So my question is; which of the two (rows or columns) give an indication of the variable or the vector direction / dimension?",,"['linear-algebra', 'vectors']"
74,what does it by raising a matrix to the power of $1/2$?,what does it by raising a matrix to the power of ?,1/2,"I came across the following which I did not understand at all. Let $A$ be a positive semi-definite. If $A(I-B)$ is positive definite, then the eigenvalues of $$A^{1/2}(I-B)A^{-1/2} = I -A^{1/2}BA^{-1/2}\tag 1$$ are all positive, which implies: $$\text {the eigenvalues of} \ B \ \text{are real and less than 1} \tag 2$$ (i) What are $A^{1/2}$ and $A^{-1/2}$ in (1)? (ii) How does the positive definiteness of $A(I-B)$ imply the eigenvalues of (1) are all positive? (iii) Also, why does (1) imply (2)? I tried to figure these out by myself, I could not find a good reference to look up. Any help would be greatly appreciated!","I came across the following which I did not understand at all. Let $A$ be a positive semi-definite. If $A(I-B)$ is positive definite, then the eigenvalues of $$A^{1/2}(I-B)A^{-1/2} = I -A^{1/2}BA^{-1/2}\tag 1$$ are all positive, which implies: $$\text {the eigenvalues of} \ B \ \text{are real and less than 1} \tag 2$$ (i) What are $A^{1/2}$ and $A^{-1/2}$ in (1)? (ii) How does the positive definiteness of $A(I-B)$ imply the eigenvalues of (1) are all positive? (iii) Also, why does (1) imply (2)? I tried to figure these out by myself, I could not find a good reference to look up. Any help would be greatly appreciated!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
75,Intuition about $v\otimes w$,Intuition about,v\otimes w,"In Physics and Differential Geometry usually tensors of type $(k,l)$ on a vector space $V$ over $\mathbb{F}$ are defined as multilinear functions $$f : \underbrace{V\times\cdots\times V}_{k \ \mathrm{terms}}\times\underbrace{V^\ast\times\cdots\times V^\ast}_{l \ \mathrm{terms}}\to\mathbb{F}$$ this makes it quite simple to gather some understanding of $(k,0)$ tensors from one intuitive point of view. They are just $k$-linear functions of vectors and can be used like linear functions of vectors or like inner products and so on. Also it is not hard to see why one would care about these. Now, on the other hand, tensors of type $(0,l)$ also appear in Physics quite frequently. Indeed Maxwell's Stress Tensor is: $$\mathcal T = \epsilon_0\left[ \mathbf{E}\otimes\mathbf{E}+c^2\mathbf{B}\otimes\mathbf{B} -\frac12\sum_i\mathbf{e}_i\otimes\mathbf{e}_i\left(E^2+c^2 B^2\right) \right].$$ Those objects are not much intuitive IMHO. First, a tensor of type $(0,l)$ is a function of linear functionals in this approach, and this makes it a little bit harder to make sense from a physical and geometrical point of view. The other possible approach to tensors is the one based on the universal property. As far as I understand, the basic idea of this approach is that in the end the construction (with quotient spaces and so on) shows that there exists a way to make sense of the product $v_1\otimes\cdots \otimes v_k$ and that it has all the nice properties we would want. In that case, a tensor of type $(0,l)$ as defined above is an element of $V\otimes\cdots\otimes V$. Of course to understand those objects, it suffices to understand for $v,w\in V$ how $v\otimes w$ can be understood. So my question is: I know the constructions are isomorphic and I know from a rigorous point of view what $v\otimes w$ is, now how can one intuitively make sense of $v\otimes w$? Again, thinking of it as a function of linear functionals doesn't seem much intuitive. So, just regarding it as an element of $V\otimes V$ how can we give to it some geometric and physical intuition? The object $v\wedge w \in V\wedge V$ has one nice way to be understood: it can be thought of as the paralelogram generated by $v$ and $w$, that is one oriented area in the same way as $v$ and $w$ are oriented segments. Now, is there a nice way to understand $v\otimes w$ too?","In Physics and Differential Geometry usually tensors of type $(k,l)$ on a vector space $V$ over $\mathbb{F}$ are defined as multilinear functions $$f : \underbrace{V\times\cdots\times V}_{k \ \mathrm{terms}}\times\underbrace{V^\ast\times\cdots\times V^\ast}_{l \ \mathrm{terms}}\to\mathbb{F}$$ this makes it quite simple to gather some understanding of $(k,0)$ tensors from one intuitive point of view. They are just $k$-linear functions of vectors and can be used like linear functions of vectors or like inner products and so on. Also it is not hard to see why one would care about these. Now, on the other hand, tensors of type $(0,l)$ also appear in Physics quite frequently. Indeed Maxwell's Stress Tensor is: $$\mathcal T = \epsilon_0\left[ \mathbf{E}\otimes\mathbf{E}+c^2\mathbf{B}\otimes\mathbf{B} -\frac12\sum_i\mathbf{e}_i\otimes\mathbf{e}_i\left(E^2+c^2 B^2\right) \right].$$ Those objects are not much intuitive IMHO. First, a tensor of type $(0,l)$ is a function of linear functionals in this approach, and this makes it a little bit harder to make sense from a physical and geometrical point of view. The other possible approach to tensors is the one based on the universal property. As far as I understand, the basic idea of this approach is that in the end the construction (with quotient spaces and so on) shows that there exists a way to make sense of the product $v_1\otimes\cdots \otimes v_k$ and that it has all the nice properties we would want. In that case, a tensor of type $(0,l)$ as defined above is an element of $V\otimes\cdots\otimes V$. Of course to understand those objects, it suffices to understand for $v,w\in V$ how $v\otimes w$ can be understood. So my question is: I know the constructions are isomorphic and I know from a rigorous point of view what $v\otimes w$ is, now how can one intuitively make sense of $v\otimes w$? Again, thinking of it as a function of linear functionals doesn't seem much intuitive. So, just regarding it as an element of $V\otimes V$ how can we give to it some geometric and physical intuition? The object $v\wedge w \in V\wedge V$ has one nice way to be understood: it can be thought of as the paralelogram generated by $v$ and $w$, that is one oriented area in the same way as $v$ and $w$ are oriented segments. Now, is there a nice way to understand $v\otimes w$ too?",,"['linear-algebra', 'vector-spaces', 'intuition', 'tensor-products', 'tensors']"
76,Why is reduced echelon form of a matrix always the identity matrix,Why is reduced echelon form of a matrix always the identity matrix,,"If $A$ is $3\times3$ matrix with linearly independent columns, then why is the reduced echelon form the identity matrix?","If $A$ is $3\times3$ matrix with linearly independent columns, then why is the reduced echelon form the identity matrix?",,"['linear-algebra', 'matrices']"
77,QR Decomposition Interpretation,QR Decomposition Interpretation,,"What is exact relationship between matrix R and input matrix A in QR factorization? Say, R gives the structure of A or R is a representation of A. How? We have Q'A = R. Does it mean A is projected to the subspcae of Q? If so, is R a representation of A in another space?","What is exact relationship between matrix R and input matrix A in QR factorization? Say, R gives the structure of A or R is a representation of A. How? We have Q'A = R. Does it mean A is projected to the subspcae of Q? If so, is R a representation of A in another space?",,"['linear-algebra', 'matrices', 'matrix-decomposition']"
78,linear functionals linearly independent,linear functionals linearly independent,,"Let $V$ be a vector space with $\dim V=n$. Let $\varphi_1,...,\varphi_n $ be linear functionals that are not $0$. Prove that $\varphi_1,...,\varphi_n $ are linearly independent if and only if $\cap_{i=1}^n \ker \varphi_i = \{0\}$. $\\$ I succeeded to prove that if they are linearly independent then the intersection is zero but I have no idea how to prove the other direction. Any suggustion? thanks!","Let $V$ be a vector space with $\dim V=n$. Let $\varphi_1,...,\varphi_n $ be linear functionals that are not $0$. Prove that $\varphi_1,...,\varphi_n $ are linearly independent if and only if $\cap_{i=1}^n \ker \varphi_i = \{0\}$. $\\$ I succeeded to prove that if they are linearly independent then the intersection is zero but I have no idea how to prove the other direction. Any suggustion? thanks!",,['linear-algebra']
79,When is block-partitioned matrix invertible?,When is block-partitioned matrix invertible?,,"Suppose I have a block-partitioned matrix $$\begin{bmatrix} \mathbf{X}_1^{\top}\mathbf{X}_1 & \mathbf{X}_1^{\top}\mathbf{X}_2 \\ \mathbf{X}_2^{\top}\mathbf{X}_1 & \mathbf{X}_2^{\top}\mathbf{X}_2 \end{bmatrix}$$ where $\mathbf{X}_1$ is $G \times K_1$ and $\mathbf{X}_2$ is $G \times K_2$ . Thus, our block-partitioned matrix is equal to the outer product $$\begin{bmatrix} \mathbf{X}_1^{\top} \\ \mathbf{X}_2^{\top} \end{bmatrix}\begin{bmatrix} \mathbf{X}_1 & \mathbf{X}_2 \end{bmatrix}$$ and its dimension is $K \times K$ where $K=K_1+K_2$ . In general, when is this block-partitioned matrix invertible? Is there a necessary and sufficient condition? Edit . My first thought is: for the block-partitioned matrix to be invertible, it is equivalent that each of the four blocks are invertible. However, I no longer think this. For example, if one of the off diagonals, say the lower left, $\mathbf{X}_2^{\top}\mathbf{X}_1=\mathbf{0}$ , the matrix could still be invertible if the blocks on the diagonal are. I can't articulate a proof of that though — could anyone please help?","Suppose I have a block-partitioned matrix where is and is . Thus, our block-partitioned matrix is equal to the outer product and its dimension is where . In general, when is this block-partitioned matrix invertible? Is there a necessary and sufficient condition? Edit . My first thought is: for the block-partitioned matrix to be invertible, it is equivalent that each of the four blocks are invertible. However, I no longer think this. For example, if one of the off diagonals, say the lower left, , the matrix could still be invertible if the blocks on the diagonal are. I can't articulate a proof of that though — could anyone please help?",\begin{bmatrix} \mathbf{X}_1^{\top}\mathbf{X}_1 & \mathbf{X}_1^{\top}\mathbf{X}_2 \\ \mathbf{X}_2^{\top}\mathbf{X}_1 & \mathbf{X}_2^{\top}\mathbf{X}_2 \end{bmatrix} \mathbf{X}_1 G \times K_1 \mathbf{X}_2 G \times K_2 \begin{bmatrix} \mathbf{X}_1^{\top} \\ \mathbf{X}_2^{\top} \end{bmatrix}\begin{bmatrix} \mathbf{X}_1 & \mathbf{X}_2 \end{bmatrix} K \times K K=K_1+K_2 \mathbf{X}_2^{\top}\mathbf{X}_1=\mathbf{0},"['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
80,"Let V be a vector space. If every subspace of V is T-invariant, prove that there exist a scalar multiple c such that T=c1v [duplicate]","Let V be a vector space. If every subspace of V is T-invariant, prove that there exist a scalar multiple c such that T=c1v [duplicate]",,"This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 9 years ago . I wrote a proof for the above question, but I am not sure whether it is right or not since I assumed linear independence. Here's the proof: Let $u$,$v$ be linearly independent vectors in $V$. $span(u)$, $span(v)$, $span(u+v)$ are all $T-invariant$. $T(v)$ is an element in $span(v) \Longrightarrow T(v) = av$ $T(u)$ is an element in $span(u) \Longrightarrow T(u) = bu$ $T(u+v)$ is an element in $span(u+v) \Longrightarrow T(u+v) = T(u)+T(v)= bu+av = c(u+v)$ Hence, $bu+av-c(u+v) = 0 \Longrightarrow (b-c)u + (a-c)v = 0$ Since u and v are linearly independent, $(b-c)=(a-c)=0 \Longrightarrow b=a$ Hence, $T(w) = kw$, for all $w$ in $V$, and $k$ any scalar. Is it right to assume linear independence? And is there any problem with my proof?","This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 9 years ago . I wrote a proof for the above question, but I am not sure whether it is right or not since I assumed linear independence. Here's the proof: Let $u$,$v$ be linearly independent vectors in $V$. $span(u)$, $span(v)$, $span(u+v)$ are all $T-invariant$. $T(v)$ is an element in $span(v) \Longrightarrow T(v) = av$ $T(u)$ is an element in $span(u) \Longrightarrow T(u) = bu$ $T(u+v)$ is an element in $span(u+v) \Longrightarrow T(u+v) = T(u)+T(v)= bu+av = c(u+v)$ Hence, $bu+av-c(u+v) = 0 \Longrightarrow (b-c)u + (a-c)v = 0$ Since u and v are linearly independent, $(b-c)=(a-c)=0 \Longrightarrow b=a$ Hence, $T(w) = kw$, for all $w$ in $V$, and $k$ any scalar. Is it right to assume linear independence? And is there any problem with my proof?",,['linear-algebra']
81,"Matrix A has eigenvalue $λ$ , Prove the eigenvalues of Matrix $(A+kI)$ is (λ + k)","Matrix A has eigenvalue  , Prove the eigenvalues of Matrix  is (λ + k)",λ (A+kI),"The matrix A has an eigenvalue $λ$ with corresponding eigenvector $e$. Prove that the matrix $(A + kI)$, where $k$ is a real constant and I is the identity matrix, has an eigenvalue $(λ + k)$ My Attempt: $$(A + kI)e$$ $$= Ae + kIe = λe + ke = (λ + k)e$$ Yes I proved it, but I'm not happy with the proof and I don't think its a good proof. Reasons: I started out assuming this : $(A + kI)e$ , But It should be : $$(A + kI)x$$ And I don't know how to prove this way ^^^ Even though it might seem obvious to some of you (not for me) and after the proof it's obvious that $x=e$ , it wasn't right for me  to start my proof with it (since its not mentioned that x=e. So How do I prove this?","The matrix A has an eigenvalue $λ$ with corresponding eigenvector $e$. Prove that the matrix $(A + kI)$, where $k$ is a real constant and I is the identity matrix, has an eigenvalue $(λ + k)$ My Attempt: $$(A + kI)e$$ $$= Ae + kIe = λe + ke = (λ + k)e$$ Yes I proved it, but I'm not happy with the proof and I don't think its a good proof. Reasons: I started out assuming this : $(A + kI)e$ , But It should be : $$(A + kI)x$$ And I don't know how to prove this way ^^^ Even though it might seem obvious to some of you (not for me) and after the proof it's obvious that $x=e$ , it wasn't right for me  to start my proof with it (since its not mentioned that x=e. So How do I prove this?",,['linear-algebra']
82,Finding eigenvectors for the largest eigenvalue vs one with the largest absolute value,Finding eigenvectors for the largest eigenvalue vs one with the largest absolute value,,If I want to solve a generalized eigenvalue problem such as: $$A x = \lambda x$$ The problem is to find eigenvectors corresponding to the largest eigenvalues (sometimes in an optimization problem that form it as a generalized eigenvalue problem). I notice some people solve this problem by finding eigenvectors corresponding to the largest eigenvalues in absolute value . Are these two method similar? What are the difference between them? I mean in what situation I should use eigenvalues in absolute value?,If I want to solve a generalized eigenvalue problem such as: $$A x = \lambda x$$ The problem is to find eigenvectors corresponding to the largest eigenvalues (sometimes in an optimization problem that form it as a generalized eigenvalue problem). I notice some people solve this problem by finding eigenvectors corresponding to the largest eigenvalues in absolute value . Are these two method similar? What are the difference between them? I mean in what situation I should use eigenvalues in absolute value?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
83,Trace of symmetric positive semidefinite matrix when diagonalized (as a bilinear form) in a non-orthogonal basis,Trace of symmetric positive semidefinite matrix when diagonalized (as a bilinear form) in a non-orthogonal basis,,"Let $\mathbf{S}$ be symmetric positive semidefinite matrix (i.e. one with all eigenvalues real and non-negative). Then there is an orthogonal matrix $\mathbf{U}$ (with its columns forming an orthonormal basis) such that $\mathbf{U}^\top \mathbf{S} \mathbf{U}$ is diagonal; this basis is of course given by eigenvectors of $\mathbf{S}$. Consider another basis $\mathbf{V}$ consisting of unit-length but non-orthogonal vectors (so columns of $\mathbf{V}$ have unit length but are not orthogonal) that also diagonalizes $\mathbf{S}$, i.e. $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ is diagonal. I suspect that the following is true: $\mathrm{Tr}(\mathbf{V}^\top \mathbf{S} \mathbf{V}) \le \mathrm{Tr}(\mathbf{S})=\mathrm{Tr}(\mathbf{U}^\top \mathbf{S} \mathbf{U})$. Is it true? If so, how can it be proved? Furthermore, is it true that the equality is reached iff V is orthogonal? Update: Following some confusion in the comments, I would like to clarify that I am considering $\mathbf{S}$ to represent a bilinear form, not a linear form. So with a change of basis it is transformed as $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ and not as $\mathbf{V}^{-1} \mathbf{S} \mathbf{V}$. Update 2 Let me illustrate where this question comes from; it might provide some additional intuition. $\mathbf{S}$ is actually a covariance matrix of some data (i.e. I have a set of data points $\mathbf{x}_i \in \mathbb{R}^N$, and $\mathbf{S} = \sum_i \mathbf{x}_i \mathbf{x}_i^\top$, up to a constant factor). Trace of $\mathbf{S}$ is total variance of the data, and it of course stays the same if coordinate system is rotated. Now for any unit vector $\mathbf{v}$, variance of the projection of the data on the axis defined by this vector is equal to $\mathbf{v}^\top\mathbf{S}\mathbf{v}$. If I take $N$ orthogonal unit vectors, then sum of these variances is equal to the total variance. I am interested in the situation when I take $N$ non-orthogonal unit vectors, but they are chosen such that all projections of the data on these vectors have zero correlation (or covariance). This condition is equivalent to $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ being diagonal. This means that my projections are ""independent""; therefore I am pretty sure that their variances together cannot exceed total variance; total variance should give maximum amount of variance that can be ""distributed"" between independent components (with maximum being achieved with principal components).","Let $\mathbf{S}$ be symmetric positive semidefinite matrix (i.e. one with all eigenvalues real and non-negative). Then there is an orthogonal matrix $\mathbf{U}$ (with its columns forming an orthonormal basis) such that $\mathbf{U}^\top \mathbf{S} \mathbf{U}$ is diagonal; this basis is of course given by eigenvectors of $\mathbf{S}$. Consider another basis $\mathbf{V}$ consisting of unit-length but non-orthogonal vectors (so columns of $\mathbf{V}$ have unit length but are not orthogonal) that also diagonalizes $\mathbf{S}$, i.e. $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ is diagonal. I suspect that the following is true: $\mathrm{Tr}(\mathbf{V}^\top \mathbf{S} \mathbf{V}) \le \mathrm{Tr}(\mathbf{S})=\mathrm{Tr}(\mathbf{U}^\top \mathbf{S} \mathbf{U})$. Is it true? If so, how can it be proved? Furthermore, is it true that the equality is reached iff V is orthogonal? Update: Following some confusion in the comments, I would like to clarify that I am considering $\mathbf{S}$ to represent a bilinear form, not a linear form. So with a change of basis it is transformed as $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ and not as $\mathbf{V}^{-1} \mathbf{S} \mathbf{V}$. Update 2 Let me illustrate where this question comes from; it might provide some additional intuition. $\mathbf{S}$ is actually a covariance matrix of some data (i.e. I have a set of data points $\mathbf{x}_i \in \mathbb{R}^N$, and $\mathbf{S} = \sum_i \mathbf{x}_i \mathbf{x}_i^\top$, up to a constant factor). Trace of $\mathbf{S}$ is total variance of the data, and it of course stays the same if coordinate system is rotated. Now for any unit vector $\mathbf{v}$, variance of the projection of the data on the axis defined by this vector is equal to $\mathbf{v}^\top\mathbf{S}\mathbf{v}$. If I take $N$ orthogonal unit vectors, then sum of these variances is equal to the total variance. I am interested in the situation when I take $N$ non-orthogonal unit vectors, but they are chosen such that all projections of the data on these vectors have zero correlation (or covariance). This condition is equivalent to $\mathbf{V}^\top \mathbf{S} \mathbf{V}$ being diagonal. This means that my projections are ""independent""; therefore I am pretty sure that their variances together cannot exceed total variance; total variance should give maximum amount of variance that can be ""distributed"" between independent components (with maximum being achieved with principal components).",,"['linear-algebra', 'matrices', 'bilinear-form', 'trace']"
84,"Linear operator T that commutes with every projection operator, infinite dimensional case","Linear operator T that commutes with every projection operator, infinite dimensional case",,"This is related to a question of Hoffman & Kunze, Linear Algebra (Section 6.7, #8, p. 219).  The question in the text asks: Let $T$ be a linear operator on $V$ which commutes with every projection operator on $V$.  What can you say about $T$? If $V$ is finite-dimensional, I think that $T$ is then a scalar multiple of the identity.  (We know from an earlier exercise that if every subspace of a finite-dimensional vector space $V$ is invariant under $T$ then $T$ is a scalar multiple of the identity.  We also know that $TE = ET \implies E(V)$ is invariant under $T$.  Since we can construct a projection $E$ such that $E(V) = W$ for every subspace $W$ of $V$, we can then conclude that $T$ is a scalar multiple of the identity.) I then have two questions: is the reasoning above for the finite-dimensional case accurate?  How do we deal with the infinite-dimensional case?  (It is possible the question implies that $V$ is finite-dimensional, but it does not state that, so I am uncertain.) For background, I am self-studying linear algebra and have covered everything in Hoffman & Kunze up to this question.","This is related to a question of Hoffman & Kunze, Linear Algebra (Section 6.7, #8, p. 219).  The question in the text asks: Let $T$ be a linear operator on $V$ which commutes with every projection operator on $V$.  What can you say about $T$? If $V$ is finite-dimensional, I think that $T$ is then a scalar multiple of the identity.  (We know from an earlier exercise that if every subspace of a finite-dimensional vector space $V$ is invariant under $T$ then $T$ is a scalar multiple of the identity.  We also know that $TE = ET \implies E(V)$ is invariant under $T$.  Since we can construct a projection $E$ such that $E(V) = W$ for every subspace $W$ of $V$, we can then conclude that $T$ is a scalar multiple of the identity.) I then have two questions: is the reasoning above for the finite-dimensional case accurate?  How do we deal with the infinite-dimensional case?  (It is possible the question implies that $V$ is finite-dimensional, but it does not state that, so I am uncertain.) For background, I am self-studying linear algebra and have covered everything in Hoffman & Kunze up to this question.",,['linear-algebra']
85,What linear transformations preserve these conditions?,What linear transformations preserve these conditions?,,"Main Question Let's define $\Gamma(n)$ as the set of real antisymmetric matrices of size $n$ ($n$ is an even Integer), fulfilling: $$ \forall \gamma\in \Gamma(n) \Rightarrow \gamma^2=-\mathbb I_n$$ where $\mathbb I_n$ is the identity matrix of size $n$. What is a nice representation of the set of all linear operators that keep matrices inside $\Gamma(n)$? $$\gamma_{i,j}\to \gamma'_{i,j} = \sum_{i',j'=1}^n \gamma_{i',j'}\beta_{i,i'}\beta_{j,j'}$$ where $\beta \in \mathbb C^{n^2}$. For instance I know that if $\gamma\to \gamma'=U\gamma U^\dagger$, where $U$ is a unitary matrix then clearly $$\gamma'^2=U\gamma U^\dagger U\gamma U^\dagger=U\gamma^2 U^\dagger=-U\mathbb I_n U^\dagger=-\mathbb I_n$$ but I don't know what unitary matrices preserve antisymmetric-ness of a matrix. Background The background is heavily related to physics so I thought I should separate it from the main question. The $\gamma$ matrices that I'm dealing with are called Covariance Matrices(CMs); and the case of this problem they are defined as: $$\gamma_{l,m}=\frac{i}{2}\text{tr}(\rho[c_l,c_m])$$ where $\rho$ is the density matrix of a pure Gaussian state, $c$'s are Majorana mode operators fulfilling $\{c_l,c_m\}=2\delta_{l,m}$ and $[\_,\_]$($\{\_,\_\}$) is a/an (anti-)commutator. Also, the condition $\gamma^2=-\mathbb I_n$ is satisfied iff the state $\rho$ is pure [1] .","Main Question Let's define $\Gamma(n)$ as the set of real antisymmetric matrices of size $n$ ($n$ is an even Integer), fulfilling: $$ \forall \gamma\in \Gamma(n) \Rightarrow \gamma^2=-\mathbb I_n$$ where $\mathbb I_n$ is the identity matrix of size $n$. What is a nice representation of the set of all linear operators that keep matrices inside $\Gamma(n)$? $$\gamma_{i,j}\to \gamma'_{i,j} = \sum_{i',j'=1}^n \gamma_{i',j'}\beta_{i,i'}\beta_{j,j'}$$ where $\beta \in \mathbb C^{n^2}$. For instance I know that if $\gamma\to \gamma'=U\gamma U^\dagger$, where $U$ is a unitary matrix then clearly $$\gamma'^2=U\gamma U^\dagger U\gamma U^\dagger=U\gamma^2 U^\dagger=-U\mathbb I_n U^\dagger=-\mathbb I_n$$ but I don't know what unitary matrices preserve antisymmetric-ness of a matrix. Background The background is heavily related to physics so I thought I should separate it from the main question. The $\gamma$ matrices that I'm dealing with are called Covariance Matrices(CMs); and the case of this problem they are defined as: $$\gamma_{l,m}=\frac{i}{2}\text{tr}(\rho[c_l,c_m])$$ where $\rho$ is the density matrix of a pure Gaussian state, $c$'s are Majorana mode operators fulfilling $\{c_l,c_m\}=2\delta_{l,m}$ and $[\_,\_]$($\{\_,\_\}$) is a/an (anti-)commutator. Also, the condition $\gamma^2=-\mathbb I_n$ is satisfied iff the state $\rho$ is pure [1] .",,"['linear-algebra', 'matrices', 'covariance']"
86,Expected value of a random matrix and the expected value of its eigenvalues,Expected value of a random matrix and the expected value of its eigenvalues,,"For $M$ being a $p \times p$ symmetric positive definite matrix. If $E(M) = A$, does that imply $E(\lambda_i) = l_i$, for $i = 1,...,p$, where $\lambda_i$ and $l_i$ are the $i$th eigenvalues of $M$ and $A$, respectively? Thanks.","For $M$ being a $p \times p$ symmetric positive definite matrix. If $E(M) = A$, does that imply $E(\lambda_i) = l_i$, for $i = 1,...,p$, where $\lambda_i$ and $l_i$ are the $i$th eigenvalues of $M$ and $A$, respectively? Thanks.",,"['linear-algebra', 'probability', 'matrices']"
87,Is a similarity map necessarily affine linear?,Is a similarity map necessarily affine linear?,,"My text on fractal geometry introduces the following definition: A map $S: \mathbb R^n \to \mathbb R^n$ is called a similarity map if $$\exists c>0 \ \forall x,y \in \mathbb R^n: |S(x)-S(y)|=c|x-y|$$ I think that if we'd demand $S$ to be affine linear then $S$ is angle preserving, that is, $S=\tau_a \circ c L$ where $\tau_a$ means shifting by $a \in \mathbb R^n$, $L$ is orthogonal, and $c>0$ is a scaling factor. But the definition doesn't require $S$ to be affine linear. Are there any non-affine-linear cases among similarity maps?","My text on fractal geometry introduces the following definition: A map $S: \mathbb R^n \to \mathbb R^n$ is called a similarity map if $$\exists c>0 \ \forall x,y \in \mathbb R^n: |S(x)-S(y)|=c|x-y|$$ I think that if we'd demand $S$ to be affine linear then $S$ is angle preserving, that is, $S=\tau_a \circ c L$ where $\tau_a$ means shifting by $a \in \mathbb R^n$, $L$ is orthogonal, and $c>0$ is a scaling factor. But the definition doesn't require $S$ to be affine linear. Are there any non-affine-linear cases among similarity maps?",,"['linear-algebra', 'fractals', 'affine-geometry', 'orthogonality']"
88,Show that $0$ and $1$ are the only real eigenvalues of $A.$,Show that  and  are the only real eigenvalues of,0 1 A.,Let $A\in M_n(\mathbb R)$ such that $A^2=A^T.$ Show that $0$ and $1$ are the only real eigenvalues of $A.$ All I can see is that $\det A=0$ or $1.$ I can't proceed any further.,Let $A\in M_n(\mathbb R)$ such that $A^2=A^T.$ Show that $0$ and $1$ are the only real eigenvalues of $A.$ All I can see is that $\det A=0$ or $1.$ I can't proceed any further.,,['linear-algebra']
89,Rotations and reflections in ${\bf R}^3$.,Rotations and reflections in .,{\bf R}^3,"By a rotation in ${\bf R}^3$ I mean an orthogonal linear transformation $f:{\bf R}^3\to {\bf R}^3$ represented by a matrix $A$ (i.e. $fx=Ax$) with $\det A=1$. By a reflection (through $S$) I mean an orthogonal  linear transformation such that for some subspace $S$, $f\mid S={\rm id}$ and $f\mid S^{\perp}=-{\rm id}$. I am currently stuck in two tasks: $(1)$ Let $\mathscr L_1=\langle(1,1,0)\rangle+(2,0,1)$ and $\mathscr L_2=\langle (2,1,3)\rangle+(1,0,4)$. I have to find a rotation such that $f(\mathscr L_1)=\mathscr L_2$. Now, the distance of both lines to the origin is $\sqrt 3$. The first line accomplishes this with $(1,-1,1)$, while the second line accomplishes this with $(-1,-1,1)$. I tried various times to define a rotation, but I failed. In particular, I know I should map $(1,-1,1)$ to $(-1,-1,1)$. $(2)$ Let $\Pi_1=\{(x_1,x_2,x_3):x_1-x_2+2x_3=k\}$ and $\Pi_2=\langle (1,0,1),(0,1,2)\rangle+(1,-1,1)$. I have to find $k$ such that there exists a reflection that maps $\Pi_1$ to $\Pi_2$ and find $f(\Pi_2)$. Now, the distance to $\Pi_1$ to the origin is $|k|/\sqrt 6$ and that of $\Pi_2$ is $3/\sqrt 6$ which gives me $k=3,-3$. I don't really know how to continue now.","By a rotation in ${\bf R}^3$ I mean an orthogonal linear transformation $f:{\bf R}^3\to {\bf R}^3$ represented by a matrix $A$ (i.e. $fx=Ax$) with $\det A=1$. By a reflection (through $S$) I mean an orthogonal  linear transformation such that for some subspace $S$, $f\mid S={\rm id}$ and $f\mid S^{\perp}=-{\rm id}$. I am currently stuck in two tasks: $(1)$ Let $\mathscr L_1=\langle(1,1,0)\rangle+(2,0,1)$ and $\mathscr L_2=\langle (2,1,3)\rangle+(1,0,4)$. I have to find a rotation such that $f(\mathscr L_1)=\mathscr L_2$. Now, the distance of both lines to the origin is $\sqrt 3$. The first line accomplishes this with $(1,-1,1)$, while the second line accomplishes this with $(-1,-1,1)$. I tried various times to define a rotation, but I failed. In particular, I know I should map $(1,-1,1)$ to $(-1,-1,1)$. $(2)$ Let $\Pi_1=\{(x_1,x_2,x_3):x_1-x_2+2x_3=k\}$ and $\Pi_2=\langle (1,0,1),(0,1,2)\rangle+(1,-1,1)$. I have to find $k$ such that there exists a reflection that maps $\Pi_1$ to $\Pi_2$ and find $f(\Pi_2)$. Now, the distance to $\Pi_1$ to the origin is $|k|/\sqrt 6$ and that of $\Pi_2$ is $3/\sqrt 6$ which gives me $k=3,-3$. I don't really know how to continue now.",,"['linear-algebra', 'euclidean-geometry', 'inner-products', 'geometric-transformation']"
90,Finding the smallest sub-family of subsets needed to form a new subset,Finding the smallest sub-family of subsets needed to form a new subset,,"TL/DR I have a universe $U$ of items $u_i$ and a family $F$ of subsets of $U$ (call them $P_j$ ⊆ $U$). Given this family of subsets, I would like to find the sub-family $C$ ⊆ $F$ of subsets that can produce a new subset $P_{new}$ of members $u_i$ by adding together (or subtracting) as few subsets $P_j$ as possible. That's the best I can do. Hopefully an example is more clear: Example For instance, if we start with the following family of subsets: $$ \begin{align}  F = \{&P_1 = \{a\},\ P_2 = \{b\},\ ...,\ P_{16} = \{p\}, \\    &P_{17} = \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}, \\     &P_{18} = \{a, b, c, d, e\}, \\    &P_{19} = \{g, h, i\} \,\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \}&\\ \end{align} $$ When requested to compute $\{a, b, c, d, e, f, g, h, i\}$, the simplest thing to do is calculate: $$\{a, b, c, d, e, f, g, h, i\} =  \{a\} + \{b\} + \{c\} +\ ...\ + \{h\} + \{i\}$$ This isn't optimal though (requires 8 additions). For instance, I know that I could more quickly produce the set if I instead took advantage of my previously computed sets (using 2 additions): $$ \begin{align} P_{new} &= \{a, b, c, d, e, f, g, h, i\}\\   &=  \{a, b, c, d, e\} + \{f\} + \{g, h, i\} \\   &=  P_{18} + P_{6} + P_{19} \\  \mathord{\therefore}\ C ⊆ F &= \{ P_{6}, P_{18}, P_{19} \} \\ \end{align} $$ Example 2 What's even more complex is that (if possible) I want to know when involving subtraction might be optimal: $$\{e, f, g, h, i\} = \{e\} + \{f\} + \{g, h, i\}$$ This is the best solution using only addition (2 operations), But I could have gotten this even faster with 1 subtraction: $$\{e, f, g, h, i\} = \{a, b, c, d, e, f, g, h, i\} - \{a, b, c, d\}$$ Why I need this Each subset $P_j$ has a value $p_j = f(P_j)$ that can be computed. The function $f(P_j)$ is additive. So $p_{\{1,2\}} = p_{\{1\}} + p_{\{2\}}$ When I start my application, I start only by calculating the value $p_i$ for each item $l_i$ on its own. For example: $$ \begin{align}    P_1 = \{a\} ,&\ \ p_1 = f(P_1) = 5 \\    P_2 = \{b\} ,&\ \ p_2 = f(P_2) = 20 \\    P_3 = \{c\} ,&\ \ p_3 = f(P_3) = 8 \\    ...\ & \end{align} $$ I then have to start servicing requests for different subsets. For example: $$ \begin{align}     P_{18} &=  \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}\  &f(P_{18}) &= 400 \\     P_{19} &=  \{b, c, d\}\                                         &f(P_{19}) &= 43\\     P_{20} &=  \{g, h, i\}\                                         &f(P_{20}) &= 30 \\     ...& \end{align} $$ My goal is to be able to process these request as fast as possible. For early requests, I unavoidably have to spend a lot of time adding up $p_j$ values. But since these values are additive, I know there should be faster ways to process requests by taking advantage of sets for which I've already computed $p_j$. If $P_{21} = \{b, c, d, g, h, i\}$ is requested, I don't want to waste precious resources retrieving the the 6 values for $p_{2}$ to $p_{7}$, and then adding these values in 5 lengthy operations, when I could have just done a single operation $p_{21} = p_{19}+p_{20}$. Not set-theory? This might actually be a better fit for linear algebra, if formulated as follows: If I have the following known equations and values: $$ \begin{align}     P_{1}  &=  a, P_{2}  =  b,\ ...,\ P_{8}  =  g &f(P_{1})  &= p_{1},\ ...\\     P_{9}  &=  a + b + c + d   &f(P_{9})  &= p_{9} \\     P_{10} &=  d + e + f + g   &f(P_{10}) &= p_{10} \\ \end{align} $$ And I wish to calculate $$ \begin{align}     P_{11} &= a + b + c + d + e + f + g  &f(P_{11}) ? \\ \end{align} $$ I want to be able discover that the fastest solution comes from $$ \begin{align}     P_{11} &= P_{9} + P_{10} - P_{4} \\     P_{11} &=  (a + b + c + d) + (d + e + f + g) - (d) \\            &=  (a + b + c + 2d + e + f + g) - (d) \\            &=  a + b + c + d + e + f + g\ \checkmark\\            \mathord{\therefore}\ p_{11} &= p_{9} + p_{10} - p_{4} \\ \end{align} $$ It's starting to look suspiciously like an np hard problem to me :( If no one can come up with an elegant way of solving the problem, I would also accept a more elegant way of wording the problem (perhaps in terms of an existing well known problem), or a bound on the complexity.","TL/DR I have a universe $U$ of items $u_i$ and a family $F$ of subsets of $U$ (call them $P_j$ ⊆ $U$). Given this family of subsets, I would like to find the sub-family $C$ ⊆ $F$ of subsets that can produce a new subset $P_{new}$ of members $u_i$ by adding together (or subtracting) as few subsets $P_j$ as possible. That's the best I can do. Hopefully an example is more clear: Example For instance, if we start with the following family of subsets: $$ \begin{align}  F = \{&P_1 = \{a\},\ P_2 = \{b\},\ ...,\ P_{16} = \{p\}, \\    &P_{17} = \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}, \\     &P_{18} = \{a, b, c, d, e\}, \\    &P_{19} = \{g, h, i\} \,\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \}&\\ \end{align} $$ When requested to compute $\{a, b, c, d, e, f, g, h, i\}$, the simplest thing to do is calculate: $$\{a, b, c, d, e, f, g, h, i\} =  \{a\} + \{b\} + \{c\} +\ ...\ + \{h\} + \{i\}$$ This isn't optimal though (requires 8 additions). For instance, I know that I could more quickly produce the set if I instead took advantage of my previously computed sets (using 2 additions): $$ \begin{align} P_{new} &= \{a, b, c, d, e, f, g, h, i\}\\   &=  \{a, b, c, d, e\} + \{f\} + \{g, h, i\} \\   &=  P_{18} + P_{6} + P_{19} \\  \mathord{\therefore}\ C ⊆ F &= \{ P_{6}, P_{18}, P_{19} \} \\ \end{align} $$ Example 2 What's even more complex is that (if possible) I want to know when involving subtraction might be optimal: $$\{e, f, g, h, i\} = \{e\} + \{f\} + \{g, h, i\}$$ This is the best solution using only addition (2 operations), But I could have gotten this even faster with 1 subtraction: $$\{e, f, g, h, i\} = \{a, b, c, d, e, f, g, h, i\} - \{a, b, c, d\}$$ Why I need this Each subset $P_j$ has a value $p_j = f(P_j)$ that can be computed. The function $f(P_j)$ is additive. So $p_{\{1,2\}} = p_{\{1\}} + p_{\{2\}}$ When I start my application, I start only by calculating the value $p_i$ for each item $l_i$ on its own. For example: $$ \begin{align}    P_1 = \{a\} ,&\ \ p_1 = f(P_1) = 5 \\    P_2 = \{b\} ,&\ \ p_2 = f(P_2) = 20 \\    P_3 = \{c\} ,&\ \ p_3 = f(P_3) = 8 \\    ...\ & \end{align} $$ I then have to start servicing requests for different subsets. For example: $$ \begin{align}     P_{18} &=  \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}\  &f(P_{18}) &= 400 \\     P_{19} &=  \{b, c, d\}\                                         &f(P_{19}) &= 43\\     P_{20} &=  \{g, h, i\}\                                         &f(P_{20}) &= 30 \\     ...& \end{align} $$ My goal is to be able to process these request as fast as possible. For early requests, I unavoidably have to spend a lot of time adding up $p_j$ values. But since these values are additive, I know there should be faster ways to process requests by taking advantage of sets for which I've already computed $p_j$. If $P_{21} = \{b, c, d, g, h, i\}$ is requested, I don't want to waste precious resources retrieving the the 6 values for $p_{2}$ to $p_{7}$, and then adding these values in 5 lengthy operations, when I could have just done a single operation $p_{21} = p_{19}+p_{20}$. Not set-theory? This might actually be a better fit for linear algebra, if formulated as follows: If I have the following known equations and values: $$ \begin{align}     P_{1}  &=  a, P_{2}  =  b,\ ...,\ P_{8}  =  g &f(P_{1})  &= p_{1},\ ...\\     P_{9}  &=  a + b + c + d   &f(P_{9})  &= p_{9} \\     P_{10} &=  d + e + f + g   &f(P_{10}) &= p_{10} \\ \end{align} $$ And I wish to calculate $$ \begin{align}     P_{11} &= a + b + c + d + e + f + g  &f(P_{11}) ? \\ \end{align} $$ I want to be able discover that the fastest solution comes from $$ \begin{align}     P_{11} &= P_{9} + P_{10} - P_{4} \\     P_{11} &=  (a + b + c + d) + (d + e + f + g) - (d) \\            &=  (a + b + c + 2d + e + f + g) - (d) \\            &=  a + b + c + d + e + f + g\ \checkmark\\            \mathord{\therefore}\ p_{11} &= p_{9} + p_{10} - p_{4} \\ \end{align} $$ It's starting to look suspiciously like an np hard problem to me :( If no one can come up with an elegant way of solving the problem, I would also accept a more elegant way of wording the problem (perhaps in terms of an existing well known problem), or a bound on the complexity.",,"['linear-algebra', 'optimization', 'packing-problem']"
91,Gramian Matrix and Non-negative Eigenvalues,Gramian Matrix and Non-negative Eigenvalues,,"I'm trying to write up solutions for my Linear Algebra students, and I can't seem to figure out this one part of a proof. They are told that a matrix $A$ is called Gramian if $$A=B^tB$$ for some real, square matrix $B$. They are then asked to prove that $A$ is symmetric (trivial) and that all of its eigenvalues are non-negative (which is the part I'm stuck on). I've tried looking up properties about Gramian matrices, but everything mentioned relates them to positive semidefinite matrices, which my students have not read anything about. I also know that $B$ and $B^t$ have the same eigenvalues, but I don't think they would have to have the same eigenvectors. Maybe I'm just tired, but any help proving this without mentioning ""positive semidefinite"" is much appreciated.","I'm trying to write up solutions for my Linear Algebra students, and I can't seem to figure out this one part of a proof. They are told that a matrix $A$ is called Gramian if $$A=B^tB$$ for some real, square matrix $B$. They are then asked to prove that $A$ is symmetric (trivial) and that all of its eigenvalues are non-negative (which is the part I'm stuck on). I've tried looking up properties about Gramian matrices, but everything mentioned relates them to positive semidefinite matrices, which my students have not read anything about. I also know that $B$ and $B^t$ have the same eigenvalues, but I don't think they would have to have the same eigenvectors. Maybe I'm just tired, but any help proving this without mentioning ""positive semidefinite"" is much appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
92,Linear and Commutative function over Square Matrices.,Linear and Commutative function over Square Matrices.,,"Find all functions $f$, such that $f(mA+nB) = mf(A) + nf(B)$ and  $f(AB) = f(BA)$ , where $A, B$ are square matrices and $ m,n$ are scalars. Need to find $f$ as an explicit function of any general matrix M. I observed that $Trace(M)$ is a valid function satisfying the condition, but any methodical approach to find the functions would be helpful.","Find all functions $f$, such that $f(mA+nB) = mf(A) + nf(B)$ and  $f(AB) = f(BA)$ , where $A, B$ are square matrices and $ m,n$ are scalars. Need to find $f$ as an explicit function of any general matrix M. I observed that $Trace(M)$ is a valid function satisfying the condition, but any methodical approach to find the functions would be helpful.",,"['linear-algebra', 'matrices', 'vector-spaces']"
93,$QR$ decomposition of rectangular block matrix,decomposition of rectangular block matrix,QR,So I am running an iterative algorithm. I have matrix $W$ of dimensions $n\times p$ which is fixed for every iteration and matrix $\sqrt{3\rho} \boldsymbol{I}$ of dimension $p\times p$ where the $\rho$ parameter changes at every iteration. And for every iteration I need to evaluate the QR decomposition of the matrix:   $$\widetilde{W} = \left[\begin{array}{c} W \\ \sqrt{3\rho} \boldsymbol{I} \end{array} \right] $$ which is a matrix of dimension $(n+p)\times p$. Since $W$ is fixed I wondering if there is any easy way to evaluate the QR decomposition of the matrix $\widetilde{W}$ by just looking at the QR decomposition of $W$? I hope to avoid evaluating the QR decomposition wach time for each different $\rho$.,So I am running an iterative algorithm. I have matrix $W$ of dimensions $n\times p$ which is fixed for every iteration and matrix $\sqrt{3\rho} \boldsymbol{I}$ of dimension $p\times p$ where the $\rho$ parameter changes at every iteration. And for every iteration I need to evaluate the QR decomposition of the matrix:   $$\widetilde{W} = \left[\begin{array}{c} W \\ \sqrt{3\rho} \boldsymbol{I} \end{array} \right] $$ which is a matrix of dimension $(n+p)\times p$. Since $W$ is fixed I wondering if there is any easy way to evaluate the QR decomposition of the matrix $\widetilde{W}$ by just looking at the QR decomposition of $W$? I hope to avoid evaluating the QR decomposition wach time for each different $\rho$.,,"['linear-algebra', 'matrices', 'matrix-decomposition']"
94,Any positive semidefinite matrix can be written as $AA^{\ast}$,Any positive semidefinite matrix can be written as,AA^{\ast},"I know that for any matrix $A$ , $AA^{\ast}$ is positive semidefinite (where $A^\ast$ is $\overline{A}^T$ ). Please help me show the following statement Any positive semidefinite can be written as $AA^{\ast}$ .","I know that for any matrix , is positive semidefinite (where is ). Please help me show the following statement Any positive semidefinite can be written as .",A AA^{\ast} A^\ast \overline{A}^T AA^{\ast},"['linear-algebra', 'matrices']"
95,Nonzero subspace that is invariant under any operator cannot be proper?,Nonzero subspace that is invariant under any operator cannot be proper?,,"This is not for homework, and I would just a like a hint please .  The question asks Prove or give a counterexample: If $U$ is a subspace of $V$ that is invariant under every operator on $V$, then $U = \{ 0 \}$ or $U = V$. In the question, $V$ is finite dimensional.  My gut feeling is that this claim is true, so I tried to begin a proof.  I began by assuming $U \neq \{ 0 \}$.  I want to try to exploit the fact that $U$ is invariant under every operator to try and force every basis vector of $V$ into $U$.  I would like just a hint please !","This is not for homework, and I would just a like a hint please .  The question asks Prove or give a counterexample: If $U$ is a subspace of $V$ that is invariant under every operator on $V$, then $U = \{ 0 \}$ or $U = V$. In the question, $V$ is finite dimensional.  My gut feeling is that this claim is true, so I tried to begin a proof.  I began by assuming $U \neq \{ 0 \}$.  I want to try to exploit the fact that $U$ is invariant under every operator to try and force every basis vector of $V$ into $U$.  I would like just a hint please !",,"['linear-algebra', 'vector-spaces']"
96,Prove $\mathbb R$ vector space over $\mathbb Q$,Prove  vector space over,\mathbb R \mathbb Q,"I am proving that $\mathbb R$ is a vector space over $\mathbb Q$.  So far, I have stated that vector addition and scalar multiplication trivially hold in $\mathbb R$. I then showed that $(\mathbb R, +)$ is an abelian group. Now I am trying to show distributivity of scalar multiplication, but I feel it's trivial to the point where I can't really prove it.  This is what I have: Let $\alpha, \beta \in \mathbb Q$ and $x,y \in \mathbb R$. Then, $(\alpha+\beta)\cdot x = \alpha \cdot x+ \beta \cdot x$ and $\alpha \cdot(x+y)=\alpha \cdot x+ \alpha \cdot y$ by the usual operations in $\mathbb R$. Is this correct? I feel it's inadequate, but I don't know how to break it down further. Any feedback is appreciated.","I am proving that $\mathbb R$ is a vector space over $\mathbb Q$.  So far, I have stated that vector addition and scalar multiplication trivially hold in $\mathbb R$. I then showed that $(\mathbb R, +)$ is an abelian group. Now I am trying to show distributivity of scalar multiplication, but I feel it's trivial to the point where I can't really prove it.  This is what I have: Let $\alpha, \beta \in \mathbb Q$ and $x,y \in \mathbb R$. Then, $(\alpha+\beta)\cdot x = \alpha \cdot x+ \beta \cdot x$ and $\alpha \cdot(x+y)=\alpha \cdot x+ \alpha \cdot y$ by the usual operations in $\mathbb R$. Is this correct? I feel it's inadequate, but I don't know how to break it down further. Any feedback is appreciated.",,"['linear-algebra', 'vector-spaces']"
97,Prove that the field F is a vector space over itself.,Prove that the field F is a vector space over itself.,,"How can I prove that a field F is a vector space over itself? Intuitively, it seems obvious because the definition of a field is nearly the same as that of a vector space, just with scalers instead of vectors. Here's what I'm thinking: Let V = { (a) | a in F } describe the vector space for F.  Then I just show that vector addition is commutative, associative, has an identity and an inverse, and that scalar multiplication is distributary, associative, and has an identity. Example 1: Commutativity of addition, Here x,y are in V (x)+(y) = (y)+(x) (x+y)=(y+x): vector addition x + (X+y) = (X + x)+y: associative property Example 2: Additive inverse x,y,0 in V (x)+(y)=(0) (x+y)=(0) vector addition Let y=-x in V (X+-x)=(0) substitute (0)=(0) simplify I don't know if I'm going in the right direction with this, although it seems like it should be a pretty simple proof.  I think mostly I'm having trouble with the notation. Any help would be greatly appreciated! Thanks in advance!","How can I prove that a field F is a vector space over itself? Intuitively, it seems obvious because the definition of a field is nearly the same as that of a vector space, just with scalers instead of vectors. Here's what I'm thinking: Let V = { (a) | a in F } describe the vector space for F.  Then I just show that vector addition is commutative, associative, has an identity and an inverse, and that scalar multiplication is distributary, associative, and has an identity. Example 1: Commutativity of addition, Here x,y are in V (x)+(y) = (y)+(x) (x+y)=(y+x): vector addition x + (X+y) = (X + x)+y: associative property Example 2: Additive inverse x,y,0 in V (x)+(y)=(0) (x+y)=(0) vector addition Let y=-x in V (X+-x)=(0) substitute (0)=(0) simplify I don't know if I'm going in the right direction with this, although it seems like it should be a pretty simple proof.  I think mostly I'm having trouble with the notation. Any help would be greatly appreciated! Thanks in advance!",,"['linear-algebra', 'vector-spaces', 'field-theory']"
98,Proof of a direct sum decomposition,Proof of a direct sum decomposition,,"I was trying to prove this statement: If $N: V \to V$ is a nilpotent operator on a complex vector space, $N^k=0$ and $U\subset V$ is a subspace with $U \cap \ker(N^{k-1})= \{0\}$ then there exists a subspace $W \subset V$ with $NW \subset W$ and $V=W\oplus (U + NU + N^2U + \dots +N^{k-1}U)$. Can you tell me please if my proof is ok? Proof by induction on $\dim(V)$. Base case: If $\dim(V) = 1$ then the statement is true because every subspace is either $\{0\}$ or $V$. Assume the statement is true if $\dim(V) = n-1$. If $\dim(V) = n$ then because $V$ is a complex vector space $N$ has an eigen vector: $Nw = \lambda w$. Two cases: Either $w \in U$ or $w \notin U$. 1) If $w \notin U$: Then $\tilde{V} = V \setminus span(w)$ is a space satisfying the induction hypothesis (for $\tilde{N} = N\mid_{\tilde{V}}$). Therefore there exists  $\tilde{W}\subset\tilde{V}$ with $\tilde{V}=\tilde{W}\oplus(U + NU +\dots +N^{k-1}U)$ and $N\tilde{W} \subset \tilde{W}$. Then (for $W=\operatorname{span}(w)+\tilde{W}$) $V= W\oplus (U+NU+\dots+N^{k-1}U)$ and $NW \subset W$. 2) If $w \in U$: Then $\tilde{V} = V \setminus \operatorname{span}(w)$ is a space satisfying the induction hypothesis with $\tilde{U}=U\setminus \operatorname{span}(w)$. Therefore there exists  $\tilde{W}\subset\tilde{V}$ with $\tilde{V}=\tilde{W}\oplus(\tilde{U} + N\tilde{U} +...N^{k-1}\tilde{U})$ and $N\tilde{W} \subset \tilde{W}$. Then $V= \tilde{W}\oplus (U+NU+....+N^{k-1}U)$.","I was trying to prove this statement: If $N: V \to V$ is a nilpotent operator on a complex vector space, $N^k=0$ and $U\subset V$ is a subspace with $U \cap \ker(N^{k-1})= \{0\}$ then there exists a subspace $W \subset V$ with $NW \subset W$ and $V=W\oplus (U + NU + N^2U + \dots +N^{k-1}U)$. Can you tell me please if my proof is ok? Proof by induction on $\dim(V)$. Base case: If $\dim(V) = 1$ then the statement is true because every subspace is either $\{0\}$ or $V$. Assume the statement is true if $\dim(V) = n-1$. If $\dim(V) = n$ then because $V$ is a complex vector space $N$ has an eigen vector: $Nw = \lambda w$. Two cases: Either $w \in U$ or $w \notin U$. 1) If $w \notin U$: Then $\tilde{V} = V \setminus span(w)$ is a space satisfying the induction hypothesis (for $\tilde{N} = N\mid_{\tilde{V}}$). Therefore there exists  $\tilde{W}\subset\tilde{V}$ with $\tilde{V}=\tilde{W}\oplus(U + NU +\dots +N^{k-1}U)$ and $N\tilde{W} \subset \tilde{W}$. Then (for $W=\operatorname{span}(w)+\tilde{W}$) $V= W\oplus (U+NU+\dots+N^{k-1}U)$ and $NW \subset W$. 2) If $w \in U$: Then $\tilde{V} = V \setminus \operatorname{span}(w)$ is a space satisfying the induction hypothesis with $\tilde{U}=U\setminus \operatorname{span}(w)$. Therefore there exists  $\tilde{W}\subset\tilde{V}$ with $\tilde{V}=\tilde{W}\oplus(\tilde{U} + N\tilde{U} +...N^{k-1}\tilde{U})$ and $N\tilde{W} \subset \tilde{W}$. Then $V= \tilde{W}\oplus (U+NU+....+N^{k-1}U)$.",,"['linear-algebra', 'vector-spaces', 'proof-verification']"
99,Trace of the $n$-th symmetric power of a linear map,Trace of the -th symmetric power of a linear map,n,Suppose $V$ is a vector space over $k$ and  $\dim(V) = N$. Let $A \in\operatorname{End}(V)$. Let $\wedge^n A \in \operatorname{End}(\wedge^n V)$ where $\wedge^n$ is the $n$-th exterior power. I am having trouble finding an expression for $\operatorname{tr}(\wedge^n A)$. The case when $A$ is diagonalizable is solved here: Symmetric and exterior power of representation My attempts so far: Use $\phi: \wedge^n V \to V^{\otimes n}$ where $\left[v\right] \overset{\phi}{\mapsto} \sum_{\sigma \in S_n}\operatorname{sign}(\sigma) \sigma(v)$ (where $\left[v\right]$ denotes the congruence class of $v \in V^{\otimes n}$ in $\wedge^n V$). Try to write down the restriction of the map $A^{\otimes n}$ to $\phi(\wedge^n A)$ as a matrix. This seems tricky and I am stuck on figuring out all the indices. Is there a more natural way to find $\operatorname{tr}(\wedge^n A)$ without writing down the matrix?,Suppose $V$ is a vector space over $k$ and  $\dim(V) = N$. Let $A \in\operatorname{End}(V)$. Let $\wedge^n A \in \operatorname{End}(\wedge^n V)$ where $\wedge^n$ is the $n$-th exterior power. I am having trouble finding an expression for $\operatorname{tr}(\wedge^n A)$. The case when $A$ is diagonalizable is solved here: Symmetric and exterior power of representation My attempts so far: Use $\phi: \wedge^n V \to V^{\otimes n}$ where $\left[v\right] \overset{\phi}{\mapsto} \sum_{\sigma \in S_n}\operatorname{sign}(\sigma) \sigma(v)$ (where $\left[v\right]$ denotes the congruence class of $v \in V^{\otimes n}$ in $\wedge^n V$). Try to write down the restriction of the map $A^{\otimes n}$ to $\phi(\wedge^n A)$ as a matrix. This seems tricky and I am stuck on figuring out all the indices. Is there a more natural way to find $\operatorname{tr}(\wedge^n A)$ without writing down the matrix?,,"['linear-algebra', 'multilinear-algebra']"
