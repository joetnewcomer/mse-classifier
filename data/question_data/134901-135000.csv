,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Discontinuous solution of $y''(x)-2(1-x)(y'(x))^2=0$ with $y(0)=1$ and $y(2)=-1$,Discontinuous solution of  with  and,y''(x)-2(1-x)(y'(x))^2=0 y(0)=1 y(2)=-1,"I tried to solve the equation $$y''(x)-2(1-x)(y'(x))^2=0$$ with the conditions $$y(0)=1, y(2)=-1.$$ It's easy to verify that the function $$y(x)=\frac{1}{1-x}$$ satisfies both the equation and the conditions. The problem is the function is discontinuous at $x=1$ while what I know -May be wrong- is the solution of differential equation is always continuous!. So I tried to solve it using Mathematica and here is the result which is a perfect continuous solution. Does the problem have more than one solution? Is the discontinuous solution wrong? Is Mathematica wrong? Did I miss something? Edit: I worked the problem manually by the method of separation and obtained two distinct solutions: $$y(x)=\frac{1}{1-x}$$ and $$y(x)=\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{x-1}{\sqrt{\frac{1}{c}-1}})+\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{1}{\sqrt{\frac{1}{c}-1}})+1$$ where $$c=-2.2767175312280725973119838$$ The question now is: Why should I consider the discontinuous function to be NOT solution of the differential equation??",I tried to solve the equation with the conditions It's easy to verify that the function satisfies both the equation and the conditions. The problem is the function is discontinuous at while what I know -May be wrong- is the solution of differential equation is always continuous!. So I tried to solve it using Mathematica and here is the result which is a perfect continuous solution. Does the problem have more than one solution? Is the discontinuous solution wrong? Is Mathematica wrong? Did I miss something? Edit: I worked the problem manually by the method of separation and obtained two distinct solutions: and where The question now is: Why should I consider the discontinuous function to be NOT solution of the differential equation??,"y''(x)-2(1-x)(y'(x))^2=0 y(0)=1, y(2)=-1. y(x)=\frac{1}{1-x} x=1 y(x)=\frac{1}{1-x} y(x)=\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{x-1}{\sqrt{\frac{1}{c}-1}})+\frac{1}{\sqrt{\frac{1}{c}-1}}tan^{-1}(\frac{1}{\sqrt{\frac{1}{c}-1}})+1 c=-2.2767175312280725973119838","['real-analysis', 'ordinary-differential-equations', 'continuity', 'boundary-value-problem', 'initial-value-problems']"
1,Solve the second order equation $u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t)$,Solve the second order equation,u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t),"I need to solve the second order equation $$u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t)$$ or alternatively, $$u''(t) = \frac{16}{81}t^4u(t) - \frac{432}{81 \beta }tu(t)$$ $\beta > 0$ . I'm not sure how to approach this as it is second order. The equation is in normal/regular form. Wolfram gives me a series solution, but I would like something closed form. The original equation was $$9f''-10t^2f'+\left(t^4+\left(\frac{48}{\beta}-10\right)t\right)f =0 $$ and I used the integrating factor $$f(t) = \exp\left({\frac{5}{27}t^3}\right)u(t)$$ to obtain the form seen above. EDIT: Here is a simpler related case which is solved: $$9 f^{\prime \prime}-10 t^2 f^{\prime}+\left(t^4-10 t\right) f=0$$ which tranforms into regular form by $$ f(t)=\exp \left(\frac{5}{27} t^3\right) u(t) $$ where $u(t)$ then satisfies the equation $$ u^{\prime \prime}-\frac{16}{81} t^4 u=0 $$ the solutions of which are $$ t^{1 / 2} I_{ \pm 1 / 6}(T) \text { and } t^{1 / 2} K_{1 / 6}(T) $$ where $T=\frac{4}{27} t^3$ and $I_{ \pm 1 / 6}(T)$ and $K_{1 / 6}(T)$ are the modified Bessel functions of order $\frac{1}{6}$ . In this way we see that the solutions of Eq. (1.4) are given by $$ t^{1 / 2} \exp \left(\frac{5}{27} t^3\right)\left\{I_{ \pm 1 / 6}(T), K_{1 / 6}(T)\right\}. $$","I need to solve the second order equation or alternatively, . I'm not sure how to approach this as it is second order. The equation is in normal/regular form. Wolfram gives me a series solution, but I would like something closed form. The original equation was and I used the integrating factor to obtain the form seen above. EDIT: Here is a simpler related case which is solved: which tranforms into regular form by where then satisfies the equation the solutions of which are where and and are the modified Bessel functions of order . In this way we see that the solutions of Eq. (1.4) are given by","u^{\prime \prime}(t)=\frac{16 t\left(\beta t^3-27\right)}{81 \beta} u(t) u''(t) = \frac{16}{81}t^4u(t) - \frac{432}{81 \beta }tu(t) \beta > 0 9f''-10t^2f'+\left(t^4+\left(\frac{48}{\beta}-10\right)t\right)f =0  f(t) = \exp\left({\frac{5}{27}t^3}\right)u(t) 9 f^{\prime \prime}-10 t^2 f^{\prime}+\left(t^4-10 t\right) f=0 
f(t)=\exp \left(\frac{5}{27} t^3\right) u(t)
 u(t) 
u^{\prime \prime}-\frac{16}{81} t^4 u=0
 
t^{1 / 2} I_{ \pm 1 / 6}(T) \text { and } t^{1 / 2} K_{1 / 6}(T)
 T=\frac{4}{27} t^3 I_{ \pm 1 / 6}(T) K_{1 / 6}(T) \frac{1}{6} 
t^{1 / 2} \exp \left(\frac{5}{27} t^3\right)\left\{I_{ \pm 1 / 6}(T), K_{1 / 6}(T)\right\}.
","['calculus', 'ordinary-differential-equations', 'special-functions', 'bessel-functions']"
2,Solving PDEs as ODEs under certain conditions,Solving PDEs as ODEs under certain conditions,,"(Before closing this question, yes, I've read this one , but my question is not about separation of variables). I'm currently studying second order PDEs with variable coefficients, and after reducing them to their canonical form, my professor solves them as if they were ODEs (except, of course, for the elliptical case, which I still don't know how to solve, although I reckon has something to do with Laplace's equation in 2 dimensions). I don't quite understand why we are allowed to do this. The only difference is the integration constants are now functions of the other variable. For instance, consider the equation: $$2u_{xx}+5yu_x+2y^2u=y$$ It's already reduced to its canonical parabolic form, and I've been taught to solve it by treating $y$ as a constant, which therefore leaves me with a constant coefficient ODE with the general solution being: $$u(x,y)=\frac{1}{2y}+C_1(y)e^{-xy/2}+C_2(y)e^{-2xy}$$ Again, why are we allowed to solve the PDE this way? Is there any theorem, proposition or some text I can refer to to address my doubt? To the obvious question, ""why couldn't we?"", I can give a couple of answers (which are all wrong, since we can do this): The coefficients $C_i$ could depend, for instance, on $y+a, \ \ a\in \mathbb{R}$ . This wouldn't affect the derivatives, but it could affect how the coefficients work, unless they are invariant under translation, which should be demonstrated Partial derivatives don't always behave as ordinary derivatives (e.g: the reciprocity relation), so we can't consider it trivial to treat them as such As always, thanks in advance for your kind responses.","(Before closing this question, yes, I've read this one , but my question is not about separation of variables). I'm currently studying second order PDEs with variable coefficients, and after reducing them to their canonical form, my professor solves them as if they were ODEs (except, of course, for the elliptical case, which I still don't know how to solve, although I reckon has something to do with Laplace's equation in 2 dimensions). I don't quite understand why we are allowed to do this. The only difference is the integration constants are now functions of the other variable. For instance, consider the equation: It's already reduced to its canonical parabolic form, and I've been taught to solve it by treating as a constant, which therefore leaves me with a constant coefficient ODE with the general solution being: Again, why are we allowed to solve the PDE this way? Is there any theorem, proposition or some text I can refer to to address my doubt? To the obvious question, ""why couldn't we?"", I can give a couple of answers (which are all wrong, since we can do this): The coefficients could depend, for instance, on . This wouldn't affect the derivatives, but it could affect how the coefficients work, unless they are invariant under translation, which should be demonstrated Partial derivatives don't always behave as ordinary derivatives (e.g: the reciprocity relation), so we can't consider it trivial to treat them as such As always, thanks in advance for your kind responses.","2u_{xx}+5yu_x+2y^2u=y y u(x,y)=\frac{1}{2y}+C_1(y)e^{-xy/2}+C_2(y)e^{-2xy} C_i y+a, \ \ a\in \mathbb{R}","['ordinary-differential-equations', 'partial-differential-equations']"
3,Solve $x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0$ for $0≤x<1$ with series,Solve  for  with series,x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0 0≤x<1,"Solve $x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0$ for $0≤x< 1$ I am supposed to solve via the series method. The answer should look like this: $\begin{aligned}y\left(x\right)=A\frac{x}{\left(1-x\right)^{2}}+B\frac{x}{\left(1-x\right)^{2}}\left(\ln\left(x\right)+\frac{1}{x}\right)\end{aligned},$ where $A, B$ are constants. My attempt: $\begin{array}{l}y'(x)=\sum_{n=1}^\infty na_nx^{n-1}=\sum_{n=0}^\infty(n+1)a_{n+1}x^n\\ y''(x)=\sum_{n=2}^\infty n(n-1)a_nx^{n-2}=\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n\end{array}$ Substituting, $$x(x-1)\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n+3x\sum_{n=0}^\infty(n+1)a_{n+1}x^n+\sum_{n=0}^\infty a_nx^n=0$$ This is where I am stuck with the algebra. And I have no idea how and where to apply the condition $0≤x<1$","Solve for I am supposed to solve via the series method. The answer should look like this: where are constants. My attempt: Substituting, This is where I am stuck with the algebra. And I have no idea how and where to apply the condition","x\left(x-1\right)y^{\prime\prime}+3xy^{\prime}+y=0 0≤x< 1 \begin{aligned}y\left(x\right)=A\frac{x}{\left(1-x\right)^{2}}+B\frac{x}{\left(1-x\right)^{2}}\left(\ln\left(x\right)+\frac{1}{x}\right)\end{aligned}, A, B \begin{array}{l}y'(x)=\sum_{n=1}^\infty na_nx^{n-1}=\sum_{n=0}^\infty(n+1)a_{n+1}x^n\\ y''(x)=\sum_{n=2}^\infty n(n-1)a_nx^{n-2}=\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n\end{array} x(x-1)\sum_{n=0}^\infty(n+2)(n+1)a_{n+2}x^n+3x\sum_{n=0}^\infty(n+1)a_{n+1}x^n+\sum_{n=0}^\infty a_nx^n=0 0≤x<1","['real-analysis', 'calculus', 'sequences-and-series', 'ordinary-differential-equations']"
4,Why does Wolfram Alpha give a wrong answer to a linear ODE with constant coefficients?,Why does Wolfram Alpha give a wrong answer to a linear ODE with constant coefficients?,,"This is very odd as it is a really simple ODE and the interpretation of the input seems correct, yet Wolfram Alpha produces rubbish: $$x'' - 2x' + x = t, \quad x(0) = 1.$$ Wolfram Alpha claims that the solution to this is $x(t) = c_1e^tt$ ( Link ). For reference, it gets it correct if you either put none or two initial conditions ( Link , Link ). Is this a known issue?","This is very odd as it is a really simple ODE and the interpretation of the input seems correct, yet Wolfram Alpha produces rubbish: Wolfram Alpha claims that the solution to this is ( Link ). For reference, it gets it correct if you either put none or two initial conditions ( Link , Link ). Is this a known issue?","x'' - 2x' + x = t, \quad x(0) = 1. x(t) = c_1e^tt","['ordinary-differential-equations', 'initial-value-problems', 'wolfram-alpha']"
5,"In ODE $\frac{dy}{dx}$ is a derivative, what is $\ dx$ and $\ dy$ when separated?","In ODE  is a derivative, what is  and  when separated?",\frac{dy}{dx} \ dx \ dy,"In Schaum's Outline Of Theory And Problems Of Partial Differential Equations, equations like these are quite common: \begin{align} a\,dy^2-2b \,dx\cdot dy+c\,dx^2=0\\ a^2 dt^2-dx^2=0 \end{align} I have not seen this way of specifying ODE's in the past and the syntax doesn't make a lot of sense to me. Multiplying and dividing by differentials is always a neat trick for the inverse function, but what does it actually mean to not divide afterwards? What does it mean to have an ODE in this differential form rather than the derivative $\frac{dy}{dx}$ form? Is there an advantage to this syntax? Can one solve ODEs directly in this differential form without first going through the derivative form?","In Schaum's Outline Of Theory And Problems Of Partial Differential Equations, equations like these are quite common: I have not seen this way of specifying ODE's in the past and the syntax doesn't make a lot of sense to me. Multiplying and dividing by differentials is always a neat trick for the inverse function, but what does it actually mean to not divide afterwards? What does it mean to have an ODE in this differential form rather than the derivative form? Is there an advantage to this syntax? Can one solve ODEs directly in this differential form without first going through the derivative form?","\begin{align}
a\,dy^2-2b \,dx\cdot dy+c\,dx^2=0\\
a^2 dt^2-dx^2=0
\end{align} \frac{dy}{dx}","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
6,Varying Coefficient Helmholtz Equation IVP,Varying Coefficient Helmholtz Equation IVP,,"I want to find an odd solution to: $$u''(x) = V(x) u(x)$$ for $V(x)$ an even function. I propose the following IVP: $$\partial_t \begin{pmatrix} u \\\ v \end{pmatrix} = \begin{pmatrix} 0 & 1 \\\ V(t) & 0 \end{pmatrix} \begin{pmatrix} u_0 \\\ v_0 \end{pmatrix}$$ with $$\begin{pmatrix} u_0 \\\ v_0 \end{pmatrix} = \begin{pmatrix} 0 \\\ 1 \end{pmatrix}$$ Now we have: $$\partial_t U = A(t) U(t)$$ And this can be solved as so: $$e^{-\int_0^t A(s) ds} \partial_t U = -\left(\partial_t e^{-\int_0^t A(s) ds} \right) U$$ Giving us $$\partial_t\left( e^{-\int_0^t A(s)  ds} U(t) \right) = 0$$ Hence, $$e^{-\int_0^t A(s) ds} U(t) = U(0)$$ And we have the matrix $$e^{-\int_0^t A(s) ds}$$ is always invertible, giving us: $$U(t) = e^{\int_0^t A(s) ds} U(0)$$ which can be solved by exponentiating the matrix: $$M(t) = \begin{pmatrix} 0 & t \\\ \int_0^t V(s) ds & 0 \end{pmatrix}$$ using traditional methods. If $V(t)$ is constant, this gives us the expected result. Should this approach work in general? What would the alternative approach for solving this for general $V$ be? I tried comparing results to numerical solutions, but they didn't seem to match.","I want to find an odd solution to: for an even function. I propose the following IVP: with Now we have: And this can be solved as so: Giving us Hence, And we have the matrix is always invertible, giving us: which can be solved by exponentiating the matrix: using traditional methods. If is constant, this gives us the expected result. Should this approach work in general? What would the alternative approach for solving this for general be? I tried comparing results to numerical solutions, but they didn't seem to match.",u''(x) = V(x) u(x) V(x) \partial_t \begin{pmatrix} u \\\ v \end{pmatrix} = \begin{pmatrix} 0 & 1 \\\ V(t) & 0 \end{pmatrix} \begin{pmatrix} u_0 \\\ v_0 \end{pmatrix} \begin{pmatrix} u_0 \\\ v_0 \end{pmatrix} = \begin{pmatrix} 0 \\\ 1 \end{pmatrix} \partial_t U = A(t) U(t) e^{-\int_0^t A(s) ds} \partial_t U = -\left(\partial_t e^{-\int_0^t A(s) ds} \right) U \partial_t\left( e^{-\int_0^t A(s)  ds} U(t) \right) = 0 e^{-\int_0^t A(s) ds} U(t) = U(0) e^{-\int_0^t A(s) ds} U(t) = e^{\int_0^t A(s) ds} U(0) M(t) = \begin{pmatrix} 0 & t \\\ \int_0^t V(s) ds & 0 \end{pmatrix} V(t) V,['ordinary-differential-equations']
7,A method to solve a system of nonlinear differential equations,A method to solve a system of nonlinear differential equations,,"Is the system of ODE $$x'(t)=y$$ $$y'(t)=\frac{x^2}{2}$$ solvable through analytic means? I might be missing something, since by the chain rule the second equation gives the second order ODE $y''=xy$ , which doesn't seem to help.","Is the system of ODE solvable through analytic means? I might be missing something, since by the chain rule the second equation gives the second order ODE , which doesn't seem to help.",x'(t)=y y'(t)=\frac{x^2}{2} y''=xy,['ordinary-differential-equations']
8,two constants of integration in first order ODE,two constants of integration in first order ODE,,"Let's take for example the equation: $$y=1+y'^2$$ Now I see that this equation is seperable but I don't understand why using the general method for implicit ODEs can lead to the following strange outcome. First we substitute: $$y'=p(x)\, \,\,\,\,\,\,\, \text{ so} \,\,\,\,\,\,\, y=y(x,p(x))$$ Then we take the full derivative of $y$ to find $p(x)$ : $$\frac{dy}{dx}=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}\frac{dp}{dx}=p(x)$$ And so we get: $$p(x)=2y'p'\Rightarrow p'=\frac{1}{2}$$ But $p'=y''$ So we have two options: One is to integrate twice and get an expression for $y$ : $$\int p' dx=p=\frac{x}{2}+C_1\\ \int p \,dx=\int y'dx=y=\frac{x^2}{4}+xC_1+C_2 \\ \Rightarrow y(x)=\frac{x^2}{4}+xC_1+C_2$$ Second is to solve for $p(x)$ and plug it in the original ODE: $$\int p' dx=p=\frac{x}{2}+C \\ y=1+p^2=1+(\frac{x}{2}+C)^2$$ Why do the two methods give us different answers and what is the intuitive meaning behind the second constant? I just can't wrap my head around it. Or maybe there is a mistake in the first method that I don't catch. Please explain.",Let's take for example the equation: Now I see that this equation is seperable but I don't understand why using the general method for implicit ODEs can lead to the following strange outcome. First we substitute: Then we take the full derivative of to find : And so we get: But So we have two options: One is to integrate twice and get an expression for : Second is to solve for and plug it in the original ODE: Why do the two methods give us different answers and what is the intuitive meaning behind the second constant? I just can't wrap my head around it. Or maybe there is a mistake in the first method that I don't catch. Please explain.,"y=1+y'^2 y'=p(x)\, \,\,\,\,\,\,\, \text{ so} \,\,\,\,\,\,\, y=y(x,p(x)) y p(x) \frac{dy}{dx}=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}\frac{dp}{dx}=p(x) p(x)=2y'p'\Rightarrow p'=\frac{1}{2} p'=y'' y \int p' dx=p=\frac{x}{2}+C_1\\ \int p \,dx=\int y'dx=y=\frac{x^2}{4}+xC_1+C_2 \\ \Rightarrow y(x)=\frac{x^2}{4}+xC_1+C_2 p(x) \int p' dx=p=\frac{x}{2}+C \\ y=1+p^2=1+(\frac{x}{2}+C)^2",['ordinary-differential-equations']
9,Real part of the solution is not a solution?,Real part of the solution is not a solution?,,"Take for example this very simple differential equation with complex coefficients: \begin{equation}\tag{1} f'(t) = - i f(t)  \end{equation} where t is a real variable. The solution is $f(t)=c e^{-i t}$ . Since in Physics only real quantities have sense, a physicist would take only the real part of this solution, i.e. $f(t)= \cos(t)$ . However, it is easy to see that $f(t)=\cos(t)$ is not a solution of (1) anymore. So, the question is: Of which equation is $f(t)=\cos(t)$ the solution? Is there a connection with Eq.(1) ? PS. If one looks for real $f$ solutions, then from Eq.(1) we must have $f'(t)=0$ and $-i f(t)=0$ , i.e. $f(t)=0$ , which is another different solution ! Which is the correct answer ?","Take for example this very simple differential equation with complex coefficients: where t is a real variable. The solution is . Since in Physics only real quantities have sense, a physicist would take only the real part of this solution, i.e. . However, it is easy to see that is not a solution of (1) anymore. So, the question is: Of which equation is the solution? Is there a connection with Eq.(1) ? PS. If one looks for real solutions, then from Eq.(1) we must have and , i.e. , which is another different solution ! Which is the correct answer ?","\begin{equation}\tag{1}
f'(t) = - i f(t) 
\end{equation} f(t)=c e^{-i t} f(t)= \cos(t) f(t)=\cos(t) f(t)=\cos(t) f f'(t)=0 -i f(t)=0 f(t)=0","['ordinary-differential-equations', 'partial-differential-equations', 'solution-verification', 'complex-numbers', 'physics']"
10,Calculus with little $o$ notation,Calculus with little  notation,o,"Let $f(x)$ be a smooth function for all $x \geq 0$ and suppose that it satisfies $$f'(x) = o(x^\alpha)$$ as $x \to 0$ and for some $\alpha > 0$ . Can we conclude that $$ f(x) = c + o ( x^{1+\alpha}) ? $$ I can prove this if $\alpha = 0$ almost trivially. In this case, we have $f'(x) = o(1)$ which implies that $f'(0) = 0$ . By definition of derivative, we have $$ f'(0) = \lim_{x \to 0} \frac{f(x)-f(0)}{x} = 0  $$ But now, by definition of little- $o$ notation, $$ f(x) - f(0) = o(x) \implies f(x) = f(0) + o(x). $$ How do I extend such an argument for all $\alpha$ ? In fact, I will also be happy if there is a proof for $\alpha \in {\mathbb N}$ . Here is my attempt at a proof for the case $\alpha = 1$ . I think if you agree that this proof is correct, then I can easily generalize this to all integer values of $\alpha$ . For $\alpha=1$ , we have $f'(x) = o(x)$ so $$ \lim_{x \to 0} \frac{f'(x)}{x} = 0  $$ Since $f'(0) = 0$ , we can use L'Hôpital's rule which implies $f''(0) = 0$ . We now use the following definition the second derivative $$ f''(0) = \lim_{x \to 0} \frac{2[f(x) - f(0)-xf'(0)]}{x^2}  = 0  $$ That this definition of $f''(0)$ is true follows from L'Hôpital's rule. Now using the definition of little- $o$ notation and using $f'(0) = 0$ again, we find $$ f(x) - f(0) = o(x^2) \implies f(x) = f(0) + o(x^2) .  $$ Assuming I haven't made any serious error above, this type of argument clearly generalizes trivially for all $\alpha \in {\mathbb N}$ .","Let be a smooth function for all and suppose that it satisfies as and for some . Can we conclude that I can prove this if almost trivially. In this case, we have which implies that . By definition of derivative, we have But now, by definition of little- notation, How do I extend such an argument for all ? In fact, I will also be happy if there is a proof for . Here is my attempt at a proof for the case . I think if you agree that this proof is correct, then I can easily generalize this to all integer values of . For , we have so Since , we can use L'Hôpital's rule which implies . We now use the following definition the second derivative That this definition of is true follows from L'Hôpital's rule. Now using the definition of little- notation and using again, we find Assuming I haven't made any serious error above, this type of argument clearly generalizes trivially for all .","f(x) x \geq 0 f'(x) = o(x^\alpha) x \to 0 \alpha > 0 
f(x) = c + o ( x^{1+\alpha}) ?
 \alpha = 0 f'(x) = o(1) f'(0) = 0 
f'(0) = \lim_{x \to 0} \frac{f(x)-f(0)}{x} = 0 
 o 
f(x) - f(0) = o(x) \implies f(x) = f(0) + o(x).
 \alpha \alpha \in {\mathbb N} \alpha = 1 \alpha \alpha=1 f'(x) = o(x) 
\lim_{x \to 0} \frac{f'(x)}{x} = 0 
 f'(0) = 0 f''(0) = 0 
f''(0) = \lim_{x \to 0} \frac{2[f(x) - f(0)-xf'(0)]}{x^2}  = 0 
 f''(0) o f'(0) = 0 
f(x) - f(0) = o(x^2) \implies f(x) = f(0) + o(x^2) . 
 \alpha \in {\mathbb N}","['ordinary-differential-equations', 'asymptotics']"
11,ODE with no solution,ODE with no solution,,"Given the ODE $$y''+16y=0$$ with $y(0)=y_0$ and $y(\pi/2)=y_1$ , I have to find for which $y_0$ and $y_1$ , the ODE will have none solution. I know there are $A,B\in\mathbb{R}$ such that $$y(t)=A\cos(4t)+B\sin(4t)$$ and when I use the initial conditions, I get $$y_1=y_0=A$$ Hence $$y(t)=y_0\cos(4t)+B\sin(4t)$$ However, I have any clue for what I should do now.","Given the ODE with and , I have to find for which and , the ODE will have none solution. I know there are such that and when I use the initial conditions, I get Hence However, I have any clue for what I should do now.","y''+16y=0 y(0)=y_0 y(\pi/2)=y_1 y_0 y_1 A,B\in\mathbb{R} y(t)=A\cos(4t)+B\sin(4t) y_1=y_0=A y(t)=y_0\cos(4t)+B\sin(4t)",['ordinary-differential-equations']
12,"If $x:[0,1]\rightarrow \mathbb{R}$ verifies $0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1]$ then $x(t)=0 \ \forall t\in[0,1]$",If  verifies  then,"x:[0,1]\rightarrow \mathbb{R} 0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1] x(t)=0 \ \forall t\in[0,1]","As part of my differential equations homework we have this problem: If $x:[0,1]\rightarrow \mathbb{R}$ verifies $0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1]$ then $x(t)=0 \ \forall t\in[0,1]$ I tried bounding  the integral using that $x$ is continuous and $[0,1]$ compact but I gained no useful info. Reasoning by contradiction doesn't seems useful neither, and I couldn't use any result from this unit as it was all about solving linear ODEs of order $\geq 2$ . Any help or hint is appreciated","As part of my differential equations homework we have this problem: If verifies then I tried bounding  the integral using that is continuous and compact but I gained no useful info. Reasoning by contradiction doesn't seems useful neither, and I couldn't use any result from this unit as it was all about solving linear ODEs of order . Any help or hint is appreciated","x:[0,1]\rightarrow \mathbb{R} 0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1] x(t)=0 \ \forall t\in[0,1] x [0,1] \geq 2","['ordinary-differential-equations', 'analysis', 'integral-inequality']"
13,Laplace transform problem involving piecewise function - Could you tell me where I'm going wrong?,Laplace transform problem involving piecewise function - Could you tell me where I'm going wrong?,,"I attempted to solve the following second-order differential equation using the Laplace transform method: $$2y'' + 3y' + y = g(t)$$ with the initial conditions $y(0) = y'(0) = 0$ . The input function $g(t)$ is defined as follows: $$g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases}$$ First, I applied the Laplace transform to both sides of the equation to obtain the transformed equation in the s-domain. Then, I rearranged the equation to solve for $Y(s)$ , the Laplace transform of $y(t)$ . Next, I applied partial fraction decomposition to express the transformed equation in a simpler form. I found the inverse Laplace transform of each term using tables and formulas. However, the resulting solution did not match the expected value of $y(4.5) = 0.20438$ . I'm looking for guidance and suggestions on how to correctly solve this differential equation and find the accurate value of $y(4.5)$ . If there are any alternative methods or insights that could be helpful, please share them with me. Thank you for your assistance and expertise in advance! Steps: Step 1: Define the differential equation: $$2y"" + 3y' + y = g(t)$$ Step 2: Define the input function $g(t)$ : $$g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases}$$ Step 3: Apply the Laplace transform to both sides of the differential equation. Use the following formulas: $$L{y""} = s^2Y(s) - sy(0) - y'(0)$$ $$L{y'} = sY(s) - y(0)$$ $$L{y} = Y(s)$$ After applying the Laplace transform, the differential equation becomes: $$2(s^2Y(s) - sy(0) - y'(0)) + 3(sY(s) - y(0)) + Y(s) = G(s)$$ Step 4: Apply the initial conditions: $y(0) = 0$ $y'(0) = 0$ Substitute these initial conditions into the Laplace transformed equation. Step 5: Substitute the Laplace transform of the input function $g(t)$ into the equation. Use the following formula for the Laplace transform of $g(t)$ : $$L(g(t)) = G(s) = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)$$ For each interval of $g(t)$ , substitute the corresponding Laplace transform into the equation. $$Y(s)[2s^2 + 3s + 1] = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)$$ $$Y(s) =\frac{1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)}{2s^2 + 3s + 1}$$ Stuck at this point!","I attempted to solve the following second-order differential equation using the Laplace transform method: with the initial conditions . The input function is defined as follows: First, I applied the Laplace transform to both sides of the equation to obtain the transformed equation in the s-domain. Then, I rearranged the equation to solve for , the Laplace transform of . Next, I applied partial fraction decomposition to express the transformed equation in a simpler form. I found the inverse Laplace transform of each term using tables and formulas. However, the resulting solution did not match the expected value of . I'm looking for guidance and suggestions on how to correctly solve this differential equation and find the accurate value of . If there are any alternative methods or insights that could be helpful, please share them with me. Thank you for your assistance and expertise in advance! Steps: Step 1: Define the differential equation: Step 2: Define the input function : Step 3: Apply the Laplace transform to both sides of the differential equation. Use the following formulas: After applying the Laplace transform, the differential equation becomes: Step 4: Apply the initial conditions: Substitute these initial conditions into the Laplace transformed equation. Step 5: Substitute the Laplace transform of the input function into the equation. Use the following formula for the Laplace transform of : For each interval of , substitute the corresponding Laplace transform into the equation. Stuck at this point!","2y'' + 3y' + y = g(t) y(0) = y'(0) = 0 g(t) g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases} Y(s) y(t) y(4.5) = 0.20438 y(4.5) 2y"" + 3y' + y = g(t) g(t) g(t)=\begin{cases} t, & 4<t<5 \\ 0,& \text{otherwise}\end{cases} L{y""} = s^2Y(s) - sy(0) - y'(0) L{y'} = sY(s) - y(0) L{y} = Y(s) 2(s^2Y(s) - sy(0) - y'(0)) + 3(sY(s) - y(0)) + Y(s) = G(s) y(0) = 0 y'(0) = 0 g(t) g(t) L(g(t)) = G(s) = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2) g(t) Y(s)[2s^2 + 3s + 1] = 1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2) Y(s) =\frac{1/s^2 - (e^{-4s}/s^2) + (e^{-5s}/s^2)}{2s^2 + 3s + 1}","['calculus', 'ordinary-differential-equations', 'laplace-transform', 'initial-value-problems']"
14,finding $x^{f(x)}$ given a point on $f(x)$,finding  given a point on,x^{f(x)} f(x),"If we have a function $f(x)$ where any point on the curve can be written as: $$\left((a), (a(1-f'(a)\ln{a})\right), a>0$$ What is $x^{f(x)}$ ? This was a question given to me by my teacher a while back, and i'm still struggling to solve it. He says that the question can be done without the use of any differential equations. Even though he said not to, I got it into a differential equation of the form: $$f'(x)= \frac{x-f(x)}{x\ln{x}}$$ Which did not help me at all, since I couldn't form two integrals with respect to $x$ and $y$ I've spent a very long time attempting various ways of approaching the question, however they all led to dead ends. I am so stuck and would love help with a solution to this (preferably without the use of differential equations, however I would also like to see how you would use differential equations to solve this).","If we have a function where any point on the curve can be written as: What is ? This was a question given to me by my teacher a while back, and i'm still struggling to solve it. He says that the question can be done without the use of any differential equations. Even though he said not to, I got it into a differential equation of the form: Which did not help me at all, since I couldn't form two integrals with respect to and I've spent a very long time attempting various ways of approaching the question, however they all led to dead ends. I am so stuck and would love help with a solution to this (preferably without the use of differential equations, however I would also like to see how you would use differential equations to solve this).","f(x) \left((a), (a(1-f'(a)\ln{a})\right), a>0 x^{f(x)} f'(x)= \frac{x-f(x)}{x\ln{x}} x y","['calculus', 'integration', 'ordinary-differential-equations', 'functions']"
15,How do I input a vector form of ODE's on Runge-Kutta-4 for generality to solve in matlab,How do I input a vector form of ODE's on Runge-Kutta-4 for generality to solve in matlab,,"I have come across a system of ODE's that are written on vector/Matrix format such that; $Ax'=b$ For simplicity, say the system of ODE's has a vector $x'$ containing first order derivatives of 2-variables: $x$ and $y$ with respect to $t$ such that $x' = [x1'  x2']^T $ with given initial conditions on $x1(0)=a0$ and $x2(0)=b0$ . Vector $b$ and Matrix $A$ contain dependent variables of $x1, x2, t$ . How would you generally input such a system in Runge-Kutta 4 to solve for $x1$ and $x2$ .  A general method that will be applicable for a larger ODE system as well. I am using matlab. For an actual example; suppose $$\pmatrix{1+x1^2& t-x2\\2x1x2&1}\pmatrix{x1'\\x2'}=\pmatrix{3x1^2x2^2+7x2-4\\x1-x2t+x2}$$ For the given initial conditions $x1(0)=10$ and $x2(0)=20$ How do I input such a format of ODE's into Runge-Kutta-4th order to solve in Matlab, a way that will be general for any larger bigger size of ODEs in a format $Ax'=b$ Thank you very much for your help in advance","I have come across a system of ODE's that are written on vector/Matrix format such that; For simplicity, say the system of ODE's has a vector containing first order derivatives of 2-variables: and with respect to such that with given initial conditions on and . Vector and Matrix contain dependent variables of . How would you generally input such a system in Runge-Kutta 4 to solve for and .  A general method that will be applicable for a larger ODE system as well. I am using matlab. For an actual example; suppose For the given initial conditions and How do I input such a format of ODE's into Runge-Kutta-4th order to solve in Matlab, a way that will be general for any larger bigger size of ODEs in a format Thank you very much for your help in advance","Ax'=b x' x y t x' = [x1'  x2']^T  x1(0)=a0 x2(0)=b0 b A x1, x2, t x1 x2 \pmatrix{1+x1^2& t-x2\\2x1x2&1}\pmatrix{x1'\\x2'}=\pmatrix{3x1^2x2^2+7x2-4\\x1-x2t+x2} x1(0)=10 x2(0)=20 Ax'=b","['ordinary-differential-equations', 'numerical-methods', 'computational-mathematics', 'nonlinear-analysis', 'runge-kutta-methods']"
16,Solving $ y'' + a y = f(x) $ with zero initial conditions,Solving  with zero initial conditions, y'' + a y = f(x) ,"Given the following initial value problem (IVP) $$ y'' + a y = f(x), \qquad  y(0)= y'(0) = 0$$ show that $$y = \frac1a \int_{0}^{x}f(t)\sin(ax-at)\ {\rm d}t$$ My attempt To solve the given initial value problem $$y''+ay=f(x)$$ we first solve the homogeneous equation $$y''+ay=0$$ The characteristic equation is $r^2+a=0$ , which has roots $r=\pm\sqrt{-a}$ . Depending on the sign of $a$ , these roots can be written as complex numbers or real numbers. If $a<0$ , we can write the roots as $r=\pm i\sqrt{a}$ . In this case, the general solution to the homogeneous equation is $$y_h(x) = c_1\cdot \cos\sqrt{-a}x + c_2\sin\sqrt{-a}x$$ If $a>0$ , we can write the roots as $r=±\sqrt(a)$ . In this case, the general solution to the homogeneous equation is $$y_h(x) = c_1\cdot e^{-\sqrt{a}x)} + c_2e^{\sqrt{a}x}$$ Proceeding as such only the former case yields the correct result. What about the latter?","Given the following initial value problem (IVP) show that My attempt To solve the given initial value problem we first solve the homogeneous equation The characteristic equation is , which has roots . Depending on the sign of , these roots can be written as complex numbers or real numbers. If , we can write the roots as . In this case, the general solution to the homogeneous equation is If , we can write the roots as . In this case, the general solution to the homogeneous equation is Proceeding as such only the former case yields the correct result. What about the latter?"," y'' + a y = f(x), \qquad  y(0)= y'(0) = 0 y = \frac1a \int_{0}^{x}f(t)\sin(ax-at)\ {\rm d}t y''+ay=f(x) y''+ay=0 r^2+a=0 r=\pm\sqrt{-a} a a<0 r=\pm i\sqrt{a} y_h(x) = c_1\cdot \cos\sqrt{-a}x + c_2\sin\sqrt{-a}x a>0 r=±\sqrt(a) y_h(x) = c_1\cdot e^{-\sqrt{a}x)} + c_2e^{\sqrt{a}x}","['calculus', 'ordinary-differential-equations', 'convolution', 'initial-value-problems']"
17,How to solve the differential equation $y'=\frac{y-xy^2}{x+x^2y}$,How to solve the differential equation,y'=\frac{y-xy^2}{x+x^2y},"$$y'=\frac{y-xy^2}{x+x^2y}$$ This is the equation I want to solve. My idea is to substitute $xy$ with $u,$ $u=xy$ and $\frac{du}{dx}=xy'+y.$ So, the equation becomes $y'=\frac{y(1-u)}{x(1+u)}.$ What I am struggling with is how to deal with the $x$ and $y$ that are left in the equation.","This is the equation I want to solve. My idea is to substitute with and So, the equation becomes What I am struggling with is how to deal with the and that are left in the equation.","y'=\frac{y-xy^2}{x+x^2y} xy u, u=xy \frac{du}{dx}=xy'+y. y'=\frac{y(1-u)}{x(1+u)}. x y","['integration', 'ordinary-differential-equations', 'substitution']"
18,Reduction of Order to Solve $y'' =-y^{3}$,Reduction of Order to Solve,y'' =-y^{3},"I want to solve $y'' +y^3 = 0$ with the boundary conditions $y(0) = a$ and $y(k) = b$ . My goal is to reduce this problem to $y' +y^2 = 0$ while solving but I'm not sure it can be done. I tried reduction of order substitutions (ie. taking $y' = w$ and $y'' = \frac{dw}{dy}y'$ ) but that did not work. Then I tried to solve in the following way $y'' y' = -y^3 y'$ $\frac{1}{2}[(y')^2]' = -[\frac{1}{4} y^4]'$ $\frac{1}{2}(y')^2 = -\frac{1}{4} y^4+C$ $(y')^2 = -\frac{1}{2} y^4+C$ $y' = \pm \sqrt{-\frac{1}{2} y^4+C}$ It seems to me if I take my original problem to be $y'' - 2y^3 = 0$ instead, I get $y' = \pm \sqrt{y^4+C}$ . If $C=0$ , this would reduce to $y' - y^2 = 0$ , which is close enough to what I want for my purposes. But I'm not sure how to get $C=0$ without a condition on the derivative, so maybe this was the wrong way to go. Can I reduce my original problem, $y'' +y^3 = 0$ , to $y' +y^2 = 0$ ? Where do my boundary conditions come into play?","I want to solve with the boundary conditions and . My goal is to reduce this problem to while solving but I'm not sure it can be done. I tried reduction of order substitutions (ie. taking and ) but that did not work. Then I tried to solve in the following way It seems to me if I take my original problem to be instead, I get . If , this would reduce to , which is close enough to what I want for my purposes. But I'm not sure how to get without a condition on the derivative, so maybe this was the wrong way to go. Can I reduce my original problem, , to ? Where do my boundary conditions come into play?",y'' +y^3 = 0 y(0) = a y(k) = b y' +y^2 = 0 y' = w y'' = \frac{dw}{dy}y' y'' y' = -y^3 y' \frac{1}{2}[(y')^2]' = -[\frac{1}{4} y^4]' \frac{1}{2}(y')^2 = -\frac{1}{4} y^4+C (y')^2 = -\frac{1}{2} y^4+C y' = \pm \sqrt{-\frac{1}{2} y^4+C} y'' - 2y^3 = 0 y' = \pm \sqrt{y^4+C} C=0 y' - y^2 = 0 C=0 y'' +y^3 = 0 y' +y^2 = 0,"['ordinary-differential-equations', 'boundary-value-problem', 'reduction-of-order-ode']"
19,How to solve $y(x)^{y'(x)} = |x|^{|x|}$ for $y(x)$?,How to solve  for ?,y(x)^{y'(x)} = |x|^{|x|} y(x),"To practice ordinary differential calculus, I set myself a few problems to solve. One of those problems is ""Solve $y(x)^{y'(x)} = |x|^{|x|}$ for $y(x)$ !"" with $x \in \mathbb{R} \backslash \left\{ 0 \right\}$ and $y(x) \in \mathbb{C} \backslash \left\{ 0 \right\}$ . So I started: $$ \begin{align*} y(x)^{y'(x)} &= |x|^{|x|} \quad\mid\quad \ln\left( ~~ \right)\\ \ln\left( y(x)^{y'(x)} \right) &= \ln\left( |x|^{|x|} \right)\\ y'(x) \cdot \ln\left( y(x) \right) &= |x| \cdot \ln\left( |x| \right) \quad\mid\quad \int ~\operatorname{d}x\\ \int y'(x) \cdot \ln\left( y(x) \right) ~\operatorname{d}x &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\ -y(x) + \ln\left( y(x)^{y(x)} \right) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\ \end{align*} $$ Since the equation reminds me of power towers, I tried to solve the equation with the lambert W-function: $$ \begin{align*} -y(x) + \ln\left( y(x)^{y(x)} \right) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\ -y(x) + \ln\left( y(x) \right) \cdot y(x) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\ -y(x) + \ln\left( y(x) \right) \cdot e^{\ln(y(x))} &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\ \end{align*} $$ Now comes the problem: I don't know how to continue. Is there a nice way to continue? Wolfram|Alpha tells me that there is solution to this formula via using the lambert W-function: $$y(x) = \frac{\int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}}{\operatorname{W}\left( \frac{\int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}}{e} \right)} \text{ or } y(x) = \frac{\int^{x}_{1} |\xi| \cdot \ln\left( |\xi| \right) ~\operatorname{d}\xi + c_{1}}{\operatorname{W}\left( \frac{\int^{x}_{1} |\xi| \cdot \ln\left( |\xi| \right) ~\operatorname{d}\xi + c_{1}}{e} \right)}$$ Just how?","To practice ordinary differential calculus, I set myself a few problems to solve. One of those problems is ""Solve for !"" with and . So I started: Since the equation reminds me of power towers, I tried to solve the equation with the lambert W-function: Now comes the problem: I don't know how to continue. Is there a nice way to continue? Wolfram|Alpha tells me that there is solution to this formula via using the lambert W-function: Just how?","y(x)^{y'(x)} = |x|^{|x|} y(x) x \in \mathbb{R} \backslash \left\{ 0 \right\} y(x) \in \mathbb{C} \backslash \left\{ 0 \right\} 
\begin{align*}
y(x)^{y'(x)} &= |x|^{|x|} \quad\mid\quad \ln\left( ~~ \right)\\
\ln\left( y(x)^{y'(x)} \right) &= \ln\left( |x|^{|x|} \right)\\
y'(x) \cdot \ln\left( y(x) \right) &= |x| \cdot \ln\left( |x| \right) \quad\mid\quad \int ~\operatorname{d}x\\
\int y'(x) \cdot \ln\left( y(x) \right) ~\operatorname{d}x &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\
-y(x) + \ln\left( y(x)^{y(x)} \right) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\
\end{align*}
 
\begin{align*}
-y(x) + \ln\left( y(x)^{y(x)} \right) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\
-y(x) + \ln\left( y(x) \right) \cdot y(x) &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\
-y(x) + \ln\left( y(x) \right) \cdot e^{\ln(y(x))} &= \int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}\\
\end{align*}
 y(x) = \frac{\int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}}{\operatorname{W}\left( \frac{\int |x| \cdot \ln\left( |x| \right) ~\operatorname{d}x + c_{1}}{e} \right)} \text{ or } y(x) = \frac{\int^{x}_{1} |\xi| \cdot \ln\left( |\xi| \right) ~\operatorname{d}\xi + c_{1}}{\operatorname{W}\left( \frac{\int^{x}_{1} |\xi| \cdot \ln\left( |\xi| \right) ~\operatorname{d}\xi + c_{1}}{e} \right)}","['real-analysis', 'calculus', 'ordinary-differential-equations', 'lambert-w']"
20,"Making an ODE exact, when formula's of exactness do not provide a solution","Making an ODE exact, when formula's of exactness do not provide a solution",,"I have $$\left(2x+ 1-\frac{y^2}{x^2}\right)dx+ \frac{2y}{x}dy= 0$$ which is not exact.  However, it could be made exact by using the formula for integrating factors. I have two ways, With $$M=\left(2x+ 1-\frac{y^2}{x^2}\right)$$ and $$N=\frac{2y}{x}$$ we can form the following integrating factors: $$\phi(x)=\frac{N_x-M_y}{M}=-\frac{\frac{4y}{x^2}}{2x+1-\frac{y^2}{x^2}}$$ or $$\psi(x)=\frac{M_y-N_x}{N}=\frac{ (x^3 - x^2 + y^2)}{yx^2}$$ Then multiplying these in, should give an exact form of the ODE. But neither of the two make the ODE exact. Are there other formulas one can use? Thanks","I have which is not exact.  However, it could be made exact by using the formula for integrating factors. I have two ways, With and we can form the following integrating factors: or Then multiplying these in, should give an exact form of the ODE. But neither of the two make the ODE exact. Are there other formulas one can use? Thanks",\left(2x+ 1-\frac{y^2}{x^2}\right)dx+ \frac{2y}{x}dy= 0 M=\left(2x+ 1-\frac{y^2}{x^2}\right) N=\frac{2y}{x} \phi(x)=\frac{N_x-M_y}{M}=-\frac{\frac{4y}{x^2}}{2x+1-\frac{y^2}{x^2}} \psi(x)=\frac{M_y-N_x}{N}=\frac{ (x^3 - x^2 + y^2)}{yx^2},['ordinary-differential-equations']
21,Classifying a strange ODE,Classifying a strange ODE,,"I have the given ODE: \begin{equation} y'= \frac{y^3}{x^3} + \frac{y}{x} + 1 \end{equation} which turns out to give a strange  solution: with the following solution curves: One of the curves, where $C=100$ has the following shape, which is really strange, nearly linear like $y=x$ over the origin, then it grows slowly like a quadratic equation. I am looking for a classification of this ODE. To me this appears as a nonlinear nonhomogeneous differential equation. Am I missing some more details?","I have the given ODE: which turns out to give a strange  solution: with the following solution curves: One of the curves, where has the following shape, which is really strange, nearly linear like over the origin, then it grows slowly like a quadratic equation. I am looking for a classification of this ODE. To me this appears as a nonlinear nonhomogeneous differential equation. Am I missing some more details?","\begin{equation}
y'= \frac{y^3}{x^3} + \frac{y}{x} + 1
\end{equation} C=100 y=x",['ordinary-differential-equations']
22,Euler's derivation of Lemniscate addition theorem,Euler's derivation of Lemniscate addition theorem,,"In the notes ""a Brief history of elliptic integral addition theorems"" , the author states at the very beginning of Chapter 4 that Euler found a general solution of the equation $$ \frac{dx}{\sqrt{1-x^4}}=\frac{dy}{\sqrt{1-y^4}}\tag{1} $$ to be $$ x=\frac{c\sqrt{1-y^4}+y\sqrt{1-c^4}}{1+x^2c^2}\tag{2} $$ I don't have a problem proving that this is a solution, but how could Euler have possibly arrived at such solution?","In the notes ""a Brief history of elliptic integral addition theorems"" , the author states at the very beginning of Chapter 4 that Euler found a general solution of the equation to be I don't have a problem proving that this is a solution, but how could Euler have possibly arrived at such solution?","
\frac{dx}{\sqrt{1-x^4}}=\frac{dy}{\sqrt{1-y^4}}\tag{1}
 
x=\frac{c\sqrt{1-y^4}+y\sqrt{1-c^4}}{1+x^2c^2}\tag{2}
","['ordinary-differential-equations', 'elliptic-integrals', 'elliptic-functions']"
23,Hint for solving differential equation,Hint for solving differential equation,,"In $\text{Shimer (2012)}^1$ the author describes the evolution of the unemployment rate as follows: $$\dot{u}_{t+\tau} = \dot{u}_t^s(\tau)-\left(u_{t+\tau}-u_t^s(\tau)\right)f_t,$$ where $f_t = -log(1-F_t \geq 0 \dots$ arrival rate of a Poisson process, $F_t \dots$ job finding probability, $u_t^s(\tau)\dots$ short term unemployment rate, $\tau \dots$ time between passed between two measurement points in a panel data set with monthly interviews. With $u_t^s(0)=0$ as an initial condition we should be able to come up for a solution for $u_{t+1}$ and $u^s_{t+1} \equiv u^s_t(1)$ : $$u_{t+1}=(1-F_t)u_t + u^s_{t+1}.$$ Trying to get there on my own, I struggle to find a proper solution approach for this kind of differential equations, since this resembles nothing of what I've learnt so far. I would appreciate if you could hint me towards the right direction so that I may solve this problem eventually. $^1 \text{Shimer, Robert (2012) Reassessing the ins and outs of unemployment, doi:10.1016/j.red.2012.02.001}$","In the author describes the evolution of the unemployment rate as follows: where arrival rate of a Poisson process, job finding probability, short term unemployment rate, time between passed between two measurement points in a panel data set with monthly interviews. With as an initial condition we should be able to come up for a solution for and : Trying to get there on my own, I struggle to find a proper solution approach for this kind of differential equations, since this resembles nothing of what I've learnt so far. I would appreciate if you could hint me towards the right direction so that I may solve this problem eventually.","\text{Shimer (2012)}^1 \dot{u}_{t+\tau} = \dot{u}_t^s(\tau)-\left(u_{t+\tau}-u_t^s(\tau)\right)f_t, f_t = -log(1-F_t \geq 0 \dots F_t \dots u_t^s(\tau)\dots \tau \dots u_t^s(0)=0 u_{t+1} u^s_{t+1} \equiv u^s_t(1) u_{t+1}=(1-F_t)u_t + u^s_{t+1}. ^1 \text{Shimer, Robert (2012) Reassessing the ins and outs of unemployment, doi:10.1016/j.red.2012.02.001}",['ordinary-differential-equations']
24,Show that $\mathcal{L}(X_M ) \alpha_X=0$,Show that,\mathcal{L}(X_M ) \alpha_X=0,"Let $M$ be a compact manifold on which act a compact lie group $G$ . Let $\langle\cdot,\cdot \rangle$ be a $G$ -invariant Riemannian metric on M. Let $X \in \mathfrak{g}$ , we denote $X_M$ the vector field on $M$ defined by $$X_M(m) = \frac{d}{dt} \Bigg\vert_{t=0} e^{-tX}.m, \qquad m \in M.$$ Consider the $1$ -form $\alpha_X$ defined by $$\alpha_X(Y): =\langle X_M, Y\rangle , \qquad Y \in TM. $$ $\textbf{Question:}$ Prove that $\mathcal{L} (X_M)\alpha_X  = 0.$ What I've tried so far is the following: Let $\phi_t: M \rightarrow M$ be the integral curve of $X_M.$ (By the definition of $X_M$ , we know that $\phi_t(m)= e^{-tX}.m$ . ) Applying the definition of the Lie derivative, I get for $m \in M$ and $Y \in T_mM$ \begin{align*} (\mathcal{L} (X_M)\alpha_X)_m(Y)  &= \frac{d}{dt} \Bigg\vert_{t=0} (\phi^*_t(\alpha_X))_m(Y))\\ &= \frac{d}{dt} \Bigg\vert_{t=0} {(\alpha_X)}_{\phi_t(m)}({(\phi_t)}_*(Y)) \\ &= \frac{d}{dt} \Bigg\vert_{t=0} \langle X_M(\phi_t(m)), (\phi_t)_*(Y) \rangle \\ &= \left\langle {\frac{d}{dt}}_{t=0}  X_M(\phi_t(m)),Y \right\rangle + \left\langle X_M(m), {\frac{d}{dt}}_{t=0} (\phi_t)_*(Y) \right\rangle. \end{align*} From here, all what I can say is that $$\frac{d}{dt} \Bigg\vert_{t=0}  X_M(\phi_t(m))= \frac{d}{dt}\Bigg\vert_{t=0} \frac{d}{ds} \Bigg\vert_{s=0} e^{-(t+s)X}.m $$ and $$ \frac{d}{dt} \Bigg\vert_{t=0} (\phi_t)_*(Y) = [X_M(m),Y].$$ But, I don't know how to continue. Any help please!","Let be a compact manifold on which act a compact lie group . Let be a -invariant Riemannian metric on M. Let , we denote the vector field on defined by Consider the -form defined by Prove that What I've tried so far is the following: Let be the integral curve of (By the definition of , we know that . ) Applying the definition of the Lie derivative, I get for and From here, all what I can say is that and But, I don't know how to continue. Any help please!","M G \langle\cdot,\cdot \rangle G X \in \mathfrak{g} X_M M X_M(m) = \frac{d}{dt} \Bigg\vert_{t=0} e^{-tX}.m, \qquad m \in M. 1 \alpha_X \alpha_X(Y): =\langle X_M, Y\rangle , \qquad Y \in TM.  \textbf{Question:} \mathcal{L} (X_M)\alpha_X  = 0. \phi_t: M \rightarrow M X_M. X_M \phi_t(m)= e^{-tX}.m m \in M Y \in T_mM \begin{align*}
(\mathcal{L} (X_M)\alpha_X)_m(Y) 
&= \frac{d}{dt} \Bigg\vert_{t=0} (\phi^*_t(\alpha_X))_m(Y))\\
&= \frac{d}{dt} \Bigg\vert_{t=0} {(\alpha_X)}_{\phi_t(m)}({(\phi_t)}_*(Y)) \\
&= \frac{d}{dt} \Bigg\vert_{t=0} \langle X_M(\phi_t(m)), (\phi_t)_*(Y) \rangle \\
&= \left\langle {\frac{d}{dt}}_{t=0}  X_M(\phi_t(m)),Y \right\rangle + \left\langle X_M(m), {\frac{d}{dt}}_{t=0} (\phi_t)_*(Y) \right\rangle.
\end{align*} \frac{d}{dt} \Bigg\vert_{t=0}  X_M(\phi_t(m))= \frac{d}{dt}\Bigg\vert_{t=0} \frac{d}{ds} \Bigg\vert_{s=0} e^{-(t+s)X}.m   \frac{d}{dt} \Bigg\vert_{t=0} (\phi_t)_*(Y) = [X_M(m),Y].","['ordinary-differential-equations', 'differential-geometry', 'lie-groups', 'group-actions', 'lie-derivative']"
25,"For a field $\theta \equiv \theta(t,x,y)$, is $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ for independent $x$,$y$,$t$?","For a field , is  for independent ,,?","\theta \equiv \theta(t,x,y) \frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t} x y t","Consider a field $\theta$ defined over its independent variables $x$ , $y$ and $t$ . $$\theta \equiv \theta(t,x,y) \tag{1}$$ Taking the derivative with respect to $t$ , $$\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}  +  \frac{\partial\theta}{\partial x} \frac{\mathrm{d}x}{\mathrm{d}t}  +   \frac{\partial\theta}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}t} \tag{2}$$ My colleague said that $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ because $x$ and $y$ are independent of $t$ . I wasn't too sure about that and gave the example that if we have a temperature field $\theta(t,x,y)$ defined over a two dimensional plane, equation (2) holds good but it doesn't mean anything till we define a trajectory through the plane parametrized by $x \equiv x(t)$ and $y \equiv y(t)$ . Then $\frac{\mathrm{d}x}{\mathrm{d}t}$ and $\frac{\mathrm{d}y}{\mathrm{d}t}$ take finite values and you can calculate an analytical expression for $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ . My colleague said my intuition was coming from particle mechanics (which he is right about) but for a field $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ . However, I still think that to say $\frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}$ is like defining a trajectory where you are standing at a fixed point ( $x$ , $y$ ) and evaluating $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ there. In this case, I would agree with him. However, if $x$ and $y$ are independent of $t$ , $\frac{\mathrm{d}x}{\mathrm{d}t}$ should be undefined and can't be zero because for a small change in $t$ , the corresponding change in $x$ could be anything since $x$ and $t$ are independent of each other. Can someone please shed some light on my issue regarding how do I make physical sense of $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ when $x$ and $y$ are independent of $t$ .","Consider a field defined over its independent variables , and . Taking the derivative with respect to , My colleague said that because and are independent of . I wasn't too sure about that and gave the example that if we have a temperature field defined over a two dimensional plane, equation (2) holds good but it doesn't mean anything till we define a trajectory through the plane parametrized by and . Then and take finite values and you can calculate an analytical expression for . My colleague said my intuition was coming from particle mechanics (which he is right about) but for a field . However, I still think that to say is like defining a trajectory where you are standing at a fixed point ( , ) and evaluating there. In this case, I would agree with him. However, if and are independent of , should be undefined and can't be zero because for a small change in , the corresponding change in could be anything since and are independent of each other. Can someone please shed some light on my issue regarding how do I make physical sense of when and are independent of .","\theta x y t \theta \equiv \theta(t,x,y) \tag{1} t \frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t}  +  \frac{\partial\theta}{\partial x} \frac{\mathrm{d}x}{\mathrm{d}t}  +   \frac{\partial\theta}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}t} \tag{2} \frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t} x y t \theta(t,x,y) x \equiv x(t) y \equiv y(t) \frac{\mathrm{d}x}{\mathrm{d}t} \frac{\mathrm{d}y}{\mathrm{d}t} \frac{\mathrm{d}\theta}{\mathrm{d}t} \frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t} \frac{\mathrm{d}\theta}{\mathrm{d}t} = \frac{\partial\theta}{\partial t} x y \frac{\mathrm{d}\theta}{\mathrm{d}t} x y t \frac{\mathrm{d}x}{\mathrm{d}t} t x x t \frac{\mathrm{d}\theta}{\mathrm{d}t} x y t","['ordinary-differential-equations', 'derivatives', 'partial-derivative']"
26,Differential equations: Shouldn't the method of separation of variables miss the solutions when x=0?,Differential equations: Shouldn't the method of separation of variables miss the solutions when x=0?,,"This question is regarding ""Example 6"" from the book ""Elementary differential equations with boundary value problems"" by Edwards and Penney. Consider the ODE: $x \frac{d y}{d x}=2 y$ (1) We apply the separation of variables technique to obtain: $\frac{1}{y}dy=\frac{2}{x}dx$ (2) which provides the general solution: $y(x)=C x^{2}$ (3) The book mentions that ""the general solution (3) satisfies (1) for any value of the constant C and for all values of the variable x"". However, I believe the statement is wrong when $x=0$ . By using the separation of variables, we divided by $x$ and $y$ to obtain (2). Thus, we make the implicit assumption that $x \neq 0$ , and the separation of variables will generate solutions where $x \neq 0$ . How can $y$ be defined for any $x$ , when we implicitly assume that $x \neq 0$ ?","This question is regarding ""Example 6"" from the book ""Elementary differential equations with boundary value problems"" by Edwards and Penney. Consider the ODE: (1) We apply the separation of variables technique to obtain: (2) which provides the general solution: (3) The book mentions that ""the general solution (3) satisfies (1) for any value of the constant C and for all values of the variable x"". However, I believe the statement is wrong when . By using the separation of variables, we divided by and to obtain (2). Thus, we make the implicit assumption that , and the separation of variables will generate solutions where . How can be defined for any , when we implicitly assume that ?",x \frac{d y}{d x}=2 y \frac{1}{y}dy=\frac{2}{x}dx y(x)=C x^{2} x=0 x y x \neq 0 x \neq 0 y x x \neq 0,['ordinary-differential-equations']
27,Solve the ODE $y'=1-\frac{y}{x}$,Solve the ODE,y'=1-\frac{y}{x},"I substituted $u=\frac{y}{x}$ then tried to solve the ODE $$\frac{u'}{2u-1}= -\frac{1}{x}$$ and I came this far $$\frac{1}{2}\ln |{2u-1}|=- \ln |{x}| + c_1$$ but then in the solution there was the step $$c_1=\ln c_2 \in \mathbb{R}, c_2 > 0$$ to get $$\ln |2u-1|=\ln{(\frac{c_2}{x})^2}$$ but why do we have this additional step? Couldn't we just calculate the solution without this step?",I substituted then tried to solve the ODE and I came this far but then in the solution there was the step to get but why do we have this additional step? Couldn't we just calculate the solution without this step?,"u=\frac{y}{x} \frac{u'}{2u-1}= -\frac{1}{x} \frac{1}{2}\ln |{2u-1}|=- \ln |{x}| + c_1 c_1=\ln c_2 \in \mathbb{R}, c_2 > 0 \ln |2u-1|=\ln{(\frac{c_2}{x})^2}","['ordinary-differential-equations', 'substitution']"
28,"Asymptotic convergence of ODE solutions to a unique function, regardless of initial conditions","Asymptotic convergence of ODE solutions to a unique function, regardless of initial conditions",,"I have a non-linear differential equation of the form $$ \frac{dy}{dx}=F(x) - G(y,x) $$ where $G$ is of the form $$ G(y,x) = y^3f_3(x)-y^2f_2(x) $$ and $f_2,f_3>0$ for all $x$ , and $F(x)>0$ for all $x$ . Below I attach results from numerical integration for different initial conditions (the integration is backwards from $x=3$ to $x=0$ ), indicated by the colored lines. It seems that regardless of the initial condition, the solutions seem to be asymptotic to a unique function. Is there a general statement I can use to prove this result? Namely, what conditions do $F,G$ need to satisfy in order for solutions to be asymptotic to a unique function, regardless of the initial conditions? Furthermore, it seems that a good approximation for this asymptotic behavior is given by the solution to the equation (shown as the black dashed line) $$ G(y,x)=F(x) $$ which one gets if he sets $dy/dx=0$ in the above equation (which, of course, is not true). How can one explain this?","I have a non-linear differential equation of the form where is of the form and for all , and for all . Below I attach results from numerical integration for different initial conditions (the integration is backwards from to ), indicated by the colored lines. It seems that regardless of the initial condition, the solutions seem to be asymptotic to a unique function. Is there a general statement I can use to prove this result? Namely, what conditions do need to satisfy in order for solutions to be asymptotic to a unique function, regardless of the initial conditions? Furthermore, it seems that a good approximation for this asymptotic behavior is given by the solution to the equation (shown as the black dashed line) which one gets if he sets in the above equation (which, of course, is not true). How can one explain this?","
\frac{dy}{dx}=F(x) - G(y,x)
 G 
G(y,x) = y^3f_3(x)-y^2f_2(x)
 f_2,f_3>0 x F(x)>0 x x=3 x=0 F,G 
G(y,x)=F(x)
 dy/dx=0","['ordinary-differential-equations', 'numerical-methods', 'asymptotics']"
29,Example of $\omega$-limit set with two singular points (at least).,Example of -limit set with two singular points (at least).,\omega,"According to Poincaré-Bendixson Theorem, if an orbit in the plane contains only finitely many fixed points, then the $\omega$ -limit set is either a fixed point, a periodic orbit, or a connected set composed of a finite number of fixed points together with orbits connecting them. We can see examples of the first and the second cases with the vector field $F(x,y)=(-\sin(x),y)$ . What are some examples of the third case?","According to Poincaré-Bendixson Theorem, if an orbit in the plane contains only finitely many fixed points, then the -limit set is either a fixed point, a periodic orbit, or a connected set composed of a finite number of fixed points together with orbits connecting them. We can see examples of the first and the second cases with the vector field . What are some examples of the third case?","\omega F(x,y)=(-\sin(x),y)","['ordinary-differential-equations', 'dynamical-systems']"
30,I am having trouble coming up with series solutions to differential equations,I am having trouble coming up with series solutions to differential equations,,"I have a problem that needs to be solved by Power Series Method. The equation is $$y'+y=2$$ I know this is trivial to use seperable equations so I know what the answer is: $$y(x)=c_1e^{-x}+2$$ but I can't derive it using the Taylor Series expansion. using power series the DE becomes $$x^n\cdot\sum{[c_{n+1}\cdot(n+1)+c_n]}=2$$ My recurrence relationship is $$c_{n+1}=\frac{2-c_n}{n+1}$$ I used out to $n=5$ , I have the general solution as $$c_{m+1}=\frac{(-1)^m(c_0+2[what\ I\ can't\ find])}{(m+1)!}$$ at $n=5$ $$[what\ I\ can't\ find]=2(4!-3!+2!)$$ If you increase n then the number of factorial terms increases. Any help is appreciated.","I have a problem that needs to be solved by Power Series Method. The equation is I know this is trivial to use seperable equations so I know what the answer is: but I can't derive it using the Taylor Series expansion. using power series the DE becomes My recurrence relationship is I used out to , I have the general solution as at If you increase n then the number of factorial terms increases. Any help is appreciated.",y'+y=2 y(x)=c_1e^{-x}+2 x^n\cdot\sum{[c_{n+1}\cdot(n+1)+c_n]}=2 c_{n+1}=\frac{2-c_n}{n+1} n=5 c_{m+1}=\frac{(-1)^m(c_0+2[what\ I\ can't\ find])}{(m+1)!} n=5 [what\ I\ can't\ find]=2(4!-3!+2!),"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
31,Find a weak solution of the ODE,Find a weak solution of the ODE,,"Find a weak solution to the following ODE: $u' + u = H_0(x)$ where $H_0(x) = \begin{cases} 0 & x < 0 \\ 1 & x \geq 0 \end{cases}$ My professor advised that we try to guess the solution and then verify it. My first guess was naive because I did not know the ""derivative"" (I put quotes here because this isn't really a derivative) of $H_0(x)$ was $\delta_0(x)$ . I thought it was $0$ . I still state this because something consistent is happening. If you do guess $u = H_0$ , we can go ahead and attempt to find the weak derivative. Consider $\phi \in C_{c}^{\infty}$ (continuous functions with compact support) Then, $\displaystyle-\int_{-\infty}^{\infty} \phi'(x)u(x)dx = -\int_{-\infty}^{0} \phi'(x) * 0 dx - \int_{0}^{\infty} \phi'(x) * 1 dx$ Using integration by parts, $\displaystyle-[\phi(x) * 0 |_{-\infty}^{0} + \int_{-\infty}^{0} \phi(x) * 0 dx - [\phi(x) * 1 |_{0}^{\infty} + \int_{0}^{\infty} \phi(x) * 0 dx$ The first term is $0$ due to the multiplication. The third term only leaves the lower limit because $\phi$ is compactly supported. Therefore, I am left with $\displaystyle\boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) * 0 dx$ . This is very close to what I wanted, but I have an extra $\phi(0)$ . After our next lecture, I found out that the weak derivative of $H_0(x)$ does not exist and we need the distribution derivative to make it $\delta_0(x)$ . Therefore, I knew my initial guess was wrong. My next guess was to solve the ODE for both ""components."" What I mean is solve $u'+u = 0$ and $u' + u = 1$ . Just to see if this worked, I first plugged these into wolfram alpha and got $c e^{-x}$ and $c e^{-x} + 1$ respectively. Therefore, my guess was $u = \begin{cases} ce^{-x} & x < 0 \\ ce^{-x} + 1 & x \geq 1 \end{cases}$ . Now, I attempted to find $u'$ \begin{align}&-\int_{-\infty}^{\infty} \phi'(x) u(x) dx = -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) (ce^{-x}+1) dx \\&= -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) dx\\  &= -\int_{-\infty}^{\infty} \phi'(x) ce^{-x} dx - [\phi(x) |_{0}^{\infty} \\ &= -[\phi(x) ce^{-x} |_{-\infty}^{\infty} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x} dx) + \phi(0)\\  &= \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x})dx\\ \end{align} . Again. $\phi(0)$ is there, At this point, I asked my professor if he could give me a hint. He told me my initial guess should be the solution to the ODE involving each component, which is exactly what I did. Since I did get the solutions through wolfram alpha,I went ahead and solved both ODEs by hand just to make sure something weird didn't happen. I ended up with $ce^{-x}$ and $1 - ce^{-x}$ respectively. While the second one is slightly different, it shouldn't make a difference because $c$ is a constant, so it could ""absorb"" the $-$ sign. I won't go through the details again, but one will end up with $\displaystyle\boxed{\phi(0)} + \int_{-\infty}^{0} \phi(x) (-ce^{-x}) dx + \int_{0}^{\infty} \phi(x) (ce^{-x}) = \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) u'(x) dx$ where $u'(x) = \begin{cases}  -ce^{-x} & x < 0 \\ ce^{-x} & x \geq 1 \end{cases}$ . Again, the $\phi(0)$ is still there and I'm not sure how to get rid of it! Does anyone see what I'm doing wrong? Is my initial guess still wrong? Thanks!","Find a weak solution to the following ODE: where My professor advised that we try to guess the solution and then verify it. My first guess was naive because I did not know the ""derivative"" (I put quotes here because this isn't really a derivative) of was . I thought it was . I still state this because something consistent is happening. If you do guess , we can go ahead and attempt to find the weak derivative. Consider (continuous functions with compact support) Then, Using integration by parts, The first term is due to the multiplication. The third term only leaves the lower limit because is compactly supported. Therefore, I am left with . This is very close to what I wanted, but I have an extra . After our next lecture, I found out that the weak derivative of does not exist and we need the distribution derivative to make it . Therefore, I knew my initial guess was wrong. My next guess was to solve the ODE for both ""components."" What I mean is solve and . Just to see if this worked, I first plugged these into wolfram alpha and got and respectively. Therefore, my guess was . Now, I attempted to find . Again. is there, At this point, I asked my professor if he could give me a hint. He told me my initial guess should be the solution to the ODE involving each component, which is exactly what I did. Since I did get the solutions through wolfram alpha,I went ahead and solved both ODEs by hand just to make sure something weird didn't happen. I ended up with and respectively. While the second one is slightly different, it shouldn't make a difference because is a constant, so it could ""absorb"" the sign. I won't go through the details again, but one will end up with where . Again, the is still there and I'm not sure how to get rid of it! Does anyone see what I'm doing wrong? Is my initial guess still wrong? Thanks!","u' + u = H_0(x) H_0(x) = \begin{cases}
0 & x < 0 \\
1 & x \geq 0 \end{cases} H_0(x) \delta_0(x) 0 u = H_0 \phi \in C_{c}^{\infty} \displaystyle-\int_{-\infty}^{\infty} \phi'(x)u(x)dx = -\int_{-\infty}^{0} \phi'(x) * 0 dx - \int_{0}^{\infty} \phi'(x) * 1 dx \displaystyle-[\phi(x) * 0 |_{-\infty}^{0} + \int_{-\infty}^{0} \phi(x) * 0 dx - [\phi(x) * 1 |_{0}^{\infty} + \int_{0}^{\infty} \phi(x) * 0 dx 0 \phi \displaystyle\boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) * 0 dx \phi(0) H_0(x) \delta_0(x) u'+u = 0 u' + u = 1 c e^{-x} c e^{-x} + 1 u = \begin{cases}
ce^{-x} & x < 0 \\
ce^{-x} + 1 & x \geq 1 \end{cases} u' \begin{align}&-\int_{-\infty}^{\infty} \phi'(x) u(x) dx = -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) (ce^{-x}+1) dx \\&= -\int_{-\infty}^{0} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) ce^{-x} dx - \int_{0}^{\infty} \phi'(x) dx\\ 
&= -\int_{-\infty}^{\infty} \phi'(x) ce^{-x} dx - [\phi(x) |_{0}^{\infty} \\ &= -[\phi(x) ce^{-x} |_{-\infty}^{\infty} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x} dx) + \phi(0)\\ 
&= \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) (-ce^{-x})dx\\ \end{align} \phi(0) ce^{-x} 1 - ce^{-x} c - \displaystyle\boxed{\phi(0)} + \int_{-\infty}^{0} \phi(x) (-ce^{-x}) dx + \int_{0}^{\infty} \phi(x) (ce^{-x}) = \boxed{\phi(0)} + \int_{-\infty}^{\infty} \phi(x) u'(x) dx u'(x) = \begin{cases} 
-ce^{-x} & x < 0 \\
ce^{-x} & x \geq 1 \end{cases} \phi(0)","['ordinary-differential-equations', 'weak-derivatives']"
32,Checking if $y=\ln(xy)$ is a solution of $(xy-x)y''+xy'^2+yy'-2y'=0$,Checking if  is a solution of,y=\ln(xy) (xy-x)y''+xy'^2+yy'-2y'=0,"Check whether $y=\ln (xy)$ is an answer of the following differential equation or not $$(xy-x)y''+xy'^2+yy'-2y'=0$$ First I tried to solve the equation, $$x(yy''-y''+y'^2)+yy'-2y'=0$$ $$x((yy')'-y'')+(yy')-2y'=0$$ Since I have $-y''$ in the parenthesis , the substitution $z=yy'$ doesn't work here but if it was $-2y''$ instead, I could use the substitution $u=yy'-2y'$ but it is not the case. My second try was taking derivative of the answer (i.e $y=\ln(xy)$ ) and plugging it in the D.E, $$y'=\frac1x+\frac{y'}y\quad\Rightarrow y'(1-\frac1y)=\frac1x\quad\Rightarrow y'=\frac y{y-1}\times \frac1x$$ $$y''=\frac{-1}{x^2}+\frac{yy''-y'^2}{y^2}\quad\Rightarrow y''=\frac{y}{y-1}\times(\frac{-1}{x^2}-\frac{y^2}{y'^2})$$ But it is getting really ugly when I plug $y,y',y''$ in the original equation.","Check whether is an answer of the following differential equation or not First I tried to solve the equation, Since I have in the parenthesis , the substitution doesn't work here but if it was instead, I could use the substitution but it is not the case. My second try was taking derivative of the answer (i.e ) and plugging it in the D.E, But it is getting really ugly when I plug in the original equation.","y=\ln (xy) (xy-x)y''+xy'^2+yy'-2y'=0 x(yy''-y''+y'^2)+yy'-2y'=0 x((yy')'-y'')+(yy')-2y'=0 -y'' z=yy' -2y'' u=yy'-2y' y=\ln(xy) y'=\frac1x+\frac{y'}y\quad\Rightarrow y'(1-\frac1y)=\frac1x\quad\Rightarrow y'=\frac y{y-1}\times \frac1x y''=\frac{-1}{x^2}+\frac{yy''-y'^2}{y^2}\quad\Rightarrow y''=\frac{y}{y-1}\times(\frac{-1}{x^2}-\frac{y^2}{y'^2}) y,y',y''","['ordinary-differential-equations', 'derivatives']"
33,Is that possible for a system to have an equilibrium point at infinity?,Is that possible for a system to have an equilibrium point at infinity?,,"I know for some system of ODEs, there is no  equilibrium at all, but I am wording that is valid to say a system has equilibrium at $\infty$ ? Consider the following system as an example. $$x' = \frac{y}{x}$$ $$y' = y - 4$$ So for this system, if we want to solve the equilibrium point, we need to solve the system of equation $$\frac{y}{x} = 0$$ $$y - 4 = 0$$ which give us $y = 4$ and $ 4/x = 0$ . So the only way for the first equation to be zero is as $x$ approaching $\infty$ $$ \lim_{x \to \infty}\frac{4}{x} = 0$$ but can we say $(\infty, 4)$ is an equilibrium point?","I know for some system of ODEs, there is no  equilibrium at all, but I am wording that is valid to say a system has equilibrium at ? Consider the following system as an example. So for this system, if we want to solve the equilibrium point, we need to solve the system of equation which give us and . So the only way for the first equation to be zero is as approaching but can we say is an equilibrium point?","\infty x' = \frac{y}{x} y' = y - 4 \frac{y}{x} = 0 y - 4 = 0 y = 4  4/x = 0 x \infty  \lim_{x \to \infty}\frac{4}{x} = 0 (\infty, 4)","['ordinary-differential-equations', 'limits', 'systems-of-equations', 'infinity']"
34,How to solve this kind of ordinary differential equations $t y y^{\prime \prime}-2 t\left(y^{\prime}\right)^{2}+3 y y^{\prime}=0$,How to solve this kind of ordinary differential equations,t y y^{\prime \prime}-2 t\left(y^{\prime}\right)^{2}+3 y y^{\prime}=0,"Often, to solve an ODE, it if enough to classify it correctly and apply correspondent well-known method. But in this case I struggle $t y y^{\prime \prime}-2 t\left(y^{\prime}\right)^{2}+3 y y^{\prime}=0$ It is non-linear homogeneous second-order ODE with variable coefficients. In textbooks, people talk about similar equations where one variable is missing (x or y or y'). In that case, we use substitution like y = z(z) or y' = p(x). But in my ODE, I do not see what to do.","Often, to solve an ODE, it if enough to classify it correctly and apply correspondent well-known method. But in this case I struggle It is non-linear homogeneous second-order ODE with variable coefficients. In textbooks, people talk about similar equations where one variable is missing (x or y or y'). In that case, we use substitution like y = z(z) or y' = p(x). But in my ODE, I do not see what to do.",t y y^{\prime \prime}-2 t\left(y^{\prime}\right)^{2}+3 y y^{\prime}=0,['ordinary-differential-equations']
35,Criteria for finite time blow up for two simple ODEs,Criteria for finite time blow up for two simple ODEs,,"I've got two simple questions on criteria for a finite time blow up of solutions of two simple ODEs: 1 : If $u$ is a solution of $u'=f(u)\ge0$ , $u(0)=u_0$ , how do we see that $u$ blows up in finite time (i.e. there is a $T>0$ s.t. $|u(t)|\xrightarrow{t\to T-}\infty$ ) if and only if $$\int_{u(0)}^\infty f^{-1}(s)\:{\rm d}s<\infty\tag1?$$ I've only got an idea for this if we additionally assume that $f$ is Lipschitz continuous. We then see that if $u:I\to\mathbb R$ is a solution of $u'=f(u)$ on some compact interval $I:=[a,b]$ , then: If $f(u(t_1))=0$ for some $t_1\in I$ , then $u\equiv u(t_1)$ on $I$ by uniqueness (for which we need the Lipschitz continuity of $f$ ). So, since $f\ge0$ , we see that $u$ can only blow up if $f>0$ on $[u(a),\infty)$ ... But how do we need to proceed? And can we drop the Lipschitz assumption? And what's happening for general, possibly negative, $f$ ? 2 : If $p>1$ , the solution of $u'=u^p$ is given by $$u(t)=((p-1)(T_0-t))^{-\frac1{p-1}}\;\;\;\text{for }t<T_0\tag2$$ for some $T_0\in\mathbb R$ . We see that $$T_0=\frac1{(p-1)u_0^{p-1}}\tag3$$ and hence $$u(t)=\left(\frac{u_0^{p-1}}{1-(p-1)u_0^{p-1}t}\right)^{\frac1{p-1}}\tag4.$$ I think we should have $u(t)\xrightarrow{t\to T_0-}\infty$ ; at least if $u_0>0$ . But what happens if $u_0\le0$ ? Does the solution then exists at all?","I've got two simple questions on criteria for a finite time blow up of solutions of two simple ODEs: 1 : If is a solution of , , how do we see that blows up in finite time (i.e. there is a s.t. ) if and only if I've only got an idea for this if we additionally assume that is Lipschitz continuous. We then see that if is a solution of on some compact interval , then: If for some , then on by uniqueness (for which we need the Lipschitz continuity of ). So, since , we see that can only blow up if on ... But how do we need to proceed? And can we drop the Lipschitz assumption? And what's happening for general, possibly negative, ? 2 : If , the solution of is given by for some . We see that and hence I think we should have ; at least if . But what happens if ? Does the solution then exists at all?","u u'=f(u)\ge0 u(0)=u_0 u T>0 |u(t)|\xrightarrow{t\to T-}\infty \int_{u(0)}^\infty f^{-1}(s)\:{\rm d}s<\infty\tag1? f u:I\to\mathbb R u'=f(u) I:=[a,b] f(u(t_1))=0 t_1\in I u\equiv u(t_1) I f f\ge0 u f>0 [u(a),\infty) f p>1 u'=u^p u(t)=((p-1)(T_0-t))^{-\frac1{p-1}}\;\;\;\text{for }t<T_0\tag2 T_0\in\mathbb R T_0=\frac1{(p-1)u_0^{p-1}}\tag3 u(t)=\left(\frac{u_0^{p-1}}{1-(p-1)u_0^{p-1}t}\right)^{\frac1{p-1}}\tag4. u(t)\xrightarrow{t\to T_0-}\infty u_0>0 u_0\le0","['ordinary-differential-equations', 'blowup']"
36,Clarification on the definition of General Solution,Clarification on the definition of General Solution,,"Given the differential equation $\frac{dy}{dx}=3{y^\frac{2}{3}}$ , the general solution is $\sqrt[3]{y}=x+C$ . But, there are two solutions curves that pass through (2,0) namely, $\sqrt[3]{y}=x-2$ and $y=0$ . Why do we call $\sqrt[3]{y}=x+C$ a general solution?  I thought ""general solution"" meant that it describes all solution curves, one for each value of $C$ .  But it clearly doesn't, as $y=0$ is a solution curve and it's not included. I believe I need help defining a ""general solution"". This seems to be glossed over in the textbook I am reading.","Given the differential equation , the general solution is . But, there are two solutions curves that pass through (2,0) namely, and . Why do we call a general solution?  I thought ""general solution"" meant that it describes all solution curves, one for each value of .  But it clearly doesn't, as is a solution curve and it's not included. I believe I need help defining a ""general solution"". This seems to be glossed over in the textbook I am reading.",\frac{dy}{dx}=3{y^\frac{2}{3}} \sqrt[3]{y}=x+C \sqrt[3]{y}=x-2 y=0 \sqrt[3]{y}=x+C C y=0,"['ordinary-differential-equations', 'terminology', 'initial-value-problems']"
37,"Solve the initial value problem $\frac{dy}{dx} + y = f(x) $, where $f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases}$, $y(0)=0$.","Solve the initial value problem , where , .",\frac{dy}{dx} + y = f(x)  f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases} y(0)=0,"This is a very simple first order ordinary linear differential equation: $$\frac{dy}{dx} + y = f(x) \tag{A}\label{A}$$ , where $f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases}$ with initial condition $y(0)=0$ The solutions of this differential equation according to the answer given in the book is: $y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ 2(e-1)e^{-x} \quad x\geq1 \end{cases} \tag{B}\label{B}$ However I find that answer incorrect and all the solutions of this initial value problem should be given by: $y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ ce^{-x} \quad x\geq1 \end{cases}$ , where c is an arbitrary real number. For $0\leq x \lt 1$ , we get the solution $$y=2(1-e^{-x}) \tag{i}\label{i}$$ For $x \geq 1$ , the initial condition is not valid and we simply get the solution $y=ce^{-x} \tag{ii}\label{ii}$ ,where $c\in \mathbb R$ This can be verified. It doesn't matter what real values does c take, $g(x)=ce^{-x}$ will be a one parameter family of solutions of the given initial value problem since we get an identity on $\mathbb R$ after substituting $g(x)=ce^{-x}$ for y and $g'(x)=-ce^{-x}$ for $\frac{dy}{dx}$ in \eqref{A} when $x\geq1$ (where g is a real function defined on $\mathbb R$ ) But the printed answer asserts that only a particular value of c, that is, $c=2e-2 \tag{iii}\label{iii}$ makes \eqref{ii} a solution of \eqref{A} Now the question came to my mind, why only this special value and where's that coming from? So I deduced that equating the value of y at $x=1$ in \eqref{i} (though we are not supposed to do that unless we are finding $\lim {y}$ as x tends to 1 from left) with y at $x=1$ in \eqref{ii} will lead to \eqref{iii} That can only mean one thing, we are making the solution function continuous at x=1. Why are we ensuring the continuity of solution function? As far as continuity is concerned, Is it necessary for every linear first order ordinary differential equation solution to be continuous everywhere? I have got a counter example against this statement. Consider a differential equation $d(xy)=0$ Which is linear since it can expressed as $xdy +ydx=0$ implies $\frac{dy}{dx} +\frac{y}{x}=0$ (derivative form) The one parameter family of solutions of this differential equation is given by $xy =k ,k \in \mathbb R$ i.e., $y=\frac{k}{x}$ which is discontinuous at $x=0$ (but $x=0$ is not in domain of definition) so still not sure about this counter example. There must be some reason for ensuring continuity atleast for this particular differential equation \eqref{A} or at the particular value x=1 since \eqref{B} doesn't seem like a misprint. Also I remember a similar problem solved using Laplace transformations led to a solution which matches to the solution obtained simply by integrating only after ensuring continuity. My final question is- Aren't all those solutions of the form $y=ce^{-x}, x\geq1$ lost where c is any real number other than one substituted for ensuring continuity? For example- $y=5e^{-x},x\geq1$ which is indeed an explicit solution of the initial value problem but not a member of solutions given in \eqref{B}.","This is a very simple first order ordinary linear differential equation: , where with initial condition The solutions of this differential equation according to the answer given in the book is: However I find that answer incorrect and all the solutions of this initial value problem should be given by: , where c is an arbitrary real number. For , we get the solution For , the initial condition is not valid and we simply get the solution ,where This can be verified. It doesn't matter what real values does c take, will be a one parameter family of solutions of the given initial value problem since we get an identity on after substituting for y and for in \eqref{A} when (where g is a real function defined on ) But the printed answer asserts that only a particular value of c, that is, makes \eqref{ii} a solution of \eqref{A} Now the question came to my mind, why only this special value and where's that coming from? So I deduced that equating the value of y at in \eqref{i} (though we are not supposed to do that unless we are finding as x tends to 1 from left) with y at in \eqref{ii} will lead to \eqref{iii} That can only mean one thing, we are making the solution function continuous at x=1. Why are we ensuring the continuity of solution function? As far as continuity is concerned, Is it necessary for every linear first order ordinary differential equation solution to be continuous everywhere? I have got a counter example against this statement. Consider a differential equation Which is linear since it can expressed as implies (derivative form) The one parameter family of solutions of this differential equation is given by i.e., which is discontinuous at (but is not in domain of definition) so still not sure about this counter example. There must be some reason for ensuring continuity atleast for this particular differential equation \eqref{A} or at the particular value x=1 since \eqref{B} doesn't seem like a misprint. Also I remember a similar problem solved using Laplace transformations led to a solution which matches to the solution obtained simply by integrating only after ensuring continuity. My final question is- Aren't all those solutions of the form lost where c is any real number other than one substituted for ensuring continuity? For example- which is indeed an explicit solution of the initial value problem but not a member of solutions given in \eqref{B}.","\frac{dy}{dx} + y = f(x) \tag{A}\label{A} f(x)=\begin{cases} 2 \quad 0\leq x \lt 1 \\ 0 \quad x\geq1 \end{cases} y(0)=0 y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ 2(e-1)e^{-x} \quad x\geq1 \end{cases} \tag{B}\label{B} y=\begin{cases} 2(1-e^{-x}) \quad 0\leq x \lt 1 \\ ce^{-x} \quad x\geq1 \end{cases} 0\leq x \lt 1 y=2(1-e^{-x}) \tag{i}\label{i} x \geq 1 y=ce^{-x} \tag{ii}\label{ii} c\in \mathbb R g(x)=ce^{-x} \mathbb R g(x)=ce^{-x} g'(x)=-ce^{-x} \frac{dy}{dx} x\geq1 \mathbb R c=2e-2 \tag{iii}\label{iii} x=1 \lim {y} x=1 d(xy)=0 xdy +ydx=0 \frac{dy}{dx} +\frac{y}{x}=0 xy =k ,k \in \mathbb R y=\frac{k}{x} x=0 x=0 y=ce^{-x}, x\geq1 y=5e^{-x},x\geq1","['calculus', 'ordinary-differential-equations']"
38,Solve $y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x)$,Solve,y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x),"When $y = \frac{1}{2}$ , $y' = 1$ , for $x=1$ Solve $$y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x)$$ First I converted it to $$p' +\frac{1}{x(1+\ln(x))}y' = \frac{2+\ln(x)}{1+\ln(x)} $$ Which looks like a nice first order differential equation. However, when $I = 1+ \ln(x)$ you solve for $p$ you get the following DE: $$y' = \frac{2x+\frac{1}{x}+C}{1+\ln(x)}$$ Which does not produce the right result when I put it through a calculator. Where is the mistake?","When , , for Solve First I converted it to Which looks like a nice first order differential equation. However, when you solve for you get the following DE: Which does not produce the right result when I put it through a calculator. Where is the mistake?",y = \frac{1}{2} y' = 1 x=1 y''(1+\ln(x)) + \frac{1}{x}y' = 2+\ln(x) p' +\frac{1}{x(1+\ln(x))}y' = \frac{2+\ln(x)}{1+\ln(x)}  I = 1+ \ln(x) p y' = \frac{2x+\frac{1}{x}+C}{1+\ln(x)},['ordinary-differential-equations']
39,"Characteristics of equations of the form $u_{xy}=f(u_x,u_y,u)$",Characteristics of equations of the form,"u_{xy}=f(u_x,u_y,u)","In the usual treatment of hyperbolic differential equations, it is always assumed that there are two families of characteristics. That is, if the equation $L[u]-f(u_x,u_y,u)=au_{xx}+2bu_{xy}+cu_{yy}-f(u_x,u_y,u)=0$ is hyperbolic, by definition the roots $\zeta_{\pm}$ of the polynomial $q(\zeta)=a\zeta^2-2b\zeta+c$ are real, and then one defines the characteristic curves by means of the ODEs $dy/dx=\zeta_{\pm}(x,y)$ . But for example for the equation $u_{xy}=u$ , the only root is $\zeta_+=\zeta_-=0$ , then we would have only one family of characteristics so the equation is not hyperbolic but parabolic, but this contradicts the fact that a equation is parabolic iff $b^2-ac=0$ and hyperbolic iff $b^2-ac>0$ . What's the true nature of the equations of the form $u_{xy}=f(u_x,u_y,u)$ ,  and what are the implications of the fact that only one set of characteristics exists, even though it's supposed to be a hyperbolic equation?","In the usual treatment of hyperbolic differential equations, it is always assumed that there are two families of characteristics. That is, if the equation is hyperbolic, by definition the roots of the polynomial are real, and then one defines the characteristic curves by means of the ODEs . But for example for the equation , the only root is , then we would have only one family of characteristics so the equation is not hyperbolic but parabolic, but this contradicts the fact that a equation is parabolic iff and hyperbolic iff . What's the true nature of the equations of the form ,  and what are the implications of the fact that only one set of characteristics exists, even though it's supposed to be a hyperbolic equation?","L[u]-f(u_x,u_y,u)=au_{xx}+2bu_{xy}+cu_{yy}-f(u_x,u_y,u)=0 \zeta_{\pm} q(\zeta)=a\zeta^2-2b\zeta+c dy/dx=\zeta_{\pm}(x,y) u_{xy}=u \zeta_+=\zeta_-=0 b^2-ac=0 b^2-ac>0 u_{xy}=f(u_x,u_y,u)","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
40,Complementary function / particular integral,Complementary function / particular integral,,"The function $y(x)$ satisfies $$ x^2 \frac{d^2 y}{dx^2} + (1-2a)x \frac{dy}{dx} + a^2 y = 3x^b, $$ where a and b are real parameters. Can somebody explain how to find the complementary function for this and how I would find what the particular integral would be where it is $b \neq - a$ . I have been grappling with the concept for the complementary function that $ycf(x)$ is equal to $A(x) + B(x)$ where they are constants but it is just going entirely over my head right now, especially the particular integral to the ODE.","The function satisfies where a and b are real parameters. Can somebody explain how to find the complementary function for this and how I would find what the particular integral would be where it is . I have been grappling with the concept for the complementary function that is equal to where they are constants but it is just going entirely over my head right now, especially the particular integral to the ODE.","y(x) 
x^2 \frac{d^2 y}{dx^2} + (1-2a)x \frac{dy}{dx} + a^2 y = 3x^b,
 b \neq - a ycf(x) A(x) + B(x)",['ordinary-differential-equations']
41,"Could there be exact solutions to the Lane-Emden equation for real n≥0 other than 0, 1, or 5?","Could there be exact solutions to the Lane-Emden equation for real n≥0 other than 0, 1, or 5?",,"This Astronomy SE answer says With a constant $k$ and the polytrop index $n$ . This is a result of the solutions of the Lane-Emden equation $$\frac{1}{\xi^2} \frac{\mathrm{d}}{\mathrm{d}\xi} \left(\xi^2\frac{\mathrm{d}\theta}{\mathrm{d}\xi}\right) + \theta^n = 0$$ which is a dimensionless form of the Poisson equation for a radially-symmetric self-gravitating polytropic fluid, thus where density follows a function of the form $\varrho = \varrho_c \theta^n$ with a central density $\varrho_c$ . This equation can be solved exactly for polytrop index 0 (isobaric polytrope), 1 (isothermal polytrope) and 5 (limited use as it results in infinite stellar radius)... Question: Wikipedia notes that exact solutions (not depending on converging series) are known only for $n=(0, 1, 5)$ . Could there be as-yet undiscovered exact solutions for other real $n \ge 0$ ? Or has it been shown that there are no others possible? Potentially related: I have a special solution for the Lane-Emden equation. Can I use it to find the general solution?","This Astronomy SE answer says With a constant and the polytrop index . This is a result of the solutions of the Lane-Emden equation which is a dimensionless form of the Poisson equation for a radially-symmetric self-gravitating polytropic fluid, thus where density follows a function of the form with a central density . This equation can be solved exactly for polytrop index 0 (isobaric polytrope), 1 (isothermal polytrope) and 5 (limited use as it results in infinite stellar radius)... Question: Wikipedia notes that exact solutions (not depending on converging series) are known only for . Could there be as-yet undiscovered exact solutions for other real ? Or has it been shown that there are no others possible? Potentially related: I have a special solution for the Lane-Emden equation. Can I use it to find the general solution?","k n \frac{1}{\xi^2} \frac{\mathrm{d}}{\mathrm{d}\xi} \left(\xi^2\frac{\mathrm{d}\theta}{\mathrm{d}\xi}\right) + \theta^n = 0 \varrho = \varrho_c \theta^n \varrho_c n=(0, 1, 5) n \ge 0","['ordinary-differential-equations', 'applications', 'mathematical-astronomy']"
42,Existence and Uniqueness theorem as it applies to finding an explicit solution,Existence and Uniqueness theorem as it applies to finding an explicit solution,,"If the conditions of the theorem are met for some ordinary differential equation, then we are guaranteed that a solution exists. However, I don't fully understand what it means for a solution to exist. If we can show that a solution exists, does that mean that it can be found explicitly using known methods? Or, are there some differential equations, that we know exist because of the theorem, but for which we can not find a general solution, and are thus forced to use numerical methods for an approximation?","If the conditions of the theorem are met for some ordinary differential equation, then we are guaranteed that a solution exists. However, I don't fully understand what it means for a solution to exist. If we can show that a solution exists, does that mean that it can be found explicitly using known methods? Or, are there some differential equations, that we know exist because of the theorem, but for which we can not find a general solution, and are thus forced to use numerical methods for an approximation?",,['ordinary-differential-equations']
43,Paradigm shifts from $2\to 3$,Paradigm shifts from,2\to 3,"Recently, I've been thinking about a common theme that I've seen all over mathematics. One often finds, that when the number of dimensions/degrees of freedom in a given scenario/problem changes from $2$ to $3$ , that some fundamental shifts in the solution or resulting behavior occur. I'll list five examples of what I'm talking about, three of which have to do with differential equations. (What can I say? Read my bio.) Before I do so, my questions are What are some other examples of this theme? Is there a universal reason why this happens? Does it have to do with $3$ being the first odd prime? Or perhaps something else? 1: Poincaré-Bendixson Theorem For those that don't know, the Poincare-Bendixson theorem is a deep result in ODEs/dynamical systems. Consider the autonomous ODE $$\dot{\mathrm{x}}=\mathrm{u}(\mathrm{x})\tag{1}$$ Where $\mathrm{x}:\mathbb{R}\to\mathbb{R}^n~;~\mathrm{x}:t\mapsto \mathrm{x}(t)$ , and $\mathrm{u}:\mathbb{R}^n\to\mathbb{R}^n$ . When $n=1$ the behavior of the solution is typically very easily to analyze heuristically, and in particular, it is rather obvious that only monotonic solutions exist. When $n=2$ , things obviously get a lot more complicated but there is in fact the powerful P-B theorem: Let $\Omega\subset \mathbb{R}^2$ be closed and bounded. If $u$ is $C^1$ in $\Omega$ , $\Omega$ contains no fixed points, and $\exists x_0\in\Omega$ such that the solution of the IVP $$\dot {\mathrm{x}}=\mathrm{u}(\mathrm{x})~~;~~\mathrm{x}(0)=\mathrm{x}_0$$ Is entirely contained in $\Omega$ , then there is at least one closed orbit in $\Omega$ . Quite a remarkable theorem if you ask me. The consequence of this theorem is that there is no chaotic behavior in two dimensions. In the plane, any solution of $1$ is either unbounded bounded and approaching a periodic limit cycle periodic So solutions that are bounded , but do not approach a stable limit cycle , i.e strange attractors , are impossible in the plane. However, when we jump from dimension two to dimension three, and any subsequent dimension, no similar result exists. Solutions of autonomous ODEs in $n>2$ dimensions can be as ""strange"" as you like. 2: Fermat's Last Theorem Consider the simple equation $$a^n+b^n=c^n\tag{2}$$ Where $a,b,c\in\mathbb{Z}\setminus \{0\}$ and $n\in\mathbb{N}$ . The question is, given $n$ , how many solutions $(a,b,c)$ exist to $\boldsymbol{(2)}$ ? When $n=1$ it is obvious that there are infinitely many solutions - the sum of any two integers is an integer. When $n=2$ , proving that infinitely many solutions exist is still rather easy, and known thousands of years ago to the Greeks. We can simply let $r,k\in\mathbb{N}$ with $k>r$ and observe that $$(k^2-r^2)^2+(2rk)^2=(k^2+r^2)^2$$ Since there are infinitely many pairs of positive integers $(k,r)$ with $k>r$ there are infinitely many solutions. However, as I am sure you are all aware, the general answer was not known until 1995, when Andrew Wiles published his complete and peer-reviewed proof of the problem, $358$ (!) years after the problem's conception by Fermat. His result was that For $n>2$ , no solutions to $\boldsymbol{(2)}$ exist. 3: The Three Body Problem Take a system of $n$ particles in $\mathbb{R}^3$ with positions $\mathrm{r}_1,\dots ,\mathrm{r}_n$ and masses $m_1,\dots, m_n$ and consider the the coupled vector IVP $$\ddot{\mathrm{r}}_{i}=\sum _{j\in \{1,\dotsc ,n\} \setminus \{i\}}\frac{-Gm_{i} m_{j}}{\| \mathrm{r}_{i} -\mathrm{r}_{j} \| ^{3}}(\mathrm{r}_{i} -\mathrm{r}_{j})$$ $$\mathrm{r}_i(0)=\mathrm{r}_{i,0}~~,~~\dot{\mathrm{r}}_i(0)=\dot{\mathrm{r}}_{i,0}$$ Where $i\in\{1,\dots ,n\}$ . When $n=1$ we just have a single stationary body. When $n=2$ things get a lot more interesting, but still the equations are easy to analyze and their behavior is easy to predict with numerical simulation - it is why we are able to predict solar ecplipses years in advance. In fact, taking the limiting case $m_2 \gg m_1$ some very precise equations for the motion of the bodies have been known for hundreds of years, namely Kepler's laws. However, when $n\geq 3$ the system becomes chaotic, with the particles exhibiting no obvious or predictable behavior. Is this because, unlike two points, one cannot in general find a line that goes through three arbitrary points? This reminds me a lot of example 1. 4: Commutative Division Algebras over $\mathbb{R}$ (Frobenius's Theorem) My shortest entry on this list, due to my extreme lack of knowledge about abstract algebra. There is (trivially) a one dimensional commutative division algrebra over $\mathbb{R}$ , namely $\mathbb{R}$ itself. There a two dimensional commutative division algebra over $\mathbb{R}$ , namely the complex numbers $\mathbb{C}$ . But there is in fact no commutative division algebra over $\mathbb{R}$ when $n>2$ . Once again, we see that changing the dimension from $2$ to $3$ completely changes the behavior. 5: The fundamental solution of Laplace's equation We seek to solve the equation $$(\boldsymbol{\triangle}u)(\mathrm{x})=\delta(\mathrm{x})$$ Here $u:\mathbb{R}^n\to\mathbb{R}$ , $\mathrm{x}\in\mathbb{R}^n$ , and $\delta$ is Dirac's delta distribution. It can be shown that, letting $V_n=\frac{\pi^{n/2}}{\Gamma(1+n/2)}$ be the volume (where, by volume I really mean the $n$ dimensional measure) of the unit $n$ ball, the solution is $$\Phi_n:\mathbb{R}^n\setminus \{0\}\to\mathbb{R}$$ $$\Phi _{n}(\mathrm{x}) =\begin{cases} \frac{1}{2} |\mathrm{x} | & n=1\\ \frac{1}{2\pi }\log |\mathrm{x} | & n=2\\ \frac{-1}{n( n-2) V_{n}} \ \frac{1}{|\mathrm{x} |^{n-2}} & n\geq 3 \end{cases}$$ This is actually more interesting - going from $n=1,2,3$ we start with a power law in $|\mathrm{x}|$ , then a logarithm, and then again a power law. However, once again we see a stark change in behavior when $n$ goes from $2\to 3$ . If you made it this far, thanks for reading. Consider leaving an answer giving other examples or perhaps a hand-wavy explanation.","Recently, I've been thinking about a common theme that I've seen all over mathematics. One often finds, that when the number of dimensions/degrees of freedom in a given scenario/problem changes from to , that some fundamental shifts in the solution or resulting behavior occur. I'll list five examples of what I'm talking about, three of which have to do with differential equations. (What can I say? Read my bio.) Before I do so, my questions are What are some other examples of this theme? Is there a universal reason why this happens? Does it have to do with being the first odd prime? Or perhaps something else? 1: Poincaré-Bendixson Theorem For those that don't know, the Poincare-Bendixson theorem is a deep result in ODEs/dynamical systems. Consider the autonomous ODE Where , and . When the behavior of the solution is typically very easily to analyze heuristically, and in particular, it is rather obvious that only monotonic solutions exist. When , things obviously get a lot more complicated but there is in fact the powerful P-B theorem: Let be closed and bounded. If is in , contains no fixed points, and such that the solution of the IVP Is entirely contained in , then there is at least one closed orbit in . Quite a remarkable theorem if you ask me. The consequence of this theorem is that there is no chaotic behavior in two dimensions. In the plane, any solution of is either unbounded bounded and approaching a periodic limit cycle periodic So solutions that are bounded , but do not approach a stable limit cycle , i.e strange attractors , are impossible in the plane. However, when we jump from dimension two to dimension three, and any subsequent dimension, no similar result exists. Solutions of autonomous ODEs in dimensions can be as ""strange"" as you like. 2: Fermat's Last Theorem Consider the simple equation Where and . The question is, given , how many solutions exist to ? When it is obvious that there are infinitely many solutions - the sum of any two integers is an integer. When , proving that infinitely many solutions exist is still rather easy, and known thousands of years ago to the Greeks. We can simply let with and observe that Since there are infinitely many pairs of positive integers with there are infinitely many solutions. However, as I am sure you are all aware, the general answer was not known until 1995, when Andrew Wiles published his complete and peer-reviewed proof of the problem, (!) years after the problem's conception by Fermat. His result was that For , no solutions to exist. 3: The Three Body Problem Take a system of particles in with positions and masses and consider the the coupled vector IVP Where . When we just have a single stationary body. When things get a lot more interesting, but still the equations are easy to analyze and their behavior is easy to predict with numerical simulation - it is why we are able to predict solar ecplipses years in advance. In fact, taking the limiting case some very precise equations for the motion of the bodies have been known for hundreds of years, namely Kepler's laws. However, when the system becomes chaotic, with the particles exhibiting no obvious or predictable behavior. Is this because, unlike two points, one cannot in general find a line that goes through three arbitrary points? This reminds me a lot of example 1. 4: Commutative Division Algebras over (Frobenius's Theorem) My shortest entry on this list, due to my extreme lack of knowledge about abstract algebra. There is (trivially) a one dimensional commutative division algrebra over , namely itself. There a two dimensional commutative division algebra over , namely the complex numbers . But there is in fact no commutative division algebra over when . Once again, we see that changing the dimension from to completely changes the behavior. 5: The fundamental solution of Laplace's equation We seek to solve the equation Here , , and is Dirac's delta distribution. It can be shown that, letting be the volume (where, by volume I really mean the dimensional measure) of the unit ball, the solution is This is actually more interesting - going from we start with a power law in , then a logarithm, and then again a power law. However, once again we see a stark change in behavior when goes from . If you made it this far, thanks for reading. Consider leaving an answer giving other examples or perhaps a hand-wavy explanation.","2 3 3 \dot{\mathrm{x}}=\mathrm{u}(\mathrm{x})\tag{1} \mathrm{x}:\mathbb{R}\to\mathbb{R}^n~;~\mathrm{x}:t\mapsto \mathrm{x}(t) \mathrm{u}:\mathbb{R}^n\to\mathbb{R}^n n=1 n=2 \Omega\subset \mathbb{R}^2 u C^1 \Omega \Omega \exists x_0\in\Omega \dot {\mathrm{x}}=\mathrm{u}(\mathrm{x})~~;~~\mathrm{x}(0)=\mathrm{x}_0 \Omega \Omega 1 n>2 a^n+b^n=c^n\tag{2} a,b,c\in\mathbb{Z}\setminus \{0\} n\in\mathbb{N} n (a,b,c) \boldsymbol{(2)} n=1 n=2 r,k\in\mathbb{N} k>r (k^2-r^2)^2+(2rk)^2=(k^2+r^2)^2 (k,r) k>r 358 n>2 \boldsymbol{(2)} n \mathbb{R}^3 \mathrm{r}_1,\dots ,\mathrm{r}_n m_1,\dots, m_n \ddot{\mathrm{r}}_{i}=\sum _{j\in \{1,\dotsc ,n\} \setminus \{i\}}\frac{-Gm_{i} m_{j}}{\| \mathrm{r}_{i} -\mathrm{r}_{j} \| ^{3}}(\mathrm{r}_{i} -\mathrm{r}_{j}) \mathrm{r}_i(0)=\mathrm{r}_{i,0}~~,~~\dot{\mathrm{r}}_i(0)=\dot{\mathrm{r}}_{i,0} i\in\{1,\dots ,n\} n=1 n=2 m_2 \gg m_1 n\geq 3 \mathbb{R} \mathbb{R} \mathbb{R} \mathbb{R} \mathbb{C} \mathbb{R} n>2 2 3 (\boldsymbol{\triangle}u)(\mathrm{x})=\delta(\mathrm{x}) u:\mathbb{R}^n\to\mathbb{R} \mathrm{x}\in\mathbb{R}^n \delta V_n=\frac{\pi^{n/2}}{\Gamma(1+n/2)} n n \Phi_n:\mathbb{R}^n\setminus \{0\}\to\mathbb{R} \Phi _{n}(\mathrm{x}) =\begin{cases}
\frac{1}{2} |\mathrm{x} | & n=1\\
\frac{1}{2\pi }\log |\mathrm{x} | & n=2\\
\frac{-1}{n( n-2) V_{n}} \ \frac{1}{|\mathrm{x} |^{n-2}} & n\geq 3
\end{cases} n=1,2,3 |\mathrm{x}| n 2\to 3","['abstract-algebra', 'ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'soft-question']"
44,Get $f(x)=u_x\frac{x}{u}$ from ODE for $u$,Get  from ODE for,f(x)=u_x\frac{x}{u} u,"Consider the Cauchy-Euler ODE \begin{align*} \frac{1}{2}x^2u_{xx}+xu_x-u=0. \end{align*} Guessing $u(x)=Cx^n$ gives \begin{align*} \frac{1}{2}n(n-1)Cx^n+nCx^n-Cx^n=0, \end{align*} which we can solve to get \begin{align*} n_1&=-2,\\ n_2&=1. \end{align*} Given initial conditions and boundary behavior, we can pin down a unique solution. I'm really interested in the function $$f(x)=u_x\frac{x}{u}.$$ Given the solution for $u$ , we can compute $f$ to be $f(x)=n$ , which is constant! Q: I wonder whether I can find $f$ , without solving the ODE first? Set $f=u_x\frac{x}{u}$ . Then, $u_x=\frac{fu}{x}$ and $u_{xx}=\frac{f_xu}{x}+\frac{fu_x}{x}-\frac{fu}{x^2}$ . The ODE then turns into \begin{align*} \frac{1}{2}\left(xf_xu+xfu_x-fu\right)+fu-u=0. \end{align*} Dividing this ODE by $u$ gives \begin{align*} \frac{1}{2}\left(xf_x+f^2-f\right)+f-1=0. \end{align*} If I assume $f_x=0$ , then I get \begin{align*} \frac{1}{2}f^2+\frac{1}{2}f-1=0, \end{align*} which is a normal quadratic equation with solutions \begin{align*} f_1&=-2,\\ f_2&=1. \end{align*} These are precisely the solutions I expected from the previous calculations. However, I had to assume $f_x=0$ . I only knew this because I already knew the solution. Why doesn't the second approach work? Is there a way to compute $f(x)$ without first solving the ODE for $u(x)$ ?","Consider the Cauchy-Euler ODE Guessing gives which we can solve to get Given initial conditions and boundary behavior, we can pin down a unique solution. I'm really interested in the function Given the solution for , we can compute to be , which is constant! Q: I wonder whether I can find , without solving the ODE first? Set . Then, and . The ODE then turns into Dividing this ODE by gives If I assume , then I get which is a normal quadratic equation with solutions These are precisely the solutions I expected from the previous calculations. However, I had to assume . I only knew this because I already knew the solution. Why doesn't the second approach work? Is there a way to compute without first solving the ODE for ?","\begin{align*}
\frac{1}{2}x^2u_{xx}+xu_x-u=0.
\end{align*} u(x)=Cx^n \begin{align*}
\frac{1}{2}n(n-1)Cx^n+nCx^n-Cx^n=0,
\end{align*} \begin{align*}
n_1&=-2,\\
n_2&=1.
\end{align*} f(x)=u_x\frac{x}{u}. u f f(x)=n f f=u_x\frac{x}{u} u_x=\frac{fu}{x} u_{xx}=\frac{f_xu}{x}+\frac{fu_x}{x}-\frac{fu}{x^2} \begin{align*}
\frac{1}{2}\left(xf_xu+xfu_x-fu\right)+fu-u=0.
\end{align*} u \begin{align*}
\frac{1}{2}\left(xf_x+f^2-f\right)+f-1=0.
\end{align*} f_x=0 \begin{align*}
\frac{1}{2}f^2+\frac{1}{2}f-1=0,
\end{align*} \begin{align*}
f_1&=-2,\\
f_2&=1.
\end{align*} f_x=0 f(x) u(x)","['real-analysis', 'ordinary-differential-equations', 'derivatives']"
45,Solve $(\cos^2{x}+y \sin{2x})\frac{{\rm d}y}{{\rm d}x}+y^2=0$,Solve,(\cos^2{x}+y \sin{2x})\frac{{\rm d}y}{{\rm d}x}+y^2=0,"Solve $$(\cos^2{x}+y \sin{2x}) \frac{{\rm d}y}{{\rm d}x}+y^2=0$$ Please help me to solve this. It is not possible for me to separate. Attempt : If $M=\cos^2{x}+y \sin{2x}, N=y^2$ then $M_y=\sin{2x}, N_x=0$ are not equal.",Solve Please help me to solve this. It is not possible for me to separate. Attempt : If then are not equal.,"(\cos^2{x}+y \sin{2x}) \frac{{\rm d}y}{{\rm d}x}+y^2=0 M=\cos^2{x}+y \sin{2x}, N=y^2 M_y=\sin{2x}, N_x=0",['ordinary-differential-equations']
46,Solve the differential equation ${y}'\left ( x+ y^{2} \right )= y$,Solve the differential equation,{y}'\left ( x+ y^{2} \right )= y,"Solve the differential equation $${y}'\left ( x+ y^{2} \right )= y$$ by jp . Wolfram|Alpha _the result is $\left ( 2y+ c_{1} \right )^{2}= c_{1}^{2}+ 4x\Leftrightarrow x= y\left ( y+ c_{2} \right )\Leftrightarrow 1= {y}'\left ( 2y+ c_{1} \right ),$ how can I break this cycle ?",Solve the differential equation by jp . Wolfram|Alpha _the result is how can I break this cycle ?,"{y}'\left ( x+ y^{2} \right )= y \left ( 2y+ c_{1} \right )^{2}= c_{1}^{2}+ 4x\Leftrightarrow x= y\left ( y+ c_{2} \right )\Leftrightarrow 1= {y}'\left ( 2y+ c_{1} \right ),","['real-analysis', 'calculus']"
47,Malthus Model - Solution Differential Equation,Malthus Model - Solution Differential Equation,,"I have this equation of a time dependent Malthus model with a term representing a time dependent immigration: $$N'(t)=r(t)N(t) + m(t)$$ with $r(t)$ and $m(t)$ both continuous and periodic with Period $T$ . I have to prove that the function $$N_\infty (t)=\int_{-\infty}^t exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(s)ds$$ is a solution of the ODE above. I tried to derive $N_\infty (t)$ to obtain our ODE, but unsuccessful. I think that I do somethings wrong on the calculation. I know that the derivative of an integral is so computed: Let $G(t):=\int_a^x f(t)dt$ . Then $G'(t)=f(x)$ So in our case I did: $$N'_\infty (t)= exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(t)$$ But I think I miss something. Someone can help me please?","I have this equation of a time dependent Malthus model with a term representing a time dependent immigration: with and both continuous and periodic with Period . I have to prove that the function is a solution of the ODE above. I tried to derive to obtain our ODE, but unsuccessful. I think that I do somethings wrong on the calculation. I know that the derivative of an integral is so computed: Let . Then So in our case I did: But I think I miss something. Someone can help me please?",N'(t)=r(t)N(t) + m(t) r(t) m(t) T N_\infty (t)=\int_{-\infty}^t exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(s)ds N_\infty (t) G(t):=\int_a^x f(t)dt G'(t)=f(x) N'_\infty (t)= exp\biggl(\int_s ^t r(\sigma)d\sigma\biggr)m(t),"['ordinary-differential-equations', 'solution-verification', 'integro-differential-equations']"
48,"Cannot solve the non-exact ODE, I don't know how to find the integrating factor such that make the exact ODE.","Cannot solve the non-exact ODE, I don't know how to find the integrating factor such that make the exact ODE.",,"Solve the ODE $xy' = y-x\cos ^2\left(\dfrac{y}{x}\right)$ . I have tried as below. I'm using exact/non exact method. We change the ODE into form $$\left(y-x\cos ^2\left(\dfrac{y}{x}\right)\right) dx-xdy = 0.$$ Let $M(x,y)=y-x\cos ^2\left(\dfrac{y}{x}\right)$ and $N(x,y)=-x$ . \begin{align*} 	\dfrac{\partial M(x,y)}{\partial y}&= 	\dfrac{\partial }{\partial y}\left(y-x\cos ^2\left(\dfrac{y}{x}\right)\right) = 	1+2\cos\left(\dfrac{y}{x}\right)\sin \left(\dfrac{y}{x}\right).\\ 	\dfrac{\partial N(x,y)}{\partial x}&= 	\dfrac{\partial }{\partial x}(-x) = -1. \end{align*} Since $\dfrac{\partial M(x,y)}{\partial y}\neq \dfrac{\partial N(x,y)}{\partial x}$ then that ODE is non exact ODE. Now I want to change it into exact ODE. We must find integrating factor to make the exact ODE. But now I don't know how to do it. \begin{align*} 	\dfrac{\partial M(x,y)}{\partial y} 	- 	\dfrac{\partial N(x,y)}{\partial x} 	= 	2+2\cos\left(\dfrac{y}{x}\right)\sin \left(\dfrac{y}{x}\right) \end{align*} Anyone can help me?",Solve the ODE . I have tried as below. I'm using exact/non exact method. We change the ODE into form Let and . Since then that ODE is non exact ODE. Now I want to change it into exact ODE. We must find integrating factor to make the exact ODE. But now I don't know how to do it. Anyone can help me?,"xy' = y-x\cos ^2\left(\dfrac{y}{x}\right) \left(y-x\cos ^2\left(\dfrac{y}{x}\right)\right) dx-xdy = 0. M(x,y)=y-x\cos ^2\left(\dfrac{y}{x}\right) N(x,y)=-x \begin{align*}
	\dfrac{\partial M(x,y)}{\partial y}&=
	\dfrac{\partial }{\partial y}\left(y-x\cos ^2\left(\dfrac{y}{x}\right)\right) =
	1+2\cos\left(\dfrac{y}{x}\right)\sin \left(\dfrac{y}{x}\right).\\
	\dfrac{\partial N(x,y)}{\partial x}&=
	\dfrac{\partial }{\partial x}(-x) = -1.
\end{align*} \dfrac{\partial M(x,y)}{\partial y}\neq \dfrac{\partial N(x,y)}{\partial x} \begin{align*}
	\dfrac{\partial M(x,y)}{\partial y}
	-
	\dfrac{\partial N(x,y)}{\partial x}
	=
	2+2\cos\left(\dfrac{y}{x}\right)\sin \left(\dfrac{y}{x}\right)
\end{align*}",['ordinary-differential-equations']
49,How do I solve this differential equation $y'' + y^2 = 0$?,How do I solve this differential equation ?,y'' + y^2 = 0,$y'' + y^2 = 0$ I tried in the following way: $y' =u.$ Therefore $y'' = u'$ $u' + y^2 =0$ $\int du = -y^2 \int dx$ $u = -y^2x + c$ $y' = -y^2x +c$ I don't know how to proceed further.,I tried in the following way: Therefore I don't know how to proceed further.,y'' + y^2 = 0 y' =u. y'' = u' u' + y^2 =0 \int du = -y^2 \int dx u = -y^2x + c y' = -y^2x +c,['ordinary-differential-equations']
50,Solving a first order ODE with absolute value,Solving a first order ODE with absolute value,,"In my class on differential equations, I have encountered the following initial value problem \begin{gather} 	y' = 1 + |y-1| \\ 	y(0) = 0 \end{gather} I cannot solve this. The ODE is first order, but the absolute value confuses me as I cannot integrate or do anything like that with it, the equation is nonlinear and has me stuck. How would one find the solution to this? I know existence and uniqueness applies even though the absolute value is not smooth, but because it is Lipschitz continuous I know a smooth solution must exist which I cannot seem to find. Can anyone show me how to solve this? I thank all helpers.","In my class on differential equations, I have encountered the following initial value problem I cannot solve this. The ODE is first order, but the absolute value confuses me as I cannot integrate or do anything like that with it, the equation is nonlinear and has me stuck. How would one find the solution to this? I know existence and uniqueness applies even though the absolute value is not smooth, but because it is Lipschitz continuous I know a smooth solution must exist which I cannot seem to find. Can anyone show me how to solve this? I thank all helpers.","\begin{gather}
	y' = 1 + |y-1| \\
	y(0) = 0
\end{gather}","['integration', 'ordinary-differential-equations', 'derivatives', 'absolute-value', 'initial-value-problems']"
51,Inconsistent solution to differential equation,Inconsistent solution to differential equation,,"$Q.$ Let f(x) be a continuous function satisfying the following differential equation- $$f(x)=(1+x^2)(1+\int_0^x\frac{f^2(t)dt}{1+t^2})$$ $$\text{Find  f(1)}$$ My Work- 1)Putting $x=0$ in the equation we get $f(0)=1$ 2)Dividing by $1+x^2$ and differentiating w.r.t. $x$ we get- $$(\frac{y}{1+x^2})'=\frac{y^2}{1+x^2}\qquad\text{∴ y=f(x)}$$ Simplifying- $$y'(1+x^2)-2xy=y^2(1+x^2)$$ so either $f(x)=0$ or $$\frac{-dy(1+x^2)}{y^2}+\frac{2xdx}{y}=(-1-x^2)dx$$ $$\frac{d}{dx}(\frac{1+x^2}{y})=\frac{d}{dx}(-x-\frac{x^3}{3})$$ $$\frac{1+x^2}{y}=-x-\frac{x^3}{3}+c$$ Using $y(0)=1$ we get $c=1$ and hence $$f(1)=-6$$ My issue- looking at the question, no matter the value of f(x), the R.H.S. of the equation must always be positive (squared) however the answer is coming to be negative. Am I missing something? Or is there any error in the question? Or is $f(x)=0$ the only acceptable solution?","Let f(x) be a continuous function satisfying the following differential equation- My Work- 1)Putting in the equation we get 2)Dividing by and differentiating w.r.t. we get- Simplifying- so either or Using we get and hence My issue- looking at the question, no matter the value of f(x), the R.H.S. of the equation must always be positive (squared) however the answer is coming to be negative. Am I missing something? Or is there any error in the question? Or is the only acceptable solution?",Q. f(x)=(1+x^2)(1+\int_0^x\frac{f^2(t)dt}{1+t^2}) \text{Find  f(1)} x=0 f(0)=1 1+x^2 x (\frac{y}{1+x^2})'=\frac{y^2}{1+x^2}\qquad\text{∴ y=f(x)} y'(1+x^2)-2xy=y^2(1+x^2) f(x)=0 \frac{-dy(1+x^2)}{y^2}+\frac{2xdx}{y}=(-1-x^2)dx \frac{d}{dx}(\frac{1+x^2}{y})=\frac{d}{dx}(-x-\frac{x^3}{3}) \frac{1+x^2}{y}=-x-\frac{x^3}{3}+c y(0)=1 c=1 f(1)=-6 f(x)=0,['ordinary-differential-equations']
52,Proving existence of solution for ODE $-s\varphi' + f'(\varphi)\varphi' = \varphi''$,Proving existence of solution for ODE,-s\varphi' + f'(\varphi)\varphi' = \varphi'',"Let $f : \Bbb{R} \to \Bbb{R}$ be twice differentiable with $f'' > 0$ , and let $u_- > u_+$ be real numbers. Show that there exists a solution $\varphi(x)$ to the following differential equation: $$ -s\varphi' + f'(\varphi)\varphi' = \varphi'' \tag{1} $$ such that $\lim_{x \to \pm\infty} \varphi(x) = u_\pm$ , and where $s = \frac{f(u_+) - f(u_-)}{u_+ - u_-}$ . My initial attempt is to observe that this DE can be nicely integrated to the following: $$ \varphi' = f(\varphi) - s\varphi + C \tag{2} $$ Thus, it suffices to show the existence of a solution for this DE instead, where we are free to choose $C$ . I attempted to bring RHS over to LHS, which gives: $$ \int \frac{1}{f(\varphi) - s\varphi + C} \; \mathrm{d}\varphi = x + D $$ where $D \in \Bbb{R}$ . Thus, if we define: $$ g(x) = \int \frac{1}{f(x) - sx + C} \; \mathrm{d}x $$ and assuming that $g$ is invertible, then $\varphi(x) = g^{-1}(x)$ would be a solution to $(2)$ . However, there are a few issues in this approach that we need to tackle: The integral will not make sense if $f(\varphi) - s\varphi + C$ vanishes at some point in $\Bbb{R}$ . As we are free to choose $C$ , if we can show that $f(\varphi) - s\varphi$ is bounded from either above or below, then such a choice of $C$ will exist. I suspect we can use the convexity and the definition of $s$ to prove this, but my attempts are futile so far. Should the integral make sense, another problem is if $g$ is invertible. However, this should not be an issue as by FTOC: $$ g'(x) = \frac{1}{f(x) - sx + C} $$ so if the denominator does not vanish, $g'$ is continuous and so must be strictly positive or negative, hence $g$ is strictly monotone, thus invertible. The biggest issue here is that this definition does not guarantee the requirement of $\lim_{x \to \pm\infty} \varphi(x) = u_\pm$ . I tried to manipulate the integral to fit this condition, but to no avail so far. I also tried other approaches, such as using Picard's iteration, but as this problem is not really an IVP they have not been successful. Any help is appreciated.","Let be twice differentiable with , and let be real numbers. Show that there exists a solution to the following differential equation: such that , and where . My initial attempt is to observe that this DE can be nicely integrated to the following: Thus, it suffices to show the existence of a solution for this DE instead, where we are free to choose . I attempted to bring RHS over to LHS, which gives: where . Thus, if we define: and assuming that is invertible, then would be a solution to . However, there are a few issues in this approach that we need to tackle: The integral will not make sense if vanishes at some point in . As we are free to choose , if we can show that is bounded from either above or below, then such a choice of will exist. I suspect we can use the convexity and the definition of to prove this, but my attempts are futile so far. Should the integral make sense, another problem is if is invertible. However, this should not be an issue as by FTOC: so if the denominator does not vanish, is continuous and so must be strictly positive or negative, hence is strictly monotone, thus invertible. The biggest issue here is that this definition does not guarantee the requirement of . I tried to manipulate the integral to fit this condition, but to no avail so far. I also tried other approaches, such as using Picard's iteration, but as this problem is not really an IVP they have not been successful. Any help is appreciated.","f : \Bbb{R} \to \Bbb{R} f'' > 0 u_- > u_+ \varphi(x) 
-s\varphi' + f'(\varphi)\varphi' = \varphi'' \tag{1}
 \lim_{x \to \pm\infty} \varphi(x) = u_\pm s = \frac{f(u_+) - f(u_-)}{u_+ - u_-} 
\varphi' = f(\varphi) - s\varphi + C \tag{2}
 C 
\int \frac{1}{f(\varphi) - s\varphi + C} \; \mathrm{d}\varphi = x + D
 D \in \Bbb{R} 
g(x) = \int \frac{1}{f(x) - sx + C} \; \mathrm{d}x
 g \varphi(x) = g^{-1}(x) (2) f(\varphi) - s\varphi + C \Bbb{R} C f(\varphi) - s\varphi C s g 
g'(x) = \frac{1}{f(x) - sx + C}
 g' g \lim_{x \to \pm\infty} \varphi(x) = u_\pm","['ordinary-differential-equations', 'partial-differential-equations']"
53,"If $y'(t) = -\sqrt{y(t)}$, $y(0) = 0$ and $y(t) \ge 0$. Then $y \equiv 0$ on $\mathbb{R}$","If ,  and . Then  on",y'(t) = -\sqrt{y(t)} y(0) = 0 y(t) \ge 0 y \equiv 0 \mathbb{R},"Consider the following ODE $$y'(t) = -\sqrt{y(t)},$$ with initial condition $y(0) = 0$ . Does this initial value problem have a unique solution ( $y \equiv 0$ on $\mathbb{R}$ ) if we further assume that $y(t) \geq 0$ for every $t$ ? We can see that if $y(t) \geq 0$ for every $t$ , then $y' \leq 0$ , so $y$ is decreasing. As $y(0) = 0$ , we get that $y \equiv 0$ on $[0, \infty)$ . However, can we conclude that $y \equiv 0$ on $\mathbb{R}$ ?","Consider the following ODE with initial condition . Does this initial value problem have a unique solution ( on ) if we further assume that for every ? We can see that if for every , then , so is decreasing. As , we get that on . However, can we conclude that on ?","y'(t) = -\sqrt{y(t)}, y(0) = 0 y \equiv 0 \mathbb{R} y(t) \geq 0 t y(t) \geq 0 t y' \leq 0 y y(0) = 0 y \equiv 0 [0, \infty) y \equiv 0 \mathbb{R}",['ordinary-differential-equations']
54,Determine if the autonomous system is Hamiltonian,Determine if the autonomous system is Hamiltonian,,"Consider the autonomous system $\dot{x}=-y-\alpha ^2xy^2$ and $\dot{y}=x^3$ , where $\alpha$ is a real parameter. (a) For which values of $\alpha$ is this system Hamiltonian? For each case, find the Hamiltonian. (b) For each value of $\alpha$ , find all equilibrium solutions of the above system. Can the principle of linearized stability be used to determine their stability? (c) Show that for all $\alpha \in R$ the origin is a stable equilibrium. (Hint: Can you use Hamiltonian functions from (a)?). So the way to solve this is to use the fact that $\dot{x}=\partial H/\partial y$ and $\dot{y}=-\partial H/\partial x$ , where $H(x,y)$ is the Hamiltonian for this system. But then $H(x,y)=-\frac{x^4}{4}+V(y)$ and $H(x,y)=-\frac{y^2}{2}+V(x)$ but I don't know how to solve this further.  The only way to have Hamiltonian is if $\alpha =0$ , ie. $H(x,y)=-\frac{x^4}{4}-\frac{y^2}{2}$ . Then we will have system $\dot{x}=-y$ and $\dot{y}=x^3$ . Is this right? So only $\alpha$ that would work is if $\alpha =0$ and then only equilibrium will be the origin $(0,0)$ . Then the Jacobian would be $Df=\begin{pmatrix} 0 & -1 \\ 3x^2 & 0 \end{pmatrix}=\begin{pmatrix} 0 & -1 \\ 0 &0\end{pmatrix}$ . So then what is equilibrium solution of this system and can principle of linearized stability be used to determine its stability? How do we show for $\alpha \in R$ , the origin is a stable equilibrium? I am not sure if I got the Hamiltonian correct, since I am assuming only $\alpha =0$ will give a Hamiltonian. But the questions assume there are other $\alpha$ values that will work just as well. Please help.","Consider the autonomous system and , where is a real parameter. (a) For which values of is this system Hamiltonian? For each case, find the Hamiltonian. (b) For each value of , find all equilibrium solutions of the above system. Can the principle of linearized stability be used to determine their stability? (c) Show that for all the origin is a stable equilibrium. (Hint: Can you use Hamiltonian functions from (a)?). So the way to solve this is to use the fact that and , where is the Hamiltonian for this system. But then and but I don't know how to solve this further.  The only way to have Hamiltonian is if , ie. . Then we will have system and . Is this right? So only that would work is if and then only equilibrium will be the origin . Then the Jacobian would be . So then what is equilibrium solution of this system and can principle of linearized stability be used to determine its stability? How do we show for , the origin is a stable equilibrium? I am not sure if I got the Hamiltonian correct, since I am assuming only will give a Hamiltonian. But the questions assume there are other values that will work just as well. Please help.","\dot{x}=-y-\alpha ^2xy^2 \dot{y}=x^3 \alpha \alpha \alpha \alpha \in R \dot{x}=\partial H/\partial y \dot{y}=-\partial H/\partial x H(x,y) H(x,y)=-\frac{x^4}{4}+V(y) H(x,y)=-\frac{y^2}{2}+V(x) \alpha =0 H(x,y)=-\frac{x^4}{4}-\frac{y^2}{2} \dot{x}=-y \dot{y}=x^3 \alpha \alpha =0 (0,0) Df=\begin{pmatrix} 0 & -1 \\ 3x^2 & 0 \end{pmatrix}=\begin{pmatrix} 0 & -1 \\ 0 &0\end{pmatrix} \alpha \in R \alpha =0 \alpha",['ordinary-differential-equations']
55,Finding the central manifold of a dynamical system,Finding the central manifold of a dynamical system,,"Take the dynamical system: $$x' = 0.5(1-x)xy$$ $$y' = -y(1-x)^3-y^2(2x^2-1.5x+0.5)-2x(1-x)^4+x(1-x)^3.$$ I want to find the central manifold and deduce its dynamics (stable or unstable). The above system is already in the following required form: $$ x' = Ax + f(x,y)$$ $$y' = By + g(x,y)$$ where necessarily $A=0$ and $B=-1$ . Given this, we can parameterise the centre manifold by: $$h(x) = ax^2+bx^3+cx^4 +O(x^5).$$ First, we compute $y' = \frac{dh}{dx}x'$ which is: $$ y' = a^2x^4 + O(x^5)$$ and we compare it with the $y'$ from the above dynamical system, which is: $$y' = -x+(5-a)x^2+(3a-b-9)x^3+(-\frac{a^2}{2}-3a+3b-c+7)x^4 + O(x^5).$$ Comparing coefficients between the two $y'$ 's gives $a=5$ , $b=6$ and $c=-27.5$ . This means that the centre manifold should be parameterised by: $$h(x) = 5x^2+6x^3-27.5x^4 +O(x^5).$$ Question : I do not believe the stated $h(x)$ to be the correct approximation to the manifold . You can see the correct centre manifold in the figure of the phase plane for the system I have attached. If you plot $h(x)$ on something like Desmos, you can clearly see that it is not a good approximation. Can you spot an error in my working or have I not included something I should have? Thanks","Take the dynamical system: I want to find the central manifold and deduce its dynamics (stable or unstable). The above system is already in the following required form: where necessarily and . Given this, we can parameterise the centre manifold by: First, we compute which is: and we compare it with the from the above dynamical system, which is: Comparing coefficients between the two 's gives , and . This means that the centre manifold should be parameterised by: Question : I do not believe the stated to be the correct approximation to the manifold . You can see the correct centre manifold in the figure of the phase plane for the system I have attached. If you plot on something like Desmos, you can clearly see that it is not a good approximation. Can you spot an error in my working or have I not included something I should have? Thanks","x' = 0.5(1-x)xy y' = -y(1-x)^3-y^2(2x^2-1.5x+0.5)-2x(1-x)^4+x(1-x)^3.  x' = Ax + f(x,y) y' = By + g(x,y) A=0 B=-1 h(x) = ax^2+bx^3+cx^4 +O(x^5). y' = \frac{dh}{dx}x'  y' = a^2x^4 + O(x^5) y' y' = -x+(5-a)x^2+(3a-b-9)x^3+(-\frac{a^2}{2}-3a+3b-c+7)x^4 + O(x^5). y' a=5 b=6 c=-27.5 h(x) = 5x^2+6x^3-27.5x^4 +O(x^5). h(x) h(x)","['calculus', 'ordinary-differential-equations', 'manifolds', 'taylor-expansion', 'dynamical-systems']"
56,Using Runge-Kutta integration to increase the speed and stability of gradient descent?,Using Runge-Kutta integration to increase the speed and stability of gradient descent?,,"For a gradient descent problem with $\mathbf{x}\in \mathbb{R}^N$ I can evaluate the gradient $\mathbf{\nabla}_\mathbf{x} \in \mathbb{R}^N$ that reduces the least squares error, $y$ . However, simply updating the position using $\mathbf{x'} = \mathbf{x} + \mathbf{\nabla}_\mathbf{x}$ converges very slowly to the global minimum of the least squares error (which is also the global minimum of the gradient magnitude, where the gradient is zero). I tried simply scaling up the step, i.e. $\mathbf{x'} = \mathbf{x} + h\mathbf{\nabla}_\mathbf{x}$ , however while this dramatically improves convergence times in some cases, it can become unstable in others (particularly when some of the components of $\mathbf{\nabla}_\mathbf{x}$ are much larger than others -- scaling up all components of the gradient can cause the gradient descent method to ""climb up the side of a canyon"" rather than descending the canyon, and the system can either oscillate or explode). I would like to use the 3rd order Runge-Kutta method to follow the curvature of the gradient space, so that I can take larger steps without the system blowing up. I have applied this to simulating mass-spring systems before (using Runge-Kutta integration to integrate acceleration to find velocity, and velocity to find position) -- however I can't figure out how to apply it to this gradient descent problem. I think I have some fundamental misunderstanding about how the Runge-Kutta methods work. They requires a function $f=(x, y)$ to be defined, which I believe computes the gradient of the curve at $x$ . However I don't understand why $y$ needs to be supplied to the function -- isn't $y$ a function of $x$ ? Can Runge-Kutta even be applied to the gradient descent problem? It seems like there should be a way to adapt Runge-Kutta to gradient descent, since each update step $\mathbf{x'} = \mathbf{x} + \mathbf{\nabla}_\mathbf{x}$ is basically an integration step. Is the step size $h$ simply the magnitude of the gradient, i.e. $h_i = |{\mathbf{\nabla}_{\mathbf{x}_i}}|$ and $\mathbf{y}_i = {\mathbf{\nabla}_{\mathbf{x}_i}} / h_i$ ? If Runge-Kutta is not applicable here, can somebody please suggest a robust and fast gradient descent algorithm to try? Some more detail: in the case of this problem, the gradient surface is fairly smooth, and quite strongly convex (there are few if any local minima that are not global minima), but the error surface is less convex. In other words, sometimes gradient descent will continue walking down the gradient slope in the direction of the global minimum of gradient, and the least squares error will increase temporarily before decreasing to the global minimum of least squares error. (The gradient is not computed from the least squares error measure itself, but using a different method that directly identifies the locally-best least squares solution, which moves the system closer to the globally-optimal least squares solution.) The gradient is therefore more reliable for gradient descent than the slope of the least squares error surface.","For a gradient descent problem with I can evaluate the gradient that reduces the least squares error, . However, simply updating the position using converges very slowly to the global minimum of the least squares error (which is also the global minimum of the gradient magnitude, where the gradient is zero). I tried simply scaling up the step, i.e. , however while this dramatically improves convergence times in some cases, it can become unstable in others (particularly when some of the components of are much larger than others -- scaling up all components of the gradient can cause the gradient descent method to ""climb up the side of a canyon"" rather than descending the canyon, and the system can either oscillate or explode). I would like to use the 3rd order Runge-Kutta method to follow the curvature of the gradient space, so that I can take larger steps without the system blowing up. I have applied this to simulating mass-spring systems before (using Runge-Kutta integration to integrate acceleration to find velocity, and velocity to find position) -- however I can't figure out how to apply it to this gradient descent problem. I think I have some fundamental misunderstanding about how the Runge-Kutta methods work. They requires a function to be defined, which I believe computes the gradient of the curve at . However I don't understand why needs to be supplied to the function -- isn't a function of ? Can Runge-Kutta even be applied to the gradient descent problem? It seems like there should be a way to adapt Runge-Kutta to gradient descent, since each update step is basically an integration step. Is the step size simply the magnitude of the gradient, i.e. and ? If Runge-Kutta is not applicable here, can somebody please suggest a robust and fast gradient descent algorithm to try? Some more detail: in the case of this problem, the gradient surface is fairly smooth, and quite strongly convex (there are few if any local minima that are not global minima), but the error surface is less convex. In other words, sometimes gradient descent will continue walking down the gradient slope in the direction of the global minimum of gradient, and the least squares error will increase temporarily before decreasing to the global minimum of least squares error. (The gradient is not computed from the least squares error measure itself, but using a different method that directly identifies the locally-best least squares solution, which moves the system closer to the globally-optimal least squares solution.) The gradient is therefore more reliable for gradient descent than the slope of the least squares error surface.","\mathbf{x}\in \mathbb{R}^N \mathbf{\nabla}_\mathbf{x} \in \mathbb{R}^N y \mathbf{x'} = \mathbf{x} + \mathbf{\nabla}_\mathbf{x} \mathbf{x'} = \mathbf{x} + h\mathbf{\nabla}_\mathbf{x} \mathbf{\nabla}_\mathbf{x} f=(x, y) x y y x \mathbf{x'} = \mathbf{x} + \mathbf{\nabla}_\mathbf{x} h h_i = |{\mathbf{\nabla}_{\mathbf{x}_i}}| \mathbf{y}_i = {\mathbf{\nabla}_{\mathbf{x}_i}} / h_i","['ordinary-differential-equations', 'numerical-methods', 'least-squares', 'gradient-descent', 'runge-kutta-methods']"
57,$\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!}$=$\frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}}$,=,\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!} \frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}},I am told to prove that : $$\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!} = \frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}}$$ where $H_n(x)$ is Hermite polynomial.I am wondering how to prove it.please help me how to prove this. Thanks in advance!,I am told to prove that : where is Hermite polynomial.I am wondering how to prove it.please help me how to prove this. Thanks in advance!,\sum_{n=0}^\infty\frac{H_n(x)H_n(y)t^n}{2^nn!} = \frac{\exp\left[\frac{2xyt-(x^2+y^2)t^2}{1-t^2}\right]}{\sqrt{1-t^2}} H_n(x),"['calculus', 'ordinary-differential-equations', 'functions', 'special-functions', 'hermite-polynomials']"
58,Finding an integration factor $x^ay^b$ to solve an ODE,Finding an integration factor  to solve an ODE,x^ay^b,"I have to solve an ODE: $$2(y-3x)dx+x\left(3-\frac{4x}{y} \right) dy=0$$ I am given that I have to use the integrating factor x^a(y^b) where a and b are real numbers in order to turn the problem into a solvable exact ODE. The problem is that I am unsure of how to find these real numbers. From my understanding, an ODE is exact with the integrating factor (I) if the partial derivative of the dx w.r.t y is equal to the partial derivative of the dy w.r.t x (both multiplied with I). I tried doing this but I am unsure of how to proceed with the algebra. So far I have: If anyone can help with the algebra to find the values for a and b that would be appreciated","I have to solve an ODE: I am given that I have to use the integrating factor x^a(y^b) where a and b are real numbers in order to turn the problem into a solvable exact ODE. The problem is that I am unsure of how to find these real numbers. From my understanding, an ODE is exact with the integrating factor (I) if the partial derivative of the dx w.r.t y is equal to the partial derivative of the dy w.r.t x (both multiplied with I). I tried doing this but I am unsure of how to proceed with the algebra. So far I have: If anyone can help with the algebra to find the values for a and b that would be appreciated",2(y-3x)dx+x\left(3-\frac{4x}{y} \right) dy=0,"['integration', 'ordinary-differential-equations', 'integrating-factor']"
59,ODE with nested boundary layers,ODE with nested boundary layers,,"Problem: Consider the equation $$\varepsilon^3 \frac{d^2y}{dx^2} + 2x^3 \frac{dy}{dx} - 4\varepsilon y = 2x^3 \qquad \qquad y(0) = a \;, \; y(1)=b$$ in the limit as $\varepsilon \rightarrow 0^+$ , where $0<a<b-1$ . Assuming that there are nested boundary layers at $x=0$ , determine the thickness of the boundary layers and the leading-order additive composite solution. Question: My attempt is shown below. Basically, I don't understand how you are meant to match the different solutions in the various domains. Would be grateful if someone could explain this to me, or give me some hints. Attempt: Firstly, the leading term $y_0$ of the outer solution satisfies $$2x^3 \frac{dy_0}{dx} =2x^3 \qquad \qquad y_0(1)=b$$ This is easily solved to give $\color{red}{y_0(x) = x+b-1}$ . Now suppose we scale the equation with $x=\varepsilon ^\alpha X$ where $\alpha>0$ and $X = \mathcal O(1)$ . The equation becomes $$\varepsilon^{3-2\alpha} \frac{d^2y}{dX^2} + 2\varepsilon^{2\alpha}X^3 \frac{dy}{dX} - 4\varepsilon y = 2\varepsilon^{3\alpha}X^3$$ The possible leading order balances come from $\alpha = 1$ and $\alpha = 1/2$ . When $\alpha = 1/2$ (i.e. $x = \varepsilon^{1/2} X$ ), the leading order term $Y_0$ in this layer should satisfy $$2X^3 \frac{dY_0}{dX} - 4Y_0 = 0 \qquad \qquad Y_0(X=0) = a$$ The general solution is $\color{red}{Y_0(X) = A\exp (-1/X^2)}$ , which cannot satisfy the boundary condition, since $a>0$ . So I suppose this is why we need another boundary layer. Scaling instead with $x = \varepsilon \tilde X$ , the leading order solution $\tilde Y_0$ in this layer satisfies $$\frac{d^2\tilde Y_0}{d\tilde X^2} - 4\tilde Y_0=0\qquad \qquad \tilde Y_0(\tilde X = 0) = a$$ The solution is $\color{red}{\tilde Y_0(\tilde X) = \tilde A\sinh(2\tilde X)} \color{green}{+a\cosh(2\tilde X)}$ . $\color{blue}{\text{Now the problem is, how do I match these three solutions?}}$ From what I have learnt, I need to do something like $$\lim_{x\rightarrow 0^+} y_0(x) = \lim_{X\rightarrow +\infty}Y_0(X)$$ to obtain $\color{blue}{A= b-1?}$ But if I similarly try to do $$\lim_{X\rightarrow 0^+}Y_0(X) = \lim_{\tilde X\rightarrow +\infty}\tilde Y_0(\tilde X)$$ it doesn't work, because $\tilde Y_0$ is not bounded??","Problem: Consider the equation in the limit as , where . Assuming that there are nested boundary layers at , determine the thickness of the boundary layers and the leading-order additive composite solution. Question: My attempt is shown below. Basically, I don't understand how you are meant to match the different solutions in the various domains. Would be grateful if someone could explain this to me, or give me some hints. Attempt: Firstly, the leading term of the outer solution satisfies This is easily solved to give . Now suppose we scale the equation with where and . The equation becomes The possible leading order balances come from and . When (i.e. ), the leading order term in this layer should satisfy The general solution is , which cannot satisfy the boundary condition, since . So I suppose this is why we need another boundary layer. Scaling instead with , the leading order solution in this layer satisfies The solution is . From what I have learnt, I need to do something like to obtain But if I similarly try to do it doesn't work, because is not bounded??","\varepsilon^3 \frac{d^2y}{dx^2} + 2x^3 \frac{dy}{dx} - 4\varepsilon y = 2x^3 \qquad \qquad y(0) = a \;, \; y(1)=b \varepsilon \rightarrow 0^+ 0<a<b-1 x=0 y_0 2x^3 \frac{dy_0}{dx} =2x^3 \qquad \qquad y_0(1)=b \color{red}{y_0(x) = x+b-1} x=\varepsilon ^\alpha X \alpha>0 X = \mathcal O(1) \varepsilon^{3-2\alpha} \frac{d^2y}{dX^2} + 2\varepsilon^{2\alpha}X^3 \frac{dy}{dX} - 4\varepsilon y = 2\varepsilon^{3\alpha}X^3 \alpha = 1 \alpha = 1/2 \alpha = 1/2 x = \varepsilon^{1/2} X Y_0 2X^3 \frac{dY_0}{dX} - 4Y_0 = 0 \qquad \qquad Y_0(X=0) = a \color{red}{Y_0(X) = A\exp (-1/X^2)} a>0 x = \varepsilon \tilde X \tilde Y_0 \frac{d^2\tilde Y_0}{d\tilde X^2} - 4\tilde Y_0=0\qquad \qquad \tilde Y_0(\tilde X = 0) = a \color{red}{\tilde Y_0(\tilde X) = \tilde A\sinh(2\tilde X)} \color{green}{+a\cosh(2\tilde X)} \color{blue}{\text{Now the problem is, how do I match these three solutions?}} \lim_{x\rightarrow 0^+} y_0(x) = \lim_{X\rightarrow +\infty}Y_0(X) \color{blue}{A= b-1?} \lim_{X\rightarrow 0^+}Y_0(X) = \lim_{\tilde X\rightarrow +\infty}\tilde Y_0(\tilde X) \tilde Y_0","['ordinary-differential-equations', 'asymptotics', 'perturbation-theory', 'boundary-layer']"
60,Absolutely continuous and almost everywhere solution of a controlled dynamical system,Absolutely continuous and almost everywhere solution of a controlled dynamical system,,"This is a doubt i had while reading the first chapter form the book Mathematical Control Theory, Jerzy Zabczyk , in the first chapter, page $11$ the author talks about solution for a class of differential equations: Consider the system of differential equations $$\frac{dq}{dt}=A(t)q(t)+a(t),~~q(t_0)=q_0 \in \mathbb{R}^n$$ on a fixed interval $[0,T]$ ; $t_0 \in [0,T]$ where $A \in M(n,n)$ i.e space of all $n \times n$ matrices over $\mathbb{R}$ , $A(t)=[a_{ij}, i=1,\ldots,n,j=1,\ldots,m]$ , $a(t) \in \mathbb{R}^n$ defined as $a(t)=(a_i(t),i=1,\ldots,n)$ , whete $t \in [0,T]$ . Now he states the theorem Theorem : Assume the elements of the function $A(\cdot)$ are locally integrable. Then there exists exactly one function $S(t)~,~t\in [0,T]$ with values in $M(n,n)$ and with absolutely continuous elements such that $$\begin{align}\frac{d}{dt}S(t)&=A(t)S(t)~~,~~\text{for almost all}~t\in [0,T] \\S(0)&=I \end{align}$$ In addition, a matrix $S(t)$ is invertible for an arbitrary $t \in [0,T]$ , and the unique solution of the equation $(1)$ is of the form $$q(t)=S(t)S^{-1}(t_0)q_0+\int_{t_0}^tS(t)S^{-1}(s)a(s)ds,~t\in[0,T]$$ The proof is not that hard to follow, he uses Banach's Fixed point theorem for the firt part , and the second part need some work but doable. But what confuses me are the parts Almost everywhere and absolutely continuous in the statement of the theorem, what does these terms mean actually in this context, i'm not able to follow these two definitions. Will be thankful if someone provides a easy definition for these. Thank you !","This is a doubt i had while reading the first chapter form the book Mathematical Control Theory, Jerzy Zabczyk , in the first chapter, page the author talks about solution for a class of differential equations: Consider the system of differential equations on a fixed interval ; where i.e space of all matrices over , , defined as , whete . Now he states the theorem Theorem : Assume the elements of the function are locally integrable. Then there exists exactly one function with values in and with absolutely continuous elements such that In addition, a matrix is invertible for an arbitrary , and the unique solution of the equation is of the form The proof is not that hard to follow, he uses Banach's Fixed point theorem for the firt part , and the second part need some work but doable. But what confuses me are the parts Almost everywhere and absolutely continuous in the statement of the theorem, what does these terms mean actually in this context, i'm not able to follow these two definitions. Will be thankful if someone provides a easy definition for these. Thank you !","11 \frac{dq}{dt}=A(t)q(t)+a(t),~~q(t_0)=q_0 \in \mathbb{R}^n [0,T] t_0 \in [0,T] A \in M(n,n) n \times n \mathbb{R} A(t)=[a_{ij}, i=1,\ldots,n,j=1,\ldots,m] a(t) \in \mathbb{R}^n a(t)=(a_i(t),i=1,\ldots,n) t \in [0,T] A(\cdot) S(t)~,~t\in [0,T] M(n,n) \begin{align}\frac{d}{dt}S(t)&=A(t)S(t)~~,~~\text{for almost all}~t\in [0,T] \\S(0)&=I \end{align} S(t) t \in [0,T] (1) q(t)=S(t)S^{-1}(t_0)q_0+\int_{t_0}^tS(t)S^{-1}(s)a(s)ds,~t\in[0,T]","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'almost-everywhere']"
61,How to find the particular integral?,How to find the particular integral?,,The question is to find the particular integral of the expression $$ (D^3+1)y = cos(2x-1) $$ I’m not sure how to go about it. How do I find the PI for this expression?,The question is to find the particular integral of the expression I’m not sure how to go about it. How do I find the PI for this expression?, (D^3+1)y = cos(2x-1) ,[]
62,Particular integral of a differential equation having a complicated exponential function,Particular integral of a differential equation having a complicated exponential function,,"So I have to solve the differential equation $x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-y=x^2e^{2x}$ I identified this as a differential in Cauchy-Euler form, and used the substitution x=e^t to obtain $\frac{d^2y}{dx^2}-y=e^{2(t+e^t)}$ The Complementary function is pretty easy. Now I cannot understand how to find the Particular Integral. I rearranged the equation as $y=\frac{e^{2t}.e^{2e^t}}{D^2-1}$ where $D=\frac{d}{dt}$ . Then I substituted $D+2$ in place of $D$ according to the known rule. Now what should I do? Also, is what I've done till now correct? I've worked with differential equations where exponential functions were paired with trigonometric or linear functions, but not such functions before. Any help is appreciated. Thanks in advance.","So I have to solve the differential equation I identified this as a differential in Cauchy-Euler form, and used the substitution x=e^t to obtain The Complementary function is pretty easy. Now I cannot understand how to find the Particular Integral. I rearranged the equation as where . Then I substituted in place of according to the known rule. Now what should I do? Also, is what I've done till now correct? I've worked with differential equations where exponential functions were paired with trigonometric or linear functions, but not such functions before. Any help is appreciated. Thanks in advance.",x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-y=x^2e^{2x} \frac{d^2y}{dx^2}-y=e^{2(t+e^t)} y=\frac{e^{2t}.e^{2e^t}}{D^2-1} D=\frac{d}{dt} D+2 D,['ordinary-differential-equations']
63,Flow of a vanishing Vector Field.,Flow of a vanishing Vector Field.,,"Let us consider a complete vector field $V:\Bbb C^n\to\Bbb C^n$ . Let us call $\phi_s:\Bbb C^n\to\Bbb C^n$ its flow at time $s\in\Bbb C$ , that is $$ \dot{\phi}_s(x)=V(\phi_s(x))\;\;\forall x\in\Bbb C^n, s\in\Bbb C $$ $$ \phi_0(x)=x\;\;\forall x\in\Bbb C^n $$ How can I prove that $V\equiv0$ on $A$ implies $\phi_s(x)=x \;\;\forall x\in A$ and $s\in\Bbb C$ ? Set $A$ as the common zero-set of a finite number of holomorphic polynomials (but maybe my statement holds for more general sets $A$ , like closed sets).","Let us consider a complete vector field . Let us call its flow at time , that is How can I prove that on implies and ? Set as the common zero-set of a finite number of holomorphic polynomials (but maybe my statement holds for more general sets , like closed sets).","V:\Bbb C^n\to\Bbb C^n \phi_s:\Bbb C^n\to\Bbb C^n s\in\Bbb C 
\dot{\phi}_s(x)=V(\phi_s(x))\;\;\forall x\in\Bbb C^n, s\in\Bbb C
 
\phi_0(x)=x\;\;\forall x\in\Bbb C^n
 V\equiv0 A \phi_s(x)=x \;\;\forall x\in A s\in\Bbb C A A","['ordinary-differential-equations', 'differential-geometry']"
64,How to solve this non-linear 1st order differential equation?,How to solve this non-linear 1st order differential equation?,,"I am trying to solve the following first order differential equation: $$ g'(R)=-2 \sqrt{R^2-g(R)}+2 R$$ By direct substitution it can be verified that an obvious solution is $$g(R)=R^2.$$ However, when doing the transformation $y(R)=R^2-g(R)$ , we get $$y'(R)=2 \sqrt{y(R)} $$ which is separable and gives (for some constant $C$ ): $$\int \frac{dy}{2y^{1/2}}=R+C $$ or $$y^{1/2}= R+C,\\ y=(R+C)^2.$$ Remembering that $y(R)=R^2-g(R)$ we finally get $$ g(R)= -C^2-2C R .$$ I also tried solving it with Mathematica , but it also misses the other solution. So, my question is where did the obvious solution $g(R)=R^2$ go?","I am trying to solve the following first order differential equation: By direct substitution it can be verified that an obvious solution is However, when doing the transformation , we get which is separable and gives (for some constant ): or Remembering that we finally get I also tried solving it with Mathematica , but it also misses the other solution. So, my question is where did the obvious solution go?"," g'(R)=-2 \sqrt{R^2-g(R)}+2 R g(R)=R^2. y(R)=R^2-g(R) y'(R)=2 \sqrt{y(R)}  C \int \frac{dy}{2y^{1/2}}=R+C  y^{1/2}= R+C,\\
y=(R+C)^2. y(R)=R^2-g(R)  g(R)= -C^2-2C R . g(R)=R^2","['ordinary-differential-equations', 'solution-verification']"
65,why do we assume this solution form in the method of variation of constants?,why do we assume this solution form in the method of variation of constants?,,"In Variation of parameters for linear ODEs of second orders, we assume that the particular solution we are looking for is of the form $y_p=u_1y_1+u_2y_2$ , where $y_1$ and $y_2$ are two solutions for the homogeneous part of the ODE, and $u_1$ and $u_2$ are two functions to determine.  Why do we assume that $y_p$ must be of this form? I can guess that should be like this based on ODEs of first orders: If we are dealing with the equation $a_1(x)y'+a_2(x)y=f(x)$ and we know that $y_h$ is a solution of $a_1(x)y'+a_2(x)y=0$ , then $\frac{y_p}{y_h}$ cannot be a constant, otherwise, $y_p$ would be a solution of the homogeneous part, hence, $u(x)=\frac{y_p}{y_h}$ is a nonconstant function.   From this, we write $y_p=u(x)y_h$ and proceed as the method suggests (I am not finishing because I am assuming the reader is familiar with it). Based on this, I can guess that for the general case should be $y_p=u_1y_1+\cdots+u_ny_n$ , where $y_1,\dots , y_n$ are solutions for the homogeneous part and $u_1,\cdots, u_n$ to determine.  Whether or not this guess works is not convincing me. Any idea is welcome to understand this. Thanks","In Variation of parameters for linear ODEs of second orders, we assume that the particular solution we are looking for is of the form , where and are two solutions for the homogeneous part of the ODE, and and are two functions to determine.  Why do we assume that must be of this form? I can guess that should be like this based on ODEs of first orders: If we are dealing with the equation and we know that is a solution of , then cannot be a constant, otherwise, would be a solution of the homogeneous part, hence, is a nonconstant function.   From this, we write and proceed as the method suggests (I am not finishing because I am assuming the reader is familiar with it). Based on this, I can guess that for the general case should be , where are solutions for the homogeneous part and to determine.  Whether or not this guess works is not convincing me. Any idea is welcome to understand this. Thanks","y_p=u_1y_1+u_2y_2 y_1 y_2 u_1 u_2 y_p a_1(x)y'+a_2(x)y=f(x) y_h a_1(x)y'+a_2(x)y=0 \frac{y_p}{y_h} y_p u(x)=\frac{y_p}{y_h} y_p=u(x)y_h y_p=u_1y_1+\cdots+u_ny_n y_1,\dots , y_n u_1,\cdots, u_n",['ordinary-differential-equations']
66,When do the solutions of a linear ODE system lie on ellipses?,When do the solutions of a linear ODE system lie on ellipses?,,"Let $x(t) \in \mathbb{R}^n$ and $A(t)$ be a $n\times n$ matrix.  When $A(t)$ is a skew-symmmetric matrix, the solutions of the linear system $\dot{x} = A(t) x$ lie on spheres centered at the origin since $$ \frac{1}{2} \frac{d}{dt} \lvert x \rvert^2 = \langle \dot{x}, x \rangle = \langle Ax, x \rangle = 0 \implies \lvert x \rvert^2 = \lvert x(0) \rvert^2 $$ Is there a specific class of matrices $A(t)$ for which the solutions lie on ellipses instead?","Let and be a matrix.  When is a skew-symmmetric matrix, the solutions of the linear system lie on spheres centered at the origin since Is there a specific class of matrices for which the solutions lie on ellipses instead?","x(t) \in \mathbb{R}^n A(t) n\times n A(t) \dot{x} = A(t) x 
\frac{1}{2} \frac{d}{dt} \lvert x \rvert^2 = \langle \dot{x}, x \rangle = \langle Ax, x \rangle = 0 \implies \lvert x \rvert^2 = \lvert x(0) \rvert^2
 A(t)",['ordinary-differential-equations']
67,Find a fundamental set of solutions for $t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0$.,Find a fundamental set of solutions for .,t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0,Find a fundamental set of solutions for $t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0$ . I haven't learned any techniques on how to solve an $n$ th order DE with nonconstant coefficients besides the Euler-Cauchy equation. Any ideas will be helpful!,Find a fundamental set of solutions for . I haven't learned any techniques on how to solve an th order DE with nonconstant coefficients besides the Euler-Cauchy equation. Any ideas will be helpful!,t^{2}y^{(4)}+ty^{(3)}+y^{(2)}-4y=0 n,[]
68,How to solve $y''(t) + 4 y (t) = 2t \cos (2t)$ on time in the exam?,How to solve  on time in the exam?,y''(t) + 4 y (t) = 2t \cos (2t),"I'm solving this ODE, which is from last year final exam Solve in $\mathbb R$ the ODE $$y''(t) + 4 y (t) = 2t \cos (2t)$$ This ODE is not hard to solve, but it takes me a lot of time to compute the derivative $y',y''$ and simplifies them. This process is unfortunately likely to cause mistake. The last year exam contains a total of $4$ questions, and has a duration of $1$ hour and $30$ minutes. Solving this ODE is just one of $4$ questions. I would like to ask for a faster way to solve this ODE. Maybe it has a special characteristics that I'm unable to recognize. My attempt: The solution is of the form $$\begin{aligned} y (t) &= \big ( a \cos (2t) + b \sin (2t) \big ) + t(ct+d) \cos (2t) + t(gt+h) \sin (2t)\\ &= a \cos (2t) + b \sin (2t) + dt \cos (2t) +ht \sin (2t) + ct^2 \cos (2t) + gt^2 \sin (2t) \end{aligned}$$ where $a,b,c,d,g,h \in \mathbb R$ . It follows that $$\begin{aligned} y' (t) &= (2b+d) \cos (2t) + (h-2a) \sin (2t)\\        & \qquad + 2(h+c) t \cos (2t) +2(g-d) t \sin (2t)\\        & \qquad + 2g t^2 \cos (2t)  -2c t^2 \sin (2t) \end{aligned}$$ and $$\begin{aligned} y'' (t) &= 2(2h-2a+c) \cos (2t) -2 (2b+2d-g) \sin (2t)\\        & \qquad + 4(2g-d) t \cos (2t) -4(h+2c) t \sin (2t)\\        & \qquad -4c t^2 \cos (2t)  -4g t^2 \sin (2t) \end{aligned}$$ Hence $$\begin{cases} 2(2h-2a+c) + 4a &=0 \\ -2 (2b+2d-g) +4b &=0 \\ 4(2g-d) + 4d &=0 \\ -4(h+2c) + 4h  &=0 \\ -4c +4c &=0 \\ -4g+4g  &=0 \\ \end{cases} \iff \begin{cases} c &=0 \\ g &=1/4 \\ h &=0 \\ d  &=1/8 \\ \end{cases}$$ In conclusion, the solution is $$y(t) = a \cos (2t) + b \sin (2t) + \frac{1}{8} t \cos (2t)   + \frac{1}{4} t^2 \sin (2t)$$","I'm solving this ODE, which is from last year final exam Solve in the ODE This ODE is not hard to solve, but it takes me a lot of time to compute the derivative and simplifies them. This process is unfortunately likely to cause mistake. The last year exam contains a total of questions, and has a duration of hour and minutes. Solving this ODE is just one of questions. I would like to ask for a faster way to solve this ODE. Maybe it has a special characteristics that I'm unable to recognize. My attempt: The solution is of the form where . It follows that and Hence In conclusion, the solution is","\mathbb R y''(t) + 4 y (t) = 2t \cos (2t) y',y'' 4 1 30 4 \begin{aligned} y (t) &= \big ( a \cos (2t) + b \sin (2t) \big ) + t(ct+d) \cos (2t) + t(gt+h) \sin (2t)\\
&= a \cos (2t) + b \sin (2t) + dt \cos (2t) +ht \sin (2t) + ct^2 \cos (2t) + gt^2 \sin (2t) \end{aligned} a,b,c,d,g,h \in \mathbb R \begin{aligned}
y' (t) &= (2b+d) \cos (2t) + (h-2a) \sin (2t)\\
       & \qquad + 2(h+c) t \cos (2t) +2(g-d) t \sin (2t)\\
       & \qquad + 2g t^2 \cos (2t)  -2c t^2 \sin (2t)
\end{aligned} \begin{aligned}
y'' (t) &= 2(2h-2a+c) \cos (2t) -2 (2b+2d-g) \sin (2t)\\
       & \qquad + 4(2g-d) t \cos (2t) -4(h+2c) t \sin (2t)\\
       & \qquad -4c t^2 \cos (2t)  -4g t^2 \sin (2t)
\end{aligned} \begin{cases}
2(2h-2a+c) + 4a &=0 \\
-2 (2b+2d-g) +4b &=0 \\
4(2g-d) + 4d &=0 \\
-4(h+2c) + 4h  &=0 \\
-4c +4c &=0 \\
-4g+4g  &=0 \\
\end{cases} \iff
\begin{cases}
c &=0 \\
g &=1/4 \\
h &=0 \\
d  &=1/8 \\
\end{cases} y(t) = a \cos (2t) + b \sin (2t) + \frac{1}{8} t \cos (2t)   + \frac{1}{4} t^2 \sin (2t)","['ordinary-differential-equations', 'alternative-proof']"
69,How to find the third eigenvalue/eigenvector pair?,How to find the third eigenvalue/eigenvector pair?,,"We are given a $3 \times 3$ real matrix $A$ , and we know it has three eigenvalues. One eigenvalue is $\lambda_1=-1$ with corresponding eigenvector $v_1=\left[\begin{matrix}     0 \\     1 \\     0 \\    \end{matrix}\right]$ and another eigenvalue $\lambda_2=1+i$ and corresponding eigenvector $v_2=\left[\begin{matrix}     1 \\     2 \\     i \\    \end{matrix}\right]$ . Given this, how can we find the third eigenvalue/eigenvector pair $(\lambda_3, v_3)$ ? The point is ultimately to be able to find the general solution to the linear DE system $x'=Ax$ . The context is that this problem came up in a qualifying exam. My linear algebra is incredibly rusty so I imagine there's just some eigenvalue/eigenvector related trick I'm not seeing. Now, considering the characteristic polynomial, it should be clear that the third eigenvalue is $\lambda_3 = 1-i$ . What's not clear to me is determining the corresponding eigenvector. Clearly, it must be linearly independent from the other two, but how can we use the given eigenvectors to deduce the third one?","We are given a real matrix , and we know it has three eigenvalues. One eigenvalue is with corresponding eigenvector and another eigenvalue and corresponding eigenvector . Given this, how can we find the third eigenvalue/eigenvector pair ? The point is ultimately to be able to find the general solution to the linear DE system . The context is that this problem came up in a qualifying exam. My linear algebra is incredibly rusty so I imagine there's just some eigenvalue/eigenvector related trick I'm not seeing. Now, considering the characteristic polynomial, it should be clear that the third eigenvalue is . What's not clear to me is determining the corresponding eigenvector. Clearly, it must be linearly independent from the other two, but how can we use the given eigenvectors to deduce the third one?","3 \times 3 A \lambda_1=-1 v_1=\left[\begin{matrix}
    0 \\
    1 \\
    0 \\
   \end{matrix}\right] \lambda_2=1+i v_2=\left[\begin{matrix}
    1 \\
    2 \\
    i \\
   \end{matrix}\right] (\lambda_3, v_3) x'=Ax \lambda_3 = 1-i","['linear-algebra', 'ordinary-differential-equations']"
70,Find the inverse Laplace transform of $X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3}$ with $\Re(s)>-1$,Find the inverse Laplace transform of  with,X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3} \Re(s)>-1,Find the inverse Laplace transform of $$X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3} \qquad \Re(s)>-1$$ I never learned how to use this in class and so I've seen a couple youtube videos however they are too easy and not as complex as this one. I am aware that this is a step function so therefore it will involve this property: $$e^{-cs}F(s)= f(t-c)u(t-c)$$ So far I factored the denominator and separated the numerator and have: $$\frac{2}{(s+1)(s+3)} + \frac{2se^{-2s}}{(s+1)(s+3)} + e^{4s}\frac{4}{(s+1)(s+3)}$$ Anything like references to problems similar as this one would help me.,Find the inverse Laplace transform of I never learned how to use this in class and so I've seen a couple youtube videos however they are too easy and not as complex as this one. I am aware that this is a step function so therefore it will involve this property: So far I factored the denominator and separated the numerator and have: Anything like references to problems similar as this one would help me.,X(S)= \frac{2+2s e^{-2s}+4e^{-4s}}{s^2+4s+3} \qquad \Re(s)>-1 e^{-cs}F(s)= f(t-c)u(t-c) \frac{2}{(s+1)(s+3)} + \frac{2se^{-2s}}{(s+1)(s+3)} + e^{4s}\frac{4}{(s+1)(s+3)},"['ordinary-differential-equations', 'laplace-transform']"
71,Giving bounds to the principal solution matrix,Giving bounds to the principal solution matrix,,"I've been trying to solve a problem from ODE and Dynamical System of G.Teschl and I got stuck in some linear algebra problem(linear algbra more than ODE I guess). Problem For any matrix A (so it can be constant or depending on $t$ ), the matrix $Re(A) = \frac{1}{2}(A + A^{*})$ is symmetric and hence has only real eigenvalues. Let $\alpha_{0}$ be its largest eigenvalue. Let $A(t)$ be given and and define $\alpha_{0}(t)$ as above, prove that $||\Pi(t,t_{0}) || \leq e^{\int_{t_{0}}^{t} \alpha_{0}(s)dx}$ for $t>t_{0}.$ My Problem I know I can consider $\frac{d}{dt}|x(t)^{2}| = 2 <x(t),\dot x(t)> = 2<x(t),Ax(t)>$ . After this, I cant find the way to relate $Re(A)$ with $A$ in order to bound my inner product(this is why I think my problem is with linear algebra more than ODE). After this I know I can use the fact that $<x,Re(A)x> \leq \alpha_{0}|x^{2}(t)|$ and then apply some gronewall's inequality. Thanks so much for your answers!","I've been trying to solve a problem from ODE and Dynamical System of G.Teschl and I got stuck in some linear algebra problem(linear algbra more than ODE I guess). Problem For any matrix A (so it can be constant or depending on ), the matrix is symmetric and hence has only real eigenvalues. Let be its largest eigenvalue. Let be given and and define as above, prove that for My Problem I know I can consider . After this, I cant find the way to relate with in order to bound my inner product(this is why I think my problem is with linear algebra more than ODE). After this I know I can use the fact that and then apply some gronewall's inequality. Thanks so much for your answers!","t Re(A) = \frac{1}{2}(A + A^{*}) \alpha_{0} A(t) \alpha_{0}(t) ||\Pi(t,t_{0}) || \leq e^{\int_{t_{0}}^{t} \alpha_{0}(s)dx} t>t_{0}. \frac{d}{dt}|x(t)^{2}| = 2 <x(t),\dot x(t)> = 2<x(t),Ax(t)> Re(A) A <x,Re(A)x> \leq \alpha_{0}|x^{2}(t)|","['calculus', 'linear-algebra', 'ordinary-differential-equations']"
72,When to include the constant of integration when finding the integrating factor,When to include the constant of integration when finding the integrating factor,,"I'm currently studying ODE's with Advanced Engineering Mathematics 10e (Kreyszig) and had a question regarding the constant of integration when finding the integrating factor. I'm currently solving an exercise problem in the section where they explain using the integrating factor to solve nonhomogeneous linear ODEs. More specifically: Solve: $$y' - 2y - x = 0$$ My approach is as follows: Since $y' - 2y = x$ , we can first find the integrating factor by: $$ Fy' - 2Fy = xF$$ where $-2F = F'$ . From here it follows that: $$ \begin{align} \frac{F'}{F} & = -2 \\ \left( \ln(F) \right)' & = -2 \\ \ln(F) & = -2x + C \\ F & = e^{-2x + C} \end{align} $$ Plugging this back into the equation above: $$ \begin{align} e^{-2x + C}y' -2e^{-2x + C}y & = xe^{-2x + C} \\ \left( e^{-2x + C}y \right)' & = xe^{-2x + C} \\ e^{-2x + C} y & = -\frac{x}{2}e^{-2x + C_1}+C_2 \end{align} $$ And this is where I get stuck. The reason why I'm confused is because I'm not sure how to deal with the constants of integration that I've introduced into the entire process after multiple integration operations. Is my overall approach correct? And if so, how might I go about dealing with the constants of integration? Thanks.","I'm currently studying ODE's with Advanced Engineering Mathematics 10e (Kreyszig) and had a question regarding the constant of integration when finding the integrating factor. I'm currently solving an exercise problem in the section where they explain using the integrating factor to solve nonhomogeneous linear ODEs. More specifically: Solve: My approach is as follows: Since , we can first find the integrating factor by: where . From here it follows that: Plugging this back into the equation above: And this is where I get stuck. The reason why I'm confused is because I'm not sure how to deal with the constants of integration that I've introduced into the entire process after multiple integration operations. Is my overall approach correct? And if so, how might I go about dealing with the constants of integration? Thanks.","y' - 2y - x = 0 y' - 2y = x 
Fy' - 2Fy = xF -2F = F' 
\begin{align}
\frac{F'}{F} & = -2 \\
\left( \ln(F) \right)' & = -2 \\
\ln(F) & = -2x + C \\
F & = e^{-2x + C}
\end{align}
 
\begin{align}
e^{-2x + C}y' -2e^{-2x + C}y & = xe^{-2x + C} \\
\left( e^{-2x + C}y \right)' & = xe^{-2x + C} \\
e^{-2x + C} y & = -\frac{x}{2}e^{-2x + C_1}+C_2
\end{align}
","['integration', 'ordinary-differential-equations']"
73,Hypergeometric function at $z=1$,Hypergeometric function at,z=1,"There is a nice formula for the value of the hypergeometric function ${}_2 F_1(a,b,c,z)$ at $z=1$ when $\Re{(c)}>\Re(b+a)$ given for example at https://en.wikipedia.org/wiki/Hypergeometric_function Is there some formula for what happens when $\Re{(c)}\leq\Re(b+a)$ . Presumably, the  function diverges but is there a known asymptotic behavior as $z\rightarrow 1^-$ ?","There is a nice formula for the value of the hypergeometric function at when given for example at https://en.wikipedia.org/wiki/Hypergeometric_function Is there some formula for what happens when . Presumably, the  function diverges but is there a known asymptotic behavior as ?","{}_2 F_1(a,b,c,z) z=1 \Re{(c)}>\Re(b+a) \Re{(c)}\leq\Re(b+a) z\rightarrow 1^-","['real-analysis', 'ordinary-differential-equations', 'asymptotics', 'special-functions', 'hypergeometric-function']"
74,Find a differential equation whose solutions are the circles $x^2+y^2=2ay$,Find a differential equation whose solutions are the circles,x^2+y^2=2ay,"I had to find differential equation of the family of circles which touch the $x$ -axis at origin. Centering the variable circle at $(0,a)$ , its equation will be $$x^2+y^2=2ay .$$ Solving conventionally by differentiating once and substituting the value of a in the equation of circle, the differential equation is $$(x^2-y^2)y_1=2xy$$ Now I might be missing some concept because I've just started with ODE, but what is wrong with the following process? Differentiating the circle equation $$x+yy_1=ay_1 ,$$ dividing both sides by $y_1$ , and differentiating both sides gives the equation $$y_1-xy_2+{y_1}^3=0 .$$ Does this not represent the family of circles? In that case the differential equations are of different orders.","I had to find differential equation of the family of circles which touch the -axis at origin. Centering the variable circle at , its equation will be Solving conventionally by differentiating once and substituting the value of a in the equation of circle, the differential equation is Now I might be missing some concept because I've just started with ODE, but what is wrong with the following process? Differentiating the circle equation dividing both sides by , and differentiating both sides gives the equation Does this not represent the family of circles? In that case the differential equations are of different orders.","x (0,a) x^2+y^2=2ay . (x^2-y^2)y_1=2xy x+yy_1=ay_1 , y_1 y_1-xy_2+{y_1}^3=0 .","['ordinary-differential-equations', 'circles']"
75,How can I Solve $y^{\prime\prime}+\frac2{x}y^{\prime}+y^3=0$?,How can I Solve ?,y^{\prime\prime}+\frac2{x}y^{\prime}+y^3=0,The equation below is a non-linear second order differential equation. $$y^{\prime\prime}+\dfrac{2}{x}y^{\prime}+y^3=0$$ I don't know how can I solve it? Would somebody help me?,The equation below is a non-linear second order differential equation. I don't know how can I solve it? Would somebody help me?,y^{\prime\prime}+\dfrac{2}{x}y^{\prime}+y^3=0,['ordinary-differential-equations']
76,How to Show the Eigenvalues of a Sturm-Liouville Equation are Real Given No Boundary Condition [closed],How to Show the Eigenvalues of a Sturm-Liouville Equation are Real Given No Boundary Condition [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How can I show that the eigenvalues of $\Psi_{xx}+u(x) \Psi = \lambda \Psi$ are real without a boundary condition?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How can I show that the eigenvalues of are real without a boundary condition?",\Psi_{xx}+u(x) \Psi = \lambda \Psi,"['real-analysis', 'ordinary-differential-equations', 'spectral-theory']"
77,Stability of the solution $\mathbf x=0$ of the DE $\ddot{\mathbf x}=A \mathbf x$,Stability of the solution  of the DE,\mathbf x=0 \ddot{\mathbf x}=A \mathbf x,"Differential equations of the form $\ddot{\mathbf x}=A \mathbf x$ is quite common in physics. Here I want to analyze the stability of the solution $\mathbf x=0$ . Let $\mathbf x_1,\mathbf x_2$ be two solutions of the DE, such that $\mathbf x_1(0)=\mathbf 0$ . The equilibrium $\mathbf x=0$ is stable iff $\forall \epsilon\forall t_0\forall\mathbf x_2,\exists\delta,|\mathbf x_2(0)|< \delta\Rightarrow |\mathbf x_1(t)-\mathbf x_2(t)|< \epsilon$ for all $t>t_0$ . My question is: what are the necessary and sufficient conditions on $A$ for the solution $\mathbf x=0$ to be stable? If we think of $\ddot{\mathbf x}$ as the acceleration due to a force, then there exist a positive constant $k$ such that the work done by the force of a virtual displacement $\delta \mathbf x$ from $\mathbf 0$ is $$ k(A\mathbf x).\mathbf x=k\mathbf x^TA\mathbf x<0. $$ So if $A$ is symmetric, then $A$ is positive definite, so all eigenvectors must be real and negative. But what about the case that $A$ is not symmetric? what are the necessary and sufficient conditions on the eigenvalues of $A$ for the solution $\mathbf x=0$ to be stable? What about the equation $\dot{\mathbf x}=B\mathbf x$ ? This can be written as $\ddot{\mathbf x}=B\dot{\mathbf x}=B^2\mathbf x$ . So we can just apply the results for two-dot equation here. Again, if $B$ is symmetric, then so is $B^2$ , so all eignvalues of $B^2$ must be negative? Is that right? I know the case $\mathbf x\in\mathbb R^2$ quite well (many DE books only discuss the case $n=2$ ). But are there generalisiation to higher dimensions ( $\mathbf x\in \mathbb R^n$ )? The case of $\lambda=0$ is quite tricky. Please discuss them in detail.","Differential equations of the form is quite common in physics. Here I want to analyze the stability of the solution . Let be two solutions of the DE, such that . The equilibrium is stable iff for all . My question is: what are the necessary and sufficient conditions on for the solution to be stable? If we think of as the acceleration due to a force, then there exist a positive constant such that the work done by the force of a virtual displacement from is So if is symmetric, then is positive definite, so all eigenvectors must be real and negative. But what about the case that is not symmetric? what are the necessary and sufficient conditions on the eigenvalues of for the solution to be stable? What about the equation ? This can be written as . So we can just apply the results for two-dot equation here. Again, if is symmetric, then so is , so all eignvalues of must be negative? Is that right? I know the case quite well (many DE books only discuss the case ). But are there generalisiation to higher dimensions ( )? The case of is quite tricky. Please discuss them in detail.","\ddot{\mathbf x}=A \mathbf x \mathbf x=0 \mathbf x_1,\mathbf x_2 \mathbf x_1(0)=\mathbf 0 \mathbf x=0 \forall \epsilon\forall t_0\forall\mathbf x_2,\exists\delta,|\mathbf x_2(0)|< \delta\Rightarrow |\mathbf x_1(t)-\mathbf x_2(t)|< \epsilon t>t_0 A \mathbf x=0 \ddot{\mathbf x} k \delta \mathbf x \mathbf 0 
k(A\mathbf x).\mathbf x=k\mathbf x^TA\mathbf x<0.
 A A A A \mathbf x=0 \dot{\mathbf x}=B\mathbf x \ddot{\mathbf x}=B\dot{\mathbf x}=B^2\mathbf x B B^2 B^2 \mathbf x\in\mathbb R^2 n=2 \mathbf x\in \mathbb R^n \lambda=0","['linear-algebra', 'ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
78,How to show that a differential equation has a periodic orbit?,How to show that a differential equation has a periodic orbit?,,"I've been learning differential equations, but I can't solve this exercise: Show that the following equation has a periodic orbit: $$u''-(1-u^2)u'+u^5 = 0$$ Can anyone show me how it is done? I've tried to write it as a system \begin{array}{lcl} x' & = & y \\ y' & = & (1-x^2)y-x^5 \end{array} but every step i take past this point leads me to a dead end.","I've been learning differential equations, but I can't solve this exercise: Show that the following equation has a periodic orbit: Can anyone show me how it is done? I've tried to write it as a system but every step i take past this point leads me to a dead end.",u''-(1-u^2)u'+u^5 = 0 \begin{array}{lcl} x' & = & y \\ y' & = & (1-x^2)y-x^5 \end{array},"['ordinary-differential-equations', 'dynamical-systems']"
79,Using a Lyapunov function to classify stability and sketching a phase portrait,Using a Lyapunov function to classify stability and sketching a phase portrait,,"Consider the system $$x' = -x^3-xy^{2k}$$ $$y' = -y^3-x^{2k}y$$ Where $k$ is a given positive integer. a.) Find and classify according to stability the equilibrium solutions. $\it{Hint:}$ Let $V(x,y) = x^2 + y^2$ b.) Sketch a phase portrait when $k = 1$ $\it{Hint:}$ What are $x'$ and $y'$ when $y=ax$ for some real number $a$ ? a.) Using $V$ , we get $\frac{d}{dt}V=2xx'+2yy'$ Plugging in our system , we get: $$\frac{d}{dt}V=2x(-x^3-xy^{2k})+2y(-y^3-x^{2k}y)$$ $$=-(x^4+y^4)-x^2y^{2k}-x^{2k}y^2<0$$ I dropped the $2$ since it doesn't matter to determine stability. We see that our own equilibrium is $(0,0)$ since setting $x'=0$ we get $$y^{2k}=-x^2$$ Which only works for $x=y=0$ Therefore our system is asymptotically stable at the origin. I am having trouble with b.), mostly because the hint is confusing me. Let $y=ax$ , then our system becomes $$x'=-x^3-a^2x^3=-x^3(1+a^2)$$ $$y'=-a^3x^3-ax^3=-ax^3(1+a^2)$$ I am not sure what to do with this. Using linearization doesn't work since the Jacobian will be the zero vector at the point of interest. I have never had a problem that asks to draw a phase portrait when linearization doesn't work, so I am hoping someone more clever than me can offer some advice.","Consider the system Where is a given positive integer. a.) Find and classify according to stability the equilibrium solutions. Let b.) Sketch a phase portrait when What are and when for some real number ? a.) Using , we get Plugging in our system , we get: I dropped the since it doesn't matter to determine stability. We see that our own equilibrium is since setting we get Which only works for Therefore our system is asymptotically stable at the origin. I am having trouble with b.), mostly because the hint is confusing me. Let , then our system becomes I am not sure what to do with this. Using linearization doesn't work since the Jacobian will be the zero vector at the point of interest. I have never had a problem that asks to draw a phase portrait when linearization doesn't work, so I am hoping someone more clever than me can offer some advice.","x' = -x^3-xy^{2k} y' = -y^3-x^{2k}y k \it{Hint:} V(x,y) = x^2 + y^2 k = 1 \it{Hint:} x' y' y=ax a V \frac{d}{dt}V=2xx'+2yy' \frac{d}{dt}V=2x(-x^3-xy^{2k})+2y(-y^3-x^{2k}y) =-(x^4+y^4)-x^2y^{2k}-x^{2k}y^2<0 2 (0,0) x'=0 y^{2k}=-x^2 x=y=0 y=ax x'=-x^3-a^2x^3=-x^3(1+a^2) y'=-a^3x^3-ax^3=-ax^3(1+a^2)","['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
80,ODE in $\mathbb{R}^n$ defined by the gradient of a function,ODE in  defined by the gradient of a function,\mathbb{R}^n,"I'm studying for an exam and I got stuck in this question: Let $x: I \to \mathbb{R}^n$ be a differentiable parametrized curve (I is an interval) in $\mathbb{R}^n$ and $f: \mathbb{R}^n \to \mathbb{R}$ be a differentiable function such that $\frac{dx}{dt}(t) = - \nabla f(x(t))$ . Show that either $x$ is a constant function or $f \circ x$ is strictly decreasing. I tried differentiating $f \circ x$ and arrived to the result that $\frac{d (f \circ x)}{dt} = - \Vert \nabla f(x(t)) \Vert ^2 \leq 0$ , which proves that $f \circ x$ is monotonically decreasing. I concluded that to show the desired result, it must be the case that if $\nabla f(x(c)) = 0$ for some $c \in I$ , then $\nabla f(x(t)) = 0$ for all $t \in I$ , but I can't understand or prove it. Can anyone shed some light?","I'm studying for an exam and I got stuck in this question: Let be a differentiable parametrized curve (I is an interval) in and be a differentiable function such that . Show that either is a constant function or is strictly decreasing. I tried differentiating and arrived to the result that , which proves that is monotonically decreasing. I concluded that to show the desired result, it must be the case that if for some , then for all , but I can't understand or prove it. Can anyone shed some light?",x: I \to \mathbb{R}^n \mathbb{R}^n f: \mathbb{R}^n \to \mathbb{R} \frac{dx}{dt}(t) = - \nabla f(x(t)) x f \circ x f \circ x \frac{d (f \circ x)}{dt} = - \Vert \nabla f(x(t)) \Vert ^2 \leq 0 f \circ x \nabla f(x(c)) = 0 c \in I \nabla f(x(t)) = 0 t \in I,"['ordinary-differential-equations', 'multivariable-calculus']"
81,Requirement of Lyapunov Stability in Asymptotic Stability,Requirement of Lyapunov Stability in Asymptotic Stability,,"In my Differential Equations course, we defined the equilibrium point $x_0$ of a dynamical system $\frac{dx}{dt} = f(x(t))$ (for $f$ defined on an open subset of $\mathbb R^n$ , say $\mathbb R^n$ itself) to be stable if it is: Lyapunov Stable There is an $\epsilon$ ball around $x_0$ such that the solutions $\varphi$ of this differential equation with initial conditions in this ball satisfy $\lim_{t \to \infty} \varphi(t) = x_0$ . I am trying to find an example of the case where the property (2) holds while the point $x_0$ is not Lyapunov stable. After some searching, I ran across Homoclinic Bifurcation , which is intuitively how I would expect Lyapunov Stability to fail, but have been unable to find examples of Homoclinic Bifurcation where property (2) holds as well. Any help would be appreciated.","In my Differential Equations course, we defined the equilibrium point of a dynamical system (for defined on an open subset of , say itself) to be stable if it is: Lyapunov Stable There is an ball around such that the solutions of this differential equation with initial conditions in this ball satisfy . I am trying to find an example of the case where the property (2) holds while the point is not Lyapunov stable. After some searching, I ran across Homoclinic Bifurcation , which is intuitively how I would expect Lyapunov Stability to fail, but have been unable to find examples of Homoclinic Bifurcation where property (2) holds as well. Any help would be appreciated.",x_0 \frac{dx}{dt} = f(x(t)) f \mathbb R^n \mathbb R^n \epsilon x_0 \varphi \lim_{t \to \infty} \varphi(t) = x_0 x_0,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'bifurcation']"
82,How to solve this first order differential separable equation that has an $xy^2$ term?,How to solve this first order differential separable equation that has an  term?,xy^2,"Solve: $$(1+x+xy^2)dy+(y+y^3)dx=0$$ I know it's separable, I've tried transforming it with a variable $v = y/x$ , but I can't get it to separate for integration. Help much appreciated.","Solve: I know it's separable, I've tried transforming it with a variable , but I can't get it to separate for integration. Help much appreciated.",(1+x+xy^2)dy+(y+y^3)dx=0 v = y/x,['ordinary-differential-equations']
83,Help solving first-order differential equation [duplicate],Help solving first-order differential equation [duplicate],,"This question already has answers here : Differential equation $y=xy' + \frac{1}{2}(y')^2$ (4 answers) How to solve differential equation $y=xy'+\frac12y'^2$ [closed] (4 answers) Closed 5 years ago . I have first-order differential equation $$y=xy'+ \frac{1}{2}(y')^{2}$$ Maybe, with this someone will find way to solve it $$\frac{1}{2}y'(2x+y')=y$$ I thought I can use $x^2+y=t$ for subtitution and when I derivate, I have $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ which is acctualy the same as previous. I don't have idea how to start..","This question already has answers here : Differential equation $y=xy' + \frac{1}{2}(y')^2$ (4 answers) How to solve differential equation $y=xy'+\frac12y'^2$ [closed] (4 answers) Closed 5 years ago . I have first-order differential equation Maybe, with this someone will find way to solve it I thought I can use for subtitution and when I derivate, I have which is acctualy the same as previous. I don't have idea how to start..",y=xy'+ \frac{1}{2}(y')^{2} \frac{1}{2}y'(2x+y')=y x^2+y=t t'=2x+y'\\(t'-2x)t'=2t-2x^2,['ordinary-differential-equations']
84,Is the Lorenz system well-posed in the Hadamard sense?,Is the Lorenz system well-posed in the Hadamard sense?,,"Apologies if this has already been discussed, but I searched the site and I couldn't find an answer. For the sake of simplicity, consider only ODEs, possibly depending on some vector of parameters $\mathbf{p}$ . The definition of a well-posed problem in the Hadamard sense is: https://en.wikipedia.org/wiki/Well-posed_problem The third point says that ""the solution's behavior changes continuously with the initial conditions"". I'm not sure what's the rigorous meaning of this. I interpret it as ""the integral operator which associates the forcing term, the parameters and the initial condition(s) to the solution of the ODE is continuous"", which seems to agree with the usual interpretation that ""small changes in the data lead to small changes in the solution"". However , the Lorenz system is usually considered ill-posed: see https://ac.els-cdn.com/089812219400188X/1-s2.0-089812219400188X-main.pdf?_tid=27b719d7-543e-4290-bfe6-e9d62887ba6c&acdnat=1549797064_ba8fd399224e0c3170aef5cc8ba4a182 Doesn't the solution depend continuously on data, though? Sure, changes $\epsilon$ in the initial conditions or in $\mathbf{p}$ will cause changes of order $\epsilon\exp(\lambda t)$ in time $t$ , which may be exponentially large, but they still depend continuously on $\epsilon$ , don't they? So, why would it be ill-posed? Also more recent paper seem to consider the Lorenz system ill-posed, though admittedly it's not stated explicitly: https://pdfs.semanticscholar.org/85e5/908a9e21fb1c017c4206cf4df4475c89bdd8.pdf","Apologies if this has already been discussed, but I searched the site and I couldn't find an answer. For the sake of simplicity, consider only ODEs, possibly depending on some vector of parameters . The definition of a well-posed problem in the Hadamard sense is: https://en.wikipedia.org/wiki/Well-posed_problem The third point says that ""the solution's behavior changes continuously with the initial conditions"". I'm not sure what's the rigorous meaning of this. I interpret it as ""the integral operator which associates the forcing term, the parameters and the initial condition(s) to the solution of the ODE is continuous"", which seems to agree with the usual interpretation that ""small changes in the data lead to small changes in the solution"". However , the Lorenz system is usually considered ill-posed: see https://ac.els-cdn.com/089812219400188X/1-s2.0-089812219400188X-main.pdf?_tid=27b719d7-543e-4290-bfe6-e9d62887ba6c&acdnat=1549797064_ba8fd399224e0c3170aef5cc8ba4a182 Doesn't the solution depend continuously on data, though? Sure, changes in the initial conditions or in will cause changes of order in time , which may be exponentially large, but they still depend continuously on , don't they? So, why would it be ill-posed? Also more recent paper seem to consider the Lorenz system ill-posed, though admittedly it's not stated explicitly: https://pdfs.semanticscholar.org/85e5/908a9e21fb1c017c4206cf4df4475c89bdd8.pdf",\mathbf{p} \epsilon \mathbf{p} \epsilon\exp(\lambda t) t \epsilon,"['ordinary-differential-equations', 'dynamical-systems', 'chaos-theory', 'stability-theory']"
85,"Solution of $y''(x)+y(x)=0$, why is it $y=A\cos(x)+B\sin(x)$?","Solution of , why is it ?",y''(x)+y(x)=0 y=A\cos(x)+B\sin(x),"We want to solve $y''(x)+y(x)=0$ . We are looking for solutions of the form $y(x)=e^{\lambda x}$ . Then, we get $$(\lambda ^2+1)e^{\lambda x}\iff \lambda ^2+1=0\iff \lambda =\pm i.$$ Therefore, the general solution is given by $$y(x)=Ae^{ix}+Be^{-ix}.$$ But why in my solution they say that it's of the form $$y(x)=C\cos(x)+D\sin(x) \ \ ?$$ How can I pass from $Ae^{ix}+Be^{-ix}$ to $C\cos(x)+D\sin(x)$ ? I know that $\cos(x)=\frac{e^{ix}+e^{-ix}}{2}$ and $\sin(x)=\frac{e^{ix}-e^{-x}}{2i}$ . Therefore, I can replace these value in $C\cos(x)+D\sin(x)$ and find $A$ and $B$ s.t. it's equal to $Ae^{ix}+Be^{-ix}$ . But I'm really asking : how do I know that $Ae^{ix}+Be^{-ix}$ can be written in the form of $C\cos(x)+D\sin(x)$ ? Because at my exam I had 0 point on $5$ because I wrote the solution as $Ae^{ix}+Be^{-ix}$ , but it's correct, no ?","We want to solve . We are looking for solutions of the form . Then, we get Therefore, the general solution is given by But why in my solution they say that it's of the form How can I pass from to ? I know that and . Therefore, I can replace these value in and find and s.t. it's equal to . But I'm really asking : how do I know that can be written in the form of ? Because at my exam I had 0 point on because I wrote the solution as , but it's correct, no ?",y''(x)+y(x)=0 y(x)=e^{\lambda x} (\lambda ^2+1)e^{\lambda x}\iff \lambda ^2+1=0\iff \lambda =\pm i. y(x)=Ae^{ix}+Be^{-ix}. y(x)=C\cos(x)+D\sin(x) \ \ ? Ae^{ix}+Be^{-ix} C\cos(x)+D\sin(x) \cos(x)=\frac{e^{ix}+e^{-ix}}{2} \sin(x)=\frac{e^{ix}-e^{-x}}{2i} C\cos(x)+D\sin(x) A B Ae^{ix}+Be^{-ix} Ae^{ix}+Be^{-ix} C\cos(x)+D\sin(x) 5 Ae^{ix}+Be^{-ix},['ordinary-differential-equations']
86,Using Laplace transform to solve differential equation $y'' -4y' = -4te^{2t}$,Using Laplace transform to solve differential equation,y'' -4y' = -4te^{2t},"$y'' -4y' = -4te^{2t}, y(0)=0, y'(0)=1$ If you take laplace Transform of all terms, isolate L(y), I got $$L(y) = \frac{1}{(p-2)^2} + \frac{-2}{(p-2)^2 -4}$$ Then, taking inverse Laplace, you get $$y(t) = te^{2t} - e^{2t}sinh(2t)$$ But the solution is just $te^{2t}$ . WHat am I doing wrong","If you take laplace Transform of all terms, isolate L(y), I got Then, taking inverse Laplace, you get But the solution is just . WHat am I doing wrong","y'' -4y' = -4te^{2t}, y(0)=0, y'(0)=1 L(y) = \frac{1}{(p-2)^2} + \frac{-2}{(p-2)^2 -4} y(t) = te^{2t} - e^{2t}sinh(2t) te^{2t}","['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
87,Periodic solutions of a planar ODE,Periodic solutions of a planar ODE,,"Linearizing the equation of the two body problem at a circular solution I came accross the following planar second order system of differential equations $$ \begin{cases} 2\frac{1}{\omega^2} \ddot{u}=3u\cos2\omega t+3v\sin2\omega t+u\\ 2\frac{1}{\omega^2}\ddot{v}=3u\sin2\omega t-3v\cos2\omega t+v \end{cases} $$ It is easy to see that $(u,v)=(-\sin\omega t,\cos\omega t)$ is a $\frac{2\pi}{\omega}$ -periodic solution. I'm wondering if there are other independent ones.  Do you have any suggestion to find them?",Linearizing the equation of the two body problem at a circular solution I came accross the following planar second order system of differential equations It is easy to see that is a -periodic solution. I'm wondering if there are other independent ones.  Do you have any suggestion to find them?,"
\begin{cases}
2\frac{1}{\omega^2} \ddot{u}=3u\cos2\omega t+3v\sin2\omega t+u\\
2\frac{1}{\omega^2}\ddot{v}=3u\sin2\omega t-3v\cos2\omega t+v
\end{cases}
 (u,v)=(-\sin\omega t,\cos\omega t) \frac{2\pi}{\omega}",['ordinary-differential-equations']
88,Solve differential equation $xyy'=x^4+y^4$,Solve differential equation,xyy'=x^4+y^4,How to find general solution to this differential equation (if it exists): $$ xyy'=x^4+y^4 ?$$ I do not know how to even approach it since I never dealt with nonlinear equations. Only thing that I notice is that $x$ and $y$ are symetric in equation: $$ y'=\dfrac{x^4+y^4}{xy}$$ so I expect (maybe) some kind of symetric function for y(x). Thanks for any help.,How to find general solution to this differential equation (if it exists): I do not know how to even approach it since I never dealt with nonlinear equations. Only thing that I notice is that and are symetric in equation: so I expect (maybe) some kind of symetric function for y(x). Thanks for any help., xyy'=x^4+y^4 ? x y  y'=\dfrac{x^4+y^4}{xy},"['ordinary-differential-equations', 'analysis', 'nonlinear-analysis']"
89,Transformation of a second order ODE,Transformation of a second order ODE,,"I have to transform a second order differential equation into a Bernoulli type differential equation, however, I am having some trouble. The original equation is: \begin{equation} u\frac{d^{2}u}{dt^{2}} - \bigg(\frac{du}{dt}\ \bigg)^{2} +(\gamma -x_{0}\beta u)u\frac{du}{dt}=0 \end{equation} By defining a new function: \begin{equation} \phi = \frac{dt}{du} \end{equation} The original equation has to be transformed into: \begin{equation} \frac{d\phi}{du} + \frac{1}{u}\phi = (y-x_{0}\beta u) \phi^{2}  \end{equation} Would anyone be willing to suggest any ideas for how this might be achieved? Thank you!","I have to transform a second order differential equation into a Bernoulli type differential equation, however, I am having some trouble. The original equation is: By defining a new function: The original equation has to be transformed into: Would anyone be willing to suggest any ideas for how this might be achieved? Thank you!","\begin{equation}
u\frac{d^{2}u}{dt^{2}} - \bigg(\frac{du}{dt}\ \bigg)^{2} +(\gamma -x_{0}\beta u)u\frac{du}{dt}=0
\end{equation} \begin{equation}
\phi = \frac{dt}{du}
\end{equation} \begin{equation}
\frac{d\phi}{du} + \frac{1}{u}\phi = (y-x_{0}\beta u) \phi^{2} 
\end{equation}",['ordinary-differential-equations']
90,How to solve this 4th order ODE with polynomial coefficients?,How to solve this 4th order ODE with polynomial coefficients?,,"Write the general solution    for $ (x^2) y'''' + (3x^2-2x)y''' + (3x^2-4x+2)y'' +(x^2-2x+2)y'   = 0 $ I tried to guess a solution and use the fact that i can decrease the ODE to less power ( to $y'''$ ) by using the Wronskian. I guessed that $ e^{-x} $ is a solution. Is it the way we solve this kind of equations? It's a homework question so i guess (i / you) can solve it. Euler ODE doesn't work here . Can you help me with the solutions i got 4 solutions : ( after solving Euler equation and moving to $ v''' $ . $y'(x) = \{ e^{-x}  ,~  \frac{x^3}{3}e^{-x} ,~ \frac{x^2}{2}e^{-x}   \}$ or any linear combination of those, uniqueness theorem doesn't apply here near $x=0$","Write the general solution    for I tried to guess a solution and use the fact that i can decrease the ODE to less power ( to ) by using the Wronskian. I guessed that is a solution. Is it the way we solve this kind of equations? It's a homework question so i guess (i / you) can solve it. Euler ODE doesn't work here . Can you help me with the solutions i got 4 solutions : ( after solving Euler equation and moving to . or any linear combination of those, uniqueness theorem doesn't apply here near"," (x^2) y'''' + (3x^2-2x)y''' + (3x^2-4x+2)y'' +(x^2-2x+2)y' 
 = 0  y'''  e^{-x}   v'''  y'(x) = \{ e^{-x}  ,~  \frac{x^3}{3}e^{-x} ,~ \frac{x^2}{2}e^{-x}   \} x=0",['ordinary-differential-equations']
91,Proving that a solution to a differential equation is monotonic,Proving that a solution to a differential equation is monotonic,,"The answer that ws given on a previous question of mine, stated that the solution to this DE: $$x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots\tag1$$ Must be monotonic. Is there a way to proof that that is the case, that the function $x(t)$ is monotonic?! Background: I've to find (the average of a function over a particular interval, where $t_1>0$ , $t_2>0$ and $t_2>t_1$ ): $$\frac{1}{t_2-t_1}\int_{t_1}^{t_2}x(t)dt\tag2$$ Where $x(t)$ in equation $(2)$ is the solution to the DE in equation $(1)$ .","The answer that ws given on a previous question of mine, stated that the solution to this DE: Must be monotonic. Is there a way to proof that that is the case, that the function is monotonic?! Background: I've to find (the average of a function over a particular interval, where , and ): Where in equation is the solution to the DE in equation .",x(t)\cdot r+x'(t)\cdot l+a\cdot\ln\left(1+\frac{x(t)}{b}\right)=0\space\Longleftrightarrow\space x(t)=\dots\tag1 x(t) t_1>0 t_2>0 t_2>t_1 \frac{1}{t_2-t_1}\int_{t_1}^{t_2}x(t)dt\tag2 x(t) (2) (1),"['ordinary-differential-equations', 'definite-integrals', 'logarithms', 'mathematical-physics', 'average']"
92,bifurcation classification,bifurcation classification,,"$$\dot x = \mu −\frac{x^2}{2}+\frac{x^4}{4}$$ a)Determine the types of bifurcations of equilibria that may occur, and their location in the $\mu$ − $x$ plane. i tried subbing in values of $µ$ and sketching the $µ$ - $x$ diagram. at the point $\mu=0$ and $µ=\frac{1}{4}$ bifurcations happen and i beleive i have drawn the graph correctly. what i want to know is how do i classify the bifurcations with 3 different bifurcation points. should i treat each one indivdually? or classify the whole thing.","a)Determine the types of bifurcations of equilibria that may occur, and their location in the − plane. i tried subbing in values of and sketching the - diagram. at the point and bifurcations happen and i beleive i have drawn the graph correctly. what i want to know is how do i classify the bifurcations with 3 different bifurcation points. should i treat each one indivdually? or classify the whole thing.",\dot x = \mu −\frac{x^2}{2}+\frac{x^4}{4} \mu x µ µ x \mu=0 µ=\frac{1}{4},"['ordinary-differential-equations', 'bifurcation']"
93,Proving that limit of a solution approaches equilibrium,Proving that limit of a solution approaches equilibrium,,"I have been given the following system of equations: $$\begin{cases} x'=y-2x^2\\ y'=x(3-y-x^2)\end{cases}$$ The equilibrium points of this system are $(0,0), (1,2), (-1,2)$ . The system has nullclines $y=2x^2, x=0, x=3-x^2$ . After determining that the equilibrium solution $(-1,2)$ is stable and drawing the phase portrait. I've been given the following question: Let $(x_0, y_0)\in \mathcal{S}_1$ ( $\mathcal{S}_1$ denotes the area $0<x<\sqrt{3}$ , $0<y<2x^2, 3-x^2$ . In this area, $x'<0$ and $y'>0$ .) and let $(x_o(t), y_0(t))$ be a solution for the system, which starts in $(x_0, y_0)$ . Show that: $$\lim\limits_{t\to\infty}(x_0(t), y_0(t))=(1,2)$$ After drawing that phase portrait and determining the sign of $x'$ and $y'$ , I understand that this is very likely. However, I do not know how to prove this explicitly. I hope someone can give me an idea as to how to prove this. Thanks is advance","I have been given the following system of equations: The equilibrium points of this system are . The system has nullclines . After determining that the equilibrium solution is stable and drawing the phase portrait. I've been given the following question: Let ( denotes the area , . In this area, and .) and let be a solution for the system, which starts in . Show that: After drawing that phase portrait and determining the sign of and , I understand that this is very likely. However, I do not know how to prove this explicitly. I hope someone can give me an idea as to how to prove this. Thanks is advance","\begin{cases}
x'=y-2x^2\\ y'=x(3-y-x^2)\end{cases} (0,0), (1,2), (-1,2) y=2x^2, x=0, x=3-x^2 (-1,2) (x_0, y_0)\in \mathcal{S}_1 \mathcal{S}_1 0<x<\sqrt{3} 0<y<2x^2, 3-x^2 x'<0 y'>0 (x_o(t), y_0(t)) (x_0, y_0) \lim\limits_{t\to\infty}(x_0(t), y_0(t))=(1,2) x' y'","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems']"
94,general solution of $\frac{dy}{dx}+P(x)y=0$,general solution of,\frac{dy}{dx}+P(x)y=0,"Consider the homogeneous first-order linear differential equation(DE), $$\frac{dy}{dx}+P(x)y=0,$$ where $P(x)$ is continuous on an interval $I=(a,b)$ . I'm trying to convince myself that its general solution on $I$ is $ce^{-\int P(x)dx}.$ My arguments are given below.  I'll appreciate it if someone can confirm or refute it, or provide a simpler/better proof. First of all, let's divide the solutions of the DE into two types.  Type 1: $y(x)\ne 0$ for any $x\in I.$ And we call the rest of the solutions Type 2. For Type 1 solutions , $y\ne 0$ , so the DE is equivalent to: $$\frac{1}{y}\frac{dy}{dx}=-P(x),$$ which can be solved by $$\int \frac{1}{y}dy=-\int P(x)dx.$$ So the general Type 1 solution is $ce^{-\int P(x)dx}, c\ne 0.$ (Note that indeed $y\ne 0$ for any $x\in I.$ ) Regarding Type 2 solutions , first note that the constant solution $y=0$ is clearly a Type 2 solution.  Furthermore, we show it's the only Type 2 solution, by an argument similar to the one in why can't a non-constant solution of an autonomous DE intersect an equilibrium solution? : Suppose $y(x)$ is a Type 2 solution with $y(\alpha)=0$ for some $\alpha\in I.$ Then by the existence and uniqueness theorem, $y(x)=0$ on $(\alpha-h, \alpha+h)\subset I$ for some $h>0.$ Let $\beta=\sup\{t: t\le b, y(t)=0, x\in (\alpha-h, t)\}$ .  Now suppose $\beta<b$ .  Then $y(\beta) \ne 0$ , by definition and the existence and uniqueness theorem.  But this means $y$ is discontinuous at $\beta$ , contradicting the fact that $y$ has to be continuous. So we must have $\beta=b,$ and $y(x)=0$ for $x\in (\alpha-h, b)$ .  By the same token, we can show $y(x)=0$ for $x\in (a, \alpha+h)$ too. Is this argument correct?  Is there a simpler proof?  Thanks a lot!","Consider the homogeneous first-order linear differential equation(DE), where is continuous on an interval . I'm trying to convince myself that its general solution on is My arguments are given below.  I'll appreciate it if someone can confirm or refute it, or provide a simpler/better proof. First of all, let's divide the solutions of the DE into two types.  Type 1: for any And we call the rest of the solutions Type 2. For Type 1 solutions , , so the DE is equivalent to: which can be solved by So the general Type 1 solution is (Note that indeed for any ) Regarding Type 2 solutions , first note that the constant solution is clearly a Type 2 solution.  Furthermore, we show it's the only Type 2 solution, by an argument similar to the one in why can't a non-constant solution of an autonomous DE intersect an equilibrium solution? : Suppose is a Type 2 solution with for some Then by the existence and uniqueness theorem, on for some Let .  Now suppose .  Then , by definition and the existence and uniqueness theorem.  But this means is discontinuous at , contradicting the fact that has to be continuous. So we must have and for .  By the same token, we can show for too. Is this argument correct?  Is there a simpler proof?  Thanks a lot!","\frac{dy}{dx}+P(x)y=0, P(x) I=(a,b) I ce^{-\int P(x)dx}. y(x)\ne 0 x\in I. y\ne 0 \frac{1}{y}\frac{dy}{dx}=-P(x), \int \frac{1}{y}dy=-\int P(x)dx. ce^{-\int P(x)dx}, c\ne 0. y\ne 0 x\in I. y=0 y(x) y(\alpha)=0 \alpha\in I. y(x)=0 (\alpha-h, \alpha+h)\subset I h>0. \beta=\sup\{t: t\le b, y(t)=0, x\in (\alpha-h, t)\} \beta<b y(\beta) \ne 0 y \beta y \beta=b, y(x)=0 x\in (\alpha-h, b) y(x)=0 x\in (a, \alpha+h)","['ordinary-differential-equations', 'proof-verification']"
95,Separation of variables for nonhomogeneous PDE,Separation of variables for nonhomogeneous PDE,,"I need to solve the equation below using separation of variables. $$\frac{\partial f(x,y)}{\partial x} - \frac{\partial f(x,y)}{\partial y} = 2$$ The thing is, i've always done with $0$ after the equal sign. I'm really stuck with that $2$ ; when doing the separation I get $X'Y-XY'=2$ and can't separate X and Y after that.","I need to solve the equation below using separation of variables. The thing is, i've always done with after the equal sign. I'm really stuck with that ; when doing the separation I get and can't separate X and Y after that.","\frac{\partial f(x,y)}{\partial x} - \frac{\partial f(x,y)}{\partial y} = 2 0 2 X'Y-XY'=2","['ordinary-differential-equations', 'partial-differential-equations']"
96,Solution to differential equation system and solution to its conversion into 2nd order differential equation,Solution to differential equation system and solution to its conversion into 2nd order differential equation,,"Following is the differential equation system with IVP $\vec{x}'=\small\begin{pmatrix}3&-9\\4&-3\end{pmatrix}\vec{x},    \vec{x}(0)=\small\begin{pmatrix}2\\-4\end{pmatrix}$ The particular solution to this differential equation system is given below. $\vec{x}(t)=\frac23\small\begin{pmatrix}3\cos{(3\sqrt{3}t)}\\\cos{(3\sqrt{3}t)}+\sqrt{3}\sin{(3\sqrt{3}t)}\end{pmatrix}+\frac{14}{3\sqrt{3}}\small\begin{pmatrix}3\sin{(3\sqrt{3}t)}\\\sin{(3\sqrt{3}t)}-\sqrt{3}\cos{(3\sqrt{3}t)}\end{pmatrix}...(1)$ When i converted this differential equation system into 2nd order differential equation, I got $y""+13y'-7y=0, y(0)=2,y'(0)=-4$ Now, the particular solution to this 2nd order equation is $-\frac{2\sqrt{6}}{3}\sin{(\sqrt{6}t)}+2\cos{(\sqrt{6}t)}...(2)$ Now why there is a difference between these two solutions namely (1) and (2)","Following is the differential equation system with IVP The particular solution to this differential equation system is given below. When i converted this differential equation system into 2nd order differential equation, I got Now, the particular solution to this 2nd order equation is Now why there is a difference between these two solutions namely (1) and (2)","\vec{x}'=\small\begin{pmatrix}3&-9\\4&-3\end{pmatrix}\vec{x},   
\vec{x}(0)=\small\begin{pmatrix}2\\-4\end{pmatrix} \vec{x}(t)=\frac23\small\begin{pmatrix}3\cos{(3\sqrt{3}t)}\\\cos{(3\sqrt{3}t)}+\sqrt{3}\sin{(3\sqrt{3}t)}\end{pmatrix}+\frac{14}{3\sqrt{3}}\small\begin{pmatrix}3\sin{(3\sqrt{3}t)}\\\sin{(3\sqrt{3}t)}-\sqrt{3}\cos{(3\sqrt{3}t)}\end{pmatrix}...(1) y""+13y'-7y=0, y(0)=2,y'(0)=-4 -\frac{2\sqrt{6}}{3}\sin{(\sqrt{6}t)}+2\cos{(\sqrt{6}t)}...(2)","['ordinary-differential-equations', 'trigonometry', 'complex-numbers', 'self-learning']"
97,Bifurcation Diagram-confusion,Bifurcation Diagram-confusion,,"I am a bit confused about how the Bifurcation Diagram of a parametric autonomous system $x'=f(x,μ)$ is defined. For the one dimentional case, I think it is more obvious to me, but still not clear enough:  For example, if $$x'=μ-x^2$$ then the equilibria are: For $μ=0$ is only the $0$ which is unstable For $ 0\ltμ$ there are two equilibria $\sqrt{μ}, - \sqrt{μ} $ with the first one unstable and the second stable. For $ μ\lt0$ there are no equilibria. Now, should the bifurcation diagram be the graph of the functions? $x=0$ , $x=\sqrt{μ}$ , $x= - \sqrt{μ} $ ? dotted where the $μ$ gives unstable equilibria? In my book I have a diagram like the following: Furthermore, what the bifurcation diagram should look like if the system $x'=f(x,μ)$ is planar? My confusion here is what the axis $x$ then should represent... Thanks.","I am a bit confused about how the Bifurcation Diagram of a parametric autonomous system is defined. For the one dimentional case, I think it is more obvious to me, but still not clear enough:  For example, if then the equilibria are: For is only the which is unstable For there are two equilibria with the first one unstable and the second stable. For there are no equilibria. Now, should the bifurcation diagram be the graph of the functions? , , ? dotted where the gives unstable equilibria? In my book I have a diagram like the following: Furthermore, what the bifurcation diagram should look like if the system is planar? My confusion here is what the axis then should represent... Thanks.","x'=f(x,μ) x'=μ-x^2 μ=0 0  0\ltμ \sqrt{μ}, - \sqrt{μ}   μ\lt0 x=0 x=\sqrt{μ} x= - \sqrt{μ}  μ x'=f(x,μ) x","['ordinary-differential-equations', 'dynamical-systems']"
98,Problems with an exact differential eqution,Problems with an exact differential eqution,,"Consider the following differential equation $$ \left(\frac{1}{x}-\frac{y^2}{(x-y)^2}\right)dx=\left(\frac{1}{y}-\frac{x^2}{(x-y)^2}\right)dy $$ I want  to find its general solution. I get that this equation is exact but, I trying to solve it for this method and it seems not  work. Can someone give me a hit?","Consider the following differential equation I want  to find its general solution. I get that this equation is exact but, I trying to solve it for this method and it seems not  work. Can someone give me a hit?","
\left(\frac{1}{x}-\frac{y^2}{(x-y)^2}\right)dx=\left(\frac{1}{y}-\frac{x^2}{(x-y)^2}\right)dy
",['ordinary-differential-equations']
99,Solution of SDE with additive noise,Solution of SDE with additive noise,,"I have a question about stochastic differential equations with additive noise. My question is: Is the solution of a SDE with additive noise almost surely equal to the solution of the corresponding deterministic equation plus the noise? Mathematically formulated, my question is the following: Let $X_t$ be the solution to the SDE $$ \mathrm{d}X_t = b(t,X_t) \mathrm{d}t + \mathrm{d} B_t,\; X_0=\xi, $$ where $B_t$ denotes a Brownian motion and with suitable assumptions on $b$ to assure existence and uniqueness of a solution. Furthermore, let $Y_t$ be the solution to the integral equation $$ \mathrm{d}Y_t = b(t,Y_t)\mathrm{d}t,\; Y_0 =\xi. $$ Is it true that it holds $X_t = Y_t +B_t$ almost surely? Thank you in advance for your input! Best, Luke","I have a question about stochastic differential equations with additive noise. My question is: Is the solution of a SDE with additive noise almost surely equal to the solution of the corresponding deterministic equation plus the noise? Mathematically formulated, my question is the following: Let $X_t$ be the solution to the SDE $$ \mathrm{d}X_t = b(t,X_t) \mathrm{d}t + \mathrm{d} B_t,\; X_0=\xi, $$ where $B_t$ denotes a Brownian motion and with suitable assumptions on $b$ to assure existence and uniqueness of a solution. Furthermore, let $Y_t$ be the solution to the integral equation $$ \mathrm{d}Y_t = b(t,Y_t)\mathrm{d}t,\; Y_0 =\xi. $$ Is it true that it holds $X_t = Y_t +B_t$ almost surely? Thank you in advance for your input! Best, Luke",,"['ordinary-differential-equations', 'stochastic-calculus', 'brownian-motion', 'stochastic-differential-equations']"
