,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A deck of $52$ cards is divided into four piles of $13$ cards. What is the probability that each pile has one ace?,A deck of  cards is divided into four piles of  cards. What is the probability that each pile has one ace?,52 13,"An ordinary deck of $52$ playing cards is randomly divided into $4$ piles of $13$ cards each. Compute the probability that each pile has exactly one ace. The answer provided is is $(39*26*13)/(51*50*49) \approx 0.105$ The above answer uses conditional probability, but I would like to know what's wrong with my reasoning: Call the four piles of partitions $1$, $2$, $3$, and $4$. For partition $1$, there are ${4 \choose 1}$ ways to choose which ace the partition will contain. Then, there are ${48\choose 12}$ ways to choose the remaining $12$ cards, as we cannot choose any other aces. For partition $2$, there are ${3\choose 1}$ ways to choose which ace the partition will contain. Then, there are ${36\choose 12}$ ways to choose the remaining $12$ cards, as there are only $36$ non-ace cards left. Following similar reasoning for partitions $3$ and $4$, we find that there are ${2\choose 1}{24 \choose 12}$ and ${1\choose 1}{12 \choose 12} = 1$ ways to form those hands. Therefore, my probability is given by $$\frac{4 \cdot {48\choose 12} + 3\cdot{36\choose 12} + 2\cdot{24\choose 12}}{{52\choose 13}{39\choose 13}{26\choose 13}} \not \approx 0.105$$ The denominator is the number of ways to choose the cards in each hand without any constraints. I am not sure what is wrong with my computation.","An ordinary deck of $52$ playing cards is randomly divided into $4$ piles of $13$ cards each. Compute the probability that each pile has exactly one ace. The answer provided is is $(39*26*13)/(51*50*49) \approx 0.105$ The above answer uses conditional probability, but I would like to know what's wrong with my reasoning: Call the four piles of partitions $1$, $2$, $3$, and $4$. For partition $1$, there are ${4 \choose 1}$ ways to choose which ace the partition will contain. Then, there are ${48\choose 12}$ ways to choose the remaining $12$ cards, as we cannot choose any other aces. For partition $2$, there are ${3\choose 1}$ ways to choose which ace the partition will contain. Then, there are ${36\choose 12}$ ways to choose the remaining $12$ cards, as there are only $36$ non-ace cards left. Following similar reasoning for partitions $3$ and $4$, we find that there are ${2\choose 1}{24 \choose 12}$ and ${1\choose 1}{12 \choose 12} = 1$ ways to form those hands. Therefore, my probability is given by $$\frac{4 \cdot {48\choose 12} + 3\cdot{36\choose 12} + 2\cdot{24\choose 12}}{{52\choose 13}{39\choose 13}{26\choose 13}} \not \approx 0.105$$ The denominator is the number of ways to choose the cards in each hand without any constraints. I am not sure what is wrong with my computation.",,['probability']
1,Conditional Expectation of Bernoulli R.V.,Conditional Expectation of Bernoulli R.V.,,"Let $X_1, X_2,\ldots, X_n$ be iid bernoulli r.v. with parameter $p$. Let $S=X_1+\cdots+X_n$ and $Y=X_1X_2$. Compute $\mathbb{E}(Y\mid S)$. I know that $\mathbb{E}(X_1\mid S) = S/n$. So If I could split $\mathbb{E}(Y\mid S)$ into two then I would be done. But I don't think that is allowed. How to proceed?","Let $X_1, X_2,\ldots, X_n$ be iid bernoulli r.v. with parameter $p$. Let $S=X_1+\cdots+X_n$ and $Y=X_1X_2$. Compute $\mathbb{E}(Y\mid S)$. I know that $\mathbb{E}(X_1\mid S) = S/n$. So If I could split $\mathbb{E}(Y\mid S)$ into two then I would be done. But I don't think that is allowed. How to proceed?",,"['probability', 'probability-theory', 'conditional-expectation']"
2,the probability of those n broken parts of sticks to form a closed polygon? [closed],the probability of those n broken parts of sticks to form a closed polygon? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose I have a stick of L length . It is divided into uniformly randomly into n parts. What is the probability of those n parts to form a closed polygon ? It will be helpful if anyone show the steps to proceed with this problem.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose I have a stick of L length . It is divided into uniformly randomly into n parts. What is the probability of those n parts to form a closed polygon ? It will be helpful if anyone show the steps to proceed with this problem.",,"['probability', 'polygons']"
3,Why does the identity $\mathbb{E}(X) = \mathbb{E}\left(\int \mathbb{1}_{u \leq X}du\right)$ hold?,Why does the identity  hold?,\mathbb{E}(X) = \mathbb{E}\left(\int \mathbb{1}_{u \leq X}du\right),"I'm reading on Hoeffding's covariance identity, the proof of which is neatly covered here , or, in a similar manner, in this MSE post , but I can't seem to fully understand the trick/property used there. I.e., assume $(X_1, Y_1)$ and $(X_2, Y_2)$ are two independent vectors with identical distribution. The key point in the proof is to note that we can write $$ \mathbb{E}[(X_1 - X_2) (Y_1 - Y_2)]$$ as $$ \mathbb{E}\left(\iint_{\mathbb{R}\times\mathbb{R}} [\mathbb{1}_{u\leq X_1} - \mathbb{1}_{u \leq X_2}] \cdot [\mathbb{1}_{v\leq Y_1} - \mathbb{1}_{v \leq Y_2}]\,du\,dv \right)$$ Why does this hold?","I'm reading on Hoeffding's covariance identity, the proof of which is neatly covered here , or, in a similar manner, in this MSE post , but I can't seem to fully understand the trick/property used there. I.e., assume $(X_1, Y_1)$ and $(X_2, Y_2)$ are two independent vectors with identical distribution. The key point in the proof is to note that we can write $$ \mathbb{E}[(X_1 - X_2) (Y_1 - Y_2)]$$ as $$ \mathbb{E}\left(\iint_{\mathbb{R}\times\mathbb{R}} [\mathbb{1}_{u\leq X_1} - \mathbb{1}_{u \leq X_2}] \cdot [\mathbb{1}_{v\leq Y_1} - \mathbb{1}_{v \leq Y_2}]\,du\,dv \right)$$ Why does this hold?",,"['probability', 'probability-theory', 'expectation', 'covariance']"
4,probabilitiy: roll die $12$ times get each side twice,probabilitiy: roll die  times get each side twice,12,this is my 3rd question today ^^ Thanks a lot for any help :-) I need to calculate how probable it is if i roll a die 12 rounds and i get each side twice. Idea: Because every side needs to be present twice and we roll the die 12 times: $$ \left ( \frac{2}{6*2} \right )^{6} = \left ( \frac{2}{12} \right )^{6} = \left ( \frac{1}{6} \right )^{6} $$ Is this correct?,this is my 3rd question today ^^ Thanks a lot for any help :-) I need to calculate how probable it is if i roll a die 12 rounds and i get each side twice. Idea: Because every side needs to be present twice and we roll the die 12 times: $$ \left ( \frac{2}{6*2} \right )^{6} = \left ( \frac{2}{12} \right )^{6} = \left ( \frac{1}{6} \right )^{6} $$ Is this correct?,,['probability']
5,"If two random variables have the same variance and expectation, do they have the same distribution?","If two random variables have the same variance and expectation, do they have the same distribution?",,"If two random variables say $X$ and $Y$ have the same variance and expectation, do they have the same distrubution?","If two random variables say $X$ and $Y$ have the same variance and expectation, do they have the same distrubution?",,"['probability', 'statistics']"
6,What is the probability that a red ball is chosen before the black ball?,What is the probability that a red ball is chosen before the black ball?,,"A box contains 2 white balls, 2 red balls and a black ball. Balls are chosen without replacement from the box. What is the probability that red ball is chosen before the black ball? I am quite confused about the question because this is a exercise arranged in ""combination"" section, however I intuitively think it as a ""permutation"" problem. The red balls and the white balls have to be different, and as a red is chosen before the black, then its order should be accounted with. Furthermore, the question is that ""the red"", thus the red ball labeled 1 and the red ball labeled 2 can be chosen without considering their orders. Can anyone give me a clue to solve this kind of problem?","A box contains 2 white balls, 2 red balls and a black ball. Balls are chosen without replacement from the box. What is the probability that red ball is chosen before the black ball? I am quite confused about the question because this is a exercise arranged in ""combination"" section, however I intuitively think it as a ""permutation"" problem. The red balls and the white balls have to be different, and as a red is chosen before the black, then its order should be accounted with. Furthermore, the question is that ""the red"", thus the red ball labeled 1 and the red ball labeled 2 can be chosen without considering their orders. Can anyone give me a clue to solve this kind of problem?",,"['probability', 'combinatorics']"
7,Shortest way to answer probability / game question?,Shortest way to answer probability / game question?,,"Your game piece is on the starting space, and the finish is 10 more spaces away. During each of your turns, you roll a fair six-sided die, and move that many spaces forward. However, if the result would put you past the finish, you do not move. What is the probability that you will reach the finish in 3 or fewer turns? What is the shortest way to get this answer? I know the answer is 2/9, but curious what is the most efficient method of answering this? EDIT: The question (and the given answer / method to obtain in a long way can be found at this link: Check out this problem I found on Brilliant! https://brilliant.org/practice/conditional-probability-casework-calculations/?problem=discrete-mathematics-problem-109432&chapter=conditional-probability-2 I would have simply copied and pasted the answer but couldn't get the graphics to display properly and it will be better displayed in the associated link.","Your game piece is on the starting space, and the finish is 10 more spaces away. During each of your turns, you roll a fair six-sided die, and move that many spaces forward. However, if the result would put you past the finish, you do not move. What is the probability that you will reach the finish in 3 or fewer turns? What is the shortest way to get this answer? I know the answer is 2/9, but curious what is the most efficient method of answering this? EDIT: The question (and the given answer / method to obtain in a long way can be found at this link: Check out this problem I found on Brilliant! https://brilliant.org/practice/conditional-probability-casework-calculations/?problem=discrete-mathematics-problem-109432&chapter=conditional-probability-2 I would have simply copied and pasted the answer but couldn't get the graphics to display properly and it will be better displayed in the associated link.",,"['calculus', 'probability', 'statistics']"
8,Could please indicate theory behind,Could please indicate theory behind,,"I was solving a probability problem, and found two ways of wolving it. Both came to right same answer (according to test answer). The first way gives a little more work but I understood the backgroud theory. My doubt is about the second way which is much simpler, but I don't know if it's valid or the theory behind. So that's what I want to know: If the simplest way is valid and if some one could explain the meaning behind or indicate the theory that explains. Problem: Given 10 students, and making a group with 3 of then, what is the probability that a specific student will be chosen? First way (more work but all the theory is explained): The number of all diferent possible groups is given by the combination C(10,3) = 120; This is the total sample space. The number of all possible groups with that specific student will be C(9,2) = 36, because one position of the three is already taken; So the probability of the specific student be chosen is 36/120 = 0,3 The second way to solve, I was told that as it's 3 positions in 10 total possibilities I could simple do 3/10...","I was solving a probability problem, and found two ways of wolving it. Both came to right same answer (according to test answer). The first way gives a little more work but I understood the backgroud theory. My doubt is about the second way which is much simpler, but I don't know if it's valid or the theory behind. So that's what I want to know: If the simplest way is valid and if some one could explain the meaning behind or indicate the theory that explains. Problem: Given 10 students, and making a group with 3 of then, what is the probability that a specific student will be chosen? First way (more work but all the theory is explained): The number of all diferent possible groups is given by the combination C(10,3) = 120; This is the total sample space. The number of all possible groups with that specific student will be C(9,2) = 36, because one position of the three is already taken; So the probability of the specific student be chosen is 36/120 = 0,3 The second way to solve, I was told that as it's 3 positions in 10 total possibilities I could simple do 3/10...",,"['probability', 'probability-theory']"
9,Probability of choosing bat,Probability of choosing bat,,I got the option A and B by manually checking but after that got stuck .,I got the option A and B by manually checking but after that got stuck .,,['probability']
10,Kullback-Leibler divergence for binomial distributions P and Q,Kullback-Leibler divergence for binomial distributions P and Q,,"Note: this is an assignment question Let $X$ be a discrete random variable with values in $\{1,...,n\}$ . $P$ denotes the distribution on $\{1,...,n\}$ when $X$ ~ $bin(n,p)$ and Q denotes the distribution on $\{1,...,n\}$ when $X$ ~ $bin(n,q)$ for $p,q \in (0,1)$ . Compute the Kullback-Leibler-distance $D(P || Q)$ . We write $X$ ~ $bin(n,p)$ if it is Binomial-distributed with parameters $n,p$ , that is $$P[X=k]=\binom{n}{k} p^k (1-p)^{n-k}$$ I have started to write down the definition of the KL divergence which is : $$D(P||Q)=\sum_{x \in X}{p(x)*log_2 \frac{p(x)}{q(x)}}.$$ After inserting my values this is: $$D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{\binom{n}{x}p^x(1-p)^{n-x}}{\binom{n}{x}q^x(1-q)^{n-x}}}.$$ from which I can factor out $\binom{n}{x}$ in the fraction: $$D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{p^x(1-p)^{n-x}}{q^x(1-q)^{n-x}}}.$$ I don't see how I can further simplify this term, can someone give me a hint?","Note: this is an assignment question Let be a discrete random variable with values in . denotes the distribution on when ~ and Q denotes the distribution on when ~ for . Compute the Kullback-Leibler-distance . We write ~ if it is Binomial-distributed with parameters , that is I have started to write down the definition of the KL divergence which is : After inserting my values this is: from which I can factor out in the fraction: I don't see how I can further simplify this term, can someone give me a hint?","X \{1,...,n\} P \{1,...,n\} X bin(n,p) \{1,...,n\} X bin(n,q) p,q \in (0,1) D(P || Q) X bin(n,p) n,p P[X=k]=\binom{n}{k} p^k (1-p)^{n-k} D(P||Q)=\sum_{x \in X}{p(x)*log_2 \frac{p(x)}{q(x)}}. D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{\binom{n}{x}p^x(1-p)^{n-x}}{\binom{n}{x}q^x(1-q)^{n-x}}}. \binom{n}{x} D(P||Q)=\sum_{x \in X}{ \binom{n}{x}p^x(1-p)^{n-x} *log_2 \frac{p^x(1-p)^{n-x}}{q^x(1-q)^{n-x}}}.",['probability']
11,Reparameterization of hyperprior distribution,Reparameterization of hyperprior distribution,,"I'm reading about Bayesian data analysis by Gelman et al. and I'm having big trouble interpreting the following part in the book (note, the rat tumor rate $\theta$ in the following text has: $\theta \sim Beta(\alpha, \beta)$ ): Choosing a standard parameterization and setting up a ‘noninformative’ hyperprior dis- tribution. Because we have no immediately available information about the distribution of tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribution for $(\alpha, \beta)$ . Before assigning a hyperprior distribution, we reparameterize in terms of $\text{logit}(\frac{\alpha}{\alpha+\beta}) = \log(\frac{\alpha}{\beta})$ and $\log(\alpha+\beta)$ , which are the logit of the mean and the logarithm of the ‘sample size’ in the beta population distribution for $θ$ . It would seem reasonable to assign independent hyperprior distributions to the prior mean and ‘sample size,’ and we use the logistic and logarithmic transformations to put each on a $(-\infty, \infty)$ scale. Unfortunately, a uniform prior density on these newly transformed parameters yields an improper posterior density, with an infinite integral in the limit $(\alpha+\beta) \rightarrow \infty$ , and so this particular prior density cannot be used here. In a problem such as this with a reasonably large amount of data, it is possible to set up a ‘noninformative’ hyperprior density that is dominated by the likelihood and yields a proper posterior distribution. One reasonable choice of diffuse hyperprior density is uniform on $(\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2})$ , which when multiplied by the appropriate Jacobian yields the following densities on the original scale , $$p(\alpha, \beta) \propto (\alpha+\beta)^{−5/2},$$ and on the natural transformed scale : $$p\left(\log\left(\frac{\alpha}{\beta}\right), \log(\alpha+\beta)\right)\propto \alpha\beta(\alpha+\beta)^{−5/2}.$$ My problem is especially the bolded parts in the text. Question (1): What does the author explicitly mean by: "" is uniform on $(\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2})$ "" Question (2): What is the appropriate Jacobian ? Question (3): How does the author arrive into the original and transformed scale priors? To me the book hides many details under the hood and makes understanding difficult for a beginner on the subject due to seemingly ambiguous text. P.S. if you need more information, or me to clarify my questions please let me know.","I'm reading about Bayesian data analysis by Gelman et al. and I'm having big trouble interpreting the following part in the book (note, the rat tumor rate in the following text has: ): Choosing a standard parameterization and setting up a ‘noninformative’ hyperprior dis- tribution. Because we have no immediately available information about the distribution of tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribution for . Before assigning a hyperprior distribution, we reparameterize in terms of and , which are the logit of the mean and the logarithm of the ‘sample size’ in the beta population distribution for . It would seem reasonable to assign independent hyperprior distributions to the prior mean and ‘sample size,’ and we use the logistic and logarithmic transformations to put each on a scale. Unfortunately, a uniform prior density on these newly transformed parameters yields an improper posterior density, with an infinite integral in the limit , and so this particular prior density cannot be used here. In a problem such as this with a reasonably large amount of data, it is possible to set up a ‘noninformative’ hyperprior density that is dominated by the likelihood and yields a proper posterior distribution. One reasonable choice of diffuse hyperprior density is uniform on , which when multiplied by the appropriate Jacobian yields the following densities on the original scale , and on the natural transformed scale : My problem is especially the bolded parts in the text. Question (1): What does the author explicitly mean by: "" is uniform on "" Question (2): What is the appropriate Jacobian ? Question (3): How does the author arrive into the original and transformed scale priors? To me the book hides many details under the hood and makes understanding difficult for a beginner on the subject due to seemingly ambiguous text. P.S. if you need more information, or me to clarify my questions please let me know.","\theta \theta \sim Beta(\alpha, \beta) (\alpha, \beta) \text{logit}(\frac{\alpha}{\alpha+\beta}) = \log(\frac{\alpha}{\beta}) \log(\alpha+\beta) θ (-\infty, \infty) (\alpha+\beta) \rightarrow \infty (\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2}) p(\alpha, \beta) \propto (\alpha+\beta)^{−5/2}, p\left(\log\left(\frac{\alpha}{\beta}\right), \log(\alpha+\beta)\right)\propto \alpha\beta(\alpha+\beta)^{−5/2}. (\frac{\alpha}{\alpha+\beta}, (\alpha+\beta)^{-1/2})","['probability', 'bayesian', 'change-of-variable']"
12,What is the probability that each book does not have a neighbour of the same color?,What is the probability that each book does not have a neighbour of the same color?,,"There are $13$ identical books which differ only in color: $2$ red, $5$ black and $6$ blue. They were randomly placed in a row. What is the probability of the event that there are no books of same color standing $2$ in a row? At first, I tried to put $6$ blue books in a row so there will be at least one space between them, and I found out that there are ${7 \choose 1}^2$ combinations to do so. But I do not know what to do next.","There are $13$ identical books which differ only in color: $2$ red, $5$ black and $6$ blue. They were randomly placed in a row. What is the probability of the event that there are no books of same color standing $2$ in a row? At first, I tried to put $6$ blue books in a row so there will be at least one space between them, and I found out that there are ${7 \choose 1}^2$ combinations to do so. But I do not know what to do next.",,"['probability', 'combinatorics']"
13,What are the odds of winning this bingo game?,What are the odds of winning this bingo game?,,"I'll explain the game real quick, it's called Pick 8. You get a sheet of paper, containing 3 rows of 8 boxes. You fill out each row with numbers 1-75, in any order, with no duplicates. The bingo caller then starts calling bingo numbers. If you get a row filled during the first 20 numbers called, you win the jackpot, which is usually around \$7000. The first person to win after 20 balls are called wins \$500. I'm interested in the jackpot. Mainly, how many sheets would I need to buy and fill out with an 8-number combination to ensure ~70% probability of winning in the first 20 numbers called? Each sheets costs \$3, so it's \$1 a row. I started by calculating how many 8-number combinations there are in a pool of 75 (where order doesn't matter). Apparently that's called a binomial coefficient, and it comes out to ~16.8 billion combinations. But I don't where to take it from there - how do I take into account the fact that I get 20 chances to get an 8-number combination correct? If I can figure out that, then I can multiply those odds by 3, and then multiply it even further to cover 70% of the possible combos, to see how many sheets I'd need to buy.","I'll explain the game real quick, it's called Pick 8. You get a sheet of paper, containing 3 rows of 8 boxes. You fill out each row with numbers 1-75, in any order, with no duplicates. The bingo caller then starts calling bingo numbers. If you get a row filled during the first 20 numbers called, you win the jackpot, which is usually around \$7000. The first person to win after 20 balls are called wins \$500. I'm interested in the jackpot. Mainly, how many sheets would I need to buy and fill out with an 8-number combination to ensure ~70% probability of winning in the first 20 numbers called? Each sheets costs \$3, so it's \$1 a row. I started by calculating how many 8-number combinations there are in a pool of 75 (where order doesn't matter). Apparently that's called a binomial coefficient, and it comes out to ~16.8 billion combinations. But I don't where to take it from there - how do I take into account the fact that I get 20 chances to get an 8-number combination correct? If I can figure out that, then I can multiply those odds by 3, and then multiply it even further to cover 70% of the possible combos, to see how many sheets I'd need to buy.",,['probability']
14,Probability of getting exactly 3 red hats or 3 blue hats,Probability of getting exactly 3 red hats or 3 blue hats,,"A drawer holds $4$ red hats and $4$ blue hats. What is the probability   of getting exactly $3$ red hats or $3$ blue hats when taking out $4$   hats randomly out of the drawer and immediately returning every hat to   the drawer before taking out the next? When I come across the question (I saw this questions two times in a week), I tried to solve in this way- probability of getting exactly $3$ red hats is $$\frac{{4\choose 3}{4\choose 1}}{{8\choose 4}} = \frac{16}{70}$$ probability of getting exactly $3$ blue hats is $$\frac{{4\choose 3}{4\choose 1}}{{8\choose 4}} = \frac{16}{70}$$ So, P(exactly $3$ red or exactly $3$ blue ) = $\frac{32}{70}$ But answer of this problem is $\frac{1}{2}$. What is my mistake?","A drawer holds $4$ red hats and $4$ blue hats. What is the probability   of getting exactly $3$ red hats or $3$ blue hats when taking out $4$   hats randomly out of the drawer and immediately returning every hat to   the drawer before taking out the next? When I come across the question (I saw this questions two times in a week), I tried to solve in this way- probability of getting exactly $3$ red hats is $$\frac{{4\choose 3}{4\choose 1}}{{8\choose 4}} = \frac{16}{70}$$ probability of getting exactly $3$ blue hats is $$\frac{{4\choose 3}{4\choose 1}}{{8\choose 4}} = \frac{16}{70}$$ So, P(exactly $3$ red or exactly $3$ blue ) = $\frac{32}{70}$ But answer of this problem is $\frac{1}{2}$. What is my mistake?",,['probability']
15,A fair coin is flipped 3 times. Probability of all $3$ heads given at least $2$ were heads,A fair coin is flipped 3 times. Probability of all  heads given at least  were heads,3 2,"Here's a problem from Prof. Blitzstein's Stat 110 textbook . Please see my solution below. A fair coin is flipped 3 times. The toss results are recorded on separate slips of paper (writing “H” if Heads and “T” if Tails), and the 3 slips of paper are thrown into a hat. (a) Find the probability that all 3 tosses landed Heads, given that at least 2 were Heads. (b) Two of the slips of paper are randomly drawn from the hat, and both show the letter H. Given this information, what is the probability that all 3 tosses landed Heads? The binomial distribution of the number of heads (#H) is as follows: $\mathbb{P}(\#H=0) = 1/8 $ $\mathbb{P}(\#H=1) = 3/8 $ $\mathbb{P}(\#H=2) = 3/8 $ $\mathbb{P}(\#H=3) = 1/8 $ a) Probability of 3 heads given at least 2 heads = $\mathbb{P}(\#H=3)/\mathbb{P}(\#H \ge 2 ) = (1/8)/(1/2)=1/4$ b) But I'm not able to understand how (b) is different from (a). To me it's the same problem and has the same answer. Can anyone help please?","Here's a problem from Prof. Blitzstein's Stat 110 textbook . Please see my solution below. A fair coin is flipped 3 times. The toss results are recorded on separate slips of paper (writing “H” if Heads and “T” if Tails), and the 3 slips of paper are thrown into a hat. (a) Find the probability that all 3 tosses landed Heads, given that at least 2 were Heads. (b) Two of the slips of paper are randomly drawn from the hat, and both show the letter H. Given this information, what is the probability that all 3 tosses landed Heads? The binomial distribution of the number of heads (#H) is as follows: a) Probability of 3 heads given at least 2 heads = b) But I'm not able to understand how (b) is different from (a). To me it's the same problem and has the same answer. Can anyone help please?",\mathbb{P}(\#H=0) = 1/8  \mathbb{P}(\#H=1) = 3/8  \mathbb{P}(\#H=2) = 3/8  \mathbb{P}(\#H=3) = 1/8  \mathbb{P}(\#H=3)/\mathbb{P}(\#H \ge 2 ) = (1/8)/(1/2)=1/4,"['probability', 'binomial-distribution']"
16,What is a probability that at 2nd turn you will pick green ball?,What is a probability that at 2nd turn you will pick green ball?,,"I have an interesting question that I certainly don't know how to solve it. I've already read many topics on probability, eg: Probability that someone will pick a red ball first? and Comparing probabilities of drawing balls of certain color, with and without replacement etc. But unfortunately, I can't apply the same methodology in this case and get the right answer from the given ones (it seems I'm really silly one). So here is the question: There are 5 balls in a bucket: green, blue, red, orange and black. Each turn you take a random ball from the bucket. What is a probability that at 2nd turn you will pick blue ball? The answers: 1/2 2/3 1/3 2/5 1/5 The first way I thought is to add probability of each turn like this: $\frac{1}{5} + \frac{1}{4}$ - 1/4 because at 2nd turn we have only four balls. However, the answer become $\frac{9}{20}$ which is not correct. I know there is something to do with either factorial or combination (just my assumption).","I have an interesting question that I certainly don't know how to solve it. I've already read many topics on probability, eg: Probability that someone will pick a red ball first? and Comparing probabilities of drawing balls of certain color, with and without replacement etc. But unfortunately, I can't apply the same methodology in this case and get the right answer from the given ones (it seems I'm really silly one). So here is the question: There are 5 balls in a bucket: green, blue, red, orange and black. Each turn you take a random ball from the bucket. What is a probability that at 2nd turn you will pick blue ball? The answers: 1/2 2/3 1/3 2/5 1/5 The first way I thought is to add probability of each turn like this: $\frac{1}{5} + \frac{1}{4}$ - 1/4 because at 2nd turn we have only four balls. However, the answer become $\frac{9}{20}$ which is not correct. I know there is something to do with either factorial or combination (just my assumption).",,['probability']
17,Labeled balls and labeled boxes at least X in a box,Labeled balls and labeled boxes at least X in a box,,"I am having a problem to solve all the exercises that involve the ""at least $1$ ball need to be in box #2"" kind of problems. For example: there are $8$ numbered cells and we drop $10$ numbered balls into them (each cell can contain unlimited number of balls). What is the probability that cell number $1$ will contain $0$ balls and cell number $2$ will contain at least $1$ ball? What I did is to calculate the probability of cell number $1$ to be empty $(0.263)$ and the probability of cell number $2$ NOT containing $0$ balls $(1 - 0.263)$ and then I multiplied them both getting a result of $0.19$ which is wrong. I searched google and found this https://www.uni-due.de/~hn213me/mt/w13/isedm/KOBallsBoxes.pdf case 1.1.2 but the problem is that we didn't learn about Stirling numbers and we never gonna learn about them as far as I know (seems way too advance for this course). How do I solve these kind of exercises? Thanks in advance","I am having a problem to solve all the exercises that involve the ""at least $1$ ball need to be in box #2"" kind of problems. For example: there are $8$ numbered cells and we drop $10$ numbered balls into them (each cell can contain unlimited number of balls). What is the probability that cell number $1$ will contain $0$ balls and cell number $2$ will contain at least $1$ ball? What I did is to calculate the probability of cell number $1$ to be empty $(0.263)$ and the probability of cell number $2$ NOT containing $0$ balls $(1 - 0.263)$ and then I multiplied them both getting a result of $0.19$ which is wrong. I searched google and found this https://www.uni-due.de/~hn213me/mt/w13/isedm/KOBallsBoxes.pdf case 1.1.2 but the problem is that we didn't learn about Stirling numbers and we never gonna learn about them as far as I know (seems way too advance for this course). How do I solve these kind of exercises? Thanks in advance",,"['probability', 'combinatorics']"
18,Aa student takes a $10$-question true/false exam and guesses. What is the probability that the student answers every question incorrectly?,Aa student takes a -question true/false exam and guesses. What is the probability that the student answers every question incorrectly?,10,Suppose a student takes a $10$-question true/false exam and guesses at every question. What is the probability that... a) the student answers every question incorrectly? b) the students answers at least one question correctly? c) the student answers exactly 5 questions correctly? So far I have solved a. I got $(1/2)^{10}$. For b I'm not sure what to do. For c would the answer be $(1/2)^5$?,Suppose a student takes a $10$-question true/false exam and guesses at every question. What is the probability that... a) the student answers every question incorrectly? b) the students answers at least one question correctly? c) the student answers exactly 5 questions correctly? So far I have solved a. I got $(1/2)^{10}$. For b I'm not sure what to do. For c would the answer be $(1/2)^5$?,,"['probability', 'discrete-mathematics', 'binomial-distribution']"
19,What is the probability of getting exactly 3 two's OR three's when a die is rolled 8 times?,What is the probability of getting exactly 3 two's OR three's when a die is rolled 8 times?,,"What is the probability of getting exactly $3$ two's OR three's when a die is rolled $8$ times? I know that $P(E) = |E| / |S|$. I believe that $|S| = 36$, since there are $36$ different combinations when rolling a die. I am not sure how to get the probability of rolling exactly $3$ two's OR three's when rolling $8$ times.","What is the probability of getting exactly $3$ two's OR three's when a die is rolled $8$ times? I know that $P(E) = |E| / |S|$. I believe that $|S| = 36$, since there are $36$ different combinations when rolling a die. I am not sure how to get the probability of rolling exactly $3$ two's OR three's when rolling $8$ times.",,"['probability', 'combinatorics', 'discrete-mathematics', 'dice']"
20,Expected number of targets hit with paired shooters,Expected number of targets hit with paired shooters,,"Consider $n$ targets, with 2 shooters aiming at each target, as shown in the picture below. If we randomly choose $k$ out of the $2n$ shooters to fire, what is the expected number $E(k)$ of targets that will be hit? I can of course solve this for simple cases and make some general observations, e.g.: Trivially $E(1)=1$, for any $n$. For $k=2$, fix one shooter, and the probability that the other randomly-chosen one is his 'partner' is $\frac{1}{2n-1}$, in which case one target is hit; otherwise, two targets are hit. From these probabilities the expectation easily follows. If $k>n$ then at least one target has both its shooters firing at it. Remove it and consider the problem with $n-1$ targets. Do this recursively until $k\leq n$. So the only uniquely interesting cases are $3 \leq k \leq n$. I am wondering if this can be solved in the general case. The expectation will suffice for me, but of course the exact probability distribution would be even nicer.","Consider $n$ targets, with 2 shooters aiming at each target, as shown in the picture below. If we randomly choose $k$ out of the $2n$ shooters to fire, what is the expected number $E(k)$ of targets that will be hit? I can of course solve this for simple cases and make some general observations, e.g.: Trivially $E(1)=1$, for any $n$. For $k=2$, fix one shooter, and the probability that the other randomly-chosen one is his 'partner' is $\frac{1}{2n-1}$, in which case one target is hit; otherwise, two targets are hit. From these probabilities the expectation easily follows. If $k>n$ then at least one target has both its shooters firing at it. Remove it and consider the problem with $n-1$ targets. Do this recursively until $k\leq n$. So the only uniquely interesting cases are $3 \leq k \leq n$. I am wondering if this can be solved in the general case. The expectation will suffice for me, but of course the exact probability distribution would be even nicer.",,"['probability', 'combinatorics', 'expectation']"
21,Variance of Expected Value,Variance of Expected Value,,"Given a random variable $x$, what is $Var(E[x])$? My intuition is that it would simply be the same as $Var(x)$, but I'm not sure how to prove that. Any and all help would be greatly appreciated!","Given a random variable $x$, what is $Var(E[x])$? My intuition is that it would simply be the same as $Var(x)$, but I'm not sure how to prove that. Any and all help would be greatly appreciated!",,"['probability', 'statistics']"
22,Drawing without replacement - prob. for an Ace followed by an Ace?,Drawing without replacement - prob. for an Ace followed by an Ace?,,"Given a standard 52-cards deck: You are extracting cards from the deck without replacement , until you get an ""Ace"" for the first time. What is the probability that the next card will be ""Ace"" too? I've already seen the following Q&A: Probability of drawing an Ace: before and after According to that thread, the answer should be:  $$\frac{4 \cdot 3}{52 \cdot 51} = \frac{1}{221}$$ But the official solution says that the answer is: $$\frac{4}{52}$$ which doesn't make sense IMHO. They solved it only with intuition or ""mind trick"", as they wrote.. My calculation: Assuming that the 1st card is Ace, then: $$\frac{4 \cdot 3}{52 \cdot 51} = \frac{1}{221}$$ Assuming that the 2nd card is Ace, then: $$\frac{(52-4) \cdot 4 \cdot 3}{52 \cdot 51 \cdot 50} = \frac{24}{5525}$$ We notice a pattern here. Having the 1st Ace at the $k$'th draw, then the probability (for a second Ace after that) is: $$ p_1 = \frac{ {_{52-4}P_{k-1}} \cdot 4 \cdot 3 }{ {_{52}P_{k-1}} \cdot {_{52-k}P_{2}} } $$ We need to consider the least-possible scenario - we draw 48 non-Ace cards, then: $$ p_2 = \frac{48! \cdot 4 \cdot 3}{ {_{52}P_{50}} } = \frac{1}{270725} $$ So, the required probability is: $$\begin{align} p &= p_2 + \sum\limits^{48}_{k=1} p_1 \\ &= p_2 + \sum\limits^{48}_{k=1} \frac{ {_{48}P_{k-1}} \cdot 4 \cdot 3 }{ {_{52}P_{k-1}} \cdot {_{52-k}P_{2}} }\\ &= \frac{1}{270725} + \frac{1696}{20825}\\ &= \frac{1297}{15925}\\ &\cong 0.081444 \end{align} $$ But my answer is far from either the official solution and from the answer in that thread.","Given a standard 52-cards deck: You are extracting cards from the deck without replacement , until you get an ""Ace"" for the first time. What is the probability that the next card will be ""Ace"" too? I've already seen the following Q&A: Probability of drawing an Ace: before and after According to that thread, the answer should be:  $$\frac{4 \cdot 3}{52 \cdot 51} = \frac{1}{221}$$ But the official solution says that the answer is: $$\frac{4}{52}$$ which doesn't make sense IMHO. They solved it only with intuition or ""mind trick"", as they wrote.. My calculation: Assuming that the 1st card is Ace, then: $$\frac{4 \cdot 3}{52 \cdot 51} = \frac{1}{221}$$ Assuming that the 2nd card is Ace, then: $$\frac{(52-4) \cdot 4 \cdot 3}{52 \cdot 51 \cdot 50} = \frac{24}{5525}$$ We notice a pattern here. Having the 1st Ace at the $k$'th draw, then the probability (for a second Ace after that) is: $$ p_1 = \frac{ {_{52-4}P_{k-1}} \cdot 4 \cdot 3 }{ {_{52}P_{k-1}} \cdot {_{52-k}P_{2}} } $$ We need to consider the least-possible scenario - we draw 48 non-Ace cards, then: $$ p_2 = \frac{48! \cdot 4 \cdot 3}{ {_{52}P_{50}} } = \frac{1}{270725} $$ So, the required probability is: $$\begin{align} p &= p_2 + \sum\limits^{48}_{k=1} p_1 \\ &= p_2 + \sum\limits^{48}_{k=1} \frac{ {_{48}P_{k-1}} \cdot 4 \cdot 3 }{ {_{52}P_{k-1}} \cdot {_{52-k}P_{2}} }\\ &= \frac{1}{270725} + \frac{1696}{20825}\\ &= \frac{1297}{15925}\\ &\cong 0.081444 \end{align} $$ But my answer is far from either the official solution and from the answer in that thread.",,['probability']
23,Probability that the equation $x^2 + k_1 x + k_0 = 0$ has real solutions,Probability that the equation  has real solutions,x^2 + k_1 x + k_0 = 0,"$k_1$, $k_0$ are random integer numbers between $1$ and $100$ (including $1$ and $100$, and uniformly distributed). What is the probability that the equation $x^2 + k_1 x + k_0 = 0$ has real solutions? This is a subproblem of another problem, and I do not know how to approach it without brute force, hope some of you will propose a fresh idea. The answer should be devised without using computer, if possible.","$k_1$, $k_0$ are random integer numbers between $1$ and $100$ (including $1$ and $100$, and uniformly distributed). What is the probability that the equation $x^2 + k_1 x + k_0 = 0$ has real solutions? This is a subproblem of another problem, and I do not know how to approach it without brute force, hope some of you will propose a fresh idea. The answer should be devised without using computer, if possible.",,['probability']
24,Probability that two sets do not intersect,Probability that two sets do not intersect,,"I'm trying to understand this simpler problem so I can apply the process to a more difficult homework problem. Let $U$ be a set with $n$ elements. Select $2$ independent random subsets $A_1, A_2 \subset U.$ Both $A_i$ are chosen so that all $2^n$ choices are equally likely. I would like to compute the probability that $A_1, A_2$ are disjoint. I am looking for a calculation based on counting. I have no idea how to compute this, and I would appreciate any help given.","I'm trying to understand this simpler problem so I can apply the process to a more difficult homework problem. Let $U$ be a set with $n$ elements. Select $2$ independent random subsets $A_1, A_2 \subset U.$ Both $A_i$ are chosen so that all $2^n$ choices are equally likely. I would like to compute the probability that $A_1, A_2$ are disjoint. I am looking for a calculation based on counting. I have no idea how to compute this, and I would appreciate any help given.",,"['probability', 'probability-theory']"
25,Is the limit of càdlàg functions càdlàg?,Is the limit of càdlàg functions càdlàg?,,Is the pointwise limit of càdlàg functions càdlàg? If not which are the weaker conditions to assure it? I cannot find a counterexample,Is the pointwise limit of càdlàg functions càdlàg? If not which are the weaker conditions to assure it? I cannot find a counterexample,,"['real-analysis', 'probability']"
26,Uncountable increasing family of $\sigma$-algebras,Uncountable increasing family of -algebras,\sigma,"Could someone give an example of what an uncountable increasing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t\geq 0}$, $(\mathcal{F}_s \subset \mathcal{F}_t$ for $s<t)$ might look like? For the discrete parameter case, I always have in mind the filtration induced on $([0,1),\mathbf{B}_{[0,1)})$ by the sequence of independent random variables $(X_n)_{n \geq 1}$ where $X_k(\omega) = \omega_k$ for $\omega = 0.\omega_1\omega_2\omega_3...$ represented in binary system. For a given $n$, $\mathcal{F}_n = \sigma(X_1,X_2,...,X_n)$ is just the $\sigma$-algebra whose atoms are the dyadic intervals $[k/2^n, (k+1)/2^n)$ for $0 \leq k \leq 2^n-1$. As $n$ increases, $\mathcal{F}_n$ gets finer and finer and ultimately ""converges"" to $\mathcal{F}=\mathbf{B}_{[0,1)}$. Are there any explicit examples in the continuous parameter case ?","Could someone give an example of what an uncountable increasing family of $\sigma$-algebras $\{\mathcal{F}_t\}_{t\geq 0}$, $(\mathcal{F}_s \subset \mathcal{F}_t$ for $s<t)$ might look like? For the discrete parameter case, I always have in mind the filtration induced on $([0,1),\mathbf{B}_{[0,1)})$ by the sequence of independent random variables $(X_n)_{n \geq 1}$ where $X_k(\omega) = \omega_k$ for $\omega = 0.\omega_1\omega_2\omega_3...$ represented in binary system. For a given $n$, $\mathcal{F}_n = \sigma(X_1,X_2,...,X_n)$ is just the $\sigma$-algebra whose atoms are the dyadic intervals $[k/2^n, (k+1)/2^n)$ for $0 \leq k \leq 2^n-1$. As $n$ increases, $\mathcal{F}_n$ gets finer and finer and ultimately ""converges"" to $\mathcal{F}=\mathbf{B}_{[0,1)}$. Are there any explicit examples in the continuous parameter case ?",,"['probability', 'probability-theory', 'measure-theory', 'stochastic-processes', 'stochastic-calculus']"
27,How to find median from a probability distribution?,How to find median from a probability distribution?,,"Having trouble on something that should be really, really easy.  I need to find the median of the following probability distribution...but according to the website I linked below...I'm doing it incorrectly. Anyways, I computed the following probability table along with its mean and variance. x    0        1        2        3        4 P(x) 0.728303 0.240297 0.029732 0.001635 0.000034 Mean = .304802 Variance = .28158 Assuming I got this much right...do I just rearrange the probabilities in ascending order and choose the value in the middle (i.e. like every other time I've ever found the median)?  If so, why does the following link give 2 and not 1 for the median from the distribution below? Number of hits, x   0       1       2       3       4 Probability, P(x)   0.10    0.20    0.30    0.25    0.15  Arranged in ascending order... Number of hits, x   0       4       1       3       2 Probability, P(x)   0.10    0.15    0.20    0.25    0.30 I chose that the median should be 1...but I guess the median is 2 according to the website.  Why doesn't my method work? http://stattrek.com/random-variable/mean-variance.aspx?Tutorial=AP What are the proper steps to finding the median in the first probability distribution given above?","Having trouble on something that should be really, really easy.  I need to find the median of the following probability distribution...but according to the website I linked below...I'm doing it incorrectly. Anyways, I computed the following probability table along with its mean and variance. x    0        1        2        3        4 P(x) 0.728303 0.240297 0.029732 0.001635 0.000034 Mean = .304802 Variance = .28158 Assuming I got this much right...do I just rearrange the probabilities in ascending order and choose the value in the middle (i.e. like every other time I've ever found the median)?  If so, why does the following link give 2 and not 1 for the median from the distribution below? Number of hits, x   0       1       2       3       4 Probability, P(x)   0.10    0.20    0.30    0.25    0.15  Arranged in ascending order... Number of hits, x   0       4       1       3       2 Probability, P(x)   0.10    0.15    0.20    0.25    0.30 I chose that the median should be 1...but I guess the median is 2 according to the website.  Why doesn't my method work? http://stattrek.com/random-variable/mean-variance.aspx?Tutorial=AP What are the proper steps to finding the median in the first probability distribution given above?",,"['probability', 'statistics', 'probability-distributions', 'median']"
28,Conditional Probability Cupcakes,Conditional Probability Cupcakes,,"This is a very interesting word problem that I came across in an old textbook of mine. So I know its got something to do with conditional probability, which yields the shortest, simplest proofs, but other than that, the textbook gave no hints really and I'm really not sure about how to approach it. Any guidance hints or help would be truly greatly appreciated. Thanks in advance :) So anyway, here the problem goes: Two boxes each contain $4$ cupcakes. One box has $3$ chocolate and $1$ vanilla, and the other box has $2$ chocolate and $2$ vanilla. A box is randomly selected, opened, and a cupcake is randomly selected. This first cupcake is vanilla. If one more cupcake is randomly selected from the same box, what is the probability that it will be vanilla ? My working so far: If we pick a box at random, and pick a vanilla cupcake, then there is a $\frac23$ probability we picked the second box. ( $2$ chances for having picked vanilla from there, against $1$ from $1$ ), and $\frac13$ for the first box. If we DID pick the first box, the chances of $2nd$ vanilla are $0$ , since there isn't one, so $\frac13 × 0$ for that. Now I am stuck.","This is a very interesting word problem that I came across in an old textbook of mine. So I know its got something to do with conditional probability, which yields the shortest, simplest proofs, but other than that, the textbook gave no hints really and I'm really not sure about how to approach it. Any guidance hints or help would be truly greatly appreciated. Thanks in advance :) So anyway, here the problem goes: Two boxes each contain cupcakes. One box has chocolate and vanilla, and the other box has chocolate and vanilla. A box is randomly selected, opened, and a cupcake is randomly selected. This first cupcake is vanilla. If one more cupcake is randomly selected from the same box, what is the probability that it will be vanilla ? My working so far: If we pick a box at random, and pick a vanilla cupcake, then there is a probability we picked the second box. ( chances for having picked vanilla from there, against from ), and for the first box. If we DID pick the first box, the chances of vanilla are , since there isn't one, so for that. Now I am stuck.",4 3 1 2 2 \frac23 2 1 1 \frac13 2nd 0 \frac13 × 0,"['probability', 'probability-theory']"
29,Explain a couple steps in proof that ${n \choose r} = {n-1 \choose r-1} + {n-1 \choose r}$,Explain a couple steps in proof that,{n \choose r} = {n-1 \choose r-1} + {n-1 \choose r},"Show  ${n \choose r}$ = ${n-1 \choose r-1}$ + ${n-1 \choose r}$ I found a similar question on here but I am looking for a little bit more of an explanation on how they simplified Right Hand Side $$= \frac{(n-1)!}{(r-1)!(n-r)!} + \frac{(n-1)!}{r!(n-r-1)!}$$                 $$= \frac{r(n-1)!+(n-r)(n-1)!}{r!(n-r)!}$$                $$= \frac{n!}{r!(n-r)!}$$                $$= \binom{n}{r}$$, the first equality because of definition ,the second equality because of summing fractions the third because of $n(n-1)!=n!$, the fourth by definition. I would like more of an explanation on equality 2 and equality 3.  I understand that they are summing fractions but I am interested in how they came up with the common denominator and then simplified.","Show  ${n \choose r}$ = ${n-1 \choose r-1}$ + ${n-1 \choose r}$ I found a similar question on here but I am looking for a little bit more of an explanation on how they simplified Right Hand Side $$= \frac{(n-1)!}{(r-1)!(n-r)!} + \frac{(n-1)!}{r!(n-r-1)!}$$                 $$= \frac{r(n-1)!+(n-r)(n-1)!}{r!(n-r)!}$$                $$= \frac{n!}{r!(n-r)!}$$                $$= \binom{n}{r}$$, the first equality because of definition ,the second equality because of summing fractions the third because of $n(n-1)!=n!$, the fourth by definition. I would like more of an explanation on equality 2 and equality 3.  I understand that they are summing fractions but I am interested in how they came up with the common denominator and then simplified.",,['probability']
30,Prove that a martingale bounded in $L_2$ converges almost surely [closed],Prove that a martingale bounded in  converges almost surely [closed],L_2,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Hi as the topic states prove that a martingale bounded in $L_2$ converges almost surely.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Hi as the topic states prove that a martingale bounded in $L_2$ converges almost surely.",,['probability']
31,Show the probability that the sum of these numbers is odd is 1/2,Show the probability that the sum of these numbers is odd is 1/2,,"Setting Let $S$ be a set of integers where at least one of the integers is odd. Suppose we pick a random subset $T$ of $S$ by including each element of $S$ independently with probability $1/2$, Show that $$\text{Pr}\Big[\Big(\sum_{i \in T} i\Big) = \text{odd}\Big] = \frac{1}{2}$$ Solution given Let $x$ be an odd integer in $T$. Then for every subset $S \subseteq T - \{x\}$, exactly one of $S$ and $S \cup \{x\}$ has an odd total, and both are subsets of $T$. Thus exactly half of the subsets have an odd total. Problem The solution makes very little sense to me. If someone has an alternate solution, or could pose the solution in a more explicable manner, it'd be really nice.","Setting Let $S$ be a set of integers where at least one of the integers is odd. Suppose we pick a random subset $T$ of $S$ by including each element of $S$ independently with probability $1/2$, Show that $$\text{Pr}\Big[\Big(\sum_{i \in T} i\Big) = \text{odd}\Big] = \frac{1}{2}$$ Solution given Let $x$ be an odd integer in $T$. Then for every subset $S \subseteq T - \{x\}$, exactly one of $S$ and $S \cup \{x\}$ has an odd total, and both are subsets of $T$. Thus exactly half of the subsets have an odd total. Problem The solution makes very little sense to me. If someone has an alternate solution, or could pose the solution in a more explicable manner, it'd be really nice.",,"['probability', 'algorithms', 'random-variables', 'algorithmic-randomness']"
32,"From the set $\{1,2,\ldots,n\}$ two numbers are chosen uniformly, with replacement. Find the probability that the product of the numbers is even.","From the set  two numbers are chosen uniformly, with replacement. Find the probability that the product of the numbers is even.","\{1,2,\ldots,n\}","The answer is given, just the reasoning is unclear. Result: $${2\over n}\left[{n\over 2}\right]-\left({\left\lfloor{n \over 2}\right\rfloor\over n}\right)^2,\text{ where $\lfloor\cdot\rfloor$ is the whole part of a number}.$$ The second part of the question is: The probability that the sum of the squares of the numbers is even: again the answer: $$1-{2\over n}\left\lfloor{n\over 2}\right\rfloor+{2\over n^2}\left\lfloor{n \over 2}\right\rfloor^2, \text{where $\lfloor\cdot\rfloor$ is the whole part of the number}.$$","The answer is given, just the reasoning is unclear. Result: $${2\over n}\left[{n\over 2}\right]-\left({\left\lfloor{n \over 2}\right\rfloor\over n}\right)^2,\text{ where $\lfloor\cdot\rfloor$ is the whole part of a number}.$$ The second part of the question is: The probability that the sum of the squares of the numbers is even: again the answer: $$1-{2\over n}\left\lfloor{n\over 2}\right\rfloor+{2\over n^2}\left\lfloor{n \over 2}\right\rfloor^2, \text{where $\lfloor\cdot\rfloor$ is the whole part of the number}.$$",,"['probability', 'probability-theory']"
33,Optimal stopping in coin tossing with finite horizon,Optimal stopping in coin tossing with finite horizon,,"There's a classic coin toss problem that asks about optimal stopping. The setup is you keep flipping a coin until you decide to stop, and when you stop you get paid $H/n%$ where $H$ is the number of heads you flipped and $n$ is the number of times you flipped. I believe that this problem is unsolved for an infinite horizon. If we limit the horizon to allow $t$ tosses at maximum, what's the optimal stopping strategy?","There's a classic coin toss problem that asks about optimal stopping. The setup is you keep flipping a coin until you decide to stop, and when you stop you get paid $H/n%$ where $H$ is the number of heads you flipped and $n$ is the number of times you flipped. I believe that this problem is unsolved for an infinite horizon. If we limit the horizon to allow $t$ tosses at maximum, what's the optimal stopping strategy?",,"['probability', 'stopping-times']"
34,How to integrate: $\int_{0}^{\infty}e^{tx}(x^2e^{-x})/2dx$,How to integrate:,\int_{0}^{\infty}e^{tx}(x^2e^{-x})/2dx,"I'm working on a few moment generating function problems and I came across: $f(x)=(x^2e^{-x})/2$ for $x>0$, and zero otherwise. Find the mean. The mean is the same as the expected value. If we find the moment generating function, $M_x(t)$, of $f(x)$ then we can take the first derivative of $M_x(t)$ at $t=0$. This will give us the mean. To find the $M_x(t)$ we take $$\int_{-\infty}^{\infty}e^{tx}f(x)dx$$ $$\int_{0}^{\infty}e^{tx}(x^2e^{-x})/2dx$$ I wrote this as: $$\frac12\int_{0}^{\infty}x^{2}e^{x(t-1)}dx$$ I'm a bit rusty on integration and if someone could help point me in the right direction as to how to tackle this guy I would greatly appreciate it!","I'm working on a few moment generating function problems and I came across: $f(x)=(x^2e^{-x})/2$ for $x>0$, and zero otherwise. Find the mean. The mean is the same as the expected value. If we find the moment generating function, $M_x(t)$, of $f(x)$ then we can take the first derivative of $M_x(t)$ at $t=0$. This will give us the mean. To find the $M_x(t)$ we take $$\int_{-\infty}^{\infty}e^{tx}f(x)dx$$ $$\int_{0}^{\infty}e^{tx}(x^2e^{-x})/2dx$$ I wrote this as: $$\frac12\int_{0}^{\infty}x^{2}e^{x(t-1)}dx$$ I'm a bit rusty on integration and if someone could help point me in the right direction as to how to tackle this guy I would greatly appreciate it!",,"['calculus', 'probability', 'integration', 'moment-generating-functions']"
35,Multivariate normal distribution independet iff uncorrelated,Multivariate normal distribution independet iff uncorrelated,,"I found a few threads about this but none of them answered my question. I am supposed to show that if you have random variables $X_1$,$X_2$ that are gaussian distributed and they fulfill that $$E(X_1)E(X_2) =E(X_1 X_2)$$ which is our definition of being uncorrelated, then they are also independent. Most answers use that in that case the matrix in a multivariate normal distribution has to be diagonal, but I don't see the relationship between $$E(X_1)E(X_2) =E(X_1 X_2)$$ and this diagonal fact? By the way: I know that this would solve the problem, as in that case we would have that $f_{(X_1,X_2)} = f_{X_1}f_{X_2}$ which would mean that they are independent, but I still don't understand this first thing.","I found a few threads about this but none of them answered my question. I am supposed to show that if you have random variables $X_1$,$X_2$ that are gaussian distributed and they fulfill that $$E(X_1)E(X_2) =E(X_1 X_2)$$ which is our definition of being uncorrelated, then they are also independent. Most answers use that in that case the matrix in a multivariate normal distribution has to be diagonal, but I don't see the relationship between $$E(X_1)E(X_2) =E(X_1 X_2)$$ and this diagonal fact? By the way: I know that this would solve the problem, as in that case we would have that $f_{(X_1,X_2)} = f_{X_1}f_{X_2}$ which would mean that they are independent, but I still don't understand this first thing.",,"['real-analysis', 'probability']"
36,Probability of a Min/Max,Probability of a Min/Max,,"I am studying probability for an exam and I am finding hard to understand the notion of $P(\min(X_1,X_2))$ and $P(\max(X_1,X_2))$, where $X$ is a discrete or a continuous variable. I have found in my book that $P(\min(X_1,X_2)>t) = P(X_1>t)\cdot P(X_2>t)$ and $P(\max(X_1,X_2)\leq t) = P(X_1\leq t)\cdot P(X_2\leq t)$. Could you explain (or link) me what is the theory behind it? Thank you very much","I am studying probability for an exam and I am finding hard to understand the notion of $P(\min(X_1,X_2))$ and $P(\max(X_1,X_2))$, where $X$ is a discrete or a continuous variable. I have found in my book that $P(\min(X_1,X_2)>t) = P(X_1>t)\cdot P(X_2>t)$ and $P(\max(X_1,X_2)\leq t) = P(X_1\leq t)\cdot P(X_2\leq t)$. Could you explain (or link) me what is the theory behind it? Thank you very much",,"['probability', 'probability-distributions', 'notation', 'terminology']"
37,Great Monty Hall application in real life?,Great Monty Hall application in real life?,,"Suppose you are doing a multiple choice question with 4 different answers you have no ideas about.  You mentally choose one (say A), and as you are about to write that down... you suddenly remember 2 answers that are wrong! And it happens that none of those 2 answers are the one you mentally chose (suppose B and C). Now, does switching your answer to D increase your chance from 25% to 75%?","Suppose you are doing a multiple choice question with 4 different answers you have no ideas about.  You mentally choose one (say A), and as you are about to write that down... you suddenly remember 2 answers that are wrong! And it happens that none of those 2 answers are the one you mentally chose (suppose B and C). Now, does switching your answer to D increase your chance from 25% to 75%?",,"['probability', 'probability-theory', 'probability-distributions', 'monty-hall']"
38,Joint distribution of $U = X + Y$ and $V = X - Y$,Joint distribution of  and,U = X + Y V = X - Y,"I have two independent continuous random variables, $X$ and $Y$, which are uniformly distributed over the interval $[0,1]$. From this I have two further random variables, $U$ and $V$, which are defined as $U = X + Y$, and $V = X - Y$. I am trying to figure out the density function for the joint probability distribution of $(U,V)$ but am struggling. I have calculated the density functions of U and V on their own, but do not think it makes sense to simply multiply these together, as I do not think U and V are independent. Is this assumption correct? My university lecturer suggested sketching out the range of values over which $(U,V)$ is defined, which seems to suggest he is trying to lead me towards a more intuitive solution, but I would appreciate any explanation (analytical or otherwise) which would help me understand how to solve problems such as these.","I have two independent continuous random variables, $X$ and $Y$, which are uniformly distributed over the interval $[0,1]$. From this I have two further random variables, $U$ and $V$, which are defined as $U = X + Y$, and $V = X - Y$. I am trying to figure out the density function for the joint probability distribution of $(U,V)$ but am struggling. I have calculated the density functions of U and V on their own, but do not think it makes sense to simply multiply these together, as I do not think U and V are independent. Is this assumption correct? My university lecturer suggested sketching out the range of values over which $(U,V)$ is defined, which seems to suggest he is trying to lead me towards a more intuitive solution, but I would appreciate any explanation (analytical or otherwise) which would help me understand how to solve problems such as these.",,"['probability', 'integration', 'probability-distributions', 'random-variables']"
39,Variance of n Bernoulli Trials,Variance of n Bernoulli Trials,,"Count the variance of n Bernoulli trials with each probability of success is p. Let random variable $X_i$ be $1$ if trial is success, or $0$ if trial fails. Then expected value $E(X_i) = 1 \times p + 0 \times (1 - p) = p$. By linearity of expectation $E(X) = p_1 + p_2 + ... + p_n = np$. To count the variance, I use this formula $V(X) = E(X^2) - E(X)^2$. where $E(X_i^2) = 1^2 \times p + 0^2 \times (1 - p) = p$ then, $E(X^2) = p_1 + p_2 + ... + p_n = np$. So, I got variance $V(X) = np - (np)^2 = np(1 - np)$. But in wiki ,  it says that the correct variance is $np(1 - p)$. Where did I do wrong? Thanks a lot for the help.","Count the variance of n Bernoulli trials with each probability of success is p. Let random variable $X_i$ be $1$ if trial is success, or $0$ if trial fails. Then expected value $E(X_i) = 1 \times p + 0 \times (1 - p) = p$. By linearity of expectation $E(X) = p_1 + p_2 + ... + p_n = np$. To count the variance, I use this formula $V(X) = E(X^2) - E(X)^2$. where $E(X_i^2) = 1^2 \times p + 0^2 \times (1 - p) = p$ then, $E(X^2) = p_1 + p_2 + ... + p_n = np$. So, I got variance $V(X) = np - (np)^2 = np(1 - np)$. But in wiki ,  it says that the correct variance is $np(1 - p)$. Where did I do wrong? Thanks a lot for the help.",,"['probability', 'discrete-mathematics']"
40,Birthday Problem applied to collisions,Birthday Problem applied to collisions,,"I'm trying to extend the birthday problem to detect collision probability in a hashing scheme. Here is my problem. I use the letters and numbers [A-Z][a-z][0-9] to make a set of keys by randomly choosing from that set 10 times and concatenating them all together. So A24cv30kf2 could be one key for example. This implies that the base set has $62^{10}$ possible keys. Next, I want to compute the probability that I would see a collision if I picked a key from that set and issued it to $n$ users. If $M$ represents the size of the set (= $62^{10}$) and $P(M,n)$ the probability then $$P(M,n) = \frac{M!}{M^{n}(M-n)!}$$ This follows by replacing 365 in the well known birthday problem shown here What I want to understand is the following: I want to check and see if my choice of concatenating 10 is right, when I know that my $n$ is of the order of a few millions and my $1 - P(M,n)$ needs to be less than 0.0001 or some small number. The large values of M & n make the above intractable to compute. Is there an easier way to compute this?","I'm trying to extend the birthday problem to detect collision probability in a hashing scheme. Here is my problem. I use the letters and numbers [A-Z][a-z][0-9] to make a set of keys by randomly choosing from that set 10 times and concatenating them all together. So A24cv30kf2 could be one key for example. This implies that the base set has $62^{10}$ possible keys. Next, I want to compute the probability that I would see a collision if I picked a key from that set and issued it to $n$ users. If $M$ represents the size of the set (= $62^{10}$) and $P(M,n)$ the probability then $$P(M,n) = \frac{M!}{M^{n}(M-n)!}$$ This follows by replacing 365 in the well known birthday problem shown here What I want to understand is the following: I want to check and see if my choice of concatenating 10 is right, when I know that my $n$ is of the order of a few millions and my $1 - P(M,n)$ needs to be less than 0.0001 or some small number. The large values of M & n make the above intractable to compute. Is there an easier way to compute this?",,"['probability', 'hash-function', 'birthday']"
41,(Probability Space) Shouldn't $\mathcal{F}$ always equal the power set of $\Omega$?,(Probability Space) Shouldn't  always equal the power set of ?,\mathcal{F} \Omega,"This is from the wikipedia article about Probability Space: A probability space consists of three parts: 1- A sample space, $\Omega$, which is the set of all possible outcomes. 2- A set of events $\mathcal{F}$, where each event is a set containing zero or more outcomes. 3- The assignment of probabilities to the events, that is, a function $P$ from events to probability levels. I can not think of a case where $\mathcal{F}$ is not equal to the power set of $\Omega$. What is the purpose of the second part in this definition then?","This is from the wikipedia article about Probability Space: A probability space consists of three parts: 1- A sample space, $\Omega$, which is the set of all possible outcomes. 2- A set of events $\mathcal{F}$, where each event is a set containing zero or more outcomes. 3- The assignment of probabilities to the events, that is, a function $P$ from events to probability levels. I can not think of a case where $\mathcal{F}$ is not equal to the power set of $\Omega$. What is the purpose of the second part in this definition then?",,"['probability', 'probability-theory']"
42,Probability of at least two events occurring.,Probability of at least two events occurring.,,"The proportion of the American adult population that supports candidate Green is p=0.22.  A SRS of 9 adults asks if they agree with the statement “I support candidate Green.”  What is the probability that at least 2 of those surveyed would agree with that statement? Ok, so far I know that the probability of at least 1 person would be .8931 -edited- was using .88 instead .78- Is that useful in solving for at least two?","The proportion of the American adult population that supports candidate Green is p=0.22.  A SRS of 9 adults asks if they agree with the statement “I support candidate Green.”  What is the probability that at least 2 of those surveyed would agree with that statement? Ok, so far I know that the probability of at least 1 person would be .8931 -edited- was using .88 instead .78- Is that useful in solving for at least two?",,['probability']
43,Where does this equality about expectation of random variables come from?,Where does this equality about expectation of random variables come from?,,"In order to prove this Lemma in my course about Probability : Let $X=(X_1,\dots ,X_p)$ be a gaussian random variable such that $\mathbb{E}[X_j]=0$ for all $j=1,\dots,p$. Then $2\mathbb{E}[\sum_{j=1}^pX_j^2] \le [\mathbb{E}[e^{-\sum_{j=1}^p X_j^2}]]^{-2}$ the teacher suppose that $X$ has a normal law i.e. $X\sim N_p(0,\Lambda)$ where $\Lambda$ is a diagonal matrix with $\lambda_{jj}=\sigma_j^2$, i.e $\Lambda=\left( \begin{array}{ccc} \lambda_{11} &  \dots & & 0 \\ 0 & \lambda_{22}  & \dots &0\\ \vdots & \vdots & \ddots & \vdots\\ 0&0&0&\lambda_{nn} \end{array} \right) = \left( \begin{array}{ccc} \sigma_1^2 &  \dots & & 0 \\ 0 & \sigma_2^2 &  \dots &0\\ \vdots & \vdots & \ddots & \vdots\\ 0&0&0&\sigma_p^2 \end{array} \right)$ Then he wrote, and this is where I get lost , $\mathbb{E}[e^{-X_j^2}]=(1+2\sigma_j^2)^{-1/2}$. Where does this result come from ?","In order to prove this Lemma in my course about Probability : Let $X=(X_1,\dots ,X_p)$ be a gaussian random variable such that $\mathbb{E}[X_j]=0$ for all $j=1,\dots,p$. Then $2\mathbb{E}[\sum_{j=1}^pX_j^2] \le [\mathbb{E}[e^{-\sum_{j=1}^p X_j^2}]]^{-2}$ the teacher suppose that $X$ has a normal law i.e. $X\sim N_p(0,\Lambda)$ where $\Lambda$ is a diagonal matrix with $\lambda_{jj}=\sigma_j^2$, i.e $\Lambda=\left( \begin{array}{ccc} \lambda_{11} &  \dots & & 0 \\ 0 & \lambda_{22}  & \dots &0\\ \vdots & \vdots & \ddots & \vdots\\ 0&0&0&\lambda_{nn} \end{array} \right) = \left( \begin{array}{ccc} \sigma_1^2 &  \dots & & 0 \\ 0 & \sigma_2^2 &  \dots &0\\ \vdots & \vdots & \ddots & \vdots\\ 0&0&0&\sigma_p^2 \end{array} \right)$ Then he wrote, and this is where I get lost , $\mathbb{E}[e^{-X_j^2}]=(1+2\sigma_j^2)^{-1/2}$. Where does this result come from ?",,"['probability', 'random-variables']"
44,Find the probability of three tosses of a fair coin,Find the probability of three tosses of a fair coin,,"Find the probability that, in three tosses of a fair coin, there are three heads, given that   there is at least one head. I manage to get $\frac{3}{6}$ or $\frac{1}{6}$ but the right answer is $\frac{1}{7}$ I have no idea, Can you please explain? thanks! Appraciate it!","Find the probability that, in three tosses of a fair coin, there are three heads, given that   there is at least one head. I manage to get $\frac{3}{6}$ or $\frac{1}{6}$ but the right answer is $\frac{1}{7}$ I have no idea, Can you please explain? thanks! Appraciate it!",,['probability']
45,Probability of getting two consecutive 7s without getting a 6 when two dice are rolled,Probability of getting two consecutive 7s without getting a 6 when two dice are rolled,,"Two dice are rolled at a time, for many time until either A or B wins. A wins if we get two consecutive 7s and B wins if we get one 6 at any time. what is the probability of A winning the game??","Two dice are rolled at a time, for many time until either A or B wins. A wins if we get two consecutive 7s and B wins if we get one 6 at any time. what is the probability of A winning the game??",,"['probability', 'dice']"
46,Coupon collector problem for collecting set k times.,Coupon collector problem for collecting set k times.,,"Recently I tried to solve a problem that based on coupons collector problem. Let it be sth like this: If a package has one of 50 random baseball cards, how many packages do you need to buy to get a complete set? (or sth like this, doesnt matter) If I need every card one time, so it makes one set that is easy, explaination is on wikipedia and I already understood it. But what if we consider to collect every card k times? (To collect k sets of cards) How can I tried to solve this problem? I found sth about Chernoff's bound (http://www.math.ucla.edu/~pak/courses/pg/l10.pdf) but I dont get it actually if it is a solution of this problem. I need to estimate E(X) and Var(X) for k sets of cards. Could anyone give me a hint how to solve this problem? Thanks for all answers:) Greetings,","Recently I tried to solve a problem that based on coupons collector problem. Let it be sth like this: If a package has one of 50 random baseball cards, how many packages do you need to buy to get a complete set? (or sth like this, doesnt matter) If I need every card one time, so it makes one set that is easy, explaination is on wikipedia and I already understood it. But what if we consider to collect every card k times? (To collect k sets of cards) How can I tried to solve this problem? I found sth about Chernoff's bound (http://www.math.ucla.edu/~pak/courses/pg/l10.pdf) but I dont get it actually if it is a solution of this problem. I need to estimate E(X) and Var(X) for k sets of cards. Could anyone give me a hint how to solve this problem? Thanks for all answers:) Greetings,",,"['probability', 'combinatorics', 'coupon-collector']"
47,Countable vs. Uncountable Probability,Countable vs. Uncountable Probability,,"In Apostol's Calculus II, he splits the calculus of probability into two chapters: Finite and Countable sets Uncountable sets He only remarks that explaining why different tools are needed for uncountable sets ""would take us too far afield."" I have an intuition for why a probability measure defined over a finite set would have different requirements than one defined over an infinite set, but I'm not really sure why uncountable sets are so different. I suspect it may have something to do with the first theorem of the chapter (if $S$ is an uncountable set, there are an most a countable number of $s\in S$ where $P(s)\not=0$), but I'm not sure what exactly. A search online doesn't seem to turn up much, so maybe this is just a decision the author made to separate them that others don't agree with? More likely I just don't know the terms to search for.","In Apostol's Calculus II, he splits the calculus of probability into two chapters: Finite and Countable sets Uncountable sets He only remarks that explaining why different tools are needed for uncountable sets ""would take us too far afield."" I have an intuition for why a probability measure defined over a finite set would have different requirements than one defined over an infinite set, but I'm not really sure why uncountable sets are so different. I suspect it may have something to do with the first theorem of the chapter (if $S$ is an uncountable set, there are an most a countable number of $s\in S$ where $P(s)\not=0$), but I'm not sure what exactly. A search online doesn't seem to turn up much, so maybe this is just a decision the author made to separate them that others don't agree with? More likely I just don't know the terms to search for.",,"['probability', 'probability-theory']"
48,Is it true that $\sum_{k=1}^n(p_k\prod_{i=1}^k(1-p_i)) \stackrel{\mbox{?}}{=} 1 - \prod_{i=1}^n(1-p_i)$,Is it true that,\sum_{k=1}^n(p_k\prod_{i=1}^k(1-p_i)) \stackrel{\mbox{?}}{=} 1 - \prod_{i=1}^n(1-p_i),"Prove that $$ p_1 + \sum_{k=2}^n \left(p_k\prod_{i=1}^{k-1}(1-p_i)\right) = 1 - \prod_{k=1}^n(1-p_k)\ .  $$ I'm working in a code where I have to do those computations. I want to see if this equality holds in general (in order to save a lot of computation time). I tested it in practice and it seems that it is true but having the general proof would be great. Thank you.","Prove that $$ p_1 + \sum_{k=2}^n \left(p_k\prod_{i=1}^{k-1}(1-p_i)\right) = 1 - \prod_{k=1}^n(1-p_k)\ .  $$ I'm working in a code where I have to do those computations. I want to see if this equality holds in general (in order to save a lot of computation time). I tested it in practice and it seems that it is true but having the general proof would be great. Thank you.",,"['probability', 'discrete-mathematics', 'arithmetic']"
49,Generalization to $N$ dimensions of distribution function evaluation over an hyper- rectangle,Generalization to  dimensions of distribution function evaluation over an hyper- rectangle,N,"Let's consider an absolutely continuous random vector $V \equiv (X,Y)$ and its associated joint distribution function $F(x,y)=Pr(X \le x,Y \le y) = \int_{-\infty}^{x}\int_{-\infty}^{y} f(x,y)\,dx\,dy$ . If we take four points in the $xy$ plane that are vertexes of a rectangle $R$ , $A \equiv (x_1,y_1) \;, B \equiv (x_1,y_0) \;,C \equiv (x_0,y_0) \;,A \equiv (x_0,y_1)$ , with $x_1 > x_0$ and $y_1>y_0$ , it is well known that the  probability that the values of the random vector $V$ are within the rectangle $R$ is given by the value of the distribution function $F(x,y)$ taken at those points according to the below formula: $Pr(x_0<X \le x_1,y_0<Y \le y_1)=F(x_1,y_1)-F(x_1,y_0)+F(x_0,y_0)-F(x_0,y_1)$ Is there an explicit formula, generalizing the one above, that applies when we move from $2$ to $N$ dimensions? In other words, given the distribution function $F(x_1,...,x_N)=Pr(X1 \le x1,\ldots,X_N \le x_N)$ , it is there a formula that allows to compute $Pr(a_1<X_1 \le b_1,\ldots,a_N<X_N \le b_N)$ by the values of the N-dimensional distribution function computed in the vertexes of the hyper-rectangle $[a_1,b_1]\times\ldots\times[a_N,b_N]$ ? What happens if values $a_i$ or $b_j$ are allowed to be $+\infty$ ? In two dimensions we find the value of the $1$ -dimensional marginal distributions $F_i(x)$ , what's found in the $N$ -dimensional case? From computational point of view, is this formula applicable in practice for value of $N$ equal to $10$ ? I suppose it involves $2^{10}$ vertexes....","Let's consider an absolutely continuous random vector and its associated joint distribution function . If we take four points in the plane that are vertexes of a rectangle , , with and , it is well known that the  probability that the values of the random vector are within the rectangle is given by the value of the distribution function taken at those points according to the below formula: Is there an explicit formula, generalizing the one above, that applies when we move from to dimensions? In other words, given the distribution function , it is there a formula that allows to compute by the values of the N-dimensional distribution function computed in the vertexes of the hyper-rectangle ? What happens if values or are allowed to be ? In two dimensions we find the value of the -dimensional marginal distributions , what's found in the -dimensional case? From computational point of view, is this formula applicable in practice for value of equal to ? I suppose it involves vertexes....","V \equiv (X,Y) F(x,y)=Pr(X \le x,Y \le y) = \int_{-\infty}^{x}\int_{-\infty}^{y} f(x,y)\,dx\,dy xy R A \equiv (x_1,y_1) \;, B \equiv (x_1,y_0) \;,C \equiv (x_0,y_0) \;,A \equiv (x_0,y_1) x_1 > x_0 y_1>y_0 V R F(x,y) Pr(x_0<X \le x_1,y_0<Y \le y_1)=F(x_1,y_1)-F(x_1,y_0)+F(x_0,y_0)-F(x_0,y_1) 2 N F(x_1,...,x_N)=Pr(X1 \le x1,\ldots,X_N \le x_N) Pr(a_1<X_1 \le b_1,\ldots,a_N<X_N \le b_N) [a_1,b_1]\times\ldots\times[a_N,b_N] a_i b_j +\infty 1 F_i(x) N N 10 2^{10}",['probability']
50,Extending the Classical Birthday Problem,Extending the Classical Birthday Problem,,"I'm reading through some notes our Probability lecturer has uploaded, and he leaves a rather interesting variation on the classical Birthday Problem as an exercise for 'the adventurous Probabilist - with too much time on their hands'. We let $B_k$ be the number of groups of $k$ individuals who all have the same birthday. From the phrasing of the first part of the problem, I feel he intends for the exercise to allow groups to overlap, i.e. if we have 3 people with birthday Jan. 1 and no other matches, then $B_3 = 1$, $B_2 = 3$, and $B_k = 0$ for $k > 3$. The classical problem determines a value for $\mathbb{P} \{B_2>0\}$ and to find the least $n$ such that the probability of two people in a population of size $n$ sharing a birthday is greater that $\frac{1}{2}$. Through a standard combinatorial approach I've identified $n=23$. Problem We are asked now to determine $\mathbb{E}[B_2]$, $\text{Var}[B_2]$, $\mathbb{E}[B_3]$, and $\text{Var}[B_3]$. I think this is where a combinatorial approach breaks down, though I'm not sure. I also wondered (beyond the realms of this exercise, but it seems an interesting problem - one which stumps me entirely), how we'd find the least $n$ such that the probability of three people in a population of size $n$ sharing a birthday is greater that $\frac{1}{2}$. I feel this could be approached using Chebyshev’s inequality though I'm yet to conclude how this could be applied successfully. The lecturer's exercise also gives rise naturally to the question on how we could determine $\mathbb{E}[B_k]$, $\text{Var}[B_k]$ for general $k$. This seems like it could be quite a cumbersome exercise in combinatorics. I would be very appreciative of anyone who can help with this exercise. Best, MM.","I'm reading through some notes our Probability lecturer has uploaded, and he leaves a rather interesting variation on the classical Birthday Problem as an exercise for 'the adventurous Probabilist - with too much time on their hands'. We let $B_k$ be the number of groups of $k$ individuals who all have the same birthday. From the phrasing of the first part of the problem, I feel he intends for the exercise to allow groups to overlap, i.e. if we have 3 people with birthday Jan. 1 and no other matches, then $B_3 = 1$, $B_2 = 3$, and $B_k = 0$ for $k > 3$. The classical problem determines a value for $\mathbb{P} \{B_2>0\}$ and to find the least $n$ such that the probability of two people in a population of size $n$ sharing a birthday is greater that $\frac{1}{2}$. Through a standard combinatorial approach I've identified $n=23$. Problem We are asked now to determine $\mathbb{E}[B_2]$, $\text{Var}[B_2]$, $\mathbb{E}[B_3]$, and $\text{Var}[B_3]$. I think this is where a combinatorial approach breaks down, though I'm not sure. I also wondered (beyond the realms of this exercise, but it seems an interesting problem - one which stumps me entirely), how we'd find the least $n$ such that the probability of three people in a population of size $n$ sharing a birthday is greater that $\frac{1}{2}$. I feel this could be approached using Chebyshev’s inequality though I'm yet to conclude how this could be applied successfully. The lecturer's exercise also gives rise naturally to the question on how we could determine $\mathbb{E}[B_k]$, $\text{Var}[B_k]$ for general $k$. This seems like it could be quite a cumbersome exercise in combinatorics. I would be very appreciative of anyone who can help with this exercise. Best, MM.",,"['probability', 'combinatorics', 'birthday']"
51,Probability of dividing a deck of cards into 4 equal piles each containing an ace,Probability of dividing a deck of cards into 4 equal piles each containing an ace,,"After dividing a standard deck of cards into 4 equal sized piles, what's the probability that exactly one ace is in each pile? I've had a couple of ideas about how to set this problem up but nothing seems to come out correctly. For instance, I can look at the probabilities that each pile has a single ace and set this up as a multiplication of conditional probabilities. It also occurred to me that I could view the events as (the event that Ace of Spades and Ace of Hearts are in different piles), (the event that Ace Spades, Diamonds, and Hearts are in different piles), and (the event that all aces are in different piles). However, I'm having a hard time even determining what the first of these probabilities should be. Is it correct that the probability of the first pile having a single ace is $\dfrac{\binom{4}{1}\binom{48}{12}}{\binom{52}{13}}$? It's been a while since I've done probability so I'm having a little trouble getting started, though I know that eventually I'm going to multiply a number of conditional probabilities together.","After dividing a standard deck of cards into 4 equal sized piles, what's the probability that exactly one ace is in each pile? I've had a couple of ideas about how to set this problem up but nothing seems to come out correctly. For instance, I can look at the probabilities that each pile has a single ace and set this up as a multiplication of conditional probabilities. It also occurred to me that I could view the events as (the event that Ace of Spades and Ace of Hearts are in different piles), (the event that Ace Spades, Diamonds, and Hearts are in different piles), and (the event that all aces are in different piles). However, I'm having a hard time even determining what the first of these probabilities should be. Is it correct that the probability of the first pile having a single ace is $\dfrac{\binom{4}{1}\binom{48}{12}}{\binom{52}{13}}$? It's been a while since I've done probability so I'm having a little trouble getting started, though I know that eventually I'm going to multiply a number of conditional probabilities together.",,['probability']
52,Markov Chains and Linear Transformations,Markov Chains and Linear Transformations,,"I just have a quick question about Markov Chain and linear algebra. Background. Let $\{M_n: n= 0, 1, 2, \dots \}$ be a Markov Chain. We can represent the transition probabilities $_{n}Q^{(i,j)}$ in a $s \times s$ matrix $Q$. Note that $_{n}Q^{(i,j)}$ is the same thing as $P(M_{m+n} = j|M_{m} = i)$. Question. Is the matrix $Q$ the same in the sense of linear transformations? In particular, suppose we have two vector spaces $V$ and $W$. If we choose an ordered basis for $V$ and an ordered basis for $W$ we can represent linear transformations from $V$ to $W$ as matrices. So then we can talk about the rank, nullity, kernel, etc...But does the term matrix in context of Markov chains just mean an array of numbers?","I just have a quick question about Markov Chain and linear algebra. Background. Let $\{M_n: n= 0, 1, 2, \dots \}$ be a Markov Chain. We can represent the transition probabilities $_{n}Q^{(i,j)}$ in a $s \times s$ matrix $Q$. Note that $_{n}Q^{(i,j)}$ is the same thing as $P(M_{m+n} = j|M_{m} = i)$. Question. Is the matrix $Q$ the same in the sense of linear transformations? In particular, suppose we have two vector spaces $V$ and $W$. If we choose an ordered basis for $V$ and an ordered basis for $W$ we can represent linear transformations from $V$ to $W$ as matrices. So then we can talk about the rank, nullity, kernel, etc...But does the term matrix in context of Markov chains just mean an array of numbers?",,"['probability', 'soft-question', 'markov-chains']"
53,Probability on an infinite plane,Probability on an infinite plane,,"(I thought this is a popular problem, but sadly Google yields nothing.) Three points are chosen at random on an infinite plane. What is the probability that they are on a line? And a variant: the plane is discrete, that is, the point coordinates are both integers. How do I even approach this?","(I thought this is a popular problem, but sadly Google yields nothing.) Three points are chosen at random on an infinite plane. What is the probability that they are on a line? And a variant: the plane is discrete, that is, the point coordinates are both integers. How do I even approach this?",,"['probability', 'geometry', 'geometric-probability']"
54,Probability of order statistics,Probability of order statistics,,"Let's say you generate 3 uniformly distributed, independent random numbers on the interval $[0,1]$. Now consider the lengths of the 4 segments made. What is the probability that the sum of the two medium-length segments is greater than $ 0.5 $? Example Let the random numbers be $ 0.5 $, $ 0.3 $, $ 0.1 $. This cuts the interval like so: |*|**|**|*****| The sum of the medium-length segments is then $ 0.2 + 0.2 = 0.4 $. Answer (numerical, no proof) I ran a computer simulation of this and got the answer: spoiler . However I can't seem to come up with a derivation of this.","Let's say you generate 3 uniformly distributed, independent random numbers on the interval $[0,1]$. Now consider the lengths of the 4 segments made. What is the probability that the sum of the two medium-length segments is greater than $ 0.5 $? Example Let the random numbers be $ 0.5 $, $ 0.3 $, $ 0.1 $. This cuts the interval like so: |*|**|**|*****| The sum of the medium-length segments is then $ 0.2 + 0.2 = 0.4 $. Answer (numerical, no proof) I ran a computer simulation of this and got the answer: spoiler . However I can't seem to come up with a derivation of this.",,"['probability', 'combinatorics']"
55,Factorial Moment of the Geometric Distribution,Factorial Moment of the Geometric Distribution,,"I am trying to caclulate the Factorial Moment of the Geometric Distribution #2 with parameter $p$. Therefore I set $\Omega = \mathbb{N}_0$ and have by using the pochhammer symbol and setting $q=1-q$ that $$E((k)_l)= \sum _{k=0}^{\infty } (k)_l p q^k = p^{-l} q \cdot l! \sum _{k=0}^{\infty } (\frac{(k+l-1)!}{(k-1)! \cdot l!}\cdot p^{l+1} q^{k-1}) $$ Now Mathematica tells me that $\sum _{k=0}^{\infty } (\frac{(k+l-1)!}{(k-1)! \cdot l!}\cdot p^{l+1} q^{k-1})=1$, but I cannot see why this identity is true. Also when using FactorialMoment[GeometricDistribution[p], l] Mathematica suggests that $E((k)_l)=(\frac{q}{p})^l l!$. Thank you in Advance for your help.","I am trying to caclulate the Factorial Moment of the Geometric Distribution #2 with parameter $p$. Therefore I set $\Omega = \mathbb{N}_0$ and have by using the pochhammer symbol and setting $q=1-q$ that $$E((k)_l)= \sum _{k=0}^{\infty } (k)_l p q^k = p^{-l} q \cdot l! \sum _{k=0}^{\infty } (\frac{(k+l-1)!}{(k-1)! \cdot l!}\cdot p^{l+1} q^{k-1}) $$ Now Mathematica tells me that $\sum _{k=0}^{\infty } (\frac{(k+l-1)!}{(k-1)! \cdot l!}\cdot p^{l+1} q^{k-1})=1$, but I cannot see why this identity is true. Also when using FactorialMoment[GeometricDistribution[p], l] Mathematica suggests that $E((k)_l)=(\frac{q}{p})^l l!$. Thank you in Advance for your help.",,"['probability', 'probability-distributions', 'factorial']"
56,A probability question,A probability question,,"Suppose $X_1, X_2, ...,$ are IID random variables with $P(X_n=1)=p$ and $P(X_n=2)=1-p$. Let $S_n=\sum_{i=1}^n X_i$. I was wondering how to find $P(S_n \neq z, \forall n \in \mathbb{N})$ for some particular natural number $z$? How about the case when $P(X_n=1) = p_1, P(X_n=2)=p_2, P(X_n=3)=1-p_1-p_2$? Does this problem belong to some kind of problems of random process? Thanks!","Suppose $X_1, X_2, ...,$ are IID random variables with $P(X_n=1)=p$ and $P(X_n=2)=1-p$. Let $S_n=\sum_{i=1}^n X_i$. I was wondering how to find $P(S_n \neq z, \forall n \in \mathbb{N})$ for some particular natural number $z$? How about the case when $P(X_n=1) = p_1, P(X_n=2)=p_2, P(X_n=3)=1-p_1-p_2$? Does this problem belong to some kind of problems of random process? Thanks!",,"['probability', 'stochastic-processes']"
57,Probability of picking an item r times out of n attempts,Probability of picking an item r times out of n attempts,,"Trying to remember my high school formulas, and coming up dry. Say I have two choices: $A$ and $B$. $P(A) = 0.25$ ; $P(B) = 0.75$ There are no conditional probabilities or anything. Each choice is independent of the last. How do I go about calculating the probability that I will choose A exactly twice, having chosen ten items?","Trying to remember my high school formulas, and coming up dry. Say I have two choices: $A$ and $B$. $P(A) = 0.25$ ; $P(B) = 0.75$ There are no conditional probabilities or anything. Each choice is independent of the last. How do I go about calculating the probability that I will choose A exactly twice, having chosen ten items?",,"['combinatorics', 'probability']"
58,Probability that n people will get one right and one left shoes from n pairs of shoes.,Probability that n people will get one right and one left shoes from n pairs of shoes.,,"Suppose we have a closet with n pairs of shoes. n people will randomly pick 2 shoes from the closet. What is the probability that all of them will have 1 right shoe and 1 left shoe? My approach to this question was like this: We have $C(2n,2)\times C(2n-2,2) \times...\times C(2,2)$ ways for n people to choose all of the shoes. Now, I can calculate the number of ways that none of the n people will have 1 left and 1 right. Meaning, I can calculate the ways that all n people will either have 2 right or 2 left. Then I will take the complement and divide by the number of total ways that n people can choose 2n shoes randomly Another approach is to count the number of ways that all of them will have both right and left however I don't know how to approach it. I thought that the first person has n ways to choose say right pair, and n ways to choose left. The second has n-1 * n-1 and so on. We get n!*n! but I am not sure if this approach is correct. Also, Can I solve it using probability and not use counting? For instance say I have n/2n for the first person to choose a right pair, then for the left 1/n. How do I continue?","Suppose we have a closet with n pairs of shoes. n people will randomly pick 2 shoes from the closet. What is the probability that all of them will have 1 right shoe and 1 left shoe? My approach to this question was like this: We have ways for n people to choose all of the shoes. Now, I can calculate the number of ways that none of the n people will have 1 left and 1 right. Meaning, I can calculate the ways that all n people will either have 2 right or 2 left. Then I will take the complement and divide by the number of total ways that n people can choose 2n shoes randomly Another approach is to count the number of ways that all of them will have both right and left however I don't know how to approach it. I thought that the first person has n ways to choose say right pair, and n ways to choose left. The second has n-1 * n-1 and so on. We get n!*n! but I am not sure if this approach is correct. Also, Can I solve it using probability and not use counting? For instance say I have n/2n for the first person to choose a right pair, then for the left 1/n. How do I continue?","C(2n,2)\times C(2n-2,2) \times...\times C(2,2)",['probability']
59,What is the relation between rate and probability? [closed],What is the relation between rate and probability? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 months ago . Improve this question Consider the specific example where a row of light bulbs (initially off), arbitrarily turn on at different rates. For a light bulb at position $x$ , call this rate $f(x)$ . This is an irreversible process, so if a light is on, it stays on. Now consider $p(x,t)$ the probability of a light bulb turning on at position $x$ and time $t$ (provided it has not yet been turned on). How are $f$ and $p$ related?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 7 months ago . Improve this question Consider the specific example where a row of light bulbs (initially off), arbitrarily turn on at different rates. For a light bulb at position , call this rate . This is an irreversible process, so if a light is on, it stays on. Now consider the probability of a light bulb turning on at position and time (provided it has not yet been turned on). How are and related?","x f(x) p(x,t) x t f p","['probability', 'statistics', 'stochastic-processes', 'random-variables']"
60,Independence and independent increments,Independence and independent increments,,"Let $X_1, X_2, X_3$ be three independent identically distributed random variables. Does this mean that $$ X_3 - X_2, \ X_2 - X_1 $$ are also independent random variables? We can instead inspect the independence of $$ X_3-X_2  \text{ and } X_1 - X_2 .$$ Obviously, $$X_3 \text{ and }X_1$$ are independent. Does adding $-X_2$ (which is independent of $X_3$ and $X_1$ ) to both sides preserve the independence?","Let be three independent identically distributed random variables. Does this mean that are also independent random variables? We can instead inspect the independence of Obviously, are independent. Does adding (which is independent of and ) to both sides preserve the independence?","X_1, X_2, X_3  X_3 - X_2, \ X_2 - X_1   X_3-X_2  \text{ and } X_1 - X_2 . X_3 \text{ and }X_1 -X_2 X_3 X_1",['probability']
61,Probability and Random Variables.,Probability and Random Variables.,,"Hi, I was trying to understand this example in the book. In the first part of the question, We've to find p.d.f ( probability density function ). For that, we take the derivative of the given distribution function F(x) and it is done.👍 But in another part of the question we've to find P(|X| < 1.5) and we have to integrate for p.d.f (Probability distribution function), Now I am confused about the ranges of those integrals. Why we must divide the range into these four pieces i.e, (-∞, -1.5), (-1.5 , 0), (0 , 1), (1 , 1.5) ? for integration. My question is, if We're trying to find P(|X| < 1.5) then why do we must divide the range of integration into these four pieces? Why is the range is going from -∞ to -1.5 (in the first integral) and not to 0? I'm confused about where did -1.5 come from? I mean, why we are including -1.5 in the range of integrals? I am assuming, it is because of the || (mod) thing. Kindly correct me if I'm wrong. Thank you.","Hi, I was trying to understand this example in the book. In the first part of the question, We've to find p.d.f ( probability density function ). For that, we take the derivative of the given distribution function F(x) and it is done.👍 But in another part of the question we've to find P(|X| < 1.5) and we have to integrate for p.d.f (Probability distribution function), Now I am confused about the ranges of those integrals. Why we must divide the range into these four pieces i.e, (-∞, -1.5), (-1.5 , 0), (0 , 1), (1 , 1.5) ? for integration. My question is, if We're trying to find P(|X| < 1.5) then why do we must divide the range of integration into these four pieces? Why is the range is going from -∞ to -1.5 (in the first integral) and not to 0? I'm confused about where did -1.5 come from? I mean, why we are including -1.5 in the range of integrals? I am assuming, it is because of the || (mod) thing. Kindly correct me if I'm wrong. Thank you.",,"['probability', 'probability-theory', 'statistics', 'definite-integrals', 'random-variables']"
62,EV of a strictly increasing sequence of n-sided dice rolls,EV of a strictly increasing sequence of n-sided dice rolls,,"You roll a fair, $n$ -sided die. If its value is higher than that of your previous roll (or this is your first roll), you add it to your score and roll again. If not, you keep any banked points and your turn is over. I'd like to know the expected score for each turn. Example playthrough with a D20: Rolls : 7, 12, 16, 13 (turn ends) Score : $7 + 12 + 16 = 35$ This process was inspired by the ball flight mechanic from the dice game Roll In One . Simulating this in Python yields the surprising (to me, at least) conclusion that the expected score is $n$ . Looking forward to any insights you have to offer!","You roll a fair, -sided die. If its value is higher than that of your previous roll (or this is your first roll), you add it to your score and roll again. If not, you keep any banked points and your turn is over. I'd like to know the expected score for each turn. Example playthrough with a D20: Rolls : 7, 12, 16, 13 (turn ends) Score : This process was inspired by the ball flight mechanic from the dice game Roll In One . Simulating this in Python yields the surprising (to me, at least) conclusion that the expected score is . Looking forward to any insights you have to offer!",n 7 + 12 + 16 = 35 n,"['probability', 'expected-value', 'dice']"
63,Probability of getting at least one spade from drawing 2 cards in succession,Probability of getting at least one spade from drawing 2 cards in succession,,"Two cards are drawn in succession without replacement from a deck of 52 playing cards. Find the probability that at least one of them is a spade. Here's my approach: We can get a spade in $13 \choose 1$ ways, and the from the rest of the cards $(52-1 = 51)$ , we can choose any card, so $52 \choose 1$ . Thus, our number of favorable outcome would be ${13 \choose 1} {51 \choose 2} = 13\times51 = 663$ , and the probability would then be $\frac{663}{1326}$ But, the book has a different answer, i.e.,it computes the number of cases where there is no spade and the number of ways for that is of course ${39 \choose 2} = 741$ , and our required probability is then $1-\frac{741}{1326} = \frac{15}{34}$ Why are the two approaches different? Where am I going wrong?","Two cards are drawn in succession without replacement from a deck of 52 playing cards. Find the probability that at least one of them is a spade. Here's my approach: We can get a spade in ways, and the from the rest of the cards , we can choose any card, so . Thus, our number of favorable outcome would be , and the probability would then be But, the book has a different answer, i.e.,it computes the number of cases where there is no spade and the number of ways for that is of course , and our required probability is then Why are the two approaches different? Where am I going wrong?",13 \choose 1 (52-1 = 51) 52 \choose 1 {13 \choose 1} {51 \choose 2} = 13\times51 = 663 \frac{663}{1326} {39 \choose 2} = 741 1-\frac{741}{1326} = \frac{15}{34},"['probability', 'combinatorics']"
64,Simple combinatorics problem - assigning people to groups,Simple combinatorics problem - assigning people to groups,,"$20$ people that A, B and C belong to are to be randomly seperated in groups of 4. I have a few questions about this problem: Q1: What is the probability, that A, B and C place in the same group? My intuition would be, that the person A has a $4/20=1/5$ chance to land in group 1 and then person B has a $3/19$ chance to land in the same group and finally person C has a $2/18$ chance to do the same. Because there are $5$ groups, this means that $\mathbb{P}(Q1)=5\frac{1}{5}\frac{3}{19}\frac{2}{18}=\frac{1}{57}$ . Q2: What is the probability that they all are placed in different groups? Place person A in group $a$ (probability $1/5$ ). Then place person B in group $b\neq a$ - probability $4/19$ . Finally place person C in group $c\notin\{a,b\}$ - probability $4/18$ . There are $\binom{5}{3}$ ways to choose $a,b,c$ , so $\mathbb{P}(Q2)=\binom{5}{3}\frac{1}{5}\frac{4}{19}\frac{4}{18}$ This somehow seems wrong to me, because it does not account for the other $17$ people. Q3: What is the conditional probability, that A,B,C land in different groups if it is already known that two of them are in different groups? This one I really struggle with. My attempt: Because there are three groups to choose from and $18$ people to match, I think that $\mathbb{P}(Q3)=3\frac{4}{18}$ . Please check my solutions and tell me what is wrong. EDIT: Thank you to everyone for the discussion and solutions, especially to @chrslg for even throwing together a little simulation. Also, Q3 is for sure related to conditional probability, so $\mathbb{P}(Q3)=\frac{\mathbb{P}(Q1)}{1-\mathbb{P}(Q2)}$ , as pointed out by you all.","people that A, B and C belong to are to be randomly seperated in groups of 4. I have a few questions about this problem: Q1: What is the probability, that A, B and C place in the same group? My intuition would be, that the person A has a chance to land in group 1 and then person B has a chance to land in the same group and finally person C has a chance to do the same. Because there are groups, this means that . Q2: What is the probability that they all are placed in different groups? Place person A in group (probability ). Then place person B in group - probability . Finally place person C in group - probability . There are ways to choose , so This somehow seems wrong to me, because it does not account for the other people. Q3: What is the conditional probability, that A,B,C land in different groups if it is already known that two of them are in different groups? This one I really struggle with. My attempt: Because there are three groups to choose from and people to match, I think that . Please check my solutions and tell me what is wrong. EDIT: Thank you to everyone for the discussion and solutions, especially to @chrslg for even throwing together a little simulation. Also, Q3 is for sure related to conditional probability, so , as pointed out by you all.","20 4/20=1/5 3/19 2/18 5 \mathbb{P}(Q1)=5\frac{1}{5}\frac{3}{19}\frac{2}{18}=\frac{1}{57} a 1/5 b\neq a 4/19 c\notin\{a,b\} 4/18 \binom{5}{3} a,b,c \mathbb{P}(Q2)=\binom{5}{3}\frac{1}{5}\frac{4}{19}\frac{4}{18} 17 18 \mathbb{P}(Q3)=3\frac{4}{18} \mathbb{P}(Q3)=\frac{\mathbb{P}(Q1)}{1-\mathbb{P}(Q2)}","['probability', 'combinatorics', 'solution-verification', 'conditional-probability']"
65,Probability that notes are in the same scale,Probability that notes are in the same scale,,"What is the probability that a random sequence of notes (on the 12-note chromatic scale) of length n is in the same major scale? Quite some time ago, I came across a song written by converting pi to base-12. (The song didn’t go on forever, obviously, but it was really cool.) The guy who wrote the song claimed that pi is a “musical number”. I wanted to make it a project to figure out whether pi is truly a musical number. That is, if pi in base-12 begins with a string of digits that, when converted to musical notes, stay in the same scale longer than statistically expected. This is a big topic depending on what types of scale(s) one could choose and what key transitions would be allowable, but I thought major would be a good place to start. Haven’t gotten very far mathematically, mainly because I can’t figure out exactly what to do or whether this is an easy or difficult problem. Edit: Since I saw this in the comments—which scale is not specified in advance. So you have the notes, and then you see if they fit ANY scale.","What is the probability that a random sequence of notes (on the 12-note chromatic scale) of length n is in the same major scale? Quite some time ago, I came across a song written by converting pi to base-12. (The song didn’t go on forever, obviously, but it was really cool.) The guy who wrote the song claimed that pi is a “musical number”. I wanted to make it a project to figure out whether pi is truly a musical number. That is, if pi in base-12 begins with a string of digits that, when converted to musical notes, stay in the same scale longer than statistically expected. This is a big topic depending on what types of scale(s) one could choose and what key transitions would be allowable, but I thought major would be a good place to start. Haven’t gotten very far mathematically, mainly because I can’t figure out exactly what to do or whether this is an easy or difficult problem. Edit: Since I saw this in the comments—which scale is not specified in advance. So you have the notes, and then you see if they fit ANY scale.",,"['probability', 'pi', 'music-theory']"
66,Can the expectation of a random variable not exist?,Can the expectation of a random variable not exist?,,"I saw a question like this. Solving that was not hard, because it only needs to show that the value of the next improper integral does not exist. $$ \int_0^\infty x\cdot\frac{2}{\pi(1+x^2)}dx=\infty $$ It is understandable by formula, but not intuitively at all. From what I've learned so far, every random variable has an expectation, and any book never said that the average may not exist. Can this actually be? Of course, it is certain that a given function satisfies the definition of a random variable. But what does it mean that the expectation of a random variable does not exist?","I saw a question like this. Solving that was not hard, because it only needs to show that the value of the next improper integral does not exist. It is understandable by formula, but not intuitively at all. From what I've learned so far, every random variable has an expectation, and any book never said that the average may not exist. Can this actually be? Of course, it is certain that a given function satisfies the definition of a random variable. But what does it mean that the expectation of a random variable does not exist?","
\int_0^\infty x\cdot\frac{2}{\pi(1+x^2)}dx=\infty
","['probability', 'statistics']"
67,Probability of same genes or cards.,Probability of same genes or cards.,,"Let's say there is a deck of 100 cards, you choose 6 cards of them. Another person also choose 6 in a new deck. What is the probability of sharing one card? What is the probability of sharing two cards? ... What is the probability of sharing 6 cards? I couldn't figure out the distribution. there are 100C6 ways of choosing 6 cards but a second person also has those possibilities. How do I know if the card is the same? If it was only a single card for both, then there are 100 ways of 100^2 = 1/100. With two cards for both... Maybe taking off artificially 6 cards from the second deck, then 100C6*94C6= number of ways without repeating a single card. Then $(100C6*94C6)/(100C6)^2=(94C6)/(100C6)=0.68$ is the percentage of no cards repeated and 1-ans is the probability of 1|2|3|4|5|6? If yes, how do I find individual cases?","Let's say there is a deck of 100 cards, you choose 6 cards of them. Another person also choose 6 in a new deck. What is the probability of sharing one card? What is the probability of sharing two cards? ... What is the probability of sharing 6 cards? I couldn't figure out the distribution. there are 100C6 ways of choosing 6 cards but a second person also has those possibilities. How do I know if the card is the same? If it was only a single card for both, then there are 100 ways of 100^2 = 1/100. With two cards for both... Maybe taking off artificially 6 cards from the second deck, then 100C6*94C6= number of ways without repeating a single card. Then is the percentage of no cards repeated and 1-ans is the probability of 1|2|3|4|5|6? If yes, how do I find individual cases?",(100C6*94C6)/(100C6)^2=(94C6)/(100C6)=0.68,"['probability', 'probability-distributions']"
68,Probability of 3 red balls out of 200 balls placed in 5 bins,Probability of 3 red balls out of 200 balls placed in 5 bins,,"Given 3 red balls, 197 black balls, and 5 bins.  The balls are randomly distributed with 40 balls in each bin.  What is the probability that the three red balls are in three separate bins (each of the three bins has one red ball; i.e., no two red balls are in the same bin)? Different variants of this question has been asked – but somehow I still have trouble computing the correct probability. One answer I found was: $$\frac{\left(\begin{array}{l} 5 \\ 1 \end{array}\right)\left(\begin{array}{l} 4 \\ 1 \end{array}\right)\left(\begin{array}{l} 3 \\ 1 \end{array}\right)}{125} = 12/25$$ It only focuses on red balls, and there are 125 ways that 3 red balls can be allocated in bins. However, another alternative way of computing this seems to be: $$ \frac{5 \times {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {200 \choose 40} {200 \choose 40}}{{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}} = 27/25 $$ Obviously it seems that the alternative way is incorrect (it's above 1 as a probability). So my question is: is this alternative way of computing incorrect? How so? How can I fix it?","Given 3 red balls, 197 black balls, and 5 bins.  The balls are randomly distributed with 40 balls in each bin.  What is the probability that the three red balls are in three separate bins (each of the three bins has one red ball; i.e., no two red balls are in the same bin)? Different variants of this question has been asked – but somehow I still have trouble computing the correct probability. One answer I found was: It only focuses on red balls, and there are 125 ways that 3 red balls can be allocated in bins. However, another alternative way of computing this seems to be: Obviously it seems that the alternative way is incorrect (it's above 1 as a probability). So my question is: is this alternative way of computing incorrect? How so? How can I fix it?","\frac{\left(\begin{array}{l}
5 \\
1
\end{array}\right)\left(\begin{array}{l}
4 \\
1
\end{array}\right)\left(\begin{array}{l}
3 \\
1
\end{array}\right)}{125} = 12/25 
\frac{5 \times {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {3 \choose 1} {199 \choose 39} {200 \choose 40} {200 \choose 40}}{{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}{200 \choose 40}} = 27/25
","['probability', 'combinatorics', 'balls-in-bins']"
69,"Game with $n$ players, respective chances of winning however many games in succession","Game with  players, respective chances of winning however many games in succession",n,"A number of persons $A$ , $B$ , $C$ , $D$ , $\ldots$ play at a game, their chances of winning any particular game being $\alpha$ , $\beta$ , $\gamma$ , $\delta$ , $\ldots$ respectively. The match is won by $A$ if he gains $a$ games in succession: by $B$ if he gains $b$ games in succession; and so on. The play continues till one of these events happens. How do I see that their chances of winning the match are proportional to $${{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}},{{(1 - \beta)\beta^b}\over{1 - \beta^b}}, {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}}, \text{etc.}$$ and that the average number of games in the match will be $$1/\left({{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}} + {{(1 - \beta)\beta^b}\over{1 - \beta^b}} + {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}} + \ldots\right)?$$","A number of persons , , , , play at a game, their chances of winning any particular game being , , , , respectively. The match is won by if he gains games in succession: by if he gains games in succession; and so on. The play continues till one of these events happens. How do I see that their chances of winning the match are proportional to and that the average number of games in the match will be","A B C D \ldots \alpha \beta \gamma \delta \ldots A a B b {{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}},{{(1 - \beta)\beta^b}\over{1 - \beta^b}}, {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}}, \text{etc.} 1/\left({{(1 - \alpha)\alpha^a}\over{1 - \alpha^a}} + {{(1 - \beta)\beta^b}\over{1 - \beta^b}} + {{(1 - \gamma)\gamma^c}\over{1 - \gamma^c}} + \ldots\right)?","['probability', 'combinatorics', 'algebra-precalculus', 'expected-value', 'markov-chains']"
70,Number of draws before you see all candies?,Number of draws before you see all candies?,,"Recently, I thought of the following question: Suppose there are 5 candies in a bag - you choose two candies, and then put these two candies back in the bag (assume each candy has an equal probability of being selected). On average, how many times do you need to choose candies before you are guaranteed to have seen every candy at least once? In a way, this problem kind of reminds me of the ""Coupon Collector Problem"" ( https://en.wikipedia.org/wiki/Coupon_collector%27s_problem ), but I am not sure how to solve this problem using the Coupon Collector framework. I thought of framing this problem as a Markov Chain: State 2 : You have observed 2 unique candies State 3: You have observed 3 unique candies State 4: You have observed 4 unique candies State 5: You have observed 5 unique candies (Absorbing State) It took me a long time, but I think I was able to create a Transition Matrix for this problem : A = matrix(    c(0.1, 0.6, 0.3, 0, 0,0.3, 0.6, 0.1, 0,0, 0.6, 0.4, 0,0,0, 1), # the data elements    nrow=4,              # number of rows    ncol=4,              # number of columns   byrow = TRUE)          [,1] [,2] [,3] [,4] [1,]  0.1  0.6  0.3  0.0 [2,]  0.0  0.3  0.6  0.1 [3,]  0.0  0.0  0.6  0.4 [4,]  0.0  0.0  0.0  1.0 From here, I suppose I could use the Theory of Markov Chains and find out the expected number of transitions until you reach the Absorbing State - but it was quite difficult to correctly calculate the transition probabilities. I imagine that once the number of states (i.e. ""candies"") increase, it will become very difficult to calculate all these transition probabilities. I was hoping for an easier way which would directly allow you to calculate the expected number of draws needed to observe ""M"" candies (at least once) with ""N"" draws and each draw of size ""K"" (e.g. M = 5, K = 2, N = ?) - provided you are given the probability of selecting any given candy (e.g. suppose the candies did not have equal probabilities of being selected). Can someone please suggest another way of solving this problem? Thanks! ""Food"" for Thought: Suppose there were ""M"" candies"" and you draw ""K"" candies ""N"" number of times. Suppose this time, you don't know the true value of ""M"" and you only have information on ""K"" and ""N"" - is there a way to estimate ""M"" based on the data you collect from ""K"" and ""N""?","Recently, I thought of the following question: Suppose there are 5 candies in a bag - you choose two candies, and then put these two candies back in the bag (assume each candy has an equal probability of being selected). On average, how many times do you need to choose candies before you are guaranteed to have seen every candy at least once? In a way, this problem kind of reminds me of the ""Coupon Collector Problem"" ( https://en.wikipedia.org/wiki/Coupon_collector%27s_problem ), but I am not sure how to solve this problem using the Coupon Collector framework. I thought of framing this problem as a Markov Chain: State 2 : You have observed 2 unique candies State 3: You have observed 3 unique candies State 4: You have observed 4 unique candies State 5: You have observed 5 unique candies (Absorbing State) It took me a long time, but I think I was able to create a Transition Matrix for this problem : A = matrix(    c(0.1, 0.6, 0.3, 0, 0,0.3, 0.6, 0.1, 0,0, 0.6, 0.4, 0,0,0, 1), # the data elements    nrow=4,              # number of rows    ncol=4,              # number of columns   byrow = TRUE)          [,1] [,2] [,3] [,4] [1,]  0.1  0.6  0.3  0.0 [2,]  0.0  0.3  0.6  0.1 [3,]  0.0  0.0  0.6  0.4 [4,]  0.0  0.0  0.0  1.0 From here, I suppose I could use the Theory of Markov Chains and find out the expected number of transitions until you reach the Absorbing State - but it was quite difficult to correctly calculate the transition probabilities. I imagine that once the number of states (i.e. ""candies"") increase, it will become very difficult to calculate all these transition probabilities. I was hoping for an easier way which would directly allow you to calculate the expected number of draws needed to observe ""M"" candies (at least once) with ""N"" draws and each draw of size ""K"" (e.g. M = 5, K = 2, N = ?) - provided you are given the probability of selecting any given candy (e.g. suppose the candies did not have equal probabilities of being selected). Can someone please suggest another way of solving this problem? Thanks! ""Food"" for Thought: Suppose there were ""M"" candies"" and you draw ""K"" candies ""N"" number of times. Suppose this time, you don't know the true value of ""M"" and you only have information on ""K"" and ""N"" - is there a way to estimate ""M"" based on the data you collect from ""K"" and ""N""?",,"['probability', 'statistics', 'markov-chains', 'coupon-collector']"
71,Construct a fair game with a $N$ sided die,Construct a fair game with a  sided die,N,"You have a $N$ sided die. And $X$ players. You have to devise a game, such that only one player wins and every player is equally likely to win. Also, the game should be finite (there shouldn't be a single infinite run in the sample space) Is it possible to construct such a game? (However complicated, doesn't matter) If yes, how? At the first glance, it seems like it isn't possible, since (For $N=6$ ) we can only have sample space of the sizes of powers of $2,3,6$ . But maybe there exists a complicated game where the sizes are different?","You have a sided die. And players. You have to devise a game, such that only one player wins and every player is equally likely to win. Also, the game should be finite (there shouldn't be a single infinite run in the sample space) Is it possible to construct such a game? (However complicated, doesn't matter) If yes, how? At the first glance, it seems like it isn't possible, since (For ) we can only have sample space of the sizes of powers of . But maybe there exists a complicated game where the sizes are different?","N X N=6 2,3,6","['probability', 'probability-theory', 'game-theory', 'puzzle']"
72,"Is it easier to pass a test with more questions and more mistakes allowed, or less questions but less mistakes allowed?","Is it easier to pass a test with more questions and more mistakes allowed, or less questions but less mistakes allowed?",,"$Q$ is a set of yes or no questions. I know the answer to $q_{know}$ of these questions ( $0\leq q_{know}\leq|Q|)$ , but I have to guess for the remaining ones, with a $0.5$ probability of guessing correctly. A test $T$ is generated by randomly selecting $n$ of these questions ( $|T|=n$ ). To pass the test, I can make $k$ mistakes at the most, with $k>0$ . What is the probability of passing the test, as a function of these parameters? Assuming $n/k%$ is constant (e.g., for every ten questions in the test, 1 mistake is allowed) is it better to take the test with more or less questions? This question came up during a discussion and I thought that the number of questions doesn't matter because the probability is always the same, but I'm starting to think that it might be more complicated than this. Should it be a product of binomial distributions?","is a set of yes or no questions. I know the answer to of these questions ( , but I have to guess for the remaining ones, with a probability of guessing correctly. A test is generated by randomly selecting of these questions ( ). To pass the test, I can make mistakes at the most, with . What is the probability of passing the test, as a function of these parameters? Assuming is constant (e.g., for every ten questions in the test, 1 mistake is allowed) is it better to take the test with more or less questions? This question came up during a discussion and I thought that the number of questions doesn't matter because the probability is always the same, but I'm starting to think that it might be more complicated than this. Should it be a product of binomial distributions?",Q q_{know} 0\leq q_{know}\leq|Q|) 0.5 T n |T|=n k k>0 n/k%,"['probability', 'probability-distributions', 'binomial-distribution']"
73,Asymptotic normality of estimator of uniform's distribution parameter,Asymptotic normality of estimator of uniform's distribution parameter,,"We have $X_1, ..., X_n \sim U[0, \theta]$ and estimator $\phi^*(X_{[n]}) = X_{(n)}$ . $X_{(n)}$ here stands for $\max_iX_i$ . I need to made this estimator unbiased and check if it is asymptotic normal. Unbiasing is easy: we need to find expectation of $\phi$ . So $\mathbb{E}\phi = \frac{n}{n+1}\theta$ . Bias is $b(\phi^*, \theta) = \mathbb{E}X_{(n)} - \theta = -\frac{\theta}{n + 1}$ . So, unbiased estimator is $\tilde{\phi^*}$ = $\frac{n+1}{n}X_{(n)}$ . And now I need to check whether this unbiased estimator is asymptotic normal , i.e. $\sqrt{n}(\frac{n+1}{n}X_{(n)} - \theta) \to \mathcal{N}(0, \sigma^2(\theta) )$ . How can I do that? Do I need to use central limit theorem?","We have and estimator . here stands for . I need to made this estimator unbiased and check if it is asymptotic normal. Unbiasing is easy: we need to find expectation of . So . Bias is . So, unbiased estimator is = . And now I need to check whether this unbiased estimator is asymptotic normal , i.e. . How can I do that? Do I need to use central limit theorem?","X_1, ..., X_n \sim U[0, \theta] \phi^*(X_{[n]}) = X_{(n)} X_{(n)} \max_iX_i \phi \mathbb{E}\phi = \frac{n}{n+1}\theta b(\phi^*, \theta) = \mathbb{E}X_{(n)} - \theta = -\frac{\theta}{n + 1} \tilde{\phi^*} \frac{n+1}{n}X_{(n)} \sqrt{n}(\frac{n+1}{n}X_{(n)} - \theta) \to \mathcal{N}(0, \sigma^2(\theta) )","['probability', 'probability-theory', 'statistics', 'central-limit-theorem', 'probability-limit-theorems']"
74,Why is this probability equation wrong?,Why is this probability equation wrong?,,"I have a problem stating ""There are 13 apples and 17 oranges. What is the probability of making a group of 4 with at least two apples?"" Using my logic (left side of the following equation), I first multiply a group of 2 that consists of 2 apples, and then another group of 2, which consists of the other 28 elements. However, this is wrong; the right way to do it is to add all possible outcomes (AAOO, AAAO and AAAA) as shown on the right side of the equation $$\frac{\binom{13}{2}\binom{28}{2}}{\binom{30}{4}} \neq \frac{\binom{13}{4}+17\binom{13}{3}+\binom{13}{2}\binom{17}{2}}{\binom{30}{4}}$$ Why is my way incorrect? Is there a smarter way to do this? p.s.: the right answer is $\frac{1079}{1827}$ if it helps...","I have a problem stating ""There are 13 apples and 17 oranges. What is the probability of making a group of 4 with at least two apples?"" Using my logic (left side of the following equation), I first multiply a group of 2 that consists of 2 apples, and then another group of 2, which consists of the other 28 elements. However, this is wrong; the right way to do it is to add all possible outcomes (AAOO, AAAO and AAAA) as shown on the right side of the equation Why is my way incorrect? Is there a smarter way to do this? p.s.: the right answer is if it helps...",\frac{\binom{13}{2}\binom{28}{2}}{\binom{30}{4}} \neq \frac{\binom{13}{4}+17\binom{13}{3}+\binom{13}{2}\binom{17}{2}}{\binom{30}{4}} \frac{1079}{1827},"['probability', 'combinatorics', 'combinations']"
75,Proving Renyi's result on the order statistics of the exponential distribution,Proving Renyi's result on the order statistics of the exponential distribution,,"The Wikipedia article on order statistics mentions the following result on the order statistics of an exponential distribution with rate parameter, $\lambda$ : $$X_{(i)} = \frac{1}{\lambda}\sum\limits_{j=1}^i \frac{Z_j}{n-j+1} \tag{1}$$ It provides no proof of this. How do I prove it? My attempt: We know that to get X_{(i)} for a distribution with inverse CDF $F_X^{-1}(x)$ , we first get the corresponding order statistic of the uniform ( $U_{(i)}$ ) and then apply the inverse CDF to it. We know that $U_{(i)} \sim B(i,n-i+1)$ . And the inverse CDF of the exponential distribution is: $F_X^{-1}(x) = -\frac{\log(1-x)}{\lambda}$ . This means that the distribution of $X_{(i)}$ should be: $-\frac{\log(1-U_{(i)})}{\lambda}$ Also, $1-U_{(i)} \sim U_{(n-i)}$ . So, the distribution of the order statistic becomes: $$X_{(i)}\sim -\frac{\log(U_{(n-i)})}{\lambda}$$ We have a Beta inside a logarithm. Don't see a path to equation (1) except maybe expressing the Beta as a Gamma and then noting that the Gamma is a sum of exponentials?","The Wikipedia article on order statistics mentions the following result on the order statistics of an exponential distribution with rate parameter, : It provides no proof of this. How do I prove it? My attempt: We know that to get X_{(i)} for a distribution with inverse CDF , we first get the corresponding order statistic of the uniform ( ) and then apply the inverse CDF to it. We know that . And the inverse CDF of the exponential distribution is: . This means that the distribution of should be: Also, . So, the distribution of the order statistic becomes: We have a Beta inside a logarithm. Don't see a path to equation (1) except maybe expressing the Beta as a Gamma and then noting that the Gamma is a sum of exponentials?","\lambda X_{(i)} = \frac{1}{\lambda}\sum\limits_{j=1}^i \frac{Z_j}{n-j+1} \tag{1} F_X^{-1}(x) U_{(i)} U_{(i)} \sim B(i,n-i+1) F_X^{-1}(x) = -\frac{\log(1-x)}{\lambda} X_{(i)} -\frac{\log(1-U_{(i)})}{\lambda} 1-U_{(i)} \sim U_{(n-i)} X_{(i)}\sim -\frac{\log(U_{(n-i)})}{\lambda}","['probability', 'exponential-distribution', 'beta-function']"
76,Clarification on convergence in distribution $\implies$ convergence in probability requiring a constant probability space,Clarification on convergence in distribution  convergence in probability requiring a constant probability space,\implies,"Let $X_n,n\geq1$ be a sequence of random variables defined on the same probability space. Show that if $X_n \to c$ in distribution, where $c$ is a constant, then also $X_n \to c$ in probability. I am attempting to formulate an efficient proof for this claim, which I have done as follows: Letting $F_n$ denote the distribution function for $X_n$ , and $F$ is that of $c$ , i.e. $$F(x)=\begin{cases} 0\;\;x\lt c\\ 1\;\;x\geq c \end{cases}$$ we have that $F_n(x)\to F(x)$ for all $x\in\mathbb{R}\setminus\{c\}$ . Choose $\epsilon\gt 0$ . Then $$\begin{align}\mathbb{P}(\lvert X_n-c\rvert\lt\epsilon)=\mathbb{P}(\{X_n\lt c+\epsilon\}\setminus\{X_n\leq c-\epsilon\})\\ =\mathbb{P}(X_n\lt c+\epsilon)-\mathbb{P}(X_n\leq c-\epsilon)\\ =F_n(c+\epsilon)-F_n(c-\epsilon)\\ \to1-0=1. \end{align}$$ Note $\epsilon$ arbitrary so the result follows. My question is what is what is the precise reason / justification for the variables to be on the same probability space? At which step would this need to be used?","Let be a sequence of random variables defined on the same probability space. Show that if in distribution, where is a constant, then also in probability. I am attempting to formulate an efficient proof for this claim, which I have done as follows: Letting denote the distribution function for , and is that of , i.e. we have that for all . Choose . Then Note arbitrary so the result follows. My question is what is what is the precise reason / justification for the variables to be on the same probability space? At which step would this need to be used?","X_n,n\geq1 X_n \to c c X_n \to c F_n X_n F c F(x)=\begin{cases}
0\;\;x\lt c\\
1\;\;x\geq c
\end{cases} F_n(x)\to F(x) x\in\mathbb{R}\setminus\{c\} \epsilon\gt 0 \begin{align}\mathbb{P}(\lvert X_n-c\rvert\lt\epsilon)=\mathbb{P}(\{X_n\lt c+\epsilon\}\setminus\{X_n\leq c-\epsilon\})\\
=\mathbb{P}(X_n\lt c+\epsilon)-\mathbb{P}(X_n\leq c-\epsilon)\\
=F_n(c+\epsilon)-F_n(c-\epsilon)\\
\to1-0=1.
\end{align} \epsilon","['probability', 'probability-theory', 'probability-distributions', 'convergence-divergence', 'proof-writing']"
77,Nine person of equal strength playing in tournament,Nine person of equal strength playing in tournament,,"Nine persons $P_i$ , i = 1, 2, ...., 9 of equal strength are playing a tournament such that they are first grouped in to three groups A, B, C each containing three persons at random and a winner from each group is selected and. a new group is formed and finally from this group the winner of the tournament is decided. Probability that $P_2$ and $P_4$ were in different groups given that $P_4 $ is the winner of the tournament is   equal to (A) $\frac{2}{3}$ (B) $\frac{1}{2}$ (C) $\frac{1}{4}$ (D) $\frac{3}{4}$ Let us treat it a case of 9 distinct balls (each player) and three sets of three identical boxes though three identical boxes are put in three distinct boxes Let us put ball representing $P_2$ & $P_4$ in different boxes and then make the number of combinati0ns ${}^7{C_3} \times {}^4{C_2} = 210$ Now the arrangement of three groups is $210\times3!=1260$ Now total number of cases without restriction = ${}^9{C_3} \times {}^6{C_3} = 1680$ Now the arrangement of three groups is $1680\times3!=10080$ Probability that $P_2$ and $P_4$ are in different group = $\frac{1260}{10080}=\frac{1}{8}$ I am not sure about my approach","Nine persons , i = 1, 2, ...., 9 of equal strength are playing a tournament such that they are first grouped in to three groups A, B, C each containing three persons at random and a winner from each group is selected and. a new group is formed and finally from this group the winner of the tournament is decided. Probability that and were in different groups given that is the winner of the tournament is   equal to (A) (B) (C) (D) Let us treat it a case of 9 distinct balls (each player) and three sets of three identical boxes though three identical boxes are put in three distinct boxes Let us put ball representing & in different boxes and then make the number of combinati0ns Now the arrangement of three groups is Now total number of cases without restriction = Now the arrangement of three groups is Probability that and are in different group = I am not sure about my approach",P_i P_2 P_4 P_4  \frac{2}{3} \frac{1}{2} \frac{1}{4} \frac{3}{4} P_2 P_4 {}^7{C_3} \times {}^4{C_2} = 210 210\times3!=1260 {}^9{C_3} \times {}^6{C_3} = 1680 1680\times3!=10080 P_2 P_4 \frac{1260}{10080}=\frac{1}{8},['probability']
78,A question about expectation of chromatic number,A question about expectation of chromatic number,,"For a graph $G$ , let $G_{1/2}$ be the subgraph of $G$ that each edge of $G$ is included independently with probability. Then for $H=G_{1/2}$ , after a moment thought, it can be proved that $\chi(H)\chi(H^c)\ge \chi(G)$ by coloring each vertex of $G$ by $(c_1,c_2)$ for color $c_1$ in the coloring of $H$ and $c_2$ in that of $H^c$ , where $H^c$ stands for the complement graph of $H$ and $\chi(G)$ stands for the vertex chromatic number of $G$ . A statement says that it implies $\mathbb{E}(G_{1/2})\ge\chi(G)^{1/2}$ . I am wondering why this statement is true. I think the above argument only implies $\mathbb{E}(\chi(H)\chi(H^c))\ge \mathbb{E}(\chi(G))=\chi(G)$ , even if we know $\mathbb{E}(\chi(H))=\mathbb{E}(\chi(H^c))$ (but they are not independent so we don't know $\mathbb{E}(\chi(H)\chi(H^c))= \mathbb{E}(\chi(H))\mathbb{E}(\chi(H^c))$ ?)","For a graph , let be the subgraph of that each edge of is included independently with probability. Then for , after a moment thought, it can be proved that by coloring each vertex of by for color in the coloring of and in that of , where stands for the complement graph of and stands for the vertex chromatic number of . A statement says that it implies . I am wondering why this statement is true. I think the above argument only implies , even if we know (but they are not independent so we don't know ?)","G G_{1/2} G G H=G_{1/2} \chi(H)\chi(H^c)\ge \chi(G) G (c_1,c_2) c_1 H c_2 H^c H^c H \chi(G) G \mathbb{E}(G_{1/2})\ge\chi(G)^{1/2} \mathbb{E}(\chi(H)\chi(H^c))\ge \mathbb{E}(\chi(G))=\chi(G) \mathbb{E}(\chi(H))=\mathbb{E}(\chi(H^c)) \mathbb{E}(\chi(H)\chi(H^c))= \mathbb{E}(\chi(H))\mathbb{E}(\chi(H^c))","['probability', 'combinatorics', 'graph-theory', 'coloring']"
79,How to sample random variables X and Y from a joint distribution.,How to sample random variables X and Y from a joint distribution.,,"I have the following joint distribution function $f(x,y)$ : $$f(x,y)=\begin{cases} \frac{1}{30}xy+\frac{x}{y^2} & \text{ for } 1\le y\le 4,\ 1/2\le x\le 3/2\\ 0 & \text{ otherwise.} \end{cases}$$ The task is to simulate $N$ samples programatically, of $X$ and $Y$ to compute $E(X)$ , $E(Y)$ , $Var(X)$ and $Var(Y)$ . So far, I have tried the following steps: Calculate the joint CDF from the above PDF. Use some multi-variate Inverse transform sampling And then from the samples of X an Y, calculate sample mean and variance. I need to know what inverse transform I can use for this PDF and how I can go about calculating sample mean and variance from the samples of X and Y. That is to say, if my approach is not entirely wrong.","I have the following joint distribution function : The task is to simulate samples programatically, of and to compute , , and . So far, I have tried the following steps: Calculate the joint CDF from the above PDF. Use some multi-variate Inverse transform sampling And then from the samples of X an Y, calculate sample mean and variance. I need to know what inverse transform I can use for this PDF and how I can go about calculating sample mean and variance from the samples of X and Y. That is to say, if my approach is not entirely wrong.","f(x,y) f(x,y)=\begin{cases}
\frac{1}{30}xy+\frac{x}{y^2} & \text{ for } 1\le y\le 4,\ 1/2\le x\le 3/2\\
0 & \text{ otherwise.}
\end{cases} N X Y E(X) E(Y) Var(X) Var(Y)","['probability', 'probability-distributions', 'random-variables', 'sampling', 'simulation']"
80,Reference and proof of binomial tail identity,Reference and proof of binomial tail identity,,"May I have a reference and/or proof of this identity? I saw it mentioned on mathoverflow and don't see how to show it. For $p \in (0,1)$ and $0 \leq k < n$ , $$ \sum_{i=0}^k {n \choose i} p^i (1-p)^{n-i} = (1-p)^{n-k} \sum_{i=0}^k {n-k-1+i \choose i} p^i .$$ I've verified this numerically and tried some applying the binomial theorem to $(1-p)^{k-i}$ on the left, but that didn't seem to help, so I thought I'd ask for a reference. Update : I have found a combinatorial proof and posted it as an answer below.","May I have a reference and/or proof of this identity? I saw it mentioned on mathoverflow and don't see how to show it. For and , I've verified this numerically and tried some applying the binomial theorem to on the left, but that didn't seem to help, so I thought I'd ask for a reference. Update : I have found a combinatorial proof and posted it as an answer below.","p \in (0,1) 0 \leq k < n  \sum_{i=0}^k {n \choose i} p^i (1-p)^{n-i} = (1-p)^{n-k} \sum_{i=0}^k {n-k-1+i \choose i} p^i . (1-p)^{k-i}","['probability', 'combinatorics', 'reference-request', 'binomial-coefficients', 'binomial-theorem']"
81,Expectation of a composite function.,Expectation of a composite function.,,"My thoughts on the solution: I noticed that the ln function ""stretches"" the intervals of X like: (-1; 0) to (-inf; 0), and (0; 1) to (0; ln2). Intuitively, it becomes clear that due to the long interval of negative values of ln(1+x), the expectation will also be negative. But I haven't figured out how to strictly prove it. In general, it is probably worth considering the integral: ∫1−1ln(x+1)f(x)dx, where f(x) is probability density function. Thanks!","My thoughts on the solution: I noticed that the ln function ""stretches"" the intervals of X like: (-1; 0) to (-inf; 0), and (0; 1) to (0; ln2). Intuitively, it becomes clear that due to the long interval of negative values of ln(1+x), the expectation will also be negative. But I haven't figured out how to strictly prove it. In general, it is probably worth considering the integral: ∫1−1ln(x+1)f(x)dx, where f(x) is probability density function. Thanks!",,"['probability', 'statistics', 'probability-distributions', 'expected-value']"
82,Probability for dummies,Probability for dummies,,"I am the least mathematical person around, so apologies if this question is really dumb, but I'm trying to improve! I've been reading loads of examples everywhere but I'm having a hard time applying the logic/rules of probability to new problems. Let's say you have to win three out of three rounds of a game in order to win a prize. It is a single player game. The probability of a boy winning a round is $.25$ , and the probability of a girl winning a round is .4. Winning one round doesn't influence the result of the next round. So if I haven't misunderstood, the probability of a girl winning a prize is $.4 \cdot .4 \cdot .4 = 0.064$ and the probability of a boy winning a prize is $.25 \cdot .25 \cdot .25 = 0.016$ Now, this is where I'm stuck. What's the overall probability of a person winning a prize if $50\%$ of the players are girls and $50\%$ of the players are boys? Is it just $.016 + 0.064 = 0.08$ ? Or should I be dividing by $2$ here somewhere given that it's $50$ percent boys and $50$ percent girls. Thanks in advance for your help.","I am the least mathematical person around, so apologies if this question is really dumb, but I'm trying to improve! I've been reading loads of examples everywhere but I'm having a hard time applying the logic/rules of probability to new problems. Let's say you have to win three out of three rounds of a game in order to win a prize. It is a single player game. The probability of a boy winning a round is , and the probability of a girl winning a round is .4. Winning one round doesn't influence the result of the next round. So if I haven't misunderstood, the probability of a girl winning a prize is and the probability of a boy winning a prize is Now, this is where I'm stuck. What's the overall probability of a person winning a prize if of the players are girls and of the players are boys? Is it just ? Or should I be dividing by here somewhere given that it's percent boys and percent girls. Thanks in advance for your help.",.25 .4 \cdot .4 \cdot .4 = 0.064 .25 \cdot .25 \cdot .25 = 0.016 50\% 50\% .016 + 0.064 = 0.08 2 50 50,['probability']
83,Show that $R_{w}$ is bounded/compact using no arbitrage or non-redundancy arguments,Show that  is bounded/compact using no arbitrage or non-redundancy arguments,R_{w},"Consider a financial market with $d+1$ assets in a one-period model. The $0-$ th asset is considered the risk-free asset, the others are risky. The vector $\overline{\pi} \in \mathbb R^{d+1}$ denotes the initial price vector at $t=0$ , and the random vector $\overline{S} \in \mathbb R^{d+1}$ denotes prices at $t=1$ . Further we have $\pi^{0}=1,\; S^{0}=1+r$ , as the $0-$ th asset is risk-free. Here a strategy $\overline{\xi}\in \mathbb R^{d+1}$ is called an arbitrage opportunity if $\overline{\xi}⋅\overline{\pi}≤0$ but $\overline{\xi}⋅\overline{S}\geq 0$ and $\mathbb P(\overline{\xi}⋅\overline{S}> 0)>0$ . Further the market model is called non-redundant if $\overline{\xi}\cdot \overline{S}=0 \; \;\mathbb P\text{-a.s.}\implies \overline{\xi}=0$ . By the Law of One price we know that in an arbitrage-free model, for any $\overline{\rho},\; \overline{\xi}$ such that $\overline{\xi}\cdot\overline{S}=\overline{\rho}\cdot\overline{S}$ a.s. it must follow that $\overline{\xi}\cdot\overline{\pi}=\overline{\rho}\cdot\overline{\pi}$ . Now as in https://quant.stackexchange.com/q/17839/42184 we need to show that for $w>0$ , $$\mathcal{R}_{w}:=\{\overline{\xi}\in \mathbb R^{d+1}:\overline{\xi}\cdot \overline{\pi}=w,\;\overline{\xi}\cdot\overline{S}\geq 0\; \text{a.s.}\}\;\; \text{ is compact} $$ As we are in finite-dimensions, I assume that we need to use $\text{compact}\iff \text{closed and bounded}$ . Closedness is no problem at all, using convergence arguments. However, I am struggling to show boundedness. As I have not used the no arbitrage arguments or the non-redundancy condition, I assume that I will need to use them in my argument. Any ideas?","Consider a financial market with assets in a one-period model. The th asset is considered the risk-free asset, the others are risky. The vector denotes the initial price vector at , and the random vector denotes prices at . Further we have , as the th asset is risk-free. Here a strategy is called an arbitrage opportunity if but and . Further the market model is called non-redundant if . By the Law of One price we know that in an arbitrage-free model, for any such that a.s. it must follow that . Now as in https://quant.stackexchange.com/q/17839/42184 we need to show that for , As we are in finite-dimensions, I assume that we need to use . Closedness is no problem at all, using convergence arguments. However, I am struggling to show boundedness. As I have not used the no arbitrage arguments or the non-redundancy condition, I assume that I will need to use them in my argument. Any ideas?","d+1 0- \overline{\pi} \in \mathbb R^{d+1} t=0 \overline{S} \in \mathbb R^{d+1} t=1 \pi^{0}=1,\; S^{0}=1+r 0- \overline{\xi}\in \mathbb R^{d+1} \overline{\xi}⋅\overline{\pi}≤0 \overline{\xi}⋅\overline{S}\geq 0 \mathbb P(\overline{\xi}⋅\overline{S}> 0)>0 \overline{\xi}\cdot \overline{S}=0 \; \;\mathbb P\text{-a.s.}\implies \overline{\xi}=0 \overline{\rho},\; \overline{\xi} \overline{\xi}\cdot\overline{S}=\overline{\rho}\cdot\overline{S} \overline{\xi}\cdot\overline{\pi}=\overline{\rho}\cdot\overline{\pi} w>0 \mathcal{R}_{w}:=\{\overline{\xi}\in \mathbb R^{d+1}:\overline{\xi}\cdot \overline{\pi}=w,\;\overline{\xi}\cdot\overline{S}\geq 0\; \text{a.s.}\}\;\; \text{ is compact}  \text{compact}\iff \text{closed and bounded}","['real-analysis', 'probability', 'probability-theory', 'convergence-divergence', 'compactness']"
84,Mean and variance of $N=\min\{{n\geq1|X_1+X_2+ \cdots +X_n > x}\}$,Mean and variance of,N=\min\{{n\geq1|X_1+X_2+ \cdots +X_n > x}\},"Let $X_r (r \geq 1)$ be an independent random variable that is uniformly distributed over $[0,1]$ and $x$ be a number between $0$ and $1$ . $N$ is defined as follows: $$N= \min\{{n \geq 1 | X_1 + X_2 + \cdots + X_n > x}\}$$ prove that for any $x$ , $P( N > n ) = \frac {x^n}{n!}$ And calculate the mean and the variance of $N$ . I am self-studying and I found this problem in my book. I have tried various methods but have gotten nowhere so far. I would really appreciate some help.","Let be an independent random variable that is uniformly distributed over and be a number between and . is defined as follows: prove that for any , And calculate the mean and the variance of . I am self-studying and I found this problem in my book. I have tried various methods but have gotten nowhere so far. I would really appreciate some help.","X_r (r \geq 1) [0,1] x 0 1 N N= \min\{{n \geq 1 | X_1 + X_2 + \cdots + X_n > x}\} x P( N > n ) = \frac {x^n}{n!} N","['probability', 'conditional-probability', 'uniform-distribution']"
85,Calculate integral $\int^{+\infty}_0 \frac{e^{-x^2}}{(x^2+\frac{1}{2})^2} dx$?,Calculate integral ?,\int^{+\infty}_0 \frac{e^{-x^2}}{(x^2+\frac{1}{2})^2} dx,"I've posted a similar integral earlier, in which the Goodwin-Staton Integral is involved, making the integral unsolvable. Now I make a little modification to make it solvable and give my answer below.","I've posted a similar integral earlier, in which the Goodwin-Staton Integral is involved, making the integral unsolvable. Now I make a little modification to make it solvable and give my answer below.",,"['probability', 'integration']"
86,"The probability of choosing a coin out of two different coins, one with tail on both faces","The probability of choosing a coin out of two different coins, one with tail on both faces",,"Sue has two coins. One is fair, with a head on one face and a tail on the other. The second is a trick coin and has a tail on both faces. Sue picks up one of the coins at random and flips it. a) Find the probability that it lands heads up. b) Given that it lands tails up , find the probability that she picked up the fair coin. My turn: a) We have one head out of three tails and one head, so the answer is $\frac{1}{4}$ . b) I do not understand how can i start with this?!","Sue has two coins. One is fair, with a head on one face and a tail on the other. The second is a trick coin and has a tail on both faces. Sue picks up one of the coins at random and flips it. a) Find the probability that it lands heads up. b) Given that it lands tails up , find the probability that she picked up the fair coin. My turn: a) We have one head out of three tails and one head, so the answer is . b) I do not understand how can i start with this?!",\frac{1}{4},"['probability', 'bayes-theorem']"
87,How to calculate the expected value of the differences between nearest ordered values?,How to calculate the expected value of the differences between nearest ordered values?,,"Imagine I generate $N$ real numbers with a uniform distribution between $0$ and $1$ . I sort them in ascending order. And I calculate the differences between each consecutive pair. For example, for $N = 3$ , it would be like this: I would like to know what is the expected value of that differences, $\Delta$ . Each pair will have a different $\Delta$ but I'm just interested on the average expected value of all $\Delta$ . As I don't know how to calculate it with equations I've done it with a simulation instead (I'm not mathematician nor statistician, I just work with computers). And what I've gotten is: if I have $N$ numbers the average distance between them is $\frac1{1+N}$ , and that's also the value between the first number and zero. I would like to know how to calculate this with equations. Intuitively I think it's the same as calculating $E\left[|X_i-X_j|\right]$ where $X_i$ and $X_j$ are two neighboring numbers in that sample. In general the expected value is calculated as: $$E[X]=\int_{-\infty}^\infty xf(x)\,dx$$ I think here we should integrate $|X_i-X_j|$ but I don't know $f(x)$ , the distribution of the differences, because I can't assume they are independent because we have to sort them and take the nearest pairs. And the absolute value complicates calculations a little bit more. There is an apparently similar question here but they are speaking about the minimum distance among all pairs.","Imagine I generate real numbers with a uniform distribution between and . I sort them in ascending order. And I calculate the differences between each consecutive pair. For example, for , it would be like this: I would like to know what is the expected value of that differences, . Each pair will have a different but I'm just interested on the average expected value of all . As I don't know how to calculate it with equations I've done it with a simulation instead (I'm not mathematician nor statistician, I just work with computers). And what I've gotten is: if I have numbers the average distance between them is , and that's also the value between the first number and zero. I would like to know how to calculate this with equations. Intuitively I think it's the same as calculating where and are two neighboring numbers in that sample. In general the expected value is calculated as: I think here we should integrate but I don't know , the distribution of the differences, because I can't assume they are independent because we have to sort them and take the nearest pairs. And the absolute value complicates calculations a little bit more. There is an apparently similar question here but they are speaking about the minimum distance among all pairs.","N 0 1 N = 3 \Delta \Delta \Delta N \frac1{1+N} E\left[|X_i-X_j|\right] X_i X_j E[X]=\int_{-\infty}^\infty xf(x)\,dx |X_i-X_j| f(x)","['probability', 'statistics', 'expected-value', 'order-statistics']"
88,Joint distribution of random variable with an order statistic,Joint distribution of random variable with an order statistic,,"Let $X_1,...,X_n$ be i.i.d. with the distribution of the $X_i$ being nice and continuous.  I'm interested in the expression of the CDF $F_{X_{(1)},X_j}(u,v)$ . To be clear $X_{(1)} = min(X_i)$ and $X_j$ just one of the n i.i.d. random variables. I'm wondering if the following derivation is correct? $$F_{X_{(1)},X_j}(u,v) = P(X_{(1)}\leq u, X_j\leq v)$$ Partition the probability by whether or not $X_j$ is the minimum $$=P(X_{(1)}\leq u,X_j\leq v,X_{(1)}= X_j)+P(X_{(1)}\leq u,X_j\leq v,X_{(1)}\neq X_j)$$ The first probability reduces down as follows $$P(X_{(1)}\leq u,X_j\leq v,X_{(1)}= X_j)$$ $$=P(X_{(1)}\leq min(u,v), X_{(1)}= X_j)$$ The above is the probability that $X_j$ is minimum of the n i.i.d. random variables and it is less than both $u$ and $v$ . Since the distribution of the $X_i$ is continuous we can compute this directly as $$=\int_{-\infty}^{min(u,v)}{f_X(x)[1-F_X(x)]^{n-1}dx}$$ $$=\frac{1-[1-F_X(min(u,v))]^{n}}{n}$$ The last equality comes from integrating by substitution. Returning to the other probability we have $$P(X_{(1)}\leq u,X_j\leq v,X_{(1)}\neq X_j)$$ $$=P(X_{(1)}\leq min(u,v),X_j\leq v,X_{(1)}\neq X_j)$$ Since the $X_i$ are i.i.d., the above is equivalent to the probability that $$P(X_j\leq v, X'_{(1)}\leq min(u,v),X'_{(1)}\leq X_j )$$ where $X'_{(1)}$ is the minimum of the other $n-1$ random variables, so that $X_j$ and $X'_{(1)}$ are independent. The probability above can be written as $$\int_{-\infty}^{min(u,v)}{[F_X(v)-F_X(x)]f_{X'_{(1)}}(x)dx}$$ Using the fact that $f_{X'_{(1)}}(x)=(n-1)f_X(x)[1-F_X(x)]^{n-2}$ we have the the above probability can be computed as $$\int_{-\infty}^{min(u,v)}{[F_X(v)-F_X(x)](n-1)f_X(x)[1-F_X(x)]^{n-2}dx}$$ To compute the above integral, let $s=min(u,v), b=F_X(v),$ and use the substitution $y=1-F_X(x)$ so that the above integral becomes $$\int_{1}^{s}{-(n-1)(b-1+y)(y)^{n-2}dy}$$ $$=-(n-1)\int_{1}^{s}{[(b-1)y^{n-2}+y^{n-1}]dy}$$ $$=-(n-1)\int_{1}^{s}{[(b-1)y^{n-2}+y^{n-1}]dy}$$ $$=(n-1)\int_{1}^{s}{[(1-b)y^{n-2}-y^{n-1}]dy}$$ $$=(1-b)y^{n-1}-y^{n}\frac{(n-1)}{n}\Big|_1^s$$ $$=[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+\frac{(n-1)}{n}(1-[1-F_X(s)]^{n})$$ So, both parts together give us $$=\frac{1-[1-F_X(s)]^{n}}{n}+[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+\frac{(n-1)(1-[1-F_X(s)]^{n})}{n}$$ $$=[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+(1-[1-F_X(s)]^{n})$$ and finally, with $s=min(u,v)$ : $$F_{X_{(1)},X_j}(u,v) = [1-F_X(v)]([1-F_X(s))]^{n-1}-1)+(1-[1-F_X(s)]^{n})$$","Let be i.i.d. with the distribution of the being nice and continuous.  I'm interested in the expression of the CDF . To be clear and just one of the n i.i.d. random variables. I'm wondering if the following derivation is correct? Partition the probability by whether or not is the minimum The first probability reduces down as follows The above is the probability that is minimum of the n i.i.d. random variables and it is less than both and . Since the distribution of the is continuous we can compute this directly as The last equality comes from integrating by substitution. Returning to the other probability we have Since the are i.i.d., the above is equivalent to the probability that where is the minimum of the other random variables, so that and are independent. The probability above can be written as Using the fact that we have the the above probability can be computed as To compute the above integral, let and use the substitution so that the above integral becomes So, both parts together give us and finally, with :","X_1,...,X_n X_i F_{X_{(1)},X_j}(u,v) X_{(1)} = min(X_i) X_j F_{X_{(1)},X_j}(u,v) = P(X_{(1)}\leq u, X_j\leq v) X_j =P(X_{(1)}\leq u,X_j\leq v,X_{(1)}= X_j)+P(X_{(1)}\leq u,X_j\leq v,X_{(1)}\neq X_j) P(X_{(1)}\leq u,X_j\leq v,X_{(1)}= X_j) =P(X_{(1)}\leq min(u,v), X_{(1)}= X_j) X_j u v X_i =\int_{-\infty}^{min(u,v)}{f_X(x)[1-F_X(x)]^{n-1}dx} =\frac{1-[1-F_X(min(u,v))]^{n}}{n} P(X_{(1)}\leq u,X_j\leq v,X_{(1)}\neq X_j) =P(X_{(1)}\leq min(u,v),X_j\leq v,X_{(1)}\neq X_j) X_i P(X_j\leq v, X'_{(1)}\leq min(u,v),X'_{(1)}\leq X_j ) X'_{(1)} n-1 X_j X'_{(1)} \int_{-\infty}^{min(u,v)}{[F_X(v)-F_X(x)]f_{X'_{(1)}}(x)dx} f_{X'_{(1)}}(x)=(n-1)f_X(x)[1-F_X(x)]^{n-2} \int_{-\infty}^{min(u,v)}{[F_X(v)-F_X(x)](n-1)f_X(x)[1-F_X(x)]^{n-2}dx} s=min(u,v), b=F_X(v), y=1-F_X(x) \int_{1}^{s}{-(n-1)(b-1+y)(y)^{n-2}dy} =-(n-1)\int_{1}^{s}{[(b-1)y^{n-2}+y^{n-1}]dy} =-(n-1)\int_{1}^{s}{[(b-1)y^{n-2}+y^{n-1}]dy} =(n-1)\int_{1}^{s}{[(1-b)y^{n-2}-y^{n-1}]dy} =(1-b)y^{n-1}-y^{n}\frac{(n-1)}{n}\Big|_1^s =[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+\frac{(n-1)}{n}(1-[1-F_X(s)]^{n}) =\frac{1-[1-F_X(s)]^{n}}{n}+[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+\frac{(n-1)(1-[1-F_X(s)]^{n})}{n} =[1-F_X(v)]([1-F_X(s)]^{n-1}-1)+(1-[1-F_X(s)]^{n}) s=min(u,v) F_{X_{(1)},X_j}(u,v) = [1-F_X(v)]([1-F_X(s))]^{n-1}-1)+(1-[1-F_X(s)]^{n})",['probability']
89,What is the expected number of distinct strings from a single edit operation?,What is the expected number of distinct strings from a single edit operation?,,"Consider a binary string of length $n$ .   An edit operation is a single character insert, delete or substitution.  Given a string $S$ , my question relates to the number of distinct strings that can be made by a single edit operation performed on $S$ . Let us write $f(S)$ for the number of distinct strings that can be made by performing a single edit operation on $S$ . For example, if $S = 1111011010$ , then $f(S) = 28$ . Let $X$ be a random variable representing a random binary string of length $n$ , with the bits chosen uniformly and independently.  My question is what is: $$\mathbb{E}(f(X))\;?$$","Consider a binary string of length .   An edit operation is a single character insert, delete or substitution.  Given a string , my question relates to the number of distinct strings that can be made by a single edit operation performed on . Let us write for the number of distinct strings that can be made by performing a single edit operation on . For example, if , then . Let be a random variable representing a random binary string of length , with the bits chosen uniformly and independently.  My question is what is:",n S S f(S) S S = 1111011010 f(S) = 28 X n \mathbb{E}(f(X))\;?,['probability']
90,Probability of list having a pair of unchanged consecutive elements once ordered,Probability of list having a pair of unchanged consecutive elements once ordered,,"I've been grading exams for most of the day. Once I finished grading, I started entering the grades into my gradebook -- one by one, from top to bottom on the stack. About halfway through, I entered one students grade and the next student on the stack was also the next person alphabetically in the gradebook. What is the probability of this happening with $n$ students, all of whom have unique names? Equivalent question: For a random permutation $\left(a_1,a_2,\ldots,a_n\right)$ of the list $\left(1,2,\ldots,n\right)$ , what is the probability that there exists at least one entry $k$ of the permutation that is followed immediately by $k+1$ (that is, $k = a_i$ and $k+1 = a_{i+1}$ for some $i \in \left\{1,2,\ldots,n\right\}$ ) ? For small $n$ , it's not hard to exhaustively calculate the probability. But my combinatorics skills are rusty, and I don't think I can easily calculate this for my 30 students.","I've been grading exams for most of the day. Once I finished grading, I started entering the grades into my gradebook -- one by one, from top to bottom on the stack. About halfway through, I entered one students grade and the next student on the stack was also the next person alphabetically in the gradebook. What is the probability of this happening with students, all of whom have unique names? Equivalent question: For a random permutation of the list , what is the probability that there exists at least one entry of the permutation that is followed immediately by (that is, and for some ) ? For small , it's not hard to exhaustively calculate the probability. But my combinatorics skills are rusty, and I don't think I can easily calculate this for my 30 students.","n \left(a_1,a_2,\ldots,a_n\right) \left(1,2,\ldots,n\right) k k+1 k = a_i k+1 = a_{i+1} i \in \left\{1,2,\ldots,n\right\} n","['probability', 'combinatorics', 'permutations']"
91,"If the probability of a dog barking one or more times in a given hour is 84%, then what is the probability of a dog barking in 30 minutes?","If the probability of a dog barking one or more times in a given hour is 84%, then what is the probability of a dog barking in 30 minutes?",,"Poorly worded title but I don't know what the nature of this probability question is called. I was asked a question: If the probability of a dog barking one or more times in a given hour is 84%, then what is the probability of a dog barking in 30 minutes? Since I was told that the first solution I wanted to jump to of 42% is incorrect, I was then presented with the following steps: The chance of the dog not barking in a given hour is 1-84% = 16% If the chance of a dog not barking over the course of 2 units - 2 half hours for a total of one hour, then x * x = 16%. Thus, the probability that the does does NOT bark in 30 minutes is $\sqrt{16%}$ = 40%. Therefore, the probability of the dog barking in a given 30 minutes is 1-40% = 60%. Question1: Is this correct? Question2: Rather than work with the inverse probability 16%, surely I can apply the same logic with just 84% and arrive at the same answer? So, if the probability of the dog barking one or more times in an hour is 84%, then this 84% could also be represented as the probability of two 30 minute instances of a dog barking at least once in each instance. In that case: p(dog barks in 1st half hour AND dog barks in second half hour) = 84%. Thus the chance of the dog barking in the first half hour is is $\sqrt{0.84}$ = 91.65%. 91.65% does not equal 60% which is what I arrived at by going the negative probability route. I was expecting both numbers to match. What is the correct way to calculate the probability of a dog barking over 30 minutes if we know that the probability of the dog barking over an hour is 84%?","Poorly worded title but I don't know what the nature of this probability question is called. I was asked a question: If the probability of a dog barking one or more times in a given hour is 84%, then what is the probability of a dog barking in 30 minutes? Since I was told that the first solution I wanted to jump to of 42% is incorrect, I was then presented with the following steps: The chance of the dog not barking in a given hour is 1-84% = 16% If the chance of a dog not barking over the course of 2 units - 2 half hours for a total of one hour, then x * x = 16%. Thus, the probability that the does does NOT bark in 30 minutes is = 40%. Therefore, the probability of the dog barking in a given 30 minutes is 1-40% = 60%. Question1: Is this correct? Question2: Rather than work with the inverse probability 16%, surely I can apply the same logic with just 84% and arrive at the same answer? So, if the probability of the dog barking one or more times in an hour is 84%, then this 84% could also be represented as the probability of two 30 minute instances of a dog barking at least once in each instance. In that case: p(dog barks in 1st half hour AND dog barks in second half hour) = 84%. Thus the chance of the dog barking in the first half hour is is = 91.65%. 91.65% does not equal 60% which is what I arrived at by going the negative probability route. I was expecting both numbers to match. What is the correct way to calculate the probability of a dog barking over 30 minutes if we know that the probability of the dog barking over an hour is 84%?",\sqrt{16%} \sqrt{0.84},['probability']
92,Probability of matching 5 cards from a deck of 40,Probability of matching 5 cards from a deck of 40,,"Suppose we have a deck of 40 cards which has, 5 Aces, 6 Kings, 9 Queens, 20 Jacks. A game is played where a contestant will continuously draw cards until they have 5 matching cards (not necessarily in order). The cards are drawn without replacement. I'm trying to find the probability of the contestant getting the five matching cards they get are all Aces (i.e. they have to get this before they get any other five matching cards). Does anyone have any idea on how to approach this?","Suppose we have a deck of 40 cards which has, 5 Aces, 6 Kings, 9 Queens, 20 Jacks. A game is played where a contestant will continuously draw cards until they have 5 matching cards (not necessarily in order). The cards are drawn without replacement. I'm trying to find the probability of the contestant getting the five matching cards they get are all Aces (i.e. they have to get this before they get any other five matching cards). Does anyone have any idea on how to approach this?",,['probability']
93,Let $T$ be the number of tosses required until three consecutive heads appear for the first time. Find $\textbf{E}(T)$.,Let  be the number of tosses required until three consecutive heads appear for the first time. Find .,T \textbf{E}(T),"A fair coin is tossed repeatedly. Let $A_{n}$ be the event that three heads have appeared in consecutive tosses for the first time on the $n$ -th toss. Let $T$ be the number of tosses required until three consecutive heads appear for the first time. Find $\textbf{P}(A_{n})$ and $\textbf{E}(T)$ . Let $U$ be the number of tosses required until the sequence $HTH$ appears for the first time. Can you find $\textbf{E}(U)$ ? EDIT The textbook provides an answer based on recurrence equations, but I seek for an alternative approach if it is possible. Can somebody please help me out? Thanks in advance.","A fair coin is tossed repeatedly. Let be the event that three heads have appeared in consecutive tosses for the first time on the -th toss. Let be the number of tosses required until three consecutive heads appear for the first time. Find and . Let be the number of tosses required until the sequence appears for the first time. Can you find ? EDIT The textbook provides an answer based on recurrence equations, but I seek for an alternative approach if it is possible. Can somebody please help me out? Thanks in advance.",A_{n} n T \textbf{P}(A_{n}) \textbf{E}(T) U HTH \textbf{E}(U),"['probability', 'probability-theory', 'probability-distributions', 'expected-value']"
94,"Prove $E[X/Y]\ge1$ for X,Y iid positive random variables.","Prove  for X,Y iid positive random variables.",E[X/Y]\ge1,"Prove $E[X/Y]\ge1$ for X,Y iid positive random variables. My attempt: Let $Z=X/Y$ $Z\in(0,\infty)$ . $$E[Z]=\int_0^\infty P(Z\gt z)dz=\int_0^\infty P(Y\lt{1\over z}X)dz$$ $$=\int_0^\infty\int_0^\infty\int_0^{{1\over z}X}f(x)f(y)dydxdz$$ $$=\int_0^\infty\int_0^\infty f(x)F({1\over z}X)dxdz$$ $$=\int_0^\infty (F(x)F({1\over z}X)|_0^{\infty}-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz$$ $$=\int_0^\infty(1-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz$$ This is how I get 1 in the equation. I hope it has something to do with the question. I know its a mess. Can anyone help me out?","Prove for X,Y iid positive random variables. My attempt: Let . This is how I get 1 in the equation. I hope it has something to do with the question. I know its a mess. Can anyone help me out?","E[X/Y]\ge1 Z=X/Y Z\in(0,\infty) E[Z]=\int_0^\infty P(Z\gt z)dz=\int_0^\infty P(Y\lt{1\over z}X)dz =\int_0^\infty\int_0^\infty\int_0^{{1\over z}X}f(x)f(y)dydxdz =\int_0^\infty\int_0^\infty f(x)F({1\over z}X)dxdz =\int_0^\infty (F(x)F({1\over z}X)|_0^{\infty}-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz =\int_0^\infty(1-\int_0^\infty F(x)f({1\over z}X){1\over z}dx)dz",['probability']
95,What is the expected number of boys?,What is the expected number of boys?,,"If the probability that a child is a boy is $p$ ,  find the expected number of boys in a family with $n$ children given that there is at least one boy? My answer: My friend says the answer is $np+q$ but my answer is $np/(1-q^n)$ . I found my answer through conditional probability by finding the $P(X=k/X>0)$ . My friend tells that we can find the answer by finding the expected number of boys for $n-1$ children and then add $1$ to the expectation.  Which one is right?","If the probability that a child is a boy is ,  find the expected number of boys in a family with children given that there is at least one boy? My answer: My friend says the answer is but my answer is . I found my answer through conditional probability by finding the . My friend tells that we can find the answer by finding the expected number of boys for children and then add to the expectation.  Which one is right?",p n np+q np/(1-q^n) P(X=k/X>0) n-1 1,['probability']
96,What is the probability to get more heads in n-1 tosses than in n tosses ? (fair coin),What is the probability to get more heads in n-1 tosses than in n tosses ? (fair coin),,My approach so far is as follows: If $X_{n-2}$ is the number of heads in $n-2$ tosses then $E[X_{n-2}] = \frac{n-2}{2}$. So up to the $n-2$ toss it is all the same regardless if we end our experiment in $n$ or $n-1$ tosses. Then if we want to ensure that we get more heads in $n-1$ than in $n$ tosses: $P(X_{n-1} > X_{n}) = \frac{1}{2} \frac{1}{2.2} = \frac{1}{8}$ where $\frac{1}{2}$ is the probability of obtaining heads in the last toss out of $n-1$ and $\frac{1}{2.2}$ is the probability of obtaining two tails in the last two of $n$ tosses ? Does this make any sense ? And what is the general approach for problems like this ? Thanks in advance!,My approach so far is as follows: If $X_{n-2}$ is the number of heads in $n-2$ tosses then $E[X_{n-2}] = \frac{n-2}{2}$. So up to the $n-2$ toss it is all the same regardless if we end our experiment in $n$ or $n-1$ tosses. Then if we want to ensure that we get more heads in $n-1$ than in $n$ tosses: $P(X_{n-1} > X_{n}) = \frac{1}{2} \frac{1}{2.2} = \frac{1}{8}$ where $\frac{1}{2}$ is the probability of obtaining heads in the last toss out of $n-1$ and $\frac{1}{2.2}$ is the probability of obtaining two tails in the last two of $n$ tosses ? Does this make any sense ? And what is the general approach for problems like this ? Thanks in advance!,,['probability']
97,What are the odds of sitting next to the same person on two flights?,What are the odds of sitting next to the same person on two flights?,,"My wife left on a business trip this morning. 20 people from the same company caught two consecutive flights. Each person checked in independently, yet my wife ended up sitting next to the same colleague on both flights! What are the odds? Assume both aeroplanes had 150 seats, in 3+3 configuration, in 25 rows. It's not exactly right but will do for the purposes of the exercise. Assume also that all other passengers checked in independently, so there are no couples choosing or being assigned seats next to each other, thus changing the odds. This isn't right either, but will also do for the purposes of the exercise. My wife and colleague were sitting next to each other, not across the aisle from each other.","My wife left on a business trip this morning. 20 people from the same company caught two consecutive flights. Each person checked in independently, yet my wife ended up sitting next to the same colleague on both flights! What are the odds? Assume both aeroplanes had 150 seats, in 3+3 configuration, in 25 rows. It's not exactly right but will do for the purposes of the exercise. Assume also that all other passengers checked in independently, so there are no couples choosing or being assigned seats next to each other, thus changing the odds. This isn't right either, but will also do for the purposes of the exercise. My wife and colleague were sitting next to each other, not across the aisle from each other.",,['probability']
98,Birthday problem: using $^nC_r$.,Birthday problem: using .,^nC_r,"In birthday problem say total number of people n < 365, then probability of all person having distinct birthday is given by, $$\frac{\text{total no. of ways of selecting $n$ numbers from $365$ without repetition}} {\text{total number of ways of selecting n object from 365 with repetition}}. $$ $$p = \frac{^{365}C_n}{^{365-1+n}C_n}$$ I know, that this is wrong, but don't know why. I want to know why is it wrong? Can this problem be solved using $^nC_r$ calculations, if not, why?","In birthday problem say total number of people n < 365, then probability of all person having distinct birthday is given by, $$\frac{\text{total no. of ways of selecting $n$ numbers from $365$ without repetition}} {\text{total number of ways of selecting n object from 365 with repetition}}. $$ $$p = \frac{^{365}C_n}{^{365-1+n}C_n}$$ I know, that this is wrong, but don't know why. I want to know why is it wrong? Can this problem be solved using $^nC_r$ calculations, if not, why?",,"['probability', 'combinatorics', 'probability-theory', 'binomial-coefficients', 'birthday']"
99,"Finding probability -picking at least one red, one blue and one green ball from an urn when six balls are selected","Finding probability -picking at least one red, one blue and one green ball from an urn when six balls are selected",,"Six balls are to be randomly chosen from an urn containing $8$ red, $10$ green, and 12 blue balls. What is the probability at least one red ball, one blue and one green ball is chosen? Sample space = $\binom{30}{6}$ P = 1 - P(All red + All Green + All Blue + Only red and Green + Only Red and Blue + Only Green and Blue ) $$P = \Large 1 - \frac{\binom{8}{6} + \binom{10}{6} + \binom{12}{6} + \binom{18}{6} + \binom{22}{6} + \binom{20}{6}}{\binom{30}{6}}$$ According to this, I got $\large 1 - \frac{133099}{593775}$,  which is $0.7758$? Is my approach correct?","Six balls are to be randomly chosen from an urn containing $8$ red, $10$ green, and 12 blue balls. What is the probability at least one red ball, one blue and one green ball is chosen? Sample space = $\binom{30}{6}$ P = 1 - P(All red + All Green + All Blue + Only red and Green + Only Red and Blue + Only Green and Blue ) $$P = \Large 1 - \frac{\binom{8}{6} + \binom{10}{6} + \binom{12}{6} + \binom{18}{6} + \binom{22}{6} + \binom{20}{6}}{\binom{30}{6}}$$ According to this, I got $\large 1 - \frac{133099}{593775}$,  which is $0.7758$? Is my approach correct?",,"['probability', 'combinatorics']"
