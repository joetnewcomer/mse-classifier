,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,A characterization of trace class operators,A characterization of trace class operators,,"Let $H$ be a separable Hilbert space and let $T\in B(H)$, such that $\displaystyle \sum_{j=1}^\infty\langle T\xi_j,\eta_j\rangle$ converges for any choice of orthonormal bases $\{\xi_j\}$, $\{\eta_j\}$. Does this imply that $T$ is trace-class? I think it is, but I couldn't really write a proof.","Let $H$ be a separable Hilbert space and let $T\in B(H)$, such that $\displaystyle \sum_{j=1}^\infty\langle T\xi_j,\eta_j\rangle$ converges for any choice of orthonormal bases $\{\xi_j\}$, $\{\eta_j\}$. Does this imply that $T$ is trace-class? I think it is, but I couldn't really write a proof.",,"['functional-analysis', 'operator-theory']"
1,Question about proof that multiplication in Banach algebra is continuous,Question about proof that multiplication in Banach algebra is continuous,,"Here's the proof in my notes: Where does the last inequality come from? If I want to show that it's continuous at $((x,y)$ I can use the inverse triangle inequality to get  $$ (\|x^\prime\| + \|y\|)\varepsilon \leq (\|x\| + \|y \| + \varepsilon)\varepsilon$$ Thanks.","Here's the proof in my notes: Where does the last inequality come from? If I want to show that it's continuous at $((x,y)$ I can use the inverse triangle inequality to get  $$ (\|x^\prime\| + \|y\|)\varepsilon \leq (\|x\| + \|y \| + \varepsilon)\varepsilon$$ Thanks.",,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
2,Completeness and Fourier series convergence,Completeness and Fourier series convergence,,"Consider the question: In an inner product space $V$, when does the Fourier series of $x$, $\sum\limits_{n=1}^k\langle e_n,x\rangle e_n$ converges to $x$ as $k\to\infty$? Well, certainly is converges for all $x$ if $V$ is a Hilbert space. But what if $V$ is not a Hilbert space? Is the completeness property that a Hilbert space possesses necessary to ensure the Fourier series converges in $V$ for all $x\in V$? Might there be incomplete inner product space $V$, e.g. maybe $c_{00}$ - sequences in $\mathbb{F}$ with finitely many non-zero entries, along with associated inner product $\langle(a_n),(b_n)\rangle = \sum\limits_{n=1}^\infty a_n \overline{b_n}$. Then we choose the obviously countable orthonormal basis. Isn't it true that for each $x$ the Fourier series converges? If this is the case then what can we say about such inner product space whose Fourier series of $x$ converges to any given $x$ in the space, assuming the space is not a Hilbert space, i.e. not complete? Is the point that these spaces are arbitrary and the completeness property will guarantee us Fourier convergence? Perhaps Hilbert spaces are, in turn, easier to deal with in general because they guarantee us nice properties. Also (more basic question), can we always find an ONB of a well-defined inner product space? Edit : I just realized Gram-Schmidt does this for us in all Hilbert spaces. I guess the question still remains for incomplete inner product spaces. Apologies for my lack of LaTeX skills.... So yeah lots of questions. Hopefully someone can tell me if I am on the right lines of thought. Thanks","Consider the question: In an inner product space $V$, when does the Fourier series of $x$, $\sum\limits_{n=1}^k\langle e_n,x\rangle e_n$ converges to $x$ as $k\to\infty$? Well, certainly is converges for all $x$ if $V$ is a Hilbert space. But what if $V$ is not a Hilbert space? Is the completeness property that a Hilbert space possesses necessary to ensure the Fourier series converges in $V$ for all $x\in V$? Might there be incomplete inner product space $V$, e.g. maybe $c_{00}$ - sequences in $\mathbb{F}$ with finitely many non-zero entries, along with associated inner product $\langle(a_n),(b_n)\rangle = \sum\limits_{n=1}^\infty a_n \overline{b_n}$. Then we choose the obviously countable orthonormal basis. Isn't it true that for each $x$ the Fourier series converges? If this is the case then what can we say about such inner product space whose Fourier series of $x$ converges to any given $x$ in the space, assuming the space is not a Hilbert space, i.e. not complete? Is the point that these spaces are arbitrary and the completeness property will guarantee us Fourier convergence? Perhaps Hilbert spaces are, in turn, easier to deal with in general because they guarantee us nice properties. Also (more basic question), can we always find an ONB of a well-defined inner product space? Edit : I just realized Gram-Schmidt does this for us in all Hilbert spaces. I guess the question still remains for incomplete inner product spaces. Apologies for my lack of LaTeX skills.... So yeah lots of questions. Hopefully someone can tell me if I am on the right lines of thought. Thanks",,"['functional-analysis', 'fourier-series']"
3,Bounded operator from a Hilbert space to $\ell^1$ is compact,Bounded operator from a Hilbert space to  is compact,\ell^1,"Let $H$ be any Hilbert space. How can we prove that any bounded linear operator $T\colon H \to \ell^1$ is compact? If we use the fact that the space $\ell^1$ has Schur property (norm and weak convergence is the same), then we need to show that for a sequence $(x_n) \subset H$ such that $||x_n|| \leq 1$ the sequence $(Tx_n) \subset \ell^1$ contains weakly convergent subsequence. But I do not know how I should do this. What property of a Hilbert spaces I need to use?","Let $H$ be any Hilbert space. How can we prove that any bounded linear operator $T\colon H \to \ell^1$ is compact? If we use the fact that the space $\ell^1$ has Schur property (norm and weak convergence is the same), then we need to show that for a sequence $(x_n) \subset H$ such that $||x_n|| \leq 1$ the sequence $(Tx_n) \subset \ell^1$ contains weakly convergent subsequence. But I do not know how I should do this. What property of a Hilbert spaces I need to use?",,"['functional-analysis', 'banach-spaces', 'hilbert-spaces', 'operator-theory']"
4,Quadratic minimization in a Hilbert space,Quadratic minimization in a Hilbert space,,"If $A$ is a positive definite matrix, then the solution to the minimization problem $(1/2)x^TAx - b^Tx$ is given by $A^{-1}b$.  I'm interested in the generalization of this to a Hilbert space.  What conditions on $A$ should be required for this to be true in a Hilbert space?  Also is there a reference that includes this? I searched Google books, but couldn't find it.","If $A$ is a positive definite matrix, then the solution to the minimization problem $(1/2)x^TAx - b^Tx$ is given by $A^{-1}b$.  I'm interested in the generalization of this to a Hilbert space.  What conditions on $A$ should be required for this to be true in a Hilbert space?  Also is there a reference that includes this? I searched Google books, but couldn't find it.",,"['functional-analysis', 'optimization', 'hilbert-spaces', 'calculus-of-variations']"
5,Spectrum of a convolution operator,Spectrum of a convolution operator,,Let $T$ be the operator from $L^2(\mathbb R^n)$ to $L^2(\mathbb R^n)$ that is given by $Tf := f * g$ where $g$ is in $L^2$. How do I now find that the spectrum of $T$ is equal to the essential range of $\hat{g}$? How is the spectrum of $T$ related to the invertibility of the operator $G\hat{f} = \hat{f}\hat{g}$? The hat denotes the Fourier transform.,Let $T$ be the operator from $L^2(\mathbb R^n)$ to $L^2(\mathbb R^n)$ that is given by $Tf := f * g$ where $g$ is in $L^2$. How do I now find that the spectrum of $T$ is equal to the essential range of $\hat{g}$? How is the spectrum of $T$ related to the invertibility of the operator $G\hat{f} = \hat{f}\hat{g}$? The hat denotes the Fourier transform.,,"['functional-analysis', 'fourier-analysis']"
6,"Why is our definition of ""smooth"" function on a totally disconnected space the ""right"" definition?","Why is our definition of ""smooth"" function on a totally disconnected space the ""right"" definition?",,"My main interest in this comes from starting to learn about representations of totally disconnected, locally compact topological groups $G$ , and their representations. I came across the definition of ""smooth"" functions $\phi:G\to \mathbb{C}$ as being those which are locally constant, and smooth representations $G\to\text{End}(V)$ being those for which $\text{Stab}_G(v)$ is open for all $v\in V$ . These definitions lead me to two questions, which I suspect do not have technically precise answers, but for which I'm hoping there exists some good heuristic motivations. Why is the definition of ""smooth"" for functions $\phi:G\to\mathbb{C}$ the ""right"" definition?  That is, why is this considered to be the notion of functions on totally disconnected spaces that is analogous to the classic notion of smooth functions $f:\mathbb{R}\to\mathbb{R}$ ? in the same manner, why is the definition of smooth representations as above, the ""right"" analogue of smooth representations of Lie groups? These don't seem totally alien to me given the nature of the topology of totally disconnected spaces, but I'm hoping someone can spin a narrative more convincing than just saying the definitions involved are very ""pointy"". Also, I suspect that the answer to 2) will follow more or less directly from any reasonable answer to 1). I'm happy to simply be directed to a source that has already laid this out.","My main interest in this comes from starting to learn about representations of totally disconnected, locally compact topological groups , and their representations. I came across the definition of ""smooth"" functions as being those which are locally constant, and smooth representations being those for which is open for all . These definitions lead me to two questions, which I suspect do not have technically precise answers, but for which I'm hoping there exists some good heuristic motivations. Why is the definition of ""smooth"" for functions the ""right"" definition?  That is, why is this considered to be the notion of functions on totally disconnected spaces that is analogous to the classic notion of smooth functions ? in the same manner, why is the definition of smooth representations as above, the ""right"" analogue of smooth representations of Lie groups? These don't seem totally alien to me given the nature of the topology of totally disconnected spaces, but I'm hoping someone can spin a narrative more convincing than just saying the definitions involved are very ""pointy"". Also, I suspect that the answer to 2) will follow more or less directly from any reasonable answer to 1). I'm happy to simply be directed to a source that has already laid this out.",G \phi:G\to \mathbb{C} G\to\text{End}(V) \text{Stab}_G(v) v\in V \phi:G\to\mathbb{C} f:\mathbb{R}\to\mathbb{R},"['functional-analysis', 'representation-theory', 'p-adic-number-theory', 'smooth-functions', 'langlands-program']"
7,"Closures, intersections and sums of subspaces in Hilbert space","Closures, intersections and sums of subspaces in Hilbert space",,"Let $X$ be a real Hilbert space, and let $U$ and $V$ be two closed linear subspaces of $X$ . Is it true that $$\overline{(U+V) \cap (U^\perp+V^\perp)} = \overline{U+V} \cap \overline{U^\perp + V^\perp}\quad?$$ The left-hand side is clearly a subset of the right-hand side, but the opposite inclusion stumps me. (The result is clearly true if $X$ is finite-dimensional because all subspaces are automatically closed.) I checked my trusted Functional Analysis book, math.stackexchange, as well as Halmos' A Hilbert Space Problem book but couldn't find anything. This should be known! Please provide a reference or thought. Thanks!","Let be a real Hilbert space, and let and be two closed linear subspaces of . Is it true that The left-hand side is clearly a subset of the right-hand side, but the opposite inclusion stumps me. (The result is clearly true if is finite-dimensional because all subspaces are automatically closed.) I checked my trusted Functional Analysis book, math.stackexchange, as well as Halmos' A Hilbert Space Problem book but couldn't find anything. This should be known! Please provide a reference or thought. Thanks!",X U V X \overline{(U+V) \cap (U^\perp+V^\perp)} = \overline{U+V} \cap \overline{U^\perp + V^\perp}\quad? X,"['functional-analysis', 'reference-request', 'hilbert-spaces']"
8,Is there hope for Einstein tensor notation in Quantum Mechanics?,Is there hope for Einstein tensor notation in Quantum Mechanics?,,"The Core Question How would one define and justify the tensor notation for vectors and operators on a separable Hilbert space? Motivation Whenever I work with finite-dimensional linear algebra, I find it very illuminating to think about vectors, linear maps, bilinear forms etc. collectively as just various types of tensors. I like how the tensor formalism puts emphasis on representation-free formulations and how even a very complicated multi-linear map can be understood in terms of its individual „indices“. When I started to learn Quantum Mechanics, I immediately tried to use the tensor notation. At a surface level it seemed to work just fine. One can formally rewrite the bra-ket expressions on left with a „tensor-like“ notation on the right: $$ \begin{aligned}   \left< \psi \mid \varphi \right>   \qquad &\leftrightsquigarrow \qquad   \overline{\psi^{\,\mu}} \; g_{\overline{\mu} \nu} \; \varphi^\nu   \\   \left| \psi \right> = \hat A \hat B \left| \varphi \right>   \qquad &\leftrightsquigarrow \qquad   \psi^{\,\mu} = A^\mu_\nu \; B^\nu_\kappa \; \varphi^\kappa   \\   W = \left| \psi \right>\left< \psi\right|, \;   \operatorname{Tr} W = 1   \qquad &\leftrightsquigarrow \qquad   W^\mu_\nu = \psi^{\,\mu} \; g_{\overline{\kappa}\nu} \; \overline{\psi^\kappa}, \;\;   W^\mu_\mu = 1 \end{aligned} $$ Furthermore, the tensor notation feels more natural when dealing with composite systems, where the state is a tensor product of states of the underlying systems. However, when I tried to put the notation on a more rigorous ground, I learned that the topic is much more difficult and nuanced than I thought. The Problem In finite-dimensional spaces, the tensor notation is made possible by several circumstances: \begin{gather} V^* \otimes V \simeq \operatorname{Hom}(V, V) \label{homomorphism} \tag{1} \\[5pt] T(\mathbf{v}) = T( \; \sum_k v^k \mathbf{e}_k \;) = \sum_k v^k \; T(\mathbf{e}_k) \label{continuity} \tag{2} \\ \big| \operatorname{Tr}(T) \, \big| < \infty \label{trace} \tag{3} \end{gather} While \eqref{homomorphism} essentially makes sure that all $k$ -covariant $l$ -contravariant tensors are from the same space, \eqref{continuity} lets us describe all tensors using their coefficients wrt. some basis and \eqref{trace} lets us express all operations on tensors using just tensor product and contraction. In a sense, neither of these is true for the separable Hilbert space $\mathcal H$ where QM is done. By a basis on a Hilbert space one usually means the Schauder basis , which describes vectors with an infinite series of coefficients. This, combined with the fact that most interesting operators in QM aren't continuous, gives us: $$   T(\mathbf{v})   = T( \; \sum_{k=1}^\infty v^k \mathbf{e}_k \;)   = T( \; \lim_{N\to\infty}\sum_{k=1}^N v^k \mathbf{e}_k \;)   \neq \lim_{N\to\infty} T( \; \sum_{k=1}^N v^k \mathbf{e}_k \;)   = \sum_{k=1}^\infty v^k \; T(\mathbf{e}_k) $$ The sequence on the RHS might either diverge, or even converge to a different value than LHS (although this is pathological and usually not considered in practise). Condition \eqref{trace} only holds for the so-called trace-class operators (not even the identity is trace-class) and the space $\mathcal H^* \widehat{\otimes} \mathcal H$ ( tensor product of Hilbert spaces ) is isomorphic to finite-rank operators, a proper subset of $\operatorname{Hom}(\mathcal H, \mathcal H)$ . A Possible Solution? (and more questions) These unfortunate circumstances mean, that if it's even possible to justify the tensor notation in infinite-dimensional spaces, it can't be by a simple generalization of the finite-dimensional case. So, is it possible to make it work? The condition \eqref{continuity} seems relatively easy to fix – one just needs to replace the Schauder basis with a Hamel basis . A serious downside to this would be that a typical operator would have an uncountable number of coefficients. For \eqref{homomorphism} it would be great if one could find a space $\mathcal G$ , such that the Hilbert space $\mathcal H$ is embedded in it $\mathcal H \subset \mathcal G$ , with the property $\mathcal G \otimes \mathcal G \simeq \operatorname{Hom}(\mathcal H^*, \mathcal H)$ . Vectors of the bigger space could then be used to construct operators and bilinear forms. Here, one immediately thinks about the Gelfand triple $\Phi \subset \mathcal H \subset \Phi^*$ – could it be that $\mathcal G = \Phi^*$ for an appropriate choice of rigging? Sadly it appears that no, as the Schwartz kernel theorem says that $\Phi^* \otimes \Phi^* \simeq \operatorname{Hom}(\Phi, \Phi^*)$ . Is such a space $\mathcal G$ even possible? If yes, would this approach generalize to tensors of higher degree? Finally, the problem with \eqref{trace} seems to be there to stay. There is no reasonable way to define the trace of identity, for example. What I'm interested in is whether, given a complicated expression, one can tell a priori which indices are contractible and which will inevitably diverge. Are these proposed “solutions” any good? Is there any literature that would investigate this topic? Or am I on the wrong path and should I stop wasting my time with tensors in the infinite dimension?","The Core Question How would one define and justify the tensor notation for vectors and operators on a separable Hilbert space? Motivation Whenever I work with finite-dimensional linear algebra, I find it very illuminating to think about vectors, linear maps, bilinear forms etc. collectively as just various types of tensors. I like how the tensor formalism puts emphasis on representation-free formulations and how even a very complicated multi-linear map can be understood in terms of its individual „indices“. When I started to learn Quantum Mechanics, I immediately tried to use the tensor notation. At a surface level it seemed to work just fine. One can formally rewrite the bra-ket expressions on left with a „tensor-like“ notation on the right: Furthermore, the tensor notation feels more natural when dealing with composite systems, where the state is a tensor product of states of the underlying systems. However, when I tried to put the notation on a more rigorous ground, I learned that the topic is much more difficult and nuanced than I thought. The Problem In finite-dimensional spaces, the tensor notation is made possible by several circumstances: While \eqref{homomorphism} essentially makes sure that all -covariant -contravariant tensors are from the same space, \eqref{continuity} lets us describe all tensors using their coefficients wrt. some basis and \eqref{trace} lets us express all operations on tensors using just tensor product and contraction. In a sense, neither of these is true for the separable Hilbert space where QM is done. By a basis on a Hilbert space one usually means the Schauder basis , which describes vectors with an infinite series of coefficients. This, combined with the fact that most interesting operators in QM aren't continuous, gives us: The sequence on the RHS might either diverge, or even converge to a different value than LHS (although this is pathological and usually not considered in practise). Condition \eqref{trace} only holds for the so-called trace-class operators (not even the identity is trace-class) and the space ( tensor product of Hilbert spaces ) is isomorphic to finite-rank operators, a proper subset of . A Possible Solution? (and more questions) These unfortunate circumstances mean, that if it's even possible to justify the tensor notation in infinite-dimensional spaces, it can't be by a simple generalization of the finite-dimensional case. So, is it possible to make it work? The condition \eqref{continuity} seems relatively easy to fix – one just needs to replace the Schauder basis with a Hamel basis . A serious downside to this would be that a typical operator would have an uncountable number of coefficients. For \eqref{homomorphism} it would be great if one could find a space , such that the Hilbert space is embedded in it , with the property . Vectors of the bigger space could then be used to construct operators and bilinear forms. Here, one immediately thinks about the Gelfand triple – could it be that for an appropriate choice of rigging? Sadly it appears that no, as the Schwartz kernel theorem says that . Is such a space even possible? If yes, would this approach generalize to tensors of higher degree? Finally, the problem with \eqref{trace} seems to be there to stay. There is no reasonable way to define the trace of identity, for example. What I'm interested in is whether, given a complicated expression, one can tell a priori which indices are contractible and which will inevitably diverge. Are these proposed “solutions” any good? Is there any literature that would investigate this topic? Or am I on the wrong path and should I stop wasting my time with tensors in the infinite dimension?","
\begin{aligned}
  \left< \psi \mid \varphi \right>
  \qquad &\leftrightsquigarrow \qquad
  \overline{\psi^{\,\mu}} \; g_{\overline{\mu} \nu} \; \varphi^\nu
  \\
  \left| \psi \right> = \hat A \hat B \left| \varphi \right>
  \qquad &\leftrightsquigarrow \qquad
  \psi^{\,\mu} = A^\mu_\nu \; B^\nu_\kappa \; \varphi^\kappa
  \\
  W = \left| \psi \right>\left< \psi\right|, \;
  \operatorname{Tr} W = 1
  \qquad &\leftrightsquigarrow \qquad
  W^\mu_\nu = \psi^{\,\mu} \; g_{\overline{\kappa}\nu} \; \overline{\psi^\kappa}, \;\;
  W^\mu_\mu = 1
\end{aligned}
 \begin{gather}
V^* \otimes V \simeq \operatorname{Hom}(V, V)
\label{homomorphism} \tag{1} \\[5pt]
T(\mathbf{v}) = T( \; \sum_k v^k \mathbf{e}_k \;) = \sum_k v^k \; T(\mathbf{e}_k)
\label{continuity} \tag{2} \\
\big| \operatorname{Tr}(T) \, \big| < \infty
\label{trace} \tag{3}
\end{gather} k l \mathcal H 
  T(\mathbf{v})
  = T( \; \sum_{k=1}^\infty v^k \mathbf{e}_k \;)
  = T( \; \lim_{N\to\infty}\sum_{k=1}^N v^k \mathbf{e}_k \;)
  \neq \lim_{N\to\infty} T( \; \sum_{k=1}^N v^k \mathbf{e}_k \;)
  = \sum_{k=1}^\infty v^k \; T(\mathbf{e}_k)
 \mathcal H^* \widehat{\otimes} \mathcal H \operatorname{Hom}(\mathcal H, \mathcal H) \mathcal G \mathcal H \mathcal H \subset \mathcal G \mathcal G \otimes \mathcal G \simeq \operatorname{Hom}(\mathcal H^*, \mathcal H) \Phi \subset \mathcal H \subset \Phi^* \mathcal G = \Phi^* \Phi^* \otimes \Phi^* \simeq \operatorname{Hom}(\Phi, \Phi^*) \mathcal G","['functional-analysis', 'hilbert-spaces', 'tensors']"
9,Do invariant functions form a Banach (sub)manifold in function spaces?,Do invariant functions form a Banach (sub)manifold in function spaces?,,"Let $G$ be a topological group, and $X$ some function space; preferably a Sobolev space $X=W^{1,p}(\Omega)$ , where $\Omega \subset \mathbb{R}^n$ is some invariant subset ( $g\Omega \subset \Omega$ ) or the whole space. Let $G$ be represented as linear, continuous operators $\pi(g)$ on $X$ in the form $$ \pi(g)f(x)=f(gx) $$ Does the set of invariant functions that satisfy $$ f(gx)=f(x) $$ form a Banach (sub)manifold of $X$ ? Do we need additional assumptions on the group (e.g.compact groups)?  As an example consider $G=O(n)$ the orthogonal group. Does the set of radial symmetric functions form a Banach (sub)manifold of $X$ ? Feel free to modify and add assumptions if necessary as I am not too familiar with this field. I am thankful for any reference, hint or remark!","Let be a topological group, and some function space; preferably a Sobolev space , where is some invariant subset ( ) or the whole space. Let be represented as linear, continuous operators on in the form Does the set of invariant functions that satisfy form a Banach (sub)manifold of ? Do we need additional assumptions on the group (e.g.compact groups)?  As an example consider the orthogonal group. Does the set of radial symmetric functions form a Banach (sub)manifold of ? Feel free to modify and add assumptions if necessary as I am not too familiar with this field. I am thankful for any reference, hint or remark!","G X X=W^{1,p}(\Omega) \Omega \subset \mathbb{R}^n g\Omega \subset \Omega G \pi(g) X 
\pi(g)f(x)=f(gx)
 
f(gx)=f(x)
 X G=O(n) X","['group-theory', 'functional-analysis', 'reference-request', 'sobolev-spaces']"
10,Convert a general second order linear PDE into a weak form for the finite element method.,Convert a general second order linear PDE into a weak form for the finite element method.,,"Problem I want to convert the general second order linear PDE problem \begin{align} \begin{cases}  a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2} +c(x,y)\frac{\partial^2 u}{\partial x \partial y}\\+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y}+f(x,y)u=g(x,y) & \text{in } R \text{ PDE} \\ u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\ \dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\ \dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\ \end{cases}  \end{align} into a weak form suitable for the finite element method. That is into the weak bilinear form $B(u,v)=L(v)$ where $B$ is bilinear, symmetric and positive definite functional and $L$ is a linear functional. Work thus far I know to how convert the following \begin{align} \begin{cases}  \dfrac{\partial^2 u}{\partial x^2}+\dfrac{\partial^2 u}{\partial y^2}+u=g(x,y) & \text{in } R \text{ PDE} \\ u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\ \dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\ \dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\ \end{cases}  \end{align} into the weak bilinear form $B(u,v)=L(v)$ where $B$ is bilinear, symmetric and positive definite and $L$ is linear. The steps are as follows (note that $v$ is our test function) \begin{align} \int \int_{R} \left(\frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}+u \right) v  \ dA &= \int \int_{R} g(x,y)  v  \ dA  \end{align} Using the identity \begin{align} \int \int_{R} v \nabla^2 u\ dA &= \int_{S} v \frac{\partial u}{\partial n}\ ds-\int\int_{R} \nabla u \cdot \nabla v\ dA \end{align} We get \begin{align} \int \int_R -\nabla u \cdot \nabla v +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S} v \frac{\partial u}{\partial n}\ ds \\ \int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_1} v \frac{\partial u}{\partial n}\ ds- \int_{S_2} v \frac{\partial u}{\partial n}\ ds - \int_{S_3} v \frac{\partial u}{\partial n}\ ds \\ \int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v (r^*_1-r^*_2 u) \ ds \\ \int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  +\int_{S_3} r^*_2 uv \ ds \\ \int \int_R -\nabla u \cdot \nabla v  +uv  \ dA +\int_{S_3} r^*_2 uv \ ds  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  \\ B(u,v)&=L(v) \end{align} Where I am having trouble I do not know what to do with the terms $$c(x,y)\frac{\partial^2 u}{\partial x \partial y}+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y}$$ as using the divergence theorem/integration by parts used in the work thus far section leaves terms that are not symmetric and therefore does not not satisfy the requirements for $B(u,v)$ . The other problem are the terms $$a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2}$$ the identity that I used in the work thus far section does not work (I am probably wrong on this part). I could really use some guidance on both of these problems. Notes This question is part of a much larger problem in which I have to use the finite element method. Once the problem is in a weak form in which the finite element/galerkin method can be applied I know what to do. From what I know the symmetry of $B(u,v)$ is essential. If there is some other weak form that works with the finite element (that is suitable for a numerical solution), that would be an acceptable answer to my problem. I have been following ""Finite Elements: A Gentle Introduction"" I could not find anything in the book that answered the problem. If you have any references that covers my problem that would be great (so far I have found nothing). Have also posted my question here , to increase interest in my problem If you have any questions feel free to ask. Notation $n$ is the vector normal to the boundary surface. $u(x,y)$ is the solution to the given PDE or ODE. $v(x,y)$ is a test function. $\int \int_{R} * \ dA$ is an integral over region $R$ . $\int_{S} * ds$ is a surface integral over $S$ . $u^*, q^*, r^*_1, r^*_2, r^*_3$ are either constants or functions used to define the boundary conditions. The surface (S) boundary conditions can be divided into Dirchlet, Neumann, and Robin boundary conditions. That is $S=S_1\cup S_2 \cup S_3$ .","Problem I want to convert the general second order linear PDE problem into a weak form suitable for the finite element method. That is into the weak bilinear form where is bilinear, symmetric and positive definite functional and is a linear functional. Work thus far I know to how convert the following into the weak bilinear form where is bilinear, symmetric and positive definite and is linear. The steps are as follows (note that is our test function) Using the identity We get Where I am having trouble I do not know what to do with the terms as using the divergence theorem/integration by parts used in the work thus far section leaves terms that are not symmetric and therefore does not not satisfy the requirements for . The other problem are the terms the identity that I used in the work thus far section does not work (I am probably wrong on this part). I could really use some guidance on both of these problems. Notes This question is part of a much larger problem in which I have to use the finite element method. Once the problem is in a weak form in which the finite element/galerkin method can be applied I know what to do. From what I know the symmetry of is essential. If there is some other weak form that works with the finite element (that is suitable for a numerical solution), that would be an acceptable answer to my problem. I have been following ""Finite Elements: A Gentle Introduction"" I could not find anything in the book that answered the problem. If you have any references that covers my problem that would be great (so far I have found nothing). Have also posted my question here , to increase interest in my problem If you have any questions feel free to ask. Notation is the vector normal to the boundary surface. is the solution to the given PDE or ODE. is a test function. is an integral over region . is a surface integral over . are either constants or functions used to define the boundary conditions. The surface (S) boundary conditions can be divided into Dirchlet, Neumann, and Robin boundary conditions. That is .","\begin{align}
\begin{cases} 
a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2} +c(x,y)\frac{\partial^2 u}{\partial x \partial y}\\+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y}+f(x,y)u=g(x,y) & \text{in } R \text{ PDE} \\
u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\
\dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\
\dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\
\end{cases} 
\end{align} B(u,v)=L(v) B L \begin{align}
\begin{cases} 
\dfrac{\partial^2 u}{\partial x^2}+\dfrac{\partial^2 u}{\partial y^2}+u=g(x,y) & \text{in } R \text{ PDE} \\
u=u^* & \text{on } S_1 \text{ Dirchlet boundary condition} \\
\dfrac{\partial u}{\partial n}=q^* & \text{on } S_2 \text{ Neumann boundary condition} \\
\dfrac{\partial u}{\partial n}=r^*_1-r^*_2 u & \text{on } S_3 \text{ Robin boundary condition} \\
\end{cases} 
\end{align} B(u,v)=L(v) B L v \begin{align}
\int \int_{R} \left(\frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2}+u \right) v  \ dA &= \int \int_{R} g(x,y)  v  \ dA 
\end{align} \begin{align}
\int \int_{R} v \nabla^2 u\ dA &= \int_{S} v \frac{\partial u}{\partial n}\ ds-\int\int_{R} \nabla u \cdot \nabla v\ dA
\end{align} \begin{align}
\int \int_R -\nabla u \cdot \nabla v +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S} v \frac{\partial u}{\partial n}\ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_1} v \frac{\partial u}{\partial n}\ ds- \int_{S_2} v \frac{\partial u}{\partial n}\ ds - \int_{S_3} v \frac{\partial u}{\partial n}\ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v (r^*_1-r^*_2 u) \ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  +\int_{S_3} r^*_2 uv \ ds \\
\int \int_R -\nabla u \cdot \nabla v  +uv  \ dA +\int_{S_3} r^*_2 uv \ ds  &= \int \int_R g  v  \ dA - \int_{S_2} v q^* \ ds - \int_{S_3} v r^*_1\ ds  \\
B(u,v)&=L(v)
\end{align} c(x,y)\frac{\partial^2 u}{\partial x \partial y}+d(x,y)\frac{\partial u}{\partial x}+e(x,y)\frac{\partial u}{\partial y} B(u,v) a(x,y)\frac{\partial^2 u}{\partial x^2}+b(x,y) \frac{\partial^2 u}{\partial y^2} B(u,v) n u(x,y) v(x,y) \int \int_{R} * \ dA R \int_{S} * ds S u^*, q^*, r^*_1, r^*_2, r^*_3 S=S_1\cup S_2 \cup S_3","['functional-analysis', 'multivariable-calculus', 'partial-differential-equations', 'numerical-methods', 'finite-element-method']"
11,"$X$ compact, $C(X)$ equipped with inner product, evaluation maps continuous, prove $X$ is finite","compact,  equipped with inner product, evaluation maps continuous, prove  is finite",X C(X) X,"I'm working on the following problem from Conway V.4. Let $X$ be compact and supppose there is a norm on $C(X)$ that is given by an inner product making $C(X)$ into a Hilbert space such that for every $x \in X$ the functional $\Lambda_x : f\mapsto f(x)$ is continuous with respect to the Hilbert space norm.  Show $X$ is finite. Idea on approach: I think we need to assume $X$ is infinite and come to some sort of contradiction. Facts noted: $\text{ball}{(C(X))}$ is weakly compact. $X^* \subset C(X)$, $X^*$ is definitely a subspace, and if it is closed under the norm induced by the inner product we have $X^*$ is Hilbert and hence reflexive. (Since I am not sure of the closedness of $X^*$ under the norm, I am not sure if this could be useful.) The hypothesis is allowing us to extend the notion of a weak-$*$ topology on $X^*$ to $C(X)$. The continuity of the $\Lambda_x$'s and Riesz Rep theorem imply that there is a $g \in C(X)$ such that $\Lambda_x(f) = \langle f,g \rangle = |f(x)| \leq M \langle f,f\rangle^{1/2}$. I'm not entirely sure how to put these pieces together.  I was hoping to take a $X \supset \{x_n\}$ with $x_n \to x$, and by weak compactness we have for any sequence $\{f_j\} \subset \text{ball}(C(X))$ there is a subsequence such that $f_{j_k}$ converges weakly to some $f \in \text{ball}(C(X))$, which by our hypotheses implies that $f_{j_k}(x_n) \to f(x_n)$ as $k \to\infty$.  We also know that $f_{j_k}(x_n) \to f_{j_k}(x)$ as $n\to \infty$.  But there is nothing contradictory I can see coming from this observation. Any hints would be appreciated. Thanks. Edit: Also not entirely sure how $X$ compact fits in exactly, as I don't think I've taken much advantage of that fact in my observations except for claiming there is a convergent sequence.","I'm working on the following problem from Conway V.4. Let $X$ be compact and supppose there is a norm on $C(X)$ that is given by an inner product making $C(X)$ into a Hilbert space such that for every $x \in X$ the functional $\Lambda_x : f\mapsto f(x)$ is continuous with respect to the Hilbert space norm.  Show $X$ is finite. Idea on approach: I think we need to assume $X$ is infinite and come to some sort of contradiction. Facts noted: $\text{ball}{(C(X))}$ is weakly compact. $X^* \subset C(X)$, $X^*$ is definitely a subspace, and if it is closed under the norm induced by the inner product we have $X^*$ is Hilbert and hence reflexive. (Since I am not sure of the closedness of $X^*$ under the norm, I am not sure if this could be useful.) The hypothesis is allowing us to extend the notion of a weak-$*$ topology on $X^*$ to $C(X)$. The continuity of the $\Lambda_x$'s and Riesz Rep theorem imply that there is a $g \in C(X)$ such that $\Lambda_x(f) = \langle f,g \rangle = |f(x)| \leq M \langle f,f\rangle^{1/2}$. I'm not entirely sure how to put these pieces together.  I was hoping to take a $X \supset \{x_n\}$ with $x_n \to x$, and by weak compactness we have for any sequence $\{f_j\} \subset \text{ball}(C(X))$ there is a subsequence such that $f_{j_k}$ converges weakly to some $f \in \text{ball}(C(X))$, which by our hypotheses implies that $f_{j_k}(x_n) \to f(x_n)$ as $k \to\infty$.  We also know that $f_{j_k}(x_n) \to f_{j_k}(x)$ as $n\to \infty$.  But there is nothing contradictory I can see coming from this observation. Any hints would be appreciated. Thanks. Edit: Also not entirely sure how $X$ compact fits in exactly, as I don't think I've taken much advantage of that fact in my observations except for claiming there is a convergent sequence.",,"['functional-analysis', 'hilbert-spaces', 'compactness']"
12,About the closedness of $\frac d{dx}$ operator,About the closedness of  operator,\frac d{dx},"The example section of the closed linear operator in the wikipedia page https://en.wikipedia.org/wiki/Unbounded_operator#Closed_linear_operators says that (1) ""Consider the derivative operator $A =\frac d {dx}$  where $X = Y = C([a, b])$ is the Banach space of all continuous functions on an interval $[a, b]$. If one takes its domain $D(A)$ to be $C^1([a, b])$, then $A$ is a closed operator, which is not bounded."" (2) ""On the other hand if $D(A) = C^\infty([a, b])$, then $A$ will no longer be closed, but it will be closable, with the closure being its extension defined on $C^1([a, b])$."" The first claim seems easy to understand. Since the limit of any sequence of continuously differentiable functions $f_n$ if converges  would be a continuously differentiable function $f$ and $\frac d {dx} f$ is a continuous function $\in C[a,b]$. However the second claim seems not to be easy to understand. Can anyone give a more rigorously proof to this two statements clearly? Thanks.","The example section of the closed linear operator in the wikipedia page https://en.wikipedia.org/wiki/Unbounded_operator#Closed_linear_operators says that (1) ""Consider the derivative operator $A =\frac d {dx}$  where $X = Y = C([a, b])$ is the Banach space of all continuous functions on an interval $[a, b]$. If one takes its domain $D(A)$ to be $C^1([a, b])$, then $A$ is a closed operator, which is not bounded."" (2) ""On the other hand if $D(A) = C^\infty([a, b])$, then $A$ will no longer be closed, but it will be closable, with the closure being its extension defined on $C^1([a, b])$."" The first claim seems easy to understand. Since the limit of any sequence of continuously differentiable functions $f_n$ if converges  would be a continuously differentiable function $f$ and $\frac d {dx} f$ is a continuous function $\in C[a,b]$. However the second claim seems not to be easy to understand. Can anyone give a more rigorously proof to this two statements clearly? Thanks.",,"['functional-analysis', 'banach-spaces', 'differential-operators']"
13,Is c isometrically isomorphic $c \times c$?,Is c isometrically isomorphic ?,c \times c,"Let $c_0$ be the space of all sequences of scalars converging to $0$ with the supremum norm and $c$ be the space of all convergent sequences of scalars with the supremum norm. Is $c$ isometrically isomorphic $c \times c$? Here $c \times c$ is given the max norm: $||(\{x_n\},\{y_n\})||=\max (||(\{x_n\},||(\{y_n\}||)$. This must have appeared on MSE but I couldn't locate it. $c_0 \times c_0$ is obviously isometrically isomorphic to $c_0$ and there cannot be an isometric isomorphism of $c$ onto $c \times c$ that maps $c_0$ onto $c_0 \times c_0$ as seen by a consideration of codimensions. But I am unable to see if c isometrically isomorphic to $c \times c$","Let $c_0$ be the space of all sequences of scalars converging to $0$ with the supremum norm and $c$ be the space of all convergent sequences of scalars with the supremum norm. Is $c$ isometrically isomorphic $c \times c$? Here $c \times c$ is given the max norm: $||(\{x_n\},\{y_n\})||=\max (||(\{x_n\},||(\{y_n\}||)$. This must have appeared on MSE but I couldn't locate it. $c_0 \times c_0$ is obviously isometrically isomorphic to $c_0$ and there cannot be an isometric isomorphism of $c$ onto $c \times c$ that maps $c_0$ onto $c_0 \times c_0$ as seen by a consideration of codimensions. But I am unable to see if c isometrically isomorphic to $c \times c$",,"['functional-analysis', 'isometry']"
14,Is the Trace of products of Hilbert-Schmidt Operators stable under cyclic permutations?,Is the Trace of products of Hilbert-Schmidt Operators stable under cyclic permutations?,,"It is known, that the product of two hilbert-schmidt operators is in the Trace class. We also know, that for a continuous, linear $S$ and any $T$ in the trace class, $\text{trace}(ST)=\text{trace}(TS)$. If we have two Hilbert-Schmidt Operators $H_{1}$ and $H_{2}$ does  $\text{trace}(H_{1}H_{2})=\text{trace}(H_{2}H_{1})$ necessarily hold? What happens to the traces if we choose any two operators  $A,B$ auch that $AB$ and $BA$ are both in the Trace class? (If i am not mistaken, that should for example happen for operators lying in Schatten-p-classes for conjugated indexes).","It is known, that the product of two hilbert-schmidt operators is in the Trace class. We also know, that for a continuous, linear $S$ and any $T$ in the trace class, $\text{trace}(ST)=\text{trace}(TS)$. If we have two Hilbert-Schmidt Operators $H_{1}$ and $H_{2}$ does  $\text{trace}(H_{1}H_{2})=\text{trace}(H_{2}H_{1})$ necessarily hold? What happens to the traces if we choose any two operators  $A,B$ auch that $AB$ and $BA$ are both in the Trace class? (If i am not mistaken, that should for example happen for operators lying in Schatten-p-classes for conjugated indexes).",,"['functional-analysis', 'operator-algebras', 'trace', 'operator-ideals']"
15,Is set of convergent sequence with sup norm Banach?,Is set of convergent sequence with sup norm Banach?,,"Let $(c, \|.\|_{\infty})$ be the vector space of all real convergent sequence with sup norm. Is the space complete? I know $(l^\infty, \|.\|_{\infty})$ is a Banach space and $c$ is subset of $l^\infty$. If I can show that $c$ is closed the it becomes complete. But all this is if the $c$ is known to be complete. What do you say about it? Is it complete? If not what Cauchy sequence in $c$ can you provide which is not convergent convergent with $\sup$ norm in $c$?","Let $(c, \|.\|_{\infty})$ be the vector space of all real convergent sequence with sup norm. Is the space complete? I know $(l^\infty, \|.\|_{\infty})$ is a Banach space and $c$ is subset of $l^\infty$. If I can show that $c$ is closed the it becomes complete. But all this is if the $c$ is known to be complete. What do you say about it? Is it complete? If not what Cauchy sequence in $c$ can you provide which is not convergent convergent with $\sup$ norm in $c$?",,"['functional-analysis', 'banach-spaces', 'complete-spaces']"
16,"""Min-norm"": does such an object make sense?","""Min-norm"": does such an object make sense?",,"One of the widely used norms on vector spaces is the $l_\infty$ norm or max-norm ; that is, for $x\in\mathbb{R}^n$, $$ \vert\vert x\vert\vert_\infty:=\max_{i=1...n}|x_i|. $$  This is a fairly classical and well-studied object. However, I'm currently wondering whether an ''opposite'', in a certain sense, object makes sense or has been studied: $$ ||x||_{-\infty}:=\min_{i=1...n}|x_i|. $$ For a lack of a better name, it can be called min-norm . Intuitively, it is reasonable to consider in some applications where, for example, two objects $x, y$ are deemed to be 'close' if they are 'close' at least in one of the coordinates. It is fairly easy to see that min-norm is not actually a norm, because it violates the definitiveness constraint: $\vert\vert x\vert\vert_{-\infty}=0$ does not imply $x=0$. On the other hand, min-norm still satisfies the two other norm axioms, the triangle inequality and positive homogenity. My question is: has this object been previously considered in maths or applications, or why not? Or does it even make sense to study it? Any relevant links or references are highly appreciated.","One of the widely used norms on vector spaces is the $l_\infty$ norm or max-norm ; that is, for $x\in\mathbb{R}^n$, $$ \vert\vert x\vert\vert_\infty:=\max_{i=1...n}|x_i|. $$  This is a fairly classical and well-studied object. However, I'm currently wondering whether an ''opposite'', in a certain sense, object makes sense or has been studied: $$ ||x||_{-\infty}:=\min_{i=1...n}|x_i|. $$ For a lack of a better name, it can be called min-norm . Intuitively, it is reasonable to consider in some applications where, for example, two objects $x, y$ are deemed to be 'close' if they are 'close' at least in one of the coordinates. It is fairly easy to see that min-norm is not actually a norm, because it violates the definitiveness constraint: $\vert\vert x\vert\vert_{-\infty}=0$ does not imply $x=0$. On the other hand, min-norm still satisfies the two other norm axioms, the triangle inequality and positive homogenity. My question is: has this object been previously considered in maths or applications, or why not? Or does it even make sense to study it? Any relevant links or references are highly appreciated.",,"['functional-analysis', 'reference-request', 'normed-spaces']"
17,Corollary of Hahn-Banach theorem regarding the norm,Corollary of Hahn-Banach theorem regarding the norm,,"Let $E$ be a normed space. Then, for each $x \in E$ we  can define its norm as  $\|x\| = \sup \{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} $. Proof: Let $x \in E$ and set $S = \sup\{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} \leq \|x\|.$ On the one-dimensional subspace $F = \mathrm{span}(x)$ we define a functional $f$ by $f(\lambda x)= \lambda \|x\| $ for all $\lambda  \in \mathbb{R}$  it follows that $f \in F^*$ with $\|f\| = 1$.  Hence, by the Hahn-Banach Theorem, there exists some $l \in E^*$ with $l|_F\equiv f$ and $\|l\| = 1$. In particular this implies  $|l(x)| = |f(x)| = \|x\|$. This implies that $S = \|X\|$ and that the supremum is attained. Now, why is it, that there a $\leq$ sign in the set of wich we take the  supremum, since the proof uses a functional that  clearly has norm equal to 1?","Let $E$ be a normed space. Then, for each $x \in E$ we  can define its norm as  $\|x\| = \sup \{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} $. Proof: Let $x \in E$ and set $S = \sup\{|l(x)| : l \in E^∗, \|l\| ≤ 1 \} \leq \|x\|.$ On the one-dimensional subspace $F = \mathrm{span}(x)$ we define a functional $f$ by $f(\lambda x)= \lambda \|x\| $ for all $\lambda  \in \mathbb{R}$  it follows that $f \in F^*$ with $\|f\| = 1$.  Hence, by the Hahn-Banach Theorem, there exists some $l \in E^*$ with $l|_F\equiv f$ and $\|l\| = 1$. In particular this implies  $|l(x)| = |f(x)| = \|x\|$. This implies that $S = \|X\|$ and that the supremum is attained. Now, why is it, that there a $\leq$ sign in the set of wich we take the  supremum, since the proof uses a functional that  clearly has norm equal to 1?",,"['functional-analysis', 'normed-spaces']"
18,Dense subspace of $\ell_2$,Dense subspace of,\ell_2,"How can I show that the subspace $\{x\in \ell_2 \mid \sum_{n=1} ^\infty x_n \frac 1{\sqrt n}=0\}$ is dense in $\ell_2$? Can it be done with ""simple"" tools just by using the definition of a dense subspace? or can it be done by using the fact that $\ell_2$ is a Hilbert space?","How can I show that the subspace $\{x\in \ell_2 \mid \sum_{n=1} ^\infty x_n \frac 1{\sqrt n}=0\}$ is dense in $\ell_2$? Can it be done with ""simple"" tools just by using the definition of a dense subspace? or can it be done by using the fact that $\ell_2$ is a Hilbert space?",,['functional-analysis']
19,Definition of Essentially Self Adjoint Operators,Definition of Essentially Self Adjoint Operators,,"I have two definitions of an essentially self adjoint operator A symmetric operator with a self adjoint closure An operator with a unique self adjoint extension. I can easily show that (1) implies (2) but the converse escapes me. I can get as far (correctly I hope) that if $T$ has a unique self adjoint extension $S$ then $T$ is densely defined, closable, and symmetric and $ T \subset cl(T) \subset S = S^* \subset [cl(T)]^* = T^*$ I have a reference Reed & Simon, V.1 p.256 which further refers to section (V.2) X.1, but I didn't find the answer there.","I have two definitions of an essentially self adjoint operator A symmetric operator with a self adjoint closure An operator with a unique self adjoint extension. I can easily show that (1) implies (2) but the converse escapes me. I can get as far (correctly I hope) that if $T$ has a unique self adjoint extension $S$ then $T$ is densely defined, closable, and symmetric and $ T \subset cl(T) \subset S = S^* \subset [cl(T)]^* = T^*$ I have a reference Reed & Simon, V.1 p.256 which further refers to section (V.2) X.1, but I didn't find the answer there.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'adjoint-operators']"
20,What is a central projection?,What is a central projection?,,"What is a central projection in the context of a C*-algebra or a W*-algebra? Unfortunately while I see results on this concept, I haven't been able to find a definition. Thanks!","What is a central projection in the context of a C*-algebra or a W*-algebra? Unfortunately while I see results on this concept, I haven't been able to find a definition. Thanks!",,"['functional-analysis', 'operator-theory', 'operator-algebras']"
21,How to determine if conditional expectations with respect to different measures are equal a.s.?,How to determine if conditional expectations with respect to different measures are equal a.s.?,,"Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $\mathcal{A}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Let $Q_{\mathcal{A}}$ be a probability measure on $(\Omega, \mathcal{A})$ and define, for all $P$-integrable $f$, $$Q(f) := \int E_P(f \mid \mathcal{A})dQ_{\mathcal{A}}.$$ Note that $E_P(f \mid \mathcal{A})$ is the conditional expectation with respect to $P$. Also note that $Q$ defines a measure on $(\Omega, \mathcal{F})$ by taking $f$ to be an indicator function (we abuse notation by writing $Q(A)$ for $A \in \mathcal{F}$). Motivation. The idea is that we start with a ""prior"" probability space $(\Omega, \mathcal{F}, P)$. This extends to a linear functional (expectation) on the space $L^1$ of $P$-integrable functions $f$. Then we ""learn"" something about the sub-algebra $\mathcal{A}$ and adopt the new probability $Q_{\mathcal{A}}$ defined on $\mathcal{A}$. The question arises: How to extend this new probability $Q_{\mathcal{A}}$ to all of $\mathcal{F}$ (and thereby $L^1$)? We consider the extension $Q$ defined above. Question. Does $E_P(f \mid \mathcal{A}) = E_Q(f \mid \mathcal{A})$ a.s. ($P$)? Added Question. Is $Q \ll P$? The a.s. equality would follow if I could show that $$\int_A E_Q(f \mid \mathcal{A}) dP = \int_A f dP$$ for all $A \in \mathcal{A}.$ But I'm not sure what can be said when integrating a $Q$-conditional expectation against the measure $P$. I believe I can show the result for the case where $\mathcal{A}$ is generated by a countable partition $\{A_i \}_{i \in I}$ with $P(A_i)>0$ and $Q_{\mathcal{A}}(A_i) > 0$ for all $i \in I$. In that case, we have $$E_Q(f \mid \mathcal{A}) = \sum_i E_Q(f \mid A_i) \mathbf{1}_{A_i},$$ so it suffices to show that $E_Q(f \mid A_i) = E_P(f \mid A_i)$ for all $i \in I$. To that end we calculate (I abuse notation, writing $A_i = \mathbf{1}_{A_i}$) $$\begin{align} E_Q(f \mid A_i) &= \frac{1}{Q(A_i)} \int_{A_i}fdQ \\  &= \frac{1}{Q(A_i)} \int E_P(fA_i \mid \mathcal{A}) dQ_{\mathcal{A}} \\  &= \frac{1}{Q(A_i)} \int \left(\sum_{j \in I} \mathbf{1}_{A_j} \frac{1}{P(A_j)} \int_{A_i}fA_j dP \right)dQ_{\mathcal{A}} \\ &= \frac{Q_{\mathcal{A}}(A_i)}{Q(A_i)} \frac{1}{P(A_i)} \int_{A_i} f dP \\ &= E_P(f \mid A_i).\end{align}$$ The problem with extending this to general $\mathcal{A}$ is that I can't say explicitly what the conditional expectations (almost surely) are.","Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $\mathcal{A}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Let $Q_{\mathcal{A}}$ be a probability measure on $(\Omega, \mathcal{A})$ and define, for all $P$-integrable $f$, $$Q(f) := \int E_P(f \mid \mathcal{A})dQ_{\mathcal{A}}.$$ Note that $E_P(f \mid \mathcal{A})$ is the conditional expectation with respect to $P$. Also note that $Q$ defines a measure on $(\Omega, \mathcal{F})$ by taking $f$ to be an indicator function (we abuse notation by writing $Q(A)$ for $A \in \mathcal{F}$). Motivation. The idea is that we start with a ""prior"" probability space $(\Omega, \mathcal{F}, P)$. This extends to a linear functional (expectation) on the space $L^1$ of $P$-integrable functions $f$. Then we ""learn"" something about the sub-algebra $\mathcal{A}$ and adopt the new probability $Q_{\mathcal{A}}$ defined on $\mathcal{A}$. The question arises: How to extend this new probability $Q_{\mathcal{A}}$ to all of $\mathcal{F}$ (and thereby $L^1$)? We consider the extension $Q$ defined above. Question. Does $E_P(f \mid \mathcal{A}) = E_Q(f \mid \mathcal{A})$ a.s. ($P$)? Added Question. Is $Q \ll P$? The a.s. equality would follow if I could show that $$\int_A E_Q(f \mid \mathcal{A}) dP = \int_A f dP$$ for all $A \in \mathcal{A}.$ But I'm not sure what can be said when integrating a $Q$-conditional expectation against the measure $P$. I believe I can show the result for the case where $\mathcal{A}$ is generated by a countable partition $\{A_i \}_{i \in I}$ with $P(A_i)>0$ and $Q_{\mathcal{A}}(A_i) > 0$ for all $i \in I$. In that case, we have $$E_Q(f \mid \mathcal{A}) = \sum_i E_Q(f \mid A_i) \mathbf{1}_{A_i},$$ so it suffices to show that $E_Q(f \mid A_i) = E_P(f \mid A_i)$ for all $i \in I$. To that end we calculate (I abuse notation, writing $A_i = \mathbf{1}_{A_i}$) $$\begin{align} E_Q(f \mid A_i) &= \frac{1}{Q(A_i)} \int_{A_i}fdQ \\  &= \frac{1}{Q(A_i)} \int E_P(fA_i \mid \mathcal{A}) dQ_{\mathcal{A}} \\  &= \frac{1}{Q(A_i)} \int \left(\sum_{j \in I} \mathbf{1}_{A_j} \frac{1}{P(A_j)} \int_{A_i}fA_j dP \right)dQ_{\mathcal{A}} \\ &= \frac{Q_{\mathcal{A}}(A_i)}{Q(A_i)} \frac{1}{P(A_i)} \int_{A_i} f dP \\ &= E_P(f \mid A_i).\end{align}$$ The problem with extending this to general $\mathcal{A}$ is that I can't say explicitly what the conditional expectations (almost surely) are.",,"['functional-analysis', 'probability-theory', 'measure-theory', 'conditional-expectation']"
22,Understanding a step in Stein's proof that the Schwartz space is closed under convolution.,Understanding a step in Stein's proof that the Schwartz space is closed under convolution.,,"I am looking at Stein's proof that if $f,g \in \mathcal{S}(\mathbb{R})$, then so is $f*g$. The definition of the Schwartz space on $\mathbb{R}$ given is, the set of all indefinitely differentiable functions $f$ so that $f$ and all its derivatives are rapidly decreasing, in the sense that  $$\sup_{x\in \mathbb{R}}|x|^k |f^{(l)}(x)| < \infty \; \text{for every}\; k,l\ge 0.$$  In the proof below, I have four questions. First, how can I check the assertion that we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l$? I tried considering the two cases, but I cannot figure out how to get this bound. In fact, I don't really understand why we need to consider two separate cases. Shouldn't we be able to get $|x|^l |g(x-y)|\le |x-y|^l|g(x-y)|+|y|^l|g(x-y)|\le (1+|y|^l)A_l\le(1+|y|)^l A_l$, where $A_l=\sup_x |x|^l |g(x)|$? Next, how can we guarantee that the integral $\int |f(y)|(1+|y|)^l dy$ is bounded for every $l\ge 0$? Finally, in the last paragraph, how is the interchange of differentation and integration justified in the case by the rapid decrease of $dg/dx$? Also, why do we only take the differentiation on $g$ and not $f$? I am not aware of this kind of  theorem. I would greatly appreciate any hints or solutions.","I am looking at Stein's proof that if $f,g \in \mathcal{S}(\mathbb{R})$, then so is $f*g$. The definition of the Schwartz space on $\mathbb{R}$ given is, the set of all indefinitely differentiable functions $f$ so that $f$ and all its derivatives are rapidly decreasing, in the sense that  $$\sup_{x\in \mathbb{R}}|x|^k |f^{(l)}(x)| < \infty \; \text{for every}\; k,l\ge 0.$$  In the proof below, I have four questions. First, how can I check the assertion that we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l$? I tried considering the two cases, but I cannot figure out how to get this bound. In fact, I don't really understand why we need to consider two separate cases. Shouldn't we be able to get $|x|^l |g(x-y)|\le |x-y|^l|g(x-y)|+|y|^l|g(x-y)|\le (1+|y|^l)A_l\le(1+|y|)^l A_l$, where $A_l=\sup_x |x|^l |g(x)|$? Next, how can we guarantee that the integral $\int |f(y)|(1+|y|)^l dy$ is bounded for every $l\ge 0$? Finally, in the last paragraph, how is the interchange of differentation and integration justified in the case by the rapid decrease of $dg/dx$? Also, why do we only take the differentiation on $g$ and not $f$? I am not aware of this kind of  theorem. I would greatly appreciate any hints or solutions.",,"['complex-analysis', 'functional-analysis', 'analysis', 'fourier-analysis', 'fourier-transform']"
23,$H^{\infty}$ is not separable,is not separable,H^{\infty},"Consider the Hardy space, $(H^{\infty},\|•\|{\tiny{\infty}})$ where, $$  \|f\|{\tiny{\infty}} = \sup\{f(z) : z\in\mathbb{D} \}<\infty,$$    for a $f\in H^{\infty}.$ Prove that $H^{\infty}$ is not separable. My attempt: I tried to find, $\phi^{i}$, $i\in I,$ functions, holomorphic, bounded over the unit disk, such that, $$B = \{\phi^{i} : i\in I\}$$ has uncountably many members and for every $i,j\in I$ with $i \neq j$, there exists a M>0 : $\|\phi^{i} - \phi^{j}\|{\tiny{\infty}}>M$. But I cant find a spesific example that works. I tried for example $\phi^{t}(z) = e^{it}f(z)$ where $f\in H^{\infty}$. Its not easy, but there must be an elementary way to solve it, without using heavy theorems ( like interpolation theorems) or algebraic methods.","Consider the Hardy space, $(H^{\infty},\|•\|{\tiny{\infty}})$ where, $$  \|f\|{\tiny{\infty}} = \sup\{f(z) : z\in\mathbb{D} \}<\infty,$$    for a $f\in H^{\infty}.$ Prove that $H^{\infty}$ is not separable. My attempt: I tried to find, $\phi^{i}$, $i\in I,$ functions, holomorphic, bounded over the unit disk, such that, $$B = \{\phi^{i} : i\in I\}$$ has uncountably many members and for every $i,j\in I$ with $i \neq j$, there exists a M>0 : $\|\phi^{i} - \phi^{j}\|{\tiny{\infty}}>M$. But I cant find a spesific example that works. I tried for example $\phi^{t}(z) = e^{it}f(z)$ where $f\in H^{\infty}$. Its not easy, but there must be an elementary way to solve it, without using heavy theorems ( like interpolation theorems) or algebraic methods.",,"['complex-analysis', 'functional-analysis']"
24,Dual space of $L^{\infty}$ - Where is the mistake?,Dual space of  - Where is the mistake?,L^{\infty},"Today I thought about this for the first time and I really cannot see what is going on. I think it is a very stupid question but I really cannot see it. Consider the space $L^{\infty}(\mathbb{R})$ with the Lebesgue measure. According to this: The Duals of $l^\infty$ and $L^{\infty}$ , an element in the dual of this space is a finite signed measure $v$ which is absolutely continuous with respect to the Lebesgue measure. By Radon - Nikodym theorem we obtain: $dv = fd\mu$ and then the bounded total variation property is equivalent to $f \in L^1$. Thus we may thus construct an isometry between $(L^{\infty})^*$ and $L^1$ in an obvious way to get reflexivity of $L^1$ which is absurd. So my question is: where is the mistake?","Today I thought about this for the first time and I really cannot see what is going on. I think it is a very stupid question but I really cannot see it. Consider the space $L^{\infty}(\mathbb{R})$ with the Lebesgue measure. According to this: The Duals of $l^\infty$ and $L^{\infty}$ , an element in the dual of this space is a finite signed measure $v$ which is absolutely continuous with respect to the Lebesgue measure. By Radon - Nikodym theorem we obtain: $dv = fd\mu$ and then the bounded total variation property is equivalent to $f \in L^1$. Thus we may thus construct an isometry between $(L^{\infty})^*$ and $L^1$ in an obvious way to get reflexivity of $L^1$ which is absurd. So my question is: where is the mistake?",,"['functional-analysis', 'measure-theory', 'lebesgue-measure', 'lp-spaces']"
25,Norm of the quotient map for a normed space [duplicate],Norm of the quotient map for a normed space [duplicate],,"This question already has an answer here : Showing that the norm of the canonical projection $X\to X/M$ is $1$ (1 answer) Closed 9 years ago . Let $X$ be a normed space and $F$ a closed subspace. On $X/F$ let us take the quotient norm $||[x]|| = \inf_{y \in F} ||x - y||$. Consider the quotient $q : X \rightarrow X/F$. I can see that, if $||x|| = 1$, then $||q(x)|| = \inf_{y \in F} ||x - y|| \leq ||x - 0|| = 1$ since $0 \in F$. This proves that $q$ is bounded and $||q|| \leq 1$. How may I show that $||q|| = 1$?","This question already has an answer here : Showing that the norm of the canonical projection $X\to X/M$ is $1$ (1 answer) Closed 9 years ago . Let $X$ be a normed space and $F$ a closed subspace. On $X/F$ let us take the quotient norm $||[x]|| = \inf_{y \in F} ||x - y||$. Consider the quotient $q : X \rightarrow X/F$. I can see that, if $||x|| = 1$, then $||q(x)|| = \inf_{y \in F} ||x - y|| \leq ||x - 0|| = 1$ since $0 \in F$. This proves that $q$ is bounded and $||q|| \leq 1$. How may I show that $||q|| = 1$?",,"['functional-analysis', 'operator-theory']"
26,Show that the trace class operators on a Hilbert space form an ideal,Show that the trace class operators on a Hilbert space form an ideal,,"Let $(H, (\cdot, \cdot))$ be a separable Hilbert space over $\mathbb{L} = \mathbb{R}$ or $\mathbb{C}$. Suppose that $\{\phi_n\}_{n=1}^\infty$ is an orthonormal basis for $H$. Let $\mathcal{B}(H)$ denote the bounded linear operators $H \to H$. For $A \in \mathcal{B}(H)$, let $|A|$ denote the positive, self-adjoint square root of $A^*A$. We say that that $A \in \mathcal{B}(H)$ is of trace class if and only if $$\sum_{n=1}^\infty (\phi_n, |A|\phi_n) < \infty.$$ Let $\mathscr{I}_1(H)$ denote the trace class on $H$. For $A \in \mathscr{I}_1(H)$, set $\operatorname{Tr}(A) = \sum_{n=1}^\infty (\phi_n, |A|\phi_n).$ I would like to prove that the trace class operators on $H$ form a two sided ideal. That is, I would like to show that, if $A \in \mathscr{I}_1(H)$ and $B \in \mathcal{B}(H)$, then $AB, BA \in \mathscr{I}_1(H).$ I am studying trace class operators from Reed and Simon's Method's of Modern Mathematical Physics, Vol.1 , and this proposition above is stated in the text. Some things that I know from my studies so far: For $A \in \mathscr{I}_1(H)$, $\operatorname{Tr}(A)$ is independent of the orthonormal basis $\{\phi_n\}_{n=1}^\infty$ that we choose for $H$. $\mathscr{I}_1(H)$ is vector subspace of $\mathcal{B}(H).$ If $A \in \mathscr{I}_1(H)$ and if $U \in \mathcal{B}(H)$ is unitary (i.e., if $\operatorname{Ran}U = H$ and $(Ux,Uy) = (x,y)$, all $x,y \in H$), then $UAU^{-1} \in  \mathscr{I}_1(H)$. I am hoping that I can prove that  $\mathscr{I}_1(H)$ is an ideal without using too much more that (1)-(3), and perhaps some calculations. I do know some basics about adjoints and compact operators, and I am familiar with the construction of the polar decomposition for elements in  $\mathcal{B}(H)$. Although I am not sure if any of these topics are relevant. I am not familiar with the spectral theorem or the continuous functional calculus (I have yet to reach that chapter in Reed and Simon). In fact, Reed and Simon offer a proof that $\mathscr{I}_1(H)$ forms an ideal, and their proof goes in two stages: a) First, they prove that every $B \in \mathcal{B}(H)$ can be written as a linear combination of four unitary operators. b) Then, they prove that, if $A \in \mathscr{I}_1(H)$  and $U$ is unitary, then $UA, AU \in \mathscr{I}_1(H)$. Using a) and b), we can conclude that  $\mathscr{I}_1(H)$ is a two-sided ideal, because it is already known that  $\mathscr{I}_1(H)$ is a vector space. The difficulty is, I do not understand the proofs that Reed and Simon give for a) and b). Based on this question I asked previoulsy, I am not even sure that a) is true in the case $\mathbb{L} = \mathbb{R}$. And in proving b), Reed and Simon claim that $|UA| = |A|$ and $|AU|= U^{-1}|A|U$, and I have had no luck proving these equalities either. Any solutions or hints are greatly appreciated, whether it be an explication of Reed and Simon's proof, or a brand new argument.  I have been stuck on this proposition for some time and I'd really like to get this figured out!","Let $(H, (\cdot, \cdot))$ be a separable Hilbert space over $\mathbb{L} = \mathbb{R}$ or $\mathbb{C}$. Suppose that $\{\phi_n\}_{n=1}^\infty$ is an orthonormal basis for $H$. Let $\mathcal{B}(H)$ denote the bounded linear operators $H \to H$. For $A \in \mathcal{B}(H)$, let $|A|$ denote the positive, self-adjoint square root of $A^*A$. We say that that $A \in \mathcal{B}(H)$ is of trace class if and only if $$\sum_{n=1}^\infty (\phi_n, |A|\phi_n) < \infty.$$ Let $\mathscr{I}_1(H)$ denote the trace class on $H$. For $A \in \mathscr{I}_1(H)$, set $\operatorname{Tr}(A) = \sum_{n=1}^\infty (\phi_n, |A|\phi_n).$ I would like to prove that the trace class operators on $H$ form a two sided ideal. That is, I would like to show that, if $A \in \mathscr{I}_1(H)$ and $B \in \mathcal{B}(H)$, then $AB, BA \in \mathscr{I}_1(H).$ I am studying trace class operators from Reed and Simon's Method's of Modern Mathematical Physics, Vol.1 , and this proposition above is stated in the text. Some things that I know from my studies so far: For $A \in \mathscr{I}_1(H)$, $\operatorname{Tr}(A)$ is independent of the orthonormal basis $\{\phi_n\}_{n=1}^\infty$ that we choose for $H$. $\mathscr{I}_1(H)$ is vector subspace of $\mathcal{B}(H).$ If $A \in \mathscr{I}_1(H)$ and if $U \in \mathcal{B}(H)$ is unitary (i.e., if $\operatorname{Ran}U = H$ and $(Ux,Uy) = (x,y)$, all $x,y \in H$), then $UAU^{-1} \in  \mathscr{I}_1(H)$. I am hoping that I can prove that  $\mathscr{I}_1(H)$ is an ideal without using too much more that (1)-(3), and perhaps some calculations. I do know some basics about adjoints and compact operators, and I am familiar with the construction of the polar decomposition for elements in  $\mathcal{B}(H)$. Although I am not sure if any of these topics are relevant. I am not familiar with the spectral theorem or the continuous functional calculus (I have yet to reach that chapter in Reed and Simon). In fact, Reed and Simon offer a proof that $\mathscr{I}_1(H)$ forms an ideal, and their proof goes in two stages: a) First, they prove that every $B \in \mathcal{B}(H)$ can be written as a linear combination of four unitary operators. b) Then, they prove that, if $A \in \mathscr{I}_1(H)$  and $U$ is unitary, then $UA, AU \in \mathscr{I}_1(H)$. Using a) and b), we can conclude that  $\mathscr{I}_1(H)$ is a two-sided ideal, because it is already known that  $\mathscr{I}_1(H)$ is a vector space. The difficulty is, I do not understand the proofs that Reed and Simon give for a) and b). Based on this question I asked previoulsy, I am not even sure that a) is true in the case $\mathbb{L} = \mathbb{R}$. And in proving b), Reed and Simon claim that $|UA| = |A|$ and $|AU|= U^{-1}|A|U$, and I have had no luck proving these equalities either. Any solutions or hints are greatly appreciated, whether it be an explication of Reed and Simon's proof, or a brand new argument.  I have been stuck on this proposition for some time and I'd really like to get this figured out!",,"['functional-analysis', 'hilbert-spaces', 'ideals', 'trace']"
27,Elliptic partial differential equations and elliptic operators,Elliptic partial differential equations and elliptic operators,,"I'm starting to study elliptic partial differential equations and I just want to know if there are any connections between the following  concepts: An elliptic partial differential equation is given as being a second-order partial differential equation of the form $$Au_{xx} + 2Bu_{xy} + Cu_{yy}+Du_{x} + Eu_{y} + F = 0$$ that satisfies the condition $B^{2}-AC < 0$. The classification seems to be connected with conic sections. And then there's the definition of an elliptic operator which is defined as a linear differential operator $L$ of order $m$ on a domain $\Omega$ in $\mathbb{R}^{d}$ given by $$Lu = \sum_{|\alpha| \leq m}a_{\alpha}(x)\partial^{\alpha}u$$ (where $\alpha$ is a multi-index) is called elliptic if for every $x$ in $\Omega$ and every non-zero $\zeta$ in $\mathbb{R}^{d}$ $$\sum_{|\alpha|=m}a_{\alpha}(x)\zeta^{\alpha} \neq 0$$ I just have a couple of questions about these concepts? Firstly, why are PDE's classified in this way where it relates to conic sections?(elliptic, parabolic,hyperbolic) Secondly, what is the connection between elliptic partial differential equations and elliptic operators? I thought that an elliptic operator would be an elliptic PDE in operator form, in the sense that say $x-y=0$ was an elliptic PDE then $f(x,y) = x-y$ would be an elliptic operator. But it seems that there is no connection between elliptic operators and elliptic PDE's? Thanks for any help.","I'm starting to study elliptic partial differential equations and I just want to know if there are any connections between the following  concepts: An elliptic partial differential equation is given as being a second-order partial differential equation of the form $$Au_{xx} + 2Bu_{xy} + Cu_{yy}+Du_{x} + Eu_{y} + F = 0$$ that satisfies the condition $B^{2}-AC < 0$. The classification seems to be connected with conic sections. And then there's the definition of an elliptic operator which is defined as a linear differential operator $L$ of order $m$ on a domain $\Omega$ in $\mathbb{R}^{d}$ given by $$Lu = \sum_{|\alpha| \leq m}a_{\alpha}(x)\partial^{\alpha}u$$ (where $\alpha$ is a multi-index) is called elliptic if for every $x$ in $\Omega$ and every non-zero $\zeta$ in $\mathbb{R}^{d}$ $$\sum_{|\alpha|=m}a_{\alpha}(x)\zeta^{\alpha} \neq 0$$ I just have a couple of questions about these concepts? Firstly, why are PDE's classified in this way where it relates to conic sections?(elliptic, parabolic,hyperbolic) Secondly, what is the connection between elliptic partial differential equations and elliptic operators? I thought that an elliptic operator would be an elliptic PDE in operator form, in the sense that say $x-y=0$ was an elliptic PDE then $f(x,y) = x-y$ would be an elliptic operator. But it seems that there is no connection between elliptic operators and elliptic PDE's? Thanks for any help.",,['functional-analysis']
28,Weak Derivative Heaviside function,Weak Derivative Heaviside function,,"I have to prove that the Heaviside function $$ H(x):=\begin{cases} 1 &\mbox{if } x \in [0,+\infty) \\  0 &\mbox{otherwise}\end{cases} $$ doesn't admit weak derivative in $L^1_{loc}(\mathbb{R})$. Is it correct the following solution? Let's suppose by contradiction that $\exists w\in L^1_{loc}(\mathbb{R})$ such that $$\int_{\mathbb{R}}H(x)v'(x)dx=\int_{\mathbb{R}}w(x)v(x)dx\qquad\forall v\in C^{\infty}_c(\mathbb{R}) $$ By the Fundamental Theorem of Calculus we have that $$\int_0^{+\infty}w(x)v(x)dx=v(0)\qquad\forall v\in C^{\infty}_c(\mathbb{R}) $$ By the Lebesgue Dominated Convergence Theorem we have that  $$\lim_{r\to 0}\int_{-r}^{r}|w(x)|dx=0 $$ Then there exists $\delta>0$ such that $\int_{-\delta}^{\delta}|w(x)|dx\le 1/2$. Let $v\in C^{\infty}_c(\mathbb{R})$ such that $\operatorname{supp}(v)\subset [-\delta,\delta]$ and $\max (v)=v(0)=1$. Then $$1=v(0)=\int_0^{+\infty}w(x)v(x)dx=\int_{-\delta}^{\delta}w(x)v(x)dx \le \max(v)\int_{-\delta}^{\delta}|w(x)|dx\le 1/2$$  Contradiction!!!","I have to prove that the Heaviside function $$ H(x):=\begin{cases} 1 &\mbox{if } x \in [0,+\infty) \\  0 &\mbox{otherwise}\end{cases} $$ doesn't admit weak derivative in $L^1_{loc}(\mathbb{R})$. Is it correct the following solution? Let's suppose by contradiction that $\exists w\in L^1_{loc}(\mathbb{R})$ such that $$\int_{\mathbb{R}}H(x)v'(x)dx=\int_{\mathbb{R}}w(x)v(x)dx\qquad\forall v\in C^{\infty}_c(\mathbb{R}) $$ By the Fundamental Theorem of Calculus we have that $$\int_0^{+\infty}w(x)v(x)dx=v(0)\qquad\forall v\in C^{\infty}_c(\mathbb{R}) $$ By the Lebesgue Dominated Convergence Theorem we have that  $$\lim_{r\to 0}\int_{-r}^{r}|w(x)|dx=0 $$ Then there exists $\delta>0$ such that $\int_{-\delta}^{\delta}|w(x)|dx\le 1/2$. Let $v\in C^{\infty}_c(\mathbb{R})$ such that $\operatorname{supp}(v)\subset [-\delta,\delta]$ and $\max (v)=v(0)=1$. Then $$1=v(0)=\int_0^{+\infty}w(x)v(x)dx=\int_{-\delta}^{\delta}w(x)v(x)dx \le \max(v)\int_{-\delta}^{\delta}|w(x)|dx\le 1/2$$  Contradiction!!!",,"['functional-analysis', 'sobolev-spaces', 'weak-derivatives']"
29,Non separable metric space implies an uncountable set with lower bounded distances?,Non separable metric space implies an uncountable set with lower bounded distances?,,"Can this be generalized to arbitrary metric spaces? That is, if $(X, d)$ is a metric space, does the fact that it is not separable imply that there exists an uncountable set $N \subset X$ and a constant $M > 0$ such that $\forall x, y \in N (x \neq y \Rightarrow d(x, y) > M)$?","Can this be generalized to arbitrary metric spaces? That is, if $(X, d)$ is a metric space, does the fact that it is not separable imply that there exists an uncountable set $N \subset X$ and a constant $M > 0$ such that $\forall x, y \in N (x \neq y \Rightarrow d(x, y) > M)$?",,"['functional-analysis', 'metric-spaces']"
30,Question on proof of weak compactness of $L^p$,Question on proof of weak compactness of,L^p,"Suppose $L^q(X,\mu)$ is separable (i.e. admits a countable dense subset). I wish to prove that every sequence $\{f_n\}$ in $L^p$ that satisfies $\sup_n \|f_n\|_p < \infty$ has a weakly convergent subsequence, i.e. a subsequence $f_{n_k}$ and $f \in L^p(X,\mu)$ such that for every $g \in L^q(X,\mu)$ we have $$\int f_{n_k}g \to \int  fg.$$ My attempt: Let $\{x_n\}$ be a countable dense subset of $L^q(X,\mu)$ . By Holder's inequality we know that $\int f_nx_1$ is a bounded sequence of real numbers and hence we can find a subsequence $f_n^1$ such that $\int f_n^1 x_1$ converges to a real number $a_1$ . Now again we find that $ \int f_n^1 x_2 $ is a bounded sequence of real numbers and so there is a subsequence $ \{f_n^2\} \subseteq \{f_n^1\} $ for which $\int f_n^2 x_2$ converges to a real number $a_2$ . Continuing this process and taking the diagonal $f_k^k$ we see that given any $x_n$ , $$\lim_{k \to \infty} \int f_k^k x_n= a_n.$$ Also, we may get a linear functional on all of $L^q(X,\mu)$ by setting $$l(x) = \lim_{k \to \infty} \int f_k^k x.$$ My question is: Why is this well-defined, namely why does $\lim_{k \to \infty} \int f_k^k x$ exist? If we know it does then the UBP gives that $l$ is bounded and so the weak limit $f$ is furnished from the proof that the dual of $L^q$ is $L^p$ .","Suppose is separable (i.e. admits a countable dense subset). I wish to prove that every sequence in that satisfies has a weakly convergent subsequence, i.e. a subsequence and such that for every we have My attempt: Let be a countable dense subset of . By Holder's inequality we know that is a bounded sequence of real numbers and hence we can find a subsequence such that converges to a real number . Now again we find that is a bounded sequence of real numbers and so there is a subsequence for which converges to a real number . Continuing this process and taking the diagonal we see that given any , Also, we may get a linear functional on all of by setting My question is: Why is this well-defined, namely why does exist? If we know it does then the UBP gives that is bounded and so the weak limit is furnished from the proof that the dual of is .","L^q(X,\mu) \{f_n\} L^p \sup_n \|f_n\|_p < \infty f_{n_k} f \in L^p(X,\mu) g \in L^q(X,\mu) \int f_{n_k}g \to \int  fg. \{x_n\} L^q(X,\mu) \int f_nx_1 f_n^1 \int f_n^1 x_1 a_1  \int f_n^1 x_2   \{f_n^2\} \subseteq \{f_n^1\}  \int f_n^2 x_2 a_2 f_k^k x_n \lim_{k \to \infty} \int f_k^k x_n= a_n. L^q(X,\mu) l(x) = \lim_{k \to \infty} \int f_k^k x. \lim_{k \to \infty} \int f_k^k x l f L^q L^p",['functional-analysis']
31,Excercise 1.13 in Brezis's Functional Analysis,Excercise 1.13 in Brezis's Functional Analysis,,"This is the Excercise 1.13 in Brezis's Functional Analysis Let $E=\mathbb{R}^n$ and let   $$P=\{x\in\mathbb{R}^n;x_i\geq 0\ \forall i=1,2,...,n\}$$   Let $M$ be a linear subspace of $E$ such that $M\cap P=\{0\}$. Prove that there is some hyperplane $H$ in $E$ such that   $$M\subset H \text{ and } H\cap P=\{0\}.$$   [Hint: Show first that $M^{\perp}\cap \text{Int}P\neq\emptyset$] I know if $f\in M^{\perp}\cap \text{Int}P$, then $f$ is just the functional to make the hyperplane. But how to prove the hint? Thanks very much!","This is the Excercise 1.13 in Brezis's Functional Analysis Let $E=\mathbb{R}^n$ and let   $$P=\{x\in\mathbb{R}^n;x_i\geq 0\ \forall i=1,2,...,n\}$$   Let $M$ be a linear subspace of $E$ such that $M\cap P=\{0\}$. Prove that there is some hyperplane $H$ in $E$ such that   $$M\subset H \text{ and } H\cap P=\{0\}.$$   [Hint: Show first that $M^{\perp}\cap \text{Int}P\neq\emptyset$] I know if $f\in M^{\perp}\cap \text{Int}P$, then $f$ is just the functional to make the hyperplane. But how to prove the hint? Thanks very much!",,['functional-analysis']
32,Weak $L^{p}$ spaces are quasi-normed?,Weak  spaces are quasi-normed?,L^{p},"Let $(X,\mathcal{B}, \mu)$ be a measure space.  Then for $0< p < \infty$ by definition $L^{p,\infty}(X,\mathcal{B}, \mu)$ is the class of all measureable functions $f$ such that \begin{eqnarray*} \|f\|_{p,\infty} &:=& \text{inf}\{c > 0: d_{f}(\alpha)\leq \frac{c^{p}}{\alpha^{p}}\text{ for all }\alpha > 0\}\\  &=& \text{sup}\{\gamma d_{f}(\gamma)^{\frac{1}{p}}:\gamma > 0\}  \end{eqnarray*} where $$d_{f}(\alpha) = \mu(\{x\in X:|f(x)| > \alpha\})$$ I'm trying to verify that $\|\cdot\|_{p,\infty}$ is a quasi-norm on $L^{p,\infty}$. The non-trivial thing to check is that for all $f,g\in L^{p,\infty}(X,\mathcal{B}, \mu)$, we have $\|f + g\|_{p,\infty} \leq c_{p}(\|f\|_{p,\infty} + \|g\|_{p,\infty})$, where $c_{p} = \text{max}\{2,2^{\frac{1}{p}}\}$. For $1\leq p < \infty$, I was able to show that $\|f + g\|_{p,\infty} \leq 2(\|f\|_{p,\infty} + \|g\|_{p,\infty})$ using the supremum definition. For $0 < p < 1$, I need to show that $\|f + g\|_{p,\infty} \leq 2^{\frac{1}{p}}(\|f\|_{p,\infty} + \|g\|_{p,\infty})$, but I'm stuck. The idea is supposed to be to use the following property of $d_{f}$: $$d_{f + g}(\alpha + \beta)\leq d_{f}(\alpha) + d_{g}(\beta)$$ which implies in particular that $$d_{f + g}(\alpha)\leq d_{f}\left(\frac{\alpha}{2}\right) + d_{g}\left(\frac{\alpha}{2}\right)$$ Can anyone help me finish the proof?  Thanks in advance!","Let $(X,\mathcal{B}, \mu)$ be a measure space.  Then for $0< p < \infty$ by definition $L^{p,\infty}(X,\mathcal{B}, \mu)$ is the class of all measureable functions $f$ such that \begin{eqnarray*} \|f\|_{p,\infty} &:=& \text{inf}\{c > 0: d_{f}(\alpha)\leq \frac{c^{p}}{\alpha^{p}}\text{ for all }\alpha > 0\}\\  &=& \text{sup}\{\gamma d_{f}(\gamma)^{\frac{1}{p}}:\gamma > 0\}  \end{eqnarray*} where $$d_{f}(\alpha) = \mu(\{x\in X:|f(x)| > \alpha\})$$ I'm trying to verify that $\|\cdot\|_{p,\infty}$ is a quasi-norm on $L^{p,\infty}$. The non-trivial thing to check is that for all $f,g\in L^{p,\infty}(X,\mathcal{B}, \mu)$, we have $\|f + g\|_{p,\infty} \leq c_{p}(\|f\|_{p,\infty} + \|g\|_{p,\infty})$, where $c_{p} = \text{max}\{2,2^{\frac{1}{p}}\}$. For $1\leq p < \infty$, I was able to show that $\|f + g\|_{p,\infty} \leq 2(\|f\|_{p,\infty} + \|g\|_{p,\infty})$ using the supremum definition. For $0 < p < 1$, I need to show that $\|f + g\|_{p,\infty} \leq 2^{\frac{1}{p}}(\|f\|_{p,\infty} + \|g\|_{p,\infty})$, but I'm stuck. The idea is supposed to be to use the following property of $d_{f}$: $$d_{f + g}(\alpha + \beta)\leq d_{f}(\alpha) + d_{g}(\beta)$$ which implies in particular that $$d_{f + g}(\alpha)\leq d_{f}\left(\frac{\alpha}{2}\right) + d_{g}\left(\frac{\alpha}{2}\right)$$ Can anyone help me finish the proof?  Thanks in advance!",,"['functional-analysis', 'measure-theory', 'harmonic-analysis', 'weak-lp-spaces']"
33,$X\subsetneqq Y$ but $X^\star=Y^\star$,but,X\subsetneqq Y X^\star=Y^\star,"Are there $X,Y$ real Banach spaces, such that $X\subsetneqq Y$ (strictly contained) and $X^\star=Y^\star$, where $\star$ denotes the topological dual? This property is not true for Hilbert spaces, nor even for $L^p$ spaces, so I was thinking to try some function space: continuous functions or bounded... Any help is appreciated. Thank you","Are there $X,Y$ real Banach spaces, such that $X\subsetneqq Y$ (strictly contained) and $X^\star=Y^\star$, where $\star$ denotes the topological dual? This property is not true for Hilbert spaces, nor even for $L^p$ spaces, so I was thinking to try some function space: continuous functions or bounded... Any help is appreciated. Thank you",,['functional-analysis']
34,separable Banach space with Banach-Mazur distances to $\ell_2^n$ bounded must be isomorphic to $\ell_2$?,separable Banach space with Banach-Mazur distances to  bounded must be isomorphic to ?,\ell_2^n \ell_2,"If $X$ is a separable infinite-dimensional Banach space and $C\in\mathbb{R}^+$ is an upper bound for the Banach-Mazur distance $d(E,\ell_2^n)$ for all $n\in\mathbb{N}$ and all $n$-dimensional $E\leq X$, why must $X$ be isomorphic to $\ell_2$? I've been thinking along the following lines: let $\{x_n:n\in\mathbb{N}\}$ be a countable, dense, linearly independent set in $X$ and let $E_n=\langle x_1,\dots,x_n\rangle$ for all $n$. Now for each $n$ there's a linear isomorphism $T_n:E_n\rightarrow\ell_2^n$ such that $\|T_n\|\|T_n^{-1}\|\leq C$. If we can somehow put together these $T_n$ to form a linear isomorphism $T:\cup_nE_n\rightarrow\cup_n\ell_2^n$ = {finite-length sequences in $\ell^2$}, then this extends by continuity to a linear isomorphism $\bar{T}:X\rightarrow\ell_2$. But how do we get $T$ from the $T_n$? Each $T_n$ is only defined on finitely many of the $x_i$. Maybe some sort of infinite series? Or can we choose each $T_n$ so as to agree with $T_{n-1}$, and still keep the $\|T_n\|\|T_n^{-1}\|\leq C$ property? Many thanks for any help with this!","If $X$ is a separable infinite-dimensional Banach space and $C\in\mathbb{R}^+$ is an upper bound for the Banach-Mazur distance $d(E,\ell_2^n)$ for all $n\in\mathbb{N}$ and all $n$-dimensional $E\leq X$, why must $X$ be isomorphic to $\ell_2$? I've been thinking along the following lines: let $\{x_n:n\in\mathbb{N}\}$ be a countable, dense, linearly independent set in $X$ and let $E_n=\langle x_1,\dots,x_n\rangle$ for all $n$. Now for each $n$ there's a linear isomorphism $T_n:E_n\rightarrow\ell_2^n$ such that $\|T_n\|\|T_n^{-1}\|\leq C$. If we can somehow put together these $T_n$ to form a linear isomorphism $T:\cup_nE_n\rightarrow\cup_n\ell_2^n$ = {finite-length sequences in $\ell^2$}, then this extends by continuity to a linear isomorphism $\bar{T}:X\rightarrow\ell_2$. But how do we get $T$ from the $T_n$? Each $T_n$ is only defined on finitely many of the $x_i$. Maybe some sort of infinite series? Or can we choose each $T_n$ so as to agree with $T_{n-1}$, and still keep the $\|T_n\|\|T_n^{-1}\|\leq C$ property? Many thanks for any help with this!",,"['functional-analysis', 'banach-spaces']"
35,"Is the space $C[0,1]$ locally compact?",Is the space  locally compact?,"C[0,1]","Let $C[0,1]$ be the space of continuous functions, equipped with the $\sup$-norm. My question is, how one can prove, that this space is not locally compact? Is it possible to show this explicitly, by providing a sequence of continuous functions, which does not contain a convergent subsequence?","Let $C[0,1]$ be the space of continuous functions, equipped with the $\sup$-norm. My question is, how one can prove, that this space is not locally compact? Is it possible to show this explicitly, by providing a sequence of continuous functions, which does not contain a convergent subsequence?",,"['functional-analysis', 'convergence-divergence']"
36,An inequality in Evans' PDE,An inequality in Evans' PDE,,"In Section $9.2$ Theorem $5$ of Lawrence Evans' Partial Differential Equations, First Edition the author proves that for a large enough $\lambda$, the equation $$\begin{array}-\Delta u+b(\nabla u)+\lambda u=0\ &\mbox{ in } U\\ u=0&\mbox{on }\partial U\end{array}$$ has a solution in $H_0^1(U)$. On page 507, the author writes $$\int_UC(|\nabla u|+1)|u|dx\leq\frac{1}{2}\int_U|\nabla u|^2dx+C\int_U(|u|^2+1)dx\ \mbox{ for }u\in H_0^1(U).$$ Here $C$ is the Lipschitz constant for the Lipschitz function $b$. My problem is that I cannot show this no matter how much I try. How is the gradient term becoming independent of $C$? Could someone please help!","In Section $9.2$ Theorem $5$ of Lawrence Evans' Partial Differential Equations, First Edition the author proves that for a large enough $\lambda$, the equation $$\begin{array}-\Delta u+b(\nabla u)+\lambda u=0\ &\mbox{ in } U\\ u=0&\mbox{on }\partial U\end{array}$$ has a solution in $H_0^1(U)$. On page 507, the author writes $$\int_UC(|\nabla u|+1)|u|dx\leq\frac{1}{2}\int_U|\nabla u|^2dx+C\int_U(|u|^2+1)dx\ \mbox{ for }u\in H_0^1(U).$$ Here $C$ is the Lipschitz constant for the Lipschitz function $b$. My problem is that I cannot show this no matter how much I try. How is the gradient term becoming independent of $C$? Could someone please help!",,['functional-analysis']
37,Example of a quasinilpotent operator,Example of a quasinilpotent operator,,"Can anybody please give me an example of a quasinilpotent operator $T$, i.e. an operator such that $\sigma(T)=\{0\}$  on $l_2$ such that it has finite dimensional but non-trivial kernel and is not compact? This is probably easy and well known but I just can't figure it out it and I am getting frustrated. Thanks!","Can anybody please give me an example of a quasinilpotent operator $T$, i.e. an operator such that $\sigma(T)=\{0\}$  on $l_2$ such that it has finite dimensional but non-trivial kernel and is not compact? This is probably easy and well known but I just can't figure it out it and I am getting frustrated. Thanks!",,"['functional-analysis', 'operator-theory']"
38,Operators from $\ell^\infty$ into $c_0$,Operators from  into,\ell^\infty c_0,"I have the following question related to $\ell^\infty(\mathbb{N}).$ How can I construct a bounded, linear operator from $\ell^\infty(\mathbb{N})$ into $c_0(\mathbb{N})$ which is non-compact? It is clear that $\ell^\infty$ is a Grothendieck space with Dunford-Pettis property, hence any operator from $\ell^\infty$ into a separable Banach space must be strictly singular. But I do not know any example above which is non-compact.","I have the following question related to $\ell^\infty(\mathbb{N}).$ How can I construct a bounded, linear operator from $\ell^\infty(\mathbb{N})$ into $c_0(\mathbb{N})$ which is non-compact? It is clear that $\ell^\infty$ is a Grothendieck space with Dunford-Pettis property, hence any operator from $\ell^\infty$ into a separable Banach space must be strictly singular. But I do not know any example above which is non-compact.",,"['functional-analysis', 'banach-spaces']"
39,Prove that if f in $C(X \times Y)$ then there exists functions.,Prove that if f in  then there exists functions.,C(X \times Y),"Let $X$ and $Y$ be compact metric spaces. I am trying to prove that if $f \in C(X \times Y)$ and $\varepsilon > 0 $, then there exist functions $g_1, g_2,...,g_n \in C(X)$ and $h_1,...h_n \in C(Y)$ so that $|f(x,y) - \sum_{k=1}^n g_k(x)h_k(y) | < \varepsilon $ for all $(x,y) \in X \times Y$.  I need help. Thank you.","Let $X$ and $Y$ be compact metric spaces. I am trying to prove that if $f \in C(X \times Y)$ and $\varepsilon > 0 $, then there exist functions $g_1, g_2,...,g_n \in C(X)$ and $h_1,...h_n \in C(Y)$ so that $|f(x,y) - \sum_{k=1}^n g_k(x)h_k(y) | < \varepsilon $ for all $(x,y) \in X \times Y$.  I need help. Thank you.",,['functional-analysis']
40,Complemented subspace of a Banach space,Complemented subspace of a Banach space,,"I have a quick question, If $E$ is a Banach space and $H$ is a closed subspace of $E$, could we affirm this proposition: If exists a linear continuous function $S:E\to H$ such that $S\circ i =Id_{_H}$ (with $i:H\hookrightarrow E$), then $H$ is complemented in $E$. I don't need the proof, simply I need to know if this is true or false, thanks.","I have a quick question, If $E$ is a Banach space and $H$ is a closed subspace of $E$, could we affirm this proposition: If exists a linear continuous function $S:E\to H$ such that $S\circ i =Id_{_H}$ (with $i:H\hookrightarrow E$), then $H$ is complemented in $E$. I don't need the proof, simply I need to know if this is true or false, thanks.",,"['functional-analysis', 'banach-spaces']"
41,Separation theorem,Separation theorem,,"Is it true that given a real vector space $X$ and two disjoint convex sets $A,B\subseteq X$, there is always a linear functional that (weakly) separates them? I.e., is there a non-zero linear functional $\phi\colon X\to \mathbb R$ and a $\gamma\in\mathbb R$, such that $\phi(a)\le \gamma \le \phi (b)$ for all $a\in A$ and $b\in B$? If not, can you give a counterexample? I know the statement is true if you add the aditional hypothesis that at least one of the sets has an internal point. This follows from the Hahn-Banach theorem using the Minkowski functional, but I was wandering whether this hypothesis is necessary.","Is it true that given a real vector space $X$ and two disjoint convex sets $A,B\subseteq X$, there is always a linear functional that (weakly) separates them? I.e., is there a non-zero linear functional $\phi\colon X\to \mathbb R$ and a $\gamma\in\mathbb R$, such that $\phi(a)\le \gamma \le \phi (b)$ for all $a\in A$ and $b\in B$? If not, can you give a counterexample? I know the statement is true if you add the aditional hypothesis that at least one of the sets has an internal point. This follows from the Hahn-Banach theorem using the Minkowski functional, but I was wandering whether this hypothesis is necessary.",,"['functional-analysis', 'convex-analysis']"
42,Differentiability of Moreau-Yosida approximation.,Differentiability of Moreau-Yosida approximation.,,"I want to show that if $X$ is a reflexive Banach space with norm of class $\mathcal{C}^1$ and $f\colon X\to\mathbb{R}\cup \{+\infty\}$ is convex and lower semicontinuous, then $f_{\lambda}$ is differentiable of class $\mathcal{C}^1$. (where $f_{\lambda}:X\to\mathbb{R}\cup \{+\infty\}$ is the Moreau-Yosida approximation: $$f_\lambda(x)=\inf_{y\in X} \left\{ f(y)+\frac{1}{2\lambda}|x-y|^2\right\})$$ Maybe, this result could be useful: If $g\colon X\to\mathbb{R}$ is convex and differentiable in every point then $g\in\mathcal{C}^1(X)$. Many thanks in advance.","I want to show that if $X$ is a reflexive Banach space with norm of class $\mathcal{C}^1$ and $f\colon X\to\mathbb{R}\cup \{+\infty\}$ is convex and lower semicontinuous, then $f_{\lambda}$ is differentiable of class $\mathcal{C}^1$. (where $f_{\lambda}:X\to\mathbb{R}\cup \{+\infty\}$ is the Moreau-Yosida approximation: $$f_\lambda(x)=\inf_{y\in X} \left\{ f(y)+\frac{1}{2\lambda}|x-y|^2\right\})$$ Maybe, this result could be useful: If $g\colon X\to\mathbb{R}$ is convex and differentiable in every point then $g\in\mathcal{C}^1(X)$. Many thanks in advance.",,"['functional-analysis', 'banach-spaces', 'approximation', 'convex-analysis']"
43,Is quasinorm always continuous?,Is quasinorm always continuous?,,"Let X be a vector space over $\mathbb{R}$ or $\mathbb{C}$, $\|\cdot\|: X\rightarrow [0,\infty)$ is called a quasi-norm if i) $\|x\|=0 \Rightarrow x=0$ ii) $\|\lambda x\|=|\lambda|\|x\|, \forall \lambda, x$ iii) $\exists K\ge 1$, s.t. $\|x+y\|\le K(\|x\|+\|y\|), \forall x,y$ My question is: If $\|x_k-x\|\rightarrow 0$, can we conclude that $\|x_k\|\rightarrow \|x\|$?","Let X be a vector space over $\mathbb{R}$ or $\mathbb{C}$, $\|\cdot\|: X\rightarrow [0,\infty)$ is called a quasi-norm if i) $\|x\|=0 \Rightarrow x=0$ ii) $\|\lambda x\|=|\lambda|\|x\|, \forall \lambda, x$ iii) $\exists K\ge 1$, s.t. $\|x+y\|\le K(\|x\|+\|y\|), \forall x,y$ My question is: If $\|x_k-x\|\rightarrow 0$, can we conclude that $\|x_k\|\rightarrow \|x\|$?",,"['functional-analysis', 'examples-counterexamples']"
44,Projecting onto the diagonal given Banach spaces with unconditional bases,Projecting onto the diagonal given Banach spaces with unconditional bases,,"Let E be a Banach space with a 1-unconditional basis $(e_n)$ (for example, $\ell^p$).  Then an operator T on E can be thought of as an infinite matrix, in the obvious way.  Clearly each scalar on the diagonal of this matrix is bounded by $\|T\|$.  By the unconditionality, it follows that if I have any diagonal matrix with uniformly bounded entries, this will define a bounded operator.  So we conclude that the projection taking the matrix of T to the diagonal matrix given by setting all off-diagonal terms zero, is a contraction. This is absolutely well-known for $\ell^p$ spaces say, and must be well-known in this more general setting, though I don't know a reference. Now suppose we have two Banach spaces E and F, both with 1-unconditional bases.  We can still think of operators as given by infinite matrices.  Is this procedure of projecting down to the diagonal still bounded?  What's hard is that we cannot expect, in general, the diagonal to be isomorphic to $\ell^\infty$ anymore.  So we need an indirect argument.  I think I can do this in an adhoc way for maps between $\ell^p$ and $\ell^q$; does anyone know a more systematic treatment, or any references?","Let E be a Banach space with a 1-unconditional basis $(e_n)$ (for example, $\ell^p$).  Then an operator T on E can be thought of as an infinite matrix, in the obvious way.  Clearly each scalar on the diagonal of this matrix is bounded by $\|T\|$.  By the unconditionality, it follows that if I have any diagonal matrix with uniformly bounded entries, this will define a bounded operator.  So we conclude that the projection taking the matrix of T to the diagonal matrix given by setting all off-diagonal terms zero, is a contraction. This is absolutely well-known for $\ell^p$ spaces say, and must be well-known in this more general setting, though I don't know a reference. Now suppose we have two Banach spaces E and F, both with 1-unconditional bases.  We can still think of operators as given by infinite matrices.  Is this procedure of projecting down to the diagonal still bounded?  What's hard is that we cannot expect, in general, the diagonal to be isomorphic to $\ell^\infty$ anymore.  So we need an indirect argument.  I think I can do this in an adhoc way for maps between $\ell^p$ and $\ell^q$; does anyone know a more systematic treatment, or any references?",,"['functional-analysis', 'banach-spaces']"
45,Would you recommend me a text book reference?,Would you recommend me a text book reference?,,"I am searching for a text book reference or lecture notes, where I can find the following theorem: I mean as similar as possible to the statement with these same words (if possible). Or including these 3 equivalences: THEOREM: Let $M\subset H$ , orthonormal, are equivalents a) $M$ is total basis (or orthonormal basis or Hilbert basis); b) For all $u \in H\quad  \sum_{i\in Ju}$ , $(u, u_i)u_i=u$ where $Ju=\{i\in J: u_i\in M \; \text{and} \; (u_i, u)\neq 0\}$ c) $\sum_{i\in J_u} |(u, u_i)|^2=|u|^2$ , for all $u \in H$ (Parseval identity) Remark: I wish this because I'm studying using Brezis' book, however there is appearing in different theorems, and it is not including the $\iff$ (equivalence).","I am searching for a text book reference or lecture notes, where I can find the following theorem: I mean as similar as possible to the statement with these same words (if possible). Or including these 3 equivalences: THEOREM: Let , orthonormal, are equivalents a) is total basis (or orthonormal basis or Hilbert basis); b) For all , where c) , for all (Parseval identity) Remark: I wish this because I'm studying using Brezis' book, however there is appearing in different theorems, and it is not including the (equivalence).","M\subset H M u \in H\quad  \sum_{i\in Ju} (u, u_i)u_i=u Ju=\{i\in J: u_i\in M \; \text{and} \; (u_i, u)\neq 0\} \sum_{i\in J_u} |(u, u_i)|^2=|u|^2 u \in H \iff","['functional-analysis', 'reference-request', 'hilbert-spaces']"
46,Operators that improve strong convergence to norm convergence?,Operators that improve strong convergence to norm convergence?,,"It is well known that if $K$ is a compact operator on a Hilbert space, and $T_n$ is a sequence of operators converging strongly to $T$ , then $K T_n$ converges in norm to $K T$ . Question : are there other operators $K$ that also satisfy this property, but are not compact ? Thanks","It is well known that if is a compact operator on a Hilbert space, and is a sequence of operators converging strongly to , then converges in norm to . Question : are there other operators that also satisfy this property, but are not compact ? Thanks",K T_n T K T_n K T K,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'compact-operators']"
47,The operator norm of the composition of linear bounded operators between Banach spaces.,The operator norm of the composition of linear bounded operators between Banach spaces.,,"The set $B(X, Y )$ is a normed linear space with the operator norm. If $T ∈ B(X, Y)$ and $S ∈ B(Y, Z)$ for $X, Y , Z$ normed linear spaces, then the composition $ST ∈ B(X, Z)$ and $\|ST\| ≤ \|S\| \|T\|.$ I don't know how to prove this. Here is my trial. Since the composition if exist of 2 linear operators is a linear operator then $ST$ is a linear operator. To prove it is bounded we prove that $\|ST x\| < \infty$ . if $\|T\| = \sup_{\|x\|=1}\|Tx\| < \infty$ . and $\|S\| = \sup_{\|y\|=1}\|Sy\| < \infty$ . I don't know how to continue.","The set is a normed linear space with the operator norm. If and for normed linear spaces, then the composition and I don't know how to prove this. Here is my trial. Since the composition if exist of 2 linear operators is a linear operator then is a linear operator. To prove it is bounded we prove that . if . and . I don't know how to continue.","B(X, Y ) T ∈ B(X, Y) S ∈ B(Y, Z) X, Y , Z ST ∈ B(X, Z) \|ST\| ≤ \|S\| \|T\|. ST \|ST x\| < \infty \|T\| = \sup_{\|x\|=1}\|Tx\| < \infty \|S\| = \sup_{\|y\|=1}\|Sy\| < \infty","['functional-analysis', 'proof-verification', 'operator-theory', 'linear-transformations']"
48,Properties of $\{x\in X\mid f(x)=||f||\}$,Properties of,\{x\in X\mid f(x)=||f||\},"Let $X$ be a normed space, $f\in X^*\setminus\{0\}$ (the continuous dual), $E:=\{x\in X\mid f(x)=\|f\|\}$. Prove that $E$ is a nonempty closed set and that $\inf \{\|x\|\mid x\in E\}=1$. I have no idea how to prove that $E$ is non empty and that $\inf \{\|x\|\mid x\in E\}=1$. $E$ is closed because it is the inverse image of a point ($\|f\|$) and $f$ is continuos, so inverse images of closed sets are closed.","Let $X$ be a normed space, $f\in X^*\setminus\{0\}$ (the continuous dual), $E:=\{x\in X\mid f(x)=\|f\|\}$. Prove that $E$ is a nonempty closed set and that $\inf \{\|x\|\mid x\in E\}=1$. I have no idea how to prove that $E$ is non empty and that $\inf \{\|x\|\mid x\in E\}=1$. $E$ is closed because it is the inverse image of a point ($\|f\|$) and $f$ is continuos, so inverse images of closed sets are closed.",,"['functional-analysis', 'normed-spaces', 'dual-spaces']"
49,The bound of Fourier series coefficients,The bound of Fourier series coefficients,,"https://www.encyclopediaofmath.org/index.php/Fourier_transform_of_a_generalized_function For a periodic generalized function $f$, I see from the link that  $$f(x) = \sum_{|k|=0}^\infty c_k(f)e ^{i\langle kw,x\rangle}, ~\left| c_k(f)\right|\le A \frac{1}{(1+k)^m}.$$ However, the definition or description of $m$ and $A$ are not given... Can anyone help me a little bit on this? Explain a little bit to me. Actually, if one can provide a reference, it will be very nice (not necessarily generalized functions, but only functions with one variable). I am thinking a question. This result could be of some help.","https://www.encyclopediaofmath.org/index.php/Fourier_transform_of_a_generalized_function For a periodic generalized function $f$, I see from the link that  $$f(x) = \sum_{|k|=0}^\infty c_k(f)e ^{i\langle kw,x\rangle}, ~\left| c_k(f)\right|\le A \frac{1}{(1+k)^m}.$$ However, the definition or description of $m$ and $A$ are not given... Can anyone help me a little bit on this? Explain a little bit to me. Actually, if one can provide a reference, it will be very nice (not necessarily generalized functions, but only functions with one variable). I am thinking a question. This result could be of some help.",,"['functional-analysis', 'fourier-analysis', 'fourier-series', 'fourier-transform']"
50,Sequence of linear functionals on $\ell^{\infty}$,Sequence of linear functionals on,\ell^{\infty},"I am having trouble with this question. Let $X$ be a normed space and let $T : X\to \ell^{\infty}$ be a bounded linear operator. $a)$ Show that there is a sequence $(f_n)_n$ in $X^*$ so that $Tx=(f_n(x))$ for all $x \in X.$ $b)$ Suppose that $X$ is a subspace of a normed space $Y$ . Show that there is a bounded linear operator $S: Y\to \ell^{\infty}$ so that $||S||=||T||$ and that $Sx = Tx$ for all $x \in X$ I am not sure how to start with question $a)$ . For $b)$ , I tried to find a $P: \ell^{\infty}\to \Bbb{R}$ so that the composite function $PT: X\to \Bbb{R}$ is a bounded linear functional. Then I applied the Hahn-Banach extension theorem to say that there exists a $PS: Y\to \Bbb{R}$ which is a bounded linear functional such that $PS(x)=PT(x)$ for all $x \in X$ . And then I got stuck. Please advise me on what to do. Thank you.","I am having trouble with this question. Let be a normed space and let be a bounded linear operator. Show that there is a sequence in so that for all Suppose that is a subspace of a normed space . Show that there is a bounded linear operator so that and that for all I am not sure how to start with question . For , I tried to find a so that the composite function is a bounded linear functional. Then I applied the Hahn-Banach extension theorem to say that there exists a which is a bounded linear functional such that for all . And then I got stuck. Please advise me on what to do. Thank you.",X T : X\to \ell^{\infty} a) (f_n)_n X^* Tx=(f_n(x)) x \in X. b) X Y S: Y\to \ell^{\infty} ||S||=||T|| Sx = Tx x \in X a) b) P: \ell^{\infty}\to \Bbb{R} PT: X\to \Bbb{R} PS: Y\to \Bbb{R} PS(x)=PT(x) x \in X,['functional-analysis']
51,Final step in proof of the Hellinger-Toeplitz Theorem,Final step in proof of the Hellinger-Toeplitz Theorem,,"Consider the following statement: ( Hellinger-Toeplitz Theorem ) Let $H$ be a Hilbert space and $A : H \to H$ be linear and symmetric,   i.e. $$\langle x,Ay \rangle = \langle Ax,y \rangle$$ holds for all   $x,y \in H$. Then $A$ is bounded. I prove this using the Banach-Steinhaus theorem. I defined $\varphi_y : H \to \mathbb{C}$ by $\varphi_y(x) := \langle A(y),x \rangle$ and $$\mathcal{F} := \{\varphi_y : y \in \partial B_1(0)\}$$ It is easy to show that $\mathcal{F}$ satisfies the prerequisites for the Banach-Steinhaus theorem and thus $$\sup_{T \in \mathcal{F}} \|T\| < \infty$$ Now I want to use this to show that $A$ is bounded. For $x \in H$ I compute $$\|A(x)\|^2 = \langle A(x),A(x) \rangle = \|x\|\langle A(x/\|x\|),A(x)\rangle = \|x\|\varphi_{x/\|x\|}(A(x))$$ But somehow I cannot get rid of the $A(x)$ in the argument of $\varphi$. Can anyone help me how to proceed?","Consider the following statement: ( Hellinger-Toeplitz Theorem ) Let $H$ be a Hilbert space and $A : H \to H$ be linear and symmetric,   i.e. $$\langle x,Ay \rangle = \langle Ax,y \rangle$$ holds for all   $x,y \in H$. Then $A$ is bounded. I prove this using the Banach-Steinhaus theorem. I defined $\varphi_y : H \to \mathbb{C}$ by $\varphi_y(x) := \langle A(y),x \rangle$ and $$\mathcal{F} := \{\varphi_y : y \in \partial B_1(0)\}$$ It is easy to show that $\mathcal{F}$ satisfies the prerequisites for the Banach-Steinhaus theorem and thus $$\sup_{T \in \mathcal{F}} \|T\| < \infty$$ Now I want to use this to show that $A$ is bounded. For $x \in H$ I compute $$\|A(x)\|^2 = \langle A(x),A(x) \rangle = \|x\|\langle A(x/\|x\|),A(x)\rangle = \|x\|\varphi_{x/\|x\|}(A(x))$$ But somehow I cannot get rid of the $A(x)$ in the argument of $\varphi$. Can anyone help me how to proceed?",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
52,Converse of Spectral Theorem for Compact Self-Adjoint Operators,Converse of Spectral Theorem for Compact Self-Adjoint Operators,,"If I have a bounded linear operator $A$ on a Hilbert space $H$ whose eigenvectors form an orthonormal basis for $H$ and whose corresponding eigenvalues go to $0$ then is $A$ compact and self-adjoint? I ask because I want to prove that $A$ defined on an orthonormal basis $\{e_k\}$ as $Ae_k=e_k/(k^2+1)$ is compact and self-adjoint. I know that it is, but I'm just wondering if appealing to the spectral theorem is valid. Thanks!","If I have a bounded linear operator $A$ on a Hilbert space $H$ whose eigenvectors form an orthonormal basis for $H$ and whose corresponding eigenvalues go to $0$ then is $A$ compact and self-adjoint? I ask because I want to prove that $A$ defined on an orthonormal basis $\{e_k\}$ as $Ae_k=e_k/(k^2+1)$ is compact and self-adjoint. I know that it is, but I'm just wondering if appealing to the spectral theorem is valid. Thanks!",,"['functional-analysis', 'eigenvalues-eigenvectors', 'hilbert-spaces', 'spectral-theory']"
53,Linear Map is injective iff Adjoint has dense image,Linear Map is injective iff Adjoint has dense image,,"$A,B$ are locally convex topological spaces, $f:A\to B$ is linear, continuous. I want to verify the adjoint map has dense image in $A^*$ with respect to the weak* topology if and only if $f$ is injective. Attempted Proof:  If $fx=0$ then $\langle fx,g\rangle=0$ for each $g\in A^*$. Hence $\langle x,f^*g\rangle=0$ so the denseness implies $\langle x,g\rangle=0$ for each $g\in A^*$. Thus $x=0$. On the other hand, suppose the range is not dense. By Hahn Banach, there is some nonzero $g\in A^*$ such that $g(fx)=0$ for each $x\in A$. Then $\langle x,f^*g \rangle=0$ for all $x$. So $f^*g=0$. This is a contradiction since injectivity of $f^*$ means that $g=0$. Question: Am I using the correct version of Hahn-Banach? And does my argument show that $f^*$ has dense image with respect to the weak topology?","$A,B$ are locally convex topological spaces, $f:A\to B$ is linear, continuous. I want to verify the adjoint map has dense image in $A^*$ with respect to the weak* topology if and only if $f$ is injective. Attempted Proof:  If $fx=0$ then $\langle fx,g\rangle=0$ for each $g\in A^*$. Hence $\langle x,f^*g\rangle=0$ so the denseness implies $\langle x,g\rangle=0$ for each $g\in A^*$. Thus $x=0$. On the other hand, suppose the range is not dense. By Hahn Banach, there is some nonzero $g\in A^*$ such that $g(fx)=0$ for each $x\in A$. Then $\langle x,f^*g \rangle=0$ for all $x$. So $f^*g=0$. This is a contradiction since injectivity of $f^*$ means that $g=0$. Question: Am I using the correct version of Hahn-Banach? And does my argument show that $f^*$ has dense image with respect to the weak topology?",,"['functional-analysis', 'locally-convex-spaces']"
54,Normal derivative of a $H^1$- Sobolev function,Normal derivative of a - Sobolev function,H^1,"Let $u\in H^1(\Omega)$, where $\Omega$ is a bounded open set of $\mathbb{R}^n$ with Lipschitz boundary. We denote the outward unit normal as $n$, defined a.e. on $\partial\Omega$, and the normal derivative of $u$ as $$ \frac{\partial u}{\partial n}:=\nabla u\cdot n. $$ Which space does the normal derivative belong to? Is it possible to show $\frac{\partial u}{\partial n}\in L^2(\partial\Omega)$? I think it's not possible if we don't require at least that $u\in H^2(\Omega)$. Indeed it is easy to get $$ \|\frac{\partial u}{\partial n}\|_{L^2(\partial \Omega)}\le \|\nabla u \|_{L^2(\partial \Omega)}. $$ By the Trace theorem, we know that $\nabla u \in L^2(\partial\Omega)$ if $\nabla u\in H^1(\Omega)$, i.e. $u\in H^2(\Omega)$. Note that my notation is quite messy when I deal with the norm of the gradient...","Let $u\in H^1(\Omega)$, where $\Omega$ is a bounded open set of $\mathbb{R}^n$ with Lipschitz boundary. We denote the outward unit normal as $n$, defined a.e. on $\partial\Omega$, and the normal derivative of $u$ as $$ \frac{\partial u}{\partial n}:=\nabla u\cdot n. $$ Which space does the normal derivative belong to? Is it possible to show $\frac{\partial u}{\partial n}\in L^2(\partial\Omega)$? I think it's not possible if we don't require at least that $u\in H^2(\Omega)$. Indeed it is easy to get $$ \|\frac{\partial u}{\partial n}\|_{L^2(\partial \Omega)}\le \|\nabla u \|_{L^2(\partial \Omega)}. $$ By the Trace theorem, we know that $\nabla u \in L^2(\partial\Omega)$ if $\nabla u\in H^1(\Omega)$, i.e. $u\in H^2(\Omega)$. Note that my notation is quite messy when I deal with the norm of the gradient...",,"['functional-analysis', 'sobolev-spaces']"
55,Hilbert-C*-Modules and interior tensor products,Hilbert-C*-Modules and interior tensor products,,"Let $A$ and $B$ be $C^*$-Algebras, $E$ a Hilbert-$A$-module, $F$ a Hilber-$B$-Module and  $$\pi: A \rightarrow \mathcal{L}_B(F)$$ a $*$-homorphisms of C*-Algebras, where $\mathcal{L}_B(F)$ denotes the C*-Algebra of adjointable operators on $F$. Now, let $x \in E$ and $y \in F$ such that $$\langle y, \pi(\langle x,x \rangle)(y) \rangle=0.$$ I want to show that for any $T \in \mathcal{L}_A(E)$ we have  $$\langle y, \pi(\langle T(x),T(x) \rangle)(y) \rangle=0.$$ This would prove that the homomorphism $$F \otimes 1: E\otimes_\pi F \rightarrow E\otimes_\pi F $$ is well defined. Thank you","Let $A$ and $B$ be $C^*$-Algebras, $E$ a Hilbert-$A$-module, $F$ a Hilber-$B$-Module and  $$\pi: A \rightarrow \mathcal{L}_B(F)$$ a $*$-homorphisms of C*-Algebras, where $\mathcal{L}_B(F)$ denotes the C*-Algebra of adjointable operators on $F$. Now, let $x \in E$ and $y \in F$ such that $$\langle y, \pi(\langle x,x \rangle)(y) \rangle=0.$$ I want to show that for any $T \in \mathcal{L}_A(E)$ we have  $$\langle y, \pi(\langle T(x),T(x) \rangle)(y) \rangle=0.$$ This would prove that the homomorphism $$F \otimes 1: E\otimes_\pi F \rightarrow E\otimes_\pi F $$ is well defined. Thank you",,"['functional-analysis', 'operator-algebras']"
56,Every bounded sequence of dual space contains a subsequence which is weak* convergent,Every bounded sequence of dual space contains a subsequence which is weak* convergent,,"I were doing this problem in Functional Analysis of Erwin Kreyszig(part 4.9, problem 10, page 269), but got stuck in the last point to come to the conclusion. Can anyone give me some hint to move on? Thanks. Let $X$ be a separable Banach space and $M \subset X'$ a bounded set. Show that every sequence of elements of $M$ contains a subsequence which is weak* convergent to an element of $X'$. Here $X'$ is the space contains all bounded linear functionals on $X$. Let $\{T_n\} \subset X'$, then we said $\{T_n\}$ weak* converges to $T \in X'$ if $\lim_{n \rightarrow \infty}{T_n(x)} = T(x)$ for all $x \in X$. What I tried so far: Suppose we have a given sequence $\{T_n\} \subset M$. From the assumption, we've already have the sequence $\{||T_n||\}$ bounded, so from Corollary 4.9-7, we only need to find a subsequence $\{T_{n_i}\}$ of $\{T_n\}$such that the sequence $\{T_{n_i}(x)\}$ is Cauchy for every x in a total subset of $X$. Because $X$ is separable, there exists a dense countable subset of $X$, we call $S$. For each $x \in X$, because $\{||T_n||\}$ bounded, we must have $\{||T_n(x)||\}$ bounded for each $x \in S$, by Bozzano-Weirstrass, there exists convergent subsequence of this sequence, and obviously, the subsequence is Cauchy sequence. Here is the place I got stuck. If $S$ is finite, then we can repeat that process for the subsequence we just found, and finally, we have the subsequence $\{T_{n_i}\}$ of $\{T_n\}$ such that the $\{T_{n_i}(x)\}$ is Cauchy for all $x \in S$. But because $S$ is not finite, but countable, we can't do that. Is there any trick to overcome this difficulty? I really appreciate any help.","I were doing this problem in Functional Analysis of Erwin Kreyszig(part 4.9, problem 10, page 269), but got stuck in the last point to come to the conclusion. Can anyone give me some hint to move on? Thanks. Let $X$ be a separable Banach space and $M \subset X'$ a bounded set. Show that every sequence of elements of $M$ contains a subsequence which is weak* convergent to an element of $X'$. Here $X'$ is the space contains all bounded linear functionals on $X$. Let $\{T_n\} \subset X'$, then we said $\{T_n\}$ weak* converges to $T \in X'$ if $\lim_{n \rightarrow \infty}{T_n(x)} = T(x)$ for all $x \in X$. What I tried so far: Suppose we have a given sequence $\{T_n\} \subset M$. From the assumption, we've already have the sequence $\{||T_n||\}$ bounded, so from Corollary 4.9-7, we only need to find a subsequence $\{T_{n_i}\}$ of $\{T_n\}$such that the sequence $\{T_{n_i}(x)\}$ is Cauchy for every x in a total subset of $X$. Because $X$ is separable, there exists a dense countable subset of $X$, we call $S$. For each $x \in X$, because $\{||T_n||\}$ bounded, we must have $\{||T_n(x)||\}$ bounded for each $x \in S$, by Bozzano-Weirstrass, there exists convergent subsequence of this sequence, and obviously, the subsequence is Cauchy sequence. Here is the place I got stuck. If $S$ is finite, then we can repeat that process for the subsequence we just found, and finally, we have the subsequence $\{T_{n_i}\}$ of $\{T_n\}$ such that the $\{T_{n_i}(x)\}$ is Cauchy for all $x \in S$. But because $S$ is not finite, but countable, we can't do that. Is there any trick to overcome this difficulty? I really appreciate any help.",,"['functional-analysis', 'banach-spaces', 'weak-convergence']"
57,How do fixed point arguments for PDE work?,How do fixed point arguments for PDE work?,,"My understanding (which I'm not altogether sure of) is that a a ""fixed point"" argument often used in PDE goes something like this: If we have some PDE like $$u=F(u),$$ we consider a sequence of functions $u^{(n)}$ recursively defined so $$u^{(n+1)}=F(u^{(n)}).$$ Then you compute an estimate like $$\|u^{(n+1)}\|_X\leq g(\|u^{(n)}\|_X)$$ (where the mathematician knows what function space $X$ should be) and then you solve the recurrence equation to find say $$\|u^{(n)}\|_X\leq c$$ and then use some kind of compactness argument to say the $u^{(n)}$ have a particular kind of limit in $X$ which by construction solves the desired equation. Is my understanding of this strategy broadly correct? There are some details I don't understand. For instance, what is the particular kind of compactness argument we need? Say $X=H^s$. A ball in $H^s$ is not compact in the norm topology, right? So how does one make this work? I ask this question because it seems that people who do PDE use things like this all the time so they skim over the technical details.","My understanding (which I'm not altogether sure of) is that a a ""fixed point"" argument often used in PDE goes something like this: If we have some PDE like $$u=F(u),$$ we consider a sequence of functions $u^{(n)}$ recursively defined so $$u^{(n+1)}=F(u^{(n)}).$$ Then you compute an estimate like $$\|u^{(n+1)}\|_X\leq g(\|u^{(n)}\|_X)$$ (where the mathematician knows what function space $X$ should be) and then you solve the recurrence equation to find say $$\|u^{(n)}\|_X\leq c$$ and then use some kind of compactness argument to say the $u^{(n)}$ have a particular kind of limit in $X$ which by construction solves the desired equation. Is my understanding of this strategy broadly correct? There are some details I don't understand. For instance, what is the particular kind of compactness argument we need? Say $X=H^s$. A ball in $H^s$ is not compact in the norm topology, right? So how does one make this work? I ask this question because it seems that people who do PDE use things like this all the time so they skim over the technical details.",,"['functional-analysis', 'partial-differential-equations']"
58,What is the implication that $\| \cdot \|_2$ and $\| \cdot \|_\infty$ are equivalent norms on $\mathbb{R^2}$,What is the implication that  and  are equivalent norms on,\| \cdot \|_2 \| \cdot \|_\infty \mathbb{R^2},"Given $\mathbb{X}$ = $\mathbb{R^2}$, consider $\| \cdot \|_2$ and $\| \cdot \|_\infty$ We can show that $\| x \|_\infty \leq \| x \|_2 \leq \sqrt2 \| x \|_\infty$ Hence $\| \cdot \|_2$ and $\| \cdot \|_\infty$ are equivalent norms Is there some deeper implication regarding this particular relationship? Why do we care if two norms are equivalent in this sense?","Given $\mathbb{X}$ = $\mathbb{R^2}$, consider $\| \cdot \|_2$ and $\| \cdot \|_\infty$ We can show that $\| x \|_\infty \leq \| x \|_2 \leq \sqrt2 \| x \|_\infty$ Hence $\| \cdot \|_2$ and $\| \cdot \|_\infty$ are equivalent norms Is there some deeper implication regarding this particular relationship? Why do we care if two norms are equivalent in this sense?",,"['functional-analysis', 'vector-spaces', 'banach-spaces', 'normed-spaces']"
59,Is this set of random variables a Hilbert space?,Is this set of random variables a Hilbert space?,,"Consider a sequence of i.i.d. random variables $\left\{ {{\varepsilon _t}} \right\}_{t = 1}^\infty $ with $E\left( {{\varepsilon _t}} \right) = 0$  and $E\left( {\varepsilon _t^2} \right) = {\sigma ^2} < \infty $ and denote by ${\varepsilon ^t} = ({\varepsilon _{1,}}{\varepsilon _{2,}} \ldots {\varepsilon _t})$ the history of the process up to and including period $t$. Let $0 < \beta  < 1$. Define P as the set of all ${R^\infty }{\rm{ - valued}}$ functions $x(\varepsilon ) = \left\{{{x_t}({\varepsilon ^t})} \right\}_{t = 1}^\infty$ such that $\sum\limits_{t = 1}^\infty  {{\beta ^t}x_t^2} \mathop  < \limits^{a.s.} \infty $ and ${E_{t = 0}}\sum\limits_{t = 0}^\infty  {{\beta ^t}x_t^2}  < \infty $ exist. For meaning/intuition: ${x_t} = {x_t}({\varepsilon ^t})$ are decision rules that can depend only on information ${\varepsilon ^t}$ available at time $t$. I have the following question: Is P a Hilbert space with the product $\langle x,y\rangle  = {E_{t = 0}}\left( {\sum\limits_{t = 1}^\infty  {{\beta ^t}{x_t}{y_t}} } \right)$ and associated norm $$\left\| x \right\| = {\langle x,x\rangle ^{1/2}} = {\left( {{E_{t = 0}}\left( {\sum\limits_{t = 0}^\infty  {{\beta ^t}x_t^2} } \right)} \right)^{1/2}}$$? (By ${E_{t = 0}}$ I mean the expectation at t=0, before any information on the ${\varepsilon _t}$ is available. By a.s. I mean almost surely.) I guess the tricky part is to prove that P is complete (with the norm (is it a norm?) just described), and perhaps, that if $\left\| x \right\| = 0$ then $x\mathop= \limits^{a.s.} 0$ ? I will be very grateful for any suggestions or references. I am not a mathematician, so even steps that may seem elementary to you would help me.","Consider a sequence of i.i.d. random variables $\left\{ {{\varepsilon _t}} \right\}_{t = 1}^\infty $ with $E\left( {{\varepsilon _t}} \right) = 0$  and $E\left( {\varepsilon _t^2} \right) = {\sigma ^2} < \infty $ and denote by ${\varepsilon ^t} = ({\varepsilon _{1,}}{\varepsilon _{2,}} \ldots {\varepsilon _t})$ the history of the process up to and including period $t$. Let $0 < \beta  < 1$. Define P as the set of all ${R^\infty }{\rm{ - valued}}$ functions $x(\varepsilon ) = \left\{{{x_t}({\varepsilon ^t})} \right\}_{t = 1}^\infty$ such that $\sum\limits_{t = 1}^\infty  {{\beta ^t}x_t^2} \mathop  < \limits^{a.s.} \infty $ and ${E_{t = 0}}\sum\limits_{t = 0}^\infty  {{\beta ^t}x_t^2}  < \infty $ exist. For meaning/intuition: ${x_t} = {x_t}({\varepsilon ^t})$ are decision rules that can depend only on information ${\varepsilon ^t}$ available at time $t$. I have the following question: Is P a Hilbert space with the product $\langle x,y\rangle  = {E_{t = 0}}\left( {\sum\limits_{t = 1}^\infty  {{\beta ^t}{x_t}{y_t}} } \right)$ and associated norm $$\left\| x \right\| = {\langle x,x\rangle ^{1/2}} = {\left( {{E_{t = 0}}\left( {\sum\limits_{t = 0}^\infty  {{\beta ^t}x_t^2} } \right)} \right)^{1/2}}$$? (By ${E_{t = 0}}$ I mean the expectation at t=0, before any information on the ${\varepsilon _t}$ is available. By a.s. I mean almost surely.) I guess the tricky part is to prove that P is complete (with the norm (is it a norm?) just described), and perhaps, that if $\left\| x \right\| = 0$ then $x\mathop= \limits^{a.s.} 0$ ? I will be very grateful for any suggestions or references. I am not a mathematician, so even steps that may seem elementary to you would help me.",,"['functional-analysis', 'probability-theory', 'stochastic-processes', 'random-variables', 'discrete-optimization']"
60,History of inner products and texts on it?,History of inner products and texts on it?,,"Where does the inner product originate from, was it defined in term of the dual or was it defined from just two copies of the space? I.e $(*,*) : V \times V \rightarrow  scalar $ or $(*,*) : V \times V^{*} \rightarrow  scalar $ and where can one find texts regarding this?","Where does the inner product originate from, was it defined in term of the dual or was it defined from just two copies of the space? I.e $(*,*) : V \times V \rightarrow  scalar $ or $(*,*) : V \times V^{*} \rightarrow  scalar $ and where can one find texts regarding this?",,"['functional-analysis', 'math-history', 'inner-products']"
61,Generalization of log-convexity (log-concavity): log-log-convexity (log-log-concavity)?,Generalization of log-convexity (log-concavity): log-log-convexity (log-log-concavity)?,,"$\underline{\mathrm{Background\; on\; function\; Convexity}}$ A function, $f$, is convex if: $$f( x\theta+y(1-\theta) ) \leq \theta f(x) + (1-\theta)f(y).$$ $f$ is concave if $-f$ is convex,  [ 1 ]. If we can demonstrate the convexity (or concavity) of a function, then function optimization can be performed using the well understood theory of convex optimization. Thus, if we can, it is beneficial to be able to do so. In some situations, a function may not be convex (or concave). However, its logarithm may still be. This is termed log-convexity (log-concavity). Determining log-convexity (log-concavity) allows us to exploit the same convex optimization procedures. $\underline{\mathrm{Question}}$ I want to know if the idea of log-convexity can be extended further. In particular, can we describe a function as being log-log-convex (log-log-concave) ? By log-log-convex (log-log-concave) I mean that if the logarithm of a function is plotted on a set of axes in which the abscissa has been expressed using a logarithmic scale, convexity (concavity) is observed. Said in a different way, if we plot a function on a set of axes where both the ordinate and abscissa have been expressed using a logarithmic scale, convexity (concavity) is observed.","$\underline{\mathrm{Background\; on\; function\; Convexity}}$ A function, $f$, is convex if: $$f( x\theta+y(1-\theta) ) \leq \theta f(x) + (1-\theta)f(y).$$ $f$ is concave if $-f$ is convex,  [ 1 ]. If we can demonstrate the convexity (or concavity) of a function, then function optimization can be performed using the well understood theory of convex optimization. Thus, if we can, it is beneficial to be able to do so. In some situations, a function may not be convex (or concave). However, its logarithm may still be. This is termed log-convexity (log-concavity). Determining log-convexity (log-concavity) allows us to exploit the same convex optimization procedures. $\underline{\mathrm{Question}}$ I want to know if the idea of log-convexity can be extended further. In particular, can we describe a function as being log-log-convex (log-log-concave) ? By log-log-convex (log-log-concave) I mean that if the logarithm of a function is plotted on a set of axes in which the abscissa has been expressed using a logarithmic scale, convexity (concavity) is observed. Said in a different way, if we plot a function on a set of axes where both the ordinate and abscissa have been expressed using a logarithmic scale, convexity (concavity) is observed.",,"['functional-analysis', 'convex-analysis', 'convex-optimization']"
62,How to find the infinitesimal generator of this semigroup?,How to find the infinitesimal generator of this semigroup?,,"Definition 1: Let $X$ be a Banach space. A semigroup is a family $\{T(t)\}_{t\geq 0}$ of continuous linear operators $T(t):X\to X$ such that $(i)\;\;T(0)=I$, where $I$ is the identity operator; $(ii)\;\;T(s)\circ T(t)=T(t+s)$ for all $t,s\geq 0$. Definition 2: the infinitesimal generator of a semigroup $\{T(t)\}_{t\geq 0}$ is the operator $A:D(A)\to X$ where: $$D(A)=\left\{x\in X;\;\lim_{h\to 0^+}\frac{T(h)x-x}{h}\text{ exists in } X \right\}$$ and $$A(x)=\lim_{h\to 0^+}\frac{T(h)x-x}{h}$$ for all $x\in D(A)$. Definition 3: the translation of the function $f:\mathbb{R}\to\mathbb{R}$ is the function $f_t:\mathbb{R}\to\mathbb{R}$given by $f_t(x)=f(x+t)$ for all $x\in\mathbb{R}$. Take $X=L^2(\mathbb{R})$ in definition 1 and consider the semigroup $T:=\{T(t)\}_{t\geq 0}$ where $T(t)f=f_t$ for all $f\in L^2(\mathbb{R})$. My problem is to find the infinitesimal generator of $T$. First of all I need to find $D(A)$, that is, I need to find  all $f\in L^2(\mathbb{R})$ such that $$\lim_{h\to 0^+}\frac{f_h-f}{h}=\lim_{h\to 0^+}\frac{T(h)f-f}{h}=g\tag{1}$$ for some $g\in L^2(\mathbb{R})$. Could someone explain me how can we conclude? Any help is appreciated. Thanks.","Definition 1: Let $X$ be a Banach space. A semigroup is a family $\{T(t)\}_{t\geq 0}$ of continuous linear operators $T(t):X\to X$ such that $(i)\;\;T(0)=I$, where $I$ is the identity operator; $(ii)\;\;T(s)\circ T(t)=T(t+s)$ for all $t,s\geq 0$. Definition 2: the infinitesimal generator of a semigroup $\{T(t)\}_{t\geq 0}$ is the operator $A:D(A)\to X$ where: $$D(A)=\left\{x\in X;\;\lim_{h\to 0^+}\frac{T(h)x-x}{h}\text{ exists in } X \right\}$$ and $$A(x)=\lim_{h\to 0^+}\frac{T(h)x-x}{h}$$ for all $x\in D(A)$. Definition 3: the translation of the function $f:\mathbb{R}\to\mathbb{R}$ is the function $f_t:\mathbb{R}\to\mathbb{R}$given by $f_t(x)=f(x+t)$ for all $x\in\mathbb{R}$. Take $X=L^2(\mathbb{R})$ in definition 1 and consider the semigroup $T:=\{T(t)\}_{t\geq 0}$ where $T(t)f=f_t$ for all $f\in L^2(\mathbb{R})$. My problem is to find the infinitesimal generator of $T$. First of all I need to find $D(A)$, that is, I need to find  all $f\in L^2(\mathbb{R})$ such that $$\lim_{h\to 0^+}\frac{f_h-f}{h}=\lim_{h\to 0^+}\frac{T(h)f-f}{h}=g\tag{1}$$ for some $g\in L^2(\mathbb{R})$. Could someone explain me how can we conclude? Any help is appreciated. Thanks.",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'banach-spaces', 'semigroup-of-operators']"
63,Complete Condition in Banach Fixed Point Theorem,Complete Condition in Banach Fixed Point Theorem,,"Can someone provide an example to show that for the Banach fixed point theorem, that is if $T : X → X$ is a contraction in a complete metric space $(X, d)$ then $T$ has a unique fixed point that $X$ is complete is an essential condition?","Can someone provide an example to show that for the Banach fixed point theorem, that is if $T : X → X$ is a contraction in a complete metric space $(X, d)$ then $T$ has a unique fixed point that $X$ is complete is an essential condition?",,"['functional-analysis', 'metric-spaces', 'fixed-point-theorems']"
64,A detail in the proof of Banach-Steinhaus theorem that I don't understand,A detail in the proof of Banach-Steinhaus theorem that I don't understand,,"I am studying functional analysis and I have seen the Banach-Steinhaus theorem. For starters, the motivation given was the question about when $\{T_{\alpha}\}_{\alpha\in A}$ are bounded by $M$ (here the linear operators are $T_{\alpha}:X\to Y$). The proof given was: Define  $$ F_{n}:=\{x\in X\mid||T_{\alpha}x||\leq n\,\forall\alpha\} $$ Then by the assumption that the operators are pointwise bounded we get   $$ X=\cup_{n}F_{n} $$ By Baire category theorem there is $n$ s.t $F_{n}$ contains a ball   $B_{\epsilon}^{X}(x_{0})$. Pick $\alpha$.  $$  T_{\alpha}(B_{\epsilon}^{X}(x_{0})-B_{\epsilon}^{X}(x_{0}))\subseteq  B_{2n}^{Y}(0) $$ Hence  $$ ||T_{\alpha}||\leq2n\cdot\frac{1}{\epsilon} $$ Can someone please explain the ""Hence"" part at the end ? So all $T_{\alpha}$ map some subset of $X$ to a bounded ball of a fixed radius $2n$, how did we bound $T_{\alpha}$ for all $x\in X$ ?","I am studying functional analysis and I have seen the Banach-Steinhaus theorem. For starters, the motivation given was the question about when $\{T_{\alpha}\}_{\alpha\in A}$ are bounded by $M$ (here the linear operators are $T_{\alpha}:X\to Y$). The proof given was: Define  $$ F_{n}:=\{x\in X\mid||T_{\alpha}x||\leq n\,\forall\alpha\} $$ Then by the assumption that the operators are pointwise bounded we get   $$ X=\cup_{n}F_{n} $$ By Baire category theorem there is $n$ s.t $F_{n}$ contains a ball   $B_{\epsilon}^{X}(x_{0})$. Pick $\alpha$.  $$  T_{\alpha}(B_{\epsilon}^{X}(x_{0})-B_{\epsilon}^{X}(x_{0}))\subseteq  B_{2n}^{Y}(0) $$ Hence  $$ ||T_{\alpha}||\leq2n\cdot\frac{1}{\epsilon} $$ Can someone please explain the ""Hence"" part at the end ? So all $T_{\alpha}$ map some subset of $X$ to a bounded ball of a fixed radius $2n$, how did we bound $T_{\alpha}$ for all $x\in X$ ?",,"['functional-analysis', 'banach-spaces']"
65,Do $L^2$ convergence and continuity imply pointwise convergence?,Do  convergence and continuity imply pointwise convergence?,L^2,It is said here that $L^2$ convergence and continuity imply pointwise convergence (just before paragraph $5.2$) but I can't find how to prove it. Does anyone see how ?,It is said here that $L^2$ convergence and continuity imply pointwise convergence (just before paragraph $5.2$) but I can't find how to prove it. Does anyone see how ?,,"['functional-analysis', 'convergence-divergence', 'continuity']"
66,States and positive elements in $C^*$-algebras,States and positive elements in -algebras,C^*,"Let $A$ be a unital $C^*$-algebra and $w$ be a state (i.e a positive linear functional such that $\|w\|=w(1_A)=1$. I'm trying to prove the following: a) if $a$ is selfadjoint and $w(a^2)=w(a)^2$ then for any $b\in A$ we have $w(ab)=w(a)w(b)=w(ba)$. b) if $\|a\|\le 1$ and  $T=\left( \begin{array}{cc} 1_A & a  \\ a^* & 1_A \\ \end{array} \right) $ then $T$ is positive. What I have done: for part a) I proved that for any $a\in A$ we have $|w(a)|\le w(|a|^2)^\frac{1}{2}$ and I know that for any state we can define a semi inner product as follows $<a,b>=w(b^*a)$ but I cant get the result I want. for part b) it is clear that $T$ is selft adjoint, so if $\sigma(T)\subset[0,\infty)$ then $T$ is positive, I'm not sure how to find the spectrum of $T$. Thank you for your help.","Let $A$ be a unital $C^*$-algebra and $w$ be a state (i.e a positive linear functional such that $\|w\|=w(1_A)=1$. I'm trying to prove the following: a) if $a$ is selfadjoint and $w(a^2)=w(a)^2$ then for any $b\in A$ we have $w(ab)=w(a)w(b)=w(ba)$. b) if $\|a\|\le 1$ and  $T=\left( \begin{array}{cc} 1_A & a  \\ a^* & 1_A \\ \end{array} \right) $ then $T$ is positive. What I have done: for part a) I proved that for any $a\in A$ we have $|w(a)|\le w(|a|^2)^\frac{1}{2}$ and I know that for any state we can define a semi inner product as follows $<a,b>=w(b^*a)$ but I cant get the result I want. for part b) it is clear that $T$ is selft adjoint, so if $\sigma(T)\subset[0,\infty)$ then $T$ is positive, I'm not sure how to find the spectrum of $T$. Thank you for your help.",,"['analysis', 'functional-analysis', 'operator-algebras', 'c-star-algebras']"
67,Image of unit ball dense under continuous map between banach spaces,Image of unit ball dense under continuous map between banach spaces,,"I am assuming that the following problem will require the open mapping theorem, or maybe the closed graph theorem. Any help that can be given will be deeply appreciated. The statement is the following: Let $T: X \to Y$ a continuous linear operator between Banach spaces $X,Y$. Show that $T$ is open if and only if the image under $T$ of the open unit ball in $X$ is dense in a neighbourhood of the origin in $Y$. Thank you very much in advance!","I am assuming that the following problem will require the open mapping theorem, or maybe the closed graph theorem. Any help that can be given will be deeply appreciated. The statement is the following: Let $T: X \to Y$ a continuous linear operator between Banach spaces $X,Y$. Show that $T$ is open if and only if the image under $T$ of the open unit ball in $X$ is dense in a neighbourhood of the origin in $Y$. Thank you very much in advance!",,"['analysis', 'functional-analysis']"
68,"How many well-behaved norms are there on $C[0,1]$?",How many well-behaved norms are there on ?,"C[0,1]","Let $\Vert \cdot \Vert$ a norm on $X=C([0,1])$ s.t. 1 . $X$ is complete w.r.t. $\Vert \cdot \Vert$; 2 . convergence in $\Vert \cdot \Vert$ implies pointwise convegence, i.e.    $$ \Vert x_n - x \Vert \to 0 \Rightarrow \forall t \in [0,1], \quad x_n(t) \to x(t). $$ Is $\Vert \cdot \Vert$ equivalent to the usual $\sup$-norm, $\Vert \cdot \Vert_{\infty}$? By open mapping and 1 (completeness), we just need to prove only one inequality of the two we need to prove equivalence (indeed, the other would follow by open mapping). Anyway, I think the answer is affirmative; am I right? I do not know how to prove it. Any ideas, please? Thanks.","Let $\Vert \cdot \Vert$ a norm on $X=C([0,1])$ s.t. 1 . $X$ is complete w.r.t. $\Vert \cdot \Vert$; 2 . convergence in $\Vert \cdot \Vert$ implies pointwise convegence, i.e.    $$ \Vert x_n - x \Vert \to 0 \Rightarrow \forall t \in [0,1], \quad x_n(t) \to x(t). $$ Is $\Vert \cdot \Vert$ equivalent to the usual $\sup$-norm, $\Vert \cdot \Vert_{\infty}$? By open mapping and 1 (completeness), we just need to prove only one inequality of the two we need to prove equivalence (indeed, the other would follow by open mapping). Anyway, I think the answer is affirmative; am I right? I do not know how to prove it. Any ideas, please? Thanks.",,"['functional-analysis', 'convergence-divergence']"
69,Fixed point of a non linear contraction in a convex set,Fixed point of a non linear contraction in a convex set,,"Hi I'm stuck on the following problem of Haim's functional analysis book. Let $C\subseteq H$ ($H$ a Hilbert space) be a non-empty closed convex subset and let $T:C\rightarrow C$ be a non linear contraction, i.e., $|Tu-Tv|\leq|u-v|$ $\forall u,v\in C$ Let $(u_{n})$ be a sequence in $C$ such that $u_{n}\rightharpoonup u$ weakly and $u_{n}-Tu_{n}\rightarrow f$ strongly. Prove that $u-Tu=f$. (HINT: start with the case $C=H$ and use the inequality $<(u-Tu)-(v-Tv),u-v>\geq0$) Deduce that if $C$ is bounded then $T$ has a fixed point. (HINT: Consider $T_{\varepsilon}u=(1-\varepsilon)Tu+\varepsilon a$ with $a\in C$ fixed and $\varepsilon>0$.) Thanks for your help.","Hi I'm stuck on the following problem of Haim's functional analysis book. Let $C\subseteq H$ ($H$ a Hilbert space) be a non-empty closed convex subset and let $T:C\rightarrow C$ be a non linear contraction, i.e., $|Tu-Tv|\leq|u-v|$ $\forall u,v\in C$ Let $(u_{n})$ be a sequence in $C$ such that $u_{n}\rightharpoonup u$ weakly and $u_{n}-Tu_{n}\rightarrow f$ strongly. Prove that $u-Tu=f$. (HINT: start with the case $C=H$ and use the inequality $<(u-Tu)-(v-Tv),u-v>\geq0$) Deduce that if $C$ is bounded then $T$ has a fixed point. (HINT: Consider $T_{\varepsilon}u=(1-\varepsilon)Tu+\varepsilon a$ with $a\in C$ fixed and $\varepsilon>0$.) Thanks for your help.",,"['functional-analysis', 'convex-analysis', 'fixed-point-theorems']"
70,What exactly is a Haar measure,What exactly is a Haar measure,,"I've come across at least 3 definitions, for example: Taken from here where $\Gamma$ is a topological group. Apparently, this definition doesn't require the Haar measure to be finite on compact sets. Or from Wikipedia : ""... In this article, the $\sigma$-algebra generated by all compact subsets of $G$ is called the Borel algebra...."" Then $\mu$ defined on this sigma algebra is a Haar measure if it's outer and inner regular, finite on compact sets and translation invariant. So I gather the important property of a Haar measure is that it's translation invariant. Question 1: What I don't gather is, what do I get if I define it on the Borel sigma algebra as opposed to defining it on the sigma algebra generated by compact sets (as they do on Wikipedia)? Question 2: Can I put additional assumptions on $G$ so that I can drop the requirement that $\mu$ has to be finite on compact sets? Question 3: As you can guess from my questions I'm poking around in the dark trying to find out how to define a Haar measure suitably. Here suitably means, I want to use it to define an inner product so I can have Fourier series. Are there several ways to do this which lead to different spaces? By this I mean, if I define it on the Borel sigma algebra, can I do Fourier series for a different set of functions than when I have a measure on the sigma algebra generated by compact sets? Or what about dropping regularity? Or dropping finiteness on compact sets? Thanks.","I've come across at least 3 definitions, for example: Taken from here where $\Gamma$ is a topological group. Apparently, this definition doesn't require the Haar measure to be finite on compact sets. Or from Wikipedia : ""... In this article, the $\sigma$-algebra generated by all compact subsets of $G$ is called the Borel algebra...."" Then $\mu$ defined on this sigma algebra is a Haar measure if it's outer and inner regular, finite on compact sets and translation invariant. So I gather the important property of a Haar measure is that it's translation invariant. Question 1: What I don't gather is, what do I get if I define it on the Borel sigma algebra as opposed to defining it on the sigma algebra generated by compact sets (as they do on Wikipedia)? Question 2: Can I put additional assumptions on $G$ so that I can drop the requirement that $\mu$ has to be finite on compact sets? Question 3: As you can guess from my questions I'm poking around in the dark trying to find out how to define a Haar measure suitably. Here suitably means, I want to use it to define an inner product so I can have Fourier series. Are there several ways to do this which lead to different spaces? By this I mean, if I define it on the Borel sigma algebra, can I do Fourier series for a different set of functions than when I have a measure on the sigma algebra generated by compact sets? Or what about dropping regularity? Or dropping finiteness on compact sets? Thanks.",,"['functional-analysis', 'measure-theory', 'fourier-analysis']"
71,Do elliptic operators on Riemannian manifolds have a regularizing effect?,Do elliptic operators on Riemannian manifolds have a regularizing effect?,,"I'm working on my master thesis and need to handle some spectral theory of the Laplace operator on compact Riemannian manifolds and especially on the sphere. While investigating essential self-adjointness I stumbled on the following problem. * Problem *$\quad$ In a compact Riemannian manifold $M$ let $$\Delta=\operatorname{div}\operatorname{grad}$$ and let $f\in L^2(M)$ be such that $(f, u-\Delta u)=0$ for every $u \in C^{\infty}(M)$. Prove that $f=0$. I believe that the claim is true, because the condition $(f, u-\Delta u)=0$ means exactly that $f$ is a distributional solution of the elliptic equation $-\Delta f + f=0$, and so I expect it to be a $H^2_{\text{loc}}$ function (see Theorem 2.1 of Berezin - Shubin's book ). Since $M$ is compact this must imply that $f\in H^1(M)$ so that integrating by parts we get $\lVert f \rVert_{H^1}^2=(f, f)+(\operatorname{grad}f, \operatorname{grad}f)=0$. Unfortunately Theorem 2.1 above is set in an open subset of the Euclidean space and I don't know if it is applicable verbatim in a Riemannian manifold. Can you point me to some reference on this? Thank you.","I'm working on my master thesis and need to handle some spectral theory of the Laplace operator on compact Riemannian manifolds and especially on the sphere. While investigating essential self-adjointness I stumbled on the following problem. * Problem *$\quad$ In a compact Riemannian manifold $M$ let $$\Delta=\operatorname{div}\operatorname{grad}$$ and let $f\in L^2(M)$ be such that $(f, u-\Delta u)=0$ for every $u \in C^{\infty}(M)$. Prove that $f=0$. I believe that the claim is true, because the condition $(f, u-\Delta u)=0$ means exactly that $f$ is a distributional solution of the elliptic equation $-\Delta f + f=0$, and so I expect it to be a $H^2_{\text{loc}}$ function (see Theorem 2.1 of Berezin - Shubin's book ). Since $M$ is compact this must imply that $f\in H^1(M)$ so that integrating by parts we get $\lVert f \rVert_{H^1}^2=(f, f)+(\operatorname{grad}f, \operatorname{grad}f)=0$. Unfortunately Theorem 2.1 above is set in an open subset of the Euclidean space and I don't know if it is applicable verbatim in a Riemannian manifold. Can you point me to some reference on this? Thank you.",,"['reference-request', 'functional-analysis', 'riemannian-geometry', 'spectral-theory']"
72,"Norm for continuous linear functionals, newbie questions","Norm for continuous linear functionals, newbie questions",,"Let $E$ be a normed vector space and let $f\colon E \to \mathbb{R}$ be a continuous linear functional. Define the dual norm of $f$ as $$ \|f\| = \sup_{\|x\|\leq 1} |f(x)|. $$ First question. I have to prove that the supremum defined is the same as we were to find it on the unitary sphere $\|x\|=1$. I fixed a $x$ on the sphere and considered the segment $tx$ with $t \in [0,1]$. So $|f(tx)|=t|f(x)|$, and the values are increasing from $0$ (in which $f$ is $0$) to $1$; repeating for all $x$ and observing that $f$ has max on the sphere, we get the result. Hope it is correct. Second question. Now I have to prove that $$ \|f\| = \sup_{\|x\|\leq 1} |f(x)| = \|f\| = \sup_{\|x\|\leq 1} f(x). $$ I don't know how to get rid of modulus, any idea?","Let $E$ be a normed vector space and let $f\colon E \to \mathbb{R}$ be a continuous linear functional. Define the dual norm of $f$ as $$ \|f\| = \sup_{\|x\|\leq 1} |f(x)|. $$ First question. I have to prove that the supremum defined is the same as we were to find it on the unitary sphere $\|x\|=1$. I fixed a $x$ on the sphere and considered the segment $tx$ with $t \in [0,1]$. So $|f(tx)|=t|f(x)|$, and the values are increasing from $0$ (in which $f$ is $0$) to $1$; repeating for all $x$ and observing that $f$ has max on the sphere, we get the result. Hope it is correct. Second question. Now I have to prove that $$ \|f\| = \sup_{\|x\|\leq 1} |f(x)| = \|f\| = \sup_{\|x\|\leq 1} f(x). $$ I don't know how to get rid of modulus, any idea?",,"['analysis', 'functional-analysis', 'normed-spaces']"
73,Existence of a completely supported probability measure,Existence of a completely supported probability measure,,"Given a compact Hausdorff space $X$, does there exist a probability $\mu$ on X such that the support of $\mu$ is $X$? This is equivalent to say, for any unital commutative C*-algebra, can we show the existence of a faithful state?","Given a compact Hausdorff space $X$, does there exist a probability $\mu$ on X such that the support of $\mu$ is $X$? This is equivalent to say, for any unital commutative C*-algebra, can we show the existence of a faithful state?",,"['measure-theory', 'functional-analysis', 'probability-theory', 'c-star-algebras']"
74,Weak convergence of Hilbert-space valued stochastic process,Weak convergence of Hilbert-space valued stochastic process,,"Suppose I have a sequence of stochastic processes $(X_n(t))_{t\in [0,1]}$ such that $X_n(t) \in H$ , where $H$ is some separable Hilbert space and $X_n \in C([0,1,], H)$ . Goal. I want to show that $X_n$ converges weakly to a stochastic process $X$ , i.e. $\mathrm{Law}(X_n) \rightharpoonup \mathrm{Law}(X)$ as probability measures on $C([0,1],H)$ . Question. Suppose I know that for any dual elements $\phi_1,\ldots,\phi_k \in H'$ , where $k$ is finite but arbitrary, it holds that the sequence of processes $(\langle{\phi_1,X_n(t)}\rangle,\ldots,\langle{\phi_k,X_n(t)}\rangle)_{t\in [0,1]} \in C([0,1], \mathbb{R}^k)$ converges weakly to $(\langle{\phi_1,X(t)}\rangle,\ldots,\langle{\phi_k,X(t)}\rangle)_{t\in [0,1]}$ . Does this imply that $X_n$ weakly converges to $X$ ? If it makes a difference, the desired limit $X$ is a Gaussian process. I suspect the answer is yes by some sort of approximation argument, but I couldn't find a reference in the usual suspects as far as texts on infinite-dimensional stochastic processes. Any help would (in particular, a specific reference if it's true) is greatly appreciated!","Suppose I have a sequence of stochastic processes such that , where is some separable Hilbert space and . Goal. I want to show that converges weakly to a stochastic process , i.e. as probability measures on . Question. Suppose I know that for any dual elements , where is finite but arbitrary, it holds that the sequence of processes converges weakly to . Does this imply that weakly converges to ? If it makes a difference, the desired limit is a Gaussian process. I suspect the answer is yes by some sort of approximation argument, but I couldn't find a reference in the usual suspects as far as texts on infinite-dimensional stochastic processes. Any help would (in particular, a specific reference if it's true) is greatly appreciated!","(X_n(t))_{t\in [0,1]} X_n(t) \in H H X_n \in C([0,1,], H) X_n X \mathrm{Law}(X_n) \rightharpoonup \mathrm{Law}(X) C([0,1],H) \phi_1,\ldots,\phi_k \in H' k (\langle{\phi_1,X_n(t)}\rangle,\ldots,\langle{\phi_k,X_n(t)}\rangle)_{t\in [0,1]} \in C([0,1], \mathbb{R}^k) (\langle{\phi_1,X(t)}\rangle,\ldots,\langle{\phi_k,X(t)}\rangle)_{t\in [0,1]} X_n X X","['functional-analysis', 'probability-theory', 'stochastic-processes', 'stochastic-analysis', 'probability-limit-theorems']"
75,Trace Class Operators On Manifolds With Boundary,Trace Class Operators On Manifolds With Boundary,,"Let $X$ be an $n$ -dimensional manifold with nonempty boundary $\partial X$ and $n\geq 2$ . Proposition 4.1 of this paper by Schrohe states that it is ""not very difficult"" to show: Proposition : A bounded operator on $L^2(X)$ with range in $H^{n+1}(X)$ is trace class. Tragically, I cannot figure out a proof. I've tried generalizing the approach outlined in chapter 8 of Roe's Elliptic Operators, Topology and Asymptotic Methods for the boundaryless case, I've thought about Mercer's Theorem but the setting seems quite different, and looked around in Hörmander's PDEs book III and I don't think it's in there. A hint, proof outline or reference would be greatly appreciated, thank you!","Let be an -dimensional manifold with nonempty boundary and . Proposition 4.1 of this paper by Schrohe states that it is ""not very difficult"" to show: Proposition : A bounded operator on with range in is trace class. Tragically, I cannot figure out a proof. I've tried generalizing the approach outlined in chapter 8 of Roe's Elliptic Operators, Topology and Asymptotic Methods for the boundaryless case, I've thought about Mercer's Theorem but the setting seems quite different, and looked around in Hörmander's PDEs book III and I don't think it's in there. A hint, proof outline or reference would be greatly appreciated, thank you!",X n \partial X n\geq 2 L^2(X) H^{n+1}(X),"['functional-analysis', 'differential-geometry', 'reference-request', 'operator-theory']"
76,"Why are coercive functions called coercive, and why is it useful?","Why are coercive functions called coercive, and why is it useful?",,"A function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is called coercive if $$ \frac{f(x)\cdot x}{\|x\|} \rightarrow \infty \;\; \text{as} \;\;\|x\|\rightarrow \infty.$$ I came across this requirement in calculus of variations, where a coercivity condition is needed to show that a sequence of functions gamma-converges to some limiting function. It seems related to showing that the compactness condition that usually accompanies Gamma-convergence - does coerciveness imply compactness? I don't see how they are related, but they seem to be. Edit: The ""compactness condition"" I refer to is that any sequence of arguments $\{u_n\}_{n\in \mathbb{N}}$ has a convergent subsequence. I think that this compactness condition is implies equi-coerciveness of the sequence $\{u_n\}$ . I don't know why coerciveness is used in proofs of gamma-convergence instead of compactness directly. Anyways, I still want to know why the functions are called ""coercive"".","A function is called coercive if I came across this requirement in calculus of variations, where a coercivity condition is needed to show that a sequence of functions gamma-converges to some limiting function. It seems related to showing that the compactness condition that usually accompanies Gamma-convergence - does coerciveness imply compactness? I don't see how they are related, but they seem to be. Edit: The ""compactness condition"" I refer to is that any sequence of arguments has a convergent subsequence. I think that this compactness condition is implies equi-coerciveness of the sequence . I don't know why coerciveness is used in proofs of gamma-convergence instead of compactness directly. Anyways, I still want to know why the functions are called ""coercive"".",f: \mathbb{R}^n \rightarrow \mathbb{R}^n  \frac{f(x)\cdot x}{\|x\|} \rightarrow \infty \;\; \text{as} \;\;\|x\|\rightarrow \infty. \{u_n\}_{n\in \mathbb{N}} \{u_n\},"['functional-analysis', 'analysis', 'terminology', 'calculus-of-variations']"
77,"Does $W^{1,1}([a,b])$ compactly embed in $L^1([a,b])$?",Does  compactly embed in ?,"W^{1,1}([a,b]) L^1([a,b])","I'm trying to prove the Remark at page 274 of the second edition of the Book ""Partial Differential Equations"" by Evans Lawrence, the Remark uses the Theorem 1 at page 272 (Rellich-Kondrachov Compcactness Theorem), here I quote the theorem, the remark and the definition of compactly embedding Definition of compactly embedding Let $X$ be a Banach space with norm $||\,\cdot\,||_X$ , let $Y$ be a Banach space with norm $||\,\cdot\,||_Y$ . One says that $X$ compactly embeds into $Y$ , which is denoted by the simbol $X \subset\subset Y$ , if and only if $X \subset Y$ there exists $C > 0$ such that $||u||_Y \leq C||u||_X$ $\forall u \in X$ each bounded sequence in $X$ has a convergent subsequence in $Y$ (Rellich-Kondrachov Compactness Theorem) Assume $U$ is a bounded $C^1$ open set of $\mathbb{R}^n$ , suppose $1 \leq p < n$ , then $W^{1,p} \subset\subset L^q(U)$ for each $1 \leq q < p^*$ Remark. Observe that since $p^* > p$ and $p^* \to \infty$ as $p \to n$ , we have in particular $W^{1,p}(U) \subset\subset L^p(U)$ for all $1 \leq p \leq \infty$ Observe that if $n < p \leq \infty$ , this follows from Morrey's inequality and the Ascoli-Arzela compactness criterion) The problem is that the proof of the Remark doesn't work when $p = n  = 1$ , this is because you can't use Rellich Kondrachov Theorem because there is no $p$ such that $1 \leq p < n$ because $n = 1$ , I asked to my professor how to prove the Remark in this case and he told me that the proof of the Rellich-Kondrachov Compactness Theorem (the one on the Evans' Book) can be modified to prove the Remark in this case, but I don't get how to do it. Therefore my question is : Is it true that $W^{1,1}(U)$ compactly embeds in $L^1(U)$ when $U$ is an open $C^1$ bounded subset of $\mathbb{R}^1$ ?? How can I prove it or disprove it?? Clearly it would be enough to prove the statement in the case where $U = (a,b)$ , so you can assume it if you wish.","I'm trying to prove the Remark at page 274 of the second edition of the Book ""Partial Differential Equations"" by Evans Lawrence, the Remark uses the Theorem 1 at page 272 (Rellich-Kondrachov Compcactness Theorem), here I quote the theorem, the remark and the definition of compactly embedding Definition of compactly embedding Let be a Banach space with norm , let be a Banach space with norm . One says that compactly embeds into , which is denoted by the simbol , if and only if there exists such that each bounded sequence in has a convergent subsequence in (Rellich-Kondrachov Compactness Theorem) Assume is a bounded open set of , suppose , then for each Remark. Observe that since and as , we have in particular for all Observe that if , this follows from Morrey's inequality and the Ascoli-Arzela compactness criterion) The problem is that the proof of the Remark doesn't work when , this is because you can't use Rellich Kondrachov Theorem because there is no such that because , I asked to my professor how to prove the Remark in this case and he told me that the proof of the Rellich-Kondrachov Compactness Theorem (the one on the Evans' Book) can be modified to prove the Remark in this case, but I don't get how to do it. Therefore my question is : Is it true that compactly embeds in when is an open bounded subset of ?? How can I prove it or disprove it?? Clearly it would be enough to prove the statement in the case where , so you can assume it if you wish.","X ||\,\cdot\,||_X Y ||\,\cdot\,||_Y X Y X \subset\subset Y X \subset Y C > 0 ||u||_Y \leq C||u||_X \forall u \in X X Y U C^1 \mathbb{R}^n 1 \leq p < n W^{1,p} \subset\subset L^q(U) 1 \leq q < p^* p^* > p p^* \to \infty p \to n W^{1,p}(U) \subset\subset L^p(U) 1 \leq p \leq \infty n < p \leq \infty p = n  = 1 p 1 \leq p < n n = 1 W^{1,1}(U) L^1(U) U C^1 \mathbb{R}^1 U = (a,b)","['functional-analysis', 'measure-theory', 'partial-differential-equations', 'banach-spaces', 'sobolev-spaces']"
78,On convolution theorem and Fourier transform,On convolution theorem and Fourier transform,,"Wikipedia says: On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group $\mathrm S$ with the Lebesgue measure is an immediate example. For a fixed $g$ in $L^1({\mathrm{S}})$ , we have the following familiar operator acting on the Hilbert space $L^2(\mathrm{S})$ : $$ T{f}(x)=\frac {1}{2\pi }\int _{\mathrm {S} }{f}(y)g(x-y)\,dy $$ The operator $T$ is compact. Is there a way to prove this? Caveat . Sadly, I know nothing about group theory, but intuitively I suppose $L^2({\mathrm S})$ means if I have a function, say in $L^2([0, 2\pi])$ , the previous convolution makes sense only if I assume the function repeats itself periodically outside $[0, 2\pi]$ .","Wikipedia says: On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group with the Lebesgue measure is an immediate example. For a fixed in , we have the following familiar operator acting on the Hilbert space : The operator is compact. Is there a way to prove this? Caveat . Sadly, I know nothing about group theory, but intuitively I suppose means if I have a function, say in , the previous convolution makes sense only if I assume the function repeats itself periodically outside .","\mathrm S g L^1({\mathrm{S}}) L^2(\mathrm{S})  T{f}(x)=\frac {1}{2\pi }\int _{\mathrm {S} }{f}(y)g(x-y)\,dy  T L^2({\mathrm S}) L^2([0, 2\pi]) [0, 2\pi]","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'fourier-transform', 'convolution']"
79,"Existence of $L^1((0,1))$ functions which blow up on every open interval",Existence of  functions which blow up on every open interval,"L^1((0,1))","Consider an open interval $(0,1) \subset \mathbb{R}$ and the subset $$ \mathcal{F} := \{f \in L^1((0,1)): \|{f\vert_{(a,b)}}\|_{\infty} = \infty \, \forall \, 0 \leq a < b < 1\} \subset L((0,1), dx). $$ I want to show that $\mathcal{F}$ is non-empty in $(L((0,1), dx), \| \, \|_{L^1})$ . For that reason, I define the sets $$G_n := \{f \in L^1((0,1)): \|{f\vert_{(a,b)}}\|_{\infty} \leq n \, \text{ for some } 0 \leq a < b < 1\}$$ and aim to show two things: $G_n$ is closed for every $n \in \mathbb{N}$ $G_n$ has empty interior for $n \in \mathbb{N}$ Then, $F_n := L^1{((0,1))}\setminus G_n$ is open and dense for every $n \in \mathbb{N}$ and $$ \mathcal{F} = \bigcap_{n \geq 1} F_n$$ is non-empty by Baire's category theorem (actually, it will be comeager). Now, closedness of $(G_n; n \geq 1)$ is clear. To prove that $G_n$ have empty interior for every $n$ , I assume, for the sake of contradiction, that they have not. That is, let $n \in \mathbb{N}$ . Then, for every $g \in G_n$ , there exists $\varepsilon > 0$ s.t. $$ B := \{h \in L^{1}((0,1)): \|h - g \|_{L^1} < \varepsilon\} \subset G_n.$$ The idea is now to construct a function $h \in L^1((0,1))$ s.t. $h \in B$ and $h \notin G_n$ , i.e. $$ \|h-g\|_{L^1} < \varepsilon /2 \quad \text{ and} \quad \|h\vert_{(a,b)} \|_{\infty} > n \quad \forall \, 0 < a < b < 1.$$ Is this a feasible approach? If yes, can you hint how this may work?","Consider an open interval and the subset I want to show that is non-empty in . For that reason, I define the sets and aim to show two things: is closed for every has empty interior for Then, is open and dense for every and is non-empty by Baire's category theorem (actually, it will be comeager). Now, closedness of is clear. To prove that have empty interior for every , I assume, for the sake of contradiction, that they have not. That is, let . Then, for every , there exists s.t. The idea is now to construct a function s.t. and , i.e. Is this a feasible approach? If yes, can you hint how this may work?","(0,1) \subset \mathbb{R}  \mathcal{F} := \{f \in L^1((0,1)): \|{f\vert_{(a,b)}}\|_{\infty} = \infty \, \forall \, 0 \leq a < b < 1\} \subset L((0,1), dx).
 \mathcal{F} (L((0,1), dx), \| \, \|_{L^1}) G_n := \{f \in L^1((0,1)): \|{f\vert_{(a,b)}}\|_{\infty} \leq n \, \text{ for some } 0 \leq a < b < 1\} G_n n \in \mathbb{N} G_n n \in \mathbb{N} F_n := L^1{((0,1))}\setminus G_n n \in \mathbb{N}  \mathcal{F} = \bigcap_{n \geq 1} F_n (G_n; n \geq 1) G_n n n \in \mathbb{N} g \in G_n \varepsilon > 0  B := \{h \in L^{1}((0,1)): \|h - g \|_{L^1} < \varepsilon\} \subset G_n. h \in L^1((0,1)) h \in B h \notin G_n 
\|h-g\|_{L^1} < \varepsilon /2 \quad \text{ and} \quad \|h\vert_{(a,b)} \|_{\infty} > n \quad \forall \, 0 < a < b < 1.","['functional-analysis', 'baire-category']"
80,Does a strictly convex and continuous function always exist?,Does a strictly convex and continuous function always exist?,,"Let $X$ be a locally convex topological vector space, and let $C$ be a nonempty convex subset of $X$ . A real-valued function $f: C \to \mathbb R$ is strictly convex if for all $\lambda \in (0,1)$ and distinct $x,y \in C$ $$f(\lambda x + (1 - \lambda)y) < \lambda f(x) + (1-\lambda) f(y).$$ Does a strictly convex, continuous, real-valued function exist on every nonempty convex $C$ ?","Let be a locally convex topological vector space, and let be a nonempty convex subset of . A real-valued function is strictly convex if for all and distinct Does a strictly convex, continuous, real-valued function exist on every nonempty convex ?","X C X f: C \to \mathbb R \lambda \in (0,1) x,y \in C f(\lambda x + (1 - \lambda)y) < \lambda f(x) + (1-\lambda) f(y). C","['functional-analysis', 'convex-analysis', 'topological-vector-spaces', 'locally-convex-spaces']"
81,"A monotone convergence theorem for nets in Reed and Simon, Vol. I","A monotone convergence theorem for nets in Reed and Simon, Vol. I",,"Reed and Simon state the following monotone convergence theorem for nets (Theorem IV.15, Vol. I). Let $\mu$ be a regular Borel measure on a compact Hausdorff space $X$ . Let $\{f_\alpha\}_{\alpha\in I}$ be an increasing net of continuous functions. Then, $f=\lim_\alpha f_\alpha \in L^1(X,d\mu)$ if and only if $\sup_\alpha \| f_\alpha \|_1 < \infty$ and in that case $\lim_\alpha \|f-f_\alpha\|_1 = 0 $ . Unfortunately, they do not give a proof. Can anyone provide one, or give a reference to one? In the notes to the chapter containing the theorem, Reed and Simon cite Bourbaki's Integration, but I've found that text difficult to navigate. I do not have access to the other text they cite (Nachbin). This is not quite your classical monotone convergence theorem , as it requires each $f_\alpha$ to be continuous. This is necessary in order to rule out common counter-examples (see, for instance, this question or these answers ) to the statement of the dominated convergence theorem for nets instead of sequences. I could try adapting a usual proof for the MCT. However, I don't quite see the role that continuity of the $f_\alpha$ 's play in the result, so I'm not sure how to make use of it. Also, I'm not actually certain that the hypotheses of the theorem rule out the example here . I suspect the monotonicity requirement would get in the way of choosing the net of functions required in the example, but I don't have a proof of that either. Reference Reed, Michael; Simon, Barry , Methods of modern mathematical physics. I: Functional analysis. Rev. and enl. ed, New York etc.: Academic Press, A Subsidiary of Harcourt Brace Jovanovich, Publishers, XV, 400 p. $ 24.00 (1980). ZBL0459.46001 .","Reed and Simon state the following monotone convergence theorem for nets (Theorem IV.15, Vol. I). Let be a regular Borel measure on a compact Hausdorff space . Let be an increasing net of continuous functions. Then, if and only if and in that case . Unfortunately, they do not give a proof. Can anyone provide one, or give a reference to one? In the notes to the chapter containing the theorem, Reed and Simon cite Bourbaki's Integration, but I've found that text difficult to navigate. I do not have access to the other text they cite (Nachbin). This is not quite your classical monotone convergence theorem , as it requires each to be continuous. This is necessary in order to rule out common counter-examples (see, for instance, this question or these answers ) to the statement of the dominated convergence theorem for nets instead of sequences. I could try adapting a usual proof for the MCT. However, I don't quite see the role that continuity of the 's play in the result, so I'm not sure how to make use of it. Also, I'm not actually certain that the hypotheses of the theorem rule out the example here . I suspect the monotonicity requirement would get in the way of choosing the net of functions required in the example, but I don't have a proof of that either. Reference Reed, Michael; Simon, Barry , Methods of modern mathematical physics. I: Functional analysis. Rev. and enl. ed, New York etc.: Academic Press, A Subsidiary of Harcourt Brace Jovanovich, Publishers, XV, 400 p. $ 24.00 (1980). ZBL0459.46001 .","\mu X \{f_\alpha\}_{\alpha\in I} f=\lim_\alpha f_\alpha \in L^1(X,d\mu) \sup_\alpha \| f_\alpha \|_1 < \infty \lim_\alpha \|f-f_\alpha\|_1 = 0  f_\alpha f_\alpha","['functional-analysis', 'measure-theory', 'reference-request']"
82,Is there a deep reason why strong estimates fail to exist for $L^1$ so often?,Is there a deep reason why strong estimates fail to exist for  so often?,L^1,"For example, the Hardy-Littlewood maximal inequality fails to have a strong type estimate in the $(1,1)$ case. Another example is the Hilbert transform which has strong type $(p,p)$ bounds for $1<p<\infty$ but fails to be bounded on $L^1$ . Is there a deep reason why such estimates fail so often for $L^1$ ?","For example, the Hardy-Littlewood maximal inequality fails to have a strong type estimate in the case. Another example is the Hilbert transform which has strong type bounds for but fails to be bounded on . Is there a deep reason why such estimates fail so often for ?","(1,1) (p,p) 1<p<\infty L^1 L^1","['functional-analysis', 'lp-spaces']"
83,An Orthogonality Problem of Eigenfunctions of homogeneous Fredholm equation,An Orthogonality Problem of Eigenfunctions of homogeneous Fredholm equation,,"Suppose we have a integral equation $$\int_{-1}^1 \frac{\text{sin }c(x-y)}{\pi (x-y)}\psi(y)dy=\lambda \psi(x),\quad|x|\le1.$$ By the Fredholm equation theory, we know that this equation has solutions in $L^2(-1,1)$ only for a discrete set of real positive values of $\lambda$ , say $\lambda_0\ge\lambda_1\ge\lambda_2\ge...,$ and as $n\to \infty$ , lim $\lambda_n=0.$ The corresponding solutions, or eigenfunctions, $\psi_0(x),\psi_1(x),\psi_2(x),...$ can be chosen to be real and orthogonal on $(-1,1).$ The author extended the range of definition of the $\psi$ 's in this way: $$(*)\qquad\qquad\qquad\qquad \psi_n(x)=\frac{1}{\lambda_n}\int_{-1}^1 \frac{\text{sin }c(x-y)}{\pi (x-y)}\psi_n(y)dy,\quad|x|\gt1.$$ And he said that, by a simple calculation, we can show that these eigenfunctions are orthogonal on $(-\infty,\infty)$ as well as on $(-1,1)$ as already noted. We normalize them to have unit energy, so that $$\int_{-\infty}^{\infty} \psi_{n}(x) \psi_{m}(x) d x=\delta_{m m}$$ and it then follows that $$\int_{-1}^{1} \psi_{n}(x) \psi_{m}(x) d x=\lambda_{n} \delta_{m n}.$$ My question is that, how to show the orthogonality on $(-\infty,\infty)\,?$ And how to derive the last equality from $\int_{-\infty}^{\infty} \psi_{n}(x) \psi_{m}(x) d x=\delta_{m m}\,?$ I tried to calculate the inner product of $\psi_n$ and $\psi_m$ , and then plug in $(*)$ , and used Fubini's theorem. But the triple integral is very messy and I can't really calculate it. And for the derivation problem, I have no idea. Any help will be appreciated.","Suppose we have a integral equation By the Fredholm equation theory, we know that this equation has solutions in only for a discrete set of real positive values of , say and as , lim The corresponding solutions, or eigenfunctions, can be chosen to be real and orthogonal on The author extended the range of definition of the 's in this way: And he said that, by a simple calculation, we can show that these eigenfunctions are orthogonal on as well as on as already noted. We normalize them to have unit energy, so that and it then follows that My question is that, how to show the orthogonality on And how to derive the last equality from I tried to calculate the inner product of and , and then plug in , and used Fubini's theorem. But the triple integral is very messy and I can't really calculate it. And for the derivation problem, I have no idea. Any help will be appreciated.","\int_{-1}^1 \frac{\text{sin }c(x-y)}{\pi (x-y)}\psi(y)dy=\lambda \psi(x),\quad|x|\le1. L^2(-1,1) \lambda \lambda_0\ge\lambda_1\ge\lambda_2\ge..., n\to \infty \lambda_n=0. \psi_0(x),\psi_1(x),\psi_2(x),... (-1,1). \psi (*)\qquad\qquad\qquad\qquad \psi_n(x)=\frac{1}{\lambda_n}\int_{-1}^1 \frac{\text{sin }c(x-y)}{\pi (x-y)}\psi_n(y)dy,\quad|x|\gt1. (-\infty,\infty) (-1,1) \int_{-\infty}^{\infty} \psi_{n}(x) \psi_{m}(x) d x=\delta_{m m} \int_{-1}^{1} \psi_{n}(x) \psi_{m}(x) d x=\lambda_{n} \delta_{m n}. (-\infty,\infty)\,? \int_{-\infty}^{\infty} \psi_{n}(x) \psi_{m}(x) d x=\delta_{m m}\,? \psi_n \psi_m (*)","['functional-analysis', 'orthogonality', 'eigenfunctions', 'integral-operators']"
84,Show that the carré du champ operator is nonnegative,Show that the carré du champ operator is nonnegative,,"Let $(E,\mathcal E)$ be a measurable space $\mathcal M_b(E,\mathcal E):=\left\{f:E\to\mathbb R\mid f\text{ is bounded and }\mathcal E\text{-measurable}\right\}$ $(\kappa_t)_{t\ge0}$ be a Markov semigroup on $(E,\mathcal E)$ and $$\kappa_tf:=\int\kappa_t(\;\cdot\;,{\rm d}y)f(y)\tag1$$ for $f\in\mathcal M_b(E,\mathcal E)$ and $t\ge0$ $\mu$ be a probability measure on $(E,\mathcal E)$ subinvariant with respect to $(\kappa_t)_{t\ge0}$ It's easy to see that $(\kappa_t)_{t\ge0}$ is a contraction semigroup on $\left(\mathcal M_b(E,\mathcal E),\left\|\;\cdot\;\right\|_{L^2(\mu)}\right)$ and hence has a unique extension to a contraction semigroup on $L^2(\mu)$ . Let $(\mathcal D(A),A)$ denote the generator of that semigroup. Let $f\in\mathcal D(A)$ such that $f^2\in\mathcal D(A)$ . I want to show that $$Af^2\ge 2fAf.\tag2$$ The crucial point might be the following: If $g:E\to\mathbb R$ is $\mathcal E$ -measurable and $(\kappa_t|g|)(x)<\infty$ for all $x\in E$ , then $$\varphi\left(\left(\kappa_tg\right)(x)\right)\le\left(\kappa_t\left(\varphi(g)\right)\right)(x)\;\;\;\text{for all }x\in E\tag3$$ for all convex $\varphi:\mathbb R\to\mathbb R$ by Jensen's inequality (Clearly, for the question we would take $\varphi(x)=x^2$ ). However, it's not clear to me how (and if at all) $(3)$ extends to $g\in L^2(\mu)$ . $^1$ Clearly, we know that there is a $(g_n)_{n\in\mathbb N}\subseteq\mathcal M_b(E,\mathcal E)$ with $$|g_n|\le|g|\;\;\;\text{for all }n\in\mathbb N\tag4$$ and $$g_n\xrightarrow{n\to\infty}g\tag5.$$ By the dominated convergence theorem (and construction of $(\kappa_t)_{t\ge0}$ ), $$\left\|\kappa_tg_n-\kappa_tg\right\|_{L^2(\mu)}\le\left\|g_n-g\right\|_{L^2(\mu)}\xrightarrow{n\to\infty}0\tag6\;\;\;\text{for all }t\ge0.$$ $(3)$ holds for $g=g_n$ . Moreover, we could extract a subsequence $\left(g_{n_k}\right)_{k\in\mathbb N}$ with $$g_{n_k}\xrightarrow{k\to\infty}g\;\;\;\mu\text{-almost surely}\tag7.$$ But that doesn't mean (does it?) $^2$ that $$\kappa_tg_{n_k}\xrightarrow{k\to\infty}\kappa_tg\;\;\;\mu\text{-almost surely for all }t\ge0\tag8.$$ So, I'm stuck at this point. $^1$ One may note that, by subinvariance, $(\kappa_t|g|)(x)<\infty$ for $\mu$ -almost all $x\in E$ , but I hope that $(3)$ can be proved by a general extension argument. $^2$ Maybe we can argue that $$\left|\kappa_tg_{n_k}-\kappa_tg_{n_l}\right|\le\kappa_t\left|g_{n_k}-g_{n_l}\right|\xrightarrow{k,\:l\to\infty}0\tag9$$ (pointwise) by the dominated convergence theorem and hence $\left(\left(\kappa_tg_{n_k}\right)(x)\right)_{k\in\mathbb N}$ is Cauchy for all $x\in E$ .","Let be a measurable space be a Markov semigroup on and for and be a probability measure on subinvariant with respect to It's easy to see that is a contraction semigroup on and hence has a unique extension to a contraction semigroup on . Let denote the generator of that semigroup. Let such that . I want to show that The crucial point might be the following: If is -measurable and for all , then for all convex by Jensen's inequality (Clearly, for the question we would take ). However, it's not clear to me how (and if at all) extends to . Clearly, we know that there is a with and By the dominated convergence theorem (and construction of ), holds for . Moreover, we could extract a subsequence with But that doesn't mean (does it?) that So, I'm stuck at this point. One may note that, by subinvariance, for -almost all , but I hope that can be proved by a general extension argument. Maybe we can argue that (pointwise) by the dominated convergence theorem and hence is Cauchy for all .","(E,\mathcal E) \mathcal M_b(E,\mathcal E):=\left\{f:E\to\mathbb R\mid f\text{ is bounded and }\mathcal E\text{-measurable}\right\} (\kappa_t)_{t\ge0} (E,\mathcal E) \kappa_tf:=\int\kappa_t(\;\cdot\;,{\rm d}y)f(y)\tag1 f\in\mathcal M_b(E,\mathcal E) t\ge0 \mu (E,\mathcal E) (\kappa_t)_{t\ge0} (\kappa_t)_{t\ge0} \left(\mathcal M_b(E,\mathcal E),\left\|\;\cdot\;\right\|_{L^2(\mu)}\right) L^2(\mu) (\mathcal D(A),A) f\in\mathcal D(A) f^2\in\mathcal D(A) Af^2\ge 2fAf.\tag2 g:E\to\mathbb R \mathcal E (\kappa_t|g|)(x)<\infty x\in E \varphi\left(\left(\kappa_tg\right)(x)\right)\le\left(\kappa_t\left(\varphi(g)\right)\right)(x)\;\;\;\text{for all }x\in E\tag3 \varphi:\mathbb R\to\mathbb R \varphi(x)=x^2 (3) g\in L^2(\mu) ^1 (g_n)_{n\in\mathbb N}\subseteq\mathcal M_b(E,\mathcal E) |g_n|\le|g|\;\;\;\text{for all }n\in\mathbb N\tag4 g_n\xrightarrow{n\to\infty}g\tag5. (\kappa_t)_{t\ge0} \left\|\kappa_tg_n-\kappa_tg\right\|_{L^2(\mu)}\le\left\|g_n-g\right\|_{L^2(\mu)}\xrightarrow{n\to\infty}0\tag6\;\;\;\text{for all }t\ge0. (3) g=g_n \left(g_{n_k}\right)_{k\in\mathbb N} g_{n_k}\xrightarrow{k\to\infty}g\;\;\;\mu\text{-almost surely}\tag7. ^2 \kappa_tg_{n_k}\xrightarrow{k\to\infty}\kappa_tg\;\;\;\mu\text{-almost surely for all }t\ge0\tag8. ^1 (\kappa_t|g|)(x)<\infty \mu x\in E (3) ^2 \left|\kappa_tg_{n_k}-\kappa_tg_{n_l}\right|\le\kappa_t\left|g_{n_k}-g_{n_l}\right|\xrightarrow{k,\:l\to\infty}0\tag9 \left(\left(\kappa_tg_{n_k}\right)(x)\right)_{k\in\mathbb N} x\in E","['functional-analysis', 'probability-theory', 'markov-process', 'stochastic-analysis', 'semigroup-of-operators']"
85,Why is there no matrix representation of a linear operator,Why is there no matrix representation of a linear operator,,"If a Hilbert space, $H$ , has orthonormal basis $\{e_i\}$ , then a linear operator $A:H \to H$ can only be defined by a matrix (i.e., $\{a_{i,j} : \langle Ae_i,e_j \rangle = a_{i,j}\}$ if the operator is bounded.  I can't fully understand why we need this condition. I don't need an explicit counter-example, since it is seems to be hard to construct unbounded linear operators , but I would like an explanation of what exactly it means for a linear operator to be unbounded. Thanks! Edit: sorry, I worded my question badly. when I said 'I would like an explanation of what exactly it means for a linear operator to be unbounded' I meant in the context of this matrix representation. I.e., if we have a discontinuous linear operator on an infinite dimensional Hilbert space, why does this matrix representation fail? To help you understand my confusion better, given this matrix representation, it's easy to see that for any $x = \sum_i x_i e_i \in H$ , $Tx = \sum_k (\sum_i x_i a_{i,j}) e_k$ . I believe this is ill-defined when $T$ is either unbounded (or discontinuous), but why is this? Is it because these infinite sums might be conditionally convergent? I can't construct any nice examples, thanks in advance!","If a Hilbert space, , has orthonormal basis , then a linear operator can only be defined by a matrix (i.e., if the operator is bounded.  I can't fully understand why we need this condition. I don't need an explicit counter-example, since it is seems to be hard to construct unbounded linear operators , but I would like an explanation of what exactly it means for a linear operator to be unbounded. Thanks! Edit: sorry, I worded my question badly. when I said 'I would like an explanation of what exactly it means for a linear operator to be unbounded' I meant in the context of this matrix representation. I.e., if we have a discontinuous linear operator on an infinite dimensional Hilbert space, why does this matrix representation fail? To help you understand my confusion better, given this matrix representation, it's easy to see that for any , . I believe this is ill-defined when is either unbounded (or discontinuous), but why is this? Is it because these infinite sums might be conditionally convergent? I can't construct any nice examples, thanks in advance!","H \{e_i\} A:H \to H \{a_{i,j} : \langle Ae_i,e_j \rangle = a_{i,j}\} x = \sum_i x_i e_i \in H Tx = \sum_k (\sum_i x_i a_{i,j}) e_k T","['functional-analysis', 'linear-transformations', 'hilbert-spaces']"
86,Random elements and random processes,Random elements and random processes,,"Suppose that $X$ is a random element with values in $L^2([0,1],\mathbb C)$ such that $\operatorname E\|X\|<\infty$. The expected value of $X$ is equal to $\mu(t)=\operatorname EX(t)$ for almost all $t\in[0,1]$. So we have a random element, but we actually want to have a random process, i.e. a collection of random variables, to evaluate the mean.  How can we define the random process $\{X(t):t\in[0,1]\}$? Strictly speaking, the space $L^2([0,1],\mathbb C)$ consists of equivalence classes of almost everywhere equal square integrable complex functions. So it does not make sense to consider the value of $f\in L^2([0,1],\mathbb C)$ at some point $t\in[0,1]$ since this value is not defined. However, if we want to define the random variable $X(t)$ for some $t\in[0,1]$ and evaluate its expected value $\operatorname EX(t)$, we need to assign a complex number $X(t,\omega)$ for each $\omega\in\Omega$ (and make sure that this map is measurable). So it seems that it is not even possible to define the random process $\{X(t):t\in[0,1]\}$ because we do not have information about the values of this function at point $t\in[0,1]$. However, I have seen a few times when the expected value of a random element with values in $L^2([0,1],\mathbb C)$ is evaluated pointwise, i.e. $\operatorname EX(t)$ for $t\in[0,1]$, so maybe it is possible to define the random process $\{X(t):t\in[0,1]\}$ in a sensible way. Any help is much appreciated!","Suppose that $X$ is a random element with values in $L^2([0,1],\mathbb C)$ such that $\operatorname E\|X\|<\infty$. The expected value of $X$ is equal to $\mu(t)=\operatorname EX(t)$ for almost all $t\in[0,1]$. So we have a random element, but we actually want to have a random process, i.e. a collection of random variables, to evaluate the mean.  How can we define the random process $\{X(t):t\in[0,1]\}$? Strictly speaking, the space $L^2([0,1],\mathbb C)$ consists of equivalence classes of almost everywhere equal square integrable complex functions. So it does not make sense to consider the value of $f\in L^2([0,1],\mathbb C)$ at some point $t\in[0,1]$ since this value is not defined. However, if we want to define the random variable $X(t)$ for some $t\in[0,1]$ and evaluate its expected value $\operatorname EX(t)$, we need to assign a complex number $X(t,\omega)$ for each $\omega\in\Omega$ (and make sure that this map is measurable). So it seems that it is not even possible to define the random process $\{X(t):t\in[0,1]\}$ because we do not have information about the values of this function at point $t\in[0,1]$. However, I have seen a few times when the expected value of a random element with values in $L^2([0,1],\mathbb C)$ is evaluated pointwise, i.e. $\operatorname EX(t)$ for $t\in[0,1]$, so maybe it is possible to define the random process $\{X(t):t\in[0,1]\}$ in a sensible way. Any help is much appreciated!",,"['functional-analysis', 'probability-theory', 'measure-theory', 'hilbert-spaces', 'expectation']"
87,Are those two definitions of orthogonal projection equivalent in a general Hilbert space?,Are those two definitions of orthogonal projection equivalent in a general Hilbert space?,,"I am taking a graduate level course in probability and we started off with some results in functional analysis. One thing that I feel I do not understand properly is the definition of an orthogonal projection in the context of Hilbert spaces. And I was not able to find (sufficiently) exhaustive discussion of the definition (for example not here ). To this end, let a Hilbert space, $H$, possibly infinite dimensional and not necessarily separable. Let a closed convex subset $S \subseteq H$. 1) One the one hand orthogonal projection $P_S :H \to S$ is defined as a mapping of every element $x \in H$ to the unique best approximation of $x$ in $s_0 \in S$. That is $s_0 \in S$ such that $$\|s_0 -x\| = \inf\{s \in S| \|x- s\|\}\,. $$ (Existence of such $P_s$ eventually implies that $H = S\oplus S^{c}$) 2) On the other hand, at various places, I have seen the definition of orthogonal projection to be an operator $P:H \to H$ such that: $P^2 =P$ and $P^* = P$. $\text{  }$ Here are two things I am trying to figure out: A. Are those two definitions identical in an (infinite) inseparable case? To go from 1) to 2) seems to be quite straightforward. However I could not figure out if definition 2) implies 1) in an (infinite) inseparable case. B. The other thing, if the set $S$ is not closed under addition the operator $P$ might not be linear, so is orthogonal projection required to be linear? I would appreciate any help.","I am taking a graduate level course in probability and we started off with some results in functional analysis. One thing that I feel I do not understand properly is the definition of an orthogonal projection in the context of Hilbert spaces. And I was not able to find (sufficiently) exhaustive discussion of the definition (for example not here ). To this end, let a Hilbert space, $H$, possibly infinite dimensional and not necessarily separable. Let a closed convex subset $S \subseteq H$. 1) One the one hand orthogonal projection $P_S :H \to S$ is defined as a mapping of every element $x \in H$ to the unique best approximation of $x$ in $s_0 \in S$. That is $s_0 \in S$ such that $$\|s_0 -x\| = \inf\{s \in S| \|x- s\|\}\,. $$ (Existence of such $P_s$ eventually implies that $H = S\oplus S^{c}$) 2) On the other hand, at various places, I have seen the definition of orthogonal projection to be an operator $P:H \to H$ such that: $P^2 =P$ and $P^* = P$. $\text{  }$ Here are two things I am trying to figure out: A. Are those two definitions identical in an (infinite) inseparable case? To go from 1) to 2) seems to be quite straightforward. However I could not figure out if definition 2) implies 1) in an (infinite) inseparable case. B. The other thing, if the set $S$ is not closed under addition the operator $P$ might not be linear, so is orthogonal projection required to be linear? I would appreciate any help.",,"['functional-analysis', 'hilbert-spaces', 'orthogonality']"
88,Where does Gelfand Theory fail for non-commutative algebras.,Where does Gelfand Theory fail for non-commutative algebras.,,"I'm trying to get my head around Gelfand theory, and I can't seem to find the subtleties between commutative and non-commutative algebras. Why is there not a one-to-one correspondence between maximal ideals of a non-commutative algebra and the character homomorphisms from the algebra to the complex plane? Doesn't the Gelfand Mazur theorem apply to these algebras to, so if $\mathfrak{a}$ is maximal, the map $$ A \to A/\mathfrak{a} \cong \mathbf{C} $$ is a homomorphism with kernel $\mathfrak{a}$. Conversely, if $\phi: A \to \mathbf{C}$ is a homomorphism, then $\phi$ is surjective, so if $\mathfrak{a} = \ker(\phi)$, $\tilde{\phi}: A/\mathfrak{a} \to \mathbf{C}$ is an isomorphism, hence $A/\mathfrak{a}$ is a field, so $\mathfrak{a}$ is maximal. What's going wrong here?","I'm trying to get my head around Gelfand theory, and I can't seem to find the subtleties between commutative and non-commutative algebras. Why is there not a one-to-one correspondence between maximal ideals of a non-commutative algebra and the character homomorphisms from the algebra to the complex plane? Doesn't the Gelfand Mazur theorem apply to these algebras to, so if $\mathfrak{a}$ is maximal, the map $$ A \to A/\mathfrak{a} \cong \mathbf{C} $$ is a homomorphism with kernel $\mathfrak{a}$. Conversely, if $\phi: A \to \mathbf{C}$ is a homomorphism, then $\phi$ is surjective, so if $\mathfrak{a} = \ker(\phi)$, $\tilde{\phi}: A/\mathfrak{a} \to \mathbf{C}$ is an isomorphism, hence $A/\mathfrak{a}$ is a field, so $\mathfrak{a}$ is maximal. What's going wrong here?",,"['functional-analysis', 'ring-theory', 'operator-algebras', 'algebras', 'gelfand-representation']"
89,Can $xy$ and $yx$ lie in different connected components of the group of invertible elements of an algebra?,Can  and  lie in different connected components of the group of invertible elements of an algebra?,xy yx,"What is  an example of  a  Banach or $C^{*}$ algebra $A$ which has two invertible elements $x, y$ such that $xy$ can not be connected to $yx$ in $G(A)$, the space of invertible elements of $A$. A possible (weak) motivation for this question is that $K_{1}(A)$  is  an Abelian group. Another motivation: Put  $F_{2}=$Free group on 2 generators $x,y$. then in the reduced $C^{*}-$algebra $C_{r}^{*} F_{2}$, $xyx^{-1}y^{-1}$ lies in the same connected component as of the identity.","What is  an example of  a  Banach or $C^{*}$ algebra $A$ which has two invertible elements $x, y$ such that $xy$ can not be connected to $yx$ in $G(A)$, the space of invertible elements of $A$. A possible (weak) motivation for this question is that $K_{1}(A)$  is  an Abelian group. Another motivation: Put  $F_{2}=$Free group on 2 generators $x,y$. then in the reduced $C^{*}-$algebra $C_{r}^{*} F_{2}$, $xyx^{-1}y^{-1}$ lies in the same connected component as of the identity.",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras', 'k-theory']"
90,Weak Convergence in $\ell^p$,Weak Convergence in,\ell^p,"First, my definition of weak convergence in $X$ is that $x_n \rightharpoonup  x$ if $\phi(x_n) \to \phi(x)$ for all $\phi \in X^*$ . I recently read the statement that $e_n \rightharpoonup 0$ in $\ell^p$ , $p>1$ , where $e_n$ is the canonical basis vector. In $\ell^2$ , this is clear to me, since the weak convergence $x_n \rightharpoonup x$ in $\ell^2$ (which is Hilbert) is equivalent to $\langle x_n, y \rangle \to \langle x, y \rangle$ for all $y \in \ell^2$ . But when $p \neq 2$ , I struggle to prove this since I don't know the form of a general functional $\phi \in X^*$ . Can you help me understand this?","First, my definition of weak convergence in is that if for all . I recently read the statement that in , , where is the canonical basis vector. In , this is clear to me, since the weak convergence in (which is Hilbert) is equivalent to for all . But when , I struggle to prove this since I don't know the form of a general functional . Can you help me understand this?","X x_n \rightharpoonup  x \phi(x_n) \to \phi(x) \phi \in X^* e_n \rightharpoonup 0 \ell^p p>1 e_n \ell^2 x_n \rightharpoonup x \ell^2 \langle x_n, y \rangle \to \langle x, y \rangle y \in \ell^2 p \neq 2 \phi \in X^*","['functional-analysis', 'convergence-divergence', 'lp-spaces', 'weak-convergence']"
91,Doubt in the proof of Banach Alaoglu Theorem,Doubt in the proof of Banach Alaoglu Theorem,,"I'm reading a proof of Banach Alaoglu Theorem from Functional Analysis by S Keshavan on page 142: Theorem: Let $V$ be a Banach space. Then $B^{*}$, the closed unit ball in $V^*$ is weak$^*$-compact. Proof: For each $x$ in $V$, we let $U_{x}$ be the closed ball of radius $||x||_{b}$ in $\mathbb{C}$. Then, we can take $U = \Pi_{x\in V} U_{x}$,which is clearly compact by Tychonoff theorem. The map $$ \tau : f \mapsto (f(x))_{x\in V} $$ defines a continuous injection from the unit ball of $B^*$ to $U$. Let $A$ denote the image of $\tau$, so it suffices to prove that $A$ is closed in $U$. Let $(f_x)_{x\in V} \in cl(\tau(B^*))$. Define for $x\in V$, $f(x)=f_x$. The proof will be complete if we can show that $f$ is linear. let $\epsilon >0$ be given. Then given $x$ and $y$ in $V$, we can find $g \in B^{*}$ such that $ \vert g(x)-f(x) \vert < \epsilon/3$,$ \vert g(y)-f(y) \vert < \epsilon/3$,$ \vert g(x+y)-f(x+y) \vert < \epsilon/3$ Can someone explain me how do we get $g$ mentioned in last paragraph?","I'm reading a proof of Banach Alaoglu Theorem from Functional Analysis by S Keshavan on page 142: Theorem: Let $V$ be a Banach space. Then $B^{*}$, the closed unit ball in $V^*$ is weak$^*$-compact. Proof: For each $x$ in $V$, we let $U_{x}$ be the closed ball of radius $||x||_{b}$ in $\mathbb{C}$. Then, we can take $U = \Pi_{x\in V} U_{x}$,which is clearly compact by Tychonoff theorem. The map $$ \tau : f \mapsto (f(x))_{x\in V} $$ defines a continuous injection from the unit ball of $B^*$ to $U$. Let $A$ denote the image of $\tau$, so it suffices to prove that $A$ is closed in $U$. Let $(f_x)_{x\in V} \in cl(\tau(B^*))$. Define for $x\in V$, $f(x)=f_x$. The proof will be complete if we can show that $f$ is linear. let $\epsilon >0$ be given. Then given $x$ and $y$ in $V$, we can find $g \in B^{*}$ such that $ \vert g(x)-f(x) \vert < \epsilon/3$,$ \vert g(y)-f(y) \vert < \epsilon/3$,$ \vert g(x+y)-f(x+y) \vert < \epsilon/3$ Can someone explain me how do we get $g$ mentioned in last paragraph?",,"['functional-analysis', 'banach-spaces']"
92,A question about convex open set in a topological vector space.,A question about convex open set in a topological vector space.,,"Supose $E$ is a topological vector space(may not be Hausdorff). $U\subset E$ is an open set such that $U+U=2U$. How to show $U$ is convex? I can see if $E$ is $T_1$，then $E$ should be Hausdorff. and every  line segment in $U$ is compact, then closed. and more,then I can show the line segment is in $U$. What about $E$ not $T_1$?","Supose $E$ is a topological vector space(may not be Hausdorff). $U\subset E$ is an open set such that $U+U=2U$. How to show $U$ is convex? I can see if $E$ is $T_1$，then $E$ should be Hausdorff. and every  line segment in $U$ is compact, then closed. and more,then I can show the line segment is in $U$. What about $E$ not $T_1$?",,"['functional-analysis', 'convex-analysis', 'topological-vector-spaces']"
93,How to obtain the inequality $\int |f|^2\log(|f|) \leq \frac{n}{4}\log\lVert f \rVert^2_{L^p} $ from Jensen's inequality?,How to obtain the inequality  from Jensen's inequality?,\int |f|^2\log(|f|) \leq \frac{n}{4}\log\lVert f \rVert^2_{L^p} ,Let $f$ be a positive function with $\lVert f \rVert_{L^2}=1$. Let $p= 2n/(n-2)$. How to obtain $\int |f|^2\log(|f|)  \leq \frac{n}{4}\log\lVert f \rVert^2_{L^p}$ from Jensen's inequality? Here all norms and integrals are over a compact manifold $M$ of dimension $n$. For context: this is a part of a proof of logarithmic Sobolev inequality . The log-Sobolev inequality follows by estimating the right hand side by $C\int |\nabla f|^2$.,Let $f$ be a positive function with $\lVert f \rVert_{L^2}=1$. Let $p= 2n/(n-2)$. How to obtain $\int |f|^2\log(|f|)  \leq \frac{n}{4}\log\lVert f \rVert^2_{L^p}$ from Jensen's inequality? Here all norms and integrals are over a compact manifold $M$ of dimension $n$. For context: this is a part of a proof of logarithmic Sobolev inequality . The log-Sobolev inequality follows by estimating the right hand side by $C\int |\nabla f|^2$.,,"['functional-analysis', 'lp-spaces', 'integral-inequality']"
94,Operator norm of the inverse,Operator norm of the inverse,,"If I made no mistake, one can calculate the operator norm of the inverse of any given (invertible) operator $A: V\rightarrow V$ via: \begin{align}\|A^{-1}\| & = \sup\left\{\frac{\|A^{-1}b\|}{\|b\|} : b\neq 0\right\} \\ & \qquad \left\downarrow A\text{ is a bijection }V\setminus\{0\}\rightarrow V\setminus\{0\}\right. \\ & = \sup\left\{\frac{\|A^{-1}Ab\|}{\|Ab\|} : b\neq 0\right\} \\ & = \sup\left\{\frac{\|b\|}{\|Ab\|} : b\neq 0\right\} \\ & = \frac{1}{\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\}}\end{align} Is there a name for the expression $\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\}$ which is similar to the operator norm but with replacing $\sup$ with $\inf$? Note: Above I assumed $\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\} > 0$ which is always the case if $V$ is finite dimensional.","If I made no mistake, one can calculate the operator norm of the inverse of any given (invertible) operator $A: V\rightarrow V$ via: \begin{align}\|A^{-1}\| & = \sup\left\{\frac{\|A^{-1}b\|}{\|b\|} : b\neq 0\right\} \\ & \qquad \left\downarrow A\text{ is a bijection }V\setminus\{0\}\rightarrow V\setminus\{0\}\right. \\ & = \sup\left\{\frac{\|A^{-1}Ab\|}{\|Ab\|} : b\neq 0\right\} \\ & = \sup\left\{\frac{\|b\|}{\|Ab\|} : b\neq 0\right\} \\ & = \frac{1}{\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\}}\end{align} Is there a name for the expression $\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\}$ which is similar to the operator norm but with replacing $\sup$ with $\inf$? Note: Above I assumed $\inf\left\{\frac{\|Ab\|}{\|b\|} : b\neq 0\right\} > 0$ which is always the case if $V$ is finite dimensional.",,"['functional-analysis', 'normed-spaces', 'inverse']"
95,Zeros/poles at Laplace and at Fourier Transform,Zeros/poles at Laplace and at Fourier Transform,,"I recently started ""relearning"" the Laplace transform, and I noticed something. It seems to me that the intuitive idea of poles and zeros is different between these two transforms! For example, in case of Fourier transform, we can write something like this for the input signal of the system: let the input be a complex sinusoid $e^{j \omega_0 t}$, then $$y(t)=\int_{-\infty}^{+\infty}d\tau \, h(\tau)x(t-\tau)=e^{j \omega_0 t} \cdot \int_{-\infty}^{+\infty}d\tau \, h(\tau)e^{-j \omega_0 \tau}=e^{j \omega_0 t} \cdot H(\omega_0),$$ where $H(\omega_0)$ is the Fourier transform of the impulse response of the system $h(t)$ on a frequency $\omega_0$. The same thing can be seen in frequency domain, where Dirac distribution $2 \pi \delta(\omega-\omega_0)$ essentially ""probes"" the transfer function $H(\omega)$ at $\omega=\omega_0$. In other words, the transfer function is acting upon the complex sinusoid and changes it accordingly. Zeros and poles in this case come naturally as those values of frequency where transfer function zeroes out or explodes. Hence, when we multiply $H(\omega)$ with a complex sinusoid at one of these frequencies, the result is clearly visible; the sinusoid either disappears or becomes really big (infinite). Now, let's focus on the Laplace transform; I am not managing to gain the same insight here. As far as the transfer function $H(s)$ is concerned, zeros and poles are still the same thing: characteristic values of $s$ where the function zeroes out or explodes, but similarity ends here; if I choose a causal complex sinusoid as an input of a causal system, i.e. $e^{s_0t}u(t)$, I can write $$y(t)=\int_{-\infty}^{+\infty}d\tau \, h(\tau)x(t-\tau)=e^{s_0t} \cdot \int_{0}^{t}d\tau \, h(\tau)e^{-s_0\tau} \neq e^{s_0t} \cdot H(s_0),$$ so, in short, I can't apply the same logic with zeros/poles as I did with Fourier transform. It's similar if I look at the situation in the $s$-domain $$Y(s)=H(s) \cdot \frac{1}{s+s_0,}$$ and it's obvious that it's not like $H(s)$ ""probes"" the complex sinusoid $e^{s_0t}u(t)$ and changes only its amplitude/phase, as was the case with Fourier transform. So, my conclusion is that, if I feed the system with signal $e^{s_zt}$, where $s_z$-value is the value where, for example, $H(s)=0$, I can't just say ""$H(s)$ is going to act upon it and change it accordingly - in this case, nullify it"". The same conclusion goes for poles. My question is, how should I interpret the poles and zeros of the Laplace transform? Thanks!","I recently started ""relearning"" the Laplace transform, and I noticed something. It seems to me that the intuitive idea of poles and zeros is different between these two transforms! For example, in case of Fourier transform, we can write something like this for the input signal of the system: let the input be a complex sinusoid $e^{j \omega_0 t}$, then $$y(t)=\int_{-\infty}^{+\infty}d\tau \, h(\tau)x(t-\tau)=e^{j \omega_0 t} \cdot \int_{-\infty}^{+\infty}d\tau \, h(\tau)e^{-j \omega_0 \tau}=e^{j \omega_0 t} \cdot H(\omega_0),$$ where $H(\omega_0)$ is the Fourier transform of the impulse response of the system $h(t)$ on a frequency $\omega_0$. The same thing can be seen in frequency domain, where Dirac distribution $2 \pi \delta(\omega-\omega_0)$ essentially ""probes"" the transfer function $H(\omega)$ at $\omega=\omega_0$. In other words, the transfer function is acting upon the complex sinusoid and changes it accordingly. Zeros and poles in this case come naturally as those values of frequency where transfer function zeroes out or explodes. Hence, when we multiply $H(\omega)$ with a complex sinusoid at one of these frequencies, the result is clearly visible; the sinusoid either disappears or becomes really big (infinite). Now, let's focus on the Laplace transform; I am not managing to gain the same insight here. As far as the transfer function $H(s)$ is concerned, zeros and poles are still the same thing: characteristic values of $s$ where the function zeroes out or explodes, but similarity ends here; if I choose a causal complex sinusoid as an input of a causal system, i.e. $e^{s_0t}u(t)$, I can write $$y(t)=\int_{-\infty}^{+\infty}d\tau \, h(\tau)x(t-\tau)=e^{s_0t} \cdot \int_{0}^{t}d\tau \, h(\tau)e^{-s_0\tau} \neq e^{s_0t} \cdot H(s_0),$$ so, in short, I can't apply the same logic with zeros/poles as I did with Fourier transform. It's similar if I look at the situation in the $s$-domain $$Y(s)=H(s) \cdot \frac{1}{s+s_0,}$$ and it's obvious that it's not like $H(s)$ ""probes"" the complex sinusoid $e^{s_0t}u(t)$ and changes only its amplitude/phase, as was the case with Fourier transform. So, my conclusion is that, if I feed the system with signal $e^{s_zt}$, where $s_z$-value is the value where, for example, $H(s)=0$, I can't just say ""$H(s)$ is going to act upon it and change it accordingly - in this case, nullify it"". The same conclusion goes for poles. My question is, how should I interpret the poles and zeros of the Laplace transform? Thanks!",,"['functional-analysis', 'ordinary-differential-equations', 'fourier-analysis', 'laplace-transform']"
96,A question about essential representation in C*-algebra,A question about essential representation in C*-algebra,,"There is a quotation of a book ""C*-algebras Finite-Dimensional Approximations"" below: Definition 1.7.4 . A representation $\pi: A \rightarrow B(H)$ is called essential if $\pi(A)$ contains no nonzero compact operators. ($A$ is a C*-algebra.) Essential representations are easy to construct: if $\pi: A\rightarrow B(H)$ is any representation, then its infinite inflation (i.e., the direct sum of infinitely many copies of $\pi$) will be essential. I can not understand why the construction above is essential?","There is a quotation of a book ""C*-algebras Finite-Dimensional Approximations"" below: Definition 1.7.4 . A representation $\pi: A \rightarrow B(H)$ is called essential if $\pi(A)$ contains no nonzero compact operators. ($A$ is a C*-algebra.) Essential representations are easy to construct: if $\pi: A\rightarrow B(H)$ is any representation, then its infinite inflation (i.e., the direct sum of infinitely many copies of $\pi$) will be essential. I can not understand why the construction above is essential?",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'von-neumann-algebras']"
97,"How should I think about reflexive spaces? What ""property"" do I get from a space being reflexive?","How should I think about reflexive spaces? What ""property"" do I get from a space being reflexive?",,"Let $(X,\| \cdot \|_X)$ be a $\mathbb{R}$-vector space with bidual space $X^{**}$. We defined $X$ to be reflexive, if the canonical embedding $\mathcal I: X \to X^{**}$ with $$\mathcal I x(l) := l(x)$$ for all $l \in X^*, x \in X$ is surjective. I know for example Hilbert spaces are reflexive, $L^p$ spaces are reflexive for $1 < p < \infty$ and $L^1$ is not reflexive. However, I have no idea how I should think about reflexive spaces. I have seen theorems requiring reflexivity in the proof, however I don't understand why there is no proof if the canonical embedding is not surjective (of course I can't find any such proof either). For example, we have seen the following ""approximation theorem"": Let $X$ be reflexive, $M \subset X$ non-empty, convex and closed, $x_0 \in X \setminus M$. Then, there exists $m_0 \in M$ such that $$\| x_0 - m_0 \|_X = \operatorname{dist} (x_0, M) = \operatorname{inf}_{m \in M} \| x_0 - m \|_X.$$ It feels to me as if the fact that $M$ is closed should suffice for this theorem, however of course I am imagining the space as $\mathbb{R}^n$ which I guess I shouldn't. Is there some ""property"" every reflexive space automatically gains? I only see that we can use surjectivity very efficiently to prove those theorems, however I have no clue why there can't be any other proof without surjectivity or without using the canonical embedding at all. Usually, for other requirements of a theorem, it is easy to understand why certain property is crucial but with this property I am struggling.","Let $(X,\| \cdot \|_X)$ be a $\mathbb{R}$-vector space with bidual space $X^{**}$. We defined $X$ to be reflexive, if the canonical embedding $\mathcal I: X \to X^{**}$ with $$\mathcal I x(l) := l(x)$$ for all $l \in X^*, x \in X$ is surjective. I know for example Hilbert spaces are reflexive, $L^p$ spaces are reflexive for $1 < p < \infty$ and $L^1$ is not reflexive. However, I have no idea how I should think about reflexive spaces. I have seen theorems requiring reflexivity in the proof, however I don't understand why there is no proof if the canonical embedding is not surjective (of course I can't find any such proof either). For example, we have seen the following ""approximation theorem"": Let $X$ be reflexive, $M \subset X$ non-empty, convex and closed, $x_0 \in X \setminus M$. Then, there exists $m_0 \in M$ such that $$\| x_0 - m_0 \|_X = \operatorname{dist} (x_0, M) = \operatorname{inf}_{m \in M} \| x_0 - m \|_X.$$ It feels to me as if the fact that $M$ is closed should suffice for this theorem, however of course I am imagining the space as $\mathbb{R}^n$ which I guess I shouldn't. Is there some ""property"" every reflexive space automatically gains? I only see that we can use surjectivity very efficiently to prove those theorems, however I have no clue why there can't be any other proof without surjectivity or without using the canonical embedding at all. Usually, for other requirements of a theorem, it is easy to understand why certain property is crucial but with this property I am struggling.",,['functional-analysis']
98,Applying Open Mapping Theorem,Applying Open Mapping Theorem,,"Let $X$ and $Y$ Banach spaces and $F: X \to Y$ a linear, continuous and surjective mapping. Show that if $K$ is a compact subset of $Y$ then there exists an $L$, a compact subset of $X$ such that $F(L)= K$. I know by the Open Mapping Theorem that $F$ is open. What else can I do? Thank yoU!","Let $X$ and $Y$ Banach spaces and $F: X \to Y$ a linear, continuous and surjective mapping. Show that if $K$ is a compact subset of $Y$ then there exists an $L$, a compact subset of $X$ such that $F(L)= K$. I know by the Open Mapping Theorem that $F$ is open. What else can I do? Thank yoU!",,['functional-analysis']
99,Continuity of double centralizers in Banach algebras,Continuity of double centralizers in Banach algebras,,"I had some problems with a certain exercise, came up with a solution, but I'm not sure it is correct. Exercise (""MURPHY, C*-Algebras and Operator Theory"", Chapter 2, exercise 1) Let $A$ be a Banach algebra such that for all $a\in A$, the implication   $$Aa=0\text{ or }aA=0\Rightarrow a=0\qquad (1)$$   holds. Let $L,R$ be linear mappings from  $A$ to itself such that for all $a,b\in A$,   $$L(ab)=L(a)b,\quad R(ab)=aR(b),\qquad (2)$$   $$\text{and } R(a)b=aL(b).\qquad (3)$$   Show that $L$ and $R$ are necessarily continuous. A double centralizer in a Banach algebra is defined to be a pair $(L,R)$ of bounded linear maps from $A$ to itself satisfying conditions (2) and (3). This exercise shows that if the Banach algebra satisfies condition (1) (for example, if it is a C*-algebra), then boundedness is consequence of (2) and (3). I spent quite some time trying to solve this. Then, Solution: By the Closed Graph Theorem, to show that $L$ is continuous, it suffices to show that for every sequence $\left\{x_n\right\}\subseteq A$ such that $x_n\rightarrow 0$ and such that $\left\{L(x_n)\right\}$ also converges, then $\lim L(x_n)=0$. Let $x_n\rightarrow 0$ and $L(x_n)\rightarrow y$ in $A$. Then for every $n\in\mathbb{N}$ and for every $b\in A$, $$bL(x_n)=R(b)x_n,\quad\text{ by (3)}.$$ Taking $n\rightarrow\infty$, we have, for every $b\in A$, $$by=R(b)0=0,$$ that is, $Ay=0$. By (1), this implies that $y=0$. Continuity of $R$  is proved similarly. Q.E.D. Now, the problem is that I didn't use condition (2), and that seems too strong. Is there any problem with my reasoning? Thank you. EDIT (see Martin's answer): I just found out that (1) and (3) imply (2). This explains why (2) was not used above. Let's show this: Suppose $A$ is an algebra (not necessarily normed) satisfying (1) and let $L,R$ be linear maps from $A$ to $A$ satisfying (3). Let $a,b\in A$. Then for every $c\in A$, $$c(L(ab)-L(a)b)=cL(ab)-cL(a)b\overset{(3)}{=}R(c)(ab)-(R(c)a)b=R(c)ab-R(c)ab=0.$$ Then $A(L(ab)-L(a)b)=0$, hence, by (1), $L(ab)=L(a)b$, for every $a,b\in A$. Similarly, $R(ab)=aR(b)$ for every $a,b\in A$. Therefore, (2) holds.","I had some problems with a certain exercise, came up with a solution, but I'm not sure it is correct. Exercise (""MURPHY, C*-Algebras and Operator Theory"", Chapter 2, exercise 1) Let $A$ be a Banach algebra such that for all $a\in A$, the implication   $$Aa=0\text{ or }aA=0\Rightarrow a=0\qquad (1)$$   holds. Let $L,R$ be linear mappings from  $A$ to itself such that for all $a,b\in A$,   $$L(ab)=L(a)b,\quad R(ab)=aR(b),\qquad (2)$$   $$\text{and } R(a)b=aL(b).\qquad (3)$$   Show that $L$ and $R$ are necessarily continuous. A double centralizer in a Banach algebra is defined to be a pair $(L,R)$ of bounded linear maps from $A$ to itself satisfying conditions (2) and (3). This exercise shows that if the Banach algebra satisfies condition (1) (for example, if it is a C*-algebra), then boundedness is consequence of (2) and (3). I spent quite some time trying to solve this. Then, Solution: By the Closed Graph Theorem, to show that $L$ is continuous, it suffices to show that for every sequence $\left\{x_n\right\}\subseteq A$ such that $x_n\rightarrow 0$ and such that $\left\{L(x_n)\right\}$ also converges, then $\lim L(x_n)=0$. Let $x_n\rightarrow 0$ and $L(x_n)\rightarrow y$ in $A$. Then for every $n\in\mathbb{N}$ and for every $b\in A$, $$bL(x_n)=R(b)x_n,\quad\text{ by (3)}.$$ Taking $n\rightarrow\infty$, we have, for every $b\in A$, $$by=R(b)0=0,$$ that is, $Ay=0$. By (1), this implies that $y=0$. Continuity of $R$  is proved similarly. Q.E.D. Now, the problem is that I didn't use condition (2), and that seems too strong. Is there any problem with my reasoning? Thank you. EDIT (see Martin's answer): I just found out that (1) and (3) imply (2). This explains why (2) was not used above. Let's show this: Suppose $A$ is an algebra (not necessarily normed) satisfying (1) and let $L,R$ be linear maps from $A$ to $A$ satisfying (3). Let $a,b\in A$. Then for every $c\in A$, $$c(L(ab)-L(a)b)=cL(ab)-cL(a)b\overset{(3)}{=}R(c)(ab)-(R(c)a)b=R(c)ab-R(c)ab=0.$$ Then $A(L(ab)-L(a)b)=0$, hence, by (1), $L(ab)=L(a)b$, for every $a,b\in A$. Similarly, $R(ab)=aR(b)$ for every $a,b\in A$. Therefore, (2) holds.",,"['analysis', 'functional-analysis', 'continuity', 'banach-algebras', 'c-star-algebras']"
