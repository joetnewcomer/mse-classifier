,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Inverse this Stokes Matrix.,Inverse this Stokes Matrix.,,"Let $\xi \in \mathbb{R}^d$ . I am looking for a way to compute the inverse of the $d \times d$ matrix by defined $M_\xi:= \xi \otimes \xi - |\xi|^2 I_d,$ where $(\xi \otimes \xi)_{ij}= \xi_i \, \xi_j.$ To give a litle bit of physical background, I believe this matrix should be invertible as it is the pseudo-differential operator of the Stokes operator $- P_\sigma \Delta$ where $P_\sigma$ is the Leray projector. I'm also expecting $M_\xi^{-1}$ to depend morally on $|\xi|^{-2}$ as we should ""integrate twice"" in the Fourier space. Any help is welcomed. Edit 1 Thanks for your help @BenGrossmann and @CalvinKhor. Indeed it seems like my matrix is not invertible since $Ker(M_{\xi})=span(\xi) \neq \{0 \}$ . I was thinking of restricting the beginning space of the linear function $f_\xi$ linked to $M_\xi$ . If I assume that $u \in \{v \in \mathbb{R}^d, \ \text{such that }v \cdot \xi = 0 \} = Span(\xi)^\perp$ , I think I can inverse the equality $$M_{\xi} u = w$$ for any $w \in Span(\xi)^\perp$ . Moreover this restriction to $ Span(\xi)^\perp$ makes sense as it corresponds to the divergence-free condition of the Stokes equation. I need to study the linear function $\tilde{f}_{\xi} : Span(\xi)^\perp \rightarrow Span(\xi)^\perp $ and try to find a good invertible expression of its corresponding matrix ( $d-1 \times d-1$ ). Edit 2 Actually, one can show that for any $u \in Span(\xi)^\perp$ , one has $M_\xi u = -|\xi|^2 u$ , so the matrix of $\tilde{f}_{\xi} $ is the same in every base of $Span(\xi)^\perp$ and is given by $- |\xi|^2 I_{d-1}$ . This matrix is invertible and its inverse is given by $-|\xi|^{-2} I_{d-1}$ .","Let . I am looking for a way to compute the inverse of the matrix by defined where To give a litle bit of physical background, I believe this matrix should be invertible as it is the pseudo-differential operator of the Stokes operator where is the Leray projector. I'm also expecting to depend morally on as we should ""integrate twice"" in the Fourier space. Any help is welcomed. Edit 1 Thanks for your help @BenGrossmann and @CalvinKhor. Indeed it seems like my matrix is not invertible since . I was thinking of restricting the beginning space of the linear function linked to . If I assume that , I think I can inverse the equality for any . Moreover this restriction to makes sense as it corresponds to the divergence-free condition of the Stokes equation. I need to study the linear function and try to find a good invertible expression of its corresponding matrix ( ). Edit 2 Actually, one can show that for any , one has , so the matrix of is the same in every base of and is given by . This matrix is invertible and its inverse is given by .","\xi \in \mathbb{R}^d d \times d M_\xi:= \xi \otimes \xi - |\xi|^2 I_d, (\xi \otimes \xi)_{ij}= \xi_i \, \xi_j. - P_\sigma \Delta P_\sigma M_\xi^{-1} |\xi|^{-2} Ker(M_{\xi})=span(\xi) \neq \{0 \} f_\xi M_\xi u \in \{v \in \mathbb{R}^d, \ \text{such that }v \cdot \xi = 0 \} = Span(\xi)^\perp M_{\xi} u = w w \in Span(\xi)^\perp  Span(\xi)^\perp \tilde{f}_{\xi} : Span(\xi)^\perp \rightarrow Span(\xi)^\perp  d-1 \times d-1 u \in Span(\xi)^\perp M_\xi u = -|\xi|^2 u \tilde{f}_{\xi}  Span(\xi)^\perp - |\xi|^2 I_{d-1} -|\xi|^{-2} I_{d-1}","['linear-algebra', 'matrices', 'partial-differential-equations', 'fourier-analysis']"
1,"If we have $A = AB$, what can we conclude about $B$?","If we have , what can we conclude about ?",A = AB B,"If we have matrices $A$ and $B$ ( $A$ not necessarily square), and $A = AB$ , what can we conclude about $B$ ? I know that the rank of $B$ needs to be at least the rank of $A$ , but I bet more could be said. As a related question, I'm also curious what are some conditions for $A = AB \Rightarrow B = I$ to be true.","If we have matrices and ( not necessarily square), and , what can we conclude about ? I know that the rank of needs to be at least the rank of , but I bet more could be said. As a related question, I'm also curious what are some conditions for to be true.",A B A A = AB B B A A = AB \Rightarrow B = I,"['linear-algebra', 'matrices', 'matrix-rank']"
2,What are the elements of this ring?,What are the elements of this ring?,,"Where $\Bbb F_5$ is the finite field of $5$ elements. I'm not sure what this notation is getting at: $$ \mathbb F_5 \left(\begin{bmatrix}1&2\\2&4\end{bmatrix}\right)$$ I assumed it was just $$ \left\{\begin{bmatrix}1&2\\2&4\end{bmatrix},\begin{bmatrix}2&4\\4&3\end{bmatrix},\begin{bmatrix}3&1\\1&2\end{bmatrix},\begin{bmatrix}4&3\\3&1\end{bmatrix},\begin{bmatrix}0&0\\0&0\end{bmatrix}\right\}$$ These matrices are nilpotent so multiplying them only generates the $0$ matrix",Where is the finite field of elements. I'm not sure what this notation is getting at: I assumed it was just These matrices are nilpotent so multiplying them only generates the matrix,"\Bbb F_5 5  \mathbb F_5 \left(\begin{bmatrix}1&2\\2&4\end{bmatrix}\right)  \left\{\begin{bmatrix}1&2\\2&4\end{bmatrix},\begin{bmatrix}2&4\\4&3\end{bmatrix},\begin{bmatrix}3&1\\1&2\end{bmatrix},\begin{bmatrix}4&3\\3&1\end{bmatrix},\begin{bmatrix}0&0\\0&0\end{bmatrix}\right\} 0","['abstract-algebra', 'matrices', 'ring-theory', 'notation', 'finite-fields']"
3,Show that a triangular block matrix (having all block matrices identical) is non diagonalizable,Show that a triangular block matrix (having all block matrices identical) is non diagonalizable,,"Let $A$ be a $n$ -square matrix. Consider the upper triangular matrix : \begin{equation} M = \begin{pmatrix} A & A \\ 0 & A \end{pmatrix} \end{equation} I need to show that $M$ is diagonalizable if and only if $A$ is the zero matrix. I have thought of considering the characteristic polynomial of $M$ that equals $\chi_A^2$ . I'm thinking of using the result that a matrix is diagonalizable if and only if its minimal polynomial is of the form $\prod_i (X-\lambda_i)$ where the $\lambda_i$ are distinct, and using the fact that : \begin{equation} P(M)= \begin{pmatrix} P(A) & (XP')(A) \\ 0 & P(A) \end{pmatrix} \end{equation} but I don't really see how.","Let be a -square matrix. Consider the upper triangular matrix : I need to show that is diagonalizable if and only if is the zero matrix. I have thought of considering the characteristic polynomial of that equals . I'm thinking of using the result that a matrix is diagonalizable if and only if its minimal polynomial is of the form where the are distinct, and using the fact that : but I don't really see how.",A n \begin{equation} M = \begin{pmatrix} A & A \\ 0 & A \end{pmatrix} \end{equation} M A M \chi_A^2 \prod_i (X-\lambda_i) \lambda_i \begin{equation} P(M)= \begin{pmatrix} P(A) & (XP')(A) \\ 0 & P(A) \end{pmatrix} \end{equation},"['linear-algebra', 'matrices', 'diagonalization', 'block-matrices']"
4,"Selection rules: Why is $\frac1m\sum_g\sum_{\lambda}^{\oplus} \underline{\underline{I}}_{\,\lambda}\otimes\underline{\underline{D}}^{(\lambda)}(g)=0$?",Selection rules: Why is ?,"\frac1m\sum_g\sum_{\lambda}^{\oplus} \underline{\underline{I}}_{\,\lambda}\otimes\underline{\underline{D}}^{(\lambda)}(g)=0","Here is the derivation from some notes given to me. I uploaded these handwritten notes for 2 reasons: To show you that this is the only source of information I have available to me (and it's incredibly difficult to learn from it). In the hopes that you can make more sense of them than I can. There are two expressions in these notes for which I would like to understand, which I have marked with red question marks above the relevant expressions. For the first expression, $$\frac1m\sum_g\underline{\underline{D}}^{(\lambda)}(g)=\begin{cases} 0,  & \text{for rest of $\lambda$'s} \\ 1, & \text{$\lambda$ for fully symmetric IRREP} \end{cases}$$ The $\underline{\underline{D}}^{(\lambda)}(g)$ in this expression (I think) is supposed to represent the triple direct product at the top of the page, namely, $$\underline{\underline{D}}^{(\lambda)}(g)\equiv\Big[{\underline{\underline{D}}^{(1)}}^*\otimes {\underline{\underline{D}}}^{\prime} \otimes {\underline{\underline{D}}^{(2)}}\Big]_{n,\,\beta,\,p,\, i,\,\alpha,\,j}$$ This $\frac1m\sum_g\underline{\underline{D}}^{(\lambda)}(g)$ as I understand it is not an orthogonality expression (but it should be), else how can it possibly be equal to zero or $1$ ? While in the middle of constructing this question I was able to find a PDF version of a book $^{\large\zeta}$ that has a specific chapter that derives selection rules in a remarkably similar way to the notes I have above: I would like to know why ""We note that the sum of matrices of any irreducible representation, other than the identity representation, over the group, is equal to the zero matrix (see Exercise 3.7)"". Which is written in the paragraph underneath equation $(21.7)$ above. This exercise 3.7 has been asked about and answered here which I found earlier and put a comment below the question to indicate the source. User @ Gerry Myerson answered that question, but I am unable to understand his proof. So in summary, I would like to know why $\sum_g\sum_{\lambda}^{\oplus} \underline{\underline{D}}^{(\lambda)}(g)=0?$ Or, if you prefer, could someone please prove why the sum over a group of the matrix elements of any irreducible representation other than the identity/fully symmetric/trivial representation is equal to zero? Update: Although this update possibly should be asked as a separate question and if I am told to do this then I will comply, the reason I ask it here is that it is simply regarding applications of the expression given in the title to this post. I have a few small questions regarding the final page of the authors' written lecture notes (page 74), embedded as an image: As ever, I have scribbled red question marks over the parts I don't understand. I know that for a direct product of two matrices, say $\underline{\underline{A}}=\begin{pmatrix}a_{11}& a_{12}\\a_{21}& a_{22}\\ \end{pmatrix} \,\, \text{and} \,\,\,\underline{\underline{B}}=\begin{pmatrix}b_{11}& b_{12} & b_{13}\\b_{21}& b_{22}& b_{23}\\b_{31}& b_{32}& b_{33} \end{pmatrix}\implies\underline{\underline{A}}\otimes \underline{\underline{B}}=\begin{pmatrix}a_{11}\,\underline{\underline{B}}& a_{12}\,\underline{\underline{B}}\\a_{21}\,\underline{\underline{B}}& a_{22}\,\underline{\underline{B}}\\ \end{pmatrix}$ But that was the direct product for matrices, how does this direct product work for characters? So looking at the character table for $C_{3v}$ the author of these notes written that for $z$ - polarised light $\langle\psi_{A_1}|\hat z | \psi_E\rangle$ for which the decomposition, ${\color{red}{\chi_{A_1}\otimes\chi_{A_1}}}\otimes\chi_E=\chi_E\ne\chi_{A_1}$ . Although I know that for a transition to be allowed, its matrix element must be non-zero, as shown to me in the proofs by @ Gerry Myerson and @ lEm . Therefore, to be non-zero the decomposition must contain the trivial representation, which in this case (from the $C_{3v}$ character table) is the IRREP $A_1$ , but why is $\chi_{A_1}\otimes\chi_{A_1}\otimes\chi_E=\chi_E$ ? Is the red part equal to 1? I know this information is coming from the character table somehow, but I don't understand how. The author then determines whether the matrix element, $\langle\psi_{A_1}|\hat x | \psi_E\rangle$ for the same transition ( $A_1 \to E$ ) for $x$ -polarised light is non-zero by the decomposition, $\chi_{A_1}\otimes\chi_{E}\otimes\chi_E=\chi_E=(4,1,0)$ . But, where does this $(4,1,0)$ come from and how does this 'contain $A_1$ '? I looked at the $C_{3v}$ character table and naively note that summing together the totals for the 3 columns gives $(4,1,0)$ , but this could just be a coincidence. I won't ask about the last 2 red question marks for now, since I may be able to answer them once I've understood the first two. Edit: @lEm Okay, I think I see it now, the $(4,1,0)$ is the whole row of $E$ multiplied by itself, not sure why though. Need to think more, any hints please anyone? $^{\large\zeta}$ The textbook page 265 embedded in this post as an image is from ""Applications of group theory in quantum mechanics"" by Petrashen & Trifonov.","Here is the derivation from some notes given to me. I uploaded these handwritten notes for 2 reasons: To show you that this is the only source of information I have available to me (and it's incredibly difficult to learn from it). In the hopes that you can make more sense of them than I can. There are two expressions in these notes for which I would like to understand, which I have marked with red question marks above the relevant expressions. For the first expression, The in this expression (I think) is supposed to represent the triple direct product at the top of the page, namely, This as I understand it is not an orthogonality expression (but it should be), else how can it possibly be equal to zero or ? While in the middle of constructing this question I was able to find a PDF version of a book that has a specific chapter that derives selection rules in a remarkably similar way to the notes I have above: I would like to know why ""We note that the sum of matrices of any irreducible representation, other than the identity representation, over the group, is equal to the zero matrix (see Exercise 3.7)"". Which is written in the paragraph underneath equation above. This exercise 3.7 has been asked about and answered here which I found earlier and put a comment below the question to indicate the source. User @ Gerry Myerson answered that question, but I am unable to understand his proof. So in summary, I would like to know why Or, if you prefer, could someone please prove why the sum over a group of the matrix elements of any irreducible representation other than the identity/fully symmetric/trivial representation is equal to zero? Update: Although this update possibly should be asked as a separate question and if I am told to do this then I will comply, the reason I ask it here is that it is simply regarding applications of the expression given in the title to this post. I have a few small questions regarding the final page of the authors' written lecture notes (page 74), embedded as an image: As ever, I have scribbled red question marks over the parts I don't understand. I know that for a direct product of two matrices, say But that was the direct product for matrices, how does this direct product work for characters? So looking at the character table for the author of these notes written that for - polarised light for which the decomposition, . Although I know that for a transition to be allowed, its matrix element must be non-zero, as shown to me in the proofs by @ Gerry Myerson and @ lEm . Therefore, to be non-zero the decomposition must contain the trivial representation, which in this case (from the character table) is the IRREP , but why is ? Is the red part equal to 1? I know this information is coming from the character table somehow, but I don't understand how. The author then determines whether the matrix element, for the same transition ( ) for -polarised light is non-zero by the decomposition, . But, where does this come from and how does this 'contain '? I looked at the character table and naively note that summing together the totals for the 3 columns gives , but this could just be a coincidence. I won't ask about the last 2 red question marks for now, since I may be able to answer them once I've understood the first two. Edit: @lEm Okay, I think I see it now, the is the whole row of multiplied by itself, not sure why though. Need to think more, any hints please anyone? The textbook page 265 embedded in this post as an image is from ""Applications of group theory in quantum mechanics"" by Petrashen & Trifonov.","\frac1m\sum_g\underline{\underline{D}}^{(\lambda)}(g)=\begin{cases}
0,  & \text{for rest of \lambda's} \\
1, & \text{\lambda for fully symmetric IRREP}
\end{cases} \underline{\underline{D}}^{(\lambda)}(g) \underline{\underline{D}}^{(\lambda)}(g)\equiv\Big[{\underline{\underline{D}}^{(1)}}^*\otimes {\underline{\underline{D}}}^{\prime} \otimes {\underline{\underline{D}}^{(2)}}\Big]_{n,\,\beta,\,p,\, i,\,\alpha,\,j} \frac1m\sum_g\underline{\underline{D}}^{(\lambda)}(g) 1 ^{\large\zeta} (21.7) \sum_g\sum_{\lambda}^{\oplus} \underline{\underline{D}}^{(\lambda)}(g)=0? \underline{\underline{A}}=\begin{pmatrix}a_{11}& a_{12}\\a_{21}& a_{22}\\ \end{pmatrix} \,\, \text{and} \,\,\,\underline{\underline{B}}=\begin{pmatrix}b_{11}& b_{12} & b_{13}\\b_{21}& b_{22}& b_{23}\\b_{31}& b_{32}& b_{33} \end{pmatrix}\implies\underline{\underline{A}}\otimes \underline{\underline{B}}=\begin{pmatrix}a_{11}\,\underline{\underline{B}}& a_{12}\,\underline{\underline{B}}\\a_{21}\,\underline{\underline{B}}& a_{22}\,\underline{\underline{B}}\\ \end{pmatrix} C_{3v} z \langle\psi_{A_1}|\hat z | \psi_E\rangle {\color{red}{\chi_{A_1}\otimes\chi_{A_1}}}\otimes\chi_E=\chi_E\ne\chi_{A_1} C_{3v} A_1 \chi_{A_1}\otimes\chi_{A_1}\otimes\chi_E=\chi_E \langle\psi_{A_1}|\hat x | \psi_E\rangle A_1 \to E x \chi_{A_1}\otimes\chi_{E}\otimes\chi_E=\chi_E=(4,1,0) (4,1,0) A_1 C_{3v} (4,1,0) (4,1,0) E ^{\large\zeta}","['matrices', 'group-theory', 'representation-theory', 'direct-sum', 'direct-product']"
5,"Are there only two fields that are subrings of the $M_{2,2}(\mathbb{R})$, up to isomorphism? (Or subfields of them)","Are there only two fields that are subrings of the , up to isomorphism? (Or subfields of them)","M_{2,2}(\mathbb{R})","Inspired by $\operatorname{Mat}_2(\mathbb{R})$ as a field ,  it made me curious if, up to isomorphism, we only get two fields as subrings of the ring of two by two matrices under usual matrix addition and subtraction, $\mathbb{R}$ from $<I>$ and $\mathbb{C}$ (via the usual way): Complex number isomorphic to certain $2\times 2$ matrices? . My instinct is yes, since we need invertible matrices that stay invertible under linear combinations,  I don't see a way of doing that without forcing the patterns like we do in the above constructions, but I'm a bit shy of a proof.  (This is idle curiosity) Edit: As was pointed out in the comments, one can easily have any subfield of $\mathbb{R}$ or $\mathbb{C}$ constructed this way.   To get at the heart of what I meant,  can we get any fields that aren't isomorphic to a subfield of $\mathbb{C}$ this way?","Inspired by $\operatorname{Mat}_2(\mathbb{R})$ as a field ,  it made me curious if, up to isomorphism, we only get two fields as subrings of the ring of two by two matrices under usual matrix addition and subtraction, from and (via the usual way): Complex number isomorphic to certain $2\times 2$ matrices? . My instinct is yes, since we need invertible matrices that stay invertible under linear combinations,  I don't see a way of doing that without forcing the patterns like we do in the above constructions, but I'm a bit shy of a proof.  (This is idle curiosity) Edit: As was pointed out in the comments, one can easily have any subfield of or constructed this way.   To get at the heart of what I meant,  can we get any fields that aren't isomorphic to a subfield of this way?",\mathbb{R} <I> \mathbb{C} \mathbb{R} \mathbb{C} \mathbb{C},"['linear-algebra', 'abstract-algebra', 'matrices', 'field-theory']"
6,How does graph Fourier transform retain structural information?,How does graph Fourier transform retain structural information?,,"Context: I am reading about graph Laplacian matrices $L = D - A$ and how their eigenvectors correspond to Fourier modes. From spectral decomposition, $ L = U \Lambda U^{T}$ , where $\Lambda$ is a diagonal matrix of the eigenvalues and $U$ is a matrix of the eigenvectors. Then we can compute the Fourier transform of a signal $\mathbf{f}$ by doing the following calculation: $\mathbf{s} = \mathbf{U}^{T} \mathbf{f}$ . Reference: pg. 83 of the following notes here Question: How does the graph Fourier transform retain structural information about the graph? Do the eigenvectors/eigenvector matrix change for different Laplacian matrices? It sounds silly, but the Fourier modes seem like they would be the same across different Laplacian matrices. My understanding was that they were similar to terms within the DFT matrix. If, in fact, the eigenvectors of the Laplacian did change between different graphs, then I suppose those eigenvectors would retain structural information about the graph. Any help would be greatly appreciated.","Context: I am reading about graph Laplacian matrices and how their eigenvectors correspond to Fourier modes. From spectral decomposition, , where is a diagonal matrix of the eigenvalues and is a matrix of the eigenvectors. Then we can compute the Fourier transform of a signal by doing the following calculation: . Reference: pg. 83 of the following notes here Question: How does the graph Fourier transform retain structural information about the graph? Do the eigenvectors/eigenvector matrix change for different Laplacian matrices? It sounds silly, but the Fourier modes seem like they would be the same across different Laplacian matrices. My understanding was that they were similar to terms within the DFT matrix. If, in fact, the eigenvectors of the Laplacian did change between different graphs, then I suppose those eigenvectors would retain structural information about the graph. Any help would be greatly appreciated.",L = D - A  L = U \Lambda U^{T} \Lambda U \mathbf{f} \mathbf{s} = \mathbf{U}^{T} \mathbf{f},"['matrices', 'graph-theory', 'fourier-transform', 'spectral-graph-theory']"
7,matrix isomorphism apparent contradiction - where is the bug?,matrix isomorphism apparent contradiction - where is the bug?,,"Let $M_2(\mathbb R)$ be the algebra of 2x2 matrices over $\mathbb R$ . Let $M^{op}_2$ be the same algebra, but with product reverted (that is, $A\cdot B \ \hbox{(in $M^{op}$)}= B\cdot A \ \hbox{(in $M$)}$ . By the Skolem-Noether theorem (if I'm not wrong), every isomorphism of $M$ into $M^{op}$ is of the form $f(M) = U^{-1}MU$ , where $U$ is an invertible matrix of $M_2$ (depending only on $f$ ). Now, matrix transposition is such an isomorphism. But I think that if there were exist a matrix $U$ such that $M^T= U^{-1}MU$ for all $M$ , this would be known (a joke, this is impossible of course). So, the theorem of Skolem-Noether is false! where is the bug in my thinking ? Edit : the Skolem-Noether theorem: Let $R$ , $S$ be finite dimensional algebras, $R$ simple and $S$ central simple. If $f, g :\ R → S$ are homomorphisms then there is an element $s ∈ S$ such that, for all $r ∈ R$ , $g(r) = s^{−1} f (r)s$ .","Let be the algebra of 2x2 matrices over . Let be the same algebra, but with product reverted (that is, . By the Skolem-Noether theorem (if I'm not wrong), every isomorphism of into is of the form , where is an invertible matrix of (depending only on ). Now, matrix transposition is such an isomorphism. But I think that if there were exist a matrix such that for all , this would be known (a joke, this is impossible of course). So, the theorem of Skolem-Noether is false! where is the bug in my thinking ? Edit : the Skolem-Noether theorem: Let , be finite dimensional algebras, simple and central simple. If are homomorphisms then there is an element such that, for all , .","M_2(\mathbb R) \mathbb R M^{op}_2 A\cdot B \ \hbox{(in M^{op})}= B\cdot A \ \hbox{(in M)} M M^{op} f(M) = U^{-1}MU U M_2 f U M^T= U^{-1}MU M R S R S f, g :\ R → S s ∈ S r ∈ R g(r) = s^{−1} f (r)s","['linear-algebra', 'matrices']"
8,What does the equality $\|Ax\|_2=\|A\|_2 \|x\|_2$ mean?,What does the equality  mean?,\|Ax\|_2=\|A\|_2 \|x\|_2,"We know that the matrix norm $\|A\|_2$ is compatible with the euclidean vector norm $\|x\|_2$ , i.e. $\|Ax\|_2 \leq \|A\|_2 \|x\|_2$ for all matrices $A$ and vectors $x$ . I am trying to understand what happens in the case that the equality is obtained, so $\|Ax\|_2=\|A\|_2 \|x\|_2$ . Does there exist a (for example) geometric interpretation? Or something like, this equality does only hold for unitary matrices? I am sorry about the confusion, I will try to clarify by splitting my question: Does $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ only hold for special matrices, e.g. unitary matrices? What happens geometrically, if $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ ? What happens geometrically, if $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for a special $x$ ? What else does $\|Ax\|_2 = \|A\|_2 \|x\|_2$ for all $x$ tell you?","We know that the matrix norm is compatible with the euclidean vector norm , i.e. for all matrices and vectors . I am trying to understand what happens in the case that the equality is obtained, so . Does there exist a (for example) geometric interpretation? Or something like, this equality does only hold for unitary matrices? I am sorry about the confusion, I will try to clarify by splitting my question: Does for all only hold for special matrices, e.g. unitary matrices? What happens geometrically, if for all ? What happens geometrically, if for a special ? What else does for all tell you?",\|A\|_2 \|x\|_2 \|Ax\|_2 \leq \|A\|_2 \|x\|_2 A x \|Ax\|_2=\|A\|_2 \|x\|_2 \|Ax\|_2 = \|A\|_2 \|x\|_2 x \|Ax\|_2 = \|A\|_2 \|x\|_2 x \|Ax\|_2 = \|A\|_2 \|x\|_2 x \|Ax\|_2 = \|A\|_2 \|x\|_2 x,"['linear-algebra', 'matrices', 'normed-spaces', 'inner-products']"
9,The form of an $n \times n$ unitary matrix,The form of an  unitary matrix,n \times n,"I recently came across this fact that any $ 2 \times 2 $ unitary matrix can be expressed as $$ \begin{bmatrix} w & z \\ -\overline{z}e^{i\theta} & \overline{w}e^{i\theta}\end{bmatrix}$$ for some $\theta \in \mathbb{R}$ and $w,z \in \mathbb{C}$ . I wanted to ask if there is any similar general form for $n \times n$ unitary matrices too.",I recently came across this fact that any unitary matrix can be expressed as for some and . I wanted to ask if there is any similar general form for unitary matrices too.," 2 \times 2   \begin{bmatrix} w & z \\ -\overline{z}e^{i\theta} & \overline{w}e^{i\theta}\end{bmatrix} \theta \in \mathbb{R} w,z \in \mathbb{C} n \times n","['linear-algebra', 'matrices', 'unitary-matrices']"
10,Proving a specific matrix is positive definite.,Proving a specific matrix is positive definite.,,"I am trying to prove the following matrix is positive definite: Let $A$ be a $n\times n$ real positive definite matrix, so it holds that $A = UDU^T$ where $$D=diag(d_{11},d_{22},\ldots,d_{nn}) > 0$$ For any $n\times1$ vector $w$ , let $q = U^Tw$ , $q$ is also $n\times1$ vector, so define the matrix $$B = Aww^TA + (w^TAw)A - 2UD^*U^T$$ where $D^*$ is a diagnoal matrix and $\{D^*\}_{ii} = (d_{ii}q_{i})^2$ . I want to prove $B$ is positive definite. I used that $A = UDU^T$ to rewrite $$B = UDqq^TDU - UD^*U^T + U(D\cdot(q^TDq))U^T - UD^*U^T$$ The eigenvector $U$ doesn't affect positive definite so I omitted it. The first part $Dqq^TD-D^*$ : provides non-diagonal elements. The second part $D\cdot(q^TDq) - D^*$ provides diagonal elements. Than I need to prove all eigenvalues of $Dqq^TD-D^* + D\cdot(q^TDq) - D^*$ are positive and I'm stuck at this point. Am I wrong? Does $B$ really positive definite ? (I think it's because I tryed a lot of simulations with no conflicts) Thank you so much for any suggestions.","I am trying to prove the following matrix is positive definite: Let be a real positive definite matrix, so it holds that where For any vector , let , is also vector, so define the matrix where is a diagnoal matrix and . I want to prove is positive definite. I used that to rewrite The eigenvector doesn't affect positive definite so I omitted it. The first part : provides non-diagonal elements. The second part provides diagonal elements. Than I need to prove all eigenvalues of are positive and I'm stuck at this point. Am I wrong? Does really positive definite ? (I think it's because I tryed a lot of simulations with no conflicts) Thank you so much for any suggestions.","A n\times n A = UDU^T D=diag(d_{11},d_{22},\ldots,d_{nn}) > 0 n\times1 w q = U^Tw q n\times1 B = Aww^TA + (w^TAw)A - 2UD^*U^T D^* \{D^*\}_{ii} = (d_{ii}q_{i})^2 B A = UDU^T B = UDqq^TDU - UD^*U^T + U(D\cdot(q^TDq))U^T - UD^*U^T U Dqq^TD-D^* D\cdot(q^TDq) - D^* Dqq^TD-D^* + D\cdot(q^TDq) - D^* B","['linear-algebra', 'matrices', 'positive-definite']"
11,Is a symmetric matrix a subspace of nxn matrices?,Is a symmetric matrix a subspace of nxn matrices?,,"Let $M$ be a vector space for all 2x2 matrices. Show that the set of all the symmetric matrices $M2$ ={ $B ∈ M2 : B=B^t$ } is a subspace of M. My solution: The null matrix is symmetric $$B=\begin{pmatrix}0&0\\0&0\end{pmatrix}$$ Let $B$ , $C$ $∈$ $M2$ , where $B = B^t$ , $C=C^t$ . Then, $B+C=B^t + C^t$ . $$B = \begin{pmatrix}a&c\\c&b\end{pmatrix}$$ $$C = \begin{pmatrix}d&f\\f&e\end{pmatrix}$$ $$B+C = \begin{pmatrix}a+d&c+f\\c+f&b+e\end{pmatrix}$$ $$B^t = \begin{pmatrix}a&c\\c&b\end{pmatrix}$$ $$C^t = \begin{pmatrix}d&f\\f&e\end{pmatrix}$$ $$B^t+C^t = \begin{pmatrix}a+d&c+f\\c+f&b+e\end{pmatrix}$$ Therefore, $B + C ∈ M2$ . Let $B ∈ M2$ , $θ ∈ R$ . Then, $θB = θB^t$ $$B = \begin{pmatrix}a&c\\c&b\end{pmatrix}$$ $$θB = \begin{pmatrix}θa&θc\\θc&θb\end{pmatrix}$$ $$B^t = \begin{pmatrix}a&c\\c&b\end{pmatrix}$$ $$θB^t = \begin{pmatrix}θa&θc\\θc&θb\end{pmatrix}$$ Therefore, $θB ∈ M2$ . So, $M2$ is, indeed, a subspace of $M$ . I am just learning linear algebra, so I apologize if I've made any mistakes. Also, I just got into this community, so I am still learning the formatting, so I apologize for any mistakes as well. Appreciate all the answers.","Let be a vector space for all 2x2 matrices. Show that the set of all the symmetric matrices ={ } is a subspace of M. My solution: The null matrix is symmetric Let , , where , . Then, . Therefore, . Let , . Then, Therefore, . So, is, indeed, a subspace of . I am just learning linear algebra, so I apologize if I've made any mistakes. Also, I just got into this community, so I am still learning the formatting, so I apologize for any mistakes as well. Appreciate all the answers.",M M2 B ∈ M2 : B=B^t B=\begin{pmatrix}0&0\\0&0\end{pmatrix} B C ∈ M2 B = B^t C=C^t B+C=B^t + C^t B = \begin{pmatrix}a&c\\c&b\end{pmatrix} C = \begin{pmatrix}d&f\\f&e\end{pmatrix} B+C = \begin{pmatrix}a+d&c+f\\c+f&b+e\end{pmatrix} B^t = \begin{pmatrix}a&c\\c&b\end{pmatrix} C^t = \begin{pmatrix}d&f\\f&e\end{pmatrix} B^t+C^t = \begin{pmatrix}a+d&c+f\\c+f&b+e\end{pmatrix} B + C ∈ M2 B ∈ M2 θ ∈ R θB = θB^t B = \begin{pmatrix}a&c\\c&b\end{pmatrix} θB = \begin{pmatrix}θa&θc\\θc&θb\end{pmatrix} B^t = \begin{pmatrix}a&c\\c&b\end{pmatrix} θB^t = \begin{pmatrix}θa&θc\\θc&θb\end{pmatrix} θB ∈ M2 M2 M,"['linear-algebra', 'matrices', 'vector-spaces', 'solution-verification']"
12,Finding a basis and dimension for a symmetric matrices subspace,Finding a basis and dimension for a symmetric matrices subspace,,"Question: Let $\mathbb{F}$ be $\mathbb{Z}_7$ . Let $A=\begin{bmatrix} 2 & 5\\ 5 & 3 \end{bmatrix} \in M_{2\times 2}( F)$ . Let $W=\left\{B\in M_{2\times 2}( F)\Bigl|( AB)^{t} =AB\right\}$ be a subspace over $\mathbb{F}$ . Find a basis and a dimension for $W$ . My attempt: $Solution.$ $\text{By the given information we have the following: }$ \begin{gather*} AB=\begin{bmatrix} 2 & 5\\ 5 & 3 \end{bmatrix} \cdotp \begin{bmatrix} a & b\\ c & d \end{bmatrix} =\begin{bmatrix} 2a+5c & 2b+5d\\ 5a+3c & 5b+3d \end{bmatrix}\\ \end{gather*} \begin{gather*} AB^{t} =\begin{bmatrix} 2a+5c & 5a+3c\\ 2b+5d & 5b+3d \end{bmatrix}\\ \end{gather*} $\text{By W's condition we get:}$ \begin{gather*} AB^{t} =AB\Longrightarrow \begin{bmatrix} 2a+5c & 5a+3c\\ 2b+5d & 5b+3d \end{bmatrix} =\begin{bmatrix} 2a+5c & 2b+5d\\ 5a+3c & 5b+3d \end{bmatrix}\\ \\ \Longrightarrow 2b+5d=5a+3c\\ \\ \Longrightarrow d=\frac{5a+3c-2b}{5} =a+\frac{3c-2b}{5} =a+\frac{3c+\overbrace{7c}^{0} -2b+\overbrace{7b}^{0}}{5} =a+2c+b \end{gather*} $\text{So $AB$ is depends on 3 free-parameters:}$ \begin{equation*}  \begin{aligned} AB=\begin{bmatrix} 2a+5c & 2b+5d\\ 5a+3c & 5b+3d \end{bmatrix} & & = & & \begin{bmatrix} 2a+5c & 5a+3c\\ 5a+3c & 5b+3( a+2c+b) \end{bmatrix}\\  & &  & & \begin{bmatrix} 2a+5c & 5a+3c\\ 5a+3c & 5b+3a+6c+3b \end{bmatrix}\\  & &  & & \begin{bmatrix} 2a+5c & 5a+3c\\ 5a+3c & 8b+3a+6c \end{bmatrix}\\  & &  & & \begin{bmatrix} 2a+5c & 5a+3c\\ 5a+3c & b+3a+6c \end{bmatrix} \end{aligned} \end{equation*} $\text{We take all the paramaters out of the marices, so we can get the basis vectors.}$ $\text{We get the following:}$ \begin{equation*} \begin{bmatrix} 2a+5c & 5a+3c\\ 5a+3c & b+3a+6c \end{bmatrix} =a\cdotp \begin{bmatrix} 2 & 5\\ 5 & 3 \end{bmatrix} +b\cdotp \begin{bmatrix} 0 & 0\\ 1 & 0 \end{bmatrix} +c\cdotp \begin{bmatrix} 5 & 3\\ 3 & 6 \end{bmatrix} \end{equation*} $\text{Which isomorphic to the following vectors: }$ \begin{equation*} ( 2,5,5,3) ,( 0,0,1,0) ,( 5,3,3,6) \end{equation*} $\text{respectively.}$ $\text{Now, we shall check whether those vectors are linear independent. }$ \begin{gather*} \begin{bmatrix} 2 & 5 & 5 & 3\\ 0 & 0 & 1 & 0\\ 5 & 3 & 3 & 6 \end{bmatrix}\xrightarrow[ \begin{array}{l} \mathcal{L}_{3} +\mathcal{L}_{1}\rightarrow \mathcal{L}_{3}\\ \end{array}]{}\begin{bmatrix} 2 & 5 & 5 & 3\\ 0 & 0 & 1 & 0\\ 0 & 1 & 1 & 2 \end{bmatrix}\xrightarrow[\mathcal{L}_{1} -5\mathcal{L}_{3}\rightarrow \mathcal{L}_{1}]{}\begin{bmatrix} 2 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 1 & 1 & 2 \end{bmatrix}\\ \\ \xrightarrow[ \begin{array}{l} \frac{\mathcal{L}_{1}}{2}\rightarrow \mathcal{L}_{1}\\ \mathcal{L}_{3} -\mathcal{L}_{2}\rightarrow \mathcal{L}_{3} \end{array}]{}\mathcal{\begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 1 & 0 & 2 \end{bmatrix}}\xrightarrow[\mathcal{L}_{3}\leftrightarrow \mathcal{L}_{2}]{}\mathcal{\begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 2\\ 0 & 0 & 1 & 0 \end{bmatrix}} \end{gather*} $\text{Therefore, the vectors are linear independent, so they are basis of $\displaystyle W$,  }$ $\text{and since we have 3 linear independent vectors, we conclude that:}$ \begin{equation*} \dim W=3 \end{equation*} Thoughts: Is what I wrote correct? or perhaps I missed something? I can't see if I am right or wrong, because I haven't solved questions of finding a basis for matrices subspaces. I will be glad for some help. Thank you!","Question: Let be . Let . Let be a subspace over . Find a basis and a dimension for . My attempt: Thoughts: Is what I wrote correct? or perhaps I missed something? I can't see if I am right or wrong, because I haven't solved questions of finding a basis for matrices subspaces. I will be glad for some help. Thank you!","\mathbb{F} \mathbb{Z}_7 A=\begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} \in M_{2\times 2}( F) W=\left\{B\in M_{2\times 2}( F)\Bigl|( AB)^{t} =AB\right\} \mathbb{F} W Solution. \text{By the given information we have the following: } \begin{gather*}
AB=\begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} \cdotp \begin{bmatrix}
a & b\\
c & d
\end{bmatrix} =\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix}\\
\end{gather*} \begin{gather*}
AB^{t} =\begin{bmatrix}
2a+5c & 5a+3c\\
2b+5d & 5b+3d
\end{bmatrix}\\
\end{gather*} \text{By W's condition we get:} \begin{gather*}
AB^{t} =AB\Longrightarrow \begin{bmatrix}
2a+5c & 5a+3c\\
2b+5d & 5b+3d
\end{bmatrix} =\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix}\\
\\
\Longrightarrow 2b+5d=5a+3c\\
\\
\Longrightarrow d=\frac{5a+3c-2b}{5} =a+\frac{3c-2b}{5} =a+\frac{3c+\overbrace{7c}^{0} -2b+\overbrace{7b}^{0}}{5} =a+2c+b
\end{gather*} \text{So AB is depends on 3 free-parameters:} \begin{equation*}
 \begin{aligned}
AB=\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix} & & = & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 5b+3( a+2c+b)
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 5b+3a+6c+3b
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 8b+3a+6c
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & b+3a+6c
\end{bmatrix}
\end{aligned}
\end{equation*} \text{We take all the paramaters out of the marices, so we can get the basis vectors.} \text{We get the following:} \begin{equation*}
\begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & b+3a+6c
\end{bmatrix} =a\cdotp \begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} +b\cdotp \begin{bmatrix}
0 & 0\\
1 & 0
\end{bmatrix} +c\cdotp \begin{bmatrix}
5 & 3\\
3 & 6
\end{bmatrix}
\end{equation*} \text{Which isomorphic to the following vectors: } \begin{equation*}
( 2,5,5,3) ,( 0,0,1,0) ,( 5,3,3,6)
\end{equation*} \text{respectively.} \text{Now, we shall check whether those vectors are linear independent. } \begin{gather*}
\begin{bmatrix}
2 & 5 & 5 & 3\\
0 & 0 & 1 & 0\\
5 & 3 & 3 & 6
\end{bmatrix}\xrightarrow[ \begin{array}{l}
\mathcal{L}_{3} +\mathcal{L}_{1}\rightarrow \mathcal{L}_{3}\\
\end{array}]{}\begin{bmatrix}
2 & 5 & 5 & 3\\
0 & 0 & 1 & 0\\
0 & 1 & 1 & 2
\end{bmatrix}\xrightarrow[\mathcal{L}_{1} -5\mathcal{L}_{3}\rightarrow \mathcal{L}_{1}]{}\begin{bmatrix}
2 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 1 & 2
\end{bmatrix}\\
\\
\xrightarrow[ \begin{array}{l}
\frac{\mathcal{L}_{1}}{2}\rightarrow \mathcal{L}_{1}\\
\mathcal{L}_{3} -\mathcal{L}_{2}\rightarrow \mathcal{L}_{3}
\end{array}]{}\mathcal{\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 2
\end{bmatrix}}\xrightarrow[\mathcal{L}_{3}\leftrightarrow \mathcal{L}_{2}]{}\mathcal{\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 2\\
0 & 0 & 1 & 0
\end{bmatrix}}
\end{gather*} \text{Therefore, the vectors are linear independent, so they are basis of \displaystyle W,  } \text{and since we have 3 linear independent vectors, we conclude that:} \begin{equation*}
\dim W=3
\end{equation*}","['linear-algebra', 'matrices']"
13,How to check that the Poisson bracket of traces of matrix powers is zero?,How to check that the Poisson bracket of traces of matrix powers is zero?,,"The canonical Lie-Poisson bracket on functions on the space $\mathfrak g$ of n×n square matrices is given by: $$\{f_1,f_2\}(a)=\langle a, [df_1(a),df_2(a)] \rangle,$$ where $a \in {\mathfrak g}^*$ , [,] is the commutator and $\langle, \rangle$ is the canonical linear pairing of $\mathfrak g$ and $\mathfrak g^*$ . A mathematical paper I am reading claims that it is obvious that the Poisson bracket of the two functions $f_k(X)= trace(X^k)$ and $f_m(X)= trace(X^m)$ is zero but I have trouble seeing this for myself. Could you help me check that $\{f_k, f_m\}=0$ ? Thank you very much!","The canonical Lie-Poisson bracket on functions on the space of n×n square matrices is given by: where , [,] is the commutator and is the canonical linear pairing of and . A mathematical paper I am reading claims that it is obvious that the Poisson bracket of the two functions and is zero but I have trouble seeing this for myself. Could you help me check that ? Thank you very much!","\mathfrak g \{f_1,f_2\}(a)=\langle a, [df_1(a),df_2(a)] \rangle, a \in {\mathfrak g}^* \langle, \rangle \mathfrak g \mathfrak g^* f_k(X)= trace(X^k) f_m(X)= trace(X^m) \{f_k, f_m\}=0","['matrices', 'poisson-geometry']"
14,"If AB is diagonalisable, then so is $(BA)^2$","If AB is diagonalisable, then so is",(BA)^2,"I have shown that the minimal polynomials of $XY$ and $YX$ differ by at most a factor of $t$ for any square complex matrices $X$ and $Y$ and that their characteristic polynomials are equal. Using this, I argued as follows: $AB$ diagonalisable so $(AB)^2$ diagonalisable so we can write down its minimal polynomial. $(BA)^2=B(ABA)$ so the minimal polynomial of $(BA)^2$ differs from the minimal polynomial of $(AB)^2$ by at most a factor of $t$ . So if $t$ doesn't divide $m_{AB}(t)$ then we're done since $m_{(BA)^2}$ then a product of distinct linear factors. I'm having trouble figuring out what to do if $t$ divides $m_{AB}$ , in other words if $AB$ has $0$ as an eigenvalue. If $m_{(BA)^2}$ drops the factor of $t$ or is $m_{(AB)^2}$ then we're done, but how can I show it doesn't gain a factor of $t$ ?","I have shown that the minimal polynomials of and differ by at most a factor of for any square complex matrices and and that their characteristic polynomials are equal. Using this, I argued as follows: diagonalisable so diagonalisable so we can write down its minimal polynomial. so the minimal polynomial of differs from the minimal polynomial of by at most a factor of . So if doesn't divide then we're done since then a product of distinct linear factors. I'm having trouble figuring out what to do if divides , in other words if has as an eigenvalue. If drops the factor of or is then we're done, but how can I show it doesn't gain a factor of ?",XY YX t X Y AB (AB)^2 (BA)^2=B(ABA) (BA)^2 (AB)^2 t t m_{AB}(t) m_{(BA)^2} t m_{AB} AB 0 m_{(BA)^2} t m_{(AB)^2} t,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
15,Matrix roots of the Hamilton–Cayley equation,Matrix roots of the Hamilton–Cayley equation,,"The Hamilton–Cayley theorem, or Cayley–Hamilton theorem, says that every $n\times n$ matrix is a zero of its own characteristic polynomial. The ring of $n\times n$ matrices is not a field and in particular, if the polynomial is factored over the field of scalars (assuming the entries in the matrix are in a field) the matrix is in some instances not a zero of any of the factors. Can anything interesting be said about the set of $n\times n$ matrices that are zeros of the characteristic polynomial?","The Hamilton–Cayley theorem, or Cayley–Hamilton theorem, says that every matrix is a zero of its own characteristic polynomial. The ring of matrices is not a field and in particular, if the polynomial is factored over the field of scalars (assuming the entries in the matrix are in a field) the matrix is in some instances not a zero of any of the factors. Can anything interesting be said about the set of matrices that are zeros of the characteristic polynomial?",n\times n n\times n n\times n,"['matrices', 'polynomials', 'cayley-hamilton']"
16,Square root of a $3×3$ matrix,Square root of a  matrix,3×3,"I was trying to find square root of this $3×3$ matrix $$\begin{pmatrix} 12&8&31\\ 7&14&28\\ 9&35&6\\ \end{pmatrix}$$ I searched Wikipedia and some site for helpful information and also try to $${ \begin{pmatrix} a&b&c\\ d&e&f\\ g&h&i\\ \end{pmatrix} }^3 =  \begin{pmatrix} 12&8&31\\ 7&14&28\\ 9&35&6\\ \end{pmatrix}$$ Though I know that a similar question has been posted for a $2×2$ matrix, the workings are different so I'll need some help on this Thanks","I was trying to find square root of this matrix I searched Wikipedia and some site for helpful information and also try to Though I know that a similar question has been posted for a matrix, the workings are different so I'll need some help on this Thanks","3×3 \begin{pmatrix}
12&8&31\\
7&14&28\\
9&35&6\\
\end{pmatrix} { \begin{pmatrix}
a&b&c\\
d&e&f\\
g&h&i\\
\end{pmatrix} }^3 = 
\begin{pmatrix}
12&8&31\\
7&14&28\\
9&35&6\\
\end{pmatrix} 2×2","['linear-algebra', 'matrices']"
17,Determinant of a sum of square matrices,Determinant of a sum of square matrices,,"Let $$A=\begin{bmatrix}0&1&0&\cdots 0\\ 0&0&1&\cdots 0\\ \vdots\\ 0&0&0&\cdots 1 \\ 1&1&1&\cdots1  \end{bmatrix}_{n\times n}$$ i.e. it has ones above the main diagonal except for the last row and the last row has all ones. I am trying to find $\det(A+A^2+\cdots+A^t)$ for $t\leq n $ $\tag{1}$ I have checked that for a few $n$ , $\det(A)=\det(A^2)=\cdots=\pm 1$ . But I am not sure how to prove that. Also determinant of sum of matrices is not equal to sum of determinant of those matrices, so I'm not sure how to find $(1)$ ? Any ideas? **EDIT:**I realize that it is hard to find exact value of $(1)$ and so an upper bound for $(1)$ is also useful to me.","Let i.e. it has ones above the main diagonal except for the last row and the last row has all ones. I am trying to find for I have checked that for a few , . But I am not sure how to prove that. Also determinant of sum of matrices is not equal to sum of determinant of those matrices, so I'm not sure how to find ? Any ideas? **EDIT:**I realize that it is hard to find exact value of and so an upper bound for is also useful to me.","A=\begin{bmatrix}0&1&0&\cdots 0\\
0&0&1&\cdots 0\\
\vdots\\
0&0&0&\cdots 1 \\
1&1&1&\cdots1
 \end{bmatrix}_{n\times n} \det(A+A^2+\cdots+A^t) t\leq n  \tag{1} n \det(A)=\det(A^2)=\cdots=\pm 1 (1) (1) (1)",['linear-algebra']
18,Complexity of computing the spectral radius of a non-symmetric square matrix,Complexity of computing the spectral radius of a non-symmetric square matrix,,"Unfortunately, I was not able to find appropriate literature that describes the computational complexity (in O notation) for computing the spectral radius of a square matrix ( not symmetric! ). In particular, my matrix is of the form $A \in \mathbb{R}^{N \times N}$ , where $N$ is a finite integer. While there is some literature on graph theory, and therefore, for symmetric matrices (adjacency matrices), in the case of non-symmetric square matrices my search was unsuccessful. I would be very grateful for any leads, also regarding spectral radius approximations.","Unfortunately, I was not able to find appropriate literature that describes the computational complexity (in O notation) for computing the spectral radius of a square matrix ( not symmetric! ). In particular, my matrix is of the form , where is a finite integer. While there is some literature on graph theory, and therefore, for symmetric matrices (adjacency matrices), in the case of non-symmetric square matrices my search was unsuccessful. I would be very grateful for any leads, also regarding spectral radius approximations.",A \in \mathbb{R}^{N \times N} N,"['matrices', 'asymptotics', 'computational-complexity', 'numerical-linear-algebra', 'spectral-radius']"
19,Show that $ \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z) = ... $,Show that, \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z) = ... ,"Let $X, Y, Z$ be $2 \times 2$ matrices.  Show that these two matrix combinations are equal: $ \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z)   $ $ \text{Tr}(X) \, \text{Tr}(YZ) + \text{Tr}(YX)\,\text{Tr}(Z)+ \text{Tr}(Z)\,\text{Tr}(XY) $ There's lots of identities for matrix trace , here's the only other one that I know, that we can switch the order of the matrix: $$ \text{Tr}(XYZ) = \text{Tr}(YZX) $$ I might specifically need to have $X,Y,Z \in \text{SL}_2(\mathbb{C})$ .  The right side looks symmetric under the cycle permutation of $(XYZ)$ (basically a triangle ) while the left side does not.","Let be matrices.  Show that these two matrix combinations are equal: There's lots of identities for matrix trace , here's the only other one that I know, that we can switch the order of the matrix: I might specifically need to have .  The right side looks symmetric under the cycle permutation of (basically a triangle ) while the left side does not.","X, Y, Z 2 \times 2  \text{Tr}(XYZ) + \text{Tr}(YXZ)+ \text{Tr}(X)\text{Tr}(Y)\text{Tr}(Z)     \text{Tr}(X) \, \text{Tr}(YZ) + \text{Tr}(YX)\,\text{Tr}(Z)+ \text{Tr}(Z)\,\text{Tr}(XY)   \text{Tr}(XYZ) = \text{Tr}(YZX)  X,Y,Z \in \text{SL}_2(\mathbb{C}) (XYZ)","['linear-algebra', 'matrices', 'invariant-theory']"
20,since a symmetric tridiagonal matrix contains only two distinct vectors,since a symmetric tridiagonal matrix contains only two distinct vectors,,"I don't understand meaning of ""since a symmetric tridiagonal matrix contains only two distinct vectors"" I write example for symmetric tridiagonal matrix. The 1st row vector and 1st column vector are the same, the 2nd row vector and 2nd column vector are the same. This way they are all eliminated. What does ""contains only two distinct vectors"" mean?","I don't understand meaning of ""since a symmetric tridiagonal matrix contains only two distinct vectors"" I write example for symmetric tridiagonal matrix. The 1st row vector and 1st column vector are the same, the 2nd row vector and 2nd column vector are the same. This way they are all eliminated. What does ""contains only two distinct vectors"" mean?",,"['linear-algebra', 'matrices']"
21,The measure of the image of the exponential of real matrices,The measure of the image of the exponential of real matrices,,"It is known that the matrix exponential over the real matrices $\exp : M_n(\mathbb{R}) \to GL_n(\mathbb{R})$ is not surjective and that its image $S $ is the subset of all invertible matrices that are the square of a real matrix. My question is about the ""size"" of that set within $GL_n(\mathbb{R})$ equipped with the Lebesgue measure of $\mathbb{R}^{n^2}$ . Do we know if $S$ has full measure ? Or is it a null set ?","It is known that the matrix exponential over the real matrices is not surjective and that its image is the subset of all invertible matrices that are the square of a real matrix. My question is about the ""size"" of that set within equipped with the Lebesgue measure of . Do we know if has full measure ? Or is it a null set ?","\exp : M_n(\mathbb{R}) \to GL_n(\mathbb{R}) S
 GL_n(\mathbb{R}) \mathbb{R}^{n^2} S","['linear-algebra', 'matrices', 'measure-theory', 'matrix-exponential']"
22,"Given that matrix $A$ is diagonalizable and has eigenvalues of $0$ or $1$, show that $A^2 = A$","Given that matrix  is diagonalizable and has eigenvalues of  or , show that",A 0 1 A^2 = A,"Suppose that $A$ is a diagonalizable $n \times n$ matrix such that the characteristic polynomial of $A$ is $p(λ)=λ^k(1−λ)^{n−k}$ , where $k$ is a positive integer such that $0≤k≤n$ . I want to prove $A^2=A$ . I know that for this, I'll need to prove that for any eigenvector $v$ , $A^2 v=Av$ . I tried setting up my proof like this: Since $A$ is diagonalizable, there exists an invertible matrix $P$ such that $A=P^{-1}DP$ , where $D$ has all the eigenvalues of $A$ on its diagonal. But since every eigenvalue is either $0$ or $1$ , $D= \lambda I$ . From there, we get: $$A = P^{-1}DP = P^{-1}(\lambda I_n)P = (\lambda I_n)(P^{-1}P) = \lambda I_n$$ $$ \implies A \times A = (\lambda I_n) \times (\lambda I_n) = \lambda ^2$$ All I've succeeded in doing here is showing $A^2 = \lambda ^2$ . How can I show also that $A^2 = A$ ? Any guidance is greatly appreciated!","Suppose that is a diagonalizable matrix such that the characteristic polynomial of is , where is a positive integer such that . I want to prove . I know that for this, I'll need to prove that for any eigenvector , . I tried setting up my proof like this: Since is diagonalizable, there exists an invertible matrix such that , where has all the eigenvalues of on its diagonal. But since every eigenvalue is either or , . From there, we get: All I've succeeded in doing here is showing . How can I show also that ? Any guidance is greatly appreciated!",A n \times n A p(λ)=λ^k(1−λ)^{n−k} k 0≤k≤n A^2=A v A^2 v=Av A P A=P^{-1}DP D A 0 1 D= \lambda I A = P^{-1}DP = P^{-1}(\lambda I_n)P = (\lambda I_n)(P^{-1}P) = \lambda I_n  \implies A \times A = (\lambda I_n) \times (\lambda I_n) = \lambda ^2 A^2 = \lambda ^2 A^2 = A,"['linear-algebra', 'matrices']"
23,Is the matrix $A − 2I$ invertible?,Is the matrix  invertible?,A − 2I,"Let $A$ be a $4 \times 4$ matrix with eigenvalues $1, 2, 3,$ and $4$ . Is matrix $A − 2I_4$ invertible? I tried to tackle this by constructing a matrix with eigenvalues $1, 2, 3,$ and $4$ : $$ \begin{bmatrix}  1 & 0 & 0 & 0\\ 0 & 2 & 0 & 0\\ 0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 4 \\ \end{bmatrix} \quad $$ Now, $$A − 2I_4 = \begin{bmatrix}  -1 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 2 \\ \end{bmatrix}$$ Since the determinant of this matrix is $0$ , it is not invertible. But this is a very specific case. How can I generalize it to all $4 \times 4$ matrices with eigenvalues $1, 2, 3,$ and $4$ ? Any help is greatly appreciated!","Let be a matrix with eigenvalues and . Is matrix invertible? I tried to tackle this by constructing a matrix with eigenvalues and : Now, Since the determinant of this matrix is , it is not invertible. But this is a very specific case. How can I generalize it to all matrices with eigenvalues and ? Any help is greatly appreciated!","A 4 \times 4 1, 2, 3, 4 A − 2I_4 1, 2, 3, 4 
\begin{bmatrix} 
1 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 3 & 0 \\
0 & 0 & 0 & 4 \\
\end{bmatrix}
\quad
 A − 2I_4 =
\begin{bmatrix} 
-1 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 2 \\
\end{bmatrix} 0 4 \times 4 1, 2, 3, 4","['linear-algebra', 'matrices']"
24,Upper Triangular Matrix with zeros up,Upper Triangular Matrix with zeros up,,"I have a linear algebra homework, but honestly I'vee been trying to solve this problem for some days without success: Suppose T $\in$ $M_{5 \times 5}$ (K) is a triangulable matrix. Be $\beta = \{ \vec{v_1}, \vec{v_2}, \vec{v_3}, \vec{v_4}, \vec{v_5} \}$ a base of $K^5$ such that: $$[T]_\beta = \begin{pmatrix} 4 & * & a & b & c \\ 0 & 4 & d & e & f \\ 0 & 0 & 5 & * & * \\ 0 & 0 & 0 & 5 & * \\ 0 & 0 & 0 & 0 & 5 \end{pmatrix}$$ And I must prove that exists a base $\beta'$ such that: $$[T]_{\beta'} = \begin{pmatrix} 4 & * & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 5 & * & * \\ 0 & 0 & 0 & 5 & * \\ 0 & 0 & 0 & 0 & 5 \end{pmatrix}$$ How can I prove it? Thoughts: If a=b=c=d=e=f=0 means that there is nothing in the coordinates corresponding to the vectors related to 4, but I don't know how to prove that there is a basis with vectors that satisfy this.","I have a linear algebra homework, but honestly I'vee been trying to solve this problem for some days without success: Suppose T (K) is a triangulable matrix. Be a base of such that: And I must prove that exists a base such that: How can I prove it? Thoughts: If a=b=c=d=e=f=0 means that there is nothing in the coordinates corresponding to the vectors related to 4, but I don't know how to prove that there is a basis with vectors that satisfy this.","\in M_{5 \times 5} \beta = \{ \vec{v_1}, \vec{v_2}, \vec{v_3}, \vec{v_4}, \vec{v_5} \} K^5 [T]_\beta = \begin{pmatrix}
4 & * & a & b & c \\
0 & 4 & d & e & f \\
0 & 0 & 5 & * & * \\
0 & 0 & 0 & 5 & * \\
0 & 0 & 0 & 0 & 5
\end{pmatrix} \beta' [T]_{\beta'} = \begin{pmatrix}
4 & * & 0 & 0 & 0 \\
0 & 4 & 0 & 0 & 0 \\
0 & 0 & 5 & * & * \\
0 & 0 & 0 & 5 & * \\
0 & 0 & 0 & 0 & 5
\end{pmatrix}",['matrices']
25,Expansion of the Frobenius norm,Expansion of the Frobenius norm,,this might be very elementary question. I was confused by looking at some different sources when expanding the Frobenius norm into trace. Would these two expressions below always be the same? Or only under certain conditions? \begin{aligned} \left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)\left(X-Y\right)^{\top}\right) \end{aligned} \begin{aligned} \left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)^{\top}\left(X-Y\right)\right) \end{aligned} Thanks,this might be very elementary question. I was confused by looking at some different sources when expanding the Frobenius norm into trace. Would these two expressions below always be the same? Or only under certain conditions? Thanks,"\begin{aligned}
\left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)\left(X-Y\right)^{\top}\right)
\end{aligned} \begin{aligned}
\left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)^{\top}\left(X-Y\right)\right)
\end{aligned}","['linear-algebra', 'matrices', 'transpose', 'matrix-norms']"
26,$P \in \text {SO}_{n} (\Bbb R)$ if $P$ is orthogonal and $P^{-1} A P$ is diagonal with $A$ symmetric?,if  is orthogonal and  is diagonal with  symmetric?,P \in \text {SO}_{n} (\Bbb R) P P^{-1} A P A,Let $A$ be an $n \times n$ real symmetric matrix. Then what I know is that there exists an orthogonal matrix $P$ such that $P^{-1} A P$ is a diagonal matrix or in other words $A$ is orthogonally diagonalizable. Can we say that $P \in \text {SO}_{n} (\Bbb R)\ $ i.e. $\ P \in \text {O}_{n} (\Bbb R)$ with $\det (P) = 1\ $ ? Any help in this regard will be highly appreciated. Thanks in advance.,Let be an real symmetric matrix. Then what I know is that there exists an orthogonal matrix such that is a diagonal matrix or in other words is orthogonally diagonalizable. Can we say that i.e. with ? Any help in this regard will be highly appreciated. Thanks in advance.,A n \times n P P^{-1} A P A P \in \text {SO}_{n} (\Bbb R)\  \ P \in \text {O}_{n} (\Bbb R) \det (P) = 1\ ,"['linear-algebra', 'matrices', 'diagonalization', 'symmetric-matrices', 'orthogonal-matrices']"
27,can a real matrix have both a real minimal polynomial and complex characteristic polynomial?,can a real matrix have both a real minimal polynomial and complex characteristic polynomial?,,"I had a test a few days ago, and I had a question there. can there be a real matrix where the minimal polynomial is $(x^2 - 3x + 2)$ and the characteristic polynomial is $(x^2 - 3x + 2)(x^2 - x + 2)$ ? I tried solving it for hours, and searched a lot for an answer. also on that note, is it possible for a minimal polynomial to not have all the eigenvalues? such as in this case? edit: thank you for the answer, however, how do I prove this without the theorem? can I prove this through the fact that it has 4 distinct eigenvalues, and therefore Diagonalizable?","I had a test a few days ago, and I had a question there. can there be a real matrix where the minimal polynomial is and the characteristic polynomial is ? I tried solving it for hours, and searched a lot for an answer. also on that note, is it possible for a minimal polynomial to not have all the eigenvalues? such as in this case? edit: thank you for the answer, however, how do I prove this without the theorem? can I prove this through the fact that it has 4 distinct eigenvalues, and therefore Diagonalizable?",(x^2 - 3x + 2) (x^2 - 3x + 2)(x^2 - x + 2),"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
28,Taking matrix derivative $\| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F$ with respect to W,Taking matrix derivative  with respect to W,\| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F,"I am trying to take the matrix derivative of the following function with respect to $\bf W$ : \begin{equation} \| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F \\ \end{equation} Where $\mathbf{X}$ is $n \times d$ , $\mathbf{W}$ is $d \times K$ and $\mathbf{1}_{n \times K}$ is a marix with all elements one. $\| \cdot \|_F$ is the Frobenius norm and $\left| \mathbf{X}\mathbf{W}\right|$ is the element wise absolute value of $\mathbf{X}\mathbf{W}$ . Any helps is highly appreciated.","I am trying to take the matrix derivative of the following function with respect to : Where is , is and is a marix with all elements one. is the Frobenius norm and is the element wise absolute value of . Any helps is highly appreciated.","\bf W \begin{equation}
\| \left| \mathbf{X}\mathbf{W}\right|-\mathbf{1}_{n \times K} \| ^2_F \\
\end{equation} \mathbf{X} n \times d \mathbf{W} d \times K \mathbf{1}_{n \times K} \| \cdot \|_F \left| \mathbf{X}\mathbf{W}\right| \mathbf{X}\mathbf{W}","['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
29,"In the Cholesky decomposition, the argument of the square root is always positive if the matrix is real and positive definite. Why?","In the Cholesky decomposition, the argument of the square root is always positive if the matrix is real and positive definite. Why?",,Computing the Cholesky decomposition for an $n \times n$ matrix $A$ you need to evaluate $$l_{jj} = \sqrt{a_{jj}-\sum_{k=1}^{j-1} l^2_{jk}}$$ The argument of the square root is always positive if $A$ is real and positive definite.  Why is that the case?,Computing the Cholesky decomposition for an matrix you need to evaluate The argument of the square root is always positive if is real and positive definite.  Why is that the case?,n \times n A l_{jj} = \sqrt{a_{jj}-\sum_{k=1}^{j-1} l^2_{jk}} A,"['matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'positive-definite', 'cholesky-decomposition']"
30,Special name for matrices with the same singular values?,Special name for matrices with the same singular values?,,"Suppose there exist matrices $A \in \Bbb C^{m \times n}$ and $B = U_1 A U_2$ , where $U_1 \in \Bbb C^{m \times m}$ and $U_2 \in \Bbb C^{n \times n}$ are unitary matrices (not necessarily related to each other). Then, $A$ and $B$ have the same singular values. The reason for this is because $A^H A$ and $B^H B$ are related the following way ( $A^H$ is the Hermitian transpose of $A$ ): $$B^H B = (U_1 A U_2)^H (U_1 A U_2) = U_2^H A^H U_1^H U_1 A U_2 = U_2^H A^H A U_2 = U_2^{-1} (A^H A) U_2$$ Since $A^H A$ and $B^H B$ are similar matrices (by definition), they share the same eigenvalues. Since the singular values of any matrix $M$ are the positive square-roots of the eigenvalues of $M^T M$ , $A$ and $B$ have the same singular values. Is there a special name relating these types of matrices (just like ""similar matrix"" relates $A$ and $B$ with the same eigenvalues)? I know that orthogonally equivalent matrices $C$ and $D = U C U^H$ are kind of similar to this, but $C$ and $D$ are always square, and the two unitary matrices are related as the inverse of one another ( $U^H = U^{-1}$ ). In my problem statement, $A$ and $B$ can be rectangular, and the 2 unitary matrices $U_1$ and $U_2$ don't have to be related to each other (in fact, even their dimensions may differ).","Suppose there exist matrices and , where and are unitary matrices (not necessarily related to each other). Then, and have the same singular values. The reason for this is because and are related the following way ( is the Hermitian transpose of ): Since and are similar matrices (by definition), they share the same eigenvalues. Since the singular values of any matrix are the positive square-roots of the eigenvalues of , and have the same singular values. Is there a special name relating these types of matrices (just like ""similar matrix"" relates and with the same eigenvalues)? I know that orthogonally equivalent matrices and are kind of similar to this, but and are always square, and the two unitary matrices are related as the inverse of one another ( ). In my problem statement, and can be rectangular, and the 2 unitary matrices and don't have to be related to each other (in fact, even their dimensions may differ).",A \in \Bbb C^{m \times n} B = U_1 A U_2 U_1 \in \Bbb C^{m \times m} U_2 \in \Bbb C^{n \times n} A B A^H A B^H B A^H A B^H B = (U_1 A U_2)^H (U_1 A U_2) = U_2^H A^H U_1^H U_1 A U_2 = U_2^H A^H A U_2 = U_2^{-1} (A^H A) U_2 A^H A B^H B M M^T M A B A B C D = U C U^H C D U^H = U^{-1} A B U_1 U_2,"['linear-algebra', 'matrices', 'terminology', 'singular-values']"
31,Jordan normal form of sum of two commuting nilpotent matrices over a finite field (variant on a linear matrix pencil problem),Jordan normal form of sum of two commuting nilpotent matrices over a finite field (variant on a linear matrix pencil problem),,"This question comes up with trying to construct Lie subalgebras of (large) Lie algebras that are invariant under a finite group $H$ . I have two isomorphic $H$ -invariant nilpotent subalgebras and am interested in the Jordan normal forms of matrices in diagonal subalgebras of these algebras. I have two commuting nilpotent matrices $A$ and $B$ , (dimension 1596, so cannot be just looked at), defined over the field $\mathbb{F}_9$ . They both cube to zero, and so $A+\lambda B$ cubes to zero for any $\lambda\in\overline{\mathbb{F}_3}$ . I'm interested in the Jordan normal form of the matrix $A+\lambda B$ , where $\lambda$ is a parameter. In all the examples I have so far, if $A$ and $B$ have the same normal form (in the particular case I have in front of me, blocks $3^{285},1^{741}$ ) then for all but finitely many values of $\lambda$ the blocks of the sum are the same. Furthermore, the number of exceptions to this statement is small, say around $2$ . This could be because my matrices, coming from Lie algebras, are very special. What I really want to know if the following: Is it true that $A+\lambda B$ has Jordan normal form independent of $\lambda$ for cofinitely many $\lambda$ ? Is there a bound on the number of exceptions, say in characteristic $3$ with cube zero matrices? If $A$ and $B$ are defined over $\mathbb{F}_q$ then do the exceptions lie in a fixed overfield, say $\mathbb{F}_{q^6}$ ? (I am thinking $6$ because then all quadratics and cubics in $\lambda$ split. I know that one needs at least $\mathbb{F}_{q^2}$ by examples.) I really want to know that the JNF of $A+\lambda B$ is what I think it should be for most elements of the algebraic closure, leaving only a finite number to check with a computer. I can do finitely many checks, but not infinitely many! Or is there an algorithm that allows us to understand such problems?","This question comes up with trying to construct Lie subalgebras of (large) Lie algebras that are invariant under a finite group . I have two isomorphic -invariant nilpotent subalgebras and am interested in the Jordan normal forms of matrices in diagonal subalgebras of these algebras. I have two commuting nilpotent matrices and , (dimension 1596, so cannot be just looked at), defined over the field . They both cube to zero, and so cubes to zero for any . I'm interested in the Jordan normal form of the matrix , where is a parameter. In all the examples I have so far, if and have the same normal form (in the particular case I have in front of me, blocks ) then for all but finitely many values of the blocks of the sum are the same. Furthermore, the number of exceptions to this statement is small, say around . This could be because my matrices, coming from Lie algebras, are very special. What I really want to know if the following: Is it true that has Jordan normal form independent of for cofinitely many ? Is there a bound on the number of exceptions, say in characteristic with cube zero matrices? If and are defined over then do the exceptions lie in a fixed overfield, say ? (I am thinking because then all quadratics and cubics in split. I know that one needs at least by examples.) I really want to know that the JNF of is what I think it should be for most elements of the algebraic closure, leaving only a finite number to check with a computer. I can do finitely many checks, but not infinitely many! Or is there an algorithm that allows us to understand such problems?","H H A B \mathbb{F}_9 A+\lambda B \lambda\in\overline{\mathbb{F}_3} A+\lambda B \lambda A B 3^{285},1^{741} \lambda 2 A+\lambda B \lambda \lambda 3 A B \mathbb{F}_q \mathbb{F}_{q^6} 6 \lambda \mathbb{F}_{q^2} A+\lambda B","['linear-algebra', 'matrices', 'lie-algebras', 'finite-fields', 'jordan-normal-form']"
32,"A question based on quadratic forms in linear algebra ( rank, representation)","A question based on quadratic forms in linear algebra ( rank, representation)",,"This particular question was asked in masters of mathematics exam of a university and I am unable to solve it. So I am asking it here. Consider the quadratic form $Q(v)=v^{t} A v$ , where $$A=\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{bmatrix},\quad v=(x, y, z, w)$$ Then $Q$ has rank 3 . $x y+z^{2}=Q(P v)$ for some invertible $4 \times 4$ real matrix $P$ $x y+y^{2}+z^{2}=Q(P v)$ for some invertible $4 \times 4$ real matrix $P$ . $x^{2}+y^{2}-z w=Q(P v)$ for some invertible $4 \times 4$ real matrix $P$ . Attempt: Determinant of $D_{1} $ =1 , $D_{2}$ = 1 , $D_{3}$ =-1 for some matrix and $D_{4}$ = -1 . So matrix is neither positive definite nor negative definite. Also, I have read everything about quadratic forms from wikipedia as quadratic forms were not covered in my linear algebra class. So, can anyone please tell how to solve this question. Can anyone please tell any textbook of linear algebra which covers quadratic forms in detail? I shall be really thankful.","This particular question was asked in masters of mathematics exam of a university and I am unable to solve it. So I am asking it here. Consider the quadratic form , where Then has rank 3 . for some invertible real matrix for some invertible real matrix . for some invertible real matrix . Attempt: Determinant of =1 , = 1 , =-1 for some matrix and = -1 . So matrix is neither positive definite nor negative definite. Also, I have read everything about quadratic forms from wikipedia as quadratic forms were not covered in my linear algebra class. So, can anyone please tell how to solve this question. Can anyone please tell any textbook of linear algebra which covers quadratic forms in detail? I shall be really thankful.","Q(v)=v^{t} A v A=\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix},\quad v=(x, y, z, w) Q x y+z^{2}=Q(P v) 4 \times 4 P x y+y^{2}+z^{2}=Q(P v) 4 \times 4 P x^{2}+y^{2}-z w=Q(P v) 4 \times 4 P D_{1}  D_{2} D_{3} D_{4}",['linear-algebra']
33,matrix inner product between positive semidefinite matrix and positive definite matrix,matrix inner product between positive semidefinite matrix and positive definite matrix,,"Let $F_0, F_1, \ldots, F_m$ be a $n \times n$ symmetric matrices. We define $$F(x) := F_0 + x_1 F_1 + \cdots + x_m F_m$$ Show that if there does not exist $x \in \Bbb R^m$ such that $F(x)$ is positive definite, then there does exist a positive semidefinite matrix $H \neq 0$ such that $$\sup_x \mbox{tr} \left( F(x) H \right) \leq 0$$ Can you solve it? Contraposition may be useful, but I have no idea.","Let be a symmetric matrices. We define Show that if there does not exist such that is positive definite, then there does exist a positive semidefinite matrix such that Can you solve it? Contraposition may be useful, but I have no idea.","F_0, F_1, \ldots, F_m n \times n F(x) := F_0 + x_1 F_1 + \cdots + x_m F_m x \in \Bbb R^m F(x) H \neq 0 \sup_x \mbox{tr} \left( F(x) H \right) \leq 0","['linear-algebra', 'matrices', 'trace', 'semidefinite-programming', 'linear-matrix-inequality']"
34,"Existence of differentiable matrix maps $M(3,\mathbb{R}) \rightarrow M(3,\mathbb{R})$",Existence of differentiable matrix maps,"M(3,\mathbb{R}) \rightarrow M(3,\mathbb{R})","Let's say that $M(3,\mathbb{R})$ is the set of square matrices of dimension $3*3$ . Is there a neighborhood $N$ of $I_3$ on which there is a differentiable square root map $f: N \rightarrow M(3,\mathbb{R})$ , with $$f(I) = \begin{pmatrix} -1 &0 &0 \\ 0 &1 &0\\ 0&0&1 \end{pmatrix}$$ and $(f(A))^2=A$ for every $A \in N$ ? Another question is as follows: Is there a neighborhood $L$ of $I_3$ on which there is a $C'$ class function $g: L \rightarrow M(3,\mathbb{R})$ , with $$g(I) = \begin{pmatrix} 0 &1 &0 \\ 0 &0 &1\\ 1&0&0 \end{pmatrix}$$ and $(g(B))^3=B$ for every $B \in L$ ? Background: I learned that matrices can represent derivatives of multivariable functions, and have understood, for example, Is there a general form for the derivative of a matrix to a power? , but I do now know how to use the conditions given by the question to answer if the questions are true or false. edit: There is a previous possibly related question, that the Linear transformation $T: M(3,\mathbb{R}) \rightarrow M(3,\mathbb{R}), T(B) = AB+BA$ (for a diagonal $A \in M(3,\mathbb{R})$ ) is invertible if the diagonal elements of $A$ satisfy a certain condition.","Let's say that is the set of square matrices of dimension . Is there a neighborhood of on which there is a differentiable square root map , with and for every ? Another question is as follows: Is there a neighborhood of on which there is a class function , with and for every ? Background: I learned that matrices can represent derivatives of multivariable functions, and have understood, for example, Is there a general form for the derivative of a matrix to a power? , but I do now know how to use the conditions given by the question to answer if the questions are true or false. edit: There is a previous possibly related question, that the Linear transformation (for a diagonal ) is invertible if the diagonal elements of satisfy a certain condition.","M(3,\mathbb{R}) 3*3 N I_3 f: N \rightarrow M(3,\mathbb{R}) f(I) = \begin{pmatrix} -1 &0 &0 \\ 0 &1 &0\\ 0&0&1 \end{pmatrix} (f(A))^2=A A \in N L I_3 C' g: L \rightarrow M(3,\mathbb{R}) g(I) = \begin{pmatrix} 0 &1 &0 \\ 0 &0 &1\\ 1&0&0 \end{pmatrix} (g(B))^3=B B \in L T: M(3,\mathbb{R}) \rightarrow M(3,\mathbb{R}), T(B) = AB+BA A \in M(3,\mathbb{R}) A","['real-analysis', 'linear-algebra', 'matrices', 'multivariable-calculus', 'matrix-calculus']"
35,"What is $\frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})}$ in terms of $\hat{\mu}_1$, $\hat{\mu}_2$ and $\hat{\Sigma}$","What is  in terms of ,  and",\frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})} \hat{\mu}_1 \hat{\mu}_2 \hat{\Sigma},"Let $X_1,...,X_{n_1}$ be an i.i.d. sample from $N_p(\mu_1,\Sigma)$ and let $Y_1,...,Y_{n_2}$ be an independent sample from $N_p(\mu_2,\Sigma)$ , for some $\mu_1,\mu_2 \in \mathbb{R}^p$ and some invertible, $p\times p$ positive definite matrix $\Sigma$ . Let $\hat{\mu}_0 := \frac{\sum_{i=1}^{n_1}x_i + \sum_{i=1}^{n_2}y_i}{n_1 + n_2}$ , $\hat{\mu}_1 := \frac{1}{n_1}\sum_{i=1}^{n_1}x_i$ and $\hat{\mu}_2 := \frac{1}{n_2}\sum_{i=1}^{n_2}y_i$ Suppose $\hat{\Sigma}_0=\frac{1}{n_1+n_2}\biggl(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_0)(x_i-\hat{\mu}_0)^T+\sum^{n_2}_{i=1}(y_i-\hat{\mu}_0)(y_i-\hat{\mu}_0)^T\biggr)$ and $\hat{\Sigma}=\frac{1}{n_1+n_2}\biggl(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)(x_i-\hat{\mu}_1)^T+\sum^{n_2}_{i=1}(y_i-\hat{\mu}_2)(y_i-\hat{\mu}_2)^T\biggr)$ I would like to show that: $$\frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})} = 1 + \frac{n_1n_2}{(n_1+n_2)^2}(\hat{\mu}_1 -\hat{\mu}_2)^T\hat\Sigma^{-1}(\hat{\mu}_1 -\hat{\mu}_2) $$ I know that $\frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})}=\det(\hat{\Sigma}^{-1/2}\hat{\Sigma}_0\hat{\Sigma}^{-1/2})$ , but I'm not sure how to continue.","Let be an i.i.d. sample from and let be an independent sample from , for some and some invertible, positive definite matrix . Let , and Suppose and I would like to show that: I know that , but I'm not sure how to continue.","X_1,...,X_{n_1} N_p(\mu_1,\Sigma) Y_1,...,Y_{n_2} N_p(\mu_2,\Sigma) \mu_1,\mu_2 \in \mathbb{R}^p p\times p \Sigma \hat{\mu}_0 := \frac{\sum_{i=1}^{n_1}x_i + \sum_{i=1}^{n_2}y_i}{n_1 + n_2} \hat{\mu}_1 := \frac{1}{n_1}\sum_{i=1}^{n_1}x_i \hat{\mu}_2 := \frac{1}{n_2}\sum_{i=1}^{n_2}y_i \hat{\Sigma}_0=\frac{1}{n_1+n_2}\biggl(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_0)(x_i-\hat{\mu}_0)^T+\sum^{n_2}_{i=1}(y_i-\hat{\mu}_0)(y_i-\hat{\mu}_0)^T\biggr) \hat{\Sigma}=\frac{1}{n_1+n_2}\biggl(\sum^{n_1}_{i=1}(x_i-\hat{\mu}_1)(x_i-\hat{\mu}_1)^T+\sum^{n_2}_{i=1}(y_i-\hat{\mu}_2)(y_i-\hat{\mu}_2)^T\biggr) \frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})} = 1 + \frac{n_1n_2}{(n_1+n_2)^2}(\hat{\mu}_1 -\hat{\mu}_2)^T\hat\Sigma^{-1}(\hat{\mu}_1 -\hat{\mu}_2)  \frac{\det(\hat{\Sigma}_0)}{\det(\hat{\Sigma})}=\det(\hat{\Sigma}^{-1/2}\hat{\Sigma}_0\hat{\Sigma}^{-1/2})","['matrices', 'statistics']"
36,Jacobson radical of upper triangular matrix ring,Jacobson radical of upper triangular matrix ring,,"I do not follow this solution for the Jacobson radical of the upper triangular matrix ring $U_2(\mathbb{Z}_{63})$ . NB: in the solution, the result that “ $1-ra$ is a unit for all $r\in R$ iff $a$ belongs to every maximal left ideal in a ring $R$ “ is being used In the last paragraph it says it is sufficient for $(1-ra),(1-tc)$ to belong to $\mathbb{Z}_{63} \setminus \{0\}$ . How can this be the case when $\mathbb{Z}_{63}$ is a domain? For example $7,9 \in \mathbb{Z}_{63}$ but their product is $0$ .","I do not follow this solution for the Jacobson radical of the upper triangular matrix ring . NB: in the solution, the result that “ is a unit for all iff belongs to every maximal left ideal in a ring “ is being used In the last paragraph it says it is sufficient for to belong to . How can this be the case when is a domain? For example but their product is .","U_2(\mathbb{Z}_{63}) 1-ra r\in R a R (1-ra),(1-tc) \mathbb{Z}_{63} \setminus \{0\} \mathbb{Z}_{63} 7,9 \in \mathbb{Z}_{63} 0","['abstract-algebra', 'matrices', 'ring-theory', 'radicals']"
37,Efficiently solving a 2D affine transformation,Efficiently solving a 2D affine transformation,,"For an affine transformation in two dimensions defined as follows: $$  p_i'=\mathbf{A}p_i \Leftrightarrow \\ \left[ \begin{matrix} x_i' \\ y_i' \end{matrix} \right] = \left[ \begin{matrix} a & b & e \\ c & d & f \end{matrix} \right] \left[ \begin{matrix} x_i \\ y_i \\ 1 \end{matrix} \right] $$ Where $(x_i,y_i), (x_i',y_i')$ are corresponding points, how can I find the parameters $\mathbf A$ efficiently? Rewriting this as a system of linear equations, given three points (six knowns, six unknowns): $$ \textbf{P}\alpha=\textbf{P}' \Leftrightarrow \\ \left[ \begin{matrix} x_0 & y_0 & 0 & 0 & 1 & 0 \\ 0 & 0 & x_0 & y_0 & 0 & 1 \\ x_1 & y_1 & 0 & 0 & 1 & 0 \\ 0 & 0 & x_1 & y_1 & 0 & 1 \\ x_2 & y_2 & 0 & 0 & 1 & 0 \\ 0 & 0 & x_2 & y_2 & 0 & 1 \\ \end{matrix} \right] \left[ \begin{matrix} a \\ b \\ c \\ d \\ e \\ f \end{matrix} \right] = \left[ \begin{matrix} x_0' \\ y_0' \\x_1' \\ y_1' \\x_2' \\ y_2' \end{matrix} \right] $$ Allows the use of an LU decomposition, which can be computed in $O(M(n))$ time, where $M(n)$ is the time to multiply two n×n matrices (according to 1 ). Can the specific structure of the $\mathbf P$ matrix be exploited to utilize the Gaussian elimination to reach the reduced row echelon form (thus solving the system) more efficiently? Is there a way to symbolically derive the required operations? By hand seems rather cumbersome Thanks","For an affine transformation in two dimensions defined as follows: Where are corresponding points, how can I find the parameters efficiently? Rewriting this as a system of linear equations, given three points (six knowns, six unknowns): Allows the use of an LU decomposition, which can be computed in time, where is the time to multiply two n×n matrices (according to 1 ). Can the specific structure of the matrix be exploited to utilize the Gaussian elimination to reach the reduced row echelon form (thus solving the system) more efficiently? Is there a way to symbolically derive the required operations? By hand seems rather cumbersome Thanks"," 
p_i'=\mathbf{A}p_i \Leftrightarrow \\
\left[
\begin{matrix}
x_i' \\ y_i'
\end{matrix}
\right]
=
\left[
\begin{matrix}
a & b & e \\
c & d & f
\end{matrix}
\right]
\left[
\begin{matrix}
x_i \\ y_i \\ 1
\end{matrix}
\right]
 (x_i,y_i), (x_i',y_i') \mathbf A 
\textbf{P}\alpha=\textbf{P}' \Leftrightarrow \\
\left[
\begin{matrix}
x_0 & y_0 & 0 & 0 & 1 & 0 \\
0 & 0 & x_0 & y_0 & 0 & 1 \\
x_1 & y_1 & 0 & 0 & 1 & 0 \\
0 & 0 & x_1 & y_1 & 0 & 1 \\
x_2 & y_2 & 0 & 0 & 1 & 0 \\
0 & 0 & x_2 & y_2 & 0 & 1 \\
\end{matrix}
\right]
\left[
\begin{matrix}
a \\ b \\ c \\
d \\ e \\ f
\end{matrix}
\right]
=
\left[
\begin{matrix}
x_0' \\ y_0' \\x_1' \\ y_1' \\x_2' \\ y_2'
\end{matrix}
\right]
 O(M(n)) M(n) \mathbf P","['linear-algebra', 'matrices', 'geometry', 'numerical-linear-algebra']"
38,Bound on the $2$-norm of a diagonal sub-matrix,Bound on the -norm of a diagonal sub-matrix,2,"Let $A \in \mathbb{R}^{n \times n}$ be an invertible real matrix and write $A_d$ for the sub-matrix consisting of its diagonal part only, namely $(A_d)_{ij} = A_{ij}$ if $i = j$ and $0$ otherwise. I can prove that $$\lVert A_d \rVert_2 \leq \lVert A_d \rVert_F \leq \lVert A \rVert_F \leq \sqrt{n}\lVert A \rVert_2 $$ but can this inequality be improved? In other words, can we find $A$ invertible such that $\lVert A_d \rVert_2 = \sqrt{n}\lVert A \rVert_2$ ?","Let be an invertible real matrix and write for the sub-matrix consisting of its diagonal part only, namely if and otherwise. I can prove that but can this inequality be improved? In other words, can we find invertible such that ?",A \in \mathbb{R}^{n \times n} A_d (A_d)_{ij} = A_{ij} i = j 0 \lVert A_d \rVert_2 \leq \lVert A_d \rVert_F \leq \lVert A \rVert_F \leq \sqrt{n}\lVert A \rVert_2  A \lVert A_d \rVert_2 = \sqrt{n}\lVert A \rVert_2,"['linear-algebra', 'matrices', 'normed-spaces']"
39,Expressions for the dual of a polyhedral cone,Expressions for the dual of a polyhedral cone,,"Let $K$ be a polyhedral cone generated by the rows of a matrix $A\in\mathbb{R}^{p\times n}$ , and constrained by the columns of a matrix $B\in\mathbb{R}^{n\times q}$ , such that $K = \text{cone}(A_1^T,\dots, A_p^T) = \{x\in\mathbb{R}^n\text{ }|\text{ }x^TB\geq\textbf{0}^T\}$ , where $A_i$ denotes the $i$ -th row vector of $A$ . Additionally, suppose the matrix $AB\in\mathbb{R}^{p\times q}$ is non-negative. We have that the dual cone $K^* := \{y\in\mathbb{R}^n\text{ }|\text{ }x^Ty\geq 0\text{ }\forall x\in K\} = \{y\in\mathbb{R}^n\text{ }|\text{ }Ay\geq\textbf{0}\}$ . I want to show that it is also true that $K^* = \text{cone}(B^1,\dots, B^q)$ , where $B^j$ denotes the $j$ -th column vector of $B$ . I can see that the ' $\supset$ ' inclusion is true, because each $B^j$ is in $K^*$ (since $K = \{x\in\mathbb{R}^n\text{ }|\text{ }x^TB\geq\textbf{0}^T\}$ , and each $A_i^TB^j = (AB)_{ij}\geq 0$ by hypothesis, so we can extend linearly to cone $(A_1^T,\dots A_p^T) = K$ ), so then again by linearity I can extend to all conic combinations of the $B^j$ 's, i.e. $K^*\supset\text{cone}(B^1,\dots, B^q)$ . However, I am struggling with the other inclusion. I know that I haven't used the fact that $K^* = \{y\in\mathbb{R}^n\text{ }|\text{ }Ay\geq\textbf{0}\}$ , but I don't see how to use this. The solution is supposed to be pretty straightforward, but unfortunately I don't see it. Would someone be able to help me see this?","Let be a polyhedral cone generated by the rows of a matrix , and constrained by the columns of a matrix , such that , where denotes the -th row vector of . Additionally, suppose the matrix is non-negative. We have that the dual cone . I want to show that it is also true that , where denotes the -th column vector of . I can see that the ' ' inclusion is true, because each is in (since , and each by hypothesis, so we can extend linearly to cone ), so then again by linearity I can extend to all conic combinations of the 's, i.e. . However, I am struggling with the other inclusion. I know that I haven't used the fact that , but I don't see how to use this. The solution is supposed to be pretty straightforward, but unfortunately I don't see it. Would someone be able to help me see this?","K A\in\mathbb{R}^{p\times n} B\in\mathbb{R}^{n\times q} K = \text{cone}(A_1^T,\dots, A_p^T) = \{x\in\mathbb{R}^n\text{ }|\text{ }x^TB\geq\textbf{0}^T\} A_i i A AB\in\mathbb{R}^{p\times q} K^* := \{y\in\mathbb{R}^n\text{ }|\text{ }x^Ty\geq 0\text{ }\forall x\in K\} = \{y\in\mathbb{R}^n\text{ }|\text{ }Ay\geq\textbf{0}\} K^* = \text{cone}(B^1,\dots, B^q) B^j j B \supset B^j K^* K = \{x\in\mathbb{R}^n\text{ }|\text{ }x^TB\geq\textbf{0}^T\} A_i^TB^j = (AB)_{ij}\geq 0 (A_1^T,\dots A_p^T) = K B^j K^*\supset\text{cone}(B^1,\dots, B^q) K^* = \{y\in\mathbb{R}^n\text{ }|\text{ }Ay\geq\textbf{0}\}","['linear-algebra', 'matrices', 'convex-analysis', 'polyhedra']"
40,Minimum eigenvalue of a graph,Minimum eigenvalue of a graph,,"Let $G$ be a graph on vertices $\{1,2,\ldots ,n\}$ . Suppose the vertex $1$ is connected to all other vertices. Let $\lambda$ be the least eigen value of the graph (i.e., its adjacency matrix). Is it true that $\lambda\geq -1$ ? Or is there any kind of lower bound of $\lambda$ for such graphs? Any comment/reference would be very helpful. Thank you.","Let be a graph on vertices . Suppose the vertex is connected to all other vertices. Let be the least eigen value of the graph (i.e., its adjacency matrix). Is it true that ? Or is there any kind of lower bound of for such graphs? Any comment/reference would be very helpful. Thank you.","G \{1,2,\ldots ,n\} 1 \lambda \lambda\geq -1 \lambda","['matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'adjacency-matrix']"
41,Proof verification: If $A \in \mathcal{M}_n$ commutes with every $B$ then $A = \lambda I$,Proof verification: If  commutes with every  then,A \in \mathcal{M}_n B A = \lambda I,"I'm doing an exercise where I have a similar statement concerning linear transformations. More specifically: $\forall \sigma \in \mathcal{L} (V) (\tau \sigma = \sigma \tau) \implies \tau = a \iota$ , where $\iota$ is the identity operator. Since every linear transformation with respect to a basis is equivalent to a matrix, I thought I could instead prove the statement in the title, that is, a square matrix which commutes with every other square matrix of same size must be a scalar multiple of $I$ . I tried constructing matrices to assist me in the proof, so I wanted to be sure if the proof is actually valid. The proof starts below: Let $M_c (l, k) \in \mathcal{M}_n (F)$ , where $n \geq 2$ be a matrix where $m_{l, l} = c$ , $m_{l, k} = -c$ , and for any other index $m_{i, j} = 0$ . Also, $l \neq k$ and $c \neq 0$ . Summarizing, the $n$ th entry in the main diagonal has value $c$ , and another entry in the same row has value $-c$ , and all other entries have value $0$ . We intend to prove that any matrix $A \in \mathcal{M}_n$ which commutes with $M_a (l, k)$ for every $1 \leq l, k \leq n$ is a diagonal matrix. For the sake of readability, the matrix will be written only as $M$ . Since both commute, for any $l$ and $k$ , we have: \begin{equation} 			\begin{split} 				[B M]_{l, l} = \sum^n_{i = 1} b_{l, i} m_{i, l} = b_{l, l} m_{l, l} = c b_{l, l} \\ 				= [M B]_{l, l} = \sum^n_{i = 1} m_{l, i} b_{i, l} = b_{l, l} m_{l, l} + b_{k, l} m_{l, k} = c b_{l, l} + (- c b_{k, l}) \; . \end{split} \end{equation} We have that $- c b_{k, l} = 0$ . Since $a \neq 0$ , the same holds for its additive inverse. This implies that $b_{k, l} = 0$ for arbitrary $k$ and $l$ where $k \neq l$ . We can choose every $k$ and $l$ between $1$ and $n$ and arrive at the same conclusion for these indexes, so we have that $b_{i, j} = 0$ for $i \neq j$ , therefore $B$ must be a diagonal matrix, with all entries outside the main diagonal equal to $0$ . Now, we pick some matrix $C \in \mathcal{M}_n$ where $c_{l, k} \neq 0$ for $l \neq k$ . Since $A$ commutes with $C$ : \begin{equation} \begin{split} 		[A C]_{l, k} = \sum^n_{i = 1} a_{l, i} c_{i, k} = a_{l, l} c_{l, k} \\ 		= [C A]_{k, j} = \sum^n_{i = 1} c_{l, i} a_{i, k} = a_{k, k} c_{l, k} \; . \end{split} \end{equation} Since $c_{l, k} \neq 0$ , this implies any two entries in the diagonal are equal, and since $l$ and $k$ are arbitrary, this applies for all entries. Since $A$ is also diagonal, we conclude that $A$ must be the multiple of an identity matrix. I just wanted to be sure that I didn't make any mistakes and that these conclusions are enough to prove the statement. Is the proof correct? Thanks in advance!","I'm doing an exercise where I have a similar statement concerning linear transformations. More specifically: , where is the identity operator. Since every linear transformation with respect to a basis is equivalent to a matrix, I thought I could instead prove the statement in the title, that is, a square matrix which commutes with every other square matrix of same size must be a scalar multiple of . I tried constructing matrices to assist me in the proof, so I wanted to be sure if the proof is actually valid. The proof starts below: Let , where be a matrix where , , and for any other index . Also, and . Summarizing, the th entry in the main diagonal has value , and another entry in the same row has value , and all other entries have value . We intend to prove that any matrix which commutes with for every is a diagonal matrix. For the sake of readability, the matrix will be written only as . Since both commute, for any and , we have: We have that . Since , the same holds for its additive inverse. This implies that for arbitrary and where . We can choose every and between and and arrive at the same conclusion for these indexes, so we have that for , therefore must be a diagonal matrix, with all entries outside the main diagonal equal to . Now, we pick some matrix where for . Since commutes with : Since , this implies any two entries in the diagonal are equal, and since and are arbitrary, this applies for all entries. Since is also diagonal, we conclude that must be the multiple of an identity matrix. I just wanted to be sure that I didn't make any mistakes and that these conclusions are enough to prove the statement. Is the proof correct? Thanks in advance!","\forall \sigma \in \mathcal{L} (V) (\tau \sigma = \sigma \tau) \implies \tau = a \iota \iota I M_c (l, k) \in \mathcal{M}_n (F) n \geq 2 m_{l, l} = c m_{l, k} = -c m_{i, j} = 0 l \neq k c \neq 0 n c -c 0 A \in \mathcal{M}_n M_a (l, k) 1 \leq l, k \leq n M l k \begin{equation}
			\begin{split}
				[B M]_{l, l} = \sum^n_{i = 1} b_{l, i} m_{i, l} = b_{l, l} m_{l, l} = c b_{l, l} \\
				= [M B]_{l, l} = \sum^n_{i = 1} m_{l, i} b_{i, l} = b_{l, l} m_{l, l} + b_{k, l} m_{l, k} = c b_{l, l} + (- c b_{k, l}) \; .
\end{split}
\end{equation} - c b_{k, l} = 0 a \neq 0 b_{k, l} = 0 k l k \neq l k l 1 n b_{i, j} = 0 i \neq j B 0 C \in \mathcal{M}_n c_{l, k} \neq 0 l \neq k A C \begin{equation}
\begin{split}
		[A C]_{l, k} = \sum^n_{i = 1} a_{l, i} c_{i, k} = a_{l, l} c_{l, k} \\
		= [C A]_{k, j} = \sum^n_{i = 1} c_{l, i} a_{i, k} = a_{k, k} c_{l, k} \; .
\end{split}
\end{equation} c_{l, k} \neq 0 l k A A","['linear-algebra', 'matrices', 'solution-verification']"
42,Does this linear subspace of matrices contain an invertible matrix?,Does this linear subspace of matrices contain an invertible matrix?,,"Let $\mathrm{M}_n(\mathbb{C})$ denote the space of $n\times n$ complex matrices, let $\mathcal{A}\subset\mathrm{M}_n(\mathbb{C})$ be any nonempty subset of matrices, and consider the set of matrices $$ \mathcal{A}^*\mathcal{A} = \{A^*B\, :\, A,B\in\mathcal{A}\}. $$ Suppose that $\mathcal{A}^*\mathcal{A}$ is a family of commuting matrices and suppose further that there exist matrices $A_1,\dots,A_N\in\mathcal{A}$ such that $A_1^*A_1+\cdots+A_N^*A_N=I$ where $I$ is the $n\times n$ identity matrix. Question : Is it necessarily the case that $\mathrm{span}(\mathcal{A})$ contains an invertible matrix? Here are some of my thoughts: One may suppose without loss of generality that $\mathcal{A}=\mathrm{span}(\mathcal{A})$ (i.e., $\mathcal{A}$ is a linear subspace of matrices), since $$ \mathrm{span}(\mathcal{A}^*\mathcal{A}) = \mathrm{span}\bigl((\mathrm{span}(\mathcal{A}))^*(\mathrm{span}(\mathcal{A})\bigr). $$ ( Edit : Note that each matrix in $\mathcal{A}^*\mathcal{A}$ is normal, since $A^*B\in\mathcal{A}^*\mathcal{A}$ implies $(A^*B)^*=B^*A\in\mathcal{A}^*\mathcal{A}$ and these matrices must commute.) Since $\mathcal{A}^*\mathcal{A}$ is a family of normal commuting matrices, there exists a unitary matrix $V$ such that $V^*A^*BV$ is a diagonal matrix for each $A,B\in\mathcal{A}$ . We may write each of the matrices $A_1,\dots,A_N$ in their polar decomposition as $$ A_i = U_i P_i $$ for some unitary matrices $U_1,\dots,U_N$ and positive semidefinite matrices $P_1,\dots,P_N$ . Now the matrix $V^*A_i^*A_iV=V^*P_i^2V$ is diagonal for each $i$ and thus $V^*P_iV$ is diagonal for each $i$ . One has that $$ (V^*P_1V)^2+ \cdots + (V^*P_NV)^2 = V^*(P_1^2+\cdots+P_N^2)V = V^*(A_1^*A_1+\cdots+A_N^*A_N)V=V^*V = I. $$ In particular, it follows that $P_1^2 + \cdots + P_N^2=I$ .  Since each of the matrices $V^*P_iV$ is diagonal and positive, we have that $$ V^*\bigl(\sum_{i=1}^NP_i^2\bigr)V  = I \quad\Rightarrow\quad V^*\bigl(\sum_{i=1}^NP_i\bigr)V >0 $$ hence $\sum_{i=1}^NP_i$ is positive definite and thus invertible. But this is not quite what I want because it is not in $\mathcal{A}$ ......","Let denote the space of complex matrices, let be any nonempty subset of matrices, and consider the set of matrices Suppose that is a family of commuting matrices and suppose further that there exist matrices such that where is the identity matrix. Question : Is it necessarily the case that contains an invertible matrix? Here are some of my thoughts: One may suppose without loss of generality that (i.e., is a linear subspace of matrices), since ( Edit : Note that each matrix in is normal, since implies and these matrices must commute.) Since is a family of normal commuting matrices, there exists a unitary matrix such that is a diagonal matrix for each . We may write each of the matrices in their polar decomposition as for some unitary matrices and positive semidefinite matrices . Now the matrix is diagonal for each and thus is diagonal for each . One has that In particular, it follows that .  Since each of the matrices is diagonal and positive, we have that hence is positive definite and thus invertible. But this is not quite what I want because it is not in ......","\mathrm{M}_n(\mathbb{C}) n\times n \mathcal{A}\subset\mathrm{M}_n(\mathbb{C}) 
\mathcal{A}^*\mathcal{A} = \{A^*B\, :\, A,B\in\mathcal{A}\}.
 \mathcal{A}^*\mathcal{A} A_1,\dots,A_N\in\mathcal{A} A_1^*A_1+\cdots+A_N^*A_N=I I n\times n \mathrm{span}(\mathcal{A}) \mathcal{A}=\mathrm{span}(\mathcal{A}) \mathcal{A} 
\mathrm{span}(\mathcal{A}^*\mathcal{A}) = \mathrm{span}\bigl((\mathrm{span}(\mathcal{A}))^*(\mathrm{span}(\mathcal{A})\bigr).
 \mathcal{A}^*\mathcal{A} A^*B\in\mathcal{A}^*\mathcal{A} (A^*B)^*=B^*A\in\mathcal{A}^*\mathcal{A} \mathcal{A}^*\mathcal{A} V V^*A^*BV A,B\in\mathcal{A} A_1,\dots,A_N 
A_i = U_i P_i
 U_1,\dots,U_N P_1,\dots,P_N V^*A_i^*A_iV=V^*P_i^2V i V^*P_iV i 
(V^*P_1V)^2+ \cdots + (V^*P_NV)^2 = V^*(P_1^2+\cdots+P_N^2)V = V^*(A_1^*A_1+\cdots+A_N^*A_N)V=V^*V = I.
 P_1^2 + \cdots + P_N^2=I V^*P_iV 
V^*\bigl(\sum_{i=1}^NP_i^2\bigr)V  = I \quad\Rightarrow\quad V^*\bigl(\sum_{i=1}^NP_i\bigr)V >0
 \sum_{i=1}^NP_i \mathcal{A}","['linear-algebra', 'matrices', 'diagonalization']"
43,"Let $A,B$ be squared matrices. Given $A=I-AB$, Prove: $B^3=0$ if and only if $A=I-B+B^2$","Let  be squared matrices. Given , Prove:  if and only if","A,B A=I-AB B^3=0 A=I-B+B^2","Let $A,B$ be squared matrices.    Given $A=I-AB$ , Prove: $B^3=0  \iff A=I-B+B^2$ The question has 3 sections: Given $A=I-AB$ , Prove that $A$ is invertible and that $BA=AB$ . Prove that if $B$ is a symmetric matrix, then $A$ is symmetric Prove: $A=I-B+B^2$ if and only if $B^3=0$ . I proved the first two sections, and did the first direction of the third section, Thus we suppose that $A=I-B+B^2$ , and I proved that $B^3=0$ as follows: Suppose $A=I-B+B^2$ , then $A=I-AB=I-B+B^2$ $$-AB=-B+B^2$$ $$\text{ we'll substitute $A$ by $I-B+B^2$}$$ $$-(I-B+B^2)B= -B+B^2$$ $$-B+B^2-B^3 =-B+B^2 \Longrightarrow B^3 =0$$ As wished. Now let $B^3=0$ . Prove: $A=I-B+B^2$ . $$AB = I-A$$ We'll multiply by $B$ on the left side: $$AB^2 = B-AB$$ $$AB^2+AB=B$$ $$AB(B+I)=B$$ and from this point I'm stuck.","Let be squared matrices.    Given , Prove: The question has 3 sections: Given , Prove that is invertible and that . Prove that if is a symmetric matrix, then is symmetric Prove: if and only if . I proved the first two sections, and did the first direction of the third section, Thus we suppose that , and I proved that as follows: Suppose , then As wished. Now let . Prove: . We'll multiply by on the left side: and from this point I'm stuck.","A,B A=I-AB B^3=0  \iff A=I-B+B^2 A=I-AB A BA=AB B A A=I-B+B^2 B^3=0 A=I-B+B^2 B^3=0 A=I-B+B^2 A=I-AB=I-B+B^2 -AB=-B+B^2 \text{ we'll substitute A by I-B+B^2} -(I-B+B^2)B= -B+B^2 -B+B^2-B^3 =-B+B^2 \Longrightarrow B^3 =0 B^3=0 A=I-B+B^2 AB = I-A B AB^2 = B-AB AB^2+AB=B AB(B+I)=B","['linear-algebra', 'matrices']"
44,Subgroups of $\text{SL}_2(p)$ isomorphic to $C_3 \rtimes C_4$,Subgroups of  isomorphic to,\text{SL}_2(p) C_3 \rtimes C_4,"Let $p$ be a prime number. I am interested in classifying all subgroups $H \subset \text{SL}_2(p)$ such that $H \cong C_3 \rtimes C_4$ , where $C_3$ and $C_4$ denote the cyclic groups of order 3 and 4 respectively. I hope that all such subgroups are conjugate to each other, but so far I haven't had any success with showing this. Frankly, I do not yet know for sure whether or not this is true, but I believe it to be true for $p = 5$ and $p = 7$ , which I hope is not just a coincidence. To be even more precise, I would hope that $N_G(H) / Z(G) \cong \text{Aut}(H)$ , where $G=\text{GL}_2(p)$ and $N_G(H)$ is the normaliser of $H$ in $G$ and $Z(G)$ is the center of $G$ . I have already proved that the centraliser of $H$ is given precisely by the center of $G$ . The proof used the fact that $H$ is not abelian and that we are working with $2 \times 2$ matrices. The group $H$ has an element of order 6, so I figured a good start would be to show that all elements in $\text{SL}_2(p)$ that have order 6 are conjugate to each other (which again seems to be true for small primes), but I have not yet succeeded in doing this either. Assuming this for a moment, a natural choice for this element of order 6 would be the matrix $ a = \begin{bmatrix} 0 & 1 \\ -1 & 1 \end{bmatrix} $ and the second generator of the group must then satisfy $b^2 = a^3 = -1$ and $ba = a^{-1}b$ . It is not hard to see that it would then follow that $ b = \begin{bmatrix} x & z-x \\ z & -x \end{bmatrix}$ where $x^2+z^2 = xz - 1$ . Perhaps one could use this to prove the statement, but I'm not sure. Does anyone have any ideas? Or would anyone know any sources in which the subgroup structure of $\text{SL}_2(p)$ or $\text{GL}_2(p)$ is treated? Thanks in advance.","Let be a prime number. I am interested in classifying all subgroups such that , where and denote the cyclic groups of order 3 and 4 respectively. I hope that all such subgroups are conjugate to each other, but so far I haven't had any success with showing this. Frankly, I do not yet know for sure whether or not this is true, but I believe it to be true for and , which I hope is not just a coincidence. To be even more precise, I would hope that , where and is the normaliser of in and is the center of . I have already proved that the centraliser of is given precisely by the center of . The proof used the fact that is not abelian and that we are working with matrices. The group has an element of order 6, so I figured a good start would be to show that all elements in that have order 6 are conjugate to each other (which again seems to be true for small primes), but I have not yet succeeded in doing this either. Assuming this for a moment, a natural choice for this element of order 6 would be the matrix and the second generator of the group must then satisfy and . It is not hard to see that it would then follow that where . Perhaps one could use this to prove the statement, but I'm not sure. Does anyone have any ideas? Or would anyone know any sources in which the subgroup structure of or is treated? Thanks in advance.","p H \subset \text{SL}_2(p) H \cong C_3 \rtimes C_4 C_3 C_4 p = 5 p = 7 N_G(H) / Z(G) \cong \text{Aut}(H) G=\text{GL}_2(p) N_G(H) H G Z(G) G H G H 2 \times 2 H \text{SL}_2(p) 
a = \begin{bmatrix} 0 & 1 \\ -1 & 1 \end{bmatrix}
 b^2 = a^3 = -1 ba = a^{-1}b  b = \begin{bmatrix} x & z-x \\ z & -x \end{bmatrix} x^2+z^2 = xz - 1 \text{SL}_2(p) \text{GL}_2(p)","['matrices', 'group-theory', 'group-isomorphism', 'semidirect-product']"
45,Derivative of quadratic with Hadamard product,Derivative of quadratic with Hadamard product,,"I proposed a similar question involving logarithms , but the problem is about scalar. I am trying to solve the more generalized form: $$ \min_{\mathbf{x} \in \mathbb{R}^N_+} \left( \sum_i \left( h_i^T(\mathbf{x}\circ\mathbf{x}) - \mathbf{c_1}\log h_i^T(\mathbf{x}\circ\mathbf{x})\right) + r_1\parallel \mathbf{x}  - \mathbf{c_2}\parallel_2^2 \right)$$ where $\mathbf{c_1} \in \mathbb{R}^+$ , $\mathbf{c_2} \in \mathbb{R}^N_+$ , $h_i \in \mathbb{R}^{N}_+$ is each column of a known matrix, $r_1 \in \mathbb{R}^+$ . All are constants. So, for scalar quadratic eqation, we can find the square root via dividing $(1+r_1)$ , but when $1$ is a matrix $H$ , how to deal with this form? But I don't know whether directly extension of scalar quadratic equation is right. Can anyone help me? Thanks in advance! Edit: formulate the problem to make it clearly.","I proposed a similar question involving logarithms , but the problem is about scalar. I am trying to solve the more generalized form: where , , is each column of a known matrix, . All are constants. So, for scalar quadratic eqation, we can find the square root via dividing , but when is a matrix , how to deal with this form? But I don't know whether directly extension of scalar quadratic equation is right. Can anyone help me? Thanks in advance! Edit: formulate the problem to make it clearly.", \min_{\mathbf{x} \in \mathbb{R}^N_+} \left( \sum_i \left( h_i^T(\mathbf{x}\circ\mathbf{x}) - \mathbf{c_1}\log h_i^T(\mathbf{x}\circ\mathbf{x})\right) + r_1\parallel \mathbf{x}  - \mathbf{c_2}\parallel_2^2 \right) \mathbf{c_1} \in \mathbb{R}^+ \mathbf{c_2} \in \mathbb{R}^N_+ h_i \in \mathbb{R}^{N}_+ r_1 \in \mathbb{R}^+ (1+r_1) 1 H,"['matrices', 'optimization', 'matrix-equations', 'matrix-calculus', 'hadamard-product']"
46,Adjoint of Sum = Sum of Adjoints?,Adjoint of Sum = Sum of Adjoints?,,"Didnt find this anywhere, just verifying. We know that: $$ (A+B)^T= A^T+B^T $$ . Does it follow that: $$ (A+B)^† = A^† +B ^† $$ for all A and B matrices (the dagger here representing the adjoint)? If so what is the proof?","Didnt find this anywhere, just verifying. We know that: . Does it follow that: for all A and B matrices (the dagger here representing the adjoint)? If so what is the proof?"," (A+B)^T= A^T+B^T   (A+B)^† = A^† +B
^† ","['matrices', 'transpose']"
47,The N(I+V) must be bigger than 4 in complex matrices?,The N(I+V) must be bigger than 4 in complex matrices?,,"In this problem, all matrices are $n*n$ with complex entries. Let $U$ and $V$ be matrices such that $UV \not = VU$ . Assume that $U$ is diagonalizable and commutes with $VUV^{-1}$ (a).For $\lambda , \mu \in \mathbb C$ , let $E_{\lambda , \mu}=\{x \in \mathbb C^n|Ux= \lambda x ,VUV^{-1}x=\mu x \}.$ Show that there exist couples $(\lambda_1,\mu_1) \not = (\lambda_2,\mu_2)$ ,satisfying $\lambda_i \not = \mu_i$ and $E_{\lambda_i , \mu_i} \not = 0$ for $ i = 1,2.$ (b).For a matrix $A$ , we define $N(A):=tr(A^*A),$ where $A^*$ is the conjugate transpose of $A$ . Assume that $U$ and $V$ are unitary (namely, $U^*U=V^*V$ is the identity matrix).Deduce that $N(1+V) \ge 4$ .","In this problem, all matrices are with complex entries. Let and be matrices such that . Assume that is diagonalizable and commutes with (a).For , let Show that there exist couples ,satisfying and for (b).For a matrix , we define where is the conjugate transpose of . Assume that and are unitary (namely, is the identity matrix).Deduce that .","n*n U V UV \not = VU U VUV^{-1} \lambda , \mu \in \mathbb C E_{\lambda , \mu}=\{x \in \mathbb C^n|Ux= \lambda x ,VUV^{-1}x=\mu x \}. (\lambda_1,\mu_1) \not = (\lambda_2,\mu_2) \lambda_i \not = \mu_i E_{\lambda_i , \mu_i} \not = 0  i = 1,2. A N(A):=tr(A^*A), A^* A U V U^*U=V^*V N(1+V) \ge 4","['linear-algebra', 'matrices']"
48,Show that $(M_n(R))[X]\cong M_n(R[X])$,Show that,(M_n(R))[X]\cong M_n(R[X]),"Let R be a ring, show that $(M_n(R))[x]\cong M_n(R[x])$ . Given a matrix $A\in M_n(R[X])$ , $(A)_{ij}=\sum_0^nb_k^{ij}x^k$ (where $b_k^{ij}$ is the  coefficent of the k'th power on the polynomial). If I define $A_k\in M_n(R)$ such that $(A_k)_{ij}=b_k^{ij}$ , and let $n>0$ such that we define $(X^n)_{ij}=\begin{cases}0&\text{if } i\neq j\\x^n&\text{if }i=j \end{cases}$ . Then $(A_kX^k)_{ij}=\sum_{r=0}^n(A_k)_{ir}(X^k)_{rj}=\sum_{r=0}^nb_k^{ir}(X^k)_{rj}=b_k^{ij}x^k$ . Therefore $(A)_{ij}=\sum_{k=0}^n(A_kX^k)_{ij}$ which implies $A=\sum_{k=0}^nA_kX^k$ .(In the case $k=0$ $A_0$ is well defined so there's not trouble there) I have two questions: 1) how do I show that the $X^k$ I defined is the one in the expansion of a polynomial in $(M_n(R))[x]$ . 2) If the above holds is this injection well defined? I can't seem to prove it. Edit: as the comment below says, $R$ may not contain a multiplicative identity.","Let R be a ring, show that . Given a matrix , (where is the  coefficent of the k'th power on the polynomial). If I define such that , and let such that we define . Then . Therefore which implies .(In the case is well defined so there's not trouble there) I have two questions: 1) how do I show that the I defined is the one in the expansion of a polynomial in . 2) If the above holds is this injection well defined? I can't seem to prove it. Edit: as the comment below says, may not contain a multiplicative identity.",(M_n(R))[x]\cong M_n(R[x]) A\in M_n(R[X]) (A)_{ij}=\sum_0^nb_k^{ij}x^k b_k^{ij} A_k\in M_n(R) (A_k)_{ij}=b_k^{ij} n>0 (X^n)_{ij}=\begin{cases}0&\text{if } i\neq j\\x^n&\text{if }i=j \end{cases} (A_kX^k)_{ij}=\sum_{r=0}^n(A_k)_{ir}(X^k)_{rj}=\sum_{r=0}^nb_k^{ir}(X^k)_{rj}=b_k^{ij}x^k (A)_{ij}=\sum_{k=0}^n(A_kX^k)_{ij} A=\sum_{k=0}^nA_kX^k k=0 A_0 X^k (M_n(R))[x] R,"['matrices', 'ring-theory', 'polynomial-rings']"
49,History of the Cauchy matrix,History of the Cauchy matrix,,"Cauchy matrix $C$ is defined by $$C_{i,j}=\frac{1}{a_i + b_j}$$ where $a_i$ and $b_j$ are any numbers so long $a_i + b_j \neq 0$ . Why did Cauchy introduce this matrix? Did he use it in the context of another problem or application?",Cauchy matrix is defined by where and are any numbers so long . Why did Cauchy introduce this matrix? Did he use it in the context of another problem or application?,"C C_{i,j}=\frac{1}{a_i + b_j} a_i b_j a_i + b_j \neq 0","['matrices', 'math-history', 'cauchy-matrices']"
50,Is my understanding of matrices correct?,Is my understanding of matrices correct?,,"Let $V = R^2$ be our vector space with the unit base vectors $J(1, 0), K(0, 1)$ . We have the linear map, $$T: V \to V$$ We can rewrite $\forall v \in V$ as a linear combination of the unit base vectors (by their very definition) and utilize the linearity of our map: $$v = xJ + yK = \begin{bmatrix}x \\ y\end{bmatrix}$$ $$T(v) = T(xJ + yK) = xT(J) +yT(K)$$ What does this mean? This means that all linear transformations can be uniquely described by the transformed unit base vectors $T_J, T_K$ . So we can notate our map in the matrix form (some kind of vector consisting of column vectors), $$T = \begin{bmatrix}T_J\\T_K\end{bmatrix}$$ And we can define matrix-vector multiplication as the dot product of them: $$\begin{bmatrix}T_J\\T_K\end{bmatrix} \cdot \begin{bmatrix}x\\y\end{bmatrix} = xT_J + yT_K = T(v).$$ But if we directly notate the $T_J, T_K$ as vectors rather than hiding them behind letters, it would be more convenient to write our matrix in row form: $$T = \begin{bmatrix}T_J \space T_K\end{bmatrix} = \begin{bmatrix} \begin{bmatrix}X_0 \\ X_1\end{bmatrix} \begin{bmatrix}Y_0 \\ Y_1\end{bmatrix}\end{bmatrix} \Rightarrow \begin{bmatrix}X_0 & Y_0 \\ X_1 & Y_1\end{bmatrix}$$ Also this form is more useful for representing our linear transformation / matrix as a set of linear equations. Is my understanding correct?","Let be our vector space with the unit base vectors . We have the linear map, We can rewrite as a linear combination of the unit base vectors (by their very definition) and utilize the linearity of our map: What does this mean? This means that all linear transformations can be uniquely described by the transformed unit base vectors . So we can notate our map in the matrix form (some kind of vector consisting of column vectors), And we can define matrix-vector multiplication as the dot product of them: But if we directly notate the as vectors rather than hiding them behind letters, it would be more convenient to write our matrix in row form: Also this form is more useful for representing our linear transformation / matrix as a set of linear equations. Is my understanding correct?","V = R^2 J(1, 0), K(0, 1) T: V \to V \forall v \in V v = xJ + yK = \begin{bmatrix}x \\ y\end{bmatrix} T(v) = T(xJ + yK) = xT(J) +yT(K) T_J, T_K T = \begin{bmatrix}T_J\\T_K\end{bmatrix} \begin{bmatrix}T_J\\T_K\end{bmatrix} \cdot \begin{bmatrix}x\\y\end{bmatrix} = xT_J + yT_K = T(v). T_J, T_K T = \begin{bmatrix}T_J \space T_K\end{bmatrix} = \begin{bmatrix} \begin{bmatrix}X_0 \\ X_1\end{bmatrix} \begin{bmatrix}Y_0 \\ Y_1\end{bmatrix}\end{bmatrix} \Rightarrow \begin{bmatrix}X_0 & Y_0 \\ X_1 & Y_1\end{bmatrix}","['linear-algebra', 'matrices', 'linear-transformations', 'intuition']"
51,How do I derive the following expression for the sum of orthogonal matrices?,How do I derive the following expression for the sum of orthogonal matrices?,,"In Johansen's book 'Likelihood-based inference in cointegrated vector autoregressive models', in order to get the expression for the Granger's representation theorem he claims that: $$\beta_\bot(\alpha'_\bot \beta_\bot )^{-1} \alpha'_\bot + \alpha (\beta' \alpha)^{-1} \beta'  = I \tag{1}$$ Where: $\alpha$ is $N\times R$ , $\text{rank}(\alpha) =R$ $\beta$ is $N\times R$ , $\text{rank}(\beta) =R$ $\beta_\bot $ is $N\times (N-R)$ , $\text{rank}(\beta_\bot) =N-R$ $\alpha_\bot $ is $N\times (N-R)$ , $\text{rank}(\alpha_\bot) =N-R$ $\alpha' \alpha_\bot =0$ $\beta' \beta_\bot =0$ I am not able to prove (1). Can you help me, please?","In Johansen's book 'Likelihood-based inference in cointegrated vector autoregressive models', in order to get the expression for the Granger's representation theorem he claims that: Where: is , is , is , is , I am not able to prove (1). Can you help me, please?",\beta_\bot(\alpha'_\bot \beta_\bot )^{-1} \alpha'_\bot + \alpha (\beta' \alpha)^{-1} \beta'  = I \tag{1} \alpha N\times R \text{rank}(\alpha) =R \beta N\times R \text{rank}(\beta) =R \beta_\bot  N\times (N-R) \text{rank}(\beta_\bot) =N-R \alpha_\bot  N\times (N-R) \text{rank}(\alpha_\bot) =N-R \alpha' \alpha_\bot =0 \beta' \beta_\bot =0,"['linear-algebra', 'matrices', 'vector-spaces', 'orthogonal-matrices', 'vector-auto-regression']"
52,"Pivots, determinant and eigenvalues","Pivots, determinant and eigenvalues",,"In symmetric matrices, Product of pivots = determinant of that matrix Determinant of the matrix = Product of eigenvalues Therefore the product of eigenvalues = product of pivots. Do any or all of the above apply to matrices that are not symmetric?","In symmetric matrices, Product of pivots = determinant of that matrix Determinant of the matrix = Product of eigenvalues Therefore the product of eigenvalues = product of pivots. Do any or all of the above apply to matrices that are not symmetric?",,['matrices']
53,finding a solution to a matrix inequality,finding a solution to a matrix inequality,,"I would like to find a 19x19 matrix V such that the following inequality holds: $$V^TAV<K$$ and where all entries of V are positive and the sum of entires in a row of V are equal to 1. Also < is taken point wise and K and A are particular matrices specified by the problem. In particular, the ij th element of A is sign(j-i) where sign(x)=|x|/x for x≠0 and 0 for x=0. K has the following property: The diagonal of K is all 0's and each other entry of K is either a 1 or e where 0 < e << 1. Also Kij = 1 iff Kji = e. Although for my specific problem there are some additional properties satisfied by A and K, I am interested in more general approaches to solving problems of this form. So far, the best idea for solving this is to use gradient descent. In particular I was considering picking a random matrix V, then computing Q as follows: $$Q=V^TAV-K$$ Then let $$err = ∑_{i,j} max(Qij,0) $$ and then I would try to minimize err via gradient descent","I would like to find a 19x19 matrix V such that the following inequality holds: and where all entries of V are positive and the sum of entires in a row of V are equal to 1. Also < is taken point wise and K and A are particular matrices specified by the problem. In particular, the ij th element of A is sign(j-i) where sign(x)=|x|/x for x≠0 and 0 for x=0. K has the following property: The diagonal of K is all 0's and each other entry of K is either a 1 or e where 0 < e << 1. Also Kij = 1 iff Kji = e. Although for my specific problem there are some additional properties satisfied by A and K, I am interested in more general approaches to solving problems of this form. So far, the best idea for solving this is to use gradient descent. In particular I was considering picking a random matrix V, then computing Q as follows: Then let and then I would try to minimize err via gradient descent","V^TAV<K Q=V^TAV-K err = ∑_{i,j} max(Qij,0) ","['matrices', 'numerical-optimization', 'gradient-descent']"
54,What can we say about $Q$ given $Q=P+\frac{1}{2}I$ where $P \in M_n \left(\mathbb{Z}\right)$,What can we say about  given  where,Q Q=P+\frac{1}{2}I P \in M_n \left(\mathbb{Z}\right),"Let $P$ be a $n\times n$ matrix with integral entries and $Q=P+\frac{1}{2}I$ , where $I$ denotes the $n\times n$ identity matrix. Then what can you say about $Q$ . Is it Idempotent? Is it Invertible? Is it Nilpotent? Is it Unipotent? According to me It is not necessary that it is Idempotent, Nilpotent, Unipotent. For a counter example take the diagonal matrix $B=\left( b_{ij}\right)$ with $b_{ii}=i$ . I claim that it is invertible. Consider the matrix $2Q=2P+I$ . Reduce each entry modulo 2. We get $I$ whose determinant is $1$ hence determinant of $2Q$ must be odd. Hence $2Q$ is invertible and hence $Q$ is invertible. Is this method correct? Do you have any other better method?","Let be a matrix with integral entries and , where denotes the identity matrix. Then what can you say about . Is it Idempotent? Is it Invertible? Is it Nilpotent? Is it Unipotent? According to me It is not necessary that it is Idempotent, Nilpotent, Unipotent. For a counter example take the diagonal matrix with . I claim that it is invertible. Consider the matrix . Reduce each entry modulo 2. We get whose determinant is hence determinant of must be odd. Hence is invertible and hence is invertible. Is this method correct? Do you have any other better method?",P n\times n Q=P+\frac{1}{2}I I n\times n Q B=\left( b_{ij}\right) b_{ii}=i 2Q=2P+I I 1 2Q 2Q Q,"['linear-algebra', 'matrices', 'block-matrices']"
55,On matrices with zero von Neumann entropy,On matrices with zero von Neumann entropy,,"I was indecisive about whether to post this problem in the Physics forum or in the Mathematics one. However, since I am mostly interested in the mathematical understanding of it, I am posting it here. Suppose I have a matrix $A$ subject to the conditions (for its trace): $$\mbox{Tr}(A) = 1$$ $$A^2 = A$$ $$A \succeq 0 $$ $$A^H = A$$ Now from this conditions I need to prove the following: $$-\mbox{Tr} (A \ln{A}) = 0$$ In physics, $A$ can be seen as a pure density matrix and $-\mbox{Tr}(A\ln{A})$ is the von Neumann entropy. How does that follow from the first four conditions?","I was indecisive about whether to post this problem in the Physics forum or in the Mathematics one. However, since I am mostly interested in the mathematical understanding of it, I am posting it here. Suppose I have a matrix subject to the conditions (for its trace): Now from this conditions I need to prove the following: In physics, can be seen as a pure density matrix and is the von Neumann entropy. How does that follow from the first four conditions?",A \mbox{Tr}(A) = 1 A^2 = A A \succeq 0  A^H = A -\mbox{Tr} (A \ln{A}) = 0 A -\mbox{Tr}(A\ln{A}),"['linear-algebra', 'matrices']"
56,"The matrix $A^2+A+m.I_n$ is non singular if $\mbox{ gcd }(m,\det\,A)=1$.",The matrix  is non singular if .,"A^2+A+m.I_n \mbox{ gcd }(m,\det\,A)=1","Let $A\in M_{n}(\Bbb{Z})$ , $m>1$ such that $\mbox{ gcd }(m,\det\,A)=1$ . Show that the matrix $A^2+A+m.I_n$ is non singular I have tried like this: Suppose $\lambda$ be any eigenvalue of $A$ . Then it is enough if we can show $\lambda^2+\lambda+m\neq0$ . But how to connect this with $\mbox{ gcd }(m,\det\,A)=1$ .","Let , such that . Show that the matrix is non singular I have tried like this: Suppose be any eigenvalue of . Then it is enough if we can show . But how to connect this with .","A\in M_{n}(\Bbb{Z}) m>1 \mbox{ gcd }(m,\det\,A)=1 A^2+A+m.I_n \lambda A \lambda^2+\lambda+m\neq0 \mbox{ gcd }(m,\det\,A)=1","['linear-algebra', 'matrices']"
57,Invertibility of a matrix in portfolio optimization,Invertibility of a matrix in portfolio optimization,,"Let $A$ be an $n\times n$ symmetric matrix with non-negative entries. Let $\mathbf{1}$ be the column vector of dimension $n$ with all entries being $1$ .  Consider the $(n+1)\times (n+1)$ matrix $$ B= \begin{bmatrix}  A & \mathbf{1} \\ \mathbf{1}^T & 0  \end{bmatrix} $$ Question: what is the condition for $A$ so that $B$ is invertible? Remark: This matrix is related to portfolio optimization problems in finance. I note that when $A$ is a constant matrix, the determinant of $B$ is $0$ and thus $B$ is not invertible.","Let be an symmetric matrix with non-negative entries. Let be the column vector of dimension with all entries being .  Consider the matrix Question: what is the condition for so that is invertible? Remark: This matrix is related to portfolio optimization problems in finance. I note that when is a constant matrix, the determinant of is and thus is not invertible.","A n\times n \mathbf{1} n 1 (n+1)\times (n+1)  B=
\begin{bmatrix} 
A & \mathbf{1} \\
\mathbf{1}^T & 0 
\end{bmatrix}
 A B A B 0 B","['linear-algebra', 'matrices', 'determinant', 'inverse', 'finance']"
58,Does $SO(n)$ lie in any $(n^2-1)$-dimensional subspace of $\mathbf R^{n^2}$?,Does  lie in any -dimensional subspace of ?,SO(n) (n^2-1) \mathbf R^{n^2},The matrix group $SO(n)$ can be treated as a submanifold of $\mathbf R^{n^2}$ . Does it lie in any $(n^2-1)$ -dimensional subspace of $\mathbf R^{n^2}$ ? For $n=2$ the answer is yes because $SO(2)$ lies in the span of the identity matrix and $\begin{pmatrix} 0 & 1\\  -1 & 0 \end{pmatrix}$ . How about for $n>2$ ? Thanks.,The matrix group can be treated as a submanifold of . Does it lie in any -dimensional subspace of ? For the answer is yes because lies in the span of the identity matrix and . How about for ? Thanks.,"SO(n) \mathbf R^{n^2} (n^2-1) \mathbf R^{n^2} n=2 SO(2) \begin{pmatrix}
0 & 1\\ 
-1 & 0
\end{pmatrix} n>2","['linear-algebra', 'matrices', 'differential-geometry', 'lie-groups']"
59,"Is there a way to describe the structure of $Aut(UT(3, p))$?",Is there a way to describe the structure of ?,"Aut(UT(3, p))","Is there a way to describe the structure of the automorphism group of $$C_{p}^2 \rtimes C_p \cong \langle x, y, z | [x,y]=z, [x,z]=[y,z]=x^p=y^p=z^p=e \rangle \cong UT(3, p)?$$ Here $p$ is an odd prime. The only thing I know about it is, that $Inn(UT(3, p)) \cong \frac{UT(3, p)}{Z(UT(3, p))} \cong C_p \times C_p$ , however $UT(3, p)$ is also very likely to have outer automorphisms, which I do not know how to describe. Also, one can see, that all inner automorphisms of $UT(3, p)$ are of the form $$\begin{pmatrix}   1 & x & y \\   0 & 1 & z\\   0 & 0 & 1   \end{pmatrix} \mapsto \begin{pmatrix}   1 & x & y + (a-c)z -ac\\   0 & 1 & z\\   0 & 0 & 1  \end{pmatrix}$$ for some $a, c \in \mathbb{F}_p$ . However, this does not help much.","Is there a way to describe the structure of the automorphism group of Here is an odd prime. The only thing I know about it is, that , however is also very likely to have outer automorphisms, which I do not know how to describe. Also, one can see, that all inner automorphisms of are of the form for some . However, this does not help much.","C_{p}^2 \rtimes C_p \cong \langle x, y, z | [x,y]=z, [x,z]=[y,z]=x^p=y^p=z^p=e \rangle \cong UT(3, p)? p Inn(UT(3, p)) \cong \frac{UT(3, p)}{Z(UT(3, p))} \cong C_p \times C_p UT(3, p) UT(3, p) \begin{pmatrix}
  1 & x & y \\
  0 & 1 & z\\
  0 & 0 & 1 
 \end{pmatrix} \mapsto \begin{pmatrix}
  1 & x & y + (a-c)z -ac\\
  0 & 1 & z\\
  0 & 0 & 1
 \end{pmatrix} a, c \in \mathbb{F}_p","['matrices', 'group-theory', 'finite-groups', 'group-presentation', 'automorphism-group']"
60,How to solve $A^{\frac 12} B A^{\frac 12} = C$ for $A$?,How to solve  for ?,A^{\frac 12} B A^{\frac 12} = C A,"Suppose that matrices $A,B,C$ are symmetric and positive definite. Then, $A$ has a unique, positive square root, which we call $A^{\frac 12}$ . If $$A^{\frac 12} B A^{\frac 12} = C$$ then can we write an expression for $A$ in terms of $B, C$ ?","Suppose that matrices are symmetric and positive definite. Then, has a unique, positive square root, which we call . If then can we write an expression for in terms of ?","A,B,C A A^{\frac 12} A^{\frac 12} B A^{\frac 12} = C A B, C","['matrices', 'matrix-equations', 'positive-definite', 'symmetric-matrices']"
61,concurrency of $n$ lines,concurrency of  lines,n,"there are $n>2$ lines of the kind $l_i : a_ix+b_iy+c_i=0, i= 1,2,3,\cdots n$ what is the condition for these n lines to be concurrent? I know that for three lines concurrency the condition is given by the determinant $\begin{vmatrix} a_1 & b_1 & c_1  \\ a_2 & b_2 & c_2 \\ a_3 & b_3 & c_3  \end{vmatrix} = 0$ but how is it best generalized for $n$ lines? do i have to write the determinant for every three line combination or is there some compact single expression for this?",there are lines of the kind what is the condition for these n lines to be concurrent? I know that for three lines concurrency the condition is given by the determinant but how is it best generalized for lines? do i have to write the determinant for every three line combination or is there some compact single expression for this?,"n>2 l_i : a_ix+b_iy+c_i=0, i= 1,2,3,\cdots n \begin{vmatrix}
a_1 & b_1 & c_1  \\
a_2 & b_2 & c_2 \\
a_3 & b_3 & c_3 
\end{vmatrix} = 0 n","['matrices', 'geometry']"
62,"Given $n^2$ different numbers to form a $n$ -degree matrix, prove that the number of possible determinants is at most $\frac{n^2!}{(n!)^2}$","Given  different numbers to form a  -degree matrix, prove that the number of possible determinants is at most",n^2 n \frac{n^2!}{(n!)^2},"Given $n^2$ different numbers from a field to form a $n$ -degree matrix, prove that the determinant can take  at most $$\frac{(n^2)!}{(n!)^2}$$ values. The number of different matrices is $(n^2)!$ .  So it suffices to prove that for every matrix, there are $(n!)^2$ matrices which have the same determinant. For a given matrix, we can arrange its columns arbitrarily, and after each column arrangement, we can perform a row arrangement to it. So under such operations, this matrix can produce $(n!)^2$ different matrices. If we want the later matrix to have the same determinant as the given one, we should confirm that the compound odevity of the column arrangement and the row arrangement to be even. Thus, only $ \text{even}\times\text{even} $ and $ \text{odd}\times\text{odd} $ can produce the same determinant. Therefore, I can only find $\frac{(n!)^2}{2}$ matrices that have the same determinant for each given matrix. So my question is where are the ""other"" $\frac{(n!)^2}{2}$ matrices?","Given different numbers from a field to form a -degree matrix, prove that the determinant can take  at most values. The number of different matrices is .  So it suffices to prove that for every matrix, there are matrices which have the same determinant. For a given matrix, we can arrange its columns arbitrarily, and after each column arrangement, we can perform a row arrangement to it. So under such operations, this matrix can produce different matrices. If we want the later matrix to have the same determinant as the given one, we should confirm that the compound odevity of the column arrangement and the row arrangement to be even. Thus, only and can produce the same determinant. Therefore, I can only find matrices that have the same determinant for each given matrix. So my question is where are the ""other"" matrices?","n^2 n \frac{(n^2)!}{(n!)^2} (n^2)! (n!)^2 (n!)^2 
\text{even}\times\text{even}
 
\text{odd}\times\text{odd}
 \frac{(n!)^2}{2} \frac{(n!)^2}{2}","['linear-algebra', 'matrices', 'determinant']"
63,Calculating only the needed part of Q of thin QR decomposition,Calculating only the needed part of Q of thin QR decomposition,,"A rectangular, $A \in \mathbb{R}^{m \times n}$ matrix, where $m \ge n$ , can be decomposed (QR factorization): $$A = \begin{bmatrix}Q_1 | Q_2 \end{bmatrix}\begin{bmatrix}R\\0\end{bmatrix}$$ where $Q_1$ and $Q_2$ has orthonormal columns, and $R$ is upper triangular. I'm implementing a routine (based on Householder reflections) which calculates $Q_1$ and $R$ (so called thin/reduced QR decomposition). My question is: is it possible to calculate $Q_1$ without calculating $Q_2$ ? The problem is that a Householder matrix is $\mathbb{R}^{m \times m}$ , and $Q_1 \in \mathbb{R}^{m \times n}$ , so I cannot multiply them. My routine currently calculates $Q=[Q_1|Q_2]$ , and then throws away the $Q_2$ part.","A rectangular, matrix, where , can be decomposed (QR factorization): where and has orthonormal columns, and is upper triangular. I'm implementing a routine (based on Householder reflections) which calculates and (so called thin/reduced QR decomposition). My question is: is it possible to calculate without calculating ? The problem is that a Householder matrix is , and , so I cannot multiply them. My routine currently calculates , and then throws away the part.",A \in \mathbb{R}^{m \times n} m \ge n A = \begin{bmatrix}Q_1 | Q_2 \end{bmatrix}\begin{bmatrix}R\\0\end{bmatrix} Q_1 Q_2 R Q_1 R Q_1 Q_2 \mathbb{R}^{m \times m} Q_1 \in \mathbb{R}^{m \times n} Q=[Q_1|Q_2] Q_2,"['linear-algebra', 'matrices', 'matrix-decomposition']"
64,Positive operator is symmetric?,Positive operator is symmetric?,,"If I understand correctly then for an operator $\mathcal{A}$ defined on a Hilbert space $\mathcal{H}$ , $\langle \mathcal{A}x,x\rangle\geq 0$ does not necessarily imply that $\mathcal{A}$ is Hermitian: $\mathcal{A} = \mathcal{A}^\ast$ . See for instance Is a positive operator symmetric? However I was shown the following proof of the supposedly erroneous statement and was wondering if there is something wrong in it for I can't find it myself? Lemma 1: $\langle Tx,x\rangle = 0$ for every $x\in \mathcal{H}$ implies that $T \equiv 0$ . Proof: We show that $\langle Tx,y\rangle = 0$ for every pair $x,y\in \mathcal{H}.$ Indeed \begin{align*} 0 = \langle T(x+y),x+y\rangle - \langle T(x-y),x-y\rangle & = 2\langle Tx,y\rangle+2\langle Ty,x\rangle. \end{align*} This implies that $$\langle Tx,y\rangle = -\langle Ty,x\rangle.$$ Exchanging $x$ with $ix$ yields $$0 = i\langle Tx,y\rangle -i\langle Ty,x\rangle$$ why $$\langle Tx,y\rangle = \langle Ty,x\rangle$$ all in all we find that $\langle Tx,y\rangle = \pm \langle Ty,x\rangle$ which implies that both are $0$ . Proof that $\langle \mathcal{A}x,x\rangle\geq 0$ implies that $\mathcal{A}^\ast = \mathcal{A}$ : We have that $$\mathbb{R}\ni\langle \mathcal{A}x,x\rangle = \langle x,\mathcal{A}^\ast x\rangle = \overline{\langle \mathcal{A}^\ast x,x\rangle } = \langle \mathcal{A}^\ast x, x\rangle\Rightarrow \langle (\mathcal{A}-\mathcal{A}^\ast)x,x\rangle = 0$$ for every $x$ thus by the lemma $\mathcal{A}-\mathcal{A}^\ast\equiv 0$ .","If I understand correctly then for an operator defined on a Hilbert space , does not necessarily imply that is Hermitian: . See for instance Is a positive operator symmetric? However I was shown the following proof of the supposedly erroneous statement and was wondering if there is something wrong in it for I can't find it myself? Lemma 1: for every implies that . Proof: We show that for every pair Indeed This implies that Exchanging with yields why all in all we find that which implies that both are . Proof that implies that : We have that for every thus by the lemma .","\mathcal{A} \mathcal{H} \langle \mathcal{A}x,x\rangle\geq 0 \mathcal{A} \mathcal{A} = \mathcal{A}^\ast \langle Tx,x\rangle = 0 x\in \mathcal{H} T \equiv 0 \langle Tx,y\rangle = 0 x,y\in \mathcal{H}. \begin{align*}
0 = \langle T(x+y),x+y\rangle - \langle T(x-y),x-y\rangle & = 2\langle Tx,y\rangle+2\langle Ty,x\rangle.
\end{align*} \langle Tx,y\rangle = -\langle Ty,x\rangle. x ix 0 = i\langle Tx,y\rangle -i\langle Ty,x\rangle \langle Tx,y\rangle = \langle Ty,x\rangle \langle Tx,y\rangle = \pm \langle Ty,x\rangle 0 \langle \mathcal{A}x,x\rangle\geq 0 \mathcal{A}^\ast = \mathcal{A} \mathbb{R}\ni\langle \mathcal{A}x,x\rangle = \langle x,\mathcal{A}^\ast x\rangle = \overline{\langle \mathcal{A}^\ast x,x\rangle } = \langle \mathcal{A}^\ast x, x\rangle\Rightarrow \langle (\mathcal{A}-\mathcal{A}^\ast)x,x\rangle = 0 x \mathcal{A}-\mathcal{A}^\ast\equiv 0","['matrices', 'functional-analysis', 'operator-theory']"
65,Understanding Variance-Covariance Matrix,Understanding Variance-Covariance Matrix,,Suppose data set is expressed by the matrix $X \in\mathbb R^{n \times d}$ where $n =$ Number of samples and $d =$ dimension/features of each sample Then what does $\operatorname{Cov}(X) \in\mathbb R^{d \times d}$ (Variance-Covariance matrix of $X$ ) represent. Does below interpretation would be right Variance-Covariance matrix of $X$ represents covariance between every pair of dimension/feature for all samples.,Suppose data set is expressed by the matrix where Number of samples and dimension/features of each sample Then what does (Variance-Covariance matrix of ) represent. Does below interpretation would be right Variance-Covariance matrix of represents covariance between every pair of dimension/feature for all samples.,X \in\mathbb R^{n \times d} n = d = \operatorname{Cov}(X) \in\mathbb R^{d \times d} X X,"['linear-algebra', 'matrices', 'covariance']"
66,Representation of an algebra is absolutely irreducible if and only the representation map is surjective,Representation of an algebra is absolutely irreducible if and only the representation map is surjective,,"This should be well known but I can't seem to locate a reference: Let $k$ be a field, $V$ a $n$ -dimensional vector space over $k$ with an action of a $k$ -algebra $A$ . We say that $V$ is an absolutely irreducible $A$ -module if for any field extension $L/k$ , $V\otimes_k L$ is irreducible as an $A\otimes_{k}L$ module. In this situation, we also have a representation map $r: A \to M_n(k) = End_k(V)$ . Why is it true that $V$ is absolutely irreducible if and only if $r$ is surjective? It is at least easy to show that if $r$ is surjective, then $V$ is absolutely irreducible. This is simply equivalent to showing that $M_n(k)$ (even $GL_n(k)$ ) acts transitively on non zero vectors in $k^n$ (plus the fact that tensor products preserve surjectivity). What about the other direction?","This should be well known but I can't seem to locate a reference: Let be a field, a -dimensional vector space over with an action of a -algebra . We say that is an absolutely irreducible -module if for any field extension , is irreducible as an module. In this situation, we also have a representation map . Why is it true that is absolutely irreducible if and only if is surjective? It is at least easy to show that if is surjective, then is absolutely irreducible. This is simply equivalent to showing that (even ) acts transitively on non zero vectors in (plus the fact that tensor products preserve surjectivity). What about the other direction?",k V n k k A V A L/k V\otimes_k L A\otimes_{k}L r: A \to M_n(k) = End_k(V) V r r V M_n(k) GL_n(k) k^n,"['matrices', 'representation-theory', 'noncommutative-algebra']"
67,Multiplication of a vector by an orthogonal matrix,Multiplication of a vector by an orthogonal matrix,,"I have a question, consider $V$ an orthogonal matrix, and $u$ and $z$ are vectors, and W is a matrix does : $V'u = W V'z  \implies   u = W z$ ? I want to get rid of the orthogonal matrix $V'$ , my intuition says that I can, but I don't know which property of the orthogonal matrices will help me to do say. Thank you in advance.","I have a question, consider an orthogonal matrix, and and are vectors, and W is a matrix does : ? I want to get rid of the orthogonal matrix , my intuition says that I can, but I don't know which property of the orthogonal matrices will help me to do say. Thank you in advance.",V u z V'u = W V'z  \implies   u = W z V',"['linear-algebra', 'matrices', 'products']"
68,Square root of Diagonal matrix,Square root of Diagonal matrix,,"I cannot find an answer to if it is generally possible to take the square root of a diagonal matrix $A$ by taking the square root of each individual component along the main diagonal, e.g. for a 2-by-2 matrix $$         \sqrt{A} = \begin{pmatrix}         \sqrt{a_1} & 0 \\         0 & \sqrt{a_2} \\         \end{pmatrix}. $$ Is this OK to do provided that it is a (square) diagonal matrix?","I cannot find an answer to if it is generally possible to take the square root of a diagonal matrix by taking the square root of each individual component along the main diagonal, e.g. for a 2-by-2 matrix Is this OK to do provided that it is a (square) diagonal matrix?","A 
        \sqrt{A} = \begin{pmatrix}
        \sqrt{a_1} & 0 \\
        0 & \sqrt{a_2} \\
        \end{pmatrix}.
","['linear-algebra', 'matrices']"
69,What is the limit of $\mathrm{Tr}(G^kM{G^*}^k)^{1/2k}$ when $k$ goes to infinity?,What is the limit of  when  goes to infinity?,\mathrm{Tr}(G^kM{G^*}^k)^{1/2k} k,"If $G\in \mathscr M_n(\mathbf C)$ then it's well known that $\lim_{k\to \infty}\|G^k\|^{1/k}=\rho(G)$ where $\rho(G)$ is the spectral radius of $G$ , the value of the limit does not depend on the choosen norm. Consequently if we take the Schur norm we get $$ \lim_{k\to \infty} \mathrm{Tr}(G^k{G^*}^k)^{1/2k}=\rho(G). $$ Now if $M$ is an hermitian positive definite matrix then $\|A\|_M=\sqrt{\mathrm{Tr}(AMA^*)}$ is still a norm on $\mathscr M_n(\mathbf C)$ and so we also get $$ \lim_{k\to \infty} \mathrm{Tr}(G^kM{G^*}^k)^{1/2k}=\rho(G). $$ I would like to know what happen when $M$ is only hermitian positive semi-definite .  More precisely my question is the following. Question :   Let $G\in \mathscr M_n(\mathbf C)$ be an invertible   matrix and let $M$ be an hermitian positive semi-definite matrix with $\mathrm{Tr}(M)=1$ . Is it true that the limit $$\lim_{k\to \infty}\mathrm{Tr}(G^kM{G^*}^k)^{1/2k}$$ exists and is always equal to the modulus of an eigenvalue of $G$ ? I know that the $\lim \inf$ is larger than the smallest singular value and that the $\lim\sup$ is smaller than the largest singular value.","If then it's well known that where is the spectral radius of , the value of the limit does not depend on the choosen norm. Consequently if we take the Schur norm we get Now if is an hermitian positive definite matrix then is still a norm on and so we also get I would like to know what happen when is only hermitian positive semi-definite .  More precisely my question is the following. Question :   Let be an invertible   matrix and let be an hermitian positive semi-definite matrix with . Is it true that the limit exists and is always equal to the modulus of an eigenvalue of ? I know that the is larger than the smallest singular value and that the is smaller than the largest singular value.","G\in \mathscr M_n(\mathbf C) \lim_{k\to \infty}\|G^k\|^{1/k}=\rho(G) \rho(G) G 
\lim_{k\to \infty} \mathrm{Tr}(G^k{G^*}^k)^{1/2k}=\rho(G).
 M \|A\|_M=\sqrt{\mathrm{Tr}(AMA^*)} \mathscr M_n(\mathbf C) 
\lim_{k\to \infty} \mathrm{Tr}(G^kM{G^*}^k)^{1/2k}=\rho(G).
 M G\in \mathscr M_n(\mathbf C) M \mathrm{Tr}(M)=1 \lim_{k\to \infty}\mathrm{Tr}(G^kM{G^*}^k)^{1/2k} G \lim \inf \lim\sup","['matrices', 'banach-algebras', 'trace', 'symmetric-matrices', 'spectral-radius']"
70,Adjoint of a one-by-one zero matrix,Adjoint of a one-by-one zero matrix,,"Our lecturer defined the adjoint of a one-by-one matrix $A \in M_{1}(F)$ to be $\text{adj} (A) = [1]$ . She did not give a specific case when $A=[0]$ . So based on that definition, $\text{adj}([0]) =[1]$ and so $\text{adj}([0])$ is nonsingular. But $\text{adj}(A)$ is nonsingular if and only if $A$ is nonsingular. And $A$ in this particular case is singular. So how should one define $\text{adj}([0])$ ?","Our lecturer defined the adjoint of a one-by-one matrix to be . She did not give a specific case when . So based on that definition, and so is nonsingular. But is nonsingular if and only if is nonsingular. And in this particular case is singular. So how should one define ?",A \in M_{1}(F) \text{adj} (A) = [1] A=[0] \text{adj}([0]) =[1] \text{adj}([0]) \text{adj}(A) A A \text{adj}([0]),"['linear-algebra', 'matrices']"
71,Prove that $(I+A^{-1})^{-1}=A(A+I)^{-1}.$,Prove that,(I+A^{-1})^{-1}=A(A+I)^{-1}.,Prove that $(I+A^{-1})^{-1}=A(A+I)^{-1}$ assuming that the inverse matrices exist. My idea is to show that $(I+A^{-1})$ is the inverse matrix of $A(A+I)^{-1}$ by proving $(I+A^{-1})A(A+I)^{-1}=I$ and $A(A+I)^{-1}(I+A^{-1})=I$ . I have started with $(I+A^{-1})A(A+I)^{-1}=(IA+A^{-1}A)(A+I)^{-1}=(A+I)(A+I)^{-1}=I$ but when I go on to prove this the other way around $A(A+I)^{-1}(I+A^{-1})=I$ I am not sure how to open up the expression.,Prove that assuming that the inverse matrices exist. My idea is to show that is the inverse matrix of by proving and . I have started with but when I go on to prove this the other way around I am not sure how to open up the expression.,(I+A^{-1})^{-1}=A(A+I)^{-1} (I+A^{-1}) A(A+I)^{-1} (I+A^{-1})A(A+I)^{-1}=I A(A+I)^{-1}(I+A^{-1})=I (I+A^{-1})A(A+I)^{-1}=(IA+A^{-1}A)(A+I)^{-1}=(A+I)(A+I)^{-1}=I A(A+I)^{-1}(I+A^{-1})=I,"['linear-algebra', 'matrices']"
72,Calculate the derivative of a scalar function w.r.t its vector argument?,Calculate the derivative of a scalar function w.r.t its vector argument?,,"If $J$ is a scalar function of two vectors $\boldsymbol{q}$ and $\boldsymbol{g}$ : $$J(\boldsymbol{q},\boldsymbol{g})=\frac{\boldsymbol{g\cdot g}}{\boldsymbol{q\cdot q}}.$$ How do I calculate the derivative $\frac{\partial J}{\partial \boldsymbol{q}}$ ? What I have tried: $$\frac{\partial J}{\partial \boldsymbol{q}}=\frac{\partial}{\partial \boldsymbol{q}}\left(\frac{\boldsymbol{g\cdot g}}{\| \boldsymbol{q} \|^2} \right)=-2\|\boldsymbol{q}\|^{-3} \boldsymbol{g\cdot g}=-2 \|\boldsymbol{q}\|\:( \boldsymbol{g\cdot g}) \|\boldsymbol{q}\|^{-4}=-2\|\boldsymbol{q}\|\: (\boldsymbol{g\cdot g}) (\|\boldsymbol{q}\|^{2})^{-2}\\=-2\|\boldsymbol{q}\|\:( \boldsymbol{g\cdot g}) (\boldsymbol{q\cdot q})^{-2}.$$ I am sure that my answer is wrong because it should have been a vector instead of a scalar. For your reference, the answer given is $$\frac{\partial J}{\partial \boldsymbol{q}}=-2\boldsymbol{q}( \boldsymbol{g\cdot g})/(\boldsymbol{q\cdot q})^2.$$ Thank you in advance.","If is a scalar function of two vectors and : How do I calculate the derivative ? What I have tried: I am sure that my answer is wrong because it should have been a vector instead of a scalar. For your reference, the answer given is Thank you in advance.","J \boldsymbol{q} \boldsymbol{g} J(\boldsymbol{q},\boldsymbol{g})=\frac{\boldsymbol{g\cdot g}}{\boldsymbol{q\cdot q}}. \frac{\partial J}{\partial \boldsymbol{q}} \frac{\partial J}{\partial \boldsymbol{q}}=\frac{\partial}{\partial \boldsymbol{q}}\left(\frac{\boldsymbol{g\cdot g}}{\| \boldsymbol{q} \|^2} \right)=-2\|\boldsymbol{q}\|^{-3} \boldsymbol{g\cdot g}=-2 \|\boldsymbol{q}\|\:( \boldsymbol{g\cdot g}) \|\boldsymbol{q}\|^{-4}=-2\|\boldsymbol{q}\|\: (\boldsymbol{g\cdot g}) (\|\boldsymbol{q}\|^{2})^{-2}\\=-2\|\boldsymbol{q}\|\:( \boldsymbol{g\cdot g}) (\boldsymbol{q\cdot q})^{-2}. \frac{\partial J}{\partial \boldsymbol{q}}=-2\boldsymbol{q}( \boldsymbol{g\cdot g})/(\boldsymbol{q\cdot q})^2.","['matrices', 'derivatives', 'vectors']"
73,What Field(s) Cover Calculus with Matrices and High Dimensional Geometry?,What Field(s) Cover Calculus with Matrices and High Dimensional Geometry?,,"I am taking a course in machine learning and have found that the linear algebra and multivariable calculus from my engineering degree only take me part way in understanding some derivations. One specific example is differentiating things related to matrices (like differentiating wrt a matrix whose determinant appears in the function...) But this is by no means the only fuzzy bit. I have done just enough geometry, linear algebra and low dimensional calculus to have a notion that these things somehow extend into things involving matrices and things involving high dimensional spaces. I've tried looking on places like wikipedia's topic listing in mathematics, the page ""Categories within Mathematics"" at arxiv.org, and undergraduate mathematics curricula however I don't think I even know enough to know whether I'm looking at what I'm looking for... if that makes sense.... Also, I've found some compilations of matrix derivatives but a) it's disconnected from any context and b) seems like a cookbook solution and so these haven't been satisfying. So... how does what I'm saying here map to topics in mathematics? If I can cheat and ask a ""sub question""... whatever these topics end up being, what are some typical paths people take to get from fairly applied linear algebra and calculus to these ""advanced"" topics? Items on such a path could be books, names of courses, whatever... I just need some guidance towards context and prerequisites.","I am taking a course in machine learning and have found that the linear algebra and multivariable calculus from my engineering degree only take me part way in understanding some derivations. One specific example is differentiating things related to matrices (like differentiating wrt a matrix whose determinant appears in the function...) But this is by no means the only fuzzy bit. I have done just enough geometry, linear algebra and low dimensional calculus to have a notion that these things somehow extend into things involving matrices and things involving high dimensional spaces. I've tried looking on places like wikipedia's topic listing in mathematics, the page ""Categories within Mathematics"" at arxiv.org, and undergraduate mathematics curricula however I don't think I even know enough to know whether I'm looking at what I'm looking for... if that makes sense.... Also, I've found some compilations of matrix derivatives but a) it's disconnected from any context and b) seems like a cookbook solution and so these haven't been satisfying. So... how does what I'm saying here map to topics in mathematics? If I can cheat and ask a ""sub question""... whatever these topics end up being, what are some typical paths people take to get from fairly applied linear algebra and calculus to these ""advanced"" topics? Items on such a path could be books, names of courses, whatever... I just need some guidance towards context and prerequisites.",,"['calculus', 'matrices', 'geometry', 'soft-question', 'learning']"
74,Rotation matrix along a custom axis,Rotation matrix along a custom axis,,"For a certain software that I'm developing, I need to create a rotation matrix for a custom axis, and being almost completely self-taught in math, I am trying to wrap my mind around it, yet failing horribly. This is the simple rotation matrix for rotation along Z axis: $$\begin{pmatrix}     \cos\theta& -\sin\theta& 0\\     \sin\theta& \cos\theta&  0\\     0&    0&     1 \end{pmatrix}$$ Can someone explain how to I modify this matrix for use with a custom axis, which is offset from the Z axis by n degrees (and on the YZ plane)? I went at this problem for 2 days now, and I still can't solve it, no matter how many times I read the wiki article about rotation matrices... Attaching a picture to help you visualize the problem. Thank you in advance.","For a certain software that I'm developing, I need to create a rotation matrix for a custom axis, and being almost completely self-taught in math, I am trying to wrap my mind around it, yet failing horribly. This is the simple rotation matrix for rotation along Z axis: Can someone explain how to I modify this matrix for use with a custom axis, which is offset from the Z axis by n degrees (and on the YZ plane)? I went at this problem for 2 days now, and I still can't solve it, no matter how many times I read the wiki article about rotation matrices... Attaching a picture to help you visualize the problem. Thank you in advance.","\begin{pmatrix}
    \cos\theta& -\sin\theta& 0\\
    \sin\theta& \cos\theta&  0\\
    0&    0&     1
\end{pmatrix}","['matrices', 'trigonometry', 'coordinate-systems', 'rotations']"
75,condition for a matrix to be pseudo-hermitian,condition for a matrix to be pseudo-hermitian,,"As we know that a matrix $H$ is said to be Hermitian if $H=H^\dagger$, and if so, then all the eigenvalues are real. While a non-hermitian matrix $P$ is said to be pseudo-hermitian if $ \eta P \eta^{-1} =P^\dagger$, ($\eta$ is some constant metric), in this, eigenvalues may also be real. Could anyone please throw some light that - from where this condition of pseudo-hermiticity  $ \eta P \eta ^{-1}=P^\dagger$, comes??","As we know that a matrix $H$ is said to be Hermitian if $H=H^\dagger$, and if so, then all the eigenvalues are real. While a non-hermitian matrix $P$ is said to be pseudo-hermitian if $ \eta P \eta^{-1} =P^\dagger$, ($\eta$ is some constant metric), in this, eigenvalues may also be real. Could anyone please throw some light that - from where this condition of pseudo-hermiticity  $ \eta P \eta ^{-1}=P^\dagger$, comes??",,"['linear-algebra', 'matrices', 'matrix-calculus']"
76,Iwasawa Matrix Decomposition Proof,Iwasawa Matrix Decomposition Proof,,"Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. Here is the proof by Serge Lang in his book Undergraduate Algebra Section 6 Chapter 4 pg 246 (Read below picture for question(s) please :) I have a few questions about this proof (understand it roughly as a whole): 1) How does one get that $B=au$, following $g^{-1}=Bk^{-1}$? 2) Why does it follow that A has positive diagonal elements - that is $a_i=b_{ii}>0$? (My guess is that the QR decomposition guarantees this for R and note $B=R$) 3) I can't see any point in the proof where Lang makes a reference to the fact that $g$ has determinant $1$, in fact it seems that $g$ could have any non-zero determinant, hence $g\in{GL_n(\Bbb{R})}$. Why is this not so? Thank you.","Iwasawa Decomposition (special case): Let $G=SL_n(\Bbb{R})$, $K=$ real unitary matrices, $U=$ upper triangular matrices with $1$'s on the diagonal (called unipotent ), and $A=$ diagonal matrices with positive elements ($0$ everywhere else). Then, the product map $U\times{A}\times{K}\rightarrow{G}$ given by $(u,a,k)\mapsto{uak}$ is a bijection. Here is the proof by Serge Lang in his book Undergraduate Algebra Section 6 Chapter 4 pg 246 (Read below picture for question(s) please :) I have a few questions about this proof (understand it roughly as a whole): 1) How does one get that $B=au$, following $g^{-1}=Bk^{-1}$? 2) Why does it follow that A has positive diagonal elements - that is $a_i=b_{ii}>0$? (My guess is that the QR decomposition guarantees this for R and note $B=R$) 3) I can't see any point in the proof where Lang makes a reference to the fact that $g$ has determinant $1$, in fact it seems that $g$ could have any non-zero determinant, hence $g\in{GL_n(\Bbb{R})}$. Why is this not so? Thank you.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'matrix-decomposition', 'linear-groups']"
77,Is there a closed-form way to make a matrix hollow?,Is there a closed-form way to make a matrix hollow?,,"Let $A\in\mathbb{R}^{N\times N}$ a matrix with entries $a_{ij} \ge 0$ and $\mathrm{det}(A)\neq 0$. I am interested in finding a closed matrix form operation such that $A$ becomes hollow. In other words, if I want to obtain   $$ \tilde{A}\in\mathbb{R}^{N\times N}\quad\text{such that}\quad \tilde{a}_{ij}=a_{ij}\;\forall (i\neq j)\quad\text{and}\quad\tilde{a}_{ii}=0 $$   with constant matrices $P$ and $Q$ such that   $$ \tilde A=PAQ $$   is it possible? I tried a simple example, when $N=2$ and with brute force I found an interesting correlation with a transformation matrix $Q$ (here $P=I$), but I do not know how (and if) it is generalizable to $N>2$: $$ A=\begin{pmatrix} a & b\\ c & d \end{pmatrix} $$ $$ \tilde{A}=\begin{pmatrix} 0 & b\\ c & 0 \end{pmatrix}=PAQ=IAQ=AQ=A\cdot\frac{1}{\det{A}}\begin{pmatrix} -bc & bd\\ ac & -bc \end{pmatrix} $$ EDIT From the comments, it has been pointed out that what I seek is not possible, meaning $P$ and $Q$ do not exist. Is there a way to prove this?","Let $A\in\mathbb{R}^{N\times N}$ a matrix with entries $a_{ij} \ge 0$ and $\mathrm{det}(A)\neq 0$. I am interested in finding a closed matrix form operation such that $A$ becomes hollow. In other words, if I want to obtain   $$ \tilde{A}\in\mathbb{R}^{N\times N}\quad\text{such that}\quad \tilde{a}_{ij}=a_{ij}\;\forall (i\neq j)\quad\text{and}\quad\tilde{a}_{ii}=0 $$   with constant matrices $P$ and $Q$ such that   $$ \tilde A=PAQ $$   is it possible? I tried a simple example, when $N=2$ and with brute force I found an interesting correlation with a transformation matrix $Q$ (here $P=I$), but I do not know how (and if) it is generalizable to $N>2$: $$ A=\begin{pmatrix} a & b\\ c & d \end{pmatrix} $$ $$ \tilde{A}=\begin{pmatrix} 0 & b\\ c & 0 \end{pmatrix}=PAQ=IAQ=AQ=A\cdot\frac{1}{\det{A}}\begin{pmatrix} -bc & bd\\ ac & -bc \end{pmatrix} $$ EDIT From the comments, it has been pointed out that what I seek is not possible, meaning $P$ and $Q$ do not exist. Is there a way to prove this?",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-decomposition']"
78,Determining the last column so that the resulting matrix is an orthogonal matrix,Determining the last column so that the resulting matrix is an orthogonal matrix,,Determine the last column so that the resulting matrix is an orthogonal   matrix $$\begin{bmatrix} \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{6}} & ? \\ \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}} & ? \\ 0 & \dfrac{2}{\sqrt{6}} & ? \end{bmatrix}$$ Can anyone please provide hints to solve this?,Determine the last column so that the resulting matrix is an orthogonal   matrix $$\begin{bmatrix} \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{6}} & ? \\ \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{6}} & ? \\ 0 & \dfrac{2}{\sqrt{6}} & ? \end{bmatrix}$$ Can anyone please provide hints to solve this?,,['linear-algebra']
79,Eigenvalues of sum of non-symmetric matrices,Eigenvalues of sum of non-symmetric matrices,,"Assume $A, B$ are real matrices. Weyl's inequalities provide bounds on the eigenvalues of $A + B$ if both are symmetric. Is there any bound if neither are symmetric? I am particularly interested about the case where $A$ and $B$ are positive stable , that is, have eigenvalues with positive real part. For instance, can one always produce $A, B$ positive stable such that $A+B$ has eigenvalues with arbitrarily negative real part?","Assume are real matrices. Weyl's inequalities provide bounds on the eigenvalues of if both are symmetric. Is there any bound if neither are symmetric? I am particularly interested about the case where and are positive stable , that is, have eigenvalues with positive real part. For instance, can one always produce positive stable such that has eigenvalues with arbitrarily negative real part?","A, B A + B A B A, B A+B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
80,Confused between Single Value Decomposition(SVD) and Diagonalization of matrix,Confused between Single Value Decomposition(SVD) and Diagonalization of matrix,,"I'm studying Principle Component Analysis (PCA), and came across this post. In which it's written that diagonalization of co-variance matrix ($C$) can be given by $$C = VLV^T$$ But as per difference between SVD and Diagonalization and this post , it's clear that diagonalization of any matrix can be given by: $$C = VLV^{-1}$$ So why the definition of SVD and diagonalization is same here ?","I'm studying Principle Component Analysis (PCA), and came across this post. In which it's written that diagonalization of co-variance matrix ($C$) can be given by $$C = VLV^T$$ But as per difference between SVD and Diagonalization and this post , it's clear that diagonalization of any matrix can be given by: $$C = VLV^{-1}$$ So why the definition of SVD and diagonalization is same here ?",,"['linear-algebra', 'matrices', 'machine-learning', 'matrix-decomposition', 'svd']"
81,Prove matrix $A$ is diagonalizable if and only if each eigenvalue has equal geometric and algebraic multiplicity.,Prove matrix  is diagonalizable if and only if each eigenvalue has equal geometric and algebraic multiplicity.,A,"Prove: If $A$ is an $n\times n$ matrix with distinct eigenvalues $\lambda_1, \dots, \lambda_k$, then $A$ is diagonalizable if and only if $g_{\lambda_i}=a_{\lambda_i}$ for $1\le i \le k$. I have seen and understood a proof for $\sum g_{\lambda_i}=n=\sum a_{\lambda_i}$ but nothing for the equality of each distinct eigenvalue.","Prove: If $A$ is an $n\times n$ matrix with distinct eigenvalues $\lambda_1, \dots, \lambda_k$, then $A$ is diagonalizable if and only if $g_{\lambda_i}=a_{\lambda_i}$ for $1\le i \le k$. I have seen and understood a proof for $\sum g_{\lambda_i}=n=\sum a_{\lambda_i}$ but nothing for the equality of each distinct eigenvalue.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
82,Forcing zero entries with unitary matrix operations.,Forcing zero entries with unitary matrix operations.,,"Starting with an arbitrary matrix $A \in \mathbb{C}^{m\times m}$, $$ A = \left[\begin{array}[cc]          ** & * & * \\ * & * & * \\ * & * & *        \end{array} \right] $$ where $*$ represents a any number. Produce $$ A = \left[\begin{array}[cc]          ** & * & 0\\ 0& * & * \\ 0& 0& *        \end{array} \right] $$ multiplying $A$ by unitary matrices only. The first 2 columns can be done with Householder reflectors, but I don't see how to get the zero in the (1,3) entry.","Starting with an arbitrary matrix $A \in \mathbb{C}^{m\times m}$, $$ A = \left[\begin{array}[cc]          ** & * & * \\ * & * & * \\ * & * & *        \end{array} \right] $$ where $*$ represents a any number. Produce $$ A = \left[\begin{array}[cc]          ** & * & 0\\ 0& * & * \\ 0& 0& *        \end{array} \right] $$ multiplying $A$ by unitary matrices only. The first 2 columns can be done with Householder reflectors, but I don't see how to get the zero in the (1,3) entry.",,"['linear-algebra', 'matrices']"
83,Rank one orthogonal projector matrix.,Rank one orthogonal projector matrix.,,"My text is covering projector matrices while building up to Householder triangularization. The main topic of discussion is orthogonal projector matrices that satisfy \begin{align} P &= P^2 \tag{1} \\ P &= P^* \tag{2} \end{align} It turns out that we can form a rank one orthogonal projector with any orthonormal vector $q \in \mathbb{C}^m$ \begin{align} P_q = qq^* \end{align} which is easily verified to satisfy (1) and (2). $P_q$'s rank $m-1$ complement is then found by $P_{\perp q} = I - P_q$. So far these facts and definitions all make sense to me. But I'm having a little trouble with the following: my book goes on to say that analogous projector matrices for arbitrary nonzero vectors $a$ can be written \begin{align} P_a &= \frac{aa^*}{a^*a} \\ \\ P_{\perp a} &= I - P_a  \end{align} It's easy to verify that these formulas satisfy (1) and (2) but what is the motivation for the scaling factor of $a^*a$? For example \begin{align} P_av = \left(\frac{a^*v}{a^*a}\right)a \implies \| P_av\|_2       =\frac{\left|a^*v\right|}{\|a\|_2^2} \|a\|_2\      = \frac{\left|a^*v\right|}{\|a\|_2} \tag{*} \end{align} Why the scaling factor? Is it true that $a^*v = \|a\|_2\|v\|_2\cos(a,v)$ for higher dimensional complex vectors? I that case (*) becomes \begin{align} \|v\|_2 \left|\cos(a,v)\right| \end{align} In hindsight that seems to make sense. $P_av$ is just putting $v$ on $a$ and scaling it to be an orthogonal projection. I guess my true question is: is the 2-norm function equivalent to the modulus function in complex spaces? In particular, for $x,y \in \mathbb{C}^m$ prove $$x^*y = \|x\|_2\|y\|_2 \cos \alpha $$","My text is covering projector matrices while building up to Householder triangularization. The main topic of discussion is orthogonal projector matrices that satisfy \begin{align} P &= P^2 \tag{1} \\ P &= P^* \tag{2} \end{align} It turns out that we can form a rank one orthogonal projector with any orthonormal vector $q \in \mathbb{C}^m$ \begin{align} P_q = qq^* \end{align} which is easily verified to satisfy (1) and (2). $P_q$'s rank $m-1$ complement is then found by $P_{\perp q} = I - P_q$. So far these facts and definitions all make sense to me. But I'm having a little trouble with the following: my book goes on to say that analogous projector matrices for arbitrary nonzero vectors $a$ can be written \begin{align} P_a &= \frac{aa^*}{a^*a} \\ \\ P_{\perp a} &= I - P_a  \end{align} It's easy to verify that these formulas satisfy (1) and (2) but what is the motivation for the scaling factor of $a^*a$? For example \begin{align} P_av = \left(\frac{a^*v}{a^*a}\right)a \implies \| P_av\|_2       =\frac{\left|a^*v\right|}{\|a\|_2^2} \|a\|_2\      = \frac{\left|a^*v\right|}{\|a\|_2} \tag{*} \end{align} Why the scaling factor? Is it true that $a^*v = \|a\|_2\|v\|_2\cos(a,v)$ for higher dimensional complex vectors? I that case (*) becomes \begin{align} \|v\|_2 \left|\cos(a,v)\right| \end{align} In hindsight that seems to make sense. $P_av$ is just putting $v$ on $a$ and scaling it to be an orthogonal projection. I guess my true question is: is the 2-norm function equivalent to the modulus function in complex spaces? In particular, for $x,y \in \mathbb{C}^m$ prove $$x^*y = \|x\|_2\|y\|_2 \cos \alpha $$",,"['linear-algebra', 'matrices', 'projection-matrices', 'rank-1-matrices']"
84,Do the complex conjugate eigenvalues add up in pairs for the addition of a matrix and its transpose?,Do the complex conjugate eigenvalues add up in pairs for the addition of a matrix and its transpose?,,"I encountered a few matrices like the ones in the following: $A_1=\begin{bmatrix}1 & -1 & 0\\0 & 1 & -1\\-1 & 0 & 1\end{bmatrix}$, $A_2=\begin{bmatrix}0.84 & -0.62 & -0.22\\-0.22 & 0.84 & -0.62\\-0.62 & -0.22 & 0.84\end{bmatrix}$ which respectively have eigenvalues $\lambda(A_1)=0,~1.5\pm{j*0.866}$ and $\lambda(A_2)=0,1.26\pm{j*0.3464}$. What surprises me is $\lambda(A_1+A^T_1)=0,3,3$ and $\lambda(A_2+A^T_2)=0,2.52,2.52$, i.e. the eigenvalues of the resultant matrix are sum of corresponding complex and complex conjugate eigenvalues. Is this associated to any property of matrices which make the resultant eigenvalues of $(A_i+A^T_i)$ twice the real part of eigenvalues of $A_i$? I have one more matrix to show similar situation. $V=\begin{bmatrix}-0.5773 & -0.5773 & 0.5773\\-0.5773 & -0.288675 & -0.288675\\-0.5773 & -0.288675 & -0.288675\end{bmatrix}$, which gives $\lambda(V)=0,-0.5773\pm{j*0.8164}$ and $\lambda(V+V^T)=0,-1.1547,-1.1547$. My approach: any matrix $A=\dfrac{1}{2}(A+A^T)+\dfrac{1}{2}(A-A^T)$ and for a nonzero vector $x$, we get  $x^T{A}x=\dfrac{1}{2}x^T(A+A^T)x$ since $x^T(A-A^T)x=0$ due to skew-symmetricity of $A-A^T$. Therefore $x^T(A+A^T)x=2x^T{A}x=2x^T{P}DP^{-1}x$, where $A$ is decomposed as $A=PDP^{-1}$ with $D$ being a diagonal matrix with all eigenvalues of $A$ along its diagonal and $P$ is a unitary matrix with complex conjugate transpose denoted by $P^*=P^{-1}$. [The eigenvectors of $A_{1},A_2,V$ are independent and constitute a basis for $\mathbb{C}^3$]. Since $2x^T{PDP^{-1}}x\leq{2}x^T\text{tr}(PDP^{-1}){x}=2x^T\text{tr}(D){x}=4*\text{real}(\lambda)\|x\|^2$. But this does not help me prove the claim. Ant hint or references will be greatly appreciated.","I encountered a few matrices like the ones in the following: $A_1=\begin{bmatrix}1 & -1 & 0\\0 & 1 & -1\\-1 & 0 & 1\end{bmatrix}$, $A_2=\begin{bmatrix}0.84 & -0.62 & -0.22\\-0.22 & 0.84 & -0.62\\-0.62 & -0.22 & 0.84\end{bmatrix}$ which respectively have eigenvalues $\lambda(A_1)=0,~1.5\pm{j*0.866}$ and $\lambda(A_2)=0,1.26\pm{j*0.3464}$. What surprises me is $\lambda(A_1+A^T_1)=0,3,3$ and $\lambda(A_2+A^T_2)=0,2.52,2.52$, i.e. the eigenvalues of the resultant matrix are sum of corresponding complex and complex conjugate eigenvalues. Is this associated to any property of matrices which make the resultant eigenvalues of $(A_i+A^T_i)$ twice the real part of eigenvalues of $A_i$? I have one more matrix to show similar situation. $V=\begin{bmatrix}-0.5773 & -0.5773 & 0.5773\\-0.5773 & -0.288675 & -0.288675\\-0.5773 & -0.288675 & -0.288675\end{bmatrix}$, which gives $\lambda(V)=0,-0.5773\pm{j*0.8164}$ and $\lambda(V+V^T)=0,-1.1547,-1.1547$. My approach: any matrix $A=\dfrac{1}{2}(A+A^T)+\dfrac{1}{2}(A-A^T)$ and for a nonzero vector $x$, we get  $x^T{A}x=\dfrac{1}{2}x^T(A+A^T)x$ since $x^T(A-A^T)x=0$ due to skew-symmetricity of $A-A^T$. Therefore $x^T(A+A^T)x=2x^T{A}x=2x^T{P}DP^{-1}x$, where $A$ is decomposed as $A=PDP^{-1}$ with $D$ being a diagonal matrix with all eigenvalues of $A$ along its diagonal and $P$ is a unitary matrix with complex conjugate transpose denoted by $P^*=P^{-1}$. [The eigenvectors of $A_{1},A_2,V$ are independent and constitute a basis for $\mathbb{C}^3$]. Since $2x^T{PDP^{-1}}x\leq{2}x^T\text{tr}(PDP^{-1}){x}=2x^T\text{tr}(D){x}=4*\text{real}(\lambda)\|x\|^2$. But this does not help me prove the claim. Ant hint or references will be greatly appreciated.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-calculus', 'matrix-decomposition']"
85,Equivalent characterisation of hypothesis & conclusion in Grothendieck's inequality,Equivalent characterisation of hypothesis & conclusion in Grothendieck's inequality,,"The basic version of Grothendieck's inequality states that for a $m\times n$ matrix $[a_{i, j}]$, if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le 1,$$ for all $x_i, y_i\in \{-1, 1\}$, then $$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K,$$ for any unit vectors $u_i, v_j$ in a Hilbert space $H$ (i.e. $\|u_i\| = \|v_j\| =1$), with $K$ an absolute constant. The following conditions are equivalent to the hypothesis and conclusion of the prior statement respectively: hypothesis: if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le \max_i |x_i| \max_j|y_j|,$$ for all $x_i, y_i\in \mathbb{R}$. conclusion: then $$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K\max_i\|u_i\|\max_j\|v_j\|,$$ for any vectors $u_i, v_j\in H$, a Hilbert space, with $K$ an absolute constant. This equivalence seems like it ought to be very simple to prove, however I seem to be having a mental block which is preventing me from proceeding & presume that I'm just not seeing something pretty obvious here. I'd be grateful for any hints or suggestions on how to approach the proof.","The basic version of Grothendieck's inequality states that for a $m\times n$ matrix $[a_{i, j}]$, if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le 1,$$ for all $x_i, y_i\in \{-1, 1\}$, then $$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K,$$ for any unit vectors $u_i, v_j$ in a Hilbert space $H$ (i.e. $\|u_i\| = \|v_j\| =1$), with $K$ an absolute constant. The following conditions are equivalent to the hypothesis and conclusion of the prior statement respectively: hypothesis: if $$\bigg|\sum_{i, j}a_{i, j}x_i y_j\bigg|\le \max_i |x_i| \max_j|y_j|,$$ for all $x_i, y_i\in \mathbb{R}$. conclusion: then $$\bigg|\sum_{i, j}a_{i, j}\langle u_i, v_j\rangle\bigg|\le K\max_i\|u_i\|\max_j\|v_j\|,$$ for any vectors $u_i, v_j\in H$, a Hilbert space, with $K$ an absolute constant. This equivalence seems like it ought to be very simple to prove, however I seem to be having a mental block which is preventing me from proceeding & presume that I'm just not seeing something pretty obvious here. I'd be grateful for any hints or suggestions on how to approach the proof.",,"['matrices', 'inequality', 'hilbert-spaces']"
86,Solving $A^*A=B$,Solving,A^*A=B,"There are two 4×4 matrices $A$ and $B$ with $B$ symmetrical and $$A^*A=B$$ Given that $A$ is completely unknown and that $B$ is known but is a function of a variable $k$, I want to solve for each element of $A$ as a function of $k$. Progress so far I believe that given the equation, we have 10 equations and 10 unknowns, meaning $A$ is also symmetrical. I first tried without the conjugate. To do so, I tried using Matlab and Maple solve functions to process this system of equations but they either never finish or give up with an error warning that I cannot make much use of. I also tried using conjugate transpose on Matlab, but it doesn't enjoy the conjugate of a symbol. I have the feeling I am doing something very wrong here. Are there perhaps any mathematical properties I can leverage to solve this?","There are two 4×4 matrices $A$ and $B$ with $B$ symmetrical and $$A^*A=B$$ Given that $A$ is completely unknown and that $B$ is known but is a function of a variable $k$, I want to solve for each element of $A$ as a function of $k$. Progress so far I believe that given the equation, we have 10 equations and 10 unknowns, meaning $A$ is also symmetrical. I first tried without the conjugate. To do so, I tried using Matlab and Maple solve functions to process this system of equations but they either never finish or give up with an error warning that I cannot make much use of. I also tried using conjugate transpose on Matlab, but it doesn't enjoy the conjugate of a symbol. I have the feeling I am doing something very wrong here. Are there perhaps any mathematical properties I can leverage to solve this?",,"['linear-algebra', 'matrices']"
87,What is the rank of $B$?,What is the rank of ?,B,"Let $A=(a_{ij})$ be the square matrix of size $2018$ defined by $$ a_{ij} = \begin{cases} 2 & \text{if } i+1=j\\ \frac{1}{3} & \text{if } i =j+1\\ 0 & \text{otherwise}\end{cases}$$ Let $B$ be the leading principal minor of $A$ of order $1009$ (i.e., the submatrix of $A$ formed by the first $1009$ rows and columns). What is the determinant of $A$? What is the rank of $B$? My solution For question 1: For an $n$ dimensional matrix of the form $$ A_n =\begin{pmatrix}  0 & u & 0 & & \cdots  & 0 \\  l & 0 & u & & \cdots &  \\  0 & l & 0 \\ \vdots & & & \ddots & & \vdots \\  0 & \cdots & & & 0 & u \\ 0 & \cdots & & & l & 0 \end{pmatrix} $$ The determinant is $$ {\rm det} [A_n] = \begin{cases} 0 & n=\mbox{odd} \\ (-l u)^{\frac{n}{2}} & n=\mbox{even} \end{cases} $$ For  case $n=2018$, $u=2$ and $l=\frac{1}{3}$ $$ {\rm det}[A_{2018}] = \left( -\frac{2}{3} \right)^{1009}. $$ I'm stuck at question 2. Any hints?","Let $A=(a_{ij})$ be the square matrix of size $2018$ defined by $$ a_{ij} = \begin{cases} 2 & \text{if } i+1=j\\ \frac{1}{3} & \text{if } i =j+1\\ 0 & \text{otherwise}\end{cases}$$ Let $B$ be the leading principal minor of $A$ of order $1009$ (i.e., the submatrix of $A$ formed by the first $1009$ rows and columns). What is the determinant of $A$? What is the rank of $B$? My solution For question 1: For an $n$ dimensional matrix of the form $$ A_n =\begin{pmatrix}  0 & u & 0 & & \cdots  & 0 \\  l & 0 & u & & \cdots &  \\  0 & l & 0 \\ \vdots & & & \ddots & & \vdots \\  0 & \cdots & & & 0 & u \\ 0 & \cdots & & & l & 0 \end{pmatrix} $$ The determinant is $$ {\rm det} [A_n] = \begin{cases} 0 & n=\mbox{odd} \\ (-l u)^{\frac{n}{2}} & n=\mbox{even} \end{cases} $$ For  case $n=2018$, $u=2$ and $l=\frac{1}{3}$ $$ {\rm det}[A_{2018}] = \left( -\frac{2}{3} \right)^{1009}. $$ I'm stuck at question 2. Any hints?",,"['linear-algebra', 'matrices']"
88,how to find such matrices to compute $\sin A$,how to find such matrices to compute,\sin A,"Could anyone tell me how to solve this one? $A=\begin{bmatrix}5&-6&-6\\ -1&4&2\\3&-6&-4 \end{bmatrix}$, and given that $\sin A=B\times C\times E$, then find $B,C,E$. I never solved such type of problem in past thanks for helping. I see charpoly of $A$ is $\lambda^3-5\lambda^2+8\lambda-4=0$. I also checked $P^{-1}AP=\text{diagonal}(2,2,1), P=\begin{bmatrix}2&2&1\\1&0&\frac{-1}{3}\\ 0&1&1\end{bmatrix}$.","Could anyone tell me how to solve this one? $A=\begin{bmatrix}5&-6&-6\\ -1&4&2\\3&-6&-4 \end{bmatrix}$, and given that $\sin A=B\times C\times E$, then find $B,C,E$. I never solved such type of problem in past thanks for helping. I see charpoly of $A$ is $\lambda^3-5\lambda^2+8\lambda-4=0$. I also checked $P^{-1}AP=\text{diagonal}(2,2,1), P=\begin{bmatrix}2&2&1\\1&0&\frac{-1}{3}\\ 0&1&1\end{bmatrix}$.",,"['matrices', 'matrix-equations']"
89,Suppose A be a matrix with Diagonal entries are $\alpha+1$ then $\alpha $ is?,Suppose A be a matrix with Diagonal entries are  then  is?,\alpha+1 \alpha ,"Suppose $A=(a_{ij})$ be a $10\times 10$ order matrix such that $a_{ij}$=$1$ for $i\neq j$ and$ a_{ii}=\alpha+1$ for $\alpha\ge 0$.Let $ \lambda$  and  $\mu $ be largest and smallest eigenvalues of $A$. If$\lambda+\mu=24$ then $\alpha=?$                           here is my attempt -- according to the question diagonal elements are $\alpha+1$ and remaining entries are 1 then as sum of eigen value =sum of diagonal $10\alpha+10$=$\lambda+\mu+\sum_{i=2}^9\lambda_i $ suppose $\lambda_1=\mu$ and $\lambda_{10}=\lambda$ then $10\alpha$=$14+\sum_{i=2}^9\lambda_i $ as $\lambda+\mu=24$ but how can I find $\alpha$, is my process is correct? If not then what is the correct process?","Suppose $A=(a_{ij})$ be a $10\times 10$ order matrix such that $a_{ij}$=$1$ for $i\neq j$ and$ a_{ii}=\alpha+1$ for $\alpha\ge 0$.Let $ \lambda$  and  $\mu $ be largest and smallest eigenvalues of $A$. If$\lambda+\mu=24$ then $\alpha=?$                           here is my attempt -- according to the question diagonal elements are $\alpha+1$ and remaining entries are 1 then as sum of eigen value =sum of diagonal $10\alpha+10$=$\lambda+\mu+\sum_{i=2}^9\lambda_i $ suppose $\lambda_1=\mu$ and $\lambda_{10}=\lambda$ then $10\alpha$=$14+\sum_{i=2}^9\lambda_i $ as $\lambda+\mu=24$ but how can I find $\alpha$, is my process is correct? If not then what is the correct process?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
90,Inverse of a modular matrix,Inverse of a modular matrix,,"I have the matrix$$ \begin{pmatrix} 1 & 5\\ 3 & 4 \end{pmatrix} \pmod{26} $$ and I need to find its inverse. I do it according to this website . I find the modular multiplicative inverse (of the matrix determinant, which is $1×4-3×5=-11$) with the extended Euclid algorithm (it is $-7 \equiv  19 \pmod{26}$). Then I have $$\frac{1}{19}×\begin{pmatrix}4 & -5\\-3 & 1\end{pmatrix}.$$ I calculate that$$-5 \equiv 21 \pmod{26},\ -3 \equiv 23 \pmod{26}.$$ No matter what I do I am not able to get the solution they have on the website, which is$$\begin{pmatrix}2 & 17\\5 & 7\end{pmatrix}.$$ Can someone help me with this? What am I doing wrong?","I have the matrix$$ \begin{pmatrix} 1 & 5\\ 3 & 4 \end{pmatrix} \pmod{26} $$ and I need to find its inverse. I do it according to this website . I find the modular multiplicative inverse (of the matrix determinant, which is $1×4-3×5=-11$) with the extended Euclid algorithm (it is $-7 \equiv  19 \pmod{26}$). Then I have $$\frac{1}{19}×\begin{pmatrix}4 & -5\\-3 & 1\end{pmatrix}.$$ I calculate that$$-5 \equiv 21 \pmod{26},\ -3 \equiv 23 \pmod{26}.$$ No matter what I do I am not able to get the solution they have on the website, which is$$\begin{pmatrix}2 & 17\\5 & 7\end{pmatrix}.$$ Can someone help me with this? What am I doing wrong?",,"['matrices', 'modular-arithmetic']"
91,Is the low-rank approximation of a matrix unique?,Is the low-rank approximation of a matrix unique?,,"Let $A \in \mathbb{R}^{m \times n}$ be a matrix with $\text{rank}(A)=\min(m,n)$. The Eckart–Young–Mirsky theorem for the Frobenius norm states that the best approximation of rank $k<\min(m,n)$ for $A$, denoted $A_k$, is: $$\arg \min_{A_k} \left\Vert A - A_k \right\Vert_F^2 = \sum_{i=1}^k \sigma_i u_i v_i^T$$ where $$A= \overset{\max(m,n)}{\underset{i=1}{\sum}} \sigma_i u_i v_i^T$$ is the singular value decomposition of $A$ (The singular vectors $u_i$ and $v_i$ are normalized and their corresponding singular values $\sigma_i$ are sorted in descending order). Is this solution the unique global minimizer?","Let $A \in \mathbb{R}^{m \times n}$ be a matrix with $\text{rank}(A)=\min(m,n)$. The Eckart–Young–Mirsky theorem for the Frobenius norm states that the best approximation of rank $k<\min(m,n)$ for $A$, denoted $A_k$, is: $$\arg \min_{A_k} \left\Vert A - A_k \right\Vert_F^2 = \sum_{i=1}^k \sigma_i u_i v_i^T$$ where $$A= \overset{\max(m,n)}{\underset{i=1}{\sum}} \sigma_i u_i v_i^T$$ is the singular value decomposition of $A$ (The singular vectors $u_i$ and $v_i$ are normalized and their corresponding singular values $\sigma_i$ are sorted in descending order). Is this solution the unique global minimizer?",,"['linear-algebra', 'matrices', 'optimization', 'matrix-rank', 'svd']"
92,Polynomial form of two variable matrix determinant function,Polynomial form of two variable matrix determinant function,,"We have the function $$f(x,y)=\det(A^2+B^2-xAB-yBA)$$ where $x, y$ are real numbers and $A, B$ are $2 \times 2$ matrices with real coefficients. What are the coefficients of $f(x,y)$ in polynomial form?","We have the function $$f(x,y)=\det(A^2+B^2-xAB-yBA)$$ where $x, y$ are real numbers and $A, B$ are $2 \times 2$ matrices with real coefficients. What are the coefficients of $f(x,y)$ in polynomial form?",,"['linear-algebra', 'matrices', 'polynomials', 'determinant']"
93,Which Matrix is an Inner Product,Which Matrix is an Inner Product,,"We define the inner product of square matrices to be $\langle\vec x, \vec y\rangle_A=\vec x^TA\vec y$. One of the matrices $$\begin{bmatrix} 1 & 2 \\ 2 & 1 \\ \end{bmatrix}$$ $$\begin{bmatrix} 2 & 1 \\ 1 & 2 \\ \end{bmatrix}$$ violates the requirement $\langle\vec x, \vec x\rangle_A > 0$ for $\vec x \neq \vec 0$. I don't really understand the inner product in general, and I am really struggling to understand and prove which matrix isn't an inner product. Any help would be greatly appreciated.","We define the inner product of square matrices to be $\langle\vec x, \vec y\rangle_A=\vec x^TA\vec y$. One of the matrices $$\begin{bmatrix} 1 & 2 \\ 2 & 1 \\ \end{bmatrix}$$ $$\begin{bmatrix} 2 & 1 \\ 1 & 2 \\ \end{bmatrix}$$ violates the requirement $\langle\vec x, \vec x\rangle_A > 0$ for $\vec x \neq \vec 0$. I don't really understand the inner product in general, and I am really struggling to understand and prove which matrix isn't an inner product. Any help would be greatly appreciated.",,"['linear-algebra', 'matrices', 'inner-products']"
94,Is $A-B$ never normal?,Is  never normal?,A-B,"Let $A,B \in M(3,\Bbb R)$, $A$ non-singular and symmetric, $B \neq 0$ such that $B^2=0$. Is $A-B$ never normal?  So far i noticed that:  $$(A-B)^*(A-B) = A^2 -AB-B^tA+B^tB$$ $$(A-B)(A-B)^*=A^2-BA-AB^t+BB^t$$ So if $B$ were symmetric $A-B$ would be normal but $B$ is nilpotent so it can't be symmetric (Symmetric matrices are diagonalizable for the spectral theorem and nilpotent matrices are not). $B$ is also different from $0$ so i believe that the statement is true but i can't demonstrate it. Thank you in advance for your help!","Let $A,B \in M(3,\Bbb R)$, $A$ non-singular and symmetric, $B \neq 0$ such that $B^2=0$. Is $A-B$ never normal?  So far i noticed that:  $$(A-B)^*(A-B) = A^2 -AB-B^tA+B^tB$$ $$(A-B)(A-B)^*=A^2-BA-AB^t+BB^t$$ So if $B$ were symmetric $A-B$ would be normal but $B$ is nilpotent so it can't be symmetric (Symmetric matrices are diagonalizable for the spectral theorem and nilpotent matrices are not). $B$ is also different from $0$ so i believe that the statement is true but i can't demonstrate it. Thank you in advance for your help!",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'nilpotence']"
95,Completing the square in $N$ dimensions,Completing the square in  dimensions,N,"This is very important for Bayesian methods in statistics, but I haven't been able to find a reference which specifically touches on my situation. Assume all matrices and vectors below are matrices and vectors with all real entries. All boldface lowercase letters are column vectors, and boldface uppercase letters are matrices (including the lowercase and uppercase Greek letters). In the below, $\mathbf{y}$,  $\boldsymbol\theta$ (with any subscripts), and $\boldsymbol\mu$ are column vectors; $\mathbf{X}$,  $\boldsymbol\Sigma$ (with any subscripts) are matrices; and $c$ is a scalar in $\mathbb{R}$. $\mathbf{X}^{\prime}$ denotes the transpose of a matrix $\mathbf{X}$. I have a sum of two quadratic forms   $$(\mathbf{y}-\mathbf{X}\boldsymbol\theta)^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol\theta)  + (\boldsymbol\theta-\boldsymbol\theta_0)^{\prime}\boldsymbol\Sigma^{-1}_0(\boldsymbol\theta-\boldsymbol\theta_0)\tag{1}$$   which I would like to write in the form    $$(\boldsymbol\theta-\boldsymbol\mu)^{\prime}\boldsymbol\Sigma^{-1}(\boldsymbol\theta-\boldsymbol\mu)  + c\tag{2}$$ I don't care what $c$ is. What I'm mainly interested in is what $\boldsymbol\mu$ and $\boldsymbol\Sigma$ are. After a lot of work, I was able to write the parts of $(1)$ which depend on $\boldsymbol\theta$ in the following form (remember, I don't care about $c$):  $$\boldsymbol\theta^{\prime}(\mathbf{X}^{\prime}\boldsymbol\Sigma^{-1}_{\boldsymbol\eta}\mathbf{X}+\boldsymbol\Sigma_0^{-1})\boldsymbol\theta-\boldsymbol\theta^{\prime}(\mathbf{X}^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}\mathbf{y}+\boldsymbol\Sigma_0^{-1}\boldsymbol\theta_0)-(\mathbf{y}^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}\mathbf{X}+\boldsymbol\theta_0^{\prime}\boldsymbol\Sigma_0^{-1})\boldsymbol\theta\tag{3}$$ You should assume that all $\boldsymbol\Sigma$ matrices, regardless of the subscript, are symmetric and positive definite. How can I write $(3)$ in the form $(2)$? I would really appreciate an explanation of how the procedure works in general.","This is very important for Bayesian methods in statistics, but I haven't been able to find a reference which specifically touches on my situation. Assume all matrices and vectors below are matrices and vectors with all real entries. All boldface lowercase letters are column vectors, and boldface uppercase letters are matrices (including the lowercase and uppercase Greek letters). In the below, $\mathbf{y}$,  $\boldsymbol\theta$ (with any subscripts), and $\boldsymbol\mu$ are column vectors; $\mathbf{X}$,  $\boldsymbol\Sigma$ (with any subscripts) are matrices; and $c$ is a scalar in $\mathbb{R}$. $\mathbf{X}^{\prime}$ denotes the transpose of a matrix $\mathbf{X}$. I have a sum of two quadratic forms   $$(\mathbf{y}-\mathbf{X}\boldsymbol\theta)^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}(\mathbf{y}-\mathbf{X}\boldsymbol\theta)  + (\boldsymbol\theta-\boldsymbol\theta_0)^{\prime}\boldsymbol\Sigma^{-1}_0(\boldsymbol\theta-\boldsymbol\theta_0)\tag{1}$$   which I would like to write in the form    $$(\boldsymbol\theta-\boldsymbol\mu)^{\prime}\boldsymbol\Sigma^{-1}(\boldsymbol\theta-\boldsymbol\mu)  + c\tag{2}$$ I don't care what $c$ is. What I'm mainly interested in is what $\boldsymbol\mu$ and $\boldsymbol\Sigma$ are. After a lot of work, I was able to write the parts of $(1)$ which depend on $\boldsymbol\theta$ in the following form (remember, I don't care about $c$):  $$\boldsymbol\theta^{\prime}(\mathbf{X}^{\prime}\boldsymbol\Sigma^{-1}_{\boldsymbol\eta}\mathbf{X}+\boldsymbol\Sigma_0^{-1})\boldsymbol\theta-\boldsymbol\theta^{\prime}(\mathbf{X}^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}\mathbf{y}+\boldsymbol\Sigma_0^{-1}\boldsymbol\theta_0)-(\mathbf{y}^{\prime}\boldsymbol\Sigma_{\boldsymbol\eta}^{-1}\mathbf{X}+\boldsymbol\theta_0^{\prime}\boldsymbol\Sigma_0^{-1})\boldsymbol\theta\tag{3}$$ You should assume that all $\boldsymbol\Sigma$ matrices, regardless of the subscript, are symmetric and positive definite. How can I write $(3)$ in the form $(2)$? I would really appreciate an explanation of how the procedure works in general.",,"['linear-algebra', 'matrices', 'bayesian', 'completing-the-square']"
96,Show that every integer eigenvalue of $A$ divides the determinant of $A$.,Show that every integer eigenvalue of  divides the determinant of .,A A,Let $A$ be a square matrix of order $n$ whose entries are all integers.   Show that every integer eigenvalue of $A$ divides the determinant of $A$. I am not able to understand how to show this. We know that $\det A$ is the product of eigen values and so every eigen value must divide $\det A$. But if a matrix has eigen values $3$ and $\frac{4}{3}$ then if I do the product then the factor $3$  gets neutralized if I do the product then how does it appear as a factor of $\det A$? Please help.,Let $A$ be a square matrix of order $n$ whose entries are all integers.   Show that every integer eigenvalue of $A$ divides the determinant of $A$. I am not able to understand how to show this. We know that $\det A$ is the product of eigen values and so every eigen value must divide $\det A$. But if a matrix has eigen values $3$ and $\frac{4}{3}$ then if I do the product then the factor $3$  gets neutralized if I do the product then how does it appear as a factor of $\det A$? Please help.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
97,Existence of the vector that decomposes the trace of diagonal and symmetric matrix,Existence of the vector that decomposes the trace of diagonal and symmetric matrix,,"Question: Suppose that $\Lambda$ is an $N × N$ real-valued diagonal   matrix and $Q$ is real symmetric. Suppose that $tr\ \Lambda \neq 0$   and $tr\ Q \neq 0$. Prove there is a vector $v \in \mathbb{R}^N$ such   that $$v^T\Lambda v = tr\ \Lambda, v^TQv = tr\ Q $$ My thought: Decompose the $\Lambda$ as $\Lambda = U^T\Lambda U, \ \forall\ U^TU = I $ and $Q = U^T\Lambda_1 U$. So $$v^T\Lambda v = (Uv)^T\Lambda (Uv)= tr\ \Lambda$$ and $$v^TQ v = (Uv)^T\Lambda_1 (Uv) = tr \ Q = tr\ \Lambda_1$$  Intutively,the structure is same so they could preserve the same property. But how could I prove the $v$ in two formulas is the same one? Could anyone help me out? Thank in advance!","Question: Suppose that $\Lambda$ is an $N × N$ real-valued diagonal   matrix and $Q$ is real symmetric. Suppose that $tr\ \Lambda \neq 0$   and $tr\ Q \neq 0$. Prove there is a vector $v \in \mathbb{R}^N$ such   that $$v^T\Lambda v = tr\ \Lambda, v^TQv = tr\ Q $$ My thought: Decompose the $\Lambda$ as $\Lambda = U^T\Lambda U, \ \forall\ U^TU = I $ and $Q = U^T\Lambda_1 U$. So $$v^T\Lambda v = (Uv)^T\Lambda (Uv)= tr\ \Lambda$$ and $$v^TQ v = (Uv)^T\Lambda_1 (Uv) = tr \ Q = tr\ \Lambda_1$$  Intutively,the structure is same so they could preserve the same property. But how could I prove the $v$ in two formulas is the same one? Could anyone help me out? Thank in advance!",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'trace']"
98,Proving if a certain matrix exists or not,Proving if a certain matrix exists or not,,"The question is: Is there a natural $n$ and $A\in M_{n}(\mathbb C)$ (complex $n\times n$ matrices) such that the conditions \begin{align} \operatorname{rank}(\,A\,) &= 10\\ \operatorname{rank}(A^2) &= 7\\ \operatorname{rank}(A^3) &= 2 \end{align} are satisfied? Either provide an example or prove it does not exist. It's clear that $10\le n$ and because $A$ is not invertible it means $11\le n$. I've tried finding an example using Jordan Blocks: there are two block with the size of $4$ or one block with the size of $5$, in order to answer on the third term. Then I tried filling the matrix with block of 3 but it doesn't work out.  I also don't have any idea how to prove such a matrix doesn't exist.","The question is: Is there a natural $n$ and $A\in M_{n}(\mathbb C)$ (complex $n\times n$ matrices) such that the conditions \begin{align} \operatorname{rank}(\,A\,) &= 10\\ \operatorname{rank}(A^2) &= 7\\ \operatorname{rank}(A^3) &= 2 \end{align} are satisfied? Either provide an example or prove it does not exist. It's clear that $10\le n$ and because $A$ is not invertible it means $11\le n$. I've tried finding an example using Jordan Blocks: there are two block with the size of $4$ or one block with the size of $5$, in order to answer on the third term. Then I tried filling the matrix with block of 3 but it doesn't work out.  I also don't have any idea how to prove such a matrix doesn't exist.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
99,"If $P,Q\ge 0;P+Q=I$, how to prove $||P-Q||_2\le1$","If , how to prove","P,Q\ge 0;P+Q=I ||P-Q||_2\le1","We define $$\|A\|_2=\max_{x\ne0}\frac{\|Ax\|_2}{\|x\|_2}$$ as the matrix norm (Spectral Norm, http://mathworld.wolfram.com/SpectralNorm.html ). If $P,Q$ are definite symmetry matrices and $P+Q=I$. How to prove that $$\|P-Q\|_2\le1$$ My thinking: In fact, from $P+Q=I$, we can conclude that each pair eigenvalue of P and Q are sum up to 1. As a result, their eigenvalue are all less than 1. But what's the next step to say that $\|P-Q\|_2\le1$?","We define $$\|A\|_2=\max_{x\ne0}\frac{\|Ax\|_2}{\|x\|_2}$$ as the matrix norm (Spectral Norm, http://mathworld.wolfram.com/SpectralNorm.html ). If $P,Q$ are definite symmetry matrices and $P+Q=I$. How to prove that $$\|P-Q\|_2\le1$$ My thinking: In fact, from $P+Q=I$, we can conclude that each pair eigenvalue of P and Q are sum up to 1. As a result, their eigenvalue are all less than 1. But what's the next step to say that $\|P-Q\|_2\le1$?",,"['matrices', 'eigenvalues-eigenvectors', 'spectral-theory', 'matrix-calculus']"
