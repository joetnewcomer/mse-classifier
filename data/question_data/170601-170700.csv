,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of Sample Variance Given Variance,Probability of Sample Variance Given Variance,,"I am trying to solve a problem that I have never seen before and cant seem to find a way to solve it so any help or tips would be appreciated! Here's the Problem: Suppose a considerable amount of effort is conducted to decrease the variability in a system. Following this, a random sample of size $n=40$ is taken from the new assembly line and the sample variance is: $$S^{2}=0.188$$ Do we have a strong numerical evidence that $σ^{2}$ has been reduced below $1.0$? Consider this Probability and give your conclusion: $$P(S^{2} \leq 0.188 \mid σ^{2}=1.0)$$ Data that I have gotten: $$n=40, S^{2}=0.188, σ^{2}=1.00, μ=9$$ Confidence Interval $= 9\pm 1.5$ I am not sure how to solve this problem, I was going to try and use the T-distribution some how but I cannot figure it out, so any help would be awesome! Thank you","I am trying to solve a problem that I have never seen before and cant seem to find a way to solve it so any help or tips would be appreciated! Here's the Problem: Suppose a considerable amount of effort is conducted to decrease the variability in a system. Following this, a random sample of size $n=40$ is taken from the new assembly line and the sample variance is: $$S^{2}=0.188$$ Do we have a strong numerical evidence that $σ^{2}$ has been reduced below $1.0$? Consider this Probability and give your conclusion: $$P(S^{2} \leq 0.188 \mid σ^{2}=1.0)$$ Data that I have gotten: $$n=40, S^{2}=0.188, σ^{2}=1.00, μ=9$$ Confidence Interval $= 9\pm 1.5$ I am not sure how to solve this problem, I was going to try and use the T-distribution some how but I cannot figure it out, so any help would be awesome! Thank you",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
1,Show that in this case $\rho \geq -\frac{1}{n-1}$,Show that in this case,\rho \geq -\frac{1}{n-1},"The bounds of correlation coefficient $\rho$ is shown to be $\pm 1$ in class. In many situations the bounds are sharper, i.e. they stay away from $+1$ or $-1$. Consider the random variables $X_1,\dots,X_n$ such that $E(X_i)=\mu$, $Var(X_i)=\sigma^2$, and for each pair $i \neq j$, $Corr(X_i,X_j)=\rho$. Such equi-correlation structure is sometimes referred to as exchangeable or compound symmetry. Show that in this case $$\rho \geq -\frac{1}{n-1}$$","The bounds of correlation coefficient $\rho$ is shown to be $\pm 1$ in class. In many situations the bounds are sharper, i.e. they stay away from $+1$ or $-1$. Consider the random variables $X_1,\dots,X_n$ such that $E(X_i)=\mu$, $Var(X_i)=\sigma^2$, and for each pair $i \neq j$, $Corr(X_i,X_j)=\rho$. Such equi-correlation structure is sometimes referred to as exchangeable or compound symmetry. Show that in this case $$\rho \geq -\frac{1}{n-1}$$",,['statistics']
2,confidence inteverval $95\%$,confidence inteverval,95\%,"how do I go about finding the $95\%$ confidence interval when I have $n=12, s_x=0.66$ and $u=35.72$ and also how many more samples would I need to reduce the ""length"" of the interval by half? so I looked in the solution and it says: $\left(35.72-2.201\cdot 0.66/\sqrt{12},35.72+2.201\cdot 0.66/\sqrt{12}\right)$, so.. I got really confused about where $2.201$ comes from? is it something they calculated or do they look up stuff like that?","how do I go about finding the $95\%$ confidence interval when I have $n=12, s_x=0.66$ and $u=35.72$ and also how many more samples would I need to reduce the ""length"" of the interval by half? so I looked in the solution and it says: $\left(35.72-2.201\cdot 0.66/\sqrt{12},35.72+2.201\cdot 0.66/\sqrt{12}\right)$, so.. I got really confused about where $2.201$ comes from? is it something they calculated or do they look up stuff like that?",,['statistics']
3,what should the null hypothesis be,what should the null hypothesis be,,A group of 50 complete a national fitness test and get a mean score of 80 out of 100. The national average is 72 with standard deviation 6. Can we conclude the group of 50 is fitter than the national average. Let ${\mu}$ be the national average score. What are the null and alternative hypothesis? I'm really confused on this on. Its almost as if I should be using a difference of means test.,A group of 50 complete a national fitness test and get a mean score of 80 out of 100. The national average is 72 with standard deviation 6. Can we conclude the group of 50 is fitter than the national average. Let ${\mu}$ be the national average score. What are the null and alternative hypothesis? I'm really confused on this on. Its almost as if I should be using a difference of means test.,,"['statistics', 'hypothesis-testing']"
4,Question about correlations in statistics,Question about correlations in statistics,,"x: 1,2,3,4,9,10 y: 12,2,3,5,9,11 What feature of the data is responsible for reducing the correlation to this value despite a strong straight-line association between x and y in most of the observations?","x: 1,2,3,4,9,10 y: 12,2,3,5,9,11 What feature of the data is responsible for reducing the correlation to this value despite a strong straight-line association between x and y in most of the observations?",,['statistics']
5,"If $U$ is uniformly distributed on $S^{2}$, then its first component is uniformly distributed on $(-1,1)$. [duplicate]","If  is uniformly distributed on , then its first component is uniformly distributed on . [duplicate]","U S^{2} (-1,1)","This question already has answers here : Random point uniform on a sphere (3 answers) Closed 3 years ago . Assume $\mathbf{U} = (U_{1}, U_{2}, U_{3})' \sim unif(S^{2})$. How would I show that $U_{1} \sim unif(-1,1)$? I don't know if I'm confusing myself, because I can't see this as being true for higher dimensions.","This question already has answers here : Random point uniform on a sphere (3 answers) Closed 3 years ago . Assume $\mathbf{U} = (U_{1}, U_{2}, U_{3})' \sim unif(S^{2})$. How would I show that $U_{1} \sim unif(-1,1)$? I don't know if I'm confusing myself, because I can't see this as being true for higher dimensions.",,"['probability', 'statistics', 'probability-theory']"
6,Convergence in total variation to Gaussian implies finite moments?,Convergence in total variation to Gaussian implies finite moments?,,"Suppose $d_\mathrm{TV}(\mathbb{P}_n,\mathcal{N}(0,1)) \to 0$ for a sequence  of probability distributions $\mathbb{P}_n$. Is each $k^{th}$ moment of $\mathbb{P}_n$ necessarily finite after some $n_k$? If yes, does each sequence of moments necessarily converge to the moments of $\mathcal{N}(0,1)$?","Suppose $d_\mathrm{TV}(\mathbb{P}_n,\mathcal{N}(0,1)) \to 0$ for a sequence  of probability distributions $\mathbb{P}_n$. Is each $k^{th}$ moment of $\mathbb{P}_n$ necessarily finite after some $n_k$? If yes, does each sequence of moments necessarily converge to the moments of $\mathcal{N}(0,1)$?",,"['probability', 'statistics']"
7,Find the value of k which makes f a density function.,Find the value of k which makes f a density function.,,"Observe the following probability density function for a continuous random variable X $$f (x) = \begin{cases} k\sqrt x (1-x) &\text{ for }x\in(0,1)\\  0 &\text{ otherwise} \end{cases} $$ Find the value of $k$ which makes $f$ a density function. My thoughts, is it the integral from $0$ to $1$ of $f(x)$?","Observe the following probability density function for a continuous random variable X $$f (x) = \begin{cases} k\sqrt x (1-x) &\text{ for }x\in(0,1)\\  0 &\text{ otherwise} \end{cases} $$ Find the value of $k$ which makes $f$ a density function. My thoughts, is it the integral from $0$ to $1$ of $f(x)$?",,"['probability', 'statistics', 'probability-distributions']"
8,Expected Number and Variance,Expected Number and Variance,,Practice Exam question. Not sure how to do this. The number of offspring of an organism is a discrete random variable with mean $\mu$ and variance $\sigma^2$. Each of its offspring reproduce in the same manner. Find the expected number of offspring in the third generation and its variance.,Practice Exam question. Not sure how to do this. The number of offspring of an organism is a discrete random variable with mean $\mu$ and variance $\sigma^2$. Each of its offspring reproduce in the same manner. Find the expected number of offspring in the third generation and its variance.,,"['probability', 'statistics']"
9,How to prove a statistic is not complete,How to prove a statistic is not complete,,"Suppose X is a Poisson($\lambda$), where $\lambda\in\{0,1,2,...\}$, how to prove X is not complete. It seems like that we need to find a function $g$ which is not identical $0$, such that the following linear system is always true $$ \sum\limits_{k=0}^{\infty}\frac{\lambda^k}{k!}g(k)=0\quad \forall \lambda\in\{0,1,2,...\} $$ But how to construct such a $g$? Thanks a lot!","Suppose X is a Poisson($\lambda$), where $\lambda\in\{0,1,2,...\}$, how to prove X is not complete. It seems like that we need to find a function $g$ which is not identical $0$, such that the following linear system is always true $$ \sum\limits_{k=0}^{\infty}\frac{\lambda^k}{k!}g(k)=0\quad \forall \lambda\in\{0,1,2,...\} $$ But how to construct such a $g$? Thanks a lot!",,"['statistics', 'statistical-inference']"
10,How to find a probility that the sample mean of a population lies in a particular range?,How to find a probility that the sample mean of a population lies in a particular range?,,"Assume that X is a random variable with mean x_mean and standard deviation x_sd . If we take a sample of n items from a population at random, what is the probability that this sample mean (say sample_x_mean ) lies between left and right values?","Assume that X is a random variable with mean x_mean and standard deviation x_sd . If we take a sample of n items from a population at random, what is the probability that this sample mean (say sample_x_mean ) lies between left and right values?",,"['statistics', 'normal-distribution']"
11,books on the application of linear algebra on statistics/finance/machine learning,books on the application of linear algebra on statistics/finance/machine learning,,"I am reading ""linear algebra done right"" by Axler and like it a lot. One thing though, in the end I would like to put these theory to use and as a math textbook it doesn't cover much application.  Would you recommend any good book that covers application but also tries to tie back to the theory . It would be even better, if the subject is mostly focused on statistics/finance/machine learning. Or some other book that has a balance between matrix and linear algebra theory, as Axler doesn't cover much matrix until the end. (how about Linear Algebra by FriedbergInsel/Spence?) Thank you very much.","I am reading ""linear algebra done right"" by Axler and like it a lot. One thing though, in the end I would like to put these theory to use and as a math textbook it doesn't cover much application.  Would you recommend any good book that covers application but also tries to tie back to the theory . It would be even better, if the subject is mostly focused on statistics/finance/machine learning. Or some other book that has a balance between matrix and linear algebra theory, as Axler doesn't cover much matrix until the end. (how about Linear Algebra by FriedbergInsel/Spence?) Thank you very much.",,"['linear-algebra', 'statistics', 'linear-programming', 'finance', 'machine-learning']"
12,Getting the cumulative distribution function for $\sqrt{X}$ from the cumulative distribution function for $X$,Getting the cumulative distribution function for  from the cumulative distribution function for,\sqrt{X} X,I've a data set $X$ which consists of randomly generated numbers. My aim is to plot the cumulative distribution function for square root of $X$ without generating data set for square root of $X$ . I'm using Mathematica tool. I'm confused and could not think of a solution. Can somebody let me know how to take the approach here ?,I've a data set which consists of randomly generated numbers. My aim is to plot the cumulative distribution function for square root of without generating data set for square root of . I'm using Mathematica tool. I'm confused and could not think of a solution. Can somebody let me know how to take the approach here ?,X X X,"['probability', 'statistics', 'discrete-mathematics']"
13,Discrete mathematics vs. non parametric statistics,Discrete mathematics vs. non parametric statistics,,Is there any meaningful connection betveen non parametric statistics and discrete mathematics? I am reading this book: http://www.amazon.com/Discrete-Mathematics-Technology-Rowan-Garnier/dp/075030135X I wonder if it is applyable in some way to chi-square test or kendall tau coefficient.,Is there any meaningful connection betveen non parametric statistics and discrete mathematics? I am reading this book: http://www.amazon.com/Discrete-Mathematics-Technology-Rowan-Garnier/dp/075030135X I wonder if it is applyable in some way to chi-square test or kendall tau coefficient.,,"['statistics', 'discrete-mathematics', 'statistical-inference']"
14,Proof of $\sigma^2\geq (\mu-m)^2$ without resorting to Jensen's or Chebychev's inequality.,Proof of  without resorting to Jensen's or Chebychev's inequality.,\sigma^2\geq (\mu-m)^2,"I asked a group of undergrad students (engineering) to prove that $\sigma^2\geq (\mu-m)^2$, where $\sigma^2$, $\mu$ and $m$ are the variance, mean and median of a continuous random variable. For the discrete case it is simple. For the continuous case the easiest way is to use Jensen's inequality. But the students do not know that inequality. I thought I had an elementary proof, but I was mistaken. Can someone reproduce the proof in this paper http://www.tandfonline.com/doi/abs/10.1080/00031305.1990.10475743?queryID=%24%7BresultBean.queryID%7D#.VCLQ2_ldXfs ? I do not have access to this one. Thank you in advance, Gustavo.","I asked a group of undergrad students (engineering) to prove that $\sigma^2\geq (\mu-m)^2$, where $\sigma^2$, $\mu$ and $m$ are the variance, mean and median of a continuous random variable. For the discrete case it is simple. For the continuous case the easiest way is to use Jensen's inequality. But the students do not know that inequality. I thought I had an elementary proof, but I was mistaken. Can someone reproduce the proof in this paper http://www.tandfonline.com/doi/abs/10.1080/00031305.1990.10475743?queryID=%24%7BresultBean.queryID%7D#.VCLQ2_ldXfs ? I do not have access to this one. Thank you in advance, Gustavo.",,"['statistics', 'inequality', 'median']"
15,Statistics - Show that $\hat{\theta}$ hat is a biased estimator of $\theta$,Statistics - Show that  hat is a biased estimator of,\hat{\theta} \theta,"I'm asked to solve this exercise, but I can't manage to find something satisfying. Any help/hint would be much appreciated. Let $Y_1, Y_2,\dots, Y_n$ denote a random variable sample of size n from a population whose density is given by : $$f(y)=\begin{cases}\frac{\alpha y^{\alpha - 1}}{\Theta^\alpha} & 0\leq y \leq \Theta \\ 0 & y \not\in[0,\Theta]\end{cases}$$ Where $\alpha > 0$ is a known fixed value but $\theta$ is unknown. We consider $\hat{\theta} = \max(Y_1,Y_2,...,Y_n)$. How can one show that $\hat{\theta}$ is a biased estimator for $\theta$. My try was to compute the distribution function $F(\hat{\theta})$ in order to calculate the probability density function and then simply prove that $E(\hat{\theta}) \neq \theta$ but it didn't get me anywhere.","I'm asked to solve this exercise, but I can't manage to find something satisfying. Any help/hint would be much appreciated. Let $Y_1, Y_2,\dots, Y_n$ denote a random variable sample of size n from a population whose density is given by : $$f(y)=\begin{cases}\frac{\alpha y^{\alpha - 1}}{\Theta^\alpha} & 0\leq y \leq \Theta \\ 0 & y \not\in[0,\Theta]\end{cases}$$ Where $\alpha > 0$ is a known fixed value but $\theta$ is unknown. We consider $\hat{\theta} = \max(Y_1,Y_2,...,Y_n)$. How can one show that $\hat{\theta}$ is a biased estimator for $\theta$. My try was to compute the distribution function $F(\hat{\theta})$ in order to calculate the probability density function and then simply prove that $E(\hat{\theta}) \neq \theta$ but it didn't get me anywhere.",,"['statistics', 'proof-verification']"
16,What is the bound on $E\|Y_n\|^4$ in terms of $n$?,What is the bound on  in terms of ?,E\|Y_n\|^4 n,"Let $X_n,n\in\mathbb{N}$ be i.i.d. zero-mean random variables in some separable Hilbert space with $E\|X_n\|^8<\infty$ and $Y_n=\frac{1}{n}\sum_{i=1}^nX_n$. I need to find bounds on $E\|Y_n\|^4$. The CLT tells me that $Y_n = O_p(n^{-1/2})$. Can I say something about $E\|Y_n\|^4$ ? As I understand, $\|\sqrt{n}Y_n\|^4\xrightarrow{d}\|G\|^4$, where $G$ is some Gaussian process, so that $\|Y_n\|^4 = O_p(n^{-2})$, but I don't see how do deal with expected value of something bounded in probability. Update: I did the following computations $$\begin{aligned} E\|Y_n\|^4 & = \frac{1}{n^4}E\left(\sum_{i=1}^n\sum_{j=1}^n\langle X_i,X_j\rangle\right)^2 \\ & = \frac{1}{n^4}\sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^nE[\langle X_i,X_j\rangle\langle X_k,X_l\rangle] \end{aligned}$$ but then it is a dissaster, to consider all cases for indices... For example, if $i\ne j\ne k\ne l$, $E[\langle X_i,X_j\rangle\langle X_k,X_l\rangle] = E[\langle X_i,X_j\rangle]E[\langle X_k,X_l\rangle]=0$ due to independence. Is there an elegant way to tackle it?","Let $X_n,n\in\mathbb{N}$ be i.i.d. zero-mean random variables in some separable Hilbert space with $E\|X_n\|^8<\infty$ and $Y_n=\frac{1}{n}\sum_{i=1}^nX_n$. I need to find bounds on $E\|Y_n\|^4$. The CLT tells me that $Y_n = O_p(n^{-1/2})$. Can I say something about $E\|Y_n\|^4$ ? As I understand, $\|\sqrt{n}Y_n\|^4\xrightarrow{d}\|G\|^4$, where $G$ is some Gaussian process, so that $\|Y_n\|^4 = O_p(n^{-2})$, but I don't see how do deal with expected value of something bounded in probability. Update: I did the following computations $$\begin{aligned} E\|Y_n\|^4 & = \frac{1}{n^4}E\left(\sum_{i=1}^n\sum_{j=1}^n\langle X_i,X_j\rangle\right)^2 \\ & = \frac{1}{n^4}\sum_{i=1}^n\sum_{j=1}^n\sum_{k=1}^n\sum_{l=1}^nE[\langle X_i,X_j\rangle\langle X_k,X_l\rangle] \end{aligned}$$ but then it is a dissaster, to consider all cases for indices... For example, if $i\ne j\ne k\ne l$, $E[\langle X_i,X_j\rangle\langle X_k,X_l\rangle] = E[\langle X_i,X_j\rangle]E[\langle X_k,X_l\rangle]=0$ due to independence. Is there an elegant way to tackle it?",,"['statistics', 'probability-theory', 'asymptotics']"
17,Solving Conditional Probability Unknkowns,Solving Conditional Probability Unknkowns,,"Given: A is the number of people who have a disease is 0.01 B is when a drug tests positive $\mathsf P(B\mid A)=0.9$ $\mathsf P(B\mid A')= 0.05$ I solved for $\mathsf P(B)=0.0585$ and $\mathsf P(A \cap B) = 0.009$ The question asks what is the probability of a person having the disease given that two independent tests that were done on him turned up positive? Essentially the questions asks for $\mathsf P(A\mid C)$, where $C$ is both tests turning up positive or $B^2$. I found $\mathsf P(C)$ from multiplying $0.0585$ by itself, but how do I find the probability that: A occurs and B occurs on both tests? I'm leaning towards $\mathsf P(A \cap B) \times \mathsf P(A \cap B)$ since both tests are independent. However, when I divide by $\mathsf P(C)$ I got $0.02367$ which is smaller than $\mathsf P(A\cap B)$. Shouldn't two positive independent tests increase the probability that the person indeed has the disease? I think my difficulty is in the syllogism of defining what A occurring and B occurring on both tests is.","Given: A is the number of people who have a disease is 0.01 B is when a drug tests positive $\mathsf P(B\mid A)=0.9$ $\mathsf P(B\mid A')= 0.05$ I solved for $\mathsf P(B)=0.0585$ and $\mathsf P(A \cap B) = 0.009$ The question asks what is the probability of a person having the disease given that two independent tests that were done on him turned up positive? Essentially the questions asks for $\mathsf P(A\mid C)$, where $C$ is both tests turning up positive or $B^2$. I found $\mathsf P(C)$ from multiplying $0.0585$ by itself, but how do I find the probability that: A occurs and B occurs on both tests? I'm leaning towards $\mathsf P(A \cap B) \times \mathsf P(A \cap B)$ since both tests are independent. However, when I divide by $\mathsf P(C)$ I got $0.02367$ which is smaller than $\mathsf P(A\cap B)$. Shouldn't two positive independent tests increase the probability that the person indeed has the disease? I think my difficulty is in the syllogism of defining what A occurring and B occurring on both tests is.",,"['statistics', 'conditional-probability']"
18,Testing statistic $\frac{MSS(X)}{MSS(Y)}$,Testing statistic,\frac{MSS(X)}{MSS(Y)},"Suppose a test statistic $\frac{MSS(X)}{MSS(Y)}$, where $MSS$ denotes Mean Sum of Squares, is to be used for testing the significance of the factor $X$. Do we need the assumption $$\mathbb E[MSS(X)]=\mathbb E[MSS(Y)]$$ to be satisfied? Why? As far i know if the assumption is true , then the test statistic follows F-distribution . Is that the only case?Is their any detail explanation?","Suppose a test statistic $\frac{MSS(X)}{MSS(Y)}$, where $MSS$ denotes Mean Sum of Squares, is to be used for testing the significance of the factor $X$. Do we need the assumption $$\mathbb E[MSS(X)]=\mathbb E[MSS(Y)]$$ to be satisfied? Why? As far i know if the assumption is true , then the test statistic follows F-distribution . Is that the only case?Is their any detail explanation?",,"['statistics', 'probability-distributions', 'self-learning', 'normal-distribution', 'statistical-inference']"
19,What statistical analysis to use on a list of dates?,What statistical analysis to use on a list of dates?,,I suffer from migraines and for many years I've kept a log of each attack.  I now want to do some analysis on the dates of each attack to see if there are any patterns. I intend to check the frequency of different days of the week - but how can I tell if any variations are statistically significant? How about mean time between attack?  Can I use this to tell if the distribution of attacks is truly random? What other approaches can I use on this dataset?,I suffer from migraines and for many years I've kept a log of each attack.  I now want to do some analysis on the dates of each attack to see if there are any patterns. I intend to check the frequency of different days of the week - but how can I tell if any variations are statistically significant? How about mean time between attack?  Can I use this to tell if the distribution of attacks is truly random? What other approaches can I use on this dataset?,,"['statistics', 'data-analysis']"
20,Prove that mean square error equals expected conditional variance,Prove that mean square error equals expected conditional variance,,"I'm a first year grad student in Statistics. The book I'm using mentioned conditional variance, and I wanted to read up more about it. I dove down the google rabbit hole and found this website. I read through it and followed the proofs. Then I came to this chunk, and I can't prove it myself. From the definition of conditional variance and the basic property above, it follows that the mean square error when $E(Y∣X)$ is used as a predictor of $Y$ is: $$E([Y−E(Y∣X)]^2 )=E[\operatorname{Var}(Y∣X)]=\operatorname{Var}(Y)−\operatorname{Var}[E(Y∣X)]$$ When I expand the LHS, I get the following: $$\begin{split} E([Y−E(Y∣X)]^2) &= E([Y^2-2YE[Y|X]+E[Y|X]^2) \\   &= E[Y^2] - 2E[YE[Y|X]] + E[E[Y|X]^2] \\   &= E[Y^2] - 2E[Y^2] + \operatorname{Var}(E[Y|X]) + E[E[Y|X]]^2 \\   &= \operatorname{Var}(E[Y|X]) + E[Y]^2 - E[Y^2]  \\ &= \operatorname{Var}(E[Y|X]) - \operatorname{Var}(Y) \end{split}$$    However, this is off by a factor of $-1$.  Can anyone point out where I went awry?","I'm a first year grad student in Statistics. The book I'm using mentioned conditional variance, and I wanted to read up more about it. I dove down the google rabbit hole and found this website. I read through it and followed the proofs. Then I came to this chunk, and I can't prove it myself. From the definition of conditional variance and the basic property above, it follows that the mean square error when $E(Y∣X)$ is used as a predictor of $Y$ is: $$E([Y−E(Y∣X)]^2 )=E[\operatorname{Var}(Y∣X)]=\operatorname{Var}(Y)−\operatorname{Var}[E(Y∣X)]$$ When I expand the LHS, I get the following: $$\begin{split} E([Y−E(Y∣X)]^2) &= E([Y^2-2YE[Y|X]+E[Y|X]^2) \\   &= E[Y^2] - 2E[YE[Y|X]] + E[E[Y|X]^2] \\   &= E[Y^2] - 2E[Y^2] + \operatorname{Var}(E[Y|X]) + E[E[Y|X]]^2 \\   &= \operatorname{Var}(E[Y|X]) + E[Y]^2 - E[Y^2]  \\ &= \operatorname{Var}(E[Y|X]) - \operatorname{Var}(Y) \end{split}$$    However, this is off by a factor of $-1$.  Can anyone point out where I went awry?",,"['probability', 'statistics', 'conditional-expectation']"
21,Prove Logarithmic function is part of exponential family,Prove Logarithmic function is part of exponential family,,"The aim is to prove that the logarithmic distribution with parameter $p (0<p<1)$ is part of the exponential family and hence, give its canonical parameter. To prove a distribution is part of the exponential family, one must express the probability function in the generic form of $$\exp\left(\frac{y\theta -b (\theta )}{\phi }+ c(y,\phi)\right)$$ where $\theta$  is the canonical parameter. The function for logarithmic is $$f(y;p)=\frac{-1}{\ln(1-p)}\frac{p^{y}}{y}$$ where $y=1,2,\ldots$ I have managed to re-express the function by: 1st step: $$\exp\left(\ln\left(\frac{-p^{y}}{y\ln(1-p)}\right)\right)$$ 2nd step: $$\exp\left(-y\ln (p)-\ln (y)-\ln(\ln(1-p)\right)$$ However, I'm stuck at step 2 and can't expand this further to the generic form. So far, I know (rightly or wrongly): $$c(y,\phi ) = -\ln y$$ $$b(\theta ) = \ln(\theta )$$ $$\phi=1$$ $$\theta=??$$ Can anyone please help? Thanks.","The aim is to prove that the logarithmic distribution with parameter $p (0<p<1)$ is part of the exponential family and hence, give its canonical parameter. To prove a distribution is part of the exponential family, one must express the probability function in the generic form of $$\exp\left(\frac{y\theta -b (\theta )}{\phi }+ c(y,\phi)\right)$$ where $\theta$  is the canonical parameter. The function for logarithmic is $$f(y;p)=\frac{-1}{\ln(1-p)}\frac{p^{y}}{y}$$ where $y=1,2,\ldots$ I have managed to re-express the function by: 1st step: $$\exp\left(\ln\left(\frac{-p^{y}}{y\ln(1-p)}\right)\right)$$ 2nd step: $$\exp\left(-y\ln (p)-\ln (y)-\ln(\ln(1-p)\right)$$ However, I'm stuck at step 2 and can't expand this further to the generic form. So far, I know (rightly or wrongly): $$c(y,\phi ) = -\ln y$$ $$b(\theta ) = \ln(\theta )$$ $$\phi=1$$ $$\theta=??$$ Can anyone please help? Thanks.",,"['statistics', 'probability-distributions']"
22,Let X be a random variable with PDF fx. Find the PDF of the random variable |X| in the following,Let X be a random variable with PDF fx. Find the PDF of the random variable |X| in the following,,"Here's my question: X is uniformly distributed in the interval $[-1,2]$. Find pdf of $|X|$... So I did P($|X| \le x$) = P($-x \le X \le x$)... From here I'm not too sure how to proceed. I know the pdf for X is 1/3 because X $\in$ $[-1,2]$ and 1/2-(-1) = 1/3. So is $|X|$ $\in$ $[0,2]$ and |X|'s pdf = 1/2? Sorry if this seems rudimentary, thank you for helping.","Here's my question: X is uniformly distributed in the interval $[-1,2]$. Find pdf of $|X|$... So I did P($|X| \le x$) = P($-x \le X \le x$)... From here I'm not too sure how to proceed. I know the pdf for X is 1/3 because X $\in$ $[-1,2]$ and 1/2-(-1) = 1/3. So is $|X|$ $\in$ $[0,2]$ and |X|'s pdf = 1/2? Sorry if this seems rudimentary, thank you for helping.",,"['probability', 'statistics', 'probability-distributions', 'uniform-distribution']"
23,How to compute of each player winning this sequence of games?,How to compute of each player winning this sequence of games?,,"Players A and B play a sequence of independent games. Player A throws a die first and wins on a ""six."" If A fails, then player B throws and wins on a ""five"" or ""six."" If B fails, then A throws and wins on a ""four,"" ""five,"" or ""six."" And so on. How to find the probability of each player winning the sequence?","Players A and B play a sequence of independent games. Player A throws a die first and wins on a ""six."" If A fails, then player B throws and wins on a ""five"" or ""six."" If B fails, then A throws and wins on a ""four,"" ""five,"" or ""six."" And so on. How to find the probability of each player winning the sequence?",,"['probability', 'statistics', 'probability-theory', 'dice']"
24,What is asymmetry index in an array of numbers?,What is asymmetry index in an array of numbers?,,"Someone gave me this question , and I cannot find any source that can be helpful. Find the asymmetry index given an array of numbers and an integer ->   find asymmetry index of the array Note: I posted this question at stackoverflow, and I have been told it is related to statistic. Can any one have some thoughts or how to answer this question?","Someone gave me this question , and I cannot find any source that can be helpful. Find the asymmetry index given an array of numbers and an integer ->   find asymmetry index of the array Note: I posted this question at stackoverflow, and I have been told it is related to statistic. Can any one have some thoughts or how to answer this question?",,['statistics']
25,Minimum number of samples to take so that proportion of smokers in sample is within a certain threshold?,Minimum number of samples to take so that proportion of smokers in sample is within a certain threshold?,,"What is the minimum number of random samples that should be taken so that with probability at least 0.95, the proportion of smokers in the sample will not differ from the unknown population of smokers by more than $\pm 0.04$? I am given the added information that: If $Z$ is $N(0,1)$, then $P(Z > 1.96) = 0.025$ This is exam revision. I can't seem to find the procedure for this in my notes so I don't know how to approach it.","What is the minimum number of random samples that should be taken so that with probability at least 0.95, the proportion of smokers in the sample will not differ from the unknown population of smokers by more than $\pm 0.04$? I am given the added information that: If $Z$ is $N(0,1)$, then $P(Z > 1.96) = 0.025$ This is exam revision. I can't seem to find the procedure for this in my notes so I don't know how to approach it.",,"['probability', 'statistics', 'normal-distribution']"
26,Statistics - Finding the median,Statistics - Finding the median,,"Problem : x =    0-4 , 4-10 , 10-18 , 18-30 , 30-40 f(x) = 15 , 35 , 20 , 20 , 10 Finding the median? This is that I did : Cumulative frequency distribution : F(x) = 15 , 50 , 70 , 90 , 100 Midpoint : 2 , 7 , 14 , 24 , 35 Formula : $Md = L_0 + \frac{\frac{n}{2}-F(x_{m-1})}{f(x_m)}*(L_1-L_0)$ n=100, $$\frac{100}{2} = 50$$ But F(x) = 50? I don't understand how to continue? Thanks.","Problem : x =    0-4 , 4-10 , 10-18 , 18-30 , 30-40 f(x) = 15 , 35 , 20 , 20 , 10 Finding the median? This is that I did : Cumulative frequency distribution : F(x) = 15 , 50 , 70 , 90 , 100 Midpoint : 2 , 7 , 14 , 24 , 35 Formula : $Md = L_0 + \frac{\frac{n}{2}-F(x_{m-1})}{f(x_m)}*(L_1-L_0)$ n=100, $$\frac{100}{2} = 50$$ But F(x) = 50? I don't understand how to continue? Thanks.",,['statistics']
27,Finding asymptotes given data,Finding asymptotes given data,,"Background : I asked this question on Stack Overflow about how to program in Java or VBA a method to calculate asymptotes given a range of data points. I believe the underlying question would be more appropriate here than on SO - if I understand the statistical way of solving the problem, I will be able to solve it programmatically. Problem : We are given $n \in [5,15]$ numbers on the interval $]0,1]$ that are the measured approximations of some real-life phenomena, call them $s_1, s_2, \cdots, s_n$. They tend to be decreasing so that $s_i<s_{i+1}$ (although they are approximations so that it's not always so). Looking at them on a graph, we see that it appears they have a horizontal asymptote as $n \rightarrow \infty$. Example: i   value -   - 1   0.8232 2   0.6032 3   0.5012 4   0.4646 5   0.45001 6   0.44981 which gives the following chart The horizontal asymptote would be $y=a$ with $a$ being some number less than $s_n$. In this case, it seems like $a$ is close to $0.44$. I have two questions: How do we find this asymptote if we do not know the underlying distribution? (I guess we assume the formula $e^{ax}+b$, is this true?) How do we find the asymptote for some confidence interval, say 95%? Do we then assume that the measurements are accurate or should we assume that each $s_i$ follows a normal distribution on its true value for some low standard deviation $\sigma_i$ (so that the chance that the true value corresponding to $s_i$ has a 67% change of being within $[s_i-\sigma_i,s_i+\sigma_i]$)?","Background : I asked this question on Stack Overflow about how to program in Java or VBA a method to calculate asymptotes given a range of data points. I believe the underlying question would be more appropriate here than on SO - if I understand the statistical way of solving the problem, I will be able to solve it programmatically. Problem : We are given $n \in [5,15]$ numbers on the interval $]0,1]$ that are the measured approximations of some real-life phenomena, call them $s_1, s_2, \cdots, s_n$. They tend to be decreasing so that $s_i<s_{i+1}$ (although they are approximations so that it's not always so). Looking at them on a graph, we see that it appears they have a horizontal asymptote as $n \rightarrow \infty$. Example: i   value -   - 1   0.8232 2   0.6032 3   0.5012 4   0.4646 5   0.45001 6   0.44981 which gives the following chart The horizontal asymptote would be $y=a$ with $a$ being some number less than $s_n$. In this case, it seems like $a$ is close to $0.44$. I have two questions: How do we find this asymptote if we do not know the underlying distribution? (I guess we assume the formula $e^{ax}+b$, is this true?) How do we find the asymptote for some confidence interval, say 95%? Do we then assume that the measurements are accurate or should we assume that each $s_i$ follows a normal distribution on its true value for some low standard deviation $\sigma_i$ (so that the chance that the true value corresponding to $s_i$ has a 67% change of being within $[s_i-\sigma_i,s_i+\sigma_i]$)?",,"['statistics', 'extrapolation']"
28,"Probability of Getting a ""Perfect Score"" in the Card Matching Game Concentration","Probability of Getting a ""Perfect Score"" in the Card Matching Game Concentration",,"A person is playing the card matching game concentration . There are 40 cards, 20 pairs total. All the cards are shuffled and placed at random face down. A turn consists of two moves and a move is simply turning a card face up to see what it is. If both cards are matched during a turn they are removed form the game and the turn total goes up by one. If the cards are not matched they are returned to the face down position and again, the turn total goes up by one. My questions: 1. What is the probability of getting a perfect score (20 turns ) in a game of concentration? 2. How do you go about finding this probability?","A person is playing the card matching game concentration . There are 40 cards, 20 pairs total. All the cards are shuffled and placed at random face down. A turn consists of two moves and a move is simply turning a card face up to see what it is. If both cards are matched during a turn they are removed form the game and the turn total goes up by one. If the cards are not matched they are returned to the face down position and again, the turn total goes up by one. My questions: 1. What is the probability of getting a perfect score (20 turns ) in a game of concentration? 2. How do you go about finding this probability?",,"['probability', 'statistics', 'card-games']"
29,Dice Rolling 4d10 with a twist,Dice Rolling 4d10 with a twist,,"Suppose I roll two 10-sided dice, 1 die has numbers o, 10, 20, 30 etc to 90.  The second die has numbers 0, 1 ,2 etc to 9.  These dice are used to create a number from 1 to 100 - example: the first die rolls a 40, the second die rolls a 9 = the number 49.  The number 100 is garnered by both dice rolling 0. If I do this twice, i.e. two people rolling at the same time (realizing that ""at the same time"" doesn't really apply - dice could care less what time it is) What is the probability and/or odds of both set of dice coming out the same number? (without deciding what that number is before the roll) AND What is the probability that both sets of dice would match a specific number say, 49? AND Are the probabilities any different if you were to use a single 100 faced die? Please feel free to explain in detail, I am not a math novice, but am not well versed in statistics.","Suppose I roll two 10-sided dice, 1 die has numbers o, 10, 20, 30 etc to 90.  The second die has numbers 0, 1 ,2 etc to 9.  These dice are used to create a number from 1 to 100 - example: the first die rolls a 40, the second die rolls a 9 = the number 49.  The number 100 is garnered by both dice rolling 0. If I do this twice, i.e. two people rolling at the same time (realizing that ""at the same time"" doesn't really apply - dice could care less what time it is) What is the probability and/or odds of both set of dice coming out the same number? (without deciding what that number is before the roll) AND What is the probability that both sets of dice would match a specific number say, 49? AND Are the probabilities any different if you were to use a single 100 faced die? Please feel free to explain in detail, I am not a math novice, but am not well versed in statistics.",,"['probability', 'statistics']"
30,Advantage of Bootstrapping Confidence Intervals over Standard Error [closed],Advantage of Bootstrapping Confidence Intervals over Standard Error [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 9 years ago . Improve this question I want to compare means from samples of varying size, plug them into a bar graph and have appropriate values to plug in for the error bars. My sample sizes are: A = 1 B = 3 C = 14 D = 7 E = 13 F = 190 G = 33 I intended to use the typical standard error calculation Standard Error = Standard Deviation/number of sample^1/2 but because many of my samples are so small, a colleague suggested I use bootstrapping to compute confidence intervals instead. He's smarter than me so I found some software that did that (scikits.bootstrap==0.3.1 for python in a pandas environment), computed my bootstrapping confidence intervals and used them to put error bars on my graph. What I want to know is WHY exactly bootstrapping is more appropriate than standard error in this case? I understand that bootstrapping works by creating many alternative versions of your sample by sampling-with-replacement and computing some descriptive statistics on that large population of hypothetical samples but I don't understand when and why you should use it instead of using the standard error. NOTE: I'm not applying these calculations to sample A because its just a single value; I'm actually dismissing it from consideration. I only included it here because it was easier to copy and paste the whole list rather than remove A and change all the letters","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 9 years ago . Improve this question I want to compare means from samples of varying size, plug them into a bar graph and have appropriate values to plug in for the error bars. My sample sizes are: A = 1 B = 3 C = 14 D = 7 E = 13 F = 190 G = 33 I intended to use the typical standard error calculation Standard Error = Standard Deviation/number of sample^1/2 but because many of my samples are so small, a colleague suggested I use bootstrapping to compute confidence intervals instead. He's smarter than me so I found some software that did that (scikits.bootstrap==0.3.1 for python in a pandas environment), computed my bootstrapping confidence intervals and used them to put error bars on my graph. What I want to know is WHY exactly bootstrapping is more appropriate than standard error in this case? I understand that bootstrapping works by creating many alternative versions of your sample by sampling-with-replacement and computing some descriptive statistics on that large population of hypothetical samples but I don't understand when and why you should use it instead of using the standard error. NOTE: I'm not applying these calculations to sample A because its just a single value; I'm actually dismissing it from consideration. I only included it here because it was easier to copy and paste the whole list rather than remove A and change all the letters",,['statistics']
31,I need help understanding this proof about convergence in distribution,I need help understanding this proof about convergence in distribution,,"The proof says that we used the fact that $(1-\epsilon)^\frac{x}{\epsilon} \rightarrow e^{-x}$ Why is this so? How do I prove this? Also, why do we need the fact that $\lfloor x/p_n \rfloor - x/p_n$ is bounded? Thanks","The proof says that we used the fact that $(1-\epsilon)^\frac{x}{\epsilon} \rightarrow e^{-x}$ Why is this so? How do I prove this? Also, why do we need the fact that $\lfloor x/p_n \rfloor - x/p_n$ is bounded? Thanks",,"['probability', 'statistics', 'probability-theory']"
32,Finding mean from die probability,Finding mean from die probability,,"Example 4.4.5: Suppose that there is a 6-sided die that is weighted in   such a way that each time the die is rolled, the probabilities of   rolling any of the numbers from 1 to 5 are all equal, but the   probability of rolling a 6 is twice the probability of roll- ing a 1.   When you roll the die once, the 6 outcomes are not equally likely.   What are the probabilities of the 6 outcomes? On the basis of the above question  a question has been asked to evaluate the mean. My problem is that, when they  evaluate the mean, they multiplied with the probability by 1, 2 ,  3  , 4 , 5 and  6 why?  In die , all are they equal probable.","Example 4.4.5: Suppose that there is a 6-sided die that is weighted in   such a way that each time the die is rolled, the probabilities of   rolling any of the numbers from 1 to 5 are all equal, but the   probability of rolling a 6 is twice the probability of roll- ing a 1.   When you roll the die once, the 6 outcomes are not equally likely.   What are the probabilities of the 6 outcomes? On the basis of the above question  a question has been asked to evaluate the mean. My problem is that, when they  evaluate the mean, they multiplied with the probability by 1, 2 ,  3  , 4 , 5 and  6 why?  In die , all are they equal probable.",,['probability']
33,"if $X_i$ are iid standard normal distributed, what is the limiting distribution of $\sum X^4 / (\sum X^2)^2$?","if  are iid standard normal distributed, what is the limiting distribution of ?",X_i \sum X^4 / (\sum X^2)^2,"If $X_i$, $i=1,\ldots,n$ are iid standard normal distributed, what is the limiting distribution of $S_n=\sum X^4 / (\sum X^2)^2$? After finding the moments and since $Cov(X^4, X^2)=0$, I have the bivariate normal $\sqrt{n} (\frac{1}{n}\sum X^4-\mu_1)\rightarrow N(0,\sigma_1^2)$ $\sqrt{n} (\frac{1}{n}\sum X^2 - \mu_2)\rightarrow N(0,\sigma_2^2)$ with $Cov(X^2,Cov^4)=0$ (not sure how to type latex matrices) and using multivariate delta method $h(x,y)=\frac{x}{y^2}$, I get $\sqrt{n}(nS_n-\frac{\mu_1}{\mu_2^2})\rightarrow N(0,\nabla h^T\sigma \nabla h)$. My question is - is this the right form for finding the limiting distribution of $S_n$? I thought the form should be $\sqrt{n}(S_n-\mu)\rightarrow N(0,\sigma^2)$, but here I have an 'extra' $n$ as the coefficient of $S_n$).","If $X_i$, $i=1,\ldots,n$ are iid standard normal distributed, what is the limiting distribution of $S_n=\sum X^4 / (\sum X^2)^2$? After finding the moments and since $Cov(X^4, X^2)=0$, I have the bivariate normal $\sqrt{n} (\frac{1}{n}\sum X^4-\mu_1)\rightarrow N(0,\sigma_1^2)$ $\sqrt{n} (\frac{1}{n}\sum X^2 - \mu_2)\rightarrow N(0,\sigma_2^2)$ with $Cov(X^2,Cov^4)=0$ (not sure how to type latex matrices) and using multivariate delta method $h(x,y)=\frac{x}{y^2}$, I get $\sqrt{n}(nS_n-\frac{\mu_1}{\mu_2^2})\rightarrow N(0,\nabla h^T\sigma \nabla h)$. My question is - is this the right form for finding the limiting distribution of $S_n$? I thought the form should be $\sqrt{n}(S_n-\mu)\rightarrow N(0,\sigma^2)$, but here I have an 'extra' $n$ as the coefficient of $S_n$).",,"['statistics', 'normal-distribution', 'central-limit-theorem']"
34,Prove the computational formula of Anderson-Darling test statistic.,Prove the computational formula of Anderson-Darling test statistic.,,"The Anderson-Darling test statistic is defined as $$n\int_{-\infty}^\infty \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))}dF(x)$$ and there is a computational formula $$A^2 = -n - S$$ where $$S = \sum_{k=1}^n\frac{2k-1}{n}\left(\ln F(Y_k) + \ln(1 - F(Y_{n+1-k}))\right)$$ $F_n(x)$ is the empirical distribution function and $F(x)$ is the cumulative distribution to which we are comparing the sample. $Y_k$ is the $k^\text{th}$ ranked element in the sample. Many books or journals I found all give these two formulas but don't give the reason and derivations. I tried to divide the integral into sub-intervals like $[Y_i, Y_{i+1}]$ to prove it, but I failed. So I want to how to prove it. Thanks!!","The Anderson-Darling test statistic is defined as $$n\int_{-\infty}^\infty \frac{(F_n(x) - F(x))^2}{F(x)(1 - F(x))}dF(x)$$ and there is a computational formula $$A^2 = -n - S$$ where $$S = \sum_{k=1}^n\frac{2k-1}{n}\left(\ln F(Y_k) + \ln(1 - F(Y_{n+1-k}))\right)$$ $F_n(x)$ is the empirical distribution function and $F(x)$ is the cumulative distribution to which we are comparing the sample. $Y_k$ is the $k^\text{th}$ ranked element in the sample. Many books or journals I found all give these two formulas but don't give the reason and derivations. I tried to divide the integral into sub-intervals like $[Y_i, Y_{i+1}]$ to prove it, but I failed. So I want to how to prove it. Thanks!!",,['statistics']
35,Question about Logistic Regression - 4,Question about Logistic Regression - 4,,"I am currently studying on logistic regression. So I have found a document on the Internet explaining about it. Somehow, it explains Bernoulli distribution in the beginning and I am having a problem to understand the equation below. I can refer to the variance explanation in wikipedia, it is still not clear for me to understand. I am new to this, and reading the paper line by line. If you think you can give me an explanation, I will be really grateful. I hope I have provided enough information for you to give me an explanation. Thank you.","I am currently studying on logistic regression. So I have found a document on the Internet explaining about it. Somehow, it explains Bernoulli distribution in the beginning and I am having a problem to understand the equation below. I can refer to the variance explanation in wikipedia, it is still not clear for me to understand. I am new to this, and reading the paper line by line. If you think you can give me an explanation, I will be really grateful. I hope I have provided enough information for you to give me an explanation. Thank you.",,"['statistics', 'statistical-inference', 'descriptive-statistics']"
36,"MLE of MVN($\mu, \Sigma$)",MLE of MVN(),"\mu, \Sigma","I'm trying to find MLE of MVN( $\mu, \Sigma$ ), i.e $N_k(\mu, \Sigma)$ with random sample $X_i, 1\le i \le n$ . It was easy to get $\widehat{\mu}= \bar{X}$ and $\hat{\Sigma} = \frac{1}{n} \sum_i (X_i - \bar{X})(X_i - \bar{X})'$ by using matrix differentiation. However, to be rigorous I need to explain the followings. log likelihood function is continuous and differentiable with respect to the parameters, i.e $(\mu, \Sigma)$ . Log-likelihood function goes to $-\infty$ as the parameter goes to its boundary. As far as I know, second derivative of log-likelihood(observed information) is negative definite and #2 are sufficient conditions for existence of unique MLE. Here's my opinion. About #1: Since log likelihood function has quadratic terms of $\mu$ , log likelihood function is continuous and differntiable w.r.t $\mu$ . Meanwhile, I can't explain why determinant of covariance matrix, which appears in log likelihood function, is continous and differentiable w.r.t $\Sigma$ . About #2: I also understand this relating to $\mu$ , but not $\Sigma$ . In other words, I wonder how to show $-\frac{n}{2}\log|\Sigma|-\frac{1}{2}\sum_i (X_i-\bar{X})'\Sigma^{-1}(X_i-\bar{X})$ , this function has diminishing boundary as parameters of $\Sigma$ goes to its boundary. Thanks for any comment in advance.","I'm trying to find MLE of MVN( ), i.e with random sample . It was easy to get and by using matrix differentiation. However, to be rigorous I need to explain the followings. log likelihood function is continuous and differentiable with respect to the parameters, i.e . Log-likelihood function goes to as the parameter goes to its boundary. As far as I know, second derivative of log-likelihood(observed information) is negative definite and #2 are sufficient conditions for existence of unique MLE. Here's my opinion. About #1: Since log likelihood function has quadratic terms of , log likelihood function is continuous and differntiable w.r.t . Meanwhile, I can't explain why determinant of covariance matrix, which appears in log likelihood function, is continous and differentiable w.r.t . About #2: I also understand this relating to , but not . In other words, I wonder how to show , this function has diminishing boundary as parameters of goes to its boundary. Thanks for any comment in advance.","\mu, \Sigma N_k(\mu, \Sigma) X_i, 1\le i \le n \widehat{\mu}= \bar{X} \hat{\Sigma} = \frac{1}{n} \sum_i (X_i - \bar{X})(X_i - \bar{X})' (\mu, \Sigma) -\infty \mu \mu \Sigma \mu \Sigma -\frac{n}{2}\log|\Sigma|-\frac{1}{2}\sum_i (X_i-\bar{X})'\Sigma^{-1}(X_i-\bar{X}) \Sigma","['statistics', 'normal-distribution']"
37,Markov chains by hand,Markov chains by hand,,"If I have a starting point: $A_T=[0,1]$ at $T=1$ and a one step transition matrix of: $B=\left[ \begin{align} &\frac34 & \frac14& \\& \frac1{20}& \frac {19}{20} &\end{align} \right]$ I can compute $A_nB=A_{n+1}$ And keep reiterating this, but it is slow by hand, I assume there is a faster way? Would diagonalizing the matrix help somehow?","If I have a starting point: $A_T=[0,1]$ at $T=1$ and a one step transition matrix of: $B=\left[ \begin{align} &\frac34 & \frac14& \\& \frac1{20}& \frac {19}{20} &\end{align} \right]$ I can compute $A_nB=A_{n+1}$ And keep reiterating this, but it is slow by hand, I assume there is a faster way? Would diagonalizing the matrix help somehow?",,"['probability', 'matrices', 'statistics', 'markov-chains']"
38,standard deviation of x= 121 divided by 121,standard deviation of x= 121 divided by 121,,"$\{X_1, X2, \ldots, X_{121}\}$ are independent and identically distributed random variables such that $E(X_i)= 3$ and $\mathrm{Var}(X_i)= 25$.  What is the standard deviation of their average?  In other words, what is the standard deviation of $\bar X= {X_1+ X_2+ \cdots + X_{121} \over 121}$?","$\{X_1, X2, \ldots, X_{121}\}$ are independent and identically distributed random variables such that $E(X_i)= 3$ and $\mathrm{Var}(X_i)= 25$.  What is the standard deviation of their average?  In other words, what is the standard deviation of $\bar X= {X_1+ X_2+ \cdots + X_{121} \over 121}$?",,['statistics']
39,Expectations and variance with rolling a dice 10 times,Expectations and variance with rolling a dice 10 times,,"Let's say you roll a fair dice 10 times and X is the number of sides that never show up. (i.e. Roll 1 - 10 = 1424145221, X = 2 because 3 and 6 never show up) Values of $N=0,1,2,3,4,5.\\ P(N=6) = 0$ because at least one of the numbers has to show up. SOLUTION: Use inclusion-exclusion","Let's say you roll a fair dice 10 times and X is the number of sides that never show up. (i.e. Roll 1 - 10 = 1424145221, X = 2 because 3 and 6 never show up) Values of $N=0,1,2,3,4,5.\\ P(N=6) = 0$ because at least one of the numbers has to show up. SOLUTION: Use inclusion-exclusion",,"['probability', 'statistics', 'summation', 'expectation', 'dice']"
40,"Is the variation of the mean 0, and the variance of standard deviation just the variance?","Is the variation of the mean 0, and the variance of standard deviation just the variance?",,"Just trying to get some properties straight. Was looking at a problem online which reads: Let X be a random variable with mean μ and variance σ2. What is the variance of X/σ+10μ? The solution of which is: $Var(X/σ+10μ)=Var(X/σ)=Var(X)/σ2=1$. I assume this means $Var(X/σ+10μ)$ expands to $Var(X/σ)$ + $Var(10μ)$, then implying $Var(10μ)=0$. Next, I assume $Var(X/σ)$ expands to $Var(X)/Var(σ)$, implying $Var(σ)=σ^2$. Is this correct? Are these universal properties for all distributions? Or am I way off target?","Just trying to get some properties straight. Was looking at a problem online which reads: Let X be a random variable with mean μ and variance σ2. What is the variance of X/σ+10μ? The solution of which is: $Var(X/σ+10μ)=Var(X/σ)=Var(X)/σ2=1$. I assume this means $Var(X/σ+10μ)$ expands to $Var(X/σ)$ + $Var(10μ)$, then implying $Var(10μ)=0$. Next, I assume $Var(X/σ)$ expands to $Var(X)/Var(σ)$, implying $Var(σ)=σ^2$. Is this correct? Are these universal properties for all distributions? Or am I way off target?",,"['statistics', 'standard-deviation']"
41,Measuring Variance of 2D Data Points,Measuring Variance of 2D Data Points,,"I have a list of weighted data points on a 2D plane in the form $(x, y)$.  I believe the mean can calculated as $\left(\frac{\sum{x_i\times w_i}}{\sum w_i}, \frac{\sum{y_i\times w_i}}{\sum w_i}\right)$. What is the best way to calculate a single value which will accurately describe the spread of the data points?","I have a list of weighted data points on a 2D plane in the form $(x, y)$.  I believe the mean can calculated as $\left(\frac{\sum{x_i\times w_i}}{\sum w_i}, \frac{\sum{y_i\times w_i}}{\sum w_i}\right)$. What is the best way to calculate a single value which will accurately describe the spread of the data points?",,"['statistics', 'descriptive-statistics']"
42,How to find the expectation value?,How to find the expectation value?,,"Suppose that an insurer has an exponential utility function $u(x)=−2e^{-2x}$. What is the minimum premium $P^{-}$ to be asked for a risk X? After solving this we reached the following, So,only need help to solve the last step.","Suppose that an insurer has an exponential utility function $u(x)=−2e^{-2x}$. What is the minimum premium $P^{-}$ to be asked for a risk X? After solving this we reached the following, So,only need help to solve the last step.",,"['probability', 'statistics', 'probability-theory', 'finance', 'risk-assessment']"
43,"Constructing a function similar to x^3 between [0,1]","Constructing a function similar to x^3 between [0,1]",,"I'm trying to construct a function $f$, in order to normalize a dataset(obviously where all the element come from $[0,1] \in \mathbb{R}$. The big picture is that the envisioned $f: [0,1] \rightarrow [0,1]$ pushes the values that fall to the right side of the initial average of the dataset to $1$, and similarly the values that fall to the left side of the average to $0$. But I want the function to act on the relatively very big and very small numbers in a stronger manner. So basically the function will look similar to $x^3$'s general pattern. Additionally I'd like to fix some values as follows: f(0) = 0, f(1) = 1, f(avg) = avg, where avg stand for the average of the initial dataset. Currently I'm having problem with fixing the endpoints. For instance $(x-avg)^3+avg$ would get me $f(avg)=avg$ but not the other two. I open to using some other formula as long as it adheres to my desired properties. Please let me know if something regarding the problem description is not clear.","I'm trying to construct a function $f$, in order to normalize a dataset(obviously where all the element come from $[0,1] \in \mathbb{R}$. The big picture is that the envisioned $f: [0,1] \rightarrow [0,1]$ pushes the values that fall to the right side of the initial average of the dataset to $1$, and similarly the values that fall to the left side of the average to $0$. But I want the function to act on the relatively very big and very small numbers in a stronger manner. So basically the function will look similar to $x^3$'s general pattern. Additionally I'd like to fix some values as follows: f(0) = 0, f(1) = 1, f(avg) = avg, where avg stand for the average of the initial dataset. Currently I'm having problem with fixing the endpoints. For instance $(x-avg)^3+avg$ would get me $f(avg)=avg$ but not the other two. I open to using some other formula as long as it adheres to my desired properties. Please let me know if something regarding the problem description is not clear.",,"['linear-algebra', 'statistics', 'functions', 'discrete-mathematics', 'continuity']"
44,Proving variance of U-statistics is decreasing,Proving variance of U-statistics is decreasing,,"I read Wassily Hoeffding's paper ""a class of statistics with asymptotically normal distribution"". In proving ""$n\sigma^{2}(U_{n})$ is decreasing in n"" in Theorem 5.2, it simply says ""using (5.33) and (5.31)"". Yet this is not obvious to me. Is there any other proof I can find to read? Thank you.","I read Wassily Hoeffding's paper ""a class of statistics with asymptotically normal distribution"". In proving ""$n\sigma^{2}(U_{n})$ is decreasing in n"" in Theorem 5.2, it simply says ""using (5.33) and (5.31)"". Yet this is not obvious to me. Is there any other proof I can find to read? Thank you.",,['statistics']
45,How to interpret right hand side of Cumulative Distribution Function,How to interpret right hand side of Cumulative Distribution Function,,"What is the type (?) of this term: $X \leq x$ in the definition of the CDF? $F_{X}(x) = P(X \leq x)$ Is $X \leq x$ a set? Is it real valued? I know that ${P}$ is a function that maps to $[0,1]$. Does it always map a set to $[0,1]$? How would I read this aloud?","What is the type (?) of this term: $X \leq x$ in the definition of the CDF? $F_{X}(x) = P(X \leq x)$ Is $X \leq x$ a set? Is it real valued? I know that ${P}$ is a function that maps to $[0,1]$. Does it always map a set to $[0,1]$? How would I read this aloud?",,['statistics']
46,Contradiction with complex gaussians...,Contradiction with complex gaussians...,,"So, I am computing something seemingly simple involving complex gaussians and constants, but I am getting a big contradiction in my calculations. The setup: Let $C$ be a complex constant, that is, $C = c_r + jc_i$. Let $G$ be a complex gaussian variable, $G = g_r + jg_i$, where $g_r$ and $g_i$ are both uncorrelated, and where each are $\sim\mathcal{N}(0,\sigma^2)$. I am computing $z = |C + G|^2$. The problem: Now, before I go on, it is obvious that the variable $z$, must always be greater than or equal to $0$, owning to the $| \cdot |^2$ operation. However when I open up and compute $z$, I get an expression that seems like it CAN be less than $0$. Opening up $z$, I get $$ z = (c_r^2 + c_i^2) + 2\Big[c_rg_r + c_ig_i \Big] + (g_r^2 + g_i^2) $$ The first term is a constant, and will always be greater than or equal to zero. The last term is has a gamma distribution, and by definition, will also always be greater than or equal to zero. However, the middle term is simply a summation of two gaussians, but this means that there is a finite probability that they take on a value of less than zero, meaning that $z$ can also be less than zero! But $z$ can never be less than zero. This is the contradiction... I am not sure where I am making a mistake in my reasoning... Thank you.","So, I am computing something seemingly simple involving complex gaussians and constants, but I am getting a big contradiction in my calculations. The setup: Let $C$ be a complex constant, that is, $C = c_r + jc_i$. Let $G$ be a complex gaussian variable, $G = g_r + jg_i$, where $g_r$ and $g_i$ are both uncorrelated, and where each are $\sim\mathcal{N}(0,\sigma^2)$. I am computing $z = |C + G|^2$. The problem: Now, before I go on, it is obvious that the variable $z$, must always be greater than or equal to $0$, owning to the $| \cdot |^2$ operation. However when I open up and compute $z$, I get an expression that seems like it CAN be less than $0$. Opening up $z$, I get $$ z = (c_r^2 + c_i^2) + 2\Big[c_rg_r + c_ig_i \Big] + (g_r^2 + g_i^2) $$ The first term is a constant, and will always be greater than or equal to zero. The last term is has a gamma distribution, and by definition, will also always be greater than or equal to zero. However, the middle term is simply a summation of two gaussians, but this means that there is a finite probability that they take on a value of less than zero, meaning that $z$ can also be less than zero! But $z$ can never be less than zero. This is the contradiction... I am not sure where I am making a mistake in my reasoning... Thank you.",,"['probability', 'complex-analysis', 'algebra-precalculus', 'statistics', 'complex-numbers']"
47,the R (programming language) library for random variable operations,the R (programming language) library for random variable operations,,"Does anyone know if the R library provides tools to perform some basic algebraic operations on r.v? Matlab doesn't have this (as far as I know), but if anyone knows about the existence of an extension to do this it would be great. Thanks for your kindness","Does anyone know if the R library provides tools to perform some basic algebraic operations on r.v? Matlab doesn't have this (as far as I know), but if anyone knows about the existence of an extension to do this it would be great. Thanks for your kindness",,"['probability', 'statistics']"
48,Find the probability distribution for the number of spades.,Find the probability distribution for the number of spades.,,Three cards are drawn in succession from a deck without replacement. Find the probability distribution for the number of spades.,Three cards are drawn in succession from a deck without replacement. Find the probability distribution for the number of spades.,,"['probability', 'statistics', 'probability-distributions']"
49,Showing the normal distribution has points of inflections at $x = \mu \pm \sigma$ and a maximum at $x = \mu$,Showing the normal distribution has points of inflections at  and a maximum at,x = \mu \pm \sigma x = \mu,"$X \sim N(\mu, \sigma^2)$ I.e. the density of $X$ is the normal distribution. I am looking to show that $f_X(x)$ has points of inflections at $x = \mu \pm \sigma$. In my notes it says that we should work with $ln(f_X(x))$ instead of $f_X(x)$ directly as the answer will be equivalent as $ln$ is an increasing function. When I get the first derivative of $ln(f_X(x))$ w.r.t. $x$ I get $\frac{-x + u}{\sigma^2}$. This implies a critical point of $x$. Then the second derivative is $\frac{-1}{\sigma^2} < 0$ so $x$ is a maximum. But how do I show the points of inflection are $x = \mu \pm \sigma$?","$X \sim N(\mu, \sigma^2)$ I.e. the density of $X$ is the normal distribution. I am looking to show that $f_X(x)$ has points of inflections at $x = \mu \pm \sigma$. In my notes it says that we should work with $ln(f_X(x))$ instead of $f_X(x)$ directly as the answer will be equivalent as $ln$ is an increasing function. When I get the first derivative of $ln(f_X(x))$ w.r.t. $x$ I get $\frac{-x + u}{\sigma^2}$. This implies a critical point of $x$. Then the second derivative is $\frac{-1}{\sigma^2} < 0$ so $x$ is a maximum. But how do I show the points of inflection are $x = \mu \pm \sigma$?",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution']"
50,Probability distribution of count of factors for all numbers,Probability distribution of count of factors for all numbers,,"Is the following known? Define ""factor count"" as the number of prime factors of the number, minus 1. For example: Prime numbers have a factor count of 1-1 = 0 4 has a factor count of (2 and 2)-1 = 1 20 has a factor count of (2 and 2 and 5)-1 = 2 24 has a factor count of (2 and 2 and 2 and 3)-1 = 3 etc. When you plot the factor counts of all numbers, it becomes a Poisson distribution with $\lambda=e$.  I have written a program that shows this.","Is the following known? Define ""factor count"" as the number of prime factors of the number, minus 1. For example: Prime numbers have a factor count of 1-1 = 0 4 has a factor count of (2 and 2)-1 = 1 20 has a factor count of (2 and 2 and 5)-1 = 2 24 has a factor count of (2 and 2 and 2 and 3)-1 = 3 etc. When you plot the factor counts of all numbers, it becomes a Poisson distribution with $\lambda=e$.  I have written a program that shows this.",,"['statistics', 'prime-numbers', 'factoring']"
51,How to use the Cramer-Rao lower bound (CRLB)to show that $\bar{Y}$ is the best unbiased estimator of $\lambda$?,How to use the Cramer-Rao lower bound (CRLB)to show that  is the best unbiased estimator of ?,\bar{Y} \lambda,"Let $Y_1,\ldots,Y_n$ be a random sample from Poisson ($\lambda$). Derive the Cramer-Rao lower bound (CRLB) for the variance of any unbiased for estimator of $\lambda$. MY APPROACH: $$ L(\lambda)=\left(\prod_{i=1}^n \frac{1}{y_i!}\right)\lambda^{\sum_{i=1}^n y_i}e^{-\lambda n} $$ $$ l(\lambda)=-\lambda n+\log(\lambda)\sum_{i=1}^n y_i-\sum_{i=1}^n \log(y_i!).$$ $$\frac{d \ln L(\lambda)}{\lambda} = \frac{1}{\lambda} \sum_{i=1}^n y_i - n$$ $$ \frac{d^2 \ln L(\lambda)}{\lambda} = \frac{-1}{\lambda^2}  \sum_{i=1}^n y_i$$ $$E\left[\frac{-1}{\lambda^2} \sum_{i=1}^n y_i\right] = \frac{-1}{\lambda^2} n \bar{y}$$. Can anyone please confirm what I am doing is correct? If so , how do I use this result to show that $\bar{Y}$ is the best unbiased estimator of $\lambda$?","Let $Y_1,\ldots,Y_n$ be a random sample from Poisson ($\lambda$). Derive the Cramer-Rao lower bound (CRLB) for the variance of any unbiased for estimator of $\lambda$. MY APPROACH: $$ L(\lambda)=\left(\prod_{i=1}^n \frac{1}{y_i!}\right)\lambda^{\sum_{i=1}^n y_i}e^{-\lambda n} $$ $$ l(\lambda)=-\lambda n+\log(\lambda)\sum_{i=1}^n y_i-\sum_{i=1}^n \log(y_i!).$$ $$\frac{d \ln L(\lambda)}{\lambda} = \frac{1}{\lambda} \sum_{i=1}^n y_i - n$$ $$ \frac{d^2 \ln L(\lambda)}{\lambda} = \frac{-1}{\lambda^2}  \sum_{i=1}^n y_i$$ $$E\left[\frac{-1}{\lambda^2} \sum_{i=1}^n y_i\right] = \frac{-1}{\lambda^2} n \bar{y}$$. Can anyone please confirm what I am doing is correct? If so , how do I use this result to show that $\bar{Y}$ is the best unbiased estimator of $\lambda$?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
52,What is the proper way of downsampling a time series containing percentage data?,What is the proper way of downsampling a time series containing percentage data?,,"I have a collection of CPU utilization, the metric is percentage (%). The sample is taken every 10 seconds. I need to use 1 minute resolution data for a calculation. What is the best way of downsampling percentage? The data looks like this: [5 6 10 5 3 12 10] I need to generate a single number out of this that represents the CPU utilization for that minute.","I have a collection of CPU utilization, the metric is percentage (%). The sample is taken every 10 seconds. I need to use 1 minute resolution data for a calculation. What is the best way of downsampling percentage? The data looks like this: [5 6 10 5 3 12 10] I need to generate a single number out of this that represents the CPU utilization for that minute.",,"['statistics', 'time-series']"
53,conjugate prior,conjugate prior,,"A class of sampling distribution is a conjugate family of a prior distribution, if the posterior distribution belongs to the same family for all priors and all samples. Why is this phrase incorrect?","A class of sampling distribution is a conjugate family of a prior distribution, if the posterior distribution belongs to the same family for all priors and all samples. Why is this phrase incorrect?",,"['statistics', 'bayesian']"
54,Card probability,Card probability,,"There are two 10-card decks, consisting of 5 red cards and 5 blue cards each. Both are shuffled separately. One card is then dealt from each deck and compared. This is repeated for all 10 pairs of cards(one of each pair is taken from each deck). What's the chance that at least one pair consists of two cards of the same color? I see that I'm supposed to use the complement as ""at least"" usually suggests that taking the complement is the easier approach. So I'm finding the prob that there are no pairs of the same color. This is where I'm stumped. I have Pr(A) = pair has no 2 reds, Pr(B) = pair has no 2 blues. For Pr(A) I calculated 20 choose 10 / 20 choose 10. But I know 20 choose 10 is wrong. Prob can't equal 1. I'm wondering if I should calculate the prob of no red for each card in the pair and for each blue card as in P(A1) = not red, P(A2) = not red, P(B1) = not blue, P(B2) = not blue. A push in the right direction would be great.","There are two 10-card decks, consisting of 5 red cards and 5 blue cards each. Both are shuffled separately. One card is then dealt from each deck and compared. This is repeated for all 10 pairs of cards(one of each pair is taken from each deck). What's the chance that at least one pair consists of two cards of the same color? I see that I'm supposed to use the complement as ""at least"" usually suggests that taking the complement is the easier approach. So I'm finding the prob that there are no pairs of the same color. This is where I'm stumped. I have Pr(A) = pair has no 2 reds, Pr(B) = pair has no 2 blues. For Pr(A) I calculated 20 choose 10 / 20 choose 10. But I know 20 choose 10 is wrong. Prob can't equal 1. I'm wondering if I should calculate the prob of no red for each card in the pair and for each blue card as in P(A1) = not red, P(A2) = not red, P(B1) = not blue, P(B2) = not blue. A push in the right direction would be great.",,['probability']
55,Normal Ratio Distribution with CDF Method,Normal Ratio Distribution with CDF Method,,"I think I'm missing something glaringly obvious here that's causing problems for me in the entire subject. I have two independent standard normal random variables, X and Y ~N(0,1), and I need to find the density of U=Y/X.  I start with f(x,y)=$\frac1{2\pi}e^{-x^2/2}e^{-y^2/2}$, then set Y=ux, and take a double integral of that.  I'm leaving out my limits of integration, because my problem is that I know that I have to integrate with $\iint$y f(x,y), but I have no idea where that y (which is needed to integrate $e^{-y^2/2}$) comes from.  Is this a Jacobian?  None of my notes or our textbook mentions any use of a Jacobian in the CDF method.  Is this just impossible with the CDF method?","I think I'm missing something glaringly obvious here that's causing problems for me in the entire subject. I have two independent standard normal random variables, X and Y ~N(0,1), and I need to find the density of U=Y/X.  I start with f(x,y)=$\frac1{2\pi}e^{-x^2/2}e^{-y^2/2}$, then set Y=ux, and take a double integral of that.  I'm leaving out my limits of integration, because my problem is that I know that I have to integrate with $\iint$y f(x,y), but I have no idea where that y (which is needed to integrate $e^{-y^2/2}$) comes from.  Is this a Jacobian?  None of my notes or our textbook mentions any use of a Jacobian in the CDF method.  Is this just impossible with the CDF method?",,"['statistics', 'normal-distribution', 'ratio']"
56,Find the probability that the difference between the sample mean and the true population mean wll not exceed 0.5 inch,Find the probability that the difference between the sample mean and the true population mean wll not exceed 0.5 inch,,"an anthropologist wishes to estimate the average height of men for a certain race of people. if the population standard deviation is assumed to be 2.5 inches and if she randomly samples 100 mean, find the probability that the difference between the sample mean and the true population mean will not exceed 0.5 inch.","an anthropologist wishes to estimate the average height of men for a certain race of people. if the population standard deviation is assumed to be 2.5 inches and if she randomly samples 100 mean, find the probability that the difference between the sample mean and the true population mean will not exceed 0.5 inch.",,['statistics']
57,"What does ""central value"" mean?","What does ""central value"" mean?",,"How to calculate central value of the following sets: I'm thinking is the same as the median - is it? $\{-2, -1, 3, 5, 7 , 1, 3 , 6, 2 , -1, -5\}$ and  $\{-2, -1, 3, 5, 7 , 1, 3 , 6, 2 , -1 \}$","How to calculate central value of the following sets: I'm thinking is the same as the median - is it? $\{-2, -1, 3, 5, 7 , 1, 3 , 6, 2 , -1, -5\}$ and  $\{-2, -1, 3, 5, 7 , 1, 3 , 6, 2 , -1 \}$",,['statistics']
58,"If $f (y\mid \theta)=(\theta + 1)y^\theta$, find an estimator for $θ$ by the method of moments.","If , find an estimator for  by the method of moments.",f (y\mid \theta)=(\theta + 1)y^\theta θ,"Let $Y_1, Y_2, . . . , Y_n$ denote a random sample from the probability density function $$f (y \mid \theta)=\begin{cases} (\theta + 1)y^\theta, &  0 < y < 1; \theta > −1,\\ 0 ,& \text{ elsewhere }\end{cases}$$ Find an estimator for $\theta$ by the method of moments. I am told that that $μ = \frac{\theta + 1}{\theta + 2} $ . I am wondering how do they show this ?",Let denote a random sample from the probability density function Find an estimator for by the method of moments. I am told that that . I am wondering how do they show this ?,"Y_1, Y_2, . . . , Y_n f (y \mid \theta)=\begin{cases} (\theta + 1)y^\theta, &  0 < y < 1; \theta > −1,\\ 0 ,& \text{ elsewhere }\end{cases} \theta μ = \frac{\theta + 1}{\theta + 2} ","['probability', 'statistics', 'probability-theory', 'probability-distributions']"
59,"Given $X$ and $Y$ are independent N(0,1) random variables and $Z = \sqrt{X^2+Y^2}$ from the marginal pdf of $Z$","Given  and  are independent N(0,1) random variables and  from the marginal pdf of",X Y Z = \sqrt{X^2+Y^2} Z,Let $X$ and $Y$ be independent $N(0; 1)$ random variables. Let $Z = \sqrt{X^2+Y^2}$. (a) Derive the marginal pdf of $Z$ and then using the marginal pdf to compute ${\rm E}[Z^2]$ (b) Can you propose a different way other than that in (a) to compute ${\rm E}[Z^2]$ (c) Compute ${\rm E}[Z]$. This is the whole question that I was asked. I can do (c) but I don't know how to find the marginal pdf of $Z$ with the given information and can't seem to find any formulas. Any help would be appreciated.,Let $X$ and $Y$ be independent $N(0; 1)$ random variables. Let $Z = \sqrt{X^2+Y^2}$. (a) Derive the marginal pdf of $Z$ and then using the marginal pdf to compute ${\rm E}[Z^2]$ (b) Can you propose a different way other than that in (a) to compute ${\rm E}[Z^2]$ (c) Compute ${\rm E}[Z]$. This is the whole question that I was asked. I can do (c) but I don't know how to find the marginal pdf of $Z$ with the given information and can't seem to find any formulas. Any help would be appreciated.,,"['probability', 'statistics', 'probability-distributions']"
60,Two Expected value definitions of the geometric random variable,Two Expected value definitions of the geometric random variable,,"Ok so I'm looking at my book and it defines the geometric distribution to be $\sum_{n=1}^{\infty}p(1-p)^{n-1}$ . My book says the expected value of a geometric random variable is $\dfrac{p}{q}$ . It proves it using the probablity generating function for the geometric random variable. However, I have seen another expected value of a geometric random variable to be $\dfrac{1}{p}$ . How are these two definitions related and when do I use one or the other?","Ok so I'm looking at my book and it defines the geometric distribution to be . My book says the expected value of a geometric random variable is . It proves it using the probablity generating function for the geometric random variable. However, I have seen another expected value of a geometric random variable to be . How are these two definitions related and when do I use one or the other?",\sum_{n=1}^{\infty}p(1-p)^{n-1} \dfrac{p}{q} \dfrac{1}{p},"['probability', 'statistics', 'probability-distributions']"
61,Traversing an array and counting the number of distanct number from the given elements in an array.,Traversing an array and counting the number of distanct number from the given elements in an array.,,"You are given an array $A[0 \ldots n-1]$ of $n$ numbers. Let $d$ be the  number of \emph{distinct} numbers that occur in this array.  For each $i$ with $0 \leq i \leq n-1$, let $N_i$ be the number of  elements in the array that are equal to $A[i]$. Show that $d = \sum_{i=0}^{n-1} 1/N_i$. Consider the following algorithm: Step 1: Choose an integer $k$ in $\{0,1,2,\ldots,n-1\}$ uniformly  at random, and let $a = A[k]$. Step 2: Traverse the array and compute the number $N_k$ of times that $a$ occurs. Step 3: Return the value $X = n/N_k$. Determine the expected value $E(X)$ of the random variable $X$.","You are given an array $A[0 \ldots n-1]$ of $n$ numbers. Let $d$ be the  number of \emph{distinct} numbers that occur in this array.  For each $i$ with $0 \leq i \leq n-1$, let $N_i$ be the number of  elements in the array that are equal to $A[i]$. Show that $d = \sum_{i=0}^{n-1} 1/N_i$. Consider the following algorithm: Step 1: Choose an integer $k$ in $\{0,1,2,\ldots,n-1\}$ uniformly  at random, and let $a = A[k]$. Step 2: Traverse the array and compute the number $N_k$ of times that $a$ occurs. Step 3: Return the value $X = n/N_k$. Determine the expected value $E(X)$ of the random variable $X$.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'stochastic-processes']"
62,Expected number of coin flips,Expected number of coin flips,,"Assume that when you flip a coin, the probability of getting heads is $1-\alpha$. If you need to flip the coin $N$ times before getting heads, then one can write the expected value of $N$ like so: $$ E[N] = \underbrace{1}_{1st\ coin\ flip} + \underbrace{\alpha}_{prob.\ it\ fails}\cdot \underbrace{E[N]}_{expected\ \#\ of\ further\ flips} $$ I was presented the above equation in a lecture, but I cannot derive it myself. Can someone shed some light for me and tell me how this is derived?","Assume that when you flip a coin, the probability of getting heads is $1-\alpha$. If you need to flip the coin $N$ times before getting heads, then one can write the expected value of $N$ like so: $$ E[N] = \underbrace{1}_{1st\ coin\ flip} + \underbrace{\alpha}_{prob.\ it\ fails}\cdot \underbrace{E[N]}_{expected\ \#\ of\ further\ flips} $$ I was presented the above equation in a lecture, but I cannot derive it myself. Can someone shed some light for me and tell me how this is derived?",,"['probability', 'statistics']"
63,Likelihood function for continuous densities,Likelihood function for continuous densities,,"When doing ML-estimates for discrete distributions, the definition of likelihood makes perfect sense $ \ L(x,\theta) = \Pi_{1:n}\ P(X_i=x_i|\theta)$ Since there is a non-infinitesimal probability that $X=x$. But why does this work for continuous distributions? Isn't $P(X=x)$ always infinitesimal? When doing excercises, everything works out fine. But I don't understand how. Why doesn't the likelihood function require intervals for $P(X)$? (Found a related thread Why is likelihood not always 0 in continuous case? but it's more about explaining why $L(x,\theta)$ can be greater than one. I still don't get how $P(X=x) \neq 0$ for continuous distributions.)","When doing ML-estimates for discrete distributions, the definition of likelihood makes perfect sense $ \ L(x,\theta) = \Pi_{1:n}\ P(X_i=x_i|\theta)$ Since there is a non-infinitesimal probability that $X=x$. But why does this work for continuous distributions? Isn't $P(X=x)$ always infinitesimal? When doing excercises, everything works out fine. But I don't understand how. Why doesn't the likelihood function require intervals for $P(X)$? (Found a related thread Why is likelihood not always 0 in continuous case? but it's more about explaining why $L(x,\theta)$ can be greater than one. I still don't get how $P(X=x) \neq 0$ for continuous distributions.)",,"['probability', 'statistics']"
64,probability of getting particular cards in a hand (cards),probability of getting particular cards in a hand (cards),,"I was given the question in class: ""What is the probability of getting a hand with 1 heart, 2 diamonds, 2 clubs?"" and ""What is the probability of getting a hand with at least 3 queens"" for the first question I assumed that the cards were removed from the deck each time so i removed them from the total each time and multiplied each probability: 13/52 * 13/51 * 12/50 * 13/49 * 12/48 but i got an abnormally small answer to this and I'm not sure if i'm on the right track.. And I'm not 100% sure what do with the second question for ""atleast"" questions i generally find the compliment of the event and then use that. Could I please get some help with these two questions?","I was given the question in class: ""What is the probability of getting a hand with 1 heart, 2 diamonds, 2 clubs?"" and ""What is the probability of getting a hand with at least 3 queens"" for the first question I assumed that the cards were removed from the deck each time so i removed them from the total each time and multiplied each probability: 13/52 * 13/51 * 12/50 * 13/49 * 12/48 but i got an abnormally small answer to this and I'm not sure if i'm on the right track.. And I'm not 100% sure what do with the second question for ""atleast"" questions i generally find the compliment of the event and then use that. Could I please get some help with these two questions?",,"['probability', 'statistics']"
65,Poisson distribution proof question,Poisson distribution proof question,,I'm reading over the Poisson distribution proof and trying to understand how $$\frac{n(n-1)\cdots(n-k+1)}{(n-\lambda)(n-\lambda)\cdots(n-\lambda)}$$ tends to 1 as $$n\rightarrow\infty\text{ ?}$$ Thanks.,I'm reading over the Poisson distribution proof and trying to understand how $$\frac{n(n-1)\cdots(n-k+1)}{(n-\lambda)(n-\lambda)\cdots(n-\lambda)}$$ tends to 1 as $$n\rightarrow\infty\text{ ?}$$ Thanks.,,"['calculus', 'probability', 'statistics']"
66,Similarity between two curves,Similarity between two curves,,I am trying to find out how well a deterministic version of a MATLAB program predicts the stochastic version. I don't know what statistical test/quantitative analysis to use.  This is what the output of the program looks like:,I am trying to find out how well a deterministic version of a MATLAB program predicts the stochastic version. I don't know what statistical test/quantitative analysis to use.  This is what the output of the program looks like:,,"['statistics', 'stochastic-processes', 'matlab']"
67,order statistics question 1,order statistics question 1,,"Let $Y1,Y2,...,Yn$ be independent, uniformly distributed random variables on the interval $[0, θ]$, $Y(k)$ the kth-order statistic, where k is an integer between $1$ and $n$. Find $E(Y(k)- Y(k-1))$, the mean difference between two successive order statistics. Interpret this result. Since I got $E(Y(k))=Kθ/(n+1)$, how can I find $E(Y(k-1))$? Thanks for help!","Let $Y1,Y2,...,Yn$ be independent, uniformly distributed random variables on the interval $[0, θ]$, $Y(k)$ the kth-order statistic, where k is an integer between $1$ and $n$. Find $E(Y(k)- Y(k-1))$, the mean difference between two successive order statistics. Interpret this result. Since I got $E(Y(k))=Kθ/(n+1)$, how can I find $E(Y(k-1))$? Thanks for help!",,['statistics']
68,Probability of always rolling 6 on a dice,Probability of always rolling 6 on a dice,,Suppose I roll a six-sided die $10$ times and each time it shows a $6$. What is the probability of the next roll coming up $6$? You might say $1/6$. But it was never declared to be a fair die. In fact it seems from the data available that there is a high probability of it being an unfair dice and the probability of another $6$ is quite high. Nevertheless it could still be coincidence. Is there any way to model this situation and describe mathematically the probability distribution of the next throw?,Suppose I roll a six-sided die $10$ times and each time it shows a $6$. What is the probability of the next roll coming up $6$? You might say $1/6$. But it was never declared to be a fair die. In fact it seems from the data available that there is a high probability of it being an unfair dice and the probability of another $6$ is quite high. Nevertheless it could still be coincidence. Is there any way to model this situation and describe mathematically the probability distribution of the next throw?,,"['probability', 'statistics', 'dice']"
69,Normal Distribution in,Normal Distribution in,,I am so confused with this problem: The middle 95% of adults have an IQ between 60 and 140. Assume that IQ for adults is normally distributed.  a. What is the average IQ for adults? The standard deviation? I got the average by subtracting the values given and then multiply it with 95%. But I dont know how to get the standard deviation because a certain number of population isn't given. Any ideas anyone?,I am so confused with this problem: The middle 95% of adults have an IQ between 60 and 140. Assume that IQ for adults is normally distributed.  a. What is the average IQ for adults? The standard deviation? I got the average by subtracting the values given and then multiply it with 95%. But I dont know how to get the standard deviation because a certain number of population isn't given. Any ideas anyone?,,"['statistics', 'normal-distribution']"
70,Uncertainty in parallel resistors.,Uncertainty in parallel resistors.,,I need help with one of my study guide questions. We learned about uncertainty in class but am not sure how to attack this problem: Could someone walk me through this example? Any help will be appreciated!,I need help with one of my study guide questions. We learned about uncertainty in class but am not sure how to attack this problem: Could someone walk me through this example? Any help will be appreciated!,,[]
71,Unbiased estimate $\lambda^2$,Unbiased estimate,\lambda^2,"Given a Poisson distribution I want to figure out whether  $d:(x_1,...,x_n) \mapsto x_1^2$ and $d':(x_1,...,x_n) \mapsto x_1x_2$ are unbiased estimations for $\lambda^2$ ? I mean it would sound reasonable if they were, cause the expected value for every $x_1$ is $\lambda$ itself, but since the product of expected values is not necessarily the expected value of a product, this does not has to be true. So how does this work here?","Given a Poisson distribution I want to figure out whether  $d:(x_1,...,x_n) \mapsto x_1^2$ and $d':(x_1,...,x_n) \mapsto x_1x_2$ are unbiased estimations for $\lambda^2$ ? I mean it would sound reasonable if they were, cause the expected value for every $x_1$ is $\lambda$ itself, but since the product of expected values is not necessarily the expected value of a product, this does not has to be true. So how does this work here?",,"['calculus', 'probability']"
72,Confidence level of random sample from continuous distribution,Confidence level of random sample from continuous distribution,,"Let $X_1,X_2,\cdots ,X_n$ be a random sample from a continuous distribution with median $\mu$. If $[X_{min}, X_{max}]$ is used as a confidence interval for $\mu$, what is its confidence level? What is the confidence level if $n=10$? The formula for the confidence interval is $\overline{x}-z(\alpha /2)(\sigma)\leq \mu \leq \overline{x}+z(\alpha /2)(\sigma)$. I know that I need to solve for $\alpha$, but I'm not sure what the fact that the question involves a random sample means for the solution.","Let $X_1,X_2,\cdots ,X_n$ be a random sample from a continuous distribution with median $\mu$. If $[X_{min}, X_{max}]$ is used as a confidence interval for $\mu$, what is its confidence level? What is the confidence level if $n=10$? The formula for the confidence interval is $\overline{x}-z(\alpha /2)(\sigma)\leq \mu \leq \overline{x}+z(\alpha /2)(\sigma)$. I know that I need to solve for $\alpha$, but I'm not sure what the fact that the question involves a random sample means for the solution.",,"['statistics', 'statistical-inference', 'descriptive-statistics']"
73,"Joint pdf of $f(x,y)=e^{-(x+y)}$",Joint pdf of,"f(x,y)=e^{-(x+y)}","I am asked to find the joint pdf of $\ f(x,y)=e^{-(x+y)} $ where $x,y$ are between $0$ and $\infty$ or $0$ otherwise. I can see its an exponential distribution, which means its continuous so I started a double integral like this: $\displaystyle\int_{0}^{\infty} \int_{0}^{\infty}e^{-x}e^{-y} dy dx$ I tried substituting in $u=e^{-x}$ and $du= -e^{-x} $ $v=e^{-y}$ and $\int v= \int   e^{-y}dy=-e^{-y} $ My result after substituting y for 0 and $\infty$ is $\int_{0}^{\infty} e^{-x}-e^{-x}dx$ = $-e^{-x}-e^{-x}$ and with limits applied I get $2$.  I can see that the answer should be EXP(1) since the exponential pdf is $\lambda e^{-\lambda x} $ so for my example $\ e^{-(x+y)} $ it must be $1$. (EXP(1) is the answer in the back of the text book for the marginal pdfs). Am I on the right track with my integration please or am I doing all of this wrong? Thanks.","I am asked to find the joint pdf of $\ f(x,y)=e^{-(x+y)} $ where $x,y$ are between $0$ and $\infty$ or $0$ otherwise. I can see its an exponential distribution, which means its continuous so I started a double integral like this: $\displaystyle\int_{0}^{\infty} \int_{0}^{\infty}e^{-x}e^{-y} dy dx$ I tried substituting in $u=e^{-x}$ and $du= -e^{-x} $ $v=e^{-y}$ and $\int v= \int   e^{-y}dy=-e^{-y} $ My result after substituting y for 0 and $\infty$ is $\int_{0}^{\infty} e^{-x}-e^{-x}dx$ = $-e^{-x}-e^{-x}$ and with limits applied I get $2$.  I can see that the answer should be EXP(1) since the exponential pdf is $\lambda e^{-\lambda x} $ so for my example $\ e^{-(x+y)} $ it must be $1$. (EXP(1) is the answer in the back of the text book for the marginal pdfs). Am I on the right track with my integration please or am I doing all of this wrong? Thanks.",,"['statistics', 'probability-distributions', 'statistical-inference']"
74,Analyzing data to determine seasonal indices?,Analyzing data to determine seasonal indices?,,"The quarterly sales levels for the past four years of a leading tyre manufacturer are displayed on the table table How to analyze the data to determine the specific seasonal indices,and what conclusion can be deduced from the results? Generally after $Q2$(second quarter) i notice a slight decrease in manufacturing ,which implies the relationship is not linear,and also with the average production for each season being respectively $2397,2623,2537,2645$, i would like to know how further this data can be analyzed and how to obtain sales for the next quarter of 2013(assuming that cyclical and irregular components are negligible)","The quarterly sales levels for the past four years of a leading tyre manufacturer are displayed on the table table How to analyze the data to determine the specific seasonal indices,and what conclusion can be deduced from the results? Generally after $Q2$(second quarter) i notice a slight decrease in manufacturing ,which implies the relationship is not linear,and also with the average production for each season being respectively $2397,2623,2537,2645$, i would like to know how further this data can be analyzed and how to obtain sales for the next quarter of 2013(assuming that cyclical and irregular components are negligible)",,"['statistics', 'data-analysis', 'descriptive-statistics']"
75,Upper and Lower one sided confidence level,Upper and Lower one sided confidence level,,"Iam trying to calculate upper and lower confidence levels for a parameter, but i can't get it straight (in this case $\sigma^2$): the reference variable: $R_{\sigma^2} := \frac{n-1s^2}{\sigma^2} \sim \chi^2(n-1)$ where $s^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n} (x_i-\bar{x})^2 $ now for the confidence interval we get something like this $1-\alpha = P\big(\chi^2_{1-\alpha/2}(n-1) < R_{\sigma^2} < \chi^2_{\alpha/2}(n-1) \big)= P\big( \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)} \big) < \sigma^2 <  \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}\big)$ ok that was the context. now i want to find the one-sided upper and lower confidence level and Iam thinking something like this: (for the one sided upper confidence level) $ \alpha=  P\big(R_{\sigma^2}  > \chi^2_{\alpha}(n-1)\big) = P\big(\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)}\  > \sigma^2 \big)  $ so the upper confidence level is given by $\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)} = \bar{\sigma^2}$ but in my book it says $\frac{(n-1)s^2}{ \chi^2_{\textbf{1-$\alpha$} }(n-1)} = \bar{\sigma^2}$  Iam a big confused about this can someone help me to understand (observe  $\chi^2_{1-\alpha}(n-1)$)","Iam trying to calculate upper and lower confidence levels for a parameter, but i can't get it straight (in this case $\sigma^2$): the reference variable: $R_{\sigma^2} := \frac{n-1s^2}{\sigma^2} \sim \chi^2(n-1)$ where $s^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n} (x_i-\bar{x})^2 $ now for the confidence interval we get something like this $1-\alpha = P\big(\chi^2_{1-\alpha/2}(n-1) < R_{\sigma^2} < \chi^2_{\alpha/2}(n-1) \big)= P\big( \frac{(n-1)s^2}{\chi^2_{\alpha/2}(n-1)} \big) < \sigma^2 <  \frac{(n-1)s^2}{\chi^2_{1-\alpha/2}(n-1)}\big)$ ok that was the context. now i want to find the one-sided upper and lower confidence level and Iam thinking something like this: (for the one sided upper confidence level) $ \alpha=  P\big(R_{\sigma^2}  > \chi^2_{\alpha}(n-1)\big) = P\big(\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)}\  > \sigma^2 \big)  $ so the upper confidence level is given by $\frac{(n-1)s^2}{ \chi^2_{\alpha}(n-1)} = \bar{\sigma^2}$ but in my book it says $\frac{(n-1)s^2}{ \chi^2_{\textbf{1-$\alpha$} }(n-1)} = \bar{\sigma^2}$  Iam a big confused about this can someone help me to understand (observe  $\chi^2_{1-\alpha}(n-1)$)",,"['statistics', 'statistical-inference']"
76,Showing that the MLE of $f(x\mid\theta)=\theta x^{\theta-1}$ is consistent.,Showing that the MLE of  is consistent.,f(x\mid\theta)=\theta x^{\theta-1},"Suppose $X_1,\ldots,X_n$ are IID RV with distribution $$f(x\mid\theta)=\theta x^{\theta-1}, \;\;0<x<1,\,\,0<\theta<\infty.$$ I've found the MLE $\hat{\theta}$ $$\hat{\theta}=\frac{-1}{\frac{1}{n}\sum_{i=1}^n \log X_i}.$$ I've also found $E[Y_i]=\frac{-1}{\theta}$ and $\operatorname{var}[Y_i]=\frac{1}{\theta^2}$ where $Y_i=\log X_i$. This is were everything goes down hill fast. Now I'm supposed to use this information to show that $\hat{\theta}$ is consistent, derive the distribution of $\hat{\theta}$ and check whether or not it's biased. I know that for consistency I need to show $P(|\hat{\theta}-\theta|>\epsilon)\rightarrow 0$ as $n\rightarrow \infty$ but I'm not sure how that helps me here. As for the distribution of $\hat{\theta}$, it looks like there's a Gamma distribution in there somewhere I just can't squeeze it out. I really appreciate any help. In light of JPi's comments I apply Slutky's theorem and have: $$\hat{\theta}=g(\bar{Y})=-\bar{Y}^{-1}$$ Which by the LLN converges to $g(E[Y])$.","Suppose $X_1,\ldots,X_n$ are IID RV with distribution $$f(x\mid\theta)=\theta x^{\theta-1}, \;\;0<x<1,\,\,0<\theta<\infty.$$ I've found the MLE $\hat{\theta}$ $$\hat{\theta}=\frac{-1}{\frac{1}{n}\sum_{i=1}^n \log X_i}.$$ I've also found $E[Y_i]=\frac{-1}{\theta}$ and $\operatorname{var}[Y_i]=\frac{1}{\theta^2}$ where $Y_i=\log X_i$. This is were everything goes down hill fast. Now I'm supposed to use this information to show that $\hat{\theta}$ is consistent, derive the distribution of $\hat{\theta}$ and check whether or not it's biased. I know that for consistency I need to show $P(|\hat{\theta}-\theta|>\epsilon)\rightarrow 0$ as $n\rightarrow \infty$ but I'm not sure how that helps me here. As for the distribution of $\hat{\theta}$, it looks like there's a Gamma distribution in there somewhere I just can't squeeze it out. I really appreciate any help. In light of JPi's comments I apply Slutky's theorem and have: $$\hat{\theta}=g(\bar{Y})=-\bar{Y}^{-1}$$ Which by the LLN converges to $g(E[Y])$.",,['statistics']
77,Smoothing linear graph,Smoothing linear graph,,Which ways I can smooth data where are random fluctuations? This is linear graph of my data input: Big curves (about time 100 etc) are desirable. I already tried sliding window. Which way do you recommend?,Which ways I can smooth data where are random fluctuations? This is linear graph of my data input: Big curves (about time 100 etc) are desirable. I already tried sliding window. Which way do you recommend?,,['statistics']
78,Sufficient statistic and conditional distribution intuition?,Sufficient statistic and conditional distribution intuition?,,"I am confused about the intuition behind the definition of a sufficient statistic. The part definition of a sufficient statistic that I am confused about is why the conditional distribution of a sample given the value of the statistic does not depend on the parameter of interest. I guess this is almost more of a question about conditional distributions maybe. But is there any intuition behind why this makes sense? I understand conditional probability in the sense of events but I am having a hard time understanding why conditioning on the statistic would make this new distribution not depend on the parameter of interest. Does conditioning on a statistic that has all the information about the parameter ""remove"" that information from the sample? I guess I'm confused about how to conceptually think about conditioning and why it makes sense that the new distribution doesn't depend on the parameter.","I am confused about the intuition behind the definition of a sufficient statistic. The part definition of a sufficient statistic that I am confused about is why the conditional distribution of a sample given the value of the statistic does not depend on the parameter of interest. I guess this is almost more of a question about conditional distributions maybe. But is there any intuition behind why this makes sense? I understand conditional probability in the sense of events but I am having a hard time understanding why conditioning on the statistic would make this new distribution not depend on the parameter of interest. Does conditioning on a statistic that has all the information about the parameter ""remove"" that information from the sample? I guess I'm confused about how to conceptually think about conditioning and why it makes sense that the new distribution doesn't depend on the parameter.",,"['statistics', 'conditional-probability']"
79,How do you describe a CDF in terms of another CDF?,How do you describe a CDF in terms of another CDF?,,"This is homework, but I'm more interested in understanding the problem than the solution, so answering with a different example is totally fine. The problem has multiple parts, but I'm only stuck on this. We have a CDF $F_x(x)$ for a continuous random variable, $X$. The CDF was not given. But we want to express the CDF $F_y(y)$ in terms of $F_x(x)$. We define $Y = 1 - 2X$. How do we find this? I know that $F_x(-\infty)=0$ and $F_x(\infty)=1$ for a CDF. I've been scouring the book and the internet but I can't find where this is explained. At first, I was thinking you could do $F_y(y)=1-2F_x(x)$ but then $F_x(-\infty)=1$ and $F_x(\infty)=-1$ right? Which isn't possible for a CDF. I'm not sure where to go from here.","This is homework, but I'm more interested in understanding the problem than the solution, so answering with a different example is totally fine. The problem has multiple parts, but I'm only stuck on this. We have a CDF $F_x(x)$ for a continuous random variable, $X$. The CDF was not given. But we want to express the CDF $F_y(y)$ in terms of $F_x(x)$. We define $Y = 1 - 2X$. How do we find this? I know that $F_x(-\infty)=0$ and $F_x(\infty)=1$ for a CDF. I've been scouring the book and the internet but I can't find where this is explained. At first, I was thinking you could do $F_y(y)=1-2F_x(x)$ but then $F_x(-\infty)=1$ and $F_x(\infty)=-1$ right? Which isn't possible for a CDF. I'm not sure where to go from here.",,"['probability', 'statistics']"
80,How to calculate an expected value from a cluster of results,How to calculate an expected value from a cluster of results,,"I have a vector of points r where each element has an x and y value. For every one of those points there is a z value, which corresponds to a measurement taken at position r. If I have say n measurements taken at n different positions (for example see below) is there a way I can use the data I have to predict the expectation value for z at a random point r_2 ?","I have a vector of points r where each element has an x and y value. For every one of those points there is a z value, which corresponds to a measurement taken at position r. If I have say n measurements taken at n different positions (for example see below) is there a way I can use the data I have to predict the expectation value for z at a random point r_2 ?",,['statistics']
81,Why is the expected value of the squared value equal to the sum of the standard deviation and the mean?,Why is the expected value of the squared value equal to the sum of the standard deviation and the mean?,,I am currently reading the proof here that one has to divide by $n-1$ in order to get an unbiased estimator of the population variance. What I do not get is this: \begin{equation}E[y_i^2] = \sigma^2 +\mu^2\end{equation} I extracted that portion from the difference between line three and four of the proof. Why is it true?,I am currently reading the proof here that one has to divide by $n-1$ in order to get an unbiased estimator of the population variance. What I do not get is this: \begin{equation}E[y_i^2] = \sigma^2 +\mu^2\end{equation} I extracted that portion from the difference between line three and four of the proof. Why is it true?,,"['probability', 'statistics', 'standard-deviation']"
82,Expectation of minimum of normally distributed random variables,Expectation of minimum of normally distributed random variables,,"Let $(X,Y)$ be normally distributed and such that $\;\;\;\;\mathrm{Cov}(X,Y)=\varrho$, and $\mathrm{Var}(X)=\mathrm{Var}(Y)=1$. For which $\varrho$ does the following equality hold? $\;\;\;\;\min (E(X),E(Y)) = E(\min(X,Y))$ For $\varrho=1$? For $\varrho=0$? For $\varrho=-1$? For any other $\varrho$?","Let $(X,Y)$ be normally distributed and such that $\;\;\;\;\mathrm{Cov}(X,Y)=\varrho$, and $\mathrm{Var}(X)=\mathrm{Var}(Y)=1$. For which $\varrho$ does the following equality hold? $\;\;\;\;\min (E(X),E(Y)) = E(\min(X,Y))$ For $\varrho=1$? For $\varrho=0$? For $\varrho=-1$? For any other $\varrho$?",,"['probability', 'statistics']"
83,"Proof for expected value of geometric RV without using derivates or other ""fancy"" methods","Proof for expected value of geometric RV without using derivates or other ""fancy"" methods",,"Is it even possible? I'm guessing it is, but I get stuck very early on: $E[X]=\sum_{k=1}^\infty kp(1-p)^{k-1}=\sum_{k=1}^\infty k(1-q)(q)^{k-1}=\sum_{k=1}^\infty k(q^{k-1}-q^k)=\sum_{k=0}^\infty (k+1)(q^{k}-q^{k+1})$ I'm trying to make stuff disappear using methods similar to what we use in the case of the telescoping series but I can't do it. All help/hints are appreciated. Thanks:)","Is it even possible? I'm guessing it is, but I get stuck very early on: $E[X]=\sum_{k=1}^\infty kp(1-p)^{k-1}=\sum_{k=1}^\infty k(1-q)(q)^{k-1}=\sum_{k=1}^\infty k(q^{k-1}-q^k)=\sum_{k=0}^\infty (k+1)(q^{k}-q^{k+1})$ I'm trying to make stuff disappear using methods similar to what we use in the case of the telescoping series but I can't do it. All help/hints are appreciated. Thanks:)",,"['probability', 'statistics', 'probability-theory']"
84,"stats - calculate marginal pdf of $f(x,y)$ - limits of integration?",stats - calculate marginal pdf of  - limits of integration?,"f(x,y)","I have the following equation $$ f(x,y) = \frac{2}{x^2 (x-1) y^{(2 x-1)/(x-1)}} \quad \forall x>1, y>1 $$ I am trying to find that marginal pdf w.r.t. $x$, $f_X(x)$. Normally I would just take $\int f(x,y) dy$ using $x$'s domain as limits, aka over $[1, \infty)$. But that integral won't converge here. What am I doing wrong? This problem should be solve-able.","I have the following equation $$ f(x,y) = \frac{2}{x^2 (x-1) y^{(2 x-1)/(x-1)}} \quad \forall x>1, y>1 $$ I am trying to find that marginal pdf w.r.t. $x$, $f_X(x)$. Normally I would just take $\int f(x,y) dy$ using $x$'s domain as limits, aka over $[1, \infty)$. But that integral won't converge here. What am I doing wrong? This problem should be solve-able.",,['statistics']
85,Building intuition of hypothesis testing,Building intuition of hypothesis testing,,"I'm studying AS level mathematics (""college"") as a mature student and trying to wrap my head around binomial probability and hypothesis testing. I understand that $X\sim B(n,p)$ describes a binomial probability distribution where $n$ is the total number of trials and $p$ is the success probability. When using this to test hypothesis with a significance level, I am trying to correctly articulate what we're doing when we evaluate the binomial distribution. Let's say we're testing whether a coin is biased towards heads. We've tossed the coin 20 times and found head comes up 16 times. We then produce the following two hypothesis for the distribution $X\sim B(20,\frac{1}{2})$: $$ H_{0} : p=0.5 $$ $$ H_{1} : p>0.5, P(X\geq 16) $$ Where $H_0$ is our null hypothesis where the coin is not biased, $H_1$ is the alternative hypothesis, the claim that the coin is biased towards heads. I then evaluate the binomial distribution: $P(X\geq16)=\frac{15}{10000}$.  Which tells me that the ""probability of getting 16 or more heads"" is very unlikely . Comparing this to the significance level (e.g. 5%) helps me determine that there's enough evidence to support the alternative hypothesis because were the coin unbiased/fair getting 16 or more heads is statistically unlikely to occur. So, my questions: Is my understanding correct according to my language above? When we evaluate the binomial probability ($P(X\geq 16)$ above) what are we actually asking? ""If less than our significance, the less likely something would happen according to the null hypothesis, thus the bias is more feasible"" or what? Can you put it in better words?","I'm studying AS level mathematics (""college"") as a mature student and trying to wrap my head around binomial probability and hypothesis testing. I understand that $X\sim B(n,p)$ describes a binomial probability distribution where $n$ is the total number of trials and $p$ is the success probability. When using this to test hypothesis with a significance level, I am trying to correctly articulate what we're doing when we evaluate the binomial distribution. Let's say we're testing whether a coin is biased towards heads. We've tossed the coin 20 times and found head comes up 16 times. We then produce the following two hypothesis for the distribution $X\sim B(20,\frac{1}{2})$: $$ H_{0} : p=0.5 $$ $$ H_{1} : p>0.5, P(X\geq 16) $$ Where $H_0$ is our null hypothesis where the coin is not biased, $H_1$ is the alternative hypothesis, the claim that the coin is biased towards heads. I then evaluate the binomial distribution: $P(X\geq16)=\frac{15}{10000}$.  Which tells me that the ""probability of getting 16 or more heads"" is very unlikely . Comparing this to the significance level (e.g. 5%) helps me determine that there's enough evidence to support the alternative hypothesis because were the coin unbiased/fair getting 16 or more heads is statistically unlikely to occur. So, my questions: Is my understanding correct according to my language above? When we evaluate the binomial probability ($P(X\geq 16)$ above) what are we actually asking? ""If less than our significance, the less likely something would happen according to the null hypothesis, thus the bias is more feasible"" or what? Can you put it in better words?",,"['statistics', 'hypothesis-testing']"
86,Chebychev to get at least 90 %,Chebychev to get at least 90 %,,"Im am given a list of integers and asked to give the interval that contains at least 90 % of my values. Values :  $62,56,72,83,66,77,62,71,50,58, 74,81,76,67,70,70,69,67,80,81, 74,53,73,55,66,88,73,61,63,70, 72,63,75,68,78,75,61,69,80,82, 87,57,74,74,85,68,75,63,81,73$ At First, I found the following values : $ avg = 70.56 $ $ variance = 80.55 $ $ standard deviation = 8.97 $ Now the part I am not sure. Chebychev inequality is $1 - \frac{1}{k^2}$ k * std var. being how far from the mean  the numbers are. I figured that with a k of 4, i get $15/16$ which is about $93 %$ Is that ok or do I have to figure out some fraction of k to get exactly 90 % that is asked ? How do I figure out precisely that fraction ? Using a k of 4, I multiplied my std var by 4, and that gives me a minimum of 34.68 and maximum of 106.44 Looking at my numbers, the min and max seems a bit off... So for now, my answer is : the interval is $[34.68,106.44]$ covers at least 93.75 %","Im am given a list of integers and asked to give the interval that contains at least 90 % of my values. Values :  $62,56,72,83,66,77,62,71,50,58, 74,81,76,67,70,70,69,67,80,81, 74,53,73,55,66,88,73,61,63,70, 72,63,75,68,78,75,61,69,80,82, 87,57,74,74,85,68,75,63,81,73$ At First, I found the following values : $ avg = 70.56 $ $ variance = 80.55 $ $ standard deviation = 8.97 $ Now the part I am not sure. Chebychev inequality is $1 - \frac{1}{k^2}$ k * std var. being how far from the mean  the numbers are. I figured that with a k of 4, i get $15/16$ which is about $93 %$ Is that ok or do I have to figure out some fraction of k to get exactly 90 % that is asked ? How do I figure out precisely that fraction ? Using a k of 4, I multiplied my std var by 4, and that gives me a minimum of 34.68 and maximum of 106.44 Looking at my numbers, the min and max seems a bit off... So for now, my answer is : the interval is $[34.68,106.44]$ covers at least 93.75 %",,"['statistics', 'standard-deviation']"
87,VC dimension for Rotatable Rectangles,VC dimension for Rotatable Rectangles,,"It can be shown that VC dimension of rotatable rectangles is 7 . The problem is I cannot understand how to approach the solution. So far I used bruteforce to solve this kind of problem, I was drawing points in different shapes and check whenever the hypothesis shatters the points. In this case the heptagon is the key. In solution it's mentioned that it's easy to show that 0,1,2,6,7 points can be shattered, except for bruteforce ""drawing"" I don't know any other way to show this. And the case with 3 is considered separately. I would appreciate if someone could explain why case with  0,1,2,6,7 can be shown easily and 3 needs special treating. Is there are any reason why 8 doesn't work here.","It can be shown that VC dimension of rotatable rectangles is 7 . The problem is I cannot understand how to approach the solution. So far I used bruteforce to solve this kind of problem, I was drawing points in different shapes and check whenever the hypothesis shatters the points. In this case the heptagon is the key. In solution it's mentioned that it's easy to show that 0,1,2,6,7 points can be shattered, except for bruteforce ""drawing"" I don't know any other way to show this. And the case with 3 is considered separately. I would appreciate if someone could explain why case with  0,1,2,6,7 can be shown easily and 3 needs special treating. Is there are any reason why 8 doesn't work here.",,"['probability', 'statistics', 'machine-learning', 'computational-geometry']"
88,"Show that as $d$ goes to $\infty$, a standardized version of $X$ has the STD Normal Dist","Show that as  goes to , a standardized version of  has the STD Normal Dist",d \infty X,"I am currently stuck on this problem and I would greatly appreciate some help. The problem is as follows: Let $X$ have a chi-square with $d$ degrees of freedom. Show that a standardized version of $X$ has a limiting standard normal distribution as $d$ goes to $\infty$. Here is my current work/take on the problem. Since $X$ is a chi-square distribution with $d$ degrees of freedom, we know that  $$X = \sum_1^d{Z_i^2}$$ Now of course, before we apply the Central Limit Theorem, we need to standardize $X$. So we need to find the $E(X)$ and $Var(X)$. $E(X)=E(\sum_1^d{Z_i^2})=d$ $Var(X)=Var(\sum_1^d{Z_i^2})=2d$ Is this correct? If so, then I would standardize like so: $$ \frac{X-d}{\sqrt{2d}} = \frac{\sum_1^d{Z_i^2}-d}{\sqrt{2d}} = \frac{\sum_1^d{(Z_i^2-1)}}{\sqrt{2d}}$$ Then what I would do is get the MGF of just ${(Z_i^2-1)}$ and then the MGF of the whole numerator then the whole thing. Is that correct? Help would be greatly appreciated in showing me where I went wrong or if the methodology is correct? thanks!","I am currently stuck on this problem and I would greatly appreciate some help. The problem is as follows: Let $X$ have a chi-square with $d$ degrees of freedom. Show that a standardized version of $X$ has a limiting standard normal distribution as $d$ goes to $\infty$. Here is my current work/take on the problem. Since $X$ is a chi-square distribution with $d$ degrees of freedom, we know that  $$X = \sum_1^d{Z_i^2}$$ Now of course, before we apply the Central Limit Theorem, we need to standardize $X$. So we need to find the $E(X)$ and $Var(X)$. $E(X)=E(\sum_1^d{Z_i^2})=d$ $Var(X)=Var(\sum_1^d{Z_i^2})=2d$ Is this correct? If so, then I would standardize like so: $$ \frac{X-d}{\sqrt{2d}} = \frac{\sum_1^d{Z_i^2}-d}{\sqrt{2d}} = \frac{\sum_1^d{(Z_i^2-1)}}{\sqrt{2d}}$$ Then what I would do is get the MGF of just ${(Z_i^2-1)}$ and then the MGF of the whole numerator then the whole thing. Is that correct? Help would be greatly appreciated in showing me where I went wrong or if the methodology is correct? thanks!",,"['probability', 'statistics', 'statistical-inference', 'moment-generating-functions']"
89,"Probability computation, tossing two dice","Probability computation, tossing two dice",,"I have some ideas on how to solve the problem, but simulations do not support my analytical results :) Toss two dice and sum their value and write it down: Denote by $X_n$ the result at $n$-th toss. Clearly $$P(X_n = k)=\frac{k-1}{36}.$$ I would like to compute $P(X_{n+1}=X_n)$, that is the probability that two consequent tosses give me the same sum. Of course $X_n$ and $X_{n+1}$ are independent but something tells me that it is not just $\frac{11}{11*11}=\frac{1}{11}$ (which means, the sum is something between 2 and 12 (11 cases), so the difference is 0 in 11 cases out of 11*11). Any suggestion on how to do it? Empirical results give me something around (0.1125, 0.1179) Thanks!","I have some ideas on how to solve the problem, but simulations do not support my analytical results :) Toss two dice and sum their value and write it down: Denote by $X_n$ the result at $n$-th toss. Clearly $$P(X_n = k)=\frac{k-1}{36}.$$ I would like to compute $P(X_{n+1}=X_n)$, that is the probability that two consequent tosses give me the same sum. Of course $X_n$ and $X_{n+1}$ are independent but something tells me that it is not just $\frac{11}{11*11}=\frac{1}{11}$ (which means, the sum is something between 2 and 12 (11 cases), so the difference is 0 in 11 cases out of 11*11). Any suggestion on how to do it? Empirical results give me something around (0.1125, 0.1179) Thanks!",,"['real-analysis', 'probability', 'statistics', 'probability-theory', 'random-variables']"
90,Showing that the MLE doesn't exist for $e^{\theta-x}$,Showing that the MLE doesn't exist for,e^{\theta-x},"There is a classic problem: Suppose that $X_1,\ldots,X_n$ form an i.i.d. sample from a distribution with the following pdf: $$f(x\mid\theta) =  \begin{cases} e^{\theta-x}\quad&\text{for }\, x> \theta \\ 0 &\text{otherwise}. \end{cases}$$ I would like to show that the MLE of $\theta$ does not exist. The argument I have is that the likelihood function will be a maximum when $\theta$ is made as large as possible subject to the strict inequality $\theta < \min\{X_1, \ldots, X_n\}$. Therefore, the value $\theta = \min\{X_1,\ldots , X_n\}$ cannot be used and there is no MLE. However, I do not understand WHY we want $\theta$ to the equal to the maximum of the values. Also, is there a way to show mathematically why this MLE doesn't exist? I get that the log-likelihood function is: $$L(\theta) = n\theta - (X_1+\ldots+X_n)$$ but when you differentiate via $\theta$ and set to $0$, we get: $n=0$. How does the fact $n=0$ fit into the fact the MLE doesn't exist for $\theta$? Thanks!","There is a classic problem: Suppose that $X_1,\ldots,X_n$ form an i.i.d. sample from a distribution with the following pdf: $$f(x\mid\theta) =  \begin{cases} e^{\theta-x}\quad&\text{for }\, x> \theta \\ 0 &\text{otherwise}. \end{cases}$$ I would like to show that the MLE of $\theta$ does not exist. The argument I have is that the likelihood function will be a maximum when $\theta$ is made as large as possible subject to the strict inequality $\theta < \min\{X_1, \ldots, X_n\}$. Therefore, the value $\theta = \min\{X_1,\ldots , X_n\}$ cannot be used and there is no MLE. However, I do not understand WHY we want $\theta$ to the equal to the maximum of the values. Also, is there a way to show mathematically why this MLE doesn't exist? I get that the log-likelihood function is: $$L(\theta) = n\theta - (X_1+\ldots+X_n)$$ but when you differentiate via $\theta$ and set to $0$, we get: $n=0$. How does the fact $n=0$ fit into the fact the MLE doesn't exist for $\theta$? Thanks!",,"['statistics', 'statistical-inference', 'exponential-distribution', 'maximum-likelihood']"
91,Testing if data has a binomial distribution?,Testing if data has a binomial distribution?,,"I need help testing that data has a binomial distribution? I am not quite sure how to do this. Question as below: A hundred students take a test on which there are 5 questions, each to be answered simply yes or no. The number of students getting 0 -> 5 questions right are as follows: no of correct answers - 0   1   2   3   4   5 frequency -   2   11  20  45  21   1 Test the hypothesis that the no of correct answers has a binomial distribution? I am just starting a statistics course (after not doing it for many years)  I have done a similar question with testing data as a poisson dist. but I'm unsure what formula to use to compute the expected values, and then would I perform the 'Chi ^2' test? Thank you in advance for any help","I need help testing that data has a binomial distribution? I am not quite sure how to do this. Question as below: A hundred students take a test on which there are 5 questions, each to be answered simply yes or no. The number of students getting 0 -> 5 questions right are as follows: no of correct answers - 0   1   2   3   4   5 frequency -   2   11  20  45  21   1 Test the hypothesis that the no of correct answers has a binomial distribution? I am just starting a statistics course (after not doing it for many years)  I have done a similar question with testing data as a poisson dist. but I'm unsure what formula to use to compute the expected values, and then would I perform the 'Chi ^2' test? Thank you in advance for any help",,['statistics']
92,What is the probability no slots contain more than two balls given I am trying to sort 5 balls into 6 slots?,What is the probability no slots contain more than two balls given I am trying to sort 5 balls into 6 slots?,,"I am having a difficult time understanding how to approach this problem.  Suppose I have $6$ total slots and $5$ balls. Now, I assign the balls at random to the slots. What is the probability that no slot will contain more than two balls? My approaches so far: I recognized that $\binom{5+(6-1)} 5$ ($10$ choose $5$) represents the total combinations per the combinations with repetitions formula. Next, I realized that maybe I can find the probability a slot has exactly $5$, $4$, $3$ balls in one spot, then take $1$ - the sum of those probabilities. The correct answer is around $80\%$ so I am way off with this approach. Do you guys have any idea? Thanks!","I am having a difficult time understanding how to approach this problem.  Suppose I have $6$ total slots and $5$ balls. Now, I assign the balls at random to the slots. What is the probability that no slot will contain more than two balls? My approaches so far: I recognized that $\binom{5+(6-1)} 5$ ($10$ choose $5$) represents the total combinations per the combinations with repetitions formula. Next, I realized that maybe I can find the probability a slot has exactly $5$, $4$, $3$ balls in one spot, then take $1$ - the sum of those probabilities. The correct answer is around $80\%$ so I am way off with this approach. Do you guys have any idea? Thanks!",,"['probability', 'combinatorics', 'statistics', 'permutations']"
93,Is it true that $H(X|Y)=H(Y|X)$?,Is it true that ?,H(X|Y)=H(Y|X),"I have some difficulties with the question whether $H(X|Y)=H(Y|X)$? From my knowledge $I(X;Y)=H(X)-H(X|Y) = H(Y)-H(Y|X)$ so $H(X|Y)=H(Y|X)$ only when $H(X)=H(Y)$ The question is whether it's the last step, can I make a further assumption about the distribution of $X$ and $Y$ or $H(X)=H(Y)$ is a last step and no further conclusions.","I have some difficulties with the question whether $H(X|Y)=H(Y|X)$? From my knowledge $I(X;Y)=H(X)-H(X|Y) = H(Y)-H(Y|X)$ so $H(X|Y)=H(Y|X)$ only when $H(X)=H(Y)$ The question is whether it's the last step, can I make a further assumption about the distribution of $X$ and $Y$ or $H(X)=H(Y)$ is a last step and no further conclusions.",,"['statistics', 'information-theory']"
94,"If Z is a random variable with a standard normal distribution, what is $P(Z^2 \lt 3.841)$?","If Z is a random variable with a standard normal distribution, what is ?",P(Z^2 \lt 3.841),"If Z is a random variable with a standard normal distribution, what is $P(Z^2 \lt 3.841)$? Can I just square root $3.841$ so that it becomes $P(Z \lt \sqrt{3.841})$ and use the normal distribution table to obtain the probability?","If Z is a random variable with a standard normal distribution, what is $P(Z^2 \lt 3.841)$? Can I just square root $3.841$ so that it becomes $P(Z \lt \sqrt{3.841})$ and use the normal distribution table to obtain the probability?",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'radicals']"
95,Real-life question: How to determine the winner of a quiz,Real-life question: How to determine the winner of a quiz,,"Imagine there's a quiz on the internet intended for a wide audience. It contains a (unlimited) number of questions, all of them with yes/no answers. A person gets one random question and must answer it, after that he can get another one. He can continue answering any number of questions he wants. So the only data you have is: Peter: 37 out of 45 questions correct Mary: 42 out of 190 questions correct John: 0 out of 127 questions correct (etc.) Now if you want to make a list of winners, how would you sort them the clever way ? If you sort the people simply by the percentage of person's correct answers then the easy way to win is to correctly answer just one question (or two, three, ...). If you sort the people simply by the number of person's correct answers then an easy way to win is to spend a loong evening answering (or merely guessing) the questions. Then Mary (42 out of 190 correct) wins over Peter (37 out of 45 correct) even if Peter is significantly better. So is there any sophisticated way how to choose a proper winner based on having just this type of data? Any statistical method that puts the best person higher than all these easy-winners?","Imagine there's a quiz on the internet intended for a wide audience. It contains a (unlimited) number of questions, all of them with yes/no answers. A person gets one random question and must answer it, after that he can get another one. He can continue answering any number of questions he wants. So the only data you have is: Peter: 37 out of 45 questions correct Mary: 42 out of 190 questions correct John: 0 out of 127 questions correct (etc.) Now if you want to make a list of winners, how would you sort them the clever way ? If you sort the people simply by the percentage of person's correct answers then the easy way to win is to correctly answer just one question (or two, three, ...). If you sort the people simply by the number of person's correct answers then an easy way to win is to spend a loong evening answering (or merely guessing) the questions. Then Mary (42 out of 190 correct) wins over Peter (37 out of 45 correct) even if Peter is significantly better. So is there any sophisticated way how to choose a proper winner based on having just this type of data? Any statistical method that puts the best person higher than all these easy-winners?",,['statistics']
96,Given X and Y are correlated and Y and Z are correlated what is the range of correlation between X and Z?,Given X and Y are correlated and Y and Z are correlated what is the range of correlation between X and Z?,,"How can I calculate the range of correlation of two variables X and Z given I have the correlations of X and Y, and Y and Z? I've found a few resources around, namely this , but I'd like a research paper (if any). Thanks!","How can I calculate the range of correlation of two variables X and Z given I have the correlations of X and Y, and Y and Z? I've found a few resources around, namely this , but I'd like a research paper (if any). Thanks!",,"['statistics', 'random-variables', 'correlation']"
97,Central Limit theorem Application on Poisson Distribution,Central Limit theorem Application on Poisson Distribution,,"Suppose that $X_1,\ldots,X_n$ is an iid sample from the Poisson distribution with mean $\lambda$. Use the Central Limit theorem to find $P(|\bar X - \lambda| < 0.1) $ as $n$ goes to infinity. My question is, if $n$ goes to infinity, the variance for $\bar X$ would be zero and it does not make any sense to me.","Suppose that $X_1,\ldots,X_n$ is an iid sample from the Poisson distribution with mean $\lambda$. Use the Central Limit theorem to find $P(|\bar X - \lambda| < 0.1) $ as $n$ goes to infinity. My question is, if $n$ goes to infinity, the variance for $\bar X$ would be zero and it does not make any sense to me.",,"['statistics', 'probability-distributions', 'central-limit-theorem']"
98,Show gamma-function $\Gamma(r)$ is well-defined for any $r >0$ (the limit of the improper definite integral exists),Show gamma-function  is well-defined for any  (the limit of the improper definite integral exists),\Gamma(r) r >0,"Introduction: I have proved the following: Suppose that Poisson events are occuring at the constant rate of $\lambda$ per unit time. Let the random variable $Y$ denote the waiting time for the rth event, then $Y$ has pdf: $$f_Y(y) = \frac {\lambda^r} {(r-1)!} y^{r-1}e^{-\lambda y} $$ (1) so $Y$ is defined for $r \in \mathbb N$. However the gamma pdf: $$f_Y(y) = \frac {\lambda^r} {\Gamma(r)} y^{r-1}e^{-\lambda y}$$ is a generalization of this function (right?), since $\Gamma(r) = (r-1)!$ for any positive integer $r$. To justify that the gamma pdf is a well-defined function i must that the gamma function: $$\Gamma(r) = \int_0^{\infty} y^{r-1}e^{-y} dy$$ is a well-defined function (I've already verified $$\int_0^{\infty}f_Y(y) dy = \int_0^{\infty} \frac {\lambda^r} {(r-1)!} y^{r-1}e^{-\lambda y} dy = 1$$ under the assumption that it is). Question: 1. How do I show that the gamma function is well-defined, that is the improper definite integral exists for any real number $r >0$ ? I know it exists for any positive integer $t > r$ (the integral with boundaries $0$ and $\infty$ of (1) is equal to $1$?) so $\Gamma(r) \le \Gamma(t)$ (right?) This implies that $\Gamma(r)$ is bounded, but how do I show the limit exists for $y \rightarrow \infty$ ? Also the functions of $y$ in $\Gamma(r)$ are both continuous, which implies the function is integrable ?","Introduction: I have proved the following: Suppose that Poisson events are occuring at the constant rate of $\lambda$ per unit time. Let the random variable $Y$ denote the waiting time for the rth event, then $Y$ has pdf: $$f_Y(y) = \frac {\lambda^r} {(r-1)!} y^{r-1}e^{-\lambda y} $$ (1) so $Y$ is defined for $r \in \mathbb N$. However the gamma pdf: $$f_Y(y) = \frac {\lambda^r} {\Gamma(r)} y^{r-1}e^{-\lambda y}$$ is a generalization of this function (right?), since $\Gamma(r) = (r-1)!$ for any positive integer $r$. To justify that the gamma pdf is a well-defined function i must that the gamma function: $$\Gamma(r) = \int_0^{\infty} y^{r-1}e^{-y} dy$$ is a well-defined function (I've already verified $$\int_0^{\infty}f_Y(y) dy = \int_0^{\infty} \frac {\lambda^r} {(r-1)!} y^{r-1}e^{-\lambda y} dy = 1$$ under the assumption that it is). Question: 1. How do I show that the gamma function is well-defined, that is the improper definite integral exists for any real number $r >0$ ? I know it exists for any positive integer $t > r$ (the integral with boundaries $0$ and $\infty$ of (1) is equal to $1$?) so $\Gamma(r) \le \Gamma(t)$ (right?) This implies that $\Gamma(r)$ is bounded, but how do I show the limit exists for $y \rightarrow \infty$ ? Also the functions of $y$ in $\Gamma(r)$ are both continuous, which implies the function is integrable ?",,"['probability', 'statistics', 'limits', 'probability-limit-theorems']"
99,Finding the variance of a statistic.,Finding the variance of a statistic.,,"$X_1,\cdots,X_n$ are independent random variables from $N(\mu,\sigma^2)$ distribution. Define $$T=\frac{1}{2(n-1)}\sum_{i=1}^{n-1}(X_{i+1}-X_i)^2$$ I have shown that it is an unbiased estimator of the variance. I need to compare its variance to that of the sample variance. Now how do I find $Var(T)$? Finding $E(T^2)$ simply by squaring the above expression and then tking expectation is becoming very clumsy!","$X_1,\cdots,X_n$ are independent random variables from $N(\mu,\sigma^2)$ distribution. Define $$T=\frac{1}{2(n-1)}\sum_{i=1}^{n-1}(X_{i+1}-X_i)^2$$ I have shown that it is an unbiased estimator of the variance. I need to compare its variance to that of the sample variance. Now how do I find $Var(T)$? Finding $E(T^2)$ simply by squaring the above expression and then tking expectation is becoming very clumsy!",,"['statistics', 'estimation']"
