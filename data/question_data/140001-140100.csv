,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Solve the following problem, $u'(t)+p(t)u(t)=0,\;\;u(0)=0,$ $p(t)=\begin{cases}2& 0\leq t< 1,\\1 &t\geq 1\end{cases}.$","Solve the following problem,","u'(t)+p(t)u(t)=0,\;\;u(0)=0, p(t)=\begin{cases}2& 0\leq t< 1,\\1 &t\geq 1\end{cases}.","Using Laplace transform, solve the following problem. $$u'(t)+p(t)u(t)=0,\;\;u(0)=0,$$ $$p(t)=\begin{cases}2& 0\leq t< 1,\\1 &t\geq 1\end{cases}.$$ Here's what I've done: Taking the Laplace transform of both sides $$L(u'(t))+L(p(t)u(t))=0,$$ $$sU(s)-u(0)+L(p(t)u(t))=0,$$ $$sU(s)+L(p(t)u(t))=0.$$ I'm stuck at this point. Please, how do I proceed?","Using Laplace transform, solve the following problem. $$u'(t)+p(t)u(t)=0,\;\;u(0)=0,$$ $$p(t)=\begin{cases}2& 0\leq t< 1,\\1 &t\geq 1\end{cases}.$$ Here's what I've done: Taking the Laplace transform of both sides $$L(u'(t))+L(p(t)u(t))=0,$$ $$sU(s)-u(0)+L(p(t)u(t))=0,$$ $$sU(s)+L(p(t)u(t))=0.$$ I'm stuck at this point. Please, how do I proceed?",,"['calculus', 'algebra-precalculus', 'ordinary-differential-equations', 'derivatives', 'laplace-transform']"
1,Show that problem is well defined for each time,Show that problem is well defined for each time,,"We have the Cauchy problem of the equation $u_t+xu_x=xu, x \in \mathbb{R}, 0<t<\infty$ with some given smooth ($C^1$) function $g$ as initial value. I want to check if the problem is well defined for each time. We know that a problem is well defined if the solution exists, is unique and depends continuously on the data of the problem. I have computed that the solution of the problem is $u(x,t)=g(xe^{-t}) e^{x(1-e^{-t})}$. So we have that the problem is well-defined if the function $u$ that we found is the unique solution of the problem and if $u$ depends continuously on the data of the problem, right? How can we deduce that there is no other solution except from $u$ ? Is it implied that $u$ depends continuously on the data of the problem, since it contains $g$ ?","We have the Cauchy problem of the equation $u_t+xu_x=xu, x \in \mathbb{R}, 0<t<\infty$ with some given smooth ($C^1$) function $g$ as initial value. I want to check if the problem is well defined for each time. We know that a problem is well defined if the solution exists, is unique and depends continuously on the data of the problem. I have computed that the solution of the problem is $u(x,t)=g(xe^{-t}) e^{x(1-e^{-t})}$. So we have that the problem is well-defined if the function $u$ that we found is the unique solution of the problem and if $u$ depends continuously on the data of the problem, right? How can we deduce that there is no other solution except from $u$ ? Is it implied that $u$ depends continuously on the data of the problem, since it contains $g$ ?",,"['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
2,Solve: $(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1)$,Solve:,(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1),Solve: $(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1)$ Is my solution correct: Answer given in the book : $y(x+1)=c_1+c_2\log(x+1)+\log3(x+1)$ For my later reference: link wolfram alpha,Solve: $(x+1)^3y''+3(x+1)^2y'+(x+1)y=6\log(x+1)$ Is my solution correct: Answer given in the book : $y(x+1)=c_1+c_2\log(x+1)+\log3(x+1)$ For my later reference: link wolfram alpha,,['ordinary-differential-equations']
3,"Solve: $\cos x\, dy=y(\sin x-y)\,dx$",Solve:,"\cos x\, dy=y(\sin x-y)\,dx","Solve: $\cos x \,dy=y(\sin x-y)\,dx$ I have done the solution the following way. Is my answer correct or wrong? The answer given in the book: $\sec x =y(\tan x +c)$","Solve: $\cos x \,dy=y(\sin x-y)\,dx$ I have done the solution the following way. Is my answer correct or wrong? The answer given in the book: $\sec x =y(\tan x +c)$",,['ordinary-differential-equations']
4,Clarification of Qualitative Behaviour of BVP Solutions Example,Clarification of Qualitative Behaviour of BVP Solutions Example,,"I have the following example from my PDE textbook ( Essential Partial Differential Equations by Griffiths, Dold, and Silvester): I don't understand how the authors found the solution $u(x)$. The exercise (5.2) they reference is as follows: Verify that (5.6) and (5.8) are solutions to the BVP in Example 5.1. But how does verifying this find us $u(x)$? Why does $|u(\frac{1}{2})| \to \infty$ as $b \to -(2n - 1)^2 \pi^2$ indicate that the BVP is not well posed for these values of $b$? For the graph (figure 5.1), in a), the authors state that, when $\epsilon > 0$, the solution is negative throughout the interval $0 < x < 1$. But we can see from the graph (figure 5.1) that, when $\epsilon = 1 > 0$, the solution is not always negative throughout the interval $0 < x < 1$. Specifically, the solution becomes positive for $b = 100$ and $b = -100$. I would greatly appreciate it if someone could please take the time to clarify the author's work here.","I have the following example from my PDE textbook ( Essential Partial Differential Equations by Griffiths, Dold, and Silvester): I don't understand how the authors found the solution $u(x)$. The exercise (5.2) they reference is as follows: Verify that (5.6) and (5.8) are solutions to the BVP in Example 5.1. But how does verifying this find us $u(x)$? Why does $|u(\frac{1}{2})| \to \infty$ as $b \to -(2n - 1)^2 \pi^2$ indicate that the BVP is not well posed for these values of $b$? For the graph (figure 5.1), in a), the authors state that, when $\epsilon > 0$, the solution is negative throughout the interval $0 < x < 1$. But we can see from the graph (figure 5.1) that, when $\epsilon = 1 > 0$, the solution is not always negative throughout the interval $0 < x < 1$. Specifically, the solution becomes positive for $b = 100$ and $b = -100$. I would greatly appreciate it if someone could please take the time to clarify the author's work here.",,"['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
5,Analytic continuation of several complex variables,Analytic continuation of several complex variables,,"Let $f(w_1,\ldots,w_n;z)$ be a holomorphic function of $n+1$ variables. For every fixed $w_1\ldots w_n$, let $g(w_1,\ldots,w_n;z)$ be an analytic continuation of $f$ as a holomorphic function of $z$. Of course, $g$ is holomorphic in $z$. Now, is $g$ holomorphic in $w_1,\ldots, w_n,z$ as well? In the concrete, I am interested in the following situation. Let $E(a_1,\ldots,a_n;z)$ be an ODE, where $z$ is a complex variable and $a_1,\ldots,a_n$ are complex parameters. Assume that $E(a_1,\ldots,a_n;z)$ has a local solution $u(a_1,\ldots,a_n;z)$ which is holomorphic in $a_1,\ldots,a_n,z$, and  $v(a_1,\ldots,a_n;z)$ is an analytic continuation of $u$. Is $v$ holomorphic in $a_1,\ldots,a_n,z$?","Let $f(w_1,\ldots,w_n;z)$ be a holomorphic function of $n+1$ variables. For every fixed $w_1\ldots w_n$, let $g(w_1,\ldots,w_n;z)$ be an analytic continuation of $f$ as a holomorphic function of $z$. Of course, $g$ is holomorphic in $z$. Now, is $g$ holomorphic in $w_1,\ldots, w_n,z$ as well? In the concrete, I am interested in the following situation. Let $E(a_1,\ldots,a_n;z)$ be an ODE, where $z$ is a complex variable and $a_1,\ldots,a_n$ are complex parameters. Assume that $E(a_1,\ldots,a_n;z)$ has a local solution $u(a_1,\ldots,a_n;z)$ which is holomorphic in $a_1,\ldots,a_n,z$, and  $v(a_1,\ldots,a_n;z)$ is an analytic continuation of $u$. Is $v$ holomorphic in $a_1,\ldots,a_n,z$?",,"['complex-analysis', 'ordinary-differential-equations', 'several-complex-variables', 'analytic-continuation']"
6,"$\frac{dx}{dt} = p, \frac{dy}{dt} = q$: Solution of these ODE imply the solution is constant along characteristics of the form $qx − py = constant$.",: Solution of these ODE imply the solution is constant along characteristics of the form .,"\frac{dx}{dt} = p, \frac{dy}{dt} = q qx − py = constant","My lecture notes state the following: When we were dealing with first order equations we saw that a differential operator of the form, $$p\frac{\partial}{\partial{x}} + q\frac{\partial}{\partial{y}}$$ Led to the characteristic equations $$\frac{dx}{dt} = p, \frac{dy}{dt} = q$$ The solution of these ODE in turn implied that the solution would be constant along characteristics of the form $qx − py = constant$ . Can someone please demonstrate this?","My lecture notes state the following: When we were dealing with first order equations we saw that a differential operator of the form, Led to the characteristic equations The solution of these ODE in turn implied that the solution would be constant along characteristics of the form . Can someone please demonstrate this?","p\frac{\partial}{\partial{x}} + q\frac{\partial}{\partial{y}} \frac{dx}{dt} = p, \frac{dy}{dt} = q qx − py = constant","['ordinary-differential-equations', 'partial-differential-equations']"
7,$\alpha$-limit set in $A=\{x\in \mathbb{R}^2|1<|x|<2\}$,-limit set in,\alpha A=\{x\in \mathbb{R}^2|1<|x|<2\},Consider the system  $$ \dot{x}=-y+x(r^4-3r^2+1)\\  \dot{y}=x+y(r^4-3r^2+1) $$  where $r^2=x^2+y^2$. Let $A=\{x\in \mathbb{R}^2|1<|x|<2\}$. Affirmation (fait accompli): $\dot{r}<0$ on the circle $r=1$ and $\dot{r}>0$ on the circle $r=2$. My question (doubt) is: Why the affirmation above implies that the $\alpha$-limit set any trajectory that starts in $A$ is in $A$? Thank You!,Consider the system  $$ \dot{x}=-y+x(r^4-3r^2+1)\\  \dot{y}=x+y(r^4-3r^2+1) $$  where $r^2=x^2+y^2$. Let $A=\{x\in \mathbb{R}^2|1<|x|<2\}$. Affirmation (fait accompli): $\dot{r}<0$ on the circle $r=1$ and $\dot{r}>0$ on the circle $r=2$. My question (doubt) is: Why the affirmation above implies that the $\alpha$-limit set any trajectory that starts in $A$ is in $A$? Thank You!,,"['ordinary-differential-equations', 'dynamical-systems']"
8,Does $f(x)=(x-2)^{\frac{2}{3}}(2x+1)$ have Point of Inflection,Does  have Point of Inflection,f(x)=(x-2)^{\frac{2}{3}}(2x+1),"Does $$f(x)=(x-2)^{\frac{2}{3}}(2x+1)$$ have Point of Inflection I differentiated it twice, getting $$f''(x)=\frac{10(2x-5)}{9 (x-2)^{\frac{4}{3}}}=0$$ which implies $$x=2.5$$ is Point of Inflection. But it is unnoticeable in Graphing calculator?","Does $$f(x)=(x-2)^{\frac{2}{3}}(2x+1)$$ have Point of Inflection I differentiated it twice, getting $$f''(x)=\frac{10(2x-5)}{9 (x-2)^{\frac{4}{3}}}=0$$ which implies $$x=2.5$$ is Point of Inflection. But it is unnoticeable in Graphing calculator?",,"['algebra-precalculus', 'ordinary-differential-equations', 'derivatives']"
9,"How to find all solutions of the ODE $x'=3x^{\frac{2}{3}}, x(0)=0$",How to find all solutions of the ODE,"x'=3x^{\frac{2}{3}}, x(0)=0","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix} 0 & \text{if }0\leq t< t_{0} \\   (t-t_{0})^3 &  \text{ if }t\geq t_{0}   \end{matrix}\right.$$ $t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix} 0 & \text{if }0\leq t< t_{0} \\   (t-t_{0})^3 &  \text{ if }t\geq t_{0}   \end{matrix}\right.$$ $t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?",,"['real-analysis', 'ordinary-differential-equations', 'analysis']"
10,Critical Points Clarification,Critical Points Clarification,,"I'm uncertain if I have all the critical points required of the system I do $\frac{dx}{dt} = x(x-2) + y^2$...and likewise..I do $\frac{dy}{dt}= y(1-x)$ I then set both equations to zero If we set $x = 0$ in the first equation, we solve the second equation $y = 0$ If we set $y = 0$ in the second equation, we solve the first equation for $x = 0$ and $x = 2$.  Likewise, I solve for y - xy = 0. Then xy = y so x = 1. We plug this solution into the first equation so that y^2 = 1 and hence, have (1,1),(1,-1) as our results as well. Hence, my critical points are then $(0,0);(2,0)$;(1,1);(1,-1) Is this the correct approach for this particular question? Likewise, what does it mean to determine the stability of the critical point and the type of critical point? Is it referring to stable , unstable , and semi-stable ? Likewise, is the type of critical point choosing amongst center , node , saddle point , and spiral ? Thank you","I'm uncertain if I have all the critical points required of the system I do $\frac{dx}{dt} = x(x-2) + y^2$...and likewise..I do $\frac{dy}{dt}= y(1-x)$ I then set both equations to zero If we set $x = 0$ in the first equation, we solve the second equation $y = 0$ If we set $y = 0$ in the second equation, we solve the first equation for $x = 0$ and $x = 2$.  Likewise, I solve for y - xy = 0. Then xy = y so x = 1. We plug this solution into the first equation so that y^2 = 1 and hence, have (1,1),(1,-1) as our results as well. Hence, my critical points are then $(0,0);(2,0)$;(1,1);(1,-1) Is this the correct approach for this particular question? Likewise, what does it mean to determine the stability of the critical point and the type of critical point? Is it referring to stable , unstable , and semi-stable ? Likewise, is the type of critical point choosing amongst center , node , saddle point , and spiral ? Thank you",,"['ordinary-differential-equations', 'nonlinear-system', 'homogeneous-equation']"
11,Resonance in Mathieu's Equation,Resonance in Mathieu's Equation,,"Consider the Mathieu's Equation: $$\frac{d^{2}u}{dt^2}+[\omega^2 + 2\epsilon \cos(2t)]u=0$$ with $u(0)=1$ and $u'(0)=0$ What I have done is, assume $u(t)=u_{0}(t)+\epsilon u_{1}(t)+\cdots$, substitute into the DE, and we get $$u_{0}(t)=\cos(\omega t)$$ and $$u_{1}(t)=\frac{(1-\omega)\cos[(2+\omega)t]-2\cos(\omega t)+(1+\omega)\cos[(2-\omega)t]}{4-4\omega^2}$$. The question is, what modes are resonant at order $\epsilon$? and what frequencies are resonant at the next order (i think it means order $\epsilon^2$)? What are the definition of modes and resonance in this problem? I have no idea how to proceed because I do not know the definition. Can anyone tell me what does it mean by ""modes"" and ""resonant""? Moreover, What is ""frequency"" here? The problem is just an ODE, where does the word ""frequency"" come from?","Consider the Mathieu's Equation: $$\frac{d^{2}u}{dt^2}+[\omega^2 + 2\epsilon \cos(2t)]u=0$$ with $u(0)=1$ and $u'(0)=0$ What I have done is, assume $u(t)=u_{0}(t)+\epsilon u_{1}(t)+\cdots$, substitute into the DE, and we get $$u_{0}(t)=\cos(\omega t)$$ and $$u_{1}(t)=\frac{(1-\omega)\cos[(2+\omega)t]-2\cos(\omega t)+(1+\omega)\cos[(2-\omega)t]}{4-4\omega^2}$$. The question is, what modes are resonant at order $\epsilon$? and what frequencies are resonant at the next order (i think it means order $\epsilon^2$)? What are the definition of modes and resonance in this problem? I have no idea how to proceed because I do not know the definition. Can anyone tell me what does it mean by ""modes"" and ""resonant""? Moreover, What is ""frequency"" here? The problem is just an ODE, where does the word ""frequency"" come from?",,"['ordinary-differential-equations', 'stability-in-odes', 'perturbation-theory']"
12,Solution of differential equation $d/dt \langle N(t) \rangle=k-\Gamma \langle N(t) \rangle$ where brackets indicate average,Solution of differential equation  where brackets indicate average,d/dt \langle N(t) \rangle=k-\Gamma \langle N(t) \rangle,"I have the following ODE yielded as a steady state solution to a more complicated ODE. The important part is just to consider the following: $$d/dt \langle N(t) \rangle=k-\Gamma \langle N(t) \rangle$$ where $\langle N(t) \rangle$ can be seen as $y$ or an average of $y$ (brackets indicating avg). I am not sure if that is the reason for the result. The solution is given by $$\langle N(t) \rangle=k/\Gamma (1-e^{-\Gamma t})+\langle N(t) \rangle e^{-\Gamma t}$$ I do not see that however. So far, I've solved first for  $$d/dt <N(t)>=\Gamma <N(t)>=0$$ Yielding $$\langle N(t) \rangle>=C\cdot e^{-\Gamma t}$$ and $C$ $$C=k/\Gamma (1-e^{-\Gamma t})+c_0$$ I do not see how it solves the equation by inserting c. Any help is highly appreciated :) edit: I had a thought it may have to do with the average being additive, and starting and  $\langle N(0) \rangle$ being the same as $\langle N(t) \rangle$ since it is steady state. I am not completely sure however.","I have the following ODE yielded as a steady state solution to a more complicated ODE. The important part is just to consider the following: $$d/dt \langle N(t) \rangle=k-\Gamma \langle N(t) \rangle$$ where $\langle N(t) \rangle$ can be seen as $y$ or an average of $y$ (brackets indicating avg). I am not sure if that is the reason for the result. The solution is given by $$\langle N(t) \rangle=k/\Gamma (1-e^{-\Gamma t})+\langle N(t) \rangle e^{-\Gamma t}$$ I do not see that however. So far, I've solved first for  $$d/dt <N(t)>=\Gamma <N(t)>=0$$ Yielding $$\langle N(t) \rangle>=C\cdot e^{-\Gamma t}$$ and $C$ $$C=k/\Gamma (1-e^{-\Gamma t})+c_0$$ I do not see how it solves the equation by inserting c. Any help is highly appreciated :) edit: I had a thought it may have to do with the average being additive, and starting and  $\langle N(0) \rangle$ being the same as $\langle N(t) \rangle$ since it is steady state. I am not completely sure however.",,['ordinary-differential-equations']
13,Unable to Find My Error for First Order Differential Problem,Unable to Find My Error for First Order Differential Problem,,"The Question: Given $f'(x) = 5f(x)$ and $f(3) = 2$ find the equation for $f(x)$ My Problem: My attempt to solve it produced the wrong answer. I got $f(x) = \frac{28}{5x - 1}$ but the correct answer is $f(x) = 2e^{5(x-3)}$ (you can follow this link to see how it is solved). Note: I am not asking how to get the correct answer, as I have already found and understood such an explanation. I am instead asking why my process is incorrect. My Method: $\frac{df(x)}{dx} = 5f(x)$ $df(x) = 5f(x)dx$ $\int{df(x)} = \int{5f(x)dx}$ $f(x) = 5f(x)\cdot x + C$ $f(3) = 5f(3)\cdot 3 + C$ $2 = 30 + C$ $C = -28$ $f(x) = 5f(x)\cdot x - 28$ $28 = f(x)(5x - 1)$ $f(x) = \frac{28}{5x - 1}$ Explanation: I know my answer is wrong, as it does not satisfy the condition $f'(x) = 5f(x)$, yet I am intrigued by this problem as I cannot seem to figure out what I did wrong. My answer does satisfy the condition $f(3) = 2$ and its derivative - $f'(x) = -\frac{140}{(5x - 1)^2}$ - is suggestive of some similarity to the original differential (eg numerator is multiplied by 5). Perhaps someone can enlighten me regarding where I went astray. Thank you.","The Question: Given $f'(x) = 5f(x)$ and $f(3) = 2$ find the equation for $f(x)$ My Problem: My attempt to solve it produced the wrong answer. I got $f(x) = \frac{28}{5x - 1}$ but the correct answer is $f(x) = 2e^{5(x-3)}$ (you can follow this link to see how it is solved). Note: I am not asking how to get the correct answer, as I have already found and understood such an explanation. I am instead asking why my process is incorrect. My Method: $\frac{df(x)}{dx} = 5f(x)$ $df(x) = 5f(x)dx$ $\int{df(x)} = \int{5f(x)dx}$ $f(x) = 5f(x)\cdot x + C$ $f(3) = 5f(3)\cdot 3 + C$ $2 = 30 + C$ $C = -28$ $f(x) = 5f(x)\cdot x - 28$ $28 = f(x)(5x - 1)$ $f(x) = \frac{28}{5x - 1}$ Explanation: I know my answer is wrong, as it does not satisfy the condition $f'(x) = 5f(x)$, yet I am intrigued by this problem as I cannot seem to figure out what I did wrong. My answer does satisfy the condition $f(3) = 2$ and its derivative - $f'(x) = -\frac{140}{(5x - 1)^2}$ - is suggestive of some similarity to the original differential (eg numerator is multiplied by 5). Perhaps someone can enlighten me regarding where I went astray. Thank you.",,"['calculus', 'ordinary-differential-equations']"
14,Relation between second derivatives and mixed derivatives,Relation between second derivatives and mixed derivatives,,"I'm studying the first chapter of Fourier Analysis -- An Introduction and unfortunately I don't understand some transitions. There is an equation given (wave equation): $$\frac{\partial^2u}{\partial t^2} = \frac{\partial^2u}{\partial x^2}. \tag{1}$$ Moreover we know that: $$u(x,t) = F(x+t) + G(x-t)$$ is the solution of the equation $(1)$, where $F, G$ are twice differentiable functions. We are to show that every solution takes this form. Now we define new variables: $\xi = x+t$, $\eta = x-t$. We define a new function: $$v(\xi, \eta) = u(x,t).$$ Everything is quite obvious for the time being. I don't understand the next step however. My book says, that: The change of variables formula shows that v satisfies: $$\frac{\partial^2v}{\partial \xi \partial \eta} =0. \tag{2}$$ I don't know why. I would appreciate any explanation. My book also says that integrating $(2)$ twice will give us: $$v(\xi, \eta) = F(\xi) + G(\eta).$$ Here again I don't know what the integration should look like. Thanks for any help!","I'm studying the first chapter of Fourier Analysis -- An Introduction and unfortunately I don't understand some transitions. There is an equation given (wave equation): $$\frac{\partial^2u}{\partial t^2} = \frac{\partial^2u}{\partial x^2}. \tag{1}$$ Moreover we know that: $$u(x,t) = F(x+t) + G(x-t)$$ is the solution of the equation $(1)$, where $F, G$ are twice differentiable functions. We are to show that every solution takes this form. Now we define new variables: $\xi = x+t$, $\eta = x-t$. We define a new function: $$v(\xi, \eta) = u(x,t).$$ Everything is quite obvious for the time being. I don't understand the next step however. My book says, that: The change of variables formula shows that v satisfies: $$\frac{\partial^2v}{\partial \xi \partial \eta} =0. \tag{2}$$ I don't know why. I would appreciate any explanation. My book also says that integrating $(2)$ twice will give us: $$v(\xi, \eta) = F(\xi) + G(\eta).$$ Here again I don't know what the integration should look like. Thanks for any help!",,"['ordinary-differential-equations', 'derivatives', 'wave-equation']"
15,Help understanding/proving a simple claim about sinks and sources.,Help understanding/proving a simple claim about sinks and sources.,,"I am reading an introductory differential equations text where the author makes a claim that I feel should be obvious, but I cannot prove to myself. The author proposes a  first-order, autonomous differential equation of the form $x' = f(x)$, where solution $x$ is a function of real variable $t$, possessing two equilibrium points, $x_l$ and $x_r$, such that $f'(x_l)>0$ and $f'(x_r)<0$. I assume these are derivatives w/ respect to $x$. Because of the sign of their derivatives, he says $x_l$ is a source and $x_r$ is a sink. Would someone please provide a hint on how to prove the previous statement? I've tried working along the lines of $$\frac{f(y)-f(x_l)}{y-x_l}=\frac{x'(y)-x'(x_l)}{y-x_l}=\frac{x'(y)}{y-x_l}>0$$ for all $y$ in some neighborhood of $x_l$, but now I'm unsure of how to work $t$ into the picture. The mixture of derivatives w.r.t $x$ and $t$ and the function $x$ being treated as a variable makes my head spin and blocks me from any good intuition. Apart from help with this specific problem, advice on how to think about/approach these problems would be welcome. Thanks in advance, Paul","I am reading an introductory differential equations text where the author makes a claim that I feel should be obvious, but I cannot prove to myself. The author proposes a  first-order, autonomous differential equation of the form $x' = f(x)$, where solution $x$ is a function of real variable $t$, possessing two equilibrium points, $x_l$ and $x_r$, such that $f'(x_l)>0$ and $f'(x_r)<0$. I assume these are derivatives w/ respect to $x$. Because of the sign of their derivatives, he says $x_l$ is a source and $x_r$ is a sink. Would someone please provide a hint on how to prove the previous statement? I've tried working along the lines of $$\frac{f(y)-f(x_l)}{y-x_l}=\frac{x'(y)-x'(x_l)}{y-x_l}=\frac{x'(y)}{y-x_l}>0$$ for all $y$ in some neighborhood of $x_l$, but now I'm unsure of how to work $t$ into the picture. The mixture of derivatives w.r.t $x$ and $t$ and the function $x$ being treated as a variable makes my head spin and blocks me from any good intuition. Apart from help with this specific problem, advice on how to think about/approach these problems would be welcome. Thanks in advance, Paul",,['ordinary-differential-equations']
16,Why are power series centered around 0,Why are power series centered around 0,,I was wondering if there was any particular reason that power series are centred around 0. $$\displaystyle f(x)=\sum_{k=0}^{\infty}c_{k}x^{k}\qquad \text{ vs  }\qquad\displaystyle f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k}$$ when using them to solve differential equations. I've tried looking online for a reason but so far have had no luck. This seems like a rather arbitrary choice that only conveniently simplifies our computation - eg. what if the function is not well-defined at 0 or deviates significantly away from 0? Apologies if this seems like a trivial question - I have not done a differential equations course yet that involves power series. Solutions involving only first-year university maths would be much appreciated.,I was wondering if there was any particular reason that power series are centred around 0. $$\displaystyle f(x)=\sum_{k=0}^{\infty}c_{k}x^{k}\qquad \text{ vs  }\qquad\displaystyle f(x)=\sum_{k=0}^{\infty}c_{k}(x-a)^{k}$$ when using them to solve differential equations. I've tried looking online for a reason but so far have had no luck. This seems like a rather arbitrary choice that only conveniently simplifies our computation - eg. what if the function is not well-defined at 0 or deviates significantly away from 0? Apologies if this seems like a trivial question - I have not done a differential equations course yet that involves power series. Solutions involving only first-year university maths would be much appreciated.,,"['ordinary-differential-equations', 'power-series']"
17,Do other people use Strang's convention for integrals?,Do other people use Strang's convention for integrals?,,"Gilbert Strang's very vivid textbook Differential Equations and Linear Algebra uses a convention for integrals which seems unusual to me.  It is very handy in this context, though, and I would like to know if other people use it.  In effect he adopts the convention that an integral $\int_{a}^{b}f(s)ds$  only goes forwards from $a$ to $b$.  That is: $$\int_{a}^{b}f(s)ds = \begin{cases}                          0, & \mbox{if } b\leq a \\                          F(b)-F(a) & \mbox{when } a\leq b\mbox{ and } \frac{dF}{ds}=f(s).                        \end{cases}$$ You can see this in his particular solution for the first order linear differential equation with shifted Heaviside function $H(t-T)$ as source.  That equation is  $$y'-ay\ =\ H(t-T).$$ Here $t$ is the usual time variable, and $T$ is a constant time.  He gives the particular solution with initial value $y(0)=0$ as $$y_p\ =\ \int_{T}^{t}e^{-as}ds \ =\ \frac{1}{a}(e^{a(t-T)} - e^{-at}).$$ The second equality is right if $t>T$, but for $t<T$ we must use Strang's convention for integrals. Do others use this convention? I can add: This book takes a very quick and practical approach a lot of the time.  It assumes without comment that (nearly) every function is real analytic, and while it points out some places where a solution goes to infinity it does not remark that this conflicts with assuming functions are limits of their Taylor series around arbitrary points.  It gives no proofs of existence or uniqueness of solutions to ode's except by implicitly suggesting these follow from integrating functions term by term in their Taylor series. But it is an extremely vivid book and I think it does well for a first course on ode's. The convention suits Strang's purposes.  You can see above it lets him write solutions more compactly.  He is never interested in moving the time variable backwards, and he usually cares more for the long run behavior of solutions than for what they do before some given time $T$. I think it would not be analysts who would use this convention (outside of Strang's textbook) but engineers or physicists.","Gilbert Strang's very vivid textbook Differential Equations and Linear Algebra uses a convention for integrals which seems unusual to me.  It is very handy in this context, though, and I would like to know if other people use it.  In effect he adopts the convention that an integral $\int_{a}^{b}f(s)ds$  only goes forwards from $a$ to $b$.  That is: $$\int_{a}^{b}f(s)ds = \begin{cases}                          0, & \mbox{if } b\leq a \\                          F(b)-F(a) & \mbox{when } a\leq b\mbox{ and } \frac{dF}{ds}=f(s).                        \end{cases}$$ You can see this in his particular solution for the first order linear differential equation with shifted Heaviside function $H(t-T)$ as source.  That equation is  $$y'-ay\ =\ H(t-T).$$ Here $t$ is the usual time variable, and $T$ is a constant time.  He gives the particular solution with initial value $y(0)=0$ as $$y_p\ =\ \int_{T}^{t}e^{-as}ds \ =\ \frac{1}{a}(e^{a(t-T)} - e^{-at}).$$ The second equality is right if $t>T$, but for $t<T$ we must use Strang's convention for integrals. Do others use this convention? I can add: This book takes a very quick and practical approach a lot of the time.  It assumes without comment that (nearly) every function is real analytic, and while it points out some places where a solution goes to infinity it does not remark that this conflicts with assuming functions are limits of their Taylor series around arbitrary points.  It gives no proofs of existence or uniqueness of solutions to ode's except by implicitly suggesting these follow from integrating functions term by term in their Taylor series. But it is an extremely vivid book and I think it does well for a first course on ode's. The convention suits Strang's purposes.  You can see above it lets him write solutions more compactly.  He is never interested in moving the time variable backwards, and he usually cares more for the long run behavior of solutions than for what they do before some given time $T$. I think it would not be analysts who would use this convention (outside of Strang's textbook) but engineers or physicists.",,"['calculus', 'ordinary-differential-equations', 'notation']"
18,"Evaluating $y(1)$ in the initial value problem $\dot y = ty + \sin y , y(0)=10^{-2}$",Evaluating  in the initial value problem,"y(1) \dot y = ty + \sin y , y(0)=10^{-2}","For the following initial value problem: $$ \left\{  \begin{array}{} \dot y=ty+\sin y \\  y(0)=10^{-2}  \end{array} \right.  $$ I need to evaluate $y(1)$. I thought about using the fact that $\dot y \leq ty+1$, and solve $\dot z=tz+1$, but I couldn't solve that. I was also given a hint to use the Integral form of the equation, that is $y=10^{-2}+\int_0^t(sy+\sin y)ds$, but I can't think of a way to use that.","For the following initial value problem: $$ \left\{  \begin{array}{} \dot y=ty+\sin y \\  y(0)=10^{-2}  \end{array} \right.  $$ I need to evaluate $y(1)$. I thought about using the fact that $\dot y \leq ty+1$, and solve $\dot z=tz+1$, but I couldn't solve that. I was also given a hint to use the Integral form of the equation, that is $y=10^{-2}+\int_0^t(sy+\sin y)ds$, but I can't think of a way to use that.",,"['ordinary-differential-equations', 'inequality', 'initial-value-problems']"
19,What does it mean to substitute $y = x''$,What does it mean to substitute,y = x'',"The textbook I'm reading says this for this problem: $$x^2y''+2xy' − 2y=0.$$ Since differentiating a power pushes down the exponent by one unit,   the form of this equation suggests that we look for possible solutions   of the type $y=x''$. On substituting this in the differential equation   and dividing by the common factor $x''$, we obtain the quadratic   equation $n(n − 1)+2n − 2=0$ What do they mean by this substitution, taking it like a literal substitution gives this nonsense which I obviously can't divide out the common factor $x''$ Literal substiution gives $$x^2(x'')'' + 2x(x'')' -2(x'') = 0$$ I can't divide everything by $x''$ because not everything is being multiplied by $x''$, for example, $(x'')''$ is the second derivative of $x''$ so I can't factor out $x''$ from that. What does the author mean by substituting $y=x''$, how did he end up with a quadratic? Screenshot:","The textbook I'm reading says this for this problem: $$x^2y''+2xy' − 2y=0.$$ Since differentiating a power pushes down the exponent by one unit,   the form of this equation suggests that we look for possible solutions   of the type $y=x''$. On substituting this in the differential equation   and dividing by the common factor $x''$, we obtain the quadratic   equation $n(n − 1)+2n − 2=0$ What do they mean by this substitution, taking it like a literal substitution gives this nonsense which I obviously can't divide out the common factor $x''$ Literal substiution gives $$x^2(x'')'' + 2x(x'')' -2(x'') = 0$$ I can't divide everything by $x''$ because not everything is being multiplied by $x''$, for example, $(x'')''$ is the second derivative of $x''$ so I can't factor out $x''$ from that. What does the author mean by substituting $y=x''$, how did he end up with a quadratic? Screenshot:",,['ordinary-differential-equations']
20,Pedal form to polar form of an ellipse,Pedal form to polar form of an ellipse,,"How do I convert the equation $$\frac{a^2b^2}{p^2}=a^2+b^2-\frac{1}{u^2}$$ into the following equivalent form? $$u^2=\frac{\sin^2 \theta}{b^2}+\frac{\cos^2 \theta}{a^2}$$ where $$\frac{1}{p^2}=u^2+\left( \frac{du}{d\theta} \right)^2$$ EDIT: I have tried and found out $$\frac{uab}{\sqrt{u^2(a^2+b^2)-1-u^4a^2b^2}}\, du = d\theta$$ How do I integrate and express the result in terms of $\sin \theta$ and $\cos \theta$? $p$ is the perpendicular distance from $O$ to the tangent line to $C$ at the point in case of pedal equation of a curve.  It is converted to $u$ as stated above.","How do I convert the equation $$\frac{a^2b^2}{p^2}=a^2+b^2-\frac{1}{u^2}$$ into the following equivalent form? $$u^2=\frac{\sin^2 \theta}{b^2}+\frac{\cos^2 \theta}{a^2}$$ where $$\frac{1}{p^2}=u^2+\left( \frac{du}{d\theta} \right)^2$$ EDIT: I have tried and found out $$\frac{uab}{\sqrt{u^2(a^2+b^2)-1-u^4a^2b^2}}\, du = d\theta$$ How do I integrate and express the result in terms of $\sin \theta$ and $\cos \theta$? $p$ is the perpendicular distance from $O$ to the tangent line to $C$ at the point in case of pedal equation of a curve.  It is converted to $u$ as stated above.",,"['integration', 'ordinary-differential-equations', 'multivariable-calculus']"
21,Undetermined Coefficients for solving non homogeneous equation,Undetermined Coefficients for solving non homogeneous equation,,"I can't seem to figure out where I am going wrong in my steps. I checked the answer and it is different. The question is: $$y'' + 2y' - 3y = 3te^t$$ The roots are: -3,1. Thus the general solution is: $$y=C_1e^{-3t} + C_2e^t$$ The particular solution i am going with is: $$y_p = Ate^t$$ $$y'_p = Ae^t + Ate^t$$ $$y''_p = 2Ae^t + Ate^t$$ Therefore: $$2Ae^t + Ate^t + 2Ae^t + 2Ate^t - 3Ate^t = 3te^t$$ $$4Ae^t = 3te^t$$ Then solving for A: $$4A=3$$ $$A=3/4$$ Thus, $y_p = \frac{3}{4}(e^t + te^t)$ Then the general solution would be: $y = C_1e^{-3t} + C_2e^t  + \frac{3}{4}(e^t + te^t)$ Any guidance with my mistake would be greatly appreciated. As an aside, what does it mean when a question asks to use the stability result to determine they will have a globally stable solution of the above question. Thank you.","I can't seem to figure out where I am going wrong in my steps. I checked the answer and it is different. The question is: $$y'' + 2y' - 3y = 3te^t$$ The roots are: -3,1. Thus the general solution is: $$y=C_1e^{-3t} + C_2e^t$$ The particular solution i am going with is: $$y_p = Ate^t$$ $$y'_p = Ae^t + Ate^t$$ $$y''_p = 2Ae^t + Ate^t$$ Therefore: $$2Ae^t + Ate^t + 2Ae^t + 2Ate^t - 3Ate^t = 3te^t$$ $$4Ae^t = 3te^t$$ Then solving for A: $$4A=3$$ $$A=3/4$$ Thus, $y_p = \frac{3}{4}(e^t + te^t)$ Then the general solution would be: $y = C_1e^{-3t} + C_2e^t  + \frac{3}{4}(e^t + te^t)$ Any guidance with my mistake would be greatly appreciated. As an aside, what does it mean when a question asks to use the stability result to determine they will have a globally stable solution of the above question. Thank you.",,"['calculus', 'ordinary-differential-equations', 'stability-in-odes']"
22,Eigenvalues of second order ordinary differential equation,Eigenvalues of second order ordinary differential equation,,"The question is find all the eigenvalue of the following equation $-\frac{d^2y}{dx^2}+x^2y=\lambda y$ I have found the first function which is $y=e^{\frac{-x^2}{2}}$, however I have no clue on how to find the rest, can someone please help me with it.","The question is find all the eigenvalue of the following equation $-\frac{d^2y}{dx^2}+x^2y=\lambda y$ I have found the first function which is $y=e^{\frac{-x^2}{2}}$, however I have no clue on how to find the rest, can someone please help me with it.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions']"
23,"Solution of $\frac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x$ given $x=0$, $y=\mathrm{e}$","Solution of  given ,",\frac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x x=0 y=\mathrm{e},"$\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x$ , $x=0$ and $y=\mathrm{e}$ . Find the particular solution. Attempt 1 $$ \dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \log|y|=\mathrm{e}^x+C\\ x=0,\,y=\mathrm{e}\implies\log|\mathrm{e}|=\log\mathrm{e}=1=1+C\implies C=0\\ \log|y|=\mathrm{e}^x\implies|y|=\mathrm{e}^{\mathrm{e}^x}\implies \color{red}{y=\pm\mathrm{e}^{\mathrm{e}^x}} $$ Attempt 2 $$ \dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \mathrm{e}^x=\log|y|+\log|C_1|=\log|C_1y|\\ \mathrm{e}^{\mathrm{e}^x}=|C_1y|=\pm C_1y=Cy\\ x=0,\,y=\mathrm{e}\implies\mathrm{e}=C\mathrm{e}\implies C=1\\ \implies \color{red}{y=\mathrm{e}^{\mathrm{e}^x}} $$ My reference also gives the solution $\log y=\mathrm{e}^x$ as in attempt 2. Why do I seem to get positive and negatve solutions in attempt 1 ? How do I eliminate the solution $y=-\mathrm{e}^{\mathrm{e}^x}$ in attempt 1 ? Note: I am not quite familiar with the idea of singularity or intermediate value theorem, as i have only done preliminary maths on first order differential equations.",", and . Find the particular solution. Attempt 1 Attempt 2 My reference also gives the solution as in attempt 2. Why do I seem to get positive and negatve solutions in attempt 1 ? How do I eliminate the solution in attempt 1 ? Note: I am not quite familiar with the idea of singularity or intermediate value theorem, as i have only done preliminary maths on first order differential equations.","\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x x=0 y=\mathrm{e} 
\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \log|y|=\mathrm{e}^x+C\\
x=0,\,y=\mathrm{e}\implies\log|\mathrm{e}|=\log\mathrm{e}=1=1+C\implies C=0\\
\log|y|=\mathrm{e}^x\implies|y|=\mathrm{e}^{\mathrm{e}^x}\implies \color{red}{y=\pm\mathrm{e}^{\mathrm{e}^x}}
 
\dfrac{\mathrm{d}y}{\mathrm{d}x}=y\mathrm{e}^x\implies\dfrac{\mathrm{d}y}{y}=\mathrm{e}^x\,\mathrm{d}x\implies \mathrm{e}^x=\log|y|+\log|C_1|=\log|C_1y|\\
\mathrm{e}^{\mathrm{e}^x}=|C_1y|=\pm C_1y=Cy\\
x=0,\,y=\mathrm{e}\implies\mathrm{e}=C\mathrm{e}\implies C=1\\
\implies \color{red}{y=\mathrm{e}^{\mathrm{e}^x}}
 \log y=\mathrm{e}^x y=-\mathrm{e}^{\mathrm{e}^x}","['ordinary-differential-equations', 'logarithms', 'exponential-function', 'absolute-value']"
24,show that any orbit that intercepts the unitary sphere $S^{n-1}$ is contained in it also,show that any orbit that intercepts the unitary sphere  is contained in it also,S^{n-1},"Let $X : U \rightarrow R^n$ a vector field such that $ \langle X(p); p \rangle = 0 \,\,\,\,\,\,\,\,\ \forall p \in R^n; \,\,\,\,\,\ ||p|| = 1$; where $\langle \cdot ; \cdot \rangle$ is the inner product canonic in $R^n$ and $||\cdot ||$ is the Euclidian norm. show that any orbit that intercepts the unitary sphere $S^{n-1} := \{p \in R^n; ||p|| = 1 \}$ is contained in the sphere. I've tried several local theorems to use, but none seems to work well. Can anyone help?","Let $X : U \rightarrow R^n$ a vector field such that $ \langle X(p); p \rangle = 0 \,\,\,\,\,\,\,\,\ \forall p \in R^n; \,\,\,\,\,\ ||p|| = 1$; where $\langle \cdot ; \cdot \rangle$ is the inner product canonic in $R^n$ and $||\cdot ||$ is the Euclidian norm. show that any orbit that intercepts the unitary sphere $S^{n-1} := \{p \in R^n; ||p|| = 1 \}$ is contained in the sphere. I've tried several local theorems to use, but none seems to work well. Can anyone help?",,"['ordinary-differential-equations', 'vector-fields']"
25,Solving $f(x)= f''(x)$,Solving,f(x)= f''(x),"I am studying limits, continuity and derivability and I have got a continuous function $f\colon \mathbb R \to \mathbb R$ such that $f''(x)= f(x)$. How do I find $f(x)$ from this information? This is the first time I have come across such a problem, that's the reason I am stuck. I know that its obvious that it should be $e^x$, but what's the formal method of finding the function? Trigonometric functions like sin or cos don't satisfy the relation. Wolfram alpha gives the solution to be $f(x)= c_1e^x + c_2 e^{-x}$","I am studying limits, continuity and derivability and I have got a continuous function $f\colon \mathbb R \to \mathbb R$ such that $f''(x)= f(x)$. How do I find $f(x)$ from this information? This is the first time I have come across such a problem, that's the reason I am stuck. I know that its obvious that it should be $e^x$, but what's the formal method of finding the function? Trigonometric functions like sin or cos don't satisfy the relation. Wolfram alpha gives the solution to be $f(x)= c_1e^x + c_2 e^{-x}$",,"['ordinary-differential-equations', 'functions']"
26,Other methods for solving homogeneous differential equation,Other methods for solving homogeneous differential equation,,"I want to find a general solution of the following homogeneous equation $$\frac{dy}{dt}=\frac{3t+12y}{t+14y}$$ I have tried using the substitution $z = y/t$ to make the equation separable, but then it gets a little bit tedious to solve. $$\frac{dy}{dt}=\frac{3+12z}{1+14z}=z+\frac{dz}{dt}t$$ $$\int \frac{1+14z}{-14z^2+11z+3} dz = \ln(t)+c_1$$ So I'm wondering if there's another way to solve it?","I want to find a general solution of the following homogeneous equation I have tried using the substitution to make the equation separable, but then it gets a little bit tedious to solve. So I'm wondering if there's another way to solve it?",\frac{dy}{dt}=\frac{3t+12y}{t+14y} z = y/t \frac{dy}{dt}=\frac{3+12z}{1+14z}=z+\frac{dz}{dt}t \int \frac{1+14z}{-14z^2+11z+3} dz = \ln(t)+c_1,['ordinary-differential-equations']
27,How to derive the representation formula from $\mathbb R^3$ to $\mathbb R^2$?,How to derive the representation formula from  to ?,\mathbb R^3 \mathbb R^2,"Prove that in $\mathbb R^2$, we have the following representation formula for harmonic function $u$:$$u(\vec x_0)=\frac{1}{2\pi}\int_{\partial D}\left(u(\vec x)\frac{\partial}{\partial \vec n}(\ln|\vec x-\vec x_0|)-\ln|\vec x-\vec x_0 |\frac{\partial u}{\partial \vec n}(\vec x)\right)\,\mathrm ds.$$ In class we  found the representation formula in $\mathbb R^3$, which is $$u(\vec x_0)=\frac{1}{4\pi}\iint_{\partial D}\left(-u(\vec x)\frac{\partial}{\partial \vec n}\left(\frac{1}{| \vec x-\vec x_0|}\right)+\frac{1}{|\vec x-\vec x_0 |}\frac{\partial u}{\partial \vec n}(\vec x)\right)\,\mathrm dA.$$ In the process we did not use at all the Dirac delta function. I have seen a proof * for the representation formula in $\mathbb R^2$ using that function and Heavyside-function as well, but I did not understand this part: $$v(x)=\frac{1}{2\pi}\log| x- x_0|$$ satisfies the identity $$\Delta v=\delta(x-x_0).$$ Could someone explain with details that part? Alternatively, how to use the representation formula in $\mathbb R^3$ to prove in $\mathbb R^2$? If you see Chee Han comment, maybe could help although I did not understand his comment. Please help me with the proof. Proof *","Prove that in $\mathbb R^2$, we have the following representation formula for harmonic function $u$:$$u(\vec x_0)=\frac{1}{2\pi}\int_{\partial D}\left(u(\vec x)\frac{\partial}{\partial \vec n}(\ln|\vec x-\vec x_0|)-\ln|\vec x-\vec x_0 |\frac{\partial u}{\partial \vec n}(\vec x)\right)\,\mathrm ds.$$ In class we  found the representation formula in $\mathbb R^3$, which is $$u(\vec x_0)=\frac{1}{4\pi}\iint_{\partial D}\left(-u(\vec x)\frac{\partial}{\partial \vec n}\left(\frac{1}{| \vec x-\vec x_0|}\right)+\frac{1}{|\vec x-\vec x_0 |}\frac{\partial u}{\partial \vec n}(\vec x)\right)\,\mathrm dA.$$ In the process we did not use at all the Dirac delta function. I have seen a proof * for the representation formula in $\mathbb R^2$ using that function and Heavyside-function as well, but I did not understand this part: $$v(x)=\frac{1}{2\pi}\log| x- x_0|$$ satisfies the identity $$\Delta v=\delta(x-x_0).$$ Could someone explain with details that part? Alternatively, how to use the representation formula in $\mathbb R^3$ to prove in $\mathbb R^2$? If you see Chee Han comment, maybe could help although I did not understand his comment. Please help me with the proof. Proof *",,"['ordinary-differential-equations', 'partial-differential-equations', 'proof-explanation', 'harmonic-functions']"
28,find value integral $\int_{1}^{2}f(x)dx$,find value integral,\int_{1}^{2}f(x)dx,For a function $f(x)$ determined and continuous with $\forall x\in $ $\Bbb R\setminus\left\{ 0 \right\}$  such that $$x^{2}f^{2}(x)+(2x-1)f(x)=xf'(x)-1$$ and $f(1)=-2$ Find  $$\int_{1}^{2} f(x)dx $$ I write:  $$(xf(x)+1)^{2}=(xf(x)+1)'$$ But I do not know how to proceed,For a function $f(x)$ determined and continuous with $\forall x\in $ $\Bbb R\setminus\left\{ 0 \right\}$  such that $$x^{2}f^{2}(x)+(2x-1)f(x)=xf'(x)-1$$ and $f(1)=-2$ Find  $$\int_{1}^{2} f(x)dx $$ I write:  $$(xf(x)+1)^{2}=(xf(x)+1)'$$ But I do not know how to proceed,,"['real-analysis', 'integration', 'ordinary-differential-equations']"
29,Tricky Bendixson-Dulac question,Tricky Bendixson-Dulac question,,"The Question: Consider the system $$\frac{dx}{dt}=x(2-x)-y \qquad \frac{dy}{dt} = y-kx$$ where $k>2$ is a constant. Show that there are no non-trivial closed trajectories in the quadrant $x,y≥0$ using the Bendixson-Dulac Theorem. Bendixson-Dulac Theorem: For the system $$\frac{dx}{dt} = X(x,y) \qquad \frac{dy}{dt} = Y(x,y)$$ where $X,Y \in C^1$, if there exists $\phi = \phi(x,y) \in C^1$ such that $$\frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) >0$$ everywhere in a simply connected region $R$, then the system has no non-trivial closed trajectories lying entirely in $R$. My Attempt: So I considered a general (continuously differentiable) function $\phi = \phi (x,y)$. Then \begin{align} & \frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) \\ = & (3\phi) + \bigg(-2\phi +2\frac{\partial \phi}{\partial x}-k\frac{\partial \phi}{\partial y} \bigg) x + \bigg(-\frac{\partial \phi}{\partial x}+\frac{\partial \phi}{\partial y} \bigg) y + \bigg(-\frac{\partial \phi}{\partial x} \bigg)x^2 \end{align} Suppose we insist that each of the four functions in brackets must be everywhere positive in the quadrant $x,y≥0$. Then $\phi>0$ from the first term. $\dfrac{\partial \phi}{\partial x}<0$ from the last term. $\dfrac{\partial \phi}{\partial y}>0$ from the third term. But this would imply that everything in the second bracket is negative, so this does not work. And I am stuck. Any hints? Why does it even matter that $k>2$?","The Question: Consider the system $$\frac{dx}{dt}=x(2-x)-y \qquad \frac{dy}{dt} = y-kx$$ where $k>2$ is a constant. Show that there are no non-trivial closed trajectories in the quadrant $x,y≥0$ using the Bendixson-Dulac Theorem. Bendixson-Dulac Theorem: For the system $$\frac{dx}{dt} = X(x,y) \qquad \frac{dy}{dt} = Y(x,y)$$ where $X,Y \in C^1$, if there exists $\phi = \phi(x,y) \in C^1$ such that $$\frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) >0$$ everywhere in a simply connected region $R$, then the system has no non-trivial closed trajectories lying entirely in $R$. My Attempt: So I considered a general (continuously differentiable) function $\phi = \phi (x,y)$. Then \begin{align} & \frac{\partial}{\partial x} \big(\phi X \big)+\frac{\partial}{\partial y} \big (\phi Y \big) \\ = & (3\phi) + \bigg(-2\phi +2\frac{\partial \phi}{\partial x}-k\frac{\partial \phi}{\partial y} \bigg) x + \bigg(-\frac{\partial \phi}{\partial x}+\frac{\partial \phi}{\partial y} \bigg) y + \bigg(-\frac{\partial \phi}{\partial x} \bigg)x^2 \end{align} Suppose we insist that each of the four functions in brackets must be everywhere positive in the quadrant $x,y≥0$. Then $\phi>0$ from the first term. $\dfrac{\partial \phi}{\partial x}<0$ from the last term. $\dfrac{\partial \phi}{\partial y}>0$ from the third term. But this would imply that everything in the second bracket is negative, so this does not work. And I am stuck. Any hints? Why does it even matter that $k>2$?",,"['ordinary-differential-equations', 'stability-in-odes']"
30,The formula for the $n^\text{th}$ term of $\frac{x^2}6 -\frac{x^4}9 + \frac{3x^6}{80} - \frac{71 x^8}{15120} + \frac{ 10361 x^{10}}{10886400} \dots$?,The formula for the  term of ?,n^\text{th} \frac{x^2}6 -\frac{x^4}9 + \frac{3x^6}{80} - \frac{71 x^8}{15120} + \frac{ 10361 x^{10}}{10886400} \dots,"I obtained an infinite series after solving a non-linear differential equation using Frobenius method. It is possible to obtain the coefficient for an arbitrary power of the variable, but also time consuming. In short, I do not have the $n^\text{th}$ term in terms of a known formula. My queries are the following: (a) Is there a general method to find the coefficient of the $n^{th}$ term of an infinite series from the first few (let us say 10) terms, given that the series is convergent? (b) The series I obtained is: $$\frac{x^2}{6} -\frac{x^4}{9} + \frac{3x^6}{80} - \frac{71 x^8}{15120} + \frac{  10361 x^{10}}{10886400} + \frac{3539 x^{12}}{29937600} + \frac{  17111641 x^{14}}{261534873600}$$ Has anyone encountered this series? Or is there a way to find the formula for its $n^{th}$ term? Edit 1: The differential equation in question is $-9f^2 +2f'^2 -3 f f'' + a_0 =0$. The initial conditions are $f(0)=a_0, f'(0) =0$. The Frobenius solution separates out as $f(x) = a_0 \cos^3(x) + g_0(x) + \frac{1}{a_0} g_1(x) + \frac{1}{a_0^2} g_2(x) + \dots$.  I could identify the first series as $a_0 \cos^3(x)$. The series in question corresponds to $g_0(x)$. Solution : The closed form of the function given by the series was determined by using perturbation theory methods. The solution is given by: $g_0(x) = \frac{1}{72} \sin (x) (5 \cos (2 x)+7) \tan (x)$","I obtained an infinite series after solving a non-linear differential equation using Frobenius method. It is possible to obtain the coefficient for an arbitrary power of the variable, but also time consuming. In short, I do not have the $n^\text{th}$ term in terms of a known formula. My queries are the following: (a) Is there a general method to find the coefficient of the $n^{th}$ term of an infinite series from the first few (let us say 10) terms, given that the series is convergent? (b) The series I obtained is: $$\frac{x^2}{6} -\frac{x^4}{9} + \frac{3x^6}{80} - \frac{71 x^8}{15120} + \frac{  10361 x^{10}}{10886400} + \frac{3539 x^{12}}{29937600} + \frac{  17111641 x^{14}}{261534873600}$$ Has anyone encountered this series? Or is there a way to find the formula for its $n^{th}$ term? Edit 1: The differential equation in question is $-9f^2 +2f'^2 -3 f f'' + a_0 =0$. The initial conditions are $f(0)=a_0, f'(0) =0$. The Frobenius solution separates out as $f(x) = a_0 \cos^3(x) + g_0(x) + \frac{1}{a_0} g_1(x) + \frac{1}{a_0^2} g_2(x) + \dots$.  I could identify the first series as $a_0 \cos^3(x)$. The series in question corresponds to $g_0(x)$. Solution : The closed form of the function given by the series was determined by using perturbation theory methods. The solution is given by: $g_0(x) = \frac{1}{72} \sin (x) (5 \cos (2 x)+7) \tan (x)$",,"['calculus', 'sequences-and-series', 'ordinary-differential-equations', 'frobenius-method']"
31,Find new first integral of DE System,Find new first integral of DE System,,"I am trying to find two first integrals of the following system: $$\begin{cases}x' = x+z\\ y'= y \\ z'=z + y^2 \end{cases}$$ A first integral can be found considering only the system in $\mathbb{R}^2$ for $y$ and $z$. Using the integrating factors $\mu_1 = \frac{1}{y^2}$ and $\mu_2 = \frac{1}{(z-y^2)^2}$ we can obtain the function $$H_1(y,z) = \sqrt{\frac{\mu_1}{\mu_2}}=\frac{z}{y} -y$$ which is a first integral of the $y,z$ system, hence a first integral of the system in $\mathbb{R}^3$. Now I'm trying to get another first integral, functionally independent to $H_1$, but can't really find a way to do it unless I use PDE's, which I want to avoid. Any help on how to get it will be appreciated! Thanks. EDIT: by first integral I refer to a function $H(x,y,z)$ such that $$H' = \left<\nabla H, (x',y',z') \right> = 0.$$","I am trying to find two first integrals of the following system: $$\begin{cases}x' = x+z\\ y'= y \\ z'=z + y^2 \end{cases}$$ A first integral can be found considering only the system in $\mathbb{R}^2$ for $y$ and $z$. Using the integrating factors $\mu_1 = \frac{1}{y^2}$ and $\mu_2 = \frac{1}{(z-y^2)^2}$ we can obtain the function $$H_1(y,z) = \sqrt{\frac{\mu_1}{\mu_2}}=\frac{z}{y} -y$$ which is a first integral of the $y,z$ system, hence a first integral of the system in $\mathbb{R}^3$. Now I'm trying to get another first integral, functionally independent to $H_1$, but can't really find a way to do it unless I use PDE's, which I want to avoid. Any help on how to get it will be appreciated! Thanks. EDIT: by first integral I refer to a function $H(x,y,z)$ such that $$H' = \left<\nabla H, (x',y',z') \right> = 0.$$",,"['ordinary-differential-equations', 'dynamical-systems']"
32,Solving Recurrence Relation for Series Solution of an ODE,Solving Recurrence Relation for Series Solution of an ODE,,"I am trying to solve the below problem: Assume $y = \sum_{n=0}^{\infty}a_nx^n$ is a solution to $(x-1)y''-(x-3)y'-y=0$. Find $a_n$. I took both derivatives of $y$, plug them into the equation, modify the indices until each series has the same $x^n$, and take terms out of the series such that they have the same index to bring all the terms under the same summation. I arrive at $\sum_{n=0}^{\infty}[(n+1)(n)a_n-(n+1)(n+2)a_{n+2} - na_n+3(n+1)a_{n+1}-a_n]x^n + (-2a_2+3a_1-a_0)=0$ Therefore $$a_2 = \frac{3a_1}{2}-\frac{a_0}{2}$$ and $$a_{n+2} = \frac{(n+3)a_{n+1}-a_n}{(n+2)}, n = 1,2,3...$$ I also find some terms as $$a_3 = \frac{4a_2}{3}-a_1 = \left(\frac{4\cdot 3}{3\cdot 2} - 1 \right) a_1-\frac{4}{3\cdot 2}a_0$$ $$a_4 = \left(\frac{5\cdot 4\cdot 3}{4\cdot 3\cdot 2} - \frac{13}{8} \right) a_1-\left(\frac{5\cdot 4}{4\cdot 3\cdot 2} - \frac{1}{8} \right)a_0$$ $$a_5 = \left(\frac{6\cdot 5\cdot 4\cdot 3}{5\cdot 4\cdot 3\cdot 2} - \frac{53}{40} \right) a_1-\left(\frac{6\cdot 5\cdot 4}{5\cdot 4\cdot 3\cdot 2} - \frac{17}{60} \right)a_0$$ I can see a pattern for the first set of coefficients, but not the next. For example the first part for $a_1$ is $a_n = \frac{n+1}{2}$ Any help would be greatly appreciated. Thanks!","I am trying to solve the below problem: Assume $y = \sum_{n=0}^{\infty}a_nx^n$ is a solution to $(x-1)y''-(x-3)y'-y=0$. Find $a_n$. I took both derivatives of $y$, plug them into the equation, modify the indices until each series has the same $x^n$, and take terms out of the series such that they have the same index to bring all the terms under the same summation. I arrive at $\sum_{n=0}^{\infty}[(n+1)(n)a_n-(n+1)(n+2)a_{n+2} - na_n+3(n+1)a_{n+1}-a_n]x^n + (-2a_2+3a_1-a_0)=0$ Therefore $$a_2 = \frac{3a_1}{2}-\frac{a_0}{2}$$ and $$a_{n+2} = \frac{(n+3)a_{n+1}-a_n}{(n+2)}, n = 1,2,3...$$ I also find some terms as $$a_3 = \frac{4a_2}{3}-a_1 = \left(\frac{4\cdot 3}{3\cdot 2} - 1 \right) a_1-\frac{4}{3\cdot 2}a_0$$ $$a_4 = \left(\frac{5\cdot 4\cdot 3}{4\cdot 3\cdot 2} - \frac{13}{8} \right) a_1-\left(\frac{5\cdot 4}{4\cdot 3\cdot 2} - \frac{1}{8} \right)a_0$$ $$a_5 = \left(\frac{6\cdot 5\cdot 4\cdot 3}{5\cdot 4\cdot 3\cdot 2} - \frac{53}{40} \right) a_1-\left(\frac{6\cdot 5\cdot 4}{5\cdot 4\cdot 3\cdot 2} - \frac{17}{60} \right)a_0$$ I can see a pattern for the first set of coefficients, but not the next. For example the first part for $a_1$ is $a_n = \frac{n+1}{2}$ Any help would be greatly appreciated. Thanks!",,"['sequences-and-series', 'ordinary-differential-equations', 'recurrence-relations', 'power-series']"
33,What does the delta notation in this formula mean?,What does the delta notation in this formula mean?,,"The following is a screenshot of the formula booklet I'll be able to use in an exam this week. I'm used to seeing the formulae for numerical differentiation in a different format though and I'm not sure how to interpret the ones in the formula booklet: I'm used to seeing the formulae in the following format: I want to know how the formulae in the book relate to the ones I'm used to using. I need to understand how to use the formula book versions of the formulae as these are the ones I will have access to in the exam. So, what do the $\Delta$, $\delta$ and $\mu$ symbols mean?","The following is a screenshot of the formula booklet I'll be able to use in an exam this week. I'm used to seeing the formulae for numerical differentiation in a different format though and I'm not sure how to interpret the ones in the formula booklet: I'm used to seeing the formulae in the following format: I want to know how the formulae in the book relate to the ones I'm used to using. I need to understand how to use the formula book versions of the formulae as these are the ones I will have access to in the exam. So, what do the $\Delta$, $\delta$ and $\mu$ symbols mean?",,"['calculus', 'ordinary-differential-equations', 'numerical-methods']"
34,Solving second order ODE:Numerical and Analytical-Help,Solving second order ODE:Numerical and Analytical-Help,,"$$\frac{d^2x}{dt^2} - a^2x + \frac{k}{x^2} = 0$$ Can anyone help me with solving this ODE ? What type of ODE is this? Is there a general solution for this? If not, please give me some help on solving this numerically. Thank you","$$\frac{d^2x}{dt^2} - a^2x + \frac{k}{x^2} = 0$$ Can anyone help me with solving this ODE ? What type of ODE is this? Is there a general solution for this? If not, please give me some help on solving this numerically. Thank you",,"['ordinary-differential-equations', 'numerical-methods']"
35,How to solve this differential equation in Mathematica?,How to solve this differential equation in Mathematica?,,"I am trying to solve a differential equation in Mathematica:     $$y'' + 2\frac{y'}{x} + (1 - \frac{e^{-x}}{x} - \frac{l(l+1)}{x^2})y = 0$$ I have initial conditions at $x=0$ as:     $$y(0) = a$$     $$y'(0) = b$$ $a$ and $b$ are some known constants. But, how do I implement it, because $x=0$ will blow up in the differential equation? I tried assigning values near zero, like assigning at $x=0.00001$, but then there appears a discontinuity near $x=0$ in the function. After finding the function, I have to normalize it and have to find value at $x=0$.","I am trying to solve a differential equation in Mathematica:     $$y'' + 2\frac{y'}{x} + (1 - \frac{e^{-x}}{x} - \frac{l(l+1)}{x^2})y = 0$$ I have initial conditions at $x=0$ as:     $$y(0) = a$$     $$y'(0) = b$$ $a$ and $b$ are some known constants. But, how do I implement it, because $x=0$ will blow up in the differential equation? I tried assigning values near zero, like assigning at $x=0.00001$, but then there appears a discontinuity near $x=0$ in the function. After finding the function, I have to normalize it and have to find value at $x=0$.",,"['ordinary-differential-equations', 'mathematica']"
36,Laplace PDE with impulse function,Laplace PDE with impulse function,,"I want to solve the PDE: $$u_{xx}+u_{yy}=-\delta(x-x_0)\delta(y-y_0)$$  in the rectangle: $\quad R=\{(x,y):0\leq x\leq a\quad0\leq y\leq b\},\quad (x_0,y_0)\in R$ with the boundary conditions: $$u_x(0,y)=u_x(a,y)=0$$ $$u(x,0)=u(x,b)=0$$ Which method should I use when there is an impulse function (in this case Dirac's delta)?","I want to solve the PDE: $$u_{xx}+u_{yy}=-\delta(x-x_0)\delta(y-y_0)$$  in the rectangle: $\quad R=\{(x,y):0\leq x\leq a\quad0\leq y\leq b\},\quad (x_0,y_0)\in R$ with the boundary conditions: $$u_x(0,y)=u_x(a,y)=0$$ $$u(x,0)=u(x,b)=0$$ Which method should I use when there is an impulse function (in this case Dirac's delta)?",,"['ordinary-differential-equations', 'partial-differential-equations', 'harmonic-functions', 'dirac-delta']"
37,Finding Eigen Values of ODE,Finding Eigen Values of ODE,,"I am asked to find non-trivial eigen values of the BVP $$y''(x)+\lambda y(x) =0, \quad y \left(\frac{\pi}{2} \right) = 0, \quad y(0)= -3y'(0).$$ However, after few routine calculations, I obtained $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$. It is also given that the eigen values may be given by $\lambda_n = 4n^2 \pi ^2$. I am unable to show the given solutions satisfy the equation  $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$.","I am asked to find non-trivial eigen values of the BVP $$y''(x)+\lambda y(x) =0, \quad y \left(\frac{\pi}{2} \right) = 0, \quad y(0)= -3y'(0).$$ However, after few routine calculations, I obtained $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$. It is also given that the eigen values may be given by $\lambda_n = 4n^2 \pi ^2$. I am unable to show the given solutions satisfy the equation  $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$.",,['ordinary-differential-equations']
38,I.V.P. : $ x^2 z_x + y^2 z_y = z^2 \; ; \; z=1 $ on the initial curve $C : y=2x $,I.V.P. :  on the initial curve, x^2 z_x + y^2 z_y = z^2 \; ; \; z=1  C : y=2x ,"Exercise : Solve the Initial Value Problem (I.V.P.)   $$\begin{cases}  x^2 z_x + y^2 z_y = z^2 \\[.5em] z=1 \text{ on } C =\{(x,y): y=2x \} \end{cases}$$   where $C$ is the initial curve of the problem. Attempt : First of all, we yield the differential problem : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{z^2}$$ We solve : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} \implies u_1 = \frac{1}{x} - \frac{1}{y} = \frac{y-x}{xy} $$ $$\frac{\mathrm{d}x}{x^2} - \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{z^2} \implies u_2 = \frac{y-x}{xy} -\frac{1}{z} $$ The parameters of the initial curve as a position vector, are :  $$C : r_C(t) = (x,2x,1) \; x \in \mathbb R$$ And then we yield the system : $$\begin{cases} Z_1 = \frac{1}{x} \\ Z_2 = \frac{1}{x} - 1 \end{cases} \Rightarrow Z_2 = Z_1 - 1 \implies z_2(x,y,z) = z_1(x,y,z) - 1$$ which finally implies that $z=1$ is a solution but also $z_0 = z(x,y) = z(x,2x) = 1$. Question : Is my approach correct ? Have I left something out of my solution ?","Exercise : Solve the Initial Value Problem (I.V.P.)   $$\begin{cases}  x^2 z_x + y^2 z_y = z^2 \\[.5em] z=1 \text{ on } C =\{(x,y): y=2x \} \end{cases}$$   where $C$ is the initial curve of the problem. Attempt : First of all, we yield the differential problem : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{z^2}$$ We solve : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} \implies u_1 = \frac{1}{x} - \frac{1}{y} = \frac{y-x}{xy} $$ $$\frac{\mathrm{d}x}{x^2} - \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{z^2} \implies u_2 = \frac{y-x}{xy} -\frac{1}{z} $$ The parameters of the initial curve as a position vector, are :  $$C : r_C(t) = (x,2x,1) \; x \in \mathbb R$$ And then we yield the system : $$\begin{cases} Z_1 = \frac{1}{x} \\ Z_2 = \frac{1}{x} - 1 \end{cases} \Rightarrow Z_2 = Z_1 - 1 \implies z_2(x,y,z) = z_1(x,y,z) - 1$$ which finally implies that $z=1$ is a solution but also $z_0 = z(x,y) = z(x,2x) = 1$. Question : Is my approach correct ? Have I left something out of my solution ?",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'initial-value-problems', 'characteristics']"
39,Exact solution of $y''=(ay+b)y'+cy^2+dy$,Exact solution of,y''=(ay+b)y'+cy^2+dy,"Consider the second order nonlinear ode of the following form  $y''=(ay+b)y'+cy^2+dy$, where $a, b, c, d$ are real constants. Can we find the exact solution of this equation?","Consider the second order nonlinear ode of the following form  $y''=(ay+b)y'+cy^2+dy$, where $a, b, c, d$ are real constants. Can we find the exact solution of this equation?",,"['real-analysis', 'ordinary-differential-equations']"
40,Solving : $x^2 z_x + y^2 z_y = 2xy$,Solving :,x^2 z_x + y^2 z_y = 2xy,"Exercise : Find the general integral and compute three different solutions for the PDE :   $$x^2 z_x + y^2 z_y = 2xy$$ Attempt : The general integral is given by a function $F \in C^1$ : $F(u_1,u_2) = 0$, where $u_1$ and $u_2$ are the integral curves, calculated by the differential problem :  $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{2xy}$$ But then I am at loss on how to calculate the general solutions asked. Also, for $u_1$ and $u_2$ : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} \implies u_1 = \frac{y-x}{xy}$$ but I am also unable to grasp a calculation for $u_2$. Any help and explanation about the general solutions and $u_2$ will be greatly appreciated as this is a new subject I am getting in.","Exercise : Find the general integral and compute three different solutions for the PDE :   $$x^2 z_x + y^2 z_y = 2xy$$ Attempt : The general integral is given by a function $F \in C^1$ : $F(u_1,u_2) = 0$, where $u_1$ and $u_2$ are the integral curves, calculated by the differential problem :  $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} = \frac{\mathrm{d}z}{2xy}$$ But then I am at loss on how to calculate the general solutions asked. Also, for $u_1$ and $u_2$ : $$\frac{\mathrm{d}x}{x^2} = \frac{\mathrm{d}y}{y^2} \implies u_1 = \frac{y-x}{xy}$$ but I am also unable to grasp a calculation for $u_2$. Any help and explanation about the general solutions and $u_2$ will be greatly appreciated as this is a new subject I am getting in.",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
41,Solve differential equation $(1+x^2) \frac{dy}{dx} - 2xy = x$,Solve differential equation,(1+x^2) \frac{dy}{dx} - 2xy = x,"Solve differential equation $$(1+x^2) \frac{dy}{dx} - 2xy = x$$ I simplified it to $$\frac{1}{1+2y} dy = \frac{x}{1+x^2} dx$$ $$ \int \frac{1}{1+2y} dy =\int \frac{x}{1+x^2} dx $$ $$\frac{1}{2} \int \frac{2}{1+2y} dy = \frac{1}{2} \int \frac{2x}{1+x^2} dx$$ $$\frac{1}{2} \ln | 1 + 2y | = \frac{1}{2} \ln | 1+x^2 | + C $$ From here, I got stuck. I have to remove y from here to solve it. The answer in the textbook gave $ y= (k(1+x^2) - 1)/{2}$ I believe $k$ is the integration constant. How do I remove the $\ln$ from both sides?","Solve differential equation $$(1+x^2) \frac{dy}{dx} - 2xy = x$$ I simplified it to $$\frac{1}{1+2y} dy = \frac{x}{1+x^2} dx$$ $$ \int \frac{1}{1+2y} dy =\int \frac{x}{1+x^2} dx $$ $$\frac{1}{2} \int \frac{2}{1+2y} dy = \frac{1}{2} \int \frac{2x}{1+x^2} dx$$ $$\frac{1}{2} \ln | 1 + 2y | = \frac{1}{2} \ln | 1+x^2 | + C $$ From here, I got stuck. I have to remove y from here to solve it. The answer in the textbook gave $ y= (k(1+x^2) - 1)/{2}$ I believe $k$ is the integration constant. How do I remove the $\ln$ from both sides?",,"['calculus', 'ordinary-differential-equations', 'logarithms']"
42,Solving the Differential Equation $u'=u^{1-k}$ by separation of variables.,Solving the Differential Equation  by separation of variables.,u'=u^{1-k},"I have the differential equation $u'=u^{1-k}$, and am supposed to solve it by separation of variables. As an attempted solution, I have $u^{k-1}du=dt$, and therefore $\frac{u^k}{k}=t+c$, and therefore $u=(kt)^\frac{1}{k}+c$. With $u(0)=1$, this is $u=(kt)^\frac{1}{k}+1$. Is this the correct solution, and also, for what values of $k<0$ does $u$ ""blow up"".","I have the differential equation $u'=u^{1-k}$, and am supposed to solve it by separation of variables. As an attempted solution, I have $u^{k-1}du=dt$, and therefore $\frac{u^k}{k}=t+c$, and therefore $u=(kt)^\frac{1}{k}+c$. With $u(0)=1$, this is $u=(kt)^\frac{1}{k}+1$. Is this the correct solution, and also, for what values of $k<0$ does $u$ ""blow up"".",,['ordinary-differential-equations']
43,Infinitely Many Zeroes in the Solutions of a Differential Equation,Infinitely Many Zeroes in the Solutions of a Differential Equation,,"I am working with the below differential equation, where $k$ is a positive integer. $\frac{d^2 y}{dx^2}+y(1+x^k)=0$ I need to show that the nontrivial solutions of this equation have infinitely many zeroes on the interval $(0,\infty)$ and also that the separation between adjacent zeroes goes to zero as $x$ tends to $\infty$. I have read about Sturm-Liouville systems and finding the eigenvalues/eigenfunctions of such a system, but not sure how I am supposed to apply the theory in this case.  Also, I have to show that solutions of the equation have infinitely many zeroes on an interval rather than the equation itself having zeroes, which is what I have seen in my notes.  Is there a way to apply the Comparison Theorem to get the relevant result?","I am working with the below differential equation, where $k$ is a positive integer. $\frac{d^2 y}{dx^2}+y(1+x^k)=0$ I need to show that the nontrivial solutions of this equation have infinitely many zeroes on the interval $(0,\infty)$ and also that the separation between adjacent zeroes goes to zero as $x$ tends to $\infty$. I have read about Sturm-Liouville systems and finding the eigenvalues/eigenfunctions of such a system, but not sure how I am supposed to apply the theory in this case.  Also, I have to show that solutions of the equation have infinitely many zeroes on an interval rather than the equation itself having zeroes, which is what I have seen in my notes.  Is there a way to apply the Comparison Theorem to get the relevant result?",,"['ordinary-differential-equations', 'sturm-liouville']"
44,Compute $ \ \frac{dM}{dt} (t) \ $ in simplest form,Compute  in simplest form, \ \frac{dM}{dt} (t) \ ,"Diffusion Equation: Let $ \ u(x,t) $ denote the concentration of a chemical  which satisfies $$ u_t=2u_{xx}+\frac{2}{5} x , \ 0<x<4 , \ t>0 $$ $$ u_x(0,t)=0, \ u_x(4,t)=1, \ u(x,0)=2x $$  Let $  \ M(t)=\int_0^4 u(x,t) dx \  \ $ denote the total amount of chemical at time $ \ t \ $. Then, (a) What is the physical meaning of the boundary condition $ \ \frac{\partial u}{\partial x}(4,t)=1 \ ? $ (b) Compute $ \ \frac{dM}{dt} (t) \ $ in simplest form. (c) Find $ \ M(t) \ $ Answer: (a) I think $ \ \frac{\partial u}{\partial x}(4,t) =1 \ $  means that the flux across the boundary is constant $=1 $ That is there is no concentration difference . (b) $ M(t)=\int_0^4 u(x,t) dx \\ \Rightarrow \frac{dM}{dt}=\int_0^4 \frac{\partial u}{\partial t} (x,t) dx =\int_0^4 (2u_{xx}+\frac{2}{5} x) dx=\left[2u_x \right]_0^4+\frac{2}{5} \left[\frac{x^2}{2} \right]_0^4 =2u_x(4,t)-2u_x(0,t)+\frac{2}{5} \times 8=2+16/5=26/5  \\ \Rightarrow \frac{dM}{dt}=\frac{26}{5} $ Am I right so far? If I am Right , then how to evaluate $ \ M(t) \ $ ? Help me doing this.","Diffusion Equation: Let $ \ u(x,t) $ denote the concentration of a chemical  which satisfies $$ u_t=2u_{xx}+\frac{2}{5} x , \ 0<x<4 , \ t>0 $$ $$ u_x(0,t)=0, \ u_x(4,t)=1, \ u(x,0)=2x $$  Let $  \ M(t)=\int_0^4 u(x,t) dx \  \ $ denote the total amount of chemical at time $ \ t \ $. Then, (a) What is the physical meaning of the boundary condition $ \ \frac{\partial u}{\partial x}(4,t)=1 \ ? $ (b) Compute $ \ \frac{dM}{dt} (t) \ $ in simplest form. (c) Find $ \ M(t) \ $ Answer: (a) I think $ \ \frac{\partial u}{\partial x}(4,t) =1 \ $  means that the flux across the boundary is constant $=1 $ That is there is no concentration difference . (b) $ M(t)=\int_0^4 u(x,t) dx \\ \Rightarrow \frac{dM}{dt}=\int_0^4 \frac{\partial u}{\partial t} (x,t) dx =\int_0^4 (2u_{xx}+\frac{2}{5} x) dx=\left[2u_x \right]_0^4+\frac{2}{5} \left[\frac{x^2}{2} \right]_0^4 =2u_x(4,t)-2u_x(0,t)+\frac{2}{5} \times 8=2+16/5=26/5  \\ \Rightarrow \frac{dM}{dt}=\frac{26}{5} $ Am I right so far? If I am Right , then how to evaluate $ \ M(t) \ $ ? Help me doing this.",,['ordinary-differential-equations']
45,Periodicity of the solution of a non-linear differential equation,Periodicity of the solution of a non-linear differential equation,,"Consider the following non-linear differential equation, $$ \dot{x}(t)=a-b\sin(x(t)), \ \ x(0)=x_0\in\mathbb{R}, $$ and assume that $a$ and $b$ are positive real numbers with $a>b$. Note that the solution $x(t)$ exists and can be analytically computed ( see here ). My question: Is $\cos(x(t))$ a zero-mean periodic function of $t\ge 0$? Further on, in case the answer to my previous question is in the affirmative, it should be that $$ \left|\int_{0}^t \cos(x(t))\, \mathrm{d}t\right|\le K(a,b), \ \ \forall t\ge 0, $$ where $K(a,b)$ is a positive constant depending on $a$ and $b$ only. So an additional question is: Can we find an explicit expression for $K(a,b)$? N.B. Numerical simulations seem to confirm the above claims. However I couldn't quite prove them. So any help is really appreciated!","Consider the following non-linear differential equation, $$ \dot{x}(t)=a-b\sin(x(t)), \ \ x(0)=x_0\in\mathbb{R}, $$ and assume that $a$ and $b$ are positive real numbers with $a>b$. Note that the solution $x(t)$ exists and can be analytically computed ( see here ). My question: Is $\cos(x(t))$ a zero-mean periodic function of $t\ge 0$? Further on, in case the answer to my previous question is in the affirmative, it should be that $$ \left|\int_{0}^t \cos(x(t))\, \mathrm{d}t\right|\le K(a,b), \ \ \forall t\ge 0, $$ where $K(a,b)$ is a positive constant depending on $a$ and $b$ only. So an additional question is: Can we find an explicit expression for $K(a,b)$? N.B. Numerical simulations seem to confirm the above claims. However I couldn't quite prove them. So any help is really appreciated!",,"['ordinary-differential-equations', 'analysis', 'periodic-functions']"
46,A question on Lyapunov exponents associated with a fixed point of a vector field.,A question on Lyapunov exponents associated with a fixed point of a vector field.,,"The Lyapunov exponent $\chi(x_{0}, e)$ in a direction $e \in \mathbb{R}^{n}$ along the trajectory $x(t,x_{0})$ of a vector field $\dot{x} = f(x)$ through a point $x_{0}$ is defined to be $$\chi(x_{0},e) = \lim_{t \to \infty}\frac{1}{t} \log \frac{||X(t;x(t,x_{0}))e||}{||e||}$$ where $X(t;x(t,x_{0}))$ is the fundamental solution matrix of the linearisation of the vector field orbit about $x(t,x_{0})$. Here $x \in \mathbb{R}^{n}$. Let us consider the case of the the orbit being a fixed point. It is known that then $\chi$ is just the real parts of the eigenvalues associated with the matrix of the vector field linearised about the fixed point. How does one prove this? Why do the imaginary parts of e-values do not contribute to $\chi$?","The Lyapunov exponent $\chi(x_{0}, e)$ in a direction $e \in \mathbb{R}^{n}$ along the trajectory $x(t,x_{0})$ of a vector field $\dot{x} = f(x)$ through a point $x_{0}$ is defined to be $$\chi(x_{0},e) = \lim_{t \to \infty}\frac{1}{t} \log \frac{||X(t;x(t,x_{0}))e||}{||e||}$$ where $X(t;x(t,x_{0}))$ is the fundamental solution matrix of the linearisation of the vector field orbit about $x(t,x_{0})$. Here $x \in \mathbb{R}^{n}$. Let us consider the case of the the orbit being a fixed point. It is known that then $\chi$ is just the real parts of the eigenvalues associated with the matrix of the vector field linearised about the fixed point. How does one prove this? Why do the imaginary parts of e-values do not contribute to $\chi$?",,"['ordinary-differential-equations', 'dynamical-systems', 'ergodic-theory', 'chaos-theory', 'stability-theory']"
47,Solving given initial value problem using the method of Laplace Transforms $y''+y=f(t)$ where $y(0) = 0$ and $y'(0) = 1$,Solving given initial value problem using the method of Laplace Transforms  where  and,y''+y=f(t) y(0) = 0 y'(0) = 1,"I have the problem asking me to solve the initial value problem using the method of Laplace Transforms given $$ y''+y=f(t); \ \ \ \ \ y(0) = 0, y'(0)=1 $$ where $$ f(t)=\begin{cases} 0,&  0<t<1 \\  1,& 1< t < 2 \\  0,&  2 < t  \end{cases} $$ I took the Laplace of both sides $$ L(y'') + L(y)=L(f)$$ I didn't bother to include the 2 other parts of the right side of the equation since they are both multiplied by 0. The result came out to be: $$ (s^2Y(s)-(s)(0)-(1)) + Y(s) = \frac{e^{-s}-e^{-2s}}{s} $$ then $$ Y(s)(s^2+1)= \frac{e^{-s}-e^{-2s}}{s}+1$$ then $$ Y(s)= \frac{e^{-s}-e^{-2s}}{s(s^2+1)}+\frac{1}{s^2+1} $$ From here, I believe I need to take the inverse Laplace to get my final answer. While I know that the 2nd term will come out to be sin(t) once I take the inverse Laplace, I'm unsure of how to go about the first one. Any help would be greatly appreciated!","I have the problem asking me to solve the initial value problem using the method of Laplace Transforms given $$ y''+y=f(t); \ \ \ \ \ y(0) = 0, y'(0)=1 $$ where $$ f(t)=\begin{cases} 0,&  0<t<1 \\  1,& 1< t < 2 \\  0,&  2 < t  \end{cases} $$ I took the Laplace of both sides $$ L(y'') + L(y)=L(f)$$ I didn't bother to include the 2 other parts of the right side of the equation since they are both multiplied by 0. The result came out to be: $$ (s^2Y(s)-(s)(0)-(1)) + Y(s) = \frac{e^{-s}-e^{-2s}}{s} $$ then $$ Y(s)(s^2+1)= \frac{e^{-s}-e^{-2s}}{s}+1$$ then $$ Y(s)= \frac{e^{-s}-e^{-2s}}{s(s^2+1)}+\frac{1}{s^2+1} $$ From here, I believe I need to take the inverse Laplace to get my final answer. While I know that the 2nd term will come out to be sin(t) once I take the inverse Laplace, I'm unsure of how to go about the first one. Any help would be greatly appreciated!",,['ordinary-differential-equations']
48,Non-linear ODE $y y'' = A/x^2$ approximate solution,Non-linear ODE  approximate solution,y y'' = A/x^2,"I'm currently stuck on approximating a solution to a non-linear ODE. I've searched high and low on this site for general methods for approximating non-linear ODE's of the type $$y(x)y''(x) = \frac{A}{x^2}, x > 0, A \in \mathbb{R}$$ with no luck. If you have a link in mind to a previously asked question regarding this, I'd be glad to see it. It could be that there is something entirely trivial that I am missing in approximating a solution to this equation! My experience with non-linear ODEs is quite limited. So this question is about finding an approximate solution to the above-mentioned non-linear ODE with BCs $y(0)=\alpha, y'(\infty)=0$, $\alpha \in [0,\pi/2]$. Any ideas? I'd love to hear your thoughts. The above equation follows by letting $u(x)=xy(x)$ in the equation $$u'''=A/u^2$$ with BCs $u(0)=\varepsilon, u'(0)=\alpha, u''(\infty)=0$ with $\varepsilon \ll 1$ (a small number) and using that $y'(x)$ is small so that $u'=y+xy'=y$ and $u''=y',u'''=y''$.","I'm currently stuck on approximating a solution to a non-linear ODE. I've searched high and low on this site for general methods for approximating non-linear ODE's of the type $$y(x)y''(x) = \frac{A}{x^2}, x > 0, A \in \mathbb{R}$$ with no luck. If you have a link in mind to a previously asked question regarding this, I'd be glad to see it. It could be that there is something entirely trivial that I am missing in approximating a solution to this equation! My experience with non-linear ODEs is quite limited. So this question is about finding an approximate solution to the above-mentioned non-linear ODE with BCs $y(0)=\alpha, y'(\infty)=0$, $\alpha \in [0,\pi/2]$. Any ideas? I'd love to hear your thoughts. The above equation follows by letting $u(x)=xy(x)$ in the equation $$u'''=A/u^2$$ with BCs $u(0)=\varepsilon, u'(0)=\alpha, u''(\infty)=0$ with $\varepsilon \ll 1$ (a small number) and using that $y'(x)$ is small so that $u'=y+xy'=y$ and $u''=y',u'''=y''$.",,['ordinary-differential-equations']
49,Prove that $\mathscr{L}^{-1}\{\frac{1}{(s+\alpha)(1-e^{-Ts})}\}=e^{-\alpha t} \left[ \frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1}\right]$,Prove that,\mathscr{L}^{-1}\{\frac{1}{(s+\alpha)(1-e^{-Ts})}\}=e^{-\alpha t} \left[ \frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1}\right],"Prove that the inverse laplace transform of : $$G(s) = \frac{1}{(s+\alpha)(1-e^{-Ts})}$$ equals $$g(t) = e^{-\alpha t} \left[ \frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1}\right]$$ for :  $$mT<t<(m+1)T \:\:\:\: , \:\:\:\: m=0,1,2,...,$$ I can't understand what is meant by the the two conditions above. What I could do with this problem is : Write $\frac{1}{1-e^{-Ts}}$ as a power series : $$\frac{1}{1-e^{-Ts}} = \sum_{n=0}^{\infty}(e^{-Ts})^{n} \:\:\:\: , \:\:\:\: |e^{-Ts}|<1$$ Then :  $$G(s) = \frac{1}{s+\alpha}\sum_{n=0}^{\infty}e^{-nTs} = \sum_{n=0}^{\infty}\frac{1}{s+\alpha}e^{-nTs}$$ Taking the inverse laplace transform : $$g(t) = \sum_{n=0}^{\infty}e^{-\alpha(t-nT)}u(t-nT)$$ And I could show that :  $$\frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1} = \left[1-e^{(m+1) \alpha T}\right]\frac{1}{1-e^{ \alpha T}} \:\:\:\: , \:\:\: |e^{- \alpha T}|<1$$ $$= \left[1-e^{(m+1) \alpha T}\right]\sum_{n=0}^{\infty}(e^{\alpha T})^{n}$$ $$ = \sum_{n=0}^{\infty}e^{n \alpha T}-\sum_{n=0}^{\infty}e^{(n+m+1) \alpha T}$$ $$= \sum_{n=0}^{\infty}e^{n \alpha T}-\sum_{n=m+1}^{\infty}e^{n \alpha T} = \sum_{n=0}^{m}e^{n \alpha T}$$ And stopped here .","Prove that the inverse laplace transform of : $$G(s) = \frac{1}{(s+\alpha)(1-e^{-Ts})}$$ equals $$g(t) = e^{-\alpha t} \left[ \frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1}\right]$$ for :  $$mT<t<(m+1)T \:\:\:\: , \:\:\:\: m=0,1,2,...,$$ I can't understand what is meant by the the two conditions above. What I could do with this problem is : Write $\frac{1}{1-e^{-Ts}}$ as a power series : $$\frac{1}{1-e^{-Ts}} = \sum_{n=0}^{\infty}(e^{-Ts})^{n} \:\:\:\: , \:\:\:\: |e^{-Ts}|<1$$ Then :  $$G(s) = \frac{1}{s+\alpha}\sum_{n=0}^{\infty}e^{-nTs} = \sum_{n=0}^{\infty}\frac{1}{s+\alpha}e^{-nTs}$$ Taking the inverse laplace transform : $$g(t) = \sum_{n=0}^{\infty}e^{-\alpha(t-nT)}u(t-nT)$$ And I could show that :  $$\frac{e^{(m+1) \alpha T}-1}{e^{ \alpha T}-1} = \left[1-e^{(m+1) \alpha T}\right]\frac{1}{1-e^{ \alpha T}} \:\:\:\: , \:\:\: |e^{- \alpha T}|<1$$ $$= \left[1-e^{(m+1) \alpha T}\right]\sum_{n=0}^{\infty}(e^{\alpha T})^{n}$$ $$ = \sum_{n=0}^{\infty}e^{n \alpha T}-\sum_{n=0}^{\infty}e^{(n+m+1) \alpha T}$$ $$= \sum_{n=0}^{\infty}e^{n \alpha T}-\sum_{n=m+1}^{\infty}e^{n \alpha T} = \sum_{n=0}^{m}e^{n \alpha T}$$ And stopped here .",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'laplace-transform']"
50,Is it possible to define the flow of a continous function?,Is it possible to define the flow of a continous function?,,"Let $f$ $\in$ $\mathcal{C}^{1}(\mathbb{R}^n,\mathbb{R}^n)$  and consider the ODE: \begin{align*} y'&=f(y)\\ y(0)&=y_0 \end{align*} Once $f$ is a $\mathcal{C}^1$ function we can define the flow $\varphi$ of $f$. The function $\varphi$ satisfies the following conditions: $\varphi:D\rightarrow \mathbb{R}^n $ $\varphi(p,0)=p \hspace{0.1cm}$; $\forall$ $p$ $\in$ $\mathbb{R}^n$ $ \frac{d}{dt}\varphi(p,t) = f(\varphi(p,t)) $, $\forall$ $(p,t)$ $\in$ $D$ where $D$ the ""definition domain of $f$"", i.e , $D =\{(x,s)\in \mathbb{R}^{n+1};$ $s$ $\in$ $(\omega_x^-,\omega_x^+)$}, being that $(\omega_x^-,\omega_x^+)$ the maximal domain of solution of the ODE \begin{align*} y'&=f(y)\\ y(0)&=x \end{align*} It's possible to prove that $D$ is an open set and $\varphi$ is a $\mathcal{C}^1$ function. Now, I would like to know if it is possible to define the flow of $f$ supposing $f$ just a continuous function, I know that maybe this is impossible because in this case, we do not have a solution uniqueness of the differential equation. My Question: If $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is a continous function, is possible to define a continous function $\varphi:D\rightarrow \mathbb{R}^n$ (where $D\supset\mathbb{R}^n \times \{0\}$ is an open subset of $\mathbb{R}^{n+1}$) such that, $\varphi(p,0) = p$ and $\frac{d}{dt}\varphi(p,s) = f(\varphi(p,s))?$","Let $f$ $\in$ $\mathcal{C}^{1}(\mathbb{R}^n,\mathbb{R}^n)$  and consider the ODE: \begin{align*} y'&=f(y)\\ y(0)&=y_0 \end{align*} Once $f$ is a $\mathcal{C}^1$ function we can define the flow $\varphi$ of $f$. The function $\varphi$ satisfies the following conditions: $\varphi:D\rightarrow \mathbb{R}^n $ $\varphi(p,0)=p \hspace{0.1cm}$; $\forall$ $p$ $\in$ $\mathbb{R}^n$ $ \frac{d}{dt}\varphi(p,t) = f(\varphi(p,t)) $, $\forall$ $(p,t)$ $\in$ $D$ where $D$ the ""definition domain of $f$"", i.e , $D =\{(x,s)\in \mathbb{R}^{n+1};$ $s$ $\in$ $(\omega_x^-,\omega_x^+)$}, being that $(\omega_x^-,\omega_x^+)$ the maximal domain of solution of the ODE \begin{align*} y'&=f(y)\\ y(0)&=x \end{align*} It's possible to prove that $D$ is an open set and $\varphi$ is a $\mathcal{C}^1$ function. Now, I would like to know if it is possible to define the flow of $f$ supposing $f$ just a continuous function, I know that maybe this is impossible because in this case, we do not have a solution uniqueness of the differential equation. My Question: If $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is a continous function, is possible to define a continous function $\varphi:D\rightarrow \mathbb{R}^n$ (where $D\supset\mathbb{R}^n \times \{0\}$ is an open subset of $\mathbb{R}^{n+1}$) such that, $\varphi(p,0) = p$ and $\frac{d}{dt}\varphi(p,s) = f(\varphi(p,s))?$",,"['ordinary-differential-equations', 'dynamical-systems']"
51,Explain why no boundary conditions are needed,Explain why no boundary conditions are needed,,"The Question: Consider the operator $$My(x) \equiv \frac{d}{dx}\biggl(a(x) \frac{dy}{dx} \biggr) + b(x)y(x) \; \; \; \; \; \; \; \; , \; \; \; \; \; \; \; \; \alpha<x<\beta $$ Suppose that $a$ satisfies $a(\alpha)=a(\beta)=0$ , and we want to solve $$My(x)=\lambda y(x)$$ I have to explain why no boundary conditions need to be given. I get how the operator is already in Sturm-Liouville form, so that it is self-adjoint, but I honestly don't get how you can solve an ODE with no boundary conditions.","The Question: Consider the operator Suppose that satisfies , and we want to solve I have to explain why no boundary conditions need to be given. I get how the operator is already in Sturm-Liouville form, so that it is self-adjoint, but I honestly don't get how you can solve an ODE with no boundary conditions.","My(x) \equiv \frac{d}{dx}\biggl(a(x) \frac{dy}{dx} \biggr) + b(x)y(x) \; \; \; \; \; \; \; \; , \; \; \; \; \; \; \; \; \alpha<x<\beta  a a(\alpha)=a(\beta)=0 My(x)=\lambda y(x)","['ordinary-differential-equations', 'boundary-value-problem', 'eigenfunctions', 'sturm-liouville']"
52,How has this result been obtained??,How has this result been obtained??,,"We have the quadratic equation $$ 2y^2 -(1+x)y + x = 0 $$ The author derives the following result from the above equation - $$ y' = \frac{y-1}{4y-1-x} = \frac{(x-3)y-x+1}{x^2-6x+1} $$ Now, I understand the first part. It has been obtained by simply differentiating w.r.t to $x$ and then solving for $y'$. But, I am unable to obtain the second part. I have tried lot of different substitutions, but haven't been able to arrive at the result. Please help!!","We have the quadratic equation $$ 2y^2 -(1+x)y + x = 0 $$ The author derives the following result from the above equation - $$ y' = \frac{y-1}{4y-1-x} = \frac{(x-3)y-x+1}{x^2-6x+1} $$ Now, I understand the first part. It has been obtained by simply differentiating w.r.t to $x$ and then solving for $y'$. But, I am unable to obtain the second part. I have tried lot of different substitutions, but haven't been able to arrive at the result. Please help!!",,"['ordinary-differential-equations', 'proof-explanation', 'quadratics']"
53,Can the quantum harmonic oscillator be solved by power series methods without going for asymptotic analysis?,Can the quantum harmonic oscillator be solved by power series methods without going for asymptotic analysis?,,"Although this is a question pertaining to Physics, since this is related to the mathematical treatment of a differential equation, I believe it is well suited for this community. While deriving the wave function for harmonic oscillator potential using Schrodinger's equation, we obtain the following equation through rearrangement of constants and nondimensionalization of the variables. $$\frac{d^2\psi}{du^2}+(\epsilon-u^2)\psi=0 \tag1$$ And then we use the technique of asymptotic analysis. This is achieved by checking the behaviour of $\psi$ at large $u$ and guessing the form of the solution as $$\psi \approx \exp(-u^2) g(u)$$ And then we obtain Hermite's differential equation for $g(u)$ which can solved by power series solution. My Question: Why can't we avoid Asymptotic Analysis and directly go for a Series Solution?   Why can't we just directly take $$\psi(u)=\sum_\limits{n=0}^{\infty} a_n u^n \tag2$$ I have checked everywhere in the internet and also in all standard books on quantum mechanics. What I have observed is that they directly go for the asymptotic analysis without stating any reason. They simply say that the asymptotic analysis will help in simplifying the calculations. However they do not mention anything about a direct solution by power series method. They do not make any comments on the possible of a direct series solution; neither why we may be able to go for such solutions nor why we can't go for such a method and have to adopt something called asymptotic analysis. I tried solving Schrödinger's Equation using such a power series as in $(2)$. What I got was:  $$2a_2+\epsilon a_0+(6a_3+\epsilon a_1)u+\sum_\limits{n=0}^{\infty} \left[(n+4)(n+3)a_{n+4}+\epsilon a_{n+2}-a_n\right]u^{n+2}=0$$ These gives 2 constants and $1$ recursion. $$(n+4)(n+3)a_{n+4}+\epsilon a_{n+2}-a_n=0$$ I know that it is difficult to obtain any nice desired result from this recursive relation. But is this correct? Is this process feasible here? I checked for singularities and found none. In case this is correct, is it so that both asymptotic analysis and my procedure are allowed but the asymptotic analysis method is most favoured since in that case we get closed form results which can be used to derive other useful results? Or is this series solution not feasible due to some reason more general?","Although this is a question pertaining to Physics, since this is related to the mathematical treatment of a differential equation, I believe it is well suited for this community. While deriving the wave function for harmonic oscillator potential using Schrodinger's equation, we obtain the following equation through rearrangement of constants and nondimensionalization of the variables. $$\frac{d^2\psi}{du^2}+(\epsilon-u^2)\psi=0 \tag1$$ And then we use the technique of asymptotic analysis. This is achieved by checking the behaviour of $\psi$ at large $u$ and guessing the form of the solution as $$\psi \approx \exp(-u^2) g(u)$$ And then we obtain Hermite's differential equation for $g(u)$ which can solved by power series solution. My Question: Why can't we avoid Asymptotic Analysis and directly go for a Series Solution?   Why can't we just directly take $$\psi(u)=\sum_\limits{n=0}^{\infty} a_n u^n \tag2$$ I have checked everywhere in the internet and also in all standard books on quantum mechanics. What I have observed is that they directly go for the asymptotic analysis without stating any reason. They simply say that the asymptotic analysis will help in simplifying the calculations. However they do not mention anything about a direct solution by power series method. They do not make any comments on the possible of a direct series solution; neither why we may be able to go for such solutions nor why we can't go for such a method and have to adopt something called asymptotic analysis. I tried solving Schrödinger's Equation using such a power series as in $(2)$. What I got was:  $$2a_2+\epsilon a_0+(6a_3+\epsilon a_1)u+\sum_\limits{n=0}^{\infty} \left[(n+4)(n+3)a_{n+4}+\epsilon a_{n+2}-a_n\right]u^{n+2}=0$$ These gives 2 constants and $1$ recursion. $$(n+4)(n+3)a_{n+4}+\epsilon a_{n+2}-a_n=0$$ I know that it is difficult to obtain any nice desired result from this recursive relation. But is this correct? Is this process feasible here? I checked for singularities and found none. In case this is correct, is it so that both asymptotic analysis and my procedure are allowed but the asymptotic analysis method is most favoured since in that case we get closed form results which can be used to derive other useful results? Or is this series solution not feasible due to some reason more general?",,"['calculus', 'ordinary-differential-equations', 'asymptotics', 'power-series', 'quantum-mechanics']"
54,Relation between simple critical points of Hamiltonian and gradient systems,Relation between simple critical points of Hamiltonian and gradient systems,,"I'm doing some exercices to see how are related the hamiltonian and the gradient systems. I did an exercise, but I don't know if my approach is correct and I have a few questions about it. Let's state it: Let $H:\mathbb{R}^2 \to \mathbb{R}$ be a $C^2$ function. We consider the Hamiltonian: \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}\end{cases} and the gradient systems: \begin{cases}\dot{x}=\frac{\partial H(x,y)}{\partial x}\\ \dot{y}=\frac{\partial H(x,y)}{\partial y}\end{cases} a) Given an ODE on the plane, we'd say that a simple critical point is a critical point whose eigenvalues are different of $0$ . Prove that if we have a simple critical point for one of those systems, it's also a simple critical point for the other one. So here's my try: Let's suppose that we have a simple critical point for a hamiltonian system. We know that if $(x,y)$ is a simple critical point, we have \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}=0\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}=0\end{cases} It's straightforward that: \begin{cases}\frac{\partial H(x,y)}{\partial x}=0\\ \frac{\partial H(x,y)}{\partial y}=0\end{cases} so it's a simple critical point of the gradient system too. It's correct? The condition of ""simple"" means that $(x,y)\neq (0,0)$ ? Thanks.","I'm doing some exercices to see how are related the hamiltonian and the gradient systems. I did an exercise, but I don't know if my approach is correct and I have a few questions about it. Let's state it: Let be a function. We consider the Hamiltonian: and the gradient systems: a) Given an ODE on the plane, we'd say that a simple critical point is a critical point whose eigenvalues are different of . Prove that if we have a simple critical point for one of those systems, it's also a simple critical point for the other one. So here's my try: Let's suppose that we have a simple critical point for a hamiltonian system. We know that if is a simple critical point, we have It's straightforward that: so it's a simple critical point of the gradient system too. It's correct? The condition of ""simple"" means that ? Thanks.","H:\mathbb{R}^2 \to \mathbb{R} C^2 \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}\end{cases} \begin{cases}\dot{x}=\frac{\partial H(x,y)}{\partial x}\\ \dot{y}=\frac{\partial H(x,y)}{\partial y}\end{cases} 0 (x,y) \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}=0\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}=0\end{cases} \begin{cases}\frac{\partial H(x,y)}{\partial x}=0\\ \frac{\partial H(x,y)}{\partial y}=0\end{cases} (x,y)\neq (0,0)",['ordinary-differential-equations']
55,Variation of constants on a second order DE,Variation of constants on a second order DE,,"Given is the DE $$\frac{d^2z}{dx^2}+z=\frac{\mu}{x^2}z$$ I have to prove that for $0<a\leqslant x$, $$z(x)=z(a)\cos(x-a)+z'(a)\sin(x-a) + \int_a^x \sin(x-\xi)\frac{\mu}{\xi^2}z(\xi) \, d\xi$$ I think it is a good idea to use variation of constants. So first I solved $\frac{d^2z}{dx^2}+z=0$, giving me $z(x)=c_1\sin(x)+c_2\cos(x)$, with $c_1, c_2$ constants. Then, I wrote $z(x)=u_1(x)\sin(x)+u_2(x)\cos(x)$ and calculated $z'(x)$ and $z''(x)$ accordingly. But here I got lost. Can anybody help me?","Given is the DE $$\frac{d^2z}{dx^2}+z=\frac{\mu}{x^2}z$$ I have to prove that for $0<a\leqslant x$, $$z(x)=z(a)\cos(x-a)+z'(a)\sin(x-a) + \int_a^x \sin(x-\xi)\frac{\mu}{\xi^2}z(\xi) \, d\xi$$ I think it is a good idea to use variation of constants. So first I solved $\frac{d^2z}{dx^2}+z=0$, giving me $z(x)=c_1\sin(x)+c_2\cos(x)$, with $c_1, c_2$ constants. Then, I wrote $z(x)=u_1(x)\sin(x)+u_2(x)\cos(x)$ and calculated $z'(x)$ and $z''(x)$ accordingly. But here I got lost. Can anybody help me?",,['ordinary-differential-equations']
56,How do we know phasors solve differential equations?,How do we know phasors solve differential equations?,,"I know that phasors are used to simplify the calculations, I also get why in AC, current in a capacitor leads the voltage by 90°, and lags for an inductor (we can see that by differentiating/integrating), but how do we know it will solve differential equations too? Thinking of sine and cosine terms as vectors really helps in adding/subtracting them. (But I don't see how it helps in multiplying/dividing) Suppose I have an RL circuit connected in series to an AC emf like this: Then, by phasors, we would say $\overline{Z} = \sqrt{R^2 + L^2 \omega^2}\angle\tan^{-1}(\omega L/R)$ or $\overline{i} = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}}\angle\tan^{-1}(-\omega L/R)$ as $\overline{i}=\displaystyle\frac{\overline{e}}{\overline{Z}}$ (Taking $\overline{e} = e_0\angle0$) but without using phasors, we would have written $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$ and try to solve that differential equation, right? How do we know we're solving that differential equation by doing phasor algebra? By the expression I got for $\overline{i}$, $i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R))$ Then I thought, is it really the solution to the differential equation $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$? I looked up the solution to that equation and it's slightly different $$i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R)) - c e^{-Rt/L}$$ According to this , the phasor answer is correct if c=0, which does not mean $i(0) = 0$, I checked the graph, but it isn't too different from the phasor answer for positive values of t. Why are we getting this extra term different from the phasor answer?","I know that phasors are used to simplify the calculations, I also get why in AC, current in a capacitor leads the voltage by 90°, and lags for an inductor (we can see that by differentiating/integrating), but how do we know it will solve differential equations too? Thinking of sine and cosine terms as vectors really helps in adding/subtracting them. (But I don't see how it helps in multiplying/dividing) Suppose I have an RL circuit connected in series to an AC emf like this: Then, by phasors, we would say $\overline{Z} = \sqrt{R^2 + L^2 \omega^2}\angle\tan^{-1}(\omega L/R)$ or $\overline{i} = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}}\angle\tan^{-1}(-\omega L/R)$ as $\overline{i}=\displaystyle\frac{\overline{e}}{\overline{Z}}$ (Taking $\overline{e} = e_0\angle0$) but without using phasors, we would have written $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$ and try to solve that differential equation, right? How do we know we're solving that differential equation by doing phasor algebra? By the expression I got for $\overline{i}$, $i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R))$ Then I thought, is it really the solution to the differential equation $+e_0\sin(\omega t)-iR - L\frac{\text{d}i}{\text{d}t} = 0$? I looked up the solution to that equation and it's slightly different $$i(t) = \displaystyle\frac{e_0}{\sqrt{R^2 + L^2 \omega^2}} \sin(\omega t - \tan^{-1}(\omega L/R)) - c e^{-Rt/L}$$ According to this , the phasor answer is correct if c=0, which does not mean $i(0) = 0$, I checked the graph, but it isn't too different from the phasor answer for positive values of t. Why are we getting this extra term different from the phasor answer?",,"['calculus', 'ordinary-differential-equations', 'physics']"
57,Is it legitimate to divide both sides of an ODE by a dependent variable that can equal zero?,Is it legitimate to divide both sides of an ODE by a dependent variable that can equal zero?,,"I have the following problem: \begin{cases} y(x) =\left(\dfrac14\right)\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 \\ y(0)=0  \end{cases} Which can be written as: $$ \pm 2\sqrt{y} = \frac{dy}{dx} $$ I then take the positive case and treat it as an autonomous, seperable ODE.  I get $f(x)=x^2$ as my solution. In order to solve this problem, I have to divide each side of the equation by $\frac{1}{\sqrt{y}}$.  But since the solution to this IVP is $y(x)=x^2$, zero is in the image of $f(x)$.  So at a particular point $1/\sqrt{y}$ is not defined.  But the solution is defined at $y =0$. In fact, $y(x)= 0$ for all x is another solution.  But aside from this solution the non-trivial solution is defined at zero also. So is it wrong to divide across by $1/\sqrt{y}$? And if so how else do I approach this question?","I have the following problem: \begin{cases} y(x) =\left(\dfrac14\right)\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 \\ y(0)=0  \end{cases} Which can be written as: $$ \pm 2\sqrt{y} = \frac{dy}{dx} $$ I then take the positive case and treat it as an autonomous, seperable ODE.  I get $f(x)=x^2$ as my solution. In order to solve this problem, I have to divide each side of the equation by $\frac{1}{\sqrt{y}}$.  But since the solution to this IVP is $y(x)=x^2$, zero is in the image of $f(x)$.  So at a particular point $1/\sqrt{y}$ is not defined.  But the solution is defined at $y =0$. In fact, $y(x)= 0$ for all x is another solution.  But aside from this solution the non-trivial solution is defined at zero also. So is it wrong to divide across by $1/\sqrt{y}$? And if so how else do I approach this question?",,['ordinary-differential-equations']
58,Adaptive Step Size in RK45 for Second-Order ODE,Adaptive Step Size in RK45 for Second-Order ODE,,"My question pertains to the answer given in this post , but I am implementing the RK45 or RKF45 algorithm. Following the explanation in that post for a second order ODE, as an example I am looking at a pendulum whose equation of motion is $$\ddot{\theta} = -\Omega_0^2 \sin\theta$$ Setting $\Omega_0$ (or converting to dimensionless variables) for simplicity, I separate this into two equation $$\dot{v} = -\sin\theta$$ $$\dot{\theta} = v$$ I change this into a vector relationship, $$ \begin{pmatrix} 	\dot{v} \\ \dot{\theta}  \end{pmatrix} = \begin{pmatrix} 	-\sin\theta(t) \\ v(t) \end{pmatrix}  $$ Let's define the vector $\vec{y} = (v,\theta)^T$, then the left hand side is $\dot{\vec{y}}$ and the right side $\vec{f}(\vec{y}) = \vec{f}(\theta,v)$. The initial condition is given by $\vec{y}(t_0)$. Note there is no explicit time dependence, therefore the RK45 equations become \begin{align*} 	\vec{k}_1 &= \delta t\,\vec{f}(\vec{y}(t_n)) \\[4pt] 	\vec{k}_2 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{1}{4}\vec{k}_1\right) \\[4pt] 	\vec{k}_3 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{3}{32}\vec{k}_1 + \tfrac{9}{32}\vec{k}_2\right) \\[4pt] 	\vec{k}_4 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{1932}{2197}\vec{k}_1 - \tfrac{7200}{2197} \vec{k}_2 + \tfrac{7296}{2197}\vec{k}_3\right) \\[4pt] 	\vec{k}_5 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{439}{216}\vec{k}_1 - 8\vec{k}_2 + \tfrac{3680}{513}\vec{k}_3 - \tfrac{845}{4104}\vec{k}_4\right) \\[4pt] 	\vec{k}_6 &= \delta t\,\vec{f}\left(\vec{y}(t_n) - \tfrac{8}{27}\vec{k}_1 + 2\vec{k}_2 - \tfrac{3544}{2565}\vec{k}_3 + \tfrac{1859}{4104}\vec{k}_4 - \tfrac{11}{40}\vec{k}_5\right) \end{align*} where $\vec{y}(t_n) = (v(t_n),\theta(t_n))^T$. The new values are given to fourth order by $$\vec{y}^{(4)}(t_{n+1}) = \vec{y}(t_n) + \left(\tfrac{25}{216} \vec{k}_1 + \tfrac{1408}{2565}\vec{k}_3 + \tfrac{2197}{4101}\vec{k}_4 - \tfrac{1}{5} \vec{k}_5\right)$$ or to fifth order by \begin{align*} 	\vec{y}^{(5)}(t_{n+1}) &= \vec{y}(t_n) + \left(\tfrac{16}{135}\vec{k}_1 + \tfrac{6656}{12825}\vec{k}_3 + \tfrac{28561}{56430}\vec{k}_4 - \tfrac{9}{50} \vec{k}_5 + \tfrac{2}{55} \vec{k}_6\right)  \end{align*} Here is where my question is. In computing the new step size, $\delta t \to s \delta t$, and the formula generalizes to  $$s = \left(\frac{\epsilon\,\delta t_{old}}{2\left|\vec{y}^{(5)}(t_{n+1}) - \vec{y}^{(4)}(t_{n+1})\right|}\right)^{1/4}$$ I assume then that I can just treat the magnitude as the vector norm, but I am not certain this is correct. I have written a code which works (produces reasonable results with a fixed step size) but the adaptive step size seems to always want to rapidly decrease to zero, even if my error tolerance is relatively large, say $10^{-4}$. I'm not sure if this is a bug in my code or if I am misunderstanding how to apply the adaptive step size algorithm. Any help appreciated.","My question pertains to the answer given in this post , but I am implementing the RK45 or RKF45 algorithm. Following the explanation in that post for a second order ODE, as an example I am looking at a pendulum whose equation of motion is $$\ddot{\theta} = -\Omega_0^2 \sin\theta$$ Setting $\Omega_0$ (or converting to dimensionless variables) for simplicity, I separate this into two equation $$\dot{v} = -\sin\theta$$ $$\dot{\theta} = v$$ I change this into a vector relationship, $$ \begin{pmatrix} 	\dot{v} \\ \dot{\theta}  \end{pmatrix} = \begin{pmatrix} 	-\sin\theta(t) \\ v(t) \end{pmatrix}  $$ Let's define the vector $\vec{y} = (v,\theta)^T$, then the left hand side is $\dot{\vec{y}}$ and the right side $\vec{f}(\vec{y}) = \vec{f}(\theta,v)$. The initial condition is given by $\vec{y}(t_0)$. Note there is no explicit time dependence, therefore the RK45 equations become \begin{align*} 	\vec{k}_1 &= \delta t\,\vec{f}(\vec{y}(t_n)) \\[4pt] 	\vec{k}_2 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{1}{4}\vec{k}_1\right) \\[4pt] 	\vec{k}_3 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{3}{32}\vec{k}_1 + \tfrac{9}{32}\vec{k}_2\right) \\[4pt] 	\vec{k}_4 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{1932}{2197}\vec{k}_1 - \tfrac{7200}{2197} \vec{k}_2 + \tfrac{7296}{2197}\vec{k}_3\right) \\[4pt] 	\vec{k}_5 &= \delta t\,\vec{f}\left(\vec{y}(t_n) + \tfrac{439}{216}\vec{k}_1 - 8\vec{k}_2 + \tfrac{3680}{513}\vec{k}_3 - \tfrac{845}{4104}\vec{k}_4\right) \\[4pt] 	\vec{k}_6 &= \delta t\,\vec{f}\left(\vec{y}(t_n) - \tfrac{8}{27}\vec{k}_1 + 2\vec{k}_2 - \tfrac{3544}{2565}\vec{k}_3 + \tfrac{1859}{4104}\vec{k}_4 - \tfrac{11}{40}\vec{k}_5\right) \end{align*} where $\vec{y}(t_n) = (v(t_n),\theta(t_n))^T$. The new values are given to fourth order by $$\vec{y}^{(4)}(t_{n+1}) = \vec{y}(t_n) + \left(\tfrac{25}{216} \vec{k}_1 + \tfrac{1408}{2565}\vec{k}_3 + \tfrac{2197}{4101}\vec{k}_4 - \tfrac{1}{5} \vec{k}_5\right)$$ or to fifth order by \begin{align*} 	\vec{y}^{(5)}(t_{n+1}) &= \vec{y}(t_n) + \left(\tfrac{16}{135}\vec{k}_1 + \tfrac{6656}{12825}\vec{k}_3 + \tfrac{28561}{56430}\vec{k}_4 - \tfrac{9}{50} \vec{k}_5 + \tfrac{2}{55} \vec{k}_6\right)  \end{align*} Here is where my question is. In computing the new step size, $\delta t \to s \delta t$, and the formula generalizes to  $$s = \left(\frac{\epsilon\,\delta t_{old}}{2\left|\vec{y}^{(5)}(t_{n+1}) - \vec{y}^{(4)}(t_{n+1})\right|}\right)^{1/4}$$ I assume then that I can just treat the magnitude as the vector norm, but I am not certain this is correct. I have written a code which works (produces reasonable results with a fixed step size) but the adaptive step size seems to always want to rapidly decrease to zero, even if my error tolerance is relatively large, say $10^{-4}$. I'm not sure if this is a bug in my code or if I am misunderstanding how to apply the adaptive step size algorithm. Any help appreciated.",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
59,Radial derivative of the characteristic function of the unit disc in $\mathbb R^2$.,Radial derivative of the characteristic function of the unit disc in .,\mathbb R^2,"I need help to answer this question from Hörmander, Let $u$ be the characteristic function of the unit disc in $\mathbb  R^2$.  Calculate $x \frac{\partial u}{\partial x} + y \frac{\partial u}{\partial y}$. The answer in the book is: minus the arc length measure on the unit circle. I am not sure if my calculations are right. Let $\phi \in D(\mathbb R^2)$, $$\left<x \frac{\partial u}{\partial x} + y\frac{\partial u}{\partial y}, \phi\right>= - \left<u,\frac{\partial {(x \phi(x,y))}}{\partial x} + \frac{\partial (y \phi(x,y))}{\partial y} \right>= - \left<u, \operatorname{div} \begin{bmatrix}x \phi(x,y)\\y \phi(x,y)\end{bmatrix}\right>. $$ Now I think I have to use the Gauss-Green formula to get the length measure, but I don't know how to finish my answer if the above calculations are right. Any help please would be perfect.","I need help to answer this question from Hörmander, Let $u$ be the characteristic function of the unit disc in $\mathbb  R^2$.  Calculate $x \frac{\partial u}{\partial x} + y \frac{\partial u}{\partial y}$. The answer in the book is: minus the arc length measure on the unit circle. I am not sure if my calculations are right. Let $\phi \in D(\mathbb R^2)$, $$\left<x \frac{\partial u}{\partial x} + y\frac{\partial u}{\partial y}, \phi\right>= - \left<u,\frac{\partial {(x \phi(x,y))}}{\partial x} + \frac{\partial (y \phi(x,y))}{\partial y} \right>= - \left<u, \operatorname{div} \begin{bmatrix}x \phi(x,y)\\y \phi(x,y)\end{bmatrix}\right>. $$ Now I think I have to use the Gauss-Green formula to get the length measure, but I don't know how to finish my answer if the above calculations are right. Any help please would be perfect.",,"['functional-analysis', 'ordinary-differential-equations', 'fourier-analysis', 'partial-derivative', 'distribution-theory']"
60,"Example: $f(x,y)$ is not Lipschitz in y but still has a unique solution to initial value problem",Example:  is not Lipschitz in y but still has a unique solution to initial value problem,"f(x,y)","Consider the differential equation $y^\prime =\sqrt{y}+1$ with $y(0)=0$. My question is how to prove the uniqueness of the solution. I found a hint for it as follows: Consider $z(x)=(\sqrt{y_1(x)}-\sqrt{y_2(x}))^2$ where $y_1,y_2$ are solutions. I'm trying to use the hint to prove the uniqueness of solution, but I do not know what the hint means. My attempt: $z'(x)=2(\sqrt{y_1(x)}-\sqrt{y_2(x)}[(\sqrt{y_1(x)})'-(\sqrt{y_2(x)})']=(\sqrt{y_1(x)}-\sqrt{y_2(x)})\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)=(y_1'(x)-y_2'(x))\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)$. I couldn't proceed it further. Please give me slightly more comment or hint for the problem. Thanks in advance!","Consider the differential equation $y^\prime =\sqrt{y}+1$ with $y(0)=0$. My question is how to prove the uniqueness of the solution. I found a hint for it as follows: Consider $z(x)=(\sqrt{y_1(x)}-\sqrt{y_2(x}))^2$ where $y_1,y_2$ are solutions. I'm trying to use the hint to prove the uniqueness of solution, but I do not know what the hint means. My attempt: $z'(x)=2(\sqrt{y_1(x)}-\sqrt{y_2(x)}[(\sqrt{y_1(x)})'-(\sqrt{y_2(x)})']=(\sqrt{y_1(x)}-\sqrt{y_2(x)})\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)=(y_1'(x)-y_2'(x))\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)$. I couldn't proceed it further. Please give me slightly more comment or hint for the problem. Thanks in advance!",,['ordinary-differential-equations']
61,How to solve the SDE $dB_t = r_tB_tdt$ when $r_t$ is stochastic?,How to solve the SDE  when  is stochastic?,dB_t = r_tB_tdt r_t,"Suppose we have $$dB_t = r_tB_tdt$$ and we have further that $r_t$ is a deterministic function of time $t$. I know that this is simply an ODE with separable variables that has standard solution in $[t,T]$: $$B_T = B_te^{\int_{t}^{T}r_sds} $$ but what if $r_t$ is stochastic? For example it could be $$dr_t = \mu r_tdt + \sigma r_tdW_t$$ where $W_t$ is the Brownian motion. Then $B_t$ itself is stochastic and how can I integrate something like $$\int_{t}^{T}\frac{dB_t}{B_t}$$ Isn't this a stochastic integral? I am having hard times figuring out why in this case the solution is the same as when $r_t$ is deterministic. Thank you very much for your help","Suppose we have $$dB_t = r_tB_tdt$$ and we have further that $r_t$ is a deterministic function of time $t$. I know that this is simply an ODE with separable variables that has standard solution in $[t,T]$: $$B_T = B_te^{\int_{t}^{T}r_sds} $$ but what if $r_t$ is stochastic? For example it could be $$dr_t = \mu r_tdt + \sigma r_tdW_t$$ where $W_t$ is the Brownian motion. Then $B_t$ itself is stochastic and how can I integrate something like $$\int_{t}^{T}\frac{dB_t}{B_t}$$ Isn't this a stochastic integral? I am having hard times figuring out why in this case the solution is the same as when $r_t$ is deterministic. Thank you very much for your help",,"['probability', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus']"
62,How to prove a fixed point is stable?,How to prove a fixed point is stable?,,"\begin{align*} \dot{x} &= 2 x - \frac{8}{5} x^2 - xy\\ \dot{y} &= \frac{5}{2} y - y^2 - 2 xy \end{align*} So I have this dynamical system. I linearized it and found that the fixed point (at $x = 1.25$, $y = 0$) is the boundary case ( https://en.wikipedia.org/wiki/Linear_dynamical_system#/media/File:LinDynSysTraceDet.jpg ). $\tau$ = -2 and $\bigtriangleup$ = 0, which means the linearization says that it is a line of fixed point, which doesn't make sense since there is only 1 fixed point. My professor says that for non-linear system there is disturbance for the boundary case, so 3 possible outcomes are possible. It can be either saddle node, line of fixed points, or stable point. I tried pplane software (you can download it online for free too), and when I zoomed it for x = [1.2, 1.3], y = [-.05, .05] . It does look like there is a line of fixed point slightly above the fixed point (1.25, 0). I'm kind of stuck here. How do I prove the stability of this fixed point now? I also found the Lyapunov function for the system:  $$V = \ln(x) - \frac{4}{5}\ln(y).$$ The two eigenvalues are -2 & 0 with eigenvectors (1,0) & (5, -8) respectively for fixed point (1.25, 0). This problem is just very weird. I have no idea what eigenvalue of 0 means. I also graphed out all the eigenvectors of the other fixed points too. Basically, I can't tell if the fixed point (1.25, 0) is stable or not. Please help!! SOME IDEAS: Let's say I have a region D bounded by four points: (1.24, .01), (1.26, .01), (1.24, 0), (1.26, 0). It's easy to see that the Lyapunov inside this region is always positive, and $\dot{V}$ is always negative when x, y > 0. This proves that the fixed point (1.25, 0) is stable at least around this neighborhood. And that means it has to be stable! Does this approach mathematically sound? Thank you!","\begin{align*} \dot{x} &= 2 x - \frac{8}{5} x^2 - xy\\ \dot{y} &= \frac{5}{2} y - y^2 - 2 xy \end{align*} So I have this dynamical system. I linearized it and found that the fixed point (at $x = 1.25$, $y = 0$) is the boundary case ( https://en.wikipedia.org/wiki/Linear_dynamical_system#/media/File:LinDynSysTraceDet.jpg ). $\tau$ = -2 and $\bigtriangleup$ = 0, which means the linearization says that it is a line of fixed point, which doesn't make sense since there is only 1 fixed point. My professor says that for non-linear system there is disturbance for the boundary case, so 3 possible outcomes are possible. It can be either saddle node, line of fixed points, or stable point. I tried pplane software (you can download it online for free too), and when I zoomed it for x = [1.2, 1.3], y = [-.05, .05] . It does look like there is a line of fixed point slightly above the fixed point (1.25, 0). I'm kind of stuck here. How do I prove the stability of this fixed point now? I also found the Lyapunov function for the system:  $$V = \ln(x) - \frac{4}{5}\ln(y).$$ The two eigenvalues are -2 & 0 with eigenvectors (1,0) & (5, -8) respectively for fixed point (1.25, 0). This problem is just very weird. I have no idea what eigenvalue of 0 means. I also graphed out all the eigenvectors of the other fixed points too. Basically, I can't tell if the fixed point (1.25, 0) is stable or not. Please help!! SOME IDEAS: Let's say I have a region D bounded by four points: (1.24, .01), (1.26, .01), (1.24, 0), (1.26, 0). It's easy to see that the Lyapunov inside this region is always positive, and $\dot{V}$ is always negative when x, y > 0. This proves that the fixed point (1.25, 0) is stable at least around this neighborhood. And that means it has to be stable! Does this approach mathematically sound? Thank you!",,"['ordinary-differential-equations', 'chaos-theory']"
63,Numerical evaluation of the period of a limit cycle,Numerical evaluation of the period of a limit cycle,,"How can I calculate all the periods of the limit cycle of the Ueda-Duffing equation with forcing: $\ddot{x} + k \dot{x} + x^3 = B \cos(t) $ for each set of parameters $(k, B)$ ? Edit: The equation exhibits sub-harmonic resonance for some sets of parameters (and chaotic behaviour too).  E.g. for $k=0.08, B=0.2$ there are 5 coexisting attractors  of period $2n\pi  $ with $n=1, 2, 3$","How can I calculate all the periods of the limit cycle of the Ueda-Duffing equation with forcing: $\ddot{x} + k \dot{x} + x^3 = B \cos(t) $ for each set of parameters $(k, B)$ ? Edit: The equation exhibits sub-harmonic resonance for some sets of parameters (and chaotic behaviour too).  E.g. for $k=0.08, B=0.2$ there are 5 coexisting attractors  of period $2n\pi  $ with $n=1, 2, 3$",,"['ordinary-differential-equations', 'numerical-methods', 'nonlinear-system']"
64,Solution of Exact and Homogeneous differential equation,Solution of Exact and Homogeneous differential equation,,Consider the equation $(5y - 2x) (\dfrac{dy}{dx}) - 2y = 0$ This equation is Exact and Homgeneous differential equation. When i use the exact method then i get the following solution $2xy -(5/2) y^2 = c$ And when I use the Homogeneous method of solution that is putting $y = vx$ in Homogeneous differential equation then I get the following solution $ln (y) + \dfrac{2x}{5y} = c$. Question is that are both right solutions. Im confused about different solution. Explain it please.,Consider the equation $(5y - 2x) (\dfrac{dy}{dx}) - 2y = 0$ This equation is Exact and Homgeneous differential equation. When i use the exact method then i get the following solution $2xy -(5/2) y^2 = c$ And when I use the Homogeneous method of solution that is putting $y = vx$ in Homogeneous differential equation then I get the following solution $ln (y) + \dfrac{2x}{5y} = c$. Question is that are both right solutions. Im confused about different solution. Explain it please.,,['ordinary-differential-equations']
65,Control theory - feedback control of a damped oscillator to stabilise velocity,Control theory - feedback control of a damped oscillator to stabilise velocity,,"I am looking at applying some simple control theory to a damped oscillator. If I have the following dynamics \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix} +   \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}   u \end{equation} Such that my $A$ matrix is $\begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}$ and by B matrix is $  \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}$ and my control is $u$, an external force on the oscillator. I can calculate the controllability matrix \begin{equation}   \mathcal{C} =   \begin{bmatrix}     0 & \dfrac{1}{m} \\     \dfrac{1}{m} & -\dfrac{\Gamma}{m} \\   \end{bmatrix}   \label{controllability_matrix} \end{equation} which has rank 2. This means the controllability matrix has full column rank, this means, as I understand it, that this system is controllable. This means that we can arbitrarily place the eigenvalues (also sometimes called poles) of the system dynamics by tuning $\mathbf{K}$ in $u = -\mathbf{K}\vec{x}$ because the system dyamics becomes $\dot{\vec{x}} = (\mathbf{A} - \mathbf{B}\mathbf{K})\vec{x}$. This also means we have reachability, meaning we can drive the system to any state, the reachable set of states $R_t = \left\{ \xi ~\epsilon ~\mathbb{R}^n \right\}$. If I then plug in $u = -K\vec{x}$ where I change $\vec{x}$ to $\vec{x}-\vec{x_t}$ where $\vec{x_t}$ is my target state I want to set the system to be driven towards. \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix}   -   \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}   \begin{bmatrix}     K_0 & K_1 \\   \end{bmatrix}   \begin{bmatrix}     x - x_t \\     \dot{x} - \dot{x}_t \\   \end{bmatrix} \end{equation} which results in \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix}   -   \begin{bmatrix}     0\\     \dfrac{1}{m}K_0(x-x_t) + \dfrac{1}{m}K_1(\dot{x}-\dot{x}_t) \\   \end{bmatrix} \end{equation} However when I put in some values for $\omega$, $\Gamma$ and $m$ and calculate the K matrix by setting the eigenvalues to be $n\times eig(A)$ [where n>1, the larger n is than 1 the more aggressive the feedback, I've used values like 1.5, 2, 3 ... etc ](this was just an initial guess - I wasn't sure where to place the eigenvalues to start - other than that they want to have a negative real value for stability and the more negative they are the more aggressive the feedback) by using K = place(A, B, eigs(A)*n) in matlab then I get a K matrix where $K_1$ is 0, and therefore I cannot control $\dot{x}$, why is this and how can I control $\dot{x}$? I've been able to simulate this the see that it can control $x$. Also, is it possible to set the system to be driven to any state by this control? It doesn't make sense that the system could be prepared in a state such as $x = 5cm$, $\dot{x} = 5m/s$ stably for example, as the positive velocity means it won't stay at $x = 5cm$. How can I calculate what states are reachable and stable?","I am looking at applying some simple control theory to a damped oscillator. If I have the following dynamics \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix} +   \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}   u \end{equation} Such that my $A$ matrix is $\begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}$ and by B matrix is $  \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}$ and my control is $u$, an external force on the oscillator. I can calculate the controllability matrix \begin{equation}   \mathcal{C} =   \begin{bmatrix}     0 & \dfrac{1}{m} \\     \dfrac{1}{m} & -\dfrac{\Gamma}{m} \\   \end{bmatrix}   \label{controllability_matrix} \end{equation} which has rank 2. This means the controllability matrix has full column rank, this means, as I understand it, that this system is controllable. This means that we can arbitrarily place the eigenvalues (also sometimes called poles) of the system dynamics by tuning $\mathbf{K}$ in $u = -\mathbf{K}\vec{x}$ because the system dyamics becomes $\dot{\vec{x}} = (\mathbf{A} - \mathbf{B}\mathbf{K})\vec{x}$. This also means we have reachability, meaning we can drive the system to any state, the reachable set of states $R_t = \left\{ \xi ~\epsilon ~\mathbb{R}^n \right\}$. If I then plug in $u = -K\vec{x}$ where I change $\vec{x}$ to $\vec{x}-\vec{x_t}$ where $\vec{x_t}$ is my target state I want to set the system to be driven towards. \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix}   -   \begin{bmatrix}     0\\     \dfrac{1}{m} \\   \end{bmatrix}   \begin{bmatrix}     K_0 & K_1 \\   \end{bmatrix}   \begin{bmatrix}     x - x_t \\     \dot{x} - \dot{x}_t \\   \end{bmatrix} \end{equation} which results in \begin{equation}   \begin{bmatrix}     \dot{x}\\     \ddot{x} \\   \end{bmatrix}   =   \begin{bmatrix}     0 & 1 \\     -\omega^2 & -\Gamma   \end{bmatrix}   \begin{bmatrix}     x\\     \dot{x} \\   \end{bmatrix}   -   \begin{bmatrix}     0\\     \dfrac{1}{m}K_0(x-x_t) + \dfrac{1}{m}K_1(\dot{x}-\dot{x}_t) \\   \end{bmatrix} \end{equation} However when I put in some values for $\omega$, $\Gamma$ and $m$ and calculate the K matrix by setting the eigenvalues to be $n\times eig(A)$ [where n>1, the larger n is than 1 the more aggressive the feedback, I've used values like 1.5, 2, 3 ... etc ](this was just an initial guess - I wasn't sure where to place the eigenvalues to start - other than that they want to have a negative real value for stability and the more negative they are the more aggressive the feedback) by using K = place(A, B, eigs(A)*n) in matlab then I get a K matrix where $K_1$ is 0, and therefore I cannot control $\dot{x}$, why is this and how can I control $\dot{x}$? I've been able to simulate this the see that it can control $x$. Also, is it possible to set the system to be driven to any state by this control? It doesn't make sense that the system could be prepared in a state such as $x = 5cm$, $\dot{x} = 5m/s$ stably for example, as the positive velocity means it won't stay at $x = 5cm$. How can I calculate what states are reachable and stable?",,"['ordinary-differential-equations', 'optimization', 'matrix-equations', 'control-theory', 'linear-control']"
66,Transient terms in the solution of a linear differential equation,Transient terms in the solution of a linear differential equation,,I know how to solve a linear differential equation. But question is that what does that mean transient terms in general solution.,I know how to solve a linear differential equation. But question is that what does that mean transient terms in general solution.,,['ordinary-differential-equations']
67,Differential with sub y = ux,Differential with sub y = ux,,I am differentiating a first order DE. I want to use the substitution y = ux and the differential $dy = udx +xdu$. I thought $dy = u + xdu$ because $dx = x$. I thought I am differentiating with respect to x. I would appreciate any feedback. thank you,I am differentiating a first order DE. I want to use the substitution y = ux and the differential $dy = udx +xdu$. I thought $dy = u + xdu$ because $dx = x$. I thought I am differentiating with respect to x. I would appreciate any feedback. thank you,,['ordinary-differential-equations']
68,Ordinary Differential Equation with 3 unknowns,Ordinary Differential Equation with 3 unknowns,,"Solve the IVP system $\displaystyle{\begin{cases}x_1'=3x_1-4x_2+4x_3\\x_2'=4x_1-5x_2+4x_3\\x_3'=4x_1-4x_2+3x_3\\x_1(0)=2,\ x_2(0)=1,\ x_3(0)=-1\end{cases}}$ I am having trouble solving this. I know one method involves finding the eigenvalues and eigenvectors but is there not a method without using linear algebra and eigenvalues?","Solve the IVP system $\displaystyle{\begin{cases}x_1'=3x_1-4x_2+4x_3\\x_2'=4x_1-5x_2+4x_3\\x_3'=4x_1-4x_2+3x_3\\x_1(0)=2,\ x_2(0)=1,\ x_3(0)=-1\end{cases}}$ I am having trouble solving this. I know one method involves finding the eigenvalues and eigenvectors but is there not a method without using linear algebra and eigenvalues?",,"['linear-algebra', 'ordinary-differential-equations']"
69,Circuit current from series resistor and inductor + pulse voltage using Laplace method,Circuit current from series resistor and inductor + pulse voltage using Laplace method,,"A series $RL$ circuit experiences a pulse of voltage, $V$, occurring during the interval $t_0 \lt t \lt t_1$.  Determine the circuit current $i(t)$. First I've broken up the pulse into $\pm V u(t - t_j), \ j=0,1$ and will add the two solutions $i_j(t), \ j=0,1$ when done, via superposition. So the differential equation modeling the system is: $\pm V u(t - t_j) = R i_j(t) + L \dfrac{d i_j(t)}{dt}$. Taking the Laplace transform and rearranging I get: $I_j(s) = \dfrac{e^{-t_j s}}{s}\dfrac{ \pm \dfrac{V}{L}}{\dfrac{R}{L} + s}$ and taking the inverse transform I get: $\pm \dfrac{V}{L} \int\limits_0^t u(\tau - t_j) e^{-\frac{R}{L}(t - \tau)} d\tau$. My question is how do I evaluate this integral, or am I doing it wrong?","A series $RL$ circuit experiences a pulse of voltage, $V$, occurring during the interval $t_0 \lt t \lt t_1$.  Determine the circuit current $i(t)$. First I've broken up the pulse into $\pm V u(t - t_j), \ j=0,1$ and will add the two solutions $i_j(t), \ j=0,1$ when done, via superposition. So the differential equation modeling the system is: $\pm V u(t - t_j) = R i_j(t) + L \dfrac{d i_j(t)}{dt}$. Taking the Laplace transform and rearranging I get: $I_j(s) = \dfrac{e^{-t_j s}}{s}\dfrac{ \pm \dfrac{V}{L}}{\dfrac{R}{L} + s}$ and taking the inverse transform I get: $\pm \dfrac{V}{L} \int\limits_0^t u(\tau - t_j) e^{-\frac{R}{L}(t - \tau)} d\tau$. My question is how do I evaluate this integral, or am I doing it wrong?",,"['calculus', 'integration', 'ordinary-differential-equations', 'definite-integrals', 'laplace-transform']"
70,Sufficient conditions for global stability from linear stability.,Sufficient conditions for global stability from linear stability.,,"Consider $$ \dot x = -Ax+f(x),$$ where $A>0$ and $f(x)$ smooth. Question: What are sufficient conditions for $f(x)$ such that the origin of the system is globally asymptotically stable? I know that the local version holds. But I am looking for global version. A trivial example would be $f(x)=Bx$ with $-A+B<0$. My approach: Let $x_l(t)$ denote the solution of the linear system and $x(t)$ of the nonlinear. Moreover, suppose that $f(x)$ is uniformly bounded, say $||f(x)||<\beta$ with $\beta>0$, and that $f(x)=0\iff x=0$. It follows that \begin{equation} \begin{split} || x(t)-x_l(t)|| &= ||\int_0^te^{-A(t-\tau)}f(x(\tau))d\tau||\\ &\leq \int_0^t  ||e^{-A(t-\tau)}||\cdot||f(x(\tau))||d\tau\\ &\leq \beta\int_0^t  ||e^{-A(t-\tau)}||d\tau,\\ \end{split} \end{equation} where the second inequality is due to our assumptions. The above error does not converge to $0$ (but to $\beta||A^{-1}||$) as $t\to\infty$. So, the assumptions only lead to stability. Naturally I can pick some specific cases where the integral can be explicitly solved to ensure that the error converges to zero, but I fail to see a general assumption on $f(x)$.","Consider $$ \dot x = -Ax+f(x),$$ where $A>0$ and $f(x)$ smooth. Question: What are sufficient conditions for $f(x)$ such that the origin of the system is globally asymptotically stable? I know that the local version holds. But I am looking for global version. A trivial example would be $f(x)=Bx$ with $-A+B<0$. My approach: Let $x_l(t)$ denote the solution of the linear system and $x(t)$ of the nonlinear. Moreover, suppose that $f(x)$ is uniformly bounded, say $||f(x)||<\beta$ with $\beta>0$, and that $f(x)=0\iff x=0$. It follows that \begin{equation} \begin{split} || x(t)-x_l(t)|| &= ||\int_0^te^{-A(t-\tau)}f(x(\tau))d\tau||\\ &\leq \int_0^t  ||e^{-A(t-\tau)}||\cdot||f(x(\tau))||d\tau\\ &\leq \beta\int_0^t  ||e^{-A(t-\tau)}||d\tau,\\ \end{split} \end{equation} where the second inequality is due to our assumptions. The above error does not converge to $0$ (but to $\beta||A^{-1}||$) as $t\to\infty$. So, the assumptions only lead to stability. Naturally I can pick some specific cases where the integral can be explicitly solved to ensure that the error converges to zero, but I fail to see a general assumption on $f(x)$.",,"['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
71,Understanding this proof of Gronwall's inequality.,Understanding this proof of Gronwall's inequality.,,"Gronwall's theorem is given as follows: Assume that for $t_0\leq t \leq t_0 + a$, with $a$ a positive constant, we have the estimate $$\phi(t)\leq \delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3\,\,\,\,\,(1.4)$$ in which, for $t_0\leq t \leq t_0 + a$, $\phi(t)$ and $\psi(t)$ are continuous functions, $\phi(t) \geq 0$ and $\psi(t)\geq 0$; $\delta_1$ and $\delta_3$ are positive constants. Then we have for $t_0\leq t\leq t_0 + a$ $$\phi(t) \leq \delta_3e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ In my book the following proof of this theorem is proof is given: From the estimate $(1.4)$ we derive $$\dfrac{\phi(t)}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq 1$$  multiplication with $\delta_1\psi(t)$ and integration yields $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ so that $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ which produces $$\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3 \leq \delta_3 e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ Applying the estimate $(1.4)$ again, but now to the lefthand side, yields the required inequality. Question: How does the step from  $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ to $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ work? I think I understand the rest of the proof but I can't figure out the above step. Thanks!","Gronwall's theorem is given as follows: Assume that for $t_0\leq t \leq t_0 + a$, with $a$ a positive constant, we have the estimate $$\phi(t)\leq \delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3\,\,\,\,\,(1.4)$$ in which, for $t_0\leq t \leq t_0 + a$, $\phi(t)$ and $\psi(t)$ are continuous functions, $\phi(t) \geq 0$ and $\psi(t)\geq 0$; $\delta_1$ and $\delta_3$ are positive constants. Then we have for $t_0\leq t\leq t_0 + a$ $$\phi(t) \leq \delta_3e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ In my book the following proof of this theorem is proof is given: From the estimate $(1.4)$ we derive $$\dfrac{\phi(t)}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq 1$$  multiplication with $\delta_1\psi(t)$ and integration yields $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ so that $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ which produces $$\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3 \leq \delta_3 e^{\delta_1\int_{t_0}^t\psi(s)ds}$$ Applying the estimate $(1.4)$ again, but now to the lefthand side, yields the required inequality. Question: How does the step from  $$\int_{t_0}^t\dfrac{\delta_1\psi(s)\phi(s)ds}{\delta_1\int_{t_0}^t\psi(\tau)\phi(\tau)d\tau + \delta_3}\leq \delta_1\int_{t_0}^t\psi(s)ds$$ to $$\ln(\delta_1\int_{t_0}^t\psi(s)\phi(s)ds + \delta_3) - \ln\delta_3\leq \delta_1\int_{t_0}^t\psi(s)ds$$ work? I think I understand the rest of the proof but I can't figure out the above step. Thanks!",,"['ordinary-differential-equations', 'numerical-methods']"
72,Frobenius Method - Non integer powers of $x$ in differential equation?,Frobenius Method - Non integer powers of  in differential equation?,x,"I am trying to solve an ODE using the Frobenius method. I understand the general process, but I do not understand how you compare coefficients when you have a $x^\frac{1}{2}$ term in the differential equatio. All my searches on google just go to non-integer differences in the indicial equation. For example an equation such as $x^2y''+(\sqrt{x}-K)y = 0 \\$ using the Frobenius series $y = \sum_{n=0}^\infty a_n x^{n+s}$ Substituting this and its derivatives into the differential equation $\sum_{n=0}^\infty a_n x^{n+s+2}+\sum_{n=0}^\infty a_n x^{n+s+1/2}-K\sum_{n=0}^\infty a_n x^{n+s} =0$ Normally I would make the first term start from $n=2$ or make $a_n\rightarrow a_{n-2}$ and do the same for the remaining terms to be able to compare powers of x. How do you proceed when there is a non-integer power of x in the differential Equation?","I am trying to solve an ODE using the Frobenius method. I understand the general process, but I do not understand how you compare coefficients when you have a $x^\frac{1}{2}$ term in the differential equatio. All my searches on google just go to non-integer differences in the indicial equation. For example an equation such as $x^2y''+(\sqrt{x}-K)y = 0 \\$ using the Frobenius series $y = \sum_{n=0}^\infty a_n x^{n+s}$ Substituting this and its derivatives into the differential equation $\sum_{n=0}^\infty a_n x^{n+s+2}+\sum_{n=0}^\infty a_n x^{n+s+1/2}-K\sum_{n=0}^\infty a_n x^{n+s} =0$ Normally I would make the first term start from $n=2$ or make $a_n\rightarrow a_{n-2}$ and do the same for the remaining terms to be able to compare powers of x. How do you proceed when there is a non-integer power of x in the differential Equation?",,"['ordinary-differential-equations', 'frobenius-method']"
73,Interpretation of graph of PDE,Interpretation of graph of PDE,,"1.Suppose we the PDE $u_{x}(x,y)=u_{y}(x,y)$. Does this simply mean we are looking for a function whos partial w.r.t $x$ and $y$ are the same att every point and then we have some additional boundary condition to get some kind of uniqueness? 2.Which in turn means that the graph of the function i.e the surface in $\mathbb{R}^{3}$ have some kind of symmetry? 3.When we have ODEs it is clear that the derivative at each point characterize the function, I have a hard time seeing the same or something similar in this case. What would be the analogy for PDE?","1.Suppose we the PDE $u_{x}(x,y)=u_{y}(x,y)$. Does this simply mean we are looking for a function whos partial w.r.t $x$ and $y$ are the same att every point and then we have some additional boundary condition to get some kind of uniqueness? 2.Which in turn means that the graph of the function i.e the surface in $\mathbb{R}^{3}$ have some kind of symmetry? 3.When we have ODEs it is clear that the derivative at each point characterize the function, I have a hard time seeing the same or something similar in this case. What would be the analogy for PDE?",,['ordinary-differential-equations']
74,Simple singularly perturbed systems,Simple singularly perturbed systems,,"I'm an engineer looking for math insights into an engineering problem. I have a simple first order ODE:  $$\dot x(t)=k\Big(-x(t)+f(t)\Big).$$ As you can see, this system behaves like a low-pass filter with $k$ acting as the filter bandwidth. The higher the filter bandwidth, the higher frequency can pass through the filter. So by hand waving, I thought when the bandwidth is very very high, almost everything passes through the filter and $x(t)$ will behave like $f(t)$. I did many Simulink runs as well as experiments with an RC circuit and a function generator that produces different signals $f(t)$. My hand-waving statement seems to be true. My questions are: 1) How do I rigorously show that as $k\to \infty,$ $x(t)$ behaves like $f(t)$? 2) I did some literature review and figured that the statement is true if $f(t)$ is constant in light of Tikhonov's result [1]. I wanted to dig deeper into the literature, but my background in maths is too limited to understand deeper results. If the statement is not true in general, what is the condition on $f(t)$ for the statement to be true? Any ideas or discussions are greatly appreciated. Thank you! [1] A.N. Tikhonov, Systems of differential equations containing a small parameter multiplying the derivative, Mat. Sb. 31 (1952) pp. 575-586","I'm an engineer looking for math insights into an engineering problem. I have a simple first order ODE:  $$\dot x(t)=k\Big(-x(t)+f(t)\Big).$$ As you can see, this system behaves like a low-pass filter with $k$ acting as the filter bandwidth. The higher the filter bandwidth, the higher frequency can pass through the filter. So by hand waving, I thought when the bandwidth is very very high, almost everything passes through the filter and $x(t)$ will behave like $f(t)$. I did many Simulink runs as well as experiments with an RC circuit and a function generator that produces different signals $f(t)$. My hand-waving statement seems to be true. My questions are: 1) How do I rigorously show that as $k\to \infty,$ $x(t)$ behaves like $f(t)$? 2) I did some literature review and figured that the statement is true if $f(t)$ is constant in light of Tikhonov's result [1]. I wanted to dig deeper into the literature, but my background in maths is too limited to understand deeper results. If the statement is not true in general, what is the condition on $f(t)$ for the statement to be true? Any ideas or discussions are greatly appreciated. Thank you! [1] A.N. Tikhonov, Systems of differential equations containing a small parameter multiplying the derivative, Mat. Sb. 31 (1952) pp. 575-586",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'perturbation-theory']"
75,Discretization for $\partial_tu = \partial_x[g\times\partial_xu]$,Discretization for,\partial_tu = \partial_x[g\times\partial_xu],"In paper Efficient and Reliable Schemes for Nonlinear Diffusion Filtering i have difficulty to understand the following step. We have equation and its discretization in 1D case: $$\partial_tu = \partial_x[g\times\partial_xu]$$ $$\frac{u_i^{k+1}-u_i^k}{\tau} = \sum_{j\in \mathcal N(i)}\frac{g_j^k+g_i^k}{2h^2}(u_j^k-u_i^k), $$ where $\mathcal N(i)$ is the set of the two neighbors of pixel $i$. I don't get how did the authors get this discretization.","In paper Efficient and Reliable Schemes for Nonlinear Diffusion Filtering i have difficulty to understand the following step. We have equation and its discretization in 1D case: $$\partial_tu = \partial_x[g\times\partial_xu]$$ $$\frac{u_i^{k+1}-u_i^k}{\tau} = \sum_{j\in \mathcal N(i)}\frac{g_j^k+g_i^k}{2h^2}(u_j^k-u_i^k), $$ where $\mathcal N(i)$ is the set of the two neighbors of pixel $i$. I don't get how did the authors get this discretization.",,"['ordinary-differential-equations', 'finite-differences']"
76,Find the Differential Equation (Integrating Factor Problem),Find the Differential Equation (Integrating Factor Problem),,"This is #11 of Section 1.5 of the book DEALA , 3E. So, we have only learned how to solve by using the integrating factor. BUT, there is a factor of 'y' on the right-hand side. What is going on? Find the differential: $x\frac{dy}{dx} + y = 3xy$ , y(1) = 0 Add: The solution I looked at after reviewing comments: $x\frac{dy}{dx} = 3xy - y$ $x\frac{dy}{dx} = (3x - 1)y$ $\frac{x}{3x-1}\frac{1}{dx} = \frac{y}{dy}$ $\int\frac{3x-1}{x}dx = \int\frac{1}{y}dy$ $\int3dx - \int\frac{1}{x}dx = \int\frac{1}{y}dy$ $ln|y| = 3x - ln|x| + C$ But, the solution in the back is y is parallel to 0. I think it's a misprint.","This is #11 of Section 1.5 of the book DEALA , 3E. So, we have only learned how to solve by using the integrating factor. BUT, there is a factor of 'y' on the right-hand side. What is going on? Find the differential: , y(1) = 0 Add: The solution I looked at after reviewing comments: But, the solution in the back is y is parallel to 0. I think it's a misprint.",x\frac{dy}{dx} + y = 3xy x\frac{dy}{dx} = 3xy - y x\frac{dy}{dx} = (3x - 1)y \frac{x}{3x-1}\frac{1}{dx} = \frac{y}{dy} \int\frac{3x-1}{x}dx = \int\frac{1}{y}dy \int3dx - \int\frac{1}{x}dx = \int\frac{1}{y}dy ln|y| = 3x - ln|x| + C,['ordinary-differential-equations']
77,"How does Mean Value Theorem guarantee there can't be exactly 3 fixed points, all stable?","How does Mean Value Theorem guarantee there can't be exactly 3 fixed points, all stable?",,"I need to find an equation $\dot x = f(x)$ (where $\dot x$ denotes $\frac{dx}{dt}$) where there are precisely three fixed points, and all of them are stable, or explain why such a situation is not possible. I have seen a solution to this problem where it says that the situation is not possible because ""the mean value theorem guarantees that between any two fixed points of the same type (stable, unstable), there must be a fixed point of the other type"". What I don't understand is why that should be true, or how you could go about proving that. My attempt thus far is: suppose $x=a$, $x=b$ are fixed points (I.e., $f(a)=f(b)=0$), then by the mean value theorem, there exists a point $x=c$ such that $c\in (a,b)$ and $f^{\prime}(c)=\frac{f(b)-f(a)}{b-a}=\frac{0}{b-a}=0$, but all this tells us is that the function goes from increasing to decreasing (or vice versa) on this interval. How does it tell us that if, say, $a$ and $b$ are both stable, that there cannot be another fixed point between them that is also stable? Thank you.","I need to find an equation $\dot x = f(x)$ (where $\dot x$ denotes $\frac{dx}{dt}$) where there are precisely three fixed points, and all of them are stable, or explain why such a situation is not possible. I have seen a solution to this problem where it says that the situation is not possible because ""the mean value theorem guarantees that between any two fixed points of the same type (stable, unstable), there must be a fixed point of the other type"". What I don't understand is why that should be true, or how you could go about proving that. My attempt thus far is: suppose $x=a$, $x=b$ are fixed points (I.e., $f(a)=f(b)=0$), then by the mean value theorem, there exists a point $x=c$ such that $c\in (a,b)$ and $f^{\prime}(c)=\frac{f(b)-f(a)}{b-a}=\frac{0}{b-a}=0$, but all this tells us is that the function goes from increasing to decreasing (or vice versa) on this interval. How does it tell us that if, say, $a$ and $b$ are both stable, that there cannot be another fixed point between them that is also stable? Thank you.",,"['calculus', 'ordinary-differential-equations']"
78,Solving pair of differential equations involving two functions,Solving pair of differential equations involving two functions,,"$$\frac{d^2h(x)}{dx^2} = f(x)\sin(h(x))$$ $$\frac{d^2f(x)}{dx^2} = \cos(h(x))$$ Is it even possible to solve the equations for $f(x)$ and $h(x)$? If so, how would one solve them?","$$\frac{d^2h(x)}{dx^2} = f(x)\sin(h(x))$$ $$\frac{d^2f(x)}{dx^2} = \cos(h(x))$$ Is it even possible to solve the equations for $f(x)$ and $h(x)$? If so, how would one solve them?",,['ordinary-differential-equations']
79,If $f$ is bounded then solutions of ODE are defined for all time?,If  is bounded then solutions of ODE are defined for all time?,f,"Let's say we have the following Cauchy Problem: $x' = f(t,x)$ $x(t_0) = x_0$ Is this statement true? If $\exists$ $K$ such that $||f(t,x)|| \leq K$ for all $(t,x) \in R \times R^n$ $\implies $ The solution is defined for all time","Let's say we have the following Cauchy Problem: $x' = f(t,x)$ $x(t_0) = x_0$ Is this statement true? If $\exists$ $K$ such that $||f(t,x)|| \leq K$ for all $(t,x) \in R \times R^n$ $\implies $ The solution is defined for all time",,['ordinary-differential-equations']
80,Solving the heat equation with robin boundary conditions,Solving the heat equation with robin boundary conditions,,"I have a coupled non-dimensional diffusion system in $v(z,\tau)$ , formulated by the following equations \begin{align} \frac{\partial v}{\partial \tau} &= \Delta\frac{\partial^2 v}{\partial z^2}, % \qquad &\text{for}\ z\in[0,1],\ \tau>0 \\  %%% \frac{\partial v}{\partial z} &= Ev, % \qquad &\text{for}\ z=0,\ \tau>0,\\  %%% \frac{\partial v}{\partial z} &= -D v, % \qquad &\text{for}\ z=1,\ \tau>0 \end{align} where $\Delta,E,D>0$ We next proceed with separation of variables, let \begin{align} v = Z(z)T(\tau) \end{align} Substitution yields the following \begin{align} \frac{1}{\Delta}\frac{\dot{T}}{T} &= \frac{Z''}{Z} = -\lambda^2 \end{align} Therefore we find \begin{align} T &\propto \exp{\left(-\Delta\lambda^2\tau\right)},\\ Z &= a \cos(\lambda z) + b\sin(\lambda z),\\ Z' &= \lambda \left( b\cos(\lambda z) -a \sin(\lambda z) \right) \end{align} WLOG we may set $a=1$ , as we will later take a linear superposition of these solution functions. Therefore we have \begin{align} Z &= \cos(\lambda z) + b\sin(\lambda z),\\ Z' &= \lambda \left( b\cos(\lambda z) - \sin(\lambda z) \right) \end{align} Therefore via our boundary condition at $z=0$ we find \begin{align} \lambda b &= E \quad\Rightarrow\quad b = \frac{E}{\lambda} \end{align} and via our second \begin{align} \lambda \left( \frac{E}{\lambda}\cos(\lambda) - \sin(\lambda) \right) &= -D\left(\cos(\lambda) + \frac{E}{\lambda}\sin(\lambda)\right)\\ %%% \Rightarrow\quad E\lambda\cos(\lambda) - \lambda^2\sin(\lambda) &= -D\cos(\lambda) - ED\sin(\lambda)\\ %%% \Rightarrow\quad \left(E\lambda+D\right)\cos(\lambda) &= \left( \lambda^2- ED\right)\sin(\lambda)\\ %%% \Rightarrow\quad \tan(\lambda) &= \frac{E\lambda+D}{\lambda^2- ED} \end{align} This has countably infinite solutions $\lambda_i$ for $i\in\mathbb{N}$ . Therefore we have the following solution for $v(z,\tau)$ \begin{align} v(z,\tau) &= \sum_{i=1}^\infty C_n \left( \cos(\lambda_i z) + \left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z) \right) \exp{\left(-\Delta\lambda_i^2\tau\right)} \end{align} Therefore at $\tau=0$ \begin{align} v(z,0) &= \sum_{i=1}^\infty C_i \left( \cos(\lambda_i z) + \left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z) \right) = 1 \end{align} How can I find $C_i$ ?. EDIT: If we define $Z_i(z)$ as follows \begin{align} Z_i(z) =  \cos(\lambda_i z) + \left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z) \end{align} then am I correct in thinking we use the following relation to find $C_i$ ? \begin{align} \int_0^1 Z_i(z)Z_j(z) \text{d}z = c_i\delta_{ij} \end{align} I'm not sure if this is the case, see this link . Does this mean my spatial basis is not orthogonal?","I have a coupled non-dimensional diffusion system in , formulated by the following equations where We next proceed with separation of variables, let Substitution yields the following Therefore we find WLOG we may set , as we will later take a linear superposition of these solution functions. Therefore we have Therefore via our boundary condition at we find and via our second This has countably infinite solutions for . Therefore we have the following solution for Therefore at How can I find ?. EDIT: If we define as follows then am I correct in thinking we use the following relation to find ? I'm not sure if this is the case, see this link . Does this mean my spatial basis is not orthogonal?","v(z,\tau) \begin{align}
\frac{\partial v}{\partial \tau} &= \Delta\frac{\partial^2 v}{\partial z^2},
%
\qquad &\text{for}\ z\in[0,1],\ \tau>0 \\ 
%%%
\frac{\partial v}{\partial z} &= Ev,
%
\qquad &\text{for}\ z=0,\ \tau>0,\\ 
%%%
\frac{\partial v}{\partial z} &= -D v,
%
\qquad &\text{for}\ z=1,\ \tau>0
\end{align} \Delta,E,D>0 \begin{align}
v = Z(z)T(\tau)
\end{align} \begin{align}
\frac{1}{\Delta}\frac{\dot{T}}{T} &= \frac{Z''}{Z} = -\lambda^2
\end{align} \begin{align}
T &\propto \exp{\left(-\Delta\lambda^2\tau\right)},\\
Z &= a \cos(\lambda z) + b\sin(\lambda z),\\
Z' &= \lambda \left( b\cos(\lambda z) -a \sin(\lambda z) \right)
\end{align} a=1 \begin{align}
Z &= \cos(\lambda z) + b\sin(\lambda z),\\
Z' &= \lambda \left( b\cos(\lambda z) - \sin(\lambda z) \right)
\end{align} z=0 \begin{align}
\lambda b &= E
\quad\Rightarrow\quad
b = \frac{E}{\lambda}
\end{align} \begin{align}
\lambda \left( \frac{E}{\lambda}\cos(\lambda) - \sin(\lambda) \right) &= -D\left(\cos(\lambda) + \frac{E}{\lambda}\sin(\lambda)\right)\\
%%%
\Rightarrow\quad
E\lambda\cos(\lambda) - \lambda^2\sin(\lambda) &= -D\cos(\lambda) - ED\sin(\lambda)\\
%%%
\Rightarrow\quad
\left(E\lambda+D\right)\cos(\lambda)
&=
\left( \lambda^2- ED\right)\sin(\lambda)\\
%%%
\Rightarrow\quad
\tan(\lambda)
&=
\frac{E\lambda+D}{\lambda^2- ED}
\end{align} \lambda_i i\in\mathbb{N} v(z,\tau) \begin{align}
v(z,\tau) &=
\sum_{i=1}^\infty C_n
\left(
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\right)
\exp{\left(-\Delta\lambda_i^2\tau\right)}
\end{align} \tau=0 \begin{align}
v(z,0) &=
\sum_{i=1}^\infty C_i
\left(
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\right)
= 1
\end{align} C_i Z_i(z) \begin{align}
Z_i(z) = 
\cos(\lambda_i z)
+
\left(\frac{E}{\lambda_i}\right)\sin(\lambda_i z)
\end{align} C_i \begin{align}
\int_0^1 Z_i(z)Z_j(z) \text{d}z = c_i\delta_{ij}
\end{align}","['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-modeling', 'heat-equation', 'linear-pde']"
81,nonlinear fourth-order differential equation type,nonlinear fourth-order differential equation type,,"Does anybody know how to reduce such nonlinear fourth-order differential equation to well-known type (like Ricatti, Painlevé, sine-Gordon etc)? $\frac{{{d^4}\phi }}{{d{x^4}}} + q\frac{{{d^2}\phi }}{{d{x^2}}} - {\omega ^2}\sin\phi  = 0$","Does anybody know how to reduce such nonlinear fourth-order differential equation to well-known type (like Ricatti, Painlevé, sine-Gordon etc)? $\frac{{{d^4}\phi }}{{d{x^4}}} + q\frac{{{d^2}\phi }}{{d{x^2}}} - {\omega ^2}\sin\phi  = 0$",,['ordinary-differential-equations']
82,Change of variables in a differential equation,Change of variables in a differential equation,,"I have the following differential equation: $$\frac{d^{2}y}{dx^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, x).$$ $J$ is the Bessel function of first type. In order to be able to solve it exactly (at least according to Mathematica), I need to change the variables so that the two terms on the LHS have equal coefficients. So, I try $x = b t$, and get $$\frac{1}{b^{2}}\frac{d^{2}y}{dt^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, bt)$$ Is this correct? Is this how I change variables in a differential equation, especially in the inhomogeneous term? When I find a solution, what variable is it in exactly? How do I recover the solution in terms of $x$? Thanks.","I have the following differential equation: $$\frac{d^{2}y}{dx^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, x).$$ $J$ is the Bessel function of first type. In order to be able to solve it exactly (at least according to Mathematica), I need to change the variables so that the two terms on the LHS have equal coefficients. So, I try $x = b t$, and get $$\frac{1}{b^{2}}\frac{d^{2}y}{dt^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, bt)$$ Is this correct? Is this how I change variables in a differential equation, especially in the inhomogeneous term? When I find a solution, what variable is it in exactly? How do I recover the solution in terms of $x$? Thanks.",,['ordinary-differential-equations']
83,differential equation $y(t)[y''(t)+2\lambda y'(t)]=(y'(t))^2$,differential equation,y(t)[y''(t)+2\lambda y'(t)]=(y'(t))^2,Can anyone please help me solve the following differential equation $$y(t)[y''(t)+2\lambda y'(t)]=(y'(t))^2$$ with $y(0)=0$. If $\lambda=0$ then clearly $y(t)=Ge^{\alpha t}$. But what if $\lambda\not=0?$,Can anyone please help me solve the following differential equation $$y(t)[y''(t)+2\lambda y'(t)]=(y'(t))^2$$ with $y(0)=0$. If $\lambda=0$ then clearly $y(t)=Ge^{\alpha t}$. But what if $\lambda\not=0?$,,['ordinary-differential-equations']
84,Finding the differential equation given certain solutions,Finding the differential equation given certain solutions,,"I'm stuck at this exercise of my notes: Find the differential equation that has the solutions: $$\phi_1= e^{2t} (13\cos{t}, −26\sin{ t}, −26 \sin {t})$$   $$\phi_2= 7e^{2t}(−2 \cos{t} − 3 \sin {t}, −6 \cos{t} + 4 \sin {t}, −6 \cos{t} + 4 \sin {t}) + 2e^{−3t}(7, 8, 34)$$   $$\phi_3=e^{2t}(\sin {t}, 2 \cos{t}, 2 \cos{t})$$ It's the first exercise on my notes like this. How do we usually attack this kind of exercises? What's the procedure to follow?","I'm stuck at this exercise of my notes: Find the differential equation that has the solutions: $$\phi_1= e^{2t} (13\cos{t}, −26\sin{ t}, −26 \sin {t})$$   $$\phi_2= 7e^{2t}(−2 \cos{t} − 3 \sin {t}, −6 \cos{t} + 4 \sin {t}, −6 \cos{t} + 4 \sin {t}) + 2e^{−3t}(7, 8, 34)$$   $$\phi_3=e^{2t}(\sin {t}, 2 \cos{t}, 2 \cos{t})$$ It's the first exercise on my notes like this. How do we usually attack this kind of exercises? What's the procedure to follow?",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'curves']"
85,Is there a function $f$ with $f'(x) = e^x * f(x)$? [closed],Is there a function  with ? [closed],f f'(x) = e^x * f(x),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Edit : I'm ''derivation'' and ''integration'' beginner. So I don't have any techniques to solve equations like that.This why I think there have to be a clever trick to solve this. a), b), c) was okay. You can see my solution below. I want to emphasize that I don't want to see the solution. Maybe someone can show me the techniques for $b)$? I found a $f$ in b), c) only with try and error. If you show me the trick, I would  try again $c)$ and $d)$ by my own. Are there any functions $f$ with following equations: $ a) f'(x) =e^x + f(x)   $  a) is clear now. Thank you for all of your help $ b) f'(x) =e^x \cdot f(x) $ solved BUT only with try and error. what is the beginner technique ( like I said: Im a beginner ) to find $f$? $ c) f'(x) =e^x \cdot f(x)^2 $ solved BUT only with try and error. what is the beginner technique to find $f$? $ d) f'(x) =e^{f(x)} $ Remark : $f$ is not a constant function. Update : $a ) f:= x \cdot e^x $. Then we use the multiplication rule for derivation. $f'$ = $x \cdot e^x + 1 \cdot e^x = x \cdot e^x + e^x$. Update2 b) $f:= c \cdot e^{e^x}.$ Then we can use the chain rule: $f' = c \cdot e^{e^x} \cdot e^x$. Update3 c) $f:= - \frac{1}{e^x +c}$. Then use chain rule and quotient rule: $ f' = e^x \cdot  \frac{1}{(e^x +c)^2}$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Edit : I'm ''derivation'' and ''integration'' beginner. So I don't have any techniques to solve equations like that.This why I think there have to be a clever trick to solve this. a), b), c) was okay. You can see my solution below. I want to emphasize that I don't want to see the solution. Maybe someone can show me the techniques for $b)$? I found a $f$ in b), c) only with try and error. If you show me the trick, I would  try again $c)$ and $d)$ by my own. Are there any functions $f$ with following equations: $ a) f'(x) =e^x + f(x)   $  a) is clear now. Thank you for all of your help $ b) f'(x) =e^x \cdot f(x) $ solved BUT only with try and error. what is the beginner technique ( like I said: Im a beginner ) to find $f$? $ c) f'(x) =e^x \cdot f(x)^2 $ solved BUT only with try and error. what is the beginner technique to find $f$? $ d) f'(x) =e^{f(x)} $ Remark : $f$ is not a constant function. Update : $a ) f:= x \cdot e^x $. Then we use the multiplication rule for derivation. $f'$ = $x \cdot e^x + 1 \cdot e^x = x \cdot e^x + e^x$. Update2 b) $f:= c \cdot e^{e^x}.$ Then we can use the chain rule: $f' = c \cdot e^{e^x} \cdot e^x$. Update3 c) $f:= - \frac{1}{e^x +c}$. Then use chain rule and quotient rule: $ f' = e^x \cdot  \frac{1}{(e^x +c)^2}$",,"['ordinary-differential-equations', 'derivatives', 'exponential-function']"
86,Analytical Solution for a Second-Order Nonlinear Differential Equation,Analytical Solution for a Second-Order Nonlinear Differential Equation,,"In the frame of semiconductor physics, I find myself in front of a smart but difficult second-order nonlinear ODE : \begin{equation} \tag{$E$-ODE} \label{eq:E-ODE} \boxed{ \phi_T \frac{\mathrm{d}^2 E}{\mathrm{d}{x}^2} + E \frac{\mathrm{d}{E}}{\mathrm{d}{x}} - \frac{e N_D}{\varepsilon} E = \kappa \text{.} } \end{equation} I prefer to consider general boundary conditons, as I am looking for an general analytical solution. Equation can be rewritten as \begin{equation*} \frac{\mathrm{d}^2 E}{\mathrm{d}{x}^2} = -\frac{1}{\phi_T} E \frac{\mathrm{d}{E}}{\mathrm{d}{x}} +  \frac{e N_D}{\varepsilon \phi_T} E + \frac{\kappa}{\phi_T} \text{.} \end{equation*} Setting \begin{equation*} \begin{aligned} & y \left( x \right) \equiv E \left( x \right) \\ & a \equiv -\frac{1}{\phi_T} \\ & b \equiv \frac{e N_D}{\varepsilon \phi_T} \\ & c \equiv \frac{\kappa}{\phi_T} \end{aligned} \end{equation*} \begin{equation*} \frac{\mathrm{d}^2 y}{\mathrm{d}{x}^2} = a y \frac{\mathrm{d}{y}}{\mathrm{d}{x}} + b y + c \end{equation*} Using shortened notations for derivatives: \begin{equation} \tag{$y$-ODE} \label{eq:y-ODE} \boxed{ y'' = a y y' + b y + c \text{.} } \end{equation} Here are my attempts : it is a second-order nonlinear differential equation; it is an autonomous equation : $y'' = F \left( y, y' \right)$; it is a Liénard equation :  \begin{equation} \tag{Liénard} \label{eq:Liénard} y'' + f \left( y \right) y' + g \left( y \right) = 0 \text{,} \end{equation} with $f \left( y \right) = - a y$  and  $g \left( y \right) = - b y - c$. with the substitution $w = y'$, it is an Abel equation of the second kind : \begin{equation} \tag{Abel} \label{eq:Abel} w w'_{y} + f \left( y \right) w + g \left( y \right) = 0 \end{equation} I have tried searching in dedicated textbooks, for instance in Polyanin Handbook of Exact Solutions for Ordinary Differential Equations , but I have got the impression that my ODE has no analytical solution... What do you think? Does one among you see or know one way to solve this equation? Under request, I might provide boundary conditions for some specific problem. Remark : I am familiar with numerical solving of ODE's, but this is not what I am looking for here. Many thanks forward, Léopold","In the frame of semiconductor physics, I find myself in front of a smart but difficult second-order nonlinear ODE : \begin{equation} \tag{$E$-ODE} \label{eq:E-ODE} \boxed{ \phi_T \frac{\mathrm{d}^2 E}{\mathrm{d}{x}^2} + E \frac{\mathrm{d}{E}}{\mathrm{d}{x}} - \frac{e N_D}{\varepsilon} E = \kappa \text{.} } \end{equation} I prefer to consider general boundary conditons, as I am looking for an general analytical solution. Equation can be rewritten as \begin{equation*} \frac{\mathrm{d}^2 E}{\mathrm{d}{x}^2} = -\frac{1}{\phi_T} E \frac{\mathrm{d}{E}}{\mathrm{d}{x}} +  \frac{e N_D}{\varepsilon \phi_T} E + \frac{\kappa}{\phi_T} \text{.} \end{equation*} Setting \begin{equation*} \begin{aligned} & y \left( x \right) \equiv E \left( x \right) \\ & a \equiv -\frac{1}{\phi_T} \\ & b \equiv \frac{e N_D}{\varepsilon \phi_T} \\ & c \equiv \frac{\kappa}{\phi_T} \end{aligned} \end{equation*} \begin{equation*} \frac{\mathrm{d}^2 y}{\mathrm{d}{x}^2} = a y \frac{\mathrm{d}{y}}{\mathrm{d}{x}} + b y + c \end{equation*} Using shortened notations for derivatives: \begin{equation} \tag{$y$-ODE} \label{eq:y-ODE} \boxed{ y'' = a y y' + b y + c \text{.} } \end{equation} Here are my attempts : it is a second-order nonlinear differential equation; it is an autonomous equation : $y'' = F \left( y, y' \right)$; it is a Liénard equation :  \begin{equation} \tag{Liénard} \label{eq:Liénard} y'' + f \left( y \right) y' + g \left( y \right) = 0 \text{,} \end{equation} with $f \left( y \right) = - a y$  and  $g \left( y \right) = - b y - c$. with the substitution $w = y'$, it is an Abel equation of the second kind : \begin{equation} \tag{Abel} \label{eq:Abel} w w'_{y} + f \left( y \right) w + g \left( y \right) = 0 \end{equation} I have tried searching in dedicated textbooks, for instance in Polyanin Handbook of Exact Solutions for Ordinary Differential Equations , but I have got the impression that my ODE has no analytical solution... What do you think? Does one among you see or know one way to solve this equation? Under request, I might provide boundary conditions for some specific problem. Remark : I am familiar with numerical solving of ODE's, but this is not what I am looking for here. Many thanks forward, Léopold",,['ordinary-differential-equations']
87,Power series solution : can we have a unique series solution(around $x=0$) for an IVP having $x=0$ a singular point?,Power series solution : can we have a unique series solution(around ) for an IVP having  a singular point?,x=0 x=0,"Studying power series and Frobenius method , I found a theorem stating that there is a unique Maclaurin series $y(x)$ satisfying the IVP  $$y''+a(x)y'+b(x)y=0 ,\ \ y(0)=\alpha \ \ ,y'(0)=\beta$$ provided $a(x)$ and $b(x)$ are analytic at $x=0$ . So does this mean that if we have $x=0$ is a regular singular point , we do not have always a unique solution for the IVP ? or not always , we may have a unique solution or not and we just do not guarantee the unique solution ? For example , the problem $$xy''-xy'+y=e^{x}\ \ \ ,y(0)=1\ \ \ ,y'(0)=2$$ The general solution ( particular and homogeneous solutions) is  $$y(x)=(e^x-x)+c_1x+c_2(-1+x\ln(x)+\frac{x^2}{2}+\frac{x^3}{12}+...)$$ (Note : the details of the solution is here Solving this non homogeneous IVP using power series ) Applying initial conditions , we find that $c_1=2$ , $c_2=0$ (unique values for the constants although $x=0$ is singular point!) However , another problem  $$x^2y''-3xy'+3y=0,\ \ y(0)=0\ \ ,y'(0)=1$$ has $x=0$ a regular singular point , and has solution $$y=c_1x+c_2x^3$$ Applying intial conditions , we find that $c_1=1$ but $c_2$ has infinite values.","Studying power series and Frobenius method , I found a theorem stating that there is a unique Maclaurin series $y(x)$ satisfying the IVP  $$y''+a(x)y'+b(x)y=0 ,\ \ y(0)=\alpha \ \ ,y'(0)=\beta$$ provided $a(x)$ and $b(x)$ are analytic at $x=0$ . So does this mean that if we have $x=0$ is a regular singular point , we do not have always a unique solution for the IVP ? or not always , we may have a unique solution or not and we just do not guarantee the unique solution ? For example , the problem $$xy''-xy'+y=e^{x}\ \ \ ,y(0)=1\ \ \ ,y'(0)=2$$ The general solution ( particular and homogeneous solutions) is  $$y(x)=(e^x-x)+c_1x+c_2(-1+x\ln(x)+\frac{x^2}{2}+\frac{x^3}{12}+...)$$ (Note : the details of the solution is here Solving this non homogeneous IVP using power series ) Applying initial conditions , we find that $c_1=2$ , $c_2=0$ (unique values for the constants although $x=0$ is singular point!) However , another problem  $$x^2y''-3xy'+3y=0,\ \ y(0)=0\ \ ,y'(0)=1$$ has $x=0$ a regular singular point , and has solution $$y=c_1x+c_2x^3$$ Applying intial conditions , we find that $c_1=1$ but $c_2$ has infinite values.",,"['ordinary-differential-equations', 'power-series']"
88,"""Limit form"" of non-autonomous linear ODE","""Limit form"" of non-autonomous linear ODE",,"Let us consider ODE $$ f^{(n)} + a_{n-1}(t) f^{(n-1)} + \ldots + a_1(t) f + a_0(t) = 0 $$ with smooth coefficients s.t. $a_i(t) \to b_i \in \mathbb R$ when $t \to +\infty$. Is it true that fundamental solutions of the initial equation tend to solutions of ""limit equation"" with constant coefficients? $$ f^{(n)} + b_{n-1} f^{(n-1)} + \ldots + b_1 f + b_0 = 0. $$ I.e. for any solution $f$ of the first equation there exists solution $\tilde f$ of the second equation such that $|f(t) - \tilde f(t)| \to 0$ when $t \to +\infty$.","Let us consider ODE $$ f^{(n)} + a_{n-1}(t) f^{(n-1)} + \ldots + a_1(t) f + a_0(t) = 0 $$ with smooth coefficients s.t. $a_i(t) \to b_i \in \mathbb R$ when $t \to +\infty$. Is it true that fundamental solutions of the initial equation tend to solutions of ""limit equation"" with constant coefficients? $$ f^{(n)} + b_{n-1} f^{(n-1)} + \ldots + b_1 f + b_0 = 0. $$ I.e. for any solution $f$ of the first equation there exists solution $\tilde f$ of the second equation such that $|f(t) - \tilde f(t)| \to 0$ when $t \to +\infty$.",,['ordinary-differential-equations']
89,General Solution of an ODE,General Solution of an ODE,,Let $f$ be a continuous function. Can anyone please help me out to find the solution of the ODE:  $$y\left( \frac{dy}{dx} + a y + b\right) = f(x)$$,Let $f$ be a continuous function. Can anyone please help me out to find the solution of the ODE:  $$y\left( \frac{dy}{dx} + a y + b\right) = f(x)$$,,['ordinary-differential-equations']
90,Separation of variables for partial differential equations - Shallow Wave Equation,Separation of variables for partial differential equations - Shallow Wave Equation,,"Question: I've tried countless times to prove this problem, but I keep getting hung up on the indices for derivatives. Could somebody please point me in the right direction? Given Solution","Question: I've tried countless times to prove this problem, but I keep getting hung up on the indices for derivatives. Could somebody please point me in the right direction? Given Solution",,"['ordinary-differential-equations', 'partial-derivative']"
91,Determining the first $3$ nonzero terms in each of two linearly independent solutions,Determining the first  nonzero terms in each of two linearly independent solutions,3,"In preparation for an upcoming exam I have came across a question that I am a little confused with. Given the following different equation, $$xy''+y'-y = 0$$ I am trying to find the first $3$ nonzero terms in each of two linearly independent solutions. I have determined the indicial equation to be, $$r^2 = 0$$ Therefore the roots are, $$r_1=0, r_2=0$$ So the roots are separated by a integer I have calculated the recurrence solution to be, $$a_n = \frac{a_{n-1}}{(n+r)^2}$$ So therefore the first solution would be the following, $$y_1(x) = 1+ x + \frac{1}{4}x^2 + \frac{1}{36}x^3...$$ However it is determining the second solution where I am having problem, so I am looking for some help with showing how this answer is calculated, thanks! The solution to the second solution is, $$y_2(x) = y_1(x)\ln x -2x - \frac{3}{4}x^2-\frac{11}{108}x^3$$","In preparation for an upcoming exam I have came across a question that I am a little confused with. Given the following different equation, $$xy''+y'-y = 0$$ I am trying to find the first $3$ nonzero terms in each of two linearly independent solutions. I have determined the indicial equation to be, $$r^2 = 0$$ Therefore the roots are, $$r_1=0, r_2=0$$ So the roots are separated by a integer I have calculated the recurrence solution to be, $$a_n = \frac{a_{n-1}}{(n+r)^2}$$ So therefore the first solution would be the following, $$y_1(x) = 1+ x + \frac{1}{4}x^2 + \frac{1}{36}x^3...$$ However it is determining the second solution where I am having problem, so I am looking for some help with showing how this answer is calculated, thanks! The solution to the second solution is, $$y_2(x) = y_1(x)\ln x -2x - \frac{3}{4}x^2-\frac{11}{108}x^3$$",,"['sequences-and-series', 'ordinary-differential-equations']"
92,Viscosity solution of Hamilton Jacobi equation,Viscosity solution of Hamilton Jacobi equation,,"In the book Partial Differential Equations by L.C. Evans there are two solution concepts for Hamilton Jacobi equations A. VISCOSITY SOLUTION (which is defined in chapter 10) B. WEAK SOLUTION (which is defined in chapter 3) I have the following doubts.... 1)Is the visocotiy solution lipschitz continuous? if so does it satatisfy the equation pointwise a.e. 2)Are these two solutions same? if so what is the justification? Definition of Viscosity solution(L.C.Evans, PDE, Chapter 10) : Assume that $u$ is bounded and uniformly continuous on $R^n \times [0,T]$,for each $T\geq 0$. We say that u is viscosity solution of the initial value problem $u_t+H(Du,x)=0$ in $R^n \times (0,\infty)$, u=g on $R^n \times \{t=0\}$  provided A)$u=g$ on $R^n \times (0,\infty)$, and B)for each v $\in C^ \infty (R^n \times (0,\infty))$, if $u-v$ has a local maximum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\leq 0$ and if if $u-v$ has a local minimum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\geq 0$","In the book Partial Differential Equations by L.C. Evans there are two solution concepts for Hamilton Jacobi equations A. VISCOSITY SOLUTION (which is defined in chapter 10) B. WEAK SOLUTION (which is defined in chapter 3) I have the following doubts.... 1)Is the visocotiy solution lipschitz continuous? if so does it satatisfy the equation pointwise a.e. 2)Are these two solutions same? if so what is the justification? Definition of Viscosity solution(L.C.Evans, PDE, Chapter 10) : Assume that $u$ is bounded and uniformly continuous on $R^n \times [0,T]$,for each $T\geq 0$. We say that u is viscosity solution of the initial value problem $u_t+H(Du,x)=0$ in $R^n \times (0,\infty)$, u=g on $R^n \times \{t=0\}$  provided A)$u=g$ on $R^n \times (0,\infty)$, and B)for each v $\in C^ \infty (R^n \times (0,\infty))$, if $u-v$ has a local maximum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\leq 0$ and if if $u-v$ has a local minimum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\geq 0$",,"['ordinary-differential-equations', 'partial-differential-equations', 'hamilton-jacobi-equation']"
93,Show that $\sin x$ lies between $x-x^3/6$ and $x \;$ $\forall x \in R$ [duplicate],Show that  lies between  and   [duplicate],\sin x x-x^3/6 x \; \forall x \in R,This question already has answers here : Proving that $x - \frac{x^3}{3!} < \sin x < x$ for all $x>0$ (5 answers) Closed 6 years ago . Show that $\sin x$ lies between $x-x^3/6$ and $x \;$ $\forall x \in R$ I am getting: $$\sin(x) = x - \frac{x^3}{3!} + R_4(x)$$ where $R_4(x) = \frac{\cos(c)x^5}{5!}$ for some $c$ between $0$ and $x$ I want to prove $R_4(x)\geq 0$ to arrive at the result $x-x^3/3 \leq \sin(x)$. for:$$0 \leq x \leq \pi/2 \Rightarrow 0<c<\pi/2 \Rightarrow R_4(x)\geq 0$$ but for: $$-\pi/2 \leq x < 0 \Rightarrow -\pi/2 < c < 0  \Rightarrow R_4(x) < 0$$ How can I proceed with this ? There are many cases that I need to check,This question already has answers here : Proving that $x - \frac{x^3}{3!} < \sin x < x$ for all $x>0$ (5 answers) Closed 6 years ago . Show that $\sin x$ lies between $x-x^3/6$ and $x \;$ $\forall x \in R$ I am getting: $$\sin(x) = x - \frac{x^3}{3!} + R_4(x)$$ where $R_4(x) = \frac{\cos(c)x^5}{5!}$ for some $c$ between $0$ and $x$ I want to prove $R_4(x)\geq 0$ to arrive at the result $x-x^3/3 \leq \sin(x)$. for:$$0 \leq x \leq \pi/2 \Rightarrow 0<c<\pi/2 \Rightarrow R_4(x)\geq 0$$ but for: $$-\pi/2 \leq x < 0 \Rightarrow -\pi/2 < c < 0  \Rightarrow R_4(x) < 0$$ How can I proceed with this ? There are many cases that I need to check,,"['real-analysis', 'ordinary-differential-equations']"
94,Differentiability of fixed point,Differentiability of fixed point,,I am trying to review some past hw problems. I was never able to figure out how to do this problem. Can anyone help me out at all? Thanks.,I am trying to review some past hw problems. I was never able to figure out how to do this problem. Can anyone help me out at all? Thanks.,,['ordinary-differential-equations']
95,Solving $\frac{d^2 \theta}{d x^2} - m^2\theta = 0$ using the Ritz method,Solving  using the Ritz method,\frac{d^2 \theta}{d x^2} - m^2\theta = 0,"I'm trying to solve the following ODE using the Ritz method: $$\frac{d^2 \theta}{d x^2} - m^2\theta = 0$$ With the boundary conditions $$\frac{d\theta}{dx}\Bigg{|}_{x=0} = 0$$ $$\theta(1) = \theta_0$$ I'm trying to follow the book ""Conduction Heat Transfer"" by Vedat S. Arpaci (chap. 8). So this is what I did so far: 1) Transform the problem into a variational problem: $$ \int_o^1 \Bigg{(}\frac{d^2 \theta}{d x^2} - m^2\theta\Bigg{)} \delta\theta\ dx = 0 $$ 2) (Ritz Method) Select a convergent sequence of functions such that $$ y(x) = \sum_{n=0}^Na_n\phi_n(x) $$ I choose the following $y(x)$: $$y(x) = \sum_{n=0}^N\theta_0(1 - (1 - x^2)(a_0 + a_1x^2 + a_2x^4 + ...))$$ Which yields the approximation $$ y(x) = \phi_0(x) = \theta_0(1 - (1 - x^2)a_0) $$ 3) Apply (2) in (1) in order to obtain $a_0$ $$ \int_o^1 \Bigg{(}\frac{d^2 \theta}{d x^2} - m^2\theta\Bigg{)} \delta\theta\ dx = $$ $$ \theta_0\int_o^1 \Bigg{(}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{)} \delta\theta\ dx = 0 $$ Where $\delta F$ is the variation of the functional $F(x, y, y^\prime)$, that is, $$ \delta F = \frac{\partial F}{\partial y}\delta y + \frac{\partial F}{\partial y^\prime}\delta y^\prime $$ And this is where I got stuck. The book suggest the following: $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \delta\theta\ dx = $$ $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \Bigg{[} -(1-x^2)\delta a_0 \Bigg{]} dx = 0 $$ But I don't understand the application of $\delta \theta$ in this case, because I though $a_0$ was meant to be a constant (Should it really?), then $\theta$ is not a functional in relation to it. Assuming $a_0$ is a function, and the step above is correct (Which I don't understand why), I would also be stuck in the next step: $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \Bigg{[} -(1-x^2)\delta a_0 \Bigg{]} dx = 0 $$ For this step, the book suggests to use the following identity, which I was unable to apply here: $$ \int_0^l(l^2-x^2)^mdx = \frac{(2m)!!}{(2m+1)!!}l^{2m+1} $$ I would be grateful for any suggestion at this point.","I'm trying to solve the following ODE using the Ritz method: $$\frac{d^2 \theta}{d x^2} - m^2\theta = 0$$ With the boundary conditions $$\frac{d\theta}{dx}\Bigg{|}_{x=0} = 0$$ $$\theta(1) = \theta_0$$ I'm trying to follow the book ""Conduction Heat Transfer"" by Vedat S. Arpaci (chap. 8). So this is what I did so far: 1) Transform the problem into a variational problem: $$ \int_o^1 \Bigg{(}\frac{d^2 \theta}{d x^2} - m^2\theta\Bigg{)} \delta\theta\ dx = 0 $$ 2) (Ritz Method) Select a convergent sequence of functions such that $$ y(x) = \sum_{n=0}^Na_n\phi_n(x) $$ I choose the following $y(x)$: $$y(x) = \sum_{n=0}^N\theta_0(1 - (1 - x^2)(a_0 + a_1x^2 + a_2x^4 + ...))$$ Which yields the approximation $$ y(x) = \phi_0(x) = \theta_0(1 - (1 - x^2)a_0) $$ 3) Apply (2) in (1) in order to obtain $a_0$ $$ \int_o^1 \Bigg{(}\frac{d^2 \theta}{d x^2} - m^2\theta\Bigg{)} \delta\theta\ dx = $$ $$ \theta_0\int_o^1 \Bigg{(}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{)} \delta\theta\ dx = 0 $$ Where $\delta F$ is the variation of the functional $F(x, y, y^\prime)$, that is, $$ \delta F = \frac{\partial F}{\partial y}\delta y + \frac{\partial F}{\partial y^\prime}\delta y^\prime $$ And this is where I got stuck. The book suggest the following: $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \delta\theta\ dx = $$ $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \Bigg{[} -(1-x^2)\delta a_0 \Bigg{]} dx = 0 $$ But I don't understand the application of $\delta \theta$ in this case, because I though $a_0$ was meant to be a constant (Should it really?), then $\theta$ is not a functional in relation to it. Assuming $a_0$ is a function, and the step above is correct (Which I don't understand why), I would also be stuck in the next step: $$ \theta_0\int_o^1 \Bigg{[}2a_0 - m^2(1 - (1-x^2)a_0) \Bigg{]} \Bigg{[} -(1-x^2)\delta a_0 \Bigg{]} dx = 0 $$ For this step, the book suggests to use the following identity, which I was unable to apply here: $$ \int_0^l(l^2-x^2)^mdx = \frac{(2m)!!}{(2m+1)!!}l^{2m+1} $$ I would be grateful for any suggestion at this point.",,"['ordinary-differential-equations', 'mathematical-physics', 'finite-element-method', 'variational-analysis']"
96,Show that there exists a non-constant periodic trajectory for the system of ODEs,Show that there exists a non-constant periodic trajectory for the system of ODEs,,I am having a hard time finding information online about how to show that there exists a non-constant periodic trajectory for the system of ODEs. For example if the system is $$x'=1-4x+x^2y$$ $$y'=3x-x^2y$$ Does anyone know where I can read about this type of problem or what steps to take to find the trajectories? Thanks.,I am having a hard time finding information online about how to show that there exists a non-constant periodic trajectory for the system of ODEs. For example if the system is $$x'=1-4x+x^2y$$ $$y'=3x-x^2y$$ Does anyone know where I can read about this type of problem or what steps to take to find the trajectories? Thanks.,,['ordinary-differential-equations']
97,What does it mean for a vector function to be twice differentiable?,What does it mean for a vector function to be twice differentiable?,,"Let $I \subseteq \mathbb{R}$ be an open interval and $\vec{f}: \mathbb{R}^2 \to \mathbb{R}^2, \vec{y}: I \to \mathbb{R}^2$ be functions such that the system $\vec{y}' = \vec{f} \circ \vec{y}$ has an isolated critical point at $\vec{0}$. Then we say the above system is locally linear at $\vec{0}$ if there is a linear map $A: \mathbb{R}^2 \to \mathbb{R}^2$ and a function $\vec{g}: \mathbb{R}^2 \to \mathbb{R}^2$ such that $$\vec{y}' = A\vec{y} + \vec{g} \circ \vec{y}$$ and $$\lim_{\vec{\epsilon} \to \vec{0}} \frac{||\vec{g}(\vec{\epsilon})||}{||\vec{\epsilon}||} = 0$$ Now in the lecture notes, it is stated that if $\vec{f}$ is a twice continuously differentiable vector function, then the system is locally linear at $\vec{0}$. While in the textbook, it is stated that suppose we write $\vec{f} = \begin{pmatrix} f_1 \\ f_2 \end{pmatrix} $, then the system is locally linear at $\vec{0}$ if both $f_1$ and $f_2$ has continuous second order partial derivatives. Can anyone explain what does it mean for a vector function to be twice differentiable and what is the relationship between the condition in the lecture notes and the condition in the textbook?","Let $I \subseteq \mathbb{R}$ be an open interval and $\vec{f}: \mathbb{R}^2 \to \mathbb{R}^2, \vec{y}: I \to \mathbb{R}^2$ be functions such that the system $\vec{y}' = \vec{f} \circ \vec{y}$ has an isolated critical point at $\vec{0}$. Then we say the above system is locally linear at $\vec{0}$ if there is a linear map $A: \mathbb{R}^2 \to \mathbb{R}^2$ and a function $\vec{g}: \mathbb{R}^2 \to \mathbb{R}^2$ such that $$\vec{y}' = A\vec{y} + \vec{g} \circ \vec{y}$$ and $$\lim_{\vec{\epsilon} \to \vec{0}} \frac{||\vec{g}(\vec{\epsilon})||}{||\vec{\epsilon}||} = 0$$ Now in the lecture notes, it is stated that if $\vec{f}$ is a twice continuously differentiable vector function, then the system is locally linear at $\vec{0}$. While in the textbook, it is stated that suppose we write $\vec{f} = \begin{pmatrix} f_1 \\ f_2 \end{pmatrix} $, then the system is locally linear at $\vec{0}$ if both $f_1$ and $f_2$ has continuous second order partial derivatives. Can anyone explain what does it mean for a vector function to be twice differentiable and what is the relationship between the condition in the lecture notes and the condition in the textbook?",,"['ordinary-differential-equations', 'multivariable-calculus', 'definition']"
98,What do I do wrong solving this differential equation?,What do I do wrong solving this differential equation?,,"$$x^2 y''-2xy'+2y=2+x$$ With $$t=\ln(x), x=et$$ I have $$y_H= c_1e^t +c_2e^{2t}$$ . Going on: $y_p$ should be rather easy to calculate, yet I fail to do it correctly. I'm calculating the determinant of the following matrices  $$W=\begin{bmatrix} e^t & e^{2t} \\ e^t & 2e^{2t}\end{bmatrix} $$ $$ W_1=\begin{bmatrix} 0 & e2t \\ 2+et & 2e2t\end{bmatrix}$$ $$ W_2=\begin{bmatrix} e^t & 0\\ e^t & 2+e^t\end{bmatrix} $$ $$y_p=u_1e^t +u_2e^{2t}$$ , where $$u_i=\frac{|W_i|}{|W|}$$ I integrate them, substitute it and get the wrong answer (should be -$x\ln(x)+1$, but I get $ -x\ln(x)+x+3$). Can you please help me figure it out, how to do it correctly? Thanks!","$$x^2 y''-2xy'+2y=2+x$$ With $$t=\ln(x), x=et$$ I have $$y_H= c_1e^t +c_2e^{2t}$$ . Going on: $y_p$ should be rather easy to calculate, yet I fail to do it correctly. I'm calculating the determinant of the following matrices  $$W=\begin{bmatrix} e^t & e^{2t} \\ e^t & 2e^{2t}\end{bmatrix} $$ $$ W_1=\begin{bmatrix} 0 & e2t \\ 2+et & 2e2t\end{bmatrix}$$ $$ W_2=\begin{bmatrix} e^t & 0\\ e^t & 2+e^t\end{bmatrix} $$ $$y_p=u_1e^t +u_2e^{2t}$$ , where $$u_i=\frac{|W_i|}{|W|}$$ I integrate them, substitute it and get the wrong answer (should be -$x\ln(x)+1$, but I get $ -x\ln(x)+x+3$). Can you please help me figure it out, how to do it correctly? Thanks!",,['ordinary-differential-equations']
99,A complex ordinary differential equation [duplicate],A complex ordinary differential equation [duplicate],,This question already has an answer here : Complex differential equation (1 answer) Closed 6 years ago . I’m trying to see what instruments I could use to analyse the following ODE in the complex plane: $ \dot{z} = \exp(it)\cdot \bar{z}$. Where $z$ is a function of real valued time. ${{}}$,This question already has an answer here : Complex differential equation (1 answer) Closed 6 years ago . I’m trying to see what instruments I could use to analyse the following ODE in the complex plane: $ \dot{z} = \exp(it)\cdot \bar{z}$. Where $z$ is a function of real valued time. ${{}}$,,"['complex-analysis', 'ordinary-differential-equations']"
