,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Real life examples of commutative but non-associative operations,Real life examples of commutative but non-associative operations,,"I've been trying to find ways to explain to people why associativity is important. Subtraction is a good example of something that isn't associative,  but it is not commutative. So the best I could come up with is paper-rock-scissors; the operation takes two inputs and puts out the winner (assuming they are different). So (paper rock) scissors= paper scissors = scissors, But paper (rock scissors)= paper rock = paper. This is a good example because it shows that associativity matters even outside of math. What other real-life examples are there of commutative but non-associative operations? Preferably those with as little necessary math background as possible.","I've been trying to find ways to explain to people why associativity is important. Subtraction is a good example of something that isn't associative,  but it is not commutative. So the best I could come up with is paper-rock-scissors; the operation takes two inputs and puts out the winner (assuming they are different). So (paper rock) scissors= paper scissors = scissors, But paper (rock scissors)= paper rock = paper. This is a good example because it shows that associativity matters even outside of math. What other real-life examples are there of commutative but non-associative operations? Preferably those with as little necessary math background as possible.",,"['abstract-algebra', 'soft-question', 'examples-counterexamples', 'binary-operations', 'associativity']"
1,"Proof that ideals in $C[0,1]$ are of the form $M_c$ that should not involve Zorn's Lemma",Proof that ideals in  are of the form  that should not involve Zorn's Lemma,"C[0,1] M_c","I am learning abstract algebra by myself using Dummit & Foote, and sometimes by working very hard I find very complicated non-proofs of things and I have no idea if they are the best or the worst ideas for those kind of things out there (obviously if they're not working they're probably wrong, but maybe not!). So I feel like talking about it and see what the audience thinks about it. Hence I am asking for a full solution only if you don't feel like giving a hint. If a hint has been given please don't try to come up with the answer and wait for me to accept. So here's the exercise (page 259, Dummit & Foote's Abstract Algebra) : Let $R = C[0,1]$ with addition and multiplication as the ring operations. For each $c \in [0,1]$ define $M_c = \{ f \in R \, | \, f(c) = 0 \}$. It can be easily seen that $M_c$ is a maximal ideal of $R$ because $f(x) = (x-c) \in M_c$ and if $g \notin M_c$ then $g(c) \neq 0$, thus $f^2 > 0$ everywhere except at $c$ and $g^2(c)> 0$, hence $f^2 + g^2$ is a strictly positive continuous function, and since any ideal $I$ such that $M_c \subsetneq I \triangleleft R$ is such that $(M_c,g) \subseteq I$, $f^2 + g^2 \in (M_c,g)$, thus $f^2 + g^2$ is a unit in $(M_c,g)$ and that means that for every $g \notin M_c$, $R = (M_c, g) = I$. Using this trick with $f^2 + g^2$ I wanted to show the following : Prove that if $M$ is any maximal ideal of $R$ then there is a real number $c \in [0,1]$ such that $M = M_c$. I tried considering $K(g) = \{ c \in [0,1] \, | \, g(c) = 0 \}$ and then defining decreasing sequences $K(g) \supsetneq K(g_1) \supsetneq K(g_2) \supseteq \dots$ (using the $f^2 + g^2$ trick) and then looking at the intersection of all those $K$'s but that got me to the weird need of using Zorn's Lemma and got me depressed after multiple tries. Also tried to use the fact that $R/M_c$ is a field but even if I had shown that it was isomorphic to $\mathbb R$ it wasn't clear what I could do in that direction so I gave up pretty fast on that one. Any hints are welcome!","I am learning abstract algebra by myself using Dummit & Foote, and sometimes by working very hard I find very complicated non-proofs of things and I have no idea if they are the best or the worst ideas for those kind of things out there (obviously if they're not working they're probably wrong, but maybe not!). So I feel like talking about it and see what the audience thinks about it. Hence I am asking for a full solution only if you don't feel like giving a hint. If a hint has been given please don't try to come up with the answer and wait for me to accept. So here's the exercise (page 259, Dummit & Foote's Abstract Algebra) : Let $R = C[0,1]$ with addition and multiplication as the ring operations. For each $c \in [0,1]$ define $M_c = \{ f \in R \, | \, f(c) = 0 \}$. It can be easily seen that $M_c$ is a maximal ideal of $R$ because $f(x) = (x-c) \in M_c$ and if $g \notin M_c$ then $g(c) \neq 0$, thus $f^2 > 0$ everywhere except at $c$ and $g^2(c)> 0$, hence $f^2 + g^2$ is a strictly positive continuous function, and since any ideal $I$ such that $M_c \subsetneq I \triangleleft R$ is such that $(M_c,g) \subseteq I$, $f^2 + g^2 \in (M_c,g)$, thus $f^2 + g^2$ is a unit in $(M_c,g)$ and that means that for every $g \notin M_c$, $R = (M_c, g) = I$. Using this trick with $f^2 + g^2$ I wanted to show the following : Prove that if $M$ is any maximal ideal of $R$ then there is a real number $c \in [0,1]$ such that $M = M_c$. I tried considering $K(g) = \{ c \in [0,1] \, | \, g(c) = 0 \}$ and then defining decreasing sequences $K(g) \supsetneq K(g_1) \supsetneq K(g_2) \supseteq \dots$ (using the $f^2 + g^2$ trick) and then looking at the intersection of all those $K$'s but that got me to the weird need of using Zorn's Lemma and got me depressed after multiple tries. Also tried to use the fact that $R/M_c$ is a field but even if I had shown that it was isomorphic to $\mathbb R$ it wasn't clear what I could do in that direction so I gave up pretty fast on that one. Any hints are welcome!",,"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals']"
2,Motivation for the proof of Eisenstein's Criterion for irreducibility of polynomials,Motivation for the proof of Eisenstein's Criterion for irreducibility of polynomials,,"I have been thinking about this for quite sometime. Eisenstein Criterion for Irreducibility: Let $f$ be a primitive polynomial over a unique factorization domain $R$ , say $$f(x)=a_0 + a_1x + a_2x^2 + \cdots + a_nx^n \;.$$ If $R$ has an irreducible element $p$ such that $$p\mid a_m\ \text{ for all }\ 0\le m\le n-1$$ $$p^2 \nmid a_0$$ $$p \nmid a_n$$ then $f$ is irreducible. Can anyone give me an explanation of how one might have conjectured this problem ? Thinking along, the same lines the first polynomial which came to my mind was $x^{2}+1 \in \mathbb{R}[x]$ which is irreducible. But there are lots of polynomials and it's very difficult to think of a condition, which would make them irreducible.","I have been thinking about this for quite sometime. Eisenstein Criterion for Irreducibility: Let be a primitive polynomial over a unique factorization domain , say If has an irreducible element such that then is irreducible. Can anyone give me an explanation of how one might have conjectured this problem ? Thinking along, the same lines the first polynomial which came to my mind was which is irreducible. But there are lots of polynomials and it's very difficult to think of a condition, which would make them irreducible.",f R f(x)=a_0 + a_1x + a_2x^2 + \cdots + a_nx^n \;. R p p\mid a_m\ \text{ for all }\ 0\le m\le n-1 p^2 \nmid a_0 p \nmid a_n f x^{2}+1 \in \mathbb{R}[x],"['abstract-algebra', 'number-theory']"
3,Intermediate ring between a field and an algebraic extension.,Intermediate ring between a field and an algebraic extension.,,"This is an exercise in some textbooks. Let $E$ be an algebraic extension of $F$. Suppose $R$ is ring that contains $F$ and is contained in $E$. Prove that $R$ is a field. The trouble is really with the inverse of $r$, where $r\in R$. How to prove that $r^{-1}\in R$, in apparent lack of a characterization of $R$. It occurred to me to use the smallest field containing $R$ ($R$ is easily shown to be an integral domain), that's the field of quotients, and proving that it's $R$ itself, but I don't really know how to proceed. A not-too-weak, not-too-strong hint will be much appreciated. Beware $ $ Readers seeking only hints should  beware that there is now a complete answer.","This is an exercise in some textbooks. Let $E$ be an algebraic extension of $F$. Suppose $R$ is ring that contains $F$ and is contained in $E$. Prove that $R$ is a field. The trouble is really with the inverse of $r$, where $r\in R$. How to prove that $r^{-1}\in R$, in apparent lack of a characterization of $R$. It occurred to me to use the smallest field containing $R$ ($R$ is easily shown to be an integral domain), that's the field of quotients, and proving that it's $R$ itself, but I don't really know how to proceed. A not-too-weak, not-too-strong hint will be much appreciated. Beware $ $ Readers seeking only hints should  beware that there is now a complete answer.",,"['abstract-algebra', 'field-theory']"
4,Units and Nilpotents,Units and Nilpotents,,"If $ua = au$, where $u$ is a unit and $a$ is a nilpotent, show that $u+a$ is a unit. I've been working on this problem for an hour that I tried to construct an element $x \in R$ such that $x(u+a) = 1 = (u+a)x$. After tried several elements and manipulated $ua = au$, I still couldn't find any clue. Can anybody give me a hint?","If $ua = au$, where $u$ is a unit and $a$ is a nilpotent, show that $u+a$ is a unit. I've been working on this problem for an hour that I tried to construct an element $x \in R$ such that $x(u+a) = 1 = (u+a)x$. After tried several elements and manipulated $ua = au$, I still couldn't find any clue. Can anybody give me a hint?",,"['abstract-algebra', 'ring-theory', 'faq']"
5,Is the set of quaternions $\mathbb{H}$ algebraically closed?,Is the set of quaternions  algebraically closed?,\mathbb{H},"A skew field $K$ is said to be algebraically closed if it contains a root for every non-constant polynomial in $K[x]$. I know that this is true for $\mathbb{C}$, which is the algebraic closure of $\mathbb{R}$ and a true field. I wonder if this is also true for the strictly skew field $\mathbb{H}$. I think it's not, but I can't find a counterexample. What about the set of octonions $\mathbb{O}$, which is no longer associative, or the set of sedenions $\mathbb{S}$, which is not even alternative? Is algebraic closedness even well-defined for non-alternative algebras?","A skew field $K$ is said to be algebraically closed if it contains a root for every non-constant polynomial in $K[x]$. I know that this is true for $\mathbb{C}$, which is the algebraic closure of $\mathbb{R}$ and a true field. I wonder if this is also true for the strictly skew field $\mathbb{H}$. I think it's not, but I can't find a counterexample. What about the set of octonions $\mathbb{O}$, which is no longer associative, or the set of sedenions $\mathbb{S}$, which is not even alternative? Is algebraic closedness even well-defined for non-alternative algebras?",,"['abstract-algebra', 'quaternions']"
6,"Is anybody researching ""ternary"" groups?","Is anybody researching ""ternary"" groups?",,"As someone who has an undergraduate education in mathematics, but didn't take it any further, I have often wondered something. Of course mathematicians like to generalize ideas. i.e. it is often better to define and write proofs for a wider scope of objects than for a specific type of object. A kind of ""paradox"" if you will - the more general your ideas, often the deeper the proofs (quoting a professor). Anyway I used to often wonder about group theory especially the idea of it being a set with a list of axioms and a binary function $a\cdot b = c$. But has anybody done research on tertiary (or is that trinary/ternary) groups? As in, the same definition of a group, but with a   $\cdot (a,b,c) = d$ function. Is there such a discipline? Perhaps it reduces to standard group theory or triviality and is provably of no interest. But since many results in Finite Groups are very difficult, notably the classification of simple groups, has anybody studied a way to generalize a group in such a way that a classification theorem becomes simpler? As a trite example: Algebra was pretty tricky before the study of imaginary numbers. Or to be even more trite: the Riemann $\zeta$ function wasn't doing much before it was extended to the whole complex plane. EDIT: Just to expand what I mean. In standard groups there is an operation  $\cdot :G\times G\to G$ I am asking about the case with an operation $\cdot :G\times G\times G\to G$","As someone who has an undergraduate education in mathematics, but didn't take it any further, I have often wondered something. Of course mathematicians like to generalize ideas. i.e. it is often better to define and write proofs for a wider scope of objects than for a specific type of object. A kind of ""paradox"" if you will - the more general your ideas, often the deeper the proofs (quoting a professor). Anyway I used to often wonder about group theory especially the idea of it being a set with a list of axioms and a binary function $a\cdot b = c$. But has anybody done research on tertiary (or is that trinary/ternary) groups? As in, the same definition of a group, but with a   $\cdot (a,b,c) = d$ function. Is there such a discipline? Perhaps it reduces to standard group theory or triviality and is provably of no interest. But since many results in Finite Groups are very difficult, notably the classification of simple groups, has anybody studied a way to generalize a group in such a way that a classification theorem becomes simpler? As a trite example: Algebra was pretty tricky before the study of imaginary numbers. Or to be even more trite: the Riemann $\zeta$ function wasn't doing much before it was extended to the whole complex plane. EDIT: Just to expand what I mean. In standard groups there is an operation  $\cdot :G\times G\to G$ I am asking about the case with an operation $\cdot :G\times G\times G\to G$",,"['abstract-algebra', 'group-theory']"
7,Complete classification of the groups for which converse of Lagrange's Theorem holds,Complete classification of the groups for which converse of Lagrange's Theorem holds,,"It is known that the converse of Lagrange's Theorem isn't true in general. More precisely it is known that the following proposition: If $G$ is a finite group of order $n$ and $m\mid n$ then there exists a subgroup $H$ of $G$ such that $\operatorname{order}(H)=m$ . isn't true for all finite groups $G$ . My questions are: For which groups $G$ does the converse of Lagrange's Theorem (as stated above) hold? More precisely, if $G$ is a group for which the converse of Lagrange's Theorem as I mentioned above holds then what properties must $G$ satisfy? If there is no complete classification of such $G$ s then can someone give me references to works by other mathematicians where they try to give at least a partial classification of these $G$ s? Please note that I am not interested in knowing a complete classification of the groups for which a partial converse holds ( Sylow's Theorems does the job in some sense). I want to know a complete classification of the groups for which the converse of Lagrange's Theorem as I mentioned above holds.","It is known that the converse of Lagrange's Theorem isn't true in general. More precisely it is known that the following proposition: If is a finite group of order and then there exists a subgroup of such that . isn't true for all finite groups . My questions are: For which groups does the converse of Lagrange's Theorem (as stated above) hold? More precisely, if is a group for which the converse of Lagrange's Theorem as I mentioned above holds then what properties must satisfy? If there is no complete classification of such s then can someone give me references to works by other mathematicians where they try to give at least a partial classification of these s? Please note that I am not interested in knowing a complete classification of the groups for which a partial converse holds ( Sylow's Theorems does the job in some sense). I want to know a complete classification of the groups for which the converse of Lagrange's Theorem as I mentioned above holds.",G n m\mid n H G \operatorname{order}(H)=m G G G G G G,"['abstract-algebra', 'group-theory']"
8,$\mathbb C[X]/(X^2)$ is isomorphic to $\mathbb R[Y]/((Y^2+1)^2)$,is isomorphic to,\mathbb C[X]/(X^2) \mathbb R[Y]/((Y^2+1)^2),This question led me to the following: Prove that $\mathbb C[X]/(X^2)$ is isomorphic to $\mathbb R[Y]/((Y^2+1)^2)$.,This question led me to the following: Prove that $\mathbb C[X]/(X^2)$ is isomorphic to $\mathbb R[Y]/((Y^2+1)^2)$.,,['commutative-algebra']
9,"$Q_8$ is isomorphic to a subgroup of $S_8$, but not isomorphic to a subgroup of $S_n$ for $n\leq 7$.","is isomorphic to a subgroup of , but not isomorphic to a subgroup of  for .",Q_8 S_8 S_n n\leq 7,"Question is to prove that : $Q_8$ is isomorphic to a subgroup of $S_8$ , but not isomorphic to a subgroup of $S_n$ for $n\leq 7$ . I see that $Q_8$ is isomorphic to subgroup of $S_8$ by left multiplication action. Hint given was to prove that stabilizer of any point contains $\{\pm 1\}$ . To prove Cayley's theorem, stating any group is isomorphic to a subgroup of $S_n$ we take action of given group on a set $A$ having same cardinality. with that motivation I want to check if there is a Isomorphism then there is a map from $G\times A \rightarrow A$ . i.e., $G$ gives a permutation group $S_A$ . I tried in same manner. Suppose $Q_8$ is isomorphic to subgroup of $S_n$ with $n\leq 7.$ Then it should come from a group action of $Q_8$ on a set of cardinality at most 7. Suppose $Q_8$ acts on a set $A$ with possible cardinality at most 7. For $a \in A,$ let $\textrm{Stab}(a)=\{g\in Q_8 : g \cdot a = a\}$ $\textrm{cl}(a)=\{g \cdot a : g\in Q_8\}$ denote the stabilizer of $a$ in $Q_8$ and the class (or orbit) of $a,$ respectively. I know the number of elements in a class of $a$ equals to the index of the stabilizer of $a$ in $Q_8.$ By the definition of class of $a$ , $\textrm{cl}(a)\subseteq A$ and as $|A|\leq 7,$ I see that $|\textrm{cl}(a)|\leq 7$ . But, $|\textrm{cl}(a)|=|Q_8: \textrm{Stab}(a)|$ for any element $a\in A$ . So, $|Q_8:\textrm{Stab}(a)|=|\textrm{cl}(a)|\leq 7$ for all $a\in A$ . So, $\textrm{Stab}(a)$ should be non-trivial subgroup of $Q_8$ (if not then $|Q_8:\textrm{Stab}(a)|=8.$ A proper non-trivial subgroup of $Q_8$ contains $\{\pm1\}$ . So, in the worst case, $\{\pm 1\}\subseteq \textrm{Stab}(a)$ for all $a\in A$ . As $\ker(\eta)=\cap_{a\in A}\textrm{Stab}(a)$ (where $\eta$ is the action of $Q_8$ on $A$ ) We see that $\{\pm 1\}\subseteq \ker(\eta)$ which means that $\ker(\eta)$ is non-trivial. Thus, there is no isomorphism (coming from $\eta$ ) between $Q_8$ and any subgroup of $S_7$ . I would be thankful if someone can check whether my approach is correct or if there is any other simple possible way. P.S : Usually what i do to see whether two groups are isomorphic or not is to check for cardinality, abelian property, no of elements with same order and so on. But I was having no idea when i fail in all these ways. With this Group actions i could see possibility for getting a precise conclusion on Isomorphisms.I would like to Thank Mr. Jyrki Lahtonen (a user of Math.SE) who made me to get used to Group actions. P.S $2$ : If any thing is wrong in my idea, it is entirely my fault, and if anything is correct in this whole credit should go to Mr. Jyrki Lahtonen","Question is to prove that : is isomorphic to a subgroup of , but not isomorphic to a subgroup of for . I see that is isomorphic to subgroup of by left multiplication action. Hint given was to prove that stabilizer of any point contains . To prove Cayley's theorem, stating any group is isomorphic to a subgroup of we take action of given group on a set having same cardinality. with that motivation I want to check if there is a Isomorphism then there is a map from . i.e., gives a permutation group . I tried in same manner. Suppose is isomorphic to subgroup of with Then it should come from a group action of on a set of cardinality at most 7. Suppose acts on a set with possible cardinality at most 7. For let denote the stabilizer of in and the class (or orbit) of respectively. I know the number of elements in a class of equals to the index of the stabilizer of in By the definition of class of , and as I see that . But, for any element . So, for all . So, should be non-trivial subgroup of (if not then A proper non-trivial subgroup of contains . So, in the worst case, for all . As (where is the action of on ) We see that which means that is non-trivial. Thus, there is no isomorphism (coming from ) between and any subgroup of . I would be thankful if someone can check whether my approach is correct or if there is any other simple possible way. P.S : Usually what i do to see whether two groups are isomorphic or not is to check for cardinality, abelian property, no of elements with same order and so on. But I was having no idea when i fail in all these ways. With this Group actions i could see possibility for getting a precise conclusion on Isomorphisms.I would like to Thank Mr. Jyrki Lahtonen (a user of Math.SE) who made me to get used to Group actions. P.S : If any thing is wrong in my idea, it is entirely my fault, and if anything is correct in this whole credit should go to Mr. Jyrki Lahtonen","Q_8 S_8 S_n n\leq 7 Q_8 S_8 \{\pm 1\} S_n A G\times A \rightarrow A G S_A Q_8 S_n n\leq 7. Q_8 Q_8 A a \in A, \textrm{Stab}(a)=\{g\in Q_8 : g \cdot a = a\} \textrm{cl}(a)=\{g \cdot a : g\in Q_8\} a Q_8 a, a a Q_8. a \textrm{cl}(a)\subseteq A |A|\leq 7, |\textrm{cl}(a)|\leq 7 |\textrm{cl}(a)|=|Q_8: \textrm{Stab}(a)| a\in A |Q_8:\textrm{Stab}(a)|=|\textrm{cl}(a)|\leq 7 a\in A \textrm{Stab}(a) Q_8 |Q_8:\textrm{Stab}(a)|=8. Q_8 \{\pm1\} \{\pm 1\}\subseteq \textrm{Stab}(a) a\in A \ker(\eta)=\cap_{a\in A}\textrm{Stab}(a) \eta Q_8 A \{\pm 1\}\subseteq \ker(\eta) \ker(\eta) \eta Q_8 S_7 2",['abstract-algebra']
10,What is reductive group intuitively?,What is reductive group intuitively?,,"I am studying Geometric invariant theory and wonder how I should understand linearly reductive algebraic group. We say that an affine algebraic group $G$ is linearly reductive if all finite dimensional $G$-modules are semi-simple. I am not sure if linearly reductive groups are the same as reductive groups, which are defined as algebraic groups $G$ over algebraically closed field such that the unipotent radical of $G$ is trivial. But this definition is still beyond my intuition. Are there any good way to understand (linearly) reductive groups? It would especially be nice if reductive (Lie) groups can be characterized in geometry.","I am studying Geometric invariant theory and wonder how I should understand linearly reductive algebraic group. We say that an affine algebraic group $G$ is linearly reductive if all finite dimensional $G$-modules are semi-simple. I am not sure if linearly reductive groups are the same as reductive groups, which are defined as algebraic groups $G$ over algebraically closed field such that the unipotent radical of $G$ is trivial. But this definition is still beyond my intuition. Are there any good way to understand (linearly) reductive groups? It would especially be nice if reductive (Lie) groups can be characterized in geometry.",,"['abstract-algebra', 'algebraic-geometry', 'lie-groups', 'reductive-groups']"
11,Does every group of order $n!$ have a subgroup of order $n$?,Does every group of order  have a subgroup of order ?,n! n,"Even with little/no knowledge of the symmetric group of degree $n$ , I think that Cayley theorem is sufficient to infer that such group must have a proper subgroup of order $n$ , for any $n>2$ . I was wondering what about groups of order $n!$ : do they always have a proper subgroup of order $n$ ? This is the case for $S_n$ . This is also the case for $C_6$ , which complete the survey for $n=3$ ,  but I can't even approach $n=4$ to look for counterexamples.","Even with little/no knowledge of the symmetric group of degree , I think that Cayley theorem is sufficient to infer that such group must have a proper subgroup of order , for any . I was wondering what about groups of order : do they always have a proper subgroup of order ? This is the case for . This is also the case for , which complete the survey for ,  but I can't even approach to look for counterexamples.",n n n>2 n! n S_n C_6 n=3 n=4,['abstract-algebra']
12,Why do direct limits preserve exactness?,Why do direct limits preserve exactness?,,"I've heard that taking direct limits is an exact functor in the category of modules, and I'm trying to figure out why, as I couldn't find a proof. Suppose you have homomorphisms $\varphi_i: K_i\to N_i$ and $\psi_i: N_i\to M_i$ for $(K_i,h^i_j)$ , $(N_i,g^i_j)$ , and $(M_i,f^i_j)$ directed systems of modules such that $0\to K_i\to N_i\to M_i\to 0$ is exact for every $i$ . Why is $0\to\varinjlim K_i\to\varinjlim N_i\to\varinjlim M_i\to 0$ also exact? So I let $\varphi:\varinjlim K_i\to\varinjlim N_i$ and $\psi:\varinjlim N_i\to\varinjlim M_i$ be the natural homomorphisms. Take $x\in\ker\psi$ . Then $x=g^i(x_i)$ for some $x_i\in N_i$ . Then $0=\psi(g^i(x_i))=f^i(\psi_i(x_i))$ . I know there exists some $j\geq i$ such that $f^i_j(\psi_i(x_i))=0$ in $M_j$ . But $f^i_j\circ\psi_i=\psi_j\circ g^i_j$ , so $g^i_j(x_i)\in\ker\psi_j=\text{im}(\varphi_j)$ . Then $g^i_j(x_i)=\varphi_j(y_j)$ for some $y_j\in K_j$ , so $$ x=g^i(x_i)=g^j(g^i_j(x_i))=g^j(\varphi_j(y_j))=\varphi(h^j(y_j)) $$ and so $\ker\psi\subseteq\text{im}\varphi$ . Conversely, suppose $x\in\text{im}\varphi$ . Then $x=\varphi(y)$ for some $y=h^i(y_i)$ and $y_i\in K_i$ . So $x=\varphi(h^i(y_i))=g^i(\varphi_i(y_i))$ . Thus $$ \psi(x)=\psi(g^i(\varphi_i(y_i)))=f^i(\psi_i(\varphi_i(y_i)))=0 $$ since $\psi_i\circ\varphi_i=0$ . Then $\ker\psi=\text{im}\varphi$ . (Please let me know if I've written nonsense, too many maps can cause me to get lost!) What is bugging me is, is $\varphi$ injective and $\psi$ surjective to see that the short exact sequence is in fact exact? Is there some obvious fact I'm missing? If possible, is there an explanation in the same vein as the above (i.e. using the maps and manipulating the elements without relying on more general facts from category theory? I'm not too knowledgeable about the latter.) Thanks.","I've heard that taking direct limits is an exact functor in the category of modules, and I'm trying to figure out why, as I couldn't find a proof. Suppose you have homomorphisms and for , , and directed systems of modules such that is exact for every . Why is also exact? So I let and be the natural homomorphisms. Take . Then for some . Then . I know there exists some such that in . But , so . Then for some , so and so . Conversely, suppose . Then for some and . So . Thus since . Then . (Please let me know if I've written nonsense, too many maps can cause me to get lost!) What is bugging me is, is injective and surjective to see that the short exact sequence is in fact exact? Is there some obvious fact I'm missing? If possible, is there an explanation in the same vein as the above (i.e. using the maps and manipulating the elements without relying on more general facts from category theory? I'm not too knowledgeable about the latter.) Thanks.","\varphi_i: K_i\to N_i \psi_i: N_i\to M_i (K_i,h^i_j) (N_i,g^i_j) (M_i,f^i_j) 0\to K_i\to N_i\to M_i\to 0 i 0\to\varinjlim K_i\to\varinjlim N_i\to\varinjlim M_i\to 0 \varphi:\varinjlim K_i\to\varinjlim N_i \psi:\varinjlim N_i\to\varinjlim M_i x\in\ker\psi x=g^i(x_i) x_i\in N_i 0=\psi(g^i(x_i))=f^i(\psi_i(x_i)) j\geq i f^i_j(\psi_i(x_i))=0 M_j f^i_j\circ\psi_i=\psi_j\circ g^i_j g^i_j(x_i)\in\ker\psi_j=\text{im}(\varphi_j) g^i_j(x_i)=\varphi_j(y_j) y_j\in K_j 
x=g^i(x_i)=g^j(g^i_j(x_i))=g^j(\varphi_j(y_j))=\varphi(h^j(y_j))
 \ker\psi\subseteq\text{im}\varphi x\in\text{im}\varphi x=\varphi(y) y=h^i(y_i) y_i\in K_i x=\varphi(h^i(y_i))=g^i(\varphi_i(y_i)) 
\psi(x)=\psi(g^i(\varphi_i(y_i)))=f^i(\psi_i(\varphi_i(y_i)))=0
 \psi_i\circ\varphi_i=0 \ker\psi=\text{im}\varphi \varphi \psi","['abstract-algebra', 'modules']"
13,When is a group ring an integral domain,When is a group ring an integral domain,,If $R$ is an integral domain (I am having $\mathbb{Z}$ or a field in mind) and $G$ a (not necessarily finite) group then  we can form the group ring $R(G)$. Note that if $g^{n+1} = e$ then $(e-g)(e+g\ldots + g^n) = e - g^{n+1} = 0$. This means if $G$ has torsion then $R(G)$ always has zero-divisors. What about the inverse? So if $G$ is torsion-free does that imply $R(G)$ having no zero-divisors.,If $R$ is an integral domain (I am having $\mathbb{Z}$ or a field in mind) and $G$ a (not necessarily finite) group then  we can form the group ring $R(G)$. Note that if $g^{n+1} = e$ then $(e-g)(e+g\ldots + g^n) = e - g^{n+1} = 0$. This means if $G$ has torsion then $R(G)$ always has zero-divisors. What about the inverse? So if $G$ is torsion-free does that imply $R(G)$ having no zero-divisors.,,"['abstract-algebra', 'ring-theory', 'group-rings']"
14,Non-associative commutative binary operation [duplicate],Non-associative commutative binary operation [duplicate],,"This question already has answers here : Does commutativity imply Associativity? (13 answers) Closed 8 years ago . Is there an example of a non-associative, commutative binary operation? What about a non-associative, commutative binary operation with identity and inverses? The only example of a non-associative binary operation I have in mind is the commutator/Lie bracket. But it is not commutative!","This question already has answers here : Does commutativity imply Associativity? (13 answers) Closed 8 years ago . Is there an example of a non-associative, commutative binary operation? What about a non-associative, commutative binary operation with identity and inverses? The only example of a non-associative binary operation I have in mind is the commutator/Lie bracket. But it is not commutative!",,"['abstract-algebra', 'examples-counterexamples', 'binary-operations', 'associativity']"
15,Are there rings whose multiplicative identity is not the number 1 or number 1-based?,Are there rings whose multiplicative identity is not the number 1 or number 1-based?,,"Reading the basic definition of rings , I wondered if there are samples of rings whose multiplicative identity is not the number 1 or number 1-based (for instance the identity matrix is 1-based). E.g. for $\Bbb Z$, if the definition of multiplication is modified (creating a non-standard algebra), could the multiplicative identity of the ring be another number, or the definition of multiplication must be ""canonical"" and must not be modified? Is there a ring (currently in use for some field of Mathematics) sample of such non-1-based multiplicative identity? I am learning by myself so I apologize if the question does not make much sense, thank you! Update 2015/05/11 : I will include some links to those wiki pages that were useful to understand the concepts written in the answers. Idempotent Element Abelian Group Homomorphism Identity Element Subring","Reading the basic definition of rings , I wondered if there are samples of rings whose multiplicative identity is not the number 1 or number 1-based (for instance the identity matrix is 1-based). E.g. for $\Bbb Z$, if the definition of multiplication is modified (creating a non-standard algebra), could the multiplicative identity of the ring be another number, or the definition of multiplication must be ""canonical"" and must not be modified? Is there a ring (currently in use for some field of Mathematics) sample of such non-1-based multiplicative identity? I am learning by myself so I apologize if the question does not make much sense, thank you! Update 2015/05/11 : I will include some links to those wiki pages that were useful to understand the concepts written in the answers. Idempotent Element Abelian Group Homomorphism Identity Element Subring",,"['abstract-algebra', 'ring-theory']"
16,A non-noetherian ring with noetherian spectrum,A non-noetherian ring with noetherian spectrum,,Question 1: Does such a ring exist? Note: The definition of a noetherian topological space is similar to that in rings or sets. Every descending chain of closed subsets stops after a finite number of steps ( Question 2: is this equivalent to saying that every descending chain of opens stops?),Question 1: Does such a ring exist? Note: The definition of a noetherian topological space is similar to that in rings or sets. Every descending chain of closed subsets stops after a finite number of steps ( Question 2: is this equivalent to saying that every descending chain of opens stops?),,"['abstract-algebra', 'commutative-algebra', 'noetherian']"
17,"Motivation for the ring product rule $(a_1, a_2, a_3) \cdot (b_1, b_2, b_3) = (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2)$",Motivation for the ring product rule,"(a_1, a_2, a_3) \cdot (b_1, b_2, b_3) = (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2)","In a lecture, our professor gave an example for a ring. He took it out of another source and mentioned that he does not know the motivation for the chosen operation. Of course, it's likely that somebody just invented an arbitrary operation satisfying ring axioms. I'd still like to try my luck whether anyone here can decipher the operation and give any kind of motivation for that example. On $\mathbb{R}^3$ define the operations $+$ and $\cdot$ by $$ \begin{aligned} (a_1, a_2, a_3) + (b_1,b_2,b_3) &= (a_1+b_1,a_2+b_2,a_3+b_3) \\ (a_1, a_2, a_3) \cdot (b_1, b_2, b_3) &= (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2). \end{aligned} $$ (The $+$ and $\cdot$ operations on the right side are the usual addition and multiplication from $\mathbb{R}$.) With those operations, one can confirm that $\left(\mathbb{R}^3, +, \cdot \right)$ is a ring.","In a lecture, our professor gave an example for a ring. He took it out of another source and mentioned that he does not know the motivation for the chosen operation. Of course, it's likely that somebody just invented an arbitrary operation satisfying ring axioms. I'd still like to try my luck whether anyone here can decipher the operation and give any kind of motivation for that example. On $\mathbb{R}^3$ define the operations $+$ and $\cdot$ by $$ \begin{aligned} (a_1, a_2, a_3) + (b_1,b_2,b_3) &= (a_1+b_1,a_2+b_2,a_3+b_3) \\ (a_1, a_2, a_3) \cdot (b_1, b_2, b_3) &= (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2). \end{aligned} $$ (The $+$ and $\cdot$ operations on the right side are the usual addition and multiplication from $\mathbb{R}$.) With those operations, one can confirm that $\left(\mathbb{R}^3, +, \cdot \right)$ is a ring.",,"['abstract-algebra', 'ring-theory']"
18,Does the intersection of two finite index subgroups have finite index?,Does the intersection of two finite index subgroups have finite index?,,"Let $(G,*)$ be a group and $H,K$ be two subgroups of $G$ of finite index (the number of left cosets of $H$ and $K$ in $G$). Is the set $H\cap K$ also a subgroup of finite index? I feel like need that $[G\colon(H\cap K)]$ is a divisor of $[G\colon H]\cdot[G\colon K]$, but I dont't know when this holds. Can somebody help me out?","Let $(G,*)$ be a group and $H,K$ be two subgroups of $G$ of finite index (the number of left cosets of $H$ and $K$ in $G$). Is the set $H\cap K$ also a subgroup of finite index? I feel like need that $[G\colon(H\cap K)]$ is a divisor of $[G\colon H]\cdot[G\colon K]$, but I dont't know when this holds. Can somebody help me out?",,"['abstract-algebra', 'group-theory']"
19,Hensel's Lemma and Implicit Function Theorem,Hensel's Lemma and Implicit Function Theorem,,"In the literature and on the web happened to me several times to read confused or simply cryptic assertions regarding the fact that Hensel's Lemma is the algebraic version of Implicit Function Theorem. I tried to explicit this relation but I failed, here there are some observations I made. A first good property of Henselian rings, so rings that satisfy Hensel's Lemma, is that their spectrum is homotopically equivalent to their closed point in the sense of Grothendieck. Precisely, if $\widehat{\pi}$ is the pro-fundamental group of a scheme as in SGA1, then $\widehat{\pi}(\operatorname{Spec}(A)) \simeq \widehat{\pi}(\operatorname{Spec}(k(m))$ , where $A$ is an Henselian ring and $k(m)$ is the residue field of the maximal ideal $m$ of $A$ . So I thought that spectra of Henselian rings were the kind of ""small neighborhoods"" in which you can write a ""function"" explicitly, thanks to Hensel's Lemma. But I'm confused in trying to understand what kind of functions I have to examine. Another observation is that Henselianity is exactly the condition needed for a local ring $R$ for having no non-trivial étale coverings of $\operatorname{Spec}(R)$ which are trivial on the closed point. Since these coverings are in correspondence with ètale algebras of $R$ I examined this direction and I found that, for any field $k$ , the $k$ -algebra of the form $k[x]/f(x)$ is ètale over $k$ if and only if $f'(x)$ is invertible in the algebra. There is also a more complicated criterion for ètale algebras over rings which uses the invertibility of the determinant of the Jacobian of a system of polynomials. This is very reminiscent of the key condition of the Implicit Function Theorem, but I don't know why. Here I put the link for the wikipedia pages of some related concepts, such as the implicit function theorem , Henselian rings and Hensel's lemma . Moreover here you can find an article with a large introduction about Henselian rings. Thank you in advance for your time.","In the literature and on the web happened to me several times to read confused or simply cryptic assertions regarding the fact that Hensel's Lemma is the algebraic version of Implicit Function Theorem. I tried to explicit this relation but I failed, here there are some observations I made. A first good property of Henselian rings, so rings that satisfy Hensel's Lemma, is that their spectrum is homotopically equivalent to their closed point in the sense of Grothendieck. Precisely, if is the pro-fundamental group of a scheme as in SGA1, then , where is an Henselian ring and is the residue field of the maximal ideal of . So I thought that spectra of Henselian rings were the kind of ""small neighborhoods"" in which you can write a ""function"" explicitly, thanks to Hensel's Lemma. But I'm confused in trying to understand what kind of functions I have to examine. Another observation is that Henselianity is exactly the condition needed for a local ring for having no non-trivial étale coverings of which are trivial on the closed point. Since these coverings are in correspondence with ètale algebras of I examined this direction and I found that, for any field , the -algebra of the form is ètale over if and only if is invertible in the algebra. There is also a more complicated criterion for ètale algebras over rings which uses the invertibility of the determinant of the Jacobian of a system of polynomials. This is very reminiscent of the key condition of the Implicit Function Theorem, but I don't know why. Here I put the link for the wikipedia pages of some related concepts, such as the implicit function theorem , Henselian rings and Hensel's lemma . Moreover here you can find an article with a large introduction about Henselian rings. Thank you in advance for your time.",\widehat{\pi} \widehat{\pi}(\operatorname{Spec}(A)) \simeq \widehat{\pi}(\operatorname{Spec}(k(m)) A k(m) m A R \operatorname{Spec}(R) R k k k[x]/f(x) k f'(x),"['abstract-algebra', 'algebraic-geometry']"
20,Is there a systematic way of finding the conjugacy class and/or centralizer of an element?,Is there a systematic way of finding the conjugacy class and/or centralizer of an element?,,"Is there a systematic way of finding the conjugacy class and centralizer of an element? Could the task be simplified if we are working with ""special groups"" such as $S_n$ or $A_n$? Are there any intuitive approaches? Thanks.","Is there a systematic way of finding the conjugacy class and centralizer of an element? Could the task be simplified if we are working with ""special groups"" such as $S_n$ or $A_n$? Are there any intuitive approaches? Thanks.",,"['abstract-algebra', 'group-theory']"
21,Hom is a left-exact functor,Hom is a left-exact functor,,"If $$ 0 \to A \to B\to C,$$ is a left exact sequence of $R$ -module, then for any $R$ -module $M$ , $$ 0 \to \operatorname{Hom}_R(M,A)\to \operatorname{Hom}_R(M,B)\to \operatorname{Hom}_R(M,C), $$ is left exact. I proved the above, and highlighted what I'm a little unfamiliar with: Let $$ 0 \to A\ \xrightarrow{i}\ B\ \xrightarrow{f}\ C, $$ and $$ 0 \to \operatorname{Hom}(M,A)\ \xrightarrow{\operatorname{Hom}(M,i)}\ \operatorname{Hom}(M,B)\ \xrightarrow{\operatorname{Hom}(M,f)}\ \operatorname{Hom}(M,C). $$ We need to show that $$ \ker\left(\operatorname{Hom}(M,f)\right)=\operatorname{Hom}(M,i)(\operatorname{Hom}(M,A)). $$ Let $i \circ \varphi \in \operatorname{RHS}$ . Then $f \circ i \circ \varphi : M \to C$ is $0$ since $f \circ i \circ \varphi(M) \subseteq f( i(A)) = f(\ker(f))=0$ . Conversely, let $\psi \in \operatorname{LHS}$ . Then $f \circ \psi = 0$ so that $ f(\psi(M))=0$ . Hence $\psi(M) \subseteq \ker(f)=i(A)$ . Since the image of $\psi$ is contained in the image of $i$ , we may factor $\psi$ as $\psi=i \varphi$ with $\varphi : M \to A$ . (Here is my trial, but I'm not fully understanding this: Since $i$ is injective, $i(A)$ is isomorphic with $A$ . So $i^{-1}(\psi (M)) \subseteq A$ and if we let $\varphi=i^{-1} \psi$ , then $\psi = i \varphi$ .) And I have one more question: The above looks very messy, especially the notation. Is there a better proof/understanding about it?","If is a left exact sequence of -module, then for any -module , is left exact. I proved the above, and highlighted what I'm a little unfamiliar with: Let and We need to show that Let . Then is since . Conversely, let . Then so that . Hence . Since the image of is contained in the image of , we may factor as with . (Here is my trial, but I'm not fully understanding this: Since is injective, is isomorphic with . So and if we let , then .) And I have one more question: The above looks very messy, especially the notation. Is there a better proof/understanding about it?","
0 \to A \to B\to C, R R M 
0 \to \operatorname{Hom}_R(M,A)\to \operatorname{Hom}_R(M,B)\to \operatorname{Hom}_R(M,C),
 
0 \to A\ \xrightarrow{i}\ B\ \xrightarrow{f}\ C,
 
0 \to \operatorname{Hom}(M,A)\ \xrightarrow{\operatorname{Hom}(M,i)}\ \operatorname{Hom}(M,B)\ \xrightarrow{\operatorname{Hom}(M,f)}\ \operatorname{Hom}(M,C).
 
\ker\left(\operatorname{Hom}(M,f)\right)=\operatorname{Hom}(M,i)(\operatorname{Hom}(M,A)).
 i \circ \varphi \in \operatorname{RHS} f \circ i \circ \varphi : M \to C 0 f \circ i \circ \varphi(M) \subseteq f( i(A)) = f(\ker(f))=0 \psi \in \operatorname{LHS} f \circ \psi = 0  f(\psi(M))=0 \psi(M) \subseteq \ker(f)=i(A) \psi i \psi \psi=i \varphi \varphi : M \to A i i(A) A i^{-1}(\psi (M)) \subseteq A \varphi=i^{-1} \psi \psi = i \varphi","['abstract-algebra', 'category-theory', 'modules', 'homological-algebra', 'exact-sequence']"
22,"What is the original source of ""fiber"" in mathematics?","What is the original source of ""fiber"" in mathematics?",,"Maybe this question is not relevant to mathematics, but I am very curious. I am studying Abstract Algebra these days. I learn that the word fibre with definition: $f^{-1}({y}) = \{x \in X | f(x) = y\}$ for $\forall y \in Y$, where $f: X \rightarrow Y$ is a map. And the definition of normal fiber in Wikipedia states: Fiber or fibre (from the Latin fibra) is a natural or synthetic substance that is significantly longer than it is wide. Fibers are often used in the manufacture of other materials. Does fiber in mathematics have any relationship with the normal defintion? Or are they somewhat alike? Why do they define the inverse image with fibre ? Can anyone help me out?","Maybe this question is not relevant to mathematics, but I am very curious. I am studying Abstract Algebra these days. I learn that the word fibre with definition: $f^{-1}({y}) = \{x \in X | f(x) = y\}$ for $\forall y \in Y$, where $f: X \rightarrow Y$ is a map. And the definition of normal fiber in Wikipedia states: Fiber or fibre (from the Latin fibra) is a natural or synthetic substance that is significantly longer than it is wide. Fibers are often used in the manufacture of other materials. Does fiber in mathematics have any relationship with the normal defintion? Or are they somewhat alike? Why do they define the inverse image with fibre ? Can anyone help me out?",,"['abstract-algebra', 'terminology', 'education', 'math-history']"
23,Application of the Sylow Theorems to groups of order $p^2q$,Application of the Sylow Theorems to groups of order,p^2q,"I am trying to show that any group of order $p^2q$ has a normal Sylow subgroup where $p$ and $q$ are distinct primes. In the case $p>q$ I have no problem.. By Sylow $n_p|q$, so $n_p$ is either $1$ or $q$.  But we also have $n_p\equiv1\mod p$ which rules out $q$.  So in the case $p>q$, $n_p=1$ and the unique p-Sylow subgroup is normal. In the case $p<q$ however, I run into a problem.. Again we have $n_q|p^2$ (so $n_q\in{1,p,p^2}$). Again, the condition $n_q\equiv1\mod q$ rules out $p$. Now I am attempting to rule out the case $n_q=p^2$:  Assume $n_q=p^2$.  Then $p^2\equiv1\mod q\implies (p+1)(p-1)\equiv0\mod q\implies q$ must divide $(p+1)$ since $p<q$ and $q$ prime.  But since $p<q$ and $q|(p+1)$ we see $q=p+1$.  For primes this only happens when $p=2, q=3$. Does this mean I need to check groups of order $2^2\cdot3=12$ or did I miss something along the way that lets me conclude $n_q\neq p^2$ and thus that $n_q=1$.","I am trying to show that any group of order $p^2q$ has a normal Sylow subgroup where $p$ and $q$ are distinct primes. In the case $p>q$ I have no problem.. By Sylow $n_p|q$, so $n_p$ is either $1$ or $q$.  But we also have $n_p\equiv1\mod p$ which rules out $q$.  So in the case $p>q$, $n_p=1$ and the unique p-Sylow subgroup is normal. In the case $p<q$ however, I run into a problem.. Again we have $n_q|p^2$ (so $n_q\in{1,p,p^2}$). Again, the condition $n_q\equiv1\mod q$ rules out $p$. Now I am attempting to rule out the case $n_q=p^2$:  Assume $n_q=p^2$.  Then $p^2\equiv1\mod q\implies (p+1)(p-1)\equiv0\mod q\implies q$ must divide $(p+1)$ since $p<q$ and $q$ prime.  But since $p<q$ and $q|(p+1)$ we see $q=p+1$.  For primes this only happens when $p=2, q=3$. Does this mean I need to check groups of order $2^2\cdot3=12$ or did I miss something along the way that lets me conclude $n_q\neq p^2$ and thus that $n_q=1$.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
24,Is every quotient of a finite abelian group $G$ isomorphic to some subgroup of $G$?,Is every quotient of a finite abelian group  isomorphic to some subgroup of ?,G G,"I'm having difficulty with exercise 1.43 of Lang's Algebra . The question states Let $H$ be a subgroup of a finite abelian group $G$. Show that $G$ has a subgroup that is isomorphic to $G/H$. Thinking about this for a bit, the only reasonable approach I could think of was to construct some surjective homomorphism $\phi\colon G\to K$ for $K\leq G$, and $\ker\phi=H$, and then just use the isomorphism theorems to get the result. After a while of trying, I've failed to come up with a good map, since $H$ seems so arbitrary. I'm curious, how can one construct the desired homomorphism? This is just the approach I thought of, if there's a better one, I wouldn't mind seeing that either/instead. Thank you.","I'm having difficulty with exercise 1.43 of Lang's Algebra . The question states Let $H$ be a subgroup of a finite abelian group $G$. Show that $G$ has a subgroup that is isomorphic to $G/H$. Thinking about this for a bit, the only reasonable approach I could think of was to construct some surjective homomorphism $\phi\colon G\to K$ for $K\leq G$, and $\ker\phi=H$, and then just use the isomorphism theorems to get the result. After a while of trying, I've failed to come up with a good map, since $H$ seems so arbitrary. I'm curious, how can one construct the desired homomorphism? This is just the approach I thought of, if there's a better one, I wouldn't mind seeing that either/instead. Thank you.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
25,How to write permutations as product of disjoint cycles and transpositions,How to write permutations as product of disjoint cycles and transpositions,,"$$\sigma=\begin{pmatrix} 1 & 2 &3 & 4& 5& 6&7 &8 &9 &10 & 11 \\ 4&2&9&10&6&5&11&7&8&1&3    \end{pmatrix}$$ (1) I am asked to write this permutation in $S_{11}$ as a product of disjoint cycles and also as a product of transpositions. (2) Also find the order of the element. Is this permutation even or odd? I think these are the disjoint cycles $E_{1}=(1,4,10)$ , $\;\operatorname{order}E_1 =3$ $E_{2}= (3,9,8,7,11),\;$ $\operatorname{order}E_{2} =5$ $E_{3}=(5,6),\;$ $\operatorname{order}E_{3} =2$ $S_{11}$ = $E_{1} \cdot E_{2}\cdot  E_{3}$ The order of $\operatorname{order}E_{3}$ is even so the order of the permutation is even. Why are they asking this? And what is the significance of it being even or odd? Transpositions – I have read this a couple times but the one example in my textbook is rather unclear, I am not sure what this means? I think the transposition for $E_{1}$ is $(1,4)(1,10)$ but I'm not sure what this means.","(1) I am asked to write this permutation in as a product of disjoint cycles and also as a product of transpositions. (2) Also find the order of the element. Is this permutation even or odd? I think these are the disjoint cycles , = The order of is even so the order of the permutation is even. Why are they asking this? And what is the significance of it being even or odd? Transpositions – I have read this a couple times but the one example in my textbook is rather unclear, I am not sure what this means? I think the transposition for is but I'm not sure what this means.","\sigma=\begin{pmatrix} 1 & 2 &3 & 4& 5& 6&7 &8 &9 &10 & 11 \\ 4&2&9&10&6&5&11&7&8&1&3    \end{pmatrix} S_{11} E_{1}=(1,4,10) \;\operatorname{order}E_1 =3 E_{2}= (3,9,8,7,11),\; \operatorname{order}E_{2} =5 E_{3}=(5,6),\; \operatorname{order}E_{3} =2 S_{11} E_{1} \cdot E_{2}\cdot  E_{3} \operatorname{order}E_{3} E_{1} (1,4)(1,10)","['abstract-algebra', 'permutations']"
26,Proof of Euler's Theorem without abstract algebra?,Proof of Euler's Theorem without abstract algebra?,,"Every proof I've seen of Euler's Theorem (that $\gcd(a,m) = 1 \implies a^{\phi(m)} \equiv 1 \pmod m$ ) involves the fact that the units of $\mathbb{Z}/m\mathbb{Z}$ form a group of order $\phi(m)$ .  While this is a perfectly good proof, I have to wonder if it was the one that Euler used.  I know that there are fairly old precursors to group theory, but it still seems incongruous. Thus, my question is: Are there proofs of Euler's Theorem that do not use group/ring theory?  In particular, what proof (if any; I don't know whether Euler actually discovered the theorem) did Euler use himself?","Every proof I've seen of Euler's Theorem (that ) involves the fact that the units of form a group of order .  While this is a perfectly good proof, I have to wonder if it was the one that Euler used.  I know that there are fairly old precursors to group theory, but it still seems incongruous. Thus, my question is: Are there proofs of Euler's Theorem that do not use group/ring theory?  In particular, what proof (if any; I don't know whether Euler actually discovered the theorem) did Euler use himself?","\gcd(a,m) = 1 \implies a^{\phi(m)} \equiv 1 \pmod m \mathbb{Z}/m\mathbb{Z} \phi(m)","['abstract-algebra', 'elementary-number-theory', 'math-history', 'totient-function']"
27,"What is an intuitive definition for ""conjugate"" in Group Theory?","What is an intuitive definition for ""conjugate"" in Group Theory?",,"In Abstract Algebra, I learned about ""conjugation"" in the context of a group $H$ being a 'normal' subgroup of $G$ if the element $xhx^{-1}\in H$ for any $x\in G$. But this is not the first time I've seen the word 'conjugate'. The other times I've seen this are in pre-calculus, when trying to rationalize a denominator, or in the case where $(x+y)$ is the conjugate of $(x-y)$. Does the Group Theory version of conjugate have any link to the pre-calculus version (and other uses)?","In Abstract Algebra, I learned about ""conjugation"" in the context of a group $H$ being a 'normal' subgroup of $G$ if the element $xhx^{-1}\in H$ for any $x\in G$. But this is not the first time I've seen the word 'conjugate'. The other times I've seen this are in pre-calculus, when trying to rationalize a denominator, or in the case where $(x+y)$ is the conjugate of $(x-y)$. Does the Group Theory version of conjugate have any link to the pre-calculus version (and other uses)?",,"['abstract-algebra', 'group-theory', 'intuition']"
28,What are the units of cyclotomic integers?,What are the units of cyclotomic integers?,,This question made me realize I had a misconception about the cyclotomic integers: I thought the units were exactly the roots of unity. There are only finitely many units but infinitely many integers so the question is impossible to solve unless there are more units. So what are the units of cyclotomic integers?,This question made me realize I had a misconception about the cyclotomic integers: I thought the units were exactly the roots of unity. There are only finitely many units but infinitely many integers so the question is impossible to solve unless there are more units. So what are the units of cyclotomic integers?,,['number-theory']
29,Why algebraic closures?,Why algebraic closures?,,"Let me begin by summarizing the question: Why do we care about fields closed under rational exponentiation, and less about fields closed under other operations? Historically the solution for polynomials was important, and people were trying to find a good way to test when a certain polynomial has a root. This led to talking about algebraically closed fields, where polynomials have roots. I will particularly focus on the rationals from now on. How do we construct the rational numbers? We begin with $\{0,1\}$ and we say that if we add $1+1+\ldots+1$ we never have $0$ . We begin by closing this set under addition, then under subtraction and then under division. However we can consider the alternative, we iterate from $\{0,1\}$ and at every step we add solutions all four operations on the elements we have thus far, so the construction would go like: $\{0,1\}$ , we begin. Next we add additive inverse, a term for $1+1$ and multiplicative inverse for $1$ : $\{-1,0,1,2\}$ . Now we add the additive inverse for $2$ , the addition of $1+2$ , and multiplicative inverse for $2$ : $\{-2,-1,0,\frac12,1,2,3\}$ . Now we add the sums possible with the elements we have so far, the missing inverses, and so on: $\{-3,-2,-1\frac12,-1,-\frac12,0,\frac14,\frac13,\frac12,1,1\frac12,2\frac12,3,3\frac12,4,5\}$ . We continue ad infinitum. It is not very hard to see that any rational number is in this set, and that this set forms a field (indeed the rationals). Consider now the family of operations $\exp_q(x)=x^q$ defined for $x\geq 0$ and for a rational number $q$ . If we reiterate the above algorithm from $\mathbb Q$ and close it under $\exp_q(x)$ for positive $q$ we end up with a subfield of the real-closure of $\mathbb Q$ . The result, while not the entire real algebraic numbers, is radically closed. Any number in the field has a root of any rational order. If we only take closure under a limited collection $\exp_q$ functions we will get a subfield of this field (e.g. close only under $\exp_{0.5}$ ). Consider now the Sine-closure of $\mathbb Q$ : $Q_0=\mathbb Q$ , we begin with the rationals. $Q_1=Q_0\cup\{\sin(x)\mid x\in Q_0\}$ , the rationals were closed under field operations, so we only need to add $\sin$ 's. $Q_2$ is the collection of all sums and multiplication of pairs from $Q_1$ , adding additive and multiplicative inverses, and adding $\sin(x)$ for $x\in Q_1$ . $Q_3$ constructed the same. We finish by taking $\mathbb Q_{\sin}=\bigcup_{n=0}^\infty Q_n$ . This is a field which extends $\mathbb Q$ and is closed under the function $\sin$ . This field contains transcendental elements and is countable, so it is a non-algebraic subfield of $\mathbb R$ . So why are we mostly interested in algebraically closed fields, and not in fields like the Sine-closure of $\mathbb Q$ (or perhaps other construction similar to this one)? Is the reason historical, is it because algebra and analysis are somewhat disjoint in their purposes and analysis takes $\mathbb{R,C}$ to begin with?","Let me begin by summarizing the question: Why do we care about fields closed under rational exponentiation, and less about fields closed under other operations? Historically the solution for polynomials was important, and people were trying to find a good way to test when a certain polynomial has a root. This led to talking about algebraically closed fields, where polynomials have roots. I will particularly focus on the rationals from now on. How do we construct the rational numbers? We begin with and we say that if we add we never have . We begin by closing this set under addition, then under subtraction and then under division. However we can consider the alternative, we iterate from and at every step we add solutions all four operations on the elements we have thus far, so the construction would go like: , we begin. Next we add additive inverse, a term for and multiplicative inverse for : . Now we add the additive inverse for , the addition of , and multiplicative inverse for : . Now we add the sums possible with the elements we have so far, the missing inverses, and so on: . We continue ad infinitum. It is not very hard to see that any rational number is in this set, and that this set forms a field (indeed the rationals). Consider now the family of operations defined for and for a rational number . If we reiterate the above algorithm from and close it under for positive we end up with a subfield of the real-closure of . The result, while not the entire real algebraic numbers, is radically closed. Any number in the field has a root of any rational order. If we only take closure under a limited collection functions we will get a subfield of this field (e.g. close only under ). Consider now the Sine-closure of : , we begin with the rationals. , the rationals were closed under field operations, so we only need to add 's. is the collection of all sums and multiplication of pairs from , adding additive and multiplicative inverses, and adding for . constructed the same. We finish by taking . This is a field which extends and is closed under the function . This field contains transcendental elements and is countable, so it is a non-algebraic subfield of . So why are we mostly interested in algebraically closed fields, and not in fields like the Sine-closure of (or perhaps other construction similar to this one)? Is the reason historical, is it because algebra and analysis are somewhat disjoint in their purposes and analysis takes to begin with?","\{0,1\} 1+1+\ldots+1 0 \{0,1\} \{0,1\} 1+1 1 \{-1,0,1,2\} 2 1+2 2 \{-2,-1,0,\frac12,1,2,3\} \{-3,-2,-1\frac12,-1,-\frac12,0,\frac14,\frac13,\frac12,1,1\frac12,2\frac12,3,3\frac12,4,5\} \exp_q(x)=x^q x\geq 0 q \mathbb Q \exp_q(x) q \mathbb Q \exp_q \exp_{0.5} \mathbb Q Q_0=\mathbb Q Q_1=Q_0\cup\{\sin(x)\mid x\in Q_0\} \sin Q_2 Q_1 \sin(x) x\in Q_1 Q_3 \mathbb Q_{\sin}=\bigcup_{n=0}^\infty Q_n \mathbb Q \sin \mathbb R \mathbb Q \mathbb{R,C}","['abstract-algebra', 'soft-question', 'field-theory']"
30,What specific algebraic properties are broken at each Cayley-Dickson stage beyond octonions?,What specific algebraic properties are broken at each Cayley-Dickson stage beyond octonions?,,"I'm starting to come around to an understanding of hypercomplex numbers, and I'm particularly fascinated by the fact that certain algebraic properties are broken as we move through each of the $2^n$ dimensions. I think I understand the first $n<4$ instances: As we move from $\mathbb{R}$ to $\mathbb{C}$ we lose ordering From $\mathbb{C}$ to $\mathbb{H}$ we lose the commutative property From $\mathbb{H}$ to $\mathbb{O}$ we lose the associative property (in the form of $(xy)z \neq x(yz)$, but apparently it's still alternative and $(xx)y = x(xy)$. Is that right?) The move from $\mathbb{O}$ to $\mathbb{S}$ is where I start to get fuzzy. From what I've read, the alternative property is broken now, such that even $(xx)y \neq x(xy)$ but that also zero divisors come into play, thus making sedenion algebra non-division. My first major question is: Does the loss of the alternative property cause the emergence of zero divisors (or vice versa) or are these unrelated breakages? My bigger question is: What specific algebraic properties break as we move into 32 dimensions, then into 64, 128, 256? I've ""read"" the de Marrais/Smith paper where they coin the terms pathions, chingons, routons and voudons. At my low level, any initial ""reading"" of such a paper is mostly just intent skimming, but I'm fairly certain they don't address my question and are focused on the nature and patterns of zero divisors in these higher dimensions. If the breakages are too complicated to simply explicate in an answer here, I'm happy to do the work and read journal articles that might help me understand, but I'd appreciate a pointer to specific papers that, given enough study, will actually address the point of my specific interest--something I can't necessarily tell with an initial glance, and might need a proper mathematician to point me in the right direction. Thank you! UPDATE: If the consensus is that this is a repeat, then ok, but I don't see how the answers to the other question about why algebraic properties break answers my questions about what algebraic properties break. Actually, the response marked as an answer in that other question doesn't actually answer that question either. It provides a helpful description of how to construct a multiplication table for higher dimension Cayley-Dickson structures, but explicitly doesn't answer the question as to why the properties break. The Baez article many people suggest in responses to all hyper-complex number questions like mine is truly excellent, but is mostly restricted to octonions, and, in the few mentions it makes of higher dimension Cayley-Dickson algebras, does not refer to what properties are broken. Perhaps the question isn't answerable, but in any case it hasn't been answered in this forum. UPDATE 2: I should add that the sub question in this post about whether the loss of the alternative property specifically leads to the presence of zero divisors in sedenion algebra is definitely unique to my question. However, perhaps I should pose that as a separate question? Sorry, I'm not sure about that aspect of forum etiquette here.","I'm starting to come around to an understanding of hypercomplex numbers, and I'm particularly fascinated by the fact that certain algebraic properties are broken as we move through each of the $2^n$ dimensions. I think I understand the first $n<4$ instances: As we move from $\mathbb{R}$ to $\mathbb{C}$ we lose ordering From $\mathbb{C}$ to $\mathbb{H}$ we lose the commutative property From $\mathbb{H}$ to $\mathbb{O}$ we lose the associative property (in the form of $(xy)z \neq x(yz)$, but apparently it's still alternative and $(xx)y = x(xy)$. Is that right?) The move from $\mathbb{O}$ to $\mathbb{S}$ is where I start to get fuzzy. From what I've read, the alternative property is broken now, such that even $(xx)y \neq x(xy)$ but that also zero divisors come into play, thus making sedenion algebra non-division. My first major question is: Does the loss of the alternative property cause the emergence of zero divisors (or vice versa) or are these unrelated breakages? My bigger question is: What specific algebraic properties break as we move into 32 dimensions, then into 64, 128, 256? I've ""read"" the de Marrais/Smith paper where they coin the terms pathions, chingons, routons and voudons. At my low level, any initial ""reading"" of such a paper is mostly just intent skimming, but I'm fairly certain they don't address my question and are focused on the nature and patterns of zero divisors in these higher dimensions. If the breakages are too complicated to simply explicate in an answer here, I'm happy to do the work and read journal articles that might help me understand, but I'd appreciate a pointer to specific papers that, given enough study, will actually address the point of my specific interest--something I can't necessarily tell with an initial glance, and might need a proper mathematician to point me in the right direction. Thank you! UPDATE: If the consensus is that this is a repeat, then ok, but I don't see how the answers to the other question about why algebraic properties break answers my questions about what algebraic properties break. Actually, the response marked as an answer in that other question doesn't actually answer that question either. It provides a helpful description of how to construct a multiplication table for higher dimension Cayley-Dickson structures, but explicitly doesn't answer the question as to why the properties break. The Baez article many people suggest in responses to all hyper-complex number questions like mine is truly excellent, but is mostly restricted to octonions, and, in the few mentions it makes of higher dimension Cayley-Dickson algebras, does not refer to what properties are broken. Perhaps the question isn't answerable, but in any case it hasn't been answered in this forum. UPDATE 2: I should add that the sub question in this post about whether the loss of the alternative property specifically leads to the presence of zero divisors in sedenion algebra is definitely unique to my question. However, perhaps I should pose that as a separate question? Sorry, I'm not sure about that aspect of forum etiquette here.",,"['abstract-algebra', 'complex-numbers', 'quaternions', 'octonions', 'sedenions']"
31,Taking the automorphism group of a group is not functorial.,Taking the automorphism group of a group is not functorial.,,"Once upon a time I proved that there is no functorial 'association' $$F:\ \mathbf{Grp}\ \longrightarrow\ \mathbf{Grp}:\ G\ \longmapsto\ \operatorname{Aut}(G).$$ A few days ago I casually mentioned this to someone, and was asked for a proof. Unfortunately I could not and still can not recall how I proved it. Here is how much of my proof I do recall: Suppose such a functor does exist. Choose some group $G$ wisely, and let $f\in\operatorname{Hom}(V_4,G)$ and $g\in\operatorname{Hom}(G,V_4)$ be such that $g\circ f=\operatorname{id}_{V_4}$. Then, because $F$ is a co- or contravariant functor we have $$F(g)\circ F(f)=F(g\circ f)=F(\operatorname{id}_{V_4})=\operatorname{id}_{\operatorname{Aut}(V_4)},$$ or $$F(f)\circ F(g)=F(g\circ f)=F(\operatorname{id}_{V_4})=\operatorname{id}_{\operatorname{Aut}(V_4)},$$ where $\operatorname{Aut}(V_4)\cong S_3$. In particular $\operatorname{Aut}(G)$ contains a subgroup isomorphic to $S_3$. Then something about the order of $\operatorname{Aut}(G)$ leads to a contradiction. I cannot for the life of me find which goup $G$ would do the trick. Any ideas?","Once upon a time I proved that there is no functorial 'association' $$F:\ \mathbf{Grp}\ \longrightarrow\ \mathbf{Grp}:\ G\ \longmapsto\ \operatorname{Aut}(G).$$ A few days ago I casually mentioned this to someone, and was asked for a proof. Unfortunately I could not and still can not recall how I proved it. Here is how much of my proof I do recall: Suppose such a functor does exist. Choose some group $G$ wisely, and let $f\in\operatorname{Hom}(V_4,G)$ and $g\in\operatorname{Hom}(G,V_4)$ be such that $g\circ f=\operatorname{id}_{V_4}$. Then, because $F$ is a co- or contravariant functor we have $$F(g)\circ F(f)=F(g\circ f)=F(\operatorname{id}_{V_4})=\operatorname{id}_{\operatorname{Aut}(V_4)},$$ or $$F(f)\circ F(g)=F(g\circ f)=F(\operatorname{id}_{V_4})=\operatorname{id}_{\operatorname{Aut}(V_4)},$$ where $\operatorname{Aut}(V_4)\cong S_3$. In particular $\operatorname{Aut}(G)$ contains a subgroup isomorphic to $S_3$. Then something about the order of $\operatorname{Aut}(G)$ leads to a contradiction. I cannot for the life of me find which goup $G$ would do the trick. Any ideas?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'category-theory', 'examples-counterexamples']"
32,What is an algebra?,What is an algebra?,,Is an algebra or 'a algebra' the same thing as an algebraic structure? Or does it have a different meaning? Thanks,Is an algebra or 'a algebra' the same thing as an algebraic structure? Or does it have a different meaning? Thanks,,"['abstract-algebra', 'definition']"
33,Universal binary operation and finite fields (ring),Universal binary operation and finite fields (ring),,"Take Boolean Algebra for instance, the underlying finite field/ring $0, 1, \{AND, OR\}$ is equivalent to $ 0, 1, \{NAND\} $ or $ 0, 1, \{ NOR \}$ where NAND and NOR are considered as universal gates. Does this property, that AND ('multiplication') and OR ('addition') can be written in terms of a single universal binary relation (e.g. NAND or NOR), hold with every finite field (or finite ring)? EDIT : I am interested in mathematical structures where boolean algebra holds (so that I can design a digital circuit.). Comments from JDK and jokiri point out that this is a valid question for finite rings at least and for finite fields in one case (i.e. $1, 0$ case).","Take Boolean Algebra for instance, the underlying finite field/ring $0, 1, \{AND, OR\}$ is equivalent to $ 0, 1, \{NAND\} $ or $ 0, 1, \{ NOR \}$ where NAND and NOR are considered as universal gates. Does this property, that AND ('multiplication') and OR ('addition') can be written in terms of a single universal binary relation (e.g. NAND or NOR), hold with every finite field (or finite ring)? EDIT : I am interested in mathematical structures where boolean algebra holds (so that I can design a digital circuit.). Comments from JDK and jokiri point out that this is a valid question for finite rings at least and for finite fields in one case (i.e. $1, 0$ case).",,"['abstract-algebra', 'finite-fields', 'boolean-algebra', 'finite-rings']"
34,If $K\cong K(X)$ then must $K$ be a field of rational functions in infinitely many variables?,If  then must  be a field of rational functions in infinitely many variables?,K\cong K(X) K,"If $k$ is any field, then the field $K=k(X_0,X_1,\dots)$ of rational functions in infinitely many variables satisfies $K(X)\cong K$ (by mapping $X$ to $X_0$ and $X_n$ to $X_{n+1}$).  My question is, does the converse hold?  That is: Suppose $K$ is a field such that $K\cong K(X)$.  Must there exist a field $k$ such that $K\cong k(X_0,X_1,\dots)$? (If such a $k$ exists, then we can in fact take $k=K$, by splitting the variables into two infinite sets.) Note that if we were talking about polynomial rings instead of fields of rational functions, the answer would be no: there exists an integral domain $R$ such that $R\cong R[X]$ but $R\not\cong S[X_0,X_1,\dots]$ for any ring $S$.  A counterexample is given at A ring isomorphic to its finite polynomial rings but not to its infinite one. However, for that example the field of fractions of $R$ actually is a field of rational functions in infinitely many variables, so it does not give a counterexample to this question. In any case, I suspect the answer is no and it may be possible to find a counterexample using some idea similar to the example there (some sort of ""all but finitely many..."" construction), but don't have any concrete idea of how to make it work. Some other (unanswered) questions that may be related: Is there a field $F$ which is isomorphic to $F(X,Y)$ but not to $F(X)$? (also on MO ), The field of fractions of the rational group algebra of a torsion free abelian group .  (In particular, an answer to the former question would give a field $F$ such that $F\cong F(X,Y)$ but $F$ is not isomorphic to a field of rational functions in infinitely many variables, which is very close to this question.)","If $k$ is any field, then the field $K=k(X_0,X_1,\dots)$ of rational functions in infinitely many variables satisfies $K(X)\cong K$ (by mapping $X$ to $X_0$ and $X_n$ to $X_{n+1}$).  My question is, does the converse hold?  That is: Suppose $K$ is a field such that $K\cong K(X)$.  Must there exist a field $k$ such that $K\cong k(X_0,X_1,\dots)$? (If such a $k$ exists, then we can in fact take $k=K$, by splitting the variables into two infinite sets.) Note that if we were talking about polynomial rings instead of fields of rational functions, the answer would be no: there exists an integral domain $R$ such that $R\cong R[X]$ but $R\not\cong S[X_0,X_1,\dots]$ for any ring $S$.  A counterexample is given at A ring isomorphic to its finite polynomial rings but not to its infinite one. However, for that example the field of fractions of $R$ actually is a field of rational functions in infinitely many variables, so it does not give a counterexample to this question. In any case, I suspect the answer is no and it may be possible to find a counterexample using some idea similar to the example there (some sort of ""all but finitely many..."" construction), but don't have any concrete idea of how to make it work. Some other (unanswered) questions that may be related: Is there a field $F$ which is isomorphic to $F(X,Y)$ but not to $F(X)$? (also on MO ), The field of fractions of the rational group algebra of a torsion free abelian group .  (In particular, an answer to the former question would give a field $F$ such that $F\cong F(X,Y)$ but $F$ is not isomorphic to a field of rational functions in infinitely many variables, which is very close to this question.)",,"['abstract-algebra', 'field-theory']"
35,What's a group whose group of automorphisms is non-abelian?,What's a group whose group of automorphisms is non-abelian?,,"I recently attended an interview for admission to graduate programs in Mathematics. The interviewing professor asked me a question - Tell me a group whose group of automorphisms is non-abelian. Because I was too nervous, I couldn't think of anything substantial. Could someone please tell me, in such a situation, what is the logical way of deducing the answer to such a question posed by the interviewer? Thank you very much for your help!","I recently attended an interview for admission to graduate programs in Mathematics. The interviewing professor asked me a question - Tell me a group whose group of automorphisms is non-abelian. Because I was too nervous, I couldn't think of anything substantial. Could someone please tell me, in such a situation, what is the logical way of deducing the answer to such a question posed by the interviewer? Thank you very much for your help!",,"['abstract-algebra', 'group-theory', 'group-homomorphism']"
36,Is it possible to make integers a field?,Is it possible to make integers a field?,,"Is it possible to define addition and/or multiplication on the set of a) natural numbers (including $0$: $0,1,2,3,...$) b) integers $(..., -2, -1, 0, 1, 2, ...)$ in such way that they will become fields? Thanks in advance.","Is it possible to define addition and/or multiplication on the set of a) natural numbers (including $0$: $0,1,2,3,...$) b) integers $(..., -2, -1, 0, 1, 2, ...)$ in such way that they will become fields? Thanks in advance.",,['abstract-algebra']
37,Is there any uncountably infinite set that does not generate the reals?,Is there any uncountably infinite set that does not generate the reals?,,"Does there exist an uncountably infinite set $X \subseteq \mathbb R$ such that $\mathbb R \neq \left<X\right>$? I can't think of any, but I'm also having trouble trying to prove that no such subset exists. For example: $\mathbb R$ is uncountable and obviously $\mathbb R = \left<\mathbb R\right>$. The Cantor set $C$ is uncountable, and we know that $C - C = [0, 1]$, so then since $\mathbb R = \left<[0, 1]\right>$ we know that $C$ also generates $\mathbb R$. Also the set of irrationals $\mathbb R \setminus \mathbb Q$ is uncountable, but we can generate all the rational numbers by fixing one irrational number $\alpha$ and then saying the any rational number $x$ shall be $(\alpha + x) - \alpha$, since both $\alpha + x$ and $\alpha$ are irrational. So the examples that quickly come to mind all generate the reals. Is there a simple counterexample?","Does there exist an uncountably infinite set $X \subseteq \mathbb R$ such that $\mathbb R \neq \left<X\right>$? I can't think of any, but I'm also having trouble trying to prove that no such subset exists. For example: $\mathbb R$ is uncountable and obviously $\mathbb R = \left<\mathbb R\right>$. The Cantor set $C$ is uncountable, and we know that $C - C = [0, 1]$, so then since $\mathbb R = \left<[0, 1]\right>$ we know that $C$ also generates $\mathbb R$. Also the set of irrationals $\mathbb R \setminus \mathbb Q$ is uncountable, but we can generate all the rational numbers by fixing one irrational number $\alpha$ and then saying the any rational number $x$ shall be $(\alpha + x) - \alpha$, since both $\alpha + x$ and $\alpha$ are irrational. So the examples that quickly come to mind all generate the reals. Is there a simple counterexample?",,"['abstract-algebra', 'group-theory']"
38,Contributions of Galois Theory to Mathematics,Contributions of Galois Theory to Mathematics,,"What are the major and minor contributions of Galois Theory to Mathematics? I mean direct contributions (like being aplied as it appears in Algebra) or simply by serving as a model to other theories. I have my own answers and point of view to this question, but I think it would be nice to know your opinion too.","What are the major and minor contributions of Galois Theory to Mathematics? I mean direct contributions (like being aplied as it appears in Algebra) or simply by serving as a model to other theories. I have my own answers and point of view to this question, but I think it would be nice to know your opinion too.",,['abstract-algebra']
39,A ring element with a left inverse but no right inverse?,A ring element with a left inverse but no right inverse?,,"Can I have a hint on how to construct a ring $A$ such that there are $a, b \in A$ for which $ab = 1$ but $ba \neq 1$ , please? It seems that square matrices over a field are out of question because of the determinants , and that implies that no faithful finite-dimensional representation must exist, and my imagination seems to have given up on me :) UPD: In retrospect, this question is quite embarrassing, because the shift operator $T: \mathbb{Z}^\omega \to \mathbb{Z}^\omega$ , $T(x_1, x_2, \ldots) = (0, x_1, x_2, \ldots)$ is the very first example in Lang’s Algebra textbook that I was using at the time…","Can I have a hint on how to construct a ring such that there are for which but , please? It seems that square matrices over a field are out of question because of the determinants , and that implies that no faithful finite-dimensional representation must exist, and my imagination seems to have given up on me :) UPD: In retrospect, this question is quite embarrassing, because the shift operator , is the very first example in Lang’s Algebra textbook that I was using at the time…","A a, b \in A ab = 1 ba \neq 1 T: \mathbb{Z}^\omega \to \mathbb{Z}^\omega T(x_1, x_2, \ldots) = (0, x_1, x_2, \ldots)","['abstract-algebra', 'ring-theory']"
40,Question about distributive law in definition of a ring,Question about distributive law in definition of a ring,,"In the definition of a ring $R$, one has $a(b+c) = ab + ac$ and $(a+b)c = ac + bc$ for all $a,b,c\in R$ My question is (just out of curiosity) if one really needs both of these. I can't think of an example of something that is not a ring that only satisfies one of the sides of the distributive law. So can one prove that if $a(b+c) = ab + ac$ for all $a,b,c$, then $(a+b)c = ac + bc$ for all $a,b,c$. Edit: I maybe should add that all rings in my definition have a unity $1$.","In the definition of a ring $R$, one has $a(b+c) = ab + ac$ and $(a+b)c = ac + bc$ for all $a,b,c\in R$ My question is (just out of curiosity) if one really needs both of these. I can't think of an example of something that is not a ring that only satisfies one of the sides of the distributive law. So can one prove that if $a(b+c) = ab + ac$ for all $a,b,c$, then $(a+b)c = ac + bc$ for all $a,b,c$. Edit: I maybe should add that all rings in my definition have a unity $1$.",,"['abstract-algebra', 'ring-theory', 'definition']"
41,Ring of polynomials over a field has infinitely many primes,Ring of polynomials over a field has infinitely many primes,,"Let $F$ be a field. Why does $F[x]$ have infinitely many irreducible elements? For the case F has characteristic 0 Then x-a is irreducible for all a $\in F$ since x satisfies no non-trivial relations in F. Obviously this argument fails for a finite field since there are only finitely many a to choose from. So how may I construct irreducible polynomials in a finite field? I figure it must involve higher powers of x, maybe $x^n-a$ ?","Let $F$ be a field. Why does $F[x]$ have infinitely many irreducible elements? For the case F has characteristic 0 Then x-a is irreducible for all a $\in F$ since x satisfies no non-trivial relations in F. Obviously this argument fails for a finite field since there are only finitely many a to choose from. So how may I construct irreducible polynomials in a finite field? I figure it must involve higher powers of x, maybe $x^n-a$ ?",,['abstract-algebra']
42,Does every infinite field contain a countably infinite subfield?,Does every infinite field contain a countably infinite subfield?,,"Does every infinite field contain a countably infinite subfield? It's easy to see that every field $K$ contains either the rational numbers $\Bbb Q$ (when $K$ has characteristic $0$) or a finite field $\Bbb F_p$ (when $K$ has characteristic $p$). Thus, in the characteristic $0$ case, the answer is an easy ""yes."" But if $K$ is infinite and has characteristic $p>0$, does the fact that $K \supset \Bbb F_p$ allow us to conclude that $K$ has a countably infinite subfield?","Does every infinite field contain a countably infinite subfield? It's easy to see that every field $K$ contains either the rational numbers $\Bbb Q$ (when $K$ has characteristic $0$) or a finite field $\Bbb F_p$ (when $K$ has characteristic $p$). Thus, in the characteristic $0$ case, the answer is an easy ""yes."" But if $K$ is infinite and has characteristic $p>0$, does the fact that $K \supset \Bbb F_p$ allow us to conclude that $K$ has a countably infinite subfield?",,"['abstract-algebra', 'field-theory', 'extension-field']"
43,Isomorphism between complex numbers minus zero and unit circle,Isomorphism between complex numbers minus zero and unit circle,,How do we show that $\mathbb{C}^{\times}$ and $S^{1}$ are isomorphic as groups?,How do we show that $\mathbb{C}^{\times}$ and $S^{1}$ are isomorphic as groups?,,['abstract-algebra']
44,"""Abstract nonsense"" proof of the splitting lemma","""Abstract nonsense"" proof of the splitting lemma",,"As remarked in the ""talk"" part of the Wikipedia article , the proof is done with elements of a set and functions. I guess it's possible to carry it out purely with ""objects"" and ""arrows"" Who volunteers to do that? Edit: If possible without that Freyd embedding theorem mentioned in the talk.","As remarked in the ""talk"" part of the Wikipedia article , the proof is done with elements of a set and functions. I guess it's possible to carry it out purely with ""objects"" and ""arrows"" Who volunteers to do that? Edit: If possible without that Freyd embedding theorem mentioned in the talk.",,['abstract-algebra']
45,UFDs are integrally closed; so too are GCD & Dedekind domains.,UFDs are integrally closed; so too are GCD & Dedekind domains.,,"Let $A$ be a UFD, $K$ its field of fractions, and $f$ an element of $A[T]$ a monic polynomial. I'm trying to prove that if $f$ has a root $\alpha \in K$, then in fact $\alpha \in A$. I'm trying to exploit the fact of something about irreducibility, will it help? I havent done anything with splitting fields, but this is something i can look for.","Let $A$ be a UFD, $K$ its field of fractions, and $f$ an element of $A[T]$ a monic polynomial. I'm trying to prove that if $f$ has a root $\alpha \in K$, then in fact $\alpha \in A$. I'm trying to exploit the fact of something about irreducibility, will it help? I havent done anything with splitting fields, but this is something i can look for.",,"['abstract-algebra', 'commutative-algebra', 'integral-dependence']"
46,Concrete examples of 2-categories,Concrete examples of 2-categories,,"I've been reading some of John Baez's work on 2-categories (eg here ) and have been trying to visualize some of the constructions he gives. I'm interested in coming up with 'concrete' examples of 2-categories. As an example of what I don't mean, I know that the category Cat forms a 2-category, where the objects are small categories, the morphisms are functors and the 2-morphisms are natural transformations. But this is too abstract for me - given that categorical constructs are what I'm having trouble understanding, it doesn't help me much to give an example from category theory! One thought I had is that you might be able to view a group as a 2-category. Taking the perspective that a group is a category with one object where the morphisms are the symmetries of the object, you should then be able to construct a 2-category by saying that the 2-morphisms are the inner automorphisms of the group. An interesting question is then what the compositional structure of the 2-morphisms is. To be really concrete, consider the group $D_3$. Here the object is an equilateral triangle, and there are six morphisms $e$, $r$, $r^2$, $m$, $mr$ and $mr^2$ where $e$ is the identity, $r$ is rotation by $2\pi/3$ and $m$ is reflection in one of the axes of symmetry, and the others are the obvious compositions of these. Then the 2-morphisms are the functions $\phi_g$ given by $\phi_g(h)=ghg^{-1}$. For this example, the 2-morphisms have the structure of the underlying group $D_3$, but clearly this isn't always the case (e.g. for any abelian group the 2-morphisms have the structure of the trivial group). I haven't worked through many of the details, but it seems like there might be the grain of an interesting line of thought here. So my questions are: Is viewing groups as 2-categories an interesting thing to do, i.e. does it give you any new perspectives that make previously esoteric facts about groups 'obvious', or at least special cases of results in 2-categories? What other 'concrete' examples of 2-categories are there?","I've been reading some of John Baez's work on 2-categories (eg here ) and have been trying to visualize some of the constructions he gives. I'm interested in coming up with 'concrete' examples of 2-categories. As an example of what I don't mean, I know that the category Cat forms a 2-category, where the objects are small categories, the morphisms are functors and the 2-morphisms are natural transformations. But this is too abstract for me - given that categorical constructs are what I'm having trouble understanding, it doesn't help me much to give an example from category theory! One thought I had is that you might be able to view a group as a 2-category. Taking the perspective that a group is a category with one object where the morphisms are the symmetries of the object, you should then be able to construct a 2-category by saying that the 2-morphisms are the inner automorphisms of the group. An interesting question is then what the compositional structure of the 2-morphisms is. To be really concrete, consider the group $D_3$. Here the object is an equilateral triangle, and there are six morphisms $e$, $r$, $r^2$, $m$, $mr$ and $mr^2$ where $e$ is the identity, $r$ is rotation by $2\pi/3$ and $m$ is reflection in one of the axes of symmetry, and the others are the obvious compositions of these. Then the 2-morphisms are the functions $\phi_g$ given by $\phi_g(h)=ghg^{-1}$. For this example, the 2-morphisms have the structure of the underlying group $D_3$, but clearly this isn't always the case (e.g. for any abelian group the 2-morphisms have the structure of the trivial group). I haven't worked through many of the details, but it seems like there might be the grain of an interesting line of thought here. So my questions are: Is viewing groups as 2-categories an interesting thing to do, i.e. does it give you any new perspectives that make previously esoteric facts about groups 'obvious', or at least special cases of results in 2-categories? What other 'concrete' examples of 2-categories are there?",,"['abstract-algebra', 'group-theory', 'category-theory', 'higher-category-theory', '2-categories']"
47,Can I represent groups geometrically?,Can I represent groups geometrically?,,"I have just taken up abstract algebra for my college and my professor was giving me an introduction to groups, but since I like geometric definitions or ways of looking at stuff, I kept thinking, ""How do you represent a group geometrically in a space?"" Is there any way of representing it?","I have just taken up abstract algebra for my college and my professor was giving me an introduction to groups, but since I like geometric definitions or ways of looking at stuff, I kept thinking, ""How do you represent a group geometrically in a space?"" Is there any way of representing it?",,"['abstract-algebra', 'group-theory']"
48,Intuition behind the Frattini subgroup,Intuition behind the Frattini subgroup,,"I am trying to get a better feel for what the Frattini subgroup really is, intuitively. Let $G$ be a group and denote its Frattini subgroup by $\Phi(G)$.  I know that $\Phi(G)$ is the intersection of the maximal subgroups of $G$, and I know that it is the set of 'non-generators' (Isaacs calls them 'useless' elements) of $G$, i.e. elements $u$ for which if $\langle X \cup \{u\} \rangle =G$, then $\langle X \rangle = G$, or equivalently, if $\langle X \rangle \ne G$, then $\langle X \cup \{u\} \rangle \ne G$, where $X \subseteq G$ is a subset of $G$, and $u \in \Phi(G)$. Since $\Phi(G)$ is the set of these elements, it would help to better understand what exactly these elements are.  Is it true that such an element $u \in \Phi(G)$ is necessarily a product of elements in $X \subseteq G$ ($u$ and $X$ as above)?  If not, what is an example where it isn't? Finally, where exactly does the connection lie between these 'non-generators' and (the intersection of) maximal subgroups?  How do we see that they must lie in a maximal subgroup, and conversely that if an element lies in all maximal subgroups then it must be a 'non-generator'? Thanks for the help, as always.","I am trying to get a better feel for what the Frattini subgroup really is, intuitively. Let $G$ be a group and denote its Frattini subgroup by $\Phi(G)$.  I know that $\Phi(G)$ is the intersection of the maximal subgroups of $G$, and I know that it is the set of 'non-generators' (Isaacs calls them 'useless' elements) of $G$, i.e. elements $u$ for which if $\langle X \cup \{u\} \rangle =G$, then $\langle X \rangle = G$, or equivalently, if $\langle X \rangle \ne G$, then $\langle X \cup \{u\} \rangle \ne G$, where $X \subseteq G$ is a subset of $G$, and $u \in \Phi(G)$. Since $\Phi(G)$ is the set of these elements, it would help to better understand what exactly these elements are.  Is it true that such an element $u \in \Phi(G)$ is necessarily a product of elements in $X \subseteq G$ ($u$ and $X$ as above)?  If not, what is an example where it isn't? Finally, where exactly does the connection lie between these 'non-generators' and (the intersection of) maximal subgroups?  How do we see that they must lie in a maximal subgroup, and conversely that if an element lies in all maximal subgroups then it must be a 'non-generator'? Thanks for the help, as always.",,"['abstract-algebra', 'group-theory', 'intuition', 'frattini-subgroup']"
49,If $R$ is a Noetherian ring then $R[[x]]$ is also Noetherian,If  is a Noetherian ring then  is also Noetherian,R R[[x]],"Let $R$ be a Noetherian ring. How can one prove that the ring of the formal power series $R[[x]]$ is again a Noetherian ring? It is well-known that the ring of polynomials $R[x]$ is Noetherian. I try imitating the standard proof of the fact by replacing ""leading coefficients"" by ""lowest coefficients"", but it does not work.","Let $R$ be a Noetherian ring. How can one prove that the ring of the formal power series $R[[x]]$ is again a Noetherian ring? It is well-known that the ring of polynomials $R[x]$ is Noetherian. I try imitating the standard proof of the fact by replacing ""leading coefficients"" by ""lowest coefficients"", but it does not work.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'noetherian', 'formal-power-series']"
50,Status of the classification of non-finitely generated abelian groups.,Status of the classification of non-finitely generated abelian groups.,,"From the Wikipedia on abelian groups : By contrast, classification of general infinitely-generated abelian groups is far from complete. How far are we from a classification exactly?   It seems like divisible groups have been classified. Which cases are left which we haven't?  What is the nature of these unknown cases that makes them so hard to understand?  Do we have examples of really ""out there"" infinitely-generated groups that don't fit into any of the current categories?","From the Wikipedia on abelian groups : By contrast, classification of general infinitely-generated abelian groups is far from complete. How far are we from a classification exactly?   It seems like divisible groups have been classified. Which cases are left which we haven't?  What is the nature of these unknown cases that makes them so hard to understand?  Do we have examples of really ""out there"" infinitely-generated groups that don't fit into any of the current categories?",,"['abstract-algebra', 'group-theory', 'reference-request', 'abelian-groups', 'open-problem']"
51,Can $G≅H$ and $G≇H$ in two different views?,Can  and  in two different views?,G≅H G≇H,"Can $G≅H$ and $G≇H$ in two different views? We have two isomorphic groups $G$ and $H$, so $G≅H$ as groups and suppose that they act on a same finite set, say $\Omega$. Can we see $G≇H$ as permutation groups . Honestly, I am intrested in this point in the following link. It is started by: Notice that different permutation groups may well be isomorphic as .... See here","Can $G≅H$ and $G≇H$ in two different views? We have two isomorphic groups $G$ and $H$, so $G≅H$ as groups and suppose that they act on a same finite set, say $\Omega$. Can we see $G≇H$ as permutation groups . Honestly, I am intrested in this point in the following link. It is started by: Notice that different permutation groups may well be isomorphic as .... See here",,"['abstract-algebra', 'group-theory', 'permutations']"
52,Is it possible for an irreducible polynomial with rational coefficients to have three zeros in an arithmetic progression?,Is it possible for an irreducible polynomial with rational coefficients to have three zeros in an arithmetic progression?,,"Assume that $p(x)\in \Bbb{Q}[x]$ is irreducible of degree $n\ge3$. Is it possible that $p(x)$ has three distinct zeros $\alpha_1,\alpha_2,\alpha_3$ such that $\alpha_1-\alpha_2=\alpha_2-\alpha_3$? As also observed by Dietrich Burde a cubic won't work here, so we need $\deg p(x)\ge4$. The argument goes as follows. If $p(x)=x^3+c_2x^2+c_1x+c_0$, then $-c_2=\alpha_1+\alpha_2+\alpha_3=3\alpha_2$ implying that $\alpha_2$ would be rational and contradicting the irreducibility of $p(x)$. This came up when I was pondering this question . There the focus was in minimizing the extension degree $[\Bbb{Q}(\alpha_1-\alpha_2):\Bbb{Q}]$. I had the idea that I want to find a case, where $\alpha_1-\alpha_2$ is fixed by a large number of elements of the Galois group $G=\operatorname{Gal}(L/\Bbb{Q})$, $L\subseteq\Bbb{C}$ the splitting field of $p(x)$. One way of enabling that would be to have a lot of repetitions among the differences $\alpha_i-\alpha_j$ of the roots $\alpha_1,\ldots,\alpha_n\in\Bbb{C}$ of $p(x)$. For the purposes of that question it turned out to be sufficient to be able to pair up the zeros of $p(x)$ in such a way that the same difference is repeated for each pair (see my answer). But can we build ""chains of zeros"" with constant interval, i.e. arithmetic progressions of zeros. Variants: If it is possible for three zeros, what about longer arithmetic progressions? Does the scene change, if we replace $\Bbb{Q}$ with another field $K$ of characteristic zero? (Artin-Schreier polynomials show that the assumption about the characteristic is relevant .)","Assume that $p(x)\in \Bbb{Q}[x]$ is irreducible of degree $n\ge3$. Is it possible that $p(x)$ has three distinct zeros $\alpha_1,\alpha_2,\alpha_3$ such that $\alpha_1-\alpha_2=\alpha_2-\alpha_3$? As also observed by Dietrich Burde a cubic won't work here, so we need $\deg p(x)\ge4$. The argument goes as follows. If $p(x)=x^3+c_2x^2+c_1x+c_0$, then $-c_2=\alpha_1+\alpha_2+\alpha_3=3\alpha_2$ implying that $\alpha_2$ would be rational and contradicting the irreducibility of $p(x)$. This came up when I was pondering this question . There the focus was in minimizing the extension degree $[\Bbb{Q}(\alpha_1-\alpha_2):\Bbb{Q}]$. I had the idea that I want to find a case, where $\alpha_1-\alpha_2$ is fixed by a large number of elements of the Galois group $G=\operatorname{Gal}(L/\Bbb{Q})$, $L\subseteq\Bbb{C}$ the splitting field of $p(x)$. One way of enabling that would be to have a lot of repetitions among the differences $\alpha_i-\alpha_j$ of the roots $\alpha_1,\ldots,\alpha_n\in\Bbb{C}$ of $p(x)$. For the purposes of that question it turned out to be sufficient to be able to pair up the zeros of $p(x)$ in such a way that the same difference is repeated for each pair (see my answer). But can we build ""chains of zeros"" with constant interval, i.e. arithmetic progressions of zeros. Variants: If it is possible for three zeros, what about longer arithmetic progressions? Does the scene change, if we replace $\Bbb{Q}$ with another field $K$ of characteristic zero? (Artin-Schreier polynomials show that the assumption about the characteristic is relevant .)",,"['abstract-algebra', 'galois-theory', 'extension-field', 'irreducible-polynomials']"
53,Values attained by $|G/Z(G)|$?,Values attained by ?,|G/Z(G)|,So I was working through some problems in a book on $p$ -groups and noticed that $p$ -groups have some really nice properties. So I started computing what the values of $|G/Z(G)|$ for $p$ -groups. I decided to see what it would be and found that it cannot attain the value of $p$ . I am interested in the whethere $|G/Z(G)|$ can attain all other powers of $p$ ? In general I am interesting in the values that $|G/Z(G)|$ can achieve. Any help is greatly appreciated.,So I was working through some problems in a book on -groups and noticed that -groups have some really nice properties. So I started computing what the values of for -groups. I decided to see what it would be and found that it cannot attain the value of . I am interested in the whethere can attain all other powers of ? In general I am interesting in the values that can achieve. Any help is greatly appreciated.,p p |G/Z(G)| p p |G/Z(G)| p |G/Z(G)|,"['abstract-algebra', 'group-theory', 'finite-groups']"
54,Non-reflexive module isomorphic to its double dual,Non-reflexive module isomorphic to its double dual,,"Could you give me an example of a non-reflexive module isomorphic to its double dual? I found an example here but I cannot understand it, do you have any simpler examples? By this question we should only use not finitely generated modules.","Could you give me an example of a non-reflexive module isomorphic to its double dual? I found an example here but I cannot understand it, do you have any simpler examples? By this question we should only use not finitely generated modules.",,"['abstract-algebra', 'commutative-algebra', 'modules']"
55,A good book for beginning Group theory,A good book for beginning Group theory,,I am new to the field of Abstract Algebra and so far it's looking to me quite tough. So far I have encountered the following books in group theory - Contemporary abstract algebra by Joseph Gallian and Algebra by Michael Artin. But can someone suggest me a book which has theorems and corollaries explained using examples and not just mere proofs?,I am new to the field of Abstract Algebra and so far it's looking to me quite tough. So far I have encountered the following books in group theory - Contemporary abstract algebra by Joseph Gallian and Algebra by Michael Artin. But can someone suggest me a book which has theorems and corollaries explained using examples and not just mere proofs?,,"['abstract-algebra', 'group-theory', 'reference-request', 'book-recommendation']"
56,Why must be the additive and multiplicative identities in a field be different?,Why must be the additive and multiplicative identities in a field be different?,,I was recently reading about fields like $\mathbb {Z}_p$ and I'm wondering what's the reason they can't be the same element. Is it about the additive identity being the only element with no multiplicative inverse? To be honest it's the only thing that comes to mind but it still doesn't tell me why it should be like that.,I was recently reading about fields like $\mathbb {Z}_p$ and I'm wondering what's the reason they can't be the same element. Is it about the additive identity being the only element with no multiplicative inverse? To be honest it's the only thing that comes to mind but it still doesn't tell me why it should be like that.,,"['abstract-algebra', 'field-theory']"
57,"$(\mathbb{Q},+)$ has no maximal subgroups",has no maximal subgroups,"(\mathbb{Q},+)","I have a problem that I don't have any idea. Show that group $(\mathbb{Q},+)$ has no maximal subgroups.","I have a problem that I don't have any idea. Show that group $(\mathbb{Q},+)$ has no maximal subgroups.",,"['abstract-algebra', 'group-theory', 'maximal-subgroup']"
58,A Book for abstract Algebra,A Book for abstract Algebra,,"I am self learning abstract algebra. I am using the book Algebra by Serge Lang. The book has different definitions for some algebraic structures. (For example, according to that book rings are defined to have multiplicative identities. Also modules are defined slightly differently....etc) Given that I like the book, is it OK to keep reading this book or should I get another one? Thank you","I am self learning abstract algebra. I am using the book Algebra by Serge Lang. The book has different definitions for some algebraic structures. (For example, according to that book rings are defined to have multiplicative identities. Also modules are defined slightly differently....etc) Given that I like the book, is it OK to keep reading this book or should I get another one? Thank you",,"['abstract-algebra', 'reference-request', 'self-learning', 'book-recommendation']"
59,Is every noninvertible matrix a zero divisor?,Is every noninvertible matrix a zero divisor?,,Is every noninvertible matrix over a field a zero divisor? Related to this: What are sufficient conditions for a matrix to be a zero divisor over a noncommutative ring?,Is every noninvertible matrix over a field a zero divisor? Related to this: What are sufficient conditions for a matrix to be a zero divisor over a noncommutative ring?,,"['abstract-algebra', 'matrices']"
60,Fields that can be ordered in more than one way,Fields that can be ordered in more than one way,,"Consider the field $\mathbb{Q}[\sqrt{5}]$.  This can be made into an ordered field in two different ways: Via the usual order $<$ inherited from $\mathbb{R}$ Or via the ""alternative"" order $\prec$ defined by $$r\prec s \iff \overline{r} < \overline{s} $$ where $\overline{r}$ denotes the conjugate, i.e. $\overline{a+b\sqrt{5}}=a-b\sqrt{5}$. More broadly, if $F$ is any field, $\sigma$ an automorphism of $F$, and $<$ an order on $F$, we can define another order $\prec$ by $a\prec b \iff \sigma(a) < \sigma(b)$.  If $(F,<)$ satisfies the axioms for an ordered field, than so does $(F,\prec)$. My question: Is there an example of a field $F$ that can be made into an ordered field in two different ways, where one of the orders is not induced from the other one by an automorphism as described above? In other words, I am looking for a field that can be made into an ordered field in two different ways that are ""really different"", i.e. not equivalent up to an automorphism of the underlying field $F$.","Consider the field $\mathbb{Q}[\sqrt{5}]$.  This can be made into an ordered field in two different ways: Via the usual order $<$ inherited from $\mathbb{R}$ Or via the ""alternative"" order $\prec$ defined by $$r\prec s \iff \overline{r} < \overline{s} $$ where $\overline{r}$ denotes the conjugate, i.e. $\overline{a+b\sqrt{5}}=a-b\sqrt{5}$. More broadly, if $F$ is any field, $\sigma$ an automorphism of $F$, and $<$ an order on $F$, we can define another order $\prec$ by $a\prec b \iff \sigma(a) < \sigma(b)$.  If $(F,<)$ satisfies the axioms for an ordered field, than so does $(F,\prec)$. My question: Is there an example of a field $F$ that can be made into an ordered field in two different ways, where one of the orders is not induced from the other one by an automorphism as described above? In other words, I am looking for a field that can be made into an ordered field in two different ways that are ""really different"", i.e. not equivalent up to an automorphism of the underlying field $F$.",,"['abstract-algebra', 'field-theory', 'examples-counterexamples', 'ordered-fields']"
61,Do we really need polynomials (In contrast to polynomial functions)?,Do we really need polynomials (In contrast to polynomial functions)?,,"In the following I'm going to call a polynomial expression an element of a suitable algebraic structure (for example a ring, since it has an addition and a multiplication) that has the form $a_{n}x^{n}+\cdots+a_{1}x+a_{0}$ , where $x$ is some fixed element of said structure, a polynomial function a function of the form $x\mapsto a_{n}x^{n}+\cdots+a_{1}x+a_{0}$ (where the same algebraic considerations as above apply) a polynomial an element of the form $a_{n}X^{n}+\cdots+a_{1}X+a_{0}$ with indeterminates $X$ (these can be formalized, if we are in a ring, with strings of $0$ s and a $1$ ). Note that when we are very rigorous/formal, polynomial expressions and polynomials are something different (although in daily life we often use them synonymously). Polynomial functions and expressions are also different from each other although in this case the relationship is a closer one, since every polynomials expression can be interpreted as a polynomial function evaluated at a certain point (thus ""polynomial functions"" are something more general than ""polynomial expressions""). My question is: Why do we use polynomials ? It seems to me that every piece of mathematics I have encountered so far, one could replace every occurrence of polynomials with polynomial expressions/functions without any major change in the rest of the proof/theorem/definition etc. My thoughts: The only reasons that I can see to use polynomials are the following two: After one makes the idea precise that one can plug ""suitable"" elements into polynomials (which may lie a ring containing the ring in which the coefficients live in), one can save time in certain setting, by handling the ""plugging of suitable elements into polynomials"" more elegantly: For example in case of the theorem of Cayley-Hamilton, which in its ""polynomial function"" version would look like: Let $A$ be an $n\times n$ matrix over $K$ , whose characteristic polynomial (function) is $x\mapsto a_{n}x^{n}+\cdots+a_{1}x+a_{0}$ . Then $$ a_{n}A^{n}+\cdots+a_{1}A+a_{0}I=0. $$ whereas the ""polynomial"" version looks more elegant: Let $A$ be an $n\times n$ matrix over $K$ , whose characteristic polynomial is $p_{A}\in K\left[X\right]$ . Then $$ p_{A}\left(A\right)=0. $$ The only thing that polynomials can ""do"", but algebraic expressions/functions can't, is to be different, when the algebraic expressions/functions are the same (i.e. there's a theorem that tells us that the mapping of polynomials to polynomials expressions/functions isn't injective, if the field is finite). Maybe this small difference makes a big enough difference to consider polynomials after all, but as I said, I haven't encountered any situation in which this difference could manifest itself. ( I'm guessing that maybe cryptography or higher number theory really needs polynomials and not just polynomial expressions/functions. Since I don't know anything about these subjects, I would be very happy with an example of a theorem (whose content isn't merely technical as it is the case with the theorem above) involving polynomials, where these absolutely cannot be replaced by polynomial expressions/functions. Conversely I would also be happy with an authoritative statement from someone knowledgeable that indeed we could dispense of polynomials, if we wanted to. )","In the following I'm going to call a polynomial expression an element of a suitable algebraic structure (for example a ring, since it has an addition and a multiplication) that has the form , where is some fixed element of said structure, a polynomial function a function of the form (where the same algebraic considerations as above apply) a polynomial an element of the form with indeterminates (these can be formalized, if we are in a ring, with strings of s and a ). Note that when we are very rigorous/formal, polynomial expressions and polynomials are something different (although in daily life we often use them synonymously). Polynomial functions and expressions are also different from each other although in this case the relationship is a closer one, since every polynomials expression can be interpreted as a polynomial function evaluated at a certain point (thus ""polynomial functions"" are something more general than ""polynomial expressions""). My question is: Why do we use polynomials ? It seems to me that every piece of mathematics I have encountered so far, one could replace every occurrence of polynomials with polynomial expressions/functions without any major change in the rest of the proof/theorem/definition etc. My thoughts: The only reasons that I can see to use polynomials are the following two: After one makes the idea precise that one can plug ""suitable"" elements into polynomials (which may lie a ring containing the ring in which the coefficients live in), one can save time in certain setting, by handling the ""plugging of suitable elements into polynomials"" more elegantly: For example in case of the theorem of Cayley-Hamilton, which in its ""polynomial function"" version would look like: Let be an matrix over , whose characteristic polynomial (function) is . Then whereas the ""polynomial"" version looks more elegant: Let be an matrix over , whose characteristic polynomial is . Then The only thing that polynomials can ""do"", but algebraic expressions/functions can't, is to be different, when the algebraic expressions/functions are the same (i.e. there's a theorem that tells us that the mapping of polynomials to polynomials expressions/functions isn't injective, if the field is finite). Maybe this small difference makes a big enough difference to consider polynomials after all, but as I said, I haven't encountered any situation in which this difference could manifest itself. ( I'm guessing that maybe cryptography or higher number theory really needs polynomials and not just polynomial expressions/functions. Since I don't know anything about these subjects, I would be very happy with an example of a theorem (whose content isn't merely technical as it is the case with the theorem above) involving polynomials, where these absolutely cannot be replaced by polynomial expressions/functions. Conversely I would also be happy with an authoritative statement from someone knowledgeable that indeed we could dispense of polynomials, if we wanted to. )","a_{n}x^{n}+\cdots+a_{1}x+a_{0} x x\mapsto a_{n}x^{n}+\cdots+a_{1}x+a_{0} a_{n}X^{n}+\cdots+a_{1}X+a_{0} X 0 1 A n\times n K x\mapsto a_{n}x^{n}+\cdots+a_{1}x+a_{0} 
a_{n}A^{n}+\cdots+a_{1}A+a_{0}I=0.
 A n\times n K p_{A}\in K\left[X\right] 
p_{A}\left(A\right)=0.
","['abstract-algebra', 'functions', 'polynomials', 'soft-question', 'finite-fields']"
62,An integral domain whose every prime ideal is principal is a PID,An integral domain whose every prime ideal is principal is a PID,,Does anyone has a simple proof of the following fact: An integral domain whose every prime ideal is principal is a principal ideal domain (PID).,Does anyone has a simple proof of the following fact: An integral domain whose every prime ideal is principal is a principal ideal domain (PID).,,"['abstract-algebra', 'commutative-algebra', 'ideals', 'principal-ideal-domains']"
63,Is there any intuitive understanding of normal subgroup?,Is there any intuitive understanding of normal subgroup?,,"As the define goes: A subgroup $N$ of a group $G$ is called a normal subgroup if it is invariant under conjugation; that is, for each element $n$ in $N$ and each $g$ in $G$ , the element $gng^{−1}$ is still in $N$ . My Question is: Can anyone give me an intuitive explanation or an example of this concept? Why it is very important in algebra?","As the define goes: A subgroup of a group is called a normal subgroup if it is invariant under conjugation; that is, for each element in and each in , the element is still in . My Question is: Can anyone give me an intuitive explanation or an example of this concept? Why it is very important in algebra?",N G n N g G gng^{−1} N,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
64,"How to think about a quotient sets modulo an equivalence relation, and well-defined functions on the quotient set.","How to think about a quotient sets modulo an equivalence relation, and well-defined functions on the quotient set.",,"Perhaps there is not a correct way to think about it but I would want to know how others think about it. Here are my problems/questions, after my definitions: Definition 1. Let $X$ be a set and $\sim$ be an equivalence relation on $X$ . Then $[x]:=\{y \in X \mid y \sim x\}$ and $X/{\sim} := \{[x] \mid x \in X\}$ . My question could be summarized to ""How should I think about $X/{\sim}$ ?"". Consider $\mathbf{Z}/{\sim}$ with $z_1 \sim z_2$ $\iff$ $z_1-z_2$ is even. One then obtains $\mathbf{Z}/{\sim} = \{[0],[1]\}=\{\{...,-4,-2,0,2,4,...\},\{...,-5,-3,-1,1,3,5,...\}\}.$ The way I think about the set of all equivalence classes is that one collects all equivalent elements into one set for all elements and obtains the set on the very right in the example. Then one picks a ""name"" for each of those sets, calling it by one of its members. In the example one has the canonical choices of $[0],[1]$ . If I now pick an arbitrary element $a \in \mathbf{Z}/{\sim}$ , then there exists a $z \in \mathbf{Z}$ such that $a=[z]$ . This is because I can simply call the set $a$ by one of its representatives, in this case $z$ or in the example above $[0]$ or $[1]$ . When defining a function it then suffices to define it on all the ""names"" $[z]$ because I can give each object in $\mathbf{Z}/{\sim}$ one. The function being well defined then comes down to showing that it is independent of the name each object has been given. Is this a valid way to think about this concept or are there other, perhaps better ways to do so? I am not sure if I am satisfied with the way I would explain it to myself since the ""giving it a name"" does not really sound that rigorous. I guess one could also view this as a sort of assignment which assigns to every set of equivalent elements a member of it (which is not well defined) and then assigns to it a value such that this process is well defined. Edit: The following is still not entirely clear to me. When defining a function from a quotient set to another set, one usually defines this in the following way: $$f: X/{\sim} \to A, \ [x] \mapsto a(x).$$ How should I think about this? Do I first choose a (arbitrary) complete system of representatives, define this function for them and then show that it is not dependent of the choice of the complete system, or do I map all $[x]$ , $ x \in X$ and then realize that the images of equivalent elements are the same, meaning that the function is well defined?","Perhaps there is not a correct way to think about it but I would want to know how others think about it. Here are my problems/questions, after my definitions: Definition 1. Let be a set and be an equivalence relation on . Then and . My question could be summarized to ""How should I think about ?"". Consider with is even. One then obtains The way I think about the set of all equivalence classes is that one collects all equivalent elements into one set for all elements and obtains the set on the very right in the example. Then one picks a ""name"" for each of those sets, calling it by one of its members. In the example one has the canonical choices of . If I now pick an arbitrary element , then there exists a such that . This is because I can simply call the set by one of its representatives, in this case or in the example above or . When defining a function it then suffices to define it on all the ""names"" because I can give each object in one. The function being well defined then comes down to showing that it is independent of the name each object has been given. Is this a valid way to think about this concept or are there other, perhaps better ways to do so? I am not sure if I am satisfied with the way I would explain it to myself since the ""giving it a name"" does not really sound that rigorous. I guess one could also view this as a sort of assignment which assigns to every set of equivalent elements a member of it (which is not well defined) and then assigns to it a value such that this process is well defined. Edit: The following is still not entirely clear to me. When defining a function from a quotient set to another set, one usually defines this in the following way: How should I think about this? Do I first choose a (arbitrary) complete system of representatives, define this function for them and then show that it is not dependent of the choice of the complete system, or do I map all , and then realize that the images of equivalent elements are the same, meaning that the function is well defined?","X \sim X [x]:=\{y \in X \mid y \sim x\} X/{\sim} := \{[x] \mid x \in X\} X/{\sim} \mathbf{Z}/{\sim} z_1 \sim z_2 \iff z_1-z_2 \mathbf{Z}/{\sim} = \{[0],[1]\}=\{\{...,-4,-2,0,2,4,...\},\{...,-5,-3,-1,1,3,5,...\}\}. [0],[1] a \in \mathbf{Z}/{\sim} z \in \mathbf{Z} a=[z] a z [0] [1] [z] \mathbf{Z}/{\sim} f: X/{\sim} \to A, \ [x] \mapsto a(x). [x]  x \in X","['abstract-algebra', 'discrete-mathematics', 'elementary-set-theory', 'relations', 'equivalence-relations']"
65,"Elements in $\hat{\mathbb{Z}}$, the profinite completion of the integers","Elements in , the profinite completion of the integers",\hat{\mathbb{Z}},"Let $\hat{\mathbb{Z}}$ be the profinite completion of $\mathbb{Z}$. Since $\hat{\mathbb{Z}}$ is the inverse limit of the rings $\mathbb{Z}/n\mathbb{Z}$, it's a subgroup of $\prod_n \mathbb{Z}/n\mathbb{Z}$. So we can represent elements in $\hat{\mathbb{Z}}$ as a subset of all possible tuples $(k_1,k_2,k_3,k_4,k_5,...)$, where each $k_n$ is an element in $\mathbb{Z}/n\mathbb{Z}$. The precise subset of such tuples which corresponds to $\hat{\mathbb{Z}}$ is given by the usual definition of the inverse limit. There is a canonical injective homomorphism $\eta: \mathbb{Z} \to \hat{\mathbb{Z}}$, such that to each $z \in \mathbb{Z}$ corresponds the tuple $\text{(z mod 1, z mod 2, z mod 3, ...)}$. However, it is well known that this homomorphism is not surjective, meaning there exist elements in $\hat{\mathbb{Z}}$ which do not correspond to anything in $\mathbb{Z}$. Does anyone know how to explicitly construct an example of such an element in $\hat{\mathbb{Z}}$, which isn't in the image of the homomorphism $\eta$, and to represent it as a tuple as outlined above?","Let $\hat{\mathbb{Z}}$ be the profinite completion of $\mathbb{Z}$. Since $\hat{\mathbb{Z}}$ is the inverse limit of the rings $\mathbb{Z}/n\mathbb{Z}$, it's a subgroup of $\prod_n \mathbb{Z}/n\mathbb{Z}$. So we can represent elements in $\hat{\mathbb{Z}}$ as a subset of all possible tuples $(k_1,k_2,k_3,k_4,k_5,...)$, where each $k_n$ is an element in $\mathbb{Z}/n\mathbb{Z}$. The precise subset of such tuples which corresponds to $\hat{\mathbb{Z}}$ is given by the usual definition of the inverse limit. There is a canonical injective homomorphism $\eta: \mathbb{Z} \to \hat{\mathbb{Z}}$, such that to each $z \in \mathbb{Z}$ corresponds the tuple $\text{(z mod 1, z mod 2, z mod 3, ...)}$. However, it is well known that this homomorphism is not surjective, meaning there exist elements in $\hat{\mathbb{Z}}$ which do not correspond to anything in $\mathbb{Z}$. Does anyone know how to explicitly construct an example of such an element in $\hat{\mathbb{Z}}$, which isn't in the image of the homomorphism $\eta$, and to represent it as a tuple as outlined above?",,"['abstract-algebra', 'group-theory', 'ring-theory', 'category-theory']"
66,"If there are injective homomorphisms between two groups in both directions, are they isomorphic?","If there are injective homomorphisms between two groups in both directions, are they isomorphic?",,"If I have two groups, $G$ and $H$ and two injective homomorphisms $\phi:G \to H$ and $\psi: H \to G$, then by the first isomorphism theorem applied to $\phi$, we have that $G \cong \mathrm{Im} (\phi)$, a subgroup of $H$. Similarly, $H$ is isomorphic to a subgroup of $G$. For finite groups, this guarantees that $G \cong H$ but does this hold for infinite groups? Weird things can happen for infinite groups, e.g $n\mathbb Z \subsetneq \mathbb Z$ but $n \mathbb Z \cong \mathbb Z$. I'm wondering if this kind of thing stops an isomorphism from occurring. I think this question generalises to rings, modules, fields and so on. Can this be answered for all of these structures?","If I have two groups, $G$ and $H$ and two injective homomorphisms $\phi:G \to H$ and $\psi: H \to G$, then by the first isomorphism theorem applied to $\phi$, we have that $G \cong \mathrm{Im} (\phi)$, a subgroup of $H$. Similarly, $H$ is isomorphic to a subgroup of $G$. For finite groups, this guarantees that $G \cong H$ but does this hold for infinite groups? Weird things can happen for infinite groups, e.g $n\mathbb Z \subsetneq \mathbb Z$ but $n \mathbb Z \cong \mathbb Z$. I'm wondering if this kind of thing stops an isomorphism from occurring. I think this question generalises to rings, modules, fields and so on. Can this be answered for all of these structures?",,"['abstract-algebra', 'group-theory']"
67,How to prove that a complex number is not a root of unity?,How to prove that a complex number is not a root of unity?,,$\frac35+i\frac45$ is not a root of unity though its absolute value is $1$. Suppose I don't have a calculator to calculate out its argument then how do I prove it? Is there any approach from abstract algebra or can it be done simply using complex numbers? Any help will be truly appreciated.,$\frac35+i\frac45$ is not a root of unity though its absolute value is $1$. Suppose I don't have a calculator to calculate out its argument then how do I prove it? Is there any approach from abstract algebra or can it be done simply using complex numbers? Any help will be truly appreciated.,,"['abstract-algebra', 'group-theory', 'ring-theory', 'complex-numbers', 'field-theory']"
68,Theorems with the greatest impact on group theory as a whole,Theorems with the greatest impact on group theory as a whole,,"In his Contemporary Abstract Algebra text, Gallian asserts that Sylow's Theorem(s) and Lagrange's Theorem are the two most important results in finite group theory.  He also provides this quote by G.A. Miller: Generally these three results are implied by the expression ""Sylow's Theorem.""  All of them are of fundamental importance.  In fact, if the theorems of group theory were arranged in order of their importance Sylow's Theorem might reasonably occupy the second place - coming next to Lagrange's Theorem in such an arrangement."" The notion of 'most important theorem' in any area of mathematics is of course highly subjective, but if we depart from this phrasing and think of theorems that are very far-reaching, widely applicable, or theorems that impacted group theory the most (surely there is a good deal of overlap), do Lagrange's and Sylow's theorems still top this list when we consider the whole of group theory?  If not, what theorem or theorems would? What theorem(s) within strictly infinite group theory would top such a list?","In his Contemporary Abstract Algebra text, Gallian asserts that Sylow's Theorem(s) and Lagrange's Theorem are the two most important results in finite group theory.  He also provides this quote by G.A. Miller: Generally these three results are implied by the expression ""Sylow's Theorem.""  All of them are of fundamental importance.  In fact, if the theorems of group theory were arranged in order of their importance Sylow's Theorem might reasonably occupy the second place - coming next to Lagrange's Theorem in such an arrangement."" The notion of 'most important theorem' in any area of mathematics is of course highly subjective, but if we depart from this phrasing and think of theorems that are very far-reaching, widely applicable, or theorems that impacted group theory the most (surely there is a good deal of overlap), do Lagrange's and Sylow's theorems still top this list when we consider the whole of group theory?  If not, what theorem or theorems would? What theorem(s) within strictly infinite group theory would top such a list?",,"['abstract-algebra', 'group-theory', 'soft-question', 'big-picture']"
69,Why is it that Complex Numbers are algebraically closed?,Why is it that Complex Numbers are algebraically closed?,,"I find it curious that Complex Numbers give enough flexibility to be algebraically closed, where the reals, rational numbers do not.  For the reals it is easy to see that they cannot be used to solve equations like $x^2 + 1 =0$.  Geometrically, one can look at the number line as see that any $x$ squared yields a positive number which when added to one cannot get you back to zero.  In the complex case, however, we are working with the plane.  In this case exponents stretch and rotate any given $x$.  It is easy to therefore see in the particular circumstance that if $x=i$ that $x^2$ rotates it to $-1$ which when added to one yields the desired result (i.e. $0$).  So because the Complex Numbers are algebraically closed, I conclude that any polynomial equation with complex coefficients my be solved by choosing one or more $x$'s in the plane and rotating them and stretching them such that they will combine using the given coefficients to produce the RHS. Question: Why is it that we do not need a larger space than the plane to solve Complex polynomial equations? I have tried to find a sufficient answer through Google, but was not able.  I also searched M.SE and could not find a sufficient answer.  I am not a mathematician, so I am looking for an intuitive answer if possible.","I find it curious that Complex Numbers give enough flexibility to be algebraically closed, where the reals, rational numbers do not.  For the reals it is easy to see that they cannot be used to solve equations like $x^2 + 1 =0$.  Geometrically, one can look at the number line as see that any $x$ squared yields a positive number which when added to one cannot get you back to zero.  In the complex case, however, we are working with the plane.  In this case exponents stretch and rotate any given $x$.  It is easy to therefore see in the particular circumstance that if $x=i$ that $x^2$ rotates it to $-1$ which when added to one yields the desired result (i.e. $0$).  So because the Complex Numbers are algebraically closed, I conclude that any polynomial equation with complex coefficients my be solved by choosing one or more $x$'s in the plane and rotating them and stretching them such that they will combine using the given coefficients to produce the RHS. Question: Why is it that we do not need a larger space than the plane to solve Complex polynomial equations? I have tried to find a sufficient answer through Google, but was not able.  I also searched M.SE and could not find a sufficient answer.  I am not a mathematician, so I am looking for an intuitive answer if possible.",,"['abstract-algebra', 'intuition', 'complex-numbers']"
70,"Who named ""Quotient groups""?","Who named ""Quotient groups""?",,"Who decided to call quotient groups quotient groups, and why did they choose that name? A lot of identities such as $$\frac{G/A}{B/A}\cong \frac{G}{B}$$ suggest that whoever invented the notation understood these things a lot better than I do... Edit : I'm also interested in the notation; I was assuming that the notation and terminology went together, but perhaps that is not the case.","Who decided to call quotient groups quotient groups, and why did they choose that name? A lot of identities such as $$\frac{G/A}{B/A}\cong \frac{G}{B}$$ suggest that whoever invented the notation understood these things a lot better than I do... Edit : I'm also interested in the notation; I was assuming that the notation and terminology went together, but perhaps that is not the case.",,"['abstract-algebra', 'group-theory']"
71,The number of elements which are squares in a finite field.,The number of elements which are squares in a finite field.,,"Meanwhile reading some introductory notes about the projective special linear group $PSL(2,q)$ wherein $q$ is the cardinal number of the field; I saw: ....in a finite field of order $q$, the number of elements ($≠0$) which are squares is $q-1$ if $q$ is even number and is $\frac{1}{2}(q-1)$ if $q$ is a odd number..."" . I can see it through $\mathbb Z_5$ or $GF(2)$. Any hints for proving above fact? Thanks.","Meanwhile reading some introductory notes about the projective special linear group $PSL(2,q)$ wherein $q$ is the cardinal number of the field; I saw: ....in a finite field of order $q$, the number of elements ($≠0$) which are squares is $q-1$ if $q$ is even number and is $\frac{1}{2}(q-1)$ if $q$ is a odd number..."" . I can see it through $\mathbb Z_5$ or $GF(2)$. Any hints for proving above fact? Thanks.",,"['abstract-algebra', 'group-theory']"
72,$A^m\hookrightarrow A^n$ implies $m\leq n$ for a ring $A\neq 0$,implies  for a ring,A^m\hookrightarrow A^n m\leq n A\neq 0,"I'm trying to prove that if $A\neq 0$ is a commutative ring and there is an injective $A$-module homomorphism $A^m\hookrightarrow A^n$ then $m\leq n$ must necessarily hold. This is exercise 2.11 from Atiyah and MacDonald's Introduction to Commutative Algebra , but unfortunately all the solutions available online are either very sparse with regard to this question or seem to use (to my surprise) a generalized version of Cramer's rule. Is there perhaps some other ""cleaner"" approach to solving this problem?","I'm trying to prove that if $A\neq 0$ is a commutative ring and there is an injective $A$-module homomorphism $A^m\hookrightarrow A^n$ then $m\leq n$ must necessarily hold. This is exercise 2.11 from Atiyah and MacDonald's Introduction to Commutative Algebra , but unfortunately all the solutions available online are either very sparse with regard to this question or seem to use (to my surprise) a generalized version of Cramer's rule. Is there perhaps some other ""cleaner"" approach to solving this problem?",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
73,Orbit stabiliser theorem as an analogue to first isomorphism theorem,Orbit stabiliser theorem as an analogue to first isomorphism theorem,,"The notes I'm using to study group theory make a remark that another appropriate name for the ""orbit stabiliser theorem"" is the ""first isomorphism theorem for group actions"". For reference, here are the two theorems: First isomorphism theorem: let $\phi: G \to H$ be a group homomorphism with kernel $K$ . Then $G / K \cong \text{Image}(\phi)$ . The isomorphism is given by $\psi: G / K \to \text{Image}(\phi)$ with $\psi(gK) = \phi (g)$ . Orbit stabiliser theorem: let $G$ be a group acting on a set $X$ . Let $x \in X$ . Then the map $\phi: G / \text{Stab} (x) \to \text{Orb} (x)$ where $g \: \text{Stab} (x) \mapsto gx$ is a bijection. Also, if $G$ is finite, then $|G| = | \text{Stab} (x) | | \text{Orb} (x) |$ . I can see why the orbit stabiliser theorem is like the first isomorphism theorem, especially when considering that the stabiliser fixes points (i.e. for the stabiliser, the group action is the same as the identity action), so the stabiliser is a kind of analogue to the kernel. Similarly, the orbit is a bit like the image of a map. What I want is to understand how they're analogous in a more rigorous way, and maybe if any results can be derived for group actions in much the same way as the second and third isomorphism theorems and the correspondence theorem can be derived from the first isomorphism theorem.","The notes I'm using to study group theory make a remark that another appropriate name for the ""orbit stabiliser theorem"" is the ""first isomorphism theorem for group actions"". For reference, here are the two theorems: First isomorphism theorem: let be a group homomorphism with kernel . Then . The isomorphism is given by with . Orbit stabiliser theorem: let be a group acting on a set . Let . Then the map where is a bijection. Also, if is finite, then . I can see why the orbit stabiliser theorem is like the first isomorphism theorem, especially when considering that the stabiliser fixes points (i.e. for the stabiliser, the group action is the same as the identity action), so the stabiliser is a kind of analogue to the kernel. Similarly, the orbit is a bit like the image of a map. What I want is to understand how they're analogous in a more rigorous way, and maybe if any results can be derived for group actions in much the same way as the second and third isomorphism theorems and the correspondence theorem can be derived from the first isomorphism theorem.",\phi: G \to H K G / K \cong \text{Image}(\phi) \psi: G / K \to \text{Image}(\phi) \psi(gK) = \phi (g) G X x \in X \phi: G / \text{Stab} (x) \to \text{Orb} (x) g \: \text{Stab} (x) \mapsto gx G |G| = | \text{Stab} (x) | | \text{Orb} (x) |,"['abstract-algebra', 'group-theory', 'group-actions', 'group-isomorphism', 'group-homomorphism']"
74,Commutative property of ring addition,Commutative property of ring addition,,"I have a simple question answer to which would help me more deeply understand the concept of (non)commutative structures. Let's take for example (our teacher's definition of) a ring: Let $R\neq \emptyset$ be a set, let $\oplus:R\times R \to R$ and $\bullet :R\times R \to R$ be binary operations. Moreover, let $(R, \oplus)$ be a commutative group, $(R, \bullet)$ be a monoid and following property holds for all $a, b, c\in R$ : $$a\bullet(b\oplus c) = (a\bullet b)\oplus(a \bullet c)$$ $$(b\oplus c)\bullet a = (b\bullet a)\oplus(c \bullet a)$$ Then ordered triple $\mathbf R = (R, \oplus, \bullet \mathbf)$ is called a (unitary) ring. Moreover, we call ring $\mathbf R$ commutative iff $(R, \bullet)$ is a commutative monoid. Commutativity of a ring is always a matter of its multiplicative operation because the additive operation is always assumed to be commutative . Could anyone explain me the bold part? Why do we even in non-commutative rings (and fields etc.) assume the addition to be always commutative? Is there some serious reason? Would it make any trouble? Or studying of structures with non-commutative addition just doesn't give us anything new so we can take addition as commutative simply because of our comfort?","I have a simple question answer to which would help me more deeply understand the concept of (non)commutative structures. Let's take for example (our teacher's definition of) a ring: Let be a set, let and be binary operations. Moreover, let be a commutative group, be a monoid and following property holds for all : Then ordered triple is called a (unitary) ring. Moreover, we call ring commutative iff is a commutative monoid. Commutativity of a ring is always a matter of its multiplicative operation because the additive operation is always assumed to be commutative . Could anyone explain me the bold part? Why do we even in non-commutative rings (and fields etc.) assume the addition to be always commutative? Is there some serious reason? Would it make any trouble? Or studying of structures with non-commutative addition just doesn't give us anything new so we can take addition as commutative simply because of our comfort?","R\neq \emptyset \oplus:R\times R \to R \bullet :R\times R \to R (R, \oplus) (R, \bullet) a, b, c\in R a\bullet(b\oplus c) = (a\bullet b)\oplus(a \bullet c) (b\oplus c)\bullet a = (b\bullet a)\oplus(c \bullet a) \mathbf R = (R, \oplus, \bullet \mathbf) \mathbf R (R, \bullet)","['abstract-algebra', 'commutative-algebra', 'ring-theory', 'noncommutative-algebra']"
75,When does the distributive law apply to ideals in a commutative ring?,When does the distributive law apply to ideals in a commutative ring?,,"Let $R$ be a commutative ring with identity and $I,J,K$ be ideals of $R$ . If $I\supseteq J$ or $I\supseteq K$ , we have the following modular law $$ I\cap (J+K)=I\cap J + I\cap K$$ I was wondering if there are situations in which the modular law holds in which the hypothesis that $I$ contains at least one of $J,K$ is relaxed. (This amounts to the lattice of ideals being distributive .) One example is when $R$ is a polynomial ring or power series ring and $I,J,K$ are monomial ideals. Of course one containment always holds $I\cap (J+K)\supseteq I\cap J +I\cap K$ . In what other situations does the other containment hold?","Let be a commutative ring with identity and be ideals of . If or , we have the following modular law I was wondering if there are situations in which the modular law holds in which the hypothesis that contains at least one of is relaxed. (This amounts to the lattice of ideals being distributive .) One example is when is a polynomial ring or power series ring and are monomial ideals. Of course one containment always holds . In what other situations does the other containment hold?","R I,J,K R I\supseteq J I\supseteq K  I\cap (J+K)=I\cap J + I\cap K I J,K R I,J,K I\cap (J+K)\supseteq I\cap J +I\cap K","['abstract-algebra', 'commutative-algebra', 'ideals', 'gcd-and-lcm']"
76,Check my proof that $(ab)^{-1} = b^{-1} a^{-1}$,Check my proof that,(ab)^{-1} = b^{-1} a^{-1},"The following question is problem Pinter's Abstract Algebra . And to put things in context: $G$ is a group and $a, b$ are elements of $G$. I want to show $(ab)^{-1}$ = $b^{-1}a^{-1}$. I originally thought of proving the fact in the following manner: \begin{align*} (ab)^{-1}(ab) &= e \newline (ab)^{-1}(ab)(b^{-1}) &= (e)(b^{-1}) \newline (ab)^{-1}(a)(bb^{-1}) &= (b^{-1}) \newline (ab)^{-1}(a)(e) &= (b^{-1}) \newline (ab)^{-1}(a) &= (b^{-1}) \newline (ab)^{-1}(a)(a^{-1}) &= (b^{-1})(a^{-1}) \newline (ab)^{-1}(e) &= (b^{-1})(a^{-1}) \newline (ab)^{-1} &= (b^{-1})(a^{-1}) \newline \end{align*} I know this may seem extremely inefficient to most, and I know there is a shorter way. But would this be considered a legitimate proof? Thanks in advance!","The following question is problem Pinter's Abstract Algebra . And to put things in context: $G$ is a group and $a, b$ are elements of $G$. I want to show $(ab)^{-1}$ = $b^{-1}a^{-1}$. I originally thought of proving the fact in the following manner: \begin{align*} (ab)^{-1}(ab) &= e \newline (ab)^{-1}(ab)(b^{-1}) &= (e)(b^{-1}) \newline (ab)^{-1}(a)(bb^{-1}) &= (b^{-1}) \newline (ab)^{-1}(a)(e) &= (b^{-1}) \newline (ab)^{-1}(a) &= (b^{-1}) \newline (ab)^{-1}(a)(a^{-1}) &= (b^{-1})(a^{-1}) \newline (ab)^{-1}(e) &= (b^{-1})(a^{-1}) \newline (ab)^{-1} &= (b^{-1})(a^{-1}) \newline \end{align*} I know this may seem extremely inefficient to most, and I know there is a shorter way. But would this be considered a legitimate proof? Thanks in advance!",,"['abstract-algebra', 'group-theory']"
77,Field automorphisms of $\mathbb{Q}$ - shouldn't there be only one?,Field automorphisms of  - shouldn't there be only one?,\mathbb{Q},"$1$ has to map to $1$, right?  So $n$ has to map to $n$ (for $n \in \mathbb{Z}$), and $\frac{1}{n}$ maps to $\frac{1}{n}$, so $\frac{n}{m}$ maps to itself, so the only possible automorphism is the identity.  Is this true or am I deceiving myself?  Because I feel like there should definitely be more automorphisms of $\mathbb{Q}$. Also, if you have some extension of $\mathbb{Q}$ (call it $E$), does every automorphism of $E$ automatically fix $\mathbb{Q}$?","$1$ has to map to $1$, right?  So $n$ has to map to $n$ (for $n \in \mathbb{Z}$), and $\frac{1}{n}$ maps to $\frac{1}{n}$, so $\frac{n}{m}$ maps to itself, so the only possible automorphism is the identity.  Is this true or am I deceiving myself?  Because I feel like there should definitely be more automorphisms of $\mathbb{Q}$. Also, if you have some extension of $\mathbb{Q}$ (call it $E$), does every automorphism of $E$ automatically fix $\mathbb{Q}$?",,['abstract-algebra']
78,"Why is $\mathbb{Q}(t,\sqrt{t^3-t})$ not a purely transcendental extension of $\mathbb{Q}$?",Why is  not a purely transcendental extension of ?,"\mathbb{Q}(t,\sqrt{t^3-t}) \mathbb{Q}","This question is taken from Dummit and Foote (14.9 #6). Any help will be appreciated: Show that if $t$ is transcendental over $\mathbb{Q}$, then $\mathbb{Q}(t,\sqrt{t^3-t})$ is not a purely transcendental extension of $\mathbb{Q}$. Here's what I've got so far: Abbreviate $\sqrt{t^3-t}$ as $u$. I've shown that the transcendence degree is 1, so the problem boils down to showing that $\mathbb{Q}(t,u) \supset \mathbb{Q}(f(t,u)/g(t,u))$ strictly, for all polynomials $f,g$ in two variables. Suppose for contradiction that $\mathbb{Q}(t,u) = \mathbb{Q}(f(t,u)/g(t,u))$. Look at this field as $\mathbb{Q}(t)[x]/(x^2-(t^3-t))$, with $\bar{x}=u$. Then since $t$ and $u$ are generated by $f/g$, we have that for some polynomials $a,b,c,d$ in 1 variable, $a(\frac{f(t,x)}{g(t,x)})/b(\frac{f(t,x)}{g(t,x)})-t \in (x^2-(t^3-t))$, and $c(\frac{f(t,x)}{g(t,x)})/d(\frac{f(t,x)}{g(t,x)}) - x \in (x^2-(t^3-t))$. I then tried playing with degrees, but I haven't found a contradiction.","This question is taken from Dummit and Foote (14.9 #6). Any help will be appreciated: Show that if $t$ is transcendental over $\mathbb{Q}$, then $\mathbb{Q}(t,\sqrt{t^3-t})$ is not a purely transcendental extension of $\mathbb{Q}$. Here's what I've got so far: Abbreviate $\sqrt{t^3-t}$ as $u$. I've shown that the transcendence degree is 1, so the problem boils down to showing that $\mathbb{Q}(t,u) \supset \mathbb{Q}(f(t,u)/g(t,u))$ strictly, for all polynomials $f,g$ in two variables. Suppose for contradiction that $\mathbb{Q}(t,u) = \mathbb{Q}(f(t,u)/g(t,u))$. Look at this field as $\mathbb{Q}(t)[x]/(x^2-(t^3-t))$, with $\bar{x}=u$. Then since $t$ and $u$ are generated by $f/g$, we have that for some polynomials $a,b,c,d$ in 1 variable, $a(\frac{f(t,x)}{g(t,x)})/b(\frac{f(t,x)}{g(t,x)})-t \in (x^2-(t^3-t))$, and $c(\frac{f(t,x)}{g(t,x)})/d(\frac{f(t,x)}{g(t,x)}) - x \in (x^2-(t^3-t))$. I then tried playing with degrees, but I haven't found a contradiction.",,"['abstract-algebra', 'field-theory']"
79,Algebraic closure for $\mathbb{Q}$ or $\mathbb{F}_p$ without Choice?,Algebraic closure for  or  without Choice?,\mathbb{Q} \mathbb{F}_p,"I know the usual proof of the existence of an algebraic closure for any field using Zorn's Lemma. The answer to this previous question makes it clear that in general, some nonconstructive axiom (not necessarily the full AC) is needed to guarantee an algebraic closure. My question is if we can avoid any of this in the cases of $\mathbb{Q}$ and $\mathbb{F}_p$. Can algebraic closures for $\mathbb{Q}$ and $\mathbb{F}_p$ be constructed in ZF? Intuitively, it seems plausible to me that they can. There are two places I see a need for an AC-typed axiom in the construction of an algebraic closure: one is to create some order (i.e. bijection with $\mathbb{N}$) for the set of polynomials whose roots I need to adjoin; two is to handle what happens when I start adjoining roots and the polynomials start factoring into smaller factors. (Which factor do I approach first?) It seems to me that $\mathbb{Q}$ and $\mathbb{F}_p$ both have structure that could be used cleverly to resolve both of these points without recourse to AC. However, I don't see the path clearly.","I know the usual proof of the existence of an algebraic closure for any field using Zorn's Lemma. The answer to this previous question makes it clear that in general, some nonconstructive axiom (not necessarily the full AC) is needed to guarantee an algebraic closure. My question is if we can avoid any of this in the cases of $\mathbb{Q}$ and $\mathbb{F}_p$. Can algebraic closures for $\mathbb{Q}$ and $\mathbb{F}_p$ be constructed in ZF? Intuitively, it seems plausible to me that they can. There are two places I see a need for an AC-typed axiom in the construction of an algebraic closure: one is to create some order (i.e. bijection with $\mathbb{N}$) for the set of polynomials whose roots I need to adjoin; two is to handle what happens when I start adjoining roots and the polynomials start factoring into smaller factors. (Which factor do I approach first?) It seems to me that $\mathbb{Q}$ and $\mathbb{F}_p$ both have structure that could be used cleverly to resolve both of these points without recourse to AC. However, I don't see the path clearly.",,"['abstract-algebra', 'field-theory', 'set-theory', 'axiom-of-choice']"
80,"If the localization of a ring $R$ at every prime ideal is an integral domain, must $R$ be an integral domain?","If the localization of a ring  at every prime ideal is an integral domain, must  be an integral domain?",R R,"Let $R$ be a commutative ring. Suppose that for every prime ideal $\mathfrak p$ of $R$, the localization $R_{\mathfrak p}$ is an integral domain. Must $R$ be a integral domain? I was trying to think of counter-examples, but kept getting $R_{(0)}$ = the zero ring, which is not a domain. Any guidance would be much appreciated. Thanks.","Let $R$ be a commutative ring. Suppose that for every prime ideal $\mathfrak p$ of $R$, the localization $R_{\mathfrak p}$ is an integral domain. Must $R$ be a integral domain? I was trying to think of counter-examples, but kept getting $R_{(0)}$ = the zero ring, which is not a domain. Any guidance would be much appreciated. Thanks.",,"['abstract-algebra', 'commutative-algebra', 'ring-theory', 'ideals']"
81,What is algebraic geometry?,What is algebraic geometry?,,"I am a second year physics undergrad, loooking to explore some areas of pure mathematics. A word that often pops up on the internet is algebraic geometry. What is this algebraic geometry exactly? Please could you give a less technical answer to describe the what this field does and how? I have done linear algebra, some group and representation theory, and some basic point set topology all from mathematical physics textbooks. Also, a brief overview of the prerequisites to study and do research in the field. I know that commutative algebra, and topology is used, but in exactly what way and how are they inter-connected? How exactly do you mix algebra with topology? Thanks!","I am a second year physics undergrad, loooking to explore some areas of pure mathematics. A word that often pops up on the internet is algebraic geometry. What is this algebraic geometry exactly? Please could you give a less technical answer to describe the what this field does and how? I have done linear algebra, some group and representation theory, and some basic point set topology all from mathematical physics textbooks. Also, a brief overview of the prerequisites to study and do research in the field. I know that commutative algebra, and topology is used, but in exactly what way and how are they inter-connected? How exactly do you mix algebra with topology? Thanks!",,['abstract-algebra']
82,Is $A \times B$ the same as $A \oplus B$?,Is  the same as ?,A \times B A \oplus B,"When $A, B$ are $K$-modules, then $A \times B$ is the same as $A \oplus B$. Let $A, B$ be two $K$-algebras, where $K$ is a field. Is $A \times B$ the same as $A \oplus B$? Thank you very much. Edit: $A \times B$ is direct product and $A \oplus B$ is direct sum. Edit: I am asking this question because I am confused with Lemma 1.6 on page 46 of the book Elements of representations of associative algebras . It is said that $\{e_1, \ldots, e_n\}$ is a complete set of primitive orthogonal idempotents. This means that $A=e_1A\oplus \cdots \oplus e_nA$. But it is said that $A$ is not nessesarily connected (connected means $A$ is a product of two non-trivial algebras, the definition of connected is on page 18 of the book, above example 4.1).","When $A, B$ are $K$-modules, then $A \times B$ is the same as $A \oplus B$. Let $A, B$ be two $K$-algebras, where $K$ is a field. Is $A \times B$ the same as $A \oplus B$? Thank you very much. Edit: $A \times B$ is direct product and $A \oplus B$ is direct sum. Edit: I am asking this question because I am confused with Lemma 1.6 on page 46 of the book Elements of representations of associative algebras . It is said that $\{e_1, \ldots, e_n\}$ is a complete set of primitive orthogonal idempotents. This means that $A=e_1A\oplus \cdots \oplus e_nA$. But it is said that $A$ is not nessesarily connected (connected means $A$ is a product of two non-trivial algebras, the definition of connected is on page 18 of the book, above example 4.1).",,"['abstract-algebra', 'ring-theory', 'notation', 'representation-theory', 'modules']"
83,"A finite, cancellative semigroup is a group","A finite, cancellative semigroup is a group",,"Let $G$ be a finite, nonempty set with an operation $*$ such that $G$ is closed under $*$ and $*$ is associative Given $a,b,c \in G$ with $a*b=a*c$, then $b=c$. Given $a,b,c \in G$ with $b*a=c*a$, then $b=c$. I want to prove that $G$ is a group, but I don't know how to show that there exists an identity $e\in  G$ such that $e*x=x$ and $x*e=x$ $\forall x \in G$. I also don't know how to show that $\forall$ x $\in G$ there exists a $y \in G$ such that $y*x=e$ and $x*y=e$. How do I do this?","Let $G$ be a finite, nonempty set with an operation $*$ such that $G$ is closed under $*$ and $*$ is associative Given $a,b,c \in G$ with $a*b=a*c$, then $b=c$. Given $a,b,c \in G$ with $b*a=c*a$, then $b=c$. I want to prove that $G$ is a group, but I don't know how to show that there exists an identity $e\in  G$ such that $e*x=x$ and $x*e=x$ $\forall x \in G$. I also don't know how to show that $\forall$ x $\in G$ there exists a $y \in G$ such that $y*x=e$ and $x*y=e$. How do I do this?",,"['abstract-algebra', 'group-theory']"
84,When does every group with order divisible by $n$ have a subgroup of order $n$?,When does every group with order divisible by  have a subgroup of order ?,n n,"According to Sylow's theorem, every finite group with order divisible by $p^k$ for some prime $p$ has a subgroup of order $p^k$. Is this the best possible result in this direction? That is, if $n$ is not a power of a prime, does there always exist a group with order divisible by $n$ that does not have a subgroup of order $n$? EDIT: Just to clarify, I am aware that groups like this exist. The standard example seems to be $A_4$, which has order divisible by $6$ but no subgroup of order $6$. What I am looking for is a proof that a counterexample exists for any $n$ that is not a power of a prime.","According to Sylow's theorem, every finite group with order divisible by $p^k$ for some prime $p$ has a subgroup of order $p^k$. Is this the best possible result in this direction? That is, if $n$ is not a power of a prime, does there always exist a group with order divisible by $n$ that does not have a subgroup of order $n$? EDIT: Just to clarify, I am aware that groups like this exist. The standard example seems to be $A_4$, which has order divisible by $6$ but no subgroup of order $6$. What I am looking for is a proof that a counterexample exists for any $n$ that is not a power of a prime.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
85,A subring of the field of fractions of a PID is a PID as well.,A subring of the field of fractions of a PID is a PID as well.,,"Let $A$ be a PID and $R$ a ring such that $A\subset R \subset \operatorname{Frac}(A)$, where $\operatorname{Frac}(A)$ denotes the field of fractions of $A$. How to show $R$ is also a PID? Any hints?","Let $A$ be a PID and $R$ a ring such that $A\subset R \subset \operatorname{Frac}(A)$, where $\operatorname{Frac}(A)$ denotes the field of fractions of $A$. How to show $R$ is also a PID? Any hints?",,"['abstract-algebra', 'commutative-algebra', 'ring-theory', 'principal-ideal-domains', 'integral-domain']"
86,A ring with few invertible elements,A ring with few invertible elements,,"Let $A$ be a ring with $0 \neq 1 $, which has $2^n-1$ invertible elements and less non-invertible elements. Prove that $A$ is a field.","Let $A$ be a ring with $0 \neq 1 $, which has $2^n-1$ invertible elements and less non-invertible elements. Prove that $A$ is a field.",,"['abstract-algebra', 'ring-theory', 'inverse', 'noncommutative-algebra', 'finite-rings']"
87,Finite group with isomorphic normal subgroups and non-isomorphic quotients?,Finite group with isomorphic normal subgroups and non-isomorphic quotients?,,"I know it is possible for a group $G$ to have normal subgroups $H, K$, such that $H\cong K$ but $G/H\not\cong G/K$, but I couldn't think of any examples with $G$ finite. What is an illustrative example?","I know it is possible for a group $G$ to have normal subgroups $H, K$, such that $H\cong K$ but $G/H\not\cong G/K$, but I couldn't think of any examples with $G$ finite. What is an illustrative example?",,"['abstract-algebra', 'group-theory', 'normal-subgroups', 'group-isomorphism', 'quotient-group']"
88,Show that an algebraically closed field must be infinite.,Show that an algebraically closed field must be infinite.,,"Show that an algebraically closed field must be infinite. Answer If F is a finite field with elements $a_1, ... , a_n$ the polynomial $f(X)=1 + \prod_{i=1}^n (X - a_i)$ has no root in F, so F cannot be algebraically closed. My Question Could we not use the same argument if F was countably infinite? Couldn't we say that if F was a field with elements $a_1, a_2, ... $ then the polynomial $f(X) = 1 + \prod_{i=1}^{\infty} (X - a_i)$ does not split over F? Thank you in advance","Show that an algebraically closed field must be infinite. Answer If F is a finite field with elements $a_1, ... , a_n$ the polynomial $f(X)=1 + \prod_{i=1}^n (X - a_i)$ has no root in F, so F cannot be algebraically closed. My Question Could we not use the same argument if F was countably infinite? Couldn't we say that if F was a field with elements $a_1, a_2, ... $ then the polynomial $f(X) = 1 + \prod_{i=1}^{\infty} (X - a_i)$ does not split over F? Thank you in advance",,['abstract-algebra']
89,What are some real-world uses of Octonions?,What are some real-world uses of Octonions?,,"... octonions are the crazy old uncle nobody lets out of the attic: they are nonassociative. Comes from a a quote by John Baez. Clearly, the sucessor to quaterions from the Cayley-Dickson process is a numerical beast, but has anybody found any real-world uses for them? For example, quaterions have a nice connection to computer graphics through the connection to SO(4), and that alone makes them worth studying. What can be done with a nonassociative algebra like the octonions? Note: simply mentioning that they have applications in fields such as string theory, special relativity, and quantum logic. is not what I'm looking for (I can read wikipedia too). A specific example, especially one that is geared to someone who is not a mathematician by trade would be nice!","... octonions are the crazy old uncle nobody lets out of the attic: they are nonassociative. Comes from a a quote by John Baez. Clearly, the sucessor to quaterions from the Cayley-Dickson process is a numerical beast, but has anybody found any real-world uses for them? For example, quaterions have a nice connection to computer graphics through the connection to SO(4), and that alone makes them worth studying. What can be done with a nonassociative algebra like the octonions? Note: simply mentioning that they have applications in fields such as string theory, special relativity, and quantum logic. is not what I'm looking for (I can read wikipedia too). A specific example, especially one that is geared to someone who is not a mathematician by trade would be nice!",,"['abstract-algebra', 'octonions', 'division-algebras']"
90,How to compute localizations of quotients of polynomial rings,How to compute localizations of quotients of polynomial rings,,"At the moment I'm trying to understand the concept of localizations of rings / modules. I have done some exercises (using the book of Atiyah / MacDonald) and I will do some more, but a more practical question took my attention. I started with an easy example and considered the ring $k[x]_x$. Localizing at $x$ means that $x$ and its powers become units in the localization. Hence $$k[x]_x\cong k[x,x^{-1}]\cong k[x,y]/\langle xy-1\rangle,$$ am I right there for a start? Then I wanted to compute the localization of $k[x,y]/\langle xy-1\rangle$ at $\langle x-1,y-1\rangle$. This should be a maximal ideal in the quotient, since $\langle xy-1\rangle\subset\langle x-1,y-1\rangle$. (Geometrically, what does this mean here? The maximal ideal corresponds to the point $p=(1,1)$ on the hyperbola $xy-1=0$, so we ""gather only local information around $p$"" in some way?) Localization commutes with the quotient, thus $$(k[x,y]/\langle xy-1\rangle)_{\langle x-1,y-1\rangle}\cong k[x,y]_{\langle x-1,y-1\rangle}/\langle xy-1\rangle_{\langle x-1,y-1\rangle},$$ and here already I am stuck. Is there a general way to compute localized rings of this form, or at least some plan that often works in a case like this? Edit: Can we guess what this should be isomorphic to when looking at it geometrically? Thank you for your help / hints in advance!","At the moment I'm trying to understand the concept of localizations of rings / modules. I have done some exercises (using the book of Atiyah / MacDonald) and I will do some more, but a more practical question took my attention. I started with an easy example and considered the ring $k[x]_x$. Localizing at $x$ means that $x$ and its powers become units in the localization. Hence $$k[x]_x\cong k[x,x^{-1}]\cong k[x,y]/\langle xy-1\rangle,$$ am I right there for a start? Then I wanted to compute the localization of $k[x,y]/\langle xy-1\rangle$ at $\langle x-1,y-1\rangle$. This should be a maximal ideal in the quotient, since $\langle xy-1\rangle\subset\langle x-1,y-1\rangle$. (Geometrically, what does this mean here? The maximal ideal corresponds to the point $p=(1,1)$ on the hyperbola $xy-1=0$, so we ""gather only local information around $p$"" in some way?) Localization commutes with the quotient, thus $$(k[x,y]/\langle xy-1\rangle)_{\langle x-1,y-1\rangle}\cong k[x,y]_{\langle x-1,y-1\rangle}/\langle xy-1\rangle_{\langle x-1,y-1\rangle},$$ and here already I am stuck. Is there a general way to compute localized rings of this form, or at least some plan that often works in a case like this? Edit: Can we guess what this should be isomorphic to when looking at it geometrically? Thank you for your help / hints in advance!",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
91,Explicit Derivation of Weierstrass Normal Form for Cubic Curve,Explicit Derivation of Weierstrass Normal Form for Cubic Curve,,"In page 22-23 of Rational Points on Elliptic Curves by Silverman and Tate, authors explain why is it possible to put every cubic curve into Weierstrass Normal Form. Here are relevant pages: (My question is at the end; I have put a red line across the point I am interested in) Could someone explain to me how to ""work out"" the algebra part? This seems pretty important derivation actually. I am afraid I don't even know how to start the algebra. I would appreciate any suggestions/hints. Thank you :) Related. There is a close question I found here on MSE. However, that particular question asks more about geometrical insight. My question is how does one explicitly go from the general cubic to the equation $xy^2+(ax+b)y=cx^2+dx+e$. By the way, the equation for the general cubic is (just so the coefficients are consistent with the above formula): $ax^3+bx^2y+cxy^2+dy^3+ex^2+fxy+gy^2+hx+iy+j=0$","In page 22-23 of Rational Points on Elliptic Curves by Silverman and Tate, authors explain why is it possible to put every cubic curve into Weierstrass Normal Form. Here are relevant pages: (My question is at the end; I have put a red line across the point I am interested in) Could someone explain to me how to ""work out"" the algebra part? This seems pretty important derivation actually. I am afraid I don't even know how to start the algebra. I would appreciate any suggestions/hints. Thank you :) Related. There is a close question I found here on MSE. However, that particular question asks more about geometrical insight. My question is how does one explicitly go from the general cubic to the equation $xy^2+(ax+b)y=cx^2+dx+e$. By the way, the equation for the general cubic is (just so the coefficients are consistent with the above formula): $ax^3+bx^2y+cxy^2+dy^3+ex^2+fxy+gy^2+hx+iy+j=0$",,"['abstract-algebra', 'number-theory', 'algebraic-geometry', 'elliptic-curves', 'algebraic-curves']"
92,"Does there exist any uncountable group , every proper subgroup of which is countable?","Does there exist any uncountable group , every proper subgroup of which is countable?",,"Does there exist an uncountable group , every proper subgroup of which is  countable ?","Does there exist an uncountable group , every proper subgroup of which is  countable ?",,"['abstract-algebra', 'group-theory']"
93,Information-theoretic aspects of mathematical systems?,Information-theoretic aspects of mathematical systems?,,"It occured to me that when you perform division in some algebraic system, such as $\frac a b = c$ in $\mathbb R$, the division itself represents a relation of sorts between $a$ and $b$, and once you calculate this relation, the resulting element $c$, being 'merely' the relation or some kind of representation of it, has lost the information about what either $a$ or $b$ may have been. So division destroys or weakens information. Other operations have similar peculiarities. Multiplication such as $a b = c$ is very 'lossy' in $\mathbb R$, but not as lossy in $\mathbb N$ since the set of possible divisors of $c$ is finite. So my question is, are there any formalizations which account for (or may be able to account for) this particular aspect of mathematical operations (or functions/relations in general)?","It occured to me that when you perform division in some algebraic system, such as $\frac a b = c$ in $\mathbb R$, the division itself represents a relation of sorts between $a$ and $b$, and once you calculate this relation, the resulting element $c$, being 'merely' the relation or some kind of representation of it, has lost the information about what either $a$ or $b$ may have been. So division destroys or weakens information. Other operations have similar peculiarities. Multiplication such as $a b = c$ is very 'lossy' in $\mathbb R$, but not as lossy in $\mathbb N$ since the set of possible divisors of $c$ is finite. So my question is, are there any formalizations which account for (or may be able to account for) this particular aspect of mathematical operations (or functions/relations in general)?",,"['abstract-algebra', 'combinatorics', 'group-theory', 'information-theory']"
94,Group $G$ whose center $Z(G)$ is cyclic and with $G/Z(G)$ commutative,Group  whose center  is cyclic and with  commutative,G Z(G) G/Z(G),I have some issue to solve following exercise. The exercise is from a French book on Algebra (cours d'Algèbre) from Jean Querré. The book is from the 1970's. If the center $Z(G)$ of a group $G$ is cyclic and $G/Z(G)$ is commutative then there exists a group $H$ such that $H \times H$ is isomorphic to $G/Z(G)$.,I have some issue to solve following exercise. The exercise is from a French book on Algebra (cours d'Algèbre) from Jean Querré. The book is from the 1970's. If the center $Z(G)$ of a group $G$ is cyclic and $G/Z(G)$ is commutative then there exists a group $H$ such that $H \times H$ is isomorphic to $G/Z(G)$.,,"['abstract-algebra', 'group-theory']"
95,Galois groups of polynomials and explicit equations for the roots,Galois groups of polynomials and explicit equations for the roots,,Lets say I have calculated the galois group of some polynomial and I also have the subgroup structure. What's an effective procedure to turn the group into equations for the actual roots of the polynomial in terms of its coefficients assuming the galois group of the polynomial is solvable?,Lets say I have calculated the galois group of some polynomial and I also have the subgroup structure. What's an effective procedure to turn the group into equations for the actual roots of the polynomial in terms of its coefficients assuming the galois group of the polynomial is solvable?,,"['abstract-algebra', 'group-theory']"
96,"Does there exist two non-constant polynomials $f(x),g(x)\in\mathbb Z[x]$ such that for all integers $m,n$, gcd$(f(m),g(n))=1$?","Does there exist two non-constant polynomials  such that for all integers , gcd?","f(x),g(x)\in\mathbb Z[x] m,n (f(m),g(n))=1","Does there exist two non-constant polynomials $f(x),g(x)\in\mathbb Z[x]$ such that for all integers $m,n$, gcd$(f(m),g(n))=1$? I think there are no such polynomials, but how to prove?","Does there exist two non-constant polynomials $f(x),g(x)\in\mathbb Z[x]$ such that for all integers $m,n$, gcd$(f(m),g(n))=1$? I think there are no such polynomials, but how to prove?",,"['abstract-algebra', 'number-theory', 'field-theory', 'algebraic-number-theory']"
97,Can algebraic numbers be compared using only rational arithmetic?,Can algebraic numbers be compared using only rational arithmetic?,,"I was working on a program to carry out some computations, and ran into an issue of needing to compare some algebraic numbers, but not having enough precision to do it without exact arithmetic, and not knowing how to do it with exact arithmetic. A little algebra shows that the statement $$a+b\sqrt{n}>0$$ is equivalent to asking that either $a^2>nb^2$ and $a>0$ or $nb^2>a^2$ and $b>0$. In particular, this means that we can easily compute the order on $\mathbb Q(\sqrt{5})$ using only rational arithmetic on the coefficients of polynomials in $\sqrt{5}$. However, it seems not so clear how to generalize this reasoning even to an example like deciding whether $a+b\sqrt[3]{n}+c\sqrt[3]{n}^2$ is positive. In general, suppose that $f$ is an irreducible polynomial in $\mathbb Q[x]$ and has some real root $\alpha$. Let $F=\mathbb Q[x]/(f)\cong \mathbb Q(\alpha)$ be the corresponding field extension. This field clearly can be ordered, as it is identified with a subfield of $\mathbb R$. Is it possible to compute an explicit order* on $F$ using only rational arithmetic? I feel that this must be possible, but can't figure out how. I'm most interested in whether, for each fixed field extension $F$, there exists an algorithm taking as input a polynomial in $\alpha$ of degree less than $\deg f$ and deciding whether it is positive or not, using a bounded number of operations. I want this primarily for field extensions of low degree, so I'm less interested in how the complexity grows as $F$ becomes more complex than in how algorithms tailored to a single $F$ fare. (*Obviously, I'm most interested in being able to compute the order on $\mathbb Q(\alpha)$ inherited from $\mathbb R$, but given that this field is isomorphic to $\mathbb Q(\alpha')$ for any other root of $f$, there are probably multiple orders - any of which would be interesting to compute)","I was working on a program to carry out some computations, and ran into an issue of needing to compare some algebraic numbers, but not having enough precision to do it without exact arithmetic, and not knowing how to do it with exact arithmetic. A little algebra shows that the statement $$a+b\sqrt{n}>0$$ is equivalent to asking that either $a^2>nb^2$ and $a>0$ or $nb^2>a^2$ and $b>0$. In particular, this means that we can easily compute the order on $\mathbb Q(\sqrt{5})$ using only rational arithmetic on the coefficients of polynomials in $\sqrt{5}$. However, it seems not so clear how to generalize this reasoning even to an example like deciding whether $a+b\sqrt[3]{n}+c\sqrt[3]{n}^2$ is positive. In general, suppose that $f$ is an irreducible polynomial in $\mathbb Q[x]$ and has some real root $\alpha$. Let $F=\mathbb Q[x]/(f)\cong \mathbb Q(\alpha)$ be the corresponding field extension. This field clearly can be ordered, as it is identified with a subfield of $\mathbb R$. Is it possible to compute an explicit order* on $F$ using only rational arithmetic? I feel that this must be possible, but can't figure out how. I'm most interested in whether, for each fixed field extension $F$, there exists an algorithm taking as input a polynomial in $\alpha$ of degree less than $\deg f$ and deciding whether it is positive or not, using a bounded number of operations. I want this primarily for field extensions of low degree, so I'm less interested in how the complexity grows as $F$ becomes more complex than in how algorithms tailored to a single $F$ fare. (*Obviously, I'm most interested in being able to compute the order on $\mathbb Q(\alpha)$ inherited from $\mathbb R$, but given that this field is isomorphic to $\mathbb Q(\alpha')$ for any other root of $f$, there are probably multiple orders - any of which would be interesting to compute)",,"['abstract-algebra', 'field-theory', 'algorithms', 'ordered-fields']"
98,Difference between linear map and homomorphism,Difference between linear map and homomorphism,,"I came across the following definition: Given a ring $A$, with a unit $1 \in A$, and $A$-modules $M$ and $N$, we denote by $Hom(M, N)$ or $Hom_A(M, N)$ the space of $A$-linear maps from $M$ to $N$. My question is: what exactly is the difference between homomorphism and a linear map? I can see that linearity is defined in terms of a vector space or module and homomorphism in terms of groups. But every linear map is a homomorphism and when treating a group as a one dimensional vector space over itself, every homo. is also a linear map. This makes me think they are kind of the same. Is it ok to think of it that way? Or am I confused? Because I feel confused. Thanks once again for your help!","I came across the following definition: Given a ring $A$, with a unit $1 \in A$, and $A$-modules $M$ and $N$, we denote by $Hom(M, N)$ or $Hom_A(M, N)$ the space of $A$-linear maps from $M$ to $N$. My question is: what exactly is the difference between homomorphism and a linear map? I can see that linearity is defined in terms of a vector space or module and homomorphism in terms of groups. But every linear map is a homomorphism and when treating a group as a one dimensional vector space over itself, every homo. is also a linear map. This makes me think they are kind of the same. Is it ok to think of it that way? Or am I confused? Because I feel confused. Thanks once again for your help!",,"['abstract-algebra', 'terminology']"
99,Is $\mathbf{C}$ the algebraic closure of any field other than $\mathbf{R}$?,Is  the algebraic closure of any field other than ?,\mathbf{C} \mathbf{R},"It seems to me (intuitively) that there should be no other fields whose algebraic closure is $\mathbf{C}$, even though I have no reason to believe it. The facts I've been using to formulate an argument are $[\mathbf{C}\mathbin{:}\mathbf{R}]=2$ and $\mathbf{R}$ is the only field with the usual analytic properties. I mean, it seems that for the complexes to be even defined we need to reference the analytic properties of the reals. Also, we know that such a field would have to be uncountable, right? This question might end up being trivial, but any information or references would be appreciated.","It seems to me (intuitively) that there should be no other fields whose algebraic closure is $\mathbf{C}$, even though I have no reason to believe it. The facts I've been using to formulate an argument are $[\mathbf{C}\mathbin{:}\mathbf{R}]=2$ and $\mathbf{R}$ is the only field with the usual analytic properties. I mean, it seems that for the complexes to be even defined we need to reference the analytic properties of the reals. Also, we know that such a field would have to be uncountable, right? This question might end up being trivial, but any information or references would be appreciated.",,"['abstract-algebra', 'field-theory']"
