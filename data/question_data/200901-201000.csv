,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Book recommendations for Riemannian geometry,Book recommendations for Riemannian geometry,,"I'm doing a PhD thesis about Riemannian geometry and i would like to improve my knowledge. I know all the basic defintions and concepts (I have the ""semiRiemannian geometry with applications to relativity"" from Barret O'neill and i have studied it during the degree and the master ). So i'm looking for books with the following to characteritics: First, a book which go forward with respect the content of O'neill's book. Secondly, it would be great that the author/s stop in the basic (or advanced) concepts to think about it and not just give the defintions and going on. Thanks for advance","I'm doing a PhD thesis about Riemannian geometry and i would like to improve my knowledge. I know all the basic defintions and concepts (I have the ""semiRiemannian geometry with applications to relativity"" from Barret O'neill and i have studied it during the degree and the master ). So i'm looking for books with the following to characteritics: First, a book which go forward with respect the content of O'neill's book. Secondly, it would be great that the author/s stop in the basic (or advanced) concepts to think about it and not just give the defintions and going on. Thanks for advance",,"['differential-geometry', 'reference-request', 'riemannian-geometry', 'book-recommendation']"
1,Geometric interpretation of $\dfrac{dg(t)}{dt}$,Geometric interpretation of,\dfrac{dg(t)}{dt},"In Riemannian geometry for some reasons we consider a variation of metric then we compute its time-depending derivative. I want to know why we do this? Is it similar to finding the critical points? If so what is the definition and intuition of critical points here? Update(29-03-2021) For example sometimes in the easiest case, we consider first-order deformation $$g_t=g+th,$$ for some symmetric tensor $h$ . Is in this case $\dfrac{dg_t}{dt}|_{t=0}=h$ ? If so what is the point of considering $g_t$ , instead we can work with $h$ ? more generally if we consider $$g_t=g+th+t^2k+\cdots ,$$ then I think the result is same as above. i.e. $\dfrac{dg_t}{dt}|_{t=0}=h$ . Am I right? Also I forgot from calculus that why we evaluate it at $t=0$ after taking derivative?","In Riemannian geometry for some reasons we consider a variation of metric then we compute its time-depending derivative. I want to know why we do this? Is it similar to finding the critical points? If so what is the definition and intuition of critical points here? Update(29-03-2021) For example sometimes in the easiest case, we consider first-order deformation for some symmetric tensor . Is in this case ? If so what is the point of considering , instead we can work with ? more generally if we consider then I think the result is same as above. i.e. . Am I right? Also I forgot from calculus that why we evaluate it at after taking derivative?","g_t=g+th, h \dfrac{dg_t}{dt}|_{t=0}=h g_t h g_t=g+th+t^2k+\cdots , \dfrac{dg_t}{dt}|_{t=0}=h t=0","['differential-geometry', 'differential-topology', 'riemannian-geometry']"
2,Divergence in local frame,Divergence in local frame,,"I am trying to solve a problem from John M. Lee's Riemannian Manifolds . The goal is to show that if we have a vector field $X = X^i E_i$ in terms of some local frame $\{E_i\}$ on $U\subset M$ , then we can write the divergence in terms of covariant derivatives: $$\text{div}(X) = X^i_{;i}$$ Where (in a coordinate frame/coframe): $$\nabla X = X^i_{;j}\partial_i\otimes dx^j = \left(\partial_j X^i + X^k\Gamma_{jk}^i\right)\partial_i\otimes dx^j$$ The hint says to ""show that it suffices to prove the formula at the origin in normal coordinates."" Let $p\in U$ be given. Since $\{E_i\}$ is a local frame, $\{E_i|_p\}$ forms a basis of $T_pM$ . Using the Gram-Schmidt algorithm, we can construct an orthonormal basis $\{F_i|_p\}$ . With the isomorphism $F : \Bbb R^n\to T_pM$ together with the exponential map, $\varphi = F^{-1} \circ \exp_p^{-1} : \mathcal U \to \Bbb R^n$ gives us normal coordinates centered at $p$ with $\mathcal U \subset U$ . Now using the coordinate expression for divergence along with properties of normal coordinates: \begin{align*} \text{div}(X)|_p &= \left(\frac{1}{\sqrt{\text{det}(g_{ij})}}\sum_{k=1}^n\frac{\partial X^k\sqrt{\text{det}(g_{ij})}}{\partial x^k}\right)\Bigg|_p\\ &= \sum_{k=1}^n\frac{\partial X^k}{dx^k}\bigg|_p\\ &= X^k_{;k}(p) - X^\ell\Gamma_{k\ell}^k(p)\\ &= X^k_{;k}(p) \end{align*} What I'm having a difficult time understanding is the ""suffices"" part of the hint. How do I extend that $\text{div}(X)=X^i_{;i}$ in normal coordinates to an arbitrary local frame? Especially since the definition of $X^i_{;j}$ includes a partial derivative, which need not be defined on an arbitrary frame? A similar question is asked here: Divergence as the trace of total covariant derivative? But I don't think it answers my question. That $X^i_{;i} = \text{tr}(\nabla X)$ gives a reason for frame-independence, but I don't see how it relates to the calculation in normal coordinates.","I am trying to solve a problem from John M. Lee's Riemannian Manifolds . The goal is to show that if we have a vector field in terms of some local frame on , then we can write the divergence in terms of covariant derivatives: Where (in a coordinate frame/coframe): The hint says to ""show that it suffices to prove the formula at the origin in normal coordinates."" Let be given. Since is a local frame, forms a basis of . Using the Gram-Schmidt algorithm, we can construct an orthonormal basis . With the isomorphism together with the exponential map, gives us normal coordinates centered at with . Now using the coordinate expression for divergence along with properties of normal coordinates: What I'm having a difficult time understanding is the ""suffices"" part of the hint. How do I extend that in normal coordinates to an arbitrary local frame? Especially since the definition of includes a partial derivative, which need not be defined on an arbitrary frame? A similar question is asked here: Divergence as the trace of total covariant derivative? But I don't think it answers my question. That gives a reason for frame-independence, but I don't see how it relates to the calculation in normal coordinates.","X = X^i E_i \{E_i\} U\subset M \text{div}(X) = X^i_{;i} \nabla X = X^i_{;j}\partial_i\otimes dx^j = \left(\partial_j X^i + X^k\Gamma_{jk}^i\right)\partial_i\otimes dx^j p\in U \{E_i\} \{E_i|_p\} T_pM \{F_i|_p\} F : \Bbb R^n\to T_pM \varphi = F^{-1} \circ \exp_p^{-1} : \mathcal U \to \Bbb R^n p \mathcal U \subset U \begin{align*}
\text{div}(X)|_p &= \left(\frac{1}{\sqrt{\text{det}(g_{ij})}}\sum_{k=1}^n\frac{\partial X^k\sqrt{\text{det}(g_{ij})}}{\partial x^k}\right)\Bigg|_p\\
&= \sum_{k=1}^n\frac{\partial X^k}{dx^k}\bigg|_p\\
&= X^k_{;k}(p) - X^\ell\Gamma_{k\ell}^k(p)\\
&= X^k_{;k}(p)
\end{align*} \text{div}(X)=X^i_{;i} X^i_{;j} X^i_{;i} = \text{tr}(\nabla X)","['differential-geometry', 'riemannian-geometry', 'divergence-operator']"
3,geodesic line on intersecting planes,geodesic line on intersecting planes,,"I have a problem and I don't understand how to solve it, please help! Prove that if two surfaces in $R^3$ intersect along a curve that is geodesic on both surfaces, and the tangent planes to the surfaces at any point of the curve do not coincide (in this situation, the surfaces are said to intersect transversely), then this curve is straight.","I have a problem and I don't understand how to solve it, please help! Prove that if two surfaces in intersect along a curve that is geodesic on both surfaces, and the tangent planes to the surfaces at any point of the curve do not coincide (in this situation, the surfaces are said to intersect transversely), then this curve is straight.",R^3,['differential-geometry']
4,Is there a global definition of the tangent bundle?,Is there a global definition of the tangent bundle?,,"The tangent bundle of a smooth manifold is usually defined by equipping the disjoint union of the tangent spaces with a smooth structure. Is there a way to define the tangent bundle as a vector bundle first, and then obtain the tangent spaces as the fibres?","The tangent bundle of a smooth manifold is usually defined by equipping the disjoint union of the tangent spaces with a smooth structure. Is there a way to define the tangent bundle as a vector bundle first, and then obtain the tangent spaces as the fibres?",,"['differential-geometry', 'smooth-manifolds', 'vector-bundles', 'tangent-bundle']"
5,Jacobian and changing coordinates proof [duplicate],Jacobian and changing coordinates proof [duplicate],,"This question already has an answer here : Wedge product and change of variables (1 answer) Closed 4 years ago . Consider a transformation between the coordinates given by $x^{a}$ to another system given by coordinates $x^{'a}$ . So a transformation of the type: $x^{a} \rightarrow x^{'a}$ . With that I can construct the Jacobian matrix: $\bigg[ \frac{\partial x^{'a}}{\partial x^{b}} \bigg]$ The determinant of this matrix is called the Jacobian determinant, hereafter J. I want to prove that: $dx^{'1}dx^{'2}...dx^{'N} = Jdx^{1}dx^{2}...dx^{N}$ where primed system is cartesian and the unprimed one is a general set of coordinates. How can I do that? I remembered that in transformation of coordinate is valid to write (using the summation convention) that: $dx^{'a} = \frac{\partial x^{'a}}{\partial x^{b}}dx^{b}$ And tried to apply that to proof that I need, but, I was not abble to go anywhere because of lot of terms shows up.  Actually, in the case we considerer just 2 dimensions I ended up with something like that: $dx^{'1}dx^{'2} =  \bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)(dx^{1})^{2}+ \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg)(dx^{2})^{2} + \bigg[\bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg) + \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)\bigg]dx^{1}dx^{2} $ As is possible to see, the last two therms inside the brackets are almost the Jacobian determinant. So my question is: what is wrong in this approach? I did found this article with a different ideia of the proof. But why should I use that instead of the mine? And also: how could I generalize the ideia of this article for N-dimensions?","This question already has an answer here : Wedge product and change of variables (1 answer) Closed 4 years ago . Consider a transformation between the coordinates given by to another system given by coordinates . So a transformation of the type: . With that I can construct the Jacobian matrix: The determinant of this matrix is called the Jacobian determinant, hereafter J. I want to prove that: where primed system is cartesian and the unprimed one is a general set of coordinates. How can I do that? I remembered that in transformation of coordinate is valid to write (using the summation convention) that: And tried to apply that to proof that I need, but, I was not abble to go anywhere because of lot of terms shows up.  Actually, in the case we considerer just 2 dimensions I ended up with something like that: As is possible to see, the last two therms inside the brackets are almost the Jacobian determinant. So my question is: what is wrong in this approach? I did found this article with a different ideia of the proof. But why should I use that instead of the mine? And also: how could I generalize the ideia of this article for N-dimensions?",x^{a} x^{'a} x^{a} \rightarrow x^{'a} \bigg[ \frac{\partial x^{'a}}{\partial x^{b}} \bigg] dx^{'1}dx^{'2}...dx^{'N} = Jdx^{1}dx^{2}...dx^{N} dx^{'a} = \frac{\partial x^{'a}}{\partial x^{b}}dx^{b} dx^{'1}dx^{'2} =  \bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)(dx^{1})^{2}+ \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg)(dx^{2})^{2} + \bigg[\bigg( \frac{\partial x^{'1}}{\partial x^{1}} \frac{\partial x^{'2}}{\partial x^{2}} \bigg) + \bigg( \frac{\partial x^{'1}}{\partial x^{2}} \frac{\partial x^{'2}}{\partial x^{1}} \bigg)\bigg]dx^{1}dx^{2} ,"['differential-geometry', 'algebraic-geometry', 'riemannian-geometry', 'analytic-geometry', 'exterior-algebra']"
6,"Intuitive understanding of 2-forms, (1,1)-tensors, and other fundamental objects of exterior algebra or tensor algebra","Intuitive understanding of 2-forms, (1,1)-tensors, and other fundamental objects of exterior algebra or tensor algebra",,"My background consists mostly of a good level in linear algebra, abstract algebra, undergrad calculus, topology & probability, and some working knowledge of geometric algebra and category theory. I'm currently learning differential geometry and tensor calculus, and hope to move onto geometric calculus and information geometry. However, I have a lot of trouble integrating abstract formulas and concepts if I don't have some intuitive mental model on which to rely. My question is very broad: what is a coherent system of visualization for $(m,n)$ -tensors ? Since this question can be answered in many ways, I'll be asking more specific questions as well. To help guide your answer (and hopefully help other learners like myself), here's some insight I've built up over the years on how to visualize some basic objects. Scalars are simply single numbers from a field $F$ . They are $(0,0)$ -tensors. A scalar field can be viewed as a coloring over a manifold, where each point gets darker and redder as the scalar at that point tends towards positive infinity, whiter as it tends towards zero, and darker and bluer as it tends towards negative infinity. Vectors are $(1,0)$ -tensors. I understand and visualize them either as an oriented arrow, or as a ""line with cyclic colors"" (1D subspace with a repeated hue shift). The norm of your vector is equivalently the distance from the tail to the head of the arrow, or the distance to get from a specific shade to the next occurence of the exact same shade on your line. The order of the hue shift RGB or BGR encodes the orientation. This second picture is often useful in seeing geometric algebra as the algebra of subspaces of a vector space. Vectors, in 'matrix form', are column vectors ( $n*1$ matrices). A vector field is a flow field where a particle follows the arrows, with one vector defined at each point on the manifold (like arrows for winds on a globe of the earth in the context meteorological data). A smooth vector field can also (sometimes? always?) be seen as a foliation, a space of solutions to a differential equation which divides the spaces into non-intersecting flow curves. Covectors, 1-forms, $(0,1)$ -tensors, I think I understand as well. By definition they are linear functions from $V$ to the underlying field $F$ . They can be seen stacks of regularly spaced  parallel (hyper)planes. They're your row vectors ( $1*n$ matrices). They ""eat"" vectors by returning a scalar, operating somewhat like a dot product (for a euclidean metric). You can also see covectors as stacked (hyper)planes that hue-shift, and if the plane that passes through the origin is ""pure red"", the result of the dot-product-like operation (applying the linear form to a vector $v$ ) is the number of times our vector $v$ passes through ""pure red"" planes, plus whatever fraction of the way to the next red plane the arrowhead happens to lie on. A higher norm for a covector means that the sheets in the stack are closer together (ie, the same vector will pass through more sheets, returning a larger scalar). Covector fields are like topographical maps. Say a particle is going through the covector field: the more it goes ""towards the top of the mountain"", the denser the material it has to go through becomes, the more its speed is ""eaten up"" by the covector field; and vice-versa, it speeds up when ""going down the mountain"", because it's going from a dense medium to a fluid medium. 2-vectors, bivectors, $(2,0)$ -tensors, are oriented areas. You can see them as parallelograms (formed as the outer product of two non-colinear vectors) or ellipses, or any 2D shape, so long as orientation (clockwise vs counter-clockwise rotation in the plane; or equivalently transparent vs textured side of the plane) and area (the norm of the bivector) are the same. I wonder if for this reason there's also a hue-shift-type visualization for bivectors; like representing a bivector as a specific ""hue-shift tiling"" of a full 2D subspace ? I can't quite work it out. Also, given that bivectors are $(2,0)$ -tensors, am I correct to assume that you could make a ""column vector of column vectors"" that could ""eat two successive 1-forms, or a single 2-form"", as a ""more appropriate"" matrix form ? (By this latter question, I mean that I have trouble understanding how $(2,0)$ -tensors, $(1,1)$ -tensors and $(0,2)$ -tensors should all look like rectangular matrices; it seems incoherent to me, like an abuse of notation: perhaps useful, but in need of clarification.) As for 2-forms (which I guess one could also call cobivectors or bicovectors), I'd expect there be some intuitive version of ""a shape, or repetition of parallel shapes, that eats bivectors"" but I can't wrap my head around it. Part of my gut intuition from working on geometric algebra, which associates covectors to $(n-1)$ -vectors, and associates applying a covector to a vector to obtaining a pseudoscalar value from the product of an $(n-1)$ -vector and a $1$ -vector which are independent (though I have no idea if this analogy is legitimate) tells me I'd have to take a stack of $(n-2)$ -dimensional spaces to represent a 2-form. Would this mean that if I consider a 1D linear subspace of $R^3$ and the set of all affine spaces parallel to this line (like a bundle of straws stretching out infinitely), a ""closeness"" or density of each straw to its neighbors would somehow encode the intensity of a flow through a bivector, and thus a 2-form ? Given that they are (0,2)-tensors, am I correct to assume that they could be represented as ""a line vector of line vectors"" ? And if this image of 2-forms is correct, how does this relate to the idea of metric tensors ? Linear maps from $F^n \to F^m$ are $(m*n)$ -matrices. They can be understood as mapping $(1,0)$ -tensors of dimension $n$ to $(1,0)$ -tensors of dimension $m$ (by right multiplication with a vector), so I suppose they are $(1,1)$ -tensors (as in, the covariant part of the matrix is combined with the contravariant column vector, and only the contravariant part of the matrix/(1,1)-tensor remains). This is also coherent with left multiplication of an $(m*n)$ -matrix by an $m$ -dimensional covector (like in the context of Markov chains), as in it consumes an $m$ -dimensional $(0,1)$ -tensor and returns an $n$ -dimensional $(0,1)$ -tensor. Finally, you often see generalized dot products (bilinear symetric forms) being used with the pattern "" $x^TSy$ "", with $S$ a symmetric matrix. Am I correct to assume that they are $(1,1)$ -tensors which are simultaneously fed $(1,0)$ -tensor and a $(0,1)$ -tensor ? Or are they indeed like ""metric tensors"", and thus (0,2)-tensors ? Given what you've just read, are there any glaring mistakes in this picture ? Are there useful visualization and insights you care to share that might go well with the current picture (both to illustrate intuitive and counter-intuitive ideas) ? Visualization tools or models for higher-valence mixed tensors ? For tensor fields (other than the basic ""bundle = hairy manifold where each hair is a tensor"") ? Of the idea of vectors, covectors, etc, $(m,n)$ -tensors in infinite-dimensional vector spaces (which I haven't even approached here) ? Any insight on seeing tensor operations as operations on hypermatrices ? All insight from any branch that touches upon these themes is welcome ! I'm sorry if this makes a lot of diffuse questions, it's just that I'm having trouble making the whole picture of "" $(m,n)$ -tensors, their geometry, their algebraic representation and how one computes with them"" all coherent in my mind. Thanks for reading, and for your help !","My background consists mostly of a good level in linear algebra, abstract algebra, undergrad calculus, topology & probability, and some working knowledge of geometric algebra and category theory. I'm currently learning differential geometry and tensor calculus, and hope to move onto geometric calculus and information geometry. However, I have a lot of trouble integrating abstract formulas and concepts if I don't have some intuitive mental model on which to rely. My question is very broad: what is a coherent system of visualization for -tensors ? Since this question can be answered in many ways, I'll be asking more specific questions as well. To help guide your answer (and hopefully help other learners like myself), here's some insight I've built up over the years on how to visualize some basic objects. Scalars are simply single numbers from a field . They are -tensors. A scalar field can be viewed as a coloring over a manifold, where each point gets darker and redder as the scalar at that point tends towards positive infinity, whiter as it tends towards zero, and darker and bluer as it tends towards negative infinity. Vectors are -tensors. I understand and visualize them either as an oriented arrow, or as a ""line with cyclic colors"" (1D subspace with a repeated hue shift). The norm of your vector is equivalently the distance from the tail to the head of the arrow, or the distance to get from a specific shade to the next occurence of the exact same shade on your line. The order of the hue shift RGB or BGR encodes the orientation. This second picture is often useful in seeing geometric algebra as the algebra of subspaces of a vector space. Vectors, in 'matrix form', are column vectors ( matrices). A vector field is a flow field where a particle follows the arrows, with one vector defined at each point on the manifold (like arrows for winds on a globe of the earth in the context meteorological data). A smooth vector field can also (sometimes? always?) be seen as a foliation, a space of solutions to a differential equation which divides the spaces into non-intersecting flow curves. Covectors, 1-forms, -tensors, I think I understand as well. By definition they are linear functions from to the underlying field . They can be seen stacks of regularly spaced  parallel (hyper)planes. They're your row vectors ( matrices). They ""eat"" vectors by returning a scalar, operating somewhat like a dot product (for a euclidean metric). You can also see covectors as stacked (hyper)planes that hue-shift, and if the plane that passes through the origin is ""pure red"", the result of the dot-product-like operation (applying the linear form to a vector ) is the number of times our vector passes through ""pure red"" planes, plus whatever fraction of the way to the next red plane the arrowhead happens to lie on. A higher norm for a covector means that the sheets in the stack are closer together (ie, the same vector will pass through more sheets, returning a larger scalar). Covector fields are like topographical maps. Say a particle is going through the covector field: the more it goes ""towards the top of the mountain"", the denser the material it has to go through becomes, the more its speed is ""eaten up"" by the covector field; and vice-versa, it speeds up when ""going down the mountain"", because it's going from a dense medium to a fluid medium. 2-vectors, bivectors, -tensors, are oriented areas. You can see them as parallelograms (formed as the outer product of two non-colinear vectors) or ellipses, or any 2D shape, so long as orientation (clockwise vs counter-clockwise rotation in the plane; or equivalently transparent vs textured side of the plane) and area (the norm of the bivector) are the same. I wonder if for this reason there's also a hue-shift-type visualization for bivectors; like representing a bivector as a specific ""hue-shift tiling"" of a full 2D subspace ? I can't quite work it out. Also, given that bivectors are -tensors, am I correct to assume that you could make a ""column vector of column vectors"" that could ""eat two successive 1-forms, or a single 2-form"", as a ""more appropriate"" matrix form ? (By this latter question, I mean that I have trouble understanding how -tensors, -tensors and -tensors should all look like rectangular matrices; it seems incoherent to me, like an abuse of notation: perhaps useful, but in need of clarification.) As for 2-forms (which I guess one could also call cobivectors or bicovectors), I'd expect there be some intuitive version of ""a shape, or repetition of parallel shapes, that eats bivectors"" but I can't wrap my head around it. Part of my gut intuition from working on geometric algebra, which associates covectors to -vectors, and associates applying a covector to a vector to obtaining a pseudoscalar value from the product of an -vector and a -vector which are independent (though I have no idea if this analogy is legitimate) tells me I'd have to take a stack of -dimensional spaces to represent a 2-form. Would this mean that if I consider a 1D linear subspace of and the set of all affine spaces parallel to this line (like a bundle of straws stretching out infinitely), a ""closeness"" or density of each straw to its neighbors would somehow encode the intensity of a flow through a bivector, and thus a 2-form ? Given that they are (0,2)-tensors, am I correct to assume that they could be represented as ""a line vector of line vectors"" ? And if this image of 2-forms is correct, how does this relate to the idea of metric tensors ? Linear maps from are -matrices. They can be understood as mapping -tensors of dimension to -tensors of dimension (by right multiplication with a vector), so I suppose they are -tensors (as in, the covariant part of the matrix is combined with the contravariant column vector, and only the contravariant part of the matrix/(1,1)-tensor remains). This is also coherent with left multiplication of an -matrix by an -dimensional covector (like in the context of Markov chains), as in it consumes an -dimensional -tensor and returns an -dimensional -tensor. Finally, you often see generalized dot products (bilinear symetric forms) being used with the pattern "" "", with a symmetric matrix. Am I correct to assume that they are -tensors which are simultaneously fed -tensor and a -tensor ? Or are they indeed like ""metric tensors"", and thus (0,2)-tensors ? Given what you've just read, are there any glaring mistakes in this picture ? Are there useful visualization and insights you care to share that might go well with the current picture (both to illustrate intuitive and counter-intuitive ideas) ? Visualization tools or models for higher-valence mixed tensors ? For tensor fields (other than the basic ""bundle = hairy manifold where each hair is a tensor"") ? Of the idea of vectors, covectors, etc, -tensors in infinite-dimensional vector spaces (which I haven't even approached here) ? Any insight on seeing tensor operations as operations on hypermatrices ? All insight from any branch that touches upon these themes is welcome ! I'm sorry if this makes a lot of diffuse questions, it's just that I'm having trouble making the whole picture of "" -tensors, their geometry, their algebraic representation and how one computes with them"" all coherent in my mind. Thanks for reading, and for your help !","(m,n) F (0,0) (1,0) n*1 (0,1) V F 1*n v v (2,0) (2,0) (2,0) (1,1) (0,2) (n-1) (n-1) 1 (n-2) R^3 F^n \to F^m (m*n) (1,0) n (1,0) m (1,1) (m*n) m m (0,1) n (0,1) x^TSy S (1,1) (1,0) (0,1) (m,n) (m,n)","['differential-geometry', 'exterior-algebra', 'geometric-algebras', 'tensor-rank']"
7,How is this expression relating circumference to 2D curvature derived? From MTW Gravitation,How is this expression relating circumference to 2D curvature derived? From MTW Gravitation,,"The excerpt below is from Box 14.1 of Misner Thorne and Wheeler's Gravitation. This equation is presented in the quoted text: $$\lim_{\epsilon\to0}\frac{6}{\epsilon^{2}}\left(1-\frac{\text{circumference}}{2\pi\epsilon}\right)=\kappa_{1}\kappa_{2}$$ The authors do not, however, derive the expression, and their verbal description of the procedure for deriving it is vague.  How is it established? I'm adding the first paragraph of Box 14.1, since it might provide a clue as to how they derived the expression in question. This is an illustration of how I see the situation.  The surface is given by the binary quadratic form of the second degree Taylor series expansion about a critical point (MacLauren series?).  The principle curves are axis aligned.  The closed curve on the surface is the graph of the image of the unit circle under the mapping $q.$ That is $\left\{\cos \theta,\sin \theta,q[\cos \theta,\sin \theta]\right\}$ .  This is not the ""circle"" for which we seek the circumference.  That set of points is determined by constant ""radial"" pathlength within the surface. Since this is currently a ""back-burner"" question, I can't invest the time to produce a more accurate illustration in the near future.  But it is also one of those problems that keeps me up at night.  I feel like I ""should"" be able to solve it. This is an illustration showing intrinsic ""circles"" on a quadratic surface.  The blue curves are simply the projection of the red ""circles"" onto the $X\times{Y}$ plane.","The excerpt below is from Box 14.1 of Misner Thorne and Wheeler's Gravitation. This equation is presented in the quoted text: The authors do not, however, derive the expression, and their verbal description of the procedure for deriving it is vague.  How is it established? I'm adding the first paragraph of Box 14.1, since it might provide a clue as to how they derived the expression in question. This is an illustration of how I see the situation.  The surface is given by the binary quadratic form of the second degree Taylor series expansion about a critical point (MacLauren series?).  The principle curves are axis aligned.  The closed curve on the surface is the graph of the image of the unit circle under the mapping That is .  This is not the ""circle"" for which we seek the circumference.  That set of points is determined by constant ""radial"" pathlength within the surface. Since this is currently a ""back-burner"" question, I can't invest the time to produce a more accurate illustration in the near future.  But it is also one of those problems that keeps me up at night.  I feel like I ""should"" be able to solve it. This is an illustration showing intrinsic ""circles"" on a quadratic surface.  The blue curves are simply the projection of the red ""circles"" onto the plane.","\lim_{\epsilon\to0}\frac{6}{\epsilon^{2}}\left(1-\frac{\text{circumference}}{2\pi\epsilon}\right)=\kappa_{1}\kappa_{2} q. \left\{\cos \theta,\sin \theta,q[\cos \theta,\sin \theta]\right\} X\times{Y}","['differential-geometry', 'surfaces', 'curvature', 'noneuclidean-geometry']"
8,"Flow of Hamiltonian vector fields, time dependent flow","Flow of Hamiltonian vector fields, time dependent flow",,"I have trouble understanding the notion of time dependent flows of Hamiltonian vector fields: Let $(M, \omega)$ be a symplectic manifold, $H:M \rightarrow M$ a Hamiltonian function. question: In my lecture, we haven't properly defined what Hamiltonian function means. Is it correct, that it's just a smooth function? Now let $X_H$ be the Hamiltonian vector field which means $\omega(X_H,.)=-dH$ ). Then the flow of $X_H$ , $\varphi_t$ , is the Hamiltonian flow. Now I know the definition of the flow from differential geometry. Here it means that $X_H(\varphi_t)= \dfrac{d}{dt} \varphi_t$ . Now we have the notion of a time- $t$ map, which I don't understand.  For example, I don't understand the following exercise: Let $\varphi_t:(M, \omega) \rightarrow (M, \omega)$ be the family of diffeomorphisms determined by the time-dependent Hamiltonian function $H : [0, 1] \times M \rightarrow \mathbb{R}$ via $\overset{.}{\varphi} = X_{H_{t}} \circ \varphi_t$ . For each $t \in (0,1)$ , write $\varphi_t$ as time one map of a family of diffeomorphisms determined by a new Hamiltonian function built from $H$ . I don't understand this exercise at all. As I see it, you can get a family of flows $\{\varphi^{s}_t\}_{s \in [0,1]}$ such that for each fixed $s$ , $\varphi^s_t$ is the Hamiltonian flow of the function $H_s: M \rightarrow M$ .  But I don't see why the parameter $t$ of the flow here is identified with the parameter $t \in [0,1]$ of the function $H$ .","I have trouble understanding the notion of time dependent flows of Hamiltonian vector fields: Let be a symplectic manifold, a Hamiltonian function. question: In my lecture, we haven't properly defined what Hamiltonian function means. Is it correct, that it's just a smooth function? Now let be the Hamiltonian vector field which means ). Then the flow of , , is the Hamiltonian flow. Now I know the definition of the flow from differential geometry. Here it means that . Now we have the notion of a time- map, which I don't understand.  For example, I don't understand the following exercise: Let be the family of diffeomorphisms determined by the time-dependent Hamiltonian function via . For each , write as time one map of a family of diffeomorphisms determined by a new Hamiltonian function built from . I don't understand this exercise at all. As I see it, you can get a family of flows such that for each fixed , is the Hamiltonian flow of the function .  But I don't see why the parameter of the flow here is identified with the parameter of the function .","(M, \omega) H:M \rightarrow M X_H \omega(X_H,.)=-dH X_H \varphi_t X_H(\varphi_t)= \dfrac{d}{dt} \varphi_t t \varphi_t:(M, \omega) \rightarrow (M, \omega) H : [0, 1] \times M \rightarrow \mathbb{R} \overset{.}{\varphi} = X_{H_{t}} \circ \varphi_t t \in (0,1) \varphi_t H \{\varphi^{s}_t\}_{s \in [0,1]} s \varphi^s_t H_s: M \rightarrow M t t \in [0,1] H","['differential-geometry', 'symplectic-geometry', 'hamilton-equations']"
9,Is differential geometry related to partial differential equations?,Is differential geometry related to partial differential equations?,,I am doing my PhD in real analysis and besides I am studying differential geometry. I have heard that differential geometry is related to PDE. Can anyone please suggest me some books regarding this? I am also interested to know what kind research work are going on this topic.,I am doing my PhD in real analysis and besides I am studying differential geometry. I have heard that differential geometry is related to PDE. Can anyone please suggest me some books regarding this? I am also interested to know what kind research work are going on this topic.,,"['differential-geometry', 'partial-differential-equations']"
10,Free homotopy class of Riemannian manifold represented by closed geodesic,Free homotopy class of Riemannian manifold represented by closed geodesic,,"I'm trying to prove the following result, which is Problem 6-17 in Lee's Introduction to Riemannian Manifolds : Suppose $(M,g)$ is a compact connected Riemannian manifold. Show that every nontrivial free homotopy class in $M$ is represented by a closed geodesic that has minimal length among all admissible loops in the given free homotopy class. This problem comes with a hint, which breaks the solution down into the following steps: Show the given free homotopy class is represented by a geodesic loop; Show the lengths of such loops have positive greatest lower bound; Choose a sequence of geodesic loops whose lengths approach this bound, and show that a subsequence converges uniformly to a geodesic loop whose length is equal to this greatest lower bound; Show the limiting curve is in the given free homotopy class; Apply the first variation formula to show that the limiting curve is in fact a closed geodesic. I have most of these steps down except 3 and 5. First of all, if I have such a sequence, finding a universally convergent subsequence seems similar in spirit to Arzelà-Ascoli, but finding the right uniformly bounded and equicontinuous sequence of functions so far has eluded me. Second of all, in proving 5, the first variation formula says that if $\Gamma_s(t)$ is a variation (i.e. a homotopy) between the limiting curve $\gamma(t)$ and one of the sequence curves $\gamma_n(t)$ , then $$ \frac{d}{ds}\bigg|_{s=0} L_g(\Gamma_s) = -\int_a^b \langle V, D_t\dot\gamma\rangle\:dt - \sum_{i=1}^{k-1} \left\langle V(a_i), \Delta_i \dot\gamma\right\rangle + \langle V(b),\dot\gamma(b)\rangle - \langle V(a), \dot\gamma(b)\rangle $$ where $V(t) = \frac{d}{ds}\big|_{s=0} \Gamma_s(t)$ is the variation field along $\gamma$ , $(a_0, \ldots, a_k)$ is an admissible partition for $V$ , and $\Delta_i \dot\gamma = \dot\gamma(a_i^+) - \dot\gamma(a_i^-)$ is the ""jump"" in the velocity vector field at $a_i$ . In this case, $V(a) = V(b)$ , so we need to show $\dot\gamma(a) = \dot\gamma(b)$ and $\Delta_i \dot\gamma = 0$ for all $i$ . Since $\Gamma_0$ is a critical point for this variation, this forumula becomes $$ \int_a^b \langle V, D_t\dot\gamma\rangle\:dt = - \sum_{i=1}^{k-1} \left\langle V(a_i), \Delta_i \dot\gamma\right\rangle + \langle V(b),\dot\gamma(b)\rangle - \langle V(a), \dot\gamma(a)\rangle $$ But why does this show the limiting curve $\gamma$ is geodesic? Where in here does it follow that $D_t\dot\gamma \equiv 0$ ? Edit 1: I think I can use the Arzelà-Ascoli theorem for $\mathbb R^n$ -valued functions of a compact space in a geodesic ball in normal coordinates around a limit point of an appropriate sequence $\{x_n\}$ in $M$ ; say, for example, $x_n = \gamma_n(0)$ , where $\{\gamma_n\}$ is the sequence of geodesic loops. Edit 2: Using the sequence $\{x_n\}$ coming from $x_n = \gamma_n(0)$ , take a convergent subsequence $x_n \to p \in M$ , and then let $w_n = \dot\gamma_n(0) \in T_{x_n}M$ for each $n$ . Assume after reparametrization that each $\gamma_n$ is unit-speed, so the parallel translations $v_n \in T_pM$ along radial geodesics from $p$ to $x$ are unit tangent vectors. In particular, they too have a convergent subsequence $v_n \to v$ . I think the resulting geodesic we want is then just $\gamma(t) = \exp_p(tv)$ .","I'm trying to prove the following result, which is Problem 6-17 in Lee's Introduction to Riemannian Manifolds : Suppose is a compact connected Riemannian manifold. Show that every nontrivial free homotopy class in is represented by a closed geodesic that has minimal length among all admissible loops in the given free homotopy class. This problem comes with a hint, which breaks the solution down into the following steps: Show the given free homotopy class is represented by a geodesic loop; Show the lengths of such loops have positive greatest lower bound; Choose a sequence of geodesic loops whose lengths approach this bound, and show that a subsequence converges uniformly to a geodesic loop whose length is equal to this greatest lower bound; Show the limiting curve is in the given free homotopy class; Apply the first variation formula to show that the limiting curve is in fact a closed geodesic. I have most of these steps down except 3 and 5. First of all, if I have such a sequence, finding a universally convergent subsequence seems similar in spirit to Arzelà-Ascoli, but finding the right uniformly bounded and equicontinuous sequence of functions so far has eluded me. Second of all, in proving 5, the first variation formula says that if is a variation (i.e. a homotopy) between the limiting curve and one of the sequence curves , then where is the variation field along , is an admissible partition for , and is the ""jump"" in the velocity vector field at . In this case, , so we need to show and for all . Since is a critical point for this variation, this forumula becomes But why does this show the limiting curve is geodesic? Where in here does it follow that ? Edit 1: I think I can use the Arzelà-Ascoli theorem for -valued functions of a compact space in a geodesic ball in normal coordinates around a limit point of an appropriate sequence in ; say, for example, , where is the sequence of geodesic loops. Edit 2: Using the sequence coming from , take a convergent subsequence , and then let for each . Assume after reparametrization that each is unit-speed, so the parallel translations along radial geodesics from to are unit tangent vectors. In particular, they too have a convergent subsequence . I think the resulting geodesic we want is then just .","(M,g) M \Gamma_s(t) \gamma(t) \gamma_n(t) 
\frac{d}{ds}\bigg|_{s=0} L_g(\Gamma_s) = -\int_a^b \langle V, D_t\dot\gamma\rangle\:dt - \sum_{i=1}^{k-1} \left\langle V(a_i), \Delta_i \dot\gamma\right\rangle + \langle V(b),\dot\gamma(b)\rangle - \langle V(a), \dot\gamma(b)\rangle
 V(t) = \frac{d}{ds}\big|_{s=0} \Gamma_s(t) \gamma (a_0, \ldots, a_k) V \Delta_i \dot\gamma = \dot\gamma(a_i^+) - \dot\gamma(a_i^-) a_i V(a) = V(b) \dot\gamma(a) = \dot\gamma(b) \Delta_i \dot\gamma = 0 i \Gamma_0 
\int_a^b \langle V, D_t\dot\gamma\rangle\:dt = - \sum_{i=1}^{k-1} \left\langle V(a_i), \Delta_i \dot\gamma\right\rangle + \langle V(b),\dot\gamma(b)\rangle - \langle V(a), \dot\gamma(a)\rangle
 \gamma D_t\dot\gamma \equiv 0 \mathbb R^n \{x_n\} M x_n = \gamma_n(0) \{\gamma_n\} \{x_n\} x_n = \gamma_n(0) x_n \to p \in M w_n = \dot\gamma_n(0) \in T_{x_n}M n \gamma_n v_n \in T_pM p x v_n \to v \gamma(t) = \exp_p(tv)","['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'geodesic']"
11,"\Choosing $f$ such that $X(f)$ vanishes on the smallest possible submanifold, for $X$ a nowhere vanishing vector field.","\Choosing  such that  vanishes on the smallest possible submanifold, for  a nowhere vanishing vector field.",f X(f) X,"Say I have a manifold, $M$ , equipped with a nowhere vanishing vector field $X$ . I wish to find an $f$ such that $X(f)=g\in C^{\infty}(M)$ vanishes on the ""smallest"" possible set of $M$ . If the zeros of $g$ are transverse to $0 \in \mathbb{R}$ then $\{x\in M| X(f)(x)=0\}$ should be an $n-1$ dimensional manifold of $M$ . I guess this is the best that I can do? Can I always do this? Edit: I should note that I am interested in the case that $M$ is compact.","Say I have a manifold, , equipped with a nowhere vanishing vector field . I wish to find an such that vanishes on the ""smallest"" possible set of . If the zeros of are transverse to then should be an dimensional manifold of . I guess this is the best that I can do? Can I always do this? Edit: I should note that I am interested in the case that is compact.",M X f X(f)=g\in C^{\infty}(M) M g 0 \in \mathbb{R} \{x\in M| X(f)(x)=0\} n-1 M M,"['differential-geometry', 'differential-topology']"
12,Complete metric,Complete metric,,"Suppose $M_1=(\mathbb{R}^2,g_s)$ is the plane with standard flat metric, it is a complete manifold. Now if I delete point origin, $M_2=(\mathbb{R}^2\setminus\{0\},g_s)$ is obviously not complete. However, when punctured plane is given a different metirc, it becomes complete, such as $$g=\frac{1}{|x|^2}g_s$$ I think it is geodesically complete, since when points get close to origin, the metric blows up. However, I don't know how to rigorously prove it?","Suppose is the plane with standard flat metric, it is a complete manifold. Now if I delete point origin, is obviously not complete. However, when punctured plane is given a different metirc, it becomes complete, such as I think it is geodesically complete, since when points get close to origin, the metric blows up. However, I don't know how to rigorously prove it?","M_1=(\mathbb{R}^2,g_s) M_2=(\mathbb{R}^2\setminus\{0\},g_s) g=\frac{1}{|x|^2}g_s","['differential-geometry', 'riemannian-geometry']"
13,Formal definition of a Pseudotensor,Formal definition of a Pseudotensor,,"In physics, a tensor is defined as a multidimensional array with a special transformation law. Therefore, a tensor of type $(r, s)$ is an geometric object $T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}[\,\underline{e}\,]$ to each basis $\underline{e} = (e_1, ..., e_n)$ of an n-dimensional vector space such that the multidimensional array obeys the transformation law $T_{i^{\prime}_{1}\dots i^{\prime}_{s}}^{j^{\prime}_{1}\dots j^{\prime}_{r}}[M\cdot \underline{e}\,]=(M^{-1})_{j_{1}}^{j^{\prime}_{1}}\cdot\dots\cdot (M^{-1})_{j_{r}}^{j^{\prime}_{r}}\cdot T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}\cdot M_{i_{1}^{\prime}}^{i_{1}}\cdot\dots\cdot M_{i_{s}^{\prime}}^{i_{s}}$ where $M$ is the transformation matrix. In calculus this objects can be defined more abstract: There a tensor of type (r,s) is an element of the abstract tensor product $T\in \underbrace{V\otimes \dots \otimes V}_{r-\text{times}}\otimes \underbrace{V^{\ast}\otimes\dots\otimes V^{\ast}}_{s-\text{times}}$ or, because of can. isomorphy, a tensor can be viewed as a  multilinear function $T:\underbrace{V^{\ast}\times\dots\times V^{\ast}}_{r-\text{times}}\times \underbrace{V\times \dots \times V}_{s-\text{times}}\to \mathbb{R}$ . Now to my question: In physics, there is also the definition of a ''pseudo-tensor'', which is an geometric object $T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}[\,\underline{e}\,]$ with the the transformation law $T_{i^{\prime}_{1}\dots i^{\prime}_{s}}^{j^{\prime}_{1}\dots j^{\prime}_{r}}[M\cdot \underline{e}\,]=\mathrm{sign}(\mathrm{det}(M))\cdot(M^{-1})_{j_{1}}^{j^{\prime}_{1}}\cdot\dots\cdot (M^{-1})_{j_{r}}^{j^{\prime}_{r}}\cdot T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}\cdot M_{i_{1}^{\prime}}^{i_{1}}\cdot\dots\cdot M_{i_{s}^{\prime}}^{i_{s}}$ Is there also an abstract definition for pseudo-tensors? Thank you!","In physics, a tensor is defined as a multidimensional array with a special transformation law. Therefore, a tensor of type is an geometric object to each basis of an n-dimensional vector space such that the multidimensional array obeys the transformation law where is the transformation matrix. In calculus this objects can be defined more abstract: There a tensor of type (r,s) is an element of the abstract tensor product or, because of can. isomorphy, a tensor can be viewed as a  multilinear function . Now to my question: In physics, there is also the definition of a ''pseudo-tensor'', which is an geometric object with the the transformation law Is there also an abstract definition for pseudo-tensors? Thank you!","(r, s) T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}[\,\underline{e}\,] \underline{e} = (e_1, ..., e_n) T_{i^{\prime}_{1}\dots i^{\prime}_{s}}^{j^{\prime}_{1}\dots j^{\prime}_{r}}[M\cdot \underline{e}\,]=(M^{-1})_{j_{1}}^{j^{\prime}_{1}}\cdot\dots\cdot (M^{-1})_{j_{r}}^{j^{\prime}_{r}}\cdot T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}\cdot M_{i_{1}^{\prime}}^{i_{1}}\cdot\dots\cdot M_{i_{s}^{\prime}}^{i_{s}} M T\in \underbrace{V\otimes \dots \otimes V}_{r-\text{times}}\otimes \underbrace{V^{\ast}\otimes\dots\otimes V^{\ast}}_{s-\text{times}} T:\underbrace{V^{\ast}\times\dots\times V^{\ast}}_{r-\text{times}}\times \underbrace{V\times \dots \times V}_{s-\text{times}}\to \mathbb{R} T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}[\,\underline{e}\,] T_{i^{\prime}_{1}\dots i^{\prime}_{s}}^{j^{\prime}_{1}\dots j^{\prime}_{r}}[M\cdot \underline{e}\,]=\mathrm{sign}(\mathrm{det}(M))\cdot(M^{-1})_{j_{1}}^{j^{\prime}_{1}}\cdot\dots\cdot (M^{-1})_{j_{r}}^{j^{\prime}_{r}}\cdot T_{i_{1}\dots i_{s}}^{j_{1}\dots j_{r}}\cdot M_{i_{1}^{\prime}}^{i_{1}}\cdot\dots\cdot M_{i_{s}^{\prime}}^{i_{s}}","['differential-geometry', 'transformation', 'tensors', 'multilinear-algebra']"
14,Prove that the Pullback of a proper embedding is surjective on p-forms,Prove that the Pullback of a proper embedding is surjective on p-forms,,"This is from an old qualifying exam, and I want to check the answer that I have given. The question is Let $N$ be a compact embedded submanifold of a manifold $M$ . Show that $\Omega^p(M) \rightarrow \Omega^p(N)$ is surjective for all $p$ . Here $\Omega^p(M)$ is the vector space of smooth $p$ -forms. Here is my attempt: Let $\iota: N \rightarrow M$ be the inclusion map and pick a point $q \in N$ . Let $(U_q, (x^i))$ be an open chart centered at $\iota(q) = q$ such that $U_q\cap N = V_q$ is a local $k$ -slice of $U_q$ . Then we know that $(\iota^{-1}(V_q), (y^i)) = (V_q, (y^i))$ where $y^i(a) = x^i(a)$ for $a\in V_q$ and $i \in \{1, \ldots, k\}$ . Then for any $\omega \in \Omega^p(N)$ , we can express $\omega$ in terms of these local coordinates as a sum over increasing multi-indices $I$ by $$\omega = {\sum_I}^\prime \omega_I dy^{i_1}\wedge \cdots \wedge dy^{i_p}$$ where each $\omega_I$ is a smooth coordinate function defined on $V_q$ . Well, if we keep the same indexing set and define the $p$ -form $\eta$ on $U_q$ by $$\eta = {\sum_I}^\prime (\eta_I) dx^{i_1}\wedge \cdots \wedge dx^{i_p}$$ with $\eta_I(\iota(a)) = \omega_I(a)$ , then \begin{align*} \iota^*\eta &= {\sum_I}^\prime (\eta_I\circ \iota) d(x^{i_1}\circ \iota)\wedge \cdots \wedge d(x^{i_p}\circ \iota)\\ &= {\sum_I}^\prime \omega_I dy^{i_1}\wedge \cdots \wedge dy^{i_p}\\ &= \omega \end{align*} hence we have that the map $\Omega^p(U_q) \rightarrow \Omega^p(V_q)$ is surjective for all $p$ . Now, we know that $\{U_q\}_{q\in N}$ forms an open cover of $N$ and admits a smooth partition of unity $\{\varphi_q\}_{q\in N}$ subordinate to this open cover. Define $\psi_q = \varphi_q\vert_{N}$ . Then the collection $\{\psi_q\}_{q\in N}$ will be a smooth partition of unity of $N$ subordinate to the open cover $\{V_q\}_{q\in N}$ . Let $\widetilde{\omega}\in \Omega^p(N)$ be any smooth $p$ form on $N$ . Since this form is smooth, we know that there is an indexed collection of $p$ -forms $\{\omega^q\}_{q\in N}$ such that $$\widetilde{\omega} = \sum_{q\in N}\psi_q\omega^q.$$ Now define the $p$ -form $\widetilde{\eta}\in \Omega^p(M)$ by $$\widetilde{\eta} = \sum_{q\in N}\varphi_q\eta^q$$ where each $\eta^q$ is such that $\iota^*\eta^q = \omega^q$ as given in the first portion of this proof and $\widetilde{\eta}|_y = 0$ for any $y\not\in\bigcup_{q\in N}U_q$ . This will be defined and smooth on all of $M$ since $supp(\varphi_q\eta^q)\subseteq U_q$ . Furthermore, since this partition of unity is locally finite, for any $x \in N$ , we have that $\widetilde{\omega}|_x = \iota^*\widetilde{\eta}|_x$ , and so $\widetilde{\omega} = \iota^*\widetilde{\eta}$ . Hence, $\Omega^p(M) \rightarrow \Omega^p(N)$ is surjective for all $p$ . $\qquad \clubsuit$","This is from an old qualifying exam, and I want to check the answer that I have given. The question is Let be a compact embedded submanifold of a manifold . Show that is surjective for all . Here is the vector space of smooth -forms. Here is my attempt: Let be the inclusion map and pick a point . Let be an open chart centered at such that is a local -slice of . Then we know that where for and . Then for any , we can express in terms of these local coordinates as a sum over increasing multi-indices by where each is a smooth coordinate function defined on . Well, if we keep the same indexing set and define the -form on by with , then hence we have that the map is surjective for all . Now, we know that forms an open cover of and admits a smooth partition of unity subordinate to this open cover. Define . Then the collection will be a smooth partition of unity of subordinate to the open cover . Let be any smooth form on . Since this form is smooth, we know that there is an indexed collection of -forms such that Now define the -form by where each is such that as given in the first portion of this proof and for any . This will be defined and smooth on all of since . Furthermore, since this partition of unity is locally finite, for any , we have that , and so . Hence, is surjective for all .","N M \Omega^p(M) \rightarrow \Omega^p(N) p \Omega^p(M) p \iota: N \rightarrow M q \in N (U_q, (x^i)) \iota(q) = q U_q\cap N = V_q k U_q (\iota^{-1}(V_q), (y^i)) = (V_q, (y^i)) y^i(a) = x^i(a) a\in V_q i \in \{1, \ldots, k\} \omega \in \Omega^p(N) \omega I \omega = {\sum_I}^\prime \omega_I dy^{i_1}\wedge \cdots \wedge dy^{i_p} \omega_I V_q p \eta U_q \eta = {\sum_I}^\prime (\eta_I) dx^{i_1}\wedge \cdots \wedge dx^{i_p} \eta_I(\iota(a)) = \omega_I(a) \begin{align*}
\iota^*\eta &= {\sum_I}^\prime (\eta_I\circ \iota) d(x^{i_1}\circ \iota)\wedge \cdots \wedge d(x^{i_p}\circ \iota)\\
&= {\sum_I}^\prime \omega_I dy^{i_1}\wedge \cdots \wedge dy^{i_p}\\
&= \omega
\end{align*} \Omega^p(U_q) \rightarrow \Omega^p(V_q) p \{U_q\}_{q\in N} N \{\varphi_q\}_{q\in N} \psi_q = \varphi_q\vert_{N} \{\psi_q\}_{q\in N} N \{V_q\}_{q\in N} \widetilde{\omega}\in \Omega^p(N) p N p \{\omega^q\}_{q\in N} \widetilde{\omega} = \sum_{q\in N}\psi_q\omega^q. p \widetilde{\eta}\in \Omega^p(M) \widetilde{\eta} = \sum_{q\in N}\varphi_q\eta^q \eta^q \iota^*\eta^q = \omega^q \widetilde{\eta}|_y = 0 y\not\in\bigcup_{q\in N}U_q M supp(\varphi_q\eta^q)\subseteq U_q x \in N \widetilde{\omega}|_x = \iota^*\widetilde{\eta}|_x \widetilde{\omega} = \iota^*\widetilde{\eta} \Omega^p(M) \rightarrow \Omega^p(N) p \qquad \clubsuit","['differential-geometry', 'differential-forms']"
15,Why does a Smooth Function on a Torus (resp. circle) have atleast $4$ (resp. $2$) Critical Points (without Morse Theory),Why does a Smooth Function on a Torus (resp. circle) have atleast  (resp. ) Critical Points (without Morse Theory),4 2,"Here's the actual question I wish to prove, along with the given hint: Let $f: \Bbb{T}^2 \to \Bbb{R}$ be a smooth map. Show that $f$ has atleast $4$ critical points ( $p$ is a critical point of $f$ if the tangent map $Tf_p: T_p\Bbb{T}^2 \to T_{f(p)}\Bbb{R}$ vanishes). Hint: Parametrize $\Bbb{T}^2$ using angles $\theta,\varphi$ , and locate the maximum and minimum points of $f(\theta, \varphi)$ , for $\varphi$ fixed, say $(\theta_{\text{max}}(\varphi), \varphi)$ and $(\theta_{\text{min}}(\varphi), \varphi)$ ; now maximize and minimize $f$ as $\varphi$ varies. I took a look at this question , but the answer there makes use of Morse Theory, which I do not know. Definitions I'm working with: The definition of torus I'm working with is $\Bbb{T}^2 = S^1 \times S^1$ , and the definition of tangent space at a point $p$ of a manifold $M$ is defined to be the collection of equivalence classes of curves: \begin{align} T_pM := \{[c] : \text{$c$ is a $C^1$ curve in $M$ with $c(0) = p$} \}. \end{align} The tangent mapping is defined as $Tf_p([c]) = [f \circ c]$ . As stated, I found the problem slightly difficult, so instead I tried to solve a simpler problem in the hopes that I'd be able to generalize it. Since, $\Bbb{T}^2$ is a product manifold $S^1 \times S^1$ , I tried to prove the following assertion instead: Every smooth function $f: S^1 \to \Bbb{R}$ has atleast two critical points. I already have a proof of this fact using the Extreme value theorem (the extrema will then be critical points), but I want to prove this assertion in a manner suggested by the hint, so that I can generalize this simple case of $f: S^1 \to \Bbb{R}$ to the general case of $f: \Bbb{T}^2 \to \Bbb{R}$ . Here's what I have done so far. In trying to follow the hint, I considered the parametrization $\alpha^{-1}: (0,2 \pi) \to S^1 \setminus \{(1,0)\}$ defined by \begin{align} \alpha^{-1}(\theta) = (\cos \theta, \sin \theta) \end{align} (I call it $\alpha^{-1}$ so that $(S^1 \setminus\{(1,0)\}, \alpha)$ is then a chart on $S^1$ ). Next, I managed to show that the point $\alpha^{-1}(\theta)$ is a critical point for $f$ if and only if $\theta$ is a critical point for $f \circ \alpha^{-1}: (0,2 \pi) \to \Bbb{R}$ . In other words, if and only if \begin{align} (f \circ \alpha^{-1})'(\theta) = \dfrac{d}{d\theta} \big( f(\cos \theta, \sin \theta)  \big) = 0 \tag{$*$} \end{align} Now, here's the problem I face: the only thing we know about $f$ is that it is a smooth function $f:S^1 \to \Bbb{R}$ , but we don't have an explicit formula for it. Also, $f$ is a function whose domain is the manifold $S^1$ , rather than an open set in $\Bbb{R}^2$ , so I can't apply the standard chain rule to $(*)$ . Because of this I'm not sure how to proceed in proving that there exist atleast two critical points for $f$ . (Also, I think I might have to use a second chart so that it covers the entire circle, but I'm having trouble with the first chart itself, so I didn't bother with a second chart). So, my questions are: Is it a good idea to try to solve my general problem by considering the lower dimensional case of $f:S^1 \to \Bbb{R}$ ? If so, how would I prove the existence of two critical points in the $S^1$ case (without extreme value theorem), and how would I generalize this to prove existence of $4$ critical points for $f: \Bbb{T}^2 \to \Bbb{R}$ ? If this isn't a viable approach, how would I make use of the hint given in the question to directly prove the existence of $4$ critical points?","Here's the actual question I wish to prove, along with the given hint: Let be a smooth map. Show that has atleast critical points ( is a critical point of if the tangent map vanishes). Hint: Parametrize using angles , and locate the maximum and minimum points of , for fixed, say and ; now maximize and minimize as varies. I took a look at this question , but the answer there makes use of Morse Theory, which I do not know. Definitions I'm working with: The definition of torus I'm working with is , and the definition of tangent space at a point of a manifold is defined to be the collection of equivalence classes of curves: The tangent mapping is defined as . As stated, I found the problem slightly difficult, so instead I tried to solve a simpler problem in the hopes that I'd be able to generalize it. Since, is a product manifold , I tried to prove the following assertion instead: Every smooth function has atleast two critical points. I already have a proof of this fact using the Extreme value theorem (the extrema will then be critical points), but I want to prove this assertion in a manner suggested by the hint, so that I can generalize this simple case of to the general case of . Here's what I have done so far. In trying to follow the hint, I considered the parametrization defined by (I call it so that is then a chart on ). Next, I managed to show that the point is a critical point for if and only if is a critical point for . In other words, if and only if Now, here's the problem I face: the only thing we know about is that it is a smooth function , but we don't have an explicit formula for it. Also, is a function whose domain is the manifold , rather than an open set in , so I can't apply the standard chain rule to . Because of this I'm not sure how to proceed in proving that there exist atleast two critical points for . (Also, I think I might have to use a second chart so that it covers the entire circle, but I'm having trouble with the first chart itself, so I didn't bother with a second chart). So, my questions are: Is it a good idea to try to solve my general problem by considering the lower dimensional case of ? If so, how would I prove the existence of two critical points in the case (without extreme value theorem), and how would I generalize this to prove existence of critical points for ? If this isn't a viable approach, how would I make use of the hint given in the question to directly prove the existence of critical points?","f: \Bbb{T}^2 \to \Bbb{R} f 4 p f Tf_p: T_p\Bbb{T}^2 \to T_{f(p)}\Bbb{R} \Bbb{T}^2 \theta,\varphi f(\theta, \varphi) \varphi (\theta_{\text{max}}(\varphi), \varphi) (\theta_{\text{min}}(\varphi), \varphi) f \varphi \Bbb{T}^2 = S^1 \times S^1 p M \begin{align}
T_pM := \{[c] : \text{c is a C^1 curve in M with c(0) = p} \}.
\end{align} Tf_p([c]) = [f \circ c] \Bbb{T}^2 S^1 \times S^1 f: S^1 \to \Bbb{R} f: S^1 \to \Bbb{R} f: \Bbb{T}^2 \to \Bbb{R} \alpha^{-1}: (0,2 \pi) \to S^1 \setminus \{(1,0)\} \begin{align}
\alpha^{-1}(\theta) = (\cos \theta, \sin \theta)
\end{align} \alpha^{-1} (S^1 \setminus\{(1,0)\}, \alpha) S^1 \alpha^{-1}(\theta) f \theta f \circ \alpha^{-1}: (0,2 \pi) \to \Bbb{R} \begin{align}
(f \circ \alpha^{-1})'(\theta) = \dfrac{d}{d\theta} \big( f(\cos \theta, \sin \theta)  \big) = 0 \tag{*}
\end{align} f f:S^1 \to \Bbb{R} f S^1 \Bbb{R}^2 (*) f f:S^1 \to \Bbb{R} S^1 4 f: \Bbb{T}^2 \to \Bbb{R} 4","['differential-geometry', 'differential-topology', 'smooth-manifolds', 'maxima-minima', 'smooth-functions']"
16,Gradient of squared distance function,Gradient of squared distance function,,"Let $\theta: M \times M \to \mathbb{R}$ the squared distance function $\theta(x,y)=d(x,y)^{2}$ on complete Riemannian manifold $M$ . I would like to calcule the gradient of $d^{2}$ , where $d^{2}_{y}(x)=d^{2}(x,y)$ , for $x,y\in M$ such that $x \notin Cut(y)\cup \{y\}$ . The idea is to consider $\alpha(t,s)$ a variation through geodesics of a minimizing geodesic $\gamma(t)$ with $\gamma(0)=y$ and $\gamma(l)=x$ , $l=d(x,y)$ and if $L(s)$ denotes the length of the geodesic $t \to \alpha(t,s)$ , then by first variation formula : $$\frac{d L(s)}{ds}\Bigr|_{s=0}= \langle V,T\rangle^{l}_{0} - \int\limits^{l}_{0} \langle V, \nabla_{T}T\rangle dt$$ where $T=\frac{\partial \alpha}{\partial t}$ and $V=\frac{\partial \alpha}{\partial s}$ . Now , since $\alpha$ os geodesic $\nabla_{T}T=0$ , and taking $\alpha$ such that $V$ is the Jacobi Field such that $V(0)=0$ and $V(l)=v$ , where $v \in T_{x} M$ , thus : $$\frac{d L(s)}{ds}\Bigr|_{s=0}=\langle v,T(l)\rangle.$$ Now, I have two questions $(1)$ Why $T(l)=\frac{-1}{l}exp^{-1}_{x}(y)$ ? $(2)$ Why $d(d_{y}(x))(v)=\frac{d L(s)}{ds}\Bigr|_{s=0}$ ? Thanks","Let the squared distance function on complete Riemannian manifold . I would like to calcule the gradient of , where , for such that . The idea is to consider a variation through geodesics of a minimizing geodesic with and , and if denotes the length of the geodesic , then by first variation formula : where and . Now , since os geodesic , and taking such that is the Jacobi Field such that and , where , thus : Now, I have two questions Why ? Why ? Thanks","\theta: M \times M \to \mathbb{R} \theta(x,y)=d(x,y)^{2} M d^{2} d^{2}_{y}(x)=d^{2}(x,y) x,y\in M x \notin Cut(y)\cup \{y\} \alpha(t,s) \gamma(t) \gamma(0)=y \gamma(l)=x l=d(x,y) L(s) t \to \alpha(t,s) \frac{d L(s)}{ds}\Bigr|_{s=0}= \langle V,T\rangle^{l}_{0} - \int\limits^{l}_{0} \langle V, \nabla_{T}T\rangle dt T=\frac{\partial \alpha}{\partial t} V=\frac{\partial \alpha}{\partial s} \alpha \nabla_{T}T=0 \alpha V V(0)=0 V(l)=v v \in T_{x} M \frac{d L(s)}{ds}\Bigr|_{s=0}=\langle v,T(l)\rangle. (1) T(l)=\frac{-1}{l}exp^{-1}_{x}(y) (2) d(d_{y}(x))(v)=\frac{d L(s)}{ds}\Bigr|_{s=0}","['differential-geometry', 'riemannian-geometry', 'calculus-of-variations', 'geodesic']"
17,Compact hypersurface [duplicate],Compact hypersurface [duplicate],,"This question already has answers here : Prove that Gauss map on M is surjective (4 answers) Closed 4 years ago . I want to show that for some compact hypersurface $M$ in $\mathbb{R}$ it holds that for any $v \in S^n$ there exists some $p \in M$ , s.t. $v$ is the unit normal to $T_pM$ . I also have the hint to look at the hyperplanes $Rv+v^{\perp}$ for some $R>0$ , s.t. this hyperplane and $M$ are disjoint and then to look at the point with minimal distance to $Rv+v^{\perp}$ . Now I can picture this (for $n \leq 3$ obviously) because then $Rv+v^{\perp}$ is some plane $\cong \mathbb{R}^2$ and if I take the point with minimal distance its obvious (graphically) that its tangent space has to be orthogonal to $v$ . But I don't know how to formalize this. I can get the point with minimal distance to $Rv+v^{\perp}$ by looking at the smooth function $f:M \rightarrow \mathbb{R}, q \mapsto dist(q,Rv+v^{\perp})$ which has a minimum since $M$ is compact. Can somebody give me some hint how to formalize this (for all dimensions)? Thanks!","This question already has answers here : Prove that Gauss map on M is surjective (4 answers) Closed 4 years ago . I want to show that for some compact hypersurface in it holds that for any there exists some , s.t. is the unit normal to . I also have the hint to look at the hyperplanes for some , s.t. this hyperplane and are disjoint and then to look at the point with minimal distance to . Now I can picture this (for obviously) because then is some plane and if I take the point with minimal distance its obvious (graphically) that its tangent space has to be orthogonal to . But I don't know how to formalize this. I can get the point with minimal distance to by looking at the smooth function which has a minimum since is compact. Can somebody give me some hint how to formalize this (for all dimensions)? Thanks!","M \mathbb{R} v \in S^n p \in M v T_pM Rv+v^{\perp} R>0 M Rv+v^{\perp} n \leq 3 Rv+v^{\perp} \cong \mathbb{R}^2 v Rv+v^{\perp} f:M \rightarrow \mathbb{R}, q \mapsto dist(q,Rv+v^{\perp}) M","['differential-geometry', 'riemannian-geometry', 'tangent-spaces']"
18,Is the quotient of a holomorphic group action a complex manifold?,Is the quotient of a holomorphic group action a complex manifold?,,"Let $M$ be a complex manifold on which the group $G$ acts properly  and holomorphic. Is $M/G$ then a complex manifold? My attempt: We need charts for $M/G$ . For any $m \in M$ we take $U\subset M$ small enough such that the restriction of the quotient map $q$ is a   homeomorphism. ( Is this possible because of proper discontinuity? )   We have a chart $\phi_U: U \to \mathbb C^n$ . As $q|_U$ is a   homeomorphism, this gives us a chart $\phi_U \circ (q|_U)^{-1}:  q(U)\to \mathbb C^n$ . We need to check that the functions $[\phi_V \circ (q|_V)^{-1}] \circ  [\phi_U \circ (q|_U)^{-1}]^{-1}: \mathbb C^n \to \mathbb C^n$ are   holomorphic. But this is the case if $(q|_U)^{-1} \circ (q|_U)$ is   holomorphic. But this is simply the translation by a group element   which is holomorphic as  we assumed the group action to be   holomorphic. Is this right and is this enough? Edit: Actually I want $G$ to also be discrete and act freely.","Let be a complex manifold on which the group acts properly  and holomorphic. Is then a complex manifold? My attempt: We need charts for . For any we take small enough such that the restriction of the quotient map is a   homeomorphism. ( Is this possible because of proper discontinuity? )   We have a chart . As is a   homeomorphism, this gives us a chart . We need to check that the functions are   holomorphic. But this is the case if is   holomorphic. But this is simply the translation by a group element   which is holomorphic as  we assumed the group action to be   holomorphic. Is this right and is this enough? Edit: Actually I want to also be discrete and act freely.","M G M/G M/G m \in M U\subset M q \phi_U: U \to \mathbb C^n q|_U \phi_U \circ (q|_U)^{-1}:
 q(U)\to \mathbb C^n [\phi_V \circ (q|_V)^{-1}] \circ
 [\phi_U \circ (q|_U)^{-1}]^{-1}: \mathbb C^n \to \mathbb C^n (q|_U)^{-1} \circ (q|_U) G","['proof-verification', 'differential-geometry', 'manifolds', 'complex-geometry', 'group-actions']"
19,Canonical transformation $T_pV \cong V$?,Canonical transformation ?,T_pV \cong V,"Apparently there is a natural isomorphism between a vector space and the tangent space of this vector space at one point. I.e., $\forall p \in V$ we have $T_pV \cong V$ . I know that the isomorphism is $v \mapsto D_{v\vert a}$ . But what is the isomorphism explicitely in the other direction? If I take a derivation $X\in T_p V$ to what vector $v \in V$ is it sent?","Apparently there is a natural isomorphism between a vector space and the tangent space of this vector space at one point. I.e., we have . I know that the isomorphism is . But what is the isomorphism explicitely in the other direction? If I take a derivation to what vector is it sent?",\forall p \in V T_pV \cong V v \mapsto D_{v\vert a} X\in T_p V v \in V,"['differential-geometry', 'vector-space-isomorphism', 'canonical-transformation']"
20,Adjoint of the Coupled Covariant Derivative on Spinors,Adjoint of the Coupled Covariant Derivative on Spinors,,"I want to understand the proof for a $C^0$ bound of solutions to the Seiberg-Witten equations. Among other places, it can be found in Kronheimer, Mrowka: ""The genus of embedded surfaces in the projective plane"", it is Lemma 2 therein. One step in the proof goes as follows: $$ \Delta \left| \Phi \right|^2 = 2 \langle \nabla_A^* \nabla_A \Phi , \Phi \rangle - 2 \langle \nabla_A \Phi, \nabla _A \Phi \rangle. $$ (This step with context is provided in the picture below) The proof that this is true comes from using the definition $\Delta = - \sum_i \nabla_i \nabla_i$ and using that $\nabla_A$ is metric. But I'm puzzled by the result. Why is the right hand side not constant $0$ ?   It should be, as $\langle \nabla_A^* \nabla_A \Phi , \Phi \rangle=\langle  \nabla_A \Phi , \nabla_A \Phi \rangle$ by the definition of adjoint.","I want to understand the proof for a bound of solutions to the Seiberg-Witten equations. Among other places, it can be found in Kronheimer, Mrowka: ""The genus of embedded surfaces in the projective plane"", it is Lemma 2 therein. One step in the proof goes as follows: (This step with context is provided in the picture below) The proof that this is true comes from using the definition and using that is metric. But I'm puzzled by the result. Why is the right hand side not constant ?   It should be, as by the definition of adjoint.","C^0 
\Delta \left| \Phi \right|^2
=
2 \langle \nabla_A^* \nabla_A \Phi , \Phi \rangle - 2 \langle \nabla_A \Phi, \nabla _A \Phi \rangle.
 \Delta = - \sum_i \nabla_i \nabla_i \nabla_A 0 \langle \nabla_A^* \nabla_A \Phi , \Phi \rangle=\langle  \nabla_A \Phi , \nabla_A \Phi \rangle","['differential-geometry', 'laplacian', 'gauge-theory', 'spin-geometry']"
21,Mean Curvature Flow of graphs,Mean Curvature Flow of graphs,,"Let $\Omega\subset \mathbb{R}^n$ be open and let $f:\Omega\times [0,T)\to \mathbb{R}$ be a smooth function. Consider the graph function $\phi:\Omega\times [0,T)\to \mathbb{R}^{n+1}$ given by $\phi(p,t)=(p,f(p,t))$ . In general $\phi$ is said to flow in mean curvature if $$  \partial_t \phi=H\nu $$ Where $H$ is the mean curvature function for the pullback metric $g_{ij}=\delta_{ij}+f_if_j$ at each time, and $$\nu=\frac{(-\nabla f,1)}{1+|\nabla f|^2}$$ is the unit vector perpendicular to the immersed manifold $\phi|_t$ . This is a very common example and yet I get confused. In fact, $$\partial_t\phi=(0,...,0,\partial_t f)=\partial_t f\textbf{e}_{n+1}$$ and therefore $\nu||\textbf{e}_{n+1}$ i.e. $\nu=\textbf{e}_{n+1}$ , and $\nabla f=0$ . This is clearly ridiculous and yet I cannot find my mistake.","Let be open and let be a smooth function. Consider the graph function given by . In general is said to flow in mean curvature if Where is the mean curvature function for the pullback metric at each time, and is the unit vector perpendicular to the immersed manifold . This is a very common example and yet I get confused. In fact, and therefore i.e. , and . This is clearly ridiculous and yet I cannot find my mistake.","\Omega\subset \mathbb{R}^n f:\Omega\times [0,T)\to \mathbb{R} \phi:\Omega\times [0,T)\to \mathbb{R}^{n+1} \phi(p,t)=(p,f(p,t)) \phi  
\partial_t \phi=H\nu
 H g_{ij}=\delta_{ij}+f_if_j \nu=\frac{(-\nabla f,1)}{1+|\nabla f|^2} \phi|_t \partial_t\phi=(0,...,0,\partial_t f)=\partial_t f\textbf{e}_{n+1} \nu||\textbf{e}_{n+1} \nu=\textbf{e}_{n+1} \nabla f=0","['differential-geometry', 'riemannian-geometry', 'mean-curvature-flows']"
22,Is the warped product: $dr^{2} + r^{2} \gamma$ always flat in 3 dimensions?,Is the warped product:  always flat in 3 dimensions?,dr^{2} + r^{2} \gamma,"Suppose you have a Riemannian 2-sphere $(S^{2} , \gamma)$ . Define a metric $g$ on $M = S^{2} \times [1,\infty)$ in this way: If $(x_{1}, x_2)$ is a coordinate chart on $S^{2}$ then $$g = dr^{2} + r^{2}\gamma_{ij}dx^i dx^j$$ Where $i,j = 1,2$ . Notice that $\gamma_{ij}$ does not depend on $r$ and so this is a warped product. My question is: is this metric flat? And is this metric the Euclidean metric on $M$ ? Let us fix a coordinate chart on $S^{2}$ call it $(\theta, \phi) \in U$ where $U$ is some open set of $\mathbb{R}^2$ . Then we are looking at $(U\times [1,\infty) , g)$ where $g = dr^2 + \tilde{\gamma}$ where $\tilde{\gamma} = r^2 \gamma$ . I think this means that the hypersurfaces $\{ r = r_{0} \}$ are all totally umbilical since the second fundamental form $K$ satisfies: $$K_{ij} = -\frac12 \frac{d}{dr} \tilde{\gamma} = -\frac1r \tilde{\gamma} $$ So the principle curvatures are $-\frac1r $ and so the Gaussian curvature is $\frac{1}{r^2} $ . Using this, I computed the Rici curvature components of M and found them to be $0$ (But I don’t trust my computations at all). This implies that the Rieman curvature tensor vanishes on $M$ and so $M$ is flat (since the Weyl part of Rieman curvature tensor vanishes for 3-dim manifolds, the vanishing of the Ricci curvature implies the vanishing of the Rieman Curvature tensor). Is there any mistake in what I am doing?","Suppose you have a Riemannian 2-sphere . Define a metric on in this way: If is a coordinate chart on then Where . Notice that does not depend on and so this is a warped product. My question is: is this metric flat? And is this metric the Euclidean metric on ? Let us fix a coordinate chart on call it where is some open set of . Then we are looking at where where . I think this means that the hypersurfaces are all totally umbilical since the second fundamental form satisfies: So the principle curvatures are and so the Gaussian curvature is . Using this, I computed the Rici curvature components of M and found them to be (But I don’t trust my computations at all). This implies that the Rieman curvature tensor vanishes on and so is flat (since the Weyl part of Rieman curvature tensor vanishes for 3-dim manifolds, the vanishing of the Ricci curvature implies the vanishing of the Rieman Curvature tensor). Is there any mistake in what I am doing?","(S^{2} , \gamma) g M = S^{2} \times [1,\infty) (x_{1}, x_2) S^{2} g = dr^{2} + r^{2}\gamma_{ij}dx^i dx^j i,j = 1,2 \gamma_{ij} r M S^{2} (\theta, \phi) \in U U \mathbb{R}^2 (U\times [1,\infty) , g) g = dr^2 + \tilde{\gamma} \tilde{\gamma} = r^2 \gamma \{ r = r_{0} \} K K_{ij} = -\frac12 \frac{d}{dr} \tilde{\gamma} = -\frac1r \tilde{\gamma}  -\frac1r  \frac{1}{r^2}  0 M M","['differential-geometry', 'riemannian-geometry']"
23,Coordinate-Free Computation of Covariant Derivative,Coordinate-Free Computation of Covariant Derivative,,"Suppose $S$ is an oriented 2-dimensional manifold equipped with a Riemannian metric $\langle \cdot,\cdot \rangle$ . Let $\theta$ be a 1-form on $S$ . By non-degeneracy, there is an identification of the tangent and cotangent bundle via the metric. In particular, there is a bundle map $Y \colon TS \rightarrow TS$ such that $$\langle Y_x(v),w \rangle = d\theta_x(v,w). $$ Likewise, there exists a vector field $Z$ on $S$ with $$\langle Z(x),v \rangle = \theta_x(v).$$ I am interested in computing the covariant derivative of $Z$ . Of course, this is possible in local coordinates, but the computation becomes super messy since the Riemannian metric comes into play several times. Hence, I would like to know if anybody can come up with a coordinate-free computation. (edited:) Even if no explicit solution can be written down, I hope to verify the following formula: $$\langle v+Z,\nabla_wZ \rangle = \langle \nabla_{(v+Z)}Z,w \rangle - \langle Y(v+Z),w \rangle.$$ (For the interested: this is an intermediate step to verify that the symplectic gradient flow of the energy Hamiltonian for a magnetic form is conjuagted to the symplectic gradient flow of the Hamiltonian $E \circ \mathcal{L}^{-1}$ ( $E$ energy, $\mathcal{L}$ Legendre transform) for the standard symplectic form via the Legendre transform.) In my context, $Y$ maps a vector $v \in T_xS$ to $s(x)Jv$ , where $s$ is a smooth function on $S$ and $J$ is the canonical almost complex structure induced by the metric. However, I feel like the computation can be done without explicitly knowing $Y$ .","Suppose is an oriented 2-dimensional manifold equipped with a Riemannian metric . Let be a 1-form on . By non-degeneracy, there is an identification of the tangent and cotangent bundle via the metric. In particular, there is a bundle map such that Likewise, there exists a vector field on with I am interested in computing the covariant derivative of . Of course, this is possible in local coordinates, but the computation becomes super messy since the Riemannian metric comes into play several times. Hence, I would like to know if anybody can come up with a coordinate-free computation. (edited:) Even if no explicit solution can be written down, I hope to verify the following formula: (For the interested: this is an intermediate step to verify that the symplectic gradient flow of the energy Hamiltonian for a magnetic form is conjuagted to the symplectic gradient flow of the Hamiltonian ( energy, Legendre transform) for the standard symplectic form via the Legendre transform.) In my context, maps a vector to , where is a smooth function on and is the canonical almost complex structure induced by the metric. However, I feel like the computation can be done without explicitly knowing .","S \langle \cdot,\cdot \rangle \theta S Y \colon TS \rightarrow TS \langle Y_x(v),w \rangle = d\theta_x(v,w).  Z S \langle Z(x),v \rangle = \theta_x(v). Z \langle v+Z,\nabla_wZ \rangle = \langle \nabla_{(v+Z)}Z,w \rangle - \langle Y(v+Z),w \rangle. E \circ \mathcal{L}^{-1} E \mathcal{L} Y v \in T_xS s(x)Jv s S J Y","['differential-geometry', 'surfaces']"
24,"Why is a flat, asymptotically flat 3-manifold isometric to $\mathbb{R}^3$?","Why is a flat, asymptotically flat 3-manifold isometric to ?",\mathbb{R}^3,"I'm currently reading Schoen & Yau's 1979 proof of the positive-mass theorem and arrived at the very last sentence on the very last page (modulo the appendix) where they say: Hence we conclude that $\mathop{Ric} = 0$ and because we are working in dimension three, $ds²$ is flat. This completes the proof of Theorem 2. Theorem 2, however, made the claim that the manifold-with-boundary $N$ is (globally) isometric to $\mathbb{R}^3$ and I haven't been able to figure out in detail why local isometry to $\mathbb{R}^3$ (away from the boundary, of course) implies global isometry in this case. (My gut tells me that I'm missing something very elementary about what 3-manifolds of the given kind must look like, so I hope you'll excuse if this is a very simple question.) For definiteness, let me restate the situation and the claim in detail: Let $N$ be a connected¹, complete², asymptotically flat Riemannian manifold-with-boundary. Here, asymptotic flatness means that (1) there is a compact set $K \subset N$ such that the open set $N \setminus K$ consists of finitely many connected components (""ends"") $N_k$ each of which is diffeomorphic to some $\mathbb{R^3} \setminus (\text{closed ball})$ , and (2) the boundary of $N$ has mean curvature $H < 0$ with respect to the outward-pointing normal $n$ . (Here, $H$ is defined as $H := \mathop{tr}_g II$ and $II(v, w) := \langle \nabla_v w, n \rangle$ is the 2nd fundamental form.) Claim: Suppose $N$ is flat and only has one end (called $N_k$ in the theorem). Then $N$ is isometrically isomorphic to $\mathbb{R}^3$ . ¹, ²: These requirements are not explicitly mentioned by Schoen & Yau but seem natural and, in fact, are necessary for some of the proofs in the paper to work. In particular, without them the claim would be false right away. I hope I'm not missing any further implicit assumptions on $N$ . My intuition is that the requirement on the boundary's mean curvature prevents it from bounding any ""holes"" in $N$ . (In particular, K cannot merely be the boundary of $N_k \subset \mathbb{R}^3$ , i.e. a sphere.) Moreover, the completeness of $N$ prevents us from choosing $K$ to be e.g. the empty set. Finally, the fact that $N = K \cup N_k$ and that $N$ is flat everywhere should prevent $K$ from including something like a flat 3-torus. After all, topologically, there would be nothing preventing me from e.g. gluing $N_k$ to $T^3$ . So I suspect that it must be the flatness forbidding this, in the sense that I cannot actually choose the throat between $N_k$ and $T^3$ to be flat. But I'm having trouble making this precise and, in particular, generalizing this argument to any other $K$ that is not a ball.","I'm currently reading Schoen & Yau's 1979 proof of the positive-mass theorem and arrived at the very last sentence on the very last page (modulo the appendix) where they say: Hence we conclude that and because we are working in dimension three, is flat. This completes the proof of Theorem 2. Theorem 2, however, made the claim that the manifold-with-boundary is (globally) isometric to and I haven't been able to figure out in detail why local isometry to (away from the boundary, of course) implies global isometry in this case. (My gut tells me that I'm missing something very elementary about what 3-manifolds of the given kind must look like, so I hope you'll excuse if this is a very simple question.) For definiteness, let me restate the situation and the claim in detail: Let be a connected¹, complete², asymptotically flat Riemannian manifold-with-boundary. Here, asymptotic flatness means that (1) there is a compact set such that the open set consists of finitely many connected components (""ends"") each of which is diffeomorphic to some , and (2) the boundary of has mean curvature with respect to the outward-pointing normal . (Here, is defined as and is the 2nd fundamental form.) Claim: Suppose is flat and only has one end (called in the theorem). Then is isometrically isomorphic to . ¹, ²: These requirements are not explicitly mentioned by Schoen & Yau but seem natural and, in fact, are necessary for some of the proofs in the paper to work. In particular, without them the claim would be false right away. I hope I'm not missing any further implicit assumptions on . My intuition is that the requirement on the boundary's mean curvature prevents it from bounding any ""holes"" in . (In particular, K cannot merely be the boundary of , i.e. a sphere.) Moreover, the completeness of prevents us from choosing to be e.g. the empty set. Finally, the fact that and that is flat everywhere should prevent from including something like a flat 3-torus. After all, topologically, there would be nothing preventing me from e.g. gluing to . So I suspect that it must be the flatness forbidding this, in the sense that I cannot actually choose the throat between and to be flat. But I'm having trouble making this precise and, in particular, generalizing this argument to any other that is not a ball.","\mathop{Ric} = 0 ds² N \mathbb{R}^3 \mathbb{R}^3 N K \subset N N \setminus K N_k \mathbb{R^3} \setminus (\text{closed ball}) N H < 0 n H H := \mathop{tr}_g II II(v, w) := \langle \nabla_v w, n \rangle N N_k N \mathbb{R}^3 N N N_k \subset \mathbb{R}^3 N K N = K \cup N_k N K N_k T^3 N_k T^3 K","['differential-geometry', 'manifolds']"
25,Flow of vector field on semi-Riemannian manifold,Flow of vector field on semi-Riemannian manifold,,"Consider $\mathbb{R}^{n+1}$ with the metric given by $$ g(x,x) = 2x_1x_2 + \sum_{i=3}^{n+1}x_i^2 $$ and $M$ the set of $x$ such that $g(x,x)=1$ . Further, take a basis $e_1,...,e_n$ for $\mathbb{R}^{n+1}$ such that $g(e_1,e_1) = -1$ let $Y$ be the vector field on $M$ given by $Y(z) = e_2 - g(z,e_2)z$ . I want to prove that $t \mapsto \alpha_z(t)$ defines the flow of $Y$ with $$\alpha_z(t) = \frac{1}{1+g(z,e_2)t} (z + \frac{t}{2} (2 + g(z,e_2)t )e_2 ).$$ So far, I have showed that $$ \dot{\alpha}_z(t) = e_2 - \frac{1}{(1+g(z,e_2)t)^2}g(z,e_2)z $$ which looks somewhat familiar. I proceeded to compute $Y(\alpha_z(t))$ but the resulting term is quite large and does not look very helpfull. As of now, all I have is $\dot{\alpha}_z(0) = Y(\alpha_z(0))$ but I do not know how to prove it for arbitrary $t$ .","Consider with the metric given by and the set of such that . Further, take a basis for such that let be the vector field on given by . I want to prove that defines the flow of with So far, I have showed that which looks somewhat familiar. I proceeded to compute but the resulting term is quite large and does not look very helpfull. As of now, all I have is but I do not know how to prove it for arbitrary .","\mathbb{R}^{n+1}  g(x,x) = 2x_1x_2 + \sum_{i=3}^{n+1}x_i^2  M x g(x,x)=1 e_1,...,e_n \mathbb{R}^{n+1} g(e_1,e_1) = -1 Y M Y(z) = e_2 - g(z,e_2)z t \mapsto \alpha_z(t) Y \alpha_z(t) = \frac{1}{1+g(z,e_2)t} (z + \frac{t}{2} (2 + g(z,e_2)t )e_2 ).  \dot{\alpha}_z(t) = e_2 - \frac{1}{(1+g(z,e_2)t)^2}g(z,e_2)z  Y(\alpha_z(t)) \dot{\alpha}_z(0) = Y(\alpha_z(0)) t","['differential-geometry', 'semi-riemannian-geometry']"
26,"Find envelope of $x \sin \theta - y \cos \theta + z = a \theta$, where $\theta$ is a parameter.","Find envelope of , where  is a parameter.",x \sin \theta - y \cos \theta + z = a \theta \theta,"I have tried to find envelope for $$x \sin \theta - y \cos \theta + z = a \theta$$ First I find derivative w.r.t. $\theta$ $$F(\theta)=x \sin \theta - y \cos \theta + z - a \theta = 0$$ $$\frac{\partial F(\theta)}{\partial \theta}=y \sin \theta + x \cos \theta - a = 0$$ Then by solving these two above equation in order to eliminate parameter $\theta$ , I find values of $\sin \theta$ and $\cos \theta$ $$\sin \theta = \frac{ax\theta + ay - xz}{x^2 + y^2}$$ and $$\cos \theta = \frac{ax - ay\theta + yz}{x^2 + y^2}$$ Finally to find value of $\theta$ I squares and add above two equations then I get a quadratic equation in $\theta$ as $$a^2\theta^2 - 2az\theta + a^2 + z^2 - x^2 - y^2=0$$ Then solving this for $\theta$ by applying quadratic formula, I get condition for real values of $\theta$ i.e. $\theta$ is real only if $$x^2+y^2 \ge a^2$$ Now here is my question. Either $x^2+y^2 \ge a^2$ is required envelope or something more to do? Because $x^2+y^2 \ge a^2$ is an equality in which we have eliminated parameter $\theta$ .","I have tried to find envelope for First I find derivative w.r.t. Then by solving these two above equation in order to eliminate parameter , I find values of and and Finally to find value of I squares and add above two equations then I get a quadratic equation in as Then solving this for by applying quadratic formula, I get condition for real values of i.e. is real only if Now here is my question. Either is required envelope or something more to do? Because is an equality in which we have eliminated parameter .",x \sin \theta - y \cos \theta + z = a \theta \theta F(\theta)=x \sin \theta - y \cos \theta + z - a \theta = 0 \frac{\partial F(\theta)}{\partial \theta}=y \sin \theta + x \cos \theta - a = 0 \theta \sin \theta \cos \theta \sin \theta = \frac{ax\theta + ay - xz}{x^2 + y^2} \cos \theta = \frac{ax - ay\theta + yz}{x^2 + y^2} \theta \theta a^2\theta^2 - 2az\theta + a^2 + z^2 - x^2 - y^2=0 \theta \theta \theta x^2+y^2 \ge a^2 x^2+y^2 \ge a^2 x^2+y^2 \ge a^2 \theta,['differential-geometry']
27,Spaces of submanifolds,Spaces of submanifolds,,"Let $M$ and $N$ be smooth manifolds with $\dim M<\dim N$ . The spaces $\mathrm{Emb}(M,N)\subset\mathrm{Imm}(M,N)$ of smooth embeddings and immersions $f:M\to N$ , respectively, are infinite dimensional Frechet manifolds. They are open subsets of the space $C^{\infty}(M,N)$ of smooth maps $M\to N$ , or equivalently, of the space $\Gamma(M,M\times N)$ of smooth sections of the trivial bundle $M\times N\to M$ . The diffeomorphism group $\mathrm{Diff}(M)$ acts naturally on these spaces by precomposition. My question is what structure the quotient spaces $\mathrm{Emb}(M,N)/\mathrm{Diff}(M)$ and $\mathrm{Imm}(M,N)/\mathrm{Diff}(M)$ have. These spaces seem to arise naturally in geometric problems, where only the image $f(M)\subset N$ of a map $f:M\to N$ is of concern, not the actual map itself. 1) Do $\mathrm{Emb}(M,N)/\mathrm{Diff}(M)$ and $\mathrm{Imm}(M,N)/\mathrm{Diff}(M)$ have a smooth manifold structure? 2) What are their topological properties, e.g. homology/homotopy? Can we compute these via some kind of ""Morse"" functions?","Let and be smooth manifolds with . The spaces of smooth embeddings and immersions , respectively, are infinite dimensional Frechet manifolds. They are open subsets of the space of smooth maps , or equivalently, of the space of smooth sections of the trivial bundle . The diffeomorphism group acts naturally on these spaces by precomposition. My question is what structure the quotient spaces and have. These spaces seem to arise naturally in geometric problems, where only the image of a map is of concern, not the actual map itself. 1) Do and have a smooth manifold structure? 2) What are their topological properties, e.g. homology/homotopy? Can we compute these via some kind of ""Morse"" functions?","M N \dim M<\dim N \mathrm{Emb}(M,N)\subset\mathrm{Imm}(M,N) f:M\to N C^{\infty}(M,N) M\to N \Gamma(M,M\times N) M\times N\to M \mathrm{Diff}(M) \mathrm{Emb}(M,N)/\mathrm{Diff}(M) \mathrm{Imm}(M,N)/\mathrm{Diff}(M) f(M)\subset N f:M\to N \mathrm{Emb}(M,N)/\mathrm{Diff}(M) \mathrm{Imm}(M,N)/\mathrm{Diff}(M)","['differential-geometry', 'algebraic-topology', 'differential-topology', 'smooth-manifolds']"
28,Lee Introduction to smooth manifolds problem 6-4,Lee Introduction to smooth manifolds problem 6-4,,"Need help with one of the problems in Lee's intro to smooth manifolds. The problem is as follows: (6-4) Let $M$ be a smooth manifold, and $B$ be a closed subset of $M$ , and let $\delta:M\rightarrow\mathbb {R}$ be a positive function. Given any function $f:M\rightarrow \mathbb{R} ^k$ , show that there is a continuous function $\tilde{f}:M\rightarrow \mathbb{R}^k$ that is smooth on $M\setminus B$ and agrees with $f$ on $B$ and is $\delta$ close to $f$ . I think I might have a solution by revising the proof to the Whitney's approximation theorems for functions(theorem 6.21), along with the help of this post Smooth extension of a continuous function on the boundary of a domain . However, Lee provides a hint to use problem 6.3, which says under the same assumptions, we can find a smooth function $\tilde{\delta}:M\rightarrow\mathbb{R}$ that is zero on $B$ , positive on $M\setminus B$ , and satisfies $\tilde{\delta}(x)<\delta(x)$ everywhere. My question is : how to use 6.3 to show 6.4? The crucial difficulty is that $f$ is not assumed to smooth on $B$ in 6.4. Any help is immensely appreciated, this is not a homework question.","Need help with one of the problems in Lee's intro to smooth manifolds. The problem is as follows: (6-4) Let be a smooth manifold, and be a closed subset of , and let be a positive function. Given any function , show that there is a continuous function that is smooth on and agrees with on and is close to . I think I might have a solution by revising the proof to the Whitney's approximation theorems for functions(theorem 6.21), along with the help of this post Smooth extension of a continuous function on the boundary of a domain . However, Lee provides a hint to use problem 6.3, which says under the same assumptions, we can find a smooth function that is zero on , positive on , and satisfies everywhere. My question is : how to use 6.3 to show 6.4? The crucial difficulty is that is not assumed to smooth on in 6.4. Any help is immensely appreciated, this is not a homework question.",M B M \delta:M\rightarrow\mathbb {R} f:M\rightarrow \mathbb{R} ^k \tilde{f}:M\rightarrow \mathbb{R}^k M\setminus B f B \delta f \tilde{\delta}:M\rightarrow\mathbb{R} B M\setminus B \tilde{\delta}(x)<\delta(x) f B,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
29,"Can we ""mod out"" a common subspace in the Grassmannian inside the exterior algebra?","Can we ""mod out"" a common subspace in the Grassmannian inside the exterior algebra?",,"While reading this paper , I have seen the following claim stated without a proof: Let $V$ be an $n$ -dimensional vector space over a field, and let $\alpha,\beta \in \bigwedge^k V$ be decomposable and non-zero. Suppose that $$\alpha=(u_1 \wedge \dots \wedge u_r) \wedge v_1 \wedge \dots \wedge v_{k-r},\beta=(u_1 \wedge \dots \wedge u_r) \wedge w_1 \wedge \dots \wedge w_{k-r},$$ where $$\text{span}(u_1 ,\dots,u_r)=\text{span}(u_1 ,\dots,u_r,v_1\dots v_{k-r}) \cap \text{span}(u_1 ,\dots,u_r,w_1\dots w_{k-r}) $$ (i.e. $\text{span}(u_1 ,\dots,u_r)$ is the intersection of the subspaces corresponding to the decomposable tensors $\alpha,\beta$ .) Then, if $\alpha+\beta$ is decomposable and non-zero, then so is $  v_1 \wedge \dots \wedge v_{k-r}+ w_1 \wedge \dots \wedge w_{k-r} $ . In other words, we can ""mod out"" the ""common intersection"" of $\alpha$ and $\beta$ . How to prove this statement? Here is my failed attempt: We can write $$\tilde u_1 \wedge \dots \wedge \tilde u_k=\alpha+\beta=(u_1 \wedge \dots \wedge u_r) \wedge \gamma, \tag{1}$$ where $\gamma=v_1 \wedge \dots \wedge v_{k-r}+ w_1 \wedge \dots \wedge w_{k-r} $ . By wedging this equality with $u_i$ , we see that $\text{span}(u_1,\dots,u_r) \subseteq \text{span}(\tilde u_1,\dots,\tilde u_k)$ . Thus, we can assume W.L.O.G that $\tilde u_i=u_i$ for $1 \le i \le r$ . Rewriting, we have $$ u_1 \wedge \dots \wedge  u_k=\alpha+\beta=(u_1 \wedge \dots \wedge u_r) \wedge \gamma, \tag{2}$$ Now, it suffices to prove that $\gamma \wedge u_j=0$ for $k <  j \le r$ , since this would imply that the dimension of the subspace of $V$ annihilating $\gamma$ is at least $r-k$ . Since it is also not greater than $r-k$ , it must be $k$ . This implies that $\gamma$ is decomposable. So, we now prove that $\gamma \wedge u_j=0$ for $k <  j \le r$ : We can complete $(u_1,\dots,u_k)$ into a basis $(u_1,\dots,u_n)$ of $V$ . Now we write $\gamma=\sum a^{i_1,\dots,i_{r-k}}u_{i_1} \wedge \dots \wedge u_{i_{r-k}}$ . Since $\alpha+\beta \neq 0$ , there is at least one summand in $\gamma$ that is not composed entirely from wedge of $u_1,\dots,u_r$ .... (I don't see how to continue). In fact, it seems that equation $(2)$ should imply that $\gamma$ is not necessarily decomposable, since it is not uniquely determined by it: Indeed, if we modify $\gamma$ by adding decomposable elements which involve  any of the $u_1,\dots,u_r$ , the RHS does not change, so the equation still holds. It seems to me then, that we should be able to convert $\gamma$ to be a non-decomposable element.","While reading this paper , I have seen the following claim stated without a proof: Let be an -dimensional vector space over a field, and let be decomposable and non-zero. Suppose that where (i.e. is the intersection of the subspaces corresponding to the decomposable tensors .) Then, if is decomposable and non-zero, then so is . In other words, we can ""mod out"" the ""common intersection"" of and . How to prove this statement? Here is my failed attempt: We can write where . By wedging this equality with , we see that . Thus, we can assume W.L.O.G that for . Rewriting, we have Now, it suffices to prove that for , since this would imply that the dimension of the subspace of annihilating is at least . Since it is also not greater than , it must be . This implies that is decomposable. So, we now prove that for : We can complete into a basis of . Now we write . Since , there is at least one summand in that is not composed entirely from wedge of .... (I don't see how to continue). In fact, it seems that equation should imply that is not necessarily decomposable, since it is not uniquely determined by it: Indeed, if we modify by adding decomposable elements which involve  any of the , the RHS does not change, so the equation still holds. It seems to me then, that we should be able to convert to be a non-decomposable element.","V n \alpha,\beta \in \bigwedge^k V \alpha=(u_1 \wedge \dots \wedge u_r) \wedge v_1 \wedge \dots \wedge v_{k-r},\beta=(u_1 \wedge \dots \wedge u_r) \wedge w_1 \wedge \dots \wedge w_{k-r}, \text{span}(u_1 ,\dots,u_r)=\text{span}(u_1 ,\dots,u_r,v_1\dots v_{k-r}) \cap \text{span}(u_1 ,\dots,u_r,w_1\dots w_{k-r})  \text{span}(u_1 ,\dots,u_r) \alpha,\beta \alpha+\beta   v_1 \wedge \dots \wedge v_{k-r}+ w_1 \wedge \dots \wedge w_{k-r}  \alpha \beta \tilde u_1 \wedge \dots \wedge \tilde u_k=\alpha+\beta=(u_1 \wedge \dots \wedge u_r) \wedge \gamma, \tag{1} \gamma=v_1 \wedge \dots \wedge v_{k-r}+ w_1 \wedge \dots \wedge w_{k-r}  u_i \text{span}(u_1,\dots,u_r) \subseteq \text{span}(\tilde u_1,\dots,\tilde u_k) \tilde u_i=u_i 1 \le i \le r  u_1 \wedge \dots \wedge  u_k=\alpha+\beta=(u_1 \wedge \dots \wedge u_r) \wedge \gamma, \tag{2} \gamma \wedge u_j=0 k <  j \le r V \gamma r-k r-k k \gamma \gamma \wedge u_j=0 k <  j \le r (u_1,\dots,u_k) (u_1,\dots,u_n) V \gamma=\sum a^{i_1,\dots,i_{r-k}}u_{i_1} \wedge \dots \wedge u_{i_{r-k}} \alpha+\beta \neq 0 \gamma u_1,\dots,u_r (2) \gamma \gamma u_1,\dots,u_r \gamma","['differential-geometry', 'multilinear-algebra', 'exterior-algebra', 'tensor-decomposition']"
30,Covariant derivative of a symmetric tensor,Covariant derivative of a symmetric tensor,,"Assume that a symmetric $(0,2)$ satisfies $$\nabla_iT_{jk}+\nabla_jT_{ik}+\nabla_kT_{ji}=0$$ where $T=T_i^i$ is constant and $\nabla_jT_{ik}\ne 0$ . What are the values of the constants $a,b,c$ such that $$a\nabla_iT_{jk}+b\nabla_jT_{ik}+c\nabla_kT_{ji}=0$$ Is there any difference if the tensor $T$ is the Ricci tensor. I think $(a,b,c)$ where $a=b=c$ are the only solution set to it however I failed to prove it nor to find the solution. Thanks in advance.",Assume that a symmetric satisfies where is constant and . What are the values of the constants such that Is there any difference if the tensor is the Ricci tensor. I think where are the only solution set to it however I failed to prove it nor to find the solution. Thanks in advance.,"(0,2) \nabla_iT_{jk}+\nabla_jT_{ik}+\nabla_kT_{ji}=0 T=T_i^i \nabla_jT_{ik}\ne 0 a,b,c a\nabla_iT_{jk}+b\nabla_jT_{ik}+c\nabla_kT_{ji}=0 T (a,b,c) a=b=c","['differential-geometry', 'riemannian-geometry', 'tensor-products', 'tensors']"
31,An example of a two dimensional integrable distribution on $SO(3)$,An example of a two dimensional integrable distribution on,SO(3),"I've been reading Manifolds, Tensor Analysis, and Applications recently, and have a question about how to construct a two dimensional integrable distribution on $SO(3)$ . Let $M$ be a manifold, the Local Frobenius Theorem says that a subbundle $E$ of $TM$ is involutive if and only if it is integrable. So I'm trying to construct two vector fields $ X, Y $ defined on open sets of $SO(3)$ such that $[X, Y]$ take values in the distribution generated by $X, Y$ . This two vector fields $X, Y$ can not be left-invariant at the same time, otherwise they can be moved to the identity, and $[X,Y]$ can be computed directly by the Lie-bracket of $\mathfrak{so(3)}$ . Identifying $\mathfrak{so(3)}$ with $R^3$ by $$\left ( \begin{array} &0 & -c & b \\ c & 0 &-a \\ -b & a & 0\\ \end{array} \right ) \cong  \left ( \begin{array} & a\\b\\c\\ \end{array} \right ) , $$ it can be verified that for any two linearly independent vectors $v_1, v_2$ in $R^3$ , $v_1 \times v_2$ can not be in $span\{v_1, v_2 \}$ , which means that $[X, Y]$ can not take values in the distribution generated by $X, Y$ . And I got stuck here. My questions are: For any given vector fields $ X, Y $ , how to compute $[X, Y]$ directly, where $X, Y$ are not assumed to be left-invariant; Do such a two dimensional integrable distribution exist? If yes, how can I construct it? If not, how to prove? Any hint or comment will be appreciate.","I've been reading Manifolds, Tensor Analysis, and Applications recently, and have a question about how to construct a two dimensional integrable distribution on . Let be a manifold, the Local Frobenius Theorem says that a subbundle of is involutive if and only if it is integrable. So I'm trying to construct two vector fields defined on open sets of such that take values in the distribution generated by . This two vector fields can not be left-invariant at the same time, otherwise they can be moved to the identity, and can be computed directly by the Lie-bracket of . Identifying with by it can be verified that for any two linearly independent vectors in , can not be in , which means that can not take values in the distribution generated by . And I got stuck here. My questions are: For any given vector fields , how to compute directly, where are not assumed to be left-invariant; Do such a two dimensional integrable distribution exist? If yes, how can I construct it? If not, how to prove? Any hint or comment will be appreciate.","SO(3) M E TM  X, Y  SO(3) [X, Y] X, Y X, Y [X,Y] \mathfrak{so(3)} \mathfrak{so(3)} R^3 \left (
\begin{array}
&0 & -c & b \\
c & 0 &-a \\
-b & a & 0\\
\end{array}
\right ) \cong 
\left (
\begin{array}
& a\\b\\c\\
\end{array}
\right ) ,
 v_1, v_2 R^3 v_1 \times v_2 span\{v_1, v_2 \} [X, Y] X, Y  X, Y  [X, Y] X, Y","['differential-geometry', 'smooth-manifolds']"
32,Definition of osculating plane,Definition of osculating plane,,"I couldn't understand the exact geometrical meaning of the osculating plane definition. Can any one help me with this? Thanks advance. Osculating plane: Let $\gamma$ be a smooth curve and P and Q be two neighboring points on $\gamma$ . The limiting position of the plane that contains the tangential line at P and passes through the point Q as Q $\to$ P is defined as the osculating plane at P. In another definition, osculating plane is a plane spanned by the tangent and normal line. But, I couldn't understand how we can find normal line geometrical?","I couldn't understand the exact geometrical meaning of the osculating plane definition. Can any one help me with this? Thanks advance. Osculating plane: Let be a smooth curve and P and Q be two neighboring points on . The limiting position of the plane that contains the tangential line at P and passes through the point Q as Q P is defined as the osculating plane at P. In another definition, osculating plane is a plane spanned by the tangent and normal line. But, I couldn't understand how we can find normal line geometrical?",\gamma \gamma \to,['differential-geometry']
33,Notation: gradient as vector field,Notation: gradient as vector field,,"Consider the tangent space $T_p\mathbb{R}^n$ , and suppose $\{\big(\frac{\partial }{\partial x^i}\big)_p\}$ is a basis. So my textbook says that the gradient of a function $f$ , $f\in C^\infty(U)$ , $U\subseteq\mathbb{R}^n$ with $U$ open, is defined to be: $$\text{grad}(f):=\sum_{i=1}^n \frac{\partial f}{\partial x_i}$$ but I am failing to see why it would not be $$:=\sum_{i=1}^n \frac{\partial f}{\partial x^i}\frac{\partial }{\partial x^i}$$ so that evaluated at $p\in U$ , we get the gradient vector at $p$ .  In other words, how is $\frac{\partial f}{\partial x^i}$ a vector field? Thanks","Consider the tangent space , and suppose is a basis. So my textbook says that the gradient of a function , , with open, is defined to be: but I am failing to see why it would not be so that evaluated at , we get the gradient vector at .  In other words, how is a vector field? Thanks",T_p\mathbb{R}^n \{\big(\frac{\partial }{\partial x^i}\big)_p\} f f\in C^\infty(U) U\subseteq\mathbb{R}^n U \text{grad}(f):=\sum_{i=1}^n \frac{\partial f}{\partial x_i} :=\sum_{i=1}^n \frac{\partial f}{\partial x^i}\frac{\partial }{\partial x^i} p\in U p \frac{\partial f}{\partial x^i},"['differential-geometry', 'smooth-manifolds', 'differential-forms', 'tangent-bundle']"
34,Is the orthogonal polar factor the unique submersion satisfying an orthogonality relation?,Is the orthogonal polar factor the unique submersion satisfying an orthogonality relation?,,"$\newcommand{\psym}{\text{Psym}_n}$ $\newcommand{\sym}{\text{sym}}$ $\newcommand{\Sym}{\operatorname{Sym}}$ $\newcommand{\Skew}{\operatorname{Skew}}$ $\renewcommand{\skew}{\operatorname{skew}}$ $\newcommand{\GLp}{\operatorname{GL}_n^+}$ $\newcommand{\SO}{\operatorname{SO}_n}$ The orthogonal polar factor map $O:\GLp \to \SO$ , defined by requiring $A= O(A)P$ for some symmetric positive-definite $P$ , is a smooth submersion satisfying $A \perp T_{O(A)}\SO$ . Question: Let $F:\GLp \to \SO$ be a smooth submersion satisfying $A \perp T_{F(A)}\SO$ . Does $F(A)=Q \cdot O(A)$ or $F(A)= O(A)  \cdot Q$ for some $Q \in \SO$ ? Edit: An equivalent reformulation of the question: $A \perp T_{F(A)}\SO=F(A)\skew \iff A \in (F(A)\skew)^\perp=F(A)(\skew)^\perp=F(A)\sym$ . Thus, if we define $S(A)=F(A)^{-1}A$ , $S:\GLp \to \sym$ is smooth. So, a submersion $F:\GLp \to \SO$ satisfies the orthogonality requirement if and only if there exist a smooth map $S:\GLp \to \sym$ , satisfying $A=F(A)S(A)$ . Using the polar decomposition, we have $O(A)P(A)=A=F(A)S(A)$ , so $S(A)=Q(A)P(A)$ where $Q(A)=F(A)^{-1}O(A)$ . We want to prove that $Q:\GLp \to \SO$ is constant. I will now prove that for matrices $A$ having distinct singular values, $Q(A)$ can obtain a finite number of values. (The set of admissible values depends on $A$ ). I am quite sure this fact can be used to force $Q$ to be constant or at least something very constrained, but I am not sure how. Here is the proof: Since $S(A)=Q(A)P(A) \in \sym$ , we have $PQ^T=(QP)^T=S^T=S=QP$ . By orthogonally diagonalizing $P$ , we can write $P=V\Sigma V^T$ , so we now have $$ V\Sigma V^T Q^T=QV\Sigma V^T \Rightarrow \Sigma V^T Q^TV=V^TQV\Sigma. $$ Setting $\tilde Q=V^TQV$ we thus have $ \Sigma \tilde Q^T= \tilde Q \Sigma$ where $\tilde Q \in \SO$ . Since we assumed that the singular values of $A$ are distinct (i.e. the diagonal entries of $\Sigma$ are distinct), an explicit calculation now shows that $ \tilde Q$ must be diagonal. Since it is also orthogonal, we must have $\tilde Q_{ii}=\pm 1$ for all $i$ . So, $\tilde Q$ can assume a finite set of values; (which implies the same thing for $ Q$ ). Comment: I am not sure for which $Q \in \SO$ $F(A)=Q\cdot O(A)$ satisfies the requirement. A necessary condition is $Q^2=Id$ ; I don't know if it's sufficient. Indeed, let $Q \in \SO$ , and set $F(A)=Q\cdot O(A)$ . Then $ A \perp T_{F(A)}\SO=T_{Q\cdot O(A)}\SO=QT_{O(A)}\SO,$ so for $A=Id$ we have $ Id \perp Q\skew \Rightarrow Q^T \perp \skew \Rightarrow Q^T \in \sym \Rightarrow Q^2=Id$ .","The orthogonal polar factor map , defined by requiring for some symmetric positive-definite , is a smooth submersion satisfying . Question: Let be a smooth submersion satisfying . Does or for some ? Edit: An equivalent reformulation of the question: . Thus, if we define , is smooth. So, a submersion satisfies the orthogonality requirement if and only if there exist a smooth map , satisfying . Using the polar decomposition, we have , so where . We want to prove that is constant. I will now prove that for matrices having distinct singular values, can obtain a finite number of values. (The set of admissible values depends on ). I am quite sure this fact can be used to force to be constant or at least something very constrained, but I am not sure how. Here is the proof: Since , we have . By orthogonally diagonalizing , we can write , so we now have Setting we thus have where . Since we assumed that the singular values of are distinct (i.e. the diagonal entries of are distinct), an explicit calculation now shows that must be diagonal. Since it is also orthogonal, we must have for all . So, can assume a finite set of values; (which implies the same thing for ). Comment: I am not sure for which satisfies the requirement. A necessary condition is ; I don't know if it's sufficient. Indeed, let , and set . Then so for we have .","\newcommand{\psym}{\text{Psym}_n} \newcommand{\sym}{\text{sym}} \newcommand{\Sym}{\operatorname{Sym}} \newcommand{\Skew}{\operatorname{Skew}} \renewcommand{\skew}{\operatorname{skew}} \newcommand{\GLp}{\operatorname{GL}_n^+} \newcommand{\SO}{\operatorname{SO}_n} O:\GLp \to \SO A=
O(A)P P A \perp T_{O(A)}\SO F:\GLp \to \SO A \perp T_{F(A)}\SO F(A)=Q \cdot O(A) F(A)= O(A)  \cdot Q Q \in \SO A \perp T_{F(A)}\SO=F(A)\skew \iff A \in (F(A)\skew)^\perp=F(A)(\skew)^\perp=F(A)\sym S(A)=F(A)^{-1}A S:\GLp \to \sym F:\GLp \to \SO S:\GLp \to \sym A=F(A)S(A) O(A)P(A)=A=F(A)S(A) S(A)=Q(A)P(A) Q(A)=F(A)^{-1}O(A) Q:\GLp \to \SO A Q(A) A Q S(A)=Q(A)P(A) \in \sym PQ^T=(QP)^T=S^T=S=QP P P=V\Sigma V^T  V\Sigma V^T Q^T=QV\Sigma V^T \Rightarrow \Sigma V^T Q^TV=V^TQV\Sigma.  \tilde Q=V^TQV  \Sigma \tilde Q^T= \tilde Q \Sigma \tilde Q \in \SO A \Sigma  \tilde Q \tilde Q_{ii}=\pm 1 i \tilde Q  Q Q \in \SO F(A)=Q\cdot O(A) Q^2=Id Q \in \SO F(A)=Q\cdot O(A)  A \perp T_{F(A)}\SO=T_{Q\cdot O(A)}\SO=QT_{O(A)}\SO, A=Id  Id \perp Q\skew \Rightarrow Q^T \perp \skew \Rightarrow Q^T \in \sym \Rightarrow Q^2=Id","['differential-geometry', 'lie-groups', 'riemannian-geometry', 'matrix-decomposition', 'orthogonal-matrices']"
35,"The Metric Tensor, A Body of Mass m and Minkowski Space","The Metric Tensor, A Body of Mass m and Minkowski Space",,"By saying the body has mass m, we mean that the metric approaches that of Minkowski space for large r and that $$g_{00} \sim 1-2m/r$$ This was under a section in General Relativity by Woodhouse titled The Fiedl of a Static Spherical Body I have no idea how the sentence implies the equation (so obviously)? I cannot see why we are only considering the 00 component and the right hand side I have no idea.","By saying the body has mass m, we mean that the metric approaches that of Minkowski space for large r and that This was under a section in General Relativity by Woodhouse titled The Fiedl of a Static Spherical Body I have no idea how the sentence implies the equation (so obviously)? I cannot see why we are only considering the 00 component and the right hand side I have no idea.",g_{00} \sim 1-2m/r,"['differential-geometry', 'mathematical-physics', 'tensors', 'general-relativity']"
36,Gaussian curvature of ruled surfaces,Gaussian curvature of ruled surfaces,,"Let $c: I \rightarrow \mathbb{R}^3$ be a regular curve, $V: I \rightarrow \mathbb{S}^2$ a vector field and $a < b$ . Then we call $$ f: (a,b)\times I \rightarrow \mathbb{R}^3, \quad f(s,t):= c(t) +sV(t) $$ a ruled surface . Show that $f$ has gaussian curvature $K(s, t) \leq 0$ . For the first fundamental form, I obtained $$ G = \begin{pmatrix} 1 & \langle V, c'\rangle \\ \langle V, c'\rangle & \lvert c' + sV' \rvert^2 \end{pmatrix} $$ and for the second fundamental form $$ B = \frac{1}{\lvert V \times ( c' + sV') \rvert} \begin{pmatrix} 0 & \langle V', V \times (c' + sV') \rangle \\ \langle V', V \times (c'+sV') \rangle & * \end{pmatrix} $$ and thus (with $V \perp V'$ ) that $$ K = \frac{\det B}{\det G} = \frac{-2 \langle V', V\times c' \rangle}{\lvert c' + sV' \rvert^2 -2\langle V, c' \rangle} \cdot \frac{1}{\lvert V \times (c' + sV') \rvert}. $$ I know that this question was already answered here: Gaussian and Mean Curvatures for a Ruled Surface . However, there are additional assumptions made such as $c' \perp V'$ and $\lvert V' \rvert = 1$ . I don't know how to apply that as my case is a bit more general.","Let be a regular curve, a vector field and . Then we call a ruled surface . Show that has gaussian curvature . For the first fundamental form, I obtained and for the second fundamental form and thus (with ) that I know that this question was already answered here: Gaussian and Mean Curvatures for a Ruled Surface . However, there are additional assumptions made such as and . I don't know how to apply that as my case is a bit more general.","c: I \rightarrow \mathbb{R}^3 V: I \rightarrow \mathbb{S}^2 a < b 
f: (a,b)\times I \rightarrow \mathbb{R}^3, \quad f(s,t):= c(t) +sV(t)
 f K(s, t) \leq 0 
G = \begin{pmatrix}
1 & \langle V, c'\rangle \\
\langle V, c'\rangle & \lvert c' + sV' \rvert^2
\end{pmatrix}
 
B = \frac{1}{\lvert V \times ( c' + sV') \rvert} \begin{pmatrix}
0 & \langle V', V \times (c' + sV') \rangle \\
\langle V', V \times (c'+sV') \rangle & *
\end{pmatrix}
 V \perp V' 
K = \frac{\det B}{\det G} = \frac{-2 \langle V', V\times c' \rangle}{\lvert c' + sV' \rvert^2 -2\langle V, c' \rangle} \cdot \frac{1}{\lvert V \times (c' + sV') \rvert}.
 c' \perp V' \lvert V' \rvert = 1","['differential-geometry', 'surfaces', 'curvature']"
37,"If $[X, Y]=0$, then $\operatorname{Ad}_{e^{tX}}Y = Y$ for all $t\in \mathbb{R}$?","If , then  for all ?","[X, Y]=0 \operatorname{Ad}_{e^{tX}}Y = Y t\in \mathbb{R}","Let $G$ be a Lie group and $\mathfrak{g}$ be its Lie algebra. Let $X$ and $Y$ be commuting elements of $\mathfrak{g}$ , i.e., $[X, Y]=0$ . I want to show that $\operatorname{Ad}_{e^{tX}}Y = Y$ for all $t\in \mathbb{R}$ . I know that tangent map of $\operatorname{Ad}$ is $\operatorname{ad}$ and $\operatorname{ad}(X)Y = [X, Y]=0$ . Can this imply $\operatorname{Ad}_{e^{tX}}Y = Y$ for all $t\in \mathbb{R}$ ? Any hints or reference are appreciated.","Let be a Lie group and be its Lie algebra. Let and be commuting elements of , i.e., . I want to show that for all . I know that tangent map of is and . Can this imply for all ? Any hints or reference are appreciated.","G \mathfrak{g} X Y \mathfrak{g} [X, Y]=0 \operatorname{Ad}_{e^{tX}}Y = Y t\in \mathbb{R} \operatorname{Ad} \operatorname{ad} \operatorname{ad}(X)Y = [X, Y]=0 \operatorname{Ad}_{e^{tX}}Y = Y t\in \mathbb{R}","['differential-geometry', 'lie-groups', 'lie-algebras']"
38,"Question concerning the surface $\phi(u, v)=(u, v^3, u-v)$",Question concerning the surface,"\phi(u, v)=(u, v^3, u-v)","Let $\phi:R^2 \rightarrow R^3$ ,$C^{\infty}$ with $\phi(u,v)=(u,v^3,u-v)$. And $\gamma(t)=(3t,t^6,3t-t^2)$ smooth curve . Prove : $\textbf{a)}$ $M=\phi(R^2)$ is a smooth surface. $\textbf{b)}$  that $\gamma(R) \subset M$. Which is the $C^{\infty}$ parametrized curve $\phi^{-1} \circ \gamma$ $\textbf{c)}$ express the velocity $\dot{\gamma(0)}$ as a linear compination of the vectors of the basis of the tangent plane $T_0M$ at $\gamma(0)=(0,0,0)$. Solution : $\textbf{a)}$ $\phi$ is smooth so only think that remains is to prove that it is an acceptable parametrization for its image. It is 1-1 as its components is 1-1. Onto into its image .its Jacobian matrix has rank 2 . Since its collumns are $(1,0,1),(0,3v^2,-1)$ which with Gauss ellimination even if $v=0$ still there are 2 independent collumns. Only think left is to prove that its inverse IS continuous.$\textbf{ How do i find its inverse?}$ $\textbf{b)}$ Let $t \in R$ then for every $(3t,t^6,3t-t^2)$ i can find $(u(t),v(t))$ such that  $\phi(u(t),v(t)=(3t,t^6,3t-t^2)$ $u(t)=3t$,$ v(t)=t^2$ so every point of the curve is on $M$. I think my justification is awkward how would i write that clearer and more rigorously mathematically??. To find the $\phi^{-1}\circ \gamma$ $\textbf{i need to find the inverse from a).}$ $\textbf{c)}$ is pure calculations it is easy.$d\phi/du(0,0)=(1,0,1)$ $d\phi/dv(0,0)=(0,0,-1)$. $\dot{\gamma(0)}=(3,0,3)=3(d\phi/du)+0(d\phi/dv)$ Are my calculations right?","Let $\phi:R^2 \rightarrow R^3$ ,$C^{\infty}$ with $\phi(u,v)=(u,v^3,u-v)$. And $\gamma(t)=(3t,t^6,3t-t^2)$ smooth curve . Prove : $\textbf{a)}$ $M=\phi(R^2)$ is a smooth surface. $\textbf{b)}$  that $\gamma(R) \subset M$. Which is the $C^{\infty}$ parametrized curve $\phi^{-1} \circ \gamma$ $\textbf{c)}$ express the velocity $\dot{\gamma(0)}$ as a linear compination of the vectors of the basis of the tangent plane $T_0M$ at $\gamma(0)=(0,0,0)$. Solution : $\textbf{a)}$ $\phi$ is smooth so only think that remains is to prove that it is an acceptable parametrization for its image. It is 1-1 as its components is 1-1. Onto into its image .its Jacobian matrix has rank 2 . Since its collumns are $(1,0,1),(0,3v^2,-1)$ which with Gauss ellimination even if $v=0$ still there are 2 independent collumns. Only think left is to prove that its inverse IS continuous.$\textbf{ How do i find its inverse?}$ $\textbf{b)}$ Let $t \in R$ then for every $(3t,t^6,3t-t^2)$ i can find $(u(t),v(t))$ such that  $\phi(u(t),v(t)=(3t,t^6,3t-t^2)$ $u(t)=3t$,$ v(t)=t^2$ so every point of the curve is on $M$. I think my justification is awkward how would i write that clearer and more rigorously mathematically??. To find the $\phi^{-1}\circ \gamma$ $\textbf{i need to find the inverse from a).}$ $\textbf{c)}$ is pure calculations it is easy.$d\phi/du(0,0)=(1,0,1)$ $d\phi/dv(0,0)=(0,0,-1)$. $\dot{\gamma(0)}=(3,0,3)=3(d\phi/du)+0(d\phi/dv)$ Are my calculations right?",,"['differential-geometry', 'surfaces']"
39,"$df_1,...,df_k$ linearly independent $\Rightarrow \frac{\omega^n}{n!}=df_1\wedge...\wedge df_k\wedge\sigma$",linearly independent,"df_1,...,df_k \Rightarrow \frac{\omega^n}{n!}=df_1\wedge...\wedge df_k\wedge\sigma","Let $(M,\omega)$ be a symplectic manifold, $H,f_1,...,f_k\in C^\infty(M)$ non-zero functions with $\{H,f_i\}=0$. If $c\in\mathbb{R}^k$ is a regular value of $F:=(f_1,...,f_k):M\to \mathbb{R}^k$, consider the submanifold $M_c:=F^{-1}(c)$. a) Let $U$ be a neighbourhood of $M_c$ in which $df_1,...,df_k$ are linearly independent. Show that $\Lambda_\omega:=\frac{\omega^n}{n!}$ can be written as $\Lambda_\omega=df_1\wedge...\wedge df_k\wedge \sigma$ for some $\sigma\in\Omega^{2n-k}(M)$. [hint: find $\sigma$ locally and use partitions of unity] b) Show that $df_1\wedge...\wedge df_k\wedge L_{X_H}(\sigma)=0$ and use this fact to see that $L_{X_H}(\sigma)$ can be written as $L_{X_H}(\sigma)=\sum_{i=1}^kdf_i\wedge \rho_i$. Conclude that $\Lambda_c:=i^*\sigma$ is invariant by the flow of $H$ (where $i:M_c\hookrightarrow M$ is the inclusion). c) Show that $\Lambda_c$ does not depend on the choice of $\sigma$. Here is where I'm at: a) Taking Darboux coordinates $(x_1,...,x_n,y_1,...,y_n)$ and considering $x_{n+i}:=y_i$, we have $\Lambda_\omega=dx_1\wedge...\wedge dx_{2n}$, $df_i=\sum_{j=1}^{2n}\frac{\partial f_i}{\partial x_j}dx_j$. Consequently $$df_1\wedge...\wedge df_k=\sum_{1\leq j_1<...<j_k\leq 2n}\det(M_{j_1,...,j_k})\,dx_{j_1}\wedge...\wedge dx_{j_k} $$ where $M_{j_1,...,j_k}$ are $k\times k$ minors of the matrix $\left(\frac{\partial f_i}{\partial x_j}\right)_{i=1,...,k,j=1,...,2n}$ given by the columns $j_1,...,j_k$. Taking $p$ with $(df_1)_p,...,(df_k)_p$ linearly independent, we may assume $\det(M_{1,...,k})\neq 0$, so $$\sigma:=\frac{1}{\det(M_{1,...,k})}dx_{k+1}\wedge...\wedge dx_{2n}$$ is such that $df_1\wedge...\wedge df_k\wedge \sigma=dx_1\wedge...\wedge dx_{2n}=\Lambda_\omega$. The problem is that this works for a neighbourhood $V\subset M$ with $V\cap U\neq\emptyset$, but I don't know how to extend it for the whole $U$. I don't get the hint, because introducing a partition $\{U_\alpha, \rho_\alpha\}$ and defining $\sigma$ as a $\rho_\alpha$-linear combination may break the equality with $\Lambda_\omega$. b) I can show that $df_1\wedge...\wedge df_k\wedge L_{X_H}(\sigma)=0$, but I don't see how to use this to prove $\sigma$ can be written that way. Besides I don't know what it means for $\Lambda_c$ to be invariant by the flow of $H$. c) Taking another $\sigma'$ with the same property, we need to prove  $$i^*(df_1)\wedge...\wedge i^*(df_k)\wedge i^*(\sigma-\sigma')=0$$ but I don't know how to deal with the pullbacks.","Let $(M,\omega)$ be a symplectic manifold, $H,f_1,...,f_k\in C^\infty(M)$ non-zero functions with $\{H,f_i\}=0$. If $c\in\mathbb{R}^k$ is a regular value of $F:=(f_1,...,f_k):M\to \mathbb{R}^k$, consider the submanifold $M_c:=F^{-1}(c)$. a) Let $U$ be a neighbourhood of $M_c$ in which $df_1,...,df_k$ are linearly independent. Show that $\Lambda_\omega:=\frac{\omega^n}{n!}$ can be written as $\Lambda_\omega=df_1\wedge...\wedge df_k\wedge \sigma$ for some $\sigma\in\Omega^{2n-k}(M)$. [hint: find $\sigma$ locally and use partitions of unity] b) Show that $df_1\wedge...\wedge df_k\wedge L_{X_H}(\sigma)=0$ and use this fact to see that $L_{X_H}(\sigma)$ can be written as $L_{X_H}(\sigma)=\sum_{i=1}^kdf_i\wedge \rho_i$. Conclude that $\Lambda_c:=i^*\sigma$ is invariant by the flow of $H$ (where $i:M_c\hookrightarrow M$ is the inclusion). c) Show that $\Lambda_c$ does not depend on the choice of $\sigma$. Here is where I'm at: a) Taking Darboux coordinates $(x_1,...,x_n,y_1,...,y_n)$ and considering $x_{n+i}:=y_i$, we have $\Lambda_\omega=dx_1\wedge...\wedge dx_{2n}$, $df_i=\sum_{j=1}^{2n}\frac{\partial f_i}{\partial x_j}dx_j$. Consequently $$df_1\wedge...\wedge df_k=\sum_{1\leq j_1<...<j_k\leq 2n}\det(M_{j_1,...,j_k})\,dx_{j_1}\wedge...\wedge dx_{j_k} $$ where $M_{j_1,...,j_k}$ are $k\times k$ minors of the matrix $\left(\frac{\partial f_i}{\partial x_j}\right)_{i=1,...,k,j=1,...,2n}$ given by the columns $j_1,...,j_k$. Taking $p$ with $(df_1)_p,...,(df_k)_p$ linearly independent, we may assume $\det(M_{1,...,k})\neq 0$, so $$\sigma:=\frac{1}{\det(M_{1,...,k})}dx_{k+1}\wedge...\wedge dx_{2n}$$ is such that $df_1\wedge...\wedge df_k\wedge \sigma=dx_1\wedge...\wedge dx_{2n}=\Lambda_\omega$. The problem is that this works for a neighbourhood $V\subset M$ with $V\cap U\neq\emptyset$, but I don't know how to extend it for the whole $U$. I don't get the hint, because introducing a partition $\{U_\alpha, \rho_\alpha\}$ and defining $\sigma$ as a $\rho_\alpha$-linear combination may break the equality with $\Lambda_\omega$. b) I can show that $df_1\wedge...\wedge df_k\wedge L_{X_H}(\sigma)=0$, but I don't see how to use this to prove $\sigma$ can be written that way. Besides I don't know what it means for $\Lambda_c$ to be invariant by the flow of $H$. c) Taking another $\sigma'$ with the same property, we need to prove  $$i^*(df_1)\wedge...\wedge i^*(df_k)\wedge i^*(\sigma-\sigma')=0$$ but I don't know how to deal with the pullbacks.",,"['differential-geometry', 'differential-forms', 'symplectic-geometry']"
40,Gradient and Riemannian Hessian of function on an ellipse,Gradient and Riemannian Hessian of function on an ellipse,,"This is from an engineering assignment on Riemannian manifolds and I confess that this is my first experience with manifolds. Here is the problem: Let $\mathcal{M}\subset \mathbb{R}^2$ be an ellipse  of fixed major and minor axis. Lets say the embedding is $x=a\cos(\theta)$ and $y=b \sin(\theta)$. Let $f(\theta): \mathcal{M} \rightarrow \mathbb{R}$ be a smooth function on the ellipse. I wish to know what the gradient and Riemannian Hessian of $f$ will be (using local coordinates). My attempt: The Jacobian is $D=[-a \sin(\theta), b \cos(\theta)]^T$ so the inherited metric is $g_{1,1}=D^tD=a^2\sin^2(\theta)+b^2 \cos^2(\theta)$. Now the gradient should be $\Delta f= g^{1,1}\frac{d}{d\theta}=\left(a^2\sin^2(\theta)+b^2 \cos(\theta)\right)^{-1} \frac{df}{d\theta}$. Here the superscript is for inverse. For the Riemannian Hessian I think it should just be $\frac{d^2}{d\theta^2}$ but I do not know how to compute it. My intuition in this problem is that it should coincide with the result from change of coordinates: Writing the gradient like $\left(\frac{df}{dx}, \frac{df}{dy}\right)$ and then using the chain rule. Similarly for the Hessian matrix $H_{i,j=1}^2=\frac{d^2f}{dx^idx^j}$  where $x^1=x, x^2=y$. However, the answers I got do not coincide with that. If it helps solve the problem, note that eventually I am hoping to verify that the trace of the Hessian is the Laplace-Beltrami, which I have also not calculated yet but hope that I can after this is figured out.","This is from an engineering assignment on Riemannian manifolds and I confess that this is my first experience with manifolds. Here is the problem: Let $\mathcal{M}\subset \mathbb{R}^2$ be an ellipse  of fixed major and minor axis. Lets say the embedding is $x=a\cos(\theta)$ and $y=b \sin(\theta)$. Let $f(\theta): \mathcal{M} \rightarrow \mathbb{R}$ be a smooth function on the ellipse. I wish to know what the gradient and Riemannian Hessian of $f$ will be (using local coordinates). My attempt: The Jacobian is $D=[-a \sin(\theta), b \cos(\theta)]^T$ so the inherited metric is $g_{1,1}=D^tD=a^2\sin^2(\theta)+b^2 \cos^2(\theta)$. Now the gradient should be $\Delta f= g^{1,1}\frac{d}{d\theta}=\left(a^2\sin^2(\theta)+b^2 \cos(\theta)\right)^{-1} \frac{df}{d\theta}$. Here the superscript is for inverse. For the Riemannian Hessian I think it should just be $\frac{d^2}{d\theta^2}$ but I do not know how to compute it. My intuition in this problem is that it should coincide with the result from change of coordinates: Writing the gradient like $\left(\frac{df}{dx}, \frac{df}{dy}\right)$ and then using the chain rule. Similarly for the Hessian matrix $H_{i,j=1}^2=\frac{d^2f}{dx^idx^j}$  where $x^1=x, x^2=y$. However, the answers I got do not coincide with that. If it helps solve the problem, note that eventually I am hoping to verify that the trace of the Hessian is the Laplace-Beltrami, which I have also not calculated yet but hope that I can after this is figured out.",,"['differential-geometry', 'riemannian-geometry']"
41,Self-dual connections and Einstein 4-manifolds,Self-dual connections and Einstein 4-manifolds,,"I'm reading Besse's book on Einstein manifolds. Theorem 13.14 (due to Atiyah, Hitchin, and Singer) states that a Riemannian 4-manifold is Einstein iff the Levi-Civita connection on $\Lambda^+$ is self-dual. Here $\Lambda^2=\Lambda^+\oplus\Lambda^-$ is the space of two forms, which decomposes due to the fact that the Hodge star operator $\ast\in \mathrm{End}(\Lambda^2)$ is an involution ($\ast^2=1$) in dimension 4. Question: Does this theorem imply that an Einstein 4-manifold is self-dual  (i.e. $W^-=0$, where $W=W^++W^-$ is the Weyl tensor of the manifold)? If not, when is an Einstein 4-manifold self-dual?","I'm reading Besse's book on Einstein manifolds. Theorem 13.14 (due to Atiyah, Hitchin, and Singer) states that a Riemannian 4-manifold is Einstein iff the Levi-Civita connection on $\Lambda^+$ is self-dual. Here $\Lambda^2=\Lambda^+\oplus\Lambda^-$ is the space of two forms, which decomposes due to the fact that the Hodge star operator $\ast\in \mathrm{End}(\Lambda^2)$ is an involution ($\ast^2=1$) in dimension 4. Question: Does this theorem imply that an Einstein 4-manifold is self-dual  (i.e. $W^-=0$, where $W=W^++W^-$ is the Weyl tensor of the manifold)? If not, when is an Einstein 4-manifold self-dual?",,"['differential-geometry', 'vector-bundles']"
42,Inclusion between spin groups?,Inclusion between spin groups?,,"I think this should have an answer, but I can't see what it is. It's inspired by the section labelled ""Spinors"" in Parker's and Taubes's paper, ""On Witten's Proof of the Positive Energy Theorem."" Here's the question: Take $V$ be a real four dimensional vector space with an inner product of signature $(3,1)$, e.g. Minkowski space. By choosing a timelike vector $e_0$ we get an inclusion of $SO(3)$ into the identity component of $SO(3,1)$ by identifying elements of $SO(3)$ with transformations that fix the the orthogonal complement of $e_0$. Does this induce an injection of $Spin(3)$ into $Spin(3,1)$ such that the inclusions commute with the covering maps?","I think this should have an answer, but I can't see what it is. It's inspired by the section labelled ""Spinors"" in Parker's and Taubes's paper, ""On Witten's Proof of the Positive Energy Theorem."" Here's the question: Take $V$ be a real four dimensional vector space with an inner product of signature $(3,1)$, e.g. Minkowski space. By choosing a timelike vector $e_0$ we get an inclusion of $SO(3)$ into the identity component of $SO(3,1)$ by identifying elements of $SO(3)$ with transformations that fix the the orthogonal complement of $e_0$. Does this induce an injection of $Spin(3)$ into $Spin(3,1)$ such that the inclusions commute with the covering maps?",,"['differential-geometry', 'lie-groups', 'general-relativity', 'spin-geometry']"
43,Integral curves of height function's Hamiltonian vector field,Integral curves of height function's Hamiltonian vector field,,"Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$. Fixed $z \in \Bbb R^3$, we define $h_z\colon \Bbb S^2 \to \Bbb R$ by $h_z(p) \doteq \langle p,z\rangle$. I computed the Hamiltonian vector field of $h_z$ as $X_{h_z}(p) = z \times p$. I would like to find the integral curves of $X_{h_z}$. This means solving the system $\alpha'(t) = z \times \alpha(t)$. If $z = (a,b,c)$ and $$Z = \begin{pmatrix} 0 & -c & b \\ c & 0 & -a \\ -b & a & 0\end{pmatrix},$$we want to solve $\alpha'(t) = Z\alpha(t)$. Let's put the initial condition $\alpha(0) = p_0$ to ensure uniqueness of solution. Then $$\alpha(t) = \exp(tZ)p_0,$$and we use Rodrigues' rotation formula to get $$\alpha(t) = \left({\rm Id} + \frac{\sin \|tz\|}{\|tz\|}Z + \frac{1-\cos\|tz\|}{\|tz\|^2}Z^2\right)p_0.$$I'm having trouble visualizing this solution even in simple cases. We can assume that $p_0 = (0,0,1)$ since rotations are symplectomorphisms. If for example $z = (0,0,1)$, then $\alpha$ degenerates to a point. If $z = (0,1,0)$, then $\alpha(t)$ has the form $(\ast,0,\ast)$ and so is a pre-geodesic. What happens in the general case? My sign convention is that if $(M,\omega)$ is a symplectic manifold and $f \in \mathcal{C}^\infty(M)$, then $X_f$ satisfies $\iota_{X_f}\omega = {\rm d}f$.","Consider in $\Bbb S^2$ the symplectic form $\omega \in \Omega^2(\Bbb S^2)$ given by $\omega_p(v,w) = \langle p, v \times w\rangle$. Fixed $z \in \Bbb R^3$, we define $h_z\colon \Bbb S^2 \to \Bbb R$ by $h_z(p) \doteq \langle p,z\rangle$. I computed the Hamiltonian vector field of $h_z$ as $X_{h_z}(p) = z \times p$. I would like to find the integral curves of $X_{h_z}$. This means solving the system $\alpha'(t) = z \times \alpha(t)$. If $z = (a,b,c)$ and $$Z = \begin{pmatrix} 0 & -c & b \\ c & 0 & -a \\ -b & a & 0\end{pmatrix},$$we want to solve $\alpha'(t) = Z\alpha(t)$. Let's put the initial condition $\alpha(0) = p_0$ to ensure uniqueness of solution. Then $$\alpha(t) = \exp(tZ)p_0,$$and we use Rodrigues' rotation formula to get $$\alpha(t) = \left({\rm Id} + \frac{\sin \|tz\|}{\|tz\|}Z + \frac{1-\cos\|tz\|}{\|tz\|^2}Z^2\right)p_0.$$I'm having trouble visualizing this solution even in simple cases. We can assume that $p_0 = (0,0,1)$ since rotations are symplectomorphisms. If for example $z = (0,0,1)$, then $\alpha$ degenerates to a point. If $z = (0,1,0)$, then $\alpha(t)$ has the form $(\ast,0,\ast)$ and so is a pre-geodesic. What happens in the general case? My sign convention is that if $(M,\omega)$ is a symplectic manifold and $f \in \mathcal{C}^\infty(M)$, then $X_f$ satisfies $\iota_{X_f}\omega = {\rm d}f$.",,"['differential-geometry', 'symplectic-geometry']"
44,The connection between the solution set $\{F^{\nu}=0\}$ being invariant under a Lie group action and $DF$ having full rank,The connection between the solution set  being invariant under a Lie group action and  having full rank,\{F^{\nu}=0\} DF,"Suppose we have a function $F:M\rightarrow\mathbb{R}^l$ from a smooth $m$-manifold $M$ to $\mathbb{R}^l$, where $l\le m$. If we denote the components of $F$ by $F^{\nu}$, we can define a system of $l$ equations: $$F^{\nu}(x)=0.\tag{1}$$ for $\nu=1,\ldots,l$. Let $G$ be a (connected local) Lie group of transformations on $M$, suppose we want to find the condition for the solution set $(1)$ to be invariant under $G$. Note that $F$ itself need not be $G$-invariant. I would naively expect that the solution set to $(1)$ would be invariant iff $w(F^{\nu})=0$ for every infinitesimal generator $w$ of $G$, since (representing the exponential map as $e^{\epsilon w}$) $$F^{\nu}(e^{\epsilon w}x)=F^{\nu}(x)+w(F^{\nu})(x)\epsilon+o(\epsilon),\tag{2}$$ and so $$\left.\frac{d}{d\epsilon}F^{\nu}(e^{\epsilon w}x)\right\rvert_{\epsilon=0}=w(F^{\nu})(x).\tag{3}$$ The left hand side will be zero iff the right hand side is. However, it turns out (Theorem 2.8 in Olver's "" Applications of Lie Groups to Differential Equations "") that in addition to $(3)$ you also need the differential $DF$ to have maximum rank (on the solution set $(1)$). Could someone please tell me what I'm missing that makes (1-3) invalid?","Suppose we have a function $F:M\rightarrow\mathbb{R}^l$ from a smooth $m$-manifold $M$ to $\mathbb{R}^l$, where $l\le m$. If we denote the components of $F$ by $F^{\nu}$, we can define a system of $l$ equations: $$F^{\nu}(x)=0.\tag{1}$$ for $\nu=1,\ldots,l$. Let $G$ be a (connected local) Lie group of transformations on $M$, suppose we want to find the condition for the solution set $(1)$ to be invariant under $G$. Note that $F$ itself need not be $G$-invariant. I would naively expect that the solution set to $(1)$ would be invariant iff $w(F^{\nu})=0$ for every infinitesimal generator $w$ of $G$, since (representing the exponential map as $e^{\epsilon w}$) $$F^{\nu}(e^{\epsilon w}x)=F^{\nu}(x)+w(F^{\nu})(x)\epsilon+o(\epsilon),\tag{2}$$ and so $$\left.\frac{d}{d\epsilon}F^{\nu}(e^{\epsilon w}x)\right\rvert_{\epsilon=0}=w(F^{\nu})(x).\tag{3}$$ The left hand side will be zero iff the right hand side is. However, it turns out (Theorem 2.8 in Olver's "" Applications of Lie Groups to Differential Equations "") that in addition to $(3)$ you also need the differential $DF$ to have maximum rank (on the solution set $(1)$). Could someone please tell me what I'm missing that makes (1-3) invalid?",,"['differential-geometry', 'lie-groups']"
45,Stokes' theorem and non-globally defined quantities,Stokes' theorem and non-globally defined quantities,,"This is a question from a physicist, so please be kind.* Suppose that $M$ is an orientable smooth manifold without boundaries and $\omega$ a form of an appropriate degree such that it can be integrated over $M$, $$ I=\int_M \omega. $$ The objective is to compute $I$. According to the Poincaré or Dolbeault-Grothendieck lemma (for real and complex manifolds respectively), locally (in some coordinate neighbourhood, $U_i$, where $M$ looks like an open subset of $\mathbb{R}^n$ or $\mathbb{C}^n$, with $n$ a positive integer, and because i don’t want to restrict the scope i will write $\mathbb{K}$ for either of the two fields) $\omega$ is exact, $$ \omega_i=dA_i, $$  where $A_i$ is some differential form defined in $U_i$ of the appropriate degree. If $A_i$ were globally defined (in which case I suppose, $A_i=A$, for all $i$) then we could apply Stokes' theorem, $$ \int_MdA = \oint_{\partial M}A=0, $$ and learn that $I=0$ (because $\partial M$ is null). I am in the unfortunate (and I believe very common) situation where I only know $\omega$ locally (in particular I have an explicit expression for $A_i$) and I want to compute $I$. So my first question is the following: What precisely does it mean for $A_i$ to be only locally and not globally defined? How can I check whether my $\omega_i$ is globally or only locally exact given only an explicit local expression for $A_i$ and the corresponding transition functions on chart overlaps? For instance, might it be true that in order for $A_i$ to be globally defined it must transform under changes of coordinates as an antisymmetric tensor (on chart overlaps, $U_i\cap U_j$), and might this be sufficient for $A_i$ to be globally defined? Suppose now that I am in the fortunate situation where I know the answer to this question, and I have concluded that $A_i$ is not globally defined and hence that $\omega$ is not globally exact. The next question is: How can I explicitly reconstruct $I$ given the explicit local expression $\omega_i=dA_i$ on $U_i$ and the corresponding transition functions on patch overlaps $U_i\cap U_j$? To be slightly more precise here I am implicitly considering an atlas for $M$, i.e. a family of charts, $\{(U_i,\phi_i)\}$, with $\{U_i\}$ a family of open sets such that $\cup_i U_i=M$, and $\phi_i:U_i\rightarrow \mathbb{K }$ a homeomorphism from $U_i$ to an open subset of $\mathbb{K}$ (in particular, the maps $\phi_i$ are to be considered known and identified with a convenient set of local coordinates). The case of interest is when the transition functions $f_{ij}=\phi_i\circ \phi_j^{-1}$ from $\phi_j(U_i\cap U_j)$ to $\phi_i(U_i\cap U_j)$ are $C^{\infty}$ and also known. For question 2 Stokes' theorem comes to the rescue, given that in a patch $U_i$, $$ \int_{U_i}dA_i = \oint_{\partial U_i}A_i, $$ but how exactly does one sum over $i$ to reconstruct the full integral $I$ making use of the transition functions which map $A_j$ to $A_i$ on patch overlaps? *I have rewritten the question completely because following a fairly extensive discussion with @John Hughes (whom I think I annoyed quite a bit, see below) and his correspondingly good comments, it became very clear that my intended question was not clearly formulated.","This is a question from a physicist, so please be kind.* Suppose that $M$ is an orientable smooth manifold without boundaries and $\omega$ a form of an appropriate degree such that it can be integrated over $M$, $$ I=\int_M \omega. $$ The objective is to compute $I$. According to the Poincaré or Dolbeault-Grothendieck lemma (for real and complex manifolds respectively), locally (in some coordinate neighbourhood, $U_i$, where $M$ looks like an open subset of $\mathbb{R}^n$ or $\mathbb{C}^n$, with $n$ a positive integer, and because i don’t want to restrict the scope i will write $\mathbb{K}$ for either of the two fields) $\omega$ is exact, $$ \omega_i=dA_i, $$  where $A_i$ is some differential form defined in $U_i$ of the appropriate degree. If $A_i$ were globally defined (in which case I suppose, $A_i=A$, for all $i$) then we could apply Stokes' theorem, $$ \int_MdA = \oint_{\partial M}A=0, $$ and learn that $I=0$ (because $\partial M$ is null). I am in the unfortunate (and I believe very common) situation where I only know $\omega$ locally (in particular I have an explicit expression for $A_i$) and I want to compute $I$. So my first question is the following: What precisely does it mean for $A_i$ to be only locally and not globally defined? How can I check whether my $\omega_i$ is globally or only locally exact given only an explicit local expression for $A_i$ and the corresponding transition functions on chart overlaps? For instance, might it be true that in order for $A_i$ to be globally defined it must transform under changes of coordinates as an antisymmetric tensor (on chart overlaps, $U_i\cap U_j$), and might this be sufficient for $A_i$ to be globally defined? Suppose now that I am in the fortunate situation where I know the answer to this question, and I have concluded that $A_i$ is not globally defined and hence that $\omega$ is not globally exact. The next question is: How can I explicitly reconstruct $I$ given the explicit local expression $\omega_i=dA_i$ on $U_i$ and the corresponding transition functions on patch overlaps $U_i\cap U_j$? To be slightly more precise here I am implicitly considering an atlas for $M$, i.e. a family of charts, $\{(U_i,\phi_i)\}$, with $\{U_i\}$ a family of open sets such that $\cup_i U_i=M$, and $\phi_i:U_i\rightarrow \mathbb{K }$ a homeomorphism from $U_i$ to an open subset of $\mathbb{K}$ (in particular, the maps $\phi_i$ are to be considered known and identified with a convenient set of local coordinates). The case of interest is when the transition functions $f_{ij}=\phi_i\circ \phi_j^{-1}$ from $\phi_j(U_i\cap U_j)$ to $\phi_i(U_i\cap U_j)$ are $C^{\infty}$ and also known. For question 2 Stokes' theorem comes to the rescue, given that in a patch $U_i$, $$ \int_{U_i}dA_i = \oint_{\partial U_i}A_i, $$ but how exactly does one sum over $i$ to reconstruct the full integral $I$ making use of the transition functions which map $A_j$ to $A_i$ on patch overlaps? *I have rewritten the question completely because following a fairly extensive discussion with @John Hughes (whom I think I annoyed quite a bit, see below) and his correspondingly good comments, it became very clear that my intended question was not clearly formulated.",,"['differential-geometry', 'differential-topology', 'riemann-surfaces', 'de-rham-cohomology']"
46,Reference Request: Infinite-Dimensional Analysis,Reference Request: Infinite-Dimensional Analysis,,"I am aware that there is a whole lot of literature out there on the expansive field of infinite-dimensional analysis . Moreover, almost every book I've glanced at so far seems to have a different focus than the next one. Specifically, I am interested in the study of maps $f: M \to H$, where $M$ is a finite-dimensional manifold and $H$ is a Hilbert space (or a Banach space). Here are some concrete topics that I want to learn more about a) Fréchet-Derivatives (generalizing total derivatives), Gateaux-Derivaties (generalizing directional derivates), their connection, and the infinite-dimensional analogue of classic theorems such as the Mean-Value Theorem or Schwarz' theorem . b) (This is most important to me) Differential forms over $M$ with values in $H$, the corresponding concept of exterior derivative on such forms, integrating $k$-forms over $k$-dimensional embedded submanifolds (via the Bochner Integral ) and a generalization of Stokes Theorem . Does anybody know a good reference for this ?","I am aware that there is a whole lot of literature out there on the expansive field of infinite-dimensional analysis . Moreover, almost every book I've glanced at so far seems to have a different focus than the next one. Specifically, I am interested in the study of maps $f: M \to H$, where $M$ is a finite-dimensional manifold and $H$ is a Hilbert space (or a Banach space). Here are some concrete topics that I want to learn more about a) Fréchet-Derivatives (generalizing total derivatives), Gateaux-Derivaties (generalizing directional derivates), their connection, and the infinite-dimensional analogue of classic theorems such as the Mean-Value Theorem or Schwarz' theorem . b) (This is most important to me) Differential forms over $M$ with values in $H$, the corresponding concept of exterior derivative on such forms, integrating $k$-forms over $k$-dimensional embedded submanifolds (via the Bochner Integral ) and a generalization of Stokes Theorem . Does anybody know a good reference for this ?",,"['differential-geometry', 'reference-request', 'hilbert-spaces', 'differential-topology']"
47,Possible condition on locally Euclidean subset of Euclidean space to be embedded submanifold II,Possible condition on locally Euclidean subset of Euclidean space to be embedded submanifold II,,"Given a locally Euclidean (locally homeomorphic to some Euclidean space) subset $X\subset\mathbb R^n$ and $p\in X$, let $\widetilde{\mathrm{T}}_pX$ denote the tangent set of $X$ at $p$, namely the set of derivatives of differentiable curves in $X$ based at $p$. Suppose for all $p\in X$ we have that $\widetilde{\mathrm{T}}_pX$ is a linear subspace of $\mathbb R^n$ of dimension $\dim_pX$. Does it follow that $X\subset\mathbb R^n$ is an embbeded differentiable submanifold? Added. Here's a thought. Perhaps we can locally construct an exponential map which is a diffeomorphism between a neighborhood of the tangent plane and a neighborhood of $p$ in $X$. Using this diffeomorphism we can move between a differentiable structure on $X$ and the differentiable functions defined on the aforementioned neighborhood of the tangent plane.","Given a locally Euclidean (locally homeomorphic to some Euclidean space) subset $X\subset\mathbb R^n$ and $p\in X$, let $\widetilde{\mathrm{T}}_pX$ denote the tangent set of $X$ at $p$, namely the set of derivatives of differentiable curves in $X$ based at $p$. Suppose for all $p\in X$ we have that $\widetilde{\mathrm{T}}_pX$ is a linear subspace of $\mathbb R^n$ of dimension $\dim_pX$. Does it follow that $X\subset\mathbb R^n$ is an embbeded differentiable submanifold? Added. Here's a thought. Perhaps we can locally construct an exponential map which is a diffeomorphism between a neighborhood of the tangent plane and a neighborhood of $p$ in $X$. Using this diffeomorphism we can move between a differentiable structure on $X$ and the differentiable functions defined on the aforementioned neighborhood of the tangent plane.",,"['calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
48,Prove the geodesic on 2-sphere is the great circle,Prove the geodesic on 2-sphere is the great circle,,"I want to use the Killing vector fields to prove the geodesic on the sphere is the great circle. First of all, the given metric is $$ds^2=d\theta^{2}+\sin^2\theta d\phi^{2},$$ where I set the radius to be $1$. By Killing equation, $$\nabla_\mu K_\nu+\nabla_\nu K_\mu =0,$$ and some computation, I have the following three Killing vectors: \begin{align*} K_1 &= \partial_{\phi} \\ K_2 &= \cos\phi \, \partial_{\theta} - \cot\theta \sin\phi \partial_{\phi} \\ K_3 &= -\sin\phi \partial_{\theta} - \cot\theta \cos\phi \partial_{\phi} \end{align*} I want to use the fact that $$\frac{d}{d\lambda}\left\{K_\mu \frac{dx^{\mu}}{d\lambda}\right\}=0,$$ where $x$ is the curve and $\frac{dx^{\mu}}{d\lambda}$ is the tangent vector. However, as everyone knows, the geodesic is the great circle. Therefore, I thought the final equation would be of the form, $$aX+bY=0,$$ where $a$ and $b$ are some constant, while $X$ and $Y$ are positions on the sphere. The equation passes the origin. But I can't see how to get the desired results, please give me some help.","I want to use the Killing vector fields to prove the geodesic on the sphere is the great circle. First of all, the given metric is $$ds^2=d\theta^{2}+\sin^2\theta d\phi^{2},$$ where I set the radius to be $1$. By Killing equation, $$\nabla_\mu K_\nu+\nabla_\nu K_\mu =0,$$ and some computation, I have the following three Killing vectors: \begin{align*} K_1 &= \partial_{\phi} \\ K_2 &= \cos\phi \, \partial_{\theta} - \cot\theta \sin\phi \partial_{\phi} \\ K_3 &= -\sin\phi \partial_{\theta} - \cot\theta \cos\phi \partial_{\phi} \end{align*} I want to use the fact that $$\frac{d}{d\lambda}\left\{K_\mu \frac{dx^{\mu}}{d\lambda}\right\}=0,$$ where $x$ is the curve and $\frac{dx^{\mu}}{d\lambda}$ is the tangent vector. However, as everyone knows, the geodesic is the great circle. Therefore, I thought the final equation would be of the form, $$aX+bY=0,$$ where $a$ and $b$ are some constant, while $X$ and $Y$ are positions on the sphere. The equation passes the origin. But I can't see how to get the desired results, please give me some help.",,"['differential-geometry', 'riemannian-geometry', 'general-relativity', 'index-notation']"
49,Surfaces of revolution with curvature 0,Surfaces of revolution with curvature 0,,"I am trying to find all the surfaces of revolution with Gaussian curvature $K \equiv 0$. This is what I got so far. If we assume the surface of revolution is parametrized by $(\varphi(v) \cos u, \varphi(v) \sin u, \delta(v))$. Then since $\varphi'' + 0\cdot\varphi = 0$, $\varphi(v) = C\cdot v$ which implies that $\delta(v) = \int_0^v \sqrt{1 - C^2}dv = \sqrt{1 - C^2}\cdot v$. I do not know how to continue from this point. Any ideas? Thanks!","I am trying to find all the surfaces of revolution with Gaussian curvature $K \equiv 0$. This is what I got so far. If we assume the surface of revolution is parametrized by $(\varphi(v) \cos u, \varphi(v) \sin u, \delta(v))$. Then since $\varphi'' + 0\cdot\varphi = 0$, $\varphi(v) = C\cdot v$ which implies that $\delta(v) = \int_0^v \sqrt{1 - C^2}dv = \sqrt{1 - C^2}\cdot v$. I do not know how to continue from this point. Any ideas? Thanks!",,"['differential-geometry', 'surfaces', 'curvature', 'solid-of-revolution']"
50,"How can I show components of something transform as a (0,2) tensor?","How can I show components of something transform as a (0,2) tensor?",,"Suppose that $B_i$ are the components of a covariant vector. Show that $\frac{\partial B_j}{\partial x^k} - \frac{\partial B_k}{\partial x^j}$ are the components of a (0,2) tensor. I know that the components of a (0,2) tensor transform like this: $\frac{\partial \bar{B_j}}{\partial \bar{x^k}} -\frac{\partial \bar{B_k}}{\partial \bar{x^j}}  = \sum_{\alpha,\beta=1}^n \left ( \frac{\partial B_{\alpha}}{\partial x^{\beta}} - \frac{\partial B_{\beta}}{\partial x^{\alpha}}\right)\frac{\partial x^{\alpha}}{\partial \bar{x}^j} \frac{\partial x^{\beta}}{\partial \bar{x}^k}$ So $$\bar{B}_j = \sum_{\alpha=1}^n B_{\alpha} \frac{\partial x^{\alpha}}{\partial\bar{x}^j}$$ and $$\bar{B}_k = \sum_{\beta=1}^n B_{\beta} \frac{\partial x^{\beta}}{\partial\bar{x}^k}$$ Can someone please help me solve this question? I am preparing for my exam and my professor said this question is important to know and understand.","Suppose that $B_i$ are the components of a covariant vector. Show that $\frac{\partial B_j}{\partial x^k} - \frac{\partial B_k}{\partial x^j}$ are the components of a (0,2) tensor. I know that the components of a (0,2) tensor transform like this: $\frac{\partial \bar{B_j}}{\partial \bar{x^k}} -\frac{\partial \bar{B_k}}{\partial \bar{x^j}}  = \sum_{\alpha,\beta=1}^n \left ( \frac{\partial B_{\alpha}}{\partial x^{\beta}} - \frac{\partial B_{\beta}}{\partial x^{\alpha}}\right)\frac{\partial x^{\alpha}}{\partial \bar{x}^j} \frac{\partial x^{\beta}}{\partial \bar{x}^k}$ So $$\bar{B}_j = \sum_{\alpha=1}^n B_{\alpha} \frac{\partial x^{\alpha}}{\partial\bar{x}^j}$$ and $$\bar{B}_k = \sum_{\beta=1}^n B_{\beta} \frac{\partial x^{\beta}}{\partial\bar{x}^k}$$ Can someone please help me solve this question? I am preparing for my exam and my professor said this question is important to know and understand.",,"['differential-geometry', 'mathematical-physics', 'tensors', 'index-notation']"
51,How can I find the components of a metric tensor?,How can I find the components of a metric tensor?,,"Suppose $\vec{\phi}(x^1,x^2) = (x^1,x^2,(x^1)^2+(x^2)^2.$ The metric tensor induced by $\vec{\phi}$ is given by: $$g = \begin{pmatrix}1+4(x^2)^2 & 4x^1x^2 \\\ 4x^1x^2 & 1+4(x^2)^2\end{pmatrix}$$ Let $x^1 = \bar{x}^1\cos\bar{x}^2$ $x^2 = \bar{x}^1\sin\bar{x}^2$ Find the components of the metric tensor $\bar{g}_{12}=\bar{g}_{21},\bar{g}_{22}$ in the $\bar{x}^1,\bar{x}^2$ coordinate system. This is what I am thinking so far, but I am confused on how to solve the problem. $$\bar{g}_{kl} = \sum_{j=1}^2 \sum_{i=1}^2 g_{ij} \frac{\partial x^i}{\partial \bar{x}^k} \frac{\partial x^j}{\partial \bar{x}^l}$$ So in the case of $\bar{g}_{12} = \bar{g}_{21}$: $$\bar{g}_{12} = \sum_{j=1}^2 \sum_{i=1}^2 g_{ij} \frac{\partial x^i}{\partial \bar{x}^1} \frac{\partial x^j}{\partial \bar{x}^2}$$ $$ = g_{11} \frac{\partial x^1}{\partial\bar{x}^1}\frac{\partial x^1}{\partial\bar{x}^1}+ g_{12} \frac{\partial x^1}{\partial\bar{x}^1}\frac{\partial x^2}{\partial\bar{x}^1}+g_{21} \frac{\partial x^2}{\partial\bar{x}^1}\frac{\partial x^1}{\partial\bar{x}^1}+g_{22} \frac{\partial x^2}{\partial\bar{x}^1}\frac{\partial x^2}{\partial\bar{x}^1}$$ $\frac{\partial x^1}{\partial\bar{x}^1} = \cos \bar{x}^2$ $\frac{\partial x^2}{\partial\bar{x}^1} = \sin \bar{x}^2$ $\frac{\partial x^1}{\partial\bar{x}^2} = -\bar{x}^1 \sin \bar{x}^2$ $\frac{\partial x^2}{\partial\bar{x}^2} = \bar{x}^1\cos \bar{x}^2$ So $\bar{g}_{12} = (1+4(x^1)^2)(\cos\bar{x}^2)^2+2(4x^1x^2)(\cos \bar{x}^2)(\sin \bar{x}^2)+(1+4(x^2)^2)(\sin\bar{x}^2)^2$","Suppose $\vec{\phi}(x^1,x^2) = (x^1,x^2,(x^1)^2+(x^2)^2.$ The metric tensor induced by $\vec{\phi}$ is given by: $$g = \begin{pmatrix}1+4(x^2)^2 & 4x^1x^2 \\\ 4x^1x^2 & 1+4(x^2)^2\end{pmatrix}$$ Let $x^1 = \bar{x}^1\cos\bar{x}^2$ $x^2 = \bar{x}^1\sin\bar{x}^2$ Find the components of the metric tensor $\bar{g}_{12}=\bar{g}_{21},\bar{g}_{22}$ in the $\bar{x}^1,\bar{x}^2$ coordinate system. This is what I am thinking so far, but I am confused on how to solve the problem. $$\bar{g}_{kl} = \sum_{j=1}^2 \sum_{i=1}^2 g_{ij} \frac{\partial x^i}{\partial \bar{x}^k} \frac{\partial x^j}{\partial \bar{x}^l}$$ So in the case of $\bar{g}_{12} = \bar{g}_{21}$: $$\bar{g}_{12} = \sum_{j=1}^2 \sum_{i=1}^2 g_{ij} \frac{\partial x^i}{\partial \bar{x}^1} \frac{\partial x^j}{\partial \bar{x}^2}$$ $$ = g_{11} \frac{\partial x^1}{\partial\bar{x}^1}\frac{\partial x^1}{\partial\bar{x}^1}+ g_{12} \frac{\partial x^1}{\partial\bar{x}^1}\frac{\partial x^2}{\partial\bar{x}^1}+g_{21} \frac{\partial x^2}{\partial\bar{x}^1}\frac{\partial x^1}{\partial\bar{x}^1}+g_{22} \frac{\partial x^2}{\partial\bar{x}^1}\frac{\partial x^2}{\partial\bar{x}^1}$$ $\frac{\partial x^1}{\partial\bar{x}^1} = \cos \bar{x}^2$ $\frac{\partial x^2}{\partial\bar{x}^1} = \sin \bar{x}^2$ $\frac{\partial x^1}{\partial\bar{x}^2} = -\bar{x}^1 \sin \bar{x}^2$ $\frac{\partial x^2}{\partial\bar{x}^2} = \bar{x}^1\cos \bar{x}^2$ So $\bar{g}_{12} = (1+4(x^1)^2)(\cos\bar{x}^2)^2+2(4x^1x^2)(\cos \bar{x}^2)(\sin \bar{x}^2)+(1+4(x^2)^2)(\sin\bar{x}^2)^2$",,['differential-geometry']
52,Can I always find a smooth bundle map relating two arbitrary smooth vector fields defined on the same smooth manifold?,Can I always find a smooth bundle map relating two arbitrary smooth vector fields defined on the same smooth manifold?,,"Suppose to consider two arbitrary smooth vector fields $X: M\rightarrow TM$ and $Y: M\rightarrow TM$ defined on the same differentiable manifold $M$. Is there always a smooth map $A: TM\rightarrow TM$ (or a composition of smooth maps, none of them necessarily induced by a diffeomorphism $\varphi: M\rightarrow M$), such that $Y=A(X)$? If the answer is no, what are the conditions under which I can find such a smooth map $A$? If you know the answer, could you please suggest me a reference where this problem is treated?","Suppose to consider two arbitrary smooth vector fields $X: M\rightarrow TM$ and $Y: M\rightarrow TM$ defined on the same differentiable manifold $M$. Is there always a smooth map $A: TM\rightarrow TM$ (or a composition of smooth maps, none of them necessarily induced by a diffeomorphism $\varphi: M\rightarrow M$), such that $Y=A(X)$? If the answer is no, what are the conditions under which I can find such a smooth map $A$? If you know the answer, could you please suggest me a reference where this problem is treated?",,"['differential-geometry', 'smooth-manifolds', 'vector-fields', 'tangent-bundle']"
53,$g^*(\mbox{d}x) = \mbox{d}x \circ g^{-1}$ for $g$ an isometry,for  an isometry,g^*(\mbox{d}x) = \mbox{d}x \circ g^{-1} g,"Suppose $(M,\gamma)$ is a Riemannian manifold, $g$ an isometry, $\mbox{d}x$ the Riemannian volume form on $(M, \gamma)$. I can't really understand these formulae, sometimes found in literature. First, \begin{equation}\tag{1} g^*(\mbox{d}x) = \mbox{d}x\circ g^{-1}. \end{equation} Next, let $f : M \to N$ be a smooth function (here, $g$ can be seen as an element of a group $G$ acting by isometries in some specified way on both $M$ and $N$); sometimes I find \begin{equation}\tag{2} \vert{\mbox{d}(g \circ f)}\vert^2 = \vert \mbox{d}f\vert^2 \circ g^{-1}. \end{equation} I can't catch the precise meaning of the rhs. I know what the operations involved are (pullback, exterior derivative, commutation property of pullback and exterior derivative, differential, chain rule...); my problem concern what one precisely means when writing a thing such as $g^*(\mbox{d}x) = \mbox{d}x \circ g^{-1}$ instead of \begin{equation} g^*(\mbox{d}x) = \sqrt{|\gamma(g(x))|}J_g(x)\mbox{d}g^1(x) \wedge\dots \wedge \mbox{d}g^m(x) = \mbox{d}(g(x)). \end{equation} Here, $m = \dim M$, $J_g(x)$ the jacobian of the coordinate transformation $g$ evalueted at $x$ while $\sqrt{|g^*(\gamma(x))|} = \sqrt{|\gamma(g(x))|}$ because $g$ is an isometry. I'm not really into differential and Riemannian geometry, so I'm probably missing something entirely elementary.","Suppose $(M,\gamma)$ is a Riemannian manifold, $g$ an isometry, $\mbox{d}x$ the Riemannian volume form on $(M, \gamma)$. I can't really understand these formulae, sometimes found in literature. First, \begin{equation}\tag{1} g^*(\mbox{d}x) = \mbox{d}x\circ g^{-1}. \end{equation} Next, let $f : M \to N$ be a smooth function (here, $g$ can be seen as an element of a group $G$ acting by isometries in some specified way on both $M$ and $N$); sometimes I find \begin{equation}\tag{2} \vert{\mbox{d}(g \circ f)}\vert^2 = \vert \mbox{d}f\vert^2 \circ g^{-1}. \end{equation} I can't catch the precise meaning of the rhs. I know what the operations involved are (pullback, exterior derivative, commutation property of pullback and exterior derivative, differential, chain rule...); my problem concern what one precisely means when writing a thing such as $g^*(\mbox{d}x) = \mbox{d}x \circ g^{-1}$ instead of \begin{equation} g^*(\mbox{d}x) = \sqrt{|\gamma(g(x))|}J_g(x)\mbox{d}g^1(x) \wedge\dots \wedge \mbox{d}g^m(x) = \mbox{d}(g(x)). \end{equation} Here, $m = \dim M$, $J_g(x)$ the jacobian of the coordinate transformation $g$ evalueted at $x$ while $\sqrt{|g^*(\gamma(x))|} = \sqrt{|\gamma(g(x))|}$ because $g$ is an isometry. I'm not really into differential and Riemannian geometry, so I'm probably missing something entirely elementary.",,"['differential-geometry', 'riemannian-geometry']"
54,"How does one show that when almost complex structure is integrable, it takes the canonical form in a local patch?","How does one show that when almost complex structure is integrable, it takes the canonical form in a local patch?",,"On an arbitrary almost complex manifold, $M$, it is said that one can always find coordinates for which the almost complex structure $J$ takes the canonical form  $$ J_p=\left[\begin{array}{cc}     0 & -1 \\     1 & 0 	\\     \end{array}\right] $$ at any given point $p$. In general, however, it is not possible to find coordinates so that $J$ takes the canonical form on an entire neighborhood of $p$. Such coordinates, if they exist, are called 'local holomorphic coordinates for $J$'. If $M$ admits local holomorphic coordinates for $J$ around every point then these patch together to form a holomorphic atlas for $M$ giving it a complex structure, which moreover induces $J$. $J$ is then said to be 'integrable'. My question is, how does one show that when local holomorphic coordinates exist, $J$ takes the canonical form in a local patch?","On an arbitrary almost complex manifold, $M$, it is said that one can always find coordinates for which the almost complex structure $J$ takes the canonical form  $$ J_p=\left[\begin{array}{cc}     0 & -1 \\     1 & 0 	\\     \end{array}\right] $$ at any given point $p$. In general, however, it is not possible to find coordinates so that $J$ takes the canonical form on an entire neighborhood of $p$. Such coordinates, if they exist, are called 'local holomorphic coordinates for $J$'. If $M$ admits local holomorphic coordinates for $J$ around every point then these patch together to form a holomorphic atlas for $M$ giving it a complex structure, which moreover induces $J$. $J$ is then said to be 'integrable'. My question is, how does one show that when local holomorphic coordinates exist, $J$ takes the canonical form in a local patch?",,"['differential-geometry', 'complex-geometry', 'almost-complex']"
55,The Maximal Diameter Theorem,The Maximal Diameter Theorem,,"I am doing some theory concerning one of my courses which is, Selected Topic in Differential Geometry and I have two questions regarding The Maximal Diameter Theorem, from the Xia book. My first question is, why can we pick such an $a$ and $b$, as written in the proof? The second would be, how to compute that the first eigenvalue of an $n$-dimensional hemisphere is equal to $n$. Thanks in advance for any help.","I am doing some theory concerning one of my courses which is, Selected Topic in Differential Geometry and I have two questions regarding The Maximal Diameter Theorem, from the Xia book. My first question is, why can we pick such an $a$ and $b$, as written in the proof? The second would be, how to compute that the first eigenvalue of an $n$-dimensional hemisphere is equal to $n$. Thanks in advance for any help.",,"['differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
56,The Definition of an Almost Complex Manifold (Nakahara),The Definition of an Almost Complex Manifold (Nakahara),,"I am having problems making sense of Michio Nakahara's definition of the Almost Complex Structure/Almost Complex Manifold, such as it appears in Geometry, Topology and Physics (2nd Edition). On p. 318-319, he writes The tensor field $J$ is called the almost complex structure of a complex manifold $M$. Note that any $2m$-dimensional manifold locally admits a tensor field $J$ which squares to $-I_{2m}$. However, $J$ may be patched across charts and defined globally only on a complex manifold. What confuses me is when he later on defines the almost complex manifold on p. 342: Let $M$ be a differentiable manifold. The pair $(M,J)$, or simply $M$, is called an almost complex manifold if there exists a tensor field $J$ of type (1,1) such that at each point $p$ of $M$, $J_p^2 = - \text{id}_{T_p M}$. The tensor field $J$ is also called the almost complex structure . What confuses me is that this definition seems to be in contradiction to the first one. As far as I can understand it, $J$ in the second definition is defined globally in the sense that it is defined at every single point on the manifold $M$. Yet if this was the case, then by the first definition, it would not only be an almost complex manifold we were looking at, but would in fact be a full complex manifold. My question therefore is, when Nakahara on p. 318-319 writes that $J$ can only be defined globally on a complex manifold, in what sense does he mean globally? And what does he mean by ""be patched across charts""? In what sense cannot $J$ for an almost complex but still non-complex manifold not be ""patched across charts and defined globally""? Many thanks.","I am having problems making sense of Michio Nakahara's definition of the Almost Complex Structure/Almost Complex Manifold, such as it appears in Geometry, Topology and Physics (2nd Edition). On p. 318-319, he writes The tensor field $J$ is called the almost complex structure of a complex manifold $M$. Note that any $2m$-dimensional manifold locally admits a tensor field $J$ which squares to $-I_{2m}$. However, $J$ may be patched across charts and defined globally only on a complex manifold. What confuses me is when he later on defines the almost complex manifold on p. 342: Let $M$ be a differentiable manifold. The pair $(M,J)$, or simply $M$, is called an almost complex manifold if there exists a tensor field $J$ of type (1,1) such that at each point $p$ of $M$, $J_p^2 = - \text{id}_{T_p M}$. The tensor field $J$ is also called the almost complex structure . What confuses me is that this definition seems to be in contradiction to the first one. As far as I can understand it, $J$ in the second definition is defined globally in the sense that it is defined at every single point on the manifold $M$. Yet if this was the case, then by the first definition, it would not only be an almost complex manifold we were looking at, but would in fact be a full complex manifold. My question therefore is, when Nakahara on p. 318-319 writes that $J$ can only be defined globally on a complex manifold, in what sense does he mean globally? And what does he mean by ""be patched across charts""? In what sense cannot $J$ for an almost complex but still non-complex manifold not be ""patched across charts and defined globally""? Many thanks.",,"['differential-geometry', 'complex-manifolds', 'almost-complex']"
57,Was a nascent inverse function theorem known to Newton?,Was a nascent inverse function theorem known to Newton?,,"More specifically, was Newton aware that given an inverse pair of functions $f$ and $h$ such that $$f(h(x)) = x = h(f(x))$$ about the origin that, for $$(x,y)=(h(y),f(x)),$$ the derivatives satisfy $$f^{'}(x) = 1/h^{'}(y)$$ or $$dy/dx = 1/(dx/dy)$$ near the origin? Heuristically, this follows symbolically from $$dy = f^{'}(x)dx = f^{'}(x)h^{'}(y)dy, $$ or, equivalently, from the chain rule applied to the top equation. And it follows geometrically for a function whose graph lies in the first quadrant by reflection through the bisector of the first quadrant, the line $y=x$ . Clearly, the slope for any tangent line is inverted by the reflection just as displacements along the $x-$ axis and the $y-$ axis are interchanged. In fact, it follows directly from the tangent line perspective since $$ y = m \; x + b$$ and $$y = \frac{1}{m}(x-b)$$ describe an inverse pair. Surely, with Newton's mastery of geometric calculus, he was aware of these relationships. Is there evidence of this in Newton's work? Related MO-Q by Ziegler. Cross-posted from this MO-Q . Edit 6/12/17: An example of a calculation incorporating the IFT that would have been obvious to Newton and plausible for him to have performed if only as a simple check of his general formulas: It was known well before Newton that $$\frac{d\tan(x)}{dx} = 1+ \tan^2(x),$$ or, with $y = \tan(x)$ , $$\frac{dy}{dx} = 1+ y^2.$$ In terms of fluxions and fluents, this could be put in the form of Newton's implicit function $$g(x,y,\dot{x},\dot{y})=\dot{y}-(1+y^2)\dot{x}=0.$$ Then $$\frac{\dot{x}}{\dot{y}}= \frac{1}{1+y^2}=\frac{dx}{dy}, $$ and application of the binomial theorem and integration would give the series $$ \arctan(y) = x = y - \frac{y^3}3+\frac{y^5}5-\frac{y^7}7+\dots. \tag3 $$ Newton could then have derived a series expression for $\tan(x)$ using his series reversion formula (see Ferraro ) for finding the series for the compositional inverse of a function from its power series. In fact, the same procedure is applied to finding a series for $\sin(x)$ in Ferraro on pages 76-78 following an alleged reconstruction by Horsley of Newton's derivation of the series. Edit (Apr 10, 2018): According to the Wikipedia article on the chain rule, both Newton and Leibniz were aware of the chain rule, and the inverse function theorem in its simplest form follows from application of the chain rule to $x = f(f^{-1}(x))$ . This would provide an easy check for the veracity of the chain rule that someone as fastidious as Newton would have used.","More specifically, was Newton aware that given an inverse pair of functions and such that about the origin that, for the derivatives satisfy or near the origin? Heuristically, this follows symbolically from or, equivalently, from the chain rule applied to the top equation. And it follows geometrically for a function whose graph lies in the first quadrant by reflection through the bisector of the first quadrant, the line . Clearly, the slope for any tangent line is inverted by the reflection just as displacements along the axis and the axis are interchanged. In fact, it follows directly from the tangent line perspective since and describe an inverse pair. Surely, with Newton's mastery of geometric calculus, he was aware of these relationships. Is there evidence of this in Newton's work? Related MO-Q by Ziegler. Cross-posted from this MO-Q . Edit 6/12/17: An example of a calculation incorporating the IFT that would have been obvious to Newton and plausible for him to have performed if only as a simple check of his general formulas: It was known well before Newton that or, with , In terms of fluxions and fluents, this could be put in the form of Newton's implicit function Then and application of the binomial theorem and integration would give the series Newton could then have derived a series expression for using his series reversion formula (see Ferraro ) for finding the series for the compositional inverse of a function from its power series. In fact, the same procedure is applied to finding a series for in Ferraro on pages 76-78 following an alleged reconstruction by Horsley of Newton's derivation of the series. Edit (Apr 10, 2018): According to the Wikipedia article on the chain rule, both Newton and Leibniz were aware of the chain rule, and the inverse function theorem in its simplest form follows from application of the chain rule to . This would provide an easy check for the veracity of the chain rule that someone as fastidious as Newton would have used.","f h f(h(x)) = x = h(f(x)) (x,y)=(h(y),f(x)), f^{'}(x) = 1/h^{'}(y) dy/dx = 1/(dx/dy) dy = f^{'}(x)dx = f^{'}(x)h^{'}(y)dy,  y=x x- y-  y = m \; x + b y = \frac{1}{m}(x-b) \frac{d\tan(x)}{dx} = 1+ \tan^2(x), y = \tan(x) \frac{dy}{dx} = 1+ y^2. g(x,y,\dot{x},\dot{y})=\dot{y}-(1+y^2)\dot{x}=0. \frac{\dot{x}}{\dot{y}}= \frac{1}{1+y^2}=\frac{dx}{dy},  
\arctan(y) = x = y - \frac{y^3}3+\frac{y^5}5-\frac{y^7}7+\dots.
\tag3
 \tan(x) \sin(x) x = f(f^{-1}(x))","['calculus', 'real-analysis', 'differential-geometry', 'reference-request', 'math-history']"
58,Two properties of surface integrals.,Two properties of surface integrals.,,"Let $\Omega$ be a bounded open set in $\mathbb{R}^n$ with $C^1$ boundary. I know the following (possible) definition for $$\int_{\partial\Omega}u\,d\sigma.$$ For each $x_0\in\partial\Omega$, there exist an open subset $U$ in $\mathbb{R}^n$ containing $x_0$, an open subset $A$ in $\mathbb{R}^{n-1}$ and a $C^1$ function $g:A\rightarrow\mathbb{R}$ such that its graph is $G_g=\partial\Omega\cap U$. By compactness of $\partial\Omega$, we can write $\partial\Omega=\cup_{j=1}^N U_j$, where each $U_j$ is open in $\mathbb{R}^n$, and $\partial\Omega\cap U_j=G_{g_j}$, where $g_j:A_j\rightarrow\mathbb{R}$ is $C^1$. Take a partition of unity $\{\eta_j\}_{j=1}^N$ corresponding to $\{U_j\}_{j=1}^N$: $\eta_j\in C_c^{\infty}(U_j)$, $0\leq\eta_j\leq 1$ and $\sum_{j=1}^N\eta_j=1$ on $\partial \Omega$. Let $u:\partial\Omega\rightarrow\mathbb{R}$ measurable. Define $u_j=u\,\eta_j$, which has support in $U_j$. Then: $$\int_{\partial\Omega}u\,d\sigma=\sum_{j=1}^N\int_{A_j}u_j(x',g_j(x'))\sqrt{1+|\nabla g_j(x')|^2}\,dx'$$(whenever the right-hand side exists). My two doubts are the following: Given a subset $\Gamma$ of $\partial\Omega$, in what sense it is understood $\int_\Gamma u\,d\sigma$? Is it true that $$\int_{\partial\Omega}u\,1_\Gamma\,d\sigma=\int_\Gamma u\,d\sigma\,?$$ ($1_\Gamma$ is the characteristic function on $\Gamma$). If $u_m\rightarrow u$ in $L^p(\partial\Omega)$, is it true that there exists a subsequence $u_{m_k}$ that converges pointwise almost everywhere on $\partial\Omega$?","Let $\Omega$ be a bounded open set in $\mathbb{R}^n$ with $C^1$ boundary. I know the following (possible) definition for $$\int_{\partial\Omega}u\,d\sigma.$$ For each $x_0\in\partial\Omega$, there exist an open subset $U$ in $\mathbb{R}^n$ containing $x_0$, an open subset $A$ in $\mathbb{R}^{n-1}$ and a $C^1$ function $g:A\rightarrow\mathbb{R}$ such that its graph is $G_g=\partial\Omega\cap U$. By compactness of $\partial\Omega$, we can write $\partial\Omega=\cup_{j=1}^N U_j$, where each $U_j$ is open in $\mathbb{R}^n$, and $\partial\Omega\cap U_j=G_{g_j}$, where $g_j:A_j\rightarrow\mathbb{R}$ is $C^1$. Take a partition of unity $\{\eta_j\}_{j=1}^N$ corresponding to $\{U_j\}_{j=1}^N$: $\eta_j\in C_c^{\infty}(U_j)$, $0\leq\eta_j\leq 1$ and $\sum_{j=1}^N\eta_j=1$ on $\partial \Omega$. Let $u:\partial\Omega\rightarrow\mathbb{R}$ measurable. Define $u_j=u\,\eta_j$, which has support in $U_j$. Then: $$\int_{\partial\Omega}u\,d\sigma=\sum_{j=1}^N\int_{A_j}u_j(x',g_j(x'))\sqrt{1+|\nabla g_j(x')|^2}\,dx'$$(whenever the right-hand side exists). My two doubts are the following: Given a subset $\Gamma$ of $\partial\Omega$, in what sense it is understood $\int_\Gamma u\,d\sigma$? Is it true that $$\int_{\partial\Omega}u\,1_\Gamma\,d\sigma=\int_\Gamma u\,d\sigma\,?$$ ($1_\Gamma$ is the characteristic function on $\Gamma$). If $u_m\rightarrow u$ in $L^p(\partial\Omega)$, is it true that there exists a subsequence $u_{m_k}$ that converges pointwise almost everywhere on $\partial\Omega$?",,"['differential-geometry', 'lebesgue-integral', 'vector-analysis', 'surface-integrals']"
59,Duality of Hodge star.,Duality of Hodge star.,,"Can someone please provide a rigorous proof that: $$\star(\star A) = (-1)^{p(n-p)}sA$$ where $\star : \Lambda^{n}\to \Lambda^{n-p }$ and $s$ is the the sign of the determinant of the metric. I am using the following definition of Hodge operator: $$\star A_{\mu_1,\ldots , \mu_{n-p}} = \frac{1}{p!} \epsilon^{\nu_1\ldots \nu_{p}}_{\mu_1,\ldots,\mu_{n-p}}A_{\nu_1,\ldots , \nu_p}$$ So my brute force approach is the following: $$(\star(\star A))_{\mu_1,\ldots , \mu_p} = \ldots = \frac{1}{(n-p)!p!}\epsilon^{\nu_1,\ldots , \nu_{n-p}}_{\mu_1,\ldots , \mu_p}\epsilon^{\alpha_1\ldots \alpha_p}_{\nu_1\ldots \nu_{n-p}}A_{\alpha_1\ldots \alpha_p}$$ How to proceed? I need to use here the definition of the determinant, but I am not sure how? Thanks in advance. Edit: $\epsilon$ is the Levi-Civita completely symmetric tensor. (i.e the Levi-Civita SYmbol multiplied by $\sqrt{|\det g|}$.","Can someone please provide a rigorous proof that: $$\star(\star A) = (-1)^{p(n-p)}sA$$ where $\star : \Lambda^{n}\to \Lambda^{n-p }$ and $s$ is the the sign of the determinant of the metric. I am using the following definition of Hodge operator: $$\star A_{\mu_1,\ldots , \mu_{n-p}} = \frac{1}{p!} \epsilon^{\nu_1\ldots \nu_{p}}_{\mu_1,\ldots,\mu_{n-p}}A_{\nu_1,\ldots , \nu_p}$$ So my brute force approach is the following: $$(\star(\star A))_{\mu_1,\ldots , \mu_p} = \ldots = \frac{1}{(n-p)!p!}\epsilon^{\nu_1,\ldots , \nu_{n-p}}_{\mu_1,\ldots , \mu_p}\epsilon^{\alpha_1\ldots \alpha_p}_{\nu_1\ldots \nu_{n-p}}A_{\alpha_1\ldots \alpha_p}$$ How to proceed? I need to use here the definition of the determinant, but I am not sure how? Thanks in advance. Edit: $\epsilon$ is the Levi-Civita completely symmetric tensor. (i.e the Levi-Civita SYmbol multiplied by $\sqrt{|\det g|}$.",,['differential-geometry']
60,differ between $M$ and $G/G_{x_0}$,differ between  and,M G/G_{x_0},I have a Lie group $G$ acting transitively on $M$. Why is $M$ diffeomorphic to $G/G_{x_0}$? Here is the function. It remains to see that it is differential both ways. $$ \phi:G/G_{x_0}\rightarrow M : g G_{x_0}\rightarrow g\cdot x_0$$ Will inverse function theorem do the trick?,I have a Lie group $G$ acting transitively on $M$. Why is $M$ diffeomorphic to $G/G_{x_0}$? Here is the function. It remains to see that it is differential both ways. $$ \phi:G/G_{x_0}\rightarrow M : g G_{x_0}\rightarrow g\cdot x_0$$ Will inverse function theorem do the trick?,,"['abstract-algebra', 'differential-geometry', 'lie-groups', 'lie-algebras']"
61,"Heat kernel, connection on fibre bundles: reference request","Heat kernel, connection on fibre bundles: reference request",,"At the moment I'm trying to read Getzler/Berline ""Heat kernels and Dirac operators"", but I'm having a hard time to follow the rather brief outline of connections (of fibre bundles, principal bundles) etc. in the first chapter. Can anyone recommend me a more detailed (modern) reference with careful explanation. (not Nomizu/Kobayashi). I'm comfortable with connections, curvature etc in the context of Riemannian geometry, also with the topological theory of fibre bundles. If you have a reference in mind, which does these things for general vector bundles, I'd be glad as well","At the moment I'm trying to read Getzler/Berline ""Heat kernels and Dirac operators"", but I'm having a hard time to follow the rather brief outline of connections (of fibre bundles, principal bundles) etc. in the first chapter. Can anyone recommend me a more detailed (modern) reference with careful explanation. (not Nomizu/Kobayashi). I'm comfortable with connections, curvature etc in the context of Riemannian geometry, also with the topological theory of fibre bundles. If you have a reference in mind, which does these things for general vector bundles, I'd be glad as well",,"['differential-geometry', 'reference-request', 'fiber-bundles', 'connections']"
62,Tangent to a fiber bundle,Tangent to a fiber bundle,,"I am trying to prove that the kernel of a push-forward is the fiber. Let $π : E → M $ be a fiber be bundle with a fiber $F$ . What is the meaning of a tangent space to a bundle? Does it means that if we have a vector, $X$ tangent to curve $\lambda$, that curve must pass to all points of the fiber or in just one point of the bundle?","I am trying to prove that the kernel of a push-forward is the fiber. Let $π : E → M $ be a fiber be bundle with a fiber $F$ . What is the meaning of a tangent space to a bundle? Does it means that if we have a vector, $X$ tangent to curve $\lambda$, that curve must pass to all points of the fiber or in just one point of the bundle?",,['differential-geometry']
63,Intuition for shape operator in local coordinates in terms of first and second fundamental forms,Intuition for shape operator in local coordinates in terms of first and second fundamental forms,,"In section 3-3 of do Carmo's Differential Geometry of Curves and Surfaces the author proves the following (formula (3) on page 155). Let $\bf x$ be a parametrization of a surface embedded in $\mathbb R ^3$. Let $N=\frac{\mathbf x_u \wedge \mathbf x_v}{\|\mathbf x_u \wedge \mathbf x_v\|}$ and let $A$ be the matrix representing the shape operator endomorphism $\mathrm d_pN$ w.r.t the basis provided by $\bf x$. Furthermore, let $\mathrm I,\mathrm I\!\mathrm I$ respectively denote the matrices representing the first and second fundamental forms w.r.t the same basis. Then $-\mathrm I\!\mathrm I=\mathrm I A$. The proof seems to just be about rearrangement, but I feel I'm missing geometric content. Is there a conceptual explanation for this formula? Is there a conceptual proof for a general embedded oriented hypersurface in Euclidean space? Added. Just to be clear, the rigorous conceptual definition of the Gauss map of an embedded oriented hypersurface $\iota:H\rightarrowtail V$ in an inner product space $V$ that I have in mind is the composite  $$H\overset{\mathbf n}{\longrightarrow}(\mathrm TH)^\perp \longrightarrow\iota^\ast (\mathrm TV)\overset{\iota ^\ast \jmath}{\longrightarrow} H\times V\overset{\pi _2}{\longrightarrow}V$$ where $\bf n$ is the normal vector field and $\jmath$ is the canonical trivialization of the tangent bundle of $V$ itself.","In section 3-3 of do Carmo's Differential Geometry of Curves and Surfaces the author proves the following (formula (3) on page 155). Let $\bf x$ be a parametrization of a surface embedded in $\mathbb R ^3$. Let $N=\frac{\mathbf x_u \wedge \mathbf x_v}{\|\mathbf x_u \wedge \mathbf x_v\|}$ and let $A$ be the matrix representing the shape operator endomorphism $\mathrm d_pN$ w.r.t the basis provided by $\bf x$. Furthermore, let $\mathrm I,\mathrm I\!\mathrm I$ respectively denote the matrices representing the first and second fundamental forms w.r.t the same basis. Then $-\mathrm I\!\mathrm I=\mathrm I A$. The proof seems to just be about rearrangement, but I feel I'm missing geometric content. Is there a conceptual explanation for this formula? Is there a conceptual proof for a general embedded oriented hypersurface in Euclidean space? Added. Just to be clear, the rigorous conceptual definition of the Gauss map of an embedded oriented hypersurface $\iota:H\rightarrowtail V$ in an inner product space $V$ that I have in mind is the composite  $$H\overset{\mathbf n}{\longrightarrow}(\mathrm TH)^\perp \longrightarrow\iota^\ast (\mathrm TV)\overset{\iota ^\ast \jmath}{\longrightarrow} H\times V\overset{\pi _2}{\longrightarrow}V$$ where $\bf n$ is the normal vector field and $\jmath$ is the canonical trivialization of the tangent bundle of $V$ itself.",,"['differential-geometry', 'intuition', 'smooth-manifolds', 'surfaces']"
64,How to derive the line element from the Fubini-Study metric?,How to derive the line element from the Fubini-Study metric?,,"On the Wikipedia page on the Fubini-Study metric , the Fubini-Study distance is given (for a Hilbert space) as $$\gamma(\psi,\phi)=\arccos\sqrt{\frac{\langle \psi|\phi\rangle\langle\phi|\psi\rangle}{\langle\psi|\psi\rangle\langle\phi|\phi\rangle}},$$ and the page says that the infitesimal form can be obtained by setting $\phi=\psi+\delta\psi$ , to get $$ds^2=\frac{\langle\delta\psi|\delta\psi\rangle}{\langle\psi|\psi\rangle}-\frac{\langle\delta\psi|\psi\rangle\langle\psi|\delta\psi\rangle}{\langle\psi|\psi\rangle^2}.$$ How does the derivation work? I tried to use a Taylor series approximation of arccos, which cancelled the square root, but I don't know where the rest came from.","On the Wikipedia page on the Fubini-Study metric , the Fubini-Study distance is given (for a Hilbert space) as and the page says that the infitesimal form can be obtained by setting , to get How does the derivation work? I tried to use a Taylor series approximation of arccos, which cancelled the square root, but I don't know where the rest came from.","\gamma(\psi,\phi)=\arccos\sqrt{\frac{\langle \psi|\phi\rangle\langle\phi|\psi\rangle}{\langle\psi|\psi\rangle\langle\phi|\phi\rangle}}, \phi=\psi+\delta\psi ds^2=\frac{\langle\delta\psi|\delta\psi\rangle}{\langle\psi|\psi\rangle}-\frac{\langle\delta\psi|\psi\rangle\langle\psi|\delta\psi\rangle}{\langle\psi|\psi\rangle^2}.","['differential-geometry', 'metric-spaces', 'hilbert-spaces', 'riemannian-geometry', 'projective-geometry']"
65,"For a vector bundle $\xi:E\to M$, why is Trace a parallel section of the bundle $End(\xi)^*$?","For a vector bundle , why is Trace a parallel section of the bundle ?",\xi:E\to M End(\xi)^*,"For any $L\in$ $End(\xi)$, $Tr(L)$ has a well defined value independent of the choice of basis, so we have a globally defined function $Tr:End(\xi)\to\mathbb{R}$. This function is tensorial, and thus represents a section of $End(\xi)^*$. My book (Walschap) claims that this section is parallel, but I don't understand his argument. Suppose we have a basis $\beta$ of parallel sections of $\xi$ and a parallel section $X$ of $End(\xi)$, all taken along a curve $c$ in $M$. Then according to my book $\beta^{-1}(t)\circ X(t)\circ \beta(t)$ is constant in the space of $n\times n$ matrices. Why? First of all I assume $\beta(t)$ here is supposed to be a matrix whose columns are the same as the chosen basis, in a particular coordinate chart. Then I see that $X(t)\circ\beta(t)$ is parallel. I'm not sure where to go from here.","For any $L\in$ $End(\xi)$, $Tr(L)$ has a well defined value independent of the choice of basis, so we have a globally defined function $Tr:End(\xi)\to\mathbb{R}$. This function is tensorial, and thus represents a section of $End(\xi)^*$. My book (Walschap) claims that this section is parallel, but I don't understand his argument. Suppose we have a basis $\beta$ of parallel sections of $\xi$ and a parallel section $X$ of $End(\xi)$, all taken along a curve $c$ in $M$. Then according to my book $\beta^{-1}(t)\circ X(t)\circ \beta(t)$ is constant in the space of $n\times n$ matrices. Why? First of all I assume $\beta(t)$ here is supposed to be a matrix whose columns are the same as the chosen basis, in a particular coordinate chart. Then I see that $X(t)\circ\beta(t)$ is parallel. I'm not sure where to go from here.",,"['differential-geometry', 'smooth-manifolds', 'vector-bundles', 'connections']"
66,Computing (and evaluating) the pullback of a differential form explicitly,Computing (and evaluating) the pullback of a differential form explicitly,,"It seems that this is a very simple exercise, but now I'm confused because I've rarely seen detailed examples of this particular kind. Let $\omega$ be the $2$-form in $\mathbb{R}^2$ given by $\omega=\omega(x, y)=dx\wedge dy$, and consider $\Phi=(\Phi_1, \Phi_2)  \colon (0, \infty) \times (0, 2\pi)\to \mathbb R^2,\qquad (r, \theta)\mapsto (r\cos \theta, r\sin \theta) $ . I'm asked to find $ \Phi^*\omega $. Moreover, if $p=\left(1, \frac\pi2\right), v=(1,0)$ and $w=(3,1)$, I must compute $(\Phi^*\omega )_p(v, w)$. At first, it is straightforward to find out that $\Phi^*\omega =r\,dr\wedge d\theta$, I don't have any problem at this point. (This is what everyone learns to do effortlesstly) On the other side, one learns that $(\Phi^*\omega )_p(v, w):=\omega_{\Phi(p)}\big(d\Phi_p(v), d\Phi_p(w)\big)$. Also,  $$d\Phi_p(v)=\begin{bmatrix} \frac{\partial \Phi_1}{\partial r}(p) & \frac{\partial \Phi_1}{\partial \theta}(p)  \\ \frac{\partial \Phi_2}{\partial r}(p) &  \frac{\partial \Phi_2}{\partial \theta}(p)\end{bmatrix} \begin{bmatrix}1 \\ 0 \end{bmatrix} =\cdots = \begin{bmatrix}0 & -1 \\ 1& 0 \end{bmatrix} \begin{bmatrix}1 \\ 0 \end{bmatrix}= \begin{bmatrix}0 \\ 1 \end{bmatrix}.$$ Similarly, $d\Phi_p(w)= \begin{bmatrix}-1 \\3\end{bmatrix}$. So now, $(\Phi^*\omega )_p(v, w)= \omega_{(0, 1)}\left( \begin{bmatrix}0 \\ 1 \end{bmatrix}, \begin{bmatrix}-1 \\3\end{bmatrix} \right)$. What's next, who is $\omega_{(0, 1)}$? It should be $dx_{(0, 1)}\wedge dy_{(0, 1)} $, but again, that doesn't tells much to me (and how to obtain $\omega_{(0, 1)}\left( \begin{bmatrix}0 \\ 1 \end{bmatrix}, \begin{bmatrix}-1 \\3\end{bmatrix} \right)$ afterwards?) [there must appear a determinant thingy, but I'm not sure if mine is right] I'm a bit rust with this things so I started (re-)doing some exercises, but I bet almost no text ever does such an explicit calculation, so this one was a bit confusing for me. Thanks in advance!","It seems that this is a very simple exercise, but now I'm confused because I've rarely seen detailed examples of this particular kind. Let $\omega$ be the $2$-form in $\mathbb{R}^2$ given by $\omega=\omega(x, y)=dx\wedge dy$, and consider $\Phi=(\Phi_1, \Phi_2)  \colon (0, \infty) \times (0, 2\pi)\to \mathbb R^2,\qquad (r, \theta)\mapsto (r\cos \theta, r\sin \theta) $ . I'm asked to find $ \Phi^*\omega $. Moreover, if $p=\left(1, \frac\pi2\right), v=(1,0)$ and $w=(3,1)$, I must compute $(\Phi^*\omega )_p(v, w)$. At first, it is straightforward to find out that $\Phi^*\omega =r\,dr\wedge d\theta$, I don't have any problem at this point. (This is what everyone learns to do effortlesstly) On the other side, one learns that $(\Phi^*\omega )_p(v, w):=\omega_{\Phi(p)}\big(d\Phi_p(v), d\Phi_p(w)\big)$. Also,  $$d\Phi_p(v)=\begin{bmatrix} \frac{\partial \Phi_1}{\partial r}(p) & \frac{\partial \Phi_1}{\partial \theta}(p)  \\ \frac{\partial \Phi_2}{\partial r}(p) &  \frac{\partial \Phi_2}{\partial \theta}(p)\end{bmatrix} \begin{bmatrix}1 \\ 0 \end{bmatrix} =\cdots = \begin{bmatrix}0 & -1 \\ 1& 0 \end{bmatrix} \begin{bmatrix}1 \\ 0 \end{bmatrix}= \begin{bmatrix}0 \\ 1 \end{bmatrix}.$$ Similarly, $d\Phi_p(w)= \begin{bmatrix}-1 \\3\end{bmatrix}$. So now, $(\Phi^*\omega )_p(v, w)= \omega_{(0, 1)}\left( \begin{bmatrix}0 \\ 1 \end{bmatrix}, \begin{bmatrix}-1 \\3\end{bmatrix} \right)$. What's next, who is $\omega_{(0, 1)}$? It should be $dx_{(0, 1)}\wedge dy_{(0, 1)} $, but again, that doesn't tells much to me (and how to obtain $\omega_{(0, 1)}\left( \begin{bmatrix}0 \\ 1 \end{bmatrix}, \begin{bmatrix}-1 \\3\end{bmatrix} \right)$ afterwards?) [there must appear a determinant thingy, but I'm not sure if mine is right] I'm a bit rust with this things so I started (re-)doing some exercises, but I bet almost no text ever does such an explicit calculation, so this one was a bit confusing for me. Thanks in advance!",,"['differential-geometry', 'differential-forms']"
67,"Is always possible say that $\langle df(\frac{\partial\varphi}{\partial u})\wedge df(\frac{\partial\varphi}{\partial v}) ,N_2\rangle>0$",Is always possible say that,"\langle df(\frac{\partial\varphi}{\partial u})\wedge df(\frac{\partial\varphi}{\partial v}) ,N_2\rangle>0","Let $S_2$ be an orientable regular surface and $f:S_1\to S_2$ be a differentiable map which is a local diffeomorphism at every $p\in S_1$. Prove that $S_1$ is orientable. My attemp: Suppose that $S_2$ is oriented by $N_2$ (since all orientable surface has a vector field of unit vectors N, we can assume that $S_2$ is oriented by $N_2$), and let $\{\varphi_{\lambda}:U_\lambda\to S_1\}_{\lambda\in\Gamma}$ a family of coordinate neighborhood of $S_1$. Since $f$ is a local diffeomorphism, then $df_p$ is injective, but in particular $T_{f(p)}(S_2)=df_p(T_p S_1)$, and if is necessary we interchange $u$ and $v$, we can assume $$\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0\qquad (*)$$ Now, such $\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ is an orientation of $S_1$. Suppose that $\varphi_{\alpha},\varphi_{\beta}\in\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ and such that $\varphi_{\alpha}(r,s)\cap\varphi_{\alpha}(u,v)\neq0$. Now, for the change of cordinates we have $$(\varphi^{-1}_{\alpha}\circ\varphi_{\beta})(r,s)=(u(r,s),v(r,s))\implies \varphi_{\beta}(r,s)=\varphi_{\alpha}((u(r,s),v(r,s)))$$ Then, $$\dfrac{\partial\varphi_{\beta}}{\partial r}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial r}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial r}$$ $$\dfrac{\partial\varphi_{\beta}}{\partial s}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial s}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial s}$$ So, $df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right)=\dfrac{\partial(u,v)}{\partial(r,s)}df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right)$. Therefore, $$0<\left\langle df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right) ,N_2\right\rangle=\dfrac{\partial(u,v)}{\partial(r,s)}\left\langle df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right),N_2\right\rangle$$ Which implies that $\dfrac{\partial(u,v)}{\partial(r,s)}>0$ and a regular surface is say ""orientable"" if is possible cover it with a family of coordinates neighborhood such that if a point belongs two differents coordinates family then, the change of coordinates has positive jacobian in such point. The solution of this problem looks fine to me, but I have the little problem that is, if is always possible say that $\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0$. My Attemp: If I have a basis for $T_{p}(S_1)$, $\{\varphi_u,\varphi_v\}$, and $df_p:T_p(S_1)\to T_{f(p)}(S_2)$ is a injective linear map, then $\{df_p(\varphi_u),df_p(\varphi_v)\}$ is linearly independent in $T_{f(p)}(S_2)$, i.e., a basis for $T_{f(p)}(S_2)$. Now, we can define a positive basis if $$\left\langle df_p(\varphi_u)\wedge df_p(\varphi_v), N_2 \right\rangle>0$$ Because a orientation $N_2$ on $S_2$ induces an orientation on each tangent space $T_{f(p)}(S_2)$. This is right?. Thanks!","Let $S_2$ be an orientable regular surface and $f:S_1\to S_2$ be a differentiable map which is a local diffeomorphism at every $p\in S_1$. Prove that $S_1$ is orientable. My attemp: Suppose that $S_2$ is oriented by $N_2$ (since all orientable surface has a vector field of unit vectors N, we can assume that $S_2$ is oriented by $N_2$), and let $\{\varphi_{\lambda}:U_\lambda\to S_1\}_{\lambda\in\Gamma}$ a family of coordinate neighborhood of $S_1$. Since $f$ is a local diffeomorphism, then $df_p$ is injective, but in particular $T_{f(p)}(S_2)=df_p(T_p S_1)$, and if is necessary we interchange $u$ and $v$, we can assume $$\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0\qquad (*)$$ Now, such $\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ is an orientation of $S_1$. Suppose that $\varphi_{\alpha},\varphi_{\beta}\in\{\varphi_{\lambda}\}_{\lambda\in\Gamma}$ and such that $\varphi_{\alpha}(r,s)\cap\varphi_{\alpha}(u,v)\neq0$. Now, for the change of cordinates we have $$(\varphi^{-1}_{\alpha}\circ\varphi_{\beta})(r,s)=(u(r,s),v(r,s))\implies \varphi_{\beta}(r,s)=\varphi_{\alpha}((u(r,s),v(r,s)))$$ Then, $$\dfrac{\partial\varphi_{\beta}}{\partial r}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial r}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial r}$$ $$\dfrac{\partial\varphi_{\beta}}{\partial s}=\dfrac{\partial\varphi_{\alpha}}{\partial u}\dfrac{\partial u}{\partial s}+\dfrac{\partial\varphi_{\alpha}}{\partial v}\dfrac{\partial v}{\partial s}$$ So, $df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right)=\dfrac{\partial(u,v)}{\partial(r,s)}df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right)$. Therefore, $$0<\left\langle df\left(\dfrac{\partial\varphi_{\beta}}{\partial r}\right)\wedge df\left(\dfrac{\partial\varphi_{\beta}}{\partial s}\right) ,N_2\right\rangle=\dfrac{\partial(u,v)}{\partial(r,s)}\left\langle df\left(\dfrac{\partial\varphi_{\alpha}}{\partial u}\right)\wedge df\left(\dfrac{\partial\varphi_{\alpha}}{\partial v}\right),N_2\right\rangle$$ Which implies that $\dfrac{\partial(u,v)}{\partial(r,s)}>0$ and a regular surface is say ""orientable"" if is possible cover it with a family of coordinates neighborhood such that if a point belongs two differents coordinates family then, the change of coordinates has positive jacobian in such point. The solution of this problem looks fine to me, but I have the little problem that is, if is always possible say that $\left\langle df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial u}\right)\wedge df_p\left(\dfrac{\partial\varphi_{\lambda}}{\partial v}\right) ,N_2\right\rangle_{f(p)}>0$. My Attemp: If I have a basis for $T_{p}(S_1)$, $\{\varphi_u,\varphi_v\}$, and $df_p:T_p(S_1)\to T_{f(p)}(S_2)$ is a injective linear map, then $\{df_p(\varphi_u),df_p(\varphi_v)\}$ is linearly independent in $T_{f(p)}(S_2)$, i.e., a basis for $T_{f(p)}(S_2)$. Now, we can define a positive basis if $$\left\langle df_p(\varphi_u)\wedge df_p(\varphi_v), N_2 \right\rangle>0$$ Because a orientation $N_2$ on $S_2$ induces an orientation on each tangent space $T_{f(p)}(S_2)$. This is right?. Thanks!",,"['differential-geometry', 'proof-verification']"
68,Lie Bracket of Lie Algebra Associated with Given Lie Group,Lie Bracket of Lie Algebra Associated with Given Lie Group,,"Let $G$ be a Lie group. Its Lie algebra, when considered as a vector space, is given by $T_e G$, the tangent space of $G$ at the identity element $e$. However, in order to fully describe the Lie algebra associated with $G$ we must also describe the commutator $\left[\cdot,\,\cdot\right]:T_eG^2\to T_eG$ and show that it obeys the three axions: bilinearity, alternativity and the Jacobi identity. Following Wikipedia, one defines first for any $g\in G$ left multiplication on $G$ by $g$ as $$ l_g:G\to G$$ given by $$ x\mapsto gx$$and a left-multiplication invariant vector field $X\in TG$ is one which obeys for every $\left(g,\,h\right)\in G^2$ $$ \left(d l_g\right)_h X_h = X_{gh} $$ where $\left(d l_g\right)_h:T_hG\to T_{gh}G$ is the differential of $l_g$ at $h$. We define $Lie\left(G\right)$ as the vector space of left-multiplication invariant vector fields. Apparently $G\times Lie\left(G\right)$ is isomorphic to $TG$ via $$\left(g,X\right)\mapsto\left(g,X_g\right)$$ so that we get a $\natural$ isomorphism $Lie\left(G\right)$ with $T_eG$ given by $$Lie\left(G\right)\ni X \mapsto X_e\in T_eG$$This last isomorphism we call $\phi$. Now apparently there is a Lie bracket structure on $TG$ which induces a Lie bracket structure on $Lie\left(G\right)$: for any $X$ and $Y$ in $TG$ we have $$ \left[X,\,Y\right]_{TG}= X\left(Y\left(\cdot\right)\right)-Y\left(X\left(\cdot\right)\right)$$ This in turn can be pushed forward to a define our desired Lie bracket on $T_e G \equiv \frak{g} $ via: If $X$ and $Y$ are any tangent vectors in $T_eG$ then $$\left[X,\, Y\right]_{T_eG}=\phi\left[\phi^{-1}\left(X\right),\,\phi^{-1}\left(Y\right)\right]_{TG}$$ My question is: is there now any easier way to describe $\left[\cdot,\,\cdot\right]:T_eG^2\to T_eG$? I tried for example to write it in coordinates and it was a mess. How do you ""easily"" see how $\left[X,\, Y\right]_{T_eG}$ acts on any scalar function $f$ given some $X$ and $Y$ in $T_eG$?","Let $G$ be a Lie group. Its Lie algebra, when considered as a vector space, is given by $T_e G$, the tangent space of $G$ at the identity element $e$. However, in order to fully describe the Lie algebra associated with $G$ we must also describe the commutator $\left[\cdot,\,\cdot\right]:T_eG^2\to T_eG$ and show that it obeys the three axions: bilinearity, alternativity and the Jacobi identity. Following Wikipedia, one defines first for any $g\in G$ left multiplication on $G$ by $g$ as $$ l_g:G\to G$$ given by $$ x\mapsto gx$$and a left-multiplication invariant vector field $X\in TG$ is one which obeys for every $\left(g,\,h\right)\in G^2$ $$ \left(d l_g\right)_h X_h = X_{gh} $$ where $\left(d l_g\right)_h:T_hG\to T_{gh}G$ is the differential of $l_g$ at $h$. We define $Lie\left(G\right)$ as the vector space of left-multiplication invariant vector fields. Apparently $G\times Lie\left(G\right)$ is isomorphic to $TG$ via $$\left(g,X\right)\mapsto\left(g,X_g\right)$$ so that we get a $\natural$ isomorphism $Lie\left(G\right)$ with $T_eG$ given by $$Lie\left(G\right)\ni X \mapsto X_e\in T_eG$$This last isomorphism we call $\phi$. Now apparently there is a Lie bracket structure on $TG$ which induces a Lie bracket structure on $Lie\left(G\right)$: for any $X$ and $Y$ in $TG$ we have $$ \left[X,\,Y\right]_{TG}= X\left(Y\left(\cdot\right)\right)-Y\left(X\left(\cdot\right)\right)$$ This in turn can be pushed forward to a define our desired Lie bracket on $T_e G \equiv \frak{g} $ via: If $X$ and $Y$ are any tangent vectors in $T_eG$ then $$\left[X,\, Y\right]_{T_eG}=\phi\left[\phi^{-1}\left(X\right),\,\phi^{-1}\left(Y\right)\right]_{TG}$$ My question is: is there now any easier way to describe $\left[\cdot,\,\cdot\right]:T_eG^2\to T_eG$? I tried for example to write it in coordinates and it was a mess. How do you ""easily"" see how $\left[X,\, Y\right]_{T_eG}$ acts on any scalar function $f$ given some $X$ and $Y$ in $T_eG$?",,"['differential-geometry', 'definition', 'lie-groups', 'lie-algebras', 'vector-fields']"
69,What is the name of the theorem that says that ${\frak{g}} \subseteq \{X \in M_n(R) \mid \forall t \in R : e^{tX} \in G\}$?,What is the name of the theorem that says that ?,{\frak{g}} \subseteq \{X \in M_n(R) \mid \forall t \in R : e^{tX} \in G\},"For a matrix Lie group $G \subseteq GL_n(\mathbb{R})$, we define the Lie algebra $\frak{g}$ as $T_0 G$. We then proceed to prove the inclusion in the title to give a definition of $\frak{g}$ in terms of the exponential map. I am very interested in learning if this theorem has a name.","For a matrix Lie group $G \subseteq GL_n(\mathbb{R})$, we define the Lie algebra $\frak{g}$ as $T_0 G$. We then proceed to prove the inclusion in the title to give a definition of $\frak{g}$ in terms of the exponential map. I am very interested in learning if this theorem has a name.",,"['differential-geometry', 'terminology']"
70,"Calculate the ""inverse"" of the Ricci tensor","Calculate the ""inverse"" of the Ricci tensor",,Let $R_{\mu\nu}$ be the coordinates of the Ricci tensor. Using that $R^{\mu\alpha}R_{\alpha\nu}=\delta_\nu^\mu$ and $R_{\mu\nu}=R_{\nu\mu}$ it is possible to get the expressión of $R^{\mu\nu}$? Many thanks!,Let $R_{\mu\nu}$ be the coordinates of the Ricci tensor. Using that $R^{\mu\alpha}R_{\alpha\nu}=\delta_\nu^\mu$ and $R_{\mu\nu}=R_{\nu\mu}$ it is possible to get the expressión of $R^{\mu\nu}$? Many thanks!,,"['differential-geometry', 'tensors']"
71,Curvature tensor of a conformally flat manifold,Curvature tensor of a conformally flat manifold,,"Let $M$ be a manifold of dimension $n>3$ and $g$ a Riemannian metric on $M$ which is conformally equivalent to a flat one. Are there formulas (different from $R(u,v)w=\nabla_u\nabla_vw-\nabla_v\nabla_uw-\nabla_{[u,v]}w$) that allow to compute easily the curvature tensor of a conformally flat manifold such as $(M,g)$?","Let $M$ be a manifold of dimension $n>3$ and $g$ a Riemannian metric on $M$ which is conformally equivalent to a flat one. Are there formulas (different from $R(u,v)w=\nabla_u\nabla_vw-\nabla_v\nabla_uw-\nabla_{[u,v]}w$) that allow to compute easily the curvature tensor of a conformally flat manifold such as $(M,g)$?",,['differential-geometry']
72,Geodesics and covering maps,Geodesics and covering maps,,"I know of the following proposition: Let $p: (N, h) \rightarrow (M,g)$ be a Riemannian covering map. The geodesics of $(M,g)$ are the projections of the geodesics of $(N,h)$, and the geodesics of $(N,h)$ are the liftings of those of $(M,g)$. If I would be asked to prove this, I would use the local length minimizing property of geodesics and the fact that the covering map doesn't change the length of a smooth curve since it is a local isometry. However, in the book ""Riemannian Geometry"" by Gallot, Hulin and Lafontaine, this fact is proven way before the local length minimizing property is discussed (Proposition 2.81). See also this question for a proof: geodesics, covering map and its lift I'm not sure I fully understand (the final step of) these proofs. Would the following argument be correct? : If one considers local coordinates, a geodesic is a smooth curve that fulfills a certain differential equation which essentially only depends on the Christoffel symbols, which in turn only depend on the Riemannian metric. Since the Riemannian metric behaves nicely under a Riemannian covering map (i.e. $h = p^*(g)$) the above statement follows. EDIT: Just to clarify, I understand how the proposition can be proven using the locally length minimizing property of geodesics. I want to understand how it can be proven without using this property of geodesics.","I know of the following proposition: Let $p: (N, h) \rightarrow (M,g)$ be a Riemannian covering map. The geodesics of $(M,g)$ are the projections of the geodesics of $(N,h)$, and the geodesics of $(N,h)$ are the liftings of those of $(M,g)$. If I would be asked to prove this, I would use the local length minimizing property of geodesics and the fact that the covering map doesn't change the length of a smooth curve since it is a local isometry. However, in the book ""Riemannian Geometry"" by Gallot, Hulin and Lafontaine, this fact is proven way before the local length minimizing property is discussed (Proposition 2.81). See also this question for a proof: geodesics, covering map and its lift I'm not sure I fully understand (the final step of) these proofs. Would the following argument be correct? : If one considers local coordinates, a geodesic is a smooth curve that fulfills a certain differential equation which essentially only depends on the Christoffel symbols, which in turn only depend on the Riemannian metric. Since the Riemannian metric behaves nicely under a Riemannian covering map (i.e. $h = p^*(g)$) the above statement follows. EDIT: Just to clarify, I understand how the proposition can be proven using the locally length minimizing property of geodesics. I want to understand how it can be proven without using this property of geodesics.",,"['differential-geometry', 'riemannian-geometry', 'covering-spaces', 'geodesic']"
73,How does the Cartan-Hadamard theorem demonstrate the impact of non-positive sectional curvature on the topology of a manifold?,How does the Cartan-Hadamard theorem demonstrate the impact of non-positive sectional curvature on the topology of a manifold?,,"My note says that Cartan-Hadamard theorem is an example of how negative sectional curvature ""impacts"" the topology of manifolds. The version of the theorem we are using is: Let $(M,g)$ be a complete Riemannian manifold with everywhere non-positive sectional curvature. Then $\exp_p: T_pM \to M$ is a covering map. I don`t understand what it means to ""impact"" the topology. How does the theorem exhibit such an effect?","My note says that Cartan-Hadamard theorem is an example of how negative sectional curvature ""impacts"" the topology of manifolds. The version of the theorem we are using is: Let $(M,g)$ be a complete Riemannian manifold with everywhere non-positive sectional curvature. Then $\exp_p: T_pM \to M$ is a covering map. I don`t understand what it means to ""impact"" the topology. How does the theorem exhibit such an effect?",,['differential-geometry']
74,Are the geodesics of $SO(n)$ rotation in a plane?,Are the geodesics of  rotation in a plane?,SO(n),"In what follows, I use "" rotation "" in a sense more closely mirroring that of everyday usage -- specifically I mean a smooth path between two elements of $SO(n)$ (since in everyday usage a rotation means progressing through all intermediate states between the initial and final states (see for example pp. 52-53 of Stillwell's Naive Lie Theory , which inspired this question, in part). Without loss of generality, let the smooth path begin at $Id \in SO(n)$ and end at some arbitrary $C \in SO(n)$ -- this is because, given any smooth path $A \to B$ , we can get a path $Id \to A^{-1} B$ by taking the left action of $A^{-1}$ which is a homeomorphism (and even a diffeomorphism, I think). Question: Do the geodesics of $SO(n)$ correspond to our ""intuitive sense"" of rotations in $\mathbb{R}^3$ , i.e. the original position and the final position being separated by rotation through a single plane, as opposed to the possibility of having to compose multiple rotations occurring in multiple planes? Remember that I am only concerned with the geodesic path between two elements of $SO(n)$ here, the fact that the path is a geodesic meaning to imply that it is in some sense the ""natural"" path of intermediate states to rotate through in order to arrive at one position from another. It would make some sense if this were the case, because then any such path would be an embedding of part of $SO(2)$ (the unit circle) into $SO(n)$ , and $SO(2)$ is one-dimensional, thus an embedding of a compact subset of it into $SO(n)$ could conceivably be a path. I am not quite sure how to phrase this question in a rigorous manner, although I have tried -- please let me know if anything is still unclear so that I can correct and improve the clarity of the question.","In what follows, I use "" rotation "" in a sense more closely mirroring that of everyday usage -- specifically I mean a smooth path between two elements of (since in everyday usage a rotation means progressing through all intermediate states between the initial and final states (see for example pp. 52-53 of Stillwell's Naive Lie Theory , which inspired this question, in part). Without loss of generality, let the smooth path begin at and end at some arbitrary -- this is because, given any smooth path , we can get a path by taking the left action of which is a homeomorphism (and even a diffeomorphism, I think). Question: Do the geodesics of correspond to our ""intuitive sense"" of rotations in , i.e. the original position and the final position being separated by rotation through a single plane, as opposed to the possibility of having to compose multiple rotations occurring in multiple planes? Remember that I am only concerned with the geodesic path between two elements of here, the fact that the path is a geodesic meaning to imply that it is in some sense the ""natural"" path of intermediate states to rotate through in order to arrive at one position from another. It would make some sense if this were the case, because then any such path would be an embedding of part of (the unit circle) into , and is one-dimensional, thus an embedding of a compact subset of it into could conceivably be a path. I am not quite sure how to phrase this question in a rigorous manner, although I have tried -- please let me know if anything is still unclear so that I can correct and improve the clarity of the question.",SO(n) Id \in SO(n) C \in SO(n) A \to B Id \to A^{-1} B A^{-1} SO(n) \mathbb{R}^3 SO(n) SO(2) SO(n) SO(2) SO(n),"['differential-geometry', 'lie-groups', 'geodesic']"
75,Broken line is NOT diffeomorphic to the real line,Broken line is NOT diffeomorphic to the real line,,"This is from Bredon's Topology and Geometry , page 71. This comes right after the very definition of differentiable manifold, so I think no use of tangent space or 'differential' is permitted. (Bredon gives two defenitions, one the usual chart and atlas definition, the other using the functional structure. He then explains that the two are equivalent.) Let $X$ be the graph of the real valued function $\theta(x) = |x|$ of a real variable $x$. Define a functional structure on $X$ by taking $f \in F(U) \iff f$ is the restriction to $U$ of a $C^\infty$ function on some open set $V$ in the plane with $U = V \cap X$. Show that $X$ with this structure is not diffeomorphic to the real line with usual $C^\infty$ structure. I thinks that if there were any diffeomorphism, something bad happens at $(0,0)$, but I just can't figure out... Please enlighten me.","This is from Bredon's Topology and Geometry , page 71. This comes right after the very definition of differentiable manifold, so I think no use of tangent space or 'differential' is permitted. (Bredon gives two defenitions, one the usual chart and atlas definition, the other using the functional structure. He then explains that the two are equivalent.) Let $X$ be the graph of the real valued function $\theta(x) = |x|$ of a real variable $x$. Define a functional structure on $X$ by taking $f \in F(U) \iff f$ is the restriction to $U$ of a $C^\infty$ function on some open set $V$ in the plane with $U = V \cap X$. Show that $X$ with this structure is not diffeomorphic to the real line with usual $C^\infty$ structure. I thinks that if there were any diffeomorphism, something bad happens at $(0,0)$, but I just can't figure out... Please enlighten me.",,"['differential-geometry', 'algebraic-topology', 'differential-topology']"
76,Perturbing a regular submanifold to ensure submersion.,Perturbing a regular submanifold to ensure submersion.,,"Suppose $M$ is a regular compact submanifold with boundary of dimension $n$ in $\mathbb R^{n+1}\smallsetminus 0$, and assume that for each $v\in S^n$ the ray emanating from $v$ intersects $M$ in at most one point. Denote by $S$ the subset of $S^{n}$ where this happens. The surjective submersion $r: \mathbb R^{n+1}\smallsetminus 0\longrightarrow S^n$ restricts to a bijection $r : M\longrightarrow S$. Note that $\ker r_{p,\ast}=\langle p\rangle$ intersects $M$ at one point, but it can happen that $\langle p\rangle \subseteq T_pM$ (for example, draw a projected $x\mapsto x^3$ around $S^1$). However, in the one dimensional case, there are only finitely many such points where this fails, and one can perturb $M$ by a diffeomorphism in a neighbourhood of such points to preserve all hypothesis and guarantee that $r$ restricts to a submersion $r: M\longrightarrow S$ which must then be a diffeomorphism. Suppose now that $n>1$. One can mimic the above to obtain a submanifold $M$ where the points where $\langle p\rangle \subseteq T_pM$ form a $n-1$ dimensional submanifold in $S^n$. For example, by extending the previous example in higher dimensions, one can produce a circle inside $S^2$, etc. Can one in general modify $M$ slightly by a diffeomorphism so that $\langle p\rangle + T_p M =T_p\mathbb R^{n+1}$?","Suppose $M$ is a regular compact submanifold with boundary of dimension $n$ in $\mathbb R^{n+1}\smallsetminus 0$, and assume that for each $v\in S^n$ the ray emanating from $v$ intersects $M$ in at most one point. Denote by $S$ the subset of $S^{n}$ where this happens. The surjective submersion $r: \mathbb R^{n+1}\smallsetminus 0\longrightarrow S^n$ restricts to a bijection $r : M\longrightarrow S$. Note that $\ker r_{p,\ast}=\langle p\rangle$ intersects $M$ at one point, but it can happen that $\langle p\rangle \subseteq T_pM$ (for example, draw a projected $x\mapsto x^3$ around $S^1$). However, in the one dimensional case, there are only finitely many such points where this fails, and one can perturb $M$ by a diffeomorphism in a neighbourhood of such points to preserve all hypothesis and guarantee that $r$ restricts to a submersion $r: M\longrightarrow S$ which must then be a diffeomorphism. Suppose now that $n>1$. One can mimic the above to obtain a submanifold $M$ where the points where $\langle p\rangle \subseteq T_pM$ form a $n-1$ dimensional submanifold in $S^n$. For example, by extending the previous example in higher dimensions, one can produce a circle inside $S^2$, etc. Can one in general modify $M$ slightly by a diffeomorphism so that $\langle p\rangle + T_p M =T_p\mathbb R^{n+1}$?",,"['differential-geometry', 'differential-topology']"
77,Computing Curvature of a Connection (Dirac Monopole),Computing Curvature of a Connection (Dirac Monopole),,"I'm trying to verify some computations in a paper I'm reading and am feeling a little lost. In particular I haven't been able to properly compute curvature of  a connection acting on a line bundle. Here's a specific example (modeled on the Dirac monopole): Consider spherical coordinates related to Euclidean coordinates by $$(t,x,y) = (R\cos \theta, R\cos \psi \sin \theta, R \sin \psi \sin \theta)$$ Take the Hermitian line bundle $L_k$ over $\mathbb{R}^3$ defined by the transition function $g_{\theta 0} = e^{ik\psi}$ on the complement of the $t$-axis from $\theta \neq 0$ to $\theta \neq \pi$. Consider the connection $\bigtriangledown$  on $L_k$ defined by connection matrices $$A_0 = (ik/2)(1+\cos \theta) d\psi$$ $$A_\pi = (ik/2)(-1+\cos \theta)d\psi$$ where the first is defined on $\theta \neq 0$, the second on $\theta \neq \pi$. Let $\phi = ik/2R$. Then, $$\bigtriangledown \phi = d\phi = -(ik/2R^2) dR = -\star((ik/2)d(\cos \theta d \psi)) = \star F_{\bigtriangledown}$$ Its the last equality that troubles me. How does one compute the connection from the matrices? I've also only computed very easy examples of curvature so I'm lost on obtaining $F_\bigtriangledown$ as well. A detailed computation/explanation would be helpful. Thanks.","I'm trying to verify some computations in a paper I'm reading and am feeling a little lost. In particular I haven't been able to properly compute curvature of  a connection acting on a line bundle. Here's a specific example (modeled on the Dirac monopole): Consider spherical coordinates related to Euclidean coordinates by $$(t,x,y) = (R\cos \theta, R\cos \psi \sin \theta, R \sin \psi \sin \theta)$$ Take the Hermitian line bundle $L_k$ over $\mathbb{R}^3$ defined by the transition function $g_{\theta 0} = e^{ik\psi}$ on the complement of the $t$-axis from $\theta \neq 0$ to $\theta \neq \pi$. Consider the connection $\bigtriangledown$  on $L_k$ defined by connection matrices $$A_0 = (ik/2)(1+\cos \theta) d\psi$$ $$A_\pi = (ik/2)(-1+\cos \theta)d\psi$$ where the first is defined on $\theta \neq 0$, the second on $\theta \neq \pi$. Let $\phi = ik/2R$. Then, $$\bigtriangledown \phi = d\phi = -(ik/2R^2) dR = -\star((ik/2)d(\cos \theta d \psi)) = \star F_{\bigtriangledown}$$ Its the last equality that troubles me. How does one compute the connection from the matrices? I've also only computed very easy examples of curvature so I'm lost on obtaining $F_\bigtriangledown$ as well. A detailed computation/explanation would be helpful. Thanks.",,"['differential-geometry', 'mathematical-physics', 'vector-bundles', 'curvature', 'connections']"
78,Tangential component of normal vector parallel along curve iff curve is geodesic?,Tangential component of normal vector parallel along curve iff curve is geodesic?,,"Exercise 6.3 (Millman & Parker, Elements of Differential Geometry) .   Let $$X_N = N - \langle N, n \rangle n $$ be the tangential component of the normal vector $N$ of a unit speed curve $\gamma$ on a surface $M$.   Prove that the following are equivalent: $X_N = 0$. $\gamma$ is a geodesic. $X_N$ is parallel along $\gamma$. I'm having trouble proving (3) implies (1) or (2). Any ideas?  I think I can do the other implications.. (1 -> 2): If $X_N = 0$, then $N = \langle N, n \rangle n$ so that $$\kappa_g := \langle \gamma'', S\rangle = \kappa \langle N, S \rangle = \kappa \langle \langle N, n \rangle n, S \rangle = 0. $$ (2 -> 1): If $\kappa_g = 0$, then $\langle N, S \rangle = 0$. Since $\{n, T ,S\}$ form an orthonormal basis, $$X_N = \langle N, T\rangle T+ \langle N, S\rangle S = 0.$$ (1 -> 3): If $X_N = 0$, then $\frac{dX_N}{dt} = 0$ is clearly perpendicular to the surface $M$. In my book's notation, $T = \gamma'$, $N= T' / \kappa$, $n$ is the unit normal to the surface, $S = n \times T$, and a vector field $X$ is said to be parallel along $\gamma$ if $dX/dt$ is perpendicular to $M$.","Exercise 6.3 (Millman & Parker, Elements of Differential Geometry) .   Let $$X_N = N - \langle N, n \rangle n $$ be the tangential component of the normal vector $N$ of a unit speed curve $\gamma$ on a surface $M$.   Prove that the following are equivalent: $X_N = 0$. $\gamma$ is a geodesic. $X_N$ is parallel along $\gamma$. I'm having trouble proving (3) implies (1) or (2). Any ideas?  I think I can do the other implications.. (1 -> 2): If $X_N = 0$, then $N = \langle N, n \rangle n$ so that $$\kappa_g := \langle \gamma'', S\rangle = \kappa \langle N, S \rangle = \kappa \langle \langle N, n \rangle n, S \rangle = 0. $$ (2 -> 1): If $\kappa_g = 0$, then $\langle N, S \rangle = 0$. Since $\{n, T ,S\}$ form an orthonormal basis, $$X_N = \langle N, T\rangle T+ \langle N, S\rangle S = 0.$$ (1 -> 3): If $X_N = 0$, then $\frac{dX_N}{dt} = 0$ is clearly perpendicular to the surface $M$. In my book's notation, $T = \gamma'$, $N= T' / \kappa$, $n$ is the unit normal to the surface, $S = n \times T$, and a vector field $X$ is said to be parallel along $\gamma$ if $dX/dt$ is perpendicular to $M$.",,['differential-geometry']
79,Maps between manifolds with boundary and homeomorphism,Maps between manifolds with boundary and homeomorphism,,"Assume we have $f:(M,\partial M)\rightarrow (N,\partial N)$ connected 3-manifolds, not compact, such that $f$ is an homeomorphism onto its image and $f(\partial M)=\partial N$. Can say that $f$ has to be surjective?","Assume we have $f:(M,\partial M)\rightarrow (N,\partial N)$ connected 3-manifolds, not compact, such that $f$ is an homeomorphism onto its image and $f(\partial M)=\partial N$. Can say that $f$ has to be surjective?",,"['differential-geometry', 'algebraic-topology', 'manifolds', 'smooth-manifolds']"
80,Normal vector field to this hypersurface,Normal vector field to this hypersurface,,"Let $M^n$ be a hypersurface of the unit sphere $S^{n+1} \subset \mathbb{R}^{n+2}$ contained in the open upper hemisphere $S^{n+1}_+$, and let $N : M \to \mathbb{R}^{n+2}$ be a unit normal vector field to $M$ (with $\langle N(p), p \rangle = 0$ for all $p$). Consider the diffeomorphism $f : S^{n+1}_+ \to \mathbb{R}^{n+1}$ obtained by central projection, that is, $$f(x_1, \dots, x_{n+2}) = \left( \frac{x_1}{x_{n+2}}, \dots, \frac{x_{n+1}}{x_{n+2}} \right).$$ How do I get a normal vector field $\overline{N}$ to the hypersurface $\overline{M} = f(M)$? I know that we can proceed like this: take a parametrization $\varphi : U \to M$ and define $$\overline{N}(f(p)) = w_1 \times \cdots \times w_n,$$ where $w_i = \frac{\partial(f \circ \varphi)}{\partial x_i}(\varphi^{-1}(p)) = Df(p) \cdot \frac{\partial\varphi}{\partial x_i}(\varphi^{-1}(p))$. But the vector products are not explicit enough. Is there a nicer expression? Thanks for your thoughts.","Let $M^n$ be a hypersurface of the unit sphere $S^{n+1} \subset \mathbb{R}^{n+2}$ contained in the open upper hemisphere $S^{n+1}_+$, and let $N : M \to \mathbb{R}^{n+2}$ be a unit normal vector field to $M$ (with $\langle N(p), p \rangle = 0$ for all $p$). Consider the diffeomorphism $f : S^{n+1}_+ \to \mathbb{R}^{n+1}$ obtained by central projection, that is, $$f(x_1, \dots, x_{n+2}) = \left( \frac{x_1}{x_{n+2}}, \dots, \frac{x_{n+1}}{x_{n+2}} \right).$$ How do I get a normal vector field $\overline{N}$ to the hypersurface $\overline{M} = f(M)$? I know that we can proceed like this: take a parametrization $\varphi : U \to M$ and define $$\overline{N}(f(p)) = w_1 \times \cdots \times w_n,$$ where $w_i = \frac{\partial(f \circ \varphi)}{\partial x_i}(\varphi^{-1}(p)) = Df(p) \cdot \frac{\partial\varphi}{\partial x_i}(\varphi^{-1}(p))$. But the vector products are not explicit enough. Is there a nicer expression? Thanks for your thoughts.",,"['differential-geometry', 'vector-fields', 'spherical-geometry']"
81,Derive $\kappa(t)=\frac{\lvert \boldsymbol{a}(t) \times \boldsymbol{v}(t) \rvert}{v^3(t)}$ directly instead of proving it.,Derive  directly instead of proving it.,\kappa(t)=\frac{\lvert \boldsymbol{a}(t) \times \boldsymbol{v}(t) \rvert}{v^3(t)},"Let $\boldsymbol{r}(t)$ be a parametrised curve. Then, $$\boldsymbol{\hat{T}}=\frac{\boldsymbol{r'}(t)}{\lvert \boldsymbol{r'}(t) \rvert }$$ $$\boldsymbol{\hat{N}}=\frac{\boldsymbol{\hat{T}'}(t)}{\lvert \boldsymbol{\hat{T}'}(t) \rvert } $$ $$\frac{d\boldsymbol{\hat{T}}}{ds}=\frac{\lvert \boldsymbol{\hat{T'}}(t) \rvert}{v(t)} \boldsymbol{\hat{N}}=\kappa(t)\boldsymbol{\hat{N}}$$ $$v(t)=\frac{ds}{dt}$$ $$\kappa(t) \equiv \frac{\lvert \boldsymbol{\hat{T'}}(t) \rvert}{v(t)} $$ $$\kappa(t)=\frac{\lvert \boldsymbol{a}(t) \times \boldsymbol{v}(t) \rvert}{v^3(t)}$$ I have seen some proofs of the last formula. But nevertheless I wonder whether it is possible to derive this formula directly. I think I need to clarify my intention: Suppose you know the definition of curvature as in the third equation but you don't know the formula for curvature as in the last equation. Then, someone asks you to give a formula for the curvature in terms of $\boldsymbol{r}(t)$ and its derivatives. How would you proceed, and how would you find this last equation? I think, it has to come from somewhere... Well, I have got another, closely linked, question: Is there a formula for $$\frac{d\boldsymbol{\hat{T}}}{ds}$$ in terms of $\boldsymbol{r}(t)$ and its derivatives?","Let $\boldsymbol{r}(t)$ be a parametrised curve. Then, $$\boldsymbol{\hat{T}}=\frac{\boldsymbol{r'}(t)}{\lvert \boldsymbol{r'}(t) \rvert }$$ $$\boldsymbol{\hat{N}}=\frac{\boldsymbol{\hat{T}'}(t)}{\lvert \boldsymbol{\hat{T}'}(t) \rvert } $$ $$\frac{d\boldsymbol{\hat{T}}}{ds}=\frac{\lvert \boldsymbol{\hat{T'}}(t) \rvert}{v(t)} \boldsymbol{\hat{N}}=\kappa(t)\boldsymbol{\hat{N}}$$ $$v(t)=\frac{ds}{dt}$$ $$\kappa(t) \equiv \frac{\lvert \boldsymbol{\hat{T'}}(t) \rvert}{v(t)} $$ $$\kappa(t)=\frac{\lvert \boldsymbol{a}(t) \times \boldsymbol{v}(t) \rvert}{v^3(t)}$$ I have seen some proofs of the last formula. But nevertheless I wonder whether it is possible to derive this formula directly. I think I need to clarify my intention: Suppose you know the definition of curvature as in the third equation but you don't know the formula for curvature as in the last equation. Then, someone asks you to give a formula for the curvature in terms of $\boldsymbol{r}(t)$ and its derivatives. How would you proceed, and how would you find this last equation? I think, it has to come from somewhere... Well, I have got another, closely linked, question: Is there a formula for $$\frac{d\boldsymbol{\hat{T}}}{ds}$$ in terms of $\boldsymbol{r}(t)$ and its derivatives?",,"['differential-geometry', 'curvature']"
82,Path density between two points,Path density between two points,,"What do i mean by path density? Consider a simple 4-connected grid with two points two spaces apart 0 0 0 0 0             Allowed      0 1 0  0 A 0 B 0             Moves =>     1 0 1 0 0 0 0 0   <= Grid                0 1 0 If we fix the path length to 4, we get 16 possible paths from A to B Adding up every visited square (divided by the total number of paths) gives us the probability of visiting a square while walking along a random path 0 0 0 0 0   0 0 0 1 0   0 0 0 0 0   0 0 0 0 0 0 1 1 2 1   0 1 1 2 0   0 1 2 2 0   0 1 1 2 0 0 0 0 0 0   0 0 0 0 0   0 0 0 0 0   0 0 0 1 0     +           +           +           + 0 0 0 0 0   0 1 0 0 0   0 0 0 0 0   0 0 0 0 0 0 2 1 1 0   0 2 1 1 0   1 2 1 1 0   0 2 2 1 0 0 1 0 0 0   0 0 0 0 0   0 0 0 0 0   0 0 0 0 0     +           +           +           + 0 1 1 1 0   0 1 1 0 0   0 0 0 0 0   0 0 1 0 0 0 1 0 1 0   0 1 1 1 0   0 1 1 1 0   0 1 2 1 0 0 0 0 0 0   0 0 0 0 0   0 1 1 0 0   0 0 0 0 0     +           +           +           + 0 0 0 0 0   0 0 1 1 0   0 0 0 0 0   0 0 0 0 0           0  3  4  3  0 0 1 0 1 0 + 0 1 1 1 0 + 0 1 1 1 0 + 0 1 2 1 0    =      1  20 18 20 1  0 1 1 1 0   0 0 0 0 0   0 0 1 1 0   0 0 1 0 0           0  3  4  3  0 There are a number of ways to tackle the problem on a grid purely combinatorial (unwieldy formulas with many binomials, not very insightful) via adjacency matrix, which to the n-th power gives the reachable positions after n steps (very powerful, one can easily implement obstacles on the grid) convolving a zero-filled grid with a 1 on starting position with the movement kernel n times (basiclly the same but less efficient, though easier to implement and visualize) How does it extend for a continuos path on a plane? My idea was to approximate ""movement in any direction by $\Delta r$"" with the same grid approach and a discretized ring as movement kernel Keeping $\Delta r = 7$ (second ring) constant while increasing the path length and distance between the two points is equivalent to making $\Delta r$ smaller without amplifying the discretization artifacts. (shown is log(1 + #pixel_visits) because the numbers are huge, in the last image there is a total of $10^{198}$ possible paths) As one would expect the outer border is elliptical. The insides look nice and smooth, somewhat like a sum of ellipses with same focal points and different excentrities, giving hope that an analytical solution might exist, but how to find it?","What do i mean by path density? Consider a simple 4-connected grid with two points two spaces apart 0 0 0 0 0             Allowed      0 1 0  0 A 0 B 0             Moves =>     1 0 1 0 0 0 0 0   <= Grid                0 1 0 If we fix the path length to 4, we get 16 possible paths from A to B Adding up every visited square (divided by the total number of paths) gives us the probability of visiting a square while walking along a random path 0 0 0 0 0   0 0 0 1 0   0 0 0 0 0   0 0 0 0 0 0 1 1 2 1   0 1 1 2 0   0 1 2 2 0   0 1 1 2 0 0 0 0 0 0   0 0 0 0 0   0 0 0 0 0   0 0 0 1 0     +           +           +           + 0 0 0 0 0   0 1 0 0 0   0 0 0 0 0   0 0 0 0 0 0 2 1 1 0   0 2 1 1 0   1 2 1 1 0   0 2 2 1 0 0 1 0 0 0   0 0 0 0 0   0 0 0 0 0   0 0 0 0 0     +           +           +           + 0 1 1 1 0   0 1 1 0 0   0 0 0 0 0   0 0 1 0 0 0 1 0 1 0   0 1 1 1 0   0 1 1 1 0   0 1 2 1 0 0 0 0 0 0   0 0 0 0 0   0 1 1 0 0   0 0 0 0 0     +           +           +           + 0 0 0 0 0   0 0 1 1 0   0 0 0 0 0   0 0 0 0 0           0  3  4  3  0 0 1 0 1 0 + 0 1 1 1 0 + 0 1 1 1 0 + 0 1 2 1 0    =      1  20 18 20 1  0 1 1 1 0   0 0 0 0 0   0 0 1 1 0   0 0 1 0 0           0  3  4  3  0 There are a number of ways to tackle the problem on a grid purely combinatorial (unwieldy formulas with many binomials, not very insightful) via adjacency matrix, which to the n-th power gives the reachable positions after n steps (very powerful, one can easily implement obstacles on the grid) convolving a zero-filled grid with a 1 on starting position with the movement kernel n times (basiclly the same but less efficient, though easier to implement and visualize) How does it extend for a continuos path on a plane? My idea was to approximate ""movement in any direction by $\Delta r$"" with the same grid approach and a discretized ring as movement kernel Keeping $\Delta r = 7$ (second ring) constant while increasing the path length and distance between the two points is equivalent to making $\Delta r$ smaller without amplifying the discretization artifacts. (shown is log(1 + #pixel_visits) because the numbers are huge, in the last image there is a total of $10^{198}$ possible paths) As one would expect the outer border is elliptical. The insides look nice and smooth, somewhat like a sum of ellipses with same focal points and different excentrities, giving hope that an analytical solution might exist, but how to find it?",,"['differential-geometry', 'density-function']"
83,On conformal metrics notation,On conformal metrics notation,,"A simple question, just for clarifying: suppose we have two riemannian metrics $g$ and $\tilde{g}$ in a differentiable manifold $M$, and assume they are conformal say, with $\tilde{g} = \mu g$ for some positive valued differentiable function $\mu : M \to \mathbb{R}$. This means that $$\tilde{g}(p)(v, w) = \mu(p) g(p)(v, w), \quad \forall  p \in M, \forall v,w \in T_p M$$ right?","A simple question, just for clarifying: suppose we have two riemannian metrics $g$ and $\tilde{g}$ in a differentiable manifold $M$, and assume they are conformal say, with $\tilde{g} = \mu g$ for some positive valued differentiable function $\mu : M \to \mathbb{R}$. This means that $$\tilde{g}(p)(v, w) = \mu(p) g(p)(v, w), \quad \forall  p \in M, \forall v,w \in T_p M$$ right?",,"['differential-geometry', 'notation', 'riemannian-geometry', 'conformal-geometry']"
84,"Flows and Lie brackets, $\beta$ not a priori smooth at $t = 0$","Flows and Lie brackets,  not a priori smooth at",\beta t = 0,"Let $X$ and $Y$ be smooth vector fields on $M$ generating flows $\phi_t$ and $\psi_t$ respectively. For $p \in M$ define$$\beta(t) := \psi_{-\sqrt{t}} \phi_{-\sqrt{t}} \psi_{\sqrt{t}} \phi_{\sqrt{t}}(p)$$for $t \in (-\epsilon , \epsilon)$ where $\epsilon$ is sufficiently small. Does it follow that$$[X, Y](f)(p) = \lim_{t \to 0} {{f(\beta(t)) - f(\beta(0))}\over t}?$$ Thoughts. I know that $\beta$ is not a priori smooth at $t = 0$ because of the $\sqrt{}$ terms. But I do not know what do from here. Could anybody help?","Let $X$ and $Y$ be smooth vector fields on $M$ generating flows $\phi_t$ and $\psi_t$ respectively. For $p \in M$ define$$\beta(t) := \psi_{-\sqrt{t}} \phi_{-\sqrt{t}} \psi_{\sqrt{t}} \phi_{\sqrt{t}}(p)$$for $t \in (-\epsilon , \epsilon)$ where $\epsilon$ is sufficiently small. Does it follow that$$[X, Y](f)(p) = \lim_{t \to 0} {{f(\beta(t)) - f(\beta(0))}\over t}?$$ Thoughts. I know that $\beta$ is not a priori smooth at $t = 0$ because of the $\sqrt{}$ terms. But I do not know what do from here. Could anybody help?",,"['differential-geometry', 'manifolds']"
85,A question my proof about the line of curvature,A question my proof about the line of curvature,,"I am working on Exercise 2.4.4 of Differential Geometry and Its Application. The problem statement and my work is available at this link . At the end of my proof, I claimed that $ S_p(\alpha') $ is both on the plane and the tangent plane, so it must be $ k \alpha' $, at this point I am unsure if that is correct. I think so because $ \alpha' $ must be on the plane and on the tangent plane, but could that two planes be the same?","I am working on Exercise 2.4.4 of Differential Geometry and Its Application. The problem statement and my work is available at this link . At the end of my proof, I claimed that $ S_p(\alpha') $ is both on the plane and the tangent plane, so it must be $ k \alpha' $, at this point I am unsure if that is correct. I think so because $ \alpha' $ must be on the plane and on the tangent plane, but could that two planes be the same?",,['differential-geometry']
86,moving frame with maple,moving frame with maple,,"I have already ask this question on stackoverflow , but since it concerns as mathematics than computer science, I ask it here too. I would like to make a classical computation using maple. I would like define an abstract moving frame (e_1,e_2), then 1) get the dual frame omega_1, omega_2 (always abstract) omega_i(e_j)=delta_i^j 2) define omega_12 such that d omega_1= omega_12 wedge omega_2 3) define Gauss curvature K such that d omega_12=K omega_1\wedge omega_2 4) Finally I would like to get an abstract formula for \overline{K} the curvature of the a new frame (\overline{e}_1,\overline{e}_2)=e^f (e_1,e_2) , where f is a abstract function, with respect to  K and f. I should use the package Differential geometry but even I don't succeed to define the frame. The exmpale in the description of the library is DGsetup([x,y,z,w],M): Example 1. Define a 3-dimensional subspace of vectors by the span of S and compute a simpler base for this subspace relative to the coordinate basis T for the tangent space of M. S1:=evalDG([D_x−D_y,D_x+D_y,D_x+D_y+D_w]) But S1 is an explicit frame I want to define an abstract one as I can do for functions. For instance if I want an abstract Liebnitz rule, diff(f(x)*g(x), x); / d      \             / d      \            |--- f(x)| g(x) + f(x) |--- g(x)|            \ dx     /             \ dx     / Here I have tryied the following: with(DifferentialGeometry);   DGsetup([x, y], M);   S1 := evalDG([e1, e2]);   M > B1 := DGbasis(S1);   Error, (in DifferentialGeometry:-DGbasis) expected 1st argument to be a list of biforms, forms, vectors tensors, matrices, vectors. Received [e1, e2] EDIT: Here is what I have start to do with Atlas. As you see, it doesn't use the symmetries of the Lie derivative. And I don't know how to make my 4)","I have already ask this question on stackoverflow , but since it concerns as mathematics than computer science, I ask it here too. I would like to make a classical computation using maple. I would like define an abstract moving frame (e_1,e_2), then 1) get the dual frame omega_1, omega_2 (always abstract) omega_i(e_j)=delta_i^j 2) define omega_12 such that d omega_1= omega_12 wedge omega_2 3) define Gauss curvature K such that d omega_12=K omega_1\wedge omega_2 4) Finally I would like to get an abstract formula for \overline{K} the curvature of the a new frame (\overline{e}_1,\overline{e}_2)=e^f (e_1,e_2) , where f is a abstract function, with respect to  K and f. I should use the package Differential geometry but even I don't succeed to define the frame. The exmpale in the description of the library is DGsetup([x,y,z,w],M): Example 1. Define a 3-dimensional subspace of vectors by the span of S and compute a simpler base for this subspace relative to the coordinate basis T for the tangent space of M. S1:=evalDG([D_x−D_y,D_x+D_y,D_x+D_y+D_w]) But S1 is an explicit frame I want to define an abstract one as I can do for functions. For instance if I want an abstract Liebnitz rule, diff(f(x)*g(x), x); / d      \             / d      \            |--- f(x)| g(x) + f(x) |--- g(x)|            \ dx     /             \ dx     / Here I have tryied the following: with(DifferentialGeometry);   DGsetup([x, y], M);   S1 := evalDG([e1, e2]);   M > B1 := DGbasis(S1);   Error, (in DifferentialGeometry:-DGbasis) expected 1st argument to be a list of biforms, forms, vectors tensors, matrices, vectors. Received [e1, e2] EDIT: Here is what I have start to do with Atlas. As you see, it doesn't use the symmetries of the Lie derivative. And I don't know how to make my 4)",,"['differential-geometry', 'maple', 'cartan-geometry']"
87,Equivalence between constant and positive metric and usual $\Re^3$ metric,Equivalence between constant and positive metric and usual  metric,\Re^3,"I'm trying to answer the following question: Is any positive and constant metric in $\Re³$ equivalent to the usal metric defined as $$ds² = dx² + dy² + dz² \tag{*}\label{1} $$ with $ds = g_{ij}dx^{i}dx^{j}$ and $g_{ij} = \delta_{ij}$. Or alternatively, does always exist a coordinate transformation from the coordinates $y^\mu$ to the coordinates $x^i$ such that \eqref{1} is recovered from $$ds² =    \tilde{g}_{\mu\nu}dy^{\mu}dy^{\nu} $$ with $\tilde{g}_{\mu\nu}$ being constant in the $y^\mu$ coordinates. My initial guess it's that this is true, however I'm not sure on how this could be proved.","I'm trying to answer the following question: Is any positive and constant metric in $\Re³$ equivalent to the usal metric defined as $$ds² = dx² + dy² + dz² \tag{*}\label{1} $$ with $ds = g_{ij}dx^{i}dx^{j}$ and $g_{ij} = \delta_{ij}$. Or alternatively, does always exist a coordinate transformation from the coordinates $y^\mu$ to the coordinates $x^i$ such that \eqref{1} is recovered from $$ds² =    \tilde{g}_{\mu\nu}dy^{\mu}dy^{\nu} $$ with $\tilde{g}_{\mu\nu}$ being constant in the $y^\mu$ coordinates. My initial guess it's that this is true, however I'm not sure on how this could be proved.",,"['differential-geometry', 'euclidean-geometry']"
88,Punctured complex projective space,Punctured complex projective space,,"Let $\mathcal{P}\mathbb{C}^{n}$ be the complex projective space of $\mathbb{C}^{n+1}$, and let $B=\{\mathbf{e}_{1},\cdots,\mathbf{e}_{n+1}\}$ be a basis in $\mathbb{C}^{n+1}$. I would like to understand what happens to $\mathcal{P}\mathbb{C}^{n}$ if I remove the points in it corresponding to the basis vectors in $B$. In the case $\mathbb{C}^{2}$ I know that $\mathcal{P}\mathbb{C}^{1}\cong S^{2}$ and, from the quantum mechanical picture of $\mathcal{P}\mathbb{C}^{1}$ as the Bloch sphere, I know that the points in $\mathcal{P}\mathbb{C}^{1}$ corresponding to two basis vectors in $\mathbb{C}^{2}$ are just antipodal points. Consequently, removing these points from $\mathcal{P}\mathbb{C}^{1}$ I get a cilinder $S^{1}\times\mathbb{R}$. Unfortunately, this visually-inspired procedure does not work in higher dimensions, and I do not know how to even start to face the problem, therefore, I appreciate any comment, suggestion or reference. Thank You. EDIT I thought of something. Let $\mathbf{E}_{j}$ be the rank-one projector associated to $\mathbf{e}_{j}\in B$, let $\mathbf{H}=\sum_{j}\nu_{j}\mathbf{E}_{j}$ be a self-adjoint operator on $\mathbb{C}^{n+1}$ such that the one-parameter group of unitary operators $\mathbf{U}_{\tau}:=\exp(-\imath\tau\mathbf{H})$ on $\mathbb{C}^{n+1}$ generated by $\mathbf{H}$ is a closed subgroup of the unitary group $U(n+1)$. Therefore, $\mathbf{U}_{\tau}$ is an action of the circle group $U(1)\cong S^{1}$ on the complex projective space $\mathcal{P}\mathbb{C}^{n}$. The fixed points of this action are just the points corresponding to the elements of $B$. Let $\mathcal{P}\mathbb{C}^{n}_{*}$ denotes the complex projective space without the fixed points of the action. Since $U(1)\cong S^{1}$ is a compact group, its action on $\mathcal{P}\mathbb{C}^{n}_{*}$ is proper. Furthermore, it is free by construction. This means that the orbit space $\mathcal{P}\mathbb{C}^{n}_{*}/U(1)\equiv M$ is a differential manifold, the canonical projection $\pi:\mathcal{P}\mathbb{C}^{n}_{*}\rightarrow M$ is a surjection, and $\left(\mathcal{P}\mathbb{C}^{n}_{*}\,;M\,;\pi\,;U(1)\right)$ is a $U(1)$-principal bundle. At this point, if the bundle is trivial, we have that $\mathcal{P}\mathbb{C}^{n}_{*}\cong M\times U(1)$, however, I am not able to go further.","Let $\mathcal{P}\mathbb{C}^{n}$ be the complex projective space of $\mathbb{C}^{n+1}$, and let $B=\{\mathbf{e}_{1},\cdots,\mathbf{e}_{n+1}\}$ be a basis in $\mathbb{C}^{n+1}$. I would like to understand what happens to $\mathcal{P}\mathbb{C}^{n}$ if I remove the points in it corresponding to the basis vectors in $B$. In the case $\mathbb{C}^{2}$ I know that $\mathcal{P}\mathbb{C}^{1}\cong S^{2}$ and, from the quantum mechanical picture of $\mathcal{P}\mathbb{C}^{1}$ as the Bloch sphere, I know that the points in $\mathcal{P}\mathbb{C}^{1}$ corresponding to two basis vectors in $\mathbb{C}^{2}$ are just antipodal points. Consequently, removing these points from $\mathcal{P}\mathbb{C}^{1}$ I get a cilinder $S^{1}\times\mathbb{R}$. Unfortunately, this visually-inspired procedure does not work in higher dimensions, and I do not know how to even start to face the problem, therefore, I appreciate any comment, suggestion or reference. Thank You. EDIT I thought of something. Let $\mathbf{E}_{j}$ be the rank-one projector associated to $\mathbf{e}_{j}\in B$, let $\mathbf{H}=\sum_{j}\nu_{j}\mathbf{E}_{j}$ be a self-adjoint operator on $\mathbb{C}^{n+1}$ such that the one-parameter group of unitary operators $\mathbf{U}_{\tau}:=\exp(-\imath\tau\mathbf{H})$ on $\mathbb{C}^{n+1}$ generated by $\mathbf{H}$ is a closed subgroup of the unitary group $U(n+1)$. Therefore, $\mathbf{U}_{\tau}$ is an action of the circle group $U(1)\cong S^{1}$ on the complex projective space $\mathcal{P}\mathbb{C}^{n}$. The fixed points of this action are just the points corresponding to the elements of $B$. Let $\mathcal{P}\mathbb{C}^{n}_{*}$ denotes the complex projective space without the fixed points of the action. Since $U(1)\cong S^{1}$ is a compact group, its action on $\mathcal{P}\mathbb{C}^{n}_{*}$ is proper. Furthermore, it is free by construction. This means that the orbit space $\mathcal{P}\mathbb{C}^{n}_{*}/U(1)\equiv M$ is a differential manifold, the canonical projection $\pi:\mathcal{P}\mathbb{C}^{n}_{*}\rightarrow M$ is a surjection, and $\left(\mathcal{P}\mathbb{C}^{n}_{*}\,;M\,;\pi\,;U(1)\right)$ is a $U(1)$-principal bundle. At this point, if the bundle is trivial, we have that $\mathcal{P}\mathbb{C}^{n}_{*}\cong M\times U(1)$, however, I am not able to go further.",,"['differential-geometry', 'projective-space']"
89,Are there surfaces that have (except for cusps or borders) a constant positive gaussian curvature but that do not have not a constant mean curvature?,Are there surfaces that have (except for cusps or borders) a constant positive gaussian curvature but that do not have not a constant mean curvature?,,"I was puzzeling with the pseudosphere , a surface that except for a cusp has a constant negative Gaussian curvature, but has not everywhere the same mean curvature. ( https://en.wikipedia.org/wiki/Pseudosphere ) This made me wonder are there also surfaces that have (for a largish area) a constant positive Gaussian curvature but not a constant mean curvature? Something like a surface that has principle curvatures $K_1=1 ,K_2=1$ for a single point or limited set of curves while for the other points in this neightboorhood the Gaussian curvature $K_1K_2 =1$ but $ K_1 \not = K_2$ and this not just at a small part of the surface but at a largish area, like an area bounded by a cusp or boundary. The surface does not need to be closed.","I was puzzeling with the pseudosphere , a surface that except for a cusp has a constant negative Gaussian curvature, but has not everywhere the same mean curvature. ( https://en.wikipedia.org/wiki/Pseudosphere ) This made me wonder are there also surfaces that have (for a largish area) a constant positive Gaussian curvature but not a constant mean curvature? Something like a surface that has principle curvatures $K_1=1 ,K_2=1$ for a single point or limited set of curves while for the other points in this neightboorhood the Gaussian curvature $K_1K_2 =1$ but $ K_1 \not = K_2$ and this not just at a small part of the surface but at a largish area, like an area bounded by a cusp or boundary. The surface does not need to be closed.",,"['differential-geometry', 'riemannian-geometry']"
90,Curvature on product Riemannian manifolds,Curvature on product Riemannian manifolds,,"I am working on the following problem from Lee's Riemannian Manifolds : Suppose $g = g_1 \oplus g_2$ is a product metric on $M_1 \times M_2$   (i.e. $$g(X_1+X_2,Y_1+Y_2) = g_1(X_1,Y_1)+g_2(X_2,Y_2),$$ where $$X = X_1 + X_2, Y = Y_1+Y_2 \in T_{(p_1,p_2)}(M_1 \times M_2) = T_{p_1}M_1 \oplus T_{p_2}M_2.$$ (a) Show that for each point $p_i \in M_i$, the submanifolds $M_1 \times \{ p_2 \}$ and $\{ p_1 \} \times M_2$ are totally geodesic. (b) If II $\subseteq T(M_1 \times M_2)$ is a 2-plane spanned by $X_1 \in TM_1$ and $X_2 \in TM_2$, show that $K$(II) $= 0$ (the sectional curvature is $0$). (c) Show that the product metric on $S^2 \times S^2$ has nonnegative sectional curvature. (d) Show that there is an embedding of $T^2$ in $S^2 \times S^2$ such that the induced metric is flat. For part (a), I think I should be showing that the second fundamental form vanishes identically, but I'm not having any luck actually proving that.  I tried using the Weingarten equation, but got nowhere.  For part (b), I think that, using the formula for sectional curvature, this reduces to showing that $Rm(X_1,X_2,X_2,X_1) = 0$, but again I'm not having any luck proving that.  For (c), I think that if we take an arbitrary plane and use an orthonormal basis, we might be able to get something, but to be honest I'm just totally lost.  I have no idea for part (d).  Any help is MUCH appreciated!!!","I am working on the following problem from Lee's Riemannian Manifolds : Suppose $g = g_1 \oplus g_2$ is a product metric on $M_1 \times M_2$   (i.e. $$g(X_1+X_2,Y_1+Y_2) = g_1(X_1,Y_1)+g_2(X_2,Y_2),$$ where $$X = X_1 + X_2, Y = Y_1+Y_2 \in T_{(p_1,p_2)}(M_1 \times M_2) = T_{p_1}M_1 \oplus T_{p_2}M_2.$$ (a) Show that for each point $p_i \in M_i$, the submanifolds $M_1 \times \{ p_2 \}$ and $\{ p_1 \} \times M_2$ are totally geodesic. (b) If II $\subseteq T(M_1 \times M_2)$ is a 2-plane spanned by $X_1 \in TM_1$ and $X_2 \in TM_2$, show that $K$(II) $= 0$ (the sectional curvature is $0$). (c) Show that the product metric on $S^2 \times S^2$ has nonnegative sectional curvature. (d) Show that there is an embedding of $T^2$ in $S^2 \times S^2$ such that the induced metric is flat. For part (a), I think I should be showing that the second fundamental form vanishes identically, but I'm not having any luck actually proving that.  I tried using the Weingarten equation, but got nowhere.  For part (b), I think that, using the formula for sectional curvature, this reduces to showing that $Rm(X_1,X_2,X_2,X_1) = 0$, but again I'm not having any luck proving that.  For (c), I think that if we take an arbitrary plane and use an orthonormal basis, we might be able to get something, but to be honest I'm just totally lost.  I have no idea for part (d).  Any help is MUCH appreciated!!!",,"['differential-geometry', 'manifolds', 'riemannian-geometry', 'curvature']"
91,Problem proving Cartan's identity,Problem proving Cartan's identity,,"There is a famous identity stating that, if $X$ is a field and $\omega$ a form, then: $$\mathcal{L}_X\omega=\iota_Xd\omega+d\iota_X\omega.$$ I'm trying to prove it. Thanks to Anthony Carapetis , I know that: $$\iota_X(\alpha\wedge\beta)=(k+\ell)(\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta),$$ where $\alpha$ is a $k$-form and $\beta$ an $\ell$-form. Now I start on Cartan. I first suppose $\omega=d\alpha$ for $\alpha$ a $(k-1)$-form. Then: $$\mathcal{L}_Xd\alpha=d\mathcal{L}_X\alpha=d(d\iota_X\alpha+\iota_Xd\alpha)=d\iota_Xd\alpha=d\iota_X\omega,$$ and being $\omega$ exact, it is closed, so $\iota_Xd\omega=\iota_X0=0$. Now consider $f\omega$ for any smooth function $f$. $f$ will then be a 0-form. If I try induction, I get stuck with: $$\mathcal{L}_X(f\omega)=f\mathcal{L}_X\omega+\mathcal{L}_Xf\cdot\omega=fd\iota_X\omega+\iota_Xdf\cdot\omega,$$ whereas the RHS of Cartan would be: \begin{align*} d\iota_X(f\omega)+\iota_Xd(f\omega)={}&d(kf\iota_X\omega)+\iota_X(df\wedge\omega)={} \\ {}={}&kdf\wedge\iota_X\omega+kfd\iota_X\omega+(k+1)\iota_Xdf\wedge\omega-(k+1)df\wedge\iota_X\omega, \end{align*} and those coefficients get in the way, because the two sides only equate for $k=1$. Am I doing something wrong? Have I used the linked question too hastily to deduce a formula for the normalized wedge? Details Anthony proved, in his answer, that, if $\overline\wedge$ denotes the unnormalized antisimmetrization, then: $$\iota_X(\alpha\overline\wedge\beta)=k\iota_X\alpha\overline\wedge\beta+(-1)^k\ell\beta\overline\wedge\iota_X\beta.$$ The relationship between $\wedge$ and $\overline\wedge$ is that, if $\alpha$ is a $k$-form and $\beta$ an $\ell$-form: $$\alpha\wedge\beta=\frac{(k+\ell)!}{k!\ell!}\alpha\overline\wedge\beta.$$ The above identity then becomes: $$\iota_X\left(\frac{k!\ell!}{(k+\ell)!}\alpha\wedge\beta\right)=\frac{(k-1)!\ell!}{(k+\ell-1)!}k\iota_X\alpha\wedge\beta+(-1)^k\frac{k!(\ell-1)!}{(k+\ell-1)!}\ell\alpha\wedge\iota_X\beta,$$ which with due simplifications done after extracting the fraction from the parenthesis on the left yields: $$\frac{1}{k+\ell}\iota_X(\alpha\wedge\beta)=\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta,$$ which is almost identical to what I said at the start of the question after the link to my previous question. Am I doing something wrong here?","There is a famous identity stating that, if $X$ is a field and $\omega$ a form, then: $$\mathcal{L}_X\omega=\iota_Xd\omega+d\iota_X\omega.$$ I'm trying to prove it. Thanks to Anthony Carapetis , I know that: $$\iota_X(\alpha\wedge\beta)=(k+\ell)(\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta),$$ where $\alpha$ is a $k$-form and $\beta$ an $\ell$-form. Now I start on Cartan. I first suppose $\omega=d\alpha$ for $\alpha$ a $(k-1)$-form. Then: $$\mathcal{L}_Xd\alpha=d\mathcal{L}_X\alpha=d(d\iota_X\alpha+\iota_Xd\alpha)=d\iota_Xd\alpha=d\iota_X\omega,$$ and being $\omega$ exact, it is closed, so $\iota_Xd\omega=\iota_X0=0$. Now consider $f\omega$ for any smooth function $f$. $f$ will then be a 0-form. If I try induction, I get stuck with: $$\mathcal{L}_X(f\omega)=f\mathcal{L}_X\omega+\mathcal{L}_Xf\cdot\omega=fd\iota_X\omega+\iota_Xdf\cdot\omega,$$ whereas the RHS of Cartan would be: \begin{align*} d\iota_X(f\omega)+\iota_Xd(f\omega)={}&d(kf\iota_X\omega)+\iota_X(df\wedge\omega)={} \\ {}={}&kdf\wedge\iota_X\omega+kfd\iota_X\omega+(k+1)\iota_Xdf\wedge\omega-(k+1)df\wedge\iota_X\omega, \end{align*} and those coefficients get in the way, because the two sides only equate for $k=1$. Am I doing something wrong? Have I used the linked question too hastily to deduce a formula for the normalized wedge? Details Anthony proved, in his answer, that, if $\overline\wedge$ denotes the unnormalized antisimmetrization, then: $$\iota_X(\alpha\overline\wedge\beta)=k\iota_X\alpha\overline\wedge\beta+(-1)^k\ell\beta\overline\wedge\iota_X\beta.$$ The relationship between $\wedge$ and $\overline\wedge$ is that, if $\alpha$ is a $k$-form and $\beta$ an $\ell$-form: $$\alpha\wedge\beta=\frac{(k+\ell)!}{k!\ell!}\alpha\overline\wedge\beta.$$ The above identity then becomes: $$\iota_X\left(\frac{k!\ell!}{(k+\ell)!}\alpha\wedge\beta\right)=\frac{(k-1)!\ell!}{(k+\ell-1)!}k\iota_X\alpha\wedge\beta+(-1)^k\frac{k!(\ell-1)!}{(k+\ell-1)!}\ell\alpha\wedge\iota_X\beta,$$ which with due simplifications done after extracting the fraction from the parenthesis on the left yields: $$\frac{1}{k+\ell}\iota_X(\alpha\wedge\beta)=\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta,$$ which is almost identical to what I said at the start of the question after the link to my previous question. Am I doing something wrong here?",,"['differential-geometry', 'differential-forms', 'exterior-algebra']"
92,Second fundamental form and metrics,Second fundamental form and metrics,,"Suppose $M$ and $N$ are orientable manifolds, $f: M \to N$ is a smooth embedding and $g$ is a Riemannian metric on $N$. When $M$ has codimension $1$ and $\vec{n}$ is a prefered unit normal section of $f$, the second fundamental form of $f$ with respect to $\vec{n}$ can be regarded as a section $II: M \to T^*M \otimes T^*M$ defined by $II_p(X,Y) = g_p(\nabla_X Y, \vec{n}_p)$. It is a quadratic form on each tangent space, and, when it is additionally positive definte, it defines a Riemannian metric on $M$. I wonder if there is a nice geometric illustration of the resulting metric on $M$ in this case. In particular, in how far does it differ from the pullback-metric $f^*(g)$ ?","Suppose $M$ and $N$ are orientable manifolds, $f: M \to N$ is a smooth embedding and $g$ is a Riemannian metric on $N$. When $M$ has codimension $1$ and $\vec{n}$ is a prefered unit normal section of $f$, the second fundamental form of $f$ with respect to $\vec{n}$ can be regarded as a section $II: M \to T^*M \otimes T^*M$ defined by $II_p(X,Y) = g_p(\nabla_X Y, \vec{n}_p)$. It is a quadratic form on each tangent space, and, when it is additionally positive definte, it defines a Riemannian metric on $M$. I wonder if there is a nice geometric illustration of the resulting metric on $M$ in this case. In particular, in how far does it differ from the pullback-metric $f^*(g)$ ?",,"['differential-geometry', 'riemannian-geometry', 'surfaces']"
93,Do Lie derivatives commute with divergence: $\mathcal{L}_\xi \nabla_\mu V^\mu = \nabla_\mu \mathcal{L}_\xi V^{\mu}$?,Do Lie derivatives commute with divergence: ?,\mathcal{L}_\xi \nabla_\mu V^\mu = \nabla_\mu \mathcal{L}_\xi V^{\mu},"So my question is: given a vector field $V^\mu$ on some manifold with metric tensor $g_{\mu\nu}$ is it necessarily true that \begin{equation} \mathcal{L}_\xi \nabla_\mu V^\mu = \nabla_\mu \mathcal{L}_\xi V^{\mu} \ ? \end{equation}  where $\nabla_\mu$ denote the covariant derivative satisfying the metric compatibility condition $\nabla_\lambda g_{\mu\nu} = 0$ and $\mathcal{L}_\xi$ is the Lie derivative. If so how do I show that? If not, then is there any condition we could impose on $\xi$ such that this is true? i.e. Is it true if $\xi$ is a Killing vector or something like that?","So my question is: given a vector field $V^\mu$ on some manifold with metric tensor $g_{\mu\nu}$ is it necessarily true that \begin{equation} \mathcal{L}_\xi \nabla_\mu V^\mu = \nabla_\mu \mathcal{L}_\xi V^{\mu} \ ? \end{equation}  where $\nabla_\mu$ denote the covariant derivative satisfying the metric compatibility condition $\nabla_\lambda g_{\mu\nu} = 0$ and $\mathcal{L}_\xi$ is the Lie derivative. If so how do I show that? If not, then is there any condition we could impose on $\xi$ such that this is true? i.e. Is it true if $\xi$ is a Killing vector or something like that?",,"['differential-geometry', 'lie-derivative']"
94,An elementary differential geometry proof,An elementary differential geometry proof,,"Given a differentiable function $k(s)$, $s\in I$, show that the parametrized plane curve having $k(s)=k$ as curvature is given by   $$ \alpha (s) = \left( \int \cos\theta(s)ds + a, \int \sin\theta(s)ds + b \right) $$   where $$ \theta(s)= \int k(s)ds + \varphi $$ and that the curve is determined up to a translation of the vector $(a,b)$ and a rotation of the angle $\varphi$. This exercise is from Do Carmo Differential Geometry of Curves and Surfaces , section 1.5. My problem is that I'm not even sure where to start. It is not clear for me what is required for this type of proofs. I mean, the proof is based on some constructive procedure? Or the usual way is, instead, start as ""if such $\alpha$ exist, it must verify such and such...""","Given a differentiable function $k(s)$, $s\in I$, show that the parametrized plane curve having $k(s)=k$ as curvature is given by   $$ \alpha (s) = \left( \int \cos\theta(s)ds + a, \int \sin\theta(s)ds + b \right) $$   where $$ \theta(s)= \int k(s)ds + \varphi $$ and that the curve is determined up to a translation of the vector $(a,b)$ and a rotation of the angle $\varphi$. This exercise is from Do Carmo Differential Geometry of Curves and Surfaces , section 1.5. My problem is that I'm not even sure where to start. It is not clear for me what is required for this type of proofs. I mean, the proof is based on some constructive procedure? Or the usual way is, instead, start as ""if such $\alpha$ exist, it must verify such and such...""",,['differential-geometry']
95,What's the derivative of a map defined on manifolds?,What's the derivative of a map defined on manifolds?,,"I'm going through Warner's book on differentiable manifolds. On page 8 he defines what it means for a map $f: U \subset M \to \mathbb R$ to be differentiable: $f$ is differentiable iff $f \circ \psi$ is differentiable for all charts $\psi$ on $M$. He does not proceed to give a definition of the derivative of $f$. I tried to do a web search but did not find a definition. Is the derivative of $f$ just defined to be the derivative of $f \circ \psi$? What's the definition of the derivative of a map defined on manifolds? Edit At the bottom of page 105 in this book (in the proof of the regular level set theorem) the author calculates the Jacobian of a map $F: M \to N$. This Jacobian contains entries of the form ${\partial F \over \partial x_i}$. So, it seems to me that the derivative, at least partial derivatives exist (although a comment below by Mariano Suarez-Alvarez suggests otherwise)","I'm going through Warner's book on differentiable manifolds. On page 8 he defines what it means for a map $f: U \subset M \to \mathbb R$ to be differentiable: $f$ is differentiable iff $f \circ \psi$ is differentiable for all charts $\psi$ on $M$. He does not proceed to give a definition of the derivative of $f$. I tried to do a web search but did not find a definition. Is the derivative of $f$ just defined to be the derivative of $f \circ \psi$? What's the definition of the derivative of a map defined on manifolds? Edit At the bottom of page 105 in this book (in the proof of the regular level set theorem) the author calculates the Jacobian of a map $F: M \to N$. This Jacobian contains entries of the form ${\partial F \over \partial x_i}$. So, it seems to me that the derivative, at least partial derivatives exist (although a comment below by Mariano Suarez-Alvarez suggests otherwise)",,"['differential-geometry', 'differential-topology', 'definition']"
96,Uniqueness of covariant derivative in Do Carmo,Uniqueness of covariant derivative in Do Carmo,,"2.2 Proposition: Let $M$ be a differentiable manifold with an affine connection $\nabla$. There exists a unique correspondence which associates to a vector field $V$ along the differentiable curve $c:I\: M$ another vector field $\frac{DV}{dt}$ along $c$, called the covariant derivative of $c$ such that: a)$\frac{D}{dt}(V+W)=\frac{DV}{dt}+\frac{DW}{dt}$ b)$\frac{D}{dt}(fV)$=$\frac{df}{dt}V+f\frac{DV}{dt}$ c)If $V$ is induced by a vector field $Y\in \mathcal{X}(M)$, then $\frac{DV}{dt}=\nabla_{dc/dt}Y$. Proof: Let us suppose initially that there exists such a correspondence. Let $\sum v^jX_j$ denote a vector field $V$ in local coordinates, and where $X_j=\frac{\partial}{\partial x_j}$. By a) and b) then $$\frac{DV}{dt}=\sum_j\frac{dv^j}{dt}X_j+\sum_j v^j\frac{DX_j}{dt}$$ by c) and i) $$\frac{DX_j}{dt}=\sum_i \frac{dx_i}{dt}\nabla_{X_i}X_j$$ Therefore, $$\frac{DV}{dt}=\sum_j \frac{dv^j}{dt}X_j+\sum_{i,j}\frac{dx_i}{dt}v^j\nabla_{X_i} X_j$$ The expression above shows us that if there is a correspondence satisfying the condition of proposition 2.2, then such a correspondence is unique. So my question is why does producing the formula in local coordinates implies uniqueness. I just don't get Do Carmo's reasoning here. Doesn't he have to check compatibility conditions with respect to other charts?","2.2 Proposition: Let $M$ be a differentiable manifold with an affine connection $\nabla$. There exists a unique correspondence which associates to a vector field $V$ along the differentiable curve $c:I\: M$ another vector field $\frac{DV}{dt}$ along $c$, called the covariant derivative of $c$ such that: a)$\frac{D}{dt}(V+W)=\frac{DV}{dt}+\frac{DW}{dt}$ b)$\frac{D}{dt}(fV)$=$\frac{df}{dt}V+f\frac{DV}{dt}$ c)If $V$ is induced by a vector field $Y\in \mathcal{X}(M)$, then $\frac{DV}{dt}=\nabla_{dc/dt}Y$. Proof: Let us suppose initially that there exists such a correspondence. Let $\sum v^jX_j$ denote a vector field $V$ in local coordinates, and where $X_j=\frac{\partial}{\partial x_j}$. By a) and b) then $$\frac{DV}{dt}=\sum_j\frac{dv^j}{dt}X_j+\sum_j v^j\frac{DX_j}{dt}$$ by c) and i) $$\frac{DX_j}{dt}=\sum_i \frac{dx_i}{dt}\nabla_{X_i}X_j$$ Therefore, $$\frac{DV}{dt}=\sum_j \frac{dv^j}{dt}X_j+\sum_{i,j}\frac{dx_i}{dt}v^j\nabla_{X_i} X_j$$ The expression above shows us that if there is a correspondence satisfying the condition of proposition 2.2, then such a correspondence is unique. So my question is why does producing the formula in local coordinates implies uniqueness. I just don't get Do Carmo's reasoning here. Doesn't he have to check compatibility conditions with respect to other charts?",,"['differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
97,Complete Riemannian metric on ${\mathbb R}^2\setminus\{0\}$.,Complete Riemannian metric on .,{\mathbb R}^2\setminus\{0\},"It seems to me that the Riemannian metric $g_{ij}=\delta_{ij}/|x|^2$ on the punctured plane is complete, but I don't find a proof not involving explicit computations of the geodesic equation. Does anyone know one?","It seems to me that the Riemannian metric $g_{ij}=\delta_{ij}/|x|^2$ on the punctured plane is complete, but I don't find a proof not involving explicit computations of the geodesic equation. Does anyone know one?",,"['differential-geometry', 'riemannian-geometry']"
98,Symbol of the differential operator on vector bundles,Symbol of the differential operator on vector bundles,,"Suppose that we have a differential operator $D:C^{\infty}(\mathbb{R}^n) \to C^{\infty}(\mathbb{R}^n)$ of the form $(Df)(x)=\sum_{|\alpha| \leq k}a_{\alpha}(x)\frac{\partial^{|\alpha|}f}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}$ where we use standard multiindices notation. Here $a_{\alpha}$ are scalar valued functions. For such $D$ we can consider the expression coming from the highest order term in $D$, where we replace each partial derivative $\frac{\partial^{|\alpha}|}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}$ by $\xi^{\alpha}:=\xi_1^{\alpha_1}...\xi_n^{\alpha_n}$ where $\xi=(\xi_1,...,\xi_n) \in \mathbb{R}^n$ (note that some authors insert also factor $i$ in order to make fulfill some positivity conditions). This is rather standard and this construction may be generalized: the most general case is when everything takes place over some (say closed) manifold $M$ and two vector bundles $E,F$ over $M$. Then the symbol may be interpreted in the following way: let $T^*_0(M)$ be the cotangent bundle with the zero section deleted and $p:T^*_0M \to M$ be a canonical projection. One considers the pullback bundles $p^*(E),p^*(F)$ and symbol is defined as homomorphism $\sigma \in Hom(p^*(E),p^*(F))$ in the following way: for $(x,v) \in T^*_0(M)$ and $e \in E$ we pick $g \in C^{\infty}(M)$ and $s \in \Gamma^{\infty}(M,E)$ (a smooth section) such that $dg_x=v$ $g(x)=0$ and $s(x)=e$. Then we define $\sigma(D)(x,v)e=D(\frac{i^k}{k!}g^ks)(x)$. My question are then following: Why this is well defined? At least I can see that everything lands in the correct spaces and also all expressions makes sense: also it is clear for me that such choices are always possible. What is not clear for me, that this definition does not depend from the choices made. Moreover I really would like to understand How this definition relates to the most simple situation for example $M$ being euclidean space and $E,F$ be trivial bundles (but not necessarily of rank 1). I would appreciate any explanation or references where this is explained in detail: I really would like to learn this material in detail.","Suppose that we have a differential operator $D:C^{\infty}(\mathbb{R}^n) \to C^{\infty}(\mathbb{R}^n)$ of the form $(Df)(x)=\sum_{|\alpha| \leq k}a_{\alpha}(x)\frac{\partial^{|\alpha|}f}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}$ where we use standard multiindices notation. Here $a_{\alpha}$ are scalar valued functions. For such $D$ we can consider the expression coming from the highest order term in $D$, where we replace each partial derivative $\frac{\partial^{|\alpha}|}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}$ by $\xi^{\alpha}:=\xi_1^{\alpha_1}...\xi_n^{\alpha_n}$ where $\xi=(\xi_1,...,\xi_n) \in \mathbb{R}^n$ (note that some authors insert also factor $i$ in order to make fulfill some positivity conditions). This is rather standard and this construction may be generalized: the most general case is when everything takes place over some (say closed) manifold $M$ and two vector bundles $E,F$ over $M$. Then the symbol may be interpreted in the following way: let $T^*_0(M)$ be the cotangent bundle with the zero section deleted and $p:T^*_0M \to M$ be a canonical projection. One considers the pullback bundles $p^*(E),p^*(F)$ and symbol is defined as homomorphism $\sigma \in Hom(p^*(E),p^*(F))$ in the following way: for $(x,v) \in T^*_0(M)$ and $e \in E$ we pick $g \in C^{\infty}(M)$ and $s \in \Gamma^{\infty}(M,E)$ (a smooth section) such that $dg_x=v$ $g(x)=0$ and $s(x)=e$. Then we define $\sigma(D)(x,v)e=D(\frac{i^k}{k!}g^ks)(x)$. My question are then following: Why this is well defined? At least I can see that everything lands in the correct spaces and also all expressions makes sense: also it is clear for me that such choices are always possible. What is not clear for me, that this definition does not depend from the choices made. Moreover I really would like to understand How this definition relates to the most simple situation for example $M$ being euclidean space and $E,F$ be trivial bundles (but not necessarily of rank 1). I would appreciate any explanation or references where this is explained in detail: I really would like to learn this material in detail.",,"['differential-geometry', 'vector-bundles']"
99,Topology of statistical manifolds,Topology of statistical manifolds,,"I am currently working with statistical manifolds. Roughly, a statistical manifold is a set of distribution parametrized by a set of parameters. However i have trouble finding more precise definition. In order to be a manifold, a set is supposed to be locally homeomorphic to $\mathbb{R}^n$. The word homeomorphic assume that the set of distributions is initially equipped with a topology. My question is the following: what is the initial topology on the set of distributions when one speaks of statistical manifolds? Thanks","I am currently working with statistical manifolds. Roughly, a statistical manifold is a set of distribution parametrized by a set of parameters. However i have trouble finding more precise definition. In order to be a manifold, a set is supposed to be locally homeomorphic to $\mathbb{R}^n$. The word homeomorphic assume that the set of distributions is initially equipped with a topology. My question is the following: what is the initial topology on the set of distributions when one speaks of statistical manifolds? Thanks",,"['differential-geometry', 'probability-distributions']"
