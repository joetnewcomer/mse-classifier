,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Counterexample to the double integral computational theorem when the double integral existence assumption is dropped?,Counterexample to the double integral computational theorem when the double integral existence assumption is dropped?,,"To make things simple, consider the simplest case of the double integral computational theorem. Throughout any phrase involving integrability is in the Riemannian sense. Let $[a, b], [c, d] \subset \mathbb{R},$ let $Q := [a, b] \times [c, d],$ and let $f: Q \to \mathbb{R}$ be bounded. If $\underset{Q}{\iint} f$ exists, and if $\int_{a}^{b} f(x, y) dx$ exists for every $y \in [c, d]$ and is integrable on $[c, d],$ then  $$\underset{Q}{\iint} f = \int_{c}^{d}\int_{a}^{b} f(x, y) dx dy.$$ I just noticed a thing that I hardly ever noticed, that is, what is a counterexample to this theorem if we drop the assumption of the existence of the integral of $f$ over $Q$?  It is intuitively plausible that we MAY just compute separately the one-dimensional integrals and then conclude that, yes, the double integral also exists?","To make things simple, consider the simplest case of the double integral computational theorem. Throughout any phrase involving integrability is in the Riemannian sense. Let $[a, b], [c, d] \subset \mathbb{R},$ let $Q := [a, b] \times [c, d],$ and let $f: Q \to \mathbb{R}$ be bounded. If $\underset{Q}{\iint} f$ exists, and if $\int_{a}^{b} f(x, y) dx$ exists for every $y \in [c, d]$ and is integrable on $[c, d],$ then  $$\underset{Q}{\iint} f = \int_{c}^{d}\int_{a}^{b} f(x, y) dx dy.$$ I just noticed a thing that I hardly ever noticed, that is, what is a counterexample to this theorem if we drop the assumption of the existence of the integral of $f$ over $Q$?  It is intuitively plausible that we MAY just compute separately the one-dimensional integrals and then conclude that, yes, the double integral also exists?",,"['real-analysis', 'multivariable-calculus']"
1,Plot the level curve of sine function in multiple variables,Plot the level curve of sine function in multiple variables,,"I'm very confused about how I could go about this, as it seems that the question cannot be done using only the information given. The question is: plot the level curve for $f(x,y) = \sin(k^2x^2 + y^2) =$ $\frac{1}{\sqrt{2}}$ for some (unknown) fixed $k \in \mathbb{R}$. I cannot see how you could plot this without knowing the value of $k$. This was an exam question and the paper asked for it to be drawn during the exam today. However, can you actually draw it when you don't have a value of $k$ to go by? For example, when I replace $k$ with $3$, Wolfram Alpha gives this: http://bit.ly/1Knjdo1 . Would this be realistic to sketch on paper during an exam? If someone could help explain this, that would help greatly, thanks.","I'm very confused about how I could go about this, as it seems that the question cannot be done using only the information given. The question is: plot the level curve for $f(x,y) = \sin(k^2x^2 + y^2) =$ $\frac{1}{\sqrt{2}}$ for some (unknown) fixed $k \in \mathbb{R}$. I cannot see how you could plot this without knowing the value of $k$. This was an exam question and the paper asked for it to be drawn during the exam today. However, can you actually draw it when you don't have a value of $k$ to go by? For example, when I replace $k$ with $3$, Wolfram Alpha gives this: http://bit.ly/1Knjdo1 . Would this be realistic to sketch on paper during an exam? If someone could help explain this, that would help greatly, thanks.",,['multivariable-calculus']
2,Are partial derivatives a special case of the total derivative or just something else entirely?,Are partial derivatives a special case of the total derivative or just something else entirely?,,I can do basic multivariable calculations using partial and total derivatives. I also know for partial derivatives the existence of all partial derivatives at a point doesn't imply continuity. Are partial derivatives a special case of the total derivative or just something else entirely? Can someone compare / contrast them?,I can do basic multivariable calculations using partial and total derivatives. I also know for partial derivatives the existence of all partial derivatives at a point doesn't imply continuity. Are partial derivatives a special case of the total derivative or just something else entirely? Can someone compare / contrast them?,,['multivariable-calculus']
3,Compute flux of vector field F through hemisphere,Compute flux of vector field F through hemisphere,,"I need help solving this question from my textbook. Compute the flux of the vector field: $$\vec F = 4xz\vec i + 2 y\vec k$$ through the surface $S$, which is the hemisphere: $x^2 + y^2 + z^2 = 9 ,  z \geq 0$ oriented upward. How do I continue? Which theorem do I need to solve this problem?","I need help solving this question from my textbook. Compute the flux of the vector field: $$\vec F = 4xz\vec i + 2 y\vec k$$ through the surface $S$, which is the hemisphere: $x^2 + y^2 + z^2 = 9 ,  z \geq 0$ oriented upward. How do I continue? Which theorem do I need to solve this problem?",,"['calculus', 'multivariable-calculus']"
4,Proving Hadamard lemma: how to apply FTC in first step?,Proving Hadamard lemma: how to apply FTC in first step?,,"I wanted to prove Hadamard's lemma but got stuck on the first step: Let $f \in C^\infty (\mathbb R^n)$ and $x_0 \in \mathbb R^n$. Then there exist $g_i \in C^\infty (\mathbb R^n)$ such that $$ f(x) = f(x_0) + \sum_{i =1}^n g_i(x) (x-x_0)$$ on some neighbourhood of $x_0$. Proof: I want to apply the Fundamental Theorem of Calculus but in $n$-dimensions. Something like this: $$ f(x) = \color{red}{f(x_0) + \int_{x_0}^x {d \over dt} f(t) dt} = f(x_0) + \int_{0}^1 {d \over dt} f(t(x-x_0) + x_0)  dt $$ except, the part in $\color{red}{red}$ s not quite right yet (as $t$ is a scalar variable). The reason why I don't know how to fix it is: it should probably be a sum of partial derivatives but then I'd have the same sum in the next equality which is not the case: the last expression in the above display style equation seems fine as it is. So my question is: What is the correct expression (between $f(x_0)$   and $f(x_0) + \int_{0}^1 {d \over dt} f(t(x-x_0) + x_0)  dt$ )that I   should obtain after applying the FTC?","I wanted to prove Hadamard's lemma but got stuck on the first step: Let $f \in C^\infty (\mathbb R^n)$ and $x_0 \in \mathbb R^n$. Then there exist $g_i \in C^\infty (\mathbb R^n)$ such that $$ f(x) = f(x_0) + \sum_{i =1}^n g_i(x) (x-x_0)$$ on some neighbourhood of $x_0$. Proof: I want to apply the Fundamental Theorem of Calculus but in $n$-dimensions. Something like this: $$ f(x) = \color{red}{f(x_0) + \int_{x_0}^x {d \over dt} f(t) dt} = f(x_0) + \int_{0}^1 {d \over dt} f(t(x-x_0) + x_0)  dt $$ except, the part in $\color{red}{red}$ s not quite right yet (as $t$ is a scalar variable). The reason why I don't know how to fix it is: it should probably be a sum of partial derivatives but then I'd have the same sum in the next equality which is not the case: the last expression in the above display style equation seems fine as it is. So my question is: What is the correct expression (between $f(x_0)$   and $f(x_0) + \int_{0}^1 {d \over dt} f(t(x-x_0) + x_0)  dt$ )that I   should obtain after applying the FTC?",,"['multivariable-calculus', 'differential-geometry']"
5,Partial derivatives of $\ln(x^2+y^2)$,Partial derivatives of,\ln(x^2+y^2),"I am new to partial derivatives and they seem pretty easy, but I am having trouble with this one: $$\frac{\partial}{\partial x} \ln(x^2+y^2)$$ now if this was just $\frac{d}{dx}\ln(x^2)$ we would get $\frac{2x}{x^2}$. So I feel we would get:$$\frac{\partial}{\partial x} \ln(x^2+y^2)=\frac{2x}{x^2+y^2}$$ and with respect to $y$ $$\frac{\partial}{\partial y} \ln(x^2+y^2)=\frac{2y}{x^2+y^2}.$$  Is that right?","I am new to partial derivatives and they seem pretty easy, but I am having trouble with this one: $$\frac{\partial}{\partial x} \ln(x^2+y^2)$$ now if this was just $\frac{d}{dx}\ln(x^2)$ we would get $\frac{2x}{x^2}$. So I feel we would get:$$\frac{\partial}{\partial x} \ln(x^2+y^2)=\frac{2x}{x^2+y^2}$$ and with respect to $y$ $$\frac{\partial}{\partial y} \ln(x^2+y^2)=\frac{2y}{x^2+y^2}.$$  Is that right?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
6,How can I go about solving this group of equations in as simple a way as possible?,How can I go about solving this group of equations in as simple a way as possible?,,"They arise from partial derivatives of the Lagrange multiplier function. Here below is the original problem: Goal function:    $$f(x,y,z)=\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2} $$   with two constraints:    $$ \begin{cases} x^2+y^2+z^2-1=0 \\ Ax+By+Cz=0  \end{cases} $$   provided that $a>b>c>0$ and $A^2+B^2+C^2=1$, determine all the possible extremums. My Lagrange function is  $$L(x,y,z,\lambda,\mu):=f(x,y,z)+\lambda (x^2+y^2+z^2-1)+\mu (Ax+By+Cz)$$ From which naturally arises $$ \begin{array}{c} \partial L/\partial x =&2x/a^2+2\lambda x+A\mu=0 \\  \partial L/\partial y =&2y/b^2+2\lambda y+B\mu=0 \\  \partial L/\partial z =&2z/c^2+2\lambda z+C\mu=0 \\ \partial L/\partial \lambda =&x^2+y^2+z^2-1=0 \\ \partial L/\partial \mu =&Ax+By+Cz=0 \end{array} $$ ( Here I want to tag them $(1)$ to $(5)$ from top to bottom, but don't know how to write the Jax code. Is it possible to insert two or more tags into one array made up of several equations and can you help me with this?) I made some failed attempts to solve them. Every time I failed because I simply didn't know how to take off the coefficients leading $x,y,z$ terms. They might be zero, which really messed things up! For example, in $(1)$ I got $2x(\frac{1}{a^2}+\lambda)+A\mu=0$ but I don't know whether $(\frac{1}{a^2}+\lambda)$ is zero or not. And for $(1)$ to $(3)$, well, I got three uncertain cases. To make things worse, I also could not determine whether $A,B,C$ or $\mu$ is zero or not.... Too many possibilities! which really freaked me out.. So, except the case-by-case discussion about the $A,B,C$ and $\lambda,\mu$, is there a possible way (or trick) to find all the possible solutions for $(x,y,z)$ without much pain? I really need some enlightening. Any useful hint or help that sheds a light on this problem will be appreciated, thanks in advance!","They arise from partial derivatives of the Lagrange multiplier function. Here below is the original problem: Goal function:    $$f(x,y,z)=\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2} $$   with two constraints:    $$ \begin{cases} x^2+y^2+z^2-1=0 \\ Ax+By+Cz=0  \end{cases} $$   provided that $a>b>c>0$ and $A^2+B^2+C^2=1$, determine all the possible extremums. My Lagrange function is  $$L(x,y,z,\lambda,\mu):=f(x,y,z)+\lambda (x^2+y^2+z^2-1)+\mu (Ax+By+Cz)$$ From which naturally arises $$ \begin{array}{c} \partial L/\partial x =&2x/a^2+2\lambda x+A\mu=0 \\  \partial L/\partial y =&2y/b^2+2\lambda y+B\mu=0 \\  \partial L/\partial z =&2z/c^2+2\lambda z+C\mu=0 \\ \partial L/\partial \lambda =&x^2+y^2+z^2-1=0 \\ \partial L/\partial \mu =&Ax+By+Cz=0 \end{array} $$ ( Here I want to tag them $(1)$ to $(5)$ from top to bottom, but don't know how to write the Jax code. Is it possible to insert two or more tags into one array made up of several equations and can you help me with this?) I made some failed attempts to solve them. Every time I failed because I simply didn't know how to take off the coefficients leading $x,y,z$ terms. They might be zero, which really messed things up! For example, in $(1)$ I got $2x(\frac{1}{a^2}+\lambda)+A\mu=0$ but I don't know whether $(\frac{1}{a^2}+\lambda)$ is zero or not. And for $(1)$ to $(3)$, well, I got three uncertain cases. To make things worse, I also could not determine whether $A,B,C$ or $\mu$ is zero or not.... Too many possibilities! which really freaked me out.. So, except the case-by-case discussion about the $A,B,C$ and $\lambda,\mu$, is there a possible way (or trick) to find all the possible solutions for $(x,y,z)$ without much pain? I really need some enlightening. Any useful hint or help that sheds a light on this problem will be appreciated, thanks in advance!",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
7,Proof of Hamilton's equation from integral invariant,Proof of Hamilton's equation from integral invariant,,"This is from pages 273 - 274 0f Whittaker's book of analytical dynamics. Its in the public domain. Let $q_1,q_2,\ldots,q_N$ be functions of time. And let $p_1,p_2,\ldots,p_N$ also be functions of time. And they are determined by the following differential equation: $$\eqalign{ \dot{q_k} &= Q_k \cr \dot{p_k} &= P_k }$$ Where the $Q's$ and $P's$ are functions on the $q's$ and $p's$ but not on time. And suppose they satisfy the following: $$\eqalign{ \frac{\partial Q_i}{\partial q_k} &= -\frac{\partial P_k}{\partial p_i} \cr \frac{\partial P_i}{\partial q_k} &= \frac{\partial P_k}{\partial q_i} \cr \frac{\partial Q_i}{\partial p_k} &= \frac{\partial Q_k}{\partial p_i} }$$ Then Whittaker claims that this implies the existance of a function $H$ for which $Q_k = \partial H /\partial p_k$ and $P_k=-\partial H/\partial q_k$. He states this without proof, but I don't see it as obvious. Can you explain? For more context, Whittaker is trying to prove Hamilton's equation from Poincare's integral invariant.","This is from pages 273 - 274 0f Whittaker's book of analytical dynamics. Its in the public domain. Let $q_1,q_2,\ldots,q_N$ be functions of time. And let $p_1,p_2,\ldots,p_N$ also be functions of time. And they are determined by the following differential equation: $$\eqalign{ \dot{q_k} &= Q_k \cr \dot{p_k} &= P_k }$$ Where the $Q's$ and $P's$ are functions on the $q's$ and $p's$ but not on time. And suppose they satisfy the following: $$\eqalign{ \frac{\partial Q_i}{\partial q_k} &= -\frac{\partial P_k}{\partial p_i} \cr \frac{\partial P_i}{\partial q_k} &= \frac{\partial P_k}{\partial q_i} \cr \frac{\partial Q_i}{\partial p_k} &= \frac{\partial Q_k}{\partial p_i} }$$ Then Whittaker claims that this implies the existance of a function $H$ for which $Q_k = \partial H /\partial p_k$ and $P_k=-\partial H/\partial q_k$. He states this without proof, but I don't see it as obvious. Can you explain? For more context, Whittaker is trying to prove Hamilton's equation from Poincare's integral invariant.",,"['ordinary-differential-equations', 'multivariable-calculus', 'classical-mechanics']"
8,Order of differentiaton for multivariable functions with arbitrary dependence of variables,Order of differentiaton for multivariable functions with arbitrary dependence of variables,,"While studying Neural Networks, I was bogged with a nasty problem, for which I did not find a satisfying answer using my mathematical knowledge. Let's assume we have a complex multivariable function, which is directly or indirectly dependent on various variables; such that we can draw an arbitrary directed acyclic graph using the variables which affect the function. To visualize the situation, let's consider the following example diagram: Here $F$ is a function of $F(X,Y,Z)$, $X$ is a function of $X(B,D)$, $Y$ is a function of $Y(C)$ and $Z$ is a function of $Z(X)$; in general, each variable is dependent on its parents directly. Much more simple tree diagrams are common in resources about multivariable chain rule and I am familiar with them. But I cannot justify the following problem: Assume that, in such a hierarchy of variables, where $F$ is the final function which is being evaluated (which is nicely smooth), we want to take the total derivative of $F$ with respect to a variable in the hierarchy; let's name this variable $Q$. By the total derivative, I mean that I am going to perturb $Q$ and look how $F$ is going to change, without holding other variables fixed, in contrast to a partial derivative. So, we are going to evaluate $\dfrac{dF}{dQ}$. I can show by induction that this is equal to $\sum_{i}\dfrac{dF}{dP_i}\dfrac{\partial P_i}{\partial Q}$ where the sum runs on the variables which directly depends on $Q$ and where I evaluate the total derivative with respect to such variables. Let's assume we have another variable, say $W$, in this hierarchy. For regular partial derivatives, the order of differentiation does not matter. I want to learn that whether this holds for this case as well, where we have $\dfrac{d^2F}{dWdQ}$ and $\dfrac{d^2F}{dQdW}$. If I take the total derivative twice, in different orders, will I obtain the same result? I tried to show this by using induction and succeeded in some simple examples, but I cannot generalize this for an arbitrary hierarchy of variables.","While studying Neural Networks, I was bogged with a nasty problem, for which I did not find a satisfying answer using my mathematical knowledge. Let's assume we have a complex multivariable function, which is directly or indirectly dependent on various variables; such that we can draw an arbitrary directed acyclic graph using the variables which affect the function. To visualize the situation, let's consider the following example diagram: Here $F$ is a function of $F(X,Y,Z)$, $X$ is a function of $X(B,D)$, $Y$ is a function of $Y(C)$ and $Z$ is a function of $Z(X)$; in general, each variable is dependent on its parents directly. Much more simple tree diagrams are common in resources about multivariable chain rule and I am familiar with them. But I cannot justify the following problem: Assume that, in such a hierarchy of variables, where $F$ is the final function which is being evaluated (which is nicely smooth), we want to take the total derivative of $F$ with respect to a variable in the hierarchy; let's name this variable $Q$. By the total derivative, I mean that I am going to perturb $Q$ and look how $F$ is going to change, without holding other variables fixed, in contrast to a partial derivative. So, we are going to evaluate $\dfrac{dF}{dQ}$. I can show by induction that this is equal to $\sum_{i}\dfrac{dF}{dP_i}\dfrac{\partial P_i}{\partial Q}$ where the sum runs on the variables which directly depends on $Q$ and where I evaluate the total derivative with respect to such variables. Let's assume we have another variable, say $W$, in this hierarchy. For regular partial derivatives, the order of differentiation does not matter. I want to learn that whether this holds for this case as well, where we have $\dfrac{d^2F}{dWdQ}$ and $\dfrac{d^2F}{dQdW}$. If I take the total derivative twice, in different orders, will I obtain the same result? I tried to show this by using induction and succeeded in some simple examples, but I cannot generalize this for an arbitrary hierarchy of variables.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
9,Deducing a Taylor expansion in an arbitrary point from a MacLauren polynomial,Deducing a Taylor expansion in an arbitrary point from a MacLauren polynomial,,"I have a function $f(x,y)=-2x^3 + 4y^3 +4xy+4x$ and I need to find a Taylor expansion, around the point $(-4,1)$ of this function. Since the function is actually a polynomial, I know this representation is also its Taylor expansion, but around $(0,0)$ . Is there any way to calculate the Taylor series around $(-4,1)$ without having to calculate all the derivatives ? (i.e.- only by using the known form of the function ) Hope I made myself clear Thanks in advance","I have a function $f(x,y)=-2x^3 + 4y^3 +4xy+4x$ and I need to find a Taylor expansion, around the point $(-4,1)$ of this function. Since the function is actually a polynomial, I know this representation is also its Taylor expansion, but around $(0,0)$ . Is there any way to calculate the Taylor series around $(-4,1)$ without having to calculate all the derivatives ? (i.e.- only by using the known form of the function ) Hope I made myself clear Thanks in advance",,"['calculus', 'multivariable-calculus']"
10,How to determine the limit of $f(\mathbf{x}) = \frac{2x^6y}{x^8+y^4+5x^4y^2}$ as $\mathbf{x} \rightarrow \mathbf{0}$?,How to determine the limit of  as ?,f(\mathbf{x}) = \frac{2x^6y}{x^8+y^4+5x^4y^2} \mathbf{x} \rightarrow \mathbf{0},"How do I determine the limit of $f(\mathbf{x}) = \frac{2x^6y}{x^8+y^4+5x^4y^2}$ as $\mathbf{x} \rightarrow \mathbf{0}$, where $f:\mathbb{R^2}-\{(0,0)\}\rightarrow \mathbb{R}$? Following the suggestion I received in this answer, I tried to convert the function into polar coordinates, and this is what I got: $$ f(r,\theta) = \frac{r^32\cos^6(\theta)\sin(\theta)}{r^4\cos^8(\theta)+\sin^4(\theta)+5r^2\cos^4(\theta)\sin^2(\theta)}$$ I am not aware of how the above expression can be further simplified further. What should I do next? Later on the question also asks if $f(\mathbf{x})y$ has a limit as $\mathbf{x}\rightarrow 0$. Please advise on the method that I should use to solve this kind of problem. EDIT: The solution given is: $f(\mathbf{x})$ has no limit as $\mathbf{x}\rightarrow \mathbf{0}$ the limit of $f(\mathbf{x})y$ is $0$ as  $\mathbf{x}\rightarrow \mathbf{0}$ The solution simply lists the steps to show why the above is true. It does not discuss the approach. In fact, it starts by saying some experimentation is usually required and did not elaborate further. As such, if I was faced with a similar question in the future, I doubt that I will be able to make progress. Therefore, how should I approach this sort of question i.e. to determine the limit of a two variable rational function as it tends to some point. What observations should I make to determine the right course of action? Even if there is no set way, what heuristics can I follow to help arrive at the answer? I've not seen any text discuss a strategy for these kind of questions, so I would also appreciate it if someone can direct me to an online resource (if available). If it helps to narrow the scope of the answer, I am doing an introductory real analysis course.","How do I determine the limit of $f(\mathbf{x}) = \frac{2x^6y}{x^8+y^4+5x^4y^2}$ as $\mathbf{x} \rightarrow \mathbf{0}$, where $f:\mathbb{R^2}-\{(0,0)\}\rightarrow \mathbb{R}$? Following the suggestion I received in this answer, I tried to convert the function into polar coordinates, and this is what I got: $$ f(r,\theta) = \frac{r^32\cos^6(\theta)\sin(\theta)}{r^4\cos^8(\theta)+\sin^4(\theta)+5r^2\cos^4(\theta)\sin^2(\theta)}$$ I am not aware of how the above expression can be further simplified further. What should I do next? Later on the question also asks if $f(\mathbf{x})y$ has a limit as $\mathbf{x}\rightarrow 0$. Please advise on the method that I should use to solve this kind of problem. EDIT: The solution given is: $f(\mathbf{x})$ has no limit as $\mathbf{x}\rightarrow \mathbf{0}$ the limit of $f(\mathbf{x})y$ is $0$ as  $\mathbf{x}\rightarrow \mathbf{0}$ The solution simply lists the steps to show why the above is true. It does not discuss the approach. In fact, it starts by saying some experimentation is usually required and did not elaborate further. As such, if I was faced with a similar question in the future, I doubt that I will be able to make progress. Therefore, how should I approach this sort of question i.e. to determine the limit of a two variable rational function as it tends to some point. What observations should I make to determine the right course of action? Even if there is no set way, what heuristics can I follow to help arrive at the answer? I've not seen any text discuss a strategy for these kind of questions, so I would also appreciate it if someone can direct me to an online resource (if available). If it helps to narrow the scope of the answer, I am doing an introductory real analysis course.",,"['calculus', 'limits', 'multivariable-calculus']"
11,Integral of $\int \int_{D} \frac{dxdy}{(4x^2+y^2)^\frac{3}{2}}$,Integral of,\int \int_{D} \frac{dxdy}{(4x^2+y^2)^\frac{3}{2}},"I have to calculate  $$\int \int_{D} \frac{dxdy}{(4x^2+y^2)^\frac{3}{2}}$$ where $D = \{(x,y)| 1\leq x \leq 2, |y|\leq \frac{x}{2} \}$ I tried to use polar coordinates but I can't find the limits of integration. Any hint?","I have to calculate  $$\int \int_{D} \frac{dxdy}{(4x^2+y^2)^\frac{3}{2}}$$ where $D = \{(x,y)| 1\leq x \leq 2, |y|\leq \frac{x}{2} \}$ I tried to use polar coordinates but I can't find the limits of integration. Any hint?",,"['integration', 'multivariable-calculus']"
12,Mass and center of mass using double integrals,Mass and center of mass using double integrals,,"Disclaimer: This was given as a homework from college but the teacher didn't teach us anything about density or mass or anything related. A lamina has the form of the region limited by the parabola $ y = x^2 $ and the straight line $ y = x $. The density varies as the distance from the $ X $ axis. Find the mass and center of mass. what i could find however is that the formula of mass is the following $$M = \int\int_R \rho(x,y)dA $$ so i tried doing something like this $$ \int_0^1\int_y^{\sqrt(y)} ? dxdy $$ the thing is that they say the density varies as the distance from the x axis, so i don't know what to replace for the density.. is it $ x + y $?","Disclaimer: This was given as a homework from college but the teacher didn't teach us anything about density or mass or anything related. A lamina has the form of the region limited by the parabola $ y = x^2 $ and the straight line $ y = x $. The density varies as the distance from the $ X $ axis. Find the mass and center of mass. what i could find however is that the formula of mass is the following $$M = \int\int_R \rho(x,y)dA $$ so i tried doing something like this $$ \int_0^1\int_y^{\sqrt(y)} ? dxdy $$ the thing is that they say the density varies as the distance from the x axis, so i don't know what to replace for the density.. is it $ x + y $?",,"['calculus', 'integration', 'multivariable-calculus']"
13,Finding the bounds of a solid for triple integrals,Finding the bounds of a solid for triple integrals,,"Ok, so I have an answer, most likely the wrong one. The question being asked is: Using polar coordinates find the volume of the solid bounded below by the $xy–plane$ and above by the surface $x^2 +y^2 +z^6 =5$. First off I found the bounds for $x,y$ and $z$. These are $x \space [0,\sqrt{5}], \space y \space[0, \sqrt{5}],  \space z \space[0, 5^{\frac{1}{6}}]$ I use these bounds and integrate the function $x^2 +y^2 +z^6 =5$ I realise this is not right, but I am unsure why? Could someone explain this to me. I know I have to write the bounds in terms of the unused variables, otherwise it wouldn't ask for polar coordinates. The best I could come up with was this: $ y \space [0, \sqrt{5}], \space x \space[0, \sqrt{5-y^2-z^6}], \space z \space [0, (5-y^2-x^2)^{\frac{1}{6}} ]$ Afterwards we multiply the integral by 4. I have 2 bounds with 2 variables and 1 bound with none. I know this can't be right, we should have 3,2,1. But I am having trouble figuring out what exactly the bounds should be. If someone could explain to me how I can find these bounds so I could use them for general questions that would be great!. I'm also pretty sure I can integrate by converting to polar coordinates, I am just having trouble with the bounds, that is all. Thanks for any help !","Ok, so I have an answer, most likely the wrong one. The question being asked is: Using polar coordinates find the volume of the solid bounded below by the $xy–plane$ and above by the surface $x^2 +y^2 +z^6 =5$. First off I found the bounds for $x,y$ and $z$. These are $x \space [0,\sqrt{5}], \space y \space[0, \sqrt{5}],  \space z \space[0, 5^{\frac{1}{6}}]$ I use these bounds and integrate the function $x^2 +y^2 +z^6 =5$ I realise this is not right, but I am unsure why? Could someone explain this to me. I know I have to write the bounds in terms of the unused variables, otherwise it wouldn't ask for polar coordinates. The best I could come up with was this: $ y \space [0, \sqrt{5}], \space x \space[0, \sqrt{5-y^2-z^6}], \space z \space [0, (5-y^2-x^2)^{\frac{1}{6}} ]$ Afterwards we multiply the integral by 4. I have 2 bounds with 2 variables and 1 bound with none. I know this can't be right, we should have 3,2,1. But I am having trouble figuring out what exactly the bounds should be. If someone could explain to me how I can find these bounds so I could use them for general questions that would be great!. I'm also pretty sure I can integrate by converting to polar coordinates, I am just having trouble with the bounds, that is all. Thanks for any help !",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
14,Where did I got wrong with this surface integral,Where did I got wrong with this surface integral,,"It appears that I don't quite have surface integrals like I thought I did.  The following is a problem from the back of the book (not homework because it wasn't prescribed but I'm working it to learn).  Here's the problem: Integrate $G(x,y,z) = x + y + z$ over the surface cut from the first octant and the plane $2x + 2y + z = 2$.  This tells me that $S = F(x,y,z) = 2x + 2y + z = 2$ is implicitly given.  Therefore, my integrating formula is $\int \int_S G(x,y,z) d\sigma = \int \int_R G(x,y,z)\frac{\left| \nabla \mathbf{F} \right|}{\left| \nabla \mathbf{F} \cdot \mathbf{p} \right|}$. For this problem, I have $\nabla \mathbf{F} = 2\mathbf{i} + 2 \mathbf{j} + \mathbf{k}$ and since the Region, $R$, I'm projecting onto is in the $xy$ plane, I'm using $\mathbf{p} = \mathbf{k}$.  Thus, $\nabla \mathbf{F} \cdot \mathbf{p} = 1$.  Also, since this surface is given implicitly, I have $z = 2 - 2x - 2y$. Therefore, I have setting things up with $0 \le x \le 1$ and $0 \le x \le 1$: $$ \begin{array}{rcl} \int \int_R G(x,y,z)\frac{\left| \nabla \mathbf{F} \right|}{\left| \nabla \mathbf{F} \cdot \mathbf{p} \right|} & = & \int_0^1 \int_0^1 x + y + (2 - 2x - 2y)(\frac{\sqrt(3)}{1})dxdy \\  & = & 3 \int_0^1 \int_0^1 2-2x-2y dxdy \\  & = & 3 \end{array} $$ However, my book tells me the answer is 2.","It appears that I don't quite have surface integrals like I thought I did.  The following is a problem from the back of the book (not homework because it wasn't prescribed but I'm working it to learn).  Here's the problem: Integrate $G(x,y,z) = x + y + z$ over the surface cut from the first octant and the plane $2x + 2y + z = 2$.  This tells me that $S = F(x,y,z) = 2x + 2y + z = 2$ is implicitly given.  Therefore, my integrating formula is $\int \int_S G(x,y,z) d\sigma = \int \int_R G(x,y,z)\frac{\left| \nabla \mathbf{F} \right|}{\left| \nabla \mathbf{F} \cdot \mathbf{p} \right|}$. For this problem, I have $\nabla \mathbf{F} = 2\mathbf{i} + 2 \mathbf{j} + \mathbf{k}$ and since the Region, $R$, I'm projecting onto is in the $xy$ plane, I'm using $\mathbf{p} = \mathbf{k}$.  Thus, $\nabla \mathbf{F} \cdot \mathbf{p} = 1$.  Also, since this surface is given implicitly, I have $z = 2 - 2x - 2y$. Therefore, I have setting things up with $0 \le x \le 1$ and $0 \le x \le 1$: $$ \begin{array}{rcl} \int \int_R G(x,y,z)\frac{\left| \nabla \mathbf{F} \right|}{\left| \nabla \mathbf{F} \cdot \mathbf{p} \right|} & = & \int_0^1 \int_0^1 x + y + (2 - 2x - 2y)(\frac{\sqrt(3)}{1})dxdy \\  & = & 3 \int_0^1 \int_0^1 2-2x-2y dxdy \\  & = & 3 \end{array} $$ However, my book tells me the answer is 2.",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
15,Fundamental theorem of calculus in multivariable calculus,Fundamental theorem of calculus in multivariable calculus,,"I'm not sure if this is the right name for it but with the theorem: Let $f:\sigma \rightarrow \mathbb{R} $ be a smooth scalar field and assume $r: [a,b] \rightarrow \mathbb {R}^n$ is a piecewise parametrisation of a path $C$ whose image is included in $\sigma $. Then $$\int \limits_C (\nabla f) \cdot dr =f(r(b))-f(r(a))$$ Can someone give some examples of when this theorem can and cannot be used please. Or is it the case of when you can or cannot easily find a scalar field s.t. the gradient of the scalar field is the vector field.","I'm not sure if this is the right name for it but with the theorem: Let $f:\sigma \rightarrow \mathbb{R} $ be a smooth scalar field and assume $r: [a,b] \rightarrow \mathbb {R}^n$ is a piecewise parametrisation of a path $C$ whose image is included in $\sigma $. Then $$\int \limits_C (\nabla f) \cdot dr =f(r(b))-f(r(a))$$ Can someone give some examples of when this theorem can and cannot be used please. Or is it the case of when you can or cannot easily find a scalar field s.t. the gradient of the scalar field is the vector field.",,['multivariable-calculus']
16,Need help solving complicated integral $\oint_{\mathcal C}\begin{pmatrix}x_2^2 \cos x_1 \\ 2x_2(1+\sin x_1)\end{pmatrix} dx$,Need help solving complicated integral,\oint_{\mathcal C}\begin{pmatrix}x_2^2 \cos x_1 \\ 2x_2(1+\sin x_1)\end{pmatrix} dx,"Let $\mathcal C$ be the curve that traces the unit circle once (counterclockwise) in $\mathbb R^2$. The starting- and endpoint is (1,0). I need to figure out a parameterization for $\mathcal C$ and calculate the following integral. $\oint_{\mathcal C}\begin{pmatrix}x_2^2 \cos x_1 \\ 2x_2(1+\sin x_1)\end{pmatrix}  dx$ Parameterization: $t\rightarrow(\cos t, \sin t), t\in[0,2\pi]$ $\vec{x}(t)=\begin{pmatrix}\cos t \\ \sin t\end{pmatrix}$; $\space \space \frac{\partial \vec{x}(t)}{\partial t} =\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix}$ $\iff d \vec{x}=\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix} d t$ However, this gives me a really complicated term that I can't simplify properly. $\oint_{0}^{2\pi} \begin{pmatrix}\sin^2 t\cdot \cos(\cos t) \\ 2 \sin t (1+\sin(\cos t)) t\end{pmatrix}\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix} dt$ Any idea how to solve this?","Let $\mathcal C$ be the curve that traces the unit circle once (counterclockwise) in $\mathbb R^2$. The starting- and endpoint is (1,0). I need to figure out a parameterization for $\mathcal C$ and calculate the following integral. $\oint_{\mathcal C}\begin{pmatrix}x_2^2 \cos x_1 \\ 2x_2(1+\sin x_1)\end{pmatrix}  dx$ Parameterization: $t\rightarrow(\cos t, \sin t), t\in[0,2\pi]$ $\vec{x}(t)=\begin{pmatrix}\cos t \\ \sin t\end{pmatrix}$; $\space \space \frac{\partial \vec{x}(t)}{\partial t} =\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix}$ $\iff d \vec{x}=\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix} d t$ However, this gives me a really complicated term that I can't simplify properly. $\oint_{0}^{2\pi} \begin{pmatrix}\sin^2 t\cdot \cos(\cos t) \\ 2 \sin t (1+\sin(\cos t)) t\end{pmatrix}\begin{pmatrix}-\sin t \\ \cos t\end{pmatrix} dt$ Any idea how to solve this?",,"['integration', 'multivariable-calculus']"
17,Multivariable Calculus: Line Integrals (Directed Curve),Multivariable Calculus: Line Integrals (Directed Curve),,"I have this math problem, that I got a bit confused on. I just need to know whether or not I did it correctly. Thanks! Question: Calculate $\oint_c xe^{z}dx+yzdy+xe^{y}dz$ over the directed curve $C$   that is parameterized by $r(t) = t^2i+t^3j+t^4k$, $0\leq t \leq1$. Work: $r(t)=<t^2, t^3, t^4>$ $r'(t)=<2t, 3t^2, 4t^3>$ $x=t^2, y=t^3, z=t^4$ $dx=2t, dy=3t^2, dz=4t^3$ I plug into the equation: $\oint_c xe^{z}dx+yzdy+xe^{y}dz$ $\int_0^1 (t^2)e^{(t^4)}(2t)+(t^3)(t^4)(3t^2)+(t^2)e^{t^3}(4t^3) dt$ = $\int_0^1 2t^3e^{t^4} + 3t^9 + 4t^5e^{t^3} dt$ = $\frac{49+15(e-1)}{30}$","I have this math problem, that I got a bit confused on. I just need to know whether or not I did it correctly. Thanks! Question: Calculate $\oint_c xe^{z}dx+yzdy+xe^{y}dz$ over the directed curve $C$   that is parameterized by $r(t) = t^2i+t^3j+t^4k$, $0\leq t \leq1$. Work: $r(t)=<t^2, t^3, t^4>$ $r'(t)=<2t, 3t^2, 4t^3>$ $x=t^2, y=t^3, z=t^4$ $dx=2t, dy=3t^2, dz=4t^3$ I plug into the equation: $\oint_c xe^{z}dx+yzdy+xe^{y}dz$ $\int_0^1 (t^2)e^{(t^4)}(2t)+(t^3)(t^4)(3t^2)+(t^2)e^{t^3}(4t^3) dt$ = $\int_0^1 2t^3e^{t^4} + 3t^9 + 4t^5e^{t^3} dt$ = $\frac{49+15(e-1)}{30}$",,"['multivariable-calculus', 'vector-fields', 'line-integrals']"
18,"Having trouble calculating $f_{xx}$ of a ""variable-heavy"" quotient.","Having trouble calculating  of a ""variable-heavy"" quotient.",f_{xx},"Let $$ f(x,y) =  \begin{cases} xy \frac{x^2 - y^2}{x^2 + y^2}, & (x,y) \ne (0,0) \\ 0, & (x,y) = (0,0) \end{cases} $$ Compute $f_x (0,0)$, $f_y (0,0)$, $f_{xx} (0,0)$, $f_{xy} (0,0)$, and determine where it is continuous. Here's what I did First, I simplified the equation: $$ f(x,y) = xy \frac{x^2 - y^2}{x^2 + y^2} = \frac{x^3y - xy^3}{x^2 + y^2}$$ Recallinig the definition of a partial derivative: $$f_x = lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{h}$$ I find that $$f_x(0,0) = lim_{h \to 0} \frac{\frac{(x+h)^3y - (x+h)y^3}{(x+h)^2 + y^2} - 0}{h} = 0$$ Because $$ lim_{h \to 0} \frac{x^3y + h(...)y - xy^3 - h(y^3)}{(x^2 + h(...) + y^2)h}$$ becomes zero when $(x,y) = (0,0)$, right? But now my concern is repeating this for $f_{xx}$. Do I repeat the operation with $f_x$ being the new $f(x,y$? Using the definition as a method? At the end I don't know if it will become 0 again. I tried it before but it becomes such a long and tedious expansion of terms. I can't see the end in sight.","Let $$ f(x,y) =  \begin{cases} xy \frac{x^2 - y^2}{x^2 + y^2}, & (x,y) \ne (0,0) \\ 0, & (x,y) = (0,0) \end{cases} $$ Compute $f_x (0,0)$, $f_y (0,0)$, $f_{xx} (0,0)$, $f_{xy} (0,0)$, and determine where it is continuous. Here's what I did First, I simplified the equation: $$ f(x,y) = xy \frac{x^2 - y^2}{x^2 + y^2} = \frac{x^3y - xy^3}{x^2 + y^2}$$ Recallinig the definition of a partial derivative: $$f_x = lim_{h \to 0} \frac{f(x+h,y) - f(x,y)}{h}$$ I find that $$f_x(0,0) = lim_{h \to 0} \frac{\frac{(x+h)^3y - (x+h)y^3}{(x+h)^2 + y^2} - 0}{h} = 0$$ Because $$ lim_{h \to 0} \frac{x^3y + h(...)y - xy^3 - h(y^3)}{(x^2 + h(...) + y^2)h}$$ becomes zero when $(x,y) = (0,0)$, right? But now my concern is repeating this for $f_{xx}$. Do I repeat the operation with $f_x$ being the new $f(x,y$? Using the definition as a method? At the end I don't know if it will become 0 again. I tried it before but it becomes such a long and tedious expansion of terms. I can't see the end in sight.",,"['real-analysis', 'multivariable-calculus', 'partial-derivative']"
19,Green's Theorem and limits on y for flux,Green's Theorem and limits on y for flux,,"I'm working through understanding the example provided in the book for the divergence integral.  The theorem (Green's): $$ \oint_C = \mathbf{F}\cdot \mathbf{T}ds = \oint_CMdy-Ndx=\int\int_R(\frac{\partial M}{\partial x}+\frac{\partial N}{\partial y} )dxdy $$ The example uses the following: $\mathbf{F}(x,y) = (x-y)\mathbf{i} + x\mathbf{j}$ over the region $\mathbf{R}$ bounded by the unit circle $C: \mathbf{r}(t)=cos(t)\mathbf{i} + sin(t)\mathbf{j}, 0 \le t \le 2\pi$. There is then the following relations: $$ \begin{array}{rr} M = cos(t) - sin(t) & dx = d(cos(t)) = -sin(t)dt \\ N = cos(t) & dy = d(sin(t)) = cos(t)dt \end{array} \\ \begin{array}{rrrr} \frac{\partial M}{\partial x}=1 & \frac{\partial M}{\partial y} = -1 & \frac{\partial N}{\partial x}=1 & \frac{\partial N}{\partial y} = 0 \end{array} $$ Now that the foundation is laid, here's the rightmost part of the first equation given: $$ \begin{array}{rcl} \int\int_R \frac{\partial M}{\partial x} + \frac{\partial N}{\partial y} dxdy & = & \int\int_R 1 + 0 dxdy \\  & = & \int\int_R dxdy \\  & = & \text{area inside unit circle} \\  & = & \pi \end{array} $$ I understand it intuitively because it's the area over that region and the area of a circle is $A = \pi\cdot r^2$.  With $r = 1$ that's obviously $\pi$.  What I'm not sure of is how to express it in an integral.  The question $x^2 + y^2 = 1$ represents the unit circle.  Thus, $x$ as a function of $y$ I get $x = \sqrt{1-y^2}$, thus the final stage I show should be: $$ \int_{?}^{?} \int_{0}^{\sqrt{1-y^2}}dx dy $$ right?  What should be used for the limits on y?  I know it's simple but I'm just not seeing it and I need some guidance. Thanks","I'm working through understanding the example provided in the book for the divergence integral.  The theorem (Green's): $$ \oint_C = \mathbf{F}\cdot \mathbf{T}ds = \oint_CMdy-Ndx=\int\int_R(\frac{\partial M}{\partial x}+\frac{\partial N}{\partial y} )dxdy $$ The example uses the following: $\mathbf{F}(x,y) = (x-y)\mathbf{i} + x\mathbf{j}$ over the region $\mathbf{R}$ bounded by the unit circle $C: \mathbf{r}(t)=cos(t)\mathbf{i} + sin(t)\mathbf{j}, 0 \le t \le 2\pi$. There is then the following relations: $$ \begin{array}{rr} M = cos(t) - sin(t) & dx = d(cos(t)) = -sin(t)dt \\ N = cos(t) & dy = d(sin(t)) = cos(t)dt \end{array} \\ \begin{array}{rrrr} \frac{\partial M}{\partial x}=1 & \frac{\partial M}{\partial y} = -1 & \frac{\partial N}{\partial x}=1 & \frac{\partial N}{\partial y} = 0 \end{array} $$ Now that the foundation is laid, here's the rightmost part of the first equation given: $$ \begin{array}{rcl} \int\int_R \frac{\partial M}{\partial x} + \frac{\partial N}{\partial y} dxdy & = & \int\int_R 1 + 0 dxdy \\  & = & \int\int_R dxdy \\  & = & \text{area inside unit circle} \\  & = & \pi \end{array} $$ I understand it intuitively because it's the area over that region and the area of a circle is $A = \pi\cdot r^2$.  With $r = 1$ that's obviously $\pi$.  What I'm not sure of is how to express it in an integral.  The question $x^2 + y^2 = 1$ represents the unit circle.  Thus, $x$ as a function of $y$ I get $x = \sqrt{1-y^2}$, thus the final stage I show should be: $$ \int_{?}^{?} \int_{0}^{\sqrt{1-y^2}}dx dy $$ right?  What should be used for the limits on y?  I know it's simple but I'm just not seeing it and I need some guidance. Thanks",,"['integration', 'multivariable-calculus', 'greens-theorem']"
20,"How to obtain the line element in cylindrical coordinates, using definition of differential forms","How to obtain the line element in cylindrical coordinates, using definition of differential forms",,"In general, a volume element is a k-form on an K-dimensional manifold. a k-form w on $\mathbb{R}^{n}$ is defined as $w(x) = \sum_{i_{1}<i_{2}<...<i_{k}} w_{i_{1}i_{2}...i_{k}}(x) \cdot\mathrm{d} x_{i_{1}}\wedge \mathrm{d} x_{i_{2}}\wedge...\wedge \mathrm{d} x_{i_{k}}$ For cylindrical coordinates, the line element is a 1-form in $\mathbb{R}^{3}$, and $\mathrm{ds}= \mathrm{dr}\hat{\mathrm{r}} + \mathrm{dz}\hat{\mathrm{z}}+\mathrm{r}\mathrm{d}\phi \hat{\phi}$. I can understand how this follows from the definition given above, since the line element is a 1-form. But I don't understand where the factor r in front of the $\mathrm{d}\phi \hat{\phi}$ comes from, using the definition of the differential form?","In general, a volume element is a k-form on an K-dimensional manifold. a k-form w on $\mathbb{R}^{n}$ is defined as $w(x) = \sum_{i_{1}<i_{2}<...<i_{k}} w_{i_{1}i_{2}...i_{k}}(x) \cdot\mathrm{d} x_{i_{1}}\wedge \mathrm{d} x_{i_{2}}\wedge...\wedge \mathrm{d} x_{i_{k}}$ For cylindrical coordinates, the line element is a 1-form in $\mathbb{R}^{3}$, and $\mathrm{ds}= \mathrm{dr}\hat{\mathrm{r}} + \mathrm{dz}\hat{\mathrm{z}}+\mathrm{r}\mathrm{d}\phi \hat{\phi}$. I can understand how this follows from the definition given above, since the line element is a 1-form. But I don't understand where the factor r in front of the $\mathrm{d}\phi \hat{\phi}$ comes from, using the definition of the differential form?",,"['multivariable-calculus', 'differential-forms']"
21,Logarithm multivariable limit $\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}}$,Logarithm multivariable limit,\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}},"Find multivariable limit $$\lim_{\left( x,y \right) \rightarrow (0,0)}\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}}$$ I was trying to find and inequality i've found out that: $$\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}} \le \frac{\sqrt{x^2+y^2} \ln(1+x^3+y^3)}{2xy} $$ and after that i am stuck. From iterated limits i've calculated before i knwo that the limit exists for certain. I have also another idea to do that precisely to $y=x$ and $y=-x$ but i do not know whether this is working properly.","Find multivariable limit $$\lim_{\left( x,y \right) \rightarrow (0,0)}\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}}$$ I was trying to find and inequality i've found out that: $$\frac{\ln(1+x^3+y^3)}{\sqrt{x^2+y^2}} \le \frac{\sqrt{x^2+y^2} \ln(1+x^3+y^3)}{2xy} $$ and after that i am stuck. From iterated limits i've calculated before i knwo that the limit exists for certain. I have also another idea to do that precisely to $y=x$ and $y=-x$ but i do not know whether this is working properly.",,"['limits', 'multivariable-calculus', 'functions', 'inequality', 'logarithms']"
22,Proving vector Identities (Using the Permutation Tensor and Kroenecker Delta),Proving vector Identities (Using the Permutation Tensor and Kroenecker Delta),,Prove the following vector identities by using permutation tensor and kroenecker delta. $$(\vec{A} \times \vec{B}) \times (\vec{C} \times \vec{D}) = (\vec{A} \cdot (\vec{B} \times \vec{D})) \cdot \vec{C} - (\vec{A} \cdot (\vec{B} \times \vec{C})) \cdot \vec{D}$$ I started by assuming $\vec{F}=\vec{A} \times \vec{B} = \varepsilon_{ijk} A_j B_j$ and $\vec{E}=\vec{C} \times \vec{D} = \varepsilon_{mnp} A_n B_p$ Then $\vec{G} = \vec{F} \times \vec{E} = \varepsilon_{qim} F_i E_m = \varepsilon_{qim} \varepsilon_{ijk} \varepsilon_{mnp}A_j B_k C_n D_p$ After this I have dont know what else to do. Could be please give me some tipps. Thanks.,Prove the following vector identities by using permutation tensor and kroenecker delta. $$(\vec{A} \times \vec{B}) \times (\vec{C} \times \vec{D}) = (\vec{A} \cdot (\vec{B} \times \vec{D})) \cdot \vec{C} - (\vec{A} \cdot (\vec{B} \times \vec{C})) \cdot \vec{D}$$ I started by assuming $\vec{F}=\vec{A} \times \vec{B} = \varepsilon_{ijk} A_j B_j$ and $\vec{E}=\vec{C} \times \vec{D} = \varepsilon_{mnp} A_n B_p$ Then $\vec{G} = \vec{F} \times \vec{E} = \varepsilon_{qim} F_i E_m = \varepsilon_{qim} \varepsilon_{ijk} \varepsilon_{mnp}A_j B_k C_n D_p$ After this I have dont know what else to do. Could be please give me some tipps. Thanks.,,"['multivariable-calculus', 'vector-analysis', 'tensors']"
23,Inequality with Laplacian,Inequality with Laplacian,,"I'm trying to solve a larger problem and have reduced it to showing that $$\int_\Omega (u-v) \Delta v dx \geq 0$$ Here $u, v$ are both continuous on $\overline\Omega$, $C^2$ on $\Omega$, which is an open region in $\mathbb{R}^n$ with smooth boundary. How can I prove this to be true? EDIT: Additional information about the problem is that $u$ and $v$ agree on $\delta \Omega$ and $u$ is harmonic. I used this info to reduce my original problem (showing that $\int_\Omega |\nabla u|^2 \leq \int_\Omega |\nabla v|^2$) to the one I have now, but it might still be relevant.","I'm trying to solve a larger problem and have reduced it to showing that $$\int_\Omega (u-v) \Delta v dx \geq 0$$ Here $u, v$ are both continuous on $\overline\Omega$, $C^2$ on $\Omega$, which is an open region in $\mathbb{R}^n$ with smooth boundary. How can I prove this to be true? EDIT: Additional information about the problem is that $u$ and $v$ agree on $\delta \Omega$ and $u$ is harmonic. I used this info to reduce my original problem (showing that $\int_\Omega |\nabla u|^2 \leq \int_\Omega |\nabla v|^2$) to the one I have now, but it might still be relevant.",,"['multivariable-calculus', 'laplacian']"
24,limits of r in cylindrical coordinates,limits of r in cylindrical coordinates,,"Find the volume of a sphere $x^2+y^2+z^2\leq 1$ contained between planes $z=1/2$ and $z=1/\sqrt2$ using cylindrical coordinates. So the limits of $\theta$ would be $0$ to $2\pi$. Limits of $z$ would be the given planes. But why cant the limits of $r$ simply be $0$ to $1$? Is it because, if this were the case, we would be finding the volume of a cylinder and not a spherical type of object?","Find the volume of a sphere $x^2+y^2+z^2\leq 1$ contained between planes $z=1/2$ and $z=1/\sqrt2$ using cylindrical coordinates. So the limits of $\theta$ would be $0$ to $2\pi$. Limits of $z$ would be the given planes. But why cant the limits of $r$ simply be $0$ to $1$? Is it because, if this were the case, we would be finding the volume of a cylinder and not a spherical type of object?",,['multivariable-calculus']
25,Two Definitions of Critical Points,Two Definitions of Critical Points,,"Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be given such that it is continuously differentiable. According to Wikipedia , a ""critical point"" of $f$ is a point $p\in\mathbb{R}^n$ such that: 1) According to one paragraph it is the condition that $\partial_j f_i = 0$ for all $j\in\{1,\dots,n\}$ and $i\in\{1,\dots,m\}$. 2) According to another paragraph, it is the condition that the matrix with the $(i,j)$ component $\partial_j f_i$ (i.e. the Jacobian matrix) has rank less than $m$. It seems like these two definitions are not equivalent. Why are there two different definitions? I can understand why the first one is called ""critical point"": it gives exactly the points which are local maximum, minimum or saddle points. Why is the second one called ""critical point""? What does it give, except for saying that the function at this point is not surjective?","Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be given such that it is continuously differentiable. According to Wikipedia , a ""critical point"" of $f$ is a point $p\in\mathbb{R}^n$ such that: 1) According to one paragraph it is the condition that $\partial_j f_i = 0$ for all $j\in\{1,\dots,n\}$ and $i\in\{1,\dots,m\}$. 2) According to another paragraph, it is the condition that the matrix with the $(i,j)$ component $\partial_j f_i$ (i.e. the Jacobian matrix) has rank less than $m$. It seems like these two definitions are not equivalent. Why are there two different definitions? I can understand why the first one is called ""critical point"": it gives exactly the points which are local maximum, minimum or saddle points. Why is the second one called ""critical point""? What does it give, except for saying that the function at this point is not surjective?",,"['analysis', 'multivariable-calculus']"
26,"Tangent plane of $f(x,y,z)$? How is this possible?",Tangent plane of ? How is this possible?,"f(x,y,z)","I am in vector calculus right now and am confused as to the following problem. From my understanding, this cannot have a tangent plane as it is a function of three variables, and would instead give a surface. However, the book asks for a tangent plane. More frustratingly, this problem is an even problem (no solution available) and there aren't any similar to it in the text. All others ask for a tangent plane of $f(x,y)$. Should I use the derivative matrix of the function and then use the linear approximation as my ""tangent plane""? Or rather solve the equations for $z = f(x,y)$ and then go from there. Thanks for any input. Compute the tangent plane of the following function at $(1,0,1)$     $$         \ f(x,y,z) =\frac{xyz}{x^2 + y^2 +z^2}         $$","I am in vector calculus right now and am confused as to the following problem. From my understanding, this cannot have a tangent plane as it is a function of three variables, and would instead give a surface. However, the book asks for a tangent plane. More frustratingly, this problem is an even problem (no solution available) and there aren't any similar to it in the text. All others ask for a tangent plane of $f(x,y)$. Should I use the derivative matrix of the function and then use the linear approximation as my ""tangent plane""? Or rather solve the equations for $z = f(x,y)$ and then go from there. Thanks for any input. Compute the tangent plane of the following function at $(1,0,1)$     $$         \ f(x,y,z) =\frac{xyz}{x^2 + y^2 +z^2}         $$",,"['calculus', 'multivariable-calculus']"
27,"Finding a limit with two independent variables: $\lim_{(x,y)\to (0,0)}\frac{x^2y^2}{x^2+y^2}$",Finding a limit with two independent variables:,"\lim_{(x,y)\to (0,0)}\frac{x^2y^2}{x^2+y^2}","I must find the following limit: $$\lim_{(x,y)\to (0,0)}\frac{x^2y^2}{x^2+y^2}$$ Substituting $y=mx$ and $y=x^2$, I have found the limit to be $0$ both times, as $x \to 0$. I have thus assumed that the above limit is $0$, and will attempt to prove it. Let $\varepsilon>0$. We have that: $$\left\lvert\frac{x^2y^2}{x^2+y^2}\right\rvert=\frac{x^2y^2}{x^2+y^2}\leq\frac{(x^2+y^2)(x^2+y^2)}{x^2+y^2}=x^2+y^2$$ However, I must find $\delta>0$ such that $0<\sqrt{x^2+y^2}<\delta$, and I cannot see a way to obtain $\sqrt{x^2+y^2}$ in the above inequality to complete the proof. Am I mistaken in my process? Thank you.","I must find the following limit: $$\lim_{(x,y)\to (0,0)}\frac{x^2y^2}{x^2+y^2}$$ Substituting $y=mx$ and $y=x^2$, I have found the limit to be $0$ both times, as $x \to 0$. I have thus assumed that the above limit is $0$, and will attempt to prove it. Let $\varepsilon>0$. We have that: $$\left\lvert\frac{x^2y^2}{x^2+y^2}\right\rvert=\frac{x^2y^2}{x^2+y^2}\leq\frac{(x^2+y^2)(x^2+y^2)}{x^2+y^2}=x^2+y^2$$ However, I must find $\delta>0$ such that $0<\sqrt{x^2+y^2}<\delta$, and I cannot see a way to obtain $\sqrt{x^2+y^2}$ in the above inequality to complete the proof. Am I mistaken in my process? Thank you.",,"['calculus', 'limits']"
28,How do I find the volume cut off of the unit ball by the plane ax+by+cz = d?,How do I find the volume cut off of the unit ball by the plane ax+by+cz = d?,,"How do I find the volume cut off of the unit ball by the plane ax+by+cz = d? I know that there's a double integral somewhere here, but I just don't understand how to attack this problem. Any guidance would be appreciated!","How do I find the volume cut off of the unit ball by the plane ax+by+cz = d? I know that there's a double integral somewhere here, but I just don't understand how to attack this problem. Any guidance would be appreciated!",,"['integration', 'general-topology', 'multivariable-calculus']"
29,Evaluating a double integral over a hemisphere,Evaluating a double integral over a hemisphere,,"Evaluate \begin{align*} \iint_S (\nabla \times \mathbf{F}) \cdot \mathbf{\hat{N}} \ dS, \end{align*} where $S$ is the hemisphere $x^2 + y^2 + z^2 = a^2, z \geq 0$ with outward normal, and $\mathbf{F} = 3y \hat{i} - 2xz \hat{j} + (x^2 - y^2) \hat{k}$. I'm not sure how to do this. We first compute the curl: \begin{align*} \nabla \times \mathbf{F} = (2x-2y) \hat{i} - 2x \hat{j} + (-2z-3) \hat{k} \end{align*} I know that Stoke's theorem says that \begin{align*} \oint_C \mathbf{F} \cdot d\mathbf{r} = \iint_S (\nabla \times \mathbf{F}) \cdot \mathbf{\hat{N}} dS, \end{align*} but that doesn't seem to help me here.","Evaluate \begin{align*} \iint_S (\nabla \times \mathbf{F}) \cdot \mathbf{\hat{N}} \ dS, \end{align*} where $S$ is the hemisphere $x^2 + y^2 + z^2 = a^2, z \geq 0$ with outward normal, and $\mathbf{F} = 3y \hat{i} - 2xz \hat{j} + (x^2 - y^2) \hat{k}$. I'm not sure how to do this. We first compute the curl: \begin{align*} \nabla \times \mathbf{F} = (2x-2y) \hat{i} - 2x \hat{j} + (-2z-3) \hat{k} \end{align*} I know that Stoke's theorem says that \begin{align*} \oint_C \mathbf{F} \cdot d\mathbf{r} = \iint_S (\nabla \times \mathbf{F}) \cdot \mathbf{\hat{N}} dS, \end{align*} but that doesn't seem to help me here.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
30,Reversing the order of integration. (Picture included),Reversing the order of integration. (Picture included),,"I wish to reverse the order of integration. So, I think the original integration is over the region above. So, reversing the order of integration, I split it into two cases. First region (from 0 to 1) comes first, then second region (from -1/2 to 0) comes next. But, I get the wrong answer. Where did I make a mistake?","I wish to reverse the order of integration. So, I think the original integration is over the region above. So, reversing the order of integration, I split it into two cases. First region (from 0 to 1) comes first, then second region (from -1/2 to 0) comes next. But, I get the wrong answer. Where did I make a mistake?",,"['calculus', 'integration', 'multivariable-calculus']"
31,"Calculate the integral $\int_{[0,1]\times[0,1]}\frac{1}{(1-xy)^a}dydx$ for $a>0$",Calculate the integral  for,"\int_{[0,1]\times[0,1]}\frac{1}{(1-xy)^a}dydx a>0","I'm having trouble with the integral $\int_{[0,1]\times[0,1]}\frac{1}{(1-xy)^a}dydx$ for the case in which $a\neq{1}$. For the case in which $a=1$ it was not difficult to calculate the integral and I got the convergent series $\sum_{n=1}^\infty\frac{1}{n^2}$. Now, for the case in which $a>0$ and $a\neq{1}$, first, I calculated the integral $\int_0^1\frac{1}{(1-xy)^a}dy$ and by making a change of variable I got the result $\frac{-1}{x}$($\frac{(1-x)^{1-a}}{1-a}$ - $\frac{1}{1-a}$) which can be simplified to $\frac{1-(1-x)^{1-a}}{x(1-a)}$ (I'm almost sure this is right, but if anyone finds my calculation of this integral with respecto to $y$ is wrong, let me know). So, then I would have to calculate the integral $\int_o^1\frac{1-(1-x)^{1-a}}{x(1-a)}dx$ and this is where I'm stuck. Maybe I should express one of the expressions in the integrand as a series? Or is there any easier approach to the problem? Any suggestions would be appreciated.","I'm having trouble with the integral $\int_{[0,1]\times[0,1]}\frac{1}{(1-xy)^a}dydx$ for the case in which $a\neq{1}$. For the case in which $a=1$ it was not difficult to calculate the integral and I got the convergent series $\sum_{n=1}^\infty\frac{1}{n^2}$. Now, for the case in which $a>0$ and $a\neq{1}$, first, I calculated the integral $\int_0^1\frac{1}{(1-xy)^a}dy$ and by making a change of variable I got the result $\frac{-1}{x}$($\frac{(1-x)^{1-a}}{1-a}$ - $\frac{1}{1-a}$) which can be simplified to $\frac{1-(1-x)^{1-a}}{x(1-a)}$ (I'm almost sure this is right, but if anyone finds my calculation of this integral with respecto to $y$ is wrong, let me know). So, then I would have to calculate the integral $\int_o^1\frac{1-(1-x)^{1-a}}{x(1-a)}dx$ and this is where I'm stuck. Maybe I should express one of the expressions in the integrand as a series? Or is there any easier approach to the problem? Any suggestions would be appreciated.",,"['integration', 'multivariable-calculus', 'definite-integrals']"
32,Continuity of a rational function,Continuity of a rational function,,"I have to evaluate the continuity of a two functions at a couple of different points and I am a bit stuck. Here are the two functions: $f(x,y) = 0$ if $(x,y)=(0,0)$, $f(x,y) = \frac{x^3y^2}{3x^4+2y^2}$ if $(x,y) \neq (0,0)$. I want to evaluate the continuity of this one at $(0,0)$ and $(1,0)$. I also have $f(x,y) = 0$ if $(x,y)=(0,0)$, $f(x,y) = \frac{y^3x}{x^2+y^6}$ if $(x,y) \neq (0,0)$. I want to evaluate the continuity of this one at $(0,0)$ and $(0,1)$. Okay, so for both of these functions at $(0,0)$ the denominator is zero along $3x^4+2y^2$ and $x^2+y^6$, respectively, so I cannot simply evaluate the limit of a sequence approaching points along this line to determine the limit. Everywhere else however, including $(1,0)$ the limit exists and is hence continuous. In my class, we did two similar examples, but both denominators were of the form $x^2+y^2$. It was easy to show that for one example, you get a different limit for various sequences approaching the origin, hence the limit DNE. For the other example, we proved a limit existed by using the squeeze theorem. But both ways seemed more to be like tricks to me. How am I supposed to know what to do here without any experience?","I have to evaluate the continuity of a two functions at a couple of different points and I am a bit stuck. Here are the two functions: $f(x,y) = 0$ if $(x,y)=(0,0)$, $f(x,y) = \frac{x^3y^2}{3x^4+2y^2}$ if $(x,y) \neq (0,0)$. I want to evaluate the continuity of this one at $(0,0)$ and $(1,0)$. I also have $f(x,y) = 0$ if $(x,y)=(0,0)$, $f(x,y) = \frac{y^3x}{x^2+y^6}$ if $(x,y) \neq (0,0)$. I want to evaluate the continuity of this one at $(0,0)$ and $(0,1)$. Okay, so for both of these functions at $(0,0)$ the denominator is zero along $3x^4+2y^2$ and $x^2+y^6$, respectively, so I cannot simply evaluate the limit of a sequence approaching points along this line to determine the limit. Everywhere else however, including $(1,0)$ the limit exists and is hence continuous. In my class, we did two similar examples, but both denominators were of the form $x^2+y^2$. It was easy to show that for one example, you get a different limit for various sequences approaching the origin, hence the limit DNE. For the other example, we proved a limit existed by using the squeeze theorem. But both ways seemed more to be like tricks to me. How am I supposed to know what to do here without any experience?",,"['limits', 'multivariable-calculus', 'continuity']"
33,How do I change the order of integration of this integral?,How do I change the order of integration of this integral?,,"$$\int\limits_1^e\int\limits_{\frac{\pi}{2}}^{\log \,x} - \sin\,y\, dy\,dx$$ I don't understand how to change the order because of the $\log\,x$ as the upper bound for the inner integral How do I change it so it looks like $$\int \int f(x)d(y)\,dx\,dy$$ Thanks","$$\int\limits_1^e\int\limits_{\frac{\pi}{2}}^{\log \,x} - \sin\,y\, dy\,dx$$ I don't understand how to change the order because of the $\log\,x$ as the upper bound for the inner integral How do I change it so it looks like $$\int \int f(x)d(y)\,dx\,dy$$ Thanks",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
34,"surfaces, curves and lines","surfaces, curves and lines",,"Could someone please assist with the following questions: Consider $f(x,y) = x^{\frac{1}{3}}y^{\frac{1}{3}}$ and take $C$ to be the curve of intersection of $z = f(x,y)$ with the plane $y=x$. Show that the curve $C$ has a tangent line at the origin? I have tried showing that the directional derivative $D_{u}f(0,0)$ exists (using the formula $D_{u}f(x_{0},y_{0}) = \lim\limits_{h \rightarrow 0}\frac{f(x_{0}+ha,y_{0}+hb)-f(x_{0},y_{0})}{h}$) by taking $u = \frac{(1,1)}{\sqrt{2}}$ (since $y = x$), but I get a limit which does not exist. Lastly, why is $f$ not differentiable at $(0,0)$? Any assistance would be appreciated.","Could someone please assist with the following questions: Consider $f(x,y) = x^{\frac{1}{3}}y^{\frac{1}{3}}$ and take $C$ to be the curve of intersection of $z = f(x,y)$ with the plane $y=x$. Show that the curve $C$ has a tangent line at the origin? I have tried showing that the directional derivative $D_{u}f(0,0)$ exists (using the formula $D_{u}f(x_{0},y_{0}) = \lim\limits_{h \rightarrow 0}\frac{f(x_{0}+ha,y_{0}+hb)-f(x_{0},y_{0})}{h}$) by taking $u = \frac{(1,1)}{\sqrt{2}}$ (since $y = x$), but I get a limit which does not exist. Lastly, why is $f$ not differentiable at $(0,0)$? Any assistance would be appreciated.",,"['calculus', 'multivariable-calculus', 'derivatives', 'surfaces']"
35,$\operatorname{curl} \mathbf{A} =0$ if and only if $A$ is conservative,if and only if  is conservative,\operatorname{curl} \mathbf{A} =0 A,Should this theorem not instead state: $\operatorname{curl}\mathbf{A}=0$ on the surface $S$ as by Stokes' theorem $\displaystyle \oint_{\gamma} \vec{A} \cdot d\vec{r}=\int_S$curl A $\cdot $ $d\vec{S}$,Should this theorem not instead state: $\operatorname{curl}\mathbf{A}=0$ on the surface $S$ as by Stokes' theorem $\displaystyle \oint_{\gamma} \vec{A} \cdot d\vec{r}=\int_S$curl A $\cdot $ $d\vec{S}$,,['multivariable-calculus']
36,Partial derivative w.r.t. to the time of a time-dependend quadratic form,Partial derivative w.r.t. to the time of a time-dependend quadratic form,,"Suppose the quadratic form $$V(x(t), t) = \frac{1}{2} x^\mathsf{T}(t) P(t) x(t)$$ where $$x(t) \in \mathbb{R}^n,~P(t) \in \mathbb{R}^{n \times n},~\text{and}~P(t) = P^\mathsf{T}(t) > 0$$ i.$\,$e. $P(t)$ is a symmetric and positive definite matrix and $t$ denotes the time as an independed variable. How does one determine $\frac{\partial V(x(t), t)}{\partial t}$?","Suppose the quadratic form $$V(x(t), t) = \frac{1}{2} x^\mathsf{T}(t) P(t) x(t)$$ where $$x(t) \in \mathbb{R}^n,~P(t) \in \mathbb{R}^{n \times n},~\text{and}~P(t) = P^\mathsf{T}(t) > 0$$ i.$\,$e. $P(t)$ is a symmetric and positive definite matrix and $t$ denotes the time as an independed variable. How does one determine $\frac{\partial V(x(t), t)}{\partial t}$?",,"['multivariable-calculus', 'partial-derivative', 'quadratic-forms']"
37,Show that $F(x) = f(\|x\|)$ is differentiable on $\mathbb{R}^n$. [duplicate],Show that  is differentiable on . [duplicate],F(x) = f(\|x\|) \mathbb{R}^n,"This question already has answers here : Multivariate differentiability verification (2 answers) Closed 9 years ago . Let an even function $f:\mathbb{R}\to\mathbb{R}$ which is even and differentiable. We define $F:\mathbb{R}^n\to\mathbb{R}$  as $F(x) = f(\|x\|)$. Show that $F(x)$ is differentiable on $\mathbb{R}^n$. My Work: Let $x_0\in\mathbb{R}^n$. If $x_0\ne 0$ then the partial derivatives of the norm are well-defined and: $$ \frac{\partial \|.\|}{\partial x_i} = \frac{x_i}{\|x\|}$$ $\frac{x_i}{\|x\|}$ continuous and since $f$ is differentiable it's partial derivatives are continuous. So we can conclude that $\frac{\partial F}{\partial x_i}$ is continuous and therefore $F$ is differentiable. If $x_0=0$ then the $\|x_0\| = 0$ and since $f$ is even $f(0)=0$ and it must be an extremum point. Therefore, $F'(0)=0$. I'd like to get a critique of my work. Am I being right/rigorous? Thanks.","This question already has answers here : Multivariate differentiability verification (2 answers) Closed 9 years ago . Let an even function $f:\mathbb{R}\to\mathbb{R}$ which is even and differentiable. We define $F:\mathbb{R}^n\to\mathbb{R}$  as $F(x) = f(\|x\|)$. Show that $F(x)$ is differentiable on $\mathbb{R}^n$. My Work: Let $x_0\in\mathbb{R}^n$. If $x_0\ne 0$ then the partial derivatives of the norm are well-defined and: $$ \frac{\partial \|.\|}{\partial x_i} = \frac{x_i}{\|x\|}$$ $\frac{x_i}{\|x\|}$ continuous and since $f$ is differentiable it's partial derivatives are continuous. So we can conclude that $\frac{\partial F}{\partial x_i}$ is continuous and therefore $F$ is differentiable. If $x_0=0$ then the $\|x_0\| = 0$ and since $f$ is even $f(0)=0$ and it must be an extremum point. Therefore, $F'(0)=0$. I'd like to get a critique of my work. Am I being right/rigorous? Thanks.",,"['calculus', 'multivariable-calculus', 'derivatives', 'normed-spaces']"
38,Showing $f=0$ almost everywhere,Showing  almost everywhere,f=0,"Let $\psi_n(x)=e^{-x^2/2}P_n(x)$ where $P_n$ is a degree $n$ polynomial with real coefficients. Assume that $$\int_{\mathbb{R}}e^{-x^2/2}P_n=0.$$ Suppose that for any $f\in L^2$, such that $$\int_\mathbb{R} e^{-x^2/2}P_n(x)f(x)=0,$$ then how would you show that $f=0$ a.e? Any ideas/hints? Thanks","Let $\psi_n(x)=e^{-x^2/2}P_n(x)$ where $P_n$ is a degree $n$ polynomial with real coefficients. Assume that $$\int_{\mathbb{R}}e^{-x^2/2}P_n=0.$$ Suppose that for any $f\in L^2$, such that $$\int_\mathbb{R} e^{-x^2/2}P_n(x)f(x)=0,$$ then how would you show that $f=0$ a.e? Any ideas/hints? Thanks",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'multivariable-calculus']"
39,checking if a function is differentiable,checking if a function is differentiable,,"How can i prove that the following function is differentiable? $$f(x_1,x_2)=\int_0^{|x_1|}g(s)ds+x_2^2$$ where $g(x)$ is an increasing function with $g(0)=0$. Its partial derivative is equal to $\frac{\partial f(x_1,x_2)}{x_1}=g(|x_1|)\frac{\partial |x_1|}{\partial x_1}$, isn't there a problem at $x_1=0$?","How can i prove that the following function is differentiable? $$f(x_1,x_2)=\int_0^{|x_1|}g(s)ds+x_2^2$$ where $g(x)$ is an increasing function with $g(0)=0$. Its partial derivative is equal to $\frac{\partial f(x_1,x_2)}{x_1}=g(|x_1|)\frac{\partial |x_1|}{\partial x_1}$, isn't there a problem at $x_1=0$?",,"['multivariable-calculus', 'derivatives']"
40,Determine a parameterization for the line which is tangent to the curve at t=2,Determine a parameterization for the line which is tangent to the curve at t=2,,"(1) A curve is given by the function $$r(t)=(t^3 -3t^2 +2t +4)i + (13-5t)j +(t^2 -t-3)k$$ Determine a parameterization for the line which is tangent to the curve at $t=2$ I started by solving for when $t=2$, and got the vector $4i + 3j - k$ I don't know what this vector means or if this was even the correct approach. Do I need to find the vector perpendicular to this, or what should I be doing instead?","(1) A curve is given by the function $$r(t)=(t^3 -3t^2 +2t +4)i + (13-5t)j +(t^2 -t-3)k$$ Determine a parameterization for the line which is tangent to the curve at $t=2$ I started by solving for when $t=2$, and got the vector $4i + 3j - k$ I don't know what this vector means or if this was even the correct approach. Do I need to find the vector perpendicular to this, or what should I be doing instead?",,"['calculus', 'multivariable-calculus', 'functions', 'parametric']"
41,"Finding a normal vector to the surface $F(u,v)=0. u=xy, v = \sqrt {x^2+z^2}$ at the point $x=1,y=1, z=\sqrt 3$",Finding a normal vector to the surface  at the point,"F(u,v)=0. u=xy, v = \sqrt {x^2+z^2} x=1,y=1, z=\sqrt 3","The three equations $F(u,v)=0. u=xy, v = \sqrt {x^2+z^2}$ define a surface in $xyz$ space. Find a normal vector to this surface at the point $x=1,y=1, z=\sqrt 3$ if it is known that $D_1F(1,2)=1$ and $D_2F(1,2)=2$ Solution Attempt: Since, $F(u,v)=0$ represents a level surface, hence, the gradient vector will be a normal vector to the surface at a given point. $F(u,v)=0 \implies \dfrac {\partial F}{\partial u} \cdot  \dfrac {\partial u}{\partial x} +  \dfrac {\partial F}{\partial v} \cdot  \dfrac {\partial v}{\partial x} =0$ $\implies \dfrac {\partial F}{\partial u} \cdot  y +  \dfrac {\partial F}{\partial v} \cdot  \dfrac  {x} {\sqrt {x^2+z^2}} =0$ At the given point, this becomes equal to : $ \dfrac {\partial F}{\partial u}  +  \dfrac {\partial F}{\partial v} \cdot  \dfrac  {1} {2} =0~~~~~........(1)$ Similarly: $\dfrac {\partial F}{\partial u} \cdot  \dfrac {\partial u}{\partial y} +  \dfrac {\partial F}{\partial v} \cdot  \dfrac {\partial v}{\partial y} =0$ $\implies \dfrac {\partial F}{\partial u} \cdot  x +  \dfrac {\partial F}{\partial v} \cdot  0 =0~~~~~......(2)$ From $(1),(2)$, we get that $ \dfrac {\partial F}{\partial u}= \dfrac {\partial F}{\partial v} = 0  $ This means, that the normal vector is $0$? Could anyone please tell me where I could have possibly gone wrong in my solution attempt? Thank you for reading through and helping!","The three equations $F(u,v)=0. u=xy, v = \sqrt {x^2+z^2}$ define a surface in $xyz$ space. Find a normal vector to this surface at the point $x=1,y=1, z=\sqrt 3$ if it is known that $D_1F(1,2)=1$ and $D_2F(1,2)=2$ Solution Attempt: Since, $F(u,v)=0$ represents a level surface, hence, the gradient vector will be a normal vector to the surface at a given point. $F(u,v)=0 \implies \dfrac {\partial F}{\partial u} \cdot  \dfrac {\partial u}{\partial x} +  \dfrac {\partial F}{\partial v} \cdot  \dfrac {\partial v}{\partial x} =0$ $\implies \dfrac {\partial F}{\partial u} \cdot  y +  \dfrac {\partial F}{\partial v} \cdot  \dfrac  {x} {\sqrt {x^2+z^2}} =0$ At the given point, this becomes equal to : $ \dfrac {\partial F}{\partial u}  +  \dfrac {\partial F}{\partial v} \cdot  \dfrac  {1} {2} =0~~~~~........(1)$ Similarly: $\dfrac {\partial F}{\partial u} \cdot  \dfrac {\partial u}{\partial y} +  \dfrac {\partial F}{\partial v} \cdot  \dfrac {\partial v}{\partial y} =0$ $\implies \dfrac {\partial F}{\partial u} \cdot  x +  \dfrac {\partial F}{\partial v} \cdot  0 =0~~~~~......(2)$ From $(1),(2)$, we get that $ \dfrac {\partial F}{\partial u}= \dfrac {\partial F}{\partial v} = 0  $ This means, that the normal vector is $0$? Could anyone please tell me where I could have possibly gone wrong in my solution attempt? Thank you for reading through and helping!",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
42,Problem with proof of multivariable limit,Problem with proof of multivariable limit,,"I've got a problem with this limit: ${\lim_{(x,y) \to (0,0)} \frac{ x^{5} + 2y^{3} }{ x^{4} + y^{2} }}$ Can you help me, please? I think it equals 0, but I don't know how to prove it.","I've got a problem with this limit: ${\lim_{(x,y) \to (0,0)} \frac{ x^{5} + 2y^{3} }{ x^{4} + y^{2} }}$ Can you help me, please? I think it equals 0, but I don't know how to prove it.",,"['limits', 'multivariable-calculus']"
43,Calculation of the normal derivative on a sphere,Calculation of the normal derivative on a sphere,,"Fix $y\in B(0,R)$ (open ball) and define for $x\in B(0,R)\setminus \{y\}$ with $$ N(x;y):=\begin{cases} \dfrac{-1}{2\pi}\log\dfrac{rr^*\rho}{R^3},& y\not=0\\ \dfrac{-1}{2\pi}\log\dfrac{r}{R},&y=0 \end{cases} $$ where $r=|x-y|$, $r^*=|x-y^*|$, $y^*:=\frac{R^2}{|y|^2}y$ and $\rho=|y|$. I want to calculate the normal derivative of $N$ on the boundary of $B(0,R)$ with $y\in B(0,R)$, i.e. $$ \nabla_xN(x;y)\cdot \nu_x $$ where $\nu_x$ is the unit normal at $x\in\partial B(0,R)$. I'm expecting to get $1/(2\pi R)$, but I don't get it. The following is my calculation (for the case $y\not=0$): I first get  $$ \frac{\rho}{R}r^*=r $$ for $x\in\partial B(0,R)$. Then $$ -2\pi N(x,y)=\log\dfrac{r^2}{R^2} $$ and I get $$ -2\pi\nabla_xN(x,y)=\frac{2}{r^2}(x-y) $$ and $\nu_x=x/R$. What goes wrong with my calculation here?","Fix $y\in B(0,R)$ (open ball) and define for $x\in B(0,R)\setminus \{y\}$ with $$ N(x;y):=\begin{cases} \dfrac{-1}{2\pi}\log\dfrac{rr^*\rho}{R^3},& y\not=0\\ \dfrac{-1}{2\pi}\log\dfrac{r}{R},&y=0 \end{cases} $$ where $r=|x-y|$, $r^*=|x-y^*|$, $y^*:=\frac{R^2}{|y|^2}y$ and $\rho=|y|$. I want to calculate the normal derivative of $N$ on the boundary of $B(0,R)$ with $y\in B(0,R)$, i.e. $$ \nabla_xN(x;y)\cdot \nu_x $$ where $\nu_x$ is the unit normal at $x\in\partial B(0,R)$. I'm expecting to get $1/(2\pi R)$, but I don't get it. The following is my calculation (for the case $y\not=0$): I first get  $$ \frac{\rho}{R}r^*=r $$ for $x\in\partial B(0,R)$. Then $$ -2\pi N(x,y)=\log\dfrac{r^2}{R^2} $$ and I get $$ -2\pi\nabla_xN(x,y)=\frac{2}{r^2}(x-y) $$ and $\nu_x=x/R$. What goes wrong with my calculation here?",,['multivariable-calculus']
44,What are the steps to this functional derivative problem?,What are the steps to this functional derivative problem?,,"I'm trying to derive equations from Matthew Beal's Thesis, Variational Algorithms for Approximate Bayesian Inference pg.55, but I'm stuck on one of the equations (well I'm stuck on a lot of equations actually). This is the equation I would like help on: $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln \frac{P(x,y)}{q_x(x)}\tag{1}$$ The equation is suppose to equate to $$\ln P(x,y)-\ln q_x(x)-1\tag{2}$$ Of course, (1) can be expanded to $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)(\ln P(x,y)-\ln q_x(x))$$ and then once more into $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln P(x,y)-\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln q_x(x)$$ I'm guessing $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln P(x,y)=\ln P(x,y)$$ and $$-\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln q_x(x)=-\ln q_x(x)-1$$  But why? Any help would be appreciated. The equations were also simplified from their original complexity.","I'm trying to derive equations from Matthew Beal's Thesis, Variational Algorithms for Approximate Bayesian Inference pg.55, but I'm stuck on one of the equations (well I'm stuck on a lot of equations actually). This is the equation I would like help on: $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln \frac{P(x,y)}{q_x(x)}\tag{1}$$ The equation is suppose to equate to $$\ln P(x,y)-\ln q_x(x)-1\tag{2}$$ Of course, (1) can be expanded to $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)(\ln P(x,y)-\ln q_x(x))$$ and then once more into $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln P(x,y)-\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln q_x(x)$$ I'm guessing $$\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln P(x,y)=\ln P(x,y)$$ and $$-\frac{\delta}{\delta q_x(x)} \int dxq_x(x)\ln q_x(x)=-\ln q_x(x)-1$$  But why? Any help would be appreciated. The equations were also simplified from their original complexity.",,"['calculus', 'integration', 'multivariable-calculus', 'derivatives', 'self-learning']"
45,Lie Derivative of a section on a vector bundle,Lie Derivative of a section on a vector bundle,,"I'm still trying to figure out how to do the Lie derivative of a Jacobian. (c.f. earlier unanswered post ). If I know how how to do Lie derivatives on section of vector bundles, that would be sufficient. In my case the vector bundle $\pi:E\rightarrow X$ is given by the dual of the pullback bundle $E=(\phi^*TY)^*$, where $\phi:X\rightarrow Y$ is a smooth map between smooth manifolds. So let $\alpha$ be a section of that bundle. How do I define $\mathcal L_v\alpha$ ? And it would be helpful also to know it in local coordinates. The reason why this would help me to define the Lie derivative of the Jacobian is that the Jacobian is a linear map from $TX\otimes (\pi^*TY)^*$ to $\mathbb R$, i.e. \begin{equation} (w,\alpha)_x\mapsto \langle D\phi\, w,\alpha\rangle_x\; \end{equation} where locally $w=\sum_iw^i(x)\frac{\partial}{\partial x^i}$ and $\alpha=\sum_b\alpha^b(x)\,dy^b$ ($x^i$ a coords on $X$ and $y^b$ are coords on $Y$) and the pairing is $\langle\cdot,\cdot\rangle_x: T_{\phi(x)}Y \times T^*_{\phi(x)}Y\rightarrow \mathbb R$. Then I could define $\big(\mathcal L_vD\phi\big) (w,\alpha)=\mathcal L_v \big(D\phi(w,\alpha)\big)-D\phi(\mathcal L_v w,\alpha)-D\phi(w,\mathcal L_v\alpha)$, knowing $\mathcal L_v\alpha$.","I'm still trying to figure out how to do the Lie derivative of a Jacobian. (c.f. earlier unanswered post ). If I know how how to do Lie derivatives on section of vector bundles, that would be sufficient. In my case the vector bundle $\pi:E\rightarrow X$ is given by the dual of the pullback bundle $E=(\phi^*TY)^*$, where $\phi:X\rightarrow Y$ is a smooth map between smooth manifolds. So let $\alpha$ be a section of that bundle. How do I define $\mathcal L_v\alpha$ ? And it would be helpful also to know it in local coordinates. The reason why this would help me to define the Lie derivative of the Jacobian is that the Jacobian is a linear map from $TX\otimes (\pi^*TY)^*$ to $\mathbb R$, i.e. \begin{equation} (w,\alpha)_x\mapsto \langle D\phi\, w,\alpha\rangle_x\; \end{equation} where locally $w=\sum_iw^i(x)\frac{\partial}{\partial x^i}$ and $\alpha=\sum_b\alpha^b(x)\,dy^b$ ($x^i$ a coords on $X$ and $y^b$ are coords on $Y$) and the pairing is $\langle\cdot,\cdot\rangle_x: T_{\phi(x)}Y \times T^*_{\phi(x)}Y\rightarrow \mathbb R$. Then I could define $\big(\mathcal L_vD\phi\big) (w,\alpha)=\mathcal L_v \big(D\phi(w,\alpha)\big)-D\phi(\mathcal L_v w,\alpha)-D\phi(w,\mathcal L_v\alpha)$, knowing $\mathcal L_v\alpha$.",,"['multivariable-calculus', 'differential-geometry', 'vector-bundles', 'lie-derivative']"
46,"Determine the volume of $A:=\{(x,y,z)\in \mathbb R^3 : \sqrt{x^2+y^2}\leq f(z)\}$",Determine the volume of,"A:=\{(x,y,z)\in \mathbb R^3 : \sqrt{x^2+y^2}\leq f(z)\}","Let $f\in L^2(\mathbb R)$ and $f\geq0$. Determine $A:=\{(x,y,z)\in \mathbb R^3 : \sqrt{x^2+y^2}\leq f(z)\}$. ""Normal"" substitution $(x=rcos(\phi),y=rsin(\phi))$ did not help a lot, since I dont have any information about f(z)","Let $f\in L^2(\mathbb R)$ and $f\geq0$. Determine $A:=\{(x,y,z)\in \mathbb R^3 : \sqrt{x^2+y^2}\leq f(z)\}$. ""Normal"" substitution $(x=rcos(\phi),y=rsin(\phi))$ did not help a lot, since I dont have any information about f(z)",,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
47,Finding the unit normal vector,Finding the unit normal vector,,"Q. Consider the following vector function. $$ r(t)= \langle 6\sqrt{2}t,e^{6t},e^{-6t} \rangle $$ Find the unit tangent and unit normal vectors T(t) and N(t). I found $$T(t)= \frac{1}{\sqrt{2+e^{12t}+e^{-12t}}}\langle \sqrt{2},e^{6t},-e^{-6t}\rangle $$ but when I try finding $N(t)=T'(t)/|T'(t)|$ the calculations just go out of hand and I cannot reach an answer. Can someone please help me in finding the answer.","Q. Consider the following vector function. $$ r(t)= \langle 6\sqrt{2}t,e^{6t},e^{-6t} \rangle $$ Find the unit tangent and unit normal vectors T(t) and N(t). I found $$T(t)= \frac{1}{\sqrt{2+e^{12t}+e^{-12t}}}\langle \sqrt{2},e^{6t},-e^{-6t}\rangle $$ but when I try finding $N(t)=T'(t)/|T'(t)|$ the calculations just go out of hand and I cannot reach an answer. Can someone please help me in finding the answer.",,"['calculus', 'multivariable-calculus', 'differential-geometry', '3d']"
48,Independence of path in a closed curve line integral,Independence of path in a closed curve line integral,,"Let $f(t)$ be a continuous function. Let $C$ be a smooth closed curve. Show that $$\oint\limits_C xf(x^2 + y^2)\,dx + y f(x^2 + y^2)\,dy = 0$$ Hint : Remember that $f(t)$ has a primitive function $F(t)$. Use this fact to construct a potential function for the vector field. If we prove that the vector field $F(x,y) = (x \cdot f(x^2 + y^2),y \cdot f(x^2 + y^2))$ is conservative in its domain, then we can use the fact $$\int\limits_C {F \cdot dr} = \varphi (r(b)) - \varphi (r(a)) = 0$$ to express the initial line integral as a potential function difference between two points on a closed smooth curve, which would imply that any path taken between these two points would yield the same integral value. Now in order to show that the given vector field is conservative we need to find its potential, which means we have to calculate $$\frac{d\varphi}{dx} = x \cdot f(x^2 + y^2)\text{ and }\frac{d\varphi}{dy} = y \cdot f(x^2 + y^2)$$ How can we find a potential function $\varphi (x,y)$ if we do not know the form of the function $f(x^2 + y^2)$? If we replace the argument with, say, $t = {x^2} + {y^2}$ we would need to calculate the integrals $$\frac{d\varphi}{dx} = x \cdot f(t)\text{ and }\frac{d\varphi}{dy} = y \cdot f(t)$$ but we cannot treat $f(t)$ as constant since its dependent on both variables $x$ and $y$. What i had in mind is to show that due to symmetry $F(x,y) = F(y,x)$, which implies that a closed loop is cut evenly by two curves ${C_1}$ and ${C_2}$, their sum defines the closed loop region. $F$ is conservative vector of the field if $$\frac{d}{dy} (x \cdot f(x^2 + y^2)) = 2xyf' = \frac{d}{dx} ( y \cdot f(x^2 + y^2))$$ Now we know that the field is conservative, which implies $F = \nabla \varphi $ for some scalar potential function $\varphi $ defined over the closed loop. Therefore, $$F \cdot dr = \left( \left( \frac{d\varphi}{dx} \right)i + \left( \frac{d\varphi}{dy} \right)j \right) \cdot \left( {dxi + dyj} \right) = \frac{{d\varphi }}{{dx}}dx + \frac{{d\varphi }}{{dy}}dy = d\varphi $$ Since $C$ is a continuous smooth, closed curve, parametrized, say, by $r = r(t),\,\,\,a \le t \le b$ then $r(a) = r(b)$ and $$\int\limits_C F \cdot dr = \int\limits_a^b \frac{d\varphi (r(t))}{dt} \, dt = \varphi (r(b)) - \varphi (r(a)) = 0$$ However, i don't think that it is sufficient to claim that the given vector field is conservative based on the equality of partial derivative of its components. We still need to find a potential function i think. For example, if instead of $F(x,y) = (x \cdot f(x^2 + y^2),y \cdot f(x^2 + y^2))$ we were given a vector field, say, $F(x,y) = (x \cdot ({x^2} + {y^2}),y \cdot (x^2 + y^2))$ we could easily find its potential to be $\varphi (x,y) = \frac{x^4}{4} + \frac{x^2 y^2}{2} + \frac{y^4}{4} = \frac{1}{4} (x^2 + y^2 )^2$. Now we can take any smooth closed curve, parametrize it and show that the potential at any two points on the curve is the same, hence its difference is zero. For example, in our case we can take a unit circle and parametrize it as $x = \cos t \text{ and }y = \sin t$. Now we just chose two random points on the curve, say, $P_1(1,0) \text{ and } P_2 (0,1)$ and show that $$\oint\limits_C x(x^2 + y^2)\,dx + y(x^2 + y^2)\,dy = \left[ \frac{1}{4} ( x^2 + y^2)^2 \right]_{(1,0)}^{(0,1)} = \frac{1}{4} - \frac{1}{4} = 0$$ How to deal when we are given $f(x^2 + y^2)$ instead of a function itself?","Let $f(t)$ be a continuous function. Let $C$ be a smooth closed curve. Show that $$\oint\limits_C xf(x^2 + y^2)\,dx + y f(x^2 + y^2)\,dy = 0$$ Hint : Remember that $f(t)$ has a primitive function $F(t)$. Use this fact to construct a potential function for the vector field. If we prove that the vector field $F(x,y) = (x \cdot f(x^2 + y^2),y \cdot f(x^2 + y^2))$ is conservative in its domain, then we can use the fact $$\int\limits_C {F \cdot dr} = \varphi (r(b)) - \varphi (r(a)) = 0$$ to express the initial line integral as a potential function difference between two points on a closed smooth curve, which would imply that any path taken between these two points would yield the same integral value. Now in order to show that the given vector field is conservative we need to find its potential, which means we have to calculate $$\frac{d\varphi}{dx} = x \cdot f(x^2 + y^2)\text{ and }\frac{d\varphi}{dy} = y \cdot f(x^2 + y^2)$$ How can we find a potential function $\varphi (x,y)$ if we do not know the form of the function $f(x^2 + y^2)$? If we replace the argument with, say, $t = {x^2} + {y^2}$ we would need to calculate the integrals $$\frac{d\varphi}{dx} = x \cdot f(t)\text{ and }\frac{d\varphi}{dy} = y \cdot f(t)$$ but we cannot treat $f(t)$ as constant since its dependent on both variables $x$ and $y$. What i had in mind is to show that due to symmetry $F(x,y) = F(y,x)$, which implies that a closed loop is cut evenly by two curves ${C_1}$ and ${C_2}$, their sum defines the closed loop region. $F$ is conservative vector of the field if $$\frac{d}{dy} (x \cdot f(x^2 + y^2)) = 2xyf' = \frac{d}{dx} ( y \cdot f(x^2 + y^2))$$ Now we know that the field is conservative, which implies $F = \nabla \varphi $ for some scalar potential function $\varphi $ defined over the closed loop. Therefore, $$F \cdot dr = \left( \left( \frac{d\varphi}{dx} \right)i + \left( \frac{d\varphi}{dy} \right)j \right) \cdot \left( {dxi + dyj} \right) = \frac{{d\varphi }}{{dx}}dx + \frac{{d\varphi }}{{dy}}dy = d\varphi $$ Since $C$ is a continuous smooth, closed curve, parametrized, say, by $r = r(t),\,\,\,a \le t \le b$ then $r(a) = r(b)$ and $$\int\limits_C F \cdot dr = \int\limits_a^b \frac{d\varphi (r(t))}{dt} \, dt = \varphi (r(b)) - \varphi (r(a)) = 0$$ However, i don't think that it is sufficient to claim that the given vector field is conservative based on the equality of partial derivative of its components. We still need to find a potential function i think. For example, if instead of $F(x,y) = (x \cdot f(x^2 + y^2),y \cdot f(x^2 + y^2))$ we were given a vector field, say, $F(x,y) = (x \cdot ({x^2} + {y^2}),y \cdot (x^2 + y^2))$ we could easily find its potential to be $\varphi (x,y) = \frac{x^4}{4} + \frac{x^2 y^2}{2} + \frac{y^4}{4} = \frac{1}{4} (x^2 + y^2 )^2$. Now we can take any smooth closed curve, parametrize it and show that the potential at any two points on the curve is the same, hence its difference is zero. For example, in our case we can take a unit circle and parametrize it as $x = \cos t \text{ and }y = \sin t$. Now we just chose two random points on the curve, say, $P_1(1,0) \text{ and } P_2 (0,1)$ and show that $$\oint\limits_C x(x^2 + y^2)\,dx + y(x^2 + y^2)\,dy = \left[ \frac{1}{4} ( x^2 + y^2)^2 \right]_{(1,0)}^{(0,1)} = \frac{1}{4} - \frac{1}{4} = 0$$ How to deal when we are given $f(x^2 + y^2)$ instead of a function itself?",,['multivariable-calculus']
49,Proof of transformation law for double integrals,Proof of transformation law for double integrals,,"The second volume of Apostol's Calculus seems rather circumspect in its discussion of the change of variables formula for double integrals.  Section 11.29 offers a proof under the following very limited circumstances: Let $R$ be a rectangle, $R^*$ its image under a one-to-one mapping $u = U(x,y)$, $v = V(x,y)$, with the inverse mapping given by $x = X(u,v)$, $y = Y(u,v)$.  Assume that both $X$ and $Y$ have continuous second-order partial derivatives, and that the Jacobian determinant $J(u,v)$ is never $0$ in $R^*$.  Then \begin{align} \iint\limits_R dx\, dy & = \iint\limits_{R^*}|J(u,v)| \,dx\,dy \end{align} The proof Apostol gives is a straightforward application of Green's Theorem, except for one small issue:  let $C$ be the boundary of $R$, $C^*$ be the boundary of $R^*$, and parameterize $C^*$ by  \begin{align} \mathbf{\alpha}(t) = W(t)\mathbf{i} + Z(t)\mathbf{j} \end{align} with $t$ on some interval $[a,b]$.  Apostol then asserts that \begin{align} \mathbf{\beta}(t) = X[W(t),Z(t)]\mathbf{i} + Y[W(t),Z(t)]\mathbf{j} \end{align} represents a parameterization of $C$.  I'm afraid I don't understand the basis for this assertion.  The continuity of $X$ and $Y$ along with the one-to-oneness of the map $x = X(u,v), y = Y(u,v)$ would seem to guarantee that $\mathbf{\beta}(t)$ is a piecewise smooth closed path within $R$, but it is unclear to me that we have boundaries mapping to boundaries.  Is there some way to show that this is the case? (Perhaps not so coincidentally, the proof that Munkres gives in his supplemental notes on MIT's OpenCourseware site is almost identical, except that in his statement of the theorem that $C$ maps to $C^*$ is an explicit hypothesis.)","The second volume of Apostol's Calculus seems rather circumspect in its discussion of the change of variables formula for double integrals.  Section 11.29 offers a proof under the following very limited circumstances: Let $R$ be a rectangle, $R^*$ its image under a one-to-one mapping $u = U(x,y)$, $v = V(x,y)$, with the inverse mapping given by $x = X(u,v)$, $y = Y(u,v)$.  Assume that both $X$ and $Y$ have continuous second-order partial derivatives, and that the Jacobian determinant $J(u,v)$ is never $0$ in $R^*$.  Then \begin{align} \iint\limits_R dx\, dy & = \iint\limits_{R^*}|J(u,v)| \,dx\,dy \end{align} The proof Apostol gives is a straightforward application of Green's Theorem, except for one small issue:  let $C$ be the boundary of $R$, $C^*$ be the boundary of $R^*$, and parameterize $C^*$ by  \begin{align} \mathbf{\alpha}(t) = W(t)\mathbf{i} + Z(t)\mathbf{j} \end{align} with $t$ on some interval $[a,b]$.  Apostol then asserts that \begin{align} \mathbf{\beta}(t) = X[W(t),Z(t)]\mathbf{i} + Y[W(t),Z(t)]\mathbf{j} \end{align} represents a parameterization of $C$.  I'm afraid I don't understand the basis for this assertion.  The continuity of $X$ and $Y$ along with the one-to-oneness of the map $x = X(u,v), y = Y(u,v)$ would seem to guarantee that $\mathbf{\beta}(t)$ is a piecewise smooth closed path within $R$, but it is unclear to me that we have boundaries mapping to boundaries.  Is there some way to show that this is the case? (Perhaps not so coincidentally, the proof that Munkres gives in his supplemental notes on MIT's OpenCourseware site is almost identical, except that in his statement of the theorem that $C$ maps to $C^*$ is an explicit hypothesis.)",,['multivariable-calculus']
50,What does it mean for a function to increase along a curve?,What does it mean for a function to increase along a curve?,,"I think that if we were to say that, for instance, $y$ increases along the curve, (with no specific rate) then this means for the derivative to simply be positive. Or does it mean to choose the positive value of a square root? I'm given the functions $x = y^2 + 2$, and $z=x+4$. I simply parametrized $t=y$ and had $\langle t^2 + 2, t, t^2 + 6\rangle$. However, I could have put $ y = \sqrt{x - 2}$ parametrizing $t=x$ to have $\langle t,\sqrt{x - 2}, t + 4 \rangle$. I don't get the same derivative obviously, and despite using the same point, which gives me different $t$ values, I don't get the same tangent vector. So does which parametrization I choose depend on the statement for $y$ to increase along a curve? I don't think it should though.","I think that if we were to say that, for instance, $y$ increases along the curve, (with no specific rate) then this means for the derivative to simply be positive. Or does it mean to choose the positive value of a square root? I'm given the functions $x = y^2 + 2$, and $z=x+4$. I simply parametrized $t=y$ and had $\langle t^2 + 2, t, t^2 + 6\rangle$. However, I could have put $ y = \sqrt{x - 2}$ parametrizing $t=x$ to have $\langle t,\sqrt{x - 2}, t + 4 \rangle$. I don't get the same derivative obviously, and despite using the same point, which gives me different $t$ values, I don't get the same tangent vector. So does which parametrization I choose depend on the statement for $y$ to increase along a curve? I don't think it should though.",,"['calculus', 'multivariable-calculus']"
51,"To check if (0,0) is local minima for$F ( x, y) = x (x - 2y^{2}) $","To check if (0,0) is local minima for","F ( x, y) = x (x - 2y^{2}) ","Hello Thanks for your time $F ( x, y) = x (x - 2y^{2}) $ . I have applied second derivative test which does not give any result . By looking at function i see that  when x is greater than  $2y^{2} $ , f is positive otherwise negative  , So according to me should be saddle point  . But textbook states it is minima .I need help.So much Thanks .So long !","Hello Thanks for your time $F ( x, y) = x (x - 2y^{2}) $ . I have applied second derivative test which does not give any result . By looking at function i see that  when x is greater than  $2y^{2} $ , f is positive otherwise negative  , So according to me should be saddle point  . But textbook states it is minima .I need help.So much Thanks .So long !",,['multivariable-calculus']
52,Nature of stationary points,Nature of stationary points,,"I have $$f(x_1,x_2) = 2x^4_1 + 2x_1x_2 + 2x_1 + (1+x_2)^2$$ How can I determine the nature of the stationary points ? I know; $$f_{x_1,x_1}(x) = 24x_1^2$$ $$f_{x_2,x_2}(x) = 2$$ $$f_{x_1,x_2}(x) = 2$$ This gives me a hessian: $$ \nabla^2 f(x) =   \left[   \begin{array}{ c c }      24x_1^2 & 2 \\      2 & 2   \end{array} \right] $$ By examining the leading principal minors; $$D_1 = \| (24x_1^2)\| \geq 0$$ and $$D_2 =  \det\left|\array{24x_1^2&2\\2&2}\right|  = 48x_1^2 -4$$ Since I have no condition on $x_1^2$ then $D_2$ can be positive, negative or even 0. So then does this mean this matrix is indefinite, and hence all the stationary points here are saddle points? Have I made a mistake in the Hessian? I feel the $24x_1^2$ term is out of place. any help is VERY MUCH appreciated.","I have $$f(x_1,x_2) = 2x^4_1 + 2x_1x_2 + 2x_1 + (1+x_2)^2$$ How can I determine the nature of the stationary points ? I know; $$f_{x_1,x_1}(x) = 24x_1^2$$ $$f_{x_2,x_2}(x) = 2$$ $$f_{x_1,x_2}(x) = 2$$ This gives me a hessian: $$ \nabla^2 f(x) =   \left[   \begin{array}{ c c }      24x_1^2 & 2 \\      2 & 2   \end{array} \right] $$ By examining the leading principal minors; $$D_1 = \| (24x_1^2)\| \geq 0$$ and $$D_2 =  \det\left|\array{24x_1^2&2\\2&2}\right|  = 48x_1^2 -4$$ Since I have no condition on $x_1^2$ then $D_2$ can be positive, negative or even 0. So then does this mean this matrix is indefinite, and hence all the stationary points here are saddle points? Have I made a mistake in the Hessian? I feel the $24x_1^2$ term is out of place. any help is VERY MUCH appreciated.",,"['multivariable-calculus', 'optimization']"
53,Improper integral (is it convergent?) (v 2.0),Improper integral (is it convergent?) (v 2.0),,"Earlier today I asked about this question: Improper integral (is it convergent?) where the integral fortunately seems to be convergent. So we have that given $\alpha\in (-1/2,0)$ there is a $\gamma \in (1,2)$ such that $$\int_0^1 \int_0^{u} \frac{((1-v)^{\alpha}-(1-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ My question now is, if we take a middle value $a\in (0,1)$ is then the following integral also finite? $$\int_0^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ Of course a naiv start would be to split up the integral as follows: \begin{align*} \int_0^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu =& \int_0^a \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu\\ &+ \int_a^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu\\ &=: (A) + (B). \end{align*} Here, $(A)$ is essentially the same as the one on top and therefore convergent. So the question is equivalent to proving that $(B)$ is either convergent or divergent. What are your feelings? Should the integral above still be convergent for any values $a\in (0,1)$ and not only when $a=1$? Any impressions? or ideas on how to prove it? Thanks a lot guys!","Earlier today I asked about this question: Improper integral (is it convergent?) where the integral fortunately seems to be convergent. So we have that given $\alpha\in (-1/2,0)$ there is a $\gamma \in (1,2)$ such that $$\int_0^1 \int_0^{u} \frac{((1-v)^{\alpha}-(1-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ My question now is, if we take a middle value $a\in (0,1)$ is then the following integral also finite? $$\int_0^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ Of course a naiv start would be to split up the integral as follows: \begin{align*} \int_0^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu =& \int_0^a \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu\\ &+ \int_a^1 \int_0^{u} \frac{((a-v)^{\alpha}-(a-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu\\ &=: (A) + (B). \end{align*} Here, $(A)$ is essentially the same as the one on top and therefore convergent. So the question is equivalent to proving that $(B)$ is either convergent or divergent. What are your feelings? Should the integral above still be convergent for any values $a\in (0,1)$ and not only when $a=1$? Any impressions? or ideas on how to prove it? Thanks a lot guys!",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals']"
54,Finding extreme value of a multivariable function,Finding extreme value of a multivariable function,,"I am trying to find the extreme value of this function $$f(x_1,\ldots,x_n) = x_1x_2^2\cdots x_n^n(1-x_1-2x_2 - \cdots - nx_n),$$ with $x_1,\ldots,x_n > 0$. I computed the partial derivatives and set them equal to $0$ which gives me $x_i = 1$ so the extreme value is $1- n(n+1)/2$ but when I plot for the case $n=2$ it doesn't look like the extreme value occur at $x_1 = x_2 = 1$","I am trying to find the extreme value of this function $$f(x_1,\ldots,x_n) = x_1x_2^2\cdots x_n^n(1-x_1-2x_2 - \cdots - nx_n),$$ with $x_1,\ldots,x_n > 0$. I computed the partial derivatives and set them equal to $0$ which gives me $x_i = 1$ so the extreme value is $1- n(n+1)/2$ but when I plot for the case $n=2$ it doesn't look like the extreme value occur at $x_1 = x_2 = 1$",,['multivariable-calculus']
55,Evaluating an integral over a curve,Evaluating an integral over a curve,,"I am having difficulty integrating $\int (y + \sin(e^{x^2})) \, dx - 2x \, dy$, over the circle $x^2 + y^2 = 1$, traversed anti-clockwise. I managed to do the differentiation and convert everything in terms of t, but I reach this step in the integration process, $-3\pi - \int_0^{2\pi} \sin(e^{\cos^2 t}) \sin(t) \, dt$, and then I don't know how to proceed from here. I tried integrating by parts but it doesn't seem to work. Does anyone know how I should solve this problem? Thanks so much!","I am having difficulty integrating $\int (y + \sin(e^{x^2})) \, dx - 2x \, dy$, over the circle $x^2 + y^2 = 1$, traversed anti-clockwise. I managed to do the differentiation and convert everything in terms of t, but I reach this step in the integration process, $-3\pi - \int_0^{2\pi} \sin(e^{\cos^2 t}) \sin(t) \, dt$, and then I don't know how to proceed from here. I tried integrating by parts but it doesn't seem to work. Does anyone know how I should solve this problem? Thanks so much!",,"['integration', 'multivariable-calculus', 'line-integrals']"
56,Proving this function is an open map,Proving this function is an open map,,"Prove the function $f(x, y, z) = (x^3, y^2-z^2, yz)$ is an open map from $\mathbb{R^3}$ to $\mathbb{R^3}$ (i.e for every open set $U$ of $\mathbb{R^3}$, $f(U)$ is open). I know, as an application of the inverse function theorem, that if the determinant of the Jacobian is non-zero in every point of the domain, then $f$ is an open map. However, the determinant of the Jacobian of $f$ is: $3x(2y^2+2z^2) = 0 \iff x = 0 $ or $(y, z)  = (0, 0)$ I've also tried the following. If $g(x, y, z) = (x^3, y, z)$ and $h(x, y, z) = (x, y^2 - z^2, yz)$, then $f = h \circ g$. Since the composition of open maps is an open map, and $g$ is clearly open since it is bijective, then all that is there left to prove is that $h$ is an open map. Again, the only way I know of proving a map is open is either by definition, or using the inverse function theorem. Are there any other useful and simple results that provide sufficient conditions for a map to be open?","Prove the function $f(x, y, z) = (x^3, y^2-z^2, yz)$ is an open map from $\mathbb{R^3}$ to $\mathbb{R^3}$ (i.e for every open set $U$ of $\mathbb{R^3}$, $f(U)$ is open). I know, as an application of the inverse function theorem, that if the determinant of the Jacobian is non-zero in every point of the domain, then $f$ is an open map. However, the determinant of the Jacobian of $f$ is: $3x(2y^2+2z^2) = 0 \iff x = 0 $ or $(y, z)  = (0, 0)$ I've also tried the following. If $g(x, y, z) = (x^3, y, z)$ and $h(x, y, z) = (x, y^2 - z^2, yz)$, then $f = h \circ g$. Since the composition of open maps is an open map, and $g$ is clearly open since it is bijective, then all that is there left to prove is that $h$ is an open map. Again, the only way I know of proving a map is open is either by definition, or using the inverse function theorem. Are there any other useful and simple results that provide sufficient conditions for a map to be open?",,"['real-analysis', 'general-topology', 'analysis', 'multivariable-calculus', 'inverse']"
57,Double integral: How to switch to polar coordinates with a difficult domain,Double integral: How to switch to polar coordinates with a difficult domain,,"i have this double integral: $$ I=\int \int_{R} (x+y),\;\;  R=\left \{ (x,y):\frac{x^{2}}{3} \leq y\leq 3,\; -1\leq x\leq 3\right \} $$ and this is the domain of integration NOT in polar coordinates: i don't see any radial simmetry, so how can i switch to polar coordinates? EDIT: the question is: What's the best way to handle such problems, when you're asked to switch in polar coordinates but with a ""not-radial like"" domain?","i have this double integral: $$ I=\int \int_{R} (x+y),\;\;  R=\left \{ (x,y):\frac{x^{2}}{3} \leq y\leq 3,\; -1\leq x\leq 3\right \} $$ and this is the domain of integration NOT in polar coordinates: i don't see any radial simmetry, so how can i switch to polar coordinates? EDIT: the question is: What's the best way to handle such problems, when you're asked to switch in polar coordinates but with a ""not-radial like"" domain?",,['multivariable-calculus']
58,Calculate the surface integral $\iint_S (\nabla \times F)\cdot dS$ over a part of a sphere,Calculate the surface integral  over a part of a sphere,\iint_S (\nabla \times F)\cdot dS,"How can I calculate the integral $$\iint_S (\nabla \times F)\cdot dS$$ where $S$  is the part of the surface of the sphere $x^2+y^2+z^2=1$ and $x+y+z\ge 1$, $F=(y-z, z-x, x-y)$. I calculated that $\nabla\times F=(-2 , -2 ,-2)$. It's difficult for me to find the section between the sphere and the plane. Also, I can't calculate the integral. Update : I found that the result is: $-6π/3^{1/2}$. Ιs it correct?","How can I calculate the integral $$\iint_S (\nabla \times F)\cdot dS$$ where $S$  is the part of the surface of the sphere $x^2+y^2+z^2=1$ and $x+y+z\ge 1$, $F=(y-z, z-x, x-y)$. I calculated that $\nabla\times F=(-2 , -2 ,-2)$. It's difficult for me to find the section between the sphere and the plane. Also, I can't calculate the integral. Update : I found that the result is: $-6π/3^{1/2}$. Ιs it correct?",,"['calculus', 'multivariable-calculus', 'vector-analysis', 'surface-integrals']"
59,Calculation of second order partial derivatives,Calculation of second order partial derivatives,,"Let $$F\left( \frac{x}{z}, \frac{y}{z} \right)=0$$ determine a function $z=z(x,y)$. Show that $$\frac{\partial^2z}{\partial x^2} \cdot \frac{\partial^2z}{\partial y^2}-\left( \frac{\partial^2z}{\partial x\partial y} \right)^2=0.$$ I was frustrated in simplifying doing the second partial derivatives...","Let $$F\left( \frac{x}{z}, \frac{y}{z} \right)=0$$ determine a function $z=z(x,y)$. Show that $$\frac{\partial^2z}{\partial x^2} \cdot \frac{\partial^2z}{\partial y^2}-\left( \frac{\partial^2z}{\partial x\partial y} \right)^2=0.$$ I was frustrated in simplifying doing the second partial derivatives...",,"['multivariable-calculus', 'partial-derivative']"
60,Tensor Calculus Second Order Derivatives,Tensor Calculus Second Order Derivatives,,"I'm learning tensor calculus by myself through lectures and texts, and I'm presented with the problem of finding the first and second order derivatives of a scalar function of three variables that vary with two parameters. I label the function $f(x^i(\mu^{\alpha}))$ where $i$ runs from one to three and $\alpha$ runs from one to two, setting the $x^i$ as the variables that change with the parameters $\mu^{\alpha}$. Geometrically this represents a surface in $\mathbb{R}^3$. I also set forth the convention for this problem will be to take Latin indices to run from one to three ( coordinates ) and Greek indicies to run from one to two ( parameters ). I first begin by finding the general form of the first derivatives of $f$ with respect to arbitrary coordinates. Brute forcing them with multivariate calculus I get: $$ \frac{\partial f}{\partial \mu^1} = \frac{\partial f}{\partial x^1} \frac{\partial x^1}{\partial \mu^1} + \frac{\partial f}{\partial x^2} \frac{\partial x^2}{\partial \mu^1} + \frac{\partial f}{\partial x^3} \frac{\partial x^3}{\partial \mu^1} $$ The same follows for differentiation w.r.t. the second parameter as well. The pattern makes itself clear and allows me to combine both of those derivatives into the single statement: $$\frac{\partial f}{\partial \mu^{\alpha}} = \frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}$$ following the Einstein Summation Convention. As for the second derivative, I attempted to carry on the tensor notation and not derive results from multivariable calculus. Beginning with a first rank, covariant tensor, I take the derivative of the previous expression with respect to the parameters again, but using a different index to include all possible second order derivative combos. In symbols: $$ \frac{\partial}{\partial \mu^{\beta}}( \frac{\partial f}{\partial \mu^{\alpha}}) = \frac{\partial}{\partial \mu^{\beta}}(\frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}) $$ Applying the chain rule--if done correctly--I get $$ \frac{\partial^2 f}{\partial \mu^{\alpha} \partial \mu^{\beta}} = \frac{\partial^2 f}{\partial x^i \partial x^j}\frac{\partial x^i}{\partial \mu^{\beta}}\frac{\partial x^j}{\partial \mu^{\alpha}} + \frac{\partial f}{\partial x^i} \frac{\partial^2 x^i}{\partial \mu^{\alpha} \partial \mu^{\beta}} $$ a second order covariant tensor. Is this correct?","I'm learning tensor calculus by myself through lectures and texts, and I'm presented with the problem of finding the first and second order derivatives of a scalar function of three variables that vary with two parameters. I label the function $f(x^i(\mu^{\alpha}))$ where $i$ runs from one to three and $\alpha$ runs from one to two, setting the $x^i$ as the variables that change with the parameters $\mu^{\alpha}$. Geometrically this represents a surface in $\mathbb{R}^3$. I also set forth the convention for this problem will be to take Latin indices to run from one to three ( coordinates ) and Greek indicies to run from one to two ( parameters ). I first begin by finding the general form of the first derivatives of $f$ with respect to arbitrary coordinates. Brute forcing them with multivariate calculus I get: $$ \frac{\partial f}{\partial \mu^1} = \frac{\partial f}{\partial x^1} \frac{\partial x^1}{\partial \mu^1} + \frac{\partial f}{\partial x^2} \frac{\partial x^2}{\partial \mu^1} + \frac{\partial f}{\partial x^3} \frac{\partial x^3}{\partial \mu^1} $$ The same follows for differentiation w.r.t. the second parameter as well. The pattern makes itself clear and allows me to combine both of those derivatives into the single statement: $$\frac{\partial f}{\partial \mu^{\alpha}} = \frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}$$ following the Einstein Summation Convention. As for the second derivative, I attempted to carry on the tensor notation and not derive results from multivariable calculus. Beginning with a first rank, covariant tensor, I take the derivative of the previous expression with respect to the parameters again, but using a different index to include all possible second order derivative combos. In symbols: $$ \frac{\partial}{\partial \mu^{\beta}}( \frac{\partial f}{\partial \mu^{\alpha}}) = \frac{\partial}{\partial \mu^{\beta}}(\frac{\partial f}{\partial x^i} \frac{\partial x^i}{\partial \mu^{\alpha}}) $$ Applying the chain rule--if done correctly--I get $$ \frac{\partial^2 f}{\partial \mu^{\alpha} \partial \mu^{\beta}} = \frac{\partial^2 f}{\partial x^i \partial x^j}\frac{\partial x^i}{\partial \mu^{\beta}}\frac{\partial x^j}{\partial \mu^{\alpha}} + \frac{\partial f}{\partial x^i} \frac{\partial^2 x^i}{\partial \mu^{\alpha} \partial \mu^{\beta}} $$ a second order covariant tensor. Is this correct?",,"['multivariable-calculus', 'tensors']"
61,Constructing functions such that integral along any closed curve is non-zero,Constructing functions such that integral along any closed curve is non-zero,,"Consider smooth maps $f: \mathbb R^2 \setminus \{(0,0)\} \to \mathbb R$. How can I construct such an $f$ with the property that $$ \oint_C f \neq 0$$ for any closed curve $C$ around the origin? Note that I am aware that $\frac{xdy-ydx}{x^2+y^2}$ has this property but knowing the answer is unhelpful to me as it does not tell me how to find such a function. Ideally, I would like to see an answer with $$ f \neq  \frac{xdy-ydx}{x^2+y^2}$$","Consider smooth maps $f: \mathbb R^2 \setminus \{(0,0)\} \to \mathbb R$. How can I construct such an $f$ with the property that $$ \oint_C f \neq 0$$ for any closed curve $C$ around the origin? Note that I am aware that $\frac{xdy-ydx}{x^2+y^2}$ has this property but knowing the answer is unhelpful to me as it does not tell me how to find such a function. Ideally, I would like to see an answer with $$ f \neq  \frac{xdy-ydx}{x^2+y^2}$$",,"['multivariable-calculus', 'differential-forms']"
62,"Finding the surface area $\iint_{s} f \, dS$ of $z=\sqrt{x^2+y^2}$ lying inside $x^2+y^2=x$",Finding the surface area  of  lying inside,"\iint_{s} f \, dS z=\sqrt{x^2+y^2} x^2+y^2=x","$z=\sqrt{x^2+y^2}$ is the surface we working on. I am a bit stuck on choosing the limits for this problem, I have done the following: $J(\text{jacobian})=\sqrt{Z^2_x+Z^2_y+1}=\left(\dfrac{x^2+y^2}{(\sqrt{x^2+y^2})^2}+1\right)^{\frac{1}{2}}=\sqrt{2}$ $$\iint_S f \, dS=\iint \sqrt{2} \, dz \, d\theta.$$ I can't explain why I chose $dz, d\theta$ ,  I guess since the one surface is bounded by $z$ and $x^2+y^2=x$ looks like the equation of a circle so $\theta$ :) Anyway, I know that $z \left[0:\sqrt{x^2+y^2} \right] \rightarrow \left[0:r \right]$ and $x^2+y^2=x\rightarrow r^2=r\cos(\theta)$ , I tried to solve for $\theta$ from that but no luck. Please assist -Thanks. I looked at this post here and it doesn't really address my issue.","is the surface we working on. I am a bit stuck on choosing the limits for this problem, I have done the following: I can't explain why I chose ,  I guess since the one surface is bounded by and looks like the equation of a circle so :) Anyway, I know that and , I tried to solve for from that but no luck. Please assist -Thanks. I looked at this post here and it doesn't really address my issue.","z=\sqrt{x^2+y^2} J(\text{jacobian})=\sqrt{Z^2_x+Z^2_y+1}=\left(\dfrac{x^2+y^2}{(\sqrt{x^2+y^2})^2}+1\right)^{\frac{1}{2}}=\sqrt{2} \iint_S f \, dS=\iint \sqrt{2} \, dz \, d\theta. dz, d\theta z x^2+y^2=x \theta z \left[0:\sqrt{x^2+y^2} \right] \rightarrow \left[0:r \right] x^2+y^2=x\rightarrow r^2=r\cos(\theta) \theta","['multivariable-calculus', 'surfaces', 'area', 'surface-integrals']"
63,What is the point in the total differential?,What is the point in the total differential?,,"The total differential is found by taking the limit of  $$\Delta f \approx f_x \Delta x+ f_y \Delta y$$ as $\Delta x, \Delta y \rightarrow 0$ to give $$df=f_x dx +f_y dy$$ Is this equation not just saying that $$0=f_x. 0 +f_y.0$$ If not please can you explain why not?","The total differential is found by taking the limit of  $$\Delta f \approx f_x \Delta x+ f_y \Delta y$$ as $\Delta x, \Delta y \rightarrow 0$ to give $$df=f_x dx +f_y dy$$ Is this equation not just saying that $$0=f_x. 0 +f_y.0$$ If not please can you explain why not?",,"['multivariable-calculus', 'partial-derivative']"
64,Gradient of the distance function,Gradient of the distance function,,"Let $\Omega$ be open, bounded subset of $\mathbb{R}^n$. Let $d(x):=dist(x,\partial\Omega)$ denotes the distance of the point $x\in\Omega$ from the boundary $\partial\Omega$. Define function $$\xi:\overline{\Omega}\to[0,1]; \quad \xi(x):=\min\left\{\frac{d(x)}{\delta},1\right\},$$ where $\delta\in (0,1]$ is fixed. I would like to prove that the following estimate for gradient holds: $$|\nabla\xi|\leqslant \frac{1}{\delta} \quad a.e.$$. So far I concluded that when $d(x)\geqslant \delta$, then $\xi(x)=1$ and therefore $\nabla\xi(x)=0$ so the inequality holds. If $d(x)<\delta$ then $\xi(x)=\frac{d(x)}{\delta}$  and the only thing I know is that distance as a Lipschitz function is differentiable a.e. by Rademacher's Theorem so at least I am allowed to differentiate it. Of course if $x\in\partial\Omega$, then $\xi(x)=0$, so the natural conclusion is that $\xi$ must somehow 'get small' when $x$ pass through $\delta$-band in $\Omega$ to reach boundary. Still it does not help me to get the estimate. I would be very pleased to see rigorous proof of this inequality. Thank you very much, K.","Let $\Omega$ be open, bounded subset of $\mathbb{R}^n$. Let $d(x):=dist(x,\partial\Omega)$ denotes the distance of the point $x\in\Omega$ from the boundary $\partial\Omega$. Define function $$\xi:\overline{\Omega}\to[0,1]; \quad \xi(x):=\min\left\{\frac{d(x)}{\delta},1\right\},$$ where $\delta\in (0,1]$ is fixed. I would like to prove that the following estimate for gradient holds: $$|\nabla\xi|\leqslant \frac{1}{\delta} \quad a.e.$$. So far I concluded that when $d(x)\geqslant \delta$, then $\xi(x)=1$ and therefore $\nabla\xi(x)=0$ so the inequality holds. If $d(x)<\delta$ then $\xi(x)=\frac{d(x)}{\delta}$  and the only thing I know is that distance as a Lipschitz function is differentiable a.e. by Rademacher's Theorem so at least I am allowed to differentiate it. Of course if $x\in\partial\Omega$, then $\xi(x)=0$, so the natural conclusion is that $\xi$ must somehow 'get small' when $x$ pass through $\delta$-band in $\Omega$ to reach boundary. Still it does not help me to get the estimate. I would be very pleased to see rigorous proof of this inequality. Thank you very much, K.",,"['multivariable-calculus', 'derivatives', 'metric-spaces']"
65,Writing in Cartesian tensor form,Writing in Cartesian tensor form,,"Write the following in Cartesian tensor form $$(1) \nabla (\operatorname{div} G) \times \nabla\Omega$$ $$(2) (\operatorname{curl}(F)\times G)\cdot \nabla(Φ)$$ I have answers for these two questions, however I do not understand how to show it step by step.","Write the following in Cartesian tensor form $$(1) \nabla (\operatorname{div} G) \times \nabla\Omega$$ $$(2) (\operatorname{curl}(F)\times G)\cdot \nabla(Φ)$$ I have answers for these two questions, however I do not understand how to show it step by step.",,"['multivariable-calculus', 'notation', 'tensors']"
66,"Using Lagrange multipliers to find the extrema of $f(x,y) = e^{2xy}$ subject to $x^2+y^2 = 16$",Using Lagrange multipliers to find the extrema of  subject to,"f(x,y) = e^{2xy} x^2+y^2 = 16","Find the maximum and minimum values of $f = e^{2xy}$ with respect to $x^2+y^2 = 16$. Using Lagrange multipliers, $\nabla f = \lambda\nabla g$. Therefore, the constraints are the following: $$x^2+y^2 = 16$$ $$2ye^{2xy} = \lambda2x$$ $$2xe^{2xy} = \lambda2y$$ How would I solve this system of equations?","Find the maximum and minimum values of $f = e^{2xy}$ with respect to $x^2+y^2 = 16$. Using Lagrange multipliers, $\nabla f = \lambda\nabla g$. Therefore, the constraints are the following: $$x^2+y^2 = 16$$ $$2ye^{2xy} = \lambda2x$$ $$2xe^{2xy} = \lambda2y$$ How would I solve this system of equations?",,"['calculus', 'multivariable-calculus', 'optimization', 'systems-of-equations', 'lagrange-multiplier']"
67,Show that particle whose position satisfies $\frac{d \mathbf{r}}{dt}= \mathbf{c} \times \mathbf{r}$ moves in a circular path with constant speed,Show that particle whose position satisfies  moves in a circular path with constant speed,\frac{d \mathbf{r}}{dt}= \mathbf{c} \times \mathbf{r},"A particle P moves so that its position vector r at time $t$ satisfies the differential equation $$\frac{d \mathbf{r}}{dt}= \mathbf{c} \times \mathbf{r},$$ where c is a constant vector. Show that P moves with constant speed on a circular patch. taking the dot product on the left with c , then using cyclic permutation we obtain; $$ \mathbf{c} \cdot \frac{d \mathbf{r}}{dt} = \mathbf{c} \cdot \mathbf{c} \times \mathbf{r} = 0$$ similarly with r $$ \mathbf{r} \cdot \frac{d \mathbf{r}}{dt} =  \mathbf{r} \cdot \mathbf{c} \times \mathbf{r} = 0 $$ At this point I did not know where to turn, so I consulted the solution manual. in the solution manual it says that the above equation $ \mathbf{r} \cdot \frac{d \mathbf{r} }{dt} $ can be integrated to give $  \mathbf{r} \cdot \mathbf{r} = R^2 $ . I cant quite see how.","A particle P moves so that its position vector r at time satisfies the differential equation where c is a constant vector. Show that P moves with constant speed on a circular patch. taking the dot product on the left with c , then using cyclic permutation we obtain; similarly with r At this point I did not know where to turn, so I consulted the solution manual. in the solution manual it says that the above equation can be integrated to give . I cant quite see how.","t \frac{d \mathbf{r}}{dt}= \mathbf{c} \times \mathbf{r},  \mathbf{c} \cdot \frac{d \mathbf{r}}{dt} = \mathbf{c} \cdot \mathbf{c} \times \mathbf{r} = 0  \mathbf{r} \cdot \frac{d \mathbf{r}}{dt} =  \mathbf{r} \cdot \mathbf{c} \times \mathbf{r} = 0   \mathbf{r} \cdot \frac{d \mathbf{r} }{dt}    \mathbf{r} \cdot \mathbf{r} = R^2 ",['multivariable-calculus']
68,How to evaluate a double integral with two Dirac functions?,How to evaluate a double integral with two Dirac functions?,,"Here I have a problem, is the solution  the same if I integrate every one? part by part? $$\int_0^Te^{-(s+\mu\lambda^2 ) t}  \int_0^l\left[\delta(x-R)\delta(t-tj)\varphi(x) \, dx\, dt\right]$$ I've already integrated but not sure about the result, I would like to corroborate it.","Here I have a problem, is the solution  the same if I integrate every one? part by part? $$\int_0^Te^{-(s+\mu\lambda^2 ) t}  \int_0^l\left[\delta(x-R)\delta(t-tj)\varphi(x) \, dx\, dt\right]$$ I've already integrated but not sure about the result, I would like to corroborate it.",,"['real-analysis', 'multivariable-calculus', 'definite-integrals', 'distribution-theory', 'dirac-delta']"
69,Vector field with gradient and integral over curve,Vector field with gradient and integral over curve,,"The problem is: Consider the vector field:  $$\textbf{F}= 4x^3y^3 \,\textbf{i} + (1+3x^4y^2) \,\textbf{j}$$ a) Find a potential function $ϕ(x,y)$, i.e. a function $ϕ(x,y)$ such that $\nabla ϕ= \textbf{F}.$ b) Compute $\int_C \textbf{F} \cdot \mathrm d \textbf{r}$ over curve $C$ given by  $$\textbf{r}(t)=\frac{\sin(t^2-t)}{2(\pi-1)} \,\textbf{i} + \frac{t}{\pi} \,\textbf{j}, \quad t \in (0, \pi)$$ I found the function for part (a) as $$\phi(x, y) = x^4y^3+y+C$$ but I didn't know what to do once I came to part (b). Am I supposed to take the derivative for $\textbf{r}(t)$?","The problem is: Consider the vector field:  $$\textbf{F}= 4x^3y^3 \,\textbf{i} + (1+3x^4y^2) \,\textbf{j}$$ a) Find a potential function $ϕ(x,y)$, i.e. a function $ϕ(x,y)$ such that $\nabla ϕ= \textbf{F}.$ b) Compute $\int_C \textbf{F} \cdot \mathrm d \textbf{r}$ over curve $C$ given by  $$\textbf{r}(t)=\frac{\sin(t^2-t)}{2(\pi-1)} \,\textbf{i} + \frac{t}{\pi} \,\textbf{j}, \quad t \in (0, \pi)$$ I found the function for part (a) as $$\phi(x, y) = x^4y^3+y+C$$ but I didn't know what to do once I came to part (b). Am I supposed to take the derivative for $\textbf{r}(t)$?",,"['integration', 'multivariable-calculus', 'vector-fields']"
70,Finding the Radius of a Circle in 3D Using Stokes Theorem,Finding the Radius of a Circle in 3D Using Stokes Theorem,,"Let $$\vec{F} (x, y, z) = xy\hat{i} +(4x - yz)\hat{j} + (xy - z^{1/2}) \hat{k},$$ and let $C$ be a circle of radius $R$ lying in the plane $x + y + z = 5$. If $$\int_C \vec{F} \cdot d\vec{r} = \pi \sqrt{3},$$ where $C$ is oriented in the counterclockwise direction when viewed from above the plane, what is the value of $r$? This question has some context and the version of Stokes Theorem to use.","Let $$\vec{F} (x, y, z) = xy\hat{i} +(4x - yz)\hat{j} + (xy - z^{1/2}) \hat{k},$$ and let $C$ be a circle of radius $R$ lying in the plane $x + y + z = 5$. If $$\int_C \vec{F} \cdot d\vec{r} = \pi \sqrt{3},$$ where $C$ is oriented in the counterclockwise direction when viewed from above the plane, what is the value of $r$? This question has some context and the version of Stokes Theorem to use.",,"['calculus', 'multivariable-calculus', 'line-integrals']"
71,multivariable calculus charge density question,multivariable calculus charge density question,,"The sphere given by $x^{2} + y^{2} + z^{2} = 4$ is submerged in an electric field with charge density given by $f(x, y, z) = x^{2} + y^{2}$. Find the total amount of electric charge on this surface. I am stuck on what integral formula to use.","The sphere given by $x^{2} + y^{2} + z^{2} = 4$ is submerged in an electric field with charge density given by $f(x, y, z) = x^{2} + y^{2}$. Find the total amount of electric charge on this surface. I am stuck on what integral formula to use.",,"['multivariable-calculus', 'vector-fields', 'line-integrals']"
72,Doubt on integrating $f$ on a given region .,Doubt on integrating  on a given region .,f,"I was asked to integrate the following function: $f(x,y,z)=1-z^2$ on $U$ where $U$ is a pyramid with the top vertex $(0,0,1)$ and base vertices :$(0,0,0),(1,0,0),(0,1,0), (1,1,0)$ the hint said that : the cross section of given pyramid at fixed value $z$ is square : $$[0,1-z]\times[0,1-z]$$ but I can't understand how is it so..kindly help ..","I was asked to integrate the following function: $f(x,y,z)=1-z^2$ on $U$ where $U$ is a pyramid with the top vertex $(0,0,1)$ and base vertices :$(0,0,0),(1,0,0),(0,1,0), (1,1,0)$ the hint said that : the cross section of given pyramid at fixed value $z$ is square : $$[0,1-z]\times[0,1-z]$$ but I can't understand how is it so..kindly help ..",,"['integration', 'multivariable-calculus']"
73,"Differential forms, number of zeros on disk","Differential forms, number of zeros on disk",,"Let $f$ be a holomorphic function on $U \subset \mathbb{C}$ and $f'(z) \neq 0$, $z \in U$. Then how do I show that all zeros of $f$ are simple and positive, and that if I have a disk $D \subset U$ where there's no zeros in $\partial D$, then $$\#\text{ of zeros of }f\text{ in }D = {1\over{2\pi i}} \int_{\partial D} {{df}\over{f}}.$$ I know how to show this more ""powerful"" techniques from ecomplex analysis, but is there a way to prove this with only the language of differential forms and real functions?","Let $f$ be a holomorphic function on $U \subset \mathbb{C}$ and $f'(z) \neq 0$, $z \in U$. Then how do I show that all zeros of $f$ are simple and positive, and that if I have a disk $D \subset U$ where there's no zeros in $\partial D$, then $$\#\text{ of zeros of }f\text{ in }D = {1\over{2\pi i}} \int_{\partial D} {{df}\over{f}}.$$ I know how to show this more ""powerful"" techniques from ecomplex analysis, but is there a way to prove this with only the language of differential forms and real functions?",,"['complex-analysis', 'multivariable-calculus']"
74,Find the Area Using Polar Coordinates and a Double Integral,Find the Area Using Polar Coordinates and a Double Integral,,"Of the area inside the smaller loop of the equation $r = 1-2sin\theta$ Here's my attempt at a solution: The shape has an inner and an outer loop, both of which will terminate at the origin. Therefore, I want to find the range of the equation which only draws the inner loop. I would assume I would select a range for $\theta$ which would set $r=0$, however, I'm honestly not sure how I would go about doing that. I can't just take all the points where $r = 0$, because then the outer loop will be included as well. How do I set this integral up? Note: I've tried to find answers online and stumbled onto this, but cannot make heads or tails of what is being described. The document claims that $ θ = ±π/4$ and $θ = ±3π/4$ are the candidates, but when I use the equation $sin\theta = 1/2$, I end up with π/6. http://jacobi.math.wvu.edu/~hjlai/Teaching/Tip-Pdf/Tip3-34.pdf (Example 3)","Of the area inside the smaller loop of the equation $r = 1-2sin\theta$ Here's my attempt at a solution: The shape has an inner and an outer loop, both of which will terminate at the origin. Therefore, I want to find the range of the equation which only draws the inner loop. I would assume I would select a range for $\theta$ which would set $r=0$, however, I'm honestly not sure how I would go about doing that. I can't just take all the points where $r = 0$, because then the outer loop will be included as well. How do I set this integral up? Note: I've tried to find answers online and stumbled onto this, but cannot make heads or tails of what is being described. The document claims that $ θ = ±π/4$ and $θ = ±3π/4$ are the candidates, but when I use the equation $sin\theta = 1/2$, I end up with π/6. http://jacobi.math.wvu.edu/~hjlai/Teaching/Tip-Pdf/Tip3-34.pdf (Example 3)",,"['calculus', 'multivariable-calculus', 'polar-coordinates', 'area']"
75,Taking the Derivative: Power Rule with Respect to Vector,Taking the Derivative: Power Rule with Respect to Vector,,"I'm trying to take the derivative of \begin{equation} \phi\left(\mathbf{x}\mathbf{\theta}\right)\mathbf{x}^{\top} \left(\frac{y-\Phi\left(\mathbf{x}\mathbf{\theta}\right)}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}\right) \end{equation} with respect to $\mathbf{\theta}$, where $\mathbf{\theta}$ is a $p \times 1$ column vector, $\mathbf{x}$ is a $1 \times p$ row vector, (so that $\mathbf{x}\mathbf{\theta}$ is a scalar), $\Phi$ is a function (representing the normal CDF but this doesn't matter) with derivative $\phi$. So I'm trying to compute \begin{equation} \nabla_{\mathbf{\theta}} \phi\left(\mathbf{x}\mathbf{\theta}\right)\mathbf{x}^{\top} \left(\frac{y-\Phi\left(\mathbf{x}\mathbf{\theta}\right)}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}\right) \end{equation} I know that the answer ends up being \begin{equation} \frac{-[\phi(\mathbf{x}\mathbf{\theta})]^2\mathbf{x}^{\top}\mathbf{x}}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}+\mathbf{L}(\mathbf{x},\mathbf{\theta})[y-\Phi(\mathbf{x}\mathbf{\theta})] \end{equation} where $\mathbf{L}(\mathbf{x},\mathbf{\theta})$ is the Jacobian of \begin{equation} \frac{\phi(\mathbf{x}\mathbf{\theta})\mathbf{x}^{\top}}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]} \end{equation} However I try to get to the answer and get overwhelmed with the problem. Any help? Thank you very much!","I'm trying to take the derivative of \begin{equation} \phi\left(\mathbf{x}\mathbf{\theta}\right)\mathbf{x}^{\top} \left(\frac{y-\Phi\left(\mathbf{x}\mathbf{\theta}\right)}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}\right) \end{equation} with respect to $\mathbf{\theta}$, where $\mathbf{\theta}$ is a $p \times 1$ column vector, $\mathbf{x}$ is a $1 \times p$ row vector, (so that $\mathbf{x}\mathbf{\theta}$ is a scalar), $\Phi$ is a function (representing the normal CDF but this doesn't matter) with derivative $\phi$. So I'm trying to compute \begin{equation} \nabla_{\mathbf{\theta}} \phi\left(\mathbf{x}\mathbf{\theta}\right)\mathbf{x}^{\top} \left(\frac{y-\Phi\left(\mathbf{x}\mathbf{\theta}\right)}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}\right) \end{equation} I know that the answer ends up being \begin{equation} \frac{-[\phi(\mathbf{x}\mathbf{\theta})]^2\mathbf{x}^{\top}\mathbf{x}}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]}+\mathbf{L}(\mathbf{x},\mathbf{\theta})[y-\Phi(\mathbf{x}\mathbf{\theta})] \end{equation} where $\mathbf{L}(\mathbf{x},\mathbf{\theta})$ is the Jacobian of \begin{equation} \frac{\phi(\mathbf{x}\mathbf{\theta})\mathbf{x}^{\top}}{\Phi\left(\mathbf{x}\mathbf{\theta}\right)\left[1-\Phi\left(\mathbf{x}\mathbf{\theta}\right)\right]} \end{equation} However I try to get to the answer and get overwhelmed with the problem. Any help? Thank you very much!",,"['matrices', 'multivariable-calculus', 'derivatives', 'vectors']"
76,How to use Cauchy-Scharw inequality to prove continuity of a function?,How to use Cauchy-Scharw inequality to prove continuity of a function?,,"I'm attempting to understand how to prove the function f such that $$f(x,y)=\frac{x^3y}{x^4+y^2}\;if\;(x,y)\neq (0,0)$$ $$f(x,y)=(0,0)\;if\;(x,y)=(0,0)$$ is continuous in $\mathbb R^2$. The solution provided says to use the Cauchy-Schwarz inequality to show that $|x^3y|=|x^2yx|\le \frac{(x^2)^2+y^2}{2}|x|$, and thus the function is lesser than or equal to $\frac{|x|}{2}$, and thus the limit goes to 0 as (x,y) goes to to (0,0).  I understand the limit part, but not the use of the inequality.  How do I use Cauchy-Schwarz to obtain this inequality","I'm attempting to understand how to prove the function f such that $$f(x,y)=\frac{x^3y}{x^4+y^2}\;if\;(x,y)\neq (0,0)$$ $$f(x,y)=(0,0)\;if\;(x,y)=(0,0)$$ is continuous in $\mathbb R^2$. The solution provided says to use the Cauchy-Schwarz inequality to show that $|x^3y|=|x^2yx|\le \frac{(x^2)^2+y^2}{2}|x|$, and thus the function is lesser than or equal to $\frac{|x|}{2}$, and thus the limit goes to 0 as (x,y) goes to to (0,0).  I understand the limit part, but not the use of the inequality.  How do I use Cauchy-Schwarz to obtain this inequality",,"['calculus', 'multivariable-calculus']"
77,"Showing $\frac{x^2y^3}{x^2+y^4}$ is differentiable everywhere, and if the first order partials are continuous","Showing  is differentiable everywhere, and if the first order partials are continuous",\frac{x^2y^3}{x^2+y^4},"So the function is defined as above, except at (0,0) the function is defined to be 0.  So I computed the first order partial derivatives, and it's obviously continuous at all points other than (0,0), and then I used the definition of the limit to compute the partial derivatives by x and y at (0,0), which are both 0. Now I'm trying to satisfy the condition $\frac{||f(x,y)-f(0,0)-T(0,0)||}{||h||}$, which could be simplified to $\frac{x^2y^3}{(x^2+y^4)\sqrt{x^2+y^2}}$, and now I'm stuck at showing that this goes to 0 as h goes to zero (where h is (x,y)). Also, could someone just let me know how to show that the partials are continuous?  I don't really know how to show they're continuous at (0,0).  Thanks in advance!","So the function is defined as above, except at (0,0) the function is defined to be 0.  So I computed the first order partial derivatives, and it's obviously continuous at all points other than (0,0), and then I used the definition of the limit to compute the partial derivatives by x and y at (0,0), which are both 0. Now I'm trying to satisfy the condition $\frac{||f(x,y)-f(0,0)-T(0,0)||}{||h||}$, which could be simplified to $\frac{x^2y^3}{(x^2+y^4)\sqrt{x^2+y^2}}$, and now I'm stuck at showing that this goes to 0 as h goes to zero (where h is (x,y)). Also, could someone just let me know how to show that the partials are continuous?  I don't really know how to show they're continuous at (0,0).  Thanks in advance!",,"['calculus', 'multivariable-calculus']"
78,Finding potential function of a vector field,Finding potential function of a vector field,,"I have this question and I don't know the answer. Suppose $U \subset \mathbb{R}^3$ is a convex open set and $\textbf{F} : U\to \mathbb{R}^3$ is a smooth vector field. Show that if $\nabla \times {F}=0$, then $\textbf{F} =\nabla u$ for some smooth function $u:  U\to \mathbb{R}$. $F=(F_1,F_2,F_3)$. The model answer to this question is letting $$u(x,y,z) = \int_{x_0}^x F_1(t,y_0,z_0) dt+\int_{y_0}^y F_2(x_0,y,z_0) dt+\int_{z_0}^z F_3(x_0,y_0,z) dt$$ However, I think it is wrong since $u_x=F_1(x,y_0,z_0)$ instead of $F_1(x,y,z)$. I tried letting $$u(x,y,z) = \int_{x_0}^x F_1(t,y_0,z_0) dt+\int_{y_0}^y F_2(x,t,z_0) dt+\int_{z_0}^z F_3(x,y,t) dt$$ This is taught by my teacher, to evaluate potential function with a line integral goes to $(x,y,z)$. However, I could figure out how will $u_x=F_1(x,y,z)$. Please help.","I have this question and I don't know the answer. Suppose $U \subset \mathbb{R}^3$ is a convex open set and $\textbf{F} : U\to \mathbb{R}^3$ is a smooth vector field. Show that if $\nabla \times {F}=0$, then $\textbf{F} =\nabla u$ for some smooth function $u:  U\to \mathbb{R}$. $F=(F_1,F_2,F_3)$. The model answer to this question is letting $$u(x,y,z) = \int_{x_0}^x F_1(t,y_0,z_0) dt+\int_{y_0}^y F_2(x_0,y,z_0) dt+\int_{z_0}^z F_3(x_0,y_0,z) dt$$ However, I think it is wrong since $u_x=F_1(x,y_0,z_0)$ instead of $F_1(x,y,z)$. I tried letting $$u(x,y,z) = \int_{x_0}^x F_1(t,y_0,z_0) dt+\int_{y_0}^y F_2(x,t,z_0) dt+\int_{z_0}^z F_3(x,y,t) dt$$ This is taught by my teacher, to evaluate potential function with a line integral goes to $(x,y,z)$. However, I could figure out how will $u_x=F_1(x,y,z)$. Please help.",,"['calculus', 'integration', 'multivariable-calculus']"
79,"If $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ smooth, $ g(x,y)= x^3 + y^3$ and $g \circ f \equiv 0$, then $\det Df \equiv 0$","If  smooth,  and , then","f: \mathbb{R}^2 \rightarrow \mathbb{R}^2  g(x,y)= x^3 + y^3 g \circ f \equiv 0 \det Df \equiv 0","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ be a smooth function and $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined by $(x,y) \mapsto x^3 + y^3$. Assume that $g \circ f$ is identically $0$. Then I want to show that $\det Df$ is identically $0$. I want to check if my solution is correct. This is what I've done: Since $g \circ f \equiv 0$, we have $D(g(f(x,y))) = 0$ for all $x,y$. Then $D(g(f)=Dg(f)Df = \nabla g (f) Df = \nabla (f_1^3 + f_2^3) Df=(3f_1^2 \nabla f_1 + 3f_2^2 \nabla f_2) Df=0$ which means that for any $x,y$ either $f_1(x,y)=f_2(x,y)=0$ or $Df=0$. This implies that $\det Df \equiv 0$.","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ be a smooth function and $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ be defined by $(x,y) \mapsto x^3 + y^3$. Assume that $g \circ f$ is identically $0$. Then I want to show that $\det Df$ is identically $0$. I want to check if my solution is correct. This is what I've done: Since $g \circ f \equiv 0$, we have $D(g(f(x,y))) = 0$ for all $x,y$. Then $D(g(f)=Dg(f)Df = \nabla g (f) Df = \nabla (f_1^3 + f_2^3) Df=(3f_1^2 \nabla f_1 + 3f_2^2 \nabla f_2) Df=0$ which means that for any $x,y$ either $f_1(x,y)=f_2(x,y)=0$ or $Df=0$. This implies that $\det Df \equiv 0$.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'proof-verification', 'determinant']"
80,Intuition behind Laurent's theorem?,Intuition behind Laurent's theorem?,,"Taylor series has a pretty nice intuitive explanation. If you know the position, velocity, acceleration and so on of a particle you can predict it's location at any time. Does a similar intuitive explanation exist for Laurent series and the theorem above (the one that describes the coefficients of the series as a line integral)?","Taylor series has a pretty nice intuitive explanation. If you know the position, velocity, acceleration and so on of a particle you can predict it's location at any time. Does a similar intuitive explanation exist for Laurent series and the theorem above (the one that describes the coefficients of the series as a line integral)?",,"['complex-analysis', 'multivariable-calculus']"
81,"Partial derivatives of a $C^2$ function $f(x,y)$ where $f(x,y) = f(y,x)$ and $f(x,x) = x$",Partial derivatives of a  function  where  and,"C^2 f(x,y) f(x,y) = f(y,x) f(x,x) = x","I'd like to show that the second-order term in the Taylor polynomial for $f$ centered at $(a,a)$ is equal to $$\frac{1}{2}f_{xx}(a,a)(x-y)^2.$$ By $C^2$ I mean that all of the first and second order partial deriatives exist and are continuous. I first looked at the form of the term, which is $$\frac{1}{2}f_{xx}(a,a)x^2 + \frac{1}{2}f_{yy}(a,a)y^2 + f_{xy}(a,a)xy.$$ Since I have $f(x,y) = f(y,x)$, differentiating with respect to the first variable gives me $f_x(x,y) = f_y(y,x)$ and differentiating again gives me $f_{xx}(x,y) = f_{yy}(y,x)$, so taking $x = y = a$ I get $f_{xx}(a,a) = f_{yy}(a,a)$. Then the term becomes $$\frac{1}{2}f_{xx}(a,a)(x^2 + y^2) + f_{xy}(a,a)xy.$$ I think what I need to do is show that $f_{xy}(a,a) = -f_{xx}(a,a)$, at which point the second derivative factors out and $x^2 + y^2 - 2xy$ can be simplified to $(x-y)^2$. However, it's not immediately clear to me how I should do this. Specifically, where does the negative sign come into play?","I'd like to show that the second-order term in the Taylor polynomial for $f$ centered at $(a,a)$ is equal to $$\frac{1}{2}f_{xx}(a,a)(x-y)^2.$$ By $C^2$ I mean that all of the first and second order partial deriatives exist and are continuous. I first looked at the form of the term, which is $$\frac{1}{2}f_{xx}(a,a)x^2 + \frac{1}{2}f_{yy}(a,a)y^2 + f_{xy}(a,a)xy.$$ Since I have $f(x,y) = f(y,x)$, differentiating with respect to the first variable gives me $f_x(x,y) = f_y(y,x)$ and differentiating again gives me $f_{xx}(x,y) = f_{yy}(y,x)$, so taking $x = y = a$ I get $f_{xx}(a,a) = f_{yy}(a,a)$. Then the term becomes $$\frac{1}{2}f_{xx}(a,a)(x^2 + y^2) + f_{xy}(a,a)xy.$$ I think what I need to do is show that $f_{xy}(a,a) = -f_{xx}(a,a)$, at which point the second derivative factors out and $x^2 + y^2 - 2xy$ can be simplified to $(x-y)^2$. However, it's not immediately clear to me how I should do this. Specifically, where does the negative sign come into play?",,['multivariable-calculus']
82,Triple Integrals Problem,Triple Integrals Problem,,"Find the volume formed by $x^2+y^2=9$, $z=0$, and $y=3z$. I am having trouble with determining the limits of integration. $x^2+y^2=9$ describes a circle centered at the origin with a radius of $3$. It would also be helpful to know how to change the Cartesian limits to polar limits of integration. I think that the $z$ value for the limit will be from $0$ to $\frac{y}{3}$ The $y$ limits of integration will be from $0$ to $3$. (since the radius is $3$).  The $z$ limits of integration will be from $0$ to $2\pi$. Integration is not the problem. It's just visualizing and labeling the limits of integration.  Thank you.","Find the volume formed by $x^2+y^2=9$, $z=0$, and $y=3z$. I am having trouble with determining the limits of integration. $x^2+y^2=9$ describes a circle centered at the origin with a radius of $3$. It would also be helpful to know how to change the Cartesian limits to polar limits of integration. I think that the $z$ value for the limit will be from $0$ to $\frac{y}{3}$ The $y$ limits of integration will be from $0$ to $3$. (since the radius is $3$).  The $z$ limits of integration will be from $0$ to $2\pi$. Integration is not the problem. It's just visualizing and labeling the limits of integration.  Thank you.",,['multivariable-calculus']
83,maximize function with two variables,maximize function with two variables,,"I would like to maximize the function: $f(x,y) = c[x\log(2y) + (1-x)\log(2(1-y))]$ subject to constraint that $x,y \in (0,1)$ to find a relationship between $x$ and $y$ that maximizes $f(x,y)$. My solution is: take partial derivative with respect to $y$ and set to 0: $\partial f/\partial y = c\times\partial/\partial y[x\log(2y) + (1-x)\log(2(1-y))] = 0$ I treat $x$ as constant and I believe this yields that: $x = y$, for the derivative to be zero. Questions: Is this the correct way to solve this, and how can we show $x = y$ is a maximum? I think the $2$ constant inside the $\log$ doesn't change the solution. If we had substituted any constant $z$ for $2$ in the equation, ie: $f(x,y) = c[x\log(zy) + (1-x)\log(z(1-y))]$ The result would be the same. Is that right?","I would like to maximize the function: $f(x,y) = c[x\log(2y) + (1-x)\log(2(1-y))]$ subject to constraint that $x,y \in (0,1)$ to find a relationship between $x$ and $y$ that maximizes $f(x,y)$. My solution is: take partial derivative with respect to $y$ and set to 0: $\partial f/\partial y = c\times\partial/\partial y[x\log(2y) + (1-x)\log(2(1-y))] = 0$ I treat $x$ as constant and I believe this yields that: $x = y$, for the derivative to be zero. Questions: Is this the correct way to solve this, and how can we show $x = y$ is a maximum? I think the $2$ constant inside the $\log$ doesn't change the solution. If we had substituted any constant $z$ for $2$ in the equation, ie: $f(x,y) = c[x\log(zy) + (1-x)\log(z(1-y))]$ The result would be the same. Is that right?",,"['calculus', 'multivariable-calculus', 'functions', 'optimization']"
84,Find maximum/minimum for $\cos(2x) + \cos(y) + \cos(2x+y) $,Find maximum/minimum for,\cos(2x) + \cos(y) + \cos(2x+y) ,I have not been able to find the critical points for $\cos(2x) + \cos(y) + \cos(2x+y) $,I have not been able to find the critical points for $\cos(2x) + \cos(y) + \cos(2x+y) $,,['multivariable-calculus']
85,"Show that $g\circ f\equiv 0$ implies $\det Df\equiv 0$, where $g(x_1,...,x_n)=x_1^5+...+x_n^5$","Show that  implies , where","g\circ f\equiv 0 \det Df\equiv 0 g(x_1,...,x_n)=x_1^5+...+x_n^5","Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be a smooth function and let $g:\mathbb{R}^n \rightarrow \mathbb{R}$ be defined by $g(x_1,...,x_n)=x_1^5+...+x_n^5$. Suppose $g\circ f\equiv 0$. Show that $\det Df\equiv 0$. I was going to start the solution from $n=2$ and then use induction on $n$. So, for $n=2$, $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ is a smooth function and $g:\mathbb{R}^2 \rightarrow \mathbb{R}$ where $g(x_1,x_2)=x_1^5+x_2^5$. Then $Df=\begin{pmatrix}   f_{x_1x_1} & f_{x_1x_2}  \\   f_{x_2x_1} & f_{x_2x_2}   \end{pmatrix}$ But now I am stuck in proving the result. Am I in the correct track? Can anyone plese give me a hint to continue? Ok so since $g\circ f$ is $0$, the image of $f$ is the line $x_1+x_2=0$ which has no min, max or saddle. If det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)>0$ OR det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)<0$, we have contradiction. But if det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)=0$, then det$Df(x_0)=0$ or, det$Df(x_0)<0$(Since $f_{x_1x_2}=f_{x_2x_1}$). So this is impossible. If det$Df(x_0)<0$ we have saddle and it is impossible. So finally we conclude that det$Df(x)=0$ for all $x$. Am I correct?","Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be a smooth function and let $g:\mathbb{R}^n \rightarrow \mathbb{R}$ be defined by $g(x_1,...,x_n)=x_1^5+...+x_n^5$. Suppose $g\circ f\equiv 0$. Show that $\det Df\equiv 0$. I was going to start the solution from $n=2$ and then use induction on $n$. So, for $n=2$, $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ is a smooth function and $g:\mathbb{R}^2 \rightarrow \mathbb{R}$ where $g(x_1,x_2)=x_1^5+x_2^5$. Then $Df=\begin{pmatrix}   f_{x_1x_1} & f_{x_1x_2}  \\   f_{x_2x_1} & f_{x_2x_2}   \end{pmatrix}$ But now I am stuck in proving the result. Am I in the correct track? Can anyone plese give me a hint to continue? Ok so since $g\circ f$ is $0$, the image of $f$ is the line $x_1+x_2=0$ which has no min, max or saddle. If det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)>0$ OR det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)<0$, we have contradiction. But if det$Df(x_0)>0$ and $f_{x_1x_1}(x_0)=0$, then det$Df(x_0)=0$ or, det$Df(x_0)<0$(Since $f_{x_1x_2}=f_{x_2x_1}$). So this is impossible. If det$Df(x_0)<0$ we have saddle and it is impossible. So finally we conclude that det$Df(x)=0$ for all $x$. Am I correct?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'jacobian']"
86,Double Integral Between Two Circles,Double Integral Between Two Circles,,"What is the area confined by these two circles? $y^2=1-(x-1)^2$ $y^2=1-x^2$ I've set up the integral: $\int_{\pi/3}^{2\pi/3}\int_{2\cos\theta}^1r\,\mathrm{d}r\mathrm{d}\theta$ Unfortunately, my answer is off so I believe that this is incorrect, I got $-\pi/6+\sqrt{3}/2$ when the correct answer is $\pi/3+\sqrt3/2$. I've checked my work so I don't believe the algebra is off. My logic behind the $r$ integral is that after the equations are converted to polar coordinates the $r$ value goes from $2\cos\theta$ to $1$. I believe that $\theta$ goes from $\pi/3$ to $2\pi/3$ also.","What is the area confined by these two circles? $y^2=1-(x-1)^2$ $y^2=1-x^2$ I've set up the integral: $\int_{\pi/3}^{2\pi/3}\int_{2\cos\theta}^1r\,\mathrm{d}r\mathrm{d}\theta$ Unfortunately, my answer is off so I believe that this is incorrect, I got $-\pi/6+\sqrt{3}/2$ when the correct answer is $\pi/3+\sqrt3/2$. I've checked my work so I don't believe the algebra is off. My logic behind the $r$ integral is that after the equations are converted to polar coordinates the $r$ value goes from $2\cos\theta$ to $1$. I believe that $\theta$ goes from $\pi/3$ to $2\pi/3$ also.",,"['calculus', 'integration', 'multivariable-calculus']"
87,function of class C ^ 1 on manifolds,function of class C ^ 1 on manifolds,,"Let $M$ be a differentiable manifold with finite dimension $ m $. Let $ f:M\rightarrow M $ a function of class $C^1$. I have a doubt about what this implies (1) or (2): $x \in M \rightarrow D_xf \in \mathcal{L}(T_xM, T_{f(x)}M)$ is continuous $Df:TM \rightarrow TM$ is continuous Note that (1) is the natural extension of $f:\mathbb{R}^m \rightarrow \mathbb{R}^m $, but the set of arrival would not be well defined (varies with $x$ ). On the other hand, (2) does not imply (in my opinion) that $M \rightarrow D_xf $ is continuous. Would appreciate any suggestions on the proper way to interpret this concept","Let $M$ be a differentiable manifold with finite dimension $ m $. Let $ f:M\rightarrow M $ a function of class $C^1$. I have a doubt about what this implies (1) or (2): $x \in M \rightarrow D_xf \in \mathcal{L}(T_xM, T_{f(x)}M)$ is continuous $Df:TM \rightarrow TM$ is continuous Note that (1) is the natural extension of $f:\mathbb{R}^m \rightarrow \mathbb{R}^m $, but the set of arrival would not be well defined (varies with $x$ ). On the other hand, (2) does not imply (in my opinion) that $M \rightarrow D_xf $ is continuous. Would appreciate any suggestions on the proper way to interpret this concept",,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'fiber-bundles']"
88,Integrating conical surfaces and Divergence Theorem,Integrating conical surfaces and Divergence Theorem,,"I am currently working through this question. This is how I have approached (a) however I am thrown by the negative sign, can somebody point out my mistake(s)? For (b) I have used Divergence/Gauss' theorem. I got $-0.75\pi$ where $s(u,v)$ was the same as the one used in (a). I believe (c) is (b)-(a)? *EDIT: I have missed u from the z component of N , this gives me an answer of -$\pi/3$","I am currently working through this question. This is how I have approached (a) however I am thrown by the negative sign, can somebody point out my mistake(s)? For (b) I have used Divergence/Gauss' theorem. I got $-0.75\pi$ where $s(u,v)$ was the same as the one used in (a). I believe (c) is (b)-(a)? *EDIT: I have missed u from the z component of N , this gives me an answer of -$\pi/3$",,"['calculus', 'integration', 'multivariable-calculus', 'proof-verification']"
89,Norm of the gradient of function $f$ on manifold $g(x)=c$,Norm of the gradient of function  on manifold,f g(x)=c,"Let $g,f:\mathbb{R}^2 \to \mathbb{R}$, $M=g^{-1}(c)$. Let say that we manage to write $f(x,y)=f_{*}(x)$ for $x\in M$. When I was calculating the square of norm of $$\nabla f_M (x,y)=\nabla f(x,y)-\frac{\langle\nabla f(x,y),\nabla g(x,y)\rangle}{\|\nabla g(x,y)\|^{2}}\nabla g(x,y)$$ I get $\frac{(f_x g_y-f_y g_x)^2}{\|\nabla g\|^2}$, which is $\frac{{f_* }^2}{1+(\frac{g_x}{g_y})^2}$. I was expecting simply ${{f_* }^2}$. What is the geometrical meaning of $\frac{1}{1+(\frac{g_x}{g_y})^2}$? What kind of result can I expect in higher dimensions?","Let $g,f:\mathbb{R}^2 \to \mathbb{R}$, $M=g^{-1}(c)$. Let say that we manage to write $f(x,y)=f_{*}(x)$ for $x\in M$. When I was calculating the square of norm of $$\nabla f_M (x,y)=\nabla f(x,y)-\frac{\langle\nabla f(x,y),\nabla g(x,y)\rangle}{\|\nabla g(x,y)\|^{2}}\nabla g(x,y)$$ I get $\frac{(f_x g_y-f_y g_x)^2}{\|\nabla g\|^2}$, which is $\frac{{f_* }^2}{1+(\frac{g_x}{g_y})^2}$. I was expecting simply ${{f_* }^2}$. What is the geometrical meaning of $\frac{1}{1+(\frac{g_x}{g_y})^2}$? What kind of result can I expect in higher dimensions?",,"['multivariable-calculus', 'manifolds', 'riemannian-geometry', 'gradient-flows']"
90,Evaluate a definite double integral of the integrand of the gaussian integral,Evaluate a definite double integral of the integrand of the gaussian integral,,"I have to solve this by changing the order of integration, $$\int_0^4 \int_\frac y4^2 e^{-x^2} \,dx\, dy$$ and I got this far, $$\int_0^1 \int_0^{4x} e^{-x^2}\, dy\, dx$$ but I'm pretty sure I'm missing something. Help please?","I have to solve this by changing the order of integration, $$\int_0^4 \int_\frac y4^2 e^{-x^2} \,dx\, dy$$ and I got this far, $$\int_0^1 \int_0^{4x} e^{-x^2}\, dy\, dx$$ but I'm pretty sure I'm missing something. Help please?",,"['calculus', 'integration', 'multivariable-calculus']"
91,Prove whether a particular function is concave,Prove whether a particular function is concave,,"Given the following equation: $$V(w) = - \frac{\alpha}{2} \left[ y_1(w) + y_2(w) + \int _{-\infty}^{+\infty} \vert y_1(w) - y_2(w) - x\vert f_{T1}(x)dx\right] \\- \beta \int _{w - y_1(w)} ^{+\infty} \left( y_1(w) + x - w\right) f_{T2}(x)dx$$ and the information that $\frac{\partial y_2(w)}{\partial w} \geq 0 $ and $\frac{\partial y_1(w)}{\partial w}=0$ $F_{k}(x)=\int\limits_{-\infty}^{x} f_{k}(t)dt$ is $k$'s cumulative distribution function which is continuous on a compact support where $k=\{T_1,T_2\}$ and $0 \leq f_k \leq 1 $ is $k$'s probability density function; and $\alpha, \beta > 0$ I would like to prove whether $V(w)$ is (strictly) concave, or if it is not concave, whether it is quasi-concave. Attempts : I tried to see if the function has a second derivative that is negative. First, I derived:  $$\frac{\partial V(w)}{\partial w} = -\alpha y_2^{\prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + \beta\left[1-F_{T_2}(w-y_1(w))\right]$$ and \begin{align} \frac{\partial }{\partial w} \left(\frac{\partial V(w)}{\partial w}\right) &= -\alpha y_2^{\prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + \beta\left[1-F_{T_2}(w-y_1(w))\right] \\ &= -\alpha \left\{ y_2^{\prime \prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + (y_2^{\prime}(w))^2 f_{T_1}\left(y_1(w)-y_2(w)\right) \right\}  \\   \quad \quad \quad  &- \beta f_{T_2} \left( w-y_1(w)\right)  \end{align} However, I could not sign $y_2^{\prime \prime}(w)$ without additional information.","Given the following equation: $$V(w) = - \frac{\alpha}{2} \left[ y_1(w) + y_2(w) + \int _{-\infty}^{+\infty} \vert y_1(w) - y_2(w) - x\vert f_{T1}(x)dx\right] \\- \beta \int _{w - y_1(w)} ^{+\infty} \left( y_1(w) + x - w\right) f_{T2}(x)dx$$ and the information that $\frac{\partial y_2(w)}{\partial w} \geq 0 $ and $\frac{\partial y_1(w)}{\partial w}=0$ $F_{k}(x)=\int\limits_{-\infty}^{x} f_{k}(t)dt$ is $k$'s cumulative distribution function which is continuous on a compact support where $k=\{T_1,T_2\}$ and $0 \leq f_k \leq 1 $ is $k$'s probability density function; and $\alpha, \beta > 0$ I would like to prove whether $V(w)$ is (strictly) concave, or if it is not concave, whether it is quasi-concave. Attempts : I tried to see if the function has a second derivative that is negative. First, I derived:  $$\frac{\partial V(w)}{\partial w} = -\alpha y_2^{\prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + \beta\left[1-F_{T_2}(w-y_1(w))\right]$$ and \begin{align} \frac{\partial }{\partial w} \left(\frac{\partial V(w)}{\partial w}\right) &= -\alpha y_2^{\prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + \beta\left[1-F_{T_2}(w-y_1(w))\right] \\ &= -\alpha \left\{ y_2^{\prime \prime}(w)\left[1-F_{T_1}\left(y_1(w)-y_2(w)\right) \right] + (y_2^{\prime}(w))^2 f_{T_1}\left(y_1(w)-y_2(w)\right) \right\}  \\   \quad \quad \quad  &- \beta f_{T_2} \left( w-y_1(w)\right)  \end{align} However, I could not sign $y_2^{\prime \prime}(w)$ without additional information.",,"['calculus', 'probability', 'multivariable-calculus', 'functions', 'derivatives']"
92,Integration of $F(\sum_k x_k)$ over positive orthant,Integration of  over positive orthant,F(\sum_k x_k),"Problem Suppose we integrate some function $F\left(\sum\limits_{k=1}^n x_k\right)$ over the positive orthant $[0,\infty)^n$. Show that this this is proportional to the integral $\int\limits_0^\infty s^{n-1}F(s)\,ds$. What is the constant of proportionality? If this is a well-known result, a reference would be appreciated. Motivation Suppose I want to solve the inhomogeneous 1st-order ODE $(1-D)f(x)=g(x)$. Then $$D(e^{-x} f)=e^{-x}(f(x)-f'(x))=e^{-x}(1-D)f(x)=e^{-x}g(x),$$ so if I integrate both sides over $[x,\infty)$ (and assume that $e^{-x}f(x)$ vanishes at infinity) I can obtain the integral representation $$f(x)=(1-D)^{-1}g(x)=\int_{x}^\infty dx'\,e^{-(x'-x)}g(x')=\int_{0}^\infty ds\,e^{-s}g(s+x) \quad\quad(x'=s+x).$$ If I instead have the 2nd-order ODE $(1-D)^2 f(x)=g(x)$, then I will instead obtain the double-integral representation \begin{align} f(x)&=(1-D)^{-2}g(x)\\&=(1-D)^{-1}\int_{0}^\infty ds\,e^{-s}g(s+x)\\&=\int_{0}^\infty \int_{0}^\infty ds' ds\,e^{-(s+s')}g(s+s'+x) \end{align} To make this more convenient, I can map $(s,s')\mapsto(\frac{s+s'}{2},\frac{s-s'}{2})$ and modify the bounds of integration accordingly to obtain $$f(x)=(1-D)^{-2}g(x)=\frac{1}{2}\int_0^\infty ds\,e^{-s}g(s+x)\int_{-s}^{s} ds'=\int_0^\infty ds\,se^{-s}g(s+x).$$ So acting twice with $(1-D)^{-1}$ merely introduces a linear factor into the integration over $s$. This surely generalizes to $n$ applications of $(1-D)^{-1}$ (presumably by introducing a factor of $s^{n-1}$ instead) but I'm unfamiliar with such an identity. Can anyone supply a proof/reference?","Problem Suppose we integrate some function $F\left(\sum\limits_{k=1}^n x_k\right)$ over the positive orthant $[0,\infty)^n$. Show that this this is proportional to the integral $\int\limits_0^\infty s^{n-1}F(s)\,ds$. What is the constant of proportionality? If this is a well-known result, a reference would be appreciated. Motivation Suppose I want to solve the inhomogeneous 1st-order ODE $(1-D)f(x)=g(x)$. Then $$D(e^{-x} f)=e^{-x}(f(x)-f'(x))=e^{-x}(1-D)f(x)=e^{-x}g(x),$$ so if I integrate both sides over $[x,\infty)$ (and assume that $e^{-x}f(x)$ vanishes at infinity) I can obtain the integral representation $$f(x)=(1-D)^{-1}g(x)=\int_{x}^\infty dx'\,e^{-(x'-x)}g(x')=\int_{0}^\infty ds\,e^{-s}g(s+x) \quad\quad(x'=s+x).$$ If I instead have the 2nd-order ODE $(1-D)^2 f(x)=g(x)$, then I will instead obtain the double-integral representation \begin{align} f(x)&=(1-D)^{-2}g(x)\\&=(1-D)^{-1}\int_{0}^\infty ds\,e^{-s}g(s+x)\\&=\int_{0}^\infty \int_{0}^\infty ds' ds\,e^{-(s+s')}g(s+s'+x) \end{align} To make this more convenient, I can map $(s,s')\mapsto(\frac{s+s'}{2},\frac{s-s'}{2})$ and modify the bounds of integration accordingly to obtain $$f(x)=(1-D)^{-2}g(x)=\frac{1}{2}\int_0^\infty ds\,e^{-s}g(s+x)\int_{-s}^{s} ds'=\int_0^\infty ds\,se^{-s}g(s+x).$$ So acting twice with $(1-D)^{-1}$ merely introduces a linear factor into the integration over $s$. This surely generalizes to $n$ applications of $(1-D)^{-1}$ (presumably by introducing a factor of $s^{n-1}$ instead) but I'm unfamiliar with such an identity. Can anyone supply a proof/reference?",,"['calculus', 'integration', 'multivariable-calculus']"
93,Inverting a function,Inverting a function,,"I am stuck with the following problem I am supposed to find the inverse of the function $g$ with $2$ variables, where $$\begin{align*}g&: R^2\to R^2 \\ g&(x,y)=(2ye^{2x}, xe^y)\end{align*}$$ I do not know even how to do it, the book does not give an example. Could you please tell me of a general method of how to invert this function? Thanks in advance!","I am stuck with the following problem I am supposed to find the inverse of the function $g$ with $2$ variables, where $$\begin{align*}g&: R^2\to R^2 \\ g&(x,y)=(2ye^{2x}, xe^y)\end{align*}$$ I do not know even how to do it, the book does not give an example. Could you please tell me of a general method of how to invert this function? Thanks in advance!",,"['real-analysis', 'analysis', 'multivariable-calculus']"
94,How do I find this partial derivative,How do I find this partial derivative,,"I have the following function u(x,y) defined as: $$u(x,y) = \frac {xy(x^2-y^2)}{(x^2+y^2)}$$ when x and y are both non zero, and $u(0,0)=0$ I want to compute its partial derivative $u_{xy}$ at (0,0). How do I do this?","I have the following function u(x,y) defined as: $$u(x,y) = \frac {xy(x^2-y^2)}{(x^2+y^2)}$$ when x and y are both non zero, and $u(0,0)=0$ I want to compute its partial derivative $u_{xy}$ at (0,0). How do I do this?",,"['multivariable-calculus', 'partial-derivative']"
95,How to convert Cartesian Vector to a Cylindrical Vector,How to convert Cartesian Vector to a Cylindrical Vector,,"I was wondering how exactly to convert a vector in cartesian coordinates, to one in cylindrical coordinates.  Given A $= 5x/(x^2+y^2) \hat i + 5y/(x^2+y^2) \hat j + z \hat k$ how would I convert A in terms of r, theta, and z? Sorry in advance for the awkwardness in the math script.","I was wondering how exactly to convert a vector in cartesian coordinates, to one in cylindrical coordinates.  Given A $= 5x/(x^2+y^2) \hat i + 5y/(x^2+y^2) \hat j + z \hat k$ how would I convert A in terms of r, theta, and z? Sorry in advance for the awkwardness in the math script.",,['multivariable-calculus']
96,Jacobian of parametrized ellipsoid with respect to parametrized sphere,Jacobian of parametrized ellipsoid with respect to parametrized sphere,,"I'm not even sure how best to phrase this question, but here goes. Given $\theta$ (elevation) and $\phi$ (azimuth), the unit sphere can be parametrized as $ x = \cos(\theta)\sin(\phi) \\ y = \cos(\theta)\cos(\phi) \\ z = \sin(\theta). $. A general ellipsoid can then be written as $X = ax$, $Y = by$, $z = cz$. I'm trying to find the Jacobian that tells you how the sphere was transformed to the ellipsoid. In my mind, this involved computing the following matrix $ \frac{\partial X}{\partial x},\frac{\partial X}{\partial y}, \frac{\partial X}{\partial z} \\ \frac{\partial Y}{\partial x}, \frac{\partial Y}{\partial y}, \frac{\partial Y}{\partial z} \\ \frac{\partial Z}{\partial x},\frac{\partial Z}{\partial y}, \frac{\partial Z}{\partial z} $ Is this correct, or should the ""matrix"" only have the diagonal entries? If it is not correct, would this idea be correct for an implicit surface? If it is correct, how do I do the differentiation for the off-diagonal entries? Could you work out a single example for differentiating a function with respect to $y$ and $z$? I've been cracking my head on this one, though it seems like it should be extremely simple.","I'm not even sure how best to phrase this question, but here goes. Given $\theta$ (elevation) and $\phi$ (azimuth), the unit sphere can be parametrized as $ x = \cos(\theta)\sin(\phi) \\ y = \cos(\theta)\cos(\phi) \\ z = \sin(\theta). $. A general ellipsoid can then be written as $X = ax$, $Y = by$, $z = cz$. I'm trying to find the Jacobian that tells you how the sphere was transformed to the ellipsoid. In my mind, this involved computing the following matrix $ \frac{\partial X}{\partial x},\frac{\partial X}{\partial y}, \frac{\partial X}{\partial z} \\ \frac{\partial Y}{\partial x}, \frac{\partial Y}{\partial y}, \frac{\partial Y}{\partial z} \\ \frac{\partial Z}{\partial x},\frac{\partial Z}{\partial y}, \frac{\partial Z}{\partial z} $ Is this correct, or should the ""matrix"" only have the diagonal entries? If it is not correct, would this idea be correct for an implicit surface? If it is correct, how do I do the differentiation for the off-diagonal entries? Could you work out a single example for differentiating a function with respect to $y$ and $z$? I've been cracking my head on this one, though it seems like it should be extremely simple.",,"['multivariable-calculus', 'differential-geometry', 'parametric']"
97,length of intersection of parabolic cylinder and a surface,length of intersection of parabolic cylinder and a surface,,"Let $C$ be the curve of intersection of the parabolic cylinder $x^2 = 2y$ and the surface $3z = xy$. Find the length of the part of $C$ from $(0, 0, 0)$ to $(6, 18, 36)$. (Hint: It may be useful to note the identity a $2 + 4a + 4 = (a + 2)^2$ in the middle of computation.) We were given the solution to this problem. I am given the two equations $x^2=xy$ and $3z=xy$ which are then parameterized into $\langle t,.5t^2,(1/6)t^3\rangle$. I was wondering if someone could help me with this parameterization step.","Let $C$ be the curve of intersection of the parabolic cylinder $x^2 = 2y$ and the surface $3z = xy$. Find the length of the part of $C$ from $(0, 0, 0)$ to $(6, 18, 36)$. (Hint: It may be useful to note the identity a $2 + 4a + 4 = (a + 2)^2$ in the middle of computation.) We were given the solution to this problem. I am given the two equations $x^2=xy$ and $3z=xy$ which are then parameterized into $\langle t,.5t^2,(1/6)t^3\rangle$. I was wondering if someone could help me with this parameterization step.",,"['multivariable-calculus', 'parametric']"
98,Divergence of a radial $1/r^2$ vector field,Divergence of a radial  vector field,1/r^2,"How to obtain the divergence of the function $F(r,\varphi,\theta)=\hat{r}/r^2$ where $\hat r$ is the unit vector in radial direction? Is there a solution without computing the surface integral for definition of divergence? The issue is: in this divergence, the delta function will be present. But if you obtain divergence from formula $\nabla\cdot F$, that is equal to zero. At the  point $r=0$,  this formula cannot be used.","How to obtain the divergence of the function $F(r,\varphi,\theta)=\hat{r}/r^2$ where $\hat r$ is the unit vector in radial direction? Is there a solution without computing the surface integral for definition of divergence? The issue is: in this divergence, the delta function will be present. But if you obtain divergence from formula $\nabla\cdot F$, that is equal to zero. At the  point $r=0$,  this formula cannot be used.",,"['multivariable-calculus', 'vector-analysis']"
99,Derivative of Quadratic Form as a Linear Approximation,Derivative of Quadratic Form as a Linear Approximation,,"I'm trying to find the derivative of the $quadratic$ form, for a $symmetric$ $n$ by $n$ matrix A and $ x \in \mathbb{R}^n $, $$ f(x) = x^tAx $$ such that the derivative is a linear map from $ \mathbb{R}^n \to \mathbb{R} $. From applying the chain rule, I was able to calculate the derivative as $ f'(x) = 2x^tA $ by expanding the matrix and using the chain rule (you end up with $ f'(x) = x^t(A^t + A) = 2x^tA $, but this clearly isn't a map from $ \mathbb{R}^n \to \mathbb{R} $, though it does appear to be the gradient of the function. Some help would be appreciated, thanks!","I'm trying to find the derivative of the $quadratic$ form, for a $symmetric$ $n$ by $n$ matrix A and $ x \in \mathbb{R}^n $, $$ f(x) = x^tAx $$ such that the derivative is a linear map from $ \mathbb{R}^n \to \mathbb{R} $. From applying the chain rule, I was able to calculate the derivative as $ f'(x) = 2x^tA $ by expanding the matrix and using the chain rule (you end up with $ f'(x) = x^t(A^t + A) = 2x^tA $, but this clearly isn't a map from $ \mathbb{R}^n \to \mathbb{R} $, though it does appear to be the gradient of the function. Some help would be appreciated, thanks!",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'differential-geometry']"
