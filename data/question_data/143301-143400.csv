,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to find this two-variables limit of $f$ as $(x,y)$ approaches to $(0,0)$?",How to find this two-variables limit of  as  approaches to ?,"f (x,y) (0,0)","I am trying to evaluate the limit $$\lim_{(x,y) \to (0,0)}\frac{2x^6-y^{10}}{x^2-y^3}$$ I have seen that both the reiterated limits are zero. Moreover, I have evaluated the limit by different curves such as $y=x^2$ and the limit equals zero as well. However, the polar coordinates $$\lim_{r \to 0}\frac{r^4(2\cos^6(\theta)-r^4\sin^{10}(\theta))}{\cos^2(\theta)-r\sin^3(\theta)}$$ make me think that the limit does not exist because it depends on the $\cos^2\theta$ factor, but I don't know how to prove it or which curve to use.","I am trying to evaluate the limit I have seen that both the reiterated limits are zero. Moreover, I have evaluated the limit by different curves such as and the limit equals zero as well. However, the polar coordinates make me think that the limit does not exist because it depends on the factor, but I don't know how to prove it or which curve to use.","\lim_{(x,y) \to (0,0)}\frac{2x^6-y^{10}}{x^2-y^3} y=x^2 \lim_{r \to 0}\frac{r^4(2\cos^6(\theta)-r^4\sin^{10}(\theta))}{\cos^2(\theta)-r\sin^3(\theta)} \cos^2\theta","['limits', 'multivariable-calculus', 'continuity']"
1,Does the mean of the ratio of the number of distinct prime factors to the number of divisors of natural numbers converge?,Does the mean of the ratio of the number of distinct prime factors to the number of divisors of natural numbers converge?,,"Let $d(n)$ and $\omega(n)$ be the number of divisors and the number of distinct prime factors of $n$ respectively. What is the limiting value of $$ \lim_{n \to \infty} \frac{1}{n}\sum_{r=1}^n \frac{\omega(r)}{d(r)} $$ For $n \le 23275000000 $ , the value is approximately $0.275967$ .","Let and be the number of divisors and the number of distinct prime factors of respectively. What is the limiting value of For , the value is approximately .","d(n) \omega(n) n 
\lim_{n \to \infty} \frac{1}{n}\sum_{r=1}^n \frac{\omega(r)}{d(r)}
 n \le 23275000000  0.275967","['limits', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'analytic-number-theory']"
2,Create Absolute Value Equation from 2 lines,Create Absolute Value Equation from 2 lines,,"I am trying to convert these 2 lines for an absolute value equation: $$y=\frac{10}{10-a}(x-a)$$ $$y=\frac{10}{a}(x-a)$$ $a$ is a parameter that has bounds $(0, 10)$ . The absolute value equation should use the first equation for positive y values and second equation for negative y values Here's a desmos link so you can easily see it: https://www.desmos.com/calculator/itxtp5eyij The reason for this is I am trying to calculate priority for tasks. $a$ is the amount of the task already completed on a range of 0 to 10, and $x$ is the time elapsed. ( $x=10$ means all the time has elapsed) The $y$ value would be the priority of the task. For example, if $a=10$ and $x=0$ it means that the task has been completed as soon as it started. Priority would start at -10 and increase over time until it hits zero. Similarly, if $a$ remains 0, as $x$ approaches 10, the priority would also increase to 10 at the same rate Any help converting this to an absolute value would be appreciated, although the main goal is to just have a concise code implementation for this (it's for an app)","I am trying to convert these 2 lines for an absolute value equation: is a parameter that has bounds . The absolute value equation should use the first equation for positive y values and second equation for negative y values Here's a desmos link so you can easily see it: https://www.desmos.com/calculator/itxtp5eyij The reason for this is I am trying to calculate priority for tasks. is the amount of the task already completed on a range of 0 to 10, and is the time elapsed. ( means all the time has elapsed) The value would be the priority of the task. For example, if and it means that the task has been completed as soon as it started. Priority would start at -10 and increase over time until it hits zero. Similarly, if remains 0, as approaches 10, the priority would also increase to 10 at the same rate Any help converting this to an absolute value would be appreciated, although the main goal is to just have a concise code implementation for this (it's for an app)","y=\frac{10}{10-a}(x-a) y=\frac{10}{a}(x-a) a (0, 10) a x x=10 y a=10 x=0 a x","['limits', 'trigonometry', 'rotations', 'absolute-value']"
3,Asymptotic behaviour of implicit functions,Asymptotic behaviour of implicit functions,,"Suppose we have an implicit equation $F\left(x,y\right)=0$ which we know defines $y = y(x)$ as a function of $x$ . Are there sufficient or necessary conditions under which we can obtain information about the asymptotic behaviour of $y(x)$ ? Particularly, what is the limit $\lim _{x\to +\infty }y\left(x\right)$ (if it even exists)? Or if we constrain ourselves to the domain $x,y>0$ then what is $\lim _{x\to 0^+ }y\left(x\right)?$","Suppose we have an implicit equation which we know defines as a function of . Are there sufficient or necessary conditions under which we can obtain information about the asymptotic behaviour of ? Particularly, what is the limit (if it even exists)? Or if we constrain ourselves to the domain then what is","F\left(x,y\right)=0 y = y(x) x y(x) \lim _{x\to +\infty }y\left(x\right) x,y>0 \lim _{x\to 0^+ }y\left(x\right)?","['calculus', 'limits', 'asymptotics', 'implicit-function-theorem', 'implicit-function']"
4,An alternating product/sum Fibonacci-like sequence,An alternating product/sum Fibonacci-like sequence,,"Starting from reals $a_1, a_2$ , define a sequence $S(a_1, a_2)$ $$a_1, a_2, a_3, a_4, \ldots, a_{k-1}, a_{k}, a_{k+1}, a_{k+2}, \ldots \;,$$ so that, given the sequence up to its $k$ -th element, $k \ge 2$ even, the next two elements are defined by \begin{eqnarray} a_{k+1} &=& a_{k-1} &\times& a_k \\ a_{k+2} &=& a_k &+& a_{k+1} \end{eqnarray} For example, the first ten elements of $S(\frac{1}{2},-\frac{1}{2})$ are: $$ \frac{1}{2},-\frac{1}{2}    ,-\frac{1}{4},-\frac{3}{4}    ,\frac{3}{16},-\frac{9}{16}    ,-\frac{27}{256},-\frac{171}{256}    ,\frac{4617}{65536},-\frac{39159}{65536},\ldots \;. $$ I wanted to explore mixing multiplication and addition. So this is like the Fibonacci sequence, except that the steps alternate between  multiplication and addition. Two questions, one specific, one general. Q1 . Numerically, the odd elements of $S(\frac{1}{2},-\frac{1}{2})$ head to $0$ , and the even elements head to $\approx -0.622748$ . You can see in the list of rationals above that the denominators are $2^m$ where $m = 2^ {\lfloor (k-1)/2 \rfloor }$ . But the pattern in the numerators seems difficult to see. How can the even and odd limits be proven, either by explicit calculation of the numerators, or by identifying dominate terms that lead to proofs of convergence? Q2 . For which $a_1, a_2$ do both the even and odd elements of $S(a_1, a_2)$ converge? It seems the convergence region in the $(a_1, a_2)$ plane includes the two diagonally opposite squares \begin{eqnarray} a_1 \in (0,1) &\;\;\mathrm{and}\;& a_2 \in (-1,0)\\ a_1 \in (-1,0) &\;\;\mathrm{and}\;& a_2 \in (0,1) \end{eqnarray} but is significantly broader. For example, $S(2, -\frac{1}{3})$ converges.","Starting from reals , define a sequence so that, given the sequence up to its -th element, even, the next two elements are defined by For example, the first ten elements of are: I wanted to explore mixing multiplication and addition. So this is like the Fibonacci sequence, except that the steps alternate between  multiplication and addition. Two questions, one specific, one general. Q1 . Numerically, the odd elements of head to , and the even elements head to . You can see in the list of rationals above that the denominators are where . But the pattern in the numerators seems difficult to see. How can the even and odd limits be proven, either by explicit calculation of the numerators, or by identifying dominate terms that lead to proofs of convergence? Q2 . For which do both the even and odd elements of converge? It seems the convergence region in the plane includes the two diagonally opposite squares but is significantly broader. For example, converges.","a_1, a_2 S(a_1, a_2) a_1, a_2, a_3, a_4, \ldots, a_{k-1}, a_{k}, a_{k+1}, a_{k+2}, \ldots \;, k k \ge 2 \begin{eqnarray}
a_{k+1} &=& a_{k-1} &\times& a_k \\
a_{k+2} &=& a_k &+& a_{k+1}
\end{eqnarray} S(\frac{1}{2},-\frac{1}{2}) 
\frac{1}{2},-\frac{1}{2}
   ,-\frac{1}{4},-\frac{3}{4}
   ,\frac{3}{16},-\frac{9}{16}
   ,-\frac{27}{256},-\frac{171}{256}
   ,\frac{4617}{65536},-\frac{39159}{65536},\ldots \;.
 S(\frac{1}{2},-\frac{1}{2}) 0 \approx -0.622748 2^m m = 2^ {\lfloor (k-1)/2 \rfloor } a_1, a_2 S(a_1, a_2) (a_1, a_2) \begin{eqnarray}
a_1 \in (0,1) &\;\;\mathrm{and}\;& a_2 \in (-1,0)\\
a_1 \in (-1,0) &\;\;\mathrm{and}\;& a_2 \in (0,1)
\end{eqnarray} S(2, -\frac{1}{3})","['sequences-and-series', 'limits']"
5,Under what conditions does the limit f(g(x)) exist?,Under what conditions does the limit f(g(x)) exist?,,Under what conditions does $$ \lim_{x \to a} f(g(x)) $$ exist? I've seen examples like this from Khan that show you can use one-sided limits and discontinuities to break a lot of expectations around limits and still have an answer: So what's the rule for when this composite limit exists?,Under what conditions does exist? I've seen examples like this from Khan that show you can use one-sided limits and discontinuities to break a lot of expectations around limits and still have an answer: So what's the rule for when this composite limit exists?,"
\lim_{x \to a} f(g(x))
","['calculus', 'limits', 'continuity']"
6,Diverge? $\lim_{n \to \infty} \sin(n)+ \sin(n^2)=?$,Diverge?,\lim_{n \to \infty} \sin(n)+ \sin(n^2)=?,"I encounter with a problem that is to find: $$\lim_{n \to \infty, n \in \mathbb{N}} \sin(n)+ \sin(n^2)=?$$ So I do know the set $\{\sin n: n \in \mathbb{N}\}$ is dense in $[-1,1]$ and also knowing that $\lim_{n \to \infty} \sin(n^2) $ does not converge and it even has infinite limit points. However, I'm running in a trouble that is proving that if $\sin(n)+ \sin(n^2)$ converges, and exists such a subsequence $\{n_k\}_{k\geq 1} \subset \mathbb{N}$ , $\lim_{k \to \infty} \sin(n_k^2) = a$ then: $$\lim_{k \to \infty} \sin(n_k) \neq L - a$$ .","I encounter with a problem that is to find: So I do know the set is dense in and also knowing that does not converge and it even has infinite limit points. However, I'm running in a trouble that is proving that if converges, and exists such a subsequence , then: .","\lim_{n \to \infty, n \in \mathbb{N}} \sin(n)+ \sin(n^2)=? \{\sin n: n \in \mathbb{N}\} [-1,1] \lim_{n \to \infty} \sin(n^2)  \sin(n)+ \sin(n^2) \{n_k\}_{k\geq 1} \subset \mathbb{N} \lim_{k \to \infty} \sin(n_k^2) = a \lim_{k \to \infty} \sin(n_k) \neq L - a","['real-analysis', 'limits']"
7,"How to calculate the series $ \sum_{n=1}^{+\infty}\frac{1}{x_n^2} $ if $ x_1, x_2, ..., x_n, ... $ are all positive roots for $ 2020\tan x=2021x $.",How to calculate the series  if  are all positive roots for .," \sum_{n=1}^{+\infty}\frac{1}{x_n^2}   x_1, x_2, ..., x_n, ...   2020\tan x=2021x ","If $ x_1, x_2, ..., x_n, ... $ are all positive roots for $ 2020\tan x=2021x $ , it is easy to verify that $ \sum_{n=1}^{+\infty}\frac{1}{x_n^2} $ is convergent. However I can not calculate the exact value for the limit. What I can do is to estimate the order of $ \frac{1}{x_n^2} $ , which is not useful for the calculation. I know that the equation $ 2020\tan x=2021x $ may have some relations with the problem of eigenvalues for the wave equation, but I cannot use it to solve this problem either. Can you give me some hints or references?","If are all positive roots for , it is easy to verify that is convergent. However I can not calculate the exact value for the limit. What I can do is to estimate the order of , which is not useful for the calculation. I know that the equation may have some relations with the problem of eigenvalues for the wave equation, but I cannot use it to solve this problem either. Can you give me some hints or references?"," x_1, x_2, ..., x_n, ...   2020\tan x=2021x   \sum_{n=1}^{+\infty}\frac{1}{x_n^2}   \frac{1}{x_n^2}   2020\tan x=2021x ","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
8,Sum of a list of numbers compared to the sum of the absolute value of a list of numbers in infinite limit,Sum of a list of numbers compared to the sum of the absolute value of a list of numbers in infinite limit,,"I have a list of $d$ real numbers $v=[b_1,b_2,...b_d]$ . I now wish to consider a sequence which depends on this list. For $n=1$ I have the following two terms $$X_1=\frac{1}{d}\left|\sum_ib_i\right|, \quad Y_1=\frac{1}{d}\sum_i|{b_i}|.$$ Clearly $Y_1\geq X_1$ . For $n=2$ , I consider $$X_2=\frac{1}{d^2\times 2}\left|\sum_{i,j}(b_i+b_j)\right|=X_1, \quad Y_2=\frac{1}{d^2\times2}\sum_{i,j}|b_i+b_j|.$$ And so on. For $n=m$ , \begin{gather*} X_m=\frac{1}{d^m\times m}\left|\sum_{a_1,\cdots,a_m}(b_{a_1}+b_{a_2}+\cdots+b_{a_m})\right|=X_1,\\ Y_m=\frac{1}{d^m\times m}\sum_{a_1,\cdots,a_m}|b_{a_1}+b_{a_2}+\cdots+b_{a_m}|. \end{gather*} I see numerically that as $n\rightarrow\infty$ , $$Y_m\to X_m.$$ Does anyone know when this is true? Or can anyone point me to any resources that might talk about similar ideas? In other words is the following true: \begin{gather*} \lim_{m\to\infty} \frac{1}{md^m}\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}|b_{a_1}+\cdots+b_{a_m}|\\ =\lim_{m\to\infty} \frac{1}{md^m}\left|\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}(b_{a_1}+\cdots+b_{a_m})\right|=\frac{1}{d}\left|\sum_{i=1}^{d}b_i\right|\ ? \end{gather*} In addition to seeing that numerically this appears to be true I have some intuition for why it may be true. There are infinitely many $b_{a_i}$ terms being summed in each term in the summation. So terms which have only a few negative $b_{a_i}$ terms will be asymptotically the same as terms with zero negative $b_{a_i}$ terms (for which the two sums coincide). Also when there are a roughly equal number of positive and negative $b_{a_i}$ terms the two sums will also be asymptotically the same. Obviously this is not at all rigorous and this is where I need help.","I have a list of real numbers . I now wish to consider a sequence which depends on this list. For I have the following two terms Clearly . For , I consider And so on. For , I see numerically that as , Does anyone know when this is true? Or can anyone point me to any resources that might talk about similar ideas? In other words is the following true: In addition to seeing that numerically this appears to be true I have some intuition for why it may be true. There are infinitely many terms being summed in each term in the summation. So terms which have only a few negative terms will be asymptotically the same as terms with zero negative terms (for which the two sums coincide). Also when there are a roughly equal number of positive and negative terms the two sums will also be asymptotically the same. Obviously this is not at all rigorous and this is where I need help.","d v=[b_1,b_2,...b_d] n=1 X_1=\frac{1}{d}\left|\sum_ib_i\right|, \quad Y_1=\frac{1}{d}\sum_i|{b_i}|. Y_1\geq X_1 n=2 X_2=\frac{1}{d^2\times 2}\left|\sum_{i,j}(b_i+b_j)\right|=X_1, \quad Y_2=\frac{1}{d^2\times2}\sum_{i,j}|b_i+b_j|. n=m \begin{gather*}
X_m=\frac{1}{d^m\times m}\left|\sum_{a_1,\cdots,a_m}(b_{a_1}+b_{a_2}+\cdots+b_{a_m})\right|=X_1,\\
Y_m=\frac{1}{d^m\times m}\sum_{a_1,\cdots,a_m}|b_{a_1}+b_{a_2}+\cdots+b_{a_m}|.
\end{gather*} n\rightarrow\infty Y_m\to X_m. \begin{gather*}
\lim_{m\to\infty} \frac{1}{md^m}\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}|b_{a_1}+\cdots+b_{a_m}|\\
=\lim_{m\to\infty} \frac{1}{md^m}\left|\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}(b_{a_1}+\cdots+b_{a_m})\right|=\frac{1}{d}\left|\sum_{i=1}^{d}b_i\right|\ ?
\end{gather*} b_{a_i} b_{a_i} b_{a_i} b_{a_i}","['sequences-and-series', 'limits', 'absolute-value']"
9,How does the limit law $\lim_{x \to a}f\bigl(g(x)\bigr)=f\left(\lim_{x \to a}g(x)\right)$ work?,How does the limit law  work?,\lim_{x \to a}f\bigl(g(x)\bigr)=f\left(\lim_{x \to a}g(x)\right),"I have $2$ questions regarding the following limit law: Suppose that $\lim_{x \to a}g(x)$ exists and is equal to $l$ , and that $f$ is continuous at $l$ . Then, $\lim_{x \to a}f\bigl(g(x)\bigr)$ exists, and $$ \DeclareMathOperator{\epsilon}{\varepsilon}  \lim_{x \to a}f\bigl(g(x)\bigr)=f(l)=f\left(\lim_{x \to a}g(x)\right) \, . $$ My questions are: Have I stated this limit law correctly? How do you prove this limit law? Here is my attempted proof: Let $\epsilon>0$ . We wish to find a $\delta>0$ such that, for all $x$ , If $0<|x-a|<\delta$ , then $|f(g(x))-f(l)|<\epsilon$ . We are given that $f$ is continuous at $l$ , i.e. that there exists a $\delta'>0$ such that, for all $y$ , If $|y-l|<\delta'$ , then $|f(y)-f(l)|<\epsilon$ . We are also given that $\lim_{x \to a}g(x)=l$ , meaning that there is a $\delta>0$ such that If $0<|x-a|<\delta$ , then $|g(x)-l|<\delta'$ Since $g(x)$ is a number $y$ satisfying $|y-l|<\delta'$ , we get that $|f(g(x))-f(l)|<\epsilon$ . Hence, if $0<|x-a|<\delta$ , then $|f(g(x))-f(l)|$ , and so $\lim_{x \to a}f(g(x))=f(l)=f\left(\lim_{x \to a}g(x)\right)$ , completing the proof.","I have questions regarding the following limit law: Suppose that exists and is equal to , and that is continuous at . Then, exists, and My questions are: Have I stated this limit law correctly? How do you prove this limit law? Here is my attempted proof: Let . We wish to find a such that, for all , If , then . We are given that is continuous at , i.e. that there exists a such that, for all , If , then . We are also given that , meaning that there is a such that If , then Since is a number satisfying , we get that . Hence, if , then , and so , completing the proof.","2 \lim_{x \to a}g(x) l f l \lim_{x \to a}f\bigl(g(x)\bigr) 
\DeclareMathOperator{\epsilon}{\varepsilon} 
\lim_{x \to a}f\bigl(g(x)\bigr)=f(l)=f\left(\lim_{x \to a}g(x)\right) \, .
 \epsilon>0 \delta>0 x 0<|x-a|<\delta |f(g(x))-f(l)|<\epsilon f l \delta'>0 y |y-l|<\delta' |f(y)-f(l)|<\epsilon \lim_{x \to a}g(x)=l \delta>0 0<|x-a|<\delta |g(x)-l|<\delta' g(x) y |y-l|<\delta' |f(g(x))-f(l)|<\epsilon 0<|x-a|<\delta |f(g(x))-f(l)| \lim_{x \to a}f(g(x))=f(l)=f\left(\lim_{x \to a}g(x)\right)","['real-analysis', 'calculus', 'limits', 'continuity', 'solution-verification']"
10,Looking for: $\lim_{n \to \infty}\frac{n!}{B_n}\sum_{i=0}^{n}\frac{B_{n-i}B_{i}}{(n-i)!i!}x^{1-i}=F(x)$,Looking for:,\lim_{n \to \infty}\frac{n!}{B_n}\sum_{i=0}^{n}\frac{B_{n-i}B_{i}}{(n-i)!i!}x^{1-i}=F(x),"Let $n=2k,k=1,2,3,,$ Proposed: $$\lim_{n \to \infty}\frac{n!}{B_n}\sum_{i=0}^{n}\frac{B_{n-i}B_{i}}{(n-i)!i!}x^{1-i}=F(x)\tag1$$ Where $B_n$ is Bernoulli numbers $\phi=\frac{1+\sqrt{5}}{2}$ I am looking for $F(x)$ , I was able to evaluate for some values of $F(x)$ , such as $$F(3)=\frac{\pi}{\sqrt{3}}$$ $$F(4)=\pi$$ $$F(5)=\frac{\pi\phi^2}{\sqrt{\phi^2+1}}$$ $$F(6)=\pi\sqrt{3}$$ $$F(8)=\pi(1+\sqrt{2})$$ $$F(10)=\pi\sqrt{\phi^3\sqrt{5}}$$ $$F(12)=\pi(2+\sqrt{3})$$","Let Proposed: Where is Bernoulli numbers I am looking for , I was able to evaluate for some values of , such as","n=2k,k=1,2,3,, \lim_{n \to \infty}\frac{n!}{B_n}\sum_{i=0}^{n}\frac{B_{n-i}B_{i}}{(n-i)!i!}x^{1-i}=F(x)\tag1 B_n \phi=\frac{1+\sqrt{5}}{2} F(x) F(x) F(3)=\frac{\pi}{\sqrt{3}} F(4)=\pi F(5)=\frac{\pi\phi^2}{\sqrt{\phi^2+1}} F(6)=\pi\sqrt{3} F(8)=\pi(1+\sqrt{2}) F(10)=\pi\sqrt{\phi^3\sqrt{5}} F(12)=\pi(2+\sqrt{3})","['sequences-and-series', 'limits']"
11,"Pointwise convergence of $f_n(x) = \sum_{i=1}^n \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right)$",Pointwise convergence of,"f_n(x) = \sum_{i=1}^n \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right)","Consider the following sequence of functions: \begin{align} f_n(x) = \sum_{i=1}^n  \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right). \end{align} We are interested in understanding how does this function convergence pointwise as $n \to \infty$ for all $x\in [0,1]$ . That is, what is \begin{align} f(x)=\lim_{n \to \infty} f_n(x) \end{align} Some thoughts: This is a periodic function that oscillates between $1$ and $-1$ .  As $n$ increases the period shrinks. I have a feeling that the limit might not exist here for all $x$ .","Consider the following sequence of functions: We are interested in understanding how does this function convergence pointwise as for all . That is, what is Some thoughts: This is a periodic function that oscillates between and .  As increases the period shrinks. I have a feeling that the limit might not exist here for all .","\begin{align}
f_n(x) = \sum_{i=1}^n  \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right).
\end{align} n \to \infty x\in [0,1] \begin{align}
f(x)=\lim_{n \to \infty} f_n(x)
\end{align} 1 -1 n x","['real-analysis', 'limits']"
12,"if $\lim_{x \to a}(f \circ g) (x)=L$, then we cannot generally conclude that $ \lim_{x \to g(a)}f(x)=L$","if , then we cannot generally conclude that",\lim_{x \to a}(f \circ g) (x)=L  \lim_{x \to g(a)}f(x)=L,"I've been playing around with the limits of compositions...e.g. $\displaystyle \lim_{x \to a}(f \circ g) (x)$ ...and wanted to confirm my intuition that just because $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , it is not necessarily the case that $\displaystyle \lim_{x \to g(a)}f(x)=L$ . Said differently, if $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , then we cannot generally conclude that $\displaystyle \lim_{x \to g(a)}f(x)=L$ . The pictures (hand drawn, sorry) I had in mind are the following: where the function $g$ maintains its value for all $x \geq a$ . The definition of $\displaystyle \lim_{x \to a}(f \circ g) (x)=L$ , which is $\forall \epsilon \gt 0 \exists \delta \gt 0 \forall x \in \mathbb R \big [ 0 \lt \lvert x -a \rvert \lt \delta \rightarrow \lvert (f \circ g)(x) - L \rvert \lt \epsilon \big ] $ for $L = f(g(a))$ , seems to be satisfied. From the graph of $f$ , clearly $\displaystyle \lim_{x \to g(a)}f(x)\neq f(g(a))$ . Is this the correct intuition?","I've been playing around with the limits of compositions...e.g. ...and wanted to confirm my intuition that just because , it is not necessarily the case that . Said differently, if , then we cannot generally conclude that . The pictures (hand drawn, sorry) I had in mind are the following: where the function maintains its value for all . The definition of , which is for , seems to be satisfied. From the graph of , clearly . Is this the correct intuition?",\displaystyle \lim_{x \to a}(f \circ g) (x) \displaystyle \lim_{x \to a}(f \circ g) (x)=L \displaystyle \lim_{x \to g(a)}f(x)=L \displaystyle \lim_{x \to a}(f \circ g) (x)=L \displaystyle \lim_{x \to g(a)}f(x)=L g x \geq a \displaystyle \lim_{x \to a}(f \circ g) (x)=L \forall \epsilon \gt 0 \exists \delta \gt 0 \forall x \in \mathbb R \big [ 0 \lt \lvert x -a \rvert \lt \delta \rightarrow \lvert (f \circ g)(x) - L \rvert \lt \epsilon \big ]  L = f(g(a)) f \displaystyle \lim_{x \to g(a)}f(x)\neq f(g(a)),"['calculus', 'limits']"
13,"$ (a_n) $ is a sequence s.t. its partial limits are $ 3 , - 1 $. Define $ b_n = | a_n - 1 | $. Show $ b_n \rightarrow 2 $",is a sequence s.t. its partial limits are . Define . Show," (a_n)   3 , - 1   b_n = | a_n - 1 |   b_n \rightarrow 2 ","Problem: Let $ (a_n) $ be a sequence s.t. its partial limits are only from the set $ \{ 3 , - 1 \} $ ( infinite limits are not allowed to be partial limits of $ (a_n) $ ). Define a new sequence $ b_n = | a_n - 1 | $ . Show that $ b_n \rightarrow 2 $ . Attempt: Suppose that $ b_n \nrightarrow 2 $ . Meaning there exists $ \epsilon>0 $ s.t. $ \forall N \in \mathbb{N} . \exists n^*>N.| |a_n - 1 | - 2 | \geq \epsilon  $ . Choose $ N=1 $ , then there exists $ n^* > N $ s.t. $ | |a_n^* - 1 | - 2 | \geq \epsilon \iff | |a_n^* - 1 | - 2 | \geq \epsilon \iff  |a_n^* - 1 | - 2  \geq \epsilon$ or $ |a_n^* - 1 | - 2  \leq - \epsilon $ . [ I am stuck. I don't see how I can continue if I assume wlog either $ |a_n^* - 1 | - 2  \geq \epsilon $ or $ |a_n^* - 1 | - 2  \leq - \epsilon $ , I think a better option would be If I wouldn't  try to prove by contradiction, but that attempt was failure for me as well ]. Do you have any ideas how I should continue? or if I don't choose to prove by contradiction, how would one proceed after taking arbitrary $ \epsilon > 0 $ and using the givens that $3 ,-1  $ are partial limits? Edit : The teacher-assistant told me that the theorem is true.","Problem: Let be a sequence s.t. its partial limits are only from the set ( infinite limits are not allowed to be partial limits of ). Define a new sequence . Show that . Attempt: Suppose that . Meaning there exists s.t. . Choose , then there exists s.t. or . [ I am stuck. I don't see how I can continue if I assume wlog either or , I think a better option would be If I wouldn't  try to prove by contradiction, but that attempt was failure for me as well ]. Do you have any ideas how I should continue? or if I don't choose to prove by contradiction, how would one proceed after taking arbitrary and using the givens that are partial limits? Edit : The teacher-assistant told me that the theorem is true."," (a_n)   \{ 3 , - 1 \}   (a_n)   b_n = | a_n - 1 |   b_n \rightarrow 2   b_n \nrightarrow 2   \epsilon>0   \forall N \in \mathbb{N} . \exists n^*>N.| |a_n - 1 | - 2 | \geq \epsilon    N=1   n^* > N   | |a_n^* - 1 | - 2 | \geq \epsilon \iff | |a_n^* - 1 | - 2 | \geq \epsilon \iff  |a_n^* - 1 | - 2  \geq \epsilon  |a_n^* - 1 | - 2  \leq - \epsilon   |a_n^* - 1 | - 2  \geq \epsilon   |a_n^* - 1 | - 2  \leq - \epsilon   \epsilon > 0  3 ,-1  ","['real-analysis', 'limits']"
14,How to solve the following limit similar to factorial type,How to solve the following limit similar to factorial type,,"Let $\alpha>1$ be a fixed real number and $M(x)=\max\left\{m\in\mathbb{N}:m!\le\alpha^x\right\}$ , prove that $$\displaystyle\lim_{n\to\infty}\displaystyle\frac{\sqrt[n]{M(1)M(2)\cdots M(n)}}{M(n)}=e^{-1}.$$ Use $$n!\sim\sqrt{2\pi n}\left(\frac{n}{e}\right)^n.$$ I got that $$\lim\limits_{n\to\infty}\dfrac{\sqrt[n]{n!}}{n}=e^{-1}.$$ But I don't know how to go on. The properties of this function $M(x)$ should be critical. Is there any way to parse this function?","Let be a fixed real number and , prove that Use I got that But I don't know how to go on. The properties of this function should be critical. Is there any way to parse this function?",\alpha>1 M(x)=\max\left\{m\in\mathbb{N}:m!\le\alpha^x\right\} \displaystyle\lim_{n\to\infty}\displaystyle\frac{\sqrt[n]{M(1)M(2)\cdots M(n)}}{M(n)}=e^{-1}. n!\sim\sqrt{2\pi n}\left(\frac{n}{e}\right)^n. \lim\limits_{n\to\infty}\dfrac{\sqrt[n]{n!}}{n}=e^{-1}. M(x),"['real-analysis', 'limits']"
15,Evaluate the limiting value using mean-value theorem,Evaluate the limiting value using mean-value theorem,,"Prove that $$\lim_{n\rightarrow\infty}\int_{n}^{n+ 1}\sin e^{x}{\rm d}x= 0$$ I already have a solution but I want to see a proof(s) using mean-value theorem, it will be travelled ! Now let me show $$\left | \int_{n}^{n+ 1}\sin e^{x}{\rm d}x \right |= \left | \int_{e^{n}}^{e^{n+ 1}}\frac{\sin x}{x}{\rm d}x \right |\leq\left | \left ( \frac{\cos e^{n}}{e^{n}}- \frac{\cos e^{n+ 1}}{e^{n+ 1}} \right )- \int_{e^{n}}^{e^{n+ 1}}\frac{\cos x}{x^{2}}{\rm d}x \right |\leq$$ $$\leq\left | \frac{\cos e^{n}}{e^{n}}- \frac{\cos e^{n+ 1}}{e^{n+ 1}} \right |+ \int_{e^{n}}^{e^{n+ 1}}\frac{{\rm d}x}{x^{2}}\leq\frac{1}{e^{n+ 1}}+ \frac{1}{e^{n}}+ \frac{1}{e^{n+ 1}}+ \frac{1}{e^{n}}\rightarrow 0\,{\rm as}\,n\rightarrow\infty$$","Prove that I already have a solution but I want to see a proof(s) using mean-value theorem, it will be travelled ! Now let me show","\lim_{n\rightarrow\infty}\int_{n}^{n+ 1}\sin e^{x}{\rm d}x= 0 \left | \int_{n}^{n+ 1}\sin e^{x}{\rm d}x \right |= \left | \int_{e^{n}}^{e^{n+ 1}}\frac{\sin x}{x}{\rm d}x \right |\leq\left | \left ( \frac{\cos e^{n}}{e^{n}}- \frac{\cos e^{n+ 1}}{e^{n+ 1}} \right )- \int_{e^{n}}^{e^{n+ 1}}\frac{\cos x}{x^{2}}{\rm d}x \right |\leq \leq\left | \frac{\cos e^{n}}{e^{n}}- \frac{\cos e^{n+ 1}}{e^{n+ 1}} \right |+ \int_{e^{n}}^{e^{n+ 1}}\frac{{\rm d}x}{x^{2}}\leq\frac{1}{e^{n+ 1}}+ \frac{1}{e^{n}}+ \frac{1}{e^{n+ 1}}+ \frac{1}{e^{n}}\rightarrow 0\,{\rm as}\,n\rightarrow\infty","['real-analysis', 'calculus']"
16,Limit of Sequence??,Limit of Sequence??,,"The sequence: $-2, 0, -1, 0, -1/2, 0, -1/4, 0, -1/8, 0, -1/16, 0\ldots$ Heyy. So I was a little confused about this question. How would this have an existing limit if it keeps alternating between a value and $0$ ? I know if you have a sequence of $1, -1, 1, -1\ldots$ that would not have a limit. So why is the sequence above have a limit? Thank you in advance for any help!",The sequence: Heyy. So I was a little confused about this question. How would this have an existing limit if it keeps alternating between a value and ? I know if you have a sequence of that would not have a limit. So why is the sequence above have a limit? Thank you in advance for any help!,"-2, 0, -1, 0, -1/2, 0, -1/4, 0, -1/8, 0, -1/16, 0\ldots 0 1, -1, 1, -1\ldots","['calculus', 'sequences-and-series', 'limits']"
17,Justify $\int_0^\infty \frac{\sin(x)}{x}dx = \lim_{\theta\to 0}\sum_{n=0}^\infty\left( \theta \cdot \frac{\sin(n\theta)}{n\theta} \right)$,Justify,\int_0^\infty \frac{\sin(x)}{x}dx = \lim_{\theta\to 0}\sum_{n=0}^\infty\left( \theta \cdot \frac{\sin(n\theta)}{n\theta} \right),"I encounter the following equation $$\int_0^\infty \frac{\sin(x)}{x}dx = \lim_{\theta\to 0}\sum_{n=0}^\infty\left( \theta \cdot \frac{\sin(n\theta)}{n\theta} \right).$$ Intuitively, I think the limit on the RHS is related to Riemann sum. However, I am not able to understand the reason behind it. Any hint is appreciated.","I encounter the following equation Intuitively, I think the limit on the RHS is related to Riemann sum. However, I am not able to understand the reason behind it. Any hint is appreciated.",\int_0^\infty \frac{\sin(x)}{x}dx = \lim_{\theta\to 0}\sum_{n=0}^\infty\left( \theta \cdot \frac{\sin(n\theta)}{n\theta} \right).,"['calculus', 'integration', 'limits', 'summation', 'riemann-sum']"
18,"For any integer $n$, $a_n$ and $b_n$ are two real numbers and function....[CONT]","For any integer ,  and  are two real numbers and function....[CONT]",n a_n b_n,"For any integer $n$ , $a_n$ and $b_n$ are two real numbers and function $$f(x)=\begin{cases} a_n + \sin \pi x & x\in [2n, 2n+1] \\\ b_n +\cos \pi x & x\in (2n-1,2n) \end{cases}$$ If $f(x)$ is continuous prove that $a_{n-1} - b_n=-1$ At $x=2n$ , $$f(2n) =a_n$$ And $$\lim_{x \to 2n^-} b_n+\cos \pi x$$ $$=\lim _{h\to 0^+} b_n +\cos \pi (2n-h)$$ $$=\lim_{h\to 0^+} b_n +\cos \pi h$$ $$=b_n+1$$ So $$a_n-b_n=1$$ But I am not able to get $a_{n-1}$ in any expression. How should I do that? Edit For the part I am not clear: Why has the RHL been chosen as the second function at $x=2n+1$ and why does $b_n$ change to $b_{n+1}$ ?","For any integer , and are two real numbers and function If is continuous prove that At , And So But I am not able to get in any expression. How should I do that? Edit For the part I am not clear: Why has the RHL been chosen as the second function at and why does change to ?","n a_n b_n f(x)=\begin{cases} a_n + \sin \pi x & x\in [2n, 2n+1] \\\ b_n +\cos \pi x & x\in (2n-1,2n) \end{cases} f(x) a_{n-1} - b_n=-1 x=2n f(2n) =a_n \lim_{x \to 2n^-} b_n+\cos \pi x =\lim _{h\to 0^+} b_n +\cos \pi (2n-h) =\lim_{h\to 0^+} b_n +\cos \pi h =b_n+1 a_n-b_n=1 a_{n-1} x=2n+1 b_n b_{n+1}","['limits', 'continuity']"
19,L'Hopital's Rule Complication,L'Hopital's Rule Complication,,"$\textrm{Let } f(x) = \begin{cases} \displaystyle\frac{g(x)}{x}~, & \!\! x \neq 0 \\ 0~, & \!\! x = 0 \end{cases} \textrm{ for all } x \in \mathbb{R}.$ $\textrm{Assume } g(0) = g'(0) = 0 \wedge g''(0) = 17.$ $\textrm{Want To Prove } f'(0) = \displaystyle\frac{17}{2}.$ Information Necessary For The Use Of L'Hopital's Rule: Important Lemmas I showed that $f$ is continuous at $0$ so that $f'(0)$ can be computed using the limit definition. In order to compute $f'(0)$ , I wish to use L'Hopital's Rule (LHR). $f'(0)$ $= \displaystyle\lim_{x \rightarrow 0} \frac{f(0 + x) - f(0)}{x}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{f(x) - f(0)}{x}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{f(x) - 0}{x}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{f(x)}{x}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{\displaystyle\frac{g(x)}{x}}{x}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{g(x)}{x^2}$ $= \displaystyle\lim_{x \rightarrow 0} \frac{g'(x)}{2x}~~~$ According to LHR: Justification In order to get $\displaystyle\frac{17}{2}$ , I must use L'Hopital's Rule again to get $\displaystyle\lim_{x \rightarrow 0} \frac{g''(x)}{2}$ . However, so far, I cannot show that $g''(x)$ is never $0$ as $x \rightarrow 0$ , which is an important property justifying the use of LHR. In other words, I have no continuity assumption or third derivative to prove there exists an interval containing $0$ where $g''(x)$ is never $0$ , using the information I have derived. This is my attempt so far of justifying my second use of LHR: Justification 2 As you can see, I am missing the final step regarding $g''(x)$ . Any help would be appreciated. Let me know if there is any function $g$ where $f'(0)$ wouldn't be $\displaystyle\frac{17}{2}$ , in which case I must assume something about $g$ or $g''$ .","Information Necessary For The Use Of L'Hopital's Rule: Important Lemmas I showed that is continuous at so that can be computed using the limit definition. In order to compute , I wish to use L'Hopital's Rule (LHR). According to LHR: Justification In order to get , I must use L'Hopital's Rule again to get . However, so far, I cannot show that is never as , which is an important property justifying the use of LHR. In other words, I have no continuity assumption or third derivative to prove there exists an interval containing where is never , using the information I have derived. This is my attempt so far of justifying my second use of LHR: Justification 2 As you can see, I am missing the final step regarding . Any help would be appreciated. Let me know if there is any function where wouldn't be , in which case I must assume something about or .","\textrm{Let } f(x) = \begin{cases} \displaystyle\frac{g(x)}{x}~, & \!\! x \neq 0 \\ 0~, & \!\! x = 0 \end{cases} \textrm{ for all } x \in \mathbb{R}. \textrm{Assume } g(0) = g'(0) = 0 \wedge g''(0) = 17. \textrm{Want To Prove } f'(0) = \displaystyle\frac{17}{2}. f 0 f'(0) f'(0) f'(0) = \displaystyle\lim_{x \rightarrow 0} \frac{f(0 + x) - f(0)}{x} = \displaystyle\lim_{x \rightarrow 0} \frac{f(x) - f(0)}{x} = \displaystyle\lim_{x \rightarrow 0} \frac{f(x) - 0}{x} = \displaystyle\lim_{x \rightarrow 0} \frac{f(x)}{x} = \displaystyle\lim_{x \rightarrow 0} \frac{\displaystyle\frac{g(x)}{x}}{x} = \displaystyle\lim_{x \rightarrow 0} \frac{g(x)}{x^2} = \displaystyle\lim_{x \rightarrow 0} \frac{g'(x)}{2x}~~~ \displaystyle\frac{17}{2} \displaystyle\lim_{x \rightarrow 0} \frac{g''(x)}{2} g''(x) 0 x \rightarrow 0 0 g''(x) 0 g''(x) g f'(0) \displaystyle\frac{17}{2} g g''","['limits', 'functions', 'derivatives', 'continuity']"
20,"Problem with showing $\lim_{n\rightarrow \infty} \int_A \cos(nxy) \, d\lambda_2=0$",Problem with showing,"\lim_{n\rightarrow \infty} \int_A \cos(nxy) \, d\lambda_2=0","I need to show that $$\lim_{n\rightarrow \infty} \int\limits_A \cos(nxy) \, d\lambda_2=0$$ for every Borel set $A\subset \mathbb{R}^2$ which has finite Lebesgue measure. I tried to use the definition of Lebesgue measure, namely $$ \int \chi_A \;\operatorname d\mu := \mu(A),   $$ but when I am trying to use this fact it is not occurring very helpful in my opinion. Is it a good idea to use Lebesgue's dominated convergence theorem? Then the integrable function $g$ would be equal to one if I am thinking correctly. Thanks in advance.","I need to show that for every Borel set which has finite Lebesgue measure. I tried to use the definition of Lebesgue measure, namely but when I am trying to use this fact it is not occurring very helpful in my opinion. Is it a good idea to use Lebesgue's dominated convergence theorem? Then the integrable function would be equal to one if I am thinking correctly. Thanks in advance.","\lim_{n\rightarrow \infty} \int\limits_A \cos(nxy) \, d\lambda_2=0 A\subset \mathbb{R}^2  \int \chi_A \;\operatorname d\mu := \mu(A),    g","['integration', 'limits', 'functions', 'lebesgue-integral', 'lebesgue-measure']"
21,Prove $\lim_{n \rightarrow \infty} f(x) f(2^2x) f(3^2x) \cdots f(n^2x) = 0$ for $f: \mathbb{R} \rightarrow \mathbb{R}$ in $L^1(\mathbb{R})$.,Prove  for  in .,\lim_{n \rightarrow \infty} f(x) f(2^2x) f(3^2x) \cdots f(n^2x) = 0 f: \mathbb{R} \rightarrow \mathbb{R} L^1(\mathbb{R}),"Here's another question that I'm stuck on from my studies for an upcoming exam.  This one comes from another practice preliminary exam. Problem Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be  a Lebesgue integrable function.  Prove that for almost every $x \in \mathbb{R}$ that $$ \lim_{n \rightarrow \infty} f(x) f(2^2x) f(3^2x) \cdots f(n^2x) = 0$$ I.e. given that $\int_\mathbb{R} |f(x)| dx < \infty$ , prove the above limit. Please, correct me if I'm wrong, confirm my thoughts, or provide hints towards a solution.  Thanks in advance! My Partial Attempt Break the support into three cases.  Either $S := \{ x \in \mathbb{R} | f(x) \neq 0\}$ (1) has $\mu(S) = 0$ , (2) finite $\mu(S) > 0$ , or (3) $\mu(S) =  \infty$ ,. (1) We have immediately that the integral $\int_\mathbb{R} |f(x)| dx = 0$ because the function is supported by a set of measure zero.  Hence, the function is zero almost everywhere and we have our result. (2) This is the tricky case for me.  This is where I'm looking for guidance if my case-by-case method is in fact a method for solution.  Otherwise, provide a hint or an alternative solution method. (3) Help here too.","Here's another question that I'm stuck on from my studies for an upcoming exam.  This one comes from another practice preliminary exam. Problem Let be  a Lebesgue integrable function.  Prove that for almost every that I.e. given that , prove the above limit. Please, correct me if I'm wrong, confirm my thoughts, or provide hints towards a solution.  Thanks in advance! My Partial Attempt Break the support into three cases.  Either (1) has , (2) finite , or (3) ,. (1) We have immediately that the integral because the function is supported by a set of measure zero.  Hence, the function is zero almost everywhere and we have our result. (2) This is the tricky case for me.  This is where I'm looking for guidance if my case-by-case method is in fact a method for solution.  Otherwise, provide a hint or an alternative solution method. (3) Help here too.",f: \mathbb{R} \rightarrow \mathbb{R} x \in \mathbb{R}  \lim_{n \rightarrow \infty} f(x) f(2^2x) f(3^2x) \cdots f(n^2x) = 0 \int_\mathbb{R} |f(x)| dx < \infty S := \{ x \in \mathbb{R} | f(x) \neq 0\} \mu(S) = 0 \mu(S) > 0 \mu(S) =  \infty \int_\mathbb{R} |f(x)| dx = 0,"['real-analysis', 'limits', 'measure-theory', 'lebesgue-integral']"
22,Can't find why the proof is false (substitution rule).,Can't find why the proof is false (substitution rule).,,"Statement : Suppose $f$ and $g$ are functions with domain $\mathbb{R}$ and we want to show that if $$\lim_{x \rightarrow a} f(x) = b\ \ \text{and}\ \lim_{x \rightarrow b} g(x) = c,\ \text{then}\ \lim_{x \rightarrow a} g(f(x)) = c.$$ I know that this statement is false $\bigg($ let $f(x)=0$ and $g(x)=\begin{cases}1&x= 0\\0&x\neq0\end{cases}$ , as $x\to 0$$\bigg)$ . But I don't understand where the following attempt to prove the statement fails. ""Proof"" :  Since $\lim_{x \rightarrow b} g(x) = c$ , $$\forall\epsilon>0,\ \exists\delta_1,\ 0<|x-b|<\delta_1\Rightarrow |g(x)-c|<\epsilon.\tag 1$$ Since $\lim_{x \rightarrow a} f(x) = b$ , we can choose $\delta_2$ such that $$0<|x-a|<\delta_2\Rightarrow |f(x)-b|<\delta_1.$$ Now, using implication $(1)$ , $$\forall\epsilon>0,\ \exists\delta_2,\ 0<|x-a|<\delta_2\Rightarrow |f(x)-b|<\delta_1 \Rightarrow |g(f(x))-c|<\epsilon,$$ which is $\lim_{x \rightarrow a} g(f(x)) = c$ . The only suspicious point that I see is that to use $(1)$ , it should be the case that $f(x)\neq b$ . But the counterexample relies on the discontinuity of $g$ .","Statement : Suppose and are functions with domain and we want to show that if I know that this statement is false let and , as . But I don't understand where the following attempt to prove the statement fails. ""Proof"" :  Since , Since , we can choose such that Now, using implication , which is . The only suspicious point that I see is that to use , it should be the case that . But the counterexample relies on the discontinuity of .","f g \mathbb{R} \lim_{x \rightarrow a} f(x) = b\ \ \text{and}\ \lim_{x \rightarrow b} g(x) = c,\ \text{then}\ \lim_{x \rightarrow a} g(f(x)) = c. \bigg( f(x)=0 g(x)=\begin{cases}1&x= 0\\0&x\neq0\end{cases} x\to 0\bigg) \lim_{x \rightarrow b} g(x) = c \forall\epsilon>0,\ \exists\delta_1,\ 0<|x-b|<\delta_1\Rightarrow |g(x)-c|<\epsilon.\tag 1 \lim_{x \rightarrow a} f(x) = b \delta_2 0<|x-a|<\delta_2\Rightarrow |f(x)-b|<\delta_1. (1) \forall\epsilon>0,\ \exists\delta_2,\ 0<|x-a|<\delta_2\Rightarrow |f(x)-b|<\delta_1 \Rightarrow |g(f(x))-c|<\epsilon, \lim_{x \rightarrow a} g(f(x)) = c (1) f(x)\neq b g","['real-analysis', 'limits', 'fake-proofs']"
23,$\epsilon$-$\delta$ definition of the limit applied to nonlinear functions (teaching),- definition of the limit applied to nonlinear functions (teaching),\epsilon \delta,"When I teach the $\epsilon$ - $\delta$ definition of the limit, I usually start with a linear function and a table of values to intuitive show the idea where the 'guesses' for $\delta$ in terms of $\epsilon$ is taken from. For example, $\displaystyle\lim_{x \to 3} (2x-4) = 2 $ , I use a table of values for $x = 3, 3.01, 3.1 ... $ with a corresponding $f(x) = 2, 2.02, 2.2, ...$ . Here we see that if the distance from $x$ is 0.01 (from 3 to 3.01), the corresponding 'distance' from $f(x)$ is 0.02. And so $\delta = \epsilon /2$ , which nicely fits in the proof. So my question is, when we move to quadratic functions, $\displaystyle\lim_{x \to 2} x^2 = 4.$ How do I use the same illustration to intuitively explain my choice for epsilon?","When I teach the - definition of the limit, I usually start with a linear function and a table of values to intuitive show the idea where the 'guesses' for in terms of is taken from. For example, , I use a table of values for with a corresponding . Here we see that if the distance from is 0.01 (from 3 to 3.01), the corresponding 'distance' from is 0.02. And so , which nicely fits in the proof. So my question is, when we move to quadratic functions, How do I use the same illustration to intuitively explain my choice for epsilon?","\epsilon \delta \delta \epsilon \displaystyle\lim_{x \to 3} (2x-4) = 2  x = 3, 3.01, 3.1 ...  f(x) = 2, 2.02, 2.2, ... x f(x) \delta = \epsilon /2 \displaystyle\lim_{x \to 2} x^2 = 4.","['limits', 'soft-question', 'education', 'epsilon-delta']"
24,Verifying tetration properties,Verifying tetration properties,,"In my previous question I asked about the numerical instability and convergence of my tetration. It would seem to be the case that it converges, but suffers from catastrophic cancellation. The definition of my tetration is provided as: $${}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)$$ where ${}^na$ for natural $n$ is defined as the usual tetration, with ${}^\infty a$ as the limit of that, and $\log^{\circ n}$ being the $n$ times applied logarithm. We consider the above for $a\in(1,e^{1/e})$ and $x\in(-2,\infty)$ . I want to prove that it satisfies the basic tetration properties: ${}^0a=1$ ${}^{x+1}a=a^{({}^xa)}$ It is easy enough to verify the first one, as we have: \begin{align}{}^0a&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^0)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^na)\\&=\lim_{n\to\infty}1\\&=1\end{align} I attempted to verify the second property: \begin{align}a^{({}^xa)}&=\lim_{n\to\infty}\log_a^{\circ(n-1)}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^{n+1}a)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\mathcal O(({}^\infty a-{}^na)^2[\ln({}^\infty a)]^x))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})\tag{$\star$}\\&={}^{x+1}a\end{align} How can I justify $(\star)$ though? Update: Rough outline of possible proof? For $t\ge1,~a>1$ , and sufficiently small $\epsilon>0$ , we have \begin{align}\log_a(t+\epsilon)&=\log_a(t)+\log_a(1+\epsilon/t)\\&\le\log_a(t)+\frac\epsilon{t\ln(a)}\end{align} In this case, $t\ge{}^xa$ by monotonicity of the logarithm and the limit. We start with $\epsilon=q^n$ where $q=\ln^2({}^\infty a)$ and apply the above $n$ times to get: $$\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\epsilon)\le\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})+\left(\frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}\right)^n$$ Seeing as we have $$\frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}=\frac{{}^\infty a}{{}^xa}\cdot\ln({}^\infty a)$$ and $\ln({}^\infty a)<1$ , this should work for sufficiently large $x$ .","In my previous question I asked about the numerical instability and convergence of my tetration. It would seem to be the case that it converges, but suffers from catastrophic cancellation. The definition of my tetration is provided as: where for natural is defined as the usual tetration, with as the limit of that, and being the times applied logarithm. We consider the above for and . I want to prove that it satisfies the basic tetration properties: It is easy enough to verify the first one, as we have: I attempted to verify the second property: How can I justify though? Update: Rough outline of possible proof? For , and sufficiently small , we have In this case, by monotonicity of the logarithm and the limit. We start with where and apply the above times to get: Seeing as we have and , this should work for sufficiently large .","{}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x) {}^na n {}^\infty a \log^{\circ n} n a\in(1,e^{1/e}) x\in(-2,\infty) {}^0a=1 {}^{x+1}a=a^{({}^xa)} \begin{align}{}^0a&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^0)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^na)\\&=\lim_{n\to\infty}1\\&=1\end{align} \begin{align}a^{({}^xa)}&=\lim_{n\to\infty}\log_a^{\circ(n-1)}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^{n+1}a)[\ln({}^\infty a)]^x)\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\mathcal O(({}^\infty a-{}^na)^2[\ln({}^\infty a)]^x))\\&=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})\tag{\star}\\&={}^{x+1}a\end{align} (\star) t\ge1,~a>1 \epsilon>0 \begin{align}\log_a(t+\epsilon)&=\log_a(t)+\log_a(1+\epsilon/t)\\&\le\log_a(t)+\frac\epsilon{t\ln(a)}\end{align} t\ge{}^xa \epsilon=q^n q=\ln^2({}^\infty a) n \log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1}+\epsilon)\le\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^{x+1})+\left(\frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}\right)^n \frac{\ln^2({}^\infty a)}{{}^xa\cdot\ln(a)}=\frac{{}^\infty a}{{}^xa}\cdot\ln({}^\infty a) \ln({}^\infty a)<1 x","['sequences-and-series', 'limits', 'tetration']"
25,Correct usage of Stolz-Cesaro theorem in finding a limit,Correct usage of Stolz-Cesaro theorem in finding a limit,,"Is this solution correct? I've seen this solution before, but I don't know why it is so. And which rule? The task is to find: $\omega=\displaystyle\lim_{n\to +\infty}\left(\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}\right)$ I know that this $\lim$ equals $\frac{1}{e},$ but do you see if this solution is correct or not? Solution is : $\displaystyle\lim_{n\to +\infty}\left(\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}\right)=\displaystyle\lim_{n\to +\infty}\left((n+1)\sqrt[n+1]{\frac{(n+1)!}{(n+1)^{n+1}}}-n\sqrt[n]{\frac{n!}{n^{n}}}\right)$ $=\displaystyle\lim_{n\to +\infty}\frac{\frac{(n+1)!}{(n+1)^{n+1}}}{\frac{n!}{n^{n}}}$ $=\frac{1}{e}$ Notes & suggestions of mine: I think he uses Stolz-Cesaro theorem: See that $\displaystyle\lim_{n\to +\infty}\frac{a_{n}}{b_{n}}=\displaystyle\lim_{n\to +\infty}\frac{a_{n+1}-a_{n}}{b_{n+1}-b_{n}}$ Chosen $a_{n}=n\sqrt[n]{\frac{n!}{n^n}}$ and $b_{n}=n$ clearly $b_{n}$ goes to $+\infty$ and $a_{n}$ goes to $+\infty$ because $n.\frac{1}{e}=+\infty$ we obtain: $\omega=\displaystyle\lim_{n\to +\infty}\frac{n\sqrt[n]{n!}}{n}$ Then use Cauchy-d'Alembert $=\displaystyle\lim_{n\to +\infty}\frac{\frac{(n+1)!}{(n+1)^{n+1}}}{\frac{n!}{n^{n}}}=\frac{1}{e}$ Is my explanation O.K.?  May I ask for a correction? Any remark ?","Is this solution correct? I've seen this solution before, but I don't know why it is so. And which rule? The task is to find: I know that this equals but do you see if this solution is correct or not? Solution is : Notes & suggestions of mine: I think he uses Stolz-Cesaro theorem: See that Chosen and clearly goes to and goes to because we obtain: Then use Cauchy-d'Alembert Is my explanation O.K.?  May I ask for a correction? Any remark ?","\omega=\displaystyle\lim_{n\to +\infty}\left(\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}\right) \lim \frac{1}{e}, \displaystyle\lim_{n\to +\infty}\left(\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}\right)=\displaystyle\lim_{n\to +\infty}\left((n+1)\sqrt[n+1]{\frac{(n+1)!}{(n+1)^{n+1}}}-n\sqrt[n]{\frac{n!}{n^{n}}}\right) =\displaystyle\lim_{n\to +\infty}\frac{\frac{(n+1)!}{(n+1)^{n+1}}}{\frac{n!}{n^{n}}} =\frac{1}{e} \displaystyle\lim_{n\to +\infty}\frac{a_{n}}{b_{n}}=\displaystyle\lim_{n\to +\infty}\frac{a_{n+1}-a_{n}}{b_{n+1}-b_{n}} a_{n}=n\sqrt[n]{\frac{n!}{n^n}} b_{n}=n b_{n} +\infty a_{n} +\infty n.\frac{1}{e}=+\infty \omega=\displaystyle\lim_{n\to +\infty}\frac{n\sqrt[n]{n!}}{n} =\displaystyle\lim_{n\to +\infty}\frac{\frac{(n+1)!}{(n+1)^{n+1}}}{\frac{n!}{n^{n}}}=\frac{1}{e}","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
26,Prove that limited increasing or decreasing successions have a limit.,Prove that limited increasing or decreasing successions have a limit.,,"Be $u_n$ an increasing sequence of numbers ${(u_1,u_2,u_3, \ldots)}$ such that for all $\ n$ , $u_{n-1}\le u_n$ Prove that if $u_n\le a, a \in \mathbb R$ , for all n then $\lim_{n\rightarrow\infty}u_n =b $ for some number $b\le a$ . I am using as definition of limit of a succession the following: $\lim_{n\rightarrow\infty}u_n =b \Leftrightarrow  ∀\delta \gt 0, ∃p \in \mathbb N :n\gt p \Rightarrow |u_n -b|\lt \delta$","Be an increasing sequence of numbers such that for all , Prove that if , for all n then for some number . I am using as definition of limit of a succession the following:","u_n {(u_1,u_2,u_3, \ldots)} \ n u_{n-1}\le u_n u_n\le a, a \in \mathbb R \lim_{n\rightarrow\infty}u_n =b  b\le a \lim_{n\rightarrow\infty}u_n =b \Leftrightarrow  ∀\delta \gt 0, ∃p \in \mathbb N :n\gt p \Rightarrow |u_n -b|\lt \delta","['sequences-and-series', 'limits', 'limits-without-lhopital']"
27,Prove that the product of two continuous functions is continuous using the sequential definition of continuity,Prove that the product of two continuous functions is continuous using the sequential definition of continuity,,"Using the following definition, prove that the product of two continuous functions is continuous. Let $f$ be a funtion from $\mathbb{R}^n$ to $\mathbb{R}$ . The function   f is continuous if for every point $p$ , for every sequence $x_n\to p$ , $f(x_n)\to f(p)$ Here is my attempt at the proof: Let $f,g:\mathbb{R}^n\to\mathbb{R}$ be continuous functions that are given. Therefore: For every point $p$ , for every sequence $a_n\to p$ , $f(b_n)\to f(p)$ . For every point $q$ , for every sequence $b_n\to p$ , $g(b_n)\to g(q)$ . Let $F:\mathbb{R}^n\to\mathbb{R}$ be a function defined by $f\cdot g$ . We wish to show that $F$ is continuous. That is, for every point $p$ , for every sequence $x_n\to p_o$ , one has $F(x_n)\to F(p_o)$ . Take the point $p_o$ to be the product of $p$ and $q$ , and the sequence $x_n$ to be the defined as $a_n b_n$ . Therefore $x_n \to p_o$ . The function $F(x_n) \to F(p_o)$ if given $\epsilon >0,$ there is some $N$ such that whenever $n>N$ , $|F(x_n) - F(p_o)| < \epsilon$ This is where I'm stuck. I think I'm on the right track with taking the product of the two sequences but it seems like I might need to take the proof by contradiction and start with $$|F(x_n) - F(p_o)| \geq \epsilon$$ Is this the correct approach to be taking? Thanks!","Using the following definition, prove that the product of two continuous functions is continuous. Let be a funtion from to . The function   f is continuous if for every point , for every sequence , Here is my attempt at the proof: Let be continuous functions that are given. Therefore: For every point , for every sequence , . For every point , for every sequence , . Let be a function defined by . We wish to show that is continuous. That is, for every point , for every sequence , one has . Take the point to be the product of and , and the sequence to be the defined as . Therefore . The function if given there is some such that whenever , This is where I'm stuck. I think I'm on the right track with taking the product of the two sequences but it seems like I might need to take the proof by contradiction and start with Is this the correct approach to be taking? Thanks!","f \mathbb{R}^n \mathbb{R} p x_n\to p f(x_n)\to f(p) f,g:\mathbb{R}^n\to\mathbb{R} p a_n\to p f(b_n)\to f(p) q b_n\to p g(b_n)\to g(q) F:\mathbb{R}^n\to\mathbb{R} f\cdot g F p x_n\to p_o F(x_n)\to F(p_o) p_o p q x_n a_n b_n x_n \to p_o F(x_n) \to F(p_o) \epsilon >0, N n>N |F(x_n) - F(p_o)| < \epsilon |F(x_n) - F(p_o)| \geq \epsilon","['limits', 'functions', 'continuity']"
28,$f$ is continuous at point $a$ iff $\lim_{h\to0}f(a+h)-f(a)=0$,is continuous at point  iff,f a \lim_{h\to0}f(a+h)-f(a)=0,"$$\lim_{h\to0}f(a+h)-f(a)=0\leftrightarrow \lim_{x\to a}f(x)=f(a)$$ Proof. $\Rightarrow:$ Assume $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon$$ Show $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|x-a|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ Let $h:=x-a$ that by assumption we have $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon$$ $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ $\Leftarrow:$ Assume $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon$$ Show $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|h|<\delta\rightarrow |f(a+h)-f(a)|<\varepsilon$$ Let $x-a:=h$ by assumption we have $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon$$ $$\forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon\tag*{$\square$}$$ I saw some notes state that $\lim_{h\to0}f(a+h)-f(a)=0$ implies continuous, but i don't know this alternative definition before, so i'm tring to prove this. the proof seems like just plug in some variables to the limit-definition Is this correct, thanks for your help.","Proof. Assume Show Let that by assumption we have Assume Show Let by assumption we have I saw some notes state that implies continuous, but i don't know this alternative definition before, so i'm tring to prove this. the proof seems like just plug in some variables to the limit-definition Is this correct, thanks for your help.","\lim_{h\to0}f(a+h)-f(a)=0\leftrightarrow \lim_{x\to a}f(x)=f(a) \Rightarrow: \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon \forall \varepsilon>0,\exists\delta>0,s.t.0<|x-a|<\delta\rightarrow |f(x)-f(a)|<\varepsilon h:=x-a \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon \Leftarrow: \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(x)-f(a)|<\varepsilon \forall \varepsilon>0,\exists\delta>0,s.t.0<|h|<\delta\rightarrow |f(a+h)-f(a)|<\varepsilon x-a:=h \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{x-a}|<\delta\rightarrow |f(a+\color{blue}{x-a})-f(a)|<\varepsilon \forall \varepsilon>0,\exists\delta>0,s.t.0<|\color{blue}{h}|<\delta\rightarrow |f(a+\color{blue}{h})-f(a)|<\varepsilon\tag*{\square} \lim_{h\to0}f(a+h)-f(a)=0","['real-analysis', 'limits', 'proof-verification', 'continuity', 'definition']"
29,How can I find this limit applying squeeze theorem?,How can I find this limit applying squeeze theorem?,,How can I evaluate this limit applying squeeze theorem? $$\lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6}$$ I found this limit with standart way: $\lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6}=\lim_{x\to6}\frac{\left(1+\sqrt{ 3-\sqrt{x-2}} \right)\times \left( 1-\sqrt{ 3-\sqrt{x-2}}\right)}{(x-6)\times \left( 1+\sqrt{ 3-\sqrt{x-2}}\right)}=\frac 18$ I want to write this limit using squeeze theorem.,How can I evaluate this limit applying squeeze theorem? I found this limit with standart way: I want to write this limit using squeeze theorem.,\lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6} \lim_{x\to6}\frac{1-\sqrt{ 3-\sqrt{x-2}}}{x-6}=\lim_{x\to6}\frac{\left(1+\sqrt{ 3-\sqrt{x-2}} \right)\times \left( 1-\sqrt{ 3-\sqrt{x-2}}\right)}{(x-6)\times \left( 1+\sqrt{ 3-\sqrt{x-2}}\right)}=\frac 18,['calculus']
30,"Limit by polar coordinates $\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0$",Limit by polar coordinates,"\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0","I need to demonstrate the following limit $$\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0$$ By polar coordinates. I apply the substitution $x=1+\rho\cos(\theta)$ , $y=\rho\sin(\theta)$ . $$\lim_{\rho\to0} \frac{\rho^2\sin^2(\theta)\log(1+\rho\cos(\theta))}{\rho^2}=\lim_{\rho\to0} \sin^2(\theta)\log(1+\rho\cos(\theta))$$ Here I have some issues with the following steps. $$|\sin^2(\theta)\log(1+\rho\cos(\theta))|\leq\rho|\sin^2(\theta)\cos(\theta)|=\rho\to0$$ I'm not sure about if that step is correct $$|\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta)$$ My thought process was that I observed that $$0\leq|\log(x)|\leq x-1$$ for $x\ge1$ . Thus, by setting $t=x-1 \to x=t+1$ , I conclude that $$0\leq|\log(t+1)|\leq t$$ For $t\ge0$ . Hence since $\cos(\theta)$ is a bounded value I conclude that $$|\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta)$$ For $\rho\ge0$ . And with this inequality I conclude that the limit is $0$ as I've show above. Is the demonstration correct?","I need to demonstrate the following limit By polar coordinates. I apply the substitution , . Here I have some issues with the following steps. I'm not sure about if that step is correct My thought process was that I observed that for . Thus, by setting , I conclude that For . Hence since is a bounded value I conclude that For . And with this inequality I conclude that the limit is as I've show above. Is the demonstration correct?","\lim_{(x,y)\to(1,0)} \frac{y^2\log(x)}{(x-1)^2+y^2}=0 x=1+\rho\cos(\theta) y=\rho\sin(\theta) \lim_{\rho\to0} \frac{\rho^2\sin^2(\theta)\log(1+\rho\cos(\theta))}{\rho^2}=\lim_{\rho\to0} \sin^2(\theta)\log(1+\rho\cos(\theta)) |\sin^2(\theta)\log(1+\rho\cos(\theta))|\leq\rho|\sin^2(\theta)\cos(\theta)|=\rho\to0 |\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta) 0\leq|\log(x)|\leq x-1 x\ge1 t=x-1 \to x=t+1 0\leq|\log(t+1)|\leq t t\ge0 \cos(\theta) |\log(1+\rho\cos(\theta))|\leq\rho\cos(\theta) \rho\ge0 0","['calculus', 'limits', 'multivariable-calculus', 'proof-verification']"
31,How to prove the limit exist?,How to prove the limit exist?,,"Let: $\displaystyle f=\int_V \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3}\ dV'$ where $V'$ is a finite volume in space $\mathbf{r}=(x,y,z)$ are coordinates of all space $\mathbf{r'}=(x',y',z')$ are coordinates of $V'$ $|\mathbf{r}-\mathbf{r'}|=[(x-x')^2+(y-y')^2+(z-z')^2]^{1/2}$ How to prove that: $\lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}$ exist $\text{ }$ MY TRY: I am not sure whether this method would work. If it doesn't please suggest another method to reach my goal. \begin{align} &\lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}\\  =&\lim\limits_{\Delta x \to 0}\dfrac{\displaystyle\int_{V'} \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3}\ dV' - \int_{V'} \dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}\ dV'}{\Delta x}\\  =&\lim\limits_{\Delta x \to 0}\displaystyle\int_{V'}                     \dfrac{\left(  \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3}  -\dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}   \right)}{\Delta x}dV' \end{align} Now if only I could take the limit inside the integral (with respect to $V′$ ),I can proceed to show the limit exists. If we can't do that and this method doesn't work, please suggest another method to show that the limit exists.","Let: where is a finite volume in space are coordinates of all space are coordinates of How to prove that: exist MY TRY: I am not sure whether this method would work. If it doesn't please suggest another method to reach my goal. Now if only I could take the limit inside the integral (with respect to ),I can proceed to show the limit exists. If we can't do that and this method doesn't work, please suggest another method to show that the limit exists.","\displaystyle f=\int_V \dfrac{x-x'}{|\mathbf{r}-\mathbf{r'}|^3}\ dV' V' \mathbf{r}=(x,y,z) \mathbf{r'}=(x',y',z') V' |\mathbf{r}-\mathbf{r'}|=[(x-x')^2+(y-y')^2+(z-z')^2]^{1/2} \lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x} \text{ } \begin{align}
&\lim\limits_{\Delta x \to 0} \dfrac{f(x+\Delta x,y,z)-f(x,y,z)}{\Delta x}\\ 
=&\lim\limits_{\Delta x \to 0}\dfrac{\displaystyle\int_{V'} \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3}\ dV' - \int_{V'} \dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}\ dV'}{\Delta x}\\ 
=&\lim\limits_{\Delta x \to 0}\displaystyle\int_{V'}                    
\dfrac{\left(  \dfrac{(x+\Delta x)-x'}{|\mathbf{r}(x+\Delta x,y,z)-\mathbf{r'}|^3} 
-\dfrac{x-x'}{|\mathbf{r}(x,y,z)-\mathbf{r'}|^3}   \right)}{\Delta x}dV'
\end{align} V′","['limits', 'functions', 'derivatives', 'definite-integrals', 'partial-derivative']"
32,Limit superior and inferior when one part diverges,Limit superior and inferior when one part diverges,,"How can I find the limit superior and inferior of given sequence: $x_n = (1 + \frac{1}{2n})\cos{\frac{n\pi }{3}}$ as $ n \in \mathbb N $ I did the following: since $\lim_{n\to\infty}(1 + \frac{1}{2n}) = 1$ and the second term' limit oscillates between -1 and 1, I decided that the supremum should be $1$ and infimum should be $-1$ . But this is an incorrect answer. What I do wrong? Thank you. UPD: It is a task from the online-courses site and there is an automatic answer checking system. So, as I said in my question, answer $-1$ and $1$ not passed and there is no explanation why. I believe there should be used Bolzano–Weierstrass theorem to find all possible convergent subsequences, and then find limits for each of them...","How can I find the limit superior and inferior of given sequence: as I did the following: since and the second term' limit oscillates between -1 and 1, I decided that the supremum should be and infimum should be . But this is an incorrect answer. What I do wrong? Thank you. UPD: It is a task from the online-courses site and there is an automatic answer checking system. So, as I said in my question, answer and not passed and there is no explanation why. I believe there should be used Bolzano–Weierstrass theorem to find all possible convergent subsequences, and then find limits for each of them...",x_n = (1 + \frac{1}{2n})\cos{\frac{n\pi }{3}}  n \in \mathbb N  \lim_{n\to\infty}(1 + \frac{1}{2n}) = 1 1 -1 -1 1,"['calculus', 'limits', 'limsup-and-liminf']"
33,Why does evaluation of a two-variable limit fail when using polar coordinates?,Why does evaluation of a two-variable limit fail when using polar coordinates?,,"The definition of the limit of a two-variable function: $\lim\limits_{(x,y)\to (a,b)}f(x,y)=L\,$ if and only if for all $\epsilon>0$ there exists a $\delta >0$ such that $$0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Consider the following proposition (I do realize that it is not true): Let $f^*(r,\theta) := f(a+r\cos\theta,b+r\sin\theta)$ . Then $$\lim\limits_{(x,y)\to(a,b)} f(x,y) = L \iff \lim\limits_{r\to0^+} f^*(r,\theta) = L$$ Proof. Suppose that $\lim\limits_{(x,y)\to(a,b)} f(x,y) = L$ . This means that for all $\epsilon>0$ there exists $\delta>0$ such that $$0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ If we let $x=a+r\cos\theta$ , $y=b+r\sin\theta$ , then we get that $$0<\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=r<\delta \implies |f^*(r,\theta)-L|<\epsilon$$ Thus, by definition, $\lim\limits_{r\to0^+}f^*(r,\theta) = L$ . Now suppose that $\lim\limits_{r\to0^+}f^*(r,\theta) = L$ . This means that for all $\epsilon>0$ there exists $\delta>0$ such that $$0<r<\delta \implies |f^*(r,\theta)-L|<\epsilon$$ Again, letting $x=a+r\cos\theta$ , $y=b+r\sin\theta$ , we get $$0<r=\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Again, by definition, $\lim\limits_{(x,y)\to(a,b)} f(x,y) = L$ . Q.E.D. I am aware that the above proof can be done by directly proving the equivalence, but I didn't want to risk making it less clear that way. The problem The proposition is incorrect, or so I am inclined to believe. Consider the following function: $$f(x,y) = \frac{x^2y}{x^4+y^2}$$ and the following limit: $$\lim\limits_{(x,y) \to (0,0)} f(x,y)$$ The function is taken from, and my question heavily relies on, this post. Changing to polar coordinates, and after some arrangements, the limit of $f(x,y)$ as $(x,y) \to (0,0)$ is $$\lim_{r \to 0^+} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$ Some answers to that question say that you have to be careful with that limit. My understanding is that this refers to ""substituting $r$ with 0"". I know that this is the wrong approach. But, I get that this limit is always 0, regardless of $\theta$ . And I didn't find it very difficult to arrive at this conclusion. I break it down into two cases. First, I assume that $\sin\theta\neq0$ . The numerator tends to 0 and the denominator tends to $\sin^2\theta$ . Therefore, the limit is just $0/\sin^2\theta$ , i.e. zero. The second case is $\sin\theta = 0$ . But, now the under-limit function is identically zero for all $r\neq0$ , yielding a limit that is zero. This proves that the limit is zero . Have I done something wrong? One answer of the above mentioned post says that, when looking for this limit, you have to analyze the case where $\theta$ is a function of $r$ . Why? With the limit being zero regardless of $\theta$ , my proposition would imply that the limit of $f(x,y)$ is 0. Yet, I know that this is not the case , because if I let $y=x^2$ , the ""limit"" evaluates to $\frac{1}{2}$ . I have always been told that for a limit to exist, it needs to be the same for every path you approach the limit point on. I've always taken that for granted, and it made intuitive sense to me. But now, thinking more deeply about it, I don't really know why that is. This also has to do with the fact that this comes up nowhere in the proof of my proposition. My thoughts My proof relies on (or so I think) the fact that every point $(x,y) \in \mathbb R$ is representable in polar form as the pair $(r,\theta)$ and that this representation is unique if we restrict $\theta \in [0, 2\pi)$ . Is this correct? As far as I can see $r$ and $\theta$ can be independent variables. I cannot figure out why one would need to allow for $\theta$ to be a function of $r$ . I also rely on the fact that $\sqrt{(x-a)^2+(x-b)^2}$ and $r$ are always equal. Am I missing something? I will sum up my questions: How does the definition of the limit imply that all paths of approach yield the same result? I am looking for an intuitive explanation. What is wrong with my proposition/proof? How does the fact that it fails for the function $f(x,y)$ and the path $(x,x^2)$ , relate to the proof of my proposition. In other words, can you pinpoint exactly where the proof fails? Where does the notion to let $\theta = \theta(r)$ (or even $r=r(\theta)$ ?) come from? I suppose that this is closely related to question 1. When can/should I use polar coordinates to prove that a limit exists ? Thank you for your patience.","The definition of the limit of a two-variable function: if and only if for all there exists a such that Consider the following proposition (I do realize that it is not true): Let . Then Proof. Suppose that . This means that for all there exists such that If we let , , then we get that Thus, by definition, . Now suppose that . This means that for all there exists such that Again, letting , , we get Again, by definition, . Q.E.D. I am aware that the above proof can be done by directly proving the equivalence, but I didn't want to risk making it less clear that way. The problem The proposition is incorrect, or so I am inclined to believe. Consider the following function: and the following limit: The function is taken from, and my question heavily relies on, this post. Changing to polar coordinates, and after some arrangements, the limit of as is Some answers to that question say that you have to be careful with that limit. My understanding is that this refers to ""substituting with 0"". I know that this is the wrong approach. But, I get that this limit is always 0, regardless of . And I didn't find it very difficult to arrive at this conclusion. I break it down into two cases. First, I assume that . The numerator tends to 0 and the denominator tends to . Therefore, the limit is just , i.e. zero. The second case is . But, now the under-limit function is identically zero for all , yielding a limit that is zero. This proves that the limit is zero . Have I done something wrong? One answer of the above mentioned post says that, when looking for this limit, you have to analyze the case where is a function of . Why? With the limit being zero regardless of , my proposition would imply that the limit of is 0. Yet, I know that this is not the case , because if I let , the ""limit"" evaluates to . I have always been told that for a limit to exist, it needs to be the same for every path you approach the limit point on. I've always taken that for granted, and it made intuitive sense to me. But now, thinking more deeply about it, I don't really know why that is. This also has to do with the fact that this comes up nowhere in the proof of my proposition. My thoughts My proof relies on (or so I think) the fact that every point is representable in polar form as the pair and that this representation is unique if we restrict . Is this correct? As far as I can see and can be independent variables. I cannot figure out why one would need to allow for to be a function of . I also rely on the fact that and are always equal. Am I missing something? I will sum up my questions: How does the definition of the limit imply that all paths of approach yield the same result? I am looking for an intuitive explanation. What is wrong with my proposition/proof? How does the fact that it fails for the function and the path , relate to the proof of my proposition. In other words, can you pinpoint exactly where the proof fails? Where does the notion to let (or even ?) come from? I suppose that this is closely related to question 1. When can/should I use polar coordinates to prove that a limit exists ? Thank you for your patience.","\lim\limits_{(x,y)\to (a,b)}f(x,y)=L\, \epsilon>0 \delta >0 0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon f^*(r,\theta) := f(a+r\cos\theta,b+r\sin\theta) \lim\limits_{(x,y)\to(a,b)} f(x,y) = L \iff \lim\limits_{r\to0^+} f^*(r,\theta) = L \lim\limits_{(x,y)\to(a,b)} f(x,y) = L \epsilon>0 \delta>0 0<\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon x=a+r\cos\theta y=b+r\sin\theta 0<\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=r<\delta \implies |f^*(r,\theta)-L|<\epsilon \lim\limits_{r\to0^+}f^*(r,\theta) = L \lim\limits_{r\to0^+}f^*(r,\theta) = L \epsilon>0 \delta>0 0<r<\delta \implies |f^*(r,\theta)-L|<\epsilon x=a+r\cos\theta y=b+r\sin\theta 0<r=\sqrt{r^2\cos^2\theta+r^2\sin^2\theta}=\sqrt{(x-a)^2+(y-b)^2}<\delta \implies |f(x,y)-L|<\epsilon \lim\limits_{(x,y)\to(a,b)} f(x,y) = L f(x,y) = \frac{x^2y}{x^4+y^2} \lim\limits_{(x,y) \to (0,0)} f(x,y) f(x,y) (x,y) \to (0,0) \lim_{r \to 0^+} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta} r \theta \sin\theta\neq0 \sin^2\theta 0/\sin^2\theta \sin\theta = 0 r\neq0 \theta r \theta f(x,y) y=x^2 \frac{1}{2} (x,y) \in \mathbb R (r,\theta) \theta \in [0, 2\pi) r \theta \theta r \sqrt{(x-a)^2+(x-b)^2} r f(x,y) (x,x^2) \theta = \theta(r) r=r(\theta)","['real-analysis', 'limits', 'multivariable-calculus', 'polar-coordinates']"
34,limit of function: $ \lim_{x \to \infty} \frac{[x]}{x}$,limit of function:, \lim_{x \to \infty} \frac{[x]}{x},"This is my homework: Limit of functions: ( $[x]$ is the total part of $x$ ) $\displaystyle \lim_{x \to \infty} \frac{[x]}{x}$ $\displaystyle \lim_{x \to -\infty} \frac{[x]}{x}$ $\displaystyle \lim_{x \to 0} \frac{[x]}{x}$ For the first one, I used the inequality $$x -1 \leq [x] \leq x.$$ So, $$\frac{1-\frac{1}{x}}{1}=\frac{x-1}{x} \leq \frac{[x]}{x} \leq \frac{x}{x}=1.$$ So my answer is $\displaystyle \lim _{x \to \infty} \frac{[x]}{x} = 1$ . Is that good? I don't know what to use in other examples $\ldots$","This is my homework: Limit of functions: ( is the total part of ) For the first one, I used the inequality So, So my answer is . Is that good? I don't know what to use in other examples",[x] x \displaystyle \lim_{x \to \infty} \frac{[x]}{x} \displaystyle \lim_{x \to -\infty} \frac{[x]}{x} \displaystyle \lim_{x \to 0} \frac{[x]}{x} x -1 \leq [x] \leq x. \frac{1-\frac{1}{x}}{1}=\frac{x-1}{x} \leq \frac{[x]}{x} \leq \frac{x}{x}=1. \displaystyle \lim _{x \to \infty} \frac{[x]}{x} = 1 \ldots,"['calculus', 'limits', 'ceiling-and-floor-functions']"
35,Does it make sense to talk about limit in this case?,Does it make sense to talk about limit in this case?,,"Suppose I have a function that is defined only for values bigger than $a$ , does it make sense to talk about limit of a function at that point, or only about limit from the right? It seems to me that we can talk about limit of a function, because if we look at the definition $$\forall \epsilon>0 \; \exists \delta>0 \; \forall x\in A: \; 0<|x-a|<\delta \Rightarrow |f(x)-L|<\epsilon$$ ( $A$ is domain of the function), we have the requirement of $x$ being in the domain and so limit still would make sense. But I am not sure. Thanks in advance.","Suppose I have a function that is defined only for values bigger than , does it make sense to talk about limit of a function at that point, or only about limit from the right? It seems to me that we can talk about limit of a function, because if we look at the definition ( is domain of the function), we have the requirement of being in the domain and so limit still would make sense. But I am not sure. Thanks in advance.",a \forall \epsilon>0 \; \exists \delta>0 \; \forall x\in A: \; 0<|x-a|<\delta \Rightarrow |f(x)-L|<\epsilon A x,"['real-analysis', 'limits']"
36,"Proving that $\lim_{n\to\infty}(\int_{a}^{b}f(x)^ndx)^{1/n} = \max_{x\in [a,b]}f(x)$ [duplicate]",Proving that  [duplicate],"\lim_{n\to\infty}(\int_{a}^{b}f(x)^ndx)^{1/n} = \max_{x\in [a,b]}f(x)","This question already has answers here : Finding $\lim\limits_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n}$ for continuous $f:[0,1]\to[0,\infty)$ [duplicate] (2 answers) Closed 5 years ago . I'm trying to prove the following statement: $$\lim_{n\to\infty}\left(\int_{a}^{b}f(x)^ndx\right)^{1/n} = \max_{x\in [a,b]}f(x)$$ where $[a,b] \subset \mathbb{R} $ and $f$ is non-negative and continuous. I've tried to prove it in a similar way that we prove that $\displaystyle\lim_{n\to\infty}(a^n+b^n)^{1/n} = b$ if $b>a$ . However, I'm stuck in the end with an iterated limit of the form $\displaystyle\lim_{n\to\infty} \lim_{\epsilon\to 0 } \left(\epsilon^{1/n}\max_{x\in [a,b]}f(x)\right)$ . Is this last expression equal to $\displaystyle\max_{x\in [a,b]}f(x)$ ? If not, could anyone please give me a hint as to how to go about this proof?","This question already has answers here : Finding $\lim\limits_{n \rightarrow \infty}\left(\int_0^1(f(x))^n\,\mathrm dx\right)^\frac{1}{n}$ for continuous $f:[0,1]\to[0,\infty)$ [duplicate] (2 answers) Closed 5 years ago . I'm trying to prove the following statement: where and is non-negative and continuous. I've tried to prove it in a similar way that we prove that if . However, I'm stuck in the end with an iterated limit of the form . Is this last expression equal to ? If not, could anyone please give me a hint as to how to go about this proof?","\lim_{n\to\infty}\left(\int_{a}^{b}f(x)^ndx\right)^{1/n} = \max_{x\in [a,b]}f(x) [a,b] \subset \mathbb{R}  f \displaystyle\lim_{n\to\infty}(a^n+b^n)^{1/n} = b b>a \displaystyle\lim_{n\to\infty} \lim_{\epsilon\to 0 } \left(\epsilon^{1/n}\max_{x\in [a,b]}f(x)\right) \displaystyle\max_{x\in [a,b]}f(x)","['real-analysis', 'calculus', 'integration', 'limits']"
37,Find $\lim_{n\to \infty}((n+1)!)^{\frac{1}{n+1}}-((n)!)^{\frac{1}{n}}.$ [duplicate],Find  [duplicate],\lim_{n\to \infty}((n+1)!)^{\frac{1}{n+1}}-((n)!)^{\frac{1}{n}}.,"This question already has an answer here : Limit of the sequence $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ (1 answer) Closed 5 years ago . Find $\lim_{n\to  \infty}((n+1)!)^{\frac{1}{n+1}}-((n)!)^{\frac{1}{n}}.$ We need to deal the limit $\lim_{n\to \infty} \frac{\log(1)+\log(2)+...+\log(n)}{n}$ . We know that $\lim_{n\to \infty} \log(n)=\infty \implies \lim_{n\to \infty} \frac{\log(1)+\log(2)+...+\log(n)}{n}=\infty$ (since, By Cauchy's first theorem on limit). Hence we get $\infty-\infty$ . How do I show that there exists finite limit?","This question already has an answer here : Limit of the sequence $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ (1 answer) Closed 5 years ago . Find We need to deal the limit . We know that (since, By Cauchy's first theorem on limit). Hence we get . How do I show that there exists finite limit?","\lim_{n\to
 \infty}((n+1)!)^{\frac{1}{n+1}}-((n)!)^{\frac{1}{n}}. \lim_{n\to \infty} \frac{\log(1)+\log(2)+...+\log(n)}{n} \lim_{n\to \infty} \log(n)=\infty \implies \lim_{n\to \infty} \frac{\log(1)+\log(2)+...+\log(n)}{n}=\infty \infty-\infty",['real-analysis']
38,Limits at Infinity and limit equality,Limits at Infinity and limit equality,,"I'm given function $f:(a,\infty)\to\mathbb{R}$ which has a limit at infinity, i.e., $\lim_{x \to \infty}f(x)$ exists, call it $L$ . And I want to show that given a function $g(x) := {f(1/x)},$ which is defined on $(0,1/a),$ that this function $g(x)$ has a limit at 0 if and only if the limit of $f$ as $x$ tends to infinity exists. I know I have to use the $\epsilon - \delta$ defintion, but before that I think the following is an equivalent formulation: \begin{gather} \lim_{x \to \infty}f(x) = \lim_{x \to 0}f(1/x). \end{gather} I know this is just an exercise in chasing the $\epsilon - \delta$ notation, but I think the ""trick"" here is to use the fact that if $f$ has a limit at infinity, then for all $\epsilon > 0,$ there exists $M > a$ such that for all $x \geq M$ we have that $|f(x) - L| < \epsilon$ . So I think the idea here is to pick my $\delta$ as $1/M$ since we have that \begin{gather} x \geq M \implies 1/x \leq 1/M \end{gather} and we know that if $x \geq M$ then $|f(x) - L| < \epsilon.$ So if we suppose $\epsilon_0 > 0$ and that $|f(1/x) - L| < \epsilon_0$ will $\delta_0 = 1/M$ suffice? My intuition says yes, but I am not sure how to formulate this rigorously.","I'm given function which has a limit at infinity, i.e., exists, call it . And I want to show that given a function which is defined on that this function has a limit at 0 if and only if the limit of as tends to infinity exists. I know I have to use the defintion, but before that I think the following is an equivalent formulation: I know this is just an exercise in chasing the notation, but I think the ""trick"" here is to use the fact that if has a limit at infinity, then for all there exists such that for all we have that . So I think the idea here is to pick my as since we have that and we know that if then So if we suppose and that will suffice? My intuition says yes, but I am not sure how to formulate this rigorously.","f:(a,\infty)\to\mathbb{R} \lim_{x \to \infty}f(x) L g(x) := {f(1/x)}, (0,1/a), g(x) f x \epsilon - \delta \begin{gather}
\lim_{x \to \infty}f(x) = \lim_{x \to 0}f(1/x).
\end{gather} \epsilon - \delta f \epsilon > 0, M > a x \geq M |f(x) - L| < \epsilon \delta 1/M \begin{gather}
x \geq M \implies 1/x \leq 1/M
\end{gather} x \geq M |f(x) - L| < \epsilon. \epsilon_0 > 0 |f(1/x) - L| < \epsilon_0 \delta_0 = 1/M","['real-analysis', 'limits', 'analysis', 'epsilon-delta']"
39,A Cauchy sequence $\{x_n\}$ with infinitely many $n$ such that $x_n = c$.,A Cauchy sequence  with infinitely many  such that .,\{x_n\} n x_n = c,"Is the following argument correct? Proposition . If $\{x_n\}$ is Cauchy sequence such that $x_n = c$ for infinitely many $n$ , then $\lim_{n\to\infty}x_n = c$ . Proof. Let $\epsilon>0$ . Since $\{x_n\}$ is a Cauchy sequence, there exists an $M\in\mathbb{N}$ such that $\forall \, j,k\ge M$ , we have $|x_j-x_k|<\epsilon$ . Now, since $x_n = c$ for infinitely many $c$ , then surely $x_r = c$ for some $r \ge M,$ implying that $|x_j-c|<\epsilon \,\,\forall j\ge M$ , completing the argument. $\blacksquare$","Is the following argument correct? Proposition . If is Cauchy sequence such that for infinitely many , then . Proof. Let . Since is a Cauchy sequence, there exists an such that , we have . Now, since for infinitely many , then surely for some implying that , completing the argument.","\{x_n\} x_n = c n \lim_{n\to\infty}x_n = c \epsilon>0 \{x_n\} M\in\mathbb{N} \forall \, j,k\ge M |x_j-x_k|<\epsilon x_n = c c x_r = c r \ge M, |x_j-c|<\epsilon \,\,\forall j\ge M \blacksquare","['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'cauchy-sequences']"
40,"Show that the limit does not exist $\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}$ [duplicate]",Show that the limit does not exist  [duplicate],"\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}","This question already has an answer here : Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist (1 answer) Closed 3 years ago . Show that the limit does not exist $\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}$ attempt: let $y = 0$ $\lim_{x \to 0} \frac{5x^2}{x^2 + 0^2} = 5$ let $x = 0$ $\lim_{y \to 0} \frac{5(0)^2}{y^2} = 0$ $5 \neq 0$ , therefore two different values, limit does not exist right?","This question already has an answer here : Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist (1 answer) Closed 3 years ago . Show that the limit does not exist attempt: let let , therefore two different values, limit does not exist right?","\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2} y = 0 \lim_{x \to 0} \frac{5x^2}{x^2 + 0^2} = 5 x = 0 \lim_{y \to 0} \frac{5(0)^2}{y^2} = 0 5 \neq 0","['limits', 'multivariable-calculus', 'proof-verification']"
41,Compute $\prod_{n=1}^{\infty} (1+x^{2n})$ for $|x|\lt 1$,Compute  for,\prod_{n=1}^{\infty} (1+x^{2n}) |x|\lt 1,Compute for $|x|\lt 1$:$$\prod_{n=1}^{\infty} (1+x^{2n})$$ I'm having trouble computing this product. My work ends up contradicting and saying $x$ has to equal $1$. Anyone know how to do this?,Compute for $|x|\lt 1$:$$\prod_{n=1}^{\infty} (1+x^{2n})$$ I'm having trouble computing this product. My work ends up contradicting and saying $x$ has to equal $1$. Anyone know how to do this?,,"['calculus', 'analysis', 'limits', 'products']"
42,I want to verify my proofs.,I want to verify my proofs.,,"1).Prove: $\lim_{n \to \infty} \frac{3n^2-4}{n^2-4} = 3$.  A proof: there exists an $\epsilon>0$. We define $N=\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil$ (notice $N>2$). So:  $$ \begin{aligned} |\frac{3n^2-4}{n^2-4}-3| < \epsilon \Leftrightarrow |\frac{3n^2-4-3n^2+12}{n^2-4}|<\epsilon \Leftrightarrow |\frac{8}{n^2-4}| < \epsilon   \Leftrightarrow  \frac{8}{n^2-4}<\epsilon \Leftrightarrow\\ 8<n^2\epsilon-4\epsilon \Leftrightarrow \frac{8+4\epsilon}{\epsilon}<n^2 \Leftrightarrow  \sqrt{\frac{8}{\epsilon}+4}<n.  \end{aligned} $$  This statement is true for all $n>N$, because: $$\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil \geq \sqrt{\frac{8}{\epsilon}+4}.$$  thus, the limit is 3. $\Box$ 2).$a_nb_n=1$. Prove or contradict: If $\lim_{n\to\infty}|a_n|=1$ so $\lim_{n\to\infty}|b_n|=1$. This statement is true. A proof: We choose $\epsilon = \frac{1}{2}$, so: $$ -\epsilon < a_nb_n-1 < \epsilon\Rightarrow -\epsilon+1<a_nb_n<\epsilon+1 \Rightarrow -\frac{1}{2}+1<a_nb_n<\frac{1}{2}+1  \Rightarrow \\ \frac{1}{2}<a_nb_n<1\frac{1}{2}  $$ Also: $$ ||a_n|-1|<\epsilon \Rightarrow \frac{1}{2} <|a_n| <1\frac{1}{2}  $$ It's obvious that $a_n\neq 0$ for almost all n, otherwise $\lim_{n\to\infty}a_n=0$. If $a_n>0$ for almost all n, also $b_n>0$ for almost all n. we can divide and get: $$ 1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{a_n}<1 \Rightarrow 1<b_n<1 $$ If $a_n<0$ for almost all n, also $b_n<0$ for almost all n. we can divide and get: $$ 1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{-a_n}<1 \Rightarrow 1<-b_n<1 $$ And from the sandwich theorem we get $\lim_{n\to\infty}|b_n| = 1$. $\Box$","1).Prove: $\lim_{n \to \infty} \frac{3n^2-4}{n^2-4} = 3$.  A proof: there exists an $\epsilon>0$. We define $N=\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil$ (notice $N>2$). So:  $$ \begin{aligned} |\frac{3n^2-4}{n^2-4}-3| < \epsilon \Leftrightarrow |\frac{3n^2-4-3n^2+12}{n^2-4}|<\epsilon \Leftrightarrow |\frac{8}{n^2-4}| < \epsilon   \Leftrightarrow  \frac{8}{n^2-4}<\epsilon \Leftrightarrow\\ 8<n^2\epsilon-4\epsilon \Leftrightarrow \frac{8+4\epsilon}{\epsilon}<n^2 \Leftrightarrow  \sqrt{\frac{8}{\epsilon}+4}<n.  \end{aligned} $$  This statement is true for all $n>N$, because: $$\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil \geq \sqrt{\frac{8}{\epsilon}+4}.$$  thus, the limit is 3. $\Box$ 2).$a_nb_n=1$. Prove or contradict: If $\lim_{n\to\infty}|a_n|=1$ so $\lim_{n\to\infty}|b_n|=1$. This statement is true. A proof: We choose $\epsilon = \frac{1}{2}$, so: $$ -\epsilon < a_nb_n-1 < \epsilon\Rightarrow -\epsilon+1<a_nb_n<\epsilon+1 \Rightarrow -\frac{1}{2}+1<a_nb_n<\frac{1}{2}+1  \Rightarrow \\ \frac{1}{2}<a_nb_n<1\frac{1}{2}  $$ Also: $$ ||a_n|-1|<\epsilon \Rightarrow \frac{1}{2} <|a_n| <1\frac{1}{2}  $$ It's obvious that $a_n\neq 0$ for almost all n, otherwise $\lim_{n\to\infty}a_n=0$. If $a_n>0$ for almost all n, also $b_n>0$ for almost all n. we can divide and get: $$ 1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{a_n}<1 \Rightarrow 1<b_n<1 $$ If $a_n<0$ for almost all n, also $b_n<0$ for almost all n. we can divide and get: $$ 1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{-a_n}<1 \Rightarrow 1<-b_n<1 $$ And from the sandwich theorem we get $\lim_{n\to\infty}|b_n| = 1$. $\Box$",,"['calculus', 'sequences-and-series', 'limits', 'proof-verification']"
43,"Prob. 14, Sec. 3.4, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: The supremum of a bounded sequence not in the range","Prob. 14, Sec. 3.4, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: The supremum of a bounded sequence not in the range",,"Here is Prob. 14, Sec. 3.4, in the book Introduction to Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let $\left( x_n \right)$ be a bounded sequence and let $s \colon= \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. Show that if $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$, then there is a subsequence of $\left( x_n \right)$ that converges to $s$. My Attempt: As $s-1 < s$, so there exists a natural number $n_1$ such that    $$ s-1 < x_{n_1} \leq s. $$   But $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_1} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have    $$ s-1 < x_{n_1} < s. \tag{1} $$ As $s-1/2 < s$, so there exists a natural number $n_2$ such that    $$ s - \frac{1}{2} < x_{n_2} \leq s.  $$   But again $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_2} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have    $$ s- \frac{1}{2} < x_{n_2} < s. \tag{2} $$ If there were no $n_2 > n_1$ for which  (2) can hold, then we would obtain    $$ x_n \leq s- \frac{1}{2} \ \mbox{ for every natural number } n > n_1. $$   So, for every natural number $n$, we would obtain $$ x_n \leq \max \left\{ \ s- \frac{1}{2}, x_1, \ldots, x_{n_1} \right\} < s. $$   And this would then imply that    $$ \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\} < s. $$ So there does exist a natural number $n_2 > n_1$ for which (2) can hold. Now suppose that $n_1, n_2, \ldots, n_k$ have been found such that $n_1 < n_2 < \cdots < n_k$ and such that    $$ s - \frac{1}{j} < x_{n_j} < s \ \mbox{ for all } j = 1, \ldots, k. $$   Then just as we proceeded from $n_1$ to $n_2$, we can find a natural number $n_{k+1} > n_k$ such that    $$ s - \frac{1}{k+1} < x_{n_{k+1} } < s. $$ In this way we obtain a subsequence $\left( x_{n_k} \right)$ of $\left( x_n \right)$ that satisfies    $$ s - \frac{1}{k} < x_{n_k} < s $$   for all $k \in \mathbb{N}$ and hence converges to $s$. Is this proof correct in each and every one of its steps? If not, then where is it incorrect? Is the presentation clear enough too?","Here is Prob. 14, Sec. 3.4, in the book Introduction to Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition: Let $\left( x_n \right)$ be a bounded sequence and let $s \colon= \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. Show that if $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$, then there is a subsequence of $\left( x_n \right)$ that converges to $s$. My Attempt: As $s-1 < s$, so there exists a natural number $n_1$ such that    $$ s-1 < x_{n_1} \leq s. $$   But $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_1} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have    $$ s-1 < x_{n_1} < s. \tag{1} $$ As $s-1/2 < s$, so there exists a natural number $n_2$ such that    $$ s - \frac{1}{2} < x_{n_2} \leq s.  $$   But again $s \not\in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$ and $x_{n_2} \in \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\}$. So we must have    $$ s- \frac{1}{2} < x_{n_2} < s. \tag{2} $$ If there were no $n_2 > n_1$ for which  (2) can hold, then we would obtain    $$ x_n \leq s- \frac{1}{2} \ \mbox{ for every natural number } n > n_1. $$   So, for every natural number $n$, we would obtain $$ x_n \leq \max \left\{ \ s- \frac{1}{2}, x_1, \ldots, x_{n_1} \right\} < s. $$   And this would then imply that    $$ \sup \left\{ \ x_n \ \colon \ n \in \mathbb{N} \ \right\} < s. $$ So there does exist a natural number $n_2 > n_1$ for which (2) can hold. Now suppose that $n_1, n_2, \ldots, n_k$ have been found such that $n_1 < n_2 < \cdots < n_k$ and such that    $$ s - \frac{1}{j} < x_{n_j} < s \ \mbox{ for all } j = 1, \ldots, k. $$   Then just as we proceeded from $n_1$ to $n_2$, we can find a natural number $n_{k+1} > n_k$ such that    $$ s - \frac{1}{k+1} < x_{n_{k+1} } < s. $$ In this way we obtain a subsequence $\left( x_{n_k} \right)$ of $\left( x_n \right)$ that satisfies    $$ s - \frac{1}{k} < x_{n_k} < s $$   for all $k \in \mathbb{N}$ and hence converges to $s$. Is this proof correct in each and every one of its steps? If not, then where is it incorrect? Is the presentation clear enough too?",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'convergence-divergence']"
44,Looking for Guidance: Behavior of $x!^{\frac{1}{x}}$...,Looking for Guidance: Behavior of ...,x!^{\frac{1}{x}},"On my own time, I've been trying to learn as much as I can about the upper levels of mathematics.  I recently came across the Gamma function:  $$\Gamma(n) = (n-1)! = \int_{0}^{\infty}(t^{x-1}e^{-x})dt = \int_{0}^{1}(-\ln(t))^{x-1}dt$$ Therefore, obviously $x! = \int_{0}^{1}(-\ln(t))^{x-1}dt$ (this can also be verified by graphing both functions).  This reminded me of something I thought about a long time ago: $f(x)=(x!)^{\frac{1}{x}}$.  Now that I understand more about mathematics, I (like many others both in general and on stackoverflow) was able to prove that $(x!)^{\frac{1}{x}}$ diverges to $\infty$.  However, when calculating $\lim_{x\to0}(x!)^{\frac{1}{x}}$, my intial guess that $\lim_{x\to0}(x!)^{\frac{1}{x}}=\gamma=0.57721...$ was proven wrong.  I found that $\lim_{x\to0}(x!)^{\frac{1}{x}}\approx0.5615...$.  This leads me to my first question.  Does this number have any significance?  Might it have any importance other than being the arbitrary number that's the answer to this question? [ANSWERED BY R_Berger] Moving on to my second question.  When I graphed this to verify my solution, I was surprised to see that the graph was practically a straight line (I expected more than a negligible curve of some type). Taking the derivative of this function would obviously be shown as $$\frac{d}{dx}\left(\int_0^1\left(-\ln\left(t\right)\right)^{x}dt\right)^{\frac{1}{x}}.$$  I was unable to find the above derivative either by hand or by using a calculator.  So, is there a way to take the above derivative or any other variant of $x!$ (or $\Gamma(x-1)$)? [NO POINTERS GIVEN YET] Lastly, most importantly, and my primary reason for asking this question... My ultimate goal is finding $\lim_{x\to\infty}\frac{d}{dx}\left(\int_0^1\left(-\ln\left(t\right)\right)^{x}dt\right)^{\frac{1}{x}}.$  If you have any pointers or small hints on anything I could look into, that would very much be appreciated. [NO POINTERS GIVEN YET] If you take the time to read and answer this, I thank you very much in advance.","On my own time, I've been trying to learn as much as I can about the upper levels of mathematics.  I recently came across the Gamma function:  $$\Gamma(n) = (n-1)! = \int_{0}^{\infty}(t^{x-1}e^{-x})dt = \int_{0}^{1}(-\ln(t))^{x-1}dt$$ Therefore, obviously $x! = \int_{0}^{1}(-\ln(t))^{x-1}dt$ (this can also be verified by graphing both functions).  This reminded me of something I thought about a long time ago: $f(x)=(x!)^{\frac{1}{x}}$.  Now that I understand more about mathematics, I (like many others both in general and on stackoverflow) was able to prove that $(x!)^{\frac{1}{x}}$ diverges to $\infty$.  However, when calculating $\lim_{x\to0}(x!)^{\frac{1}{x}}$, my intial guess that $\lim_{x\to0}(x!)^{\frac{1}{x}}=\gamma=0.57721...$ was proven wrong.  I found that $\lim_{x\to0}(x!)^{\frac{1}{x}}\approx0.5615...$.  This leads me to my first question.  Does this number have any significance?  Might it have any importance other than being the arbitrary number that's the answer to this question? [ANSWERED BY R_Berger] Moving on to my second question.  When I graphed this to verify my solution, I was surprised to see that the graph was practically a straight line (I expected more than a negligible curve of some type). Taking the derivative of this function would obviously be shown as $$\frac{d}{dx}\left(\int_0^1\left(-\ln\left(t\right)\right)^{x}dt\right)^{\frac{1}{x}}.$$  I was unable to find the above derivative either by hand or by using a calculator.  So, is there a way to take the above derivative or any other variant of $x!$ (or $\Gamma(x-1)$)? [NO POINTERS GIVEN YET] Lastly, most importantly, and my primary reason for asking this question... My ultimate goal is finding $\lim_{x\to\infty}\frac{d}{dx}\left(\int_0^1\left(-\ln\left(t\right)\right)^{x}dt\right)^{\frac{1}{x}}.$  If you have any pointers or small hints on anything I could look into, that would very much be appreciated. [NO POINTERS GIVEN YET] If you take the time to read and answer this, I thank you very much in advance.",,"['calculus', 'limits', 'derivatives', 'gamma-function']"
45,How to prove this specific function has limit either $\infty$ or 0?,How to prove this specific function has limit either  or 0?,\infty,"If $f:\mathbb R^+ \to \mathbb R^+$, and for all $y>0$, the following equation holds (i.e. it is slowly varying fuction.) $$\lim_{x\to +\infty} \frac{f(xy)}{f(x)}=1$$ How to prove the following? $$\lim_{x\to +\infty} x^\rho f(x) = \begin{cases}0 & \rho<0\\\infty & \rho>0\end{cases}$$ Notice that $f$ is not necessarily continuous. This problem seems simple but I can't solve it.","If $f:\mathbb R^+ \to \mathbb R^+$, and for all $y>0$, the following equation holds (i.e. it is slowly varying fuction.) $$\lim_{x\to +\infty} \frac{f(xy)}{f(x)}=1$$ How to prove the following? $$\lim_{x\to +\infty} x^\rho f(x) = \begin{cases}0 & \rho<0\\\infty & \rho>0\end{cases}$$ Notice that $f$ is not necessarily continuous. This problem seems simple but I can't solve it.",,"['calculus', 'real-analysis', 'limits']"
46,Alternate proof for Viète's infinite product of nested radicals [duplicate],Alternate proof for Viète's infinite product of nested radicals [duplicate],,This question already has answers here : Calculate $\sqrt{\frac{1}{2}} \times \sqrt{\frac{1}{2} + \frac{1}{2}\sqrt{\frac{1}{2}}} \times \ldots $ (2 answers) Closed 2 years ago . I am looking for alternate proof for Viete's infinite product of nested radicals. ( Reference - Wikipedia ) Basically we need to find $\lim_{n\to \infty}\prod_{k=1}^{n} T_k$ where $$T_{k+1} = \sqrt{\left(\frac{T_k + 1}{2}\right)}$$ and $T_1 = \sqrt{\frac{1}{2}}$. Series looks like $$\sqrt{\frac{1}{2}} \cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}} \cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}}}...$$ Miss gave a solution treating $\cos(\theta) = \frac{1}{\sqrt2}$ that is $\theta = 45^\circ$. The series result is given easily using the identity $\cos(\theta) + 1 = 2 \cos^2(\theta/2)$ and using $\sin(2\theta) = 2\sin(\theta)\cos(\theta)$. The final result is $\frac{\sin(2\theta)}{2\theta} = \frac{2}{\pi}$. I look for alternate ways to get to this! I am open to calculus methods.,This question already has answers here : Calculate $\sqrt{\frac{1}{2}} \times \sqrt{\frac{1}{2} + \frac{1}{2}\sqrt{\frac{1}{2}}} \times \ldots $ (2 answers) Closed 2 years ago . I am looking for alternate proof for Viete's infinite product of nested radicals. ( Reference - Wikipedia ) Basically we need to find $\lim_{n\to \infty}\prod_{k=1}^{n} T_k$ where $$T_{k+1} = \sqrt{\left(\frac{T_k + 1}{2}\right)}$$ and $T_1 = \sqrt{\frac{1}{2}}$. Series looks like $$\sqrt{\frac{1}{2}} \cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}} \cdot \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{\frac{1}{2}}}}...$$ Miss gave a solution treating $\cos(\theta) = \frac{1}{\sqrt2}$ that is $\theta = 45^\circ$. The series result is given easily using the identity $\cos(\theta) + 1 = 2 \cos^2(\theta/2)$ and using $\sin(2\theta) = 2\sin(\theta)\cos(\theta)$. The final result is $\frac{\sin(2\theta)}{2\theta} = \frac{2}{\pi}$. I look for alternate ways to get to this! I am open to calculus methods.,,"['calculus', 'sequences-and-series', 'limits', 'infinite-product', 'nested-radicals']"
47,"Find a sequence in $[0,\ln 4]$",Find a sequence in,"[0,\ln 4]","Find an $a_1, a_2, \dots$ sequence in the interval $[0,\ln 4]$, such that for any $x<y$ positive integers $$|a_x-a_y|\geq \dfrac{1}{y}$$ I know the well known $$a_n=\dfrac{(-1)^{n+1}}{n}$$ sequence, and I tried making new sequences from that, but none of them worked.","Find an $a_1, a_2, \dots$ sequence in the interval $[0,\ln 4]$, such that for any $x<y$ positive integers $$|a_x-a_y|\geq \dfrac{1}{y}$$ I know the well known $$a_n=\dfrac{(-1)^{n+1}}{n}$$ sequence, and I tried making new sequences from that, but none of them worked.",,"['integration', 'limits']"
48,What's the limit of a modified Bessel function of the first kind as the order approaches infinity?,What's the limit of a modified Bessel function of the first kind as the order approaches infinity?,,"I'm trying to find an estimate of the cumulative distribution function of the Von Mises distribution. The CDF is $D(x) = \frac{1}{2\pi I_0(b)}\left(xI_0(b)+2\sum_{j=1}^\inf \frac{I_j(b)\sin\left[j(x-a)\right]}{j}\right)$, where $I_j$ is the modified Bessel function of the first kind of order $j$. This substantially exceeds my calculus skills. Assistance would be appreciated. (My goal is actually to estimate the portion of the distribution between two points $x_1$ and $x_2$, if that makes it any easier.) (Note: I'm aware of the questions asked regarding the limit of the Bessel function $I_j(x)$ as $x$ approaches infinity. My question concerns the limit when the order, $j$, approaches infinity.)","I'm trying to find an estimate of the cumulative distribution function of the Von Mises distribution. The CDF is $D(x) = \frac{1}{2\pi I_0(b)}\left(xI_0(b)+2\sum_{j=1}^\inf \frac{I_j(b)\sin\left[j(x-a)\right]}{j}\right)$, where $I_j$ is the modified Bessel function of the first kind of order $j$. This substantially exceeds my calculus skills. Assistance would be appreciated. (My goal is actually to estimate the portion of the distribution between two points $x_1$ and $x_2$, if that makes it any easier.) (Note: I'm aware of the questions asked regarding the limit of the Bessel function $I_j(x)$ as $x$ approaches infinity. My question concerns the limit when the order, $j$, approaches infinity.)",,"['integration', 'limits', 'statistics', 'bessel-functions']"
49,"Which of the following sequences $(f_n)$ converge uniformly on [0,1]?","Which of the following sequences  converge uniformly on [0,1]?",(f_n),"as the title states, which $f_n$ converge uniformly on $[0,1]$ (i) $f_n(x)=\frac{x}{1+nx}~~~~~$     (ii) $f_n(x)=\frac{nx}{e^{nx^2}}~~~~$     (iii)$f_n(x)=n^{\frac{1}{2}}x(1-x)^{n}$ there are another 3 more but these are the ones ive done so far and i want to make sure my work is correct. definition of pointwise convergence is $$\forall \epsilon > 0, \wedge x\in[0,1] ~ \exists N\in\mathbb{N}. \text{s.t for } n>N \Longrightarrow |f_n(x)-f(x)|\leq \epsilon$$ then uniform convergence is $$\forall \epsilon > 0,\exists N\in \mathbb{N} \text{ s.t } \forall n>N \Longrightarrow \sup_{x\in[0,1]}|f_n(x)-f(x)| \leq \epsilon $$ so for each of the above i just need to make sure that they first converge pointwise. then see if they converge uniformly. (now it's been a while since i last did this so i may be rusty which is why i'm looking for confirmation that i'm doing it right) first things first, for (i): $$\lim_{n \longrightarrow \infty}f_n(x)=\lim_{n \longrightarrow \infty}\frac{x}{1+nx}=\lim_{n \longrightarrow \infty}\frac{n}{n}\frac{x/n}{1/n+x}=0 \text{ as }n  \rightarrow \infty $$ so the function converges pointwise to the zero function (yes?) so if this is uniformly convergent the limit of the sup of this function should also converge to the zero function so $$\sup_{x\in [0,1]}|f_n(x)-f|=\sup_{x\in [0,1]}|f_n(x)|=\sup_{x\in [0,1]}|\frac{x}{1+nx}|=\frac{1}{1+n}=0 \text{ as } n\rightarrow \infty$$ so it does converge uniformly. (yes?) i use similiar reasoning on (ii) to argue that also converges uniformly and on (iii) i find $$\lim_{n \longrightarrow \infty}f_n(x)=\lim_{n \longrightarrow \infty}n^{\frac{1}{2}}x(1-x)^{n}$$ for x = 0 or 1 we have $f_n(x)=0~\forall n$ which i believes suggests $$0 \leq f_n(x) \leq 0 $$ which suggests $$f_n(x)=0$$ and alternative arguement which i'm more confident about is the idea that $(1-x)^{n}\rightarrow 0$ faster than $x\sqrt{n} \rightarrow \infty$ which would give me the same limit of $0$ am i on the right tracks?","as the title states, which $f_n$ converge uniformly on $[0,1]$ (i) $f_n(x)=\frac{x}{1+nx}~~~~~$     (ii) $f_n(x)=\frac{nx}{e^{nx^2}}~~~~$     (iii)$f_n(x)=n^{\frac{1}{2}}x(1-x)^{n}$ there are another 3 more but these are the ones ive done so far and i want to make sure my work is correct. definition of pointwise convergence is $$\forall \epsilon > 0, \wedge x\in[0,1] ~ \exists N\in\mathbb{N}. \text{s.t for } n>N \Longrightarrow |f_n(x)-f(x)|\leq \epsilon$$ then uniform convergence is $$\forall \epsilon > 0,\exists N\in \mathbb{N} \text{ s.t } \forall n>N \Longrightarrow \sup_{x\in[0,1]}|f_n(x)-f(x)| \leq \epsilon $$ so for each of the above i just need to make sure that they first converge pointwise. then see if they converge uniformly. (now it's been a while since i last did this so i may be rusty which is why i'm looking for confirmation that i'm doing it right) first things first, for (i): $$\lim_{n \longrightarrow \infty}f_n(x)=\lim_{n \longrightarrow \infty}\frac{x}{1+nx}=\lim_{n \longrightarrow \infty}\frac{n}{n}\frac{x/n}{1/n+x}=0 \text{ as }n  \rightarrow \infty $$ so the function converges pointwise to the zero function (yes?) so if this is uniformly convergent the limit of the sup of this function should also converge to the zero function so $$\sup_{x\in [0,1]}|f_n(x)-f|=\sup_{x\in [0,1]}|f_n(x)|=\sup_{x\in [0,1]}|\frac{x}{1+nx}|=\frac{1}{1+n}=0 \text{ as } n\rightarrow \infty$$ so it does converge uniformly. (yes?) i use similiar reasoning on (ii) to argue that also converges uniformly and on (iii) i find $$\lim_{n \longrightarrow \infty}f_n(x)=\lim_{n \longrightarrow \infty}n^{\frac{1}{2}}x(1-x)^{n}$$ for x = 0 or 1 we have $f_n(x)=0~\forall n$ which i believes suggests $$0 \leq f_n(x) \leq 0 $$ which suggests $$f_n(x)=0$$ and alternative arguement which i'm more confident about is the idea that $(1-x)^{n}\rightarrow 0$ faster than $x\sqrt{n} \rightarrow \infty$ which would give me the same limit of $0$ am i on the right tracks?",,"['real-analysis', 'limits', 'convergence-divergence', 'uniform-convergence']"
50,Prove that $\lim\limits_{n\to\infty}\left(\sum_{k=0}^{n}\lambda_k\right)\left(\sum_{k=0}^{n}\frac{\lambda_k }{a_k}\right)^{-1}= \lim_{n\to\infty} a_n$,Prove that,\lim\limits_{n\to\infty}\left(\sum_{k=0}^{n}\lambda_k\right)\left(\sum_{k=0}^{n}\frac{\lambda_k }{a_k}\right)^{-1}= \lim_{n\to\infty} a_n,"Suppose that, $a_n\to \ell\neq 0$ is a converging sequence of non vanishing` complex numbers and $\{\lambda_n\}$ is a sequence of positifs real numbers such that $\sum\limits_{k=0}^{\infty}\lambda_k = \infty$ Then, show that,  $$\lim_{n\to\infty}\left(\sum_\limits{k=0}^{n}\lambda_k\right)\left(\sum_\limits{k=0}^{n}\frac{\lambda_k }{a_k}\right)^{-1}= \ell =\lim_{n\to\infty} a_n$$ I have no clue on how to start.","Suppose that, $a_n\to \ell\neq 0$ is a converging sequence of non vanishing` complex numbers and $\{\lambda_n\}$ is a sequence of positifs real numbers such that $\sum\limits_{k=0}^{\infty}\lambda_k = \infty$ Then, show that,  $$\lim_{n\to\infty}\left(\sum_\limits{k=0}^{n}\lambda_k\right)\left(\sum_\limits{k=0}^{n}\frac{\lambda_k }{a_k}\right)^{-1}= \ell =\lim_{n\to\infty} a_n$$ I have no clue on how to start.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
51,Finding the limiting value in the form of recurrence,Finding the limiting value in the form of recurrence,,"Now, this is actually a weird one, because I usually wouldn't suspect one of these to pop up on facebook; but is there anything to this problem, is it solvable, or is it just pure jitter, and if not, how is it solved? $$\mathrm{For\space every\space integer\space} n \geq 0,$$ $$ I_n=\int_0^{\huge\frac{\pi}{2}}\cos^{2n}x\space\mathrm{d}x\space;\space J_n=\int^{\huge\frac{\pi}{2}}_0 x^2\cos^{2n}x\space \mathrm{d}x$$ $$\mathrm{Find\space the\space limit \space below:}$$ $$ \lim_{n \to + \infty} 2 \sum_{k=1}^n\left(\frac{J_{k-1}}{I_{k-1}}-\frac{J_k}{I_k}\right)$$","Now, this is actually a weird one, because I usually wouldn't suspect one of these to pop up on facebook; but is there anything to this problem, is it solvable, or is it just pure jitter, and if not, how is it solved?","\mathrm{For\space every\space integer\space} n \geq 0,  I_n=\int_0^{\huge\frac{\pi}{2}}\cos^{2n}x\space\mathrm{d}x\space;\space J_n=\int^{\huge\frac{\pi}{2}}_0 x^2\cos^{2n}x\space \mathrm{d}x \mathrm{Find\space the\space limit \space below:}  \lim_{n \to + \infty} 2 \sum_{k=1}^n\left(\frac{J_{k-1}}{I_{k-1}}-\frac{J_k}{I_k}\right)","['calculus', 'real-analysis', 'integration', 'analysis', 'limits']"
52,"If the limit along all continuous paths is $0$ for $f(x,y)$, must the limit actually be $0$?","If the limit along all continuous paths is  for , must the limit actually be ?","0 f(x,y) 0","Say we have $f:\Bbb R^2 \to \Bbb R$ i.e. $z=f(x,y)$. We know that the limit $(x,y)\to(0,0)$ exists and is equal to $0$ for all continuous paths then can we conclude that the double limit is actually $0$ ? Why or why not? I feel that the answer should be yes as even for functions discontinuous at origin, the limit should exist. I can't prove it rigorously though. Any suggestions?","Say we have $f:\Bbb R^2 \to \Bbb R$ i.e. $z=f(x,y)$. We know that the limit $(x,y)\to(0,0)$ exists and is equal to $0$ for all continuous paths then can we conclude that the double limit is actually $0$ ? Why or why not? I feel that the answer should be yes as even for functions discontinuous at origin, the limit should exist. I can't prove it rigorously though. Any suggestions?",,['limits']
53,Value of $\int_{- \infty}^0 \frac{\mathrm dx}{(49+x) \sqrt{x}}$?,Value of ?,\int_{- \infty}^0 \frac{\mathrm dx}{(49+x) \sqrt{x}},"I've been trying to find this integral:$$\int_{- \infty}^0 \frac{\mathrm dx}{(49+x) \sqrt{x}}$$ I used a trigonometric substitution, and after some work arrived at this: $$\arctan\left(\frac{\sqrt{x}}{7}\right)$$ So if written properly, it would look like: $$\lim_{k \to -\infty}  \arctan\left.\left(\frac{\sqrt{x}}{7}\right) \right|^{\,0}_{\,k}$$ Evaluating zero wasn't an issue, however taking the limit was. How can k approach negative infinity if it'll appear in a square root? Wolfram Alpha told me this integral did not converge. Another online calculator told me it was zero. The person who gave this problem said it converged. Any ideas? Thank you for your input!","I've been trying to find this integral:$$\int_{- \infty}^0 \frac{\mathrm dx}{(49+x) \sqrt{x}}$$ I used a trigonometric substitution, and after some work arrived at this: $$\arctan\left(\frac{\sqrt{x}}{7}\right)$$ So if written properly, it would look like: $$\lim_{k \to -\infty}  \arctan\left.\left(\frac{\sqrt{x}}{7}\right) \right|^{\,0}_{\,k}$$ Evaluating zero wasn't an issue, however taking the limit was. How can k approach negative infinity if it'll appear in a square root? Wolfram Alpha told me this integral did not converge. Another online calculator told me it was zero. The person who gave this problem said it converged. Any ideas? Thank you for your input!",,"['calculus', 'limits', 'definite-integrals']"
54,Limit of a $p$-adic function and L'Hôpital's rule.,Limit of a -adic function and L'Hôpital's rule.,p,"Let $k$ be a fixed positive integer and $\operatorname{val}_p$ be the $p$-adic valuation on $\mathbb{Z}_p$. Let $n$ be a natural number. My question is finding the following limit: $$\lim_{n \rightarrow \infty}\frac{n^k}{p^{\operatorname{val}_p(n!)}}.$$ I guess that this limit should be zero; because I guess that $\operatorname{val}_p(n!)$ is like $\dfrac{n}{p-1}$ for large $n$ (I know for sure that $\operatorname{val}_p(n!)$ is certainly less than $\dfrac{n}{p-1}$) and I apply the classical L.Hopital's rule in calculus (although its not relevant to the $p$-adic case) to the limit $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}$ and by L.Hopital's rule, taking repeated derivatives, I get that $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}=0$ which gives me the feeling that $\lim_{n \rightarrow \infty} \dfrac{n^k}{p^{\operatorname{val}_p(n!)}}=0$. Thanks in advance for help and for explanations","Let $k$ be a fixed positive integer and $\operatorname{val}_p$ be the $p$-adic valuation on $\mathbb{Z}_p$. Let $n$ be a natural number. My question is finding the following limit: $$\lim_{n \rightarrow \infty}\frac{n^k}{p^{\operatorname{val}_p(n!)}}.$$ I guess that this limit should be zero; because I guess that $\operatorname{val}_p(n!)$ is like $\dfrac{n}{p-1}$ for large $n$ (I know for sure that $\operatorname{val}_p(n!)$ is certainly less than $\dfrac{n}{p-1}$) and I apply the classical L.Hopital's rule in calculus (although its not relevant to the $p$-adic case) to the limit $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}$ and by L.Hopital's rule, taking repeated derivatives, I get that $\lim_{x \rightarrow \infty} \dfrac{x^k}{p^{x/(p-1)}}=0$ which gives me the feeling that $\lim_{n \rightarrow \infty} \dfrac{n^k}{p^{\operatorname{val}_p(n!)}}=0$. Thanks in advance for help and for explanations",,"['abstract-algebra', 'number-theory', 'limits', 'representation-theory', 'p-adic-number-theory']"
55,Finding the derivative at a particular point,Finding the derivative at a particular point,,"So I've got this question and I'm not 100% if I've actually answered it correctly, would be appreciated if you can check it and tell me whether its correct if not can you tell me where I went wrong and where I can improve thanks! :) Question Find the derivative of the function $$f(x)=\begin{cases} x^2\sin(\frac{1}{x}), &x \neq 0 \\ 0, & x=0 \end{cases}$$ at $x=0$ Working $\lim_{h\to 0}\frac{f(h)-f(0)}{h}=\lim_{h\to 0} h^2\sin(\frac{1}{h})$ *Apply squeeze theorem. $-1\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq1$ $\therefore \lim_{h\to 0}h^2(\frac{1}{-1})\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq\lim_{h\to 0}h^2(\frac{1}{1})$ $0\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq 0$ $\implies \lim_{h\to 0}h^2\sin(\frac{1}{h})=0$","So I've got this question and I'm not 100% if I've actually answered it correctly, would be appreciated if you can check it and tell me whether its correct if not can you tell me where I went wrong and where I can improve thanks! :) Question Find the derivative of the function $$f(x)=\begin{cases} x^2\sin(\frac{1}{x}), &x \neq 0 \\ 0, & x=0 \end{cases}$$ at $x=0$ Working $\lim_{h\to 0}\frac{f(h)-f(0)}{h}=\lim_{h\to 0} h^2\sin(\frac{1}{h})$ *Apply squeeze theorem. $-1\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq1$ $\therefore \lim_{h\to 0}h^2(\frac{1}{-1})\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq\lim_{h\to 0}h^2(\frac{1}{1})$ $0\leq\lim_{h\to 0}h^2\sin(\frac{1}{h})\leq 0$ $\implies \lim_{h\to 0}h^2\sin(\frac{1}{h})=0$",,"['calculus', 'limits', 'derivatives', 'proof-verification']"
56,$\displaystyle \lim_{x \to 0+} \frac{d^k}{dx^k} \sqrt[x]{x}$ for all $k \in \mathbb{N}$ and $x>0$,for all  and,\displaystyle \lim_{x \to 0+} \frac{d^k}{dx^k} \sqrt[x]{x} k \in \mathbb{N} x>0,"I managed to show with de L'Hospital that $$\lim_{x \to 0+}  \sqrt[x]{x} = 0.$$ I calculated the first two derivatives and realised that it is getting more and more complicated due to the product rule and powers of logarithms to show that the limit  $$\lim_{x \to 0+} \frac{d^k}{dx^k} \sqrt[x]{x} = 0$$ which some plots that I made are indicating. I also thought about using  $$\lim_{x \to 0+}  \exp\left(-{\frac{1}{x}}\right)^{\ln(x)}$$ because inductively the limits of $\exp\left(- \frac{1}{x} \right)$ are a lot easier to calculate, though I didn't come up with a general formula and don't know how to use it properly with the given problem above. A result that might be helpful, that I could proof, was that for all $\alpha >0$ it follows that $$ \lim_{x \to 0+}  x^{\alpha} \, \ln(x) = 0.$$ Could you offer me any hints how to get to the desired result by induction? Is there an easy way to exchange the limit with the derivative? Ideas so far: Let's restrict ourself to the compact intervall $[0,1]$. $f$ being a realvalued continous and bounded function defined on $(0,1]$ is uniform continous iff $f$ is can be continously extended on $[0,1]$. As this is valid for $k=0$, we exchange the first derivative with the limit and get that $\lim_{x \to 0+} (x^{\frac{1}{x}}) ' = 0$. As $(x^{\frac{1}{x}}) '$ is bounded, we can repeat the argument inductively. The first two derivates are: $$(x^{\frac{1}{x}}) ' = \exp \left( \tfrac{\ln(x)}{x} \right)\,\left( \frac{1-\ln(x)}{x^2} \right) = \sqrt[x]{x}\,\left( \frac{1-\ln(x)}{x^2} \right)$$ $$ (x^{\frac{1}{x}}) ''  =  \sqrt[x]{x} \, \left( \ln^2(x) - 3\,x + 2\, (x-1)\, \ln(x) + 1  \right) \, \frac{1}{x^4} $$","I managed to show with de L'Hospital that $$\lim_{x \to 0+}  \sqrt[x]{x} = 0.$$ I calculated the first two derivatives and realised that it is getting more and more complicated due to the product rule and powers of logarithms to show that the limit  $$\lim_{x \to 0+} \frac{d^k}{dx^k} \sqrt[x]{x} = 0$$ which some plots that I made are indicating. I also thought about using  $$\lim_{x \to 0+}  \exp\left(-{\frac{1}{x}}\right)^{\ln(x)}$$ because inductively the limits of $\exp\left(- \frac{1}{x} \right)$ are a lot easier to calculate, though I didn't come up with a general formula and don't know how to use it properly with the given problem above. A result that might be helpful, that I could proof, was that for all $\alpha >0$ it follows that $$ \lim_{x \to 0+}  x^{\alpha} \, \ln(x) = 0.$$ Could you offer me any hints how to get to the desired result by induction? Is there an easy way to exchange the limit with the derivative? Ideas so far: Let's restrict ourself to the compact intervall $[0,1]$. $f$ being a realvalued continous and bounded function defined on $(0,1]$ is uniform continous iff $f$ is can be continously extended on $[0,1]$. As this is valid for $k=0$, we exchange the first derivative with the limit and get that $\lim_{x \to 0+} (x^{\frac{1}{x}}) ' = 0$. As $(x^{\frac{1}{x}}) '$ is bounded, we can repeat the argument inductively. The first two derivates are: $$(x^{\frac{1}{x}}) ' = \exp \left( \tfrac{\ln(x)}{x} \right)\,\left( \frac{1-\ln(x)}{x^2} \right) = \sqrt[x]{x}\,\left( \frac{1-\ln(x)}{x^2} \right)$$ $$ (x^{\frac{1}{x}}) ''  =  \sqrt[x]{x} \, \left( \ln^2(x) - 3\,x + 2\, (x-1)\, \ln(x) + 1  \right) \, \frac{1}{x^4} $$",,"['calculus', 'real-analysis', 'limits', 'radicals']"
57,Identify $\lim\limits_{x \to +\infty } x^2 \left(\sqrt{x^4+x+1}-\sqrt{x^4+x+5}\right)$,Identify,\lim\limits_{x \to +\infty } x^2 \left(\sqrt{x^4+x+1}-\sqrt{x^4+x+5}\right),Identify $$\lim\limits_{x \to +\infty } x^2 \left(\sqrt{x^4+x+1}-\sqrt{x^4+x+5}\right)$$ My Try : $$\sqrt{x^4+x+1}=\sqrt{x^4(1+\frac{1}{x^3}+\frac{1}{x^4})}=x^2\sqrt{(1+\frac{1}{x^3}+\frac{1}{x^4})}$$ Now : $$\frac{1}{x^3}+\frac{1}{x^4}=z$$ $$(1+z)^{\frac{1}{n}}= 1 + \frac1n x + \frac{1 - n}{2n^2}x^2 + \frac{2n^2 - 3n + 1}{6n^3}x^3 + O(x^4)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac12 z - \frac{1}{8}z^2 + + O(z^3)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac12 (\frac{1}{x^3}+\frac{1}{x^4}) - \frac{1}{8}(\frac{1}{x^3}+\frac{1}{x^4})^2 + O(z^3)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac{1}{2x^3}+\frac{1}{2x^4} - \frac{x^2+2x+1}{8x^8} + O(z^3)$$ And : $$\sqrt{x^4+x+1}=\sqrt{x^4(1+\frac{1}{x^3}+\frac{5}{x^4})}=x^2\sqrt{(1+\frac{1}{x^3}+\frac{5}{x^4})}$$ So : $$(1+(\frac{1}{x^3}+\frac{5}{x^4}))^{\frac{1}{2}}= 1 +  \frac{1}{2x^3}+\frac{5}{2x^4} -  \frac{x^2+10x+25}{8x^8}+ O(z^3)$$ $$\lim\limits_{x \to +\infty }=x^4( 1 + \frac{1}{2x^3}+\frac{1}{2x^4} - \frac{x^2+2x+1}{8x^8} + O(z^3)-( 1 +  \frac{1}{2x^3}+\frac{5}{2x^4} -  \frac{x^2+10x+25}{8x^8}+ O(z^3)))$$ $$\lim\limits_{x \to +\infty }=x^4(\frac{1}{2x^4}-\frac{5}{2x^4})=x^4(\frac{-4}{2x^4})=-2$$ is it right ?,Identify $$\lim\limits_{x \to +\infty } x^2 \left(\sqrt{x^4+x+1}-\sqrt{x^4+x+5}\right)$$ My Try : $$\sqrt{x^4+x+1}=\sqrt{x^4(1+\frac{1}{x^3}+\frac{1}{x^4})}=x^2\sqrt{(1+\frac{1}{x^3}+\frac{1}{x^4})}$$ Now : $$\frac{1}{x^3}+\frac{1}{x^4}=z$$ $$(1+z)^{\frac{1}{n}}= 1 + \frac1n x + \frac{1 - n}{2n^2}x^2 + \frac{2n^2 - 3n + 1}{6n^3}x^3 + O(x^4)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac12 z - \frac{1}{8}z^2 + + O(z^3)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac12 (\frac{1}{x^3}+\frac{1}{x^4}) - \frac{1}{8}(\frac{1}{x^3}+\frac{1}{x^4})^2 + O(z^3)$$ $$(1+(\frac{1}{x^3}+\frac{1}{x^4}))^{\frac{1}{2}}= 1 + \frac{1}{2x^3}+\frac{1}{2x^4} - \frac{x^2+2x+1}{8x^8} + O(z^3)$$ And : $$\sqrt{x^4+x+1}=\sqrt{x^4(1+\frac{1}{x^3}+\frac{5}{x^4})}=x^2\sqrt{(1+\frac{1}{x^3}+\frac{5}{x^4})}$$ So : $$(1+(\frac{1}{x^3}+\frac{5}{x^4}))^{\frac{1}{2}}= 1 +  \frac{1}{2x^3}+\frac{5}{2x^4} -  \frac{x^2+10x+25}{8x^8}+ O(z^3)$$ $$\lim\limits_{x \to +\infty }=x^4( 1 + \frac{1}{2x^3}+\frac{1}{2x^4} - \frac{x^2+2x+1}{8x^8} + O(z^3)-( 1 +  \frac{1}{2x^3}+\frac{5}{2x^4} -  \frac{x^2+10x+25}{8x^8}+ O(z^3)))$$ $$\lim\limits_{x \to +\infty }=x^4(\frac{1}{2x^4}-\frac{5}{2x^4})=x^4(\frac{-4}{2x^4})=-2$$ is it right ?,,['limits']
58,Prove that these definitions of $e$ are equivalent [duplicate],Prove that these definitions of  are equivalent [duplicate],e,"This question already has answers here : Particular definition of e (3 answers) Closed 7 years ago . Im sorry if this is stupid or obvious, but why $$ e=\lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n$$ AND $e$ is the unique positive number for which $$ \lim_{h\rightarrow 0}\frac{e^h-1}{h}=1$$? I mean, how do we know that these two definitions are equivalent? Again, maybe it's easy, but I'm just beginning calculus and our teacher just dropped those two definitions...","This question already has answers here : Particular definition of e (3 answers) Closed 7 years ago . Im sorry if this is stupid or obvious, but why $$ e=\lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n$$ AND $e$ is the unique positive number for which $$ \lim_{h\rightarrow 0}\frac{e^h-1}{h}=1$$? I mean, how do we know that these two definitions are equivalent? Again, maybe it's easy, but I'm just beginning calculus and our teacher just dropped those two definitions...",,"['calculus', 'limits']"
59,e as limit with a rational sequence instead of n,e as limit with a rational sequence instead of n,,"Hello and happy New Year to the Stackexchange community. I had trouble solving this seemingly trivial problem and it would be great if you could help me out. $$\lim_{n \to \infty}(1+\frac{1}{n})^{n}=e$$ This definition of e is given and I want to show that the following identitiy is true as well. $$\lim_{n \to \infty} (1+\frac{1}{q_{n}})^{q_{n}} = e$$ with $q_{n}$ as a positive rational sequence which diverges to $+\infty$ (therefore, $\lim_{n \to \infty} q_{n}=+\infty$). I figured that since $$\lim_{n \to \infty} n = +\infty = \lim_{n \to \infty} q_{n}$$ I could move $\lim_{n \to \infty}$ inside the given expression, so that we have $$\lim_{n \to \infty}(1+\frac{1}{q_{n}})^{q_{n}}=(1+\frac{1}{\lim_{n \to \infty} q_{n}})^{\lim_{n \to \infty}q_{n}} = (1+\frac{1}{\lim_{n \to \infty} n})^{\lim_{n \to \infty} n} = \lim_{n \to \infty}(1+\frac{1}{n})^{n}=e.$$ However, apperantly, I need to show that the sequence is continuous in order to do this and my course didn't introduce continuity yet. Of course, I could first prove the therom needed to move the limit inside the expression and then show the sequence is continous, but I feel like there is a much more easier and elegant solution to this. I'm sorry if this is a duplicate of a previously answered question (at least I couldn't find one) and any insight would be appreciated. Thank you very much. Update — I think I managed to solve it by brute force, but it took me hours and my proof is 2+ pages long. I'm sure that my proof was not the intended one and there must be a simpler one. Anyway, thank you for the help and when I have time, I will update this question with my solution as reference.","Hello and happy New Year to the Stackexchange community. I had trouble solving this seemingly trivial problem and it would be great if you could help me out. $$\lim_{n \to \infty}(1+\frac{1}{n})^{n}=e$$ This definition of e is given and I want to show that the following identitiy is true as well. $$\lim_{n \to \infty} (1+\frac{1}{q_{n}})^{q_{n}} = e$$ with $q_{n}$ as a positive rational sequence which diverges to $+\infty$ (therefore, $\lim_{n \to \infty} q_{n}=+\infty$). I figured that since $$\lim_{n \to \infty} n = +\infty = \lim_{n \to \infty} q_{n}$$ I could move $\lim_{n \to \infty}$ inside the given expression, so that we have $$\lim_{n \to \infty}(1+\frac{1}{q_{n}})^{q_{n}}=(1+\frac{1}{\lim_{n \to \infty} q_{n}})^{\lim_{n \to \infty}q_{n}} = (1+\frac{1}{\lim_{n \to \infty} n})^{\lim_{n \to \infty} n} = \lim_{n \to \infty}(1+\frac{1}{n})^{n}=e.$$ However, apperantly, I need to show that the sequence is continuous in order to do this and my course didn't introduce continuity yet. Of course, I could first prove the therom needed to move the limit inside the expression and then show the sequence is continous, but I feel like there is a much more easier and elegant solution to this. I'm sorry if this is a duplicate of a previously answered question (at least I couldn't find one) and any insight would be appreciated. Thank you very much. Update — I think I managed to solve it by brute force, but it took me hours and my proof is 2+ pages long. I'm sure that my proof was not the intended one and there must be a simpler one. Anyway, thank you for the help and when I have time, I will update this question with my solution as reference.",,[]
60,nth Root of Product of Fractions,nth Root of Product of Fractions,,"Let $$f_n(x) = \left| \prod_{k=0}^n \left( x - \frac{k}{n} \right) \right|^{\frac{1}{n}}$$ Based purely on examination of the graph below, which has $n=100$, it appears that for $x \not \in \mathbb{Q} \cap [0, 1], \; \, f_n(x) \to f(x)$ as $n \to \infty$ where $f$ is some continuous function. I have been attempting to work towards identifying $f$, although unfortunately haven't really gotten anywhere. Otherwise some insight into proving (or disproving) any of the following observations would also be useful: $f(0) = f(1) = e^{-1}$ $f(\frac{1}{2}-x) = f(\frac{1}{2}+x)$ $f$ has a minimum at $x = \frac{1}{2}$ Asymptotic behavior of $f$ as $|x| \to \infty$ I have also attempted to consider $\log f_n(x) = \frac{1}{n} \sum_{k=0}^n \log |x-\frac{k}{n}|$, but to no avail.","Let $$f_n(x) = \left| \prod_{k=0}^n \left( x - \frac{k}{n} \right) \right|^{\frac{1}{n}}$$ Based purely on examination of the graph below, which has $n=100$, it appears that for $x \not \in \mathbb{Q} \cap [0, 1], \; \, f_n(x) \to f(x)$ as $n \to \infty$ where $f$ is some continuous function. I have been attempting to work towards identifying $f$, although unfortunately haven't really gotten anywhere. Otherwise some insight into proving (or disproving) any of the following observations would also be useful: $f(0) = f(1) = e^{-1}$ $f(\frac{1}{2}-x) = f(\frac{1}{2}+x)$ $f$ has a minimum at $x = \frac{1}{2}$ Asymptotic behavior of $f$ as $|x| \to \infty$ I have also attempted to consider $\log f_n(x) = \frac{1}{n} \sum_{k=0}^n \log |x-\frac{k}{n}|$, but to no avail.",,"['limits', 'infinite-product']"
61,Geometric transformations and limit (pi=4 revisited!!),Geometric transformations and limit (pi=4 revisited!!),,"Consider three geometric transformations. 1st: The geometric ""proof"" that hypotenuse of a right triangle  is equal to the sum of squares of the other sides. The link is here. 2nd: The famous ""pi=4"" ""proof"" by geometric transformation 3rd: Archimedes's method of calculation  of pi What is inherently different in these transformations that 1st 2nd yield wrong results while the 3rd doesn't? Is there any heuristic which could hint us when the transformation is valid for calculation of the limit? I have read the discussion on mathstackexchange about pi=4, but still I couldn't get that intuition. There was an interesting argument about the divergence of the derivatives of those two curves. But I still don't understand if it is a satisfactory condition to prove that Archimedes construction was a valid one. If possible, please give 2 two explanations from intuitional and strictly mathematical point of view.","Consider three geometric transformations. 1st: The geometric ""proof"" that hypotenuse of a right triangle  is equal to the sum of squares of the other sides. The link is here. 2nd: The famous ""pi=4"" ""proof"" by geometric transformation 3rd: Archimedes's method of calculation  of pi What is inherently different in these transformations that 1st 2nd yield wrong results while the 3rd doesn't? Is there any heuristic which could hint us when the transformation is valid for calculation of the limit? I have read the discussion on mathstackexchange about pi=4, but still I couldn't get that intuition. There was an interesting argument about the divergence of the derivatives of those two curves. But I still don't understand if it is a satisfactory condition to prove that Archimedes construction was a valid one. If possible, please give 2 two explanations from intuitional and strictly mathematical point of view.",,"['calculus', 'real-analysis', 'limits', 'pi', 'fake-proofs']"
62,Limit values of a continuous function with a parameter,Limit values of a continuous function with a parameter,,"Denote $S=(0,1]\times[0,1]$ a square without the left side. Does there exist  a function $f$ smooth enough on $S$, say $f\in C^1(S)$ or $C(S)$ s.t. I) $f$ is not bounded in any neighbourhood of any point $(0,y)$ on the left side of $S$; II) for every $y\in[0,1]$ there exists $\lim_{x\to+0}f(x,y)=g(y)$ and $g\in C([0,1])$?","Denote $S=(0,1]\times[0,1]$ a square without the left side. Does there exist  a function $f$ smooth enough on $S$, say $f\in C^1(S)$ or $C(S)$ s.t. I) $f$ is not bounded in any neighbourhood of any point $(0,y)$ on the left side of $S$; II) for every $y\in[0,1]$ there exists $\lim_{x\to+0}f(x,y)=g(y)$ and $g\in C([0,1])$?",,"['real-analysis', 'limits', 'continuity']"
63,Does the converse to Kronecker's lemma hold?,Does the converse to Kronecker's lemma hold?,,"Odds are that this question has been answered already and even that the argument is not too complicated, but here it goes: Assume that $(a_{k})_{k\in\mathbb{N}}$ is a sequence of real numbers and $(b_{k})_{k\in\mathbb{N}}$ is a sequence of positive numbers with $\lim_{n}b_{n}=\infty$. Can we infer from the hypothesis $$\sum_{k}a_{k}$$ is not convergent that $$\limsup_{n}|\frac{1}{b_{n}}\sum_{k=1}^{n}a_{k}b_{k}|>0?$$  In other words, does the converse to Kronecker's lemma hold? For the purpose of my question, you can assume that $(a_{n})_{n}$ is positive and decreasing and that $b_{n}=n$.","Odds are that this question has been answered already and even that the argument is not too complicated, but here it goes: Assume that $(a_{k})_{k\in\mathbb{N}}$ is a sequence of real numbers and $(b_{k})_{k\in\mathbb{N}}$ is a sequence of positive numbers with $\lim_{n}b_{n}=\infty$. Can we infer from the hypothesis $$\sum_{k}a_{k}$$ is not convergent that $$\limsup_{n}|\frac{1}{b_{n}}\sum_{k=1}^{n}a_{k}b_{k}|>0?$$  In other words, does the converse to Kronecker's lemma hold? For the purpose of my question, you can assume that $(a_{n})_{n}$ is positive and decreasing and that $b_{n}=n$.",,"['sequences-and-series', 'limits', 'convergence-divergence', 'cesaro-summable']"
64,$\lim_{n \to \infty} \mid a_n + 3(\frac{n-2}{n})^n \mid^{\frac1n} = \frac35$. Then find $\lim_{n \to \infty} a_n$. [duplicate],. Then find . [duplicate],\lim_{n \to \infty} \mid a_n + 3(\frac{n-2}{n})^n \mid^{\frac1n} = \frac35 \lim_{n \to \infty} a_n,This question already has answers here : What is the value of lim$_{n\to \infty} a_n$ if $\lim_{n \to \infty}\left|a_n+ 3\left(\frac{n-2}{n}\right)^n \right|^{\frac{1}{n}}=\frac{3}{5}$? (3 answers) Closed 7 years ago . Let $\{a_n\}$ be a sequence of real numbers such that $$\lim_{n \to \infty} \mid a_n + 3(\frac{n-2}{n})^n \mid^{\frac1n} = \frac35$$ Then find  $\lim_{n \to \infty} a_n$. Tried very hard yet not able to crack it. Help Needed. Anyone able to do the sum??,This question already has answers here : What is the value of lim$_{n\to \infty} a_n$ if $\lim_{n \to \infty}\left|a_n+ 3\left(\frac{n-2}{n}\right)^n \right|^{\frac{1}{n}}=\frac{3}{5}$? (3 answers) Closed 7 years ago . Let $\{a_n\}$ be a sequence of real numbers such that $$\lim_{n \to \infty} \mid a_n + 3(\frac{n-2}{n})^n \mid^{\frac1n} = \frac35$$ Then find  $\lim_{n \to \infty} a_n$. Tried very hard yet not able to crack it. Help Needed. Anyone able to do the sum??,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
65,Does the limit exist for $y=\sqrt{x}\sin \frac{1}{x}$?,Does the limit exist for ?,y=\sqrt{x}\sin \frac{1}{x},"For the graph of   $$y=\sqrt{x}\sin \frac{1}{x},$$ Do the following limits exist? If so, what is it? (a) $\lim_{x \to 0^+} f(x)$ (b) $\lim_{x \to 0^-}f(x)$ (c) $\lim_{x \to 0}f(x)$ By the way, the graph is $\sin \frac{1}{x}$ inside of the parabola $x=y^2$ Here's what I got (a)  Yes, the limit is $0$ (b)  No (c) No I am confused on (a) because if $x$ approaches $0$ from the right hand side, then, according to the parabola, it reaches the limit $0$. But if $x$ approaches $0$ from the right hand side, according to $\sin\frac{1}{x}$, there is not limit. Please help?","For the graph of   $$y=\sqrt{x}\sin \frac{1}{x},$$ Do the following limits exist? If so, what is it? (a) $\lim_{x \to 0^+} f(x)$ (b) $\lim_{x \to 0^-}f(x)$ (c) $\lim_{x \to 0}f(x)$ By the way, the graph is $\sin \frac{1}{x}$ inside of the parabola $x=y^2$ Here's what I got (a)  Yes, the limit is $0$ (b)  No (c) No I am confused on (a) because if $x$ approaches $0$ from the right hand side, then, according to the parabola, it reaches the limit $0$. But if $x$ approaches $0$ from the right hand side, according to $\sin\frac{1}{x}$, there is not limit. Please help?",,"['calculus', 'limits', 'continuity']"
66,"Prove that if $\lim _{x\to \infty } f(x)$ exists ,then $\lim_{x\to \infty} f(x)=0$","Prove that if  exists ,then",\lim _{x\to \infty } f(x) \lim_{x\to \infty} f(x)=0,"Let $f:\Bbb R\to \Bbb R$ be a continuous function such that $\int _0^\infty f(x)\text{dx}$ exists. Prove that If $\lim _{x\to \infty }  f(x)$ exists, then $\lim_{x\to \infty} f(x)=0$ If $f$ is non-negative then $\lim _{x\to \infty }  f(x)$ must exist and $\lim_{x\to \infty} f(x)=0$ My try To prove that $\lim_{x\to \infty} f(x)=0$ we should show that $\exists G>0$ such that $x>G\implies |f(x)|<\epsilon $ for any $\epsilon>0$ But I can't find out how to show this. Please help.","Let be a continuous function such that exists. Prove that If exists, then If is non-negative then must exist and My try To prove that we should show that such that for any But I can't find out how to show this. Please help.",f:\Bbb R\to \Bbb R \int _0^\infty f(x)\text{dx} \lim _{x\to \infty }  f(x) \lim_{x\to \infty} f(x)=0 f \lim _{x\to \infty }  f(x) \lim_{x\to \infty} f(x)=0 \lim_{x\to \infty} f(x)=0 \exists G>0 x>G\implies |f(x)|<\epsilon  \epsilon>0,"['real-analysis', 'limits', 'continuity']"
67,"For what $m,n$ does the limit exist.",For what  does the limit exist.,"m,n","Let $f: (0, \infty) \times (0, \infty) \to [0, \infty)$ by given by $$f(x,y) = \frac{x^ny^m}{x+y}.$$ Find all $m,n$ such that $\lim_{(x,y) \to (0,0)} f(x,y)$ exists. Consider the line $y=kx$ for some $k \in \mathbb{R}$. We observe that \begin{eqnarray*} \lim_{(x,y) \to (0,0)} \frac{x^ny^m}{x+y} &=& \lim_{(x,kx) \to (0,0)} \frac{x^n(kx)^m}{x+(kx)} \\ &=& \lim_{(x,kx) \to (0,0)} \frac{k^m x^{n+m}}{x(1+k)} \\ &=& \lim_{(x,kx) \to (0,0)} \frac{k^m}{1+k} \cdot x^{n+m-1}. \end{eqnarray*} This diverges if $n+m-1 < 0$. Also, the limit is dependent on $k$ if $n+m-1=0$. Therefore, we see that $\lim_{(x,y) \to (0,0)} f(x,y)$ exists if $n+m -1 > 0$. My question is regarding sufficiency, sure, my solution is necessary, but it is sufficient to simply take the path $y=kx$?","Let $f: (0, \infty) \times (0, \infty) \to [0, \infty)$ by given by $$f(x,y) = \frac{x^ny^m}{x+y}.$$ Find all $m,n$ such that $\lim_{(x,y) \to (0,0)} f(x,y)$ exists. Consider the line $y=kx$ for some $k \in \mathbb{R}$. We observe that \begin{eqnarray*} \lim_{(x,y) \to (0,0)} \frac{x^ny^m}{x+y} &=& \lim_{(x,kx) \to (0,0)} \frac{x^n(kx)^m}{x+(kx)} \\ &=& \lim_{(x,kx) \to (0,0)} \frac{k^m x^{n+m}}{x(1+k)} \\ &=& \lim_{(x,kx) \to (0,0)} \frac{k^m}{1+k} \cdot x^{n+m-1}. \end{eqnarray*} This diverges if $n+m-1 < 0$. Also, the limit is dependent on $k$ if $n+m-1=0$. Therefore, we see that $\lim_{(x,y) \to (0,0)} f(x,y)$ exists if $n+m -1 > 0$. My question is regarding sufficiency, sure, my solution is necessary, but it is sufficient to simply take the path $y=kx$?",,"['calculus', 'real-analysis']"
68,Relationship between Cesàro's Lemma and Stolz–Cesàro theorem,Relationship between Cesàro's Lemma and Stolz–Cesàro theorem,,Probability with Martingales: From Wiki : What is their relationship? Does any imply the other?,Probability with Martingales: From Wiki : What is their relationship? Does any imply the other?,,"['real-analysis', 'sequences-and-series', 'limits']"
69,Calculating $\lim \limits_{x \to \infty} \frac{x+\frac12\cos x}{x-\frac12\sin x}$ using the sandwich theorem,Calculating  using the sandwich theorem,\lim \limits_{x \to \infty} \frac{x+\frac12\cos x}{x-\frac12\sin x},"Calculating $\lim \limits_{x \to \infty} \dfrac{x+\frac12\cos x}{x-\frac12\sin x}$ Correct me if I'm wrong: $\cos x$ and $\sin x$ are bounded so that $$|\cos x|\le 1,\qquad |\sin x|\le1$$ Therefore I can say: $$ \frac{x-\frac12}{x+\frac12}\le \frac{x+\frac12\cos x}{x-\frac12\sin x}\le  \frac{x+\frac12}{x-\frac12} $$ the limits of the left and right side are equal to 1, therefore the the limit I'm looking for is also equal to 1 . The answer is correct, but what I'm not sure is $$ \frac{x-\frac12}{x+\frac12}\le\frac{x+\frac12\cos x}{x-\frac12\sin x} $$ was this step correct?","Calculating $\lim \limits_{x \to \infty} \dfrac{x+\frac12\cos x}{x-\frac12\sin x}$ Correct me if I'm wrong: $\cos x$ and $\sin x$ are bounded so that $$|\cos x|\le 1,\qquad |\sin x|\le1$$ Therefore I can say: $$ \frac{x-\frac12}{x+\frac12}\le \frac{x+\frac12\cos x}{x-\frac12\sin x}\le  \frac{x+\frac12}{x-\frac12} $$ the limits of the left and right side are equal to 1, therefore the the limit I'm looking for is also equal to 1 . The answer is correct, but what I'm not sure is $$ \frac{x-\frac12}{x+\frac12}\le\frac{x+\frac12\cos x}{x-\frac12\sin x} $$ was this step correct?",,['limits']
70,Prove that $\lim_{y\to\infty}\frac{y}{t-1}e^{y(t-1)} = 0$.,Prove that .,\lim_{y\to\infty}\frac{y}{t-1}e^{y(t-1)} = 0,"For the distribution $g(y)=ye^{-y}$ for $y\geq 0$ and $0$ otherwise, I have shown, through integrating by parts, that the moment generating function is: $M_Y(t)=\int_0^\infty e^{ty}ye^{-y}dy=\left[\frac{y}{t-1}e^{y(t-1)}\right]_0 ^\infty-\int_0^\infty \frac{1}{t-1}e^{y(t-1)}=\left[\frac{y}{t-1}e^{y(t-1)}\right]_0 ^\infty +\frac{1}{(t-1)^2}$ given $t<1$. However, I need to show that $\lim_{y\to\infty}\frac{y}{t-1}e^{y(t-1)} = 0$ in order to conclude that the $M_Y(t)=\frac{1}{(t-1)^2}$. I am having trouble evaluating the limit and can not apply the product rule for limits since the two constituent functions of it do not both have a finite limit. Thank you for any help.","For the distribution $g(y)=ye^{-y}$ for $y\geq 0$ and $0$ otherwise, I have shown, through integrating by parts, that the moment generating function is: $M_Y(t)=\int_0^\infty e^{ty}ye^{-y}dy=\left[\frac{y}{t-1}e^{y(t-1)}\right]_0 ^\infty-\int_0^\infty \frac{1}{t-1}e^{y(t-1)}=\left[\frac{y}{t-1}e^{y(t-1)}\right]_0 ^\infty +\frac{1}{(t-1)^2}$ given $t<1$. However, I need to show that $\lim_{y\to\infty}\frac{y}{t-1}e^{y(t-1)} = 0$ in order to conclude that the $M_Y(t)=\frac{1}{(t-1)^2}$. I am having trouble evaluating the limit and can not apply the product rule for limits since the two constituent functions of it do not both have a finite limit. Thank you for any help.",,"['limits', 'definite-integrals', 'moment-generating-functions']"
71,"When does $ \lim\limits_{n\to \infty} \, (\underbrace{f \circ \dots \circ f}_{n}) (\xi)$ exist?",When does  exist?," \lim\limits_{n\to \infty} \, (\underbrace{f \circ \dots \circ f}_{n}) (\xi)","$\newcommand{\coloneqq}{\colon\!=}$ Let $k \in \mathbb{N}$ and $a_0, \dots, a_k \in \mathbb{R}$ and $a_k \neq 0$. Let  $$\begin{align*} f: \mathbb{R}\smallsetminus \{0\} &\to \mathbb{R} \\ \forall x \in \mathbb{R}\smallsetminus \{0\} \quad x&\mapsto \sum_{i=0}^{k} \frac{a_i}{x^i} \end{align*}$$ Let's define $f^{\left[n\right]}$ recursively  $$ \begin{align*} f^{\left[1\right]} &\coloneqq f \\ \forall n \in \mathbb{N}\smallsetminus \{0\} \quad f^{\left[n+1\right]} &\coloneqq f \circ f^{\left[n\right]}  \end{align*}$$ Let $\xi \in \mathbb{R} \smallsetminus \{0\}$ such that $f(\xi)\neq 0$. What conditions on $a_0, \dots, a_k$ and $\xi$ are sufficient for the existence of the following limit? $$ \lim\limits_{n\to \infty} \, f^{\left[n\right]} (\xi)$$ I have only solved the case $k=1$ so far. If I am not mistaken, the limit exists when $P_2(x):=x^2-a_0x-a_1$ has real roots and $a_0 \neq 0$. Moreover, if the limit exists, then: if $\xi$ is a root of $P_2(x)$, then the limit converges to it, otherwise it is the value of the continued fraction $\left[ \overline{a_0,\frac{a_0}{a_1}} \right]$, which is a root of $P_2(x)$.","$\newcommand{\coloneqq}{\colon\!=}$ Let $k \in \mathbb{N}$ and $a_0, \dots, a_k \in \mathbb{R}$ and $a_k \neq 0$. Let  $$\begin{align*} f: \mathbb{R}\smallsetminus \{0\} &\to \mathbb{R} \\ \forall x \in \mathbb{R}\smallsetminus \{0\} \quad x&\mapsto \sum_{i=0}^{k} \frac{a_i}{x^i} \end{align*}$$ Let's define $f^{\left[n\right]}$ recursively  $$ \begin{align*} f^{\left[1\right]} &\coloneqq f \\ \forall n \in \mathbb{N}\smallsetminus \{0\} \quad f^{\left[n+1\right]} &\coloneqq f \circ f^{\left[n\right]}  \end{align*}$$ Let $\xi \in \mathbb{R} \smallsetminus \{0\}$ such that $f(\xi)\neq 0$. What conditions on $a_0, \dots, a_k$ and $\xi$ are sufficient for the existence of the following limit? $$ \lim\limits_{n\to \infty} \, f^{\left[n\right]} (\xi)$$ I have only solved the case $k=1$ so far. If I am not mistaken, the limit exists when $P_2(x):=x^2-a_0x-a_1$ has real roots and $a_0 \neq 0$. Moreover, if the limit exists, then: if $\xi$ is a root of $P_2(x)$, then the limit converges to it, otherwise it is the value of the continued fraction $\left[ \overline{a_0,\frac{a_0}{a_1}} \right]$, which is a root of $P_2(x)$.",,"['real-analysis', 'limits']"
72,What is $\sum_{k=0}^\infty {k^k \over (k!)^2}$? [duplicate],What is ? [duplicate],\sum_{k=0}^\infty {k^k \over (k!)^2},"This question already has answers here : Series $\sum_{n=1}^\infty \frac{n^n}{(n!)^2}$ (2 answers) Closed 4 years ago . I know that this series converges, and the limit is approximately 3.548. But what is the exact sum, and how do you determine it?","This question already has answers here : Series $\sum_{n=1}^\infty \frac{n^n}{(n!)^2}$ (2 answers) Closed 4 years ago . I know that this series converges, and the limit is approximately 3.548. But what is the exact sum, and how do you determine it?",,"['sequences-and-series', 'limits']"
73,$\lim \frac{e^{h^2}-1}{h}$ as h goes to $0$,as h goes to,\lim \frac{e^{h^2}-1}{h} 0,Find $\displaystyle \lim_{h \to 0} \frac{e^{h^2}-1}{h}$ $\displaystyle \lim_{h \to 0} \frac{e^{h^2}-1}{h} =\lim_{h \to 0} \frac{e^{(h+x)^2}-e^{x^2}}{h}\bigg|_{x=0} = \left(e^{x^2}\right)'\bigg|_{x=0} = 2 \cdot 0 \cdot e^0 =  0. $ Is this correct?,Find $\displaystyle \lim_{h \to 0} \frac{e^{h^2}-1}{h}$ $\displaystyle \lim_{h \to 0} \frac{e^{h^2}-1}{h} =\lim_{h \to 0} \frac{e^{(h+x)^2}-e^{x^2}}{h}\bigg|_{x=0} = \left(e^{x^2}\right)'\bigg|_{x=0} = 2 \cdot 0 \cdot e^0 =  0. $ Is this correct?,,"['calculus', 'limits', 'proof-verification']"
74,Convergence of improper integral with $f(x)\to 1$ as $x\to +\infty$,Convergence of improper integral with  as,f(x)\to 1 x\to +\infty,"Suppose $f\in \mathscr{R}$ on $[0,A]$ for all $A<\infty$, and $f(x)\to 1$ as $x\to +\infty$. Prove that $$\lim \limits_{t\to 0}t\int_{0}^{\infty}e^{-tx}f(x)dx=1 \quad (t>0).$$ Proof: Let's define $F(t)=t\int_{0}^{\infty}e^{-tx}(f(x)-1)dx$ for $t>0$. Let $\epsilon>0$ be given then $\exists A=A(\epsilon)>0$ such that for any $x\geqslant A$ we have $|f(x)-1|<{\epsilon}/{2}.$ Then $$|F(t)|=\left|t\int_{0}^{\infty}e^{-tx}(f(x)-1)dx\right|\leqslant t\int_{0}^{\infty}e^{-tx}|f(x)-1|dx=t\left(\int_{0}^{A}+\int_{A}^{\infty} \right)\leqslant$$$$\leqslant t\int_{0}^{A}e^{-tx}|f(x)-1|dx+t\frac{\epsilon}{2}\int_{A}^{\infty}e^{-tx}dx.$$ Since $|f-1|\in \mathscr{R}$ on $[0,A]$ then $|f-1|$ is bounded on $[0,A]$ (Rudin's assumption on Chapter 6) and let $C=\sup \limits_{[0,A]}|f(x)-1|$ then we get that: $$|F(t)|\le Ct\int_{0}^{A}e^{-tx}dx+t\frac{\epsilon}{2}\int_{A}^{\infty}e^{-tx}dx=C(1-e^{-At})+\dfrac{\epsilon}{2}e^{-At}<ACt+\dfrac{\epsilon}{2}$$ since $0<e^{-At}<1$ and $e^{-At}>1-At$. Taking $\delta=\dfrac{\epsilon}{2AC}$ then for any $t\in (0,\delta)$ we get $|F(t)|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$ which is equivalent to $\lim \limits_{t\to 0+}F(t)=1$ and we get our desired result. Can anyone check my proof please? I would be very grateful for your help!","Suppose $f\in \mathscr{R}$ on $[0,A]$ for all $A<\infty$, and $f(x)\to 1$ as $x\to +\infty$. Prove that $$\lim \limits_{t\to 0}t\int_{0}^{\infty}e^{-tx}f(x)dx=1 \quad (t>0).$$ Proof: Let's define $F(t)=t\int_{0}^{\infty}e^{-tx}(f(x)-1)dx$ for $t>0$. Let $\epsilon>0$ be given then $\exists A=A(\epsilon)>0$ such that for any $x\geqslant A$ we have $|f(x)-1|<{\epsilon}/{2}.$ Then $$|F(t)|=\left|t\int_{0}^{\infty}e^{-tx}(f(x)-1)dx\right|\leqslant t\int_{0}^{\infty}e^{-tx}|f(x)-1|dx=t\left(\int_{0}^{A}+\int_{A}^{\infty} \right)\leqslant$$$$\leqslant t\int_{0}^{A}e^{-tx}|f(x)-1|dx+t\frac{\epsilon}{2}\int_{A}^{\infty}e^{-tx}dx.$$ Since $|f-1|\in \mathscr{R}$ on $[0,A]$ then $|f-1|$ is bounded on $[0,A]$ (Rudin's assumption on Chapter 6) and let $C=\sup \limits_{[0,A]}|f(x)-1|$ then we get that: $$|F(t)|\le Ct\int_{0}^{A}e^{-tx}dx+t\frac{\epsilon}{2}\int_{A}^{\infty}e^{-tx}dx=C(1-e^{-At})+\dfrac{\epsilon}{2}e^{-At}<ACt+\dfrac{\epsilon}{2}$$ since $0<e^{-At}<1$ and $e^{-At}>1-At$. Taking $\delta=\dfrac{\epsilon}{2AC}$ then for any $t\in (0,\delta)$ we get $|F(t)|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$ which is equivalent to $\lim \limits_{t\to 0+}F(t)=1$ and we get our desired result. Can anyone check my proof please? I would be very grateful for your help!",,"['real-analysis', 'integration', 'limits', 'improper-integrals']"
75,How to evaluate $\lim_{q\to \infty} \left(\sum_{i=1}^m p_i^{q}\right)^{\frac{1}{1-q}}$?,How to evaluate ?,\lim_{q\to \infty} \left(\sum_{i=1}^m p_i^{q}\right)^{\frac{1}{1-q}},Here is the problem. I have to compute the limit: $$ \lim_{q\to \infty} \left(\sum_{i=1}^m p_i^{q}\right)^{\frac{1}{1-q}}  $$ where $p_i$s are numbers from $0$ to $1$ and $\sum_{i=1}^mp_i=1$. I found that solution should be $\frac1{p_{max}}$ but I don't know why. Thanks a lot for your time.,Here is the problem. I have to compute the limit: $$ \lim_{q\to \infty} \left(\sum_{i=1}^m p_i^{q}\right)^{\frac{1}{1-q}}  $$ where $p_i$s are numbers from $0$ to $1$ and $\sum_{i=1}^mp_i=1$. I found that solution should be $\frac1{p_{max}}$ but I don't know why. Thanks a lot for your time.,,"['limits', 'summation']"
76,Function continuity $(x^2 - 1)/( x - 1)$,Function continuity,(x^2 - 1)/( x - 1),"What can you say about the function continuity of $$\frac{x^2 - 1}{x - 1}$$ at $x = 1$? There is an asymptote at $x = 1$, however the limit as $x$ goes to $1$ becomes $2$ because $f(x) = x + 1$ after factoring, also the limit becomes $2$ because we again factor, so in total by definition the function is continuous, however if we draw the graph then there is an asymptote at $x = 1$, so it's discontinuous. So in then final result what can we say about the continuity of this function?","What can you say about the function continuity of $$\frac{x^2 - 1}{x - 1}$$ at $x = 1$? There is an asymptote at $x = 1$, however the limit as $x$ goes to $1$ becomes $2$ because $f(x) = x + 1$ after factoring, also the limit becomes $2$ because we again factor, so in total by definition the function is continuous, however if we draw the graph then there is an asymptote at $x = 1$, so it's discontinuous. So in then final result what can we say about the continuity of this function?",,"['limits', 'functions', 'continuity']"
77,"Limit towards infinity, definition and proof?","Limit towards infinity, definition and proof?",,"I have a question regarding an assignment in school. We are learning about limit, epsilon-delta proofs. This is an assignment that I have done but still need to correct. a) Give a proper definition of what it means for a general sequence $$\lim_{x\to\infty} a_n = \infty$$ b) Recall $ n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \cdot \cdot n $ Use the definition from a) to show that $n!$ grows faster than $2^n$. That is, show that; $$ \lim_{n\to\infty} \frac{n!}{2^n} = \infty $$ Ok so there is what I did the first time handing it in. I compared the ratio to show that $a_{n+1} > a_n $ Which we could not use according to my teacher since we have not proved ratio or done them in class yet. I know for an expression to go towards infinity it must not be bounded from above and it must be increasing as the n gets larger. I hafto use my definition in a) and also I have stated another definition in b). That is the definition of an increasing sequence wich is, if increasing:$$ a_{n+1} > a_n $$ I have been trying to use induction and I can show with induction that $ n! > 2^n \forall n \geq 4$ But this according to my teacher is not proof enough that the sequence is going towards infinity when n is going to infinity. I know I am supposed to use induction, but how? I also know that I hafto use my definition from a) and that I should start with: Let $ \varepsilon > 0 $ be given (fixed but unknown). My teacher showed this information: He said to use $1) a_n \geq \frac{n}{4}$ ok! and $n\geq 1$. Or we could use $2) a_n \geq \frac{n}{2}$ ok, and $n \geq 2$. Or third, we could also use $3) a_n \geq n$ ok and $n \geq 6$ We will hafto prove it and I would assume induction would be a good proof? Start with basecase, n = 4 (could choose any basecase but it has to be 1 or more in this case, if i'd choose 1)), then $$\frac{4 \cdot 3 \cdot 2 \cdot 1}{2^4} \geq \frac{n}{4}$$, which is ok.  Then after this i would hafto do induction step, where I assume that it holds for n = k and then it would also hold for n = k + 1. But here Im insecure about exactley what I do. I know I need to prove the induction hypothesis, but exactley what would be the induction hypothesis. Do I prove this???; $$\frac{k(k+1)!}{2^{k+1}} \geq \frac{(k+1)}{4}$$ And how exactly would the following computations look? I find the n! term hard to work with. After the induction proof I guess I also need to find an n, from the definition in a), so that $a_n > \varepsilon$. This I also find hard. I know that the formula should be $$ n \geq 4\varepsilon+1$$ This implies that $\frac{n}{4} > \varepsilon$, since $a_n \geq \frac{4}{n}$ we have, $a_n > \varepsilon$. A good choice when \varepsilon is uknown would then be $$N = 4 \varepsilon +1$$. I understand this reasoning and I understand that N can be made a function of epsilon. I understand epsilon is the challange and to prove the limit exists all challanges, epsilon, can be answered with this N, if the limit is infinity (although infinity is not a number, so the sequence is not convergeing, but divergeing when it goes towards infinity) the expression can with this N always be made larger than any epsilon. My problem is the algebra. How do I find this N? The induction proof, how do I prove that it holds for n=k+1?? I think i have problems overall just getting the assignment to sync and really be sure that I have used the defn. in a accurate, that I hve proven the sequence is growing and that it is going towards infinity. I'ts like I'm almost there and understand it, but there is still some part missing. I think mainly it's the n! term that is giving me problems. If I had $\frac{n+1}{2} > \varepsilon \leftrightarrow n+1 > 2\varepsilon \leftrightarrow n > (2\varepsilon) - 1$. Here the n would have been very easy to compute. Oh well, now it is'nt. So help is much wanted and needed ! :)","I have a question regarding an assignment in school. We are learning about limit, epsilon-delta proofs. This is an assignment that I have done but still need to correct. a) Give a proper definition of what it means for a general sequence $$\lim_{x\to\infty} a_n = \infty$$ b) Recall $ n! = 1 \cdot 2 \cdot 3 \cdot 4 \cdot \cdot \cdot n $ Use the definition from a) to show that $n!$ grows faster than $2^n$. That is, show that; $$ \lim_{n\to\infty} \frac{n!}{2^n} = \infty $$ Ok so there is what I did the first time handing it in. I compared the ratio to show that $a_{n+1} > a_n $ Which we could not use according to my teacher since we have not proved ratio or done them in class yet. I know for an expression to go towards infinity it must not be bounded from above and it must be increasing as the n gets larger. I hafto use my definition in a) and also I have stated another definition in b). That is the definition of an increasing sequence wich is, if increasing:$$ a_{n+1} > a_n $$ I have been trying to use induction and I can show with induction that $ n! > 2^n \forall n \geq 4$ But this according to my teacher is not proof enough that the sequence is going towards infinity when n is going to infinity. I know I am supposed to use induction, but how? I also know that I hafto use my definition from a) and that I should start with: Let $ \varepsilon > 0 $ be given (fixed but unknown). My teacher showed this information: He said to use $1) a_n \geq \frac{n}{4}$ ok! and $n\geq 1$. Or we could use $2) a_n \geq \frac{n}{2}$ ok, and $n \geq 2$. Or third, we could also use $3) a_n \geq n$ ok and $n \geq 6$ We will hafto prove it and I would assume induction would be a good proof? Start with basecase, n = 4 (could choose any basecase but it has to be 1 or more in this case, if i'd choose 1)), then $$\frac{4 \cdot 3 \cdot 2 \cdot 1}{2^4} \geq \frac{n}{4}$$, which is ok.  Then after this i would hafto do induction step, where I assume that it holds for n = k and then it would also hold for n = k + 1. But here Im insecure about exactley what I do. I know I need to prove the induction hypothesis, but exactley what would be the induction hypothesis. Do I prove this???; $$\frac{k(k+1)!}{2^{k+1}} \geq \frac{(k+1)}{4}$$ And how exactly would the following computations look? I find the n! term hard to work with. After the induction proof I guess I also need to find an n, from the definition in a), so that $a_n > \varepsilon$. This I also find hard. I know that the formula should be $$ n \geq 4\varepsilon+1$$ This implies that $\frac{n}{4} > \varepsilon$, since $a_n \geq \frac{4}{n}$ we have, $a_n > \varepsilon$. A good choice when \varepsilon is uknown would then be $$N = 4 \varepsilon +1$$. I understand this reasoning and I understand that N can be made a function of epsilon. I understand epsilon is the challange and to prove the limit exists all challanges, epsilon, can be answered with this N, if the limit is infinity (although infinity is not a number, so the sequence is not convergeing, but divergeing when it goes towards infinity) the expression can with this N always be made larger than any epsilon. My problem is the algebra. How do I find this N? The induction proof, how do I prove that it holds for n=k+1?? I think i have problems overall just getting the assignment to sync and really be sure that I have used the defn. in a accurate, that I hve proven the sequence is growing and that it is going towards infinity. I'ts like I'm almost there and understand it, but there is still some part missing. I think mainly it's the n! term that is giving me problems. If I had $\frac{n+1}{2} > \varepsilon \leftrightarrow n+1 > 2\varepsilon \leftrightarrow n > (2\varepsilon) - 1$. Here the n would have been very easy to compute. Oh well, now it is'nt. So help is much wanted and needed ! :)",,"['calculus', 'analysis', 'limits']"
78,How to compute $\lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)}$,How to compute,\lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)},"Let  $f:\mathbb{R}\longrightarrow \mathbb{R}$ a function such that : $f(x)=x-x^3+o(x^3).$ Compute     $$ \lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)}$$ My thoughts: note that : $e^{x}=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}+o(x^3)$ $\sin(x)=x-\dfrac{x^3}{6}+o(x^3)$ $e^{f(x)}=1+f(x)+\dfrac{f(x)^2}{2}+\dfrac{f(x)^3}{6}+o(f(x)^3)$ $\sin(f(2x))=f(2x)-\dfrac{f(2x)^3}{6}+o(x^3)$ $f(x)=x-x^3+o(x^3)$ $f(2x)=2x-8x^3+o(x^3)$ indeed, \begin{align} e^{f(x)}&=1+f(x)+\dfrac{f(x)^2}{2}+\dfrac{f(x)^3}{6}+o(f(x)^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ \end{align} or $\left(x-x^3+o(x^3) \right)^2=(x^{2}-2x^{4}+x^{6} )+o(x^3)=x^{2}+o(x^3) $ \begin{align} \left(x-x^3+o(x^3) \right)^3&=(x-x^{3}+o(x^3))(x-x^{3}+o(x^3))^2=(x-x^{3}+o(x^3))(x^{2}+o(x^3))\\ &=x^3+o(x^3)  \end{align} we plug those values in $e^{f(x)}$ \begin{align} e^{f(x)}&=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{ x^{2}+o(x^3)}{2}+\dfrac{x^3+o(x^3)}{6}+o(x^3)\\ e^{f(x)}&=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3) \end{align} then \begin{align} e^{f(x)}-e^{x}&=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-e^{x}\\ &=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-(1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}+o(x^3))\\ &=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-1-x-\dfrac{x^2}{2}-\dfrac{x^3}{6}+o(x^3))\\ e^{f(x)}-e^{x}&=-x^{3}+o(x^3) \end{align} $$\fbox{$e^{f(x)}-e^{x}=-x^{3}+o(x^3)$}$$ \begin{align} \sin(f(2x))&=\left(2x-8x^3+o(x^3)\right)-\dfrac{\left(2x-8x^3+o(x^3)\right)^3}{6}+o(x^3)\\ \end{align} or  $\left(2x-8x^3+o(x^3)\right)^3=(8x^{3}\left(1-4x^{2}\right)+o(x^3)=8x^{3}+o(x^3) $ then \begin{align} \sin(f(2x))&=\left(2x-8x^3+o(x^3)\right)-\dfrac{\left(2x-8x^3+o(x^3)\right)^3}{6}+o(x^3)\\ &=\left(2x-8x^3+o(x^3)\right)-\dfrac{8x^{3}+o(x^3)}{6}+o(x^3)\\ &=2x-8x^3-\dfrac{8x^{3}}{6}+o(x^3)\\ &=2x-\dfrac{28x^{3}}{3}+o(x^3)\\ \end{align} $$\fbox{$2x-\sin(f(2x))=\dfrac{28x^{3}}{3}+o(x^3))$}$$ \begin{align} \dfrac{e^{f(x)}-e^x}{2x-\sin\left(f(2x)\right)}&=\dfrac{-x^{3}+o(x^3)}{\dfrac{28x^{3}}{3}+o(x^3)}=-\dfrac{-3}{28}+o(x^3)\\ \end{align} $$\lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)}=\dfrac{-3}{28}$$ Is my proof correct Please if you find any mistake try to correct it with details of calculations","Let  $f:\mathbb{R}\longrightarrow \mathbb{R}$ a function such that : $f(x)=x-x^3+o(x^3).$ Compute     $$ \lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)}$$ My thoughts: note that : $e^{x}=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}+o(x^3)$ $\sin(x)=x-\dfrac{x^3}{6}+o(x^3)$ $e^{f(x)}=1+f(x)+\dfrac{f(x)^2}{2}+\dfrac{f(x)^3}{6}+o(f(x)^3)$ $\sin(f(2x))=f(2x)-\dfrac{f(2x)^3}{6}+o(x^3)$ $f(x)=x-x^3+o(x^3)$ $f(2x)=2x-8x^3+o(x^3)$ indeed, \begin{align} e^{f(x)}&=1+f(x)+\dfrac{f(x)^2}{2}+\dfrac{f(x)^3}{6}+o(f(x)^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ \end{align} or $\left(x-x^3+o(x^3) \right)^2=(x^{2}-2x^{4}+x^{6} )+o(x^3)=x^{2}+o(x^3) $ \begin{align} \left(x-x^3+o(x^3) \right)^3&=(x-x^{3}+o(x^3))(x-x^{3}+o(x^3))^2=(x-x^{3}+o(x^3))(x^{2}+o(x^3))\\ &=x^3+o(x^3)  \end{align} we plug those values in $e^{f(x)}$ \begin{align} e^{f(x)}&=1+\left(x-x^3+o(x^3) \right)+\dfrac{\left(x-x^3+o(x^3) \right)^2}{2}+\dfrac{\left(x-x^3+o(x^3) \right)^3}{6}+o(x^3)\\ &=1+\left(x-x^3+o(x^3) \right)+\dfrac{ x^{2}+o(x^3)}{2}+\dfrac{x^3+o(x^3)}{6}+o(x^3)\\ e^{f(x)}&=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3) \end{align} then \begin{align} e^{f(x)}-e^{x}&=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-e^{x}\\ &=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-(1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}+o(x^3))\\ &=1+x+\dfrac{x^2}{2}+\dfrac{x^3}{6}-x^3+o(x^3)-1-x-\dfrac{x^2}{2}-\dfrac{x^3}{6}+o(x^3))\\ e^{f(x)}-e^{x}&=-x^{3}+o(x^3) \end{align} $$\fbox{$e^{f(x)}-e^{x}=-x^{3}+o(x^3)$}$$ \begin{align} \sin(f(2x))&=\left(2x-8x^3+o(x^3)\right)-\dfrac{\left(2x-8x^3+o(x^3)\right)^3}{6}+o(x^3)\\ \end{align} or  $\left(2x-8x^3+o(x^3)\right)^3=(8x^{3}\left(1-4x^{2}\right)+o(x^3)=8x^{3}+o(x^3) $ then \begin{align} \sin(f(2x))&=\left(2x-8x^3+o(x^3)\right)-\dfrac{\left(2x-8x^3+o(x^3)\right)^3}{6}+o(x^3)\\ &=\left(2x-8x^3+o(x^3)\right)-\dfrac{8x^{3}+o(x^3)}{6}+o(x^3)\\ &=2x-8x^3-\dfrac{8x^{3}}{6}+o(x^3)\\ &=2x-\dfrac{28x^{3}}{3}+o(x^3)\\ \end{align} $$\fbox{$2x-\sin(f(2x))=\dfrac{28x^{3}}{3}+o(x^3))$}$$ \begin{align} \dfrac{e^{f(x)}-e^x}{2x-\sin\left(f(2x)\right)}&=\dfrac{-x^{3}+o(x^3)}{\dfrac{28x^{3}}{3}+o(x^3)}=-\dfrac{-3}{28}+o(x^3)\\ \end{align} $$\lim\limits_{x\to 0}\dfrac{e^{f(x)}-e^x}{2x-\sin\left( f(2x) \right)}=\dfrac{-3}{28}$$ Is my proof correct Please if you find any mistake try to correct it with details of calculations",,"['calculus', 'limits', 'power-series', 'taylor-expansion', 'solution-verification']"
79,Limit of $2^{(\log_2 n)^2}/2^{(\log_2 n)^3}$,Limit of,2^{(\log_2 n)^2}/2^{(\log_2 n)^3},I am trying to find the following limit $$\lim_{n \to \infty} \frac{2^{(\log_2 n)^2}} {2^{(\log_2 n)^3}}$$ I really don't know where to start and any help would be appreciated!,I am trying to find the following limit $$\lim_{n \to \infty} \frac{2^{(\log_2 n)^2}} {2^{(\log_2 n)^3}}$$ I really don't know where to start and any help would be appreciated!,,"['limits', 'logarithms']"
80,evaluating some limits with $\ln(x)$,evaluating some limits with,\ln(x),I don't understand how to prove these results. $\lim\limits_{x \to +\infty}\dfrac{\ln{x}}{x} = 0$ $\lim\limits_{x \to 0^{+}}x\ln{x} = 0$,I don't understand how to prove these results. $\lim\limits_{x \to +\infty}\dfrac{\ln{x}}{x} = 0$ $\lim\limits_{x \to 0^{+}}x\ln{x} = 0$,,['calculus']
81,Proving an equivalent definition of the $\lim_{x\to a}f(x)$ exists [duplicate],Proving an equivalent definition of the  exists [duplicate],\lim_{x\to a}f(x),"This question already has an answer here : proof that an alternative definition of limit is equivalent to the usual one (1 answer) Closed 8 years ago . Prove that the following statements are equivalent. (a) $\lim_{x\to a}f(x)$ exists (b) Given $\epsilon \gt 0$, there is a $\delta \gt 0$ such that if $0\lt |x-a| \lt \delta, 0\lt |y-a| \lt \delta$, then $|f(x)-f(y)|\lt \epsilon.$ (a) $\to$ (b) is easy. But I'm having trouble showing $(b)\to (a)$, actually I'm not even sure if this direction is true. My initial thought was that I can prove this by the contrapositive. For some $\epsilon \gt 0$, suppose the limit does not exist at $a$. Then there are some sequences $x_n$, $y_n$ such that they both tend to $a$, but are never $a$, however, $\lim f(x_n)\neq \lim f(y_n)$. Then since $\lim x_n=\lim y_n=a$, we can choose a large enough $N$ such that if $n\ge N$, then $|x_n-a|\lt \delta, |y_n-a|\lt \delta$. Then we get $|f(x_n)-f(y_n)|\gt 0$. However, I realized that I cannot prove that such $x_n$ and $y_n$ exist in the first place. From definition, the function $f$ does not have a limit at $a$ if and only if there exists a sequence $x_n$ with $x_n\neq a$ for all $n\in \mathbb{N}$ such that the sequence $x_n$ converges to $a$ but the sequence $f(x_n)$ does not converge in $\mathbb{R}$. This does not mean that I can find two sequences with different limits from the function values. How can I solve this problem? I would greatly appreciate any help.","This question already has an answer here : proof that an alternative definition of limit is equivalent to the usual one (1 answer) Closed 8 years ago . Prove that the following statements are equivalent. (a) $\lim_{x\to a}f(x)$ exists (b) Given $\epsilon \gt 0$, there is a $\delta \gt 0$ such that if $0\lt |x-a| \lt \delta, 0\lt |y-a| \lt \delta$, then $|f(x)-f(y)|\lt \epsilon.$ (a) $\to$ (b) is easy. But I'm having trouble showing $(b)\to (a)$, actually I'm not even sure if this direction is true. My initial thought was that I can prove this by the contrapositive. For some $\epsilon \gt 0$, suppose the limit does not exist at $a$. Then there are some sequences $x_n$, $y_n$ such that they both tend to $a$, but are never $a$, however, $\lim f(x_n)\neq \lim f(y_n)$. Then since $\lim x_n=\lim y_n=a$, we can choose a large enough $N$ such that if $n\ge N$, then $|x_n-a|\lt \delta, |y_n-a|\lt \delta$. Then we get $|f(x_n)-f(y_n)|\gt 0$. However, I realized that I cannot prove that such $x_n$ and $y_n$ exist in the first place. From definition, the function $f$ does not have a limit at $a$ if and only if there exists a sequence $x_n$ with $x_n\neq a$ for all $n\in \mathbb{N}$ such that the sequence $x_n$ converges to $a$ but the sequence $f(x_n)$ does not converge in $\mathbb{R}$. This does not mean that I can find two sequences with different limits from the function values. How can I solve this problem? I would greatly appreciate any help.",,"['calculus', 'real-analysis', 'analysis', 'limits']"
82,Find product limit of this recursively-defined sequence?,Find product limit of this recursively-defined sequence?,,"Problem： if $a_1=3$, $a_n=2a_{n-1}^2-1$, $n\ge2$, find the limit of this expression:   $$\lim\limits_{n \to ∞} \prod\limits_{k=1}^{n-1} (1+\frac {1}{a_k})$$ The original problem asks to find this: $$\lim\limits_{n \to ∞} \frac {a_n}{2^na_1a_2.....a_{n-1}}$$ The solution uses relation $a_n^2-1=2a_{n-1}^2×2(a_{n-1}^2-1)$ recursively and find out that the limit is $\sqrt2$;   while I believe there got to be some other ways to do this. So I start by finding a simpler form of relation from expression $a_n=2a_{n-1}^2-1$ which is $a_n-1=2(a_{n-1}+1)(a_{n-1}-1)$, and finally I come to the product expression and obviously got stuck, I just don't have enough weaponry to deal with problems like this, trust me I always stumble on this type... How should one correctly use the recursive relation to gradually approach the result? Is it mostly intuition or experience? And what are the first thing you tried when you first see this problem?","Problem： if $a_1=3$, $a_n=2a_{n-1}^2-1$, $n\ge2$, find the limit of this expression:   $$\lim\limits_{n \to ∞} \prod\limits_{k=1}^{n-1} (1+\frac {1}{a_k})$$ The original problem asks to find this: $$\lim\limits_{n \to ∞} \frac {a_n}{2^na_1a_2.....a_{n-1}}$$ The solution uses relation $a_n^2-1=2a_{n-1}^2×2(a_{n-1}^2-1)$ recursively and find out that the limit is $\sqrt2$;   while I believe there got to be some other ways to do this. So I start by finding a simpler form of relation from expression $a_n=2a_{n-1}^2-1$ which is $a_n-1=2(a_{n-1}+1)(a_{n-1}-1)$, and finally I come to the product expression and obviously got stuck, I just don't have enough weaponry to deal with problems like this, trust me I always stumble on this type... How should one correctly use the recursive relation to gradually approach the result? Is it mostly intuition or experience? And what are the first thing you tried when you first see this problem?",,"['sequences-and-series', 'limits', 'recurrence-relations']"
83,"Evaluate $ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} $",Evaluate," \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} ","$$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x +  \frac{y^2}{2} \right )}{x^2+y^2} $$ I've a few doubts about this limit. I mean, if I take polar coordinates, I get that the limit doesn't exist. And Wolfram agrees with me. Even though, I've found a solution of this problem that doesn't say the same thing; which I transcribe next: · The Taylor's second order polynomial of $ e^{x+y^2} $ in $ (0,0) $ is $ 1 + x + \frac{1}{2}x^2 + y^2 $. · The Taylor's second order polynomial of $ \sin \left ( x + \frac{y^2}{2} \right ) $ in $ (0,0) $ is $ x + \frac{1}{2}y^2 $. Then: $$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} = \lim_{(x,y)\to(0,0)} \frac {1 + x + \frac{1}{2}x^2 + y^2-1-(x + \frac{1}{2}y^2)}{x^2+y^2} = \frac {1}{2} $$ Where's the mistake in this? What I've tried so far: $$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} $$ Let be $ \rho \geq 0 $ and $ \varphi \in [0,2\pi) $, so: $$ \left\{\begin{matrix} x = \rho\cos(\varphi)\\  y = \rho\sin(\varphi) \end{matrix}\right. $$ Then: $$ \begin{align*} \lim_{\rho\to 0} \frac {e^{\rho\cos(\varphi)+(\rho\sin(\varphi))^2}-1-\sin \left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{(\rho\cos(\varphi))^2+(\rho\sin(\varphi))^2} &= \lim_{\rho\to 0} \frac {\rho\cos(\varphi)+(\rho\sin(\varphi))^2-\left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{(\rho\cos(\varphi))^2+(\rho\sin(\varphi))^2} \\   &= \lim_{\rho\to 0} \frac {\rho\cos(\varphi)+(\rho\sin(\varphi))^2-\left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{\rho^2} \\   &= \lim_{\rho\to 0} \frac {(\rho\sin(\varphi))^2- \frac{(\rho\sin(\varphi))^2}{2}}{\rho^2} \\   &= \lim_{\rho\to 0} \frac {1}{2}\sin^2(\varphi) \end{align*} $$ Which limit doesn't exit, because the result depends of $ \varphi $ which varies in $ [0,2\pi) $. And here you can see that Wolfram agrees. According to a comment below, I've calculated a third order Taylor's polynomial (respecto to $ \rho$) of $ e^{\rho\cos(\varphi)+(\rho\sin(\varphi))^2} $. I found that any Taylor's polynomial (in $ \rho = 0 $) of order grater than 2 is exactly $ 1+\rho\cos(\varphi)+(\rho\sin(\varphi))^2 $.","$$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x +  \frac{y^2}{2} \right )}{x^2+y^2} $$ I've a few doubts about this limit. I mean, if I take polar coordinates, I get that the limit doesn't exist. And Wolfram agrees with me. Even though, I've found a solution of this problem that doesn't say the same thing; which I transcribe next: · The Taylor's second order polynomial of $ e^{x+y^2} $ in $ (0,0) $ is $ 1 + x + \frac{1}{2}x^2 + y^2 $. · The Taylor's second order polynomial of $ \sin \left ( x + \frac{y^2}{2} \right ) $ in $ (0,0) $ is $ x + \frac{1}{2}y^2 $. Then: $$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} = \lim_{(x,y)\to(0,0)} \frac {1 + x + \frac{1}{2}x^2 + y^2-1-(x + \frac{1}{2}y^2)}{x^2+y^2} = \frac {1}{2} $$ Where's the mistake in this? What I've tried so far: $$ \lim_{(x,y)\to(0,0)} \frac {e^{x+y^2}-1-\sin \left ( x + \frac{y^2}{2} \right )}{x^2+y^2} $$ Let be $ \rho \geq 0 $ and $ \varphi \in [0,2\pi) $, so: $$ \left\{\begin{matrix} x = \rho\cos(\varphi)\\  y = \rho\sin(\varphi) \end{matrix}\right. $$ Then: $$ \begin{align*} \lim_{\rho\to 0} \frac {e^{\rho\cos(\varphi)+(\rho\sin(\varphi))^2}-1-\sin \left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{(\rho\cos(\varphi))^2+(\rho\sin(\varphi))^2} &= \lim_{\rho\to 0} \frac {\rho\cos(\varphi)+(\rho\sin(\varphi))^2-\left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{(\rho\cos(\varphi))^2+(\rho\sin(\varphi))^2} \\   &= \lim_{\rho\to 0} \frac {\rho\cos(\varphi)+(\rho\sin(\varphi))^2-\left ( \rho\cos(\varphi) + \frac{(\rho\sin(\varphi))^2}{2} \right )}{\rho^2} \\   &= \lim_{\rho\to 0} \frac {(\rho\sin(\varphi))^2- \frac{(\rho\sin(\varphi))^2}{2}}{\rho^2} \\   &= \lim_{\rho\to 0} \frac {1}{2}\sin^2(\varphi) \end{align*} $$ Which limit doesn't exit, because the result depends of $ \varphi $ which varies in $ [0,2\pi) $. And here you can see that Wolfram agrees. According to a comment below, I've calculated a third order Taylor's polynomial (respecto to $ \rho$) of $ e^{\rho\cos(\varphi)+(\rho\sin(\varphi))^2} $. I found that any Taylor's polynomial (in $ \rho = 0 $) of order grater than 2 is exactly $ 1+\rho\cos(\varphi)+(\rho\sin(\varphi))^2 $.",,['limits']
84,Iteration with bounded converging sequence as input,Iteration with bounded converging sequence as input,,"Consider the sequence of vectors $( x_k )_{k=0}^{\infty}$ such that $x_k \in X \subset \mathbb{R}^n$ for all $k$, where $X$ is compact, and $x_k \rightarrow \bar{x} \in X$. Consider a vector $y_0 \in Y$ and a Lipschitz continuous function $f : \mathbb{R}^n \times \mathbb{R}^n \rightarrow Y \subset \mathbb{R}^n$, where $Y$ is compact, with the following property. For all $z \in X$, there exists $y \in Y$ such that the sequence of vectors $\left( y_{k+1}:= f(y_k, z) \right)_{k=0}^{\infty}$ is such that $y_k \rightarrow y$. Prove or disprove the following statement:  $$\exists \bar{y} \in Y : \ y_{k+1} := f( y_k, x_k ) \rightarrow \bar{y} \in Y$$ Comments: I have tried to take $\bar{k}$ large enough, so that $x_k \in \bar{x} + \epsilon B$ for all $k \geq \bar{k}$. But then I am not sure how to exploit the Lipschitz continuity of $f$ to claim that $y_k$ stays close to $f(\bar{y},\bar{x})$. Clearly, if the sequence $(y_k)_k$ converges to some $\bar{y} \in Y$, then $\bar{y} = f(\bar{y}, \bar{x})$.","Consider the sequence of vectors $( x_k )_{k=0}^{\infty}$ such that $x_k \in X \subset \mathbb{R}^n$ for all $k$, where $X$ is compact, and $x_k \rightarrow \bar{x} \in X$. Consider a vector $y_0 \in Y$ and a Lipschitz continuous function $f : \mathbb{R}^n \times \mathbb{R}^n \rightarrow Y \subset \mathbb{R}^n$, where $Y$ is compact, with the following property. For all $z \in X$, there exists $y \in Y$ such that the sequence of vectors $\left( y_{k+1}:= f(y_k, z) \right)_{k=0}^{\infty}$ is such that $y_k \rightarrow y$. Prove or disprove the following statement:  $$\exists \bar{y} \in Y : \ y_{k+1} := f( y_k, x_k ) \rightarrow \bar{y} \in Y$$ Comments: I have tried to take $\bar{k}$ large enough, so that $x_k \in \bar{x} + \epsilon B$ for all $k \geq \bar{k}$. But then I am not sure how to exploit the Lipschitz continuity of $f$ to claim that $y_k$ stays close to $f(\bar{y},\bar{x})$. Clearly, if the sequence $(y_k)_k$ converges to some $\bar{y} \in Y$, then $\bar{y} = f(\bar{y}, \bar{x})$.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'lipschitz-functions']"
85,Need help with taylor series.,Need help with taylor series.,,Evaluate the limit   $$\lim\limits_{x \to 1} \frac{1-x + \ln x}{1+ \cos πx}$$ The limit im trying to get is $-\frac{1}{π^2}$ as I've solved from l'Hopitals rule. Now I need to solve the limit by using Taylor Series and this is what i did so far $$\begin{align*} f(x) &= 1-x + \ln x = 1 -x + (x-1) + \frac{1}{2} (x-1)^2 + \frac{1}{3} (x-1)^3 - \frac{1}{4} (x-1)^4 + \ldots \\ g(x) &= 1+\cos πx = 1+\left[ 1+\frac{1}{2!} (πx)^2 + \frac{1}{4!} (πx)^4 - \frac{1}{6!} (πx)^6  +\ldots \right] \\ \frac {f(x)}{g(x)} & = \frac{\frac{1}{2} (x-1)^2 + \frac{1}{3} (x-1)^3 - \frac{1}{4}(x-1)^4 + \ldots} {2-\frac{1}{2!} (πx)^2 + \frac{1}{4!} (πx)^4 - \frac{1}{6!} (πx)^6+\ldots} \end{align*}$$ I have no idea where to go to solve for $-\frac{1}{π^2}$ now. Please help,Evaluate the limit   $$\lim\limits_{x \to 1} \frac{1-x + \ln x}{1+ \cos πx}$$ The limit im trying to get is $-\frac{1}{π^2}$ as I've solved from l'Hopitals rule. Now I need to solve the limit by using Taylor Series and this is what i did so far $$\begin{align*} f(x) &= 1-x + \ln x = 1 -x + (x-1) + \frac{1}{2} (x-1)^2 + \frac{1}{3} (x-1)^3 - \frac{1}{4} (x-1)^4 + \ldots \\ g(x) &= 1+\cos πx = 1+\left[ 1+\frac{1}{2!} (πx)^2 + \frac{1}{4!} (πx)^4 - \frac{1}{6!} (πx)^6  +\ldots \right] \\ \frac {f(x)}{g(x)} & = \frac{\frac{1}{2} (x-1)^2 + \frac{1}{3} (x-1)^3 - \frac{1}{4}(x-1)^4 + \ldots} {2-\frac{1}{2!} (πx)^2 + \frac{1}{4!} (πx)^4 - \frac{1}{6!} (πx)^6+\ldots} \end{align*}$$ I have no idea where to go to solve for $-\frac{1}{π^2}$ now. Please help,,"['limits', 'taylor-expansion', 'limits-without-lhopital']"
86,Limit and Integral problem work verification-2,Limit and Integral problem work verification-2,,"I have to calculate the following: $$\large\lim_{x \to \infty}\left(\frac {\displaystyle\int\limits_{x^{2}}^{2x}t^{4}e^{t^{2}}dt}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)$$ My attempt: Let $F(x)=\displaystyle\int\limits_0^xt^4e^{t^2}dt$. Then, $$\large\lim_{x\to\infty}\left(\frac{\displaystyle\int\limits_{x^{2}}^{2x}t^{4}e^{t^{2}}dt}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)=\lim_{x \to \infty}\left(\frac {F(2x) - F(x^2)}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)$$ Applying L'Hôpital's rule, we have, $$\large\begin{align}\lim_{x \to \infty}\left(\frac {32x^4e^{4x^2} - 2x^9e^{x^4}}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}}\right) &= \lim_{x \to \infty}(32x^4e^{4x^2-x} - 2x^9e^{x^4-x}) \\&= \lim_{x \to \infty}\bigg(2x^4e^{4x^2-x}(16-x^5e^{x^4-4x^2})\bigg) = -\infty\end{align}$$ Am I right?","I have to calculate the following: $$\large\lim_{x \to \infty}\left(\frac {\displaystyle\int\limits_{x^{2}}^{2x}t^{4}e^{t^{2}}dt}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)$$ My attempt: Let $F(x)=\displaystyle\int\limits_0^xt^4e^{t^2}dt$. Then, $$\large\lim_{x\to\infty}\left(\frac{\displaystyle\int\limits_{x^{2}}^{2x}t^{4}e^{t^{2}}dt}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)=\lim_{x \to \infty}\left(\frac {F(2x) - F(x^2)}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}-\frac{x^4}{24}}\right)$$ Applying L'Hôpital's rule, we have, $$\large\begin{align}\lim_{x \to \infty}\left(\frac {32x^4e^{4x^2} - 2x^9e^{x^4}}{e^{x}-1-x - \frac{x^2}{2}- \frac{x^3}{6}}\right) &= \lim_{x \to \infty}(32x^4e^{4x^2-x} - 2x^9e^{x^4-x}) \\&= \lim_{x \to \infty}\bigg(2x^4e^{4x^2-x}(16-x^5e^{x^4-4x^2})\bigg) = -\infty\end{align}$$ Am I right?",,"['integration', 'limits']"
87,limit with two variable,limit with two variable,,"How to calculate: $$ \lim_{(x,y) \to (0,0)}        \frac{5x^6 + y^2}{x^3 + 2y} $$ I think the result should be $0$, but how do I prove it? I tried by the definition, but I could not resolve that. I cannot use the different paths to prove that, because there are an infinity of paths and I would have to calculate them all, and that's impossible.  I believe the only way is by definition.","How to calculate: $$ \lim_{(x,y) \to (0,0)}        \frac{5x^6 + y^2}{x^3 + 2y} $$ I think the result should be $0$, but how do I prove it? I tried by the definition, but I could not resolve that. I cannot use the different paths to prove that, because there are an infinity of paths and I would have to calculate them all, and that's impossible.  I believe the only way is by definition.",,['limits']
88,"If $\limsup_n\sqrt[n]{a_n}=\frac{1}{r}$, then $\limsup_n\sqrt[n]{(n+1)a_{n+1}}=?$","If , then",\limsup_n\sqrt[n]{a_n}=\frac{1}{r} \limsup_n\sqrt[n]{(n+1)a_{n+1}}=?,"If $\limsup_n\sqrt[n]{|a_n|}=\frac{1}{r}$, then $\limsup_n\sqrt[n]{|(n+1)a_{n+1}|}=?$ Can I separate the product of the second limit as $$\limsup_n\sqrt[n]{|(n+1)a_{n+1}|}=\limsup_n\sqrt[n]{(n+1)}\limsup_n\sqrt[n]{|a_{n+1}|}$$since the first limit exists and $\sqrt[n]{(n+1)}$ is bounded, if not does the RHS majorize LHS, so that I deduce the limit is also $\frac{1}{r}$ (Original Exercise: If $\sum_na_nz^n$ has radius of convergence $r$, so has $\sum_n na_nz^{n-1}$)","If $\limsup_n\sqrt[n]{|a_n|}=\frac{1}{r}$, then $\limsup_n\sqrt[n]{|(n+1)a_{n+1}|}=?$ Can I separate the product of the second limit as $$\limsup_n\sqrt[n]{|(n+1)a_{n+1}|}=\limsup_n\sqrt[n]{(n+1)}\limsup_n\sqrt[n]{|a_{n+1}|}$$since the first limit exists and $\sqrt[n]{(n+1)}$ is bounded, if not does the RHS majorize LHS, so that I deduce the limit is also $\frac{1}{r}$ (Original Exercise: If $\sum_na_nz^n$ has radius of convergence $r$, so has $\sum_n na_nz^{n-1}$)",,"['sequences-and-series', 'limits', 'limsup-and-liminf']"
89,Proof of boundedness of a function,Proof of boundedness of a function,,Let $|x|<1$ and $f(x)=\displaystyle\frac{e^{\frac{1}{1+x}}}{(|x|-1)^{-2}}.$ Is $f(x)$ bounded?,Let $|x|<1$ and $f(x)=\displaystyle\frac{e^{\frac{1}{1+x}}}{(|x|-1)^{-2}}.$ Is $f(x)$ bounded?,,"['calculus', 'real-analysis', 'limits']"
90,Precise limit definition,Precise limit definition,,Prove: $$\displaystyle \lim_{x \to 2} \frac 1x = \frac 12.$$ We need to find a $\delta$ in terms of $\epsilon$. Here is what I did so far: \begin{align} \left|\frac 1x-\frac 12 \right| &< \epsilon \\ -\epsilon < \frac 1x-\frac 12 &< \epsilon \\ -\epsilon + \frac 12< \frac 1x&< \epsilon + \frac 12 \\ \frac 2{1+2\epsilon} < x &< \frac 1{1-2\epsilon} \end{align} I am having trouble with this last step. Did I do this correctly?,Prove: $$\displaystyle \lim_{x \to 2} \frac 1x = \frac 12.$$ We need to find a $\delta$ in terms of $\epsilon$. Here is what I did so far: \begin{align} \left|\frac 1x-\frac 12 \right| &< \epsilon \\ -\epsilon < \frac 1x-\frac 12 &< \epsilon \\ -\epsilon + \frac 12< \frac 1x&< \epsilon + \frac 12 \\ \frac 2{1+2\epsilon} < x &< \frac 1{1-2\epsilon} \end{align} I am having trouble with this last step. Did I do this correctly?,,"['calculus', 'limits', 'epsilon-delta']"
91,how to show existence of limit? [closed],how to show existence of limit? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I'm working some problems, and the questions states; ""decide if a limit exists. If it exists, find it"". But how am I supposed to do this? The tools I have at my disposal are the limit definition and what the limit is when we combine functions with known limits. It's a function in two variables.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I'm working some problems, and the questions states; ""decide if a limit exists. If it exists, find it"". But how am I supposed to do this? The tools I have at my disposal are the limit definition and what the limit is when we combine functions with known limits. It's a function in two variables.",,['limits']
92,How can I prove a limit is infinity?,How can I prove a limit is infinity?,,"How can I prove that the $\lim \limits_{x \to 1^+} \frac{x^2}{x-1}=\infty$ using the $\epsilon -\delta$ definition of a limit? I think I start with $\forall$M>0, want $\delta$>0. After that I'm not sure.","How can I prove that the $\lim \limits_{x \to 1^+} \frac{x^2}{x-1}=\infty$ using the $\epsilon -\delta$ definition of a limit? I think I start with $\forall$M>0, want $\delta$>0. After that I'm not sure.",,"['calculus', 'limits', 'epsilon-delta']"
93,Limit of a sequence of a function,Limit of a sequence of a function,,"Let, $f_{n}(x)=\dfrac{1}{n} \sum_{k=0}^{n} \sqrt {k(n-k)}{n\choose k}x^{k}(1-x)^{n-k}$ for $x\in [0,1],n=0,1,2,...$ If $\displaystyle \lim_{n\to \infty}f_{n}(x)=f(x)$ for $x\in [0,1]$, then the maximum value of $f(x)$ on $[0,1]$ is (A) 1 (B) $\dfrac{1}{2}$ (C) $\dfrac{1}{3}$ (D) $\dfrac{1}{4}$ I want to find the value of $\displaystyle \lim_{n\to \infty}f_{n}(x)$ but I can't find it. I only know that $\sum_{k=0}^{n}{n\choose k}x^{k}(1-x)^{n-k}=1$. But I can't express this sum separately.","Let, $f_{n}(x)=\dfrac{1}{n} \sum_{k=0}^{n} \sqrt {k(n-k)}{n\choose k}x^{k}(1-x)^{n-k}$ for $x\in [0,1],n=0,1,2,...$ If $\displaystyle \lim_{n\to \infty}f_{n}(x)=f(x)$ for $x\in [0,1]$, then the maximum value of $f(x)$ on $[0,1]$ is (A) 1 (B) $\dfrac{1}{2}$ (C) $\dfrac{1}{3}$ (D) $\dfrac{1}{4}$ I want to find the value of $\displaystyle \lim_{n\to \infty}f_{n}(x)$ but I can't find it. I only know that $\sum_{k=0}^{n}{n\choose k}x^{k}(1-x)^{n-k}=1$. But I can't express this sum separately.",,"['real-analysis', 'sequences-and-series', 'limits']"
94,Showing that $g$ is continuous by showing that a series converges?,Showing that  is continuous by showing that a series converges?,g,"Define $g:\Bbb{R}\setminus\Bbb{Z}\to\Bbb{R}$ by   $$g(x)=\dfrac{1}{x}+\sum\limits_{n=1}^\infty\dfrac{2x}{x^2-n^2}.$$Show   that $g$ is continuous. $g$ can be rewritten as $$g(x)=\dfrac{1}{x}+\sum\limits_{n=1}^\infty\dfrac{1}{n+x}-\dfrac{1}{n-x}.$$Let $\epsilon>0$. Now, our job is to find a $\delta>0$ such that whenever $\left|x-y\right|<\delta$, $$\left|\dfrac{1}{x}-\dfrac{1}{y}+\sum\left(\dfrac{1}{n+x}-\dfrac{1}{n+y}+\dfrac{1}{n-y}-\dfrac{1}{n-x}\right)\right|<\epsilon.$$ That is; $$\left|\dfrac{y-x}{xy}+(y-x)\sum\left(\dfrac{1}{(n-y)(n-x)}-\dfrac{1}{(n+x)(n+y)}\right)\right|<\epsilon.$$ I believe I need to find the limit of that last sum, but I don't know how. Or at least I should find an upper bound $A$ so that I can find a $\delta$ such that $\delta<\dfrac{\epsilon}{AB}$ where $\dfrac{1}{xy}<B$. What should I do?","Define $g:\Bbb{R}\setminus\Bbb{Z}\to\Bbb{R}$ by   $$g(x)=\dfrac{1}{x}+\sum\limits_{n=1}^\infty\dfrac{2x}{x^2-n^2}.$$Show   that $g$ is continuous. $g$ can be rewritten as $$g(x)=\dfrac{1}{x}+\sum\limits_{n=1}^\infty\dfrac{1}{n+x}-\dfrac{1}{n-x}.$$Let $\epsilon>0$. Now, our job is to find a $\delta>0$ such that whenever $\left|x-y\right|<\delta$, $$\left|\dfrac{1}{x}-\dfrac{1}{y}+\sum\left(\dfrac{1}{n+x}-\dfrac{1}{n+y}+\dfrac{1}{n-y}-\dfrac{1}{n-x}\right)\right|<\epsilon.$$ That is; $$\left|\dfrac{y-x}{xy}+(y-x)\sum\left(\dfrac{1}{(n-y)(n-x)}-\dfrac{1}{(n+x)(n+y)}\right)\right|<\epsilon.$$ I believe I need to find the limit of that last sum, but I don't know how. Or at least I should find an upper bound $A$ so that I can find a $\delta$ such that $\delta<\dfrac{\epsilon}{AB}$ where $\dfrac{1}{xy}<B$. What should I do?",,"['real-analysis', 'sequences-and-series', 'limits', 'continuity']"
95,Find the first two non vanishing maclaurin terms,Find the first two non vanishing maclaurin terms,,"Find the first two nonvanishing terms in the Maclaurin series of $\sin(x + x^3)$. Suggestion: use the Maclaurin series of $\sin(y)$ and write $y = x + x^3$ Using this result, find $\lim\limits_{x\to 0}\frac{\sin(x + x^3)−x}{x^3}$ $\sin(y)= y-\frac{y^3}{3!}+\frac{y^5}{5!}+\frac{y^7}{7!}$ $y= x+x^3$ $x+x^3-\frac{(x+x^3)^3}{3!}+\frac{(x+x^3)^5}{5!}-\frac{(x+x^3)^7}{7!}$ Now what do I do?","Find the first two nonvanishing terms in the Maclaurin series of $\sin(x + x^3)$. Suggestion: use the Maclaurin series of $\sin(y)$ and write $y = x + x^3$ Using this result, find $\lim\limits_{x\to 0}\frac{\sin(x + x^3)−x}{x^3}$ $\sin(y)= y-\frac{y^3}{3!}+\frac{y^5}{5!}+\frac{y^7}{7!}$ $y= x+x^3$ $x+x^3-\frac{(x+x^3)^3}{3!}+\frac{(x+x^3)^5}{5!}-\frac{(x+x^3)^7}{7!}$ Now what do I do?",,"['limits', 'taylor-expansion']"
96,Limit of $(1-2^{-x})^x$,Limit of,(1-2^{-x})^x,"I am observing that $(1-2^{-x})^x \to 1$ as $x \to \infty$, but am having trouble proving this. Why does the $-x$ ""beat"" the $x$? I thought of maybe considering that $$1-(1-2^{-n})^n = 2^{-n}(1+2^{-n}+2^{-2n}+\cdots+2^{-n(n-1)})\le2^{-n} \frac{1}{1-2^{-n}} \to 0,$$ as $n \to \infty$ and then noting that $(1-2^{-x})^x$ is continuous to go from integers $n$ to any real $x$. Is there a more elegant solution, or any better intuition? Moreover, it seems that $(1-2^{-ax})^{bx}\to 1$ holds as well for any $a,b>0$.","I am observing that $(1-2^{-x})^x \to 1$ as $x \to \infty$, but am having trouble proving this. Why does the $-x$ ""beat"" the $x$? I thought of maybe considering that $$1-(1-2^{-n})^n = 2^{-n}(1+2^{-n}+2^{-2n}+\cdots+2^{-n(n-1)})\le2^{-n} \frac{1}{1-2^{-n}} \to 0,$$ as $n \to \infty$ and then noting that $(1-2^{-x})^x$ is continuous to go from integers $n$ to any real $x$. Is there a more elegant solution, or any better intuition? Moreover, it seems that $(1-2^{-ax})^{bx}\to 1$ holds as well for any $a,b>0$.",,"['limits', 'convergence-divergence']"
97,Find derivative of $f(x)=\frac{1}{\sqrt{x+2}}$ by definition,Find derivative of  by definition,f(x)=\frac{1}{\sqrt{x+2}},Use the definition of a derivative to find the derivative of: $$f(x)=\frac{1}{\sqrt{x+2}}+2x$$ my work: $$\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ $$\lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}+2(x+h)-\frac{1}{\sqrt{x+2}}-2x}{h}$$ $$\lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}+2h-\frac{1}{\sqrt{x+2}}}{h}$$ $$\lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}-\frac{1}{\sqrt{x+2}}}{h}+2$$ $$\lim_{h\to 0}\frac{1}{h}\frac{\sqrt{x+2}-\sqrt{x+h+2}}{\sqrt{x+h+2}\sqrt{x+2}}+2$$ I don't know what to do from here.,Use the definition of a derivative to find the derivative of: my work: I don't know what to do from here.,f(x)=\frac{1}{\sqrt{x+2}}+2x \lim_{h\to0}\frac{f(x+h)-f(x)}{h} \lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}+2(x+h)-\frac{1}{\sqrt{x+2}}-2x}{h} \lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}+2h-\frac{1}{\sqrt{x+2}}}{h} \lim_{h\to0}\frac{\frac{1}{\sqrt{x+h+2}}-\frac{1}{\sqrt{x+2}}}{h}+2 \lim_{h\to 0}\frac{1}{h}\frac{\sqrt{x+2}-\sqrt{x+h+2}}{\sqrt{x+h+2}\sqrt{x+2}}+2,"['calculus', 'limits', 'derivatives', 'radicals']"
98,Calculation of the limit for a composite function,Calculation of the limit for a composite function,,"Let's assume that we have a composite function $g(f(x))$, which is not defined at $f(x)=0$. We know that $\lim_{x \to 0} f(x) =0$. Further, we know that $\lim_{f(x) \to 0}g(f(x))=L $. $f(x)$ is a well behaving, continuous function; $g(y)$ is similar, only it is just not defined at $y=f(x)=0$. I wonder that if it is possible to show that it is $\lim_{x \to 0}g(f(x))  = L$. I don't know exactly whether it is even possible to evaluate a limit with a dependent ($f(x)$ here, with $\lim_{f(x) \to 0}g(f(x))=L$) variable, but it looks like so intuitive that I thought it can be shown in a rigorous way. Here is what I have tried so far: First, we fix $\epsilon > 0$; we know that there is a $\delta$ such that it is $|g(f(x))-L| < \epsilon$, whenever $0<|f(x)|<\delta$. We pick such a $\delta$. Now I must be able to show that for that $\delta$, there are $x$ values such that $0 < |x| < \delta' \implies 0<|f(x)| <\delta$ . Since we know $\lim_{x \to 0} f(x)=0$, due to the definition of this limit, such a $\delta'$ always exists. Is my proof valid? If not, how this problem can be proven or disproven?  Thanks in advance.","Let's assume that we have a composite function $g(f(x))$, which is not defined at $f(x)=0$. We know that $\lim_{x \to 0} f(x) =0$. Further, we know that $\lim_{f(x) \to 0}g(f(x))=L $. $f(x)$ is a well behaving, continuous function; $g(y)$ is similar, only it is just not defined at $y=f(x)=0$. I wonder that if it is possible to show that it is $\lim_{x \to 0}g(f(x))  = L$. I don't know exactly whether it is even possible to evaluate a limit with a dependent ($f(x)$ here, with $\lim_{f(x) \to 0}g(f(x))=L$) variable, but it looks like so intuitive that I thought it can be shown in a rigorous way. Here is what I have tried so far: First, we fix $\epsilon > 0$; we know that there is a $\delta$ such that it is $|g(f(x))-L| < \epsilon$, whenever $0<|f(x)|<\delta$. We pick such a $\delta$. Now I must be able to show that for that $\delta$, there are $x$ values such that $0 < |x| < \delta' \implies 0<|f(x)| <\delta$ . Since we know $\lim_{x \to 0} f(x)=0$, due to the definition of this limit, such a $\delta'$ always exists. Is my proof valid? If not, how this problem can be proven or disproven?  Thanks in advance.",,"['calculus', 'real-analysis', 'limits']"
99,"Find the value of $f_{xy}$ at the point (0,0)","Find the value of  at the point (0,0)",f_{xy},"Let f be the function defined for all (x,y) as follows: $f(x,y)= \begin{cases} \frac{xy(x^2-y^2)}{x^2+y^2}, &\text{if }(x,y)\ne(0,0)\\ 0, &\text{if }(x,y)=(0,0) \end{cases}$ What is the value of $\frac{\partial^2(f)}{\partial x\partial y}$ at $(0,0)$? My work so far has been, if we do the normal calculation, $\partial f/\partial x= \dfrac{x^4y-y^5+4x^2*y^3}{(x^2+y^2)^2}$ and then differentiate the result with respect to y, and we got, $\dfrac{x^6+13x^4*y^2+27x^2*y^4-9y^6}{(x^2+y^2)^3}$ but I don't know how to proceed. Before going this way, I am thinking of polar coordinates: $x=r\cos a$, $y=r\sin a$, then $f(x,y) = f(r\cos a, r\sin a) = 1/2  \sin(4a)*r^2$ and $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial r} \cdot \frac{\partial r}{\partial x} = 4r\sin(a)\cos(2a) = g$, and then differential this result with respect to y = rsin(a), I got $\partial g/\partial y = \partial g/\partial r \cdot \partial r/\partial y=4\cos(2a)$, so when when r, a goes to 0, the result should be 4? The solution says -1. Any ideas?","Let f be the function defined for all (x,y) as follows: $f(x,y)= \begin{cases} \frac{xy(x^2-y^2)}{x^2+y^2}, &\text{if }(x,y)\ne(0,0)\\ 0, &\text{if }(x,y)=(0,0) \end{cases}$ What is the value of $\frac{\partial^2(f)}{\partial x\partial y}$ at $(0,0)$? My work so far has been, if we do the normal calculation, $\partial f/\partial x= \dfrac{x^4y-y^5+4x^2*y^3}{(x^2+y^2)^2}$ and then differentiate the result with respect to y, and we got, $\dfrac{x^6+13x^4*y^2+27x^2*y^4-9y^6}{(x^2+y^2)^3}$ but I don't know how to proceed. Before going this way, I am thinking of polar coordinates: $x=r\cos a$, $y=r\sin a$, then $f(x,y) = f(r\cos a, r\sin a) = 1/2  \sin(4a)*r^2$ and $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial r} \cdot \frac{\partial r}{\partial x} = 4r\sin(a)\cos(2a) = g$, and then differential this result with respect to y = rsin(a), I got $\partial g/\partial y = \partial g/\partial r \cdot \partial r/\partial y=4\cos(2a)$, so when when r, a goes to 0, the result should be 4? The solution says -1. Any ideas?",,"['limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
