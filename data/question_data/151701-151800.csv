,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Interchanging supremum and integral (another question),Interchanging supremum and integral (another question),,"Suppose that $\varphi$ is a smooth $\mathcal{K}_{\infty}$-function and $\bar{\textbf{B}}$ is the unit ball set in $\mathbb{R}^{n}$. Let $x : \mathbb{R}_{\geq 0} \times \mathbb{R}^{n} \to \mathbb{R}^{n}$ be such that $x(.,\xi)$ is differentiable for each $\xi$ and $x(t,.)$ is uniformly locally Lipschitz for each $t$. Also let $\omega : \mathbb{R}^{n} \to \mathbb{R}_{\geq 0}$ be a continuous function on $\mathbb{R}^{n}$ with $\omega(0) = 0$ and $\omega(x) \to \infty$ as $x \to \infty$. I'd like to know that whether the following holds $$ \sup_{\xi \in \bar{\textbf{B}}} \int_{s=0}^{s=t}{\varphi(\omega(x(s,\xi)))ds} = \int_{s=0}^{s=t}{\sup_{\xi \in \bar{\textbf{B}}} \varphi(\omega(x(s,\xi)))ds}. $$ In other words, are the supremum and integral signs interchangable?","Suppose that $\varphi$ is a smooth $\mathcal{K}_{\infty}$-function and $\bar{\textbf{B}}$ is the unit ball set in $\mathbb{R}^{n}$. Let $x : \mathbb{R}_{\geq 0} \times \mathbb{R}^{n} \to \mathbb{R}^{n}$ be such that $x(.,\xi)$ is differentiable for each $\xi$ and $x(t,.)$ is uniformly locally Lipschitz for each $t$. Also let $\omega : \mathbb{R}^{n} \to \mathbb{R}_{\geq 0}$ be a continuous function on $\mathbb{R}^{n}$ with $\omega(0) = 0$ and $\omega(x) \to \infty$ as $x \to \infty$. I'd like to know that whether the following holds $$ \sup_{\xi \in \bar{\textbf{B}}} \int_{s=0}^{s=t}{\varphi(\omega(x(s,\xi)))ds} = \int_{s=0}^{s=t}{\sup_{\xi \in \bar{\textbf{B}}} \varphi(\omega(x(s,\xi)))ds}. $$ In other words, are the supremum and integral signs interchangable?",,"['real-analysis', 'analysis', 'measure-theory', 'integration']"
1,Hilbert basis of vector space,Hilbert basis of vector space,,"Let $V$ a vector space with inner product and $X\subset V$ orthonormal. Prove that exists a Hilbert basis (an orthonormal set of vectors with the property that every vector in $V$ can be written as an infinite linear combination of the vectors in the basis) such that $X\subset B$. I can consider $B=X\cup X^{\perp}$ and this set will be maximal, but I am not sure about this, is it correct? Another idea is using the Zorn lemma, but I need a ""chain"", how can I build this chain? Thanks for your help.","Let $V$ a vector space with inner product and $X\subset V$ orthonormal. Prove that exists a Hilbert basis (an orthonormal set of vectors with the property that every vector in $V$ can be written as an infinite linear combination of the vectors in the basis) such that $X\subset B$. I can consider $B=X\cup X^{\perp}$ and this set will be maximal, but I am not sure about this, is it correct? Another idea is using the Zorn lemma, but I need a ""chain"", how can I build this chain? Thanks for your help.",,"['linear-algebra', 'analysis', 'functional-analysis']"
2,"Closed form solution to $\{a_n\}_{n=1}^{\infty} = 1,2,2,3,3,3,...$",Closed form solution to,"\{a_n\}_{n=1}^{\infty} = 1,2,2,3,3,3,...",I had thought about this sequence (where each positive integer $n$ shows up $n$ times) the other day and think I have a closed form solution. First of all we know that the last time that $k$ shows up in the sequence is at $a_{\frac{k(k+1)}{2}}$.  We want to see if  $a_n$ is one of these last occurrences. So we solve $$\frac{k(k+1)}{2} = n$$ for $k$. If $k$ is an integer then we know $a_n = k$ but if $k$ is not an integer then $a_n$ must be $\left \lceil k \right \rceil$ since the last occurrence of $\left \lfloor k \right \rfloor$ occurred before $n$. Therefore the value of  $a_n$ is: $$\left \lceil \frac{\sqrt{8n+1} -1}{2}\right \rceil$$ Is this correct? Is there anything interesting about this sequence other that that the sum of it values and the sum of its reciprocals both diverge?  Thanks.,I had thought about this sequence (where each positive integer $n$ shows up $n$ times) the other day and think I have a closed form solution. First of all we know that the last time that $k$ shows up in the sequence is at $a_{\frac{k(k+1)}{2}}$.  We want to see if  $a_n$ is one of these last occurrences. So we solve $$\frac{k(k+1)}{2} = n$$ for $k$. If $k$ is an integer then we know $a_n = k$ but if $k$ is not an integer then $a_n$ must be $\left \lceil k \right \rceil$ since the last occurrence of $\left \lfloor k \right \rfloor$ occurred before $n$. Therefore the value of  $a_n$ is: $$\left \lceil \frac{\sqrt{8n+1} -1}{2}\right \rceil$$ Is this correct? Is there anything interesting about this sequence other that that the sum of it values and the sum of its reciprocals both diverge?  Thanks.,,['analysis']
3,"Example of a function f such that $f^2$ is Riemann-Stieltjes integrable on [a,b] but f is not. [duplicate]","Example of a function f such that  is Riemann-Stieltjes integrable on [a,b] but f is not. [duplicate]",f^2,"This question already has answers here : Closed 11 years ago . Possible Duplicate: If $f^2$ is Riemann Integrable is $f$ always Riemann Integrable? Example of a function f such that $f^2$ is Riemann-Stieltjes integrable on [a,b] but f is not. I was considering f=$(\chi_{\mathbb{Q}}-\chi_{\mathbb{I}})$ the difference of the characteristic functions for the rationals and irrationals respectively.  I know that this is not Riemann Stieltjes integrable but I'm wondering if $f^2$ is. $f^2$=$(\chi_{\mathbb{Q}}-\chi_{\mathbb{I}})^2$=$\chi_{\mathbb{Q}}^2 +\chi_{\mathbb{I}}^2 - 2\chi_{\mathbb{I}}\chi_{\mathbb{Q}}$ = $\chi_{\mathbb{Q}} + \chi_{\mathbb{I}}$ $\equiv 1$ on [a,b]. Is a correct way of viewing this?","This question already has answers here : Closed 11 years ago . Possible Duplicate: If $f^2$ is Riemann Integrable is $f$ always Riemann Integrable? Example of a function f such that $f^2$ is Riemann-Stieltjes integrable on [a,b] but f is not. I was considering f=$(\chi_{\mathbb{Q}}-\chi_{\mathbb{I}})$ the difference of the characteristic functions for the rationals and irrationals respectively.  I know that this is not Riemann Stieltjes integrable but I'm wondering if $f^2$ is. $f^2$=$(\chi_{\mathbb{Q}}-\chi_{\mathbb{I}})^2$=$\chi_{\mathbb{Q}}^2 +\chi_{\mathbb{I}}^2 - 2\chi_{\mathbb{I}}\chi_{\mathbb{Q}}$ = $\chi_{\mathbb{Q}} + \chi_{\mathbb{I}}$ $\equiv 1$ on [a,b]. Is a correct way of viewing this?",,"['real-analysis', 'analysis', 'integration', 'functions']"
4,"Cosine function is decreasing on $(0,\pi)$",Cosine function is decreasing on,"(0,\pi)","How to prove that cosine function is decreasing on the interval $(0,\pi)$ if we are allowed to use only the definition of cosine through exponential function (or Taylor series)?","How to prove that cosine function is decreasing on the interval $(0,\pi)$ if we are allowed to use only the definition of cosine through exponential function (or Taylor series)?",,['analysis']
5,Let $f$ be analytic on $D = \{z \in \Bbb{C} : |z| < 1\}$ and $f(0) = 0$.,Let  be analytic on  and .,f D = \{z \in \Bbb{C} : |z| < 1\} f(0) = 0,"I was stuck on the following problem: Let $f$ be analytic on $D = \{z \in \Bbb{C} : |z| < 1\}$ and $f(0) = 0$. Define   $$g(z) = \begin{cases} \displaystyle \frac{f(z)}{z} & z \neq 0, \\ f'(0) & z = 0. \end{cases}$$ Then which of the following option(s) is/are correct? $g$ is discontinuous at $z = 0$ for all $f$. $g$ is continuous, but not analytic, at $z = 0$ for all $f$. $g$ is analytic at $z = 0$ for all $f$. $g$ is analytic at $z = 0$ only if $f'(0) = 0$. Can someone point me in the right direction with some explanation? Thanks in advance for your time. EDIT: I have posted an answer .Feel free to  comment if I missed anything in my answer.","I was stuck on the following problem: Let $f$ be analytic on $D = \{z \in \Bbb{C} : |z| < 1\}$ and $f(0) = 0$. Define   $$g(z) = \begin{cases} \displaystyle \frac{f(z)}{z} & z \neq 0, \\ f'(0) & z = 0. \end{cases}$$ Then which of the following option(s) is/are correct? $g$ is discontinuous at $z = 0$ for all $f$. $g$ is continuous, but not analytic, at $z = 0$ for all $f$. $g$ is analytic at $z = 0$ for all $f$. $g$ is analytic at $z = 0$ only if $f'(0) = 0$. Can someone point me in the right direction with some explanation? Thanks in advance for your time. EDIT: I have posted an answer .Feel free to  comment if I missed anything in my answer.",,"['complex-analysis', 'analysis']"
6,Showing that a certain function is a local diffeomorphism,Showing that a certain function is a local diffeomorphism,,"I have to show that $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2: (x,y) \mapsto (e^x(x \cos y - y \sin y),e^x(x \sin y + y \cos y)$ is a local diffeomorphism in ever point not $(-1,0)$. I have no idea how to invert $f$ on some restricted domain. My guess is that we have to restrict the domain, jut say cos and sin have inverses. However, I have no idea what I could pick as a viable inverse? Since taking logarithms, dividing the result and multiplying or squaring just makes the result more complicated. Could anyone give me a hint?","I have to show that $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2: (x,y) \mapsto (e^x(x \cos y - y \sin y),e^x(x \sin y + y \cos y)$ is a local diffeomorphism in ever point not $(-1,0)$. I have no idea how to invert $f$ on some restricted domain. My guess is that we have to restrict the domain, jut say cos and sin have inverses. However, I have no idea what I could pick as a viable inverse? Since taking logarithms, dividing the result and multiplying or squaring just makes the result more complicated. Could anyone give me a hint?",,['analysis']
7,Someone know a specific maximum principle...? [Solved],Someone know a specific maximum principle...? [Solved],,"I need a maximum principle in unbounded domains: if $u$ is a solution, bounded in $\Omega$, satisfying $$\Delta u+c(x)u=0, \ \ in \ \Omega,$$ $c\in L^\infty$, $$u\leq0 \ \ in \ \Omega$$ $$u(x_0)=0, \ \ x_0\in\Omega$$ Then  $$u\equiv0 \ \ in \ \Omega$$ Someone know where I can find this statement?","I need a maximum principle in unbounded domains: if $u$ is a solution, bounded in $\Omega$, satisfying $$\Delta u+c(x)u=0, \ \ in \ \Omega,$$ $c\in L^\infty$, $$u\leq0 \ \ in \ \Omega$$ $$u(x_0)=0, \ \ x_0\in\Omega$$ Then  $$u\equiv0 \ \ in \ \Omega$$ Someone know where I can find this statement?",,"['analysis', 'partial-differential-equations']"
8,Convergence of Lebesgue integrable functions in an arbitrary measure.,Convergence of Lebesgue integrable functions in an arbitrary measure.,,"I'm a bit stuck on this problem, and I was hoping someone could point me in the right direction. Suppose $f, f_1, f_2,\ldots \in L^{1}(\Omega,A,\mu)$ , and further suppose that $\lim_{n \to \infty} \int_{\Omega} |f-f_n| \, d\mu = 0$. Show that $f_n \rightarrow f$ in measure $\mu$. In case you aren't sure, $L^1(\Omega,A,\mu)$ is the complex Lebesgue integrable functions on $\Omega$ with measure $\mu$. I believe I have to use the Dominated convergence theorem to get this result, and they usually do some trick like taking a new function $g$ that relates to $f$ and $f_n$ in some way, but I'm not seeing it. Any advice?","I'm a bit stuck on this problem, and I was hoping someone could point me in the right direction. Suppose $f, f_1, f_2,\ldots \in L^{1}(\Omega,A,\mu)$ , and further suppose that $\lim_{n \to \infty} \int_{\Omega} |f-f_n| \, d\mu = 0$. Show that $f_n \rightarrow f$ in measure $\mu$. In case you aren't sure, $L^1(\Omega,A,\mu)$ is the complex Lebesgue integrable functions on $\Omega$ with measure $\mu$. I believe I have to use the Dominated convergence theorem to get this result, and they usually do some trick like taking a new function $g$ that relates to $f$ and $f_n$ in some way, but I'm not seeing it. Any advice?",,"['analysis', 'measure-theory']"
9,Help with norm definition,Help with norm definition,,"I'd like to show that the following defines a norm on $\mathbb C^n$: $||x||=(a_1^2+a_2^2)^{1/2}$ Where $x=(x_1,..,x_n)$, $a_1$ is the maximum of the $|x_i|$'s and $a_2$ the second maximum. But I have problems with the triangle inequality axiom... Hope somebody could help!","I'd like to show that the following defines a norm on $\mathbb C^n$: $||x||=(a_1^2+a_2^2)^{1/2}$ Where $x=(x_1,..,x_n)$, $a_1$ is the maximum of the $|x_i|$'s and $a_2$ the second maximum. But I have problems with the triangle inequality axiom... Hope somebody could help!",,['analysis']
10,Function extending a Lipschitz function,Function extending a Lipschitz function,,"Let $X$ be a metric space with a metric $d$, let $E\subset X$.  We have a function $f:E \rightarrow \mathbb R$ satisfying for some $M>0$: $$ |f(x)-f(y)|\leq M d(x,y) \quad \text{for } x,y \in E. $$ I wish to show that a function  $$ F(x)=\sup_{y \in E} [f(y)-M d(x,y)]  $$ for  $x \in X$, is finite (that is $F: X \rightarrow \mathbb R$).","Let $X$ be a metric space with a metric $d$, let $E\subset X$.  We have a function $f:E \rightarrow \mathbb R$ satisfying for some $M>0$: $$ |f(x)-f(y)|\leq M d(x,y) \quad \text{for } x,y \in E. $$ I wish to show that a function  $$ F(x)=\sup_{y \in E} [f(y)-M d(x,y)]  $$ for  $x \in X$, is finite (that is $F: X \rightarrow \mathbb R$).",,"['analysis', 'metric-spaces', 'holder-spaces']"
11,About Cauchy product of some sequences,About Cauchy product of some sequences,,"Assume that we have two sequences $(a_n)_{n \in \mathbb Z}, (b_n)_{n \in \mathbb Z}$ such  that for each $l\in \mathbb N $ the sequence $\left(|n|^l a_n\right)_{n \in \mathbb Z}$ is bounded, there exists $s \in \mathbb N$ such that $\displaystyle\sum_{n \in \mathbb Z} \frac{|b_n|^2}{(1+n^2)^s}< \infty$. Let $\displaystyle c_n=\sum_{k \in \mathbb Z} a_k b_{n-k}$ for $n \in \mathbb Z$.  Is it then true that $$\sum_{n \in \mathbb Z} |c_n|^2 <\infty ?$$","Assume that we have two sequences $(a_n)_{n \in \mathbb Z}, (b_n)_{n \in \mathbb Z}$ such  that for each $l\in \mathbb N $ the sequence $\left(|n|^l a_n\right)_{n \in \mathbb Z}$ is bounded, there exists $s \in \mathbb N$ such that $\displaystyle\sum_{n \in \mathbb Z} \frac{|b_n|^2}{(1+n^2)^s}< \infty$. Let $\displaystyle c_n=\sum_{k \in \mathbb Z} a_k b_{n-k}$ for $n \in \mathbb Z$.  Is it then true that $$\sum_{n \in \mathbb Z} |c_n|^2 <\infty ?$$",,['analysis']
12,Calculating digamma and trigamma functions,Calculating digamma and trigamma functions,,"What is the best method to calculate the value of digamma and trigamma functions? Wikipedia suggests using recurrence relations $\psi_0(x+1) = \psi_0(x) + 1/x$, $\quad\psi_1(x+1) = \psi_1(x) - 1/x^2$ to make $x$ big enough, and then evaluating corresponding power series up to a certain term $\psi_0(x) = \ln x - 1/2x - \sum\limits_{n=1}^{\infty} \dfrac{B_{2n}}{2n \cdot x^{2n}}$ $\psi_1(x) = 1/x + 1/2x^2 + \sum\limits_{n=1}^{\infty} \dfrac{B_{2n}}{x^{2n+1}}$ However, these series do not converge absolutely (moreover, $n$'th term tends to infinity, since $B_{2n}$ grows very fast), and I wasn't able to find any way to estimate residuals and relative errors.","What is the best method to calculate the value of digamma and trigamma functions? Wikipedia suggests using recurrence relations $\psi_0(x+1) = \psi_0(x) + 1/x$, $\quad\psi_1(x+1) = \psi_1(x) - 1/x^2$ to make $x$ big enough, and then evaluating corresponding power series up to a certain term $\psi_0(x) = \ln x - 1/2x - \sum\limits_{n=1}^{\infty} \dfrac{B_{2n}}{2n \cdot x^{2n}}$ $\psi_1(x) = 1/x + 1/2x^2 + \sum\limits_{n=1}^{\infty} \dfrac{B_{2n}}{x^{2n+1}}$ However, these series do not converge absolutely (moreover, $n$'th term tends to infinity, since $B_{2n}$ grows very fast), and I wasn't able to find any way to estimate residuals and relative errors.",,"['sequences-and-series', 'analysis', 'special-functions']"
13,2D basis functions orthogonal under exponential kernel,2D basis functions orthogonal under exponential kernel,,"In one dimension, the Laguerre polynomials are orthogonal under exponential weighting: $$ \int_0^\infty L_n(x) L_m(x) e^{-x} \, dx = 0, n \ne m $$ Does anyone know what the corresponding basis functions would be in 2 dimensions? $$ \int_{-\infty}^\infty \int_{-\infty}^\infty F_n(x,y) F_m(x,y) e^{-r} \, dx \, dy = 0, r= \sqrt{x^2+y^2}, n \ne m $$ The Zernike polynomials are orthogonal, but with uniform weight and over the unit disk. The underlying problem is to compute an estimator for a missing pixel.  A series of orthogonal functions are helpful since you can then incrementally compute a 1-st order estimator, then a second order, then a third order, and so on.  The exponential arises since (in natural scenes anyway) presumably pixels further away have less influence.","In one dimension, the Laguerre polynomials are orthogonal under exponential weighting: $$ \int_0^\infty L_n(x) L_m(x) e^{-x} \, dx = 0, n \ne m $$ Does anyone know what the corresponding basis functions would be in 2 dimensions? $$ \int_{-\infty}^\infty \int_{-\infty}^\infty F_n(x,y) F_m(x,y) e^{-r} \, dx \, dy = 0, r= \sqrt{x^2+y^2}, n \ne m $$ The Zernike polynomials are orthogonal, but with uniform weight and over the unit disk. The underlying problem is to compute an estimator for a missing pixel.  A series of orthogonal functions are helpful since you can then incrementally compute a 1-st order estimator, then a second order, then a third order, and so on.  The exponential arises since (in natural scenes anyway) presumably pixels further away have less influence.",,['analysis']
14,Same Morse coordinates for different Morse functions,Same Morse coordinates for different Morse functions,,"Let $f,g\in C^\infty(\mathbb R^n;\mathbb R)$ be two Morse functions having both a critical point at $0$. Is it always possible to find local coordinates around $0$ such that both $f $ and $g$ become quadratic in the new coordinates? After the comment of Matt, I realized that i forgot an important assumption: $0$ is a critical point of index $0$ of $f$.","Let $f,g\in C^\infty(\mathbb R^n;\mathbb R)$ be two Morse functions having both a critical point at $0$. Is it always possible to find local coordinates around $0$ such that both $f $ and $g$ become quadratic in the new coordinates? After the comment of Matt, I realized that i forgot an important assumption: $0$ is a critical point of index $0$ of $f$.",,"['linear-algebra', 'analysis', 'multivariable-calculus']"
15,Cauchy sequences when $p$ is a function,Cauchy sequences when  is a function,p,"Consider $p$ being a positive bounded and measurable function and $\{f_k\}$ a sequence satisfying $$\int_{R^d} |f_k(x)|^{p(x)}dx<\infty$$ and $$\lim_{m,j\to \infty}\int_{R^d} |f_j(x)-f_m(x)|^{p(x)}dx=0$$ then there is a $f$ such that $$\int_{R^d} |f(x)|^{p(x)}dx<\infty$$ and $$\lim_{m\to \infty}\int_{R^d} |f(x)-f_m(x)|^{p(x)}dx=0.$$ I proved the result for $p$ being a step function but I couldn't extend the result to the general case. Other way was to use a mimesis of that $L^p$ is complete.","Consider $p$ being a positive bounded and measurable function and $\{f_k\}$ a sequence satisfying $$\int_{R^d} |f_k(x)|^{p(x)}dx<\infty$$ and $$\lim_{m,j\to \infty}\int_{R^d} |f_j(x)-f_m(x)|^{p(x)}dx=0$$ then there is a $f$ such that $$\int_{R^d} |f(x)|^{p(x)}dx<\infty$$ and $$\lim_{m\to \infty}\int_{R^d} |f(x)-f_m(x)|^{p(x)}dx=0.$$ I proved the result for $p$ being a step function but I couldn't extend the result to the general case. Other way was to use a mimesis of that $L^p$ is complete.",,"['analysis', 'measure-theory', 'integration', 'convergence-divergence']"
16,Derivative of an indicator inside an integral,Derivative of an indicator inside an integral,,"I have a fairly basic question that relates to understanding a particular derivation. I have the following function $Q(x) = E\left[I(F(x+\varepsilon)>c)\right]$, where $x \in R$, $\varepsilon \sim N(0,\sigma^2)$ for a known $\sigma>0$, $c$ is a scalar constant, $F(\cdot)$ is a known, bounded function, and $I(\cdot)$ is an indicator function. This can be written as: $E[I(F(x+\varepsilon)>c)] = \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(x+\varepsilon)>c) \exp\left(-\frac{\varepsilon^2}{2\sigma^2} \right)}d\varepsilon $ Let $y = x + \varepsilon$ and rewrite the above as: $E[I(F(x+\varepsilon)>c)] = \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(y)>c) \exp\left(-\frac{(y-x)^2}{2\sigma^2} \right)}dy$ I am interested in calculating $\frac{\partial Q(x)}{\partial x}$. The derivative is: $\frac{\partial E[I(F(x+\varepsilon)>c)]}{\partial x}= \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(y)>c) \exp\left(-\frac{(y-x)^2}{2\sigma^2} \right)} \frac{(y-x)}{\sigma^2}dy \quad \quad $   (1) Substituting back in again and rearranging slightly: $\frac{\partial E[I(F(x+\varepsilon)>c)]}{\partial x} = \frac{1}{\sigma^2} E[I(F(x+\varepsilon)>c)\varepsilon]$ Finally, here are my two questions:  (1) is the above derivation correct? (2) If so, I am trying to understand why in eq(1) I do not need to take the derivative of $I(F(y)>c)$ with respect to $x$. I know there has been a change of variables, but $y$ is obviously a function of $x$. Yet I was told that ""once you do the substitution, $y=x+\varepsilon$, then y is just a dummy variable (i.e. an index) which runs from -infinity to infinity, and one cannot take the derivative in a dummy variable."" I know you can't take the derivative of an indicator function, but somehow I can't get this clear in my head. Thanks. More broadly, in my actual problem, $x$ and $\varepsilon$ are high dimensional and I am trying to compute the derivative via Monte Carlo integration....","I have a fairly basic question that relates to understanding a particular derivation. I have the following function $Q(x) = E\left[I(F(x+\varepsilon)>c)\right]$, where $x \in R$, $\varepsilon \sim N(0,\sigma^2)$ for a known $\sigma>0$, $c$ is a scalar constant, $F(\cdot)$ is a known, bounded function, and $I(\cdot)$ is an indicator function. This can be written as: $E[I(F(x+\varepsilon)>c)] = \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(x+\varepsilon)>c) \exp\left(-\frac{\varepsilon^2}{2\sigma^2} \right)}d\varepsilon $ Let $y = x + \varepsilon$ and rewrite the above as: $E[I(F(x+\varepsilon)>c)] = \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(y)>c) \exp\left(-\frac{(y-x)^2}{2\sigma^2} \right)}dy$ I am interested in calculating $\frac{\partial Q(x)}{\partial x}$. The derivative is: $\frac{\partial E[I(F(x+\varepsilon)>c)]}{\partial x}= \frac{1}{{\sqrt{2\pi}}\sigma} \int{I(F(y)>c) \exp\left(-\frac{(y-x)^2}{2\sigma^2} \right)} \frac{(y-x)}{\sigma^2}dy \quad \quad $   (1) Substituting back in again and rearranging slightly: $\frac{\partial E[I(F(x+\varepsilon)>c)]}{\partial x} = \frac{1}{\sigma^2} E[I(F(x+\varepsilon)>c)\varepsilon]$ Finally, here are my two questions:  (1) is the above derivation correct? (2) If so, I am trying to understand why in eq(1) I do not need to take the derivative of $I(F(y)>c)$ with respect to $x$. I know there has been a change of variables, but $y$ is obviously a function of $x$. Yet I was told that ""once you do the substitution, $y=x+\varepsilon$, then y is just a dummy variable (i.e. an index) which runs from -infinity to infinity, and one cannot take the derivative in a dummy variable."" I know you can't take the derivative of an indicator function, but somehow I can't get this clear in my head. Thanks. More broadly, in my actual problem, $x$ and $\varepsilon$ are high dimensional and I am trying to compute the derivative via Monte Carlo integration....",,"['calculus', 'probability', 'analysis', 'functions']"
17,problem in measure theory,problem in measure theory,,"I would a appreciate if someone could take the time to check if my solution to the following problem is correct: From http://www.math.chalmers.se/~borell/MeasureTheory.pdf , page 64, ex.6. Let $(X, \cal{M}, \mu)$ be a positive measure space and suppose $f$ and $g$ are non-negative measurable functions such that $$ \int_{A} fd\mu = \int_A gd\mu,\quad \text{all } A \in \cal{M}. $$ $(a).$ Prove that $f = g$ a.e. $[\mu]$ if $\mu$ is $\sigma$ -finite. $(b).$ Prove that the conclusion of Part $(a)$ may fail if $\mu$ is not $\sigma$ -finite. Consider the set where $f > g.$ Denote this set $A.$ But $A=\cup_n[A_n]$ where $$A_n=\{x:f(x)>g(x)+1/n\},$$ a strictly increasing sequence hence $\mu(A)=\lim_n \mu(A_n).$ So if $\mu(A)>0,$ there is a $N$ such that $\mu(A_N)>0$ Since $X$ is $\sigma$ -finite $X=\cup_m\{X_m\}$ where $\mu(X_m)< \infty.$ If $\mu(A_N)>0$ then there must exist an $M$ s.t. $\mu(\cap{A_N,X_M})>0.$ and hence integral_intersection $\{A_N,X_M\} {f} \ge$ integral_intersection $\{A_N,X_M\} {g} + 1/N\mu(\cap\{A_N,X_M\}),$ a contradiction unless the left-hand side is infinite. In that case consider $C_n=\{x: g(x) < n \}.$ Choose $M$ s.t. $\mu(A_M,X_M,C_M) > 0,$ now integrate over this set instead to arrive at the desired contradiction. Hence $\mu(A)=0$ The same applies to the set $B=\{x:g(x)>f(x)\}, \mu(B)=0.$","I would a appreciate if someone could take the time to check if my solution to the following problem is correct: From http://www.math.chalmers.se/~borell/MeasureTheory.pdf , page 64, ex.6. Let be a positive measure space and suppose and are non-negative measurable functions such that Prove that a.e. if is -finite. Prove that the conclusion of Part may fail if is not -finite. Consider the set where Denote this set But where a strictly increasing sequence hence So if there is a such that Since is -finite where If then there must exist an s.t. and hence integral_intersection integral_intersection a contradiction unless the left-hand side is infinite. In that case consider Choose s.t. now integrate over this set instead to arrive at the desired contradiction. Hence The same applies to the set","(X, \cal{M}, \mu) f g  \int_{A} fd\mu = \int_A gd\mu,\quad \text{all } A \in \cal{M}.  (a). f = g [\mu] \mu \sigma (b). (a) \mu \sigma f > g. A. A=\cup_n[A_n] A_n=\{x:f(x)>g(x)+1/n\}, \mu(A)=\lim_n \mu(A_n). \mu(A)>0, N \mu(A_N)>0 X \sigma X=\cup_m\{X_m\} \mu(X_m)< \infty. \mu(A_N)>0 M \mu(\cap{A_N,X_M})>0. \{A_N,X_M\} {f} \ge \{A_N,X_M\} {g} + 1/N\mu(\cap\{A_N,X_M\}), C_n=\{x: g(x) < n \}. M \mu(A_M,X_M,C_M) > 0, \mu(A)=0 B=\{x:g(x)>f(x)\}, \mu(B)=0.","['analysis', 'measure-theory']"
18,Matrix inversion of an analytical function,Matrix inversion of an analytical function,,"Following problem: I have a function $f(x_1,x_2)$ and Im looking for the inverse $finv(x_1,x_2)$ of the function which is defined through: $\int f(x_1,y)\cdot finv(y,x_2) d y =\delta(x_1,x_2) $ where $\delta(x_1,x_2) $ is the Dirac delta function. When I'm trying to discretize $f$ and then to invert as usual matrix I get numerically bad results. I mean the result is ok, but I need extremely fine discretization. I'm sure there should be some other method than the poor man inversion. Something like inversion with weights or similar. Probably there exist already a c++ library for such problem.","Following problem: I have a function $f(x_1,x_2)$ and Im looking for the inverse $finv(x_1,x_2)$ of the function which is defined through: $\int f(x_1,y)\cdot finv(y,x_2) d y =\delta(x_1,x_2) $ where $\delta(x_1,x_2) $ is the Dirac delta function. When I'm trying to discretize $f$ and then to invert as usual matrix I get numerically bad results. I mean the result is ok, but I need extremely fine discretization. I'm sure there should be some other method than the poor man inversion. Something like inversion with weights or similar. Probably there exist already a c++ library for such problem.",,"['linear-algebra', 'analysis', 'functional-analysis', 'numerical-linear-algebra', 'integral-equations']"
19,Radius of convergence of the inverse of a power series,Radius of convergence of the inverse of a power series,,Let $a = \sum_k a_k X^k \in \mathbb C [\![ X ]\!]$ with $a_0 = 1$ and convergence radius $\rho_a > 0$. I want to show that the convergence radius of the inverse $b = \sum_k b_k X^k \in \mathbb C [\![ X ]\!]$ is also greater than 0. How can I do that?,Let $a = \sum_k a_k X^k \in \mathbb C [\![ X ]\!]$ with $a_0 = 1$ and convergence radius $\rho_a > 0$. I want to show that the convergence radius of the inverse $b = \sum_k b_k X^k \in \mathbb C [\![ X ]\!]$ is also greater than 0. How can I do that?,,"['calculus', 'analysis']"
20,Smoothness and Hessian of convex conjugate/Legendre transform,Smoothness and Hessian of convex conjugate/Legendre transform,,"Let $p:[0,\infty) \times \mathbb{R}^n \to \mathbb{R}$ be a smooth convex function for all $x \in \mathbb{R}^n$. We define its Legendre transform (or convex conjugate)  as $$ p^*(t,y)=\sup_{x \in \mathbb{R^n}}\left\{ x \cdot y - p(t,x)\right\} $$ I know differentiability and convexity are dual concepts, so convexity of $p$ implies differentiability of $p^*$. However, I should be able to get smoothness of $p^*$ and Hessian $D^2_y p^*$ bounded away from zero and $+\infty$. How do I do this? Thanks in advance for any helpful comments, ideas, etc.","Let $p:[0,\infty) \times \mathbb{R}^n \to \mathbb{R}$ be a smooth convex function for all $x \in \mathbb{R}^n$. We define its Legendre transform (or convex conjugate)  as $$ p^*(t,y)=\sup_{x \in \mathbb{R^n}}\left\{ x \cdot y - p(t,x)\right\} $$ I know differentiability and convexity are dual concepts, so convexity of $p$ implies differentiability of $p^*$. However, I should be able to get smoothness of $p^*$ and Hessian $D^2_y p^*$ bounded away from zero and $+\infty$. How do I do this? Thanks in advance for any helpful comments, ideas, etc.",,"['analysis', 'convex-analysis']"
21,"To prove $f(x)\rightarrow \infty$ with a ""home made"" strategy","To prove  with a ""home made"" strategy",f(x)\rightarrow \infty,"My goal is to prove that: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} \rightarrow \infty$ for all $ x\rightarrow 1^+$ In order to show this statement I show that no matter how big you choose $N\in \mathbb{R}$, you can always find a $\delta >0$ so $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0>N$ when $x\in \left]1,1+\delta\right[$ My idea is to use some kind of ""induction"". First/the start : N = 1.25 If we look at the case where $N = 1.25$ I conclude that i can just choose $\delta = 1$ so the following statement is true: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0> 1.25$ for all $x\in \left]1,2\right[$ The ""induction""-step I now assume that the following statement is true for an $N$: There exists a $\delta>0$ so $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0>N$ for all $x\in \left]1,1+\delta\right[$ Now I multiply the above equation with $x$ ($x$ is greater than $1$) $\displaystyle x+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots +0>xN$ for all $x\in \left]1,1+\delta\right[$ Then I do some magic and get from the above: $\displaystyle 1+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots+0>xN-x+1$ for all $x\in \left]1,1+\delta\right[$ I now see that following is true: $xN-x+1 > N$ Because I dont need to go below $N=1.25$ and can therefore do the following calculations: $xN-x+1 = x(N-1)+1> N \Leftrightarrow \\ x(N-1)>N-1 \Leftrightarrow \\ x>1$ From all this I can finally conclude that: $\displaystyle 1+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ I now write the above as: $1+\dfrac{1}{2^{x-\frac{\ln(x)}{\ln(2)}}}+\dfrac{1}{3^{x-\frac{\ln(x)}{\ln(3)}}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ Because I can use the following true statement: $\dfrac{1}{x} = a^{-\frac{\ln(x)}{\ln(a)}}$ From the above I can get a new statement: $1+\dfrac{1}{2^{x-\frac{\ln(x)}{\ln(2)}}}+\dfrac{1}{3^{x-\frac{\ln(x)}{\ln(2)}}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ I substitue $y = x-\frac{\ln(x)}{\ln(2)}$ and get: $1+\dfrac{1}{2^{y}}+\dfrac{1}{3^{y}}+\cdots+0>xN-x+1>N$ This is true for all $y\in \left]1,1+\delta_{1}\right[$ where $\delta_{1}>0$ It is true, because: for $1+\delta>x>1$ I can get $2x-2>\ln(x) \Leftrightarrow x-1>\dfrac{\ln(x)}{2} \implies y=x-\dfrac{\ln(x)}{2}>1$ and because $x$ is limited by $1+\delta$ I can find a new smaller $\delta_{2}>0$ so $1+\dfrac{1}{2^{y}}+\dfrac{1}{3^{y}}+\cdots+0>xN-x+1>N$ is true for all $y\in \left]1,1+\delta_{2}\right[$ Finally Now I'm done. If I choose a $N\in \mathbb{R}$ I can go iteratively from 1.25 to a value above $N$. My question My question is very simple. I know it's long (and ""ugly""), but does it looks right? The strategy is ""homemade"", so I'm a bit insecure.","My goal is to prove that: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} \rightarrow \infty$ for all $ x\rightarrow 1^+$ In order to show this statement I show that no matter how big you choose $N\in \mathbb{R}$, you can always find a $\delta >0$ so $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0>N$ when $x\in \left]1,1+\delta\right[$ My idea is to use some kind of ""induction"". First/the start : N = 1.25 If we look at the case where $N = 1.25$ I conclude that i can just choose $\delta = 1$ so the following statement is true: $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0> 1.25$ for all $x\in \left]1,2\right[$ The ""induction""-step I now assume that the following statement is true for an $N$: There exists a $\delta>0$ so $\displaystyle\sum\limits_{n=1}^\infty \frac{1}{n^{x}} = 1+\frac{1}{2^{x}}+\cdots+0>N$ for all $x\in \left]1,1+\delta\right[$ Now I multiply the above equation with $x$ ($x$ is greater than $1$) $\displaystyle x+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots +0>xN$ for all $x\in \left]1,1+\delta\right[$ Then I do some magic and get from the above: $\displaystyle 1+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots+0>xN-x+1$ for all $x\in \left]1,1+\delta\right[$ I now see that following is true: $xN-x+1 > N$ Because I dont need to go below $N=1.25$ and can therefore do the following calculations: $xN-x+1 = x(N-1)+1> N \Leftrightarrow \\ x(N-1)>N-1 \Leftrightarrow \\ x>1$ From all this I can finally conclude that: $\displaystyle 1+\frac{x}{2^{x}}+\frac{x}{3^{x}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ I now write the above as: $1+\dfrac{1}{2^{x-\frac{\ln(x)}{\ln(2)}}}+\dfrac{1}{3^{x-\frac{\ln(x)}{\ln(3)}}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ Because I can use the following true statement: $\dfrac{1}{x} = a^{-\frac{\ln(x)}{\ln(a)}}$ From the above I can get a new statement: $1+\dfrac{1}{2^{x-\frac{\ln(x)}{\ln(2)}}}+\dfrac{1}{3^{x-\frac{\ln(x)}{\ln(2)}}}+\cdots+0>xN-x+1>N$ for all $x\in \left]1,1+\delta\right[$ I substitue $y = x-\frac{\ln(x)}{\ln(2)}$ and get: $1+\dfrac{1}{2^{y}}+\dfrac{1}{3^{y}}+\cdots+0>xN-x+1>N$ This is true for all $y\in \left]1,1+\delta_{1}\right[$ where $\delta_{1}>0$ It is true, because: for $1+\delta>x>1$ I can get $2x-2>\ln(x) \Leftrightarrow x-1>\dfrac{\ln(x)}{2} \implies y=x-\dfrac{\ln(x)}{2}>1$ and because $x$ is limited by $1+\delta$ I can find a new smaller $\delta_{2}>0$ so $1+\dfrac{1}{2^{y}}+\dfrac{1}{3^{y}}+\cdots+0>xN-x+1>N$ is true for all $y\in \left]1,1+\delta_{2}\right[$ Finally Now I'm done. If I choose a $N\in \mathbb{R}$ I can go iteratively from 1.25 to a value above $N$. My question My question is very simple. I know it's long (and ""ugly""), but does it looks right? The strategy is ""homemade"", so I'm a bit insecure.",,"['real-analysis', 'sequences-and-series', 'analysis']"
22,Showing that a function below is in VMO,Showing that a function below is in VMO,,I am trying to show that the function $\log||\log |x||$ is in VMO . The book that I was reading just mentioned that it was a straightforward verification. Can some help me to figure it out?,I am trying to show that the function $\log||\log |x||$ is in VMO . The book that I was reading just mentioned that it was a straightforward verification. Can some help me to figure it out?,,"['real-analysis', 'analysis']"
23,Is the ball measure of non-compactness a Lipschitz map?,Is the ball measure of non-compactness a Lipschitz map?,,"Let $(M,d)$ be a metric space and let $H(M)$ denote the set of closed and bounded subset in $M$. Then $(H(M),d_H)$ is a metric space where $d_H$ denotes the Hausdorff distance. Let $\chi$ be the Hausdorff (or ball) measure of non-compactness How do I prove that the map $\chi\colon H(M)\to\mathbf{R}:B\mapsto\chi(B)$ is Lipschitz?","Let $(M,d)$ be a metric space and let $H(M)$ denote the set of closed and bounded subset in $M$. Then $(H(M),d_H)$ is a metric space where $d_H$ denotes the Hausdorff distance. Let $\chi$ be the Hausdorff (or ball) measure of non-compactness How do I prove that the map $\chi\colon H(M)\to\mathbf{R}:B\mapsto\chi(B)$ is Lipschitz?",,"['real-analysis', 'analysis', 'functional-analysis', 'fractals']"
24,Accumulation point in a pre-image,Accumulation point in a pre-image,,"Let $f:U\subset\mathbb{R}^m\rightarrow\mathbb{R}^n$ be a differentiable function over an open set $U$. If there is a $b\in\mathbb{R}^n$ such that the set $f^{-1}(b)$ has an accumulation point $a\in\mathbb{R}^n$, then $(Df)_a:\mathbb{R}^m\rightarrow\mathbb{R}^n$ is not injective.","Let $f:U\subset\mathbb{R}^m\rightarrow\mathbb{R}^n$ be a differentiable function over an open set $U$. If there is a $b\in\mathbb{R}^n$ such that the set $f^{-1}(b)$ has an accumulation point $a\in\mathbb{R}^n$, then $(Df)_a:\mathbb{R}^m\rightarrow\mathbb{R}^n$ is not injective.",,"['analysis', 'multivariable-calculus']"
25,Antisymmetric functions in higher dimensions,Antisymmetric functions in higher dimensions,,"For an antisymmetric function $f:\mathbb R\rightarrow \mathbb R$ (i.e. $f(x)=-f(-x)$ ) we have that a necessary condition for the differential of $f$ of order $r$ to not vanish at $0$ is that $r$ is odd. My question is: what if I consider an antisymmetric $f:\mathbb R^n\rightarrow \mathbb R$ with $n\geq 2$ ? Here antisymmetric means: $f(x)= \text{sign}(\pi) f(\pi x)$ for every signed permutation $\pi$ of the coordinates (a signed pemutation is a permutation which is also allowed to change the sign of the coordinate. I hope it's clear what I mean. I don't know if it's standard terminology). I think that now we have that necessary condition for the differential of $f$ order $r$ to not vanish at $0$ is that $r=k n$ with $k$ odd. Is this true? In case it is, how do you prove it? Any help would be appreciated.","For an antisymmetric function (i.e. ) we have that a necessary condition for the differential of of order to not vanish at is that is odd. My question is: what if I consider an antisymmetric with ? Here antisymmetric means: for every signed permutation of the coordinates (a signed pemutation is a permutation which is also allowed to change the sign of the coordinate. I hope it's clear what I mean. I don't know if it's standard terminology). I think that now we have that necessary condition for the differential of order to not vanish at is that with odd. Is this true? In case it is, how do you prove it? Any help would be appreciated.",f:\mathbb R\rightarrow \mathbb R f(x)=-f(-x) f r 0 r f:\mathbb R^n\rightarrow \mathbb R n\geq 2 f(x)= \text{sign}(\pi) f(\pi x) \pi f r 0 r=k n k,"['calculus', 'analysis', 'functions', 'multivariable-calculus', 'tensor-products']"
26,Existence of a differentiable curve,Existence of a differentiable curve,,"Let $f: [a, b] \to \mathbb{R}^n$ where a and b are real numbers. Prove that the curve $f$ is differentiable if and only if there is an open interval $I$ that contains the closed interval $[a, b]$, and a differentiable curve $g: I \to \mathbb{R}^n$ such that its restriction to the interval $[a, b]$ is the curve $f$. Showing from 'down' to 'up' should be trivial, because if we have a differentiable $g$ on an interval $I$, then it should specifically be differentiable on the interval $[a, b]$, and if we let $f$ be the restriction of the function $g$ on that interval then $f$ must be differentiable. Going from 'up' to 'down' is what is bothering me. So now I construct a $g$ on $(a-1,b+1)$ as $g(x)=f(x)$ for $x \in [a,b]$ $g(x)=f(a)+f'(a)(x-a)$ for $x \in (a-1,a)$ $g(x)=f(b)+f'(b)(x-b)$ for $x \in (b,b+1)$ We see that $g$ is differentiable on $(a-1, b+1)$ except for maybe on $a, b$. So I examine the left and right sided limits: the left sided limit $$\lim _ {t \to a-} \frac{g(a) - g(t)}{t} = \lim_{t \to a-} \frac{f(a) + f'(a)(t-a) - f(a) - f'(a)(a-a)}{t} = \lim_{t \to a-} \frac{f'(a)t - f'(a)a}{t}$$ And the right sided limit $$\lim_{t \to a+} \frac{f(t) - f(a)}{t}$$ I've been trying to bridge these two limits for over an hour now. Am I approaching this problem incorrectly or am I missing something in the two one sided limits?","Let $f: [a, b] \to \mathbb{R}^n$ where a and b are real numbers. Prove that the curve $f$ is differentiable if and only if there is an open interval $I$ that contains the closed interval $[a, b]$, and a differentiable curve $g: I \to \mathbb{R}^n$ such that its restriction to the interval $[a, b]$ is the curve $f$. Showing from 'down' to 'up' should be trivial, because if we have a differentiable $g$ on an interval $I$, then it should specifically be differentiable on the interval $[a, b]$, and if we let $f$ be the restriction of the function $g$ on that interval then $f$ must be differentiable. Going from 'up' to 'down' is what is bothering me. So now I construct a $g$ on $(a-1,b+1)$ as $g(x)=f(x)$ for $x \in [a,b]$ $g(x)=f(a)+f'(a)(x-a)$ for $x \in (a-1,a)$ $g(x)=f(b)+f'(b)(x-b)$ for $x \in (b,b+1)$ We see that $g$ is differentiable on $(a-1, b+1)$ except for maybe on $a, b$. So I examine the left and right sided limits: the left sided limit $$\lim _ {t \to a-} \frac{g(a) - g(t)}{t} = \lim_{t \to a-} \frac{f(a) + f'(a)(t-a) - f(a) - f'(a)(a-a)}{t} = \lim_{t \to a-} \frac{f'(a)t - f'(a)a}{t}$$ And the right sided limit $$\lim_{t \to a+} \frac{f(t) - f(a)}{t}$$ I've been trying to bridge these two limits for over an hour now. Am I approaching this problem incorrectly or am I missing something in the two one sided limits?",,"['calculus', 'general-topology', 'analysis']"
27,Construct a function that is differentiable only on the rationals.,Construct a function that is differentiable only on the rationals.,,"In the spirit of having handy counter examples, is it possible to construct a function that is differentiable on all of the rational numbers and nowhere else? Similarly, if a function is holomorphic at a point, must it always be holomorphic in an open neighborhood around that point? Constructive proofs and counterexamples will be given priority.","In the spirit of having handy counter examples, is it possible to construct a function that is differentiable on all of the rational numbers and nowhere else? Similarly, if a function is holomorphic at a point, must it always be holomorphic in an open neighborhood around that point? Constructive proofs and counterexamples will be given priority.",,"['calculus', 'real-analysis', 'analysis', 'complex-analysis']"
28,Handling Cross ratios ( Fractional linear transformations ),Handling Cross ratios ( Fractional linear transformations ),,"According to remmert there is  a relationship between the crossratios: $$C(z,u,v,w) = \frac{(z-v)(u-w)}{(z-w)(u-v)} \text{ and } C(z,v,u,w)= \frac{(z-u)(v-w)}{(z-w)(v-u)}$$ where $z,u,v,w \in \mathbb{C}$ I have tried multiplying things out to see a relationship, but I don't see anything. What could be meant with ""relationship""? Another question deals with the value of the crossratio: On which of the three arcs through $u,v,w$ must z lie so that $0 < C(z,u,v,w) < 1$? How does one see that there must be three arcs through u,v,w without plotting and how can we characterize the cross ratio without plotting? Thanks for every suggestion.","According to remmert there is  a relationship between the crossratios: $$C(z,u,v,w) = \frac{(z-v)(u-w)}{(z-w)(u-v)} \text{ and } C(z,v,u,w)= \frac{(z-u)(v-w)}{(z-w)(v-u)}$$ where $z,u,v,w \in \mathbb{C}$ I have tried multiplying things out to see a relationship, but I don't see anything. What could be meant with ""relationship""? Another question deals with the value of the crossratio: On which of the three arcs through $u,v,w$ must z lie so that $0 < C(z,u,v,w) < 1$? How does one see that there must be three arcs through u,v,w without plotting and how can we characterize the cross ratio without plotting? Thanks for every suggestion.",,"['analysis', 'complex-analysis']"
29,Differentiability in $\mathbb{R}^n$ and chain rule,Differentiability in  and chain rule,\mathbb{R}^n,"I have a question: Consider a function $g:\mathbb{R}^{n}\rightarrow \mathbb{R}$ is differentiable. Find the derivative of the function: $G(x)=[ g\left ( x,x^{2},...,x^{n} \right )]^{2}$ where $x\in \mathbb{R}$. Here, G is a function of one variable, so I tried to apply the chain rule to find its derivative as follows: $G^{'}\left ( x \right )=2H^{'}\left ( x \right )H(x)$ where $H\left ( u_{1},u_{2},...,u_{n} \right )=g\left ( x,x^{2},...,x^{n} \right )$. Now to find the derivative of $H$, I did the following: $$\frac{d}{dx}H=\frac{\partial H}{\partial u_{1}}\frac{\partial u_{1}}{\partial x}+...\frac{\partial H}{\partial u_{n}}\frac{\partial u_{n}}{\partial x}.$$ Does what I did make sense?","I have a question: Consider a function $g:\mathbb{R}^{n}\rightarrow \mathbb{R}$ is differentiable. Find the derivative of the function: $G(x)=[ g\left ( x,x^{2},...,x^{n} \right )]^{2}$ where $x\in \mathbb{R}$. Here, G is a function of one variable, so I tried to apply the chain rule to find its derivative as follows: $G^{'}\left ( x \right )=2H^{'}\left ( x \right )H(x)$ where $H\left ( u_{1},u_{2},...,u_{n} \right )=g\left ( x,x^{2},...,x^{n} \right )$. Now to find the derivative of $H$, I did the following: $$\frac{d}{dx}H=\frac{\partial H}{\partial u_{1}}\frac{\partial u_{1}}{\partial x}+...\frac{\partial H}{\partial u_{n}}\frac{\partial u_{n}}{\partial x}.$$ Does what I did make sense?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
30,An inequality with radicals,An inequality with radicals,,"If $s_{1}\ge t_{1}\ge t_{2}\ge s_{2}\ge0$, does one always have $(s_{1}-t_{1}+s_{2}+t_{2})^{1/2}\ge\sqrt{s_{1}}-\sqrt{t_{1}}+\sqrt{t_{2}}-\sqrt{s_{2}}$? Thanks a lot!","If $s_{1}\ge t_{1}\ge t_{2}\ge s_{2}\ge0$, does one always have $(s_{1}-t_{1}+s_{2}+t_{2})^{1/2}\ge\sqrt{s_{1}}-\sqrt{t_{1}}+\sqrt{t_{2}}-\sqrt{s_{2}}$? Thanks a lot!",,"['calculus', 'algebra-precalculus', 'analysis', 'inequality']"
31,Norm with special conditions,Norm with special conditions,,"Let $N$ be a norm on $\mathbb R^2$, such that $N ( \mathbb Z^2) \subset \mathbb N $, where $\mathbb Z^2 =\{ (a,b)\mid a\mbox{ and }b \mbox{ are integers}\}$. Help me to prove that for $u$, $v$ fixed vectors the following limit exists $$\lim_{ t\to 0^+} \frac{N(u+tv)-N(u)}t. $$","Let $N$ be a norm on $\mathbb R^2$, such that $N ( \mathbb Z^2) \subset \mathbb N $, where $\mathbb Z^2 =\{ (a,b)\mid a\mbox{ and }b \mbox{ are integers}\}$. Help me to prove that for $u$, $v$ fixed vectors the following limit exists $$\lim_{ t\to 0^+} \frac{N(u+tv)-N(u)}t. $$",,"['analysis', 'normed-spaces']"
32,Upper bound and monotony,Upper bound and monotony,,"I'm trying to figure out wether I understand the meaning of the following sequence  correctly. Let $\displaystyle a_n = \sum_{k=1}^n\frac{1}{k+n}, n \in \mathbb{N}$. Is this a correct upper bound? $$\sum_{k=1}^n\frac{1}{k+n} < \sum_{k=1}^n\frac{1}{n} = n \cdot\frac{1}{n} = 1.$$ Is there a way to proof the monotony?","I'm trying to figure out wether I understand the meaning of the following sequence  correctly. Let $\displaystyle a_n = \sum_{k=1}^n\frac{1}{k+n}, n \in \mathbb{N}$. Is this a correct upper bound? $$\sum_{k=1}^n\frac{1}{k+n} < \sum_{k=1}^n\frac{1}{n} = n \cdot\frac{1}{n} = 1.$$ Is there a way to proof the monotony?",,"['real-analysis', 'sequences-and-series', 'analysis']"
33,How to minimize this function difference,How to minimize this function difference,,"Sorry about this somewhat lengthy introduction to my question. I thought it might be useful to know what I'm trying to do. I decided that I would like to have sequence of polynomials in $\mathbb{P}_n (x)$ defined in domain $x\in[0,1]$ which gradually learn a function. So, given an initial polynomial $P_{\alpha_0}(x)=\sum_{i=1}^n a_i x^{i-1}$ where $\alpha_0 = (a_1,\ldots,a_n) \in \mathbb{R}^n$, I would introduce a new point $(x_0,y_0) \in [0,1]\times\mathbb{R}$ and try to find all $\beta \in\mathbb{R}^n$ such that  $$ P_{\alpha_0 + \beta}(x_0)=y_0 $$ If the solution space to that is $S$, then the next step would be to find optimal solution which somehow minimises the non-local difference. Define a norm which penalizes non-local difference: $$ {\lVert P_{\alpha} \rVert}_{(x_0)} = \int_0^1 {(x-x_0)}^2{(P_{\alpha}(x))^2} dx $$ The objective would be to find such $\beta \in S$ that it minimises $$ {\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)} $$ and then have for the next round $\alpha_1 = \alpha_0 + \beta$. Looking at solutions for the standard basis $\beta_i=(0,\ldots,0,b_{i,i},0,\ldots,0)$ (i.e. all but the $i$:th element are zero) one finds easily that $$ P_{\alpha_0+\beta_i}(x_0)=y_0 $$ when $$ b_{i,i} = \frac{y_0 - P_{\alpha_0}(x_0)}{x_0^{i-1}} $$ Let's use $b_i = b_{i,i}$ for short. The solution space $S$ is linear combination $\sum t_i \beta_i$ with the requirement that $\sum t_i = 1$. Now, do you have any elegant ways to find the minimum for $$ {\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)} = \sum_{i,j}^n t_i t_j b_i b_j g_{x_0}(i,j) $$ where  $$ g_{x_0}(i,j) = (\frac{1}{i+j+1} -\frac{2 x_0}{i+j} + \frac{x_0^2}{i+j-1}) $$ from the solution hyperplane $\sum t_i = 1$? It's rather easy to find the equation for the null space of the partial differentials $$ \frac{d{\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)}}{d t_k} = \sum_i t_i b_i g_{x_0}(k,j) $$  This is a linear mapping of the form $M t = 0 $ where $M_{i,j} = b_i g_{x_0}(i,j)$. Testing numerically with some functions seems to indicate that $M$ is mostly of full rank and in those cases the solution would be $t=0$ which does not fulfill the requirement that $\sum t_i = 1$. Adding this requirement to the mapping gives overdetermined equation $$ \left( \begin{matrix} M \\ 1 \end{matrix} \right) t =   \left( \begin{matrix} 0 \\ 1 \end{matrix} \right) $$","Sorry about this somewhat lengthy introduction to my question. I thought it might be useful to know what I'm trying to do. I decided that I would like to have sequence of polynomials in $\mathbb{P}_n (x)$ defined in domain $x\in[0,1]$ which gradually learn a function. So, given an initial polynomial $P_{\alpha_0}(x)=\sum_{i=1}^n a_i x^{i-1}$ where $\alpha_0 = (a_1,\ldots,a_n) \in \mathbb{R}^n$, I would introduce a new point $(x_0,y_0) \in [0,1]\times\mathbb{R}$ and try to find all $\beta \in\mathbb{R}^n$ such that  $$ P_{\alpha_0 + \beta}(x_0)=y_0 $$ If the solution space to that is $S$, then the next step would be to find optimal solution which somehow minimises the non-local difference. Define a norm which penalizes non-local difference: $$ {\lVert P_{\alpha} \rVert}_{(x_0)} = \int_0^1 {(x-x_0)}^2{(P_{\alpha}(x))^2} dx $$ The objective would be to find such $\beta \in S$ that it minimises $$ {\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)} $$ and then have for the next round $\alpha_1 = \alpha_0 + \beta$. Looking at solutions for the standard basis $\beta_i=(0,\ldots,0,b_{i,i},0,\ldots,0)$ (i.e. all but the $i$:th element are zero) one finds easily that $$ P_{\alpha_0+\beta_i}(x_0)=y_0 $$ when $$ b_{i,i} = \frac{y_0 - P_{\alpha_0}(x_0)}{x_0^{i-1}} $$ Let's use $b_i = b_{i,i}$ for short. The solution space $S$ is linear combination $\sum t_i \beta_i$ with the requirement that $\sum t_i = 1$. Now, do you have any elegant ways to find the minimum for $$ {\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)} = \sum_{i,j}^n t_i t_j b_i b_j g_{x_0}(i,j) $$ where  $$ g_{x_0}(i,j) = (\frac{1}{i+j+1} -\frac{2 x_0}{i+j} + \frac{x_0^2}{i+j-1}) $$ from the solution hyperplane $\sum t_i = 1$? It's rather easy to find the equation for the null space of the partial differentials $$ \frac{d{\lVert P_{\alpha_0} -P_{\alpha_0+\beta}\rVert}_{(x_0)}}{d t_k} = \sum_i t_i b_i g_{x_0}(k,j) $$  This is a linear mapping of the form $M t = 0 $ where $M_{i,j} = b_i g_{x_0}(i,j)$. Testing numerically with some functions seems to indicate that $M$ is mostly of full rank and in those cases the solution would be $t=0$ which does not fulfill the requirement that $\sum t_i = 1$. Adding this requirement to the mapping gives overdetermined equation $$ \left( \begin{matrix} M \\ 1 \end{matrix} \right) t =   \left( \begin{matrix} 0 \\ 1 \end{matrix} \right) $$",,"['linear-algebra', 'analysis', 'functional-analysis', 'polynomials', 'interpolation']"
34,Measurability of the derivative,Measurability of the derivative,,"A local analysis textbook I have used has the following exercise: Let $X$ be a finite-dimensional, $Y$ a separable Banach-space, $f\colon X\rightarrowtail Y$ any function. Show that $f'$ is Borel. I have no idea how to approach this. I looked at the proof of Rademacher, but even that only gives Lebesgue-measurability as far as I can see, and this has no assumptions on the function at all. Any help would be appreciated.","A local analysis textbook I have used has the following exercise: Let $X$ be a finite-dimensional, $Y$ a separable Banach-space, $f\colon X\rightarrowtail Y$ any function. Show that $f'$ is Borel. I have no idea how to approach this. I looked at the proof of Rademacher, but even that only gives Lebesgue-measurability as far as I can see, and this has no assumptions on the function at all. Any help would be appreciated.",,"['calculus', 'analysis']"
35,Convergence in the mean of Fourier series,Convergence in the mean of Fourier series,,"I need to do some research on fourier series for my analysis class so I'm trying to find info (preferably a book or paper with the proof) on this: ""If $f$ is Riemann integrable on $[-l,l]$ then its fourier series converge in the mean to $f$ on $[-l,l]$"" Or in other words that "" the fourier trigonometric system is complete over the Riemann integrable functions on $[-l,l]$ "" I'm talking about this specific fourier series $$f \sim \frac{a_0}{2} + \sum_{n=1}^{\infty}{a_n\cos\frac{n\pi}{l}x + b_n\sin\frac{n\pi}{l}x}$$ Where $$a_n = \frac{1}{l}\int_{-l}^l{f(x)\cos\frac{n\pi}{l}x \, dx}$$ and $$b_n = \frac{1}{l}\int_{-l}^l{f(x)\sin\frac{n\pi}{l}x \, dx}$$ Any info you can give me on this would be appreciated.","I need to do some research on fourier series for my analysis class so I'm trying to find info (preferably a book or paper with the proof) on this: ""If $f$ is Riemann integrable on $[-l,l]$ then its fourier series converge in the mean to $f$ on $[-l,l]$"" Or in other words that "" the fourier trigonometric system is complete over the Riemann integrable functions on $[-l,l]$ "" I'm talking about this specific fourier series $$f \sim \frac{a_0}{2} + \sum_{n=1}^{\infty}{a_n\cos\frac{n\pi}{l}x + b_n\sin\frac{n\pi}{l}x}$$ Where $$a_n = \frac{1}{l}\int_{-l}^l{f(x)\cos\frac{n\pi}{l}x \, dx}$$ and $$b_n = \frac{1}{l}\int_{-l}^l{f(x)\sin\frac{n\pi}{l}x \, dx}$$ Any info you can give me on this would be appreciated.",,"['analysis', 'fourier-series']"
36,Proof of a binomial theorem based inequality?,Proof of a binomial theorem based inequality?,,"Let $k \in N, x \gt 0$. Show that there exists some $n_2 \in \mathbb{N}$ so that $\forall n \geq n_2: (1+x)^n \gt n^k$. Hint: binomial theorem. My thought on this is first to make the substitution $(1+x)=b$ which means $b>1$ and $b^n>1$. This would also be true if $k=0$ and $n=1$ thus $n_2=1$. Next step I can think of is using archimedian property $a>0, y \in \mathbb{R}, m \in \mathbb{N}, ma>y$. This will result in $b^n=(1+x)^n=ma > y = n^k$. My current idea would be to replace $n$ with some other cleverly devised number OR using induction because of the request for all $n$ and maybe then I can use binomial theorem to finally solve this. What's really throwing me off is that $n$ is the exponent on the left and also the base on the right (that is why I thought about replacing $n$). Any hints on how to get to the next step? Thanks.","Let $k \in N, x \gt 0$. Show that there exists some $n_2 \in \mathbb{N}$ so that $\forall n \geq n_2: (1+x)^n \gt n^k$. Hint: binomial theorem. My thought on this is first to make the substitution $(1+x)=b$ which means $b>1$ and $b^n>1$. This would also be true if $k=0$ and $n=1$ thus $n_2=1$. Next step I can think of is using archimedian property $a>0, y \in \mathbb{R}, m \in \mathbb{N}, ma>y$. This will result in $b^n=(1+x)^n=ma > y = n^k$. My current idea would be to replace $n$ with some other cleverly devised number OR using induction because of the request for all $n$ and maybe then I can use binomial theorem to finally solve this. What's really throwing me off is that $n$ is the exponent on the left and also the base on the right (that is why I thought about replacing $n$). Any hints on how to get to the next step? Thanks.",,['analysis']
37,Cauchy-Schwarz for Multiple Integrals,Cauchy-Schwarz for Multiple Integrals,,Is there a generalization of the Cauchy-Schwarz Inequality for multiple integrals?,Is there a generalization of the Cauchy-Schwarz Inequality for multiple integrals?,,['analysis']
38,Biggest subset of $L^1$ for which products and integrals commute,Biggest subset of  for which products and integrals commute,L^1,"I asked myself a question during my first year and never find any interesting clue about it. One of the first thing you learned in integral calculus is that $\int fg \neq \int f \int g$ in general . One can ask the following question : When do we have $\int_I fg = \int_I f \int_I g$ ? Can I find a maximal subset of $CM(K)^2$ or $L^1(\mathbb{R})^2$ for example ? A full ""algebric"" characterization of such couples ? Sure there are some trivial couples, but the more the time passes, the more I have the feeling that the answer is : this set exists, and there is nothing more interesting to say about it. But it is not really a big satisfaction ... Any ideas/answers about these questions ? PS : It's my first post on math.se, and it seems that my questions are appropriate after reading the FAQ. I hope so, but please redirect me if you think that I'm not on the right place.","I asked myself a question during my first year and never find any interesting clue about it. One of the first thing you learned in integral calculus is that $\int fg \neq \int f \int g$ in general . One can ask the following question : When do we have $\int_I fg = \int_I f \int_I g$ ? Can I find a maximal subset of $CM(K)^2$ or $L^1(\mathbb{R})^2$ for example ? A full ""algebric"" characterization of such couples ? Sure there are some trivial couples, but the more the time passes, the more I have the feeling that the answer is : this set exists, and there is nothing more interesting to say about it. But it is not really a big satisfaction ... Any ideas/answers about these questions ? PS : It's my first post on math.se, and it seems that my questions are appropriate after reading the FAQ. I hope so, but please redirect me if you think that I'm not on the right place.",,"['calculus', 'analysis']"
39,p-seminorms on smooth functions are equivalent,p-seminorms on smooth functions are equivalent,,"Let $K \subset R^n$ be compact, and let $C_0^{\infty}(K)$ be the space of smooth functions on with support in $K$. For $p \in [1,\infty), \alpha$ multiindex, let $|f|_{\alpha,p} = (\int_K |D^{\alpha}f|^p)^{\frac{1}{p}}$, and $|f|_{\alpha,\infty} = sup_K |D^{\alpha}f|$ - as usual. For $p \in [1,\infty]$, let $\tau_p$ be the locally convex topology generated on $C_0^{\infty}(K)$ by the seminorms $(|f|_{\alpha,p})_{\alpha}$. Claim: All the $\tau_p$ are the same. One proof of this might utilize the Sobolev theory (Kondrashow embedding theorem) which is too heavy as tool for my taste. Unfortunately, I don't know a more elementary proof. Furthermore I guess a beautiful proof of this might work similarly for other spaces like the Schwartz-space or the space of smooth functions on the whole of $R^n$. Do you have any good idea?","Let $K \subset R^n$ be compact, and let $C_0^{\infty}(K)$ be the space of smooth functions on with support in $K$. For $p \in [1,\infty), \alpha$ multiindex, let $|f|_{\alpha,p} = (\int_K |D^{\alpha}f|^p)^{\frac{1}{p}}$, and $|f|_{\alpha,\infty} = sup_K |D^{\alpha}f|$ - as usual. For $p \in [1,\infty]$, let $\tau_p$ be the locally convex topology generated on $C_0^{\infty}(K)$ by the seminorms $(|f|_{\alpha,p})_{\alpha}$. Claim: All the $\tau_p$ are the same. One proof of this might utilize the Sobolev theory (Kondrashow embedding theorem) which is too heavy as tool for my taste. Unfortunately, I don't know a more elementary proof. Furthermore I guess a beautiful proof of this might work similarly for other spaces like the Schwartz-space or the space of smooth functions on the whole of $R^n$. Do you have any good idea?",,"['analysis', 'functional-analysis']"
40,Does the existence of an infimum imply that the set of lower bounds of a set is totally ordered?,Does the existence of an infimum imply that the set of lower bounds of a set is totally ordered?,,"Say we have a partially ordered set $(S,\preceq)$, and some subset $E\subseteq S$ such that $E$ is bounded below and $\inf E$ exists. My question is, since $S$ is not totally ordered is it possible to have two lower bounds that are not comparable, and hence be unable to say whether one lower bound is really the greatest lower bound since you cannot compare to every other lower bound? If such a thing is not possible, (which I believe is the case since it is given $\inf E$ exists) would it also be fair to say that the existence of $\inf E$ implies that the set $L(E)$ of lower bounds of $E$ is a totally ordered subset of $S$? Thanks.","Say we have a partially ordered set $(S,\preceq)$, and some subset $E\subseteq S$ such that $E$ is bounded below and $\inf E$ exists. My question is, since $S$ is not totally ordered is it possible to have two lower bounds that are not comparable, and hence be unable to say whether one lower bound is really the greatest lower bound since you cannot compare to every other lower bound? If such a thing is not possible, (which I believe is the case since it is given $\inf E$ exists) would it also be fair to say that the existence of $\inf E$ implies that the set $L(E)$ of lower bounds of $E$ is a totally ordered subset of $S$? Thanks.",,"['analysis', 'order-theory']"
41,Is there a website that has all the special functions? [closed],Is there a website that has all the special functions? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 days ago . Improve this question There are a lot of special functions, and I wonder if there is a website that collects all of them, similar to how the Encyclopedia of Triangle Centers collects information on triangle centers. Another question is: What are the criteria for defining a new special function? One can define anything if they want to, so is there any established criteria for what would be considered and accepted as a new special function? For example I  can just define a function like $\Theta(x)$ to be the inverse function for $e^{\sin(x)}\cos(x)+ 5x^x-\Gamma(\gamma x) $ where $x\in [0, \frac \pi 2 ]$ Now this is a new function, what one should do to make similar functions an ""acceptable"" special function? I think  a function to be considered a new one is that it can't be obtained via definite combinations of the known functions. In geometry if one discovers a new triangle center he can just submit it to ETC I wonder if  a similar thing exist for special functions? It seems like the such websites don't exist but I think a websites  that collects special functions like how ETC collect triangle centers is a good idea that I want to make this website but I don't have enough knowledge in mathematics or programming  to make such thing I wonder whether I can request such Idea in MSE or MO for the community ton do.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 days ago . Improve this question There are a lot of special functions, and I wonder if there is a website that collects all of them, similar to how the Encyclopedia of Triangle Centers collects information on triangle centers. Another question is: What are the criteria for defining a new special function? One can define anything if they want to, so is there any established criteria for what would be considered and accepted as a new special function? For example I  can just define a function like to be the inverse function for where Now this is a new function, what one should do to make similar functions an ""acceptable"" special function? I think  a function to be considered a new one is that it can't be obtained via definite combinations of the known functions. In geometry if one discovers a new triangle center he can just submit it to ETC I wonder if  a similar thing exist for special functions? It seems like the such websites don't exist but I think a websites  that collects special functions like how ETC collect triangle centers is a good idea that I want to make this website but I don't have enough knowledge in mathematics or programming  to make such thing I wonder whether I can request such Idea in MSE or MO for the community ton do.","\Theta(x) e^{\sin(x)}\cos(x)+ 5x^x-\Gamma(\gamma x)  x\in [0, \frac \pi 2 ]","['real-analysis', 'complex-analysis', 'analysis', 'soft-question', 'special-functions']"
42,Equivalent definitions of affine function: Does $f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y)$ on convex set $S$ imply linearity of $f$ on $S$?,Equivalent definitions of affine function: Does  on convex set  imply linearity of  on ?,f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y) S f S,"This bounty has ended . Answers to this question are eligible for a +50 reputation bounty. Bounty grace period ends in 3 hours . LordOfNumbers wants to draw more attention to this question. For a function $f\colon\mathbb{R}^n\to\mathbb{R}$ the following two are equivalent \begin{align*} &\text{(i) there exist $a\in\mathbb{R}^n$, $b\in\mathbb{R}$ so that $f(x)=a^{t}x+b$}\newline &\text{(ii) for all $x,y\in\mathbb{R}^n$ and $\gamma\in(0,1)$ it holds $f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y)$}. \end{align*} See for example here for a proof I want to argue that the statement still holds if we consider functions $f:X\to\mathbb{R}$ where $X\subset\mathbb{R}^n$ is a convex set. My intuitive idea: Obviously $(i)\implies (ii)$ still holds in that case. On the other hand, if we assume that (ii) is true, then we could consider the affine space spanned by $X$ . Let us call this affine space $Y$ . I think it should be true, that the interior of $X$ is open in $Y$ . So we could probably define an affine function $\tilde{f}:Y\to\mathbb{R}$ that extends $f$ first. And then extend $\tilde{f}$ to all of $\mathbb{R}^n$ (somehow). I am hoping that someone knows how to do this rigorously.","This bounty has ended . Answers to this question are eligible for a +50 reputation bounty. Bounty grace period ends in 3 hours . LordOfNumbers wants to draw more attention to this question. For a function the following two are equivalent See for example here for a proof I want to argue that the statement still holds if we consider functions where is a convex set. My intuitive idea: Obviously still holds in that case. On the other hand, if we assume that (ii) is true, then we could consider the affine space spanned by . Let us call this affine space . I think it should be true, that the interior of is open in . So we could probably define an affine function that extends first. And then extend to all of (somehow). I am hoping that someone knows how to do this rigorously.","f\colon\mathbb{R}^n\to\mathbb{R} \begin{align*}
&\text{(i) there exist a\in\mathbb{R}^n, b\in\mathbb{R} so that f(x)=a^{t}x+b}\newline
&\text{(ii) for all x,y\in\mathbb{R}^n and \gamma\in(0,1) it holds f(\gamma x +(1-\gamma)y)=\gamma f(x)+(1-\gamma)f(y)}.
\end{align*} f:X\to\mathbb{R} X\subset\mathbb{R}^n (i)\implies (ii) X Y X Y \tilde{f}:Y\to\mathbb{R} f \tilde{f} \mathbb{R}^n","['real-analysis', 'analysis', 'convex-analysis']"
43,Barycentre of a ball,Barycentre of a ball,,"I saw the following definition for the barycentre of a set $\Omega \subseteq \mathbb{R}^n$ : $$ \mathrm{bc}^\Omega=\frac{1}{\mathrm{vol}(\Omega)}\int_\Omega x dx \in\mathbb{R}^n. $$ So I wanted to compute the barycenter for a ball of radius $r$ with center $x_0, \quad \Omega= B_r(x_0)$ , and from my understanding it should be $\mathrm{bc}^\Omega=x_0.$ In the Original post my computation was wrong here is the corrected version: \begin{equation} \begin{aligned} \mathrm{bc}^\Omega_i&=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}\xi_i dS\xi d\rho\\   &=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}|\xi-x_0|\left(\left\langle e_i, \frac{\xi-x_0}{|\xi-x_0|} \right\rangle + \left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle \right) dS\xi d\rho\\   &=\frac{1}{\omega_n r^n}\int_0^r\rho\int_{B_\rho(x_0)}div(e_i) dx + \int_{\partial B_\rho(x_0)}\rho\left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle dS\xi d\rho\\   &=\frac{x_{0, i} \cdot n}{r^n}\int_{0}^{r}\rho^{n-1}d\rho=x_{0, i} \end{aligned} \end{equation} Where I first transformed the integral domain to integrate over spheres, then reformulated the integrand to include the unit normal to those spheres. Applied the divergence theorem to the constant $e_i$ and integrated as usual in the second summand. I also used the formulae: $$ \int_{B_r(x_0)}1 dx= r^n\omega_n,\quad  \int_{\partial B_r(x_0)}1 dS\xi= r^{n-1}\omega_n\cdot n $$ where $\omega_n=\frac{\pi^{n/2}}{\Gamma(\frac n2+1)}$ is the volume of the unit ball.","I saw the following definition for the barycentre of a set : So I wanted to compute the barycenter for a ball of radius with center , and from my understanding it should be In the Original post my computation was wrong here is the corrected version: Where I first transformed the integral domain to integrate over spheres, then reformulated the integrand to include the unit normal to those spheres. Applied the divergence theorem to the constant and integrated as usual in the second summand. I also used the formulae: where is the volume of the unit ball.","\Omega \subseteq \mathbb{R}^n 
\mathrm{bc}^\Omega=\frac{1}{\mathrm{vol}(\Omega)}\int_\Omega x dx \in\mathbb{R}^n.
 r x_0, \quad \Omega= B_r(x_0) \mathrm{bc}^\Omega=x_0. \begin{equation}
\begin{aligned}
\mathrm{bc}^\Omega_i&=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}\xi_i dS\xi d\rho\\
  &=\frac{1}{\omega_n r^n}\int_0^r\int_{\partial B_\rho(x_0)}|\xi-x_0|\left(\left\langle e_i, \frac{\xi-x_0}{|\xi-x_0|} \right\rangle + \left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle \right) dS\xi d\rho\\
  &=\frac{1}{\omega_n r^n}\int_0^r\rho\int_{B_\rho(x_0)}div(e_i) dx + \int_{\partial B_\rho(x_0)}\rho\left\langle e_i, \frac{x_0}{|\xi-x_0|} \right\rangle dS\xi d\rho\\
  &=\frac{x_{0, i} \cdot n}{r^n}\int_{0}^{r}\rho^{n-1}d\rho=x_{0, i}
\end{aligned}
\end{equation} e_i 
\int_{B_r(x_0)}1 dx= r^n\omega_n,\quad 
\int_{\partial B_r(x_0)}1 dS\xi= r^{n-1}\omega_n\cdot n
 \omega_n=\frac{\pi^{n/2}}{\Gamma(\frac n2+1)}","['real-analysis', 'calculus', 'integration', 'analysis', 'vector-analysis']"
44,"Show that if $f$ is a measurable complex-valued function on $(X,\mathscr{A})$, then $|f|$ is also measurable.","Show that if  is a measurable complex-valued function on , then  is also measurable.","f (X,\mathscr{A}) |f|","I need to show If $f$ is a measurable complex-valued function on $(X,\mathscr{A})$ , then $|f|$ is also measurable. I tried it myself, but don't know if my work is correct or not? Could someone please help me check? Thank you very much! My attempt: Let $f:(X,\mathscr{A})\to(\mathbb{C},\mathscr{B}(\mathbb{C}))$ be measurable. Write $f(x)=u(x)+iv(x)$ , where $u,v:X\to\mathbb{R}$ . Then $|f(x)|=\sqrt{u^2(x)+v^2(x)}\in\mathbb{R}$ . We want to show that $|f|:(X,\mathscr{A})\to(\mathbb{R},\mathscr{B}(\mathbb{R}))$ is measurable. Let $\mathscr{B}_0$ be the collection of all subintervals of $\mathbb{R}$ of the form $(-\infty,b]$ . Then the Borel sigma-algebra $\mathscr{B}(\mathbb{R})$ is equal to $\sigma(\mathscr{B}_0)$ . Let $B\in\mathscr{B}_0$ , then $B=\{z\in\mathbb{R}:a\leq b\}$ for some $b\in\mathbb{R}$ . Then \begin{align} |f|^{-1}(B) = \{x\in X:|f(x)|\in B\} = \{x\in X:|f(x)|\leq b\}. \end{align} If $b<0$ , then $|f|^{-1}(b)=\emptyset\in\mathscr{A}$ . So suppose $b\geq0$ . Since $f$ is measurable with respect to $\mathscr{A}$ and $\mathscr{B}(\mathbb{C})$ , its real and imaginary part $u$ and $v$ are $\mathscr{A}$ -measurable, so are $u^2$ , $v^2$ , and $u^2+v^2$ . Thus, \begin{align} |f|^{-1}(B) = \{x\in X:|f(x)|\leq b\} = \{x\in X:u^2(x)+v^2(x)\leq b^2\} \in\mathscr{A}. \end{align} Since $B$ is arbitrary, we have proved that $|f|$ is measurable. If there is any mistake, please point it out! I really appreciate it!","I need to show If is a measurable complex-valued function on , then is also measurable. I tried it myself, but don't know if my work is correct or not? Could someone please help me check? Thank you very much! My attempt: Let be measurable. Write , where . Then . We want to show that is measurable. Let be the collection of all subintervals of of the form . Then the Borel sigma-algebra is equal to . Let , then for some . Then If , then . So suppose . Since is measurable with respect to and , its real and imaginary part and are -measurable, so are , , and . Thus, Since is arbitrary, we have proved that is measurable. If there is any mistake, please point it out! I really appreciate it!","f (X,\mathscr{A}) |f| f:(X,\mathscr{A})\to(\mathbb{C},\mathscr{B}(\mathbb{C})) f(x)=u(x)+iv(x) u,v:X\to\mathbb{R} |f(x)|=\sqrt{u^2(x)+v^2(x)}\in\mathbb{R} |f|:(X,\mathscr{A})\to(\mathbb{R},\mathscr{B}(\mathbb{R})) \mathscr{B}_0 \mathbb{R} (-\infty,b] \mathscr{B}(\mathbb{R}) \sigma(\mathscr{B}_0) B\in\mathscr{B}_0 B=\{z\in\mathbb{R}:a\leq b\} b\in\mathbb{R} \begin{align}
|f|^{-1}(B) = \{x\in X:|f(x)|\in B\} = \{x\in X:|f(x)|\leq b\}.
\end{align} b<0 |f|^{-1}(b)=\emptyset\in\mathscr{A} b\geq0 f \mathscr{A} \mathscr{B}(\mathbb{C}) u v \mathscr{A} u^2 v^2 u^2+v^2 \begin{align}
|f|^{-1}(B) = \{x\in X:|f(x)|\leq b\} = \{x\in X:u^2(x)+v^2(x)\leq b^2\} \in\mathscr{A}.
\end{align} B |f|","['real-analysis', 'analysis', 'measure-theory', 'solution-verification', 'proof-writing']"
45,"Prove that the function $\|\cdot\|_1:C[a,b]\to\mathbb{R}$ defined by $\|f\|_1 = \int_a^b|f|d\lambda$ satisfies $\|f\|_1=0$ if and only if $f=0$. [duplicate]",Prove that the function  defined by  satisfies  if and only if . [duplicate],"\|\cdot\|_1:C[a,b]\to\mathbb{R} \|f\|_1 = \int_a^b|f|d\lambda \|f\|_1=0 f=0","This question already has an answer here : Why must the integral of a nonzero continuous function be nonzero? (1 answer) Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Problem Let $[a,b]$ be a closed and bounded interval, and let $C[a,b]$ be the vector space of all continuous real-valued functions on $[a,b]$ . Define the function $\|\cdot\|_1:C[a,b]\to\mathbb{R}$ by letting $$     \|f\|_1=\int_a^b|f|d\lambda. $$ Prove that if a function $f\in C[a,b]$ satisfies $\|f\|_1=0$ , then $f=0$ . (Note that $\lambda$ is the Lebesgue measure.) My Attempt Let $f\in C[a,b]$ . If $f=0$ , then $\|f\|_1 = \int_1^b0d\lambda = 0$ . Now suppose that $$\|f\|_1=\int_a^b|f|d\lambda= \sup\left\{\int hd\lambda:h\in\mathscr{S}_+\ \text{and}\ h\leq|f|\chi_{[a,b]}\right\} = 0.$$ Assume to the contrary that $f(x)\neq0$ for some $x\in[a,b]$ . Then $|f(x)|>0$ . Since $f$ is continuous, $|f|$ must be continuous and there must exists a closed and bounded interval $[c,d]\subseteq[a,b]$ such that $|f(z)|>0$ for all $z\in[c,d]$ . Define a function $h$ by letting $h(z)=0$ for all $z\notin[c,d]$ and $h(z)=\min_{z\in[c,d]}|f(z)|$ (note that the minimum exists because $|f|$ is a continuous function on a compact set $[c,d]$ ). Then $h\in\mathscr{S}_+$ and $h\leq|f|\chi_{[a,b]}$ , but $\int hd\lambda>0$ , which is a contradiction. Thus, $f=0$ . Some definitions: Definition $\quad$ Let $(X,\mathscr{A})$ be a measurable space. $\mathscr{S}_+$ is the set of simple nonnegative real-valued $\mathscr{A}$ -measurable functions on $X$ . Definition $\quad$ $\chi_A$ is the characteristic function of the set $A$ . My Question I am not sure if my attempt is correct. Could someone please help me check? Especially, I claimed that ""there must exists a closed and bounded interval $[c,d]\subseteq[a,b]$ such that $|f(z)|>0$ for all $z\in[c,d]$ "" without proof. (In fact, I am not sure how to prove this rigorously, but I seriously believe this is correct.) Thank you very much!","This question already has an answer here : Why must the integral of a nonzero continuous function be nonzero? (1 answer) Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Problem Let be a closed and bounded interval, and let be the vector space of all continuous real-valued functions on . Define the function by letting Prove that if a function satisfies , then . (Note that is the Lebesgue measure.) My Attempt Let . If , then . Now suppose that Assume to the contrary that for some . Then . Since is continuous, must be continuous and there must exists a closed and bounded interval such that for all . Define a function by letting for all and (note that the minimum exists because is a continuous function on a compact set ). Then and , but , which is a contradiction. Thus, . Some definitions: Definition Let be a measurable space. is the set of simple nonnegative real-valued -measurable functions on . Definition is the characteristic function of the set . My Question I am not sure if my attempt is correct. Could someone please help me check? Especially, I claimed that ""there must exists a closed and bounded interval such that for all "" without proof. (In fact, I am not sure how to prove this rigorously, but I seriously believe this is correct.) Thank you very much!","[a,b] C[a,b] [a,b] \|\cdot\|_1:C[a,b]\to\mathbb{R} 
    \|f\|_1=\int_a^b|f|d\lambda.
 f\in C[a,b] \|f\|_1=0 f=0 \lambda f\in C[a,b] f=0 \|f\|_1 = \int_1^b0d\lambda = 0 \|f\|_1=\int_a^b|f|d\lambda= \sup\left\{\int hd\lambda:h\in\mathscr{S}_+\ \text{and}\ h\leq|f|\chi_{[a,b]}\right\} = 0. f(x)\neq0 x\in[a,b] |f(x)|>0 f |f| [c,d]\subseteq[a,b] |f(z)|>0 z\in[c,d] h h(z)=0 z\notin[c,d] h(z)=\min_{z\in[c,d]}|f(z)| |f| [c,d] h\in\mathscr{S}_+ h\leq|f|\chi_{[a,b]} \int hd\lambda>0 f=0 \quad (X,\mathscr{A}) \mathscr{S}_+ \mathscr{A} X \quad \chi_A A [c,d]\subseteq[a,b] |f(z)|>0 z\in[c,d]","['real-analysis', 'analysis', 'measure-theory', 'normed-spaces', 'lebesgue-integral']"
46,Understanding Proof of Proposition 2.2.5 from Measure Theory by Donald Cohn,Understanding Proof of Proposition 2.2.5 from Measure Theory by Donald Cohn,,"Overview I am self-studying Donald Cohn's Measury Theory . I have some questions about his proof of Proposition 2.2.5, and I would like to confirm if my attempt to understand his proof is correct. Here is the statement of the proposition with the proof: Proposition 2.2.5 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $\mathscr{A}_{\mu}$ be the completion of $\mathscr{A}$ under $\mu$ . Then a function $f:X\to[-\infty,+\infty]$ is $\mathscr{A}_{\mu}$ -measurable if and only if there are $\mathscr{A}$ -measurable functions $f_0,f_1:X\to[-\infty,+\infty]$ such that \begin{align}     f_0\leq f\leq f_1\ \textit{holds everywhere on $X$}\tag1 \end{align} and \begin{align}     f_0=f_1\ \textit{holds $\mu$-almost everywhere on $X$.}\tag2 \end{align} Proof $\quad$ First suppose that there exist $\mathscr{A}$ -measurable functions $f_0$ and $f_1$ that satisfy (1) and (2). Then $f_0$ is $\mathscr{A}_{\mu}$ -measurable and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. Hence, Proposition 2.2.2 applied to the space $(X,\mathscr{A}_{\mu},\overline{\mu})$ , implies that $f$ is $\mathscr{A}_{\mu}$ -measurable. Now suppose that $f:X\to[-\infty,+\infty]$ is $\mathscr{A}_{\mu}$ -measurable. If $f$ is simple and $[0,+\infty)$ -valued, say attaining values $a_1,\dots,a_k$ on the sets $A_1,\dots,A_k$ , then there are sets $B_1,\dots,B_k$ and $C_1,\dots,C_k$ that belong to $\mathscr{A}$ and satisfy $C_i\subseteq A_i\subseteq B_i$ and $\mu(B_i-C_i)=0$ for each $i$ . The functions $f_0$ and $f_1$ defined by $f_0=\sum_ia_i\chi_{C_i}$ and $f_1=\sum_ia_i\chi_{B_i}$ then satisfy (1) and (2). We can deal with the case where $f$ is simple and real-valued by applying the preceding argument to the positive and negative parts of $f$ . Finally, let $f:X\to[-\infty,+\infty]$ be an arbitrary $\mathscr{A}_{\mu}$ -measurable function, and choose a sequence $\{g_n\}$ of simple $\mathscr{A}_{\mu}$ -measurable functions from $X$ to $\mathbb{R}$ such that $f(x) = \lim_{n\to\infty}g_n(x)$ holds at each $x$ in $X$ . If for each $n$ , we choose $\mathscr{A}$ -measurable functions $g_{0,n}$ and $g_{1,n}$ such that \begin{align*}     g_{0,n}\leq g_n\leq g_{1,n}\ \text{holds everywhere on $X$} \end{align*} and \begin{align*}     g_{0,n}=g_{1,n}\ \text{holds $\mu$-almost everywhere on $X$,} \end{align*} then the required functions $f_0$ and $f_1$ can be constructed by letting $f_0$ be $\limsup_{n\to\infty}g_{0,n}$ and $f_1$ be $\liminf_{n\to\infty}g_{1,n}$ . My Questions In the first paragraph of the proof, it asserts that $f_0$ is $\mathscr{A}_{\mu}$ -measurable and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. But why? In the second paragraph of the proof, it says that "" $f$ ... attaining values $a_1,\dots,a_k$ on the sets $A_1,\dots,A_k$ . My question is why can we assume that the set of possible values attained by $f$ is finite? Still in the second paragrph, why do $f_0=\sum_ia_i\chi_{C_i}$ and $f_1=\sum_ia_i\chi_{B_i}$ satisfy (1) and (2)? The existence of $g_{0,n}$ and $g_{1,n}$ for each $n$ ? In the last paragraph, why do $f_0=\limsup_{n\to\infty}g_{0,n}$ and $f_1=\liminf_{n\to\infty}g_{1,n}$ satisfy (1) and (2)? My Attempts Here are my attempts so far: First suppose that there exist $\mathscr{A}$ -measurable functions $f_0$ and $f_1$ that satisfy (1) and (2). The measurability of $f_0$ implies that $\{x\in X:f_0(x)\leq t\}\in\mathscr{A}\subseteq\mathscr{A}_{\mu}$ . It follows that $f_0$ is $\mathscr{A}_{\mu}$ -measurable. Since $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ , we have $\{x\in X:f_0(x)\neq f_1(x)\}$ is $\mu$ -negligible. So let $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:f_0(x)\neq f_1(x)\}\subseteq A$ , and $\mu(A)=0$ . Since $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ , it follows that $\{x\in X:f(x)\neq f_0(x)\} \subseteq \{x\in X:f_0(x)\neq f_1(x)\}$ . Therefore, the set $A$ satisfies $A\in\mathscr{A}\subseteq\mathscr{A}_{\mu}$ , $\{x\in X:f(x)\neq f_0(x)\}\subseteq A$ , and $\overline{\mu}(A)=\mu(A)=0$ , so that the set $\{x\in X:f(x)\neq f_0(x)\}$ is $\overline{\mu}$ -negligible and $f=f_0$ holds $\overline{\mu}$ -almost everywhere. I still don't know why. I think the set of possible values may not even be countable. If $x\in A_i$ , then $f(x)=a_i$ . Also, $x\in A_i\subseteq B_i$ implies that $\chi_{B_i}(x)=1$ , so that $f_1(x)\geq a_i$ . Now, \begin{align*} \chi_{C_i}(x)= \begin{cases} 1\ \text{if $x \in C_i$}\\ 0\ \text{if $x \in A_i-C_i$} \end{cases}. \end{align*} Moreover if $x \in A_i-C_i$ then $x \notin C_j$ for $j\neq i$ , because otherwise if $x\in C_j\subseteq A_j$ it would contradicts $A_i\bigcap A_j=\emptyset$ for $i\neq j$ . Therefore, $f_0(x)\leq a_i$ . Thus, $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ . We now show that $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ . Consider the set $\bigcup_i(B_i-C_i)$ . Since each $B_i$ and $C_i$ belongs to $\mathscr{A}$ , we have $\bigcup_i(B_i-C_i)\in\mathscr{A}$ . Since $\mu(B_i-C_i)=0$ for each $i$ , the countable additivity of $\mu$ implies that $\mu\left(\bigcup_i(B_i-C_i)\right)=0$ . Now suppose $f_0(x)\neq f_1(x)$ for some $x$ . If $x$ is in some $C_i$ , then $x\in A_i\subseteq B_i$ and $x\notin A_j$ for $j\neq i$ and so $x\notin B_j$ for $j\neq i$ . Then $f_0(x) = a_i = f_1(x)$ , a contradiction. Therefore, $x$ is not in any $C_i$ . But $x$ must be in some $B_i$ . Thus, $x\in\bigcup_i(B_i-C_i)$ , and so $\{x\in X:f_0(x)\neq f_1(x)\}\subseteq\bigcup_i(B_i-C_i)$ . Hence, $\{x\in X:f_0(x)\neq f_1(x)\}$ is $\mu$ -negligible and $f_0=f_1$ holds $\mu$ -almost everywhere on $X$ . The third paragraph in the original proof guarantees the existence of such $g_{0,n}$ and $g_{1,n}$ for each $n$ . To show $f_0(x)\leq f(x)\leq f_1(x)$ for all $x\in X$ , note that \begin{align*} f_0(x) &= \limsup_{n\to\infty}g_{0,n}(x)\\ &=\inf_k\sup_{n\geq k}g_{0,n}(x)\\ &\leq\inf_k\sup_{n\geq k}g_{n}(x)\\ &=\limsup_{n\to\infty}g_n(x)\\ &=\lim_{n\to\infty}g_n(x)\\ &= f(x)\\ &= \lim_{n\to\infty}g_n(x)\\ &= \liminf_{n\to\infty}g_n(x)\\ &= \sup_k\inf_{n\geq K}g_n(x)\\ &\leq \sup_k\inf_{n\geq k}g_{1,n}(x)\\ &= \liminf_{n\to\infty}g_{1,n}(x)\\ &= f_1(x), \end{align*} and we are done. We now prove $f_0=f_1$ holds $\mu$ -almost everywhere. Suppose that $\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)$ ; that is, suppose that \begin{align*} \inf_k\sup_{n\geq k}g_{0,n}(x)\neq\sup_k\inf_{n\geq k}g_{1,n}(x).\tag3 \end{align*} Then $g_{0,n}\neq g_{1,n}$ for some $n$ . For otherwise, if $g_{0,n} = g_{1,n}$ , we would have equality in (3). Therefore, $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\}$ . Since $g_{0,n}=g_{1,n}$ holds $\mu$ -almost everywhere, it follows that $\{x\in X:g_{0,n}\neq g_{1,n}\}$ is $\mu$ -negligible; that is, there exists an $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A}$ , and $\mu(A)=0$ . Hence, there exists an $A\subseteq X$ such that $A\in\mathscr{A}$ , $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A}$ , and $\mu(A)=0$ , which means $\{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}$ is $\mu$ -negligible, and so $f_0=f_1$ holds $\mu$ -almost everywhere. For This Post Could someone please help me with my question 2? Moreover, I would really appreciate it if someone could help me check if my attempt for the other four questions are correct!","Overview I am self-studying Donald Cohn's Measury Theory . I have some questions about his proof of Proposition 2.2.5, and I would like to confirm if my attempt to understand his proof is correct. Here is the statement of the proposition with the proof: Proposition 2.2.5 Let be a measure space, and let be the completion of under . Then a function is -measurable if and only if there are -measurable functions such that and Proof First suppose that there exist -measurable functions and that satisfy (1) and (2). Then is -measurable and holds -almost everywhere. Hence, Proposition 2.2.2 applied to the space , implies that is -measurable. Now suppose that is -measurable. If is simple and -valued, say attaining values on the sets , then there are sets and that belong to and satisfy and for each . The functions and defined by and then satisfy (1) and (2). We can deal with the case where is simple and real-valued by applying the preceding argument to the positive and negative parts of . Finally, let be an arbitrary -measurable function, and choose a sequence of simple -measurable functions from to such that holds at each in . If for each , we choose -measurable functions and such that and then the required functions and can be constructed by letting be and be . My Questions In the first paragraph of the proof, it asserts that is -measurable and holds -almost everywhere. But why? In the second paragraph of the proof, it says that "" ... attaining values on the sets . My question is why can we assume that the set of possible values attained by is finite? Still in the second paragrph, why do and satisfy (1) and (2)? The existence of and for each ? In the last paragraph, why do and satisfy (1) and (2)? My Attempts Here are my attempts so far: First suppose that there exist -measurable functions and that satisfy (1) and (2). The measurability of implies that . It follows that is -measurable. Since holds -almost everywhere on , we have is -negligible. So let such that , , and . Since for all , it follows that . Therefore, the set satisfies , , and , so that the set is -negligible and holds -almost everywhere. I still don't know why. I think the set of possible values may not even be countable. If , then . Also, implies that , so that . Now, Moreover if then for , because otherwise if it would contradicts for . Therefore, . Thus, for all . We now show that holds -almost everywhere on . Consider the set . Since each and belongs to , we have . Since for each , the countable additivity of implies that . Now suppose for some . If is in some , then and for and so for . Then , a contradiction. Therefore, is not in any . But must be in some . Thus, , and so . Hence, is -negligible and holds -almost everywhere on . The third paragraph in the original proof guarantees the existence of such and for each . To show for all , note that and we are done. We now prove holds -almost everywhere. Suppose that ; that is, suppose that Then for some . For otherwise, if , we would have equality in (3). Therefore, . Since holds -almost everywhere, it follows that is -negligible; that is, there exists an such that , , and . Hence, there exists an such that , , and , which means is -negligible, and so holds -almost everywhere. For This Post Could someone please help me with my question 2? Moreover, I would really appreciate it if someone could help me check if my attempt for the other four questions are correct!","\quad (X,\mathscr{A},\mu) \mathscr{A}_{\mu} \mathscr{A} \mu f:X\to[-\infty,+\infty] \mathscr{A}_{\mu} \mathscr{A} f_0,f_1:X\to[-\infty,+\infty] \begin{align}
    f_0\leq f\leq f_1\ \textit{holds everywhere on X}\tag1
\end{align} \begin{align}
    f_0=f_1\ \textit{holds \mu-almost everywhere on X.}\tag2
\end{align} \quad \mathscr{A} f_0 f_1 f_0 \mathscr{A}_{\mu} f=f_0 \overline{\mu} (X,\mathscr{A}_{\mu},\overline{\mu}) f \mathscr{A}_{\mu} f:X\to[-\infty,+\infty] \mathscr{A}_{\mu} f [0,+\infty) a_1,\dots,a_k A_1,\dots,A_k B_1,\dots,B_k C_1,\dots,C_k \mathscr{A} C_i\subseteq A_i\subseteq B_i \mu(B_i-C_i)=0 i f_0 f_1 f_0=\sum_ia_i\chi_{C_i} f_1=\sum_ia_i\chi_{B_i} f f f:X\to[-\infty,+\infty] \mathscr{A}_{\mu} \{g_n\} \mathscr{A}_{\mu} X \mathbb{R} f(x) = \lim_{n\to\infty}g_n(x) x X n \mathscr{A} g_{0,n} g_{1,n} \begin{align*}
    g_{0,n}\leq g_n\leq g_{1,n}\ \text{holds everywhere on X}
\end{align*} \begin{align*}
    g_{0,n}=g_{1,n}\ \text{holds \mu-almost everywhere on X,}
\end{align*} f_0 f_1 f_0 \limsup_{n\to\infty}g_{0,n} f_1 \liminf_{n\to\infty}g_{1,n} f_0 \mathscr{A}_{\mu} f=f_0 \overline{\mu} f a_1,\dots,a_k A_1,\dots,A_k f f_0=\sum_ia_i\chi_{C_i} f_1=\sum_ia_i\chi_{B_i} g_{0,n} g_{1,n} n f_0=\limsup_{n\to\infty}g_{0,n} f_1=\liminf_{n\to\infty}g_{1,n} \mathscr{A} f_0 f_1 f_0 \{x\in X:f_0(x)\leq t\}\in\mathscr{A}\subseteq\mathscr{A}_{\mu} f_0 \mathscr{A}_{\mu} f_0=f_1 \mu X \{x\in X:f_0(x)\neq f_1(x)\} \mu A\subseteq X A\in\mathscr{A} \{x\in X:f_0(x)\neq f_1(x)\}\subseteq A \mu(A)=0 f_0(x)\leq f(x)\leq f_1(x) x\in X \{x\in X:f(x)\neq f_0(x)\} \subseteq \{x\in X:f_0(x)\neq f_1(x)\} A A\in\mathscr{A}\subseteq\mathscr{A}_{\mu} \{x\in X:f(x)\neq f_0(x)\}\subseteq A \overline{\mu}(A)=\mu(A)=0 \{x\in X:f(x)\neq f_0(x)\} \overline{\mu} f=f_0 \overline{\mu} x\in A_i f(x)=a_i x\in A_i\subseteq B_i \chi_{B_i}(x)=1 f_1(x)\geq a_i \begin{align*}
\chi_{C_i}(x)=
\begin{cases}
1\ \text{if x \in C_i}\\
0\ \text{if x \in A_i-C_i}
\end{cases}.
\end{align*} x \in A_i-C_i x \notin C_j j\neq i x\in C_j\subseteq A_j A_i\bigcap A_j=\emptyset i\neq j f_0(x)\leq a_i f_0(x)\leq f(x)\leq f_1(x) x\in X f_0=f_1 \mu X \bigcup_i(B_i-C_i) B_i C_i \mathscr{A} \bigcup_i(B_i-C_i)\in\mathscr{A} \mu(B_i-C_i)=0 i \mu \mu\left(\bigcup_i(B_i-C_i)\right)=0 f_0(x)\neq f_1(x) x x C_i x\in A_i\subseteq B_i x\notin A_j j\neq i x\notin B_j j\neq i f_0(x) = a_i = f_1(x) x C_i x B_i x\in\bigcup_i(B_i-C_i) \{x\in X:f_0(x)\neq f_1(x)\}\subseteq\bigcup_i(B_i-C_i) \{x\in X:f_0(x)\neq f_1(x)\} \mu f_0=f_1 \mu X g_{0,n} g_{1,n} n f_0(x)\leq f(x)\leq f_1(x) x\in X \begin{align*}
f_0(x) &= \limsup_{n\to\infty}g_{0,n}(x)\\
&=\inf_k\sup_{n\geq k}g_{0,n}(x)\\
&\leq\inf_k\sup_{n\geq k}g_{n}(x)\\
&=\limsup_{n\to\infty}g_n(x)\\
&=\lim_{n\to\infty}g_n(x)\\
&= f(x)\\
&= \lim_{n\to\infty}g_n(x)\\
&= \liminf_{n\to\infty}g_n(x)\\
&= \sup_k\inf_{n\geq K}g_n(x)\\
&\leq \sup_k\inf_{n\geq k}g_{1,n}(x)\\
&= \liminf_{n\to\infty}g_{1,n}(x)\\
&= f_1(x),
\end{align*} f_0=f_1 \mu \limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x) \begin{align*}
\inf_k\sup_{n\geq k}g_{0,n}(x)\neq\sup_k\inf_{n\geq k}g_{1,n}(x).\tag3
\end{align*} g_{0,n}\neq g_{1,n} n g_{0,n} = g_{1,n} \{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\} g_{0,n}=g_{1,n} \mu \{x\in X:g_{0,n}\neq g_{1,n}\} \mu A\subseteq X A\in\mathscr{A} \{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A} \mu(A)=0 A\subseteq X A\in\mathscr{A} \{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\}\subseteq\{x\in X:g_{0,n}\neq g_{1,n}\}\in\mathscr{A} \mu(A)=0 \{x\in X:\limsup_{n\to\infty}g_{0,n}(x)\neq\liminf_{n\to\infty}g_{1,n}(x)\} \mu f_0=f_1 \mu","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation', 'almost-everywhere']"
47,"How to show that, the weak topology is the coarsest topology such that all $f:E \rightarrow \mathbb{K}$ are continuous?","How to show that, the weak topology is the coarsest topology such that all  are continuous?",f:E \rightarrow \mathbb{K},"Let E be a normed space, $E':=\{f:E \rightarrow \mathbb{K}| f \text{ is continuous and linear}\}$ . Define $p_f(x):=|f(x)| where f \in E'$ and $x \in E$ . Consider the family of seminorms $\mathcal{P}=\{p_f |f \in E'\}$ . The topology induced by $\mathcal{P}$ is called the weak topology. The weak topology is the coarsest topology such that every $f \in E'$ is continuous. I would like to show the topology induced by $\mathcal{P}$ is the ""coarsest topology such that every $f \in E'$ is continuous"". I will call the topology induced by $\mathcal{P}$ $\tau_{\mathcal{P}}$ . I have thought about two ways to approach this: Approach 1: My first idea was to just go by the definition of coarsest topology, i.e. Suppose $\tau$ is a topolgy such that  every $f \in E'$ is continuous. I need to show that $\tau_{\mathcal{P}} \subseteq \tau$ . Approach 2: The idea hear is to use topology to ""generate"" the coarsest topology and then show that it is euqal to $\tau_{\mathcal{P}}$ . Let $X$ be a set and let Pow denote the power set. I know that if $\mathcal{S}\subseteq Pow(X)$ . Then $\mathcal{S}$ is a subbasis for the coarsest topology $\tau$ on $X$ such all Elements of $\mathcal{S}$ are open. I thought about defining $\mathcal{S}:=\{f^{-1}(U)| f \in E' \text{ and } U \text{ open in } \mathbb{K}\}$ . By defining $\mathcal{B}:=\{\cap_{S \in \mathcal{F}} |\mathcal{F} \subseteq S, \mathcal{F} \text{is finite}\}$ I can obtain a basis for the topology. If I now let $\tau:=\{\cup_{B \in \mathcal{C}} B|\mathcal{C} \subset \mathcal{B} \}$ I obtain the coarsest topology such that each $f \in E'$ is continuous, i.e. $\tau \subseteq \tau_{\mathcal{P}}$ . The only thing left to show is: $\tau_{\mathcal{P}} \subseteq \tau$ . So, let $U \in \tau_{\mathcal{P}}$ . Being open w.r.t $\tau_{\mathcal{P}}$ means that for every $x_0 \in U$ there exist $p_1,...,p_n$ and $r_1,...,r_n$ such that $\cap_{i=1}^n B_{p_i}(x_0,r_i) \subseteq U$ , where $B_{p}(x_0,r):=\{x \in E |p(x-x_0)<r\}$ . For $U$ to be open w.r.t $\tau$ means that there exist $f_1,...,f_n$ and $U_1,..., U_n$ open in $\mathbb{K}$ such that $\cap_{i=1}^n f^{-1}_i(U_i) \subseteq U$ . But I do not know how to argue that final step. Help or solution would be very appreciated.","Let E be a normed space, . Define and . Consider the family of seminorms . The topology induced by is called the weak topology. The weak topology is the coarsest topology such that every is continuous. I would like to show the topology induced by is the ""coarsest topology such that every is continuous"". I will call the topology induced by . I have thought about two ways to approach this: Approach 1: My first idea was to just go by the definition of coarsest topology, i.e. Suppose is a topolgy such that  every is continuous. I need to show that . Approach 2: The idea hear is to use topology to ""generate"" the coarsest topology and then show that it is euqal to . Let be a set and let Pow denote the power set. I know that if . Then is a subbasis for the coarsest topology on such all Elements of are open. I thought about defining . By defining I can obtain a basis for the topology. If I now let I obtain the coarsest topology such that each is continuous, i.e. . The only thing left to show is: . So, let . Being open w.r.t means that for every there exist and such that , where . For to be open w.r.t means that there exist and open in such that . But I do not know how to argue that final step. Help or solution would be very appreciated.","E':=\{f:E \rightarrow \mathbb{K}| f \text{ is continuous and linear}\} p_f(x):=|f(x)| where f \in E' x \in E \mathcal{P}=\{p_f |f \in E'\} \mathcal{P} f \in E' \mathcal{P} f \in E' \mathcal{P} \tau_{\mathcal{P}} \tau f \in E' \tau_{\mathcal{P}} \subseteq \tau \tau_{\mathcal{P}} X \mathcal{S}\subseteq Pow(X) \mathcal{S} \tau X \mathcal{S} \mathcal{S}:=\{f^{-1}(U)| f \in E' \text{ and } U \text{ open in } \mathbb{K}\} \mathcal{B}:=\{\cap_{S \in \mathcal{F}} |\mathcal{F} \subseteq S, \mathcal{F} \text{is finite}\} \tau:=\{\cup_{B \in \mathcal{C}} B|\mathcal{C} \subset \mathcal{B} \} f \in E' \tau \subseteq \tau_{\mathcal{P}} \tau_{\mathcal{P}} \subseteq \tau U \in \tau_{\mathcal{P}} \tau_{\mathcal{P}} x_0 \in U p_1,...,p_n r_1,...,r_n \cap_{i=1}^n B_{p_i}(x_0,r_i) \subseteq U B_{p}(x_0,r):=\{x \in E |p(x-x_0)<r\} U \tau f_1,...,f_n U_1,..., U_n \mathbb{K} \cap_{i=1}^n f^{-1}_i(U_i) \subseteq U","['general-topology', 'functional-analysis', 'analysis', 'locally-convex-spaces', 'weak-topology']"
48,Markov Property of a Ito Process,Markov Property of a Ito Process,,"This is the excercise: Let $B_t$ be 1d Brownian motion with $B_0=0$ . Define $$ X_t=X_t^x=x\cdot\exp\left(ct+\alpha B_t\right) $$ where $c,\alpha$ are constants and $x$ is non-random. Prove directly from the definition that $X_t$ is a Markov process, i.e., that $$ \mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right] $$ for bounded Borel-measurable $f$ . Here is my attempt Let $\mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right)$ and $\mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right)$ . Since $X_s$ is $\mathcal{F}_t^B$ -measurable then $\mathcal{F}_t^X\subset\mathcal{F}_t^B$ we have, for any bounded Borel function $f(x)$ , $$ \begin{aligned} \mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]&=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^X\right]\\ &=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^B\right]\\ &=\mathbb{E}^{B_t}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\right]\in\sigma(B_t)=\sigma(X_t). \end{aligned} $$ So $\mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]=\mathbb{E}\left[f\left(X_{t+h}\right)|X_t\right]$ Let $\mathcal{M}_t=\mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right)$ and $\mathcal{F}_t=\mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right)$ . Since $X_s$ is $\mathcal{F}_t^B$ -measurable then $\mathcal{F}_t^X\subset\mathcal{F}_t^B$ we have, for any bounded Borel function $f(x)$ , $$ \begin{aligned} \mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+ch+\alpha B_{t+h}\pm\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left( B_{t+h}-B_{h}\right)+ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right] \end{aligned} $$ Notice that $\hat{B_{t}}=B_{t+h}-B_{h}$ is a Brownian Motion and it is equal in distribution to $B_{t}$ , so $$ \begin{aligned} \mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\ &=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)\cdot x\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\ &=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\ &=\mathbb{E}\left[f\left(\exp\left(ct+\alpha B_t \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha B_t \right)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right] \end{aligned} $$ And at the same time, if we where to start at $y=X_t=x\cdot\exp(ct+\alpha B_t)$ , we would have that $$ \begin{aligned} \mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\ &=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|X_t\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right]. \end{aligned} $$ Since $\sigma(X_t)=\sigma(B_t)$ , we have that $$ \begin{aligned} \mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\ &=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right]\\ &=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]. \end{aligned} $$ In other words, we have just prove that $$ \mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]. $$ Could anyone please check if my reasoning is correct? Thanks","This is the excercise: Let be 1d Brownian motion with . Define where are constants and is non-random. Prove directly from the definition that is a Markov process, i.e., that for bounded Borel-measurable . Here is my attempt Let and . Since is -measurable then we have, for any bounded Borel function , So Let and . Since is -measurable then we have, for any bounded Borel function , Notice that is a Brownian Motion and it is equal in distribution to , so And at the same time, if we where to start at , we would have that Since , we have that In other words, we have just prove that Could anyone please check if my reasoning is correct? Thanks","B_t B_0=0 
X_t=X_t^x=x\cdot\exp\left(ct+\alpha B_t\right)
 c,\alpha x X_t 
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]
 f \mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right) \mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right) X_s \mathcal{F}_t^B \mathcal{F}_t^X\subset\mathcal{F}_t^B f(x) 
\begin{aligned}
\mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]&=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^X\right]\\
&=\mathbb{E}^x\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t^B\right]\\
&=\mathbb{E}^{B_t}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\right]\in\sigma(B_t)=\sigma(X_t).
\end{aligned}
 \mathbb{E}^x\left[f\left(X_{t+h}\right)\bigg|\mathcal{F}_t^X\right]=\mathbb{E}\left[f\left(X_{t+h}\right)|X_t\right] \mathcal{M}_t=\mathcal{F}_t^X:=\sigma\left(X_s:s\le t\right) \mathcal{F}_t=\mathcal{F}_t^B:=\sigma\left(B_s:s\le t\right) X_s \mathcal{F}_t^B \mathcal{F}_t^X\subset\mathcal{F}_t^B f(x) 
\begin{aligned}
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(c(t+h)+\alpha B_{t+h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+ch+\alpha B_{t+h}\pm\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left( B_{t+h}-B_{h}\right)+ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]
\end{aligned}
 \hat{B_{t}}=B_{t+h}-B_{h} B_{t} 
\begin{aligned}
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha\left(B_{t+h}-B_{h}\right)\right)\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\mathcal{F}_t\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)\cdot x\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha\hat{B_{t}} \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(\exp\left(ct+\alpha B_t \right)X_h\right)\bigg|\sigma\left(B_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp\left(ct+\alpha B_t \right)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right]
\end{aligned}
 y=X_t=x\cdot\exp(ct+\alpha B_t) 
\begin{aligned}
\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|X_t\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right].
\end{aligned}
 \sigma(X_t)=\sigma(B_t) 
\begin{aligned}
\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right]&=\mathbb{E}\left[f\left(y\cdot\exp\left(ch+\alpha B_{h}\right)\right)\right]\bigg|_{y=X_t}\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(X_s:s\le t\right)\right]\\
&=\mathbb{E}\left[f\left(x\cdot\exp(ct+\alpha B_t)\cdot\exp\left(ch+\alpha B_{h}\right)\right)\bigg|\sigma\left(B_s:s\le t\right)\right].
\end{aligned}
 
\mathbb{E}\left[f\left(X_{t+h}\right)|\mathcal{F}_t\right]=\mathbb{E}^{X_{t}}\left[f\left(X_{h}\right)\right].
","['analysis', 'stochastic-processes', 'brownian-motion', 'markov-process', 'stochastic-differential-equations']"
49,Does every $\sigma$-homomorphism of a probability algebra come from a measure preserving transformation on the probability space?,Does every -homomorphism of a probability algebra come from a measure preserving transformation on the probability space?,\sigma,"Let $(X,\mathcal{A},\mu)$ be an atomless probability space and let $A\sim B$ whenever $\mu(A\vartriangle B)=0$ for each $A$ and $B$ in $\mathcal{A}$ . This way, if $\mathbb{A}$ is the set of $\sim$ -equivalences classes in $\mathcal{A}$ , then $\mathbb{A}$ inherits $\cap,\cup,\cdot^c$ and $\mu$ from $(X,\mathcal{A},\mu)$ , becoming a probability algebra. We can define a complete metric in $\mathbb{A}$ by $d([A],[B]):=\mu(A\vartriangle B)$ , making it a structure in the sense of continuous model theory. Given any measure preserving trasnformation $T:X\rightarrow X$ , it defines a measure preserving $\sigma$ -homomorphism $\check{T}:\mathbb{A}\rightarrow\mathbb{A}$ by $\check{T}([A]):=[T^{-1}(A)]$ . Is it true that for any measure preserving $\sigma$ -homomorphism $\tau:\mathbb{A}\rightarrow\mathbb{A}$ there exists a measure preserving transformation $T:X\rightarrow X$ such that $\tau=\check{T}$ ? Is there a reference for such a theorem? If not, could you give a counterexample of such a $\tau$ ? I know that this is true if $(X,\mathcal{A},\mu)$ is a Lebesgue-standard space (i.e. if $\mathbb{A}$ is separable), it is a theorem in Royden. But is it still true if $\mathbb{A}$ has a higher metric density? It would be perfectly ok if this answer is restricted to just automorphisms. Thanks, I appreciate any answer.","Let be an atomless probability space and let whenever for each and in . This way, if is the set of -equivalences classes in , then inherits and from , becoming a probability algebra. We can define a complete metric in by , making it a structure in the sense of continuous model theory. Given any measure preserving trasnformation , it defines a measure preserving -homomorphism by . Is it true that for any measure preserving -homomorphism there exists a measure preserving transformation such that ? Is there a reference for such a theorem? If not, could you give a counterexample of such a ? I know that this is true if is a Lebesgue-standard space (i.e. if is separable), it is a theorem in Royden. But is it still true if has a higher metric density? It would be perfectly ok if this answer is restricted to just automorphisms. Thanks, I appreciate any answer.","(X,\mathcal{A},\mu) A\sim B \mu(A\vartriangle B)=0 A B \mathcal{A} \mathbb{A} \sim \mathcal{A} \mathbb{A} \cap,\cup,\cdot^c \mu (X,\mathcal{A},\mu) \mathbb{A} d([A],[B]):=\mu(A\vartriangle B) T:X\rightarrow X \sigma \check{T}:\mathbb{A}\rightarrow\mathbb{A} \check{T}([A]):=[T^{-1}(A)] \sigma \tau:\mathbb{A}\rightarrow\mathbb{A} T:X\rightarrow X \tau=\check{T} \tau (X,\mathcal{A},\mu) \mathbb{A} \mathbb{A}","['analysis', 'measure-theory', 'logic', 'model-theory', 'ergodic-theory']"
50,A remarkable Definite Integral by Glasser 2013,A remarkable Definite Integral by Glasser 2013,,"Just was stuck by "" A remarkable Definite Integral "" From formula (3), if I substitute $a=1$ , then \begin{align} \int_{-\infty}^{\infty}\frac{2e^{-(x^2+i\pi x)t}}{e^x+e^{-x}}dx=\pi e^{-\frac{\pi^2}{4}t} \end{align} But I couldn't proceed further. Would someone can give hints on what tricks can be used to solve this integral? EDIT reminded by the comments that I have limited myself through converting $\cosh$ to $e^x+e^{-x}$ and then missed possibility of the pole $\frac{i\pi}{2}$ .","Just was stuck by "" A remarkable Definite Integral "" From formula (3), if I substitute , then But I couldn't proceed further. Would someone can give hints on what tricks can be used to solve this integral? EDIT reminded by the comments that I have limited myself through converting to and then missed possibility of the pole .","a=1 \begin{align}
\int_{-\infty}^{\infty}\frac{2e^{-(x^2+i\pi x)t}}{e^x+e^{-x}}dx=\pi e^{-\frac{\pi^2}{4}t}
\end{align} \cosh e^x+e^{-x} \frac{i\pi}{2}","['integration', 'analysis']"
51,Explanation of the proof of Theorem 6.6 in Rudin's Functional Analysis,Explanation of the proof of Theorem 6.6 in Rudin's Functional Analysis,,"Everything that follows is from Rudin's Functional Analysis : $\def\L{\Lambda} \def\DDD{\mathcal{D}} \def\sbe{\subseteq} \def\W{\Omega} \def\RR{\mathbb{R}} \def\CC{\mathbb{C}} $ Below $\DDD$ is the space of test (smooth, compactly supported) functions from an open set $\Omega\sbe\RR^n$ to $\CC$ . Such space is given a complete, unmetrizable topology $\tau$ . Similarly $\DDD_K$ is the space of test functions $\Omega\to\CC$ whose support lies in the compact set $K$ . Each $\DDD_K$ has a Fréchet space topology $\tau_K$ which corresponds with the subspace topology under $\tau$ . Theorem 6.6: suppose $\L$ is a linear mapping of $\DDD$ into a lctvs $Y$ . Then the following are equivalent: a) $\L$ is continuous. b) $\L$ is bounded. c) If $\phi_i\to 0$ in $\DDD(\W)$ , then $\L\phi_i\to0$ in $Y$ . d) The restriction of $\L$ to any $\DDD_K\sbe\DDD(\W)$ is continuous. The proof shows a) $\implies$ b) $\implies$ c) $\implies$ d) $\implies$ a). I struggle to understand the steps b) $\implies$ c) and d) $\implies$ a): b) $\implies$ c): there is a compact subset $K\sbe\W$ that contains the support of every $\phi_i$ , thus the $\phi_i$ are members of the subset $\DDD_K$ and $\phi_i\to 0$ holds in $\DDD_K$ . The restriction of $\L$ to this $\DDD_K$ is bounded (why?) and since $\DDD_K$ is metrizable it follows that $\L\phi_i\to 0$ in $Y$ . d) $\implies$ a): let $U$ be a convex balanced neighborhood of $0$ in $Y$ , and put $V=\L^{-1}(U)$ . Then $V$ is convex and balanced (why?) . Now $V$ is open if and only if $\DDD_K\cap V\in\tau_K$ for every compact $K\sbe \W$ (I understand the only if implication only) . This proves the equivalence of a) and d) (how?) .","Everything that follows is from Rudin's Functional Analysis : Below is the space of test (smooth, compactly supported) functions from an open set to . Such space is given a complete, unmetrizable topology . Similarly is the space of test functions whose support lies in the compact set . Each has a Fréchet space topology which corresponds with the subspace topology under . Theorem 6.6: suppose is a linear mapping of into a lctvs . Then the following are equivalent: a) is continuous. b) is bounded. c) If in , then in . d) The restriction of to any is continuous. The proof shows a) b) c) d) a). I struggle to understand the steps b) c) and d) a): b) c): there is a compact subset that contains the support of every , thus the are members of the subset and holds in . The restriction of to this is bounded (why?) and since is metrizable it follows that in . d) a): let be a convex balanced neighborhood of in , and put . Then is convex and balanced (why?) . Now is open if and only if for every compact (I understand the only if implication only) . This proves the equivalence of a) and d) (how?) .",\def\L{\Lambda} \def\DDD{\mathcal{D}} \def\sbe{\subseteq} \def\W{\Omega} \def\RR{\mathbb{R}} \def\CC{\mathbb{C}}  \DDD \Omega\sbe\RR^n \CC \tau \DDD_K \Omega\to\CC K \DDD_K \tau_K \tau \L \DDD Y \L \L \phi_i\to 0 \DDD(\W) \L\phi_i\to0 Y \L \DDD_K\sbe\DDD(\W) \implies \implies \implies \implies \implies \implies \implies K\sbe\W \phi_i \phi_i \DDD_K \phi_i\to 0 \DDD_K \L \DDD_K \DDD_K \L\phi_i\to 0 Y \implies U 0 Y V=\L^{-1}(U) V V \DDD_K\cap V\in\tau_K K\sbe \W,"['functional-analysis', 'analysis', 'proof-explanation', 'distribution-theory']"
52,Uniform Boundedness of a $C_0$-group,Uniform Boundedness of a -group,C_0,"Let $p\in [1,\infty)$ . On $X=L^p(\mathbb{R}; \mathbb{C}^2)$ we consider the operator $D=A+B$ , where $A=\begin{pmatrix}-\partial_x & 0 \\ 0 & \partial_x \end{pmatrix}\quad{and}\quad  B=c(x)\begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix}$ (with domain $\mathsf{Dom}(D)=W^{1,p}(\mathbb{R};\mathbb{C}^2)$ ). Here, $c\colon \mathbb{R}\to \mathbb{R}$ is a real-valued $L^\infty$ -function. My question is: Does $D$ generate a bounded $C_0$ -group on $X$ ? My thoughts so far: It is clear that $A$ generates a bounded $C_0$ -group on $X$ . In fact, $(e^{tA}f)(x)=\begin{pmatrix} f_1(x-t)\\ f_2(x+t)\end{pmatrix}$ for a.e. $x\in \mathbb{R}$ , all $t\in \mathbb{R}$ and all $f=(f_1,f_2)\in X$ , from which we immediately get that $(e^{tA})_{t\in \mathbb{R}}$ is a $C_0$ -group of contractions on $X$ . Now, as $B$ is bounded linear operator on $X$ , it follows from standard pertubation theory that $D$ is the generator of a $C_0$ -group $(e^{tD})_{t\in \mathbb{R}}$ . However, pertubation theory only guarantees a growth rate $\omega\geq 0$ and and $M\geq 1$ such that $\|e^{tD}\|\leq Me^{\omega |t|}$ for all $t\in \mathbb{R}$ and this $\omega$ can be strictly positive in general (consider for example the case where $B$ was the identity matrix). So, the question is if $\omega$ can be chosen to be zero, and this seems to be a much more delicate matter. If $f=(f_1,f_2)\in \mathsf{Dom}(D)$ , then $(u,v)(t):=e^{tD}f$ for $t\in \mathbb{R}$ solves the abstract Cauchy problem $(\ast) \begin{cases} u'(t)=-\partial_x u(t)+cv(t) \quad (t\in \mathbb{R}), \quad u(0)=f_1,\\ v'(t)=\hphantom{-}\partial_x u(t)-cu(t) \quad (t\in \mathbb{R}), \quad v(0)=f_2. \end{cases} $ The boundedness of $(e^{tD})_{t\in \mathbb{R}}$ is then equivalent to the existence of $M\geq 1$ such that the estimate $\|(u,v)(t)\|_{X}\leq M \|(f_1,f_2)\|_X$ for all $t\in \mathbb{R}$ holds. The problem is that I am not able to find any reasonable representation of the solution of $(\ast)$ which allows me estimate its $X$ -norm in an effective way. Here, the problem is that $(\ast)$ is coupled (it is a coupled system of transport equations) and that $c$ is only asssumed to be $L^\infty$ which makes taking one additional time derivative in order to obtain a decoupled wave equation problematic. My hope is that $(e^{tD})_{t\in \mathbb{R}}$ is indeed bounded, and if not I am curious to know if it is possible to give (smallness-)conditions on $c$ that make the $C_0$ -group bounded. Any hints would be greatly appreciated.","Let . On we consider the operator , where (with domain ). Here, is a real-valued -function. My question is: Does generate a bounded -group on ? My thoughts so far: It is clear that generates a bounded -group on . In fact, for a.e. , all and all , from which we immediately get that is a -group of contractions on . Now, as is bounded linear operator on , it follows from standard pertubation theory that is the generator of a -group . However, pertubation theory only guarantees a growth rate and and such that for all and this can be strictly positive in general (consider for example the case where was the identity matrix). So, the question is if can be chosen to be zero, and this seems to be a much more delicate matter. If , then for solves the abstract Cauchy problem The boundedness of is then equivalent to the existence of such that the estimate for all holds. The problem is that I am not able to find any reasonable representation of the solution of which allows me estimate its -norm in an effective way. Here, the problem is that is coupled (it is a coupled system of transport equations) and that is only asssumed to be which makes taking one additional time derivative in order to obtain a decoupled wave equation problematic. My hope is that is indeed bounded, and if not I am curious to know if it is possible to give (smallness-)conditions on that make the -group bounded. Any hints would be greatly appreciated.","p\in [1,\infty) X=L^p(\mathbb{R}; \mathbb{C}^2) D=A+B A=\begin{pmatrix}-\partial_x & 0 \\ 0 & \partial_x \end{pmatrix}\quad{and}\quad  B=c(x)\begin{pmatrix}0 & 1 \\ -1 & 0\end{pmatrix} \mathsf{Dom}(D)=W^{1,p}(\mathbb{R};\mathbb{C}^2) c\colon \mathbb{R}\to \mathbb{R} L^\infty D C_0 X A C_0 X (e^{tA}f)(x)=\begin{pmatrix} f_1(x-t)\\ f_2(x+t)\end{pmatrix} x\in \mathbb{R} t\in \mathbb{R} f=(f_1,f_2)\in X (e^{tA})_{t\in \mathbb{R}} C_0 X B X D C_0 (e^{tD})_{t\in \mathbb{R}} \omega\geq 0 M\geq 1 \|e^{tD}\|\leq Me^{\omega |t|} t\in \mathbb{R} \omega B \omega f=(f_1,f_2)\in \mathsf{Dom}(D) (u,v)(t):=e^{tD}f t\in \mathbb{R} (\ast) \begin{cases}
u'(t)=-\partial_x u(t)+cv(t) \quad (t\in \mathbb{R}), \quad u(0)=f_1,\\
v'(t)=\hphantom{-}\partial_x u(t)-cu(t) \quad (t\in \mathbb{R}), \quad v(0)=f_2.
\end{cases}
 (e^{tD})_{t\in \mathbb{R}} M\geq 1 \|(u,v)(t)\|_{X}\leq M \|(f_1,f_2)\|_X t\in \mathbb{R} (\ast) X (\ast) c L^\infty (e^{tD})_{t\in \mathbb{R}} c C_0","['functional-analysis', 'analysis', 'partial-differential-equations', 'semigroup-of-operators', 'transport-equation']"
53,help computing an integral/hankel transform,help computing an integral/hankel transform,,"I'm having some trouble computing the following integral/Hankel Transform: $$\int_0^\infty \frac{ue^{-u}}{u^2 + a^2} J_0(2\sqrt{u}) du$$ where $a$ is some real number. The actual integral I want to compute is $$\int_0^\infty \frac{ue^{-u}}{u^2 + (2\pi n)^2} I_0(2\sqrt{bui}) du$$ where $n$ is a positive integer, and $b$ is a real, positive number. I'm sure the computations are symmetric, so the first integral (probably) is sufficient. An integral I have encountered in my search that could help are $$\int_0^\infty \frac{x \sin(ax)}{x^2 + b^2} J_0(yx) dx = \frac{\pi}{2} e^{-ab} I_0(by)$$ for $y \leq a$ and $$\int_0^\infty \frac{x \cos(ax)}{x^2 + b^2} J_0(yx) dx = \cosh(ab) K_0(by)$$ for $y \geq a$ from the link: Double integral with Hankel transform Since $\sin,\cos$ is just related to the exponential, this could help. Thank you so much!","I'm having some trouble computing the following integral/Hankel Transform: where is some real number. The actual integral I want to compute is where is a positive integer, and is a real, positive number. I'm sure the computations are symmetric, so the first integral (probably) is sufficient. An integral I have encountered in my search that could help are for and for from the link: Double integral with Hankel transform Since is just related to the exponential, this could help. Thank you so much!","\int_0^\infty \frac{ue^{-u}}{u^2 + a^2} J_0(2\sqrt{u}) du a \int_0^\infty \frac{ue^{-u}}{u^2 + (2\pi n)^2} I_0(2\sqrt{bui}) du n b \int_0^\infty \frac{x \sin(ax)}{x^2 + b^2} J_0(yx) dx = \frac{\pi}{2} e^{-ab} I_0(by) y \leq a \int_0^\infty \frac{x \cos(ax)}{x^2 + b^2} J_0(yx) dx = \cosh(ab) K_0(by) y \geq a \sin,\cos","['integration', 'complex-analysis', 'analysis', 'definite-integrals', 'bessel-functions']"
54,A question on Beukers proof of irrationality of $\zeta(3)$,A question on Beukers proof of irrationality of,\zeta(3),"I am reading the paper A note on the Irrationality of ζ(2) and ζ(3) by F. Beukers . In equation $(7)$ , we have $$I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nw^n(1-w)^n}{(1-(1-xy)w)^{n+1}} dx dy dw \tag{1}$$ Using this above equation, I need to prove that there exists $A_n,B_n\in\mathbb{Z}$ such that $$I_n=\frac{A_n+B_n\zeta(3)}{d_n^3}\tag{2}$$ where $d_n=  \operatorname{lcm}  (1,2,...,n)$ I tried using $n$ -fold partial integration by parts with respect to $w$ in $(1)$ by taking $u=w^n(1-w)^n$ and $v=\frac{x^n(1-x)^ny^n(1-y)^n}{(1-(1-xy)w)^{n+1}}$ , so that $$I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nP_n(w)}{(1-(1-xy)w)(1-xy)^n} dx dy dw$$ where $P_n(w)=\frac{d^n}{dw^n}(\frac{w^n(1-w)^n}{n!})$ is the shifted Legendre Polynomial. Now $$P_n(w)=\sum_{i=0}^{n}(-1)^i \binom{n}{i}\binom{n+i}{i} w^i$$ $$(1-x)^n=\sum_{j=0}^{n}(-1)^j \binom{n}{j} x^j$$ $$(1-y)^n=\sum_{k=0}^{n}(-1)^k \binom{n}{k} y^k$$ $$\frac{1}{1-(1-xy)w}=\sum_{r=0}^{\infty}(1-xy)^r w^r$$ So $I_n$ becomes $$I_n=\sum_{i,j,k=0}^n(-1)^{i+j+k}\binom{n}{i}\binom{n+i}{i}\binom{n}{j}\binom{n}{k}\sum_{r=0}^{\infty}\int_{(0,1)^3} \frac{x^{n+j} y^{n+k} w^{i+r}(1-xy)^r}{(1-xy)^n}dx dy dw$$ Thank You.","I am reading the paper A note on the Irrationality of ζ(2) and ζ(3) by F. Beukers . In equation , we have Using this above equation, I need to prove that there exists such that where I tried using -fold partial integration by parts with respect to in by taking and , so that where is the shifted Legendre Polynomial. Now So becomes Thank You.","(7) I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nw^n(1-w)^n}{(1-(1-xy)w)^{n+1}} dx dy dw \tag{1} A_n,B_n\in\mathbb{Z} I_n=\frac{A_n+B_n\zeta(3)}{d_n^3}\tag{2} d_n=  \operatorname{lcm}  (1,2,...,n) n w (1) u=w^n(1-w)^n v=\frac{x^n(1-x)^ny^n(1-y)^n}{(1-(1-xy)w)^{n+1}} I_n=\int_{(0,1)^3}\frac{x^n(1-x)^ny^n(1-y)^nP_n(w)}{(1-(1-xy)w)(1-xy)^n} dx dy dw P_n(w)=\frac{d^n}{dw^n}(\frac{w^n(1-w)^n}{n!}) P_n(w)=\sum_{i=0}^{n}(-1)^i \binom{n}{i}\binom{n+i}{i} w^i (1-x)^n=\sum_{j=0}^{n}(-1)^j \binom{n}{j} x^j (1-y)^n=\sum_{k=0}^{n}(-1)^k \binom{n}{k} y^k \frac{1}{1-(1-xy)w}=\sum_{r=0}^{\infty}(1-xy)^r w^r I_n I_n=\sum_{i,j,k=0}^n(-1)^{i+j+k}\binom{n}{i}\binom{n+i}{i}\binom{n}{j}\binom{n}{k}\sum_{r=0}^{\infty}\int_{(0,1)^3} \frac{x^{n+j} y^{n+k} w^{i+r}(1-xy)^r}{(1-xy)^n}dx dy dw","['complex-analysis', 'analysis', 'number-theory', 'analytic-number-theory', 'riemann-zeta']"
55,A question about the Hardy-Littlewood maximal function,A question about the Hardy-Littlewood maximal function,,Let $h \in L^1(\mathbb{R}).$ Consider the Hardy-Littlewood maximal function defined as $$h^*(b) = \sup_{t > 0}\frac{1}{2t}\int_{b-t}^{b+t}h.$$ Does it hold that $\{b\in \mathbb{R} : h^*(b) = \infty\}$ is a closed set?,Let Consider the Hardy-Littlewood maximal function defined as Does it hold that is a closed set?,h \in L^1(\mathbb{R}). h^*(b) = \sup_{t > 0}\frac{1}{2t}\int_{b-t}^{b+t}h. \{b\in \mathbb{R} : h^*(b) = \infty\},"['real-analysis', 'analysis', 'measure-theory']"
56,Subset of $X$ closed under countable disjoint unions and complements a $\sigma$-algebra?,Subset of  closed under countable disjoint unions and complements a -algebra?,X \sigma,"Be wary not to confuse this post with this one . In this case, $M$ is simply a subset of the power set of $X$ . Let $X$ be a set and $M$ be a nonempty collection of subsets of $X$ . Is it true that if $M$ is closed under complements and countable unions of disjoint sets, then $M$ is a $\sigma$ -algebra? [Hint: any countable union of sets can be written as a countable union of disjoint sets.] The hint confuses me because it seems to suggest that the question is true. But I don't think it is. Just choose $X= \{a,b,c,d\}$ and choose $$M = \{\varnothing, X, \{a,b\}, \{b,c\}, \{c,d\}, \{a,d\}\}$$ Then $M$ is certainly closed under countable disjoint union and complements, but $\{a,b\}\cup \{b,c\} = \{a,b,c\} \not\in M$ . So $M$ is not closed under countable unions and so $M$ is not a $\sigma$ -algebra. I suspect I'm missing something here because this was too easy to do, especially given the hint.","Be wary not to confuse this post with this one . In this case, is simply a subset of the power set of . Let be a set and be a nonempty collection of subsets of . Is it true that if is closed under complements and countable unions of disjoint sets, then is a -algebra? [Hint: any countable union of sets can be written as a countable union of disjoint sets.] The hint confuses me because it seems to suggest that the question is true. But I don't think it is. Just choose and choose Then is certainly closed under countable disjoint union and complements, but . So is not closed under countable unions and so is not a -algebra. I suspect I'm missing something here because this was too easy to do, especially given the hint.","M X X M X M M \sigma X= \{a,b,c,d\} M = \{\varnothing, X, \{a,b\}, \{b,c\}, \{c,d\}, \{a,d\}\} M \{a,b\}\cup \{b,c\} = \{a,b,c\} \not\in M M M \sigma","['analysis', 'measure-theory']"
57,Two formulations of Riesz–Markov–Kakutani representation theorem,Two formulations of Riesz–Markov–Kakutani representation theorem,,"Let $ X $ be a locally compact Hausdorff space, and $ C_c(X) $ be the space of all complex-valued continuous functions with compact support on $ X $ . As far as I know, there are two formulations of Riesz–Markov–Kakutani representation theorem. Formulation 1 (e.g., D. L. Cohn, Measure Theory , 2nd edition, Theorem 7.2.8). Positive linear functionals on $ C_c(X) $ correspond bijectively to positive Borel measures $ \mu $ on $ X $ that is outer regular for all Borel subsets $ A \subseteq X $ , which means $$ \mu(A) = \inf \{\mu(U) \mid \text{open subsets $ U \subseteq X $ containing $ A $}\}, $$ inner regular for open subsets $ U \subseteq X $ , which means $$ \mu(U) = \sup \{\mu(K) \mid \text{compact subsets $ K \subseteq U $}\}, $$ finite on compact subsets. Formulation 2 (e.g., N. Bourbaki, Integration , Chapter IX, Section 3, No. 2, Theorem 2). Positive linear functionals on $ C_c(X) $ correspond bijectively to positive Borel measures $ \mu $ on $ X $ that is inner regular for all Borel subsets $ A \subseteq X $ , which means $$ \mu(A) = \sup \{\mu(K) \mid \text{compact subsets $ K \subseteq A $}\}, $$ finite on compact subsets. So, there should be a natural correspondence between the classes of positive Borel measures in Formulation 1 and 2. Question. Is it possible to describe and prove this correpondence directly (i.e., without using positive linear functionals)?","Let be a locally compact Hausdorff space, and be the space of all complex-valued continuous functions with compact support on . As far as I know, there are two formulations of Riesz–Markov–Kakutani representation theorem. Formulation 1 (e.g., D. L. Cohn, Measure Theory , 2nd edition, Theorem 7.2.8). Positive linear functionals on correspond bijectively to positive Borel measures on that is outer regular for all Borel subsets , which means inner regular for open subsets , which means finite on compact subsets. Formulation 2 (e.g., N. Bourbaki, Integration , Chapter IX, Section 3, No. 2, Theorem 2). Positive linear functionals on correspond bijectively to positive Borel measures on that is inner regular for all Borel subsets , which means finite on compact subsets. So, there should be a natural correspondence between the classes of positive Borel measures in Formulation 1 and 2. Question. Is it possible to describe and prove this correpondence directly (i.e., without using positive linear functionals)?"," X   C_c(X)   X   C_c(X)   \mu   X   A \subseteq X   \mu(A) = \inf \{\mu(U) \mid \text{open subsets  U \subseteq X  containing  A }\},   U \subseteq X   \mu(U) = \sup \{\mu(K) \mid \text{compact subsets  K \subseteq U }\},   C_c(X)   \mu   X   A \subseteq X   \mu(A) = \sup \{\mu(K) \mid \text{compact subsets  K \subseteq A }\}, ","['integration', 'functional-analysis', 'analysis', 'measure-theory']"
58,Embedding discrete functions as continuous functions,Embedding discrete functions as continuous functions,,"$\require{AMScd}$ Given a map $f:\mathbb{N}\to\mathbb{N}$ . Is there a countable set $A \subset [0,1]$ , bijection $\tau:\mathbb{N} \to A$ , and a continuous function $g \in C([0,1],\mathbb{R})$ , such that $\tau \circ f = g \circ \tau$ , i.e., $$ f(n) =  \tau^{-1}(g(\tau(n)), \quad \forall n \in \mathbb{N}. $$ $$ \begin{CD} \mathbb{N} @>{f}>> \mathbb{N} \\ @V{\tau}VV @A{\tau^{-1}}AA \\ A @>{g}>> A  \end{CD} $$ Try for example: $$ f(n) =  \begin{cases} 1, ~ n=2k+1\\ n/2, ~n=2k \end{cases}, k \in \mathbb{N}, $$ and another example: $f(n) = m$ if $n$ is a product of $m$ prime numbers (identical or different).","Given a map . Is there a countable set , bijection , and a continuous function , such that , i.e., Try for example: and another example: if is a product of prime numbers (identical or different).","\require{AMScd} f:\mathbb{N}\to\mathbb{N} A \subset [0,1] \tau:\mathbb{N} \to A g \in C([0,1],\mathbb{R}) \tau \circ f = g \circ \tau 
f(n) =  \tau^{-1}(g(\tau(n)), \quad \forall n \in \mathbb{N}.
 
\begin{CD}
\mathbb{N} @>{f}>> \mathbb{N} \\
@V{\tau}VV @A{\tau^{-1}}AA \\
A @>{g}>> A 
\end{CD}
 
f(n) = 
\begin{cases}
1, ~ n=2k+1\\
n/2, ~n=2k
\end{cases}, k \in \mathbb{N},
 f(n) = m n m","['real-analysis', 'general-topology', 'analysis', 'number-theory', 'set-theory']"
59,Alternative to Rudin's method to prove Q does not have the least upper bound property,Alternative to Rudin's method to prove Q does not have the least upper bound property,,"I came across a proof that that shows that if x is the supremum of the set of rationals such that $x > 0$ and $x^2 < 2$ , then $x > 1$ and $x^2 = 2$ . This is a similar problem to the beginning of Rudin's famous textbook. The part of this proof that I cannot see how to do is to derive two definitions of h. There are two definitions of h used, one used to prove by contradiction that $x^2$ is not less than 2, and the other to prove by contradiction that $x^2$ is not greater than 2. I cannot see how to derive the equations used for h. There are other ways to define h, but I am trying to specifically derive these definitions so I can follow the rest of the text. Here is how it is written: Suppose $x \in Q$ and $x$ = supremum of E = $\{q \in Q: q>0$ and $q^2 < 2\}$ Then, $x \ge 1$ and $x^2 = 2$ Pf: Showing $x \ge 1$ : Since $1 \in$ E and $x = sup$ E , $x \ge 1$ Showing $x^2\ge 2$ (by contradiction): Assume $x^2 < 2$ . Define $h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\}$ Then, if $x^2 < 2$ then $h > 0$ We now prove that $x + h \in E$ . Indeed, $(x + h)^2 = x^2 + 2xh + h^2 < x^2 + h(2x + 1)$ as $h < 1$ . Hence $(x + h)^2 \le x^2 + (2 - x^2)\cdot \dfrac{2x + 1}{2(2x+1)}$ $= x^2 + \dfrac{2-x^2}{2} < 2 + \dfrac{2-2}{2} = 2$ Therefore, $x + h \in E$ and $x + h > x$ so x is not an upper bound for E. Therefore, x is not sup(E), which is a contradiction. Hence, $x^2 \ge 2$ . Showing $x^2\le 2$ (by contradiction): We now prove that $x^2 \le 2$ . Suppose $x^2 > 2$ . Let $h = \dfrac{x^2-2}{2x}$ . Then $h>0$ and $x-h > 0$ .  Showing that x - h is an upper bound for E: We have $(x - h)^2 = x^2 - 2xh + h^2$ $= x^2 - (x^2 - 2) + h^2$ $= 2 + h^2$ $> 2$ . Thus for all $q \in E$ , $q < x-h < x$ so $x$ is not the supremum of E, which contradicts assumption. Therefore, since as per above $x^2 \ge 2$ and $x^2 \le 2$ , we conclude that $x^2 = 2$ . Question about the above proof I follow all of the above and can write the proof in an alternative way with h defined as $1/n$ , $n \in N$ , but I cannot seem to derive these formulas for h: $h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\}$ and $h = \dfrac{x^2-2}{2x}$ I am probably making some kind of algebraic error or not seeing an inequality that induces the definitions. Can someone help me derive these definitions/formulas for h in a step by step manner? Many thanks.","I came across a proof that that shows that if x is the supremum of the set of rationals such that and , then and . This is a similar problem to the beginning of Rudin's famous textbook. The part of this proof that I cannot see how to do is to derive two definitions of h. There are two definitions of h used, one used to prove by contradiction that is not less than 2, and the other to prove by contradiction that is not greater than 2. I cannot see how to derive the equations used for h. There are other ways to define h, but I am trying to specifically derive these definitions so I can follow the rest of the text. Here is how it is written: Suppose and = supremum of E = and Then, and Pf: Showing : Since E and E , Showing (by contradiction): Assume . Define Then, if then We now prove that . Indeed, as . Hence Therefore, and so x is not an upper bound for E. Therefore, x is not sup(E), which is a contradiction. Hence, . Showing (by contradiction): We now prove that . Suppose . Let . Then and .  Showing that x - h is an upper bound for E: We have . Thus for all , so is not the supremum of E, which contradicts assumption. Therefore, since as per above and , we conclude that . Question about the above proof I follow all of the above and can write the proof in an alternative way with h defined as , , but I cannot seem to derive these formulas for h: and I am probably making some kind of algebraic error or not seeing an inequality that induces the definitions. Can someone help me derive these definitions/formulas for h in a step by step manner? Many thanks.","x > 0 x^2 < 2 x > 1 x^2 = 2 x^2 x^2 x \in Q x \{q \in Q: q>0 q^2 < 2\} x \ge 1 x^2 = 2 x \ge 1 1 \in x = sup x \ge 1 x^2\ge 2 x^2 < 2 h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\} x^2 < 2 h > 0 x + h \in E (x + h)^2 = x^2 + 2xh + h^2 < x^2 + h(2x + 1) h < 1 (x + h)^2 \le x^2 + (2 - x^2)\cdot \dfrac{2x + 1}{2(2x+1)} = x^2 + \dfrac{2-x^2}{2} < 2 + \dfrac{2-2}{2} = 2 x + h \in E x + h > x x^2 \ge 2 x^2\le 2 x^2 \le 2 x^2 > 2 h = \dfrac{x^2-2}{2x} h>0 x-h > 0 (x - h)^2 = x^2 - 2xh + h^2 = x^2 - (x^2 - 2) + h^2 = 2 + h^2 > 2 q \in E q < x-h < x x x^2 \ge 2 x^2 \le 2 x^2 = 2 1/n n \in N h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\} h = \dfrac{x^2-2}{2x}","['real-analysis', 'analysis']"
60,"Integration by substitution, but from sphere to sphere","Integration by substitution, but from sphere to sphere",,"Suppose we have a smooth injection $\Phi(x,y)$ from an open set $\Omega \subset \mathbb{R}^2$ to $S = \Phi(\Omega) \subset \mathbb{R}^3$ . It is well-known that the integration by substitution formula holds: $$ \int_{S} f(x,y,z) \, \text{d}\sigma(x,y,z) = \int_{\Omega} f(\Phi(x,y)) \, J(x,y) \, \text{d}x \, \text{d}y $$ where $J(x,y)$ denotes the Jacobian. Recently, I have been working on projects related to integration on a sphere. For example, consider a smooth bijection $\Phi(\omega)$ from $S^2$ to $S^2$ , the sphere in 3-dimensional space. I am curious if there exists a formula like: $$ \int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, T(\omega) \, \text{d}\omega $$ where $T(\omega)$ is only determined by the transition function $\Phi$ . I tried to use the Jacobian, but it is evidently incorrect because $T$ is not even differentiable on the sphere in 3-dimensional space (it is only defined on the sphere!). I read some papers and found something like: $$ \int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, ||DT_\omega(\vec s)\times DT_\omega(\vec t)|| \, \text{d}\omega $$ where(copied from paper): $\vec s$ and $\vec t$ are orthonormal tangent vectors of the unit sphere at $\omega$ and the $DT_{\omega}$ is the 'differential' of $T$ with respect to the vector $\omega$ , The norm of the cross product of transformed tangent vectors accounts for the distortion in the integration domain, similar to the Jacobian determinant for a change of variables in ambient space. But it doesn't provide any precise definition. I don't know how to define the 'differential' of $T$ w.r.t. $\omega$ . I would be more than grateful if you could provide any clues, thank you! Update: I found something like: $$\int_\mathcal{S^2}{f(\omega)}\text d\sigma(\omega)=\int_\mathcal{S^2}{f(\Phi(\omega))\frac{|\omega^TJ^*(\omega)F(\omega)|}{|F(\omega)|^3}}\text d\sigma(\omega)$$ where $\Phi((x,y,z)^T)=\frac{F((x,y,z)^T)}{|F((x,y,z)^T)|}$ , $\Phi$ is bijection from sphere to sphere, $F$ is smooth and non-zero on the sphere. $J$ denotes the Jacobian of $F$ and $J^*$ is the adjugate matrix.","Suppose we have a smooth injection from an open set to . It is well-known that the integration by substitution formula holds: where denotes the Jacobian. Recently, I have been working on projects related to integration on a sphere. For example, consider a smooth bijection from to , the sphere in 3-dimensional space. I am curious if there exists a formula like: where is only determined by the transition function . I tried to use the Jacobian, but it is evidently incorrect because is not even differentiable on the sphere in 3-dimensional space (it is only defined on the sphere!). I read some papers and found something like: where(copied from paper): and are orthonormal tangent vectors of the unit sphere at and the is the 'differential' of with respect to the vector , The norm of the cross product of transformed tangent vectors accounts for the distortion in the integration domain, similar to the Jacobian determinant for a change of variables in ambient space. But it doesn't provide any precise definition. I don't know how to define the 'differential' of w.r.t. . I would be more than grateful if you could provide any clues, thank you! Update: I found something like: where , is bijection from sphere to sphere, is smooth and non-zero on the sphere. denotes the Jacobian of and is the adjugate matrix.","\Phi(x,y) \Omega \subset \mathbb{R}^2 S = \Phi(\Omega) \subset \mathbb{R}^3 
\int_{S} f(x,y,z) \, \text{d}\sigma(x,y,z) = \int_{\Omega} f(\Phi(x,y)) \, J(x,y) \, \text{d}x \, \text{d}y
 J(x,y) \Phi(\omega) S^2 S^2 
\int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, T(\omega) \, \text{d}\omega
 T(\omega) \Phi T 
\int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, ||DT_\omega(\vec s)\times DT_\omega(\vec t)|| \, \text{d}\omega
 \vec s \vec t \omega DT_{\omega} T \omega T \omega \int_\mathcal{S^2}{f(\omega)}\text d\sigma(\omega)=\int_\mathcal{S^2}{f(\Phi(\omega))\frac{|\omega^TJ^*(\omega)F(\omega)|}{|F(\omega)|^3}}\text d\sigma(\omega) \Phi((x,y,z)^T)=\frac{F((x,y,z)^T)}{|F((x,y,z)^T)|} \Phi F J F J^*","['calculus', 'analysis', 'differential-geometry', 'field-theory']"
61,Analog of Mahler's theorem over other non archimedean fields,Analog of Mahler's theorem over other non archimedean fields,,"For any continuous function $f:\mathbb{Z}_p \to \mathbb{Q}_p$ , Mahler's theorem provides us with a relatively explicit series of polynomials converging uniformly to $f$ . Is there any analogue for other non archimedean fields? In particular, what about the completion $\mathbb{C}_p$ of the algebraic closure of $\mathbb{Q}_p$ ?","For any continuous function , Mahler's theorem provides us with a relatively explicit series of polynomials converging uniformly to . Is there any analogue for other non archimedean fields? In particular, what about the completion of the algebraic closure of ?",f:\mathbb{Z}_p \to \mathbb{Q}_p f \mathbb{C}_p \mathbb{Q}_p,"['abstract-algebra', 'analysis', 'number-theory', 'p-adic-number-theory', 'nonarchimedian-analysis']"
62,"Given a representation of $C_0(Y)$ on $C_0(X)$, trying to find a continuous function that satisfies a certain condition.","Given a representation of  on , trying to find a continuous function that satisfies a certain condition.",C_0(Y) C_0(X),"Let $X$ and $Y$ be locally compact Hausdorff, consider an algebra homomorphism $\phi:C_0(Y) \rightarrow C_b(X)$ . We have a representation, which is an algebra homomorphism, $\pi$ of $C_0(Y)$ on $C_0(X)$ defined by $$\pi(g)(f) := \phi(g) \cdot f$$ Assume that this representation is non degenerate i.e. the linear span $\text{span}\{\pi(g)(f):g \in C_0(Y), f \in C_0(X)\}$ is dense in $C_0(X)$ . I am trying to find a continuous function $F:X \rightarrow Y$ such that $\phi(g) = g \circ F$ . For the reverse, if $F$ is continuous and define $\phi(g) := g \circ F$ , then this representation $\pi$ is non degenerate using Stone-Weierstrass theorem. However I am having trouble showing the reverse.","Let and be locally compact Hausdorff, consider an algebra homomorphism . We have a representation, which is an algebra homomorphism, of on defined by Assume that this representation is non degenerate i.e. the linear span is dense in . I am trying to find a continuous function such that . For the reverse, if is continuous and define , then this representation is non degenerate using Stone-Weierstrass theorem. However I am having trouble showing the reverse.","X Y \phi:C_0(Y) \rightarrow C_b(X) \pi C_0(Y) C_0(X) \pi(g)(f) := \phi(g) \cdot f \text{span}\{\pi(g)(f):g \in C_0(Y), f \in C_0(X)\} C_0(X) F:X \rightarrow Y \phi(g) = g \circ F F \phi(g) := g \circ F \pi","['linear-algebra', 'general-topology', 'analysis', 'banach-spaces']"
63,"Properties of $\phi = \hat f \hat g$ for $f,g \in L^1(\Bbb R) \cap L^2(\Bbb R)$",Properties of  for,"\phi = \hat f \hat g f,g \in L^1(\Bbb R) \cap L^2(\Bbb R)","Let $f,g \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ and define $\phi(x) = \widehat f(x) \widehat g(x)$ for all $x\in \Bbb R$ . First of all, I'd like to show that $\phi \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ . As $f,g \in L^2(\Bbb R)$ , we have $\hat f,\hat g \in L^2(\Bbb R)$ . That $\phi \in L^1(\Bbb R)$ follows from $\|\phi\|_1 \le \|\hat f\|_2\|\hat g\|_2 < \infty$ . Next, $|\phi| \le |\hat f \hat g| \le \|f\|_1 |\hat g|$ gives $\|\phi\|_2 \le \|f\|_1 \|\hat g\|_2 < \infty$ . So, $\phi \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ . Next, suppose $f$ is even and $g$ is odd. I want to show $\lim_{\xi \to 0} \widehat \phi(\xi) = 0$ . As $f$ is even and $g$ is odd, we have $\widehat f(\xi) = \widehat f(-\xi)$ and $\widehat g(\xi) + \widehat g(-\xi) = 0$ for all $\xi \in \Bbb R$ . By continuity of $\widehat g$ , this gives $\widehat g(0) = \int_{\Bbb R} g(t)\, dt =  0$ . As $\phi = \hat f \hat g = \widehat{f\ast g}$ , we have $\widehat\phi(\xi) = \widehat{\widehat{f\ast g}}(\xi) = f\ast g(-\xi)$ . That is, $$\widehat\phi(\xi) =f\ast g(-\xi) = \int_{\Bbb R} f(t) g(-\xi - t)\, dt = -\int_{\Bbb R} f(t) g(\xi + t)\, dt$$ The substitution $u = -t$ gives $$-\int_{\Bbb R} f(t) g(\xi + t)\, dt = -\int_{\Bbb R} f(u)g(\xi - u)\, du = - f\ast g(\xi)$$ and so $$\widehat \phi(\xi) = \frac{f\ast g(-\xi) - f\ast g(\xi) }{2}$$ giving $$\lim_{\xi \to 0} \widehat \phi(\xi) = \lim_{\xi \to 0}\frac{f\ast g(-\xi) - f\ast g(\xi) }{2} = 0$$ Lastly, if $\operatorname{supp} f, \operatorname{supp} g \subset [0,1]$ , I want to show $$\int_{-\infty}^\infty \phi(x)\, dx = 0$$ Indeed, $$\int_{-\infty}^\infty \hat f(x) \hat g(x)\, dx  = \int_{-\infty}^\infty f(-x) g(x)\, dx = 0$$ as $\widehat{\widehat{f(x)}} = f(-x)$ and so $\operatorname{supp} \widehat{\widehat{f}} \subset [-1,0]$ . I'd like to know if my work is correct. Thank you!","Let and define for all . First of all, I'd like to show that . As , we have . That follows from . Next, gives . So, . Next, suppose is even and is odd. I want to show . As is even and is odd, we have and for all . By continuity of , this gives . As , we have . That is, The substitution gives and so giving Lastly, if , I want to show Indeed, as and so . I'd like to know if my work is correct. Thank you!","f,g \in L^1 (\Bbb R) \cap L^2(\Bbb R) \phi(x) = \widehat f(x) \widehat g(x) x\in \Bbb R \phi \in L^1 (\Bbb R) \cap L^2(\Bbb R) f,g \in L^2(\Bbb R) \hat f,\hat g \in L^2(\Bbb R) \phi \in L^1(\Bbb R) \|\phi\|_1 \le \|\hat f\|_2\|\hat g\|_2 < \infty |\phi| \le |\hat f \hat g| \le \|f\|_1 |\hat g| \|\phi\|_2 \le \|f\|_1 \|\hat g\|_2 < \infty \phi \in L^1 (\Bbb R) \cap L^2(\Bbb R) f g \lim_{\xi \to 0} \widehat \phi(\xi) = 0 f g \widehat f(\xi) = \widehat f(-\xi) \widehat g(\xi) + \widehat g(-\xi) = 0 \xi \in \Bbb R \widehat g \widehat g(0) = \int_{\Bbb R} g(t)\, dt =  0 \phi = \hat f \hat g = \widehat{f\ast g} \widehat\phi(\xi) = \widehat{\widehat{f\ast g}}(\xi) = f\ast g(-\xi) \widehat\phi(\xi) =f\ast g(-\xi) = \int_{\Bbb R} f(t) g(-\xi - t)\, dt = -\int_{\Bbb R} f(t) g(\xi + t)\, dt u = -t -\int_{\Bbb R} f(t) g(\xi + t)\, dt = -\int_{\Bbb R} f(u)g(\xi - u)\, du = - f\ast g(\xi) \widehat \phi(\xi) = \frac{f\ast g(-\xi) - f\ast g(\xi) }{2} \lim_{\xi \to 0} \widehat \phi(\xi) = \lim_{\xi \to 0}\frac{f\ast g(-\xi) - f\ast g(\xi) }{2} = 0 \operatorname{supp} f, \operatorname{supp} g \subset [0,1] \int_{-\infty}^\infty \phi(x)\, dx = 0 \int_{-\infty}^\infty \hat f(x) \hat g(x)\, dx  = \int_{-\infty}^\infty f(-x) g(x)\, dx = 0 \widehat{\widehat{f(x)}} = f(-x) \operatorname{supp} \widehat{\widehat{f}} \subset [-1,0]","['analysis', 'solution-verification', 'fourier-analysis', 'fourier-transform']"
64,Conditions for the derivative of a function to be a proper map,Conditions for the derivative of a function to be a proper map,,"Let $f:U \to \mathbb{R}^{m}$ be a continuously differentiable function, where $U \subset \mathbb{R}^{n}$ . In this case, the function \begin{equation*} Df:U \to L(\mathbb{R}^{n},\mathbb{R}^{m}) \end{equation*} is continuous. Could one formulate conditions on $f$ , which would suffice to assure that $Df$ is a proper map? Here by proper I mean the preimage of every compact set is compact.","Let be a continuously differentiable function, where . In this case, the function is continuous. Could one formulate conditions on , which would suffice to assure that is a proper map? Here by proper I mean the preimage of every compact set is compact.","f:U \to \mathbb{R}^{m} U \subset \mathbb{R}^{n} \begin{equation*}
Df:U \to L(\mathbb{R}^{n},\mathbb{R}^{m})
\end{equation*} f Df","['real-analysis', 'general-topology', 'analysis', 'differential-topology']"
65,An exercise in analysis about normed vector spaces,An exercise in analysis about normed vector spaces,,"I have found the following exercise : Let $E$ be a normed vector space and $x,y\in E$ be non-zero vectors. Show that $\left\Vert x-y\right\Vert \geq \frac{1}{4}\left( \left\Vert x\right\Vert +\left\Vert y\right\Vert \right) \left\Vert \frac{x}{\left\Vert x\right\Vert }-\frac{y}{\left\Vert y\right\Vert }\right\Vert $ Show that the constant $\frac{1}{4}$ can not be replaced with a greater one. I was able to prove the above inequality but concerning the constant $\frac{1% }{4}$ I have shawn that it can be replaced with $\frac{1}{2}$ in case of Hilbert spaces. So my question is : is there an example of a normed verctor space where the constant $\frac{1}{4}$ can not be improved ? Thank you !",I have found the following exercise : Let be a normed vector space and be non-zero vectors. Show that Show that the constant can not be replaced with a greater one. I was able to prove the above inequality but concerning the constant I have shawn that it can be replaced with in case of Hilbert spaces. So my question is : is there an example of a normed verctor space where the constant can not be improved ? Thank you !,"E x,y\in E \left\Vert x-y\right\Vert \geq \frac{1}{4}\left( \left\Vert
x\right\Vert +\left\Vert y\right\Vert \right) \left\Vert \frac{x}{\left\Vert
x\right\Vert }-\frac{y}{\left\Vert y\right\Vert }\right\Vert  \frac{1}{4} \frac{1%
}{4} \frac{1}{2} \frac{1}{4}","['analysis', 'inequality', 'vector-spaces', 'normed-spaces', 'problem-solving']"
66,Functional series $\sum_{n=1}^{\infty}a_n \sin (b_n x)$ is differentiable.,Functional series  is differentiable.,\sum_{n=1}^{\infty}a_n \sin (b_n x),"How to prove this conclusion? $$a_n,b_n\geq0,\sum_{n=1}^{\infty}a_n ＜\infty$$ And if functional series $$\sum_{n=1}^{\infty}a_n \sin (b_n x)$$ is differentiable for any $x\in \textbf{R}$ , then $$\sum_{n=1}^{\infty}a_nb_n＜\infty$$ And I have known that it may be solved by differentiating it term by term and get values on x=0 of the series $\sum_{n=1}^{\infty}a_nb_n\cos(b_nx)$ ,but I'm doubted how to prove that the order of differentiation and summation could be exchangeable,i.e. $$\sum_{n=1}^{\infty}a_nb_n=\sum_{n=1}^{\infty}a_nb_n\cos(b_nx)|_{x=0}=\frac{\mathrm{d} }{\mathrm{d} x}\sum_{n=1}^{\infty}a_n \sin (b_n x) |_{x=0}$$ Thanks for your reading and assistance.","How to prove this conclusion? And if functional series is differentiable for any , then And I have known that it may be solved by differentiating it term by term and get values on x=0 of the series ,but I'm doubted how to prove that the order of differentiation and summation could be exchangeable,i.e. Thanks for your reading and assistance.","a_n,b_n\geq0,\sum_{n=1}^{\infty}a_n ＜\infty \sum_{n=1}^{\infty}a_n \sin (b_n x) x\in \textbf{R} \sum_{n=1}^{\infty}a_nb_n＜\infty \sum_{n=1}^{\infty}a_nb_n\cos(b_nx) \sum_{n=1}^{\infty}a_nb_n=\sum_{n=1}^{\infty}a_nb_n\cos(b_nx)|_{x=0}=\frac{\mathrm{d} }{\mathrm{d} x}\sum_{n=1}^{\infty}a_n \sin (b_n x) |_{x=0}","['real-analysis', 'sequences-and-series', 'analysis']"
67,Is this structure enough to span the set of all smooth functions?,Is this structure enough to span the set of all smooth functions?,,"Background: I had a thought on algebraically describing the set of all smooth (in the sense that it has a derivative of any order, $C^\infty$ ) functions from an euclidean space $\mathbb{R}^n$ to $\mathbb{R}$ . Call this set $M_n$ for the dimension $n$ . I know it's an algebra, but it obviously has a much stricter structure. For example, if $f:\mathbb{R}^m\rightarrow\mathbb{R}$ is a smooth function and $g_1,\dots,g_m\in M_n$ then $f(g_1,\dots,g_m)\in M_n$ too. Now my question is, can I bound the value of $m$ ? A concrete example question: Is there a finite subset $G$ of $M_3$ such that the subset $P$ of $M_3$ , constructed as the minimal set which satisfies: $G\subseteq P$ , $(\forall f\in M_2)(\forall g,h\in P)\;f(g,h)\in P$ , is equal to $M_3$ ( $P=M_3$ )? My idea: I am imagining that if I tile $\mathbb{R}^3$ with cubes (make a grid) and then ""randomly"" choose some subset of cubes and stuff these cubes with bump functions, while leaving other cubes map to zero, then I can prove that this function is not in $P$ . However, I can't realize this idea.","Background: I had a thought on algebraically describing the set of all smooth (in the sense that it has a derivative of any order, ) functions from an euclidean space to . Call this set for the dimension . I know it's an algebra, but it obviously has a much stricter structure. For example, if is a smooth function and then too. Now my question is, can I bound the value of ? A concrete example question: Is there a finite subset of such that the subset of , constructed as the minimal set which satisfies: , , is equal to ( )? My idea: I am imagining that if I tile with cubes (make a grid) and then ""randomly"" choose some subset of cubes and stuff these cubes with bump functions, while leaving other cubes map to zero, then I can prove that this function is not in . However, I can't realize this idea.","C^\infty \mathbb{R}^n \mathbb{R} M_n n f:\mathbb{R}^m\rightarrow\mathbb{R} g_1,\dots,g_m\in M_n f(g_1,\dots,g_m)\in M_n m G M_3 P M_3 G\subseteq P (\forall f\in M_2)(\forall g,h\in P)\;f(g,h)\in P M_3 P=M_3 \mathbb{R}^3 P","['analysis', 'smooth-functions']"
68,Show that $\mathfrak{Re}(\textrm{Li}_2(e^{ix}))=\frac{x^2}{4}-\frac{\pi x}{2}+\frac{\pi^2}{6}$ (polylogarithm),Show that  (polylogarithm),\mathfrak{Re}(\textrm{Li}_2(e^{ix}))=\frac{x^2}{4}-\frac{\pi x}{2}+\frac{\pi^2}{6},I am working with the polylogarithm function and want to find closed expressions for $\textrm{Li}_2(e^{ix})$ . If I plot the function $\mathfrak{Re}(\textrm{Li}_2(e^{ix}))$ I get $y=\dfrac{x^2}{4}-\dfrac{\pi x}{2}+\dfrac{\pi^2}{6}$ in the interval from 0 to $2\pi$ . How can I see this connection? Is there something similar possible for $\mathfrak{Im}(\textrm{Li}_2(e^{ix}))$ ? Edit: I saw this but was not able to use it.,I am working with the polylogarithm function and want to find closed expressions for . If I plot the function I get in the interval from 0 to . How can I see this connection? Is there something similar possible for ? Edit: I saw this but was not able to use it.,\textrm{Li}_2(e^{ix}) \mathfrak{Re}(\textrm{Li}_2(e^{ix})) y=\dfrac{x^2}{4}-\dfrac{\pi x}{2}+\dfrac{\pi^2}{6} 2\pi \mathfrak{Im}(\textrm{Li}_2(e^{ix})),"['complex-analysis', 'analysis', 'polylogarithm']"
69,Book recommendations for functional analysis and analysis on manifolds to study optimization.,Book recommendations for functional analysis and analysis on manifolds to study optimization.,,"I'm about to start a PhD in an optimization lab, and I'd like to study some analysis beforehand. So far, I've studied linear, non-linear, and integer optimization during my undergrad courses, and they were pretty theoretical. However, I've heard that functional analysis and analysis on manyfolds also hold many important results for optimization (mostly for non-linear), and I'd like to take a look before the PhD starts. For functional analysis, my uni uses the Kreyszig's book, and a random one for analysis on manyfolds. However, I'm aware that only a few chapters in these books are directly relevant to optimization. For instance, in Kreyszig's book, I know that the sections on metric spaces and Banach's fixed point theorem are essential, but I'm uncertain about the rest. In a nutshell: Could you guys recommend me some books and chapters in each book that will be most helpful for this purpose? :)","I'm about to start a PhD in an optimization lab, and I'd like to study some analysis beforehand. So far, I've studied linear, non-linear, and integer optimization during my undergrad courses, and they were pretty theoretical. However, I've heard that functional analysis and analysis on manyfolds also hold many important results for optimization (mostly for non-linear), and I'd like to take a look before the PhD starts. For functional analysis, my uni uses the Kreyszig's book, and a random one for analysis on manyfolds. However, I'm aware that only a few chapters in these books are directly relevant to optimization. For instance, in Kreyszig's book, I know that the sections on metric spaces and Banach's fixed point theorem are essential, but I'm uncertain about the rest. In a nutshell: Could you guys recommend me some books and chapters in each book that will be most helpful for this purpose? :)",,"['functional-analysis', 'analysis', 'optimization', 'book-recommendation']"
70,Is $a_n=\ln(n)/\sqrt n$ increasing or deacreasing,Is  increasing or deacreasing,a_n=\ln(n)/\sqrt n,How do we proving that: $$\forall n\ge 9 : a_n = \frac{\ln(n)}{\sqrt n}$$ increasing or decreasing My methode Let : $f(x)=\frac{\ln x}{\sqrt x} $ where $ x \ge 9$ $$f'(x)=\frac{\frac{\sqrt x}{x}-\frac{\ln x}{2\sqrt x}}{x}=\frac{1}{x^2}\left({\sqrt{x} - \frac{1}{2}\sqrt{x}\ln x}\right)$$ We have : $$x \ge 9 \implies f'(x) \le 0$$ therefore : $f(x)$ decreasing Finally: $a_n$ decreasing $\forall n\ge 9$ My question: are another way method ? I appreciate your interest,How do we proving that: increasing or decreasing My methode Let : where We have : therefore : decreasing Finally: decreasing My question: are another way method ? I appreciate your interest,\forall n\ge 9 : a_n = \frac{\ln(n)}{\sqrt n} f(x)=\frac{\ln x}{\sqrt x}   x \ge 9 f'(x)=\frac{\frac{\sqrt x}{x}-\frac{\ln x}{2\sqrt x}}{x}=\frac{1}{x^2}\left({\sqrt{x} - \frac{1}{2}\sqrt{x}\ln x}\right) x \ge 9 \implies f'(x) \le 0 f(x) a_n \forall n\ge 9,"['real-analysis', 'sequences-and-series', 'analysis']"
71,$A \subset \ell^2(\Bbb Z)$ as a collection of Fourier coefficients,as a collection of Fourier coefficients,A \subset \ell^2(\Bbb Z),"Let $A$ be a closed subspace of $\ell^2(\Bbb Z)$ . Suppose for every $\{a_n\}_n \in A$ , we have $\{a_{n+m}\}_n \in A$ for each $m \in \Bbb Z$ . Show that there exists a measurable set $E\subset \Bbb T$ such that $$A = \{\{\hat f(n)\}_{n\in \Bbb Z}: f\in L^2(\Bbb T), \operatorname{supp} f \subset E\}$$ Let $\mathcal F: L^2(\Bbb T) \to \ell^2(\Bbb Z)$ be the usual map $f\mapsto \{\hat f(n)\}_{n\in \Bbb Z}$ . This is a surjective isometry, and so $\mathcal F^{-1}$ makes sense. My first guess for $E$ is $\bigcup_{f\in \mathcal F^{-1} (A)} \operatorname{supp} f$ but this may not even be a measurable set. I'd like some hints or ideas to complete this proof!","Let be a closed subspace of . Suppose for every , we have for each . Show that there exists a measurable set such that Let be the usual map . This is a surjective isometry, and so makes sense. My first guess for is but this may not even be a measurable set. I'd like some hints or ideas to complete this proof!","A \ell^2(\Bbb Z) \{a_n\}_n \in A \{a_{n+m}\}_n \in A m \in \Bbb Z E\subset \Bbb T A = \{\{\hat f(n)\}_{n\in \Bbb Z}: f\in L^2(\Bbb T), \operatorname{supp} f \subset E\} \mathcal F: L^2(\Bbb T) \to \ell^2(\Bbb Z) f\mapsto \{\hat f(n)\}_{n\in \Bbb Z} \mathcal F^{-1} E \bigcup_{f\in \mathcal F^{-1} (A)} \operatorname{supp} f","['functional-analysis', 'analysis', 'fourier-analysis']"
72,"How to show $ |(Bx,x)|\leq (Ax,x) $ for any $ x\in D(A) $ here?",How to show  for any  here?," |(Bx,x)|\leq (Ax,x)   x\in D(A) ","On the Hilbert space $ H $ , $ A $ is a non-negative self-adjoint operator and $ B $ is a symmetric operator. Let $ D(B)\supset D(A) $ , where $ D(A) $ and $ D(B) $ are definite domain for $ A $ and $ B $ repectively. Assume that \begin{align*} \|Bx\|\leq \|Ax\|,\quad\forall x\in D(A). \end{align*} Show that $ |(Bx,x)|\leq (Ax,x) $ for any $ x\in D(A) $ . Here I want to use the norm $ \|\cdot\| $ to represent the $ (Ax,x) $ and $ (Bx,x) $ but I do not how to go on. Can you give me some hints or references?","On the Hilbert space , is a non-negative self-adjoint operator and is a symmetric operator. Let , where and are definite domain for and repectively. Assume that Show that for any . Here I want to use the norm to represent the and but I do not how to go on. Can you give me some hints or references?"," H   A   B   D(B)\supset D(A)   D(A)   D(B)   A   B  \begin{align*}
\|Bx\|\leq \|Ax\|,\quad\forall x\in D(A).
\end{align*}  |(Bx,x)|\leq (Ax,x)   x\in D(A)   \|\cdot\|   (Ax,x)   (Bx,x) ","['functional-analysis', 'analysis', 'operator-theory', 'unbounded-operators']"
73,Convergence of a sum to an integral using Riemann sum,Convergence of a sum to an integral using Riemann sum,,"In physics, the following problem arises in the context of statistical mechanics. Let $L \gg 1$ be a fixed parameter. Let $\beta > 0$ be also fixed. Consider the following series: $$\rho = \frac{1}{L^{3}}\sum_{p \in \frac{2\pi}{L}\mathbb{Z}^{3}}\frac{1}{e^{\beta(|p|^{2}-\mu)}-1}. \tag{1}\label{1}$$ Here, $|p|^{2} = p_{1}^{2}+p_{2}^{2}+p_{3}^{2}$ , as usual and $\mu$ is (another) parameter, which is supposed to be $-\infty < \mu \le 0$ . One wants to take $L\to \infty$ and replace the sum by an integral: $$\frac{1}{L^{3}}\sum_{p\in \frac{2\pi}{L}\mathbb{Z}^{3}} \to \frac{1}{(2\pi)^{3}}\int dp$$ My question is: what are the conditions which allow such replacement or, putting in another words, what conditions guarantee convergence of the sum into an integral? Let me just explain the motivation of the question. It turns out that, if one replaces the series by an integral without worrying about the convergence, it leads to a very strange physical consequence where the density of the system is bounded when $\mu \to 0^{-}$ . The explanation in the physics literature is that $\mu$ is itself a function of $L^{3}$ , implicitly by (\ref{1}). Hence, one has to be careful when taking the limit $L\to \infty$ . So, if the parameters $\beta$ and $\rho$ are such that, in the limit $L\to \infty$ , $\mu \to 1$ , then the term $(e^{-\beta\mu}-1)^{-1}$ (the term with $p=0$ ) in the sum (\ref{1}) diverges, and one cannot replace the sum by an integral. So, getting back to my question, I wonder if that ensuring that every term in the sum (\ref{1}) is not divergent for every $\mu$ (also in the limit $\mu \to 0$ ) is sufficient to ensure that the sum converges to an integral.","In physics, the following problem arises in the context of statistical mechanics. Let be a fixed parameter. Let be also fixed. Consider the following series: Here, , as usual and is (another) parameter, which is supposed to be . One wants to take and replace the sum by an integral: My question is: what are the conditions which allow such replacement or, putting in another words, what conditions guarantee convergence of the sum into an integral? Let me just explain the motivation of the question. It turns out that, if one replaces the series by an integral without worrying about the convergence, it leads to a very strange physical consequence where the density of the system is bounded when . The explanation in the physics literature is that is itself a function of , implicitly by (\ref{1}). Hence, one has to be careful when taking the limit . So, if the parameters and are such that, in the limit , , then the term (the term with ) in the sum (\ref{1}) diverges, and one cannot replace the sum by an integral. So, getting back to my question, I wonder if that ensuring that every term in the sum (\ref{1}) is not divergent for every (also in the limit ) is sufficient to ensure that the sum converges to an integral.",L \gg 1 \beta > 0 \rho = \frac{1}{L^{3}}\sum_{p \in \frac{2\pi}{L}\mathbb{Z}^{3}}\frac{1}{e^{\beta(|p|^{2}-\mu)}-1}. \tag{1}\label{1} |p|^{2} = p_{1}^{2}+p_{2}^{2}+p_{3}^{2} \mu -\infty < \mu \le 0 L\to \infty \frac{1}{L^{3}}\sum_{p\in \frac{2\pi}{L}\mathbb{Z}^{3}} \to \frac{1}{(2\pi)^{3}}\int dp \mu \to 0^{-} \mu L^{3} L\to \infty \beta \rho L\to \infty \mu \to 1 (e^{-\beta\mu}-1)^{-1} p=0 \mu \mu \to 0,"['real-analysis', 'analysis', 'discrete-mathematics', 'mathematical-physics']"
74,"Given behaviours at small and large values of a parameter, how can one deduce there is a singular (non-analytic) point?","Given behaviours at small and large values of a parameter, how can one deduce there is a singular (non-analytic) point?",,"A reoccurring statement in condensed matter physics is vaguely the following: Let $f:\mathbb{R}^n\times\mathbb{R}\rightarrow\mathbb{C}$ be some function. $\mathbb{R}^n$ are the spatial coordinates and $\mathbb{R}$ represents temperature. This function is shown to behave very differently at $T\rightarrow 0$ and $T\rightarrow \infty$ . For example, it may hava a finite value for low $T$ but is identically $0$ at large enough $T$ . Another example is that $f \sim \mid{r}\mid ^k$ for low $T$ and $f \sim e^{-m\mid r\mid}$ for large enough $T$ . By $\sim$ I mean that the function converges to these functions at these limits. Now, it is claimed that these types of functions can not be analytic with respect to $T$ for all $T$ : There is a point in which the behavior is singular. My questions are the following: How does one prove this for the cases above? What type of arguments and from which fields are used for these types of problems? Particular theorems will be very welcomed A proof for similar cases will also be a good answer, as my goal is understanding the formal arguments","A reoccurring statement in condensed matter physics is vaguely the following: Let be some function. are the spatial coordinates and represents temperature. This function is shown to behave very differently at and . For example, it may hava a finite value for low but is identically at large enough . Another example is that for low and for large enough . By I mean that the function converges to these functions at these limits. Now, it is claimed that these types of functions can not be analytic with respect to for all : There is a point in which the behavior is singular. My questions are the following: How does one prove this for the cases above? What type of arguments and from which fields are used for these types of problems? Particular theorems will be very welcomed A proof for similar cases will also be a good answer, as my goal is understanding the formal arguments",f:\mathbb{R}^n\times\mathbb{R}\rightarrow\mathbb{C} \mathbb{R}^n \mathbb{R} T\rightarrow 0 T\rightarrow \infty T 0 T f \sim \mid{r}\mid ^k T f \sim e^{-m\mid r\mid} T \sim T T,"['real-analysis', 'complex-analysis', 'analysis', 'asymptotics', 'mathematical-physics']"
75,Does there exist a continuous open map from the closed annulus to the closed disk?,Does there exist a continuous open map from the closed annulus to the closed disk?,,"In this MSE post A function $f:\mathbb{R}^2\to\mathbb{R}^2$ that is open and closed, but not continuous. , user Moishe Kohan provides an example of a non-continuous open and closed (""clopen"") function $\mathbb R^n \to \mathbb R^n$ for $n\geq 3$ by citing and using the following propositions of David Wilson: Propositions 1 and 3 of D. Wilson, Open mappings of the universal curve onto continuous curves. Trans. Amer. Math. Soc. 168 (1972), 497–515. [Moishe Kohan's] Proposition [cooked up from Wilson's propositions]. Let $I^n$ be the closed $n$ -dimensional cube, $n\ge 3$ , and $J^n\subset int(I^n)$ is a closed subcube. Let $Q$ denote the interior of $J^n$ . Then, there exists an open continuous map $g: I^n \setminus Q\to I^n$ which equals the identity on the boundary of $I^n$ and sends $\partial Q$ to the interior of $I^n$ . I am interested in the $2$ -dimensional case, for which I propose the following conjecture: Conjecture: there are no continuous open (w.r.t. subspace topologies) maps $f$ from the closed annulus $\mathbb A:= \{x\in \mathbb R^2: 1\leq |x|\leq 2\}$ to the closed disk $\mathbb D := \overline{B(0,2)}$ , which restricts to the identity map on the outer boundary circle $\partial \mathbb D = C(0,2)\subseteq \mathbb A$ , and sends the inner boundary circle $C(0,1)$ to the interior of $\mathbb D$ . I was not able to prove or disprove this conjecture for even the Simpler Case: where $f$ maps the inner boundary circle $C(0,1)$ to the single point $0\in \mathbb D$ . Some attempts I made on the Simpler Case . Attempt 1: Because we want $f$ to be open, we in particular want any open neighborhood in $\mathbb A$ of any boundary point $b\in C(0,1)$ to map to an open set containing $0\in \mathbb D$ , in particular containing some $B(0,\epsilon)$ . My idea was map circles $C(0,1+\eta)$ to circles $C(0,\eta)$ , and to to ""swirl""/""smear"" the circles $C(0,1+\eta)$ more and more extremely as $\eta \searrow 0$ , so that even a very small neighborhood $B(b,\delta)\cap \mathbb A$ of $b\in C(0,1)$ containing just a $\approx \frac{2\delta}{2\pi}$ fraction of the circles $C(0,1+\eta)$ for small enough $\eta>0$ would get ""swirled""/""smeared"" to contain the entire circle $C(0,\eta)$ . So something like $f$ maps $z:=re^{i\theta}\in \mathbb A$ (thinking of $\mathbb R^2$ as the complex plane) to $(r-1)e^{i\theta\cdot \frac{1}{r-1}}$ , mapping an arc of the circle $C(0,1+\eta)$ of arclength $\ell$ to an arc of the circle $C(0,\eta)$ with arclength $\ell \cdot \frac{1}{r-1}$ . Unfortunately, the corresponding formula for $f$ would be $$f(z)=\frac{|z|-1}{|z|^\frac{1}{|z|-1}} \cdot z^{\frac{1}{|z|-1}},$$ which maybe looks fine, until one remembers that raising a complex number to a non-integer power needs (non-continuous) branch cuts to define. :( Attempt 2: One can make a continuous ""swirling only"" (no ""spreading"", i.e. no multiplicative factor in the argument variable) by mapping $r e^{i\theta} \mapsto (r-1) e^{i\theta + i\cdot \frac{1}{r-1}}$ , i.e. $$f(z) = \frac{|z|-1}{|z|} \cdot z \cdot e^{\frac{1}{|z|-1}},$$ which I think is continuous, and does ""swirl"" a very small neighborhood $N_b:= B(b,\delta)\cap \mathbb A$ to something that does sort of ""wrap around"" $0 \in \mathbb D$ , but in doing so has a lot of holes: this $f$ preserves that proportion of the arclength of $N_b \cap C(1+\eta)$ , meaning $\text{arclength}(N_b \cap C(1+\eta)) = \text{arclength}(f(N_b) \cap C(0,\eta))$ , so $f$ can't possibly map $N_b$ to something containing $B(0,\epsilon)$ . Attempt 3: Finally, because of this ""monodromy problem"" of defining this ""swirling/smearing"" map on the entire circle $C(0,1+\eta)$ , I had an idea of cutting up $C(0,1+\eta)$ into $\frac 1\eta$ many pieces (restricting to $\eta \in \{\frac 1n: n=2, 3, 4, \ldots\}$ ), making $f$ ""swirl/smear"" each of those pieces into the entirety of $C(0,\eta)$ , thus guaranteeing that $f(N_b)$ contains complete circles $C(0,\eta)$ arbitrarily close to $0\in \mathbb D$ . More precisely, I would partition $C(0,1+\frac 1n)$ into $2n$ equally sized, equally spaced pieces, and define $\{K_{n, i}\}_{i=1}^n$ to be the closures of the say odd-indexed pieces. I can map $K_{n,i}$ continuously to cover the entirety of $C(0,\frac 1n)$ . Then for any $N_b:= B(b,\delta)\cap \mathbb A$ , it does contain some $K_{n,i}$ for all $n$ sufficiently large, and hence $f(N_b)$ contains $C(0,\frac 1n)$ for all $n$ sufficiently large. I can extend this $f$ defined on $C(0,1) \cup \bigcup_{n=2}^\infty\bigcup_{i=1}^n K_{n,i}$ via the Tietze extension theorem to a continuous function $\mathbb A \to \mathbb D$ , but again I really doubt it is a an open map. (Cross posted to MO https://mathoverflow.net/questions/452140/does-there-exist-a-continuous-open-map-from-the-closed-annulus-to-the-closed-dis upon suggestion from comments)","In this MSE post A function $f:\mathbb{R}^2\to\mathbb{R}^2$ that is open and closed, but not continuous. , user Moishe Kohan provides an example of a non-continuous open and closed (""clopen"") function for by citing and using the following propositions of David Wilson: Propositions 1 and 3 of D. Wilson, Open mappings of the universal curve onto continuous curves. Trans. Amer. Math. Soc. 168 (1972), 497–515. [Moishe Kohan's] Proposition [cooked up from Wilson's propositions]. Let be the closed -dimensional cube, , and is a closed subcube. Let denote the interior of . Then, there exists an open continuous map which equals the identity on the boundary of and sends to the interior of . I am interested in the -dimensional case, for which I propose the following conjecture: Conjecture: there are no continuous open (w.r.t. subspace topologies) maps from the closed annulus to the closed disk , which restricts to the identity map on the outer boundary circle , and sends the inner boundary circle to the interior of . I was not able to prove or disprove this conjecture for even the Simpler Case: where maps the inner boundary circle to the single point . Some attempts I made on the Simpler Case . Attempt 1: Because we want to be open, we in particular want any open neighborhood in of any boundary point to map to an open set containing , in particular containing some . My idea was map circles to circles , and to to ""swirl""/""smear"" the circles more and more extremely as , so that even a very small neighborhood of containing just a fraction of the circles for small enough would get ""swirled""/""smeared"" to contain the entire circle . So something like maps (thinking of as the complex plane) to , mapping an arc of the circle of arclength to an arc of the circle with arclength . Unfortunately, the corresponding formula for would be which maybe looks fine, until one remembers that raising a complex number to a non-integer power needs (non-continuous) branch cuts to define. :( Attempt 2: One can make a continuous ""swirling only"" (no ""spreading"", i.e. no multiplicative factor in the argument variable) by mapping , i.e. which I think is continuous, and does ""swirl"" a very small neighborhood to something that does sort of ""wrap around"" , but in doing so has a lot of holes: this preserves that proportion of the arclength of , meaning , so can't possibly map to something containing . Attempt 3: Finally, because of this ""monodromy problem"" of defining this ""swirling/smearing"" map on the entire circle , I had an idea of cutting up into many pieces (restricting to ), making ""swirl/smear"" each of those pieces into the entirety of , thus guaranteeing that contains complete circles arbitrarily close to . More precisely, I would partition into equally sized, equally spaced pieces, and define to be the closures of the say odd-indexed pieces. I can map continuously to cover the entirety of . Then for any , it does contain some for all sufficiently large, and hence contains for all sufficiently large. I can extend this defined on via the Tietze extension theorem to a continuous function , but again I really doubt it is a an open map. (Cross posted to MO https://mathoverflow.net/questions/452140/does-there-exist-a-continuous-open-map-from-the-closed-annulus-to-the-closed-dis upon suggestion from comments)","\mathbb R^n \to \mathbb R^n n\geq 3 I^n n n\ge 3 J^n\subset int(I^n) Q J^n g: I^n \setminus Q\to I^n I^n \partial Q I^n 2 f \mathbb A:= \{x\in \mathbb R^2: 1\leq |x|\leq 2\} \mathbb D := \overline{B(0,2)} \partial \mathbb D = C(0,2)\subseteq \mathbb A C(0,1) \mathbb D f C(0,1) 0\in \mathbb D f \mathbb A b\in C(0,1) 0\in \mathbb D B(0,\epsilon) C(0,1+\eta) C(0,\eta) C(0,1+\eta) \eta \searrow 0 B(b,\delta)\cap \mathbb A b\in C(0,1) \approx \frac{2\delta}{2\pi} C(0,1+\eta) \eta>0 C(0,\eta) f z:=re^{i\theta}\in \mathbb A \mathbb R^2 (r-1)e^{i\theta\cdot \frac{1}{r-1}} C(0,1+\eta) \ell C(0,\eta) \ell \cdot \frac{1}{r-1} f f(z)=\frac{|z|-1}{|z|^\frac{1}{|z|-1}} \cdot z^{\frac{1}{|z|-1}}, r e^{i\theta} \mapsto (r-1) e^{i\theta + i\cdot \frac{1}{r-1}} f(z) = \frac{|z|-1}{|z|} \cdot z \cdot e^{\frac{1}{|z|-1}}, N_b:= B(b,\delta)\cap \mathbb A 0 \in \mathbb D f N_b \cap C(1+\eta) \text{arclength}(N_b \cap C(1+\eta)) = \text{arclength}(f(N_b) \cap C(0,\eta)) f N_b B(0,\epsilon) C(0,1+\eta) C(0,1+\eta) \frac 1\eta \eta \in \{\frac 1n: n=2, 3, 4, \ldots\} f C(0,\eta) f(N_b) C(0,\eta) 0\in \mathbb D C(0,1+\frac 1n) 2n \{K_{n, i}\}_{i=1}^n K_{n,i} C(0,\frac 1n) N_b:= B(b,\delta)\cap \mathbb A K_{n,i} n f(N_b) C(0,\frac 1n) n f C(0,1) \cup \bigcup_{n=2}^\infty\bigcup_{i=1}^n K_{n,i} \mathbb A \to \mathbb D","['real-analysis', 'general-topology', 'analysis', 'continuity', 'open-map']"
76,About the square root of a positive operator,About the square root of a positive operator,,"Let $A$ be a positive bounded linear operator on a Hilbert space $\mathscr{H}$ , that is, $\langle \psi,A\psi\rangle \ge 0$ holds for every $\psi \in \mathscr{H}$ . We write this property as $A \ge 0$ . I am reading Reed & Simon's proof of the following result. Theorem: Let $A$ be positive, bounded and linear. Then there exists a unique bounded linear operator $B$ such that $B \ge 0$ and $B^{2} = A$ . Moreover, $B$ commutes with every bounded linear operator which commutes with $A$ . Suppose $\|A\| \le 1$ . To define $B$ , one considers the power series: $$B = \sqrt{A} = \sqrt{I - (I-A)} = 1+c_{1}(I-A) + c_{2}(I-A)^{2} + \cdots, $$ which is absolutely convergent when $\|I-A\| \le 1$ . My first question is: how to show that $B^{2} = A$ ? I know that, because the series for $B$ is absolutely convergent, one can square it and write it as: $$B^{2} = \sum_{n=0}^{\infty}a_{n}(I-T)^{n}$$ with: $$a_{n} = \sum_{m=0}^{n}c_{m}c_{n-m} $$ and $c_{0} = 1$ . I also know that: $$1-z = \sum_{n=0}^{\infty}a_{n}z^{n}.$$ Question 1: Is this enough to prove that $B^{2} = I - (I-A) = A$ ? Question 2: To prove uniqueness, the authors assume $B'$ is another positive bounded linear operator such that $(B')^{2} = A$ . Then, they claim that $B - B'$ is self-adjoint. I don't see, however, why this is true. A positive bounded linear operator is only self-adjoint in the case of complex Hilbert spaces, but there is no mention about the underlying Hilbert space to be complex in their theorem. So, why is this true?","Let be a positive bounded linear operator on a Hilbert space , that is, holds for every . We write this property as . I am reading Reed & Simon's proof of the following result. Theorem: Let be positive, bounded and linear. Then there exists a unique bounded linear operator such that and . Moreover, commutes with every bounded linear operator which commutes with . Suppose . To define , one considers the power series: which is absolutely convergent when . My first question is: how to show that ? I know that, because the series for is absolutely convergent, one can square it and write it as: with: and . I also know that: Question 1: Is this enough to prove that ? Question 2: To prove uniqueness, the authors assume is another positive bounded linear operator such that . Then, they claim that is self-adjoint. I don't see, however, why this is true. A positive bounded linear operator is only self-adjoint in the case of complex Hilbert spaces, but there is no mention about the underlying Hilbert space to be complex in their theorem. So, why is this true?","A \mathscr{H} \langle \psi,A\psi\rangle \ge 0 \psi \in \mathscr{H} A \ge 0 A B B \ge 0 B^{2} = A B A \|A\| \le 1 B B = \sqrt{A} = \sqrt{I - (I-A)} = 1+c_{1}(I-A) + c_{2}(I-A)^{2} + \cdots,  \|I-A\| \le 1 B^{2} = A B B^{2} = \sum_{n=0}^{\infty}a_{n}(I-T)^{n} a_{n} = \sum_{m=0}^{n}c_{m}c_{n-m}  c_{0} = 1 1-z = \sum_{n=0}^{\infty}a_{n}z^{n}. B^{2} = I - (I-A) = A B' (B')^{2} = A B - B'","['functional-analysis', 'analysis', 'proof-explanation', 'operator-theory', 'hilbert-spaces']"
77,"Let $f$ be continuous function on $[a,b]$ and $g$ be Riemann integrable on [a,b], then the integral $fdg$ exists. [closed]","Let  be continuous function on  and  be Riemann integrable on [a,b], then the integral  exists. [closed]","f [a,b] g fdg","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 11 months ago . Improve this question Prove or disprove: Let $f$ be continuous function on $[a,b]$ and $g$ be Riemann integrable on [a,b], then the integral $fdg$ exists. My motivation to problem is studying the connections between the Riemann Integration and Riemann-Stieltjes integration, and trying to put conditions.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 11 months ago . Improve this question Prove or disprove: Let be continuous function on and be Riemann integrable on [a,b], then the integral exists. My motivation to problem is studying the connections between the Riemann Integration and Riemann-Stieltjes integration, and trying to put conditions.","f [a,b] g fdg","['real-analysis', 'analysis']"
78,"How to prove that $\phi'(z)<0$ for $\theta\in (0,\pi)$?",How to prove that  for ?,"\phi'(z)<0 \theta\in (0,\pi)","Let $a_1=(1,0), a_2$ be two points on the unit circle T of the complex place $C$ . Assume that the angle between $a_1$ and $a_2$ are $\theta$ . See the below graph: Define the function $$ r(z)=\frac{1}{z-a_1}+\frac{1}{z-a_2} $$ Define the integral: $$ \phi(\theta):=\int_D |r(z)|dz $$ where $D$ is a unit disk. I guess that $\phi(\theta)$ is decreasing on $\theta\in (0,\pi)$ and increasing on $\theta\in (\pi,2\pi)$ . Moreover, $\phi'(z)=0$ as $z=\pi$ . How to prove that $\phi'(z)<0$ for $\theta\in (0,\pi)$ ? I am not sure how to compute the integral over the disk... I know how to integral it around a closed path.","Let be two points on the unit circle T of the complex place . Assume that the angle between and are . See the below graph: Define the function Define the integral: where is a unit disk. I guess that is decreasing on and increasing on . Moreover, as . How to prove that for ? I am not sure how to compute the integral over the disk... I know how to integral it around a closed path.","a_1=(1,0), a_2 C a_1 a_2 \theta 
r(z)=\frac{1}{z-a_1}+\frac{1}{z-a_2}
 
\phi(\theta):=\int_D |r(z)|dz
 D \phi(\theta) \theta\in (0,\pi) \theta\in (\pi,2\pi) \phi'(z)=0 z=\pi \phi'(z)<0 \theta\in (0,\pi)","['complex-analysis', 'analysis']"
79,Simplest form for $\int_0^\infty \frac{\sin(ax)}{b+x}~dx$,Simplest form for,\int_0^\infty \frac{\sin(ax)}{b+x}~dx,"Regarding the integral, $$I=\int_0^\infty \frac{\sin(ax)}{b+x}~dx,~~~~a>0\land b>0$$ Let $t=ax$ $$I=\int_0^\infty \frac{\sin(t)}{ab+t}~dt$$ Let $\theta=ab+t$ $$\begin{align} I&=\int_{ab}^\infty \frac{\sin(\theta-ab)}{\theta}~d\theta\\ \\ &=\cos(ab)\int_{ab}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta\\ \\ &=\cos(ab)\int_{0}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\cos(ab)\int_{0}^{ab} \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta \end{align}$$ Define the sine integral and cosine integral as: $$\text{Si}(z)=\int_0^z \frac{\sin(\theta)}{\theta}d\theta,~~~~~\text{Ci}(z)=-\int_z^\infty \frac{\cos(\theta)}{\theta}d\theta$$ We get: $$\boxed{\int_0^\infty \frac{\sin(ax)}{b+x}~dx=\frac\pi2\cos(ab)-\cos(ab)\text{Si}(ab)+\sin(ab)\text{Ci}(ab)~}$$ Question: Is this the simplest form we can get for this integral?","Regarding the integral, Let Let Define the sine integral and cosine integral as: We get: Question: Is this the simplest form we can get for this integral?","I=\int_0^\infty \frac{\sin(ax)}{b+x}~dx,~~~~a>0\land b>0 t=ax I=\int_0^\infty \frac{\sin(t)}{ab+t}~dt \theta=ab+t \begin{align} I&=\int_{ab}^\infty \frac{\sin(\theta-ab)}{\theta}~d\theta\\
\\
&=\cos(ab)\int_{ab}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta\\
\\
&=\cos(ab)\int_{0}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\cos(ab)\int_{0}^{ab} \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta \end{align} \text{Si}(z)=\int_0^z \frac{\sin(\theta)}{\theta}d\theta,~~~~~\text{Ci}(z)=-\int_z^\infty \frac{\cos(\theta)}{\theta}d\theta \boxed{\int_0^\infty \frac{\sin(ax)}{b+x}~dx=\frac\pi2\cos(ab)-\cos(ab)\text{Si}(ab)+\sin(ab)\text{Ci}(ab)~}","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'improper-integrals']"
80,Prove that $-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3)$,Prove that,-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3),Prove that $$-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3)$$ where ${\rm Li}_2(x)$ is the Poly Logarithm function and $\zeta(s)$ is the Riemann zeta function Let $$I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx $$ Substituting $x=1-y$ gives $$I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{1-x}\ dx $$ Adding the above two equations we have $$2I=-\int_{0}^{1}{\rm Li}_2(-x(1-x))\left(\frac{1}{x}+\frac{1}{1-x}\right)\ dx $$ $$2I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ dx \tag{1}$$ Now we have the integral representation of ${\rm Li}_2(z)$ as $${\rm Li}_2(z)=-\int_{0}^{1} \frac{\log(1-tz)}{t} \ dt$$ For $z=-x(1-x)$ we have $${\rm Li}_2(-x(1-x))=-\int_{0}^{1} \frac{\log(1+tx(1-x))}{t} \ dt\tag{2}$$ So from $(1)$ and $(2)$ $$2I=\int_{0}^{1}\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ \frac{\log(1+tx(1-x))}{t} \ dt\ dx $$ Now I am unable to apply integration by parts in the last equation. Edit I came across this question on MSE click here . I am unable to understand the following equality $$-\frac{1}{2}\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{x}\:dx-\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{1-x}\:dx=-\frac{4}{3}\ln ^3\left(\phi \right)-2\sum _{k=1}^{\infty }\frac{1}{k^3}+2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}}$$ The first integral I am able to evaluate. I am having problem in the second integral. Also how do we get the following? $$2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}}=\frac{8}{5}\zeta \left(3\right)+\frac{4}{3}\ln ^3\left(\phi \right)-\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right)+\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right)$$,Prove that where is the Poly Logarithm function and is the Riemann zeta function Let Substituting gives Adding the above two equations we have Now we have the integral representation of as For we have So from and Now I am unable to apply integration by parts in the last equation. Edit I came across this question on MSE click here . I am unable to understand the following equality The first integral I am able to evaluate. I am having problem in the second integral. Also how do we get the following?,-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx=\frac{2}{5}\zeta(3) {\rm Li}_2(x) \zeta(s) I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x}\ dx  x=1-y I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{1-x}\ dx  2I=-\int_{0}^{1}{\rm Li}_2(-x(1-x))\left(\frac{1}{x}+\frac{1}{1-x}\right)\ dx  2I=-\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ dx \tag{1} {\rm Li}_2(z) {\rm Li}_2(z)=-\int_{0}^{1} \frac{\log(1-tz)}{t} \ dt z=-x(1-x) {\rm Li}_2(-x(1-x))=-\int_{0}^{1} \frac{\log(1+tx(1-x))}{t} \ dt\tag{2} (1) (2) 2I=\int_{0}^{1}\int_{0}^{1}\frac{{\rm Li}_2(-x(1-x))}{x(1-x)}\ \frac{\log(1+tx(1-x))}{t} \ dt\ dx  -\frac{1}{2}\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{x}\:dx-\int _{\frac{1}{\phi ^2}}^1\frac{\ln ^2\left(x\right)}{1-x}\:dx=-\frac{4}{3}\ln ^3\left(\phi \right)-2\sum _{k=1}^{\infty }\frac{1}{k^3}+2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}} 2\sum _{k=1}^{\infty }\frac{1}{k^3\:\phi ^{2k}}+4\ln \left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k^2\:\phi ^{2k}}+4\ln ^2\left(\phi \right)\sum _{k=1}^{\infty }\frac{1}{k\:\phi ^{2k}}=\frac{8}{5}\zeta \left(3\right)+\frac{4}{3}\ln ^3\left(\phi \right)-\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right)+\frac{8}{5}\ln \left(\phi \right)\zeta \left(2\right),"['complex-analysis', 'analysis', 'number-theory', 'analytic-number-theory', 'polylogarithm']"
81,Arbitrarily slow convergence rate,Arbitrarily slow convergence rate,,"First I will present two definitions. Let $f:X \to X$ a continuous measurable transformation. A probability measure $\mu$ defined on the Borel sets is said to be $f$ -invariant if $\mu(f^{-1}(A)) = \mu(A)$ for every Borel set $A$ . A map $f$ is said to be mixing with respect to some $f$ -invariant probability measure $\mu$ if $$|\mu(f^{-n}(A) \cap B) - \mu(A)\mu(B)| \to 0, \qquad\text{when } n \to \infty,$$ for any measurable sets $A,B$ . A natural question that arises is the speed of mixing . In some books and articles I see some variation of the phrase:  "" It is possible to find subsets $A, B$ such that the convergence in the definition of mixing is arbitrarily slow. "" When talking about speed of mixing, I'm talking about a sequence $a(n) \downarrow 0$ , such that $$|\mu(f^{-n}(A) \cap B) - \mu(A)\mu(B)| \le Ca(n), \qquad\text{for all } n.$$ Thus, what the phrase means that if $f$ is mixing then given a sequence it is possible to find sets $A,B$ , so that the inequality above is not true for all $n$ . I haven't found a proof for this claim, just specific examples . What I managed to do was prove this statement when I use the definition of mixing with correlation function (see definition below) in $L^2$ . For this, I used the Riesz representation theorem. For the case above I couldn't prove it. I also don't know if it is necessary to make any assumptions about the space $X$ , in this case we only have $\mu(X)=1$ . I would appreciate an idea for the proof or a reference. Definition of mixing with functions: For measurable functions $\varphi,\psi : X \to \mathbb{R}$ we define the correlation function $$C_n(\varphi,\psi) =\left|\int \psi(\varphi\circ f^n)~d\mu - \int \psi ~d\mu \int \varphi~d\mu\right|.$$ We say that $f$ is mixing if $C_n(\varphi,\psi) \to 0$ as $n\to\infty$ .","First I will present two definitions. Let a continuous measurable transformation. A probability measure defined on the Borel sets is said to be -invariant if for every Borel set . A map is said to be mixing with respect to some -invariant probability measure if for any measurable sets . A natural question that arises is the speed of mixing . In some books and articles I see some variation of the phrase:  "" It is possible to find subsets such that the convergence in the definition of mixing is arbitrarily slow. "" When talking about speed of mixing, I'm talking about a sequence , such that Thus, what the phrase means that if is mixing then given a sequence it is possible to find sets , so that the inequality above is not true for all . I haven't found a proof for this claim, just specific examples . What I managed to do was prove this statement when I use the definition of mixing with correlation function (see definition below) in . For this, I used the Riesz representation theorem. For the case above I couldn't prove it. I also don't know if it is necessary to make any assumptions about the space , in this case we only have . I would appreciate an idea for the proof or a reference. Definition of mixing with functions: For measurable functions we define the correlation function We say that is mixing if as .","f:X \to X \mu f \mu(f^{-1}(A)) = \mu(A) A f f \mu |\mu(f^{-n}(A) \cap B) - \mu(A)\mu(B)| \to 0, \qquad\text{when } n \to \infty, A,B A, B a(n) \downarrow 0 |\mu(f^{-n}(A) \cap B) - \mu(A)\mu(B)| \le Ca(n), \qquad\text{for all } n. f A,B n L^2 X \mu(X)=1 \varphi,\psi : X \to \mathbb{R} C_n(\varphi,\psi) =\left|\int \psi(\varphi\circ f^n)~d\mu - \int \psi ~d\mu \int \varphi~d\mu\right|. f C_n(\varphi,\psi) \to 0 n\to\infty","['probability', 'analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
82,Seeking a (simple) proof that the sphere packing density is always less than $1$ in $n \ge 2$ dimension.,Seeking a (simple) proof that the sphere packing density is always less than  in  dimension.,1 n \ge 2,"For each natural number $n$ , we may define the (optimal) sphere packing density in $\Bbb R^n$ to be the number $$ D_n = \limsup_{r\to\infty} D_n(r), $$ where $$\begin{align} D_n(r) = \sup \Big\{ \frac{k\omega_n}{(2r)^n} :& \text{there exist $a_1,\dots,a_k$ such that $B(a_j)\subset [-r,r]^n$} \\ &\ \  \text{and $B(a_j)$'s are pairwise disjoint} \Big\}. \end{align}$$ Of course, $B(a)$ denotes the open unit ball centered at $a$ , and $\omega_n$ is the $n$ -volume of the unit ball in $\Bbb R^n$ . I have no idea if this is how the density is normally defined or not, but it should be equivalent to the standard definition nonetheless. My $D_n(r)$ is the maximal density of packing unit balls into an $n$ -cube of side length $2r$ , and $D_n$ is the limit as the side length  of this cube goes to infinity. Is there a relatively simple argument to show that $D_n <1$ for all $n\ge 2$ ? I know that calculating $D_n$ precisely is a hard task (except for some very special values of $n$ ), for example the Kepler conjecture, which says that $D_3 = \frac{\pi}{3\sqrt{2}} \simeq 0.7405$ , has only been proved in the last ten year, after it had been an open problem for several centuries. I don't need to know that value of $D_n$ , I just want to know if it's always bounded away from $1$ . While it seems intuitive that we should have $D_n < 1$ always (except for the trivial case $D_1$ , where $1$ -dimensional balls are just lines), I can't think of a quick proof of it off the top of my head. It doesn't seem too hard to show that $D_n(r) < 1$ for each fixed $r$ since there is definitely some wasted space between the unit balls, however, I can't think of any a priori reason why there wouldn't be a better and better possible configuration as $r$ grows large that may make $D_n(r)\to 1$ as $r\to\infty$ . I am thinking that adapting the proof of Besicovitch covering theorem could do the trick, but I haven't tried writing down the details yet. I know essentially nothing regarding this topic, so perhaps the answer to my question is already well-known or even a standard result in sphere packing. If so, I would be delighted if anyone could point me to a good resource that discusses this kind of question.","For each natural number , we may define the (optimal) sphere packing density in to be the number where Of course, denotes the open unit ball centered at , and is the -volume of the unit ball in . I have no idea if this is how the density is normally defined or not, but it should be equivalent to the standard definition nonetheless. My is the maximal density of packing unit balls into an -cube of side length , and is the limit as the side length  of this cube goes to infinity. Is there a relatively simple argument to show that for all ? I know that calculating precisely is a hard task (except for some very special values of ), for example the Kepler conjecture, which says that , has only been proved in the last ten year, after it had been an open problem for several centuries. I don't need to know that value of , I just want to know if it's always bounded away from . While it seems intuitive that we should have always (except for the trivial case , where -dimensional balls are just lines), I can't think of a quick proof of it off the top of my head. It doesn't seem too hard to show that for each fixed since there is definitely some wasted space between the unit balls, however, I can't think of any a priori reason why there wouldn't be a better and better possible configuration as grows large that may make as . I am thinking that adapting the proof of Besicovitch covering theorem could do the trick, but I haven't tried writing down the details yet. I know essentially nothing regarding this topic, so perhaps the answer to my question is already well-known or even a standard result in sphere packing. If so, I would be delighted if anyone could point me to a good resource that discusses this kind of question.","n \Bbb R^n 
D_n = \limsup_{r\to\infty} D_n(r),
 \begin{align}
D_n(r) = \sup \Big\{ \frac{k\omega_n}{(2r)^n} :& \text{there exist a_1,\dots,a_k such that B(a_j)\subset [-r,r]^n} \\
&\ \  \text{and B(a_j)'s are pairwise disjoint} \Big\}.
\end{align} B(a) a \omega_n n \Bbb R^n D_n(r) n 2r D_n D_n <1 n\ge 2 D_n n D_3 = \frac{\pi}{3\sqrt{2}} \simeq 0.7405 D_n 1 D_n < 1 D_1 1 D_n(r) < 1 r r D_n(r)\to 1 r\to\infty","['geometry', 'analysis', 'measure-theory', 'packing-problem']"
83,"Proof: Let $x$, $y \in Q$, $y > 0$, $x > 1$. Then there is an integer $n$ such that $x^n < y ≤ x^{n+1}$.","Proof: Let , , , . Then there is an integer  such that .",x y \in Q y > 0 x > 1 n x^n < y ≤ x^{n+1},"I want to know if my proof of the following result is correct: Let $x$ , $y \in Q$ , $y > 0$ , $x > 1$ . Then there is an integer $n$ such that $x^n < y ≤ x^{n+1}$ . Proof: Suppose I have proved that ""let $x$ , $y \in Q$ , $y > 0$ , $x > 1$ . Then there is a positive integer $n$ such that $x^n > y$ ."" By the above result, there is a positive integer $m_0$ such that $x^{m_0} > y$ . Consider the set $S = \{n \in \mathbb{N}: y > x^{m_0 - n}\}$ . Again, by the above result, there exists a positive integer $m$ such that $1/y < x^m$ . Then $x^{-m} < y < x^{m_0}$ . Thus, $m_0 + m \in S$ where $m_0 + m \in \mathbb{N}$ . By the well-ordering principle, $S$ has a minimal element $m_1$ . Then I claim: $x^{m_0 - m_1} < y \leq x^{m_0 - m_1 + 1}$ . Indeed, $m_1 \neq 0$ since $0 \notin S$ . Thus, $m_1 - 1 \in \mathbb{N}$ , and so if $y > x^{m_0 - m_1 + 1}$ , then $m_1 - 1 \in S$ , contradicting the minimality of $m_1$ . Now we can conclude by letting $n = m_0 - m_1$ .","I want to know if my proof of the following result is correct: Let , , , . Then there is an integer such that . Proof: Suppose I have proved that ""let , , , . Then there is a positive integer such that ."" By the above result, there is a positive integer such that . Consider the set . Again, by the above result, there exists a positive integer such that . Then . Thus, where . By the well-ordering principle, has a minimal element . Then I claim: . Indeed, since . Thus, , and so if , then , contradicting the minimality of . Now we can conclude by letting .",x y \in Q y > 0 x > 1 n x^n < y ≤ x^{n+1} x y \in Q y > 0 x > 1 n x^n > y m_0 x^{m_0} > y S = \{n \in \mathbb{N}: y > x^{m_0 - n}\} m 1/y < x^m x^{-m} < y < x^{m_0} m_0 + m \in S m_0 + m \in \mathbb{N} S m_1 x^{m_0 - m_1} < y \leq x^{m_0 - m_1 + 1} m_1 \neq 0 0 \notin S m_1 - 1 \in \mathbb{N} y > x^{m_0 - m_1 + 1} m_1 - 1 \in S m_1 n = m_0 - m_1,"['real-analysis', 'analysis', 'solution-verification', 'rational-numbers']"
84,Showing that $\{ e_n \}_{n\in\mathbb Z}$ is an orthonormal basis,Showing that  is an orthonormal basis,\{ e_n \}_{n\in\mathbb Z},"I have to show that $ \{ e_n \}_{n \in \mathbb Z} $ , defined by $$ e_n(x) := \frac{1}{\sqrt 2} \mathrm e^{\mathrm i \pi n x} $$ is an orthonormal basis of $L^2([-1,1])$ , equipped with the scalar product $\langle f, g \rangle = \int_{-1}^1 \overline f g $ . I already proved that it is an orthogonal set and it remains to show completeness, i.e. that $0$ is the only vector that is orthogonal to all $e_n$ . As a hint, it is given to show that $ \mathrm{span}\{e_n\}_{n\in\mathbb Z}$ is dense in $C := \{ f \in C([-1,1]) : f(-1) = f(1) \} \cong C(S^1)$ w.r.t. the supremum norm by using Stone-Weierstrass. I don’t want a full proof, rather I want to understand the rough idea. I already do not understand how proving denseness as recommended in the hint helps me in the proof of completeness. I mean, i have to get from „Assume $\langle e_n, f \rangle = 0 \, \forall n \in \mathbb Z$ .“ somehow to „That implies $f = 0$ .“ What’s the idea in between and how does denseness of $C$ play a role here?","I have to show that , defined by is an orthonormal basis of , equipped with the scalar product . I already proved that it is an orthogonal set and it remains to show completeness, i.e. that is the only vector that is orthogonal to all . As a hint, it is given to show that is dense in w.r.t. the supremum norm by using Stone-Weierstrass. I don’t want a full proof, rather I want to understand the rough idea. I already do not understand how proving denseness as recommended in the hint helps me in the proof of completeness. I mean, i have to get from „Assume .“ somehow to „That implies .“ What’s the idea in between and how does denseness of play a role here?"," \{ e_n \}_{n \in \mathbb Z}   e_n(x) := \frac{1}{\sqrt 2} \mathrm e^{\mathrm i \pi n x}  L^2([-1,1]) \langle f, g \rangle = \int_{-1}^1 \overline f g  0 e_n  \mathrm{span}\{e_n\}_{n\in\mathbb Z} C := \{ f \in C([-1,1]) : f(-1) = f(1) \} \cong C(S^1) \langle e_n, f \rangle = 0 \, \forall n \in \mathbb Z f = 0 C","['functional-analysis', 'analysis', 'hilbert-spaces']"
85,Integrability of mean value function,Integrability of mean value function,,"I'm consider a one-dimensional function $u\in L^1(I)$ where $I$ is a bounded open interval. I first extend $u$ to zero outside $I$ . Then I take a small parameter $\epsilon>0$ and define $$u_{\epsilon}(t)=\frac{\int_{t-\epsilon}^{t}u(s)ds}{\epsilon}.$$ By the integrability of $u$ , I know that every point $t\in I$ is a Lebesgue point so $\lim\limits_{\epsilon\to0}u_{\epsilon}(t)=u(t)$ for a.e. $t\in I$ . But I would like to know if $u_{\epsilon}$ can keep the integrability of $u$ i.e. whether $u_{\epsilon}\in L^1(I)$ ? If it can keep this, can we pass to higher dimension? Add what I tried: to show the integrability is actually to calculate that \begin{align*} \int_{I}|u_{\epsilon}(t)|dt&\leq\frac{1}{\epsilon}\int_{I}\int_{t-\epsilon}^{t}|u(s)|dsdt\\ &\leq\frac{1}{\epsilon}\int_{I}\int_{I}|u(s)|dsdt\\ &=\frac{|I|}{\epsilon}\cdot\lVert u\rVert_{1}<\infty. \end{align*} So I guess it can keep the integrability. Does it make sense?","I'm consider a one-dimensional function where is a bounded open interval. I first extend to zero outside . Then I take a small parameter and define By the integrability of , I know that every point is a Lebesgue point so for a.e. . But I would like to know if can keep the integrability of i.e. whether ? If it can keep this, can we pass to higher dimension? Add what I tried: to show the integrability is actually to calculate that So I guess it can keep the integrability. Does it make sense?","u\in L^1(I) I u I \epsilon>0 u_{\epsilon}(t)=\frac{\int_{t-\epsilon}^{t}u(s)ds}{\epsilon}. u t\in I \lim\limits_{\epsilon\to0}u_{\epsilon}(t)=u(t) t\in I u_{\epsilon} u u_{\epsilon}\in L^1(I) \begin{align*}
\int_{I}|u_{\epsilon}(t)|dt&\leq\frac{1}{\epsilon}\int_{I}\int_{t-\epsilon}^{t}|u(s)|dsdt\\
&\leq\frac{1}{\epsilon}\int_{I}\int_{I}|u(s)|dsdt\\
&=\frac{|I|}{\epsilon}\cdot\lVert u\rVert_{1}<\infty.
\end{align*}","['real-analysis', 'analysis']"
86,"If $|f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}}$, why is true that $\|\nabla f\|_{\infty} \le c?$","If , why is true that",|f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}} \|\nabla f\|_{\infty} \le c?,"Let $N>4$ and $2a>N$ . Consider the function $f:\mathbb R^N\to\mathbb R$ be a function of class $C^2$ such that $$|f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}}.$$ Is it possible to use this information to deduce that there exists a constant $c>0$ such that $$\|\nabla f\|_{\infty} \le c?$$ At calc class today, it was exhibited as an obvious thing while proving a theorem. Could someone please help me understanding why is that true? If it is not, which extra assumption on $f$ could guarantee that $\|\nabla f\|_{\infty}\le c?$ I am a 2 year physics student and I am not so familiar with this kind of things. Thank you everyone.","Let and . Consider the function be a function of class such that Is it possible to use this information to deduce that there exists a constant such that At calc class today, it was exhibited as an obvious thing while proving a theorem. Could someone please help me understanding why is that true? If it is not, which extra assumption on could guarantee that I am a 2 year physics student and I am not so familiar with this kind of things. Thank you everyone.",N>4 2a>N f:\mathbb R^N\to\mathbb R C^2 |f(x)|\le \frac{1}{(1+|x|^2)^{\alpha/2}}. c>0 \|\nabla f\|_{\infty} \le c? f \|\nabla f\|_{\infty}\le c?,"['real-analysis', 'calculus', 'analysis', 'upper-lower-bounds']"
87,An upper bound of $n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k$ as $n$ fixed and $k$ varies,An upper bound of  as  fixed and  varies,n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k n k,"I'd like to ask an upper bound $C_n$ such that $$ S_n(k):=n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k\leq C_n\cdot(1-1/n)^k $$ for all (sufficiently large) $k$ . A naive upper bound is to replace all $i^k$ with $(n-1)^k$ , which produces $$ S_n(k)\leq(2^n-2)\cdot(1-1/n)^k. $$ On the other hand, by only considering $i=n-1$ term one get $$ S_n(k)\geq n\cdot(1-1/n)^k. $$ I'd like to ask if one can get $C_n$ better than $2^n-2$ (namely, growth slower as $n\to\infty$ )? I notice that $$ \sum_{i=1}^{n-1}\binom{n}{i}i^k=\left(X\frac{\mathrm d}{\mathrm dX}\right)^k \big((1+X)^n-1-X^n\big)\bigg|_{X=1}. $$ But I failed to find a way to produce an upper bound from this. Any suggestions? EDIT: It turns out that I have asked a stupid question. It is equivalent to ask an upper bound of $$ C_n(k):=(n-1)^k\sum_{i=1}^{n-1}\binom{n}{i}i^k =\sum_{i=1}^{n-1}\binom{n}{i}\left(\frac{i}{n-1}\right)^k $$ for either (i) all $k\geq 0$ or (ii) for sufficiently large $k$ . It's clear that $C_n(k)$ decreases as $k$ increases. For (i) the $2^n-2$ cannot be improved since $C_n(0)=2^n-2$ . For (ii) it's $n+\epsilon$ for any $\epsilon>0$ (here the ""sufficiently large"" depending on $n$ and $\epsilon$ ) since $\lim_{k\to\infty}C_n(k)=n$ . What estimation is useful in my application is still not clear to me. Perhaps choose a $k_0$ (independent of $n$ ) and ask a simple upper bound of $C_n(k_0)$ ?","I'd like to ask an upper bound such that for all (sufficiently large) . A naive upper bound is to replace all with , which produces On the other hand, by only considering term one get I'd like to ask if one can get better than (namely, growth slower as )? I notice that But I failed to find a way to produce an upper bound from this. Any suggestions? EDIT: It turns out that I have asked a stupid question. It is equivalent to ask an upper bound of for either (i) all or (ii) for sufficiently large . It's clear that decreases as increases. For (i) the cannot be improved since . For (ii) it's for any (here the ""sufficiently large"" depending on and ) since . What estimation is useful in my application is still not clear to me. Perhaps choose a (independent of ) and ask a simple upper bound of ?","C_n 
S_n(k):=n^{-k}\sum_{i=1}^{n-1}\binom{n}{i}i^k\leq C_n\cdot(1-1/n)^k
 k i^k (n-1)^k 
S_n(k)\leq(2^n-2)\cdot(1-1/n)^k.
 i=n-1 
S_n(k)\geq n\cdot(1-1/n)^k.
 C_n 2^n-2 n\to\infty 
\sum_{i=1}^{n-1}\binom{n}{i}i^k=\left(X\frac{\mathrm d}{\mathrm dX}\right)^k
\big((1+X)^n-1-X^n\big)\bigg|_{X=1}.
 
C_n(k):=(n-1)^k\sum_{i=1}^{n-1}\binom{n}{i}i^k
=\sum_{i=1}^{n-1}\binom{n}{i}\left(\frac{i}{n-1}\right)^k
 k\geq 0 k C_n(k) k 2^n-2 C_n(0)=2^n-2 n+\epsilon \epsilon>0 n \epsilon \lim_{k\to\infty}C_n(k)=n k_0 n C_n(k_0)","['combinatorics', 'analysis', 'inequality']"
88,Prove a function defined on an open ball with radius $1$ is onto a open ball with radius $0.4$,Prove a function defined on an open ball with radius  is onto a open ball with radius,1 0.4,"I use $B_{r}$ to denote an open ball centered at $0$ with radius $r$ . Consider Euclidean metric and a function $$\begin{align} f:B_{1} \to \mathbb{R}^n \end{align}$$ with $f(0)=0$ such that $$\begin{align} \forall x,y\in B_{1}, x\ne y\implies|f(y)-f(x)-(y-x)|<0.1|y-x|  &&(*) \end{align}$$ I want to show this function is onto ball $B_{0.4}$ . That is, for all $z\in B_{0.4}$ ,there exists an $x$ such that $z=f(x)$ I tried to use $|z|<0.4$ , $f(0)=0$ and $(*)$ . Let $y=0$ and $x\ne 0$ , and use $(*)$ I get $$\begin{align} |f(x)-x|<0.1|x|<0.1 \end{align}$$ since $x\in B_{1}$ , $|x|<1$ . Since $|f(x)-x|<0.1$ , I can say that $f(x)-x\in B_{0.1}$ Also, I can see the function is injective. However, I can't relate all the things that I have to prove $f$ is onto $B_{0,4}$ , or maybe I miss some hidden information in the problem. Any help on this? Thanks.","I use to denote an open ball centered at with radius . Consider Euclidean metric and a function with such that I want to show this function is onto ball . That is, for all ,there exists an such that I tried to use , and . Let and , and use I get since , . Since , I can say that Also, I can see the function is injective. However, I can't relate all the things that I have to prove is onto , or maybe I miss some hidden information in the problem. Any help on this? Thanks.","B_{r} 0 r \begin{align}
f:B_{1} \to \mathbb{R}^n
\end{align} f(0)=0 \begin{align}
\forall x,y\in B_{1}, x\ne y\implies|f(y)-f(x)-(y-x)|<0.1|y-x|  &&(*)
\end{align} B_{0.4} z\in B_{0.4} x z=f(x) |z|<0.4 f(0)=0 (*) y=0 x\ne 0 (*) \begin{align}
|f(x)-x|<0.1|x|<0.1
\end{align} x\in B_{1} |x|<1 |f(x)-x|<0.1 f(x)-x\in B_{0.1} f B_{0,4}","['general-topology', 'analysis']"
89,Deducing the Fermi sea is ground state for the kinetic energy operator $\sum_j(-\Delta_{x_j})$,Deducing the Fermi sea is ground state for the kinetic energy operator,\sum_j(-\Delta_{x_j}),"I am reading a paper and have no background in quantum mechanics. Any help/references would be appreciated. The ground state for a system of $N$ non-interacting fermions on the unit torus $\mathbb T^d$ has the following simple form. In frequency space, $$\Big(\psi_N,\sum_j(-\Delta_{x_j})\psi_N\Big)=\sum_{p_i\in\mathbb Z^d,\,i=1,...,N}\Big(\sum_{j=1}^N p_j^2\Big)|\widehat\psi_N(p_1,...,p_N)|^2.$$ (This follows from Plancherel on $\mathbb T^d$ .) My question is about the following conclusion from this result. The paper I am reading says, Therefore, it is easy to see that the lowest energy configuration is given by $\widehat\psi_N={\bigwedge}_{i\in F_n}\delta_i$ , where $F_N\subset\mathbb Z^d$ is the subset of $N$ distinct lattice points closest to the origin, and $\delta_i$ is the Kronecker delta at $i\in\mathbb Z^d$ . Clearly, the set $F_N$ is an approximate ball $B_{R_N}(0)\cap\mathbb Z^d$ of radius $R_N\sim N^{1/d}$ , and it is referred to as the Fermi sea. My questions are: How do I deduce from the equation that the lowest energy configuration is given by $\widehat\psi_N={\bigwedge}_{i\in F_n}\delta_i$ , where $F_N\subset\mathbb Z^d$ is the subset of $N$ distinct lattice points closest to the origin? Why is $F_N$ is an approximate ball $B_{R_N}(0)\cap\mathbb Z^d$ of radius $R_N\sim N^{1/d}$ ?","I am reading a paper and have no background in quantum mechanics. Any help/references would be appreciated. The ground state for a system of non-interacting fermions on the unit torus has the following simple form. In frequency space, (This follows from Plancherel on .) My question is about the following conclusion from this result. The paper I am reading says, Therefore, it is easy to see that the lowest energy configuration is given by , where is the subset of distinct lattice points closest to the origin, and is the Kronecker delta at . Clearly, the set is an approximate ball of radius , and it is referred to as the Fermi sea. My questions are: How do I deduce from the equation that the lowest energy configuration is given by , where is the subset of distinct lattice points closest to the origin? Why is is an approximate ball of radius ?","N \mathbb T^d \Big(\psi_N,\sum_j(-\Delta_{x_j})\psi_N\Big)=\sum_{p_i\in\mathbb Z^d,\,i=1,...,N}\Big(\sum_{j=1}^N p_j^2\Big)|\widehat\psi_N(p_1,...,p_N)|^2. \mathbb T^d \widehat\psi_N={\bigwedge}_{i\in F_n}\delta_i F_N\subset\mathbb Z^d N \delta_i i\in\mathbb Z^d F_N B_{R_N}(0)\cap\mathbb Z^d R_N\sim N^{1/d} \widehat\psi_N={\bigwedge}_{i\in F_n}\delta_i F_N\subset\mathbb Z^d N F_N B_{R_N}(0)\cap\mathbb Z^d R_N\sim N^{1/d}","['functional-analysis', 'analysis', 'partial-differential-equations', 'physics', 'quantum-mechanics']"
90,Classifying divergent sequences in a metric space,Classifying divergent sequences in a metric space,,"To my understanding, in $\mathbb{R}$ , we have the following ways in which a sequence can diverge: The sequence could diverge off into $\infty$ or $-\infty$ ( relevant generalization ) Divergence by oscillation: A sequence that neither converges to a finite number nor diverges to either $∞$ or $−∞$ is said to oscillate or diverge by oscillation. 2.1.An oscillating sequence with finite amplitude is called a finitely oscillating sequence. eg: $\{ (-1)^n \}$ 2.2.An oscillating sequence with infinite amplitude is called an infinitely oscillating sequence.   Eg: $\{ n(-1)^n \}$ In contrast, for a general metric space, is there a way to classify the ways of divergence similar to the above? If so, what are some examples of ways which a sequence could diverge in a general metric space which can't happen in $\mathbb{R}$ ? Notes: On divergence by oscillation Source","To my understanding, in , we have the following ways in which a sequence can diverge: The sequence could diverge off into or ( relevant generalization ) Divergence by oscillation: A sequence that neither converges to a finite number nor diverges to either or is said to oscillate or diverge by oscillation. 2.1.An oscillating sequence with finite amplitude is called a finitely oscillating sequence. eg: 2.2.An oscillating sequence with infinite amplitude is called an infinitely oscillating sequence.   Eg: In contrast, for a general metric space, is there a way to classify the ways of divergence similar to the above? If so, what are some examples of ways which a sequence could diverge in a general metric space which can't happen in ? Notes: On divergence by oscillation Source",\mathbb{R} \infty -\infty ∞ −∞ \{ (-1)^n \} \{ n(-1)^n \} \mathbb{R},"['sequences-and-series', 'analysis', 'metric-spaces', 'applications']"
91,Motivating Gauss's suggestions of prize problems for the Goettingen university.,Motivating Gauss's suggestions of prize problems for the Goettingen university.,,"(This question was posted before in History of Science and Mathematics stackexchange, but since I recieved no comments after 6 days, I decided to reask it here.) P. 220-221 of volume 12 of Gauss's werke contain a complete list of the prize problems which Gauss suggested to the Goettingen university in the years 1830, 1834, 1842 and 1849. Those prize problems can shed some light on his intentions for further studies of several mathematical questions whose solution he did not complete. Here is a Google translation (from Latin) of some of the problems in the list: 1830, May 6. [1] Derive the general criterion for the solvability of the differential trinomial $pdx^2+2qdxdy+rdy^2$ into two factors, each of which is a complete differential. [2] Determine the line joining two given points which, when rotated about a given axis, produces a surface of smallest area. [3] To explain the character of the curve, in which the radius of curvature is everywhere reciprocally proportional to the length of the curve. [4] To progress the current state of our knowledge about the periodically variable stars. [5] Demonstrate the equality between the infinite series $$1+(\frac{1}{2})^3x+(\frac{1\cdot3}{2\cdot4})^3x^2+(\frac{1\cdot3\cdot5}{2\cdot4\cdot6})^3x^3+etc$$ and the square of the infinite series $$1+(\frac{1}{4})^2x+(\frac{1\cdot5}{4\cdot8})^2x^2+(\frac{1\cdot5\cdot9}{4\cdot8\cdot12})^2x^3+etc$$ 1834, May 21. [1] Determine the moment of inertia of the five Platonic solids with respect to any axis through the center.[2] To explain the various methods of solving Kepler's problem, especially by means of infinite series, as well as addressing the degree of convergence of these developments. 1842, May 23. [1] To understand the methods for finding any number of right angled spherical triangles whose sides and angles have rational sines and cosines. Note that I have omitted a few problems for which the translation was not good enough. Comments: Problem [2] from 1830 concerns ""minimal surfaces"" and was solved by Gauss's pupil Benjamin Goldschmidt. Since the solution of this problem (the catenary) was already well known during the 18th century (L. Euler discovered it), I guess Gauss's real intention was to make the methods related to minimal surfaces more rigorous. Problem [3] from 1830 concerns the so-called "" Euler spiral "". Problem [4] from 1830 concerns, according to other sources I read ,the construction of a reliable ""photometer"" (this is the subject of another question I posted long ago on hsm stackexchange). Problem [5] from 1830 was not set as a prize problem by the Goettingen university (it remained merely a suggestion). However, Gauss apparently solved it by himself in p.191-193 of volume 10 of his werke; he apparently shows that when $x=1$ than the first series and the square of the second series are both equal to $2\frac{\varpi^2}{\pi^2}$ . It seems that Gauss's identity is a case of Clausen's formula (Clausen deduces the same identity in his article ); this raises the question why Gauss suggested to set this as a prize problem if Thomas Clausen has already proved it in 1828. Problem [1] from 1834 was solved by the german mathematician Feodor Deahna in his prize awarded essay from 1835 ""Momenta inertiae singulorum quinque corporum regularium"". Apart from the fact that the Platonic solids are spherical tops, I find this problem to be also very interesting. What is most interesting to me is problem [1] from 1830 - it deals with a differential form (such as the first and second fundamental forms), therefore it seems that it is concerned with a certain problem of differential geometry. Since it was only 2 years after Gauss's groundbreaking memoir on differential geometry, I guess it is reasonable to believe it might be related to some differential-geometric problem that occupied Gauss's mind during this time. My questions Since the only problem whose meaning I did not recognize yet is problem [1] from 1830, my main question is about the meaning of it, and how it merges into the context of Gauss's differential geometric ideas. Regarding the other problems, since it is not possible to cover the whole context for all these problems in a single post, I mainly aim to make a ""brain storming"" on Gauss's list of problems, so any interesting/useful comments on the other problems will be blessed! Several clues regarding problem [1] : Based on several clues in Gauss's Nachlass, I have a very ""unfounded intuition"" that problem [1] is related to a problem in differential geometry which occupied him during this time. In p.446 of volume 8 of Gauss's werke, Gauss states the following: The conditional equation that $$pdx^2+2qdxdy+rdy^2$$ is a pure product of two total differentials is the following: $$0 = 2(q^2-pr)(\frac{\partial^2p}{\partial y^2}-2\frac{\partial^2q}{\partial x\partial y}+\frac{\partial^2r}{\partial x^2})+p(\frac{\partial p}{\partial y}\frac{\partial r}{\partial y}-2\frac{\partial q}{\partial x}\frac{\partial r}{\partial y}+(\frac{\partial r}{\partial x})^2)+q(\frac{\partial p}{\partial x}\frac{\partial r}{\partial y}-\frac{\partial p}{\partial y}\frac{\partial r}{\partial x}+4\frac{\partial q}{\partial x}\frac{\partial q}{\partial y}-2\frac{\partial q}{\partial x}\frac{\partial r}{\partial x}-2\frac{\partial p}{\partial y}\frac{\partial q}{\partial y})+r(\frac{\partial p}{\partial x}\frac{\partial r}{\partial x}-2\frac{\partial p}{\partial x}\frac{\partial q}{\partial y}+(\frac{\partial p}{\partial y})^2)$$ In his comments on this note, Stackel remarks that in the same notebook in which Gauss wrote this statement, immediately after it appears a short note entitled ""general solution to the problem of development of a surface"", which appears in p.447-448 of the same volume. The mathematician Julius Weingarten comments on Gauss's approach in this ""general solution"", and says several things which I was unable to understand but concludes that Gauss's solution is incomplete. Rephrasing Gauss's problem in modern terms, I guess what he attempted to solve is: ""given the surface metric, find all possible isometric embeddings of it in $\mathbb{R^3}$ "" (I interpreted ""development"" as ""isometric embedding""). This problem in an extension of the Theorema Egregium, and since it is perhaps the only aspect that Gauss left uncomplete in his memoir of differential geometry, I suspect there might be a connection. Many mathematicians - among them Ferdinand Minding and Weingarten himself - have contributed to its solution. Since I don't have enough knowledge in differential geometry to understand what is going on there, I stress that this supposed connection between Gauss's prize problem from 1830 and the problem of ""finding all isometric embeddings"" is entirely a conjecture of mine based on ""crossing the loose ends"". I have no idea how the problem of factoring a differential form into two total differentials is related to isometric embedding of a given metric.","(This question was posted before in History of Science and Mathematics stackexchange, but since I recieved no comments after 6 days, I decided to reask it here.) P. 220-221 of volume 12 of Gauss's werke contain a complete list of the prize problems which Gauss suggested to the Goettingen university in the years 1830, 1834, 1842 and 1849. Those prize problems can shed some light on his intentions for further studies of several mathematical questions whose solution he did not complete. Here is a Google translation (from Latin) of some of the problems in the list: 1830, May 6. [1] Derive the general criterion for the solvability of the differential trinomial into two factors, each of which is a complete differential. [2] Determine the line joining two given points which, when rotated about a given axis, produces a surface of smallest area. [3] To explain the character of the curve, in which the radius of curvature is everywhere reciprocally proportional to the length of the curve. [4] To progress the current state of our knowledge about the periodically variable stars. [5] Demonstrate the equality between the infinite series and the square of the infinite series 1834, May 21. [1] Determine the moment of inertia of the five Platonic solids with respect to any axis through the center.[2] To explain the various methods of solving Kepler's problem, especially by means of infinite series, as well as addressing the degree of convergence of these developments. 1842, May 23. [1] To understand the methods for finding any number of right angled spherical triangles whose sides and angles have rational sines and cosines. Note that I have omitted a few problems for which the translation was not good enough. Comments: Problem [2] from 1830 concerns ""minimal surfaces"" and was solved by Gauss's pupil Benjamin Goldschmidt. Since the solution of this problem (the catenary) was already well known during the 18th century (L. Euler discovered it), I guess Gauss's real intention was to make the methods related to minimal surfaces more rigorous. Problem [3] from 1830 concerns the so-called "" Euler spiral "". Problem [4] from 1830 concerns, according to other sources I read ,the construction of a reliable ""photometer"" (this is the subject of another question I posted long ago on hsm stackexchange). Problem [5] from 1830 was not set as a prize problem by the Goettingen university (it remained merely a suggestion). However, Gauss apparently solved it by himself in p.191-193 of volume 10 of his werke; he apparently shows that when than the first series and the square of the second series are both equal to . It seems that Gauss's identity is a case of Clausen's formula (Clausen deduces the same identity in his article ); this raises the question why Gauss suggested to set this as a prize problem if Thomas Clausen has already proved it in 1828. Problem [1] from 1834 was solved by the german mathematician Feodor Deahna in his prize awarded essay from 1835 ""Momenta inertiae singulorum quinque corporum regularium"". Apart from the fact that the Platonic solids are spherical tops, I find this problem to be also very interesting. What is most interesting to me is problem [1] from 1830 - it deals with a differential form (such as the first and second fundamental forms), therefore it seems that it is concerned with a certain problem of differential geometry. Since it was only 2 years after Gauss's groundbreaking memoir on differential geometry, I guess it is reasonable to believe it might be related to some differential-geometric problem that occupied Gauss's mind during this time. My questions Since the only problem whose meaning I did not recognize yet is problem [1] from 1830, my main question is about the meaning of it, and how it merges into the context of Gauss's differential geometric ideas. Regarding the other problems, since it is not possible to cover the whole context for all these problems in a single post, I mainly aim to make a ""brain storming"" on Gauss's list of problems, so any interesting/useful comments on the other problems will be blessed! Several clues regarding problem [1] : Based on several clues in Gauss's Nachlass, I have a very ""unfounded intuition"" that problem [1] is related to a problem in differential geometry which occupied him during this time. In p.446 of volume 8 of Gauss's werke, Gauss states the following: The conditional equation that is a pure product of two total differentials is the following: In his comments on this note, Stackel remarks that in the same notebook in which Gauss wrote this statement, immediately after it appears a short note entitled ""general solution to the problem of development of a surface"", which appears in p.447-448 of the same volume. The mathematician Julius Weingarten comments on Gauss's approach in this ""general solution"", and says several things which I was unable to understand but concludes that Gauss's solution is incomplete. Rephrasing Gauss's problem in modern terms, I guess what he attempted to solve is: ""given the surface metric, find all possible isometric embeddings of it in "" (I interpreted ""development"" as ""isometric embedding""). This problem in an extension of the Theorema Egregium, and since it is perhaps the only aspect that Gauss left uncomplete in his memoir of differential geometry, I suspect there might be a connection. Many mathematicians - among them Ferdinand Minding and Weingarten himself - have contributed to its solution. Since I don't have enough knowledge in differential geometry to understand what is going on there, I stress that this supposed connection between Gauss's prize problem from 1830 and the problem of ""finding all isometric embeddings"" is entirely a conjecture of mine based on ""crossing the loose ends"". I have no idea how the problem of factoring a differential form into two total differentials is related to isometric embedding of a given metric.",pdx^2+2qdxdy+rdy^2 1+(\frac{1}{2})^3x+(\frac{1\cdot3}{2\cdot4})^3x^2+(\frac{1\cdot3\cdot5}{2\cdot4\cdot6})^3x^3+etc 1+(\frac{1}{4})^2x+(\frac{1\cdot5}{4\cdot8})^2x^2+(\frac{1\cdot5\cdot9}{4\cdot8\cdot12})^2x^3+etc x=1 2\frac{\varpi^2}{\pi^2} pdx^2+2qdxdy+rdy^2 0 = 2(q^2-pr)(\frac{\partial^2p}{\partial y^2}-2\frac{\partial^2q}{\partial x\partial y}+\frac{\partial^2r}{\partial x^2})+p(\frac{\partial p}{\partial y}\frac{\partial r}{\partial y}-2\frac{\partial q}{\partial x}\frac{\partial r}{\partial y}+(\frac{\partial r}{\partial x})^2)+q(\frac{\partial p}{\partial x}\frac{\partial r}{\partial y}-\frac{\partial p}{\partial y}\frac{\partial r}{\partial x}+4\frac{\partial q}{\partial x}\frac{\partial q}{\partial y}-2\frac{\partial q}{\partial x}\frac{\partial r}{\partial x}-2\frac{\partial p}{\partial y}\frac{\partial q}{\partial y})+r(\frac{\partial p}{\partial x}\frac{\partial r}{\partial x}-2\frac{\partial p}{\partial x}\frac{\partial q}{\partial y}+(\frac{\partial p}{\partial y})^2) \mathbb{R^3},"['analysis', 'differential-geometry', 'math-history']"
92,Strassmann's thoerem and irrationality measure of certain number,Strassmann's thoerem and irrationality measure of certain number,,"In this note from Keith Conrad, he explains an interesting application of Strassmann's theorem to the divergence of certain linear recurrence integer sequence. More precisely, the sequence defined as $a_0 = a_1 = 1$ and $a_{m} = 2a_{m-1} - 3a_{m-2}$ satisfies $\lim_{m\to \infty} |a_m| = \infty$ . It seems somewhat easy to prove at first glance (and the actual behavior of $|a_m|$ is exponential), one needs to deal with possible cancellation. The general term is given by $$ a_m = \frac{1}{2} ((1 + \sqrt{-2})^{m} + (1 - \sqrt{-2})^{m}) = \sqrt{3}^m \cos (m \alpha) $$ where $\alpha = \arctan(\sqrt{2})$ , and if $m\alpha$ is sufficiently close to the odd multiple of $\pi / 2$ , then $\cos(m\alpha)$ could be very close to zero. So one may need to show that $\cos(m\alpha)$ can't be exponentially small with respect to $m$ in some sense, or use $p$ -adic method as in Conrad's note. What I thought is that once we know $\lim_{m\to\infty}|a_m| = \infty$ , this would tell us that $\alpha / \pi$ can't be very close to rational numbers, and may tell something about irrationality measure of $\alpha / \pi$ . I tried some but didn't get anything useful at this moment. Is it possible to deduce some information on the irrationality measure of $\alpha /\pi$ using divergence, at least finiteness or infiniteness?","In this note from Keith Conrad, he explains an interesting application of Strassmann's theorem to the divergence of certain linear recurrence integer sequence. More precisely, the sequence defined as and satisfies . It seems somewhat easy to prove at first glance (and the actual behavior of is exponential), one needs to deal with possible cancellation. The general term is given by where , and if is sufficiently close to the odd multiple of , then could be very close to zero. So one may need to show that can't be exponentially small with respect to in some sense, or use -adic method as in Conrad's note. What I thought is that once we know , this would tell us that can't be very close to rational numbers, and may tell something about irrationality measure of . I tried some but didn't get anything useful at this moment. Is it possible to deduce some information on the irrationality measure of using divergence, at least finiteness or infiniteness?","a_0 = a_1 = 1 a_{m} = 2a_{m-1} - 3a_{m-2} \lim_{m\to \infty} |a_m| = \infty |a_m| 
a_m = \frac{1}{2} ((1 + \sqrt{-2})^{m} + (1 - \sqrt{-2})^{m}) = \sqrt{3}^m \cos (m \alpha)
 \alpha = \arctan(\sqrt{2}) m\alpha \pi / 2 \cos(m\alpha) \cos(m\alpha) m p \lim_{m\to\infty}|a_m| = \infty \alpha / \pi \alpha / \pi \alpha /\pi","['sequences-and-series', 'analysis', 'number-theory', 'p-adic-number-theory', 'irrationality-measure']"
93,"If $a_n=\sin(n^2)$, how to construct (if possible) a subsequence such that $\lim a_{n_k}\rightarrow 0$?","If , how to construct (if possible) a subsequence such that ?",a_n=\sin(n^2) \lim a_{n_k}\rightarrow 0,"For $a_n=\sin(n)$ , it can use the continued fraction method to construct the subsequence, so that $\lim a_{n_k}\rightarrow 0$ . But for $a_n=\sin(n^2)$ , how to construct (if possible) a subsequence such that it converges to $0$ ?","For , it can use the continued fraction method to construct the subsequence, so that . But for , how to construct (if possible) a subsequence such that it converges to ?",a_n=\sin(n) \lim a_{n_k}\rightarrow 0 a_n=\sin(n^2) 0,['analysis']
94,When can we interchange x and y in a double integral?,When can we interchange x and y in a double integral?,,"I understand the principle of substituting variables in a double integral and using the Jacobian to multiply by the correct factor, but if I have a double integral in x and y, when is it justified to be able to interchange the x and y terms? For example in the following integral: $$\int \int \frac{1+2x^2}{1+x^4+6x^2y^2+y^4} -\frac{1+y^2}{2+x^4+y^4}dxdy$$ integrated over $x^2+y^2\leq R^2$ for some positive real $R$ . Can I interchange the x and y terms without changing the value of the integral? I.e. is this integral equal to the following integral: $$\int \int \frac{1+2y^2}{1+y^4+6x^2y^2+x^4} -\frac{1+x^2}{2+x^4+y^4}dxdy$$ (x and y have been interchanged). If yes is it because the region where it is integrated in is circular thus independent of x and y? I am having a tough time understanding when we can do this and when we can't.","I understand the principle of substituting variables in a double integral and using the Jacobian to multiply by the correct factor, but if I have a double integral in x and y, when is it justified to be able to interchange the x and y terms? For example in the following integral: integrated over for some positive real . Can I interchange the x and y terms without changing the value of the integral? I.e. is this integral equal to the following integral: (x and y have been interchanged). If yes is it because the region where it is integrated in is circular thus independent of x and y? I am having a tough time understanding when we can do this and when we can't.",\int \int \frac{1+2x^2}{1+x^4+6x^2y^2+y^4} -\frac{1+y^2}{2+x^4+y^4}dxdy x^2+y^2\leq R^2 R \int \int \frac{1+2y^2}{1+y^4+6x^2y^2+x^4} -\frac{1+x^2}{2+x^4+y^4}dxdy,"['integration', 'analysis', 'improper-integrals']"
95,Proving a property of the Fourier coefficients of a continuous function,Proving a property of the Fourier coefficients of a continuous function,,"We have that the function f defined on [- $\pi$ , $\pi$ ] is continuous, and we want to prove the following statement about its Fourier coefficients $c_n$ $$c_n = c_{-n}\;\; \text{for all}\; n\; \Rightarrow \text{f even}. $$ I am using the convention $$c_n = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt .$$ I have come up with two ways to show it but I am not sure if there are any flaws with method 2. Method 1: $$c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt $$ The variable change $t\to -t$ in the second integral gives the equality $$ \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(-t)e^{-int}dt \\ \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} (f(t)-f(-t))e^{-int}dx = 0 $$ Now, the integrand in the last integral is a continuous function and for such functions the integral is zero iff the integral is identically zero over that interval. Hence $$ (f(t)-f(-t))e^{-int} = 0 \iff f(t) = f(-t), t\in[-\pi,\pi] \iff \text{f even}. $$ Method 2: $$ c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt \\ \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)(e^{int}-e^{-int})dx = 0 \iff \int_{-\pi}^{\pi} f(t)sin(nt) dt = 0 \\ \Rightarrow f(t)sin(nt)\;\; \text{odd}\; \Rightarrow f(t)\; \text{even}.$$ I am unsure of whether I can conclude that f(t)sin(nt) is an odd function. I guess there is the possibility that the integral over the symmetric interval vanishes for other reasons. I think the implication only goes in the direction of ""if g is odd then $\int_{-a}^a g(t)dt = 0$ "". I also wanted to ask if the statements $$ c_n = -c_{-n} \; \Rightarrow \; \text{f odd}. $$ and $$\overline{c_n} = c_{-n} \; \Rightarrow \; \text{f real-valued}$$ are proven in the same manner as the above.","We have that the function f defined on [- , ] is continuous, and we want to prove the following statement about its Fourier coefficients I am using the convention I have come up with two ways to show it but I am not sure if there are any flaws with method 2. Method 1: The variable change in the second integral gives the equality Now, the integrand in the last integral is a continuous function and for such functions the integral is zero iff the integral is identically zero over that interval. Hence Method 2: I am unsure of whether I can conclude that f(t)sin(nt) is an odd function. I guess there is the possibility that the integral over the symmetric interval vanishes for other reasons. I think the implication only goes in the direction of ""if g is odd then "". I also wanted to ask if the statements and are proven in the same manner as the above.","\pi \pi c_n c_n = c_{-n}\;\; \text{for all}\; n\; \Rightarrow \text{f even}.  c_n = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt . c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt  t\to -t  \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(-t)e^{-int}dt \\
\iff \frac{1}{2\pi} \int_{-\pi}^{\pi} (f(t)-f(-t))e^{-int}dx = 0   (f(t)-f(-t))e^{-int} = 0 \iff f(t) = f(-t), t\in[-\pi,\pi] \iff \text{f even}.   c_n = c_{-n} \iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{-int}dt = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)e^{int}dt \\
\iff \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)(e^{int}-e^{-int})dx = 0 \iff \int_{-\pi}^{\pi} f(t)sin(nt) dt = 0 \\
\Rightarrow f(t)sin(nt)\;\; \text{odd}\; \Rightarrow f(t)\; \text{even}. \int_{-a}^a g(t)dt = 0  c_n = -c_{-n} \; \Rightarrow \; \text{f odd}.  \overline{c_n} = c_{-n} \; \Rightarrow \; \text{f real-valued}","['calculus', 'analysis', 'fourier-analysis', 'fourier-series']"
96,Approximation scheme for potentials in Schrodinger Equation- A Request,Approximation scheme for potentials in Schrodinger Equation- A Request,,"Setup: The Time Independent Schrodinger Equation(Eigenvalue problem): $(-\frac{\hbar^2}{2m}\Delta +V)\psi = E\psi$ When dealing with computing bound states/bound state energies of say an electron in complicated potentials, one trick I have seen informally is that we can replace the given complicated potential with something that has 'similar behaviour' but has known eigenstates/eigenvalues to 'estimate' the eigenstates/eigenvalues. What I wish to know is, under what conditions is something like this scheme faithful/what conditions need to be imposed to achieve this kind of approximation? Roughly speaking, If $V$ is the given potential, and $V_{\epsilon}$ is another potential whose eigenvalues and eigenstates are known, such that $\|V_\epsilon-V\|<\epsilon$ (in some norm) then if $E_{\epsilon}, E$ denote the ground state energies in $V_{\epsilon}, V$ respectively, then what is an estimate on $|E_{\epsilon}-E|$ ? Is it of the order $\epsilon$ ? I do not know what this class of problems is called so I could not look for it by just googling for example. Any references on it are requested.","Setup: The Time Independent Schrodinger Equation(Eigenvalue problem): When dealing with computing bound states/bound state energies of say an electron in complicated potentials, one trick I have seen informally is that we can replace the given complicated potential with something that has 'similar behaviour' but has known eigenstates/eigenvalues to 'estimate' the eigenstates/eigenvalues. What I wish to know is, under what conditions is something like this scheme faithful/what conditions need to be imposed to achieve this kind of approximation? Roughly speaking, If is the given potential, and is another potential whose eigenvalues and eigenstates are known, such that (in some norm) then if denote the ground state energies in respectively, then what is an estimate on ? Is it of the order ? I do not know what this class of problems is called so I could not look for it by just googling for example. Any references on it are requested.","(-\frac{\hbar^2}{2m}\Delta +V)\psi = E\psi V V_{\epsilon} \|V_\epsilon-V\|<\epsilon E_{\epsilon}, E V_{\epsilon}, V |E_{\epsilon}-E| \epsilon","['analysis', 'partial-differential-equations', 'reference-request', 'physics', 'mathematical-physics']"
97,Trying to solve a functional equation,Trying to solve a functional equation,,"Let $a_n$ be a sequence of strictly positive real numbers such that $\lim_{n \to \infty}a_n=0$ . Find all functions $f: \mathbb{R} \to \mathbb{R}$ that admit primitives(i.e. there exists a function $F:\mathbb{R} \to \mathbb{R}$ such that $\frac{dF(x)}{dx}=f(x), \forall x \in \mathbb{R}$ ) and satisfies the following equality $$2f(x)=f(x+a_n)+f(x-a_n), \forall x \in \mathbb{R}, \forall n \in \mathbb{N}$$ My approach to this problem was to first use the fact that there exists a function $F:\mathbb{R} \to \mathbb{R}$ , such that $F'(x)=f(x)$ , so the equation becomes $2F(x)=F(x+a_n)+F(x-a_n)+C_n$ , for some arbitrary constant $C_n$ . Rearranging the terms, we get that $$0=\frac{F(x+a_n)-F(x)}{a_n}-\frac{F(x-a_n)-F(x)}{-a_n} + \frac{C_n}{a_n}$$ Because $a_n$ approaches $0$ we can take the limit as $n$ approaches $\infty$ , and because $F$ is differentiable, we obtain $0=\lim_{n \to \infty} \frac{C_n}{a_n}$ . Therefore $\lim_{n \to \infty}C_n=0$ , but that is not useful. $f(x)=0$ , $f(x)=k, F(x)=kx$ satisfy the equation, but I  cannot seem to deduce them. Did I do a mistake or is the exercise wrong?","Let be a sequence of strictly positive real numbers such that . Find all functions that admit primitives(i.e. there exists a function such that ) and satisfies the following equality My approach to this problem was to first use the fact that there exists a function , such that , so the equation becomes , for some arbitrary constant . Rearranging the terms, we get that Because approaches we can take the limit as approaches , and because is differentiable, we obtain . Therefore , but that is not useful. , satisfy the equation, but I  cannot seem to deduce them. Did I do a mistake or is the exercise wrong?","a_n \lim_{n \to \infty}a_n=0 f: \mathbb{R} \to \mathbb{R} F:\mathbb{R} \to \mathbb{R} \frac{dF(x)}{dx}=f(x), \forall x \in \mathbb{R} 2f(x)=f(x+a_n)+f(x-a_n), \forall x \in \mathbb{R}, \forall n \in \mathbb{N} F:\mathbb{R} \to \mathbb{R} F'(x)=f(x) 2F(x)=F(x+a_n)+F(x-a_n)+C_n C_n 0=\frac{F(x+a_n)-F(x)}{a_n}-\frac{F(x-a_n)-F(x)}{-a_n} + \frac{C_n}{a_n} a_n 0 n \infty F 0=\lim_{n \to \infty} \frac{C_n}{a_n} \lim_{n \to \infty}C_n=0 f(x)=0 f(x)=k, F(x)=kx","['real-analysis', 'calculus', 'limits', 'analysis', 'derivatives']"
98,What is this combinatorics problem involving polynomials? [closed],What is this combinatorics problem involving polynomials? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I am trying to find when a certain descriptor for a polynomial equals zero. Given a set of discrete complex points $B \subset \mathbb{C}$ , a map $p(b):B \to \mathbb{N}$ , and a set $A \subseteq B$ such that $\forall a \in A, p(a) = 1$ . I am asking when the following formula is equal to zero: $$0 =\sum_{a \in A} \prod_{\substack{b \in B, \\ a \neq b}} (a - b)^{p(b)}$$ I am fairly certain of a couple of things. For example, if $B = A$ and $|B| > 1$ , then the equation is zero, if $A = \emptyset$ , it is also obviously zero. The answer cannot be zero if $|B| = 2$ and $|A| = 1$ . I am thinking that the formula depends on the size of $A$ , the size of $B$ and the values of $p(b) \mod 2$ , which makes me believe this is a combinatorics problem. It looks very fun if anyone wants a try, but I am feeling stumped for the moment.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I am trying to find when a certain descriptor for a polynomial equals zero. Given a set of discrete complex points , a map , and a set such that . I am asking when the following formula is equal to zero: I am fairly certain of a couple of things. For example, if and , then the equation is zero, if , it is also obviously zero. The answer cannot be zero if and . I am thinking that the formula depends on the size of , the size of and the values of , which makes me believe this is a combinatorics problem. It looks very fun if anyone wants a try, but I am feeling stumped for the moment.","B \subset \mathbb{C} p(b):B \to \mathbb{N} A \subseteq B \forall a \in A, p(a) = 1 0 =\sum_{a \in A} \prod_{\substack{b \in B, \\ a \neq b}} (a - b)^{p(b)} B = A |B| > 1 A = \emptyset |B| = 2 |A| = 1 A B p(b) \mod 2","['combinatorics', 'analysis']"
99,Proving a basic form of monotone convergence,Proving a basic form of monotone convergence,,"Problem: Definition 4.4 of Probability Theory: A Comprehensive Course by Klenke says that If $f \colon \varOmega \to [0, \infty]$ is measurable, then we define the integral of $f$ with respect to $\mu$ by \begin{equation} \int f \, d\mu := \sup \bigl\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \bigr\} \end{equation} where $\mathbb{E}^+$ is the space of non-negative simple functions and $I \colon \mathbb{E}^+ \to [0, \infty]$ is such that $I(f) = \sum_{i = 1}^m \alpha_i \, \mu(A_i)$ . I know that for $f \in \mathbb{E}^+$ , there is a sequence of non-negative simple functions $(f_n)_{n \in \mathbb N}$ such that $f_n \uparrow f$ . I would like to convince myself that \begin{equation} \lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu \end{equation} so that I can think of $\int f \, d\mu$ as the limit of the integrals of non-negative simple functions that approximate $f$ . My attempted proof that $\lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu$ is shown below. Attempted Proof: I know that $f \leq g$ implies that $I(f) \leq I(g)$ . This gives us a monotone increasing sequence $\left(I(f_n)\right)_{n \in \mathbb N}$ of real numbers. I know that \begin{equation}     \lim_{n \to \infty} I(f_n) = \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} \end{equation} i.e. the sequence converges to its supremum (which may be infinite). It remains to show that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} = \int f \, d\mu$ . By definition, $\int f \, d\mu := \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ . If $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} \neq \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ , then because $\left\{ I(f_n) \colon n \in \mathbb N \right\} \subset \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ , we must have $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\}$ . Suppose this is true. Then there exists $g \in \mathbb{E}^+$ , $g \leq f$ such that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(g)$ . However, because $f_n \uparrow f$ , we know that there exists an $n \in \mathbb N$ such that $g \leq f_n$ . This implies that $I(g) \leq I(f_n)$ , which implies that $\sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(f_n)$ , which is clearly a contradiction.","Problem: Definition 4.4 of Probability Theory: A Comprehensive Course by Klenke says that If is measurable, then we define the integral of with respect to by where is the space of non-negative simple functions and is such that . I know that for , there is a sequence of non-negative simple functions such that . I would like to convince myself that so that I can think of as the limit of the integrals of non-negative simple functions that approximate . My attempted proof that is shown below. Attempted Proof: I know that implies that . This gives us a monotone increasing sequence of real numbers. I know that i.e. the sequence converges to its supremum (which may be infinite). It remains to show that . By definition, . If , then because , we must have . Suppose this is true. Then there exists , such that . However, because , we know that there exists an such that . This implies that , which implies that , which is clearly a contradiction.","f \colon \varOmega \to [0, \infty] f \mu \begin{equation}
\int f \, d\mu := \sup \bigl\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \bigr\}
\end{equation} \mathbb{E}^+ I \colon \mathbb{E}^+ \to [0, \infty] I(f) = \sum_{i = 1}^m \alpha_i \, \mu(A_i) f \in \mathbb{E}^+ (f_n)_{n \in \mathbb N} f_n \uparrow f \begin{equation}
\lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu
\end{equation} \int f \, d\mu f \lim_{n \to \infty} \int f_n \, d\mu = \int f \, d\mu f \leq g I(f) \leq I(g) \left(I(f_n)\right)_{n \in \mathbb N} \begin{equation}
    \lim_{n \to \infty} I(f_n) = \sup \left\{ I(f_n) \colon n \in \mathbb N \right\}
\end{equation} \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} = \int f \, d\mu \int f \, d\mu := \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\} \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} \neq \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\} \left\{ I(f_n) \colon n \in \mathbb N \right\} \subset \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\} \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < \sup \left\{ I(g) \colon g \in \mathbb{E}^+, g \leq f \right\} g \in \mathbb{E}^+ g \leq f \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(g) f_n \uparrow f n \in \mathbb N g \leq f_n I(g) \leq I(f_n) \sup \left\{ I(f_n) \colon n \in \mathbb N \right\} < I(f_n)","['real-analysis', 'analysis', 'measure-theory', 'solution-verification', 'lebesgue-integral']"
