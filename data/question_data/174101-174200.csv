,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Cheese, mouse and a cat","Cheese, mouse and a cat",,I have a problem and I don't know how to solve it because I don't know where to start.  If we have the following situation: Room 1-Room 2-Room 3-Room 4-Room 5 There is a little mouse in room 4 and he always forgets in which room he has been when going to the next room. In room 5 there is a big hungry cat waiting for him and in room 1 there is cheese. What is the chance he will get the cheese and not being eaten by the cat? My error solution: If I go like $\displaystyle \left(\frac{1}{2}\right)^{3} + \left(\frac{1}{2}\right)^{4} +$ (endless possibilities). I know that this is not the way I should calculate it. This is I think a geometric distribution because it is memoryless. On the other hand I can use binomial distribution to calculate it but there are endless possibilities.. I just need a push in the right direction. Thanks in advance.,I have a problem and I don't know how to solve it because I don't know where to start.  If we have the following situation: Room 1-Room 2-Room 3-Room 4-Room 5 There is a little mouse in room 4 and he always forgets in which room he has been when going to the next room. In room 5 there is a big hungry cat waiting for him and in room 1 there is cheese. What is the chance he will get the cheese and not being eaten by the cat? My error solution: If I go like $\displaystyle \left(\frac{1}{2}\right)^{3} + \left(\frac{1}{2}\right)^{4} +$ (endless possibilities). I know that this is not the way I should calculate it. This is I think a geometric distribution because it is memoryless. On the other hand I can use binomial distribution to calculate it but there are endless possibilities.. I just need a push in the right direction. Thanks in advance.,,"['probability', 'statistics']"
1,Probability of repeating at least 2 digits,Probability of repeating at least 2 digits,,"Pin codes consist of 4 digits between 0 and 9. If a pin-code were generated by a random number generator (e.g. by 4 ten-sided dice), what is the probability that it will have at least two digits that repeat?","Pin codes consist of 4 digits between 0 and 9. If a pin-code were generated by a random number generator (e.g. by 4 ten-sided dice), what is the probability that it will have at least two digits that repeat?",,"['probability', 'statistics']"
2,Probability problem found in textbook,Probability problem found in textbook,,"""Shown is the board for a simple dice game. You roll a die and move the same number of squares (for example if your first roll is a $3$, move to the $3$ square). If you land on an arrow's tail, you must move to the square where that arrow's head is. You win if you land on the ""6"" square or beyond (for example rolling a $5$ when you are on the $4$ square will win). What is the probability of winning this game in EXACTLY $2$ moves?"" I'm having trouble understanding the wording and how to calculate the probability for this, could anyone help? Reading the sample solution didn't give me more hints on what exactly the rules for the game are.","""Shown is the board for a simple dice game. You roll a die and move the same number of squares (for example if your first roll is a $3$, move to the $3$ square). If you land on an arrow's tail, you must move to the square where that arrow's head is. You win if you land on the ""6"" square or beyond (for example rolling a $5$ when you are on the $4$ square will win). What is the probability of winning this game in EXACTLY $2$ moves?"" I'm having trouble understanding the wording and how to calculate the probability for this, could anyone help? Reading the sample solution didn't give me more hints on what exactly the rules for the game are.",,"['probability', 'statistics']"
3,Stem-Leaf Display,Stem-Leaf Display,,"I am reading an example, one that is concerned with the topic mentioned in the title of this thread. The example problem is: ""The use of alcohol by college students is of great concern not only to those in the academic community but also, because of potential health and safety consequences, to society at large. The article “Health and Behavioral Consequences of Binge Drinking in College” (J. of the Amer. Med. Assoc.,1994: 1672–1677) reported on a comprehensive study of heavy drinking on campuses across the United States. A binge episode was defined as five or more drinks in a row for males and four or more for females. Figure 1.4 shows a stem-and-leaf display of 140 values of $x=~the~percentage~of~binge~drinkers$ of undergraduate students who are binge drinkers. (These values were not given in the cited article, but our display agrees with a picture of the data that did appear.)"" So, from my understanding, the first row corresponds to the percentage value $04\%$; the second row corresponds to $11.34567889\%$, and so on. What confuses me is a few statements they make later on: Suppose the observations had been listed in alphabetical order by school name, as $16\%$ $33\%$ $64\%$ $37\%$ $31\%$... Then placing these values on the display in this order would result in the stem 1 row having 6 as its first leaf, and the beginning of the stem 3 row would be 3 | 371..."" Where are they getting these extra digits? And how does 3 | 371 correspond to the third entry of this list? If I was to write the stem-leaf display of these few pieces of data, I would write: 1 | 6 3 | 3 6 | 4 3 | 7 3 | 1 Is the book wrong?","I am reading an example, one that is concerned with the topic mentioned in the title of this thread. The example problem is: ""The use of alcohol by college students is of great concern not only to those in the academic community but also, because of potential health and safety consequences, to society at large. The article “Health and Behavioral Consequences of Binge Drinking in College” (J. of the Amer. Med. Assoc.,1994: 1672–1677) reported on a comprehensive study of heavy drinking on campuses across the United States. A binge episode was defined as five or more drinks in a row for males and four or more for females. Figure 1.4 shows a stem-and-leaf display of 140 values of $x=~the~percentage~of~binge~drinkers$ of undergraduate students who are binge drinkers. (These values were not given in the cited article, but our display agrees with a picture of the data that did appear.)"" So, from my understanding, the first row corresponds to the percentage value $04\%$; the second row corresponds to $11.34567889\%$, and so on. What confuses me is a few statements they make later on: Suppose the observations had been listed in alphabetical order by school name, as $16\%$ $33\%$ $64\%$ $37\%$ $31\%$... Then placing these values on the display in this order would result in the stem 1 row having 6 as its first leaf, and the beginning of the stem 3 row would be 3 | 371..."" Where are they getting these extra digits? And how does 3 | 371 correspond to the third entry of this list? If I was to write the stem-leaf display of these few pieces of data, I would write: 1 | 6 3 | 3 6 | 4 3 | 7 3 | 1 Is the book wrong?",,['statistics']
4,Mathematics or statistics?,Mathematics or statistics?,,Can you please tell me what is ((stochastic modelling and statistical analysis of spatio-temporal data)) related to? I mean Mathematics or statistics? Is it good subject for student who interested in pure mathematics ?,Can you please tell me what is ((stochastic modelling and statistical analysis of spatio-temporal data)) related to? I mean Mathematics or statistics? Is it good subject for student who interested in pure mathematics ?,,"['reference-request', 'statistics', 'education']"
5,Statistic Missing Value [closed],Statistic Missing Value [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question A professor has recorded exam grades for $30$ students in his class, $1$ of the $30$ grades is unreadable. The mean score on the exam was $82$, and the mean score of the $29$ available scores is $84$, What is the value of the unreadable score?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question A professor has recorded exam grades for $30$ students in his class, $1$ of the $30$ grades is unreadable. The mean score on the exam was $82$, and the mean score of the $29$ available scores is $84$, What is the value of the unreadable score?",,"['statistics', 'average']"
6,Probability mass function and conditional probabilities,Probability mass function and conditional probabilities,,"If $p_X(k)$ and $p_{Y|X}(y|k)$ are given, how can we calculate $p_Y(y)$? We cannot assume that $p_Y(y)$ and $p_X(k)$ are independent. I know that $p_X(k) \cdot p_{Y|X}(y|k) = p_{Y,X}(y,k)$ but how can I isolate $p_Y(y)$?","If $p_X(k)$ and $p_{Y|X}(y|k)$ are given, how can we calculate $p_Y(y)$? We cannot assume that $p_Y(y)$ and $p_X(k)$ are independent. I know that $p_X(k) \cdot p_{Y|X}(y|k) = p_{Y,X}(y,k)$ but how can I isolate $p_Y(y)$?",,"['probability', 'statistics']"
7,What is the rationale behind the evaluation of the Expectation operator?,What is the rationale behind the evaluation of the Expectation operator?,,"We know that the expectation operator is defined for a random variable $x$, as such: $$ \mathbb{E} \left\{x\right\} = \int_{-\infty}^{\infty} x \: p_x(x) \; \mathrm{d}x $$ Where $p_x{x}$ is the PDF of the random variable $x$. If there is an arbitrary(?) function $f$ acting on the random variable $x$, then the expected value of this function can also be written as: $$ \mathbb{E}\left\{f(x) \right\} = \int_{-\infty}^{\infty} f(x) \: p_x(x) \: \mathrm{d}x $$ My questions are: On many algorithms that I study, (statistical in nature), one often finds themselves taking the expected value of some entity, that is a function of the random variable $x$. In the reverse case, one can also find themselves poking around and manipulating the probability distribution function of $x$, and then we can 'take it back' into an expression using the expectation operator. Upon evaluating the expected value of $x$ however, ($\mathbb{E[x]})$, I often come across this estimation formula: $$ \mathbb{E}\left\{x\right\} \approx \frac{1}{N}\sum_{n=1}^{N} x[n] $$ and similarly, $$ \mathbb{E}\left\{f(x)\right\} \approx \frac{1}{N}\sum_{n=1}^{N} f(x[n]) $$ Where each $x[n]$ is an individual realization of the random variable $x$. My question is, why is this formula true, and how did it come about? Every book I read seems to just include it as if it fell from the sky one day and no explanation is given as to why it is true. Could someone please give an intuitive and mathematical explanation for why - and more importantly, how this happens to be true? What is the history/rationale behind it? Many thanks.","We know that the expectation operator is defined for a random variable $x$, as such: $$ \mathbb{E} \left\{x\right\} = \int_{-\infty}^{\infty} x \: p_x(x) \; \mathrm{d}x $$ Where $p_x{x}$ is the PDF of the random variable $x$. If there is an arbitrary(?) function $f$ acting on the random variable $x$, then the expected value of this function can also be written as: $$ \mathbb{E}\left\{f(x) \right\} = \int_{-\infty}^{\infty} f(x) \: p_x(x) \: \mathrm{d}x $$ My questions are: On many algorithms that I study, (statistical in nature), one often finds themselves taking the expected value of some entity, that is a function of the random variable $x$. In the reverse case, one can also find themselves poking around and manipulating the probability distribution function of $x$, and then we can 'take it back' into an expression using the expectation operator. Upon evaluating the expected value of $x$ however, ($\mathbb{E[x]})$, I often come across this estimation formula: $$ \mathbb{E}\left\{x\right\} \approx \frac{1}{N}\sum_{n=1}^{N} x[n] $$ and similarly, $$ \mathbb{E}\left\{f(x)\right\} \approx \frac{1}{N}\sum_{n=1}^{N} f(x[n]) $$ Where each $x[n]$ is an individual realization of the random variable $x$. My question is, why is this formula true, and how did it come about? Every book I read seems to just include it as if it fell from the sky one day and no explanation is given as to why it is true. Could someone please give an intuitive and mathematical explanation for why - and more importantly, how this happens to be true? What is the history/rationale behind it? Many thanks.",,"['probability', 'statistics']"
8,Transitivity of uncorrelated random variables?,Transitivity of uncorrelated random variables?,,"Suppose $cov(X,Y)=0\;$ and $\;cov(Y,M)=0$. Does this imply $cov(X,M)=0\;$, if all distinct RV are normal?","Suppose $cov(X,Y)=0\;$ and $\;cov(Y,M)=0$. Does this imply $cov(X,M)=0\;$, if all distinct RV are normal?",,['statistics']
9,"Density function $Y= \max(X_1, X_2)$",Density function,"Y= \max(X_1, X_2)","If $X_1$ and $X_2$ are independent random variables each of which has density function of the form: $$f(x)= \Bigg\{ \begin{array}{cc} 2x;&0<x<1\\ 0; & \text{otherwise} \end{array} $$ Let $Y = \max\{X_1, X_2\}$; show the density function of $Y$ is $4y^3$.","If $X_1$ and $X_2$ are independent random variables each of which has density function of the form: $$f(x)= \Bigg\{ \begin{array}{cc} 2x;&0<x<1\\ 0; & \text{otherwise} \end{array} $$ Let $Y = \max\{X_1, X_2\}$; show the density function of $Y$ is $4y^3$.",,['statistics']
10,"Given a victory condition and a set strategy, what are the chances of winning on a given turn in a game of Magic: The Gathering?","Given a victory condition and a set strategy, what are the chances of winning on a given turn in a game of Magic: The Gathering?",,"Tl;DR :  You have winning cards. To win, you must be able to play those cards, and have them in your hand. Your hand is randomly drawn. When might you win? How could find the answer to this (very complex) problem? (I realize that my question isn't very formal, but I'm not exactly sure how to correctly pose the question. Feel free to edit. Also, draw3cards or Boards & Games don't like math questions) The decklist (If you know  magic, you should skip the next section, and probably the one after that, too) Basic Rules You have a 60-card deck, which has just been shuffled. At the beginning of the game, you draw 7 cards. From then on, each turn you draw one card. All cards except lands require mana to play. Mana is drawn from your mana pool. A land card on the field can be tapped (used) to add a mana of a color to your mana pool. (Mana either has a color, or is uncolored. Islands for blue mana, Swamps for black). Colored mana can always be turned into uncolored mana. Each turn, you may put no more than one land from your hand onto the field. It can immediately be tapped for mana. Combo explanation You have your Myr. As the decklist shows, you have Silver and Leaden myr, which can be tapped for Blue or Black mana, respectively. They cost two mana each, of any color. In the same vein, Alloy Myr grants one mana of any color, and Palladium Myr grants two uncolored mana You also have the Myr Galvanizer. For one mana and being tapped, it untaps all other Myr. If you have one Galvanizer, and your mana-myr can be tapped for more than one, you can get an extra mana If you have two Galvanizers, and your mana-myr can be tapped for more than one, you can have infinite mana If you have infinite mana, you can win with Exsanguinate (which requires two black mana - so if you have two Galvanizers and a Palladium Myr, but only one swamp, you can't win with Exsanguinate) ...or you could win with Blue Sun's Zenith, which requires 3 blue mana. It can also be useful without infinite mana, since it allows you to draw cards (one card for every mana of any color you pay beyond 3 blue) You also have three shapeshifters - Cackling Image (one mana of any color, two blue mana), Cryptoplasm (same), and Evil Twin (two uncolored, one black, one blue). All of these can make a copy of a creature on the field You also have Diabolic Tutor, which gives you any card you want for two black mana, two uncolored mana The other cards are irrelevant. Question Given that you try to get mana-myr on the field first, what chances are there of winning on what turn? I realize that this is a very tricky question that probably involves quite a bit of work, so an explication of how to get an answer is almost as good an answer itself. Also really useful would be a generalization of this. For extra points, ideas about finding the optimal number of each card (keeping the irrelevant defensive cards, and no more than 70 cards) would be very appreciated.","Tl;DR :  You have winning cards. To win, you must be able to play those cards, and have them in your hand. Your hand is randomly drawn. When might you win? How could find the answer to this (very complex) problem? (I realize that my question isn't very formal, but I'm not exactly sure how to correctly pose the question. Feel free to edit. Also, draw3cards or Boards & Games don't like math questions) The decklist (If you know  magic, you should skip the next section, and probably the one after that, too) Basic Rules You have a 60-card deck, which has just been shuffled. At the beginning of the game, you draw 7 cards. From then on, each turn you draw one card. All cards except lands require mana to play. Mana is drawn from your mana pool. A land card on the field can be tapped (used) to add a mana of a color to your mana pool. (Mana either has a color, or is uncolored. Islands for blue mana, Swamps for black). Colored mana can always be turned into uncolored mana. Each turn, you may put no more than one land from your hand onto the field. It can immediately be tapped for mana. Combo explanation You have your Myr. As the decklist shows, you have Silver and Leaden myr, which can be tapped for Blue or Black mana, respectively. They cost two mana each, of any color. In the same vein, Alloy Myr grants one mana of any color, and Palladium Myr grants two uncolored mana You also have the Myr Galvanizer. For one mana and being tapped, it untaps all other Myr. If you have one Galvanizer, and your mana-myr can be tapped for more than one, you can get an extra mana If you have two Galvanizers, and your mana-myr can be tapped for more than one, you can have infinite mana If you have infinite mana, you can win with Exsanguinate (which requires two black mana - so if you have two Galvanizers and a Palladium Myr, but only one swamp, you can't win with Exsanguinate) ...or you could win with Blue Sun's Zenith, which requires 3 blue mana. It can also be useful without infinite mana, since it allows you to draw cards (one card for every mana of any color you pay beyond 3 blue) You also have three shapeshifters - Cackling Image (one mana of any color, two blue mana), Cryptoplasm (same), and Evil Twin (two uncolored, one black, one blue). All of these can make a copy of a creature on the field You also have Diabolic Tutor, which gives you any card you want for two black mana, two uncolored mana The other cards are irrelevant. Question Given that you try to get mana-myr on the field first, what chances are there of winning on what turn? I realize that this is a very tricky question that probably involves quite a bit of work, so an explication of how to get an answer is almost as good an answer itself. Also really useful would be a generalization of this. For extra points, ideas about finding the optimal number of each card (keeping the irrelevant defensive cards, and no more than 70 cards) would be very appreciated.",,"['probability', 'statistics', 'probability-distributions', 'recreational-mathematics', 'game-theory']"
11,"Why Binomial Distribution formula includes the ""not-happening"" probability?","Why Binomial Distribution formula includes the ""not-happening"" probability?",,"Suppose I have a dice with 6 sides, and I let a random variable $X$ be the number of times I get 3 points when I throw the dice. So I throw the dice for $10$ times, I want to find the probability of getting 3 points from the dice for $4$ times, ie: $P(X=4)$. Since the order doesn't matter, there are $ \binom{10}{4}=210 $ ways and the chance of getting a 3 point is $\frac { 1 }{ 6 } $. Also, because I want to have $4$ of such occurrence, it would be $\frac{1}{6}^4$. So, I could just calculate $P(X=4)=\binom{10}{4}\frac { 1 }{ 6 }^4 =0.027006173\approx 2.7\%$. But, suppose if I use the Binomial Distribution formula, it would be a little different because it needs to multiply the ""not-happening"" probability to it. The Binomial Distribution looks like this: $$P(X=x)=\binom{n}{x}p^x(1-p)^{n-x}$$ So if I plug in my values, it would be: $$ P(X=4)=\binom{10}{4}(\frac{1}{6})^4(\frac{5}{6})^{6}=0.054=5.4\% $$ Here, $2.1\%$ is lesser than $5.4\%$. What's the difference between the two values? Which is the correct value? Intuitively, I find the Binomial Distribution may be more accurate since it dictates the situation to consider both the happening and not-happening outcomes. But usually, I thought we just multiply the probabilities of what we want it to happen as long as the events are independent. So the first method sounds quite okay too. Eg: What's the probability to get 2 heads out of 5 flips of a fair coin, I just use $\frac { 1}{ 2} \times \frac { 1}{ 2} $. The not-happening probabilities are not cared of.","Suppose I have a dice with 6 sides, and I let a random variable $X$ be the number of times I get 3 points when I throw the dice. So I throw the dice for $10$ times, I want to find the probability of getting 3 points from the dice for $4$ times, ie: $P(X=4)$. Since the order doesn't matter, there are $ \binom{10}{4}=210 $ ways and the chance of getting a 3 point is $\frac { 1 }{ 6 } $. Also, because I want to have $4$ of such occurrence, it would be $\frac{1}{6}^4$. So, I could just calculate $P(X=4)=\binom{10}{4}\frac { 1 }{ 6 }^4 =0.027006173\approx 2.7\%$. But, suppose if I use the Binomial Distribution formula, it would be a little different because it needs to multiply the ""not-happening"" probability to it. The Binomial Distribution looks like this: $$P(X=x)=\binom{n}{x}p^x(1-p)^{n-x}$$ So if I plug in my values, it would be: $$ P(X=4)=\binom{10}{4}(\frac{1}{6})^4(\frac{5}{6})^{6}=0.054=5.4\% $$ Here, $2.1\%$ is lesser than $5.4\%$. What's the difference between the two values? Which is the correct value? Intuitively, I find the Binomial Distribution may be more accurate since it dictates the situation to consider both the happening and not-happening outcomes. But usually, I thought we just multiply the probabilities of what we want it to happen as long as the events are independent. So the first method sounds quite okay too. Eg: What's the probability to get 2 heads out of 5 flips of a fair coin, I just use $\frac { 1}{ 2} \times \frac { 1}{ 2} $. The not-happening probabilities are not cared of.",,"['probability', 'statistics', 'probability-distributions']"
12,Heads or tails probability,Heads or tails probability,,"I'm working on a maths exercise and came across this question. The probability of a ""heads"" when throwing a coin twice is 2 / 3. This could be explained by the following: • The first time is ""heads"". The second throw is unnecessary. The result is H; • The first time is ""tails"" and twice ""heads"". The result is TH; • The first time is ""tails"" and twice ""tails"". The result is TT; The outcome: {H, TH, TT}. two of the three results include a ""heads"", it follows that the probability of a ""heads"" is 2/3 What's wrong with this reasoning? I think the answer is 1/2, is that right? Ps. my first language isn't english, Thanks Jef","I'm working on a maths exercise and came across this question. The probability of a ""heads"" when throwing a coin twice is 2 / 3. This could be explained by the following: • The first time is ""heads"". The second throw is unnecessary. The result is H; • The first time is ""tails"" and twice ""heads"". The result is TH; • The first time is ""tails"" and twice ""tails"". The result is TT; The outcome: {H, TH, TT}. two of the three results include a ""heads"", it follows that the probability of a ""heads"" is 2/3 What's wrong with this reasoning? I think the answer is 1/2, is that right? Ps. my first language isn't english, Thanks Jef",,"['probability', 'statistics']"
13,why is this $\bar X$ a.s. irrational?,why is this  a.s. irrational?,\bar X,"Suppose that $X_1, X_2,\cdots, X_n$ are iid random variables from $N(\theta,1)$, $\theta$ is rational. Then we know that $\bar X \sim N(\theta,1/n)$. It is said that $\bar X$ is almost surely irrational. I am wondering why is it irrational a.s.? How can I interpret it?","Suppose that $X_1, X_2,\cdots, X_n$ are iid random variables from $N(\theta,1)$, $\theta$ is rational. Then we know that $\bar X \sim N(\theta,1/n)$. It is said that $\bar X$ is almost surely irrational. I am wondering why is it irrational a.s.? How can I interpret it?",,"['statistics', 'probability-distributions']"
14,Probability with a collection problem,Probability with a collection problem,,"A collection of tickets comes in four colors: red, blue, white, and green. There are twice as many reds as blues, equal numbers of blues and whites, and three times as many green as whites. I choose 5 tickets at random with replacement. Let $X$ be the number of different colors that appear. a) Find a numerical expression for $P(X\ge 4)$. b) Find a numercial expression for $E(X)$. My understanding of this problem is that $P(X\ge 4)$ is the same as $P(X=4)$ since there are only four colors to choose so that X cannot be 5. Then it becomes a hypergeometric distribution. But the answer on the solution manual is pretty scary which is  $$\dfrac{5!}{1!1!1!2!}[2(1/7)^3(2/7)(3/7)+(1/7)^2(2/7)^2(3/7)+(1/7)^2(2/7)(3/7)^2] $$ and $E(X)$ is  $$ 2[1-(6/7)^5]+1-(5/7)^5 + 1- (4/7)^5$$ Could someone explain a little bit for me? I am so confused.","A collection of tickets comes in four colors: red, blue, white, and green. There are twice as many reds as blues, equal numbers of blues and whites, and three times as many green as whites. I choose 5 tickets at random with replacement. Let $X$ be the number of different colors that appear. a) Find a numerical expression for $P(X\ge 4)$. b) Find a numercial expression for $E(X)$. My understanding of this problem is that $P(X\ge 4)$ is the same as $P(X=4)$ since there are only four colors to choose so that X cannot be 5. Then it becomes a hypergeometric distribution. But the answer on the solution manual is pretty scary which is  $$\dfrac{5!}{1!1!1!2!}[2(1/7)^3(2/7)(3/7)+(1/7)^2(2/7)^2(3/7)+(1/7)^2(2/7)(3/7)^2] $$ and $E(X)$ is  $$ 2[1-(6/7)^5]+1-(5/7)^5 + 1- (4/7)^5$$ Could someone explain a little bit for me? I am so confused.",,"['probability', 'statistics']"
15,Probability of receiving packets,Probability of receiving packets,,"Suppose we send $n$ packets over the Internet and assume that all $n$ packets are routed along the same path, and with probability $p$, one of the links along the path fails and all $n$ packets are lost. Otherwise all packets are received.. What kind of distributions is this and how to compute the distribution and expectation of this model. I have something like: Let $X$ be the random variable of the number of packets loss. And $X$ can only be 0 or $n$. So $P(X=0) = (1-p)^n$ and $P(X=n) = p$. But I don't recognize this distribution and don't really know how to compute the expectation of this if this is correct distribution. Any help? Please","Suppose we send $n$ packets over the Internet and assume that all $n$ packets are routed along the same path, and with probability $p$, one of the links along the path fails and all $n$ packets are lost. Otherwise all packets are received.. What kind of distributions is this and how to compute the distribution and expectation of this model. I have something like: Let $X$ be the random variable of the number of packets loss. And $X$ can only be 0 or $n$. So $P(X=0) = (1-p)^n$ and $P(X=n) = p$. But I don't recognize this distribution and don't really know how to compute the expectation of this if this is correct distribution. Any help? Please",,"['probability', 'statistics']"
16,Scaling a histogram in the face of outliers,Scaling a histogram in the face of outliers,,"I am trying to figure out how to display a histogram of a digital image in the face of massive outliers (lots of shadows, highlights, or lots of anything inbetween).  If I simply choose the bin with the most entries to be the '100% height' of my fixed display area the rest of the bins are dwarfed and you can't really get any useful information by looking at it. I attempted to find the standard deviation between the bins then only include bins within a certain number of std devs from the average when picking the '100% height' bin, but it didn't turn out too well in the general case... certain number of std devs worked well for some images and not others. A good example of what I want is Photoshop's histogram, but I'm not sure how they accomplish this and I've come up short on the google.  Does anyone have any advice?","I am trying to figure out how to display a histogram of a digital image in the face of massive outliers (lots of shadows, highlights, or lots of anything inbetween).  If I simply choose the bin with the most entries to be the '100% height' of my fixed display area the rest of the bins are dwarfed and you can't really get any useful information by looking at it. I attempted to find the standard deviation between the bins then only include bins within a certain number of std devs from the average when picking the '100% height' bin, but it didn't turn out too well in the general case... certain number of std devs worked well for some images and not others. A good example of what I want is Photoshop's histogram, but I'm not sure how they accomplish this and I've come up short on the google.  Does anyone have any advice?",,"['statistics', 'image-processing']"
17,expectation of incomplete gamma,expectation of incomplete gamma,,"Is the expectation of the (upper/lower) incomplete gamma function known? $$\int_0^{+\infty} x \Gamma(A, x) \mathrm dx$$","Is the expectation of the (upper/lower) incomplete gamma function known? $$\int_0^{+\infty} x \Gamma(A, x) \mathrm dx$$",,"['calculus', 'probability', 'statistics', 'integration', 'special-functions']"
18,Geometric Distribution #2,Geometric Distribution #2,,"a TV show tests a number of people who claim to be ""psychic"".  The test involves blindly predicting each outcomes of 5 rolls of a fair die.  The TV show will declare such a person as psychic if they correctly predict at least 3 of the die rolls. Find the probability that at least one of the first 100 people tested will be declared to be psychic by the TV show.","a TV show tests a number of people who claim to be ""psychic"".  The test involves blindly predicting each outcomes of 5 rolls of a fair die.  The TV show will declare such a person as psychic if they correctly predict at least 3 of the die rolls. Find the probability that at least one of the first 100 people tested will be declared to be psychic by the TV show.",,['statistics']
19,Simple stats question-correlation coefficient,Simple stats question-correlation coefficient,,"Let's say we have two exams, each out of 50 points. The correlation rate between them is 0.75. If the teacher decides to add 10 points to the results of the first test, what will happen to the corr. rate? The way I see it, the correlation should decrease, but by how much? Would it decrease by 1/5=20%? And the result would have been the same even if she subtracted 10 points from the first test, correct?","Let's say we have two exams, each out of 50 points. The correlation rate between them is 0.75. If the teacher decides to add 10 points to the results of the first test, what will happen to the corr. rate? The way I see it, the correlation should decrease, but by how much? Would it decrease by 1/5=20%? And the result would have been the same even if she subtracted 10 points from the first test, correct?",,[]
20,Distribution of $\frac{X_1 X_2}{\sum_{i=1}^n X_i^2}$,Distribution of,\frac{X_1 X_2}{\sum_{i=1}^n X_i^2},"I currently have a problem in deriving the distribution (or moments) of the random variable \begin{align*} T = \frac{X_1X_2}{\sum_{i=1}^n X_i^2},\\ \text{where } X_i \sim N(0, 1). \end{align*} I attempted to utilize the fact that $\sum_{i=1}^n X_i^2 \sim \chi_{(n)}.$ However, because the numerator and the denominator is not independent, I have encountered difficulty in proceeding. As far as I am aware, I have not found any known distribution (or moments) for the above random variable $T$ .","I currently have a problem in deriving the distribution (or moments) of the random variable I attempted to utilize the fact that However, because the numerator and the denominator is not independent, I have encountered difficulty in proceeding. As far as I am aware, I have not found any known distribution (or moments) for the above random variable .","\begin{align*}
T = \frac{X_1X_2}{\sum_{i=1}^n X_i^2},\\
\text{where } X_i \sim N(0, 1).
\end{align*} \sum_{i=1}^n X_i^2 \sim \chi_{(n)}. T","['statistics', 'probability-distributions']"
21,unbiased estimator of sample mean,unbiased estimator of sample mean,,"The question: Given a random sample $X_1,...,X_n$ show that $\frac{1}{n}\sum_{i=1}^n X_i$ is an unbiased estimator for $E(X_1)$ . My confusion: Given a statistical model $(\Omega,\Sigma,p_{\theta})$ , where $p_{\theta}$ is a parameterized collection of probability measures, We define $E_{\theta}(X)=\int_{\Omega}X(\omega)p_{\theta}(d\omega)$ , where $X:\Omega\to\mathbb{R}$ . If $\frac{1}{n}\sum_{i=1}^nX_i$ is unbiased, we need $E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=\theta$ , when $\theta=E(X_1)$ . But what is $E(X_1)$ ? With respect to what measure are we integrating, if we're considering all these different measures on the space? For any $\theta$ , $E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=E_{\theta}(X_1)$ , because each $X_i$ is identically distributed. My guess is, you would say something like, ""for a fixed probability measure $p_{\varphi}$ on $(\Omega,\Sigma)$ , $\frac{1}{n}\sum_{i=1}^nX_i$ is an unbiased estimator for $E_{\varphi}(X_1)$ ."" Is this right?","The question: Given a random sample show that is an unbiased estimator for . My confusion: Given a statistical model , where is a parameterized collection of probability measures, We define , where . If is unbiased, we need , when . But what is ? With respect to what measure are we integrating, if we're considering all these different measures on the space? For any , , because each is identically distributed. My guess is, you would say something like, ""for a fixed probability measure on , is an unbiased estimator for ."" Is this right?","X_1,...,X_n \frac{1}{n}\sum_{i=1}^n X_i E(X_1) (\Omega,\Sigma,p_{\theta}) p_{\theta} E_{\theta}(X)=\int_{\Omega}X(\omega)p_{\theta}(d\omega) X:\Omega\to\mathbb{R} \frac{1}{n}\sum_{i=1}^nX_i E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=\theta \theta=E(X_1) E(X_1) \theta E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=E_{\theta}(X_1) X_i p_{\varphi} (\Omega,\Sigma) \frac{1}{n}\sum_{i=1}^nX_i E_{\varphi}(X_1)","['statistics', 'definition']"
22,What is statistically more accurate - the average of a dataset of calculated concentrations or the sum of the total mass divided by the total volume? [closed],What is statistically more accurate - the average of a dataset of calculated concentrations or the sum of the total mass divided by the total volume? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question Hopefully an easy question to answer. But if I have a series of five water samples, each sample is a different volume with a different number of plastic particles contained within. I can calculate the concentration of each sample by dividing the mass of particles by the volume of the sample (e.g. 500 mg of particles and 1 litre of water = 500 mg/L). If I want to understand the true average of the dataset, would it be more appropriate to take the average of the concentrations of all the samples OR take the total mass of all the samples combined and divide that by the total volume to get an average concentration? See below for example: Total Sample volume (L) Sample mass (mg) Sample concentration (mg/L) 4 253 63.3 6 439 73.2 5 205 41.0 9 226 25.1 4 30 7.5 Total 28 1153 Would the correct average in this case be 42.0 mg/L (the average of the calculated concentrations) OR the calculated average that divides the total sum of the sample mass (i.e. 1153 mg) by the total sample volume (28 L), i.e. 41.2 mg/L? I know in this case the discrepancy between the values is small, however if you expand the dataset this discrepancy will increase. I feel the answer to this question is a simple mathematical response, but I just can't seem to come to it myself. I appreciate the help!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question Hopefully an easy question to answer. But if I have a series of five water samples, each sample is a different volume with a different number of plastic particles contained within. I can calculate the concentration of each sample by dividing the mass of particles by the volume of the sample (e.g. 500 mg of particles and 1 litre of water = 500 mg/L). If I want to understand the true average of the dataset, would it be more appropriate to take the average of the concentrations of all the samples OR take the total mass of all the samples combined and divide that by the total volume to get an average concentration? See below for example: Total Sample volume (L) Sample mass (mg) Sample concentration (mg/L) 4 253 63.3 6 439 73.2 5 205 41.0 9 226 25.1 4 30 7.5 Total 28 1153 Would the correct average in this case be 42.0 mg/L (the average of the calculated concentrations) OR the calculated average that divides the total sum of the sample mass (i.e. 1153 mg) by the total sample volume (28 L), i.e. 41.2 mg/L? I know in this case the discrepancy between the values is small, however if you expand the dataset this discrepancy will increase. I feel the answer to this question is a simple mathematical response, but I just can't seem to come to it myself. I appreciate the help!",,"['algebra-precalculus', 'statistics', 'arithmetic', 'average', 'means']"
23,"Let $\bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$ where $n=1,2,\ldots$. Then find $\lim_{{n\to\infty}} P(\bar X_n=2)$",Let  where . Then find,"\bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i n=1,2,\ldots \lim_{{n\to\infty}} P(\bar X_n=2)","Let $\{X_n\}_{n\geq1}$ be a sequence of iid random variables having a common density function $$f(x) = \begin{cases}      xe^{-x} & \text{if } x\geq0 \\     0 & \text{otherwise} \end{cases}$$ Let $\bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$ where $n=1,2,\ldots$ . Then find $\lim_{{n\to\infty}} P(\bar X_n=2)$ . My try My first thought as well but I think the problem is deeper than that. I think that as the n tends towards infinity the probability of the the sample mean converging to the population mean is 1. Looking at proving this. By the Central Limit Theorem the sample mean distribution can be approximated by a Normal distribution with $$\mu = 2,~\sigma = \sqrt{\dfrac{2}{n}}$$ As $n\to \infty$ this becomes a delta function centered at $2$",Let be a sequence of iid random variables having a common density function Let where . Then find . My try My first thought as well but I think the problem is deeper than that. I think that as the n tends towards infinity the probability of the the sample mean converging to the population mean is 1. Looking at proving this. By the Central Limit Theorem the sample mean distribution can be approximated by a Normal distribution with As this becomes a delta function centered at,"\{X_n\}_{n\geq1} f(x) = \begin{cases} 
    xe^{-x} & \text{if } x\geq0 \\
    0 & \text{otherwise}
\end{cases} \bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i n=1,2,\ldots \lim_{{n\to\infty}} P(\bar X_n=2) \mu = 2,~\sigma = \sqrt{\dfrac{2}{n}} n\to \infty 2","['probability', 'statistics', 'solution-verification']"
24,Biased estimator of number of edges in a graph,Biased estimator of number of edges in a graph,,"I meet this problem when doing my scientific research. Let $G$ be a simple graph with $n$ vertices, i.e. $G$ is undirected and does not have any loops or multiple edges. We randomly sample some vertices using the following rule. Each vertex is sampled with the same probability $\alpha$ independently. Let $G_1$ be the subgraph induced by the sampled vertices, i.e. an edge of $G$ is also in $G_1$ if and only if both endpoints of this edge are sampled. Suppose we observes that $G_1$ has $n_1$ vertices and $m_1$ edges. Our goal to estimate the number of edges $\hat{m}$ in $G$ using $n,n_1,m_1$ but without $\alpha$ . Since $$ \hat{m}=\left(\frac{1}{\alpha}\right)^2 m_1 $$ is an unbiased estimator and $$ \alpha=\mathbb{E}\left[\frac{n_1}{n}\right], $$ we replace $\alpha$ in this estimator with $\frac{n_1}{n}$ and get the following estimator $$\hat{m}=\left(\frac{n}{n_1}\right)^2 m_1.$$ However, this is a biased estimator, while the correct unbiased estimator is $$\hat{m}=\frac{n(n-1)}{n_1(n_1-1)} m_1.$$ This seems strange. Can you give me an explanation? What is the difference between these two estimators?","I meet this problem when doing my scientific research. Let be a simple graph with vertices, i.e. is undirected and does not have any loops or multiple edges. We randomly sample some vertices using the following rule. Each vertex is sampled with the same probability independently. Let be the subgraph induced by the sampled vertices, i.e. an edge of is also in if and only if both endpoints of this edge are sampled. Suppose we observes that has vertices and edges. Our goal to estimate the number of edges in using but without . Since is an unbiased estimator and we replace in this estimator with and get the following estimator However, this is a biased estimator, while the correct unbiased estimator is This seems strange. Can you give me an explanation? What is the difference between these two estimators?","G n G \alpha G_1 G G_1 G_1 n_1 m_1 \hat{m} G n,n_1,m_1 \alpha  \hat{m}=\left(\frac{1}{\alpha}\right)^2 m_1   \alpha=\mathbb{E}\left[\frac{n_1}{n}\right],  \alpha \frac{n_1}{n} \hat{m}=\left(\frac{n}{n_1}\right)^2 m_1. \hat{m}=\frac{n(n-1)}{n_1(n_1-1)} m_1.","['probability', 'statistics', 'graph-theory', 'estimation']"
25,Creating a new mean,Creating a new mean,,"I was wondering: are there some necessary criteria to be respected and fulfilled for creating a new statistical mean? This question came up to my mind while studying arithmeticl mean, gometric mean and harmonic mean. I couldn't notice that, for example, geometric mean cannot accept zero or negative inputs. So I thought: let's create a new mean (in the same spirit, ish, of someone creating a new distance in mathematics, despite a distance must respect well given properties). So I thought of those trivial criteria, but I wonder if there is something necessary. Let's call $\theta$ the new mean, acting on $n$ variables. If $a, b > 0$ then $\theta(a, b) > 0$ $\theta(a, b) < a + b$ $\theta(a, b) < \max\{ a, b \}$ $\theta(a, b) > \min \{a, b \}$ Is there anything else one would expect?","I was wondering: are there some necessary criteria to be respected and fulfilled for creating a new statistical mean? This question came up to my mind while studying arithmeticl mean, gometric mean and harmonic mean. I couldn't notice that, for example, geometric mean cannot accept zero or negative inputs. So I thought: let's create a new mean (in the same spirit, ish, of someone creating a new distance in mathematics, despite a distance must respect well given properties). So I thought of those trivial criteria, but I wonder if there is something necessary. Let's call the new mean, acting on variables. If then Is there anything else one would expect?","\theta n a, b > 0 \theta(a, b) > 0 \theta(a, b) < a + b \theta(a, b) < \max\{ a, b \} \theta(a, b) > \min \{a, b \}","['analysis', 'statistics', 'means']"
26,Does $\mathbb{E}\left[\frac{X}{X+Y}\right]\le \frac{\mathbb{E}[X]}{\mathbb{E}[X+Y]}$ hold?,Does  hold?,\mathbb{E}\left[\frac{X}{X+Y}\right]\le \frac{\mathbb{E}[X]}{\mathbb{E}[X+Y]},"Assume that two random variables $X$ and $Y$ satisfy $X\ge 0$ , $Y\ge0$ and $X+Y\ge 1$ . Does it hold that $$\mathbb{E}\left[\frac{X}{X+Y}\right]\le \frac{\mathbb{E}[X]}{\mathbb{E}[X+Y]},$$ whereby $\mathbb{E}[\cdot]$ denotes the expected value? Jensen's inequality may be helpful, but I don't know how to apply it here. I want to add the additional condition that X increases if X+Y increases.","Assume that two random variables and satisfy , and . Does it hold that whereby denotes the expected value? Jensen's inequality may be helpful, but I don't know how to apply it here. I want to add the additional condition that X increases if X+Y increases.","X Y X\ge 0 Y\ge0 X+Y\ge 1 \mathbb{E}\left[\frac{X}{X+Y}\right]\le \frac{\mathbb{E}[X]}{\mathbb{E}[X+Y]}, \mathbb{E}[\cdot]","['probability', 'statistics', 'inequality', 'expected-value']"
27,"Find Var(Y) given Y is uniformly distributed in (X,1) and X is uniformly distributed in (0,1)","Find Var(Y) given Y is uniformly distributed in (X,1) and X is uniformly distributed in (0,1)",,"I approached this problem by using the formula Var(Y) = $E(Y^2)-E(Y)^{2}$ . E(Y) = $\frac{3}{4}$ However, my difficulty was finding $E(Y^2)$ . Can someone explain why this integral setup is wrong? $E(Y^2) =  \int_{0}^{1} \int_{x}^{1} y^2  dy dx $","I approached this problem by using the formula Var(Y) = . E(Y) = However, my difficulty was finding . Can someone explain why this integral setup is wrong?",E(Y^2)-E(Y)^{2} \frac{3}{4} E(Y^2) E(Y^2) =  \int_{0}^{1} \int_{x}^{1} y^2  dy dx ,"['probability', 'statistics', 'expected-value', 'variance']"
28,Why does a covariance matrix have to be positive semi definite on an intuitive level,Why does a covariance matrix have to be positive semi definite on an intuitive level,,"If I have a covariance matrix of some random vector $X$ with expectation $\mu \in \mathbb{R}^n$ it is not that difficult to show that its covariance matrix is positive semi definite; given $$ Cov(X) = \mathbb{E}[(X-\mu) (X-\mu)^T] $$ For any vector $z\in \mathbb{R}^n$ we have $$ z^T Cov(X) z = z^T \mathbb{E}[(X-\mu) (X-\mu)^T] z =  \mathbb{E}[z^T(X-\mu) (X-\mu)^T z] $$ which is just the inner product squared $$ \mathbb{E}[z^T(X-\mu) (X-\mu)^T z] = \mathbb{E}[<z,(X-\mu)>^2] \geq 0 $$ and hence always greater than or equal $0$ . I don't really understand however why a non PSD cannot function as a covariance matrix on an intuitive level. Suppose you have a non-PSD matrix. Can you prove by contradiction that it can not be the covariance matrix of some random vector X?",If I have a covariance matrix of some random vector with expectation it is not that difficult to show that its covariance matrix is positive semi definite; given For any vector we have which is just the inner product squared and hence always greater than or equal . I don't really understand however why a non PSD cannot function as a covariance matrix on an intuitive level. Suppose you have a non-PSD matrix. Can you prove by contradiction that it can not be the covariance matrix of some random vector X?,"X \mu \in \mathbb{R}^n 
Cov(X) = \mathbb{E}[(X-\mu) (X-\mu)^T]
 z\in \mathbb{R}^n 
z^T Cov(X) z = z^T \mathbb{E}[(X-\mu) (X-\mu)^T] z =  \mathbb{E}[z^T(X-\mu) (X-\mu)^T z]
 
\mathbb{E}[z^T(X-\mu) (X-\mu)^T z] = \mathbb{E}[<z,(X-\mu)>^2] \geq 0
 0","['probability', 'statistics', 'covariance']"
29,What is the probability that N points randomly selected on a circle all lie within a fraction f of the circumference?,What is the probability that N points randomly selected on a circle all lie within a fraction f of the circumference?,,"How do I prove that the answer to this question: If N objects are placed randomly in a ring, what is the probability that they all lie together along an arc of length that is a fraction, f < 1/2, of the circumference or shorter? is: $Nf^{N-1}$ . This problem comes up in astronomy in the guise of what the chances are that these N planets in the sky are above the horizon at some point in a day.  It can also come up in coverage problems and length of gaps of coverage from independent Earth-observing satellites.","How do I prove that the answer to this question: If N objects are placed randomly in a ring, what is the probability that they all lie together along an arc of length that is a fraction, f < 1/2, of the circumference or shorter? is: . This problem comes up in astronomy in the guise of what the chances are that these N planets in the sky are above the horizon at some point in a day.  It can also come up in coverage problems and length of gaps of coverage from independent Earth-observing satellites.",Nf^{N-1},"['probability', 'statistics']"
30,Distribution of the square of a random normal variable,Distribution of the square of a random normal variable,,"If we have $X \sim N(\mu, \sigma^2)$ , then what distribution does $X^2$ follow? In the case of the standard normal distribution, this is the chi-squared distribution, and in the case of unit variance - the non-central chi-squared distribution. However, is there a particular distribution for the case of zero mean and non-unit variance, or in the general case? Are the PDF and CDF of closed form?","If we have , then what distribution does follow? In the case of the standard normal distribution, this is the chi-squared distribution, and in the case of unit variance - the non-central chi-squared distribution. However, is there a particular distribution for the case of zero mean and non-unit variance, or in the general case? Are the PDF and CDF of closed form?","X \sim N(\mu, \sigma^2) X^2","['statistics', 'probability-distributions', 'normal-distribution']"
31,Estimating $\lambda$ in a Poisson Distribution from a set of data,Estimating  in a Poisson Distribution from a set of data,\lambda,"I need to estimate $\lambda$ from this data. The observed frequencies / probabilities are obtained by doing each total number observed divided by $280$ . I know that $P(X=0) = \frac{e^{-\lambda}\cdot \lambda^0}{0!} = 0.514$ , so this gives $\lambda = 0.666$ . My notes say this is a correct way of doing it. However, my notes also say I can solve by obtaining a sample average to give $\lambda = 0.684 $ . My notes say this gives the theoretical values in the table. How do I do this ? I don't know how the theoretical values were obtained.","I need to estimate from this data. The observed frequencies / probabilities are obtained by doing each total number observed divided by . I know that , so this gives . My notes say this is a correct way of doing it. However, my notes also say I can solve by obtaining a sample average to give . My notes say this gives the theoretical values in the table. How do I do this ? I don't know how the theoretical values were obtained.",\lambda 280 P(X=0) = \frac{e^{-\lambda}\cdot \lambda^0}{0!} = 0.514 \lambda = 0.666 \lambda = 0.684 ,"['probability', 'statistics']"
32,Using a Continuous Time Markov Chain for Discrete Times,Using a Continuous Time Markov Chain for Discrete Times,,"As I understand, there some major differences between Discrete Time Markov Chains compared to Continuous Time Markov Chains. For example: Discrete Time Markov Chain: Characterized by a constant transition probability matrix ""P"" Continuous Time Markov Chain: Characterized by a time dependent transition probability matrix ""P(t)"" and a constant infinitesimal generator matrix ""Q"". The Continuous Time Markov Chain is based on the Exponential Distribution and thereby must obey the Memoryless Property. Suppose I collect data at discrete time points (e.g. the health of a patient at the start of every month, e.g. ""healthy"", ""sick"", ""very sick"") - technically speaking, there is nothing stopping me from ""tricking"" my computer and saying that these time measurements are actually continuous , and then estimating the P(t) and Q matrix - even though these concepts (i.e. P(t) and Q) are not defined in a Discrete Time Markov Chain. Based on this set-up (i.e. creating a Continuous Time Markov Chain using fundamentally Discrete data), I anticipate that the P(t) matrix will be likely be ""constant"" between time intervals (i.e. staircase/stepwise function) - e.g. the probability of transitioning between two states remains identical from the start to the end of any given month. My Question: Is what I have described (e.g. piecewise approximation) a somewhat valid approach insofar as treating discrete times as continuous when using Markov Chains? Or is this fundamentally incorrect? The reason I am interested in specifically using a Continuous Time Markov Chain (even if there is discrete data) is that this allows me in theory to obtain time-dependent transition probabilities (even if they are constant between time intervals) as opposed to constant transition probabilities - and these time-dependent transition probabilities can be useful in different real-world applications. Thanks! Note: I understand the risks in using such an approach - in real life, the health of a medical patient might deteriorate or improve within a given month, but the Continuous Time Markov Chain created on discrete data would not be able to pick up on this. However, all things being equal - this kind of information would fundamentally not be captured within the discrete data and the Discrete Time Markov Chain would fundamentally not be able to pick up on this. Therefore, I was interested in learning about whether or not analyzing discrete times using a Continuous Time Markov Chain might provide some benefits over a Discrete Time Markov Chain while likely not incurring any additional disadvantages (i.e. best case scenario - slightly better; worse case scenario - roughly equivalent).","As I understand, there some major differences between Discrete Time Markov Chains compared to Continuous Time Markov Chains. For example: Discrete Time Markov Chain: Characterized by a constant transition probability matrix ""P"" Continuous Time Markov Chain: Characterized by a time dependent transition probability matrix ""P(t)"" and a constant infinitesimal generator matrix ""Q"". The Continuous Time Markov Chain is based on the Exponential Distribution and thereby must obey the Memoryless Property. Suppose I collect data at discrete time points (e.g. the health of a patient at the start of every month, e.g. ""healthy"", ""sick"", ""very sick"") - technically speaking, there is nothing stopping me from ""tricking"" my computer and saying that these time measurements are actually continuous , and then estimating the P(t) and Q matrix - even though these concepts (i.e. P(t) and Q) are not defined in a Discrete Time Markov Chain. Based on this set-up (i.e. creating a Continuous Time Markov Chain using fundamentally Discrete data), I anticipate that the P(t) matrix will be likely be ""constant"" between time intervals (i.e. staircase/stepwise function) - e.g. the probability of transitioning between two states remains identical from the start to the end of any given month. My Question: Is what I have described (e.g. piecewise approximation) a somewhat valid approach insofar as treating discrete times as continuous when using Markov Chains? Or is this fundamentally incorrect? The reason I am interested in specifically using a Continuous Time Markov Chain (even if there is discrete data) is that this allows me in theory to obtain time-dependent transition probabilities (even if they are constant between time intervals) as opposed to constant transition probabilities - and these time-dependent transition probabilities can be useful in different real-world applications. Thanks! Note: I understand the risks in using such an approach - in real life, the health of a medical patient might deteriorate or improve within a given month, but the Continuous Time Markov Chain created on discrete data would not be able to pick up on this. However, all things being equal - this kind of information would fundamentally not be captured within the discrete data and the Discrete Time Markov Chain would fundamentally not be able to pick up on this. Therefore, I was interested in learning about whether or not analyzing discrete times using a Continuous Time Markov Chain might provide some benefits over a Discrete Time Markov Chain while likely not incurring any additional disadvantages (i.e. best case scenario - slightly better; worse case scenario - roughly equivalent).",,"['probability', 'statistics', 'markov-chains']"
33,Can Strong LLN and Weak LLN apply to continuous distributions?,Can Strong LLN and Weak LLN apply to continuous distributions?,,"Can Strong LLN and Weak LLN apply to continuous distributions? Or it can only apply to discrete distributions? Representation of LLN: $X_1,X_2,\ldots,X_n$ are i.i.d, and their expectation values are finite, then $$(X_1+X_2+\cdots+X_n)/n \longrightarrow  \mathrm{E}(X),$$ as $n$ goes to infinity. Moreover, just to confirm, Central limit theorem also can apply to continuous distributions, right?","Can Strong LLN and Weak LLN apply to continuous distributions? Or it can only apply to discrete distributions? Representation of LLN: are i.i.d, and their expectation values are finite, then as goes to infinity. Moreover, just to confirm, Central limit theorem also can apply to continuous distributions, right?","X_1,X_2,\ldots,X_n (X_1+X_2+\cdots+X_n)/n \longrightarrow  \mathrm{E}(X), n","['probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
34,Give the density of the product of independent Uniform random variables,Give the density of the product of independent Uniform random variables,,"I had a lot of trouble doing this problem on my exam. Can anybody please tell me why my attempt fails? Let $X$ ~ $Uniform(0,1)$ and $Y$ ~ $Uniform(0,1)$ be independent random variables. Find the density of $Z = XY.$ Solution attempt: For $a\in(0,1):$ $F_Z(a) = P(Z<a) = P(XY < a) = P(Y < \frac{a}X) = \int_0^1\int_0^{\frac{a}x}f_X(x)f_Y(y)dydx = \int_0^1\int_0^{\frac{a}x}1dydx = \int_0^1(\frac{a}x)dx = (aln|x|)\rvert_0^1 = aln(1) - aln(0) = -aln(0)$ which is undefined..",I had a lot of trouble doing this problem on my exam. Can anybody please tell me why my attempt fails? Let ~ and ~ be independent random variables. Find the density of Solution attempt: For which is undefined..,"X Uniform(0,1) Y Uniform(0,1) Z = XY. a\in(0,1): F_Z(a) = P(Z<a) = P(XY < a) = P(Y < \frac{a}X) = \int_0^1\int_0^{\frac{a}x}f_X(x)f_Y(y)dydx = \int_0^1\int_0^{\frac{a}x}1dydx = \int_0^1(\frac{a}x)dx = (aln|x|)\rvert_0^1 = aln(1) - aln(0) = -aln(0)","['probability', 'statistics', 'probability-distributions', 'solution-verification', 'density-function']"
35,Probability of receiving a signal,Probability of receiving a signal,,"You have a signal receiver that is able to receive two signals, 1 and 2. Both signals have mean 0 and arrive with equal frequency. Signal 1 is normally distributed with variance 4, and signal 2 is normally distributed with variance 9. You receive a signal with magnitude 2. What is the probability that the signal you received is signal 1? Do I solve this problem by finding the sd of signal 1 as $\sqrt{4}=2$ , that a signal with magnitude 2 is $\frac{2-0}{2} = 1$ standard deviation away from the mean, and the probability that that occurs is the answer? I'm also confused on how to incorporate the distribution of signal 2 in my answer.","You have a signal receiver that is able to receive two signals, 1 and 2. Both signals have mean 0 and arrive with equal frequency. Signal 1 is normally distributed with variance 4, and signal 2 is normally distributed with variance 9. You receive a signal with magnitude 2. What is the probability that the signal you received is signal 1? Do I solve this problem by finding the sd of signal 1 as , that a signal with magnitude 2 is standard deviation away from the mean, and the probability that that occurs is the answer? I'm also confused on how to incorporate the distribution of signal 2 in my answer.",\sqrt{4}=2 \frac{2-0}{2} = 1,['statistics']
36,"Probability of choose point in interval (0,1)","Probability of choose point in interval (0,1)",,"Choose a point random uniformly in $(0,1)$ . Then, this point divides the interval (0,1) into two sub-intervals. Compute the expected length of the interval containing a fixed point $s \in [0,1]$ . Compute the expected distance of the randomly chosen point from $s$ . My approach: My intuition is that the expected length of the interval containing $s$ is 1/4, since on average the sub-intervals should be about length 1/2 each, and $s$ is in exactly one of them. I don't know how to mathematically prove this intuition if it's correct. I have no idea how to approach 2.","Choose a point random uniformly in . Then, this point divides the interval (0,1) into two sub-intervals. Compute the expected length of the interval containing a fixed point . Compute the expected distance of the randomly chosen point from . My approach: My intuition is that the expected length of the interval containing is 1/4, since on average the sub-intervals should be about length 1/2 each, and is in exactly one of them. I don't know how to mathematically prove this intuition if it's correct. I have no idea how to approach 2.","(0,1) s \in [0,1] s s s","['real-analysis', 'probability', 'statistics']"
37,Definition of confidence interval,Definition of confidence interval,,"Consider the statistical model $\left( E, \{\Bbb P_ \theta\}_{\theta \in \Theta}\right)$ (with $\Theta \subseteq \Bbb R$ ) and a random i.i.d sample $X_1, \ldots, X_n$ from the true distribution $P_{\bar{\theta}}.$ Note that we fix the true parameter $\bar{\theta}$ to differentiate it from the generic $\theta$ that is used for denoting the elements in $\Theta.$ Let $\alpha \in (0, 1).$ Which of the following definitions of a confidence interval of level $1 - \alpha$ is the correct one? Could you point me to a reference on the correct definition? A). An interval $\mathcal{I}_n$ whose boundaries do not depend on $\bar{\theta}$ (but are possibly a function of the sample) such that $\Bbb P_{\theta}(\mathcal{I}_n \ni \bar{\theta}) \geq 1 - \alpha \;\;\forall \;\theta \in \Theta.$ B). An interval $\mathcal{I}_n$ whose boundaries do not depend on $\bar{\theta}$ (but are possibly a function of the sample) such that $\Bbb P_{\theta}(\mathcal{I}_n \ni \theta) \geq 1 - \alpha \;\;\forall \;\theta \in \Theta.$ C). An interval $\mathcal{I}_n$ whose boundaries do not depend on $\bar{\theta}$ (but are possibly a function of the sample) such that $\Bbb P_{\bar{\theta}}(\mathcal{I}_n \ni \bar{\theta}) \geq 1 - \alpha.$",Consider the statistical model (with ) and a random i.i.d sample from the true distribution Note that we fix the true parameter to differentiate it from the generic that is used for denoting the elements in Let Which of the following definitions of a confidence interval of level is the correct one? Could you point me to a reference on the correct definition? A). An interval whose boundaries do not depend on (but are possibly a function of the sample) such that B). An interval whose boundaries do not depend on (but are possibly a function of the sample) such that C). An interval whose boundaries do not depend on (but are possibly a function of the sample) such that,"\left( E, \{\Bbb P_ \theta\}_{\theta \in \Theta}\right) \Theta \subseteq \Bbb R X_1, \ldots, X_n P_{\bar{\theta}}. \bar{\theta} \theta \Theta. \alpha \in (0, 1). 1 - \alpha \mathcal{I}_n \bar{\theta} \Bbb P_{\theta}(\mathcal{I}_n \ni \bar{\theta}) \geq 1 - \alpha \;\;\forall \;\theta \in \Theta. \mathcal{I}_n \bar{\theta} \Bbb P_{\theta}(\mathcal{I}_n \ni \theta) \geq 1 - \alpha \;\;\forall \;\theta \in \Theta. \mathcal{I}_n \bar{\theta} \Bbb P_{\bar{\theta}}(\mathcal{I}_n \ni \bar{\theta}) \geq 1 - \alpha.","['probability-theory', 'statistics', 'reference-request', 'statistical-inference', 'confidence-interval']"
38,Expected Value Notation Question,Expected Value Notation Question,,Does the red marked box this mean $E[(X-E[X])^2]$ or $E[X-E[X]]^2$ ?,Does the red marked box this mean or ?,E[(X-E[X])^2] E[X-E[X]]^2,"['probability', 'statistics', 'expected-value']"
39,MLE of the Geometric Distribution,MLE of the Geometric Distribution,,"Suppose that $X_{1},X_{2},...,X_{n}$ are independently and identically distributed as $Ge(\theta)$ . (i) Find the maximum likelihood estimator of $\theta$ My solution: $\theta = \frac{n}{\sum_{i=1}^{n}x_{i}}$ Therefore, $E(\hat\theta) = \frac{1}{\theta}$ (ii) Hence show that the maximum likelihood estimator of $\psi = \frac{(1-\theta)}{\theta}$ is the sample mean $(\bar X)$ . Try as I might, I can't re-arrange the answer to question 1 into the form shown in question 2. Please may someone help me?","Suppose that are independently and identically distributed as . (i) Find the maximum likelihood estimator of My solution: Therefore, (ii) Hence show that the maximum likelihood estimator of is the sample mean . Try as I might, I can't re-arrange the answer to question 1 into the form shown in question 2. Please may someone help me?","X_{1},X_{2},...,X_{n} Ge(\theta) \theta \theta = \frac{n}{\sum_{i=1}^{n}x_{i}} E(\hat\theta) = \frac{1}{\theta} \psi = \frac{(1-\theta)}{\theta} (\bar X)","['statistics', 'probability-distributions', 'means', 'maximum-likelihood']"
40,Inclusion–exclusion principle; what is $(-1)^{n+1}$,Inclusion–exclusion principle; what is,(-1)^{n+1},"could somebody kindly confirm that my understanding of inclusion-exclusion matches it's formula. for a 3 sets example; we add 3 unions, subtract the total of all 3 pairwise intersections and add the triple-wise intersections as such; $A_1\cup A_2\cup A_3= A_1+A_2+A_3-A_1\cap A_2 - A_1\cap A_3-A_2\cap A_3+A_1\cap A_2\cap A_3$ in summary, it is adding all sets, subtract the over-count and adding back the ""over-subtract"" for multiple sets; $ \bigcup_{i=1}^n A_i = \sum_{i=1}^n A_i - \sum_{i<j} A_i \cap A_j + \sum_{i<j<k} A_i \cap A_j \cap A_k - \dots + (-1)^{n+1} A_i \cap \dots A_n$ $\sum_{i=1}^n A_i $ ; Include the cardinalities of the sets $\sum_{i<j} A_i \cap A_j $ ; Exclude the cardinalities of the pairwise intersections. $\sum_{i<j<k} A_i \cap A_j \cap A_k $ ; Include the cardinalities of the triple-wise intersections. $\dots$ $(-1)^{n+1} A_i \cap \dots A_n $ ; Include cardinality of the $n$ -tuple-wise intersection. If the above are correct, what does $(-1)^{n+1}$ represents? kindly advise. Thank you","could somebody kindly confirm that my understanding of inclusion-exclusion matches it's formula. for a 3 sets example; we add 3 unions, subtract the total of all 3 pairwise intersections and add the triple-wise intersections as such; in summary, it is adding all sets, subtract the over-count and adding back the ""over-subtract"" for multiple sets; ; Include the cardinalities of the sets ; Exclude the cardinalities of the pairwise intersections. ; Include the cardinalities of the triple-wise intersections. ; Include cardinality of the -tuple-wise intersection. If the above are correct, what does represents? kindly advise. Thank you",A_1\cup A_2\cup A_3= A_1+A_2+A_3-A_1\cap A_2 - A_1\cap A_3-A_2\cap A_3+A_1\cap A_2\cap A_3  \bigcup_{i=1}^n A_i = \sum_{i=1}^n A_i - \sum_{i<j} A_i \cap A_j + \sum_{i<j<k} A_i \cap A_j \cap A_k - \dots + (-1)^{n+1} A_i \cap \dots A_n \sum_{i=1}^n A_i  \sum_{i<j} A_i \cap A_j  \sum_{i<j<k} A_i \cap A_j \cap A_k  \dots (-1)^{n+1} A_i \cap \dots A_n  n (-1)^{n+1},"['probability', 'combinatorics', 'statistics', 'elementary-set-theory', 'inclusion-exclusion']"
41,Why is $\Pr(|X-Y|>t)\le\Pr(|X|>t/2)+\Pr(|Y|>t/2)$?,Why is ?,\Pr(|X-Y|>t)\le\Pr(|X|>t/2)+\Pr(|Y|>t/2),"I am having trouble proving the inequality in the title of this post. This inequality is commonly used in Statistical Learning Theory, when proving Symmetrization lemmas. When used in such context, it usually takes the following form : $$\mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i(f(X_i) - f(X_i'))\right\vert>t\right) \le \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i f(X_i) \right\vert>t/2\right) + \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_if(X_i') \right\vert>t/2\right)$$ Where the $\varepsilon_i$ are i.i.d. Rademacher random variables (i.e. take value $\pm 1$ with probability $1/2$ ), $(X_i)$ are i.i.d. random variables and $(X_i')$ are i.i.d. copies of $(X_i)$ and $f$ is some deterministic function. The above inequality can be found, e.g., in the paper A few notes on Statistical Learning Theory by Shahar Mendelson (page 6), and is said to directly follow from the triangle inequality. Although it looks very simple, I can't see why this inequality holds. Here is what I have so far : First, $\mathbb P(|a-b|>t) = 1 - \mathbb P(|a-b|\le t)$ . Furthermore, by the triangle inequality, it is straightforward to see that $\{|a|\le t/2\}\cap\{|b|\le t/2\} \subseteq \{|a-b|\le t\}$ . We thus have the following bound $$\mathbb P(|a-b|>t) \le 1- \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right)$$ Now I'd like to have a bound of the form $1 - \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right) \le \mathbb P\left(\{|a|> t/2\}\cup\{|b|> t/2\}\right) $ for the RHS of the inequality, from which I would be able to conclude by the union bound, but I can't prove the desired inequality... It seems very elementary, what am I missing here ?","I am having trouble proving the inequality in the title of this post. This inequality is commonly used in Statistical Learning Theory, when proving Symmetrization lemmas. When used in such context, it usually takes the following form : Where the are i.i.d. Rademacher random variables (i.e. take value with probability ), are i.i.d. random variables and are i.i.d. copies of and is some deterministic function. The above inequality can be found, e.g., in the paper A few notes on Statistical Learning Theory by Shahar Mendelson (page 6), and is said to directly follow from the triangle inequality. Although it looks very simple, I can't see why this inequality holds. Here is what I have so far : First, . Furthermore, by the triangle inequality, it is straightforward to see that . We thus have the following bound Now I'd like to have a bound of the form for the RHS of the inequality, from which I would be able to conclude by the union bound, but I can't prove the desired inequality... It seems very elementary, what am I missing here ?",\mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i(f(X_i) - f(X_i'))\right\vert>t\right) \le \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_i f(X_i) \right\vert>t/2\right) + \mathbb P\left(\left\vert\sum_{i=1}^n\varepsilon_if(X_i') \right\vert>t/2\right) \varepsilon_i \pm 1 1/2 (X_i) (X_i') (X_i) f \mathbb P(|a-b|>t) = 1 - \mathbb P(|a-b|\le t) \{|a|\le t/2\}\cap\{|b|\le t/2\} \subseteq \{|a-b|\le t\} \mathbb P(|a-b|>t) \le 1- \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right) 1 - \mathbb P\left(\{|a|\le t/2\}\cap\{|b|\le t/2\}\right) \le \mathbb P\left(\{|a|> t/2\}\cup\{|b|> t/2\}\right) ,"['probability', 'probability-theory', 'statistics', 'inequality']"
42,How to test $\mu_1 = \mu_2 = \mu_3$ when the covariance matrix is known/unknown?,How to test  when the covariance matrix is known/unknown?,\mu_1 = \mu_2 = \mu_3,"I need to verify the hypothesis H0: $\mu_1 = \mu_2 = \mu_3$ vs the alternative(at least one mean doesn't equal to the others). Where $N=20$ and the estimated mean $\overline{x} = (1, 0, -1)^T$ . Popluation comes from normal distribution $N_3(μ,Σ)$ . a) with known covariance matrix $ \begin{bmatrix} 3 & 2 & 1 \\ 2 & 3 & 1 \\ 1 & 1 & 4  \end{bmatrix}  $ . I wrote the hypothesis in the form of $C\mu$ where $ C = \begin{bmatrix} 1 & -1 & 0 \\ 1 & 0 & -1 \\ \end{bmatrix}  $ . Then I calculated the statistics (I found the formula in the book, is it ok?) in such a way $$ T^2 = N(C\overline{x})^T(CSC^T)^{-1}(C\overline{x})$$ but then I cannot find one good critical value to compare it to (I found this formula, but again could you please confirm/deny is it correct): $$ T^2_{n-1,N-1,\alpha} = \frac{(N-1)(n-1)}{N-n+1}, \quad \text{where $n$ is the dimension of the mean distribution, here it's 3.}$$ Is this formula correct? (I saw other formulas as well and I am confused.) B) What will be the difference in the calculations if we assume that the covariance matrix is unknown ? Note: I don't need any coding solutions (R, python, etc.); I want to understand it on the theoretical level.","I need to verify the hypothesis H0: vs the alternative(at least one mean doesn't equal to the others). Where and the estimated mean . Popluation comes from normal distribution . a) with known covariance matrix . I wrote the hypothesis in the form of where . Then I calculated the statistics (I found the formula in the book, is it ok?) in such a way but then I cannot find one good critical value to compare it to (I found this formula, but again could you please confirm/deny is it correct): Is this formula correct? (I saw other formulas as well and I am confused.) B) What will be the difference in the calculations if we assume that the covariance matrix is unknown ? Note: I don't need any coding solutions (R, python, etc.); I want to understand it on the theoretical level.","\mu_1 = \mu_2 = \mu_3 N=20 \overline{x} = (1, 0, -1)^T N_3(μ,Σ)  \begin{bmatrix}
3 & 2 & 1 \\
2 & 3 & 1 \\
1 & 1 & 4 
\end{bmatrix}   C\mu  C = \begin{bmatrix}
1 & -1 & 0 \\
1 & 0 & -1 \\
\end{bmatrix}    T^2 = N(C\overline{x})^T(CSC^T)^{-1}(C\overline{x})  T^2_{n-1,N-1,\alpha} = \frac{(N-1)(n-1)}{N-n+1}, \quad \text{where n is the dimension of the mean distribution, here it's 3.}","['statistics', 'hypothesis-testing', 'means']"
43,Why is $P(|x|\ge\varepsilon) = P(x^2\ge \varepsilon^2)$?,Why is ?,P(|x|\ge\varepsilon) = P(x^2\ge \varepsilon^2),I am trying to understand the Chebyshev inequality. The step I cannot really follow is the identity: $P(|x-\mu|\ge\varepsilon) = P((x-\mu)^2 \ge \varepsilon^2)$ . Can anyone explain the intuition behind this or give some other form of insight? That would be really helpful! Thanks!,I am trying to understand the Chebyshev inequality. The step I cannot really follow is the identity: . Can anyone explain the intuition behind this or give some other form of insight? That would be really helpful! Thanks!,P(|x-\mu|\ge\varepsilon) = P((x-\mu)^2 \ge \varepsilon^2),"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
44,Understanding the concept of infinitely often,Understanding the concept of infinitely often,,"My book in stochastic processes has this section about i.o which I don’t really understand. Is there any example of any other to explain this? How can I understand this? It states the following: Let $A_{1}, A_{2}, \ldots$ be a sequence of subsets of $\Omega$ . We define $$ \left(A_{n} \text { i.o. }\right)=\bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A_{m} $$ The abbreviation i.o. stands for infinitely often.",My book in stochastic processes has this section about i.o which I don’t really understand. Is there any example of any other to explain this? How can I understand this? It states the following: Let be a sequence of subsets of . We define The abbreviation i.o. stands for infinitely often.,"A_{1}, A_{2}, \ldots \Omega 
\left(A_{n} \text { i.o. }\right)=\bigcap_{n=1}^{\infty} \bigcup_{m=n}^{\infty} A_{m}
","['probability', 'measure-theory', 'statistics', 'stochastic-processes', 'borel-cantelli-lemmas']"
45,Lottery fairness problem [closed],Lottery fairness problem [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm wondering if this scheme is fair or not. I'm not an expert in math so I have no clue. When lottery room starts the winning number is set by random integer between 0-999. Every player enters the lottery also given random integer between 0-999, there is no duplicate. After lottery ends the payer closest to the winning number is the winner. For example: Winning number is 500 Player 1: assigned random number 123 Player 2: assigned random number 789 Player 3: assigned random number 456 So the winner is Player 3. Is there any problem with this, can I call it fair? In my opinion it seems fair because everything is random, but then again there is something that disapproves its fairness. Thank you.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm wondering if this scheme is fair or not. I'm not an expert in math so I have no clue. When lottery room starts the winning number is set by random integer between 0-999. Every player enters the lottery also given random integer between 0-999, there is no duplicate. After lottery ends the payer closest to the winning number is the winner. For example: Winning number is 500 Player 1: assigned random number 123 Player 2: assigned random number 789 Player 3: assigned random number 456 So the winner is Player 3. Is there any problem with this, can I call it fair? In my opinion it seems fair because everything is random, but then again there is something that disapproves its fairness. Thank you.",,"['probability', 'statistics', 'random', 'lotteries']"
46,Find unbiased estimator of $\theta$ when $f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }}$,Find unbiased estimator of  when,\theta f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }},"Let $f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }}$ be a probability density function of sample $(X_{1},X_{2},...,X_{n})$ where $x\geq 0$ . Find unbiased estimator for $\theta$ . Here is my solution : $E\left [ \bar{X} \right ] = E\left [ X \right ]=\frac{1}{\theta }\int_{0}^{\infty }2x^{2}e^{\frac{-x^{2}}{\theta }}dx=\frac{1}{\theta }\int_{0}^{\infty }xe^{\frac{-x^{2}}{\theta }}2xdx$ Let $$  t=\frac{x^{2}}{\theta } (t=\frac{x^{2}}{\theta }\Rightarrow \theta t=x^{2}\Rightarrow x=\sqrt{\theta t},x\geq 0) , dt=\frac{2x}{\theta }d\theta \Rightarrow \theta dt=2xdx$$ $=\frac{1}{\theta }\int_{0}^{\infty }(\theta )^{\frac{1}{2}}e^{-t}\theta dt=\frac{\theta ^{\frac{3}{2}}}{\theta }\int_{0}^{\infty }t^{\frac{1}{2}}e^{-t}dt=\sqrt{\theta }.\Gamma (\frac{3}{2})=\sqrt{\theta }.\frac{\sqrt{\pi}}{2}$ $\Rightarrow E\left ( \bar{X} \right )=\sqrt{\theta }.\frac{\sqrt{\pi}}{2}\Rightarrow E(\frac{4\bar{(X)}^{2}}{\pi})=\theta $ Hence unbiased estimator for $\theta$ is $\hat{\theta }=\frac{4\bar{(X)}^{2}}{\pi}$ Is my solution right? I am not sure the calculation. Any idea will be appreciated.",Let be a probability density function of sample where . Find unbiased estimator for . Here is my solution : Let Hence unbiased estimator for is Is my solution right? I am not sure the calculation. Any idea will be appreciated.,"f(x;\theta )=\frac{2x}{\theta }e^{\frac{-x^{2}}{\theta }} (X_{1},X_{2},...,X_{n}) x\geq 0 \theta E\left [ \bar{X} \right ] = E\left [ X \right ]=\frac{1}{\theta }\int_{0}^{\infty }2x^{2}e^{\frac{-x^{2}}{\theta }}dx=\frac{1}{\theta }\int_{0}^{\infty }xe^{\frac{-x^{2}}{\theta }}2xdx   t=\frac{x^{2}}{\theta } (t=\frac{x^{2}}{\theta }\Rightarrow \theta t=x^{2}\Rightarrow x=\sqrt{\theta t},x\geq 0) , dt=\frac{2x}{\theta }d\theta \Rightarrow \theta dt=2xdx =\frac{1}{\theta }\int_{0}^{\infty }(\theta )^{\frac{1}{2}}e^{-t}\theta dt=\frac{\theta ^{\frac{3}{2}}}{\theta }\int_{0}^{\infty }t^{\frac{1}{2}}e^{-t}dt=\sqrt{\theta }.\Gamma (\frac{3}{2})=\sqrt{\theta }.\frac{\sqrt{\pi}}{2} \Rightarrow E\left ( \bar{X} \right )=\sqrt{\theta }.\frac{\sqrt{\pi}}{2}\Rightarrow E(\frac{4\bar{(X)}^{2}}{\pi})=\theta  \theta \hat{\theta }=\frac{4\bar{(X)}^{2}}{\pi}","['statistics', 'probability-distributions', 'expected-value', 'parameter-estimation']"
47,"If $X\sim U(0,1)$ then $Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda )$",If  then,"X\sim U(0,1) Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda )","If $X\sim U(0,1)$ then $Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda )$ Here is my solution : If $X\sim U(0,1)$ then $f(x)=1$ where $0\leq x\leq 1$ . $0<x\Rightarrow -x\leq 0\Rightarrow 1-x\leq 1\Rightarrow \ln (1-x)\leq 0\Rightarrow 0<-\frac{1}{\lambda }\ln (1-x)\Rightarrow y>0$ $F(y)=P(Y\leq y)=P(-\frac{1}{\lambda }\ln (1-X)\leq y)=P(X\leq 1-e^{-\lambda y})=\int_{0}^{1-e^{-\lambda y}}dx=1-e^{-\lambda y}$ From here, $$\begin{cases}  f(y)=\frac{dF(y)}{dy}=\lambda e^{-\lambda y}& \text{ if } y>0 \\   0& \text{otherwise } \end{cases}$$ But I am confused the interval of uniform distribution. Should I take the interval $0<x<1$ or $0\leq x\leq 1$ ? If I take $0\leq x\leq 1$ then $y\geq 0$ but from definiton y can not be $0$ when $f(y)=\lambda e^{-\lambda y}$ . Any help will be appreciated.","If then Here is my solution : If then where . From here, But I am confused the interval of uniform distribution. Should I take the interval or ? If I take then but from definiton y can not be when . Any help will be appreciated.","X\sim U(0,1) Y=-\frac{1}{\lambda }\ln(1-X)\sim Exp(\lambda ) X\sim U(0,1) f(x)=1 0\leq x\leq 1 0<x\Rightarrow -x\leq 0\Rightarrow 1-x\leq 1\Rightarrow \ln (1-x)\leq 0\Rightarrow 0<-\frac{1}{\lambda }\ln (1-x)\Rightarrow y>0 F(y)=P(Y\leq y)=P(-\frac{1}{\lambda }\ln (1-X)\leq y)=P(X\leq 1-e^{-\lambda y})=\int_{0}^{1-e^{-\lambda y}}dx=1-e^{-\lambda y} \begin{cases}
 f(y)=\frac{dF(y)}{dy}=\lambda e^{-\lambda y}& \text{ if } y>0 \\ 
 0& \text{otherwise }
\end{cases} 0<x<1 0\leq x\leq 1 0\leq x\leq 1 y\geq 0 0 f(y)=\lambda e^{-\lambda y}","['probability-theory', 'statistics', 'probability-distributions', 'uniform-distribution', 'exponential-distribution']"
48,Maximum likelihood estimator doesn't exist,Maximum likelihood estimator doesn't exist,,"I was reading a paper, and the last paragraph it says For example, consider the density function $$p_{(\theta,\sigma)}(x)=\frac{1}{2\sqrt{2\pi}}e^{\large{-\frac{1}{2}(x-\theta)^2}} + \frac{1}{2\sqrt{2\pi} \sigma}e^{\large{-\frac{1}{2\sigma^2}(x-\theta)^2}} $$ of the sequence of independent and identically distributed chance variable $X_1,X_2, \dots$ Here $\theta \in \mathbb{R}$ and $\sigma > 0$ . It is easy to see that the supremum of the likelihood function is almost always infinite , no MLE exists [...] So, the likelihood function would be $\prod_{i=1}^np_{(\theta,\sigma)}(x_i)$ by the i.i.d. condition, but I don't see why this wouldn't have maximum. Even the case where $\theta$ is fixed and $\sigma$ is near to $0$ , the function is close to $0$ . Can I get some insights please?","I was reading a paper, and the last paragraph it says For example, consider the density function of the sequence of independent and identically distributed chance variable Here and . It is easy to see that the supremum of the likelihood function is almost always infinite , no MLE exists [...] So, the likelihood function would be by the i.i.d. condition, but I don't see why this wouldn't have maximum. Even the case where is fixed and is near to , the function is close to . Can I get some insights please?","p_{(\theta,\sigma)}(x)=\frac{1}{2\sqrt{2\pi}}e^{\large{-\frac{1}{2}(x-\theta)^2}} + \frac{1}{2\sqrt{2\pi} \sigma}e^{\large{-\frac{1}{2\sigma^2}(x-\theta)^2}}  X_1,X_2, \dots \theta \in \mathbb{R} \sigma > 0 \prod_{i=1}^np_{(\theta,\sigma)}(x_i) \theta \sigma 0 0","['probability', 'statistics', 'maximum-likelihood', 'log-likelihood']"
49,Examples of probability distributions that the product of their random variable is also in same distribution.,Examples of probability distributions that the product of their random variable is also in same distribution.,,"Heads up: my statistics knowledge is very limited, I am not a mathematician. I've recently learned that given two random variables $x$ and $y$ sampled from the same normal distribution their product $xy$ is not normal but it instead belongs to a modified bessel function distribution of the second kind (I believe this is only for $\mu=0$ and $\sigma = 1$ ). What are some well known probability distributions such that $xy$ belongs to the same distribution? I am specially interested in symmetric distributions around the mean.","Heads up: my statistics knowledge is very limited, I am not a mathematician. I've recently learned that given two random variables and sampled from the same normal distribution their product is not normal but it instead belongs to a modified bessel function distribution of the second kind (I believe this is only for and ). What are some well known probability distributions such that belongs to the same distribution? I am specially interested in symmetric distributions around the mean.",x y xy \mu=0 \sigma = 1 xy,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'examples-counterexamples']"
50,How to show $Var(T_n)=\infty$?,How to show ?,Var(T_n)=\infty,"This question is from George Casella Statistical Inference textbook question 10.5. For the mean $\bar{X_n}$ of n iid normal observations with $EX=\mu$ and $VarX=\sigma^2$ , if we take $T_n=\sqrt{n}/\bar{X_n}$ then show $Var(T_n)=\infty$ . The solution is : I don't quite understand this solution. Please see my process is correct or not. First, I need to know the pdf of $\bar{X_n}$ , which is $n(\mu, \sigma^2/n)$ . Hence, $$E(T_n^2)=\int_{-\infty}^{\infty}\frac{n}{x^2}*pdf=\int_{-\infty}^{\infty}\frac{n}{x^2}\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}exp(-\frac{n(x-\mu)^2}{2\sigma^2})$$ . Is my above step correct?","This question is from George Casella Statistical Inference textbook question 10.5. For the mean of n iid normal observations with and , if we take then show . The solution is : I don't quite understand this solution. Please see my process is correct or not. First, I need to know the pdf of , which is . Hence, . Is my above step correct?","\bar{X_n} EX=\mu VarX=\sigma^2 T_n=\sqrt{n}/\bar{X_n} Var(T_n)=\infty \bar{X_n} n(\mu, \sigma^2/n) E(T_n^2)=\int_{-\infty}^{\infty}\frac{n}{x^2}*pdf=\int_{-\infty}^{\infty}\frac{n}{x^2}\frac{\sqrt{n}}{\sqrt{2\pi\sigma^2}}exp(-\frac{n(x-\mu)^2}{2\sigma^2})","['probability', 'integration', 'statistics', 'normal-distribution']"
51,Expected Value of Binomial Distribution,Expected Value of Binomial Distribution,,"What is the expected value of the absolute value of the difference between the number of incoming tails and the number of incoming heads  when a coin is tossed 5 times? Here is what I think : Let $X$ is random variable of number of incoming heads then $p=\frac{1}{2}$ and $n=5$ so $X\sim Binom(5,\frac{1}{2})$ $X$ and $Y=5-X$ ( $Y$ is random variable of number of incoming tails) then $E(K)=E(\left|X-(5-X)\right|)=E(\left|2X-5\right|)=E(2X-5)=2E(X)-5=2.\frac{5}{2}-5=0$ where $K=\left|X-Y\right|=\left|2X-5\right|$ and $E(X)=n.p=5.\frac{1}{2}=\frac{5}{2}$ But the right answer is $\frac{15}{8}$ . Where am I making a mistake? Any help will be appreciated.",What is the expected value of the absolute value of the difference between the number of incoming tails and the number of incoming heads  when a coin is tossed 5 times? Here is what I think : Let is random variable of number of incoming heads then and so and ( is random variable of number of incoming tails) then where and But the right answer is . Where am I making a mistake? Any help will be appreciated.,"X p=\frac{1}{2} n=5 X\sim Binom(5,\frac{1}{2}) X Y=5-X Y E(K)=E(\left|X-(5-X)\right|)=E(\left|2X-5\right|)=E(2X-5)=2E(X)-5=2.\frac{5}{2}-5=0 K=\left|X-Y\right|=\left|2X-5\right| E(X)=n.p=5.\frac{1}{2}=\frac{5}{2} \frac{15}{8}","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'binomial-distribution']"
52,Minimize the standard deviation in list of numbers by subtracting from each value,Minimize the standard deviation in list of numbers by subtracting from each value,,"Suppose we have a list of values, $L = (x_1, x_2, ..., x_n)$ , and another list of numbers, $Q = (q_1, q_2, ..., q_n), q_i$ being the maximum value we can subtract from $x_i$ (so from $x_i$ we can subtract any number in $[0; q_i]$ . I want to find a combination of subtractions such that the standard deviation of the resulting list will be minimal. (In other words, I want to find a set of values, $R = (r_1, r_2, ..., r_n)$ , $r_i \in [0; q_i]$ such that the standard deviation of $L' = (x_1-r_1, x_2-r_2, ..., x_n-r_n)$ is minimal.) A graphical example (the blue points are the plotted $x_i$ ): Any help would be appreciated. Thanks.","Suppose we have a list of values, , and another list of numbers, being the maximum value we can subtract from (so from we can subtract any number in . I want to find a combination of subtractions such that the standard deviation of the resulting list will be minimal. (In other words, I want to find a set of values, , such that the standard deviation of is minimal.) A graphical example (the blue points are the plotted ): Any help would be appreciated. Thanks.","L = (x_1, x_2, ..., x_n) Q = (q_1, q_2, ..., q_n), q_i x_i x_i [0; q_i] R = (r_1, r_2, ..., r_n) r_i \in [0; q_i] L' = (x_1-r_1, x_2-r_2, ..., x_n-r_n) x_i","['probability', 'statistics', 'optimization', 'standard-deviation']"
53,Does any probability distribution have an entropy defined?,Does any probability distribution have an entropy defined?,,"There are lot of probability distribution having infinite moments, for example Cauchy distribution has even the first moment infinite. Therefore, often we cannot calculate second moments used in measures of dispersion or uncertainty (variation and standard deviation). In some application, for example finance, we could replace such measures with entropy. However, firstly I would like to know whether any probability distribution has defined entropy (in particular Shannon entropy). In case of discrete distributions with finite number of possible outcome the entropy is $$ H = -\sum_{i=1}^{n} p_i \log p_i, $$ where $p_i$ is probability of i th outcome. Since $p_i > 0$ and number of terms in the sum is finite, the sum is defined and it is finite. But I am getting stuck with case $n \rightarrow +\infty$ . In this case I need to prove that the sum  under condition that $\sum_{i=1}^{+\infty}p_i = 1$ converges. Similarly in case of continuous distribution, I would need to prove that integral $$ H = -\int_{\mathbb{R}} f(x) \log f(x) \mathrm{d}x $$ for any real function satisfying $f(x) > 0\,\, \forall \in \mathbb{R}$ and $\int_\mathbb{R}f(x)\mathrm{d}x=1$ exists and it is finite. Is it possible to prove these statements?","There are lot of probability distribution having infinite moments, for example Cauchy distribution has even the first moment infinite. Therefore, often we cannot calculate second moments used in measures of dispersion or uncertainty (variation and standard deviation). In some application, for example finance, we could replace such measures with entropy. However, firstly I would like to know whether any probability distribution has defined entropy (in particular Shannon entropy). In case of discrete distributions with finite number of possible outcome the entropy is where is probability of i th outcome. Since and number of terms in the sum is finite, the sum is defined and it is finite. But I am getting stuck with case . In this case I need to prove that the sum  under condition that converges. Similarly in case of continuous distribution, I would need to prove that integral for any real function satisfying and exists and it is finite. Is it possible to prove these statements?","
H = -\sum_{i=1}^{n} p_i \log p_i,
 p_i p_i > 0 n \rightarrow +\infty \sum_{i=1}^{+\infty}p_i = 1 
H = -\int_{\mathbb{R}} f(x) \log f(x) \mathrm{d}x
 f(x) > 0\,\, \forall \in \mathbb{R} \int_\mathbb{R}f(x)\mathrm{d}x=1","['statistics', 'convergence-divergence', 'entropy']"
54,Sum of squared probability distribution $\sum_c p_c^2$ is a measure of uncertainty?,Sum of squared probability distribution  is a measure of uncertainty?,\sum_c p_c^2,"Can the sum of squared probabilities, i.e. $\sum_c p_c^2$ , be considered to be a measure of uncertainty? If so, does it have a mathematical name or theory? The form is similar to Shannon's entropy, $-\sum_c p_c \log(p_c)$ . It satisfies $\frac{1}{C} \leq \sum_c p_c^2 \leq 1$ , partly according to: Is there some general lower bound for sum of squared probabilities? , which shows that the lower bound is satisfied when $\vec{p}=\frac{1}{C}(1,\cdots,1)$ . $\sum_c p_c^2$ never exceeds 1, since $\sum_c p_c=1$ and $p_c^2 \leq p_c$ ( $\because 0 \leq p_c \leq 1$ ). An obvious case for the upper bound is $\vec{p}=(0,\cdots,0,1,0,\cdots,0)$ . Taking them together, it seems like that it can be considered to be a measure of uncertainty. It is appreciated if anyone could provide any names/theories. Best,","Can the sum of squared probabilities, i.e. , be considered to be a measure of uncertainty? If so, does it have a mathematical name or theory? The form is similar to Shannon's entropy, . It satisfies , partly according to: Is there some general lower bound for sum of squared probabilities? , which shows that the lower bound is satisfied when . never exceeds 1, since and ( ). An obvious case for the upper bound is . Taking them together, it seems like that it can be considered to be a measure of uncertainty. It is appreciated if anyone could provide any names/theories. Best,","\sum_c p_c^2 -\sum_c p_c \log(p_c) \frac{1}{C} \leq \sum_c p_c^2 \leq 1 \vec{p}=\frac{1}{C}(1,\cdots,1) \sum_c p_c^2 \sum_c p_c=1 p_c^2 \leq p_c \because 0 \leq p_c \leq 1 \vec{p}=(0,\cdots,0,1,0,\cdots,0)","['probability', 'statistics', 'information-theory', 'entropy']"
55,What is the Type 1 error for the following test?,What is the Type 1 error for the following test?,,"For testing the hypotheses H0: P = 0.8 versus H1: P ≠ 0.8 at 1% significance level, we obtain a sample of n = 100 and p = 0.75. Options: A. 0.100 B. 0.140 C. 0.212  (Correct answer) D. 0.010 E. 0.020 So far I have, z= (P^-P)/std dev = -1.25 za/2 = 2.575 Don't reject null But I don't know how to get the answer","For testing the hypotheses H0: P = 0.8 versus H1: P ≠ 0.8 at 1% significance level, we obtain a sample of n = 100 and p = 0.75. Options: A. 0.100 B. 0.140 C. 0.212  (Correct answer) D. 0.010 E. 0.020 So far I have, z= (P^-P)/std dev = -1.25 za/2 = 2.575 Don't reject null But I don't know how to get the answer",,"['probability', 'statistics', 'probability-distributions']"
56,Notation in the definition of random variable,Notation in the definition of random variable,,"I am trying to understand definition of random variable. Given a probability space $(\Omega, \mathcal F, P),$ a random variable $X$ is a function from the sample space $\Omega$ to the real numbers $\mathbb R.$ Once the outcome $\omega \in \Omega$ of the experiment is revealed, the corresponding $X(ω)$ is known as realization of the random variable. Consider two sample spaces $\Omega_1$ and $\Omega_2$ and a sigma-algebra $\mathcal F_2$ of sets in $\Omega_2.$ Then, for $X$ to be a random variable, there must exist a sigma-algebra $\mathcal F_1$ in $Ω_1$ such that for any set $S$ in $\mathcal F_2$ the inverse image of $S,$ defined by: $$X^{-1}(S) := \{ω \mid X(ω) ∈ S\}$$ I do not know how to read this and I am hoping some one can explain each part to me.","I am trying to understand definition of random variable. Given a probability space a random variable is a function from the sample space to the real numbers Once the outcome of the experiment is revealed, the corresponding is known as realization of the random variable. Consider two sample spaces and and a sigma-algebra of sets in Then, for to be a random variable, there must exist a sigma-algebra in such that for any set in the inverse image of defined by: I do not know how to read this and I am hoping some one can explain each part to me.","(\Omega, \mathcal F, P), X \Omega \mathbb R. \omega \in \Omega X(ω) \Omega_1 \Omega_2 \mathcal F_2 \Omega_2. X \mathcal F_1 Ω_1 S \mathcal F_2 S, X^{-1}(S) := \{ω \mid X(ω) ∈ S\}","['probability', 'statistics', 'notation', 'definition']"
57,Why is gradient the direction of steepest ascent?,Why is gradient the direction of steepest ascent?,,"$$f(x_1,x_2,\dots, x_n):\mathbb{R}^n \to \mathbb{R}$$ The definition of the gradient is $$ \frac{\partial f}{\partial x_1}\hat{e}_1 +\ \cdots +\frac{\partial f}{\partial x_n}\hat{e}_n$$ which is a vector. Reading this definition makes me consider that each component of the gradient corresponds to the rate of change with respect to my objective function if I go along with the direction $\hat{e}_i$ . But I can't see why this vector ( defined by the definition of the gradient ) has anything to do with the steepest ascent. Why do I get maximal value again if I move along with the direction of gradient?",The definition of the gradient is which is a vector. Reading this definition makes me consider that each component of the gradient corresponds to the rate of change with respect to my objective function if I go along with the direction . But I can't see why this vector ( defined by the definition of the gradient ) has anything to do with the steepest ascent. Why do I get maximal value again if I move along with the direction of gradient?,"f(x_1,x_2,\dots, x_n):\mathbb{R}^n \to \mathbb{R}  \frac{\partial f}{\partial x_1}\hat{e}_1 +\ \cdots +\frac{\partial f}{\partial x_n}\hat{e}_n \hat{e}_i","['multivariable-calculus', 'vector-analysis', 'scalar-fields']"
58,Intuitive interpretation of the Laplacian Operator,Intuitive interpretation of the Laplacian Operator,,"Just as the gradient is ""the direction of steepest ascent"", and the divergence is ""amount of stuff created at a point"", is there a nice interpretation of the Laplacian Operator (a.k.a. divergence of gradient)?","Just as the gradient is ""the direction of steepest ascent"", and the divergence is ""amount of stuff created at a point"", is there a nice interpretation of the Laplacian Operator (a.k.a. divergence of gradient)?",,"['multivariable-calculus', 'intuition', 'laplacian']"
59,Partial derivative in gradient descent for two variables,Partial derivative in gradient descent for two variables,,"I've started taking an online machine learning class , and the first learning algorithm that we are going to be using is a form of linear regression using gradient descent. I don't have much of a background in high level math, but here is what I understand so far. Given $m$ number of items in our learning set, with $x$ and $y$ values, we must find the best fit line $h_\theta(x) = \theta_0+\theta_1x$ . The cost function for any guess of $\theta_0,\theta_1$ can be computed as: $$J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$$ where $x^{(i)}$ and $y^{(i)}$ are the $x$ and $y$ values for the $i^{th}$ component in the learning set. If we substitute for $h_\theta(x)$ , $$J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(\theta_0 + \theta_1x^{(i)} - y^{(i)})^2$$ Then, the goal of gradient descent can be expressed as $$\min_{\theta_0, \theta_1}\;J(\theta_0, \theta_1)$$ Finally, each step in the gradient descent can be described as: $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1)$$ for $j = 0$ and $j = 1$ with $\alpha$ being a constant representing the rate of step. I have no idea how to do the partial derivative. I have never taken calculus, but conceptually I understand what a derivative represents. The instructor gives us the partial derivatives for both $\theta_0$ and $\theta_1$ and says not to worry if we don't know how it was derived. (I suppose, technically, it is a computer class, not a mathematics class) However, I would very much like to understand this if possible. Could someone show how the partial derivative could be taken, or link to some resource that I could use to learn more? I apologize if I haven't used the correct terminology in my question; I'm very new to this subject.","I've started taking an online machine learning class , and the first learning algorithm that we are going to be using is a form of linear regression using gradient descent. I don't have much of a background in high level math, but here is what I understand so far. Given number of items in our learning set, with and values, we must find the best fit line . The cost function for any guess of can be computed as: where and are the and values for the component in the learning set. If we substitute for , Then, the goal of gradient descent can be expressed as Finally, each step in the gradient descent can be described as: for and with being a constant representing the rate of step. I have no idea how to do the partial derivative. I have never taken calculus, but conceptually I understand what a derivative represents. The instructor gives us the partial derivatives for both and and says not to worry if we don't know how it was derived. (I suppose, technically, it is a computer class, not a mathematics class) However, I would very much like to understand this if possible. Could someone show how the partial derivative could be taken, or link to some resource that I could use to learn more? I apologize if I haven't used the correct terminology in my question; I'm very new to this subject.","m x y h_\theta(x) = \theta_0+\theta_1x \theta_0,\theta_1 J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 x^{(i)} y^{(i)} x y i^{th} h_\theta(x) J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(\theta_0 + \theta_1x^{(i)} - y^{(i)})^2 \min_{\theta_0, \theta_1}\;J(\theta_0, \theta_1) \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1) j = 0 j = 1 \alpha \theta_0 \theta_1","['multivariable-calculus', 'partial-derivative', 'numerical-optimization', 'gradient-descent']"
60,What exactly is the difference between a derivative and a total derivative?,What exactly is the difference between a derivative and a total derivative?,,"I am not too grounded in differentiation but today, I was posed with a supposedly easy question $w = f(x,y) = x^2 + y^2$ where $x = r\sin\theta $ and $y = r\cos\theta$ requiring the solution to $\partial w / \partial r$ and $\partial w / \partial \theta $. I simply solved the former using the trig identity $\sin^2 \theta + \cos^2 \theta = 1$, resulting to $\partial w / \partial r = 2r$. However I was told that this solution could not be applied to this question because I should be solving for the total derivative . I could not find any good resource online to explain clearly to me the difference between a normal derivative and a total derivative and why my solution here was wrong . Is there anyone who could explain the difference to me using a practical example? Thanks!","I am not too grounded in differentiation but today, I was posed with a supposedly easy question $w = f(x,y) = x^2 + y^2$ where $x = r\sin\theta $ and $y = r\cos\theta$ requiring the solution to $\partial w / \partial r$ and $\partial w / \partial \theta $. I simply solved the former using the trig identity $\sin^2 \theta + \cos^2 \theta = 1$, resulting to $\partial w / \partial r = 2r$. However I was told that this solution could not be applied to this question because I should be solving for the total derivative . I could not find any good resource online to explain clearly to me the difference between a normal derivative and a total derivative and why my solution here was wrong . Is there anyone who could explain the difference to me using a practical example? Thanks!",,"['multivariable-calculus', 'derivatives']"
61,Difference between gradient and Jacobian,Difference between gradient and Jacobian,,"Could anyone explain in simple words (and maybe with an example) what the difference between the gradient and the Jacobian is? The gradient is a vector with the partial derivatives, right?","Could anyone explain in simple words (and maybe with an example) what the difference between the gradient and the Jacobian is? The gradient is a vector with the partial derivatives, right?",,"['multivariable-calculus', 'derivatives', 'vector-fields', 'jacobian', 'scalar-fields']"
62,"What's the largest possible volume of a taco, and how do I make one that big?","What's the largest possible volume of a taco, and how do I make one that big?",,"Let $f$ be a continuous, even function over some interval $I=[-a,a]$ such that the total arc length of $f$ over $I$ is at least $2$ , $f(0)=0$ , and $f$ is increasing on $(0,a)$ .  [You might imagine something like $f(x)=x^2$ .] View the graph of $z=f(x)$ as a surface in $\mathbb{R}^3$ (so, parallel to the $y$ -axis). Model a corn tortilla (or pita bread if you prefer) as a pliable disk of radius $1$ , place it in the $xy$ -plane with its center at the origin, and wrap the tortilla up against the surface $z=f(x)$ . [It should now be clear why there was the arc length condition above.] At this point, we have something like a taco shell.  Now take its convex hull and consider its volume $V$ . Question: Can we determine how to maximize $V$ with respect to all such functions $f$ ?  Because, you know, I want my taco to be the biggest. Elementary candidates for $f$ include a semicircle function, a parabola, an absolute value function, a cosine wave, etc.  But if there is a unique answer, it might not be an elementary curve at all. Edit: This probably shouldn't have been asked in a way that relies on a function $f$ .  Maybe the biggest taco has regions where the shell is curved completely vertical, and we no longer have the graph of a continuous function.  If you prefer, consider curves parametrized by arc length $$\left(X(t),0,Z(t)\right):[0,1]\to xz\mbox{-plane}$$ with $X(0)=Z(0)=0$ , and $X$ and $Z$ (not necessarily strictly) increasing.  (And of course then reflect that curve through the $yz$ -plane and cross it with the $y$ -axis to get the entire surface.)","Let be a continuous, even function over some interval such that the total arc length of over is at least , , and is increasing on .  [You might imagine something like .] View the graph of as a surface in (so, parallel to the -axis). Model a corn tortilla (or pita bread if you prefer) as a pliable disk of radius , place it in the -plane with its center at the origin, and wrap the tortilla up against the surface . [It should now be clear why there was the arc length condition above.] At this point, we have something like a taco shell.  Now take its convex hull and consider its volume . Question: Can we determine how to maximize with respect to all such functions ?  Because, you know, I want my taco to be the biggest. Elementary candidates for include a semicircle function, a parabola, an absolute value function, a cosine wave, etc.  But if there is a unique answer, it might not be an elementary curve at all. Edit: This probably shouldn't have been asked in a way that relies on a function .  Maybe the biggest taco has regions where the shell is curved completely vertical, and we no longer have the graph of a continuous function.  If you prefer, consider curves parametrized by arc length with , and and (not necessarily strictly) increasing.  (And of course then reflect that curve through the -plane and cross it with the -axis to get the entire surface.)","f I=[-a,a] f I 2 f(0)=0 f (0,a) f(x)=x^2 z=f(x) \mathbb{R}^3 y 1 xy z=f(x) V V f f f \left(X(t),0,Z(t)\right):[0,1]\to xz\mbox{-plane} X(0)=Z(0)=0 X Z yz y","['differential-geometry', 'multivariable-calculus', 'optimization', 'recreational-mathematics', 'calculus-of-variations']"
63,"What is the solution to Nash's problem presented in ""A Beautiful Mind""?","What is the solution to Nash's problem presented in ""A Beautiful Mind""?",,"I was watching the said movie the other night, and I started thinking about the equation posed by Nash in the movie.  More specifically, the one he said would take some students a lifetime to solve (obviously, an exaggeration).  Nonetheless, one can't say it's a simple problem. Anyway, here it is $$V = \{F:\mathbb{R^3}\setminus X\rightarrow \mathbb{R^3} \text{ so } \hspace{1mm}\nabla \times F=0\}$$ $$W = \{F = \nabla g\}$$ $$\dim(V/W) = \; 8$$ I haven't actually attempted a solution myself to be honest, but I thought it would be an interesting question to pose. I have done a quick search on this site and Google, but there were surprisingly few results. In any case, I was curious if anyone knew the answer aside from the trivial.","I was watching the said movie the other night, and I started thinking about the equation posed by Nash in the movie.  More specifically, the one he said would take some students a lifetime to solve (obviously, an exaggeration).  Nonetheless, one can't say it's a simple problem. Anyway, here it is I haven't actually attempted a solution myself to be honest, but I thought it would be an interesting question to pose. I have done a quick search on this site and Google, but there were surprisingly few results. In any case, I was curious if anyone knew the answer aside from the trivial.",V = \{F:\mathbb{R^3}\setminus X\rightarrow \mathbb{R^3} \text{ so } \hspace{1mm}\nabla \times F=0\} W = \{F = \nabla g\} \dim(V/W) = \; 8,"['differential-geometry', 'multivariable-calculus', 'partial-differential-equations', 'homology-cohomology', 'popular-math']"
64,Gradient of 2-norm squared,Gradient of 2-norm squared,,Could someone please provide a proof for why the gradient of the squared $2$ -norm of $x$ is equal to $2x$ ? $$\nabla\|x\|_2^2 = 2x$$,Could someone please provide a proof for why the gradient of the squared -norm of is equal to ?,2 x 2x \nabla\|x\|_2^2 = 2x,"['multivariable-calculus', 'vector-analysis', 'normed-spaces', 'scalar-fields']"
65,How to take the gradient of the quadratic form?,How to take the gradient of the quadratic form?,,"It's stated that the gradient of: $$\frac{1}{2}x^TAx - b^Tx +c$$ is $$\frac{1}{2}A^Tx + \frac{1}{2}Ax - b$$ How do you grind out this equation? Or specifically, how do you get from $x^TAx$ to $A^Tx + Ax$?","It's stated that the gradient of: $$\frac{1}{2}x^TAx - b^Tx +c$$ is $$\frac{1}{2}A^Tx + \frac{1}{2}Ax - b$$ How do you grind out this equation? Or specifically, how do you get from $x^TAx$ to $A^Tx + Ax$?",,"['multivariable-calculus', 'derivatives', 'matrix-calculus', 'quadratic-forms', 'scalar-fields']"
66,(Theoretical) Multivariable Calculus Textbooks [duplicate],(Theoretical) Multivariable Calculus Textbooks [duplicate],,"This question already has answers here : References for multivariable calculus (13 answers) Closed 10 years ago . (Note that I have used bold text frequently simply to highlight the key points of my question for those who do not have the time to read through it thoroughly (it is not very long, however); I hope this is not considered offensive.) There are many textbooks on multivariable calculus. However, some textbooks on multivariable calculus do not focus very much on the theoretical foundations of the subject. For example, a textbook might state a result along the lines of ""the order of partial differentiation is immaterial"" without proof and ask the student to use this rule to solve problems. Similarly, theorems such as those due to Green and Stokes are often not proved in their full generality. Therefore, I ask the following question: What are some good theoretical   multivariable calculus textbooks ? Since ""theoretical"" is somewhat ambiguous, let me state the following criteria which I would like a ""theoretical"" textbook on multivariable calculus to satisfy: The textbook should be rigorous and it should not state a theorem without proof if the theorem is proved in at least one other multivariable calculus textbook. (Of course, the textbook may omit certain theorems; however, this criterion at least ensures that major theorems in multivariable calculus are not stated without proof and used purely for the sake of computations. Also, this criterion permits the textbook to state an interesting theorem if it is beyond the scope of all multivariable calculus textbooks.) The textbook should be primarily based on developing the theoretical foundations of multivariable calculus; therefore, applications such as learning how to compute the partial derivative of a function, learning how to solve extremum problems, learning how to compute etc. should be kept to a minimum . In particular, the textbook can assume that the reader has already seen at least an informal treatment of the subject where these aspects are emphasized. The textbook should have a rigorous treatment of differentiability in $n$-dimensional Euclidean space (e.g., the inverse and implicit function theorems should be proven), Riemann integration in $n$-space , and differential forms (e.g., Stokes theorem should be proven). It would also be a bonus if the book treated the general concept of a manifold. Textbooks with minimal prerequisites are preferred ; however, please feel free to suggest books meeting the above criteria even if the prerequisites are quite demanding. Finally, it would also be preferable, but not essential, for the book to only treat multivariable calculus . Examples of books meeting the above criteria: ""Analysis on Manifolds"" by James Munkres, ""Principles of Mathematical Analysis"" by Walter Rudin, and ""Calculus on Manifolds"" by Michael Spivak. Although I have studied theoretical multivariable calculus already (four years ago), I could never find ""the perfect book"" (relative to myself, of course). Every book has its virtues; Rudin for its elegance, Munkres for its beautiful exposition, and Spivak for its ""quick and dirty"" approach. I am hoping that someone will be able to suggest a book that (relative to myself) is ""perfect"". Also, this question can be useful to other students who have not yet studied the subject and wish to learn it. Thank you very much for all answers! Please do feel free to suggest as many books as you can think of so we can form a big list. Also, please try to explain why a particular book is good or at least why you think it is good. I suppose it is fine to suggest a book that is already suggested provided you have a different view as to why the book is good.","This question already has answers here : References for multivariable calculus (13 answers) Closed 10 years ago . (Note that I have used bold text frequently simply to highlight the key points of my question for those who do not have the time to read through it thoroughly (it is not very long, however); I hope this is not considered offensive.) There are many textbooks on multivariable calculus. However, some textbooks on multivariable calculus do not focus very much on the theoretical foundations of the subject. For example, a textbook might state a result along the lines of ""the order of partial differentiation is immaterial"" without proof and ask the student to use this rule to solve problems. Similarly, theorems such as those due to Green and Stokes are often not proved in their full generality. Therefore, I ask the following question: What are some good theoretical   multivariable calculus textbooks ? Since ""theoretical"" is somewhat ambiguous, let me state the following criteria which I would like a ""theoretical"" textbook on multivariable calculus to satisfy: The textbook should be rigorous and it should not state a theorem without proof if the theorem is proved in at least one other multivariable calculus textbook. (Of course, the textbook may omit certain theorems; however, this criterion at least ensures that major theorems in multivariable calculus are not stated without proof and used purely for the sake of computations. Also, this criterion permits the textbook to state an interesting theorem if it is beyond the scope of all multivariable calculus textbooks.) The textbook should be primarily based on developing the theoretical foundations of multivariable calculus; therefore, applications such as learning how to compute the partial derivative of a function, learning how to solve extremum problems, learning how to compute etc. should be kept to a minimum . In particular, the textbook can assume that the reader has already seen at least an informal treatment of the subject where these aspects are emphasized. The textbook should have a rigorous treatment of differentiability in $n$-dimensional Euclidean space (e.g., the inverse and implicit function theorems should be proven), Riemann integration in $n$-space , and differential forms (e.g., Stokes theorem should be proven). It would also be a bonus if the book treated the general concept of a manifold. Textbooks with minimal prerequisites are preferred ; however, please feel free to suggest books meeting the above criteria even if the prerequisites are quite demanding. Finally, it would also be preferable, but not essential, for the book to only treat multivariable calculus . Examples of books meeting the above criteria: ""Analysis on Manifolds"" by James Munkres, ""Principles of Mathematical Analysis"" by Walter Rudin, and ""Calculus on Manifolds"" by Michael Spivak. Although I have studied theoretical multivariable calculus already (four years ago), I could never find ""the perfect book"" (relative to myself, of course). Every book has its virtues; Rudin for its elegance, Munkres for its beautiful exposition, and Spivak for its ""quick and dirty"" approach. I am hoping that someone will be able to suggest a book that (relative to myself) is ""perfect"". Also, this question can be useful to other students who have not yet studied the subject and wish to learn it. Thank you very much for all answers! Please do feel free to suggest as many books as you can think of so we can form a big list. Also, please try to explain why a particular book is good or at least why you think it is good. I suppose it is fine to suggest a book that is already suggested provided you have a different view as to why the book is good.",,"['multivariable-calculus', 'reference-request', 'book-recommendation', 'big-list']"
67,How would you discover Stokes's theorem?,How would you discover Stokes's theorem?,,"Let $S$ be a smooth oriented surface in $\mathbb R^3$ with boundary $C$ , and let $f: \mathbb R^3 \to \mathbb R^3$ be a continuously differentiable vector field on $\mathbb R^3$ . Stokes's theorem states that $$ \int_C f \cdot dr = \int_S (\nabla \times f) \cdot dA. $$ In other words, the line integral of $f$ over the curve $C$ is equal to the integral of the curl of $f$ over the surface $S$ . Here the orientation of the boundary $C$ is induced by the orientation of $S$ . Question: How might somebody have derived or discovered this formula? Where does this formula come from? The goal is to provide an intuitive explanation of Stokes's theorem, rather than a rigorous proof. (I'll post an answer below.)","Let be a smooth oriented surface in with boundary , and let be a continuously differentiable vector field on . Stokes's theorem states that In other words, the line integral of over the curve is equal to the integral of the curl of over the surface . Here the orientation of the boundary is induced by the orientation of . Question: How might somebody have derived or discovered this formula? Where does this formula come from? The goal is to provide an intuitive explanation of Stokes's theorem, rather than a rigorous proof. (I'll post an answer below.)","S \mathbb R^3 C f: \mathbb R^3 \to \mathbb R^3 \mathbb R^3 
\int_C f \cdot dr = \int_S (\nabla \times f) \cdot dA.
 f C f S C S","['multivariable-calculus', 'stokes-theorem']"
68,Gradient of a dot product,Gradient of a dot product,,"The wikipedia formula for the gradient of a dot product is given as $$\nabla(a\cdot b) = (a\cdot\nabla)b +(b\cdot \nabla)a + a\times(\nabla\times b)+ b\times (\nabla \times a)$$ However, I also found the formula $$\nabla(a\cdot b) = (\nabla a)\cdot b + (\nabla b)\cdot a  $$ So... what is going on here?  The second formula seems much easier.  Are these equivalent?","The wikipedia formula for the gradient of a dot product is given as $$\nabla(a\cdot b) = (a\cdot\nabla)b +(b\cdot \nabla)a + a\times(\nabla\times b)+ b\times (\nabla \times a)$$ However, I also found the formula $$\nabla(a\cdot b) = (\nabla a)\cdot b + (\nabla b)\cdot a  $$ So... what is going on here?  The second formula seems much easier.  Are these equivalent?",,"['multivariable-calculus', 'vector-analysis']"
69,Are there other kinds of bump functions than $e^\frac1{x^2-1}$?,Are there other kinds of bump functions than ?,e^\frac1{x^2-1},"I've only seen the bump function $e^\frac1{x^2-1}$ so far.  Where could I find examples of functions $C^∞$ on $\mathbb{R}$ that are zero everywhere except on $(-1,1)$? Are there others that do not involve the exponential function?  Are there any with a closed form integral?  Is there a preferred function?","I've only seen the bump function $e^\frac1{x^2-1}$ so far.  Where could I find examples of functions $C^∞$ on $\mathbb{R}$ that are zero everywhere except on $(-1,1)$? Are there others that do not involve the exponential function?  Are there any with a closed form integral?  Is there a preferred function?",,"['multivariable-calculus', 'big-list', 'smooth-functions']"
70,Geometric interpretation of mixed partial derivatives?,Geometric interpretation of mixed partial derivatives?,,"I'm looking for a geometric interpretation of this theorem: My book doesn't give any kind of explanation of it. Again, I'm not looking for a proof - I'm looking for a geometric interpretation. Thanks.","I'm looking for a geometric interpretation of this theorem: My book doesn't give any kind of explanation of it. Again, I'm not looking for a proof - I'm looking for a geometric interpretation. Thanks.",,"['multivariable-calculus', 'derivatives']"
71,What is a differential form?,What is a differential form?,,"can someone please informally (but intuitively) explain what ""differential form"" mean? I know that there is (of course) some formalism behind it - definition and possible operations with differential forms, but what is the motivation of introducing and using this object (differential form)? I have heard that they somehow generalize integration, are used for integration of manifolds and can evaluate k-dimensional integrals in n-dimensional space ( $k \leq n$ ), but is it really true and is it the main motivation of introducing this object into mathematics? Thank you for explanation","can someone please informally (but intuitively) explain what ""differential form"" mean? I know that there is (of course) some formalism behind it - definition and possible operations with differential forms, but what is the motivation of introducing and using this object (differential form)? I have heard that they somehow generalize integration, are used for integration of manifolds and can evaluate k-dimensional integrals in n-dimensional space ( ), but is it really true and is it the main motivation of introducing this object into mathematics? Thank you for explanation",k \leq n,['multivariable-calculus']
72,What is an intuitive explanation for $\operatorname{div} \operatorname{curl} F = 0$?,What is an intuitive explanation for ?,\operatorname{div} \operatorname{curl} F = 0,"I am aware of an intuitive explanation  for $\operatorname{curl} \operatorname{grad} F = 0$ (a block placed on a mountainous frictionless surface will slide to lower ground without spinning), and was wondering if there were a similar explanation for $\operatorname{div} \operatorname{curl} F = 0$.","I am aware of an intuitive explanation  for $\operatorname{curl} \operatorname{grad} F = 0$ (a block placed on a mountainous frictionless surface will slide to lower ground without spinning), and was wondering if there were a similar explanation for $\operatorname{div} \operatorname{curl} F = 0$.",,"['multivariable-calculus', 'intuition']"
73,What does it mean to take the gradient of a vector field?,What does it mean to take the gradient of a vector field?,,"What does it mean to take the gradient of a vector field? $\nabla \vec{v}(x,y,z)$ ? I only understand what it means to take the grad of a scalar field.",What does it mean to take the gradient of a vector field? ? I only understand what it means to take the grad of a scalar field.,"\nabla \vec{v}(x,y,z)","['multivariable-calculus', 'vector-fields']"
74,Is there a vector field that is equal to its own curl?,Is there a vector field that is equal to its own curl?,,I was wondering if there is a vector field that satisfies the following condition: $$\vec F=\nabla \times \vec F$$,I was wondering if there is a vector field that satisfies the following condition: $$\vec F=\nabla \times \vec F$$,,"['multivariable-calculus', 'partial-differential-equations']"
75,What does a triple integral represent?,What does a triple integral represent?,,"From my understanding if the integrand is 1, then it gives you the volume of the region defined by the bounds. But what does the value of a triple integral represent if the integrand is a function for a surface in space?","From my understanding if the integrand is 1, then it gives you the volume of the region defined by the bounds. But what does the value of a triple integral represent if the integrand is a function for a surface in space?",,['multivariable-calculus']
76,Proof for the curl of a curl of a vector field,Proof for the curl of a curl of a vector field,,"For a vector field $\textbf{A}$, the curl of the curl is defined by $$\nabla\times\left(\nabla\times\textbf{A}\right)=\nabla\left(\nabla\cdot\textbf{A}\right)-\nabla^2\textbf{A}$$ where $\nabla$ is the usual del operator and $\nabla^2$ is the vector Laplacian. How can I prove this relation? I tried the ugly/unefficient/brute-force method, by getting an expression for the LHS and the RHS for an arbitrary vector field $$\textbf{A}=\left(a(x,y,z),b(x,y,z),c(x,y,z)\right)$$ It does work (duh), but is there a more elegant way of doing this? Using matrix notation maybe? EDIT: I got very good answers, from various perspectives. I would say @Spencer's derivation is the one I was looking for, using Einstein notation - and as a physics student, this was very helpful. However, @Vectornaut's solution not only is short and elegant, but it also introduced me to a whole new range of mathematics - and as a theoretical physics student, I appreciate learning new mathematical theories and trying to see how we can use them in physics.","For a vector field $\textbf{A}$, the curl of the curl is defined by $$\nabla\times\left(\nabla\times\textbf{A}\right)=\nabla\left(\nabla\cdot\textbf{A}\right)-\nabla^2\textbf{A}$$ where $\nabla$ is the usual del operator and $\nabla^2$ is the vector Laplacian. How can I prove this relation? I tried the ugly/unefficient/brute-force method, by getting an expression for the LHS and the RHS for an arbitrary vector field $$\textbf{A}=\left(a(x,y,z),b(x,y,z),c(x,y,z)\right)$$ It does work (duh), but is there a more elegant way of doing this? Using matrix notation maybe? EDIT: I got very good answers, from various perspectives. I would say @Spencer's derivation is the one I was looking for, using Einstein notation - and as a physics student, this was very helpful. However, @Vectornaut's solution not only is short and elegant, but it also introduced me to a whole new range of mathematics - and as a theoretical physics student, I appreciate learning new mathematical theories and trying to see how we can use them in physics.",,"['multivariable-calculus', 'vector-analysis', 'vector-fields', 'curl']"
77,Taking a derivative with respect to a matrix,Taking a derivative with respect to a matrix,,"I'm studying about EM-algorithm and on one point in my reference the author is taking a derivative of a function  with respect to a matrix. Could someone explain how does one take the derivative of a function with respect to a matrix...I don't understand the idea. For example, lets say we have a multidimensional Gaussian function: $$f(\textbf{x}, \Sigma, \boldsymbol \mu) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}}\exp\left( -\frac{1}{2}(\textbf{x}-\boldsymbol \mu)^T\Sigma^{-1}(\textbf{x}-\boldsymbol \mu)\right),$$ where $\textbf{x} = (x_1, ..., x_n)$,  $\;\;x_i \in \mathbb R$, $\;\;\boldsymbol \mu = (\mu_1, ..., \mu_n)$, $\;\;\mu_i \in \mathbb R$ and $\Sigma$ is the $n\times n$ covariance matrix. How would one calculate $\displaystyle \frac{\partial f}{\partial \Sigma}$? What about $\displaystyle \frac{\partial f}{\partial \boldsymbol \mu}$ or $\displaystyle \frac{\partial f}{\partial \textbf{x}}$ (Aren't these two actually just special cases of the first one)? Thnx for any help. If you're wondering where I got this question in my mind, I got it from reading this reference: (page 14) http://ptgmedia.pearsoncmg.com/images/0131478249/samplechapter/0131478249_ch03.pdf UPDATE: I added the particular part from my reference here if someone is interested :) I highlighted the parts where I got confused, namely the part where the author takes the derivative with respect to a matrix (the sigma in the picture is also a covariance matrix. The author is estimating the optimal parameters for Gaussian mixture model, by using the EM-algorithm): $Q(\theta|\theta_n)\equiv E_Z\{\log p(Z,X|\theta)|X,\theta_n\}$","I'm studying about EM-algorithm and on one point in my reference the author is taking a derivative of a function  with respect to a matrix. Could someone explain how does one take the derivative of a function with respect to a matrix...I don't understand the idea. For example, lets say we have a multidimensional Gaussian function: $$f(\textbf{x}, \Sigma, \boldsymbol \mu) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}}\exp\left( -\frac{1}{2}(\textbf{x}-\boldsymbol \mu)^T\Sigma^{-1}(\textbf{x}-\boldsymbol \mu)\right),$$ where $\textbf{x} = (x_1, ..., x_n)$,  $\;\;x_i \in \mathbb R$, $\;\;\boldsymbol \mu = (\mu_1, ..., \mu_n)$, $\;\;\mu_i \in \mathbb R$ and $\Sigma$ is the $n\times n$ covariance matrix. How would one calculate $\displaystyle \frac{\partial f}{\partial \Sigma}$? What about $\displaystyle \frac{\partial f}{\partial \boldsymbol \mu}$ or $\displaystyle \frac{\partial f}{\partial \textbf{x}}$ (Aren't these two actually just special cases of the first one)? Thnx for any help. If you're wondering where I got this question in my mind, I got it from reading this reference: (page 14) http://ptgmedia.pearsoncmg.com/images/0131478249/samplechapter/0131478249_ch03.pdf UPDATE: I added the particular part from my reference here if someone is interested :) I highlighted the parts where I got confused, namely the part where the author takes the derivative with respect to a matrix (the sigma in the picture is also a covariance matrix. The author is estimating the optimal parameters for Gaussian mixture model, by using the EM-algorithm): $Q(\theta|\theta_n)\equiv E_Z\{\log p(Z,X|\theta)|X,\theta_n\}$",,"['multivariable-calculus', 'derivatives', 'normal-distribution', 'matrix-calculus', 'scalar-fields']"
78,Stronger than Nesbitt inequality,Stronger than Nesbitt inequality,,"For $x,y,z >0$, prove that   $$\frac{x}{y+z}+\frac{y}{z+x}+\frac{z}{x+y} \geqslant \sqrt{\frac94+\frac32 \cdot \frac{(y-z)^2}{xy+yz+zx}}$$ Observation: This inequality is stronger than the famous Nesbitt's Inequality $$\frac{x}{y+z}+\frac{y}{z+x}+\frac{z}{x+y} \geqslant \frac32 $$ for positive $x,y,z$ We have three variables but the symmetry holds only for two variables $y,z$, resulting in a very difficult inequality. Brute force and Largrange Multiplier are too complicated. The constant $\frac32$ is closed to the best constant. Thus, this inequality is very sharp, simple AM-GM estimation did not work. Update: As point out by Michael Rozenberg, this inequality is still unsolved","For $x,y,z >0$, prove that   $$\frac{x}{y+z}+\frac{y}{z+x}+\frac{z}{x+y} \geqslant \sqrt{\frac94+\frac32 \cdot \frac{(y-z)^2}{xy+yz+zx}}$$ Observation: This inequality is stronger than the famous Nesbitt's Inequality $$\frac{x}{y+z}+\frac{y}{z+x}+\frac{z}{x+y} \geqslant \frac32 $$ for positive $x,y,z$ We have three variables but the symmetry holds only for two variables $y,z$, resulting in a very difficult inequality. Brute force and Largrange Multiplier are too complicated. The constant $\frac32$ is closed to the best constant. Thus, this inequality is very sharp, simple AM-GM estimation did not work. Update: As point out by Michael Rozenberg, this inequality is still unsolved",,"['multivariable-calculus', 'inequality']"
79,What does it mean to multiply differentials?,What does it mean to multiply differentials?,,"Often times in multi-variable calculus you would have expressions for the differentials of area and volume like this $dA =dxdy$ or $dV = dxdydz$  which we are supposed to just accept because it makes sense in that if you take a tiny piece of area/volume it looks like a square blah blah....But  cannot get my head around it especially when doing integral substitutions. In single variable calculus it made sense, for example consider the integral $$\int f'(g) \frac{dg}{dx} dx = \int f'(g)dg = f(x) + c$$ this makes sense as $f'(x) = g'(x)f'(g)$ by the chain rule but when preforming a substitution in a double integral  $$\int \int f(x,y) dx dy = \int \int f(u,v) \det(J)du dv$$ where $$J = \frac{\partial(x,y)}{\partial(u,v)} = \begin{pmatrix} \frac{\partial x}{\partial u} &  \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u}    & \frac{\partial y}{\partial v}\end{pmatrix}$$ How on earth have they magically jumped and concluded that $dxdy$ (which itself is not explained) equal to $\det(J)dudv$ ? This has caused me so many problems conceptually and i was not able to find any clear explanation to why this is the case and even what is meant by the term $dxdy$","Often times in multi-variable calculus you would have expressions for the differentials of area and volume like this $dA =dxdy$ or $dV = dxdydz$  which we are supposed to just accept because it makes sense in that if you take a tiny piece of area/volume it looks like a square blah blah....But  cannot get my head around it especially when doing integral substitutions. In single variable calculus it made sense, for example consider the integral $$\int f'(g) \frac{dg}{dx} dx = \int f'(g)dg = f(x) + c$$ this makes sense as $f'(x) = g'(x)f'(g)$ by the chain rule but when preforming a substitution in a double integral  $$\int \int f(x,y) dx dy = \int \int f(u,v) \det(J)du dv$$ where $$J = \frac{\partial(x,y)}{\partial(u,v)} = \begin{pmatrix} \frac{\partial x}{\partial u} &  \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u}    & \frac{\partial y}{\partial v}\end{pmatrix}$$ How on earth have they magically jumped and concluded that $dxdy$ (which itself is not explained) equal to $\det(J)dudv$ ? This has caused me so many problems conceptually and i was not able to find any clear explanation to why this is the case and even what is meant by the term $dxdy$",,"['multivariable-calculus', 'differential-forms']"
80,How to obtain the gradient in polar coordinates,How to obtain the gradient in polar coordinates,,I'm not sure on how to find the gradient in polar coordinates. The thing that troubles me the most is how to find the unit vectors $\hat{r}$ and $\hat{\theta}$. My approach for the rest is expressing the partial derivatives in respect of $r$ and $\theta$ using the chain rule. How can I get around solving this problem?,I'm not sure on how to find the gradient in polar coordinates. The thing that troubles me the most is how to find the unit vectors $\hat{r}$ and $\hat{\theta}$. My approach for the rest is expressing the partial derivatives in respect of $r$ and $\theta$ using the chain rule. How can I get around solving this problem?,,"['multivariable-calculus', 'polar-coordinates']"
81,Why gradient vector is perpendicular to the plane,Why gradient vector is perpendicular to the plane,,"I know what gradient vector or $\nabla F$ is and I know how to prove that it is orthogonal to the surface (using calculation - not intuitive). In a particular case, in which we have a three variable function, I want to know why the gradient vector is perpendicular as mentioned. I mean, not in theoretical terms, but intuitive.","I know what gradient vector or $\nabla F$ is and I know how to prove that it is orthogonal to the surface (using calculation - not intuitive). In a particular case, in which we have a three variable function, I want to know why the gradient vector is perpendicular as mentioned. I mean, not in theoretical terms, but intuitive.",,['multivariable-calculus']
82,Notation for partial derivative of functions of functions,Notation for partial derivative of functions of functions,,"Say we have a function $f(x(t),t)$ and we take the partial derivative of $f(x(t),t)$ with respect to $t$ $$\frac{\partial f(x(t),t)}{\partial t}$$ Do we hold $x(t)$ constant? For example, if I had $f(x(t),t) = x(t)^2 + t$ where $x(t)=t^2$ , I believe $\frac{\partial f(x(t),t)}{\partial x(t)}=2x(t)$ but  does $\frac{\partial f(x(t),t)}{\partial t}=4t+1 $ or $1$ ? To further my understanding, is the following true: $$ \frac{\partial f(x(t),t)}{\partial t}=\frac{df(x(t),t)}{dt}$$ because $f(x(t),t)=f(t)$ after you simplify it by substituting in $x(t)$ into $f(x(t),t)$ ? But if this is true, in the above example, $\frac{\partial f(x(t),t)}{\partial x(t)}=0$ because $f(x(t),t)=f(t)$ doesn't actually depend on $x(t)$ . I see that my understanding is wrong but I can't figure out where. Any insight appreciated, thanks in advance!","Say we have a function and we take the partial derivative of with respect to Do we hold constant? For example, if I had where , I believe but  does or ? To further my understanding, is the following true: because after you simplify it by substituting in into ? But if this is true, in the above example, because doesn't actually depend on . I see that my understanding is wrong but I can't figure out where. Any insight appreciated, thanks in advance!","f(x(t),t) f(x(t),t) t \frac{\partial f(x(t),t)}{\partial t} x(t) f(x(t),t) = x(t)^2 + t x(t)=t^2 \frac{\partial f(x(t),t)}{\partial x(t)}=2x(t) \frac{\partial f(x(t),t)}{\partial t}=4t+1  1  \frac{\partial f(x(t),t)}{\partial t}=\frac{df(x(t),t)}{dt} f(x(t),t)=f(t) x(t) f(x(t),t) \frac{\partial f(x(t),t)}{\partial x(t)}=0 f(x(t),t)=f(t) x(t)","['multivariable-calculus', 'derivatives', 'partial-derivative']"
83,Derivative of the $2$-norm of a multivariate function,Derivative of the -norm of a multivariate function,2,"I've got a function $$g(x,y) = \| f(x,y) \|_2$$ and I want to calculate its derivatives with respect to $x$ and $y$ . Using Mathematica, differentiating w.r.t. $x$ gives me $ f'_x(x,y) \text{Norm}'( f(x,y))$ , where Norm is $\| \cdot \|$ . I read here that $$d\|{\bf x}\| = \frac{ {\bf x}^Td{\bf x}}{\|{\bf x}\|}$$ at least for the $2$ -norm. Point is, as inside the norm I have a multivariate function, I'm still confused on how to calculate $ f'_x(x,y) \text{Norm}'( f(x,y))$ I think it should be $f'_x(x,y) \frac{f(x,y)}{||f(x,y)||}$ , but some verification would be great :)","I've got a function and I want to calculate its derivatives with respect to and . Using Mathematica, differentiating w.r.t. gives me , where Norm is . I read here that at least for the -norm. Point is, as inside the norm I have a multivariate function, I'm still confused on how to calculate I think it should be , but some verification would be great :)","g(x,y) = \| f(x,y) \|_2 x y x  f'_x(x,y) \text{Norm}'( f(x,y)) \| \cdot \| d\|{\bf x}\| = \frac{ {\bf x}^Td{\bf x}}{\|{\bf x}\|} 2  f'_x(x,y) \text{Norm}'( f(x,y)) f'_x(x,y) \frac{f(x,y)}{||f(x,y)||}","['multivariable-calculus', 'derivatives', 'partial-derivative']"
84,"The connection between the Jacobian, Hessian and the gradient?","The connection between the Jacobian, Hessian and the gradient?",,"In this Wikipedia article they have this to say about the gradient: If $m = 1$, $\mathbf{f}$ is a scalar field and the Jacobian matrix is reduced to a row vector of partial derivatives of $\mathbf{f}$—i.e. the gradient of $\mathbf{f}$. As well as The Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix, which in a sense is the ""second derivative"" of the function in question. So I tried doing the calculations, and was stumped. If we let $f: \mathbb{R}^n \to \mathbb{R}$, then  $$Df = \begin{bmatrix} \frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n} \end{bmatrix} = \nabla f$$ So far so good, but when I try to calculate the Jacobian matrix of the gradient I get $$D^2f = \begin{bmatrix}  \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_2 x_1} & \dots & \frac{\partial^2 f}{\partial x_n x_1} \\ \frac{\partial^2 f}{\partial x_1 x_2} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_n x_2} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_1 x_n} & \frac{\partial^2 f}{\partial x_2 x_n} & \dots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}$$ Which according to this article, is not equal to the Hessian matrix but rather its transpose, and from what I can gather the Hessian is not generally symmetric. So I have two questions, is the gradient generally thought of as a row vector? And did I do something wrong when I calculated the Jacobian of the gradient of $f$, or is the Wikipedia article incorrect?","In this Wikipedia article they have this to say about the gradient: If $m = 1$, $\mathbf{f}$ is a scalar field and the Jacobian matrix is reduced to a row vector of partial derivatives of $\mathbf{f}$—i.e. the gradient of $\mathbf{f}$. As well as The Jacobian of the gradient of a scalar function of several variables has a special name: the Hessian matrix, which in a sense is the ""second derivative"" of the function in question. So I tried doing the calculations, and was stumped. If we let $f: \mathbb{R}^n \to \mathbb{R}$, then  $$Df = \begin{bmatrix} \frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n} \end{bmatrix} = \nabla f$$ So far so good, but when I try to calculate the Jacobian matrix of the gradient I get $$D^2f = \begin{bmatrix}  \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_2 x_1} & \dots & \frac{\partial^2 f}{\partial x_n x_1} \\ \frac{\partial^2 f}{\partial x_1 x_2} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_n x_2} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_1 x_n} & \frac{\partial^2 f}{\partial x_2 x_n} & \dots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}$$ Which according to this article, is not equal to the Hessian matrix but rather its transpose, and from what I can gather the Hessian is not generally symmetric. So I have two questions, is the gradient generally thought of as a row vector? And did I do something wrong when I calculated the Jacobian of the gradient of $f$, or is the Wikipedia article incorrect?",,"['multivariable-calculus', 'jacobian', 'hessian-matrix']"
85,Surface Element in Spherical Coordinates,Surface Element in Spherical Coordinates,,"In spherical polars,  $$x=r\cos(\phi)\sin(\theta)$$ $$y=r\sin(\phi)\sin(\theta)$$ $$z=r\cos(\theta)$$ I want to work out an integral over the surface of a sphere - ie $r$ constant. I'm able to derive through scale factors, ie $\delta(s)^2=h_1^2\delta(\theta)^2+h_2^2\delta(\phi)^2$ (note $\delta(r)=0$), that: $$h_1=r\sin(\theta),h_2=r$$ $$dA=h_1h_2=r^2\sin(\theta)$$ I'm just wondering is there an ""easier"" way to do this (eg. Jacobian determinant when I'm varying all 3 variables). I know you can supposedly visualize a change of area on the surface of the sphere, but I'm not particularly good at doing that sadly.","In spherical polars,  $$x=r\cos(\phi)\sin(\theta)$$ $$y=r\sin(\phi)\sin(\theta)$$ $$z=r\cos(\theta)$$ I want to work out an integral over the surface of a sphere - ie $r$ constant. I'm able to derive through scale factors, ie $\delta(s)^2=h_1^2\delta(\theta)^2+h_2^2\delta(\phi)^2$ (note $\delta(r)=0$), that: $$h_1=r\sin(\theta),h_2=r$$ $$dA=h_1h_2=r^2\sin(\theta)$$ I'm just wondering is there an ""easier"" way to do this (eg. Jacobian determinant when I'm varying all 3 variables). I know you can supposedly visualize a change of area on the surface of the sphere, but I'm not particularly good at doing that sadly.",,"['differential-geometry', 'multivariable-calculus', 'vector-analysis', 'spherical-coordinates']"
86,Can the curl operator be generalized to non-3D?,Can the curl operator be generalized to non-3D?,,"In three dimensions, the curl operator $\newcommand{curl}{\operatorname{curl}}\curl = \vec\nabla\times$ fulfils the equations $$\curl^2 = \newcommand{grad}{\operatorname{grad}}\renewcommand{div}{\operatorname{div}}\grad\div-\Delta,\\   \curl\grad = 0,\\   \div\curl = 0 $$ where $\Delta$ denotes the (vector) Laplacian $\nabla^2$. Since none of these equations requires the cross-product, which is only defined in three dimensions, can they be used to generalize the curl operator to an arbitrary $d$-dimensional space? I know taking the root of an operator is not exactly a funny thing, but Dirac has managed that before , even if it led to requiring anticommuting Grassman numbers and spinors... So my questions are: In which dimensions $d$ is $\curl :=+\sqrt{\grad\div-\Delta}$ uniquely defined (by the additional constraints mentioned above, or maybe including other properties of the 3D $\curl$)? In which of these dimensions does this work for simple complex numbers without requiring the introduction of spinors? How to actually calculate it? Bonus question: Use this generalization to generalize the cross product Note to answerers: I declared the operators \curl , \grad and \div for convenience, they should work everywhere below the question. As an explicit example, observe $n=2$: Claiming that $\curl = \begin{pmatrix}a&b\\c&d\end{pmatrix}$, use $\curl\grad=0$ and $\div\curl=0$ to obtain $$\curl = \alpha\begin{pmatrix}\partial_y^2 & -\partial_x\partial_y \\ -\partial_x\partial_y & \partial_x^2\end{pmatrix}$$ the square of which is $$\curl^2=\alpha^2\begin{pmatrix}\partial_y^2\Delta & -\partial_x\partial_y\Delta\\ -\partial_x\partial_y\Delta & \partial_x^2\Delta\end{pmatrix} \stackrel!= \begin{pmatrix}-\partial_y^2 & \partial_x\partial_y\\\partial_x\partial_y & -\partial_x^2\end{pmatrix} = \grad\div-\Delta$$ So $\alpha^2 \stackrel!= -\Delta^{-1}$ and formally $$\curl = \begin{pmatrix}\partial_y^2 & -\partial_x\partial_y\\ -\partial_x\partial_y & \partial_x^2\end{pmatrix}\otimes(\sqrt{-\Delta})^{-1}$$ $\sqrt\Delta$ requires Spinors, as feared, and I assume they will arise for all even dimensions. One interesting question is, are usual vectors enough in the odd dimensional extension? But doing this manually with even just a $5\times5$ matrix would be a bit too tedious...","In three dimensions, the curl operator $\newcommand{curl}{\operatorname{curl}}\curl = \vec\nabla\times$ fulfils the equations $$\curl^2 = \newcommand{grad}{\operatorname{grad}}\renewcommand{div}{\operatorname{div}}\grad\div-\Delta,\\   \curl\grad = 0,\\   \div\curl = 0 $$ where $\Delta$ denotes the (vector) Laplacian $\nabla^2$. Since none of these equations requires the cross-product, which is only defined in three dimensions, can they be used to generalize the curl operator to an arbitrary $d$-dimensional space? I know taking the root of an operator is not exactly a funny thing, but Dirac has managed that before , even if it led to requiring anticommuting Grassman numbers and spinors... So my questions are: In which dimensions $d$ is $\curl :=+\sqrt{\grad\div-\Delta}$ uniquely defined (by the additional constraints mentioned above, or maybe including other properties of the 3D $\curl$)? In which of these dimensions does this work for simple complex numbers without requiring the introduction of spinors? How to actually calculate it? Bonus question: Use this generalization to generalize the cross product Note to answerers: I declared the operators \curl , \grad and \div for convenience, they should work everywhere below the question. As an explicit example, observe $n=2$: Claiming that $\curl = \begin{pmatrix}a&b\\c&d\end{pmatrix}$, use $\curl\grad=0$ and $\div\curl=0$ to obtain $$\curl = \alpha\begin{pmatrix}\partial_y^2 & -\partial_x\partial_y \\ -\partial_x\partial_y & \partial_x^2\end{pmatrix}$$ the square of which is $$\curl^2=\alpha^2\begin{pmatrix}\partial_y^2\Delta & -\partial_x\partial_y\Delta\\ -\partial_x\partial_y\Delta & \partial_x^2\Delta\end{pmatrix} \stackrel!= \begin{pmatrix}-\partial_y^2 & \partial_x\partial_y\\\partial_x\partial_y & -\partial_x^2\end{pmatrix} = \grad\div-\Delta$$ So $\alpha^2 \stackrel!= -\Delta^{-1}$ and formally $$\curl = \begin{pmatrix}\partial_y^2 & -\partial_x\partial_y\\ -\partial_x\partial_y & \partial_x^2\end{pmatrix}\otimes(\sqrt{-\Delta})^{-1}$$ $\sqrt\Delta$ requires Spinors, as feared, and I assume they will arise for all even dimensions. One interesting question is, are usual vectors enough in the odd dimensional extension? But doing this manually with even just a $5\times5$ matrix would be a bit too tedious...",,"['differential-geometry', 'multivariable-calculus']"
87,"Del. $\partial, \delta, \nabla $: Correct enunciation",Del. : Correct enunciation,"\partial, \delta, \nabla ","I've come across various different symbols being pronounced as ""del"". What is the internationally accepted del? If not internationally, then what's the English/American(specify which one if they are different) one that most lecturers/&c use? $\partial$: I have heard $\frac{\partial}{\partial x}$ being called ""del by del x"", and (rarely) ""dou by dou x"" and ""der by der x"". $\partial$ can be used without a fraction (einstein notation), in which case it gets confusing. $\nabla$: Called Nabla or del. This has four different uses, which can be easily distinguished while reading out loud, but it gets confusing when the first and last uses (grad and covariant derivative) get mixed up with $\partial$ and $\delta$ Gradient/grad: $\vec{\nabla}\phi$ (phi is a scalar). Read as ""nabla phi"", or ""del phi"". Divergence/div: $\vec{\nabla}\cdot\vec{v}$ Pretty clear, can be read as ""nabla dot"" or ""del dot"" Curl/rot: $\vec{\nabla}\times\vec{v}$ Also clear, can be read as ""nabla cross"" or ""del cross"" Covariant derivative: $\vec{\nabla}_{\vec{u}}\vec{v}$: Can be read as ""del v"" or ""nabla v"" . I've seen it called ""del u v"" also. $\delta$ : Read out as ""delta"", but I've heard it used as ""del"" as well. This entire thing has confused me. My questions are: Which one can be correctly called ""del""? I'm fine with div/curl being read out as del, as the dot/cross can be read out as well. The confusion is between the convariant derivative, grad, partial derivative, and lowercase delta. Or is it just a matter of context? Where did this confusing terminology come from in the first place? Why name something del when we already have a bunch of other dels? A timeline of the dels would be appreciated, but not necessary :-).","I've come across various different symbols being pronounced as ""del"". What is the internationally accepted del? If not internationally, then what's the English/American(specify which one if they are different) one that most lecturers/&c use? $\partial$: I have heard $\frac{\partial}{\partial x}$ being called ""del by del x"", and (rarely) ""dou by dou x"" and ""der by der x"". $\partial$ can be used without a fraction (einstein notation), in which case it gets confusing. $\nabla$: Called Nabla or del. This has four different uses, which can be easily distinguished while reading out loud, but it gets confusing when the first and last uses (grad and covariant derivative) get mixed up with $\partial$ and $\delta$ Gradient/grad: $\vec{\nabla}\phi$ (phi is a scalar). Read as ""nabla phi"", or ""del phi"". Divergence/div: $\vec{\nabla}\cdot\vec{v}$ Pretty clear, can be read as ""nabla dot"" or ""del dot"" Curl/rot: $\vec{\nabla}\times\vec{v}$ Also clear, can be read as ""nabla cross"" or ""del cross"" Covariant derivative: $\vec{\nabla}_{\vec{u}}\vec{v}$: Can be read as ""del v"" or ""nabla v"" . I've seen it called ""del u v"" also. $\delta$ : Read out as ""delta"", but I've heard it used as ""del"" as well. This entire thing has confused me. My questions are: Which one can be correctly called ""del""? I'm fine with div/curl being read out as del, as the dot/cross can be read out as well. The confusion is between the convariant derivative, grad, partial derivative, and lowercase delta. Or is it just a matter of context? Where did this confusing terminology come from in the first place? Why name something del when we already have a bunch of other dels? A timeline of the dels would be appreciated, but not necessary :-).",,"['multivariable-calculus', 'terminology']"
88,Why do we care about differential forms? (Confused over construction),Why do we care about differential forms? (Confused over construction),,"So it's said that differential forms provide a coordinate free approach to multivariable calculus. Well, in short I just don't get this, despite reading from many sources. I shall explain how it all looks to me. Let's just stick to $\mathbb{R}^2$ for the sake of simplicity (maybe this is the down fall..). Picking some point $P=(x,y)\in\mathbb{R}^2$, we could ask about the directional derivative of some function $f:\mathbb{R}^2\rightarrow \mathbb{R}$, in direction $v=a\vec{i}+b\vec{j}$.  This will be $$(\nabla \cdot v)|_P(f)=a\dfrac{\partial f}{\partial x}|_P+b\dfrac{\partial f}{\partial y}|_P =\underbrace{ (a\dfrac{\partial }{\partial x}|_P+b\dfrac{\partial }{\partial y}|_P)}_\text{$w_P$ }(f)$$ Where we can think of $w_P$ as an element of the tangent space at $P$. Now this in itself is a little weird; why have differential operators as a basis for something geometrical like a tangent space to a manifold? In any case, we apply these vectors to a function defined on our manifold, and we get the value we wanted out. So who cares about differential forms? We just did all this without them. We could've done this by calculating $\mathrm{d}f$, in some basis $\mathrm{d}x, \mathrm{d}y$ (which is quite confusing), and then calculating $\mathrm{d}f(w_P)$, but what do we gain in doing it this way? I mentioned I think the $\mathrm{d}x$'s are confusing. Well, $\mathrm{d}x$ is just the function $\mathrm{d}x(\frac{\partial}{\partial x})=1$ and 0 for any other differential operator -  why write this as $\mathrm{d}x$, which had always previously been used to mean an infinitesimal increase in x? Now I can understand caring about the dual of the tangent space. We are combining a vector in the tangent space with something and we're getting a scalar out of it - this something should then probably belong to the dual space. But if we're thinking of just the vector, then the function $f$ on the manifold needs to be encoded by the 1-form, right? Well, we can have 1-forms which aren't derivatives of any function on the manifold - what should it mean to combine such forms with tangent vectors? And lastly, if we're writing all our forms in terms of $\mathrm{d}x$'s etc., where the $x$'s are exactly the coordinates of the manifold, then how exactly have we escaped dependence on coordinates? We're still essentially calculating with respect to a given coordinate system as in usual multivariable calculus!","So it's said that differential forms provide a coordinate free approach to multivariable calculus. Well, in short I just don't get this, despite reading from many sources. I shall explain how it all looks to me. Let's just stick to $\mathbb{R}^2$ for the sake of simplicity (maybe this is the down fall..). Picking some point $P=(x,y)\in\mathbb{R}^2$, we could ask about the directional derivative of some function $f:\mathbb{R}^2\rightarrow \mathbb{R}$, in direction $v=a\vec{i}+b\vec{j}$.  This will be $$(\nabla \cdot v)|_P(f)=a\dfrac{\partial f}{\partial x}|_P+b\dfrac{\partial f}{\partial y}|_P =\underbrace{ (a\dfrac{\partial }{\partial x}|_P+b\dfrac{\partial }{\partial y}|_P)}_\text{$w_P$ }(f)$$ Where we can think of $w_P$ as an element of the tangent space at $P$. Now this in itself is a little weird; why have differential operators as a basis for something geometrical like a tangent space to a manifold? In any case, we apply these vectors to a function defined on our manifold, and we get the value we wanted out. So who cares about differential forms? We just did all this without them. We could've done this by calculating $\mathrm{d}f$, in some basis $\mathrm{d}x, \mathrm{d}y$ (which is quite confusing), and then calculating $\mathrm{d}f(w_P)$, but what do we gain in doing it this way? I mentioned I think the $\mathrm{d}x$'s are confusing. Well, $\mathrm{d}x$ is just the function $\mathrm{d}x(\frac{\partial}{\partial x})=1$ and 0 for any other differential operator -  why write this as $\mathrm{d}x$, which had always previously been used to mean an infinitesimal increase in x? Now I can understand caring about the dual of the tangent space. We are combining a vector in the tangent space with something and we're getting a scalar out of it - this something should then probably belong to the dual space. But if we're thinking of just the vector, then the function $f$ on the manifold needs to be encoded by the 1-form, right? Well, we can have 1-forms which aren't derivatives of any function on the manifold - what should it mean to combine such forms with tangent vectors? And lastly, if we're writing all our forms in terms of $\mathrm{d}x$'s etc., where the $x$'s are exactly the coordinates of the manifold, then how exactly have we escaped dependence on coordinates? We're still essentially calculating with respect to a given coordinate system as in usual multivariable calculus!",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
89,"Geometric interpretation of $\frac {\partial^2} {\partial x \partial y} f(x,y)$",Geometric interpretation of,"\frac {\partial^2} {\partial x \partial y} f(x,y)","Is there any geometric interpretation for the following second partial derivative? $$f_{xy} = \frac {\partial^2 f} {\partial x \partial y}$$ In particular, I'm trying to understand the determinant from second partial derivative test for determining whether a critical point is a minima/maxima/saddle points: $$D(a, b) = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2$$ I have no trouble understanding $f_{xx}(x,y)$ and $f_{yy}(x,y)$ as the of measure of concavity/convexity of $f$ in the direction of $x$ and $y$ axis. But what does $f_{xy}(x,y)$ mean?","Is there any geometric interpretation for the following second partial derivative? In particular, I'm trying to understand the determinant from second partial derivative test for determining whether a critical point is a minima/maxima/saddle points: I have no trouble understanding and as the of measure of concavity/convexity of in the direction of and axis. But what does mean?","f_{xy} = \frac {\partial^2 f} {\partial x \partial y} D(a, b) = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2 f_{xx}(x,y) f_{yy}(x,y) f x y f_{xy}(x,y)","['multivariable-calculus', 'partial-derivative', 'hessian-matrix', 'scalar-fields', 'geometric-interpretation']"
90,When did Fubini's name get applied to the theorem without measures?,When did Fubini's name get applied to the theorem without measures?,,"Fubini's theorem, from 1907, expresses integration with respect to a product measure in terms of iterated integrals. The simpler version of this theorem for multiple Riemann integrals was used long before Fubini was around and of course was not known by his name. Nowadays it is common for the relation between multiple Riemann integrals and iterated integrals to be called Fubini's theorem in books. A colleague of mine asked me when the label ""Fubini's theorem"" was first applied to this theorem about multiple Riemann integrals. (He considers it something of a travesty to use Fubini's name for this result in multivariable calculus books, where there is no measure theory content. As an example, in the 4th edition of Calculus (1990) by Larson, Hostetler, and Edwards the authors write ""The following theorem was proved by the Italian mathematician Guido Fubini"" and then they give a theorem on double integrals of continuous functions which certainly was not proved by Fubini.) I found this theorem does not have Fubini's name in some calculus and analysis books written decades ago: Whittaker and Watson's Modern Analysis (4th ed., 1927), Volume II of Apostol's Calculus (1962), Rudin's Principle of Mathematical Analysis (3rd ed., 1964), Thomas's Calculus and Analytic Geometry (4th ed., 1969), Bers's Calculus (1969), Loomis's Calculus (1974), Sherman Stein's Calculus and Analytic Geometry (2nd ed., 1977), George Simmons's Calculus with Analytic Geometry (1985), Marsden and Weinstein's Calculus III (1985), and Leithold's The Calculus with Analytic Geometry (5th ed., 1986). They all call this result something like ""the theorem on iterated integrals"". I found the name ""Fubini's theorem"" used for multiple Riemann integrals in Spivak's Calculus on Manifolds (1965). Does anyone know of an earlier usage of the label ""Fubini's theorem"" for multiple Riemann integrals?","Fubini's theorem, from 1907, expresses integration with respect to a product measure in terms of iterated integrals. The simpler version of this theorem for multiple Riemann integrals was used long before Fubini was around and of course was not known by his name. Nowadays it is common for the relation between multiple Riemann integrals and iterated integrals to be called Fubini's theorem in books. A colleague of mine asked me when the label ""Fubini's theorem"" was first applied to this theorem about multiple Riemann integrals. (He considers it something of a travesty to use Fubini's name for this result in multivariable calculus books, where there is no measure theory content. As an example, in the 4th edition of Calculus (1990) by Larson, Hostetler, and Edwards the authors write ""The following theorem was proved by the Italian mathematician Guido Fubini"" and then they give a theorem on double integrals of continuous functions which certainly was not proved by Fubini.) I found this theorem does not have Fubini's name in some calculus and analysis books written decades ago: Whittaker and Watson's Modern Analysis (4th ed., 1927), Volume II of Apostol's Calculus (1962), Rudin's Principle of Mathematical Analysis (3rd ed., 1964), Thomas's Calculus and Analytic Geometry (4th ed., 1969), Bers's Calculus (1969), Loomis's Calculus (1974), Sherman Stein's Calculus and Analytic Geometry (2nd ed., 1977), George Simmons's Calculus with Analytic Geometry (1985), Marsden and Weinstein's Calculus III (1985), and Leithold's The Calculus with Analytic Geometry (5th ed., 1986). They all call this result something like ""the theorem on iterated integrals"". I found the name ""Fubini's theorem"" used for multiple Riemann integrals in Spivak's Calculus on Manifolds (1965). Does anyone know of an earlier usage of the label ""Fubini's theorem"" for multiple Riemann integrals?",,"['reference-request', 'multivariable-calculus', 'math-history']"
91,Pushforward of Lie Bracket,Pushforward of Lie Bracket,,"I am trying to figure out why the following equality is true : $$f_*[X,Y]=[f_*X,f_*Y]$$ where $f:M\rightarrow N$ is a diffeomorphism, $M$, $N$ are smooth manifolds, $X$, $Y$ are smooth vector fields on $M$. I have tried to write $$f_*[X,Y]=\dfrac{\partial f^i}{\partial x^j}\left( \chi^k \dfrac{\partial \psi^j}{\partial x^k}-\psi^k \dfrac{\partial \chi^j}{\partial x^k}\right)\dfrac {\partial}{\partial y^i}$$ where $$X=\chi^k \dfrac{\partial}{\partial x^k},Y=\psi^k \dfrac{\partial}{\partial x^k}, [X,Y]=\left( \chi^k \dfrac{\partial \psi^j}{\partial x^k}-\psi^k \dfrac{\partial \chi^j}{\partial x^k}\right)\dfrac {\partial}{\partial x^j}.$$ However, when it comes to write the second part of the equality: $$[f_*X,f_*Y]=\left( (f_*X)^k \dfrac{\partial (f_*Y)^j}{\partial y^k}-(f_*Y)^k \dfrac{\partial (f_*X)^j}{\partial y^k}\right)\dfrac {\partial}{\partial y^j}$$ where $y^j$ is a coordinate basis of N. The problem I face is that I cannot differentiate $f_*Y, f_*X$ with respect to the basis $y^j$, in the above expression. Any help would be appreciated.  ( I would prefer an answer which is based on the definition of Lie Bracket with coordinates, as I worked above)","I am trying to figure out why the following equality is true : $$f_*[X,Y]=[f_*X,f_*Y]$$ where $f:M\rightarrow N$ is a diffeomorphism, $M$, $N$ are smooth manifolds, $X$, $Y$ are smooth vector fields on $M$. I have tried to write $$f_*[X,Y]=\dfrac{\partial f^i}{\partial x^j}\left( \chi^k \dfrac{\partial \psi^j}{\partial x^k}-\psi^k \dfrac{\partial \chi^j}{\partial x^k}\right)\dfrac {\partial}{\partial y^i}$$ where $$X=\chi^k \dfrac{\partial}{\partial x^k},Y=\psi^k \dfrac{\partial}{\partial x^k}, [X,Y]=\left( \chi^k \dfrac{\partial \psi^j}{\partial x^k}-\psi^k \dfrac{\partial \chi^j}{\partial x^k}\right)\dfrac {\partial}{\partial x^j}.$$ However, when it comes to write the second part of the equality: $$[f_*X,f_*Y]=\left( (f_*X)^k \dfrac{\partial (f_*Y)^j}{\partial y^k}-(f_*Y)^k \dfrac{\partial (f_*X)^j}{\partial y^k}\right)\dfrac {\partial}{\partial y^j}$$ where $y^j$ is a coordinate basis of N. The problem I face is that I cannot differentiate $f_*Y, f_*X$ with respect to the basis $y^j$, in the above expression. Any help would be appreciated.  ( I would prefer an answer which is based on the definition of Lie Bracket with coordinates, as I worked above)",,"['multivariable-calculus', 'manifolds']"
92,Rigorous proof that $dx dy=r\ dr\ d\theta$,Rigorous proof that,dx dy=r\ dr\ d\theta,"I get the graphic explanation, i.e. that the area $dA$ of the sector's increment can be looked upon as a polar ""rectangle"" as $dr$ and $d\theta$ are infinitesimal, but how do you prove this rigorously? 1. Geometrically, the exact area would be $$\frac{(r+dr)^2d\theta}{2} - \frac{r^2d\theta}{2}$$$$= (r + \frac{dr}{2}) dr d\theta  $$$$= r dr d\theta + \frac{dr^2 d\theta}{2}.$$ How do we get rid of $\frac{dr^2 d\theta}{2}$? Is it too insignificant in value compared to $r dr d\theta$ so that it vanishes? If so, is it the reason it gets ignored? And what's more, geometrically speaking, why would the two areas - $dx dy$ and $r dr d\theta$, be equal? 2. Approaching it algebraically, setting $$x = r\sin\theta$$$$y = r \cos\theta$$ gives $$\frac{dx}{d\theta} = r\cos\theta,  dx = r\cos{\theta} d\theta$$$$ \frac{dy}{dr} = \cos\theta,  dy = \cos\theta dr. $$ Multiplying both equations, side by side, gives $$dxdy = r\cos^2\theta dr d\theta.$$ Again I get an extra term, which is $\cos^2 \theta$. In both cases I am unable to derive that $dxdy = rdrd\theta$. What do I do wrong in my reasoning? It all makes me think that I'm getting something essential terribly wrong.","I get the graphic explanation, i.e. that the area $dA$ of the sector's increment can be looked upon as a polar ""rectangle"" as $dr$ and $d\theta$ are infinitesimal, but how do you prove this rigorously? 1. Geometrically, the exact area would be $$\frac{(r+dr)^2d\theta}{2} - \frac{r^2d\theta}{2}$$$$= (r + \frac{dr}{2}) dr d\theta  $$$$= r dr d\theta + \frac{dr^2 d\theta}{2}.$$ How do we get rid of $\frac{dr^2 d\theta}{2}$? Is it too insignificant in value compared to $r dr d\theta$ so that it vanishes? If so, is it the reason it gets ignored? And what's more, geometrically speaking, why would the two areas - $dx dy$ and $r dr d\theta$, be equal? 2. Approaching it algebraically, setting $$x = r\sin\theta$$$$y = r \cos\theta$$ gives $$\frac{dx}{d\theta} = r\cos\theta,  dx = r\cos{\theta} d\theta$$$$ \frac{dy}{dr} = \cos\theta,  dy = \cos\theta dr. $$ Multiplying both equations, side by side, gives $$dxdy = r\cos^2\theta dr d\theta.$$ Again I get an extra term, which is $\cos^2 \theta$. In both cases I am unable to derive that $dxdy = rdrd\theta$. What do I do wrong in my reasoning? It all makes me think that I'm getting something essential terribly wrong.",,"['multivariable-calculus', 'area', 'polar-coordinates', 'multiple-integral']"
93,Interesting implicit surfaces in $\mathbb{R}^3$,Interesting implicit surfaces in,\mathbb{R}^3,"I have just written a small program in C++ and OpenGl to plot implicit surfaces in $\mathbb{R}^3$ for a Graphical Computing class and now I'm in need of more interesting surfaces to implement! Some that I've implemented are: Basic surfaces like spheres and cylinders; Nordstrand's Weird Surface; Klein Quartic; Goursat's Surface; Heart Surface; So, my question is, what are other interesting implicit surfaces in $\mathbb{R}^3?$ P.S.: I know this is kind of vague, but anything you find interesting will be of use. (: P.P.S: Turn this into a community wiki, if need be.","I have just written a small program in C++ and OpenGl to plot implicit surfaces in $\mathbb{R}^3$ for a Graphical Computing class and now I'm in need of more interesting surfaces to implement! Some that I've implemented are: Basic surfaces like spheres and cylinders; Nordstrand's Weird Surface; Klein Quartic; Goursat's Surface; Heart Surface; So, my question is, what are other interesting implicit surfaces in $\mathbb{R}^3?$ P.S.: I know this is kind of vague, but anything you find interesting will be of use. (: P.P.S: Turn this into a community wiki, if need be.",,"['algebraic-geometry', 'multivariable-calculus', 'visualization', 'surfaces']"
94,Understanding the derivative as a linear transformation,Understanding the derivative as a linear transformation,,"It's been a while now I am studying multivariable calculus and the concept of differentiation in space (or higher dimension). I saw relative posts but one question remains. I can't understand the concept of linear transformation that we use to define the Frechet derivative. In single variable the derivative is the best linear approximation of the function, so I guess this extends to multivariable but we can't use a number for this (why?) and instead we use a matrix. Can someone clears this for me in plain english?","It's been a while now I am studying multivariable calculus and the concept of differentiation in space (or higher dimension). I saw relative posts but one question remains. I can't understand the concept of linear transformation that we use to define the Frechet derivative. In single variable the derivative is the best linear approximation of the function, so I guess this extends to multivariable but we can't use a number for this (why?) and instead we use a matrix. Can someone clears this for me in plain english?",,[]
95,Use of partial derivatives as basis vector,Use of partial derivatives as basis vector,,"I am trying to understand use of partial derivatives as basis functions from differential geometry In tangent space $\mathbb{R^n}$ at point $p$, the basis vectors $e_1, e_2,...,e_n$ can be written as $\frac {\partial}{\partial x^1} \bigg|_p,\frac {\partial}{\partial x^2} \bigg|_p,...,\frac {\partial}{\partial x^n} \bigg|_p$ Let's say in 2 dimensional Euclidean space, a function $f : \mathbb {R^2}\rightarrow \mathbb {R^2}$ is $x^2 + y^2=4$ , a circle with radius 2.  Tangent at point $p$ (2,0) will be $0e_1 + e_2$.  If I say $f =x^2 + y^2-4 =0$, $\frac {\partial f}{\partial x} \bigg|_{p=(2,0)} = 4 \quad$ and   $\quad \frac {\partial f}{\partial y} \bigg|_{p=(2,0)} = 4$ This does not make sense of the partial derivatives as basis vectors. Any comments?","I am trying to understand use of partial derivatives as basis functions from differential geometry In tangent space $\mathbb{R^n}$ at point $p$, the basis vectors $e_1, e_2,...,e_n$ can be written as $\frac {\partial}{\partial x^1} \bigg|_p,\frac {\partial}{\partial x^2} \bigg|_p,...,\frac {\partial}{\partial x^n} \bigg|_p$ Let's say in 2 dimensional Euclidean space, a function $f : \mathbb {R^2}\rightarrow \mathbb {R^2}$ is $x^2 + y^2=4$ , a circle with radius 2.  Tangent at point $p$ (2,0) will be $0e_1 + e_2$.  If I say $f =x^2 + y^2-4 =0$, $\frac {\partial f}{\partial x} \bigg|_{p=(2,0)} = 4 \quad$ and   $\quad \frac {\partial f}{\partial y} \bigg|_{p=(2,0)} = 4$ This does not make sense of the partial derivatives as basis vectors. Any comments?",,"['multivariable-calculus', 'differential-geometry', 'manifolds']"
96,Derivation of the method of Lagrange multipliers?,Derivation of the method of Lagrange multipliers?,,"I've always used the method of Lagrange multipliers with blind confidence that it will give the correct results when optimizing problems with constraints. But I would like to know if anyone can provide or recommend a derivation of the method at physics undergraduate level that can highlight its limitations, if any.","I've always used the method of Lagrange multipliers with blind confidence that it will give the correct results when optimizing problems with constraints. But I would like to know if anyone can provide or recommend a derivation of the method at physics undergraduate level that can highlight its limitations, if any.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
97,"Derivative of the $f(x,y)=\min(x,y)$",Derivative of the,"f(x,y)=\min(x,y)","I just encountered this function $f(x,y)=\min(x,y)$. I wonder what  the partial derivatives of it look like.","I just encountered this function $f(x,y)=\min(x,y)$. I wonder what  the partial derivatives of it look like.",,"['multivariable-calculus', 'derivatives']"
98,Why does dust gather in corners?,Why does dust gather in corners?,,"I've noticed when sweeping the floor that dust gathers particularly in the corners. I assume there is a fluid mechanics reason for this. Does anyone know what it is? Edit: No, really, this is a mathematical question. Air blows around the room, which constitutes a vector field. Let's say it blows through a square room with doors near bottom-left and top-right. Air blows from bottom-left to top-right. There are dust particles in the room, let's say uniformly distributed at first. But after application of this flow they are not uniformly distributed. They pile in the corners. Maybe that's because vortices form more in the corners, maybe some other reason. It's not because of where I start sweeping. It is a physics question, but obviously knowledge of Newtonian mechanics won't solve it. It comes down to fluid flow and vector fields, which is math.","I've noticed when sweeping the floor that dust gathers particularly in the corners. I assume there is a fluid mechanics reason for this. Does anyone know what it is? Edit: No, really, this is a mathematical question. Air blows around the room, which constitutes a vector field. Let's say it blows through a square room with doors near bottom-left and top-right. Air blows from bottom-left to top-right. There are dust particles in the room, let's say uniformly distributed at first. But after application of this flow they are not uniformly distributed. They pile in the corners. Maybe that's because vortices form more in the corners, maybe some other reason. It's not because of where I start sweeping. It is a physics question, but obviously knowledge of Newtonian mechanics won't solve it. It comes down to fluid flow and vector fields, which is math.",,"['physics', 'applications', 'multivariable-calculus']"
99,What is the motivation for differential forms?,What is the motivation for differential forms?,,"I am that point in my mathematical career where I am learning differential forms. I am reading from M.Spivak's Calculus on Manifolds. So far I have gone over the tensor and wedge products and their properties, defined forms, learned of their pullbacks and the properties of these pullbacks, and defined the differential operator while learning some of its properties. I am currently reading about exact/closed forms in the build up to a certain ""Poincare Lemma"". While the theory all seems to be fitting together (albeit with a bit of effort), there has been a nagging question. What is the motivation here? It has been my experience that many mathematical constructions (that I have encountered at least) are done with the goal of better understanding something. I feel like this thing is missing from my understanding of differential forms. Any insight will be appreciated.","I am that point in my mathematical career where I am learning differential forms. I am reading from M.Spivak's Calculus on Manifolds. So far I have gone over the tensor and wedge products and their properties, defined forms, learned of their pullbacks and the properties of these pullbacks, and defined the differential operator while learning some of its properties. I am currently reading about exact/closed forms in the build up to a certain ""Poincare Lemma"". While the theory all seems to be fitting together (albeit with a bit of effort), there has been a nagging question. What is the motivation here? It has been my experience that many mathematical constructions (that I have encountered at least) are done with the goal of better understanding something. I feel like this thing is missing from my understanding of differential forms. Any insight will be appreciated.",,"['differential-geometry', 'multivariable-calculus', 'soft-question', 'differential-forms']"
