,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Suppose $a_n \in (0,1)$, then prove that $na_n \to \infty \iff (1-a_n)^n \to 0$","Suppose , then prove that","a_n \in (0,1) na_n \to \infty \iff (1-a_n)^n \to 0","Suppose $a_n \in (0,1)$ , then prove that $na_n \to \infty \iff (1-a_n)^n \to 0$ . Assume $na_n\to \infty$ , then we have $$(1-a_n)^n \leq e^{-na_n} \to 0$$ where we use $1+x \leq e^x$ and plug in $x = -a_n$ . But I don't know how to do the other direction. Any hints?","Suppose , then prove that . Assume , then we have where we use and plug in . But I don't know how to do the other direction. Any hints?","a_n \in (0,1) na_n \to \infty \iff (1-a_n)^n \to 0 na_n\to \infty (1-a_n)^n \leq e^{-na_n} \to 0 1+x \leq e^x x = -a_n","['real-analysis', 'sequences-and-series', 'limits']"
1,Compute $r$ such that $\lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0$?,Compute  such that ?,r \lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0,"I am trying to compute $r$ such that: $$\lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0$$ I tried a few things, mostly multiplying it by some conjugate and using identities like $e^{\log(x)}=x$ but couldn't find out. In the book, the answer seems to be $r=\frac{1}{2}$ but I can't manipulate it in order to get this value. Can you help me? In the book I'm reading (Efimov's Higher Mathematics), this is called ""computing the order of $3^{\sqrt{x}}-1$ relative to $x$ "".","I am trying to compute such that: I tried a few things, mostly multiplying it by some conjugate and using identities like but couldn't find out. In the book, the answer seems to be but I can't manipulate it in order to get this value. Can you help me? In the book I'm reading (Efimov's Higher Mathematics), this is called ""computing the order of relative to "".",r \lim_{x\to 0}\frac{3^{\sqrt{x}}-1}{x^r} \neq 0 e^{\log(x)}=x r=\frac{1}{2} 3^{\sqrt{x}}-1 x,"['limits', 'limits-without-lhopital']"
2,How to solve this indeterminate form?,How to solve this indeterminate form?,,"Let $a_1(n)=n$ , $a_k(n)=2+\sqrt{a_{k-1}(n)},k\ge2$ , then can we determine the limit of the following indeterminate form $$ \lim_{k\to+\infty}\frac{\sqrt{2-\sqrt{a_k(2)}}}{\sqrt{2-\sqrt{a_k(3)}}}. $$ I have no idea to solve it, and I tried to calculate it in Matlab, and the answer maybe is $\frac{3}{2}$ .","Let , , then can we determine the limit of the following indeterminate form I have no idea to solve it, and I tried to calculate it in Matlab, and the answer maybe is .","a_1(n)=n a_k(n)=2+\sqrt{a_{k-1}(n)},k\ge2 
\lim_{k\to+\infty}\frac{\sqrt{2-\sqrt{a_k(2)}}}{\sqrt{2-\sqrt{a_k(3)}}}.
 \frac{3}{2}","['limits', 'analysis']"
3,"If $g(t,x) \rightarrow 0$ as $x \rightarrow \infty$, then decays uniformly in $t$?","If  as , then decays uniformly in ?","g(t,x) \rightarrow 0 x \rightarrow \infty t","Assume we have continuous $g:[0,T]\times \mathbb{R}\rightarrow \mathbb{R}$ such that $$ \lim_{x \rightarrow \pm \infty} g(t,x)=0 $$ for every $t \in [0,T]$ . Then let $(t_k)_{k\in\mathbb{N}}\subseteq [0,T]$ converge to some $t \in [0,T]$ and $(x_k)_{k \in \mathbb{N}}\subseteq\mathbb{R}$ diverge to $\infty$ . Is it then true that $$ \lim_{k \rightarrow \infty} g(t_k,x_k) =0? $$ Initially, I thought yes. I tried to use continuity to make $\lvert g(t_k,x_k)-g(t,x)\rvert$ small. But continuity is not uniform, such that there is noch guarantee the above expression goes to $0$ as $k\rightarrow \infty$ . So there might be a counterexample , although I do not see it…","Assume we have continuous such that for every . Then let converge to some and diverge to . Is it then true that Initially, I thought yes. I tried to use continuity to make small. But continuity is not uniform, such that there is noch guarantee the above expression goes to as . So there might be a counterexample , although I do not see it…","g:[0,T]\times \mathbb{R}\rightarrow \mathbb{R} 
\lim_{x \rightarrow \pm \infty} g(t,x)=0
 t \in [0,T] (t_k)_{k\in\mathbb{N}}\subseteq [0,T] t \in [0,T] (x_k)_{k \in \mathbb{N}}\subseteq\mathbb{R} \infty 
\lim_{k \rightarrow \infty} g(t_k,x_k) =0?
 \lvert g(t_k,x_k)-g(t,x)\rvert 0 k\rightarrow \infty","['real-analysis', 'limits', 'continuity', 'examples-counterexamples']"
4,Asymptotic of $\sum_{k=0}^\infty\frac{\Gamma (k+n+1) \Gamma (3 k+n+1)}{\Gamma (k+2) \Gamma (3 k+2 n+2)}$ as $n\to \infty$,Asymptotic of  as,\sum_{k=0}^\infty\frac{\Gamma (k+n+1) \Gamma (3 k+n+1)}{\Gamma (k+2) \Gamma (3 k+2 n+2)} n\to \infty,"Let $a,b,c,d$ be fixed and in a small neighborhood of $0$ , so the series $$f(n) = \sum_{k=0}^\infty\frac{\Gamma (a+k+n+1) \Gamma (b+3 k+n+1)}{\Gamma (c+k+2) \Gamma (d+3 k+2 n+2)}$$ converges, the title is special case $a=b=c=d=0$ . Question: What is the leading asymptotic of $f(n)$ as $n\to \infty$ , with general $a,b,c,d$ near $0$ . The expression of $f(n)$ is a hypergeometric series and can be converted into an integral form, then a similar analysis like here should be feasible. But this approach is quite complicated, I'm looking for alternative solutions. Thank you very much.","Let be fixed and in a small neighborhood of , so the series converges, the title is special case . Question: What is the leading asymptotic of as , with general near . The expression of is a hypergeometric series and can be converted into an integral form, then a similar analysis like here should be feasible. But this approach is quite complicated, I'm looking for alternative solutions. Thank you very much.","a,b,c,d 0 f(n) = \sum_{k=0}^\infty\frac{\Gamma (a+k+n+1) \Gamma (b+3 k+n+1)}{\Gamma (c+k+2) \Gamma (d+3 k+2 n+2)} a=b=c=d=0 f(n) n\to \infty a,b,c,d 0 f(n)","['real-analysis', 'sequences-and-series', 'limits', 'asymptotics', 'hypergeometric-function']"
5,$\infty-\infty$ indeterminate case,indeterminate case,\infty-\infty,"The methods yield two different answers. Could you explain the reason clearly and in detailed? Question: $$\lim_{{x \to \infty}} \left( \sqrt{x^2 + 6x + 14} - (x+1) \right) = ?$$ Solution: Method 1: By employing the conjugate of the original problem, the solution is as follows: \begin{aligned} & =\operatorname{lim}_{x \rightarrow \infty}\left[\sqrt{(x+3)^2+5}-(x+1)\right] \\ & =\lim _{x \rightarrow \infty}\left[\sqrt{(x+3)^2 \cdot\left(1+\frac{5}{(x+3)^2}\right)}-(x+1)\right] \\ & =\lim _{x \rightarrow \infty}\left[|x+3| \cdot \sqrt{1+\frac{5}{(x + 3)^2}}-(x+1)\right] \\ & =\lim _{x \rightarrow \infty}[(x+3)-(x+1)]=2 \end{aligned} Method 2: Alternatively, \begin{aligned} & =\lim_{x \rightarrow \infty}\left[\sqrt{x^2 \cdot\left(1+\frac{6}{x}+\frac{14}{x^2}\right)}-(x+1)\right] \\ & =\lim_{x \rightarrow \infty} \left[|x| \cdot \sqrt{1+\frac{6}{x}+\frac{14 }{x^2}}-(x+1)\right] . \\ & =\lim_{x \rightarrow \infty}[(x)-(x+1)]=-1 \end{aligned} In some books, the authors solve the examples with $\infty-\infty$ indeterminate case by multiplying and dividing the expression by its conjugate in the $\infty-\infty$ indeterminate case. But is it not $\frac{\infty}{\infty}$ ? How can we do it? Please see my other question in this site.","The methods yield two different answers. Could you explain the reason clearly and in detailed? Question: Solution: Method 1: By employing the conjugate of the original problem, the solution is as follows: Method 2: Alternatively, In some books, the authors solve the examples with indeterminate case by multiplying and dividing the expression by its conjugate in the indeterminate case. But is it not ? How can we do it? Please see my other question in this site.","\lim_{{x \to \infty}} \left( \sqrt{x^2 + 6x + 14} - (x+1) \right) = ? \begin{aligned}
& =\operatorname{lim}_{x \rightarrow \infty}\left[\sqrt{(x+3)^2+5}-(x+1)\right] \\
& =\lim _{x \rightarrow \infty}\left[\sqrt{(x+3)^2 \cdot\left(1+\frac{5}{(x+3)^2}\right)}-(x+1)\right] \\
& =\lim _{x \rightarrow \infty}\left[|x+3| \cdot \sqrt{1+\frac{5}{(x + 3)^2}}-(x+1)\right] \\
& =\lim _{x \rightarrow \infty}[(x+3)-(x+1)]=2
\end{aligned} \begin{aligned}
& =\lim_{x \rightarrow \infty}\left[\sqrt{x^2 \cdot\left(1+\frac{6}{x}+\frac{14}{x^2}\right)}-(x+1)\right] \\
& =\lim_{x \rightarrow \infty} \left[|x| \cdot \sqrt{1+\frac{6}{x}+\frac{14 }{x^2}}-(x+1)\right] . \\
& =\lim_{x \rightarrow \infty}[(x)-(x+1)]=-1
\end{aligned} \infty-\infty \infty-\infty \frac{\infty}{\infty}","['limits', 'indeterminate-forms']"
6,What is $\lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x}$?,What is ?,\lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x},"I want to find a closed form for the average value of $\cos\{t-\cos t\}$ where $\{n\}$ denotes the fractional part of $n$ . I do not have experience finding an average value over an infinite domain but I assume it would be something like this: $$\lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x}$$ I am not sure if the limit converges. Here is what I have from Desmos: $$\begin{array}{|c|c|} \hline x & \frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x} \\\hline 10 & 0.829164368874\\\hline 10*10^3 & 0.834299716027\\\hline 10*10^6 & 0.859423358961\\\hline 10*10^9 & 0.840428861971\\\hline 10*10^{14} & 0.826897135363\\\hline 10*10^{15} & 0.958730802913\\\hline 10*10^{16} & 0.995401790748\\\hline 10*10^{26} & 0.999999999999\\\hline 10*10^{27} & 1\\\hline \end{array}$$ As you can see, the limit appears to oscillate which makes sense after looking at the graph. When it gets to 10 $^{\text{15}}$ , it suddenly jumps up and begins to approach 1. My intuition tells me this is probably a bug because I am using such high numbers. I tried to solve it by hand but failed to figure out how to convert it to a summation. I have a few ideas to what it converges to (assuming it does and is not 1) but they are basically wild guesses. $$\cos(\cos(1))\approx 0.857553215846$$ $$\frac{\pi^2}{12}\approx 0.822467033424$$ All help is appreciated :)","I want to find a closed form for the average value of where denotes the fractional part of . I do not have experience finding an average value over an infinite domain but I assume it would be something like this: I am not sure if the limit converges. Here is what I have from Desmos: As you can see, the limit appears to oscillate which makes sense after looking at the graph. When it gets to 10 , it suddenly jumps up and begins to approach 1. My intuition tells me this is probably a bug because I am using such high numbers. I tried to solve it by hand but failed to figure out how to convert it to a summation. I have a few ideas to what it converges to (assuming it does and is not 1) but they are basically wild guesses. All help is appreciated :)","\cos\{t-\cos t\} \{n\} n \lim_{x\to\infty}\frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x} \begin{array}{|c|c|}
\hline
x & \frac{\int_{0}^{x}\cos\{t-\cos t\}dt}{x} \\\hline
10 & 0.829164368874\\\hline
10*10^3 & 0.834299716027\\\hline
10*10^6 & 0.859423358961\\\hline
10*10^9 & 0.840428861971\\\hline
10*10^{14} & 0.826897135363\\\hline
10*10^{15} & 0.958730802913\\\hline
10*10^{16} & 0.995401790748\\\hline
10*10^{26} & 0.999999999999\\\hline
10*10^{27} & 1\\\hline
\end{array} ^{\text{15}} \cos(\cos(1))\approx 0.857553215846 \frac{\pi^2}{12}\approx 0.822467033424","['integration', 'limits', 'improper-integrals', 'closed-form', 'average']"
7,"If $\lim_{n\to\infty}A_n=A\neq\varnothing$ as set-theoretic limit, is it true that $\lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A)$?","If  as set-theoretic limit, is it true that ?",\lim_{n\to\infty}A_n=A\neq\varnothing \lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A),"Let $(X, d)$ be a metric space with topology induced by the metric $d$ , and $(A_n)_{n=1}^\infty$ be a sequence of sets in $X$ such that $\lim_{n\to\infty}A_n = A \neq \varnothing$ as a set-theoretic limit . Is it then true that $\lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A)$ ? Knowing the definition of point-wise continuity in the standard topology of metric spaces, I am wondering whether this is a question about the ""set-wise"" continuity of certain functions in metric spaces. So, is there some known result/counterexample that allows me to conclude that $\lim_{n\to\infty}\mathrm{diam}(A_n) =\mathrm{diam}(A)$ in general, or is this something you have to (painfully) check everytime in practise?","Let be a metric space with topology induced by the metric , and be a sequence of sets in such that as a set-theoretic limit . Is it then true that ? Knowing the definition of point-wise continuity in the standard topology of metric spaces, I am wondering whether this is a question about the ""set-wise"" continuity of certain functions in metric spaces. So, is there some known result/counterexample that allows me to conclude that in general, or is this something you have to (painfully) check everytime in practise?","(X, d) d (A_n)_{n=1}^\infty X \lim_{n\to\infty}A_n = A \neq \varnothing \lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A) \lim_{n\to\infty}\mathrm{diam}(A_n) =\mathrm{diam}(A)","['real-analysis', 'general-topology', 'limits', 'continuity', 'metric-spaces']"
8,how to evaluate $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!} $,how to evaluate,\lim_{n \to \infty} n\frac{(n-1)!!}{n!!} ,"I want to evaluate $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!} $ I was solving the integral: $$I_n:=\lim_{ n \to \infty} \int_0 ^n   \left( \frac{2nt}{t^2+n^2} \right)^n dt$$ and I did the following let $t=n \tan(x)$ , $dt= n \sec^2{(x)} dx$ then $$I_n =\lim_{n \to \infty} n\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \sec^2{(x)}dx =\lim_{n \to \infty} n\left( \int_0 ^{\frac{\pi}{4}} \sin^n{(2x)}dx +\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \tan^2{(x)}dx \right) \geq \lim_{n \to \infty} \frac{n}{ 2} \int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx$$ There is famous formula (Wallis formula) for $\int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx$ which is $\frac{(n-1)!!}{n!!} $ if $n$ is odd and $ \frac{(n-1)!!}{n!!} \frac{ \pi}{2}$ if $n$ is even,I do believe the integral diverge to infinity i.e $\lim_{n \to \infty} n\frac{(n-1)!!}{n!!}= \infty $ but I couldn't prove that.","I want to evaluate I was solving the integral: and I did the following let , then There is famous formula (Wallis formula) for which is if is odd and if is even,I do believe the integral diverge to infinity i.e but I couldn't prove that.",\lim_{n \to \infty} n\frac{(n-1)!!}{n!!}  I_n:=\lim_{ n \to \infty} \int_0 ^n   \left( \frac{2nt}{t^2+n^2} \right)^n dt t=n \tan(x) dt= n \sec^2{(x)} dx I_n =\lim_{n \to \infty} n\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \sec^2{(x)}dx =\lim_{n \to \infty} n\left( \int_0 ^{\frac{\pi}{4}} \sin^n{(2x)}dx +\int_0 ^{\frac{\pi}{4}} \sin^n{(2x)} \tan^2{(x)}dx \right) \geq \lim_{n \to \infty} \frac{n}{ 2} \int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx \int_0 ^{\frac{\pi}{2}} \sin^n{(x)}dx \frac{(n-1)!!}{n!!}  n  \frac{(n-1)!!}{n!!} \frac{ \pi}{2} n \lim_{n \to \infty} n\frac{(n-1)!!}{n!!}= \infty ,"['calculus', 'limits', 'factorial', 'limits-without-lhopital']"
9,Solving $\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr)$ algebraically,Solving  algebraically,\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr),I want to solve the following limit using algebraic techniques learnt in calculus class (including l'Hospital rule): $$\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr)$$ I have tried simplifying the expression and putting over the same denomniator. $$\lim_{x\to0^+}\frac{e^{1/x}(\cos(x)-1)}{\cos(x)}$$ Then I just need to solve this expression: $$\lim_{x\to0^+}e^{1/x}(\cos(x)-1)$$ I have tried expressing it as a fraction to apply l'Hospital's rule but it only seems to get worst. $$\lim_{x\to0^+}\frac{\cos(x)-1}{e^{-1/x}}=\lim_{x\to0^+}\frac{-x^2\sin(x)}{e^{-1/x}}$$,I want to solve the following limit using algebraic techniques learnt in calculus class (including l'Hospital rule): I have tried simplifying the expression and putting over the same denomniator. Then I just need to solve this expression: I have tried expressing it as a fraction to apply l'Hospital's rule but it only seems to get worst.,\lim_{x\to0^+}e^{1/x}\bigl(1-\sec(x)\bigr) \lim_{x\to0^+}\frac{e^{1/x}(\cos(x)-1)}{\cos(x)} \lim_{x\to0^+}e^{1/x}(\cos(x)-1) \lim_{x\to0^+}\frac{\cos(x)-1}{e^{-1/x}}=\lim_{x\to0^+}\frac{-x^2\sin(x)}{e^{-1/x}},"['calculus', 'limits', 'trigonometry', 'exponential-function']"
10,Evaluating $\lim_{x\to\infty}\frac{\ln\cos\frac {48} x}{\ln\cos\frac{1}{12x}}$ without using L'Hopital,Evaluating  without using L'Hopital,\lim_{x\to\infty}\frac{\ln\cos\frac {48} x}{\ln\cos\frac{1}{12x}},"I've seen some similar question, but none with $x\to\infty$ $$\lim_{x\to\infty}\frac{\ln\cos\frac {48} x}{\ln\cos\frac{1}{12x}}$$ one of the ideas that i tried is, with $t=\frac{48}{x}$ , changing the limit to: $$\lim_{t\to0}\frac{\ln\cos t}{\ln\cos\frac{t}{576}}$$ but it got me stuck pretty much there. Thanks and sorry if it's basically a duplicate.","I've seen some similar question, but none with one of the ideas that i tried is, with , changing the limit to: but it got me stuck pretty much there. Thanks and sorry if it's basically a duplicate.",x\to\infty \lim_{x\to\infty}\frac{\ln\cos\frac {48} x}{\ln\cos\frac{1}{12x}} t=\frac{48}{x} \lim_{t\to0}\frac{\ln\cos t}{\ln\cos\frac{t}{576}},"['calculus', 'limits', 'limits-without-lhopital']"
11,Stuck trying to prove the product property of limits,Stuck trying to prove the product property of limits,,"I'm trying to come up with a proof of the statement ""if $\lim_{x\to p}f(x)=A$ and $\lim_{x\to p}g(x)=B$ , then $\lim_{x\to p}f(x)g(x)=AB$ "". This is what I've attempted: Choose some $\epsilon$ > $0$ . Since $\lim_{x\to p}f(x)=A$ and $\lim_{x\to p}g(x)=B$ , there must exist a $\delta_{1}$ and $\delta_{2}$ such that: $$0<|x-p|<\delta_{1}\implies|f(x)-A|<\sqrt{\epsilon}$$ $$0<|x-p|<\delta_{2}\implies|g(x)-B|<\sqrt{\epsilon}$$ Because if $\epsilon>0$ , then $\sqrt{\epsilon}>0$ and hence a satisfactory $\delta$ must exist for $\sqrt{\epsilon}$ . Let $\delta_{3}=\min(\delta_{1},\delta_{2})$ . Then we must have $$0<|x-p|<\delta_{3}\implies|f(x)-A|\cdot|g(x)-B|<\epsilon,$$ because $(\sqrt{\epsilon})^2=\epsilon$ . Now I feel as if I must find a way to show that $$|f(x)-A|\cdot|g(x)-B|=|f(x)g(x)-Ag(x)-Bf(x)+AB|\ge|f(x)g(x)-AB|,$$ but this statement seems to boil down to showing that $-AB\le -Ag(x)-Bf(x)+AB$ , which doesn't look like it holds true in the general case. This makes me feel as if I've made some mistake somewhere along the way. I can't think of any other way to approach this problem, so I've ended up stuck at this point.","I'm trying to come up with a proof of the statement ""if and , then "". This is what I've attempted: Choose some > . Since and , there must exist a and such that: Because if , then and hence a satisfactory must exist for . Let . Then we must have because . Now I feel as if I must find a way to show that but this statement seems to boil down to showing that , which doesn't look like it holds true in the general case. This makes me feel as if I've made some mistake somewhere along the way. I can't think of any other way to approach this problem, so I've ended up stuck at this point.","\lim_{x\to p}f(x)=A \lim_{x\to p}g(x)=B \lim_{x\to p}f(x)g(x)=AB \epsilon 0 \lim_{x\to p}f(x)=A \lim_{x\to p}g(x)=B \delta_{1} \delta_{2} 0<|x-p|<\delta_{1}\implies|f(x)-A|<\sqrt{\epsilon} 0<|x-p|<\delta_{2}\implies|g(x)-B|<\sqrt{\epsilon} \epsilon>0 \sqrt{\epsilon}>0 \delta \sqrt{\epsilon} \delta_{3}=\min(\delta_{1},\delta_{2}) 0<|x-p|<\delta_{3}\implies|f(x)-A|\cdot|g(x)-B|<\epsilon, (\sqrt{\epsilon})^2=\epsilon |f(x)-A|\cdot|g(x)-B|=|f(x)g(x)-Ag(x)-Bf(x)+AB|\ge|f(x)g(x)-AB|, -AB\le -Ag(x)-Bf(x)+AB","['real-analysis', 'calculus', 'limits']"
12,How to solve this mathematical analysis limit problem?,How to solve this mathematical analysis limit problem?,,"The following question is the third problem of the analysis section in the 2023 Mathematics Department Transfer Exam for Peking University. “ Calculate the limit of sequence: $$\lim_{n\to\infty}\sum_{k=1}^{n}\left(\frac{1}{n}+\dfrac{k}{n^2}\right)^{1+\frac{2k}{n^2}}$$ ” My idea is to transform the limit into some kind of Riemann integral, but I don't know how to do it. Or there may be other methods, such as the Sandwich theorem, etc.","The following question is the third problem of the analysis section in the 2023 Mathematics Department Transfer Exam for Peking University. “ Calculate the limit of sequence: ” My idea is to transform the limit into some kind of Riemann integral, but I don't know how to do it. Or there may be other methods, such as the Sandwich theorem, etc.",\lim_{n\to\infty}\sum_{k=1}^{n}\left(\frac{1}{n}+\dfrac{k}{n^2}\right)^{1+\frac{2k}{n^2}},"['real-analysis', 'limits', 'analysis']"
13,What would the floor of 0.99999.... [duplicate],What would the floor of 0.99999.... [duplicate],,"This question already has answers here : Is it true that $0.999999999\ldots=1$? (31 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I saw a post on r/learnmath about this, but I wanted to hear what you guys think. If I think in terms of sequences, I believe that it would be equal to zero, since 0.9999...9 (with n '9's) would always be less than 1 and greater than 0 for all natural n and hence, the floor would be zero (making the limit as n goes to infinity 0). However, if we first evaluate what's inside, then we'd get the floor of 1, which is just 1. I personally think the first 'method' yielding 0 is more consistent with the idea of sequences.","This question already has answers here : Is it true that $0.999999999\ldots=1$? (31 answers) Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I saw a post on r/learnmath about this, but I wanted to hear what you guys think. If I think in terms of sequences, I believe that it would be equal to zero, since 0.9999...9 (with n '9's) would always be less than 1 and greater than 0 for all natural n and hence, the floor would be zero (making the limit as n goes to infinity 0). However, if we first evaluate what's inside, then we'd get the floor of 1, which is just 1. I personally think the first 'method' yielding 0 is more consistent with the idea of sequences.",,"['sequences-and-series', 'limits']"
14,Limit related Binomial Problem,Limit related Binomial Problem,,Evaluate $\lim_{n\rightarrow∞}\left(\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\left(\frac{1}{r+3}\right)\right)}\right)$ My approach is as follow but not able to integrate it or convert into a limit series as it involves binomial function $^nC_r$ $\left(\frac{1}{r+3}\right)=\int\limits_{0}^{1}{x^{r+2} dx}\implies \lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\int\limits_{0}^{1}{x^{r+2} dx}\right)}}=\lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left({n \choose r}*\int\limits_{0}^{1}{x^2*\left(\frac{x}{n}\right)^r dx}\right)}}$,Evaluate My approach is as follow but not able to integrate it or convert into a limit series as it involves binomial function,\lim_{n\rightarrow∞}\left(\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\left(\frac{1}{r+3}\right)\right)}\right) ^nC_r \left(\frac{1}{r+3}\right)=\int\limits_{0}^{1}{x^{r+2} dx}\implies \lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left(\frac{n \choose r}{n^r}*\int\limits_{0}^{1}{x^{r+2} dx}\right)}}=\lim\limits_{n\rightarrow∞}{\sum\limits_{r=0}^{n}{\left({n \choose r}*\int\limits_{0}^{1}{x^2*\left(\frac{x}{n}\right)^r dx}\right)}},"['limits', 'binomial-theorem']"
15,Solve $\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}})$ without L'Hopital,Solve  without L'Hopital,\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}}),"I've seen the solution to $\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}})$ using L'Hopital and I was wondering if there's a way to find out the result without it. My initial attempt was outright stupid of me because I tried to substitute the limit of $\frac{\ln{(1+\frac{1}{x})}}{\frac{1}{x}}$ as $x$ approaches $\infty$ with $1$ , which results in the initial limit being $0$ . That's obviously false as I ignored the fact that I cannot do such a substitution when the limit is in an indeterminate form. That being said, how could you solve this limit without L'Hopital?","I've seen the solution to using L'Hopital and I was wondering if there's a way to find out the result without it. My initial attempt was outright stupid of me because I tried to substitute the limit of as approaches with , which results in the initial limit being . That's obviously false as I ignored the fact that I cannot do such a substitution when the limit is in an indeterminate form. That being said, how could you solve this limit without L'Hopital?",\lim_{x\to\infty}(x-x^2 \ln{\frac{1+x}{x}}) \frac{\ln{(1+\frac{1}{x})}}{\frac{1}{x}} x \infty 1 0,"['calculus', 'limits', 'limits-without-lhopital']"
16,Evaluating the limit $\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x$,Evaluating the limit,\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x,"How can I evaluate the following limit? $$\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x$$ To solve this limit, I divided the numerator and denominator by $x^5$ and I got $$\lim_{x\to\infty}\left(\frac {1+\frac {1}{x}\pi+\frac {1}{x^5}e}{1+\frac {1}{x}e+\frac {1}{x^5}\pi}\right)^x$$ But I realized when $x\to\infty$ the expression is still indeterminate. I tried changing the $x=\frac {1}{y}$ variable to be able to apply Lophital's rule, but I couldn't get a useful result.","How can I evaluate the following limit? To solve this limit, I divided the numerator and denominator by and I got But I realized when the expression is still indeterminate. I tried changing the variable to be able to apply Lophital's rule, but I couldn't get a useful result.",\lim_{x\to\infty}\left(\frac {x^5+\pi x^4+e}{x^5+ex^4+\pi}\right)^x x^5 \lim_{x\to\infty}\left(\frac {1+\frac {1}{x}\pi+\frac {1}{x^5}e}{1+\frac {1}{x}e+\frac {1}{x^5}\pi}\right)^x x\to\infty x=\frac {1}{y},"['calculus', 'limits']"
17,Stolz-Cesaro converse theorem proof - possible mistake?,Stolz-Cesaro converse theorem proof - possible mistake?,,"The Converse Stolz-Cesaro theorem states that if $(b_n)_n$ is strictly monotone and divergent and: $$ \lim_{n \to \infty}\frac{b_n}{b_{n+1}} = B \in \mathbb{R} \setminus \{1\} , \,\,\,\,\, \lim_{n \to \infty}\frac{a_n}{b_n} = L\in\mathbb{R}\cup\{\pm\infty\} \,\, \implies \,\,\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L $$ In every book/article/internet post I found the following proof which is clearly true if $L\in\mathbb{R}$ : $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} \left(1 - \frac{b_n}{b_{n+1}} \right) = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n+1}} = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}}$$ and, hence, $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} =  \left(1 - \frac{b_n}{b_{n+1}} \right)^{-1} \left(\frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}} \right)$$ Since $B \neq 1$ , the limit on the RHS exists and $$\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = (1- B)^{-1}(L - L B) = L$$ My question is: What if $L=+\infty$ ? Does the same result hold? Because the last relation might be false for $B>0$ ...","The Converse Stolz-Cesaro theorem states that if is strictly monotone and divergent and: In every book/article/internet post I found the following proof which is clearly true if : and, hence, Since , the limit on the RHS exists and My question is: What if ? Does the same result hold? Because the last relation might be false for ...","(b_n)_n  \lim_{n \to \infty}\frac{b_n}{b_{n+1}} = B \in \mathbb{R} \setminus
\{1\} , \,\,\,\,\, \lim_{n \to \infty}\frac{a_n}{b_n} = L\in\mathbb{R}\cup\{\pm\infty\} \,\,
\implies \,\,\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L  L\in\mathbb{R} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} \left(1 - \frac{b_n}{b_{n+1}} \right) = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n+1}} = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} =  \left(1 - \frac{b_n}{b_{n+1}} \right)^{-1} \left(\frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}} \right) B \neq 1 \lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = (1- B)^{-1}(L - L B) = L L=+\infty B>0","['real-analysis', 'sequences-and-series', 'limits']"
18,Help with this 2 variables limit,Help with this 2 variables limit,,"I'm asked to calculate this limit: $$ \lim_{(x,y)\rightarrow(0,0)}\frac{\ln(1+x)+\ln(1+y)}{x+y} $$ After calculating iterated limits and using some directions ( $y=\lambda x$ and $ y=\lambda x^2$ ) all I can deduce is that the limit is actually $1$ , but using polar coordinates I'm not able to prove it. Is there any other way to solve it?","I'm asked to calculate this limit: After calculating iterated limits and using some directions ( and ) all I can deduce is that the limit is actually , but using polar coordinates I'm not able to prove it. Is there any other way to solve it?","
\lim_{(x,y)\rightarrow(0,0)}\frac{\ln(1+x)+\ln(1+y)}{x+y}
 y=\lambda x  y=\lambda x^2 1","['limits', 'multivariable-calculus', 'limits-without-lhopital']"
19,Error in an exponential/logarithmic limit calculation,Error in an exponential/logarithmic limit calculation,,"Below is my calculation, but the result is incorrect. It should be $21$ ( WolframAlpha ), but I get $2$ . My textbook explains a different way to get the correct answer, but I couldn't figure out why my calculation leads to an incorrect result. Which part did I get wrong? $$\lim_{x \to 0}\frac{2}{x} \ln \frac{e^x(e^{20x}-1)}{20(e^x-1)}$$ $$=\lim_{x \to 0}\frac{2}{x} \ln (e^x \times \frac{e^{20x}-1}{20x} \times \frac{x}{e^x-1})$$ $$=\lim_{x \to 0}\frac{2}{x} \ln (e^x \times 1 \times 1)$$ $$=\lim_{x \to 0}\frac{2}{x} \ln {e^x}=\lim_{x \to 0}\frac{2}{x} \times x=2$$","Below is my calculation, but the result is incorrect. It should be ( WolframAlpha ), but I get . My textbook explains a different way to get the correct answer, but I couldn't figure out why my calculation leads to an incorrect result. Which part did I get wrong?",21 2 \lim_{x \to 0}\frac{2}{x} \ln \frac{e^x(e^{20x}-1)}{20(e^x-1)} =\lim_{x \to 0}\frac{2}{x} \ln (e^x \times \frac{e^{20x}-1}{20x} \times \frac{x}{e^x-1}) =\lim_{x \to 0}\frac{2}{x} \ln (e^x \times 1 \times 1) =\lim_{x \to 0}\frac{2}{x} \ln {e^x}=\lim_{x \to 0}\frac{2}{x} \times x=2,"['real-analysis', 'limits', 'analysis', 'logarithms', 'exponential-function']"
20,How do I evaluate $\displaystyle \lim_{x \to -\infty}\left(\sqrt{x^2-8x+1}-x\right)$?,How do I evaluate ?,\displaystyle \lim_{x \to -\infty}\left(\sqrt{x^2-8x+1}-x\right),"As asked in the title. I want to know how can I show that the value approaches to positive infinity as x approaches to negative infinity,without looking at its graph. Besides, I know that I can use the formula $\displaystyle (a+b)(a-b)=a^2-b^2$ , but then I don't know what should I do next.","As asked in the title. I want to know how can I show that the value approaches to positive infinity as x approaches to negative infinity,without looking at its graph. Besides, I know that I can use the formula , but then I don't know what should I do next.",\displaystyle (a+b)(a-b)=a^2-b^2,"['calculus', 'limits']"
21,Evaluate $\lim\limits_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}}$.,Evaluate .,\lim\limits_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}},"Numerical experimentation suggests that the following limit converges. $$L=\lim_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}}$$ According to both Desmos and Wolfram, for $n=10^5, 10^6, 10^7$ , the values of $L$ are $0.504116, 0.504069, 0.504068$ , respectively. Does this limit converge, and if so, is there a closed form? By looking at the graph of $y=\sin{\left(\frac{n}{x}\right)}$ from $x=1$ to $x=n$ , I can see why the limit should converge, but I don't know how to prove this rigorously. I doubt there is a closed form. (In case you're wondering where this question comes from, I just made it up after thinking about another question .)","Numerical experimentation suggests that the following limit converges. According to both Desmos and Wolfram, for , the values of are , respectively. Does this limit converge, and if so, is there a closed form? By looking at the graph of from to , I can see why the limit should converge, but I don't know how to prove this rigorously. I doubt there is a closed form. (In case you're wondering where this question comes from, I just made it up after thinking about another question .)","L=\lim_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}} n=10^5, 10^6, 10^7 L 0.504116, 0.504069, 0.504068 y=\sin{\left(\frac{n}{x}\right)} x=1 x=n","['real-analysis', 'limits', 'trigonometry']"
22,Simpler approach when calculating $\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x}$?,Simpler approach when calculating ?,\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x},"I know this is a really simple problem, but I can't figure out what logical mistake I am making on my approach : $$\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x}$$ Dividing numerator and denominator with $4^x$ , $$ \lim_{x\to0} \frac{({\frac{e^2}{4}) }^x-{(\frac{e}{4}})^x}{1 - ({\frac{1}{2}})^x} $$ $\left(\frac{1}{2}\right)^x$ and $\left(\frac{e}{4}\right)^x$ converge to $0,$ so I thought this limit converges to $1$ . (+ additional approach) $$     \lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x} \\     = \lim_{x\to0} \frac{e^{2x} - e^x}{x} \cdot \frac{x}{4^x - 2^x} \\     = \lim_{x\to0} \left(\frac{e^{2x} - 1}{x} - \frac{e^x - 1}{x} \right) \cdot \frac{1}{\left(\frac{4^x - 1}{x} - \frac{2^x - 1}{x} \right)} $$ And this becomes $(2-1) \times \frac{1}{\ln4 - \ln2 } = \frac{1}{\ln2}$ , using the definition of derivative. I wonder if there's a simpler solution without using L'Hopital's rule.","I know this is a really simple problem, but I can't figure out what logical mistake I am making on my approach : Dividing numerator and denominator with , and converge to so I thought this limit converges to . (+ additional approach) And this becomes , using the definition of derivative. I wonder if there's a simpler solution without using L'Hopital's rule.","\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x} 4^x  \lim_{x\to0} \frac{({\frac{e^2}{4}) }^x-{(\frac{e}{4}})^x}{1 - ({\frac{1}{2}})^x}  \left(\frac{1}{2}\right)^x \left(\frac{e}{4}\right)^x 0, 1 
    \lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x} \\
    = \lim_{x\to0} \frac{e^{2x} - e^x}{x} \cdot \frac{x}{4^x - 2^x} \\
    = \lim_{x\to0} \left(\frac{e^{2x} - 1}{x} - \frac{e^x - 1}{x} \right) \cdot \frac{1}{\left(\frac{4^x - 1}{x} - \frac{2^x - 1}{x} \right)}
 (2-1) \times \frac{1}{\ln4 - \ln2 } = \frac{1}{\ln2}","['limits', 'convergence-divergence', 'eulers-number-e']"
23,A functional calculus question,A functional calculus question,,"Let $f(x)$ be a continuous function in the real number set, such that $$f(x)=\begin{cases} \pi, \,\, \text{if} \,\, x> 1\\  g(x), \,\,\text{if}\,\,x≤ 1\end{cases}$$ Which of these statements are always correct? 1. $$\lim_{x\to 1} g(x)=\pi$$ 2. $$g(1)=\pi$$ 3. $$\lim_{x\to 1^-}\frac{f(x)}{g(x)}=1$$ My attempts. Maybe I can not see the right connection about the continuity between $f(x)$ and $g(x)$ . That is my first problem. We have $$f(1)=\lim_{x\to 1^+} f(x)=\pi=\lim_{x\to\ 1^-}f(x)=g(1)\implies \pi=g(1) $$ I think this doesn't imply, $\lim_{x\to 1} g(x)=\pi$ . I want to say that about $3$ , that is not always correct. Because, it can be $$\lim_{x\to 1^-}g(x)=0$$ But, $$g(1)=\pi$$ So, my answer is $2$ . It is possible that, I am completely wrong. I am not sure, what is going on here, exactly.","Let be a continuous function in the real number set, such that Which of these statements are always correct? 1. 2. 3. My attempts. Maybe I can not see the right connection about the continuity between and . That is my first problem. We have I think this doesn't imply, . I want to say that about , that is not always correct. Because, it can be But, So, my answer is . It is possible that, I am completely wrong. I am not sure, what is going on here, exactly.","f(x) f(x)=\begin{cases} \pi, \,\, \text{if} \,\, x> 1\\  g(x), \,\,\text{if}\,\,x≤ 1\end{cases} \lim_{x\to 1} g(x)=\pi g(1)=\pi \lim_{x\to 1^-}\frac{f(x)}{g(x)}=1 f(x) g(x) f(1)=\lim_{x\to 1^+} f(x)=\pi=\lim_{x\to\ 1^-}f(x)=g(1)\implies \pi=g(1)
 \lim_{x\to 1} g(x)=\pi 3 \lim_{x\to 1^-}g(x)=0 g(1)=\pi 2","['real-analysis', 'calculus', 'limits', 'continuity']"
24,Find $\sum_{k=1}^\infty\frac{1}{x_k^2-1}$ where $x_1=2$ and $x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2}$ for $n \ge 2$,Find  where  and  for,\sum_{k=1}^\infty\frac{1}{x_k^2-1} x_1=2 x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2} n \ge 2,"Given $x_1=2$ and $x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2}, n\geq 2$ Prove that $y_n=\sum_{k=1}^{n}\frac{1}{x_k^2-1}, n\geq 1$ converges and find its limit. To prove a convergence we can just estimate $x_n > n$ , therefore $y_n<z_n$ , where $z_n=\sum_{k=1}^{n}\frac{1}{k^2-1}$ and $z_n$ converges, then $y_n$ converges too. We can notice that $x_n^2+2x_n+5=(x_n+1)^2+4$ . So $x_{n+1}$ is one of the roots of the equation: $x_{n+1}^2-(x_n+1)x_{n+1}-1=0$ So $x_{n+1}^2-1=(x_n+1)x_{n+1}$ and therefore: $y_n=\sum_{k=1}^n \frac{1}{(x_{n-1}+1)x_{n}}$ I'm stuck here.","Given and Prove that converges and find its limit. To prove a convergence we can just estimate , therefore , where and converges, then converges too. We can notice that . So is one of the roots of the equation: So and therefore: I'm stuck here.","x_1=2 x_{n+1}=\frac{x_n+1+\sqrt{x_n^2+2x_n+5}}{2}, n\geq 2 y_n=\sum_{k=1}^{n}\frac{1}{x_k^2-1}, n\geq 1 x_n > n y_n<z_n z_n=\sum_{k=1}^{n}\frac{1}{k^2-1} z_n y_n x_n^2+2x_n+5=(x_n+1)^2+4 x_{n+1} x_{n+1}^2-(x_n+1)x_{n+1}-1=0 x_{n+1}^2-1=(x_n+1)x_{n+1} y_n=\sum_{k=1}^n \frac{1}{(x_{n-1}+1)x_{n}}","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
25,Why does $f(x_i)-f(x_{i-1})$ not go to $0$ when finding arc length/surface area?,Why does  not go to  when finding arc length/surface area?,f(x_i)-f(x_{i-1}) 0,"I didn't really know what to title the question, but in class, we found the formula for surface area was: $A=\sum\limits_{i=1}^n\pi[f(x_i)+f(x_{i-1})]\sqrt{(x_i-x_{i-1})^2+(f(x_i)-f(x_{i-1}))^2}$ because surface area of a frustum is $A=\pi(R+r)l$ where $l$ is slant height. This makes sense to me but then the next step says that because $f(x_i)\approx f(x_i*)$ and $f(x_{i-1})\approx f(x_i*)$ . Then they say $A=2\pi f(x_i*)\sqrt{1+f'(x_i*)^2}$ . The square root was replace because of MVT which made sense to me, but if the approximation with $f(x_i*)$ is valid, then why wouldn't the square root just have the $f(x)$ part just go to $0$ and it'd just be $\Delta x$ . If that substitution is invalid, then why does it work in the first part of the formula to give us $2f(x_i*)$ . Essentially what I'm asking is why are we able to say $f(x_i)\approx f(x_i*)$ and $f(x_{i-1})\approx f(x_i*)$ for the first part of the formula, but can't put those in for the part under the square root. I know this result would give us $\int2\pi f(x)dx$ which is wrong because it should be $ds$ , but it feels wrong to apply the substitution in one place but not the other. Thank you in advance for your help!","I didn't really know what to title the question, but in class, we found the formula for surface area was: because surface area of a frustum is where is slant height. This makes sense to me but then the next step says that because and . Then they say . The square root was replace because of MVT which made sense to me, but if the approximation with is valid, then why wouldn't the square root just have the part just go to and it'd just be . If that substitution is invalid, then why does it work in the first part of the formula to give us . Essentially what I'm asking is why are we able to say and for the first part of the formula, but can't put those in for the part under the square root. I know this result would give us which is wrong because it should be , but it feels wrong to apply the substitution in one place but not the other. Thank you in advance for your help!",A=\sum\limits_{i=1}^n\pi[f(x_i)+f(x_{i-1})]\sqrt{(x_i-x_{i-1})^2+(f(x_i)-f(x_{i-1}))^2} A=\pi(R+r)l l f(x_i)\approx f(x_i*) f(x_{i-1})\approx f(x_i*) A=2\pi f(x_i*)\sqrt{1+f'(x_i*)^2} f(x_i*) f(x) 0 \Delta x 2f(x_i*) f(x_i)\approx f(x_i*) f(x_{i-1})\approx f(x_i*) \int2\pi f(x)dx ds,"['calculus', 'integration', 'limits']"
26,find $\lim_{n\to\infty}x_n= \left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right)$,find,\lim_{n\to\infty}x_n= \left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right),"I want to determine the limit of the sequence $$x_n=\left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right)$$ Since the sequence is of the form $x_n=\frac{v_n}{v_n}$ with $v_n$ a positively divergent sequence, I thought to apply the Stolz criterion and find the limit of $$x_n=\frac{{v_{n+1}}-u_n}{v_{n+1}-v_n}$$ However I don't get anything concrete that allows me to find the limit, any help please?","I want to determine the limit of the sequence Since the sequence is of the form with a positively divergent sequence, I thought to apply the Stolz criterion and find the limit of However I don't get anything concrete that allows me to find the limit, any help please?",x_n=\left(\frac{e\sqrt[3]{e}...\sqrt[n]{e}}{n}\right) x_n=\frac{v_n}{v_n} v_n x_n=\frac{{v_{n+1}}-u_n}{v_{n+1}-v_n},"['calculus', 'sequences-and-series', 'limits']"
27,How to solve derivative/limit of $f(x)=x\sqrt{4-x^2}$,How to solve derivative/limit of,f(x)=x\sqrt{4-x^2},"I'm trying to differentiate $x\sqrt{4-x^2}$ using the definition of derivative. So it would be something like $$\underset{h\to 0}{\text{lim}}\frac{(h+x) \sqrt{4-\left(h^2+2 h x+x^2\right)}-x \sqrt{4-x^2}}{h}$$ I was trying to solve and I just can end up with something like $$\underset{h\to 0}{\text{lim}}\frac{(x+h)\sqrt{4-x^2-2xh-h^2}-x\sqrt{4-x^2}}h \cdot \frac{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}$$ $$\underset{h\to 0}{\text{lim}}\frac{-3x^2h-3xh^2+4h-h^3+\sqrt{4-x^2}-\sqrt{4-x^2-2xh+h^2}}{h\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}$$ Now if I group on h, I will have some tricky 3 instead of 2. The idea is I should have something like $h(2x^2+4)$ that would cancel up. I'm quite stuck can I ask a little of help? I know wolframalpha exists but it refuses to create the step by step solution with the error ""Ops we don't have a step by step solution for this query"". The final result shall be $$-\frac{2 \left(x^2-2\right)}{\sqrt{4-x^2}}$$","I'm trying to differentiate using the definition of derivative. So it would be something like I was trying to solve and I just can end up with something like Now if I group on h, I will have some tricky 3 instead of 2. The idea is I should have something like that would cancel up. I'm quite stuck can I ask a little of help? I know wolframalpha exists but it refuses to create the step by step solution with the error ""Ops we don't have a step by step solution for this query"". The final result shall be",x\sqrt{4-x^2} \underset{h\to 0}{\text{lim}}\frac{(h+x) \sqrt{4-\left(h^2+2 h x+x^2\right)}-x \sqrt{4-x^2}}{h} \underset{h\to 0}{\text{lim}}\frac{(x+h)\sqrt{4-x^2-2xh-h^2}-x\sqrt{4-x^2}}h \cdot \frac{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}}{\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}} \underset{h\to 0}{\text{lim}}\frac{-3x^2h-3xh^2+4h-h^3+\sqrt{4-x^2}-\sqrt{4-x^2-2xh+h^2}}{h\sqrt{4-x^2-2xh-h^2}+\sqrt{4-x^2}} h(2x^2+4) -\frac{2 \left(x^2-2\right)}{\sqrt{4-x^2}},['limits']
28,Can the exponential map of $SO(3)$ be expressed as a limit?,Can the exponential map of  be expressed as a limit?,SO(3),"According to this textbook (page 14), the exponential map of some Lie algebras (like $SO(2)$ ) can be thought of as repeated multiplication of matrices with infinitesimally small rotations. $$ \lim_{k\to\infty} \left(\mathbf{I} + \alpha_a\frac{X_a}{k}\right)^k = \sum_{m=0}^{\infty} \frac{1}{m!}\left(\alpha_aX_a\right)^m \equiv e^{\alpha_aX_a}, $$ where $X_a$ is the $a$ -th generator (basis vector) of the Lie algebra and $\alpha_a$ is the coefficient of the $a$ -th basis vector (the textbook seems to be using Einstein's summation notation). I feel like this derivation of the exponential map for ""sphere-like"" manifolds like $SO(2)$ or $SO(3)$ makes sense and thus I wanted to test whether this intuitive definition via a limit, when simplified, can indeed lead to the analytical version of the exponential map. I therefore first decided to use Python's symbolic math package SymPy to verify whether the equation holds for the Lie algebra of $SO(2)$ and SymPy was able to simplify the symbolic limit into the analytical form of the exponential map (3.6) in the textbook. I plugged the generator $X_{SO(2)}$ into the limit and after simplification, I got the expected analytical form $\exp_{SO(2)}(x)$ . $$ \begin{aligned} X_{SO(2)} &= \begin{bmatrix}0 & -x\\x & 0\end{bmatrix}\\ \exp_{SO(2)}(x) &= \begin{bmatrix}\cos x & -\sin x\\\sin x & \cos x\end{bmatrix} \end{aligned}  $$ However, when I tried to simplify the limit with generators for $SO(3)$ , the analytical expression and the limit were not the same! I wondered whether the same limit could be used to derive the analytical form of the exponential map of $SO(3)$ , so I found the generator $X_{SO(3)}$ of $SO(3)$ and the expected analytical exponential map $\exp_{SO(3)}(X)$ in this paper (page 5, Example 3 and 4): $$ \begin{aligned} X_{SO(3)} &= \begin{bmatrix}0 & -z & y\\ z & 0 & -x\\ -y & x & 0\end{bmatrix}\\ \exp_{SO(3)}(X) &= \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta}) \end{aligned}  $$ I assume that $\theta = \sqrt{x^2+y^2+z^2}$ . I expected the following to be true: $$ \lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k \stackrel{?}{=} \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\\ \lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k - \left(\mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\right) \stackrel{?}{=} \begin{bmatrix}0 & 0 &0\\0 & 0 &0\\0 & 0 &0\end{bmatrix} $$ However, after attempting to use SymPy to simplify the expression above, I did not get a $\mathbf{0}$ matrix. It seems like one of the following is true: I might be using a wrong analytical form $\exp_{SO(3)}(x,y,z)$ . I might misunderstood what $\theta$ stands for in $\exp_{SO(3)}(x,y,z)$ . I made a mistake when re-writing the expressions into SymPy. SymPy is unable to correctly simplify the expression. The limit does not hold for $SO(3)$ . What did I do wrong? And if the limit does not hold for $SO(3)$ , what are some other ways to derive the exponential map for $SO(3)$ in an intuitive and satisfying way? Below is the Python/SymPy code I used: from sympy import * init_printing()   x, y, z = symbols('x y z') G = Matrix([     [0, -z, y],     [z, 0, -x],     [-y, x, 0] ])  # The analytic expression of the exponential map omega = sqrt(x*x + y*y + z*z) analytic_expression = Identity(3).as_explicit() + sin(omega) * G + (1 - cos(omega)) * G**2  # Expressing the exponential map as a limit n = symbols('n', integer=True) limit_expression = (Identity(3).as_explicit() + G/n)**n limit_expression = limit_expression.limit(n, oo)  # If both expressions are exactly the same, their difference should be the zero matrix diff_matrix = simplify(analytic_expression - limit_expression)  # diff_matrix is NOT zero. Why? print(diff_matrix) EDIT: To rule-out the possibility of a bug in SymPy, I have also tried to simplify the expression using Wolfram Mathematica (I assume Mathematica is more reliable than SymPy). However, not even Mathematica simplified the difference between the limit and the analytical solution into the $\mathbf{0}$ matrix. The Mathematica code: Clear[""Global`*""] X = {     {0, -z, y},     {z, 0, -x},     {-y, x, 0} }  omega = Sqrt[x^2 + y^2 + z^2]  analyticExpression = IdentityMatrix[3] + Sin[omega]*X + (1 - Cos[omega]) * (X.X) limitExpression = Limit[MatrixPower[IdentityMatrix[3] + X/n, n], n -> Infinity]  (analyticExpression - limitExpression) // FullSimplify // MatrixForm","According to this textbook (page 14), the exponential map of some Lie algebras (like ) can be thought of as repeated multiplication of matrices with infinitesimally small rotations. where is the -th generator (basis vector) of the Lie algebra and is the coefficient of the -th basis vector (the textbook seems to be using Einstein's summation notation). I feel like this derivation of the exponential map for ""sphere-like"" manifolds like or makes sense and thus I wanted to test whether this intuitive definition via a limit, when simplified, can indeed lead to the analytical version of the exponential map. I therefore first decided to use Python's symbolic math package SymPy to verify whether the equation holds for the Lie algebra of and SymPy was able to simplify the symbolic limit into the analytical form of the exponential map (3.6) in the textbook. I plugged the generator into the limit and after simplification, I got the expected analytical form . However, when I tried to simplify the limit with generators for , the analytical expression and the limit were not the same! I wondered whether the same limit could be used to derive the analytical form of the exponential map of , so I found the generator of and the expected analytical exponential map in this paper (page 5, Example 3 and 4): I assume that . I expected the following to be true: However, after attempting to use SymPy to simplify the expression above, I did not get a matrix. It seems like one of the following is true: I might be using a wrong analytical form . I might misunderstood what stands for in . I made a mistake when re-writing the expressions into SymPy. SymPy is unable to correctly simplify the expression. The limit does not hold for . What did I do wrong? And if the limit does not hold for , what are some other ways to derive the exponential map for in an intuitive and satisfying way? Below is the Python/SymPy code I used: from sympy import * init_printing()   x, y, z = symbols('x y z') G = Matrix([     [0, -z, y],     [z, 0, -x],     [-y, x, 0] ])  # The analytic expression of the exponential map omega = sqrt(x*x + y*y + z*z) analytic_expression = Identity(3).as_explicit() + sin(omega) * G + (1 - cos(omega)) * G**2  # Expressing the exponential map as a limit n = symbols('n', integer=True) limit_expression = (Identity(3).as_explicit() + G/n)**n limit_expression = limit_expression.limit(n, oo)  # If both expressions are exactly the same, their difference should be the zero matrix diff_matrix = simplify(analytic_expression - limit_expression)  # diff_matrix is NOT zero. Why? print(diff_matrix) EDIT: To rule-out the possibility of a bug in SymPy, I have also tried to simplify the expression using Wolfram Mathematica (I assume Mathematica is more reliable than SymPy). However, not even Mathematica simplified the difference between the limit and the analytical solution into the matrix. The Mathematica code: Clear[""Global`*""] X = {     {0, -z, y},     {z, 0, -x},     {-y, x, 0} }  omega = Sqrt[x^2 + y^2 + z^2]  analyticExpression = IdentityMatrix[3] + Sin[omega]*X + (1 - Cos[omega]) * (X.X) limitExpression = Limit[MatrixPower[IdentityMatrix[3] + X/n, n], n -> Infinity]  (analyticExpression - limitExpression) // FullSimplify // MatrixForm","SO(2) 
\lim_{k\to\infty} \left(\mathbf{I} + \alpha_a\frac{X_a}{k}\right)^k = \sum_{m=0}^{\infty} \frac{1}{m!}\left(\alpha_aX_a\right)^m \equiv e^{\alpha_aX_a},
 X_a a \alpha_a a SO(2) SO(3) SO(2) X_{SO(2)} \exp_{SO(2)}(x) 
\begin{aligned}
X_{SO(2)} &= \begin{bmatrix}0 & -x\\x & 0\end{bmatrix}\\
\exp_{SO(2)}(x) &= \begin{bmatrix}\cos x & -\sin x\\\sin x & \cos x\end{bmatrix}
\end{aligned} 
 SO(3) SO(3) X_{SO(3)} SO(3) \exp_{SO(3)}(X) 
\begin{aligned}
X_{SO(3)} &= \begin{bmatrix}0 & -z & y\\ z & 0 & -x\\ -y & x & 0\end{bmatrix}\\
\exp_{SO(3)}(X) &= \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})
\end{aligned} 
 \theta = \sqrt{x^2+y^2+z^2} 
\lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k \stackrel{?}{=} \mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\\
\lim_{k\to\infty} \left(\mathbf{I} + \frac{X_{SO(3)}}{k}\right)^k - \left(\mathbf{I} + X_{SO(3)} \sin{\theta} + X^2_{SO(3)} (1 - \cos{\theta})\right) \stackrel{?}{=} \begin{bmatrix}0 & 0 &0\\0 & 0 &0\\0 & 0 &0\end{bmatrix}
 \mathbf{0} \exp_{SO(3)}(x,y,z) \theta \exp_{SO(3)}(x,y,z) SO(3) SO(3) SO(3) \mathbf{0}","['limits', 'lie-groups', 'lie-algebras', 'rotations']"
29,"$x_{n+1}=-1+\sqrt[n]{1+nx_n}$, $x_1>0$ limits",",  limits",x_{n+1}=-1+\sqrt[n]{1+nx_n} x_1>0,"Let $x_{n+1}=-1+\sqrt[n]{1+nx_n}$ , $x_1>0$ . How to find $\lim_{n\to\infty} n x_n$ , $\lim_{n\to\infty} \frac{x_{n+1}}{x_n}$ ? I don't know from where to start (I only concluded that $x_n$ is decreasing by Bernoulli's inequality and bounded from below, therefore it converges to 0. Also, I think that $\lim_{n\to\infty} n x_n$ can be calculated by Stolz theorem). Any help is welcome. Thanks in advance.","Let , . How to find , ? I don't know from where to start (I only concluded that is decreasing by Bernoulli's inequality and bounded from below, therefore it converges to 0. Also, I think that can be calculated by Stolz theorem). Any help is welcome. Thanks in advance.",x_{n+1}=-1+\sqrt[n]{1+nx_n} x_1>0 \lim_{n\to\infty} n x_n \lim_{n\to\infty} \frac{x_{n+1}}{x_n} x_n \lim_{n\to\infty} n x_n,"['real-analysis', 'limits', 'limsup-and-liminf']"
30,Proving $\lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2}$ with $\epsilon- \delta$,Proving  with,\lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2} \epsilon- \delta,"Today, I spent some hours trying to solve this problem but I can't reach the conclusion: Show with $\epsilon-\delta $ definition the following limit: $$\lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2}$$ So, first of all, I set up the inequality: $$\left|\frac{\ln(x)}{2\cdot \ln(x+1)}-\frac{1}{2}\right|<\epsilon$$ Then: $$\frac{1}{2}-\epsilon<\frac{\ln(x)}{2\cdot \ln(x+1)}<\frac{1}{2}+\epsilon \implies (1-2\epsilon)\cdot \ln(x+1) < \ln(x) < (1+2\epsilon)\cdot \ln(x+1)$$ And elevating with base $e$ to cancel the $\ln(x)$ : $$(x+1)^{1-2\epsilon}<x<(x+1)^{1+2\epsilon}$$ I want to apply the limit $\lim_{x \to +\infty}\frac{(1+f(x))^\alpha-1}{\alpha\cdot f(x)}=1$ with $f(x) \to_{x \to 0} 0$ . So, I have: $$x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon}<x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1+2\epsilon}$$ Then: $$ \left\{\begin{matrix} x>x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon} \\ x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x} \right )^{1+2\epsilon} \end{matrix}\right. $$ So: $$ \left\{\begin{matrix} x^{2\epsilon} - 1>\left(1+\frac{1}{x}\right)^{1-2\epsilon} - 1 \\ \frac{1}{x^{2\epsilon}}-1<\left(1+\frac{1}{x} \right )^{1+2\epsilon}-1 \end{matrix}\right. $$ When $x$ approaches $+\infty$ : $$ \sim \left\{\begin{matrix} x^{2\epsilon}-1>\frac{1-2\epsilon}{x} \\\frac{1}{x^{2\epsilon}}-1<\frac{1+2\epsilon}{x} \end{matrix}\right. $$ Notice that the second inequality is always true because $\frac{1}{x^{2\epsilon}}\to 0^+$ when $x \to +\infty$ , so that left side is negative while right side $\frac{1-2\epsilon}{x}$ is positive. Now, in some way I have to find $\delta = N$ . I noticed that: $$x^{2\epsilon}-1\sim x^{2\epsilon} \;\;\; x \to +\infty$$ and then for the first inequality: $$x^{2\epsilon}>\frac{1-2\epsilon}{x} \implies x> \delta(\epsilon)= N =(1-2\epsilon)^{1+2\epsilon}$$ Here, tere is a problem because when $\epsilon \to 0^+$ , $\delta = N \to 1$ and not to $+\infty$ . Is this anyway correct? Or, I can't apply asympothics reduction with $\epsilon - \delta$ ? Edit: I  all three solutions proposed, $\delta = N$ goes to infinity when $\epsilon \to 0^+$ . In my answer, $N$ tends to $1$ , so is my solution not corret? If it's yes, why? Thanks.","Today, I spent some hours trying to solve this problem but I can't reach the conclusion: Show with definition the following limit: So, first of all, I set up the inequality: Then: And elevating with base to cancel the : I want to apply the limit with . So, I have: Then: So: When approaches : Notice that the second inequality is always true because when , so that left side is negative while right side is positive. Now, in some way I have to find . I noticed that: and then for the first inequality: Here, tere is a problem because when , and not to . Is this anyway correct? Or, I can't apply asympothics reduction with ? Edit: I  all three solutions proposed, goes to infinity when . In my answer, tends to , so is my solution not corret? If it's yes, why? Thanks.","\epsilon-\delta  \lim_{x \to +\infty}\frac{\ln(x)}{2\cdot \ln(x+1)} = \frac{1}{2} \left|\frac{\ln(x)}{2\cdot \ln(x+1)}-\frac{1}{2}\right|<\epsilon \frac{1}{2}-\epsilon<\frac{\ln(x)}{2\cdot \ln(x+1)}<\frac{1}{2}+\epsilon \implies (1-2\epsilon)\cdot \ln(x+1) < \ln(x) < (1+2\epsilon)\cdot \ln(x+1) e \ln(x) (x+1)^{1-2\epsilon}<x<(x+1)^{1+2\epsilon} \lim_{x \to +\infty}\frac{(1+f(x))^\alpha-1}{\alpha\cdot f(x)}=1 f(x) \to_{x \to 0} 0 x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon}<x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1+2\epsilon} 
\left\{\begin{matrix}
x>x^{1-2\epsilon}\cdot\left(1+\frac{1}{x}\right)^{1-2\epsilon}
\\ x<x^{1+2\epsilon}\cdot\left(1+\frac{1}{x} \right )^{1+2\epsilon}
\end{matrix}\right.
 
\left\{\begin{matrix}
x^{2\epsilon} - 1>\left(1+\frac{1}{x}\right)^{1-2\epsilon} - 1
\\ \frac{1}{x^{2\epsilon}}-1<\left(1+\frac{1}{x} \right )^{1+2\epsilon}-1
\end{matrix}\right.
 x +\infty 
\sim
\left\{\begin{matrix}
x^{2\epsilon}-1>\frac{1-2\epsilon}{x}
\\\frac{1}{x^{2\epsilon}}-1<\frac{1+2\epsilon}{x}
\end{matrix}\right.
 \frac{1}{x^{2\epsilon}}\to 0^+ x \to +\infty \frac{1-2\epsilon}{x} \delta = N x^{2\epsilon}-1\sim x^{2\epsilon} \;\;\; x \to +\infty x^{2\epsilon}>\frac{1-2\epsilon}{x} \implies x> \delta(\epsilon)= N =(1-2\epsilon)^{1+2\epsilon} \epsilon \to 0^+ \delta = N \to 1 +\infty \epsilon - \delta \delta = N \epsilon \to 0^+ N 1","['calculus', 'limits', 'solution-verification', 'epsilon-delta']"
31,Can you move a limit into an exponent? [duplicate],Can you move a limit into an exponent? [duplicate],,"This question already has an answer here : Why is it justified to move the limit into the exponent? (1 answer) Closed 2 years ago . I’m attempting to solve a limit problem, and my current solution requires moving a limit inside the exponent. Symbolically, I’m attempting the following: $$\lim_{x\rightarrow c}e^{\frac{f(x)}{g(x)}}=e^{\lim_{x\rightarrow c}\frac{f(x)}{g(x)}}$$ At this point the various functions are such that I can show that the limit of $\frac{f(x)}{g(x)}$ is $0$ by L’Hôspital’s Rule, and I can conclude that the limit of the overall expression is therefore $1$ . But is this true? I know I can pull terms in and out of limits so long as those terms don’t rely on the variable with respect to which the limit is being evaluated, but can one move the limit inside the function and evaluate it this way? If yes, how do you prove that; if no, why?","This question already has an answer here : Why is it justified to move the limit into the exponent? (1 answer) Closed 2 years ago . I’m attempting to solve a limit problem, and my current solution requires moving a limit inside the exponent. Symbolically, I’m attempting the following: At this point the various functions are such that I can show that the limit of is by L’Hôspital’s Rule, and I can conclude that the limit of the overall expression is therefore . But is this true? I know I can pull terms in and out of limits so long as those terms don’t rely on the variable with respect to which the limit is being evaluated, but can one move the limit inside the function and evaluate it this way? If yes, how do you prove that; if no, why?",\lim_{x\rightarrow c}e^{\frac{f(x)}{g(x)}}=e^{\lim_{x\rightarrow c}\frac{f(x)}{g(x)}} \frac{f(x)}{g(x)} 0 1,['limits']
32,Find the right derivative of $\sin^2\left(x \sin \frac1x\right)$ at $x = 0$.,Find the right derivative of  at .,\sin^2\left(x \sin \frac1x\right) x = 0,"$$f(x)=\sin^2\left(x \sin \frac1x\right)$$ I was trying to find the right derivative of $f$ at $x = 0$ . ( In this question, it says to take the f(0) as f is right continuous at x = 0. Therefore, considering the right continuous at x = 0, I obtained f(0) = 0 ) before that, I know, I should find whether $f$ is the right differentiable at $x = 0$ . Is $f$ right differentiable at $x = 0?$ If so, what is the right derivative of $f$ at $0?$ I think $f$ is not right differentiable at $x = 0$ . It would be great if someone could clarify this. Any help would be appreciated.","I was trying to find the right derivative of at . ( In this question, it says to take the f(0) as f is right continuous at x = 0. Therefore, considering the right continuous at x = 0, I obtained f(0) = 0 ) before that, I know, I should find whether is the right differentiable at . Is right differentiable at If so, what is the right derivative of at I think is not right differentiable at . It would be great if someone could clarify this. Any help would be appreciated.",f(x)=\sin^2\left(x \sin \frac1x\right) f x = 0 f x = 0 f x = 0? f 0? f x = 0,"['limits', 'functions', 'derivatives']"
33,How to reach this sum from the following limit?,How to reach this sum from the following limit?,,"I have come across this in a probability question, and couldn't reach it by myself, and in the solution it's done in one step and I'm unsure as of why it's correct: $$\sum^{m-1}_{i=0}\binom{\lfloor{nt}\rfloor}{i}(\frac{\lambda}{n})^i(1-\frac{\lambda}{n})^{-i}(1-\frac{\lambda}{n})^{\lfloor{nt}\rfloor}$$ When $n \to \infty$ , The answer is: $$\sum^{m-1}_{i=0}\frac{(\lambda t)^i}{i!}e^{-\lambda t}$$ I know that formally I need to use the squeeze theorem for $\lfloor{nt}\rfloor$ , but working with intuition something seems off for me: $(1-\frac{\lambda}{n})^{\lfloor{n t\rfloor}}\to e^{-\lambda t}$ . ( $1-\frac{\lambda}{n})^{-i}\to 1$ . What confuses me is how did they get rid of ( $\frac{\lambda}{n})$ which goes to $0$ when $n\to \infty$ , I'm trying to open the binomial, but I'm not sure how to deal with the $(nt)!$ . Any help is really appreciated, thanks in advance!","I have come across this in a probability question, and couldn't reach it by myself, and in the solution it's done in one step and I'm unsure as of why it's correct: When , The answer is: I know that formally I need to use the squeeze theorem for , but working with intuition something seems off for me: . ( . What confuses me is how did they get rid of ( which goes to when , I'm trying to open the binomial, but I'm not sure how to deal with the . Any help is really appreciated, thanks in advance!",\sum^{m-1}_{i=0}\binom{\lfloor{nt}\rfloor}{i}(\frac{\lambda}{n})^i(1-\frac{\lambda}{n})^{-i}(1-\frac{\lambda}{n})^{\lfloor{nt}\rfloor} n \to \infty \sum^{m-1}_{i=0}\frac{(\lambda t)^i}{i!}e^{-\lambda t} \lfloor{nt}\rfloor (1-\frac{\lambda}{n})^{\lfloor{n t\rfloor}}\to e^{-\lambda t} 1-\frac{\lambda}{n})^{-i}\to 1 \frac{\lambda}{n}) 0 n\to \infty (nt)!,"['probability', 'limits', 'probability-distributions']"
34,Weaking the path test for multivariable limits,Weaking the path test for multivariable limits,,"The multivariable limit $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ exists and is equal to some scalar $L$ if and only if the limit $\lim_{t\to 1} f(r(t))$ exists and is equal to $L$ for all functions $r:\mathbb{R}\to \mathbb{R}^2$ such that $\lim_{t\to 1} r(t) = (x_0, y_0)$ . It is common to use this fact as a way to conclude that a certain limit does not exist, by checking that taking the limit along different paths leads to different limits or by showing that the limit along a certain path does not exist. I wonder if this characterizarion can be weakened in the following way: The multivariable limit $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ exists and is equal to some scalar $L$ if and only if $\lim_{y\to y_0} f(x_0,y)=L$ and $\lim_{x\to x_0} f(x,g(x)) = L$ for all functions $g:\mathbb{R}\to \mathbb{R}$ such that $\lim_{x\to x_0} g(x) = y_0$ . That is, it the limit exists and is the same going through all graphs of functions $g(x)$ and through the $y$ axis, can I conclude the existence of the multivariable limit? EDIT: I substituted my initial condition of $\lim_{y\to y_0} \lim_{x\to x_0} f(x,y)=L$ by $\lim_{x\to x_0} f(x,g(x)) = L$ since, as José Carlos Santos noted in the comments, formalizes better the idea of approaching the limit vertically.","The multivariable limit exists and is equal to some scalar if and only if the limit exists and is equal to for all functions such that . It is common to use this fact as a way to conclude that a certain limit does not exist, by checking that taking the limit along different paths leads to different limits or by showing that the limit along a certain path does not exist. I wonder if this characterizarion can be weakened in the following way: The multivariable limit exists and is equal to some scalar if and only if and for all functions such that . That is, it the limit exists and is the same going through all graphs of functions and through the axis, can I conclude the existence of the multivariable limit? EDIT: I substituted my initial condition of by since, as José Carlos Santos noted in the comments, formalizes better the idea of approaching the limit vertically.","\lim_{(x,y)\to (x_0,y_0)} f(x,y) L \lim_{t\to 1} f(r(t)) L r:\mathbb{R}\to \mathbb{R}^2 \lim_{t\to 1} r(t) = (x_0, y_0) \lim_{(x,y)\to (x_0,y_0)} f(x,y) L \lim_{y\to y_0} f(x_0,y)=L \lim_{x\to x_0} f(x,g(x)) = L g:\mathbb{R}\to \mathbb{R} \lim_{x\to x_0} g(x) = y_0 g(x) y \lim_{y\to y_0} \lim_{x\to x_0} f(x,y)=L \lim_{x\to x_0} f(x,g(x)) = L","['real-analysis', 'calculus', 'limits', 'multivariable-calculus']"
35,Find limit $\lim_{n \to \infty} \sum_{k=1}^{n} \frac{k^4}{k^5+n^5}$.,Find limit .,\lim_{n \to \infty} \sum_{k=1}^{n} \frac{k^4}{k^5+n^5},"Find the following limit: $$\lim_{n \to \infty} \sum_{k=1}^{n} \frac{k^4}{k^5+n^5}$$ I had an idea of using upper Riemann sum for function $x^4$ on interval $[0,1]$ but I don't know how to deal with $k^5$ in denominator which ruins my approach. Kindly asking for some help.",Find the following limit: I had an idea of using upper Riemann sum for function on interval but I don't know how to deal with in denominator which ruins my approach. Kindly asking for some help.,"\lim_{n \to \infty} \sum_{k=1}^{n} \frac{k^4}{k^5+n^5} x^4 [0,1] k^5","['calculus', 'integration', 'limits', 'summation', 'riemann-sum']"
36,Find the limit of given expressions:,Find the limit of given expressions:,,"Given sequences $$a_n=\int_0^1 (1-x^2)^n \,dx$$ and $$b_n=\int_0^1 (1-x^3)^n \,dx$$ where $n \in \mathbb{N}$ , find $$\displaystyle\lim_{n\to \infty}(10\sqrt[n]{a_n}+5\sqrt[n]{b_n}).$$ So far: I tried integrating expansions of $(1-x^2)^n$ and $(1-x^3)^n$ to get the series $C_0-\frac{C_1}{3}+\frac{C_2}{5}...\frac{(-1)^nC_n}{2n+1}$ and $C_0-\frac{C_1}{4}+\frac{C_2}{7}...\frac{(-1)^nC_n}{3n+1}$ . Since that was not working out, I tried integrating by parts: $$ \begin{align} a_n=\int_{0}^{1}(1-x^2)^ndx&=\left[x(1-x^2)^n\right]_{0}^{1}+2n\int_{0}^{1}x^2(1-x^2)^{n-1}dx \\\\&=0+2n\int_{0}^{1}\left[(1-(1-x^2))(1-x^2)^{n-1}\right]dx \\\\&=2na_{n-1}-2na_{n} \end{align} $$ then, with $a_0=1,\,a_1=\frac23,$ $$ a_{n}=\frac{2n}{2n+1}\cdot a_{n-1}, \quad n\ge1. $$ Hence it comes out as $$a_n=\frac{1\cdot2\cdot4\cdot6...2n}{1\cdot3\cdot5\cdot7...2n+1}$$ Similarly integrating $b_n$ , the result came out as: $$b_n=\frac{3n}{3n+1}\cdot b_{n-1}, \quad n\ge1. $$ $$b_0=1,\,b_1=\frac34$$ Transforming into $$b_n=\frac{1\cdot3\cdot6\cdot9...3n}{1\cdot4\cdot7\cdot10...3n+1}$$ Now I have to somehow put it into the given limit, but I have no idea how.","Given sequences and where , find So far: I tried integrating expansions of and to get the series and . Since that was not working out, I tried integrating by parts: then, with Hence it comes out as Similarly integrating , the result came out as: Transforming into Now I have to somehow put it into the given limit, but I have no idea how.","a_n=\int_0^1 (1-x^2)^n \,dx b_n=\int_0^1 (1-x^3)^n \,dx n \in \mathbb{N} \displaystyle\lim_{n\to \infty}(10\sqrt[n]{a_n}+5\sqrt[n]{b_n}). (1-x^2)^n (1-x^3)^n C_0-\frac{C_1}{3}+\frac{C_2}{5}...\frac{(-1)^nC_n}{2n+1} C_0-\frac{C_1}{4}+\frac{C_2}{7}...\frac{(-1)^nC_n}{3n+1}  \begin{align} a_n=\int_{0}^{1}(1-x^2)^ndx&=\left[x(1-x^2)^n\right]_{0}^{1}+2n\int_{0}^{1}x^2(1-x^2)^{n-1}dx \\\\&=0+2n\int_{0}^{1}\left[(1-(1-x^2))(1-x^2)^{n-1}\right]dx \\\\&=2na_{n-1}-2na_{n} \end{align}  a_0=1,\,a_1=\frac23,  a_{n}=\frac{2n}{2n+1}\cdot a_{n-1}, \quad n\ge1.  a_n=\frac{1\cdot2\cdot4\cdot6...2n}{1\cdot3\cdot5\cdot7...2n+1} b_n b_n=\frac{3n}{3n+1}\cdot b_{n-1}, \quad n\ge1.  b_0=1,\,b_1=\frac34 b_n=\frac{1\cdot3\cdot6\cdot9...3n}{1\cdot4\cdot7\cdot10...3n+1}","['calculus', 'limits', 'definite-integrals', 'binomial-coefficients']"
37,"Can we make a set out of $\epsilon$-$\delta$ pairs, given limit?","Can we make a set out of - pairs, given limit?",\epsilon \delta,"If I was given that $\lim_{x\to x_0}f(x)=L$ , for some $f:\mathbb R\to\mathbb R$ . This means that for every $\epsilon>0$ , there is some $\delta>0$ , such that $|x-x_0|<\delta\implies|f(x)-L|<\epsilon$ . These $\delta$ values for corresponding $\epsilon$ exist, but are unknown. Given this can I make a set like: $$S=\{(\epsilon,\delta)\in\mathbb R\times\mathbb R\mid (\epsilon>0)\wedge(\delta>0:(\forall x:|x-x_0|<\delta\implies|f(x)-L|<\epsilon))\}$$ (I didn't know how to write it in a way for everyone to understand). Essentially I want to make a set (or show it exists) of all possible $\epsilon$ - $\delta$ pairs satisfying the limit definition. Is the set possible? Why / why not? I can think of the Zorn's lemma, as this seems like a 'maximal set', but I don't know how to apply it. What kind of ordering would it be? What are the chains? Maybe I can make a family: $F=\{U_i\mid i\in\mathbb R\}:U_i=\{\delta\mid\forall x: |x-x_0|<\delta\implies |f(x)-L|<i\}$ ? I've only ever seen Zorn's lemma in a proof showing every vector space has a basis.","If I was given that , for some . This means that for every , there is some , such that . These values for corresponding exist, but are unknown. Given this can I make a set like: (I didn't know how to write it in a way for everyone to understand). Essentially I want to make a set (or show it exists) of all possible - pairs satisfying the limit definition. Is the set possible? Why / why not? I can think of the Zorn's lemma, as this seems like a 'maximal set', but I don't know how to apply it. What kind of ordering would it be? What are the chains? Maybe I can make a family: ? I've only ever seen Zorn's lemma in a proof showing every vector space has a basis.","\lim_{x\to x_0}f(x)=L f:\mathbb R\to\mathbb R \epsilon>0 \delta>0 |x-x_0|<\delta\implies|f(x)-L|<\epsilon \delta \epsilon S=\{(\epsilon,\delta)\in\mathbb R\times\mathbb R\mid (\epsilon>0)\wedge(\delta>0:(\forall x:|x-x_0|<\delta\implies|f(x)-L|<\epsilon))\} \epsilon \delta F=\{U_i\mid i\in\mathbb R\}:U_i=\{\delta\mid\forall x: |x-x_0|<\delta\implies |f(x)-L|<i\}","['limits', 'elementary-set-theory', 'epsilon-delta']"
38,Cannot understand the conclusion of the proof that $\lim_\limits{x \to \infty} ( 1 + \frac{1}{x})^{x} = e$,Cannot understand the conclusion of the proof that,\lim_\limits{x \to \infty} ( 1 + \frac{1}{x})^{x} = e,"I'm struggling to understand an unusual proof that $\lim \limits_{x \to \infty} \left( 1 + \frac{1}{x} \right)^{x} = e$ Note: before starting this proof we know already the following limit for $a_{n}$ $\lim \limits_{n \to \infty} \left( 1 + \frac{1}{n} \right)^{n} = e$ . The way the proof proceeds is the following: ""we know that $a_{n}$ is a growing sequence that goes to $e$ , therefore for each $\epsilon > 0 $ , there is a $v$ (which we can assume to be bigger than $\frac{1}{\epsilon}$ ) such that for each $n > v$ we have"": $e - \epsilon < \left( 1 + \dfrac{1}{n} ​\right)^{n} < e$ Here I understand that this comes from the fact that: $ \left| \left(1+\dfrac{1}{n} \right)^{n} - e  \right| < \epsilon$ and the member on the the RHS ( $e$ ) comes from the fact that $\epsilon + e > \epsilon$ but please correct me if I'm wrong. Now, assuming that $x > v+1$ and $[x]$ being the integer part of $x$ , we have $[x] > v$ and the author proceeds to squeeze $\left( 1 + \dfrac{1}{x}\right)^{x}$ as follows $\left( 1 + \dfrac{1}{[x] + 1}\right)^{[x]} < \left(1 + \dfrac{1}{x}\right)^{x} < \left( 1 + \dfrac{1}{[x]}\right)^{[x]+1}$ Then there are some additional steps here that I can follow without problems (I can add them if needed), at the end the author ends up with: $e - (1+e)\epsilon \leq \left( 1 + \dfrac{1}{x}\right)^{x} \leq e + e\epsilon$ and concludes by saying that this proves the original statement that $\lim\limits_{x \to \infty} \left( 1 + \frac{1}{x}\right)^{x} = e$ . This is the part that I do not understand, I could have understood that if it were $e-\epsilon$ on the left side and $e+\epsilon$ on the right side, that will fit the definition of the limit; is there any reason why a multiple of $\epsilon$ will continue to fit the definition? I spent hours for the past two days looking for an explanation of that or a similar proof and I couldn't find anything.","I'm struggling to understand an unusual proof that Note: before starting this proof we know already the following limit for . The way the proof proceeds is the following: ""we know that is a growing sequence that goes to , therefore for each , there is a (which we can assume to be bigger than ) such that for each we have"": Here I understand that this comes from the fact that: and the member on the the RHS ( ) comes from the fact that but please correct me if I'm wrong. Now, assuming that and being the integer part of , we have and the author proceeds to squeeze as follows Then there are some additional steps here that I can follow without problems (I can add them if needed), at the end the author ends up with: and concludes by saying that this proves the original statement that . This is the part that I do not understand, I could have understood that if it were on the left side and on the right side, that will fit the definition of the limit; is there any reason why a multiple of will continue to fit the definition? I spent hours for the past two days looking for an explanation of that or a similar proof and I couldn't find anything.",\lim \limits_{x \to \infty} \left( 1 + \frac{1}{x} \right)^{x} = e a_{n} \lim \limits_{n \to \infty} \left( 1 + \frac{1}{n} \right)^{n} = e a_{n} e \epsilon > 0  v \frac{1}{\epsilon} n > v e - \epsilon < \left( 1 + \dfrac{1}{n} ​\right)^{n} < e  \left| \left(1+\dfrac{1}{n} \right)^{n} - e  \right| < \epsilon e \epsilon + e > \epsilon x > v+1 [x] x [x] > v \left( 1 + \dfrac{1}{x}\right)^{x} \left( 1 + \dfrac{1}{[x] + 1}\right)^{[x]} < \left(1 + \dfrac{1}{x}\right)^{x} < \left( 1 + \dfrac{1}{[x]}\right)^{[x]+1} e - (1+e)\epsilon \leq \left( 1 + \dfrac{1}{x}\right)^{x} \leq e + e\epsilon \lim\limits_{x \to \infty} \left( 1 + \frac{1}{x}\right)^{x} = e e-\epsilon e+\epsilon \epsilon,"['real-analysis', 'limits', 'proof-explanation']"
39,What is the series of identities for these hyper-operations?,What is the series of identities for these hyper-operations?,,"Suppose we define the hyper-operations iteratively off the previous hyper-operation. We begin with the zeroth hyperoperation being addition, $$a\circ_0b:=a+b.$$ We then assert that $$a\circ_{n+1} b:=\exp(\ln(a)\circ_n\ln(b)).$$ Although not immediately obvious, $a\circ_1 b\equiv a\times b$ . So $\circ_1$ is equivalent to multiplication. But $a\circ_2 b\neq a^b$ , but instead reduces to $a^{\ln(b)}$ . Thus $\circ_2$ is not the traditional exponentiation (instead, the so-called ""powerlog"" in this video ). Doing so allows each hyper-operation to be associative and communitive over it's previous operation, which are nice properties that regular exponentiation does not have. My question is: What is the series of identities, $e_n$ , such that $a\circ_n e_n \equiv a$ , and what is the limit of $\lim_{n\to\infty}e_n$ ? We know that $e_0=0$ , as $a\circ_00\equiv a$ , since $a\circ_0b:=a+b$ . We also know that $e_1=1$ , as $a\circ_11\equiv a$ , since $a\circ_1b:=a\times b$ I can also deduce that $e_2=e$ , as $a\circ_2 e\equiv a$ , since $a\circ_2b:=a^{\ln b}$ . But I would like to know if there is a general formula for $e_n$ , and what the limit is (if any). Kind regards to any insight.","Suppose we define the hyper-operations iteratively off the previous hyper-operation. We begin with the zeroth hyperoperation being addition, We then assert that Although not immediately obvious, . So is equivalent to multiplication. But , but instead reduces to . Thus is not the traditional exponentiation (instead, the so-called ""powerlog"" in this video ). Doing so allows each hyper-operation to be associative and communitive over it's previous operation, which are nice properties that regular exponentiation does not have. My question is: What is the series of identities, , such that , and what is the limit of ? We know that , as , since . We also know that , as , since I can also deduce that , as , since . But I would like to know if there is a general formula for , and what the limit is (if any). Kind regards to any insight.",a\circ_0b:=a+b. a\circ_{n+1} b:=\exp(\ln(a)\circ_n\ln(b)). a\circ_1 b\equiv a\times b \circ_1 a\circ_2 b\neq a^b a^{\ln(b)} \circ_2 e_n a\circ_n e_n \equiv a \lim_{n\to\infty}e_n e_0=0 a\circ_00\equiv a a\circ_0b:=a+b e_1=1 a\circ_11\equiv a a\circ_1b:=a\times b e_2=e a\circ_2 e\equiv a a\circ_2b:=a^{\ln b} e_n,"['limits', 'hyperoperation']"
40,"If $f(x+y) = f(x)f(y)$ and $f(x)$ is monotone, prove $f(x)$ differentiable.","If  and  is monotone, prove  differentiable.",f(x+y) = f(x)f(y) f(x) f(x),"If $f(x)$ is monotone and $f(x)$ is not a constant, $\forall x,y \in (-\infty,+\infty)$ $f(x+y) = f(x)f(y)$ , prove that: (1) $f(0) = 1$ ; (2) $f(x)>0$ ; (3) $f(x)$ is continuous; (4) $f(x)$ is differentiable; I can prove (1) (2) but got stuck on (3) and (4), as $$ f'(x) = \lim_{\Delta x \to 0}\frac{ f(x+\Delta x)-f(x)}{\Delta x}=\lim_{\Delta x \to 0}\frac{f(x)[f(\Delta x)-1]}{\Delta x} = f(x)f'(0) $$ I Guess if it's differentiable at one point $x=0$ , it will be differentiable at any x, but how to prove $x=0$ is differentiable? Thank you!","If is monotone and is not a constant, , prove that: (1) ; (2) ; (3) is continuous; (4) is differentiable; I can prove (1) (2) but got stuck on (3) and (4), as I Guess if it's differentiable at one point , it will be differentiable at any x, but how to prove is differentiable? Thank you!","f(x) f(x) \forall x,y \in (-\infty,+\infty) f(x+y) = f(x)f(y) f(0) = 1 f(x)>0 f(x) f(x) 
f'(x) = \lim_{\Delta x \to 0}\frac{ f(x+\Delta x)-f(x)}{\Delta x}=\lim_{\Delta x \to 0}\frac{f(x)[f(\Delta x)-1]}{\Delta x} = f(x)f'(0)
 x=0 x=0","['real-analysis', 'calculus', 'limits', 'analysis']"
41,$\lim_{x\to\infty}$ when the function is defined only on a bounded subset of $\mathbb{R}$,when the function is defined only on a bounded subset of,\lim_{x\to\infty} \mathbb{R},"I know that, for example, if $$f(x)=\frac{e^{-\sin x}}{x}$$ for all real $x\gt 0$ , then $\lim_{x\to\infty}f(x)=0$ . However, how do we compute $\lim_{x\to\infty}$ when the function is defined only on a bounded subset of $\mathbb{R}$ ? If the domain of $g$ is the interval $[0,\pi]$ and $g=\sin$ on $[0,\pi]$ , is it true that $$\lim_{x\to\infty}g(x)=0?$$ It's true if and only if for every $\varepsilon\gt 0$ there exists $M\gt 0$ such that $|g(x)|\lt \varepsilon$ whenever $x\gt M$ . And it seems that for $x\in [0,\pi]$ we can make $|g(x)|$ arbitrarily close to $0$ and at the same time, we don't get to choose from any $x\gt\pi$ . Or is this a misinterpretation? Edit to make this clear: The definition of limit I am using is: If the domain of $f$ is $D$ , then $$\lim_{x\to\infty}f(x)=L\iff \forall \varepsilon \gt 0\, \exists M\gt 0 \,\forall x\in D:\, x\gt M\implies |f(x)-L|\lt \varepsilon .$$","I know that, for example, if for all real , then . However, how do we compute when the function is defined only on a bounded subset of ? If the domain of is the interval and on , is it true that It's true if and only if for every there exists such that whenever . And it seems that for we can make arbitrarily close to and at the same time, we don't get to choose from any . Or is this a misinterpretation? Edit to make this clear: The definition of limit I am using is: If the domain of is , then","f(x)=\frac{e^{-\sin x}}{x} x\gt 0 \lim_{x\to\infty}f(x)=0 \lim_{x\to\infty} \mathbb{R} g [0,\pi] g=\sin [0,\pi] \lim_{x\to\infty}g(x)=0? \varepsilon\gt 0 M\gt 0 |g(x)|\lt \varepsilon x\gt M x\in [0,\pi] |g(x)| 0 x\gt\pi f D \lim_{x\to\infty}f(x)=L\iff \forall \varepsilon \gt 0\, \exists M\gt 0 \,\forall x\in D:\, x\gt M\implies |f(x)-L|\lt \varepsilon .","['real-analysis', 'calculus', 'limits']"
42,Filling in details for calculation of the limit $\lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right)$,Filling in details for calculation of the limit,\lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right),"I want to evaluate the following limit using asymptotics \begin{equation} \lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) \tag{1} \end{equation} This is an example problem and this was the solution: \begin{gather} \frac{x^{3} +x}{1+x^{3}} =\left( 1+\frac{1}{x^{2}}\right)\left( 1+\frac{1}{x^{3}}\right)^{-1} =\left( 1+\frac{1}{x^{2}}\right)\left( 1-\frac{1}{x^{3}} +\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) \tag{2}\\ =1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{3} \end{gather} And then \begin{gather} \sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} =\left( 1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\right)^{\frac{1}{7}} \tag{4}\\ =1+\frac{1}{7} .\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{5} \end{gather} \begin{equation} \cos\frac{1}{x} =1-\frac{1}{2x^{2}} +\mathcal{O}\left(\frac{1}{x^{4}}\right) \tag{6} \end{equation} From above, we obtain: \begin{equation} \left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{7} \end{equation} Hence the required limit is: \begin{equation} \lim _{x\rightarrow \infty }\left(\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\right) =\frac{9}{14} \tag{8} \end{equation} I tried to fill in the details for the steps involved in the above steps. I supplied the details for $\displaystyle ( 3)$ as below: \begin{gather} \left( 1+\frac{1}{x^{2}}\right)\left( 1-\frac{1}{x^{3}} +\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) =1+\frac{1}{x^{2}} +\frac{1}{x^{3}}\left( -1-\frac{1}{x^{2}} +\left( x^{3} +x\right)\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) \tag{9}\\ =1+\frac{1}{x^{2}} +\frac{1}{x^{3}}\left( -1-\frac{1}{x^{2}} +\frac{\left( x^{3} +x\right)}{x^{6}}\mathcal{O}( 1)\right) =1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{10} \end{gather} where in (9) and (10), I have used the standard result: $\displaystyle \frac{\mathcal{O}( f( x))}{g( x)} =\mathcal{O}\left(\frac{f( x)}{g( x)}\right)$ , if $\displaystyle g( x) \neq 0$ . I tried to get (5) from (4) but failed. Then I supplied details for $\displaystyle ( 7)$ as below: \begin{gather*} \left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) -\mathcal{O}\left(\frac{1}{x^{4}}\right) =\frac{9}{14x^{2}} +\frac{1}{x^{3}}\left(\mathcal{O}( 1) -\mathcal{O}\left(\frac{1}{x}\right)\right)\\ =\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\\ \Longrightarrow \lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\lim _{x\rightarrow \infty }\left(\frac{9}{14} +\mathcal{O}\left(\frac{1}{x}\right)\right) =\frac{9}{14} \end{gather*} Any help in getting (5) from (4) is much appreciated. Thanks. \begin{equation*} \end{equation*} \begin{equation*} \end{equation*}","I want to evaluate the following limit using asymptotics This is an example problem and this was the solution: And then From above, we obtain: Hence the required limit is: I tried to fill in the details for the steps involved in the above steps. I supplied the details for as below: where in (9) and (10), I have used the standard result: , if . I tried to get (5) from (4) but failed. Then I supplied details for as below: Any help in getting (5) from (4) is much appreciated. Thanks.","\begin{equation}
\lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) \tag{1}
\end{equation} \begin{gather}
\frac{x^{3} +x}{1+x^{3}} =\left( 1+\frac{1}{x^{2}}\right)\left( 1+\frac{1}{x^{3}}\right)^{-1} =\left( 1+\frac{1}{x^{2}}\right)\left( 1-\frac{1}{x^{3}} +\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) \tag{2}\\
=1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{3}
\end{gather} \begin{gather}
\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} =\left( 1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\right)^{\frac{1}{7}} \tag{4}\\
=1+\frac{1}{7} .\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{5}
\end{gather} \begin{equation}
\cos\frac{1}{x} =1-\frac{1}{2x^{2}} +\mathcal{O}\left(\frac{1}{x^{4}}\right) \tag{6}
\end{equation} \begin{equation}
\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{7}
\end{equation} \begin{equation}
\lim _{x\rightarrow \infty }\left(\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\right) =\frac{9}{14} \tag{8}
\end{equation} \displaystyle ( 3) \begin{gather}
\left( 1+\frac{1}{x^{2}}\right)\left( 1-\frac{1}{x^{3}} +\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) =1+\frac{1}{x^{2}} +\frac{1}{x^{3}}\left( -1-\frac{1}{x^{2}} +\left( x^{3} +x\right)\mathcal{O}\left(\frac{1}{x^{6}}\right)\right) \tag{9}\\
=1+\frac{1}{x^{2}} +\frac{1}{x^{3}}\left( -1-\frac{1}{x^{2}} +\frac{\left( x^{3} +x\right)}{x^{6}}\mathcal{O}( 1)\right) =1+\frac{1}{x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) \tag{10}
\end{gather} \displaystyle \frac{\mathcal{O}( f( x))}{g( x)} =\mathcal{O}\left(\frac{f( x)}{g( x)}\right) \displaystyle g( x) \neq 0 \displaystyle ( 7) \begin{gather*}
\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right) -\mathcal{O}\left(\frac{1}{x^{4}}\right) =\frac{9}{14x^{2}} +\frac{1}{x^{3}}\left(\mathcal{O}( 1) -\mathcal{O}\left(\frac{1}{x}\right)\right)\\
=\frac{9}{14x^{2}} +\mathcal{O}\left(\frac{1}{x^{3}}\right)\\
\Longrightarrow \lim _{x\rightarrow \infty } x^{2}\left(\sqrt[7]{\frac{x^{3} +x}{1+x^{3}}} -\cos\frac{1}{x}\right) =\lim _{x\rightarrow \infty }\left(\frac{9}{14} +\mathcal{O}\left(\frac{1}{x}\right)\right) =\frac{9}{14}
\end{gather*} \begin{equation*}
\end{equation*} \begin{equation*}
\end{equation*}","['real-analysis', 'limits', 'asymptotics']"
43,Limit involving prime counting function,Limit involving prime counting function,,"Recently I came across this problem: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n} $$ Where $\pi(n)$ is prime counting function which counts number of primes $\le n$ . & $$H_{n}=\sum_{k=1}^{n} \frac{1}{k}$$ I would attempt the problem as follows. By using prime number theorem which states: $$\lim_{n \rightarrow \infty} \frac{\pi(n)}{\frac{n}{\ln(n)}}=1 $$ So will subtitute $\pi(n)$ with $\frac{n}{\ln(n)}$ as both are asymptotically equal , in limit will get: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}= \lim_{n\rightarrow \infty} \frac{n}{\frac{n}{\ln(n)}} -H_{n}= \lim_{n\rightarrow \infty} \ln(n) -H_{n}=-\gamma$$ Where $\gamma$ is euler macheroni constant. However, when I input this limit in wolfram alpha I get: $$\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}=-1-\gamma$$ https://www.wolframalpha.com/input/?i=limit+calculator&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limitfunction%22%7D+-%3E%22x%2Fpi%28x%29-H_x%22&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limit%22%7D+-%3E%22infinity%22 So how is this the case? Is there an asymptotic equation/series expansion of the prime counting function that can help prove that this is the case?","Recently I came across this problem: Where is prime counting function which counts number of primes . & I would attempt the problem as follows. By using prime number theorem which states: So will subtitute with as both are asymptotically equal , in limit will get: Where is euler macheroni constant. However, when I input this limit in wolfram alpha I get: https://www.wolframalpha.com/input/?i=limit+calculator&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limitfunction%22%7D+-%3E%22x%2Fpi%28x%29-H_x%22&assumption=%7B%22F%22%2C+%22Limit%22%2C+%22limit%22%7D+-%3E%22infinity%22 So how is this the case? Is there an asymptotic equation/series expansion of the prime counting function that can help prove that this is the case?",\lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}  \pi(n) \le n H_{n}=\sum_{k=1}^{n} \frac{1}{k} \lim_{n \rightarrow \infty} \frac{\pi(n)}{\frac{n}{\ln(n)}}=1  \pi(n) \frac{n}{\ln(n)} \lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}= \lim_{n\rightarrow \infty} \frac{n}{\frac{n}{\ln(n)}} -H_{n}= \lim_{n\rightarrow \infty} \ln(n) -H_{n}=-\gamma \gamma \lim_{n\rightarrow \infty} \frac{n}{\pi(n)} -H_{n}=-1-\gamma,"['real-analysis', 'limits', 'number-theory']"
44,Convergence of $ \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)} $,Convergence of, \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)} ,Let $ m \in \mathbb{N} $ . Find to what does the series $ \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)}  $ converge to My attempt: ( I did as discussed in Infinite Series $\sum 1/(n(n+1))$ ) Let $ N>m $ . $  \begin{align} S_N & = \sum\limits_{n=1}^{N}\frac{1}{n(n+m)}  = \frac{1}{m}\sum_{n=1}^N \left(\dfrac1n - \dfrac1{n+m}\right) = \frac{1}{m}( \sum_{n=1}^N \dfrac1 n - \sum_{n=1}^N \dfrac1{n+m}) = \frac{1}{m}(\sum_{n=1}^N \dfrac1n - \sum_{n=m+1}^{N+m} \dfrac1{n} )\\ & = \frac{1}{m}(\sum_{n=1}^m \dfrac1n + \sum_{n=m+1}^N \dfrac1n - \sum_{n=m+1}^N\dfrac1n -  \sum_{n=N+1}^{N+m}\dfrac1n ) = \frac{1}{m}(\sum_{n=1}^m \dfrac1n  -  \sum_{n=N+1}^{N+m}\dfrac1n )  ~~\text{ [ from here I got stuck] } \end{align}   $,Let . Find to what does the series converge to My attempt: ( I did as discussed in Infinite Series $\sum 1/(n(n+1))$ ) Let .," m \in \mathbb{N}   \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)}    N>m   
\begin{align}
S_N & = \sum\limits_{n=1}^{N}\frac{1}{n(n+m)}  = \frac{1}{m}\sum_{n=1}^N \left(\dfrac1n - \dfrac1{n+m}\right) = \frac{1}{m}( \sum_{n=1}^N \dfrac1 n - \sum_{n=1}^N \dfrac1{n+m}) = \frac{1}{m}(\sum_{n=1}^N \dfrac1n - \sum_{n=m+1}^{N+m} \dfrac1{n} )\\
& = \frac{1}{m}(\sum_{n=1}^m \dfrac1n + \sum_{n=m+1}^N \dfrac1n - \sum_{n=m+1}^N\dfrac1n -  \sum_{n=N+1}^{N+m}\dfrac1n ) = \frac{1}{m}(\sum_{n=1}^m \dfrac1n  -  \sum_{n=N+1}^{N+m}\dfrac1n ) 
~~\text{ [ from here I got stuck] } \end{align} 
 ","['real-analysis', 'sequences-and-series', 'limits']"
45,Problem about convergent series,Problem about convergent series,,"I'm trying to prove this series converges by using some sort of comparison test. $$\sum_{n=1}^{\infty}\frac{1}{n^{0.51}}-\sin\left(\frac{1}{n^{0.51}}\right)$$ I know by $\sin n\le n$ that the series is positive, so I went with the direction of using the comparison test. But I can't seem to find a function that is always greater than the expression in the series that also converges..","I'm trying to prove this series converges by using some sort of comparison test. I know by that the series is positive, so I went with the direction of using the comparison test. But I can't seem to find a function that is always greater than the expression in the series that also converges..",\sum_{n=1}^{\infty}\frac{1}{n^{0.51}}-\sin\left(\frac{1}{n^{0.51}}\right) \sin n\le n,"['sequences-and-series', 'limits', 'number-comparison']"
46,Interchange of limit and derivative [closed],Interchange of limit and derivative [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have two sequences of functions, $f_n(x)$ , $g_n(x)$ ., where the following relation holds $$ \forall n, \hspace{5mm} \frac{d}{dx}f_n(x) = g_n(x) $$ Also $$ \lim_{n \rightarrow \infty} f_n(x) = f(x) $$ We know that $f_n$ and $f$ are continuous functions. Under what conditions we are allowed to say $$ \lim_{n \rightarrow \infty} g_n(x) = \frac{d}{dx}f(x) $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have two sequences of functions, , ., where the following relation holds Also We know that and are continuous functions. Under what conditions we are allowed to say","f_n(x) g_n(x) 
\forall n, \hspace{5mm} \frac{d}{dx}f_n(x) = g_n(x)
 
\lim_{n \rightarrow \infty} f_n(x) = f(x)
 f_n f 
\lim_{n \rightarrow \infty} g_n(x) = \frac{d}{dx}f(x)
","['real-analysis', 'limits', 'analysis', 'derivatives']"
47,Asymptotic analysis: difference between big O and big Omega limits?,Asymptotic analysis: difference between big O and big Omega limits?,,"So I've started university again after taking a break and my first assignment in algorithms is to prove whether $f(n)$ is in Big O / Big Omega of $g(n)$ . These are the limit rules I have: $$\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} < \infty \Longrightarrow f \in O(g) \\ \lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = 0 \Longrightarrow f \in o(g) \\ \lim\limits_{n\to\infty} \frac{f(n)}{g(n)} > 0 \Longrightarrow f \in \Omega(g) \\ \lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = \infty \Longrightarrow f \in ω(g)$$ My question is: What is the difference between Big O and Big Omega in terms of limits ? Isn't every value bigger than $0$ also smaller than infinity? Also the following problem: $$\lim\limits_{n\to\infty} \frac{n+\pi}{n-e^{2021}} \text{ with l'Hopital} = \frac{1}{1} = 1$$ So it would be $>0$ , so Big Omega, but if I do l'Hopital again, it comes down to zero, meaning small $o$ ...? Am I missing something here? My brain is still hazy from the long break, so maybe I'm confusing simple things here ;)","So I've started university again after taking a break and my first assignment in algorithms is to prove whether is in Big O / Big Omega of . These are the limit rules I have: My question is: What is the difference between Big O and Big Omega in terms of limits ? Isn't every value bigger than also smaller than infinity? Also the following problem: So it would be , so Big Omega, but if I do l'Hopital again, it comes down to zero, meaning small ...? Am I missing something here? My brain is still hazy from the long break, so maybe I'm confusing simple things here ;)","f(n) g(n) \lim\limits_{n\to\infty} \frac{f(n)}{g(n)} < \infty \Longrightarrow f \in O(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = 0 \Longrightarrow f \in o(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} > 0 \Longrightarrow f \in \Omega(g) \\
\lim\limits_{n\to\infty} \frac{f(n)}{g(n)} = \infty \Longrightarrow f \in ω(g) 0 \lim\limits_{n\to\infty} \frac{n+\pi}{n-e^{2021}} \text{ with l'Hopital} = \frac{1}{1} = 1 >0 o","['limits', 'proof-explanation', 'asymptotics']"
48,When I can divide and multiply in a limit by $x$,When I can divide and multiply in a limit by,x,"I have formulated an old question but in that case I have committed many errors in limit calculus and  I understand my doubt was not absolutely clear so I have decided to remove it (it had not answer) and now I try to formulate it better. If I have a limit as $x$ goes to $\infty$ or $x\to 0$ I can multiply and divide by $x$ in order for instance to regain a notable limit. For instance $$\lim_{x\to 0}\frac{\sin^2{x}}{x}=\lim_{x\to 0}x\frac{\sin^2{x}}{x^2}=0$$ Now: if I have for istance a function $f(x)$ s.t $\lim_{x\to \infty}\frac{f(x)}{x}=1$ and I have to compute $\lim_{x\to \infty}f(x)-x$ , if I apply the previous argument ONLY on the term $f(x)$ I will obtain: $\lim_{x\to \infty}x-x=0$ . $\textbf{My doubt is: }$ is it allowed what i have done in the last?","I have formulated an old question but in that case I have committed many errors in limit calculus and  I understand my doubt was not absolutely clear so I have decided to remove it (it had not answer) and now I try to formulate it better. If I have a limit as goes to or I can multiply and divide by in order for instance to regain a notable limit. For instance Now: if I have for istance a function s.t and I have to compute , if I apply the previous argument ONLY on the term I will obtain: . is it allowed what i have done in the last?",x \infty x\to 0 x \lim_{x\to 0}\frac{\sin^2{x}}{x}=\lim_{x\to 0}x\frac{\sin^2{x}}{x^2}=0 f(x) \lim_{x\to \infty}\frac{f(x)}{x}=1 \lim_{x\to \infty}f(x)-x f(x) \lim_{x\to \infty}x-x=0 \textbf{My doubt is: },"['real-analysis', 'calculus', 'limits']"
49,"Why do we call a limit that evaluates to $\displaystyle\pm\infty$ a particular instance of ""The Limit Does Not Exist""","Why do we call a limit that evaluates to  a particular instance of ""The Limit Does Not Exist""",\displaystyle\pm\infty,"The $\epsilon$ - $\delta$ definition of a limit is stated as: $\displaystyle \lim_{x \to a}f(x)=L \iff \forall \epsilon \gt 0\  \exists\delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [ 0\lt \lvert x -a \rvert \lt \delta \rightarrow\lvert f(x) - L \rvert \lt \epsilon \big ] \quad \dagger$ More generally, one would state: $$\displaystyle \lim_{x \to a}f(x) \text{ exists} \iff \exists L \in \mathbb R \text{ s.t. }  \forall \epsilon \gt 0\  \exists\delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [ 0\lt \lvert x -a \rvert \lt \delta \rightarrow\lvert f(x) - L \rvert \lt \epsilon \big ] $$ The corresponding negation of $\dagger$ is: $\displaystyle \lim_{x \to a}f(x)\neq L \iff \exists\epsilon \gt 0\  \text{ s.t. }  \forall\delta \gt 0 \ \exists x \in \mathbb R  \text{ s.t. }\big [ 0\lt \lvert x -a \rvert \lt \delta \land \lvert f(x) - L \rvert \geq \epsilon \big ] $ More generally, one would state: $$\displaystyle \lim_{x \to a}f(x) \text{ does not exist} \iff \forall L \in \mathbb R\   \exists\epsilon \gt 0\  \text{ s.t. }  \forall\delta \gt 0 \ \exists x \in \mathbb R  \text{ s.t. } \big [ 0\lt \lvert x -a \rvert \lt \delta \land \lvert f(x) - L \rvert \geq \epsilon \big ] \quad \dagger \dagger$$ Given the above definitions , consider what one means for a limit evaluating to $\displaystyle \pm \infty$ : $\displaystyle\lim_{x\to a} f(x) = +\infty \iff \forall M \gt 0 \ \exists \delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [0 \lt \lvert x-a \rvert \lt \delta \rightarrow f(x) \gt M \big ]$ or $\displaystyle\lim_{x\to a} f(x) = -\infty \iff \forall N \lt 0 \ \exists \delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [0 \lt \lvert x-a \rvert \lt \delta \rightarrow f(x) \lt N \big ]$ Within the analysis community, when one claims "" $\displaystyle\lim_{x\to a} f(x) = \displaystyle  +\infty$ "" (or $- \infty$ ), I have seen that it is acceptable to claim "" The limit does not exist "". Based on the negated expression described in $\dagger \dagger$ , I am having difficulties seeing why this is justified. Could someone explain what is meant by ""the limit does not exist"" in the context of infinities? It appears to me that the first order logics do not match .","The - definition of a limit is stated as: More generally, one would state: The corresponding negation of is: More generally, one would state: Given the above definitions , consider what one means for a limit evaluating to : or Within the analysis community, when one claims "" "" (or ), I have seen that it is acceptable to claim "" The limit does not exist "". Based on the negated expression described in , I am having difficulties seeing why this is justified. Could someone explain what is meant by ""the limit does not exist"" in the context of infinities? It appears to me that the first order logics do not match .",\epsilon \delta \displaystyle \lim_{x \to a}f(x)=L \iff \forall \epsilon \gt 0\  \exists\delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [ 0\lt \lvert x -a \rvert \lt \delta \rightarrow\lvert f(x) - L \rvert \lt \epsilon \big ] \quad \dagger \displaystyle \lim_{x \to a}f(x) \text{ exists} \iff \exists L \in \mathbb R \text{ s.t. }  \forall \epsilon \gt 0\  \exists\delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [ 0\lt \lvert x -a \rvert \lt \delta \rightarrow\lvert f(x) - L \rvert \lt \epsilon \big ]  \dagger \displaystyle \lim_{x \to a}f(x)\neq L \iff \exists\epsilon \gt 0\  \text{ s.t. }  \forall\delta \gt 0 \ \exists x \in \mathbb R  \text{ s.t. }\big [ 0\lt \lvert x -a \rvert \lt \delta \land \lvert f(x) - L \rvert \geq \epsilon \big ]  \displaystyle \lim_{x \to a}f(x) \text{ does not exist} \iff \forall L \in \mathbb R\   \exists\epsilon \gt 0\  \text{ s.t. }  \forall\delta \gt 0 \ \exists x \in \mathbb R  \text{ s.t. } \big [ 0\lt \lvert x -a \rvert \lt \delta \land \lvert f(x) - L \rvert \geq \epsilon \big ] \quad \dagger \dagger \displaystyle \pm \infty \displaystyle\lim_{x\to a} f(x) = +\infty \iff \forall M \gt 0 \ \exists \delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [0 \lt \lvert x-a \rvert \lt \delta \rightarrow f(x) \gt M \big ] \displaystyle\lim_{x\to a} f(x) = -\infty \iff \forall N \lt 0 \ \exists \delta \gt 0 \text{ s.t. } \forall x \in \mathbb R \big [0 \lt \lvert x-a \rvert \lt \delta \rightarrow f(x) \lt N \big ] \displaystyle\lim_{x\to a} f(x) = \displaystyle  +\infty - \infty \dagger \dagger,"['limits', 'first-order-logic']"
50,How does one prove the following inequality?,How does one prove the following inequality?,,"I am currently studying Calculus and I have stumbled upon an inequality, which is crucial to one of the proofs in the “Limits of functions”, namely the following limit ( $ m \in \mathbb{N} $ ): $$ \lim_{x \to 0} \dfrac{\sqrt[m]{1+x} - 1}{x} = \dfrac{1}{m} $$ And the inequality I’m talking about is ( $|x| > 1$ ): $$ 1 - |x| < \sqrt[m]{1+x}  < 1 + |x| $$ The textbook says, that the limit immediately follows from the latter expression, but I can’t see how that is even possible, since we put $ |x| > 1 $ but in the limit we head $x$ towards 0. How should I deal with this?","I am currently studying Calculus and I have stumbled upon an inequality, which is crucial to one of the proofs in the “Limits of functions”, namely the following limit ( ): And the inequality I’m talking about is ( ): The textbook says, that the limit immediately follows from the latter expression, but I can’t see how that is even possible, since we put but in the limit we head towards 0. How should I deal with this?", m \in \mathbb{N}   \lim_{x \to 0} \dfrac{\sqrt[m]{1+x} - 1}{x} = \dfrac{1}{m}  |x| > 1  1 - |x| < \sqrt[m]{1+x}  < 1 + |x|   |x| > 1  x,"['calculus', 'limits', 'functions']"
51,Sequence queue - converges or diverges?,Sequence queue - converges or diverges?,,"I have the next sequence queue: $\sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}}$ . Does the queue converges or diverge? My attempt: I have tried to show that $\frac{1}{n^2}>|\frac{n^2-n-1}{n^4+n^2+1}|$ for any $n>0$ , and then by the comparation test we get that since $\sum_{n=1}^{\infty}{\frac{1}{n^2}}$ converges, we have that $\sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|}$ converges too, and by a theorem we have that since $\sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|}$ $\implies$ $\sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}}$ converges. Is that right?","I have the next sequence queue: . Does the queue converges or diverge? My attempt: I have tried to show that for any , and then by the comparation test we get that since converges, we have that converges too, and by a theorem we have that since converges. Is that right?",\sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}} \frac{1}{n^2}>|\frac{n^2-n-1}{n^4+n^2+1}| n>0 \sum_{n=1}^{\infty}{\frac{1}{n^2}} \sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|} \sum_{n=1}^{\infty}{|\frac{n^2-n-1}{n^4+n^2+1}|} \implies \sum_{n=1}^{\infty}{\frac{n^2-n-1}{n^4+n^2+1}},"['calculus', 'limits']"
52,Prove $\displaystyle \dfrac{n}{n + 1} > M$ whenever $n > N$ for some integer $N$,Prove  whenever  for some integer,\displaystyle \dfrac{n}{n + 1} > M n > N N,"Q: If $M$ is a positive number less than 1. Prove the terms in $\displaystyle \left\{\dfrac{n}{n + 1}\right\}_{n = 1}^{\infty}$ exceed $M$ for sufficiently large $n$ ; that is, prove $\displaystyle \dfrac{n}{n + 1} > M$ whenever $n > N$ for some integer $N$ . Not exactly sure how to prove this but here is my attempt so far. By the definition of the limit, $\displaystyle \lim_{n \to \infty} \dfrac{n}{n + 1} > M$ , we have $\forall M\in(0, 1)\exists N\in\mathbb{N}$ such that if \begin{equation*} n > N \hspace{1cm} \rightarrow \hspace{1cm} a_n > M \end{equation*} If $a_n = \dfrac{n}{n + 1}$ , then we want to show that it exceeds $M$ as $n$ gets larger. This is the part where I am kind of lost. I am not exactly sure how to proceed from here. Some tips or advice would be useful. Thanks","Q: If is a positive number less than 1. Prove the terms in exceed for sufficiently large ; that is, prove whenever for some integer . Not exactly sure how to prove this but here is my attempt so far. By the definition of the limit, , we have such that if If , then we want to show that it exceeds as gets larger. This is the part where I am kind of lost. I am not exactly sure how to proceed from here. Some tips or advice would be useful. Thanks","M \displaystyle \left\{\dfrac{n}{n + 1}\right\}_{n = 1}^{\infty} M n \displaystyle \dfrac{n}{n + 1} > M n > N N \displaystyle \lim_{n \to \infty} \dfrac{n}{n + 1} > M \forall M\in(0, 1)\exists N\in\mathbb{N} \begin{equation*}
n > N \hspace{1cm} \rightarrow \hspace{1cm} a_n > M
\end{equation*} a_n = \dfrac{n}{n + 1} M n","['calculus', 'sequences-and-series', 'limits', 'epsilon-delta']"
53,How to go about finding whether this limit exists: $\lim_{x \to 0} x [[\frac{1}{x}]]$?,How to go about finding whether this limit exists: ?,\lim_{x \to 0} x [[\frac{1}{x}]],"I am trying to solve few challenge questions on Real Analysis from Kaczor and Nowak's Problems in Mathematical Analysis , to become more proficient and stimulate thinking. I'd like someone to (a) verify if my proof is correct (b) is there a way to rigorously show that the limit does not exist? Note. $[[x]]$ is the greatest integer less than or equal to $x$ for all $x \in \mathbf{R}$ . Find the limits or state that they do not exist. Problem 1.1.1 (b) $\lim_{x \to 0}x\cdot[[\frac{1}{x}]]$ Proof. Consider the sequences $a_n:=\frac{1}{n}$ and $b_n=-\frac{1}{n}$ . Both these sequences converge to zero. The corresponding image sequences are, \begin{align*} (f(a_n)) &= \frac{1}{1}\cdot 1, \frac{1}{2} \cdot 2, \frac{1}{3}\cdot 3, \ldots \\ &= 1,1,1,1,1,\ldots \end{align*} \begin{align*} (f(b_n)) &= -\frac{1}{1}\cdot (-1), -\frac{1}{2} \cdot (-2), -\frac{1}{3}\cdot (-3), \ldots \\ &= 1,1,1,1,1,\ldots \end{align*} Also, consider the sequence $c_n:=\frac{1}{\sqrt{n}}$ . The sequence $(c_n)$ also converges to zero. \begin{align*} (f(c_n)) &= \frac{1}{\sqrt{1}}\cdot [[\sqrt{1}]], \frac{1}{\sqrt{2}} \cdot [[\sqrt{2}]], \frac{1}{\sqrt{3}}\cdot [[\sqrt{3}]], \frac{1}{\sqrt{4}} \cdot [[\sqrt{4}]], \ldots \\ &= 1,\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}},1,\frac{2}{\sqrt{5}},\frac{2}{\sqrt{6}},\frac{2}{\sqrt{7}},\frac{2}{\sqrt{8}},1,\ldots \end{align*} Thus, the image sequence oscillates between $0$ and $1$ and is not convergent. This violates the definition of functional limits. We require that, for all sequences $(x_n)$ in the domain of the function $f$ , such that $(x_n) \to a$ , $x_n \ne a$ for all $n \in \mathbf{N}$ , the image sequence $f(x_n)$ converges to $L$ . Then, $\lim_{x \to a}f(x) = L$ . So, the above limit does not exist.","I am trying to solve few challenge questions on Real Analysis from Kaczor and Nowak's Problems in Mathematical Analysis , to become more proficient and stimulate thinking. I'd like someone to (a) verify if my proof is correct (b) is there a way to rigorously show that the limit does not exist? Note. is the greatest integer less than or equal to for all . Find the limits or state that they do not exist. Problem 1.1.1 (b) Proof. Consider the sequences and . Both these sequences converge to zero. The corresponding image sequences are, Also, consider the sequence . The sequence also converges to zero. Thus, the image sequence oscillates between and and is not convergent. This violates the definition of functional limits. We require that, for all sequences in the domain of the function , such that , for all , the image sequence converges to . Then, . So, the above limit does not exist.","[[x]] x x \in \mathbf{R} \lim_{x \to 0}x\cdot[[\frac{1}{x}]] a_n:=\frac{1}{n} b_n=-\frac{1}{n} \begin{align*}
(f(a_n)) &= \frac{1}{1}\cdot 1, \frac{1}{2} \cdot 2, \frac{1}{3}\cdot 3, \ldots \\
&= 1,1,1,1,1,\ldots
\end{align*} \begin{align*}
(f(b_n)) &= -\frac{1}{1}\cdot (-1), -\frac{1}{2} \cdot (-2), -\frac{1}{3}\cdot (-3), \ldots \\
&= 1,1,1,1,1,\ldots
\end{align*} c_n:=\frac{1}{\sqrt{n}} (c_n) \begin{align*}
(f(c_n)) &= \frac{1}{\sqrt{1}}\cdot [[\sqrt{1}]], \frac{1}{\sqrt{2}} \cdot [[\sqrt{2}]], \frac{1}{\sqrt{3}}\cdot [[\sqrt{3}]], \frac{1}{\sqrt{4}} \cdot [[\sqrt{4}]], \ldots \\
&= 1,\frac{1}{\sqrt{2}},\frac{1}{\sqrt{3}},1,\frac{2}{\sqrt{5}},\frac{2}{\sqrt{6}},\frac{2}{\sqrt{7}},\frac{2}{\sqrt{8}},1,\ldots
\end{align*} 0 1 (x_n) f (x_n) \to a x_n \ne a n \in \mathbf{N} f(x_n) L \lim_{x \to a}f(x) = L","['real-analysis', 'limits', 'solution-verification']"
54,Convergence of $\;\sum^\infty_{n=1}\frac{(n+1)^{n-1}}{(-n)^n}\;$,Convergence of,\;\sum^\infty_{n=1}\frac{(n+1)^{n-1}}{(-n)^n}\;,"Does the following series converge? $$\sum^\infty_{n=1}\frac{(n+1)^{n-1}}{(-n)^n}$$ I've tried both the root- and the ratiotest, where the resulting sequence seems to always converge to $1$ . I've also tried the Leibniz test where: $$\left|\frac{(n+1)^{n-1}}{(-n)^n}\right| \;=\; \left(\frac{n+1}{n}\right)^n \;\cdot\; \frac{1}{n+1} \; \longrightarrow\; e\cdot0\;=\; 0$$ but am unable to show that the absolute value of the sequence is also decreasing. Any hints?","Does the following series converge? I've tried both the root- and the ratiotest, where the resulting sequence seems to always converge to . I've also tried the Leibniz test where: but am unable to show that the absolute value of the sequence is also decreasing. Any hints?",\sum^\infty_{n=1}\frac{(n+1)^{n-1}}{(-n)^n} 1 \left|\frac{(n+1)^{n-1}}{(-n)^n}\right| \;=\; \left(\frac{n+1}{n}\right)^n \;\cdot\; \frac{1}{n+1} \; \longrightarrow\; e\cdot0\;=\; 0,"['calculus', 'sequences-and-series', 'limits']"
55,Limit of a integral,Limit of a integral,,"For $f:[0,1]\rightarrow \mathbb{R}$ continuous, let $a >0.$ \begin{eqnarray*} L=\lim_{\varepsilon \rightarrow 0}\int_{\varepsilon a}^{\varepsilon b} \frac{f(x)}{x} dx \end{eqnarray*} Show that $L=f(0)\ln(\frac{b}{a})$ .The first I thought was in mean value theorem for integrals. but  I don't know how can start, if can give me some hint to start. I will be gratefull.","For continuous, let Show that .The first I thought was in mean value theorem for integrals. but  I don't know how can start, if can give me some hint to start. I will be gratefull.","f:[0,1]\rightarrow \mathbb{R} a >0. \begin{eqnarray*}
L=\lim_{\varepsilon \rightarrow 0}\int_{\varepsilon a}^{\varepsilon b} \frac{f(x)}{x} dx
\end{eqnarray*} L=f(0)\ln(\frac{b}{a})","['real-analysis', 'calculus', 'limits', 'analysis']"
56,"Apparently in this form the limit is ""trivially"" 1/2","Apparently in this form the limit is ""trivially"" 1/2",,"I'll admit, I'm struggling to keep up with the material in my analysis classes, but it only demoralizes me more when the questions in my textbooks have explained solutions, but I don't even understand how you go from one step to another. This is the final from, that's supposed to be close to the simplest form (so close that it's ""trivial"" for the student to find it): $\lim _{n\to \infty }\left(\frac{\left(c+1\right)n^c-n^{c+1}+\left(n-1\right)^{c+1}}{\left(c+1\right)\left(n^c-\left(n-1\right)^c\right)}\right)=\frac{1}{2}$ Why is this true? I don't see it at all.","I'll admit, I'm struggling to keep up with the material in my analysis classes, but it only demoralizes me more when the questions in my textbooks have explained solutions, but I don't even understand how you go from one step to another. This is the final from, that's supposed to be close to the simplest form (so close that it's ""trivial"" for the student to find it): Why is this true? I don't see it at all.",\lim _{n\to \infty }\left(\frac{\left(c+1\right)n^c-n^{c+1}+\left(n-1\right)^{c+1}}{\left(c+1\right)\left(n^c-\left(n-1\right)^c\right)}\right)=\frac{1}{2},"['real-analysis', 'limits']"
57,Do I need to analyse sequence given by $ x_{1+n} = \frac{1}{2 + x_{n}}$ without an equation with $0$?,Do I need to analyse sequence given by  without an equation with ?, x_{1+n} = \frac{1}{2 + x_{n}} 0,"I have a problem with exercises with sequences given by recursion when I need to ""prove convergence and find limit if it exists"" and I am given a recursion of that kind: $$ x_{1+n} = \frac{1}{2 + x_{n}}, x_1 \in (0 ; \infty)$$ It is fairly easy to find the limit - I just assume that the limit exists in $ \mathbb{R}$ and then use arithmetic properties of limits: $$\lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} x_{n}$$ $$\lim_{n \to \infty} x_{n} = l, l \in \mathbb{R}>0$$ Taking my recursion: $$l = \frac{1}{2 + l}$$ $$l^2 +2l - 1 = 0$$ $$l_1 = \sqrt{2} - 1 \in D$$ $$l_2 = -1 - \sqrt{2} \notin D$$ So my only possible limit in $ \mathbb{R}$ is $l = \sqrt{2} - 1$ . That is if I can actualy prove that the limit exists - that is: the sequence is monotonous and bounded. And here is my problem - it is just impossible to analyse without computer the difference of: $$x_{1+n} - x_{n} = \frac{1}{2 + x_{n}} - x_{n}$$ In search of limits I just multiply both sides of equation by $ \lim_{n \to \infty} x_{n} = l$ and it is impossible to do so here, so I get: $$x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}}$$ Then I can't tell when it is bigger than $0$ to analyse monotonicity and I can't the see for which values o $n$ which values of $n+1$ i get (to get the boundary) because min value gets crazy. So I just waneted to ask - am I missing something? Is it possible to make here $x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}}$ an equality with $0$ and analyse simpler function (red one on the picture)?","I have a problem with exercises with sequences given by recursion when I need to ""prove convergence and find limit if it exists"" and I am given a recursion of that kind: It is fairly easy to find the limit - I just assume that the limit exists in and then use arithmetic properties of limits: Taking my recursion: So my only possible limit in is . That is if I can actualy prove that the limit exists - that is: the sequence is monotonous and bounded. And here is my problem - it is just impossible to analyse without computer the difference of: In search of limits I just multiply both sides of equation by and it is impossible to do so here, so I get: Then I can't tell when it is bigger than to analyse monotonicity and I can't the see for which values o which values of i get (to get the boundary) because min value gets crazy. So I just waneted to ask - am I missing something? Is it possible to make here an equality with and analyse simpler function (red one on the picture)?"," x_{1+n} = \frac{1}{2 + x_{n}}, x_1 \in (0 ; \infty)  \mathbb{R} \lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} x_{n} \lim_{n \to \infty} x_{n} = l, l \in \mathbb{R}>0 l = \frac{1}{2 + l} l^2 +2l - 1 = 0 l_1 = \sqrt{2} - 1 \in D l_2 = -1 - \sqrt{2} \notin D  \mathbb{R} l = \sqrt{2} - 1 x_{1+n} - x_{n} = \frac{1}{2 + x_{n}} - x_{n}  \lim_{n \to \infty} x_{n} = l x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}} 0 n n+1 x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}} 0","['real-analysis', 'sequences-and-series', 'limits', 'analysis']"
58,What's the result of $\lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x}$?,What's the result of ?,\lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x},"During a correction of an exercise in a differentiability class this limit showed up and the teacher said that it could be done with L'Hopital easily, but me and a couple of friends tried doing it and neither of us got it. How can it be solved? I know the limit is 0 thanks to Geogebra. $$ \lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x}$$","During a correction of an exercise in a differentiability class this limit showed up and the teacher said that it could be done with L'Hopital easily, but me and a couple of friends tried doing it and neither of us got it. How can it be solved? I know the limit is 0 thanks to Geogebra.", \lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x},"['calculus', 'limits']"
59,How to prove $\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}}$,How to prove,\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}},"Today I have came across a problem.It is harder than I think. Given that (1) $f(x)$ is differentiable on $\left[ {{\rm{0}},{\rm{ + }}\infty } \right)$ , (2) $f'(x)$ is uniformly continuous on $\left[ {{\rm{0}},{\rm{ + }}\infty } \right)$ , and (3) $\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f(x)$ is existent, prove that $$\mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}}.$$ If not, is there any counter example? (I've been working on this for a long time.)","Today I have came across a problem.It is harder than I think. Given that (1) is differentiable on , (2) is uniformly continuous on , and (3) is existent, prove that If not, is there any counter example? (I've been working on this for a long time.)","f(x) \left[ {{\rm{0}},{\rm{ + }}\infty } \right) f'(x) \left[ {{\rm{0}},{\rm{ + }}\infty } \right) \mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f(x) \mathop {\lim }\limits_{x \to {\rm{ + }}\infty } f'(x){\rm{ = 0}}.","['real-analysis', 'limits', 'continuity', 'uniform-continuity', 'probability-limit-theorems']"
60,How to Compute the answer to the following limit problems?,How to Compute the answer to the following limit problems?,,"According to the answers in my book e = 4 and b = -2, I am unsure about e, but for b I would have assumed it to be -1. Can someone explain to me why the answers are what they are?","According to the answers in my book e = 4 and b = -2, I am unsure about e, but for b I would have assumed it to be -1. Can someone explain to me why the answers are what they are?",,"['calculus', 'limits']"
61,Evaluating $\lim_{n\to\infty}\prod_{i=2}^{n}{\frac{i^k-1}{i^k+1}}$ for various values of $k$,Evaluating  for various values of,\lim_{n\to\infty}\prod_{i=2}^{n}{\frac{i^k-1}{i^k+1}} k,"Let's consider the following function: $$ f(n, k) = \prod_{i = 2}^{n}{\frac{i^k - 1}{i^k + 1}} $$ I'm interested in the limit of $f$ as $n$ approaches infinity for various values of $k$ . I managed to calculate the limit for $k = 1$ (it's $2$ ) and $k = 3$ (it's $2/3$ ). For other values of $k$ the limit seems to be irrational, but I only managed to approximate it with direct calculations. My question: How to find the limit analytically for other $k$ ? What's so special about $3$ and $1$ being rational limits?","Let's consider the following function: I'm interested in the limit of as approaches infinity for various values of . I managed to calculate the limit for (it's ) and (it's ). For other values of the limit seems to be irrational, but I only managed to approximate it with direct calculations. My question: How to find the limit analytically for other ? What's so special about and being rational limits?","
f(n, k) = \prod_{i = 2}^{n}{\frac{i^k - 1}{i^k + 1}}
 f n k k = 1 2 k = 3 2/3 k k 3 1","['limits', 'products']"
62,Evaluate $S=\sum_{k=1}^\infty \frac{(-1)^{k-1}} k \sum_{n=0 }^\infty \frac 1 { k \cdot 2^n+1 } $,Evaluate,S=\sum_{k=1}^\infty \frac{(-1)^{k-1}} k \sum_{n=0 }^\infty \frac 1 { k \cdot 2^n+1 } ,"I am stuck in this problem, which is to evaluate the following series: $$\sum _{k=1}^\infty \frac {(-1)^{k-1} } k \sum_{n=0}^\infty \frac 1 {k \cdot 2^n +1}.$$ I have to solve it using high school methods. I tried to by expanding the inner sum. I also thought in the direction of limit of infinite sum but not able to proceed, even the first step. Please help.","I am stuck in this problem, which is to evaluate the following series: I have to solve it using high school methods. I tried to by expanding the inner sum. I also thought in the direction of limit of infinite sum but not able to proceed, even the first step. Please help.",\sum _{k=1}^\infty \frac {(-1)^{k-1} } k \sum_{n=0}^\infty \frac 1 {k \cdot 2^n +1}.,"['sequences-and-series', 'limits']"
63,"How to calculate $\lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy}$?",How to calculate ?,"\lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy}","The wolframalpha gives the answer $0$ : Wolframalpha culculation I tried like this: Let $x=r\cos(\theta)$ and $y=r\sin(\theta)$ ,then: $\lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy}$ = $\lim \limits_{r→0} \frac{r^4}{r^2 \sin(\theta)\cos(\theta)}$ = $\lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)} =0$ but it seems wrong when $\theta =0$ and the limit $\lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)}$ may not exist. So how to calculate the limit?","The wolframalpha gives the answer : Wolframalpha culculation I tried like this: Let and ,then: = = but it seems wrong when and the limit may not exist. So how to calculate the limit?","0 x=r\cos(\theta) y=r\sin(\theta) \lim \limits_{(x,y)→(0,0)} \frac{(x^2+y^2)^2}{xy} \lim \limits_{r→0} \frac{r^4}{r^2 \sin(\theta)\cos(\theta)} \lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)} =0 \theta =0 \lim \limits_{r→0} \frac{2r^2}{\sin(2\theta)}","['limits', 'multivariable-calculus']"
64,"Let $L=\lim_{x\to 0} \frac{ a-\sqrt {a^2-x^2} -\frac{x^2}{2}}{x^4}$, $a>0$. If $L$ is finite, find $a$ and $L$","Let , . If  is finite, find  and",L=\lim_{x\to 0} \frac{ a-\sqrt {a^2-x^2} -\frac{x^2}{2}}{x^4} a>0 L a L,"I need a hint to start this question, because I have no idea how to do it. It’s a $\frac 00$ form, so L’Hospital can be applied, but that would be extremely tedious. Expansion can’t be used because there is no function to use it for. How do I start it?","I need a hint to start this question, because I have no idea how to do it. It’s a form, so L’Hospital can be applied, but that would be extremely tedious. Expansion can’t be used because there is no function to use it for. How do I start it?",\frac 00,"['limits', 'limits-without-lhopital']"
65,"solving, the following limit","solving, the following limit",,"So, the following is the question  given: I could solve it partially, here is my approach: The limit is of the form $(A+B)/C$ where $A$ and $B$ both approach $e^3$ while $C$ approaches $0$ , This can be found out by simply evaluating $A$ and $B$ separately. Now, we can write the limit as $$ \lim_{t \to 0}  [(1+3t+2t^2)^{1/t} - e^3]/t -\lim_{t \to 0} [(1+3t-2t^2)^{1/t} - e^3]/t $$ but I couldn't evaluate these two limits at least using LH rule as the derivative of numerator is quite a long expression. Kindly suggest a way to solve this question, all help is greatly appreciated.","So, the following is the question  given: I could solve it partially, here is my approach: The limit is of the form where and both approach while approaches , This can be found out by simply evaluating and separately. Now, we can write the limit as but I couldn't evaluate these two limits at least using LH rule as the derivative of numerator is quite a long expression. Kindly suggest a way to solve this question, all help is greatly appreciated.",(A+B)/C A B e^3 C 0 A B  \lim_{t \to 0}  [(1+3t+2t^2)^{1/t} - e^3]/t -\lim_{t \to 0} [(1+3t-2t^2)^{1/t} - e^3]/t ,"['calculus', 'limits']"
66,"Different approaches in evaluating the limit $\frac{(x^3+y^3)}{(x^2-y^2)}$ when $(x,y)\to(0,0)$.",Different approaches in evaluating the limit  when .,"\frac{(x^3+y^3)}{(x^2-y^2)} (x,y)\to(0,0)","Note that this question has been previously asked here . I understood the solutions available there but I have two different approaches to this problem, I'm not sure whether they are correct. I need to know whether both of these solutions are correct and complete. If not, why are they incorrect? Approach 1 Take path 1 as $y=3x$ , hence limit goes to $0$ . Take path 2 as $y=(-x^3+x^2-y^2)^{1/3}$ , hence limit goes to $1$ . Therefore, the limit does not exist. Is the second path a valid path cause $y$ is not necessarily $0$ when $x=0$ ? Approach 2 Take $x=r\cos\theta$ and $y=r\sin\theta$ . We have $r\frac{\cos^3\theta+\sin^3\theta}{\cos^2\theta-\sin^2\theta}$ . Take path $r = \cos^2\theta-\sin^2\theta$ hence limit goes to $\cos^3\theta+\sin^3\theta$ which is different for every $\theta$ and hence limit cannot exist. Is this choice of $r$ allowed?","Note that this question has been previously asked here . I understood the solutions available there but I have two different approaches to this problem, I'm not sure whether they are correct. I need to know whether both of these solutions are correct and complete. If not, why are they incorrect? Approach 1 Take path 1 as , hence limit goes to . Take path 2 as , hence limit goes to . Therefore, the limit does not exist. Is the second path a valid path cause is not necessarily when ? Approach 2 Take and . We have . Take path hence limit goes to which is different for every and hence limit cannot exist. Is this choice of allowed?",y=3x 0 y=(-x^3+x^2-y^2)^{1/3} 1 y 0 x=0 x=r\cos\theta y=r\sin\theta r\frac{\cos^3\theta+\sin^3\theta}{\cos^2\theta-\sin^2\theta} r = \cos^2\theta-\sin^2\theta \cos^3\theta+\sin^3\theta \theta r,"['real-analysis', 'calculus', 'limits', 'multivariable-calculus']"
67,Prove that $\lim \limits_{T\to \infty} \frac{1}{T} \int_{-T/2}^{T/2} \cos(\omega t + \theta)dt = 0$,Prove that,\lim \limits_{T\to \infty} \frac{1}{T} \int_{-T/2}^{T/2} \cos(\omega t + \theta)dt = 0,"How to prove that $L=0$ if $$L = \lim \limits_{T\to \infty} \frac{1}{T}\int\limits_{-T/2}^{T/2} \cos(\omega t + \theta)\, dt.$$ I want to say the numerator is equal to zero as $T$ approaches infinity because integrating a period function over an infinite range is zero, however, the denominator also grows to infinity. Is that enough to prove that $L=0$ ?","How to prove that if I want to say the numerator is equal to zero as approaches infinity because integrating a period function over an infinite range is zero, however, the denominator also grows to infinity. Is that enough to prove that ?","L=0 L = \lim \limits_{T\to \infty} \frac{1}{T}\int\limits_{-T/2}^{T/2} \cos(\omega t + \theta)\, dt. T L=0","['calculus', 'integration', 'limits']"
68,Problem related to real monic quadratic polynomial,Problem related to real monic quadratic polynomial,,"Let $f(x)$ be a real monic quadratic polynomial. If ${x_1},{x_2},{x_3},{x_4},{x_5}$ be the $5$ points where $g(x) = |f(|x|)|$ is non-differentiable and $\sum_{i = 1}^5 {\left| {{x_i}} \right| = 8} $ then   find the value of $\frac{1}{5}\mathop {\lim }\limits_{x \to \infty } \frac{{{x^2} - f\left( x \right)}}{x}$ . My approach is as follow, real monic quadratic means that leading coefficient viz. the value of $a$ in $ax^2+bx+c=0$ is $1$ . That is the equation is of the form $x^2+bx+c$ , for real case $b^2-4c\ge0$ . But not able to approach.","Let be a real monic quadratic polynomial. If be the points where is non-differentiable and then   find the value of . My approach is as follow, real monic quadratic means that leading coefficient viz. the value of in is . That is the equation is of the form , for real case . But not able to approach.","f(x) {x_1},{x_2},{x_3},{x_4},{x_5} 5 g(x) = |f(|x|)| \sum_{i = 1}^5 {\left| {{x_i}} \right| = 8}  \frac{1}{5}\mathop {\lim }\limits_{x \to \infty } \frac{{{x^2} - f\left( x \right)}}{x} a ax^2+bx+c=0 1 x^2+bx+c b^2-4c\ge0","['real-analysis', 'calculus', 'limits', 'polynomials', 'quadratics']"
69,limit of subsequence where $X_n - X_{n-1}\rightarrow 0$.,limit of subsequence where .,X_n - X_{n-1}\rightarrow 0,"suppose $X_{n}$ is a sequence of real numbers such that $X_{n} - X_{n-1} \rightarrow 0$ . prove that the limit of subsequence is empty or single point set or interval . . I know the limit of subsequence is the set of limits of subsequences of { $P_{n}$ } n=1,2,... { $P_{n}$ } is the sequence in metric space (X,d). . My effort in this regard is as follows: Suppose it has two boundary points. We want to prove that all points between these two points are boundary points.such as a & b. Consider a point between these two points.such as c.(a<c<b) Now we have to consider the radius of the neighborhood around this point. . Now I can not calculate this radius correctly and I do not know how the rest of the question will be proved. Please help me!","suppose is a sequence of real numbers such that . prove that the limit of subsequence is empty or single point set or interval . . I know the limit of subsequence is the set of limits of subsequences of { } n=1,2,... { } is the sequence in metric space (X,d). . My effort in this regard is as follows: Suppose it has two boundary points. We want to prove that all points between these two points are boundary points.such as a & b. Consider a point between these two points.such as c.(a<c<b) Now we have to consider the radius of the neighborhood around this point. . Now I can not calculate this radius correctly and I do not know how the rest of the question will be proved. Please help me!",X_{n} X_{n} - X_{n-1} \rightarrow 0 P_{n} P_{n},"['limits', 'analysis']"
70,Prove that the serie $\sum_{n=1}^\infty \frac{1}{n(n + a)}$ converges,Prove that the serie  converges,\sum_{n=1}^\infty \frac{1}{n(n + a)},"I was trying to solve this problem but i got stucked. I use the Radio Test to compute the convergence interval, but it doesnt works in this case, i need help... Prove that the serie $\sum_{n=1}^\infty \frac{1}{n(n + a)}$ converges Calculate the sum of the serie","I was trying to solve this problem but i got stucked. I use the Radio Test to compute the convergence interval, but it doesnt works in this case, i need help... Prove that the serie converges Calculate the sum of the serie",\sum_{n=1}^\infty \frac{1}{n(n + a)},"['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
71,Evaluate the limit $\lim\sqrt[n]{\frac{1}{n!}\sum(m^m)}$,Evaluate the limit,\lim\sqrt[n]{\frac{1}{n!}\sum(m^m)},"In some problem, I need to evaluate this limit: $$\lim_{n\rightarrow \infty}\sqrt[n]{\frac{1}{n!}\sum^n_{m=0}(m^m)}.$$ I know about Taylor series and that kind of stuff. I'm not sure where to start, maybe Stirling but after using it I still could not solve it. Any help will be appreciated. Using Stirling's equivalence, I get to: $$\lim_{n\rightarrow \infty}\frac{e}{n}\sqrt[n]{\frac{\sum^n_{m=0}(m^m)}{\sqrt{2\pi n}}}$$ I don't know if this is useful anyway.","In some problem, I need to evaluate this limit: I know about Taylor series and that kind of stuff. I'm not sure where to start, maybe Stirling but after using it I still could not solve it. Any help will be appreciated. Using Stirling's equivalence, I get to: I don't know if this is useful anyway.",\lim_{n\rightarrow \infty}\sqrt[n]{\frac{1}{n!}\sum^n_{m=0}(m^m)}. \lim_{n\rightarrow \infty}\frac{e}{n}\sqrt[n]{\frac{\sum^n_{m=0}(m^m)}{\sqrt{2\pi n}}},"['sequences-and-series', 'limits']"
72,Limit of integrals where both bounds and integrand depend on $n$,Limit of integrals where both bounds and integrand depend on,n,"I want to find the limit $$\lim_{n\rightarrow\infty}\int_{[0,n]}\left(1+\frac{x}{n}\right)^n e^{-2x} \, d\lambda(x)$$ I have noted the following: It is well known that $(1+\frac{x}{n})^n$ converges to $\exp(x)$ in the limit. So I'm certain the limit is equal to the improper integral $$\int_0^\infty e^{-x} \, dx=1$$ My question is: How can I argue/use the convergence of the integrated when I can't pull the limit into the integral, as the bound depends on it? Also: I'm not taking the measure $\lambda$ into account at all, am I making a mistake there?","I want to find the limit I have noted the following: It is well known that converges to in the limit. So I'm certain the limit is equal to the improper integral My question is: How can I argue/use the convergence of the integrated when I can't pull the limit into the integral, as the bound depends on it? Also: I'm not taking the measure into account at all, am I making a mistake there?","\lim_{n\rightarrow\infty}\int_{[0,n]}\left(1+\frac{x}{n}\right)^n e^{-2x} \, d\lambda(x) (1+\frac{x}{n})^n \exp(x) \int_0^\infty e^{-x} \, dx=1 \lambda","['integration', 'sequences-and-series', 'limits', 'measure-theory']"
73,How can I solve $\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}}$?,How can I solve ?,\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}},"How can I solve $\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}}$ using $\displaystyle{\lim_{x \to +\infty} {(1+\frac{1}{x})^x = e}}$ ? What should I do with the constant "" $k$ ?","How can I solve using ? What should I do with the constant "" ?",\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}} \displaystyle{\lim_{x \to +\infty} {(1+\frac{1}{x})^x = e}} k,"['calculus', 'limits']"
74,"Prove. $\lim_{x \to 2} \frac{x+3}{x-1} = 5$, from first princibles.","Prove. , from first princibles.",\lim_{x \to 2} \frac{x+3}{x-1} = 5,"I am a student and I'm practising proving limits from first principles. I've been asked the question above, I have attempted an answer, but any tips or tricks anyone has to prod me in the right direction would be much appreciated! Firstly I fix $\epsilon > 0$ and fing $\delta > 0$ such that. $$0< |x-2| < \delta \implies \bigg| \frac{x+3}{x-1} -5 \bigg|<\epsilon$$ $$\impliedby \bigg|-\frac{4(x-2)}{x-1} \bigg|$$ $$\impliedby \bigg|\frac{4(x-2)}{x-1} \bigg|$$ $$\impliedby \frac{4}{|x-1|}|x-2| $$ Let $|x-2| < 1$ then $-1 < x-2 < 1$ or $0<x-1<2$ Let $\epsilon >0$ be given. Choose $\delta = min(1,\frac{\epsilon}{2}$ ) Then $|x-2|< \delta \implies \frac{4}{|x-1|}|x-2| < 2\delta \leq \epsilon  $ Thanks for your time!","I am a student and I'm practising proving limits from first principles. I've been asked the question above, I have attempted an answer, but any tips or tricks anyone has to prod me in the right direction would be much appreciated! Firstly I fix and fing such that. Let then or Let be given. Choose ) Then Thanks for your time!","\epsilon > 0 \delta > 0 0< |x-2| < \delta \implies \bigg| \frac{x+3}{x-1} -5 \bigg|<\epsilon \impliedby \bigg|-\frac{4(x-2)}{x-1} \bigg| \impliedby \bigg|\frac{4(x-2)}{x-1} \bigg| \impliedby \frac{4}{|x-1|}|x-2|  |x-2| < 1 -1 < x-2 < 1 0<x-1<2 \epsilon >0 \delta = min(1,\frac{\epsilon}{2} |x-2|< \delta \implies \frac{4}{|x-1|}|x-2| < 2\delta \leq \epsilon  ","['real-analysis', 'limits', 'solution-verification']"
75,find $\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt$,find,\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt,"I am asked to find with the help of this theorem/property the limit: $$\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt$$ My attempt: For my $f$ I will use $t^{x-1}$ since I clearly see that $f_n=t^{x-1}(1-t/n)^n \to f$ uniformly. But how does the integral $\int^{\infty}_0t^{x-1}dt$ converge? Am I doing something wrong? Also, how do I pick a function that is greater than $f_n$ and for which I can show converges? I was thinking of picking $t^x$ for the function $g(x)$ but then $t^x$ is not finite. Any help is appreciated","I am asked to find with the help of this theorem/property the limit: My attempt: For my I will use since I clearly see that uniformly. But how does the integral converge? Am I doing something wrong? Also, how do I pick a function that is greater than and for which I can show converges? I was thinking of picking for the function but then is not finite. Any help is appreciated",\lim_{n\to\infty}\int^n_0t^{x-1}(1-t/n)^ndt f t^{x-1} f_n=t^{x-1}(1-t/n)^n \to f \int^{\infty}_0t^{x-1}dt f_n t^x g(x) t^x,['real-analysis']
76,"Is $\lim_{n\to\infty} \frac{4n}{a^2_n}=\pi e$ for $a_{n+2}=a_{n+1}+\frac{a_n}{2n}$, $a_1=0,a_2=1$?","Is  for , ?","\lim_{n\to\infty} \frac{4n}{a^2_n}=\pi e a_{n+2}=a_{n+1}+\frac{a_n}{2n} a_1=0,a_2=1","I have found this limit in https://oeis.org/A019609 and I was wondering how to prove it (if it is actually correct): $$\lim_{n\to\infty} \frac{4n}{a^2_n}=\pi e$$ where $$a_1=0,a_2=1, a_{n+2}=a_{n+1}+\frac{a_n}{2n}.$$ By computer evaluation, it is correct for $2$ digits after decimal point at about $n\approx 24100$ , so if it is correct, it converges really slow. I've attempted to prove this by first considering generating function $f(x)=\sum_{n \geq 1}a_nx^n$ and then trying to get asymptotics of its coefficients. By using recurrence, we get $f(x)/x^2-1=f(x)/x+\sum \frac{a_n}{2n}x^n$ , and after differentiation we get differential equation which solves to $$f(x)=\frac{e^{-x/2}x^2}{(1-x)^{3/2}}.$$ Now I think this is a step away from getting asymptotics of $a_n$ , but I don't know how. Can anybody show how to finish this? Or maybe there is another way? Also, I don't think it is useful, but here is at least closed form obtained from the $f(x)$ using binomial series and exponential function series: $$ a_n=\sum_{i=0}^{n-2}\frac{(-1)^n}{2^i i!}\binom{-3/2}{n-i-2}. $$ Closest to this question seems to be Mirror algorithm for computing $\pi$ and $e$ - does it hint on some connection between them? , where there are two sequences approaching $\pi$ and $e$ and solutions seem to use same approach using generating functions, so this seems to be on the right track.","I have found this limit in https://oeis.org/A019609 and I was wondering how to prove it (if it is actually correct): where By computer evaluation, it is correct for digits after decimal point at about , so if it is correct, it converges really slow. I've attempted to prove this by first considering generating function and then trying to get asymptotics of its coefficients. By using recurrence, we get , and after differentiation we get differential equation which solves to Now I think this is a step away from getting asymptotics of , but I don't know how. Can anybody show how to finish this? Or maybe there is another way? Also, I don't think it is useful, but here is at least closed form obtained from the using binomial series and exponential function series: Closest to this question seems to be Mirror algorithm for computing $\pi$ and $e$ - does it hint on some connection between them? , where there are two sequences approaching and and solutions seem to use same approach using generating functions, so this seems to be on the right track.","\lim_{n\to\infty} \frac{4n}{a^2_n}=\pi e a_1=0,a_2=1, a_{n+2}=a_{n+1}+\frac{a_n}{2n}. 2 n\approx 24100 f(x)=\sum_{n \geq 1}a_nx^n f(x)/x^2-1=f(x)/x+\sum \frac{a_n}{2n}x^n f(x)=\frac{e^{-x/2}x^2}{(1-x)^{3/2}}. a_n f(x) 
a_n=\sum_{i=0}^{n-2}\frac{(-1)^n}{2^i i!}\binom{-3/2}{n-i-2}.
 \pi e","['sequences-and-series', 'limits', 'generating-functions']"
77,Find the limit of $\frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x}$ as $x \to 0$,Find the limit of  as,\frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x} x \to 0,$$\lim_{x \to 0} \frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x}$$ I tried to solve this using L'Hopital rule but the resulting differential got too messy $$=\lim_{x \to 0} \frac{e^{\tan x}\sec^2x - e^x + \sec x - 1 }{\tan^2x}$$ $$=\lim_{x \to 0} \frac{e^{\tan x}(\sec^4x+2\sec^2x\tan x) - e^x + \sec x\tan x }{2\tan x \sec^2x}$$ What should I do from here? Differentiate again or use a different strategy?,I tried to solve this using L'Hopital rule but the resulting differential got too messy What should I do from here? Differentiate again or use a different strategy?,\lim_{x \to 0} \frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x} =\lim_{x \to 0} \frac{e^{\tan x}\sec^2x - e^x + \sec x - 1 }{\tan^2x} =\lim_{x \to 0} \frac{e^{\tan x}(\sec^4x+2\sec^2x\tan x) - e^x + \sec x\tan x }{2\tan x \sec^2x},['limits']
78,Use epsilon-delta definition of limit to establish the following: $\displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3}$,Use epsilon-delta definition of limit to establish the following:,\displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3},"I understand that my solution here is probably not the most efficient (My professor's solution is ""cleaner"") but it is how my mind attacked the problem.  I have been losing lots of points for minor details that I've been unable to see.  Does the following proof hold?  Am I making any major (or minor) errors? \begin{align*} \left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|&\leq\left|\frac{1}{2+\sqrt{x}}\right|+\left|\frac{1}{3}\right|<\epsilon~~~\mbox{(by triangle inequality)}\\ &\implies\left|\frac{1}{2+\sqrt{x}}\right|+\frac{1}{3}<\epsilon\\ &\implies\left|\frac{1}{2+\sqrt{x}}\right| < \epsilon-\frac{1}{3}\\ &\implies \frac{1}{2} < \epsilon-\frac{1}{3}~~~~\mbox{(Because, }\sqrt{x}~\mbox{only a real number when } x\geq 0.)\\ &\implies 1<2(\epsilon-\frac{1}{3})\\ &\implies \left|x-1\right|<2\epsilon-\frac{2}{3}=\delta~~~~\mbox{(Because, choosing }x~s.t.~0<x<2\implies~-1<x-1<1)\\ \end{align*} $\therefore \left|x-1\right|<\delta\implies\left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|<\epsilon$ and $\displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3}$","I understand that my solution here is probably not the most efficient (My professor's solution is ""cleaner"") but it is how my mind attacked the problem.  I have been losing lots of points for minor details that I've been unable to see.  Does the following proof hold?  Am I making any major (or minor) errors? and","\begin{align*}
\left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|&\leq\left|\frac{1}{2+\sqrt{x}}\right|+\left|\frac{1}{3}\right|<\epsilon~~~\mbox{(by triangle inequality)}\\
&\implies\left|\frac{1}{2+\sqrt{x}}\right|+\frac{1}{3}<\epsilon\\
&\implies\left|\frac{1}{2+\sqrt{x}}\right| < \epsilon-\frac{1}{3}\\
&\implies \frac{1}{2} < \epsilon-\frac{1}{3}~~~~\mbox{(Because, }\sqrt{x}~\mbox{only a real number when } x\geq 0.)\\
&\implies 1<2(\epsilon-\frac{1}{3})\\
&\implies \left|x-1\right|<2\epsilon-\frac{2}{3}=\delta~~~~\mbox{(Because, choosing }x~s.t.~0<x<2\implies~-1<x-1<1)\\
\end{align*} \therefore \left|x-1\right|<\delta\implies\left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|<\epsilon \displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3}","['limits', 'solution-verification', 'epsilon-delta']"
79,An indeterminate limit form of infinity/infinity,An indeterminate limit form of infinity/infinity,,"I am trying to solve the limit: $$\lim_{x\to\infty}x^\frac{5}{3}\left(\left(x+\sin\left(\frac{1}{x}\right)\right)^\frac{1}{3}-x^\frac{1}{3}\right)$$ I was trying to find a way to bring it into a fraction form to apply L'Hospital's rule, and I tried using $$a-b=\frac{a^3-b^3}{a^2+ab+b^2}$$ But it made it even more complex and after applying L'Hospital's rule I got stuck with all the terms. Is there a smarter way to evaluate it?","I am trying to solve the limit: I was trying to find a way to bring it into a fraction form to apply L'Hospital's rule, and I tried using But it made it even more complex and after applying L'Hospital's rule I got stuck with all the terms. Is there a smarter way to evaluate it?",\lim_{x\to\infty}x^\frac{5}{3}\left(\left(x+\sin\left(\frac{1}{x}\right)\right)^\frac{1}{3}-x^\frac{1}{3}\right) a-b=\frac{a^3-b^3}{a^2+ab+b^2},"['calculus', 'limits', 'indeterminate-forms']"
80,"Given a recurrence formula, evaluate $\lim\limits_{n\to \infty} n^2 x_n^3$","Given a recurrence formula, evaluate",\lim\limits_{n\to \infty} n^2 x_n^3,"Define a sequence $(x_n)_{n\geq 0}$ with a fixed initial term $x_0 > 0$ such that: $$x_0 + x_1+\ldots+x_n=\frac{1}{\sqrt{x_{n+1}}}$$ Evaluate $$\lim_{n\to \infty} n^2 x_{n}^3$$ My attempt: I should define a new sequence $s_n = \displaystyle\sum_{i = 0}^nx_i$ with the recurrence formula: $$s_{n+1} = s_n+\frac{1}{s_n^2}$$ $(s_n)_{n\geq 0}$ is increasing and divergent, and I want the limit of: $$n^2x_n^3 = \frac{n^2}{s_{n-1}^6}$$ So, I can look instead for the limit: $$\lim_{n\to \infty} \frac{s^3_n}{n}$$ For $s_n^3$ , the recurrence formula gives $$s_{n+1}^3 = \left(\frac{1+s_n^3}{s_n^2}\right)^3$$ Looking at the function $f:(0, \infty) \to (0, \infty),\ f(x) = \dfrac{(x+1)^3}{x^2}$ this is increasing from a certain point forward and it feels that I should squeeze it between $3n$ and $3n+\text{something negligible}$ , but I can't give a solid argument for this.","Define a sequence with a fixed initial term such that: Evaluate My attempt: I should define a new sequence with the recurrence formula: is increasing and divergent, and I want the limit of: So, I can look instead for the limit: For , the recurrence formula gives Looking at the function this is increasing from a certain point forward and it feels that I should squeeze it between and , but I can't give a solid argument for this.","(x_n)_{n\geq 0} x_0 > 0 x_0 + x_1+\ldots+x_n=\frac{1}{\sqrt{x_{n+1}}} \lim_{n\to \infty} n^2 x_{n}^3 s_n = \displaystyle\sum_{i = 0}^nx_i s_{n+1} = s_n+\frac{1}{s_n^2} (s_n)_{n\geq 0} n^2x_n^3 = \frac{n^2}{s_{n-1}^6} \lim_{n\to \infty} \frac{s^3_n}{n} s_n^3 s_{n+1}^3 = \left(\frac{1+s_n^3}{s_n^2}\right)^3 f:(0, \infty) \to (0, \infty),\ f(x) = \dfrac{(x+1)^3}{x^2} 3n 3n+\text{something negligible}","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
81,"How do we know that the bounds chosen to apply the squeeze theorem are ""tight"" enough?","How do we know that the bounds chosen to apply the squeeze theorem are ""tight"" enough?",,"I have been examining many different examples and I found no objective justification to the chosen bounds in none of them, as if the choice was an intuitive process. Is it really just that? For the lower bound, do we ""begin with"" a 0 and start looking for a bound that is the furthest possible from 0 and that still satisfies the inequality? What about the upper bound? It just seems to me that it is very easy to choose the wrong bounds given the commonly counterintuitive nature of limits for beginners. Is there a way to be sure that it is the right/not right one and not fall onto that? (In case it makes a difference, the examples I have examined were applied on sequences) Any help is welcome, thank  you.","I have been examining many different examples and I found no objective justification to the chosen bounds in none of them, as if the choice was an intuitive process. Is it really just that? For the lower bound, do we ""begin with"" a 0 and start looking for a bound that is the furthest possible from 0 and that still satisfies the inequality? What about the upper bound? It just seems to me that it is very easy to choose the wrong bounds given the commonly counterintuitive nature of limits for beginners. Is there a way to be sure that it is the right/not right one and not fall onto that? (In case it makes a difference, the examples I have examined were applied on sequences) Any help is welcome, thank  you.",,"['real-analysis', 'sequences-and-series', 'limits']"
82,"If $\lim \limits_{x \to a}\left(f(x)+\frac{1}{f(x)}\right)=2,$ then $\lim \limits_{x \to a}f(x)=1$ [duplicate]",If  then  [duplicate],"\lim \limits_{x \to a}\left(f(x)+\frac{1}{f(x)}\right)=2, \lim \limits_{x \to a}f(x)=1","This question already has answers here : Prove limit of function (6 answers) Closed 4 years ago . Let $f:(a-\epsilon,a+\epsilon)\to(0,\infty).$ If $$\lim \limits_{x \to a}\left(f(x)+\frac{1}{f(x)}\right)=2,$$ prove that $$\lim \limits_{x \to a}f(x)=1.$$ I'm trying to get something using $\epsilon-\delta$ definition but I'm stuck . How to use $(a-\epsilon,a+\epsilon)$ information I'm wondering . $$~~$$ An answer",This question already has answers here : Prove limit of function (6 answers) Closed 4 years ago . Let If prove that I'm trying to get something using definition but I'm stuck . How to use information I'm wondering . An answer,"f:(a-\epsilon,a+\epsilon)\to(0,\infty). \lim \limits_{x \to a}\left(f(x)+\frac{1}{f(x)}\right)=2, \lim \limits_{x \to a}f(x)=1. \epsilon-\delta (a-\epsilon,a+\epsilon) ~~","['real-analysis', 'limits']"
83,Numerical instability of an extended tetration,Numerical instability of an extended tetration,,"For bases $a\in(1,e^{1/e})$ , ${}^na=a^{({}^{n-1}a)}=a^{a^{a^{.^{.^{.^a}}}}}$ converges to a value denoted as ${}^\infty a$ . By observing the convergence rate of this sequence, we can derive the limit: $$\lim_{n\to\infty}\frac{{}^\infty a-{}^{n+x}a}{{}^\infty a-{}^na}=[\ln({}^\infty a)]^x$$ By supposing we seek a continuous version of tetration that satisfies this, and rearranging so that ${}^xa$ is solved for, we derive: $${}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\tag1$$ where $\log^{\circ n}$ is the logarithm applied $n$ times. As an example, with $n=10$ , I obtained the following plot: which looks really nice. Then looking at $n=15$ , I get this: which raises concern. For $a$ close to $1$ and $n=10$ , I get It would seem to work well for small $n$ and large $a$ , but then for larger $n$ or smaller $a$ , it becomes unstable. As far as I can tell, this issue is due to the amount of numerical precision required while evaluating $(1)$ , especially when the base is closer to $1$ . So the first question is whether this is due to numerical precision, or if it's simply because $(1)$ does not converge. If it's the former, then is there any way to circumvent this without brute forcing with more precision? And how should I pick the values of $n$ for a given base $a$ (and $x$ )? If it's the latter, then does it converge anywhere? Code for computing $(1)$ , showing the following for $a=\sqrt2$ and $x=1.5$ : n   1.4142135623730^^n -------------------------- 0   1.42291711861386 1   1.4657586018199498 2   1.4910645646490854 3   1.5069501895748705 4   1.5172760309843982 5   1.5241342747726574 6   1.528753204049527 7   1.5318927292918296 8   1.5340399138955585 9   1.5355145848360043 10  1.5365302824374432 ... 45  1.538805432574356 46  1.5388054445894592 47  1.5388054519338499 48  1.5388054652284342 49  1.5388054823911386 50  1.538805506512146 ... 90  1.7233534923554696 91  1.755592017472159 92  2.0000000000000004 93  2.000000000000001 94  2.000000000000001 Showing the apparent value of $^{1.5}\sqrt2\simeq1.5388$ followed by divergence.","For bases , converges to a value denoted as . By observing the convergence rate of this sequence, we can derive the limit: By supposing we seek a continuous version of tetration that satisfies this, and rearranging so that is solved for, we derive: where is the logarithm applied times. As an example, with , I obtained the following plot: which looks really nice. Then looking at , I get this: which raises concern. For close to and , I get It would seem to work well for small and large , but then for larger or smaller , it becomes unstable. As far as I can tell, this issue is due to the amount of numerical precision required while evaluating , especially when the base is closer to . So the first question is whether this is due to numerical precision, or if it's simply because does not converge. If it's the former, then is there any way to circumvent this without brute forcing with more precision? And how should I pick the values of for a given base (and )? If it's the latter, then does it converge anywhere? Code for computing , showing the following for and : n   1.4142135623730^^n -------------------------- 0   1.42291711861386 1   1.4657586018199498 2   1.4910645646490854 3   1.5069501895748705 4   1.5172760309843982 5   1.5241342747726574 6   1.528753204049527 7   1.5318927292918296 8   1.5340399138955585 9   1.5355145848360043 10  1.5365302824374432 ... 45  1.538805432574356 46  1.5388054445894592 47  1.5388054519338499 48  1.5388054652284342 49  1.5388054823911386 50  1.538805506512146 ... 90  1.7233534923554696 91  1.755592017472159 92  2.0000000000000004 93  2.000000000000001 94  2.000000000000001 Showing the apparent value of followed by divergence.","a\in(1,e^{1/e}) {}^na=a^{({}^{n-1}a)}=a^{a^{a^{.^{.^{.^a}}}}} {}^\infty a \lim_{n\to\infty}\frac{{}^\infty a-{}^{n+x}a}{{}^\infty a-{}^na}=[\ln({}^\infty a)]^x {}^xa {}^xa=\lim_{n\to\infty}\log_a^{\circ n}({}^\infty a-({}^\infty a-{}^na)[\ln({}^\infty a)]^x)\tag1 \log^{\circ n} n n=10 n=15 a 1 n=10 n a n a (1) 1 (1) n a x (1) a=\sqrt2 x=1.5 ^{1.5}\sqrt2\simeq1.5388","['sequences-and-series', 'limits', 'convergence-divergence', 'asymptotics', 'tetration']"
84,Show that $\lim\limits_{x\to 0} \frac{\sin x\sin^{-1}x-x^2}{x^6}=\frac1{18}$,Show that,\lim\limits_{x\to 0} \frac{\sin x\sin^{-1}x-x^2}{x^6}=\frac1{18},Question: Show that $\lim\limits_{x\to 0} \dfrac{\sin x\sin^{-1}x-x^2}{x^6}=\dfrac{1}{18}$ My effort: $\lim\limits_{x\to 0} \dfrac{\sin x\sin^{-1}x-x^2}{x^6}=\lim\limits_{x\to 0} \dfrac{\dfrac{\sin x}{x} x\sin^{-1}x-x^2}{x^6}=\lim\limits_{x\to 0} \dfrac{\sin^{-1}x-x}{x^5}=\lim\limits_{x\to 0} \dfrac{\frac{1}{\sqrt{1-x^2}}-1}{5x^4}$ . Is my approach correct?,Question: Show that My effort: . Is my approach correct?,\lim\limits_{x\to 0} \dfrac{\sin x\sin^{-1}x-x^2}{x^6}=\dfrac{1}{18} \lim\limits_{x\to 0} \dfrac{\sin x\sin^{-1}x-x^2}{x^6}=\lim\limits_{x\to 0} \dfrac{\dfrac{\sin x}{x} x\sin^{-1}x-x^2}{x^6}=\lim\limits_{x\to 0} \dfrac{\sin^{-1}x-x}{x^5}=\lim\limits_{x\to 0} \dfrac{\frac{1}{\sqrt{1-x^2}}-1}{5x^4},['limits']
85,Proof that limit $\frac{\ln(x)}x$ for $x\to\infty$ is $0$.,Proof that limit  for  is .,\frac{\ln(x)}x x\to\infty 0,"Consider this proof of $\lim_{x\to{+\infty}} \frac{\ln(x)}{x} =0 $ . Using the simple substitution $x\mapsto x^n$ we have that $\forall n\in\mathbf{N}$ $$\lim_{x\to{+\infty}} \frac{\ln(x)}{x} = \lim_{x\to{+\infty}} \frac{n\cdot \ln(x)}{x^n}$$ and so we can write $$\lim_{x\to{+\infty}} \frac{\ln(x)}{x} = \lim_{n\to{+\infty}} \lim_{x\to{+\infty}} \frac{n\cdot \ln(x)}{x^n}= \lim_{x\to{+\infty}} \lim_{n\to{+\infty}} \frac{n\cdot \ln(x)}{x^n}.$$ Now, the inner limit is 0 for all $x>1$ so it leaves $$\lim_{x\to{+\infty}} \frac{\ln(x)}{x^n} = \lim_{x\to{+\infty}} 0 = 0.$$ Is this proof correct?","Consider this proof of . Using the simple substitution we have that and so we can write Now, the inner limit is 0 for all so it leaves Is this proof correct?",\lim_{x\to{+\infty}} \frac{\ln(x)}{x} =0  x\mapsto x^n \forall n\in\mathbf{N} \lim_{x\to{+\infty}} \frac{\ln(x)}{x} = \lim_{x\to{+\infty}} \frac{n\cdot \ln(x)}{x^n} \lim_{x\to{+\infty}} \frac{\ln(x)}{x} = \lim_{n\to{+\infty}} \lim_{x\to{+\infty}} \frac{n\cdot \ln(x)}{x^n}= \lim_{x\to{+\infty}} \lim_{n\to{+\infty}} \frac{n\cdot \ln(x)}{x^n}. x>1 \lim_{x\to{+\infty}} \frac{\ln(x)}{x^n} = \lim_{x\to{+\infty}} 0 = 0.,"['limits', 'proof-verification', 'limits-without-lhopital']"
86,Evaluate $\lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}$,Evaluate,\lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x},Solve that $$\lim_{x\to0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}$$ $$ \lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}=\lim_{x\to 0}\bigg[1+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}\\ [1^t+1^t+....+1^t]^{1/t}=n^{1/t}\leq[1^t+2^t+....+n^t]^{1/t}\leq [n.n^t]^{1/t}=n.n^{1/t} $$ Can I use squeeze theorem here or is there a better way ? Note: My reference gives the solution $n$,Solve that Can I use squeeze theorem here or is there a better way ? Note: My reference gives the solution,"\lim_{x\to0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x} 
\lim_{x\to 0}\bigg[1^{1/\sin^2x}+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}=\lim_{x\to 0}\bigg[1+2^{1/\sin^2x}+....+n^{1/\sin^2x}\bigg]^{\sin^2x}\\
[1^t+1^t+....+1^t]^{1/t}=n^{1/t}\leq[1^t+2^t+....+n^t]^{1/t}\leq [n.n^t]^{1/t}=n.n^{1/t}
 n","['calculus', 'limits', 'limits-without-lhopital']"
87,How to find the limit of this function using L'Hospital's rule?,How to find the limit of this function using L'Hospital's rule?,,"$$\lim_{x\rightarrow 0} \,\,\left( \sqrt[3]{1+2x+x^3} - \frac{2x}{2x+3}   \right) ^ {\frac1{x^3}} $$ I have already tried several options, but the only answer I have gotten so far is $e^{\infty}$ , which is incorrect. The correct answer is $e^\frac{43}{81}$ , which is easy to get by using Taylor series, but our task was to get the same one by using L'Hospital's rule. Can you help me with it?","I have already tried several options, but the only answer I have gotten so far is , which is incorrect. The correct answer is , which is easy to get by using Taylor series, but our task was to get the same one by using L'Hospital's rule. Can you help me with it?","\lim_{x\rightarrow 0} \,\,\left( \sqrt[3]{1+2x+x^3} - \frac{2x}{2x+3}   \right) ^ {\frac1{x^3}}  e^{\infty} e^\frac{43}{81}",['limits']
88,"Limit with radicals, $\cos$, $\ln$ and powers","Limit with radicals, ,  and powers",\cos \ln,$\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(2^{-\frac{1}{x}}\;-\;3^{-\frac{1}{x}}\Big)}{\ln(x-1)^{\frac{1}{x}}-\ln{x^{\frac{1}{x}}}}}=\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(2^{-\frac{1}{x}}\;-\;3^{-\frac{1}{x}}\Big)}{\ln(\frac{x-1}{x})^{\frac{1}{x}}}}=\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(\frac{\sqrt[x]{3}-\sqrt[x]{2}}{\sqrt[x]{6}}\Big)}{\frac{1}{x}\ln{\Big(1-\frac{1}{x}\Big)}}}=0$ My answer is rather imprecise: $$\underset{x\rightarrow +\infty}\lim{\cos{\frac{1}{x^3}}}=1\implies\underset{x\rightarrow +\infty}\lim{\sqrt[6]{1-\cos{\frac{1}{x^3}}}}=0$$ $$\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[x]{3}-\sqrt[x]{2}}{\sqrt[x]{6}}}=0?$$ I am aware of the mistake I have made by writting $0$ for an undefined term. $$\underset{x\rightarrow +\infty}\lim{\Big(1-\frac{1}{x^3}\Big)}=1\implies \ln{\Big(1-\frac{1}{x}\Big)}<0\implies\underset{x\rightarrow +\infty}\lim{\Bigg(\frac{1}{\frac{1}{x}\ln{\Big(1-\frac{1}{x}\Big)}}\Bigg)=-\infty}$$ The limit of the denumerator is $0$ $\&$ the limit of the whole expression is $0$ . How can I prove this concisely?,My answer is rather imprecise: I am aware of the mistake I have made by writting for an undefined term. The limit of the denumerator is the limit of the whole expression is . How can I prove this concisely?,\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(2^{-\frac{1}{x}}\;-\;3^{-\frac{1}{x}}\Big)}{\ln(x-1)^{\frac{1}{x}}-\ln{x^{\frac{1}{x}}}}}=\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(2^{-\frac{1}{x}}\;-\;3^{-\frac{1}{x}}\Big)}{\ln(\frac{x-1}{x})^{\frac{1}{x}}}}=\underset{x\rightarrow +\infty}\lim{\frac{\sqrt[6]{1-\cos{\frac{1}{x^3}}}\Big(\frac{\sqrt[x]{3}-\sqrt[x]{2}}{\sqrt[x]{6}}\Big)}{\frac{1}{x}\ln{\Big(1-\frac{1}{x}\Big)}}}=0 \underset{x\rightarrow +\infty}\lim{\cos{\frac{1}{x^3}}}=1\implies\underset{x\rightarrow +\infty}\lim{\sqrt[6]{1-\cos{\frac{1}{x^3}}}}=0 \underset{x\rightarrow +\infty}\lim{\frac{\sqrt[x]{3}-\sqrt[x]{2}}{\sqrt[x]{6}}}=0? 0 \underset{x\rightarrow +\infty}\lim{\Big(1-\frac{1}{x^3}\Big)}=1\implies \ln{\Big(1-\frac{1}{x}\Big)}<0\implies\underset{x\rightarrow +\infty}\lim{\Bigg(\frac{1}{\frac{1}{x}\ln{\Big(1-\frac{1}{x}\Big)}}\Bigg)=-\infty} 0 \& 0,"['real-analysis', 'calculus', 'limits', 'proof-writing']"
89,Find $\lim\limits_{n \to \infty} \sqrt[3]{n^3+2n^2+1}-\sqrt[3]{n^3-1}$.,Find .,\lim\limits_{n \to \infty} \sqrt[3]{n^3+2n^2+1}-\sqrt[3]{n^3-1},"I have to find the limit: $$\lim\limits_{n \to \infty} \sqrt[3]{n^3+2n^2+1}-\sqrt[3]{n^3-1}$$ I tried multiplying with the conjugate of the formula: $$(a-b)(a^2+ab+b^2)=a^3-b^3$$ So I got: $$\lim\limits_{n \to \infty} \dfrac{n^3+2n^2+1-n^3+1}{\sqrt[3]{(n^3+2n^2+1)^2} + \sqrt[3]{(n^3+2n^2+1)(n^3-1)} + \sqrt[3]{(n^3-1)^2}}$$ $$\lim\limits_{n \to \infty} \dfrac{2n^2+2}{\sqrt[3]{n^6+4n^5+4n^4+2n^3+4n^2+1} + \sqrt[3]{n^6+2n^5-2n^2-1} + \sqrt[3]{n^6-2n^3+1}}$$ And I saw that we can factor $n^2$ in the denominator and if we do the same in the numerator, we'd get that the limit is equal to $2$ . The problem is that my textbook claims that this limit is actually $\dfrac{2}{3}$ . I don't see why should I have a $3$ in the denominator since the coefficient of $n^2$ would be $1$ if I would carry out the factoring to detail. So, what did I do wrong?","I have to find the limit: I tried multiplying with the conjugate of the formula: So I got: And I saw that we can factor in the denominator and if we do the same in the numerator, we'd get that the limit is equal to . The problem is that my textbook claims that this limit is actually . I don't see why should I have a in the denominator since the coefficient of would be if I would carry out the factoring to detail. So, what did I do wrong?",\lim\limits_{n \to \infty} \sqrt[3]{n^3+2n^2+1}-\sqrt[3]{n^3-1} (a-b)(a^2+ab+b^2)=a^3-b^3 \lim\limits_{n \to \infty} \dfrac{n^3+2n^2+1-n^3+1}{\sqrt[3]{(n^3+2n^2+1)^2} + \sqrt[3]{(n^3+2n^2+1)(n^3-1)} + \sqrt[3]{(n^3-1)^2}} \lim\limits_{n \to \infty} \dfrac{2n^2+2}{\sqrt[3]{n^6+4n^5+4n^4+2n^3+4n^2+1} + \sqrt[3]{n^6+2n^5-2n^2-1} + \sqrt[3]{n^6-2n^3+1}} n^2 2 \dfrac{2}{3} 3 n^2 1,['calculus']
90,Compute $\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n$,Compute,\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n,"I am trying to solve the following problem: Compute $\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n$ when $\lim_{n \to \infty} a_n^n=a>0$ and $\lim_{n \to \infty} b_n^n=b>0$ such that $a_n,b_n>0 \ \forall \ n \ \in \mathbb{N}$ . I tried to use the Sandwich Theorem to come up with an answer, but my upper bound was not tight: $\max(a_n,b_n)\ge(\frac{a_n+b_n}{2}) \ge \sqrt{a_nb_n}$ On passing to the limits I got the following: $\max(a,b)\ge \lim_{n \to \infty}(\frac{a_n+b_n}{2}) \ge \sqrt{ab}$ But this doesn't help me at all. How could I actually compute the limit?","I am trying to solve the following problem: Compute when and such that . I tried to use the Sandwich Theorem to come up with an answer, but my upper bound was not tight: On passing to the limits I got the following: But this doesn't help me at all. How could I actually compute the limit?","\lim_{n \to \infty}(\frac{a_n+b_n}{2})^n \lim_{n \to \infty} a_n^n=a>0 \lim_{n \to \infty} b_n^n=b>0 a_n,b_n>0 \ \forall \ n \ \in \mathbb{N} \max(a_n,b_n)\ge(\frac{a_n+b_n}{2}) \ge \sqrt{a_nb_n} \max(a,b)\ge \lim_{n \to \infty}(\frac{a_n+b_n}{2}) \ge \sqrt{ab}","['calculus', 'sequences-and-series', 'limits']"
91,Limit $\lim_{x\rightarrow 0}\frac{e^{x}-1}{x^{2}}$ and problems,Limit  and problems,\lim_{x\rightarrow 0}\frac{e^{x}-1}{x^{2}},"The problem is straight forward enough, but I'm stuck. $$\lim_{x\rightarrow 0} \frac{e^x-1}{x^2}$$ Since I get $0 / 0$ I can use L'Hopital's rule, but after the first differentiation I get $1/0$ .  So, I can't continue with using that rule.  What approach next?","The problem is straight forward enough, but I'm stuck. Since I get I can use L'Hopital's rule, but after the first differentiation I get .  So, I can't continue with using that rule.  What approach next?",\lim_{x\rightarrow 0} \frac{e^x-1}{x^2} 0 / 0 1/0,"['calculus', 'limits', 'limits-without-lhopital']"
92,Show with the epsilon-delta definition that $\lim_{x \to 2} \frac{1}{x - 1} = 1$,Show with the epsilon-delta definition that,\lim_{x \to 2} \frac{1}{x - 1} = 1,"I have an assignment about epsilon-delta proofs and I'm having trouble with this one. I have worked it through using some of the methods I've picked up for similar proofs but it's just something about this particular expression that doesn't sit right with me. Any feedback would be very helpful. This is how far I've come: Let $\varepsilon > 0$ . We want to find a $\delta$ so that $\left|\frac{1}{x - 1} - 1\right| < \varepsilon$ when $0 < |x - 2| < \delta$ . We expand the expression: \begin{align*}     \left|\frac{1}{x - 1} - 1\right| &< \varepsilon \\     \left|\frac{1}{x - 1} - \frac{x - 1}{x - 1}\right| &< \varepsilon \\     \left|\frac{2 - x}{x - 1}\right| &< \varepsilon \\     |{x - 1}| &< \frac{|x - 2|}{\varepsilon} \\ \end{align*} We could let $\delta = \dfrac{|x - 2|}{\varepsilon}$ but $|x - 2|$ contains an unwanted variable. Since the limit is only relevant when $x$ is close to $a$ we'll restrict $x$ so that it's at most $1$ from $a$ or in other words, in our case, that $|x - 1| < 1$ . This means $0 < x < 2$ and that $-2 < x - 2 < 0$ . Looking at our previous inequality \begin{align*}     |{x - 1}| &< \frac{|x - 2|}{\varepsilon} \end{align*} we see that the right-hand side is the smallest when $|x - 2|$ is the smallest which by the range above is when $x - 2 = -2$ and then we have that \begin{align*}     |{x - 1}| &< \frac{|x - 2|}{\varepsilon} < \frac{2}{\varepsilon} \end{align*} We now have the two inequalities $|x - 1| < 1$ and $|x - 1| < \frac{2}{\varepsilon}$ . Let $\delta = \textrm{min}(1, \frac{2}{\varepsilon})$ and by definition we have that for every $\varepsilon > 0$ there is a $\delta$ so that $|f(x) - A| < \varepsilon$ for every $x$ in the domain that satisfies $0 < |x - a| < \delta$ . $\blacksquare$","I have an assignment about epsilon-delta proofs and I'm having trouble with this one. I have worked it through using some of the methods I've picked up for similar proofs but it's just something about this particular expression that doesn't sit right with me. Any feedback would be very helpful. This is how far I've come: Let . We want to find a so that when . We expand the expression: We could let but contains an unwanted variable. Since the limit is only relevant when is close to we'll restrict so that it's at most from or in other words, in our case, that . This means and that . Looking at our previous inequality we see that the right-hand side is the smallest when is the smallest which by the range above is when and then we have that We now have the two inequalities and . Let and by definition we have that for every there is a so that for every in the domain that satisfies .","\varepsilon > 0 \delta \left|\frac{1}{x - 1} - 1\right| < \varepsilon 0 < |x - 2| < \delta \begin{align*}
    \left|\frac{1}{x - 1} - 1\right| &< \varepsilon \\
    \left|\frac{1}{x - 1} - \frac{x - 1}{x - 1}\right| &< \varepsilon \\
    \left|\frac{2 - x}{x - 1}\right| &< \varepsilon \\
    |{x - 1}| &< \frac{|x - 2|}{\varepsilon} \\
\end{align*} \delta = \dfrac{|x - 2|}{\varepsilon} |x - 2| x a x 1 a |x - 1| < 1 0 < x < 2 -2 < x - 2 < 0 \begin{align*}
    |{x - 1}| &< \frac{|x - 2|}{\varepsilon}
\end{align*} |x - 2| x - 2 = -2 \begin{align*}
    |{x - 1}| &< \frac{|x - 2|}{\varepsilon} < \frac{2}{\varepsilon}
\end{align*} |x - 1| < 1 |x - 1| < \frac{2}{\varepsilon} \delta = \textrm{min}(1, \frac{2}{\varepsilon}) \varepsilon > 0 \delta |f(x) - A| < \varepsilon x 0 < |x - a| < \delta \blacksquare","['calculus', 'limits', 'proof-verification', 'epsilon-delta']"
93,limit of a n-square root and series of exponents,limit of a n-square root and series of exponents,,"Again, I'm having trouble with the infinite limits: $$  (1) .... \lim_{n \to \infty} \sqrt[n]{ a^n+b^n } $$ with $a,b$ positive reals. and to show if the following series is divergent or convergent $$ (2) ......\sum_{n=1}^{\infty} \frac{5^{n}-2^{n}}{7^n-6^n} $$ To be honest, don't have any idea how to approach them, at least for the (2) I may use their exponential representations, as follows: $$\sum_{n=1}^{\infty} \frac{e^{n \ln 5}-e^{n\ln 2}}{e^{n\ln 7}-6^{n \ln 6}}  $$ and in that case the series will diverge. But for (1) don't know. Thanks in advance!","Again, I'm having trouble with the infinite limits: with positive reals. and to show if the following series is divergent or convergent To be honest, don't have any idea how to approach them, at least for the (2) I may use their exponential representations, as follows: and in that case the series will diverge. But for (1) don't know. Thanks in advance!","  (1) .... \lim_{n \to \infty} \sqrt[n]{ a^n+b^n }  a,b  (2) ......\sum_{n=1}^{\infty} \frac{5^{n}-2^{n}}{7^n-6^n}  \sum_{n=1}^{\infty} \frac{e^{n \ln 5}-e^{n\ln 2}}{e^{n\ln 7}-6^{n \ln 6}}  ","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
94,Solving $ \lim_{x\to0}{\frac{1}{x^2}-\cot^2(x)}$,Solving, \lim_{x\to0}{\frac{1}{x^2}-\cot^2(x)},"Evaluate: $$ \lim_{x\to0}{\frac{1}{x^2}-\cot^2(x)}$$ My approach : $$\lim_{x\to0}{\frac{1}{x^2}-\frac{\cos^2(x)}{\sin^2(x)}}$$ $$ \lim_{x\to0}\frac{\sin^2(x)-x^2\cos^2(x)}{x^2\sin^2(x)} $$ Using $$\lim_{x\to0}\frac{\sin^2(x)}{x^2}=1 $$ $$ \lim_{x\to0}\frac{\sin^2(x)-x^2\cos^2(x)}{x^4} $$ $$ \lim_{x\to0}\frac{\sin^2(x)}{x^2}\cdot\frac{1}{x^2}-\frac{\cos^2(x)}{x^2} $$ $$ \lim_{x\to0}\frac{1}{x^2}-\frac{\cos^2(x)}{x^2} $$ Applying L Hopital, $$\lim_{x\to0}\frac{2\sin(x)\cos(x)}{2x}=1$$ But the actual answer is $\frac{2}{3}$ . What am I doing wrong here?","Evaluate: My approach : Using Applying L Hopital, But the actual answer is . What am I doing wrong here?", \lim_{x\to0}{\frac{1}{x^2}-\cot^2(x)} \lim_{x\to0}{\frac{1}{x^2}-\frac{\cos^2(x)}{\sin^2(x)}}  \lim_{x\to0}\frac{\sin^2(x)-x^2\cos^2(x)}{x^2\sin^2(x)}  \lim_{x\to0}\frac{\sin^2(x)}{x^2}=1   \lim_{x\to0}\frac{\sin^2(x)-x^2\cos^2(x)}{x^4}   \lim_{x\to0}\frac{\sin^2(x)}{x^2}\cdot\frac{1}{x^2}-\frac{\cos^2(x)}{x^2}   \lim_{x\to0}\frac{1}{x^2}-\frac{\cos^2(x)}{x^2}  \lim_{x\to0}\frac{2\sin(x)\cos(x)}{2x}=1 \frac{2}{3},"['limits', 'trigonometry']"
95,$\lim_{n \in \mathbb{N}}n\cdot\mathrm{e}^{1/n} - n$,,\lim_{n \in \mathbb{N}}n\cdot\mathrm{e}^{1/n} - n,"If I replace $\newcommand{\euler}{\mathrm{e}}$$n \to x$ where $x$ is a continuous variable. Then, I can easily apply L'Hopital's rule to $\frac{\euler^{1/x} - 1}{1/x}$ to get that the limit is $1$ . I don't know if this is legal? Is it? There is a corresponding L'Hopital's rule for sequences . But I have not had success in applying that either.","If I replace where is a continuous variable. Then, I can easily apply L'Hopital's rule to to get that the limit is . I don't know if this is legal? Is it? There is a corresponding L'Hopital's rule for sequences . But I have not had success in applying that either.",\newcommand{\euler}{\mathrm{e}}n \to x x \frac{\euler^{1/x} - 1}{1/x} 1,"['real-analysis', 'sequences-and-series', 'limits']"
96,Limit of this expression when n tends to infinity,Limit of this expression when n tends to infinity,,The limit is equal to: $$\lim_{n\to\infty} n^2 \int_0^1 \frac{1}{(1+x^2)^n} dx$$ P.S. What should be the better approach for such kinds of problems?,The limit is equal to: P.S. What should be the better approach for such kinds of problems?,\lim_{n\to\infty} n^2 \int_0^1 \frac{1}{(1+x^2)^n} dx,"['calculus', 'limits', 'definite-integrals']"
97,Does this limit exist and what's its value? $\lim_{n\to\infty}n^y\sum_ {i=1}^{n}\left[e^{-i}-\left(1-\frac{i}{n}\right)^{\!n}\right]$,Does this limit exist and what's its value?,\lim_{n\to\infty}n^y\sum_ {i=1}^{n}\left[e^{-i}-\left(1-\frac{i}{n}\right)^{\!n}\right],"Find $$\lim_{n\to \infty}\left\{n^y \sum_ {i=1}^{n}\left[\;e^{-i} -\left(1- \frac {i} {n}\right)^{\!n}\;\right]\right\}$$ I became interested in this problem because the YouTube series BlackPenRedPen (as part of the solution of another limit problem: https://www.youtube.com/watch?v=nPNB26hxLPc&t=7s ) solved the problem in the case $y = 0$ by interchanging sum and limit without justification. I realized that the methods used to correctly solve the case $y = 0$ (i.e., the Monotone Convergence Theorem) might solve the stated problem. Before starting to work on the the problem, I was trying to ascertain if the solution was known or easier than I thought. This problem has been criticized because I didn't supply the reason I was interested in its solution. If one needs a reason to study mathematical questions (beyond interest), then we we should stop working on The Twin Prime Conjecture, Goldbach's Conjecture, the Collatz Problem, the ABC Conjecture, etc.","Find I became interested in this problem because the YouTube series BlackPenRedPen (as part of the solution of another limit problem: https://www.youtube.com/watch?v=nPNB26hxLPc&t=7s ) solved the problem in the case by interchanging sum and limit without justification. I realized that the methods used to correctly solve the case (i.e., the Monotone Convergence Theorem) might solve the stated problem. Before starting to work on the the problem, I was trying to ascertain if the solution was known or easier than I thought. This problem has been criticized because I didn't supply the reason I was interested in its solution. If one needs a reason to study mathematical questions (beyond interest), then we we should stop working on The Twin Prime Conjecture, Goldbach's Conjecture, the Collatz Problem, the ABC Conjecture, etc.",\lim_{n\to \infty}\left\{n^y \sum_ {i=1}^{n}\left[\;e^{-i} -\left(1- \frac {i} {n}\right)^{\!n}\;\right]\right\} y = 0 y = 0,"['calculus', 'limits']"
98,Determining convergence of divergence with justification: $\sum\limits_{n=1}^\infty\left(\sqrt[n]{n}-1\right)$ [duplicate],Determining convergence of divergence with justification:  [duplicate],\sum\limits_{n=1}^\infty\left(\sqrt[n]{n}-1\right),This question already has answers here : Convergence of $\sum_{n=1}^\infty{\left(\sqrt[n]{n}-1\right)}$ (3 answers) Closed 4 years ago . So I was asked to determine and justify the convergence or divergence of $$\sum_{n=1}^\infty\left(\sqrt[n]{n}-1\right)$$ and I think it diverges and was trying to use the Comparison Test by finding a function less than it that diverges but I am not sure what the function would be... Thanks in advance for any help!,This question already has answers here : Convergence of $\sum_{n=1}^\infty{\left(\sqrt[n]{n}-1\right)}$ (3 answers) Closed 4 years ago . So I was asked to determine and justify the convergence or divergence of and I think it diverges and was trying to use the Comparison Test by finding a function less than it that diverges but I am not sure what the function would be... Thanks in advance for any help!,\sum_{n=1}^\infty\left(\sqrt[n]{n}-1\right),"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
99,Continuity at a Point (Local Property),Continuity at a Point (Local Property),,"Can anyone please explain me the following paragraph regarding limits and continuity?: One thing to note about continuity is that it is a local property. What this means is that, for any $a \in \mathbb{R}$ , if two functions $f$ and $g$ are equal on an open interval containing $a$ , then either both $f$ and $g$ are continuous at $a$ or both are discontinuous at $a$ . This is because they have the same value at $a$ and the same limit at $a$ (if these exist). I don't clearly understand what 'local property' means. Moreover, I think that the statement ""if two functions $f$ and $g$ are equal on an open interval containing $a$ , then either both $f$ and $g$ are continuous at $a$ or both are discontinuous at $a$ "" is false. Here's a counterexample that I came up with: Let $a = 0$ , $f(x) = x^2 + 1$ , $g(x) = \left\{ \begin{array}{ll}       x & \text{if } x\neq 0 \\       1& \text{if } x = 0 \end{array}  \right.$ Here, $f$ is continuous at $x = 0$ while $g$ is not. Yet both are equal at $x = 0$ . Edit: thank you for pointing out my error. I was wondering if there is any need for the interval to be open? Why can it not be a closed interval containing $a$ ?","Can anyone please explain me the following paragraph regarding limits and continuity?: One thing to note about continuity is that it is a local property. What this means is that, for any , if two functions and are equal on an open interval containing , then either both and are continuous at or both are discontinuous at . This is because they have the same value at and the same limit at (if these exist). I don't clearly understand what 'local property' means. Moreover, I think that the statement ""if two functions and are equal on an open interval containing , then either both and are continuous at or both are discontinuous at "" is false. Here's a counterexample that I came up with: Let , , Here, is continuous at while is not. Yet both are equal at . Edit: thank you for pointing out my error. I was wondering if there is any need for the interval to be open? Why can it not be a closed interval containing ?","a \in \mathbb{R} f g a f g a a a a f g a f g a a a = 0 f(x) = x^2 + 1 g(x) = \left\{
\begin{array}{ll}
      x & \text{if } x\neq 0 \\
      1& \text{if } x = 0
\end{array} 
\right. f x = 0 g x = 0 a","['real-analysis', 'limits', 'continuity']"
