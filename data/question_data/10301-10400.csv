,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can we prove AM-GM Inequality using these integrals?,Can we prove AM-GM Inequality using these integrals?,,"I came across these two results recently: $$ \int_a^b \sqrt{\left(1-\dfrac{a}{x}\right)\left(\dfrac{b}{x}-1\right)} \: dx = \pi\left(\dfrac{a+b}{2} - \sqrt{ab}\right)$$ $$ \int_a^c \sqrt[3]{\left| \left(1-\dfrac{a}{x}\right)\left(1-\dfrac{b}{x}\right)\left(1-\dfrac{c}{x}\right)\right|} \: dx = \dfrac{2\pi}{\sqrt{3}}\left(\dfrac{a+b+c}{3} - \sqrt[3]{abc}\right)$$ for $0<a\leq b\leq c$ . I haven't tried to solve the first one yet, but I have an idea of how to approach it, namely using the substitution $x=a\cos^2\theta+b\sin^2\theta$ . I have no idea how to approach the second one, however. I think that the most interesting thing about the results above is that it seems like there is a proof for the AM-GM inequality hidden within. Clearly both integrands are positive and so the AM-GM falls out for the 2 and 3 variable case. All that is required is to prove the results. My question is twofold: How would the second integral be computed? Is there an approach using elementary techniques? Can this be generalised to prove the AM-GM inequality for $n$ -variables?","I came across these two results recently: for . I haven't tried to solve the first one yet, but I have an idea of how to approach it, namely using the substitution . I have no idea how to approach the second one, however. I think that the most interesting thing about the results above is that it seems like there is a proof for the AM-GM inequality hidden within. Clearly both integrands are positive and so the AM-GM falls out for the 2 and 3 variable case. All that is required is to prove the results. My question is twofold: How would the second integral be computed? Is there an approach using elementary techniques? Can this be generalised to prove the AM-GM inequality for -variables?", \int_a^b \sqrt{\left(1-\dfrac{a}{x}\right)\left(\dfrac{b}{x}-1\right)} \: dx = \pi\left(\dfrac{a+b}{2} - \sqrt{ab}\right)  \int_a^c \sqrt[3]{\left| \left(1-\dfrac{a}{x}\right)\left(1-\dfrac{b}{x}\right)\left(1-\dfrac{c}{x}\right)\right|} \: dx = \dfrac{2\pi}{\sqrt{3}}\left(\dfrac{a+b+c}{3} - \sqrt[3]{abc}\right) 0<a\leq b\leq c x=a\cos^2\theta+b\sin^2\theta n,"['calculus', 'integration', 'inequality', 'a.m.-g.m.-inequality']"
1,Is there a geometric intuition for integration by parts? [duplicate],Is there a geometric intuition for integration by parts? [duplicate],,"This question already has answers here : What is integration by parts, really? (8 answers) Closed 3 years ago . Is there a geometric intuition for integration by parts? $$\int f(x)g'(x)\,dx = f(x)g(x) - \int g(x)f'(x)\,dx$$ This can, of course, be shown algebraically by product rule, but still where is geometric intuition? I have seen geometry of IBP using parametric equations but I don't get it. Newest edit: few similar questions has been asked before, but they use parametric equations to show geometry behind IBP. I am interested if there is geometric intuition which uses functions in Cartesian plane or some other, maybe more natural, explanation.","This question already has answers here : What is integration by parts, really? (8 answers) Closed 3 years ago . Is there a geometric intuition for integration by parts? This can, of course, be shown algebraically by product rule, but still where is geometric intuition? I have seen geometry of IBP using parametric equations but I don't get it. Newest edit: few similar questions has been asked before, but they use parametric equations to show geometry behind IBP. I am interested if there is geometric intuition which uses functions in Cartesian plane or some other, maybe more natural, explanation.","\int f(x)g'(x)\,dx = f(x)g(x) - \int g(x)f'(x)\,dx","['calculus', 'integration', 'geometry', 'intuition']"
2,Integral $\int_0^1 dx \frac{\ln x \ln^2(1-x)\ln(1+x)}{x}$,Integral,\int_0^1 dx \frac{\ln x \ln^2(1-x)\ln(1+x)}{x},"I am trying to calculate $$ I:=\int_0^1 dx \frac{\ln x \ln^2(1-x)\ln(1+x)}{x}$$ Note, the closed form is beautiful (yes beautiful ) and is given by $$ I=−\frac{3}{8}\zeta_2\zeta_3 -\frac{2}{3}\zeta_2\ln^3 2  +\frac{7}{4}\zeta_3\ln^2 2-\frac{7}{2}\zeta_5+4\ln 2 \operatorname{Li}_4\left(\frac{1}{2}\right)+\frac{2}{15}\ln^5 2+4\operatorname{Li}_5\left(\frac{1}{2}\right) $$ where $$ \zeta_s=\sum_{n=1}^\infty \frac{1}{n^{s}},\qquad \operatorname{Li}_s(z)=\sum_{n=1}^\infty \frac{z^n}{n^s},\qquad\text{for}\ |z|<1. $$ I succeeded in writing the integral as $$ I=-\sum_{i=0}^\infty \int_0^1  x^i\ln x\ln(1+x)\ln(1-x)\ dx, $$ but I am confused as to where to go from here.  Possibly I was thinking of trying to use Mellin transforms or residues. A reference to aid us is here . (Since somebody has asked for reference) We can also write I as $$ I=\sum_{i=0}^\infty \sum_{j=1}^\infty \frac{1}{j}\sum_{k=1}^\infty \frac{1}{k} \int_0^1  x^{i+j+k} \ln x\ dx $$ using $$ \int_0^1 x^n \ln x\ dx= -\frac{1}{(n+1)^2}, $$ we can simplify this, but I am not sure then how to compute the triple sum.  Thank you again.","I am trying to calculate Note, the closed form is beautiful (yes beautiful ) and is given by where I succeeded in writing the integral as but I am confused as to where to go from here.  Possibly I was thinking of trying to use Mellin transforms or residues. A reference to aid us is here . (Since somebody has asked for reference) We can also write I as using we can simplify this, but I am not sure then how to compute the triple sum.  Thank you again.","
I:=\int_0^1 dx \frac{\ln x \ln^2(1-x)\ln(1+x)}{x} 
I=−\frac{3}{8}\zeta_2\zeta_3 -\frac{2}{3}\zeta_2\ln^3 2  +\frac{7}{4}\zeta_3\ln^2 2-\frac{7}{2}\zeta_5+4\ln 2 \operatorname{Li}_4\left(\frac{1}{2}\right)+\frac{2}{15}\ln^5 2+4\operatorname{Li}_5\left(\frac{1}{2}\right)
 
\zeta_s=\sum_{n=1}^\infty \frac{1}{n^{s}},\qquad \operatorname{Li}_s(z)=\sum_{n=1}^\infty \frac{z^n}{n^s},\qquad\text{for}\ |z|<1.
 
I=-\sum_{i=0}^\infty \int_0^1  x^i\ln x\ln(1+x)\ln(1-x)\ dx,
 
I=\sum_{i=0}^\infty \sum_{j=1}^\infty \frac{1}{j}\sum_{k=1}^\infty \frac{1}{k} \int_0^1  x^{i+j+k} \ln x\ dx
 
\int_0^1 x^n \ln x\ dx= -\frac{1}{(n+1)^2},
","['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'harmonic-numbers']"
3,Why should the substitution be injective when integrating by substitution?,Why should the substitution be injective when integrating by substitution?,,"I made a silly mistake in evaluating some integral by using a non-injective $u$-substitution.  But why should $u$-substitutions be injective in the first place? I reasoned in the following way: the formula $$ \int_{\phi(a)}^{\phi(b)}g(x)\ dx = \int_a^b g(\phi(t))\phi^\prime(t)\ dt $$ holds for a general $C^1$ function $\phi$, even if it is not injective.  When you calculate an integral of the form $\int_a^b f(\phi(t))\ dt$, to use the formula above from right to left , you should find a function $f$ such that $$ f(\phi(t)) = g(\phi(t))\phi^\prime(t), $$ which do not exist if $\phi$ is not injective, i.e., $\phi(t) = 0$ for some $t$. This is why substitutions should be injective. Is my reasoning correct?  If so, I believe that if $\phi^\prime(t) = 0 \Rightarrow f(\phi(t)) = 0$, a function $g$ that satisfies the formula above may exist and $\phi$ should not necessarily be injective.  Is this right? I am often confused about the fact $\phi$ should be injective.  Is there an intuitive way to interpret this fact, so that I always remember to take a $\phi$ that is injective? I would be grateful if you could help me understand this matter.","I made a silly mistake in evaluating some integral by using a non-injective $u$-substitution.  But why should $u$-substitutions be injective in the first place? I reasoned in the following way: the formula $$ \int_{\phi(a)}^{\phi(b)}g(x)\ dx = \int_a^b g(\phi(t))\phi^\prime(t)\ dt $$ holds for a general $C^1$ function $\phi$, even if it is not injective.  When you calculate an integral of the form $\int_a^b f(\phi(t))\ dt$, to use the formula above from right to left , you should find a function $f$ such that $$ f(\phi(t)) = g(\phi(t))\phi^\prime(t), $$ which do not exist if $\phi$ is not injective, i.e., $\phi(t) = 0$ for some $t$. This is why substitutions should be injective. Is my reasoning correct?  If so, I believe that if $\phi^\prime(t) = 0 \Rightarrow f(\phi(t)) = 0$, a function $g$ that satisfies the formula above may exist and $\phi$ should not necessarily be injective.  Is this right? I am often confused about the fact $\phi$ should be injective.  Is there an intuitive way to interpret this fact, so that I always remember to take a $\phi$ that is injective? I would be grateful if you could help me understand this matter.",,"['calculus', 'integration']"
4,Integral whose upper limit is the integral itself: $\int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x$,Integral whose upper limit is the integral itself:,\int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x,"I recently encountered the following definite integral: $$\int_0^{\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x$$ where ""$\ldots$"" seems to indicate that the upper limit of $\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x$ is also $\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x$, so that the upper limit of the integral repeats itself infinitely, somewhat similarly to a continued fraction. I tried to solve this by renaming the integral $\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x$ in the upper limit $U$, so that we get $$\int_0^{\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x =\int_0^U \frac{1}{\sqrt{x}} \ \mathrm{d}x=\left[2\sqrt{x} \vphantom{\frac 1 1} \right]_0^U = 2\sqrt{U}$$ If we now consider that the integral in the upper limit $U$ is in fact equal to the original definite integral which we are trying to evaluate, we find that $U=2\sqrt{U}\Rightarrow U^2=4U\Rightarrow U^2-4U=0$, which is a quadratic equation with solutions $U_1=4$ and $U_2=0$. This seems to imply that $$\int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x=4\ \ \vee\ \ \int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x=0$$ My question is: Is one (or both) of these solutions correct, and is there a way to prove (or disprove) this (assuming that what I've written here is not sufficient proof)?","I recently encountered the following definite integral: $$\int_0^{\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x$$ where ""$\ldots$"" seems to indicate that the upper limit of $\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x$ is also $\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x$, so that the upper limit of the integral repeats itself infinitely, somewhat similarly to a continued fraction. I tried to solve this by renaming the integral $\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x$ in the upper limit $U$, so that we get $$\int_0^{\int_0^\ldots \frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x =\int_0^U \frac{1}{\sqrt{x}} \ \mathrm{d}x=\left[2\sqrt{x} \vphantom{\frac 1 1} \right]_0^U = 2\sqrt{U}$$ If we now consider that the integral in the upper limit $U$ is in fact equal to the original definite integral which we are trying to evaluate, we find that $U=2\sqrt{U}\Rightarrow U^2=4U\Rightarrow U^2-4U=0$, which is a quadratic equation with solutions $U_1=4$ and $U_2=0$. This seems to imply that $$\int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x=4\ \ \vee\ \ \int_{0}^{\int_{0}^{\ldots}\frac{1}{\sqrt{x}} \ \mathrm{d}x} \frac{1}{\sqrt{x}} \ \mathrm{d}x=0$$ My question is: Is one (or both) of these solutions correct, and is there a way to prove (or disprove) this (assuming that what I've written here is not sufficient proof)?",,"['calculus', 'integration', 'definite-integrals']"
5,Does Newton's method converge for all polynomials?,Does Newton's method converge for all polynomials?,,"I made an implementation of Newton's method (starting at $x=\pi$ ) for polynomials in the programming language Uiua ( here it is if you are interested, but be warned that Uiua looks weird). It works fine for most polynomials but there are a few that it gets stuck on. I tried [12 ¯9 ¯21 37 ¯41 ¯10 31 49] , (a degree 7 polynomial with coefficients listed from lowest degree to highest) and the execution time ran out. Inspecting further, it seems to be oscillating somewhat randomly in the range $[0.3,0.7]$ , even after 500+ iterations, when the actual answer is $-0.568$ . Another online implementation also fails to converge when starting at $x=\pi$ . My question is whether Newton's method should be working in this case or if I should expect to find there is some problem in my code or in the way that Uiua handles floats. Does Newton's method always converge for polynomials?","I made an implementation of Newton's method (starting at ) for polynomials in the programming language Uiua ( here it is if you are interested, but be warned that Uiua looks weird). It works fine for most polynomials but there are a few that it gets stuck on. I tried [12 ¯9 ¯21 37 ¯41 ¯10 31 49] , (a degree 7 polynomial with coefficients listed from lowest degree to highest) and the execution time ran out. Inspecting further, it seems to be oscillating somewhat randomly in the range , even after 500+ iterations, when the actual answer is . Another online implementation also fails to converge when starting at . My question is whether Newton's method should be working in this case or if I should expect to find there is some problem in my code or in the way that Uiua handles floats. Does Newton's method always converge for polynomials?","x=\pi [0.3,0.7] -0.568 x=\pi","['calculus', 'polynomials', 'convergence-divergence', 'roots']"
6,Evaluating $\int_0^{\frac\pi2}\frac{\ln{(\sin x)}\ \ln{(\cos x})}{\tan x}\ dx$,Evaluating,\int_0^{\frac\pi2}\frac{\ln{(\sin x)}\ \ln{(\cos x})}{\tan x}\ dx,I need to solve $$ \int_0^{\Large\frac\pi2}\frac{\ln{(\sin x)}\ \ln{(\cos x})}{\tan x}\ dx $$ I tried to use symmetric properties of the trigonometric functions as is commonly used to compute $$ \int_0^{\Large\frac\pi2}\ln\sin x\ dx = -\frac{\pi}{2}\ln2 $$ but never succeeded. (see this for example),I need to solve I tried to use symmetric properties of the trigonometric functions as is commonly used to compute but never succeeded. (see this for example),"
\int_0^{\Large\frac\pi2}\frac{\ln{(\sin x)}\ \ln{(\cos x})}{\tan x}\ dx
 
\int_0^{\Large\frac\pi2}\ln\sin x\ dx = -\frac{\pi}{2}\ln2
","['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
7,Closed form for $\sum_{n=-\infty}^{\infty}\frac{1}{(n-a)^2+b^2}$.,Closed form for .,\sum_{n=-\infty}^{\infty}\frac{1}{(n-a)^2+b^2},Find the closed form of $$\sum_{n=-\infty}^{\infty}\frac{1}{(n-a)^2+b^2}.$$ We can use the Fourier series of $e^{-bx}$ ( $|x|<\pi$ ) to find $$\sum_{n=-\infty}^{\infty}\frac{1}{n^2+b^2}.$$ But here it seems difficult to find the closed form.,Find the closed form of We can use the Fourier series of ( ) to find But here it seems difficult to find the closed form.,\sum_{n=-\infty}^{\infty}\frac{1}{(n-a)^2+b^2}. e^{-bx} |x|<\pi \sum_{n=-\infty}^{\infty}\frac{1}{n^2+b^2}.,"['calculus', 'sequences-and-series', 'complex-analysis', 'fourier-analysis', 'closed-form']"
8,Notation for partial derivatives,Notation for partial derivatives,,"I thought that the meaning of $$ \frac{\partial f(x, y, z)}{\partial x} $$ is differentiation on $x$ with fixed $y$ and $z$. So $(x, y, z)$ in the numerator is just saying which variables are fixed. If I need to indicate where the derivative is evaluated, I write it in the right of a vertical bar as a subscript. But today my teacher used $(x, y, z)$ in the numerator to denote where the derivative is evaluated. So, for example, $$ \frac{\partial f(0, 0, 0)}{\partial x} $$ means $$ \frac{\partial f(x, y, z)}{\partial x} \bigg\rvert_{x=0,y=0,z=0} $$ Is that a standard convention? If so, what is the meaning of this? $$ \frac{\partial f(x, y, g(x, y))}{\partial x} $$ I have two candidates. One is a partial derivative of the composition of $f$ and $g$ where $g$ has some fixed value, and the other is the partial derivative of $f$ on $x$ evaluated at $(x, y, g(x, y))$. I think the two are not the same.","I thought that the meaning of $$ \frac{\partial f(x, y, z)}{\partial x} $$ is differentiation on $x$ with fixed $y$ and $z$. So $(x, y, z)$ in the numerator is just saying which variables are fixed. If I need to indicate where the derivative is evaluated, I write it in the right of a vertical bar as a subscript. But today my teacher used $(x, y, z)$ in the numerator to denote where the derivative is evaluated. So, for example, $$ \frac{\partial f(0, 0, 0)}{\partial x} $$ means $$ \frac{\partial f(x, y, z)}{\partial x} \bigg\rvert_{x=0,y=0,z=0} $$ Is that a standard convention? If so, what is the meaning of this? $$ \frac{\partial f(x, y, g(x, y))}{\partial x} $$ I have two candidates. One is a partial derivative of the composition of $f$ and $g$ where $g$ has some fixed value, and the other is the partial derivative of $f$ on $x$ evaluated at $(x, y, g(x, y))$. I think the two are not the same.",,"['calculus', 'notation', 'partial-derivative']"
9,"Closed form for $\prod_{n=1}^\infty\sqrt[2^n]{\tanh(2^n)},$",Closed form for,"\prod_{n=1}^\infty\sqrt[2^n]{\tanh(2^n)},","Please help me to find a closed form for the infinite product $$\prod_{n=1}^\infty\sqrt[2^n]{\tanh(2^n)},$$ where $\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$  is the hyperbolic tangent .","Please help me to find a closed form for the infinite product $$\prod_{n=1}^\infty\sqrt[2^n]{\tanh(2^n)},$$ where $\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$  is the hyperbolic tangent .",,"['calculus', 'closed-form', 'infinite-product', 'hyperbolic-functions', 'trigonometric-series']"
10,"Closed form for $\int_0^1\frac{x^{5/6}}{(1-x)^{1/6}\,(1+2\,x)^{4/3}}\log\left(\frac{1+2x}{x\,(1-x)}\right)\,dx$",Closed form for,"\int_0^1\frac{x^{5/6}}{(1-x)^{1/6}\,(1+2\,x)^{4/3}}\log\left(\frac{1+2x}{x\,(1-x)}\right)\,dx","I need to evaluate this integral: $$Q=\int_0^1\frac{x^{5/6}}{(1-x)^{1/6}\,(1+2\,x)^{4/3}}\log\left(\frac{1+2x}{x\,(1-x)}\right)\,dx.$$ I tried it in Mathematica , but it was not able to find a closed form. A numerical integration returned $$Q\approx0.670803371410017436741...$$ Is it possible to find a closed form for $Q$? This integral is not from a book, it is part of some calculations related to theoretical physics. I do not have a particular reason to be sure that a closed form exists.","I need to evaluate this integral: $$Q=\int_0^1\frac{x^{5/6}}{(1-x)^{1/6}\,(1+2\,x)^{4/3}}\log\left(\frac{1+2x}{x\,(1-x)}\right)\,dx.$$ I tried it in Mathematica , but it was not able to find a closed form. A numerical integration returned $$Q\approx0.670803371410017436741...$$ Is it possible to find a closed form for $Q$? This integral is not from a book, it is part of some calculations related to theoretical physics. I do not have a particular reason to be sure that a closed form exists.",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
11,On the convergence of $\sum_{n = 1}^\infty\frac{\sin\left(n^a\right)}{n^b}$,On the convergence of,\sum_{n = 1}^\infty\frac{\sin\left(n^a\right)}{n^b},"Given the infinite series $$\begin{aligned}\sum_{n = 1}^{\infty}\end{aligned} \frac{\sin\left(n^a\right)}{n^b}$$ with $a,\,b \in \mathbb{R}$ , study when it converges and when it diverges. Easy cases $\forall\,a \in \mathbb{R}$ we have $\left|\frac{\sin\left(n^a\right)}{n^b}\right| \le \frac{1}{n^b}$ so the series $\color{green}{\text{converges}}$ for $b > 1$ . If $a \le 0$ we have $\frac{\sin\left(n^a\right)}{n^b} \le \frac{1}{n^{b-a}}$ so the series $\color{blue}{\text{diverges}}$ for $b \le a + 1$ and $\color{green}{\text{converges}}$ for $b > a + 1$ . If $a > 0 \, \land \, b \le 0$ we have $\not\exists \begin{aligned}\lim_{n \to \infty} \frac{\sin\left(n^a\right)}{n^b} \end{aligned}$ so the series $\color{blue}{\text{diverges}}$ . If $a = 1\, \land \, b > 0$ the series $\color{green}{\text{converges}}$ by Abel-Dirichlet's test . Hard cases If $0 < a < 1\, \land \, 0 < b \le 1-a$ the series $\color{blue}{\text{diverges}}$ by proof of i707107 . If $0 < a < \frac{1}{2}\, \land \, a < b \le 1-a$ the series $\color{blue}{\text{diverges}}$ by proof of RRL . If $0 < a < 1\, \land \, b > 1-a$ the series $\color{green}{\text{converges}}$ by proof of i707107 . If $a > 0\, \land \, b > \max(a,\,1-a)$ the series $\color{green}{\text{converges}}$ by proof of RRL . If $k \in \mathbb{Z}_{\ge 2}, $ $k-1 < a < k\, \land \, b > 1 - \frac{k-a}{2^k-2}$ the series $\color{green}{\text{converges}}$ by proof of i707107 . If $a > 0 \, \land \, b = 1$ the series $\color{green}{\text{converges}}$ by proof David Speyer (+ i707107 in the comments). If $a = 2 \, \land \, 0 < b \le \frac{1}{2}$ the series $\color{blue}{\text{diverges}}$ ( Theorem 2.30 by Hardy&Littlewood ). $\color{red}{\textbf{Open cases}}$ $a = \frac{3}{2} \land b = \frac{1}{4}$ : $a = \frac{3}{2} \land b = \frac{1}{2}$ : $a = \frac{3}{2} \land b = \frac{3}{4}$ : $a = 2 \land b = \frac{3}{4}$ : $a = \frac{5}{2} \land b = \frac{1}{2}$ :","Given the infinite series with , study when it converges and when it diverges. Easy cases we have so the series for . If we have so the series for and for . If we have so the series . If the series by Abel-Dirichlet's test . Hard cases If the series by proof of i707107 . If the series by proof of RRL . If the series by proof of i707107 . If the series by proof of RRL . If the series by proof of i707107 . If the series by proof David Speyer (+ i707107 in the comments). If the series ( Theorem 2.30 by Hardy&Littlewood ). : : : : :","\begin{aligned}\sum_{n = 1}^{\infty}\end{aligned} \frac{\sin\left(n^a\right)}{n^b} a,\,b \in \mathbb{R} \forall\,a \in \mathbb{R} \left|\frac{\sin\left(n^a\right)}{n^b}\right| \le \frac{1}{n^b} \color{green}{\text{converges}} b > 1 a \le 0 \frac{\sin\left(n^a\right)}{n^b} \le \frac{1}{n^{b-a}} \color{blue}{\text{diverges}} b \le a + 1 \color{green}{\text{converges}} b > a + 1 a > 0 \, \land \, b \le 0 \not\exists \begin{aligned}\lim_{n \to \infty} \frac{\sin\left(n^a\right)}{n^b} \end{aligned} \color{blue}{\text{diverges}} a = 1\, \land \, b > 0 \color{green}{\text{converges}} 0 < a < 1\, \land \, 0 < b \le 1-a \color{blue}{\text{diverges}} 0 < a < \frac{1}{2}\, \land \, a < b \le 1-a \color{blue}{\text{diverges}} 0 < a < 1\, \land \, b > 1-a \color{green}{\text{converges}} a > 0\, \land \, b > \max(a,\,1-a) \color{green}{\text{converges}} k \in \mathbb{Z}_{\ge 2},  k-1 < a < k\, \land \, b > 1 - \frac{k-a}{2^k-2} \color{green}{\text{converges}} a > 0 \, \land \, b = 1 \color{green}{\text{converges}} a = 2 \, \land \, 0 < b \le \frac{1}{2} \color{blue}{\text{diverges}} \color{red}{\textbf{Open cases}} a = \frac{3}{2} \land b = \frac{1}{4} a = \frac{3}{2} \land b = \frac{1}{2} a = \frac{3}{2} \land b = \frac{3}{4} a = 2 \land b = \frac{3}{4} a = \frac{5}{2} \land b = \frac{1}{2}","['calculus', 'sequences-and-series', 'convergence-divergence', 'divergent-series']"
12,Are elementary and generalized hypergeometric functions sufficient to express all algebraic numbers?,Are elementary and generalized hypergeometric functions sufficient to express all algebraic numbers?,,"Are (integers) plus (elementary functions) plus (generalized hypergeometric functions) sufficient to represent any algebraic number? For example, the real algebraic number $\alpha\in(-1,0)$ satisfying $$65536\,\alpha^{10}+327680\,\alpha^9+327680\,\alpha^8-655360\,\alpha   ^7-983040\,\alpha^6+16720896\,\alpha^5\\+20983040\,\alpha^4-655360\,\alpha   ^3-109155805\,\alpha^2-30844195\,\alpha +16762589=0$$ can be represented as $$\alpha={_4F_3}\left(\begin{array}c\frac15,\frac25,\frac35,\frac45\\\frac12,\frac34,\frac54\end{array}\middle|\frac1{\sqrt5}\right)-\frac{1+\sqrt5}2.$$ (see Bring radical for details) Here are answers where I used some particular cases when this representation is possible: [1] , [2] . These cases are motivating to try to find a general method applicable to all algebraic numbers.","Are (integers) plus (elementary functions) plus (generalized hypergeometric functions) sufficient to represent any algebraic number? For example, the real algebraic number $\alpha\in(-1,0)$ satisfying $$65536\,\alpha^{10}+327680\,\alpha^9+327680\,\alpha^8-655360\,\alpha   ^7-983040\,\alpha^6+16720896\,\alpha^5\\+20983040\,\alpha^4-655360\,\alpha   ^3-109155805\,\alpha^2-30844195\,\alpha +16762589=0$$ can be represented as $$\alpha={_4F_3}\left(\begin{array}c\frac15,\frac25,\frac35,\frac45\\\frac12,\frac34,\frac54\end{array}\middle|\frac1{\sqrt5}\right)-\frac{1+\sqrt5}2.$$ (see Bring radical for details) Here are answers where I used some particular cases when this representation is possible: [1] , [2] . These cases are motivating to try to find a general method applicable to all algebraic numbers.",,"['calculus', 'algebraic-number-theory', 'special-functions', 'hypergeometric-function', 'radicals']"
13,Examples of Taylor series with interesting convergence along the boundary of convergence?,Examples of Taylor series with interesting convergence along the boundary of convergence?,,"In most standard examples of power series, the question of convergence along the boundary of convergence has one of several ""simple"" answers.  (I am considering power series of a complex variable.) The series always converges along its boundary $\left(\displaystyle\sum_{n=1}^\infty\frac{x^n}{n^2}\right)$ The series never converges along its boundary $\left(\displaystyle\sum_{n=0}^\infty\ x^n\right)$ The series diverges at precisely one point along its boundary $\left(\displaystyle\sum_{n=1}^\infty\ \frac{x^n}{n}\right)$ The series diverges at a finite number of points along its boundary (add several examples of the preceding type together). Can anything else happen? For starters, are there examples where the series converges at precisely a finite number of points along the boundary? Can there be a dense mixture of convergence and divergence along the boundary?  For example, maybe a series with radius $1$ that converges for $x=\mathrm{e}^{2\pi it}$ with $t$ rational, but diverges when $t$ is irrational. Can there be convergence in large connected regions on the boundary with simultaneous divergence in other large connected regions?  For example, a series with radius $1$ that converges along the ""right side"" of the boundary $\left(x=\mathrm{e}^{2\pi it}\mbox{ with }t\in\left(-\frac{1}{4},\frac{1}{4}\right)\right)$ and diverges elsewhere on the boundary. I'm curious for any examples of these types or any type beyond the bulleted types.","In most standard examples of power series, the question of convergence along the boundary of convergence has one of several ""simple"" answers.  (I am considering power series of a complex variable.) The series always converges along its boundary $\left(\displaystyle\sum_{n=1}^\infty\frac{x^n}{n^2}\right)$ The series never converges along its boundary $\left(\displaystyle\sum_{n=0}^\infty\ x^n\right)$ The series diverges at precisely one point along its boundary $\left(\displaystyle\sum_{n=1}^\infty\ \frac{x^n}{n}\right)$ The series diverges at a finite number of points along its boundary (add several examples of the preceding type together). Can anything else happen? For starters, are there examples where the series converges at precisely a finite number of points along the boundary? Can there be a dense mixture of convergence and divergence along the boundary?  For example, maybe a series with radius $1$ that converges for $x=\mathrm{e}^{2\pi it}$ with $t$ rational, but diverges when $t$ is irrational. Can there be convergence in large connected regions on the boundary with simultaneous divergence in other large connected regions?  For example, a series with radius $1$ that converges along the ""right side"" of the boundary $\left(x=\mathrm{e}^{2\pi it}\mbox{ with }t\in\left(-\frac{1}{4},\frac{1}{4}\right)\right)$ and diverges elsewhere on the boundary. I'm curious for any examples of these types or any type beyond the bulleted types.",,"['calculus', 'complex-analysis', 'convergence-divergence']"
14,Evaluating the infinite series $\sum\limits_{n=1}^\infty(\sin\frac1{2n}-\sin\frac1{2n+1})$,Evaluating the infinite series,\sum\limits_{n=1}^\infty(\sin\frac1{2n}-\sin\frac1{2n+1}),"I've been bored and came across in my book a pretty straightforward series problem, namely to determine the convergence of $$ \sum_{n = 1}^{\infty} \left[\sin\left(1 \over 2n\right) - \sin\left(1 \over 2n + 1\right)\right] $$ Doing so was trivial by rewriting it as an alternating series involving the term $(-1)^k\sin\frac1k$ . Naturally, though, I was curious as to whether this series can be reduced to a simpler closed form in terms of more fundamental constants. Unfortunately I do not immediately know of any techniques of use here or even whether it permits such a 'nice' form. Do any of you? I do know from playing with the Euler-Maclaurin sums the value should be something near $0.290674$ . As $n\to\infty$ I know the sequence terms behave increasingly like those of the alternating harmonic series (as $\sin x\sim x$ for $|x|\ll1$ ), which helps explain why it appears relatively near $1-\log2$ . I have also found that the difference between it and the alternating harmonic series starting with $1/2$ is near $0.016179$ . I should note that I am a high school student with an amateur interest in recreational math. My knowledge extends only as far as elementary calculus of multiple variables and first-year ordinary and partial differential equations. It may very well be that an obvious approach exists that I've completely missed and so I feel obligated to apologize in advance.","I've been bored and came across in my book a pretty straightforward series problem, namely to determine the convergence of Doing so was trivial by rewriting it as an alternating series involving the term . Naturally, though, I was curious as to whether this series can be reduced to a simpler closed form in terms of more fundamental constants. Unfortunately I do not immediately know of any techniques of use here or even whether it permits such a 'nice' form. Do any of you? I do know from playing with the Euler-Maclaurin sums the value should be something near . As I know the sequence terms behave increasingly like those of the alternating harmonic series (as for ), which helps explain why it appears relatively near . I have also found that the difference between it and the alternating harmonic series starting with is near . I should note that I am a high school student with an amateur interest in recreational math. My knowledge extends only as far as elementary calculus of multiple variables and first-year ordinary and partial differential equations. It may very well be that an obvious approach exists that I've completely missed and so I feel obligated to apologize in advance.","
\sum_{n = 1}^{\infty}
\left[\sin\left(1 \over 2n\right) - \sin\left(1 \over 2n + 1\right)\right]
 (-1)^k\sin\frac1k 0.290674 n\to\infty \sin x\sim x |x|\ll1 1-\log2 1/2 0.016179","['calculus', 'sequences-and-series', 'limits']"
15,Is there another way to solve this integral?,Is there another way to solve this integral?,,My way to solve this integral. I wonder is there another way to solve it as it's very long for me. $$\int_{0}^{\pi}\frac{1-\sin (x)}{\sin (x)+1}dx$$ Let $$u=\tan (\frac{x}{2})$$ $$du=\frac{1}{2}\sec ^2(\frac{x}{2})dx $$ By Weierstrass Substitution $$\sin (x)=\frac{2u}{u^2+1}$$ $$\cos (x)=\frac{1-u^2}{u^2+1}$$ $$dx=\frac{2du}{u^2+1}$$ $$=\int_{0}^{\infty }\frac{2(1-\frac{2u}{u^2+1})}{(u^2+1)(\frac{2u}{u^2+1}+1)}du$$ $$=\int_{0}^{\infty }\frac{2(u-1)^2}{u^4+2u^3+2u^2+2u+1}du $$ $$=2\int_{0}^{\infty }\frac{(u-1)^2}{u^4+2u^3+2u^2+2u+1}du  $$ $$=2\int_{0}^{\infty }\frac{(u-1)^2}{(u+1)^2(u^2+1)}du $$ $$=2\int_{0}^{\infty }(\frac{2}{(u+1)^2}-\frac{1}{u^2+1})du $$ $$=-2\int_{0}^{\infty  }\frac{1}{u^2+1}du+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du $$ $$\lim_{b\rightarrow \infty }\left | (-2\tan^{-1}(u)) \right |_{0}^{b}+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ $$=(\lim_{b\rightarrow \infty}-2\tan^{-1}(b))+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ $$=-\pi+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ Let $$s=u+1$$ $$ds=du$$ $$=-\pi+4\int_{1}^{\infty}\frac{1}{s^2}ds$$ $$=-\pi+\lim_{b\rightarrow \infty}\left | (-\frac{4}{s}) \right |_{1}^{b}$$ $$=-\pi+(\lim_{b\rightarrow \infty} -\frac{4}{b}) +4$$ $$=4-\pi$$ $$\approx 0.85841$$,My way to solve this integral. I wonder is there another way to solve it as it's very long for me. $$\int_{0}^{\pi}\frac{1-\sin (x)}{\sin (x)+1}dx$$ Let $$u=\tan (\frac{x}{2})$$ $$du=\frac{1}{2}\sec ^2(\frac{x}{2})dx $$ By Weierstrass Substitution $$\sin (x)=\frac{2u}{u^2+1}$$ $$\cos (x)=\frac{1-u^2}{u^2+1}$$ $$dx=\frac{2du}{u^2+1}$$ $$=\int_{0}^{\infty }\frac{2(1-\frac{2u}{u^2+1})}{(u^2+1)(\frac{2u}{u^2+1}+1)}du$$ $$=\int_{0}^{\infty }\frac{2(u-1)^2}{u^4+2u^3+2u^2+2u+1}du $$ $$=2\int_{0}^{\infty }\frac{(u-1)^2}{u^4+2u^3+2u^2+2u+1}du  $$ $$=2\int_{0}^{\infty }\frac{(u-1)^2}{(u+1)^2(u^2+1)}du $$ $$=2\int_{0}^{\infty }(\frac{2}{(u+1)^2}-\frac{1}{u^2+1})du $$ $$=-2\int_{0}^{\infty  }\frac{1}{u^2+1}du+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du $$ $$\lim_{b\rightarrow \infty }\left | (-2\tan^{-1}(u)) \right |_{0}^{b}+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ $$=(\lim_{b\rightarrow \infty}-2\tan^{-1}(b))+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ $$=-\pi+4\int_{0}^{\infty}\frac{1}{(u+1)^2}du$$ Let $$s=u+1$$ $$ds=du$$ $$=-\pi+4\int_{1}^{\infty}\frac{1}{s^2}ds$$ $$=-\pi+\lim_{b\rightarrow \infty}\left | (-\frac{4}{s}) \right |_{1}^{b}$$ $$=-\pi+(\lim_{b\rightarrow \infty} -\frac{4}{b}) +4$$ $$=4-\pi$$ $$\approx 0.85841$$,,"['calculus', 'integration', 'definite-integrals']"
16,Why does a circle enclose the largest area?,Why does a circle enclose the largest area?,,"In this wikipedia, article http://en.wikipedia.org/wiki/Circle#Area_enclosed its stated that the circle is the closed curve which has the maximum area for a given arc length. First, of all, I would like to see different proofs, for this result. (If there are any elementary ones!) One, interesting observation, which one can think while seeing this problem, is: How does one propose such type of problem? Does, anyone take all closed curves, and calculate their area to come this conclusion? I don't think that's the right intuition.","In this wikipedia, article http://en.wikipedia.org/wiki/Circle#Area_enclosed its stated that the circle is the closed curve which has the maximum area for a given arc length. First, of all, I would like to see different proofs, for this result. (If there are any elementary ones!) One, interesting observation, which one can think while seeing this problem, is: How does one propose such type of problem? Does, anyone take all closed curves, and calculate their area to come this conclusion? I don't think that's the right intuition.",,"['calculus', 'geometry']"
17,Why does L'Hôpital's rule work?,Why does L'Hôpital's rule work?,,"Can anyone tell me why the L'Hôpital's rule works for evaluating limits of the form $\frac{0}{0}$ and $\frac{\infty}{\infty}$ ? What I understand about limits is that when you divide a really small thing (that is $\rightarrow0$ ) by another really small thing, we get a finite value which may not be so small. So how does differentiating the numerator and denominator help us get the Limit of a function?","Can anyone tell me why the L'Hôpital's rule works for evaluating limits of the form and ? What I understand about limits is that when you divide a really small thing (that is ) by another really small thing, we get a finite value which may not be so small. So how does differentiating the numerator and denominator help us get the Limit of a function?",\frac{0}{0} \frac{\infty}{\infty} \rightarrow0,['calculus']
18,Is differentiating on both sides of an equation allowed? [duplicate],Is differentiating on both sides of an equation allowed? [duplicate],,"This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$ So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?","This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$ So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?",,['calculus']
19,How is the derivative geometrically inverse of integral?,How is the derivative geometrically inverse of integral?,,"I know that derivative is the slope of the tangent line, and that integral is the area under the curve. My question is that how these two distinct concepts are geometrically related? What is the relation between the slope of the tangent line and the area under the curve? If these are inverse of each other, then there should be a relation between them, I believe.","I know that derivative is the slope of the tangent line, and that integral is the area under the curve. My question is that how these two distinct concepts are geometrically related? What is the relation between the slope of the tangent line and the area under the curve? If these are inverse of each other, then there should be a relation between them, I believe.",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
20,Using the Limit definition to find the derivative of $e^x$,Using the Limit definition to find the derivative of,e^x,"I was wondering how we could use the limit definition $$ \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$$ to find the derivative of $e^x$, I get to a point where I do not know how to simplify the indeterminate $\frac{0}{0}$. Below is what I have already done $$\begin{align} &\lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \\ &\lim_{h \rightarrow 0} \frac{e^{x+h}-e^x}{h} \\ &\lim_{h \rightarrow 0} \frac{e^x (e^h-1)}{h} \\ &e^x \cdot \lim_{h \rightarrow 0} \frac{e^h-1}{h} \end{align}$$ Where can I go from here? Because, the $\lim$ portion reduces to indeterminate when $0$ is subbed into $h$.","I was wondering how we could use the limit definition $$ \lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$$ to find the derivative of $e^x$, I get to a point where I do not know how to simplify the indeterminate $\frac{0}{0}$. Below is what I have already done $$\begin{align} &\lim_{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \\ &\lim_{h \rightarrow 0} \frac{e^{x+h}-e^x}{h} \\ &\lim_{h \rightarrow 0} \frac{e^x (e^h-1)}{h} \\ &e^x \cdot \lim_{h \rightarrow 0} \frac{e^h-1}{h} \end{align}$$ Where can I go from here? Because, the $\lim$ portion reduces to indeterminate when $0$ is subbed into $h$.",,"['calculus', 'derivatives']"
21,Relation between the roots of a function,Relation between the roots of a function,,"I have this question from my exam where it's asked to find the sum: $$S=\sum_{k=1}^n \frac{1}{(1-r_k)^2}$$ where $r_k$ are the roots of $$f(x)=x^n-2x+2\quad,n\ge3$$ I recalled this relation $$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$ and quickly realised that $$\frac{d}{dx}\frac{1}{x}=-\frac{1}{x^2}$$ and using derivative would easily produce my answer. Indeed $$\frac{d}{dx}\left( \frac{f'(x)}{f(x)} \right)=\frac{f''(x)f(x)-(f'(x))^2}{f(x)^2}=-\sum_{k=1}^n \frac{1}{(x-r_k)^2}$$ Evaluating at $x=1$ gives $$S=\frac{(n-2)^2-n(n-1)}{1^2}=4-3n$$ However after the exam I realised that I have no ideea why $$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$ and I just memorized it, is there a nice way to show it?  And of course maybe a more elegant solution to this exam question?","I have this question from my exam where it's asked to find the sum: $$S=\sum_{k=1}^n \frac{1}{(1-r_k)^2}$$ where $r_k$ are the roots of $$f(x)=x^n-2x+2\quad,n\ge3$$ I recalled this relation $$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$ and quickly realised that $$\frac{d}{dx}\frac{1}{x}=-\frac{1}{x^2}$$ and using derivative would easily produce my answer. Indeed $$\frac{d}{dx}\left( \frac{f'(x)}{f(x)} \right)=\frac{f''(x)f(x)-(f'(x))^2}{f(x)^2}=-\sum_{k=1}^n \frac{1}{(x-r_k)^2}$$ Evaluating at $x=1$ gives $$S=\frac{(n-2)^2-n(n-1)}{1^2}=4-3n$$ However after the exam I realised that I have no ideea why $$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$ and I just memorized it, is there a nice way to show it?  And of course maybe a more elegant solution to this exam question?",,"['calculus', 'functions', 'polynomials', 'roots']"
22,What books are recommended for learning calculus on my own? [duplicate],What books are recommended for learning calculus on my own? [duplicate],,"This question already has answers here : Calculus book recommendations (for complete beginner) [closed] (9 answers) Closed 9 years ago . I recently graduated with a degree in bachelor of science with a focus interactive and multimedia design. I had to opportunity to take 1 C++ course and 1 HTML course. I was also only required to take one math class, introduction to calculus (and it was hardly calculus, it was more of a math class for designers) Since graduating I have pursued a career in web development. I'm considering going back for a MS in computer science or MS in web technologies. I feel my math has suffered dramatically as I have always been behind most others on my math studies (middle school through college). I want to learn calculus as I feel it's an integral part of any CS degree. Does anyone have any recommendations on any great books that I can read through and teach my self a bit of calculus or other advanced math topics? Thanks for any advice","This question already has answers here : Calculus book recommendations (for complete beginner) [closed] (9 answers) Closed 9 years ago . I recently graduated with a degree in bachelor of science with a focus interactive and multimedia design. I had to opportunity to take 1 C++ course and 1 HTML course. I was also only required to take one math class, introduction to calculus (and it was hardly calculus, it was more of a math class for designers) Since graduating I have pursued a career in web development. I'm considering going back for a MS in computer science or MS in web technologies. I feel my math has suffered dramatically as I have always been behind most others on my math studies (middle school through college). I want to learn calculus as I feel it's an integral part of any CS degree. Does anyone have any recommendations on any great books that I can read through and teach my self a bit of calculus or other advanced math topics? Thanks for any advice",,"['calculus', 'soft-question', 'book-recommendation']"
23,How to Evaluate the Integral? $\int_{0}^{1}\frac{\ln\left( \frac{x+1}{2x^2} \right)}{\sqrt{x^2+2x}}dx=\frac{\pi^2}{2}$,How to Evaluate the Integral?,\int_{0}^{1}\frac{\ln\left( \frac{x+1}{2x^2} \right)}{\sqrt{x^2+2x}}dx=\frac{\pi^2}{2},"I am trying to find a closed form for $$ \int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right) {{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,}. $$ I have done trig substitution and it results in $$ \int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right) {{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,} = \int_{0}^{\pi/3}\sec\left(\theta\right) \ln\left(\frac{\sec\left(\theta\right)} {2\left[\sec\left(\theta\right) - 1\right]^{\,2}} \right){\rm d}\theta $$ which doesn't help. By part integration with $\displaystyle u = \ln\left(\frac{x + 1}{2x^{2}} \right)$ , $\displaystyle\,\,{\rm d}v=\frac{\displaystyle\,\,{\rm d}x}{\,\sqrt{\,{x^{2} + 2x}\,}\,}$ also makes it more complicated. I appreciate any help on this problem.","I am trying to find a closed form for I have done trig substitution and it results in which doesn't help. By part integration with , also makes it more complicated. I appreciate any help on this problem.","
\int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right)
{{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,}.
 
\int_{0}^{1}\ln\left(\frac{x + 1}{2x^{2}}\right)
{{\rm d}x \over \,\sqrt{\,{x^{2} + 2x}\,}\,} =
\int_{0}^{\pi/3}\sec\left(\theta\right)
\ln\left(\frac{\sec\left(\theta\right)}
{2\left[\sec\left(\theta\right) - 1\right]^{\,2}} \right){\rm d}\theta
 \displaystyle u = \ln\left(\frac{x + 1}{2x^{2}} \right) \displaystyle\,\,{\rm d}v=\frac{\displaystyle\,\,{\rm d}x}{\,\sqrt{\,{x^{2} + 2x}\,}\,}","['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'polylogarithm']"
24,Can this ant find its way back to the nest?,Can this ant find its way back to the nest?,,"So the puzzle is like this: An ant is out from its nest searching for food. It travels in a straight line from its nest. After this ant gets 40 ft away from the nest, suddenly a rain starts to pour and washes away all its scent trail. This ant has the strength of traveling 280 ft more then it will starve to death. Suppose this ant's nest is a huge wall and this ant can travel in a whatever curve it wants, how can this ant find its way back? I interpret it as: I start at the origin. I know that there is a straight line with distance 40 ft to the origin, but I don't know the direction. In what parametric curve I will sure hit the line when the parameter $t$ is increasing, while the total arc length is less than or equal to 280 ft. I asked a friend of mine who is a PhD in math, he told me this is a calculus of variation problem. I wonder if I could use basic calculus stuff to solve this puzzle (I have learned ODE as well). My hunch tells me that a spiral should be used as the path, yet I am not sure what kind of spiral to use here. Any hint shall be appreciated. Thanks dudes! Clarification by dfeuer As some people seem to be having trouble understanding the problem description, I'll add an equivalent one that should be clear: Starting in the center of a circle of radius 40 ft, draw a path with the shortest possible length that intersects every line that is tangent to the circle.","So the puzzle is like this: An ant is out from its nest searching for food. It travels in a straight line from its nest. After this ant gets 40 ft away from the nest, suddenly a rain starts to pour and washes away all its scent trail. This ant has the strength of traveling 280 ft more then it will starve to death. Suppose this ant's nest is a huge wall and this ant can travel in a whatever curve it wants, how can this ant find its way back? I interpret it as: I start at the origin. I know that there is a straight line with distance 40 ft to the origin, but I don't know the direction. In what parametric curve I will sure hit the line when the parameter $t$ is increasing, while the total arc length is less than or equal to 280 ft. I asked a friend of mine who is a PhD in math, he told me this is a calculus of variation problem. I wonder if I could use basic calculus stuff to solve this puzzle (I have learned ODE as well). My hunch tells me that a spiral should be used as the path, yet I am not sure what kind of spiral to use here. Any hint shall be appreciated. Thanks dudes! Clarification by dfeuer As some people seem to be having trouble understanding the problem description, I'll add an equivalent one that should be clear: Starting in the center of a circle of radius 40 ft, draw a path with the shortest possible length that intersects every line that is tangent to the circle.",,"['calculus', 'optimization', 'recreational-mathematics', 'puzzle', 'calculus-of-variations']"
25,Find the closed form of $\sum_{n=1}^{\infty} \frac{H_{ n}}{2^nn^4}$,Find the closed form of,\sum_{n=1}^{\infty} \frac{H_{ n}}{2^nn^4},"One of the possible ways of computing the series is to obtain the generating function, but this might be a tedious, hard work, pretty hard to obtain. What would you propose then? $$\sum_{n=1}^{\infty}  \frac{H_{ n}}{2^nn^4}$$","One of the possible ways of computing the series is to obtain the generating function, but this might be a tedious, hard work, pretty hard to obtain. What would you propose then? $$\sum_{n=1}^{\infty}  \frac{H_{ n}}{2^nn^4}$$",,"['calculus', 'integration', 'sequences-and-series', 'closed-form', 'harmonic-numbers']"
26,Evaluating $\int_0^\pi\arctan\bigl(\frac{\ln\sin x}{x}\bigr)\mathrm{d}x$,Evaluating,\int_0^\pi\arctan\bigl(\frac{\ln\sin x}{x}\bigr)\mathrm{d}x,"I found the following integral as a by product of another one. It has a nice closed form. $$ \int_{0}^{\pi} \arctan\left(\ln\left(\sin x \right) \over x\right)\,{\rm d}x  $$ Mathematica and Maple fail to give the answer. Could you find it? Hint 1: The closed form is $$ -\pi\arctan \left(2\ln 2  \over \pi\right)  $$ Hint 2: The following integral may help $$ \int_{0}^{\pi}{x \over x^{2} + \ln^{2}\left(\alpha\sin x \right)} \,{\rm d}x $$ ( see this post ).","I found the following integral as a by product of another one. It has a nice closed form. $$ \int_{0}^{\pi} \arctan\left(\ln\left(\sin x \right) \over x\right)\,{\rm d}x  $$ Mathematica and Maple fail to give the answer. Could you find it? Hint 1: The closed form is $$ -\pi\arctan \left(2\ln 2  \over \pi\right)  $$ Hint 2: The following integral may help $$ \int_{0}^{\pi}{x \over x^{2} + \ln^{2}\left(\alpha\sin x \right)} \,{\rm d}x $$ ( see this post ).",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
27,Integral ${\large\int}_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}}$,Integral,{\large\int}_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}},How to prove the following conjectured identity? $$\int_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}}\stackrel{\color{#a0a0a0}?}=\frac{\sqrt[4]6}{3\sqrt\pi}\Gamma^2\big(\tfrac14\big)\tag1$$ It holds numerically with precision of at least $1000$ decimal digits. Are there any other integers under the radical except $7$ and $1$ that result in a nice closed form?,How to prove the following conjectured identity? $$\int_0^\infty\frac{dx}{\sqrt[4]{7+\cosh x}}\stackrel{\color{#a0a0a0}?}=\frac{\sqrt[4]6}{3\sqrt\pi}\Gamma^2\big(\tfrac14\big)\tag1$$ It holds numerically with precision of at least $1000$ decimal digits. Are there any other integers under the radical except $7$ and $1$ that result in a nice closed form?,,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'hyperbolic-functions']"
28,Can anyone explain the intuitive meaning of 'integrating on both sides of the equation' when solving differential equations?,Can anyone explain the intuitive meaning of 'integrating on both sides of the equation' when solving differential equations?,,"For solving differential equations, especially the ones of the form $$g(x)dx = h(y)dy$$ we solve the equation by integrating on both sides to reveal the solution. Understanding this for differentiating the equation on both sides is relatively easy. We know that we can formulate an alternative equation in terms of differentials for the original equation involved and come out with a new differential equation that holds because of the properties of the differentials. But how does it work for integration on both sides? Am I missing any point here? I have referred to multiple books but none give a satisfactory explanation. Integrating an equation on both sides seems really wrong , if I may dare to use the word. Please help. I'm stuck with this thing and I can only begin to understand differential equations once this is cleared from my head. Thank you very much!","For solving differential equations, especially the ones of the form we solve the equation by integrating on both sides to reveal the solution. Understanding this for differentiating the equation on both sides is relatively easy. We know that we can formulate an alternative equation in terms of differentials for the original equation involved and come out with a new differential equation that holds because of the properties of the differentials. But how does it work for integration on both sides? Am I missing any point here? I have referred to multiple books but none give a satisfactory explanation. Integrating an equation on both sides seems really wrong , if I may dare to use the word. Please help. I'm stuck with this thing and I can only begin to understand differential equations once this is cleared from my head. Thank you very much!",g(x)dx = h(y)dy,"['calculus', 'integration', 'ordinary-differential-equations']"
29,"Prove $_2F_1\left(\frac13,\frac13;\frac56;-27\right)\stackrel{\color{#808080}?}=\frac47$",Prove,"_2F_1\left(\frac13,\frac13;\frac56;-27\right)\stackrel{\color{#808080}?}=\frac47","I discovered the following conjecture numerically, but have not been able to prove it yet: $$_2F_1\left(\frac13,\frac13;\frac56;-27\right)\stackrel{\color{#808080}?}=\frac47.\tag1$$ The equality holds with at least $10000$ decimal digits of precision. It can be written in equivalent forms in terms of definite integrals: $${\large\int}_0^1\frac{dx}{\sqrt{1-x}\ \sqrt[3]{x^2+(3x)^3}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]4\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right),\tag2$$ or $${\large\int}_0^\pi\frac{d\phi}{\sqrt[3]{\sin\phi}\,\sqrt[3]{55+12\sqrt{21}\cos\phi}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]4\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right).\tag3$$ Update: A several more equivalent forms: $$_2F_1\left(\frac13,\frac12;\frac56;\frac{27}{28}\right)\stackrel{\color{#808080}?}=\frac{2^{\small8/3}}{7^{\small2/3}}\tag4$$ $$\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]2\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right)\tag5$$ $$C_{\small-1/3}^{\small(1/3)}(55)\stackrel{\color{#808080}?}=\frac{3}{7\pi^2}\Gamma^3\!\!\left(\tfrac13\right)\tag6$$ $$P_{\small-1/2}^{\small1/6}(55)\stackrel{\color{#808080}?}=\frac{\sqrt2\,\sqrt[4]3\,e^{\small-\pi\,i/12}}{7^{\small13/12}\,\pi^{\small3/2}}\Gamma^2\!\!\left(\tfrac13\right)\tag7$$ where $C_n^{(\lambda)}(x)$ is the Gegenbauer polynomial and $P_l^m(x)$ is the Legendre function of the first kind . Please suggest ideas how to prove this conjecture. What are other points where the function $_2F_1\left(\frac13,\frac13;\frac56;z\right)$ takes simple special values?","I discovered the following conjecture numerically, but have not been able to prove it yet: $$_2F_1\left(\frac13,\frac13;\frac56;-27\right)\stackrel{\color{#808080}?}=\frac47.\tag1$$ The equality holds with at least $10000$ decimal digits of precision. It can be written in equivalent forms in terms of definite integrals: $${\large\int}_0^1\frac{dx}{\sqrt{1-x}\ \sqrt[3]{x^2+(3x)^3}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]4\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right),\tag2$$ or $${\large\int}_0^\pi\frac{d\phi}{\sqrt[3]{\sin\phi}\,\sqrt[3]{55+12\sqrt{21}\cos\phi}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]4\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right).\tag3$$ Update: A several more equivalent forms: $$_2F_1\left(\frac13,\frac12;\frac56;\frac{27}{28}\right)\stackrel{\color{#808080}?}=\frac{2^{\small8/3}}{7^{\small2/3}}\tag4$$ $$\int_0^\infty\frac{dx}{\sqrt[3]{55+\cosh x}}\stackrel{\color{#808080}?}=\frac{\sqrt[3]2\,\sqrt3}{7\pi}\Gamma^3\!\!\left(\tfrac13\right)\tag5$$ $$C_{\small-1/3}^{\small(1/3)}(55)\stackrel{\color{#808080}?}=\frac{3}{7\pi^2}\Gamma^3\!\!\left(\tfrac13\right)\tag6$$ $$P_{\small-1/2}^{\small1/6}(55)\stackrel{\color{#808080}?}=\frac{\sqrt2\,\sqrt[4]3\,e^{\small-\pi\,i/12}}{7^{\small13/12}\,\pi^{\small3/2}}\Gamma^2\!\!\left(\tfrac13\right)\tag7$$ where $C_n^{(\lambda)}(x)$ is the Gegenbauer polynomial and $P_l^m(x)$ is the Legendre function of the first kind . Please suggest ideas how to prove this conjecture. What are other points where the function $_2F_1\left(\frac13,\frac13;\frac56;z\right)$ takes simple special values?",,"['calculus', 'definite-integrals', 'special-functions', 'closed-form', 'hypergeometric-function']"
30,Limit with a big exponentiation tower,Limit with a big exponentiation tower,,"Find the value of the following limit:   $$\huge\lim_{x\to\infty}e^{e^{e^{\biggl(x\,+\,e^{-\left(a+x+e^{\Large x}+e^{\Large e^x}\right)}\biggr)}}}-e^{e^{e^{x}}}$$ ( original image ) I don't even know how to start with. (this problem was shared in Brilliant.org) Some of the ideas I tried is to take the natural log of this expression and reduce it to $\ln(a/b)$ then use L'Hopital's but that made it false!! I know the value of the limit it is $e^{-a}$ but please how to prove it?","Find the value of the following limit:   $$\huge\lim_{x\to\infty}e^{e^{e^{\biggl(x\,+\,e^{-\left(a+x+e^{\Large x}+e^{\Large e^x}\right)}\biggr)}}}-e^{e^{e^{x}}}$$ ( original image ) I don't even know how to start with. (this problem was shared in Brilliant.org) Some of the ideas I tried is to take the natural log of this expression and reduce it to $\ln(a/b)$ then use L'Hopital's but that made it false!! I know the value of the limit it is $e^{-a}$ but please how to prove it?",,"['calculus', 'limits', 'exponential-function']"
31,Limits notation,Limits notation,,I'm wondering what is the difference in the use of $$\lim\limits_{x \downarrow  a}$$ $$\lim\limits_{x \searrow  a}$$ $$\lim\limits_{x \nearrow  a}$$ $$\lim\limits_{x \uparrow  a}$$ I see them around and I don't know what they really mean. Do the arrows characterize how $x$ tends to $a$?,I'm wondering what is the difference in the use of $$\lim\limits_{x \downarrow  a}$$ $$\lim\limits_{x \searrow  a}$$ $$\lim\limits_{x \nearrow  a}$$ $$\lim\limits_{x \uparrow  a}$$ I see them around and I don't know what they really mean. Do the arrows characterize how $x$ tends to $a$?,,"['calculus', 'limits', 'notation']"
32,Mystery integral in the first issue of Annals of Mathematics,Mystery integral in the first issue of Annals of Mathematics,,"Vol 1. No. 1 of Annals of Mathematics has an ""Exercises"" section with this unusual integral sent in by a Professor Lewis Green Barbour: $$\int_{\frac \pi 2}^\pi \sqrt{1-\frac 1 2 \cos^2\vartheta + \sin\vartheta\sin2\vartheta}\,{\rm d}\vartheta$$ The other exercises are answered in later issues but this one seems to have been quietly forgotten. I've tried modern software and it's stumped on solving the indefinite integral. Numerical methods yield a value: $$\approx 0.8277600029391442$$ However, this doesn't seem to be a closed-form number in any obvious way. Searches of various number databases turn up nothing. Does this integral have a closed-form solution? If not, what methods would a reader in 1884 have used to solve it? Given how famous the Annals became, does this unsolved exercise from the very first issue have any lore attached to it?","Vol 1. No. 1 of Annals of Mathematics has an ""Exercises"" section with this unusual integral sent in by a Professor Lewis Green Barbour: The other exercises are answered in later issues but this one seems to have been quietly forgotten. I've tried modern software and it's stumped on solving the indefinite integral. Numerical methods yield a value: However, this doesn't seem to be a closed-form number in any obvious way. Searches of various number databases turn up nothing. Does this integral have a closed-form solution? If not, what methods would a reader in 1884 have used to solve it? Given how famous the Annals became, does this unsolved exercise from the very first issue have any lore attached to it?","\int_{\frac \pi 2}^\pi \sqrt{1-\frac 1 2 \cos^2\vartheta + \sin\vartheta\sin2\vartheta}\,{\rm d}\vartheta \approx 0.8277600029391442","['calculus', 'definite-integrals', 'math-history']"
33,Cannabis Equation,Cannabis Equation,,How can an equation for the following curve be derived? $$r=(1+0.9 \cos(8 \theta)) (1+0.1 \cos(24 \theta)) (0.9+0.1 \cos(200 \theta)) (1+\sin(\theta))$$ (From WolframAlpha ),How can an equation for the following curve be derived? $$r=(1+0.9 \cos(8 \theta)) (1+0.1 \cos(24 \theta)) (0.9+0.1 \cos(200 \theta)) (1+\sin(\theta))$$ (From WolframAlpha ),,"['calculus', 'algebra-precalculus', 'graphing-functions', 'polar-coordinates']"
34,how to compute the de Rham cohomology of the punctured plane just by Calculus?,how to compute the de Rham cohomology of the punctured plane just by Calculus?,,"I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\mathbb{R}^2\setminus\{0\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?","I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\mathbb{R}^2\setminus\{0\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?",,"['calculus', 'differential-geometry', 'algebraic-topology', 'homology-cohomology', 'smooth-manifolds']"
35,"Integral $\int_0^\infty\frac{1}{x\,\sqrt{2}+\sqrt{2\,x^2+1}}\cdot\frac{\log x}{\sqrt{x^2+1}}\mathrm dx$",Integral,"\int_0^\infty\frac{1}{x\,\sqrt{2}+\sqrt{2\,x^2+1}}\cdot\frac{\log x}{\sqrt{x^2+1}}\mathrm dx","I need your assistance with evaluating the integral $$\int_0^\infty\frac{1}{x\,\sqrt{2}+\sqrt{2\,x^2+1}}\cdot\frac{\log x}{\sqrt{x^2+1}}dx$$ I tried manual integration by parts, but it seemed to only complicate the integrand more. I also tried to evaluate it with a CAS, but it was not able to handle it.","I need your assistance with evaluating the integral $$\int_0^\infty\frac{1}{x\,\sqrt{2}+\sqrt{2\,x^2+1}}\cdot\frac{\log x}{\sqrt{x^2+1}}dx$$ I tried manual integration by parts, but it seemed to only complicate the integrand more. I also tried to evaluate it with a CAS, but it was not able to handle it.",,"['calculus', 'integration', 'logarithms', 'improper-integrals', 'closed-form']"
36,Differentiablility over closed intervals,Differentiablility over closed intervals,,"In calculus of one variable I read: A function $f$ is differentiable on an open interval if it is differentiable at every number of the interval. I wonder why in the definition we suppose that the interval is open. This is the case in Rolle theorem, Mean value theorem,etc. Do we have a notion of a function being differentiable on a closed interval $[a,b]$ ?","In calculus of one variable I read: A function is differentiable on an open interval if it is differentiable at every number of the interval. I wonder why in the definition we suppose that the interval is open. This is the case in Rolle theorem, Mean value theorem,etc. Do we have a notion of a function being differentiable on a closed interval ?","f [a,b]","['calculus', 'derivatives']"
37,How to show that $\lim\limits_{x \to \infty} f'(x) = 0$ implies $\lim\limits_{x \to \infty} \frac{f(x)}{x} = 0$?,How to show that  implies ?,\lim\limits_{x \to \infty} f'(x) = 0 \lim\limits_{x \to \infty} \frac{f(x)}{x} = 0,"I was trying to work out a problem I found online. Here is the problem statement: Let $f(x)$ be continuously differentiable on $(0, \infty)$ and suppose $\lim\limits_{x \to \infty} f'(x) = 0$. Prove that $\lim\limits_{x \to \infty} \frac{f(x)}{x} = 0$. (source: http://www.math.vt.edu/people/plinnell/Vtregional/E79/index.html ) The first idea that came to my mind was to show that for all $\epsilon > 0$, we have $|f(x)| < \epsilon|x|$ for sufficiently large $x$. (And I believe I could do this using the fact that $f'(x) \to 0$ as $x \to \infty$.) However, I was wondering if there was a different (and nicer or cleverer) way. Here's an idea I had in mind: If $f$ is bounded, then $\frac{f(x)}{x}$ clearly goes to zero. If $\lim\limits_{x \to \infty} f(x)$ is either $+\infty$ or $-\infty$, then we can apply l'Hôpital's rule (to get $\lim\limits_{x \to \infty} \frac{f(x)}{x} = \lim\limits_{x \to \infty} \frac{f'(x)}{1} = 0$). However, I'm not sure what I could do in the remaining case (when $f$ is unbounded but oscillates like crazy). Is there a way to finish the proof from here? Also, are there other ways of proving the given statement?","I was trying to work out a problem I found online. Here is the problem statement: Let $f(x)$ be continuously differentiable on $(0, \infty)$ and suppose $\lim\limits_{x \to \infty} f'(x) = 0$. Prove that $\lim\limits_{x \to \infty} \frac{f(x)}{x} = 0$. (source: http://www.math.vt.edu/people/plinnell/Vtregional/E79/index.html ) The first idea that came to my mind was to show that for all $\epsilon > 0$, we have $|f(x)| < \epsilon|x|$ for sufficiently large $x$. (And I believe I could do this using the fact that $f'(x) \to 0$ as $x \to \infty$.) However, I was wondering if there was a different (and nicer or cleverer) way. Here's an idea I had in mind: If $f$ is bounded, then $\frac{f(x)}{x}$ clearly goes to zero. If $\lim\limits_{x \to \infty} f(x)$ is either $+\infty$ or $-\infty$, then we can apply l'Hôpital's rule (to get $\lim\limits_{x \to \infty} \frac{f(x)}{x} = \lim\limits_{x \to \infty} \frac{f'(x)}{1} = 0$). However, I'm not sure what I could do in the remaining case (when $f$ is unbounded but oscillates like crazy). Is there a way to finish the proof from here? Also, are there other ways of proving the given statement?",,"['calculus', 'limits']"
38,"Prove $\int_0^1\frac{x^2-2\,x+2\ln(1+x)}{x^3\,\sqrt{1-x^2}}\mathrm dx=\frac{\pi^2}8-\frac12$",Prove,"\int_0^1\frac{x^2-2\,x+2\ln(1+x)}{x^3\,\sqrt{1-x^2}}\mathrm dx=\frac{\pi^2}8-\frac12","How can I prove the following identity? $$\int_0^1\frac{x^2-2\,x+2\ln(1+x)}{x^3\,\sqrt{1-x^2}}\mathrm dx=\frac{\pi^2}8-\frac12$$","How can I prove the following identity? $$\int_0^1\frac{x^2-2\,x+2\ln(1+x)}{x^3\,\sqrt{1-x^2}}\mathrm dx=\frac{\pi^2}8-\frac12$$",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
39,Why does area differentiate to perimeter for circles and not for squares?,Why does area differentiate to perimeter for circles and not for squares?,,"I read this question the other day and it got me thinking: the area of a circle is $\pi r^2$, which differentiates to $2 \pi r$, which is just the perimeter of the circle. Why doesn't the same thing happen for squares? If we start with the area formula for squares, $l^2$, this differentiates to $2l$ which is sort of right but only half the perimeter. I asked my calculus teacher and he couldn't tell me why. Can anyone explain???","I read this question the other day and it got me thinking: the area of a circle is $\pi r^2$, which differentiates to $2 \pi r$, which is just the perimeter of the circle. Why doesn't the same thing happen for squares? If we start with the area formula for squares, $l^2$, this differentiates to $2l$ which is sort of right but only half the perimeter. I asked my calculus teacher and he couldn't tell me why. Can anyone explain???",,"['calculus', 'geometry', 'circles', 'area']"
40,Correct way to calculate numeric derivative in discrete time?,Correct way to calculate numeric derivative in discrete time?,,"Given a set of discrete measurements in time $x_t, t \in \{0,\Delta t, 2\Delta t,\ldots,T-\Delta t,T\}$, what is the correct way to compute the discrete derivative $\dot x_t$. Is it more correct to take the difference with the previous value: $$\dot x_t = \frac{x_t-x_{t-1}}{\Delta t}$$ or with the next value in time: $$\dot x_t = \frac{x_{t+1}-x_t}{\Delta t}$$ It may be that both are correct in different contexts, but I'm not sure which is appropriate when. Likewise, how should the second derivative be computed: $$\ddot x_t = \frac{\dot x_{t+1}-\dot x_t}{\Delta t}$$ or $$\ddot x_t = \frac{\dot x_t-\dot x_{t-1}}{\Delta t}$$ For a fairly smooth discrete function and small $\Delta t$ both methods are roughly the same, but is one more correct than the other?","Given a set of discrete measurements in time $x_t, t \in \{0,\Delta t, 2\Delta t,\ldots,T-\Delta t,T\}$, what is the correct way to compute the discrete derivative $\dot x_t$. Is it more correct to take the difference with the previous value: $$\dot x_t = \frac{x_t-x_{t-1}}{\Delta t}$$ or with the next value in time: $$\dot x_t = \frac{x_{t+1}-x_t}{\Delta t}$$ It may be that both are correct in different contexts, but I'm not sure which is appropriate when. Likewise, how should the second derivative be computed: $$\ddot x_t = \frac{\dot x_{t+1}-\dot x_t}{\Delta t}$$ or $$\ddot x_t = \frac{\dot x_t-\dot x_{t-1}}{\Delta t}$$ For a fairly smooth discrete function and small $\Delta t$ both methods are roughly the same, but is one more correct than the other?",,"['calculus', 'discrete-mathematics', 'signal-processing', 'control-theory']"
41,"A closed form for $\int_0^\infty\frac{\ln(x+4)}{\sqrt{x\,(x+3)\,(x+4)}}dx$",A closed form for,"\int_0^\infty\frac{\ln(x+4)}{\sqrt{x\,(x+3)\,(x+4)}}dx","I need to a evaluate the following integral $$I=\int_0^\infty\frac{\ln(x+4)}{\sqrt{x\,(x+3)\,(x+4)}}dx.$$ Both Mathematica and Maple failed to evaluate it in a closed form, and lookups of the approximate numeric value $4.555919963334436...$ in ISC+ and WolframAlpha did not return plausible closed form candidates either. Does anybody by any chance have an idea if a closed form exists for this integral, and what it could be?","I need to a evaluate the following integral $$I=\int_0^\infty\frac{\ln(x+4)}{\sqrt{x\,(x+3)\,(x+4)}}dx.$$ Both Mathematica and Maple failed to evaluate it in a closed form, and lookups of the approximate numeric value $4.555919963334436...$ in ISC+ and WolframAlpha did not return plausible closed form candidates either. Does anybody by any chance have an idea if a closed form exists for this integral, and what it could be?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
42,Taylor's Theorem with Peano's Form of Remainder,Taylor's Theorem with Peano's Form of Remainder,,"The following form of Taylor's Theorem with minimal hypotheses is not widely popular and goes by the name of Taylor's Theorem with Peano's Form of Remainder : Taylor's Theorem with Peano's Form of Remainder : If $f$ is a function such that its $n^{\text{th}}$ derivative at $a$ (i.e. $f^{(n)}(a)$) exists then $$f(a + h) = f(a) + hf'(a) + \frac{h^{2}}{2!}f''(a) + \cdots + \frac{h^{n}}{n!}f^{(n)}(a) + o(h^{n})$$ where $o(h^{n})$ represents a function $g(h)$ with $g(h)/h^{n} \to 0$ as $h \to 0$. One of the proofs (search ""Proof of Taylor's Theorem"" in this blog post ) of this theorem uses repeated application of L'Hospital's Rule. And it appears that proofs of the above theorem  apart from the one via L'Hospital's Rule are not well known . I have asked this question to get other proofs of this theorem which do not rely on L'Hospital's Rule and instead use simpler ideas. BTW I am also posting one proof of my own as a community wiki.","The following form of Taylor's Theorem with minimal hypotheses is not widely popular and goes by the name of Taylor's Theorem with Peano's Form of Remainder : Taylor's Theorem with Peano's Form of Remainder : If $f$ is a function such that its $n^{\text{th}}$ derivative at $a$ (i.e. $f^{(n)}(a)$) exists then $$f(a + h) = f(a) + hf'(a) + \frac{h^{2}}{2!}f''(a) + \cdots + \frac{h^{n}}{n!}f^{(n)}(a) + o(h^{n})$$ where $o(h^{n})$ represents a function $g(h)$ with $g(h)/h^{n} \to 0$ as $h \to 0$. One of the proofs (search ""Proof of Taylor's Theorem"" in this blog post ) of this theorem uses repeated application of L'Hospital's Rule. And it appears that proofs of the above theorem  apart from the one via L'Hospital's Rule are not well known . I have asked this question to get other proofs of this theorem which do not rely on L'Hospital's Rule and instead use simpler ideas. BTW I am also posting one proof of my own as a community wiki.",,['calculus']
43,What is the derivative of: $f(x)=x^{2x^{3x^{4x^{5x^{6x^{7x^{.{^{.^{.}}}}}}}}}}$?,What is the derivative of: ?,f(x)=x^{2x^{3x^{4x^{5x^{6x^{7x^{.{^{.^{.}}}}}}}}}},"I happened to ponder about the differentiation of the following function: $$f(x)=x^{2x^{3x^{4x^{5x^{6x^{7x^{.{^{.^{.}}}}}}}}}}$$ Now, while I do know how to manipulate power towers to a certain extent, and know the general formula to differentiate $g(x)$ wrt $x$, where $$g(x)=f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{.{^{.^{.}}}}}}}}}}$$  I'm still unable to figure out as to how I can adequately manipulate the function to differentiate it within its domain of convergence. General formula: $$g'(x)=\frac{g^2(x)f'(x)}{f(x)\left[1-g(x)\ln(f(x))\right]}$$","I happened to ponder about the differentiation of the following function: $$f(x)=x^{2x^{3x^{4x^{5x^{6x^{7x^{.{^{.^{.}}}}}}}}}}$$ Now, while I do know how to manipulate power towers to a certain extent, and know the general formula to differentiate $g(x)$ wrt $x$, where $$g(x)=f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{f(x)^{.{^{.^{.}}}}}}}}}}$$  I'm still unable to figure out as to how I can adequately manipulate the function to differentiate it within its domain of convergence. General formula: $$g'(x)=\frac{g^2(x)f'(x)}{f(x)\left[1-g(x)\ln(f(x))\right]}$$",,"['calculus', 'functions', 'derivatives', 'power-towers']"
44,"The ""natural"" Sophomore's Dream integral: $\int_{0}^{\infty} x^{-x}\ dx$","The ""natural"" Sophomore's Dream integral:",\int_{0}^{\infty} x^{-x}\ dx,"I have been wondering about this for a while, but with no real luck in figuring it out. The famous ""Sophomore's dream"" identity refers to two similar integrals, one of which is $$\int_{0}^{1} x^{-x}\ dx = \sum_{n=1}^{\infty} n^{-n}$$ which has a pleasing symmetry between integrand and summand. However, I also notice that $x \mapsto x^{-x}$ is a rapidly-converging-to-zero function of $x$ , and hence it seems like it might be more ""natural"" to then wonder about the integral $$\int_{0}^{\infty} x^{-x}\ dx$$ . Numerical integration suggests it is approximately 1.99545596. Yet what I would like to find is some other representation that is not directly an integral, whether it be a series, or even a finite expression using any already-established mathematical functions and/or constants. And it doesn't seem very clear at all how to do this. Obviously, with upper limit $\infty$ , Taylor expansion of the integrand is of no use since it will only ever have finite radius of convergence. The other line of attack that I thus think of is to try and express $x^{-x}$ as a series of some form of decaying functions that are simpler to integrate. We do have that $$x^{-x} = e^{-x \ln x}$$ but this is of no use: it doesn't yield any series in terms of $e^{-x}$ -like terms. We have the interesting substitution $x = e^{W(u)}$ , $dx = \frac{1}{1 + W(u)}\ du$ (equiv. to $u = x \ln x$ ) with the Lambert W-function, which gives $$\int_{0}^{\infty} x^{-x}\ dx = \int_{0}^{\infty} e^{-u} \frac{du}{1 + W(u)}$$ but this doesn't help for series expansions. What is known about this integral?","I have been wondering about this for a while, but with no real luck in figuring it out. The famous ""Sophomore's dream"" identity refers to two similar integrals, one of which is which has a pleasing symmetry between integrand and summand. However, I also notice that is a rapidly-converging-to-zero function of , and hence it seems like it might be more ""natural"" to then wonder about the integral . Numerical integration suggests it is approximately 1.99545596. Yet what I would like to find is some other representation that is not directly an integral, whether it be a series, or even a finite expression using any already-established mathematical functions and/or constants. And it doesn't seem very clear at all how to do this. Obviously, with upper limit , Taylor expansion of the integrand is of no use since it will only ever have finite radius of convergence. The other line of attack that I thus think of is to try and express as a series of some form of decaying functions that are simpler to integrate. We do have that but this is of no use: it doesn't yield any series in terms of -like terms. We have the interesting substitution , (equiv. to ) with the Lambert W-function, which gives but this doesn't help for series expansions. What is known about this integral?",\int_{0}^{1} x^{-x}\ dx = \sum_{n=1}^{\infty} n^{-n} x \mapsto x^{-x} x \int_{0}^{\infty} x^{-x}\ dx \infty x^{-x} x^{-x} = e^{-x \ln x} e^{-x} x = e^{W(u)} dx = \frac{1}{1 + W(u)}\ du u = x \ln x \int_{0}^{\infty} x^{-x}\ dx = \int_{0}^{\infty} e^{-u} \frac{du}{1 + W(u)},"['calculus', 'integration', 'definite-integrals']"
45,"Crazy $\int_0^\infty{_3F_2}\left(\begin{array}c\tfrac58,\tfrac58,\tfrac98\\\tfrac12,\tfrac{13}8\end{array}\middle|\ {-x}\right)^2\frac{dx}{\sqrt x}$",Crazy,"\int_0^\infty{_3F_2}\left(\begin{array}c\tfrac58,\tfrac58,\tfrac98\\\tfrac12,\tfrac{13}8\end{array}\middle|\ {-x}\right)^2\frac{dx}{\sqrt x}","Is there any chance to find a closed form for this integral? $$I=\int_0^\infty{_3F_2}\left(\begin{array}c\tfrac58,\tfrac58,\tfrac98\\\tfrac12,\tfrac{13}8\end{array}\middle|\ {-x}\right)^2\frac{dx}{\sqrt x}$$","Is there any chance to find a closed form for this integral? $$I=\int_0^\infty{_3F_2}\left(\begin{array}c\tfrac58,\tfrac58,\tfrac98\\\tfrac12,\tfrac{13}8\end{array}\middle|\ {-x}\right)^2\frac{dx}{\sqrt x}$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'hypergeometric-function']"
46,Which is the easiest way to evaluate $\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x})$?,Which is the easiest way to evaluate ?,\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x}),"Which is the easiest way to evaluate $\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x})$? I have reduced this problem to $$   2\int_0^{\pi/2} \sqrt{\tan x} \ dx$$ but now, evaluating this integral is giving me some problems, simply substituting $u=\tan(x)$ and then $\mathrm{d}u=\sec^2(x)\mathrm{d}x \Rightarrow \frac{\mathrm{d}u}{1+u^2}=\mathrm{d}x$ and which in turn gives something a bit ugly, I was wondering which is the most elegant way to evaluate this?","Which is the easiest way to evaluate $\int \limits_{0}^{\pi/2} (\sqrt{\tan x} +\sqrt{\cot x})$? I have reduced this problem to $$   2\int_0^{\pi/2} \sqrt{\tan x} \ dx$$ but now, evaluating this integral is giving me some problems, simply substituting $u=\tan(x)$ and then $\mathrm{d}u=\sec^2(x)\mathrm{d}x \Rightarrow \frac{\mathrm{d}u}{1+u^2}=\mathrm{d}x$ and which in turn gives something a bit ugly, I was wondering which is the most elegant way to evaluate this?",,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
47,Integral $\int_0^1\frac{1-x^2+\left(1+x^2\right)\ln x}{\left(x+x^2\right)\ln^3x}dx$,Integral,\int_0^1\frac{1-x^2+\left(1+x^2\right)\ln x}{\left(x+x^2\right)\ln^3x}dx,"I'm struggling with this integral $$I=\int_0^1\frac{1-x^2+\left(1+x^2\right)\ln x}{\left(x+x^2\right)\ln^3x}dx.\tag1$$ Mathematica could not evaluate it in a closed form. Its numeric value is approximately $I\approx0.7804287418294087023386965471512328112...$ $^\text{[more]}$ , but I could not find a plausible closed form for this number using inverse symbolic calculators available online. Update : Based on Raymond Manzoni's comment below, there is actually a conjectural closed form, numerically matching up to at least $10^3$ decimal digits: $$I\stackrel?=6\ln A-\frac{\ln4}3-\frac14,\tag2$$ where $A$ is the Glaisher-Kinkelin constant : $$A=\exp\left(\frac1{12}-\zeta'(-1)\right).\tag3$$ Could you suggest a proof of the conjecture $(2)$?","I'm struggling with this integral $$I=\int_0^1\frac{1-x^2+\left(1+x^2\right)\ln x}{\left(x+x^2\right)\ln^3x}dx.\tag1$$ Mathematica could not evaluate it in a closed form. Its numeric value is approximately $I\approx0.7804287418294087023386965471512328112...$ $^\text{[more]}$ , but I could not find a plausible closed form for this number using inverse symbolic calculators available online. Update : Based on Raymond Manzoni's comment below, there is actually a conjectural closed form, numerically matching up to at least $10^3$ decimal digits: $$I\stackrel?=6\ln A-\frac{\ln4}3-\frac14,\tag2$$ where $A$ is the Glaisher-Kinkelin constant : $$A=\exp\left(\frac1{12}-\zeta'(-1)\right).\tag3$$ Could you suggest a proof of the conjecture $(2)$?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
48,A novelty integral for $\pi$,A novelty integral for,\pi,"My lab friends always play a mentally challenging brain game every month to keep our mind running on all four cylinders and the last month challenge was to find a novelty expression for $\pi$. In order to stick to the rule, of course we must avoid the good old Ramanujan and online available expressions, for instance: the coolest ways of expressing $\pi$ on Quora. The winner of the last month challenge is this integral $${\large\int_0^\infty}\frac{(1+x)\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)-2x\log(1+x)\log x}{x^{3/2}(1+x)\log^2x}\ dx={\Large\pi}$$ The equality is precise to at least thousand decimal places. Unfortunately, my friend who proposes this integral keeping the mystery to himself. I tried to crack this integral while waiting for a solution to be offered by one of my friends, but failed to get any. I have tried to break this integral into two part: $${\large\int_0^\infty}\frac{\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)}{x^{3/2}\log^2x}\ dx-2{\large\int_0^\infty}\frac{\log(1+x)}{\sqrt{x}(1+x)\log x}\ dx$$ but each integrals diverges. I have tried many substitutions like $x=y-1$, $x=\frac{1}{y}$, or $x=\tan^2y$ hoping for familiar functions, but couldn't get one. I also tried the method of differentiation under integral sign by introducing $$I(s)={\large\int_0^\infty}x^{s}\cdot\frac{(1+x)\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)-2x\log(1+x)\log x}{(1+x)\log^2x}\ dx$$ and differentiating twice with respect to $s$ to get rid of $\log^2x$ couldn't work either. I have a strong feeling that I miss something completely obvious in my calculation. I'm not having much success in evaluating this integral since two weeks ago, so I thought it's about time to ask you for help. Can you help me out to prove it, please?","My lab friends always play a mentally challenging brain game every month to keep our mind running on all four cylinders and the last month challenge was to find a novelty expression for $\pi$. In order to stick to the rule, of course we must avoid the good old Ramanujan and online available expressions, for instance: the coolest ways of expressing $\pi$ on Quora. The winner of the last month challenge is this integral $${\large\int_0^\infty}\frac{(1+x)\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)-2x\log(1+x)\log x}{x^{3/2}(1+x)\log^2x}\ dx={\Large\pi}$$ The equality is precise to at least thousand decimal places. Unfortunately, my friend who proposes this integral keeping the mystery to himself. I tried to crack this integral while waiting for a solution to be offered by one of my friends, but failed to get any. I have tried to break this integral into two part: $${\large\int_0^\infty}\frac{\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)}{x^{3/2}\log^2x}\ dx-2{\large\int_0^\infty}\frac{\log(1+x)}{\sqrt{x}(1+x)\log x}\ dx$$ but each integrals diverges. I have tried many substitutions like $x=y-1$, $x=\frac{1}{y}$, or $x=\tan^2y$ hoping for familiar functions, but couldn't get one. I also tried the method of differentiation under integral sign by introducing $$I(s)={\large\int_0^\infty}x^{s}\cdot\frac{(1+x)\log(1+x)(2+\log x)\log\left(\!\frac{1+x}{2}\!\right)-2x\log(1+x)\log x}{(1+x)\log^2x}\ dx$$ and differentiating twice with respect to $s$ to get rid of $\log^2x$ couldn't work either. I have a strong feeling that I miss something completely obvious in my calculation. I'm not having much success in evaluating this integral since two weeks ago, so I thought it's about time to ask you for help. Can you help me out to prove it, please?",,"['calculus', 'integration', 'proof-verification', 'improper-integrals', 'pi']"
49,Do functions all have an infinite number of limits?,Do functions all have an infinite number of limits?,,"I originally understood limits to be where functions run towards $\pm\infty$ as they approach some specific $x$ value and where they run towards (but never touch) some specific value (like $0$) as $x$ approaches infinity, thus making that value impossible to reach (a limit a function can't cross, in a Zeno's paradox -like way). Now that I'm beginning to actually study calculus, I'm seeing that limits are somehow more broad. Specifically, I now see limits are always referred to in relation to some stated $x$ value being approached (as indicated by the conventional notation: $\lim_{x\to p} f(x)$). But, this makes it seem to me like you can pick any value (any $p$) you want, that the limit is simply whatever value the function approaches as $x$ approaches whatever value you decided to pick. Wouldn't that mean functions have an infinite number of limits? (You can find an infinite number of points on a line/curve, after all.) If so, what's so limiting about ""limits"" then? Also, wouldn't this make limits the most stupidly obvious things? For example: $f(x)=x^2$ will obviously approach $4$ as you pick $x$ values arbitrarily closer and closer to $2$ ($1.9, 1.99. 1.999, 1.9999, 1.99999,$ etc )? If functions don't have an infinite number of limits, than how do you recognize which values for $x$ to approach make sense? Obviously, preconceived notions can screw with actually learning how a thing works because it can frame the information you're trying to integrate within a meaningless perspective, but figuring out how to shed those preconceived notions can be hard when you don't understand where you've gone wrong in the first place. ...oh, god, someone help me. I'm stuck in a loop.","I originally understood limits to be where functions run towards $\pm\infty$ as they approach some specific $x$ value and where they run towards (but never touch) some specific value (like $0$) as $x$ approaches infinity, thus making that value impossible to reach (a limit a function can't cross, in a Zeno's paradox -like way). Now that I'm beginning to actually study calculus, I'm seeing that limits are somehow more broad. Specifically, I now see limits are always referred to in relation to some stated $x$ value being approached (as indicated by the conventional notation: $\lim_{x\to p} f(x)$). But, this makes it seem to me like you can pick any value (any $p$) you want, that the limit is simply whatever value the function approaches as $x$ approaches whatever value you decided to pick. Wouldn't that mean functions have an infinite number of limits? (You can find an infinite number of points on a line/curve, after all.) If so, what's so limiting about ""limits"" then? Also, wouldn't this make limits the most stupidly obvious things? For example: $f(x)=x^2$ will obviously approach $4$ as you pick $x$ values arbitrarily closer and closer to $2$ ($1.9, 1.99. 1.999, 1.9999, 1.99999,$ etc )? If functions don't have an infinite number of limits, than how do you recognize which values for $x$ to approach make sense? Obviously, preconceived notions can screw with actually learning how a thing works because it can frame the information you're trying to integrate within a meaningless perspective, but figuring out how to shed those preconceived notions can be hard when you don't understand where you've gone wrong in the first place. ...oh, god, someone help me. I'm stuck in a loop.",,"['calculus', 'limits']"
50,Unifying the connections between the trigonometric and hyperbolic functions,Unifying the connections between the trigonometric and hyperbolic functions,,"There are many, many connections between the trigonometric and hyperbolic functions, some of which are listed here. It is probably too optimistic to expect that a single insight could explain all of these connections, but is there a holistic way of seeing the parallels between $\sin$ and $\sinh$ , $\cos$ and $\cosh$ ? Can all of these seemingly disparate connections be shown to be essentially the same, or at least very similar? Geometric connections Sine and cosine parameterise the unit circle $x^2+y^2=1$ , just as hyperbolic sine and cosine parameterise the 'unit hyperbola' $x^2-y^2=1$ . Both circles and hyperbolas are conic sections . The sector of the circle connecting the points $(0,0)$ , $(1,0)$ , and $(\cos t,\sin t)$ has an area of $t/2$ . The region of the hyperbola connecting the points $(0,0)$ , $(1,0)$ , and $(\cosh t,\sinh t)$ has an area of $t/2$ . This can even be used to define the hyperbolic functions geometrically , and many authors do the same with the trigonometric functions. Sine and hyperbolic sine are odd, whereas cosine and hyperbolic cosine are even. But sine and cosine are periodic functions, unlike the hyperbolic counterparts. The analogue of the identity $\cos^2x+\sin^2x \equiv 1$ is $\cosh^2x-\sinh^2x \equiv 1$ . The compound angle formulae are almost identical to their hyperbolic counterparts, save for a pesky minus sign: \begin{align} \sin(x+y) &= \sin(x)\cos(y)+\cos(x)\sin(y) \\ \sinh(x+y) &= \sinh(x)\cosh(y) + \cosh(x)\sinh(y) \\[4pt] \cos(x+y) &= \cos(x)\cos(y) \color{red}{-} \sin(x)\sin(y) \\ \cosh(x+y) &= \cosh(x)\cosh(y) \color{blue}{+} \sinh(x)\sinh(y) \, . \end{align} In general, given a trigonometric identity, it is (usually) possible to find a corresponding hyperbolic identity using Osborn's rule: replace every occurrence of $\cos$ with $\cosh$ ; replace every occurrence of $\sin$ by $\sinh$ , but negate the product of two $\sinh$ terms. Analytic connections $\sin$ is the unique solution to the initial value problem \begin{align} f''(x) &= \color{red}{-}f(x) \\ f'(0) &= 1 \\ f(0) &= 0 \, , \end{align} and the corresponding initial value problem for $\sinh$ is the same, except $f''(x) = \color{blue}{+}f(x)$ . Likewise, the initial value problem for $\cos$ is \begin{align} f''(x) &= \color{red}{-}f(x) \\ f'(0) &= 0 \\ f(0) &= 1 \, , \end{align} and again we see a mysterious sign change for $\cosh$ : $f''(x) = \color{blue}{+}f(x)$ . It follows that the higher-order derivatives of $\sin$ and $\sinh$ form periodic sequences. If we solve the initial value problems shown above, we obtain the exponential forms of all $4$ functions: \begin{align} \sin x &= \frac{e^{\color{green}{i}x}-e^{-\color{green}{i}x}}{2\color{green}{i}} \quad{} \cos x = \frac{e^{\color{\green}{i}x}+ e^{-\color{green}{i}x}}{2} \\[3pt] \sinh x &= \frac{e^{x}-e^{-x}}{2} \quad{} \cosh x = \frac{e^x + e^{-x}}{2} \, . \end{align} All $4$ functions are analytic, and their Taylor series bear a striking resemblance to each other: \begin{align} \sin x &= x \color{red}{-} \frac{x^3}{3!} + \frac{x^5}{5!} \color{red}{-} \frac{x^7}{7!} + \ldots \\[4pt] \sinh x &= x \color{blue}{+} \frac{x^3}{3!} + \frac{x^5}{5!} \color{blue}{+} \frac{x^7}{7!} + \ldots \\[4pt] \cos x &= 1 \color{red}{-} \frac{x^2}{2!} + \frac{x^4}{4!} \color{red}{-} \frac{x^6}{6!} + \ldots \\[4pt] \cosh x &= 1 \color{blue}{+} \frac{x^2}{2!} + \frac{x^4}{4!} \color{blue}{+} \frac{x^6}{6!} + \ldots \end{align} And Euler's formula $$ e^{ix} = \cos x + i \sin x $$ is replaced by the underwhelming $$ e^x = \cosh x + \sinh x \, . $$","There are many, many connections between the trigonometric and hyperbolic functions, some of which are listed here. It is probably too optimistic to expect that a single insight could explain all of these connections, but is there a holistic way of seeing the parallels between and , and ? Can all of these seemingly disparate connections be shown to be essentially the same, or at least very similar? Geometric connections Sine and cosine parameterise the unit circle , just as hyperbolic sine and cosine parameterise the 'unit hyperbola' . Both circles and hyperbolas are conic sections . The sector of the circle connecting the points , , and has an area of . The region of the hyperbola connecting the points , , and has an area of . This can even be used to define the hyperbolic functions geometrically , and many authors do the same with the trigonometric functions. Sine and hyperbolic sine are odd, whereas cosine and hyperbolic cosine are even. But sine and cosine are periodic functions, unlike the hyperbolic counterparts. The analogue of the identity is . The compound angle formulae are almost identical to their hyperbolic counterparts, save for a pesky minus sign: In general, given a trigonometric identity, it is (usually) possible to find a corresponding hyperbolic identity using Osborn's rule: replace every occurrence of with ; replace every occurrence of by , but negate the product of two terms. Analytic connections is the unique solution to the initial value problem and the corresponding initial value problem for is the same, except . Likewise, the initial value problem for is and again we see a mysterious sign change for : . It follows that the higher-order derivatives of and form periodic sequences. If we solve the initial value problems shown above, we obtain the exponential forms of all functions: All functions are analytic, and their Taylor series bear a striking resemblance to each other: And Euler's formula is replaced by the underwhelming","\sin \sinh \cos \cosh x^2+y^2=1 x^2-y^2=1 (0,0) (1,0) (\cos t,\sin t) t/2 (0,0) (1,0) (\cosh t,\sinh t) t/2 \cos^2x+\sin^2x \equiv 1 \cosh^2x-\sinh^2x \equiv 1 \begin{align}
\sin(x+y) &= \sin(x)\cos(y)+\cos(x)\sin(y) \\
\sinh(x+y) &= \sinh(x)\cosh(y) + \cosh(x)\sinh(y) \\[4pt]
\cos(x+y) &= \cos(x)\cos(y) \color{red}{-} \sin(x)\sin(y) \\
\cosh(x+y) &= \cosh(x)\cosh(y) \color{blue}{+} \sinh(x)\sinh(y) \, .
\end{align} \cos \cosh \sin \sinh \sinh \sin \begin{align}
f''(x) &= \color{red}{-}f(x) \\
f'(0) &= 1 \\
f(0) &= 0 \, ,
\end{align} \sinh f''(x) = \color{blue}{+}f(x) \cos \begin{align}
f''(x) &= \color{red}{-}f(x) \\
f'(0) &= 0 \\
f(0) &= 1 \, ,
\end{align} \cosh f''(x) = \color{blue}{+}f(x) \sin \sinh 4 \begin{align}
\sin x &= \frac{e^{\color{green}{i}x}-e^{-\color{green}{i}x}}{2\color{green}{i}} \quad{} \cos x = \frac{e^{\color{\green}{i}x}+ e^{-\color{green}{i}x}}{2} \\[3pt]
\sinh x &= \frac{e^{x}-e^{-x}}{2} \quad{} \cosh x = \frac{e^x + e^{-x}}{2} \, .
\end{align} 4 \begin{align}
\sin x &= x \color{red}{-} \frac{x^3}{3!} + \frac{x^5}{5!} \color{red}{-} \frac{x^7}{7!} + \ldots \\[4pt]
\sinh x &= x \color{blue}{+} \frac{x^3}{3!} + \frac{x^5}{5!} \color{blue}{+} \frac{x^7}{7!} + \ldots \\[4pt]
\cos x &= 1 \color{red}{-} \frac{x^2}{2!} + \frac{x^4}{4!} \color{red}{-} \frac{x^6}{6!} + \ldots \\[4pt]
\cosh x &= 1 \color{blue}{+} \frac{x^2}{2!} + \frac{x^4}{4!} \color{blue}{+} \frac{x^6}{6!} + \ldots
\end{align} 
e^{ix} = \cos x + i \sin x
 
e^x = \cosh x + \sinh x \, .
","['calculus', 'trigonometry', 'exponential-function', 'hyperbolic-functions']"
51,"Integral $\int_0^\infty\frac{\ln\left(1+x+\sqrt{x^2+2\,x}\right)\,\ln\left(1+\sqrt{x^2+2\,x+2}\right)}{x^2+2x+1}dx$",Integral,"\int_0^\infty\frac{\ln\left(1+x+\sqrt{x^2+2\,x}\right)\,\ln\left(1+\sqrt{x^2+2\,x+2}\right)}{x^2+2x+1}dx","Could you suggest any ideas how to evaluate this integral? Is there a closed-form result? $$\int_0^\infty\frac{\ln\left(1+x+\sqrt{x^2+2\,x}\right)\,\ln\left(1+\sqrt{x^2+2\,x+2}\right)}{x^2+2x+1}dx$$","Could you suggest any ideas how to evaluate this integral? Is there a closed-form result? $$\int_0^\infty\frac{\ln\left(1+x+\sqrt{x^2+2\,x}\right)\,\ln\left(1+\sqrt{x^2+2\,x+2}\right)}{x^2+2x+1}dx$$",,"['calculus', 'integration', 'logarithms', 'improper-integrals', 'closed-form']"
52,Power series representation of arctangent: fails to converge everywhere,Power series representation of arctangent: fails to converge everywhere,,"My understanding of power series turns out to be less-well-formed than I thought.  To confess, I took my two courses in analysis in grad school (one real, one complex) and got out. Since this is my Calc II class, let's keep everything in real variables, please.  It's not hard to derive the power series for $\arctan(x)$ as $$ \arctan(x) = \sum_{n=0}^\infty \frac{(-1)^n}{2n+1} x^{2n+1}, \ -1 \leq x \leq 1. $$ Also not hard to work out the interval of convergence for the right-hand side.  So far, so good. Here's my question and why I suddenly see how naive I am.  I tend to think of $\arctan$ as an incredibly nice function, so I expect its power/Taylor series to converge everywhere.  In short, I view $\arctan$ as being just as nice as $f(x) = e^x$, whose power series representation converges everywhere (domain of the power series matches the domain of the function).  Same story for $\sin(x)$ and $\cos(x)$.  They're ""nice"" so their power series converge on their entire domain. When the power series for something like $\ln (x)$ or $\frac{1}{x}$ has finite radius, I'm completely fine with that as there is an obvious discontinuity that you bump into as you work your way out from the center.  But why does the power series for $\arctan(x)$ have a finite radius?  I know that something goes wrong with Taylor's remainder and this is what prevents the series from representing $\arctan(x)$ everywhere, but I would appreciate an explanation from the point of view of properties of $\arctan(x)$ and not its power series:  what is it about $\arctan(x)$ that prevents its power series from being optimally ""nice""?","My understanding of power series turns out to be less-well-formed than I thought.  To confess, I took my two courses in analysis in grad school (one real, one complex) and got out. Since this is my Calc II class, let's keep everything in real variables, please.  It's not hard to derive the power series for $\arctan(x)$ as $$ \arctan(x) = \sum_{n=0}^\infty \frac{(-1)^n}{2n+1} x^{2n+1}, \ -1 \leq x \leq 1. $$ Also not hard to work out the interval of convergence for the right-hand side.  So far, so good. Here's my question and why I suddenly see how naive I am.  I tend to think of $\arctan$ as an incredibly nice function, so I expect its power/Taylor series to converge everywhere.  In short, I view $\arctan$ as being just as nice as $f(x) = e^x$, whose power series representation converges everywhere (domain of the power series matches the domain of the function).  Same story for $\sin(x)$ and $\cos(x)$.  They're ""nice"" so their power series converge on their entire domain. When the power series for something like $\ln (x)$ or $\frac{1}{x}$ has finite radius, I'm completely fine with that as there is an obvious discontinuity that you bump into as you work your way out from the center.  But why does the power series for $\arctan(x)$ have a finite radius?  I know that something goes wrong with Taylor's remainder and this is what prevents the series from representing $\arctan(x)$ everywhere, but I would appreciate an explanation from the point of view of properties of $\arctan(x)$ and not its power series:  what is it about $\arctan(x)$ that prevents its power series from being optimally ""nice""?",,"['calculus', 'power-series']"
53,How find this sum $\sum\limits_{n=0}^{\infty}\frac{1}{(3n+1)(3n+2)(3n+3)}$,How find this sum,\sum\limits_{n=0}^{\infty}\frac{1}{(3n+1)(3n+2)(3n+3)},"Find this sum $$I=\sum_{n=0}^{\infty}\dfrac{1}{(3n+1)(3n+2)(3n+3)}$$ My try: let $$f(x)=\sum_{n=0}^{\infty}\dfrac{x^{3n+3}}{(3n+1)(3n+2)(3n+3)},|x|\le 1$$ then we have $$f^{(3)}(x)=\sum_{n=0}^{\infty}x^{3n}=\dfrac{1}{1-x^3}$$ then we find the $f(x)$,Following is very ugly(can you someone can post your follow solution,) have other simple methods? Thank you very much.","Find this sum $$I=\sum_{n=0}^{\infty}\dfrac{1}{(3n+1)(3n+2)(3n+3)}$$ My try: let $$f(x)=\sum_{n=0}^{\infty}\dfrac{x^{3n+3}}{(3n+1)(3n+2)(3n+3)},|x|\le 1$$ then we have $$f^{(3)}(x)=\sum_{n=0}^{\infty}x^{3n}=\dfrac{1}{1-x^3}$$ then we find the $f(x)$,Following is very ugly(can you someone can post your follow solution,) have other simple methods? Thank you very much.",,"['calculus', 'sequences-and-series', 'summation']"
54,What's umbral calculus about?,What's umbral calculus about?,,"I've read Wikipedia about it and it says: In mathematics before the 1970s, the term umbral calculus referred to   the surprising similarity between seemingly unrelated polynomial   equations and certain shadowy techniques used to 'prove' them. What are these techniques? These similarities allow one to construct umbral proofs, which, on the   surface cannot be correct, but seem to work anyway. What does ""seem to work"" mean here? It seems that umbral calculus is a mathematical idea with almost no uses, why? (At least it's not so famous as calculus and algebra, for example.)","I've read Wikipedia about it and it says: In mathematics before the 1970s, the term umbral calculus referred to   the surprising similarity between seemingly unrelated polynomial   equations and certain shadowy techniques used to 'prove' them. What are these techniques? These similarities allow one to construct umbral proofs, which, on the   surface cannot be correct, but seem to work anyway. What does ""seem to work"" mean here? It seems that umbral calculus is a mathematical idea with almost no uses, why? (At least it's not so famous as calculus and algebra, for example.)",,"['calculus', 'umbral-calculus']"
55,"Integral $\int_0^\infty\frac{\ln\left(\sqrt{x+1\vphantom{x^0}}-1\right)\,\ln\left(\sqrt{x^{-1}+1}+1\right)}{(x+1)^{3/2}}dx$",Integral,"\int_0^\infty\frac{\ln\left(\sqrt{x+1\vphantom{x^0}}-1\right)\,\ln\left(\sqrt{x^{-1}+1}+1\right)}{(x+1)^{3/2}}dx","Another integral similar to my previous question : $$\int_0^\infty\frac{\ln\left(\sqrt{x+1\vphantom{x^0}}-1\right)\,\ln\left(\sqrt{x^{-1}+1}+1\right)}{(x+1)^{3/2}}dx$$ Can someone suggest how to evaluate it? Is there a closed form?",Another integral similar to my previous question : Can someone suggest how to evaluate it? Is there a closed form?,"\int_0^\infty\frac{\ln\left(\sqrt{x+1\vphantom{x^0}}-1\right)\,\ln\left(\sqrt{x^{-1}+1}+1\right)}{(x+1)^{3/2}}dx","['calculus', 'integration', 'logarithms', 'improper-integrals', 'closed-form']"
56,Closed form for $\sum_{n=1}^\infty\frac{1}{2^n\left(1+\sqrt[2^n]{2}\right)}$,Closed form for,\sum_{n=1}^\infty\frac{1}{2^n\left(1+\sqrt[2^n]{2}\right)},Here is another infinite sum I need you help with: $$\sum_{n=1}^\infty\frac{1}{2^n\left(1+\sqrt[2^n]{2}\right)}.$$ I was told it could be represented in terms of elementary functions and integers.,Here is another infinite sum I need you help with: $$\sum_{n=1}^\infty\frac{1}{2^n\left(1+\sqrt[2^n]{2}\right)}.$$ I was told it could be represented in terms of elementary functions and integers.,,"['calculus', 'sequences-and-series', 'closed-form']"
57,slope of a line in 3D coordinate system,slope of a line in 3D coordinate system,,"Suppose I have $2$ points in a 3D coordinate space. Say $p_1=(5,5,5)$, $p_2=(1,2,3)$.  How do I find the slope of the line joining $p_1$ and $p_2$?  After getting the slope (which I assume will be an integer) how do I get the coordinates of any other arbitrary point on this line? I do understand my maths skills are not what they should be :) but i would appreciate any help, or a reference to some document/book where I can learn the basics of 3D coordinate geometry.","Suppose I have $2$ points in a 3D coordinate space. Say $p_1=(5,5,5)$, $p_2=(1,2,3)$.  How do I find the slope of the line joining $p_1$ and $p_2$?  After getting the slope (which I assume will be an integer) how do I get the coordinates of any other arbitrary point on this line? I do understand my maths skills are not what they should be :) but i would appreciate any help, or a reference to some document/book where I can learn the basics of 3D coordinate geometry.",,"['calculus', 'linear-algebra', '3d']"
58,relation between integral and summation,relation between integral and summation,,What is the relation between a summation and an integral ? This question is actually based on a previous question of mine here where I got two answers (one is based on summation notation) and the other is based on integral notation and I do not know yet which one to accept . So I would like to understand the connect between the two ?,What is the relation between a summation and an integral ? This question is actually based on a previous question of mine here where I got two answers (one is based on summation notation) and the other is based on integral notation and I do not know yet which one to accept . So I would like to understand the connect between the two ?,,"['calculus', 'sequences-and-series', 'integration']"
59,Integrate $\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx$,Integrate,\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx,"Please help me to solve this integral: $$\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx.$$ I managed to calculate an indefinite integral of the left part: $$\int\frac{\cos x}{\sin x}x\ \mathrm dx=\ x\log(2\sin x)+\frac{1}{2} \Im\ \text{Li}_2(e^{2\ x\ i}),$$ where $\Im\ \text{Li}_2(z)$ denotes the imaginary part of the dilogarithm . The corresponding definite integral $$\int_0^\pi\frac{\cos x}{\sin x}x\ \mathrm dx$$ diverges. So, it looks like in the original integral summands compensate each other's singularities to avoid divergence. I tried a numerical integration and it looks plausible that $$\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx\stackrel{?}{=}\pi \log 54,$$ but I have no idea how to prove it.","Please help me to solve this integral: $$\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx.$$ I managed to calculate an indefinite integral of the left part: $$\int\frac{\cos x}{\sin x}x\ \mathrm dx=\ x\log(2\sin x)+\frac{1}{2} \Im\ \text{Li}_2(e^{2\ x\ i}),$$ where $\Im\ \text{Li}_2(z)$ denotes the imaginary part of the dilogarithm . The corresponding definite integral $$\int_0^\pi\frac{\cos x}{\sin x}x\ \mathrm dx$$ diverges. So, it looks like in the original integral summands compensate each other's singularities to avoid divergence. I tried a numerical integration and it looks plausible that $$\int_0^\pi\frac{3\cos x+\sqrt{8+\cos^2 x}}{\sin x}x\ \mathrm dx\stackrel{?}{=}\pi \log 54,$$ but I have no idea how to prove it.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'trigonometric-integrals']"
60,Integral $\int_0^1\frac{\ln x}{\left(1+x\right)\left(1+x^{-\left(2+\sqrt3\right)}\right)}dx$,Integral,\int_0^1\frac{\ln x}{\left(1+x\right)\left(1+x^{-\left(2+\sqrt3\right)}\right)}dx,"There is a curious known integral: $$\int_0^1\frac{\ln\left(1+x^{2+\sqrt{3\vphantom{\large3}}}\right)}{1+x}dx=\frac{\pi^2}{12}\left(1-\sqrt{3\vphantom{\large3}}\right)+\ln \left(1+\sqrt{3\vphantom{\large3}}\right)\ln2.$$ If we consider $\alpha=2+\sqrt{3\vphantom{\large3}}$ as a parameter and take a derivative w.r.t. $\alpha$ at this point, we get the following: $$I=\int_0^1\frac{\ln x}{\left(1+x\right)\left(1+x^{-\left(2+\sqrt{3\vphantom{\large3}}\right)}\right)}dx.$$ Is it possible to express the integral $I$ in a closed form?","There is a curious known integral: $$\int_0^1\frac{\ln\left(1+x^{2+\sqrt{3\vphantom{\large3}}}\right)}{1+x}dx=\frac{\pi^2}{12}\left(1-\sqrt{3\vphantom{\large3}}\right)+\ln \left(1+\sqrt{3\vphantom{\large3}}\right)\ln2.$$ If we consider $\alpha=2+\sqrt{3\vphantom{\large3}}$ as a parameter and take a derivative w.r.t. $\alpha$ at this point, we get the following: $$I=\int_0^1\frac{\ln x}{\left(1+x\right)\left(1+x^{-\left(2+\sqrt{3\vphantom{\large3}}\right)}\right)}dx.$$ Is it possible to express the integral $I$ in a closed form?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
61,"Is there any meaning to an ""infinite derivative""?","Is there any meaning to an ""infinite derivative""?",,"I've been thinking about this: say you have an infinitely differentiable function. Then you can form a sequence $f(x), f'(x), f''(x), \cdots, f^{(n)}(x), \cdots$ and attempt to take its limit. For some functions this is easy: for $e^x$ the limit will be itself, $\sin x$ and $\cos x$ won't have a limit, every polynomial will eventually be $0$, etc. I was wondering, does this have any use? Is there any interesting interpretation of the infinite derivative, and does it existing or not tell us anything about the function?","I've been thinking about this: say you have an infinitely differentiable function. Then you can form a sequence $f(x), f'(x), f''(x), \cdots, f^{(n)}(x), \cdots$ and attempt to take its limit. For some functions this is easy: for $e^x$ the limit will be itself, $\sin x$ and $\cos x$ won't have a limit, every polynomial will eventually be $0$, etc. I was wondering, does this have any use? Is there any interesting interpretation of the infinite derivative, and does it existing or not tell us anything about the function?",,"['calculus', 'derivatives']"
62,Convergence of a series with repeated sines,Convergence of a series with repeated sines,,"Show that the series  $$ \sum_{n=1}^\infty \frac{\sin\big(\sin(n)\big)}{n}, $$ converges. More generally, show that for every $k\in\mathbb N$ the series $$ \sum_{n=1}^\infty \frac{\sigma_k(n)}{n}, $$ converges, where $\sigma_1(x)=\sin(x)$ and $\sigma_{k+1}(x)=\sin\big(\sigma_k(x)\big)$. Note. I am looking for an elementary proof, if such is available. I do know that such elementary proof exists, for $k=2$, since that case was  qualifying exam (University of Adelaide) a few years ago. On the other hand, I know of a non elementary proof (for $k$ general), using the nontrivial fact that $\dfrac{1}{2\pi}$ has finite irrationality measure . I imagine that any proof would have as a first step establishing the fact that the sequence $$ \sum_{j=1}^n \sigma_k(j), \quad n\in\mathbb N,  $$ is bounded. Update. Actually, using advanced tools (i.e., irrationality measure),  it turns out that even $$ \sum_{n=1}^\infty \frac{\sigma_k(n)}{n^a}, $$ converges, for every $a>0$, and $k\in\mathbb N$, while the same tools can not determine whether $$ \sum_{n=1}^\infty \frac{\sin\big(\sin(\beta n)\big)}{n^a}, $$ converges, for every $\beta$.","Show that the series  $$ \sum_{n=1}^\infty \frac{\sin\big(\sin(n)\big)}{n}, $$ converges. More generally, show that for every $k\in\mathbb N$ the series $$ \sum_{n=1}^\infty \frac{\sigma_k(n)}{n}, $$ converges, where $\sigma_1(x)=\sin(x)$ and $\sigma_{k+1}(x)=\sin\big(\sigma_k(x)\big)$. Note. I am looking for an elementary proof, if such is available. I do know that such elementary proof exists, for $k=2$, since that case was  qualifying exam (University of Adelaide) a few years ago. On the other hand, I know of a non elementary proof (for $k$ general), using the nontrivial fact that $\dfrac{1}{2\pi}$ has finite irrationality measure . I imagine that any proof would have as a first step establishing the fact that the sequence $$ \sum_{j=1}^n \sigma_k(j), \quad n\in\mathbb N,  $$ is bounded. Update. Actually, using advanced tools (i.e., irrationality measure),  it turns out that even $$ \sum_{n=1}^\infty \frac{\sigma_k(n)}{n^a}, $$ converges, for every $a>0$, and $k\in\mathbb N$, while the same tools can not determine whether $$ \sum_{n=1}^\infty \frac{\sin\big(\sin(\beta n)\big)}{n^a}, $$ converges, for every $\beta$.",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
63,Can the lion protect the sheep from three wolves?,Can the lion protect the sheep from three wolves?,,"Generally, in pursuit-evasion games, there's one prey and one or many pursuers. I'd like to know how extending the food chain would change the dynamics of such games. Specifically, let's consider a closed, circular shape arena in $\mathbb{R}^2$ . Three wolves are uniformly distributed at the border. The sheep and his lion friend are at the center. If $d(w(t),s(t))=0$ , the wolf eats the sheep, if $d(l(t),w(t))=0$ , the lion eats the wolf, where w(t), s(t) and l(t) are the trajectories of the animals, and $d$ measures euclidean distance. The pack of wolves work as a group, their goal being to eat the sheep. The goal of the lion-sheep team is to prevent the sheep from being eaten, indefinitely. All animals move continuously in time at equal speed and are intelligent. Can the lion protect the sheep from the wolves? In general, how many lions are necessary to protect the sheep from a pack of $N$ wolves uniformly distributed at the border? This is a puzzle I originally asked here on puzzling.stackexchange. I know already that A single wolf doesn't catch the sheep. Two wolves will catch the sheep. $N-1$ lions are sufficient to fend off $N$ wolves. See here for the proof of those claims. I'm interested in knowing whether $N-2$ or less lions can fend off $N$ wolves.","Generally, in pursuit-evasion games, there's one prey and one or many pursuers. I'd like to know how extending the food chain would change the dynamics of such games. Specifically, let's consider a closed, circular shape arena in . Three wolves are uniformly distributed at the border. The sheep and his lion friend are at the center. If , the wolf eats the sheep, if , the lion eats the wolf, where w(t), s(t) and l(t) are the trajectories of the animals, and measures euclidean distance. The pack of wolves work as a group, their goal being to eat the sheep. The goal of the lion-sheep team is to prevent the sheep from being eaten, indefinitely. All animals move continuously in time at equal speed and are intelligent. Can the lion protect the sheep from the wolves? In general, how many lions are necessary to protect the sheep from a pack of wolves uniformly distributed at the border? This is a puzzle I originally asked here on puzzling.stackexchange. I know already that A single wolf doesn't catch the sheep. Two wolves will catch the sheep. lions are sufficient to fend off wolves. See here for the proof of those claims. I'm interested in knowing whether or less lions can fend off wolves.","\mathbb{R}^2 d(w(t),s(t))=0 d(l(t),w(t))=0 d N N-1 N N-2 N","['calculus', 'geometry', 'optimization', 'dynamical-systems', 'game-theory']"
64,Proving $\mathrm e <3$,Proving,\mathrm e <3,"Well I am just asking myself if there's a more elegant way of proving $$2<\exp(1)=\mathrm e<3$$ than doing it by induction and using the fact of $\lim\limits_{n\rightarrow\infty}\left(1+\frac1n\right)^n=\mathrm e$ , is there one (or some) alternative way(s)?","Well I am just asking myself if there's a more elegant way of proving than doing it by induction and using the fact of , is there one (or some) alternative way(s)?",2<\exp(1)=\mathrm e<3 \lim\limits_{n\rightarrow\infty}\left(1+\frac1n\right)^n=\mathrm e,"['calculus', 'inequality', 'exponential-function']"
65,Curl of Cross Product of Two Vectors,Curl of Cross Product of Two Vectors,,"I want to prove the following identity $$\text{curl } \left(\textbf{F}\times \textbf{G}\right) = \textbf{F}\text{ div}\textbf{ G}- \textbf{G}\text{ div}\textbf{ F}+ \left(\textbf{G}\cdot \nabla \right)\textbf{F}- \left(\textbf{F}\cdot \nabla \right)\textbf{G}$$ But I do not know how! Also, what does $\textbf{F}\cdot \nabla $ mean, isn't it the divergence of $\textbf{F}$!","I want to prove the following identity $$\text{curl } \left(\textbf{F}\times \textbf{G}\right) = \textbf{F}\text{ div}\textbf{ G}- \textbf{G}\text{ div}\textbf{ F}+ \left(\textbf{G}\cdot \nabla \right)\textbf{F}- \left(\textbf{F}\cdot \nabla \right)\textbf{G}$$ But I do not know how! Also, what does $\textbf{F}\cdot \nabla $ mean, isn't it the divergence of $\textbf{F}$!",,"['calculus', 'multivariable-calculus', 'vector-spaces']"
66,First and second derivative of a summation,First and second derivative of a summation,,"Consider the function $f(\mu) = \sum_{i = 1}^{n} (x_i - \mu)^2$,  where $x_i = i,\,i=1, 2,\dots, n$. What is the first and second derivative of $f(\mu)$?","Consider the function $f(\mu) = \sum_{i = 1}^{n} (x_i - \mu)^2$,  where $x_i = i,\,i=1, 2,\dots, n$. What is the first and second derivative of $f(\mu)$?",,"['calculus', 'summation']"
67,Need help with $\int_0^\pi\arctan^2\left(\frac{\sin x}{2+\cos x}\right)dx$,Need help with,\int_0^\pi\arctan^2\left(\frac{\sin x}{2+\cos x}\right)dx,"Please help me to evaluate this integral: $$\int_0^\pi\arctan^2\left(\frac{\sin x}{2+\cos x}\right)dx$$ Using substitution $x=2\arctan t$ it can be transformed to: $$\int_0^\infty\frac{2}{1+t^2}\arctan^2\left(\frac{2t}{3+t^2}\right)dt$$ Then I tried integration by parts, but without any success...","Please help me to evaluate this integral: $$\int_0^\pi\arctan^2\left(\frac{\sin x}{2+\cos x}\right)dx$$ Using substitution $x=2\arctan t$ it can be transformed to: $$\int_0^\infty\frac{2}{1+t^2}\arctan^2\left(\frac{2t}{3+t^2}\right)dt$$ Then I tried integration by parts, but without any success...",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
68,Integral of Combination Log and Inverse Trig Function,Integral of Combination Log and Inverse Trig Function,,"Does the following integral have a closed-form ?: \begin{equation} \int_{0}^{1}{\ln\left(\,x\,\right) \over 1 + x}\,\arccos\left(\,x\,\right) \,{\rm d}x \end{equation} This integral has been posted in Integral and Series a week ago but it remains unsolved. So, I decide to post it here. Could anyone here please help me to find the closed-form preferably with elementary ways ( high school methods ) ?. Any help would be greatly appreciated. Thank you.","Does the following integral have a closed-form ?: \begin{equation} \int_{0}^{1}{\ln\left(\,x\,\right) \over 1 + x}\,\arccos\left(\,x\,\right) \,{\rm d}x \end{equation} This integral has been posted in Integral and Series a week ago but it remains unsolved. So, I decide to post it here. Could anyone here please help me to find the closed-form preferably with elementary ways ( high school methods ) ?. Any help would be greatly appreciated. Thank you.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
69,Prove $\int_0^1 \frac{t^2-1}{(t^2+1)\log t}dt = 2\log\left( \frac{2\Gamma \left( \frac{5}{4}\right)}{\Gamma\left( \frac{3}{4}\right)}\right)$,Prove,\int_0^1 \frac{t^2-1}{(t^2+1)\log t}dt = 2\log\left( \frac{2\Gamma \left( \frac{5}{4}\right)}{\Gamma\left( \frac{3}{4}\right)}\right),I am trying to prove that $$\int_0^1 \frac{t^2-1}{(t^2+1)\log t}dt = 2\log\left( \frac{2\Gamma \left( \frac{5}{4}\right)}{\Gamma\left( \frac{3}{4}\right)}\right)$$ I know how to deal with integrals involving cyclotomic polynomials and nested logarithms but I have no idea with this one.,I am trying to prove that $$\int_0^1 \frac{t^2-1}{(t^2+1)\log t}dt = 2\log\left( \frac{2\Gamma \left( \frac{5}{4}\right)}{\Gamma\left( \frac{3}{4}\right)}\right)$$ I know how to deal with integrals involving cyclotomic polynomials and nested logarithms but I have no idea with this one.,,"['calculus', 'integration', 'definite-integrals']"
70,Reversing the Order of Integration and Summation,Reversing the Order of Integration and Summation,,"I am trying to understand when we can interchange the order of Integration and Summation. I am increasingly encountering Integrals; some of which are being solved by interchanging the order of Summation and  Integration, and some which cannot (for no given reason) be solved using this. Despite looking at a variety of sites, I was unable to understand when we can do so. $$$$ I came up with the following two requirements here on MSE: $$$$ If $f_n(x)\ge 0$ for all $x,n$ $$\sum \int f_n(x) \, dx = \int \sum f_n(x) \,dx$$ Also if $\sum \int |f_n| < \infty$ or $\int \sum |f_n| < \infty$ , then $$\int \sum f_n = \sum \int f_n$$ I would be grateful if somebody could  please explain this to me. Thanks very much in advance.","I am trying to understand when we can interchange the order of Integration and Summation. I am increasingly encountering Integrals; some of which are being solved by interchanging the order of Summation and  Integration, and some which cannot (for no given reason) be solved using this. Despite looking at a variety of sites, I was unable to understand when we can do so. I came up with the following two requirements here on MSE: If for all Also if or , then I would be grateful if somebody could  please explain this to me. Thanks very much in advance.","  f_n(x)\ge 0 x,n \sum \int f_n(x) \, dx = \int \sum f_n(x) \,dx \sum \int |f_n| < \infty \int \sum |f_n| < \infty \int \sum f_n = \sum \int f_n","['calculus', 'integration', 'summation']"
71,tough integral involving $\sin(x^2)$ and $\sinh^2 (x)$,tough integral involving  and,\sin(x^2) \sinh^2 (x),"I ran across this integral I get no where with. Can someone suggest a method of attack?. $$\int_0^{\infty}\frac{\sin(\pi x^2)}{\sinh^2 (\pi x)}\mathrm dx=\frac{2-\sqrt{2}}{4}$$ I tried series, imaginary parts, and so forth, but have made no progress. Thanks very much.","I ran across this integral I get no where with. Can someone suggest a method of attack?. $$\int_0^{\infty}\frac{\sin(\pi x^2)}{\sinh^2 (\pi x)}\mathrm dx=\frac{2-\sqrt{2}}{4}$$ I tried series, imaginary parts, and so forth, but have made no progress. Thanks very much.",,"['calculus', 'integration', 'improper-integrals']"
72,How do we know the Riemann Sum gives us the area under the curve? [duplicate],How do we know the Riemann Sum gives us the area under the curve? [duplicate],,"This question already has answers here : Why is the area under a curve the integral? (6 answers) Closed 4 years ago . I'm currently learning calculus (high school senior), and I am not comfortable with the idea that the limit of the sums of rectangles actually converges to the area under the curve. I know it looks like it does, but how do we know for sure?  Couldn't the tiny errors beneath/over the curve accumulate as we add more and more rectangles? What's troubling me is the whole Pi = 4 thing with the staircase approximating a circle pointwise, and how it's wrong and the perimeter of the staircase shape does not approach the circumference of the circle, even though pointwise it does approach a circle. So how are the increasingly many, increasingly small errors in Riemann sums any different? How do we know the error in each step decreases faster than the number of errors increases? I would really like to see a proof of this. Thanks so much!","This question already has answers here : Why is the area under a curve the integral? (6 answers) Closed 4 years ago . I'm currently learning calculus (high school senior), and I am not comfortable with the idea that the limit of the sums of rectangles actually converges to the area under the curve. I know it looks like it does, but how do we know for sure?  Couldn't the tiny errors beneath/over the curve accumulate as we add more and more rectangles? What's troubling me is the whole Pi = 4 thing with the staircase approximating a circle pointwise, and how it's wrong and the perimeter of the staircase shape does not approach the circumference of the circle, even though pointwise it does approach a circle. So how are the increasingly many, increasingly small errors in Riemann sums any different? How do we know the error in each step decreases faster than the number of errors increases? I would really like to see a proof of this. Thanks so much!",,"['calculus', 'integration', 'limits']"
73,"Integral ${\large\int}_0^1\ln(1-x)\ln(1+x)\ln^2x\,dx$",Integral,"{\large\int}_0^1\ln(1-x)\ln(1+x)\ln^2x\,dx","This problem was posted at I&S a week ago, and no attempts to solve it have been posted there yet. It looks very alluring, so I decided to repost it here: Prove:   $$\int_0^1\ln(1-x)\ln(1+x)\ln^2x\,dx=24-\frac{4\pi^2}3-\frac{11\pi^4}{720}-12\ln2\\+2\ln^22-\frac16\ln^42+\pi ^2\ln2+\frac{\pi^2}6\ln^22-4\operatorname{Li}_4\!\left(\tfrac12\right)-\frac{35}4\zeta(3)+\frac72\zeta(3)\ln2.$$ I found a paper where some similar integrals are evaluated: J. A. M. Vermaseren , Harmonic sums, Mellin transforms and Integrals, Int. J. Mod. Phys. A, 14 (1999), 2037-2076, DOI: 10.1142/S0217751X99001032 , but it's not quite easy to read for me. Maybe it could be of some help for this problem.","This problem was posted at I&S a week ago, and no attempts to solve it have been posted there yet. It looks very alluring, so I decided to repost it here: Prove:   $$\int_0^1\ln(1-x)\ln(1+x)\ln^2x\,dx=24-\frac{4\pi^2}3-\frac{11\pi^4}{720}-12\ln2\\+2\ln^22-\frac16\ln^42+\pi ^2\ln2+\frac{\pi^2}6\ln^22-4\operatorname{Li}_4\!\left(\tfrac12\right)-\frac{35}4\zeta(3)+\frac72\zeta(3)\ln2.$$ I found a paper where some similar integrals are evaluated: J. A. M. Vermaseren , Harmonic sums, Mellin transforms and Integrals, Int. J. Mod. Phys. A, 14 (1999), 2037-2076, DOI: 10.1142/S0217751X99001032 , but it's not quite easy to read for me. Maybe it could be of some help for this problem.",,"['calculus', 'definite-integrals', 'logarithms', 'harmonic-numbers', 'polylogarithm']"
74,The series $\sum_{n=1}^\infty\frac1n$ diverges!,The series  diverges!,\sum_{n=1}^\infty\frac1n,We all know that the following harmonic series $$\sum_{n=1}^\infty\frac1n=\frac 1 1 + \frac 12 + \frac 13 + \cdots $$ diverges and grows very slowly!! I have seen many proofs of the result but recently found the following: $$S =\frac 1 1 + \frac 12 + \frac 13 +\frac 14+ \frac 15+ \frac 16+ \cdots$$ $$> \frac 12+\frac 12+ \frac 14+ \frac 14+ \frac 16+ \frac 16+ \cdots =\frac 1 1 + \frac 12 + \frac 13 +\cdots = S.$$ In this way we see that $S > S$. Can we conclude from this that $S$ is divergent??,We all know that the following harmonic series $$\sum_{n=1}^\infty\frac1n=\frac 1 1 + \frac 12 + \frac 13 + \cdots $$ diverges and grows very slowly!! I have seen many proofs of the result but recently found the following: $$S =\frac 1 1 + \frac 12 + \frac 13 +\frac 14+ \frac 15+ \frac 16+ \cdots$$ $$> \frac 12+\frac 12+ \frac 14+ \frac 14+ \frac 16+ \frac 16+ \cdots =\frac 1 1 + \frac 12 + \frac 13 +\cdots = S.$$ In this way we see that $S > S$. Can we conclude from this that $S$ is divergent??,,"['calculus', 'sequences-and-series', 'proof-verification', 'divergent-series']"
75,"How prove there is no continuous functions $f:[0,1]\to \mathbb R$, such that $f(x)+f(x^2)=x$.","How prove there is no continuous functions , such that .","f:[0,1]\to \mathbb R f(x)+f(x^2)=x","Prove that there is no continuous functions $f:[0,1]\to R$, such that $$ f(x)+f(x^2)=x. $$ My try. Assume that there is a continuous function with this property. Thus, for any $n\ge 1$ and all $x\in [0,1]$, \begin{align*} f(x)&=x-f(x^2)=x-\big(x^2-f(x^4)\big)=x-x^2+\big(x^4-f(x^8)\big)=\cdots\\ &=x-x^2+x^4-\cdots+(-1)^n\left(x^{2^n}-f\big(x^{2^{n+1}}\big)\right) \end{align*} since $f(0)=0$ and $\displaystyle\lim_{n\to \infty}x^{2^{n+1}}=0$ for any $x\in(0,1)$,it follows by the continuity of $f$ that $\displaystyle\lim_{n\to \infty}f\big(x^{2^{n+1}}\big)=0$, hence $$f(x)=x-x^2+x^4-x^8+\cdots+(-1)^nx^{2^n}+\cdots$$ for any $x\in (0,1)$ Why do I have prove that there is exists such a continuous functions $f$? Maybe  my example is wrong? Why? if my   method is wrong,then How prove this problem ? Thank you.","Prove that there is no continuous functions $f:[0,1]\to R$, such that $$ f(x)+f(x^2)=x. $$ My try. Assume that there is a continuous function with this property. Thus, for any $n\ge 1$ and all $x\in [0,1]$, \begin{align*} f(x)&=x-f(x^2)=x-\big(x^2-f(x^4)\big)=x-x^2+\big(x^4-f(x^8)\big)=\cdots\\ &=x-x^2+x^4-\cdots+(-1)^n\left(x^{2^n}-f\big(x^{2^{n+1}}\big)\right) \end{align*} since $f(0)=0$ and $\displaystyle\lim_{n\to \infty}x^{2^{n+1}}=0$ for any $x\in(0,1)$,it follows by the continuity of $f$ that $\displaystyle\lim_{n\to \infty}f\big(x^{2^{n+1}}\big)=0$, hence $$f(x)=x-x^2+x^4-x^8+\cdots+(-1)^nx^{2^n}+\cdots$$ for any $x\in (0,1)$ Why do I have prove that there is exists such a continuous functions $f$? Maybe  my example is wrong? Why? if my   method is wrong,then How prove this problem ? Thank you.",,"['calculus', 'analysis', 'continuity', 'power-series', 'functional-equations']"
76,Did I derive a new form of the gamma function?,Did I derive a new form of the gamma function?,,"I wish to extend the factorial to non-integer arguments in a unique way, given the following conditions: $n!=n(n-1)!$ $1!=1$ To anyone interested in viewing the final form before reading the whole post: $$x!=\exp\left[\int_0^x\left(-\gamma+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]$$ $$f(x):=\ln(x!)$$ $$f(x)=\ln(x!)=\ln(x)+\ln((x-1)!)=\ln(x)+f(x-1)$$ $$f(x)=f(x-1)+\ln(x)$$ $$\frac d{dx}f(x)=\frac d{dx}f(x-1)+\ln(x)$$ $$f'(x)=f'(x-1)+\frac1x\tag1$$ $$f'(x)=f'(x-2)+\frac1{x-1}+\frac1x$$ $$=f'(0)+1+\frac12+\frac13+\dots+\frac1x$$ for $x\in\mathbb N$ : $$f'(x)=f'(0)+\sum_{n=1}^x\frac1n\tag2$$ Euler has a nice extension of the harmonic numbers to non-integer arguments, $$f'(x)=f'(0)+\int_0^1\frac{1-t^x}{1-t}dt\tag{2.1}$$ from the FTOC we have $$\ln(x!)=\int_0^x\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi$$ $$x!=\exp\left[\int_0^x\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]\tag3$$ And with $f'(0)=-\gamma$ , the Euler mascheroni constant, we should get the gamma function.  Or we may just let it sit as an unknown parameter. My questions are if this captures all possible extensions of the factorial with the given conditions, since, if it did, it'd be a pretty good general extension to the factorial? Given a few more assumptions, it is easy enough to set bounds to what $f'(0)$ might be as well. Notably, this representation fails when considering $\Re(x)\le-1$ , but coupled with the first condition, it is extendable to all $x$ , except of course the negative integers. robjohn♦ notes an extension to the harmonic numbers that converges for $x\in\mathbb C$ , except the negative integers: $$\int_0^1\frac{1-t^\phi}{1-t}dt=\sum_{n=1}^\infty\left(\frac1n-\frac1{n+\phi}\right)$$ Any suggestions on things I could've improved and flaws in this would be nice. Edit: Using the second condition and $x=1$ , we may have $$1=\exp\left[\int_0^1\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]$$ $$\implies f'(0)=-\int_0^1\int_0^1\frac{1-t^\phi}{1-t}dt\ d\phi$$ $$f'(0)=-\gamma$$ where $\gamma$ is the Euler-mascheroni constant. Using this we get a new form of the gamma function(?): $$\boxed{x!=\exp\left[\int_0^x\left(-\gamma+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]}\tag4$$ $$=\exp\left[\int_0^x\left(-\gamma+\sum_{n=1}^\infty\left(\frac1n-\frac1{n+\phi}\right)\right)d\phi\right]$$ I'm not sure how to deal with trivial manipulations of this expression, as surely someone is gonna say ""hey, just multiply everything by $(1+\sin(2\pi x))$ and it will still satisfy the conditions, right?"" But regardless, I think this is a pretty cool new gamma function? Also, references to this if it's not new. If someone could make a graph of this to look at, you would be great.","I wish to extend the factorial to non-integer arguments in a unique way, given the following conditions: To anyone interested in viewing the final form before reading the whole post: for : Euler has a nice extension of the harmonic numbers to non-integer arguments, from the FTOC we have And with , the Euler mascheroni constant, we should get the gamma function.  Or we may just let it sit as an unknown parameter. My questions are if this captures all possible extensions of the factorial with the given conditions, since, if it did, it'd be a pretty good general extension to the factorial? Given a few more assumptions, it is easy enough to set bounds to what might be as well. Notably, this representation fails when considering , but coupled with the first condition, it is extendable to all , except of course the negative integers. robjohn♦ notes an extension to the harmonic numbers that converges for , except the negative integers: Any suggestions on things I could've improved and flaws in this would be nice. Edit: Using the second condition and , we may have where is the Euler-mascheroni constant. Using this we get a new form of the gamma function(?): I'm not sure how to deal with trivial manipulations of this expression, as surely someone is gonna say ""hey, just multiply everything by and it will still satisfy the conditions, right?"" But regardless, I think this is a pretty cool new gamma function? Also, references to this if it's not new. If someone could make a graph of this to look at, you would be great.",n!=n(n-1)! 1!=1 x!=\exp\left[\int_0^x\left(-\gamma+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right] f(x):=\ln(x!) f(x)=\ln(x!)=\ln(x)+\ln((x-1)!)=\ln(x)+f(x-1) f(x)=f(x-1)+\ln(x) \frac d{dx}f(x)=\frac d{dx}f(x-1)+\ln(x) f'(x)=f'(x-1)+\frac1x\tag1 f'(x)=f'(x-2)+\frac1{x-1}+\frac1x =f'(0)+1+\frac12+\frac13+\dots+\frac1x x\in\mathbb N f'(x)=f'(0)+\sum_{n=1}^x\frac1n\tag2 f'(x)=f'(0)+\int_0^1\frac{1-t^x}{1-t}dt\tag{2.1} \ln(x!)=\int_0^x\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi x!=\exp\left[\int_0^x\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]\tag3 f'(0)=-\gamma f'(0) \Re(x)\le-1 x x\in\mathbb C \int_0^1\frac{1-t^\phi}{1-t}dt=\sum_{n=1}^\infty\left(\frac1n-\frac1{n+\phi}\right) x=1 1=\exp\left[\int_0^1\left(f'(0)+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right] \implies f'(0)=-\int_0^1\int_0^1\frac{1-t^\phi}{1-t}dt\ d\phi f'(0)=-\gamma \gamma \boxed{x!=\exp\left[\int_0^x\left(-\gamma+\int_0^1\frac{1-t^\phi}{1-t}dt\right)d\phi\right]}\tag4 =\exp\left[\int_0^x\left(-\gamma+\sum_{n=1}^\infty\left(\frac1n-\frac1{n+\phi}\right)\right)d\phi\right] (1+\sin(2\pi x)),"['calculus', 'reference-request', 'recreational-mathematics', 'factorial', 'gamma-function']"
77,"The right ""weigh"" to do integrals","The right ""weigh"" to do integrals",,"Back in the day, before approximation methods like splines became vogue in my line of work, one way of computing the area under an empirically drawn curve was to painstakingly sketch it on a piece of graphing paper (usually with the assistance of a French curve) along with the axes, painstakingly cut along the curve, weigh the cut pieces, cut out a square equivalent to one square unit of the graphing paper, weigh that one as well, and reckon the area from the information now available. One time, when we were faced with determining the area of a curve that crossed the horizontal axis thrice, I did the careful cutting of the paper, and made sure to separate the pieces above the horizontal axis from the pieces below the horizontal axis. Then, my boss suddenly scooped up all the pieces and weighed them all. I argued that the grouped pieces should have been weighed separately, and then subtract the weights of the ""below"" group from the ""above"" group, while my boss argued that we were calculating the total area, and thus, not separating the pieces was justifiable. Which one of us was correct?","Back in the day, before approximation methods like splines became vogue in my line of work, one way of computing the area under an empirically drawn curve was to painstakingly sketch it on a piece of graphing paper (usually with the assistance of a French curve) along with the axes, painstakingly cut along the curve, weigh the cut pieces, cut out a square equivalent to one square unit of the graphing paper, weigh that one as well, and reckon the area from the information now available. One time, when we were faced with determining the area of a curve that crossed the horizontal axis thrice, I did the careful cutting of the paper, and made sure to separate the pieces above the horizontal axis from the pieces below the horizontal axis. Then, my boss suddenly scooped up all the pieces and weighed them all. I argued that the grouped pieces should have been weighed separately, and then subtract the weights of the ""below"" group from the ""above"" group, while my boss argued that we were calculating the total area, and thus, not separating the pieces was justifiable. Which one of us was correct?",,"['calculus', 'approximation', 'integration']"
78,"Integral $\int_0^1\ln\ln\,_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)\,dx$",Integral,"\int_0^1\ln\ln\,_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)\,dx","I encountered this scary integral $$\int_0^1\ln\ln\,_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)\,dx$$ where $_3F_2$ is a generalized hypergeometric function $$_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)=1+\frac{8\,\pi}{3\,\sqrt{3}}\sum _{n=1}^\infty\frac{2^{-8\,n}\ \Gamma(4 \,n)}{\Gamma(n)\ \Gamma(n+1)\ \Gamma\left(n+\frac{2}{3}\right)\ \Gamma\left(n+\frac{4}{3}\right)}x^n.$$ I do not hope much that anything can be done with it, but maybe somebody got an idea how to find a closed form for this integral.","I encountered this scary integral $$\int_0^1\ln\ln\,_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)\,dx$$ where $_3F_2$ is a generalized hypergeometric function $$_3F_2\left(\frac{1}{4},\frac{1}{2},\frac{3}{4};\frac{2}{3},\frac{4}{3};x\right)=1+\frac{8\,\pi}{3\,\sqrt{3}}\sum _{n=1}^\infty\frac{2^{-8\,n}\ \Gamma(4 \,n)}{\Gamma(n)\ \Gamma(n+1)\ \Gamma\left(n+\frac{2}{3}\right)\ \Gamma\left(n+\frac{4}{3}\right)}x^n.$$ I do not hope much that anything can be done with it, but maybe somebody got an idea how to find a closed form for this integral.",,"['calculus', 'integration', 'logarithms', 'closed-form', 'hypergeometric-function']"
79,Limit of recursive sequence $a_{n+1} = \frac{a_n}{1- \{a_n\}}$,Limit of recursive sequence,a_{n+1} = \frac{a_n}{1- \{a_n\}},"Consider the following sequence: let $a_0>0$ be rational. Define $$a_{n+1}= \frac{a_n}{1-\{a_n\}},$$ where $\{a_n\}$ is the fractional part of $a_n$ (i.e. $\{a_n\} = a_n - \lfloor a_n\rfloor$). Show that $a_n$ converges, and find its limit. We can show it converges as follows: suppose $a_n = p_n/q_n = k_n + r_n/q_n$, where $p_n = k_nq_n + r_n$, $0 \leq r_n < q_n$. Then $$a_{n+1} = \frac{p_n/q_n}{1-r_n/q_n} = \frac{p_n}{q_n - r_n},$$so the denominator will keep decreasing until it is a divisor of $p_0$ (maybe 1). Also, note we may take $p_n = p_0$ for all $n$. Further, the limit will be $\leq \frac{p_0}{\operatorname{gcf}{(p_0,q_0)}}$, because if $f \mid p_0$ and $f\mid q_n$, then $f\mid (p_0 - k_nq_n)=r_n$, so $f \mid q_n - r_n = q_{n+1}$. But the limit may be strictly smaller; for instance, $a_0 = 30/7$ converges right away to 6. Can we say anything else about the limit of a sequence starting with $a_0$? This was a problem on a qualifier, so I suspect there is more to the answer, but maybe not.","Consider the following sequence: let $a_0>0$ be rational. Define $$a_{n+1}= \frac{a_n}{1-\{a_n\}},$$ where $\{a_n\}$ is the fractional part of $a_n$ (i.e. $\{a_n\} = a_n - \lfloor a_n\rfloor$). Show that $a_n$ converges, and find its limit. We can show it converges as follows: suppose $a_n = p_n/q_n = k_n + r_n/q_n$, where $p_n = k_nq_n + r_n$, $0 \leq r_n < q_n$. Then $$a_{n+1} = \frac{p_n/q_n}{1-r_n/q_n} = \frac{p_n}{q_n - r_n},$$so the denominator will keep decreasing until it is a divisor of $p_0$ (maybe 1). Also, note we may take $p_n = p_0$ for all $n$. Further, the limit will be $\leq \frac{p_0}{\operatorname{gcf}{(p_0,q_0)}}$, because if $f \mid p_0$ and $f\mid q_n$, then $f\mid (p_0 - k_nq_n)=r_n$, so $f \mid q_n - r_n = q_{n+1}$. But the limit may be strictly smaller; for instance, $a_0 = 30/7$ converges right away to 6. Can we say anything else about the limit of a sequence starting with $a_0$? This was a problem on a qualifier, so I suspect there is more to the answer, but maybe not.",,"['calculus', 'sequences-and-series', 'elementary-number-theory', 'recurrence-relations']"
80,Are $\sin$ and $\cos$ the only functions whose derivatives are equal to each other up to a sign? [closed],Are  and  the only functions whose derivatives are equal to each other up to a sign? [closed],\sin \cos,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Are $\sin$ and $\cos$   the only functions that satisfy the following relationship:  $$  x'(t) = -y(t)$$  and $$ y'(t) = x(t) $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Are $\sin$ and $\cos$   the only functions that satisfy the following relationship:  $$  x'(t) = -y(t)$$  and $$ y'(t) = x(t) $$",,"['calculus', 'derivatives']"
81,Why does $1/x$ diverge?,Why does  diverge?,1/x,"So for the formula $\dfrac {1}{n}$ , If you were to add up all $y$ values from $n=1$ to $n=∞$ , wouldn't the sum $$\sum_{n=1}^\infty \frac{1}{n}$$ approach a number because even though you are always adding, aren't you just adding smaller and smaller numbers? Wouldn't this mean that it approached a certain number?","So for the formula , If you were to add up all values from to , wouldn't the sum approach a number because even though you are always adding, aren't you just adding smaller and smaller numbers? Wouldn't this mean that it approached a certain number?",\dfrac {1}{n} y n=1 n=∞ \sum_{n=1}^\infty \frac{1}{n},"['calculus', 'sequences-and-series', 'integration']"
82,Prove that $f$ continuous and $\int_a^\infty |f(x)|\;dx$ finite imply $\lim\limits_{ x \to \infty } f(x)=0$,Prove that  continuous and  finite imply,f \int_a^\infty |f(x)|\;dx \lim\limits_{ x \to \infty } f(x)=0,"I'm trying to prove  the following claim: If $f$ is continuous  and $\displaystyle\int_a^\infty |f(x)|\;dx$ is finite then $\lim\limits_{ x \to \infty } f(x)=0$ . Here the counter example of all these sine functions won't do, since  their infinite integral does not converge absolutely. Any hints for a proof?","I'm trying to prove  the following claim: If is continuous  and is finite then . Here the counter example of all these sine functions won't do, since  their infinite integral does not converge absolutely. Any hints for a proof?",f \displaystyle\int_a^\infty |f(x)|\;dx \lim\limits_{ x \to \infty } f(x)=0,"['calculus', 'integration']"
83,Ramanujan's approximation for $\pi$,Ramanujan's approximation for,\pi,"In 1910, Srinivasa Ramanujan found several rapidly converging infinite series of $\pi$, such as $$ \frac{1}{\pi} = \frac{2\sqrt{2}}{9801} \sum^\infty_{k=0} \frac{(4k)!(1103+26390k)}{(k!)^4 396^{4k}}. $$ Wikipedia says this formula computes a further eight decimal places of $\pi$ with each term in the series. There are also generalizations called Ramanujan–Sato series . I tested it with Maple and really each term gives more eight right places. Anyone could tell me, how could each step give more eight correct digits?","In 1910, Srinivasa Ramanujan found several rapidly converging infinite series of $\pi$, such as $$ \frac{1}{\pi} = \frac{2\sqrt{2}}{9801} \sum^\infty_{k=0} \frac{(4k)!(1103+26390k)}{(k!)^4 396^{4k}}. $$ Wikipedia says this formula computes a further eight decimal places of $\pi$ with each term in the series. There are also generalizations called Ramanujan–Sato series . I tested it with Maple and really each term gives more eight right places. Anyone could tell me, how could each step give more eight correct digits?",,"['calculus', 'power-series', 'approximation', 'pi']"
84,What is the difference between the gradient and the directional derivative?,What is the difference between the gradient and the directional derivative?,,"Consider a function $g: \mathbb{R^3} \to \mathbb{R}$ defined by $g(x,y,z) = z^2 -x^3 + 2z + 3y^3$ Find the gradient of $g$ at the point $(2,1,-1)$ . Is the gradient $\nabla g(2,1,-1)$ given by a vector, that is, $\nabla g(2,1,-1) = -12i + 9j$ ? If so, then what does the directional derivative mean?","Consider a function defined by Find the gradient of at the point . Is the gradient given by a vector, that is, ? If so, then what does the directional derivative mean?","g: \mathbb{R^3} \to \mathbb{R} g(x,y,z) = z^2 -x^3 + 2z + 3y^3 g (2,1,-1) \nabla g(2,1,-1) \nabla g(2,1,-1) = -12i + 9j","['calculus', 'derivatives']"
85,Floor function properties: $[2x] = [x] + [ x + \frac12 ]$ and $[nx] = \sum_{k = 0}^{n - 1} [ x + \frac{k}{n} ] $,Floor function properties:  and,[2x] = [x] + [ x + \frac12 ] [nx] = \sum_{k = 0}^{n - 1} [ x + \frac{k}{n} ] ,"I'm doing some exercises on Apostol's calculus, on the floor function. Now, he doesn't give an explicit definition of $[x]$, so I'm going with this one: DEFINITION Given  $x\in \Bbb R$, the integer part of $x$ is the unique $z\in \Bbb Z$ such that $$z\leq x < z+1$$ and we denote it by $[x]$. Now he asks to prove some basic things about it, such as: if $n\in \Bbb Z$, then $[x+n]=[x]+n$ So I proved it like this: Let $z=[x+n]$ and $z'=[x]$. Then we have that $$z\leq x+n<z+1$$ $$z'\leq x<z'+1$$ Then $$z'+n\leq x+n<z'+n+1$$ But since $z'$ is an integer, so is $z'+n$. Since $z$ is unique, it must be that $z'+n=z$. However, this doesn't seem to get me anywhere to prove that  $$\left[ {2x} \right] = \left[ x \right] + \left[ {x + \frac{1}{2}} \right]$$ in and in general that $$\left[ {nx} \right] = \sum\limits_{k = 0}^{n - 1} {\left[ {x + \frac{k}{n}} \right]} $$ Obviously one could do an informal proof thinking about ""the carries"", but that's not the idea, let alone how tedious it would be. Maybe there is some easier or clearer characterization of $[x]$ in terms of $x$ to work this out. Another property is $$[-x]=\begin{cases}-[x]\text{ ; if }x\in \Bbb Z \cr-[x]-1 \text{ ; otherwise}\end{cases}$$ I argue: if $x\in\Bbb Z$, it is clear $[x]=x$. Then $-[x]=-x$, and $-[x]\in \Bbb Z$ so $[-[x]]=-[x]=[-x]$. For the other, I guess one could say: $$n \leqslant x < n + 1 \Rightarrow  - n - 1 < x \leqslant -n$$ and since $x$ is not an integer, this should be the same as $$ - n - 1 \leqslant -x < -n$$ $$ - n - 1 \leqslant -x < (-n-1)+1$$ So $[-x]=-[x]-1$","I'm doing some exercises on Apostol's calculus, on the floor function. Now, he doesn't give an explicit definition of $[x]$, so I'm going with this one: DEFINITION Given  $x\in \Bbb R$, the integer part of $x$ is the unique $z\in \Bbb Z$ such that $$z\leq x < z+1$$ and we denote it by $[x]$. Now he asks to prove some basic things about it, such as: if $n\in \Bbb Z$, then $[x+n]=[x]+n$ So I proved it like this: Let $z=[x+n]$ and $z'=[x]$. Then we have that $$z\leq x+n<z+1$$ $$z'\leq x<z'+1$$ Then $$z'+n\leq x+n<z'+n+1$$ But since $z'$ is an integer, so is $z'+n$. Since $z$ is unique, it must be that $z'+n=z$. However, this doesn't seem to get me anywhere to prove that  $$\left[ {2x} \right] = \left[ x \right] + \left[ {x + \frac{1}{2}} \right]$$ in and in general that $$\left[ {nx} \right] = \sum\limits_{k = 0}^{n - 1} {\left[ {x + \frac{k}{n}} \right]} $$ Obviously one could do an informal proof thinking about ""the carries"", but that's not the idea, let alone how tedious it would be. Maybe there is some easier or clearer characterization of $[x]$ in terms of $x$ to work this out. Another property is $$[-x]=\begin{cases}-[x]\text{ ; if }x\in \Bbb Z \cr-[x]-1 \text{ ; otherwise}\end{cases}$$ I argue: if $x\in\Bbb Z$, it is clear $[x]=x$. Then $-[x]=-x$, and $-[x]\in \Bbb Z$ so $[-[x]]=-[x]=[-x]$. For the other, I guess one could say: $$n \leqslant x < n + 1 \Rightarrow  - n - 1 < x \leqslant -n$$ and since $x$ is not an integer, this should be the same as $$ - n - 1 \leqslant -x < -n$$ $$ - n - 1 \leqslant -x < (-n-1)+1$$ So $[-x]=-[x]-1$",,"['calculus', 'summation', 'ceiling-and-floor-functions']"
86,How can a function with a hole (removable discontinuity) equal a function with no hole?,How can a function with a hole (removable discontinuity) equal a function with no hole?,,"I've done some research, and I'm hoping someone can check me. My question was this: Assume I have the function $f(x) = \frac{(x-3)(x+2)}{(x-3)}$ , so it has removable discontinuity at $x = 3$ . We remove that discontinuity with algebra: $f(x) = \frac{(x-3)(x+2)}{(x-3)} = (x+2)$ . BUT, the graph of the first function has a hole at $x = 3$ , and the graph of the second function is continuous everywhere. How can they be ""equal"" if one has a hole and the other does not? I think that this is the answer: Because the original function is undefined at the point $x = 3$ , we have to restrict the domain to $\mathbb{R} \setminus 3$ . And when we manipulate that function with algebra, the final result, $f(x) = (x + 2)$ is still using this restricted domain. So even though the function $f(x) = (x+2)$ would not have a hole if the domain were all of $\mathbb{R}$ , we are sort of ""imposing"" a hole at $x = 3$ by continuing to throw that point out of the domain. And then just to close the loop: Removing the removable discontinuity is useful because it allows us to ""pretend"" that we're working with a function that is everywhere continuous, which helps us easily find the limit. But the reality is that the function $f(x) = (x +2)$ is actually NOT continuous everywhere when we restrict the domain by throwing out the point 3. Or am I now taking things too far? Thanks in advance! EDIT: For anyone coming across this in the future, in addition to the excellent answers below, I also found this other question about the continuity of functions with removable discontinuities helpful.","I've done some research, and I'm hoping someone can check me. My question was this: Assume I have the function , so it has removable discontinuity at . We remove that discontinuity with algebra: . BUT, the graph of the first function has a hole at , and the graph of the second function is continuous everywhere. How can they be ""equal"" if one has a hole and the other does not? I think that this is the answer: Because the original function is undefined at the point , we have to restrict the domain to . And when we manipulate that function with algebra, the final result, is still using this restricted domain. So even though the function would not have a hole if the domain were all of , we are sort of ""imposing"" a hole at by continuing to throw that point out of the domain. And then just to close the loop: Removing the removable discontinuity is useful because it allows us to ""pretend"" that we're working with a function that is everywhere continuous, which helps us easily find the limit. But the reality is that the function is actually NOT continuous everywhere when we restrict the domain by throwing out the point 3. Or am I now taking things too far? Thanks in advance! EDIT: For anyone coming across this in the future, in addition to the excellent answers below, I also found this other question about the continuity of functions with removable discontinuities helpful.",f(x) = \frac{(x-3)(x+2)}{(x-3)} x = 3 f(x) = \frac{(x-3)(x+2)}{(x-3)} = (x+2) x = 3 x = 3 \mathbb{R} \setminus 3 f(x) = (x + 2) f(x) = (x+2) \mathbb{R} x = 3 f(x) = (x +2),"['calculus', 'limits', 'continuity', 'faq']"
87,$\ln|x|$ vs.$\ln(x)$? When is the $\ln$ antiderivative marked as an absolute value?,vs.? When is the  antiderivative marked as an absolute value?,\ln|x| \ln(x) \ln,"One of the answers to the problems I'm doing had straight lines:     $$ \ln|y^2-25|$$ versus another problem's just now:     $$ \ln(1+e^r) $$ I know this is probably to do with the absolute value.  Is the absolute value marking necessary because #1 was the antiderivative of a squared variable expression that could be either positive or negative (and had to be positive because, well, natural log) and the second was positive by default? Sorry if this is me asking and answering my own question; I'd just love to get confirmation in case I'm wrong.","One of the answers to the problems I'm doing had straight lines:     $$ \ln|y^2-25|$$ versus another problem's just now:     $$ \ln(1+e^r) $$ I know this is probably to do with the absolute value.  Is the absolute value marking necessary because #1 was the antiderivative of a squared variable expression that could be either positive or negative (and had to be positive because, well, natural log) and the second was positive by default? Sorry if this is me asking and answering my own question; I'd just love to get confirmation in case I'm wrong.",,"['calculus', 'integration']"
88,Can derivatives be defined as anti-integrals?,Can derivatives be defined as anti-integrals?,,I see integrals defined as anti-derivatives but for some reason I haven't come across the reverse. Both seem equally implied by the fundamental theorem of calculus. This emerged as a sticking point in this question .,I see integrals defined as anti-derivatives but for some reason I haven't come across the reverse. Both seem equally implied by the fundamental theorem of calculus. This emerged as a sticking point in this question .,,"['calculus', 'terminology']"
89,A limit problem $\lim\limits_{x \to 0}\frac{x\sin(\sin x) - \sin^{2}x}{x^{6}}$,A limit problem,\lim\limits_{x \to 0}\frac{x\sin(\sin x) - \sin^{2}x}{x^{6}},"This is a problem from ""A Course of Pure Mathematics"" by G H Hardy. Find the limit $$\lim_{x \to 0}\frac{x\sin(\sin x) - \sin^{2}x}{x^{6}}$$ I had solved it long back (solution presented in my blog here ) but I had to use the L'Hospital's Rule (another alternative is Taylor's series). This problem is given in an introductory chapter on limits and the concept of Taylor series or L'Hospital's rule is provided in a later chapter in the same book. So I am damn sure that there is a mechanism to evaluate this limit by simpler methods involving basic algebraic and trigonometric manipulations and use of limit $$\lim_{x \to 0}\frac{\sin x}{x} = 1$$ but I have not been able to find such a solution till now. If someone has any ideas in this direction please help me out. PS: The answer is $1/18$ and can be easily verified by a calculator by putting $x = 0.01$","This is a problem from ""A Course of Pure Mathematics"" by G H Hardy. Find the limit $$\lim_{x \to 0}\frac{x\sin(\sin x) - \sin^{2}x}{x^{6}}$$ I had solved it long back (solution presented in my blog here ) but I had to use the L'Hospital's Rule (another alternative is Taylor's series). This problem is given in an introductory chapter on limits and the concept of Taylor series or L'Hospital's rule is provided in a later chapter in the same book. So I am damn sure that there is a mechanism to evaluate this limit by simpler methods involving basic algebraic and trigonometric manipulations and use of limit $$\lim_{x \to 0}\frac{\sin x}{x} = 1$$ but I have not been able to find such a solution till now. If someone has any ideas in this direction please help me out. PS: The answer is $1/18$ and can be easily verified by a calculator by putting $x = 0.01$",,"['calculus', 'limits', 'alternative-proof']"
90,Are there ways of finding the $n$-th derivative of a function without computing the $(n-1)$-th derivative?,Are there ways of finding the -th derivative of a function without computing the -th derivative?,n (n-1),"Say we have a function $f(x)$ that is infinitely differentiable at some point. Is it possible to find $f^{(n)}(x)$ without having to find first $f^{(n-1)}(x)$?  If so, does it take less effort than computing preceding derivatives (i.e. $f'(x), f''(x), \cdots, f^{(n-1)}(x)$)? I often find it very tedious to find multiple derivatives so I was wondering if someone knows the answer to this question.","Say we have a function $f(x)$ that is infinitely differentiable at some point. Is it possible to find $f^{(n)}(x)$ without having to find first $f^{(n-1)}(x)$?  If so, does it take less effort than computing preceding derivatives (i.e. $f'(x), f''(x), \cdots, f^{(n-1)}(x)$)? I often find it very tedious to find multiple derivatives so I was wondering if someone knows the answer to this question.",,"['calculus', 'derivatives']"
91,Different definitions of trigonometric functions,Different definitions of trigonometric functions,,"In school, we learn that sin is ""opposite over hypotenuse"" and cos is ""adjacent over hypotenuse"". Later on, we learn the power series definitions of sin and cos. How can one prove that these two definitions are equivalent?","In school, we learn that sin is ""opposite over hypotenuse"" and cos is ""adjacent over hypotenuse"". Later on, we learn the power series definitions of sin and cos. How can one prove that these two definitions are equivalent?",,"['geometry', 'calculus', 'trigonometry']"
92,"What does ""approach zero"" really mean?","What does ""approach zero"" really mean?",,"We repeat the phrase ""approach zero"" regularly, but what exactly does it mean to say ""$\delta t$ approaches zero""? P.S. If this a stupid question, then please forgive me.","We repeat the phrase ""approach zero"" regularly, but what exactly does it mean to say ""$\delta t$ approaches zero""? P.S. If this a stupid question, then please forgive me.",,"['calculus', 'definition']"
93,Is there something fundamental that makes integrals nontrivial to solve? [duplicate],Is there something fundamental that makes integrals nontrivial to solve? [duplicate],,"This question already has answers here : Why is integration so much harder than differentiation? (6 answers) Closed 7 years ago . First of all, I apologize if this isn't the right forum. The thing with derivatives is that once you learn product/quotient/chain rule and the formulas for trig/exponential/logarithmic functions, you can take the derivative of any function (as far as I can tell). With integrals, the methods to solving are much more complex, with u-subs, integration by parts, partial fraction decomposition, etc. And then you have equations like $\int \sqrt{\sin x\cos x}dx$ which are simple to write but impossible to solve in terms of elementary functions . Does this have something to do with the fundamental theorem of calculus, and how the integral is defined as the area under a curve, or something else? I'm only in Calc II right now, so it's possible that I will learn more about why this is later on in my math ""schedule"".","This question already has answers here : Why is integration so much harder than differentiation? (6 answers) Closed 7 years ago . First of all, I apologize if this isn't the right forum. The thing with derivatives is that once you learn product/quotient/chain rule and the formulas for trig/exponential/logarithmic functions, you can take the derivative of any function (as far as I can tell). With integrals, the methods to solving are much more complex, with u-subs, integration by parts, partial fraction decomposition, etc. And then you have equations like $\int \sqrt{\sin x\cos x}dx$ which are simple to write but impossible to solve in terms of elementary functions . Does this have something to do with the fundamental theorem of calculus, and how the integral is defined as the area under a curve, or something else? I'm only in Calc II right now, so it's possible that I will learn more about why this is later on in my math ""schedule"".",,"['calculus', 'definite-integrals']"
94,Are the definite and indefinite integrals actually two different things? Where is the flaw in my understanding?,Are the definite and indefinite integrals actually two different things? Where is the flaw in my understanding?,,"Some context: I'm an engineer, and I tend to have a rather unusual way of understanding and thinking about things, most likely related to my being autistic. I found this question on the HNQ and upon reading it I felt rather confused. I have always understood that the indefinite and definite integrals are two sides of the same underlying concept, that there is no meaningful difference between them other than that one evaluates to a function and the other to a quantity. Essentially, the definite integral is what you get from running the numbers on the result of an indefinite integral, and saying it's unrelated to the definite integral is like saying that $\sin(x)$ is fundamentally different from $\sin(a)$ , assuming $x$ is a variable and $a$ a constant, for no reason other than that $x$ is a variable and $a$ a constant. To me, whether the definite and indefinite integrals are two sides of the same thing, or two closely related but different things, seems more like a question of philosophy than mathematics. Where is the flaw in my understanding? Is there one?","Some context: I'm an engineer, and I tend to have a rather unusual way of understanding and thinking about things, most likely related to my being autistic. I found this question on the HNQ and upon reading it I felt rather confused. I have always understood that the indefinite and definite integrals are two sides of the same underlying concept, that there is no meaningful difference between them other than that one evaluates to a function and the other to a quantity. Essentially, the definite integral is what you get from running the numbers on the result of an indefinite integral, and saying it's unrelated to the definite integral is like saying that is fundamentally different from , assuming is a variable and a constant, for no reason other than that is a variable and a constant. To me, whether the definite and indefinite integrals are two sides of the same thing, or two closely related but different things, seems more like a question of philosophy than mathematics. Where is the flaw in my understanding? Is there one?",\sin(x) \sin(a) x a x a,"['calculus', 'integration', 'terminology']"
95,When to do u-substitution and when to integrate by parts,When to do u-substitution and when to integrate by parts,,"I'm in my first semester of calculus, so the problems I'm facing are about as hard as those on KhanAcademy calculus playlist . I'm currently doing integration, a somewhat difficult part of the course. Doing derivatives is mechanics; finding the integral is an art. The two main techniques showed are u-substitution integration by parts My question is: are there any rules of thumb (preferably with a logical reason behind it) of when to use which?","I'm in my first semester of calculus, so the problems I'm facing are about as hard as those on KhanAcademy calculus playlist . I'm currently doing integration, a somewhat difficult part of the course. Doing derivatives is mechanics; finding the integral is an art. The two main techniques showed are u-substitution integration by parts My question is: are there any rules of thumb (preferably with a logical reason behind it) of when to use which?",,['calculus']
96,Understanding the Frechet derivative,Understanding the Frechet derivative,,"What is the relationship between the normal 'high school' concept of a derivative and a Frechet derivative? According to wikipedia the Frechet ""extends the idea of the derivative from real-valued functions of one real variable to functions on Banach spaces."" Suppose we have $f:(\mathbb{R},\|\cdot\|_{\mathbb{R}})\rightarrow (\mathbb{R},\|\cdot\|_{\mathbb{R}})$ where $\|x\|_{\mathbb{R}}=|x|$ and $f(x)=x$. I think $(\mathbb{R},\|\cdot\|_{\mathbb{R}})$ is Banach so I feel that ideally the Frechet derivative of $f(x)$ at 0 should be equal to our standard derivative at $0$ which is $f'(0)=1$. But $$\lim_{h \rightarrow 0} \frac{|f(x+h)-f(x)-1|}{|h|} \neq 0$$ So my question is how does the Frechet derivative extend the idea of a derivative from real-valued function of one variable?","What is the relationship between the normal 'high school' concept of a derivative and a Frechet derivative? According to wikipedia the Frechet ""extends the idea of the derivative from real-valued functions of one real variable to functions on Banach spaces."" Suppose we have $f:(\mathbb{R},\|\cdot\|_{\mathbb{R}})\rightarrow (\mathbb{R},\|\cdot\|_{\mathbb{R}})$ where $\|x\|_{\mathbb{R}}=|x|$ and $f(x)=x$. I think $(\mathbb{R},\|\cdot\|_{\mathbb{R}})$ is Banach so I feel that ideally the Frechet derivative of $f(x)$ at 0 should be equal to our standard derivative at $0$ which is $f'(0)=1$. But $$\lim_{h \rightarrow 0} \frac{|f(x+h)-f(x)-1|}{|h|} \neq 0$$ So my question is how does the Frechet derivative extend the idea of a derivative from real-valued function of one variable?",,"['calculus', 'analysis', 'banach-spaces']"
97,Does the series $\sum\limits_{n=1}^{\infty}\frac{\sin(n-\sqrt{n^2+n})}{n}$ converge?,Does the series  converge?,\sum\limits_{n=1}^{\infty}\frac{\sin(n-\sqrt{n^2+n})}{n},"I'm just reviewing for my exam tomorow looking at old exams, unfortunately I don't have solutions.  Here is a question I found : determine if the series converges or diverges.  If it converges find it's limit. $$\displaystyle \sum\limits_{n=1}^{\infty}\dfrac{\sin(n-\sqrt{n^2+n})}{n}$$ I've ruled down possible tests to the limit comparison test, but I feel like I've made a mistake somewhere. divergence test - limit is 0 by the squeeze theorem integral test - who knows how to solve this comparison test - series is not positive ratio root tests - on the absolute value of the series, this wouldn't work out alternating series test - would not work, the series is not decreasing or alternating Any ideas what to compare this series here with or where my mistake is on my reasoning above?","I'm just reviewing for my exam tomorow looking at old exams, unfortunately I don't have solutions.  Here is a question I found : determine if the series converges or diverges.  If it converges find it's limit. $$\displaystyle \sum\limits_{n=1}^{\infty}\dfrac{\sin(n-\sqrt{n^2+n})}{n}$$ I've ruled down possible tests to the limit comparison test, but I feel like I've made a mistake somewhere. divergence test - limit is 0 by the squeeze theorem integral test - who knows how to solve this comparison test - series is not positive ratio root tests - on the absolute value of the series, this wouldn't work out alternating series test - would not work, the series is not decreasing or alternating Any ideas what to compare this series here with or where my mistake is on my reasoning above?",,"['calculus', 'sequences-and-series']"
98,"What's wrong with this ""backwards"" definition of limit?","What's wrong with this ""backwards"" definition of limit?",,"Can anyone please give an example of why the following definition of $\displaystyle{\lim_{x \to a} f(x) =L}$ is NOT correct?: $\forall$ $\delta >0$ $\exists$ $\epsilon>0$ such that if $0<|x-a|<\delta$ then $|f(x)-L|<\epsilon$ I've been trying to solve this for a while, and I think it would give me a greater understanding of why the limit definition is what it is, because this alternative definition seems quite logical and similar to the real one, yet it supposedly shouldn't work.","Can anyone please give an example of why the following definition of $\displaystyle{\lim_{x \to a} f(x) =L}$ is NOT correct?: $\forall$ $\delta >0$ $\exists$ $\epsilon>0$ such that if $0<|x-a|<\delta$ then $|f(x)-L|<\epsilon$ I've been trying to solve this for a while, and I think it would give me a greater understanding of why the limit definition is what it is, because this alternative definition seems quite logical and similar to the real one, yet it supposedly shouldn't work.",,"['calculus', 'limits', 'definition', 'epsilon-delta']"
99,What is the difference between statistical mean and calculus mean?,What is the difference between statistical mean and calculus mean?,,"For example in statistics we learn that mean = E(x) of a function which is defined as $$\mu = \int_a^b xf(x) \,dx$$ however in calculus we learn that $$\mu = \frac {1}{b-a}\int_a^b f(x) \,dx $$ What is the difference between the means in statistics and calculus and why don't they give the same answer? thank you","For example in statistics we learn that mean = E(x) of a function which is defined as $$\mu = \int_a^b xf(x) \,dx$$ however in calculus we learn that $$\mu = \frac {1}{b-a}\int_a^b f(x) \,dx $$ What is the difference between the means in statistics and calculus and why don't they give the same answer? thank you",,"['calculus', 'statistics', 'average']"
