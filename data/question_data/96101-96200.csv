,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Showing that the pull-back of the Euclidean metric by an entire function is a metric,Showing that the pull-back of the Euclidean metric by an entire function is a metric,,"Suppose $f$ is a non-constant entire function. Define $$ d_f(a,b) = \inf_{\gamma} \ell(f\circ \gamma), $$ where $a,b \in \mathbb{C}$, $\ell$ is the euclidean length, and $\gamma$ is a path connecting $a$ and $b$. I'm trying to prove $d_f$ is a metric. I'm having trouble showing that $$ a\ne b  \, \Longrightarrow \, d_f(a,b) \ne 0 .$$ Now if $f(a)\ne f(b)$, this is clear. If, however, $f(a)=f(b)$, is it not possible that there are paths, $\gamma_n$, connecting $a$ and $b$, such that $\ell(f\circ \gamma_n)$ approches $0$ ?","Suppose $f$ is a non-constant entire function. Define $$ d_f(a,b) = \inf_{\gamma} \ell(f\circ \gamma), $$ where $a,b \in \mathbb{C}$, $\ell$ is the euclidean length, and $\gamma$ is a path connecting $a$ and $b$. I'm trying to prove $d_f$ is a metric. I'm having trouble showing that $$ a\ne b  \, \Longrightarrow \, d_f(a,b) \ne 0 .$$ Now if $f(a)\ne f(b)$, this is clear. If, however, $f(a)=f(b)$, is it not possible that there are paths, $\gamma_n$, connecting $a$ and $b$, such that $\ell(f\circ \gamma_n)$ approches $0$ ?",,['complex-analysis']
1,"Is it true that if function $f$ is analytic( ~ holomorphic) in $\Omega \subset\Bbb C$, then it satisfies C-R equations?","Is it true that if function  is analytic( ~ holomorphic) in , then it satisfies C-R equations?",f \Omega \subset\Bbb C,"Is it true that if function $f$ is analytic ($\sim$ holomorphic) in $\Omega \subset \Bbb C$, then it satisfies the Cauchy-Riemann (C-R) equations? And is it t true that if $f$ satisfies C-R equations and the functions $u(x,y)$ and $v(x,y)$ (the real and imaginary parts of $f(x,y)$ respectively) have first partial derivatives which are continuous, then $f$ is analytic ($\sim$ holomorphic)?","Is it true that if function $f$ is analytic ($\sim$ holomorphic) in $\Omega \subset \Bbb C$, then it satisfies the Cauchy-Riemann (C-R) equations? And is it t true that if $f$ satisfies C-R equations and the functions $u(x,y)$ and $v(x,y)$ (the real and imaginary parts of $f(x,y)$ respectively) have first partial derivatives which are continuous, then $f$ is analytic ($\sim$ holomorphic)?",,[]
2,application of the maximum modulus theorem,application of the maximum modulus theorem,,"Let $f$ be holomorphic on the unit disc and continuous on the unit circle. Suppose there is an $M \in \mathbb{R}$ such that $|f(z)| \leq M$ on the unit circle and let $\alpha_1, \alpha_2, ..., \alpha_n$ be zeros of $f$ in the unit disc listed according to multiplicity. Show that $|f(z)| \leq M \frac{|z-\alpha_1| \cdots |z- \alpha_n|}{|1-z \overline{\alpha_1}| \cdots |1-z \overline{\alpha_n}|}$. Why can't I apply the Maximum Modulus theorem to $f$ directly? Is there something I am missing?","Let $f$ be holomorphic on the unit disc and continuous on the unit circle. Suppose there is an $M \in \mathbb{R}$ such that $|f(z)| \leq M$ on the unit circle and let $\alpha_1, \alpha_2, ..., \alpha_n$ be zeros of $f$ in the unit disc listed according to multiplicity. Show that $|f(z)| \leq M \frac{|z-\alpha_1| \cdots |z- \alpha_n|}{|1-z \overline{\alpha_1}| \cdots |1-z \overline{\alpha_n}|}$. Why can't I apply the Maximum Modulus theorem to $f$ directly? Is there something I am missing?",,['complex-analysis']
3,A simple question : Is $g(z + \Delta z).g(z) = g(z)^2$?,A simple question : Is ?,g(z + \Delta z).g(z) = g(z)^2,"Is $g(z + \Delta z).g(z) = g(z)^2$ ? The full expression is a $\lim_{\Delta z \to 0} \frac {A}{G} $, where $G$ is the left-hand side of the above expression. It's a question about a solution to a problem that I'm going through, and I can't post it because I don't have enough reps. But the gist of it is that the above expression is taken to be true and thus he factors a $g(z)^2$ out of the limits in the denominator. I'd appreciate it if somebody explains how this is possible. Thanks!","Is $g(z + \Delta z).g(z) = g(z)^2$ ? The full expression is a $\lim_{\Delta z \to 0} \frac {A}{G} $, where $G$ is the left-hand side of the above expression. It's a question about a solution to a problem that I'm going through, and I can't post it because I don't have enough reps. But the gist of it is that the above expression is taken to be true and thus he factors a $g(z)^2$ out of the limits in the denominator. I'd appreciate it if somebody explains how this is possible. Thanks!",,"['complex-analysis', 'limits']"
4,Antiderivative simply connected region,Antiderivative simply connected region,,"Why do analytic functions always have an antiderivative on a simply connected region? Thank you for your time, Chris","Why do analytic functions always have an antiderivative on a simply connected region? Thank you for your time, Chris",,['complex-analysis']
5,Upper bound for complex polynomial,Upper bound for complex polynomial,,I have a polynomial $p$ of degree $n$ satisfying $\lvert p(z) \lvert \leq c\ \ \forall z\in\partial B_1(0)$. (Isn't this true for any polynomial?) Show $\lvert p(z)\lvert \leq c \lvert z\lvert^n \ \ \forall z\in \mathbb{C}\backslash B_1(0)$. The obvious attempt would be $|p(z)|=|p(\lvert z\lvert\frac{z}{\lvert z\lvert})|=|\sum_{i=0}^n a_i |z|^i (\frac{z}{\lvert z\lvert})^i|\leq |z|^n \sum_{i=0}^n |a_i (\frac{z}{\lvert z\lvert})^i|$ which doesn't lead anywhere. I guess I have to apply some maximum principle but don't know how.,I have a polynomial $p$ of degree $n$ satisfying $\lvert p(z) \lvert \leq c\ \ \forall z\in\partial B_1(0)$. (Isn't this true for any polynomial?) Show $\lvert p(z)\lvert \leq c \lvert z\lvert^n \ \ \forall z\in \mathbb{C}\backslash B_1(0)$. The obvious attempt would be $|p(z)|=|p(\lvert z\lvert\frac{z}{\lvert z\lvert})|=|\sum_{i=0}^n a_i |z|^i (\frac{z}{\lvert z\lvert})^i|\leq |z|^n \sum_{i=0}^n |a_i (\frac{z}{\lvert z\lvert})^i|$ which doesn't lead anywhere. I guess I have to apply some maximum principle but don't know how.,,['complex-analysis']
6,Riemann sphere and Maps,Riemann sphere and Maps,,"Could somebody please clarify the following for me? I am not too clear about the relationship between the Riemann sphere and Möbius maps. I know that we can through projection make some Möbius maps correspond to isometries of the sphere. But it is not a bijection right? Which maps have corresponding isometries and which don't, vice versa? Thanks","Could somebody please clarify the following for me? I am not too clear about the relationship between the Riemann sphere and Möbius maps. I know that we can through projection make some Möbius maps correspond to isometries of the sphere. But it is not a bijection right? Which maps have corresponding isometries and which don't, vice versa? Thanks",,"['linear-algebra', 'complex-analysis', 'conformal-geometry']"
7,Finding $\frac{1}{2\pi}\int_{0}^{2\pi} \cos^{2n} x dx$,Finding,\frac{1}{2\pi}\int_{0}^{2\pi} \cos^{2n} x dx,"I have a question that asks me to find the value of $\displaystyle\frac{1}{2\pi}\int_{0}^{2\pi} \cos^{2n} x dx$  $\ $ by considering the integral $$ \displaystyle \oint_{\gamma} \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}dz$$ where $\gamma$ is the circle of radius 1 centered at the origin. So I expanded the integrand out using the binomial theorem to get the following: $$\displaystyle \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}=\sum \binom{2n}{k} z^{2n-2k-1}$$ so the integral of each of these terms is 0 apart from when $k=n$ and we get $\int_{\gamma}z^{-1} dz=2\pi{i}$ and so we have that: $$ \displaystyle \oint_{\gamma} \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}dz=\binom{2n}{n} 2\pi{i}$$ If we now let $z=e^{it}$ on $\gamma$ then the integral becomes: $$\displaystyle \int_{0}^{2\pi} \frac{ie^{it}}{e^{it}} \left ( e^{it}+e^{-it} \right )^{2n}dt=  {i}\int_{0}^{2\pi} \left ( e^{it}+e^{-it} \right )^{2n}dt={i}\int_{0}^{2\pi} \left ( 2{\cos(t)} \right ) ^{2n}dt=\binom{2n}{n} 2\pi{i}$$ and finally: $$\displaystyle\int_{0}^{2\pi} \left ( \cos(t) \right ) ^{2n}dt=\binom{2n}{n} \frac{2\pi}{2^{2n}}$$ However I checked the answers that I have and it is the same apart from on the right hand side there is no $\dfrac{1}{2^{2n}}$, I was wondering what I had done wrong? Thanks very much for any help","I have a question that asks me to find the value of $\displaystyle\frac{1}{2\pi}\int_{0}^{2\pi} \cos^{2n} x dx$  $\ $ by considering the integral $$ \displaystyle \oint_{\gamma} \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}dz$$ where $\gamma$ is the circle of radius 1 centered at the origin. So I expanded the integrand out using the binomial theorem to get the following: $$\displaystyle \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}=\sum \binom{2n}{k} z^{2n-2k-1}$$ so the integral of each of these terms is 0 apart from when $k=n$ and we get $\int_{\gamma}z^{-1} dz=2\pi{i}$ and so we have that: $$ \displaystyle \oint_{\gamma} \frac{1}{z}\left ( z+\frac{1}{z} \right ) ^{2n}dz=\binom{2n}{n} 2\pi{i}$$ If we now let $z=e^{it}$ on $\gamma$ then the integral becomes: $$\displaystyle \int_{0}^{2\pi} \frac{ie^{it}}{e^{it}} \left ( e^{it}+e^{-it} \right )^{2n}dt=  {i}\int_{0}^{2\pi} \left ( e^{it}+e^{-it} \right )^{2n}dt={i}\int_{0}^{2\pi} \left ( 2{\cos(t)} \right ) ^{2n}dt=\binom{2n}{n} 2\pi{i}$$ and finally: $$\displaystyle\int_{0}^{2\pi} \left ( \cos(t) \right ) ^{2n}dt=\binom{2n}{n} \frac{2\pi}{2^{2n}}$$ However I checked the answers that I have and it is the same apart from on the right hand side there is no $\dfrac{1}{2^{2n}}$, I was wondering what I had done wrong? Thanks very much for any help",,"['complex-analysis', 'contour-integration']"
8,Discrepancy in the Application of the Identity $\sum_{n=-\infty}^\infty f(n) = -\sum_{j=1}^l \operatorname{Res}(g;a_j)$,Discrepancy in the Application of the Identity,\sum_{n=-\infty}^\infty f(n) = -\sum_{j=1}^l \operatorname{Res}(g;a_j),"The theorem in its entirety is as follows: Let $a_1,\ldots,a_l\in\mathbb{C}$ be pairwise different non-integral numbers.  Let f be an analytic function in $\mathbb{C}-\{a_1,\ldots,a_l\}$ and set $g(z):=\pi \cot(\pi z)f(z)$, such that $|z^2f(z)|$ is bounded outside a suitable compact set.  Then: $$\sum_{n=-\infty}^\infty f(n) = -\sum_{j=1}^l \operatorname{Res}(g;a_j)$$ The book wants me to use this theorem to prove that $\sum_{n=1}^\infty \frac{1}{n^{2}} = \frac{\pi^2}{6}$.  Everything points to me setting $f(z)=\frac{1}{z^2}$, but the pole of $\frac{1}{z^2}$ is zero which is an integral number and is thus a point at which f must be analytic, thus the theorem cannot be applied, what am I missing here?  Thanks. Edit: I guess I'm suppose to slightly modify the theorem so it works for an overlapping pole, I probably don't need clarification on this after all.","The theorem in its entirety is as follows: Let $a_1,\ldots,a_l\in\mathbb{C}$ be pairwise different non-integral numbers.  Let f be an analytic function in $\mathbb{C}-\{a_1,\ldots,a_l\}$ and set $g(z):=\pi \cot(\pi z)f(z)$, such that $|z^2f(z)|$ is bounded outside a suitable compact set.  Then: $$\sum_{n=-\infty}^\infty f(n) = -\sum_{j=1}^l \operatorname{Res}(g;a_j)$$ The book wants me to use this theorem to prove that $\sum_{n=1}^\infty \frac{1}{n^{2}} = \frac{\pi^2}{6}$.  Everything points to me setting $f(z)=\frac{1}{z^2}$, but the pole of $\frac{1}{z^2}$ is zero which is an integral number and is thus a point at which f must be analytic, thus the theorem cannot be applied, what am I missing here?  Thanks. Edit: I guess I'm suppose to slightly modify the theorem so it works for an overlapping pole, I probably don't need clarification on this after all.",,['complex-analysis']
9,Infinite product formula for a complex function,Infinite product formula for a complex function,,"Construct a function with zero at $z=0$ and zeros at $z=-n$ with multiplicities $n$. My answer is $$f(z) = z\prod_{n=1}^{\infty}\left[E_n\left(-\frac zn\right)\right]^n,$$ where $E_n(z)=(1-z)\exp\left(\sum_{k=1}^n\frac{z^k}k\right)$. Is that right? And does the product converge?","Construct a function with zero at $z=0$ and zeros at $z=-n$ with multiplicities $n$. My answer is $$f(z) = z\prod_{n=1}^{\infty}\left[E_n\left(-\frac zn\right)\right]^n,$$ where $E_n(z)=(1-z)\exp\left(\sum_{k=1}^n\frac{z^k}k\right)$. Is that right? And does the product converge?",,['complex-analysis']
10,Local normalization of algebraic curves,Local normalization of algebraic curves,,"I am currently reading about the normalization theorem: Suppose $C$ is an irreducible plane algebraic curve, and let S be the set of singular points. Then there exists a compact Riemann surface $\hat C$ and a holomorphic map from the surface to $C$ such that the image of $\hat C$ is $C$, it's injective on the set of singular points, and the inverse image of the singular points is finite. To prove this, we locally normalize each singular point by factoring it into irreducible local analytic curve components (using Weierstrass polynomials). Then for every curve component, we get can get a holomorphic, injective map from a disk at the origin of the complex plane onto the component, that is biholomorphic if we remove the origin from the disk and the origin from the curve component (where we have placed the singular point at the origin). Gluing all of these disks to the curve gives the normalization. My question is: why do these curve components correspond exactly to the structure of the curve? The theorem just says that it factors and that a normalization is possible. Specifically: 1) If we have an ordinary double point, intuitively, it should factor into two analytic curve components. Why is this? 2) Why does the curve factor into a single irreducible component at smooth points? The precise details can be found in ""Introduction to Algebraic Curves"" by Griffiths.","I am currently reading about the normalization theorem: Suppose $C$ is an irreducible plane algebraic curve, and let S be the set of singular points. Then there exists a compact Riemann surface $\hat C$ and a holomorphic map from the surface to $C$ such that the image of $\hat C$ is $C$, it's injective on the set of singular points, and the inverse image of the singular points is finite. To prove this, we locally normalize each singular point by factoring it into irreducible local analytic curve components (using Weierstrass polynomials). Then for every curve component, we get can get a holomorphic, injective map from a disk at the origin of the complex plane onto the component, that is biholomorphic if we remove the origin from the disk and the origin from the curve component (where we have placed the singular point at the origin). Gluing all of these disks to the curve gives the normalization. My question is: why do these curve components correspond exactly to the structure of the curve? The theorem just says that it factors and that a normalization is possible. Specifically: 1) If we have an ordinary double point, intuitively, it should factor into two analytic curve components. Why is this? 2) Why does the curve factor into a single irreducible component at smooth points? The precise details can be found in ""Introduction to Algebraic Curves"" by Griffiths.",,"['complex-analysis', 'manifolds', 'riemann-surfaces']"
11,Showing this integral from complex analysis is an integer without residues,Showing this integral from complex analysis is an integer without residues,,"I've already proven that the winding number is an integer, now I want to show that, given the following assumptions: The function $f$ is holomorphic on the domain $D$ $\gamma$ is a piecewise-smooth, closed curve in $D$ $f$ does not vanish on $\gamma$ It's true that $$ \frac{1}{2\pi i}\int_{\gamma}\frac{f '(\xi)}{f(\xi)}d\xi \in \mathbb{Z}.$$ Here's what I'm thinking, although it's not at all a rigorous proof: $$\frac{1}{2\pi i}\int_{\gamma}\frac{f '(\xi)}{f(\xi)}d\xi=\frac{1}{2\pi i}\int_{\gamma}\frac{d}{d\xi}\log_{R}(f(\xi))d\xi=\frac{1}{2\pi i}\{\log_{R}(f(\xi_1))-\log_{R}(f(\xi_2))\}$$ where $\log_{R}$ is the logarithm function defined on the riemann surface from the wikipedia page on complex logarithms wiki page and $\xi_1,\xi_2$ are two points which are the same when considered as points of $\mathbb{C}$ (because $\gamma$ is closed) but which have different arguments, differing by  $k2\pi i$ for $k \in \mathbb{Z}$, when considered as points on the Riemann surface $R$. I think you probably get that idea that I've got. IT could be totally off, but this is what I was led to in thinking about it. Trouble is, we haven't really discussed this Riemann surface, so I doubt it's what is expected. Any comments or suggestions?","I've already proven that the winding number is an integer, now I want to show that, given the following assumptions: The function $f$ is holomorphic on the domain $D$ $\gamma$ is a piecewise-smooth, closed curve in $D$ $f$ does not vanish on $\gamma$ It's true that $$ \frac{1}{2\pi i}\int_{\gamma}\frac{f '(\xi)}{f(\xi)}d\xi \in \mathbb{Z}.$$ Here's what I'm thinking, although it's not at all a rigorous proof: $$\frac{1}{2\pi i}\int_{\gamma}\frac{f '(\xi)}{f(\xi)}d\xi=\frac{1}{2\pi i}\int_{\gamma}\frac{d}{d\xi}\log_{R}(f(\xi))d\xi=\frac{1}{2\pi i}\{\log_{R}(f(\xi_1))-\log_{R}(f(\xi_2))\}$$ where $\log_{R}$ is the logarithm function defined on the riemann surface from the wikipedia page on complex logarithms wiki page and $\xi_1,\xi_2$ are two points which are the same when considered as points of $\mathbb{C}$ (because $\gamma$ is closed) but which have different arguments, differing by  $k2\pi i$ for $k \in \mathbb{Z}$, when considered as points on the Riemann surface $R$. I think you probably get that idea that I've got. IT could be totally off, but this is what I was led to in thinking about it. Trouble is, we haven't really discussed this Riemann surface, so I doubt it's what is expected. Any comments or suggestions?",,['complex-analysis']
12,Find number of roots in some area (Rouché's theorem),Find number of roots in some area (Rouché's theorem),,"The task is to find number of $ {z^4} + {z^3} - 4z + 1 = 0$ in the area  $1 < \left| z \right| < 2$.  (this task is in the Rouché's theorem paragraph) I used this theorem many times, but I don't know to solve this task. This is simple to find number of roots in the area $0 < \left| z \right| < 1$, but I don't know how to do the same in another area: $0 < \left| z \right| < 2$. Of course, I now number of roots with the help of Wolfram Alpha, for example. Could you help, please, how to solve this task with the Rouché's theorem?","The task is to find number of $ {z^4} + {z^3} - 4z + 1 = 0$ in the area  $1 < \left| z \right| < 2$.  (this task is in the Rouché's theorem paragraph) I used this theorem many times, but I don't know to solve this task. This is simple to find number of roots in the area $0 < \left| z \right| < 1$, but I don't know how to do the same in another area: $0 < \left| z \right| < 2$. Of course, I now number of roots with the help of Wolfram Alpha, for example. Could you help, please, how to solve this task with the Rouché's theorem?",,"['complex-analysis', 'polynomials', 'inequality']"
13,coloring the inside point for Julia Fractal,coloring the inside point for Julia Fractal,,"I am trying to continuous coloring the inside point for a fractal image,such as $z \to z^2+C$. For those outside point, we can use the escape iteration to determine the color, just as the description in the Wikipedia . However, how to coloring those inside point, which wouldn't escape? I mean use some good color. Has anyone done it before and can give some suggestions? Best Regards,","I am trying to continuous coloring the inside point for a fractal image,such as $z \to z^2+C$. For those outside point, we can use the escape iteration to determine the color, just as the description in the Wikipedia . However, how to coloring those inside point, which wouldn't escape? I mean use some good color. Has anyone done it before and can give some suggestions? Best Regards,",,"['complex-analysis', 'dynamical-systems', 'fractals']"
14,Consequence of Riemann mapping theorem,Consequence of Riemann mapping theorem,,Let $\mathbb{H}$ denote the complex upper half plane (not including the real axis). Let $A$ be a bounded subset with $A = \mathbb{H} \cap \overline{A}$ and $\mathbb{H} \backslash A$ simply connected. Why does it follow from the Riemann mapping theorem that there are conformal maps $g: \mathbb{H} \backslash A \rightarrow \mathbb{H}$ with $|g(z)| \rightarrow \infty$ as $z \rightarrow \infty$?,Let $\mathbb{H}$ denote the complex upper half plane (not including the real axis). Let $A$ be a bounded subset with $A = \mathbb{H} \cap \overline{A}$ and $\mathbb{H} \backslash A$ simply connected. Why does it follow from the Riemann mapping theorem that there are conformal maps $g: \mathbb{H} \backslash A \rightarrow \mathbb{H}$ with $|g(z)| \rightarrow \infty$ as $z \rightarrow \infty$?,,['complex-analysis']
15,Subrings of formal series rings,Subrings of formal series rings,,"Let $k$ be a field and $A = k[[x_1, \dots, x_n ]]$ be the ring of formal series in $n$ variables. Consider $g_1, \dots, g_m \in A$ such that $g_1(0) = \cdots = g_m(0) = 0$. For every $f \in k[[t_1, \dots, t_m]]$ we can consider the formal series $f(g_1, \dots, g_m) \in A$ because the $g_i$'s have no constant term. Now consider the subring $B$ of $A$ made up of series of the form $f(g_1, \dots, g_m)$ as $f$ varies in $k[[t_1, \dots,t_m]]$. It is clear that $B$ is a noetherian local domain with $\dim B \leq m$. (1) What are the main properties of the ring extension $B \subseteq A$? When is $A$ flat over $B$? (2) After reading Problem 3 at page 115 of Milnor's Singular points of complex hypersurfaces , I have made the following conjecture that is an algebraization of the book's problem. Conjecture. If $m = n$ and $\sqrt{A g_1 + \cdots + A g_n } = A x_1 + \cdots + A x_n$, then $A$ is a finite free $B$-module of rank $\mu$ and $\dim_k A/(g_1, \dots, g_n) = \mu$. May someone prove or disprove this conjecture or tell me a good reference for complete noetherian rings?","Let $k$ be a field and $A = k[[x_1, \dots, x_n ]]$ be the ring of formal series in $n$ variables. Consider $g_1, \dots, g_m \in A$ such that $g_1(0) = \cdots = g_m(0) = 0$. For every $f \in k[[t_1, \dots, t_m]]$ we can consider the formal series $f(g_1, \dots, g_m) \in A$ because the $g_i$'s have no constant term. Now consider the subring $B$ of $A$ made up of series of the form $f(g_1, \dots, g_m)$ as $f$ varies in $k[[t_1, \dots,t_m]]$. It is clear that $B$ is a noetherian local domain with $\dim B \leq m$. (1) What are the main properties of the ring extension $B \subseteq A$? When is $A$ flat over $B$? (2) After reading Problem 3 at page 115 of Milnor's Singular points of complex hypersurfaces , I have made the following conjecture that is an algebraization of the book's problem. Conjecture. If $m = n$ and $\sqrt{A g_1 + \cdots + A g_n } = A x_1 + \cdots + A x_n$, then $A$ is a finite free $B$-module of rank $\mu$ and $\dim_k A/(g_1, \dots, g_n) = \mu$. May someone prove or disprove this conjecture or tell me a good reference for complete noetherian rings?",,"['abstract-algebra', 'complex-analysis', 'algebraic-geometry', 'commutative-algebra', 'ring-theory']"
16,Function Elements in Weyl's Riemann Surface Text,Function Elements in Weyl's Riemann Surface Text,,"Motivation and Background: I'm reading Weyl's text The Concept of a Riemann Surface and I'm having a bit of difficulty.  (I can't find an online version which is not a google book, so if the following terms are non-standard, could someone point me to the more standard terms?)  I will italicize relevant terms for easy reading. He begins by noting that a function element is a power series at some point that converges in some disk of positive radius, and that an analytic function is the collection of all of the function elements gained by analytic continuation.  So far, so good.  He then notes we should further generalize our function elements so that we can include things like poles and branch cuts, and eventually we will get to the concept of an analytic form . Following this, he notes that we can take a power series $\sum_{i=0}^{\infty} a_{i}(z-a)^{i}$ and introduce some $t$ such that $z = a + t$ so that we now have $\sum_{i=0}^{\infty} a_{i}t^{i}$.  He then notes, ""If we abandon the distinguished role played by $z$ and also allow a finite number of negative powers of $t$, we obtain a more general formulation.""  He then lets $z = P(t)$ and $u = Q(t)$ be any two series with only a finite number of negative powers of $t$ which, for a sufficiently small neighborhood of the origin, both converge and no two different values of $t$ in this neighborhood give the same pair of values $(z,u)$.  This pair now defines a function element. Main Question: I do not see how these two series can represent a function element as we defined before.  I also cannot see the significance of having no two different values of $t$ giving the same pair $(z,u)$.  I'm not exactly sure what makes this definition more general than the previous one, and I'm not exactly sure why we cannot introduce branch cuts by using the same analytic continuation techniques as before.  I'm also not sure why replacing ""$z-a$"" with $t$ allows us to ""abandon the distinguished role played by $z$.* If someone could lead me in the right direction, I'd appreciate it!","Motivation and Background: I'm reading Weyl's text The Concept of a Riemann Surface and I'm having a bit of difficulty.  (I can't find an online version which is not a google book, so if the following terms are non-standard, could someone point me to the more standard terms?)  I will italicize relevant terms for easy reading. He begins by noting that a function element is a power series at some point that converges in some disk of positive radius, and that an analytic function is the collection of all of the function elements gained by analytic continuation.  So far, so good.  He then notes we should further generalize our function elements so that we can include things like poles and branch cuts, and eventually we will get to the concept of an analytic form . Following this, he notes that we can take a power series $\sum_{i=0}^{\infty} a_{i}(z-a)^{i}$ and introduce some $t$ such that $z = a + t$ so that we now have $\sum_{i=0}^{\infty} a_{i}t^{i}$.  He then notes, ""If we abandon the distinguished role played by $z$ and also allow a finite number of negative powers of $t$, we obtain a more general formulation.""  He then lets $z = P(t)$ and $u = Q(t)$ be any two series with only a finite number of negative powers of $t$ which, for a sufficiently small neighborhood of the origin, both converge and no two different values of $t$ in this neighborhood give the same pair of values $(z,u)$.  This pair now defines a function element. Main Question: I do not see how these two series can represent a function element as we defined before.  I also cannot see the significance of having no two different values of $t$ giving the same pair $(z,u)$.  I'm not exactly sure what makes this definition more general than the previous one, and I'm not exactly sure why we cannot introduce branch cuts by using the same analytic continuation techniques as before.  I'm also not sure why replacing ""$z-a$"" with $t$ allows us to ""abandon the distinguished role played by $z$.* If someone could lead me in the right direction, I'd appreciate it!",,['complex-analysis']
17,A Question on Schlicht Functions,A Question on Schlicht Functions,,"Denote the class of Schlicht functions (injective, holomorphic on the unit disk, with $f(0)=0$ and $f'(0)=1$) by $\mathcal{S}$.  And let $\gamma :[0,\infty )\rightarrow \mathbb{C}$ be a simple curve (continuous and injective) and such that $\gamma (0)=0$ and $\lim _{t\to \infty}\gamma (t)=\infty$. I have to show that, for each such $\gamma$, there is a unique $t>0$ so that $\mathbb{C} \backslash \gamma \left( [t,\infty )\right)$ is the image of some $f\in \mathcal{S}$. Here's my idea so far: For each $t$, via the Riemann Maping Theorem, I can construct a bijective, holomorphic function $f_t:\mathbb{C} \backslash \gamma \left( [t,\infty )\right) \rightarrow \mathbb{D}$ ($\mathbb{D}$ is the unit disk) such that $f_t(0)=0$.  Furthermore, such an $f_t$ is unique up to a choice of the argument of $f_t'(0)$.  If I could somehow show that there must be some $t>0$ such that $\left| f_t'(0)\right| =1$ I would be done, but I am stuck as how to do this. Anyone have any ideas?  In the same direction or not, all suggestions are welcome.","Denote the class of Schlicht functions (injective, holomorphic on the unit disk, with $f(0)=0$ and $f'(0)=1$) by $\mathcal{S}$.  And let $\gamma :[0,\infty )\rightarrow \mathbb{C}$ be a simple curve (continuous and injective) and such that $\gamma (0)=0$ and $\lim _{t\to \infty}\gamma (t)=\infty$. I have to show that, for each such $\gamma$, there is a unique $t>0$ so that $\mathbb{C} \backslash \gamma \left( [t,\infty )\right)$ is the image of some $f\in \mathcal{S}$. Here's my idea so far: For each $t$, via the Riemann Maping Theorem, I can construct a bijective, holomorphic function $f_t:\mathbb{C} \backslash \gamma \left( [t,\infty )\right) \rightarrow \mathbb{D}$ ($\mathbb{D}$ is the unit disk) such that $f_t(0)=0$.  Furthermore, such an $f_t$ is unique up to a choice of the argument of $f_t'(0)$.  If I could somehow show that there must be some $t>0$ such that $\left| f_t'(0)\right| =1$ I would be done, but I am stuck as how to do this. Anyone have any ideas?  In the same direction or not, all suggestions are welcome.",,['complex-analysis']
18,ODE textbook with links to Complex analysis,ODE textbook with links to Complex analysis,,"I'm currently working with singular second order differential equations and I'm finding that the standard ODE textbooks available to me aren't very helpful. Most give rote definitions of ordinary/regular points of ODEs, and offer up only the Frobenius method for regular singular points(I'm working with singular, complex ODEs). Some sources may mention complex analysis/branch cuts. Some may deal with singular ODEs. Some may discuss things geometrically. Nothing so far has mentioned Riemann Surfaces. It's all very disparate, and I find myself lacking a clear understanding of ODEs in the complex plain (and by extension, real valued ODEs as well). What I'm looking for is an ODE textbook which gives a comprehensive treatment of ODEs in the complex plain, in particular one which discusses second order complex ODEs and their (3 (2?)) singular points. I would also like one which gives a more geometric treatment. Basically, I need a textbook which goes beyond series solutions and gives a general theory of complex (2nd order) ODEs. Can anyone make any recommendations?","I'm currently working with singular second order differential equations and I'm finding that the standard ODE textbooks available to me aren't very helpful. Most give rote definitions of ordinary/regular points of ODEs, and offer up only the Frobenius method for regular singular points(I'm working with singular, complex ODEs). Some sources may mention complex analysis/branch cuts. Some may deal with singular ODEs. Some may discuss things geometrically. Nothing so far has mentioned Riemann Surfaces. It's all very disparate, and I find myself lacking a clear understanding of ODEs in the complex plain (and by extension, real valued ODEs as well). What I'm looking for is an ODE textbook which gives a comprehensive treatment of ODEs in the complex plain, in particular one which discusses second order complex ODEs and their (3 (2?)) singular points. I would also like one which gives a more geometric treatment. Basically, I need a textbook which goes beyond series solutions and gives a general theory of complex (2nd order) ODEs. Can anyone make any recommendations?",,"['complex-analysis', 'ordinary-differential-equations', 'reference-request']"
19,Inequality involving log and e,Inequality involving log and e,,"I need to show that $\displaystyle{\int_{0}^{1} \frac{dx}{|1-e^{2\pi i\tau}|}} \ll -\log y$ where $\tau = x + iy$ and  $0 < y < \frac{1}{10}$. I began showing it by using the lower estimate of triangle inequality, i.e.,  $$ \frac{1}{|1-e^{2\pi i\tau}|} \leq \frac{1}{|1-e^{-2\pi y}|} \> . $$ Then, as the integration is with respect to $x$, it seems that I need to show that the latter is far less than $\log(\frac{1}{y})$. Am I doing it right? How do I proceed from here?","I need to show that $\displaystyle{\int_{0}^{1} \frac{dx}{|1-e^{2\pi i\tau}|}} \ll -\log y$ where $\tau = x + iy$ and  $0 < y < \frac{1}{10}$. I began showing it by using the lower estimate of triangle inequality, i.e.,  $$ \frac{1}{|1-e^{2\pi i\tau}|} \leq \frac{1}{|1-e^{-2\pi y}|} \> . $$ Then, as the integration is with respect to $x$, it seems that I need to show that the latter is far less than $\log(\frac{1}{y})$. Am I doing it right? How do I proceed from here?",,['complex-analysis']
20,Extend an holomorphic function defined on a torus,Extend an holomorphic function defined on a torus,,"Suppose we have an holomorphic function  $$ f : \frac{\mathbb{C}}{\Lambda} \mapsto \frac{\mathbb{C}}{\Lambda}  $$ where $\Lambda$ is a lattice. Is it always possible to find another function $\psi : \mathbb{C} \mapsto \mathbb{C}$ which extend $f$, i.e. such that $$ \forall x \in \mathbb{C}, \, \overline{\psi(x)} = f(\overline x) $$ or, at least, which extend $f$ on a neighbourhood of each point. I met this issue when I was reading "" rational points on elliptic curves "" of Silverman & Tate.  To give the context, we have an elliptic curve $C(\mathbb C)$ and an endomorphism of $C(\mathbb C)$, and because we know there exists an isomorphism $ \frac{\mathbb C}{\Lambda} \rightarrow C(\mathbb C)$ for a lattice $\Lambda$ (using Weierstrass $\wp$ function), we finally find an ""holomorphic"" (I don't really know what holomorphic on $\frac{\mathbb C}{\Lambda}$ mean) function $f$, and we use a function like $\psi$ and complex analysis to discover that  $$ f : z \in \frac{\mathbb C}{\Lambda} \mapsto cz $$ with $c \in \mathbb C$. (Yes, it explain the name ""complex multiplication"") You can find an article which explain this part of the book at : CMpaper","Suppose we have an holomorphic function  $$ f : \frac{\mathbb{C}}{\Lambda} \mapsto \frac{\mathbb{C}}{\Lambda}  $$ where $\Lambda$ is a lattice. Is it always possible to find another function $\psi : \mathbb{C} \mapsto \mathbb{C}$ which extend $f$, i.e. such that $$ \forall x \in \mathbb{C}, \, \overline{\psi(x)} = f(\overline x) $$ or, at least, which extend $f$ on a neighbourhood of each point. I met this issue when I was reading "" rational points on elliptic curves "" of Silverman & Tate.  To give the context, we have an elliptic curve $C(\mathbb C)$ and an endomorphism of $C(\mathbb C)$, and because we know there exists an isomorphism $ \frac{\mathbb C}{\Lambda} \rightarrow C(\mathbb C)$ for a lattice $\Lambda$ (using Weierstrass $\wp$ function), we finally find an ""holomorphic"" (I don't really know what holomorphic on $\frac{\mathbb C}{\Lambda}$ mean) function $f$, and we use a function like $\psi$ and complex analysis to discover that  $$ f : z \in \frac{\mathbb C}{\Lambda} \mapsto cz $$ with $c \in \mathbb C$. (Yes, it explain the name ""complex multiplication"") You can find an article which explain this part of the book at : CMpaper",,"['number-theory', 'complex-analysis', 'elliptic-curves']"
21,Cauchy product of zeta function,Cauchy product of zeta function,,"$\zeta(s)= \sum_{n=1}^{\infty} \frac{1}{n^s}; s \in \mathbb{C}$ . For $Re(s) > 1 $ , we have that the above series converges absolutely. And in this case, I wrote the Cauchy product of $\zeta(s)$ by itself as: \begin{align*} \sum_{n=1}^{\infty} \sum_{k=1}^{n}\frac{1}{k^s}\frac{1}{(n+1-k)^s} \end{align*} But now I can't seem to find a way to arrive at the result $$ \sum_{n=1}^{\infty}\frac{d(n)}{n^s}$$ where $d(n)$ is the number of divisors of $n$ . Any suggestions?",". For , we have that the above series converges absolutely. And in this case, I wrote the Cauchy product of by itself as: But now I can't seem to find a way to arrive at the result where is the number of divisors of . Any suggestions?","\zeta(s)= \sum_{n=1}^{\infty} \frac{1}{n^s}; s \in \mathbb{C} Re(s) > 1  \zeta(s) \begin{align*}
\sum_{n=1}^{\infty} \sum_{k=1}^{n}\frac{1}{k^s}\frac{1}{(n+1-k)^s}
\end{align*}  \sum_{n=1}^{\infty}\frac{d(n)}{n^s} d(n) n","['sequences-and-series', 'complex-analysis', 'riemann-zeta', 'cauchy-product']"
22,Differentiability of function $f:\mathbb{C} \to \mathbb{C}; f(z)=z\exp(\bar{z})$,Differentiability of function,f:\mathbb{C} \to \mathbb{C}; f(z)=z\exp(\bar{z}),"Let $f: \mathbb{C} \to \mathbb{C}$ be defined by $f(z)=z\exp(\bar{z})$ . Identify the points where $f$ has complex derivative. At $z=0$ : $$ \lim_{z \to 0} \frac{f(z)-f(0)}{z-0}  = \lim_{z \to 0} \frac{z\exp(\bar{z})}{z}  = \lim_{z \to 0} \exp(\bar{z}) = 1 $$ So, $f$ is differentiable at $0$ . But for $\mathbb{C}^{\ast}$ with the prior knowledge that $\exp(\bar{z})$ is not differentiable anywhere, $f$ is not complex-differentiable at any non-zero complex number, because that'd imply $$ \exp(\bar{z}) = \frac{z\exp(\bar{z})}{z} $$ would be complex-differentiable as it'd be the product of complex-differentiable functions on $\mathbb{C}^{\ast}$ . So $f$ is only complex-differentiable at $0$ . Is this argument correct?","Let be defined by . Identify the points where has complex derivative. At : So, is differentiable at . But for with the prior knowledge that is not differentiable anywhere, is not complex-differentiable at any non-zero complex number, because that'd imply would be complex-differentiable as it'd be the product of complex-differentiable functions on . So is only complex-differentiable at . Is this argument correct?","f: \mathbb{C} \to \mathbb{C} f(z)=z\exp(\bar{z}) f z=0 
\lim_{z \to 0} \frac{f(z)-f(0)}{z-0} 
= \lim_{z \to 0} \frac{z\exp(\bar{z})}{z} 
= \lim_{z \to 0} \exp(\bar{z}) = 1
 f 0 \mathbb{C}^{\ast} \exp(\bar{z}) f 
\exp(\bar{z}) = \frac{z\exp(\bar{z})}{z}
 \mathbb{C}^{\ast} f 0",['complex-analysis']
23,Applying Whittaker's and Watson's Lagrange theorem to $\sin$,Applying Whittaker's and Watson's Lagrange theorem to,\sin,"I want to use the Lagrange inversion theorem from Whittaker and Watson , p. 133, to find the power series of $\sin^{-1}$ at the origin. It should be a valid procedure because $\sin^{-1}$ is analytic at the origin. The theorem is stated as follows: Let $f(z)$ and $\phi (z)$ be functions of $z$ analytic on and inside a contour $C$ surrounding a point $a$ , and let $t$ be such that the inequality $$|t\phi (z)|\lt |z-a|$$ is satisfied at all points $z$ on the perimeter of $C$ ; then the equation $$\zeta=a+t\phi (\zeta),$$ regarded as an equation in $\zeta$ , has one root in the interior of $C$ ; and further any function of $\zeta$ analytic on and inside $C$ can be expanded as a power series in $t$ by the formula $$f(\zeta)=f(a)+\displaystyle\sum_{n=1}^\infty \dfrac{t^n}{n!}\dfrac{d^{n-1}}{da^{n-1}}[f'(a)\{\phi (a)\}^n].$$ First, I transform that theorem to a more tractable form for my problem. We can obviously take $f=\operatorname{id}$ . Let $$\phi (z)=\dfrac{z-a}{g(z)-g(a)}.$$ Then $$\zeta =a+t\dfrac{\zeta-a}{g(\zeta)-g(a)}$$ which gives $$t=g(\zeta)-g(a).$$ So far, we have $$\zeta=a+\displaystyle\sum_{n=1}^\infty \dfrac{(g(\zeta)-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n.$$ Then let $\zeta=g^{-1}(z)$ , so finally $$g^{-1}(z)=a+\displaystyle\sum_{n=1}^\infty \dfrac{(z-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n.$$ We have to find $z$ such that $$|t \phi(z)|\lt |z-a|$$ for all $z$ on the perimeter of $C$ ; i.e. $$\left|(g(\zeta)-g(a))\dfrac{z-a}{g(z)-g(a)}\right|\lt |z-a|,$$ i.e. $$|g(\zeta)-g(a)|\lt |g(z)-g(a)|$$ which, after using $\zeta=g^{-1}(z)$ and $a=0$ and $\sin 0=0$ simplifies to $$|z|\lt |g(z)|$$ but there exists no contour $C$ surrounding the origin such that for all points $z$ on its perimeter we have $|z|\lt |\sin z|$ . What should I do?","I want to use the Lagrange inversion theorem from Whittaker and Watson , p. 133, to find the power series of at the origin. It should be a valid procedure because is analytic at the origin. The theorem is stated as follows: Let and be functions of analytic on and inside a contour surrounding a point , and let be such that the inequality is satisfied at all points on the perimeter of ; then the equation regarded as an equation in , has one root in the interior of ; and further any function of analytic on and inside can be expanded as a power series in by the formula First, I transform that theorem to a more tractable form for my problem. We can obviously take . Let Then which gives So far, we have Then let , so finally We have to find such that for all on the perimeter of ; i.e. i.e. which, after using and and simplifies to but there exists no contour surrounding the origin such that for all points on its perimeter we have . What should I do?","\sin^{-1} \sin^{-1} f(z) \phi (z) z C a t |t\phi (z)|\lt |z-a| z C \zeta=a+t\phi (\zeta), \zeta C \zeta C t f(\zeta)=f(a)+\displaystyle\sum_{n=1}^\infty \dfrac{t^n}{n!}\dfrac{d^{n-1}}{da^{n-1}}[f'(a)\{\phi (a)\}^n]. f=\operatorname{id} \phi (z)=\dfrac{z-a}{g(z)-g(a)}. \zeta =a+t\dfrac{\zeta-a}{g(\zeta)-g(a)} t=g(\zeta)-g(a). \zeta=a+\displaystyle\sum_{n=1}^\infty \dfrac{(g(\zeta)-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n. \zeta=g^{-1}(z) g^{-1}(z)=a+\displaystyle\sum_{n=1}^\infty \dfrac{(z-g(a))^n}{n!}\displaystyle\lim_{z\to a}\dfrac{d^{n-1}}{da^{n-1}} \left(\dfrac{z-a}{g(z)-g(a)}\right)^n. z |t \phi(z)|\lt |z-a| z C \left|(g(\zeta)-g(a))\dfrac{z-a}{g(z)-g(a)}\right|\lt |z-a|, |g(\zeta)-g(a)|\lt |g(z)-g(a)| \zeta=g^{-1}(z) a=0 \sin 0=0 |z|\lt |g(z)| C z |z|\lt |\sin z|","['sequences-and-series', 'complex-analysis', 'inequality']"
24,Can we write every $C^1$ complex function on the unit circle as the the difference of two approriate functions?,Can we write every  complex function on the unit circle as the the difference of two approriate functions?,C^1,"If $g$ is $C^1$ on the unit circle $C(0,1)$ . Then there is a function $f^+$ holomorphic on $B(0,1)$ and continuous on $\bar B(0,1)$ , a function $f^-$ holomorphic on $\mathbb{C}\backslash\bar B(0,1)$ and continuous on $\mathbb{C}\backslash B(0,1)$ , such that $g(z)=f^+(z)-f^-(z)$ on the unit circle. This seems like it could be a direct consequence of some of the lecture results (e.g. Cauchy's Integral Formula, Taylor series), but cauchy's formula just gives a holomorphic function on $B(0,1)$ , I do not know how that relates to the value of $g(z)$ . While Taylor series only applies for functions holomorphic on some domain. If we can extend $g$ to some holomorphic function on $\mathbb{C}$ then surely the result follows... But how do we do something like that? Any help will be appreciated.","If is on the unit circle . Then there is a function holomorphic on and continuous on , a function holomorphic on and continuous on , such that on the unit circle. This seems like it could be a direct consequence of some of the lecture results (e.g. Cauchy's Integral Formula, Taylor series), but cauchy's formula just gives a holomorphic function on , I do not know how that relates to the value of . While Taylor series only applies for functions holomorphic on some domain. If we can extend to some holomorphic function on then surely the result follows... But how do we do something like that? Any help will be appreciated.","g C^1 C(0,1) f^+ B(0,1) \bar B(0,1) f^- \mathbb{C}\backslash\bar B(0,1) \mathbb{C}\backslash B(0,1) g(z)=f^+(z)-f^-(z) B(0,1) g(z) g \mathbb{C}","['complex-analysis', 'analytic-functions']"
25,Hadamard products and sums of reciprocals of solutions to $x=\tan x$,Hadamard products and sums of reciprocals of solutions to,x=\tan x,"There are infinitely many real solutions to the equation $\tan x=x$ . Denote the increasing sequence of positive solutions by $ (\lambda_n)_{n=1}^{\infty}$ . I want to evaluate the sum $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} \hspace{1cm} (1) $$ (the series can be seen to converge by using a crude approximation such as $\lambda_n \approx \frac{\pi(2n+1)}{2}$ ). To find the sum I'm thinking to use Hadamard's product theorem applied to a function whose roots are $\lambda_n$ and then to expand the product and compare coefficients as in Euler's evaluation of $\zeta(2)$ . The Hadamard product theorem states that any entire function $f(z)$ of growth order $\alpha<2$ can be written as $$ f(z) = z^m e^{Az+B} \prod_{\rho} e^{z/\rho} \left(1-\frac{z}{\rho} \right) $$ where the product is over all the non-zero complex roots $\rho$ of $f$ repeated according to multiplicity, $m \geq 0$ is the order of the root of $f$ at $z=0$ , and $A,B$ are two constants to be determined. Now $ \tan z-z $ is not entire but it has the same non-zero roots as the function $ f(z) = \sin z - z \cos z$ which is entire. These are all real roots of order 1 and they are symmetric since the function is odd. Thus the non-zero roots of $f(z)$ are at $\pm \lambda_n$ and there is also a triple root at $z=0$ . Further, the function has growth order 1, so applying Hadamard's product formula gives $$ \sin z-z \cos z = e^{Az+B} z^3 \prod_{n=1}^{\infty} e^{z/\lambda_n} \left(1-\frac{z}{\lambda_n} \right) e^{z/(-\lambda_n)} \left(1-\frac{z}{(-\lambda_n)} \right) = e^{Az+B} z^3 \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right) $$ and we may compute $$e^B = \lim_{z \to 0} \frac{\sin z-z\cos z}{z^3} = \frac{1}{3} $$ and $A=0$ since $\frac{\sin z-z\cos z}{z^3}$ is an even function. Therefore $$ \sin z-z \cos z = \frac{z^3}{3} \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right) \hspace{1cm} (2) $$ We may now expand the left side in a Taylor series and compare coefficients with the right side to obtain $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} = \frac{1}{10} $$ as well as higher order sums such as $$ \sum_{n=1}^{\infty} \frac{1}{\lambda_n^4} = \frac{1}{350} $$ and so on. All sums of reciprocals of even powers will be rational numbers. My questions are: Is this solution correct? Are there other ways of evaluating the series $(1)$ without using the Hadamard product? Are there any general techniques (contour integration for example) available for evaluating sums of the form $$ \sum_{\rho} g(\rho) $$ where the sum is over all roots $\rho$ of an entire function $f$ and $g$ is some other analytic function for which the series converges? Thanks!","There are infinitely many real solutions to the equation . Denote the increasing sequence of positive solutions by . I want to evaluate the sum (the series can be seen to converge by using a crude approximation such as ). To find the sum I'm thinking to use Hadamard's product theorem applied to a function whose roots are and then to expand the product and compare coefficients as in Euler's evaluation of . The Hadamard product theorem states that any entire function of growth order can be written as where the product is over all the non-zero complex roots of repeated according to multiplicity, is the order of the root of at , and are two constants to be determined. Now is not entire but it has the same non-zero roots as the function which is entire. These are all real roots of order 1 and they are symmetric since the function is odd. Thus the non-zero roots of are at and there is also a triple root at . Further, the function has growth order 1, so applying Hadamard's product formula gives and we may compute and since is an even function. Therefore We may now expand the left side in a Taylor series and compare coefficients with the right side to obtain as well as higher order sums such as and so on. All sums of reciprocals of even powers will be rational numbers. My questions are: Is this solution correct? Are there other ways of evaluating the series without using the Hadamard product? Are there any general techniques (contour integration for example) available for evaluating sums of the form where the sum is over all roots of an entire function and is some other analytic function for which the series converges? Thanks!","\tan x=x  (\lambda_n)_{n=1}^{\infty}  \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} \hspace{1cm} (1)  \lambda_n \approx \frac{\pi(2n+1)}{2} \lambda_n \zeta(2) f(z) \alpha<2  f(z) = z^m e^{Az+B} \prod_{\rho} e^{z/\rho} \left(1-\frac{z}{\rho} \right)  \rho f m \geq 0 f z=0 A,B  \tan z-z   f(z) = \sin z - z \cos z f(z) \pm \lambda_n z=0  \sin z-z \cos z = e^{Az+B} z^3 \prod_{n=1}^{\infty} e^{z/\lambda_n} \left(1-\frac{z}{\lambda_n} \right) e^{z/(-\lambda_n)} \left(1-\frac{z}{(-\lambda_n)} \right) = e^{Az+B} z^3 \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right)  e^B = \lim_{z \to 0} \frac{\sin z-z\cos z}{z^3} = \frac{1}{3}  A=0 \frac{\sin z-z\cos z}{z^3}  \sin z-z \cos z = \frac{z^3}{3} \prod_{n=1}^{\infty} \left(1-\frac{z^2}{\lambda_n^2} \right) \hspace{1cm} (2)   \sum_{n=1}^{\infty} \frac{1}{\lambda_n^2} = \frac{1}{10}   \sum_{n=1}^{\infty} \frac{1}{\lambda_n^4} = \frac{1}{350}  (1)  \sum_{\rho} g(\rho)  \rho f g","['complex-analysis', 'hadamard-product']"
26,Polynomial identity : $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$,Polynomial identity :  for,|P(z)|^2=|Q(z)|^2-|R(z)|^2 z \in \mathbb{D},"Let us denote by $\mathbb{D}=\{z : |z| <1\}$ and $\mathbb{T}=\{z: |z|=1\}$ . Suppose that $P, Q$ and $R$ are polynomials that satisfy the following: $|Q(z)| \geq |R(z)| $ for all $z \in \overline{\mathbb{D}}$ $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{T}$ $P$ and $Q$ are non-vanishing on $\overline{\mathbb{D}}$ $|P(z)|^2 \leq |Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$ . My question : $|P(z)|^2=|Q(z)|^2-|R(z)|^2$ for $z \in \mathbb{D}$ ? I am trying to prove it. I think some version of maximum-modulus or identity theorem can be useful here. But what functions to choose to apply maximum modulus. Any hint how to proceed. My Attempt: Note that for any $z \in \mathbb{T}$ , we have $P(z)\overline{P(\frac{1}{\overline{z}})}=|P(z)|^2$ . Thus, condition (2) implies that $P(z)\overline{P(\frac{1}{\overline{z}})}= Q(z)\overline{Q(\frac{1}{\overline{z}})} -R(z)\overline{R(\frac{1}{\overline{z}})} $ for all $z \in \mathbb{T}$ . Hence, the map $F: \mathbb{D}\setminus \{0\} \to \mathbb{C}, F(z)= P(z)\overline{P(\frac{1}{\overline{z}})} - Q(z)\overline{Q(\frac{1}{\overline{z}})} - R(z)\overline{R(\frac{1}{\overline{z}})}$ is zero on $\mathbb{T}$ .","Let us denote by and . Suppose that and are polynomials that satisfy the following: for all for and are non-vanishing on for . My question : for ? I am trying to prove it. I think some version of maximum-modulus or identity theorem can be useful here. But what functions to choose to apply maximum modulus. Any hint how to proceed. My Attempt: Note that for any , we have . Thus, condition (2) implies that for all . Hence, the map is zero on .","\mathbb{D}=\{z : |z| <1\} \mathbb{T}=\{z: |z|=1\} P, Q R |Q(z)| \geq |R(z)|  z \in \overline{\mathbb{D}} |P(z)|^2=|Q(z)|^2-|R(z)|^2 z \in \mathbb{T} P Q \overline{\mathbb{D}} |P(z)|^2 \leq |Q(z)|^2-|R(z)|^2 z \in \mathbb{D} |P(z)|^2=|Q(z)|^2-|R(z)|^2 z \in \mathbb{D} z \in \mathbb{T} P(z)\overline{P(\frac{1}{\overline{z}})}=|P(z)|^2 P(z)\overline{P(\frac{1}{\overline{z}})}= Q(z)\overline{Q(\frac{1}{\overline{z}})} -R(z)\overline{R(\frac{1}{\overline{z}})}  z \in \mathbb{T} F: \mathbb{D}\setminus \{0\} \to \mathbb{C}, F(z)= P(z)\overline{P(\frac{1}{\overline{z}})} - Q(z)\overline{Q(\frac{1}{\overline{z}})} - R(z)\overline{R(\frac{1}{\overline{z}})} \mathbb{T}","['complex-analysis', 'complex-numbers', 'complex-geometry', 'analytic-functions', 'maximum-principle']"
27,Functional equation: $f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n$?,Functional equation: ?,f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n,"Im looking for functions $f(z)$ such that $f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n = f(0)^2 + f(1)^2 z + f(2)^2 z^2 + f(3)^2 z^3 + ...$ and $f(n)$ are all real. And I wonder how fast $f(n)$ grows. I had this question in my head for a long time but I got reminded by these Function $f(x)$, such that $\sum_{n=0}^{\infty} f(n) x^n = f(x)$ Does there exist a function that generates itself? and I am grateful to them. edit PROOF BY CONTRADICTION : infinite radius is not possible : Note $f(z)$ can not be a nonconstant polynomial. This also implies that : $f(n) = 0 $ for all $n > N$ for some $N$ , cannot be true unless $f(z)=0$ . Also notice that the radius must be $\infty$ since $f(n)$ goes to infinity. So apart from the trivial constant functions, we are dealing with real entire transcendental functions. That implies that $f(n)^2$ converges to $0$ faster than exponential. So it might make more sense to speak about how fast $\frac{1}{f(n)}$ grows. EVEN STRONGER : One can show that most $f(n)$ are not $0$ ; the tail of $f(n)$ must have no zero values, since all taylor coefficients are positive. And finally realize that therefore $f(1),f(2)$ must be nonzero, we can show that the start also can contain no zero's. So we end up with concluding that only $f(0)$ can be zero, what makes perfect sense since the function is strictly increasing on the positive reals. But now I run into a problem. Since $f(z)$ is strictly increasing , so is $f(n)$ . But strictly increasing $f(n)$ implies $0 < C < f(n)^2$ for some constant $C$ . Hence the radius is not infinite unless $f(z)$ is a constant !!! So we end up studying functions with finite radius, than can somehow get the real values $f(n)$ . Analytic continuation around a pole or such might work. Even lacunary series than can be extended might work. So the situation is not so clear and simple.","Im looking for functions such that and are all real. And I wonder how fast grows. I had this question in my head for a long time but I got reminded by these Function $f(x)$, such that $\sum_{n=0}^{\infty} f(n) x^n = f(x)$ Does there exist a function that generates itself? and I am grateful to them. edit PROOF BY CONTRADICTION : infinite radius is not possible : Note can not be a nonconstant polynomial. This also implies that : for all for some , cannot be true unless . Also notice that the radius must be since goes to infinity. So apart from the trivial constant functions, we are dealing with real entire transcendental functions. That implies that converges to faster than exponential. So it might make more sense to speak about how fast grows. EVEN STRONGER : One can show that most are not ; the tail of must have no zero values, since all taylor coefficients are positive. And finally realize that therefore must be nonzero, we can show that the start also can contain no zero's. So we end up with concluding that only can be zero, what makes perfect sense since the function is strictly increasing on the positive reals. But now I run into a problem. Since is strictly increasing , so is . But strictly increasing implies for some constant . Hence the radius is not infinite unless is a constant !!! So we end up studying functions with finite radius, than can somehow get the real values . Analytic continuation around a pole or such might work. Even lacunary series than can be extended might work. So the situation is not so clear and simple.","f(z) f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n = f(0)^2 + f(1)^2 z + f(2)^2 z^2 + f(3)^2 z^3 + ... f(n) f(n) f(z) f(n) = 0  n > N N f(z)=0 \infty f(n) f(n)^2 0 \frac{1}{f(n)} f(n) 0 f(n) f(1),f(2) f(0) f(z) f(n) f(n) 0 < C < f(n)^2 C f(z) f(n)","['sequences-and-series', 'complex-analysis', 'taylor-expansion', 'generating-functions', 'functional-equations']"
28,Is there a sequence of polynomials $p_n $such that $p_n(0)=1$ for all $n$ but $\lim_{n \rightarrow \infty} p_n(z)=0$?,Is there a sequence of polynomials such that  for all  but ?,p_n  p_n(0)=1 n \lim_{n \rightarrow \infty} p_n(z)=0,"I want to solve the following problem: Is there a sequence of polynomials $p_n$ , such that $p_n(0)=1$ , $n \in \mathbb{N}$ , but $\lim_{n \rightarrow \infty} p_n(z)=0$ for all $z \neq 0$ . As a hint I got: “Consider $K_n:=(\{ z \in \mathbb{C}:|z|\leq n \} \setminus \{ z \in \mathbb{C}: d(z,[0,n]) <\frac{1}{n}\}) \cup \{0\} \cup [\frac{1}{n},n]$ , then show that $K_n $ is compact and $\mathbb{C}\setminus K$ is connected. Then use Runge theorem to find a polynomial.” My approach: Obviously $\{ \ z \in \mathbb{C}:|z|\leq n \}$ is compact, and $\{ z \in \mathbb{C}: d(z,[0,n]) <\frac{1}{n}\}$ is open. Now subtracting an open set from a compact set yields a compact set. Further $\{0\}$ and $\{[\frac{1}{n},n]\}$ are both compact. Thus $K_n$ is compact. Now I am struggling to show that $\mathbb{C}\setminus K$ is connected. The only thing that comes to mind is to use the definition of connected sets. (A set $E$ is called connected, if it can not be writen as the union of two disjoint nonempty sets $A$ and $B$ which statisfy $A \cap E \neq \emptyset$ and $B \cap E \neq \emptyset$ ). Further I am not sure on how to use Runge’s theorem to “find a polynomial”. The version of Runge’s theorem that I know (and think may be useful) is: Let $\Omega$ be an open subset of $\mathbb{C}$ . Then $\mathbb{\hat{C}}\setminus \Omega$ is connected $\iff$ all $f \in H(\Omega)$ can be approximated uniformly on compact sets in $\Omega$ by polynomials.","I want to solve the following problem: Is there a sequence of polynomials , such that , , but for all . As a hint I got: “Consider , then show that is compact and is connected. Then use Runge theorem to find a polynomial.” My approach: Obviously is compact, and is open. Now subtracting an open set from a compact set yields a compact set. Further and are both compact. Thus is compact. Now I am struggling to show that is connected. The only thing that comes to mind is to use the definition of connected sets. (A set is called connected, if it can not be writen as the union of two disjoint nonempty sets and which statisfy and ). Further I am not sure on how to use Runge’s theorem to “find a polynomial”. The version of Runge’s theorem that I know (and think may be useful) is: Let be an open subset of . Then is connected all can be approximated uniformly on compact sets in by polynomials.","p_n p_n(0)=1 n \in \mathbb{N} \lim_{n \rightarrow \infty} p_n(z)=0 z \neq 0 K_n:=(\{ z \in \mathbb{C}:|z|\leq n \} \setminus \{ z \in \mathbb{C}: d(z,[0,n]) <\frac{1}{n}\}) \cup \{0\} \cup [\frac{1}{n},n] K_n  \mathbb{C}\setminus K \{ \ z \in \mathbb{C}:|z|\leq n \} \{ z \in \mathbb{C}: d(z,[0,n]) <\frac{1}{n}\} \{0\} \{[\frac{1}{n},n]\} K_n \mathbb{C}\setminus K E A B A \cap E \neq \emptyset B \cap E \neq \emptyset \Omega \mathbb{C} \mathbb{\hat{C}}\setminus \Omega \iff f \in H(\Omega) \Omega",['complex-analysis']
29,Some problems related to Achim Klenke: Probability theory 3.1.2,Some problems related to Achim Klenke: Probability theory 3.1.2,,"The originally problem is as follows: Give an example for two different probability generating functions that coincide at countably many points $x_i ∈ (0, 1)$ , $i ∈ \mathbb N$ . In other words, find $\{a_n\}_{n \in \mathbb N},\{b_n\}_{n \in \mathbb N}$ , two different sequences of non negative real numbers which sum to $1$ , but such that: $$\sum_{n \in \mathbb N} a_nx^n = \sum_{n \in \mathbb N} b_nx^n $$ For countably infinite many points $x\in (0,1)$ . My thoughts are as follows, if we let the LHS be $f(x)$ and the RHS be $g(x)$ then $f,g$ are holomorphic with radius of convergence $1$ , and hence so is $f-g$ . If the zero set of $f-g$ (i.e. where $f=g$ ) has a limit point in the interior of the unit disc, then $f=g$ identically, so we must have that the zero set of $f-g$ (in the unit interval) is a sequence approaching $1$ . Furthermore, let $t_n$ be the sequence of zeros of $f-g$ , we may cite the Blaschke condition to rule out sequences such as the harmonic series (i.e. $t_n = 1-\frac 1n$ since we need the sum of $1-t_n$ to be finite). Also, suppose $h(x): = \sum_{n \in \mathbb N} c_nx^n$ is analytic such that the sum of its coefficients is absolutely convergent, and such that $h(x_i) = 0$ for some sequence $x_i$ tending to 1, then we can normalize $c_n$ such that $\sum_{n \in \mathbb N}c_n^+ = \sum_{n \in \mathbb N}c_n^-=1$ (the superscript $+$ denotes $\min(\cdot,0)$ , similarly for $-$ ), then letting $f=\sum_{n \in \mathbb N}c_n^+x^n, g=\sum_{n \in \mathbb N}c_n^-x^n$ gives us the desired coefficients. To this end, I think $h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right)$ works but I have no proof of this (the coefficients are hard to compute explicitly). My question is as follows: I know that the product of Blaschke factors (which satisfy the Blaschke condition) defines a bounded holomorphic function on the unit disc, is it true that such a function also has absolutely convergent coefficients? can the coefficients of $h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right)$ be explicitly found/proven to be absolutely convergent in sum or is it just too much effort? If so is there a example with simpler example that's easier to work with? If such an example cannot be written down exactly. Is it possible to at least prove the existence of it? Or is there no such example possible?","The originally problem is as follows: Give an example for two different probability generating functions that coincide at countably many points , . In other words, find , two different sequences of non negative real numbers which sum to , but such that: For countably infinite many points . My thoughts are as follows, if we let the LHS be and the RHS be then are holomorphic with radius of convergence , and hence so is . If the zero set of (i.e. where ) has a limit point in the interior of the unit disc, then identically, so we must have that the zero set of (in the unit interval) is a sequence approaching . Furthermore, let be the sequence of zeros of , we may cite the Blaschke condition to rule out sequences such as the harmonic series (i.e. since we need the sum of to be finite). Also, suppose is analytic such that the sum of its coefficients is absolutely convergent, and such that for some sequence tending to 1, then we can normalize such that (the superscript denotes , similarly for ), then letting gives us the desired coefficients. To this end, I think works but I have no proof of this (the coefficients are hard to compute explicitly). My question is as follows: I know that the product of Blaschke factors (which satisfy the Blaschke condition) defines a bounded holomorphic function on the unit disc, is it true that such a function also has absolutely convergent coefficients? can the coefficients of be explicitly found/proven to be absolutely convergent in sum or is it just too much effort? If so is there a example with simpler example that's easier to work with? If such an example cannot be written down exactly. Is it possible to at least prove the existence of it? Or is there no such example possible?","x_i ∈ (0, 1) i ∈ \mathbb N \{a_n\}_{n \in \mathbb N},\{b_n\}_{n \in \mathbb N} 1 \sum_{n \in \mathbb N} a_nx^n = \sum_{n \in \mathbb N} b_nx^n  x\in (0,1) f(x) g(x) f,g 1 f-g f-g f=g f=g f-g 1 t_n f-g t_n = 1-\frac 1n 1-t_n h(x): = \sum_{n \in \mathbb N} c_nx^n h(x_i) = 0 x_i c_n \sum_{n \in \mathbb N}c_n^+ = \sum_{n \in \mathbb N}c_n^-=1 + \min(\cdot,0) - f=\sum_{n \in \mathbb N}c_n^+x^n, g=\sum_{n \in \mathbb N}c_n^-x^n h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right) h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right)","['complex-analysis', 'probability-theory', 'analytic-functions']"
30,Zeros of $f(z) = \sum_{n= 0}^{\infty} \frac{z^n}{n!\exp(\ln^2(n+1))} $ and $\lim_{x \to -\infty} f(x) = A $?,Zeros of  and ?,f(z) = \sum_{n= 0}^{\infty} \frac{z^n}{n!\exp(\ln^2(n+1))}  \lim_{x \to -\infty} f(x) = A ,"Consider the entire function $$ f(z) = \sum_{n= 0}^{\infty} \frac{z^n}{n!\exp(\ln^2(n+1))}, $$ where $\ln^2(n+1)$ stands for $(\ln(n+1))^2$ . Where are the zero's of this function? That is, what are the solutions of $$ f(z) = 0$$ Notice this function is entire and completely determined by its zero's. This follows from the fact that it grows much slower than exponential on the complex plane and theorems like Weierstrass factorization theorem and Hadamard factorization theorem. Does this function have infinitely many real zero's ? Does the real limit $$\lim_{x \to -\infty} f(x) = A$$ exist ? Sorry for asking 3 things at once but it is so related. It would probably be silly turning this into 3 seperate question posts. Background For those wondering why I picked this function I was looking for real zero's of entire functions that resemble $\exp(x)$ or somewhat slower functions. See : $0 \leq C + \sum_{n=1}^{+\infty} \frac{x^n}{(n+1)^n} $ ,where $C = 1.036055393...$ is the best possible constant. and the related questions. I want to understand special functions better, in particular those that are completely determined by their (complex) zero's. $\exp(\ln^2(n+1))$ grows faster than any polynomial, so this function is way different than $\exp(x)$ or $\operatorname{Ei}(x) - \ln(x)$ or their generalizations. I need imo a better understanding of limits in real analysis. Limits of special functions or showing their (non)existance seems like a good start beyond the trivial things tools and tricks.","Consider the entire function where stands for . Where are the zero's of this function? That is, what are the solutions of Notice this function is entire and completely determined by its zero's. This follows from the fact that it grows much slower than exponential on the complex plane and theorems like Weierstrass factorization theorem and Hadamard factorization theorem. Does this function have infinitely many real zero's ? Does the real limit exist ? Sorry for asking 3 things at once but it is so related. It would probably be silly turning this into 3 seperate question posts. Background For those wondering why I picked this function I was looking for real zero's of entire functions that resemble or somewhat slower functions. See : $0 \leq C + \sum_{n=1}^{+\infty} \frac{x^n}{(n+1)^n} $ ,where $C = 1.036055393...$ is the best possible constant. and the related questions. I want to understand special functions better, in particular those that are completely determined by their (complex) zero's. grows faster than any polynomial, so this function is way different than or or their generalizations. I need imo a better understanding of limits in real analysis. Limits of special functions or showing their (non)existance seems like a good start beyond the trivial things tools and tricks."," f(z) = \sum_{n= 0}^{\infty} \frac{z^n}{n!\exp(\ln^2(n+1))},  \ln^2(n+1) (\ln(n+1))^2  f(z) = 0 \lim_{x \to -\infty} f(x) = A \exp(x) \exp(\ln^2(n+1)) \exp(x) \operatorname{Ei}(x) - \ln(x)","['real-analysis', 'complex-analysis', 'limits', 'taylor-expansion', 'roots']"
31,Intuition for Reinhardt domains,Intuition for Reinhardt domains,,"I'm taking a complex analysis course, and in a lecture on several complex variables my professor defined Reinhardt domains in $\mathbb{C}^n$ as domains invariant under the action of the n-dimensional torus. I'd like help to better understand this concept. Some questions: The only examples I can think of are either n-dimensional balls or shells. Are there any other Reinhardt domains? My professor drew a box with the top right corner cut out (in the space of absolute values) and claimed this was also a Reinhardt domain in $\mathbb{C}^2.$ I don't see why that would be: aren't there rotations that would land part of this domain into that corner? I don't yet understand why this would be a useful definition to make. Why is this the right concept for domains for functions of several variables? Any help would be appreciated","I'm taking a complex analysis course, and in a lecture on several complex variables my professor defined Reinhardt domains in as domains invariant under the action of the n-dimensional torus. I'd like help to better understand this concept. Some questions: The only examples I can think of are either n-dimensional balls or shells. Are there any other Reinhardt domains? My professor drew a box with the top right corner cut out (in the space of absolute values) and claimed this was also a Reinhardt domain in I don't see why that would be: aren't there rotations that would land part of this domain into that corner? I don't yet understand why this would be a useful definition to make. Why is this the right concept for domains for functions of several variables? Any help would be appreciated",\mathbb{C}^n \mathbb{C}^2.,"['complex-analysis', 'intuition', 'several-complex-variables']"
32,Cauchy Residue Theorem with Logarithms,Cauchy Residue Theorem with Logarithms,,"I've been trying to use Cauchy's Residue Theorem to calculate the following integral, but I am getting stuck with the logarithm and not sure how to proceed. Q: Evaluate $\int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx$ First, let $I = \int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx$ . We can rewrite the integrand so that \begin{equation*} I = \int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx = \frac{1}{2} \int_{-\infty}^{\infty} \frac{2\ln|x|}{(x^2 + 1)(x + 1)} dx \Rightarrow 2I = \int_{-\infty}^{\infty} \frac{\ln(x^2)}{(x^2 + 1)(x + 1)} dx \end{equation*} So we first find what $J = 2I$ is. Let $f(z) = \frac{\ln(z^2)}{(z^2 + 1)(z + 1)}$ . Note that $f(z)$ has simple poles located at $z_0 = i$ , $z_1 = -i$ and $z_2 = -1$ . Furthermore, note that $z \neq 0$ from $\ln(z^2)$ . Then for $\epsilon$ ""small"" and $\rho$ ""large"", define $\Gamma_{\rho, \epsilon} = [-\rho, -\epsilon] + C_{\epsilon}^+ + [\epsilon, \rho] + C_{\rho}^+$ to be the contour that is oriented once in the counterclockwise direction. Note that only $z_0$ and $z_2$ are only inside this contour. Then by using Cauchy's Residue Theorem, we have \begin{align*} \lim_{\rho \to \infty, \epsilon \to 0} \int_{\Gamma_{\rho, \epsilon}} f(z) dz &= \lim_{\rho \to \infty, \epsilon \to 0} \int_{[-\rho, -\epsilon]} f(z) dz + \lim_{\epsilon \to 0} \int_{C_{\epsilon}^+} f(z) dz + \lim_{\rho \to \infty, \epsilon \to 0} \int_{[\epsilon, \rho]} f(z) dz + \lim_{\rho \to \infty} \int_{C_{\rho}^+} f(z) dz \\ &= 2\pi i [Res(f, z_0) + Res(f, z_2)] \end{align*} Computing the residues we have \begin{align*} Res(f, z_0) = \lim_{z \to i} (z - i)f(z) = \lim_{z \to i} \frac{\ln(z^2)}{(z + i)(z + 1)} = \frac{\ln(-1)}{2i(1 + i)} = \frac{i\pi}{2i(1 + i)} = \frac{\pi(1 - i)}{4} \\ Res(f, z_2) = \lim_{z \to -1} (z + 1)f(z) = \lim_{z \to -1} \frac{\ln(z^2)}{(z + i)(z - i)} = 0 \end{align*} I am not sure if this is the correct way to approach this problem and this is where I am getting stuck. I am not sure if I have used the log correctly in this sense.","I've been trying to use Cauchy's Residue Theorem to calculate the following integral, but I am getting stuck with the logarithm and not sure how to proceed. Q: Evaluate First, let . We can rewrite the integrand so that So we first find what is. Let . Note that has simple poles located at , and . Furthermore, note that from . Then for ""small"" and ""large"", define to be the contour that is oriented once in the counterclockwise direction. Note that only and are only inside this contour. Then by using Cauchy's Residue Theorem, we have Computing the residues we have I am not sure if this is the correct way to approach this problem and this is where I am getting stuck. I am not sure if I have used the log correctly in this sense.","\int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx I = \int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx \begin{equation*}
I = \int_{-\infty}^{\infty} \frac{\ln|x|}{(x^2 + 1)(x + 1)} dx = \frac{1}{2} \int_{-\infty}^{\infty} \frac{2\ln|x|}{(x^2 + 1)(x + 1)} dx \Rightarrow 2I = \int_{-\infty}^{\infty} \frac{\ln(x^2)}{(x^2 + 1)(x + 1)} dx
\end{equation*} J = 2I f(z) = \frac{\ln(z^2)}{(z^2 + 1)(z + 1)} f(z) z_0 = i z_1 = -i z_2 = -1 z \neq 0 \ln(z^2) \epsilon \rho \Gamma_{\rho, \epsilon} = [-\rho, -\epsilon] + C_{\epsilon}^+ + [\epsilon, \rho] + C_{\rho}^+ z_0 z_2 \begin{align*}
\lim_{\rho \to \infty, \epsilon \to 0} \int_{\Gamma_{\rho, \epsilon}} f(z) dz &= \lim_{\rho \to \infty, \epsilon \to 0} \int_{[-\rho, -\epsilon]} f(z) dz + \lim_{\epsilon \to 0} \int_{C_{\epsilon}^+} f(z) dz + \lim_{\rho \to \infty, \epsilon \to 0} \int_{[\epsilon, \rho]} f(z) dz + \lim_{\rho \to \infty} \int_{C_{\rho}^+} f(z) dz \\
&= 2\pi i [Res(f, z_0) + Res(f, z_2)]
\end{align*} \begin{align*}
Res(f, z_0) = \lim_{z \to i} (z - i)f(z) = \lim_{z \to i} \frac{\ln(z^2)}{(z + i)(z + 1)} = \frac{\ln(-1)}{2i(1 + i)} = \frac{i\pi}{2i(1 + i)} = \frac{\pi(1 - i)}{4} \\
Res(f, z_2) = \lim_{z \to -1} (z + 1)f(z) = \lim_{z \to -1} \frac{\ln(z^2)}{(z + i)(z - i)} = 0
\end{align*}","['complex-analysis', 'complex-integration', 'cauchy-principal-value']"
33,Writing $(-z)^\alpha$ in terms of $z^\alpha$,Writing  in terms of,(-z)^\alpha z^\alpha,"I have a very silly confusion in complex analysis. Let $z\in \mathbb{C}$ and $\alpha\in \mathbb{C}$ . Now consider $(-z)^\alpha$ and suppose that we want to relate this to $z^\alpha$ . One thing that I immediately notice is that $(-z)^{\alpha} = (-1)^\alpha z^\alpha$ seems to be ambiguous. In fact, we can write $-1 = e^{i\pi+2\pi k i}$ for $k\in \mathbb{Z}$ , but the choices seem to give different results depending on what $\alpha$ is. In fact we have $$(-z)^\alpha = e^{2\pi k\alpha i} e^{i\alpha \pi} z^\alpha\tag{1}.$$ Now the prefactor $e^{2\pi k\alpha i}$ is in general non-trivial if $\alpha\notin \mathbb{Z}$ . I feel that the right way to write $(-z)^\alpha$ in terms of $z^\alpha$ is by taking into account the branch cut, but I feel a bit confused in how this should be done correctly. So what is going on here? Why writing $(-z)^\alpha$ in terms of $z^\alpha$ seems highly ambiguous? How to identify the correct choice in a given situation?","I have a very silly confusion in complex analysis. Let and . Now consider and suppose that we want to relate this to . One thing that I immediately notice is that seems to be ambiguous. In fact, we can write for , but the choices seem to give different results depending on what is. In fact we have Now the prefactor is in general non-trivial if . I feel that the right way to write in terms of is by taking into account the branch cut, but I feel a bit confused in how this should be done correctly. So what is going on here? Why writing in terms of seems highly ambiguous? How to identify the correct choice in a given situation?",z\in \mathbb{C} \alpha\in \mathbb{C} (-z)^\alpha z^\alpha (-z)^{\alpha} = (-1)^\alpha z^\alpha -1 = e^{i\pi+2\pi k i} k\in \mathbb{Z} \alpha (-z)^\alpha = e^{2\pi k\alpha i} e^{i\alpha \pi} z^\alpha\tag{1}. e^{2\pi k\alpha i} \alpha\notin \mathbb{Z} (-z)^\alpha z^\alpha (-z)^\alpha z^\alpha,"['complex-analysis', 'complex-numbers']"
34,$\alpha$-quantum wedge in Liouville Quantum Gravity,-quantum wedge in Liouville Quantum Gravity,\alpha,"We consider a proper simply connected subdomain $D\subset\mathbb{C}$ and let $\overline{\mathcal{D}}(D)$ be the space of smooth functions in $D$ with finite Dirichlet energy, which we consider modulo constants. The closure of $\overline{\mathcal{D}}(D)$ with respect to the Dirichlet inner product $$\langle f,g\rangle_\nabla:=\frac{1}{2\pi}\int_D\nabla f\cdot\nabla g$$ will be denoted by $\overline{H}^1(D)$ . Let $\{\overline{f}_j\}_{j\in\mathbb{N}}$ be an orthonormal basis for $\overline{H}^1(D)$ and let $\{X_j\}_{j\in\mathbb{N}}$ be independent $\mathcal{N}(0,1)$ random variables. The random series $$\overline{h}_n:=\sum_{j\in\mathbb{N}}X_j\overline{f}_j$$ converges almost surely in the space of distributions modulo constants, and the law of the limit does not depend on the choice of orthonormal basis $\{\overline{f}_j\}_{j\in\mathbb{N}}$ . The Neumann Gaussian free field (NGFF) is defined to be the limit $h$ of this random series. Let $S=\mathbb{R}\times(0,\pi)$ be an infinite strip. We can decompose $\overline{H}^1(S)$ as follows: Let $\overline{\mathcal{H}}_{\text{rad}}$ be the subspace of $\overline{H}^1(S)$ obtained by closing the smooth functions which are constant on each vertical segment, modulo constants and let $\mathcal{H}_{\text{circ}}$ be the subspace obtained by closing off the smooth functions which have mean zero on all vertical segments. We have that $$\overline{H}^1(S)=\overline{\mathcal{H}}_{\text{rad}}\oplus\mathcal{H}_{\text{circ}}.$$ This allows us to write the NGFF on $S$ as $h=h_{\text{rad}}^{\text{GFF}}+h_{\text{circ}}^{\text{GFF}}$ , where $h_{\text{rad}}^{\text{GFF}},h_{\text{circ}}^{\text{GFF}}$ are independent, $h_{\text{rad}}^{\text{GFF}}(z)=B_{2s}$ if $\text{Re}(z)=s$ , where $B$ is an independent standard Brownian motion modulo constants, and $h_{\text{circ}}^{\text{GFF}}(z)$ has mean zero on each vertical segment. Now let $\tilde{\mathcal{M}}$ be the space of signed Radon measures $\rho$ on $S$ with finite and equal positive and negative mass, so that $\int_D\rho(dx)=0$ , such that $$\overline{f}\mapsto\int_D\overline{f}(x)\rho(dx)$$ defines a continuous linear functional on $\overline{H}^1(S)$ . We define $$\mathcal{M}:=\{\rho:\rho=\tilde{\rho}+f\text{ with }\tilde{\rho}\in\tilde{\mathcal{M}}\text{ and }f\in\mathcal{D}_0(S)\},$$ where $\mathcal{D}_0(S)$ is the space of smooth test functions that are compactly supported in $S$ . An $\alpha$ -quantum wedge is defined as follows: Let $$h_{\text{rad}}=\begin{cases}B_{2s}+(\alpha+Q),&\text{if Re}(z)=\text{ and }s\geq0;\\\widehat{B}_{-2s}+(\alpha-Q)s,&\text{if Re}(z)=s\text{ and }s<0,\end{cases}$$ where $B$ is a standard Brownian motion, and $\widehat{B}$ is an independent Brownian motion conditioned so that $\widehat{B}_{2t}+(Q-\alpha)t>0$ for all $t>0$ and $Q$ is just a constant $Q=\frac{2}{\gamma}+\frac{\gamma}{2}$ and $\gamma\in[0,2)$ . Let $h_{\text{circ}}$ be a stochastic process indexed by $\mathcal{M}$ that is independent of $h_{\text{rad}}$ and has the same law as $h_{\text{circ}}^{\text{GFF}}$ . We call $h=h_{\text{rad}}+h_{\text{circ}}$ an $\alpha$ -quantum wedge in $S$ . Can someone help me understand why the $\alpha$ -quantum wedge is important for LQG? To my eyes the definition includes a lot of formalities that don't necessarily form a good picture or vision in my head. Why is this construction necessary/useful? Edit: I want to explain why this is a math question, and not a physics question, since I've been asked. Liouville quantum gravity (LQG) is a ""poorly"" named field of math. It was inspired by Polyakov's 1984 seminal work The Quantum Geometry of Bosonic Strings . From that paper blossomed the field of Liouville conformal field theory (LCFT), a 2D theory of quantum gravity. When physicists first tried to solve LCFT, they did so through a nonrigorous analytic continuation argument, from which they found the DOZZ formula -- A sort of good guess. When mathematicians came to tackle the problem (just like how the Feynman path integral is formalized by Brownian bridges), they discovered that a two dimension version of the Brownian bridge, called the Gaussian free field (GFF), was involved. Now, LQG has branched off significantly from LCFT: The rigorous proof of the DOZZ formula was recently completed, and the next goal is to work on the conformal bootstrap (this is work in LCFT; I'll be publishing notes soon on the rigorous proof of the DOZZ formula, since it takes hundreds of pages across multiple papers and books). LQG on the other hand, focuses more on the random geometry. We consider things like random models on graphs, and discover that their scaling limits are Liouville quantum gravity surfaces, explicitly construct a random metrics, and as you saw above, notions of GFF, etc. This is only the tip of the iceberg for LQG, which has deep connections to imaginary geometry, SLE and other random processes, etc., and even the motivating LCFT.","We consider a proper simply connected subdomain and let be the space of smooth functions in with finite Dirichlet energy, which we consider modulo constants. The closure of with respect to the Dirichlet inner product will be denoted by . Let be an orthonormal basis for and let be independent random variables. The random series converges almost surely in the space of distributions modulo constants, and the law of the limit does not depend on the choice of orthonormal basis . The Neumann Gaussian free field (NGFF) is defined to be the limit of this random series. Let be an infinite strip. We can decompose as follows: Let be the subspace of obtained by closing the smooth functions which are constant on each vertical segment, modulo constants and let be the subspace obtained by closing off the smooth functions which have mean zero on all vertical segments. We have that This allows us to write the NGFF on as , where are independent, if , where is an independent standard Brownian motion modulo constants, and has mean zero on each vertical segment. Now let be the space of signed Radon measures on with finite and equal positive and negative mass, so that , such that defines a continuous linear functional on . We define where is the space of smooth test functions that are compactly supported in . An -quantum wedge is defined as follows: Let where is a standard Brownian motion, and is an independent Brownian motion conditioned so that for all and is just a constant and . Let be a stochastic process indexed by that is independent of and has the same law as . We call an -quantum wedge in . Can someone help me understand why the -quantum wedge is important for LQG? To my eyes the definition includes a lot of formalities that don't necessarily form a good picture or vision in my head. Why is this construction necessary/useful? Edit: I want to explain why this is a math question, and not a physics question, since I've been asked. Liouville quantum gravity (LQG) is a ""poorly"" named field of math. It was inspired by Polyakov's 1984 seminal work The Quantum Geometry of Bosonic Strings . From that paper blossomed the field of Liouville conformal field theory (LCFT), a 2D theory of quantum gravity. When physicists first tried to solve LCFT, they did so through a nonrigorous analytic continuation argument, from which they found the DOZZ formula -- A sort of good guess. When mathematicians came to tackle the problem (just like how the Feynman path integral is formalized by Brownian bridges), they discovered that a two dimension version of the Brownian bridge, called the Gaussian free field (GFF), was involved. Now, LQG has branched off significantly from LCFT: The rigorous proof of the DOZZ formula was recently completed, and the next goal is to work on the conformal bootstrap (this is work in LCFT; I'll be publishing notes soon on the rigorous proof of the DOZZ formula, since it takes hundreds of pages across multiple papers and books). LQG on the other hand, focuses more on the random geometry. We consider things like random models on graphs, and discover that their scaling limits are Liouville quantum gravity surfaces, explicitly construct a random metrics, and as you saw above, notions of GFF, etc. This is only the tip of the iceberg for LQG, which has deep connections to imaginary geometry, SLE and other random processes, etc., and even the motivating LCFT.","D\subset\mathbb{C} \overline{\mathcal{D}}(D) D \overline{\mathcal{D}}(D) \langle f,g\rangle_\nabla:=\frac{1}{2\pi}\int_D\nabla f\cdot\nabla g \overline{H}^1(D) \{\overline{f}_j\}_{j\in\mathbb{N}} \overline{H}^1(D) \{X_j\}_{j\in\mathbb{N}} \mathcal{N}(0,1) \overline{h}_n:=\sum_{j\in\mathbb{N}}X_j\overline{f}_j \{\overline{f}_j\}_{j\in\mathbb{N}} h S=\mathbb{R}\times(0,\pi) \overline{H}^1(S) \overline{\mathcal{H}}_{\text{rad}} \overline{H}^1(S) \mathcal{H}_{\text{circ}} \overline{H}^1(S)=\overline{\mathcal{H}}_{\text{rad}}\oplus\mathcal{H}_{\text{circ}}. S h=h_{\text{rad}}^{\text{GFF}}+h_{\text{circ}}^{\text{GFF}} h_{\text{rad}}^{\text{GFF}},h_{\text{circ}}^{\text{GFF}} h_{\text{rad}}^{\text{GFF}}(z)=B_{2s} \text{Re}(z)=s B h_{\text{circ}}^{\text{GFF}}(z) \tilde{\mathcal{M}} \rho S \int_D\rho(dx)=0 \overline{f}\mapsto\int_D\overline{f}(x)\rho(dx) \overline{H}^1(S) \mathcal{M}:=\{\rho:\rho=\tilde{\rho}+f\text{ with }\tilde{\rho}\in\tilde{\mathcal{M}}\text{ and }f\in\mathcal{D}_0(S)\}, \mathcal{D}_0(S) S \alpha h_{\text{rad}}=\begin{cases}B_{2s}+(\alpha+Q),&\text{if Re}(z)=\text{ and }s\geq0;\\\widehat{B}_{-2s}+(\alpha-Q)s,&\text{if Re}(z)=s\text{ and }s<0,\end{cases} B \widehat{B} \widehat{B}_{2t}+(Q-\alpha)t>0 t>0 Q Q=\frac{2}{\gamma}+\frac{\gamma}{2} \gamma\in[0,2) h_{\text{circ}} \mathcal{M} h_{\text{rad}} h_{\text{circ}}^{\text{GFF}} h=h_{\text{rad}}+h_{\text{circ}} \alpha S \alpha","['complex-analysis', 'probability-theory', 'analysis', 'stochastic-processes', 'brownian-motion']"
35,Numerical computation of Laurent coefficients for rational functions,Numerical computation of Laurent coefficients for rational functions,,"In terms of a project I am trying to implement the Carathéodory-Fejér algorithm, if interested see [1] or [2] . However, in one of the steps I need to compute the Laurent coefficients of a rational function $f$ , which does only has single poly, on the unit circle. The usual formula $$c_k=\frac {1}{2\pi i} \int_\Gamma \frac{f(z)}{z^{k+1}} dz=\frac {1}{2\pi} \int_0^{2\pi} f(e^{i \varphi})e^{-ik\varphi}d\varphi,$$ is due to oscillation not well-suited for numerical integration. Does any one have some ideas, what I could try in order to determine those coefficients for an arbitrary rational function, with just single pols (no pols on the unit circle itself)?","In terms of a project I am trying to implement the Carathéodory-Fejér algorithm, if interested see [1] or [2] . However, in one of the steps I need to compute the Laurent coefficients of a rational function , which does only has single poly, on the unit circle. The usual formula is due to oscillation not well-suited for numerical integration. Does any one have some ideas, what I could try in order to determine those coefficients for an arbitrary rational function, with just single pols (no pols on the unit circle itself)?","f c_k=\frac {1}{2\pi i} \int_\Gamma \frac{f(z)}{z^{k+1}} dz=\frac {1}{2\pi} \int_0^{2\pi} f(e^{i \varphi})e^{-ik\varphi}d\varphi,","['complex-analysis', 'numerical-methods', 'complex-integration', 'numerical-calculus']"
36,Question about zeros of finite order functions [duplicate],Question about zeros of finite order functions [duplicate],,"This question already has an answer here : $f$ is entire without any zeros then there is an entire function $g$ such that $f=e^g$ (1 answer) Closed last year . Context, I was reading Joseph Bak and Donald J. Newman's Complex Analysis as a refresher on complex analysis (I have some prior experience but am washed) and I was attempting to understand theorem 16.13. Suppose $f$ is an entire function of finite order. Then either $f$ has infinitely many zeroes or $$f (z) = Q(z)e^{P(z)}$$ where $Q$ and $P$ are polynomials. In particular, the following step, let $f$ be entire and of finite order (bounded in magnitude by $A|z|^n$ for some integer $n$ and real constant $A$ ). The book goes on to say that if $f$ has finitely many zeros, then $f(z) = Q(z)g(z)$ where: $$Q(z):=(z-z_1)\cdots(z-z_n)$$ Where $z_1,\dots,z_n$ are the zeros of $f$ and $g$ is nonzero and holomorphic. Then it goes on to claim that $\ln(g)$ is entire. This is where the argument loses me. I understand that $g$ is nonzero, but that doesn't mean that the image of $g$ doesn't cross the branch cut of $\ln$ . I'm guessing that they want to define a branch cut of $\ln$ such that this is true but I cannot understand what specific cut they are using. Moreover, $g$ , being holomorphic would attain every value (except $0$ by assumption) by Picard's Theorem, so how exactly are they defining this branch cut here? Or is there something else that I am missing? If this argument is indeed flawed, how can it be fixed? Image of original text attached for context, page 237:","This question already has an answer here : $f$ is entire without any zeros then there is an entire function $g$ such that $f=e^g$ (1 answer) Closed last year . Context, I was reading Joseph Bak and Donald J. Newman's Complex Analysis as a refresher on complex analysis (I have some prior experience but am washed) and I was attempting to understand theorem 16.13. Suppose is an entire function of finite order. Then either has infinitely many zeroes or where and are polynomials. In particular, the following step, let be entire and of finite order (bounded in magnitude by for some integer and real constant ). The book goes on to say that if has finitely many zeros, then where: Where are the zeros of and is nonzero and holomorphic. Then it goes on to claim that is entire. This is where the argument loses me. I understand that is nonzero, but that doesn't mean that the image of doesn't cross the branch cut of . I'm guessing that they want to define a branch cut of such that this is true but I cannot understand what specific cut they are using. Moreover, , being holomorphic would attain every value (except by assumption) by Picard's Theorem, so how exactly are they defining this branch cut here? Or is there something else that I am missing? If this argument is indeed flawed, how can it be fixed? Image of original text attached for context, page 237:","f f f (z) = Q(z)e^{P(z)} Q P f A|z|^n n A f f(z) = Q(z)g(z) Q(z):=(z-z_1)\cdots(z-z_n) z_1,\dots,z_n f g \ln(g) g g \ln \ln g 0",['complex-analysis']
37,Is this a thing? Visualizing complex functions with 3D animation,Is this a thing? Visualizing complex functions with 3D animation,,"I have read about various ways to visual complex functions, e.g. colored graphs, vector fields, conformal maps, etc. Here is another way: 3D animation. Picture a 3D plot with three axes for $\text{Re}(z), \text{Im}(z), \text{Re}(f(z))$ . Press ""play"" and the plot starts moving, with time $t=\text{Im}(f(z))$ . Alternatively, the three axes could be $|z|, \operatorname{arg}(z), |f(z)|$ , with time $t=\operatorname{arg}(f(z))$ . Questions: Is this method of visualizing complex functions known? Could it be useful? (Needham's book Visual Complex Analysis does not seem to mention this method.)","I have read about various ways to visual complex functions, e.g. colored graphs, vector fields, conformal maps, etc. Here is another way: 3D animation. Picture a 3D plot with three axes for . Press ""play"" and the plot starts moving, with time . Alternatively, the three axes could be , with time . Questions: Is this method of visualizing complex functions known? Could it be useful? (Needham's book Visual Complex Analysis does not seem to mention this method.)","\text{Re}(z), \text{Im}(z), \text{Re}(f(z)) t=\text{Im}(f(z)) |z|, \operatorname{arg}(z), |f(z)| t=\operatorname{arg}(f(z))","['complex-analysis', 'functions', 'visualization']"
38,Trying to understand the Kramer-Kronig relations with example $f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)$,Trying to understand the Kramer-Kronig relations with example,f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2),"Trying to understand the Kramer-Kronig relations with example $f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)$ Introduction Let think in the function: $$f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)\equiv \left(\frac{1-t^2+|1-t^2|}{2}\right)^4\tag{Eq. 1}\label{Eq. 1}$$ which is a real-valued square-integrable even function, with $\theta(t)$ the unitary step function . At can be seen in Wolfram-Alpha , the Fourier Transform is given by: $$\begin{array}{r c l} F(w)  & = & \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt \\ & = & \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt \\ & = & \displaystyle{\frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)}  \tag{Eq. 2}\label{Eq. 2} \\ \end{array}$$ which is reviewed in detail in this question since shows an horrid spike when plotted, but it end to be an artifact due current software, since it is continuous at the origin as is show in this answer by @CalvinKhor. As expected, since $f(t)$ is real-valued and an even function, its Fourier transform $F(w)$ is also real-valued and even function (see Wikipedia Table line $111$ ). Main part In the Wikipedia page for the Kramers-Kronig relations is stated that if a function $x(t)$ have an initial time $t_0$ such as $x(t) = 0\ \forall\ t<t_0$ , and without loss of generality using the assumption that $t_0=0$ , then if the Fourier Transform $X(w) \in \mathbb{C}$ could be split as $X(w) = U(w)+iV(w)$ with both $U(w),\ V(w) \in \mathbb{R}$ real-valued, then it is possible to find under some mild-assumptions that: $$\begin{array}{r c l} U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\ V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 3}\label{Eq. 3} \end{array}$$ which is traditionally stated as the property a causal function should fulfill. And here is where I got into a problem: thinking that $f(t)$ it is a real-valued causal function, in principle it should be fulfilling \eqref{Eq. 3}, but since its Fourier transform its real valued, the split $F(w) = U(w)+iV(w)$ will lead: $$\begin{array}{r c l} U(w) & = & \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big) \\ V(w) & = & 0 \tag{Eq. 4}\label{Eq. 4} \end{array}$$ And I don't know how it could be possible then to fulfill with these values the relations of \eqref{Eq. 3}: $$\begin{array}{r c l} U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\textbf{0}}{\xi-w}\ d\xi \overset{?!}{\equiv} \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)\\ V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\frac{768}{\xi^9}\cdot \Big(5\ \xi\ (2\ \xi^2-21)\cos(\xi)+(\xi^4-45\ \xi^2+105) \sin(\xi)\Big)}{\xi-w}\ d\xi \overset{?!}{\equiv} 0 \tag{Eq. 5}\label{Eq. 5} \end{array}$$ and even it could be possible of having a zero-valued integral, in \eqref{Eq. 5} showing a function as result of integrating a zero makes no sense for me , so surely I have something mistakenly understood, or otherwise there some assumptions of the Kramer-Kronig relation that $f(t)$ is not fulfilling, and I don't know which it is (at least I checked that $F(w)$ decrease much master than $1/|w|$ , other assumptions I don't really understand them so I cannot check them). Hope you could explain with detail and sources what is happening here. Added later Reading again the Wikipedia page for the Kramers-Kronig relations I think the problem could be in the requirement ""Suppose this function is analytic in the closed upper half-plane of $w$ "", which could be requiring than the Fourier Transform must have a non-zero imaginary part $V(w) \neq 0$ from which I think I have understood from the page for analytic signal . But this makes the problem even worst: with the following procedure I would try to make appear the Kramers-Kronig relations under a few assumptions and using the real/imaginary split in general form, and I think without loss of generality it means that by replacing the terms by zero on the imaginary part of the spectrum it should still be holding as true, but as it could be seen is not what end by happening, and I don't know why is that so: Let be $x(t)$ a square-integrable real-valued continuous function with Fourier Transform $X(w)$ such as $X(w)=U(w)+iV(w)$ with $U(w)$ and $V(w)$ real-valued functions in the angular frequency variable $w$ ( $X(w)\in\mathbb{C};\,U(w),V(w) \in\,\mathbb{R}$ ). Since $x(t)$ is real-valued then $X(-w) = \overline{X(w)}$ his complex conjugate. Let also $x(t)$ be a causal function, so, there exist an initial time $t_0$ where $x(t)=0,\ \forall t<t_0$ . For simplicity, but without loss of generality, I will assume that $t_0=0$ . With this, it will be equivalent to represent $x(t)$ as $x(t) = x(t)\cdot\theta(t)$ with $\theta(t)$ the unitary step function . This assumption have a serious consequence: since $x(t)$ is not extended from $t \to \pm \infty$ , then it cannot be band-limited, as is stated in Wikipedia , so the domain on the frequencies must go from $w\in \ (-\infty,\ \infty)$ . Also having an initial time can be described through the Kramer-Kronig relation , which are used as a characteristic a causal function must fulfill, obtained by applying the Fourier Transform the last equation: $$\begin{array}{r c l} X(w) & = & \mathbb{F}\left\{x(t)\cdot\theta(t) \right\}(w) \\ & = & \frac{1}{2\pi} X(w)\circledast\mathbb{F}\left\{\theta(t) \right\}(w) \\ & = & \frac{1}{2\pi} X(w)\circledast\left[\pi\delta(w) - \mathit{P\!V}\frac{i}{w} \right](w)\\ & = & \frac{X(0)}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iX(\xi)}{\xi-w}\ d\xi \tag{Eq. 6}\label{Eq. 6} \end{array}$$ where $\circledast$ is the Convolution operator, and $\mathit{P\!V}$ means the Principal Value which if there exist a singularity at point "" $\epsilon$ "", it can be calculated as $\mathit{P\!V}\int\limits_{-\infty}^{\infty}f(x)\ dx =\lim\limits_{c\rightarrow 0^+}\left[\int\limits_{-\infty}^{\epsilon-c}f(x)\ dx +\int\limits_{\epsilon+c}^{\infty}f(x)\ dx \right]$ . By splitting $X(w) = U(w)+iV(w)$ and using the property for real-valued functions one gets that: $$\begin{array}{r c l} X(-w)  & =  & \overline{X(w)}\\ \Rightarrow U(-w) + iV(-w) & = & U(w)-iV(w) \\ \Rightarrow U(-w) & = & U(w)\,\,\text{even function}\\ V(-w) & = & -V(w)\,\,\text{odd function}\\ \Rightarrow V(-0) = -V(0) & \Rightarrow & V(0) = 0 \end{array}$$ This split into \eqref{Eq. 6} leads to: $$\begin{array}{r c l} U(w)+iV(w) & = & \frac{U(0)}{2}+\frac{i\require{cancel}\cancel{V(0)}}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iU(\xi)}{\xi-w}\ d\xi +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\ \text{pairing real and imaginary terms}\,\Rightarrow U(w) & = & \frac{U(0)}{2} +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\ V(w) & = & -\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 7}\label{Eq. 7} \end{array}$$ Which are ""almost"" the relation shown in Wikipedia page for the Kramer-Kronig relation (show on \eqref{Eq. 8}): there is a constant $U(0)$ and a $1/2$ term don't fit: I don't know exactly why is that, and this is not the formal way to find the relations, but since it don't change why I want to ask, and I found more intuitive this procedure, please keep it mind. If you know how to get the following: $$\begin{array}{r c l} U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\ V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 8}\label{Eq. 8} \end{array}$$ from \eqref{Eq. 7} I will love to know how to do it: I think is related with the fact that the Fourier Transform just go from $t=0\to\infty$ as it kind of fit with the definition of the analytic signal , but to be honest I don't know in the formal definitions how they make appear the DC component $U(0)$ from an even function $V(w)$ . Also, as you can see from \eqref{Eq. 7} by replacing just $V(w)=0$ the equalities do not hold true, which means that the Kramer-Kronig relations, the only tool for causality analysis I know from signal analysis, don't hold for any squared-integrable real-valued causal continuous function $x(t)$ , which don't seen right to me since physical signals in time should fulfill it (as the particular example of the question $f(t)$ ). As can you see I am quite lost, so please elaborate about why this is isn't working . 2nd Added later The full conditions shown in Wikipedia for the Kramer-Kronig relation says: ""Suppose this function is analytic in the closed upper half-plane of $w$ and vanishes faster than $1/|w|$ as $|w|\to \infty$ . Slightly weaker conditions are also possible."" The closed upper half-plane is defined in Wikipedia as ""the union of the upper half-plane $\mathcal{H}\equiv \{x+iy\,|\,y>0;\,x,\ y\in\mathbb{R}\}$ and the real axis. It is the closure of the upper half-plane."" So, since $f(t)$ lives in the real axis as also do $F(w)$ , which I believe it is indeed analytic in $w$ , and also if I plot the Fourier Transform of Eq. 2 jointly with the function $1/|w|$ it shows indeed a decay faster than the $1/|w|$ , so in principle I think its fulfilling the conditions, as can be seen here So $f(t)$ should be fulfilling the conditions of the Kramers-Kronig relations, so I don't know why the previous equations aren't fulfilled. Hope you explain in detail if I have misunderstood some of the conditions.","Trying to understand the Kramer-Kronig relations with example Introduction Let think in the function: which is a real-valued square-integrable even function, with the unitary step function . At can be seen in Wolfram-Alpha , the Fourier Transform is given by: which is reviewed in detail in this question since shows an horrid spike when plotted, but it end to be an artifact due current software, since it is continuous at the origin as is show in this answer by @CalvinKhor. As expected, since is real-valued and an even function, its Fourier transform is also real-valued and even function (see Wikipedia Table line ). Main part In the Wikipedia page for the Kramers-Kronig relations is stated that if a function have an initial time such as , and without loss of generality using the assumption that , then if the Fourier Transform could be split as with both real-valued, then it is possible to find under some mild-assumptions that: which is traditionally stated as the property a causal function should fulfill. And here is where I got into a problem: thinking that it is a real-valued causal function, in principle it should be fulfilling \eqref{Eq. 3}, but since its Fourier transform its real valued, the split will lead: And I don't know how it could be possible then to fulfill with these values the relations of \eqref{Eq. 3}: and even it could be possible of having a zero-valued integral, in \eqref{Eq. 5} showing a function as result of integrating a zero makes no sense for me , so surely I have something mistakenly understood, or otherwise there some assumptions of the Kramer-Kronig relation that is not fulfilling, and I don't know which it is (at least I checked that decrease much master than , other assumptions I don't really understand them so I cannot check them). Hope you could explain with detail and sources what is happening here. Added later Reading again the Wikipedia page for the Kramers-Kronig relations I think the problem could be in the requirement ""Suppose this function is analytic in the closed upper half-plane of "", which could be requiring than the Fourier Transform must have a non-zero imaginary part from which I think I have understood from the page for analytic signal . But this makes the problem even worst: with the following procedure I would try to make appear the Kramers-Kronig relations under a few assumptions and using the real/imaginary split in general form, and I think without loss of generality it means that by replacing the terms by zero on the imaginary part of the spectrum it should still be holding as true, but as it could be seen is not what end by happening, and I don't know why is that so: Let be a square-integrable real-valued continuous function with Fourier Transform such as with and real-valued functions in the angular frequency variable ( ). Since is real-valued then his complex conjugate. Let also be a causal function, so, there exist an initial time where . For simplicity, but without loss of generality, I will assume that . With this, it will be equivalent to represent as with the unitary step function . This assumption have a serious consequence: since is not extended from , then it cannot be band-limited, as is stated in Wikipedia , so the domain on the frequencies must go from . Also having an initial time can be described through the Kramer-Kronig relation , which are used as a characteristic a causal function must fulfill, obtained by applying the Fourier Transform the last equation: where is the Convolution operator, and means the Principal Value which if there exist a singularity at point "" "", it can be calculated as . By splitting and using the property for real-valued functions one gets that: This split into \eqref{Eq. 6} leads to: Which are ""almost"" the relation shown in Wikipedia page for the Kramer-Kronig relation (show on \eqref{Eq. 8}): there is a constant and a term don't fit: I don't know exactly why is that, and this is not the formal way to find the relations, but since it don't change why I want to ask, and I found more intuitive this procedure, please keep it mind. If you know how to get the following: from \eqref{Eq. 7} I will love to know how to do it: I think is related with the fact that the Fourier Transform just go from as it kind of fit with the definition of the analytic signal , but to be honest I don't know in the formal definitions how they make appear the DC component from an even function . Also, as you can see from \eqref{Eq. 7} by replacing just the equalities do not hold true, which means that the Kramer-Kronig relations, the only tool for causality analysis I know from signal analysis, don't hold for any squared-integrable real-valued causal continuous function , which don't seen right to me since physical signals in time should fulfill it (as the particular example of the question ). As can you see I am quite lost, so please elaborate about why this is isn't working . 2nd Added later The full conditions shown in Wikipedia for the Kramer-Kronig relation says: ""Suppose this function is analytic in the closed upper half-plane of and vanishes faster than as . Slightly weaker conditions are also possible."" The closed upper half-plane is defined in Wikipedia as ""the union of the upper half-plane and the real axis. It is the closure of the upper half-plane."" So, since lives in the real axis as also do , which I believe it is indeed analytic in , and also if I plot the Fourier Transform of Eq. 2 jointly with the function it shows indeed a decay faster than the , so in principle I think its fulfilling the conditions, as can be seen here So should be fulfilling the conditions of the Kramers-Kronig relations, so I don't know why the previous equations aren't fulfilled. Hope you explain in detail if I have misunderstood some of the conditions.","f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2) f(t) =\left(1-t^2\right)^4\cdot\theta(1-t^2)\equiv \left(\frac{1-t^2+|1-t^2|}{2}\right)^4\tag{Eq. 1}\label{Eq. 1} \theta(t) \begin{array}{r c l}
F(w)  & = & \int\limits_{-\infty}^{\infty} f(t)\,e^{-iwt}\,dt \\
& = & \int\limits_{-1}^{1} (1-t^2)^4\,e^{-iwt}\,dt \\ & = & \displaystyle{\frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)}  \tag{Eq. 2}\label{Eq. 2} \\
\end{array} f(t) F(w) 111 x(t) t_0 x(t) = 0\ \forall\ t<t_0 t_0=0 X(w) \in \mathbb{C} X(w) = U(w)+iV(w) U(w),\ V(w) \in \mathbb{R} \begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 3}\label{Eq. 3}
\end{array} f(t) F(w) = U(w)+iV(w) \begin{array}{r c l}
U(w) & = & \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big) \\
V(w) & = & 0 \tag{Eq. 4}\label{Eq. 4}
\end{array} \begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\textbf{0}}{\xi-w}\ d\xi \overset{?!}{\equiv} \frac{768}{w^9}\cdot \Big(5\ w\ (2\ w^2-21)\cos(w)+(w^4-45\ w^2+105) \sin(w)\Big)\\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{\frac{768}{\xi^9}\cdot \Big(5\ \xi\ (2\ \xi^2-21)\cos(\xi)+(\xi^4-45\ \xi^2+105) \sin(\xi)\Big)}{\xi-w}\ d\xi \overset{?!}{\equiv} 0 \tag{Eq. 5}\label{Eq. 5}
\end{array} f(t) F(w) 1/|w| w V(w) \neq 0 x(t) X(w) X(w)=U(w)+iV(w) U(w) V(w) w X(w)\in\mathbb{C};\,U(w),V(w) \in\,\mathbb{R} x(t) X(-w) = \overline{X(w)} x(t) t_0 x(t)=0,\ \forall t<t_0 t_0=0 x(t) x(t) = x(t)\cdot\theta(t) \theta(t) x(t) t \to \pm \infty w\in \ (-\infty,\ \infty) \begin{array}{r c l}
X(w) & = & \mathbb{F}\left\{x(t)\cdot\theta(t) \right\}(w) \\
& = & \frac{1}{2\pi} X(w)\circledast\mathbb{F}\left\{\theta(t) \right\}(w) \\
& = & \frac{1}{2\pi} X(w)\circledast\left[\pi\delta(w) - \mathit{P\!V}\frac{i}{w} \right](w)\\
& = & \frac{X(0)}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iX(\xi)}{\xi-w}\ d\xi \tag{Eq. 6}\label{Eq. 6}
\end{array} \circledast \mathit{P\!V} \epsilon \mathit{P\!V}\int\limits_{-\infty}^{\infty}f(x)\ dx =\lim\limits_{c\rightarrow 0^+}\left[\int\limits_{-\infty}^{\epsilon-c}f(x)\ dx +\int\limits_{\epsilon+c}^{\infty}f(x)\ dx \right] X(w) = U(w)+iV(w) \begin{array}{r c l}
X(-w)  & =  & \overline{X(w)}\\
\Rightarrow U(-w) + iV(-w) & = & U(w)-iV(w) \\
\Rightarrow U(-w) & = & U(w)\,\,\text{even function}\\
V(-w) & = & -V(w)\,\,\text{odd function}\\
\Rightarrow V(-0) = -V(0) & \Rightarrow & V(0) = 0
\end{array} \begin{array}{r c l}
U(w)+iV(w) & = & \frac{U(0)}{2}+\frac{i\require{cancel}\cancel{V(0)}}{2}-\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{iU(\xi)}{\xi-w}\ d\xi +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
\text{pairing real and imaginary terms}\,\Rightarrow U(w) & = & \frac{U(0)}{2} +\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{2\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 7}\label{Eq. 7}
\end{array} U(0) 1/2 \begin{array}{r c l}
U(w) & = & \frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{V(\xi)}{\xi-w}\ d\xi \\
V(w) & = & -\frac{1}{\pi}\mathit{P\!V}\int\limits_{-\infty}^{\infty} \frac{U(\xi)}{\xi-w}\ d\xi\tag{Eq. 8}\label{Eq. 8}
\end{array} t=0\to\infty U(0) V(w) V(w)=0 x(t) f(t) w 1/|w| |w|\to \infty \mathcal{H}\equiv \{x+iy\,|\,y>0;\,x,\ y\in\mathbb{R}\} f(t) F(w) w 1/|w| 1/|w| f(t)","['real-analysis', 'complex-analysis', 'fourier-analysis', 'fourier-transform', 'signal-processing']"
39,Evaluate $\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\mathrm d\theta$ using the Cauchy residue theorem. [duplicate],Evaluate  using the Cauchy residue theorem. [duplicate],\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\mathrm d\theta,"This question already has answers here : Solve $\int_{-\pi}^{\pi}\frac{1}{1+\sin^{^{2}}t}dt$ (4 answers) Closed 1 year ago . The goal is to evaluate $\displaystyle\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\,\mathrm d\theta\;$ using the Cauchy residue theorem. Using the substitution $z=e^{i\theta}$ and $\;\sin\theta =\dfrac{z-1/z}{2i}\;$ with $\mathrm d\theta=\dfrac{\mathrm dz}{iz}\;,\;$ I rewrite the integral as $\displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left[1+\left(\frac{z-1/z}{2i}\right)^2\right]iz}=\int_{|z|=1}\frac{\mathrm dz}{\left(1+\frac{z^2-2+\frac{1}{z^2}}{-4}\right)iz}=$ $\displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left(-4z+z^3-2z+\frac{1}{z}\right)\frac{-i}{4}}=\int_{|z|=1}\frac{\mathrm dz}{\left(z^4-6z^2+1\right)\frac{-i}{4z}}=\int_{|z|=1}\frac{(-4z/i)\mathrm dz}{z^4-6z^2+1}$ I can't spot a mistake in my algebra, though my brain often gets stuck and I can't find the simplest errors. Do you see any error? My problem is that I can't factor the denominator $z^4-6z^2+1$ . I want to do this so that I can find out what the residues are and apply the Cauchy residue theorem. Please show me the algebraic steps to get this integral in my desired form.(where residue theorem is easily applied). Please also point out any mistakes I have made.","This question already has answers here : Solve $\int_{-\pi}^{\pi}\frac{1}{1+\sin^{^{2}}t}dt$ (4 answers) Closed 1 year ago . The goal is to evaluate using the Cauchy residue theorem. Using the substitution and with I rewrite the integral as I can't spot a mistake in my algebra, though my brain often gets stuck and I can't find the simplest errors. Do you see any error? My problem is that I can't factor the denominator . I want to do this so that I can find out what the residues are and apply the Cauchy residue theorem. Please show me the algebraic steps to get this integral in my desired form.(where residue theorem is easily applied). Please also point out any mistakes I have made.","\displaystyle\int^{\pi}_{-\pi}\frac{1}{1+\sin^2\!\theta}\,\mathrm d\theta\; z=e^{i\theta} \;\sin\theta =\dfrac{z-1/z}{2i}\; \mathrm d\theta=\dfrac{\mathrm dz}{iz}\;,\; \displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left[1+\left(\frac{z-1/z}{2i}\right)^2\right]iz}=\int_{|z|=1}\frac{\mathrm dz}{\left(1+\frac{z^2-2+\frac{1}{z^2}}{-4}\right)iz}= \displaystyle\int_{|z|=1}\frac{\mathrm dz}{\left(-4z+z^3-2z+\frac{1}{z}\right)\frac{-i}{4}}=\int_{|z|=1}\frac{\mathrm dz}{\left(z^4-6z^2+1\right)\frac{-i}{4z}}=\int_{|z|=1}\frac{(-4z/i)\mathrm dz}{z^4-6z^2+1} z^4-6z^2+1",['complex-analysis']
40,Laurent series for $\frac{1}{(z-1)^2(z+2)}$,Laurent series for,\frac{1}{(z-1)^2(z+2)},"Find Laurent expansions for $$\frac{1}{(z-1)^2(z+2)}$$ in $A_1 = D(0,1)$ , $A_2 = {z : 1 < |z| < 2}$ , $A_3 = {{z : \sqrt{2} < |z-i| < \sqrt{5}}}$ I've split it into partial fractions and am trying to apply Taylor's expansion to the square term and convergent sum of geometric to the other two. I'm confused because I'm not sure how you're meant to know the expansion point (like $z$ or $z-1$ etc). Please can someone tell me how to do the $A_2$ scenario? Thanks in advance.","Find Laurent expansions for in , , I've split it into partial fractions and am trying to apply Taylor's expansion to the square term and convergent sum of geometric to the other two. I'm confused because I'm not sure how you're meant to know the expansion point (like or etc). Please can someone tell me how to do the scenario? Thanks in advance.","\frac{1}{(z-1)^2(z+2)} A_1 = D(0,1) A_2 = {z : 1 < |z| < 2} A_3 = {{z : \sqrt{2} < |z-i| < \sqrt{5}}} z z-1 A_2","['complex-analysis', 'laurent-series']"
41,"Evaluate $\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz$ where $\gamma = C(0,3)$ with positive orientation.",Evaluate  where  with positive orientation.,"\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz \gamma = C(0,3)","I need to evaluate $\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz$ where $\gamma = C(0,3)$ with positive orientation. We know that: $$\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z^2 - 2z} dz + \int_{\gamma} \frac{e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z(z - 2)} dz + \int_{\gamma} \frac{e^z}{z(z - 2)} dz$$ To evaluate $2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz$ we take: $$2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz = 2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz +  2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz $$ Where $\gamma_1$ is a circle around pole $z = 2$ and $\gamma_2$ is a circle around pole $z = 0$ . Using Cauchy's Integral Formula: $$2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz = 2 * 2 \pi i \frac{ \sin(2)}{2} = 2 \pi i \sin(2)$$ $$2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz = 2 * 2 \pi i \frac{ \sin(0)}{-2} = 0$$ To evaluate $\int_{\gamma} \frac{e^z}{z(z - 2)} dz$ we take: $$\int_{\gamma} \frac{e^z}{z(z - 2)} dz = \int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz + \int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz $$ Where $\gamma_1$ is a circle around pole $z = 2$ and $\gamma_2$ is a circle around pole $z = 0$ . Using Cauchy's Integral Formula: $$\int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz = 2 \pi i \frac{e^2}{2} = \pi i e^2$$ $$\int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz = 2 \pi i \frac{e^0}{-2} = -\pi i$$ Taking everything together we get: $$\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \pi i \sin(2) + \pi i e^2 -\pi i$$ Is that correct? I did it just by looking in the textbook and examples so it may be all wrong.",I need to evaluate where with positive orientation. We know that: To evaluate we take: Where is a circle around pole and is a circle around pole . Using Cauchy's Integral Formula: To evaluate we take: Where is a circle around pole and is a circle around pole . Using Cauchy's Integral Formula: Taking everything together we get: Is that correct? I did it just by looking in the textbook and examples so it may be all wrong.,"\int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz \gamma = C(0,3) \int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z^2 - 2z} dz + \int_{\gamma} \frac{e^z}{z^2 - 2z} dz = 2 \int_{\gamma} \frac{ \sin(z)}{z(z - 2)} dz + \int_{\gamma} \frac{e^z}{z(z - 2)} dz 2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz 2\int_{\gamma} \frac{ \sin(z)}{z(z-2)} dz = 2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz +  2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz  \gamma_1 z = 2 \gamma_2 z = 0 2 \int_{\gamma_1} \dfrac{ \sin(z)/z}{(z-2)}dz = 2 * 2 \pi i \frac{ \sin(2)}{2} = 2 \pi i \sin(2) 2\int_{\gamma_2} \dfrac{ \sin(z)/\{z-2\}}{z}dz = 2 * 2 \pi i \frac{ \sin(0)}{-2} = 0 \int_{\gamma} \frac{e^z}{z(z - 2)} dz \int_{\gamma} \frac{e^z}{z(z - 2)} dz = \int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz + \int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz  \gamma_1 z = 2 \gamma_2 z = 0 \int_{\gamma_1} \dfrac{e^z/z}{(z-2)}dz = 2 \pi i \frac{e^2}{2} = \pi i e^2 \int_{\gamma_2} \dfrac{e^z/\{z-2\}}{z}dz = 2 \pi i \frac{e^0}{-2} = -\pi i \int_{\gamma} \frac{2 \sin(z) +e^z}{z^2 - 2z} dz = 2 \pi i \sin(2) + \pi i e^2 -\pi i","['complex-analysis', 'complex-integration']"
42,Definition of meromorphic function on complex manifolds,Definition of meromorphic function on complex manifolds,,"There are typically four(or more?) definition of meromorphic function on Riemann surface and three definitions of meromorphic function on complex manifolds, I want to show they are equivalent (I will not consider the additional one on Riemann surface as it's specified for that case): Definition 1 Define the sheaf of meromorphic function to be $$\mathcal{M}_X=\coprod_{x \in X} \mathcal{M}_{X, x}$$ with topology that open sets are union of $\{G_x/H_x\mid x\in V\}\subset \mathcal{M}_X$ with $V$ open and $G,H \in \mathcal{O}_X(V)$ . Meromorphic function is the section of the sheaf above. Definition 2 the sheaf of meromorphic function is sheafification of the following sheaf: Let $U$ be an open subset $U = \cup U_i$ for $U_i$ connected component then we have the presheaf $$U\mapsto \Pi_i \text{ Frac}(\mathcal{O}_X(U_i))$$ it's not hard to show the sheaf in definition 1is the Etale space assciated to this presheaf. Definition 3 Let $U \subset \mathbb{C}^n$ be open. A meromorphic function $f$ on $U$ is a function on the complement of a nowhere dense subset $S \subset U$ with the following property: There exist an open cover $U=\bigcup U_i$ and holomorphic functions $g_i, h_i: U_i \rightarrow \mathbb{C}$ with $\left.\left.h_i\right|_{U_i \backslash S} \cdot f\right|_{U_i \backslash S}=\left.g_i\right|_{U_i \backslash S}$ . I was trying to show that definition 3 are equivalent to 1 (and 2), however I don't have good idea. I see the point is the section of the sheaf, is a continuous map from $X\to \mathcal{M}_X$ , with the topology, contains all the $$[(U,G/H)]=\{G_x/H_x \mid x \in U\}$$ being open in the etale space topology. Therefore if $s:X\to \mathcal{M}_X$ such that $s_x = G_x / H_x$ then locally for where $G$ and $H$ are defined, $s_y = G_y/H_y$ for $y\in U$ (that's guaranteed by the continuity of the section). Therefore, locally the information of the section is equivalent to the information of pair $(G,H)$ , definition 3 use that locally information $(G_\alpha,H_\alpha)$ to define the section. And you see that $U_i$ in definition 3 are the sets of open subsets that the representative element $(G,H)$ are defined.","There are typically four(or more?) definition of meromorphic function on Riemann surface and three definitions of meromorphic function on complex manifolds, I want to show they are equivalent (I will not consider the additional one on Riemann surface as it's specified for that case): Definition 1 Define the sheaf of meromorphic function to be with topology that open sets are union of with open and . Meromorphic function is the section of the sheaf above. Definition 2 the sheaf of meromorphic function is sheafification of the following sheaf: Let be an open subset for connected component then we have the presheaf it's not hard to show the sheaf in definition 1is the Etale space assciated to this presheaf. Definition 3 Let be open. A meromorphic function on is a function on the complement of a nowhere dense subset with the following property: There exist an open cover and holomorphic functions with . I was trying to show that definition 3 are equivalent to 1 (and 2), however I don't have good idea. I see the point is the section of the sheaf, is a continuous map from , with the topology, contains all the being open in the etale space topology. Therefore if such that then locally for where and are defined, for (that's guaranteed by the continuity of the section). Therefore, locally the information of the section is equivalent to the information of pair , definition 3 use that locally information to define the section. And you see that in definition 3 are the sets of open subsets that the representative element are defined.","\mathcal{M}_X=\coprod_{x \in X} \mathcal{M}_{X, x} \{G_x/H_x\mid x\in V\}\subset \mathcal{M}_X V G,H \in \mathcal{O}_X(V) U U = \cup U_i U_i U\mapsto \Pi_i \text{ Frac}(\mathcal{O}_X(U_i)) U \subset \mathbb{C}^n f U S \subset U U=\bigcup U_i g_i, h_i: U_i \rightarrow \mathbb{C} \left.\left.h_i\right|_{U_i \backslash S} \cdot f\right|_{U_i \backslash S}=\left.g_i\right|_{U_i \backslash S} X\to \mathcal{M}_X [(U,G/H)]=\{G_x/H_x \mid x \in U\} s:X\to \mathcal{M}_X s_x = G_x / H_x G H s_y = G_y/H_y y\in U (G,H) (G_\alpha,H_\alpha) U_i (G,H)","['complex-analysis', 'differential-geometry', 'smooth-manifolds', 'complex-geometry', 'meromorphic-functions']"
43,The coefficients of the power series $\frac{1}{1-z-z^2}$ centered at $0$ are the numbers in the Fibonacci sequence [duplicate],The coefficients of the power series  centered at  are the numbers in the Fibonacci sequence [duplicate],\frac{1}{1-z-z^2} 0,"This question already has answers here : Summation of Fibonacci numbers. (3 answers) Closed 1 year ago . So I need to show the coefficients of the power series centered about $z=0$ of $\frac{1}{1-z-z^2}$ are $1,1,2,3,5,8,13,21,\dots$ Here's what I have thus far but I am unable to show all coefficients are equal to the coefficients of the series. Let $F(z):= \sum_{n=0}^\infty a_nz^n$ Then \begin{align} \sum_{n=0}^\infty a_nz^n &= a_0 + a_1z + z^2 \sum_{n=0}^\infty (a_{n+2})z^n \\ &= 1+z+ z^2 \sum_{n=0}^\infty (a_{n+1}+a_n)z^n\\ &= 1+ z+ z \sum_{n=0}^\infty a_nz^n - 1 + z^2(\sum_{n=0}^\infty a_nz^n)\\ &= 1+ z(\sum_{n=0}^\infty a_nz^n)+z^2(\sum_{n=0}^\infty a_nz^n) \end{align} Then setting $F(z)= \sum_{n=0}^\infty a_nz^n$ , bringing everything to the other side and dividing we obtain $$F(z) = \frac{1}{1-z-z^2}$$ But I am stuck here. I saw another version of this asked but it uses the Cauchy Integral formula which we have not yet covered in the course. The hint was to write $\sum_{n=0}^\infty a_nz^n$ and use the recursive formula to rewrite the power series.","This question already has answers here : Summation of Fibonacci numbers. (3 answers) Closed 1 year ago . So I need to show the coefficients of the power series centered about of are Here's what I have thus far but I am unable to show all coefficients are equal to the coefficients of the series. Let Then Then setting , bringing everything to the other side and dividing we obtain But I am stuck here. I saw another version of this asked but it uses the Cauchy Integral formula which we have not yet covered in the course. The hint was to write and use the recursive formula to rewrite the power series.","z=0 \frac{1}{1-z-z^2} 1,1,2,3,5,8,13,21,\dots F(z):= \sum_{n=0}^\infty a_nz^n \begin{align}
\sum_{n=0}^\infty a_nz^n &= a_0 + a_1z + z^2 \sum_{n=0}^\infty (a_{n+2})z^n \\
&= 1+z+ z^2 \sum_{n=0}^\infty (a_{n+1}+a_n)z^n\\
&= 1+ z+ z \sum_{n=0}^\infty a_nz^n - 1 + z^2(\sum_{n=0}^\infty a_nz^n)\\
&= 1+ z(\sum_{n=0}^\infty a_nz^n)+z^2(\sum_{n=0}^\infty a_nz^n)
\end{align} F(z)= \sum_{n=0}^\infty a_nz^n F(z) = \frac{1}{1-z-z^2} \sum_{n=0}^\infty a_nz^n","['complex-analysis', 'generating-functions', 'fibonacci-numbers']"
44,Find all entire functions $f$ such that $|f(z)| \leq |\sin(z)|$,Find all entire functions  such that,f |f(z)| \leq |\sin(z)|,"I am trying to solve the following exercise: Find all entire functions $f$ such that $|f(z)| \leq |\sin(z)|$ , $\forall z \in \mathbb{C}$ I think Liouville's Theorem is the way to go. Liouville's Theorem states that: Every bounded entire function must be constant. Since $\cos(z)=0$ for $z=\frac{2k+1}{2} \pi$ , my answer would be that the only entire function is the zero function $g\equiv 0$ . Am I correct? Edit: I got a little bit confused, because in $\mathbb{R}$ , sin is bounded with $|\sin(x)|<1$ . Because of this I thought that I only need to search constant functions f, such that $|f(z)| \leq |\sin(z)|$ . This is why I thought that the Zero-Function is the only option. Considering the comments, $f(z):= a \sin(z)$ with $|a| \leq 1$ also fullfill the condition wanted. How can I proof that these are all function?","I am trying to solve the following exercise: Find all entire functions such that , I think Liouville's Theorem is the way to go. Liouville's Theorem states that: Every bounded entire function must be constant. Since for , my answer would be that the only entire function is the zero function . Am I correct? Edit: I got a little bit confused, because in , sin is bounded with . Because of this I thought that I only need to search constant functions f, such that . This is why I thought that the Zero-Function is the only option. Considering the comments, with also fullfill the condition wanted. How can I proof that these are all function?",f |f(z)| \leq |\sin(z)| \forall z \in \mathbb{C} \cos(z)=0 z=\frac{2k+1}{2} \pi g\equiv 0 \mathbb{R} |\sin(x)|<1 |f(z)| \leq |\sin(z)| f(z):= a \sin(z) |a| \leq 1,['complex-analysis']
45,"Constant $A$: $|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$ for all $m\in \mathbb{N}_{>0}$, $\theta \in [0, 2\pi)$","Constant :  for all ,","A |e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0 m\in \mathbb{N}_{>0} \theta \in [0, 2\pi)","Problem : Prove that there exists some real constant $A$ such that $$|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$$ for any natural numbers $m \geq 1$ and any real number $0 \leq \theta < 2\pi$ . Context: I am currently working on my bachelor's thesis and trying to show that the following contour integral tends towards zero for fixed $\Re(z) < 0$ for large natural numbers $m$ : $\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^w - 1}dw$ . I could already find the following: \begin{equation}  |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}|  \end{equation} but to continue I am trying to show the following inequality for any natural numbers $m \geq 1$ and any real number $0 \leq \theta < 2\pi$ : $$|e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0$$ for some real constant $A$ . I do not need to know the specific constant but just that there is one. With this I could conclude: \begin{equation}  |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}|      \leq  \thinspace2\pi (2m+1)\pi \frac{((2m+1)\pi)^{\Re(z)-1}}{A}     =  \thinspace 2\pi \frac{((2m+1)\pi)^{\Re(z)}}{A} \xrightarrow{m\rightarrow\infty} 0. \end{equation} The only problem is that I have no clue where to start, could someone give me an idea of how to maybe find something? I know that graphically $A \approx 1$ but besides that I am stuck. Edit: I tried approaching this with the inverse triangle inequality but that won't get us there because if we write: $|e^{(2k+1)\pi e^{i\theta}}-1|\geq |e^{(2k+1)\pi \cos\theta}-1| $ then we can no longer find such an $A$ since $\theta = \frac{\pi}{2}$ will result in |1-1| = 0. So I am asking if someone maybe knows a different approach.","Problem : Prove that there exists some real constant such that for any natural numbers and any real number . Context: I am currently working on my bachelor's thesis and trying to show that the following contour integral tends towards zero for fixed for large natural numbers : . I could already find the following: but to continue I am trying to show the following inequality for any natural numbers and any real number : for some real constant . I do not need to know the specific constant but just that there is one. With this I could conclude: The only problem is that I have no clue where to start, could someone give me an idea of how to maybe find something? I know that graphically but besides that I am stuck. Edit: I tried approaching this with the inverse triangle inequality but that won't get us there because if we write: then we can no longer find such an since will result in |1-1| = 0. So I am asking if someone maybe knows a different approach.","A |e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0 m \geq 1 0 \leq \theta < 2\pi \Re(z) < 0 m \int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^w - 1}dw \begin{equation}
 |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}| 
\end{equation} m \geq 1 0 \leq \theta < 2\pi |e^{(2m+1)\pi e^{i\theta}}-1| \geq A > 0 A \begin{equation}
 |\int_{|w| = (2m+1)\pi} \frac{(-w)^{z-1}}{e^{w} - 1} \thinspace dw| \leq\thinspace 2\pi (2m+1)\pi \max_{|w| =(2m+1)\pi}|\frac{(-w)^{z-1}}{e^{w} - 1}| 
    \leq  \thinspace2\pi (2m+1)\pi \frac{((2m+1)\pi)^{\Re(z)-1}}{A}
    =  \thinspace 2\pi \frac{((2m+1)\pi)^{\Re(z)}}{A} \xrightarrow{m\rightarrow\infty} 0.
\end{equation} A \approx 1 |e^{(2k+1)\pi e^{i\theta}}-1|\geq |e^{(2k+1)\pi \cos\theta}-1|  A \theta = \frac{\pi}{2}","['complex-analysis', 'inequality', 'complex-numbers', 'absolute-value']"
46,Fitting a statement in a long list of equivalent results,Fitting a statement in a long list of equivalent results,,"Theorem. Suppose that $D\subset \mathbb C$ is a connected open set. The following are equivalent. Either $D=\mathbb C$ or $D$ is conformally equivalent to $\mathbb D$ . $D$ is homeomorphic to $\mathbb D$ . $D$ is simply connected. Ind( $\gamma,z)=0$ for every smooth closed curve $\gamma$ in $D$ and every $z\in\mathbb C\backslash D$ . $\mathbb C_{\infty}\backslash D$ is connected. If $f\in H(D)$ then there exists a sequence of polynomials $(P_{n})$ such that $P_{n}\to f$ uniformly on compact subsets of $D$ . $\int_{\gamma} f(z)\,dz=0$ for every $f\in H(D)$ and every smooth closed curve $\gamma$ in $D$ . If $f\in H(D)$ then there exists $F\in H(D)$ with $F'=f$ . If $u:D\to \mathbb R$ is harmonic then $u=\mathbb R$ e $(f)$ for some $f\in H(D)$ . If $f\in H(D)$ has no zero in $D$ then there exists $L\in H(D)$ with $f=e^{L}$ . If $f\in H(D)$ has no zero in $D$ then there exists $g\in H(D)$ with $g^{2}=f$ . The proof to this theorem can be found in David C. Ullrich's book Complex Made Simple, and it is surprisingly nice that it was proven as a chain (as in $1\implies 2\implies\cdots\implies 11\implies 1$ ). Let $D$ and $G$ be connected open subsets of $\mathbb C$ with $D\subset G$ . If $(f,D)$ is a function element which admits continuation along every path in $G$ which starts at a point of $D$ then we say that $(f,D)$ admits unrestricted continuation in $G$ . Now, consider the following: Proposition. Let $D$ be an open connected subset of the plane. $D$ is simply connected if and only if for every functional element $(f,A)$ , $A\subset D$ that admits unrestricted continuation in $D$ there exists a function $F\in H(D)$ such that $f=F\rvert_{A}$ . Between what numbers in the theorem would this proposition fit the best, in order to obtain a chain-like proof of the theorem?","Theorem. Suppose that is a connected open set. The following are equivalent. Either or is conformally equivalent to . is homeomorphic to . is simply connected. Ind( for every smooth closed curve in and every . is connected. If then there exists a sequence of polynomials such that uniformly on compact subsets of . for every and every smooth closed curve in . If then there exists with . If is harmonic then e for some . If has no zero in then there exists with . If has no zero in then there exists with . The proof to this theorem can be found in David C. Ullrich's book Complex Made Simple, and it is surprisingly nice that it was proven as a chain (as in ). Let and be connected open subsets of with . If is a function element which admits continuation along every path in which starts at a point of then we say that admits unrestricted continuation in . Now, consider the following: Proposition. Let be an open connected subset of the plane. is simply connected if and only if for every functional element , that admits unrestricted continuation in there exists a function such that . Between what numbers in the theorem would this proposition fit the best, in order to obtain a chain-like proof of the theorem?","D\subset \mathbb C D=\mathbb C D \mathbb D D \mathbb D D \gamma,z)=0 \gamma D z\in\mathbb C\backslash D \mathbb C_{\infty}\backslash D f\in H(D) (P_{n}) P_{n}\to f D \int_{\gamma} f(z)\,dz=0 f\in H(D) \gamma D f\in H(D) F\in H(D) F'=f u:D\to \mathbb R u=\mathbb R (f) f\in H(D) f\in H(D) D L\in H(D) f=e^{L} f\in H(D) D g\in H(D) g^{2}=f 1\implies 2\implies\cdots\implies 11\implies 1 D G \mathbb C D\subset G (f,D) G D (f,D) G D D (f,A) A\subset D D F\in H(D) f=F\rvert_{A}","['complex-analysis', 'analytic-continuation']"
47,How to approach the problem of summation of Eisenstein series on shifted lattices?,How to approach the problem of summation of Eisenstein series on shifted lattices?,,"This question is an attempt to complete the issues discussed in a previous question of mine ( How did Gauss sum Eisenstein series? ), since my updated question did not recieve any attention. In my previous question, I mentioned that Gauss wrote an infinite series for the logarithm of the denominator of $\mathbb{sinlemn}(z) = \frac{M(z)}{N(z)}$ . Since $$N(z) = \prod (1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ (that is, $\mathbb{sinlemn}(z)$ has poles at Gaussian half-integers multiples of $\varpi$ ). Gauss wrote: $$\mathbb{log}N(z) =\frac{1}{12}z^4 - \frac{1}{280}z^8 +\frac{1}{4950}z^{12} - ...$$ . Since the logarithm of a infinite product equals an infinite sum of logarithms, one gets that: $$\mathbb{log}N(z) = \sum \mathbb{log}(1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ , and by the taylor series expansion of $\mathbb{log}(1-z)$ one gets that Gauss's infinite series for $\mathbb{log}N(z)$ is equivalent to the summation of a kind of ""generalized Eisenstein series"" in which the lattice is shifted by $(\frac{1}{2}+\frac{1}{2}i)\varpi$ . Since  such shifted lattice cannot be generated by some action of the modular group on the lattice of Gaussian integers (if it was, one could use the modularity of the Eisenstein series and deduce the series for $\mathbb{log}N(z)$ from that of $\mathbb{log}M(z)$ ), I wondered what tools enable to sum such series and what was Gauss's original method in this case. I tried to make a Google search about ""modular forms defined on shifted lattices"", but without success. Side remark I have no intention to ""spam"" StackExchange Mathematics with multitudes of more or less similar questions, so I have no problem to close this question and instead get an answer to my original (updated) question, if other users will vote to do so.","This question is an attempt to complete the issues discussed in a previous question of mine ( How did Gauss sum Eisenstein series? ), since my updated question did not recieve any attention. In my previous question, I mentioned that Gauss wrote an infinite series for the logarithm of the denominator of . Since (that is, has poles at Gaussian half-integers multiples of ). Gauss wrote: . Since the logarithm of a infinite product equals an infinite sum of logarithms, one gets that: , and by the taylor series expansion of one gets that Gauss's infinite series for is equivalent to the summation of a kind of ""generalized Eisenstein series"" in which the lattice is shifted by . Since  such shifted lattice cannot be generated by some action of the modular group on the lattice of Gaussian integers (if it was, one could use the modularity of the Eisenstein series and deduce the series for from that of ), I wondered what tools enable to sum such series and what was Gauss's original method in this case. I tried to make a Google search about ""modular forms defined on shifted lattices"", but without success. Side remark I have no intention to ""spam"" StackExchange Mathematics with multitudes of more or less similar questions, so I have no problem to close this question and instead get an answer to my original (updated) question, if other users will vote to do so.",\mathbb{sinlemn}(z) = \frac{M(z)}{N(z)} N(z) = \prod (1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi}) \mathbb{sinlemn}(z) \varpi \mathbb{log}N(z) =\frac{1}{12}z^4 - \frac{1}{280}z^8 +\frac{1}{4950}z^{12} - ... \mathbb{log}N(z) = \sum \mathbb{log}(1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi}) \mathbb{log}(1-z) \mathbb{log}N(z) (\frac{1}{2}+\frac{1}{2}i)\varpi \mathbb{log}N(z) \mathbb{log}M(z),"['complex-analysis', 'math-history', 'elliptic-functions']"
48,Calculate $\lim_{j \rightarrow \infty} \int_0^j (1+\frac{x}{j})^j e^{-\pi x}dx$,Calculate,\lim_{j \rightarrow \infty} \int_0^j (1+\frac{x}{j})^j e^{-\pi x}dx,"My approach: $$\lim_{j \rightarrow \infty} \int_0^j \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx =  \lim_{j \rightarrow \infty} \int_0^{\infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx.$$ I'm not sure if I can do the above, but I need it to apply Lebesgue as follows: I see that $(1+\frac{x}{j})^j \rightarrow e^x$ and $(1+\frac{x}{j})^j$ is monotonic increasing, so $e^x$ is a dominating function that is integrable. Hence I can apply Lebesgue: \begin{align*} \lim_{j \rightarrow \infty} \int_0^{\infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x} dx & = \int_0^{\infty} \lim_{j \rightarrow \infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx \\ &= \int_0^{\infty} e^x e^{-\pi x} dx= \int_0^{\infty} e^{(1-\pi) x} dx= - \frac{1}{1-\pi}. \end{align*} My colleagues all have a different result though and Wolfram Alpha exceeds computation time. I cannot spot a mistake.","My approach: I'm not sure if I can do the above, but I need it to apply Lebesgue as follows: I see that and is monotonic increasing, so is a dominating function that is integrable. Hence I can apply Lebesgue: My colleagues all have a different result though and Wolfram Alpha exceeds computation time. I cannot spot a mistake.","\lim_{j \rightarrow \infty} \int_0^j \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx = 
\lim_{j \rightarrow \infty} \int_0^{\infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx. (1+\frac{x}{j})^j \rightarrow e^x (1+\frac{x}{j})^j e^x \begin{align*}
\lim_{j \rightarrow \infty} \int_0^{\infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x} dx & = \int_0^{\infty} \lim_{j \rightarrow \infty} \left(1+\frac{x}{j}\right)^j e^{-\pi x}dx \\ &= \int_0^{\infty} e^x e^{-\pi x} dx= \int_0^{\infty} e^{(1-\pi) x} dx= - \frac{1}{1-\pi}.
\end{align*}","['real-analysis', 'integration', 'complex-analysis', 'lebesgue-integral']"
49,"Ahlfors page 123: Compute $\int_{|z|=2}z^n(1-z)^mdz$. What happens when $n,m<0$? Residue Theory? Cauchy's Theorem?",Ahlfors page 123: Compute . What happens when ? Residue Theory? Cauchy's Theorem?,"\int_{|z|=2}z^n(1-z)^mdz n,m<0","This is a question from Ahlfors, page $123$ , number $1b$ :  Compute $\int_{|z|=2}z^n(1-z)^mdz$ .  He doesn't specify anything about $n$ and $m$ on the page, so I am not sure if they are natural numbers or integers.  If they are natural numbers, then the integrand is analytic and so the integral is $0$ , so I assume they aren't natural.  So, I imagine we consider cases depending on which $n$ or $m$ is less than $0$ and which one is greater than or equal to $0$ .  When doing so, we either get a pole of order $n$ at $0$ or a pole of order $m$ at $1$ .  I can write these out in the standard residue form, but is there a way to ""clean them up"", so to speak?  In particular, if you were, say, teaching a class on Complex Analysis, what would you expect from your students?  (note: I am not a current student, just looking back through some old notes and problems from several years ago). EDIT: In particular, I am talking about, for instance, when $n\geq 0$ and $m<0$ , then we have $\int_{|z|=2}\frac{z^n}{(1-z)^m}dz=\frac{1}{(m-1)!}\lim_{z\rightarrow m}\frac{z^{m-1}}{dz^{m-1}}z^n$ .  But, $m<0$ , so what is the negative-th derivative of $z^n$ ?  Similarly, we can consider the case when $n<0$ , $m\geq 0$ and $n,m<0$ .  (I feel like I am missing some cases).  Moreover, this problem comes in Ahlfors book BEFORE Residue theory.  So, maybe there is a better way to tackle it?  I just wasn't seeing a nice way of using Cauchy's theorem. EDIT (number 2): The question was asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ for $n,m\in\Bbb Z$ , but I'm finding issues with the answer.  In particular, I am not seeing how they arrived to their answer, and they are only dealing with one case. EDIT (number 3, last update): The question was also asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ , handling most of the cases.  However, the case whenever both $n,m$ are strictly negative is still not clear to me.","This is a question from Ahlfors, page , number :  Compute .  He doesn't specify anything about and on the page, so I am not sure if they are natural numbers or integers.  If they are natural numbers, then the integrand is analytic and so the integral is , so I assume they aren't natural.  So, I imagine we consider cases depending on which or is less than and which one is greater than or equal to .  When doing so, we either get a pole of order at or a pole of order at .  I can write these out in the standard residue form, but is there a way to ""clean them up"", so to speak?  In particular, if you were, say, teaching a class on Complex Analysis, what would you expect from your students?  (note: I am not a current student, just looking back through some old notes and problems from several years ago). EDIT: In particular, I am talking about, for instance, when and , then we have .  But, , so what is the negative-th derivative of ?  Similarly, we can consider the case when , and .  (I feel like I am missing some cases).  Moreover, this problem comes in Ahlfors book BEFORE Residue theory.  So, maybe there is a better way to tackle it?  I just wasn't seeing a nice way of using Cauchy's theorem. EDIT (number 2): The question was asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ for $n,m\in\Bbb Z$ , but I'm finding issues with the answer.  In particular, I am not seeing how they arrived to their answer, and they are only dealing with one case. EDIT (number 3, last update): The question was also asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ , handling most of the cases.  However, the case whenever both are strictly negative is still not clear to me.","123 1b \int_{|z|=2}z^n(1-z)^mdz n m 0 n m 0 0 n 0 m 1 n\geq 0 m<0 \int_{|z|=2}\frac{z^n}{(1-z)^m}dz=\frac{1}{(m-1)!}\lim_{z\rightarrow m}\frac{z^{m-1}}{dz^{m-1}}z^n m<0 z^n n<0 m\geq 0 n,m<0 n,m","['complex-analysis', 'complex-integration', 'residue-calculus', 'cauchy-integral-formula']"
50,"Elliptic curves, $j$-invariant and example of $j(\Lambda)=0$","Elliptic curves, -invariant and example of",j j(\Lambda)=0,"First, consider $\Lambda=\mathbb{Z}\bigoplus\omega\mathbb{Z}$ with $\omega$ the third root of unity in the upper half plane. I know that the lattice is such that $g_2(\Lambda)=0$ , where $g_2$ is the coefficient of the differential equation $$(\wp')^2=4\wp^3+g_2(\Lambda)\wp+g_3(\Lambda).$$ We know that for any $\tau\in\mathbb{C}$ $$g_3(\tau\Lambda)=\frac{1}{\tau^6}g_3(\Lambda).$$ We also know that $g_2(\tau\Lambda)=0$ for all $\tau$ . Does this mean that given a curve of the kind $y^2=4x^3+b $ we can map $\mathbb{C}/\Lambda$ to it by just taking the $\wp$ function of the lattice $\tau\mathbb{Z}\bigoplus\tau\omega\mathbb{Z}$ , $\tau=\sqrt[6]{\frac{g_3(\Lambda)}{b}}$ ? More in general, by the lifting properties we know that the only way two complex tori are isomorphic is if their lattices satisfy $\Lambda'=\tau\Lambda$ , in particular all the elliptic curves given before are isomorphic (obviously). We can also see that $$j(\Lambda)=1728\frac{g_2(\Lambda)^3}{g_2(\Lambda)^3-27g_3(\Lambda)^2}=j(\tau\Lambda),$$ so essentially it is invariant under isomorphisms of tori. Is this the motivation behind the definition of modular form(function), which is an equivalent condition for the above homogeneity condition? Another question is: Is it possible to show, in a not too sophisticated way, that the converse also holds, hence that the $j$ -invariant is essentially injective up to isomorphism of lattices, without using similar results about the corresponding elliptic curves, and that $g_2$ and $g_3$ are surjective once we know the surjectivity of $j$ (which I have shown)?","First, consider with the third root of unity in the upper half plane. I know that the lattice is such that , where is the coefficient of the differential equation We know that for any We also know that for all . Does this mean that given a curve of the kind we can map to it by just taking the function of the lattice , ? More in general, by the lifting properties we know that the only way two complex tori are isomorphic is if their lattices satisfy , in particular all the elliptic curves given before are isomorphic (obviously). We can also see that so essentially it is invariant under isomorphisms of tori. Is this the motivation behind the definition of modular form(function), which is an equivalent condition for the above homogeneity condition? Another question is: Is it possible to show, in a not too sophisticated way, that the converse also holds, hence that the -invariant is essentially injective up to isomorphism of lattices, without using similar results about the corresponding elliptic curves, and that and are surjective once we know the surjectivity of (which I have shown)?","\Lambda=\mathbb{Z}\bigoplus\omega\mathbb{Z} \omega g_2(\Lambda)=0 g_2 (\wp')^2=4\wp^3+g_2(\Lambda)\wp+g_3(\Lambda). \tau\in\mathbb{C} g_3(\tau\Lambda)=\frac{1}{\tau^6}g_3(\Lambda). g_2(\tau\Lambda)=0 \tau y^2=4x^3+b  \mathbb{C}/\Lambda \wp \tau\mathbb{Z}\bigoplus\tau\omega\mathbb{Z} \tau=\sqrt[6]{\frac{g_3(\Lambda)}{b}} \Lambda'=\tau\Lambda j(\Lambda)=1728\frac{g_2(\Lambda)^3}{g_2(\Lambda)^3-27g_3(\Lambda)^2}=j(\tau\Lambda), j g_2 g_3 j","['complex-analysis', 'algebraic-geometry', 'modular-forms']"
51,Generalizing Euler's infinite product of cosines,Generalizing Euler's infinite product of cosines,,"The following formula is attributed to Euler: $$\frac{\sin(2 k)}{2 k} = \prod_{n=0}^\infty \cos(k \frac{1}{2^n})$$ This can be shown through $m$ applications of $\sin(x) = 2 \sin(x/2) \cos(x/2)$ to find $\sin(x) = 2^m \sin(x/2^m) \prod_{n=1}^m \cos(\frac{x}{2^n})$ . Taking $m \rightarrow \infty$ and replacing $x$ with $2k$ completes the proof. There are simple generalizations for other powers of $2$ inside the parentheses, such as $$\frac{\sin(2 k)}{2 k}\frac{\sin(\sqrt{2} k)}{\sqrt{2} k} = \prod_{n=0}^\infty \cos( k\frac{1}{\sqrt{2}^n})$$ which follow from separating the terms with even and odd powers $n$ . There are yet more generalizations that start from $$\frac{\sin(x)}{x} = \prod_{n=1}^{\infty} \frac{\sin(\frac{x}{q^{n-1}})}{n \sin(\frac{x}{q^{n}})}$$ and are most easily seen by taking the upper bound on the product to be $m$ , telescoping the terms in the product, and then taking $m \rightarrow \infty$ . Now for the question. Consider the following function: $$f_{\lambda, r}(k) = \prod_{n=0}^\infty\left(\cos(k\lambda^n)+ i r \sin(k\lambda^n) \right)$$ Note that it is clear that when $r=1$ , the formula above reduces to $e^{i \frac{k}{1-\lambda}}$ , and when $r=0$ and $\lambda=1/2, 1/\sqrt{2}$ , the formula reduces to the two cases at the top of the question. Are there specific cases of $\lambda$ and $r$ when one can simplify $f_{\lambda, r}(k)$ to some simple function of $k$ ? In particular, I would like the formula to hold for all $k$ , but I am happy with fixed, nontrivial choices of $\lambda$ and $r$ ( $\lambda \neq -1,0,1$ , $r\neq -1,0,1$ ). As an example, an evaluation for the case $r=1/2$ and $\lambda = 1/2$ would suffice.","The following formula is attributed to Euler: This can be shown through applications of to find . Taking and replacing with completes the proof. There are simple generalizations for other powers of inside the parentheses, such as which follow from separating the terms with even and odd powers . There are yet more generalizations that start from and are most easily seen by taking the upper bound on the product to be , telescoping the terms in the product, and then taking . Now for the question. Consider the following function: Note that it is clear that when , the formula above reduces to , and when and , the formula reduces to the two cases at the top of the question. Are there specific cases of and when one can simplify to some simple function of ? In particular, I would like the formula to hold for all , but I am happy with fixed, nontrivial choices of and ( , ). As an example, an evaluation for the case and would suffice.","\frac{\sin(2 k)}{2 k} = \prod_{n=0}^\infty \cos(k \frac{1}{2^n}) m \sin(x) = 2 \sin(x/2) \cos(x/2) \sin(x) = 2^m \sin(x/2^m) \prod_{n=1}^m \cos(\frac{x}{2^n}) m \rightarrow \infty x 2k 2 \frac{\sin(2 k)}{2 k}\frac{\sin(\sqrt{2} k)}{\sqrt{2} k} = \prod_{n=0}^\infty \cos( k\frac{1}{\sqrt{2}^n}) n \frac{\sin(x)}{x} = \prod_{n=1}^{\infty} \frac{\sin(\frac{x}{q^{n-1}})}{n \sin(\frac{x}{q^{n}})} m m \rightarrow \infty f_{\lambda, r}(k) = \prod_{n=0}^\infty\left(\cos(k\lambda^n)+ i r \sin(k\lambda^n) \right) r=1 e^{i \frac{k}{1-\lambda}} r=0 \lambda=1/2, 1/\sqrt{2} \lambda r f_{\lambda, r}(k) k k \lambda r \lambda \neq -1,0,1 r\neq -1,0,1 r=1/2 \lambda = 1/2","['complex-analysis', 'infinite-product']"
52,Selberg Integral - Change of Variables,Selberg Integral - Change of Variables,,"I am trying to evaluate a Selberg-Like integral and I need to make the integral separable (unbinding the variables), but I can't come up with an appropriate change of variables. The integral is: $$\underset{[0,1]^m}{\int\dots\int} d^2z_1\dots d^2z_m \prod_{i<j} |z_i-z_j|^{-\gamma^2} f(z_1,\dots, z_m).$$ I am thinking about this as a function of $\gamma$ , so the function $f(z_1,\dots, z_m)$ doesn't really matter yet. I need to introduce a change of variables such that $\prod_{i<j} |z_i-z_)|^{-\gamma^2}$ can be split into terms that include only one variable each. The case with $m=2$ is easy (polar coordinates). For $m=3$ , I tried the following: $x_1-x_2=y_1$ and $x_1-x_3=y_1y_2$ . This works fine when $m=3$ , but I am not sure it can be extended to any $m\in\mathbb{N}$ .","I am trying to evaluate a Selberg-Like integral and I need to make the integral separable (unbinding the variables), but I can't come up with an appropriate change of variables. The integral is: I am thinking about this as a function of , so the function doesn't really matter yet. I need to introduce a change of variables such that can be split into terms that include only one variable each. The case with is easy (polar coordinates). For , I tried the following: and . This works fine when , but I am not sure it can be extended to any .","\underset{[0,1]^m}{\int\dots\int} d^2z_1\dots d^2z_m \prod_{i<j} |z_i-z_j|^{-\gamma^2} f(z_1,\dots, z_m). \gamma f(z_1,\dots, z_m) \prod_{i<j} |z_i-z_)|^{-\gamma^2} m=2 m=3 x_1-x_2=y_1 x_1-x_3=y_1y_2 m=3 m\in\mathbb{N}","['complex-analysis', 'self-learning', 'change-of-variable']"
53,Sequence of unit disk automorphisms converge l.u. to an automorphism,Sequence of unit disk automorphisms converge l.u. to an automorphism,,"Given $\{f_n\}$ sequence of automorphisms on the (open) unit disk converging locally uniformly to some nonconstant function $f$ . Show that $f$ is an automorphism of the open unit disk. For injectivity we may use Hurwitz theorem (and identity theorem). For surjectivity I wanted to use maximum modulus principle on $\frac{1}{f(z)-w}$ for some $w \notin f(D)$ , but $f$ is not defined on the unit circle...?","Given sequence of automorphisms on the (open) unit disk converging locally uniformly to some nonconstant function . Show that is an automorphism of the open unit disk. For injectivity we may use Hurwitz theorem (and identity theorem). For surjectivity I wanted to use maximum modulus principle on for some , but is not defined on the unit circle...?",\{f_n\} f f \frac{1}{f(z)-w} w \notin f(D) f,"['complex-analysis', 'convergence-divergence']"
54,Integro-Differential equation from my Complex analysis exam,Integro-Differential equation from my Complex analysis exam,,"My recent complex analysis exam had the following problem as the last question, which I had a hard time solving. The problem Use the Laplace transform to solve the following differential equation for $u(t)$ . $$\dfrac{du(t)}{dt}+\dfrac{1}{2}\int_{0}^{t} e^{-t'}u(t-t')\,dt'=0$$ with the initial condition $u(0)=1.$ My attempt When applying the Laplace transform, the first term becomes $s\hat{u}(s)-u(0)=s\hat{u}(s)-1$ For the second term I used the formula for a Laplace transform of a convolution integral $$ L\bigg\{ \int_{0}^{t} g(\tau)f(t-\tau)\,d\tau\bigg\} = \hat{f}(s)\hat{g}(s) $$ This approach meant that the second term would be $\dfrac{1}{2}e^{-s}\hat{u}(s)$ After isolationg for $\hat{u}(s)$ I had $$ \hat{u}(s)= \dfrac{1}{s+\dfrac{1}{2}e^{-s}} $$ I then tried to apply the inverse Laplace transform $$u(s)=\dfrac{1}{2\pi i}\int_{\lambda - i\infty}^{\lambda + i\infty} \dfrac{e^{st}}{s+\dfrac{1}{2}e^{-s}}\,ds$$ When trying to find a singular point in the integrand, I found the solution $s=\mathrm{LambertW}\left(-\dfrac{1}{2}\right)$ I am not entirely familiar with the LambertW-function, and my attempt ended here. My question Did I make any mistakes leading up to the inverse Laplace? Is my approach even correct? How would you go about solving? Is this considered an Integro-differential-equation? Thanks for your time. :)","My recent complex analysis exam had the following problem as the last question, which I had a hard time solving. The problem Use the Laplace transform to solve the following differential equation for . with the initial condition My attempt When applying the Laplace transform, the first term becomes For the second term I used the formula for a Laplace transform of a convolution integral This approach meant that the second term would be After isolationg for I had I then tried to apply the inverse Laplace transform When trying to find a singular point in the integrand, I found the solution I am not entirely familiar with the LambertW-function, and my attempt ended here. My question Did I make any mistakes leading up to the inverse Laplace? Is my approach even correct? How would you go about solving? Is this considered an Integro-differential-equation? Thanks for your time. :)","u(t) \dfrac{du(t)}{dt}+\dfrac{1}{2}\int_{0}^{t} e^{-t'}u(t-t')\,dt'=0 u(0)=1. s\hat{u}(s)-u(0)=s\hat{u}(s)-1  L\bigg\{ \int_{0}^{t} g(\tau)f(t-\tau)\,d\tau\bigg\} = \hat{f}(s)\hat{g}(s)  \dfrac{1}{2}e^{-s}\hat{u}(s) \hat{u}(s)  \hat{u}(s)= \dfrac{1}{s+\dfrac{1}{2}e^{-s}}  u(s)=\dfrac{1}{2\pi i}\int_{\lambda - i\infty}^{\lambda + i\infty} \dfrac{e^{st}}{s+\dfrac{1}{2}e^{-s}}\,ds s=\mathrm{LambertW}\left(-\dfrac{1}{2}\right)","['complex-analysis', 'laplace-transform', 'integro-differential-equations']"
55,"Evaluating $\sum_{k=-m}^{m}(-1)^k \, e^{-\frac{i\pi k}{m}}\frac{2\left(ze^{\frac{i\pi k}{m}}\right)^2}{\left(ze^{\frac{i\pi k}{m}}\right)^2 - q^2}$",Evaluating,"\sum_{k=-m}^{m}(-1)^k \, e^{-\frac{i\pi k}{m}}\frac{2\left(ze^{\frac{i\pi k}{m}}\right)^2}{\left(ze^{\frac{i\pi k}{m}}\right)^2 - q^2}","From quite some time I'm struggling on proving that: $$\sum_{k=-n}^{n}(-1)^k \, e^{-\frac{i\pi k}{m}}\dfrac{2\left(ze^{\frac{i\pi k}{m}}\right)^2}{\left(ze^{\frac{i\pi k}{m}}\right)^2 - q^2} = 2z^2m\,\dfrac{q^{m-1}z^{m-1}}{z^{2m} - q^{2m}} $$ where $m = 2n+1$ . My intuition to prove this would be proving that the partial fraction expansion of the right-hand side coincides with the left-hand side. However I've no idea if that would get us anywhere. I'm looking for a real analytic proof but a proof using complex analysis and residue theorem would be most welcomed. Any help or hints would be highly appreciated. Thanks for reading.",From quite some time I'm struggling on proving that: where . My intuition to prove this would be proving that the partial fraction expansion of the right-hand side coincides with the left-hand side. However I've no idea if that would get us anywhere. I'm looking for a real analytic proof but a proof using complex analysis and residue theorem would be most welcomed. Any help or hints would be highly appreciated. Thanks for reading.,"\sum_{k=-n}^{n}(-1)^k \, e^{-\frac{i\pi k}{m}}\dfrac{2\left(ze^{\frac{i\pi k}{m}}\right)^2}{\left(ze^{\frac{i\pi k}{m}}\right)^2 - q^2} = 2z^2m\,\dfrac{q^{m-1}z^{m-1}}{z^{2m} - q^{2m}}  m = 2n+1","['real-analysis', 'complex-analysis', 'number-theory', 'closed-form']"
56,Applying Hadamard's Three Lines Theorem to prove that $f(z) = f(r) (z/r)^k$,Applying Hadamard's Three Lines Theorem to prove that,f(z) = f(r) (z/r)^k,"I am currently stuck on a complex analysis exercise, which I find quite difficult, here it is: ""Let $0<r<R$ , define the Annulus as $A_{r,R} := \{z \in \mathbb{C}: r<|z|<R \}$ . Let $f:\overline{A_{r,R}} \rightarrow \overline{A_{\tilde {r},\tilde {R}}}$ be a holomorphic function on $A_{r,R}$ , with $0<r<R<\infty$ , $0<\tilde{r}<\tilde{R}<\infty$ , such that: $|z| =r \implies |f(z)|=\tilde{r}, |z| = R \implies |f(z)| = \tilde{R}$ . Show that there is a natural number $k$ , such that $f(z) = f(r) (z/r)^k$ for all $z \in \overline{A_{r,R}}$ ."" The exercise has a tip: ""Apply Hadamard's Three Lines Theorem on an appropriate function"". I have read something about Hadamard's Three Circle Theorem, which has similar conditions, however, it only says something about the supremums of $|f|$ on specific circles, but here we want to say something about $f$ , so I'm not sure if it's possible to apply it here. My first and only idea was to consider a holomorphic function $g :  S \rightarrow \overline{A_{r,R}}$ , which bijectively maps the strip $ S := \{ z \in \mathbb{C} \cup \{\infty\}: r \leq \Re(z) \leq R\}$ to $\overline{A_{r,R}}$ in a ""Riemann-way"". Then we could consider a map $ \gamma: S \rightarrow \overline{A_{\tilde {r},\tilde {R}}}$ , $\gamma = f \circ g$ . We know that $f$ is bounded by $\tilde{R}$ , so $\gamma$ is a bounded holomorphic function, if we define $\Gamma(x) := \sup \{\gamma(z): \Re(z) = x \}$ , we can apply Hadamard's TLT, so $\Gamma(x) \leq \Gamma(r)^{1-x} \Gamma(R)^x = \tilde{r}^{1-x}\tilde{R}^x$ $\forall x \in [r,R]$ . That's all I have. But what now? I don't see any way to continue, what else could I try, does someone have a tip? Thanks in advance! Edit: We got a solution for this problem, but coming up with the function $g$ doesnt seem very trivial, but the key idea is to apply the Three Lines Theorem on the function $$g(z) := f(r \exp(z \ln(R/r))) / (\tilde{r} \exp(z \ln(\tilde{R} / \tilde{r})))$$ And on its reciprocal $1/g(z)$ to show that $|g(z)| = 1$ . If anyone is interested I can give more details.","I am currently stuck on a complex analysis exercise, which I find quite difficult, here it is: ""Let , define the Annulus as . Let be a holomorphic function on , with , , such that: . Show that there is a natural number , such that for all ."" The exercise has a tip: ""Apply Hadamard's Three Lines Theorem on an appropriate function"". I have read something about Hadamard's Three Circle Theorem, which has similar conditions, however, it only says something about the supremums of on specific circles, but here we want to say something about , so I'm not sure if it's possible to apply it here. My first and only idea was to consider a holomorphic function , which bijectively maps the strip to in a ""Riemann-way"". Then we could consider a map , . We know that is bounded by , so is a bounded holomorphic function, if we define , we can apply Hadamard's TLT, so . That's all I have. But what now? I don't see any way to continue, what else could I try, does someone have a tip? Thanks in advance! Edit: We got a solution for this problem, but coming up with the function doesnt seem very trivial, but the key idea is to apply the Three Lines Theorem on the function And on its reciprocal to show that . If anyone is interested I can give more details.","0<r<R A_{r,R} := \{z \in \mathbb{C}: r<|z|<R \} f:\overline{A_{r,R}} \rightarrow \overline{A_{\tilde {r},\tilde {R}}} A_{r,R} 0<r<R<\infty 0<\tilde{r}<\tilde{R}<\infty |z| =r \implies |f(z)|=\tilde{r}, |z| = R \implies |f(z)| = \tilde{R} k f(z) = f(r) (z/r)^k z \in \overline{A_{r,R}} |f| f g :  S \rightarrow \overline{A_{r,R}}  S := \{ z \in \mathbb{C} \cup \{\infty\}: r \leq \Re(z) \leq R\} \overline{A_{r,R}}  \gamma: S \rightarrow \overline{A_{\tilde {r},\tilde {R}}} \gamma = f \circ g f \tilde{R} \gamma \Gamma(x) := \sup \{\gamma(z): \Re(z) = x \} \Gamma(x) \leq \Gamma(r)^{1-x} \Gamma(R)^x = \tilde{r}^{1-x}\tilde{R}^x \forall x \in [r,R] g g(z) := f(r \exp(z \ln(R/r))) / (\tilde{r} \exp(z \ln(\tilde{R} / \tilde{r}))) 1/g(z) |g(z)| = 1",['complex-analysis']
57,"Show that the ""distinguished power"" of a continuous complex-valued function is well-defined","Show that the ""distinguished power"" of a continuous complex-valued function is well-defined",,"Let $E$ be a normed $\mathbb R$ -vector space $^1$ and $\ln$ denote the principal branch of the complex logarithm. We can show the following result: Theorem : Let $\varphi:E\to\mathbb C\setminus\{0\}$ with $\varphi(0)=1$ . If $\left.\varphi\right|_{\overline B_r(0)}$ is uniformly continuous and $$\inf_{x\in\overline B_r(0)}|\varphi(x)|>0\tag1$$ for all $r>0$ , then there is a unique $f\in C(E,\mathbb C)$ with $f(0)=0$ and $e^f=\varphi$ . Moreover, for every $n\in\mathbb N$ , there is a unique $g_n\in C(E,\mathbb C\setminus\{0\})$ with $g_n(0)=1$ and $g_n^n=\varphi$ ; in fact, $$g_n=e^{\frac fn}\tag2.$$ By this result, the notation $$\varphi^{\frac1n}:=g_n\;\;\;\text{for }n\in\mathbb N$$ is well-defined. Question : For $m,n\in\mathbb N$ , it is tempting to write $\varphi^{\frac mn}$ instead of $g_n^m$ . Are we able to justify this notation by showing that whenver $m,m',n,n'\in\mathbb N$ satisfy $\frac mn=\frac{m'}{n'}$ , then $g_n^m=g_{n'}^{m'}$ ? I wasn't able to show this result, but I'm really sure that it holds. My main problem is that it is intuitively so trivial that it's easy to make a stupid mistake. I wasn't able to utilize this, but we may note $$\left(g_{mn}^m\right)=\varphi\tag3$$ and hence $$g_n=g_{mn}^m\tag4.$$ $^1$ If this generality is preventing you from providing an answer, feel free to assume $E=\mathbb R^d$ (in which case we may further assumpe that $\varphi$ is uniformly continuous (on the whole space) and hence $(1)$ is trivially satisfied by compactness of closed balls in $\mathbb R^d$ ).","Let be a normed -vector space and denote the principal branch of the complex logarithm. We can show the following result: Theorem : Let with . If is uniformly continuous and for all , then there is a unique with and . Moreover, for every , there is a unique with and ; in fact, By this result, the notation is well-defined. Question : For , it is tempting to write instead of . Are we able to justify this notation by showing that whenver satisfy , then ? I wasn't able to show this result, but I'm really sure that it holds. My main problem is that it is intuitively so trivial that it's easy to make a stupid mistake. I wasn't able to utilize this, but we may note and hence If this generality is preventing you from providing an answer, feel free to assume (in which case we may further assumpe that is uniformly continuous (on the whole space) and hence is trivially satisfied by compactness of closed balls in ).","E \mathbb R ^1 \ln \varphi:E\to\mathbb C\setminus\{0\} \varphi(0)=1 \left.\varphi\right|_{\overline B_r(0)} \inf_{x\in\overline B_r(0)}|\varphi(x)|>0\tag1 r>0 f\in C(E,\mathbb C) f(0)=0 e^f=\varphi n\in\mathbb N g_n\in C(E,\mathbb C\setminus\{0\}) g_n(0)=1 g_n^n=\varphi g_n=e^{\frac fn}\tag2. \varphi^{\frac1n}:=g_n\;\;\;\text{for }n\in\mathbb N m,n\in\mathbb N \varphi^{\frac mn} g_n^m m,m',n,n'\in\mathbb N \frac mn=\frac{m'}{n'} g_n^m=g_{n'}^{m'} \left(g_{mn}^m\right)=\varphi\tag3 g_n=g_{mn}^m\tag4. ^1 E=\mathbb R^d \varphi (1) \mathbb R^d","['complex-analysis', 'complex-numbers', 'logarithms', 'radicals', 'levy-processes']"
58,Show that the series of derivatives of an entire function converges,Show that the series of derivatives of an entire function converges,,"I was asked to show that if $f(z)$ is holomorphic at a neighbourhood of $a$ , and $\sum_{n\geq0}f^{(n)}(a)$ converges, then $f$ is entire and $\sum_{n\geq0}f^{(n)}(z)$ converges for all z in the complex plane.It is easy to prove that $f$ is entire since $f^{(n)}(a)$ is bounded and $\frac{|z-a|^{n}}{n!}$ is smaller than a geometric sequence for sufficiently large $n$ . I have trouble with the second assertion. I can only get the result when the convergence at $a$ is absolute. The method was shown Show that the series of derivatives of a certain entire function converges everywhere . But for the non-absolute convergent case, I don't know how to proceed.","I was asked to show that if is holomorphic at a neighbourhood of , and converges, then is entire and converges for all z in the complex plane.It is easy to prove that is entire since is bounded and is smaller than a geometric sequence for sufficiently large . I have trouble with the second assertion. I can only get the result when the convergence at is absolute. The method was shown Show that the series of derivatives of a certain entire function converges everywhere . But for the non-absolute convergent case, I don't know how to proceed.",f(z) a \sum_{n\geq0}f^{(n)}(a) f \sum_{n\geq0}f^{(n)}(z) f f^{(n)}(a) \frac{|z-a|^{n}}{n!} n a,['complex-analysis']
59,Showing that the order of the zero of the $j$ function is $3$,Showing that the order of the zero of the  function is,j 3,"I'm aware that the $j$ -invariant can be defined as a function on the upper half-plane $\Pi^{+}$ (as seen in the following wikipedia article: https://en.wikipedia.org/wiki/J-invariant ): $$ j(\tau)=1728\frac{(g_{2}(\tau))^{3}}{(g_{2}(\tau)^{3}-27g_{3}(\tau)^{2})} $$ I have to show that the zero of $j(\tau)$ at $p= e^{\frac{2 \pi i}{3}}$ has order 3. Differentiating in a ""naive"" way gives: $$ j'(\tau) = \frac{3g_{2}(\tau)^{2}((g_{2}(\tau)^{3}-27g_{3}(\tau)^{2}))- g_{2}(\tau))^{3}(3g_{2}(\tau)^{2})-54g_{3}(\tau))}{(g_{2}(\tau)^{3}-27g_{3}(\tau)^{2})^{2}} $$ , this implies that $j'(e^{\frac{2 \pi i}{3}})=0$ In this way, I can show that $$ j''(p) = 0$$ , but that $$j'''(p) \neq 0 $$ , so the zero of the function is of order 3. But, I'm not sure if this style of differentiation is correct for a function like the $j$ -invariant. Can someone tell me if I'm doing this in an appropriate manner?","I'm aware that the -invariant can be defined as a function on the upper half-plane (as seen in the following wikipedia article: https://en.wikipedia.org/wiki/J-invariant ): I have to show that the zero of at has order 3. Differentiating in a ""naive"" way gives: , this implies that In this way, I can show that , but that , so the zero of the function is of order 3. But, I'm not sure if this style of differentiation is correct for a function like the -invariant. Can someone tell me if I'm doing this in an appropriate manner?",j \Pi^{+}  j(\tau)=1728\frac{(g_{2}(\tau))^{3}}{(g_{2}(\tau)^{3}-27g_{3}(\tau)^{2})}  j(\tau) p= e^{\frac{2 \pi i}{3}}  j'(\tau) = \frac{3g_{2}(\tau)^{2}((g_{2}(\tau)^{3}-27g_{3}(\tau)^{2}))- g_{2}(\tau))^{3}(3g_{2}(\tau)^{2})-54g_{3}(\tau))}{(g_{2}(\tau)^{3}-27g_{3}(\tau)^{2})^{2}}  j'(e^{\frac{2 \pi i}{3}})=0  j''(p) = 0 j'''(p) \neq 0  j,"['complex-analysis', 'elliptic-curves', 'modular-forms']"
60,For what $z$ does the sequence $z_n=nz^n$ converge?,For what  does the sequence  converge?,z z_n=nz^n,"For what $z$ does the sequence $z_n=nz^n$ converge? Attempt Consider $\sum_{n\geq 1}nz^n $ and notice $$\lim_{n \to \infty}\frac{z_{n+1}}{z_n}=(1+\frac{1}{n})z=z$$ but the serie converges iff $|z|<1$ therefore $z_n$ converges iff $|z|<1$ and in fact $z_n \to 0$ Is my answer right?  or I should consider $z^n=\cos n \theta+ i \sin n \theta$ and consider the limit $$\lim_{n \to \infty} n \left( \cos n \theta+i \sin n \theta \right)$$ and compare $$ \lim_{n \to \infty} n \left( \cos n \theta \right)\wedge \lim_{n \to \infty} n \left( i\sin n \theta \right)$$ There exists a general form of attack this kind of problem, when they request see for what values of $z$ a given sequence converges?","For what does the sequence converge? Attempt Consider and notice but the serie converges iff therefore converges iff and in fact Is my answer right?  or I should consider and consider the limit and compare There exists a general form of attack this kind of problem, when they request see for what values of a given sequence converges?",z z_n=nz^n \sum_{n\geq 1}nz^n  \lim_{n \to \infty}\frac{z_{n+1}}{z_n}=(1+\frac{1}{n})z=z |z|<1 z_n |z|<1 z_n \to 0 z^n=\cos n \theta+ i \sin n \theta \lim_{n \to \infty} n \left( \cos n \theta+i \sin n \theta \right)  \lim_{n \to \infty} n \left( \cos n \theta \right)\wedge \lim_{n \to \infty} n \left( i\sin n \theta \right) z,"['sequences-and-series', 'complex-analysis', 'complex-numbers']"
61,Multivariate residues in simple cases,Multivariate residues in simple cases,,"I have been looking at residues of multivariate functions and found there are quite a few difficulties (see e.g. Multivariate Residue Theorem? or Multivariate/multidimensional residues ). In the literature, this is discussed in the context of manifolds, 1-forms and currents. Unfortunately, I am not an expert on manifolds. Question: Are there ""simple rules"" deriving from the general treatment that can be applied in more basic cases. I am thinking of multivariate functions $f(x,y,z, ...)$ with simple poles at equal points $x=y,~ x=z, ...$ , where I would like to evaluate the residue at multiple, coinciding points $x=y=z$ as consecutive residues $\text{Res}_{x=y} \text{Res}_{y=z} \cdots$ in a consistent way. Example: Consider the function $f(x,y,z) = \frac{1}{(x-y)(x-z)}$ defined on $\mathbb{C}^3$ . It has singularities on and the 1-dimensional subspaces $\{(x,y,z) | x=y \}$ and $\{ (x,y,z) | x=z\}$ which intersect at $x=y=z$ . Computing the residue on the intersection can be done through consecutive application of residues: $$ \text{Res}_{y=z} \text{Res}_{x=y} \frac{1}{(x-y)(x-z)} = \text{Res}_{y=z} \frac{1}{(y-z)} = 1.  $$ However, exchanging the residues leads to a wrong result: $$ \text{Res}_{x=y} \text{Res}_{y=z} \frac{1}{(x-y)(x-z)} = 0. $$ Is there a procedure that tells me how to correctly take certain residues or at least relate different combinations of residues which give the same result? (For example $\text{Res}_{y=z} \text{Res}_{x=y}$ and $\text{Res}_{z=x} \text{Res}_{y=x}$ in the previous example) Background: In quantum field theory, amplitudes (vacuum expectation values of time-ordered products of fields) are meromorphic functions in $\mathbb{C}^k$ . Poles correspond to the temporary fusion of particles and higher-order poles at the intersection of more than two points (which I would like to evaluate as consecutive residues) appear when there are more complicated composite particles. I appreciate any help or literature recommendation!","I have been looking at residues of multivariate functions and found there are quite a few difficulties (see e.g. Multivariate Residue Theorem? or Multivariate/multidimensional residues ). In the literature, this is discussed in the context of manifolds, 1-forms and currents. Unfortunately, I am not an expert on manifolds. Question: Are there ""simple rules"" deriving from the general treatment that can be applied in more basic cases. I am thinking of multivariate functions with simple poles at equal points , where I would like to evaluate the residue at multiple, coinciding points as consecutive residues in a consistent way. Example: Consider the function defined on . It has singularities on and the 1-dimensional subspaces and which intersect at . Computing the residue on the intersection can be done through consecutive application of residues: However, exchanging the residues leads to a wrong result: Is there a procedure that tells me how to correctly take certain residues or at least relate different combinations of residues which give the same result? (For example and in the previous example) Background: In quantum field theory, amplitudes (vacuum expectation values of time-ordered products of fields) are meromorphic functions in . Poles correspond to the temporary fusion of particles and higher-order poles at the intersection of more than two points (which I would like to evaluate as consecutive residues) appear when there are more complicated composite particles. I appreciate any help or literature recommendation!","f(x,y,z, ...) x=y,~ x=z, ... x=y=z \text{Res}_{x=y} \text{Res}_{y=z} \cdots f(x,y,z) = \frac{1}{(x-y)(x-z)} \mathbb{C}^3 \{(x,y,z) | x=y \} \{ (x,y,z) | x=z\} x=y=z 
\text{Res}_{y=z} \text{Res}_{x=y} \frac{1}{(x-y)(x-z)} = \text{Res}_{y=z} \frac{1}{(y-z)} = 1. 
 
\text{Res}_{x=y} \text{Res}_{y=z} \frac{1}{(x-y)(x-z)} = 0.
 \text{Res}_{y=z} \text{Res}_{x=y} \text{Res}_{z=x} \text{Res}_{y=x} \mathbb{C}^k","['complex-analysis', 'multivariable-calculus', 'residue-calculus']"
62,Parametrization of Contour $C$,Parametrization of Contour,C,"For the function $f(z)=1$ $(z\in \mathbb{C})$ and C is an arbitrary contour from any fixed point ${z}_{1}$ to any fixed point ${z}_{2}$ in the $z$ plane. Use parametric representations for $C$ to evaluate $\int _{C}^{}f(z)dz$ . Here is my solution: Let ${z}_{1}$ and ${z}_{2}$ are two fixed point and $C$ is straight line. Then $z=(1-t){z}_{1}+t{z}_{2}$ where $t\in \left[0,1\right]$ , from here $dz=-{z}_{1}dt+{z}_{2}dt$ . Then we get $\int _{{z}_{1}}^{{z}_{2}}f(z)dz=\int _{0}^{1}1({z}_{2}dt-{z}_{1}dt)=\int _{0}^{1}{z}_{2}dt-\int _{0}^{1}{z}_{1}dt={z}_{2}-{z}_{1}$ Hence, $\int _{C}^{}f(z)dz=\int _{{z}_{1}}^{{z}_{2}}f(z)dz=\int _{{z}_{1}}^{{z}_{2}}1dz={z}_{2}-{z}_{1}$ Is that my solution right ? I feel that I'm missing something. Any help will be appreciated.","For the function and C is an arbitrary contour from any fixed point to any fixed point in the plane. Use parametric representations for to evaluate . Here is my solution: Let and are two fixed point and is straight line. Then where , from here . Then we get Hence, Is that my solution right ? I feel that I'm missing something. Any help will be appreciated.","f(z)=1 (z\in \mathbb{C}) {z}_{1} {z}_{2} z C \int _{C}^{}f(z)dz {z}_{1} {z}_{2} C z=(1-t){z}_{1}+t{z}_{2} t\in \left[0,1\right] dz=-{z}_{1}dt+{z}_{2}dt \int _{{z}_{1}}^{{z}_{2}}f(z)dz=\int _{0}^{1}1({z}_{2}dt-{z}_{1}dt)=\int _{0}^{1}{z}_{2}dt-\int _{0}^{1}{z}_{1}dt={z}_{2}-{z}_{1} \int _{C}^{}f(z)dz=\int _{{z}_{1}}^{{z}_{2}}f(z)dz=\int _{{z}_{1}}^{{z}_{2}}1dz={z}_{2}-{z}_{1}","['complex-analysis', 'analysis', 'contour-integration', 'complex-integration', 'parametrization']"
63,Finding convolution of two functions,Finding convolution of two functions,,"A common engineering notational convention is: wikipedia ${\displaystyle f(x)*g(x)\,:=\underbrace {\int_{0}^{x}f(\tau )g(x-\tau )\,d\tau } _{(f*g)(x)}.}$ I want to write the following expression as the convolution of two functions. In the other words; what is the function $g(x)$ in the definition for the integral below: \begin{equation} \frac{(\rho+1)^{1-\alpha}}{\Gamma(\alpha)} \int_{a}^{x}\left(x^{\rho+1}-\tau^{\rho+1}\right)^{\alpha-1} \tau^{\rho} f(\tau) \,d \tau \end{equation} where $\alpha$ and $\rho \neq-1$ are real numbers and $x > a$ , $\Gamma$ is gamma function . My Try: Let \begin{align} k&=\tau^{\rho+1}\\ dk&=(\rho+1)\tau^{\rho}\,d \tau \implies \frac{dk}{(\rho+1)}=\tau^{\rho}\,d \tau. \end{align} Substituting the last equality to the integral in the question, we have \begin{equation} \frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{a^{\rho}}^{x^{\rho}}\left(x^{\rho+1}-k\right)^{\alpha-1} f(k^{1/(\rho+1)})\, dk \end{equation} And then? P.S. If the last expression was \begin{equation} \frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{0}^{x}\left(x-k\right)^{\alpha-1} f(k)\, dk \end{equation} $g(x)$ would be as follows: $$g(x)=\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} x^{\alpha-1}.$$","A common engineering notational convention is: wikipedia I want to write the following expression as the convolution of two functions. In the other words; what is the function in the definition for the integral below: where and are real numbers and , is gamma function . My Try: Let Substituting the last equality to the integral in the question, we have And then? P.S. If the last expression was would be as follows:","{\displaystyle f(x)*g(x)\,:=\underbrace {\int_{0}^{x}f(\tau )g(x-\tau )\,d\tau } _{(f*g)(x)}.} g(x) \begin{equation}
\frac{(\rho+1)^{1-\alpha}}{\Gamma(\alpha)} \int_{a}^{x}\left(x^{\rho+1}-\tau^{\rho+1}\right)^{\alpha-1} \tau^{\rho} f(\tau) \,d \tau
\end{equation} \alpha \rho \neq-1 x > a \Gamma \begin{align}
k&=\tau^{\rho+1}\\
dk&=(\rho+1)\tau^{\rho}\,d \tau \implies \frac{dk}{(\rho+1)}=\tau^{\rho}\,d \tau.
\end{align} \begin{equation}
\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{a^{\rho}}^{x^{\rho}}\left(x^{\rho+1}-k\right)^{\alpha-1} f(k^{1/(\rho+1)})\, dk
\end{equation} \begin{equation}
\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{0}^{x}\left(x-k\right)^{\alpha-1} f(k)\, dk
\end{equation} g(x) g(x)=\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} x^{\alpha-1}.","['integration', 'complex-analysis', 'analysis', 'mathematical-physics', 'convolution']"
64,Analytic function with derivative zero,Analytic function with derivative zero,,"Theorem :- Let $f(z) =u(x,y) +iv(x, y)$ If, $f'(z)=0$ everywhere in a domain $D$ then $f(z)$ must be constant throughout D. Here In this theorem, by using cauchy reimann equations, we get $u_x =v_y=0$ and $u_y=-v_x=0$ Now I am not getting why it does not suffice to show that f is constant. Please help me with this, (I am thinking that both partial derivatives are zero than $u(x, y)$ is constant similarly $v(x, y)$ ) I don't understand what I am missing.","Theorem :- Let If, everywhere in a domain then must be constant throughout D. Here In this theorem, by using cauchy reimann equations, we get and Now I am not getting why it does not suffice to show that f is constant. Please help me with this, (I am thinking that both partial derivatives are zero than is constant similarly ) I don't understand what I am missing.","f(z) =u(x,y) +iv(x, y) f'(z)=0 D f(z) u_x =v_y=0 u_y=-v_x=0 u(x, y) v(x, y)",['complex-analysis']
65,Why $\lim_{r\to 1^-} \int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta=-\infty$?,Why ?,\lim_{r\to 1^-} \int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta=-\infty,"I am studying Greene and Krantz' ""Function theory in one complex variable"". In section $13.4$ , they want to prove that, given $f\in H^p, p\in (0,+\infty),$ if the radial limit function $\tilde f$ is zero on a set of positive measure then $f\equiv 0$ . Their proof is quite short: if $f\not\equiv 0$ , we can assume wlog $f(0)\neq 0$ and then by Jensen's formula (given $r<1:\forall \vartheta f(re^{i\vartheta})\neq 0$ ) $$-\infty<\ln(|f(0)|)\le\int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta/2\pi $$ I have a problem with the following passage: As $r\to 1^-$ through such $^1$ values, the right hand side of this expression tends to $$\frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde{f}(e^{i\vartheta})|)d\vartheta=-\infty$$ I am unsure about one thing: Why is the limit equal to $\frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde f|)$ ? I tried using dominated convergence and Fatou's lemma, to no avail. Thanks for the help, and Happy Holidays! Note: for completeness, the proof then concludes by noting that we reached a contradiction and thus $f$ cannot be $\not\equiv 0$ . $^1$ : meaning $r$ such that $f$ is not zero on $|z|=r$ .","I am studying Greene and Krantz' ""Function theory in one complex variable"". In section , they want to prove that, given if the radial limit function is zero on a set of positive measure then . Their proof is quite short: if , we can assume wlog and then by Jensen's formula (given ) I have a problem with the following passage: As through such values, the right hand side of this expression tends to I am unsure about one thing: Why is the limit equal to ? I tried using dominated convergence and Fatou's lemma, to no avail. Thanks for the help, and Happy Holidays! Note: for completeness, the proof then concludes by noting that we reached a contradiction and thus cannot be . : meaning such that is not zero on .","13.4 f\in H^p, p\in (0,+\infty), \tilde f f\equiv 0 f\not\equiv 0 f(0)\neq 0 r<1:\forall \vartheta f(re^{i\vartheta})\neq 0 -\infty<\ln(|f(0)|)\le\int_0^{2\pi}\ln(|f(re^{i\vartheta})|)d\vartheta/2\pi  r\to 1^- ^1 \frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde{f}(e^{i\vartheta})|)d\vartheta=-\infty \frac{1}{2\pi}\int_0^{2\pi}\ln(|\tilde f|) f \not\equiv 0 ^1 r f |z|=r",['complex-analysis']
66,Applying Argument Principle to find zeroes in a quadrant when there is a real root,Applying Argument Principle to find zeroes in a quadrant when there is a real root,,"I was trying to find the number of roots of the polynomial $p(x)=z^9+2z^5-2z^4+z+3$ that lie in the second quadrant. I know that $p(-1)<0$ and $p(0)>0$ , so there is some real root on the negative real axis between $-1$ and $0$ . This means that I cannot use either Rouche's Theorem or the Argument Principle to find number of roots in the second quadrant by checking the strict inequality/change in argument around the boundary of the quarter disc in the second quadrant, because there is a zero on $\delta D$ . I tried applying argument principle on the quarter disc where instead of the negative real axis, I took it along $re^{i(\pi - \varepsilon)}$ , but still got nowhere. Any help on how to go about this would be appreciated!","I was trying to find the number of roots of the polynomial that lie in the second quadrant. I know that and , so there is some real root on the negative real axis between and . This means that I cannot use either Rouche's Theorem or the Argument Principle to find number of roots in the second quadrant by checking the strict inequality/change in argument around the boundary of the quarter disc in the second quadrant, because there is a zero on . I tried applying argument principle on the quarter disc where instead of the negative real axis, I took it along , but still got nowhere. Any help on how to go about this would be appreciated!",p(x)=z^9+2z^5-2z^4+z+3 p(-1)<0 p(0)>0 -1 0 \delta D re^{i(\pi - \varepsilon)},"['complex-analysis', 'polynomials', 'rouches-theorem']"
67,How analytic continuation allows for proof of these 2 theorems in theory of Partitions,How analytic continuation allows for proof of these 2 theorems in theory of Partitions,,"Consider these 2 theorems in textbook apsotol introduction to analytic number theory. 1st is generating functions for partitions I have self studied text and need help in verifying the argument of use of analytic continuation in  last image( last line of proof) : Is it due to reason that we can differentiate the formula derived infinitely many times for all complex numbers in disk |x|<1? Similarly, in case of proof of Euler Pentagonal Theorem here : See 2nd line of First image : Author says about analytic continuation. My understanding is that is it due to the fact that infinitely times differentiable in |x|<1 . Am i right or wrong. Do I need to add something else. I ask my questions here because there is no one to whom I can ask as it is not taught in my university. Kindly shed some light on this.","Consider these 2 theorems in textbook apsotol introduction to analytic number theory. 1st is generating functions for partitions I have self studied text and need help in verifying the argument of use of analytic continuation in  last image( last line of proof) : Is it due to reason that we can differentiate the formula derived infinitely many times for all complex numbers in disk |x|<1? Similarly, in case of proof of Euler Pentagonal Theorem here : See 2nd line of First image : Author says about analytic continuation. My understanding is that is it due to the fact that infinitely times differentiable in |x|<1 . Am i right or wrong. Do I need to add something else. I ask my questions here because there is no one to whom I can ask as it is not taught in my university. Kindly shed some light on this.",,"['complex-analysis', 'number-theory']"
68,Calculating length of circular arc,Calculating length of circular arc,,"I'm currently working through a step-by-step example which calculates the length of a circular arc $\gamma$ of radius $r$ subtended by angles $\theta_1$ and $\theta_2$ . The circular arc is parametrised as $\gamma (t) = c + re^{it}$ for $\theta_1 \leq t \leq \theta_2$ . To calculate the length, we need to work out the derivative, $\gamma'$ , which is $\gamma'(t)=rie^{it}$ . Then by the definition of length, we get $$L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt$$ I understand everything up until this point, but then the solution goes onto simplifying the integral further, and I do not understand why this can be done. $$L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt = r\int_{\theta_2}^{\theta_1}dt = r(\theta_2 - \theta_1)$$ Surely by integrating, you would get the following instead? $$L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt = r\int_{\theta_2}^{\theta_1}|ie^{it}| dt = r(e^{i\theta_2} - e^{i\theta_1})$$ Any help showing where I've gone wrong would be much appreciated!","I'm currently working through a step-by-step example which calculates the length of a circular arc of radius subtended by angles and . The circular arc is parametrised as for . To calculate the length, we need to work out the derivative, , which is . Then by the definition of length, we get I understand everything up until this point, but then the solution goes onto simplifying the integral further, and I do not understand why this can be done. Surely by integrating, you would get the following instead? Any help showing where I've gone wrong would be much appreciated!",\gamma r \theta_1 \theta_2 \gamma (t) = c + re^{it} \theta_1 \leq t \leq \theta_2 \gamma' \gamma'(t)=rie^{it} L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt = r\int_{\theta_2}^{\theta_1}dt = r(\theta_2 - \theta_1) L(\gamma) = \int_{\theta_2}^{\theta_1}|rie^{it}| dt = r\int_{\theta_2}^{\theta_1}|ie^{it}| dt = r(e^{i\theta_2} - e^{i\theta_1}),"['complex-analysis', 'multivariable-calculus', 'arc-length']"
69,Using Fundamental Theorem of Algebra to find $z_0$ such that $|p(z_0)| < |p(0)|$,Using Fundamental Theorem of Algebra to find  such that,z_0 |p(z_0)| < |p(0)|,"A question from the book Hubbard's Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach (5th edition): I was able to easily find some point $z_0 = i/3$ (just by trying out points), but am unsure how to use their equation in their proof to find this point. Consider the polynomial $p(z) = z^8 + z^4 + z^2 + 1$ where $z \in \mathbb{C}$ , use the construction in the proof of the fundamental theorem of algebra, using equation 1.6.28 to find a point $z_0$ such that $|p(z_0)| < |p(0)| = 1$ . Here is a rough idea of how they prove The Fundamental Theorem of Algebra in their book: First, they showed that for any monic polynomial of degree $k > 0$ with complex coefficients $p(z) = z^k + a_{k-1} z^{k-1} + \dots + a_0$ , that $|p(z)|$ always has a global minimum at $z_0 \in \mathbb{C}$ for some $R>0$ with $|z_0| \leq R$ . Afterwards to show $p(z_0) = 0$ , they assume instead $p(z_0) \neq 0$ to find a point $z$ such that $|p(z)| < |p(z_0)|$ , which would lead to a  contradiction. To do this, they let $z = z_0 + u$ so that: $$\begin{align} p(z) &= (z_0 + u)^k + a_{k-1} (z_0 + u)^{k-1} + \dots + a_0 \\ &= u^k + b_{k-1}u^{k-1} + \dots + b_0 \\ &= q(u) \end{align}$$ where it can be shown $b_0 = z_0^k + a_{k-1}z_0^{k-1} + \dots + a_0 = p(z_0) \neq 0$ If we let $j>0$ be the smallest power of $q(u)$ with a non-zero coefficient, then we have equation 1.6.28 : $$\boxed{ q(u) = b_0 + b_j u^j + (b_{j+1} u^{j+1} + \dots + u^k) = p(z) = p(z_0 + u)}$$ Noting $u$ can be written as $\rho e^{i\theta}$ , it can be imagined $b_0 + b_j u^j$ is travelling around a circle with center $b_0 = p(z_0)$ . It can then be shown there exists a $\rho$ (I am not detailing its value here) such that $|b_j|\rho^j < |b_0|$ ,  so that for some values of $\theta$ we have $|b_0 + b_j u^j| < |b_0|$ (i.e. visually it will be a point on the line segment between $0$ and $b_0$ )  and $|b_{j+1} u^{j+1} + \dots + u^k| < |b_j|\rho^j$ (the distance between the points $b_0 + b_ju^j$ and $b_0$ ). This leads to the contradiction $|p(z)| = |p(z_0 + u)| < |b_0| = |p(z_0)|$ , since $|p(z_0)|$ is the minimum of the modulus of polynomial $p$ . I detailed the proof if it is needed (if you think I have left something out from the book let me know), but now I am uncertain how to use the equation 1.6.28 (boxed equation above) in their proof to find this point $z_0$ . Any hints or ideas would be greatly appreciated.","A question from the book Hubbard's Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach (5th edition): I was able to easily find some point (just by trying out points), but am unsure how to use their equation in their proof to find this point. Consider the polynomial where , use the construction in the proof of the fundamental theorem of algebra, using equation 1.6.28 to find a point such that . Here is a rough idea of how they prove The Fundamental Theorem of Algebra in their book: First, they showed that for any monic polynomial of degree with complex coefficients , that always has a global minimum at for some with . Afterwards to show , they assume instead to find a point such that , which would lead to a  contradiction. To do this, they let so that: where it can be shown If we let be the smallest power of with a non-zero coefficient, then we have equation 1.6.28 : Noting can be written as , it can be imagined is travelling around a circle with center . It can then be shown there exists a (I am not detailing its value here) such that ,  so that for some values of we have (i.e. visually it will be a point on the line segment between and )  and (the distance between the points and ). This leads to the contradiction , since is the minimum of the modulus of polynomial . I detailed the proof if it is needed (if you think I have left something out from the book let me know), but now I am uncertain how to use the equation 1.6.28 (boxed equation above) in their proof to find this point . Any hints or ideas would be greatly appreciated.",z_0 = i/3 p(z) = z^8 + z^4 + z^2 + 1 z \in \mathbb{C} z_0 |p(z_0)| < |p(0)| = 1 k > 0 p(z) = z^k + a_{k-1} z^{k-1} + \dots + a_0 |p(z)| z_0 \in \mathbb{C} R>0 |z_0| \leq R p(z_0) = 0 p(z_0) \neq 0 z |p(z)| < |p(z_0)| z = z_0 + u \begin{align} p(z) &= (z_0 + u)^k + a_{k-1} (z_0 + u)^{k-1} + \dots + a_0 \\ &= u^k + b_{k-1}u^{k-1} + \dots + b_0 \\ &= q(u) \end{align} b_0 = z_0^k + a_{k-1}z_0^{k-1} + \dots + a_0 = p(z_0) \neq 0 j>0 q(u) \boxed{ q(u) = b_0 + b_j u^j + (b_{j+1} u^{j+1} + \dots + u^k) = p(z) = p(z_0 + u)} u \rho e^{i\theta} b_0 + b_j u^j b_0 = p(z_0) \rho |b_j|\rho^j < |b_0| \theta |b_0 + b_j u^j| < |b_0| 0 b_0 |b_{j+1} u^{j+1} + \dots + u^k| < |b_j|\rho^j b_0 + b_ju^j b_0 |p(z)| = |p(z_0 + u)| < |b_0| = |p(z_0)| |p(z_0)| p z_0,"['complex-analysis', 'complex-numbers']"
70,Improper Definite integral $\int_{-\infty}^\infty -\frac{i \pi e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p}dp$,Improper Definite integral,\int_{-\infty}^\infty -\frac{i \pi e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p}dp,"I came across this improper integral that I couldn't solve $$\int_{-\infty}^\infty -\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp$$ My guess would be to use Residue theorem but it does not seem help. My try so far is that it has a pole at $p=0$ . With $a>0$ , closing the contour upward, I compute the residue $$\lim_{p\to 0}-\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} p =-I \pi $$ Thus, the value of the integral is $2\pi I Res(f,0)= 2\pi^2$ . This is definitely not the correct answer (which I confirmed by numerical integration). Taking the hint from the comment I proceeded the following way $\int_{-\infty}^\infty e^{-iap} sech(\frac{cp}{2})=\frac{2 \pi  \text{sech}\left(\frac{\pi  a}{c}\right)}{c}$ which comes from the fact that Fourier transform of sech function is sech function itself. Now, to account the p in the denominator, I need to integrate this result and add a delta function which gives $\int_{-\infty}^\infty \frac{ e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp =\int \frac{2 \pi  \text{sech}\left(\frac{\pi  a}{c}\right)}{c} da =-\frac{2 \pi ^2 \tanh \left(\frac{\pi  a}{c}\right) \text{sech}\left(\frac{\pi  a}{c}\right)}{c^2}+ \delta(a)$ Multiplying by the factor $-i\pi$ on both sides, I get $$\int_{-\infty}^\infty -\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp=\frac{2 i \pi ^3 \tanh \left(\frac{\pi  a}{c}\right) \text{sech}\left(\frac{\pi  a}{c}\right)}{c^2}-i\pi \delta (a)$$ This is supposed to be the correct answer. But that still does not match with the numerical integration.","I came across this improper integral that I couldn't solve My guess would be to use Residue theorem but it does not seem help. My try so far is that it has a pole at . With , closing the contour upward, I compute the residue Thus, the value of the integral is . This is definitely not the correct answer (which I confirmed by numerical integration). Taking the hint from the comment I proceeded the following way which comes from the fact that Fourier transform of sech function is sech function itself. Now, to account the p in the denominator, I need to integrate this result and add a delta function which gives Multiplying by the factor on both sides, I get This is supposed to be the correct answer. But that still does not match with the numerical integration.","\int_{-\infty}^\infty -\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp p=0 a>0 \lim_{p\to 0}-\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} p =-I \pi  2\pi I Res(f,0)= 2\pi^2 \int_{-\infty}^\infty e^{-iap} sech(\frac{cp}{2})=\frac{2 \pi  \text{sech}\left(\frac{\pi  a}{c}\right)}{c} \int_{-\infty}^\infty \frac{ e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp =\int \frac{2 \pi  \text{sech}\left(\frac{\pi  a}{c}\right)}{c} da =-\frac{2 \pi ^2 \tanh \left(\frac{\pi  a}{c}\right) \text{sech}\left(\frac{\pi  a}{c}\right)}{c^2}+ \delta(a) -i\pi \int_{-\infty}^\infty -\frac{i \pi  e^{-i a p} \text{sech}\left(\frac{c p}{2}\right)}{p} dp=\frac{2 i \pi ^3 \tanh \left(\frac{\pi  a}{c}\right) \text{sech}\left(\frac{\pi  a}{c}\right)}{c^2}-i\pi \delta (a)","['complex-analysis', 'definite-integrals', 'improper-integrals', 'complex-integration', 'residue-calculus']"
71,Prove that if the only singularities of a function are poles then the function must be rational.,Prove that if the only singularities of a function are poles then the function must be rational.,,"Prove that if the only singularities of a function are poles then the function must be rational. So originally this was an iff statement, but I have solved the other direction of the proof. I'm just not quite sure how to prove it going this direction. I'm thinking to assume the only singularities of a function are poles, but I'm not sure how to work this down to being only for rational functions. I toyed with the idea of contradiction, but still wasn't quite sure how I should go about proving this.","Prove that if the only singularities of a function are poles then the function must be rational. So originally this was an iff statement, but I have solved the other direction of the proof. I'm just not quite sure how to prove it going this direction. I'm thinking to assume the only singularities of a function are poles, but I'm not sure how to work this down to being only for rational functions. I toyed with the idea of contradiction, but still wasn't quite sure how I should go about proving this.",,"['complex-analysis', 'complex-numbers', 'singularity']"
72,Measurability of a function on sub sigma algebra,Measurability of a function on sub sigma algebra,,"Let $\Omega$ be a compact Hausdorff space in $\mathbb{C}^2$ . Let $\sigma_\Omega$ be the Borel sigma algebra on $\Omega$ . Let $f: \Omega\longrightarrow\partial \mathbb{D}$ be a non constant continuous  function. Let $\sigma_{\partial \mathbb{D}}$ be the Borel sigma algebra on $\partial \mathbb{D}$ (Unit circle on the complex plane). Now consider the sigma algebra $\sigma_0=\{f^{-1}(A): \;A\in \sigma_{\partial \mathbb{D}}\}\subset \sigma_\Omega$ . Now consider the function $g: \Omega\longrightarrow\mathbb{C}$ as $g(z_1,z_2)=z_1^mz_2^n(\bar{z_1})^p(\bar{z_2})^q$ , where $m,n,p,q\in\mathbb{N}.$ Now as $g$ is continuous, it is clearly measurable w.r.t $\sigma_\Omega$ but. will it be measurable w.r.t $\sigma_0$ ? Is there any additional condition that can be put on $f$ , so that the above holds?","Let be a compact Hausdorff space in . Let be the Borel sigma algebra on . Let be a non constant continuous  function. Let be the Borel sigma algebra on (Unit circle on the complex plane). Now consider the sigma algebra . Now consider the function as , where Now as is continuous, it is clearly measurable w.r.t but. will it be measurable w.r.t ? Is there any additional condition that can be put on , so that the above holds?","\Omega \mathbb{C}^2 \sigma_\Omega \Omega f: \Omega\longrightarrow\partial \mathbb{D} \sigma_{\partial \mathbb{D}} \partial \mathbb{D} \sigma_0=\{f^{-1}(A): \;A\in \sigma_{\partial \mathbb{D}}\}\subset \sigma_\Omega g: \Omega\longrightarrow\mathbb{C} g(z_1,z_2)=z_1^mz_2^n(\bar{z_1})^p(\bar{z_2})^q m,n,p,q\in\mathbb{N}. g \sigma_\Omega \sigma_0 f","['complex-analysis', 'measure-theory', 'conditional-expectation']"
73,Showing a series of functions converges to a periodic function.,Showing a series of functions converges to a periodic function.,,"Let $z = x+iy$ and let $k \in \mathbb{C}$ be a constant. I am reading a paper from Perelman where he considers the function, \begin{align*} \mathrm{f}&:\mathbb{C}\backslash\ D \longrightarrow \mathbb{R}\\[3mm] &:z \longmapsto \mathfrak{R}[\ln\left(1+\frac{4k^{3}}{(z + k)^{2} (z - 2k)}\right)] \end{align*} where $k \in \mathbb{C}$ is a constant and $D$ is a discrete subset of $\mathbb{C}$ Perelma says that that the series converges $$ \sum_{a,b\ \in\ \mathbb{Z}}\mathrm{f}\left(z + a + \mathrm{i}b\right) $$ converges to a function, $\mathrm{g}\left(z\right)$ , which $\textbf{is}$ $1$ -periodic. It is not clear to me whether I have to find this function $\mathrm{g}\left(z\right)$ explicitly in order to show this $1$ -periodicity ?. In any case, I am quite at a loss as how to move forward to show the $1$ -periodicity and convergence of this series. By "" $1$ -periodic"", I mean with respect to $x$ and $y$ . $\textbf{EDIT/UPDATE}$ : I am now clear on the $1$ -periodicity of the series, however it is still unclear to me why this series converges? $\textbf{EDIT/UPDATE}$ : I realised I copied Perelman's function incorrectly, I have since changed this in the text above, also you can see the relevant part of his paper here .","Let and let be a constant. I am reading a paper from Perelman where he considers the function, where is a constant and is a discrete subset of Perelma says that that the series converges converges to a function, , which -periodic. It is not clear to me whether I have to find this function explicitly in order to show this -periodicity ?. In any case, I am quite at a loss as how to move forward to show the -periodicity and convergence of this series. By "" -periodic"", I mean with respect to and . : I am now clear on the -periodicity of the series, however it is still unclear to me why this series converges? : I realised I copied Perelman's function incorrectly, I have since changed this in the text above, also you can see the relevant part of his paper here .","z = x+iy k \in \mathbb{C} \begin{align*}
\mathrm{f}&:\mathbb{C}\backslash\ D \longrightarrow \mathbb{R}\\[3mm]
&:z \longmapsto \mathfrak{R}[\ln\left(1+\frac{4k^{3}}{(z + k)^{2} (z - 2k)}\right)]
\end{align*} k \in \mathbb{C} D \mathbb{C} 
\sum_{a,b\ \in\ \mathbb{Z}}\mathrm{f}\left(z + a + \mathrm{i}b\right)
 \mathrm{g}\left(z\right) \textbf{is} 1 \mathrm{g}\left(z\right) 1 1 1 x y \textbf{EDIT/UPDATE} 1 \textbf{EDIT/UPDATE}","['sequences-and-series', 'complex-analysis', 'analysis', 'periodic-functions']"
74,Consider the function $f (z) = z +2z^2 +3z^3 +··· = \sum_{n≥0} nz^n$ defined on the open disk $\{z \vert|z| < 1\}$. Choose the correct option,Consider the function  defined on the open disk . Choose the correct option,f (z) = z +2z^2 +3z^3 +··· = \sum_{n≥0} nz^n \{z \vert|z| < 1\},"Consider the function $f (z) = z +2z^2 +3z^3 +··· = \sum_{n≥0} nz^n$ defined on the open disk $\{z \vert|z| < 1\}$ . Choose the correct option: (a) f is not injective but attains every complex value at least once. (b) f is injective but does not attain every complex value. (c) f is injective and attains every complex value. (d) None of the above. Attempt: $f(z)=z +2z^2 +3z^3 +··· =z(1+2z+3z^2+...)=\frac{z}{(1-z)^2}.$ Now, for injectivity: $f(z_1)=f(z_2)\implies z_1(1-z_2)^2=z_2(1-z_1)^2\implies (z_1-z_2)(1-z_1z_2)=0$ . Since in the open unit disk $z_1z_2\neq1$ , so we have $z_1=z_2$ and hence the function is injective. I looked the function $\frac{z}{(1-z)^2}$ known as the Koebe function and wikipedia says $f(z)$ can never achieve $\frac{-1}{4}$ as $f(z)=\frac{-1}{4} \iff z=-1$ but, $z=-1$ is not in the unit disk, I wish to see a different approach on the surjectivity part. Can someone please help me with the onto (surjective) part?","Consider the function defined on the open disk . Choose the correct option: (a) f is not injective but attains every complex value at least once. (b) f is injective but does not attain every complex value. (c) f is injective and attains every complex value. (d) None of the above. Attempt: Now, for injectivity: . Since in the open unit disk , so we have and hence the function is injective. I looked the function known as the Koebe function and wikipedia says can never achieve as but, is not in the unit disk, I wish to see a different approach on the surjectivity part. Can someone please help me with the onto (surjective) part?",f (z) = z +2z^2 +3z^3 +··· = \sum_{n≥0} nz^n \{z \vert|z| < 1\} f(z)=z +2z^2 +3z^3 +··· =z(1+2z+3z^2+...)=\frac{z}{(1-z)^2}. f(z_1)=f(z_2)\implies z_1(1-z_2)^2=z_2(1-z_1)^2\implies (z_1-z_2)(1-z_1z_2)=0 z_1z_2\neq1 z_1=z_2 \frac{z}{(1-z)^2} f(z) \frac{-1}{4} f(z)=\frac{-1}{4} \iff z=-1 z=-1,"['complex-analysis', 'summation']"
75,Can quasiconformal mappings converge uniformly to a homeomorphism that is NOT quasiconformal?,Can quasiconformal mappings converge uniformly to a homeomorphism that is NOT quasiconformal?,,"My question concerns the following situation: Let $G$ be a domain in $\mathbb{C}$ and $f_n: G \rightarrow \mathbb{C}$ be a sequence of quasiconformal mappings. Suppose that $f_n$ converges uniformly on $G$ , i.e. with respect to the supremum metric on $G$ , to a homeomorphism $f: G \rightarrow \mathbb{C}$ . My question is: Is it possible that the limit mapping $f$ is NOT quasiconformal, even though being a homeomorphism? Note that the answer is surely ""NO"" if the $f_n$ are all $K$ -quasiconformal for some fixed $K < \infty$ , by well-known convergence results on quasiconformal mappings; in this case, the limit mapping $f$ will be $K$ -quasiconformal again. Hence the interesting part of my question is when the condition "" the $f_n$ are $K$ -quasiconformal mappings "" is dropped, i.e. the maximal dilatations of the $f_n$ are not uniformly bounded by some constant $K$ ...I do not know what could possibly happen in this situation (maybe the situation at hand actually forces the $f_n$ to be $K$ -quasiconformal?), unfortunately all convergence results on quasiconformal mappings I am aware of deal with the situation that the $f_n$ are all $K$ -quasiconformal. Any kind of help is highly appreciated - thanks in advance!","My question concerns the following situation: Let be a domain in and be a sequence of quasiconformal mappings. Suppose that converges uniformly on , i.e. with respect to the supremum metric on , to a homeomorphism . My question is: Is it possible that the limit mapping is NOT quasiconformal, even though being a homeomorphism? Note that the answer is surely ""NO"" if the are all -quasiconformal for some fixed , by well-known convergence results on quasiconformal mappings; in this case, the limit mapping will be -quasiconformal again. Hence the interesting part of my question is when the condition "" the are -quasiconformal mappings "" is dropped, i.e. the maximal dilatations of the are not uniformly bounded by some constant ...I do not know what could possibly happen in this situation (maybe the situation at hand actually forces the to be -quasiconformal?), unfortunately all convergence results on quasiconformal mappings I am aware of deal with the situation that the are all -quasiconformal. Any kind of help is highly appreciated - thanks in advance!",G \mathbb{C} f_n: G \rightarrow \mathbb{C} f_n G G f: G \rightarrow \mathbb{C} f f_n K K < \infty f K f_n K f_n K f_n K f_n K,"['complex-analysis', 'uniform-convergence', 'quasiconformal-maps']"
76,Estimate the derivative with Cauchy integral formula,Estimate the derivative with Cauchy integral formula,,"Suppose a metric $d$ is defined on the space of entire functions as follows: $$d(f, g)=\sum_{n=1}^{\infty} \min \left\{\frac{1}{2^{n}}, \max _{|z| \leq n}|f(z)-g(z)|\right\}$$ Is the operator of differentiation (the operator sending $f$ to $f^\prime$ )  continuous on this metric space of functions? Explain why or why not. Here is my try with Cauchy's Integral Formula. For $|z|<n$ , $$\begin{aligned}|f^\prime(z)-g^\prime(z)|&= |\frac{1}{2\pi i}\int_{|w|=n}\frac{f(w)-g(w)}{(w-z)^2} dw|\\ &\leq \frac{1}{2\pi} \max_{|w|\leq n}|f(w)-g(w)| \int_{|w|=n} \frac{1}{(w-z)^2} dw.  \end{aligned}$$ Since $z$ can be very closed to the boundary, $|w-z|$ can be very small. I don't know how to bound the integral in the inequality. Any idea to solve this question?","Suppose a metric is defined on the space of entire functions as follows: Is the operator of differentiation (the operator sending to )  continuous on this metric space of functions? Explain why or why not. Here is my try with Cauchy's Integral Formula. For , Since can be very closed to the boundary, can be very small. I don't know how to bound the integral in the inequality. Any idea to solve this question?","d d(f, g)=\sum_{n=1}^{\infty} \min \left\{\frac{1}{2^{n}}, \max _{|z| \leq n}|f(z)-g(z)|\right\} f f^\prime |z|<n \begin{aligned}|f^\prime(z)-g^\prime(z)|&=
|\frac{1}{2\pi i}\int_{|w|=n}\frac{f(w)-g(w)}{(w-z)^2} dw|\\
&\leq \frac{1}{2\pi} \max_{|w|\leq n}|f(w)-g(w)| \int_{|w|=n} \frac{1}{(w-z)^2} dw.
 \end{aligned} z |w-z|",['complex-analysis']
77,"Asymptotic behavior of $\sum_{k=0}^{N-1} \sum_{n \neq k} \frac{1}{\vert e_k^N-e_n^N \vert^2}$ as $N\to\infty$, where $e_k^N :=e^{2\pi i k/N}$","Asymptotic behavior of  as , where",\sum_{k=0}^{N-1} \sum_{n \neq k} \frac{1}{\vert e_k^N-e_n^N \vert^2} N\to\infty e_k^N :=e^{2\pi i k/N},"Consider the unit circle $\{x \in \mathbb C: \vert x \vert_2=1 \}$ then we can now consider the roots of unity $$e_k^N :=e^{2\pi i k/N}\text{ for }k \in \{0,..,N-1\}$$ on that circle. We can now define the sum over all possible inverse-square distances $$\sum_{k=0}^{N-1} \sum_{n \neq k} \frac{1}{\vert e_k^N-e_n^N \vert^2}$$ My question is: How does this sum behave as $N$ tends to infinity? Obviously, it will go to infinity, but what is the asymptotic behaviour? Please let me know if you have any questions.","Consider the unit circle then we can now consider the roots of unity on that circle. We can now define the sum over all possible inverse-square distances My question is: How does this sum behave as tends to infinity? Obviously, it will go to infinity, but what is the asymptotic behaviour? Please let me know if you have any questions.","\{x \in \mathbb C: \vert x \vert_2=1 \} e_k^N :=e^{2\pi i k/N}\text{ for }k \in \{0,..,N-1\} \sum_{k=0}^{N-1} \sum_{n \neq k} \frac{1}{\vert e_k^N-e_n^N \vert^2} N","['real-analysis', 'calculus']"
78,Prove that the integral of $e^{k\cos t}\cos(k\sin t)$ from $0$ to $2\pi$ equals $2\pi$,Prove that the integral of  from  to  equals,e^{k\cos t}\cos(k\sin t) 0 2\pi 2\pi,"Prove that $$I=\int_0^{2\pi}e^{k\cos t}\cos(k\sin t)\,dx=2\pi$$ and prove that $$J=\int_0^{2\pi}e^{k\cos t}\sin(k\sin t)\,dx=0$$ With the help of the following integral: $$H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx$$ I proved that the  : $$H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx=2i\pi$$ and I proved that $$I+iJ=\frac{H}{i}=2\pi$$ but what should I do next to prove that $I=2\pi$ or to prove that $J=0$ ? Is it by using $Re(z)=\cos t$ ?",Prove that and prove that With the help of the following integral: I proved that the  : and I proved that but what should I do next to prove that or to prove that ? Is it by using ?,"I=\int_0^{2\pi}e^{k\cos t}\cos(k\sin t)\,dx=2\pi J=\int_0^{2\pi}e^{k\cos t}\sin(k\sin t)\,dx=0 H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx H=\int_{|z|=1}\frac{e^{kz}}{z}\,dx=2i\pi I+iJ=\frac{H}{i}=2\pi I=2\pi J=0 Re(z)=\cos t","['integration', 'complex-analysis', 'definite-integrals', 'complex-numbers', 'bessel-functions']"
79,"$f(z) = \frac{1}{\pi} \int_E \frac{f(w)}{(1-z \bar{w})^2} dudv $ , where $w = u + iv$",", where",f(z) = \frac{1}{\pi} \int_E \frac{f(w)}{(1-z \bar{w})^2} dudv  w = u + iv,"Let $f(z)$ be holomorphic on an open region containing the closed unit disc $E = \{z \in \mathbb{C} : \mid z \mid \leq 1 \}$ . Then show that $$ f(z) = \frac{1}{\pi} \int_E \frac{f(w)}{(1-z \bar{w})^2} dudv , \;\; \text{where} \;\; w=u+iv$$ for every $z$ with $\mid z \mid <1$ . My attempt $f(z)= \frac{1}{2\pi i} \int_{C_r}\frac{f(\xi)}{\xi - z} d\xi  =  \frac{1}{2\pi}  \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta $ , by cauchy integral formula , where $C_r$ is a circle positively oriented contour centered at $z$ . Then, $f(z) = \int_0^1 f(z) dr= \int_0^1  \frac{1}{2\pi}  \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta dr  = \frac{1}{2\pi} \int_0^1    \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta dr $ . By polar coordinate argument, $f(z) = \frac{1}{2\pi} \int_E     \frac{f(u+iv)} {u+iv - z} \frac{u+iv}{\mid u+iv \mid}dudv = \frac{1}{2\pi} \int_E     \frac{f(w)} {w - z} \frac{w}{\mid w \mid}dudv $ . May I ask you how to proceed this problem?","Let be holomorphic on an open region containing the closed unit disc . Then show that for every with . My attempt , by cauchy integral formula , where is a circle positively oriented contour centered at . Then, . By polar coordinate argument, . May I ask you how to proceed this problem?","f(z) E = \{z \in \mathbb{C} : \mid z \mid \leq 1 \}  f(z) = \frac{1}{\pi} \int_E \frac{f(w)}{(1-z \bar{w})^2} dudv , \;\; \text{where} \;\; w=u+iv z \mid z \mid <1 f(z)= \frac{1}{2\pi i} \int_{C_r}\frac{f(\xi)}{\xi - z} d\xi  =  \frac{1}{2\pi}  \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta  C_r z f(z) = \int_0^1 f(z) dr= \int_0^1  \frac{1}{2\pi}  \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta dr  = \frac{1}{2\pi} \int_0^1    \int_0^{2\pi} \frac{f(r e^{i \theta})} {r e^{i \theta} - z} r e^{i \theta} d\theta dr  f(z) = \frac{1}{2\pi} \int_E     \frac{f(u+iv)} {u+iv - z} \frac{u+iv}{\mid u+iv \mid}dudv = \frac{1}{2\pi} \int_E     \frac{f(w)} {w - z} \frac{w}{\mid w \mid}dudv ","['complex-analysis', 'complex-integration']"
80,Transformation of vertical line under $f(z)=\tan(z)$,Transformation of vertical line under,f(z)=\tan(z),"I've managed to prove that $$\tan(z)=\tan(x+iy)=\frac{\sin 2x}{\cos 2x+\cosh 2y}+i\,\frac{\sinh 2y}{\cos 2x+\cosh 2y}$$ Now, I've also been able to use this identity to show that the vertical line $x=\pi/4$ gets mapped onto a portion of a circle. However, I am wondering if there is a way to use the tangent identity above to show that if $x=a$ is a vertical line, where $-\pi/4<a<\pi/4$ , then $\tan(z)$ maps this line onto a portion of a circle. Update Thanks to Maxim's comment below, solving the first of the two equations below $$u=\frac{\sin 2a}{\cos 2a+\cosh 2y}\qquad\text{and}\qquad v=\frac{\sinh 2y}{\cos 2a+\cosh 2y}$$ gave: $$\cosh 2y=\frac{\sin 2a-u\cos 2a}{u}$$ Substituting this into the second equation and solving, gave: $$\sinh 2y=\frac{v\sin 2a}{u}$$ Substituting each of these into $\cosh^22y-\sinh^22y=1$ and expanding and manipulating and completing the square led to the result $$\left(u+\frac{\cos 2a}{\sin 2a}\right)^2+v^2=1+\frac{\cos^22a}{\sin^22a}$$ Substituting $a=\pi/8$ gives the equation $(u+1)^2+v^2=2$ . Here is the Matlab code and image, which shows that his suggestion worked. figure('Units','Normalized','Position',[0.1,0.1,0.8,0.5]) y=linspace(-5,5); x=pi/8*ones(size(y)); subplot(1,2,1), plot(x,y,'LineWidth',2,'Color','r'), hold on grid on, axis([-3,3,-3,3]), xticks(-3*pi/4:pi/4:3*pi/4) xticklabels({'-3\pi/4','-\pi/2','-\pi/4','0','pi/4','\pi/2','3\pi/4'}) xlabel('x'),ylabel('y'), ax=gca; ax.FontSize=14; ax=gca; ax.XAxisLocation='origin'; ax.YAxisLocation='origin'; f=@(z) tan(z); z=complex(x,y); w=f(z);  subplot(1,2,2) plot(w,'LineWidth',2,'Color','b'), hold on g=@(u,v) (u+1).^2+v.^2-2; fimplicit(g) grid on, axis([-3,3,-3,3]),  xlabel('u'),ylabel('v'), ax=gca; ax.FontSize=14; ax.XAxisLocation='origin'; ax.YAxisLocation='origin'; Note that the blue part in the graph on the right is image of the vertical line under the transformation $f(z)=\tan(z)$ . It is only a portion of the full circle, but exactly what I wanted.","I've managed to prove that Now, I've also been able to use this identity to show that the vertical line gets mapped onto a portion of a circle. However, I am wondering if there is a way to use the tangent identity above to show that if is a vertical line, where , then maps this line onto a portion of a circle. Update Thanks to Maxim's comment below, solving the first of the two equations below gave: Substituting this into the second equation and solving, gave: Substituting each of these into and expanding and manipulating and completing the square led to the result Substituting gives the equation . Here is the Matlab code and image, which shows that his suggestion worked. figure('Units','Normalized','Position',[0.1,0.1,0.8,0.5]) y=linspace(-5,5); x=pi/8*ones(size(y)); subplot(1,2,1), plot(x,y,'LineWidth',2,'Color','r'), hold on grid on, axis([-3,3,-3,3]), xticks(-3*pi/4:pi/4:3*pi/4) xticklabels({'-3\pi/4','-\pi/2','-\pi/4','0','pi/4','\pi/2','3\pi/4'}) xlabel('x'),ylabel('y'), ax=gca; ax.FontSize=14; ax=gca; ax.XAxisLocation='origin'; ax.YAxisLocation='origin'; f=@(z) tan(z); z=complex(x,y); w=f(z);  subplot(1,2,2) plot(w,'LineWidth',2,'Color','b'), hold on g=@(u,v) (u+1).^2+v.^2-2; fimplicit(g) grid on, axis([-3,3,-3,3]),  xlabel('u'),ylabel('v'), ax=gca; ax.FontSize=14; ax.XAxisLocation='origin'; ax.YAxisLocation='origin'; Note that the blue part in the graph on the right is image of the vertical line under the transformation . It is only a portion of the full circle, but exactly what I wanted.","\tan(z)=\tan(x+iy)=\frac{\sin 2x}{\cos 2x+\cosh 2y}+i\,\frac{\sinh 2y}{\cos 2x+\cosh 2y} x=\pi/4 x=a -\pi/4<a<\pi/4 \tan(z) u=\frac{\sin 2a}{\cos 2a+\cosh 2y}\qquad\text{and}\qquad v=\frac{\sinh 2y}{\cos 2a+\cosh 2y} \cosh 2y=\frac{\sin 2a-u\cos 2a}{u} \sinh 2y=\frac{v\sin 2a}{u} \cosh^22y-\sinh^22y=1 \left(u+\frac{\cos 2a}{\sin 2a}\right)^2+v^2=1+\frac{\cos^22a}{\sin^22a} a=\pi/8 (u+1)^2+v^2=2 f(z)=\tan(z)",['complex-analysis']
81,Proof from From Visual Complex Functions,Proof from From Visual Complex Functions,,"From Visual Complex Functions with Phase Portraits by Wegert (author). From the book. I am stuck on the proof of the theorem. I have indicated by ( ͡° ͜ʖ ͡°) where in the proof I start to get lost. The definition for $q$ confuses me too. Here's the theorem that's being proved. Theorem 3.2.9. If $f$ is analytic at $z_{0}$ and $f(z_{0}) \neq 0$ ,  then $1/f$ is analytic at $z_{0}.$ The Taylor coefficients $b_{k}$ of $1/f$ at $z_{0}$ can be computed recursively from the Taylor coefficients $a_{k}$ of $f$ by $b_{0} :=1/a_{0}$ and $b_{k} :=-\displaystyle \frac{1}{a_{0}}(a_{1}b_{k-1}+a_{2}b_{k-2}+\ldots+a_{k}b_{0})$ , $k=1$ , 2, . . . .   (3.37) The proof makes use of the following lemma. Lemma 3.2.2 (Abel-Weierstrass). Let $R$ be the radius of convergence of the power series (3.16). (i)  If $0\leq r< R$ ,   then there exists a constant $c$ such that for all $k\in\mathbb{N}$ $r^{k}| a_{k}|\ \leq c$ .   (3.22) (ii)   If there exist positive numbers $r$ and $c$ such that  (3.22)  holds for all sufficiently large $k\in\mathbb{N}$ ,  then $R\geq r.$ The inequality (3.22) is also known as  Cauchy's estimate. Please explain from the line to the end. Here is the Actual steps in the proof:-- Proof. 1. In the first step we assume that the function $1/f$ is analytic at $z_{0}$ . Then its Taylor series $\displaystyle \frac{1}{f(z)}=b_{0}+b_{1}(z-z_{0})+b_{2}(z-z_{0})^{2}+\ldots+b_{k}(z-z_{0})^{k}+\ldots$ (3.38) converges in a neighborhood of $z_{0}$ and its Cauchy product with the Taylor series of $f$ is the constant function 1. The latter is equivalent to the infinite system of equations \begin{aligned} a_{0}b_{0}=1  \\ a_{0}b_{1}+a_{1}b_{0}=0 \\ a_{0}b_{2}+a_{1}b_{1}+a_{2}b_{0}=0 \end{aligned} Since $a_{0}\neq 0$ , this triangular system can be solved with respect to the coefficients $b_{k}$ , which yields the recursion (3.37). It remains to prove that the series (3.38), with coefficients $b_{k}$ given by the recursion (3.37), indeed has a positive radius of convergence. By Cauchy's estimate (3.22) in Lemma 3.2.2, there are positive numbers $c$ and $r$ such that $|a_{n}| \leq cr^{-n}$ for all $n\in \mathbb{N}$ . We set $q :=1+c/|a_{0}|$ and show that ( ͡° ͜ʖ ͡°) $|b_{n}| \displaystyle \leq\frac{c}{|a_{0}|^{2}}\frac{q^{n-1}}{r^{n}},n=$ 1, 2, . . . .   (3.39) For $n=1$ we have $b_{1} =-a_{1}/a_{0}^{2}$ and $|a_{1}| \leq c/r$ , so that indeed $$ |b_{1}|=\frac{a_{1}}{a_{0}^{2}}\leq\ \frac{c}{|a_{0}|^{2}}\frac{1}{r}. $$ Now we assume that (3.39) holds for all $n=1$ , 2, . . . , $k-1$ and consider the case where $n=k$ . Using $|b_{0}| = 1/|a_{0}|$ , the recursive definition of $b_{k}$ , and the triangle inequality, we estimate $$ |b_{k}|\leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}|a_{k-j}||b_{j}|) $$ $$ \leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}\frac{c}{r^{k-j}}\frac{c}{r^{j}|a_{0}|^{2}}q^{j-1}) $$ $$ \leq\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\ \sum_{j=0}^{k-2}q^{j}) $$ $$ =\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\frac{q^{k-1}-1}{q-1})\ =\frac{c}{r^{k}|a_{0}|^{2}}q^{k-1}, $$ which gives (3.39) for $n= k$ and thus for all $n$ . Consequently, by Lemma 3.2.2, the power series (3.38) has radius of convergence not less than $r/q. \square $ Since posting I looked hard at it and it has suddenly all opened up to me like a flower bud tighly closed now fully open. I understand.","From Visual Complex Functions with Phase Portraits by Wegert (author). From the book. I am stuck on the proof of the theorem. I have indicated by ( ͡° ͜ʖ ͡°) where in the proof I start to get lost. The definition for confuses me too. Here's the theorem that's being proved. Theorem 3.2.9. If is analytic at and ,  then is analytic at The Taylor coefficients of at can be computed recursively from the Taylor coefficients of by and , , 2, . . . .   (3.37) The proof makes use of the following lemma. Lemma 3.2.2 (Abel-Weierstrass). Let be the radius of convergence of the power series (3.16). (i)  If ,   then there exists a constant such that for all .   (3.22) (ii)   If there exist positive numbers and such that  (3.22)  holds for all sufficiently large ,  then The inequality (3.22) is also known as  Cauchy's estimate. Please explain from the line to the end. Here is the Actual steps in the proof:-- Proof. 1. In the first step we assume that the function is analytic at . Then its Taylor series (3.38) converges in a neighborhood of and its Cauchy product with the Taylor series of is the constant function 1. The latter is equivalent to the infinite system of equations Since , this triangular system can be solved with respect to the coefficients , which yields the recursion (3.37). It remains to prove that the series (3.38), with coefficients given by the recursion (3.37), indeed has a positive radius of convergence. By Cauchy's estimate (3.22) in Lemma 3.2.2, there are positive numbers and such that for all . We set and show that ( ͡° ͜ʖ ͡°) 1, 2, . . . .   (3.39) For we have and , so that indeed Now we assume that (3.39) holds for all , 2, . . . , and consider the case where . Using , the recursive definition of , and the triangle inequality, we estimate which gives (3.39) for and thus for all . Consequently, by Lemma 3.2.2, the power series (3.38) has radius of convergence not less than Since posting I looked hard at it and it has suddenly all opened up to me like a flower bud tighly closed now fully open. I understand.","q f z_{0} f(z_{0}) \neq 0 1/f z_{0}. b_{k} 1/f z_{0} a_{k} f b_{0} :=1/a_{0} b_{k} :=-\displaystyle \frac{1}{a_{0}}(a_{1}b_{k-1}+a_{2}b_{k-2}+\ldots+a_{k}b_{0}) k=1 R 0\leq r< R c k\in\mathbb{N} r^{k}| a_{k}|\ \leq c r c k\in\mathbb{N} R\geq r. 1/f z_{0} \displaystyle \frac{1}{f(z)}=b_{0}+b_{1}(z-z_{0})+b_{2}(z-z_{0})^{2}+\ldots+b_{k}(z-z_{0})^{k}+\ldots z_{0} f \begin{aligned}
a_{0}b_{0}=1  \\
a_{0}b_{1}+a_{1}b_{0}=0 \\
a_{0}b_{2}+a_{1}b_{1}+a_{2}b_{0}=0
\end{aligned} a_{0}\neq 0 b_{k} b_{k} c r |a_{n}| \leq cr^{-n} n\in \mathbb{N} q :=1+c/|a_{0}| |b_{n}| \displaystyle \leq\frac{c}{|a_{0}|^{2}}\frac{q^{n-1}}{r^{n}},n= n=1 b_{1} =-a_{1}/a_{0}^{2} |a_{1}| \leq c/r 
|b_{1}|=\frac{a_{1}}{a_{0}^{2}}\leq\ \frac{c}{|a_{0}|^{2}}\frac{1}{r}.
 n=1 k-1 n=k |b_{0}| = 1/|a_{0}| b_{k} 
|b_{k}|\leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}|a_{k-j}||b_{j}|)
 
\leq\frac{1}{|a_{0}|}(|a_{k}b_{0}|+\sum_{j=1}^{k-1}\frac{c}{r^{k-j}}\frac{c}{r^{j}|a_{0}|^{2}}q^{j-1})
 
\leq\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\ \sum_{j=0}^{k-2}q^{j})
 
=\frac{c}{r^{k}|a_{0}|^{2}}(1+\frac{c}{|a_{0}|}\frac{q^{k-1}-1}{q-1})\ =\frac{c}{r^{k}|a_{0}|^{2}}q^{k-1},
 n= k n r/q. \square ","['complex-analysis', 'power-series']"
82,Number of complex roots,Number of complex roots,,"Find total number of roots inside $|z| \le 1$ for $\sin(z^{100})=z/11$ . I tried to apply Rouché's theorem: For any two complex-valued functions $f$ and $g$ holomorphic inside some region $K$ with closed contour $\partial K$ if $|g(z)| < |f(z)|$ on $\partial K$ then $f$ and $f + g$ have the same number of zeros inside $K$ . Also, $\sin(z^{100})=\sum_{n=0}^\infty \frac{(z^{100})^{2n+1}}{(2n+1)!}\cdot(-1)^n$ However, it gets me nowhere. Could you please give me a hint?","Find total number of roots inside for . I tried to apply Rouché's theorem: For any two complex-valued functions and holomorphic inside some region with closed contour if on then and have the same number of zeros inside . Also, However, it gets me nowhere. Could you please give me a hint?",|z| \le 1 \sin(z^{100})=z/11 f g K \partial K |g(z)| < |f(z)| \partial K f f + g K \sin(z^{100})=\sum_{n=0}^\infty \frac{(z^{100})^{2n+1}}{(2n+1)!}\cdot(-1)^n,['complex-analysis']
83,Calculate a fourier transform of inverse of polynomial $\frac{1}{Q(x)}$,Calculate a fourier transform of inverse of polynomial,\frac{1}{Q(x)},"I'm working with Exercise 4 of Chapter 4, Stein & Shakarchi ""Complex Analysis"" . The problem is Suppose $Q$ is a polynomial of degree $\geq2$ with distinct roots, none lying on the real axis. Calculate $$F(\xi) = \int_{-\infty}^{\infty} \dfrac{e^{-2\pi i x \xi}}{Q(x)}\ dx,\space\xi\in\mathbb R$$ in terms of the roots of $Q$ . What happens when several roots coincide? For $\xi=0$ , I approached by residue formula using upper half circle $C_R^+$ and $R\to\infty$ for the first summation and $C_R^-$ for the second summation and got the below: $$\dfrac{F(0)}{2\pi i} = \sum_{\Im(s_i)>0} \operatorname{Res}\dfrac{1}{Q(z)}=-\sum_{\Im(s_i)<0} \operatorname{Res}\dfrac{1}{Q(z)} $$ where $s_i$ 's are distinct roots of $Q$ . I think there would be a better representation for this using $Q(x)=A\prod(x-s_i)$ so that I can get a ""Fourier transform formula"" of $\frac{1}{Q(x)}$ , but I can't go further. For $\xi<0$ and $\xi>0$ , I find that I should use $C_R^+$ and $C_R^-$ respectively but that's all. I don't know about the the role of coinciding several roots too. How can I get some meaningful formulas for this problem? Edit Thanks for hints and answer. Using all I got for $\xi\geq 0$ , $$ F(\xi) = -2\pi i \sum_{\Im(s_i)<0} \operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = -2\pi i \sum_{\Im(s_i)<0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)} $$ where the second equality holds when $Q(x)$ has all distinct roots and $Q’(s_i)=A\prod_{s_j\neq s_i}(s_j-s_i)$ . Also for $\xi\leq 0$ , $$ F(\xi) = 2\pi i \sum_{\Im(s_i)>0}\operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = 2\pi i \sum_{\Im(s_i)>0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)} $$ For multiple roots, the residue formula of $\frac{1}{Q(z)}$ where $Q(z)=A\prod(z-s_i)^{r_i}$ would be different, but again I can’t approach. The residue formula for $n$ -th order poles requires $(n-1)$ -times of differentiation, and I feel this direct route is too dirty. How can I approach this?","I'm working with Exercise 4 of Chapter 4, Stein & Shakarchi ""Complex Analysis"" . The problem is Suppose is a polynomial of degree with distinct roots, none lying on the real axis. Calculate in terms of the roots of . What happens when several roots coincide? For , I approached by residue formula using upper half circle and for the first summation and for the second summation and got the below: where 's are distinct roots of . I think there would be a better representation for this using so that I can get a ""Fourier transform formula"" of , but I can't go further. For and , I find that I should use and respectively but that's all. I don't know about the the role of coinciding several roots too. How can I get some meaningful formulas for this problem? Edit Thanks for hints and answer. Using all I got for , where the second equality holds when has all distinct roots and . Also for , For multiple roots, the residue formula of where would be different, but again I can’t approach. The residue formula for -th order poles requires -times of differentiation, and I feel this direct route is too dirty. How can I approach this?","Q \geq2 F(\xi) = \int_{-\infty}^{\infty} \dfrac{e^{-2\pi i x \xi}}{Q(x)}\ dx,\space\xi\in\mathbb R Q \xi=0 C_R^+ R\to\infty C_R^- \dfrac{F(0)}{2\pi i} = \sum_{\Im(s_i)>0} \operatorname{Res}\dfrac{1}{Q(z)}=-\sum_{\Im(s_i)<0} \operatorname{Res}\dfrac{1}{Q(z)}  s_i Q Q(x)=A\prod(x-s_i) \frac{1}{Q(x)} \xi<0 \xi>0 C_R^+ C_R^- \xi\geq 0  F(\xi) = -2\pi i \sum_{\Im(s_i)<0} \operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = -2\pi i \sum_{\Im(s_i)<0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)}  Q(x) Q’(s_i)=A\prod_{s_j\neq s_i}(s_j-s_i) \xi\leq 0  F(\xi) = 2\pi i \sum_{\Im(s_i)>0}\operatorname{Res} \dfrac{e^{-2\pi i x \xi}}{Q(x)} = 2\pi i \sum_{\Im(s_i)>0} \dfrac{e^{-2\pi i s_i \xi}}{Q’(s_i)}  \frac{1}{Q(z)} Q(z)=A\prod(z-s_i)^{r_i} n (n-1)","['complex-analysis', 'fourier-transform']"
84,"Prove that if $f$ is entire and $\vert f(z^2) \vert \leq 2 \vert f(z) \vert$, then $f$ is constant","Prove that if  is entire and , then  is constant",f \vert f(z^2) \vert \leq 2 \vert f(z) \vert f,I'm not sure if this requires Liouville's theorem or the use of the integral formula for the Taylor coefficients but I cant get either to work. By the formula for the Taylor coefficients for $f(z^2)$ we have that: $$\vert a_n\vert  = \Big\vert \frac{1}{2 \pi i} \int_{\vert z\vert =R} \frac{f(z^2)}{z^{n+1}} dz\Big\vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{\vert f(z^2) \vert}{\vert z \vert^{n+1}} \vert dz \vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{2\vert f(z) \vert}{\vert z \vert^{n+1}} \vert dz \vert  $$ and then this can be bounded by $$ \frac{1}{2 \pi } \frac{M_f(R)}{R^{n+1}} 2 \pi R =  \frac{M_f(R)}{R^{n}} $$ where $M_f(R)$ is the max of $\vert f\vert $ on the circle $\vert z \vert =R$ . I had wanted to take the limit as $R \to \infty$ and have that it equals $0$ unless $n=0$ but this isn't clear to me.,I'm not sure if this requires Liouville's theorem or the use of the integral formula for the Taylor coefficients but I cant get either to work. By the formula for the Taylor coefficients for we have that: and then this can be bounded by where is the max of on the circle . I had wanted to take the limit as and have that it equals unless but this isn't clear to me.,f(z^2) \vert a_n\vert  = \Big\vert \frac{1}{2 \pi i} \int_{\vert z\vert =R} \frac{f(z^2)}{z^{n+1}} dz\Big\vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{\vert f(z^2) \vert}{\vert z \vert^{n+1}} \vert dz \vert \leq \frac{1}{2 \pi} \int_{\vert z \vert =R} \frac{2\vert f(z) \vert}{\vert z \vert^{n+1}} \vert dz \vert    \frac{1}{2 \pi } \frac{M_f(R)}{R^{n+1}} 2 \pi R =  \frac{M_f(R)}{R^{n}}  M_f(R) \vert f\vert  \vert z \vert =R R \to \infty 0 n=0,[]
85,Meromorphic continuation of the multifactorial,Meromorphic continuation of the multifactorial,,"The double factorial $z!!$ , like the normal factorial function, can be extended to the complex plane using $$ z!! = 2^{z/2} \left(\frac{\pi}{2}\right)^{(\cos\pi z-1)/4} \left(\frac{z}{2}\right)! $$ which is a slightly nicer-looking variant from Wolfram Mathworld and is what you get when you type in ""double factorial"" in Wolfram Alpha. (NB $z! = \Gamma(z+1)$ just for consistency here.) The function can take complex numbers, and satisfies the empty product , $0!! = 1$ as well as the double factorial for negative numbers using the identity $$ x!! = x(x-2)!! \implies x!! = \frac{(x+2)!!}{x+2} $$ thus the poles at even negative numbers and the intermediate values $(-1)!! = 1$ , $(-3)!! = -1$ , $(-5)!! = 1/3$ , etc. My question is that is there a similar extension for the triple factorial and further multifactorials? I know there are extensions for the general multifactorial, but they don't cover all the premises raised here. For example, this function is one possible extension: $$ z\overset{\alpha}{\overbrace{!\cdots !}} = \alpha^{(z-1)/\alpha}\frac{(z/\alpha)!}{(1/\alpha)!}$$ but here $0!\cdots ! \neq 1$ . Has there been found a meromorphic function $z\overset{\alpha}{\overbrace{!\cdots !}}, \alpha\gt 0$ (probably in the form $f(z)(z/\alpha)!$ ) to the multifactorial such that they agree with the following...? $$z\overset{\alpha}{\overbrace{!\cdots !}} = \prod_{k=0}^{\left\lfloor\frac{z-1}{\alpha}\right\rfloor}(z-\alpha k)\qquad z\in\mathbb{N} \tag{1} $$ $$ \left.\begin{aligned} z\overset{\alpha}{\overbrace{!\cdots !}} = \frac{(z+\alpha) \overset{\alpha}{\overbrace{!\cdots !}}}{z+\alpha} \\ \implies (-n\alpha)\overset{\alpha}{\overbrace{!\cdots !}} = \pm\infty \end{aligned} \ \right\} \ n\in\mathbb{N}\backslash 0 \tag{2} $$ $$ 0\overset{\alpha}{\overbrace{!\cdots !}}  =1 \tag{3}$$","The double factorial , like the normal factorial function, can be extended to the complex plane using which is a slightly nicer-looking variant from Wolfram Mathworld and is what you get when you type in ""double factorial"" in Wolfram Alpha. (NB just for consistency here.) The function can take complex numbers, and satisfies the empty product , as well as the double factorial for negative numbers using the identity thus the poles at even negative numbers and the intermediate values , , , etc. My question is that is there a similar extension for the triple factorial and further multifactorials? I know there are extensions for the general multifactorial, but they don't cover all the premises raised here. For example, this function is one possible extension: but here . Has there been found a meromorphic function (probably in the form ) to the multifactorial such that they agree with the following...?","z!!  z!! = 2^{z/2} \left(\frac{\pi}{2}\right)^{(\cos\pi z-1)/4} \left(\frac{z}{2}\right)!  z! = \Gamma(z+1) 0!! = 1  x!! = x(x-2)!! \implies x!! = \frac{(x+2)!!}{x+2}  (-1)!! = 1 (-3)!! = -1 (-5)!! = 1/3  z\overset{\alpha}{\overbrace{!\cdots !}} = \alpha^{(z-1)/\alpha}\frac{(z/\alpha)!}{(1/\alpha)!} 0!\cdots ! \neq 1 z\overset{\alpha}{\overbrace{!\cdots !}}, \alpha\gt 0 f(z)(z/\alpha)! z\overset{\alpha}{\overbrace{!\cdots !}} = \prod_{k=0}^{\left\lfloor\frac{z-1}{\alpha}\right\rfloor}(z-\alpha k)\qquad z\in\mathbb{N} \tag{1}   \left.\begin{aligned} z\overset{\alpha}{\overbrace{!\cdots !}} = \frac{(z+\alpha) \overset{\alpha}{\overbrace{!\cdots !}}}{z+\alpha} \\ \implies (-n\alpha)\overset{\alpha}{\overbrace{!\cdots !}} = \pm\infty \end{aligned} \ \right\} \ n\in\mathbb{N}\backslash 0 \tag{2}   0\overset{\alpha}{\overbrace{!\cdots !}}  =1 \tag{3}","['complex-analysis', 'factorial', 'gamma-function', 'analytic-continuation', 'meromorphic-functions']"
86,How to determinate the the number of crossing points?,How to determinate the the number of crossing points?,,"This question is an extension of the question: how-to-determine-the-convergence-the-start-and-the-finish-points . One can apply the next algoritm and obtaine the 1-2-3 grid pattern. On square grid paper start in the middle. Draw a line 1-unit long. Turn a right angle clockwise. Draw a line 2-unit long. Turn a right angle clockwise. Draw a line 3-unit long. Repeat steps 1-6 four times. Аfter the step 6 you return to the start point (the red square #1 ). This rule is correct for any a-b-c grid pattern, where $a, b, c \in \mathbb{N}$ .  On the figure you can see the five crossing points (four green and one blue) for the '1-2-3' pattern. The red line marks the movement by steps 1-6 from the origin. The order of $a$ , $b$ , and $c$ in the triplet is irrelevant and a desired closed-path motion can be obtained by combining translation and rotation for the original closed-path motion through matrix multiplications. Question. Is there a rule by which it can be determined the number of self-crossing points (nodes), k ,  for the a-b-c grid pattern? I am looking for a function f(a,b,c)=k . My Attempt based on Graph Theory. Below on figure you can see four patterns: 5-2-2 , 3-2-4 , 3-2-2 and 3-3-3 pattern. Start and finish points denoted by the square 1 . For the 5-2-2 pattern you can see four crossing points, for the 3-2-4 pattern -- eight crossing points, for the 3-2-2 pattern -- 12 crossing points, for the 3-3-3 pattern -- self-crossing points are absent. In the last case we can say that path was repeated in four points (squares with numbers #1, #4, #7, #10). When forming a triplet $(a, b, c)$ it is necessary to consider at least three  cases: a) all numbers are equal to one another $a = b = c$ , b) all numbers are  unique $a \neq b \neq c$ , and c) a triplet has the two repeating elements, for example, $a = b$ . In the last case, there are three possible relations between $a$ and $b$ should be emphasized: a) $a = 2\cdot b$ , b) $a> 2\cdot b$ , and c) $a < 2\cdot b$ . We have found the seven different cases but can defined f() for five cases only: $f(a,b,c)= \left\{ \begin{array}{ll} 0, & a=b=c, \\ 8, & a<b<c \quad \text{and} \quad c < a+b, \\ ?, & a<b<c \quad \text{and} \quad c=a+b, \\ ?, & a<b<c \quad \text{and} \quad a+b<c, \\ 12, & a=b<c \quad \text{and} \quad c<a+a, \\ 5, & a=b<c \quad \text{and} \quad c=a+a, \\ 4, & a=b<c \quad \text{and} \quad a+a<c. \end{array} \right.$ As one can see a resulted graph is connected, its vertices can have odd or even degrees (2, 3 and 4). On the figure the vertecies with degree equals to 2 is colored by red, the vertecies with degree equals to 3 is colored by green and  the degree of blue vertecies is 4 . Based on examples above we can suggest that the number of self-crossing point k equals to the number of vertices that have degree more than 2 . My Attempt based on Complex Analisys. Follow to the Chris Culter answer , we work in the complex plane. Let $z=e^{i\alpha}$ . Then the first part of closed-path motion (red lines on the figures above) is: $$az^0+bz+cz^2 \tag{A}$$ and $\alpha=-\frac{\pi}{2}$ . We return to the start point #1 after $N=4$ steps.  We write equations for second, third and fourth segments by analogy: $$az^3+bz^4+cz^5, \tag{B}$$ $$az^6+bz^7+cz^8, \tag{C}$$ $$az^{9}+bz^{10}+cz^{11}. \tag{D}$$ The full path equation is $$(az^0+bz+cz^2)+(az^3+bz^4+cz^5)+ (az^6+bz^7+cz^8)+ (az^{9}+bz^{10}+cz^{11})= 0. \tag{1}$$ Then $z=e^{i\alpha}=cos(\alpha)+i \cdot sin(\alpha)=cos(-\frac{\pi}{2})+i \cdot sin(-\frac{\pi}{2}). \tag{2}$ Substitute the expression (2) in the formula (1) and collect similar terms: $$a - i \cdot b - c + i \cdot a + b - i \cdot c - a + i \cdot b + c - i \cdot a - b + i \cdot c = 0.$$ Let us take the fist pair of relations (A) $az^0+bz+cz^2$ , (B) $az^3+bz^4+cz^5$ and the 3-2-4 pattern ( $a=3$ , $b=2$ , $c=4$ ). Using the Wolfram alpha the equation $3+2 \cdot z+4\cdot z^2=3 \cdot z^3+2 \cdot z^4+4 \cdot z^5$ was solved. We found one real root $z=1$ and four complex solutions. Refs Marc Lackenby (2016) Elementary Knot Theory","This question is an extension of the question: how-to-determine-the-convergence-the-start-and-the-finish-points . One can apply the next algoritm and obtaine the 1-2-3 grid pattern. On square grid paper start in the middle. Draw a line 1-unit long. Turn a right angle clockwise. Draw a line 2-unit long. Turn a right angle clockwise. Draw a line 3-unit long. Repeat steps 1-6 four times. Аfter the step 6 you return to the start point (the red square #1 ). This rule is correct for any a-b-c grid pattern, where .  On the figure you can see the five crossing points (four green and one blue) for the '1-2-3' pattern. The red line marks the movement by steps 1-6 from the origin. The order of , , and in the triplet is irrelevant and a desired closed-path motion can be obtained by combining translation and rotation for the original closed-path motion through matrix multiplications. Question. Is there a rule by which it can be determined the number of self-crossing points (nodes), k ,  for the a-b-c grid pattern? I am looking for a function f(a,b,c)=k . My Attempt based on Graph Theory. Below on figure you can see four patterns: 5-2-2 , 3-2-4 , 3-2-2 and 3-3-3 pattern. Start and finish points denoted by the square 1 . For the 5-2-2 pattern you can see four crossing points, for the 3-2-4 pattern -- eight crossing points, for the 3-2-2 pattern -- 12 crossing points, for the 3-3-3 pattern -- self-crossing points are absent. In the last case we can say that path was repeated in four points (squares with numbers #1, #4, #7, #10). When forming a triplet it is necessary to consider at least three  cases: a) all numbers are equal to one another , b) all numbers are  unique , and c) a triplet has the two repeating elements, for example, . In the last case, there are three possible relations between and should be emphasized: a) , b) , and c) . We have found the seven different cases but can defined f() for five cases only: As one can see a resulted graph is connected, its vertices can have odd or even degrees (2, 3 and 4). On the figure the vertecies with degree equals to 2 is colored by red, the vertecies with degree equals to 3 is colored by green and  the degree of blue vertecies is 4 . Based on examples above we can suggest that the number of self-crossing point k equals to the number of vertices that have degree more than 2 . My Attempt based on Complex Analisys. Follow to the Chris Culter answer , we work in the complex plane. Let . Then the first part of closed-path motion (red lines on the figures above) is: and . We return to the start point #1 after steps.  We write equations for second, third and fourth segments by analogy: The full path equation is Then Substitute the expression (2) in the formula (1) and collect similar terms: Let us take the fist pair of relations (A) , (B) and the 3-2-4 pattern ( , , ). Using the Wolfram alpha the equation was solved. We found one real root and four complex solutions. Refs Marc Lackenby (2016) Elementary Knot Theory","a, b, c \in \mathbb{N} a b c (a, b, c) a = b = c a \neq b \neq c a = b a b a = 2\cdot b a> 2\cdot b a < 2\cdot b f(a,b,c)=
\left\{
\begin{array}{ll}
0, & a=b=c, \\
8, & a<b<c \quad \text{and} \quad c < a+b, \\
?, & a<b<c \quad \text{and} \quad c=a+b, \\
?, & a<b<c \quad \text{and} \quad a+b<c, \\
12, & a=b<c \quad \text{and} \quad c<a+a, \\
5, & a=b<c \quad \text{and} \quad c=a+a, \\
4, & a=b<c \quad \text{and} \quad a+a<c.
\end{array}
\right. z=e^{i\alpha} az^0+bz+cz^2 \tag{A} \alpha=-\frac{\pi}{2} N=4 az^3+bz^4+cz^5, \tag{B} az^6+bz^7+cz^8, \tag{C} az^{9}+bz^{10}+cz^{11}. \tag{D} (az^0+bz+cz^2)+(az^3+bz^4+cz^5)+
(az^6+bz^7+cz^8)+ (az^{9}+bz^{10}+cz^{11})= 0. \tag{1} z=e^{i\alpha}=cos(\alpha)+i \cdot sin(\alpha)=cos(-\frac{\pi}{2})+i \cdot sin(-\frac{\pi}{2}). \tag{2} a - i \cdot b - c + i \cdot a + b - i \cdot c -
a + i \cdot b + c - i \cdot a - b + i \cdot c = 0. az^0+bz+cz^2 az^3+bz^4+cz^5 a=3 b=2 c=4 3+2 \cdot z+4\cdot z^2=3 \cdot z^3+2 \cdot z^4+4 \cdot z^5 z=1","['complex-analysis', 'geometry', 'puzzle', 'knot-theory', 'planar-graphs']"
87,Compute $\int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx$ using Gamma function,Compute  using Gamma function,\int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx,"I want to compute $$\int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx$$ using Gamma function. I know that by change of variable, $y=\sqrt{x}$ , one gets $$\int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx=2\int_{0}^{+\infty}\sin y^2\ dy=\frac{\sqrt{2\pi}}{2}$$ by Fresnel's integral. I try it by considering this: $$\int_{0}^{+\infty}x^{-\frac{1}{2}}e^{ix}\ dx$$ It converges for both real and imaginary part using Dirichlet test, and $0$ is not a problem here. Let the square root take the pricipal branch where $\sqrt{1}=1$ . Let $y=-ix$ , then $$\int_{0}^{+\infty}x^{-\frac{1}{2}}e^{ix}\ dx=\sqrt{i}\int_{0}^{-i\infty}y^{-\frac{1}{2}}e^{-y}\ dy=(\frac{\sqrt{2}}{2} + \frac{\sqrt{2}}{2}i)\Gamma(\frac{1}{2})=\frac{\sqrt{2\pi}}{2} + \frac{\sqrt{2\pi}}{2}i$$ And it coincides with the final answer! My problem is, suppose $L$ is a ray starting from $0$ and has an angle $\phi$ with the $x$ -axis, and let $\phi\in(0,2\pi)$ . I want to argue that (maybe it is incorrect though) $$\Gamma(z)=\int_{L}t^{z-1}e^{-t}\ dt$$ I firstly know that it converges when $\Re(z)>0$ . Choose a contour like sector, let $L_1=\{z=x+iy:y=0, r<x<R\}$ , $L_2=\{z=Re^{i\theta}:0<\theta<\phi\}$ , $L_3=\{z=xe^{i\phi}:r<x<R\}$ and $L_4=\{z=re^{i\theta}:0<\theta<\phi\}$ , where $r<R$ and the contour is counterclockwise. By Cauchy theorem we have the contour integral should be $0$ . Easy to see that (let $z=x+iy$ ) $$ \lim_{r\to0+, R\to+\infty}\int_{L_1}t^{z-1}e^{-t}\ dt=\Gamma(z) $$ $$ |\int_{L_4}t^{z-1}e^{-t}\ dt|=|\int_{\phi}^{0} e^{-re^{i\theta}} (re^{i\theta})^{z-1}ire^{i\theta}\ d\theta| \leq \int_{0}^{\phi} e^{-r\cos\theta} |r^{z}e^{i\theta(z-1)}|\ d\theta=\int_{0}^{\phi} e^{-r\cos\theta} r^{x}e^{-\theta y}\ d\theta \to 0, r \to 0+ $$ But when considering $L_2$ : $$ |\int_{L_2}t^{z-1}e^{-t}\ dt|\leq\int_{0}^{\phi} e^{-R\cos\theta} R^{x}e^{-\theta y}\ d\theta $$ when for example, $\frac{3\pi}{2}>\phi>\frac{\pi}{2}$ , we have $\cos\theta<0$ and I failed to prove the above integral goes to zero when $R\to +\infty$ . Is my usage of Gamma function to compute the original integral a coincidence to get the correct result, or there is a way to prove my argument? Thank you so much!","I want to compute using Gamma function. I know that by change of variable, , one gets by Fresnel's integral. I try it by considering this: It converges for both real and imaginary part using Dirichlet test, and is not a problem here. Let the square root take the pricipal branch where . Let , then And it coincides with the final answer! My problem is, suppose is a ray starting from and has an angle with the -axis, and let . I want to argue that (maybe it is incorrect though) I firstly know that it converges when . Choose a contour like sector, let , , and , where and the contour is counterclockwise. By Cauchy theorem we have the contour integral should be . Easy to see that (let ) But when considering : when for example, , we have and I failed to prove the above integral goes to zero when . Is my usage of Gamma function to compute the original integral a coincidence to get the correct result, or there is a way to prove my argument? Thank you so much!","\int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx y=\sqrt{x} \int_{0}^{+\infty} \frac{\sin x}{\sqrt{x}}\ dx=2\int_{0}^{+\infty}\sin y^2\ dy=\frac{\sqrt{2\pi}}{2} \int_{0}^{+\infty}x^{-\frac{1}{2}}e^{ix}\ dx 0 \sqrt{1}=1 y=-ix \int_{0}^{+\infty}x^{-\frac{1}{2}}e^{ix}\ dx=\sqrt{i}\int_{0}^{-i\infty}y^{-\frac{1}{2}}e^{-y}\ dy=(\frac{\sqrt{2}}{2} + \frac{\sqrt{2}}{2}i)\Gamma(\frac{1}{2})=\frac{\sqrt{2\pi}}{2} + \frac{\sqrt{2\pi}}{2}i L 0 \phi x \phi\in(0,2\pi) \Gamma(z)=\int_{L}t^{z-1}e^{-t}\ dt \Re(z)>0 L_1=\{z=x+iy:y=0, r<x<R\} L_2=\{z=Re^{i\theta}:0<\theta<\phi\} L_3=\{z=xe^{i\phi}:r<x<R\} L_4=\{z=re^{i\theta}:0<\theta<\phi\} r<R 0 z=x+iy 
\lim_{r\to0+, R\to+\infty}\int_{L_1}t^{z-1}e^{-t}\ dt=\Gamma(z)
 
|\int_{L_4}t^{z-1}e^{-t}\ dt|=|\int_{\phi}^{0} e^{-re^{i\theta}} (re^{i\theta})^{z-1}ire^{i\theta}\ d\theta| \leq \int_{0}^{\phi} e^{-r\cos\theta} |r^{z}e^{i\theta(z-1)}|\ d\theta=\int_{0}^{\phi} e^{-r\cos\theta} r^{x}e^{-\theta y}\ d\theta \to 0, r \to 0+
 L_2 
|\int_{L_2}t^{z-1}e^{-t}\ dt|\leq\int_{0}^{\phi} e^{-R\cos\theta} R^{x}e^{-\theta y}\ d\theta
 \frac{3\pi}{2}>\phi>\frac{\pi}{2} \cos\theta<0 R\to +\infty","['complex-analysis', 'gamma-function']"
88,"Finding the harmonic conjugate of $T(x,y)= e^{-y} \sin x$?",Finding the harmonic conjugate of ?,"T(x,y)= e^{-y} \sin x","I try the following two method on finding the harmonic conjugate of $T(x,y)= e^{-y} \sin x$ : Method 1 : by a method of the book Complex Variables and Applications by Brown and Churchill, Chapter 9 Section 104, $$v(x,y) = \int_{(0,0)}^{(x,y)} -u_t(s, t)\ ds + u_s(s, t)\ dt = \int_{(0,0)}^{(x,y)} e^{-y} \sin x \ dx + e^{-y} \cos x \ dy = -e^{-y} \cos x - e^{-y} \cos x +C = -2e^{-y} \cos x +C$$ which $C$ can be chosen to be zero. Method 1 : Since $T(x,y)= e^{-y} \sin x$ and its harmonic conjugate must be the real and imaginary parts of an analytic function, respectively, so it can be $f(z)=-ie^{iz} =e^{-y} \sin x - ie^{-y} \cos x.$ So why there is an extra $2$ coefficient in Method 1? Where did I do wrong? Added. Probably the Method 1 is wrong. It comes from if $F_x(x,y) = P(x,y), \  F_y(x,y) = Q(x,y)$ holds in here . For example the method gives the correct answer for $u=xy$ but again it give a wrong answer for $u=x^3-3xy^2:$ that is $v=6x^2y-y^3$ ; but the correct one is $v=3x^2y-y^3$ . But it looks impossible such a famous book to make a mistake. (???)","I try the following two method on finding the harmonic conjugate of : Method 1 : by a method of the book Complex Variables and Applications by Brown and Churchill, Chapter 9 Section 104, which can be chosen to be zero. Method 1 : Since and its harmonic conjugate must be the real and imaginary parts of an analytic function, respectively, so it can be So why there is an extra coefficient in Method 1? Where did I do wrong? Added. Probably the Method 1 is wrong. It comes from if holds in here . For example the method gives the correct answer for but again it give a wrong answer for that is ; but the correct one is . But it looks impossible such a famous book to make a mistake. (???)","T(x,y)= e^{-y} \sin x v(x,y) = \int_{(0,0)}^{(x,y)} -u_t(s, t)\ ds + u_s(s, t)\ dt = \int_{(0,0)}^{(x,y)} e^{-y} \sin x \ dx + e^{-y} \cos x \ dy = -e^{-y} \cos x - e^{-y} \cos x +C = -2e^{-y} \cos x +C C T(x,y)= e^{-y} \sin x f(z)=-ie^{iz} =e^{-y} \sin x - ie^{-y} \cos x. 2 F_x(x,y) = P(x,y), \  F_y(x,y) = Q(x,y) u=xy u=x^3-3xy^2: v=6x^2y-y^3 v=3x^2y-y^3","['complex-analysis', 'solution-verification']"
89,"Can there be a non-isolated ""pole"" or ""removable singularity""?","Can there be a non-isolated ""pole"" or ""removable singularity""?",,A pole or removable or even essential singularity must be isolated a priori. But still we can try to talk about the limit of the function at the point even on a disk removing some (countable amount of) points. A well-know example of non-isolated singularity will be $z=0$ for $$f(z)=\frac{1}{\sin(\frac{1}{z})}.$$ But $\lim_{z\to0}f(z)$ does not exist. My question is can there be a function with non-isolated singularity at $0$ with $\lim_{z\to0}|f(z)|=\infty$ or even $\lim_{z\to0}f(z)=c$ ?,A pole or removable or even essential singularity must be isolated a priori. But still we can try to talk about the limit of the function at the point even on a disk removing some (countable amount of) points. A well-know example of non-isolated singularity will be for But does not exist. My question is can there be a function with non-isolated singularity at with or even ?,z=0 f(z)=\frac{1}{\sin(\frac{1}{z})}. \lim_{z\to0}f(z) 0 \lim_{z\to0}|f(z)|=\infty \lim_{z\to0}f(z)=c,['complex-analysis']
90,Complex integration lemma: shorter proof?,Complex integration lemma: shorter proof?,,"The black line is the branch cut. Lemma $$\lim_{\Delta\to0^+}\left(\int_{\gamma_1}+\int_{\gamma_2}\right)f(z)\ln(z-s)dz=-2\pi i\int_{pe^{i\theta}}^{qe^{i\theta}}f(t)dt$$ where $\arg(z-s)\in[\theta,\theta+2\pi)$ , $f$ being holomorphic on the path of integration. Many advanced users on this site use this lemma without stating, letting alone proving it. I wrote a proof here , but it is quite long. Is there a shorter proof of this lemma?","The black line is the branch cut. Lemma where , being holomorphic on the path of integration. Many advanced users on this site use this lemma without stating, letting alone proving it. I wrote a proof here , but it is quite long. Is there a shorter proof of this lemma?","\lim_{\Delta\to0^+}\left(\int_{\gamma_1}+\int_{\gamma_2}\right)f(z)\ln(z-s)dz=-2\pi i\int_{pe^{i\theta}}^{qe^{i\theta}}f(t)dt \arg(z-s)\in[\theta,\theta+2\pi) f","['complex-analysis', 'complex-integration']"
91,Zeros of partial sums of the exponential [duplicate],Zeros of partial sums of the exponential [duplicate],,"This question already has answers here : Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls (3 answers) How prove this $|z|>1$ with $1+z+\frac{z^2}{2!}+\cdots+\frac{z^n}{n!}=0$ (1 answer) Closed 5 years ago . I am trying to show that if $$f_n(z)=1+z+\frac{z^2}{2!}+...+\frac{z^n}{n!}$$ Then $f_n(z)$ don’t have zeros inside the unitary disk.  I have tryied to use Rouche’s theorem or use that in the limit the polinomial converges to the exponential, but i dont get hoy to do this.","This question already has answers here : Complex zeros of the polynomials $\sum_{k=0}^{n} z^k/k!$, inside balls (3 answers) How prove this $|z|>1$ with $1+z+\frac{z^2}{2!}+\cdots+\frac{z^n}{n!}=0$ (1 answer) Closed 5 years ago . I am trying to show that if Then don’t have zeros inside the unitary disk.  I have tryied to use Rouche’s theorem or use that in the limit the polinomial converges to the exponential, but i dont get hoy to do this.",f_n(z)=1+z+\frac{z^2}{2!}+...+\frac{z^n}{n!} f_n(z),"['complex-analysis', 'power-series', 'taylor-expansion']"
92,Uniform convergence of $\int_{a}^{\infty}{\frac{\sin x}{x^s}}dx$ for $\Re(s)>0$?,Uniform convergence of  for ?,\int_{a}^{\infty}{\frac{\sin x}{x^s}}dx \Re(s)>0,"For $a>0$ , does $$f_b(s)=\int_{a}^{b}{\frac{\sin x}{x^s}}dx$$ converge uniformly for all compact subsets of $\{s\in\mathbb C|\Re(s)>0\}$ when $b\to\infty$ ? For $\Re(s)>1$ the integral converges absolutely, and since $g_s(z)=\frac{\sin z}{z^s}$ is holomorphic on $\{x\in\mathbb R|x>0\}$ , by applying the Weierstrass theorem (which states that for a family of holomorphic functions converging uniformly, the family of derivative of those functions converges uniformly and equals the derivative of the converging function of the given family) it is not difficult to prove. However, for $0<\Re(s)\leq 1$ the integral only seems to converge conditionally at best, which makes it more difficult. Any good ways of proving convergence?(or maybe divergence?)","For , does converge uniformly for all compact subsets of when ? For the integral converges absolutely, and since is holomorphic on , by applying the Weierstrass theorem (which states that for a family of holomorphic functions converging uniformly, the family of derivative of those functions converges uniformly and equals the derivative of the converging function of the given family) it is not difficult to prove. However, for the integral only seems to converge conditionally at best, which makes it more difficult. Any good ways of proving convergence?(or maybe divergence?)",a>0 f_b(s)=\int_{a}^{b}{\frac{\sin x}{x^s}}dx \{s\in\mathbb C|\Re(s)>0\} b\to\infty \Re(s)>1 g_s(z)=\frac{\sin z}{z^s} \{x\in\mathbb R|x>0\} 0<\Re(s)\leq 1,"['complex-analysis', 'uniform-convergence']"
93,How to solve a specific complex integral: $\int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz$,How to solve a specific complex integral:,\int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz,"In a test today, we were given a specific integral to solve: for a curve $M$ oriented clockwise, being a rectangle with vertices $(1,2), (-1,2), (-1,-1), (1,-1)$ , $$\int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz$$ We were not actually taught how to solve integrals of this form at this point - which was a bit eyebrow-raising for a bunch of us. The professor said it was more of a test of confidence ... or something. Either way a little weird to put on a test, but okay. So, my question is, how would one solve it? My Attempt: Post-script from a month after I posted this: this approach did touch on the correction but was wrong. The substitution was a big reason why. Very recently, we discussed expressing complex functions as a power series. If we try to express the function $f(z)$ as a power series about $z = 0$ , $$f(z) = \sum_{n=0}^{\infty} a_n z^n$$ then each coefficient $a_n$ is given by either of the below, $$a_n = \frac{-1}{2\pi i} \int_{M} \frac{f(\zeta)}{\zeta^{n+1}} d \zeta = \frac{1}{n!} f^{n}(0)$$ (The negative comes from $M$ being oriented clockwise.) Well, if we make the substitution $\zeta = 3z+1$ in our original integral (yielding $d\zeta = 3dz$ ), we have $$\int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz= \frac{1}{3} \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta$$ If we let $f(\zeta)$ be given by $f(\zeta) = (2\zeta-1)^5 \cos(\zeta)$ , we then essentially match the form of the integral in the definition of the coefficients above if $n=1$ , i.e. $$a_2 = \frac{-1}{2\pi i}  \int_M \frac{f(\zeta)}{\zeta^2}d\zeta = \frac{-1}{2\pi i} \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta  = \frac{1}{1!} f^{1}(0)$$ or, essentially, $$\int_M \frac{f(\zeta)}{\zeta^2}d\zeta = \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta  = -2 \pi i \cdot f'(0)$$ Would this be right so far? From here, it's basically arithmetic: find the derivative of $f(\zeta)$ , evaluate it for $\zeta = 0$ , and multiply by $\frac{1}{3}$ to return to the integral we got by the substitution $\zeta = 3z+1$ = and $d\zeta = 3dz$ . I'm not going to bore you with that arithmetic, I'm more concerned with just the overarching idea of how to solve this integral, as opposed to the actual answer, since I'm not sure if I have the right idea. Actually I feel pretty sure I don't, but I couldn't think of anything else that would apply.","In a test today, we were given a specific integral to solve: for a curve oriented clockwise, being a rectangle with vertices , We were not actually taught how to solve integrals of this form at this point - which was a bit eyebrow-raising for a bunch of us. The professor said it was more of a test of confidence ... or something. Either way a little weird to put on a test, but okay. So, my question is, how would one solve it? My Attempt: Post-script from a month after I posted this: this approach did touch on the correction but was wrong. The substitution was a big reason why. Very recently, we discussed expressing complex functions as a power series. If we try to express the function as a power series about , then each coefficient is given by either of the below, (The negative comes from being oriented clockwise.) Well, if we make the substitution in our original integral (yielding ), we have If we let be given by , we then essentially match the form of the integral in the definition of the coefficients above if , i.e. or, essentially, Would this be right so far? From here, it's basically arithmetic: find the derivative of , evaluate it for , and multiply by to return to the integral we got by the substitution = and . I'm not going to bore you with that arithmetic, I'm more concerned with just the overarching idea of how to solve this integral, as opposed to the actual answer, since I'm not sure if I have the right idea. Actually I feel pretty sure I don't, but I couldn't think of anything else that would apply.","M (1,2), (-1,2), (-1,-1), (1,-1) \int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz f(z) z = 0 f(z) = \sum_{n=0}^{\infty} a_n z^n a_n a_n = \frac{-1}{2\pi i} \int_{M} \frac{f(\zeta)}{\zeta^{n+1}} d \zeta = \frac{1}{n!} f^{n}(0) M \zeta = 3z+1 d\zeta = 3dz \int_M \frac{(6z+1)^5 \cos(3z+1)}{(3z+1)^2}dz= \frac{1}{3} \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta f(\zeta) f(\zeta) = (2\zeta-1)^5 \cos(\zeta) n=1 a_2 = \frac{-1}{2\pi i}  \int_M \frac{f(\zeta)}{\zeta^2}d\zeta = \frac{-1}{2\pi i} \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta  = \frac{1}{1!} f^{1}(0) \int_M \frac{f(\zeta)}{\zeta^2}d\zeta = \int_M \frac{(2\zeta-1)^5 \cos(\zeta)}{\zeta^2}d\zeta  = -2 \pi i \cdot f'(0) f(\zeta) \zeta = 0 \frac{1}{3} \zeta = 3z+1 d\zeta = 3dz","['complex-analysis', 'complex-integration']"
94,What are the transformations that preserve cross ratios on a sphere in higher dimensions?,What are the transformations that preserve cross ratios on a sphere in higher dimensions?,,"If we have four points $x,y,z,w$ on a sphere, then the cross ratio is $\frac{|x-z|}{|x-w|}\frac{|y-w|}{|y-z|}$ . If we consider $S^1 \subseteq \mathbb{C}$ , then the transformations of $\mathbb{C}$ which preserve the cross ratios on the circle are precisely the Mobius transformations which map the open unit disc to itself. Is there a nice classification for transformations preserving cross ratios of spheres in higher dimensions?","If we have four points on a sphere, then the cross ratio is . If we consider , then the transformations of which preserve the cross ratios on the circle are precisely the Mobius transformations which map the open unit disc to itself. Is there a nice classification for transformations preserving cross ratios of spheres in higher dimensions?","x,y,z,w \frac{|x-z|}{|x-w|}\frac{|y-w|}{|y-z|} S^1 \subseteq \mathbb{C} \mathbb{C}","['complex-analysis', 'projective-geometry', 'hyperbolic-geometry', 'mobius-transformation']"
95,Difference real and complex fourier series,Difference real and complex fourier series,,"I'm working on fourier series and I'm trying to compute the fourier transformation for the $2\pi$ -periodic function of $f(x)=x^2$ with $x \in [-\pi,\pi]$ . Now with the real way, that is $$f(x) \sim \frac{a_{0}}{2}+\sum\limits_{n=1}^{\infty}a_{n}\cos(nx)+b_{n}\sin(nx)$$ and I found $$f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{4}{n^2}(-1)^{n}\cos(nx).$$ Now I also tried to compute with the imaginary way, that is with $$f(x) \sim c_{0}+\sum\limits_{n=-\infty}^{\infty}c_{n} e^{inx},$$ with $$c_{n}=\frac{1}{2\pi} \int\limits_{-\pi}^{\pi}f(x)e^{-inx},$$ and I found $$f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{2}{n^2} (-1)^{n} e^{inx},$$ which doesn't seem to be the same. I'm sure about the real computation, any suggestions where I go wrong with the imaginary part?","I'm working on fourier series and I'm trying to compute the fourier transformation for the -periodic function of with . Now with the real way, that is and I found Now I also tried to compute with the imaginary way, that is with with and I found which doesn't seem to be the same. I'm sure about the real computation, any suggestions where I go wrong with the imaginary part?","2\pi f(x)=x^2 x \in [-\pi,\pi] f(x) \sim \frac{a_{0}}{2}+\sum\limits_{n=1}^{\infty}a_{n}\cos(nx)+b_{n}\sin(nx) f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{4}{n^2}(-1)^{n}\cos(nx). f(x) \sim c_{0}+\sum\limits_{n=-\infty}^{\infty}c_{n} e^{inx}, c_{n}=\frac{1}{2\pi} \int\limits_{-\pi}^{\pi}f(x)e^{-inx}, f(x) \sim \frac{\pi^2}{3}+\sum\limits_{n=1}^{\infty} \frac{2}{n^2} (-1)^{n} e^{inx},","['complex-analysis', 'fourier-analysis', 'fourier-series', 'fourier-transform', 'complex-integration']"
96,A quick question about complex integrals and Cauchy's integral formula,A quick question about complex integrals and Cauchy's integral formula,,"I'll spare the specifics for brevity's sake, but in essence the problem I'm posed is finding $$\int_C \frac{(z-1)^3 \cdot e^z \cdot cos(z)}{z}dz$$ along two different closed loops $C$ . Each is a rectangle, oriented clockwise. One of them encloses the discontinuity of this function (i.e. $z=0$ ) and another doesn't. Post-Script (December 2018): I recognize that the ""discontinuity"" mentioned is in reality a singularity. I'm mostly just wanting to double-check my approach to this since it's explicitly specified that ""this shouldn't take much computation,"" and want to double-check I'm on the right path. My thoughts on the matter: For $C$ being the closed loop not enclosing the discontinuity, the integral would be $0$ per Cauchy's integral theorem. For $C$ being the closed loop that encloses the discontinuity, the integral would be $2\pi i f(0)$ , from Cauchy's integral formula, where $f(z)$ is the numerator of the integrand (sans the $dz$ of course), and "" $0$ "" coming from being the point of discontinuity. I have a rough intuition for why this might be - it's fairly heuristic and informal though - so I just wanted to make sure I was on the right track. Thanks in advance.","I'll spare the specifics for brevity's sake, but in essence the problem I'm posed is finding along two different closed loops . Each is a rectangle, oriented clockwise. One of them encloses the discontinuity of this function (i.e. ) and another doesn't. Post-Script (December 2018): I recognize that the ""discontinuity"" mentioned is in reality a singularity. I'm mostly just wanting to double-check my approach to this since it's explicitly specified that ""this shouldn't take much computation,"" and want to double-check I'm on the right path. My thoughts on the matter: For being the closed loop not enclosing the discontinuity, the integral would be per Cauchy's integral theorem. For being the closed loop that encloses the discontinuity, the integral would be , from Cauchy's integral formula, where is the numerator of the integrand (sans the of course), and "" "" coming from being the point of discontinuity. I have a rough intuition for why this might be - it's fairly heuristic and informal though - so I just wanted to make sure I was on the right track. Thanks in advance.",\int_C \frac{(z-1)^3 \cdot e^z \cdot cos(z)}{z}dz C z=0 C 0 C 2\pi i f(0) f(z) dz 0,"['complex-analysis', 'complex-integration']"
97,Evaluating $\sum_{n=1}^{\infty} \frac{sin(nz)}{2^n}$ [closed],Evaluating  [closed],\sum_{n=1}^{\infty} \frac{sin(nz)}{2^n},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I managed to show that: $$\sum_{z=1}^{\infty} \frac{sin(nz)}{2^n}$$ is analytic on $\{z \in \mathbb{C}| -log(2)<Im(Z)<log(2)\}$ I am assuming that is correct? I am now stuck as to how to evaluate the series on that set. How would I get started? I have so far re-written: $$\sum_{z=1}^{\infty} \frac{sin(nz)}{2^n} = \frac{1}{2i}[\sum \frac{e^{inz}}{2^n} - \sum \frac{e^{-inz}}{2^n}]$$ Thank you!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I managed to show that: is analytic on I am assuming that is correct? I am now stuck as to how to evaluate the series on that set. How would I get started? I have so far re-written: Thank you!",\sum_{z=1}^{\infty} \frac{sin(nz)}{2^n} \{z \in \mathbb{C}| -log(2)<Im(Z)<log(2)\} \sum_{z=1}^{\infty} \frac{sin(nz)}{2^n} = \frac{1}{2i}[\sum \frac{e^{inz}}{2^n} - \sum \frac{e^{-inz}}{2^n}],"['complex-analysis', 'power-series']"
98,How can I evaluate this complex integral $\int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz$?,How can I evaluate this complex integral ?,\int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz,"I'm trying to evaluate the following complex integral using the residue method. $$\int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz$$ The point $z_0=0$ seems to be a singularity. I'm not sure but I think it's also a non-removable one. I tried using the Taylor expansion of $e^x$ and $\cos{x}$ as that usually helps. $$e^{\frac{1}{z}}\cos{\frac{1}{z}}=(1+\frac{1}z+\frac{1}{2!z^2}+\frac{1}{3!z^3}+...)(1-\frac{1}{2!z^2}+\frac{1}{4!z^4}-...)\\=>e^{\frac{1}{z}}\cos{\frac{1}{z}}=(1+\frac{1}{z}+...)$$ It seems like the negative power terms are infinite showing that $z_0=0$ is no pole. If I'm correct, the coefficient of $1/z$, which is $1$, is the residue of the singularity and this leads to the result:$$ \int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz=2\pi i$$ I don't think I've evaluated other integrals with non-removable singularities and I'm not sure about the whole process..","I'm trying to evaluate the following complex integral using the residue method. $$\int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz$$ The point $z_0=0$ seems to be a singularity. I'm not sure but I think it's also a non-removable one. I tried using the Taylor expansion of $e^x$ and $\cos{x}$ as that usually helps. $$e^{\frac{1}{z}}\cos{\frac{1}{z}}=(1+\frac{1}z+\frac{1}{2!z^2}+\frac{1}{3!z^3}+...)(1-\frac{1}{2!z^2}+\frac{1}{4!z^4}-...)\\=>e^{\frac{1}{z}}\cos{\frac{1}{z}}=(1+\frac{1}{z}+...)$$ It seems like the negative power terms are infinite showing that $z_0=0$ is no pole. If I'm correct, the coefficient of $1/z$, which is $1$, is the residue of the singularity and this leads to the result:$$ \int_{|z|=1}e^{\frac{1}{z}}\cos{\frac{1}{z}}dz=2\pi i$$ I don't think I've evaluated other integrals with non-removable singularities and I'm not sure about the whole process..",,"['complex-analysis', 'contour-integration', 'laurent-series']"
99,Bubbling off in Deligne-Mumford compactification,Bubbling off in Deligne-Mumford compactification,,"I'm trying to understand an argument in Casim Abbas' 'An Introduction to Compactness Results in Symplectic Field Theory'. Here's where I stuck: (It's on chapter 3.3.2, Adding additional marked points in the book, if you have access to it.) Suppose we have a marked Riemann surface $(S,M)$ , where $S$ is the Riemann surface and $M\subseteq S$ is the set of marked points. (For simplicity, assume there is no nodal points on $S$ .) Then we are interested in the limit of the sequence of marked Riemann surface $S_n:=(S,M\cup \left\{y_n^{(0)},y_n^{(1)}\right\})$ where the added marked points are given so that $d_n(y^{(0)}_n,y^{(1)}_n)$ goes to zero as $n$ goes to infinity. We further assume that there exists a holomorphic chart of radius $R_n\to \infty$ which maps $y_n^{(i)}$ to $i\in \mathbb{C}$ . Now the argument in the book goes like this: First, as the two marked points gets closer and closer, we may restrict ourselves on a pair of pants. (In fact, we need to consider the case that the two marked points are lying on two adjacent pairs of pants, but let's just concentrate on this for simplicity.) So the pair of pants of our concern looks like the first pants as above. Now he considers a pair of pants decomposition of $S_n$ . The following is what was stated: Because the surface $S_n$ contain annuli $\cong D_{R_n}\setminus D_2$ with larger and larger moduli and $y_n^{(i)}\in D_2$ , the last pair of pants decomposition cannot occur by Bers' theorem. In fact, we must have the length of $\gamma_n$ to go to $0$ . Q. Could you explain this part a bit more? I guess that, as we need to use Bers' theorem, it is implicit that the lower dotted circle in the last pair of pants in the figure gets longer and longer as $n$ goes to infinity. Is it correct? Why? And I have no idea how to see the next claim that the length of $\gamma_n$ goes to 0. Edit: The following is Bers' theorem, paraphrased in my own preference. (Bers' theorem) Given a topological type (ie, the genus, number of marked points, and the number of boundary components) of a Riemann surface $S$ fixed, there is a uniform bound on the length of each boundary component of each pair of pants in a pair of pants decomposition of $S$ which only depends on the topological type of $S$ and the length of the boundary components of $S$ .","I'm trying to understand an argument in Casim Abbas' 'An Introduction to Compactness Results in Symplectic Field Theory'. Here's where I stuck: (It's on chapter 3.3.2, Adding additional marked points in the book, if you have access to it.) Suppose we have a marked Riemann surface , where is the Riemann surface and is the set of marked points. (For simplicity, assume there is no nodal points on .) Then we are interested in the limit of the sequence of marked Riemann surface where the added marked points are given so that goes to zero as goes to infinity. We further assume that there exists a holomorphic chart of radius which maps to . Now the argument in the book goes like this: First, as the two marked points gets closer and closer, we may restrict ourselves on a pair of pants. (In fact, we need to consider the case that the two marked points are lying on two adjacent pairs of pants, but let's just concentrate on this for simplicity.) So the pair of pants of our concern looks like the first pants as above. Now he considers a pair of pants decomposition of . The following is what was stated: Because the surface contain annuli with larger and larger moduli and , the last pair of pants decomposition cannot occur by Bers' theorem. In fact, we must have the length of to go to . Q. Could you explain this part a bit more? I guess that, as we need to use Bers' theorem, it is implicit that the lower dotted circle in the last pair of pants in the figure gets longer and longer as goes to infinity. Is it correct? Why? And I have no idea how to see the next claim that the length of goes to 0. Edit: The following is Bers' theorem, paraphrased in my own preference. (Bers' theorem) Given a topological type (ie, the genus, number of marked points, and the number of boundary components) of a Riemann surface fixed, there is a uniform bound on the length of each boundary component of each pair of pants in a pair of pants decomposition of which only depends on the topological type of and the length of the boundary components of .","(S,M) S M\subseteq S S S_n:=(S,M\cup \left\{y_n^{(0)},y_n^{(1)}\right\}) d_n(y^{(0)}_n,y^{(1)}_n) n R_n\to \infty y_n^{(i)} i\in \mathbb{C} S_n S_n \cong D_{R_n}\setminus D_2 y_n^{(i)}\in D_2 \gamma_n 0 n \gamma_n S S S S","['complex-analysis', 'symplectic-geometry', 'teichmueller-theory']"
