,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Problem related to the dimension of the vector space,Problem related to the dimension of the vector space,,"I came across the following problem that says: Let $C$ be a $n\times n$ real matrix. Let $W$ be the vector space   spanned by $\{1,C,C^2,\ldots,C^{2n}\}$. Then, which of the following about the dimension of the vector space W is/are correct? The dimension of $W$ is: $2n$ at most $n$ $n^2$ at most $2n$. Please help.Thanks in advance for your time.","I came across the following problem that says: Let $C$ be a $n\times n$ real matrix. Let $W$ be the vector space   spanned by $\{1,C,C^2,\ldots,C^{2n}\}$. Then, which of the following about the dimension of the vector space W is/are correct? The dimension of $W$ is: $2n$ at most $n$ $n^2$ at most $2n$. Please help.Thanks in advance for your time.",,"['linear-algebra', 'real-analysis', 'analysis']"
1,Estimation of $\int \limits_{0}^{\infty}e^{-ax}\frac{1}{x}dx.$,Estimation of,\int \limits_{0}^{\infty}e^{-ax}\frac{1}{x}dx.,"I want to find an estimation for the integral $$\int \limits_{0}^{\infty}e^{-ax}\frac{1}{x}dx.$$ So, I change the variable $u=e^{-ax}$ and I get $$\int \limits_{0}^{e^{-a}}-\frac{1}{\ln u}du.$$ Since $u\in (0,e^{-ax})$ , then $0<\ln(u+1)<\ln(e^{-ax}+1)$. Thus, $$\int \limits_{0}^{e^{-a}}-\frac{1}{\ln (u+1)}du<\int \limits_{0}^{e^{-a}}-\frac{1}{\ln u}du.$$ And it does not make sense to achieve any estimation.  Could you please give me a clue? How to find an upper bound for this case?","I want to find an estimation for the integral $$\int \limits_{0}^{\infty}e^{-ax}\frac{1}{x}dx.$$ So, I change the variable $u=e^{-ax}$ and I get $$\int \limits_{0}^{e^{-a}}-\frac{1}{\ln u}du.$$ Since $u\in (0,e^{-ax})$ , then $0<\ln(u+1)<\ln(e^{-ax}+1)$. Thus, $$\int \limits_{0}^{e^{-a}}-\frac{1}{\ln (u+1)}du<\int \limits_{0}^{e^{-a}}-\frac{1}{\ln u}du.$$ And it does not make sense to achieve any estimation.  Could you please give me a clue? How to find an upper bound for this case?",,"['real-analysis', 'analysis']"
2,Second derivative of convex function,Second derivative of convex function,,"Let $f(x)$, $x>0$ be a convex function. Then it's distributional second derivative is defined by the rule $$    \langle f''(x),\varphi(x)\rangle = \langle f(x), \varphi''(x)\rangle $$ for any $\varphi \in \mathcal{D}(0,\infty)$. If function $f(x) \in C^2(0,\infty)$ then  $$    \langle f''(x),\varphi(x)\rangle \geqslant 0  $$ for any $\varphi \in \mathcal{D}(0,\infty)$, $\varphi \geqslant 0$. It is true for general convex function $f(x)$? There is a theorem that states that any distribution may be approximated by sequence of smooth functions. I tried to use it, but without success.","Let $f(x)$, $x>0$ be a convex function. Then it's distributional second derivative is defined by the rule $$    \langle f''(x),\varphi(x)\rangle = \langle f(x), \varphi''(x)\rangle $$ for any $\varphi \in \mathcal{D}(0,\infty)$. If function $f(x) \in C^2(0,\infty)$ then  $$    \langle f''(x),\varphi(x)\rangle \geqslant 0  $$ for any $\varphi \in \mathcal{D}(0,\infty)$, $\varphi \geqslant 0$. It is true for general convex function $f(x)$? There is a theorem that states that any distribution may be approximated by sequence of smooth functions. I tried to use it, but without success.",,"['analysis', 'distribution-theory']"
3,"Prove that there exist one subsequence s.t. $\forall \;0\le a<b\le 1$, $\lim_{ k\to \infty} \int_ a^b f_{n_k }(t)dt$ exists.","Prove that there exist one subsequence s.t. ,  exists.",\forall \;0\le a<b\le 1 \lim_{ k\to \infty} \int_ a^b f_{n_k }(t)dt,"Problem: Let $(f_n)$ be a uniformly bounded sequence of real valued continuous functions on $[0,1]$. Prove that there is ONE subsequence $(f_{n_k})$ such that for every  $0\le a < b \le 1$, we have $$\lim_{k\to\infty} \int_a^b \! f_{n_k}(t) dt $$ exists. Context: Advanced Undergraduate Analysis. Familiar with Real Analysis by Carothers and Principles of Analysis by Rudin I think it would be obvious to show this for all rationals inbetween a and b but I do not know how to start showing that there is a single subsequence. Any help would be appreciated, Thank you.","Problem: Let $(f_n)$ be a uniformly bounded sequence of real valued continuous functions on $[0,1]$. Prove that there is ONE subsequence $(f_{n_k})$ such that for every  $0\le a < b \le 1$, we have $$\lim_{k\to\infty} \int_a^b \! f_{n_k}(t) dt $$ exists. Context: Advanced Undergraduate Analysis. Familiar with Real Analysis by Carothers and Principles of Analysis by Rudin I think it would be obvious to show this for all rationals inbetween a and b but I do not know how to start showing that there is a single subsequence. Any help would be appreciated, Thank you.",,"['real-analysis', 'sequences-and-series', 'analysis', 'integration']"
4,Very basic question about the definition of continuous of a functions,Very basic question about the definition of continuous of a functions,,"Suppose say $f:\{0,1\}\to \{1,2\}$ is $f$ continuous? Say $f(0)=2,f(1)=1$ I know the definition of continuous function. In my point of view, i think it is continuous as we can simply take $\epsilon=\delta$, so for any $x\in \{0,1\},|x-1|<\delta \implies |f(x)-f(1)|<\epsilon$. Is it correct?","Suppose say $f:\{0,1\}\to \{1,2\}$ is $f$ continuous? Say $f(0)=2,f(1)=1$ I know the definition of continuous function. In my point of view, i think it is continuous as we can simply take $\epsilon=\delta$, so for any $x\in \{0,1\},|x-1|<\delta \implies |f(x)-f(1)|<\epsilon$. Is it correct?",,"['real-analysis', 'analysis', 'functions', 'continuity']"
5,Compactness of unit ball in weak-operator topology.,Compactness of unit ball in weak-operator topology.,,"I'm reading Richard Kadison's book about operator algebras, and in the demonstration that the unit ball is compact in weak-operator topology, the author defines a function from the set of bounded operators on a Hilbert space $H$, to a product of disks: \begin{align*}F:\mathcal{B}(H)\rightarrow&\prod_{x,y\in H}\mathbb{D}_{x,y}\\  T\rightarrow & \{\langle Tx,y\rangle\ :\ x,y\in H\} \end{align*} If we set the product topology on $\prod_{x,y\in H}\mathbb{D}_{x,y}$, the function above is continuous because of the topology of $\mathcal{B}(H)$ is induced by functions of that type, but I can't see why this function is a homeomorphism. Why the inverse is also continuous?","I'm reading Richard Kadison's book about operator algebras, and in the demonstration that the unit ball is compact in weak-operator topology, the author defines a function from the set of bounded operators on a Hilbert space $H$, to a product of disks: \begin{align*}F:\mathcal{B}(H)\rightarrow&\prod_{x,y\in H}\mathbb{D}_{x,y}\\  T\rightarrow & \{\langle Tx,y\rangle\ :\ x,y\in H\} \end{align*} If we set the product topology on $\prod_{x,y\in H}\mathbb{D}_{x,y}$, the function above is continuous because of the topology of $\mathcal{B}(H)$ is induced by functions of that type, but I can't see why this function is a homeomorphism. Why the inverse is also continuous?",,['analysis']
6,"Lipschitz maps on $[0,1]$",Lipschitz maps on,"[0,1]","Here is just a little curiosity. Assume the $f : [0,1] \to [0,1]$ is a Lipschitz function that maps 0 to 0 and 1 to 1. If we impose that the Lipschitz constant of $f$ is $\le 1$, can $f$ be anything other than the identity map?","Here is just a little curiosity. Assume the $f : [0,1] \to [0,1]$ is a Lipschitz function that maps 0 to 0 and 1 to 1. If we impose that the Lipschitz constant of $f$ is $\le 1$, can $f$ be anything other than the identity map?",,['analysis']
7,A question about The Implicit Function Theorem,A question about The Implicit Function Theorem,,"here is a problem im trying to solve for a few days, and Im not getting sucess. Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $f\in C^1$ and $F(x,y,z)=f(y/x,z/x)$. Consider a level surface $S$ defined by $F(x,y,z)=0$ and $(x_0,y_0,z_0)\in S$. What are the conditions, in a neighborhood of $(x_0,y_0,z_0)$, that we can write $S$ in the form $z=g(x,y)$ ? Also, show that at this conditions, its true that $$x\frac{\partial g}{\partial x}(x,y)+y\frac{\partial g}{\partial y}(x,y)=g(x,y)$$ Well, here is what I've done: To use the Implicit Function Theorem I choose $F(x_0,y_0,z_0)=0$ and I have to show that $\frac{\partial F}{\partial z}(x_0,y_0,z_0)\neq 0$.  $$\frac{\partial F}{\partial z}(x_0,y_0,z_0)=\textrm{lim}_{t\rightarrow 0}\frac{F(x_0,y_0,z_0+t)-F(x_0,y_0,z_0)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{F(x_0,y_0,z_0+t)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0+t)/x_0)}{t}\neq 0$$ Here I tried something that I dont know if is right, and even if is right, i couldnt conclude the right answer from this. $$\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0+t)/x_0)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0/x_0)+(t/x_0))}{t}=\frac{1}{x_0}\frac{\partial f}{\partial y}(y_0/x_0,z_0/x_0)$$ I did this variable change $u=t/x_0$ with $u\rightarrow 0$ and calculate the limit. But Im really not sure about it and I didnt found the last equality they are asking. Thank you everyone one more time.","here is a problem im trying to solve for a few days, and Im not getting sucess. Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $f\in C^1$ and $F(x,y,z)=f(y/x,z/x)$. Consider a level surface $S$ defined by $F(x,y,z)=0$ and $(x_0,y_0,z_0)\in S$. What are the conditions, in a neighborhood of $(x_0,y_0,z_0)$, that we can write $S$ in the form $z=g(x,y)$ ? Also, show that at this conditions, its true that $$x\frac{\partial g}{\partial x}(x,y)+y\frac{\partial g}{\partial y}(x,y)=g(x,y)$$ Well, here is what I've done: To use the Implicit Function Theorem I choose $F(x_0,y_0,z_0)=0$ and I have to show that $\frac{\partial F}{\partial z}(x_0,y_0,z_0)\neq 0$.  $$\frac{\partial F}{\partial z}(x_0,y_0,z_0)=\textrm{lim}_{t\rightarrow 0}\frac{F(x_0,y_0,z_0+t)-F(x_0,y_0,z_0)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{F(x_0,y_0,z_0+t)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0+t)/x_0)}{t}\neq 0$$ Here I tried something that I dont know if is right, and even if is right, i couldnt conclude the right answer from this. $$\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0+t)/x_0)}{t}=\textrm{lim}_{t\rightarrow 0}\frac{f(y_0/x_0,(z_0/x_0)+(t/x_0))}{t}=\frac{1}{x_0}\frac{\partial f}{\partial y}(y_0/x_0,z_0/x_0)$$ I did this variable change $u=t/x_0$ with $u\rightarrow 0$ and calculate the limit. But Im really not sure about it and I didnt found the last equality they are asking. Thank you everyone one more time.",,"['real-analysis', 'analysis']"
8,Study the convergence of the following series,Study the convergence of the following series,,"I have to study the convergence of the following series: $$\sum_{n\ge1} \frac{ n! } { p (p+1) \cdots (p + n - 1) }\text{ where }p > 0.$$ I tried d'Alembert criterion but $\lim_{n\to\infty} \frac {a_{n+1}} {a_n} = 1$ (where $a_n = \frac{ n! } { p  (p+1) \cdots (p + n - 1) }$). Because that limit is $1$ the nature of the series is inconclusive. Intuitively I can say that the series is convergent because when $p\in\{1,2,n\}$ the sum becomes: For $p = 1$, $\sum_{n\ge1} \frac{ n! } {1\cdot2\cdots(1+n-1)} = \sum_{n\ge1} \frac{n!}{n!} = n$ is convergent and For $p = 2$, $\sum_{n\ge1} \frac{ n! } {2\cdot3\cdots(2+n-1)} = \sum_{n\ge1} \frac{n!}{2\cdot3\cdots(n+1)} =  \sum_{n\ge1} \frac{n!}{(n+1)!} =\sum_{n\ge1} \frac{1}{n+1}$ is convergent and For $p = n$, $\sum_{n\ge1} \frac{ n! } {n(n+1)\cdots(2n-1)} = \sum_{n\ge1} \frac{(n-2)!}{(n+2)(n+3)\cdots(2n-1)}$ is convergent (d'Alembert) How do I proof my intuition is a rigorous mathematical way?","I have to study the convergence of the following series: $$\sum_{n\ge1} \frac{ n! } { p (p+1) \cdots (p + n - 1) }\text{ where }p > 0.$$ I tried d'Alembert criterion but $\lim_{n\to\infty} \frac {a_{n+1}} {a_n} = 1$ (where $a_n = \frac{ n! } { p  (p+1) \cdots (p + n - 1) }$). Because that limit is $1$ the nature of the series is inconclusive. Intuitively I can say that the series is convergent because when $p\in\{1,2,n\}$ the sum becomes: For $p = 1$, $\sum_{n\ge1} \frac{ n! } {1\cdot2\cdots(1+n-1)} = \sum_{n\ge1} \frac{n!}{n!} = n$ is convergent and For $p = 2$, $\sum_{n\ge1} \frac{ n! } {2\cdot3\cdots(2+n-1)} = \sum_{n\ge1} \frac{n!}{2\cdot3\cdots(n+1)} =  \sum_{n\ge1} \frac{n!}{(n+1)!} =\sum_{n\ge1} \frac{1}{n+1}$ is convergent and For $p = n$, $\sum_{n\ge1} \frac{ n! } {n(n+1)\cdots(2n-1)} = \sum_{n\ge1} \frac{(n-2)!}{(n+2)(n+3)\cdots(2n-1)}$ is convergent (d'Alembert) How do I proof my intuition is a rigorous mathematical way?",,"['calculus', 'analysis']"
9,Question about $L^p$ spaces,Question about  spaces,L^p,"Suppose $1<p<\infty$ and let $L^1$ and $L^p$ denote the usual Lebesgue spaces on $[0,1]$.  Let $$A=\{f\in L^1:\|f\|_p\leq 1\}.$$  Show $A$ is closed in $L^1$. I took a sequence $\{f_n\}$ in $A$ and assumed it converges to $f$ in $L^1$. I am having trouble showing $\|f\|_p\leq 1$.","Suppose $1<p<\infty$ and let $L^1$ and $L^p$ denote the usual Lebesgue spaces on $[0,1]$.  Let $$A=\{f\in L^1:\|f\|_p\leq 1\}.$$  Show $A$ is closed in $L^1$. I took a sequence $\{f_n\}$ in $A$ and assumed it converges to $f$ in $L^1$. I am having trouble showing $\|f\|_p\leq 1$.",,"['general-topology', 'analysis', 'measure-theory']"
10,Proving $\sup\left\{ r\in\mathbb{Q}:r^{2}<3\right\}=\sqrt{3}$,Proving,\sup\left\{ r\in\mathbb{Q}:r^{2}<3\right\}=\sqrt{3},"Let $E=\left\{ r\in\mathbb{Q}:r^{2}<3\right\}$.  Prove that $\sup E=\sqrt{3}$. Since $E$  is bounded from above by $\sqrt{3}$  and is nonempty, $\alpha:=\sup E$  must exist by the Least Upper Bound Principle. Now I am stuck. Suppose $\alpha<\sqrt{3}$. Then what? Edit: It just hit me.  If $\alpha < \sqrt{3}$ then since the rationals are dense in $\mathbb{R}$, there exists $q\in\mathbb{Q}$ such that $\alpha < q <\sqrt{3}$ and hence $\alpha^2 <q^2 <3$ which contradicts the fact that $\alpha$ is the sup of $E$","Let $E=\left\{ r\in\mathbb{Q}:r^{2}<3\right\}$.  Prove that $\sup E=\sqrt{3}$. Since $E$  is bounded from above by $\sqrt{3}$  and is nonempty, $\alpha:=\sup E$  must exist by the Least Upper Bound Principle. Now I am stuck. Suppose $\alpha<\sqrt{3}$. Then what? Edit: It just hit me.  If $\alpha < \sqrt{3}$ then since the rationals are dense in $\mathbb{R}$, there exists $q\in\mathbb{Q}$ such that $\alpha < q <\sqrt{3}$ and hence $\alpha^2 <q^2 <3$ which contradicts the fact that $\alpha$ is the sup of $E$",,['analysis']
11,The ratio of two strictly increasing functions,The ratio of two strictly increasing functions,,"although it seems very simple and obvious, I have no idea how to give an analytical proof for this problem. I will be very happy if there are some smart ideas... Given, $f_1(a), f_2(a),..., f_n(a)$ and $g_1(a), g_2(a),..., g_n(a)$ are strictly increasing positive functions of $a$. It is also known that $\frac{f_1(a)}{g_1(a)}$, $\frac{f_2(a)}{g_2(a)}$,...,$\frac{f_n(a)}{g_n(a)}$ are strictly increasing functions of a. I want to know if \begin{equation} \frac{f_1(a)+f_2(a)+...+f_n(a)}{g_1(a)+g_2(a)+...+g_n(a)} \end{equation} is also an increasing function of $a$.","although it seems very simple and obvious, I have no idea how to give an analytical proof for this problem. I will be very happy if there are some smart ideas... Given, $f_1(a), f_2(a),..., f_n(a)$ and $g_1(a), g_2(a),..., g_n(a)$ are strictly increasing positive functions of $a$. It is also known that $\frac{f_1(a)}{g_1(a)}$, $\frac{f_2(a)}{g_2(a)}$,...,$\frac{f_n(a)}{g_n(a)}$ are strictly increasing functions of a. I want to know if \begin{equation} \frac{f_1(a)+f_2(a)+...+f_n(a)}{g_1(a)+g_2(a)+...+g_n(a)} \end{equation} is also an increasing function of $a$.",,['analysis']
12,Origin of the mixed norm,Origin of the mixed norm,,"One can define the ($\alpha, \beta$)-mixed norm for a matrix $A$ as \[ \|A \|_{\alpha, \beta} = (\sum \|a_i\|_\alpha^\beta)^{1/\beta} \] where $a_i$ is the $i^{\text{th}}$ column of $A$ and $\|a_i\|_\alpha$ is the usual $\alpha$-norm \[ \|v\|_\alpha = (\sum_i v_i^\alpha)^{1/\alpha} \] Is this a well-known norm in that there's a reference for where it's first defined ?","One can define the ($\alpha, \beta$)-mixed norm for a matrix $A$ as \[ \|A \|_{\alpha, \beta} = (\sum \|a_i\|_\alpha^\beta)^{1/\beta} \] where $a_i$ is the $i^{\text{th}}$ column of $A$ and $\|a_i\|_\alpha$ is the usual $\alpha$-norm \[ \|v\|_\alpha = (\sum_i v_i^\alpha)^{1/\alpha} \] Is this a well-known norm in that there's a reference for where it's first defined ?",,"['linear-algebra', 'analysis', 'reference-request']"
13,Functional disequality,Functional disequality,,"Let $f \in C^{2}([a,b]) \ $, $f(a)= f(b) = 0 \ $, $f(x) > 0 \ \forall x \in (a,b) \ $, $f(x) + f(x)''>0  \ $. Then $b-a \ge \pi $.  Any hint?","Let $f \in C^{2}([a,b]) \ $, $f(a)= f(b) = 0 \ $, $f(x) > 0 \ \forall x \in (a,b) \ $, $f(x) + f(x)''>0  \ $. Then $b-a \ge \pi $.  Any hint?",,['analysis']
14,Convex Conjugate of Absolute Norm,Convex Conjugate of Absolute Norm,,"Let $f:\mathbb{R}\rightarrow[-\infty,\infty]$ be a continuous function. The convex conjugate of $f$ is: $$f^*(p) := \sup_{x\in\mathbb{R}}\{px-f(x)\}~.$$ Furthermore, let us define the subderivative $\partial f(a)$ of $f$ at $a$: $$\partial f(a) :=  \{y\in[-\infty,\infty]: f(x)-f(a)\ge y(x-a)\}~.$$ I found out that for $f(x)=|x|$: $$ f^*(p) =  \begin{cases} 0 & \text{for } |p|\le 1\\ \infty &\text{else}.\end{cases}  $$ How can we show that?","Let $f:\mathbb{R}\rightarrow[-\infty,\infty]$ be a continuous function. The convex conjugate of $f$ is: $$f^*(p) := \sup_{x\in\mathbb{R}}\{px-f(x)\}~.$$ Furthermore, let us define the subderivative $\partial f(a)$ of $f$ at $a$: $$\partial f(a) :=  \{y\in[-\infty,\infty]: f(x)-f(a)\ge y(x-a)\}~.$$ I found out that for $f(x)=|x|$: $$ f^*(p) =  \begin{cases} 0 & \text{for } |p|\le 1\\ \infty &\text{else}.\end{cases}  $$ How can we show that?",,"['analysis', 'convex-analysis', 'convex-optimization']"
15,simple real analysis question on integration,simple real analysis question on integration,,"it is trivial that $\int_0^{2\pi} \cos(x)\,dx = 0$.  Intuitively, it is clear that for a strictly decreasing positive function $f(x)$,  $$ \int_0^{2\pi} f(x) \cos(x)\,dx \ge 0 $$ but I have no glue how to prove that one. Any hints? Cheers, Eric edit: I checked it for power-type functions $(a+t)^{-\alpha}$ numerically.","it is trivial that $\int_0^{2\pi} \cos(x)\,dx = 0$.  Intuitively, it is clear that for a strictly decreasing positive function $f(x)$,  $$ \int_0^{2\pi} f(x) \cos(x)\,dx \ge 0 $$ but I have no glue how to prove that one. Any hints? Cheers, Eric edit: I checked it for power-type functions $(a+t)^{-\alpha}$ numerically.",,"['analysis', 'integration']"
16,Primes and Riemann zeta function.,Primes and Riemann zeta function.,,Primes numbers and Riemann zeta function. Question 1: Is there a proof of the infinitude of prime numbers using the Riemann Zeta function. Exboço could show me a proof of this where I could find it? Question 2: What is the relationship of this function with the famous Riemann hypothesis?,Primes numbers and Riemann zeta function. Question 1: Is there a proof of the infinitude of prime numbers using the Riemann Zeta function. Exboço could show me a proof of this where I could find it? Question 2: What is the relationship of this function with the famous Riemann hypothesis?,,"['number-theory', 'analysis', 'riemann-zeta']"
17,how to apply maximum principle in this PDE?,how to apply maximum principle in this PDE?,,"I have a PDE in the bounded domain $\Omega$: $-\Delta u+ a(x)u=0$ with $u=0$ on $\partial \Omega$, and $a(x)>0$. How do I show that $u\equiv 0$ in $\Omega$? I think I should use maximum principle somewhere but I do not know how to apply it.","I have a PDE in the bounded domain $\Omega$: $-\Delta u+ a(x)u=0$ with $u=0$ on $\partial \Omega$, and $a(x)>0$. How do I show that $u\equiv 0$ in $\Omega$? I think I should use maximum principle somewhere but I do not know how to apply it.",,"['analysis', 'partial-differential-equations']"
18,Uniform convergence for sequences of functions,Uniform convergence for sequences of functions,,"Please help me out with this problem: Let $f_n(x) : [0,1] \to \mathbb R$ be a sequence of continuous functions convergent at every $x \in [0,1]$ to a continuous function $f: [0,1]  \to \mathbb R$ . Does $f_n$ converge uniformly to $f$? My first solution: I read this example in a different question posted earlier and I think it works: Take a sequence $f_n:[0,1] \to \mathbb R$ such that $f_n$ increases linearly from $0$ to $1$ on the interval $\left[ 0,\frac{1}{n} \right]$, decreases linearly from $1$ to $0$ on the intreval $\left[ \frac{1}{n}, \frac{2}{n} \right]$, and is $0$ on $\left[ \frac{2}{n},1 \right]$. Then $f_{n} \to 0$ nonuniformly. Is this answer true? Also, if anyone can give some examples of such function with detailed proof, I will very appreciative?","Please help me out with this problem: Let $f_n(x) : [0,1] \to \mathbb R$ be a sequence of continuous functions convergent at every $x \in [0,1]$ to a continuous function $f: [0,1]  \to \mathbb R$ . Does $f_n$ converge uniformly to $f$? My first solution: I read this example in a different question posted earlier and I think it works: Take a sequence $f_n:[0,1] \to \mathbb R$ such that $f_n$ increases linearly from $0$ to $1$ on the interval $\left[ 0,\frac{1}{n} \right]$, decreases linearly from $1$ to $0$ on the intreval $\left[ \frac{1}{n}, \frac{2}{n} \right]$, and is $0$ on $\left[ \frac{2}{n},1 \right]$. Then $f_{n} \to 0$ nonuniformly. Is this answer true? Also, if anyone can give some examples of such function with detailed proof, I will very appreciative?",,"['calculus', 'real-analysis', 'analysis']"
19,Show that the following identity is formally correct,Show that the following identity is formally correct,,"$$\sum_{k=0}^{\infty}\frac{1}{2^{2k}}\sum_{k=0}^{\infty}\frac{1}{3^{2k}}\sum_{k=0}^{\infty}\frac{1}{5^{2k}}\cdots=\sum_{n=1}^{\infty}\frac{1}{n^2}$$ Ignoring problems connected with convergence and rearrangement of terms, and the denominators on the left are the even powers of the primes.","$$\sum_{k=0}^{\infty}\frac{1}{2^{2k}}\sum_{k=0}^{\infty}\frac{1}{3^{2k}}\sum_{k=0}^{\infty}\frac{1}{5^{2k}}\cdots=\sum_{n=1}^{\infty}\frac{1}{n^2}$$ Ignoring problems connected with convergence and rearrangement of terms, and the denominators on the left are the even powers of the primes.",,"['analysis', 'elementary-number-theory']"
20,An inequality with integrals,An inequality with integrals,,"Let $\phi\colon[0,1] \to \mathbb R$ be such that $\phi,\phi^\prime,\phi^{\prime\prime}$ are continuous on $[0,1]$, then the following inequality holds: $$\int_0^1\cos x\frac{x\phi^\prime(x)-\phi(x)+\phi(0)}{x^2}\mathrm dx < \frac32\|\phi^{\prime\prime}\|_\infty$$ I have no idea how to solve this problem, could you help me please?","Let $\phi\colon[0,1] \to \mathbb R$ be such that $\phi,\phi^\prime,\phi^{\prime\prime}$ are continuous on $[0,1]$, then the following inequality holds: $$\int_0^1\cos x\frac{x\phi^\prime(x)-\phi(x)+\phi(0)}{x^2}\mathrm dx < \frac32\|\phi^{\prime\prime}\|_\infty$$ I have no idea how to solve this problem, could you help me please?",,"['real-analysis', 'analysis', 'inequality']"
21,An inequality of integrals,An inequality of integrals,,"Let $f\in C^1([a,b])$ with $f(a)=0$. How can I show that there exists a positive constant $M$ independent of $f$ such that $\int^b_a|f(x)|^2dx\leq M\int^b_a|f^\prime(x)|^2dx$?","Let $f\in C^1([a,b])$ with $f(a)=0$. How can I show that there exists a positive constant $M$ independent of $f$ such that $\int^b_a|f(x)|^2dx\leq M\int^b_a|f^\prime(x)|^2dx$?",,"['real-analysis', 'analysis', 'inequality']"
22,Fourier series at discontinuities,Fourier series at discontinuities,,"I was reading about Fourier series and have a doubt concerning it. The book I am reading from does not seem to help. As I understand, $\{e_0=\frac{1}{\sqrt{2}},e_1=sin(x),e_2=cos(x),e_3=sin(2x),e_4=cos(2x)\cdots\}$ is a basis for the inner product space of piecewise continuous functions in $[-\pi,\pi]$ with inner product $<f,g>=\frac{1}{\pi}\int_{-\pi}^{\pi}f\bar g$. Hence any function in this space may be represented by $f=\sum_{k=0}^{\infty}<f,e_k>e_k$. My question is what happens at points of discontinuity x. As f is identical with the series (which by the way is unclear to me as to why it coverges) shouldn't f(x) be identical with the series at x, i.e. $f(x)=\sum_{k=0}^{\infty}<f,e_k>e_k(x)$. But Dirichlet's theorem (stated without proof in my book) says that at points of discontinuity, the series $\sum_{k=0}^{\infty}<f,e_k>e_k(x)$ converges to $\frac{f(x-)+f(x+)}{2}$ and not to f(x). Why is this so? Thanks.","I was reading about Fourier series and have a doubt concerning it. The book I am reading from does not seem to help. As I understand, $\{e_0=\frac{1}{\sqrt{2}},e_1=sin(x),e_2=cos(x),e_3=sin(2x),e_4=cos(2x)\cdots\}$ is a basis for the inner product space of piecewise continuous functions in $[-\pi,\pi]$ with inner product $<f,g>=\frac{1}{\pi}\int_{-\pi}^{\pi}f\bar g$. Hence any function in this space may be represented by $f=\sum_{k=0}^{\infty}<f,e_k>e_k$. My question is what happens at points of discontinuity x. As f is identical with the series (which by the way is unclear to me as to why it coverges) shouldn't f(x) be identical with the series at x, i.e. $f(x)=\sum_{k=0}^{\infty}<f,e_k>e_k(x)$. But Dirichlet's theorem (stated without proof in my book) says that at points of discontinuity, the series $\sum_{k=0}^{\infty}<f,e_k>e_k(x)$ converges to $\frac{f(x-)+f(x+)}{2}$ and not to f(x). Why is this so? Thanks.",,['analysis']
23,Compute unknown limit from known integral in Mathematica?,Compute unknown limit from known integral in Mathematica?,,"I have an integral $\int_a^b \! f(x) \, \mathrm{d}x = c$ where I know c and either a or b. Now I want to compute either b or a (i.e., the missing limit). How would I do that in Mathematica/Wolfram Alpha?","I have an integral $\int_a^b \! f(x) \, \mathrm{d}x = c$ where I know c and either a or b. Now I want to compute either b or a (i.e., the missing limit). How would I do that in Mathematica/Wolfram Alpha?",,"['analysis', 'integration', 'mathematica']"
24,"A sufficient condition for $U \subseteq \mathbb{R}^2$ such that $f(x,y) = f(x)$",A sufficient condition for  such that,"U \subseteq \mathbb{R}^2 f(x,y) = f(x)","I have another short question. Let $U \subseteq \mathbb{R}^2$ be open and $f: U \rightarrow \mathbb{R}$ be continuously differentiable. Also, $\partial_y f(x,y) = 0$ for all $(x,y) \in U$. I want to find a sufficient condition for $U$ such that $f$ only depends on $x$. Of course, the condition shouldn't be too restrictive. Is it sufficient for $U$ to be connected? Thanks a lot for any help.","I have another short question. Let $U \subseteq \mathbb{R}^2$ be open and $f: U \rightarrow \mathbb{R}$ be continuously differentiable. Also, $\partial_y f(x,y) = 0$ for all $(x,y) \in U$. I want to find a sufficient condition for $U$ such that $f$ only depends on $x$. Of course, the condition shouldn't be too restrictive. Is it sufficient for $U$ to be connected? Thanks a lot for any help.",,['analysis']
25,Prove a property holds on a metric space if it holds on a dense subset,Prove a property holds on a metric space if it holds on a dense subset,,"For the past two weeks, I've tried to prove two different results that hold the same structure: Suppose a property that holds for a dense subset of a metric space. Prove that it holds for the entire metric space. In this case, both questions were related to uniform convergence: i) Show that if $(f_{n})$ is a sequence of continuous functions on M, and if $\sum_{n=1}^{\infty}f_{n}$ is converges uniformly on a dense subset A of M, then the series converges uniformly on all of M. ii) Let A be a dense subset of M. If $(f_{n})$ is a sequence of continuous functions on M, and if the sequence converges uniformly on A, prove that $(f_{n})$ converges uniformly on M. My questions are the following: What is the framework I should have in my mind when trying to prove these kinds of properties? What is the basic frame of mind you have when proving such results? Because I feel that I'm missing something, that is, my intuition about how a dense subset works is not strong enough. By the way, I'm sorry for the wall of text :) Thanks in advance!","For the past two weeks, I've tried to prove two different results that hold the same structure: Suppose a property that holds for a dense subset of a metric space. Prove that it holds for the entire metric space. In this case, both questions were related to uniform convergence: i) Show that if $(f_{n})$ is a sequence of continuous functions on M, and if $\sum_{n=1}^{\infty}f_{n}$ is converges uniformly on a dense subset A of M, then the series converges uniformly on all of M. ii) Let A be a dense subset of M. If $(f_{n})$ is a sequence of continuous functions on M, and if the sequence converges uniformly on A, prove that $(f_{n})$ converges uniformly on M. My questions are the following: What is the framework I should have in my mind when trying to prove these kinds of properties? What is the basic frame of mind you have when proving such results? Because I feel that I'm missing something, that is, my intuition about how a dense subset works is not strong enough. By the way, I'm sorry for the wall of text :) Thanks in advance!",,"['real-analysis', 'analysis']"
26,Switching Limits and Suprema,Switching Limits and Suprema,,"suppose, $f(x,y)$ is a bounded continuous function on $\mathbb{R}^2$. Consider $$\lim_{ y \rightarrow y' }\> \sup_{x \in \mathbb{R}} f(x,y).$$ In how far can you switch suprema and limits, or which possible term comes closest to switching these two? Ideally, there would be something like $$\sup_{x \in \mathbb{R}}\> \lim_{ y \rightarrow y' } f(x,y).$$ Best","suppose, $f(x,y)$ is a bounded continuous function on $\mathbb{R}^2$. Consider $$\lim_{ y \rightarrow y' }\> \sup_{x \in \mathbb{R}} f(x,y).$$ In how far can you switch suprema and limits, or which possible term comes closest to switching these two? Ideally, there would be something like $$\sup_{x \in \mathbb{R}}\> \lim_{ y \rightarrow y' } f(x,y).$$ Best",,['analysis']
27,The set of all ellipsoids $\mathcal{E}(A)$ contained in a bounded open set $A$ is compact,The set of all ellipsoids  contained in a bounded open set  is compact,\mathcal{E}(A) A,"We call $A\subset\Bbb R^d$ a convex body if $A$ is a convex, non-empty, open, and bounded. The open unit ball in $\Bbb R^d$ is denoted by $B_d$ . In Tao-Vu's book, they say: Define an ellipsoid to be any set $E$ of the form $E = L(B_d) + x_0$ , where $B_d$ is the unit ball, $x_0 ∈ \Bbb R^d$ , and $L$ is a (possibly degenerate) linear transformation in $\Bbb R^d$ ; we allow the ellipsoid to be degenerate for compactness reasons. Since $A$ is open and bounded, it is easy to see that the set of all ellipsoids $E$ contained in $A$ is a compact set (with respect to the usual topology on $L$ and $x_0$ .) It seems the convexity assumption on $A$ is not required to show the compactness of the set of ellipsoids. The set can be written explicitly as $$\mathcal E(A) = \{E = L(B_d) + x_0: L:\Bbb R^d \to \Bbb R^d \text{ linear, }x_0 \in \Bbb R^d, E \subset A\}.$$ In particular, we can identify every ellipsoid $L(B_d) + x_0$ with a tuple $(L,x_0)$ , so $$\mathcal E(A) \subset M_d(\Bbb R) \times \Bbb R^d,$$ and we want to show that $\mathcal E(A)$ is compact with respect to the topology on $M_d(\Bbb R) \times \Bbb R^d$ . How should this argument go? I tried a sequential argument, with ellipsoids $\{(L_n,x_n)\}_{n=1}^\infty$ contained in $A$ , but I'm not sure how to use the compactness of $\overline{A}$ to extract a convergent subsequence. Thank you!","We call a convex body if is a convex, non-empty, open, and bounded. The open unit ball in is denoted by . In Tao-Vu's book, they say: Define an ellipsoid to be any set of the form , where is the unit ball, , and is a (possibly degenerate) linear transformation in ; we allow the ellipsoid to be degenerate for compactness reasons. Since is open and bounded, it is easy to see that the set of all ellipsoids contained in is a compact set (with respect to the usual topology on and .) It seems the convexity assumption on is not required to show the compactness of the set of ellipsoids. The set can be written explicitly as In particular, we can identify every ellipsoid with a tuple , so and we want to show that is compact with respect to the topology on . How should this argument go? I tried a sequential argument, with ellipsoids contained in , but I'm not sure how to use the compactness of to extract a convergent subsequence. Thank you!","A\subset\Bbb R^d A \Bbb R^d B_d E E = L(B_d) + x_0 B_d x_0 ∈ \Bbb R^d L \Bbb R^d A E A L x_0 A \mathcal E(A) = \{E = L(B_d) + x_0: L:\Bbb R^d \to \Bbb R^d \text{ linear, }x_0 \in \Bbb R^d, E \subset A\}. L(B_d) + x_0 (L,x_0) \mathcal E(A) \subset M_d(\Bbb R) \times \Bbb R^d, \mathcal E(A) M_d(\Bbb R) \times \Bbb R^d \{(L_n,x_n)\}_{n=1}^\infty A \overline{A}","['real-analysis', 'general-topology', 'analysis', 'compactness', 'ellipsoids']"
28,"If $\int_Afd\mu\geq0$ for all $A\in\mathscr{A}$, then $\int f\chi_Ad\mu=0$ for $A=\{x\in X:f(x)<0\}$","If  for all , then  for",\int_Afd\mu\geq0 A\in\mathscr{A} \int f\chi_Ad\mu=0 A=\{x\in X:f(x)<0\},"I am self-studying measure theory using Measure Theory by Donald Cohn. I am confused by his proof of the following result: Corollary 2.3.13 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[-\infty,+\infty]$ -valued integrable function on $X$ such that $\int_Afd\mu\geq0$ holds for all $A$ in $\mathscr{A}$ . Then $f\geq0$ holds $\mu$ -almost everywhere. Proof $\quad$ Let $A = \{x \in X:f(x) < 0\}$ . Then $\int f\chi_Ad\mu = \int_Afd\mu = 0$ (since $f<0$ on $A$ , yet we are assuming that $\int_Afd\mu\geq0$ ). $\dots$ I couldn't understand why $\int f\chi_Ad\mu = \int_Afd\mu = 0$ . Basically, this is how I got stuck: Let $A=\{x\in X:f(x)<0\}$ . Then $A\in\mathscr{A}$ because $f$ is integrable which means it must be $\mathscr{A}$ -measurable. We also have that $f\chi_A\leq0$ . Then $\int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\}$ , where $\mathscr{S}_+$ is the set of all nonnegative simple real-valued $\mathscr{A}$ -measurable functions on $X$ . However, shouldn't such $g$ not exist? Or, perhaps more appropriately, if $f(x)\geq0$ for all $x$ in $X$ then $A=\emptyset$ so that $\int f\chi_Ad\mu = \int0d\mu = 0$ ; if $f(x)<0$ for some $x$ in $X$ then there would be no $g$ in $\mathscr{S}_+$ such that $g\leq f\chi_A$ , so that $\int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\} = \sup\{\emptyset\} = -\infty$ . $***************\ *$ So, it seems to me that the assumption $\int_Afd\mu\geq0$ for all $A\in\mathscr{A}$ implies that $f(x)\geq0$ for all $x$ in $X$ . But why? Here are some of my thoughts: I think we need to consider two cases: Suppose $\int_Afd\mu=+\infty$ , which means either $\int(f\chi_A)^+d\mu$ or $\int(f\chi_A)^-d\mu$ or both is not finite. Suppose $\int_Afd\mu\geq0$ and it is finite. However, I don't know how to proceed next. I don't know if any of the above makes sense. I really appreciate it if someone could help me out here! Update $\quad$ I just realized that everything below the "" $***\ *$ "" doesn't really make sense, because the question want us to prove $f\geq0$ holds $\mu$ -almost everywhere. For a construction of the integral in this book, please see this post .","I am self-studying measure theory using Measure Theory by Donald Cohn. I am confused by his proof of the following result: Corollary 2.3.13 Let be a measure space, and let be a -valued integrable function on such that holds for all in . Then holds -almost everywhere. Proof Let . Then (since on , yet we are assuming that ). I couldn't understand why . Basically, this is how I got stuck: Let . Then because is integrable which means it must be -measurable. We also have that . Then , where is the set of all nonnegative simple real-valued -measurable functions on . However, shouldn't such not exist? Or, perhaps more appropriately, if for all in then so that ; if for some in then there would be no in such that , so that . So, it seems to me that the assumption for all implies that for all in . But why? Here are some of my thoughts: I think we need to consider two cases: Suppose , which means either or or both is not finite. Suppose and it is finite. However, I don't know how to proceed next. I don't know if any of the above makes sense. I really appreciate it if someone could help me out here! Update I just realized that everything below the "" "" doesn't really make sense, because the question want us to prove holds -almost everywhere. For a construction of the integral in this book, please see this post .","\quad (X,\mathscr{A},\mu) f [-\infty,+\infty] X \int_Afd\mu\geq0 A \mathscr{A} f\geq0 \mu \quad A = \{x \in X:f(x) < 0\} \int f\chi_Ad\mu = \int_Afd\mu = 0 f<0 A \int_Afd\mu\geq0 \dots \int f\chi_Ad\mu = \int_Afd\mu = 0 A=\{x\in X:f(x)<0\} A\in\mathscr{A} f \mathscr{A} f\chi_A\leq0 \int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\} \mathscr{S}_+ \mathscr{A} X g f(x)\geq0 x X A=\emptyset \int f\chi_Ad\mu = \int0d\mu = 0 f(x)<0 x X g \mathscr{S}_+ g\leq f\chi_A \int f\chi_Ad\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\chi_A\right\} = \sup\{\emptyset\} = -\infty ***************\ * \int_Afd\mu\geq0 A\in\mathscr{A} f(x)\geq0 x X \int_Afd\mu=+\infty \int(f\chi_A)^+d\mu \int(f\chi_A)^-d\mu \int_Afd\mu\geq0 \quad ***\ * f\geq0 \mu","['real-analysis', 'integration', 'analysis', 'measure-theory', 'measurable-functions']"
29,"Interior and closure of sets on metric spaces combined $ A, A^{\circ}, \overline{A^{\circ}}, \ldots $",Interior and closure of sets on metric spaces combined," A, A^{\circ}, \overline{A^{\circ}}, \ldots ","For $ A \subseteq X $ where $ X $ is a metric space, the following sets can be formed: $ A, A^{\circ}, \overline{A^{\circ}}, ( \overline{A^{\circ}} )^{\circ}, \ldots $ and $ \overline{A}, ( \overline{A} )^{\circ}, \overline{( \overline{A} )^{\circ}}, \ldots $ a.) Show: This system of sets consists of at most 7 different sets. b.) Find an example $ A \subseteq X $ , where indeed 7 different sets occur. I've been struggling with this for a while now. I firstly tried to construct the example first to understand it better, so that $ ( \overline{A^{\circ}} )$ and $( \overline{A} )^{\circ}$ are different but this gets messy really fast. I just couldnt find a good way to start this problem so im very glad for any help.","For where is a metric space, the following sets can be formed: and a.) Show: This system of sets consists of at most 7 different sets. b.) Find an example , where indeed 7 different sets occur. I've been struggling with this for a while now. I firstly tried to construct the example first to understand it better, so that and are different but this gets messy really fast. I just couldnt find a good way to start this problem so im very glad for any help."," A \subseteq X   X   A, A^{\circ}, \overline{A^{\circ}}, ( \overline{A^{\circ}} )^{\circ}, \ldots   \overline{A}, ( \overline{A} )^{\circ}, \overline{( \overline{A} )^{\circ}}, \ldots   A \subseteq X   ( \overline{A^{\circ}} ) ( \overline{A} )^{\circ}","['real-analysis', 'analysis', 'vector-spaces', 'metric-spaces', 'function-and-relation-composition']"
30,When is the ratio of largest number and smallest number when the sum and sum of squares is fixed,When is the ratio of largest number and smallest number when the sum and sum of squares is fixed,,"Given $n$ positive real numbers $a_1\geq a_2,\cdots\geq a_n>0$ . Assume $\sum_{i=1}^n a_i=b_1, \sum_{i=1}^n a_i^2=b_2$ . I want to find an upper bound on $\frac{a_1}{a_n}$ , and the condition when this upper bound is achieved. My intuition is that the upper bound is achieved either $a_1=a_2=,\cdots=a_{n-1}>a_n$ or $a_1>a_2=a_3=\cdots a_n$ , but I don't know how to prove it. Some ideas: if for any fixed small $a_n$ , the equations $a_1+\cdots+a_{n-1}=b_1-a_n, a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2$ has solutions, then one can let $a_n\to 0$ , and the $\frac{a_1}{a_n}\to\infty$ . To prevent this from happening, we need some conditions on $b_1, b_2$ so that $a_n$ has a positive lower bound. Specifically, one can use the following inequality \begin{align} \frac{a_1+\cdots+a_{n-1}}{n-1} \leq \sqrt{\frac{a_1^2+\cdots+a_{n-1}^2}{n-1}} \end{align} We plug in $a_1+\cdots+a_{n-1}=b_1-a_n$ and $a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2$ , after some calculations, we get $na_n^2-2b_1a_n+b_1^2-(n-1)b_2\leq 0$ . Therefore, as long as $b_1^2-(n-1)b_2\leq 0$ , then $a_n$ will have a positive lower bound. Then, my question is when $b_1^2-(n-1)b_2\leq 0$ , what is the maximum value of $\frac{a_1}{a_n}$ and when the maximum is achieved?","Given positive real numbers . Assume . I want to find an upper bound on , and the condition when this upper bound is achieved. My intuition is that the upper bound is achieved either or , but I don't know how to prove it. Some ideas: if for any fixed small , the equations has solutions, then one can let , and the . To prevent this from happening, we need some conditions on so that has a positive lower bound. Specifically, one can use the following inequality We plug in and , after some calculations, we get . Therefore, as long as , then will have a positive lower bound. Then, my question is when , what is the maximum value of and when the maximum is achieved?","n a_1\geq a_2,\cdots\geq a_n>0 \sum_{i=1}^n a_i=b_1, \sum_{i=1}^n a_i^2=b_2 \frac{a_1}{a_n} a_1=a_2=,\cdots=a_{n-1}>a_n a_1>a_2=a_3=\cdots a_n a_n a_1+\cdots+a_{n-1}=b_1-a_n, a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2 a_n\to 0 \frac{a_1}{a_n}\to\infty b_1, b_2 a_n \begin{align}
\frac{a_1+\cdots+a_{n-1}}{n-1} \leq \sqrt{\frac{a_1^2+\cdots+a_{n-1}^2}{n-1}}
\end{align} a_1+\cdots+a_{n-1}=b_1-a_n a_1^2+\cdots+a_{n-1}^2=b_2-a_n^2 na_n^2-2b_1a_n+b_1^2-(n-1)b_2\leq 0 b_1^2-(n-1)b_2\leq 0 a_n b_1^2-(n-1)b_2\leq 0 \frac{a_1}{a_n}","['combinatorics', 'analysis', 'inequality']"
31,Proving the convergence of a series with very little information,Proving the convergence of a series with very little information,,"Let $m \in \mathbb{N}$ be a fixed natural number and $ (a_n)_{n \geq 1}$ be a sequence of positive real numbers such that $\forall n \geq 1\colon a_{n+1} \leq a_n - a_{mn}$ . Prove that the series $\sum_{n=1}^{\infty}n^\alpha a_n$ converges $\forall \alpha \in \mathbb{R}_+$ . My first idea was to show that $a_n$ is decreasing. From the hypothesis, we have that $ 0 < a_{mn} \leq a_n-a_{n+1}$ . So we can sum this equality up from $n=s$ to $n=t$ , so we get $\sum_{n=s}^t a_{nm} \leq a_s-a_{t+1}$ . However I do not think that it helps that much.","Let be a fixed natural number and be a sequence of positive real numbers such that . Prove that the series converges . My first idea was to show that is decreasing. From the hypothesis, we have that . So we can sum this equality up from to , so we get . However I do not think that it helps that much.",m \in \mathbb{N}  (a_n)_{n \geq 1} \forall n \geq 1\colon a_{n+1} \leq a_n - a_{mn} \sum_{n=1}^{\infty}n^\alpha a_n \forall \alpha \in \mathbb{R}_+ a_n  0 < a_{mn} \leq a_n-a_{n+1} n=s n=t \sum_{n=s}^t a_{nm} \leq a_s-a_{t+1},"['real-analysis', 'calculus', 'sequences-and-series', 'analysis', 'problem-solving']"
32,Proof verification: Steinhaus Theorem,Proof verification: Steinhaus Theorem,,"Theorem (Steinhaus): If $E \subseteq \mathbb{R}$ is a measurable set with positive Lebesgue measure, then the set $$ E - E = \{e_1 - e_2 : e_1, e_2 \in E\} $$ contains an open interval around the origin. Let $m$ denote the lebesgue measure. It suffices to prove the case where $m(E) < \infty$ , since $m$ is a semifinite measure. There exists an interval $(a,b)$ where $a,b\neq \pm \infty$ , such that $m((a,b)\cap E) > \frac{3}{4} m((a,b))$ (by a well-know theorem? Chapter 1 exercise 30 in folland). Let $I = [a,b)$ . It is clear that $I$ satisfies the same inequality since it differs $(a,b)$ by a measure of $0$ . We we will show that $(-\frac{1}{10}m(I),\frac{1}{10}m(I)) \subseteq E-E$ . Suppose for sake of contradiction that $\exists k \in (-\frac{1}{10}m(I),\frac{1}{10}m(I))$ such that it is not in $E-E$ . We assume $k>0$ , since $E-E$ is clearly symmetric. Consider the intervals $I_0 = [a,a+k), I_1 = [a+k,a+2k),\dots, I_n = [a+nk,a+(n+1)k)$ where $a+nk<b\leq a+(n+1)k.$ Basically $\{I_j\}$ is the minimal half interval cover of $[a,b)$ each of length $k$ . Then $$m(E\cap I)\leq m(\bigcup_{i=0}^n E \cap I_i)=\sum_{i=0}^nm(E\cap I_i),$$ since $I \subseteq \cup I_i$ and $\{I_i\}$ are pairwise disjoint. Assume that $n+1$ is even. The odd case is similar with some trivial details in the boundary case. Let $\omega_0 = m(E\cap I_0), \omega_1 = m(E\cap I_1)\omega_2 = m(E\cap I_2), \dots ,\omega_n = m(E\cap I_n)$ . Notice that $E\cap I_{2t+1} \subseteq I_{2t+1} \setminus (E \cap I_{2t} + k)$ , since if $x \in E\cap I_{2t+1}$ and $x \in (E \cap I_{2t} + k)$ , then $x=y+k$ for some $y \in E \cap I_{2t}$ , so that $x-y = k$ , a contradiction to $k\notin E-E$ . Thus $m(E\cap I_{2t+1}) \leq m(I_{2t+1} \setminus (E \cap I_{2t} + k)) = m(I_{2t+1}) - m(E \cap I_{2t} + k)$ . By translation invariance of lebesgue measure and the fact that each interval is of length $k$ , it is also equal to $k-\omega_{2t}$ . Thus, $\omega_{2t}+\omega_{2t+1} \leq k$ . So $$\sum_{i=0}^nm(E\cap I_i)\leq \frac{n+1}{2}k \leq \frac{m(I)/k+1}{2}\cdot k= \frac{m(I)}{2}+\frac{1}{2}k<\frac{m(I)}{2}+\frac{m(I)}{10}<\frac{3}{4}m(I),$$ So $m(E\cap I) < \frac{3}{4}m(I)$ . But $m(E\cap I) > \frac{3}{4} m(I)$ by our choice of $I$ , a contradiction.","Theorem (Steinhaus): If is a measurable set with positive Lebesgue measure, then the set contains an open interval around the origin. Let denote the lebesgue measure. It suffices to prove the case where , since is a semifinite measure. There exists an interval where , such that (by a well-know theorem? Chapter 1 exercise 30 in folland). Let . It is clear that satisfies the same inequality since it differs by a measure of . We we will show that . Suppose for sake of contradiction that such that it is not in . We assume , since is clearly symmetric. Consider the intervals where Basically is the minimal half interval cover of each of length . Then since and are pairwise disjoint. Assume that is even. The odd case is similar with some trivial details in the boundary case. Let . Notice that , since if and , then for some , so that , a contradiction to . Thus . By translation invariance of lebesgue measure and the fact that each interval is of length , it is also equal to . Thus, . So So . But by our choice of , a contradiction.","E \subseteq \mathbb{R} 
E - E = \{e_1 - e_2 : e_1, e_2 \in E\}
 m m(E) < \infty m (a,b) a,b\neq \pm \infty m((a,b)\cap E) > \frac{3}{4} m((a,b)) I = [a,b) I (a,b) 0 (-\frac{1}{10}m(I),\frac{1}{10}m(I)) \subseteq E-E \exists k \in (-\frac{1}{10}m(I),\frac{1}{10}m(I)) E-E k>0 E-E I_0 = [a,a+k), I_1 = [a+k,a+2k),\dots, I_n = [a+nk,a+(n+1)k) a+nk<b\leq a+(n+1)k. \{I_j\} [a,b) k m(E\cap I)\leq m(\bigcup_{i=0}^n E \cap I_i)=\sum_{i=0}^nm(E\cap I_i), I \subseteq \cup I_i \{I_i\} n+1 \omega_0 = m(E\cap I_0), \omega_1 = m(E\cap I_1)\omega_2 = m(E\cap I_2), \dots ,\omega_n = m(E\cap I_n) E\cap I_{2t+1} \subseteq I_{2t+1} \setminus (E \cap I_{2t} + k) x \in E\cap I_{2t+1} x \in (E \cap I_{2t} + k) x=y+k y \in E \cap I_{2t} x-y = k k\notin E-E m(E\cap I_{2t+1}) \leq m(I_{2t+1} \setminus (E \cap I_{2t} + k)) = m(I_{2t+1}) - m(E \cap I_{2t} + k) k k-\omega_{2t} \omega_{2t}+\omega_{2t+1} \leq k \sum_{i=0}^nm(E\cap I_i)\leq \frac{n+1}{2}k \leq \frac{m(I)/k+1}{2}\cdot k= \frac{m(I)}{2}+\frac{1}{2}k<\frac{m(I)}{2}+\frac{m(I)}{10}<\frac{3}{4}m(I), m(E\cap I) < \frac{3}{4}m(I) m(E\cap I) > \frac{3}{4} m(I) I","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
33,"$f$ and $1/p$ are positive and decreasing, $\int_1^\infty f<\infty$, $\int_1^\infty\frac{1}{p}=\infty$, show $\lim_{x\rightarrow \infty}p(x)f(x)=0$","and  are positive and decreasing, , , show",f 1/p \int_1^\infty f<\infty \int_1^\infty\frac{1}{p}=\infty \lim_{x\rightarrow \infty}p(x)f(x)=0,"Here I'm interesting in prove (or disprove and counterexamples) the following Claim : Claim : For $p: [1,\infty)\longrightarrow\mathbb R$ and $f: [1,\infty)\longrightarrow\mathbb R$ where $p$ is positive and monotonic increasing, $f$ is non-negative and monotonic decreasing. If $\int_1^\infty f<\infty$ , $\int_1^\infty\frac{1}{p}=\infty$ , then $\lim\limits_{x\longrightarrow \infty}p(x)f(x)=0$ . This claim is based on the following proposition: Proposition : For $f: [1,\infty)\longrightarrow\mathbb R$ to be nonnegative and decreasing with $\int_1^\infty f<\infty$ , show that $\lim\limits_{x\longrightarrow\infty}xf(x)=0$ . For the above proposition is the special case for the claim when $p(x)=1/x$ . The proof of the proposition can proceed in many ways. The following is my proof. Proof of the proposition: Assume not, there exists $\varepsilon_0>0$ , $\forall n\in\mathbb N_+$ , there exists $x_n\in \mathbb R$ such that $x_n>n$ and $f(x_n)\geq \frac{\varepsilon_0}{x_n}$ . Then we integrate $f(x)$ over $[x_n/2,x_n]$ , by the monotonic decreasing property of $f$ , we may know that $f(x)\geq f(x_n)\geq\frac{\varepsilon_0}{x_n}\geq \frac{\varepsilon_0}{2x}dx$ for all $x\in [x_n/2,x_n]$ . Hence: $$\int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{2x}dx=\frac{\varepsilon_0 \log 2}{2}$$ contradicting with the condition that $\int_1^\infty f<\infty$ My Question : I have tried to prove the claim in the similar way to the proof of the proposition . Then I have $$\int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{p(2x)}= \int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt$$ However I find it difficult to use Cauchy criterion or the monotone convergence theorem to argue that $\int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt\geq \eta$ for some $\eta$ . Or I can proceed the proof only if $p$ has an explicit form. Any thoughts and inspirations are welcome. Thank you!","Here I'm interesting in prove (or disprove and counterexamples) the following Claim : Claim : For and where is positive and monotonic increasing, is non-negative and monotonic decreasing. If , , then . This claim is based on the following proposition: Proposition : For to be nonnegative and decreasing with , show that . For the above proposition is the special case for the claim when . The proof of the proposition can proceed in many ways. The following is my proof. Proof of the proposition: Assume not, there exists , , there exists such that and . Then we integrate over , by the monotonic decreasing property of , we may know that for all . Hence: contradicting with the condition that My Question : I have tried to prove the claim in the similar way to the proof of the proposition . Then I have However I find it difficult to use Cauchy criterion or the monotone convergence theorem to argue that for some . Or I can proceed the proof only if has an explicit form. Any thoughts and inspirations are welcome. Thank you!","p: [1,\infty)\longrightarrow\mathbb R f: [1,\infty)\longrightarrow\mathbb R p f \int_1^\infty f<\infty \int_1^\infty\frac{1}{p}=\infty \lim\limits_{x\longrightarrow \infty}p(x)f(x)=0 f: [1,\infty)\longrightarrow\mathbb R \int_1^\infty f<\infty \lim\limits_{x\longrightarrow\infty}xf(x)=0 p(x)=1/x \varepsilon_0>0 \forall n\in\mathbb N_+ x_n\in \mathbb R x_n>n f(x_n)\geq \frac{\varepsilon_0}{x_n} f(x) [x_n/2,x_n] f f(x)\geq f(x_n)\geq\frac{\varepsilon_0}{x_n}\geq \frac{\varepsilon_0}{2x}dx x\in [x_n/2,x_n] \int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{2x}dx=\frac{\varepsilon_0 \log 2}{2} \int_1^\infty f<\infty \int_{x_n/2}^{x_n} f\geq \int_{x_n/2}^{x_n} \frac{\varepsilon_0}{p(2x)}= \int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt \int_{x_n}^{2x_n} \frac{\varepsilon_0}{2p(t)}dt\geq \eta \eta p","['real-analysis', 'calculus', 'analysis', 'improper-integrals', 'divergent-series']"
34,asypmtotic series of $\tan x$ in terms of $x \cos x$,asypmtotic series of  in terms of,\tan x x \cos x,"I have come across an asymptotic series of $\tan x$ in the form of $\tan x \sim x\cos x + \frac{5}{6}(x\cos x)^3 + \frac{161}{120}(x\cos x)^5$ as $x\to 0$ , but I have no idea how this is derived. I am only familiar with the result $\tan x \sim x + x^3/3 + 2/15 x^5$ by Taylor expansion. How is the expansion around $x\cos x$ obtained?","I have come across an asymptotic series of in the form of as , but I have no idea how this is derived. I am only familiar with the result by Taylor expansion. How is the expansion around obtained?",\tan x \tan x \sim x\cos x + \frac{5}{6}(x\cos x)^3 + \frac{161}{120}(x\cos x)^5 x\to 0 \tan x \sim x + x^3/3 + 2/15 x^5 x\cos x,"['calculus', 'analysis', 'asymptotics', 'taylor-expansion']"
35,Proving that $\int f^2\log \left(\frac{|f|}{\|f\|_2}\right)dx \leq \int f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx$.,Proving that .,\int f^2\log \left(\frac{|f|}{\|f\|_2}\right)dx \leq \int f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx,"I want to prove the following inequality: $$\int_{-\infty}^{\infty} f^2\log \left(\frac{|f|}{\|f\|_2}\right) dx \leq \int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx$$ Note that this inequality remains the same if we multiply $f$ by a scalar $c$ . Hence, without loss of generality, we may assume that $\|f\|_2=1$ . Partial solution: By Holder's inequality, and by recalling that $\|f\|_2=1$ , we have that $$\int_{-\infty}^{\infty} f^2\log \left(\frac{|f|}{\|f\|_2}\right) dx \leq \left[\int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx\right]^{1/2}$$ Hence, if $\int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx\geq 1$ , then this implies the inequality that we want. However, I'm unable to prove this inequality if $0\leq \int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx<1$ . I am probably missing something quite elementary.","I want to prove the following inequality: Note that this inequality remains the same if we multiply by a scalar . Hence, without loss of generality, we may assume that . Partial solution: By Holder's inequality, and by recalling that , we have that Hence, if , then this implies the inequality that we want. However, I'm unable to prove this inequality if . I am probably missing something quite elementary.",\int_{-\infty}^{\infty} f^2\log \left(\frac{|f|}{\|f\|_2}\right) dx \leq \int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx f c \|f\|_2=1 \|f\|_2=1 \int_{-\infty}^{\infty} f^2\log \left(\frac{|f|}{\|f\|_2}\right) dx \leq \left[\int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx\right]^{1/2} \int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx\geq 1 0\leq \int_{-\infty}^{\infty} f^2\left(\log \frac{|f|}{\|f\|_2}\right)^2 dx<1,"['analysis', 'inequality']"
36,A characterization of closure of a certain class of sets in $\mathbb{R}^n$,A characterization of closure of a certain class of sets in,\mathbb{R}^n,"Consider a set $K\subset \mathbb{R}^n$ that is symmetric ( $B = -B$ ) and verifies $aK\subset bK$ if $|a|<|b|$ . Can I conclude that $\overline{K} = \cap_{a>1}aK$ ? If not, does the result hold assuming extra properties, like $K$ being convex? Thoughts: when drawing it in the plane it seems pretty clear. I know this is not the best argument, but I don't think this is something that depends on dimension, so I think working in $\mathbb{R}^n$ would suffice. It seems a good idea to me to use that $x\in \overline{K} \iff d(x,K) = 0$ , but I am not sure of how to write the proof. I also am interested if the property holds in general normed vector spaces.","Consider a set that is symmetric ( ) and verifies if . Can I conclude that ? If not, does the result hold assuming extra properties, like being convex? Thoughts: when drawing it in the plane it seems pretty clear. I know this is not the best argument, but I don't think this is something that depends on dimension, so I think working in would suffice. It seems a good idea to me to use that , but I am not sure of how to write the proof. I also am interested if the property holds in general normed vector spaces.","K\subset \mathbb{R}^n B = -B aK\subset bK |a|<|b| \overline{K} = \cap_{a>1}aK K \mathbb{R}^n x\in \overline{K} \iff d(x,K) = 0","['real-analysis', 'general-topology', 'analysis', 'normed-spaces', 'topological-vector-spaces']"
37,Find the maximum and minimum of a function with three variables,Find the maximum and minimum of a function with three variables,,"The problem is finding the maximum and minimum of the following three variables function: $f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)}$ , $(x, y, z)\in \Bbb R^3$ I noticed since $f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)}=[(\sqrt 2x +\frac{\sqrt 2}{2}z)^2+(\sqrt 2y +\frac{\sqrt 2}{2}z)^2]e^{-(x^2+y^2+z^2)}\ge0$ and the equality holds iff $(x, y, z)=t(2, 2, -1)$ , $t\in \Bbb R$ , Thus the minimum of $f$ is $0$ . But now I don't know how to get the maximum. There is no way to apply Lagrange multiplier since there's no contraints. I also thought of using Hessian matrix, but there's just too much calculation and I don't think that's the best way to do it. Any help would be appreciated!","The problem is finding the maximum and minimum of the following three variables function: , I noticed since and the equality holds iff , , Thus the minimum of is . But now I don't know how to get the maximum. There is no way to apply Lagrange multiplier since there's no contraints. I also thought of using Hessian matrix, but there's just too much calculation and I don't think that's the best way to do it. Any help would be appreciated!","f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)} (x, y, z)\in \Bbb R^3 f(x, y, z)=(2x^2+2y^2+z^2+2yz+2zx)e^{-(x^2+y^2+z^2)}=[(\sqrt 2x +\frac{\sqrt 2}{2}z)^2+(\sqrt 2y +\frac{\sqrt 2}{2}z)^2]e^{-(x^2+y^2+z^2)}\ge0 (x, y, z)=t(2, 2, -1) t\in \Bbb R f 0","['analysis', 'multivariable-calculus', 'partial-derivative', 'maxima-minima', 'lagrange-multiplier']"
38,Every Cauchy sequence can be written as the sum of an increasing Cauchy sequence and a decreasing Cauchy sequence,Every Cauchy sequence can be written as the sum of an increasing Cauchy sequence and a decreasing Cauchy sequence,,"I'm trying to solve the following exercise. Let $K$ be an ordered field and $(x_n)_{n\in \mathbb{N}}$ be an Cauchy sequence in $K$ . Show that there exists an increasing Cauchy sequence $(y_n)_{n\in \mathbb{N}}$ and a decreasing Cauchy sequence $(z_n)_{n\in \mathbb{N}}$ with $$x_n=y_n+z_n.$$ If the field were complete, it would be easier. But I'm not sure how to construct $(y_n)_{n\in \mathbb{N}}$ and $(z_n)_{n\in \mathbb{N}}$ without an existing limit.","I'm trying to solve the following exercise. Let be an ordered field and be an Cauchy sequence in . Show that there exists an increasing Cauchy sequence and a decreasing Cauchy sequence with If the field were complete, it would be easier. But I'm not sure how to construct and without an existing limit.",K (x_n)_{n\in \mathbb{N}} K (y_n)_{n\in \mathbb{N}} (z_n)_{n\in \mathbb{N}} x_n=y_n+z_n. (y_n)_{n\in \mathbb{N}} (z_n)_{n\in \mathbb{N}},"['real-analysis', 'analysis', 'cauchy-sequences']"
39,How to solve $\int \frac{f(x)}{f^{\prime}(x)} dx$,How to solve,\int \frac{f(x)}{f^{\prime}(x)} dx,"Let $f:\mathbb{R}\to\mathbb{R}$ be of class $C^{\infty}$ with $f^{\prime}\not\equiv0$ . There exists a formula to solve the integral $$\int \frac{f(x)}{f^{\prime}(x)} dx?$$ Since I know that is $$\int \frac{f^{\prime}(x)}{f(x)} dx=\log(|f(x)|) +c,$$ I was wondering how to act when in the first case. I hope someone could help. Thank you in advance.",Let be of class with . There exists a formula to solve the integral Since I know that is I was wondering how to act when in the first case. I hope someone could help. Thank you in advance.,"f:\mathbb{R}\to\mathbb{R} C^{\infty} f^{\prime}\not\equiv0 \int \frac{f(x)}{f^{\prime}(x)} dx? \int \frac{f^{\prime}(x)}{f(x)} dx=\log(|f(x)|) +c,","['real-analysis', 'calculus', 'integration', 'analysis']"
40,Understanding Product-Sigma-Algebra,Understanding Product-Sigma-Algebra,,"I know the following defintion for a Product-Sigma-Algebra: Let be $ (\Omega_1,\mathcal{A}_1,\mu_1) $ and $ (\Omega_2,\mathcal{A}_2,\mu_2) $ two measure spaces. The Sigma-Algebra over $ \Omega_1\times \Omega_2 $ generated by the sets of the form $ A_1\times A_2 , A_i\in \mathcal{A}_i, i=1,2 $ is called Product-Sigma-Alegbra of $ \mathcal{A}_1 $ and $ \mathcal{A}_2 $ and is named by $ \mathcal{A}_1\otimes \mathcal{A}_2  $ such that $ \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right) $ . Why does contain $ \mathcal{A}_1\otimes \mathcal{A}_2  $ the set $ \mathcal{A}_1\times \mathcal{A}_2  $ and in which relation? Is it like "" $\in$ "" or "" $\subseteq$ ""? I tried to comprehend it by an example: $ \Omega_1=\{7\}, \mathcal{A}_1=\{\emptyset, \{7\}\}$ and $\Omega_2=\{1,2\},\mathcal{A}_2=\{\emptyset, \{1\},\{2\},\{1,2\}\} $ and $ \Omega_1\times \Omega_2=\{(7,1),(7,2)\} $ . Now I calculate $$\mathcal{A}_1\times \mathcal{A}_2=\{(\emptyset,\emptyset),(\emptyset,\{1\}),(\emptyset,\{2\}),(\emptyset,\{1,2\}),(\{7\},\emptyset),(\{7\},\{1\}),(\{7\},\{2\}),(\{7\},\{1,2\})\}$$ and $$ \begin{aligned}&\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\\[10pt]=&\{\emptyset\times \emptyset,\emptyset\times \{1\},\emptyset\times \{2\},\emptyset\times \{1,2\},\{7\}\times \emptyset, \{7\}\times \{1\}, \{7\}\times \{2\}, \{7\}\times \{1,2\}\}\\[10pt]=&\{\emptyset,\emptyset,\emptyset,\emptyset,\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}\\[10pt]=&\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}. \end{aligned} $$ Finally I get $$ \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right)=\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\} $$ but the elements in the set $ \mathcal{A}_1\times \mathcal{A}_2 $ are different. What went wrong?","I know the following defintion for a Product-Sigma-Algebra: Let be and two measure spaces. The Sigma-Algebra over generated by the sets of the form is called Product-Sigma-Alegbra of and and is named by such that . Why does contain the set and in which relation? Is it like "" "" or "" ""? I tried to comprehend it by an example: and and . Now I calculate and Finally I get but the elements in the set are different. What went wrong?"," (\Omega_1,\mathcal{A}_1,\mu_1)   (\Omega_2,\mathcal{A}_2,\mu_2)   \Omega_1\times \Omega_2   A_1\times A_2 , A_i\in \mathcal{A}_i, i=1,2   \mathcal{A}_1   \mathcal{A}_2   \mathcal{A}_1\otimes \mathcal{A}_2    \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right)   \mathcal{A}_1\otimes \mathcal{A}_2    \mathcal{A}_1\times \mathcal{A}_2   \in \subseteq  \Omega_1=\{7\}, \mathcal{A}_1=\{\emptyset, \{7\}\} \Omega_2=\{1,2\},\mathcal{A}_2=\{\emptyset, \{1\},\{2\},\{1,2\}\}   \Omega_1\times \Omega_2=\{(7,1),(7,2)\}  \mathcal{A}_1\times \mathcal{A}_2=\{(\emptyset,\emptyset),(\emptyset,\{1\}),(\emptyset,\{2\}),(\emptyset,\{1,2\}),(\{7\},\emptyset),(\{7\},\{1\}),(\{7\},\{2\}),(\{7\},\{1,2\})\}  \begin{aligned}&\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\\[10pt]=&\{\emptyset\times \emptyset,\emptyset\times \{1\},\emptyset\times \{2\},\emptyset\times \{1,2\},\{7\}\times \emptyset, \{7\}\times \{1\}, \{7\}\times \{2\}, \{7\}\times \{1,2\}\}\\[10pt]=&\{\emptyset,\emptyset,\emptyset,\emptyset,\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}\\[10pt]=&\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}. \end{aligned}   \mathcal{A}_1\otimes \mathcal{A}_2=\sigma\left(\{A_1 \times A_2:\ (A_1,A_2)\in \mathcal{A}_1\times \mathcal{A}_2\}\right)=\{\emptyset,\{(7,1)\},\{(7,2)\},\{(7,1),(7,2)\}\}   \mathcal{A}_1\times \mathcal{A}_2 ","['analysis', 'measure-theory']"
41,Connection between compactly supported smooth functions on a bounded domain and sobolev spaces,Connection between compactly supported smooth functions on a bounded domain and sobolev spaces,,"I am interested in finding a direct yes or no answer for the following: In general, on a bounded domain $\Omega$ in $R^n$ with say $C^1$ boundary, can we that say any function $f$ $\in$ $C_c^{\infty}$ ( $\Omega$ ), where $C_c^{\infty}$ ( $\Omega$ ) is the space of all compactly supported smooth functions defined on $\Omega$ , must also belong to a sobolev space $W^{k,p}(\Omega)$ , for $1 \le p < \infty$ ? I appreciate anyone's help, and enlightenment. If not can someone please give an example? Thanks, Sandy","I am interested in finding a direct yes or no answer for the following: In general, on a bounded domain in with say boundary, can we that say any function ( ), where ( ) is the space of all compactly supported smooth functions defined on , must also belong to a sobolev space , for ? I appreciate anyone's help, and enlightenment. If not can someone please give an example? Thanks, Sandy","\Omega R^n C^1 f \in C_c^{\infty} \Omega C_c^{\infty} \Omega \Omega W^{k,p}(\Omega) 1 \le p < \infty","['real-analysis', 'analysis', 'derivatives', 'sobolev-spaces', 'smooth-functions']"
42,Really dumb clarification on logarithm domain.,Really dumb clarification on logarithm domain.,,"I have the inequality $$\log(1+3x) \geq \log(4x-10)$$ Where $\log$ is meant in base $10$ .\ Now, checking the respective domains, I found the whole domain is $$\Omega: x > \frac{5}{2}$$ So I can exponentiate, comparing the two arguments. Yet I was wondering: if I apply log property, moving to the left all the terms, then I have $$\log\left(\dfrac{1+3x}{4x-10}\right) \geq 0$$ But now the domain changes, reading $$\Omega: x\in (-\infty, -1/3) \cup (5/2, +\infty)$$ Which one is correct? I am confused on this thing.","I have the inequality Where is meant in base .\ Now, checking the respective domains, I found the whole domain is So I can exponentiate, comparing the two arguments. Yet I was wondering: if I apply log property, moving to the left all the terms, then I have But now the domain changes, reading Which one is correct? I am confused on this thing.","\log(1+3x) \geq \log(4x-10) \log 10 \Omega: x > \frac{5}{2} \log\left(\dfrac{1+3x}{4x-10}\right) \geq 0 \Omega: x\in (-\infty, -1/3) \cup (5/2, +\infty)","['calculus', 'analysis', 'logarithms']"
43,How to find the extreme values of the following function?,How to find the extreme values of the following function?,,"Function $$ f(x,y) = 3x^2 + 3xy + y^2 + y^3 $$ My Solution $$f_x = 6x + 5y + 3y^2 = 0$$ $$f_y = 3x + 2y +3y^2 = 0$$ Solve system of equations to find y=-x, substitute back into $f_x$ and $f_y$ to find (x,y) = (0,0) or/and (-1/3 , 1/3). $$D = f_{xx} f_{yy} - {f_{xy}}^2$$ This gives that (0,0) is local minimum and (-1/3 , 1/3) is a saddle point. BUT, the actual solution to this question says that (0,0) is the only extreme point, so what happened to (-1/3 , 1/3) ?? Thank you for the help! :)","Function My Solution Solve system of equations to find y=-x, substitute back into and to find (x,y) = (0,0) or/and (-1/3 , 1/3). This gives that (0,0) is local minimum and (-1/3 , 1/3) is a saddle point. BUT, the actual solution to this question says that (0,0) is the only extreme point, so what happened to (-1/3 , 1/3) ?? Thank you for the help! :)"," f(x,y) = 3x^2 + 3xy + y^2 + y^3  f_x = 6x + 5y + 3y^2 = 0 f_y = 3x + 2y +3y^2 = 0 f_x f_y D = f_{xx} f_{yy} - {f_{xy}}^2","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'maxima-minima']"
44,Proof of an upper bound of the product of binomial coefficients on a same row of Pascal triangle [duplicate],Proof of an upper bound of the product of binomial coefficients on a same row of Pascal triangle [duplicate],,"This question already has an answer here : A product inequality with binomial coefficients [duplicate] (1 answer) Closed 1 year ago . We have to prove the following inequality: $$\forall n \in \mathbb{N}, n \ge 2, \ \ \Pi_{k=0}^n \binom{n}{k} \le \left(\frac{2^n-2}{n-1}\right)^{n-1}$$ the progress where I am at, is visible in the following picture: I'm especially confused, since the term (2*((n-1)/n)) has the limit zero when calculated to infinity and the other term diverges to infinity when calculated there. so the first question would be, if these two terms are multiplied, which limit comes out as a result? the second question would be simply how to continue on from there, or if you know a better way to approach this inequality proof. I am very thankful for your helpful answers.","This question already has an answer here : A product inequality with binomial coefficients [duplicate] (1 answer) Closed 1 year ago . We have to prove the following inequality: the progress where I am at, is visible in the following picture: I'm especially confused, since the term (2*((n-1)/n)) has the limit zero when calculated to infinity and the other term diverges to infinity when calculated there. so the first question would be, if these two terms are multiplied, which limit comes out as a result? the second question would be simply how to continue on from there, or if you know a better way to approach this inequality proof. I am very thankful for your helpful answers.","\forall n \in \mathbb{N}, n \ge 2, \ \ \Pi_{k=0}^n \binom{n}{k} \le \left(\frac{2^n-2}{n-1}\right)^{n-1}","['analysis', 'inequality', 'solution-verification', 'induction']"
45,evaluation of a definite integral involving inverse hyperbolic functions,evaluation of a definite integral involving inverse hyperbolic functions,,The following integral appears during the analysis of statistical mechanical models. $$\int\limits_0^{\pi /2}{\operatorname{arctanh} \left[ {\sqrt {1 - \sin (x)\sin (x){a^2}} } \right]dx} $$ Is there a closed form solution for the integral?,The following integral appears during the analysis of statistical mechanical models. Is there a closed form solution for the integral?,\int\limits_0^{\pi /2}{\operatorname{arctanh} \left[ {\sqrt {1 - \sin (x)\sin (x){a^2}} } \right]dx} ,"['calculus', 'integration', 'analysis', 'improper-integrals', 'approximation']"
46,Question about the definition of the outer measure,Question about the definition of the outer measure,,"From Rudin's Principles of Mathematical Analysis , we define the outer measure as so: Definition $11.7$ : Let $\mu$ be additive, regular, nonnegative, and finite on $\mathcal{E}$ . Consider countable coverings of any set $E \subset \mathbb{R}^n$ by open elementary sets $A_n$ : $$E \subseteq \bigcup_{n=1}^\infty A_n.$$ Define $$\mu^\ast(E) = \inf \sum_{n=1}^\infty \mu(A_n),$$ the infimum being taken over all countable coverings of $E$ by open elementary sets. $\mu^\ast(E)$ is called the outer measure of $E$ , corresponding to $\mu$ . I apologize beforehand as I know there are a multitude of questions regarding the outer-measure, but I have a specific issue that makes the definition a little unclear to me and I hope someone can help me understand it better. I posted the definition from baby Rudin above for reference. Now my issue is this, it appears we did not put any restrictions on the coverings of $E$ . I am not talking about openness or countability. Rather, the way I am reading the definition is this: As long as $E \subset \bigcup_{n=1}^{\infty} A_n$ , then its outer measure is $$\mu^{*}(E) = \inf \left\{\sum_{n=1}^{\infty} \mu(A_n) \right\} $$ So basically, you can give me a finite cover for a finite compact set $E$ , but then add $\mathbb{R}$ to the union of its covers $\bigcup_{n=1}^{N} A_n$ , $(N < \infty)$ , which is true, then $$\mu^{*}(E) = \inf \left\{\sum_{n=1}^{N} \mu(A_n) + \mu\mathbb{R} \right\} = inf\{\infty \} = \infty$$","From Rudin's Principles of Mathematical Analysis , we define the outer measure as so: Definition : Let be additive, regular, nonnegative, and finite on . Consider countable coverings of any set by open elementary sets : Define the infimum being taken over all countable coverings of by open elementary sets. is called the outer measure of , corresponding to . I apologize beforehand as I know there are a multitude of questions regarding the outer-measure, but I have a specific issue that makes the definition a little unclear to me and I hope someone can help me understand it better. I posted the definition from baby Rudin above for reference. Now my issue is this, it appears we did not put any restrictions on the coverings of . I am not talking about openness or countability. Rather, the way I am reading the definition is this: As long as , then its outer measure is So basically, you can give me a finite cover for a finite compact set , but then add to the union of its covers , , which is true, then","11.7 \mu \mathcal{E} E \subset \mathbb{R}^n A_n E \subseteq \bigcup_{n=1}^\infty A_n. \mu^\ast(E) = \inf \sum_{n=1}^\infty \mu(A_n), E \mu^\ast(E) E \mu E E \subset \bigcup_{n=1}^{\infty} A_n \mu^{*}(E) = \inf \left\{\sum_{n=1}^{\infty} \mu(A_n) \right\}  E \mathbb{R} \bigcup_{n=1}^{N} A_n (N < \infty) \mu^{*}(E) = \inf \left\{\sum_{n=1}^{N} \mu(A_n) + \mu\mathbb{R} \right\} = inf\{\infty \} = \infty","['real-analysis', 'analysis']"
47,Show the special unitary group is connected,Show the special unitary group is connected,,"Let $SU(n)$ be the group of $n \times n $ special unitary complex matrices. Show that $SU(n)$ is connected. Here are some thoughts: By definition $SU(n) = \{ A \in GL(n): A^*A=I=AA^*, det (A)=I \}$ . I know that $det: SU(n) \rightarrow \{1\}$ is a continuous surjective map. I also know that continuous image of a connected set is connected. But in this case it's not really helping. I showed that the group $SU(n)$ is compact. But from here I'm not sure how to proceed to show connectedness. Any help will be appreciated!",Let be the group of special unitary complex matrices. Show that is connected. Here are some thoughts: By definition . I know that is a continuous surjective map. I also know that continuous image of a connected set is connected. But in this case it's not really helping. I showed that the group is compact. But from here I'm not sure how to proceed to show connectedness. Any help will be appreciated!,"SU(n) n \times n  SU(n) SU(n) = \{ A \in GL(n): A^*A=I=AA^*, det (A)=I \} det: SU(n) \rightarrow \{1\} SU(n)","['real-analysis', 'group-theory', 'analysis']"
48,"If $g$ is continuous and $A$ is a closed set and $A\subset \mathbb{R}$, then $g^{-1}(A)$ is closed","If  is continuous and  is a closed set and , then  is closed",g A A\subset \mathbb{R} g^{-1}(A),"Definition: Let $g$ be defined on all of $\mathbb{R}$ . If $A$ is a subset of $\mathbb{R}$ , define $$g^{-1}(A)=\{ x\in\mathbb{R}:~ g(x)\in A\}$$ True or false: If $g$ is continuous and $A$ is a closed set and $A\subset \mathbb{R}$ , then $g^{-1}(A)$ is closed. This problem is found from Understanding Analysis (Ex.4.4.12(d), by Abbott, Stephen). I found the answer from internet showing this statement is True. But can I use the following counter-example? $$\begin{align} g(x)&=3x-1,~x\in[1/3,~2/3]\\ \\ g(x)&=1, ~~~~~~~~~~x\in (2/3,1)\\ \\ g(x)&=0,~~~~~~~~~~x\in(0,1/3) \end{align}$$ So $A=[0,1]$ is closed. But $g^{-1}(A)=(0,1)$ is not closed.","Definition: Let be defined on all of . If is a subset of , define True or false: If is continuous and is a closed set and , then is closed. This problem is found from Understanding Analysis (Ex.4.4.12(d), by Abbott, Stephen). I found the answer from internet showing this statement is True. But can I use the following counter-example? So is closed. But is not closed.","g \mathbb{R} A \mathbb{R} g^{-1}(A)=\{ x\in\mathbb{R}:~ g(x)\in A\} g A A\subset \mathbb{R} g^{-1}(A) \begin{align} g(x)&=3x-1,~x\in[1/3,~2/3]\\
\\
g(x)&=1, ~~~~~~~~~~x\in (2/3,1)\\
\\
g(x)&=0,~~~~~~~~~~x\in(0,1/3)
\end{align} A=[0,1] g^{-1}(A)=(0,1)","['real-analysis', 'analysis']"
49,Fourier transform is continuous using $\epsilon$-$\delta$ definition of continuity,Fourier transform is continuous using - definition of continuity,\epsilon \delta,"Let $f\in L^1(\mathbb{R}^n)$ . Then the Fourier transform $\hat{f}$ is given by $$\hat{f}(\xi):=\int_{\mathbb{R}^n} e^{-2\pi i x\cdot \xi} f(x) dx.$$ The dominated convergence theorem confirms that the Fourier transform is a continuous function. Indeed, if the sequence $\xi_n\rightarrow \xi_0$ then $\hat{f}(\xi_n)\rightarrow\hat{f}(\xi_0)$ since, for every $n\geq 1$ , $|e^{-2\pi i x\cdot \xi_n} f(x)|\leq |f(x)|\in L^1(\mathbb{R}^n)$ . I want to prove that $\hat{f}$ is continuous using the $\epsilon$ - $\delta$ definition. My attempt: Fix $\xi_0$ in $\mathbb{R}^n$ and let $\epsilon>0$ . We need to prove that there exists $\delta>0$ such that $$|\xi-\xi_0|<\delta \implies |\hat{f}(\xi)-\hat{f}(\xi_0)|<\epsilon.$$ We have $$\hat{f}(\xi)-\hat{f}(\xi_0)=\int_{\mathbb{R}^n} (e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}) f(x) dx.$$ Define $$g(t):=e^{-2\pi i x\cdot ((1-t)\xi+t\xi_0)}.$$ Then $g$ is a smooth function. By the mean-value theorem $$e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}=g(1)-g(0)= g^{\prime}(s)= 2\pi i e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x\cdot (\xi-\xi_0)$$ for some $s\in (0,1)$ . Therefore $$\hat{f}(\xi)-\hat{f}(\xi_0)=2\pi i (\xi-\xi_0)\cdot\int_{\mathbb{R}^n} e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x f(x) dx.$$ Now, if $f$ has compact support, or decay faster than $1/|x|$ as $|x|\rightarrow \infty$ we are done since, in that case, $$|\hat{f}(\xi)-\hat{f}(\xi_0)|\leq C |\xi-\xi_0|$$ with $C=2\pi \int_{\mathbb{R}^n} |x| |f(x)| dx$ . If $f$ is just an $L^1$ function we must use the cancellation due to the oscillatory exponential factor (may be using Riemann-Lebesgue lemma).","Let . Then the Fourier transform is given by The dominated convergence theorem confirms that the Fourier transform is a continuous function. Indeed, if the sequence then since, for every , . I want to prove that is continuous using the - definition. My attempt: Fix in and let . We need to prove that there exists such that We have Define Then is a smooth function. By the mean-value theorem for some . Therefore Now, if has compact support, or decay faster than as we are done since, in that case, with . If is just an function we must use the cancellation due to the oscillatory exponential factor (may be using Riemann-Lebesgue lemma).","f\in L^1(\mathbb{R}^n) \hat{f} \hat{f}(\xi):=\int_{\mathbb{R}^n} e^{-2\pi i x\cdot \xi} f(x) dx. \xi_n\rightarrow \xi_0 \hat{f}(\xi_n)\rightarrow\hat{f}(\xi_0) n\geq 1 |e^{-2\pi i x\cdot \xi_n} f(x)|\leq |f(x)|\in L^1(\mathbb{R}^n) \hat{f} \epsilon \delta \xi_0 \mathbb{R}^n \epsilon>0 \delta>0 |\xi-\xi_0|<\delta \implies |\hat{f}(\xi)-\hat{f}(\xi_0)|<\epsilon. \hat{f}(\xi)-\hat{f}(\xi_0)=\int_{\mathbb{R}^n} (e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}) f(x) dx. g(t):=e^{-2\pi i x\cdot ((1-t)\xi+t\xi_0)}. g e^{-2\pi i x\cdot \xi}-e^{-2\pi i x\cdot \xi_0}=g(1)-g(0)= g^{\prime}(s)=
2\pi i e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x\cdot (\xi-\xi_0) s\in (0,1) \hat{f}(\xi)-\hat{f}(\xi_0)=2\pi i (\xi-\xi_0)\cdot\int_{\mathbb{R}^n} e^{-2\pi i x\cdot ((1-s)\xi+s\xi_0)}x f(x) dx. f 1/|x| |x|\rightarrow \infty |\hat{f}(\xi)-\hat{f}(\xi_0)|\leq C |\xi-\xi_0| C=2\pi \int_{\mathbb{R}^n} |x| |f(x)| dx f L^1","['real-analysis', 'analysis', 'complex-numbers', 'epsilon-delta']"
50,"$\lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0$ if $I_s(\mu) < \infty$",if,"\lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0 I_s(\mu) < \infty","$\mathcal M(\Bbb R^n)$ denotes the set of all Borel measures $\mu$ on $\Bbb R^n$ with $0 < \mu(\Bbb R^n) < \infty$ and compact support $\operatorname{spt}\mu \subset \Bbb R^n$ . Suppose $\mu\in \mathcal M(\Bbb R^n)$ and $I_s(\mu) = \gamma(n,s)\int |\hat\mu(x)|^2 |x|^{s-n}\, dx < \infty$ . Then, $$|\hat\mu(x)|\le |x|^{-s/2}$$ for 'most' $x$ with large norm. Here, 'most' simply means what is needed in order that the above integral would be finite. For example, we must have $$\lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0$$ I am trying to see why $\lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0$ in this case. By definition, $$\lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0$$ means that for all $\epsilon > 0$ , there exists $R_\epsilon > 0$ such that $$R\ge R_\epsilon \implies |R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) | < \epsilon$$ Suppose the negation is true, i.e., there exists $\epsilon > 0$ such that for every $R > 0$ , there exists $r \ge R$ such that $$\mathcal L^n(\{x\in B(0,r): |\hat\mu(x)| > |x|^{-s/2}\}) \ge \epsilon r^n$$ which implies $$\mathcal L^n(\{x\in B(0,r): |\hat\mu(x)| > |x|^{-s/2}\}) \ge \epsilon R^n$$ How should I proceed? Thank you! Note: $\mathcal L^n$ denotes the $n$ -dimensional Lebesgue measure. Reference: Fourier Analysis and Hausdorff Dimension by Pertti Mattila.","denotes the set of all Borel measures on with and compact support . Suppose and . Then, for 'most' with large norm. Here, 'most' simply means what is needed in order that the above integral would be finite. For example, we must have I am trying to see why in this case. By definition, means that for all , there exists such that Suppose the negation is true, i.e., there exists such that for every , there exists such that which implies How should I proceed? Thank you! Note: denotes the -dimensional Lebesgue measure. Reference: Fourier Analysis and Hausdorff Dimension by Pertti Mattila.","\mathcal M(\Bbb R^n) \mu \Bbb R^n 0 < \mu(\Bbb R^n) < \infty \operatorname{spt}\mu \subset \Bbb R^n \mu\in \mathcal M(\Bbb R^n) I_s(\mu) = \gamma(n,s)\int |\hat\mu(x)|^2 |x|^{s-n}\, dx < \infty |\hat\mu(x)|\le |x|^{-s/2} x \lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0 \lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0 \lim_{R\to\infty} R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) = 0 \epsilon > 0 R_\epsilon > 0 R\ge R_\epsilon \implies |R^{-n} \mathcal L^n(\{x\in B(0,R): |\hat\mu(x)| > |x|^{-s/2}\}) | < \epsilon \epsilon > 0 R > 0 r \ge R \mathcal L^n(\{x\in B(0,r): |\hat\mu(x)| > |x|^{-s/2}\}) \ge \epsilon r^n \mathcal L^n(\{x\in B(0,r): |\hat\mu(x)| > |x|^{-s/2}\}) \ge \epsilon R^n \mathcal L^n n","['analysis', 'measure-theory', 'fourier-analysis', 'fourier-transform', 'dimension-theory-analysis']"
51,"Analytic formula for $f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds$",Analytic formula for,"f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds","For any $\alpha \in (0,1)$ , define $\displaystyle f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds$ . Question. Is there an analytic formula for $f(\alpha)$ , say, in terms of special functions ? Note. I'm also fine with good upper-bounds for $f(\alpha)$ in terms of simpler expressions in $\alpha$ .","For any , define . Question. Is there an analytic formula for , say, in terms of special functions ? Note. I'm also fine with good upper-bounds for in terms of simpler expressions in .","\alpha \in (0,1) \displaystyle f(\alpha):= \int_{-\infty}^\infty\frac{|e^{is}-1|}{|s|^{2\alpha+1}}\,ds f(\alpha) f(\alpha) \alpha","['real-analysis', 'analysis', 'fourier-analysis', 'special-functions']"
52,L^2 function also bounded almost everywhere?,L^2 function also bounded almost everywhere?,,"Let $B$ be a bounded subset of $\mathbb{R}^n$ and suppose we have $f\in L^2(B;\mathbb{R})$ such that $$||f||^2=\int\limits_B(f(x))^2dx < \infty.$$ How can I show that $f$ is pointwise bounded almost everywhere, i.e. there exists $a,b\in \mathbb{R}$ such that $a\leq f(x)\leq b$ for almost all $x\in B$ ?","Let be a bounded subset of and suppose we have such that How can I show that is pointwise bounded almost everywhere, i.e. there exists such that for almost all ?","B \mathbb{R}^n f\in L^2(B;\mathbb{R}) ||f||^2=\int\limits_B(f(x))^2dx < \infty. f a,b\in \mathbb{R} a\leq f(x)\leq b x\in B","['analysis', 'measure-theory']"
53,Why this Taylor expansion of $ \dfrac{1}{1+\cos x}$ is wrong?,Why this Taylor expansion of  is wrong?, \dfrac{1}{1+\cos x},"I want to expand the function $$ \dfrac{1}{1+\cos x} $$ at $x = 0$ The way I did this is first regard $\cos x $ as a whole and denote it by $y$ The expansion will become $$ \dfrac{1}{1+y}=1-y+y^2+o(y^2)$$ and then return $y=\cos x$ back and I want to expand each $\cos x$ at $ x=0$ Since $$ \cos x = 1-\dfrac{x^2}{2} + o(x^2) $$ $ o(y^2) = o(\big(1-\dfrac{x^2}{2} + o(x^2)\big)^2)=o(x^2)$ and the result is $$ \dfrac{1}{1+\cos x}=1-1+\dfrac{x^2}{2}+1-x^2+o(x^2)=1-\dfrac{x^2}{2}+o(x^2)$$ but this is wrong Thus, my question is why this is wrong. Also by the same way, I can correctly get the expansion of $ \tan(\tan x)$ and $\sin(\sin x)$ For example, $ \tan(\tan x)$ . I want to expand this at $ x=0$ I regard the inner $\tan x$ as $y$ , so the expansion will be $$ \tan(y)=y+\dfrac{1}{3}y^3+o(y^3)$$ Then by some calculations, I can get $ o(y^3)= o(x^3),\ \ y=\tan x=x+\dfrac{1}{3}x^3+o(x^3), \ \ y^3=\tan^3 x=(x+o(x))^3 =x^3+o(x^3)$ Then the result is $$ x+\dfrac{2}{3}x^3+o(x^3)$$ which is right","I want to expand the function at The way I did this is first regard as a whole and denote it by The expansion will become and then return back and I want to expand each at Since and the result is but this is wrong Thus, my question is why this is wrong. Also by the same way, I can correctly get the expansion of and For example, . I want to expand this at I regard the inner as , so the expansion will be Then by some calculations, I can get Then the result is which is right"," \dfrac{1}{1+\cos x}  x = 0 \cos x  y  \dfrac{1}{1+y}=1-y+y^2+o(y^2) y=\cos x \cos x  x=0  \cos x = 1-\dfrac{x^2}{2} + o(x^2)   o(y^2) = o(\big(1-\dfrac{x^2}{2} + o(x^2)\big)^2)=o(x^2)  \dfrac{1}{1+\cos x}=1-1+\dfrac{x^2}{2}+1-x^2+o(x^2)=1-\dfrac{x^2}{2}+o(x^2)  \tan(\tan x) \sin(\sin x)  \tan(\tan x)  x=0 \tan x y  \tan(y)=y+\dfrac{1}{3}y^3+o(y^3)  o(y^3)= o(x^3),\ \ y=\tan x=x+\dfrac{1}{3}x^3+o(x^3), \ \ y^3=\tan^3 x=(x+o(x))^3 =x^3+o(x^3)  x+\dfrac{2}{3}x^3+o(x^3)","['analysis', 'taylor-expansion']"
54,"Proving a bounded function is integrable on $[a,b]$",Proving a bounded function is integrable on,"[a,b]","I need to prove that Let $f:[a,b] \to \mathbf{R}$ be a bounded map and let $f$ be an integrable map on the interval $[c,b]$ for all $c \in (a,b).$ Then, $f$ is integrable on $[a,b].$ My attempt: By hypotesis, $f:[a,b] \to \mathbf{R}$ is bounded, so there exists $0 < K \in \mathbf{R}$ such that $|f(x)| \leq K$ , for all $x \in [a,b].$ Let $\varepsilon>0,$ and take $c \in (a,b)$ such that $K \cdot (c-a) \leq \frac{\varepsilon}{4}$ . By hypotesis, $f$ is integrable on $[c,b],$ then, there exists a partition $\left\{t_1, \dots, t_n \right\} \subset [a,b]$ such that $\sum_{i=2}^{n} \omega_i(t_i-t_{i-1}) < \frac{\varepsilon}{2}$ . Let's make $t_{1}=a$ , then we get  a partition $\left\{t_0, t_1, \dots, t_n \right\}$ of $[a,b]$ . Now I need to prove that $\omega_0 \leq 2K$ , for me to get that $f$ is integrable on $[a,b]$ . Am I following the correct idea? I don't know how to procceed. Any ideas would be appreciated!","I need to prove that Let be a bounded map and let be an integrable map on the interval for all Then, is integrable on My attempt: By hypotesis, is bounded, so there exists such that , for all Let and take such that . By hypotesis, is integrable on then, there exists a partition such that . Let's make , then we get  a partition of . Now I need to prove that , for me to get that is integrable on . Am I following the correct idea? I don't know how to procceed. Any ideas would be appreciated!","f:[a,b] \to \mathbf{R} f [c,b] c \in (a,b). f [a,b]. f:[a,b] \to \mathbf{R} 0 < K \in \mathbf{R} |f(x)| \leq K x \in [a,b]. \varepsilon>0, c \in (a,b) K \cdot (c-a) \leq \frac{\varepsilon}{4} f [c,b], \left\{t_1, \dots, t_n \right\} \subset [a,b] \sum_{i=2}^{n} \omega_i(t_i-t_{i-1}) < \frac{\varepsilon}{2} t_{1}=a \left\{t_0, t_1, \dots, t_n \right\} [a,b] \omega_0 \leq 2K f [a,b]","['real-analysis', 'calculus', 'integration', 'analysis', 'definite-integrals']"
55,Is the semicubical parabola differentiable?,Is the semicubical parabola differentiable?,,"Let $\alpha(t)=(t^2,t^3)$ for $t \in \mathbb{R}$ . The image of the curve looks like the semicubical parabola. I wanted to know if $\alpha$ is differentiable. The component functions are both differentiable, with $\alpha'(t)=(2t,3t^2)$ But looking at its image, it should not be differentiable in $t=0$ . I am a little bit confused, because as far as I know, if $f :[a,b] \rightarrow \mathbb{R}^n$ . Then f is differentiable in $x_0$ if every component function $f_1,...,f_n$ is differentiable in $x_0$ . I did some thinking/calculating and on one hand $\alpha$ should be differnetiable, but on the other hand the curve $\beta(t)=(x,x^{2/3})$ has the same image whilst not being differentiable in $0$ . Is $\alpha$ differentiable in $0$ (and why/why not)? Are there ""representation functions"" of a image that are not differentiable and other representations that are? Edit: I meant, the graph of the function $\beta(t)=x^{2/3}$ is the same as the image of $\alpha$ .","Let for . The image of the curve looks like the semicubical parabola. I wanted to know if is differentiable. The component functions are both differentiable, with But looking at its image, it should not be differentiable in . I am a little bit confused, because as far as I know, if . Then f is differentiable in if every component function is differentiable in . I did some thinking/calculating and on one hand should be differnetiable, but on the other hand the curve has the same image whilst not being differentiable in . Is differentiable in (and why/why not)? Are there ""representation functions"" of a image that are not differentiable and other representations that are? Edit: I meant, the graph of the function is the same as the image of .","\alpha(t)=(t^2,t^3) t \in \mathbb{R} \alpha \alpha'(t)=(2t,3t^2) t=0 f :[a,b] \rightarrow \mathbb{R}^n x_0 f_1,...,f_n x_0 \alpha \beta(t)=(x,x^{2/3}) 0 \alpha 0 \beta(t)=x^{2/3} \alpha","['analysis', 'differential-geometry']"
56,Calculating integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$,Calculating integrals of the form,\int\frac{dx}{(1+x^{q})^\frac{p}{r}},"I am trying to understand how to calculate integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$ , where $p,q,r \in  \Bbb Z$ . I know how to calculate the integral $\int \frac{dx}{(1+x^2)^\frac{1}{2}}$ . However I cannot do it if $\int \frac{dx}{(1+x^3)^\frac{1}{2}}$ or $\int \frac{dx}{(1+x^5)^\frac{1}{2}}$ . Is there any method of evaluating closed from expressions of integrals of the form $\int\frac{dx}{(1+x^{q})^\frac{p}{r}}$ ? I have tried U-subbing using $x=z^{r/q} -1$ ,but it doesn't work.","I am trying to understand how to calculate integrals of the form , where . I know how to calculate the integral . However I cannot do it if or . Is there any method of evaluating closed from expressions of integrals of the form ? I have tried U-subbing using ,but it doesn't work.","\int\frac{dx}{(1+x^{q})^\frac{p}{r}} p,q,r \in  \Bbb Z \int \frac{dx}{(1+x^2)^\frac{1}{2}} \int \frac{dx}{(1+x^3)^\frac{1}{2}} \int \frac{dx}{(1+x^5)^\frac{1}{2}} \int\frac{dx}{(1+x^{q})^\frac{p}{r}} x=z^{r/q} -1","['calculus', 'analysis', 'indefinite-integrals']"
57,"Problem $u_t - u_{xx} = e^t$ in $\mathbb{R} \times (0,+\infty)$",Problem  in,"u_t - u_{xx} = e^t \mathbb{R} \times (0,+\infty)","Well, I did some calculations but I'm in doubt about this problem: $$\left\{\begin{array}{rcl} u_t - u_{xx} & = & e^t, \ \mathbb{R} \times (0,+\infty)\\ u(x,0) & = & \cos(3x), \ \mathbb{R}. \end{array}\right.$$ First, note that $$\mathcal{F}(u_t) = \mathcal{F}(u_{xx} + e^t) = -\xi^2\mathcal{F}(u) + \mathcal{F}(e^t) = -\xi^2\mathcal{F}(u) + \sqrt{2\pi}e^t\delta(\xi),$$ where $\delta$ is the Dirac delta. In addition, we also have $$\mathcal{F}(\cos(3x)) = \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)].$$ Therefore, the ODE system follows: $$\left\{\begin{array}{rcl} \widehat{u_t}(\xi,t) & = & -\xi^2\widehat{u}(\xi,t) + \sqrt{2\pi}e^t\delta(\xi)\\ \widehat{u}(\xi,0) & = & \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)]. \end{array}\right.$$ For each $\xi \in \mathbb{R}$ , we can explicitly solve the ODE by the integral factor method, obtaining: $$\widehat{u}(\xi,t) = \dfrac{\sqrt{2\pi}\delta(\xi)}{1 + \xi^2} e^{t} + Ke^{-\xi^2 t}$$ where $K$ is a constant. From the initial condition, it follows that $$K = \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)] - \dfrac{\sqrt{2\pi}\delta(\xi)}{1 + \xi^2}.$$ Therefore, applying the inverse Fourier transform, it follows that: $$u(x,t) = e^t + \cos(3x) - 1.$$ Doubt: I don't know if I'm faltering on some calculation, but this solution doesn't seem to satisfy me $u_t - u_xx = e^t$ . Was there a wrong passage? Can you help me?","Well, I did some calculations but I'm in doubt about this problem: First, note that where is the Dirac delta. In addition, we also have Therefore, the ODE system follows: For each , we can explicitly solve the ODE by the integral factor method, obtaining: where is a constant. From the initial condition, it follows that Therefore, applying the inverse Fourier transform, it follows that: Doubt: I don't know if I'm faltering on some calculation, but this solution doesn't seem to satisfy me . Was there a wrong passage? Can you help me?","\left\{\begin{array}{rcl}
u_t - u_{xx} & = & e^t, \ \mathbb{R} \times (0,+\infty)\\
u(x,0) & = & \cos(3x), \ \mathbb{R}.
\end{array}\right. \mathcal{F}(u_t) = \mathcal{F}(u_{xx} + e^t) = -\xi^2\mathcal{F}(u) + \mathcal{F}(e^t) = -\xi^2\mathcal{F}(u) + \sqrt{2\pi}e^t\delta(\xi), \delta \mathcal{F}(\cos(3x)) = \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)]. \left\{\begin{array}{rcl}
\widehat{u_t}(\xi,t) & = & -\xi^2\widehat{u}(\xi,t) + \sqrt{2\pi}e^t\delta(\xi)\\
\widehat{u}(\xi,0) & = & \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)].
\end{array}\right. \xi \in \mathbb{R} \widehat{u}(\xi,t) = \dfrac{\sqrt{2\pi}\delta(\xi)}{1 + \xi^2} e^{t} + Ke^{-\xi^2 t} K K = \sqrt{\dfrac{\pi}{2}}[\delta(\xi - 3) + \delta(\xi + 3)] - \dfrac{\sqrt{2\pi}\delta(\xi)}{1 + \xi^2}. u(x,t) = e^t + \cos(3x) - 1. u_t - u_xx = e^t","['analysis', 'partial-differential-equations', 'fourier-transform', 'heat-equation']"
58,"How to show the following inequality $_2F_1\left(5.5, 1, 5;-x^2\right)>0$?",How to show the following inequality ?,"_2F_1\left(5.5, 1, 5;-x^2\right)>0","Consider the function $_2F_1\left(5.5, 1, 5;-|x|^2\right)$ for $x\in \mathbb{R}^n.$ I want to show that this function is positive. I checked that it does not have any roots so can I conclude the inequality by using continuity in $x$ of the function $_2F_1\left(5.5, 1, 5;-|x|^2\right)$ ?",Consider the function for I want to show that this function is positive. I checked that it does not have any roots so can I conclude the inequality by using continuity in of the function ?,"_2F_1\left(5.5, 1, 5;-|x|^2\right) x\in \mathbb{R}^n. x _2F_1\left(5.5, 1, 5;-|x|^2\right)","['analysis', 'inequality', 'proof-writing', 'roots', 'hypergeometric-function']"
59,Infinite natural numbers?,Infinite natural numbers?,,"Only using the successor function $\nu$ and the other axioms, how do we guarantee that the ""next"" generated number is different from all the ""previous"" numbers (I am using quotations because we havent defined order yet)? Concrete example: Prove that 3 is different from all ""previous"" numbers. $$0\neq \nu(\nu(\nu(0)))$$ This is true by axoiom 4. Now we prove that $1\neq3$ $$\nu(0) \neq \nu(\nu(\nu(0)))$$ by injection axiom of $\nu$ we get $$ 0 \neq \nu(\nu(0))$$ And by axoiom 4, this is a true statement. Now we could prove $2\neq3$ but i think my point is clear. We just ""peel"" off $\nu$ until we arrive at some form of $0 \neq \nu(*)$ . I don't see how one would prove this generally, that every ""next"" generated number is different from all the  ""previous"" ones without really defining order. But in order to define order we need to guarantee that every ""next"" generated number is different from the previous ones hence we are looping around. I still gave it a go and came up with some kind of ""proof"" $$ 0 \neq \nu(m) \ , \ \forall m \in \mathbb{N}$$ This is guaranteed by axiom 4. And now by applying the $\nu$ ""as often as you want"" to both sides we allways obtain that both sides are not equal due to injection.","Only using the successor function and the other axioms, how do we guarantee that the ""next"" generated number is different from all the ""previous"" numbers (I am using quotations because we havent defined order yet)? Concrete example: Prove that 3 is different from all ""previous"" numbers. This is true by axoiom 4. Now we prove that by injection axiom of we get And by axoiom 4, this is a true statement. Now we could prove but i think my point is clear. We just ""peel"" off until we arrive at some form of . I don't see how one would prove this generally, that every ""next"" generated number is different from all the  ""previous"" ones without really defining order. But in order to define order we need to guarantee that every ""next"" generated number is different from the previous ones hence we are looping around. I still gave it a go and came up with some kind of ""proof"" This is guaranteed by axiom 4. And now by applying the ""as often as you want"" to both sides we allways obtain that both sides are not equal due to injection.","\nu 0\neq \nu(\nu(\nu(0))) 1\neq3 \nu(0) \neq \nu(\nu(\nu(0))) \nu  0 \neq \nu(\nu(0)) 2\neq3 \nu 0 \neq \nu(*)  0 \neq \nu(m) \ , \ \forall m \in \mathbb{N} \nu","['analysis', 'elementary-set-theory', 'logic', 'natural-numbers']"
60,Is the function $f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)}$ decreasing?,Is the function  decreasing?,f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)},"We know that the limit of $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ is $$1/e=0.3678794411714423215955237701614608674458111310317678345078368016$$ , see What is $\lim_{n\to \infty }\left(\sqrt[\leftroot{-2}\uproot{2}n+1]{(n+1)!}-\sqrt[\leftroot{-2}\uproot{2}n]{n!}\right)$? Use Wolfram we have that $$a_1=\sqrt[2]{2!}-\sqrt[1]{1!}=0.4142135623730950488016887242096980785696718753769480731766797379$$ $$a_2=\sqrt[3]{3!}-\sqrt[2]{2!}=0.4029070304590446100895230321175624238585385877642715983046545599$$ $$a_3=\sqrt[4]{4!}-\sqrt[3]{3!}=0.3962432465685035259263687905534027946796259019185992795523061114$$ $$a_4=\sqrt[5]{5!}-\sqrt[4]{4!}=0.3918072452967087075081863770479530310194707973034612596596597742$$ ,... an so on. From this data we can guess that $a_n$ is decreasing. More generally, let $$f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)},$$ where $\Gamma(x)$ is the Gamma function $$\Gamma(x)=\int_0^\infty e^{-t}t^x\dfrac{dt}{t}.$$ Plot the graph of $f(x)$ , we can see that $f$ decreases.  Can anyone prove this conclusion？","We know that the limit of is , see What is $\lim_{n\to \infty }\left(\sqrt[\leftroot{-2}\uproot{2}n+1]{(n+1)!}-\sqrt[\leftroot{-2}\uproot{2}n]{n!}\right)$? Use Wolfram we have that ,... an so on. From this data we can guess that is decreasing. More generally, let where is the Gamma function Plot the graph of , we can see that decreases.  Can anyone prove this conclusion？","a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!} 1/e=0.3678794411714423215955237701614608674458111310317678345078368016 a_1=\sqrt[2]{2!}-\sqrt[1]{1!}=0.4142135623730950488016887242096980785696718753769480731766797379 a_2=\sqrt[3]{3!}-\sqrt[2]{2!}=0.4029070304590446100895230321175624238585385877642715983046545599 a_3=\sqrt[4]{4!}-\sqrt[3]{3!}=0.3962432465685035259263687905534027946796259019185992795523061114 a_4=\sqrt[5]{5!}-\sqrt[4]{4!}=0.3918072452967087075081863770479530310194707973034612596596597742 a_n f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)}, \Gamma(x) \Gamma(x)=\int_0^\infty e^{-t}t^x\dfrac{dt}{t}. f(x) f","['real-analysis', 'analysis', 'inequality', 'gamma-function']"
61,Continuous function $f$ from Euclidean topology to discrete topology is constant.,Continuous function  from Euclidean topology to discrete topology is constant.,f,"let the standard metric $d(x,y)=|y-x|$ and $\rho_{disc}$ be the discrete metric defined by $${\displaystyle \rho_{disc} (x,y)={\begin{cases}1&{\mbox{if}}\ x\neq y,\\0&{\mbox{if}}\ x=y\end{cases}}}$$ Let $V=(\mathbb{R}, \rho_{disc}$ ) and $W=(\mathbb{R},d)$ be two metric spaces (that has $\mathbb{R}$ as underlying sets, but given by two different metrices as written). Show that a function $f:W\rightarrow V$ is continuous if and only if  it is constant using the supremum property (directly/indirectly) So far Have tried for a lot of time now but I just do not see why I have to use the supremum property and how. It makes absolutely no sense to me. The only thing that seems straight forward to me is that if $f$ is constant (but I do not even use  the supremum property here obviously) then it is continuous. The other implication does make sense to me using the supremum property.","let the standard metric and be the discrete metric defined by Let ) and be two metric spaces (that has as underlying sets, but given by two different metrices as written). Show that a function is continuous if and only if  it is constant using the supremum property (directly/indirectly) So far Have tried for a lot of time now but I just do not see why I have to use the supremum property and how. It makes absolutely no sense to me. The only thing that seems straight forward to me is that if is constant (but I do not even use  the supremum property here obviously) then it is continuous. The other implication does make sense to me using the supremum property.","d(x,y)=|y-x| \rho_{disc} {\displaystyle \rho_{disc} (x,y)={\begin{cases}1&{\mbox{if}}\ x\neq y,\\0&{\mbox{if}}\ x=y\end{cases}}} V=(\mathbb{R}, \rho_{disc} W=(\mathbb{R},d) \mathbb{R} f:W\rightarrow V f","['real-analysis', 'analysis']"
62,Fourier series derivatives and convergence,Fourier series derivatives and convergence,,"Let $f(x)=x\sin(x)$ , $x\in[-\pi,\pi$ Find the Fourier series of $g(x)=f'(x)=\sin(x)+x\cos(x)$ , $x\in(-\pi,\pi)$ and show that it converges pointwise to $g$ . Try I found the FS of the function $f(x)=x\sin(x)$ , $x\in[-\pi,\pi)$ (in the just previous problem) to $1-\frac{1}{4}\cos(x)+\sum_{n=2}^{\infty} \left (\frac{2(-1)^{n+1}}{n^2-1} \right) \cos(nx)$ and shown that it converges uniformly to $f$ . (or written as $$1-\frac{1}{2}\cos(x)-2\sum_{k=2}^{\infty}\frac{(-1)^k}{(k^2-1)}\cos(kx)$$ Is there a trick to finding the Fourier of the derivative of my function given that I have the Fourier for my function? How do I show pointwise convergence to $g$ ? *I know about convolutions as I suspect I need that in this problem. I think pointwise convergence is true if it is continuous and periodic.","Let , Find the Fourier series of , and show that it converges pointwise to . Try I found the FS of the function , (in the just previous problem) to and shown that it converges uniformly to . (or written as Is there a trick to finding the Fourier of the derivative of my function given that I have the Fourier for my function? How do I show pointwise convergence to ? *I know about convolutions as I suspect I need that in this problem. I think pointwise convergence is true if it is continuous and periodic.","f(x)=x\sin(x) x\in[-\pi,\pi g(x)=f'(x)=\sin(x)+x\cos(x) x\in(-\pi,\pi) g f(x)=x\sin(x) x\in[-\pi,\pi) 1-\frac{1}{4}\cos(x)+\sum_{n=2}^{\infty} \left (\frac{2(-1)^{n+1}}{n^2-1} \right) \cos(nx) f 1-\frac{1}{2}\cos(x)-2\sum_{k=2}^{\infty}\frac{(-1)^k}{(k^2-1)}\cos(kx) g","['real-analysis', 'analysis']"
63,References on piecewise linear functions,References on piecewise linear functions,,I've been looking for references on piecewise linear functions in analysis books but I can't find many. Some treat the subject a bit passing and not general. Do you know of any references that might be useful? Thanks in advance.,I've been looking for references on piecewise linear functions in analysis books but I can't find many. Some treat the subject a bit passing and not general. Do you know of any references that might be useful? Thanks in advance.,,"['real-analysis', 'analysis']"
64,How to write $f(x)=x^n+(1-x)^n$ in terms of $r=x(1-x)$,How to write  in terms of,f(x)=x^n+(1-x)^n r=x(1-x),"The functions $f_n(x) = x^n + (1-x)^n$ with $n\in\mathbb N$ obviously have $f_n(x)=f_n(1-x)$ . Thus we can write $f_n(x)=\sum_k a_{n,k} r^k$ with $r=x\cdot(1-x)$ . Is there a nice way to determine the $a_{n,k}$ ?",The functions with obviously have . Thus we can write with . Is there a nice way to determine the ?,"f_n(x) = x^n + (1-x)^n n\in\mathbb N f_n(x)=f_n(1-x) f_n(x)=\sum_k a_{n,k} r^k r=x\cdot(1-x) a_{n,k}","['analysis', 'polynomials', 'symmetry']"
65,Differentiation to get $F'(0)$,Differentiation to get,F'(0),"I have $$f(x)=\begin{cases}        \sin\left(\frac{1}{x}\right) &\text{ if } x\neq 0, \\       0 & \text{ if }x=0.     \end{cases}$$ and define $F(x)=\int_0^x f(t) dt$ . I want to show that $F'(0)=f(0)$ . My idea is $$F'(0)=\lim_{h\to 0}\frac{F(h)-F(0)}{h}=\lim_{h\to 0}\frac{\int_0^h \sin\left(\frac{1}{x}\right) dx}{h}$$ and using a change of variables $t=\frac{1}{x}$ we have $$F'(0)=\lim_{h\to 0}\frac{\int_{\frac{1}{h}}^\infty\frac{\sin t}{t^2} dt}{h}.$$ I am not sure how to get this equal to $0$ now? Any help would be much appreciated.",I have and define . I want to show that . My idea is and using a change of variables we have I am not sure how to get this equal to now? Any help would be much appreciated.,"f(x)=\begin{cases} 
      \sin\left(\frac{1}{x}\right) &\text{ if } x\neq 0, \\
      0 & \text{ if }x=0. 
   \end{cases} F(x)=\int_0^x f(t) dt F'(0)=f(0) F'(0)=\lim_{h\to 0}\frac{F(h)-F(0)}{h}=\lim_{h\to 0}\frac{\int_0^h \sin\left(\frac{1}{x}\right) dx}{h} t=\frac{1}{x} F'(0)=\lim_{h\to 0}\frac{\int_{\frac{1}{h}}^\infty\frac{\sin t}{t^2} dt}{h}. 0","['real-analysis', 'calculus', 'analysis', 'derivatives']"
66,Convergence of integral $\int_{17}^\infty \frac{(\ln{\ln{x})}\sin{(e^{ax}})}{x+e}$,Convergence of integral,\int_{17}^\infty \frac{(\ln{\ln{x})}\sin{(e^{ax}})}{x+e},"For which $a\in\mathbb{R}$ does the integral $\int_{17}^\infty \frac{(\ln{\ln{x})}\sin{(e^{ax}})}{x+e}$ converge? I tried with substitution $t=e^{ax}$ , but don't know how to do the rest? Any help is welcome. Thanks in advance","For which does the integral converge? I tried with substitution , but don't know how to do the rest? Any help is welcome. Thanks in advance",a\in\mathbb{R} \int_{17}^\infty \frac{(\ln{\ln{x})}\sin{(e^{ax}})}{x+e} t=e^{ax},"['integration', 'analysis', 'definite-integrals']"
67,"Prob. 48, Chap. 1, in Royden's REAL ANALYSIS: At what points is this function continuous?","Prob. 48, Chap. 1, in Royden's REAL ANALYSIS: At what points is this function continuous?",,"Here is Prob. 48, Chap. 1, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Define the real-valued function $f$ on $\mathbb{R}$ by setting $$ f(x) = \begin{cases} x & \mbox{ if $x$ is irrational} \\ p \sin \frac{1}{q} & \mbox{ if $x = \frac{p}{q}$ is in lowest terms}. \end{cases} $$ At what points is $f$ continuous? I think the sequential criterion for continuity is what we will have to apply here. The function is not continuous at $x = 0 = \frac{0}{1}$ , because $f(0) = 0$ , but if we consider the sequence $\left( \frac{1}{2^n} \right)_{n \in \mathbb{N}}$ converging to $0$ , then the image sequence $\left( \sin \frac{1}{2^n} \right)_{n \in \mathbb{N} }$ does not converge to $f(0)$ . Am I right? How to check if $f$ is or is not continuous at a point $a \neq 0$ ?","Here is Prob. 48, Chap. 1, in the book Real Analysis by H.L. Royden and P.M. Fitzpatrick, 4th edition: Define the real-valued function on by setting At what points is continuous? I think the sequential criterion for continuity is what we will have to apply here. The function is not continuous at , because , but if we consider the sequence converging to , then the image sequence does not converge to . Am I right? How to check if is or is not continuous at a point ?","f \mathbb{R} 
f(x) = \begin{cases} x & \mbox{ if x is irrational} \\ p \sin \frac{1}{q} & \mbox{ if x = \frac{p}{q} is in lowest terms}. \end{cases}
 f x = 0 = \frac{0}{1} f(0) = 0 \left( \frac{1}{2^n} \right)_{n \in \mathbb{N}} 0 \left( \sin \frac{1}{2^n} \right)_{n \in \mathbb{N} } f(0) f a \neq 0","['real-analysis', 'analysis', 'continuity']"
68,Suppose $f:X\to Y$ is continuous at $a$. Can I use $a$ in my $\epsilon-\delta$ proof?,Suppose  is continuous at . Can I use  in my  proof?,f:X\to Y a a \epsilon-\delta,"For example, is the following proof valid? Assume $f:\mathbb{R}\to\mathbb{R}$ is continuous everywhere. Let $a\in\mathbb{R}$ be a designated point of continuity. Let $\epsilon>0$ and find $\delta>0$ such that $|x-a|<\delta$ implies $|f(x)-f(a)|<\min(\epsilon,\frac{\epsilon}{\epsilon+2|f(a)|})$ , then \begin{align*} |f(x)-f(a)|&<\frac{\epsilon}{|f(x)|+|f(a)|}\hspace{15 mm} b/c\hspace{5 mm} |f(x)|<\epsilon+|f(a)|\\ &\leq\frac{\epsilon}{|f(x)+f(a)|} \end{align*} Therefore, $|f^2(x)-f^2(a)|<\epsilon$ and so $f^2$ is continuous on $\mathbb{R}$ . I believe this is good because $a$ is chosen arbitrarily from the outset. I just don't like it. If it is indeed valid, maybe you can help convince me it's not so bad?","For example, is the following proof valid? Assume is continuous everywhere. Let be a designated point of continuity. Let and find such that implies , then Therefore, and so is continuous on . I believe this is good because is chosen arbitrarily from the outset. I just don't like it. If it is indeed valid, maybe you can help convince me it's not so bad?","f:\mathbb{R}\to\mathbb{R} a\in\mathbb{R} \epsilon>0 \delta>0 |x-a|<\delta |f(x)-f(a)|<\min(\epsilon,\frac{\epsilon}{\epsilon+2|f(a)|}) \begin{align*}
|f(x)-f(a)|&<\frac{\epsilon}{|f(x)|+|f(a)|}\hspace{15 mm} b/c\hspace{5 mm} |f(x)|<\epsilon+|f(a)|\\
&\leq\frac{\epsilon}{|f(x)+f(a)|}
\end{align*} |f^2(x)-f^2(a)|<\epsilon f^2 \mathbb{R} a","['real-analysis', 'calculus', 'analysis', 'continuity', 'epsilon-delta']"
69,Pulling out a function from an integral. When is this allowed?,Pulling out a function from an integral. When is this allowed?,,"If I have a function which does not depend on the variable which is the integrating variable, can I pull that function out? That is, suppose I have $\int g(x,t)f(u_i - u(x,t))du_i$ . Can I pull out the $g(x,t)$ ? What about for when I have this: $\int \frac{\partial g}{\partial t}(x,t) f(u_i - u(x,t))du_i$ ? I know that by Leibniz's rule I can pull out the $\frac{\partial }{\partial t}$ , but I do not know if I am allowed to pull out the entire term $\frac{\partial g}{\partial t}(x,t)\int f(u_i - u(x,t))du_i$ .","If I have a function which does not depend on the variable which is the integrating variable, can I pull that function out? That is, suppose I have . Can I pull out the ? What about for when I have this: ? I know that by Leibniz's rule I can pull out the , but I do not know if I am allowed to pull out the entire term .","\int g(x,t)f(u_i - u(x,t))du_i g(x,t) \int \frac{\partial g}{\partial t}(x,t) f(u_i - u(x,t))du_i \frac{\partial }{\partial t} \frac{\partial g}{\partial t}(x,t)\int f(u_i - u(x,t))du_i","['real-analysis', 'calculus', 'integration', 'analysis', 'partial-derivative']"
70,Computing the value of $\zeta(6)$ and $\zeta(8)$,Computing the value of  and,\zeta(6) \zeta(8),"I know the closed form formula for calculating the values of Zeta function at even integers. I was able to derive it from the coefficients of the power series of $-\dfrac{\pi x}{2}\cot(\pi x)$ . Now, I am looking at the way with which Euler was able to derive the values of $\zeta(2n)$ . I have seen this "" trick "" in Brilliant: So I have: $$\dfrac{\sin(x)}{x}=1-\dfrac{x^2}{3!}+\dfrac{x^4}{5!}-\dfrac{x^6}{7!}+\dfrac{x^8}{9!}...\tag{1}$$ $$\dfrac{\sin(x)}{x}=\left(1-\dfrac{x^2}{\pi^2}\right)\left(1-\dfrac{x^2}{4\pi^2}\right)\left(1-\dfrac{x^2}{9\pi^2}\right)\left(1-\dfrac{x^2}{16\pi^2}\right)...\tag{2}$$ $$\dfrac{\sin(ix)}{ix}=1-\dfrac{(ix)^2}{3!}+\dfrac{(ix)^4}{5!}-\dfrac{(ix)^6}{7!}...\tag{3}$$ $$\dfrac{\sin(ix)}{ix}=1+\dfrac{x^2}{3!}+\dfrac{x^4}{5!}+\dfrac{x^6}{7!}...\tag{4}$$ $$\dfrac{\sin(ix)}{ix}=\left(1-\dfrac{(ix)^2}{\pi^2}\right)\left(1-\dfrac{(ix)^2}{4\pi^2}\right)\left(1-\dfrac{(ix)^2}{9\pi^2}\right)\left(1-\dfrac{(ix)^2}{16\pi^2}\right)...\tag{5}$$ $$\dfrac{\sin(ix)}{ix}=\left(1+\dfrac{x^2}{\pi^2}\right)\left(1+\dfrac{x^2}{4\pi^2}\right)\left(1+\dfrac{x^2}{9\pi^2}\right)\left(1+\dfrac{x^2}{16\pi^2}\right)...\tag{6}$$ Multiply $(6)$ and $(2)$ , I have: $$\dfrac{\sin(x)\sin(ix)}{ix^2}=1+\left(\dfrac{x^2}{3!}-\dfrac{x^2}{3!}\right)+\left(\dfrac{x^4}{5!}+\dfrac{x^4}{5!}-\dfrac{x^4}{(3!)^2}\right)+...\tag{7}$$ $$\dfrac{\sin(x)\sin(ix)}{ix^2}=\left(1+\dfrac{x^2}{\pi^2}\right)\left(1-\dfrac{x^2}{\pi^2}\right)\left(1+\dfrac{x^2}{4\pi^2}\right)\left(1-\dfrac{x^2}{4\pi^2}\right)\left(1+\dfrac{x^2}{9\pi^2}\right)\left(1-\dfrac{x^2}{9\pi^2}\right)...\tag{8}$$ $$\dfrac{\sin(x)\sin(ix)}{ix^2}=\left(1-\dfrac{x^4}{\pi^4}\right)\left(1-\dfrac{x^4}{16\pi^4}\right)\left(1-\dfrac{x^4}{81\pi^4}\right)...\tag{9}$$ Comparing the coeffcients of $x^4$ in the infinite Maclaurin series and the infinite product, we will arrive at $\zeta(4)=\dfrac{\pi^4}{90}$ . Now that I wish to continue this process to see if I can get $\zeta(6)$ , $\zeta(8)$ , $\zeta(10)$ , I don't know how to continue. The trick above works fine for $\zeta(4)$ , but what about higher $\zeta(2n)$ . I look around and find this interesting post of user17762 : In his answer, he wrote the function on the LHS as $\dfrac{i\sin(z)\sin(\dfrac{z}{i})}{z^2}$ . A little bit of manipulation shows that it is exactly like $\dfrac{\sin(x)\sin(ix)}{ix^2}$ or $\dfrac{\sin(x)\sinh(x)}{x^2}$ I have tried to multiply again $\dfrac{\sin(x)\sinh(x)\sin(x/i^2)}{x^2(\frac{x}{i^2})}=\dfrac{\sin^2(x)\sinh(x)}{x^3}$ , using the hint found in the answer of the member above. Then I use Wolfram to check the series expansion of this function but the third term of $x^6$ is $\dfrac{5}{3024}$ . I thought that I should a find a function where its coefficient of $x^6$ should be $\dfrac{1}{945}$ . I am not sure if my reasoning is problematic. Is there an algorithm to find the function on the LHS?","I know the closed form formula for calculating the values of Zeta function at even integers. I was able to derive it from the coefficients of the power series of . Now, I am looking at the way with which Euler was able to derive the values of . I have seen this "" trick "" in Brilliant: So I have: Multiply and , I have: Comparing the coeffcients of in the infinite Maclaurin series and the infinite product, we will arrive at . Now that I wish to continue this process to see if I can get , , , I don't know how to continue. The trick above works fine for , but what about higher . I look around and find this interesting post of user17762 : In his answer, he wrote the function on the LHS as . A little bit of manipulation shows that it is exactly like or I have tried to multiply again , using the hint found in the answer of the member above. Then I use Wolfram to check the series expansion of this function but the third term of is . I thought that I should a find a function where its coefficient of should be . I am not sure if my reasoning is problematic. Is there an algorithm to find the function on the LHS?",-\dfrac{\pi x}{2}\cot(\pi x) \zeta(2n) \dfrac{\sin(x)}{x}=1-\dfrac{x^2}{3!}+\dfrac{x^4}{5!}-\dfrac{x^6}{7!}+\dfrac{x^8}{9!}...\tag{1} \dfrac{\sin(x)}{x}=\left(1-\dfrac{x^2}{\pi^2}\right)\left(1-\dfrac{x^2}{4\pi^2}\right)\left(1-\dfrac{x^2}{9\pi^2}\right)\left(1-\dfrac{x^2}{16\pi^2}\right)...\tag{2} \dfrac{\sin(ix)}{ix}=1-\dfrac{(ix)^2}{3!}+\dfrac{(ix)^4}{5!}-\dfrac{(ix)^6}{7!}...\tag{3} \dfrac{\sin(ix)}{ix}=1+\dfrac{x^2}{3!}+\dfrac{x^4}{5!}+\dfrac{x^6}{7!}...\tag{4} \dfrac{\sin(ix)}{ix}=\left(1-\dfrac{(ix)^2}{\pi^2}\right)\left(1-\dfrac{(ix)^2}{4\pi^2}\right)\left(1-\dfrac{(ix)^2}{9\pi^2}\right)\left(1-\dfrac{(ix)^2}{16\pi^2}\right)...\tag{5} \dfrac{\sin(ix)}{ix}=\left(1+\dfrac{x^2}{\pi^2}\right)\left(1+\dfrac{x^2}{4\pi^2}\right)\left(1+\dfrac{x^2}{9\pi^2}\right)\left(1+\dfrac{x^2}{16\pi^2}\right)...\tag{6} (6) (2) \dfrac{\sin(x)\sin(ix)}{ix^2}=1+\left(\dfrac{x^2}{3!}-\dfrac{x^2}{3!}\right)+\left(\dfrac{x^4}{5!}+\dfrac{x^4}{5!}-\dfrac{x^4}{(3!)^2}\right)+...\tag{7} \dfrac{\sin(x)\sin(ix)}{ix^2}=\left(1+\dfrac{x^2}{\pi^2}\right)\left(1-\dfrac{x^2}{\pi^2}\right)\left(1+\dfrac{x^2}{4\pi^2}\right)\left(1-\dfrac{x^2}{4\pi^2}\right)\left(1+\dfrac{x^2}{9\pi^2}\right)\left(1-\dfrac{x^2}{9\pi^2}\right)...\tag{8} \dfrac{\sin(x)\sin(ix)}{ix^2}=\left(1-\dfrac{x^4}{\pi^4}\right)\left(1-\dfrac{x^4}{16\pi^4}\right)\left(1-\dfrac{x^4}{81\pi^4}\right)...\tag{9} x^4 \zeta(4)=\dfrac{\pi^4}{90} \zeta(6) \zeta(8) \zeta(10) \zeta(4) \zeta(2n) \dfrac{i\sin(z)\sin(\dfrac{z}{i})}{z^2} \dfrac{\sin(x)\sin(ix)}{ix^2} \dfrac{\sin(x)\sinh(x)}{x^2} \dfrac{\sin(x)\sinh(x)\sin(x/i^2)}{x^2(\frac{x}{i^2})}=\dfrac{\sin^2(x)\sinh(x)}{x^3} x^6 \dfrac{5}{3024} x^6 \dfrac{1}{945},"['real-analysis', 'sequences-and-series', 'analysis', 'riemann-zeta']"
71,What is wrong with my derivation of the general formula of the power series for $\cos^{2}(x)$,What is wrong with my derivation of the general formula of the power series for,\cos^{2}(x),"I have arduously obtained the series for $\cos^2(x)$ by using long multiplication method (multiplying twice the Taylor series of $\cos(x)$ which is $\sum_{n=0}^{+\infty} \frac{(-1)^nx^{2n}}{(2n)!}$ $$\cos^2(x)=1-x^2+\dfrac{1}{3}x^4-\dfrac{2}{45}x^6+\dfrac{1}{315}x^8+...$$ I wish to manipulate with the sigma notation to reduce the cost of labour, so I square the above general formula for $\cos(x)$ : $$\cos^2(x)=\sum_{n=1}^{+\infty} \left(\frac{(-1)^nx^{2n}}{(2n!)}\right)^2=\sum_{n=1}^{+\infty} \dfrac{(-1)^{2n}x^{4n}}{[(2n)!]^2}$$ But this is plain wrong, the formula that I obtain is not the general formula for the series of $\cos^2(x)$ . So what is wrong with my manipulation with the sigma notation of this series. The power of $x$ and $-1$ is too high, it should be $2n$ . Also, is my denominator correct? How do you obtain the general formulae for this series?","I have arduously obtained the series for by using long multiplication method (multiplying twice the Taylor series of which is I wish to manipulate with the sigma notation to reduce the cost of labour, so I square the above general formula for : But this is plain wrong, the formula that I obtain is not the general formula for the series of . So what is wrong with my manipulation with the sigma notation of this series. The power of and is too high, it should be . Also, is my denominator correct? How do you obtain the general formulae for this series?",\cos^2(x) \cos(x) \sum_{n=0}^{+\infty} \frac{(-1)^nx^{2n}}{(2n)!} \cos^2(x)=1-x^2+\dfrac{1}{3}x^4-\dfrac{2}{45}x^6+\dfrac{1}{315}x^8+... \cos(x) \cos^2(x)=\sum_{n=1}^{+\infty} \left(\frac{(-1)^nx^{2n}}{(2n!)}\right)^2=\sum_{n=1}^{+\infty} \dfrac{(-1)^{2n}x^{4n}}{[(2n)!]^2} \cos^2(x) x -1 2n,"['calculus', 'sequences-and-series', 'analysis', 'power-series', 'taylor-expansion']"
72,Topological conjugacy between linear maps in dimension $1$,Topological conjugacy between linear maps in dimension,1,Let $A_{\alpha}: \mathbb{R} \to \mathbb{R}$ denote the linear map \begin{align} A_{\alpha}(x)=\alpha x. \end{align} By definition two homeomorphisms $f:X \to X$ and $g:X \to X $ are topologically conjugate to each other if there is a homeomorphism $h:X \to X $ such that $hf=gh$ . I wish to prove that if $ 0 < \alpha < 1$ and $ 0 < \beta < 1 $ then $A_{\alpha}$ and $A_{\beta}$ are topologically conjugate. Could anyone help me find this homeomorphism?,Let denote the linear map By definition two homeomorphisms and are topologically conjugate to each other if there is a homeomorphism such that . I wish to prove that if and then and are topologically conjugate. Could anyone help me find this homeomorphism?,"A_{\alpha}: \mathbb{R} \to \mathbb{R} \begin{align}
A_{\alpha}(x)=\alpha x.
\end{align} f:X \to X g:X \to X  h:X \to X  hf=gh  0 < \alpha < 1  0 < \beta < 1  A_{\alpha} A_{\beta}","['general-topology', 'analysis', 'dynamical-systems', 'topological-dynamics']"
73,"Prove that the map $f:[0,1]\to l_{\infty}([0,1];\Bbb{R})$ defined by $t\mapsto 1_{[t,1]}$ is Riemann integrable.",Prove that the map  defined by  is Riemann integrable.,"f:[0,1]\to l_{\infty}([0,1];\Bbb{R}) t\mapsto 1_{[t,1]}","A map $f:[0,1]\to F$ (where $F$ is a banach space) is said to be Riemann integrable if $\exists I\in F$ such that for every $\epsilon>0$ there is $\delta>0$ satisfying $$\left\lVert I-\sum\limits_{i=1}^n  f(\xi_i)(t_i-t_{i-1})\right\rVert_F<\epsilon$$ for all partions $P=\{0=t_0<t_1<\cdots<t_n=1\}$ of $[0,1]$ with $\lVert  P\rVert:=\underset{i}{\text{max}}(t_i-t_{i-1})<\delta$ and for every choice of $\xi_i\in(t_{i-1},t_i)$ Here $l_{\infty}([0,1];\Bbb{R}):=\left\{f:[0,1]\to\Bbb{R}|\ \lVert f\rVert_\infty=\underset{t}{\text{sup}}| f(t)|<\infty\right\}$ is banach space with the sup norm $\lVert\cdot\rVert_\infty$ . The map $f:[0,1]\to l_{\infty}([0,1];\Bbb{R})$ is defined as $\displaystyle{f(t)=1_{[t,1]}}\ \forall t\in[0,1]$ . Recall that- $$ 1_{[t,1]}(x)=\begin{cases} 1&\text{if }x\in [t,1]\\ 0&\text{if }x\in [0,t)  \end{cases}$$ I have to find $I\in l_{\infty}([0,1];\Bbb{R})$ such that the expression above in the definition of riemann integrebility is satisfied. I don't have any idea how to think about this $I$ . Initially, I have tried with $I=1_{[0,1]}$ (which is basically the constant function $1$ ), but it will not work because for any partion $P=\{0=t_0<t_1<\cdots<t_n=1\}$ we have $\left| 1_{[0,1]}(0)-\sum\limits_{i=1}^n  f(\xi_i)(0)(t_i-t_{i-1})\right|=\left| 1-1_{[\xi_1,1]}(0)t_1\right|=1$ (as $\xi_1>0$ ). This implies $\left\lVert 1_{[0,1]}-\sum\limits_{i=1}^n  f(\xi_i)(t_i-t_{i-1})\right\rVert_\infty\ge 1$ . So, $I=1_{[0,1]}$ fails. Then I tried with constant function $0$ , but it fails in the similar fashion (by evaluating at $1$ ). Can anyone help me in this regard? Thanks for help in advance.","A map (where is a banach space) is said to be Riemann integrable if such that for every there is satisfying for all partions of with and for every choice of Here is banach space with the sup norm . The map is defined as . Recall that- I have to find such that the expression above in the definition of riemann integrebility is satisfied. I don't have any idea how to think about this . Initially, I have tried with (which is basically the constant function ), but it will not work because for any partion we have (as ). This implies . So, fails. Then I tried with constant function , but it fails in the similar fashion (by evaluating at ). Can anyone help me in this regard? Thanks for help in advance.","f:[0,1]\to F F \exists I\in F \epsilon>0 \delta>0 \left\lVert I-\sum\limits_{i=1}^n
 f(\xi_i)(t_i-t_{i-1})\right\rVert_F<\epsilon P=\{0=t_0<t_1<\cdots<t_n=1\} [0,1] \lVert
 P\rVert:=\underset{i}{\text{max}}(t_i-t_{i-1})<\delta \xi_i\in(t_{i-1},t_i) l_{\infty}([0,1];\Bbb{R}):=\left\{f:[0,1]\to\Bbb{R}|\ \lVert f\rVert_\infty=\underset{t}{\text{sup}}| f(t)|<\infty\right\} \lVert\cdot\rVert_\infty f:[0,1]\to l_{\infty}([0,1];\Bbb{R}) \displaystyle{f(t)=1_{[t,1]}}\ \forall t\in[0,1] 
1_{[t,1]}(x)=\begin{cases}
1&\text{if }x\in [t,1]\\
0&\text{if }x\in [0,t) 
\end{cases} I\in l_{\infty}([0,1];\Bbb{R}) I I=1_{[0,1]} 1 P=\{0=t_0<t_1<\cdots<t_n=1\} \left| 1_{[0,1]}(0)-\sum\limits_{i=1}^n
 f(\xi_i)(0)(t_i-t_{i-1})\right|=\left| 1-1_{[\xi_1,1]}(0)t_1\right|=1 \xi_1>0 \left\lVert 1_{[0,1]}-\sum\limits_{i=1}^n
 f(\xi_i)(t_i-t_{i-1})\right\rVert_\infty\ge 1 I=1_{[0,1]} 0 1","['analysis', 'normed-spaces', 'banach-spaces', 'riemann-integration']"
74,Rudin RCA Problem 6.4,Rudin RCA Problem 6.4,,"I'm trying to solve the following Problem 6.4 from Rudin's RCA: Suppose that $1\leq p\leq \infty$ , and $q$ is the exponent conjugate to $p$ . Suppose that $\mu$ is a $\sigma$ -finite measure and $g$ is a measurable function such that $fg\in L^1(\mu)$ for every $f\in L^p(\mu)$ . Prove that then $g\in L^q(\mu)$ . I can get to the point where $\int_{E}|g|d\mu<\infty$ for every finite measure set $E$ , but I'm not sure where to proceed. I would appreciate any suggestions. Thanks!","I'm trying to solve the following Problem 6.4 from Rudin's RCA: Suppose that , and is the exponent conjugate to . Suppose that is a -finite measure and is a measurable function such that for every . Prove that then . I can get to the point where for every finite measure set , but I'm not sure where to proceed. I would appreciate any suggestions. Thanks!",1\leq p\leq \infty q p \mu \sigma g fg\in L^1(\mu) f\in L^p(\mu) g\in L^q(\mu) \int_{E}|g|d\mu<\infty E,"['analysis', 'measure-theory', 'riesz-representation-theorem']"
75,Does the operator $(\hat{f}\cdot m )^\vee$ maps Schwartz in it self?,Does the operator  maps Schwartz in it self?,(\hat{f}\cdot m )^\vee,"Given $m \in L^\infty$ and $\phi \in \mathcal{S}$ a Schwartz function, is it true that $(\hat{f}\cdot m)^\vee$ is a Schwartz function?? I trying to prove this so I could conclude that operator of the form $(\hat{f}\cdot m)^\vee$ maps $\mathcal{S}$ to it self. Attempt: Given $\alpha, \beta$ multi-index, we have to prove that $$\sup_{x \in \mathbb{R^n}}|x^\alpha\partial^\beta(\hat{f}\cdot m)^\vee(x)| < \infty. $$ When $\alpha = 0$ , using some properties of Fourier tranform, we get $$\partial^\beta(\hat{f}\cdot m)^\vee(x) = ((2\pi i \xi)^\beta\hat{f}(\xi)m(\xi))^\vee(x) = ((\partial^\beta f)^\wedge\cdot m)^\vee(x).$$ Then, taking the absolute value of the expression above and by definition of inverse Fourier transform, \begin{align*} |\partial^\beta(\hat{f}\cdot m)^\vee(x)| = & \left| \int_\mathbb{R^n} (\partial^\beta f)^\wedge(\xi)m(\xi) e^{2\pi i \xi\cdot x} d\xi  \right| \\ \leq & \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)||m(\xi)|d\xi \\ \leq &\|m\|_{L^\infty} \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)|d\xi  \\ =& \|m\|_{L^\infty} \|(\partial^\beta f)^\wedge\|_{L^1}. \end{align*} The $L^1$ -norm of $(\partial^\beta f)^\wedge$ is finite, since this is a Schwartz function. My problem is for $\alpha \neq 0$ . For simplicity and in view of properties of Fourier transform, I changed $x^\alpha$ for $(-2\pi i x)^\alpha$ and I want to show that the supreme of $|(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)|$ over all $x \in \mathbb{R^n}$ is finite: \begin{align*} |(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)| =  & |(-2\pi i x)^\alpha ((\partial^\beta f)^\wedge\cdot m)^\vee(x)| \\ = & |[\partial^\alpha((\partial^\beta f)^\wedge \cdot m)]^\vee(x)|. \end{align*} How do I proceed from here?? Does it make sense the derivative $\partial^\alpha((\partial^\beta f)^\wedge \cdot m)$ ??","Given and a Schwartz function, is it true that is a Schwartz function?? I trying to prove this so I could conclude that operator of the form maps to it self. Attempt: Given multi-index, we have to prove that When , using some properties of Fourier tranform, we get Then, taking the absolute value of the expression above and by definition of inverse Fourier transform, The -norm of is finite, since this is a Schwartz function. My problem is for . For simplicity and in view of properties of Fourier transform, I changed for and I want to show that the supreme of over all is finite: How do I proceed from here?? Does it make sense the derivative ??","m \in L^\infty \phi \in \mathcal{S} (\hat{f}\cdot m)^\vee (\hat{f}\cdot m)^\vee \mathcal{S} \alpha, \beta \sup_{x \in \mathbb{R^n}}|x^\alpha\partial^\beta(\hat{f}\cdot m)^\vee(x)| < \infty.  \alpha = 0 \partial^\beta(\hat{f}\cdot m)^\vee(x) = ((2\pi i \xi)^\beta\hat{f}(\xi)m(\xi))^\vee(x) = ((\partial^\beta f)^\wedge\cdot m)^\vee(x). \begin{align*}
|\partial^\beta(\hat{f}\cdot m)^\vee(x)| = &
\left| \int_\mathbb{R^n} (\partial^\beta f)^\wedge(\xi)m(\xi) e^{2\pi i \xi\cdot x} d\xi  \right| \\
\leq & \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)||m(\xi)|d\xi \\
\leq &\|m\|_{L^\infty} \int_\mathbb{R^n} |(\partial^\beta f)^\wedge(\xi)|d\xi  \\
=& \|m\|_{L^\infty} \|(\partial^\beta f)^\wedge\|_{L^1}.
\end{align*} L^1 (\partial^\beta f)^\wedge \alpha \neq 0 x^\alpha (-2\pi i x)^\alpha |(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)| x \in \mathbb{R^n} \begin{align*}
|(-2\pi i x)^\alpha \partial^\beta(\hat{f}\cdot m)^\vee(x)| =  & |(-2\pi i x)^\alpha ((\partial^\beta f)^\wedge\cdot m)^\vee(x)| \\
= & |[\partial^\alpha((\partial^\beta f)^\wedge \cdot m)]^\vee(x)|.
\end{align*} \partial^\alpha((\partial^\beta f)^\wedge \cdot m)","['real-analysis', 'analysis', 'fourier-transform', 'harmonic-analysis']"
76,"Multivariable chain rule for $f(x,y,g(x,y))$ when finding $\tfrac{\partial f}{\partial x}$.",Multivariable chain rule for  when finding .,"f(x,y,g(x,y)) \tfrac{\partial f}{\partial x}","Consider the function $f(x,y,z)$ where $z=g(x,y)$ . If I took the partial derivative wrt $x$ what I'd end up getting would be: $$ \dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} \dfrac{\partial x}{\partial x}+\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} $$ $$ \dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} $$ Well, this is quite obviously wrong, since that would imply $\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} = 0$ . How would I correctly express the partial derivative with respect to x in this case?","Consider the function where . If I took the partial derivative wrt what I'd end up getting would be: Well, this is quite obviously wrong, since that would imply . How would I correctly express the partial derivative with respect to x in this case?","f(x,y,z) z=g(x,y) x 
\dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} \dfrac{\partial x}{\partial x}+\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x}
 
\dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x}
 \dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} = 0","['calculus', 'analysis', 'multivariable-calculus', 'chain-rule']"
77,looking for 8 digit numbers with 4 digits being used twice,looking for 8 digit numbers with 4 digits being used twice,,I'm looking for $8$ digit numbers with $4$ digits being used twice. for example : $11223344$ and $12123434$ and $11002233$ Its not allowed to use one digit like: $11223345$ for $4$ digit numbers with $2$ digits being used twice I have computed $243$ numbers. I need to find out how many 8-digit numbers satisfy afore-mentioned conditon,I'm looking for digit numbers with digits being used twice. for example : and and Its not allowed to use one digit like: for digit numbers with digits being used twice I have computed numbers. I need to find out how many 8-digit numbers satisfy afore-mentioned conditon,8 4 11223344 12123434 11002233 11223345 4 2 243,['analysis']
78,Between two Real Numbers exists Rational Number,Between two Real Numbers exists Rational Number,,"I can't understand the following proof I found in Rudin, ""Principles of mathematical analysis"". This is the statement: if $x\in\mathbb{R}, y\in\mathbb{R}$ and $x<y$ , then there exists $p\in\mathbb{Q}$ such that $x<p<y$ . Here is the proof I can't understand. proof. 1) Since $x<y$ , we have $y-x>0$ (ok, I got it) 2) Archimedean principle furnishes a positive integer $n$ such that $n(y-x)>1$ (ok) 3) Apply archimedean principle again to obtain positive integers $m_1,m_2$ such that $m_1>nx$ and $m_2>-nx$ (ok) 4) Then $-m_2<nx<m_1$ (ok) 5) Hence there is an integer $m$ (with $-m_2\leq m\leq m_1$ ) such that $m-1\leq nx< m$ Here is where I have some problems. Why do I need integers $-m_2$ and $m_1$ ? Couldn't just say: every real number lies between an integer and its successor? Moreover, even knowing the existence of integers $m_1, m_2$ , how do I deduce the fact that $m$ exists with that property? 6) If we combine these inequalities, we obtain $nx<m\leq 1*nx<ny$ (ok) 7) Since $n>0$ , it follows that $x<\frac{m}{n}<y$ (ok) 8) Take $p=\frac{m}{n}$ . (ok)","I can't understand the following proof I found in Rudin, ""Principles of mathematical analysis"". This is the statement: if and , then there exists such that . Here is the proof I can't understand. proof. 1) Since , we have (ok, I got it) 2) Archimedean principle furnishes a positive integer such that (ok) 3) Apply archimedean principle again to obtain positive integers such that and (ok) 4) Then (ok) 5) Hence there is an integer (with ) such that Here is where I have some problems. Why do I need integers and ? Couldn't just say: every real number lies between an integer and its successor? Moreover, even knowing the existence of integers , how do I deduce the fact that exists with that property? 6) If we combine these inequalities, we obtain (ok) 7) Since , it follows that (ok) 8) Take . (ok)","x\in\mathbb{R}, y\in\mathbb{R} x<y p\in\mathbb{Q} x<p<y x<y y-x>0 n n(y-x)>1 m_1,m_2 m_1>nx m_2>-nx -m_2<nx<m_1 m -m_2\leq m\leq m_1 m-1\leq nx< m -m_2 m_1 m_1, m_2 m nx<m\leq 1*nx<ny n>0 x<\frac{m}{n}<y p=\frac{m}{n}","['analysis', 'proof-explanation']"
79,Divergence of improper integral if $f$ has limit $l>0$,Divergence of improper integral if  has limit,f l>0,"Lets suppose that $f:[a,\infty) \to \mathbb{R}$ is a Riemann integrable function and lets suppose that $$\lim_{x \to \infty} f(x) = l > 0$$ I am trying to prove that then $$\int_a^\infty f(x) \text{d}x=\infty$$ By hypothesis $f$ has limit $l > 0$ as $x \to \infty$ , so we know that for all $\varepsilon>0$ exists $K_{\varepsilon} > 0$ such that for all $x \geq K_{\varepsilon}$ it is $f(x)>l-\varepsilon$ . Since by hypothesis $l > 0$ and for the arbitrarity of $\varepsilon>0$ we can choose $\varepsilon=l/2$ ; so we have the estime $f(x)>l/2$ . So it is $$\int_a^\infty f(x) \text{d}x =\int_a^{K_\varepsilon} f(x) \text{d}x+\int_{K_\varepsilon}^\infty f(x) \text{d}x > \int_a^\infty \frac{l}{2} \text{d}x =\infty$$ Since the first integral on the right hand side is finite it does not influence the convergence, so we concentrate on the second integral; by the limit estimation we have that $$\int_{K_\varepsilon}^\infty f(x) \text{d}x > \int_{K_\varepsilon}^\infty \frac{l}{2} \text{d}x =\infty$$ So the integral is divergent. Some questions: 1) is the context correct? I've assumed that "" $f:[a,\infty)$ Riemann integrable"" means that the only point we have to study is when $x \to \infty$ because of the unboundedness; 2) when I split the integral in two integrals I suppose that $a<K_\varepsilon$ , can I do this? If yes, why? 3) is the proof correct in general? If not, where are the mistakes? If yes, how can I improve it? Thanks.","Lets suppose that is a Riemann integrable function and lets suppose that I am trying to prove that then By hypothesis has limit as , so we know that for all exists such that for all it is . Since by hypothesis and for the arbitrarity of we can choose ; so we have the estime . So it is Since the first integral on the right hand side is finite it does not influence the convergence, so we concentrate on the second integral; by the limit estimation we have that So the integral is divergent. Some questions: 1) is the context correct? I've assumed that "" Riemann integrable"" means that the only point we have to study is when because of the unboundedness; 2) when I split the integral in two integrals I suppose that , can I do this? If yes, why? 3) is the proof correct in general? If not, where are the mistakes? If yes, how can I improve it? Thanks.","f:[a,\infty) \to \mathbb{R} \lim_{x \to \infty} f(x) = l > 0 \int_a^\infty f(x) \text{d}x=\infty f l > 0 x \to \infty \varepsilon>0 K_{\varepsilon} > 0 x \geq K_{\varepsilon} f(x)>l-\varepsilon l > 0 \varepsilon>0 \varepsilon=l/2 f(x)>l/2 \int_a^\infty f(x) \text{d}x =\int_a^{K_\varepsilon} f(x) \text{d}x+\int_{K_\varepsilon}^\infty f(x) \text{d}x > \int_a^\infty \frac{l}{2} \text{d}x =\infty \int_{K_\varepsilon}^\infty f(x) \text{d}x > \int_{K_\varepsilon}^\infty \frac{l}{2} \text{d}x =\infty f:[a,\infty) x \to \infty a<K_\varepsilon","['integration', 'analysis', 'definite-integrals', 'improper-integrals', 'solution-verification']"
80,"Proof of Luzin's Theorem on Axler's Measure, Integration & Real Analysis","Proof of Luzin's Theorem on Axler's Measure, Integration & Real Analysis",,"I would appreciate some help understanding a step on the proof of Luzin's Theorem on Sheldon Axler's Measure, Integration & Real Analysis (open access here , Theorem 2.91 pg 66). Basically, there is a finite collection of disjoint Borel sets $D_1, \dots, D_n$ , colsed sets $F_k \subset D_k$ and open sets $G_k\supset D_k$ Then, he defines a set $$F=\left(\bigcup_{k=1}^n F_k\right)\cup \left(\bigcap_{k=1}^n \mathbb{R}\setminus G_k\right)$$ And then claims that $\mathbb{R}\setminus F = \bigcup_{k=1}^n (G_k \setminus F_k)$ . I cannot see how this follows, unless $G_i \cap F_j= \emptyset$ for $i\neq j$ , and I don't see why this would be the case. Many thanks!","I would appreciate some help understanding a step on the proof of Luzin's Theorem on Sheldon Axler's Measure, Integration & Real Analysis (open access here , Theorem 2.91 pg 66). Basically, there is a finite collection of disjoint Borel sets , colsed sets and open sets Then, he defines a set And then claims that . I cannot see how this follows, unless for , and I don't see why this would be the case. Many thanks!","D_1, \dots, D_n F_k \subset D_k G_k\supset D_k F=\left(\bigcup_{k=1}^n F_k\right)\cup \left(\bigcap_{k=1}^n \mathbb{R}\setminus G_k\right) \mathbb{R}\setminus F = \bigcup_{k=1}^n (G_k \setminus F_k) G_i \cap F_j= \emptyset i\neq j","['analysis', 'measure-theory', 'elementary-set-theory']"
81,Dominated Convergence and Monotone Convergence,Dominated Convergence and Monotone Convergence,,"Assuming that the measure space is $\sigma$ -finite, how can I show that the Dominated convergence theorem implies the monotone convergence theorem?","Assuming that the measure space is -finite, how can I show that the Dominated convergence theorem implies the monotone convergence theorem?",\sigma,"['integration', 'analysis', 'measure-theory']"
82,Is there a term for functions where $f(x) - x$ is periodic?,Is there a term for functions where  is periodic?,f(x) - x,"Title says it all really. I'm working with a type of function $f: \mathbb{R} \rightarrow \mathbb{R}$ with the property that $f(x+k) = f(x)+k$ for integers $k$ , or equivalently, $f(x) - x$ is periodic with period $1$ . Is there a name in common usage for this type of function? Or if not, what would be an appropriate name? ""Semi-periodic""? ""Periodically increasing""?","Title says it all really. I'm working with a type of function with the property that for integers , or equivalently, is periodic with period . Is there a name in common usage for this type of function? Or if not, what would be an appropriate name? ""Semi-periodic""? ""Periodically increasing""?",f: \mathbb{R} \rightarrow \mathbb{R} f(x+k) = f(x)+k k f(x) - x 1,"['real-analysis', 'analysis', 'functions', 'periodic-functions']"
83,"Existence of $\lim_{k\to +\infty}\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x$",Existence of,"\lim_{k\to +\infty}\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x","Prove that existence of limit $$\lim_{k\to +\infty}\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x$$ in $\mathbb{R}$ . Attempt . Let $A_k$ be the $k$ -th term of the sequence. Then $(A_k)$ is increasing. If we proved that it is upper bounded, then we would have convergence. But the estimation: $$\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x\leqslant \frac12\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\sin^2x}}\,\mathrm{d}x=+\infty,$$ although corrent, seems to overestimate the integral. Thanks for the help.","Prove that existence of limit in . Attempt . Let be the -th term of the sequence. Then is increasing. If we proved that it is upper bounded, then we would have convergence. But the estimation: although corrent, seems to overestimate the integral. Thanks for the help.","\lim_{k\to +\infty}\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x \mathbb{R} A_k k (A_k) \int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\frac{k^2}{k^2+1} \,\sin^2x} + \sqrt{1-\sin^2x}}\,\mathrm{d}x\leqslant \frac12\int\limits_{0}^{\frac{\pi}{2}}\frac{\sin^2x}{\sqrt{1-\sin^2x}}\,\mathrm{d}x=+\infty,","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'analysis']"
84,"How to prove $\int^{\infty}_0 e^{-c^2/a^2}c^4\,dc=\frac{3}{8}a^5\sqrt\pi$?",How to prove ?,"\int^{\infty}_0 e^{-c^2/a^2}c^4\,dc=\frac{3}{8}a^5\sqrt\pi","How do i prove this integral can someone give me proof i've been trying this too long i tried direct integration by parts, differentiation all to no avail.Please support us :) $$\int^{\infty}_0 e^{\frac{-c^2}{a^2}}c^4\,dc=\frac{3}{8}a^5\sqrt\pi$$","How do i prove this integral can someone give me proof i've been trying this too long i tried direct integration by parts, differentiation all to no avail.Please support us :)","\int^{\infty}_0 e^{\frac{-c^2}{a^2}}c^4\,dc=\frac{3}{8}a^5\sqrt\pi","['real-analysis', 'integration', 'analysis', 'improper-integrals']"
85,Uniform Continuity of $x^2$.,Uniform Continuity of .,x^2,"Assume all the standards, such as $f:\mathbb R→\mathbb R$ and all that other jargon. I guess it doesn't really matter, but my main question is why isn't $f(x)=x^2$ uniformly continuous. I know if we restrict the domain to some subset $D\subset \mathbb R$ , then $f$ is uniformly continuous on $D$ , but not on the entire domain of $\mathbb R$ .","Assume all the standards, such as and all that other jargon. I guess it doesn't really matter, but my main question is why isn't uniformly continuous. I know if we restrict the domain to some subset , then is uniformly continuous on , but not on the entire domain of .",f:\mathbb R→\mathbb R f(x)=x^2 D\subset \mathbb R f D \mathbb R,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
86,Does the series of the reciprocals of Sophie Germain primes converge?,Does the series of the reciprocals of Sophie Germain primes converge?,,"Yesterday when reading Landau's Elementary Number Theory, I came across Brun's theorem which states that the reciprocals of twin primes add up to a finite sum (the series converges), and this made me wonder if the same is true for Sophie Germain primes (the reciprocals add up to a finite sum). But I cannot Google out anything. Can anyone give an answer or point me to some references? Thanks!","Yesterday when reading Landau's Elementary Number Theory, I came across Brun's theorem which states that the reciprocals of twin primes add up to a finite sum (the series converges), and this made me wonder if the same is true for Sophie Germain primes (the reciprocals add up to a finite sum). But I cannot Google out anything. Can anyone give an answer or point me to some references? Thanks!",,"['number-theory', 'analysis']"
87,Find the asymptote of the function $f(x) = \sqrt{\frac{x^3}{x - 3}} - x$,Find the asymptote of the function,f(x) = \sqrt{\frac{x^3}{x - 3}} - x,"We have a function $f(x) = \sqrt{\frac{x^3}{x - 3}} - x$ and when $x$ goes towards $-\infty$ , we have an asymptote $y = -2x - 3/2$ . How we get this asymptote?","We have a function and when goes towards , we have an asymptote . How we get this asymptote?",f(x) = \sqrt{\frac{x^3}{x - 3}} - x x -\infty y = -2x - 3/2,['analysis']
88,"Baby Rudin 3.54, Concerning the Definition of Rearrangement of a Series","Baby Rudin 3.54, Concerning the Definition of Rearrangement of a Series",,"This theorem has already been asked about, however my specific question, to my knowledge has not.  I hope it will be acceptable if I refer the reader of this question to either here or here for Rudin's proof, since it has already been written up. Rudin says that for ""sequences $(m_n)$ , $(k_n)$ "" the following is ""clearly a rearrangement of $\Sigma a_n$ .'' $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1}  + \cdots + P_{m_2} - Q_{k_1+1} - \cdots - Q_{k_2} + \cdots $$ Can anyone tell me, is he saying that $\Sigma a_{n'}$ is a rearrangement of $\Sigma a_n$ where $(a_{n'})$ is defined $$ a_{1'} := \sum_{i=1}^{m_1} P_i \hspace{1mm}-\hspace{1mm} \sum_{j=1}^{k_1} Q_j, \quad\ldots\quad a_{n'} := \sum_{i=m_{n'}+1}^{m_{{n'}+1}} P_i \hspace{1mm}-\hspace{1mm} \sum_{i=k_{n'}+1}^{k_{{n'}+1}} Q_j, \quad\ldots $$ for any sequences of integers $(m_n)$ and $(k_n)$ ? Or is he saying that $\Sigma a_{n'}$ is a rearrangement of $\Sigma a_n$ where $(a_{n'})$ is defined $$ a_1 := P_1, \quad\ldots\quad a_{n'}  =  \begin{cases} P_{\ell+1} &\text{ if }\hspace{2mm} \ell\ni a_{n'-1} = P_\ell \wedge \ell\neq m_n\hspace{1mm}\forall n\in\{1,2,\ldots\} \\ Q_{\ell+1} &\text{ if }\hspace{2mm} \ell\ni a_{n'-1} = Q_\ell \wedge \ell\neq k_n\hspace{1mm}\forall n\in\{1,2,\ldots\} \\ Q_{k_\ell} &\text{ if }\hspace{2mm} \exists\ell\ni a_{n'-1} = P_{m_\ell} \\ P_{m_\ell + 1} &\text{ if }\hspace{2mm} \exists\ell\ni a_{n'-1} = Q_{m_\ell} \end{cases} $$ Moreover, this only makes sense if $(m_n)$ and $(k_n)$ are strictly increasing sequences of positive integers (although Rudin does not make this specification), is that correct? Thanks to anyone who is willing to help. Edit: My suspicion is that the latter of these two is correct, but I would be grateful for a second or third opinion.","This theorem has already been asked about, however my specific question, to my knowledge has not.  I hope it will be acceptable if I refer the reader of this question to either here or here for Rudin's proof, since it has already been written up. Rudin says that for ""sequences , "" the following is ""clearly a rearrangement of .'' Can anyone tell me, is he saying that is a rearrangement of where is defined for any sequences of integers and ? Or is he saying that is a rearrangement of where is defined Moreover, this only makes sense if and are strictly increasing sequences of positive integers (although Rudin does not make this specification), is that correct? Thanks to anyone who is willing to help. Edit: My suspicion is that the latter of these two is correct, but I would be grateful for a second or third opinion.","(m_n) (k_n) \Sigma a_n 
P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1}  + \cdots + P_{m_2} - Q_{k_1+1} - \cdots - Q_{k_2} + \cdots
 \Sigma a_{n'} \Sigma a_n (a_{n'}) 
a_{1'} := \sum_{i=1}^{m_1} P_i \hspace{1mm}-\hspace{1mm} \sum_{j=1}^{k_1} Q_j,
\quad\ldots\quad
a_{n'} := \sum_{i=m_{n'}+1}^{m_{{n'}+1}} P_i \hspace{1mm}-\hspace{1mm} \sum_{i=k_{n'}+1}^{k_{{n'}+1}} Q_j,
\quad\ldots
 (m_n) (k_n) \Sigma a_{n'} \Sigma a_n (a_{n'}) 
a_1 := P_1, \quad\ldots\quad a_{n'} 
= 
\begin{cases}
P_{\ell+1} &\text{ if }\hspace{2mm} \ell\ni a_{n'-1} = P_\ell \wedge \ell\neq m_n\hspace{1mm}\forall n\in\{1,2,\ldots\}
\\
Q_{\ell+1} &\text{ if }\hspace{2mm} \ell\ni a_{n'-1} = Q_\ell \wedge \ell\neq k_n\hspace{1mm}\forall n\in\{1,2,\ldots\}
\\
Q_{k_\ell} &\text{ if }\hspace{2mm} \exists\ell\ni a_{n'-1} = P_{m_\ell}
\\
P_{m_\ell + 1} &\text{ if }\hspace{2mm} \exists\ell\ni a_{n'-1} = Q_{m_\ell}
\end{cases}
 (m_n) (k_n)","['real-analysis', 'sequences-and-series', 'analysis']"
89,Integral $\int_0^\infty\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u$,Integral,\int_0^\infty\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u,"I was studying the motion of a particle in a certain magnetic field and one of the quantities that arose was given by the titular integral $$ F(\alpha,\beta)=\int_0^\infty\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u $$ Here $\alpha\in\mathbb{R}$ , $\beta>0$ and $J_0$ is a Bessel function. The integral does exist. I found the following theorem in a book on real analysis: Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be continuous on $[a,b]$ while $g:\mathbb{R}\rightarrow\mathbb{R}$ is nonnegative, decreasing and continuously differentiable on $[a,b]$ . Then $\exists\xi\in[a,b]$ : $$ \int_a^bf(x)g(x)\mathrm{d}x=g(a)\int_a^\xi f(x)\mathrm{d}x $$ Taking into account the fact that $\exp(i\alpha\cos u)-J_0(\alpha)$ is periodic with period $2\pi$ and that its integral over one period is $0$ , we can apply the above theorem seperately to the real and imaginary parts of the original integral to find that for all $a,b>0$ : $$ \left\vert\int_a^b\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u\right\vert<\frac{6\pi}{1+\beta a} $$ This implies convergence. However, I can't find any way to express $F$ in terms of other functions, be it ordinary or special. This wouldn't be too much of a problem if the  integrand weren't oscillatory, making numerical evaluation difficult. Therefore, my question is twofold: is there a closed form for $F$ , perhaps in terms of special functions, if not, are there at least any efficient methods for numerical computation of $F$ ?","I was studying the motion of a particle in a certain magnetic field and one of the quantities that arose was given by the titular integral Here , and is a Bessel function. The integral does exist. I found the following theorem in a book on real analysis: Let be continuous on while is nonnegative, decreasing and continuously differentiable on . Then : Taking into account the fact that is periodic with period and that its integral over one period is , we can apply the above theorem seperately to the real and imaginary parts of the original integral to find that for all : This implies convergence. However, I can't find any way to express in terms of other functions, be it ordinary or special. This wouldn't be too much of a problem if the  integrand weren't oscillatory, making numerical evaluation difficult. Therefore, my question is twofold: is there a closed form for , perhaps in terms of special functions, if not, are there at least any efficient methods for numerical computation of ?","
F(\alpha,\beta)=\int_0^\infty\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u
 \alpha\in\mathbb{R} \beta>0 J_0 f:\mathbb{R}\rightarrow\mathbb{R} [a,b] g:\mathbb{R}\rightarrow\mathbb{R} [a,b] \exists\xi\in[a,b] 
\int_a^bf(x)g(x)\mathrm{d}x=g(a)\int_a^\xi f(x)\mathrm{d}x
 \exp(i\alpha\cos u)-J_0(\alpha) 2\pi 0 a,b>0 
\left\vert\int_a^b\frac{\exp(i\alpha\cos u)-J_0(\alpha)}{1+\beta u}\mathrm{d}u\right\vert<\frac{6\pi}{1+\beta a}
 F F F","['integration', 'analysis', 'definite-integrals', 'numerical-methods', 'special-functions']"
90,"If $\int_a ^b f df=0$ and f is continuous, then f is the function constant $0$","If  and f is continuous, then f is the function constant",\int_a ^b f df=0 0,"I have been studied some properties of Riemann Stieltjes integral, and i found this: If $\int_a ^b f df=0$ and f is continuous for every $a<b$ in $R$ , then f is the function constant $0$ without proof I know, by definition $\int_a ^b f df = \sum_{i=0} ^{n} f(\lambda_i)(f(x_{i})-f(x_{i-1}))$ and if $f$ is constant, is clear $(f(x_{i})-f(x_{i-1}))=0$ for every $i$ but why $f$ must be a constant zero?","I have been studied some properties of Riemann Stieltjes integral, and i found this: If and f is continuous for every in , then f is the function constant without proof I know, by definition and if is constant, is clear for every but why must be a constant zero?",\int_a ^b f df=0 a<b R 0 \int_a ^b f df = \sum_{i=0} ^{n} f(\lambda_i)(f(x_{i})-f(x_{i-1})) f (f(x_{i})-f(x_{i-1}))=0 i f,"['integration', 'analysis']"
91,Absolutely continuous function not differentiable on uncountable set,Absolutely continuous function not differentiable on uncountable set,,"From real analysis one knows that an absolutely continuous function is differentiable a.e.. Is there a function showing that this statement cannot be made into ""every AC function is differentiable except on a countable set of points""?","From real analysis one knows that an absolutely continuous function is differentiable a.e.. Is there a function showing that this statement cannot be made into ""every AC function is differentiable except on a countable set of points""?",,"['real-analysis', 'analysis', 'derivatives', 'absolute-continuity']"
92,Understanding generalized Holder inequality proof,Understanding generalized Holder inequality proof,,"There are questions that concerns me when I read the following proof regarding the generalized Holder inequality : Let $U$ be a subset of $\mathbb{R}$ . Let $1 < p, q, r < \infty$ with $p^{-1} + q^{-1} + r^{-1} = 1$ . Let $f \in L^p(U), g \in L^q(U)$ and $h \in L^r(U)$ . Then $$||fgh||_1 \leq ||f||_p||g||_q||h||_r.$$ Assume that we have the original version of Holder inequality : Let $1 < p,q < \infty$ . For $f \in L^p(U)$ and $g \in L^q(U)$ , $$||fg||_1 \leq ||f||_p||g||_q.$$ $\textbf{Proof}$ Let $s = (1/p + 1/q)^{-1}.$ Then $1/s + 1/r = 1.$ Then apply the original Holder inequlity gives $$\int_U (fg)h dx \leq ||h||_r (\int_U (fg)^s)^{1/s}.$$ Then apply Holder again to $(fg)^s$ to get the result. $\textbf{Question}$ My confusion is that when $s$ is set. The next step is to apply the original Holder inequality to $(fg)$ and $h$ . Clearly, $h \in L^r$ . But how do we know that $fg \in L^s$ ? Is it trivial to see that $fg \in L^s$ ?? I try to verify this, but not quite successful. (Note if $fg$ is NOT in $L^s$ , then its integrate is $\infty$ . How can $\infty \leq ||f||_p||g||_q$ which suppose to be a finite number!! )","There are questions that concerns me when I read the following proof regarding the generalized Holder inequality : Let be a subset of . Let with . Let and . Then Assume that we have the original version of Holder inequality : Let . For and , Let Then Then apply the original Holder inequlity gives Then apply Holder again to to get the result. My confusion is that when is set. The next step is to apply the original Holder inequality to and . Clearly, . But how do we know that ? Is it trivial to see that ?? I try to verify this, but not quite successful. (Note if is NOT in , then its integrate is . How can which suppose to be a finite number!! )","U \mathbb{R} 1 < p, q, r < \infty p^{-1} + q^{-1} + r^{-1} = 1 f \in L^p(U), g \in L^q(U) h \in L^r(U) ||fgh||_1 \leq ||f||_p||g||_q||h||_r. 1 < p,q < \infty f \in L^p(U) g \in L^q(U) ||fg||_1 \leq ||f||_p||g||_q. \textbf{Proof} s = (1/p + 1/q)^{-1}. 1/s + 1/r = 1. \int_U (fg)h dx \leq ||h||_r (\int_U (fg)^s)^{1/s}. (fg)^s \textbf{Question} s (fg) h h \in L^r fg \in L^s fg \in L^s fg L^s \infty \infty \leq ||f||_p||g||_q","['real-analysis', 'analysis', 'holder-inequality']"
93,How to compute $\int_{\frac{1-p}{2}}^{\frac{1+p}{2}}x^k(1-x)^kdx$?,How to compute ?,\int_{\frac{1-p}{2}}^{\frac{1+p}{2}}x^k(1-x)^kdx,"Is there a way to compute $$I=\int_{\frac{1-p}{2}}^{\frac{1+p}{2}}x^k(1-x)^kdx$$ for very large positive integer $k\gg 1$ and small $p\in (0,1)$ ? (one may assume $p$ satisfies $p=\Omega(1/\sqrt{k})$ ) I am trying to find a better upper bound on this integral than $p2^{-2k}$ . Especially, how far it is from the lower bound $p\Big(\frac{1-p}{2}\Big)^k\Big(\frac{1+p}{2}\Big)^k$ ? PS. Is it possible to bound by $2^{-2k}\exp(-\Omega(k^{1/2}))$ ?","Is there a way to compute for very large positive integer and small ? (one may assume satisfies ) I am trying to find a better upper bound on this integral than . Especially, how far it is from the lower bound ? PS. Is it possible to bound by ?","I=\int_{\frac{1-p}{2}}^{\frac{1+p}{2}}x^k(1-x)^kdx k\gg 1 p\in (0,1) p p=\Omega(1/\sqrt{k}) p2^{-2k} p\Big(\frac{1-p}{2}\Big)^k\Big(\frac{1+p}{2}\Big)^k 2^{-2k}\exp(-\Omega(k^{1/2}))","['calculus', 'integration', 'analysis']"
94,Limit point alternative definition,Limit point alternative definition,,"I have a question, why isn't an alternative definition of a $p \in X$ being a limit point of a set $E$ : $\forall r>0 N_r(p) \cap E \neq \emptyset$ , why does it have to be the punctured neighborhood? Note that $E$ is a subset of $X$ and $X$ is a metric space. Note that the definition of a limit point of a set E is a point $p \in X$ such that every neighborhood contains a point $q$ such that $q\neq p$ and $ q\in E$ .","I have a question, why isn't an alternative definition of a being a limit point of a set : , why does it have to be the punctured neighborhood? Note that is a subset of and is a metric space. Note that the definition of a limit point of a set E is a point such that every neighborhood contains a point such that and .",p \in X E \forall r>0 N_r(p) \cap E \neq \emptyset E X X p \in X q q\neq p  q\in E,"['real-analysis', 'general-topology', 'analysis', 'proof-verification', 'definition']"
95,simple proof $f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z)$,simple proof,f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z),"$f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z)$ So the thing that I need to do is to show that $f(2019) = \pi$ . So the thing that I have tried is to calculate $f(0), f(1)$ , and to see some kind of connection (or recurrence relation), so I can easily calculate $f(2019)$ , but I couldn't find any. Any tips?","So the thing that I need to do is to show that . So the thing that I have tried is to calculate , and to see some kind of connection (or recurrence relation), so I can easily calculate , but I couldn't find any. Any tips?","f(z) = \arcsin(\frac{2z}{(1+z^2)}) + 2\arctan(z) f(2019) = \pi f(0), f(1) f(2019)","['analysis', 'functions']"
96,"First mean value theorem of integration, $\xi \in (a,b)$ instead of $[a,b]$?","First mean value theorem of integration,  instead of ?","\xi \in (a,b) [a,b]","The first mean value theorem of integration states that If $f\in C[a,b]$ and $g\in \mathcal{R}[a,b]$ , $g$ is nonnegative, then $\exists \xi\in [a,b]$ such that $$\int_a^b (f\cdot g)(x) dx=f(\xi)\int_a^b g(x) dx.$$ Is it possible to replace $\exists \xi\in [a,b]$ by $\exists \xi\in (a,b)$ ? It is difficult to imagine how this could be wrong. Edit: added condition $g$ is nonnegative.","The first mean value theorem of integration states that If and , is nonnegative, then such that Is it possible to replace by ? It is difficult to imagine how this could be wrong. Edit: added condition is nonnegative.","f\in C[a,b] g\in \mathcal{R}[a,b] g \exists \xi\in [a,b] \int_a^b (f\cdot g)(x) dx=f(\xi)\int_a^b g(x) dx. \exists \xi\in [a,b] \exists \xi\in (a,b) g","['real-analysis', 'calculus', 'analysis']"
97,Prove $f(x) = x^2\sin\left(\frac{1}{x}\right)$ is Lipschitz (no use of derivative),Prove  is Lipschitz (no use of derivative),f(x) = x^2\sin\left(\frac{1}{x}\right),"Prove that $f:\mathbb{R}\to \mathbb{R}$ such that $$ f(x) = \left\{    \begin{array}{c l}     x^2\, \sin\left(\frac{1}{x}\right) & ,\quad x\neq 0\\     0 & ,\quad x=0   \end{array} \right.$$ is Lipschitz (without use of derivatives). Attempt . I am aware ( Lipschitz-continuous $f(x)=x^2\cdot \sin\left(\frac{1}{x}\right)$ )  that: $$|f(x)-f(y)|\leq 3|x-y| ~~~\forall~x,~y\in \mathbb{R},$$ but I am looking for a proof, without use of derivatives.  I tried: for $x,y\neq 0$ : \begin{eqnarray} x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y}  &=& (x^2-y^2)\sin\frac{1}{x} + y^2\left ( \sin\frac{1}{x} - \sin\frac{1}{y} \right ),\nonumber \end{eqnarray} so: $$\left | x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right | \leq |x^2 - y^2| + y^2 \left | \sin\frac{1}{x} - \sin\frac{1}{y} \right |.$$ Since: $$\left | \sin\frac{1}{x} - \sin\frac{1}{y} \right | \leq  \left| \frac{1}{x} - \frac{1}{y}\right |= \frac{|x - y|}{xy},$$ we get: $$\left | x^2 \sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right | \leq \left(x+y+\frac{y}{x}\right)|x-y|.$$ Unfortunatelly , the quantity $x+y+\frac{y}{x}$ grows to $+\infty$ , either for large $x$ , or for $x\approx 0.$ Thanks in advance for the help.","Prove that such that is Lipschitz (without use of derivatives). Attempt . I am aware ( Lipschitz-continuous )  that: but I am looking for a proof, without use of derivatives.  I tried: for : so: Since: we get: Unfortunatelly , the quantity grows to , either for large , or for Thanks in advance for the help.","f:\mathbb{R}\to \mathbb{R}  f(x) = \left\{ 
  \begin{array}{c l}
    x^2\, \sin\left(\frac{1}{x}\right) & ,\quad x\neq 0\\
    0 & ,\quad x=0
  \end{array} \right. f(x)=x^2\cdot \sin\left(\frac{1}{x}\right) |f(x)-f(y)|\leq 3|x-y| ~~~\forall~x,~y\in \mathbb{R}, x,y\neq 0 \begin{eqnarray}
x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y}
 &=& (x^2-y^2)\sin\frac{1}{x} + y^2\left ( \sin\frac{1}{x} - \sin\frac{1}{y} \right ),\nonumber
\end{eqnarray} \left | x^2\sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right |
\leq |x^2 - y^2| + y^2 \left | \sin\frac{1}{x} - \sin\frac{1}{y} \right |. \left | \sin\frac{1}{x} - \sin\frac{1}{y} \right | \leq  \left| \frac{1}{x} - \frac{1}{y}\right |= \frac{|x - y|}{xy}, \left | x^2 \sin\frac{1}{x} - y^2 \sin\frac{1}{y} \right |
\leq \left(x+y+\frac{y}{x}\right)|x-y|. x+y+\frac{y}{x} +\infty x x\approx 0.","['real-analysis', 'analysis', 'lipschitz-functions']"
98,"Is $\sum _{n=1} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 }$ uniformly convergent on $(-\pi , \pi)$?",Is  uniformly convergent on ?,"\sum _{n=1} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 } (-\pi , \pi)","Is this series uniformly convergent on $(-\pi , \pi)$ : $$\sum _{n=1} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 }\,?$$ My Attempt: If the series were convergent we would have got a natural number $k$ for a fixed $\epsilon>0$ such that $\sum _{n=k} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 } < \epsilon$ for all $x \in (-\pi , \pi)$ .  But for this case we will get the term $f_n$ in the summation greater than $1$ for $-\pi + 1/n$ .  So for every $n$ we will get a $x\in (-\pi , \pi) $ such that summation at $x$ is greater than $1$ . That's why the series is not uniformly convergent on $(-\pi , \pi)$ . Can somebody please tell me if I have gone wrong anywhere?",Is this series uniformly convergent on : My Attempt: If the series were convergent we would have got a natural number for a fixed such that for all .  But for this case we will get the term in the summation greater than for .  So for every we will get a such that summation at is greater than . That's why the series is not uniformly convergent on . Can somebody please tell me if I have gone wrong anywhere?,"(-\pi , \pi) \sum _{n=1} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 }\,? k \epsilon>0 \sum _{n=k} ^ \infty \frac{1}{(x+\pi)^2 \cdot n^2 } < \epsilon x \in (-\pi , \pi) f_n 1 -\pi + 1/n n x\in (-\pi , \pi)  x 1 (-\pi , \pi)","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'uniform-convergence']"
99,"Prove tht $f(x) = x^{2}$ is uniformly continuous on $\bigcup_{n = 1}^{\infty} [n, n + 1/n^{3}]$.",Prove tht  is uniformly continuous on .,"f(x) = x^{2} \bigcup_{n = 1}^{\infty} [n, n + 1/n^{3}]","How can I show that the function $f(x) = x^{2}$ is uniformly continuous on $\bigcup_{n = 1}^{\infty} [n, n + \frac{1}{n^{3}}]$ ? As $n \to \infty$ , I know that we get only all of the integers; however, the result doesn't make sense to me. I'm aware of both the sequential and $\epsilon-\delta$ definitions of continuity, but I didn't get anywhere with them.","How can I show that the function is uniformly continuous on ? As , I know that we get only all of the integers; however, the result doesn't make sense to me. I'm aware of both the sequential and definitions of continuity, but I didn't get anywhere with them.","f(x) = x^{2} \bigcup_{n = 1}^{\infty} [n, n + \frac{1}{n^{3}}] n \to \infty \epsilon-\delta","['real-analysis', 'analysis']"
