,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Splitting the action of functionals in duals of Sobolev spaces,Splitting the action of functionals in duals of Sobolev spaces,,"Update: After some more thinking and asking I've come to the conclusion that there is no reasonable way to achieve this for all possible $\varphi$ because of the mixed terms. I believe something useful can only be said under additional assumptions on the behaviour on the boundary both of $g$ and the $\varphi$. Let $\Omega \subset \mathbb{R}^n$ be a bounded Lipschitz domain (or at most $C^{1, 1}$) with boundary split in open, disjoint subsets $\Gamma_1, \ldots, \Gamma_k$ each with Lipschitz boundary as well. Let $g \in H^{- 1 / 2} ( \Gamma)$, the dual of the Hilbert space $H^{1 / 2} ( \Gamma) = W^{1 / 2, 2} ( \Gamma) = \{ v \in L^2 ( \Gamma) : ( v, v)_{1 / 2} < \infty \}$ where the scalar product is given by $$ ( u, v)_{1 / 2} := \int_{\Gamma} uv + \int_{\Gamma} \int_{\Gamma}    \frac{u ( x) v ( y)}{| x - y |^n} d x d y. $$ Question: Under what assumptions may I split the action of $g$ in a way like the   following?   $$ \langle g, \varphi \rangle_{- 1 / 2, \Gamma} \overset{!}{=} \sum_{i = 1}^k \langle g_i, \varphi_{| \Gamma_i} \rangle_{- 1 / 2, \Gamma_i}, $$   where the $g_i$ are somehow the ""restrictions"" of $g$, $\varphi$ may be   taken in $C^{\infty} ( \Gamma)$ and the $\langle \cdot, \cdot \rangle_{- 1 / 2, \Gamma_i}$ are the duality pairings of $H^{- 1 / 2} ( \Gamma_i) \times H^{1 / 2} ( \Gamma_i)$. Using the representation of $g$ with the scalar product of $H^{1 / 2} ( \Gamma)$ has led me nowhere because of the double integral, where mixed terms appear. I know that the $\Gamma_i$ being Lipschitz, there exist continuous embeddings $H^{1 / 2} ( \Gamma_i) \overset{E_i}{\hookrightarrow} H^{1 / 2} ( \Gamma)$ which allow to define for example $$ \langle g_i, \varphi_{| \Gamma_i} \rangle_{- 1 / 2, \Gamma_i} :=    \langle g, E_i ( \varphi_{| \Gamma_i}) \rangle_{- 1 / 2, \Gamma}, $$ but I think this doesn't achieve much because the extensions with $E_i$ aren't extensions by zero. This somehow reflects the same situation as with the ""mixed terms"" in the double integral. Help anyone? Thanks!","Update: After some more thinking and asking I've come to the conclusion that there is no reasonable way to achieve this for all possible $\varphi$ because of the mixed terms. I believe something useful can only be said under additional assumptions on the behaviour on the boundary both of $g$ and the $\varphi$. Let $\Omega \subset \mathbb{R}^n$ be a bounded Lipschitz domain (or at most $C^{1, 1}$) with boundary split in open, disjoint subsets $\Gamma_1, \ldots, \Gamma_k$ each with Lipschitz boundary as well. Let $g \in H^{- 1 / 2} ( \Gamma)$, the dual of the Hilbert space $H^{1 / 2} ( \Gamma) = W^{1 / 2, 2} ( \Gamma) = \{ v \in L^2 ( \Gamma) : ( v, v)_{1 / 2} < \infty \}$ where the scalar product is given by $$ ( u, v)_{1 / 2} := \int_{\Gamma} uv + \int_{\Gamma} \int_{\Gamma}    \frac{u ( x) v ( y)}{| x - y |^n} d x d y. $$ Question: Under what assumptions may I split the action of $g$ in a way like the   following?   $$ \langle g, \varphi \rangle_{- 1 / 2, \Gamma} \overset{!}{=} \sum_{i = 1}^k \langle g_i, \varphi_{| \Gamma_i} \rangle_{- 1 / 2, \Gamma_i}, $$   where the $g_i$ are somehow the ""restrictions"" of $g$, $\varphi$ may be   taken in $C^{\infty} ( \Gamma)$ and the $\langle \cdot, \cdot \rangle_{- 1 / 2, \Gamma_i}$ are the duality pairings of $H^{- 1 / 2} ( \Gamma_i) \times H^{1 / 2} ( \Gamma_i)$. Using the representation of $g$ with the scalar product of $H^{1 / 2} ( \Gamma)$ has led me nowhere because of the double integral, where mixed terms appear. I know that the $\Gamma_i$ being Lipschitz, there exist continuous embeddings $H^{1 / 2} ( \Gamma_i) \overset{E_i}{\hookrightarrow} H^{1 / 2} ( \Gamma)$ which allow to define for example $$ \langle g_i, \varphi_{| \Gamma_i} \rangle_{- 1 / 2, \Gamma_i} :=    \langle g, E_i ( \varphi_{| \Gamma_i}) \rangle_{- 1 / 2, \Gamma}, $$ but I think this doesn't achieve much because the extensions with $E_i$ aren't extensions by zero. This somehow reflects the same situation as with the ""mixed terms"" in the double integral. Help anyone? Thanks!",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
1,The control of norm in quotient algebra,The control of norm in quotient algebra,,"Let $B_1,B_2$ be two Banach spaces and $L(B_i,B_j),K(B_i,B_j)(i,j=1,2)$ spaces of bounded and compact linear operator between them respectively. If $T \in L(B_1,B_1)$, we have a $S \in K(B_1,B_2)$ and a constant $c>0$ such that for any $v \in B_1$,$${\left\| {Tv} \right\|_{{B_1}}} \le c{\left\| v \right\|_{{B_1}}} + {\left\| {Sv} \right\|_{{B_2}}}.$$ My question is, can we find a $A \in K(B_1,B_1)$, such that ${\left\| {T - A} \right\|_{L({B_1},{B_1})}} \le c$?","Let $B_1,B_2$ be two Banach spaces and $L(B_i,B_j),K(B_i,B_j)(i,j=1,2)$ spaces of bounded and compact linear operator between them respectively. If $T \in L(B_1,B_1)$, we have a $S \in K(B_1,B_2)$ and a constant $c>0$ such that for any $v \in B_1$,$${\left\| {Tv} \right\|_{{B_1}}} \le c{\left\| v \right\|_{{B_1}}} + {\left\| {Sv} \right\|_{{B_2}}}.$$ My question is, can we find a $A \in K(B_1,B_1)$, such that ${\left\| {T - A} \right\|_{L({B_1},{B_1})}} \le c$?",,"['functional-analysis', 'banach-spaces', 'operator-theory', 'compact-operators']"
2,Borel - Caratheodory Inequality,Borel - Caratheodory Inequality,,"If $f$ is a complex-valued function analytic on $\{z:\vert z \vert \leqq r \}$, then for $\vert z \vert <r $, $$ \vert f(z) \vert \leqq \frac{2\vert z \vert}{2-\vert z \vert} \sup\{\Re f(w): \vert w \vert=\vert z \vert\}+\frac{r+\vert z \vert}{r-\vert z \vert}\vert f(0) \vert. $$","If $f$ is a complex-valued function analytic on $\{z:\vert z \vert \leqq r \}$, then for $\vert z \vert <r $, $$ \vert f(z) \vert \leqq \frac{2\vert z \vert}{2-\vert z \vert} \sup\{\Re f(w): \vert w \vert=\vert z \vert\}+\frac{r+\vert z \vert}{r-\vert z \vert}\vert f(0) \vert. $$",,"['functional-analysis', 'spectral-theory']"
3,"Application of a result on some bounded functionals on a subspace of $C([0,1])$",Application of a result on some bounded functionals on a subspace of,"C([0,1])","The following result was proved in a previous post: Bounded functionals on Banach spaces. Let $(X, \|.\|)$ be a Banach space such that $X \subset C([0,1]) $ For every $r\in \mathbb{Q}\cap[0,1], f\mapsto f(r)$ defines a bounded linear functional on $X$. There exists a $C>0$ such that, for all $f\in X$, $$\sup_{x\in[0,1]} |f(x)| \leq C\|f\|.$$ Question: Does anyone know an example of space $X$ where this result is interesting? Indeed I feel that the example above could be a very nice application of the Banach-Steinhaus theorem, but the examples of spaces $X$ I thought of were too simple: One could easily prove the result without the Banach-Steinhaus theorem. The assumption on boundedness would be easy to prove for all $r$ in $[0,1]$. If someone has an example of space $X$ satisfing the first point, even without the second one, I am already interested.","The following result was proved in a previous post: Bounded functionals on Banach spaces. Let $(X, \|.\|)$ be a Banach space such that $X \subset C([0,1]) $ For every $r\in \mathbb{Q}\cap[0,1], f\mapsto f(r)$ defines a bounded linear functional on $X$. There exists a $C>0$ such that, for all $f\in X$, $$\sup_{x\in[0,1]} |f(x)| \leq C\|f\|.$$ Question: Does anyone know an example of space $X$ where this result is interesting? Indeed I feel that the example above could be a very nice application of the Banach-Steinhaus theorem, but the examples of spaces $X$ I thought of were too simple: One could easily prove the result without the Banach-Steinhaus theorem. The assumption on boundedness would be easy to prove for all $r$ in $[0,1]$. If someone has an example of space $X$ satisfing the first point, even without the second one, I am already interested.",,"['functional-analysis', 'banach-spaces', 'operator-theory', 'examples-counterexamples']"
4,$(\lambda-a)^{-1}$ as limits of 'polynomials',as limits of 'polynomials',(\lambda-a)^{-1},"For a unital $C^*$-algebra $\mathcal{A}$ the spectral permanence gives \begin{equation} \sigma_{\mathcal{B}}(a)=\sigma_{\mathcal{A}}(a) \end{equation} for any unital $C^*$-subalgebra $\mathcal{B}$. It is natural to look at the smallest such subalgebras, namely, the $C^*$-subalgebra generated by $1,a$ and $a^*$. Then the permanence says if $\lambda-a$ is invertible, then $\lambda-a$ is in the closed linear span of products of $1,a$ and $a^*$ (although order of multiplications matters here and it is not actually a polynomial). I am wondering whether there is some canonical way to construct these 'polynomials'. That is, given $a\in\mathcal{A}$ invertible, how can one find explicitly the linear span of products of $1,a$ and $a^*$ that converges to $a^{-1}$? Thanks!","For a unital $C^*$-algebra $\mathcal{A}$ the spectral permanence gives \begin{equation} \sigma_{\mathcal{B}}(a)=\sigma_{\mathcal{A}}(a) \end{equation} for any unital $C^*$-subalgebra $\mathcal{B}$. It is natural to look at the smallest such subalgebras, namely, the $C^*$-subalgebra generated by $1,a$ and $a^*$. Then the permanence says if $\lambda-a$ is invertible, then $\lambda-a$ is in the closed linear span of products of $1,a$ and $a^*$ (although order of multiplications matters here and it is not actually a polynomial). I am wondering whether there is some canonical way to construct these 'polynomials'. That is, given $a\in\mathcal{A}$ invertible, how can one find explicitly the linear span of products of $1,a$ and $a^*$ that converges to $a^{-1}$? Thanks!",,"['functional-analysis', 'banach-spaces', 'spectral-theory', 'banach-algebras', 'c-star-algebras']"
5,Proving continuity on spaces of distributions?,Proving continuity on spaces of distributions?,,"Let $\mathcal{D}'(\Omega)$ be the space of distributions on an open set $\Omega$, and $\mathcal{E}'(\Omega)$ the compactly supported ones. When you have a linear operator $T:\mathcal{D}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ or $T:\mathcal{E}'(\Omega)\rightarrow\mathcal{E}'(\Omega)$, I often find it easy to prove that convergent sequences get mapped to convergent sequences (in the weak topology), but proving continuity with respect to the weak topology is much harder. Since these spaces are not first-countable (I think), one must work with a local basis around 0. For example, if you have a pseudodifferential operator $P:\mathcal{E}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ which is properly supported, then it continuously extends to a (necessarily unique) operator $P:\mathcal{D}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ and maps $\mathcal{E}'(\Omega)$ continuously into $\mathcal{E}'(\Omega)$. I was able to prove the first statement, but I was able to prove the second statement only in terms of sequential continuity (and even this was quite difficult). In many textbooks involving distributions, authors don't seem to be careful about this. They give an argument proving sequential continuity and claim that it is continuous in the more general sense. Is there a general scheme for converting sequential continuity arguments to actual continuity arguments for these spaces? Or can you point me to a detailed proof of the aforementioned fact about proper PDOs?","Let $\mathcal{D}'(\Omega)$ be the space of distributions on an open set $\Omega$, and $\mathcal{E}'(\Omega)$ the compactly supported ones. When you have a linear operator $T:\mathcal{D}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ or $T:\mathcal{E}'(\Omega)\rightarrow\mathcal{E}'(\Omega)$, I often find it easy to prove that convergent sequences get mapped to convergent sequences (in the weak topology), but proving continuity with respect to the weak topology is much harder. Since these spaces are not first-countable (I think), one must work with a local basis around 0. For example, if you have a pseudodifferential operator $P:\mathcal{E}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ which is properly supported, then it continuously extends to a (necessarily unique) operator $P:\mathcal{D}'(\Omega)\rightarrow\mathcal{D}'(\Omega)$ and maps $\mathcal{E}'(\Omega)$ continuously into $\mathcal{E}'(\Omega)$. I was able to prove the first statement, but I was able to prove the second statement only in terms of sequential continuity (and even this was quite difficult). In many textbooks involving distributions, authors don't seem to be careful about this. They give an argument proving sequential continuity and claim that it is continuous in the more general sense. Is there a general scheme for converting sequential continuity arguments to actual continuity arguments for these spaces? Or can you point me to a detailed proof of the aforementioned fact about proper PDOs?",,"['functional-analysis', 'continuity', 'differential-operators']"
6,Confused by a proof in Rudin *Functional Analysis*,Confused by a proof in Rudin *Functional Analysis*,,"I am reading Rudin's Functional Analysis and got quite confused by his proof of Thm 8.5, that is, the existence of fundamental solutions for differential operator $P(D)$, where $P$ is a polynomial. The proof begins with defining a norm on the space of test functions $D(\mathbb{R}^d)$ by \begin{equation}\|\psi\|=\int_{T^d}\int_{\mathbb{R}^d}|\hat{\psi}(t+r\omega)|dm_d(t)d\sigma_d(\omega),\end{equation}where $r>0$ is fixed, $T^d$ is the torus in $\mathbb{C}^d$, $\sigma_d$ the Haar measure for this torus, and $m_d$ is the Lebesgue measure for $\mathbb{R}^d$. Then the proof goes on well, and this norm is used to prove the existence of a distribution, that is, our fundamental solution. However, I do not know how Rudin thinks about this strange norm. It reminds of the norm on Sobolev spaces but you do not have the integration over torus there. Can somebody give some reference or background information about this norm? Thanks!","I am reading Rudin's Functional Analysis and got quite confused by his proof of Thm 8.5, that is, the existence of fundamental solutions for differential operator $P(D)$, where $P$ is a polynomial. The proof begins with defining a norm on the space of test functions $D(\mathbb{R}^d)$ by \begin{equation}\|\psi\|=\int_{T^d}\int_{\mathbb{R}^d}|\hat{\psi}(t+r\omega)|dm_d(t)d\sigma_d(\omega),\end{equation}where $r>0$ is fixed, $T^d$ is the torus in $\mathbb{C}^d$, $\sigma_d$ the Haar measure for this torus, and $m_d$ is the Lebesgue measure for $\mathbb{R}^d$. Then the proof goes on well, and this norm is used to prove the existence of a distribution, that is, our fundamental solution. However, I do not know how Rudin thinks about this strange norm. It reminds of the norm on Sobolev spaces but you do not have the integration over torus there. Can somebody give some reference or background information about this norm? Thanks!",,"['complex-analysis', 'functional-analysis', 'ordinary-differential-equations', 'operator-theory']"
7,Conditions for the sequence being weakly convergent,Conditions for the sequence being weakly convergent,,"Let $H=\ell_2$ be the Hilbert space of the square-summable sequences where $$ \langle x,y\rangle=\sum_{i=1}^{\infty}x_iy_i, \quad \|x\|=\sqrt{\langle x,x\rangle}. $$ Let $F: H\rightarrow H$ be an affine mapping , i.e. $$ F[\lambda u+(1-\lambda)v]=\lambda F(u)+(1-\lambda)F(v), \quad \forall u,v\in H, \lambda\in \mathbb{R}. $$  Let $\{u^k\}$ be a sequence given by $$ u^{k+1}=F(u^k) \quad k\in\mathbb{N}, $$ where $u^0$ is an any point in $H$. Find the conditions on $F$ and $u^0$ such that $\{u^k\}$ is weakly convergent but not strongly convergent. Example. If $u^0=(1,0,0,\ldots,0,\ldots)$ and $F(u)$ is given by $$ F(u)=(0,u_1, u_2, \ldots, u_n, \ldots) \quad \forall u=(u_1,u_2,\ldots, u_n, \ldots)\in H. $$ The sequence $\{u^k\}$ generated by the formula $u^{k+1}=F(u^k)$ is given by $$ u^0=(1,0,\ldots, 0,\ldots), \quad u^1=(0,1,0,\ldots, 0, \ldots), \ldots, u^n=(0,0,\ldots, 1, 0, \ldots),\ldots $$ is weakly convergent but not strongly convergent to $0\in H$.","Let $H=\ell_2$ be the Hilbert space of the square-summable sequences where $$ \langle x,y\rangle=\sum_{i=1}^{\infty}x_iy_i, \quad \|x\|=\sqrt{\langle x,x\rangle}. $$ Let $F: H\rightarrow H$ be an affine mapping , i.e. $$ F[\lambda u+(1-\lambda)v]=\lambda F(u)+(1-\lambda)F(v), \quad \forall u,v\in H, \lambda\in \mathbb{R}. $$  Let $\{u^k\}$ be a sequence given by $$ u^{k+1}=F(u^k) \quad k\in\mathbb{N}, $$ where $u^0$ is an any point in $H$. Find the conditions on $F$ and $u^0$ such that $\{u^k\}$ is weakly convergent but not strongly convergent. Example. If $u^0=(1,0,0,\ldots,0,\ldots)$ and $F(u)$ is given by $$ F(u)=(0,u_1, u_2, \ldots, u_n, \ldots) \quad \forall u=(u_1,u_2,\ldots, u_n, \ldots)\in H. $$ The sequence $\{u^k\}$ generated by the formula $u^{k+1}=F(u^k)$ is given by $$ u^0=(1,0,\ldots, 0,\ldots), \quad u^1=(0,1,0,\ldots, 0, \ldots), \ldots, u^n=(0,0,\ldots, 1, 0, \ldots),\ldots $$ is weakly convergent but not strongly convergent to $0\in H$.",,"['functional-analysis', 'hilbert-spaces']"
8,Proofing an inequality with a slowly varying function,Proofing an inequality with a slowly varying function,,"I am working right now with ""Independent and Stationary Sequences of Random Variables"" from Ibragimov 1971. I am trying to understand the proof of the following Lemma (18.2.4): $h: \mathbb N \rightarrow \mathbb R$ is a slowly varying function, i.e. for all $a > 0$ $$ \lim_{n\to\infty}\frac{h(an)}{h(n)}=1. $$ For all sufficiently small $c$ and all sufficiently large $n$, $$ \frac{h(cn)}{h(n)} < c^{-\frac 1 2}. $$ We remark that this inequality holds for all $c<c_0$ where $c_0$ does not depend on $n$. Proof: From what has been proved about $h(n)$, [I come to this later] \begin{align*} \log \frac{h(cn)}{h(n)} & = \sum_{k=0}^{\lfloor-\frac{\log c}{\log 2}\rfloor} \log h(\lfloor2^{-k-1}n\rfloor) - \log h(\lfloor2^{-k}n\rfloor) + \log h(cn) - \log h(\lfloor2^{-\lfloor \frac{\log c}{\log 2}\rfloor} n\rfloor) \\ & < \frac 1 2 \log c^{-1}. \end{align*} Thats what is in the book. The first ""="" is a telescoping sum, but the last term of the equation should be  $\log h(\lfloor2^{\lfloor \frac{\log c}{\log 2}\rfloor} n\rfloor)$ because for all $x < 0, x \notin \mathbb Z$ $$ -\lfloor -x \rfloor - 1 = \lfloor x \rfloor. $$ Note that $2^{\frac{\log c}{\log 2}} = c$ and every term of the series goes to zero, since $\lim_{n\to\infty}\frac{h(2n)}{h(n)}=1$. But I dont understand the ""<"". Do you have an idea? Here is what has been proved about $h$ before this lemma:(I couldn't use these information) $$ \text{for all } \varepsilon > 0: \\  \lim n^\varepsilon h(n) = \infty \\ \lim n^{-\varepsilon} h(n) = 0. $$ $$ \text{If $n$ is sufficiently large, then } \\ \sup_{n\le r\le 2n} \frac{h(n)}{h(r)} \le 4. $$","I am working right now with ""Independent and Stationary Sequences of Random Variables"" from Ibragimov 1971. I am trying to understand the proof of the following Lemma (18.2.4): $h: \mathbb N \rightarrow \mathbb R$ is a slowly varying function, i.e. for all $a > 0$ $$ \lim_{n\to\infty}\frac{h(an)}{h(n)}=1. $$ For all sufficiently small $c$ and all sufficiently large $n$, $$ \frac{h(cn)}{h(n)} < c^{-\frac 1 2}. $$ We remark that this inequality holds for all $c<c_0$ where $c_0$ does not depend on $n$. Proof: From what has been proved about $h(n)$, [I come to this later] \begin{align*} \log \frac{h(cn)}{h(n)} & = \sum_{k=0}^{\lfloor-\frac{\log c}{\log 2}\rfloor} \log h(\lfloor2^{-k-1}n\rfloor) - \log h(\lfloor2^{-k}n\rfloor) + \log h(cn) - \log h(\lfloor2^{-\lfloor \frac{\log c}{\log 2}\rfloor} n\rfloor) \\ & < \frac 1 2 \log c^{-1}. \end{align*} Thats what is in the book. The first ""="" is a telescoping sum, but the last term of the equation should be  $\log h(\lfloor2^{\lfloor \frac{\log c}{\log 2}\rfloor} n\rfloor)$ because for all $x < 0, x \notin \mathbb Z$ $$ -\lfloor -x \rfloor - 1 = \lfloor x \rfloor. $$ Note that $2^{\frac{\log c}{\log 2}} = c$ and every term of the series goes to zero, since $\lim_{n\to\infty}\frac{h(2n)}{h(n)}=1$. But I dont understand the ""<"". Do you have an idea? Here is what has been proved about $h$ before this lemma:(I couldn't use these information) $$ \text{for all } \varepsilon > 0: \\  \lim n^\varepsilon h(n) = \infty \\ \lim n^{-\varepsilon} h(n) = 0. $$ $$ \text{If $n$ is sufficiently large, then } \\ \sup_{n\le r\le 2n} \frac{h(n)}{h(r)} \le 4. $$",,"['real-analysis', 'functional-analysis']"
9,What is a good resource for functional derivatives and functional determinants?,What is a good resource for functional derivatives and functional determinants?,,"What is a good resource for functional derivatives, functional determinants,  etc.? What is the branch of mathematics dealing with those things? It is not in my functional analysis book. What is a good resource on functionals in general?","What is a good resource for functional derivatives, functional determinants,  etc.? What is the branch of mathematics dealing with those things? It is not in my functional analysis book. What is a good resource on functionals in general?",,"['functional-analysis', 'reference-request', 'online-resources']"
10,A compact operator is bounded,A compact operator is bounded,,"I'd like to show that a compact linear operator $T: X \to Y$ between normed spaces is bounded. Can you tell me if this is right? If $T$ is compact, then the closure of the image of $B(0,1)$ and hence of $B(0,n)$ is compact. Then $\|T\| = \sup_{\|x\| = 1} \|Tx\| \leq \sup_{x \in B(0,2)} \|Tx\| \leq K$ for some $K \in \mathbb R_{\geq 0}$ since $TB(0,2) \subset \overline{TB(0,2)}$ and $\overline{TB(0,2)}$ is compact hence bounded.","I'd like to show that a compact linear operator between normed spaces is bounded. Can you tell me if this is right? If is compact, then the closure of the image of and hence of is compact. Then for some since and is compact hence bounded.","T: X \to Y T B(0,1) B(0,n) \|T\| = \sup_{\|x\| = 1} \|Tx\| \leq \sup_{x \in B(0,2)} \|Tx\| \leq K K \in \mathbb R_{\geq 0} TB(0,2) \subset \overline{TB(0,2)} \overline{TB(0,2)}",['functional-analysis']
11,Prove that a flat shape minimizes a functional,Prove that a flat shape minimizes a functional,,"The following functional arises in an information theoretic problem that I work on currently. $$I(G(\omega)) = \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}d\omega-\frac{\left| \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}\exp(-i\omega)d\omega\right|^2}{ \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}d\omega},$$ where $\kappa<1$, $A>0$, and $G(\omega)\geq 0$. Now I would like to minimize $I(G(\omega))$ under the constraint of unit area of $G(\omega)$, i.e., $$\int_{-\kappa \pi}^{\kappa \pi} G(\omega)d\omega=1.$$ My hypothesis is that a flat $G(\omega)=1/2\kappa\pi$ is optimal, but I cannot prove that (Matlab hints towards it). I can take the functional derivative and then apply the Lagrange multiplier method. But then I get stuck. A flat shape appears to be a stationary point (the functional derivative is zero), but how to prove global optimality? Is $I$ perhaps convex...?","The following functional arises in an information theoretic problem that I work on currently. $$I(G(\omega)) = \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}d\omega-\frac{\left| \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}\exp(-i\omega)d\omega\right|^2}{ \int_{-\kappa\pi}^{\kappa\pi} \frac{A}{G(\omega)+A}d\omega},$$ where $\kappa<1$, $A>0$, and $G(\omega)\geq 0$. Now I would like to minimize $I(G(\omega))$ under the constraint of unit area of $G(\omega)$, i.e., $$\int_{-\kappa \pi}^{\kappa \pi} G(\omega)d\omega=1.$$ My hypothesis is that a flat $G(\omega)=1/2\kappa\pi$ is optimal, but I cannot prove that (Matlab hints towards it). I can take the functional derivative and then apply the Lagrange multiplier method. But then I get stuck. A flat shape appears to be a stationary point (the functional derivative is zero), but how to prove global optimality? Is $I$ perhaps convex...?",,"['functional-analysis', 'calculus-of-variations']"
12,Continuous spectral value of right shift operator $\ell^2(\mathbb{N})$,Continuous spectral value of right shift operator,\ell^2(\mathbb{N}),"Let $T:\ell^2(\mathbb{N} \to \ell^2(\mathbb{N})$ be the operator that sends$(x_1,x_2,x_3,...) \to (0,x_1,x_2,x_3,....)$. I want to show that $\lambda = 1$ is in the continuous spectrum. To approach this, I first showed that $\lambda$ cannot be an eigenvalue. If it were,  $Tx = x$ for some $x \in \ell^2$, which would imply $(0,x_1,x_2,x_3,....) = (x_1,x_2,x_3,....)$ and hence we have $x_1 = 0$, $x_2 = x_1$, $x_2 = x_3$, .... which implies $x = 0$. So, $\lambda = 1$ is not an eigenvalue. It is not in the residual spectrum, since the range of $T-I$ is dense. If it weren't, then its adjoint should have nonzero kernel, so Ker$(T^*-I)$ should be nonempty. However, $(T^* -I)x = 0$ implies $(x_1-x_2,x_2-x_3,x_3-x_4,... ) = 0$, so $x_1 = x_2 = x_3 = ....$. This will only be in $\ell^2$ if $x_1 = 0$, so $x = 0$, so the kernel is just $x = 0$, so we have the range of $T-I$ must be dense in $\ell^2$. Now, I get a bit stuck. I know so far that $\lambda = 1$ is neither an eigenvalue nor in the residual spectrum. To show that $\lambda = 1$ is in the continuous spectrum, I need to show that $(T-I)^{-1}$ is unbounded. I considered actually computing explicitly computing the inverse and showing that I can find $y_n \in \ell^2$ such that $||(T-I)^{-1} y_n||$ grows arbitrarily large (with $||y_n|| = 1$).  Here was my attempt, but it doesn't seem correct. Since $(T-I)x = (-x_1, x_1 - x_2, x_2, - x_3, ...)$, if we have $(T-I)^{-1} y = x$, then we know that $y_1 = -x_1$. We next see that $y_2 = x_1 - x_2$, so $x_2 = x_1 - y_2 = -y_ 1- y_2$. Next, we have $y_3 = x_2 - x_3$, so $x_3 = x_2 - y_3 = -y_1 - y_2 - y_3$. This pattern seems to repeat, so it appears that I find that the $j$-th entry of  $(T-I)^{-1} y = \sum_{i=1}^j -y_i$. If I try taking the norm of this, I find $||(T-I)^{-1}y||^2 = \sum_{j=1}^{\infty} ( \sum_{i=1}^j -y_i)^2$. My guess from here was to consider $y_k$ to be the sequence $(\frac{1}{n}$ where $n$ goes from $1$ to $k$ and zeros otherwise. Then, I have $y_k$ is bounded above by $\sqrt{\frac{\pi^2}{6}}$ in the norm (summing the series $\sum \frac{1}{n^2}$), but when I compute $||(T-I)^{-1}y_k||^2 = \sum_{j=1}^k (\sum_{i=1}^j -\frac{1}{i} )^2$, each component has a harmonic series term, so the norm of this thing should be growing very large and get larger as $k \to \infty$. So, even though $||y_k||^2 \leq \frac{\pi^2}{6}$, the  norms of $(T-I)^{-1} y_k$ grow arbitrarily large, so $(T-I)^{-1}$ is unbounded. Am I on the right track at least? If anyone has any suggestions on a better way to approach the problem, that would be very helpful. Is there a ""general"" approach to showing that a value is in the continuous spectrum? If anyone has a suggestion on a textbook that discusses these problems, that would also be quite helpful. Thanks!","Let $T:\ell^2(\mathbb{N} \to \ell^2(\mathbb{N})$ be the operator that sends$(x_1,x_2,x_3,...) \to (0,x_1,x_2,x_3,....)$. I want to show that $\lambda = 1$ is in the continuous spectrum. To approach this, I first showed that $\lambda$ cannot be an eigenvalue. If it were,  $Tx = x$ for some $x \in \ell^2$, which would imply $(0,x_1,x_2,x_3,....) = (x_1,x_2,x_3,....)$ and hence we have $x_1 = 0$, $x_2 = x_1$, $x_2 = x_3$, .... which implies $x = 0$. So, $\lambda = 1$ is not an eigenvalue. It is not in the residual spectrum, since the range of $T-I$ is dense. If it weren't, then its adjoint should have nonzero kernel, so Ker$(T^*-I)$ should be nonempty. However, $(T^* -I)x = 0$ implies $(x_1-x_2,x_2-x_3,x_3-x_4,... ) = 0$, so $x_1 = x_2 = x_3 = ....$. This will only be in $\ell^2$ if $x_1 = 0$, so $x = 0$, so the kernel is just $x = 0$, so we have the range of $T-I$ must be dense in $\ell^2$. Now, I get a bit stuck. I know so far that $\lambda = 1$ is neither an eigenvalue nor in the residual spectrum. To show that $\lambda = 1$ is in the continuous spectrum, I need to show that $(T-I)^{-1}$ is unbounded. I considered actually computing explicitly computing the inverse and showing that I can find $y_n \in \ell^2$ such that $||(T-I)^{-1} y_n||$ grows arbitrarily large (with $||y_n|| = 1$).  Here was my attempt, but it doesn't seem correct. Since $(T-I)x = (-x_1, x_1 - x_2, x_2, - x_3, ...)$, if we have $(T-I)^{-1} y = x$, then we know that $y_1 = -x_1$. We next see that $y_2 = x_1 - x_2$, so $x_2 = x_1 - y_2 = -y_ 1- y_2$. Next, we have $y_3 = x_2 - x_3$, so $x_3 = x_2 - y_3 = -y_1 - y_2 - y_3$. This pattern seems to repeat, so it appears that I find that the $j$-th entry of  $(T-I)^{-1} y = \sum_{i=1}^j -y_i$. If I try taking the norm of this, I find $||(T-I)^{-1}y||^2 = \sum_{j=1}^{\infty} ( \sum_{i=1}^j -y_i)^2$. My guess from here was to consider $y_k$ to be the sequence $(\frac{1}{n}$ where $n$ goes from $1$ to $k$ and zeros otherwise. Then, I have $y_k$ is bounded above by $\sqrt{\frac{\pi^2}{6}}$ in the norm (summing the series $\sum \frac{1}{n^2}$), but when I compute $||(T-I)^{-1}y_k||^2 = \sum_{j=1}^k (\sum_{i=1}^j -\frac{1}{i} )^2$, each component has a harmonic series term, so the norm of this thing should be growing very large and get larger as $k \to \infty$. So, even though $||y_k||^2 \leq \frac{\pi^2}{6}$, the  norms of $(T-I)^{-1} y_k$ grow arbitrarily large, so $(T-I)^{-1}$ is unbounded. Am I on the right track at least? If anyone has any suggestions on a better way to approach the problem, that would be very helpful. Is there a ""general"" approach to showing that a value is in the continuous spectrum? If anyone has a suggestion on a textbook that discusses these problems, that would also be quite helpful. Thanks!",,"['functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory']"
13,discontinuous Linear operator from a Banach space to a normed vector space,discontinuous Linear operator from a Banach space to a normed vector space,,How do I find that example of a discontinuous linear operator A from a Banach space to a normed vector space such that A has a closed graph?,How do I find that example of a discontinuous linear operator A from a Banach space to a normed vector space such that A has a closed graph?,,['functional-analysis']
14,Nonnegative linear functionals over $l^\infty$,Nonnegative linear functionals over,l^\infty,"My purpose is a clarification of the role of the axiom of choice in constructing limits for bounded sequences. Namely, we want a linear functional of norm 1 defined on the space of all bounded complex sequences that takes nonnegative sequences to nonnegative numbers. We also want that it annihilate all sequences with finite number of nonzero elements, and that it take constant sequences to the same constant. It is well known that such functionals can be realized as limits along free ultrafilters, thus the axiom of choice is needed. I would like to understand if the construction can be simplified if, instead of the ultrafilters, we allow functionals with properties as described above. Here are my questions: Does there exist an explicit construction of such a functional? If not, does this mean that there are formal reasons why the explicit construction cannot exist? Is it possible to establish the fact that the functional exists without using the axiom of choice? Update after discussion: It is important that there are different ""levels"" of the axiom of choice. If I felt this better when I wrote this posting, I would also add the following question answered below in the affirmative: Is it possible to construct the functional as desired without ultrafilters, but with using only the Hahn-Banach theorem for the space $l^\infty$? This question looks more natural and elementary than the list of my questions, but the path sometimes gives you more than the goal. Many thanks to you all for the very helphul and interesting discussion!","My purpose is a clarification of the role of the axiom of choice in constructing limits for bounded sequences. Namely, we want a linear functional of norm 1 defined on the space of all bounded complex sequences that takes nonnegative sequences to nonnegative numbers. We also want that it annihilate all sequences with finite number of nonzero elements, and that it take constant sequences to the same constant. It is well known that such functionals can be realized as limits along free ultrafilters, thus the axiom of choice is needed. I would like to understand if the construction can be simplified if, instead of the ultrafilters, we allow functionals with properties as described above. Here are my questions: Does there exist an explicit construction of such a functional? If not, does this mean that there are formal reasons why the explicit construction cannot exist? Is it possible to establish the fact that the functional exists without using the axiom of choice? Update after discussion: It is important that there are different ""levels"" of the axiom of choice. If I felt this better when I wrote this posting, I would also add the following question answered below in the affirmative: Is it possible to construct the functional as desired without ultrafilters, but with using only the Hahn-Banach theorem for the space $l^\infty$? This question looks more natural and elementary than the list of my questions, but the path sometimes gives you more than the goal. Many thanks to you all for the very helphul and interesting discussion!",,"['functional-analysis', 'set-theory', 'banach-spaces', 'axiom-of-choice', 'constructive-mathematics']"
15,Definitions of weak (topological) mixing,Definitions of weak (topological) mixing,,"Let $X$ be a compact (metric) space and $T:X\rightarrow X$ be a continuous map.  Let $U_T:C(X)\rightarrow C(X)$ be the linear operator $U_T(f) = f\circ T$ . Then Wikipedia (see http://en.wikipedia.org/wiki/Weak_mixing#Topological_mixing ) (vaguely) says that $T$ is weak topological mixing if, whenever $U_t(f)=\lambda f$ for some $\lambda\in\mathbb C$ and $f\in C(X)$ , then $f$ is a constant function. But Terry Tao (see Definition 3 of http://terrytao.wordpress.com/2008/01/28/254a-lecture-7-structural-theory-of-topological-dynamical-systems/ ) says that $T$ is topologically weakly mixing if $T\times T$ is topologically transitive , that is, if $U,V\subseteq X\times X$ are open then there is $n\in\mathbb Z$ with $(T\times T)^n(U)\cap V\not=\emptyset$ .  I guess this is equivalent to saying that for $A,B,C,D\subseteq X$ open we can find one $n\in\mathbb Z$ with both $T^n(A)\cap B$ and $T^n(C)\cap D$ non-empty. Unfortunately, if I look in e.g. Brin and Stuck's book, then topologically transitive is defined to mean that for some $(x,y)\in X\times X$ , the forward orbit $\{ (T^n(x),T^n(y)) : n\geq 1 \}$ is dense in $X\times X$ . ( Edit: Thinking about this, B&S, Prop 2.2.1 shows that Tao's definition implies the B&S definition; and, at least if $T$ is a homeomorphism, the converse holds.  But it doesn't seem to if $T$ is not surjective). Is the definition of mixing involving $U_T$ equivalent to the usual one?  If so, can anyone supply a reference or a sketch proof?  I am beginning to think that Wikipedia has confused topological and measure-theoretic mixing. If they are equivalent, do I really need $T$ to be a homeomorphism?  (One could either replace $\mathbb Z$ by $\mathbb N$ in Tao's definition, or let $T^{-1}$ be the inverse image, and iterate).  Do I need $X$ to be metric? Edit: The reference Willie found gives the following: Suppose $T$ is a homeomorphism and $T\times T$ is minimal in the B&S sense.  Then if $f\in C(X)$ with $f\circ T = \lambda f$ , necessarily $\lambda\in\mathbb T$ (as $T$ is a homeomorphism).  Consider $g(s,t) = f(s)\overline{f(t)}$ , which defines a continuous function on $X\times X$ .  Then $g(T^ns,T^nt) = f(T^ns)\overline{f(T^nt)} = \lambda^n f(s) \overline{\lambda}^n \overline{f(t)} = f(s)\overline{f(t)} = g(s,t)$ .  There is $(x,y)$ such that $\{ (T^nx,T^ny):n\geq 1\}$ is dense in $X\times X$ , from which it follows that $g$ must be constant (as $g$ is continuous).  So in particular $f(x)\overline{f(y)} = |f(x)|^2$ for all $x,y$ , so $f$ is constant. So that's one implication...","Let be a compact (metric) space and be a continuous map.  Let be the linear operator . Then Wikipedia (see http://en.wikipedia.org/wiki/Weak_mixing#Topological_mixing ) (vaguely) says that is weak topological mixing if, whenever for some and , then is a constant function. But Terry Tao (see Definition 3 of http://terrytao.wordpress.com/2008/01/28/254a-lecture-7-structural-theory-of-topological-dynamical-systems/ ) says that is topologically weakly mixing if is topologically transitive , that is, if are open then there is with .  I guess this is equivalent to saying that for open we can find one with both and non-empty. Unfortunately, if I look in e.g. Brin and Stuck's book, then topologically transitive is defined to mean that for some , the forward orbit is dense in . ( Edit: Thinking about this, B&S, Prop 2.2.1 shows that Tao's definition implies the B&S definition; and, at least if is a homeomorphism, the converse holds.  But it doesn't seem to if is not surjective). Is the definition of mixing involving equivalent to the usual one?  If so, can anyone supply a reference or a sketch proof?  I am beginning to think that Wikipedia has confused topological and measure-theoretic mixing. If they are equivalent, do I really need to be a homeomorphism?  (One could either replace by in Tao's definition, or let be the inverse image, and iterate).  Do I need to be metric? Edit: The reference Willie found gives the following: Suppose is a homeomorphism and is minimal in the B&S sense.  Then if with , necessarily (as is a homeomorphism).  Consider , which defines a continuous function on .  Then .  There is such that is dense in , from which it follows that must be constant (as is continuous).  So in particular for all , so is constant. So that's one implication...","X T:X\rightarrow X U_T:C(X)\rightarrow C(X) U_T(f) = f\circ T T U_t(f)=\lambda f \lambda\in\mathbb C f\in C(X) f T T\times T U,V\subseteq X\times X n\in\mathbb Z (T\times T)^n(U)\cap V\not=\emptyset A,B,C,D\subseteq X n\in\mathbb Z T^n(A)\cap B T^n(C)\cap D (x,y)\in X\times X \{ (T^n(x),T^n(y)) : n\geq 1 \} X\times X T T U_T T \mathbb Z \mathbb N T^{-1} X T T\times T f\in C(X) f\circ T = \lambda f \lambda\in\mathbb T T g(s,t) = f(s)\overline{f(t)} X\times X g(T^ns,T^nt) = f(T^ns)\overline{f(T^nt)} = \lambda^n f(s) \overline{\lambda}^n \overline{f(t)} = f(s)\overline{f(t)} = g(s,t) (x,y) \{ (T^nx,T^ny):n\geq 1\} X\times X g g f(x)\overline{f(y)} = |f(x)|^2 x,y f","['reference-request', 'functional-analysis', 'dynamical-systems']"
16,Fractional iteration of the Newton-approximation-formula: how to resolve one unknown parameter?,Fractional iteration of the Newton-approximation-formula: how to resolve one unknown parameter?,,"For my own exercising I tried to find a closed form expression for the Newton-approximation algorithm, beginning with the simple example for getting the squareroot of some given $ \small z^2 $ by recursion according to $$ \small x_0 = 1 \qquad x_{k+1} = \left( {z^2 \over x_k}+x_k \right)/2 $$ where only $ \small z^2 $ is known and z is sought by $ \small z = \lim_{k \to \infty} x_k$. After expanding some iterations in symbolic variables I got the pattern for a closed form $$ \small x_k = z{ (z+x_0)^{2^k} + (z-x_0)^{2^k} \over (z+x_0)^{2^k} - (z-x_0)^{2^k} }  $$ which might further be compacted to $$ \small x_k = z \cdot \left(1 - { 2\over 1- \left({ z+x_0 \over z-x_0 }\right)^{2^k} } \right) $$ If z is known then this is a nice form for fractionally iterating the original recursion; given some $ \small x_0$ and $ \small \sqrt{z^2}$ I can plot the function and see the quality of approximation by a smooth curve (its logarithm shows a parabola), ... but the requirement is obviously from the statement of the problem ("" find the squareroot of z "") that z should occur in even powers only. Well, if I expand the first version of the two closed forms then only the binomial coefficients at even order in the numerator and at odd order in the denominator remain (where the z at the beginning of the formula cancels against one occurence in the denominator) and thus I've to deal with the given $ \small z^2$ only. However, then I have variable number of terms for varying k and for fractional k their number is even infinite, so this is not the final solution where I'm after. I might overlook something trivial, but how could I reformulate this such that I get even powers of z only and still have an expression, where k can be taken fractional? Just a short addendum Sep 18. The Kalantari-paper has the formula a bit different; significant is a sign-change(I'll have to check my other derivation again) such that I should assume the more correct form: $$ \small x_k = z{ (x_0+z)^{2^k} + (x_0-z)^{2^k} \over (x_0+z)^{2^k} - (x_0-z)^{2^k} }  $$ This can be reformulated into the surprising form, where k is the index of iterations if the Newton-formula is employed: $$ \small {x_k-z \over x_k+z}= \left( {x_0-z \over x_0+z}\right)^{2^k}$$ This form has the interesting property, that -once $ \small \left| x_0-z \right| <1 $ - then it is immediately visible, that each iteration approximates zero with quadratic error reduction, simply because of the repeated squaring of the small numerator. [update] While for integer-indexed iterations the sign-question is irrelevant (and the significant terms in the binomial expansions cancel, so the effect is invisible) this is not so for the 0'th iteration: then the numerators and denominators of the lhs and rhs should be equal, so the Kalantari-sign-scheme was the meaningful one.","For my own exercising I tried to find a closed form expression for the Newton-approximation algorithm, beginning with the simple example for getting the squareroot of some given $ \small z^2 $ by recursion according to $$ \small x_0 = 1 \qquad x_{k+1} = \left( {z^2 \over x_k}+x_k \right)/2 $$ where only $ \small z^2 $ is known and z is sought by $ \small z = \lim_{k \to \infty} x_k$. After expanding some iterations in symbolic variables I got the pattern for a closed form $$ \small x_k = z{ (z+x_0)^{2^k} + (z-x_0)^{2^k} \over (z+x_0)^{2^k} - (z-x_0)^{2^k} }  $$ which might further be compacted to $$ \small x_k = z \cdot \left(1 - { 2\over 1- \left({ z+x_0 \over z-x_0 }\right)^{2^k} } \right) $$ If z is known then this is a nice form for fractionally iterating the original recursion; given some $ \small x_0$ and $ \small \sqrt{z^2}$ I can plot the function and see the quality of approximation by a smooth curve (its logarithm shows a parabola), ... but the requirement is obviously from the statement of the problem ("" find the squareroot of z "") that z should occur in even powers only. Well, if I expand the first version of the two closed forms then only the binomial coefficients at even order in the numerator and at odd order in the denominator remain (where the z at the beginning of the formula cancels against one occurence in the denominator) and thus I've to deal with the given $ \small z^2$ only. However, then I have variable number of terms for varying k and for fractional k their number is even infinite, so this is not the final solution where I'm after. I might overlook something trivial, but how could I reformulate this such that I get even powers of z only and still have an expression, where k can be taken fractional? Just a short addendum Sep 18. The Kalantari-paper has the formula a bit different; significant is a sign-change(I'll have to check my other derivation again) such that I should assume the more correct form: $$ \small x_k = z{ (x_0+z)^{2^k} + (x_0-z)^{2^k} \over (x_0+z)^{2^k} - (x_0-z)^{2^k} }  $$ This can be reformulated into the surprising form, where k is the index of iterations if the Newton-formula is employed: $$ \small {x_k-z \over x_k+z}= \left( {x_0-z \over x_0+z}\right)^{2^k}$$ This form has the interesting property, that -once $ \small \left| x_0-z \right| <1 $ - then it is immediately visible, that each iteration approximates zero with quadratic error reduction, simply because of the repeated squaring of the small numerator. [update] While for integer-indexed iterations the sign-question is irrelevant (and the significant terms in the binomial expansions cancel, so the effect is invisible) this is not so for the 0'th iteration: then the numerators and denominators of the lhs and rhs should be equal, so the Kalantari-sign-scheme was the meaningful one.",,"['functional-analysis', 'elementary-number-theory', 'recursive-algorithms']"
17,Measure Theory and Integrals of Characteristic Functions,Measure Theory and Integrals of Characteristic Functions,,"Given two sets of finite measure in $\mathbb{R}$ say, $E$ and $F$, and their characteristic functions $\chi_E$ and $\chi_F$, can somebody show that $\chi_E\ast\chi_F(x)$ (the convolution) is a continuous function of $x$?  This is a qual problem from an old qual that I'm studying, and I cannot figure it out.  If we were dealing with continuous functions or mollifiers or something it would be straightforward, but what if the sets $E$ and $F$ are somehow pathological, like the Cantor set, or something like that? Thanks!","Given two sets of finite measure in $\mathbb{R}$ say, $E$ and $F$, and their characteristic functions $\chi_E$ and $\chi_F$, can somebody show that $\chi_E\ast\chi_F(x)$ (the convolution) is a continuous function of $x$?  This is a qual problem from an old qual that I'm studying, and I cannot figure it out.  If we were dealing with continuous functions or mollifiers or something it would be straightforward, but what if the sets $E$ and $F$ are somehow pathological, like the Cantor set, or something like that? Thanks!",,"['measure-theory', 'functional-analysis']"
18,If $A$ is normal show that $A+A^*$ bijective implies $A$ is bijective,If  is normal show that  bijective implies  is bijective,A A+A^* A,Suppose $A$ is a normal ( $||Ax||=||A^*x||$ for all $x \in H$ ) bounded linear operator on a Hilbert space $H$ . We want to show that if $A+A^*$ is a bijection then so is $A$ . A is injective is very easy since $kerA=kerA^*$ we have that  if $Ax=0$ then $x \in ker(A)\cap ker(A^*)$ so $x \in ker(A+A^*)$ so $x=0_H$ . I'm stuck on surjective. My hint is to use the open mapping theorem (so $A+A^*$ is open); but I can't see how this helps me at all. I thought maybe I could prove $Im(A)$ was dense and closed but I can't relate either to $A+A^*$ . EDIT In,Suppose is a normal ( for all ) bounded linear operator on a Hilbert space . We want to show that if is a bijection then so is . A is injective is very easy since we have that  if then so so . I'm stuck on surjective. My hint is to use the open mapping theorem (so is open); but I can't see how this helps me at all. I thought maybe I could prove was dense and closed but I can't relate either to . EDIT In,A ||Ax||=||A^*x|| x \in H H A+A^* A kerA=kerA^* Ax=0 x \in ker(A)\cap ker(A^*) x \in ker(A+A^*) x=0_H A+A^* Im(A) A+A^*,['functional-analysis']
19,Negative eigenvalues of Sturm-Liouville problem,Negative eigenvalues of Sturm-Liouville problem,,"I am trying to solve the following Sturm-Liouville problem on the circle $S^1$, i.e. the closed interval $[0,1]$ with periodic boundary conditions $(y(0)=y(1))$: $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=-\frac{\lambda}{p(x)}y.$$ Here $p$ is a positive function and is also the first eigenfunction (corresponding to $\lambda_0=-1$). In particular, I am would like to know how many negative eigenvalues the system has. Idea : Note that there can be only finitely many negative eigenvalues, since the eigenvalues satisfy $$-\infty <\lambda_0<\lambda_1\leq \dots \to \infty.$$ A theorem (8.3.1) from Coddington-Levinson says that the eigenfunctions $y_{2i+1}$ and $y_{2i+2}$ corresponding to $\lambda_{2i+1},\lambda_{2i+2}$ have precisely $2i+1$ zeros on $[0,1)$. Therefore, if $\lambda =0$ was an eigenvalue, I could count the zeros of the corresponding eigenfunction to determine how many negative eigenvalues there are. So I tried solving $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=0,$$ but haven't gotten anywhere. Are there any general results on the number of negative eigenvalues of Sturm-Liouville systems?","I am trying to solve the following Sturm-Liouville problem on the circle $S^1$, i.e. the closed interval $[0,1]$ with periodic boundary conditions $(y(0)=y(1))$: $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=-\frac{\lambda}{p(x)}y.$$ Here $p$ is a positive function and is also the first eigenfunction (corresponding to $\lambda_0=-1$). In particular, I am would like to know how many negative eigenvalues the system has. Idea : Note that there can be only finitely many negative eigenvalues, since the eigenvalues satisfy $$-\infty <\lambda_0<\lambda_1\leq \dots \to \infty.$$ A theorem (8.3.1) from Coddington-Levinson says that the eigenfunctions $y_{2i+1}$ and $y_{2i+2}$ corresponding to $\lambda_{2i+1},\lambda_{2i+2}$ have precisely $2i+1$ zeros on $[0,1)$. Therefore, if $\lambda =0$ was an eigenvalue, I could count the zeros of the corresponding eigenfunction to determine how many negative eigenvalues there are. So I tried solving $$\frac{d}{dx}\left[\frac{1}{p(x)}\frac{dy}{dx}\right]+\frac{1}{p(x)}(p^2(x)+\frac{1}{2})y=0,$$ but haven't gotten anywhere. Are there any general results on the number of negative eigenvalues of Sturm-Liouville systems?",,"['functional-analysis', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems', 'sturm-liouville']"
20,Nuclear operators,Nuclear operators,,"While studying for my thesis (in dynamical systems) I've encountered multiple times with the concept of nuclear operators and nuclear spaces, often linked with the works of Grothendieck. For example, when studying the generalized transfer operator (or Ruelle operator) for the Gauss Map, Dieter Mayer points out that this operator is in fact nuclear (On the thermodynamic formalism for the Gauss map). While I can understand the definition of a nuclear operator, I still cannot get the real importance of being nuclear of order zero. Usually I'm interested in spectral gap properties for transfer operators, but is there any implication of the nuclear property? Also, any reference for nuclear operators and Fredholm kernels would be appreciated, since trying to learn directly from Grothendieck's works has been really difficult for me. Thanks in advance.","While studying for my thesis (in dynamical systems) I've encountered multiple times with the concept of nuclear operators and nuclear spaces, often linked with the works of Grothendieck. For example, when studying the generalized transfer operator (or Ruelle operator) for the Gauss Map, Dieter Mayer points out that this operator is in fact nuclear (On the thermodynamic formalism for the Gauss map). While I can understand the definition of a nuclear operator, I still cannot get the real importance of being nuclear of order zero. Usually I'm interested in spectral gap properties for transfer operators, but is there any implication of the nuclear property? Also, any reference for nuclear operators and Fredholm kernels would be appreciated, since trying to learn directly from Grothendieck's works has been really difficult for me. Thanks in advance.",,"['functional-analysis', 'dynamical-systems']"
21,Definition of Sobolev space $H^s$ and domain of $-\Delta^s$,Definition of Sobolev space  and domain of,H^s -\Delta^s,"The spaces below are on $\partial\Omega$, the boundary of a bounded smooth domain $\Omega$. I read this in the book on page 141. Define $H^2 := \{ u \in L^2 \mid (-\Delta u) \in L^2\}$. And then: We can define $H^s$ ($s \in \mathbb{R}$) as the domain of $(-\Delta)^s$. If $\lambda_j, e_j$ are the eigenpairs of $-\Delta$, then   $$H^s = \{ u \in (C_0^\infty)' \mid  \sum_j\lambda_j^{2s}|(u,e_j)_{L^2}|^2 < \infty \}$$ Now I'm confused, shouldn't the power of the eigenvalues be $s$, not $2s$? For example take $u \in H^1$. Then $u=\sum (u,e_j)e_j$, so $-\Delta u = \sum \lambda_j(u,e_j)e_j$ in the weak sense, so $\langle -\Delta u, u \rangle = \sum \lambda_j |(u,e_j)|^2 < \infty$. But with the definition of $H^1$ above, we would need $\sum \lambda_j^2|(u,e_j)|^2 < \infty$. Why this discrepancy?","The spaces below are on $\partial\Omega$, the boundary of a bounded smooth domain $\Omega$. I read this in the book on page 141. Define $H^2 := \{ u \in L^2 \mid (-\Delta u) \in L^2\}$. And then: We can define $H^s$ ($s \in \mathbb{R}$) as the domain of $(-\Delta)^s$. If $\lambda_j, e_j$ are the eigenpairs of $-\Delta$, then   $$H^s = \{ u \in (C_0^\infty)' \mid  \sum_j\lambda_j^{2s}|(u,e_j)_{L^2}|^2 < \infty \}$$ Now I'm confused, shouldn't the power of the eigenvalues be $s$, not $2s$? For example take $u \in H^1$. Then $u=\sum (u,e_j)e_j$, so $-\Delta u = \sum \lambda_j(u,e_j)e_j$ in the weak sense, so $\langle -\Delta u, u \rangle = \sum \lambda_j |(u,e_j)|^2 < \infty$. But with the definition of $H^1$ above, we would need $\sum \lambda_j^2|(u,e_j)|^2 < \infty$. Why this discrepancy?",,"['functional-analysis', 'sobolev-spaces', 'laplacian']"
22,Scale invariance and the Mellin transform?,Scale invariance and the Mellin transform?,,"The Mellin transform of a function is given by: $$\mathcal{M}[f](s) = \int_0^{\infty}x^{s-1}f(x)dx$$ Supposedly, the magnitude of the Mellin transform is invariant to scaling, analogous to how the magnitude of the Fourier transform is invariant to translation. Yet, when I try to derive this I get: $g(x) = f(kx)$ $|\mathcal{M}[g](s)| = |\int_0^{\infty}x^{s-1}f(kx)dx|=|\int_0^{\infty}(\frac{x'}{k})^{s-1}f(x')\frac{dx'}{k}|=|k^{-s}||\mathcal{M}[f](s)|$ which is not invariant, though it does have a simple relation between them. Where did I go wrong?","The Mellin transform of a function is given by: $$\mathcal{M}[f](s) = \int_0^{\infty}x^{s-1}f(x)dx$$ Supposedly, the magnitude of the Mellin transform is invariant to scaling, analogous to how the magnitude of the Fourier transform is invariant to translation. Yet, when I try to derive this I get: $g(x) = f(kx)$ $|\mathcal{M}[g](s)| = |\int_0^{\infty}x^{s-1}f(kx)dx|=|\int_0^{\infty}(\frac{x'}{k})^{s-1}f(x')\frac{dx'}{k}|=|k^{-s}||\mathcal{M}[f](s)|$ which is not invariant, though it does have a simple relation between them. Where did I go wrong?",,"['functional-analysis', 'fourier-analysis', 'mellin-transform']"
23,Why is the support of Dirac distribution $\{0\}$?,Why is the support of Dirac distribution ?,\{0\},"Distributions are of two types: those that are obtained from locally integrable functions, and those that aren't. For the first type, the support of distribution is simply the support of the function. For the other kind of distribution, for example, the Dirac Delta 'function', we can't find the support this way. The support of the Dirac Delta distribution is given to be the set $\{0\}$. Can someone help me in understanding why?","Distributions are of two types: those that are obtained from locally integrable functions, and those that aren't. For the first type, the support of distribution is simply the support of the function. For the other kind of distribution, for example, the Dirac Delta 'function', we can't find the support this way. The support of the Dirac Delta distribution is given to be the set $\{0\}$. Can someone help me in understanding why?",,"['functional-analysis', 'distribution-theory']"
24,"Spectrum of Multiplication Operator $T$ in $C[0,1]$",Spectrum of Multiplication Operator  in,"T C[0,1]","""Let $X=C[0,1]$ and $v \in X$ be a fixed function. Let $T$ be the multiplication operator by $v$, i.e. $Tx(t)=v(t)x(t)$. Find the spectrum of $T$."" This is an exercise from a PDF of notes I found online. I'm trying to better understand Spectral Theory. So $\lambda$ is a regular value if $(T-\lambda I)^{-1}$ exists, is bounded, and dense (I think there is a lemma which lets us not worry about the dense part). The set of all regular values is $\rho(T)$ and the spectrum is $\sigma(T)=\mathbb{C}\setminus \rho(T)$. It seems to me that $(T-\lambda I)^{-1}$ maps some $y(t)$ to $\frac{y(t) + \lambda}{v(t)}$. However, this would be problematic if $v(t)=0$ for some values of of $t \in [0,1]$ I don't have much of an understanding of all these definitions so if someone could give a solution to this example, I think that would help clear up some of the ideas for me. Thanks in advance.","""Let $X=C[0,1]$ and $v \in X$ be a fixed function. Let $T$ be the multiplication operator by $v$, i.e. $Tx(t)=v(t)x(t)$. Find the spectrum of $T$."" This is an exercise from a PDF of notes I found online. I'm trying to better understand Spectral Theory. So $\lambda$ is a regular value if $(T-\lambda I)^{-1}$ exists, is bounded, and dense (I think there is a lemma which lets us not worry about the dense part). The set of all regular values is $\rho(T)$ and the spectrum is $\sigma(T)=\mathbb{C}\setminus \rho(T)$. It seems to me that $(T-\lambda I)^{-1}$ maps some $y(t)$ to $\frac{y(t) + \lambda}{v(t)}$. However, this would be problematic if $v(t)=0$ for some values of of $t \in [0,1]$ I don't have much of an understanding of all these definitions so if someone could give a solution to this example, I think that would help clear up some of the ideas for me. Thanks in advance.",,"['functional-analysis', 'spectral-theory']"
25,A subspace of a dual space is norm closed if and only if it is weak star closed.,A subspace of a dual space is norm closed if and only if it is weak star closed.,,"I am trying to figure out if the statement holds true, the literature i am following says that its not true but i don't seem to understand, If $Y$ is a Banach space and let subspace $A \subset Y'$, such that $Y'$ is a dual . $A$ is norm closed if and only if $A$ is weak star closed ? Looks like reflexivity comes into play to argue this statement .  Thank you for your hints !","I am trying to figure out if the statement holds true, the literature i am following says that its not true but i don't seem to understand, If $Y$ is a Banach space and let subspace $A \subset Y'$, such that $Y'$ is a dual . $A$ is norm closed if and only if $A$ is weak star closed ? Looks like reflexivity comes into play to argue this statement .  Thank you for your hints !",,['functional-analysis']
26,A proof of the Riesz representation theorem,A proof of the Riesz representation theorem,,"I'm having trouble filling the steps in this guided proof of Riesz's representation theorem. (I already have a proof I can understand, but I'd like to understand this one too.) Let $H$ be a Hilbert space, and $\varphi : H \to \mathbb{C}$ a bounded linear functional. If $\varphi = 0$ then we are done; otherwise, by scaling, we may assume without loss of generality that $\| \varphi \| = 1$. So, for each $n$, there is a unit vector $h_n$ in $H$ such that $| \varphi(h_n) | > 1 - \frac{1}{n}$. By multiplying each one by an appropriate complex number of unit modulus, we may assume $\varphi(h_n) \in \mathbb{R}$ and $\varphi(h_n) > 1 - \frac{1}{n}$. Now I run into a problem — I can see that everything follows from the first step, but the first step is eluding me at the moment. $h_n \longrightarrow h$ for some $h$ in $H$. Why? If $H$ is finite-dimensional then certainly there is a convergent subsequence, but I don't see how we can assert the existence of such an $h$ without knowing more about the relative distances of the $h_n$. $h$ is orthogonal to the kernel of $\varphi$. I think the idea here is to show that $\| h - u \|$ is minimised over $u \in \ker \varphi$ when $u = 0$, by exploiting the fact that $\| h - u \| \ge | \varphi(h - u) | = | \varphi(h) | = 1$. $\ker \varphi \oplus \operatorname{span} \{ h \} = H$, by e.g. rank–nullity or orthogonal decomposition. Hence $\varphi(x) = \langle x, h \rangle$ for all $x \in H$, by decomposing $x$ using the above decomposition of $H$ and linearity.","I'm having trouble filling the steps in this guided proof of Riesz's representation theorem. (I already have a proof I can understand, but I'd like to understand this one too.) Let $H$ be a Hilbert space, and $\varphi : H \to \mathbb{C}$ a bounded linear functional. If $\varphi = 0$ then we are done; otherwise, by scaling, we may assume without loss of generality that $\| \varphi \| = 1$. So, for each $n$, there is a unit vector $h_n$ in $H$ such that $| \varphi(h_n) | > 1 - \frac{1}{n}$. By multiplying each one by an appropriate complex number of unit modulus, we may assume $\varphi(h_n) \in \mathbb{R}$ and $\varphi(h_n) > 1 - \frac{1}{n}$. Now I run into a problem — I can see that everything follows from the first step, but the first step is eluding me at the moment. $h_n \longrightarrow h$ for some $h$ in $H$. Why? If $H$ is finite-dimensional then certainly there is a convergent subsequence, but I don't see how we can assert the existence of such an $h$ without knowing more about the relative distances of the $h_n$. $h$ is orthogonal to the kernel of $\varphi$. I think the idea here is to show that $\| h - u \|$ is minimised over $u \in \ker \varphi$ when $u = 0$, by exploiting the fact that $\| h - u \| \ge | \varphi(h - u) | = | \varphi(h) | = 1$. $\ker \varphi \oplus \operatorname{span} \{ h \} = H$, by e.g. rank–nullity or orthogonal decomposition. Hence $\varphi(x) = \langle x, h \rangle$ for all $x \in H$, by decomposing $x$ using the above decomposition of $H$ and linearity.",,"['functional-analysis', 'hilbert-spaces', 'riesz-representation-theorem']"
27,Confusion on unit impulse function $\delta(t)$,Confusion on unit impulse function,\delta(t),"$\delta(t)$ is a singular function and when I'm learning Signals and Systems I learned that $\delta(t)$ is an even function, and all of its odd order derivatives are odd function. Then we have $\delta^{'}(t)=-\delta^{'}(-t)$, let t=0 then $\delta^{'}(0)=0$. This is obviously wrong! But Where? And if it's wrong,then $\delta^{'}(0)$=?","$\delta(t)$ is a singular function and when I'm learning Signals and Systems I learned that $\delta(t)$ is an even function, and all of its odd order derivatives are odd function. Then we have $\delta^{'}(t)=-\delta^{'}(-t)$, let t=0 then $\delta^{'}(0)=0$. This is obviously wrong! But Where? And if it's wrong,then $\delta^{'}(0)$=?",,"['functional-analysis', 'distribution-theory']"
28,Counter example of rellich-kondrachov compact embedding theorem on unbounded domain,Counter example of rellich-kondrachov compact embedding theorem on unbounded domain,,"The Rellich-Kondrachov Compactness Theorem says that when $U$ is a bounded set with $C^1$ boundary then $W^{1,p}(U)$ is compactly embedded into $L^{q}(U)$ for every $1 \leq q < p^{*}$ . What if $U$ is unbounded, e.g. $U=\mathbb{R}^n$ ?","The Rellich-Kondrachov Compactness Theorem says that when is a bounded set with boundary then is compactly embedded into for every . What if is unbounded, e.g. ?","U C^1 W^{1,p}(U) L^{q}(U) 1 \leq q < p^{*} U U=\mathbb{R}^n","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
29,Understanding the proof $C(X)$ is complete,Understanding the proof  is complete,C(X),"Prove $(C_X,||.||)$ ,where $||.||$ is the maximum norm and X is compact, is complete. The following proof was given. It is the one I am striving to understand: Let $(f_n)$ be a Cauchy sequence: $\forall\epsilon>0\exists N\in\mathbb{N}:n,m\geqslant N\implies ||f_m-f_n||<\epsilon$ $\forall t\in X$ $0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0$ as $m,n\to\infty$ $\forall t\in X\:, (f_n(t))_{n\in\mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$ . Then $(f_n)_n\to f$ uniformly then $f$ is continuous. This is how the proof was handed to me. I think I can fill the gaps but I would need someone to back me on that. So first the author considers a Cauchy sequence and assumes it converges in $C(X)$ Then it arrives to the following inequality: $0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0$ as $m,n\to\infty$ since the it assumed $\max_{x\in X}|f_n(x)-f_m(x)|\to 0$ then $|f_n(t)-f_m(t)|\to 0$ So the convergence in $C(X)$ verifies that the same Cauchy sequence converges in $\mathbb{R}$ that is by assumption complete with the usual topology. Since $X$ is compact then $f_n$ converges uniformly in $\mathbb{R}$ and it converges to a continuous function. Therefore it converges in $C(X)$ proving the latter is complete. Question : Is this the reasoning behind the proof? Thanks in advance!","Prove ,where is the maximum norm and X is compact, is complete. The following proof was given. It is the one I am striving to understand: Let be a Cauchy sequence: as is a Cauchy sequence in . Then uniformly then is continuous. This is how the proof was handed to me. I think I can fill the gaps but I would need someone to back me on that. So first the author considers a Cauchy sequence and assumes it converges in Then it arrives to the following inequality: as since the it assumed then So the convergence in verifies that the same Cauchy sequence converges in that is by assumption complete with the usual topology. Since is compact then converges uniformly in and it converges to a continuous function. Therefore it converges in proving the latter is complete. Question : Is this the reasoning behind the proof? Thanks in advance!","(C_X,||.||) ||.|| (f_n) \forall\epsilon>0\exists N\in\mathbb{N}:n,m\geqslant N\implies ||f_m-f_n||<\epsilon \forall t\in X 0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0 m,n\to\infty \forall t\in X\:, (f_n(t))_{n\in\mathbb{N}} \mathbb{R} (f_n)_n\to f f C(X) 0\leqslant |f_n(t)-f_m(t)|\leqslant \max_{x\in X}|f_n(x)-f_m(x)|\to 0 m,n\to\infty \max_{x\in X}|f_n(x)-f_m(x)|\to 0 |f_n(t)-f_m(t)|\to 0 C(X) \mathbb{R} X f_n \mathbb{R} C(X)","['real-analysis', 'functional-analysis', 'analysis', 'proof-writing', 'proof-explanation']"
30,Why we define the completeness of a space by the converge of a Cauchy sequence rather than a normal sequence?,Why we define the completeness of a space by the converge of a Cauchy sequence rather than a normal sequence?,,"The intuition of the completeness to me is that the limit of any sequence converges to the point inside the set itself. But why we define a set to be complete as any Cauchy sequence converge into the set itself? It seems a more complex definition than a simple convergent sequence. Why we use Cauchy sequence rather than a simple sequence? For the simple sequence, can we use $\lim_{n\rightarrow \infty }x_n=x$?","The intuition of the completeness to me is that the limit of any sequence converges to the point inside the set itself. But why we define a set to be complete as any Cauchy sequence converge into the set itself? It seems a more complex definition than a simple convergent sequence. Why we use Cauchy sequence rather than a simple sequence? For the simple sequence, can we use $\lim_{n\rightarrow \infty }x_n=x$?",,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'cauchy-sequences']"
31,Question about Fredholm operator,Question about Fredholm operator,,"$X,Y$ are Banach spaces and $A\in B(X,Y)$ is a Fredholm operator (that is, the dimensions of ker($A$) and coker($A$) are both finite), then are closed linear subspaces ker($A$) and Im($A$) complemented? (A closed linear subspace $H$ in a Banach space $Z$ is called complemented iff there is a closed linear subspace $G$ such that $H+G=Z$ and $H \cap G=0$)","$X,Y$ are Banach spaces and $A\in B(X,Y)$ is a Fredholm operator (that is, the dimensions of ker($A$) and coker($A$) are both finite), then are closed linear subspaces ker($A$) and Im($A$) complemented? (A closed linear subspace $H$ in a Banach space $Z$ is called complemented iff there is a closed linear subspace $G$ such that $H+G=Z$ and $H \cap G=0$)",,"['functional-analysis', 'banach-spaces', 'operator-theory']"
32,What is the intuition behind distributional derivative and why distributional derivative is useful?,What is the intuition behind distributional derivative and why distributional derivative is useful?,,"Why we study distributional derivative? Let $\Omega\subset \Bbb{R}^n$ be any open set. $D(\Omega)=C_c^{\infty}(\Omega) $ : Linear space of test functions i.e smooth functions with compact support. $D'(\Omega) $ : Continuous dual of $D(\Omega) $ For $f\in D'(\Omega) $ we define distributional derivative of $f$ , $D^{\alpha}f$ or $\partial^{\alpha}f$ by $$\langle\partial^{\alpha}f,\varphi\rangle=(-1)^{|\alpha|}\langle f,\partial^{\alpha}\varphi\rangle$$ There are locally integrable function which is not differentiable in classical sense but the regular distribution generated by the locally integrable function possess distributional derivative. What is the intuition behind distributional derivative and why distributional derivative is useful? Can you explain some application where we need some sort of differentiation but classical differentiation is no longer useful?","Why we study distributional derivative? Let be any open set. : Linear space of test functions i.e smooth functions with compact support. : Continuous dual of For we define distributional derivative of , or by There are locally integrable function which is not differentiable in classical sense but the regular distribution generated by the locally integrable function possess distributional derivative. What is the intuition behind distributional derivative and why distributional derivative is useful? Can you explain some application where we need some sort of differentiation but classical differentiation is no longer useful?","\Omega\subset \Bbb{R}^n D(\Omega)=C_c^{\infty}(\Omega)  D'(\Omega)  D(\Omega)  f\in D'(\Omega)  f D^{\alpha}f \partial^{\alpha}f \langle\partial^{\alpha}f,\varphi\rangle=(-1)^{|\alpha|}\langle f,\partial^{\alpha}\varphi\rangle","['real-analysis', 'functional-analysis', 'derivatives', 'distribution-theory', 'applications']"
33,Why is this operator not compact?,Why is this operator not compact?,,"Let $(a_n)$ be a sequence that tends to zero and $0<a_n<1$. Why is then the mapping $$ \ell_2 \rightarrow \ell_2,\ (x_1,x_2,\ldots)\mapsto ((1-a_1)x_1,(1-a_2)x_2,\ldots)$$ not compact ? Someone said I should look at its inverse, but that didn't helpt me either...","Let $(a_n)$ be a sequence that tends to zero and $0<a_n<1$. Why is then the mapping $$ \ell_2 \rightarrow \ell_2,\ (x_1,x_2,\ldots)\mapsto ((1-a_1)x_1,(1-a_2)x_2,\ldots)$$ not compact ? Someone said I should look at its inverse, but that didn't helpt me either...",,['functional-analysis']
34,"Check that $T: \ell^2 \to \ell^2$ is not surjective where $T(x_1, x_2, x_3, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, \ldots)$",Check that  is not surjective where,"T: \ell^2 \to \ell^2 T(x_1, x_2, x_3, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, \ldots)","Let $T: \ell^2 \to \ell^2$ the operator defined as $$T(x_1, x_2, x_3, x_4, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, x_4+ x_5, \ldots)$$ where $$\ell^2 = \left\{ (x_n)_{n=1}^\infty : \sum_{n=1}^\infty |x_n|^2 < \infty \right\}$$ I need to prove that $T$ is not surjective. I managed to prove that $T$ is injective and that $T = I + S$ , where $I$ is the identity operator and $S$ is the right shift operator. I know that $S$ is not surjective, but no idea how to deal with $T$ . Any help will be appreciated.","Let the operator defined as where I need to prove that is not surjective. I managed to prove that is injective and that , where is the identity operator and is the right shift operator. I know that is not surjective, but no idea how to deal with . Any help will be appreciated.","T: \ell^2 \to \ell^2 T(x_1, x_2, x_3, x_4, \ldots) = (x_1 + x_2, x_2 + x_3, x_3 + x_4, x_4+ x_5, \ldots) \ell^2 = \left\{ (x_n)_{n=1}^\infty : \sum_{n=1}^\infty |x_n|^2 < \infty \right\} T T T = I + S I S S T","['functional-analysis', 'lp-spaces']"
35,Prove that a Banach space cannot be reflexive if some strict closed subspace of its dual space separates its points [closed],Prove that a Banach space cannot be reflexive if some strict closed subspace of its dual space separates its points [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let $X$ be a Banach space and let $Z$ be a closed subspace of $X^*$ such that $Z\neq X^*$ . Suppose $Z$ separates the points in $X$ , that is, if $x \in X$ and $x^*(x) = 0$ $\forall x^* \in Z$ then $x = 0$ . Prove that $X$ is not reflexive, i.e. that the canonical injection $J_X$ : $X$ $\rightarrow$ $X^{**}$ is not surjective. How can I do that?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let be a Banach space and let be a closed subspace of such that . Suppose separates the points in , that is, if and then . Prove that is not reflexive, i.e. that the canonical injection : is not surjective. How can I do that?",X Z X^* Z\neq X^* Z X x \in X x^*(x) = 0 \forall x^* \in Z x = 0 X J_X X \rightarrow X^{**},"['functional-analysis', 'hahn-banach-theorem', 'reflexive-space']"
36,"If $f : \mathbb{R}^3 \to \mathbb{R}$ is Schwartz, is $F(x):=\int_{\mathbb{R}^3} \frac{f(y)}{\lvert x-y \rvert} d^3y$ also Schwartz?","If  is Schwartz, is  also Schwartz?",f : \mathbb{R}^3 \to \mathbb{R} F(x):=\int_{\mathbb{R}^3} \frac{f(y)}{\lvert x-y \rvert} d^3y,"I am having trouble proving / disproving the question in the title. That is, let $f : \mathbb{R}^3 \to \mathbb{R}$ be a real-valued Schwartz function. Then, I wonder if \begin{equation} F(x):=\int_{\mathbb{R}^3} \frac{f(y)}{\lvert x-y \rvert} d^3y \end{equation} is also a Schwartz function. At least it seems clear from the property of convolution that $F(x)$ is smooth. However, I cannot figure out decay properties of $F(x)$ . Could anyone please help me?","I am having trouble proving / disproving the question in the title. That is, let be a real-valued Schwartz function. Then, I wonder if is also a Schwartz function. At least it seems clear from the property of convolution that is smooth. However, I cannot figure out decay properties of . Could anyone please help me?","f : \mathbb{R}^3 \to \mathbb{R} \begin{equation}
F(x):=\int_{\mathbb{R}^3} \frac{f(y)}{\lvert x-y \rvert} d^3y
\end{equation} F(x) F(x)","['real-analysis', 'calculus', 'functional-analysis', 'schwartz-space']"
37,Non-trivial dense subalgebra of continuous function,Non-trivial dense subalgebra of continuous function,,"When we talk about dense subalgebra of continuous function on a compact interval, we usually think of smooth functioon or polynomials. Can we find another dense subalgebra which does not contain any non-constant polynomials? I know Stone-Weierstrass theorem gives us the condition of subalgebra being dense iff it separate points, but I haven't seen any concrete examples.","When we talk about dense subalgebra of continuous function on a compact interval, we usually think of smooth functioon or polynomials. Can we find another dense subalgebra which does not contain any non-constant polynomials? I know Stone-Weierstrass theorem gives us the condition of subalgebra being dense iff it separate points, but I haven't seen any concrete examples.",,"['real-analysis', 'functional-analysis']"
38,Examples of self adjoint compact operators on Hilbert spaces,Examples of self adjoint compact operators on Hilbert spaces,,"As the titles says, I'm looking for examples of self adjoint compact operators on Hilbert spaces. So far I know of the diagonal operator on $\ell^2$ , $$ (Tx)_i = \alpha_ix_i  $$ for some sequence $\alpha_i \to 0$ ;  and the Hilbert Schmidt integral operator in $L^2(\Omega)$ , $$ Kg = \int_{\Omega}K(x,y)g(y)dy $$ with $K \in L^2(\Omega^2)$ a symmetric Hilbert-Schmidt kernel. I would also like to know of some applications that use Hilbert Schmidt integral operators. Edit: I'd be really grateful to know about the behaviour of the norm of the operators as well. Thanks in advance.","As the titles says, I'm looking for examples of self adjoint compact operators on Hilbert spaces. So far I know of the diagonal operator on , for some sequence ;  and the Hilbert Schmidt integral operator in , with a symmetric Hilbert-Schmidt kernel. I would also like to know of some applications that use Hilbert Schmidt integral operators. Edit: I'd be really grateful to know about the behaviour of the norm of the operators as well. Thanks in advance.","\ell^2 
(Tx)_i = \alpha_ix_i 
 \alpha_i \to 0 L^2(\Omega) 
Kg = \int_{\Omega}K(x,y)g(y)dy
 K \in L^2(\Omega^2)","['functional-analysis', 'hilbert-spaces', 'big-list']"
39,Normal Operator: Empty Spectrum,Normal Operator: Empty Spectrum,,Given a Hilbert space $\mathcal{H}$. For normal operators: $$N^*N=NN^*:\quad\sigma(N)\neq\varnothing$$ How can I check this?,Given a Hilbert space $\mathcal{H}$. For normal operators: $$N^*N=NN^*:\quad\sigma(N)\neq\varnothing$$ How can I check this?,,"['functional-analysis', 'hilbert-spaces', 'spectral-theory']"
40,"$L^2([0,1])$ is a set of first category in $L^1([0,1])$?",is a set of first category in ?,"L^2([0,1]) L^1([0,1])","How to show that $L^2([0,1])$ is a set of first category in $L^1([0,1])$? Thank you.","How to show that $L^2([0,1])$ is a set of first category in $L^1([0,1])$? Thank you.",,['functional-analysis']
41,"$\sum_{n=1}^\infty (AB e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n)$",,"\sum_{n=1}^\infty (AB e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n)","Let $H$ be a Hilbert space, $A$ and $B$ bounded linear operators on $H$ , and $\{e_n\}$ an orthonormal basis for $H$ . I am following a proof that claims it is obvious that $$\sum_{n=1}^\infty (AB e_n, e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n).$$ However I do not see why this is necessarily true. How can one prove this?","Let be a Hilbert space, and bounded linear operators on , and an orthonormal basis for . I am following a proof that claims it is obvious that However I do not see why this is necessarily true. How can one prove this?","H A B H \{e_n\} H \sum_{n=1}^\infty (AB e_n, e_n) = \sum_{n=1}^\infty \sum_{m=1}^\infty (Be_n, e_m) (Ae_m, e_n).","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
42,Prove that every reflexive Banach space is weakly complete.,Prove that every reflexive Banach space is weakly complete.,,Here is the question: A sequence $(x_{n})$ in a normed linear space $X$ is weakly Cauchy if $(Tx_{n})$ is a Cauchy sequence for every $T \in X^*.$ The space $X$ is weakly complete if every weakly Cauchy sequence in $X$ is weakly convergent. Prove that every reflexive Banach space is weakly complete. Could anyone help me in this proof please?,Here is the question: A sequence in a normed linear space is weakly Cauchy if is a Cauchy sequence for every The space is weakly complete if every weakly Cauchy sequence in is weakly convergent. Prove that every reflexive Banach space is weakly complete. Could anyone help me in this proof please?,(x_{n}) X (Tx_{n}) T \in X^*. X X,['functional-analysis']
43,Folland Real Analysis 5.2,Folland Real Analysis 5.2,,"In Folland's second edition of Real Analysis , part of Proposition 5.2 (paraphrased) reads: Let $\mathcal X$ and $\mathcal Y$ be normed vector spaces and   $T:\mathcal X\to\mathcal Y$ a linear map. If $T$ is continuous at $0$,   then $T$ is bounded. His proof is as follows: If $T$ is continuous at $0\in\mathcal X$, there is a neighborhood $U$   of $0$ such that $T(U)\subset\{y\in\mathcal Y:\|y\|\leq1\}$, and $U$   must contain a ball $B=\{x\in\mathcal X:\|x\|\leq\delta\}$ about $0$;   thus $\|Tx\|\leq1$ when $\|x\|\leq\delta$. Since $T$ commutes with   scalar multiplication, it follows that $\|Tx\|\leq a\delta^{-1}$   whenever $\|x\|\leq a$, that is,   $\color{red}{\|Tx\|\leq\delta^{-1}\|x\|}$. I do not understand how the highlighted part was obtained since $\|x\|\leq a$ (It would make sense to me if $\|x\|\geq a$.).","In Folland's second edition of Real Analysis , part of Proposition 5.2 (paraphrased) reads: Let $\mathcal X$ and $\mathcal Y$ be normed vector spaces and   $T:\mathcal X\to\mathcal Y$ a linear map. If $T$ is continuous at $0$,   then $T$ is bounded. His proof is as follows: If $T$ is continuous at $0\in\mathcal X$, there is a neighborhood $U$   of $0$ such that $T(U)\subset\{y\in\mathcal Y:\|y\|\leq1\}$, and $U$   must contain a ball $B=\{x\in\mathcal X:\|x\|\leq\delta\}$ about $0$;   thus $\|Tx\|\leq1$ when $\|x\|\leq\delta$. Since $T$ commutes with   scalar multiplication, it follows that $\|Tx\|\leq a\delta^{-1}$   whenever $\|x\|\leq a$, that is,   $\color{red}{\|Tx\|\leq\delta^{-1}\|x\|}$. I do not understand how the highlighted part was obtained since $\|x\|\leq a$ (It would make sense to me if $\|x\|\geq a$.).",,"['real-analysis', 'functional-analysis', 'banach-spaces', 'linear-transformations']"
44,How to prove $ A^{\perp} $ is a closed linear subspace?,How to prove  is a closed linear subspace?, A^{\perp} ,Suppose $ X $ is an inner product space and $ A\subseteq X $. I need to prove that $ A^{\perp} $ is a closed linear subspace of $ X $. Can anyone give me a idea?,Suppose $ X $ is an inner product space and $ A\subseteq X $. I need to prove that $ A^{\perp} $ is a closed linear subspace of $ X $. Can anyone give me a idea?,,"['functional-analysis', 'vector-spaces', 'inner-products']"
45,Do there exist two singular measures whose convolution is absolutely continuous?,Do there exist two singular measures whose convolution is absolutely continuous?,,"Let $\mu, \nu$ be finite complex measures with compact supports on the real line, and assume that they are singular with respect to the Lebesgue measure. Can their convolution $\mu\ast\nu$ have a nonzero absolutely continuous component?","Let $\mu, \nu$ be finite complex measures with compact supports on the real line, and assume that they are singular with respect to the Lebesgue measure. Can their convolution $\mu\ast\nu$ have a nonzero absolutely continuous component?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'singular-measures']"
46,"Do $e_1$, $e_2$, ... generate the entire $l_2$ space?","Do , , ... generate the entire  space?",e_1 e_2 l_2,"From our textbook goes some statement like this: ... let $X$ be the linear subspace of $l_2$ generated by the vectors $$\left\{e_1,e_2,e_3,...\right\}$$ ... Which feels strange to me because I thought $e_1=(1,0,0,...)$, $e_2=(0,1,0,...)$, ... are sufficient to generate the entire $l_2$ space, so the $X$ here is actually just $l_2$ itself. Isn't that true?","From our textbook goes some statement like this: ... let $X$ be the linear subspace of $l_2$ generated by the vectors $$\left\{e_1,e_2,e_3,...\right\}$$ ... Which feels strange to me because I thought $e_1=(1,0,0,...)$, $e_2=(0,1,0,...)$, ... are sufficient to generate the entire $l_2$ space, so the $X$ here is actually just $l_2$ itself. Isn't that true?",,['functional-analysis']
47,Why is $f(t) = e^{ta}$ differentiable in a unital Banach algebra?,Why is  differentiable in a unital Banach algebra?,f(t) = e^{ta},"Let $A$ be a unital Banach algebra. For $a\in A$ , we define $$\exp(a):= \sum_{n=0}^\infty \frac{a^n}{n!}$$ Consider the function $$f: \Bbb{R} \to A: t \mapsto \exp(ta) = \sum_{n=0}^\infty \frac{t^n a^n}{n!}$$ In the book I'm reading, it is claimed that $f'(t) = af(t)$ by differentiating term by term. How can we justify the differentiation term by term? Or what is another way to  show that $f$ is differentiable with $f'(t) = af(t)$ . Maybe some argument with functionals?","Let be a unital Banach algebra. For , we define Consider the function In the book I'm reading, it is claimed that by differentiating term by term. How can we justify the differentiation term by term? Or what is another way to  show that is differentiable with . Maybe some argument with functionals?",A a\in A \exp(a):= \sum_{n=0}^\infty \frac{a^n}{n!} f: \Bbb{R} \to A: t \mapsto \exp(ta) = \sum_{n=0}^\infty \frac{t^n a^n}{n!} f'(t) = af(t) f f'(t) = af(t),"['functional-analysis', 'exponential-function']"
48,Examples of diagonal argument in Mathematics?,Examples of diagonal argument in Mathematics?,,"I have seen several examples of diagonal arguments. One of them is, of course, Cantor's proof that $\mathbb R$ is not countable. A diagonal argument can also be used to show that every bounded sequence in $\ell^\infty$ has a pointwise convergent subsequence. Here is a third example, where we are going to prove the following theorem: Let $X$ be a metric space. $A\subseteq X$ . If $\forall \epsilon>0$ , $\exists x_1,x_2,\ldots, x_n\in A, A=\bigcup_{k=1}^n B(x_k,\epsilon)$ (i.e. totally bounded), then all sequences in $A$ has a Cauchy subsequence. Proof. Let $(x_n)\subseteq A$ be a sequence. Let $F_k$ be a finite $(1/k)$ -net of $A$ . Define the sequences of positive integers $n_{r,s}$ as follows: $(x_{n_{1,s}})_{s=1}^\infty$ is the part of $(x_n)$ that lies in $B(p_1,1)$ , where $p_1\in F_1$ . Such $p_1$ exists because $(x_n)$ is infinite but $F_1$ is finite. $(x_{n_{r+1,s}})_{s=1}^\infty$ is the part of $(x_{n_{r,s}})_{s=1}^\infty$ that lies in $B(p_{r+1},1/(r+1))$ . Now, let $n_k=n_{k,k}$ . Then $(x_{n_k})$ is a Cauchy subsequence. Are there any other interesting examples of ""diagonal"" proof in mathematics? From the three examples above, it appears that diagonal arguments help us repeat a process infinitely many times. (For example, the construction of $n_{r,s}$ cannot be repeated to obtain something like $n_{\infty, s}$ , since the interesction of a descending chain of infinite subsets of $\mathbb N$ might be finite -- but a diagonal argument help us get what we want.) What is the essence of ""diagonal""?","I have seen several examples of diagonal arguments. One of them is, of course, Cantor's proof that is not countable. A diagonal argument can also be used to show that every bounded sequence in has a pointwise convergent subsequence. Here is a third example, where we are going to prove the following theorem: Let be a metric space. . If , (i.e. totally bounded), then all sequences in has a Cauchy subsequence. Proof. Let be a sequence. Let be a finite -net of . Define the sequences of positive integers as follows: is the part of that lies in , where . Such exists because is infinite but is finite. is the part of that lies in . Now, let . Then is a Cauchy subsequence. Are there any other interesting examples of ""diagonal"" proof in mathematics? From the three examples above, it appears that diagonal arguments help us repeat a process infinitely many times. (For example, the construction of cannot be repeated to obtain something like , since the interesction of a descending chain of infinite subsets of might be finite -- but a diagonal argument help us get what we want.) What is the essence of ""diagonal""?","\mathbb R \ell^\infty X A\subseteq X \forall \epsilon>0 \exists x_1,x_2,\ldots, x_n\in A, A=\bigcup_{k=1}^n B(x_k,\epsilon) A (x_n)\subseteq A F_k (1/k) A n_{r,s} (x_{n_{1,s}})_{s=1}^\infty (x_n) B(p_1,1) p_1\in F_1 p_1 (x_n) F_1 (x_{n_{r+1,s}})_{s=1}^\infty (x_{n_{r,s}})_{s=1}^\infty B(p_{r+1},1/(r+1)) n_k=n_{k,k} (x_{n_k}) n_{r,s} n_{\infty, s} \mathbb N","['real-analysis', 'functional-analysis', 'set-theory']"
49,"Extrema of $f(x,y)=\sqrt{x^2+y^2} \cdot e^{-(x^{2}+y^{2})}$",Extrema of,"f(x,y)=\sqrt{x^2+y^2} \cdot e^{-(x^{2}+y^{2})}","i have problems solving this task here: We have a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$, $$ f(x,y)=\sqrt{x^2+y^2} \cdot e^{-(x^{2}+y^{2})} $$ Calculate the local extrema of $f$. Decide for all whether it is a strict local minimum or strict local maximum. Find the global maximum and minimum of $f$. My main problem is to calculate the local extrema of $f$. Normally i would calculate the partial derivatives and set them 0. Like: $\frac{\partial f}{\partial x} = \frac{xe^{-x^{2}-y^{2}}}{\sqrt{x^{2}+y^{2}}}-2xe^{-x^{2}-y^{2}}\sqrt{x^{2}+y^{2}}$ $\frac{\partial f}{\partial x} = \frac{e^{-x^{2}-y^{2}}(-2x^{2}y-2y^{3}+y)}{\sqrt{x^{2}+y^{2}}}$ If you just help me finding the points of the local extrema i would be very happy. Sitting now since a few days on this task.","i have problems solving this task here: We have a function $f:\mathbb{R}^2\rightarrow\mathbb{R}$, $$ f(x,y)=\sqrt{x^2+y^2} \cdot e^{-(x^{2}+y^{2})} $$ Calculate the local extrema of $f$. Decide for all whether it is a strict local minimum or strict local maximum. Find the global maximum and minimum of $f$. My main problem is to calculate the local extrema of $f$. Normally i would calculate the partial derivatives and set them 0. Like: $\frac{\partial f}{\partial x} = \frac{xe^{-x^{2}-y^{2}}}{\sqrt{x^{2}+y^{2}}}-2xe^{-x^{2}-y^{2}}\sqrt{x^{2}+y^{2}}$ $\frac{\partial f}{\partial x} = \frac{e^{-x^{2}-y^{2}}(-2x^{2}y-2y^{3}+y)}{\sqrt{x^{2}+y^{2}}}$ If you just help me finding the points of the local extrema i would be very happy. Sitting now since a few days on this task.",,"['calculus', 'functional-analysis', 'partial-derivative']"
50,question on the proof of uniqueness of completion,question on the proof of uniqueness of completion,,"In the book by Erwin Kreyszig, ""Introductory Functional Analysis with Applications"" : I don't understand something in proof of completion theorem. In the fourth part of proof that about uniqueness of completion. The question is why the distances on $\tilde{X}$ and $\hat{X}$ must be the same?  Can anyone explain that for me :)","In the book by Erwin Kreyszig, ""Introductory Functional Analysis with Applications"" : I don't understand something in proof of completion theorem. In the fourth part of proof that about uniqueness of completion. The question is why the distances on $\tilde{X}$ and $\hat{X}$ must be the same?  Can anyone explain that for me :)",,"['analysis', 'functional-analysis', 'isometry']"
51,Closed Unit ball in $\ell^2$ is not compact?,Closed Unit ball in  is not compact?,\ell^2,"How will we prove  that the closed unit ball in $\ell^2$ is closed, bounded, convex but not compact","How will we prove  that the closed unit ball in $\ell^2$ is closed, bounded, convex but not compact",,"['real-analysis', 'functional-analysis', 'compactness', 'lp-spaces']"
52,Orthonormal basis in $L^2(\Omega)$ for bounded $\Omega$,Orthonormal basis in  for bounded,L^2(\Omega) \Omega,"In the one dimension case, where $\Omega\subseteq{\bf R}$ is a bounded domain, for example $\Omega=[0,2\pi]$, one can find a orthonormal basis  $\{e_n\}_{n\in {\bf Z}}$ for $L^2(\Omega)$ where $$e_n(x)=\frac{1}{\sqrt{2\pi}}e^{inx}.$$ In the high dimension, say, $\Omega\subseteq {\bf R}^n$ and $\Omega$ being bounded, can one still "" construct "" the orthonormal basis?","In the one dimension case, where $\Omega\subseteq{\bf R}$ is a bounded domain, for example $\Omega=[0,2\pi]$, one can find a orthonormal basis  $\{e_n\}_{n\in {\bf Z}}$ for $L^2(\Omega)$ where $$e_n(x)=\frac{1}{\sqrt{2\pi}}e^{inx}.$$ In the high dimension, say, $\Omega\subseteq {\bf R}^n$ and $\Omega$ being bounded, can one still "" construct "" the orthonormal basis?",,['real-analysis']
53,Linear Operators on Hilbert Spaces,Linear Operators on Hilbert Spaces,,"Here is a question that I cannot figure out myself: Let $H$ and $K$ be separable Hilbert spaces (possibly infinite-dimensional), and $\lbrace e_i\rbrace_{i\in \mathbb{N}}$ be the orthonormal basis for $H$ . $T: H \to K$ a $\textbf{linear operator (not necessarily bounded)}$ such that $$\sum_{i\in\mathbb{N}}\vert\vert T(e_i)\vert\vert_{K}^2< \infty$$ Where $\vert \vert \cdot\vert\vert_{K}$ is the norm of $K$ induced by inner product. $\textbf{Prove or disprove}$ that $T$ is a bounded linear operator. The key problem here is that I cannot passage the limit, i.e., $T(\sum\limits_{i}e_i) = \sum\limits_{i}T(e_i)$ may not hold. Thanks in advance for any help!","Here is a question that I cannot figure out myself: Let and be separable Hilbert spaces (possibly infinite-dimensional), and be the orthonormal basis for . a such that Where is the norm of induced by inner product. that is a bounded linear operator. The key problem here is that I cannot passage the limit, i.e., may not hold. Thanks in advance for any help!",H K \lbrace e_i\rbrace_{i\in \mathbb{N}} H T: H \to K \textbf{linear operator (not necessarily bounded)} \sum_{i\in\mathbb{N}}\vert\vert T(e_i)\vert\vert_{K}^2< \infty \vert \vert \cdot\vert\vert_{K} K \textbf{Prove or disprove} T T(\sum\limits_{i}e_i) = \sum\limits_{i}T(e_i),"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
54,"""Adding one dimension"" to an infinite dimensional topological vector space","""Adding one dimension"" to an infinite dimensional topological vector space",,"Let $V$ be an infinite-dimensional Hausdorff topological vector space over $\mathbb{R}$ . Is it true that $V\oplus\mathbb{R}$ is isomorphic (as a TVS) to $V$ itself? (How about $V\oplus V$ ?) If yes, I also wonder whether for every $v \ne 0$ in $V$ there exists an isomorphism $f : V \cong V \oplus \mathbb{R}$ such that the projection of $f(v)$ to $\mathbb{R}$ is nonzero. If no: What is the simplest counterexample? What are some minimal conditions (complete/locally convex/normed/inner product etc.) that guarantee the existence of such isomorphisms? (If $V$ is a Hilbert space, desired isomorphisms exist in all three cases above.) I looked briefly into Schaefer and Wolff's Topological Vector Spaces but did not find any clue.","Let be an infinite-dimensional Hausdorff topological vector space over . Is it true that is isomorphic (as a TVS) to itself? (How about ?) If yes, I also wonder whether for every in there exists an isomorphism such that the projection of to is nonzero. If no: What is the simplest counterexample? What are some minimal conditions (complete/locally convex/normed/inner product etc.) that guarantee the existence of such isomorphisms? (If is a Hilbert space, desired isomorphisms exist in all three cases above.) I looked briefly into Schaefer and Wolff's Topological Vector Spaces but did not find any clue.",V \mathbb{R} V\oplus\mathbb{R} V V\oplus V v \ne 0 V f : V \cong V \oplus \mathbb{R} f(v) \mathbb{R} V,['functional-analysis']
55,Fractional Sobolev Spaces and Trace Theory,Fractional Sobolev Spaces and Trace Theory,,"I've been working with fractional Sobolev Spaces for a while and I still don't get how is it connected to trace theory, is there any literature which goes deeper into such relationship? From the boook Fractional Spaces for the Theory of Elliptic PDE by Françoise Demengel   Gilbert Demengel It says that the need of such spaces lies on the existence of the trace for the derivatives , which makes sense since we have things like Neumman conditions. However it doesn't really tell you how a trace is defined for derivatives. The big question is why on such spaces, what is the real advantage on fractional Sobolev spaces and the relation to the distance of traces? And if there is any intuitive idea of such spaces and the need of them? Thanks in advance.","I've been working with fractional Sobolev Spaces for a while and I still don't get how is it connected to trace theory, is there any literature which goes deeper into such relationship? From the boook Fractional Spaces for the Theory of Elliptic PDE by Françoise Demengel   Gilbert Demengel It says that the need of such spaces lies on the existence of the trace for the derivatives , which makes sense since we have things like Neumman conditions. However it doesn't really tell you how a trace is defined for derivatives. The big question is why on such spaces, what is the real advantage on fractional Sobolev spaces and the relation to the distance of traces? And if there is any intuitive idea of such spaces and the need of them? Thanks in advance.",,"['functional-analysis', 'sobolev-spaces', 'trace', 'fractional-sobolev-spaces']"
56,Does each compact operator have a non–zero eigenvector? [closed],Does each compact operator have a non–zero eigenvector? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have learned that 0 must be an eigehvalue for a compact operator. But I do not know if there exists a compact operator that have no non–zero eigenvector. Any hint would be most appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I have learned that 0 must be an eigehvalue for a compact operator. But I do not know if there exists a compact operator that have no non–zero eigenvector. Any hint would be most appreciated.",,"['functional-analysis', 'operator-theory', 'spectral-theory', 'compact-operators']"
57,Spectrum of the Fourier Transform operator,Spectrum of the Fourier Transform operator,,"I am asked to find the spectrum of the Fourier transform operator $\mathcal F:L^2(\mathbb R)\to L^2(\mathbb R)$, where $\mathcal Fu(\omega)=\int_\mathbb R \frac 1{\sqrt{2\pi}}u(x)e^{-ix\omega} dx$. I know that $\mathcal F^4=\mathrm{Id}$, and I read this thread already Two questions in spectral theory: the spectrum of the Fourier transform and the Hamiltonian of the hydrogen atom. . I'm lost in the last passage, how do I show that there is no residual spectrum?","I am asked to find the spectrum of the Fourier transform operator $\mathcal F:L^2(\mathbb R)\to L^2(\mathbb R)$, where $\mathcal Fu(\omega)=\int_\mathbb R \frac 1{\sqrt{2\pi}}u(x)e^{-ix\omega} dx$. I know that $\mathcal F^4=\mathrm{Id}$, and I read this thread already Two questions in spectral theory: the spectrum of the Fourier transform and the Hamiltonian of the hydrogen atom. . I'm lost in the last passage, how do I show that there is no residual spectrum?",,"['functional-analysis', 'analysis', 'fourier-transform']"
58,$Q(Q)$ is not complete while it is a finite dimensional vector space.,is not complete while it is a finite dimensional vector space.,Q(Q),Theorem: Every finite dimensional normed space is complete. $Q$ is a finite-dimensional normed space over $Q$ with the norm $||x||=$ $|x|$. But it is not complete. We have a Cauchy sequence $(1+1/n)^n$ tending to $e\in Q^c$.,Theorem: Every finite dimensional normed space is complete. $Q$ is a finite-dimensional normed space over $Q$ with the norm $||x||=$ $|x|$. But it is not complete. We have a Cauchy sequence $(1+1/n)^n$ tending to $e\in Q^c$.,,"['functional-analysis', 'normed-spaces']"
59,$\operatorname{span}\{x_n:n\in \Bbb N\}$is dense if $\sum_{n=1}^\infty \|x_n-e_n\|^2<1$,is dense if,\operatorname{span}\{x_n:n\in \Bbb N\} \sum_{n=1}^\infty \|x_n-e_n\|^2<1,"Let $H$ be a Hilbert space with orthonormal basis $\{e_1,e_2,\cdots\}$. Suppose $(x_n)$ is a sequence in $H$ with $\sum_{n=1}^\infty \|x_n-e_n\|^2<1$. Claim: The span of the $x_n$ is dense in $H$. I don't have a very good attempt, as I don't know how to use the sum condition. We can express any $x\in H$ as $x=\sum_{k=1}^\infty \langle x,e_k\rangle e_k$. Now we need to find an approximating sequence of $x$ in $\operatorname{span}\{x_1,x_2,\cdots\}$. In a first attempt, I tried using $y_n=\sum_{k=1}^n \langle x,e_k\rangle x_k\in \operatorname{span}\{x_1,x_2,\cdots\}$ as the approximating sequence but it didn't pan out. How could we proceed?","Let $H$ be a Hilbert space with orthonormal basis $\{e_1,e_2,\cdots\}$. Suppose $(x_n)$ is a sequence in $H$ with $\sum_{n=1}^\infty \|x_n-e_n\|^2<1$. Claim: The span of the $x_n$ is dense in $H$. I don't have a very good attempt, as I don't know how to use the sum condition. We can express any $x\in H$ as $x=\sum_{k=1}^\infty \langle x,e_k\rangle e_k$. Now we need to find an approximating sequence of $x$ in $\operatorname{span}\{x_1,x_2,\cdots\}$. In a first attempt, I tried using $y_n=\sum_{k=1}^n \langle x,e_k\rangle x_k\in \operatorname{span}\{x_1,x_2,\cdots\}$ as the approximating sequence but it didn't pan out. How could we proceed?",,"['functional-analysis', 'hilbert-spaces']"
60,"For a hermitian element $a$ in a $C^*$-algebra, show that $\|a^{2n}\| = \|a\|^{2n}$","For a hermitian element  in a -algebra, show that",a C^* \|a^{2n}\| = \|a\|^{2n},"Let $\mathcal{A}$ be a $C^*$-algebra. Suppose that $a \in \mathcal{A}$ with the property that $a^* = a$ (that is, suppose that $a$ is hermitian ). I would like to show that $\|a^{2n}\| = \|a\|^{2n}$ for all $n \ge 1$. This fact is stated in Conway's A Course in Functional Analysis , and I'm having trouble proving it. Here's what I have so far: the $n =1 $ base cases just uses the basic $C^*$-algebra property: $$\|a^2\| = \|a^* a \| = \|a\|^2.$$ But now I get stuck on the induction step (assume we have $\|a^{2n}\| = \|a\|^{2n}$, show that $\|a^{2n + 2}\| = \|a\|^{2n + 2}$). I have been trying to give myself some insight by studying small cases, for instance, $n = 3$: $$\|a^6\| = \|(a^3)^2\| = \|a^3\|^2,$$ where the last equality follows from the base case. But then I end up with an odd exponent inside the norm. This is what is really giving me trouble. Hints or solutions are greatly appreciated.","Let $\mathcal{A}$ be a $C^*$-algebra. Suppose that $a \in \mathcal{A}$ with the property that $a^* = a$ (that is, suppose that $a$ is hermitian ). I would like to show that $\|a^{2n}\| = \|a\|^{2n}$ for all $n \ge 1$. This fact is stated in Conway's A Course in Functional Analysis , and I'm having trouble proving it. Here's what I have so far: the $n =1 $ base cases just uses the basic $C^*$-algebra property: $$\|a^2\| = \|a^* a \| = \|a\|^2.$$ But now I get stuck on the induction step (assume we have $\|a^{2n}\| = \|a\|^{2n}$, show that $\|a^{2n + 2}\| = \|a\|^{2n + 2}$). I have been trying to give myself some insight by studying small cases, for instance, $n = 3$: $$\|a^6\| = \|(a^3)^2\| = \|a^3\|^2,$$ where the last equality follows from the base case. But then I end up with an odd exponent inside the norm. This is what is really giving me trouble. Hints or solutions are greatly appreciated.",,"['functional-analysis', 'c-star-algebras', 'banach-algebras']"
61,Relation of Hodge Theorem to Eigenfunction Basis of Laplacian,Relation of Hodge Theorem to Eigenfunction Basis of Laplacian,,"The classical Hodge theorem I know of relates the de Rham cohomology groups isomorphically to the space of harmonic forms and shows that $Id=\pi+\Delta G$ , where $\pi$ is the harmonic projection of $k$ -forms and $G$ is the Green's Operator for the Laplacian $\Delta=d\delta +\delta d$ . ( http://en.wikipedia.org/wiki/Hodge_theory ) It isn't clear to me how this relates to the following ""Hodge theorem"" from The Laplacian on a Riemannian Manifold by Steven Rosenberg : Let $(M,g)$ be a compact, connected oriented Riemannian manifold. Then there exist an orthonormal basis of eigenfunctions (eigenforms) for $L^2(M,g)$ (or $L^2\Lambda^k(M,g)$ ) of the Laplacian. $0$ is an eigenvalue with multiplicity 1, and all other eigenvalues are strictly positive, accumulate at infinity, and have finite multiplicities. In particular, I don't see the connection to the orthonormal basis of eigenfunctions part: how does this follow from the classical version?","The classical Hodge theorem I know of relates the de Rham cohomology groups isomorphically to the space of harmonic forms and shows that , where is the harmonic projection of -forms and is the Green's Operator for the Laplacian . ( http://en.wikipedia.org/wiki/Hodge_theory ) It isn't clear to me how this relates to the following ""Hodge theorem"" from The Laplacian on a Riemannian Manifold by Steven Rosenberg : Let be a compact, connected oriented Riemannian manifold. Then there exist an orthonormal basis of eigenfunctions (eigenforms) for (or ) of the Laplacian. is an eigenvalue with multiplicity 1, and all other eigenvalues are strictly positive, accumulate at infinity, and have finite multiplicities. In particular, I don't see the connection to the orthonormal basis of eigenfunctions part: how does this follow from the classical version?","Id=\pi+\Delta G \pi k G \Delta=d\delta +\delta d (M,g) L^2(M,g) L^2\Lambda^k(M,g) 0","['functional-analysis', 'differential-geometry', 'spectral-theory', 'hodge-theory']"
62,Proving $\|T^n\|\leq \|T\|^n$,Proving,\|T^n\|\leq \|T\|^n,"Let $T:X\to X$ be a linear bounded mapping. I have to prove $\|T^n\|\leq \|T\|^n$. Let $Tx=cx$, where $c>0$. This is a linear mapping. $$T^2 x=T(Tx)=T(cx)=cTx=c^2 x.$$ Hence $\|T^2x\|=c^2\|x\|.$ Similarly, $\|T^n x\|=c^n\|x\|$. $$\|Tx\|^n=c^n \|x\|^n.$$ Hence, if $\|T^n\|\leq \|T\|^n$, then $c^n\|x\|\leq c^n\|x\|^n\implies \|x\|^{n-1}\geq 1$. How is this always true? Isn't this dependent on the condition that $\|x\|\geq 1$? Thanks in advance!","Let $T:X\to X$ be a linear bounded mapping. I have to prove $\|T^n\|\leq \|T\|^n$. Let $Tx=cx$, where $c>0$. This is a linear mapping. $$T^2 x=T(Tx)=T(cx)=cTx=c^2 x.$$ Hence $\|T^2x\|=c^2\|x\|.$ Similarly, $\|T^n x\|=c^n\|x\|$. $$\|Tx\|^n=c^n \|x\|^n.$$ Hence, if $\|T^n\|\leq \|T\|^n$, then $c^n\|x\|\leq c^n\|x\|^n\implies \|x\|^{n-1}\geq 1$. How is this always true? Isn't this dependent on the condition that $\|x\|\geq 1$? Thanks in advance!",,[]
63,Is projection on a closed subspace of a Banach space bounded?,Is projection on a closed subspace of a Banach space bounded?,,We know that any projection on a closed subspace of a Hilbert space is bounded. Is it true for any Banach space?  Any help would be appreciable.,We know that any projection on a closed subspace of a Hilbert space is bounded. Is it true for any Banach space?  Any help would be appreciable.,,['functional-analysis']
64,Why are positive linear functionals on $C^*$-algebras always bounded?,Why are positive linear functionals on -algebras always bounded?,C^*,We say that a linear functional $f$ on a $C^*$ -algebra $A$ is positive if $f(a^*a)\geq 0$ for all $a\in A$ . Why must it be the case that every positive linear functional on a $C^*$ -algebra is bounded?,We say that a linear functional on a -algebra is positive if for all . Why must it be the case that every positive linear functional on a -algebra is bounded?,f C^* A f(a^*a)\geq 0 a\in A C^*,"['functional-analysis', 'c-star-algebras']"
65,Weak limit and strong limit,Weak limit and strong limit,,"Let $X$ be a Banach space and let $x_n \overbrace{\rightarrow}^w x$ and $x_n \overbrace{\rightarrow}^s z$ can we then say that $x = z$? My try: $$\| x- z\| = \sup_{\ell \leq 1} |\ell(x-z)| = \sup_{\ell \leq 1} |\ell x -\ell z| \leq \epsilon$$ Where $\ell$ is a continuous functional in $X'$ Is this correct? is there any easier way? Thanks Btw if this already is correct, should I delete the post or  what do I do?","Let $X$ be a Banach space and let $x_n \overbrace{\rightarrow}^w x$ and $x_n \overbrace{\rightarrow}^s z$ can we then say that $x = z$? My try: $$\| x- z\| = \sup_{\ell \leq 1} |\ell(x-z)| = \sup_{\ell \leq 1} |\ell x -\ell z| \leq \epsilon$$ Where $\ell$ is a continuous functional in $X'$ Is this correct? is there any easier way? Thanks Btw if this already is correct, should I delete the post or  what do I do?",,"['functional-analysis', 'banach-spaces', 'operator-theory']"
66,Relationship between integral equations and partial differential equations,Relationship between integral equations and partial differential equations,,"In my functional analysis class, we did a lot of problems involving integral equations such as proving existence & uniqueness using spectral theory and the Banach fixed point theorem. I've never seen integral equations outside of functional analysis, but apparently they are useful for ordinary/partial differential equations. If someone familiar with integral equation methods could give some motivation, I would really appreciate it. Also, are there any good textbooks that discuss integral equation methods for PDE's?","In my functional analysis class, we did a lot of problems involving integral equations such as proving existence & uniqueness using spectral theory and the Banach fixed point theorem. I've never seen integral equations outside of functional analysis, but apparently they are useful for ordinary/partial differential equations. If someone familiar with integral equation methods could give some motivation, I would really appreciate it. Also, are there any good textbooks that discuss integral equation methods for PDE's?",,"['functional-analysis', 'partial-differential-equations', 'integral-equations']"
67,Why it is sufficient to show $|f'(z)-1|<1$?,Why it is sufficient to show ?,|f'(z)-1|<1,"According to an article entitled ""On the Univalency of Certain Analytic Functions"" by Wang et al. (2006) , we have to show that $|f'(z)-1|<1$ in order to find the radius of univalency for the class $Q(\alpha,\beta,\gamma)$. Note that $Q(\alpha,\beta,\gamma)$ denotes the class of functions of the form $$f(z)=z+a_{2}z^{2}+\cdots$$ which are analytic in open unit disk, $D=\{z:|z|<1\}$, and satisfy the condition $$\mathfrak{Re} \left\{\frac{\alpha f(z)}{z}+\beta f'(z)\right\}>\gamma \qquad (\alpha, \beta >0;\ 0 \leq \gamma<\alpha+ \beta\leq 1;\ z\in D)$$ Why it is sufficient to show that $|f'(z)-1|<1$?","According to an article entitled ""On the Univalency of Certain Analytic Functions"" by Wang et al. (2006) , we have to show that $|f'(z)-1|<1$ in order to find the radius of univalency for the class $Q(\alpha,\beta,\gamma)$. Note that $Q(\alpha,\beta,\gamma)$ denotes the class of functions of the form $$f(z)=z+a_{2}z^{2}+\cdots$$ which are analytic in open unit disk, $D=\{z:|z|<1\}$, and satisfy the condition $$\mathfrak{Re} \left\{\frac{\alpha f(z)}{z}+\beta f'(z)\right\}>\gamma \qquad (\alpha, \beta >0;\ 0 \leq \gamma<\alpha+ \beta\leq 1;\ z\in D)$$ Why it is sufficient to show that $|f'(z)-1|<1$?",,"['analysis', 'functional-analysis', 'analytic-geometry', 'convex-analysis']"
68,Is a continuous function on compact convex set where the boundary is mapped to the set a self mapping?,Is a continuous function on compact convex set where the boundary is mapped to the set a self mapping?,,"Let $K ⊂ R^n$ be convex and compact with $0$ in the interior of $K$ . Let $f ∈ C(K, R^n)$ with $f(∂K) ⊂ K$ . If this is the case, do we in fact have $f(K) \subset K$ . It is probably not the case that the image of a convex set is convex as those are hard to prove, but we at least know it is compact and connected (also path-connected). But at the same time, just being continuous is not a strong enough property to make further conclusions. The image of boundary is not necessarily a boundary unless $f$ is a diffeomorphism, but here we don't even have a homeomorphism, but just continuity.","Let be convex and compact with in the interior of . Let with . If this is the case, do we in fact have . It is probably not the case that the image of a convex set is convex as those are hard to prove, but we at least know it is compact and connected (also path-connected). But at the same time, just being continuous is not a strong enough property to make further conclusions. The image of boundary is not necessarily a boundary unless is a diffeomorphism, but here we don't even have a homeomorphism, but just continuity.","K ⊂ R^n 0 K f ∈ C(K, R^n) f(∂K) ⊂ K f(K) \subset K f","['real-analysis', 'functional-analysis', 'analysis', 'nonlinear-analysis']"
69,"If $\varphi: A \to B$ is surjective, then is $\varphi^{**}: A^{**}\to B^{**}$ surjective?","If  is surjective, then is  surjective?",\varphi: A \to B \varphi^{**}: A^{**}\to B^{**},"Let $A$ and $B$ be $C^*$ -algebras and consider a $*$ -homomorphism $\varphi: A \to B$ . Then the biduals $A^{**}$ and $B^{**}$ carry natural $C^*$ -structures (coming from the enveloping von Neumann algebra) so that the map $$\varphi^{**}: A^{**}\to B^{**}$$ is again a $*$ -morphism. If $\varphi$ is isometric (i.e. injective), it is easily verified (using general properties of adjoint maps on normed spaces) that $\varphi^{**}: A^{**}\to B^{**}$ is again isometric. I'm now wondering if surjectivity of $\varphi$ implies surjectivity of $\varphi^{**}$ . We can't use the straightforward route: we have $\varphi$ surjective $\implies $ $\varphi^*$ injective but this does not allow us to conclude that $\varphi^{**}$ is surjective. I'm starting to think there may be a counterexample!","Let and be -algebras and consider a -homomorphism . Then the biduals and carry natural -structures (coming from the enveloping von Neumann algebra) so that the map is again a -morphism. If is isometric (i.e. injective), it is easily verified (using general properties of adjoint maps on normed spaces) that is again isometric. I'm now wondering if surjectivity of implies surjectivity of . We can't use the straightforward route: we have surjective injective but this does not allow us to conclude that is surjective. I'm starting to think there may be a counterexample!",A B C^* * \varphi: A \to B A^{**} B^{**} C^* \varphi^{**}: A^{**}\to B^{**} * \varphi \varphi^{**}: A^{**}\to B^{**} \varphi \varphi^{**} \varphi \implies  \varphi^* \varphi^{**},"['functional-analysis', 'banach-spaces', 'c-star-algebras', 'von-neumann-algebras']"
70,"If $f''\ge 0$, prove that $f(x+f'(x)) \ge f(x)$","If , prove that",f''\ge 0 f(x+f'(x)) \ge f(x),"Question: $f$ and $f'$ are differentiable, and $f''\ge 0$ . Then, prove that $\forall x \in \mathbb R$ , $f(x+f'(x))\ge f(x)$ . Since $f''\ge 0$ , I'd like to apply Jensen's theorem, which is: $$f(tx_1 + (1-t)x_2) \le tf(x_1) + (1-t)f(x_2) $$ However, it was hard to determine the value of $x_1$ and $x_2$ . Another way came up to my mind was to set the new function $$g(x)=f(x+f'(x))-f(x)$$ and prove that $g(x)\ge 0$ by using $g'(x)$ . Unfortunately, when we calculate the derivation of $g(x)$ as following: $$ g'(x)= f''(x)f'(x+f'(x))-f'(x)$$ eventually, there was nothing I can find. Could you give some key points to this proof? Thanks for your advice.","Question: and are differentiable, and . Then, prove that , . Since , I'd like to apply Jensen's theorem, which is: However, it was hard to determine the value of and . Another way came up to my mind was to set the new function and prove that by using . Unfortunately, when we calculate the derivation of as following: eventually, there was nothing I can find. Could you give some key points to this proof? Thanks for your advice.",f f' f''\ge 0 \forall x \in \mathbb R f(x+f'(x))\ge f(x) f''\ge 0 f(tx_1 + (1-t)x_2) \le tf(x_1) + (1-t)f(x_2)  x_1 x_2 g(x)=f(x+f'(x))-f(x) g(x)\ge 0 g'(x) g(x)  g'(x)= f''(x)f'(x+f'(x))-f'(x),"['calculus', 'functional-analysis', 'convex-analysis', 'functional-inequalities']"
71,What is the space of random variables?,What is the space of random variables?,,"I know that you can define function spaces, and random variables are essentially functions. But from my limited knowledge, I have never ever came across the notion of the ""space of random variables"" and if it is practical to even define these sets. So, let $X: \{\text{events}\} \to \mathbb{R} $ be any random variable with property $P$ . Is there a name for the space of all such random variables $\mathcal{X} = \{X | X \text{ satisfies } P\}$ for some particular properties $P$ ?","I know that you can define function spaces, and random variables are essentially functions. But from my limited knowledge, I have never ever came across the notion of the ""space of random variables"" and if it is practical to even define these sets. So, let be any random variable with property . Is there a name for the space of all such random variables for some particular properties ?",X: \{\text{events}\} \to \mathbb{R}  P \mathcal{X} = \{X | X \text{ satisfies } P\} P,"['functional-analysis', 'probability-theory', 'vector-spaces', 'soft-question', 'random-variables']"
72,$X'$ finite-dimensional implies $X$ finite-dimensional,finite-dimensional implies  finite-dimensional,X' X,"How would one prove, for any normed space $X$ that if $X'$ is finite dimensional, then $X$ is finite-dimensional? Here $X'$ denotes the space of all bounded functionals $f: X \to\mathbb F$ If anyone would give me a hint, no need for full answer. I really don't know where to start. Appreciate all the answers.","How would one prove, for any normed space that if is finite dimensional, then is finite-dimensional? Here denotes the space of all bounded functionals If anyone would give me a hint, no need for full answer. I really don't know where to start. Appreciate all the answers.",X X' X X' f: X \to\mathbb F,"['functional-analysis', 'normed-spaces', 'dual-spaces']"
73,"Why is $d(x,0)$ not a norm?",Why is  not a norm?,"d(x,0)","If $\|x\|$ is a norm, then we can define $d(x,y):=\|x-y\|$ and it will be a metric. Now, if $d$ is a metric, why is $\|x\|:= d(x,0)$ not a norm? I think it fail for the sub-linearity, but I don't see how.","If $\|x\|$ is a norm, then we can define $d(x,y):=\|x-y\|$ and it will be a metric. Now, if $d$ is a metric, why is $\|x\|:= d(x,0)$ not a norm? I think it fail for the sub-linearity, but I don't see how.",,['functional-analysis']
74,Density of finite rank operator in compact operators on Hilbert spaces,Density of finite rank operator in compact operators on Hilbert spaces,,"Show the subspace of finite rank operators (defined on a Hilbert space) is dense   in the space of all compact operators. I just started reading about compact operators and I saw this questions.  So far I learnt any finite rank operator on a Hilbert space is compact, to show it's dense in space of all compact operators, there must exist a sequence of finite rank operator $(T_n)$  such that it converges to compact operator $T$, but I'm not sure how I can construct this sequence, I may need to think about complete orthogonal sequence $\{e_1\:,e_2\dots\} $ of the Hilbert spaces, so I get  $$T(x)=\sum_{i=1}^{\infty}\langle T(x),e_i\rangle e_i$$ If I define $T_n(x)=\sum_{i=1}^{n}\langle T(x),e_i\rangle e_i$, this is off course a finite dimensional and compact operator. Can I say it's the sequence that converges to $T$? Or maybe I'm totally wrong, but I appreciate, if you give me some hints about this.","Show the subspace of finite rank operators (defined on a Hilbert space) is dense   in the space of all compact operators. I just started reading about compact operators and I saw this questions.  So far I learnt any finite rank operator on a Hilbert space is compact, to show it's dense in space of all compact operators, there must exist a sequence of finite rank operator $(T_n)$  such that it converges to compact operator $T$, but I'm not sure how I can construct this sequence, I may need to think about complete orthogonal sequence $\{e_1\:,e_2\dots\} $ of the Hilbert spaces, so I get  $$T(x)=\sum_{i=1}^{\infty}\langle T(x),e_i\rangle e_i$$ If I define $T_n(x)=\sum_{i=1}^{n}\langle T(x),e_i\rangle e_i$, this is off course a finite dimensional and compact operator. Can I say it's the sequence that converges to $T$? Or maybe I'm totally wrong, but I appreciate, if you give me some hints about this.",,"['functional-analysis', 'hilbert-spaces', 'compact-operators']"
75,"Hyperplane separation theorem, continuous linear functional","Hyperplane separation theorem, continuous linear functional",,One version of the Hyperplane separation theorem states that two non-intersecting convex sets in a normed linear space can be separated by a linear functional.  In the proof I know they use the gauge (or Minkowsky functional) to show this. There's another version of this theorem that states that two such convex sets can be separated by a continuous linear functional. I wonder if somebody can explain to me how. Is the gauge bounded by the norm ( $ p_K(x)\leq C\lVert x \rVert$)?Is this true for all $x$ in a normed linear space?,One version of the Hyperplane separation theorem states that two non-intersecting convex sets in a normed linear space can be separated by a linear functional.  In the proof I know they use the gauge (or Minkowsky functional) to show this. There's another version of this theorem that states that two such convex sets can be separated by a continuous linear functional. I wonder if somebody can explain to me how. Is the gauge bounded by the norm ( $ p_K(x)\leq C\lVert x \rVert$)?Is this true for all $x$ in a normed linear space?,,"['functional-analysis', 'normed-spaces', 'topological-vector-spaces']"
76,Why are two characters of a commutative Banach algebra with the same kernel equal?,Why are two characters of a commutative Banach algebra with the same kernel equal?,,"Let $A$ be a commutative Banach algebra. Let $\chi_1$ and $\chi_2$ be characters of $A$. I am having some difficulty seeing why the following statement is true: If $\ker \chi_1 = \ker\chi_2$, then since $\chi_1(\textbf{1})=\chi_2(\textbf{1})=\textbf{1}$, we have that $\chi_1 = \chi_2$.","Let $A$ be a commutative Banach algebra. Let $\chi_1$ and $\chi_2$ be characters of $A$. I am having some difficulty seeing why the following statement is true: If $\ker \chi_1 = \ker\chi_2$, then since $\chi_1(\textbf{1})=\chi_2(\textbf{1})=\textbf{1}$, we have that $\chi_1 = \chi_2$.",,"['functional-analysis', 'banach-algebras']"
77,Open subsets of the space of linear operators,Open subsets of the space of linear operators,,"If we have a Banach space $X$ and we consider the space $L(X,X)$ of linear operators. Now we have the operator norm here and this induces a metric, which in turn induces the topology. Since this is a metric space, we have the notion of open sets. So how would one show that a certain subspace is open? I'm having difficulty seeing what the open balls look like. So the question from Folland says that: if $T$ is invertible, and $||S-T||\leq||T^{-1}||^{-1}$, then S is invertible. He then concludes that the set of Invertible Operators is open. I dont see how this follows, could someone explain please?","If we have a Banach space $X$ and we consider the space $L(X,X)$ of linear operators. Now we have the operator norm here and this induces a metric, which in turn induces the topology. Since this is a metric space, we have the notion of open sets. So how would one show that a certain subspace is open? I'm having difficulty seeing what the open balls look like. So the question from Folland says that: if $T$ is invertible, and $||S-T||\leq||T^{-1}||^{-1}$, then S is invertible. He then concludes that the set of Invertible Operators is open. I dont see how this follows, could someone explain please?",,['functional-analysis']
78,Is $C(\mathbb R)$ Complete?,Is  Complete?,C(\mathbb R),"I'm trying to prove an exercise from Carthers' book chapter10 of Real Analysis, problem claimed as, where $C(\mathbb R)$ denote the infinity norm space of all continuous functions on real line. I tried to use the hint. However, I got an counterexample that probably works for incomplete $C([-n,n])$. That is: $f_n(x) = x^{2n}$ does not converge to a continuous function in $C([-1,1])$. Where is my fault?","I'm trying to prove an exercise from Carthers' book chapter10 of Real Analysis, problem claimed as, where $C(\mathbb R)$ denote the infinity norm space of all continuous functions on real line. I tried to use the hint. However, I got an counterexample that probably works for incomplete $C([-n,n])$. That is: $f_n(x) = x^{2n}$ does not converge to a continuous function in $C([-1,1])$. Where is my fault?",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
79,Find a space whose dual does not separate points,Find a space whose dual does not separate points,,"I read about the fact that for a locally convex topological vector space $X$, its dual $X^*$ separates points, i.e. for any $x\neq y$ in $X$, $\exists f \in X^*$ such that $f(x)\neq f(y)$. Could you help me to find a non locally convex topological vector space such that its dual does not separate points? Thanks","I read about the fact that for a locally convex topological vector space $X$, its dual $X^*$ separates points, i.e. for any $x\neq y$ in $X$, $\exists f \in X^*$ such that $f(x)\neq f(y)$. Could you help me to find a non locally convex topological vector space such that its dual does not separate points? Thanks",,['functional-analysis']
80,Banach Spaces: Totally Bounded Subsets,Banach Spaces: Totally Bounded Subsets,,"As an easy consequence of Riesz' lemma it is known that infinite dimensional Banach spaces possess bounded subsets that fail to be totally bounded. On the other hand in finite dimensional Banach spaces any bounded subset happens to be totally bounded as well. So the question arises wether totally bounded subsets always sit within finite dimensional spaces, or put in equivalent form: Is there an infinite dimensional space which possesses a totally bounded subset not lying entirely in a finite dimensional subspace?","As an easy consequence of Riesz' lemma it is known that infinite dimensional Banach spaces possess bounded subsets that fail to be totally bounded. On the other hand in finite dimensional Banach spaces any bounded subset happens to be totally bounded as well. So the question arises wether totally bounded subsets always sit within finite dimensional spaces, or put in equivalent form: Is there an infinite dimensional space which possesses a totally bounded subset not lying entirely in a finite dimensional subspace?",,"['functional-analysis', 'banach-spaces']"
81,Book suggestion geometry of Banach spaces,Book suggestion geometry of Banach spaces,,I am studying geometry of Banach spaces and applications in metric fixed point theory. Does anyone have a good recommendation of books//lectures/resources/etc.? Thanks.,I am studying geometry of Banach spaces and applications in metric fixed point theory. Does anyone have a good recommendation of books//lectures/resources/etc.? Thanks.,,"['functional-analysis', 'reference-request', 'banach-spaces', 'book-recommendation']"
82,"$C([0,1])$ is not weakly sequentially complete.",is not weakly sequentially complete.,"C([0,1])","I'm studying Functional Analysis by myself. For a counterexample of every Banach space is not weakly sequentially complete, I was suggested to check $C([0,1])$ is not weakly sequentially complete. For this, let $f_n(t)=1-nt$ if $0\leq t\leq \frac{1}{n}$ and $f_n(t)=0$ if $\frac{1}{n}\leq t \leq 1$. But I do not know how to use monotone convergence theorem to show $\{f_n\}$ is weakly Cauchy and also why it is not weakly sequentially complete. Please help me. Thanks in advance.","I'm studying Functional Analysis by myself. For a counterexample of every Banach space is not weakly sequentially complete, I was suggested to check $C([0,1])$ is not weakly sequentially complete. For this, let $f_n(t)=1-nt$ if $0\leq t\leq \frac{1}{n}$ and $f_n(t)=0$ if $\frac{1}{n}\leq t \leq 1$. But I do not know how to use monotone convergence theorem to show $\{f_n\}$ is weakly Cauchy and also why it is not weakly sequentially complete. Please help me. Thanks in advance.",,['functional-analysis']
83,A counter example of best approximation,A counter example of best approximation,,"Construct a point $f\in C[0,1]$ and a closed subspace $V\subset C[0,1]$ such that $f$ does not have a best approximation in $V$. Definition: $C[0,1]$ is the set of countinous function with the norm $||f||=\max\limits_{x\in[a,b]}|f(x)|$. Let $B$ be a normed vector space with norm $||\cdot||$. Set $V\subset B$ and $y_0\in B$.  A vector $x_0\in V$ is called the best approximation of $y_0$ in V if  $$||y_0-x_0||=\operatorname{dist}(y_0,V)=\inf_{x\in V}||y_0-x||$$","Construct a point $f\in C[0,1]$ and a closed subspace $V\subset C[0,1]$ such that $f$ does not have a best approximation in $V$. Definition: $C[0,1]$ is the set of countinous function with the norm $||f||=\max\limits_{x\in[a,b]}|f(x)|$. Let $B$ be a normed vector space with norm $||\cdot||$. Set $V\subset B$ and $y_0\in B$.  A vector $x_0\in V$ is called the best approximation of $y_0$ in V if  $$||y_0-x_0||=\operatorname{dist}(y_0,V)=\inf_{x\in V}||y_0-x||$$",,"['functional-analysis', 'approximation', 'examples-counterexamples', 'normed-spaces']"
84,"Prove: every subspace of finite codimension is dense in $L^p$ with $p \in (0,1)$",Prove: every subspace of finite codimension is dense in  with,"L^p p \in (0,1)","$X$: vector space, $N$ is subspace of $X$, the codimension of $N$ in $X$ is, by definition, the dimension of the quotient space $X/N$. Suppose $0<p<1$ and prove that every subspace of finite codimension is den in $L^p$ (Problem 1.11,p.39 in Functional analysis)(Rudin). Thanks in advance.","$X$: vector space, $N$ is subspace of $X$, the codimension of $N$ in $X$ is, by definition, the dimension of the quotient space $X/N$. Suppose $0<p<1$ and prove that every subspace of finite codimension is den in $L^p$ (Problem 1.11,p.39 in Functional analysis)(Rudin). Thanks in advance.",,"['real-analysis', 'complex-analysis', 'functional-analysis']"
85,"Function $f$ such that Fourier-series converges uniformly, but the series of the derivatives are divergent","Function  such that Fourier-series converges uniformly, but the series of the derivatives are divergent",f,"I am studying Fourier-transformation right now, and I am asking if there exists a function $f$ such that is Fourier-series converges uniformly, the Fourier-series of $f'$ only in $L_2$ and that $f''$ is divergent?","I am studying Fourier-transformation right now, and I am asking if there exists a function $f$ such that is Fourier-series converges uniformly, the Fourier-series of $f'$ only in $L_2$ and that $f''$ is divergent?",,"['functional-analysis', 'fourier-analysis', 'fourier-series', 'distribution-theory']"
86,Normed Linear Space: Why does $\|x_n\|\to 0$ imply $x_n \to 0$?,Normed Linear Space: Why does  imply ?,\|x_n\|\to 0 x_n \to 0,"I can prove the contrapositive: $x_n$ does not tend to $0$ implies either: $x_n$ diverges (does not converge), in which case neither does $\|x_n\|$, or $x_n$ converges to $x \neq 0$ which implies $\|x_n\|$ converges to $\|x\|\neq 0$. Okay, but is there a simpler way to do this?","I can prove the contrapositive: $x_n$ does not tend to $0$ implies either: $x_n$ diverges (does not converge), in which case neither does $\|x_n\|$, or $x_n$ converges to $x \neq 0$ which implies $\|x_n\|$ converges to $\|x\|\neq 0$. Okay, but is there a simpler way to do this?",,"['functional-analysis', 'normed-spaces']"
87,"$C[0,1]$ is NOT a Banach Space w.r.t $\|\cdot\|_2$",is NOT a Banach Space w.r.t,"C[0,1] \|\cdot\|_2","I'm trying to find a cauchy sequence in $C[0,1]$ that converges under $\|\cdot\|_2$ to a limit which isn't continuous. Any ideas?","I'm trying to find a cauchy sequence in $C[0,1]$ that converges under $\|\cdot\|_2$ to a limit which isn't continuous. Any ideas?",,"['functional-analysis', 'banach-spaces', 'hilbert-spaces']"
88,"if $A^k$ is compact for some positive $k$, is it possible that $A$ is not bounded?","if  is compact for some positive , is it possible that  is not bounded?",A^k k A,"If $A$ is a linear operator on a normed vector space $X$ and $A^k$ is compact for some positive integer $k$, is it possible that $A$ is unbounded? If not, how to prove that $A$ is bounded?","If $A$ is a linear operator on a normed vector space $X$ and $A^k$ is compact for some positive integer $k$, is it possible that $A$ is unbounded? If not, how to prove that $A$ is bounded?",,"['functional-analysis', 'operator-theory']"
89,Does this contradict the Closed Graph Theorem?,Does this contradict the Closed Graph Theorem?,,"Question: Let $C^{'}[0,1]$ and $C[0,1]$ be endowed with sup norm. Define $T:C^{'}[0,1]\to C[0,1]$ by $$Tf=f^{'}\text{ for each }f\in C^{'}[0,1]$$ Where, $'$, indicates differentiation. Prove that $T$ is a linear map with closed graph but it is not bounded. $Proof:$ For linearity: Let $f_{1}$, $f_{2}\in C^{'}[0,1]$ $\alpha,\beta$ scalars, then $$T(\alpha f_{1}+\beta f_{2})=(\alpha f_{1}+\beta f_{2})^{'}=\alpha f_{1}^{'}+\beta f_{2}^{'}=\alpha Tf_{1}+\beta Tf_{2}.$$ For the closer of $T$, Let ${f_{n}}$ be a sequence in $C^{'}[0,1]$ such that $f_{n}\to f$ and $Tf_{n}=f_{n}^{'}\to y$. Then $\|Tf_{n}-y\|=\sup_{t\in [0,1]}|T(f_{n})(t)-y(t)|=\sup_{t\in [0,1]}|T(f_{n}^{'})(t)-y(t)|\to 0$ as $n\to \infty$ Thus, the convergence is uniform and $y(t)=\lim_{n\to \infty}f_{n}^{'}(t)$. Since the convergence is uniform $f^{'}(t)=y(t)$ for all $t\in [0,1]$. Thus, $f\in C^{'}[0,1]$ and $Tf=y$ so $T$ is closed. To show $T$ is not bounded, take $f_{n}(t)=t^{n}$, then $\|f_{n}\|=\sup_{t\in [0,1]}|t^{n}|=1$ and $f^{'}_{n}(t)=nt^{n-1}$ so that $\|Tf_{n}\|=\sup_{t\in [0,1]}|nt^{n-1}|=n$. Thus, $T$ is not bounded. This implies $T$ is not continuous. My question here is, does this example contradict the Closed Graph theorem? Which says: If you have two Banach spaces $X$ and $Y$ and $T$ a linear map from $X$ to $Y$ such that the graph of $T$ is closed. Then $T$ is continuous.","Question: Let $C^{'}[0,1]$ and $C[0,1]$ be endowed with sup norm. Define $T:C^{'}[0,1]\to C[0,1]$ by $$Tf=f^{'}\text{ for each }f\in C^{'}[0,1]$$ Where, $'$, indicates differentiation. Prove that $T$ is a linear map with closed graph but it is not bounded. $Proof:$ For linearity: Let $f_{1}$, $f_{2}\in C^{'}[0,1]$ $\alpha,\beta$ scalars, then $$T(\alpha f_{1}+\beta f_{2})=(\alpha f_{1}+\beta f_{2})^{'}=\alpha f_{1}^{'}+\beta f_{2}^{'}=\alpha Tf_{1}+\beta Tf_{2}.$$ For the closer of $T$, Let ${f_{n}}$ be a sequence in $C^{'}[0,1]$ such that $f_{n}\to f$ and $Tf_{n}=f_{n}^{'}\to y$. Then $\|Tf_{n}-y\|=\sup_{t\in [0,1]}|T(f_{n})(t)-y(t)|=\sup_{t\in [0,1]}|T(f_{n}^{'})(t)-y(t)|\to 0$ as $n\to \infty$ Thus, the convergence is uniform and $y(t)=\lim_{n\to \infty}f_{n}^{'}(t)$. Since the convergence is uniform $f^{'}(t)=y(t)$ for all $t\in [0,1]$. Thus, $f\in C^{'}[0,1]$ and $Tf=y$ so $T$ is closed. To show $T$ is not bounded, take $f_{n}(t)=t^{n}$, then $\|f_{n}\|=\sup_{t\in [0,1]}|t^{n}|=1$ and $f^{'}_{n}(t)=nt^{n-1}$ so that $\|Tf_{n}\|=\sup_{t\in [0,1]}|nt^{n-1}|=n$. Thus, $T$ is not bounded. This implies $T$ is not continuous. My question here is, does this example contradict the Closed Graph theorem? Which says: If you have two Banach spaces $X$ and $Y$ and $T$ a linear map from $X$ to $Y$ such that the graph of $T$ is closed. Then $T$ is continuous.",,['functional-analysis']
90,Existence of Banach limits: Translation invariance,Existence of Banach limits: Translation invariance,,"A positive functional $\Phi$ on $\ell^{\infty}$ is said to be a Banach limit if $\Phi(1,1,1,\ldots)=1$ and $\Phi\circ L=\Phi$ where $L$ is the left shift operator on $\ell^{\infty}$ . Show that there exists a Banach limit. My attempt: Consider the functionals $p: (x_n)\mapsto \limsup x_n$ on $\ell^{\infty}$ and $f:(x_n)\mapsto \lim x_n$ on $c$ (the space of convergent sequences). Then $f$ is linear, $p$ is sublinear, and $f\le p$ on $c$ . Therefore  by Hahn Banach theorem, there exists a functional $\Phi$ on $\ell^{\infty}$ such that $\Phi=f$ on $c$ and $\Phi\le p$ on $\ell^{\infty}$ . As $f$ is positive, so is $\Phi$ . Moreover, $\Phi(1,1,1,\ldots)=f(1,1,1,\ldots)=1$ . My question: How to show that $\Phi\circ L=\Phi$ ? I observed the following: $f\circ L=f=p$ on $c$ and $p\circ L=p$ on $\ell^{\infty}$ but don't know if it's useful.","A positive functional on is said to be a Banach limit if and where is the left shift operator on . Show that there exists a Banach limit. My attempt: Consider the functionals on and on (the space of convergent sequences). Then is linear, is sublinear, and on . Therefore  by Hahn Banach theorem, there exists a functional on such that on and on . As is positive, so is . Moreover, . My question: How to show that ? I observed the following: on and on but don't know if it's useful.","\Phi \ell^{\infty} \Phi(1,1,1,\ldots)=1 \Phi\circ L=\Phi L \ell^{\infty} p: (x_n)\mapsto \limsup x_n \ell^{\infty} f:(x_n)\mapsto \lim x_n c f p f\le p c \Phi \ell^{\infty} \Phi=f c \Phi\le p \ell^{\infty} f \Phi \Phi(1,1,1,\ldots)=f(1,1,1,\ldots)=1 \Phi\circ L=\Phi f\circ L=f=p c p\circ L=p \ell^{\infty}","['functional-analysis', 'operator-theory', 'hahn-banach-theorem']"
91,Relation of Hamel basis with the equation $f(x + y) = f(x) + f(y)$? [duplicate],Relation of Hamel basis with the equation ? [duplicate],f(x + y) = f(x) + f(y),"This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 2 years ago . I am reading ""Linear and Nonlinear Functional Analysis with Applications by Philippe G. Ciarlet "", which explains the origin of hamel basis by a problem: Describe the set $F$ of all functions: $f: \mathbb{R} \rightarrow \mathbb{R}$ that satisfying the functional equation $$ f(x + y) = f(x) + f(y) $$ for all $x,y\in \mathbb{R}$ Actually I don't know how to describe this in a right way, and I always foucs on some concrete quality (for example, $f(0) = 0, f(x) = f(-x)$ ). Therefore, I don't figure out the connection between this and Hamel basis.","This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 2 years ago . I am reading ""Linear and Nonlinear Functional Analysis with Applications by Philippe G. Ciarlet "", which explains the origin of hamel basis by a problem: Describe the set of all functions: that satisfying the functional equation for all Actually I don't know how to describe this in a right way, and I always foucs on some concrete quality (for example, ). Therefore, I don't figure out the connection between this and Hamel basis.","F f: \mathbb{R} \rightarrow \mathbb{R} 
f(x + y) = f(x) + f(y)
 x,y\in \mathbb{R} f(0) = 0, f(x) = f(-x)","['real-analysis', 'functional-analysis', 'analysis', 'functions', 'hamel-basis']"
92,Is Schrodinger operator with potential self adjoint,Is Schrodinger operator with potential self adjoint,,"Consider real-valued one-dimensional Schrodinger operator with potential $V(x)$ , s.t. $L: H^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ $$L(u)=-u''+V(x)u$$ with $V(x)$ bounded Is this operator self-adjoint?","Consider real-valued one-dimensional Schrodinger operator with potential , s.t. with bounded Is this operator self-adjoint?",V(x) L: H^2(\mathbb{R})\rightarrow L^2(\mathbb{R}) L(u)=-u''+V(x)u V(x),"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
93,A problem in open mapping theoram from Kreyszig Section 4.12 Problem 6,A problem in open mapping theoram from Kreyszig Section 4.12 Problem 6,,I am studying Functional analysis from Kreyszig book and can somebody please help with this problem Problem: Let $X$ and $Y$ be Banach Spaces and $T : X \to Y$ be an injective bounded linear operator. Show that $ T^{-1} : \mathscr R(T) \to X$ is bounded iff $\mathscr R(T)$ is closed in $Y$ . Can somebody please give hints on how to proceed through this question.,I am studying Functional analysis from Kreyszig book and can somebody please help with this problem Problem: Let and be Banach Spaces and be an injective bounded linear operator. Show that is bounded iff is closed in . Can somebody please give hints on how to proceed through this question.,X Y T : X \to Y  T^{-1} : \mathscr R(T) \to X \mathscr R(T) Y,[]
94,Another proof of reflexive Hilbert spaces,Another proof of reflexive Hilbert spaces,,"Here is the proof that every Hilbert space is refexive: Let $\varphi\in\mathcal{H^{**}}$ be arbitrary. By Riesz, there is a unique $f_\varphi\in\mathcal{H^*}$ with $\varphi(f)=\langle\,f,f_\varphi\rangle$ for all $f \in\mathcal{H^*} $ . Using the same notation and theorem, we have $\hat{y}_{f_\varphi}(f)= f(y_{f_\varphi})=\langle\,y_{f_\varphi},y_f\rangle=\langle\,f,f_\varphi\rangle=\varphi(f)$ This implies $\hat{y}_{f_\varphi}=\varphi$ , thus $\mathcal{H}$ reflexive. I understood all the steps except for the last implication. Basically, we just showed that $2$ functionals from bi-dual space $\mathcal{H^{**}}$ are the same, why would it imply that $\mathcal{H}$ is reflexive? Any explanation would be highly appreciated!","Here is the proof that every Hilbert space is refexive: Let be arbitrary. By Riesz, there is a unique with for all . Using the same notation and theorem, we have This implies , thus reflexive. I understood all the steps except for the last implication. Basically, we just showed that functionals from bi-dual space are the same, why would it imply that is reflexive? Any explanation would be highly appreciated!","\varphi\in\mathcal{H^{**}} f_\varphi\in\mathcal{H^*} \varphi(f)=\langle\,f,f_\varphi\rangle f \in\mathcal{H^*}  \hat{y}_{f_\varphi}(f)= f(y_{f_\varphi})=\langle\,y_{f_\varphi},y_f\rangle=\langle\,f,f_\varphi\rangle=\varphi(f) \hat{y}_{f_\varphi}=\varphi \mathcal{H} 2 \mathcal{H^{**}} \mathcal{H}","['functional-analysis', 'hilbert-spaces']"
95,Integral of a generalized function,Integral of a generalized function,,"Let $\mathcal{S}$ be a Schwartz space and $\delta_{a}$ the following distribution: $$\delta_{a}: \phi \rightarrow \phi(a) \ \ \ \ \text{ for each } \phi\in\mathcal{S}$$ Now, we routinely see something like: $$\int_{-\infty}^{\infty} f(x)\delta_a dx = f(a)$$ where $f$ is not necessarily $\in \mathcal{S}$ (e.g. $f \in L^2$ ). I'm having a hard time interpreting this integral using the language of distributions. For example, if $f$ was in $\mathcal{S}$ , I could say the integral is just giving me the value of the functional $\delta_a$ at a point in $\mathcal{S}$ . But what about when $f\notin \mathcal{S}$ ?","Let be a Schwartz space and the following distribution: Now, we routinely see something like: where is not necessarily (e.g. ). I'm having a hard time interpreting this integral using the language of distributions. For example, if was in , I could say the integral is just giving me the value of the functional at a point in . But what about when ?",\mathcal{S} \delta_{a} \delta_{a}: \phi \rightarrow \phi(a) \ \ \ \ \text{ for each } \phi\in\mathcal{S} \int_{-\infty}^{\infty} f(x)\delta_a dx = f(a) f \in \mathcal{S} f \in L^2 f \mathcal{S} \delta_a \mathcal{S} f\notin \mathcal{S},"['functional-analysis', 'distribution-theory', 'dirac-delta']"
96,why a subspace is closed?,why a subspace is closed?,,"Let $E$ be $K-$ vector space with a norm $\|\cdot\|$ , and $F$ a subspace with dimension $n$ . Show that $F$ is a closed set . I am trying to show that any convergent sequence of elements $(x_n)_{n \in N}$ of $F$ converge to an element $x \in F$ . This means I need to show that $||x_n-x|| \rightarrow 0$ I see that like a distance which is attended by the fuction $y \rightarrow ||x-y||$ . How can I show that $ x \in F$ ? I have been reading a solution which I don't really understand the intuition behind it : let $x$ be in $E$ and let $B=\{ y \in F ||y-x|| \leq ||x||\}$ after showing that its a compact, they tried to show that $inf_{y \in F}(||x-y||)= \lambda $ exists. by definition of $\lambda$ it is the distance between $x$ and $F$","Let be vector space with a norm , and a subspace with dimension . Show that is a closed set . I am trying to show that any convergent sequence of elements of converge to an element . This means I need to show that I see that like a distance which is attended by the fuction . How can I show that ? I have been reading a solution which I don't really understand the intuition behind it : let be in and let after showing that its a compact, they tried to show that exists. by definition of it is the distance between and",E K- \|\cdot\| F n F (x_n)_{n \in N} F x \in F ||x_n-x|| \rightarrow 0 y \rightarrow ||x-y||  x \in F x E B=\{ y \in F ||y-x|| \leq ||x||\} inf_{y \in F}(||x-y||)= \lambda  \lambda x F,"['functional-analysis', 'normed-spaces', 'topological-vector-spaces']"
97,Invertibility of the square root of an operator,Invertibility of the square root of an operator,,"Let $\mathcal{B}(F)$ the algebra of all bounded linear operators on a complex Hilbert space $F$ . Let $M\in \mathcal{B}(F)^+$ (i.e. $\langle Mx\;, \;x\rangle\geq 0$ for all $x\in F$ ). The square root of $M$ is defined to be the unique operator $N$ such that $$N^2=M.$$ In this case we write $N=M^{1/2}$ If $M$ is an invertible operators, is $M^{1/2}$ invertible?","Let the algebra of all bounded linear operators on a complex Hilbert space . Let (i.e. for all ). The square root of is defined to be the unique operator such that In this case we write If is an invertible operators, is invertible?","\mathcal{B}(F) F M\in \mathcal{B}(F)^+ \langle Mx\;, \;x\rangle\geq 0 x\in F M N N^2=M. N=M^{1/2} M M^{1/2}","['functional-analysis', 'operator-theory']"
98,Schwartz kernel theorem and dual topologies,Schwartz kernel theorem and dual topologies,,"We define the space of linear and continuous operators from $\mathcal{S}(\mathbb{R}^d)$ to $\mathcal{S}'(\mathbb{R}^d)$ as $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$. The Schwartz kernel theorem says that there is an isomorphism between  $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$ and   $\mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$, thanks to the relation that associated to a ""kernel"" $ K \in \mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$ the operator $\mathcal{K}$ defined by  $$\langle \mathcal{K}\{\varphi_1\} , \varphi_2 \rangle = \langle \varphi_1\otimes \varphi_2 , K \rangle $$ for any $\varphi_1, \varphi_2 \in \mathcal{S}(\mathbb{R}^d)$. It is often only implicit that the adequate topology on the dual is the weak*-topology. Is there a good reason to consider this topology for the dual? Is the result false if one endows $\mathcal{S}'(\mathbb{R}^d)$ with the strong topology?","We define the space of linear and continuous operators from $\mathcal{S}(\mathbb{R}^d)$ to $\mathcal{S}'(\mathbb{R}^d)$ as $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$. The Schwartz kernel theorem says that there is an isomorphism between  $\mathcal{L}(\mathcal{S}(\mathbb{R}^d), \mathcal{S}'(\mathbb{R}^d))$ and   $\mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$, thanks to the relation that associated to a ""kernel"" $ K \in \mathcal{S}'(\mathbb{R^d}\times \mathbb{R}^d)$ the operator $\mathcal{K}$ defined by  $$\langle \mathcal{K}\{\varphi_1\} , \varphi_2 \rangle = \langle \varphi_1\otimes \varphi_2 , K \rangle $$ for any $\varphi_1, \varphi_2 \in \mathcal{S}(\mathbb{R}^d)$. It is often only implicit that the adequate topology on the dual is the weak*-topology. Is there a good reason to consider this topology for the dual? Is the result false if one endows $\mathcal{S}'(\mathbb{R}^d)$ with the strong topology?",,"['functional-analysis', 'distribution-theory', 'schwartz-space']"
99,Separable Hilbert space has a countable orthonormal basis,Separable Hilbert space has a countable orthonormal basis,,"I 'm studying about Hilbert Spaces this semester, and the following is a Proposition from yesterday's class which I can't completely understand. ""Obviously,the closed linear span of $V\;$ coincides with $H$."" It doesn't seem so obvious to me. It might be really silly, but how do I know that the closed linear span of a dense subset of $H$ is also dense in $H$? I have the feeling that it's quite elementary but I'm new to Functional Analysis. I would appreciate any help. Thanks in advnace!!","I 'm studying about Hilbert Spaces this semester, and the following is a Proposition from yesterday's class which I can't completely understand. ""Obviously,the closed linear span of $V\;$ coincides with $H$."" It doesn't seem so obvious to me. It might be really silly, but how do I know that the closed linear span of a dense subset of $H$ is also dense in $H$? I have the feeling that it's quite elementary but I'm new to Functional Analysis. I would appreciate any help. Thanks in advnace!!",,"['functional-analysis', 'hilbert-spaces', 'proof-explanation']"
