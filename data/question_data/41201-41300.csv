,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is duality an exact functor on Banach spaces or Hilbert spaces?,Is duality an exact functor on Banach spaces or Hilbert spaces?,,"Let $V,V',V''$ and $W$ be vector spaces over $k$. Then, it is known that $\operatorname{Hom}(\cdot,V)$ is a contravariant exact functor, i.e. for each exact sequence $0\to V'\to V\to V'' \to 0$, and each $W$, the induced sequence $0\to \operatorname{Hom}(V'',W)\to\operatorname{Hom}(V,W)\to\operatorname{Hom}(V',W)\to 0$ is exact. But what if all spaces involved are Banach spaces (over $\mathbb{C}$ or $\mathbb{R}$) and if we replace $\operatorname{Hom}(\cdot,V)$ by $\operatorname{L}(\cdot,V)$, i.e. the continuous linear maps with domain $V$? Is this functor still exact? What if we restict ourselves even stronger to Hilbert spaces? I'm particularly interested in the specialization of $W=\mathbb{R}$ or $W=\mathbb{C}$, where we get the dual spaces (again, only continuous maps considered in the Banach- or Hilbert case). I'm learning for an exam and the books I've been reading say nothing about it... Thank you!","Let $V,V',V''$ and $W$ be vector spaces over $k$. Then, it is known that $\operatorname{Hom}(\cdot,V)$ is a contravariant exact functor, i.e. for each exact sequence $0\to V'\to V\to V'' \to 0$, and each $W$, the induced sequence $0\to \operatorname{Hom}(V'',W)\to\operatorname{Hom}(V,W)\to\operatorname{Hom}(V',W)\to 0$ is exact. But what if all spaces involved are Banach spaces (over $\mathbb{C}$ or $\mathbb{R}$) and if we replace $\operatorname{Hom}(\cdot,V)$ by $\operatorname{L}(\cdot,V)$, i.e. the continuous linear maps with domain $V$? Is this functor still exact? What if we restict ourselves even stronger to Hilbert spaces? I'm particularly interested in the specialization of $W=\mathbb{R}$ or $W=\mathbb{C}$, where we get the dual spaces (again, only continuous maps considered in the Banach- or Hilbert case). I'm learning for an exam and the books I've been reading say nothing about it... Thank you!",,"['abstract-algebra', 'functional-analysis', 'category-theory', 'banach-spaces', 'hilbert-spaces']"
1,Equivalent definition of algebraically closed,Equivalent definition of algebraically closed,,"In Hungerford's Algebra text, it is stated that a field $K$ is algebraically closed iff there exists a subfield $F$ such that $K$ is algebraic over $F$ and all polynomials in $F[x]$ split in $K[x]$.   This seems to be a much weaker condition that $K$ being algebraically closed.  I don't see why it is true (it is not proven in the text) Is there a particular reason why this is true?","In Hungerford's Algebra text, it is stated that a field $K$ is algebraically closed iff there exists a subfield $F$ such that $K$ is algebraic over $F$ and all polynomials in $F[x]$ split in $K[x]$.   This seems to be a much weaker condition that $K$ being algebraically closed.  I don't see why it is true (it is not proven in the text) Is there a particular reason why this is true?",,"['abstract-algebra', 'field-theory', 'extension-field']"
2,How to show the intersection of a prime ideal and a subring is a prime ideal,How to show the intersection of a prime ideal and a subring is a prime ideal,,"I am a self-studier. This is a problem (1.5b) from Andrew Baker's excellent notes (freely available for download) on Galois Theory. $R \subseteq S$ are rings containing $1$. $Q$ is a prime ideal of $S$. Show $Q \cap R$ is a prime ideal of $R$. I have seen this can be proved knowing that the inverse of a ring homomorphism (where $1$ maps to $1$) of a prime ideal is a prime ideal. And considering $\phi: R \rightarrow S$ as an inclusion map and taking the inverse of $Q$, one can get the desired result. I was trying to prove this more ""directly"" and it is here I would appreciate help. The approach I am taking is to show that if $ab \in Q \cap R$, then WLOG $a \in Q \cap R$. Since $Q$ is prime, $a \in Q$. I would appreciate help as to how to show $a \in R$. Is it valid to say that since $1 \in R$ then $a1 \in R$? As always thanks for you help.","I am a self-studier. This is a problem (1.5b) from Andrew Baker's excellent notes (freely available for download) on Galois Theory. $R \subseteq S$ are rings containing $1$. $Q$ is a prime ideal of $S$. Show $Q \cap R$ is a prime ideal of $R$. I have seen this can be proved knowing that the inverse of a ring homomorphism (where $1$ maps to $1$) of a prime ideal is a prime ideal. And considering $\phi: R \rightarrow S$ as an inclusion map and taking the inverse of $Q$, one can get the desired result. I was trying to prove this more ""directly"" and it is here I would appreciate help. The approach I am taking is to show that if $ab \in Q \cap R$, then WLOG $a \in Q \cap R$. Since $Q$ is prime, $a \in Q$. I would appreciate help as to how to show $a \in R$. Is it valid to say that since $1 \in R$ then $a1 \in R$? As always thanks for you help.",,[]
3,Additive functor over a short split exact sequence.,Additive functor over a short split exact sequence.,,"$0\to A'\stackrel{f}{\longrightarrow} A \stackrel{g}{\longrightarrow} A''\to 0$ is a short split exact sequence, where $A'$, $A$, $A''$ are $R$-modules, and $T$ is an additive functor from $R$-$\mathsf{mod}$ to $\mathsf{Ab}$. Then we have the sequence $0\to TA'\stackrel{T(f)}{\longrightarrow} TA \stackrel{T(g)}{\longrightarrow} TA''\to 0$ is a split exact sequence again. I know that the second sequence is split. That $T(f)$ is injective and $T(g)$ is surjective is also clear for me too. My question is why $\ker T(g)=\mathrm{im}T(f)$. 1. Some comments I found from the Internet told that an additive functor preserves binary direct sum.Does it help here? I know very few about module theory.I don't know whether this is  the reason why I get stuck here. 2.I have checked from this math.SE post . But the answer there seems to be not suitable for my question. Thanks in advance.","$0\to A'\stackrel{f}{\longrightarrow} A \stackrel{g}{\longrightarrow} A''\to 0$ is a short split exact sequence, where $A'$, $A$, $A''$ are $R$-modules, and $T$ is an additive functor from $R$-$\mathsf{mod}$ to $\mathsf{Ab}$. Then we have the sequence $0\to TA'\stackrel{T(f)}{\longrightarrow} TA \stackrel{T(g)}{\longrightarrow} TA''\to 0$ is a split exact sequence again. I know that the second sequence is split. That $T(f)$ is injective and $T(g)$ is surjective is also clear for me too. My question is why $\ker T(g)=\mathrm{im}T(f)$. 1. Some comments I found from the Internet told that an additive functor preserves binary direct sum.Does it help here? I know very few about module theory.I don't know whether this is  the reason why I get stuck here. 2.I have checked from this math.SE post . But the answer there seems to be not suitable for my question. Thanks in advance.",,"['abstract-algebra', 'category-theory', 'modules', 'homological-algebra']"
4,A property of finite field of order $2^n$,A property of finite field of order,2^n,"Suppose $a$ and $b$ are elements of a finite field of order $2^n$ with $n$ odd and $a^2+ab+b^2=0$. Is it necessary that both $a$ and $b$ must be zero ? I understand that the field has characteristic $2$ but don't know how to use the fact that $n$ is odd, please help.","Suppose $a$ and $b$ are elements of a finite field of order $2^n$ with $n$ odd and $a^2+ab+b^2=0$. Is it necessary that both $a$ and $b$ must be zero ? I understand that the field has characteristic $2$ but don't know how to use the fact that $n$ is odd, please help.",,"['abstract-algebra', 'finite-fields']"
5,Local-global principle for Galois groups,Local-global principle for Galois groups,,"Is there any sort of local-global principle for Galois groups? For example, given a polynomial $f \in \mathbb{Q}[X]$ and assuming we know $\mathrm{Gal}(f/\mathbb{Q}_p)$ at all primes $p \le \infty$, what can we say about $\mathrm{Gal}(f/\mathbb{Q})$?","Is there any sort of local-global principle for Galois groups? For example, given a polynomial $f \in \mathbb{Q}[X]$ and assuming we know $\mathrm{Gal}(f/\mathbb{Q}_p)$ at all primes $p \le \infty$, what can we say about $\mathrm{Gal}(f/\mathbb{Q})$?",,"['abstract-algebra', 'galois-theory']"
6,Inverses in the ring of formal power series over a field.,Inverses in the ring of formal power series over a field.,,"Let $\mathbb{F}$ be a field, and consider $\mathbb{F}[[x]]$, the ring of formal power series with coefficients in $\mathbb{F}$, i.e. the set of expressions of the form $$\sum_{n=0}^{\infty}a_n x^n,\quad a_n\in\mathbb{F}$$ with the usual rules for addition and multiplication. How to show that any element that does not belong to the principal ideal $\langle x\rangle$ is invertible?","Let $\mathbb{F}$ be a field, and consider $\mathbb{F}[[x]]$, the ring of formal power series with coefficients in $\mathbb{F}$, i.e. the set of expressions of the form $$\sum_{n=0}^{\infty}a_n x^n,\quad a_n\in\mathbb{F}$$ with the usual rules for addition and multiplication. How to show that any element that does not belong to the principal ideal $\langle x\rangle$ is invertible?",,"['abstract-algebra', 'ring-theory', 'power-series']"
7,Noetherian module implies finite direct sum of indecomposables?,Noetherian module implies finite direct sum of indecomposables?,,"Noetherian module implies finite direct sum of indecomposables? Let R be a ring and let M be a Noetherian R-module. If M is indecomposable we are done. Otherwise, M is a direct sum of two proper and non-trivial submodules. If M were also Artinian, I could use induction on the (finite) length of M and prove the result in the title. I don't know how to proceed in the general case. Thanks in advance for any ideas!","Noetherian module implies finite direct sum of indecomposables? Let R be a ring and let M be a Noetherian R-module. If M is indecomposable we are done. Otherwise, M is a direct sum of two proper and non-trivial submodules. If M were also Artinian, I could use induction on the (finite) length of M and prove the result in the title. I don't know how to proceed in the general case. Thanks in advance for any ideas!",,"['abstract-algebra', 'modules']"
8,Is $(XY - 1)$ a maximal ideal in $k[[X]][Y]$?,Is  a maximal ideal in ?,(XY - 1) k[[X]][Y],"Is $(XY - 1)$ a maximal ideal in $k[[X]][Y]$, and if so, how can I see it? It is at least prime because the generator is irreducible, and by the same argument it is maximal among all principal ideals. But I haven't gotten further than that - Finding units in the quotient ring didn't turn out well, either.","Is $(XY - 1)$ a maximal ideal in $k[[X]][Y]$, and if so, how can I see it? It is at least prime because the generator is irreducible, and by the same argument it is maximal among all principal ideals. But I haven't gotten further than that - Finding units in the quotient ring didn't turn out well, either.",,"['abstract-algebra', 'commutative-algebra', 'ideals', 'maximal-and-prime-ideals']"
9,Can non-unital rings be replaced by R-algebras?,Can non-unital rings be replaced by R-algebras?,,"While working through some lecture notes on semigroups, it seemed to me like a semigroup doesn't buy you much generality over a monoid. But I wondered whether the situation is different for non-unital versus (unital) rings. Then I worked through the following reasons against non-unital rings , which made the point that all naturally occurring non-unital (sub)rings actually have important additional structures, giving ideals and R-algebras as examples. But the author of that paper later later agreed to most of the reasons for non-unital rings . Let $A$ be a ring (possibly non-unital) and $\tilde{A}=\mathbb Z\oplus A$ as abelian group. I wondered what Martin Brandenburg meant by the ""obvious"" multiplication so that $A\subseteq \tilde{A}$ is an ideal and $1\in\mathbb Z$ is the identity. After some trying, I came up with $$ (r,a)\cdot(s,b)=(rs,rb+sa+ab)$$ I haven't checked associativity, but at least the above two conditions are satisfied. Now I wonder what would happen if I replace $\mathbb Z$ by an arbitrary commutative ring $R$ with identity $1$, and assume that I'm also given a scalar multiplication $R \times A \mapsto A$ denoted by $(r, a) \mapsto ra$. Would the above multiplication turn $R\oplus A$ into an (unital) ring, if $A$ is an $R$-algebra? And would conversely $A$ be an $R$-algebra, whenever $R\oplus A$ happens to be an (unital) ring under the above multiplication?","While working through some lecture notes on semigroups, it seemed to me like a semigroup doesn't buy you much generality over a monoid. But I wondered whether the situation is different for non-unital versus (unital) rings. Then I worked through the following reasons against non-unital rings , which made the point that all naturally occurring non-unital (sub)rings actually have important additional structures, giving ideals and R-algebras as examples. But the author of that paper later later agreed to most of the reasons for non-unital rings . Let $A$ be a ring (possibly non-unital) and $\tilde{A}=\mathbb Z\oplus A$ as abelian group. I wondered what Martin Brandenburg meant by the ""obvious"" multiplication so that $A\subseteq \tilde{A}$ is an ideal and $1\in\mathbb Z$ is the identity. After some trying, I came up with $$ (r,a)\cdot(s,b)=(rs,rb+sa+ab)$$ I haven't checked associativity, but at least the above two conditions are satisfied. Now I wonder what would happen if I replace $\mathbb Z$ by an arbitrary commutative ring $R$ with identity $1$, and assume that I'm also given a scalar multiplication $R \times A \mapsto A$ denoted by $(r, a) \mapsto ra$. Would the above multiplication turn $R\oplus A$ into an (unital) ring, if $A$ is an $R$-algebra? And would conversely $A$ be an $R$-algebra, whenever $R\oplus A$ happens to be an (unital) ring under the above multiplication?",,"['abstract-algebra', 'ring-theory']"
10,Proof of Yoneda Lemma [closed],Proof of Yoneda Lemma [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Can anyone explain to me Yoneda Lemma proof in great details? i.e. they usually say "" ... it is easy to see that these morphisms are inverse to each other.."" without explanation.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Can anyone explain to me Yoneda Lemma proof in great details? i.e. they usually say "" ... it is easy to see that these morphisms are inverse to each other.."" without explanation.",,['abstract-algebra']
11,Left exactness of inverse limit,Left exactness of inverse limit,,Is the left exactness of inverse limit (in the category of modules over a ring) a general property regardless of the indexing set?  (Let's assume it is still directed.) The only proof I can find requires integers as the indexing set.,Is the left exactness of inverse limit (in the category of modules over a ring) a general property regardless of the indexing set?  (Let's assume it is still directed.) The only proof I can find requires integers as the indexing set.,,"['abstract-algebra', 'commutative-algebra']"
12,Whence this generalization of linear (in)dependence?,Whence this generalization of linear (in)dependence?,,"I recently came across a definition of (in)dependence that is supposed to be a generalization of linear (in)dependence among a set of vectors: An element $x$ is dependent on a set of elements $\{a_1, \cdots, a_n\}$ iff any two real-valued additive functions that take equal values on each of the elements $a_1, \cdots, a_n$ also agree on $x$.  A set of elements $\{a_1,\cdots,a_n\}$ is independent iff no member of the set is dependent on the rest. The term ""additive function"" used in this definition describes a (real-valued) function $f$ such that $f(a + b) = f(a) + f(b)\;\; \forall a, b \in \mathrm{dom}(f)$.  Of course, this presupposes that some form of addition is defined in the domain of such $f$.  It is safe to assume that this domain is an Abelian group, but you may assume additional structure if this is necessary to answer my question. I find this definition of dependence much harder to think about and work with than the standard definition of linear dependence for vectors, so I wonder what's the justification for such a counterintuitive definition. Is anyone familiar with this way of defining dependence/independence?  If so, what algebraic structure is it used for?","I recently came across a definition of (in)dependence that is supposed to be a generalization of linear (in)dependence among a set of vectors: An element $x$ is dependent on a set of elements $\{a_1, \cdots, a_n\}$ iff any two real-valued additive functions that take equal values on each of the elements $a_1, \cdots, a_n$ also agree on $x$.  A set of elements $\{a_1,\cdots,a_n\}$ is independent iff no member of the set is dependent on the rest. The term ""additive function"" used in this definition describes a (real-valued) function $f$ such that $f(a + b) = f(a) + f(b)\;\; \forall a, b \in \mathrm{dom}(f)$.  Of course, this presupposes that some form of addition is defined in the domain of such $f$.  It is safe to assume that this domain is an Abelian group, but you may assume additional structure if this is necessary to answer my question. I find this definition of dependence much harder to think about and work with than the standard definition of linear dependence for vectors, so I wonder what's the justification for such a counterintuitive definition. Is anyone familiar with this way of defining dependence/independence?  If so, what algebraic structure is it used for?",,"['abstract-algebra', 'modules', 'abelian-groups']"
13,Finite rings of prime order must have a multiplicative identity,Finite rings of prime order must have a multiplicative identity,,"The standard definition of a ring is an abelian group that is a monoid under multiplication (with distributivity). However, there are some books that have a weaker definition implying that a ring only has to be closed under multiplication (no identity). There is a problem in my algebra book asking me to prove that if a ring (defined in the second way) has $p$ elements, where $p$ is prime, and the multiplication is not trivial (i.e. sending everything to $0$ ), then the ring is forced to have a multiplicative identity. Its seems like a trivial proof, but I just can't see what I'm missing. What I have so far: Given $R$ is a ring with $p$ elements and $R$ is an abelian group of prime order, therefore it is cyclically generated, and of characteristic $p$ and isomorphic to $\mathbb{Z}/p\mathbb{Z}$ . Essentially it boils down to showing $\mathbb{Z}/p\mathbb{Z}$ is forced to have a multiplicative identity, but I just can't see where this comes from (every resource I found seems to take this as a fact). Since this is a requirement regardless of multiplicative structure, I can't just use the fact that $\mathbb{Z}/p\mathbb{Z} - \{ 0 \}$ is a group under the typical multiplication.","The standard definition of a ring is an abelian group that is a monoid under multiplication (with distributivity). However, there are some books that have a weaker definition implying that a ring only has to be closed under multiplication (no identity). There is a problem in my algebra book asking me to prove that if a ring (defined in the second way) has elements, where is prime, and the multiplication is not trivial (i.e. sending everything to ), then the ring is forced to have a multiplicative identity. Its seems like a trivial proof, but I just can't see what I'm missing. What I have so far: Given is a ring with elements and is an abelian group of prime order, therefore it is cyclically generated, and of characteristic and isomorphic to . Essentially it boils down to showing is forced to have a multiplicative identity, but I just can't see where this comes from (every resource I found seems to take this as a fact). Since this is a requirement regardless of multiplicative structure, I can't just use the fact that is a group under the typical multiplication.",p p 0 R p R p \mathbb{Z}/p\mathbb{Z} \mathbb{Z}/p\mathbb{Z} \mathbb{Z}/p\mathbb{Z} - \{ 0 \},"['abstract-algebra', 'ring-theory', 'rngs']"
14,Are groups always isomorphic to their image under power maps?,Are groups always isomorphic to their image under power maps?,,"Let $(G, \cdot)$ be a finite group of order $n$ . Consider the map: $$G \to G, g \mapsto g^k$$ for $k$ , $n$ coprime. This is injective, but generally not a homomorphism. Define a new group $G_k = (G,\circ)$ by: $$g \circ h = (g^k h^k)^{\frac{1}{k}}$$ where naturally $g \mapsto g^{\frac{1}{k}}$ is the (well-defined) inverse of $g \mapsto g^k$ . It is easily verified that this gives a valid group operation. Question : Is it true that $G \cong G_k$ for all $k$ coprime to $n$ ? The map is clearly an isomorphism if $G$ is abelian (or more generally, $k$ -abelian). Also, note that the order of any $g \in G_k$ is the same as the order of $g \in G$ . Thus, $G, G_k$ have the same order sequence, so according to this answer , any counterexample must have $n \ge 16$ , and I am not very familiar with these groups. (But I do not expect the statement to be true.) I also noticed that if $G \sim H$ when $H \cong G_k$ for some $k$ (coprime to $|G|$ ), then $\sim$ is an equivalence relation.","Let be a finite group of order . Consider the map: for , coprime. This is injective, but generally not a homomorphism. Define a new group by: where naturally is the (well-defined) inverse of . It is easily verified that this gives a valid group operation. Question : Is it true that for all coprime to ? The map is clearly an isomorphism if is abelian (or more generally, -abelian). Also, note that the order of any is the same as the order of . Thus, have the same order sequence, so according to this answer , any counterexample must have , and I am not very familiar with these groups. (But I do not expect the statement to be true.) I also noticed that if when for some (coprime to ), then is an equivalence relation.","(G, \cdot) n G \to G, g \mapsto g^k k n G_k = (G,\circ) g \circ h = (g^k h^k)^{\frac{1}{k}} g \mapsto g^{\frac{1}{k}} g \mapsto g^k G \cong G_k k n G k g \in G_k g \in G G, G_k n \ge 16 G \sim H H \cong G_k k |G| \sim","['abstract-algebra', 'group-theory', 'finite-groups']"
15,Which pairs of groups are quotients of some group by isomorphic subgroups?,Which pairs of groups are quotients of some group by isomorphic subgroups?,,"We know that there exists a group $G$ with normal subgroups $N_1,N_2$ where $N_1\cong N_2$ but ${G\over N_1}\not\cong {G\over N_2}$ . Since this is the case it is natural to wonder about which pairs of groups can each be represented by quotients of some group by isomorphic subgroups. In particular, for which pairs of groups $H_1,H_2$ do there exist a group $G$ and normal subgroups $N_1,N_2$ of $G$ such that $N_1\cong N_2$ , $H_1\cong {G\over N_1}$ , and $H_2\cong {G\over N_2}$ ? Are there any nice conditions on such pairs guaranteeing the existence of these quotients? Or is it perhaps even possible to find such a $G,N_1,N_2$ whenever $H_1$ and $H_2$ are of the same order?","We know that there exists a group with normal subgroups where but . Since this is the case it is natural to wonder about which pairs of groups can each be represented by quotients of some group by isomorphic subgroups. In particular, for which pairs of groups do there exist a group and normal subgroups of such that , , and ? Are there any nice conditions on such pairs guaranteeing the existence of these quotients? Or is it perhaps even possible to find such a whenever and are of the same order?","G N_1,N_2 N_1\cong N_2 {G\over N_1}\not\cong {G\over N_2} H_1,H_2 G N_1,N_2 G N_1\cong N_2 H_1\cong {G\over N_1} H_2\cong {G\over N_2} G,N_1,N_2 H_1 H_2","['abstract-algebra', 'group-theory', 'finite-groups']"
16,What are the conditions for a system of equations to have a solution in some abelian group extension?,What are the conditions for a system of equations to have a solution in some abelian group extension?,,"Given an abelian group $G$ and a system of equations $$a_1 = n^1_1 y_1 + ... + n^1_r y_r$$ $$...$$ $$a_s = n^s_1 y_1 + ... + n^s_r y_r$$ for $a_i \in G$ and $n^i_j \in \mathbb{Z}_{\geq 0}$ , under what conditions does there exist a group containing $G$ as a subgroup in which this system is solvable? In particular, can the conditions be stated in the first-order logic of group theory? For example, a system $$a_1 = y_1$$ $$a_2 = y_1$$ is solvable in an extension if and only if $a_1 = a_2$ , so its conditions can be stated in first-order logic. Furthermore, any single equation $a = n_1 y_1 + ... + n_r y_r$ with at least one $n_i \neq 0$ can be solved in some extension of any group, since in $H = (G \times \mathbb{Z}) / \langle (a, -n_i)\rangle $ we have $y_i = (0, 1)$ as a solution, with $G$ embedding into it by $\alpha: G \rightarrow H: g \mapsto (g, 0)$ , as described by this answer","Given an abelian group and a system of equations for and , under what conditions does there exist a group containing as a subgroup in which this system is solvable? In particular, can the conditions be stated in the first-order logic of group theory? For example, a system is solvable in an extension if and only if , so its conditions can be stated in first-order logic. Furthermore, any single equation with at least one can be solved in some extension of any group, since in we have as a solution, with embedding into it by , as described by this answer","G a_1 = n^1_1 y_1 + ... + n^1_r y_r ... a_s = n^s_1 y_1 + ... + n^s_r y_r a_i \in G n^i_j \in \mathbb{Z}_{\geq 0} G a_1 = y_1 a_2 = y_1 a_1 = a_2 a = n_1 y_1 + ... + n_r y_r n_i \neq 0 H = (G \times \mathbb{Z}) / \langle (a, -n_i)\rangle  y_i = (0, 1) G \alpha: G \rightarrow H: g \mapsto (g, 0)","['abstract-algebra', 'systems-of-equations', 'first-order-logic', 'model-theory', 'abelian-groups']"
17,Why is it so computationally hard to determine group isomorphism?,Why is it so computationally hard to determine group isomorphism?,,"Finding an isomorphism requires to show that for 2 groups $G$ and $H$ , there exists a bijective map $\phi : G\to H$ such that $$\phi(ab)=\phi(a)\phi(b)$$ For all $a,b \in G$ . This is (probably naively) pretty straight forward, and there are plenty of theorems that allow us to show that groups of specific orders must be isomorphic to some specific set of groups. So, my question (which I hope isn’t too loaded) is What intrinsically makes finding if 2 groups are isomorphic so hard? Is it showing that they are bijective, is it showing that they are operation preserving, or is it something entirely different?","Finding an isomorphism requires to show that for 2 groups and , there exists a bijective map such that For all . This is (probably naively) pretty straight forward, and there are plenty of theorems that allow us to show that groups of specific orders must be isomorphic to some specific set of groups. So, my question (which I hope isn’t too loaded) is What intrinsically makes finding if 2 groups are isomorphic so hard? Is it showing that they are bijective, is it showing that they are operation preserving, or is it something entirely different?","G H \phi : G\to H \phi(ab)=\phi(a)\phi(b) a,b \in G","['abstract-algebra', 'group-theory', 'computational-complexity', 'group-isomorphism', 'computational-algebra']"
18,Does every group have an object of symmetry?,Does every group have an object of symmetry?,,"I'm aware of Cayley's theorem, which says that every group is isomorphic to some subgroup of a symmetric group. But it's not clear to me whether symmetric groups themselves (apart from their name) capture the notion of geometric symmetry that ""objects of symmetry"" have (and by geometric symmetry I mean the type of symmetry expressed when we talk about the rotations and flips of a square ( $D_4$ ), or the symmetries of a cube ( $S_4$ )) Some stackexchange posts answer the question, but I can't tell if the first one is talking about symmetry (as in symmetric group) or symmetry (as in symmetry of a square), and the second answer is a bit too technical for me.... Furthermore, group explorer doesn't have objects of Symmetry for $Q_4$ and $Z_2 \times Z_4$ . Is that for lack of imagination, an incomplete database, or because there is no object of symmetry for these groups (and the many others there)? Thanks","I'm aware of Cayley's theorem, which says that every group is isomorphic to some subgroup of a symmetric group. But it's not clear to me whether symmetric groups themselves (apart from their name) capture the notion of geometric symmetry that ""objects of symmetry"" have (and by geometric symmetry I mean the type of symmetry expressed when we talk about the rotations and flips of a square ( ), or the symmetries of a cube ( )) Some stackexchange posts answer the question, but I can't tell if the first one is talking about symmetry (as in symmetric group) or symmetry (as in symmetry of a square), and the second answer is a bit too technical for me.... Furthermore, group explorer doesn't have objects of Symmetry for and . Is that for lack of imagination, an incomplete database, or because there is no object of symmetry for these groups (and the many others there)? Thanks",D_4 S_4 Q_4 Z_2 \times Z_4,"['abstract-algebra', 'group-theory', 'geometry', 'symmetry']"
19,What are some techniques for embedding a finite group into $S_m$ for $m$ as small as possible?,What are some techniques for embedding a finite group into  for  as small as possible?,S_m m,"I know that if $|G| = n$ , then $G$ can be embedded into $S_n$ . But the group $S_n$ is very large compared to $G$ , so I was wondering if there are general ways of embedding $G$ into a smaller symmetric group. (By general I don't mean that it has to work for all groups, but hopefully for large classes of groups) Also, I was wondering embedding groups into smaller symmetric groups is common/useful, or just a curiosity? The only approach I could think of is to let $G$ act on various things, and hope that the action is faithful. One nice thing, for example, is that if $G$ has a simple subgroup $H$ (which is itself non-normal in $G$ ), then the action of $G$ on the left cosets of $H$ is faithful, since according to this question, the kernel must be trivial. Furthermore, if you let $G$ act by conjugation on a subgroup, I believe the action is sometimes faithful, sometimes not (has to do with weather the conjugates are disjoint).","I know that if , then can be embedded into . But the group is very large compared to , so I was wondering if there are general ways of embedding into a smaller symmetric group. (By general I don't mean that it has to work for all groups, but hopefully for large classes of groups) Also, I was wondering embedding groups into smaller symmetric groups is common/useful, or just a curiosity? The only approach I could think of is to let act on various things, and hope that the action is faithful. One nice thing, for example, is that if has a simple subgroup (which is itself non-normal in ), then the action of on the left cosets of is faithful, since according to this question, the kernel must be trivial. Furthermore, if you let act by conjugation on a subgroup, I believe the action is sometimes faithful, sometimes not (has to do with weather the conjugates are disjoint).",|G| = n G S_n S_n G G G G H G G H G,"['abstract-algebra', 'group-theory', 'permutations']"
20,$X^{2n} - 2X^n -7$ irreducible over $\mathbb{Q}$?,irreducible over ?,X^{2n} - 2X^n -7 \mathbb{Q},I read something somewhere that would imply $X^{2n} - 2X^n -7$ is irreducible over $\mathbb{Q}$ . Is there an easy way to prove this?,I read something somewhere that would imply is irreducible over . Is there an easy way to prove this?,X^{2n} - 2X^n -7 \mathbb{Q},"['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
21,"Is this a group? If so, what group is it?","Is this a group? If so, what group is it?",,"I have the following group (at least, I think it's a group) generated by $\langle a,b,c \rangle$ where the operation $\cdot$ obeys the following rules: $a^2=b^2=c^2=1$ (where $1$ is the identity). $\cdot$ is associative. $(cb)(bc) = (bc)(cb) = 1$ , $(bc)^3 = (cb)^3 = 1$ , $(bc)^2 = cb$ , $(bcb)^2 = 1$ , $cbc = bcb$ , $\forall x, \space xa = ax$ . Some of these might be redundant, which is fine, but it's obviously an issue if there's a contradiction, but I can't find one. From these rules, I believe this is a group of order 12 with the elements $\{1,a,b,c,ab,ac,bc,cb,abc,acb,bcb,abcb\}$ . However, I've been looking at the groups of order 12, and this one doesn't seem to be isomorphic to any of them. It's not Abelian, which narrows it down to the Alternating Group, the Dihedral Group, and the Dicyclic Group. It's not the Alternating Group, as that only has 3 elements that square to $1$ , whereas my group has 8 that do ( $\{1,a,b,c,ab,ac,bcb,abcb\}$ ). The Dihedral Group has the right amount of elements that square to $1$ , but has an order 6 element, which my group doesn't have. I hadn't heard of the Dicyclic Group until today, and I've been having trouble finding information on it, but it also seems like it has an order 6 element. So what am I doing wrong here? So did I miss something here and it is actually isomorphic to one of these groups? Is my group ill-defined to begin with? Is it well-defined but not a group? (I'm almost sure this isn't it, because I forced the operation to be associative, and I have an identity, and everything seems to have an inverse.) Did I miscalculate the number of elements? Or some other mistake entirely?","I have the following group (at least, I think it's a group) generated by where the operation obeys the following rules: (where is the identity). is associative. , , , , , . Some of these might be redundant, which is fine, but it's obviously an issue if there's a contradiction, but I can't find one. From these rules, I believe this is a group of order 12 with the elements . However, I've been looking at the groups of order 12, and this one doesn't seem to be isomorphic to any of them. It's not Abelian, which narrows it down to the Alternating Group, the Dihedral Group, and the Dicyclic Group. It's not the Alternating Group, as that only has 3 elements that square to , whereas my group has 8 that do ( ). The Dihedral Group has the right amount of elements that square to , but has an order 6 element, which my group doesn't have. I hadn't heard of the Dicyclic Group until today, and I've been having trouble finding information on it, but it also seems like it has an order 6 element. So what am I doing wrong here? So did I miss something here and it is actually isomorphic to one of these groups? Is my group ill-defined to begin with? Is it well-defined but not a group? (I'm almost sure this isn't it, because I forced the operation to be associative, and I have an identity, and everything seems to have an inverse.) Did I miscalculate the number of elements? Or some other mistake entirely?","\langle a,b,c \rangle \cdot a^2=b^2=c^2=1 1 \cdot (cb)(bc) = (bc)(cb) = 1 (bc)^3 = (cb)^3 = 1 (bc)^2 = cb (bcb)^2 = 1 cbc = bcb \forall x, \space xa = ax \{1,a,b,c,ab,ac,bc,cb,abc,acb,bcb,abcb\} 1 \{1,a,b,c,ab,ac,bcb,abcb\} 1","['abstract-algebra', 'group-theory', 'finite-groups', 'group-presentation', 'combinatorial-group-theory']"
22,$\operatorname{Aut} (G)$ is isomorphic to $\operatorname{Aut} (H)$ then is it necessary that $G$ is isomorphic to $H$?,is isomorphic to  then is it necessary that  is isomorphic to ?,\operatorname{Aut} (G) \operatorname{Aut} (H) G H,"If $\operatorname{Aut} (G)$ is isomorphic to $\operatorname{Aut} (H)$ then is it necessary that $G$ is isomorphic to $H$ ? My answer is no. $\operatorname{Aut} (\mathbb{Z)}$ is isomorphic to $Z_2$ and $\operatorname{Aut} (Z_3)$ is also isomorphic to $U(3)$ , which is isomorphic to $Z_2$ . But $\mathbb Z$ is not isomorphic to $Z_3.$ Correct? Thanks","If is isomorphic to then is it necessary that is isomorphic to ? My answer is no. is isomorphic to and is also isomorphic to , which is isomorphic to . But is not isomorphic to Correct? Thanks",\operatorname{Aut} (G) \operatorname{Aut} (H) G H \operatorname{Aut} (\mathbb{Z)} Z_2 \operatorname{Aut} (Z_3) U(3) Z_2 \mathbb Z Z_3.,"['abstract-algebra', 'group-theory']"
23,"If the square of every element of a ring is in the center, must the ring be commutative?","If the square of every element of a ring is in the center, must the ring be commutative?",,"Let $R$ be a ring with identity such that the square of any element belongs to the center of $R$ . Is it necessary true that $R$ is commutative? (I can show that for any $x,y\in R$ , $2(xy-yx) =0 $ but I cannot prove commutativity of $R$ .)","Let be a ring with identity such that the square of any element belongs to the center of . Is it necessary true that is commutative? (I can show that for any , but I cannot prove commutativity of .)","R R R x,y\in R 2(xy-yx) =0  R","['abstract-algebra', 'ring-theory']"
24,"Why is the dimension of a ring, with finite spectrum, $\leq 1$?","Why is the dimension of a ring, with finite spectrum, ?",\leq 1,"Why is the dimension of a ring, with finite spectrum, $\leq 1$? It is clear that it works for the case if the ring is noetherian, due to the fact that every ideal of height $2$ is an infinite union of prime ideals of height $1$. It seems like one could use the prime avoidance lemma, but I am really unsure how to work it out.","Why is the dimension of a ring, with finite spectrum, $\leq 1$? It is clear that it works for the case if the ring is noetherian, due to the fact that every ideal of height $2$ is an infinite union of prime ideals of height $1$. It seems like one could use the prime avoidance lemma, but I am really unsure how to work it out.",,"['abstract-algebra', 'commutative-algebra']"
25,UFD iff PID in Dedekind domain.,UFD iff PID in Dedekind domain.,,Let $A$ be a Dedekind domain. PID implies UFD. So for the other direction assume $A$ is an UFD. In this proof the author only considers prime ideals instead of any proper ideal. Why is this sufficient?,Let $A$ be a Dedekind domain. PID implies UFD. So for the other direction assume $A$ is an UFD. In this proof the author only considers prime ideals instead of any proper ideal. Why is this sufficient?,,"['abstract-algebra', 'algebraic-number-theory']"
26,"Let $f(x)$ be an irreducible polynomial over $F$ of degree $n$, and let $K$ be a field extension of $F$ with $[K:F]=m$","Let  be an irreducible polynomial over  of degree , and let  be a field extension of  with",f(x) F n K F [K:F]=m,"Let $f(x)$ be an irreducible polynomial over $F$ of degree $n$, and let $K$ be a field extension of $F$ with $[K:F]=m$. If $\gcd(n,m)=1$, show that $f$ is irreducible over $K$. I thought suppose $f$ reducible over $K$, so $$f(x) = \underbrace{p(x)}_{(\deg = r)}\underbrace{q(x)}_{(\deg = s)}$$ where $1<r,s<n$ and both divides $n$. I imagine that $r$ or $s$ divides $m$ too, but I don't know how to prove this. Any hint? Or is there a better way?","Let $f(x)$ be an irreducible polynomial over $F$ of degree $n$, and let $K$ be a field extension of $F$ with $[K:F]=m$. If $\gcd(n,m)=1$, show that $f$ is irreducible over $K$. I thought suppose $f$ reducible over $K$, so $$f(x) = \underbrace{p(x)}_{(\deg = r)}\underbrace{q(x)}_{(\deg = s)}$$ where $1<r,s<n$ and both divides $n$. I imagine that $r$ or $s$ divides $m$ too, but I don't know how to prove this. Any hint? Or is there a better way?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
27,Uniqueness of Left Adjoint,Uniqueness of Left Adjoint,,"On nLab, it says that the left adjoint of a functor is unique, but it does not give a proof. Most of the proofs I have seen use the Yoneda lemma, but the book I am using states this fact (without proof) before stating the Yoneda lemma. How is this fact proven?","On nLab, it says that the left adjoint of a functor is unique, but it does not give a proof. Most of the proofs I have seen use the Yoneda lemma, but the book I am using states this fact (without proof) before stating the Yoneda lemma. How is this fact proven?",,"['abstract-algebra', 'category-theory', 'adjoint-functors']"
28,Motivation for learning about fundamental groups and covering spaces,Motivation for learning about fundamental groups and covering spaces,,"I am about to give a seminar presentation in an algebra seminar (undergraduate level) about the Hopf fibration $SU(2) \longrightarrow SO(3)$ for which I will use an algebraic-topological approach using the more topological representations $SU(2) \cong S^3$ as well as $SO(3) \cong \mathbb{R}P^3$. For this, I want to introduce the concepts of universal coverings and fundamental groups. My question now is, how can I give some motivation for math-undergraduates (especially those possible 'only' interested in Algebra) to learn about fundamental groups/universal coverings? For me, it's just an interesting concept, but that's not very convincing, I suppose... Hence: What are good reasons to learn about fundamental groups (and universal coverings?) when you're an undergraduate, (possibly mainly) interested in Algebra and not necessarily interested in algebraic topology?","I am about to give a seminar presentation in an algebra seminar (undergraduate level) about the Hopf fibration $SU(2) \longrightarrow SO(3)$ for which I will use an algebraic-topological approach using the more topological representations $SU(2) \cong S^3$ as well as $SO(3) \cong \mathbb{R}P^3$. For this, I want to introduce the concepts of universal coverings and fundamental groups. My question now is, how can I give some motivation for math-undergraduates (especially those possible 'only' interested in Algebra) to learn about fundamental groups/universal coverings? For me, it's just an interesting concept, but that's not very convincing, I suppose... Hence: What are good reasons to learn about fundamental groups (and universal coverings?) when you're an undergraduate, (possibly mainly) interested in Algebra and not necessarily interested in algebraic topology?",,"['abstract-algebra', 'algebraic-topology', 'fundamental-groups', 'advice']"
29,A topology on algebras that coincides with a certain idea of density,A topology on algebras that coincides with a certain idea of density,,"In the category of Hausdorff spaces and continuous maps, the inclusion $\mathbb{Q} \to \mathbb{R}$ is epic, because $\mathbb{Q}$ is dense in $\mathbb{R}$ and I think in a way the notion of epic arrow is a good translation of what density means. For instance in the category of rings and ring morphisms, the inclusion $\mathbb{Z}\to\mathbb{Q}$ is again epic, and this shows that in a sense $\mathbb{Z}$ is dense in $\mathbb{Q}$: $\mathbb{Q}$ is (as a ring) ""entirely determined"" by $\mathbb{Z}$. Obviously this density isn't the one in the sense of the usual topology on these rings. However I was wondering if this can be captured in a certain topology on $\mathbb{Q}$, and more generally I have the following question : Given a category of structured sets (for instance a category of algebras in the sense of universal algebra- in particular varieties for instance) and structure-preserving maps, can the structured sets always be furnished with a topology that makes epimorphisms in said category the morphisms whose image is dense (wrt to said topology) ? If you have an answer in a specific case, I'll take it as well, I don't necessarily need an answer as general as the question. For instance if you have an answer for the category of rings, or for varieties, or for algebras, etc. I'll be glad to hear it. EDIT : As has been noted in a comment, the trivial topology would work. As this obviously isn't satisfactory, I'll think about other things that such a topology would have to verify. But still, you can try and guess what I would expect from it, until I come up with natural things to prevent trivial examples","In the category of Hausdorff spaces and continuous maps, the inclusion $\mathbb{Q} \to \mathbb{R}$ is epic, because $\mathbb{Q}$ is dense in $\mathbb{R}$ and I think in a way the notion of epic arrow is a good translation of what density means. For instance in the category of rings and ring morphisms, the inclusion $\mathbb{Z}\to\mathbb{Q}$ is again epic, and this shows that in a sense $\mathbb{Z}$ is dense in $\mathbb{Q}$: $\mathbb{Q}$ is (as a ring) ""entirely determined"" by $\mathbb{Z}$. Obviously this density isn't the one in the sense of the usual topology on these rings. However I was wondering if this can be captured in a certain topology on $\mathbb{Q}$, and more generally I have the following question : Given a category of structured sets (for instance a category of algebras in the sense of universal algebra- in particular varieties for instance) and structure-preserving maps, can the structured sets always be furnished with a topology that makes epimorphisms in said category the morphisms whose image is dense (wrt to said topology) ? If you have an answer in a specific case, I'll take it as well, I don't necessarily need an answer as general as the question. For instance if you have an answer for the category of rings, or for varieties, or for algebras, etc. I'll be glad to hear it. EDIT : As has been noted in a comment, the trivial topology would work. As this obviously isn't satisfactory, I'll think about other things that such a topology would have to verify. But still, you can try and guess what I would expect from it, until I come up with natural things to prevent trivial examples",,"['abstract-algebra', 'general-topology', 'category-theory', 'universal-algebra']"
30,Galois group of $f(x^2)$,Galois group of,f(x^2),"If the Galois group of some irreducible polynomial $f(x)$ over some field $F$ is known, is there some method for calculating the Galois group of the polynomial $f(x^2)$ over the same field?","If the Galois group of some irreducible polynomial $f(x)$ over some field $F$ is known, is there some method for calculating the Galois group of the polynomial $f(x^2)$ over the same field?",,"['abstract-algebra', 'field-theory', 'galois-theory']"
31,Showing a module is finitely generated and projective,Showing a module is finitely generated and projective,,"Let $R$ be a commutative ring and $V$ be an $R$-module such that $V\otimes_{R}V \cong R$ as $R$-modules. I want to show that $V$ is a finitely generated and projective $R$-module. What I've considered: Clearly $V\otimes_{R}V$ is finitely generated and projective, since $R$ is immediately from definitions. For projectivity, given a short exact sequence of $R$-modules: $$M \xrightarrow{f} V \rightarrow 0$$ we can apply the right exact functor $\_\otimes_{R} V$, yielding: $$ M\otimes_{R} V\xrightarrow{f\otimes\text{id}} V \otimes_{R} V \rightarrow 0.$$ As $ V \otimes_{R} V$ is projective, this splits, so, morally, I just want to  ""restrict the splitting map to the pure tensors of the form $v \otimes 1$"" and conclude. The only thing I've thought of that could potentially make this precise is to show that the splitting map must be of the form $s=s_{1}\otimes s_{2}$. For being finitely generated, it seems ""intuitively clear"" to me that if $V \otimes_{R} V$ is finitely generated, then so must be $V$, but I'm having trouble showing this carefully. In particular, I feel good about the converse statement, as discussed in Tensor product of two finitely generated modules","Let $R$ be a commutative ring and $V$ be an $R$-module such that $V\otimes_{R}V \cong R$ as $R$-modules. I want to show that $V$ is a finitely generated and projective $R$-module. What I've considered: Clearly $V\otimes_{R}V$ is finitely generated and projective, since $R$ is immediately from definitions. For projectivity, given a short exact sequence of $R$-modules: $$M \xrightarrow{f} V \rightarrow 0$$ we can apply the right exact functor $\_\otimes_{R} V$, yielding: $$ M\otimes_{R} V\xrightarrow{f\otimes\text{id}} V \otimes_{R} V \rightarrow 0.$$ As $ V \otimes_{R} V$ is projective, this splits, so, morally, I just want to  ""restrict the splitting map to the pure tensors of the form $v \otimes 1$"" and conclude. The only thing I've thought of that could potentially make this precise is to show that the splitting map must be of the form $s=s_{1}\otimes s_{2}$. For being finitely generated, it seems ""intuitively clear"" to me that if $V \otimes_{R} V$ is finitely generated, then so must be $V$, but I'm having trouble showing this carefully. In particular, I feel good about the converse statement, as discussed in Tensor product of two finitely generated modules",,"['abstract-algebra', 'commutative-algebra', 'modules', 'tensor-products', 'projective-module']"
32,What is the difference between free groups and a free product?,What is the difference between free groups and a free product?,,"I am encountering free products for the first time in Algebraic Topology during the discussion of van Kampen's theorem, and I can't seem to tell the difference between a free product of groups, and a free group. The definition I know (of a free product) is: Let $G,H$ be groups. Define $G *H$ to be the set of all formal words $g_1h_1 \cdots g_nh_k$ where $g_i \in G$ and $h_j \in H$. Then $G*H$ is a group under the operation of juxtaposition, and the identity is the empty word. This definition can be extended to the free product of an arbitrary collection $G_\alpha$ of groups, but I don't see how this definition is different from the free group on a set $S$. My guesses at the differences: Free groups can be formed with any set $S$, whereas free products are defined for collections of groups . Free products respects relations between the groups. For example,  in $G*H$ the word $g_1g_2h_1$ is just $g_3h_1$ for some $g_3=g_1g_2 \in G$. Whereas there is no underlying relation for words in a free group (except for the formal cancellation of words). And one last questions: are all free groups also free products? Or is the inclusion in the other direction?","I am encountering free products for the first time in Algebraic Topology during the discussion of van Kampen's theorem, and I can't seem to tell the difference between a free product of groups, and a free group. The definition I know (of a free product) is: Let $G,H$ be groups. Define $G *H$ to be the set of all formal words $g_1h_1 \cdots g_nh_k$ where $g_i \in G$ and $h_j \in H$. Then $G*H$ is a group under the operation of juxtaposition, and the identity is the empty word. This definition can be extended to the free product of an arbitrary collection $G_\alpha$ of groups, but I don't see how this definition is different from the free group on a set $S$. My guesses at the differences: Free groups can be formed with any set $S$, whereas free products are defined for collections of groups . Free products respects relations between the groups. For example,  in $G*H$ the word $g_1g_2h_1$ is just $g_3h_1$ for some $g_3=g_1g_2 \in G$. Whereas there is no underlying relation for words in a free group (except for the formal cancellation of words). And one last questions: are all free groups also free products? Or is the inclusion in the other direction?",,"['abstract-algebra', 'algebraic-topology', 'free-groups', 'free-product']"
33,A degree $4$ polynomial whose Galois group is isomorphic to $S_4$.,A degree  polynomial whose Galois group is isomorphic to .,4 S_4,"I am reading an article about Galois groups. The article states that: It can also be shown that for each degree $d$ there exist polynomials whose Galois group is the fully symmetric group $S_d$. I think that the Galois group of a quadratic is isomorphic to $S_2$ if the roots are not rational. I think that the Galois group of $x^3 - a$ is isomorphic to $S_3$ if $a$ is not a perfect cube. Is it difficult to find an example of a degree $4$ polynomial whose Galois group is isomorphic to $S_4$?  I am reading a text book and so far most of the examples have been for Galois groups of ""small"" order. I have not progressed to the point where I can appreciate a proof of the statement made in the article but I would like to see an example of such a polynomial of degree $4$ or hear from someone knowledgeable that this is a difficult and/or tedious task.","I am reading an article about Galois groups. The article states that: It can also be shown that for each degree $d$ there exist polynomials whose Galois group is the fully symmetric group $S_d$. I think that the Galois group of a quadratic is isomorphic to $S_2$ if the roots are not rational. I think that the Galois group of $x^3 - a$ is isomorphic to $S_3$ if $a$ is not a perfect cube. Is it difficult to find an example of a degree $4$ polynomial whose Galois group is isomorphic to $S_4$?  I am reading a text book and so far most of the examples have been for Galois groups of ""small"" order. I have not progressed to the point where I can appreciate a proof of the statement made in the article but I would like to see an example of such a polynomial of degree $4$ or hear from someone knowledgeable that this is a difficult and/or tedious task.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
34,Proof that the solutions are algebraic functions,Proof that the solutions are algebraic functions,,"I am looking at the following: $$$$ $$$$ I haven't really understood the proof... Why do we consider the differential equation $y'=P(x)y$ ? Why does the sentence:  ""If $(3)_{\mathfrak{p}}$ has a solution in $\overline{K}_{\mathfrak{p}}(x)$, then $(3)_{\mathfrak{p}}$ has also a solution $y_{\mathfrak{p}}$ in $\overline{K}_{\mathfrak{p}}[x]$."" stand? Also why do we put $\displaystyle{y_{\mathfrak{p}}=\prod_i (x-\overline{\alpha}_i)^{c_i}}$ ? $$$$ EDIT1: I found now the following sentence: The constant field of the differential field $k((x))$ is $k((x^p))$.  Hence if $(1)_p$ has a solution in $k((x))$, multiplication by a suitable constant yields a solution in $k[[x]]$. Do we maybe have the following? We suppose that $(3)_{\mathfrak{p}}$ has a solution in $\overline{K}_{\mathfrak{p}}(x)$.  The constant field of $\overline{K}_{\mathfrak{p}}(x)$ is $\overline{K}_{\mathfrak{p}}(x^p)$.  So if we multiply the equation by a suitable constant, it follows that $(3)_{\mathfrak{p}}$ has a solution also in $\overline{K}_{\mathfrak{p}}[x]$. $$$$ EDIT2: Could you explain to me how exactly we conclude that $\beta_i \in \mathbb{Q}$ ?","I am looking at the following: $$$$ $$$$ I haven't really understood the proof... Why do we consider the differential equation $y'=P(x)y$ ? Why does the sentence:  ""If $(3)_{\mathfrak{p}}$ has a solution in $\overline{K}_{\mathfrak{p}}(x)$, then $(3)_{\mathfrak{p}}$ has also a solution $y_{\mathfrak{p}}$ in $\overline{K}_{\mathfrak{p}}[x]$."" stand? Also why do we put $\displaystyle{y_{\mathfrak{p}}=\prod_i (x-\overline{\alpha}_i)^{c_i}}$ ? $$$$ EDIT1: I found now the following sentence: The constant field of the differential field $k((x))$ is $k((x^p))$.  Hence if $(1)_p$ has a solution in $k((x))$, multiplication by a suitable constant yields a solution in $k[[x]]$. Do we maybe have the following? We suppose that $(3)_{\mathfrak{p}}$ has a solution in $\overline{K}_{\mathfrak{p}}(x)$.  The constant field of $\overline{K}_{\mathfrak{p}}(x)$ is $\overline{K}_{\mathfrak{p}}(x^p)$.  So if we multiply the equation by a suitable constant, it follows that $(3)_{\mathfrak{p}}$ has a solution also in $\overline{K}_{\mathfrak{p}}[x]$. $$$$ EDIT2: Could you explain to me how exactly we conclude that $\beta_i \in \mathbb{Q}$ ?",,"['abstract-algebra', 'number-theory', 'elementary-number-theory', 'ideals', 'differential-algebra']"
35,"Is a polynomial $f$ zero at $(a_1,\ldots,a_n)$ iff $f$ lies in the ideal $(X_1-a_1,\ldots,X_n-a_n)$?",Is a polynomial  zero at  iff  lies in the ideal ?,"f (a_1,\ldots,a_n) f (X_1-a_1,\ldots,X_n-a_n)","This is probably a very silly question: If $R$ is an arbitrary commutative ring with unit and $f\in R[X]$ a polynomial, then for any element $a\in R$ we have $$f(a)=0 \Longleftrightarrow X-a ~\mbox{ divides }~ f \Longleftrightarrow f\in (X-a)$$ where the last equivalence is clear. The first is probably a little surprising as $R[X]$ is usually not euclidean and it is perhaps not clear how to divide by $X-a$. Now let $f\in R[X_1,\ldots, X_n]$ be a polynomial. How can I see for an element $(a_1,\ldots,a_n)\in R^n$ that   $$f(a_1,\ldots,a_n)=0 \Longleftrightarrow f\in (X_1-a_1,\ldots,X_n-a_n) ?$$   If this does not work in general, let $R=K$ be a field.","This is probably a very silly question: If $R$ is an arbitrary commutative ring with unit and $f\in R[X]$ a polynomial, then for any element $a\in R$ we have $$f(a)=0 \Longleftrightarrow X-a ~\mbox{ divides }~ f \Longleftrightarrow f\in (X-a)$$ where the last equivalence is clear. The first is probably a little surprising as $R[X]$ is usually not euclidean and it is perhaps not clear how to divide by $X-a$. Now let $f\in R[X_1,\ldots, X_n]$ be a polynomial. How can I see for an element $(a_1,\ldots,a_n)\in R^n$ that   $$f(a_1,\ldots,a_n)=0 \Longleftrightarrow f\in (X_1-a_1,\ldots,X_n-a_n) ?$$   If this does not work in general, let $R=K$ be a field.",,"['abstract-algebra', 'polynomials', 'ideals', 'roots']"
36,Shortest abstract algebra book,Shortest abstract algebra book,,"I'm familiar with rigorous linear algebra, and I've had a very elementary course on modern algebra. I'm not interested in algebra but I need to learn more about it. Hence I'm looking for a concise and self-contained book on abstract algebra which covers what is needed for applications in the parts of mathematics relevant to physics, esp. differential geometry (incl. Lie theory and de Rham cohomology) and operator algebras. I'm not sure what topics exactly the book needs to cover, but probably someone here does. I'm also not sure if such a book exists: perhaps these areas are too broad. It doesn't have to literally be a book: it could be a chapter or appendix in another book, or lecture notes, but it should include nontrivial proofs. It would be best if the book assumes a knowledge of linear algebra so that the general linear group, etc. can be used as examples. To clarify: of course I'll have to look up specialized topics in one of the encyclopedic books, but I'm trying to find something that quickly covers the basics.","I'm familiar with rigorous linear algebra, and I've had a very elementary course on modern algebra. I'm not interested in algebra but I need to learn more about it. Hence I'm looking for a concise and self-contained book on abstract algebra which covers what is needed for applications in the parts of mathematics relevant to physics, esp. differential geometry (incl. Lie theory and de Rham cohomology) and operator algebras. I'm not sure what topics exactly the book needs to cover, but probably someone here does. I'm also not sure if such a book exists: perhaps these areas are too broad. It doesn't have to literally be a book: it could be a chapter or appendix in another book, or lecture notes, but it should include nontrivial proofs. It would be best if the book assumes a knowledge of linear algebra so that the general linear group, etc. can be used as examples. To clarify: of course I'll have to look up specialized topics in one of the encyclopedic books, but I'm trying to find something that quickly covers the basics.",,"['abstract-algebra', 'reference-request', 'book-recommendation']"
37,Modular Forms: Find a set of representatives for the cusps of $\Gamma_0(4)$,Modular Forms: Find a set of representatives for the cusps of,\Gamma_0(4),"We write $SL_2(\mathbb{Z})$ as $\coprod\alpha_i\Gamma_0(4)$, where $\coprod$ means the disjoint union, and the $\alpha_i$ are the coset representatives of $SL_2(\mathbb{Z})\diagup\Gamma_0(4)$. We were told that the cusps are then the points $\alpha_i\infty$, as $\alpha_i$ varies. I understand this, but then I am having trouble working out what the coset reps are for this particular example. Since $$\Gamma_0(4)=\left\lbrace \begin{pmatrix} a&b\\4c&d\\ \end{pmatrix}:a,b,c,d\in \mathbb{Z}\right\rbrace $$ I thought the representatives could be something like $\left\lbrace\begin{pmatrix}a&b\\c'&d\\ \end{pmatrix}\right\rbrace$, where $0\leq c'\leq 3$ and $a,b,d$ are arbitrary. But then I know that $SL_2(\mathbb{Z})\diagup\Gamma_0(4)$ has order $6$, due to the formula $\left(SL_2(\mathbb{Z}):\Gamma_0(N)\right)=N\prod_{p|N} (1+\frac 1p)$. This may be more of an algebraic problem than a modular forms one specifically, but I am struggling here!","We write $SL_2(\mathbb{Z})$ as $\coprod\alpha_i\Gamma_0(4)$, where $\coprod$ means the disjoint union, and the $\alpha_i$ are the coset representatives of $SL_2(\mathbb{Z})\diagup\Gamma_0(4)$. We were told that the cusps are then the points $\alpha_i\infty$, as $\alpha_i$ varies. I understand this, but then I am having trouble working out what the coset reps are for this particular example. Since $$\Gamma_0(4)=\left\lbrace \begin{pmatrix} a&b\\4c&d\\ \end{pmatrix}:a,b,c,d\in \mathbb{Z}\right\rbrace $$ I thought the representatives could be something like $\left\lbrace\begin{pmatrix}a&b\\c'&d\\ \end{pmatrix}\right\rbrace$, where $0\leq c'\leq 3$ and $a,b,d$ are arbitrary. But then I know that $SL_2(\mathbb{Z})\diagup\Gamma_0(4)$ has order $6$, due to the formula $\left(SL_2(\mathbb{Z}):\Gamma_0(N)\right)=N\prod_{p|N} (1+\frac 1p)$. This may be more of an algebraic problem than a modular forms one specifically, but I am struggling here!",,"['abstract-algebra', 'modular-forms']"
38,Help to prove that $ U_{p} $ is a cyclic group.,Help to prove that  is a cyclic group., U_{p} ,"As part of my study of Abstract Algebra, I’m trying to prove that $ U_{p} $ is cyclic for $ p $ a prime number. It’s a classical result, but I’m trying to prove it following four steps stated as problems in a book. I was able to handle Problems $ 1 $ and $ 2 $, but $ 3 $ is giving me trouble. Problem $ 3 $. Let $ G $ be a finite abelian group of order $ n $ for which the number of solutions of the equation $ x^{m} = e_{G} $ is at most $ m $ for any $ m $ dividing $ n $. Prove that $ G $ must be cyclic. Hint: Let $ \psi(m) $ be the number of elements of $ G $ of order $ m $. Show that $ \psi(m) \leq \phi(m) $ and use Problem $ 2 $ . ($ \phi $ is Euler’s totient function .) The other two problems are: Problem $ 1 $. In a cyclic group of order $ n $, show that for each positive integer $ m $ that divides $ n $, there are $ \phi(m) $ elements of order $ m $. Problem $ 2 $. Using Problem $ 1 $ , show that $ \displaystyle n = \sum_{m|n} \phi(m) $. I sincerely have no clue on how to solve, or merely attack, Problem $ 3 $ , so I came here looking for a better hint. I’m supposed to use only basic group-theoretic results up to Lagrange’s Theorem . Note: It’s not homework, just personal study. The book is I. N. Herstein’s Abstract Algebra .","As part of my study of Abstract Algebra, I’m trying to prove that $ U_{p} $ is cyclic for $ p $ a prime number. It’s a classical result, but I’m trying to prove it following four steps stated as problems in a book. I was able to handle Problems $ 1 $ and $ 2 $, but $ 3 $ is giving me trouble. Problem $ 3 $. Let $ G $ be a finite abelian group of order $ n $ for which the number of solutions of the equation $ x^{m} = e_{G} $ is at most $ m $ for any $ m $ dividing $ n $. Prove that $ G $ must be cyclic. Hint: Let $ \psi(m) $ be the number of elements of $ G $ of order $ m $. Show that $ \psi(m) \leq \phi(m) $ and use Problem $ 2 $ . ($ \phi $ is Euler’s totient function .) The other two problems are: Problem $ 1 $. In a cyclic group of order $ n $, show that for each positive integer $ m $ that divides $ n $, there are $ \phi(m) $ elements of order $ m $. Problem $ 2 $. Using Problem $ 1 $ , show that $ \displaystyle n = \sum_{m|n} \phi(m) $. I sincerely have no clue on how to solve, or merely attack, Problem $ 3 $ , so I came here looking for a better hint. I’m supposed to use only basic group-theoretic results up to Lagrange’s Theorem . Note: It’s not homework, just personal study. The book is I. N. Herstein’s Abstract Algebra .",,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups', 'totient-function']"
39,Divisibility of $3^n-1$ by $2^n$.,Divisibility of  by .,3^n-1 2^n,"It is a curious fact that $3^n-1$ is divisible by $2^n$ iff $n=1,2,4$. (It turns out to have applications in algebraic topology, differential topology and algebra.) Does anyone know of a very short proof of this fact? Methods should come from (elementary) number theory or abstract algebra.","It is a curious fact that $3^n-1$ is divisible by $2^n$ iff $n=1,2,4$. (It turns out to have applications in algebraic topology, differential topology and algebra.) Does anyone know of a very short proof of this fact? Methods should come from (elementary) number theory or abstract algebra.",,['abstract-algebra']
40,When are two simple tensors $m' \otimes n'$ and $m \otimes n$ equal? (tensor product over modules),When are two simple tensors  and  equal? (tensor product over modules),m' \otimes n' m \otimes n,"Suppose that $M$ is a right R-module and $N$ is a left $R$-module. We can construct $M \underset{R}\otimes N$ and give it an Abelian group structure by considering the free R-module $K$ generated by the relations $\{(x+x',y)-(x,y)-(x',y),(x,y+y')-(x,y)-(x,y'),(xr,y)-(x,ry)\}$ and then quotienting $M \times N$ by $K$. If $R$ is commutative this Abelian group turns into an $R$-module itself. Now, the question is, what does $m' \otimes n' = m \otimes n$ mean in this situation? So, if I were given two simple tensors, how could I know if these two were equal or not? I've been thinking about this for a while, but it's still pretty vague for me. A simpler question is when $m \otimes n=0$ in $M \underset{R}\otimes N$?","Suppose that $M$ is a right R-module and $N$ is a left $R$-module. We can construct $M \underset{R}\otimes N$ and give it an Abelian group structure by considering the free R-module $K$ generated by the relations $\{(x+x',y)-(x,y)-(x',y),(x,y+y')-(x,y)-(x,y'),(xr,y)-(x,ry)\}$ and then quotienting $M \times N$ by $K$. If $R$ is commutative this Abelian group turns into an $R$-module itself. Now, the question is, what does $m' \otimes n' = m \otimes n$ mean in this situation? So, if I were given two simple tensors, how could I know if these two were equal or not? I've been thinking about this for a while, but it's still pretty vague for me. A simpler question is when $m \otimes n=0$ in $M \underset{R}\otimes N$?",,"['abstract-algebra', 'modules', 'tensor-products', 'multilinear-algebra']"
41,Prove that stabilizer subgroups of G are conjugate to each other,Prove that stabilizer subgroups of G are conjugate to each other,,"Suppose that a group $G$ acts on a set $X$. Show that if $x_1$ and $x_2$ in X are in   the same $G$-orbit, then their stabilizer subgroups of $G$ are conjugate   to each other. My proof: Assume $x_1 = g_1x$ and $x_2 = g_2 x$ for some $g_1, g_2 \in G$. Let $h \in G_{x_1}$. We claim that $g_2g_1^{-1}hg_1g_2^{-1}$ is in $G_{x_2}$, thus proving that the two stabilizer subgroups are conjugate to each other. Indeed,  $$\begin{align} x_1&=g_1x\\ g_2g_1^{-1}x_1&=g_2x\\ g_2g_1^{-1}hx_1&=g_2x\\ g_2g_1^{-1}hg_1x&=g_2x\\ (g_2g_1^{-1}hg_1g_2^{-1})x_2&=x_2\\ \end{align}$$ as desired. I think it is a bit messy. Can you please comment on my proof and leave your own proof so that I can learn in a better way? Thanks in advance.","Suppose that a group $G$ acts on a set $X$. Show that if $x_1$ and $x_2$ in X are in   the same $G$-orbit, then their stabilizer subgroups of $G$ are conjugate   to each other. My proof: Assume $x_1 = g_1x$ and $x_2 = g_2 x$ for some $g_1, g_2 \in G$. Let $h \in G_{x_1}$. We claim that $g_2g_1^{-1}hg_1g_2^{-1}$ is in $G_{x_2}$, thus proving that the two stabilizer subgroups are conjugate to each other. Indeed,  $$\begin{align} x_1&=g_1x\\ g_2g_1^{-1}x_1&=g_2x\\ g_2g_1^{-1}hx_1&=g_2x\\ g_2g_1^{-1}hg_1x&=g_2x\\ (g_2g_1^{-1}hg_1g_2^{-1})x_2&=x_2\\ \end{align}$$ as desired. I think it is a bit messy. Can you please comment on my proof and leave your own proof so that I can learn in a better way? Thanks in advance.",,"['abstract-algebra', 'group-theory', 'proof-verification', 'proof-writing']"
42,Prove that some bivariate polynomial is irreducible,Prove that some bivariate polynomial is irreducible,,"How can I prove that $p(x,y)=2xy+x^2+y^3$ is irreducible over the ring $\mathbb{R}[x,y]$? Is there, for instance, any way to generalize Eisenstein's criterion to polynomials in several variables? I have tried a proof by contradiction without success, also I am looking for stronger techniques when it comes to polynomials in more than one variable. Any help?","How can I prove that $p(x,y)=2xy+x^2+y^3$ is irreducible over the ring $\mathbb{R}[x,y]$? Is there, for instance, any way to generalize Eisenstein's criterion to polynomials in several variables? I have tried a proof by contradiction without success, also I am looking for stronger techniques when it comes to polynomials in more than one variable. Any help?",,['abstract-algebra']
43,$|G|=p^3$. Prove that $\phi(x)=x^p$ is a homomorphism,. Prove that  is a homomorphism,|G|=p^3 \phi(x)=x^p,"Let $p$ be an odd prime and $G$ a group of order $p^3$. Prove that the $p$-th power map $x  \mapsto x^p$ is a homomorphism $G \rightarrow G$. The abelian case is easy. Suppose $G$ is non-abelian group. If $x \in G$  or $y \in G$ has order $p$, then $\phi(xy)=\phi(x)\phi(y)$. Thus, we can assume that $x,y$ are order of $p^2$. That's what I did. I don't have any tools. I need your help.","Let $p$ be an odd prime and $G$ a group of order $p^3$. Prove that the $p$-th power map $x  \mapsto x^p$ is a homomorphism $G \rightarrow G$. The abelian case is easy. Suppose $G$ is non-abelian group. If $x \in G$  or $y \in G$ has order $p$, then $\phi(xy)=\phi(x)\phi(y)$. Thus, we can assume that $x,y$ are order of $p^2$. That's what I did. I don't have any tools. I need your help.",,"['abstract-algebra', 'group-theory']"
44,$a^{\phi (n) +1} \equiv a \pmod{\! n}; $ Carmichael generalization of Fermat & Euler theorems.,Carmichael generalization of Fermat & Euler theorems.,a^{\phi (n) +1} \equiv a \pmod{\! n}; ,"I want to know a proof of an alternative form of Fermat-Euler's theorem $$a^{\phi (n) +1} \equiv a \pmod n$$ I searched some number theory books and a cryptography book and internet, but there were only proofs of the original theorem $a^{\phi (n)} \equiv 1 \pmod n$ or in case n=pq which is used for RSA. So I would be very thankful if someone show me a proof or book I should read.","I want to know a proof of an alternative form of Fermat-Euler's theorem I searched some number theory books and a cryptography book and internet, but there were only proofs of the original theorem or in case n=pq which is used for RSA. So I would be very thankful if someone show me a proof or book I should read.",a^{\phi (n) +1} \equiv a \pmod n a^{\phi (n)} \equiv 1 \pmod n,"['abstract-algebra', 'number-theory', 'modular-arithmetic', 'cryptography', 'totient-function']"
45,Commutator subgroup $G'$ is a characteristic subgroup of $G$,Commutator subgroup  is a characteristic subgroup of,G' G,"For any group $G$, prove that the commutator subgroup $G'$ is a characteristic subgroup of $G$. Let $U=\{xyx^{-1}y^{-1}|x, y \in G\}$. Now $G'$ is the smallest subgroup of $G$ which contains $U$. We need to show that $T(G') \subset G'$ for all automorphisms $T$ of $G$. If every element of $G'$ is of the form $g'=\{xyx^{-1}y^{-1}\}$, then $T(xyx^{-1}y^{-1 })=T(x)T(y)T(x^{-1})T(y^{-1})=T(x)T(y){T(x)}^{-1}{T(y)}^{-1}$. Since $T$ is an automorphism $T(g) \in G$ for all $g\in G$. Hence $T(x)T(y){T(x)}^{-1}{T(y)}^{-1}\in G$. But this is of the form of an element in $U$. Hence $T(g')\in U$. So $T(g') \in G'$. Is this the end of it?? I mean is every element in $G'$ of the form mentioned above??","For any group $G$, prove that the commutator subgroup $G'$ is a characteristic subgroup of $G$. Let $U=\{xyx^{-1}y^{-1}|x, y \in G\}$. Now $G'$ is the smallest subgroup of $G$ which contains $U$. We need to show that $T(G') \subset G'$ for all automorphisms $T$ of $G$. If every element of $G'$ is of the form $g'=\{xyx^{-1}y^{-1}\}$, then $T(xyx^{-1}y^{-1 })=T(x)T(y)T(x^{-1})T(y^{-1})=T(x)T(y){T(x)}^{-1}{T(y)}^{-1}$. Since $T$ is an automorphism $T(g) \in G$ for all $g\in G$. Hence $T(x)T(y){T(x)}^{-1}{T(y)}^{-1}\in G$. But this is of the form of an element in $U$. Hence $T(g')\in U$. So $T(g') \in G'$. Is this the end of it?? I mean is every element in $G'$ of the form mentioned above??",,"['abstract-algebra', 'group-theory', 'characteristic-subgroups', 'derived-subgroup']"
46,Field extensions and algebraic/transcendental elements,Field extensions and algebraic/transcendental elements,,"Let $E$ be an extension of field $F$, and let $\alpha, \beta \in E$. Suppose $\alpha$ is transcendental over $F$ but algebraic over $F(\beta)$. Show that $\beta$ is algebraic over $F(\alpha)$. Okay, first questions: What does the notation $F(\alpha)$ and $F(\beta)$ mean? And being transcendental means it solves no equations with rational coefficients, but what does it mean for a field?","Let $E$ be an extension of field $F$, and let $\alpha, \beta \in E$. Suppose $\alpha$ is transcendental over $F$ but algebraic over $F(\beta)$. Show that $\beta$ is algebraic over $F(\alpha)$. Okay, first questions: What does the notation $F(\alpha)$ and $F(\beta)$ mean? And being transcendental means it solves no equations with rational coefficients, but what does it mean for a field?",,"['abstract-algebra', 'ring-theory', 'field-theory']"
47,Show from the axioms: Addition in a quasifield is abelian,Show from the axioms: Addition in a quasifield is abelian,,"According to wikipedia a quasifield is an algebraic structure $(Q,+,\cdot)$ such that $(Q,+)$ is a group. (As usual, we denote its identity element by $0$.) $(Q\setminus\{0\},\cdot)$ is a loop . (Its identity element will be denoted by $1$.) Left distributive law: $a\cdot (b+c) = a\cdot b + a\cdot c$ for all $a,b,c\in Q$. Each equation $a\cdot x = b\cdot x + c$ with $a,b,c\in Q$ and $a\neq b$ has a unique solution $x\in Q$. Now the Wikipedia article claims that One can prove that the axioms imply that the additive group (Q,+) is abelian. My question is: How? For ordinary rings, we are in a similar situation. It is possible to drop the commutativity of the addition from the axioms, since it follows from the other axioms: $a + b = b + a$ is implied by $$ a + a + b + b = (a\cdot 1 + a\cdot 1) + (b\cdot 1 + b\cdot 1) \overset{\text{L}}{=} a\cdot (1 + 1) + b\cdot (1+1)\\\overset{\text{R}}{=} (a+b)\cdot (1+1) \overset{\text{L}}{=} (a+b)\cdot 1 + (a + b)\cdot 1 = a + b + a + b. $$ This is not too bad in our situation, since the proof doesn't rely on the associativity of the multiplication (which we don't have in a quasifield). However, the problem is that both distributive laws have been used (tagged with L and R). More precisely, step R is not covered by the above axioms. I wonder if we can find an alternative justification for that equality based on property 4, which looks a bit related to the missing right distributive law.","According to wikipedia a quasifield is an algebraic structure $(Q,+,\cdot)$ such that $(Q,+)$ is a group. (As usual, we denote its identity element by $0$.) $(Q\setminus\{0\},\cdot)$ is a loop . (Its identity element will be denoted by $1$.) Left distributive law: $a\cdot (b+c) = a\cdot b + a\cdot c$ for all $a,b,c\in Q$. Each equation $a\cdot x = b\cdot x + c$ with $a,b,c\in Q$ and $a\neq b$ has a unique solution $x\in Q$. Now the Wikipedia article claims that One can prove that the axioms imply that the additive group (Q,+) is abelian. My question is: How? For ordinary rings, we are in a similar situation. It is possible to drop the commutativity of the addition from the axioms, since it follows from the other axioms: $a + b = b + a$ is implied by $$ a + a + b + b = (a\cdot 1 + a\cdot 1) + (b\cdot 1 + b\cdot 1) \overset{\text{L}}{=} a\cdot (1 + 1) + b\cdot (1+1)\\\overset{\text{R}}{=} (a+b)\cdot (1+1) \overset{\text{L}}{=} (a+b)\cdot 1 + (a + b)\cdot 1 = a + b + a + b. $$ This is not too bad in our situation, since the proof doesn't rely on the associativity of the multiplication (which we don't have in a quasifield). However, the problem is that both distributive laws have been used (tagged with L and R). More precisely, step R is not covered by the above axioms. I wonder if we can find an alternative justification for that equality based on property 4, which looks a bit related to the missing right distributive law.",,"['abstract-algebra', 'ring-theory', 'field-theory', 'abelian-groups']"
48,How to show there exists no solution to a discrete logarithm problem on an Elliptic Curve?,How to show there exists no solution to a discrete logarithm problem on an Elliptic Curve?,,"The exact problem is to show that $\nexists$k such that $k(1,2) = (4,5)$ on the elliptic curve defined by $\widetilde{E}: y^2 = x^3 -14x + 17$ over $\mathbb Q$. Background: E: $y^2 = x^3 + 3$ over $F_7$. On E we have 4(1,2) = (4,5). Now the points P = (1,2) and Q = (4,5) are on both E and $\widetilde{E}$ and $\widetilde{E}$ was found by uplifting E (from mod 7 to Q). Also it was discovered that 2(1,2) = (1,-2) and 3(1,2) = $\infty $ mod 73 on $\widetilde{E}$.","The exact problem is to show that $\nexists$k such that $k(1,2) = (4,5)$ on the elliptic curve defined by $\widetilde{E}: y^2 = x^3 -14x + 17$ over $\mathbb Q$. Background: E: $y^2 = x^3 + 3$ over $F_7$. On E we have 4(1,2) = (4,5). Now the points P = (1,2) and Q = (4,5) are on both E and $\widetilde{E}$ and $\widetilde{E}$ was found by uplifting E (from mod 7 to Q). Also it was discovered that 2(1,2) = (1,-2) and 3(1,2) = $\infty $ mod 73 on $\widetilde{E}$.",,"['abstract-algebra', 'algebraic-geometry', 'elliptic-curves']"
49,Zorn's Lemma in noetherian modules,Zorn's Lemma in noetherian modules,,"For noetherian modules, we have in particular the equivalent definitions that the Ascending Chain Condition holds and that every nonempty subset of submodules has a maximal element. Now I can prove the ""ACC $\Rightarrow$ every nonempty set of submodules has a maximal element"" implication by the following argument: Let $S$ be a nonempty set of submodules, take $N_1 \in S$. If $N_1$ is not maximal, there exists a $N_2 \supsetneq N_1$ per definition. By induction we get a chain $N_1 \subsetneq N_2 \subsetneq \dotsb$ and by ACC this induction terminates after finitely many, say $n$, steps. Then $N_n$ is maximal. My question is: Did I miss the use of Zorn's Lemma somewhere? This proof doesn't seem to use it, and the other implications in the equivalence work fine without it too, yet I've seen some sources (e.g. Wikipedia, first answer here: Is every Noetherian module finitely generated? ) that require it. I'm fine with Zorn's Lemma by the way, just feeling a bit stupid right now.","For noetherian modules, we have in particular the equivalent definitions that the Ascending Chain Condition holds and that every nonempty subset of submodules has a maximal element. Now I can prove the ""ACC $\Rightarrow$ every nonempty set of submodules has a maximal element"" implication by the following argument: Let $S$ be a nonempty set of submodules, take $N_1 \in S$. If $N_1$ is not maximal, there exists a $N_2 \supsetneq N_1$ per definition. By induction we get a chain $N_1 \subsetneq N_2 \subsetneq \dotsb$ and by ACC this induction terminates after finitely many, say $n$, steps. Then $N_n$ is maximal. My question is: Did I miss the use of Zorn's Lemma somewhere? This proof doesn't seem to use it, and the other implications in the equivalence work fine without it too, yet I've seen some sources (e.g. Wikipedia, first answer here: Is every Noetherian module finitely generated? ) that require it. I'm fine with Zorn's Lemma by the way, just feeling a bit stupid right now.",,"['abstract-algebra', 'set-theory', 'modules']"
50,"If $σ^2$ is the identity map from $G$ to $G$, prove that $G$ is abelian.","If  is the identity map from  to , prove that  is abelian.",σ^2 G G G,"Let $G$ be a finite group which possesses an automorphism $σ$ such that $σ(g)=g$ if and only if $g=1$. If $σ^2$ is the identity map from $G$ to $G$, prove that $G$ is abelian. This is what I got Let $G$ be a finite group which possesses an automorphism $σ$ such that $σ(g)=g$ if and only if $g=1$.  Assume that $σ^2$ is the identity map from $G$ to $G$, we will show that $G $is abelian. Let $g,h∈G$, since $σ^2$ is the identity map from $G$ to $G$ $σ^2 (g)=g$ $σ^2 (h)=h$ And $σ^2 (gh)=gh=σ^2 (g) σ^2 (h)$ Since $σ∈Aut(G)$, $σ$ is isomorphism (homophism and bijective) , so  $σ(gh)=σ(g)σ(h)=gh$. Thus, $gh=1=hg$. Hence, $G$ is abelian. Did I do it correctly, I keep feeling I missed something.","Let $G$ be a finite group which possesses an automorphism $σ$ such that $σ(g)=g$ if and only if $g=1$. If $σ^2$ is the identity map from $G$ to $G$, prove that $G$ is abelian. This is what I got Let $G$ be a finite group which possesses an automorphism $σ$ such that $σ(g)=g$ if and only if $g=1$.  Assume that $σ^2$ is the identity map from $G$ to $G$, we will show that $G $is abelian. Let $g,h∈G$, since $σ^2$ is the identity map from $G$ to $G$ $σ^2 (g)=g$ $σ^2 (h)=h$ And $σ^2 (gh)=gh=σ^2 (g) σ^2 (h)$ Since $σ∈Aut(G)$, $σ$ is isomorphism (homophism and bijective) , so  $σ(gh)=σ(g)σ(h)=gh$. Thus, $gh=1=hg$. Hence, $G$ is abelian. Did I do it correctly, I keep feeling I missed something.",,"['abstract-algebra', 'finite-groups', 'abelian-groups']"
51,Infinite Galois Theory,Infinite Galois Theory,,"In infinite Galois theory for the one-one correspondence as in the finite case, one needs to introduce Krull topology. What is the intuition behind defining such topology ?","In infinite Galois theory for the one-one correspondence as in the finite case, one needs to introduce Krull topology. What is the intuition behind defining such topology ?",,"['abstract-algebra', 'general-topology', 'group-theory', 'galois-theory']"
52,Ext of an $\mathfrak{m}$-primary ideal,Ext of an -primary ideal,\mathfrak{m},"Let $(A,\mathfrak m,k)$ be a Noetherian local ring, $M$ a finitely generated $A$-module, and $I$ an $\mathfrak{m}$-primary ideal. If $\operatorname{Ext}^{i}_{A}(A/\mathfrak{m},M)=0$ then $\operatorname{Ext}^{i}_{A}(A/I,M)=0$?","Let $(A,\mathfrak m,k)$ be a Noetherian local ring, $M$ a finitely generated $A$-module, and $I$ an $\mathfrak{m}$-primary ideal. If $\operatorname{Ext}^{i}_{A}(A/\mathfrak{m},M)=0$ then $\operatorname{Ext}^{i}_{A}(A/I,M)=0$?",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra']"
53,Does $f=g_1^{n_1}\cdots g_k^{n_k}$ imply $\operatorname{Gal}(f)=\operatorname{Gal}(g_1)\times\cdots\times\operatorname{Gal}g_k)$?,Does  imply ?,f=g_1^{n_1}\cdots g_k^{n_k} \operatorname{Gal}(f)=\operatorname{Gal}(g_1)\times\cdots\times\operatorname{Gal}g_k),"Let $f\in \mathbb{Z}[x]$ and $f=g_1^{n_1}\cdots g_k^{n_k}$ where $g_1,\cdots, g_k$ are distinct irreducible polynomials over $\mathbb{Q}$ . Whether does it hold $\operatorname{Gal}(f)=\operatorname{Gal}(g_1)\times\cdots\times \operatorname{Gal}(g_k)$ ? Thank you a lot. Is this right or wrong? How to prove? (we may also add some conditions if cannot prove)",Let and where are distinct irreducible polynomials over . Whether does it hold ? Thank you a lot. Is this right or wrong? How to prove? (we may also add some conditions if cannot prove),"f\in \mathbb{Z}[x] f=g_1^{n_1}\cdots g_k^{n_k} g_1,\cdots, g_k \mathbb{Q} \operatorname{Gal}(f)=\operatorname{Gal}(g_1)\times\cdots\times \operatorname{Gal}(g_k)","['abstract-algebra', 'polynomials', 'finite-groups', 'field-theory', 'galois-theory']"
54,The group of invertible elements of $\mathbb F_{p}[x]/(x^m)$ is not a cyclic group.,The group of invertible elements of  is not a cyclic group.,\mathbb F_{p}[x]/(x^m),"I am stuck in a question about finite fields and would like to ask you for some help. Given an integer $m\geq 2$ and $p$ a prime number, show that  $(\mathbb F_{p}[x]/(x^m))^{\times}$ (the group of all invertible elements of $\mathbb F_{p}[x]/(x^m)$) is not a cyclic group. I don't really know how to start. I know a lemma which tells me that for every finite field $\mathbb F_{q}$ with $q$ elements, the multiplicative group of the nonzero elements $\mathbb F_{q}^{\times}$ is cyclic. But I don't think that I should use it here... Does anyone have an idea? Thank you in advance!","I am stuck in a question about finite fields and would like to ask you for some help. Given an integer $m\geq 2$ and $p$ a prime number, show that  $(\mathbb F_{p}[x]/(x^m))^{\times}$ (the group of all invertible elements of $\mathbb F_{p}[x]/(x^m)$) is not a cyclic group. I don't really know how to start. I know a lemma which tells me that for every finite field $\mathbb F_{q}$ with $q$ elements, the multiplicative group of the nonzero elements $\mathbb F_{q}^{\times}$ is cyclic. But I don't think that I should use it here... Does anyone have an idea? Thank you in advance!",,"['abstract-algebra', 'group-theory', 'finite-groups', 'finite-fields']"
55,Proof of Zassenhaus Lemma,Proof of Zassenhaus Lemma,,"Lemma 4.52 from Advanced Modern Algebra by Rotman: Given four subgroups $A \triangleleft A^*$ and $B \triangleleft B^*$ of a group G, then $A(A^* \cap B) \triangleleft A(A^* \cap B^*)$ , $B(B^* \cap A) \triangleleft B(B^* \cap A*)$ , and there is an isomorphism $$\frac{A(A^* \cap B^*)}{A(A^* \cap B)} \cong \frac{B(B^* \cap A^*)}{B(B^* \cap A)}$$ Proof We claim that $*(A \cap B^*) \triangleleft (A^* \cap B^*)$ : that is, if $c \in A \cap B^*$ and $x \in A^* \cap B^*$ , then $xcx^{-1} \in A \cap B^*$ . Now $xcx^{-1} \in A$ because $c \in A$ , $x \in A^*$ , and $A \triangleleft A^*$ ; but also $xcx^{-1} \in B^*$ , because $c,x \in B^*$ . Hence, $(A \cap B^*) \triangleleft (A^* \cap B^*)$ ; similarly, $(A^* \cap B) \triangleleft (A^* \cap B^*)$ . Therefore, the subset D, defined by $D = (A \cap B^*)(A^* \cap B)$ , is a normal subgroup of $A^* \cap B^*$ , because it is generated by two normal subgroups. Using the symmetry in the remark, it suffices to show that there is an isomorphism $$\frac{A(A^* \cap B^*)}{A(A^* \cap B)} \rightarrow \frac{A^* \cap B^*}{D}.$$ Define $\phi: A(A^* \cap B^*) \rightarrow (A^* \cap B^*)/D$ by $\phi: ax \rightarrow xD$ , where $a \in A$ and $x \in A^* \cap B^*$ . Now $\phi$ is well-defined: if ax=a'x', where $ax=a'x'$ , where $a' \in A$ and $x' \in A^* \cap B^*$ , then $(a')^{-1}a = x'x^{-1} \in A \cap (A^* \cap B^*) = A \cap B^* \subseteq D$ . Also, $\phi$ is a homomorphism: $axa'x' = a''xx'$ , where $a''=a(xa'x^{-1}) \in A$ (because $A \triangleleft A^*)$ , and so $\phi(axa'x') = \phi(a''xx') = xx'D = \phi(ax)\phi(a'x').$ It is routine to check that $\phi$ is surjective and that $ker \phi = A(A^* \cap B)$ . The First Isomorphism Thoerem completes the proof. I have some problems with seeing that the kernel is $A(A^* \cap B)$ . I understand that any element that belongs to $A(A^* \cap B)$ is of course mapped to the identity. But how do we know that these are the only elements in the kernel? Because we also know that $(A \cap B^*) \subseteq *A^* \cap B^*$ , and any element that belongs to $A(A \cap B^*)$ must also be mapped to the identity, right? Thanks in advance","Lemma 4.52 from Advanced Modern Algebra by Rotman: Given four subgroups and of a group G, then , , and there is an isomorphism Proof We claim that : that is, if and , then . Now because , , and ; but also , because . Hence, ; similarly, . Therefore, the subset D, defined by , is a normal subgroup of , because it is generated by two normal subgroups. Using the symmetry in the remark, it suffices to show that there is an isomorphism Define by , where and . Now is well-defined: if ax=a'x', where , where and , then . Also, is a homomorphism: , where (because , and so It is routine to check that is surjective and that . The First Isomorphism Thoerem completes the proof. I have some problems with seeing that the kernel is . I understand that any element that belongs to is of course mapped to the identity. But how do we know that these are the only elements in the kernel? Because we also know that , and any element that belongs to must also be mapped to the identity, right? Thanks in advance","A \triangleleft A^* B \triangleleft B^* A(A^* \cap B) \triangleleft A(A^* \cap B^*) B(B^* \cap A) \triangleleft B(B^* \cap A*) \frac{A(A^* \cap B^*)}{A(A^* \cap B)} \cong \frac{B(B^* \cap A^*)}{B(B^* \cap A)} *(A \cap B^*) \triangleleft (A^* \cap B^*) c \in A \cap B^* x \in A^* \cap B^* xcx^{-1} \in A \cap B^* xcx^{-1} \in A c \in A x \in A^* A \triangleleft A^* xcx^{-1} \in B^* c,x \in B^* (A \cap B^*) \triangleleft (A^* \cap B^*) (A^* \cap B) \triangleleft (A^* \cap B^*) D = (A \cap B^*)(A^* \cap B) A^* \cap B^* \frac{A(A^* \cap B^*)}{A(A^* \cap B)} \rightarrow \frac{A^* \cap B^*}{D}. \phi: A(A^* \cap B^*) \rightarrow (A^* \cap B^*)/D \phi: ax \rightarrow xD a \in A x \in A^* \cap B^* \phi ax=a'x' a' \in A x' \in A^* \cap B^* (a')^{-1}a = x'x^{-1} \in A \cap (A^* \cap B^*) = A \cap B^* \subseteq D \phi axa'x' = a''xx' a''=a(xa'x^{-1}) \in A A \triangleleft A^*) \phi(axa'x') = \phi(a''xx') = xx'D = \phi(ax)\phi(a'x'). \phi ker \phi = A(A^* \cap B) A(A^* \cap B) A(A^* \cap B) (A \cap B^*) \subseteq *A^* \cap B^* A(A \cap B^*)",['abstract-algebra']
56,"Is the set of submonoids of $(\Bbb N,+)$ countable?",Is the set of submonoids of  countable?,"(\Bbb N,+)","Having the monoid $(\Bbb N,+)$, I wonder if there are countable many submonoids. There are obviously infinitely many since $S_n = \{kn \mid k \in \Bbb N\}$ is a submonoid for any $n \in \Bbb N$. My conjecture is that the set of all submonoids is countable, because I think the following statements (which I failed to prove so far) hold for any submonoid $S$ of $(\Bbb N,+)$: there is an odd element in $S$ $\Rightarrow \exists e \forall f: (f \ge e \rightarrow f \in S)$ $\Rightarrow \Bbb N \setminus S$ is finite all elements of $S$ are even $\Rightarrow \exists e \forall f: (2f \ge e \rightarrow 2f \in S)$ $\Rightarrow \Bbb N \setminus (S \cup \{1,3,5,...\})$ is finite In both cases we can identify the submonoid by a finite set of numbers which are not elements of the submonoid. Therefore we have only countable many possibilities. Can you complete this approach or provide a better one?","Having the monoid $(\Bbb N,+)$, I wonder if there are countable many submonoids. There are obviously infinitely many since $S_n = \{kn \mid k \in \Bbb N\}$ is a submonoid for any $n \in \Bbb N$. My conjecture is that the set of all submonoids is countable, because I think the following statements (which I failed to prove so far) hold for any submonoid $S$ of $(\Bbb N,+)$: there is an odd element in $S$ $\Rightarrow \exists e \forall f: (f \ge e \rightarrow f \in S)$ $\Rightarrow \Bbb N \setminus S$ is finite all elements of $S$ are even $\Rightarrow \exists e \forall f: (2f \ge e \rightarrow 2f \in S)$ $\Rightarrow \Bbb N \setminus (S \cup \{1,3,5,...\})$ is finite In both cases we can identify the submonoid by a finite set of numbers which are not elements of the submonoid. Therefore we have only countable many possibilities. Can you complete this approach or provide a better one?",,"['abstract-algebra', 'monoid']"
57,All number fields with absolute value of discriminant $\le 20$,All number fields with absolute value of discriminant,\le 20,I need to find all number fields with absolute value of discriminant $\le 20$. Using Minkovsky's theorem I understood that it should be quadric or cubic extension. The case of quadric is very easy. As for cubic case I understood that it should have 2 complex embeddings and his ideal class group should be trivial. But I don't know to how find all of them.,I need to find all number fields with absolute value of discriminant $\le 20$. Using Minkovsky's theorem I understood that it should be quadric or cubic extension. The case of quadric is very easy. As for cubic case I understood that it should have 2 complex embeddings and his ideal class group should be trivial. But I don't know to how find all of them.,,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'extension-field']"
58,"${\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1}$",,"{\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1}","I'm trying to prove that ${\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1}$ (p prime). I know that ${\rm Aut}\,\mathbb Z_p$ has $p-1$ elements because $\mathbb Z_p$ has $p-1$ possiblities of generators, so intuitively I see ${\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1}$ , but I couldn't prove it formally. I'm trying to build an isomorphic function, but I don't how to do it. Am I in the right way? Maybe I'm forgetting some trick or something. Thanks","I'm trying to prove that (p prime). I know that has elements because has possiblities of generators, so intuitively I see , but I couldn't prove it formally. I'm trying to build an isomorphic function, but I don't how to do it. Am I in the right way? Maybe I'm forgetting some trick or something. Thanks","{\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1} {\rm Aut}\,\mathbb Z_p p-1 \mathbb Z_p p-1 {\rm Aut}\,\mathbb Z_p\simeq \mathbb Z_{p-1}","['group-theory', 'finite-groups', 'modular-arithmetic', 'abstract-algebra']"
59,One-parameter group not a group? Why?,One-parameter group not a group? Why?,,"So one-parameter group $G$ is defined with a continuous group homomorphism $\phi: \mathbb{R} \rightarrow G$. But according to the texts I read, they say that $G$ must be distinguished from groups as $G$ is not a group. So my wonder is, but there is group homomorphism there... So what is going on?","So one-parameter group $G$ is defined with a continuous group homomorphism $\phi: \mathbb{R} \rightarrow G$. But according to the texts I read, they say that $G$ must be distinguished from groups as $G$ is not a group. So my wonder is, but there is group homomorphism there... So what is going on?",,"['abstract-algebra', 'group-theory']"
60,"In $S_7$, $x^2=(1234)$ has no solutions but $x^3=(1234)$ has at least two","In ,  has no solutions but  has at least two",S_7 x^2=(1234) x^3=(1234),"How would I go about showing that in $S_7$, the equation $x^2=(1234)$ has no solutions but the equation $x^3=(1234)$ has at least two solutions?","How would I go about showing that in $S_7$, the equation $x^2=(1234)$ has no solutions but the equation $x^3=(1234)$ has at least two solutions?",,"['abstract-algebra', 'group-theory', 'permutations']"
61,Examples of unusual group operations from outside of group theory.,Examples of unusual group operations from outside of group theory.,,"Although it is certainly important to study frequently seen group operations like permutations, function composition, word operations, and so on, I find it fascinating to see group structure applied in strange and complicated ways to other disciplines. Lately MSE has got me looking at the group of functions $f:\mathbb{N}\rightarrow \mathbb{C}$ with $f(1)\not= 0$ under the Dirichlet product $\star$ given by $$(f\star g)(n)=\sum_{ab=n}f(a)g(b).$$ I think this is a great example of an unusual group operation.  I don't usually get into this type of arcane number theoretic stuff and never would have thought to look at this type of structure myself. What are some other examples of obscure group operations I may not have heard of?","Although it is certainly important to study frequently seen group operations like permutations, function composition, word operations, and so on, I find it fascinating to see group structure applied in strange and complicated ways to other disciplines. Lately MSE has got me looking at the group of functions $f:\mathbb{N}\rightarrow \mathbb{C}$ with $f(1)\not= 0$ under the Dirichlet product $\star$ given by $$(f\star g)(n)=\sum_{ab=n}f(a)g(b).$$ I think this is a great example of an unusual group operation.  I don't usually get into this type of arcane number theoretic stuff and never would have thought to look at this type of structure myself. What are some other examples of obscure group operations I may not have heard of?",,"['abstract-algebra', 'group-theory', 'big-list']"
62,Bivariate polynomials over finite fields,Bivariate polynomials over finite fields,,"If $f$ is a bivariate polynomial of degree $r$ over $\mathbb{Z}_p$, then the number of solutions to $f(x,y)=0$ should be less than $rp$. This can be seen by writing $f(x,y) = \sum_{i=0}^r a_i(x) y^{r-i}$ where $a_i(x)$ are univariate polynomials of degree utmost $i$. For each fixed $x$ the $f(x,y)$ is a univariate polynomial in $y$ of degree $r$ and only $r$ roots are possible. There are $p$ $x$'s so the total is $rp$. I wanted to know if this is this the best bound possible. While the univariate  case seems to be much studied, I could not find many references for this question on multivariate polynomials. Thanks, Phanindra","If $f$ is a bivariate polynomial of degree $r$ over $\mathbb{Z}_p$, then the number of solutions to $f(x,y)=0$ should be less than $rp$. This can be seen by writing $f(x,y) = \sum_{i=0}^r a_i(x) y^{r-i}$ where $a_i(x)$ are univariate polynomials of degree utmost $i$. For each fixed $x$ the $f(x,y)$ is a univariate polynomial in $y$ of degree $r$ and only $r$ roots are possible. There are $p$ $x$'s so the total is $rp$. I wanted to know if this is this the best bound possible. While the univariate  case seems to be much studied, I could not find many references for this question on multivariate polynomials. Thanks, Phanindra",,['abstract-algebra']
63,Cyclotomic extensions of $\Bbb Q$,Cyclotomic extensions of,\Bbb Q,"Let $n>4$, and $(h,n) = 1$. How to show that $[\mathbb{Q}(\tan 2 \pi h/n):\mathbb{Q}]= \phi(n)$ or $\phi(n)/2$ or $\phi(n)/4$ respectively if $\gcd(n,8)<4$  or $\gcd(n,8)=4$ or $\gcd(n,8)>4$. (In fact, this is a question from J. McCarthy's Algebraic Extensions of Fields, Ch. 2 . We know from a previous question that $[\mathbb{Q}(\cos 2\pi h/n):\mathbb{Q}]=\phi(n)/2$ if $n>2$ and $\gcd(n,h)=1$; and also that if $n>4,$ $[\mathbb{Q}(\sin 2\pi h/n):\mathbb{Q}]=\phi(n), \phi(n)/4$ or $\phi(n)/2$, respectively if $\gcd(n,8)<4$, $\gcd(n,8)=4$ or $\gcd(n,8)>4$.)","Let $n>4$, and $(h,n) = 1$. How to show that $[\mathbb{Q}(\tan 2 \pi h/n):\mathbb{Q}]= \phi(n)$ or $\phi(n)/2$ or $\phi(n)/4$ respectively if $\gcd(n,8)<4$  or $\gcd(n,8)=4$ or $\gcd(n,8)>4$. (In fact, this is a question from J. McCarthy's Algebraic Extensions of Fields, Ch. 2 . We know from a previous question that $[\mathbb{Q}(\cos 2\pi h/n):\mathbb{Q}]=\phi(n)/2$ if $n>2$ and $\gcd(n,h)=1$; and also that if $n>4,$ $[\mathbb{Q}(\sin 2\pi h/n):\mathbb{Q}]=\phi(n), \phi(n)/4$ or $\phi(n)/2$, respectively if $\gcd(n,8)<4$, $\gcd(n,8)=4$ or $\gcd(n,8)>4$.)",,"['abstract-algebra', 'field-theory', 'algebraic-number-theory']"
64,"Let K/F be a finite extension, given a polynomial in K[x] find another so that their product is in F[x]","Let K/F be a finite extension, given a polynomial in K[x] find another so that their product is in F[x]",,"Let $K$ be a finite extension of a field $F$, and let $f(x)$ be in $K[x]$. Prove that there is a nonzero polynomial $g(x)$ in $K[x]$ such that $f(x)g(x)$ is in $F[x]$. Should I do this by induction on the degree of $f(x)$? Obviously if $n=0$, then $g(x)=1/f(x)$ Let $f(x) = a_nx^n+...a_1x+a_0$       then I know that there exists a h(x) so that $(f(x)-a_nx^n)h(x)$      is in $F[x]$. I want now to find a $g(x)=h(x)+i(x)$ so that $f(x)g(x)$ is in $F[x]$. Thus I need to find an $i(x)$ so that $a_nx^nh(x)+i(x)f(x)$ is in $F[x]$. I feel like this is wrong because I have no control over the degrees of $h(x)$. Any suggestions?","Let $K$ be a finite extension of a field $F$, and let $f(x)$ be in $K[x]$. Prove that there is a nonzero polynomial $g(x)$ in $K[x]$ such that $f(x)g(x)$ is in $F[x]$. Should I do this by induction on the degree of $f(x)$? Obviously if $n=0$, then $g(x)=1/f(x)$ Let $f(x) = a_nx^n+...a_1x+a_0$       then I know that there exists a h(x) so that $(f(x)-a_nx^n)h(x)$      is in $F[x]$. I want now to find a $g(x)=h(x)+i(x)$ so that $f(x)g(x)$ is in $F[x]$. Thus I need to find an $i(x)$ so that $a_nx^nh(x)+i(x)f(x)$ is in $F[x]$. I feel like this is wrong because I have no control over the degrees of $h(x)$. Any suggestions?",,"['abstract-algebra', 'polynomials', 'field-theory', 'galois-theory']"
65,Isomorphism of modules + tensor product,Isomorphism of modules + tensor product,,"Is it true that: $$M{\otimes}_{A}(A/I) \cong M/IM$$ and $$IM \cong I {\otimes}_AM$$ where $A$ is a commutative ring, $M$ an $A$-module, and $I \subset A$ an ideal.","Is it true that: $$M{\otimes}_{A}(A/I) \cong M/IM$$ and $$IM \cong I {\otimes}_AM$$ where $A$ is a commutative ring, $M$ an $A$-module, and $I \subset A$ an ideal.",,"['abstract-algebra', 'modules', 'ideals', 'tensor-products']"
66,Why an initial ring is a domain?,Why an initial ring is a domain?,,"It is a well known fact that $\mathbb{Z}$ , the ring of integers, is a domain. On the other hand, $\mathbb{Z}$ is also the initial object in the category Ring. If one defines $\mathbb{Z}$ as the initial object in Ring, is it possible to prove it is a domain without an explicit construction?","It is a well known fact that , the ring of integers, is a domain. On the other hand, is also the initial object in the category Ring. If one defines as the initial object in Ring, is it possible to prove it is a domain without an explicit construction?",\mathbb{Z} \mathbb{Z} \mathbb{Z},"['abstract-algebra', 'category-theory', 'soft-question']"
67,$u^{2}-1$ is a unit in a ring $A$,is a unit in a ring,u^{2}-1 A,"Hello I have the next question that I want to prove (or one counterexample if there is one) Let $A$ be a ring with maximal ideal $M$ and quotient field $k=A/M$ of size at least $4$ such that the natural induced map $p:A^{\ast}\rightarrow k^{\ast}$ is surjective, then there exist $u\in A^{\ast}$ such that $u^{2}-1\in A^{\ast}$ I tried this Since $|k|\geq 4$ then there exist $x\in k^{\ast}$ with $x^{2}\not=1$ . Now since $p$ is surjective, we get that $x=p(u)=\overline{u}$ with $u\in A^{\ast}$ , then $\overline{u}^{2}\not=1$ in $k$ . Thus $\overline{u}^{2}-1\in k^{\ast}$ . From this I have that $p(a)=\overline{u}^{2}-1$ with $a\in  A^{\ast}$ . It follows that $u^{2}-1=a+m$ with $a\in  A^{\ast}$ , $m\in  M$ . From this I am not sure how to continue in order to prove that $u^{2}-1\in A^{\ast}$ . I was thinking that since $a$ is unit then $(u^{2}-1)a^{-1}=1+m'$ with $m'=ma^{-1}\in M$ . Thus $1+m'$ is a unit if and only if $u^{2}-1$ is a unit. So I need to prove that $1+m'$ is unit (That is true if $A$ is a local ring) That's my idea if there is a suggestion or hint  I would appreciate. Thanks!","Hello I have the next question that I want to prove (or one counterexample if there is one) Let be a ring with maximal ideal and quotient field of size at least such that the natural induced map is surjective, then there exist such that I tried this Since then there exist with . Now since is surjective, we get that with , then in . Thus . From this I have that with . It follows that with , . From this I am not sure how to continue in order to prove that . I was thinking that since is unit then with . Thus is a unit if and only if is a unit. So I need to prove that is unit (That is true if is a local ring) That's my idea if there is a suggestion or hint  I would appreciate. Thanks!",A M k=A/M 4 p:A^{\ast}\rightarrow k^{\ast} u\in A^{\ast} u^{2}-1\in A^{\ast} |k|\geq 4 x\in k^{\ast} x^{2}\not=1 p x=p(u)=\overline{u} u\in A^{\ast} \overline{u}^{2}\not=1 k \overline{u}^{2}-1\in k^{\ast} p(a)=\overline{u}^{2}-1 a\in  A^{\ast} u^{2}-1=a+m a\in  A^{\ast} m\in  M u^{2}-1\in A^{\ast} a (u^{2}-1)a^{-1}=1+m' m'=ma^{-1}\in M 1+m' u^{2}-1 1+m' A,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
68,In a von Neumann regular commutative ring with unity every finitely generated ideal is principal,In a von Neumann regular commutative ring with unity every finitely generated ideal is principal,,"Let $R$ be a commutative, von Neumann regular ring with unity. How to show that every finitely generated ideal in $R$ is principal? I can see, in view of mathematical induction, it suffices to show that any ideal generated by two elements of $R$ must be principal. Let $I=(a,b)$ be an ideal of $R.$ Since $I$ is commutative with unity, $I=\{xa+yb:x,y\in R\}.$ Also since $R$ is regular there exist $r,s\in R$ such that $a=ara=ra^2$ and $b=bsb=sb^2.$ However I cannot figureout which element would generate $I.$ Please help.","Let be a commutative, von Neumann regular ring with unity. How to show that every finitely generated ideal in is principal? I can see, in view of mathematical induction, it suffices to show that any ideal generated by two elements of must be principal. Let be an ideal of Since is commutative with unity, Also since is regular there exist such that and However I cannot figureout which element would generate Please help.","R R R I=(a,b) R. I I=\{xa+yb:x,y\in R\}. R r,s\in R a=ara=ra^2 b=bsb=sb^2. I.","['abstract-algebra', 'ring-theory']"
69,Intersection multiplicity does not go down after restriction to closed subvariety: proof using filtrations,Intersection multiplicity does not go down after restriction to closed subvariety: proof using filtrations,,"Let $S$ be a noetherian domain, and $f\in S$ neither a unit nor a zero divisor. Let $P$ be a minimal prime over $f$ , let $I\subset S$ be a prime ideal, suppose the image of $f$ in $S/I$ is neither a unit nor a zero-divisor, and let $Q$ be a minimal prime over $S/(I+f)$ containing $P$ . I'm looking to verify that $\operatorname{length}_{S_P} S_P/(f)\leq \operatorname{length}_{S_Q} S_Q/(I+f)$ . This is basically the statement that if $V\subset W$ are two varieties which both intersect a hypersurface $H$ properly, the intersection multiplicity of $V\cap H$ along any component $V'$ of $V\cap H$ is at least as large as the intersection multiplicity of $W\cap H$ along an irreducible component $W'$ of $W\cap H$ containing $V'$ . I'm looking to prove this before saying something about it in my lecture when covering material related to Hartshorne I.7, but I'm having trouble finishing the argument. The main tool that comes to mind is the fact that any finitely generated module over a noetherian ring $R$ has a finite filtration by submodules $M_i$ so that the subquotients $M_{i+1}/M_i$ are isomorphic to $R/\mathfrak{p}_i$ for $\mathfrak{p}_i$ a prime ideal of $R$ , and the number of times a minimal prime shows up is the length of the module over that minimal prime. Unfortunately, I am a little rusty in this area and I can't quite finish the argument about how the filtrations of $S/(f)$ and $S/(I+f)$ relate to each other. It seems clear that I should take a filtration of one and either push it forward or pull it back along the obvious map $S/(f)\to S/(I+f)$ and argue from there, but I have had no further success. One attempt: Localizing at $Q$ , we get a finite filtration $\{M_i\}$ of $S_Q/(f)$ with subquotients $(S/\mathfrak{p}_i)_Q$ for $P\subset \mathfrak{p}_i\subset Q$ . Under the quotient map $\alpha:S_Q/f\to S_Q/(I+f)$ , we have that $\{\alpha(M_i)\}$ form a filtration of $S_Q/(I+f)$ , and $M_{i+1}/M_i=S_Q/\mathfrak{p}_i$ surjects on to $\alpha(M_{i+1})/\alpha(M_i)$ . If this latter module is nonzero, we're done - it has length at least one, so $\operatorname{length}_{S_Q} S_Q/(I+f)$ is at least the number of terms in the filtration of $S_Q/(f)$ , which is at most the number of times $S_Q/P$ appears in that filtration. But I've been getting turned around in why this ought to be the case for a while, and I could use some help. I'm looking for some help filling in the details of this argument. Alternate methods and suggestions are also welcome!","Let be a noetherian domain, and neither a unit nor a zero divisor. Let be a minimal prime over , let be a prime ideal, suppose the image of in is neither a unit nor a zero-divisor, and let be a minimal prime over containing . I'm looking to verify that . This is basically the statement that if are two varieties which both intersect a hypersurface properly, the intersection multiplicity of along any component of is at least as large as the intersection multiplicity of along an irreducible component of containing . I'm looking to prove this before saying something about it in my lecture when covering material related to Hartshorne I.7, but I'm having trouble finishing the argument. The main tool that comes to mind is the fact that any finitely generated module over a noetherian ring has a finite filtration by submodules so that the subquotients are isomorphic to for a prime ideal of , and the number of times a minimal prime shows up is the length of the module over that minimal prime. Unfortunately, I am a little rusty in this area and I can't quite finish the argument about how the filtrations of and relate to each other. It seems clear that I should take a filtration of one and either push it forward or pull it back along the obvious map and argue from there, but I have had no further success. One attempt: Localizing at , we get a finite filtration of with subquotients for . Under the quotient map , we have that form a filtration of , and surjects on to . If this latter module is nonzero, we're done - it has length at least one, so is at least the number of terms in the filtration of , which is at most the number of times appears in that filtration. But I've been getting turned around in why this ought to be the case for a while, and I could use some help. I'm looking for some help filling in the details of this argument. Alternate methods and suggestions are also welcome!",S f\in S P f I\subset S f S/I Q S/(I+f) P \operatorname{length}_{S_P} S_P/(f)\leq \operatorname{length}_{S_Q} S_Q/(I+f) V\subset W H V\cap H V' V\cap H W\cap H W' W\cap H V' R M_i M_{i+1}/M_i R/\mathfrak{p}_i \mathfrak{p}_i R S/(f) S/(I+f) S/(f)\to S/(I+f) Q \{M_i\} S_Q/(f) (S/\mathfrak{p}_i)_Q P\subset \mathfrak{p}_i\subset Q \alpha:S_Q/f\to S_Q/(I+f) \{\alpha(M_i)\} S_Q/(I+f) M_{i+1}/M_i=S_Q/\mathfrak{p}_i \alpha(M_{i+1})/\alpha(M_i) \operatorname{length}_{S_Q} S_Q/(I+f) S_Q/(f) S_Q/P,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'intersection-theory', 'filtrations']"
70,Finding a primitive element in a field with 27 elements. [duplicate],Finding a primitive element in a field with 27 elements. [duplicate],,"This question already has answers here : Find generator of multiplicative group of $\mathbb{F}_{27}$ (2 answers) Closed 4 years ago . I am trying to construct a field with 27 elements, and find a primitive element in that field. I considered the irreducible polynomial $f(x)=x^3+2x+1$ over $\mathbb{Z}_3[x]$ . Then I considered $$\mathbb{Z}_3[x]/\langle f\rangle.$$ This is a field with $3^{\deg f}=3^3=27$ elements. I know that the unique elements of this field are given by $$\{a_0+a_1t+a_2t^2:a_i\in\mathbb{Z}_3\}$$ where $t=x+\langle f\rangle$ . Now my question is, what is an efficient way (for beginners) to find a primitive element of this field? One could argue that it suffices to find an element $u\in\mathbb{Z}_3[x]/\langle f\rangle$ with $\text{ord}(u)\neq 1,2,13$ , but finding such a $u$ is computationally tedious (at least to me). Any comments or advice appreciated.","This question already has answers here : Find generator of multiplicative group of $\mathbb{F}_{27}$ (2 answers) Closed 4 years ago . I am trying to construct a field with 27 elements, and find a primitive element in that field. I considered the irreducible polynomial over . Then I considered This is a field with elements. I know that the unique elements of this field are given by where . Now my question is, what is an efficient way (for beginners) to find a primitive element of this field? One could argue that it suffices to find an element with , but finding such a is computationally tedious (at least to me). Any comments or advice appreciated.","f(x)=x^3+2x+1 \mathbb{Z}_3[x] \mathbb{Z}_3[x]/\langle f\rangle. 3^{\deg f}=3^3=27 \{a_0+a_1t+a_2t^2:a_i\in\mathbb{Z}_3\} t=x+\langle f\rangle u\in\mathbb{Z}_3[x]/\langle f\rangle \text{ord}(u)\neq 1,2,13 u","['abstract-algebra', 'field-theory', 'finite-fields']"
71,Counting the number of particular subgroups of a finite abelian group,Counting the number of particular subgroups of a finite abelian group,,"I want to count subgroups $H$ isomorphic to $\mathbb{Z}/3\mathbb{Z}\times  \mathbb{Z}/3\mathbb{Z}$ in a group $G$ isomorphic to $\mathbb{Z}/3\mathbb{Z}\times\mathbb{Z}/9\mathbb{Z}\times\mathbb{Z}/81\mathbb{Z}$ My idea is that, I count the elements of order $3$ in $G$ , that are $3\cdot3\cdot3-1=26$ and they are the possible elements of the first $\mathbb{Z}/3\mathbb{Z}$ in $H$ , and then I count again the elements of order $3$ in $G$ but I remove two possible elements because the generators of $\mathbb{Z}/3\mathbb{Z}\times\{0\}$ and of the second $\{0\}\times\mathbb{Z}/3\mathbb{Z}$ have trivial intersection. So there should be $26\cdot24$ subgroups. Is this way to go right?","I want to count subgroups isomorphic to in a group isomorphic to My idea is that, I count the elements of order in , that are and they are the possible elements of the first in , and then I count again the elements of order in but I remove two possible elements because the generators of and of the second have trivial intersection. So there should be subgroups. Is this way to go right?",H \mathbb{Z}/3\mathbb{Z}\times  \mathbb{Z}/3\mathbb{Z} G \mathbb{Z}/3\mathbb{Z}\times\mathbb{Z}/9\mathbb{Z}\times\mathbb{Z}/81\mathbb{Z} 3 G 3\cdot3\cdot3-1=26 \mathbb{Z}/3\mathbb{Z} H 3 G \mathbb{Z}/3\mathbb{Z}\times\{0\} \{0\}\times\mathbb{Z}/3\mathbb{Z} 26\cdot24,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
72,"Does this ""reverse distributivity"" ever occur: $a \circ (b\times c) = (a \times b) \circ (a \times c)$?","Does this ""reverse distributivity"" ever occur: ?",a \circ (b\times c) = (a \times b) \circ (a \times c),"I was reading a book containing a typo to the effect that they defined the distributive property as: $$ a \circ (b\times c) = (a \times b) \circ (a \times c) \tag{*}\label{*} $$ which is wrong of course. I will call the property (*) ""reverse distributivity"" for now. It got me wondering: Are there any examples of structures with this ""reverse distributivity""? What can we say about such a structure? And are there names for these things? Some findings so far: If we assume the existence of neutral elements, then things quickly degenerate. Assume that $(M, \circ, 1_\circ, \times, 1_\times)$ is an algebraic structure with two binary operators satisfying (*), and where $1_\circ$ and $1_\times$ are neutral elements. Then we have: $$ 1_\circ  = (1_\circ \times 1_\times) \circ (1_\circ \times 1_\times) \stackrel{\eqref{*}} = 1_\circ \circ (1_\times \times 1_\times) = 1_\times $$ so the identity elements are in fact equal. Let $1 := 1_\times = 1_\circ$ . Then, for any $a,b\in M$ : $$ a \times b = 1 \circ (a \times b) \stackrel{\eqref{*}} = (1 \times a ) \circ (1 \times b) = a \circ b $$ so in fact the two compositions are the same. In this case (*) becomes $$ a \circ (b\circ c) = (a \circ b) \circ (a \circ c) $$ which seems to be known as self-distributivity and shows up in a number of places (e.g. group conjugation and logical implication). But if we want two (different) compositions that satisfy (*), then this shows that they at least cannot both have neutral elements. (If we only assume the existence of $1_\times$ , then we can show that $a \circ a = a \circ 1_\times$ for all $a$ ). I haven't gone much further than this.","I was reading a book containing a typo to the effect that they defined the distributive property as: which is wrong of course. I will call the property (*) ""reverse distributivity"" for now. It got me wondering: Are there any examples of structures with this ""reverse distributivity""? What can we say about such a structure? And are there names for these things? Some findings so far: If we assume the existence of neutral elements, then things quickly degenerate. Assume that is an algebraic structure with two binary operators satisfying (*), and where and are neutral elements. Then we have: so the identity elements are in fact equal. Let . Then, for any : so in fact the two compositions are the same. In this case (*) becomes which seems to be known as self-distributivity and shows up in a number of places (e.g. group conjugation and logical implication). But if we want two (different) compositions that satisfy (*), then this shows that they at least cannot both have neutral elements. (If we only assume the existence of , then we can show that for all ). I haven't gone much further than this.","
a \circ (b\times c) = (a \times b) \circ (a \times c) \tag{*}\label{*}
 (M, \circ, 1_\circ, \times, 1_\times) 1_\circ 1_\times 
1_\circ  = (1_\circ \times 1_\times) \circ (1_\circ \times 1_\times)
\stackrel{\eqref{*}} = 1_\circ \circ (1_\times \times 1_\times)
= 1_\times
 1 := 1_\times = 1_\circ a,b\in M 
a \times b = 1 \circ (a \times b) \stackrel{\eqref{*}} =
(1 \times a ) \circ (1 \times b) = a \circ b
 
a \circ (b\circ c) = (a \circ b) \circ (a \circ c)
 1_\times a \circ a = a \circ 1_\times a","['abstract-algebra', 'axioms', 'universal-algebra']"
73,Is the localization of intersection of modules equal to the intersection of appropriate localizations?,Is the localization of intersection of modules equal to the intersection of appropriate localizations?,,"Given a commutative unital ring $R$ , and a multiplicative subset $S\subseteq R$ , I know that for two $R$ -submodules $M_1,M_2$ of $M$ , we have: (I) $S^{-1}M_1 \cap S^{-1}M_2= S^{-1}(M_1\cap M_2)$ (II) $S^{-1}M_1+S^{-1}M_2=S^{-1}(M_1+M_2)$ My question is does this also hold for infinite intersections and sums. I am pretty sure that a sum of localizations is equal to the localization of the sum, simply because every element in a sum of modules can be discussed within a finite sum. I suspect that this is untrue for the intersection, but I could not think of a counter-example.","Given a commutative unital ring , and a multiplicative subset , I know that for two -submodules of , we have: (I) (II) My question is does this also hold for infinite intersections and sums. I am pretty sure that a sum of localizations is equal to the localization of the sum, simply because every element in a sum of modules can be discussed within a finite sum. I suspect that this is untrue for the intersection, but I could not think of a counter-example.","R S\subseteq R R M_1,M_2 M S^{-1}M_1 \cap S^{-1}M_2= S^{-1}(M_1\cap M_2) S^{-1}M_1+S^{-1}M_2=S^{-1}(M_1+M_2)","['abstract-algebra', 'commutative-algebra', 'modules', 'localization']"
74,What is the reason for commutative property during multiplication of real numbers,What is the reason for commutative property during multiplication of real numbers,,"This may seem like a really stupid question, but I am unable to rationalize with myself as to why the commutative property exists when multiplying 2 real number. Take for example: $2 * 5 = 10$ This actually means that if we add $2$ $5$ times we will get $10$ . But its kind of amazing when we can say for sure that if we add $5$ $2$ times we will also get $10$ . What is the reason for this property, I know I may be over-thinking this, but I can't understand intuitively why this happens. I know that $5$ actually ""contains"" $2$ but how does that guarantee commutative property ? For example matrix multiplication is not commutative, yet real number multiplication is. Am I just overthinking something simple? Need some clarity.","This may seem like a really stupid question, but I am unable to rationalize with myself as to why the commutative property exists when multiplying 2 real number. Take for example: This actually means that if we add times we will get . But its kind of amazing when we can say for sure that if we add times we will also get . What is the reason for this property, I know I may be over-thinking this, but I can't understand intuitively why this happens. I know that actually ""contains"" but how does that guarantee commutative property ? For example matrix multiplication is not commutative, yet real number multiplication is. Am I just overthinking something simple? Need some clarity.",2 * 5 = 10 2 5 10 5 2 10 5 2,"['abstract-algebra', 'logic']"
75,Showing that all elements of a Field Extension are transcendental,Showing that all elements of a Field Extension are transcendental,,"My question is whether my strategy for this proof is correct. I'll put the fact to be proven, my strategy, and why I am hesitant about my strategy. Thing to be proven: I want to show that if $E$ is an extension field of $F$ , and $\alpha \in E$ is transcendental over $F$ then every element of $F(\alpha)$ (field of quotients of $F[\alpha]$ ) is also transcendental over $F$ . My Proof Strategy: Assume $\beta \in F(\alpha)$ and $\beta$ is algebraic. Then $\exists p(x) \in F[x]$ such that $p(\beta) = 0$ . But since $\beta$ is a polynomial in $\alpha$ , that means that some polynomial in $\alpha$ with coefficients in $F$ is equal to $0$ so $\exists q(x) \in F[x]$ such that $q(\alpha) = 0$ . My Concerns: $\beta$ is in a field of quotients of $F[\alpha]$ , but this can be circumvented because in order for that to be $0$ we just need the ""numerator"" which is in $F[\alpha]$ to have a root. Showing rigorously that $p(\beta)$ is a polynomial in $\alpha$ seems like a pain. It makes sense, but doing a general solution seems hard. This makes me think that an easier solution might be to somehow employ theorems about the fact that $\langle p(x)\rangle$ must be a maximal ideal. So my question is, which route would yield an easier and more elegant solution?","My question is whether my strategy for this proof is correct. I'll put the fact to be proven, my strategy, and why I am hesitant about my strategy. Thing to be proven: I want to show that if is an extension field of , and is transcendental over then every element of (field of quotients of ) is also transcendental over . My Proof Strategy: Assume and is algebraic. Then such that . But since is a polynomial in , that means that some polynomial in with coefficients in is equal to so such that . My Concerns: is in a field of quotients of , but this can be circumvented because in order for that to be we just need the ""numerator"" which is in to have a root. Showing rigorously that is a polynomial in seems like a pain. It makes sense, but doing a general solution seems hard. This makes me think that an easier solution might be to somehow employ theorems about the fact that must be a maximal ideal. So my question is, which route would yield an easier and more elegant solution?",E F \alpha \in E F F(\alpha) F[\alpha] F \beta \in F(\alpha) \beta \exists p(x) \in F[x] p(\beta) = 0 \beta \alpha \alpha F 0 \exists q(x) \in F[x] q(\alpha) = 0 \beta F[\alpha] 0 F[\alpha] p(\beta) \alpha \langle p(x)\rangle,"['abstract-algebra', 'field-theory', 'extension-field']"
76,How can I compute the discriminant of the field $\mathbb{Q}(\sqrt[3]{28})$?,How can I compute the discriminant of the field ?,\mathbb{Q}(\sqrt[3]{28}),"$$\newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}} \newcommand{\al}{\alpha} \newcommand{\bcal}{\mathcal{B}} \newcommand{\qroot}{\sqrt[3]} \newcommand{\froot}{\sqrt[4]} $$ I have a problem which consist in 3 problems. I solve part 1 and 2, but I want to make sure it is correct. Also I dont know how to solve part 3. Anyone ? Thanks Consider the number field $E = \Q(\sqrt[3]{28})$ . Find $T_{E|\Q}(\alpha)$ and $N_{E|\Q}(\alpha)$ for every $\alpha \in E$ . Let $\mathcal{O}[\qroot{28}]$ be the set of all integral elements in $E$ . Show that if $\beta = \frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98})$ , then $$\beta \in \mathcal{O}[\qroot{28}]$$ Consider the set $$ \mathcal{B} = \{\qroot{28},\qroot{98},\frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98}) \}$$ Assuming that $\mathcal{B}$ is an integral basis for $\mathcal{O}(\qroot{28})$ , Calculate the field discriminant of the number field $E$ . $\textbf{Solution for 1}$ : One ordered basis for the field extension is $\bcal = \{1,\qroot{28},\qroot{98}\}$ . Any element $\al \in E$ is of the form $$ \al = a + b\qroot{28} + c\qroot{98}$$ Multiplying $\al$ for each basis elements we obtain $$ \al\cdot1 = a + b\qroot{28} + c\qroot{98}$$ $$ \al\cdot\qroot{28} = a\qroot{28} + 2b\qroot{98} + 14c$$ $$ \al\cdot\qroot{98} = a\qroot{98} + 14b + 7c\qroot{28}$$ Hence, $$[\al]_\bcal = \begin{bmatrix} a & 14c & 14b \\ b & a & 7c \\ c & 2b & a \end{bmatrix}$$ and we obtain that $$T_{E|\Q}(\alpha) = 3a$$ $$N_{E|\Q}(\alpha) = Det([\al]_{\bcal}) =a^3 - 42 abc + 28 b^3 + 98 c^3\hspace{5pt} \square $$ $\textbf{Solution for 2:}$ Consider $\beta = \frac{1}{3}(1 + 7 \qroot{28} + 2 \qroot{98})$ . Let $\gamma = \beta^3 - \beta^2$ . Then we must find $c \in \mathbb{Z}$ such that $\gamma + c\beta \in \mathbb{Z}$ . We obtain that $$\gamma = \frac{1}{3}(1154 + 455\qroot{28} +130\qroot{98})$$ Hence, for $c = -65$ we obtain $$\gamma + c\beta =  \frac{1154}{3} - \frac{65}{3} = 363$$ . $$ \therefore \beta^3 - \beta^2 - 65\beta - 363 = 0$$ $$ \therefore \beta \in \mathcal{O}[\qroot{28}] \hspace{5pt} \square$$","I have a problem which consist in 3 problems. I solve part 1 and 2, but I want to make sure it is correct. Also I dont know how to solve part 3. Anyone ? Thanks Consider the number field . Find and for every . Let be the set of all integral elements in . Show that if , then Consider the set Assuming that is an integral basis for , Calculate the field discriminant of the number field . : One ordered basis for the field extension is . Any element is of the form Multiplying for each basis elements we obtain Hence, and we obtain that Consider . Let . Then we must find such that . We obtain that Hence, for we obtain .","\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\al}{\alpha}
\newcommand{\bcal}{\mathcal{B}}
\newcommand{\qroot}{\sqrt[3]}
\newcommand{\froot}{\sqrt[4]}
 E = \Q(\sqrt[3]{28}) T_{E|\Q}(\alpha) N_{E|\Q}(\alpha) \alpha \in E \mathcal{O}[\qroot{28}] E \beta = \frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98}) \beta \in \mathcal{O}[\qroot{28}]  \mathcal{B} = \{\qroot{28},\qroot{98},\frac{1}{3}(1 + 7\qroot{28} + 2\qroot{98}) \} \mathcal{B} \mathcal{O}(\qroot{28}) E \textbf{Solution for 1} \bcal = \{1,\qroot{28},\qroot{98}\} \al \in E  \al = a + b\qroot{28} + c\qroot{98} \al  \al\cdot1 = a + b\qroot{28} + c\qroot{98}  \al\cdot\qroot{28} = a\qroot{28} + 2b\qroot{98} + 14c  \al\cdot\qroot{98} = a\qroot{98} + 14b + 7c\qroot{28} [\al]_\bcal = \begin{bmatrix} a & 14c & 14b \\ b & a & 7c \\ c & 2b & a \end{bmatrix} T_{E|\Q}(\alpha) = 3a N_{E|\Q}(\alpha) = Det([\al]_{\bcal}) =a^3 - 42 abc + 28 b^3 + 98 c^3\hspace{5pt} \square  \textbf{Solution for 2:} \beta = \frac{1}{3}(1 + 7 \qroot{28} + 2 \qroot{98}) \gamma = \beta^3 - \beta^2 c \in \mathbb{Z} \gamma + c\beta \in \mathbb{Z} \gamma = \frac{1}{3}(1154 + 455\qroot{28} +130\qroot{98}) c = -65 \gamma + c\beta =  \frac{1154}{3} - \frac{65}{3} = 363  \therefore \beta^3 - \beta^2 - 65\beta - 363 = 0  \therefore \beta \in \mathcal{O}[\qroot{28}] \hspace{5pt} \square","['abstract-algebra', 'field-theory', 'extension-field', 'integral-extensions']"
77,Proving there are an infinite number of irreducible monic polynomials over a field,Proving there are an infinite number of irreducible monic polynomials over a field,,"I was given this problem, and the solution I came up with is eerily similar to the classic Euclid proof on infinitude of primes, so I'm feeling like I must be wrong somewhere. Problem : Given $k$ any field, show that there are infinitely many irreducible monic polynomials in $k[X]$. My attempt : assume there are finitely many irreducible monic polynomials $f_1, f_2, ..., f_n$. Then consider $p = 1 + f_1 f_2 ... f_n$, which is monic since all the $f_i$ are monic and $1$ is constant. By assumption, $p$ must be reducible, since it is not equal to any of the $f_i$. So there must exist some $g \in k[X]$ such that $g$ divides $f_1 f_2 ... f_n$ -- a contradiction since the $f_i$ are assumed to be irreducible. I think my mistake was in assuming $p$ is not equal to any of the $f_i$ -- I suppose in some finite field perhaps there is a case where that could be... I'm trying to work out in my head whether that's possible. Am I on the right track or just totally off? TIA! Jason EDIT: should be that one of these $f_i$ must divide $p$, since $p$ is reducible, but that's not possible because then they would have to divide 1.","I was given this problem, and the solution I came up with is eerily similar to the classic Euclid proof on infinitude of primes, so I'm feeling like I must be wrong somewhere. Problem : Given $k$ any field, show that there are infinitely many irreducible monic polynomials in $k[X]$. My attempt : assume there are finitely many irreducible monic polynomials $f_1, f_2, ..., f_n$. Then consider $p = 1 + f_1 f_2 ... f_n$, which is monic since all the $f_i$ are monic and $1$ is constant. By assumption, $p$ must be reducible, since it is not equal to any of the $f_i$. So there must exist some $g \in k[X]$ such that $g$ divides $f_1 f_2 ... f_n$ -- a contradiction since the $f_i$ are assumed to be irreducible. I think my mistake was in assuming $p$ is not equal to any of the $f_i$ -- I suppose in some finite field perhaps there is a case where that could be... I'm trying to work out in my head whether that's possible. Am I on the right track or just totally off? TIA! Jason EDIT: should be that one of these $f_i$ must divide $p$, since $p$ is reducible, but that's not possible because then they would have to divide 1.",,"['abstract-algebra', 'algebraic-geometry', 'ring-theory']"
78,Structures with $x*(y*z) = y*(x*z)$,Structures with,x*(y*z) = y*(x*z),"In reading http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=182143561104878BDABB72258DA254D0?doi=10.1.1.18.2521&rep=rep1&type=pdf , they mentioned an interesting relation -- they had a magma $(X,*)$ with the property $x*(y*z) = y*(x*z)$. Their specific example was to start with a set $S$ , and build your structure on its power set $X=P(S)$; then take the vector space $\mathbb{R}^{P(S)}$. Their operation acted on vectors in this space: for two vectors $x$ and $y$ , the element of their product indexed by $I \subseteq S$ is given by $(x*y)(I) = \sum_{K \subseteq S} x(K)y(I\cup K)$. I think this is an interesting relation, because it's so ""close"" to several other nice properties. If you had any right identity, then $x*y = x*(y*e) = y*(x*e) = y*x$, so the structure is commutative. If you had commutativity, then $ x*(y*z) = x*(z*y) = z*(x*y) = (x*y)*z $, so you'd have associativity. Thus just a right identity is enough to imply that you're a full-on commutative monoid. There is a weak reverse, that associativity implies $(x*y)*z = (y*x)*z$, which is like a weak version of commutativity: $x*y\simeq y*x$ in the sense that are equivalent under maps $(- * z)$ for all $z$. So I want to know what these structures look like when you don't have a right identity. Their example does have a left identity, the vector I'll call $1_0$, with $1_0(\emptyset)=1$ and all other elements of the vector equal to zero. If we restrict our structure to just the vectors $v$ such that $v(\emptyset)\neq 0$, then we are still closed under $*$, and get a notion of inverse: for any vector $v$, we can define the vector $v^{-1}$ by $v^{-1}(\emptyset) = \frac{1}{v(\emptyset)}$ and $v^{-1}(K)=0$ for all other $K$. Then $v*v^{-1} = 1_0$, so in this sense we get an inverse to the left-identity. So we can have a left-identity and an inverse, without the right-inverse/associativity/commutativity properties. I could only think of three other examples of such a structure. One is to take any commutative semigroup. It doesn't have an identity necessarily, but it still has the commutativity and associativity. The second is based on Boolean logic: in some set of axioms and with some set of statements $X$, we can determine $*$ as implication $\to$ in the sense of ""$X\to Y$ means $Y$ can be proven from $X$ in this system"". Then $x\to(y\to z)$ is equivalent to the statement $y\to(x\to z)$, thus ""equal"" in this structure. This still has left-identity given by TRUE, as $TRUE\to X$ is equivalent to $X$. The third is to take a semilattice $(X,\wedge,\le)$, together with a negation map $\neg$ such that $x\le y \implies \neg y\le \neg x$. Then you can get the type of structure described above by taking $x*y = \neg x \wedge y$: We know that $x*(y*z)$ is the greatest lower bound of $\neg x$, $\neg y$, and $z$. As a concrete example, we can take $X = \mathbb{Z}\setminus \{0\}$ and $\wedge$ as $\max$: then we don't have any absorbing elements or identities. I suspect this might be related to the second example above through Heyting algebras somehow. Are there any names for such structures? Are there any classification theorems for them? They seem so ""close"" to such nice structures, I really feel that there should be some sort of results! :) Thank you for any information!","In reading http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=182143561104878BDABB72258DA254D0?doi=10.1.1.18.2521&rep=rep1&type=pdf , they mentioned an interesting relation -- they had a magma $(X,*)$ with the property $x*(y*z) = y*(x*z)$. Their specific example was to start with a set $S$ , and build your structure on its power set $X=P(S)$; then take the vector space $\mathbb{R}^{P(S)}$. Their operation acted on vectors in this space: for two vectors $x$ and $y$ , the element of their product indexed by $I \subseteq S$ is given by $(x*y)(I) = \sum_{K \subseteq S} x(K)y(I\cup K)$. I think this is an interesting relation, because it's so ""close"" to several other nice properties. If you had any right identity, then $x*y = x*(y*e) = y*(x*e) = y*x$, so the structure is commutative. If you had commutativity, then $ x*(y*z) = x*(z*y) = z*(x*y) = (x*y)*z $, so you'd have associativity. Thus just a right identity is enough to imply that you're a full-on commutative monoid. There is a weak reverse, that associativity implies $(x*y)*z = (y*x)*z$, which is like a weak version of commutativity: $x*y\simeq y*x$ in the sense that are equivalent under maps $(- * z)$ for all $z$. So I want to know what these structures look like when you don't have a right identity. Their example does have a left identity, the vector I'll call $1_0$, with $1_0(\emptyset)=1$ and all other elements of the vector equal to zero. If we restrict our structure to just the vectors $v$ such that $v(\emptyset)\neq 0$, then we are still closed under $*$, and get a notion of inverse: for any vector $v$, we can define the vector $v^{-1}$ by $v^{-1}(\emptyset) = \frac{1}{v(\emptyset)}$ and $v^{-1}(K)=0$ for all other $K$. Then $v*v^{-1} = 1_0$, so in this sense we get an inverse to the left-identity. So we can have a left-identity and an inverse, without the right-inverse/associativity/commutativity properties. I could only think of three other examples of such a structure. One is to take any commutative semigroup. It doesn't have an identity necessarily, but it still has the commutativity and associativity. The second is based on Boolean logic: in some set of axioms and with some set of statements $X$, we can determine $*$ as implication $\to$ in the sense of ""$X\to Y$ means $Y$ can be proven from $X$ in this system"". Then $x\to(y\to z)$ is equivalent to the statement $y\to(x\to z)$, thus ""equal"" in this structure. This still has left-identity given by TRUE, as $TRUE\to X$ is equivalent to $X$. The third is to take a semilattice $(X,\wedge,\le)$, together with a negation map $\neg$ such that $x\le y \implies \neg y\le \neg x$. Then you can get the type of structure described above by taking $x*y = \neg x \wedge y$: We know that $x*(y*z)$ is the greatest lower bound of $\neg x$, $\neg y$, and $z$. As a concrete example, we can take $X = \mathbb{Z}\setminus \{0\}$ and $\wedge$ as $\max$: then we don't have any absorbing elements or identities. I suspect this might be related to the second example above through Heyting algebras somehow. Are there any names for such structures? Are there any classification theorems for them? They seem so ""close"" to such nice structures, I really feel that there should be some sort of results! :) Thank you for any information!",,"['abstract-algebra', 'lattice-orders', 'binary-operations', 'magma']"
79,$p$-groups in which the centralizers are normal,-groups in which the centralizers are normal,p,"Let $|G|=p^n$ and $p$ a prime and let $|G:C_G(x)|\leq p$ for all $x\in G$ . Prove: $(a)~~~~~C_G(x)\trianglelefteq G$ for all $x\in G;$ $(b)~~~~~G’\leq Z(G);$ ${\color{red}{{(c)~~~~~|G’|\leq p}.}}$ Edit: I’m now only confused about $\bf (c)$ . For $\bf(c)$ , the nice answers below are not complete. I was told that $\bf (c)$ has something to do with this paper ( or here Knoche,H.G.: Überden Frobeniusschen Klassenbegriff in nilpotenten Gruppen , Math. Z. 55 (1951), 71–83. ), but reading the whole paper would be difficult for me, since I’m no German speaker; more unfortunately, I have not even been able to figure out which part I need to read . How does that paper help? I’m quite a beginner in group-theory, and I would be grateful if I could receive your instruction.","Let and a prime and let for all . Prove: for all Edit: I’m now only confused about . For , the nice answers below are not complete. I was told that has something to do with this paper ( or here Knoche,H.G.: Überden Frobeniusschen Klassenbegriff in nilpotenten Gruppen , Math. Z. 55 (1951), 71–83. ), but reading the whole paper would be difficult for me, since I’m no German speaker; more unfortunately, I have not even been able to figure out which part I need to read . How does that paper help? I’m quite a beginner in group-theory, and I would be grateful if I could receive your instruction.",|G|=p^n p |G:C_G(x)|\leq p x\in G (a)~~~~~C_G(x)\trianglelefteq G x\in G; (b)~~~~~G’\leq Z(G); {\color{red}{{(c)~~~~~|G’|\leq p}.}} \bf (c) \bf(c) \bf (c),['abstract-algebra']
80,A question regarding a central idempotent in a ring $R$,A question regarding a central idempotent in a ring,R,"I am trying to solve the problem: Question: In a ring $R$ with identity, if every idempotent is central, then prove that for $a, b \in R$, $$ab =1 \implies ba=1$$ I have done in the following manner: $$ab=1\\ \implies b(ab)=b\\ \implies (ba)b-b=0\\ \implies (ba-1)b=0$$ Case 1: If $R$ contains no divisor of Zero then as $b \ne 0$ ,we get $$ba-1=0 \implies ba=1$$ Case 2: If $R$ contains divisor of Zeros then we may have $$ba-1\ne0\\ \implies ba\ne1\\ \implies (ab)a\ne a$$ But $ab=1$ hence $a\ne a$, an absurd condition. So $ba-1=0$ or $ba=1$. I think I have solved the problem but I haven't used the given conditions ""Every idempotent is central"". So I think there is something wrong which I have done but can't find out! Please rectify my mistake if I am wrong and provide any hint to solve it.","I am trying to solve the problem: Question: In a ring $R$ with identity, if every idempotent is central, then prove that for $a, b \in R$, $$ab =1 \implies ba=1$$ I have done in the following manner: $$ab=1\\ \implies b(ab)=b\\ \implies (ba)b-b=0\\ \implies (ba-1)b=0$$ Case 1: If $R$ contains no divisor of Zero then as $b \ne 0$ ,we get $$ba-1=0 \implies ba=1$$ Case 2: If $R$ contains divisor of Zeros then we may have $$ba-1\ne0\\ \implies ba\ne1\\ \implies (ab)a\ne a$$ But $ab=1$ hence $a\ne a$, an absurd condition. So $ba-1=0$ or $ba=1$. I think I have solved the problem but I haven't used the given conditions ""Every idempotent is central"". So I think there is something wrong which I have done but can't find out! Please rectify my mistake if I am wrong and provide any hint to solve it.",,"['abstract-algebra', 'ring-theory']"
81,Why does an integral extension over a ring has the same Krull dimension as the ring?,Why does an integral extension over a ring has the same Krull dimension as the ring?,,"The Krull dimension of a ring R is defined as the supremum of the lengths of chains of prime ideals contained in R. I heard that an integral extension over a ring R has the same Krull dimension as R, however, I don't really see why this is true.","The Krull dimension of a ring R is defined as the supremum of the lengths of chains of prime ideals contained in R. I heard that an integral extension over a ring R has the same Krull dimension as R, however, I don't really see why this is true.",,"['abstract-algebra', 'commutative-algebra', 'integral-extensions']"
82,Local ring with finite maximal ideal is finite,Local ring with finite maximal ideal is finite,,"Let $(R,  m)$ be a commutative local ring which is not a field such that $m$ is finite. Then is it true that $R$ is finite ? I can see that $R$ has finitely many ideals and all proper ideals are finite; so in particular $R$ is Artinian. Moreover $m=R\setminus U(R)$ is finite where $U(R)$ denotes the group of units of $R$ . To show $R$ is finite it would be enough to show either $U(R)$ is finite or that $R/m$ is finite. But I am unable to conclude either. Is the claim at all true ? Please help. Thanks in advance.","Let $(R,  m)$ be a commutative local ring which is not a field such that $m$ is finite. Then is it true that $R$ is finite ? I can see that $R$ has finitely many ideals and all proper ideals are finite; so in particular $R$ is Artinian. Moreover $m=R\setminus U(R)$ is finite where $U(R)$ denotes the group of units of $R$ . To show $R$ is finite it would be enough to show either $U(R)$ is finite or that $R/m$ is finite. But I am unable to conclude either. Is the claim at all true ? Please help. Thanks in advance.",,"['abstract-algebra', 'commutative-algebra']"
83,Galois group after base change,Galois group after base change,,Suppose that $f$ is a polynomial with coefficients in the function field $\mathbb Q(t)$ such that $f$ is irreducible over $\mathbb C(t)$. Is the Galois group of $f$ over $\mathbb C(t)$ isomorphic to its Galois group over $\mathbb Q(t)$?,Suppose that $f$ is a polynomial with coefficients in the function field $\mathbb Q(t)$ such that $f$ is irreducible over $\mathbb C(t)$. Is the Galois group of $f$ over $\mathbb C(t)$ isomorphic to its Galois group over $\mathbb Q(t)$?,,"['abstract-algebra', 'field-theory', 'galois-theory']"
84,Adjunctions are Kan Extensions.,Adjunctions are Kan Extensions.,,"I have been trying to understand the statement of the title but seems that I am getting stuck on something in the very last of its proof. To start with, assume that we have the adjunction $\mathsf{C} \overset{\mathcal{F}} {\underset{\mathcal{G}}\rightleftarrows} \mathsf{D}$, $\mathcal{F} \dashv \mathcal{G}$. Our statement, is that we have the following diagram \begin{array}{cc} \,\,\,\,\,\,\,\mathsf{C} \\ \\ \mathcal{F} \downarrow & \,\,\,\,\,\,\,\searrow \,{id_{\mathsf{C}}} \\ \\ \mathsf{D} & \xrightarrow{\mathcal{G} \cong Lan_{\mathcal{F}}id_{\mathsf{C}}}  & \mathsf{C}, \end{array} In other words, that $\mathcal{G}$, is the left Kan Extension of the identity functor $id_{\mathsf{C}}$ along $\mathcal{F}$. In order to prove that, we have to prove that $\mathcal{G}$, fulfils the universality of left Kan extensions. Where is my problem: Apparently, due to the adjunction we can define the natural transformation $ \eta : id_{\mathsf{C}} \rightarrow \mathcal{G \circ F}$, to be the unit of the adjunction. Then we assume that another functor $\mathcal{H}: \mathsf{D} \rightarrow \mathsf{C}$ along with a natural transformation $\gamma : id_{\mathsf{C}} \rightarrow \mathcal{H \circ F}$, exists and then we have to find a unique natural transformation $\delta : \mathcal{G} \rightarrow \mathcal{H}$, such that $ \delta_{\mathcal{F}} \circ \eta_{\mathcal{F}}= \gamma_{\mathcal{F}}.$ However, to find out $\delta$, it's kind of simple, since due to the adjunction again, we have the following adjunction too $ \mathsf{C}^{\mathsf{C}} \overset{\mathcal{G}^{*}} {\underset{\mathcal{F}^{*}} \leftrightarrows} \mathsf{C}^{\mathsf{D}}$. Therefore the uniqueness of transformation follows. What I cannot prove is why we end up having the composition $ \delta_{\mathcal{F}} \circ \eta_{\mathcal{F}}= \gamma_{\mathcal{F}}$. Could you please help me out? Thank you.","I have been trying to understand the statement of the title but seems that I am getting stuck on something in the very last of its proof. To start with, assume that we have the adjunction $\mathsf{C} \overset{\mathcal{F}} {\underset{\mathcal{G}}\rightleftarrows} \mathsf{D}$, $\mathcal{F} \dashv \mathcal{G}$. Our statement, is that we have the following diagram \begin{array}{cc} \,\,\,\,\,\,\,\mathsf{C} \\ \\ \mathcal{F} \downarrow & \,\,\,\,\,\,\,\searrow \,{id_{\mathsf{C}}} \\ \\ \mathsf{D} & \xrightarrow{\mathcal{G} \cong Lan_{\mathcal{F}}id_{\mathsf{C}}}  & \mathsf{C}, \end{array} In other words, that $\mathcal{G}$, is the left Kan Extension of the identity functor $id_{\mathsf{C}}$ along $\mathcal{F}$. In order to prove that, we have to prove that $\mathcal{G}$, fulfils the universality of left Kan extensions. Where is my problem: Apparently, due to the adjunction we can define the natural transformation $ \eta : id_{\mathsf{C}} \rightarrow \mathcal{G \circ F}$, to be the unit of the adjunction. Then we assume that another functor $\mathcal{H}: \mathsf{D} \rightarrow \mathsf{C}$ along with a natural transformation $\gamma : id_{\mathsf{C}} \rightarrow \mathcal{H \circ F}$, exists and then we have to find a unique natural transformation $\delta : \mathcal{G} \rightarrow \mathcal{H}$, such that $ \delta_{\mathcal{F}} \circ \eta_{\mathcal{F}}= \gamma_{\mathcal{F}}.$ However, to find out $\delta$, it's kind of simple, since due to the adjunction again, we have the following adjunction too $ \mathsf{C}^{\mathsf{C}} \overset{\mathcal{G}^{*}} {\underset{\mathcal{F}^{*}} \leftrightarrows} \mathsf{C}^{\mathsf{D}}$. Therefore the uniqueness of transformation follows. What I cannot prove is why we end up having the composition $ \delta_{\mathcal{F}} \circ \eta_{\mathcal{F}}= \gamma_{\mathcal{F}}$. Could you please help me out? Thank you.",,['abstract-algebra']
85,Algebraic over $\Bbb Q$,Algebraic over,\Bbb Q,"Hi I was wondering if anyone could help me with the following problem I'm trying to solve it is below. Let $\alpha=\sqrt[4]{20}$ , then put $F=\{a_0+a_1\alpha+a_2\alpha^2+a_3\alpha^3:a_0,a_1,a_2,a_3\in \Bbb Q\}$ . The questions are: (i) Prove that $F=\Bbb Q(\alpha)$ and (ii) evaluate $(1+\alpha^3)^{-1}$ as an element of $F$ and verify your answer. My attempt is as follows (i) $\alpha=\sqrt[4]{20}$ $=>\alpha^4-20=0$ Now let $m(x)=x^4-20$ then $m(x)\in\Bbb Q[x]$ and $m(x)$ is monic Now $m(\alpha)=\alpha^4-20=0$ thus $m(x)$ is the minimum polynomial of $\alpha$ over $\Bbb Q$ $<=> m(x)$ is irreducible in $\Bbb Q[x]$ Now $m(x)\in\Bbb Z[x]$ and is irreducible in $\Bbb Z[x]$ by E.I.T using prime $5$ , and since $m(x)$ is irred in $\Bbb Z[x]$ this implies $m(x)$ is irreducible in $\Bbb Q[x]$ So $x^4-20$ is the min poly of $\alpha$ over $\Bbb Q$ since $\alpha$ is algebraic over $\Bbb Q$ thus $[\Bbb Q(\alpha):\Bbb Q]=deg(m(x))=4$ . Thus $1,\alpha,\alpha^2,\alpha^3$ is a basis for $\Bbb Q(\alpha)$ over $\Bbb Q$ Thus $\Bbb Q(\alpha)=a_0+a_1(\alpha)+a_2(\alpha^2)+a_3(\alpha^3):a_0,a_1,a_2,a_3\in \Bbb Q=F$ as required This is the end of my solution for part (i) I can't see anywhere I'm going wrong with this part but I put it on here in case I have made an error somewhere? part (ii) Evaluate $(1+\alpha^3)^{-1}$ Put $g(x)=1+x^3)$ then I need to show that the $gcd(m(x),g(x))=1$ as I need to find an $R(x),S(x):1=R(x)(1+x^3)+S(x)(x^4-20)$ this is as far as I have got, I dont know where to go from here so any help would be highly appreciated","Hi I was wondering if anyone could help me with the following problem I'm trying to solve it is below. Let , then put . The questions are: (i) Prove that and (ii) evaluate as an element of and verify your answer. My attempt is as follows (i) Now let then and is monic Now thus is the minimum polynomial of over is irreducible in Now and is irreducible in by E.I.T using prime , and since is irred in this implies is irreducible in So is the min poly of over since is algebraic over thus . Thus is a basis for over Thus as required This is the end of my solution for part (i) I can't see anywhere I'm going wrong with this part but I put it on here in case I have made an error somewhere? part (ii) Evaluate Put then I need to show that the as I need to find an this is as far as I have got, I dont know where to go from here so any help would be highly appreciated","\alpha=\sqrt[4]{20} F=\{a_0+a_1\alpha+a_2\alpha^2+a_3\alpha^3:a_0,a_1,a_2,a_3\in \Bbb Q\} F=\Bbb Q(\alpha) (1+\alpha^3)^{-1} F \alpha=\sqrt[4]{20} =>\alpha^4-20=0 m(x)=x^4-20 m(x)\in\Bbb Q[x] m(x) m(\alpha)=\alpha^4-20=0 m(x) \alpha \Bbb Q <=> m(x) \Bbb Q[x] m(x)\in\Bbb Z[x] \Bbb Z[x] 5 m(x) \Bbb Z[x] m(x) \Bbb Q[x] x^4-20 \alpha \Bbb Q \alpha \Bbb Q [\Bbb Q(\alpha):\Bbb Q]=deg(m(x))=4 1,\alpha,\alpha^2,\alpha^3 \Bbb Q(\alpha) \Bbb Q \Bbb Q(\alpha)=a_0+a_1(\alpha)+a_2(\alpha^2)+a_3(\alpha^3):a_0,a_1,a_2,a_3\in \Bbb Q=F (1+\alpha^3)^{-1} g(x)=1+x^3) gcd(m(x),g(x))=1 R(x),S(x):1=R(x)(1+x^3)+S(x)(x^4-20)",['abstract-algebra']
86,Understanding an elementary proof from Dummit and Foote,Understanding an elementary proof from Dummit and Foote,,"The following proof is from Dummit and Foote's book, $Abstract$ $Algebra$. The proof is well structured and makes sense, save two minor parts. When they say $G/K $ is isomorphic to a subgroup of $S_p$, why is the justification the $1^{st}$ isomorphism theorem and not Cayley's Theorem? Second, why does $k=1\implies H=K$?","The following proof is from Dummit and Foote's book, $Abstract$ $Algebra$. The proof is well structured and makes sense, save two minor parts. When they say $G/K $ is isomorphic to a subgroup of $S_p$, why is the justification the $1^{st}$ isomorphism theorem and not Cayley's Theorem? Second, why does $k=1\implies H=K$?",,"['abstract-algebra', 'group-theory']"
87,"Let $R$ be an integral domain and $I,J$ be ideals such that $IJ$ is a principal ideal. Then $I$ is finitely generated?",Let  be an integral domain and  be ideals such that  is a principal ideal. Then  is finitely generated?,"R I,J IJ I","Let $R$ be an integral domain and $I,J$ be ideals such that $IJ$ is a principal ideal. Then is it true that $I$ is finitely generated ? I was thinking like if $IJ=(a)$ , where $a=\sum_{i=1}^{k} x_iy_i$ , $x_i\in I , y_i \in J$ , then we might have $I=(x_1,...,x_k)$ , but I am not sure and I cannot proceed further . Please help , Thanks in advance","Let be an integral domain and be ideals such that is a principal ideal. Then is it true that is finitely generated ? I was thinking like if , where , , then we might have , but I am not sure and I cannot proceed further . Please help , Thanks in advance","R I,J IJ I IJ=(a) a=\sum_{i=1}^{k} x_iy_i x_i\in I , y_i \in J I=(x_1,...,x_k)","['abstract-algebra', 'ring-theory']"
88,Classification of finite minimal non-cyclic group,Classification of finite minimal non-cyclic group,,I am looking for a complete classification of minimal finite non-cyclic groups. Is there any paper or book?,I am looking for a complete classification of minimal finite non-cyclic groups. Is there any paper or book?,,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups']"
89,Show that the permutation $(1 \space 2 \space 3)$ can not be a cube of any element of $S_n.$,Show that the permutation  can not be a cube of any element of,(1 \space 2 \space 3) S_n.,"Here is my try: If there exists $a \in S_n$ such that $a^3=(1 \space 2 \space  3)$, then $a^9=e$ where $e$ is identity in $S_n$. Then $o(a)=9$. I don't know how to proceed further. Can anyone provide me a hint? Thanks.","Here is my try: If there exists $a \in S_n$ such that $a^3=(1 \space 2 \space  3)$, then $a^9=e$ where $e$ is identity in $S_n$. Then $o(a)=9$. I don't know how to proceed further. Can anyone provide me a hint? Thanks.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'permutations', 'symmetric-groups']"
90,Nullstellensatz for non-algebraically closed fields,Nullstellensatz for non-algebraically closed fields,,"I'm trying to prove that the Nullstellensatz holds for non algebraically closed fields, when the variety is taken over the algebraic closure. Let $R=K[x_1,...,x_n]$ and $\overline{K}$ the algebraic closure of $K$. I was able to prove that $\sqrt{I}\subseteq \mathcal{I}_R(\mathcal{V}_{\overline{K}^n}(I))$ for any ideal $I$. I'm struggling a bit with the other direction. My attempt goes as follows: It is clear that given any ideal $J$, $V_{K^n}(J)\subseteq  \mathcal{V}_{\overline{K}^n}(J)$. Applying $\mathcal{I}_{\overline{K}^n}$ reverses the order, so we have: $$\mathcal{I}_{\overline{K}^n}(V_{K^n}(J))\supseteq  \mathcal{I}_{\overline{K}^n}(\mathcal{V}_{\overline{K}^n}(J))=\sqrt{I}$$ where the equality is just the normal Nullstellensatz. I don't know if this idea seems fruitful since I haven't been able to get the reverse inclusion. Any ideas on how to show this direction would he highly appreciated.","I'm trying to prove that the Nullstellensatz holds for non algebraically closed fields, when the variety is taken over the algebraic closure. Let $R=K[x_1,...,x_n]$ and $\overline{K}$ the algebraic closure of $K$. I was able to prove that $\sqrt{I}\subseteq \mathcal{I}_R(\mathcal{V}_{\overline{K}^n}(I))$ for any ideal $I$. I'm struggling a bit with the other direction. My attempt goes as follows: It is clear that given any ideal $J$, $V_{K^n}(J)\subseteq  \mathcal{V}_{\overline{K}^n}(J)$. Applying $\mathcal{I}_{\overline{K}^n}$ reverses the order, so we have: $$\mathcal{I}_{\overline{K}^n}(V_{K^n}(J))\supseteq  \mathcal{I}_{\overline{K}^n}(\mathcal{V}_{\overline{K}^n}(J))=\sqrt{I}$$ where the equality is just the normal Nullstellensatz. I don't know if this idea seems fruitful since I haven't been able to get the reverse inclusion. Any ideas on how to show this direction would he highly appreciated.",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
91,Finding an explicit isomorphism from $\mathbb{Z}^{4}/H$ to $\mathbb{Z} \oplus \mathbb{Z}/18\mathbb{Z}$,Finding an explicit isomorphism from  to,\mathbb{Z}^{4}/H \mathbb{Z} \oplus \mathbb{Z}/18\mathbb{Z},"There was a past qualifying exam problem, I was having trouble with, it is stated below as follows: In the group $G= \mathbb{Z} \times \mathbb{Z}\times \mathbb{Z}\times \mathbb{Z}=\mathbb{Z}^{4}$ , let $H$ be the subgroup generated by $[0,0,3,1], [0,6,0,0], [0,1,0,1]$ . Find an explicit isomorphism between $G/H$ and a product of cyclic groups. I truly do not fully understand how to define such a map on $G/H$ . I have given some thought to this. We have that $H$ consists of integer combinations of the generators above, that is, if $\xi \in H$ , then $\xi=a[0,0,3,1]+b[0,6,0,0]+c[0,1,0,1]$ . In particular, we have that $\xi$ is in the image of a module homomorphism $\mathbb{Z}^{3} \rightarrow \mathbb{Z}^{4}$ whose matrix with respect to a standard basis $(e_{1},e_{2}, e_{3})$ and $(f_{1}, f_{2}, f_{3}, f_{4})$ for $\mathbb{Z}^{3}$ and $\mathbb{Z}^{4}$ respectively, will be $$A=\begin{bmatrix} 0&0&0\\ 0&6&1\\ 3&0&0\\ 1&0&1\\ \end{bmatrix}.$$ We can perform row operations by multiplying on the left by the matrix $P$ and column operations by multiplying  on the right by the matrix $Q$ $$P= \begin{bmatrix} 0&0&0&1\\ 0&1&0&0\\ 0&3&1&-3\\ 1&0&0&0\\ \end{bmatrix},$$ $$Q=\begin{bmatrix} 1&6&-1\\ 0&1&0\\ 0&-6&1\\ \end{bmatrix}$$ to obtain the matrix $$PAQ=\begin{bmatrix} 1&0&0\\ 0&0&1\\ 0&18&0\\ 0&0&0\\ \end{bmatrix}.$$ I believe this tells us if $\xi \in H$ , then $\xi=[a,6b,18c,0]$ for $[a,b,c] \in \mathbb{Z}^{3}$ . From here, I think we can conclude that $$\mathbb{Z}^{4}/H \cong \mathbb{Z} \times  \mathbb{Z}/18\mathbb{Z}.$$ I do not see how to explicitly write a function between these two objects. Do I think of $\mathbb{Z}^{4}/H$ as cosets? The matrices $P$ and $Q$ above, tell us exactly the change of basis in the domain and codomain, I was wondering if I could use that. Thanks","There was a past qualifying exam problem, I was having trouble with, it is stated below as follows: In the group , let be the subgroup generated by . Find an explicit isomorphism between and a product of cyclic groups. I truly do not fully understand how to define such a map on . I have given some thought to this. We have that consists of integer combinations of the generators above, that is, if , then . In particular, we have that is in the image of a module homomorphism whose matrix with respect to a standard basis and for and respectively, will be We can perform row operations by multiplying on the left by the matrix and column operations by multiplying  on the right by the matrix to obtain the matrix I believe this tells us if , then for . From here, I think we can conclude that I do not see how to explicitly write a function between these two objects. Do I think of as cosets? The matrices and above, tell us exactly the change of basis in the domain and codomain, I was wondering if I could use that. Thanks","G= \mathbb{Z} \times \mathbb{Z}\times \mathbb{Z}\times \mathbb{Z}=\mathbb{Z}^{4} H [0,0,3,1], [0,6,0,0], [0,1,0,1] G/H G/H H \xi \in H \xi=a[0,0,3,1]+b[0,6,0,0]+c[0,1,0,1] \xi \mathbb{Z}^{3} \rightarrow \mathbb{Z}^{4} (e_{1},e_{2}, e_{3}) (f_{1}, f_{2}, f_{3}, f_{4}) \mathbb{Z}^{3} \mathbb{Z}^{4} A=\begin{bmatrix}
0&0&0\\
0&6&1\\
3&0&0\\
1&0&1\\
\end{bmatrix}. P Q P=
\begin{bmatrix}
0&0&0&1\\
0&1&0&0\\
0&3&1&-3\\
1&0&0&0\\
\end{bmatrix}, Q=\begin{bmatrix}
1&6&-1\\
0&1&0\\
0&-6&1\\
\end{bmatrix} PAQ=\begin{bmatrix}
1&0&0\\
0&0&1\\
0&18&0\\
0&0&0\\
\end{bmatrix}. \xi \in H \xi=[a,6b,18c,0] [a,b,c] \in \mathbb{Z}^{3} \mathbb{Z}^{4}/H \cong \mathbb{Z} \times  \mathbb{Z}/18\mathbb{Z}. \mathbb{Z}^{4}/H P Q","['abstract-algebra', 'modules', 'abelian-groups']"
92,Why is abelianness such a precious property?,Why is abelianness such a precious property?,,"My abstract algebra teacher said the other day that constructions like ideals and cosets and normal subgroups are ""trying to capture a little bit of abelianness."" He has used phrases like ""magic happens"" when speaking of this property, or qualities that mimic commutativity in some way. So why is it such a game-changing quality? Thanks!","My abstract algebra teacher said the other day that constructions like ideals and cosets and normal subgroups are ""trying to capture a little bit of abelianness."" He has used phrases like ""magic happens"" when speaking of this property, or qualities that mimic commutativity in some way. So why is it such a game-changing quality? Thanks!",,"['abstract-algebra', 'intuition', 'abelian-groups']"
93,What is a split $\mathbb{K}$-algebra?,What is a split -algebra?,\mathbb{K},"After some considerations the article I'm reading concludes: ""...hence H is a simple split $\mathbb{K}$-algebra"". I can't find this definition anywhere: what does ""split"" mean?","After some considerations the article I'm reading concludes: ""...hence H is a simple split $\mathbb{K}$-algebra"". I can't find this definition anywhere: what does ""split"" mean?",,"['abstract-algebra', 'terminology']"
94,Prove that $\mathbb{Z}[i]/\langle 1+i \rangle \cong \mathbb{Z}/2\mathbb{Z}$ [duplicate],Prove that  [duplicate],\mathbb{Z}[i]/\langle 1+i \rangle \cong \mathbb{Z}/2\mathbb{Z},"This question already has answers here : Quotient ring of Gaussian integers (7 answers) Closed 5 years ago . Prove that: $\mathbb{Z}[i]/\langle 1+i \rangle \cong \mathbb{Z}/2\mathbb{Z}$. This is my first time using the First Isomorphism Theorem for Rings and I am looking for feedback for whether some steps in this proof could have been done better. From my understanding, I need to: Define a surjective homomorphism $\phi$ between $\mathbb{Z}[i]$ and $\mathbb{Z}/2\mathbb{Z}$. Show that $\ker(\phi) = \langle 1+i \rangle$. Define $\phi: \mathbb{Z}[i] \to \mathbb{Z}/2\mathbb{Z}$ by $\phi(a+bi)=[a+b]_2$. $\boldsymbol {\phi}  \textbf{ is surjective:}$ Given $a+bi$, we have $a,b \in \mathbb{Z}$. So take $b$ to be $0$, then we have $\phi (a+0i)=[a]_2$, and clearly we can get $0$ and $1$. $\boldsymbol{\phi} \textbf{ is a homomorphism:} $ $$\begin{align} \phi((a+bi)+(c+di)) &=\phi((a+c)+(b+d)i) \\ &=[(a+c)+(b+d)]_2+[c+d]_2 \\ &=[(a+b)+(c+d)]_2 \\ & =[a+b]_2+[c+d]_2 \\ &= \phi(a+bi)+\phi(c+di).\end{align}$$ $$\begin{align} \phi((a+bi)(c+di)) &= \phi((ac-bd)+(ad+bc)i) \\ &=[(ac-bd)+(ad+bc)]_2 \\ &=[(ac+bd)+(ad+bc)]_2 \\ &=[(a+b)(c+d)]_2 \\ &=[a+b]_2[c+d]_2 \\ &=\phi(a+bi)\phi(c+di). \end{align}$$ $\boldsymbol{\ker(\phi)=\langle 1+i \rangle}$: We have that $\ker(\phi) = \{ (a+bi) \in \mathbb{Z}[i]: \phi(a+bi) =[a+b]_2 =[0]_2\}$. From my understanding, elements in $\langle 1+i \rangle$ are just of the type $(1+i)(c+di)$ where $c,d \in \mathbb{Z}$. I've tried to show that one is a subset of the other. Another idea I have is in noting that $[a+b]_2=[0]_2 \iff 2|(a+b)-0$ and somehow link it to showing that these are the same elements in $\langle 1+i \rangle$, but I could not do it. I have not managed to come up with a reasonable proof in any of these approaches. How does one actually show this?","This question already has answers here : Quotient ring of Gaussian integers (7 answers) Closed 5 years ago . Prove that: $\mathbb{Z}[i]/\langle 1+i \rangle \cong \mathbb{Z}/2\mathbb{Z}$. This is my first time using the First Isomorphism Theorem for Rings and I am looking for feedback for whether some steps in this proof could have been done better. From my understanding, I need to: Define a surjective homomorphism $\phi$ between $\mathbb{Z}[i]$ and $\mathbb{Z}/2\mathbb{Z}$. Show that $\ker(\phi) = \langle 1+i \rangle$. Define $\phi: \mathbb{Z}[i] \to \mathbb{Z}/2\mathbb{Z}$ by $\phi(a+bi)=[a+b]_2$. $\boldsymbol {\phi}  \textbf{ is surjective:}$ Given $a+bi$, we have $a,b \in \mathbb{Z}$. So take $b$ to be $0$, then we have $\phi (a+0i)=[a]_2$, and clearly we can get $0$ and $1$. $\boldsymbol{\phi} \textbf{ is a homomorphism:} $ $$\begin{align} \phi((a+bi)+(c+di)) &=\phi((a+c)+(b+d)i) \\ &=[(a+c)+(b+d)]_2+[c+d]_2 \\ &=[(a+b)+(c+d)]_2 \\ & =[a+b]_2+[c+d]_2 \\ &= \phi(a+bi)+\phi(c+di).\end{align}$$ $$\begin{align} \phi((a+bi)(c+di)) &= \phi((ac-bd)+(ad+bc)i) \\ &=[(ac-bd)+(ad+bc)]_2 \\ &=[(ac+bd)+(ad+bc)]_2 \\ &=[(a+b)(c+d)]_2 \\ &=[a+b]_2[c+d]_2 \\ &=\phi(a+bi)\phi(c+di). \end{align}$$ $\boldsymbol{\ker(\phi)=\langle 1+i \rangle}$: We have that $\ker(\phi) = \{ (a+bi) \in \mathbb{Z}[i]: \phi(a+bi) =[a+b]_2 =[0]_2\}$. From my understanding, elements in $\langle 1+i \rangle$ are just of the type $(1+i)(c+di)$ where $c,d \in \mathbb{Z}$. I've tried to show that one is a subset of the other. Another idea I have is in noting that $[a+b]_2=[0]_2 \iff 2|(a+b)-0$ and somehow link it to showing that these are the same elements in $\langle 1+i \rangle$, but I could not do it. I have not managed to come up with a reasonable proof in any of these approaches. How does one actually show this?",,[]
95,What are super-translations?,What are super-translations?,,"There's been a lot of news lately about a possible solution to the black hole information paradox from a presentation given by Stephen Hawking to the KTH Royal Institute of Technology in Stockholm . Many of them mention the term super translation . This article , for example, states (with emphasis added): His flash of inspiration came when listening to a lecture in April   about what are called super-translations , a bit of the heady   branch of mathematics known as group theory. Dr Hawking thinks that   incoming particles shed their information like a coat as they pass   into a black hole, leaving it draped on the event horizon itself. Super-translations mathematically describe how that information influx can slightly jiggle the fabric of space at the horizon, in turn   shifting around when and how the black hole radiates. What exactly are super-translations, mathematically? I have been trying to search for this term online, but the results are flooded with news articles. Where can I find some sources that describe super-translations in greater detail? EDIT: Another description of super-translations is provided in a recent article : The horizon of a black hole has the weird feature that it’s a sphere   and it’s expanding outward at the speed of light. For every point on   the sphere, there’s a light ray. So it’s composed of light rays. But   it doesn’t get any bigger and that’s because of the force of gravity   and the curvature of space. And, by the way, that’s why nothing that   is inside a black hole can get out—because the boundary of the black   hole itself is already moving at the speed of light. There’s this symmetry of a black hole that we all knew about in which   you move uniformly forward and backward in time along all of the light   rays. But there’s another symmetry, which is the new thing in this   paper (though various forms of it have been discussed elsewhere). It’s   a symmetry in which the individual light rays are moved up and down.   See, individual light rays can’t talk to each other—if you’re riding   on a light ray, causality prevents you from talking to somebody riding   on an adjacent light ray. So these light rays are not tethered   together. You can slide them up and down relative to one another. That   sliding is called a super-translation. And in a way, it looks like you're not doing anything. Think of a   bundle of infinitely long straws and you move one up and down relative   to the other. Are you doing anything, or not? What we showed is that   you are doing something. It turns out that adding a soft graviton has   an alternate description as a super-translation in which you move some   of these light rays back and forth relative to one another. That’s super-translations on black holes. Super-translations were   introduced in the 1960s, and they were talking not about the light   rays that comprise the boundary of spacetime at the horizon of a black   hole but the light rays that comprise the boundary of spacetime out at   infinity. The story started by analyzing those supertranslations.","There's been a lot of news lately about a possible solution to the black hole information paradox from a presentation given by Stephen Hawking to the KTH Royal Institute of Technology in Stockholm . Many of them mention the term super translation . This article , for example, states (with emphasis added): His flash of inspiration came when listening to a lecture in April   about what are called super-translations , a bit of the heady   branch of mathematics known as group theory. Dr Hawking thinks that   incoming particles shed their information like a coat as they pass   into a black hole, leaving it draped on the event horizon itself. Super-translations mathematically describe how that information influx can slightly jiggle the fabric of space at the horizon, in turn   shifting around when and how the black hole radiates. What exactly are super-translations, mathematically? I have been trying to search for this term online, but the results are flooded with news articles. Where can I find some sources that describe super-translations in greater detail? EDIT: Another description of super-translations is provided in a recent article : The horizon of a black hole has the weird feature that it’s a sphere   and it’s expanding outward at the speed of light. For every point on   the sphere, there’s a light ray. So it’s composed of light rays. But   it doesn’t get any bigger and that’s because of the force of gravity   and the curvature of space. And, by the way, that’s why nothing that   is inside a black hole can get out—because the boundary of the black   hole itself is already moving at the speed of light. There’s this symmetry of a black hole that we all knew about in which   you move uniformly forward and backward in time along all of the light   rays. But there’s another symmetry, which is the new thing in this   paper (though various forms of it have been discussed elsewhere). It’s   a symmetry in which the individual light rays are moved up and down.   See, individual light rays can’t talk to each other—if you’re riding   on a light ray, causality prevents you from talking to somebody riding   on an adjacent light ray. So these light rays are not tethered   together. You can slide them up and down relative to one another. That   sliding is called a super-translation. And in a way, it looks like you're not doing anything. Think of a   bundle of infinitely long straws and you move one up and down relative   to the other. Are you doing anything, or not? What we showed is that   you are doing something. It turns out that adding a soft graviton has   an alternate description as a super-translation in which you move some   of these light rays back and forth relative to one another. That’s super-translations on black holes. Super-translations were   introduced in the 1960s, and they were talking not about the light   rays that comprise the boundary of spacetime at the horizon of a black   hole but the light rays that comprise the boundary of spacetime out at   infinity. The story started by analyzing those supertranslations.",,"['abstract-algebra', 'group-theory', 'physics', 'mathematical-physics']"
96,$G/N$ read as $G$ modulo $N$.,read as  modulo .,G/N G N,"In my abstract algebra course, the instructor is calling $G/N$ (the set of left Cosets of $N$ in $G$) $G\: mod\: N$. This has not yet been explained. Why is this the case? My immediate suspicion is some isomorphism between $G/N$ and the integers modulo $n$.","In my abstract algebra course, the instructor is calling $G/N$ (the set of left Cosets of $N$ in $G$) $G\: mod\: N$. This has not yet been explained. Why is this the case? My immediate suspicion is some isomorphism between $G/N$ and the integers modulo $n$.",,"['abstract-algebra', 'terminology']"
97,"Integral basis of $\mathbb{Q}(\theta)$, where $\theta^3-\theta-4=0$","Integral basis of , where",\mathbb{Q}(\theta) \theta^3-\theta-4=0,"I am working on the text book ""algebraic number theory"" by Jurgen Neukirch(P15, exercise 6). To prove the integer basis is$ \{1, \theta, \frac{\theta^2+\theta}{2}\}$. After a long and tedious calculation, I still get nothing.(Following the method which can be used in exercise 5: the $\mathbb{Q}(\sqrt[3]{2})$'s case, which is typical and you can find in stackexchange!). Any help is going to be appreciated.","I am working on the text book ""algebraic number theory"" by Jurgen Neukirch(P15, exercise 6). To prove the integer basis is$ \{1, \theta, \frac{\theta^2+\theta}{2}\}$. After a long and tedious calculation, I still get nothing.(Following the method which can be used in exercise 5: the $\mathbb{Q}(\sqrt[3]{2})$'s case, which is typical and you can find in stackexchange!). Any help is going to be appreciated.",,"['abstract-algebra', 'algebraic-number-theory', 'extension-field']"
98,Is the set of all rational numbers with odd denominators a subring of $\Bbb Q$?,Is the set of all rational numbers with odd denominators a subring of ?,\Bbb Q,"Is the set of all rational numbers with odd denominators a subring of $\Bbb Q$?(When the fraction is completely reduced) I have tried to apply the subring test on this, and this means I want to show that it is closed under subtraction and multiplication. To show it is closed under multiplication, I have done the following: $$\frac{a}{2b+1}\frac{c}{2d+1},a,b,c,d\in\Bbb Z$$ $$=\frac{ac}{4bd+2b+2d+1}$$ Which still has odd denominator. With regard to subtraction we have: $$\frac{a}{2b+1}-\frac{c}{2d+1}$$ $$=\frac{a(2d+1)-c(2b+1)}{(2b+1)(2d+1)}=\frac{a(2d+1)-c(2b+1)}{4bd+2b+2d+1}$$ and since the denominator is odd, even if it isn't reduced, it will reduce to another odd denominator. So we have closure under both of these operations it would seem. Is this sufficient? Also if it were to show the set of all rational numbers with even demoninator, all I would need is $\frac{1}{2}-\frac{3}{2}=-\frac{2}{2}=-1$ to get a counter example right?","Is the set of all rational numbers with odd denominators a subring of $\Bbb Q$?(When the fraction is completely reduced) I have tried to apply the subring test on this, and this means I want to show that it is closed under subtraction and multiplication. To show it is closed under multiplication, I have done the following: $$\frac{a}{2b+1}\frac{c}{2d+1},a,b,c,d\in\Bbb Z$$ $$=\frac{ac}{4bd+2b+2d+1}$$ Which still has odd denominator. With regard to subtraction we have: $$\frac{a}{2b+1}-\frac{c}{2d+1}$$ $$=\frac{a(2d+1)-c(2b+1)}{(2b+1)(2d+1)}=\frac{a(2d+1)-c(2b+1)}{4bd+2b+2d+1}$$ and since the denominator is odd, even if it isn't reduced, it will reduce to another odd denominator. So we have closure under both of these operations it would seem. Is this sufficient? Also if it were to show the set of all rational numbers with even demoninator, all I would need is $\frac{1}{2}-\frac{3}{2}=-\frac{2}{2}=-1$ to get a counter example right?",,"['abstract-algebra', 'ring-theory', 'proof-verification']"
99,Distributor? Distributive analog of commutator and associator?,Distributor? Distributive analog of commutator and associator?,,"Motivation: ""the commutator gives an indication of the extent to which a certain binary operation fails to be commutative"" ( http://en.wikipedia.org/wiki/Commutator ). For example (courtesy of wikipedia), in a group, $G$, for each $g,h \in G$, the commutator of $g$ and $h$, $[g,h]$ is defined by, $$[g,h] = g^{-1}h^{-1}gh$$ ""the term associator is used in different ways as a measure of the nonassociativity of an algebraic structure"" ( http://en.wikipedia.org/wiki/Associator ). For example (thanks wikipedia!), for a nonassociative ring or algebra $R$, the associator is the multilinear map $[\cdot,\cdot,\cdot] : R \times R \times R \to R$ given by, $$[x,y,z] = (xy)z-x(yz)$$ Comment: I was expecting to find an analog to the associator and commutator for the property of distributivity. I've failed to find such an analog on wikipedia and via google searches. This lack of information gives me a sad face. Question: My question is, does an established analog to the (concept of the) associator and commutator exist for the property of distributivity? If so are there any good sources of information on it and its use? In certain situations we can define an operator that measures the degree to which one binary operation fails to be (left/right) distributive over another and call it the (left/right) distributor. For example suppose $(S,+,\times)$ is an algebraic structure such that $(S,+)$ is an abelian group and $(S,\times)$ is a semigroup. Then we can define the left distributor, $\mathrm{Ldis} : S \times S \times S \to S$, by $$\mathrm{Ldis}(x,y,z) = xy+xz - x(y+z)$$ Similarly we can define the right distributor, $\mathrm{Rdis} : S \times S \times S \to S$, by $$\mathrm{Rdis}(x,y,z) = yx+zx-(y+z)x$$ where, as per usual, $-$ is used to denote additive inverse, juxtaposition is used in place of $\times$, and $\times$ has higher precedence than $+$. In the case that $\times$ is commutative, $\mathrm{Ldis} = \mathrm{Rdis}$, and we can define the distributor, $\mathrm{dis}$, by setting it equal to the left or right distributor. Example: In any ring we have, $$\mathrm{dis}(x,y,z) = 0$$ Example: In any wheel algebra we have, $$\mathrm{dis}(x,y,z) = 0x$$ Side question: Can anyone think of any other structures with a distributor-like operator satisfying an identity which does not force said structure to be trivial? Any help or information on the subject is very much appreciated. Thank you for your time and consideration. I hope you have a great day. Sincerely, DAS","Motivation: ""the commutator gives an indication of the extent to which a certain binary operation fails to be commutative"" ( http://en.wikipedia.org/wiki/Commutator ). For example (courtesy of wikipedia), in a group, $G$, for each $g,h \in G$, the commutator of $g$ and $h$, $[g,h]$ is defined by, $$[g,h] = g^{-1}h^{-1}gh$$ ""the term associator is used in different ways as a measure of the nonassociativity of an algebraic structure"" ( http://en.wikipedia.org/wiki/Associator ). For example (thanks wikipedia!), for a nonassociative ring or algebra $R$, the associator is the multilinear map $[\cdot,\cdot,\cdot] : R \times R \times R \to R$ given by, $$[x,y,z] = (xy)z-x(yz)$$ Comment: I was expecting to find an analog to the associator and commutator for the property of distributivity. I've failed to find such an analog on wikipedia and via google searches. This lack of information gives me a sad face. Question: My question is, does an established analog to the (concept of the) associator and commutator exist for the property of distributivity? If so are there any good sources of information on it and its use? In certain situations we can define an operator that measures the degree to which one binary operation fails to be (left/right) distributive over another and call it the (left/right) distributor. For example suppose $(S,+,\times)$ is an algebraic structure such that $(S,+)$ is an abelian group and $(S,\times)$ is a semigroup. Then we can define the left distributor, $\mathrm{Ldis} : S \times S \times S \to S$, by $$\mathrm{Ldis}(x,y,z) = xy+xz - x(y+z)$$ Similarly we can define the right distributor, $\mathrm{Rdis} : S \times S \times S \to S$, by $$\mathrm{Rdis}(x,y,z) = yx+zx-(y+z)x$$ where, as per usual, $-$ is used to denote additive inverse, juxtaposition is used in place of $\times$, and $\times$ has higher precedence than $+$. In the case that $\times$ is commutative, $\mathrm{Ldis} = \mathrm{Rdis}$, and we can define the distributor, $\mathrm{dis}$, by setting it equal to the left or right distributor. Example: In any ring we have, $$\mathrm{dis}(x,y,z) = 0$$ Example: In any wheel algebra we have, $$\mathrm{dis}(x,y,z) = 0x$$ Side question: Can anyone think of any other structures with a distributor-like operator satisfying an identity which does not force said structure to be trivial? Any help or information on the subject is very much appreciated. Thank you for your time and consideration. I hope you have a great day. Sincerely, DAS",,"['abstract-algebra', 'category-theory', 'universal-algebra']"
