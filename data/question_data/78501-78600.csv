,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that a homomorphism from $SL_2(\mathbb{Z})$ to $ SL_2(\mathbb{F}_p)$ is surjective,Show that a homomorphism from  to  is surjective,SL_2(\mathbb{Z})  SL_2(\mathbb{F}_p),"The homomorphism sends the entries of matrices in $SL_2(\mathbb{Z})$ to their congruence classes mod $p$ . After a lot of work I could prove that a matrix $$\begin{pmatrix}a & b\\\ c & d\end{pmatrix}$$ in $ SL_2(\mathbb{F}_p)$ has a preimage in $SL_2(\mathbb{Z})$ if $a$ and $c$ are relatively prime and I can compute it. But I can't prove the general case and the method I used to prove the preceding assertion was so complicated that it can't be the right way to approach the problem. So there has to be another, more elegant, approach. I attacked the problem head on and ended up with a system of diophantine equations which was difficult to solve as one of the equations was quadratic.","The homomorphism sends the entries of matrices in to their congruence classes mod . After a lot of work I could prove that a matrix in has a preimage in if and are relatively prime and I can compute it. But I can't prove the general case and the method I used to prove the preceding assertion was so complicated that it can't be the right way to approach the problem. So there has to be another, more elegant, approach. I attacked the problem head on and ended up with a system of diophantine equations which was difficult to solve as one of the equations was quadratic.",SL_2(\mathbb{Z}) p \begin{pmatrix}a & b\\\ c & d\end{pmatrix}  SL_2(\mathbb{F}_p) SL_2(\mathbb{Z}) a c,"['abstract-algebra', 'matrices', 'number-theory', 'field-theory']"
1,bias searching for submatrix of higher sum in random matrices,bias searching for submatrix of higher sum in random matrices,,"This question is a follow up to the question that I asked in Stack Overflow . I now have a hint on the origin of the problem, but I still can't model it, and I was wondering if someone knows a way to do so. The problem: I generate 100,000 random matrices $11\times 11$ and I want to know which part of the random matrices has greater numbers. To do so I use a sliding window of $3\times 3$ that moves one cell at a time (overlapping windows, that are always inside the random matrix ). For each of the 100K random matrices I store the position of the  sub-matrix with highest sum. I finally plot the cumulative result of the 100K highest sums. This is the script I use, and the result I obtain: from random import random     from matplotlib import pyplot as plt     import numpy as np          size = 11          positions = np.zeros((size, size))          for _ in range(100000):         matrix = [[random() for _ in range(size)] for _ in range(size)]         max_value = 0         max_coord = 0, 0         for beg in range(1, size - 1):             for end in range(1, size - 1):                 suma = sum(matrix[i][j]                             for i in range(beg - 1, beg + 2)                             for j in range(end - 1, end + 2))                 if suma >= max_value:                     max_value = suma                     max_coord = beg, end         positions[max_coord] += 1          plt.imshow(positions[1:10,1:10], origin='lower')     plt.colorbar() The explanation I got was that the cells on the edges and, in particular in the corners visited less visits from the sliding window, and thus may have a more directed contribution to their corresponding sub-matrices. this bellow shows how many times a cell is visited by a sliding window: And this matches some how the previous result. BUT , in order to contrast better the result I continued with my simulation and went beyond the 100K (>300M), this is the result: It seems that there are more categories, even in regions where the sliding window passes the same number of times. If we plot the distribution of counts by cell: plt.figure(figsize=(12,8))     vals, icoord, jcoord = zip(*sorted([(v, i, j) for i, l in enumerate(positions[0:9,0:9]) for j, v in enumerate(l)]))     vals = [i / sum(vals) for i in vals]     color_list = [tuple(c) for c in plt.cm.tab10(np.linspace(0,1, 10))]     colors = [color_list[0]]     ncolor = 0     plt.plot(vals, color='grey', alpha=0.5, lw=1)     for n, (a, b) in enumerate(zip(vals[:-1], vals[1::])):         if b - a > 0.00002:             ncolor += 1         colors.append(color_list[ncolor])     plt.scatter(range(len(vals)), vals, color=colors, alpha=0.75)     plt.axhline(1 / 81, color='black', ls='--', alpha=0.7)     plt.text(0, 1 / 81, 'expected', color='black', alpha=0.75)     plt.xlabel('sorted cells')     plt.ylabel('count per cell')     plt.gca().yaxis.grid() we obtain this: I highlighted in different colours what seems to be clearly defined categories (I think that with more simulation I can find more categories). As final dumb check I coloured previous matrix with these categories and check if the result was symmetric: plt.figure(figsize=(5.28, 5.3))     plt.scatter(icoord, jcoord, c=colors, s=1000, marker='s') I have now done enough simulations to correct for this effect in my analysis, but I am still very curious about if there could be some analytical solution to this problem.","This question is a follow up to the question that I asked in Stack Overflow . I now have a hint on the origin of the problem, but I still can't model it, and I was wondering if someone knows a way to do so. The problem: I generate 100,000 random matrices and I want to know which part of the random matrices has greater numbers. To do so I use a sliding window of that moves one cell at a time (overlapping windows, that are always inside the random matrix ). For each of the 100K random matrices I store the position of the  sub-matrix with highest sum. I finally plot the cumulative result of the 100K highest sums. This is the script I use, and the result I obtain: from random import random     from matplotlib import pyplot as plt     import numpy as np          size = 11          positions = np.zeros((size, size))          for _ in range(100000):         matrix = [[random() for _ in range(size)] for _ in range(size)]         max_value = 0         max_coord = 0, 0         for beg in range(1, size - 1):             for end in range(1, size - 1):                 suma = sum(matrix[i][j]                             for i in range(beg - 1, beg + 2)                             for j in range(end - 1, end + 2))                 if suma >= max_value:                     max_value = suma                     max_coord = beg, end         positions[max_coord] += 1          plt.imshow(positions[1:10,1:10], origin='lower')     plt.colorbar() The explanation I got was that the cells on the edges and, in particular in the corners visited less visits from the sliding window, and thus may have a more directed contribution to their corresponding sub-matrices. this bellow shows how many times a cell is visited by a sliding window: And this matches some how the previous result. BUT , in order to contrast better the result I continued with my simulation and went beyond the 100K (>300M), this is the result: It seems that there are more categories, even in regions where the sliding window passes the same number of times. If we plot the distribution of counts by cell: plt.figure(figsize=(12,8))     vals, icoord, jcoord = zip(*sorted([(v, i, j) for i, l in enumerate(positions[0:9,0:9]) for j, v in enumerate(l)]))     vals = [i / sum(vals) for i in vals]     color_list = [tuple(c) for c in plt.cm.tab10(np.linspace(0,1, 10))]     colors = [color_list[0]]     ncolor = 0     plt.plot(vals, color='grey', alpha=0.5, lw=1)     for n, (a, b) in enumerate(zip(vals[:-1], vals[1::])):         if b - a > 0.00002:             ncolor += 1         colors.append(color_list[ncolor])     plt.scatter(range(len(vals)), vals, color=colors, alpha=0.75)     plt.axhline(1 / 81, color='black', ls='--', alpha=0.7)     plt.text(0, 1 / 81, 'expected', color='black', alpha=0.75)     plt.xlabel('sorted cells')     plt.ylabel('count per cell')     plt.gca().yaxis.grid() we obtain this: I highlighted in different colours what seems to be clearly defined categories (I think that with more simulation I can find more categories). As final dumb check I coloured previous matrix with these categories and check if the result was symmetric: plt.figure(figsize=(5.28, 5.3))     plt.scatter(icoord, jcoord, c=colors, s=1000, marker='s') I have now done enough simulations to correct for this effect in my analysis, but I am still very curious about if there could be some analytical solution to this problem.",11\times 11 3\times 3,"['matrices', 'random']"
2,Bounding spectral norm of matrix of binomial entries with small probabilities,Bounding spectral norm of matrix of binomial entries with small probabilities,,"Consider an $n \times n$ matrix $X$ where entries $$     X_{ij} = \begin{cases}         C, & \text{w.p. } p\\         0, & \text{w.p. } 1-p,\\         \end{cases} $$ where $p$ is very small. I am interested in bounding the spectral norm $\|X\|$ . The entries of $X_{ij}$ are sub-Gaussian with $\|X_{ij}\|_{\psi_2} = \frac{C}{\sqrt{\log(2/p)}}$ , and as such, Theorem 4.4.5 of Vershynin gives $$\|X\| \lesssim \|X_{ij}\|_{\psi_2}\sqrt{n}$$ with high probability. The definition of sub-Gaussian norm $\| \cdot \|_{\psi_2}$ I am using here is Definition 2.5.6 in Vershynin. This is fine if $p=0.5$ or so, but in my case, $p$ is very small. And as such, this bound is not tight at all. I would intuitively expect that the spectral norm should scale as $\sqrt{pn}$ or something similar. In my case, $X_{ij}$ is small because it is only large with very small probability. This is not captured by the sub-Gaussian norm, because all it cares about are the tails (which are sub-Gaussian for any bounded random variable). There is an analogous issue in the scalar setting. The sub-gaussian random variables are exactly those variables that obey a Hoeffding's inequality (Theorem 2.2.2 in Vershynin). However, as he points out in Section 2.3, the Hoeffding inequality is useless for Bernoulli random variables with small $p$ . Instead, you want to use the Chernoff inequality (Theorem 2.3.1) which is sensitive to small $p$ . Are there any bounds for $\|X\|$ when the entries are Bernoulli with small $p$ ?","Consider an matrix where entries where is very small. I am interested in bounding the spectral norm . The entries of are sub-Gaussian with , and as such, Theorem 4.4.5 of Vershynin gives with high probability. The definition of sub-Gaussian norm I am using here is Definition 2.5.6 in Vershynin. This is fine if or so, but in my case, is very small. And as such, this bound is not tight at all. I would intuitively expect that the spectral norm should scale as or something similar. In my case, is small because it is only large with very small probability. This is not captured by the sub-Gaussian norm, because all it cares about are the tails (which are sub-Gaussian for any bounded random variable). There is an analogous issue in the scalar setting. The sub-gaussian random variables are exactly those variables that obey a Hoeffding's inequality (Theorem 2.2.2 in Vershynin). However, as he points out in Section 2.3, the Hoeffding inequality is useless for Bernoulli random variables with small . Instead, you want to use the Chernoff inequality (Theorem 2.3.1) which is sensitive to small . Are there any bounds for when the entries are Bernoulli with small ?","n \times n X 
    X_{ij} = \begin{cases}
        C, & \text{w.p. } p\\
        0, & \text{w.p. } 1-p,\\
        \end{cases}
 p \|X\| X_{ij} \|X_{ij}\|_{\psi_2} = \frac{C}{\sqrt{\log(2/p)}} \|X\| \lesssim \|X_{ij}\|_{\psi_2}\sqrt{n} \| \cdot \|_{\psi_2} p=0.5 p \sqrt{pn} X_{ij} p p \|X\| p","['matrices', 'normed-spaces', 'binomial-distribution', 'random-matrices', 'matrix-norms']"
3,"Clebsch--Gordan decomposition for $\mathrm{SU}(2)$, in indices","Clebsch--Gordan decomposition for , in indices",\mathrm{SU}(2),"Let $\pi_m$, $m \geq 0$, be the unitary irreps of $\mathrm{SU}(2)$. The Clebsch--Gordan decomposition then gives that $$ \pi_m \otimes \pi_n = \bigoplus_{k=0}^{\min(m,n)}\pi_{m+n-2k}.$$ But suppose I want to think of this decomposition as matrices. Evaluating at a point $x \in \mathrm{SU}(2)$, on the left I have $$ (\pi_m(x))_{ij} (\pi_n(x))_{pq}.$$ How do the indices $i,j$ and $p,q$ correspond to the indices on the big matrix on the right? Some motivation for the question. Expressions involving products of Fourier coefficients arise naturally in nonlinear harmonic analysis on $\mathrm{SU}(2)$. In this specific instance I am interested in calculating $\operatorname{Tr}(\hat{f}(\pi_m) \pi_m(x)) \operatorname{Tr}(\hat{f}(\pi_n)\pi_n(x)) = \hat{f}(\pi_m)^{ji} \hat{f}(\pi_n)^{qp} \pi_m(x)_{ij} \pi_n(x)_{pq}$ using the Clebsch--Gordan expansion above. Here $\hat{f}(\pi) = \int_{\mathrm{SU}(2)} f(x) \pi(x^{-1}) \mathrm{d} x. $","Let $\pi_m$, $m \geq 0$, be the unitary irreps of $\mathrm{SU}(2)$. The Clebsch--Gordan decomposition then gives that $$ \pi_m \otimes \pi_n = \bigoplus_{k=0}^{\min(m,n)}\pi_{m+n-2k}.$$ But suppose I want to think of this decomposition as matrices. Evaluating at a point $x \in \mathrm{SU}(2)$, on the left I have $$ (\pi_m(x))_{ij} (\pi_n(x))_{pq}.$$ How do the indices $i,j$ and $p,q$ correspond to the indices on the big matrix on the right? Some motivation for the question. Expressions involving products of Fourier coefficients arise naturally in nonlinear harmonic analysis on $\mathrm{SU}(2)$. In this specific instance I am interested in calculating $\operatorname{Tr}(\hat{f}(\pi_m) \pi_m(x)) \operatorname{Tr}(\hat{f}(\pi_n)\pi_n(x)) = \hat{f}(\pi_m)^{ji} \hat{f}(\pi_n)^{qp} \pi_m(x)_{ij} \pi_n(x)_{pq}$ using the Clebsch--Gordan expansion above. Here $\hat{f}(\pi) = \int_{\mathrm{SU}(2)} f(x) \pi(x^{-1}) \mathrm{d} x. $",,"['matrices', 'representation-theory', 'lie-groups']"
4,Is this block matrix totally unimodular?,Is this block matrix totally unimodular?,,"Suppose matrix $A \in \mathbb{R}^{m×n}$ has the consecutive ones property and, thus, is totally unimodular. Is the following block matrix also totally unimodular (TU)? $$B = \begin{pmatrix}         A & 0 & \dots & 0\\        0 & A & \dots & 0\\        \vdots & \vdots & \ddots & \vdots\\        0 & 0 & \dots & A\\        I & I & \dots & I\end{pmatrix}$$ This type of matrix appears when dealing with the Coloring Problem over Interval Graphs, which can be solved in polynomial time, but I don't know if there is any result on the total unimodularity of such matrix. A similar question was asked here ( Is this block matrix also totally unimodular? ), where $A$ was known to be TU. In this case the block matrix can be non-TU, however the counter-example given uses a matrix A that does not have the consecutive one's property.","Suppose matrix has the consecutive ones property and, thus, is totally unimodular. Is the following block matrix also totally unimodular (TU)? This type of matrix appears when dealing with the Coloring Problem over Interval Graphs, which can be solved in polynomial time, but I don't know if there is any result on the total unimodularity of such matrix. A similar question was asked here ( Is this block matrix also totally unimodular? ), where was known to be TU. In this case the block matrix can be non-TU, however the counter-example given uses a matrix A that does not have the consecutive one's property.","A \in \mathbb{R}^{m×n} B = \begin{pmatrix} 
       A & 0 & \dots & 0\\
       0 & A & \dots & 0\\
       \vdots & \vdots & \ddots & \vdots\\
       0 & 0 & \dots & A\\
       I & I & \dots & I\end{pmatrix} A","['matrices', 'block-matrices', 'total-unimodularity', 'unimodular-matrices']"
5,Zolotarev number and commuting matrices,Zolotarev number and commuting matrices,,"Recently in a post ( link ) upper bounds on the singular values $\sigma_j(X)$ of a matrix $X$ have been considered. To restate the central observation, it says that if $AX−XB=F$ for $A$ and $B$ normal matrices, then we have that $$σ_{1+νk}(X)≤Z_k(σ(A),σ(B)) \sigma_1, \;\;\;\;\;\; ν=rank(F),$$ where $σ(A)$ and $σ(B)$ are the spectra of $A$ and $B$, respectively, and $Z_k(E,F)$ the Zolotarev number. This nice result is from here . In the reference, as far as I can see the examples are throughly for disjoint $E$ and $F$. Are there any result when $E$ and $F$ are not disjoint?","Recently in a post ( link ) upper bounds on the singular values $\sigma_j(X)$ of a matrix $X$ have been considered. To restate the central observation, it says that if $AX−XB=F$ for $A$ and $B$ normal matrices, then we have that $$σ_{1+νk}(X)≤Z_k(σ(A),σ(B)) \sigma_1, \;\;\;\;\;\; ν=rank(F),$$ where $σ(A)$ and $σ(B)$ are the spectra of $A$ and $B$, respectively, and $Z_k(E,F)$ the Zolotarev number. This nice result is from here . In the reference, as far as I can see the examples are throughly for disjoint $E$ and $F$. Are there any result when $E$ and $F$ are not disjoint?",,"['matrices', 'eigenvalues-eigenvectors', 'rational-functions', 'approximation-theory', 'singular-values']"
6,"Is ""$:$"" so-called Frobenius inner product?","Is """" so-called Frobenius inner product?",:,"I see some notation like \begin{align*} \int \nabla \mathbf{u} : \nabla \mathbf{v} \; dx \end{align*} Here I think the two vectors $\mathbf{u}$ and $\mathbf{v}$ should be column vectors, i.e. $\mathbf{u} = [u_1,u_2,...,u_n]^T$ and $\mathbf{v} = [v_1,v_2,...,v_n]^T$. Is that right? So, here $\nabla \mathbf{u}$ will be a Jacobian matrix, and $\nabla \mathbf{v}$ as well. Right? Then, does the operation $\nabla \mathbf{u} : \nabla \mathbf{v}$ mean the element-wise multiplication such that \begin{align*} \nabla \mathbf{u} : \nabla \mathbf{v} &= (\nabla \otimes \mathbf{u}) : (\nabla \otimes \mathbf{v}) \\ &= \left( \begin{bmatrix} \nabla_1 \\ \nabla_2 \\ \vdots \\ \nabla_n \end{bmatrix} \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix} \right) : \left( \begin{bmatrix} \nabla_1 \\ \nabla_2 \\ \vdots \\ \nabla_n \end{bmatrix}  \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix} \right) \\ &=  \begin{bmatrix} \nabla_1 u_1 \nabla_1 v_1 & \nabla_1 u_2 \nabla_1 v_2 & \cdots & \nabla_1 u_n \nabla_1 v_n \\ \nabla_2 u_1 \nabla_2 v_1 & \nabla_2 u_2 \nabla_2 v_2 & \cdots & \nabla_2 u_n \nabla_2 v_n \\ \vdots & \vdots & \vdots & \vdots \\ \nabla_n u_1 \nabla_n v_1 & \nabla_n u_2 \nabla_n v_2 & \cdots & \nabla_n u_n \nabla_n v_n \end{bmatrix}.  \end{align*} Am I right? I didn't ever see ""$:$"" before, but I think it is so-called the Frobenius inner product, though its Wiki page doesn't mention this notation.","I see some notation like \begin{align*} \int \nabla \mathbf{u} : \nabla \mathbf{v} \; dx \end{align*} Here I think the two vectors $\mathbf{u}$ and $\mathbf{v}$ should be column vectors, i.e. $\mathbf{u} = [u_1,u_2,...,u_n]^T$ and $\mathbf{v} = [v_1,v_2,...,v_n]^T$. Is that right? So, here $\nabla \mathbf{u}$ will be a Jacobian matrix, and $\nabla \mathbf{v}$ as well. Right? Then, does the operation $\nabla \mathbf{u} : \nabla \mathbf{v}$ mean the element-wise multiplication such that \begin{align*} \nabla \mathbf{u} : \nabla \mathbf{v} &= (\nabla \otimes \mathbf{u}) : (\nabla \otimes \mathbf{v}) \\ &= \left( \begin{bmatrix} \nabla_1 \\ \nabla_2 \\ \vdots \\ \nabla_n \end{bmatrix} \begin{bmatrix} u_1 & u_2 & \cdots & u_n \end{bmatrix} \right) : \left( \begin{bmatrix} \nabla_1 \\ \nabla_2 \\ \vdots \\ \nabla_n \end{bmatrix}  \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix} \right) \\ &=  \begin{bmatrix} \nabla_1 u_1 \nabla_1 v_1 & \nabla_1 u_2 \nabla_1 v_2 & \cdots & \nabla_1 u_n \nabla_1 v_n \\ \nabla_2 u_1 \nabla_2 v_1 & \nabla_2 u_2 \nabla_2 v_2 & \cdots & \nabla_2 u_n \nabla_2 v_n \\ \vdots & \vdots & \vdots & \vdots \\ \nabla_n u_1 \nabla_n v_1 & \nabla_n u_2 \nabla_n v_2 & \cdots & \nabla_n u_n \nabla_n v_n \end{bmatrix}.  \end{align*} Am I right? I didn't ever see ""$:$"" before, but I think it is so-called the Frobenius inner product, though its Wiki page doesn't mention this notation.",,"['matrices', 'notation', 'terminology', 'tensor-products', 'tensors']"
7,Pseudoinverse as a submatrix of matrix inverse,Pseudoinverse as a submatrix of matrix inverse,,"Suppose I have a device that can compute 2x2 (complex) matrix inverses. (For now, assume only invertible matrices, $A$, are ever provided as input): $A\triangleq \begin{bmatrix}         a_{11} & a_{12}\\         a_{21} & a_{22}\\         \end{bmatrix}$ $A^{-1}= \frac{1}{(a_{11}a_{22}-a_{12}a_{21})}\begin{bmatrix}         a_{22} & -a_{12}\\         -a_{21} & a_{11}\\         \end{bmatrix}$ I can use the same device to compute the pseudoinverse of a 2x1 vector (where $^H$ denotes complex conjugate transpose): $\underline{b}^+=\frac{1}{(\underline{b}^H\underline{b})}\underline{b}^H$ For example, I can achieve this by inputting the following matrix to my matrix inversion device (where $^*$ denotes complex conjugate): $B= \begin{bmatrix}         b_{1} & -b^*_2\\         b_{2} & b^*_1\\         \end{bmatrix}$ Since the inverse is: $B^{-1}= \frac{1}{(\underline{b}^H\underline{b})}\begin{bmatrix}         b^*_1 & b^*_2\\         -b_2 & b_1\\         \end{bmatrix}$ and the first row of the result is $\underline{b}^+$. My question is whether this is generalisable to larger matrices. For example, if I have a 3x3 matrix inversion device, can I arrange inputs such that I can read off a 3x2 matrix pseudoinverse? Or have I just stumbled upon the only special case?","Suppose I have a device that can compute 2x2 (complex) matrix inverses. (For now, assume only invertible matrices, $A$, are ever provided as input): $A\triangleq \begin{bmatrix}         a_{11} & a_{12}\\         a_{21} & a_{22}\\         \end{bmatrix}$ $A^{-1}= \frac{1}{(a_{11}a_{22}-a_{12}a_{21})}\begin{bmatrix}         a_{22} & -a_{12}\\         -a_{21} & a_{11}\\         \end{bmatrix}$ I can use the same device to compute the pseudoinverse of a 2x1 vector (where $^H$ denotes complex conjugate transpose): $\underline{b}^+=\frac{1}{(\underline{b}^H\underline{b})}\underline{b}^H$ For example, I can achieve this by inputting the following matrix to my matrix inversion device (where $^*$ denotes complex conjugate): $B= \begin{bmatrix}         b_{1} & -b^*_2\\         b_{2} & b^*_1\\         \end{bmatrix}$ Since the inverse is: $B^{-1}= \frac{1}{(\underline{b}^H\underline{b})}\begin{bmatrix}         b^*_1 & b^*_2\\         -b_2 & b_1\\         \end{bmatrix}$ and the first row of the result is $\underline{b}^+$. My question is whether this is generalisable to larger matrices. For example, if I have a 3x3 matrix inversion device, can I arrange inputs such that I can read off a 3x2 matrix pseudoinverse? Or have I just stumbled upon the only special case?",,"['matrices', 'inverse', 'pseudoinverse']"
8,What happens to woodbury matrix identity when A is not invertible?,What happens to woodbury matrix identity when A is not invertible?,,"The Woodbury matrix identity is \begin{equation} (A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}. \end{equation} This formula suppose that $A$, $(A+UCV)$ and $(C^{-1}+VA^{-1}U)$ are invertible. What happens to this formula when $A$ is not invertible please? Do you know a formula for that case please? I have read the paper "" A Sherman Morrison Woodbury Identity for Rank Augmenting Matrices with Application to Centering "" of Kurt S. Riedel but his formula looks so complicated. Thanks.","The Woodbury matrix identity is \begin{equation} (A+UCV)^{-1}=A^{-1}-A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}. \end{equation} This formula suppose that $A$, $(A+UCV)$ and $(C^{-1}+VA^{-1}U)$ are invertible. What happens to this formula when $A$ is not invertible please? Do you know a formula for that case please? I have read the paper "" A Sherman Morrison Woodbury Identity for Rank Augmenting Matrices with Application to Centering "" of Kurt S. Riedel but his formula looks so complicated. Thanks.",,"['matrices', 'inverse', 'matrix-decomposition', 'pseudoinverse']"
9,"Stability by $T$ of a sequence defined by $\ker T^r$ if spectrum(T)=$\{0,\lambda\}$",Stability by  of a sequence defined by  if spectrum(T)=,"T \ker T^r \{0,\lambda\}","Suppose that the only eigenvalues of T are 0 and λ, where λ $\neq$ 0. Let $W = T^r(V )$, r satisfies ker $T^r$=ker $T^{r+1}$. Show that $T(W) ⊆ W$, and that the restriction of T to W has $λ$ as its only eigenvalue. Let S denote the restriction of $(T − λI)$ to W. Show that 0 is the only eigenvalue of S. I can show $T(W) ⊆ W$, but how to show the two only eigenvalues? Can anyone give a hint?","Suppose that the only eigenvalues of T are 0 and λ, where λ $\neq$ 0. Let $W = T^r(V )$, r satisfies ker $T^r$=ker $T^{r+1}$. Show that $T(W) ⊆ W$, and that the restriction of T to W has $λ$ as its only eigenvalue. Let S denote the restriction of $(T − λI)$ to W. Show that 0 is the only eigenvalue of S. I can show $T(W) ⊆ W$, but how to show the two only eigenvalues? Can anyone give a hint?",,"['matrices', 'eigenvalues-eigenvectors']"
10,Find the set of numbers for which some sum over permutations is independent of the initial value,Find the set of numbers for which some sum over permutations is independent of the initial value,,"Question I have a sequence of natural numbers $0 = k_0 < k_1 < k_2 < \dots < k_n$ and we want to find the set of numbers which satisfy some property, so we want to find (in function of $n$): $$ \mathcal{A}_n := \{ (k_1,\dots,k_n) \in \mathbb{N}^{n} \mid (k_0,\dots,k_n) \mbox{ satisfy property }A\} $$ The property $A$ we are looking at is the following: let $\sigma$ be an arbitrary permutation on $\{k_0,\dots,k_n\}$ and $k \in \{k_0,\dots,k_n\}$ be arbitrary, we define: $$ V(\sigma,k) := \sum_{i=0}^{k_n-1} \delta_{\{\sigma^{k_n-i-1}(k) > i\}} $$ where we define $\delta_{\{a > b\}}$ to be zero if $a \leq b$ and one if $a > b$. We say that $(k_0,\dots,k_n)$ satisfy property $A$ if: $$ \exists \sigma: \forall k,l \in \{k_0,\dots,k_n\}: V(\sigma,k) = V(\sigma,l), $$ i.e. there exists some permutation s.t. $V(\sigma,k)$ doesn't depend on the initial state $k$. Simplified version I would already be quite happy if it was possible to show that $\forall n: \mathcal{A}_n \neq \emptyset$. Lower dimensional example I have done the calculations for some values of $n$. $n=1$ We have: $$ \mathcal{A}_n = \times 2\mathbb{N}, $$ indeed, for the trivial permutation this sum is dependant on $k$ as we have: $$ V(\mbox{id},k) = \begin{cases} k_1 & \mbox{ if } k = k_1\\ 0 & \mbox{ if } k = k_0. \end{cases} $$ and the only other permutation $\sigma$ we have is defined by sending $0$ to $k_1$ and $k_1$ to $0$, the sum then just counts the number of times $\sigma^{k_1-i-1}$, for $k_1$ odd this the sum differs by one depending on we start at $k$ or not (as we start and end with the same value), whilst for even $k_1$ we start and end with the same value and thus the sum yields the same result in both cases (i.e. starting from $k_1$ or $0$). $n=2$ In this case the solution is still quite elegant as we have: $$ \mathcal{A}_n = \{(k_1,k_2) \mid k_1,k_2 \mbox{ and } 0 \mbox{ are distinct modulo } 3.\} $$ which can be seen analogously to the 2 dimensional case (except for using other permutations). $n=3$ Numerically I suspect that $\mathcal{A}_n$ can be characterized by the matrix: $$ M_3 \begin{pmatrix} 1 & 2 & 1\\  1 & 3 & 0\\  1 & 0 & 3\\  1 & 1 & 2\\  2 & 3 & 3\\  2 & 0 & 2\\  2 & 2 & 0\\ 3 & 0 & 1\\ 3 & 1 & 0\\ 3 & 2 & 3\\ 0 & 1 & 3\\ \end{pmatrix} $$ where $(k_1,k_2,k_3)$ is in $\mathcal{A}_n$ if the vector $(\mod(k_1,4),\mod(k_2,4),\mod(k_3,4))$ is a row in the matrix $M_3$. Numerically I can compute these matrices for pretty large $n$ but I would like to have some general expression for it, but I feel like I lack the algebraic insights to do this. Origin Of The problem I have a supply process with $n+1$ different lead times (with one equal to $0$) and I cycle through all the different lead times. I wonder if we have for any number of different lead times that we can cycle through these lead times in such a way that the inventory remains the same over time.","Question I have a sequence of natural numbers $0 = k_0 < k_1 < k_2 < \dots < k_n$ and we want to find the set of numbers which satisfy some property, so we want to find (in function of $n$): $$ \mathcal{A}_n := \{ (k_1,\dots,k_n) \in \mathbb{N}^{n} \mid (k_0,\dots,k_n) \mbox{ satisfy property }A\} $$ The property $A$ we are looking at is the following: let $\sigma$ be an arbitrary permutation on $\{k_0,\dots,k_n\}$ and $k \in \{k_0,\dots,k_n\}$ be arbitrary, we define: $$ V(\sigma,k) := \sum_{i=0}^{k_n-1} \delta_{\{\sigma^{k_n-i-1}(k) > i\}} $$ where we define $\delta_{\{a > b\}}$ to be zero if $a \leq b$ and one if $a > b$. We say that $(k_0,\dots,k_n)$ satisfy property $A$ if: $$ \exists \sigma: \forall k,l \in \{k_0,\dots,k_n\}: V(\sigma,k) = V(\sigma,l), $$ i.e. there exists some permutation s.t. $V(\sigma,k)$ doesn't depend on the initial state $k$. Simplified version I would already be quite happy if it was possible to show that $\forall n: \mathcal{A}_n \neq \emptyset$. Lower dimensional example I have done the calculations for some values of $n$. $n=1$ We have: $$ \mathcal{A}_n = \times 2\mathbb{N}, $$ indeed, for the trivial permutation this sum is dependant on $k$ as we have: $$ V(\mbox{id},k) = \begin{cases} k_1 & \mbox{ if } k = k_1\\ 0 & \mbox{ if } k = k_0. \end{cases} $$ and the only other permutation $\sigma$ we have is defined by sending $0$ to $k_1$ and $k_1$ to $0$, the sum then just counts the number of times $\sigma^{k_1-i-1}$, for $k_1$ odd this the sum differs by one depending on we start at $k$ or not (as we start and end with the same value), whilst for even $k_1$ we start and end with the same value and thus the sum yields the same result in both cases (i.e. starting from $k_1$ or $0$). $n=2$ In this case the solution is still quite elegant as we have: $$ \mathcal{A}_n = \{(k_1,k_2) \mid k_1,k_2 \mbox{ and } 0 \mbox{ are distinct modulo } 3.\} $$ which can be seen analogously to the 2 dimensional case (except for using other permutations). $n=3$ Numerically I suspect that $\mathcal{A}_n$ can be characterized by the matrix: $$ M_3 \begin{pmatrix} 1 & 2 & 1\\  1 & 3 & 0\\  1 & 0 & 3\\  1 & 1 & 2\\  2 & 3 & 3\\  2 & 0 & 2\\  2 & 2 & 0\\ 3 & 0 & 1\\ 3 & 1 & 0\\ 3 & 2 & 3\\ 0 & 1 & 3\\ \end{pmatrix} $$ where $(k_1,k_2,k_3)$ is in $\mathcal{A}_n$ if the vector $(\mod(k_1,4),\mod(k_2,4),\mod(k_3,4))$ is a row in the matrix $M_3$. Numerically I can compute these matrices for pretty large $n$ but I would like to have some general expression for it, but I feel like I lack the algebraic insights to do this. Origin Of The problem I have a supply process with $n+1$ different lead times (with one equal to $0$) and I cycle through all the different lead times. I wonder if we have for any number of different lead times that we can cycle through these lead times in such a way that the inventory remains the same over time.",,"['abstract-algebra', 'matrices', 'permutations', 'modular-arithmetic']"
11,Checking positive semidefiniteness in MATLAB,Checking positive semidefiniteness in MATLAB,,"Let $\mathbf{A}$ be a $n\times n$ matrix. I want to check in MATLAB if it is PSD or not. Which tests, in MATLAB, should I do for this purpose? I know that if $\mathbf{A}$ is PSD then following holds eigenvalue decomposition: [V,lambda]=eig(A) , then $\lambda_i\geq 0 \hspace{2mm}\forall \hspace{2mm} i=1:n$ Cholesky decomposition: [R,p] = chol(A) then $p=0$ WHY: I am asking for a list because I am having an issue. I started with a matrix $\mathbf{X}$ that was supposed to be PSD because of the way I was constructing it. However, some of its eigen values were negative. It turned out that floating point arithmetic is to be blamed for this. Problem: I came across https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd . According to this function, $\mathbf{A}$=nearestSPD($\mathbf{X}$), $\mathbf{A}$ is the nearest SPD to $\mathbf{X}$. In help section, author mentioned that [R,p]=chol(A) will return p=0 which proves that A is SPD. Now the problem is that after getting $\mathbf{A}$, I do get p=0 for chol($\mathbf{A}$), but I am still getting one or more negative eigen values. Although the magnitude of these negative eigen values is very small something like e-15 but still the sign is negative. So I don't understand why these tests are giving different results and what should one do in such a case.","Let $\mathbf{A}$ be a $n\times n$ matrix. I want to check in MATLAB if it is PSD or not. Which tests, in MATLAB, should I do for this purpose? I know that if $\mathbf{A}$ is PSD then following holds eigenvalue decomposition: [V,lambda]=eig(A) , then $\lambda_i\geq 0 \hspace{2mm}\forall \hspace{2mm} i=1:n$ Cholesky decomposition: [R,p] = chol(A) then $p=0$ WHY: I am asking for a list because I am having an issue. I started with a matrix $\mathbf{X}$ that was supposed to be PSD because of the way I was constructing it. However, some of its eigen values were negative. It turned out that floating point arithmetic is to be blamed for this. Problem: I came across https://www.mathworks.com/matlabcentral/fileexchange/42885-nearestspd . According to this function, $\mathbf{A}$=nearestSPD($\mathbf{X}$), $\mathbf{A}$ is the nearest SPD to $\mathbf{X}$. In help section, author mentioned that [R,p]=chol(A) will return p=0 which proves that A is SPD. Now the problem is that after getting $\mathbf{A}$, I do get p=0 for chol($\mathbf{A}$), but I am still getting one or more negative eigen values. Although the magnitude of these negative eigen values is very small something like e-15 but still the sign is negative. So I don't understand why these tests are giving different results and what should one do in such a case.",,"['matrices', 'eigenvalues-eigenvectors', 'matlab', 'matrix-decomposition', 'positive-semidefinite']"
12,Complex SO(n) and Degenerations,Complex SO(n) and Degenerations,,"Let's say you have a matrix $A\in\Bbb C^{n\times n}$ with $A^T A = 0$, like for example this one: $$ A = \begin{pmatrix} 1 & -i \\ i & 1 \end{pmatrix} $$ I ask the following: Question: Is there a sequence of matrices $(S_k)_{k\in\Bbb N}$ with $S_k^T\cdot S_k = I$ and $\lim\limits_{k\to\infty} S_k A = 0$ ? For the matrix above, you can pick $\theta_k:=k\cdot(1-i)$ and $$ S_k = \begin{pmatrix} \cos(\theta_k) & -\sin(\theta_k) \\ \sin(\theta_k) & \cos(\theta_k) \end{pmatrix}. $$ This is not entirely obvious, but a straightforward computation. I say that because a computer algebra system did it. PS: I have somewhat involved reasons to ask this question, and laying them out would basically just yield many paragraphs full of buzzwords, and I do not think it would make the question any more clear. I do believe that the answer is affirmative, though. Edit: I decided to add at least a few buzzwords. Let $O_n=\{ S\in\mathbb C^{n\times n} \mid S^T S = I \}$ be the orthogonal group. It is a reductive group acting on the space $W=\Bbb C^{n\times n}$ by left multiplication. I am asking whether the Nullcone of this action is equal to $N=\{ A \mid A^T A = 0 \}$. Note that $N$ is closed and certainly contains the Nullcone, because if a sequence $(S_k)$ with the above property exists, then $A^TA=A^TS_k^TSA=(S_kA)^T(S_kA)\to 0$ as $k\to\infty$, which means $A^TA=0$.","Let's say you have a matrix $A\in\Bbb C^{n\times n}$ with $A^T A = 0$, like for example this one: $$ A = \begin{pmatrix} 1 & -i \\ i & 1 \end{pmatrix} $$ I ask the following: Question: Is there a sequence of matrices $(S_k)_{k\in\Bbb N}$ with $S_k^T\cdot S_k = I$ and $\lim\limits_{k\to\infty} S_k A = 0$ ? For the matrix above, you can pick $\theta_k:=k\cdot(1-i)$ and $$ S_k = \begin{pmatrix} \cos(\theta_k) & -\sin(\theta_k) \\ \sin(\theta_k) & \cos(\theta_k) \end{pmatrix}. $$ This is not entirely obvious, but a straightforward computation. I say that because a computer algebra system did it. PS: I have somewhat involved reasons to ask this question, and laying them out would basically just yield many paragraphs full of buzzwords, and I do not think it would make the question any more clear. I do believe that the answer is affirmative, though. Edit: I decided to add at least a few buzzwords. Let $O_n=\{ S\in\mathbb C^{n\times n} \mid S^T S = I \}$ be the orthogonal group. It is a reductive group acting on the space $W=\Bbb C^{n\times n}$ by left multiplication. I am asking whether the Nullcone of this action is equal to $N=\{ A \mid A^T A = 0 \}$. Note that $N$ is closed and certainly contains the Nullcone, because if a sequence $(S_k)$ with the above property exists, then $A^TA=A^TS_k^TSA=(S_kA)^T(S_kA)\to 0$ as $k\to\infty$, which means $A^TA=0$.",,"['matrices', 'algebraic-geometry', 'lie-groups', 'complex-geometry', 'algebraic-groups']"
13,Is there any specific criteria for a matrix $A(t)$ to have either time-dependent or independent eigenvectors?,Is there any specific criteria for a matrix  to have either time-dependent or independent eigenvectors?,A(t),"I am investigating properties of a matrix $$A(t_1,t_2) \equiv U_1(t_1) \otimes U_2(t_2) - U_2(t_2) \otimes U_1(t_1)$$ where $U_1$ and $U_2$ are time-dependent unitary matrices. I'm finding that for some choices of $U_1$ and $U_2$, when $t_1 \neq t_2$, the eigenvectors of $A$ are time-independent. Are there any testable properties of $A, U_1, U_2$ which could be used to predict this?","I am investigating properties of a matrix $$A(t_1,t_2) \equiv U_1(t_1) \otimes U_2(t_2) - U_2(t_2) \otimes U_1(t_1)$$ where $U_1$ and $U_2$ are time-dependent unitary matrices. I'm finding that for some choices of $U_1$ and $U_2$, when $t_1 \neq t_2$, the eigenvectors of $A$ are time-independent. Are there any testable properties of $A, U_1, U_2$ which could be used to predict this?",,"['matrices', 'eigenvalues-eigenvectors', 'kronecker-product']"
14,Spectral norm of a matrix of cosines,Spectral norm of a matrix of cosines,,"I am considering the following matrix: $$ M_m = \begin{bmatrix} \cos\bigl(\tfrac{0\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{0\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{0\cdot m}{m}\pi\bigr) \\ \cos\bigl(\tfrac{1\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{1\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{1\cdot m}{m}\pi\bigr) \\ \vdots & \vdots & \ddots & \vdots \\ \cos\bigl(\tfrac{m\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{m\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{m\cdot m}{m}\pi\bigr) \\ \end{bmatrix} \in \mathbb R^{(m+1)\times(m+1)} $$ A short notation would be $$M_m = \Bigl(\cos\bigl(\tfrac{ij}{m}\pi\bigr)\Bigr)_{i,j=0}^m.$$ I want to calculate the spectral norm $\|M_m\|_2$. Since $M_m$ is symmetric, one could investigate its Eigenvalues, but I could not come up with anything. Maybe one can come up with a clever vector $v$ with $\|v\|_2 = 1$, where it can be seen that it maximizes $\|M_m\cdot v\|_2$? I already saw that numerically, the norm $\|M_m\|_2$ seems to grow like $\sqrt m$, so I suppose in the algebraic expression for $\|M_m\|_2$ there should be something like $\sqrt m$. I also saw that I may be a good idea to consider $m=2k$ and $m=2k+1$ differently, since it seems the norm is jumping around between two slightly different functions. I would conjecture that for $$ \alpha(m) = \begin{cases}\frac1{\sqrt2}\sqrt{2m+1 + \sqrt{4m+1}},&m\text{ is even}\\ \frac12\sqrt{4m+1 + \sqrt{8m+1}},&m\text{ is odd}\\\end{cases} $$ it holds $$ \alpha(m) = \|M_m\|_2. $$ I know that it is true for $m=2,\dots,8$, because of this Mathematica Code: mM[m_] := Table[Cos[(i j)/m Pi], {i, 0, m}, {j, 0, m}] alpha[m_] := If[EvenQ[m],   Sqrt[(2 m + 1 + Sqrt[4 m + 1])/2],   Sqrt[ 4 m + 1 + Sqrt[8 m + 1]]/2  ] Table[Norm[mM[m]] - alpha[m], {m, 2, 8}] The conjecture can be formulated a bit different and easier: For $$ \beta(m) = \begin{cases}\frac12(1 + \sqrt{4m+1}),&m\text{ is even}\\ \frac{\sqrt{2}}4(1 + \sqrt{8m+1}),&m\text{ is odd}\\\end{cases} $$ it (probably) holds $$ \beta(m) = \|M_m\|_2. $$ Again, validation for $m=2,\dots,8$ via Mathematica: mM[m_] := Table[Cos[(i j)/m Pi], {i, 0, m}, {j, 0, m}] beta[m_] :=   If [EvenQ[m], (1 + Sqrt[4 m + 1])/2, (1 + Sqrt[8 m + 1]) Sqrt[2]/4] Table[Norm[mM[m]] - beta[m], {m, 2, 8}] // FullSimplify","I am considering the following matrix: $$ M_m = \begin{bmatrix} \cos\bigl(\tfrac{0\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{0\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{0\cdot m}{m}\pi\bigr) \\ \cos\bigl(\tfrac{1\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{1\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{1\cdot m}{m}\pi\bigr) \\ \vdots & \vdots & \ddots & \vdots \\ \cos\bigl(\tfrac{m\cdot 0}{m}\pi\bigr) & \cos\bigl(\tfrac{m\cdot 1}{m}\pi\bigr) & \dots & \cos\bigl(\tfrac{m\cdot m}{m}\pi\bigr) \\ \end{bmatrix} \in \mathbb R^{(m+1)\times(m+1)} $$ A short notation would be $$M_m = \Bigl(\cos\bigl(\tfrac{ij}{m}\pi\bigr)\Bigr)_{i,j=0}^m.$$ I want to calculate the spectral norm $\|M_m\|_2$. Since $M_m$ is symmetric, one could investigate its Eigenvalues, but I could not come up with anything. Maybe one can come up with a clever vector $v$ with $\|v\|_2 = 1$, where it can be seen that it maximizes $\|M_m\cdot v\|_2$? I already saw that numerically, the norm $\|M_m\|_2$ seems to grow like $\sqrt m$, so I suppose in the algebraic expression for $\|M_m\|_2$ there should be something like $\sqrt m$. I also saw that I may be a good idea to consider $m=2k$ and $m=2k+1$ differently, since it seems the norm is jumping around between two slightly different functions. I would conjecture that for $$ \alpha(m) = \begin{cases}\frac1{\sqrt2}\sqrt{2m+1 + \sqrt{4m+1}},&m\text{ is even}\\ \frac12\sqrt{4m+1 + \sqrt{8m+1}},&m\text{ is odd}\\\end{cases} $$ it holds $$ \alpha(m) = \|M_m\|_2. $$ I know that it is true for $m=2,\dots,8$, because of this Mathematica Code: mM[m_] := Table[Cos[(i j)/m Pi], {i, 0, m}, {j, 0, m}] alpha[m_] := If[EvenQ[m],   Sqrt[(2 m + 1 + Sqrt[4 m + 1])/2],   Sqrt[ 4 m + 1 + Sqrt[8 m + 1]]/2  ] Table[Norm[mM[m]] - alpha[m], {m, 2, 8}] The conjecture can be formulated a bit different and easier: For $$ \beta(m) = \begin{cases}\frac12(1 + \sqrt{4m+1}),&m\text{ is even}\\ \frac{\sqrt{2}}4(1 + \sqrt{8m+1}),&m\text{ is odd}\\\end{cases} $$ it (probably) holds $$ \beta(m) = \|M_m\|_2. $$ Again, validation for $m=2,\dots,8$ via Mathematica: mM[m_] := Table[Cos[(i j)/m Pi], {i, 0, m}, {j, 0, m}] beta[m_] :=   If [EvenQ[m], (1 + Sqrt[4 m + 1])/2, (1 + Sqrt[8 m + 1]) Sqrt[2]/4] Table[Norm[mM[m]] - beta[m], {m, 2, 8}] // FullSimplify",,"['matrices', 'trigonometry', 'spectral-norm']"
15,How to find smallest enclosing ellipse when multiple lines are given ? This ellipse needs to intersect all those lines,How to find smallest enclosing ellipse when multiple lines are given ? This ellipse needs to intersect all those lines,,"I'm trying to find the smallest ellipse in terms of circumference. I suspect the smallest enclosing ellipse will intersect some lines in one single point. Given a line l: px + qy + r = 0 , L : { p  q  r } I can try to find this ellipse by using the following equation: ap 2 n + bp n q n + cq 2 n + dp n r n + eq n r n + fr 2 n = 0. Whereby n represents the number of lines. Using multiple lines I can find the values of a ... f . Later I can use the following equation K: LBL T = 0 , whereby B contains the parameters a ... f . I was actually wondering if there was a simpler or more elegant way to solve this problem.","I'm trying to find the smallest ellipse in terms of circumference. I suspect the smallest enclosing ellipse will intersect some lines in one single point. Given a line l: px + qy + r = 0 , L : { p  q  r } I can try to find this ellipse by using the following equation: ap 2 n + bp n q n + cq 2 n + dp n r n + eq n r n + fr 2 n = 0. Whereby n represents the number of lines. Using multiple lines I can find the values of a ... f . Later I can use the following equation K: LBL T = 0 , whereby B contains the parameters a ... f . I was actually wondering if there was a simpler or more elegant way to solve this problem.",,"['matrices', 'geometry', 'trigonometry', 'conic-sections']"
16,Matrix exponential of the sum of two skew-symmetric matrices,Matrix exponential of the sum of two skew-symmetric matrices,,"This is my first message in this site. I'm a mechanical engineer with, amongst others, an interest in inertial navigation. I'm currently reading the book ""Principles of GNSS, Inertial and Multisensor Integrated Navigation Systems"" from Paul D. Groves. On the development of one equation (eq. 5.26 in the second edition of the book) I came across this step: $$C_b^e (t) \exp⁡[(Ω_{ib}^b-Ω_{ie}^b ) τ_i ]=C_b^e (t) \exp⁡(Ω_{ib}^b τ_i )-C_b^e (t)[\exp⁡(Ω_{ie}^b τ_i )-I_3 ]$$ The $C$s are rotation matrices. The $\Omega$s are the skew-symmetric matrices associated to rotation velocities between two reference systems. The sub- and superindexes $i$, $e$ and $b$ refer to $inertial$, $earth$ and $body$ reference systems. $t$ is time, $\tau_i$ is the time lapse between two sample/integration steps. What I can't understand is how the matrix exponential of the difference of two matrices can convert in the difference of the matrix exponentials of those two matrices (plus the identity matrix). I've seen this operation in several places on the book, so I doubt it's an erratum. I guess one might prove it by means of the series expansion of the matrix exponential function and some property of skew-symmetric matrices, but I didn't get very far this way. Can somebody please cast some light on this step? Thanks in advance! Greetings Guillermo","This is my first message in this site. I'm a mechanical engineer with, amongst others, an interest in inertial navigation. I'm currently reading the book ""Principles of GNSS, Inertial and Multisensor Integrated Navigation Systems"" from Paul D. Groves. On the development of one equation (eq. 5.26 in the second edition of the book) I came across this step: $$C_b^e (t) \exp⁡[(Ω_{ib}^b-Ω_{ie}^b ) τ_i ]=C_b^e (t) \exp⁡(Ω_{ib}^b τ_i )-C_b^e (t)[\exp⁡(Ω_{ie}^b τ_i )-I_3 ]$$ The $C$s are rotation matrices. The $\Omega$s are the skew-symmetric matrices associated to rotation velocities between two reference systems. The sub- and superindexes $i$, $e$ and $b$ refer to $inertial$, $earth$ and $body$ reference systems. $t$ is time, $\tau_i$ is the time lapse between two sample/integration steps. What I can't understand is how the matrix exponential of the difference of two matrices can convert in the difference of the matrix exponentials of those two matrices (plus the identity matrix). I've seen this operation in several places on the book, so I doubt it's an erratum. I guess one might prove it by means of the series expansion of the matrix exponential function and some property of skew-symmetric matrices, but I didn't get very far this way. Can somebody please cast some light on this step? Thanks in advance! Greetings Guillermo",,"['matrices', 'rotations', 'matrix-exponential', 'kinematics', 'skew-symmetric-matrices']"
17,Conditions for Trace Inequality,Conditions for Trace Inequality,,"Consider the $M \times M$ complex positive semidefinite matrices ${\bf A}, {\bf B}, {\bf Z}$. We have the relation $\mu_{\text{max}}{\bf I} \succeq {\bf A} \succeq {\bf B} \succ \mu_{\text{min}}{\bf I} $, i.e. both are nonsingular and the eigenvalues are bounded by $\mu_{\text{max}}$ and $\mu_{\text{min}}$. Further assume $\bf Z = {\bf U} {\bf U}^H$ for some complex $M \times N$ matrix $\bf U$, so $\bf Z$ might be singular. I need to find under what conditions it holds that  $$\left\| {\bf A} \bf{U} \right\|_F^2 \geq \left\| {\bf B} \bf{U} \right\|_F^2$$ or equivalently $$\text{Tr}\left( \left( {\bf A}^2 - {\bf B}^2 \right) {\bf Z}\right) \geq 0 . \tag{1}$$ Intuitively one might say that (1) is fulfilled if ${\bf A} \succeq {\bf B} \succ {\bf 0}$ holds. This intuition will (usually) also be confirmed by numerical tests with random positive semidefinite matrices ${\bf A}, {\bf B}, {\bf Z}$. But inequality (1) is not true in general. Since the matrix square is not operator monotone we have $$ {\bf A} \succeq {\bf B} \quad \rightarrow \quad {\bf A}^2 \nsucceq {\bf B}^2. $$ For a counter example, assume that ${\bf A}^2 - {\bf B}^2$ has some negative eigenvalue $\lambda({\bf A}^2 - {\bf B}^2) < 0$ with corresponding eigenvector $\bf u$. If matrix $\bf Z$ is chosen as ${\bf Z} = {\bf u} \bf{u}^H$ then we have $$\text{Tr}\left( \left( {\bf A}^2 - {\bf B}^2 \right) {\bf Z}\right) < 0$$ i.e., inequality (1) is not fulfilled. So I wonder if we can make any statements on the trace inequality if we only know the bounds $\mu_{\text{max}}$ and $\mu_{\text{min}}$ on the eigenvalues of ${\bf A}$ and ${\bf B}$ and the eigenvalues of ${\bf Z}$. My current approach is as follows: Define ${\bf C} = {\bf A}^2 - {\bf B}^2 \nsucceq {\bf 0}$ which is a hermitian matrix with real eigenvalues. Further define the sorted eigenvalues of $\bf C$ and $\bf Z$ as  $$\lambda_1({\bf C}) \geq \ldots \geq \lambda_M({\bf C}) \quad \text{ and } \quad \lambda_1({\bf Z}) \geq \ldots \geq \lambda_M({\bf Z})\geq 0.\tag{2}$$ Then we can bound the trace in (1) as follows $$ \sum_{m=1}^M \lambda_m ({\bf C}) \; \lambda_{M-m+1} ({\bf Z}) \leq  \text{Tr} ( {\bf C} {\bf Z} ) \leq \sum_{m=1}^M \lambda_m ({\bf C}) \; \lambda_{m} ({\bf Z}) \tag{3} $$ (see ""A Trace Inequality for Matrix Product"" by Lasserre). Also, since $$ \text{Tr} \left( {\bf A}^2 - {\bf B}^2 \right) = \text{Tr} \left( \left( {\bf A} + {\bf B} \right) \left( {\bf A} - {\bf B} \right) \right) \geq 0 \tag{4} $$  we know that the sum of eigenvalues is nonnegative. Further, by Weyl's theorem we can bound the eigenvalues of $\bf{C}$ as $$  \mu_{\text{min}}^2 - \mu_{\text{max}}^2  \leq \lambda_M^2( {\bf A} ) - \lambda_1^2({\bf B})  \leq \lambda_M( {\bf A}^2 - {\bf B}^2 )  \leq \; ... \quad\quad\quad \\ \quad\quad\quad ... \; \leq \lambda_1( {\bf A}^2 - {\bf B}^2 )  \leq \lambda_1^2( {\bf A} ) - \lambda_M^2({\bf B})   \leq \mu_{\text{max}}^2 - \mu_{\text{min}}^2  \tag{5} $$ where the lower (more interesting) bound is quite loose. From equations (2)-(5) we can form a set of linear inequalities. For better illustration assume that $M=4$. Then we have $$ \left[ \begin{matrix} -\lambda_4({\bf Z}) & -\lambda_3({\bf Z}) & -\lambda_2({\bf Z}) & -\lambda_1({\bf Z}) \\ 1 & 1 & 1 & 1 \\ 1 & -1 & 0 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & -1 \\ -1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right] \left[  \begin{matrix} \lambda_1({\bf C}) \\ \lambda_2({\bf C}) \\ \lambda_3({\bf C}) \\ \lambda_4({\bf C})  \end{matrix} \right] \geq \left[  \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \mu_{\text{min}}^2 - \mu_{\text{max}}^2  \\ \mu_{\text{min}}^2 - \mu_{\text{max}}^2 \end{matrix} \right] \tag{6} $$ where the 1st ineqaulity stems from (3), the 2nd stems from (4), 3rd to 5th stem from (2) and 6th and 7th stem from (5). If the ineqaulities in (6) describe an empty set for eigenvalues $\lambda_m({\bf C})$, then inequality (1) must be true. Farka's lemma can be used to see if (6) is infeasible. I get good results if I have a good lower bound for (5), but the lower bound by Weyl's theorem is too loose. Are there any better lower bounds than Weyl's theorem?","Consider the $M \times M$ complex positive semidefinite matrices ${\bf A}, {\bf B}, {\bf Z}$. We have the relation $\mu_{\text{max}}{\bf I} \succeq {\bf A} \succeq {\bf B} \succ \mu_{\text{min}}{\bf I} $, i.e. both are nonsingular and the eigenvalues are bounded by $\mu_{\text{max}}$ and $\mu_{\text{min}}$. Further assume $\bf Z = {\bf U} {\bf U}^H$ for some complex $M \times N$ matrix $\bf U$, so $\bf Z$ might be singular. I need to find under what conditions it holds that  $$\left\| {\bf A} \bf{U} \right\|_F^2 \geq \left\| {\bf B} \bf{U} \right\|_F^2$$ or equivalently $$\text{Tr}\left( \left( {\bf A}^2 - {\bf B}^2 \right) {\bf Z}\right) \geq 0 . \tag{1}$$ Intuitively one might say that (1) is fulfilled if ${\bf A} \succeq {\bf B} \succ {\bf 0}$ holds. This intuition will (usually) also be confirmed by numerical tests with random positive semidefinite matrices ${\bf A}, {\bf B}, {\bf Z}$. But inequality (1) is not true in general. Since the matrix square is not operator monotone we have $$ {\bf A} \succeq {\bf B} \quad \rightarrow \quad {\bf A}^2 \nsucceq {\bf B}^2. $$ For a counter example, assume that ${\bf A}^2 - {\bf B}^2$ has some negative eigenvalue $\lambda({\bf A}^2 - {\bf B}^2) < 0$ with corresponding eigenvector $\bf u$. If matrix $\bf Z$ is chosen as ${\bf Z} = {\bf u} \bf{u}^H$ then we have $$\text{Tr}\left( \left( {\bf A}^2 - {\bf B}^2 \right) {\bf Z}\right) < 0$$ i.e., inequality (1) is not fulfilled. So I wonder if we can make any statements on the trace inequality if we only know the bounds $\mu_{\text{max}}$ and $\mu_{\text{min}}$ on the eigenvalues of ${\bf A}$ and ${\bf B}$ and the eigenvalues of ${\bf Z}$. My current approach is as follows: Define ${\bf C} = {\bf A}^2 - {\bf B}^2 \nsucceq {\bf 0}$ which is a hermitian matrix with real eigenvalues. Further define the sorted eigenvalues of $\bf C$ and $\bf Z$ as  $$\lambda_1({\bf C}) \geq \ldots \geq \lambda_M({\bf C}) \quad \text{ and } \quad \lambda_1({\bf Z}) \geq \ldots \geq \lambda_M({\bf Z})\geq 0.\tag{2}$$ Then we can bound the trace in (1) as follows $$ \sum_{m=1}^M \lambda_m ({\bf C}) \; \lambda_{M-m+1} ({\bf Z}) \leq  \text{Tr} ( {\bf C} {\bf Z} ) \leq \sum_{m=1}^M \lambda_m ({\bf C}) \; \lambda_{m} ({\bf Z}) \tag{3} $$ (see ""A Trace Inequality for Matrix Product"" by Lasserre). Also, since $$ \text{Tr} \left( {\bf A}^2 - {\bf B}^2 \right) = \text{Tr} \left( \left( {\bf A} + {\bf B} \right) \left( {\bf A} - {\bf B} \right) \right) \geq 0 \tag{4} $$  we know that the sum of eigenvalues is nonnegative. Further, by Weyl's theorem we can bound the eigenvalues of $\bf{C}$ as $$  \mu_{\text{min}}^2 - \mu_{\text{max}}^2  \leq \lambda_M^2( {\bf A} ) - \lambda_1^2({\bf B})  \leq \lambda_M( {\bf A}^2 - {\bf B}^2 )  \leq \; ... \quad\quad\quad \\ \quad\quad\quad ... \; \leq \lambda_1( {\bf A}^2 - {\bf B}^2 )  \leq \lambda_1^2( {\bf A} ) - \lambda_M^2({\bf B})   \leq \mu_{\text{max}}^2 - \mu_{\text{min}}^2  \tag{5} $$ where the lower (more interesting) bound is quite loose. From equations (2)-(5) we can form a set of linear inequalities. For better illustration assume that $M=4$. Then we have $$ \left[ \begin{matrix} -\lambda_4({\bf Z}) & -\lambda_3({\bf Z}) & -\lambda_2({\bf Z}) & -\lambda_1({\bf Z}) \\ 1 & 1 & 1 & 1 \\ 1 & -1 & 0 & 0 \\ 0 & 1 & -1 & 0 \\ 0 & 0 & 1 & -1 \\ -1 & 0 & 0 & 0\\ 0 & 0 & 0 & 1 \\ \end{matrix} \right] \left[  \begin{matrix} \lambda_1({\bf C}) \\ \lambda_2({\bf C}) \\ \lambda_3({\bf C}) \\ \lambda_4({\bf C})  \end{matrix} \right] \geq \left[  \begin{matrix} 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \mu_{\text{min}}^2 - \mu_{\text{max}}^2  \\ \mu_{\text{min}}^2 - \mu_{\text{max}}^2 \end{matrix} \right] \tag{6} $$ where the 1st ineqaulity stems from (3), the 2nd stems from (4), 3rd to 5th stem from (2) and 6th and 7th stem from (5). If the ineqaulities in (6) describe an empty set for eigenvalues $\lambda_m({\bf C})$, then inequality (1) must be true. Farka's lemma can be used to see if (6) is infeasible. I get good results if I have a good lower bound for (5), but the lower bound by Weyl's theorem is too loose. Are there any better lower bounds than Weyl's theorem?",,"['matrices', 'inequality', 'trace']"
18,What is the explicit formula (solution) to this recursively defined binary matrix?,What is the explicit formula (solution) to this recursively defined binary matrix?,,"My question concerns the following binary matrix (call it matrix $A$). Or rather the entire family of such matrices, for some number of columns $n$ and rows $2^n$. The ellipses indicate that the pattern continues, not that the matrix is infinite. Matrix A : $$         \begin{matrix}         1 & 1 & 1 & 1\cdots\\         0 & 1 & 1 & 1\cdots\\         1 & 0 & 1 & 1\cdots\\ 0 & 0 & 1 & 1\cdots\\ 1 & 1 & 0 & 1\cdots\\ 0 & 1 & 0 & 1\cdots\\ 1 & 0 & 0 & 1\cdots\\ 0 & 0 & 0 & 1\cdots\\ 1 & 1 & 1 & 1\cdots\\         0 & 1 & 1 & 0\cdots\\         1 & 0 & 1 & 0\cdots\\ 0 & 0 & 1 & 0\cdots\\ 1 & 1 & 0 & 0\cdots\\ 0 & 1 & 0 & 0\cdots\\ 1 & 0 & 0 & 0\cdots\\ 0 & 0 & 0 & 0\cdots\\ \vdots & \vdots & \vdots &\vdots         \end{matrix} $$ I see such matrices (arrays) as rather mathematically interesting objects, which can be said to represent a number of things, e.g. it's rows can be taken to be in the images of characteristic functions of all the combinations on an $n$ element set; or as all the valuations on $n$ propositional variables, in a logical context. This is just naming a few canonical interpretations of $A$. Anyone who's done any introductory logic, discrete mathematics, or computer science would have seen such matrices. It's columns are defined in a simple (as a recurrence relation?) manner. What interested me for sometime, was an explicit, functional representation of this matrix $A$, or it's sibling $B$ (with the $0$'s and $1$'s swapped). That is, I couldn't find anywhere the function: $$\theta: \mathbb N \times \mathbb N \rightarrow \{0,1\}$$ Such that on row-number and column-number input, it gives the matrix value for that row-column coordinate. Not succeeding in finding such a function, I decided to derive one myself (see below), for which I have a proof here . It's not the cleanest proof, but most of it is captured in a diagram that illustrates the key pattern, which makes it easier to follow. I'd appreciate any comments to the proof, or corrections if an error is spotted, or an obvious simple solution, which I missed, although that's not my main question here. The row ""coordinate"" is denoted with $k$ and the column one with $i$. $$\theta(k,i)= \left\lfloor \frac {k-1}{2^{i-1}} \right\rfloor - 2\left\lfloor \frac {k-1}{2^{i}}+\frac 12 \right\rfloor +1$$ The solution to the sibling matrix $B$ turns out to have a simpler form (I think this is correct, but I'm not entirely sure): $$\theta(k,i)= \left\lfloor \frac {k-1}{2^{i-1}} \right\rfloor - 2\left\lfloor \frac {k-1}{2^{i}} \right\rfloor $$ My question is this: where can I find a solution to this (perhaps someone can give a simpler one), or results that have this solution as an immediate corollary? Also, where could I read-up more about the properties of such matrices? For I can't believe that such solutions don't exist already, given the interesting nature of such mathematical objects. I suspect that the above solutions are special cases of a more general result; even more general than merely extending the image of $\theta$ to $\{0,1,...,n\}$ for all $n \in \mathbb N$, which shouldn't be a difficult exercise (I actually have that extended solution somewhere).","My question concerns the following binary matrix (call it matrix $A$). Or rather the entire family of such matrices, for some number of columns $n$ and rows $2^n$. The ellipses indicate that the pattern continues, not that the matrix is infinite. Matrix A : $$         \begin{matrix}         1 & 1 & 1 & 1\cdots\\         0 & 1 & 1 & 1\cdots\\         1 & 0 & 1 & 1\cdots\\ 0 & 0 & 1 & 1\cdots\\ 1 & 1 & 0 & 1\cdots\\ 0 & 1 & 0 & 1\cdots\\ 1 & 0 & 0 & 1\cdots\\ 0 & 0 & 0 & 1\cdots\\ 1 & 1 & 1 & 1\cdots\\         0 & 1 & 1 & 0\cdots\\         1 & 0 & 1 & 0\cdots\\ 0 & 0 & 1 & 0\cdots\\ 1 & 1 & 0 & 0\cdots\\ 0 & 1 & 0 & 0\cdots\\ 1 & 0 & 0 & 0\cdots\\ 0 & 0 & 0 & 0\cdots\\ \vdots & \vdots & \vdots &\vdots         \end{matrix} $$ I see such matrices (arrays) as rather mathematically interesting objects, which can be said to represent a number of things, e.g. it's rows can be taken to be in the images of characteristic functions of all the combinations on an $n$ element set; or as all the valuations on $n$ propositional variables, in a logical context. This is just naming a few canonical interpretations of $A$. Anyone who's done any introductory logic, discrete mathematics, or computer science would have seen such matrices. It's columns are defined in a simple (as a recurrence relation?) manner. What interested me for sometime, was an explicit, functional representation of this matrix $A$, or it's sibling $B$ (with the $0$'s and $1$'s swapped). That is, I couldn't find anywhere the function: $$\theta: \mathbb N \times \mathbb N \rightarrow \{0,1\}$$ Such that on row-number and column-number input, it gives the matrix value for that row-column coordinate. Not succeeding in finding such a function, I decided to derive one myself (see below), for which I have a proof here . It's not the cleanest proof, but most of it is captured in a diagram that illustrates the key pattern, which makes it easier to follow. I'd appreciate any comments to the proof, or corrections if an error is spotted, or an obvious simple solution, which I missed, although that's not my main question here. The row ""coordinate"" is denoted with $k$ and the column one with $i$. $$\theta(k,i)= \left\lfloor \frac {k-1}{2^{i-1}} \right\rfloor - 2\left\lfloor \frac {k-1}{2^{i}}+\frac 12 \right\rfloor +1$$ The solution to the sibling matrix $B$ turns out to have a simpler form (I think this is correct, but I'm not entirely sure): $$\theta(k,i)= \left\lfloor \frac {k-1}{2^{i-1}} \right\rfloor - 2\left\lfloor \frac {k-1}{2^{i}} \right\rfloor $$ My question is this: where can I find a solution to this (perhaps someone can give a simpler one), or results that have this solution as an immediate corollary? Also, where could I read-up more about the properties of such matrices? For I can't believe that such solutions don't exist already, given the interesting nature of such mathematical objects. I suspect that the above solutions are special cases of a more general result; even more general than merely extending the image of $\theta$ to $\{0,1,...,n\}$ for all $n \in \mathbb N$, which shouldn't be a difficult exercise (I actually have that extended solution somewhere).",,"['matrices', 'elementary-number-theory', 'logic', 'recurrence-relations', 'binary']"
19,Representing Matrix Subgroups,Representing Matrix Subgroups,,"Suppose I wanted to describe the subgroup of $GL_n(\mathbb{R})$ of matrices of the form $$ \left [ \begin{array}{cc} A & B \\ 0 & C \\ \end{array} \right ] $$ where $A \in GL_k(\mathbb{R}),\; C \in GL_{n-k}(\mathbb{R})$ and say for argument $B \in M_{k, n-k}(\mathbb{R})$.  What is the proper way to express what this subgroup is using the the other groups $GL_k(\mathbb{R}), GL_{n-k}(\mathbb{R})$, and $M_{k,n-k}(\mathbb{R})$?  For instance, I know that the special Euclidean group has the form $SE(n) = SO(n) \ltimes \mathbb{R}^n$, but I'm not sure where this derivation came from. Ultimately I'd like to be able to recreate simple descriptions for matrix subgroups of this type.  Ultimately my goal is to take quotients of certain matrix Lie groups, so I'd like to know when subgroups of this type can be represented as direct or semi-direct products of certain subgroups. Reference requests and full explanations are welcome.","Suppose I wanted to describe the subgroup of $GL_n(\mathbb{R})$ of matrices of the form $$ \left [ \begin{array}{cc} A & B \\ 0 & C \\ \end{array} \right ] $$ where $A \in GL_k(\mathbb{R}),\; C \in GL_{n-k}(\mathbb{R})$ and say for argument $B \in M_{k, n-k}(\mathbb{R})$.  What is the proper way to express what this subgroup is using the the other groups $GL_k(\mathbb{R}), GL_{n-k}(\mathbb{R})$, and $M_{k,n-k}(\mathbb{R})$?  For instance, I know that the special Euclidean group has the form $SE(n) = SO(n) \ltimes \mathbb{R}^n$, but I'm not sure where this derivation came from. Ultimately I'd like to be able to recreate simple descriptions for matrix subgroups of this type.  Ultimately my goal is to take quotients of certain matrix Lie groups, so I'd like to know when subgroups of this type can be represented as direct or semi-direct products of certain subgroups. Reference requests and full explanations are welcome.",,"['abstract-algebra', 'matrices', 'lie-groups', 'semidirect-product']"
20,Proving generalized Cassini's identity using determinant?,Proving generalized Cassini's identity using determinant?,,"Motivation It is not hard to show, by using the general solution, that Proposition. If $(a_{n})_{n\in\Bbb{Z}}$ satisfies the recursive formula $ a_{n+2} = pa_{n+1} + qa_{n}$ , then for any $n, i, j$ we have $$ a_{n+i}a_{n+j} - a_{n+i+j}a_{n} = (-q)^{n}(a_{i}a_{j} - a_{i+j}a_{0}). \tag{*}$$ Indeed when $a_{n} = F_{n}$ and $i = j = 1$ , (*) reduces to the Cassini's identity . But by noting that the Cassini's identity is also proved by taking determinant of the identity $$ \begin{pmatrix}1 & 1 \\ 1 & 0 \end{pmatrix}^{n} = \begin{pmatrix} F_{n+1} & F_{n} \\ F_{n} & F_{n-1} \end{pmatrix}, \tag{1} $$ I suspect that (*) may also be proved in a similar way which does not rely on the knowledge of the general solution. Meditating on the identity (*), it is quite tempting to consider the matrix $$ A_{n} = \begin{pmatrix} a_{n+i+j} & a_{n+i} \\ a_{n+j} & a_{n} \end{pmatrix} $$ as (*) is now read as $\det A_{n} = (-q)^{n}\det A_{0}$ . And in view of the proof of the Cassini's identity, we would like to claim that $$ A_{n} = \begin{pmatrix} p & q \\ 1 & 0 \end{pmatrix}^{n} A_{0}. $$ But this is not true except for some special case (such as $i = j = 1$ ). And neither I was able to find an alternative matrix relation that produces (*). Question So my question as follows: Is there any matrix relation that is analogous to (1) and produces (*)?","Motivation It is not hard to show, by using the general solution, that Proposition. If satisfies the recursive formula , then for any we have Indeed when and , (*) reduces to the Cassini's identity . But by noting that the Cassini's identity is also proved by taking determinant of the identity I suspect that (*) may also be proved in a similar way which does not rely on the knowledge of the general solution. Meditating on the identity (*), it is quite tempting to consider the matrix as (*) is now read as . And in view of the proof of the Cassini's identity, we would like to claim that But this is not true except for some special case (such as ). And neither I was able to find an alternative matrix relation that produces (*). Question So my question as follows: Is there any matrix relation that is analogous to (1) and produces (*)?","(a_{n})_{n\in\Bbb{Z}}  a_{n+2} = pa_{n+1} + qa_{n} n, i, j  a_{n+i}a_{n+j} - a_{n+i+j}a_{n} = (-q)^{n}(a_{i}a_{j} - a_{i+j}a_{0}). \tag{*} a_{n} = F_{n} i = j = 1  \begin{pmatrix}1 & 1 \\ 1 & 0 \end{pmatrix}^{n} = \begin{pmatrix} F_{n+1} & F_{n} \\ F_{n} & F_{n-1} \end{pmatrix}, \tag{1}   A_{n} = \begin{pmatrix} a_{n+i+j} & a_{n+i} \\ a_{n+j} & a_{n} \end{pmatrix}  \det A_{n} = (-q)^{n}\det A_{0}  A_{n} = \begin{pmatrix} p & q \\ 1 & 0 \end{pmatrix}^{n} A_{0}.  i = j = 1","['matrices', 'determinant', 'fibonacci-numbers']"
21,Clifford algebra - Gamma matrices,Clifford algebra - Gamma matrices,,"Let's say we have $\gamma^{a}$ matrices $(a=1,2,...,D)$. They satisfy the following condition $$\gamma^{a}\gamma^{b}+\gamma^{b}\gamma^{a}=2\delta^{ab}I^{N\times N}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ all gamma matrices are $N\times N$. Then we denote $$ \Gamma^{A} : \big\{I,\gamma^{a_{1}},\gamma^{a_{1}a_{2}},\gamma^{a_{1}a_{2}a_{3}},...,\gamma^{a_{1}a_{2}a_{3}...a_{D}} \big\} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ where $\gamma^{a_{1}a_{2}...a_{n}}=\gamma^{a_{1}}\gamma^{a_{2}} ...\gamma^{a_{n}}$ and $a_{1}<a_{2}<...<a_{D}$.  The total number of $\Gamma^{A}$ matrices is $2^{D}$. Also denote $\tilde{\gamma}=\gamma^{1}\gamma^{2}...\gamma^{D}$. It's easy to derive the following formula: if $\Gamma^{A}=\gamma^{a_{1}a_{2}...a_{n}}$ then $\displaystyle\sum_{a=1}^{D}\gamma^{a}\Gamma^{A}\gamma^{a}=(-1)^{n}(D-2n)\Gamma^{A}$. If we take the trace of this expression we get $[D-(-1)^{n}(D-2n)]Tr(\Gamma^{A})=0$ from where we conclude $Tr(\Gamma^{A})=0$   if $\Gamma^{A}\neq I,\tilde{\gamma} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(3)$ $Tr(\tilde{\gamma})=0$ if $D$ is even $\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(4)$ Questions 1) Formulas (3) and (4) do not give us any information about the trace of $\tilde{\gamma}$ matrix in odd dimension. How to prove that $Tr(\tilde{\gamma})\neq 0$ if $D$ is odd? 2) I found in many books that $\Gamma^{A}$ matrices are liner independent and they form basis. This means $$\displaystyle\sum_{A=1}^{2^{D}}x_{A}\Gamma^{A}=0$$ holds if and only if $x_{A}=0$. We can prove that fact using $Tr(\Gamma^{A})=0$. But we know that if $D$ is odd  $Tr(\tilde{\gamma})\neq 0$. So, here is my question: Is it true that $\Gamma^{A}$ matrices are linear independent? If it is true, how to prove that in odd dimension? 3) We take $N\times N$ matrices $\gamma^{a}$, with $a=1,2,...,D$. How can we express $D$ with $N$? If we fix $D$ what is the minimal dimension of gamma matrices? And how to prove that?","Let's say we have $\gamma^{a}$ matrices $(a=1,2,...,D)$. They satisfy the following condition $$\gamma^{a}\gamma^{b}+\gamma^{b}\gamma^{a}=2\delta^{ab}I^{N\times N}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ all gamma matrices are $N\times N$. Then we denote $$ \Gamma^{A} : \big\{I,\gamma^{a_{1}},\gamma^{a_{1}a_{2}},\gamma^{a_{1}a_{2}a_{3}},...,\gamma^{a_{1}a_{2}a_{3}...a_{D}} \big\} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ where $\gamma^{a_{1}a_{2}...a_{n}}=\gamma^{a_{1}}\gamma^{a_{2}} ...\gamma^{a_{n}}$ and $a_{1}<a_{2}<...<a_{D}$.  The total number of $\Gamma^{A}$ matrices is $2^{D}$. Also denote $\tilde{\gamma}=\gamma^{1}\gamma^{2}...\gamma^{D}$. It's easy to derive the following formula: if $\Gamma^{A}=\gamma^{a_{1}a_{2}...a_{n}}$ then $\displaystyle\sum_{a=1}^{D}\gamma^{a}\Gamma^{A}\gamma^{a}=(-1)^{n}(D-2n)\Gamma^{A}$. If we take the trace of this expression we get $[D-(-1)^{n}(D-2n)]Tr(\Gamma^{A})=0$ from where we conclude $Tr(\Gamma^{A})=0$   if $\Gamma^{A}\neq I,\tilde{\gamma} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(3)$ $Tr(\tilde{\gamma})=0$ if $D$ is even $\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(4)$ Questions 1) Formulas (3) and (4) do not give us any information about the trace of $\tilde{\gamma}$ matrix in odd dimension. How to prove that $Tr(\tilde{\gamma})\neq 0$ if $D$ is odd? 2) I found in many books that $\Gamma^{A}$ matrices are liner independent and they form basis. This means $$\displaystyle\sum_{A=1}^{2^{D}}x_{A}\Gamma^{A}=0$$ holds if and only if $x_{A}=0$. We can prove that fact using $Tr(\Gamma^{A})=0$. But we know that if $D$ is odd  $Tr(\tilde{\gamma})\neq 0$. So, here is my question: Is it true that $\Gamma^{A}$ matrices are linear independent? If it is true, how to prove that in odd dimension? 3) We take $N\times N$ matrices $\gamma^{a}$, with $a=1,2,...,D$. How can we express $D$ with $N$? If we fix $D$ what is the minimal dimension of gamma matrices? And how to prove that?",,"['matrices', 'representation-theory', 'clifford-algebras']"
22,Which of the statements are true if $ABCDE = I$?,Which of the statements are true if ?,ABCDE = I,"Let $A,B,C,D,E$ be five real square matrices of the same order such that $ABCDE = I$, where $I$ is the unit matrix. Then which of the following are true? (A) $B^{−1}A^{−1}= EDC$ (B) $BA$ is a nonsingular matrix (C) $ABC$ commutes with $DE$ (D) $ABCD = {1\over detE}$ Adj $E$.","Let $A,B,C,D,E$ be five real square matrices of the same order such that $ABCDE = I$, where $I$ is the unit matrix. Then which of the following are true? (A) $B^{−1}A^{−1}= EDC$ (B) $BA$ is a nonsingular matrix (C) $ABC$ commutes with $DE$ (D) $ABCD = {1\over detE}$ Adj $E$.",,['matrices']
23,Matrix Help: Combinations,Matrix Help: Combinations,,"Given a 10 by 10 matrix filled with 0s and 1s, how many possible outcomes are there? It sounds easy enough as a combination of $2^{100}$. The kicker to the question is there MUST be exactly five 1's in every row and every column. Given this is an extra credit school assignment I understand if no one wishes to help but I would like to gain an understanding of the mathematical process","Given a 10 by 10 matrix filled with 0s and 1s, how many possible outcomes are there? It sounds easy enough as a combination of $2^{100}$. The kicker to the question is there MUST be exactly five 1's in every row and every column. Given this is an extra credit school assignment I understand if no one wishes to help but I would like to gain an understanding of the mathematical process",,"['matrices', 'combinations']"
24,What does the metric matrix G tell us here,What does the metric matrix G tell us here,,Let $\phi:U \rightarrow S \subseteq \mathbb{R}^3$ be a chart from $U \subseteq \mathbb{R}^2$ to a surface $S$.  $G = g_{ij}$ be the metric matrix such that $ g_{ij} = \frac{\partial \phi}{\partial x_i} \cdotp \frac{\partial \phi}{\partial x_j}$. What information does $G$ give us? Edit: I have found that if two maps from the same $U$ have the same $G$ then there exists an isometry between them. However this does not increase my geometrical understanding of $G$.,Let $\phi:U \rightarrow S \subseteq \mathbb{R}^3$ be a chart from $U \subseteq \mathbb{R}^2$ to a surface $S$.  $G = g_{ij}$ be the metric matrix such that $ g_{ij} = \frac{\partial \phi}{\partial x_i} \cdotp \frac{\partial \phi}{\partial x_j}$. What information does $G$ give us? Edit: I have found that if two maps from the same $U$ have the same $G$ then there exists an isometry between them. However this does not increase my geometrical understanding of $G$.,,"['matrices', 'geometry', 'surfaces']"
25,Entries of a Haar distributed unitary matrix,Entries of a Haar distributed unitary matrix,,The eigenvector matrix of a Wishart matrix is Haar distributed and that implies that the eigenvectors are uniformly distributed on a sphere. I'm interested to know what is the distribution of individual entries of this haar distributed eigenvector matrix?,The eigenvector matrix of a Wishart matrix is Haar distributed and that implies that the eigenvectors are uniformly distributed on a sphere. I'm interested to know what is the distribution of individual entries of this haar distributed eigenvector matrix?,,"['matrices', 'probability-distributions', 'eigenvalues-eigenvectors', 'random', 'random-matrices']"
26,"Definition and some elementary properties of the ""vector turn map""","Definition and some elementary properties of the ""vector turn map""",,"This is actually a follow up question . I have to apologise for the length of it. I didn't anticipate to be that long. I hope that it will be proved interesting nevertheless. I used the name ""vector turn map"" due to lack of a better alternative. Let $f_A$ be a map defined as following. Choose any $2\times 2$ real matrix $A$. Then take a normalised vector, apply the map on the vector and renormalise it. So $f_A$ takes a normal vector and gives a normal vector, so it can be thought as a map $f_A:S^1\to S^1$. I thought of that because of an error propagation problem I am working on. My idea was to see the asymptotic behaviour of a random error vector. This lead to the Jacobian matrix applied to the error vector multiple times, so I want to see how this vector turns. My problem was over the complex numbers so I was searching for any known results. However after playing a bit with that, I figured out that it won't help me but I thought it is an interesting puzzle. So I thought I should write what I found out and ask for more information. To compute the map, we do the following. Take $\phi\in[0,1)$ to be the coordinate on $S^1$. Construct the vector $[\cos 2\pi \phi,\sin 2\pi \phi]$. Apply the map $A\cdot[\cos 2\pi \phi,\sin 2\pi \phi]$. And finally take the angle of the new vector. This can be written as $$f_A(\phi)=\frac{1}{2\pi}\mbox{arctan2}(A\cdot[\cos 2\pi \phi,\sin 2\pi \phi]).$$ The multiplication and division by $2\pi$ is to ensure that $\phi\in[0,1)$ or rather $\phi\in[-\frac{1}{2},\frac{1}{2})$ depending on the definition on arctan2, it is not important at all. A few things can be told about $f_A$ right away. If $Av=\lambda v$ with $\lambda>0$ then $\phi_\lambda=\frac{1}{2\pi}\mbox{arctan2}(v)$ is a fixed point of $f_A$. Moreover $\phi_\lambda+\frac{1}{2}$ is also a fixed point since $-v$ is also an eigenvector. Also through some calculations if $\lambda_1>\lambda_2>0$ are eigenvalues of $A$ then $\phi_{\lambda_1}$ and $\phi_{\lambda_1}+\frac{1}{2}$ are stable points and $\phi_{\lambda_2}$, $\phi_{\lambda_2}+\frac{1}{2}$ are unstable. This is easy to see by taking the diagonal matrix $D=((1,0),(0,2))$. Then $D\cdot[x,y]=[x,2y]$, so we see that $D\cdot[x,y]$ turns a bit towards the vertical axis if $y\ne0$. If $\lambda_1<\lambda_2<0$, then we see that $f_A(\phi_{\lambda_i})=\phi_{\lambda_i}+\frac{1}{2}$. The situation is similar, but now instead of a fixed point, we have a 2-periodic orbit. The one that corresponds to the eigenvalue with the largest absolute value is stable and the other is unstable. If one eigenvalue is positive and the other is negative, then there is a fixed point and a 2-periodic orbit, with their stability determined by the absolute value of the eigenvalues. If $A$ has only one eigenvector (it is similar to a Jordan block) with positive eigenvalue. Then there are only two fixed points that are stable from one direction and unstable from the other. If the eigenvalue is negative then instead of a fixed point we have a 2-periodic orbit with similar properties. An very useful observation is that similar matrices correspond to ""similar"" maps. Similar in the sense that they have the same number of fixed points (or 2-periodic orbits) with the same stability. I haven't really proven that but with a little thought it becomes almost obvious. I think it can be proven by writing down a couple of commutative diagrams. Now with that said, it is natural to define an other map $g_A$ by taking $\phi\in[0,\frac{1}{2})$. This can be done by using arctan instead of arctan2 (and tweaking the details). Then fixed points and 2-periodic orbits of $f_A$ are fixed points of $g_A$ with the same stability. So in a sense one could say that $g_A$ is a simplification of $f_A$. However this is true only in the $2\times2$ case. The real difference is that $f_A:S^1\to S^1$ but $g_A:\mathbb{P}^1\to \mathbb{P}^1$ ($\mathbb{P}^1$ is the projective line which here is practiacally the same as the circle). By the way the $2\times 2$ case is nice because one can draw pictures. By plugging in the equation in a program like mathematica or matlab, one can get the cobweb diagram and see  the behaviour of the map. The real fun begins when the same is considered in higher dimensions. So let $A$ be an $n\times n$ real matrix. Then by the same procedure one can define $f_A:S^{n-1}\to S^{n-1}$. Again eigenvectors give fixed points or 2-periodic orbits. Based on intuition I would expect that that the stability is determined by the eigenvalue, so the largest eigenvalues (in absolute value) will correspond to a stable point, the smallest to an unstable and the intermediate to some kind of saddles. Now we also have $g_A:\mathbb{P}^n\to \mathbb{P}^n$, so now the difference is rather significant. A conjugacy between $f_A$ and $g_A$ is expected, as in the $2\times 2$ case, but I would expect some complications. Finally it would be also interesting to check complex cases of the map for their behaviour. One interesting question for example is whether the $4\times 4$ real case actually contains the $2\times 2$ complex one. Anyway, I would hope that this is something that is known and that people have worked on it. If not I would be interested to anything that anyone finds. I just put it here as a mathematical curiosity.","This is actually a follow up question . I have to apologise for the length of it. I didn't anticipate to be that long. I hope that it will be proved interesting nevertheless. I used the name ""vector turn map"" due to lack of a better alternative. Let $f_A$ be a map defined as following. Choose any $2\times 2$ real matrix $A$. Then take a normalised vector, apply the map on the vector and renormalise it. So $f_A$ takes a normal vector and gives a normal vector, so it can be thought as a map $f_A:S^1\to S^1$. I thought of that because of an error propagation problem I am working on. My idea was to see the asymptotic behaviour of a random error vector. This lead to the Jacobian matrix applied to the error vector multiple times, so I want to see how this vector turns. My problem was over the complex numbers so I was searching for any known results. However after playing a bit with that, I figured out that it won't help me but I thought it is an interesting puzzle. So I thought I should write what I found out and ask for more information. To compute the map, we do the following. Take $\phi\in[0,1)$ to be the coordinate on $S^1$. Construct the vector $[\cos 2\pi \phi,\sin 2\pi \phi]$. Apply the map $A\cdot[\cos 2\pi \phi,\sin 2\pi \phi]$. And finally take the angle of the new vector. This can be written as $$f_A(\phi)=\frac{1}{2\pi}\mbox{arctan2}(A\cdot[\cos 2\pi \phi,\sin 2\pi \phi]).$$ The multiplication and division by $2\pi$ is to ensure that $\phi\in[0,1)$ or rather $\phi\in[-\frac{1}{2},\frac{1}{2})$ depending on the definition on arctan2, it is not important at all. A few things can be told about $f_A$ right away. If $Av=\lambda v$ with $\lambda>0$ then $\phi_\lambda=\frac{1}{2\pi}\mbox{arctan2}(v)$ is a fixed point of $f_A$. Moreover $\phi_\lambda+\frac{1}{2}$ is also a fixed point since $-v$ is also an eigenvector. Also through some calculations if $\lambda_1>\lambda_2>0$ are eigenvalues of $A$ then $\phi_{\lambda_1}$ and $\phi_{\lambda_1}+\frac{1}{2}$ are stable points and $\phi_{\lambda_2}$, $\phi_{\lambda_2}+\frac{1}{2}$ are unstable. This is easy to see by taking the diagonal matrix $D=((1,0),(0,2))$. Then $D\cdot[x,y]=[x,2y]$, so we see that $D\cdot[x,y]$ turns a bit towards the vertical axis if $y\ne0$. If $\lambda_1<\lambda_2<0$, then we see that $f_A(\phi_{\lambda_i})=\phi_{\lambda_i}+\frac{1}{2}$. The situation is similar, but now instead of a fixed point, we have a 2-periodic orbit. The one that corresponds to the eigenvalue with the largest absolute value is stable and the other is unstable. If one eigenvalue is positive and the other is negative, then there is a fixed point and a 2-periodic orbit, with their stability determined by the absolute value of the eigenvalues. If $A$ has only one eigenvector (it is similar to a Jordan block) with positive eigenvalue. Then there are only two fixed points that are stable from one direction and unstable from the other. If the eigenvalue is negative then instead of a fixed point we have a 2-periodic orbit with similar properties. An very useful observation is that similar matrices correspond to ""similar"" maps. Similar in the sense that they have the same number of fixed points (or 2-periodic orbits) with the same stability. I haven't really proven that but with a little thought it becomes almost obvious. I think it can be proven by writing down a couple of commutative diagrams. Now with that said, it is natural to define an other map $g_A$ by taking $\phi\in[0,\frac{1}{2})$. This can be done by using arctan instead of arctan2 (and tweaking the details). Then fixed points and 2-periodic orbits of $f_A$ are fixed points of $g_A$ with the same stability. So in a sense one could say that $g_A$ is a simplification of $f_A$. However this is true only in the $2\times2$ case. The real difference is that $f_A:S^1\to S^1$ but $g_A:\mathbb{P}^1\to \mathbb{P}^1$ ($\mathbb{P}^1$ is the projective line which here is practiacally the same as the circle). By the way the $2\times 2$ case is nice because one can draw pictures. By plugging in the equation in a program like mathematica or matlab, one can get the cobweb diagram and see  the behaviour of the map. The real fun begins when the same is considered in higher dimensions. So let $A$ be an $n\times n$ real matrix. Then by the same procedure one can define $f_A:S^{n-1}\to S^{n-1}$. Again eigenvectors give fixed points or 2-periodic orbits. Based on intuition I would expect that that the stability is determined by the eigenvalue, so the largest eigenvalues (in absolute value) will correspond to a stable point, the smallest to an unstable and the intermediate to some kind of saddles. Now we also have $g_A:\mathbb{P}^n\to \mathbb{P}^n$, so now the difference is rather significant. A conjugacy between $f_A$ and $g_A$ is expected, as in the $2\times 2$ case, but I would expect some complications. Finally it would be also interesting to check complex cases of the map for their behaviour. One interesting question for example is whether the $4\times 4$ real case actually contains the $2\times 2$ complex one. Anyway, I would hope that this is something that is known and that people have worked on it. If not I would be interested to anything that anyone finds. I just put it here as a mathematical curiosity.",,"['matrices', 'dynamical-systems']"
27,Classification of $n \times n$ matrix in which each two components differ in each row and column up to automorphism,Classification of  matrix in which each two components differ in each row and column up to automorphism,n \times n,"Suppose, we define a class $A$ of  $n \times n$ matrix as follows: $$\text{In each Row }i, \text{for any} j,k\ (1 \leq j,k \leq n )\ a_{ij} \neq a_{ik} $$   $$\text{In each Column }l, \text{for any } p,q\ (1 \leq p,q \leq n )\ a_{pl} \neq a_{ql} $$ $$\text{Each component } a_{rs} \in \mathbb{N} \cap [1, n] $$ Two matrices $A$ and $B$ that both satisfy the above conditions(that is,  in each row and column,  no two components are the same) are isomorphic, if: $$\text{There is a permutation } f: \{1,2,\ldots,n\} \to \{1,2,\ldots,n\}\text{ such that  for any } 1 \leq j,k \leq n, f(a_{jk}) = b_{jk}$$ My question is, given $n$, how many kinds of $n \times n$ matrices are there up to the defined isomorphism? Motivation :Inspired by Samuel Reid's question , I am trying to find a generalization of $n$-person zero sum sequential game of perfect information. A natural way to generalize the payoff set is to replace it by a partition of the set of all outcomes with exactly $n$ atoms. Each person has a complete strict order on those $n$ atoms. To avoid difficulty  in Feanor's answer, one way is to require that, for each atom of outcomes, there isn't a pair of players that assigns the same ranking to it. In the cases $n = 2,3$, there is only one kind of players' preference up to isomorphism. But What can we say in general.","Suppose, we define a class $A$ of  $n \times n$ matrix as follows: $$\text{In each Row }i, \text{for any} j,k\ (1 \leq j,k \leq n )\ a_{ij} \neq a_{ik} $$   $$\text{In each Column }l, \text{for any } p,q\ (1 \leq p,q \leq n )\ a_{pl} \neq a_{ql} $$ $$\text{Each component } a_{rs} \in \mathbb{N} \cap [1, n] $$ Two matrices $A$ and $B$ that both satisfy the above conditions(that is,  in each row and column,  no two components are the same) are isomorphic, if: $$\text{There is a permutation } f: \{1,2,\ldots,n\} \to \{1,2,\ldots,n\}\text{ such that  for any } 1 \leq j,k \leq n, f(a_{jk}) = b_{jk}$$ My question is, given $n$, how many kinds of $n \times n$ matrices are there up to the defined isomorphism? Motivation :Inspired by Samuel Reid's question , I am trying to find a generalization of $n$-person zero sum sequential game of perfect information. A natural way to generalize the payoff set is to replace it by a partition of the set of all outcomes with exactly $n$ atoms. Each person has a complete strict order on those $n$ atoms. To avoid difficulty  in Feanor's answer, one way is to require that, for each atom of outcomes, there isn't a pair of players that assigns the same ranking to it. In the cases $n = 2,3$, there is only one kind of players' preference up to isomorphism. But What can we say in general.",,"['abstract-algebra', 'matrices', 'permutations']"
28,Rank of matrix as a difference of ranks,Rank of matrix as a difference of ranks,,"If $X$ is an $n \times p$ matrix of rank $r$ and $C = AX$ for some $q \times n$ matrix $A$ with rank$(A) = q$, how do I show that rank $(X(I-C^{-}C))=$ rank$(X)-$ rank$(C)$?  I can show that rank $(X(I-C^{-}C))\geq $ rank$(X)-$ rank$(C)$, but how do I get the reverse inequality? Note : $C^{-}$ is a generalized inverse of $C$. Any help would be appreciated.","If $X$ is an $n \times p$ matrix of rank $r$ and $C = AX$ for some $q \times n$ matrix $A$ with rank$(A) = q$, how do I show that rank $(X(I-C^{-}C))=$ rank$(X)-$ rank$(C)$?  I can show that rank $(X(I-C^{-}C))\geq $ rank$(X)-$ rank$(C)$, but how do I get the reverse inequality? Note : $C^{-}$ is a generalized inverse of $C$. Any help would be appreciated.",,['matrices']
29,Parallel rank computation of huge sparse matrices over the binary field,Parallel rank computation of huge sparse matrices over the binary field,,"I am looking for a parallel algorithm to compute ranks of huge, sparse binary matrices over $\Bbb F_2$ , say, $10^5 \times 10^5$ with $10^7$ ones in total. Currently, I am doing this by packing $64$ bits in a long and applying Gaussian elimination with XOR, swapping unneeded parts of the matrix to hard disk. While this goes rather fast (even such huge matrices take only a few days) I feel it's a pity that: I'm not using the fact that it's sparse, I feel this could give a huge improvement; I'm not using the fact that I have multiple processors available ( $1000$ + in a GPU) I wondered if anyone is aware of any better methods? I saw this paper: Link but only computing the characteristic polynomial seems to be much more expensive than what I'm doing now. Or am I missing something? How should I compute this determinant polynomial efficiently? Another approach would be working via https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1632 (Wayne Eberly, Erich Kaltofen: On Randomized Lanczos Algorithms) and its derivates such as http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.1619 (Jean-Guillaume Dumas, Gilles Villard: Computing the Rank of Large Sparse Matrices over Finite Fields), but this doesn't work for ${\rm GF}(2)$ (which is not even mentioned in the paper).","I am looking for a parallel algorithm to compute ranks of huge, sparse binary matrices over , say, with ones in total. Currently, I am doing this by packing bits in a long and applying Gaussian elimination with XOR, swapping unneeded parts of the matrix to hard disk. While this goes rather fast (even such huge matrices take only a few days) I feel it's a pity that: I'm not using the fact that it's sparse, I feel this could give a huge improvement; I'm not using the fact that I have multiple processors available ( + in a GPU) I wondered if anyone is aware of any better methods? I saw this paper: Link but only computing the characteristic polynomial seems to be much more expensive than what I'm doing now. Or am I missing something? How should I compute this determinant polynomial efficiently? Another approach would be working via https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.1632 (Wayne Eberly, Erich Kaltofen: On Randomized Lanczos Algorithms) and its derivates such as http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.12.1619 (Jean-Guillaume Dumas, Gilles Villard: Computing the Rank of Large Sparse Matrices over Finite Fields), but this doesn't work for (which is not even mentioned in the paper).",\Bbb F_2 10^5 \times 10^5 10^7 64 1000 {\rm GF}(2),"['matrices', 'finite-fields', 'numerical-linear-algebra', 'matrix-rank', 'sparse-matrices']"
30,Evaluating matrix-continued fractions?,Evaluating matrix-continued fractions?,,"I have a matrix-valued continued fraction defined in the following way: $\alpha_n$ and $\beta_n$ are matrices, and I am interested in the quantity  $A_1$, where all the $A_n$, $n = 1, 2, \dots$ are given by $$A_n = \left[ 1 - \beta_n A_{n+1} \right]^{-1} \alpha_n$$ I know that $\lim\limits_{n\to \infty} A_n = 0$, so a way to calculate $A_1$ is to start at some cutoff $N$ and evaluate $A_1$ up to that value, and then to re-calculate it for a larger cutoff $N$ until the change in the result is sufficiently small. What I do not like about this approach is that I cannot reuse intermediate results from smaller cutoffs when I calculate the result for the larger cutoff. I know that for ordinary (i.e. scalar) continued fractions, there is the modified Lentz's version as described in Numerical Recipes and I wondered if I there is a similar method for my matrix-valued example. In case it helps, maybe some information regarding the matrices $\alpha_n$ and $\beta_n$. They are sparse They are not square matrices, with $\alpha_n$ having more columns than rows and $\beta_n$ having more rows than columns. Motivation: I work on an implementation/refinement on a neat method to compute lattice Green's functions as given in the following reference. Berciu, M., & Cook, A. M. (2010). Efficient computation of lattice Greenʼs functions for models with nearest-neighbour hopping. EPL (Europhysics Letters) , 92(4), 40003 Without much detail: We have quantities $G(n_x,n_y)$ and for these we have coupled recurrence relations that link quantities where one of the two arguments gets reduced or increased by 1. Noting that the relations couple quantities that differ in $M := n_x + n_y$ by $\pm 1$, we can define vectors  $V_M$ that contain all quantities $G(n_x, n_z)$ with $n_x + n_y = M = const$, and then the recurrence relations can be written in matrix form as  $$V_M = \alpha_M V_{M-1} + \beta_M V_{M+1}$$ Some physicla arguments demand that $V_M = 0$ for very large $M$, and using this as a hard cut-off, we can write $V_M = A_M V_{M-1}$ where $A_M$ is given by the recurrence relation I mention above. If I choose the cut-off by hand, I can compute the result, in the same way that I could compute a normal continued fraction approximatively by just computing the $n$-th convergent for a large $n$. But I am interested in having a cut-off that automatically ensures that the calculation actually converged.","I have a matrix-valued continued fraction defined in the following way: $\alpha_n$ and $\beta_n$ are matrices, and I am interested in the quantity  $A_1$, where all the $A_n$, $n = 1, 2, \dots$ are given by $$A_n = \left[ 1 - \beta_n A_{n+1} \right]^{-1} \alpha_n$$ I know that $\lim\limits_{n\to \infty} A_n = 0$, so a way to calculate $A_1$ is to start at some cutoff $N$ and evaluate $A_1$ up to that value, and then to re-calculate it for a larger cutoff $N$ until the change in the result is sufficiently small. What I do not like about this approach is that I cannot reuse intermediate results from smaller cutoffs when I calculate the result for the larger cutoff. I know that for ordinary (i.e. scalar) continued fractions, there is the modified Lentz's version as described in Numerical Recipes and I wondered if I there is a similar method for my matrix-valued example. In case it helps, maybe some information regarding the matrices $\alpha_n$ and $\beta_n$. They are sparse They are not square matrices, with $\alpha_n$ having more columns than rows and $\beta_n$ having more rows than columns. Motivation: I work on an implementation/refinement on a neat method to compute lattice Green's functions as given in the following reference. Berciu, M., & Cook, A. M. (2010). Efficient computation of lattice Greenʼs functions for models with nearest-neighbour hopping. EPL (Europhysics Letters) , 92(4), 40003 Without much detail: We have quantities $G(n_x,n_y)$ and for these we have coupled recurrence relations that link quantities where one of the two arguments gets reduced or increased by 1. Noting that the relations couple quantities that differ in $M := n_x + n_y$ by $\pm 1$, we can define vectors  $V_M$ that contain all quantities $G(n_x, n_z)$ with $n_x + n_y = M = const$, and then the recurrence relations can be written in matrix form as  $$V_M = \alpha_M V_{M-1} + \beta_M V_{M+1}$$ Some physicla arguments demand that $V_M = 0$ for very large $M$, and using this as a hard cut-off, we can write $V_M = A_M V_{M-1}$ where $A_M$ is given by the recurrence relation I mention above. If I choose the cut-off by hand, I can compute the result, in the same way that I could compute a normal continued fraction approximatively by just computing the $n$-th convergent for a large $n$. But I am interested in having a cut-off that automatically ensures that the calculation actually converged.",,"['matrices', 'numerical-methods', 'continued-fractions']"
31,Did my understanding of $GL_n( \mathbb{K})/ SL_n( \mathbb{K})$ is correct? (basic understanding question) [closed],Did my understanding of  is correct? (basic understanding question) [closed],GL_n( \mathbb{K})/ SL_n( \mathbb{K}),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question I am learning algebra by myself and i am trying to understand quotient group. I have met this expression (without further explanations) $GL_n( \mathbb{K})/  SL_n( \mathbb{K})$ and I want to be sure that I understood it well. Question: Did my understanding of $GL_n( \mathbb{K})/ SL_n( \mathbb{K})$ is correct? 1- $GL_n( \mathbb{K})$ = This is the set of all $n \times n$ invertible matrices over a field $\mathbb{K}$ . This is a group with the operation of matrix multiplication "" $ \cdot$ "" . $SL_n( \mathbb{K})$ = This is a subgroup of $GL_n(\mathbb{K})$ consisting of all $n \times n$ matrices with a determinant equal to $1$ . $GL_n( \mathbb{K})/  SL_n( \mathbb{K})$ is by definition the set of all set of the form $\underline{\underline{G}} \cdot  SL_n( \mathbb{K}) $ with $\underline{\underline{G}} \in GL_n( \mathbb{K})$ 2- The quotient group $GL_n(\mathbb{K}) / SL_n(\mathbb{K})$ essentially classifies the elements of $GL_n(\mathbb{K})$ according to their determinant. Two matrices in $GL_n(\mathbb{K})$ are considered equivalent in this quotient group if their ratio is in $SL_n(\mathbb{K})$ . In simpler terms, matrices are equivalent if they differ by a matrix whose determinant is 1. Hence for exemple with $n=2$ we have that: $\begin{pmatrix} 2 & 0 \\  0 & 2  \end{pmatrix}$ is equivalent to $\begin{pmatrix} 4 & 2 \\  2 & 2  \end{pmatrix}$ since with $\begin{pmatrix} 2 & 1 \\  1 & 1  \end{pmatrix} \in SL_n(\mathbb{K})$ we can writte $\begin{pmatrix} 2 & 0 \\  0 & 2  \end{pmatrix} \cdot \begin{pmatrix} 2 & 1 \\  1 & 1  \end{pmatrix} = \begin{pmatrix} 4 & 2 \\  2 & 2  \end{pmatrix}$ More basically we have with (for exemple) $\underline{\underline{G}}= \begin{pmatrix} 2 & 0 \\  0 & 2  \end{pmatrix}$ that the matrix $\begin{pmatrix} 4 & 2 \\  2 & 2  \end{pmatrix}$ is include in the subset $\underline{\underline{G}} \cdot  SL_n( \mathbb{K})$ . Is my understanding correct? Thank you for your help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question I am learning algebra by myself and i am trying to understand quotient group. I have met this expression (without further explanations) and I want to be sure that I understood it well. Question: Did my understanding of is correct? 1- = This is the set of all invertible matrices over a field . This is a group with the operation of matrix multiplication "" "" . = This is a subgroup of consisting of all matrices with a determinant equal to . is by definition the set of all set of the form with 2- The quotient group essentially classifies the elements of according to their determinant. Two matrices in are considered equivalent in this quotient group if their ratio is in . In simpler terms, matrices are equivalent if they differ by a matrix whose determinant is 1. Hence for exemple with we have that: is equivalent to since with we can writte More basically we have with (for exemple) that the matrix is include in the subset . Is my understanding correct? Thank you for your help.","GL_n( \mathbb{K})/  SL_n( \mathbb{K}) GL_n( \mathbb{K})/ SL_n( \mathbb{K}) GL_n( \mathbb{K}) n \times n \mathbb{K}  \cdot SL_n( \mathbb{K}) GL_n(\mathbb{K}) n \times n 1 GL_n( \mathbb{K})/  SL_n( \mathbb{K}) \underline{\underline{G}} \cdot  SL_n( \mathbb{K})  \underline{\underline{G}} \in GL_n( \mathbb{K}) GL_n(\mathbb{K}) / SL_n(\mathbb{K}) GL_n(\mathbb{K}) GL_n(\mathbb{K}) SL_n(\mathbb{K}) n=2 \begin{pmatrix}
2 & 0 \\ 
0 & 2 
\end{pmatrix} \begin{pmatrix}
4 & 2 \\ 
2 & 2 
\end{pmatrix} \begin{pmatrix}
2 & 1 \\ 
1 & 1 
\end{pmatrix} \in SL_n(\mathbb{K}) \begin{pmatrix}
2 & 0 \\ 
0 & 2 
\end{pmatrix} \cdot \begin{pmatrix}
2 & 1 \\ 
1 & 1 
\end{pmatrix} = \begin{pmatrix}
4 & 2 \\ 
2 & 2 
\end{pmatrix} \underline{\underline{G}}= \begin{pmatrix}
2 & 0 \\ 
0 & 2 
\end{pmatrix} \begin{pmatrix}
4 & 2 \\ 
2 & 2 
\end{pmatrix} \underline{\underline{G}} \cdot  SL_n( \mathbb{K})","['abstract-algebra', 'matrices', 'group-theory', 'solution-verification', 'soft-question']"
32,Significance of Rank of Jacobian,Significance of Rank of Jacobian,,I have been struggling with this question. What is the significance of finding the rank of a Jacobian matrix of a function? I understand that the Rank of a matrix signifies the number of linearly independent rows / columns. How does this idea extend to finding the rank of a Jacobian matrix? Thanks !! :),I have been struggling with this question. What is the significance of finding the rank of a Jacobian matrix of a function? I understand that the Rank of a matrix signifies the number of linearly independent rows / columns. How does this idea extend to finding the rank of a Jacobian matrix? Thanks !! :),,"['matrices', 'nonlinear-optimization']"
33,How to represent Hadamard product in terms of matrix multiplication?,How to represent Hadamard product in terms of matrix multiplication?,,"In the case of two vectors $u, v$ with dimensions $n\times 1$, their Hadamard product can be represented by the following matrix multiplication: $$\mathrm{diag}(u)v = \left[\begin{array}{ccc} u_{1} &  & 0\\  & \ddots\\ 0 &  & u_{n} \end{array}\right]\left[\begin{array}{c} v_{1}\\ \vdots\\ v_{n} \end{array}\right]=\left[\begin{array}{c} u_{1}v_{1}\\ \vdots\\ u_{n}v_{n} \end{array}\right]\equiv u\circ v$$ Is there a way to generalize this for Hadamard products of matrices?","In the case of two vectors $u, v$ with dimensions $n\times 1$, their Hadamard product can be represented by the following matrix multiplication: $$\mathrm{diag}(u)v = \left[\begin{array}{ccc} u_{1} &  & 0\\  & \ddots\\ 0 &  & u_{n} \end{array}\right]\left[\begin{array}{c} v_{1}\\ \vdots\\ v_{n} \end{array}\right]=\left[\begin{array}{c} u_{1}v_{1}\\ \vdots\\ u_{n}v_{n} \end{array}\right]\equiv u\circ v$$ Is there a way to generalize this for Hadamard products of matrices?",,"['matrices', 'products']"
34,Practical application of matrices and determinants,Practical application of matrices and determinants,,"I have learned recently about matrices and determinants and also about the geometrical interpretations, i.e, how the matrix is used for linear transformations and how determinants tell us about area/volume changes. My school textbooks tell me that matrices and determinants can be used to solve a system of equations, but I feel that such a vast concept would have more practical applications. My question is: what are the various ways the concept of matrices and determinants is employed in science or everyday life?","I have learned recently about matrices and determinants and also about the geometrical interpretations, i.e, how the matrix is used for linear transformations and how determinants tell us about area/volume changes. My school textbooks tell me that matrices and determinants can be used to solve a system of equations, but I feel that such a vast concept would have more practical applications. My question is: what are the various ways the concept of matrices and determinants is employed in science or everyday life?",,"['matrices', 'soft-question', 'determinant', 'applications']"
35,"Computing $e^{tA}$, where $A$ is a matrix","Computing , where  is a matrix",e^{tA} A,"Given $$A = \begin{bmatrix} 0&-1\\1&0\end{bmatrix}$$ find $e^{tA}$, where $t \in \mathbb{R}$. I have read about calculating the matrix exponential here . I know that when $A$ is written in its diagonal form it's kind of easy. The same with nilpotent matrices. Unfortunately, I don't know how to deal with $t$. Especially when the eigenvalues are complex (in the given example they are).","Given $$A = \begin{bmatrix} 0&-1\\1&0\end{bmatrix}$$ find $e^{tA}$, where $t \in \mathbb{R}$. I have read about calculating the matrix exponential here . I know that when $A$ is written in its diagonal form it's kind of easy. The same with nilpotent matrices. Unfortunately, I don't know how to deal with $t$. Especially when the eigenvalues are complex (in the given example they are).",,"['matrices', 'exponential-function', 'matrix-exponential']"
36,Matrix $\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$ to a large power [duplicate],Matrix  to a large power [duplicate],\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix},This question already has answers here : How can I show that $\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$? (7 answers) Closed 6 years ago . Compute $\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}^{99}$ What is the easier way to do this other than multiplying the entire thing out? Thanks,This question already has answers here : How can I show that $\begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}^n = \begin{pmatrix} 1 & n \\ 0 & 1\end{pmatrix}$? (7 answers) Closed 6 years ago . Compute $\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}^{99}$ What is the easier way to do this other than multiplying the entire thing out? Thanks,,"['matrices', 'exponentiation']"
37,"An example of a $4×4$ matrix $A$ such that $A\not= I$, $A^2\not=I$, ..., $A^5 = I$","An example of a  matrix  such that , , ...,",4×4 A A\not= I A^2\not=I A^5 = I,How do I go about solving this? I went for tutoring and the tutor said I am trying to get to an Identity matrix so I should start with an identity matrix and mix the values around till I get a solution. I have worked on this for 2 hours now and there has to be an easier way. Please help!,How do I go about solving this? I went for tutoring and the tutor said I am trying to get to an Identity matrix so I should start with an identity matrix and mix the values around till I get a solution. I have worked on this for 2 hours now and there has to be an easier way. Please help!,,['matrices']
38,Matrix with the same value in all entries — which notation to use?,Matrix with the same value in all entries — which notation to use?,,"Pretty trivial but for a matrix \begin{bmatrix}x&x&\ldots&x&x\\x&x&\ldots&x&x\\\vdots&\vdots&\ddots&x&x\\x&x&\ldots&x&x\end{bmatrix} with $N\times N $ , is there is simpler notation. I had $A=[x_{i,j}]\in \Bbb R^{N\times N}$ in my mind, but I can't find anywhere were it is used Edit: I forgot to mention but, I need to work with the elements in the matrix, so like doing scalar multiplication (to do a proof by the principle of mathematical induction question). But it a $3\times 3$ so I figured out I do have to write every single element 9 times in a row. Is there any notation that can make my math simpler?","Pretty trivial but for a matrix with , is there is simpler notation. I had in my mind, but I can't find anywhere were it is used Edit: I forgot to mention but, I need to work with the elements in the matrix, so like doing scalar multiplication (to do a proof by the principle of mathematical induction question). But it a so I figured out I do have to write every single element 9 times in a row. Is there any notation that can make my math simpler?","\begin{bmatrix}x&x&\ldots&x&x\\x&x&\ldots&x&x\\\vdots&\vdots&\ddots&x&x\\x&x&\ldots&x&x\end{bmatrix} N\times N  A=[x_{i,j}]\in \Bbb R^{N\times N} 3\times 3","['matrices', 'notation']"
39,Minimum Elementary row/column transformations to find Inverse of given Matric,Minimum Elementary row/column transformations to find Inverse of given Matric,,"While working out some elementary transformation to find Inverse of matrix, it get in my mind, what is the minimum number of elementary transformations needed to find the inverse of a matrix? Editor's note: see Finding the inverse of a matrix by elementary transformations. for the method of elementary transformation EDIT. Here is a detailed description of the studied issue. We are interested in the inversion of matrices, defined on a field (finite or not), by methods of Gauss type; we know the maximum complexity of these methods. We ask which are the matrices whose inversion requires the complete progress of the algorithm; in particular, do we find ourselves in the worst case almost always?","While working out some elementary transformation to find Inverse of matrix, it get in my mind, what is the minimum number of elementary transformations needed to find the inverse of a matrix? Editor's note: see Finding the inverse of a matrix by elementary transformations. for the method of elementary transformation EDIT. Here is a detailed description of the studied issue. We are interested in the inversion of matrices, defined on a field (finite or not), by methods of Gauss type; we know the maximum complexity of these methods. We ask which are the matrices whose inversion requires the complete progress of the algorithm; in particular, do we find ourselves in the worst case almost always?",,['matrices']
40,Need to find matrix formulation,Need to find matrix formulation,,"I have a $B$ matrix: $B = B_{ij}$ I need to find closed matrix formulation of: $$\sum_i \sum_j \sum_m \sum_n  B_{ij} B_{jm} B_{mn} B_{ni}$$ But I am so confused! Edit by Henrik: Originally, it was asked to express $$\sum_i \sum_j \sum_m \sum_n  B_{mi} B_{mj} B_{ni} B_{nj}$$ in terms of matrices. actually, there is also a condition: i is not equal to j.","I have a matrix: I need to find closed matrix formulation of: But I am so confused! Edit by Henrik: Originally, it was asked to express in terms of matrices. actually, there is also a condition: i is not equal to j.",B B = B_{ij} \sum_i \sum_j \sum_m \sum_n  B_{ij} B_{jm} B_{mn} B_{ni} \sum_i \sum_j \sum_m \sum_n  B_{mi} B_{mj} B_{ni} B_{nj},['matrices']
41,All matrices which commute with all $2\times 2$ matrices,All matrices which commute with all  matrices,2\times 2,"I would like to find all matrices which commute with all $2\times2$ matrices.  I started solving problem in this way: 1) I have this matrix $A$ with real numbers: $$A=\left[\begin{array}{cc}a &b\\c &d\end{array}\right]$$ 2) Matrix which commute with matrix $A$ is matrix $B$: $$B = \left[\begin{array}{cc}e& f\\ g &h\end{array}\right]$$ 3) When i solve the equation $AB=BA$, I get this: $$\left[\begin{array}{cc}ae+bg&af+bh\\ce+dg&cf+dh\end{array}\right]=\left[\begin{array}{cc}ea+cf&eb+df\\ga+ch&gb+dh\end{array}\right]$$ How I can get the general look of wanted matrix?","I would like to find all matrices which commute with all $2\times2$ matrices.  I started solving problem in this way: 1) I have this matrix $A$ with real numbers: $$A=\left[\begin{array}{cc}a &b\\c &d\end{array}\right]$$ 2) Matrix which commute with matrix $A$ is matrix $B$: $$B = \left[\begin{array}{cc}e& f\\ g &h\end{array}\right]$$ 3) When i solve the equation $AB=BA$, I get this: $$\left[\begin{array}{cc}ae+bg&af+bh\\ce+dg&cf+dh\end{array}\right]=\left[\begin{array}{cc}ea+cf&eb+df\\ga+ch&gb+dh\end{array}\right]$$ How I can get the general look of wanted matrix?",,['matrices']
42,Is there a $3\times 3$ magic square adding up to $7$?,Is there a  magic square adding up to ?,3\times 3 7,"I suspect that there is no magic square with natural number entries (matrix where each row, column and long diagonal add up to the same number) which would add up to $7$. There is no restriction on which numbers have to appear in the slots, also the numbers that do appear are not resricted to single digit numbers, so 12 can appear in the magic square) Is it correct? If yes, how can I prove it? (I see there are quite a lot of questions about magic squares on the site, but I havent been able to find an exact duplicate.) What follows is my attempt write a magic square adding up to 7. 2, 3,2 - 3,2,2 - 2,2,3 -","I suspect that there is no magic square with natural number entries (matrix where each row, column and long diagonal add up to the same number) which would add up to $7$. There is no restriction on which numbers have to appear in the slots, also the numbers that do appear are not resricted to single digit numbers, so 12 can appear in the magic square) Is it correct? If yes, how can I prove it? (I see there are quite a lot of questions about magic squares on the site, but I havent been able to find an exact duplicate.) What follows is my attempt write a magic square adding up to 7. 2, 3,2 - 3,2,2 - 2,2,3 -",,"['matrices', 'elementary-number-theory', 'recreational-mathematics', 'magic-square']"
43,Is it easier to determine that a matrix is singular than it is to determine nonsingular?,Is it easier to determine that a matrix is singular than it is to determine nonsingular?,,"I came across this line ""It is often easier to determine that a matrix is singular than it is to determine that a matrix is nonsingular. The facts below illustrate this. Fact 1.10. Let $A\in\mathbb C^{n\times n}$ and $x,b\in\mathbb C^n$. If $x\ne0$ and $Ax=0$, then $A$ is singular. If $x\ne0$ and $A$ is nonsingular, then $Ax\ne0$. If $Ax=b$, where $A$ is nonsingular and $b\ne0$, then $x\ne0$. I am not sure how that it is illustrated.  I understand the facts and agree with them.  But how they make one to determine if a matrix is singular in a EASIER way, I do not understand.  Any suggestions?","I came across this line ""It is often easier to determine that a matrix is singular than it is to determine that a matrix is nonsingular. The facts below illustrate this. Fact 1.10. Let $A\in\mathbb C^{n\times n}$ and $x,b\in\mathbb C^n$. If $x\ne0$ and $Ax=0$, then $A$ is singular. If $x\ne0$ and $A$ is nonsingular, then $Ax\ne0$. If $Ax=b$, where $A$ is nonsingular and $b\ne0$, then $x\ne0$. I am not sure how that it is illustrated.  I understand the facts and agree with them.  But how they make one to determine if a matrix is singular in a EASIER way, I do not understand.  Any suggestions?",,['matrices']
44,Is there relationship between magnitude of matrix-vector multiplication and determinant of that matrix?,Is there relationship between magnitude of matrix-vector multiplication and determinant of that matrix?,,If I have a matrix $A$ and vector $x$ is there such a relationship or something similar involving determinants? $$\|Ax\| \leq |\det A|\|x\|$$ where the absolute values indicate the usual Euclidean norm?,If I have a matrix $A$ and vector $x$ is there such a relationship or something similar involving determinants? $$\|Ax\| \leq |\det A|\|x\|$$ where the absolute values indicate the usual Euclidean norm?,,['matrices']
45,Eigenvectors of real symmetric matrices are orthogonal (more discussion),Eigenvectors of real symmetric matrices are orthogonal (more discussion),,"This is an old question, and the proof is here The proof assumed different eigenvalues with different eigenvectors. My question is how about the repeated root? How to guarantee there will not be only one independent eigenvector such that all eigenvectors can form the orthogonal basis of the vector space?","This is an old question, and the proof is here The proof assumed different eigenvalues with different eigenvectors. My question is how about the repeated root? How to guarantee there will not be only one independent eigenvector such that all eigenvectors can form the orthogonal basis of the vector space?",,"['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
46,Is Induction Independent of the Other Axioms of PA?,Is Induction Independent of the Other Axioms of PA?,,"I am trying to come up with a model of first order Peano Arithmetic (PA) where induction fails. Let $PA^{-IND}$ have the same axioms as PA except the first order induction axiom schema is replaced with its negation. I need to show there exists a predicate, $P$, that makes first order induction false. $P$ must satisfy $P(0) \land \forall x(P(x) \rightarrow P(Sx))$, yet $\forall x(P(x))$ is false. We can prove multiplication is commutative using double induction on $P(x,y)= (xy=yx)$. Why Does Induction Prove Multiplication is Commutative? Consider the 2x2 matrices $M_2(N)$ with the standard definitions for matrix addition and multiplication. Let the zero matrix be $0$ and the identity matrix be $S0$. $\forall x(Sx=x+S0)$ is a theorem of $PA^{-IND}$. Matrix multiplication is not commutative, yet we can prove multiplication is commutative for all the successors of $0$. Would $M_2(N)$ be a model of $PA^{-IND}$? The negation of first order induction says there exists a predicate: $P(0) \land \forall x(P(x) \rightarrow P(Sx)) \land Ex( Not(Px))$ This looks like quantification over predicates but it isn't. The induction schema requires us to add an infinite number of axioms to the language. The negation of induction only requires the addition of a single axiom. Unlike PA, $PA^{-IND}$ has a finite number of axioms. I have simply added a new predicate to the language. I am using the axioms of PA given by Wikipedia for First Order Arithmetic . These axioms use induction to prove commutativity. Without induction, these axioms are very weak. They don't even require addition to be commutative. I would be interested in any model of $PA^{-IND}$.","I am trying to come up with a model of first order Peano Arithmetic (PA) where induction fails. Let $PA^{-IND}$ have the same axioms as PA except the first order induction axiom schema is replaced with its negation. I need to show there exists a predicate, $P$, that makes first order induction false. $P$ must satisfy $P(0) \land \forall x(P(x) \rightarrow P(Sx))$, yet $\forall x(P(x))$ is false. We can prove multiplication is commutative using double induction on $P(x,y)= (xy=yx)$. Why Does Induction Prove Multiplication is Commutative? Consider the 2x2 matrices $M_2(N)$ with the standard definitions for matrix addition and multiplication. Let the zero matrix be $0$ and the identity matrix be $S0$. $\forall x(Sx=x+S0)$ is a theorem of $PA^{-IND}$. Matrix multiplication is not commutative, yet we can prove multiplication is commutative for all the successors of $0$. Would $M_2(N)$ be a model of $PA^{-IND}$? The negation of first order induction says there exists a predicate: $P(0) \land \forall x(P(x) \rightarrow P(Sx)) \land Ex( Not(Px))$ This looks like quantification over predicates but it isn't. The induction schema requires us to add an infinite number of axioms to the language. The negation of induction only requires the addition of a single axiom. Unlike PA, $PA^{-IND}$ has a finite number of axioms. I have simply added a new predicate to the language. I am using the axioms of PA given by Wikipedia for First Order Arithmetic . These axioms use induction to prove commutativity. Without induction, these axioms are very weak. They don't even require addition to be commutative. I would be interested in any model of $PA^{-IND}$.",,"['matrices', 'induction', 'model-theory', 'peano-axioms']"
47,Online tool for diagonalizing matrices?,Online tool for diagonalizing matrices?,,"I need some online tool for diagonalizing 2x2 matrices or at least finding the eigenvectors and eigenvalues of it. I don't like to download any stuf because I'm not able to, some online tool will do the job. Thanks.","I need some online tool for diagonalizing 2x2 matrices or at least finding the eigenvectors and eigenvalues of it. I don't like to download any stuf because I'm not able to, some online tool will do the job. Thanks.",,['matrices']
48,Easy ways to calculate matrix exponential of a particular $4\times 4$ matrix,Easy ways to calculate matrix exponential of a particular  matrix,4\times 4,Wondering how to find the matrix exponential of the following matrix without having to do through the long process of finding eigenvalues/eigenvectors and Jordan forms.  Is there a quicker way to do it using Sine/Cosine ? If  $$A= \begin{pmatrix}0 & 1 & 1 & 1 \\                       1 & 0 & 1 & 1 \\                      1 & 1 & 0 & 1 \\                      1 & 1 & 1 & 0 \\ \end{pmatrix}$$ Find $e^{tA}$.,Wondering how to find the matrix exponential of the following matrix without having to do through the long process of finding eigenvalues/eigenvectors and Jordan forms.  Is there a quicker way to do it using Sine/Cosine ? If  $$A= \begin{pmatrix}0 & 1 & 1 & 1 \\                       1 & 0 & 1 & 1 \\                      1 & 1 & 0 & 1 \\                      1 & 1 & 1 & 0 \\ \end{pmatrix}$$ Find $e^{tA}$.,,"['matrices', 'ordinary-differential-equations', 'matrix-exponential']"
49,How many elements of order 3 in this set of 2×2 matrices?,How many elements of order 3 in this set of 2×2 matrices?,,"Let $M$ be a group, $M=\bigg  \{ \bigg(   {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \bigg)\bigg| a,c \in \mathbb{Z}_3\setminus \{0\}, b\in \mathbb{Z}_3 \bigg \}$ The elements of $M$ of order 3 are ${\bigg(\begin{array}{cc}    1 & b \\    0 & 1 \\   \end{array}\bigg) }$. I thought those were ${\bigg(\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array}\bigg) }, {\bigg(\begin{array}{cc}    1 & 1 \\    0 & 1 \\   \end{array}\bigg) },{\bigg(\begin{array}{cc}    1 & 2 \\    0 & 1 \\   \end{array}\bigg) }$, since $b \in \mathbb{Z}_3$. But my answer sheet doesn't mention ${\bigg(\begin{array}{cc}    1 & 0 \\    0 & 1 \\ \end{array}} \bigg ) $, but that's just a mistake in the sheet, right? Or have I misunderstood something?","Let $M$ be a group, $M=\bigg  \{ \bigg(   {\begin{array}{cc}    a & b \\    0 & c \\   \end{array} } \bigg)\bigg| a,c \in \mathbb{Z}_3\setminus \{0\}, b\in \mathbb{Z}_3 \bigg \}$ The elements of $M$ of order 3 are ${\bigg(\begin{array}{cc}    1 & b \\    0 & 1 \\   \end{array}\bigg) }$. I thought those were ${\bigg(\begin{array}{cc}    1 & 0 \\    0 & 1 \\   \end{array}\bigg) }, {\bigg(\begin{array}{cc}    1 & 1 \\    0 & 1 \\   \end{array}\bigg) },{\bigg(\begin{array}{cc}    1 & 2 \\    0 & 1 \\   \end{array}\bigg) }$, since $b \in \mathbb{Z}_3$. But my answer sheet doesn't mention ${\bigg(\begin{array}{cc}    1 & 0 \\    0 & 1 \\ \end{array}} \bigg ) $, but that's just a mistake in the sheet, right? Or have I misunderstood something?",,"['matrices', 'group-theory']"
50,Reference books for learning matrices from the beginning?,Reference books for learning matrices from the beginning?,,I am going to learn about matrices by myself. Can anyone recommend me some good books on matrices with exercises.,I am going to learn about matrices by myself. Can anyone recommend me some good books on matrices with exercises.,,"['matrices', 'reference-request']"
51,matrix representations and polynomials,matrix representations and polynomials,,"I just investigated the following matrix and some of its lower powers:  $${\bf M} = \left[\begin{array}{cccc} 1&0&0&0\\ 1&1&0&0\\ 1&1&1&0\\ 1&1&1&1 \end{array}\right] , {\bf M}^2 = \left[\begin{array}{cccc} 1&0&0&0\\ 2&1&0&0\\ 3&2&1&0\\ 4&3&2&1 \end{array}\right], \\{\bf M}^3 =  \left[\begin{array}{cccc} 1&0&0&0\\ 3&1&0&0\\ 6&3&1&0\\ 10&6&3&1 \end{array}\right] , {\bf M}^4 = \left[\begin{array}{cccc} 1&0&0&0\\ 4&1&0&0\\ 10&4&1&0\\ 20&10&4&1 \end{array}\right] $$ As a function of the exponents, indices (1,1) seem to be constant 1, (2,1) seem to be linear function, (3,1) seem to be arithmetic sum and the fourth one seems to be sums of arithmetic sums. I have some faint memory that these should be polynomials of increasing order (well I know it for sure up to the arithmetic sum), but I can't seem to remember what they were called or how to calculate them. Does it make sense to do polynomial regression or is that uneccessary? This is following the train of thought from matrix representation of generating element for parabola , maybe you can see what I'm jabbing away at. So the question is: is there an easy way to linearly combine the first columns of these matrices to generate the first 4 monomials? I can always do a linear regression with monomial basis functions, but that would be a little silly if this is already a well known way to do it.","I just investigated the following matrix and some of its lower powers:  $${\bf M} = \left[\begin{array}{cccc} 1&0&0&0\\ 1&1&0&0\\ 1&1&1&0\\ 1&1&1&1 \end{array}\right] , {\bf M}^2 = \left[\begin{array}{cccc} 1&0&0&0\\ 2&1&0&0\\ 3&2&1&0\\ 4&3&2&1 \end{array}\right], \\{\bf M}^3 =  \left[\begin{array}{cccc} 1&0&0&0\\ 3&1&0&0\\ 6&3&1&0\\ 10&6&3&1 \end{array}\right] , {\bf M}^4 = \left[\begin{array}{cccc} 1&0&0&0\\ 4&1&0&0\\ 10&4&1&0\\ 20&10&4&1 \end{array}\right] $$ As a function of the exponents, indices (1,1) seem to be constant 1, (2,1) seem to be linear function, (3,1) seem to be arithmetic sum and the fourth one seems to be sums of arithmetic sums. I have some faint memory that these should be polynomials of increasing order (well I know it for sure up to the arithmetic sum), but I can't seem to remember what they were called or how to calculate them. Does it make sense to do polynomial regression or is that uneccessary? This is following the train of thought from matrix representation of generating element for parabola , maybe you can see what I'm jabbing away at. So the question is: is there an easy way to linearly combine the first columns of these matrices to generate the first 4 monomials? I can always do a linear regression with monomial basis functions, but that would be a little silly if this is already a well known way to do it.",,"['matrices', 'polynomials', 'representation-theory', 'binomial-coefficients']"
52,Matrices as generators of free group.,Matrices as generators of free group.,,In the introduction section of the paper Triples of $2\times 2$ matrices which generate free groups the authors mentioning some thing... In my words: The matrices $\begin{pmatrix}1 & 0 \\ 2 & 1\end{pmatrix}$ and $\begin{pmatrix}1 & 2 \\ 0 & 1\end{pmatrix}$ are generating the free group of two generators. How to prove the above statement?,In the introduction section of the paper Triples of $2\times 2$ matrices which generate free groups the authors mentioning some thing... In my words: The matrices $\begin{pmatrix}1 & 0 \\ 2 & 1\end{pmatrix}$ and $\begin{pmatrix}1 & 2 \\ 0 & 1\end{pmatrix}$ are generating the free group of two generators. How to prove the above statement?,,"['abstract-algebra', 'group-theory', 'matrices', 'free-groups']"
53,Show orthogonal upper triangular matrix is diagonal,Show orthogonal upper triangular matrix is diagonal,,How do I go about showing that an upper triangular matrix with orthogonal columns has to be a diagonal matrix? I know that the property $M^\text{T}M = I$ should be used but I'm not sure how.,How do I go about showing that an upper triangular matrix with orthogonal columns has to be a diagonal matrix? I know that the property $M^\text{T}M = I$ should be used but I'm not sure how.,,['matrices']
54,Show that there exists a ring isomorphism $g:T\longrightarrow M_n(S)$,Show that there exists a ring isomorphism,g:T\longrightarrow M_n(S),"Let $R,T$ be two rings and a homomorphism $f:M_n(R)\longrightarrow T$ . Show that there exists a ring $S$ and a ring isomorphism $g:T\longrightarrow M_n(S)$ . This is an exercise from a ring theory course. I have no idea how to start this and find such a ring $S$ . If I could find such a ring, I think I could construct such an isomorphism. Any hints on how to begin this?","Let be two rings and a homomorphism . Show that there exists a ring and a ring isomorphism . This is an exercise from a ring theory course. I have no idea how to start this and find such a ring . If I could find such a ring, I think I could construct such an isomorphism. Any hints on how to begin this?","R,T f:M_n(R)\longrightarrow T S g:T\longrightarrow M_n(S) S","['abstract-algebra', 'matrices', 'ring-theory']"
55,Matrix exponential: $\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix}$,Matrix exponential:,\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix},"It is asked to calculate $e^A$, where $$A=\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix}$$ I begin evaluating some powers of A: $A^0= \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\; ; A=\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix} \; ; A^2 = \begin{pmatrix} -4 & 0 \\ 0 & -4\end{pmatrix} \; ; A^3 = \begin{pmatrix} 0 & -4 \\ 16 & 0\end{pmatrix}\; ; $ $ A^4=\begin{pmatrix} 16 & 0 \\ 0 & 16\end{pmatrix},\; \ldots$ I've noted that, since $$e^A = \sum_{k=0}^\infty \frac{A^k}{k!}$$ we will have the cosine series at the principal diagonal for $\cos(2)$. But  couldnt get what we will have in $(e^A)_{12}$ and $(e^A)_{21}$. Also, we know that if $B=\begin{pmatrix} 0 & \alpha \\ -\alpha & 0 \end{pmatrix}$, then $e^B = \begin{pmatrix} \cos(\alpha) & \sin(\alpha) \\ -\sin(\alpha) & \cos(\alpha) \end{pmatrix} $. Is there a general formula for $$B=\begin{pmatrix} 0& \alpha \\ \beta & 0 \end{pmatrix}$$? Thanks!","It is asked to calculate $e^A$, where $$A=\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix}$$ I begin evaluating some powers of A: $A^0= \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\; ; A=\begin{pmatrix} 0 & 1 \\ -4 & 0 \end{pmatrix} \; ; A^2 = \begin{pmatrix} -4 & 0 \\ 0 & -4\end{pmatrix} \; ; A^3 = \begin{pmatrix} 0 & -4 \\ 16 & 0\end{pmatrix}\; ; $ $ A^4=\begin{pmatrix} 16 & 0 \\ 0 & 16\end{pmatrix},\; \ldots$ I've noted that, since $$e^A = \sum_{k=0}^\infty \frac{A^k}{k!}$$ we will have the cosine series at the principal diagonal for $\cos(2)$. But  couldnt get what we will have in $(e^A)_{12}$ and $(e^A)_{21}$. Also, we know that if $B=\begin{pmatrix} 0 & \alpha \\ -\alpha & 0 \end{pmatrix}$, then $e^B = \begin{pmatrix} \cos(\alpha) & \sin(\alpha) \\ -\sin(\alpha) & \cos(\alpha) \end{pmatrix} $. Is there a general formula for $$B=\begin{pmatrix} 0& \alpha \\ \beta & 0 \end{pmatrix}$$? Thanks!",,"['matrices', 'exponentiation', 'matrix-calculus']"
56,Sum of eigenvalues of a symmetric matrix,Sum of eigenvalues of a symmetric matrix,,"Problem to calculate the sum of eigenvalues of a matrix: $$ \begin{pmatrix}         1 & 1 & 3 \\         1 & 5 & 1 \\         3 & 1 & 1 \\         \end{pmatrix}$$ I can calculate the eigenvalues by using the characteristic equation and then add them up. However, the given hint for this problem was that the sum of eigenvalues is the sum of diagonal elements, making this a $10$ sec problem. So I am wondering if all symmetric matrices have this property (sum of eigenvalues of a symmetric matrix is the sum of its diagonal elements)? But I couldn't find such property mentioned online or in the book. I tried with a few symmetric matrices on wolframalpha and it seems to be true. Please help to clarify this doubt.","Problem to calculate the sum of eigenvalues of a matrix: $$ \begin{pmatrix}         1 & 1 & 3 \\         1 & 5 & 1 \\         3 & 1 & 1 \\         \end{pmatrix}$$ I can calculate the eigenvalues by using the characteristic equation and then add them up. However, the given hint for this problem was that the sum of eigenvalues is the sum of diagonal elements, making this a $10$ sec problem. So I am wondering if all symmetric matrices have this property (sum of eigenvalues of a symmetric matrix is the sum of its diagonal elements)? But I couldn't find such property mentioned online or in the book. I tried with a few symmetric matrices on wolframalpha and it seems to be true. Please help to clarify this doubt.",,"['matrices', 'eigenvalues-eigenvectors']"
57,Do matrices with central symmetry form a group?,Do matrices with central symmetry form a group?,,"Consider the set of $N\times N$ matrices that satisfy the property $$\mathcal{H} = \{H\,|\, H_{ij}=H_{N+1-i,N+1-j}, \det H \neq 0\}$$ or in matrix forms $$\begin{pmatrix}a_{1} & a_{2} & \cdots & a_{N-1} & a_{N}\\ b_{1} & b_{2} & \cdots & b_{N-1} & b_{N}\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ b_{N} & b_{N-1} & \cdots & b_{2} & b_{1}\\ a_{N} & a_{N-1} & \cdots & a_{2} & a_{1} \end{pmatrix}$$ Do these matrices have a name? Do they form a group? It can be easily shown that the identity is in the group, i.e. $I\in \mathcal{H}$. Also the set is closed under multiplication, i.e. if $A\in \mathcal{H}$ and $B\in \mathcal{H}$ then $AB\in\mathcal{H}$. To see why, consider $$\begin{align*} (AB)_{mn}&=\sum_i A_{mi}B_{in}=\sum_i A_{N+1-m,N+1-i}B_{N+1-i, N+1-n}\\ &=\sum_{i^\prime} A_{N+1-m,i^\prime}B_{i^\prime, N+1-n}=(AB)_{N+1-m,N+1-n} \end{align*}$$ The last property that needs to be shown for the set to be a group is that the inverse is also in the set, i.e. if $A\in\mathcal{H}$, is it true that $A^{-1}\in\mathcal{H}$? EDIT: Added the condition that matrices should also be invertible.","Consider the set of $N\times N$ matrices that satisfy the property $$\mathcal{H} = \{H\,|\, H_{ij}=H_{N+1-i,N+1-j}, \det H \neq 0\}$$ or in matrix forms $$\begin{pmatrix}a_{1} & a_{2} & \cdots & a_{N-1} & a_{N}\\ b_{1} & b_{2} & \cdots & b_{N-1} & b_{N}\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ b_{N} & b_{N-1} & \cdots & b_{2} & b_{1}\\ a_{N} & a_{N-1} & \cdots & a_{2} & a_{1} \end{pmatrix}$$ Do these matrices have a name? Do they form a group? It can be easily shown that the identity is in the group, i.e. $I\in \mathcal{H}$. Also the set is closed under multiplication, i.e. if $A\in \mathcal{H}$ and $B\in \mathcal{H}$ then $AB\in\mathcal{H}$. To see why, consider $$\begin{align*} (AB)_{mn}&=\sum_i A_{mi}B_{in}=\sum_i A_{N+1-m,N+1-i}B_{N+1-i, N+1-n}\\ &=\sum_{i^\prime} A_{N+1-m,i^\prime}B_{i^\prime, N+1-n}=(AB)_{N+1-m,N+1-n} \end{align*}$$ The last property that needs to be shown for the set to be a group is that the inverse is also in the set, i.e. if $A\in\mathcal{H}$, is it true that $A^{-1}\in\mathcal{H}$? EDIT: Added the condition that matrices should also be invertible.",,"['group-theory', 'matrices']"
58,"Let $A, B$ be square matrices of equal size ($n \times n$) and $A^2 + B =A^2B$. Prove that $AB=BA$",Let  be square matrices of equal size () and . Prove that,"A, B n \times n A^2 + B =A^2B AB=BA","I need help with solving the problem about square matrices of equal size. I know that if $A + B = AB$ , then $AB = BA$ , but I can't prove this one. Please advise how to solve or think about this. Thanks in advance )","I need help with solving the problem about square matrices of equal size. I know that if , then , but I can't prove this one. Please advise how to solve or think about this. Thanks in advance )",A + B = AB AB = BA,['matrices']
59,"If 2 matrices are such that $(A+B)^k=A^k+B^k$ for $k=2,3$, show that $(A+B)^m=A^m+B^m $ for all $m \in \mathbb{N}$","If 2 matrices are such that  for , show that  for all","(A+B)^k=A^k+B^k k=2,3 (A+B)^m=A^m+B^m  m \in \mathbb{N}","Let $A,B \in M_n(C) $. The matrix  $A-B$ is invertible and $(A+B)^k=A^k+B^k $, $k \in {2,3} $. Prove that $(A+B)^m=A^m+B^m $ for every $m \in N $. PS. I obtained $AB+BA=0$ and $A^2B+B^2A=0$, but I need your help, please :(","Let $A,B \in M_n(C) $. The matrix  $A-B$ is invertible and $(A+B)^k=A^k+B^k $, $k \in {2,3} $. Prove that $(A+B)^m=A^m+B^m $ for every $m \in N $. PS. I obtained $AB+BA=0$ and $A^2B+B^2A=0$, but I need your help, please :(",,['matrices']
60,Seeing $PSL_2(\mathbb{C}) \cong SO_3(\mathbb{C})$,Seeing,PSL_2(\mathbb{C}) \cong SO_3(\mathbb{C}),"How can I see the isomorphism between projective special linear group (order 2) and the special orthogonal group (order 3)? I know only this setting $PSL_2(\mathbb{C}) = SL_2(\mathbb{C})/Z(SL_2(\mathbb{C})$, but it has not helped me a lot.","How can I see the isomorphism between projective special linear group (order 2) and the special orthogonal group (order 3)? I know only this setting $PSL_2(\mathbb{C}) = SL_2(\mathbb{C})/Z(SL_2(\mathbb{C})$, but it has not helped me a lot.",,"['matrices', 'group-theory', 'lie-groups', 'group-isomorphism', 'exceptional-isomorphisms']"
61,Polynomial of $3\times 3$ matrix with $\operatorname{tr}(A)=\operatorname{tr}(A^2)=0$ and $\det(A)=1$,Polynomial of  matrix with  and,3\times 3 \operatorname{tr}(A)=\operatorname{tr}(A^2)=0 \det(A)=1,"If $A$ be an invertible matrix $M_3(\mathbb{R})$ and we have $\operatorname{tr}(A)=\operatorname{tr}(A^2)=0$ and $\det(A)=1$, then what is the characteristic polynomial of $A$?","If $A$ be an invertible matrix $M_3(\mathbb{R})$ and we have $\operatorname{tr}(A)=\operatorname{tr}(A^2)=0$ and $\det(A)=1$, then what is the characteristic polynomial of $A$?",,"['matrices', 'polynomials', 'determinant', 'trace']"
62,Algorithm for real matrix given the complex eigenvalues,Algorithm for real matrix given the complex eigenvalues,,Given complex eigenvalues (occurring in conjugate pairs) how to get a single instance of a real matrix which has these eigenvalues. I know the matrix is not unique as eigenvectors are not fixed but in my case any real matrix will suffice.,Given complex eigenvalues (occurring in conjugate pairs) how to get a single instance of a real matrix which has these eigenvalues. I know the matrix is not unique as eigenvectors are not fixed but in my case any real matrix will suffice.,,"['matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
63,Gradient of $X \mapsto a^T X b$ when $X$ is symmetric,Gradient of  when  is symmetric,X \mapsto a^T X b X,"For matrix $X \in \Bbb R^{n \times n}$ , $a \in \Bbb R^n$ , and $b \in \Bbb R^n$ , I know the following holds $$\nabla_X \left( a^T X b \right) = a{b^T}$$ However, it seems that if $X$ is a symmetric matrix ( $X \in \Bbb S^n$ ), then $$ \nabla_X \left( {a^T} X b \right) = \frac{1}{2}(a{b^T} + {b}a^T) $$ How to understand it? If $X \in \Bbb S^n$ , then the dimension of $X$ is $\frac{n(n+1)}{2}$ . Why should we get $n^2$ elements after differentiation?","For matrix , , and , I know the following holds However, it seems that if is a symmetric matrix ( ), then How to understand it? If , then the dimension of is . Why should we get elements after differentiation?",X \in \Bbb R^{n \times n} a \in \Bbb R^n b \in \Bbb R^n \nabla_X \left( a^T X b \right) = a{b^T} X X \in \Bbb S^n  \nabla_X \left( {a^T} X b \right) = \frac{1}{2}(a{b^T} + {b}a^T)  X \in \Bbb S^n X \frac{n(n+1)}{2} n^2,"['matrices', 'matrix-calculus', 'symmetric-matrices', 'scalar-fields']"
64,Homotopy of Involutory Matrices?,Homotopy of Involutory Matrices?,,"I want to construct a homotopy from the matrix $$ \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & -1 & 0 & 0\\ 0 & 0 & -1 & 0\\ 0 & 0 & 0 & 1\\ \end{bmatrix} $$ to the identity matrix within the space of involutory matrices ($A^{2} = I$). Is this even possible? Also, how do I determine the number of path connected components of this space?","I want to construct a homotopy from the matrix $$ \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & -1 & 0 & 0\\ 0 & 0 & -1 & 0\\ 0 & 0 & 0 & 1\\ \end{bmatrix} $$ to the identity matrix within the space of involutory matrices ($A^{2} = I$). Is this even possible? Also, how do I determine the number of path connected components of this space?",,"['matrices', 'homotopy-theory', 'connectedness']"
65,Do row and column permutations generate all permutations?,Do row and column permutations generate all permutations?,,"Suppose $m,n\ge 1$ are integers. Do row and column permutations of an $m\times n$ matrix generate the group of all permutations of the $mn$ entries of the matrix? More formally, let $A_1$ be the matrix $$ A_1 = \left[\begin{array}{cccc} 1 & 2 & \cdots & n \\ n+1 & n+2 & \cdots & 2n \\ \vdots & \vdots & \ddots &\vdots \\ (m-1)n+1 & (m-1)n + 2 & \cdots & mn  \end{array}\right] $$ and let $A_2$ be an $m\times n$ matrix such that each of the numbers $1,\ldots, mn$ appears (exactly once) as an entry in $A_2$. Is there necessarily a sequence of row and column permutations that transforms $A_1$ into $A_2$? If not, can one easily characterize which permutations are generated by row and column permutations?","Suppose $m,n\ge 1$ are integers. Do row and column permutations of an $m\times n$ matrix generate the group of all permutations of the $mn$ entries of the matrix? More formally, let $A_1$ be the matrix $$ A_1 = \left[\begin{array}{cccc} 1 & 2 & \cdots & n \\ n+1 & n+2 & \cdots & 2n \\ \vdots & \vdots & \ddots &\vdots \\ (m-1)n+1 & (m-1)n + 2 & \cdots & mn  \end{array}\right] $$ and let $A_2$ be an $m\times n$ matrix such that each of the numbers $1,\ldots, mn$ appears (exactly once) as an entry in $A_2$. Is there necessarily a sequence of row and column permutations that transforms $A_1$ into $A_2$? If not, can one easily characterize which permutations are generated by row and column permutations?",,"['group-theory', 'matrices', 'group-actions', 'permutations']"
66,Finding normalised eigenvectors...,Finding normalised eigenvectors...,,"I'm trying to find the eigenvector/eigenvalues of the $2\times2$ matrix: \begin{pmatrix}4 & 2 \\ 2 & 3 \end{pmatrix} This is my work: $$\det(A-\lambda I) = \lambda^2-7 \lambda+8=0 \iff \lambda=\frac{7+\sqrt{17}}{2} \ \lor \ \lambda= \frac{7-\sqrt{17}}{2}$$ $x_1$ (eigenvector)=\begin{pmatrix} (1+\sqrt17)/4 \\ k  \end{pmatrix} , where k is any number. How do I ""NORMALISE"" this eigenvector? Can someone check my working because I'm getting weird answers.","I'm trying to find the eigenvector/eigenvalues of the $2\times2$ matrix: \begin{pmatrix}4 & 2 \\ 2 & 3 \end{pmatrix} This is my work: $$\det(A-\lambda I) = \lambda^2-7 \lambda+8=0 \iff \lambda=\frac{7+\sqrt{17}}{2} \ \lor \ \lambda= \frac{7-\sqrt{17}}{2}$$ $x_1$ (eigenvector)=\begin{pmatrix} (1+\sqrt17)/4 \\ k  \end{pmatrix} , where k is any number. How do I ""NORMALISE"" this eigenvector? Can someone check my working because I'm getting weird answers.",,"['matrices', 'eigenvalues-eigenvectors']"
67,The possible number of zero entries in $n\times n$ matrix that would make the determinant non-zero,The possible number of zero entries in  matrix that would make the determinant non-zero,n\times n,"While preparing an exam, I found the following question: What is the largest possible number of zero entries in any $5 \times 5$   matrix with a non-zero determinant? 25 15 16 20 I know that the answer is 20, but why? I can find the formula or the theorem that makes that answer true. Any help please?","While preparing an exam, I found the following question: What is the largest possible number of zero entries in any $5 \times 5$   matrix with a non-zero determinant? 25 15 16 20 I know that the answer is 20, but why? I can find the formula or the theorem that makes that answer true. Any help please?",,"['matrices', 'determinant']"
68,Proving that the matrix is not invertible.,Proving that the matrix is not invertible.,,A is a 2x3 matrix and B is a 3x2. How can i prove that the matrix D = AB is not invertible. I could not go further in this problem. The only thing that i have found is the multiply of these two matrix will be 2x2 matrix but how can i find it is not invetible?,A is a 2x3 matrix and B is a 3x2. How can i prove that the matrix D = AB is not invertible. I could not go further in this problem. The only thing that i have found is the multiply of these two matrix will be 2x2 matrix but how can i find it is not invetible?,,"['matrices', 'inverse']"
69,Normal subgroup of $\operatorname{SL}_2(K)$,Normal subgroup of,\operatorname{SL}_2(K),"The setup: Let $K$ be a field which is not of characteristic 2 and which contains at least seven elements. Let $N$ be a normal subgroup of $\operatorname{SL}_2(K)$ which contains a matrix $A \neq \pm I$ . The problem: (a) Show that $N$ contains an upper triangular matrix other than $\pm I$ . (b) Show that $N$ contains a unit upper triangular matrix other than $\pm I$ . Now the problem goes on, and the end goal is to show that there are infinitely many nonabelian simple groups. I've got part (c) through (f), but I simply can't get anywhere with these two. There is a hint attached to part (a), suggesting we conjugate to get a 0, then compute a commutator with a diagonal matrix to place the 0. Conjugation isn't exactly the most elegant operation here. For any $\begin{pmatrix} a&b\\c&d \end{pmatrix}$ in $N$ , and any $\begin{pmatrix} r&s\\t&u\end{pmatrix}$ in $\operatorname{SL}_2(K)$ , we have $$ \begin{pmatrix} u&-s\\-t&r\end{pmatrix}\begin{pmatrix} a&b\\c&d\end{pmatrix}\begin{pmatrix} r&s\\t&u\end{pmatrix} = \begin{pmatrix} u(ar+bt)-s(cr+dt)&u(as+bu)-s(cs+du)\\r(cr+dt)-t(ar+bt)&r(cs+du)-t(as+bu)\end{pmatrix},$$ and none of those look particularly easy to force to be 0. (I can get the bottom left to $-b$ or $-c$ , but those haven't been helpful on their own.) I don't know what to do for the commutator yet, but to be fair, I don't know where my 0 is supposed to be, so that will hopefully resolve with the issue above. I'm hoping my problems with part (b) also stem from my lack of progress with part (a), so any hints would be greatly appreciated. Thanks!","The setup: Let be a field which is not of characteristic 2 and which contains at least seven elements. Let be a normal subgroup of which contains a matrix . The problem: (a) Show that contains an upper triangular matrix other than . (b) Show that contains a unit upper triangular matrix other than . Now the problem goes on, and the end goal is to show that there are infinitely many nonabelian simple groups. I've got part (c) through (f), but I simply can't get anywhere with these two. There is a hint attached to part (a), suggesting we conjugate to get a 0, then compute a commutator with a diagonal matrix to place the 0. Conjugation isn't exactly the most elegant operation here. For any in , and any in , we have and none of those look particularly easy to force to be 0. (I can get the bottom left to or , but those haven't been helpful on their own.) I don't know what to do for the commutator yet, but to be fair, I don't know where my 0 is supposed to be, so that will hopefully resolve with the issue above. I'm hoping my problems with part (b) also stem from my lack of progress with part (a), so any hints would be greatly appreciated. Thanks!","K N \operatorname{SL}_2(K) A \neq \pm I N \pm I N \pm I \begin{pmatrix} a&b\\c&d \end{pmatrix} N \begin{pmatrix} r&s\\t&u\end{pmatrix} \operatorname{SL}_2(K)  \begin{pmatrix} u&-s\\-t&r\end{pmatrix}\begin{pmatrix} a&b\\c&d\end{pmatrix}\begin{pmatrix} r&s\\t&u\end{pmatrix} = \begin{pmatrix} u(ar+bt)-s(cr+dt)&u(as+bu)-s(cs+du)\\r(cr+dt)-t(ar+bt)&r(cs+du)-t(as+bu)\end{pmatrix}, -b -c","['group-theory', 'matrices']"
70,What is the operation between a bivector and a vector that outputs another vector?,What is the operation between a bivector and a vector that outputs another vector?,,"I'll start with a physical example. Let's say we have the angular velocity (in 3D euclidean space with orthonormal basis spanning it), which has the bivector representation $$\Omega = \omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy}$$ for unit bivectors $\mathbf e_{yz}, \mathbf e_{xz}, \mathbf e_{xy}$ , and the skew symmetric matrix representation $$\Omega = \begin{bmatrix} 0 & -\omega_z & \omega_y \\ \omega_z & 0 & -\omega_x \\ -\omega_y & \omega_x & 0  \end{bmatrix}$$ The hodge dual of this bivector is the axial vector $$\star\Omega = \boldsymbol\omega = \omega_x\mathbf e_x  + \omega _y \mathbf e_{y} + \omega_z \mathbf e_{z}$$ When we calculate the velocity of a particle with a position vector $\mathbf r = r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z}$ and angular velocity $\Omega$ , then the velocity of the particle is said to be given by $$\mathbf v = \Omega \mathbf r \iff \mathbf v = \boldsymbol\omega\times \mathbf r$$ What I don't understand is what the operation between $\Omega$ and $\mathbf r$ is. I understand that the hodge dual of a wedge product is a cross product, which gives us the relationship that $$\text{vector}\wedge\text{vector} = \text{bivector} \iff \text{vector}\times \text{vector}=\text{axial vector}$$ However, there is also the relationship that $$\text{vector}\times \text{axial vector} = \text{vector}$$ which seems to be the relationship here. Now, if we replace the axial vector with the actual bivector (as seen in the angular velocity example above), what is the operation between the vector and the bivector??? $$ \text{bivector } ??? \text{ vector} = \text{vector}$$ Obviously, when we write the bivector as a skew symmetric matrix, we're just performing regular matrix multiplication. However, if we're writing it as just the sum of unit bivectors, what is this operation?? $$\langle\omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy}\rangle \text{ ??? } \langle r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z} \rangle$$ I initially thought it might be a wedge product, but quickly realized that it would just be wedging a bivector and a vector which returns a trivector, not a regular vector that I want. I can't think of any other operation that would return a vector between a bivector and a vector. Any help? Thanks!","I'll start with a physical example. Let's say we have the angular velocity (in 3D euclidean space with orthonormal basis spanning it), which has the bivector representation for unit bivectors , and the skew symmetric matrix representation The hodge dual of this bivector is the axial vector When we calculate the velocity of a particle with a position vector and angular velocity , then the velocity of the particle is said to be given by What I don't understand is what the operation between and is. I understand that the hodge dual of a wedge product is a cross product, which gives us the relationship that However, there is also the relationship that which seems to be the relationship here. Now, if we replace the axial vector with the actual bivector (as seen in the angular velocity example above), what is the operation between the vector and the bivector??? Obviously, when we write the bivector as a skew symmetric matrix, we're just performing regular matrix multiplication. However, if we're writing it as just the sum of unit bivectors, what is this operation?? I initially thought it might be a wedge product, but quickly realized that it would just be wedging a bivector and a vector which returns a trivector, not a regular vector that I want. I can't think of any other operation that would return a vector between a bivector and a vector. Any help? Thanks!","\Omega = \omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy} \mathbf e_{yz}, \mathbf e_{xz}, \mathbf e_{xy} \Omega =
\begin{bmatrix}
0 & -\omega_z & \omega_y \\
\omega_z & 0 & -\omega_x \\
-\omega_y & \omega_x & 0 
\end{bmatrix} \star\Omega = \boldsymbol\omega = \omega_x\mathbf e_x  + \omega _y \mathbf e_{y} + \omega_z \mathbf e_{z} \mathbf r = r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z} \Omega \mathbf v = \Omega \mathbf r \iff \mathbf v = \boldsymbol\omega\times \mathbf r \Omega \mathbf r \text{vector}\wedge\text{vector} = \text{bivector} \iff \text{vector}\times \text{vector}=\text{axial vector} \text{vector}\times \text{axial vector} = \text{vector}  \text{bivector } ??? \text{ vector} = \text{vector} \langle\omega_x \mathbf e_{yz} + \omega _y \mathbf e_{xz} + \omega_z \mathbf e_{xy}\rangle \text{ ??? } \langle r_x\mathbf e_x  + r_y \mathbf e_{y} + r_z \mathbf e_{z} \rangle","['matrices', 'vectors', 'exterior-algebra', 'cross-product']"
71,Algebra and matrices.,Algebra and matrices.,,"Q. Solve for $x$, $y$ and $z$ if $$(x+y)(x+z)=15, $$ $$(y+z)(y+x)=18, $$ $$(z+x)(z+y)=30. $$ Solution: I expanded each equation above as : $$x^2+xz+yx+yz=15, \tag{1}$$ $$y^2+xz+yx+yz=18, \tag{2}$$ $$z^2+xz+yx+yz=30. \tag{3}$$ Then I subtracted $(1)-(3)$, $(2)-(1)$ and $(3)-(2)$; so I got the equations as below: $$x^2-z^2=-15 \tag{4}$$ $$y^2-x^2=3 \tag{5}$$ $$z^2-y^2=12 \tag{6}$$ I then tried solving $(4)$,$(5)$,$(6)$ by using matrices, but I couldn't reach any solution. Please advise. Thank You.","Q. Solve for $x$, $y$ and $z$ if $$(x+y)(x+z)=15, $$ $$(y+z)(y+x)=18, $$ $$(z+x)(z+y)=30. $$ Solution: I expanded each equation above as : $$x^2+xz+yx+yz=15, \tag{1}$$ $$y^2+xz+yx+yz=18, \tag{2}$$ $$z^2+xz+yx+yz=30. \tag{3}$$ Then I subtracted $(1)-(3)$, $(2)-(1)$ and $(3)-(2)$; so I got the equations as below: $$x^2-z^2=-15 \tag{4}$$ $$y^2-x^2=3 \tag{5}$$ $$z^2-y^2=12 \tag{6}$$ I then tried solving $(4)$,$(5)$,$(6)$ by using matrices, but I couldn't reach any solution. Please advise. Thank You.",,"['matrices', 'algebra-precalculus', 'matrix-equations']"
72,"""Dividing"" both sides of an inequality by a vector","""Dividing"" both sides of an inequality by a vector",,"As ridiculous as the question sounds, I'm just referring to a very specific case. Suppose that I'm given two $n \times 1$ vectors $x$ and $c$, an $m \times n$ matrix $A$ and a $m \times 1$ vector $y$, and the following inequality: $$c^Tx \geq y^TAx$$ If $x \geq 0$ (all its components are $\geq 0$), intuitively it seems to follow that $c^T \geq y^TA$ and hence that $c \geq A^Ty$. It's incorrect to say that we ""multiply both sides by $x^{-1}$"" because there's no notion of the inverse of a column vector as such. Is there any rigorous justification for: $$c^Tx \geq y^TAx \implies c^T \geq y^TA$$ provided that $x$ is non-negative (or for reversing the above inequality if $x$ is negative)? (Clarification: By $a \geq b$, where $a, b \in \mathbb{R}^n$, I mean that $a_i \geq b_i$ for all $i = 1, 2, \ldots, n$.)","As ridiculous as the question sounds, I'm just referring to a very specific case. Suppose that I'm given two $n \times 1$ vectors $x$ and $c$, an $m \times n$ matrix $A$ and a $m \times 1$ vector $y$, and the following inequality: $$c^Tx \geq y^TAx$$ If $x \geq 0$ (all its components are $\geq 0$), intuitively it seems to follow that $c^T \geq y^TA$ and hence that $c \geq A^Ty$. It's incorrect to say that we ""multiply both sides by $x^{-1}$"" because there's no notion of the inverse of a column vector as such. Is there any rigorous justification for: $$c^Tx \geq y^TAx \implies c^T \geq y^TA$$ provided that $x$ is non-negative (or for reversing the above inequality if $x$ is negative)? (Clarification: By $a \geq b$, where $a, b \in \mathbb{R}^n$, I mean that $a_i \geq b_i$ for all $i = 1, 2, \ldots, n$.)",,"['matrices', 'matrix-equations']"
73,What happens to a Matrix after moving to the other side of the equal sign?,What happens to a Matrix after moving to the other side of the equal sign?,,I have the following equation where each variable is a matrix that can be multiplied A * B = C I have matrix A and C but matrix B is unknown and I need to get its value the way I though about it is to make a the following B = C / A which means B = C * (1/A) which means B = C * A -1 where A -1 means the inverse of A is that correct or should something happens to matrix A after moving to the other side of the equation?,I have the following equation where each variable is a matrix that can be multiplied A * B = C I have matrix A and C but matrix B is unknown and I need to get its value the way I though about it is to make a the following B = C / A which means B = C * (1/A) which means B = C * A -1 where A -1 means the inverse of A is that correct or should something happens to matrix A after moving to the other side of the equation?,,['matrices']
74,why is representing rotations through quaternions more compact and quicker than using matrices??,why is representing rotations through quaternions more compact and quicker than using matrices??,,"According to the wikipedia page on Quaternions: The representations of rotations by quaternions are more compact and quicker to compute than the representations by matrices. However, I have to admit, I don't fully understand what quaternions are and why they are useful. I have tried to read the article, but I don't understand why defining such a system is useful. It appears to a define a four dimensional space in which 3 components are imaginary and one is real. Is this attempting to describe spacetime? Regardless, I was hoping someone here could show how to represent a rotation using both quaternions and matrices and compare the two for me.","According to the wikipedia page on Quaternions: The representations of rotations by quaternions are more compact and quicker to compute than the representations by matrices. However, I have to admit, I don't fully understand what quaternions are and why they are useful. I have tried to read the article, but I don't understand why defining such a system is useful. It appears to a define a four dimensional space in which 3 components are imaginary and one is real. Is this attempting to describe spacetime? Regardless, I was hoping someone here could show how to represent a rotation using both quaternions and matrices and compare the two for me.",,"['matrices', 'rotations', 'quaternions']"
75,Inverse of the sum of the inverse of two matrices,Inverse of the sum of the inverse of two matrices,,"I need to compute $ (A^{-1} + B^{-1})^{-1} $. Both $A$ and $B$ are symmetric and $A$ is invertible and PSD. I already know $B^{-1}$ and $A$, but I don't have $A^{-1}$ and $B$. Is there a formula to compute that avoids two inversions? EDIT: Another related question is if there are situations (e.g. $A$ and $B$ have additional properties) where it is possible to avoid both the inversions and under which conditions. As additional properties consider that I know also $B$ together with $A$ and $B^{-1}$ and that $B$ is invertible and PSD as $A$. Furthermore, both $A$ and $B$ are square matrices. With these properties, is there a formual (or even an approximation) to avoid both the inversions?","I need to compute $ (A^{-1} + B^{-1})^{-1} $. Both $A$ and $B$ are symmetric and $A$ is invertible and PSD. I already know $B^{-1}$ and $A$, but I don't have $A^{-1}$ and $B$. Is there a formula to compute that avoids two inversions? EDIT: Another related question is if there are situations (e.g. $A$ and $B$ have additional properties) where it is possible to avoid both the inversions and under which conditions. As additional properties consider that I know also $B$ together with $A$ and $B^{-1}$ and that $B$ is invertible and PSD as $A$. Furthermore, both $A$ and $B$ are square matrices. With these properties, is there a formual (or even an approximation) to avoid both the inversions?",,"['matrices', 'inverse']"
76,Does a symmetric matrix $A^2$ imply a symmetric $A$?,Does a symmetric matrix  imply a symmetric ?,A^2 A,Does a symmetric matrix $A^2$  imply a symmetric $A$? Any help would be much appreciated.,Does a symmetric matrix $A^2$  imply a symmetric $A$? Any help would be much appreciated.,,"['matrices', 'examples-counterexamples']"
77,Find the EigenValues and EigenVectors of the matrix associated with quadratic form,Find the EigenValues and EigenVectors of the matrix associated with quadratic form,,Problem: Find the EigenValues and EigenVectors of the matrix associated with quadratic forms $2x^2+6y^2+2z^2+8xz$. I know how to convert a set of polynomial equations to a matrix but I have no clue how to convert this single quadratic equation with 3 variables into a matrix. Google tells me to use Cayley-Hamilton theorem... how?,Problem: Find the EigenValues and EigenVectors of the matrix associated with quadratic forms $2x^2+6y^2+2z^2+8xz$. I know how to convert a set of polynomial equations to a matrix but I have no clue how to convert this single quadratic equation with 3 variables into a matrix. Google tells me to use Cayley-Hamilton theorem... how?,,"['matrices', 'quadratic-forms']"
78,Proof - Square Matrix has maximal rank if and only if it is invertible,Proof - Square Matrix has maximal rank if and only if it is invertible,,Could someone help me with the proof that a square matrix has maximal rank if and only if it is invertible? Thanks to everybody,Could someone help me with the proof that a square matrix has maximal rank if and only if it is invertible? Thanks to everybody,,['matrices']
79,Probability of a $1000 \times 1000$ square matrix over $\mathbb{Z}_2$ having full rank,Probability of a  square matrix over  having full rank,1000 \times 1000 \mathbb{Z}_2,"There are only two entries, $0$ and $1$, over $\mathbb{Z}_2$. Thus, only $16$ possible $2\times2$ matrices over $\mathbb{Z}_2$, and $6$ of them have full rank: $$\begin{pmatrix}0&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}1&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}0&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}0&1\\   1&1\end{pmatrix}  \quad \begin{pmatrix}1&1\\   0&1\end{pmatrix}  \quad \begin{pmatrix}1&0\\   0&1\end{pmatrix}$$ Randomly generate a $n \times n$ matrix over $\mathbb{Z}_2$ (where $n$ is big, say, $1000$). What's the probability that the matrix has full rank?","There are only two entries, $0$ and $1$, over $\mathbb{Z}_2$. Thus, only $16$ possible $2\times2$ matrices over $\mathbb{Z}_2$, and $6$ of them have full rank: $$\begin{pmatrix}0&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}1&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}0&1\\   1&0\end{pmatrix}  \quad \begin{pmatrix}0&1\\   1&1\end{pmatrix}  \quad \begin{pmatrix}1&1\\   0&1\end{pmatrix}  \quad \begin{pmatrix}1&0\\   0&1\end{pmatrix}$$ Randomly generate a $n \times n$ matrix over $\mathbb{Z}_2$ (where $n$ is big, say, $1000$). What's the probability that the matrix has full rank?",,"['matrices', 'matrix-rank']"
80,Decompose matrix A into its symmetric and skew-symmetric parts,Decompose matrix A into its symmetric and skew-symmetric parts,,"I have this square matrix: $$A=\begin{pmatrix}0&-14\\14&0\end{pmatrix}$$ and I want to find its symmetric and skew-symmetric parts but I am confuse because it is already a skew symmetric matrix, and when finding the symmetric part I get a zero matrix. Is that possible?","I have this square matrix: $$A=\begin{pmatrix}0&-14\\14&0\end{pmatrix}$$ and I want to find its symmetric and skew-symmetric parts but I am confuse because it is already a skew symmetric matrix, and when finding the symmetric part I get a zero matrix. Is that possible?",,"['matrices', 'transpose']"
81,Computing the inverse of some matrix,Computing the inverse of some matrix,,"Let $A(n) = (a_{i,j})_{1\le i,j \le n}$ be defined through:  $a_{i,j} = 1, \text{ if } i \equiv 0 \mod (j)$, $0$ otherwise. What is the inverse of $A$? Is there an ""easy"" formula for the inverse? It seems that the inverse of the matrix has only entries $0,1,-1$.","Let $A(n) = (a_{i,j})_{1\le i,j \le n}$ be defined through:  $a_{i,j} = 1, \text{ if } i \equiv 0 \mod (j)$, $0$ otherwise. What is the inverse of $A$? Is there an ""easy"" formula for the inverse? It seems that the inverse of the matrix has only entries $0,1,-1$.",,['matrices']
82,Show that if $\operatorname{trace}(AB) = 0$ and $\operatorname{rank} (A)=1$ then $ABA=0$,Show that if  and  then,\operatorname{trace}(AB) = 0 \operatorname{rank} (A)=1 ABA=0,I know that $$AB-BA=A \iff A \text{ is singular}$$ $A$ and $B$ can be complex. Any hints?,I know that $$AB-BA=A \iff A \text{ is singular}$$ $A$ and $B$ can be complex. Any hints?,,"['matrices', 'matrix-rank', 'trace']"
83,Finding the units in $M_n(R)$,Finding the units in,M_n(R),"Let $R$ be a ring. Then I would like to know the units in $M_n(R)$. Here is my idea. The determinant is a homomorphism from $M_n(R)$ to $R$. If $A$ is a unit, then there is a $B$ such that $AB = I$. Then taking determinants one finds that $\det(A)$ is a unit in $R$. So therefore the units in $M_n(R)$ are those matrices $A$ where $\det(A)$ is a unit in $R$. Is this correct? Can this be done more explicitly?","Let $R$ be a ring. Then I would like to know the units in $M_n(R)$. Here is my idea. The determinant is a homomorphism from $M_n(R)$ to $R$. If $A$ is a unit, then there is a $B$ such that $AB = I$. Then taking determinants one finds that $\det(A)$ is a unit in $R$. So therefore the units in $M_n(R)$ are those matrices $A$ where $\det(A)$ is a unit in $R$. Is this correct? Can this be done more explicitly?",,"['abstract-algebra', 'matrices', 'ring-theory']"
84,Prove a matrix is invertible [duplicate],Prove a matrix is invertible [duplicate],,"This question already has answers here : If $\mathbf{A}$ is a $2\times 2$ matrix that satisfies $\mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0}$, then $\mathbf{A}$ is invertible (4 answers) Closed 8 years ago . The $2 \times 2$ matrix ${A}$ satisfies $A^2 - 4 {A} - 7{I} = {0},$ where ${I}$ is the $2 \times 2$ identity matrix. Prove that ${A}$ is invertible. What is the best way to do this?","This question already has answers here : If $\mathbf{A}$ is a $2\times 2$ matrix that satisfies $\mathbf{A}^2 - 4\mathbf{A} - 7\mathbf{I} = \mathbf{0}$, then $\mathbf{A}$ is invertible (4 answers) Closed 8 years ago . The $2 \times 2$ matrix ${A}$ satisfies $A^2 - 4 {A} - 7{I} = {0},$ where ${I}$ is the $2 \times 2$ identity matrix. Prove that ${A}$ is invertible. What is the best way to do this?",,['matrices']
85,Is $\exp:\mathbb{M_n}\to\mathbb{M_n}$ injective?,Is  injective?,\exp:\mathbb{M_n}\to\mathbb{M_n},"This is related to a personal exploration of isometries of directed graphs, motivated by my son's Lego Duplo train tracks and identifying ""interesting"" layouts.  If $M$ is the adjacency matrix for a particular directed graph corresponding to a track layout, $e^M$ can aid in identifying a representative of the equivalence class under isometry. This is not the best approach to isometries of directed graphs, but it did raise the interesting question: Let ${\mathbb{M}}_n$ be the space of square $n\times n$ matrices with real entries.  For any $M\in{\mathbb{M}}_n$ we have $$e^M=\exp(M)=\sum_{k=0}^\infty \frac{1}{k!}M^k.$$ Is $e^M$ injective? In other words, are there two distinct $M_0,M_1\in {\mathbb{M}}_n$ such that $e^{M_0}=e^{M_1}$? For $n=1$ it is clearly injective. At $n=2$ I haven't been able to convince myself (never mind prove) it is injective.","This is related to a personal exploration of isometries of directed graphs, motivated by my son's Lego Duplo train tracks and identifying ""interesting"" layouts.  If $M$ is the adjacency matrix for a particular directed graph corresponding to a track layout, $e^M$ can aid in identifying a representative of the equivalence class under isometry. This is not the best approach to isometries of directed graphs, but it did raise the interesting question: Let ${\mathbb{M}}_n$ be the space of square $n\times n$ matrices with real entries.  For any $M\in{\mathbb{M}}_n$ we have $$e^M=\exp(M)=\sum_{k=0}^\infty \frac{1}{k!}M^k.$$ Is $e^M$ injective? In other words, are there two distinct $M_0,M_1\in {\mathbb{M}}_n$ such that $e^{M_0}=e^{M_1}$? For $n=1$ it is clearly injective. At $n=2$ I haven't been able to convince myself (never mind prove) it is injective.",,"['matrices', 'exponentiation', 'matrix-equations', 'graph-isomorphism']"
86,How to calculate the inverse of a complex matrix?,How to calculate the inverse of a complex matrix?,,"How can I calculate the inverse of $$H = \pmatrix{ h_{00} & h_{01} \\ h_{10} & h_{11}},$$ where $h_{00}$, $h_{01}$, $h_{10}$, and $h_{11}$ are complex numbers?","How can I calculate the inverse of $$H = \pmatrix{ h_{00} & h_{01} \\ h_{10} & h_{11}},$$ where $h_{00}$, $h_{01}$, $h_{10}$, and $h_{11}$ are complex numbers?",,"['matrices', 'inverse']"
87,Solutions of Matrix Equation $XAX^{T}=A$ for unknown $X$,Solutions of Matrix Equation  for unknown,XAX^{T}=A X,"Given a square, symmetric positive semidefinite matrix $A$, I am looking for solutions to the matrix equation $XAX^{T}=A$ for unknown $X$ (not necessarily symmetric). Clearly, setting $X$ equal to the identity matrix solves the equation, but I was wondering if there is a systematic procedure for looking for the non-trivial solutions. The only restriction on $X$ is that it is invertible. Thanks!","Given a square, symmetric positive semidefinite matrix $A$, I am looking for solutions to the matrix equation $XAX^{T}=A$ for unknown $X$ (not necessarily symmetric). Clearly, setting $X$ equal to the identity matrix solves the equation, but I was wondering if there is a systematic procedure for looking for the non-trivial solutions. The only restriction on $X$ is that it is invertible. Thanks!",,['matrices']
88,Easy proof that $\exp{Xt} = I \Rightarrow X = 0$,Easy proof that,\exp{Xt} = I \Rightarrow X = 0,"Let $X\in \mathbb{C}^{n\times n}$ and $I$ is identity matrix , than if:   $$ \forall t\in \mathbb{R}\quad e^{Xt} = I $$   than   $$ X = 0. $$ I'm looking for short and slick proof of this statement. The only way I can think of to prove this is take Jordan normal form of $X = SJS^{-1}$. $J$ can be writen as $J = D+N$ where $D$ is diagonal and $N$ nillpotent(those ones on superdiagonal) and $DN = ND$ so $e^{D+N} = e^D e^N$. So we want to show that if $e^{(D+N)t} = I$ than $D=N=0$. Because $J$ is block diagonal we can work on those blocks separately. So we can assume that $D = \lambda I$ and $N$ is zero or has ones everywhere on superdiagonal. If I'm right than for nonzero $N$ we have $$ (e^{Nt})_{ij} = \frac{t^{j-i}}{(j-i)!} \qquad i\leq j $$ zero everywhere else. If $N$ is nonzero than: $$ e^{(D+N)t}_{ij} = e^{\lambda t} \frac{t^{j-i}}{(j-i)!}  \qquad i\leq j $$ So this would have to hold $$ e^{\lambda t} \frac{t^{j-i}}{(j-i)!} = \delta_{ij} \qquad i\leq j $$ That is not possible thus $N$ has to be zero. If $N$ is zero than we get $e^{\lambda t} = 1$. So $\lambda = 0$. I don't like this proof, there has to be simpler one.","Let $X\in \mathbb{C}^{n\times n}$ and $I$ is identity matrix , than if:   $$ \forall t\in \mathbb{R}\quad e^{Xt} = I $$   than   $$ X = 0. $$ I'm looking for short and slick proof of this statement. The only way I can think of to prove this is take Jordan normal form of $X = SJS^{-1}$. $J$ can be writen as $J = D+N$ where $D$ is diagonal and $N$ nillpotent(those ones on superdiagonal) and $DN = ND$ so $e^{D+N} = e^D e^N$. So we want to show that if $e^{(D+N)t} = I$ than $D=N=0$. Because $J$ is block diagonal we can work on those blocks separately. So we can assume that $D = \lambda I$ and $N$ is zero or has ones everywhere on superdiagonal. If I'm right than for nonzero $N$ we have $$ (e^{Nt})_{ij} = \frac{t^{j-i}}{(j-i)!} \qquad i\leq j $$ zero everywhere else. If $N$ is nonzero than: $$ e^{(D+N)t}_{ij} = e^{\lambda t} \frac{t^{j-i}}{(j-i)!}  \qquad i\leq j $$ So this would have to hold $$ e^{\lambda t} \frac{t^{j-i}}{(j-i)!} = \delta_{ij} \qquad i\leq j $$ That is not possible thus $N$ has to be zero. If $N$ is zero than we get $e^{\lambda t} = 1$. So $\lambda = 0$. I don't like this proof, there has to be simpler one.",,"['matrices', 'lie-groups', 'lie-algebras', 'exponential-function']"
89,Order of matrices in $SL_2({\mathbb{F}_q})$,Order of matrices in,SL_2({\mathbb{F}_q}),"Could you tell me how to prove that in $SL_2({\mathbb{F}_q})$ the only element of even order is $-I$ ($ \ I$ - identity  matrix)? I would really appreciate a thorough explanation, because I cannot find anything on my own. Here , page 29, is why I'm asking.","Could you tell me how to prove that in $SL_2({\mathbb{F}_q})$ the only element of even order is $-I$ ($ \ I$ - identity  matrix)? I would really appreciate a thorough explanation, because I cannot find anything on my own. Here , page 29, is why I'm asking.",,"['abstract-algebra', 'matrices', 'finite-fields']"
90,Find the inverse of a $4\times4$ matrix,Find the inverse of a  matrix,4\times4,"My matrix looks like this: $$\left(\begin{array}{rrrr}     1&  1 & 1 & 1\\     1& -1 & 1 & 0\\     1&  1 & 0 & 0\\     1&  0 & 0 & 0  \end{array}\right)$$ The right lower half are all zeros. Is there a quick way to find an inverse of this matrix? I have the solution, but I'm unable to find the algorithm to get the inverse.","My matrix looks like this: $$\left(\begin{array}{rrrr}     1&  1 & 1 & 1\\     1& -1 & 1 & 0\\     1&  1 & 0 & 0\\     1&  0 & 0 & 0  \end{array}\right)$$ The right lower half are all zeros. Is there a quick way to find an inverse of this matrix? I have the solution, but I'm unable to find the algorithm to get the inverse.",,"['matrices', 'inverse']"
91,Traceless matrices and commutators,Traceless matrices and commutators,,"Any traceless $n\times n$ matrix with coefficients in a field of caracteristic $0$ is a commutator (or Lie bracket) of two matrices. What happens when the field has positive caracteristic? When trying to reproduce the proof I have in the caracteristic $0$ case for the positive caractersitic case, I run into two problems: multiples of the identity may have trace $=0$. a matrix may have a spectrum equal to the whole field. Are all traceless matrices commutators? If not, for which $n\in\mathbb{N}\setminus\lbrace 0  \rbrace$ and fields $k$ does it still hold? EDIT given a traceless matrix $M$, I want to know wether there are two matrices $A,B$ with $M=AB-BA$.","Any traceless $n\times n$ matrix with coefficients in a field of caracteristic $0$ is a commutator (or Lie bracket) of two matrices. What happens when the field has positive caracteristic? When trying to reproduce the proof I have in the caracteristic $0$ case for the positive caractersitic case, I run into two problems: multiples of the identity may have trace $=0$. a matrix may have a spectrum equal to the whole field. Are all traceless matrices commutators? If not, for which $n\in\mathbb{N}\setminus\lbrace 0  \rbrace$ and fields $k$ does it still hold? EDIT given a traceless matrix $M$, I want to know wether there are two matrices $A,B$ with $M=AB-BA$.",,"['matrices', 'finite-fields']"
92,Using rotation matrix vs sin & cos,Using rotation matrix vs sin & cos,,"I am trying to understand when/why you would use the 2D rotation matrix vs just using $\cos$ and $\sin$ in order to change a point's position in 2D space. For example, I have a straight line from point A and point B. I want to change the location of point B but keep it the same distance ( $length$ ) away from point A so that the line would draw to point B's new position. What would be the benefit of using the matrix below: $$\begin{bmatrix}\cos \theta &-\sin \theta \\\sin \theta &\cos \theta \end{bmatrix}$$ ...instead of just using: $Bx = Ax + \cos(\theta) * length$ $By = Ay + \sin(\theta) * length$","I am trying to understand when/why you would use the 2D rotation matrix vs just using and in order to change a point's position in 2D space. For example, I have a straight line from point A and point B. I want to change the location of point B but keep it the same distance ( ) away from point A so that the line would draw to point B's new position. What would be the benefit of using the matrix below: ...instead of just using:",\cos \sin length \begin{bmatrix}\cos \theta &-\sin \theta \\\sin \theta &\cos \theta \end{bmatrix} Bx = Ax + \cos(\theta) * length By = Ay + \sin(\theta) * length,"['matrices', 'trigonometry']"
93,Prove that $ \det(A^4 + A^2 B^2 + 2A^2 + I) \geq 0 $,Prove that, \det(A^4 + A^2 B^2 + 2A^2 + I) \geq 0 ,"Problem: Let $ A $ and $ B $ be an $ n \times n $ matrices with real entries. If $ AB = -BA $ , prove that $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} \geq 0. $$ My Approach: If $ A $ invertible, then $$ AB = -BA \implies AA^{-1}B = -BA^{-1}A \implies B = -B \implies B = O. $$ So, $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{\left(A^4 + 2A^2 + I\right)} = {\det}{\left(\left(A^2 + I\right)^{2}\right)} = \left({\det}{\left(A^{2} + I\right)}\right)^2 \geq 0. $$ If $ B $ invertible, then $$ AB = -BA \implies AB^{-1}B = -BB^{-1}A \implies A = -A \implies A = O. $$ So, $$ {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{(I)} = 1 \geq 0. $$ My Questions: Can we 'center' multiply both sides of the equation $ AB = -BA $ with a matrix? For example, in my solutions, $ AB = -BA \implies AA^{-1}B = -BA^{-1}A $ . How do we prove it when $ A $ and $ B $ not invertible? Thanks","Problem: Let and be an matrices with real entries. If , prove that My Approach: If invertible, then So, If invertible, then So, My Questions: Can we 'center' multiply both sides of the equation with a matrix? For example, in my solutions, . How do we prove it when and not invertible? Thanks", A   B   n \times n   AB = -BA   {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} \geq 0.   A   AB = -BA \implies AA^{-1}B = -BA^{-1}A \implies B = -B \implies B = O.   {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{\left(A^4 + 2A^2 + I\right)} = {\det}{\left(\left(A^2 + I\right)^{2}\right)} = \left({\det}{\left(A^{2} + I\right)}\right)^2 \geq 0.   B   AB = -BA \implies AB^{-1}B = -BB^{-1}A \implies A = -A \implies A = O.   {\det}{\left(A^4 + A^2B^2 + 2A^2 + I\right)} = {\det}{(I)} = 1 \geq 0.   AB = -BA   AB = -BA \implies AA^{-1}B = -BA^{-1}A   A   B ,"['matrices', 'inequality', 'determinant']"
94,What is inverse of a matrix whose diagonal elements are all zero.,What is inverse of a matrix whose diagonal elements are all zero.,,"I am a science researcher and I got a problem to find the generic inverse of the following matrix: $$  A_n = \left(\begin{array}{ccc}         0 & a_2 & a_3 & ... & a_{n-1} & a_n \\       a_1 &   0 & a_3 & ... & a_{n-1} & a_n \\       a_1 & a_2 &   0 & ... & a_{n-1} & a_n \\       ... & ... & ... & ... & ...     & ... \\       ... & ... & ... & ... & ...     & ... \\       a_1 & a_2 & a_3 & ... &       0 & a_n \\       a_1 & a_2 & a_3 & ... & a_{n-1} &   0 \\       \end{array}\right)  $$ I figured out that for n=2,3 $$ A_2^{-1} = \left(\begin{array}{ccc}         0 & a_2^{-1}  \\       a_1^{-1} &   0  \\       \end{array}\right)  $$ $$ A_3^{-1} = \frac{1}{2} \left(\begin{array}{ccc}         -a_1^{-1} &  a_1^{-1} &  a_1^{-1} \\          a_2^{-1} & -a_2^{-1} &  a_2^{-1} \\          a_3^{-1} &  a_3^{-1} & -a_3^{-1} \\       \end{array}\right)  $$ but can we extend it to a general case? Can anyone help?? Thanks! [later] It looks like it is $$ m_{ij} =  -\frac{n-2}{n-1} a_{i}^{-1}\ (i=j)   $$ $$        = \frac{1}{n-1} a_{i}^{-1}(else) $$","I am a science researcher and I got a problem to find the generic inverse of the following matrix: I figured out that for n=2,3 but can we extend it to a general case? Can anyone help?? Thanks! [later] It looks like it is"," 
A_n = \left(\begin{array}{ccc}
        0 & a_2 & a_3 & ... & a_{n-1} & a_n \\
      a_1 &   0 & a_3 & ... & a_{n-1} & a_n \\
      a_1 & a_2 &   0 & ... & a_{n-1} & a_n \\
      ... & ... & ... & ... & ...     & ... \\
      ... & ... & ... & ... & ...     & ... \\
      a_1 & a_2 & a_3 & ... &       0 & a_n \\
      a_1 & a_2 & a_3 & ... & a_{n-1} &   0 \\
      \end{array}\right) 
 
A_2^{-1} = \left(\begin{array}{ccc}
        0 & a_2^{-1}  \\
      a_1^{-1} &   0  \\
      \end{array}\right) 
 
A_3^{-1} = \frac{1}{2} \left(\begin{array}{ccc}
        -a_1^{-1} &  a_1^{-1} &  a_1^{-1} \\
         a_2^{-1} & -a_2^{-1} &  a_2^{-1} \\
         a_3^{-1} &  a_3^{-1} & -a_3^{-1} \\
      \end{array}\right) 
 
m_{ij} =  -\frac{n-2}{n-1} a_{i}^{-1}\ (i=j)  
 
       = \frac{1}{n-1} a_{i}^{-1}(else)
","['matrices', 'matrix-calculus']"
95,Any reference(a book) that defines the $n$-dimensional rotation matrix?,Any reference(a book) that defines the -dimensional rotation matrix?,n,"I want to refer to a mathematics book that explains the n-dimensional rotation matrix or rotation transformation. Wikipedia concentrates most on 2D or 3D. There are things that one can say definition here and there , but I think it is not a good idea to use the definition there. Actually they don't seem to be definitions. Strang's ""Linear Algebra"", Barret O'neill's ""Elementary Differential Geometry"" deal only with 2D or 3D cases. I think physicist are more interested in the general case, due to the theory of relativity. I found one explanation in ""Geometrical Methods of Mathematical Physics"" by Bernard Schutz. But I think it doesn't define the rotation matrix. Artin's ""Geometric Alegebra"" defines the rotation group as an isometry $\sigma:V\to V$ such that $\det\sigma=1$ . But the language there is so abstract that I can't catch any of them. Can anyone give a reference that defines rotation transformation on $\mathbb R^n$ and state as a property that $A$ is a rotation matrix if and only if $A\in SO(n)$ ? This is the end of the question and the below is what I wanted to do. I wanted to prove that if $A\in SO(n)$ , then $A$ is a rotation about a line through the origin in $\mathbb R^n$ . So I need to define the rotation transformation(or matrix) in $n$ dimensional Euclidean space.","I want to refer to a mathematics book that explains the n-dimensional rotation matrix or rotation transformation. Wikipedia concentrates most on 2D or 3D. There are things that one can say definition here and there , but I think it is not a good idea to use the definition there. Actually they don't seem to be definitions. Strang's ""Linear Algebra"", Barret O'neill's ""Elementary Differential Geometry"" deal only with 2D or 3D cases. I think physicist are more interested in the general case, due to the theory of relativity. I found one explanation in ""Geometrical Methods of Mathematical Physics"" by Bernard Schutz. But I think it doesn't define the rotation matrix. Artin's ""Geometric Alegebra"" defines the rotation group as an isometry such that . But the language there is so abstract that I can't catch any of them. Can anyone give a reference that defines rotation transformation on and state as a property that is a rotation matrix if and only if ? This is the end of the question and the below is what I wanted to do. I wanted to prove that if , then is a rotation about a line through the origin in . So I need to define the rotation transformation(or matrix) in dimensional Euclidean space.",\sigma:V\to V \det\sigma=1 \mathbb R^n A A\in SO(n) A\in SO(n) A \mathbb R^n n,"['matrices', 'geometry', 'reference-request', 'rotations']"
96,"Show that $A^{-1} + B^{-1}$ is invertible when $A,B$ and $A+B$ are invertible",Show that  is invertible when  and  are invertible,"A^{-1} + B^{-1} A,B A+B","I have the following issue: $A,B\in\mathbb C^{n\times n}$ invertible, such that also $A + B$ is invertible. How is it shown that $A^{-1} + B^{-1}$ is invertible?","I have the following issue: invertible, such that also is invertible. How is it shown that is invertible?","A,B\in\mathbb C^{n\times n} A + B A^{-1} + B^{-1}","['matrices', 'matrix-equations']"
97,What are the homotopy groups of the space of matrices with rank bigger than $k$?,What are the homotopy groups of the space of matrices with rank bigger than ?,k,"Let $H_{>k}$ be the space of real $d \times d$ matrices of rank bigger than $k$, for some fixed $k$. What are the homotopy groups $\pi_n(H_{>k})$? In particular, I would like to know whether or not they are finitely generated for $n \ge 2$? (The reason is that this is a necessary condition for a manifold to be a homogeneous space , and I wonder whether or not $H_{>k}$ is such a space .)","Let $H_{>k}$ be the space of real $d \times d$ matrices of rank bigger than $k$, for some fixed $k$. What are the homotopy groups $\pi_n(H_{>k})$? In particular, I would like to know whether or not they are finitely generated for $n \ge 2$? (The reason is that this is a necessary condition for a manifold to be a homogeneous space , and I wonder whether or not $H_{>k}$ is such a space .)",,"['matrices', 'algebraic-topology', 'homotopy-theory', 'matrix-rank', 'higher-homotopy-groups']"
98,Is the function $A \mapsto \det(A)$ convex over the set of positive definite matrices?,Is the function  convex over the set of positive definite matrices?,A \mapsto \det(A),"Let $\mathbb{P}^{n \times n}(\mathbb{R})$ denote the set of positive definite matrices. Is the following function convex? $$ \det: A\in \mathbb{P}^{n \times n}(\mathbb{R}) \to \det (A)$$ I think the answer is yes, but I cannot prove it directly using the definition of convex function. How can I do?","Let denote the set of positive definite matrices. Is the following function convex? I think the answer is yes, but I cannot prove it directly using the definition of convex function. How can I do?",\mathbb{P}^{n \times n}(\mathbb{R})  \det: A\in \mathbb{P}^{n \times n}(\mathbb{R}) \to \det (A),"['matrices', 'convex-analysis', 'determinant']"
99,Show that $\det(I-xx{^T})=0$ if $x{^T}x=1$,Show that  if,\det(I-xx{^T})=0 x{^T}x=1,Show that $$\det(I-xx{^T})=0$$ if $x{^T}x=1$.,Show that $$\det(I-xx{^T})=0$$ if $x{^T}x=1$.,,"['matrices', 'determinant']"
