,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Characterizing a Function Using the Gradient,Characterizing a Function Using the Gradient,,"I am watching one of Ted Shifrin's multivariable calculus lectures and I have a question about some of the reasoning used in one of the examples. I will state the reasoning below. Suppose $f:\mathbb{R}^2 \to \mathbb{R}$ is differentiable and $$x^2 \frac{\partial f}{\partial x} - y \frac{\partial f}{\partial y}  = 0$$ everywhere. We can use the gradient to determine what the graph of $f$ looks like. Note that this equation above may be rewritten as: $$\nabla{f} \cdot \begin{bmatrix}x^2 \\ -y\end{bmatrix}=0$$ Thus, we see that $\begin{bmatrix}x^2 \\ -y\end{bmatrix}$ is tangent to the level curve through $\begin{bmatrix}x \\ y\end{bmatrix}$ . On a level curve, the slope at $\langle{x,y}\rangle$ is $-\frac{y}{x^2}$ . Suppose that the level curve is given by $y = g(x)$ . Then $$g'(x) = \frac{dy}{dx} = -\frac{y}{x^2}$$ We use separation of variables to compute $$ \begin{align*}\frac{dy}{dx} = -\frac{y}{x^2} &\iff \frac{1}{y} dy = -\frac{1}{x^2}dx \\  \log|y| &= \frac{1}{x} + C\end{align*} $$ Exponentiating, we see $$|y| = ce^{\frac{1}{x}}, \quad \text{ for } c \in \mathbb{R}$$ Consider an arbitrary differentiable function $\Phi(y), \enspace  y\geq 0$ . Setting $f(1,y) = \Phi(y)$ should uniquely determine $f$ for all $x,y \geq 0$ . Observe that $\Phi$ gives us information regarding the behavior of $f$ on the line $x=1$ . Given $\langle{X,Y}\rangle$ , we can determine the constant associated with the level curve by considering the restriction $$Y=ce^{\frac{1}{X}} \implies c = e^{-\frac{1}{X}}Y$$ Now, along the line $x=1$ , we know that it must intersect the level curve at $y = ce^{\frac{1}{1}}= ce$ . So, we see that $f(X,Y) = \Phi(e\cdot e^{-\frac{1}{X}} Y) $ . In summary: $$f(x,y) = \Phi(e^{1-\frac{1}{x}}y)$$ My questions are: How did we know to consider the form $y=g(x)$ for the level curve? In fact, we do not exactly get a function as our answer. Is the purpose of this just to specify some arbitrary function and then consider what possible functions satisfy the condition given by our knowledge of the slope at different points? How was $f(1,y) = \Phi(y)$ chosen? Could we have considered a line other than $x=1$ for our definition of $\Phi$ and determined functions that satisfy this differential equation? Why does considering the point $(1, ce)$ and $\Phi(ce)$ happen to give us the correct equation for $f(x,y)$ ?","I am watching one of Ted Shifrin's multivariable calculus lectures and I have a question about some of the reasoning used in one of the examples. I will state the reasoning below. Suppose is differentiable and everywhere. We can use the gradient to determine what the graph of looks like. Note that this equation above may be rewritten as: Thus, we see that is tangent to the level curve through . On a level curve, the slope at is . Suppose that the level curve is given by . Then We use separation of variables to compute Exponentiating, we see Consider an arbitrary differentiable function . Setting should uniquely determine for all . Observe that gives us information regarding the behavior of on the line . Given , we can determine the constant associated with the level curve by considering the restriction Now, along the line , we know that it must intersect the level curve at . So, we see that . In summary: My questions are: How did we know to consider the form for the level curve? In fact, we do not exactly get a function as our answer. Is the purpose of this just to specify some arbitrary function and then consider what possible functions satisfy the condition given by our knowledge of the slope at different points? How was chosen? Could we have considered a line other than for our definition of and determined functions that satisfy this differential equation? Why does considering the point and happen to give us the correct equation for ?","f:\mathbb{R}^2 \to \mathbb{R} x^2 \frac{\partial f}{\partial x} - y \frac{\partial f}{\partial y}  = 0 f \nabla{f} \cdot \begin{bmatrix}x^2 \\ -y\end{bmatrix}=0 \begin{bmatrix}x^2 \\ -y\end{bmatrix} \begin{bmatrix}x \\ y\end{bmatrix} \langle{x,y}\rangle -\frac{y}{x^2} y = g(x) g'(x) = \frac{dy}{dx} = -\frac{y}{x^2}  \begin{align*}\frac{dy}{dx} = -\frac{y}{x^2} &\iff \frac{1}{y} dy = -\frac{1}{x^2}dx \\ 
\log|y| &= \frac{1}{x} + C\end{align*}  |y| = ce^{\frac{1}{x}}, \quad \text{ for } c \in \mathbb{R} \Phi(y), \enspace  y\geq 0 f(1,y) = \Phi(y) f x,y \geq 0 \Phi f x=1 \langle{X,Y}\rangle Y=ce^{\frac{1}{X}} \implies c = e^{-\frac{1}{X}}Y x=1 y = ce^{\frac{1}{1}}= ce f(X,Y) = \Phi(e\cdot e^{-\frac{1}{X}} Y)  f(x,y) = \Phi(e^{1-\frac{1}{x}}y) y=g(x) f(1,y) = \Phi(y) x=1 \Phi (1, ce) \Phi(ce) f(x,y)",['multivariable-calculus']
1,"Struggling with a Calculus III problem :$\iint x+y\ dS,$ where $S(u,v)=2\cos(u)\vec i+2\sin(u)\vec j+v \vec k$ and $0\le u\le \pi/2;\;0 \le v \le9.$",Struggling with a Calculus III problem : where  and,"\iint x+y\ dS, S(u,v)=2\cos(u)\vec i+2\sin(u)\vec j+v \vec k 0\le u\le \pi/2;\;0 \le v \le9.","So I have had Calc III many moons ago but I cannot seem to solve this problem for my son who is taking it now.  Worked it four ways and got four different answers.  Hoping someone here can set me right. Problem: Solve $\iint f(x,y)\,dS,$ where $$f(x,y) = x + y$$ and $$S(u,v) = 2\cos(u)\vec i + 2\sin(u)\vec j + v \vec k$$ $$0\le u \le \pi/2;\;0 \le v \le 9. $$ My solution is: $$\frac{\partial S}{\partial u} = -2\sin(u) \vec i + 2\cos(u) \vec j$$ $$\frac{\partial S}{\partial v} = \vec k$$ $$\frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} = 2\cos(u) \vec i - 2 \sin(u) \vec j $$ $$\left\lvert\frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} \right\rvert = \sqrt{4\cos^2(u) + 4 \sin^2(u)}=2$$ $$\iint(x+y)\,dS =\iint(x+y)2\,dA$$ $$\int^{\pi/2}_0\int^2_0(r\cos\theta +r\sin\theta) 2r\,dr\,d\theta =$$ $$\int^{\pi/2}_0\int^2_02r^2(\cos\theta +\sin\theta) \,dr\,d\theta =$$ $$\int^{\pi/2}_0(16/3)(\cos\theta +\sin\theta) \,d\theta =$$ $$32/3.$$ His online program doesn’t like this answer.  I got the same answer by staying in rectangular coordinates.  Feel like I’m setting something up wrong. Thanks in advance for any help.",So I have had Calc III many moons ago but I cannot seem to solve this problem for my son who is taking it now.  Worked it four ways and got four different answers.  Hoping someone here can set me right. Problem: Solve where and My solution is: His online program doesn’t like this answer.  I got the same answer by staying in rectangular coordinates.  Feel like I’m setting something up wrong. Thanks in advance for any help.,"\iint f(x,y)\,dS, f(x,y) = x + y S(u,v) = 2\cos(u)\vec i + 2\sin(u)\vec j + v \vec k 0\le u \le \pi/2;\;0 \le v \le 9.  \frac{\partial S}{\partial u} = -2\sin(u) \vec i + 2\cos(u) \vec j \frac{\partial S}{\partial v} = \vec k \frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} = 2\cos(u) \vec i - 2 \sin(u) \vec j  \left\lvert\frac{\partial S}{\partial u} \times \frac{\partial S}{\partial v} \right\rvert = \sqrt{4\cos^2(u) + 4 \sin^2(u)}=2 \iint(x+y)\,dS =\iint(x+y)2\,dA \int^{\pi/2}_0\int^2_0(r\cos\theta +r\sin\theta) 2r\,dr\,d\theta = \int^{\pi/2}_0\int^2_02r^2(\cos\theta +\sin\theta) \,dr\,d\theta = \int^{\pi/2}_0(16/3)(\cos\theta +\sin\theta) \,d\theta = 32/3.","['calculus', 'integration', 'multivariable-calculus', 'surface-integrals', 'cylindrical-coordinates']"
2,"Problem 3-33 in ""Calculus on Manifolds"" by Michael Spivak. What is $g$? I cannot understand the author's intention for this problem at all.","Problem 3-33 in ""Calculus on Manifolds"" by Michael Spivak. What is ? I cannot understand the author's intention for this problem at all.",g,"Problem 3-33. If $f:[a,b]\times [c,d]\to\mathbb{R}$ is continuous and $D_2f$ is continuous, define $F(x,y)=\int_a^x f(t,y)dt$ . (a) Find $D_1F$ and $D_2F$ . (b) If $G(x)=\int_a^{g(x)} f(t,x)dt$ , find $G'(x)$ . I solved (a) as follows: $D_1F(x,y)=f(x,y)$ by a famous theorem in one variable calculus. $D_2F(x,y)=\int_a^x D_2f(t,y)dt$ by Problem 3-32 . But I could not solve (b). At first, what is $g$ ? I guess $g$ is a function from $[c,d]$ to $[a,b]$ and $g$ is differentiable on $[c,d]$ . $G(x)=F(g(x),x)$ . $G'(x)=D_1F(g(x),x)\cdot g'(x)+D_2F(g(x),x)=f(g(x),x)\cdot g'(x)+\int_a^{g(x)} D_2f(t,x)dt$ . I cannot understand the author's intention for this problem at all.","Problem 3-33. If is continuous and is continuous, define . (a) Find and . (b) If , find . I solved (a) as follows: by a famous theorem in one variable calculus. by Problem 3-32 . But I could not solve (b). At first, what is ? I guess is a function from to and is differentiable on . . . I cannot understand the author's intention for this problem at all.","f:[a,b]\times [c,d]\to\mathbb{R} D_2f F(x,y)=\int_a^x f(t,y)dt D_1F D_2F G(x)=\int_a^{g(x)} f(t,x)dt G'(x) D_1F(x,y)=f(x,y) D_2F(x,y)=\int_a^x D_2f(t,y)dt g g [c,d] [a,b] g [c,d] G(x)=F(g(x),x) G'(x)=D_1F(g(x),x)\cdot g'(x)+D_2F(g(x),x)=f(g(x),x)\cdot g'(x)+\int_a^{g(x)} D_2f(t,x)dt","['integration', 'multivariable-calculus', 'partial-derivative']"
3,Slope of a hyperplane,Slope of a hyperplane,,"I was reading a book (Nonlinear Elliptic Equations of the Second Order by Qin Han), and there was something which I didn't really understand. I will try to simply the setting for my question. Let $a\in \mathbb R^n \setminus \{0\}.$ We consider the function $\ell(x)=M - a\cdot x$ , where $M$ is a positive constant and $x\in \mathbb R^n$ . Then the graph of $\ell$ shall be a hyperplane (in $\mathbb R^{n+1}$ ) passing through the point $(0,M)$ , where $0\in \mathbb R^n$ . The author wrote that, the graph of $\ell$ has the slope $a$ , which I didn't understand. I have two questions. Q1: I looked up on MSE and someone said that the gradient is just the slope (of a plane), cf. https://math.stackexchange.com/a/712763/507382. Is this the definition of the slope of a plane? Why does it make sense geometrically? Q2: Intuitively, $\nabla \ell= -a$ , and the graph of $\ell$ shall have the slope $-a$ , which is not $a$ as he said? Thanks for help.","I was reading a book (Nonlinear Elliptic Equations of the Second Order by Qin Han), and there was something which I didn't really understand. I will try to simply the setting for my question. Let We consider the function , where is a positive constant and . Then the graph of shall be a hyperplane (in ) passing through the point , where . The author wrote that, the graph of has the slope , which I didn't understand. I have two questions. Q1: I looked up on MSE and someone said that the gradient is just the slope (of a plane), cf. https://math.stackexchange.com/a/712763/507382. Is this the definition of the slope of a plane? Why does it make sense geometrically? Q2: Intuitively, , and the graph of shall have the slope , which is not as he said? Thanks for help.","a\in \mathbb R^n \setminus \{0\}. \ell(x)=M - a\cdot x M x\in \mathbb R^n \ell \mathbb R^{n+1} (0,M) 0\in \mathbb R^n \ell a \nabla \ell= -a \ell -a a","['multivariable-calculus', 'plane-geometry']"
4,Paraboloid and Sphere,Paraboloid and Sphere,,"Find the volume of the solid region inside the sphere $x^2+y^2+z^2=6$ and above the paraboloid $z=x^2+y^2.$ I set both equations equal to each other and obtained $z=2$ and $z=-3$ . Since clearly $z>0,$ that means I only consider $z=2.$ Subbing this into both equations gives $x^2+y^2=2$ . I sketch this circle and obtain $0 \leq \theta \leq 2 \pi$ and $0 \leq r \leq \sqrt2.$ Also, clearly $r^2 \leq z \leq \sqrt{6-r^2}.$ Now I want to convert into spherical coordinates. I don't know how to find the limits for $\phi$ . I think the lower limit is $0$ but I am not sure about the upper limit. I think the upper limit for $p$ is $6$ . Not sure how to find the rest.","Find the volume of the solid region inside the sphere and above the paraboloid I set both equations equal to each other and obtained and . Since clearly that means I only consider Subbing this into both equations gives . I sketch this circle and obtain and Also, clearly Now I want to convert into spherical coordinates. I don't know how to find the limits for . I think the lower limit is but I am not sure about the upper limit. I think the upper limit for is . Not sure how to find the rest.","x^2+y^2+z^2=6 z=x^2+y^2. z=2 z=-3 z>0, z=2. x^2+y^2=2 0 \leq \theta \leq 2 \pi 0 \leq r \leq \sqrt2. r^2 \leq z \leq \sqrt{6-r^2}. \phi 0 p 6","['calculus', 'integration', 'geometry', 'multivariable-calculus', 'spherical-coordinates']"
5,Help for evaluating the line integrals with Green's Theorem,Help for evaluating the line integrals with Green's Theorem,,"Earlier, when I scrolled the Instagram posts I found a mathematical problem uploaded by The Vegan Math Guy like the following because this problem looks interesting to me to be solved. The mathematical problem is to find $\displaystyle \int\limits_C x^4 \,\mathrm dx+ xy \,\mathrm dy$ where $C$ is the area shown by the following graph. In my solution, I'd use Green's Theorem because I've seen this line same as told in Ron Larson's calculus book at Theorem 15.8. Green's Theorem says that: Let $R$ be a simply connected region with a piecewise smooth boundary $C$ , oriented counterclockwise (that is $C$ is traversed once so that the region $R$ always lies to the left). If $M$ and $N$ have continuous first partial derivatives in an open region containing $D$ , then $$\displaystyle \int\limits_C M\,\mathrm dx+N\,\mathrm dy=\iint\limits_R\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA$$ Then I follow the steps as demonstrated in Example 1 on p. 1025, like this: Examine the conditions Is $C$ positively oriented? Yes. Is $C$ piecewise smooth? Yes. Is $C$ closed and simply connected? Yes. Find the partial derivative the function have continuous first partial derivatives. Let $M=xy$ and $N=x^4$ . \begin{align*} \frac{\partial M}{\partial y}\implies\frac{\partial }{\partial y}\left[ xy \right]&=x\frac{\partial}{\partial y}[y]\\&=x \\ \frac{\partial N}{\partial x}\implies\frac{\partial }{\partial x}\left[ x^4 \right]&=4x^3 \end{align*} Then taking Green's Theorem, I evaluated like this: \begin{align*} \displaystyle \int\limits_C x^4\,\mathrm dx+xy\,\mathrm dy&=\iint\limits_D\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA \\&=\int\limits_0^1\int\limits_{0}^{1-x}\left( 4x^3 \right)-x\,\mathrm dy\mathrm dx\\&= \int\limits_0^1 \left[ 4x^3y-xy \right]^{1-x}_0\,\mathrm dx\\&=\int\limits_0^1\left( 4x^3(1-x)-x(1-x) \right)\,\mathrm dx\\&=\int\limits_0^1 4x^3-4x^4-x+x^2\,\mathrm dx\\&=\left[ x^4-\frac{4x^5}{5}-\frac{x^2}{2}+\frac{x^3}{3} \right]_0^1\\&=1-\frac{4}{5}-\frac12+\frac13\\&=\dfrac{1}{30} \end{align*} After evaluating the line integral, I see why my line integral is different than the one of the comments on Instagram @a_generic_nerd2 that shares his solution. This user got $\frac16$ while me $\frac{1}{30}$ , because I making that taking the partial derivative of $x^4$ with respect to $x$ for $N$ and for $M$ , I taking the partial derivative of $xy$ with respect to $y$ , but this user does reverse from what I do on the partial derivative. In your opinion, am I doing wrong? Here's the link that leads to the source I mentioned in this problem.","Earlier, when I scrolled the Instagram posts I found a mathematical problem uploaded by The Vegan Math Guy like the following because this problem looks interesting to me to be solved. The mathematical problem is to find where is the area shown by the following graph. In my solution, I'd use Green's Theorem because I've seen this line same as told in Ron Larson's calculus book at Theorem 15.8. Green's Theorem says that: Let be a simply connected region with a piecewise smooth boundary , oriented counterclockwise (that is is traversed once so that the region always lies to the left). If and have continuous first partial derivatives in an open region containing , then Then I follow the steps as demonstrated in Example 1 on p. 1025, like this: Examine the conditions Is positively oriented? Yes. Is piecewise smooth? Yes. Is closed and simply connected? Yes. Find the partial derivative the function have continuous first partial derivatives. Let and . Then taking Green's Theorem, I evaluated like this: After evaluating the line integral, I see why my line integral is different than the one of the comments on Instagram @a_generic_nerd2 that shares his solution. This user got while me , because I making that taking the partial derivative of with respect to for and for , I taking the partial derivative of with respect to , but this user does reverse from what I do on the partial derivative. In your opinion, am I doing wrong? Here's the link that leads to the source I mentioned in this problem.","\displaystyle \int\limits_C x^4 \,\mathrm dx+ xy \,\mathrm dy C R C C R M N D \displaystyle \int\limits_C M\,\mathrm dx+N\,\mathrm dy=\iint\limits_R\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA C C C M=xy N=x^4 \begin{align*} \frac{\partial M}{\partial y}\implies\frac{\partial }{\partial y}\left[ xy \right]&=x\frac{\partial}{\partial y}[y]\\&=x \\ \frac{\partial N}{\partial x}\implies\frac{\partial }{\partial x}\left[ x^4 \right]&=4x^3 \end{align*} \begin{align*} \displaystyle \int\limits_C x^4\,\mathrm dx+xy\,\mathrm dy&=\iint\limits_D\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA \\&=\int\limits_0^1\int\limits_{0}^{1-x}\left( 4x^3 \right)-x\,\mathrm dy\mathrm dx\\&= \int\limits_0^1 \left[ 4x^3y-xy \right]^{1-x}_0\,\mathrm dx\\&=\int\limits_0^1\left( 4x^3(1-x)-x(1-x) \right)\,\mathrm dx\\&=\int\limits_0^1 4x^3-4x^4-x+x^2\,\mathrm dx\\&=\left[ x^4-\frac{4x^5}{5}-\frac{x^2}{2}+\frac{x^3}{3} \right]_0^1\\&=1-\frac{4}{5}-\frac12+\frac13\\&=\dfrac{1}{30} \end{align*} \frac16 \frac{1}{30} x^4 x N M xy y","['calculus', 'integration', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
6,Does problem 2-9 of Spivak's Calculus on Manifolds contain an error?,Does problem 2-9 of Spivak's Calculus on Manifolds contain an error?,,"In Michael Spivak's Calculus on Manifolds , problem 2-9, part (a) on page 18 reads as follows: Two functions $f,g:\mathbb R\to\mathbb R$ are equal up to $\mathbf n$ th order at $a$ if $$ \lim_{h\to 0}\frac{f(a+h)-g(a+h)}{h^n}=0 $$ a) Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g(x)=a_0+a_1(x-a)$ such that $f$ and $g$ are equal up to first order at $a$ . As written, I think the ""if"" direction is false. For instance, take $a=0$ , and define $f(x)$ as $x$ for $x\neq0$ , and as $1$ for $x=0$ . Then, $f$ is obviously not differentiable at $0$ , but $f$ and the identity function are equal up to first order at $0$ . Question: is my objection correct? If so, I think the problem can be repaired by requiring continuity. If $a\in\mathbb R$ , $f:\mathbb R\to\mathbb R$ is continuous at $a$ , and there is an affine map $g$ such that $f$ and $g$ are equal up to first order, then I believe it follows that $f$ is differentiable at $a$ .","In Michael Spivak's Calculus on Manifolds , problem 2-9, part (a) on page 18 reads as follows: Two functions are equal up to th order at if a) Show that is differentiable at if and only if there is a function of the form such that and are equal up to first order at . As written, I think the ""if"" direction is false. For instance, take , and define as for , and as for . Then, is obviously not differentiable at , but and the identity function are equal up to first order at . Question: is my objection correct? If so, I think the problem can be repaired by requiring continuity. If , is continuous at , and there is an affine map such that and are equal up to first order, then I believe it follows that is differentiable at .","f,g:\mathbb R\to\mathbb R \mathbf n a 
\lim_{h\to 0}\frac{f(a+h)-g(a+h)}{h^n}=0
 f a g g(x)=a_0+a_1(x-a) f g a a=0 f(x) x x\neq0 1 x=0 f 0 f 0 a\in\mathbb R f:\mathbb R\to\mathbb R a g f g f a","['real-analysis', 'multivariable-calculus']"
7,Intuition for the inverse Hessian matrix,Intuition for the inverse Hessian matrix,,"Consider a function $f:\mathbb{R}^n\to\mathbb{R}$ and denote by $H$ its Hessian matrix . I understand that $H$ provides a measure of the curvature of $f$ in all directions and plays a role in the Taylor approximation of $f$ . But I keep encountering the inverse of $H$ , and I was wondering if there is some intuition about $H^{-1}$ ? What does an element $H^{-1}_{ij}$ tell us about about the shape of $f$ ? Is there a geometric interpretation? EDIT: Some examples where the inverse Hessian plays a role. For instance, in maximum likelihood estimation the negative of the expected value of the Hessian of the log likelihood function is the Fisher information , and the inverse of that quantity gives us the covariance matrix of the estimated parameter. In Newton's optimization method , the inverse Hessian of the function tells us how big a step to take toward the optimum.","Consider a function and denote by its Hessian matrix . I understand that provides a measure of the curvature of in all directions and plays a role in the Taylor approximation of . But I keep encountering the inverse of , and I was wondering if there is some intuition about ? What does an element tell us about about the shape of ? Is there a geometric interpretation? EDIT: Some examples where the inverse Hessian plays a role. For instance, in maximum likelihood estimation the negative of the expected value of the Hessian of the log likelihood function is the Fisher information , and the inverse of that quantity gives us the covariance matrix of the estimated parameter. In Newton's optimization method , the inverse Hessian of the function tells us how big a step to take toward the optimum.",f:\mathbb{R}^n\to\mathbb{R} H H f f H H^{-1} H^{-1}_{ij} f,"['multivariable-calculus', 'approximation', 'curvature', 'hessian-matrix']"
8,"$ab+bc+ca=3,$ find the minimal value $k:$ $\left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3.$",find the minimal value,"ab+bc+ca=3, k: \left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3.","Let $a,b,c\ge 0: ab+bc+ca=3.$ Find the minimal value of constant $k$ satisfying $$\left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3.$$ I've proved when $k=\dfrac{1}{2}.$ Actually, by Holder $$\left(\sqrt{\frac{c+b}{a+cb}}\right)^2.\sum_{cyc}(b+c)^2(a+bc)(2a+b+c)^3\ge [\sum_{cyc}(b+c)(2a+b+c)]^3,$$ the following inequality is ugly but we can use $uvw.$ By checking $k=\dfrac{1}{3},$ the problem seems true but I've had no proof yet. So, what is the range of $k$ ? Thank you very much. Update I see that when $k\ge 1$ we can use AM-GM $$\left(\frac{a+b}{c+ab}\right)^k+k-1\ge k.\frac{a+b}{c+ab}$$ Similarly, we just need to prove $$\frac{a+b}{c+ab}+\frac{c+b}{a+cb}+\frac{a+c}{b+ca}\ge 3$$ But by C-S we will prove $$4(a+b+c)^2\ge 3.(6+3(a+b+c)-3abc) \iff 4(a+b+c)^2-9(a+b+c)-18+9abc\ge 0$$ Is that true approach ? Id est, we need to consider in case $k\le 1.$","Let Find the minimal value of constant satisfying I've proved when Actually, by Holder the following inequality is ugly but we can use By checking the problem seems true but I've had no proof yet. So, what is the range of ? Thank you very much. Update I see that when we can use AM-GM Similarly, we just need to prove But by C-S we will prove Is that true approach ? Id est, we need to consider in case","a,b,c\ge 0: ab+bc+ca=3. k \left(\frac{a+b}{c+ab}\right)^k+\left(\frac{c+b}{a+cb}\right)^k+\left(\frac{a+c}{b+ac}\right)^k\ge 3. k=\dfrac{1}{2}. \left(\sqrt{\frac{c+b}{a+cb}}\right)^2.\sum_{cyc}(b+c)^2(a+bc)(2a+b+c)^3\ge [\sum_{cyc}(b+c)(2a+b+c)]^3, uvw. k=\dfrac{1}{3}, k k\ge 1 \left(\frac{a+b}{c+ab}\right)^k+k-1\ge k.\frac{a+b}{c+ab} \frac{a+b}{c+ab}+\frac{c+b}{a+cb}+\frac{a+c}{b+ca}\ge 3 4(a+b+c)^2\ge 3.(6+3(a+b+c)-3abc) \iff 4(a+b+c)^2-9(a+b+c)-18+9abc\ge 0 k\le 1.","['multivariable-calculus', 'inequality']"
9,Can we classify all functions whose gradient is an eigenvector of the Hessian?,Can we classify all functions whose gradient is an eigenvector of the Hessian?,,"Let's treat the case of two dimensions, then we don't have freedom with the other eigenvector. Is there any classification of all smooth functions $f$ on $\mathbb{R^2}$ such that $\nabla f$ is an eigenvector of $D^2f$ at every point? Such functions solve a second-order quasilinear PDE, but makes me wonder if there is a reference with a more ""explicit"" classification.","Let's treat the case of two dimensions, then we don't have freedom with the other eigenvector. Is there any classification of all smooth functions on such that is an eigenvector of at every point? Such functions solve a second-order quasilinear PDE, but makes me wonder if there is a reference with a more ""explicit"" classification.",f \mathbb{R^2} \nabla f D^2f,"['matrices', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'hessian-matrix', 'scalar-fields']"
10,How to find the surface area of the spout of a teapot?,How to find the surface area of the spout of a teapot?,,"I am trying to find the surface area of a teapot. I found the surface area of the body of the teapot by taking a picture of it, graphing the picture and then calculating the surface area of revolution. However, I don't know how to find the surface area (or an estimate of the surface area) of the teapot's spout. Here are some pictures of teapots with spouts that looks like the one that my teapot has: I would seriously appreciate it if someone can give me details about how to find the surface area of the spout, or provide links to articles or papers that shows how to do this. Thank you in advance.","I am trying to find the surface area of a teapot. I found the surface area of the body of the teapot by taking a picture of it, graphing the picture and then calculating the surface area of revolution. However, I don't know how to find the surface area (or an estimate of the surface area) of the teapot's spout. Here are some pictures of teapots with spouts that looks like the one that my teapot has: I would seriously appreciate it if someone can give me details about how to find the surface area of the spout, or provide links to articles or papers that shows how to do this. Thank you in advance.",,"['calculus', 'geometry', 'multivariable-calculus', 'mathematical-modeling']"
11,"Is my proof of the multivariate chain rule, rigorous?","Is my proof of the multivariate chain rule, rigorous?",,"Theorem. Let $U$ , $V$ , $W$ be normed linear spaces. Let $\Omega$ be open in $U$ and $\Upsilon$ be open in $V$ . Let $f\colon \Omega\to\Upsilon$ be (Fréchet) differentiable at $c\in\Omega$ and $g\colon \Upsilon\to W$ be differentiable at $f(c)\in\Upsilon$ . Then $g\circ f$ is differentiable at $c$ with $$ D(g\circ f)(c) = Dg(f(c))\circ Df(c)\text. $$ My proof: Denote $L := Df(c)$ and $M := Dg(f(c))$ . Let $\delta_1 > 0$ be such that for all $y\in B_{\delta_1}(f(c))\cap\Upsilon\setminus\{f(c)\}$ , we have $$ \lVert g(y) - g(f(c)) - M(y - f(c)) \rVert < \epsilon \lVert y - f(c) \rVert.\tag{1} $$ Since $f$ will be continuous at $c$ , take $\delta_2 > 0$ such that for all $x\in B_{\delta_2}(c)\cap \Omega\setminus\{c\}$ , we have $$ \lVert f(x) - f(c) \rVert < \delta_2 $$ which due to (1) implies that $$ \lVert g(f(x)) - g(f(c)) - M(f(x) - f(c)) \rVert < \epsilon \lVert f(x) - f(c) \rVert.\tag{2} $$ W.l.o.g., assume that for all $x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\}$ , we have $$ \lVert f(x) - f(c) - L(x - c) \rVert < \epsilon\lVert x - c\rVert.\tag{3} $$ Now, for a general $x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\}$ , we have $$ \begin{align*} &\lVert g(f(x)) - g(f(c)) - ML(x - c)\rVert\\[1ex] & \le \lVert g(f(x)) - g(f(c)) - ML(f(x) - f(c))\rVert \\ & \phantom{abcdefghijklmn} + \lVert M(f(x) - f(c) - L(x - c))\rVert\\[1ex] & \le \epsilon\lVert f(x) - f(c) \rVert + \lVert M\rVert\, \lVert (f(x) - f(c) - L(x - c))\rVert & \text{by (2)}\\[1ex] & \le (\epsilon + \lVert M\rVert) \lVert f(x) - f(c) - L(x - c)\rVert  + \epsilon\lVert L(x - c)\\[1ex] & < (\epsilon + \lVert M\rVert)\epsilon\lVert x - c\rVert +\epsilon\lVert L\rVert\, \lVert x - c\rVert & \text{by (3)}\\[1ex] &= \epsilon(\epsilon + \lVert M\rVert +\lVert L\rVert)\lVert x - c\rVert. \end{align*} $$ And we are virtually done. Question: Do you spot any loose ends in the above proof? By $ML$ , I mean $M\circ L$ . Note that the spaces can be over $\mathbb R$ or $\mathbb C$ (but it's common for all, $U$ , $V$ , $W$ ), and possibly infinite dimensional.","Theorem. Let , , be normed linear spaces. Let be open in and be open in . Let be (Fréchet) differentiable at and be differentiable at . Then is differentiable at with My proof: Denote and . Let be such that for all , we have Since will be continuous at , take such that for all , we have which due to (1) implies that W.l.o.g., assume that for all , we have Now, for a general , we have And we are virtually done. Question: Do you spot any loose ends in the above proof? By , I mean . Note that the spaces can be over or (but it's common for all, , , ), and possibly infinite dimensional.","U V W \Omega U \Upsilon V f\colon \Omega\to\Upsilon c\in\Omega g\colon \Upsilon\to W f(c)\in\Upsilon g\circ f c 
D(g\circ f)(c) = Dg(f(c))\circ Df(c)\text.
 L := Df(c) M := Dg(f(c)) \delta_1 > 0 y\in B_{\delta_1}(f(c))\cap\Upsilon\setminus\{f(c)\} 
\lVert g(y) - g(f(c)) - M(y - f(c)) \rVert < \epsilon \lVert y - f(c) \rVert.\tag{1}
 f c \delta_2 > 0 x\in B_{\delta_2}(c)\cap \Omega\setminus\{c\} 
\lVert f(x) - f(c) \rVert < \delta_2
 
\lVert g(f(x)) - g(f(c)) - M(f(x) - f(c)) \rVert < \epsilon \lVert f(x) - f(c) \rVert.\tag{2}
 x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\} 
\lVert f(x) - f(c) - L(x - c) \rVert < \epsilon\lVert x - c\rVert.\tag{3}
 x\in B_{\delta_2}(c)\cap\Omega\setminus\{c\} 
\begin{align*}
&\lVert g(f(x)) - g(f(c)) - ML(x - c)\rVert\\[1ex]
& \le \lVert g(f(x)) - g(f(c)) - ML(f(x) - f(c))\rVert \\
& \phantom{abcdefghijklmn} + \lVert M(f(x) - f(c) - L(x - c))\rVert\\[1ex]
& \le \epsilon\lVert f(x) - f(c) \rVert + \lVert M\rVert\, \lVert (f(x) - f(c) - L(x - c))\rVert & \text{by (2)}\\[1ex]
& \le (\epsilon + \lVert M\rVert) \lVert f(x) - f(c) - L(x - c)\rVert  + \epsilon\lVert L(x - c)\\[1ex]
& < (\epsilon + \lVert M\rVert)\epsilon\lVert x - c\rVert +\epsilon\lVert L\rVert\, \lVert x - c\rVert & \text{by (3)}\\[1ex]
&= \epsilon(\epsilon + \lVert M\rVert +\lVert L\rVert)\lVert x - c\rVert.
\end{align*}
 ML M\circ L \mathbb R \mathbb C U V W","['multivariable-calculus', 'derivatives', 'solution-verification', 'normed-spaces', 'chain-rule']"
12,Limits of multivariable functions : How to find sequences that disprove their existence.,Limits of multivariable functions : How to find sequences that disprove their existence.,,"I understand that to check whether a multivariable function $f$ has a limit or is continuous for some coordinates - let's call them $x$ - one must find that : $$\forall(x_n)_n \vert \lim_{n\rightarrow+\infty}{x_n}=x : \lim_{n\rightarrow+\infty}{f(x_n)=f(x)}$$ However, I must say I do not understand the process of finding such $(x_n)_n$ when disproving the existence of such a limit ; and our teacher has made no attempt to explain. When giving the solutions to such problems, he just pulls a solution out of his hat and explains why it works. Which in my opinion completely fails to address the actual problem, checking that a solution works is usually trivial . So how would you go about finding such a sequence ? In essence what I'm asking is not ""What is the solution to such a problem"" but ""What is the process by which a solution is produced"". There is perhaps a funny parallel to $P$ vs $nP$ where solutions to $nP$ problems are easy to check but hard to find. For those that would prefer a concrete example, here is one. Study the existence of the following limit : $$\lim_{(x,y)\rightarrow(0,0)\,,\,(x, y)\ne(0, 0)}\frac{x^2y}{x+y}$$ ( You should find that such a limit does not exist, however what I am interested in is how you will go about finding two sequences such that they converge to $(0,0)$ but make the function itself converge to two different values )","I understand that to check whether a multivariable function has a limit or is continuous for some coordinates - let's call them - one must find that : However, I must say I do not understand the process of finding such when disproving the existence of such a limit ; and our teacher has made no attempt to explain. When giving the solutions to such problems, he just pulls a solution out of his hat and explains why it works. Which in my opinion completely fails to address the actual problem, checking that a solution works is usually trivial . So how would you go about finding such a sequence ? In essence what I'm asking is not ""What is the solution to such a problem"" but ""What is the process by which a solution is produced"". There is perhaps a funny parallel to vs where solutions to problems are easy to check but hard to find. For those that would prefer a concrete example, here is one. Study the existence of the following limit : ( You should find that such a limit does not exist, however what I am interested in is how you will go about finding two sequences such that they converge to but make the function itself converge to two different values )","f x \forall(x_n)_n \vert \lim_{n\rightarrow+\infty}{x_n}=x : \lim_{n\rightarrow+\infty}{f(x_n)=f(x)} (x_n)_n P nP nP \lim_{(x,y)\rightarrow(0,0)\,,\,(x, y)\ne(0, 0)}\frac{x^2y}{x+y} (0,0)","['limits', 'multivariable-calculus']"
13,Inverse function theorem: Lemma 5 from Terence Tao's blog,Inverse function theorem: Lemma 5 from Terence Tao's blog,,"I'm trying to understand the proof of Lemma 5 from Terence Tao's blog , i.e., Let $X$ be an open subset of $\mathbb R^n$ and $f:X \to \mathbb R^n$ be differentiable such that $\partial f (x)$ is invertible for all $x\in X$ . Fix $x_0 \in X$ and $y_0 := f(x_0)$ and let $$ K := \{x \in X: f(x)=y_0\}. $$ Lemma 5: Let $H$ be the connected component of $K$ that contains $x_0$ . Then $H = \{x_0\}$ . Could you have a check on my attempt? Proof: Assume the contrary that $H \neq \{x_0\}$ . Then there is a path $\gamma:[0,1] \to H$ such that $\gamma(0) = x_0$ and $\gamma(1) \in H \setminus \{x_0\}$ . We have $f \circ \gamma = y_0$ . Then $$ \begin{align} \lim_{t \to 0^+} \frac{f \circ \gamma (t) - f(x_0) - \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} &= -\lim_{t \to 0^+} \frac{ \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} \\ &= - \partial f (x_0) \left (\lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} \right ). \end{align} $$ Because $\partial f (x_0)$ is invertible, it is bijective. It follows that $$ \lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} = 0, $$ which is a contradiction.","I'm trying to understand the proof of Lemma 5 from Terence Tao's blog , i.e., Let be an open subset of and be differentiable such that is invertible for all . Fix and and let Lemma 5: Let be the connected component of that contains . Then . Could you have a check on my attempt? Proof: Assume the contrary that . Then there is a path such that and . We have . Then Because is invertible, it is bijective. It follows that which is a contradiction.","X \mathbb R^n f:X \to \mathbb R^n \partial f (x) x\in X x_0 \in X y_0 := f(x_0) 
K := \{x \in X: f(x)=y_0\}.
 H K x_0 H = \{x_0\} H \neq \{x_0\} \gamma:[0,1] \to H \gamma(0) = x_0 \gamma(1) \in H \setminus \{x_0\} f \circ \gamma = y_0 
\begin{align}
\lim_{t \to 0^+} \frac{f \circ \gamma (t) - f(x_0) - \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} &= -\lim_{t \to 0^+} \frac{ \partial f (x_0)(\gamma(t)-x_0)}{|\gamma(t)-x_0|} \\
&= - \partial f (x_0) \left (\lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} \right ).
\end{align}
 \partial f (x_0) 
\lim_{t \to 0^+}\frac{\gamma(t)-x_0}{|\gamma(t)-x_0|} = 0,
","['multivariable-calculus', 'derivatives', 'inverse-function']"
14,Find the domain to make the integral max,Find the domain to make the integral max,,"Find the domain $D \subset \mathbb{R}^2$ such that the value of the integral $$ \iint\limits_D (1-x^2-y^2) \,\mathrm{d}A $$ is maximum. I only have basic definitions and properties of Darboux integral on a general domain; no Fubini yet and no change of variables.  I'm thinking to use the following facts: If $A \subset B$ , $f$ is integrable on $A$ and $B$ with $f \ge 0$ , then $\int\limits_A f \le \int\limits_B f$ . If $f \le g$ , then $\int\limits_D f \le \int\limits_D g$ . Applying to this problem, I'm going to find the maximum area of $D$ such that $1-x^2-y^2 \ge 0$ .  That means $D$ must be a disk center $O$ and radius $1$ . Am I right?  I feel like my work is not concrete at all, not mathematically reasoning. What should I do?  Thank you very much.","Find the domain such that the value of the integral is maximum. I only have basic definitions and properties of Darboux integral on a general domain; no Fubini yet and no change of variables.  I'm thinking to use the following facts: If , is integrable on and with , then . If , then . Applying to this problem, I'm going to find the maximum area of such that .  That means must be a disk center and radius . Am I right?  I feel like my work is not concrete at all, not mathematically reasoning. What should I do?  Thank you very much.","D \subset \mathbb{R}^2 
\iint\limits_D (1-x^2-y^2) \,\mathrm{d}A
 A \subset B f A B f \ge 0 \int\limits_A f \le \int\limits_B f f \le g \int\limits_D f \le \int\limits_D g D 1-x^2-y^2 \ge 0 D O 1","['multivariable-calculus', 'area', 'multiple-integral']"
15,Parametrize ellipse in the xy-plane,Parametrize ellipse in the xy-plane,,"I want to parametrize the surface $(\frac{x}{a})^2 + (\frac{y}{b})^2 = 1$ in the xy-plane in $\mathbb R^3$ My attempt is $G(r,\theta) = (r \cos\theta,\frac{b}{a} \sin \theta,0)$ where $ \theta \in [0,2\pi] , r \in [0,a]$ Is my approach correct?",I want to parametrize the surface in the xy-plane in My attempt is where Is my approach correct?,"(\frac{x}{a})^2 + (\frac{y}{b})^2 = 1 \mathbb R^3 G(r,\theta) = (r \cos\theta,\frac{b}{a} \sin \theta,0)  \theta \in [0,2\pi] , r \in [0,a]","['calculus', 'multivariable-calculus', 'surface-integrals', 'parametrization']"
16,Proving mean value theorem in case of vector valued functions,Proving mean value theorem in case of vector valued functions,,"Let $f:U\to \mathbb R^n$ be differentiable on an open subset $U\subset \mathbb R^m$ . Suppose that $a,b\in U$ are such that the set $L:=\{a+(b-a)t: t\in [0,1]\}\subset U.$ Then, the following holds: $\|f(b)-f(a)\|\le \sup_{z\in L}\|Df_z\|_{\text{op}}\,\|b-a\|$ , where op stands for the operator norm. Proof: If $f(b)=f(a)$ , then there is nothing to prove so assume that $f(b)\ne f(a)$ . Define $u:=f(b)-f(a)$ and $g(t):=u\cdot f(a+t(b-a)), t\in [0,1]$ and $\cdot$ in definition of $g$ is the usual dot product. $g$ is differentiable on $(0,1)$ and continuous on $[0,1]$ .  So by mean value theorem, there exists a $\rho\in (0,1)$ such that $$g(1)-g(0)=g'(\rho)=u\cdot \color{blue}{f'(a+\rho(b-a); b-a)}=u\cdot (f(b)-f(a))\tag 1$$ The blue colored term in $(1)$ is the directional derivative of $f$ at $a+\rho(b-a)$ along $b-a$ . Define $\alpha(t):=a+t(b-a)$ . It follows by $(1)$ that $$u\cdot (f(b)-f(a))=u\cdot Df_{\alpha(\rho)}(b-a)\tag 2$$ It follows by $(2)$ that $\|u\|^2=\|u\cdot Df_{\alpha(\rho)}(b-a)\|\le\|u\|\,\|Df_{\alpha(\rho)}\|_{\text{op}}\, \|b-a\|\tag 3$ The desired inequality follows by $(3)$ . Explanation for: Para before $(1)$ : I used the following lemma: If $h:\mathbb R\to \mathbb R^n$ is a function such that $\lim_{x\to a} h(x)$ exists for some $a$ , then for any $u\in \mathbb R^n$ , $\lim_{x\to a} u\cdot h(x)=u\cdot (\lim_{x\to a}h(x)).$ $(3)$ : For any linear map $T:\mathbb R^m\to \mathbb R^n$ , $\|Tx\|\le \|T\|_{\text{op}}\,\|x\|$ for all $x\in \mathbb R^m$ . Is my proof correct? Thanks.","Let be differentiable on an open subset . Suppose that are such that the set Then, the following holds: , where op stands for the operator norm. Proof: If , then there is nothing to prove so assume that . Define and and in definition of is the usual dot product. is differentiable on and continuous on .  So by mean value theorem, there exists a such that The blue colored term in is the directional derivative of at along . Define . It follows by that It follows by that The desired inequality follows by . Explanation for: Para before : I used the following lemma: If is a function such that exists for some , then for any , : For any linear map , for all . Is my proof correct? Thanks.","f:U\to \mathbb R^n U\subset \mathbb R^m a,b\in U L:=\{a+(b-a)t: t\in [0,1]\}\subset U. \|f(b)-f(a)\|\le \sup_{z\in L}\|Df_z\|_{\text{op}}\,\|b-a\| f(b)=f(a) f(b)\ne f(a) u:=f(b)-f(a) g(t):=u\cdot f(a+t(b-a)), t\in [0,1] \cdot g g (0,1) [0,1] \rho\in (0,1) g(1)-g(0)=g'(\rho)=u\cdot \color{blue}{f'(a+\rho(b-a); b-a)}=u\cdot (f(b)-f(a))\tag 1 (1) f a+\rho(b-a) b-a \alpha(t):=a+t(b-a) (1) u\cdot (f(b)-f(a))=u\cdot Df_{\alpha(\rho)}(b-a)\tag 2 (2) \|u\|^2=\|u\cdot Df_{\alpha(\rho)}(b-a)\|\le\|u\|\,\|Df_{\alpha(\rho)}\|_{\text{op}}\, \|b-a\|\tag 3 (3) (1) h:\mathbb R\to \mathbb R^n \lim_{x\to a} h(x) a u\in \mathbb R^n \lim_{x\to a} u\cdot h(x)=u\cdot (\lim_{x\to a}h(x)). (3) T:\mathbb R^m\to \mathbb R^n \|Tx\|\le \|T\|_{\text{op}}\,\|x\| x\in \mathbb R^m","['real-analysis', 'calculus', 'multivariable-calculus', 'solution-verification']"
17,Is there a name for $(\det JJ^\top)^{1/2}$ where $J$ is a non-square Jacobian matrix?,Is there a name for  where  is a non-square Jacobian matrix?,(\det JJ^\top)^{1/2} J,"Background: Let $f: \mathbb R^n \to \mathbb R^m$ be a differentiable vector-valued function, with components $f_i: \mathbb R^n \to \mathbb R, \textrm{ for }1\le i\le m$ .  Then the Jacobian matrix is the $m \times n$ matrix $$\textbf{J} = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}$$ As is well-known, in the case in which $n = m$ the Jacobian matrix is square, and its determinant, the Jacobian determinant , is the scalar $J = \det \textbf{J}$ . The Jacobian determinant has many uses, including when using a substitution during integration. But what if $m \ne n$ ? In this case, $\textbf{J}$ is not square, so $\det \textbf{J}$ is not defined.  However, in this case, the quantity $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ is still meaningful, and generalizes the idea (and some of the uses) of the Jacobian determinant: For example, consider a function $\vec r: \mathbb R^2 \to \mathbb R^3$ , which we can regard as a parametrization of a surface in $\mathbb R^3$ . If we write $\vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle$ , then the Jacobian matrix is $\textbf{J} = \begin{bmatrix} x_u & y_u & z_u \\ x_v & y_v & z_v \end{bmatrix}$ ; in this situation it can be shown that $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \| \vec r_u \times \vec r_v \|$ .  We recognize the quantity that relates area in the surface to area in the domain: $dS = \| \vec r_u \times \vec r_v \| \, dA$ . On the other hand consider a function $\vec r: \mathbb R^1 \to \mathbb R^n$ , which we can interpret as a parametrization of a path in $\mathbb R^n$ .  If we write $\vec r(t) = \langle x_1(t), x_2(t), \dots, x_n(t) \rangle$ then in this case $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \left| \frac{d\vec r}{dt} \right|$ . Once again we recognize the quantity that relates arc length in the path to length in the domain: $ds = \left| \frac{d\vec r}{dt} \right| \, dt$ . Finally we note that if $n = m$ then $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ is identical to the ""usual"" Jacobian determinant $J$ . My question: Does the quantity $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ have a standard name in the case of a non-square Jacobian matrix $\textbf{J}$ ?  The phrase ""Jacobian determinant"" seems, as far as I can tell, to only be used in the context of square matrices, despite the fact that the quantity generalizes quite naturally to the non-square case, and encompasses several ""special formulas"" in a single neat form.  But I am unaware of any established terminology for this quantity.  Is there one?","Background: Let be a differentiable vector-valued function, with components .  Then the Jacobian matrix is the matrix As is well-known, in the case in which the Jacobian matrix is square, and its determinant, the Jacobian determinant , is the scalar . The Jacobian determinant has many uses, including when using a substitution during integration. But what if ? In this case, is not square, so is not defined.  However, in this case, the quantity is still meaningful, and generalizes the idea (and some of the uses) of the Jacobian determinant: For example, consider a function , which we can regard as a parametrization of a surface in . If we write , then the Jacobian matrix is ; in this situation it can be shown that .  We recognize the quantity that relates area in the surface to area in the domain: . On the other hand consider a function , which we can interpret as a parametrization of a path in .  If we write then in this case . Once again we recognize the quantity that relates arc length in the path to length in the domain: . Finally we note that if then is identical to the ""usual"" Jacobian determinant . My question: Does the quantity have a standard name in the case of a non-square Jacobian matrix ?  The phrase ""Jacobian determinant"" seems, as far as I can tell, to only be used in the context of square matrices, despite the fact that the quantity generalizes quite naturally to the non-square case, and encompasses several ""special formulas"" in a single neat form.  But I am unaware of any established terminology for this quantity.  Is there one?","f: \mathbb R^n \to \mathbb R^m f_i: \mathbb R^n \to \mathbb R, \textrm{ for }1\le i\le m m \times n \textbf{J} = \begin{bmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n} \end{bmatrix} n = m J = \det \textbf{J} m \ne n \textbf{J} \det \textbf{J} \left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} \vec r: \mathbb R^2 \to \mathbb R^3 \mathbb R^3 \vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle \textbf{J} = \begin{bmatrix} x_u & y_u & z_u \\ x_v & y_v & z_v \end{bmatrix} \left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \| \vec r_u \times \vec r_v \| dS = \| \vec r_u \times \vec r_v \| \, dA \vec r: \mathbb R^1 \to \mathbb R^n \mathbb R^n \vec r(t) = \langle x_1(t), x_2(t), \dots, x_n(t) \rangle \left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \left| \frac{d\vec r}{dt} \right| ds = \left| \frac{d\vec r}{dt} \right| \, dt n = m \left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} J \left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} \textbf{J}","['multivariable-calculus', 'terminology', 'jacobian']"
18,Sketching the region of integration and writing an equivalent double integral and evaluating it.,Sketching the region of integration and writing an equivalent double integral and evaluating it.,,"The question says to sketch the region of integration and reverse the order of integration and then evaluate if possible. $$\int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}(y)}\sqrt{xy}dxdy$$ This is what I graphed as the region of integration: So when I reverse the order of integration I get: $$\int_{0}^{\tan^{-1}(\sqrt{3})}\int_{\tan x}^{\sqrt{3}} \sqrt{xy} dy dx$$ $$=\int_{0}^{\tan^{-1}(\sqrt{3})} \sqrt{x}\frac{2}{3}y^{3/2} \Bigg|_{\tan x}^{\sqrt{3}} dx$$ $$=\int_{0}^{\tan^{-1}(\sqrt{3})}  \frac{2}{3}\sqrt{x} \Bigg[3^{3/4}-(\tan x)^{3/2}\Bigg]dx$$ $$=\frac{2}{3}\int_{0}^{\tan^{-1}(\sqrt{3})} 3^{3/4}\sqrt{x}-\sqrt{x}(\tan x)^{3/2}dx$$ I don't know what to do next to evaluate the integral.  Can someone please help me? The question says to find the value of the integral if possible, so does that mean I cannot find the value of this integral? If so, why not?","The question says to sketch the region of integration and reverse the order of integration and then evaluate if possible. This is what I graphed as the region of integration: So when I reverse the order of integration I get: I don't know what to do next to evaluate the integral.  Can someone please help me? The question says to find the value of the integral if possible, so does that mean I cannot find the value of this integral? If so, why not?",\int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}(y)}\sqrt{xy}dxdy \int_{0}^{\tan^{-1}(\sqrt{3})}\int_{\tan x}^{\sqrt{3}} \sqrt{xy} dy dx =\int_{0}^{\tan^{-1}(\sqrt{3})} \sqrt{x}\frac{2}{3}y^{3/2} \Bigg|_{\tan x}^{\sqrt{3}} dx =\int_{0}^{\tan^{-1}(\sqrt{3})}  \frac{2}{3}\sqrt{x} \Bigg[3^{3/4}-(\tan x)^{3/2}\Bigg]dx =\frac{2}{3}\int_{0}^{\tan^{-1}(\sqrt{3})} 3^{3/4}\sqrt{x}-\sqrt{x}(\tan x)^{3/2}dx,['multivariable-calculus']
19,Converting a double integral from cartesian to polar coordinates,Converting a double integral from cartesian to polar coordinates,,"Question: The integral $$\int\limits_0^3 \int\limits_{|x|}^{\sqrt{18-x^2}} \sqrt{x^2+y^2} dy\ dx$$ has the following form in polar coordinates My work so far: $y=\sqrt{18-x^2} \implies x^2+y^2=18 \implies r^2=18, r=3\sqrt2$ $y=|x|$ This is where I'm stuck $r\sin\theta =|r\cos\theta|$ does this happen at $\theta=\pi/4$ ? All I did was graph $y=|x|$ and see that makes a 45 degree angle. What value of $r$ corresponds to that? $x=0=r\cos\theta \implies 3\sqrt{2}\cos\theta=0 \implies \theta=\frac{\pi}{2}$ $x=3=r\cos\theta \implies 3\sqrt{2}\cos\theta=3 \implies \theta=\frac{\pi}{4}$ $\sqrt{x^2+y^2}=r$ So in polar form does the integral turn into $$\int_{\pi/4}^{\pi/2} \int_{?}^{3\sqrt{2}}r^2drd\theta$$ Again I'm not sure what the lower bound is for $r$ or how I would go about finding that. Also is the rest of my work correct? EDIT: picture of graph",Question: The integral has the following form in polar coordinates My work so far: This is where I'm stuck does this happen at ? All I did was graph and see that makes a 45 degree angle. What value of corresponds to that? So in polar form does the integral turn into Again I'm not sure what the lower bound is for or how I would go about finding that. Also is the rest of my work correct? EDIT: picture of graph,"\int\limits_0^3 \int\limits_{|x|}^{\sqrt{18-x^2}} \sqrt{x^2+y^2} dy\ dx y=\sqrt{18-x^2} \implies x^2+y^2=18 \implies r^2=18, r=3\sqrt2 y=|x| r\sin\theta =|r\cos\theta| \theta=\pi/4 y=|x| r x=0=r\cos\theta \implies 3\sqrt{2}\cos\theta=0 \implies \theta=\frac{\pi}{2} x=3=r\cos\theta \implies 3\sqrt{2}\cos\theta=3 \implies \theta=\frac{\pi}{4} \sqrt{x^2+y^2}=r \int_{\pi/4}^{\pi/2} \int_{?}^{3\sqrt{2}}r^2drd\theta r","['multivariable-calculus', 'polar-coordinates']"
20,Does $e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n$ hold for matrices?,Does  hold for matrices?,e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n,"Let $X$ be a $d \times d$ real matrix, $d>1$ . Is it true that $$ e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n\,\,\,? $$ Edit: It seems that this question is a duplicate . To make it more interesting then, I am asking the following: Can the density argument via diagonalizable matrices be made rigorous? Write $g(X)=e^X, f(X)=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n $ . Then $f$ and $g$ agree on diagonalizable matrices (via the classical real-valued case). To deduce they coincide on all matrices, though, requires proving both functions are continuous . How do you prove that $f$ is continuous? (I guess one might try to prove uniform convergence?)","Let be a real matrix, . Is it true that Edit: It seems that this question is a duplicate . To make it more interesting then, I am asking the following: Can the density argument via diagonalizable matrices be made rigorous? Write . Then and agree on diagonalizable matrices (via the classical real-valued case). To deduce they coincide on all matrices, though, requires proving both functions are continuous . How do you prove that is continuous? (I guess one might try to prove uniform convergence?)","X d \times d d>1 
e^X=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n\,\,\,?
 g(X)=e^X, f(X)=\lim_{n \to \infty}\left(\text{Id}+\frac{X}{n}\right)^n
 f g f","['multivariable-calculus', 'exponential-function', 'matrix-calculus', 'matrix-exponential']"
21,Show example that if $\phi$ is not injective then the change of variables might yield different result,Show example that if  is not injective then the change of variables might yield different result,\phi,"I need to find non-injective parameterization to some domain such that : $$\int_D f(x,y) dxdy \ne \int_E (f\circ \phi)(u,v)J(u,v) dudv$$ The example I found turns out to be wrong. (The integral is the same when integrating using change of variables and direct computation). My try : The domain is $D=\{ 1\le xy \le 2 , 1 \le \frac {x}{y} \le 2\}$ and I want to calculate $\int_D 1dxdy$ The try is wrong, Thanks to @user1027216 comment. I searched for a counter-example for hours in the internet and couldn't find one. Any help would be appreciated. Thank you in advance.","I need to find non-injective parameterization to some domain such that : The example I found turns out to be wrong. (The integral is the same when integrating using change of variables and direct computation). My try : The domain is and I want to calculate The try is wrong, Thanks to @user1027216 comment. I searched for a counter-example for hours in the internet and couldn't find one. Any help would be appreciated. Thank you in advance.","\int_D f(x,y) dxdy \ne \int_E (f\circ \phi)(u,v)J(u,v) dudv D=\{ 1\le xy \le 2 , 1 \le \frac {x}{y} \le 2\} \int_D 1dxdy","['multivariable-calculus', 'multiple-integral', 'change-of-variable']"
22,Volume of a solid enclosed by the hyperboloid and cone,Volume of a solid enclosed by the hyperboloid and cone,,"Compute the volume of the solid inside the cone $2(x^2+y^2)=z^2$ and hyperboloid $x^2+y^2=z^2-a^2.$ Source Demidovich, task 2024 My attempt: Because of the symmetry with respect to the $xy$ plane, we can compute the volume above the plane and multiply it by $2$ . Intersection of the cone and hyperboloid : $$\begin{cases}x^2+y^2=\frac{z^2}2\\x^2+y^2=z^2-a^2\end{cases}\implies x^2+y^2=\frac{z^2}2=a^2$$ is the circle of the radius $a$ . I used the cylindrical coordinates $$\begin{aligned}x=r\cos\varphi\\y=r\sin\varphi\\z=z, r\in[0,a]\\\varphi\in[0,2\pi]\\z\in\left(\sqrt 2r,\sqrt{r^2+a^2}\right)\end{aligned}$$ so the Jacobian equals $r$ and volume is $\begin{aligned}2\int_0^{2\pi}\int_0^a\int_{\sqrt 2r}^{\sqrt{r^2+a^2}}r\mathrm dz\mathrm dr\mathrm d\varphi&=4\pi\int_0^a\left(\sqrt{r^2+a^2}-\sqrt 2r\right)\mathrm dr\\&=4\pi\int_0^a\left(r\sqrt{r^2+a^2}-\sqrt 2r^2\right)\mathrm dr\\&=4\pi\left(\frac{(r^2+a^2)^{3/2}}3-\sqrt 2\frac{r^3}3\right)\Big|_0^a\\&=4\pi\left(\frac{2\sqrt 2 a^3}3-\sqrt 2\frac{a^3}3\right)\\&=\frac43a^3\pi,\end{aligned}$ but the solution is $\frac43a^3\pi(\sqrt 2-1),$ and I can't find my mistake. Can anybody see it?","Compute the volume of the solid inside the cone and hyperboloid Source Demidovich, task 2024 My attempt: Because of the symmetry with respect to the plane, we can compute the volume above the plane and multiply it by . Intersection of the cone and hyperboloid : is the circle of the radius . I used the cylindrical coordinates so the Jacobian equals and volume is but the solution is and I can't find my mistake. Can anybody see it?","2(x^2+y^2)=z^2 x^2+y^2=z^2-a^2. xy 2 \begin{cases}x^2+y^2=\frac{z^2}2\\x^2+y^2=z^2-a^2\end{cases}\implies x^2+y^2=\frac{z^2}2=a^2 a \begin{aligned}x=r\cos\varphi\\y=r\sin\varphi\\z=z, r\in[0,a]\\\varphi\in[0,2\pi]\\z\in\left(\sqrt 2r,\sqrt{r^2+a^2}\right)\end{aligned} r \begin{aligned}2\int_0^{2\pi}\int_0^a\int_{\sqrt 2r}^{\sqrt{r^2+a^2}}r\mathrm dz\mathrm dr\mathrm d\varphi&=4\pi\int_0^a\left(\sqrt{r^2+a^2}-\sqrt 2r\right)\mathrm dr\\&=4\pi\int_0^a\left(r\sqrt{r^2+a^2}-\sqrt 2r^2\right)\mathrm dr\\&=4\pi\left(\frac{(r^2+a^2)^{3/2}}3-\sqrt 2\frac{r^3}3\right)\Big|_0^a\\&=4\pi\left(\frac{2\sqrt 2 a^3}3-\sqrt 2\frac{a^3}3\right)\\&=\frac43a^3\pi,\end{aligned} \frac43a^3\pi(\sqrt 2-1),","['integration', 'multivariable-calculus']"
23,Identity for $\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d}$,Identity for,\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d},"I am looking for an identity for the following derivative: $$\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d},$$ where $\mathbf{a}$ , $\mathbf{b}$ , $\mathbf{c}$ , and $\mathbf{d}$ are column vectors. My approach to expand the expression is as follows: \begin{align*} \nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d} &= [ ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} + \mathbf{c} \otimes\nabla(\mathbf{a} \cdot \mathbf{b}) ] \cdot \mathbf{d} \\ &= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \} \cdot \mathbf{d}\\ &= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \}^\top \mathbf{d}\\ &= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \otimes \mathbf{c} \} \mathbf{d}\\ &= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + (\mathbf{c} \cdot \mathbf{d}) [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}]. \end{align*} Here, $D \equiv \nabla^\top$ , i.e., $\nabla$ stands for the gradient operator and $D$ denotes the Jacobian. My question is, is this expansion correct? Moreover, in my understanding, the resulting expression is again a column vector. Is it right?","I am looking for an identity for the following derivative: where , , , and are column vectors. My approach to expand the expression is as follows: Here, , i.e., stands for the gradient operator and denotes the Jacobian. My question is, is this expansion correct? Moreover, in my understanding, the resulting expression is again a column vector. Is it right?","\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d}, \mathbf{a} \mathbf{b} \mathbf{c} \mathbf{d} \begin{align*}
\nabla[( \mathbf{a} \cdot \mathbf{b} )\mathbf{c}] \cdot \mathbf{d}
&= [ ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} + \mathbf{c} \otimes\nabla(\mathbf{a} \cdot \mathbf{b}) ] \cdot \mathbf{d} \\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \} \cdot \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ \mathbf{c} \otimes [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \}^\top \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + \{ [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}] \otimes \mathbf{c} \} \mathbf{d}\\
&= ( \mathbf{a} \cdot \mathbf{b} ) D \mathbf{c} \cdot \mathbf{d} + (\mathbf{c} \cdot \mathbf{d}) [ (D\mathbf{a})^\top \mathbf{b} + (D\mathbf{b})^\top \mathbf{a}].
\end{align*} D \equiv \nabla^\top \nabla D","['multivariable-calculus', 'derivatives', 'partial-derivative']"
24,Building a particular function with directional derivatives but not differentiable,Building a particular function with directional derivatives but not differentiable,,"I am interested in building a function $f:]0,1[^2 \rightarrow \mathbb{R}$ such that $f$ is continuous $f$ has directional derivatives everywhere and in every directions $\forall t\in ]0,1[$ , $$\frac{\partial f}{\partial x}(t,t)=0$$ $$\frac{\partial f}{\partial y}(t,t)=0$$ the function $g(t)=f(t,t)$ is non constant. It is clear that this function cannot be differentiable. Is it possible to build an $f$ like this? If yes, can we build an example? If no, is there a simple argument proving it? Thank you.","I am interested in building a function such that is continuous has directional derivatives everywhere and in every directions , the function is non constant. It is clear that this function cannot be differentiable. Is it possible to build an like this? If yes, can we build an example? If no, is there a simple argument proving it? Thank you.","f:]0,1[^2 \rightarrow \mathbb{R} f f \forall t\in ]0,1[ \frac{\partial f}{\partial x}(t,t)=0 \frac{\partial f}{\partial y}(t,t)=0 g(t)=f(t,t) f",['multivariable-calculus']
25,"Proving using definition that $f=(f_1,f_2,...,f_m)$ is differentiable at $c$ iff $f_i$'s are differentiable at $c$.",Proving using definition that  is differentiable at  iff 's are differentiable at .,"f=(f_1,f_2,...,f_m) c f_i c","I want to prove the following statement: Suppose that $S$ is a non-empty subset of $\mathbb R^n$ . Suppose that $f:S\to \mathbb R^m$ is differentiable at an interior point $c$ of $S$ . Suppose that $f=(f_1,...,f_m)$ , where $f_i:S\to \mathbb R$ for every $i\in \{1,2,...,m\}$ . Then $f$ is differentiable at $c\in S$ if and only if $f_i$ is differentiable at $c$ for every $i\in \{1,2,...,m\}$ . Definition of differentiability: $f$ is said to be differentiable at an interior point $c$ of $S$ if there exists $h>0$ and a linear transformation $T_c:\mathbb R^n\to \mathbb R^m$ such that for all $||v||<h$ the following holds: $f(c+v)-f(c)=T_c(v)+o(||v||), v\to 0$ . $T_c$ is also denoted by $f'(c)$ . I tried to prove it as follows: ( $\Leftarrow$ ) Suppose that $f_i$ 'are differentiable at $c$ . It follows that \begin{align*} f_i(c+v)-f_i(c)-f_i'(c)v=o(||v||) &\implies f_i(c+v)-f_i(c)-\nabla f_i(c).v=o(||v||)\\&\implies \sum_{i=1}^m\{f_i(c+v)-f_i(c)-\nabla f_i(c).v\}e_i=\sum e_i o(||v||)\\&\implies f(c+v)-f(c)-(\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{red}{\sum e_i o(||v||)=o(||v||}\\&\color{blue}{\implies }f'(c) \text{ exists.} \end{align*} The red part is due to the fact that: $\frac{\sum o(||v||)}{||v||}=(\sum e_i)o(1)\to 0$ as $v\to 0$ . The implication highlighted in blue is true because $(\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{green}{\begin {pmatrix} \nabla f_1 (c)\\\nabla f_2 (c)\\...\\ \nabla f_m (c)\end{pmatrix}}v$ . The matrix in green is an $m\times n$ matrix and hence satisfies differentiability definition. Now for the other direction: ( $\Rightarrow$ ) Suppose that $f$ is differentiable at $c$ so $\sum_{i=1}^m\{f_i(c+v)-f_i(c)-\color{purple}{\nabla f_i(c)}.v\}e_i=o(||v||)$ . For any $j\in \{1,2,...,m\}$ , the following holds: $|f_j(c+v)-f_j(c)-\nabla f_j(c).v|\leq \sqrt{\sum_{i=1}^m|f_i(c+v)-f_i(c)-\nabla f_i(c).v|^2}$ . It follows that: $\frac{|f_j(c+v)-f_j(c)-\nabla f_j(c).v|}{||v||}\leq o(1)$ . So taking $v\to 0$ , it follows that $f_j(c+v)-f_j(c)-\nabla f_j(c).v=o(||v||)$ . It follows by definition of differentiablity that $f_j$ is differentiable for every $j\in \{1,2,...,n\}$ . Is my proof correct? Thanks.","I want to prove the following statement: Suppose that is a non-empty subset of . Suppose that is differentiable at an interior point of . Suppose that , where for every . Then is differentiable at if and only if is differentiable at for every . Definition of differentiability: is said to be differentiable at an interior point of if there exists and a linear transformation such that for all the following holds: . is also denoted by . I tried to prove it as follows: ( ) Suppose that 'are differentiable at . It follows that The red part is due to the fact that: as . The implication highlighted in blue is true because . The matrix in green is an matrix and hence satisfies differentiability definition. Now for the other direction: ( ) Suppose that is differentiable at so . For any , the following holds: . It follows that: . So taking , it follows that . It follows by definition of differentiablity that is differentiable for every . Is my proof correct? Thanks.","S \mathbb R^n f:S\to \mathbb R^m c S f=(f_1,...,f_m) f_i:S\to \mathbb R i\in \{1,2,...,m\} f c\in S f_i c i\in \{1,2,...,m\} f c S h>0 T_c:\mathbb R^n\to \mathbb R^m ||v||<h f(c+v)-f(c)=T_c(v)+o(||v||), v\to 0 T_c f'(c) \Leftarrow f_i c \begin{align*}
f_i(c+v)-f_i(c)-f_i'(c)v=o(||v||)
&\implies f_i(c+v)-f_i(c)-\nabla f_i(c).v=o(||v||)\\&\implies \sum_{i=1}^m\{f_i(c+v)-f_i(c)-\nabla f_i(c).v\}e_i=\sum e_i o(||v||)\\&\implies f(c+v)-f(c)-(\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{red}{\sum e_i o(||v||)=o(||v||}\\&\color{blue}{\implies }f'(c) \text{ exists.}
\end{align*} \frac{\sum o(||v||)}{||v||}=(\sum e_i)o(1)\to 0 v\to 0 (\nabla f_1 (c).v, \nabla f_2(c).v,\cdots, \nabla f_m(c).v)=\color{green}{\begin {pmatrix} \nabla f_1 (c)\\\nabla f_2 (c)\\...\\ \nabla f_m (c)\end{pmatrix}}v m\times n \Rightarrow f c \sum_{i=1}^m\{f_i(c+v)-f_i(c)-\color{purple}{\nabla f_i(c)}.v\}e_i=o(||v||) j\in \{1,2,...,m\} |f_j(c+v)-f_j(c)-\nabla f_j(c).v|\leq \sqrt{\sum_{i=1}^m|f_i(c+v)-f_i(c)-\nabla f_i(c).v|^2} \frac{|f_j(c+v)-f_j(c)-\nabla f_j(c).v|}{||v||}\leq o(1) v\to 0 f_j(c+v)-f_j(c)-\nabla f_j(c).v=o(||v||) f_j j\in \{1,2,...,n\}","['real-analysis', 'multivariable-calculus', 'solution-verification']"
26,"Given $T_2((0,0),(h_1,h_2))=1+2h_1^2-3h_2^2$ and $g_h(t)=f(th),$ what are possible values of $g_h'(0)$ and $g_h''(0)$?",Given  and  what are possible values of  and ?,"T_2((0,0),(h_1,h_2))=1+2h_1^2-3h_2^2 g_h(t)=f(th), g_h'(0) g_h''(0)","Let $f:\Bbb R^2\to\Bbb R$ be a function of the class $C^2$ whose Taylor polynomial of degree $2$ at $(0,0)$ is given by $$T_2((0,0),(h_1,h_2))=1+2h_1^2-3h_2^2.$$ For a unit vector $h\in\Bbb R^2$ define a function $g_h:\Bbb R\to\Bbb R$ by $g_h(t):=f(th).$ What are possible values of $g_h'(0)$ and $g_h''(0)$ ? My attempt: Since $h_1,h_2\in\Bbb R$ are independent, I think we can write down $T_2((0,0),(h_1,h_2))$ by definition and compare the coefficients of the polynomial on the $LHS$ with those on the $RHS$ to find the $\nabla f(0,0)$ and $H_f(0,0):$ $$\begin{aligned} &f(0,0)+Df(0,0)(h_1,h_2)+ \frac1{2!}D^2f(0,0)((h_1,h_2),(h_1,h_2))\\ =&\ f(0,0)+\frac{\partial f}{\partial x_1}(0,0)h_1+\frac{\partial f}{\partial x_2}(0,0)h_2\\ &+\frac1{2!}\left(\frac{\partial^2 f}{\partial x_1^2}(0,0)h_1^2+\frac{\partial^2f}{\partial x_1\partial x_2}(0,0)h_1h_2+\frac{\partial^2f}{\partial x_2\partial x_1}(0,0)h_1h_2+\frac{\partial^2 f}{\partial x_2^2}(0,0)h_2^2\right)\\ =&\ f(0,0)+\frac{\partial f}{\partial x_1}(0,0)h_1+\frac{\partial f}{\partial x_2}(0,0)h_2\\ &+\frac1{2!}\frac{\partial^2 f}{\partial x_1^2}(0,0)h_1^2+\frac{\partial^2f}{\partial x_1\partial x_2}(0,0)h_1h_2+\frac1{2!}\frac{\partial^2 f}{\partial x_2^2}(0,0)h_2^2 \end{aligned}$$ $$\frac{\partial f}{\partial x_i}(0,0)=0, \quad \frac{\partial^2f}{\partial x_1\partial x_2}(0,0)=0, \quad  \frac{\partial^2f}{\partial x_1^2}(0,0)=4, \quad \frac{\partial^2 f}{\partial x_2^2}(0,0)=-6$$ $$\operatorname{grad}f(0,0)=(0,0), H_f(0,0)=\begin{bmatrix}4&0\\0&-6\end{bmatrix}$$ So, $(0,0)$ is a stationary point, but $H_f(0,0)$ is indefinite, so $(0,0)$ isn't a point of the extreme, otherwise, $H_f(0,0)$ would be positive or negative semidefinite. Next, $$g_h'(0)=\lim_{t\to 0}\frac{f(th)-f(0,0)}t=\frac{\partial f}{\partial h}(0)=Df(0)h=0.$$ Now, let $\gamma:\Bbb R\to\Bbb R^2,\gamma(t)=th.$ Then $g_h''(0)=(f\circ\gamma)''(0).$ I tried computed it in a more general setting when $f:\Bbb R^n\to\Bbb R$ and $\gamma:\Bbb R\to\Bbb R^n$ : $$\begin{aligned}(f\circ\gamma)'(c)=Df(\gamma (c))\gamma'(c)&=\begin{bmatrix}\frac{\partial f}{\partial x_1}(\gamma(c))&\ldots&\frac{\partial f}{\partial x_n}(\gamma(c))\end{bmatrix}\cdot\begin{bmatrix}\gamma_1'(c)\\\vdots\\ \gamma'_n(c)\end{bmatrix}\\&=\sum_{j=1}^n\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j'(c) \end{aligned}$$ $$\begin{aligned}D\left(\frac{\partial f}{\partial x_j}(\gamma(c))\right)&=D\frac{\partial f}{\partial x_j}(\gamma(c))\gamma'(c)\\&=\begin{bmatrix}\frac{\partial^2 f}{\partial x_1\partial x_j}(\gamma (c))&\ldots&\frac{\partial^2f}{\partial x_n\partial x_j}(\gamma(c))\end{bmatrix}\cdot\begin{bmatrix}\gamma_1'(c)\\\vdots\\\gamma_n'(c)\end{bmatrix}\\&=\sum_{i=1}^n\frac{\partial^2f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\end{aligned}$$ $$\begin{aligned}(f\circ\gamma)''(c)&=\sum_{j=1}^n\left(\sum_{i=1}^n\frac{\partial^2f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\gamma_j'(c)+\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j''(c)\right)\\&=\sum_{i,j=1}^n\frac{\partial^2 f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\gamma_j'(c)+\sum_{j=1}^n\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j''(c)\\&= Hf(\gamma(c))(\gamma'(c),\gamma'(c))+\nabla f(\gamma(c))\gamma''(c)\end{aligned}$$ So, $$\begin{aligned}g_h''(0)&=H_f(0,0)((h_1,h_2),(h_1,h_2))+\nabla f(0,0)\cdot(0,0)^T\\&=\begin{bmatrix}4&0\\0&-6\end{bmatrix}\cdot\begin{bmatrix}h_1\\h_2\end{bmatrix}\cdot\begin{bmatrix}h_1\\h_2\end{bmatrix}\\&=4h_1^2-6h_2^2\\&=4(h_1^2+h_2^2)-10h_2^2\\&=4-10h_2^2\end{aligned}\\\implies g_h''(0)\in[-6,4]$$ because $h_2\in[-1,1]\implies h_2^2\in[0,1]$ Is my answer correct? Is there any other way of solving this?","Let be a function of the class whose Taylor polynomial of degree at is given by For a unit vector define a function by What are possible values of and ? My attempt: Since are independent, I think we can write down by definition and compare the coefficients of the polynomial on the with those on the to find the and So, is a stationary point, but is indefinite, so isn't a point of the extreme, otherwise, would be positive or negative semidefinite. Next, Now, let Then I tried computed it in a more general setting when and : So, because Is my answer correct? Is there any other way of solving this?","f:\Bbb R^2\to\Bbb R C^2 2 (0,0) T_2((0,0),(h_1,h_2))=1+2h_1^2-3h_2^2. h\in\Bbb R^2 g_h:\Bbb R\to\Bbb R g_h(t):=f(th). g_h'(0) g_h''(0) h_1,h_2\in\Bbb R T_2((0,0),(h_1,h_2)) LHS RHS \nabla f(0,0) H_f(0,0): \begin{aligned}
&f(0,0)+Df(0,0)(h_1,h_2)+ \frac1{2!}D^2f(0,0)((h_1,h_2),(h_1,h_2))\\
=&\ f(0,0)+\frac{\partial f}{\partial x_1}(0,0)h_1+\frac{\partial f}{\partial x_2}(0,0)h_2\\
&+\frac1{2!}\left(\frac{\partial^2 f}{\partial x_1^2}(0,0)h_1^2+\frac{\partial^2f}{\partial x_1\partial x_2}(0,0)h_1h_2+\frac{\partial^2f}{\partial x_2\partial x_1}(0,0)h_1h_2+\frac{\partial^2 f}{\partial x_2^2}(0,0)h_2^2\right)\\
=&\ f(0,0)+\frac{\partial f}{\partial x_1}(0,0)h_1+\frac{\partial f}{\partial x_2}(0,0)h_2\\
&+\frac1{2!}\frac{\partial^2 f}{\partial x_1^2}(0,0)h_1^2+\frac{\partial^2f}{\partial x_1\partial x_2}(0,0)h_1h_2+\frac1{2!}\frac{\partial^2 f}{\partial x_2^2}(0,0)h_2^2
\end{aligned} \frac{\partial f}{\partial x_i}(0,0)=0,
\quad
\frac{\partial^2f}{\partial x_1\partial x_2}(0,0)=0,
\quad 
\frac{\partial^2f}{\partial x_1^2}(0,0)=4,
\quad
\frac{\partial^2 f}{\partial x_2^2}(0,0)=-6 \operatorname{grad}f(0,0)=(0,0), H_f(0,0)=\begin{bmatrix}4&0\\0&-6\end{bmatrix} (0,0) H_f(0,0) (0,0) H_f(0,0) g_h'(0)=\lim_{t\to 0}\frac{f(th)-f(0,0)}t=\frac{\partial f}{\partial h}(0)=Df(0)h=0. \gamma:\Bbb R\to\Bbb R^2,\gamma(t)=th. g_h''(0)=(f\circ\gamma)''(0). f:\Bbb R^n\to\Bbb R \gamma:\Bbb R\to\Bbb R^n \begin{aligned}(f\circ\gamma)'(c)=Df(\gamma (c))\gamma'(c)&=\begin{bmatrix}\frac{\partial f}{\partial x_1}(\gamma(c))&\ldots&\frac{\partial f}{\partial x_n}(\gamma(c))\end{bmatrix}\cdot\begin{bmatrix}\gamma_1'(c)\\\vdots\\ \gamma'_n(c)\end{bmatrix}\\&=\sum_{j=1}^n\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j'(c)
\end{aligned} \begin{aligned}D\left(\frac{\partial f}{\partial x_j}(\gamma(c))\right)&=D\frac{\partial f}{\partial x_j}(\gamma(c))\gamma'(c)\\&=\begin{bmatrix}\frac{\partial^2 f}{\partial x_1\partial x_j}(\gamma (c))&\ldots&\frac{\partial^2f}{\partial x_n\partial x_j}(\gamma(c))\end{bmatrix}\cdot\begin{bmatrix}\gamma_1'(c)\\\vdots\\\gamma_n'(c)\end{bmatrix}\\&=\sum_{i=1}^n\frac{\partial^2f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\end{aligned} \begin{aligned}(f\circ\gamma)''(c)&=\sum_{j=1}^n\left(\sum_{i=1}^n\frac{\partial^2f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\gamma_j'(c)+\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j''(c)\right)\\&=\sum_{i,j=1}^n\frac{\partial^2 f}{\partial x_i\partial x_j}(\gamma(c))\gamma_i'(c)\gamma_j'(c)+\sum_{j=1}^n\frac{\partial f}{\partial x_j}(\gamma(c))\gamma_j''(c)\\&= Hf(\gamma(c))(\gamma'(c),\gamma'(c))+\nabla f(\gamma(c))\gamma''(c)\end{aligned} \begin{aligned}g_h''(0)&=H_f(0,0)((h_1,h_2),(h_1,h_2))+\nabla f(0,0)\cdot(0,0)^T\\&=\begin{bmatrix}4&0\\0&-6\end{bmatrix}\cdot\begin{bmatrix}h_1\\h_2\end{bmatrix}\cdot\begin{bmatrix}h_1\\h_2\end{bmatrix}\\&=4h_1^2-6h_2^2\\&=4(h_1^2+h_2^2)-10h_2^2\\&=4-10h_2^2\end{aligned}\\\implies g_h''(0)\in[-6,4] h_2\in[-1,1]\implies h_2^2\in[0,1]","['real-analysis', 'multivariable-calculus', 'derivatives', 'solution-verification']"
27,Stability of steady states using the Jacobian (linear approximation),Stability of steady states using the Jacobian (linear approximation),,"I'm studying the stability of steady states by means of the eigenvalues of $J$ . So far the criteria is this: All eigenvalues $\gt 0 \implies$ unstable All eigenvalues $\lt 0 \implies$ stable . In 2D: one eigenvalue $\gt 0$ , and another $\lt 0 \implies$ saddle . All eigenvalues $\leq 0 \implies$ critical (stability cannot be analyzed using this approximation). But what about the following cases: Saddle points in $n$ -dim (how do they look like?), can an eigenvalue be zero? Eigenvalues $\geq 0$ , how do I classify these?","I'm studying the stability of steady states by means of the eigenvalues of . So far the criteria is this: All eigenvalues unstable All eigenvalues stable . In 2D: one eigenvalue , and another saddle . All eigenvalues critical (stability cannot be analyzed using this approximation). But what about the following cases: Saddle points in -dim (how do they look like?), can an eigenvalue be zero? Eigenvalues , how do I classify these?",J \gt 0 \implies \lt 0 \implies \gt 0 \lt 0 \implies \leq 0 \implies n \geq 0,"['multivariable-calculus', 'eigenvalues-eigenvectors', 'nonlinear-system', 'jacobian', 'steady-state']"
28,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be continuously differentiable. $\forall x,y\in \mathbb{R}^n$, $d\big(f(x),f(y)\big)\geq a\cdot d(x,y)$","Let  be continuously differentiable. ,","f:\mathbb{R}^n\rightarrow \mathbb{R}^n \forall x,y\in \mathbb{R}^n d\big(f(x),f(y)\big)\geq a\cdot d(x,y)","Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be continuously differentiable. If there exists a real number $a>0$ , such that $\forall x,y\in \mathbb{R}^n$ , $d\big(f(x),f(y)\big)\geq a\cdot d(x,y)$ , please show that $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is a $C^1$ -diffeomorphism. It's clear that $f$ is injective, and the differential $df(x)$ is invertible for any $x\in \mathbb{R}$ , since $f(x+v)=f(x)+df(x)(v)+o(v)$ , as $v\rightarrow 0$ . Then by the inverse function theorem, we have $f$ is a locally diffeomorphism. If $f$ is surjective, then we can show that $f$ is a $C^1$ -diffeomorphism. How can I show that $f$ is surjective?","Let be continuously differentiable. If there exists a real number , such that , , please show that is a -diffeomorphism. It's clear that is injective, and the differential is invertible for any , since , as . Then by the inverse function theorem, we have is a locally diffeomorphism. If is surjective, then we can show that is a -diffeomorphism. How can I show that is surjective?","f:\mathbb{R}^n\rightarrow \mathbb{R}^n a>0 \forall x,y\in \mathbb{R}^n d\big(f(x),f(y)\big)\geq a\cdot d(x,y) f:\mathbb{R}^n\rightarrow \mathbb{R}^n C^1 f df(x) x\in \mathbb{R} f(x+v)=f(x)+df(x)(v)+o(v) v\rightarrow 0 f f f C^1 f","['real-analysis', 'multivariable-calculus', 'diffeomorphism']"
29,Domain in proof of Euler's theorem for homogeneous functions,Domain in proof of Euler's theorem for homogeneous functions,,"$$ \newcommand{\R}{\mathbb{R}} \newcommand{\ra}{\rightarrow} \newcommand{\d}{\text{d}} $$ Most statements of Euler's theorem for homogeneous functions I found restrict the domain of the considered function, and/or the homogeneity degree, and/or the rescaling parameter. For example Wikipedia states the theorem for positively homogeneous functions $f: \R^n \setminus {0} \ra \R$ Apostol at pag. 287 considers only positive rescaling parameters. I don't see what goes wrong with this generality: Definition Let $k$ be an integer. A differentiable function $f: \R^n \ra \R$ is $k$ -homogeneous if $$ f(ax) = a^kf(x) $$ for all $x \in \R^n$ and all $a \in \R \setminus {0}$ . Euler's Theorem Let $k$ be an integer and let $f: \R^n \ra \R$ be a differentiable function. Then $f$ is $k$ -homogeneous if and only if $$ x \cdot \d{f}(x) = kf(x) $$ for all $x \in \R^n$ . Proof Let $f$ be $k$ -homogeneous. Then for any nonzero real $a$ , $f(ax) = a^k f(x)$ . Differentiate with respect to $a$ : $$ \d{f}(ax) \cdot x =  ka^{k-1}f(x) $$ In particular this must be true for $a$ = 1, so $$ \d{f}(x) \cdot x = kf(x) $$ Conversely, let the condition above hold; then for any real $a$ , $ax \cdot \d{f}(ax) = k f (ax)$ . Define $g(a) = f(ax)$ for any fixed $x \in \R^n$ . I guess here it is actually necessary to consider $g(a) = f(ax)$ for any fixed $x \in \R^n \setminus {0}$ . Differentiate $g$ with respect to $a$ , and assume $a \neq 0$ : $$ \frac{\d{}}{\d{a}}g(a) = \d{f}(ax) \cdot x = \frac{\d{f}(ax) \cdot ax}{a} = \frac{k}{a}f(ax) = \frac{k}{a}g(a) $$ The solution of this ODE is $g(a) = g(1) a^k$ , i.e. $f(ax) = a^k f(x)$ for all nonzero $a$ . To wrap up I'm not sure whether it is necessary to state the theorem excluding the origin of $\R^n$ from the domain of $f$ ; The only constraint on $a$ seems to be $a \neq 0$ ; I see no constraint on the homogeneity degree $k$ , namely the theorem seems to hold for $k>0$ , $k=0$ and $k<0$ . Is this correct?","Most statements of Euler's theorem for homogeneous functions I found restrict the domain of the considered function, and/or the homogeneity degree, and/or the rescaling parameter. For example Wikipedia states the theorem for positively homogeneous functions Apostol at pag. 287 considers only positive rescaling parameters. I don't see what goes wrong with this generality: Definition Let be an integer. A differentiable function is -homogeneous if for all and all . Euler's Theorem Let be an integer and let be a differentiable function. Then is -homogeneous if and only if for all . Proof Let be -homogeneous. Then for any nonzero real , . Differentiate with respect to : In particular this must be true for = 1, so Conversely, let the condition above hold; then for any real , . Define for any fixed . I guess here it is actually necessary to consider for any fixed . Differentiate with respect to , and assume : The solution of this ODE is , i.e. for all nonzero . To wrap up I'm not sure whether it is necessary to state the theorem excluding the origin of from the domain of ; The only constraint on seems to be ; I see no constraint on the homogeneity degree , namely the theorem seems to hold for , and . Is this correct?","
\newcommand{\R}{\mathbb{R}}
\newcommand{\ra}{\rightarrow}
\newcommand{\d}{\text{d}}
 f: \R^n \setminus {0} \ra \R k f: \R^n \ra \R k 
f(ax) = a^kf(x)
 x \in \R^n a \in \R \setminus {0} k f: \R^n \ra \R f k 
x \cdot \d{f}(x) = kf(x)
 x \in \R^n f k a f(ax) = a^k f(x) a 
\d{f}(ax) \cdot x =  ka^{k-1}f(x)
 a 
\d{f}(x) \cdot x = kf(x)
 a ax \cdot \d{f}(ax) = k f (ax) g(a) = f(ax) x \in \R^n g(a) = f(ax) x \in \R^n \setminus {0} g a a \neq 0 
\frac{\d{}}{\d{a}}g(a) = \d{f}(ax) \cdot x = \frac{\d{f}(ax) \cdot ax}{a} = \frac{k}{a}f(ax) = \frac{k}{a}g(a)
 g(a) = g(1) a^k f(ax) = a^k f(x) a \R^n f a a \neq 0 k k>0 k=0 k<0",['multivariable-calculus']
30,Show that Newton update in two variables converges,Show that Newton update in two variables converges,,"I am having two update rules of a two variable function $f(x, y)$ , i.e., $$x \mapsto h_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x,y)} \tag{1}$$ $$y \mapsto h_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x,y)} \tag {2}$$ We know that $(1)$ and $(2)$ converge when applied seperately. My goal is to prove that when these rules are applied alternatively they do find a root for $f(x,y)$ and converge to a stationary point. In [1] , it is shown that $(1)$ and $(2)$ are contractions mappings. I am trying to understand if $$H(x,y) = \begin{bmatrix} h_1(x,y) \\ h_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x, y)}{\partial_y f(x,y)} \end{bmatrix}$$ is a contraction mapping. Following the same procedure as in [1] we have $$\begin{aligned}\|H(x,y) -H(x',y') \| = &  \left\| \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x,y)}{\partial_y f(x,y)} \end{bmatrix} - \begin{bmatrix} x' \\ y'\end{bmatrix} + \begin{bmatrix} \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix}\right\|  \\ = & \left\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} -  \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x,y)}{\partial_y f(x,y)} - \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix} \right\|. \end{aligned}\tag{3}$$ Following the same procedure as in [1] , I am trying to apply mean value theorem with two variables but I can not show that $$\| H(x,y) - H(x',y')\| \leq k~ \| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{4}$$ for some appropriate $k$ . Could you please someone help to show that $H(x, y)$ is (or it is not) a contraction mapping? EDIT1 : If we use the modified Newton method we have $$x \mapsto c_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x^0,y^0)} \tag{5}$$ $$y \mapsto c_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x^0,y^0)} \tag {6}$$ By mean value theorem we have $$\| C(x,y) - C(x',y')\| \leq \| \nabla C(x,y)^T \|\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{7}$$ where $$C(x,y) = \begin{bmatrix} c_1(x,y) \\ c_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x^0,y^0)} \\ \frac{f(x, y)}{\partial_y f(x^0,y^0)} \end{bmatrix}\tag{8}$$ Also, I have that $\partial_x f(x, y)$ and $\partial_y f(x, y)$ are decreasing and positive. Thus, there exists $m1,m_2$ and $M_1, M_2$ such that $$0 < m_1 < \partial_x f(x,y) < \frac{M_1}{2} \tag{9}$$ and $$0 < m_2 < \partial_x f(x,y) < \frac{M_2}{2}, \tag{10}$$ where $M_1 = \max_x \{ \partial_x f(x,y) \}$ and $M_2 = \max_y \{ \partial_x f(x,y) \}$ . Thus, we need to show that $ \| \nabla C(x,y)^T \|<1$ in order $C(x,y)$ to be a contraction mapping. We have $$ \| \nabla C(x,y)^T \| = \| I - \begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix} \| \leq 1 + \| \underbrace{\begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix}}_{A} \|.\tag{11}$$ The eigenvalues of $A$ are the roots of $$det(A -\lambda I) = 0.\tag{12}$$ Doing computations, we conclude that $$\lambda (\lambda - \partial_x f(x,y) / M_1 - \partial_y f(x,y) / M_2 ) = 0 \tag{13}$$ which gives $$\lambda = 0 \tag{14} \Rightarrow \| \nabla C(x,y)^T \|<1 $$ and $$\lambda = \partial_x f(x,y) / M_1 + \partial_y f(x,y) / M_2 < 1 \Rightarrow \| \nabla C(x,y)^T \|< 2 \tag{15}$$ where $(9)$ and $(10)$ are used in the last inequality. If we keep $\lambda = 0 $ , $C(x,y)$ is a contraction by not in the case where $\lambda < 1$ . Not a satisfactory result. Can we do better? Any comments are highly appreciated. EDIT2: Please let me add some more information of the problem I am facing. Let $t \colon \mathbb{R}^2 \to \mathbb{R}$ . I am trying to find a way to get the roots of the nonlinear equation $$f (x,y )= 1/ \| t (x, y)\| - 1/ y \tag{16}$$ subject to $x \geq x_0$ and $y \geq y_0$ . As far as I understand this is a single equation with two unknowns. Maybe this is the reason the mapping is not a contraction. Any information to understand the general behavior of $(16)$ along with the aforementioned discussion is highly appreciated.","I am having two update rules of a two variable function , i.e., We know that and converge when applied seperately. My goal is to prove that when these rules are applied alternatively they do find a root for and converge to a stationary point. In [1] , it is shown that and are contractions mappings. I am trying to understand if is a contraction mapping. Following the same procedure as in [1] we have Following the same procedure as in [1] , I am trying to apply mean value theorem with two variables but I can not show that for some appropriate . Could you please someone help to show that is (or it is not) a contraction mapping? EDIT1 : If we use the modified Newton method we have By mean value theorem we have where Also, I have that and are decreasing and positive. Thus, there exists and such that and where and . Thus, we need to show that in order to be a contraction mapping. We have The eigenvalues of are the roots of Doing computations, we conclude that which gives and where and are used in the last inequality. If we keep , is a contraction by not in the case where . Not a satisfactory result. Can we do better? Any comments are highly appreciated. EDIT2: Please let me add some more information of the problem I am facing. Let . I am trying to find a way to get the roots of the nonlinear equation subject to and . As far as I understand this is a single equation with two unknowns. Maybe this is the reason the mapping is not a contraction. Any information to understand the general behavior of along with the aforementioned discussion is highly appreciated.","f(x, y) x \mapsto h_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x,y)} \tag{1} y \mapsto h_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x,y)} \tag {2} (1) (2) f(x,y) (1) (2) H(x,y) = \begin{bmatrix} h_1(x,y) \\ h_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x, y)}{\partial_y f(x,y)} \end{bmatrix} \begin{aligned}\|H(x,y) -H(x',y') \| = &  \left\| \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} \\ \frac{f(x,y)}{\partial_y f(x,y)} \end{bmatrix} - \begin{bmatrix} x' \\ y'\end{bmatrix} + \begin{bmatrix} \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix}\right\|  \\ = & \left\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x,y)} -  \frac{f(x', y')}{\partial_x f(x',y')} \\ \frac{f(x,y)}{\partial_y f(x,y)} - \frac{f(x', y')}{\partial_y f(x',y')} \end{bmatrix} \right\|. \end{aligned}\tag{3} \| H(x,y) - H(x',y')\| \leq k~ \| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{4} k H(x, y) x \mapsto c_1(x,y):=x - \frac{f(x, y)}{\partial_x f(x^0,y^0)} \tag{5} y \mapsto c_2(x,y):=y - \frac{f(x, y)}{\partial_y f(x^0,y^0)} \tag {6} \| C(x,y) - C(x',y')\| \leq \| \nabla C(x,y)^T \|\| \begin{bmatrix} x-x' \\ y-y' \end{bmatrix} \|\tag{7} C(x,y) = \begin{bmatrix} c_1(x,y) \\ c_2(x,y) \end{bmatrix} = \begin{bmatrix} x \\ y\end{bmatrix} - \begin{bmatrix} \frac{f(x, y)}{\partial_x f(x^0,y^0)} \\ \frac{f(x, y)}{\partial_y f(x^0,y^0)} \end{bmatrix}\tag{8} \partial_x f(x, y) \partial_y f(x, y) m1,m_2 M_1, M_2 0 < m_1 < \partial_x f(x,y) < \frac{M_1}{2} \tag{9} 0 < m_2 < \partial_x f(x,y) < \frac{M_2}{2}, \tag{10} M_1 = \max_x \{ \partial_x f(x,y) \} M_2 = \max_y \{ \partial_x f(x,y) \}  \| \nabla C(x,y)^T \|<1 C(x,y)  \| \nabla C(x,y)^T \| = \| I - \begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix} \| \leq 1 + \| \underbrace{\begin{bmatrix} \partial_x f(x,y)/M_1 & \partial_y f(x,y)/M_1 \\ \partial_x f(x,y)/M_2 & \partial_y f(x,y)/M_2 \end{bmatrix}}_{A} \|.\tag{11} A det(A -\lambda I) = 0.\tag{12} \lambda (\lambda - \partial_x f(x,y) / M_1 - \partial_y f(x,y) / M_2 ) = 0 \tag{13} \lambda = 0 \tag{14} \Rightarrow \| \nabla C(x,y)^T \|<1  \lambda = \partial_x f(x,y) / M_1 + \partial_y f(x,y) / M_2 < 1 \Rightarrow \| \nabla C(x,y)^T \|< 2 \tag{15} (9) (10) \lambda = 0  C(x,y) \lambda < 1 t \colon \mathbb{R}^2 \to \mathbb{R} f (x,y )= 1/ \| t (x, y)\| - 1/ y \tag{16} x \geq x_0 y \geq y_0 (16)","['linear-algebra', 'multivariable-calculus', 'optimization', 'newton-raphson', 'contraction-operator']"
31,Smoothness of the average value.,Smoothness of the average value.,,"Let $\Omega$ be an open set in $\mathbb R^n$ and let $F : \Omega \times \mathbb R\to \mathbb R$ be a smooth function. Let $G$ be the ""average of $F$ "" $$ G(x) = \int_0^1 F(x, t) dt.$$ By the intermediate value theorem, for each $x\in \Omega$ , there is $t(x) \in [0,1]$ so that $$ G(x) = F(x, t(x)).$$ Question : Can I choose this $t$ smoothly? Some observation: If $t$ is smooth and $F_t (x, t) \neq 0$ for all $(x, t)$ , $t$ satisfies the PDE $$ \nabla t = \frac{1}{F_t (x, t(x))} \big(\nabla G (x)- \nabla F(x, t(x))\big).$$ so when e.g. $n=1$ , this becomes an ODE and is always solvable and $t$ is smooth. What happens for general $n$ and when $F_t$ is possibly zero somewhere? I believe that $t(x)$ can be chosen to be continuous, but how about smoothness?","Let be an open set in and let be a smooth function. Let be the ""average of "" By the intermediate value theorem, for each , there is so that Question : Can I choose this smoothly? Some observation: If is smooth and for all , satisfies the PDE so when e.g. , this becomes an ODE and is always solvable and is smooth. What happens for general and when is possibly zero somewhere? I believe that can be chosen to be continuous, but how about smoothness?","\Omega \mathbb R^n F : \Omega \times \mathbb R\to \mathbb R G F  G(x) = \int_0^1 F(x, t) dt. x\in \Omega t(x) \in [0,1]  G(x) = F(x, t(x)). t t F_t (x, t) \neq 0 (x, t) t  \nabla t = \frac{1}{F_t (x, t(x))} \big(\nabla G (x)- \nabla F(x, t(x))\big). n=1 t n F_t t(x)","['multivariable-calculus', 'partial-differential-equations', 'smooth-functions']"
32,normal vector after transformation,normal vector after transformation,,"Let $E$ subset of $\mathbb{R^n}$ such that $E$ has smooth boundary. If $F:\mathbb{R^n }\rightarrow \mathbb{R}^n$ is a diffeomorphism and $G = F^{-1}$ its inverse. Let $\nu_{F(E)}(z)$ be the normal vector of $F(E)$ at $y \in \partial F(E)$ . I'm trying to understand why $$\nu_{F(E)}(z) = \frac{\nabla G(z)^*\nu_E(G(z))}{|\nabla G(z)^*\nu_E(G(z))|}.$$ Here I believe $\nabla G(z)^*$ is the adjoint of the linear operator $\nabla G(z)$ . My attempt: If $x \in \partial E$ and $\phi:U\subset \mathbb{R}^n \rightarrow \mathbb{R}$ such that $\{ y \in U \,| \phi(y)=0\} = \partial E \cap U$ then $$\nu_E(x) =\frac{ \nabla \phi(x)}{|\nabla \phi(x)|} .$$ Now $\phi\circ G: F(U) \rightarrow \mathbb{R}$ I believe gives the boundary of $F(E)$ near $F(x)=z$ then $$\frac{\nabla(\phi\circ G)(z)}{|\nabla(\phi\circ G)(z)|}= \nu_{F(E)}(z).$$ By the chain rule $$\nabla(\phi \circ G)(z) = \nabla \phi(G(z)) DG(z)$$ This where I got stuck. Would appreciate the help! *This is from Maggi's book on finite perimeter page 196.",Let subset of such that has smooth boundary. If is a diffeomorphism and its inverse. Let be the normal vector of at . I'm trying to understand why Here I believe is the adjoint of the linear operator . My attempt: If and such that then Now I believe gives the boundary of near then By the chain rule This where I got stuck. Would appreciate the help! *This is from Maggi's book on finite perimeter page 196.,"E \mathbb{R^n} E F:\mathbb{R^n }\rightarrow \mathbb{R}^n G = F^{-1} \nu_{F(E)}(z) F(E) y \in \partial F(E) \nu_{F(E)}(z) = \frac{\nabla G(z)^*\nu_E(G(z))}{|\nabla G(z)^*\nu_E(G(z))|}. \nabla G(z)^* \nabla G(z) x \in \partial E \phi:U\subset \mathbb{R}^n \rightarrow \mathbb{R} \{ y \in U \,| \phi(y)=0\} = \partial E \cap U \nu_E(x) =\frac{ \nabla \phi(x)}{|\nabla \phi(x)|} . \phi\circ G: F(U) \rightarrow \mathbb{R} F(E) F(x)=z \frac{\nabla(\phi\circ G)(z)}{|\nabla(\phi\circ G)(z)|}= \nu_{F(E)}(z). \nabla(\phi \circ G)(z) = \nabla \phi(G(z)) DG(z)","['multivariable-calculus', 'derivatives', 'differential-geometry']"
33,generalization of 2nd derivative test to multi dimensional when the hessian is inonclusive,generalization of 2nd derivative test to multi dimensional when the hessian is inonclusive,,"We all know the 2nd derivative test in its original form, if $f'(x)=0$ , then if $f''(x)<0$ the point is max, and if $f''(x)>0$ the point is min. We also know the generalization for the case is inconclusive with one variable: (I) https://en.wikipedia.org/wiki/Derivative_test#Higher-order_derivative_test We also know the generalization for multi variable:(II) https://en.wikipedia.org/wiki/Second_partial_derivative_test#Functions_of_many_variables The question is if there is a generalization of the two, say for multivariable if none of the conditions are met, then we can take the next derivative until we find a derivative which is not zero and use the conditions in (II) to decide whether it's a max or min. So prove or disprove, one can just take nth derivative and check using the II conditions, that if an extremum is a max or min, or disprove via a counterexample .","We all know the 2nd derivative test in its original form, if , then if the point is max, and if the point is min. We also know the generalization for the case is inconclusive with one variable: (I) https://en.wikipedia.org/wiki/Derivative_test#Higher-order_derivative_test We also know the generalization for multi variable:(II) https://en.wikipedia.org/wiki/Second_partial_derivative_test#Functions_of_many_variables The question is if there is a generalization of the two, say for multivariable if none of the conditions are met, then we can take the next derivative until we find a derivative which is not zero and use the conditions in (II) to decide whether it's a max or min. So prove or disprove, one can just take nth derivative and check using the II conditions, that if an extremum is a max or min, or disprove via a counterexample .",f'(x)=0 f''(x)<0 f''(x)>0,"['multivariable-calculus', 'derivatives']"
34,Solving these coupled differential equations.,Solving these coupled differential equations.,,"I have a set of coupled differential equations of the form $$ \omega y_1 \frac{\partial q_1}{\partial y_1}+\omega y_2 \frac{\partial q_1}{\partial y_2}+\omega y_3 \frac{\partial q_1}{\partial y_3}+g\frac{\partial q_1}{\partial y_1}+t q_{3}e^{-g/\omega (y_{3}-y_1)}-t q_{2}e^{-g/\omega (y_{2}-y_3)}=(E+g^2/\omega)q_1$$ $$ \omega y_1 \frac{\partial q_2}{\partial y_1}+\omega y_2 \frac{\partial q_2}{\partial y_2}+\omega y_3 \frac{\partial q_2}{\partial y_3}+g\frac{\partial q_2}{\partial y_2}+t q_{1}e^{-g/\omega (y_{1}-y_2)}-t q_{3}e^{-g/\omega (y_{3}-y_1)}=(E+g^2/\omega)q_2$$ $$ \omega y_1 \frac{\partial q_3}{\partial y_1}+\omega y_2 \frac{\partial q_3}{\partial y_2}+\omega y_3 \frac{\partial q_3}{\partial y_3}+g\frac{\partial q_3}{\partial y_3}+t q_{2}e^{-g/\omega (y_{2}-y_3)}-t q_{1}e^{-g/\omega (y_{1}-y_2)}=(E+g^2/\omega)q_3$$ How do I solve these multivariate coupled differential equations? When I add all of these equations together, I obtain $$\omega y_1 \frac{\partial}{\partial y_1}(q_1+q_2+q_3)+\omega y_2 \frac{\partial}{\partial y_2}(q_1+q_2+q_3)+\omega y_3 \frac{\partial}{\partial y_3}(q_1+q_2+q_3)+g \nabla\cdot q=(E+g^2/\omega)(q_1+q_2+q_3) $$ I am very confused about how to handle the $\nabla\cdot q=\frac{\partial q_1}{\partial y_1}+\frac{\partial q_2}{\partial y_2}+\frac{\partial q_3}{\partial y_3}$ term.","I have a set of coupled differential equations of the form How do I solve these multivariate coupled differential equations? When I add all of these equations together, I obtain I am very confused about how to handle the term.", \omega y_1 \frac{\partial q_1}{\partial y_1}+\omega y_2 \frac{\partial q_1}{\partial y_2}+\omega y_3 \frac{\partial q_1}{\partial y_3}+g\frac{\partial q_1}{\partial y_1}+t q_{3}e^{-g/\omega (y_{3}-y_1)}-t q_{2}e^{-g/\omega (y_{2}-y_3)}=(E+g^2/\omega)q_1  \omega y_1 \frac{\partial q_2}{\partial y_1}+\omega y_2 \frac{\partial q_2}{\partial y_2}+\omega y_3 \frac{\partial q_2}{\partial y_3}+g\frac{\partial q_2}{\partial y_2}+t q_{1}e^{-g/\omega (y_{1}-y_2)}-t q_{3}e^{-g/\omega (y_{3}-y_1)}=(E+g^2/\omega)q_2  \omega y_1 \frac{\partial q_3}{\partial y_1}+\omega y_2 \frac{\partial q_3}{\partial y_2}+\omega y_3 \frac{\partial q_3}{\partial y_3}+g\frac{\partial q_3}{\partial y_3}+t q_{2}e^{-g/\omega (y_{2}-y_3)}-t q_{1}e^{-g/\omega (y_{1}-y_2)}=(E+g^2/\omega)q_3 \omega y_1 \frac{\partial}{\partial y_1}(q_1+q_2+q_3)+\omega y_2 \frac{\partial}{\partial y_2}(q_1+q_2+q_3)+\omega y_3 \frac{\partial}{\partial y_3}(q_1+q_2+q_3)+g \nabla\cdot q=(E+g^2/\omega)(q_1+q_2+q_3)  \nabla\cdot q=\frac{\partial q_1}{\partial y_1}+\frac{\partial q_2}{\partial y_2}+\frac{\partial q_3}{\partial y_3},"['multivariable-calculus', 'partial-differential-equations']"
35,"What does $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ look like and ""repeat"" extrema?","What does  and  look like and ""repeat"" extrema?","D = \{(x, y) : 0 ≤ x ≤ 1 1 ≤ y ≤ 2\}","I have to find the absolute min and max of $f(x, y) = x^2 - x^2y$ on D. So first my question is what does this closed, bounded set, $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ look like? I have drawn what I think it looks like, below If my drawing is correct, To find global extrema, I am finding the max/min on the boundary and the critical points to do some comparisons. I have no problem finding the critical points, but a bit confused about the boundaries. Can I continue this problem by investigating each of the boundary/line? For example, setting $x = 1, 1 ≤ y ≤ 2$ (right) , $x= 0, 1 ≤ y ≤ 2$ (left) $y = 2, 0 ≤ x ≤ 1$ (top) $y =1, 0 ≤ x ≤ 1$ (bottom) And find the max/min on each boundary? Do i have to do this? I have done some previous questions, and realised that some of the extrema occur more than once. How can I avoid these ""repeats"" extrema? Do we always get ""repeats""? How do you tell? EDIT: Also can you have critical points in general  but have NO critical points satisfying $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ ? Then the max/min cannot occur here right?","I have to find the absolute min and max of on D. So first my question is what does this closed, bounded set, and look like? I have drawn what I think it looks like, below If my drawing is correct, To find global extrema, I am finding the max/min on the boundary and the critical points to do some comparisons. I have no problem finding the critical points, but a bit confused about the boundaries. Can I continue this problem by investigating each of the boundary/line? For example, setting (right) , (left) (top) (bottom) And find the max/min on each boundary? Do i have to do this? I have done some previous questions, and realised that some of the extrema occur more than once. How can I avoid these ""repeats"" extrema? Do we always get ""repeats""? How do you tell? EDIT: Also can you have critical points in general  but have NO critical points satisfying and ? Then the max/min cannot occur here right?","f(x, y) = x^2 - x^2y D = \{(x, y) : 0 ≤ x ≤ 1 1 ≤ y ≤ 2\} x = 1, 1 ≤ y ≤ 2 x= 0, 1 ≤ y ≤ 2 y = 2, 0 ≤ x ≤ 1 y =1, 0 ≤ x ≤ 1 D = \{(x, y) : 0 ≤ x ≤ 1 1 ≤ y ≤ 2\}","['multivariable-calculus', 'maxima-minima']"
36,Multivariate Residue Theorem?,Multivariate Residue Theorem?,,"Is there an extension of the residue theorem to multivariate complex functions? Say you have a function of $n$ complex variables $s_{n}$ and you wish to integrate it over some region in $\mathbb{C}^{n}$. Can you exploit the singularities of the function as you would in the single variable case to evaluate the integral? For example  \begin{equation}G=\int_{\Omega} d^{n}s \frac{1}{\sum_{i} s_{i}^{2}}\end{equation} This is singular for the $n$ roots of the equation $\sum_{i}s_{i}^{2}=0$ but I can't think of a way to extend the residue theorem to such a case. I guess you could go to 'polar' coordinates and it may simplify this one, but what about more complicated functions where this isn't possible? I'm sorry if the question is ill-defined.","Is there an extension of the residue theorem to multivariate complex functions? Say you have a function of $n$ complex variables $s_{n}$ and you wish to integrate it over some region in $\mathbb{C}^{n}$. Can you exploit the singularities of the function as you would in the single variable case to evaluate the integral? For example  \begin{equation}G=\int_{\Omega} d^{n}s \frac{1}{\sum_{i} s_{i}^{2}}\end{equation} This is singular for the $n$ roots of the equation $\sum_{i}s_{i}^{2}=0$ but I can't think of a way to extend the residue theorem to such a case. I guess you could go to 'polar' coordinates and it may simplify this one, but what about more complicated functions where this isn't possible? I'm sorry if the question is ill-defined.",,"['complex-analysis', 'reference-request', 'several-complex-variables']"
37,Lipschitz continuous gradient and quadratic bound property,Lipschitz continuous gradient and quadratic bound property,,"Let $\left\lVert \cdot \right\rVert$ denote the Euclidean norm. A differentiable function $f: D \subseteq \mathbb{R}^n \to \mathbb{R}$ is said to have Lipschitz continuous gradient with parameter $L > 0$ if $$ \left\lVert \nabla f(x) - \nabla f(y) \right\rVert  \leq L  \left\lVert x-y \right\rVert \text{for all } x, y \in D.  \tag{1}\label{1}$$ A standard result is that a function $f$ (not necessarily convex) with Lipschitz continuous gradient satisfies the following ""quadratic bound property"": $$|f(y) - f(x) - \nabla f(x)^T(y-x)| \leq \frac{L}{2} \left\lVert x - y \right\rVert^2 \text{ for all } x, y \in D. \tag{2}\label{2}$$ I am thinking about the converse, namely if a function $f$ satisfies \eqref{2}, does it necessarily satisfy \eqref{1}? In http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf (slide 17) it seems like if we for instance assume that $f$ is convex with co-coercive gradient, the quadratic bound property implies Lipschitz continuity of the gradient. However, without these assumptions I strongly suspect that \eqref{2} does not necessarily imply \eqref{1}. Just for fun, I have (without success) been trying to come up with an example of a function satisfying \eqref{2} but not \eqref{1}. Can anyone provide an example of such a function, or motivate why it does not exist?","Let denote the Euclidean norm. A differentiable function is said to have Lipschitz continuous gradient with parameter if A standard result is that a function (not necessarily convex) with Lipschitz continuous gradient satisfies the following ""quadratic bound property"": I am thinking about the converse, namely if a function satisfies \eqref{2}, does it necessarily satisfy \eqref{1}? In http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf (slide 17) it seems like if we for instance assume that is convex with co-coercive gradient, the quadratic bound property implies Lipschitz continuity of the gradient. However, without these assumptions I strongly suspect that \eqref{2} does not necessarily imply \eqref{1}. Just for fun, I have (without success) been trying to come up with an example of a function satisfying \eqref{2} but not \eqref{1}. Can anyone provide an example of such a function, or motivate why it does not exist?","\left\lVert \cdot \right\rVert f: D \subseteq \mathbb{R}^n \to \mathbb{R} L > 0  \left\lVert \nabla f(x) - \nabla f(y) \right\rVert  \leq L  \left\lVert x-y \right\rVert \text{for all } x, y \in D.  \tag{1}\label{1} f |f(y) - f(x) - \nabla f(x)^T(y-x)| \leq \frac{L}{2} \left\lVert x - y \right\rVert^2 \text{ for all } x, y \in D. \tag{2}\label{2} f f","['multivariable-calculus', 'convex-analysis', 'convex-optimization', 'lipschitz-functions']"
38,Is a multivariable function with coplanar tangent lines always differentiable?,Is a multivariable function with coplanar tangent lines always differentiable?,,"It's easy to find a function $f(x,y)$ whose directional derivatives are all zero at a point but which is not differentiable at that point.  But my question is, is it true that a function $f(x,y)$ is differentiable at a point if and only if the tangent line of every differentiable curve lying on the surface and passing through the point lies on the same plane? If not, does anyone know of a counterexample?  For instance, is it possible for $$ \frac{d}{dt} f\bigl( x(t), y(t) \bigr)\Big|_{t=t_0} $$ to equal $0$ for every differentiable curve $\langle x, y \rangle = \langle x(t), y(t) \rangle$ passing through $\langle x(t_0), y(t_0) \rangle$ at $t=t_0$ without $f$ being differentiable at that point?","It's easy to find a function whose directional derivatives are all zero at a point but which is not differentiable at that point.  But my question is, is it true that a function is differentiable at a point if and only if the tangent line of every differentiable curve lying on the surface and passing through the point lies on the same plane? If not, does anyone know of a counterexample?  For instance, is it possible for to equal for every differentiable curve passing through at without being differentiable at that point?","f(x,y) f(x,y) 
\frac{d}{dt} f\bigl( x(t), y(t) \bigr)\Big|_{t=t_0}
 0 \langle x, y \rangle = \langle x(t), y(t) \rangle \langle x(t_0), y(t_0) \rangle t=t_0 f","['real-analysis', 'multivariable-calculus', 'derivatives', 'examples-counterexamples', 'tangent-spaces']"
39,"A multivariable function that is not differentiable only at $(-1<x<1, y=0)$",A multivariable function that is not differentiable only at,"(-1<x<1, y=0)","I have to think of a function with two variables( $f: R^2 \rightarrow R)$ that is continuous at every points in $R^2$ but not differentiable only at $B= \{ (x,y)|-1<x<1, y=0 \}$ . At first I came up with this function: $$ f(x,y)= \begin{cases} \sqrt{(x+1)^2+y^2},& x<-1\\\\ |y|,&-1\le x \le 1 \\\\ \sqrt{(x-1)^2+y^2},&x>1 \end{cases} $$ But I realized that this function is not differentiable in points $(-1,0)$ and $(1,0)$ which are not included in $B$ . Can anybody help me with this problem?",I have to think of a function with two variables( that is continuous at every points in but not differentiable only at . At first I came up with this function: But I realized that this function is not differentiable in points and which are not included in . Can anybody help me with this problem?,"f: R^2 \rightarrow R) R^2 B= \{ (x,y)|-1<x<1, y=0 \} 
f(x,y)= \begin{cases} \sqrt{(x+1)^2+y^2},& x<-1\\\\ |y|,&-1\le x \le 1 \\\\ \sqrt{(x-1)^2+y^2},&x>1 \end{cases}
 (-1,0) (1,0) B","['multivariable-calculus', 'derivatives']"
40,"If $f:{\Bbb R}^m \to {\Bbb R}^n$ is Lipschitz on all compact sets, under what conditions is $f$ a $C^1$ map?","If  is Lipschitz on all compact sets, under what conditions is  a  map?",f:{\Bbb R}^m \to {\Bbb R}^n f C^1,"If $f:{\Bbb R}^m \to {\Bbb R}^n$ is Lipschitz on all compact sets, under what conditions is $f$ a $C^1$ map? Background : I have proved the following result, and I am looking for a converse . Proposition. Let $f: {\Bbb R}^m \to {\Bbb R}^n$ be a $C^1$ mapping and let $K \subset {\Bbb R}^m$ be compact. Then the restriction $f|_K$ of $f$ to $K$ is Lipschitz continuous. The proof of this result, and some special cases, can be found here: Post 1 , Post 2 , Post 3 , and Post 4 . In general, differentiability is a stronger condition than continuity, so there is no reason to expect the ""Lipschitz on all compact sets"" assumption above to imply differentiability. What other additional assumptions on $f$ would be required to ensure that it is a $C^1$ mapping? Rademacher's Theorem is a related result; but I am clearly looking for something stronger. The goal is to characterize $C^1$ maps in terms of Lipschitz continuity on compact sets, and other additional conditions if required. Thank you, and I am excited to see where this goes!","If is Lipschitz on all compact sets, under what conditions is a map? Background : I have proved the following result, and I am looking for a converse . Proposition. Let be a mapping and let be compact. Then the restriction of to is Lipschitz continuous. The proof of this result, and some special cases, can be found here: Post 1 , Post 2 , Post 3 , and Post 4 . In general, differentiability is a stronger condition than continuity, so there is no reason to expect the ""Lipschitz on all compact sets"" assumption above to imply differentiability. What other additional assumptions on would be required to ensure that it is a mapping? Rademacher's Theorem is a related result; but I am clearly looking for something stronger. The goal is to characterize maps in terms of Lipschitz continuity on compact sets, and other additional conditions if required. Thank you, and I am excited to see where this goes!",f:{\Bbb R}^m \to {\Bbb R}^n f C^1 f: {\Bbb R}^m \to {\Bbb R}^n C^1 K \subset {\Bbb R}^m f|_K f K f C^1 C^1,"['real-analysis', 'multivariable-calculus', 'compactness', 'lipschitz-functions']"
41,Existence and uniqueness in a vector-field,Existence and uniqueness in a vector-field,,"Let $f: [0,1]^n \rightarrow \mathbb{R}_+^n$ be a vector-field where the function $f_i$ for each dimension $i$ is defined as: $$ f_i(\vec{x}) = \sqrt{ \sum_{j=1}^n x_i \cdot x_j \cdot C_{i,j}} $$ Where we are given the $n \times n$ matrix denoted $C$ with elements $C_{i,j} \in [0,1]$ for all $i$ and $j$ , and the diagonal equals 1 so $C_{i,i}=1$ , and the other elements are symmetrical so $C_{i,j} = C_{j,i}$ . We are also given $\vec{a} = [a_1, a_2, .., a_n]$ with $a_i \in [0,1]$ for all $i$ . Question 1: Does a solution $\vec{x} \in [0,1]^n$ exist that causes $f(\vec{x})$ to equal $\vec{a}$ ? Question 2: Is $\vec{x}$ unique? Question 3: How can we find $\vec{x}$ ? Please explain in a manner that can be understood by amateur mathematicians like myself. Note that I have found a couple of numerical algorithms to solve this problem, one is shown in another question , and another way is to derive the inverse of $f_i(\vec{x})$ and use that to find each $x_i$ that causes $f_i(\vec{x}) = a_i$ , but this causes $f_j(\vec{x}) \neq a_j$ for the other dimensions $j$ , so we need to do this for several iterations and then it converges to the correct solution. However, I am having trouble formally proving the convergence of those algorithms, so I would like to know if we can at least claim that a unique solution even exists, using only mathematical arguments? Thanks!","Let be a vector-field where the function for each dimension is defined as: Where we are given the matrix denoted with elements for all and , and the diagonal equals 1 so , and the other elements are symmetrical so . We are also given with for all . Question 1: Does a solution exist that causes to equal ? Question 2: Is unique? Question 3: How can we find ? Please explain in a manner that can be understood by amateur mathematicians like myself. Note that I have found a couple of numerical algorithms to solve this problem, one is shown in another question , and another way is to derive the inverse of and use that to find each that causes , but this causes for the other dimensions , so we need to do this for several iterations and then it converges to the correct solution. However, I am having trouble formally proving the convergence of those algorithms, so I would like to know if we can at least claim that a unique solution even exists, using only mathematical arguments? Thanks!","f: [0,1]^n \rightarrow \mathbb{R}_+^n f_i i 
f_i(\vec{x}) = \sqrt{ \sum_{j=1}^n x_i \cdot x_j \cdot C_{i,j}}
 n \times n C C_{i,j} \in [0,1] i j C_{i,i}=1 C_{i,j} = C_{j,i} \vec{a} = [a_1, a_2, .., a_n] a_i \in [0,1] i \vec{x} \in [0,1]^n f(\vec{x}) \vec{a} \vec{x} \vec{x} f_i(\vec{x}) x_i f_i(\vec{x}) = a_i f_j(\vec{x}) \neq a_j j","['multivariable-calculus', 'functions']"
42,Doubt with Chain rule differentiation in multivariate calculus.,Doubt with Chain rule differentiation in multivariate calculus.,,"Suppose $E\subseteq\mathbb R^n$ and $f$ maps $E$ into $\mathbb R^m$ . Let $g$ map a subset of $\mathbb R^m$ into $\mathbb R^p$ . If $f$ is differentiable at $x\in E$ and $g$ is differentiable at $f(x) \in f(E)$ , then the composition $g \circ f$ is differentiable at $x$ and $$(g\circ f)'(x) = g'(f(x)) f'(x).$$ where the indicated product is matrix multiplication. Although this version of the chain rule may look a bit strange, it is really just the familiar chain rule of calculus in a new guise. You can convince yourself of this fact by writing the formula out in terms of partial derivatives. This is something I don't understand. I don't find this definition of chain rule intuitive.Can someone explain this?","Suppose and maps into . Let map a subset of into . If is differentiable at and is differentiable at , then the composition is differentiable at and where the indicated product is matrix multiplication. Although this version of the chain rule may look a bit strange, it is really just the familiar chain rule of calculus in a new guise. You can convince yourself of this fact by writing the formula out in terms of partial derivatives. This is something I don't understand. I don't find this definition of chain rule intuitive.Can someone explain this?",E\subseteq\mathbb R^n f E \mathbb R^m g \mathbb R^m \mathbb R^p f x\in E g f(x) \in f(E) g \circ f x (g\circ f)'(x) = g'(f(x)) f'(x).,"['multivariable-calculus', 'chain-rule']"
43,Write a double integral for surface area of cone $z=1-\sqrt{x^2+y^2}$ above the plane $x+2z+1=0$.,Write a double integral for surface area of cone  above the plane .,z=1-\sqrt{x^2+y^2} x+2z+1=0,"Write a double integral for surface area of cone $z=1-\sqrt{x^2+y^2}$ above the plane $x+2z+1=0$ . First I started with finding projection of these two in $xOy$ plane: $-\frac{1}{2} - \frac{x}{2}=1-\sqrt{x^2+y^2}$ ... which is ellipse $$\frac{(x-1)^2}{2^2} + \frac{y^2}{(\sqrt{3})^2}=1.$$ Then using formula $$P=\iint_D \sqrt{1+p^2+q^2}dxdy;$$ I have $z(x,y)=1-\sqrt{x^2+y^2}$ I can easily find partials $p$ and $q$ and I get $\sqrt{2}$ for the $P=\int\int_D \sqrt{2}dxdy$ . Now the area over which the integration is done $D$ : Since it is ellipse I put elliptical coordinates to be: $x=1+2rcost$ and $y=\sqrt{3}rsint$ , where $0\leq r \leq 1$ and $0\leq t \leq 2\pi$ and Jacobian is $2\sqrt{3}r$ . Final, the integral would look like this: $$P=\int_{0}^{2\pi}\int_{0}^{1} \sqrt{2}2\sqrt{3}rdrdt.$$ Is this correct?","Write a double integral for surface area of cone above the plane . First I started with finding projection of these two in plane: ... which is ellipse Then using formula I have I can easily find partials and and I get for the . Now the area over which the integration is done : Since it is ellipse I put elliptical coordinates to be: and , where and and Jacobian is . Final, the integral would look like this: Is this correct?","z=1-\sqrt{x^2+y^2} x+2z+1=0 xOy -\frac{1}{2} - \frac{x}{2}=1-\sqrt{x^2+y^2} \frac{(x-1)^2}{2^2} + \frac{y^2}{(\sqrt{3})^2}=1. P=\iint_D \sqrt{1+p^2+q^2}dxdy; z(x,y)=1-\sqrt{x^2+y^2} p q \sqrt{2} P=\int\int_D \sqrt{2}dxdy D x=1+2rcost y=\sqrt{3}rsint 0\leq r \leq 1 0\leq t \leq 2\pi 2\sqrt{3}r P=\int_{0}^{2\pi}\int_{0}^{1} \sqrt{2}2\sqrt{3}rdrdt.","['multivariable-calculus', 'multiple-integral']"
44,"How to find $ \iint_{D_R}e^{-x}\arctan(\frac{y}{x})\,dx\,dy\,$?",How to find ?," \iint_{D_R}e^{-x}\arctan(\frac{y}{x})\,dx\,dy\,","How to find $$ \iint_{D_R}e^{-x}\arctan\frac{y}{x}\,dx\,dy\,,$$ where $$ D_R=\left\{(x,y) \mid \frac{R}{2}\leq x\leq R, 0\leq y \leq \frac{2}{R}x-1 \right\}\,?$$ I know that $$\int \arctan \frac{y}{x}\, dy =y \arctan \frac{y}{x} -\frac{x}{2} \ln\left(1+\left(\frac{y}{x}\right)^2\right)+C$$ So how to find $$\int_{R/2}^R e^{-x} \left(\frac{2}{R}x-1 \right) \arctan \frac{\frac{2}{R}x-1}{x} -\frac{x}{2} \ln\left(1+\left(\frac{\frac{2}{R}x-1}{x}\right)^2\right)\,dx$$ and it's difficult to calculate.",How to find where I know that So how to find and it's difficult to calculate.," \iint_{D_R}e^{-x}\arctan\frac{y}{x}\,dx\,dy\,,  D_R=\left\{(x,y) \mid \frac{R}{2}\leq x\leq R, 0\leq y \leq \frac{2}{R}x-1 \right\}\,? \int \arctan \frac{y}{x}\, dy =y \arctan \frac{y}{x} -\frac{x}{2} \ln\left(1+\left(\frac{y}{x}\right)^2\right)+C \int_{R/2}^R e^{-x} \left(\frac{2}{R}x-1 \right) \arctan \frac{\frac{2}{R}x-1}{x} -\frac{x}{2} \ln\left(1+\left(\frac{\frac{2}{R}x-1}{x}\right)^2\right)\,dx","['multivariable-calculus', 'multiple-integral']"
45,Coordinate Independence of Differential Forms Understanding,Coordinate Independence of Differential Forms Understanding,,"In A Geometric Approach to Differential Forms [Bachman, David], the author states the following: (On the geometric interpretation of 1-forms in $\mathbb{R^2}$ ) - 'We can interpret the act of multiplying by a constant geometrically. Suppose $\mathbb{w}$ is given by $adx+bdy$ . Then the value of $\mathbb{w}$ (V1) is the length of the projection of V1 onto the line, $l$ , where $\frac{<a,b>}{|<a,b>|^2}$ is a basis vector for $l$ . This interpretation has a huge advantage... it is coordinate free. Forms are objects that exist independently of our choice of coordinates.' I understand the geometric intuition, however, I fail to see why this is coordinate free. I would like to find out where I go wrong. What I see is that, as $\mathbb{w}$ relies on $dx$ and $dy$ , the differentials of coordinate functions themselves, how can it then be coordinate free? Furthermore, in $\frac{<a,b>}{|<a,b>|^2}$ , $<a.b>$ and thus its magnitude also depends on $dx$ and $dy$ so I fail to see how this is independent of coordinates - rather, the opposite as it makes an explicit reference to the differentials of coordinate functions? Would be very grateful if someone can show me at what point my logic goes faulty... Note: The author defines $dx$ and $dy$ as the coordinate function for $T_p\mathbb{R^2}$ , such that a vector in $T_p\mathbb{R^2}$ can be written $<dx,dy>$ where $<1,0>=\frac{d(x+t,y)}{dt}$ and $<0,1>=\frac{d(x,y+t)}{dt}$","In A Geometric Approach to Differential Forms [Bachman, David], the author states the following: (On the geometric interpretation of 1-forms in ) - 'We can interpret the act of multiplying by a constant geometrically. Suppose is given by . Then the value of (V1) is the length of the projection of V1 onto the line, , where is a basis vector for . This interpretation has a huge advantage... it is coordinate free. Forms are objects that exist independently of our choice of coordinates.' I understand the geometric intuition, however, I fail to see why this is coordinate free. I would like to find out where I go wrong. What I see is that, as relies on and , the differentials of coordinate functions themselves, how can it then be coordinate free? Furthermore, in , and thus its magnitude also depends on and so I fail to see how this is independent of coordinates - rather, the opposite as it makes an explicit reference to the differentials of coordinate functions? Would be very grateful if someone can show me at what point my logic goes faulty... Note: The author defines and as the coordinate function for , such that a vector in can be written where and","\mathbb{R^2} \mathbb{w} adx+bdy \mathbb{w} l \frac{<a,b>}{|<a,b>|^2} l \mathbb{w} dx dy \frac{<a,b>}{|<a,b>|^2} <a.b> dx dy dx dy T_p\mathbb{R^2} T_p\mathbb{R^2} <dx,dy> <1,0>=\frac{d(x+t,y)}{dt} <0,1>=\frac{d(x,y+t)}{dt}","['multivariable-calculus', 'differential-geometry', 'differential-forms']"
46,Integrating the derivative of a function with respect the components of a vector,Integrating the derivative of a function with respect the components of a vector,,"I'm stuck determining the integrals of the partial derivatives of a scalar function $F:\mathbb{R}^3\times\mathbb{R}^3\rightarrow\mathbb{R}$ with respect to the components of a vector. In the case of the problem I am trying to solve, $F=F(\vec x, \vec y)$ and the following relations hold $$ y_i=\frac{\partial F}{\partial x_i} \tag{1}$$ $$ x_i=\frac{\partial F}{\partial y_i}\tag{2} $$ My attempt at a solution Performing the ""partial integration"" over $(1)$ and $(2)$ , $$F(\vec x, \vec y)=\int\frac{\partial F}{\partial x_i}dx_i=\int y_idx_i+g_i(x_j, \vec y)=x_iy_i +g_i(x_j, \vec y) \tag{3}$$ $$ F(\vec x, \vec y)=\int\frac{\partial F}{\partial y_i}dy_i=\int x_idy_i+h_i(\vec x,y_j)=x_iy_i +h_i(\vec x,y_j) \tag{4}$$ Where $j\neq i$ in the functions $g_i$ and $h_i$ . I see that adding for $i=1,2,3$ the RHS $(3)$ and $(4)$ we can write $$3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y) \tag{5}$$ $$3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{6}$$ Obtaining this relation for the functions $g_i$ and $h_i$ $$\sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y)=\sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{7}$$ Would this approach be correct? Could we tell something else about the integrating ""constants"" $g_i$ and $h_i$ in addition to $(7)$ ?","I'm stuck determining the integrals of the partial derivatives of a scalar function with respect to the components of a vector. In the case of the problem I am trying to solve, and the following relations hold My attempt at a solution Performing the ""partial integration"" over and , Where in the functions and . I see that adding for the RHS and we can write Obtaining this relation for the functions and Would this approach be correct? Could we tell something else about the integrating ""constants"" and in addition to ?","F:\mathbb{R}^3\times\mathbb{R}^3\rightarrow\mathbb{R} F=F(\vec x, \vec y) 
y_i=\frac{\partial F}{\partial x_i} \tag{1} 
x_i=\frac{\partial F}{\partial y_i}\tag{2}
 (1) (2) F(\vec x, \vec y)=\int\frac{\partial F}{\partial x_i}dx_i=\int y_idx_i+g_i(x_j, \vec y)=x_iy_i +g_i(x_j, \vec y) \tag{3} 
F(\vec x, \vec y)=\int\frac{\partial F}{\partial y_i}dy_i=\int x_idy_i+h_i(\vec x,y_j)=x_iy_i +h_i(\vec x,y_j) \tag{4} j\neq i g_i h_i i=1,2,3 (3) (4) 3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y) \tag{5} 3F(\vec x, \vec y)=\vec x · \vec y + \sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{6} g_i h_i \sum_{i=1\\j\neq i}^3 g_i(x_j, \vec y)=\sum_{i=1\\j\neq i}^3 h_i(\vec x,y_j) \tag{7} g_i h_i (7)","['integration', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
47,Uniqueness of Taylor expansion?,Uniqueness of Taylor expansion?,,"I am struggling with proof of Morse's lemma in my textbook. The context you need to know: Function $f: \mathbb{R}^m \rightarrow \mathbb{R}$ , $f \in C^3$ . It has nondegenerate critical point at $x=0$ . $f(0)=0$ Now what happened. In the proof we managed to represent our function $f$ the following way: $$ f(x) = x^T \cdot H(x) \cdot x $$ Where $H(x)$ is symmetric (can be viewed as a quadratic form at each point $x$ ) $m \times m$ matrix. Every entry of $H(x)$ is a $C^1$ function of $x$ . Now they tell "" since the uniqueness of a Taylor expansion and continuity of entries of $H(x)$ we can conclude that $H(0)=\textbf{H}_f(0)$ "" ( $\textbf{H}_f$ denotes hessian matrix of the function $f$ ). This ""conclusion"" actually strikes me out. Let's look at the Taylor's expansion of a function $f$ near $x=0$ : $$ f(x)-f(0) = x^T \cdot \nabla f(0) + x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2) $$ And since $f(0)=0$ and $\nabla f(0)=0$ (critical point) we now can rewrite $f(x)$ as: $$ f(x)=x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2) $$ Compare it with earlier result: $$ f(x) = x^T \cdot H(x) \cdot x $$ Does it straightly follows that $H(0)=\textbf{H}_f(0)$ ? I think ""the uniqueness of Taylor expansion"" is not applicable here since $x^T \cdot H(x) \cdot x$ is not a polynomial function of coordinates of $x$ . UPDATE As I understood, we need to show that this condition $$ x^T \cdot (H(x) - \textbf{H}_f(0)) \cdot x = o(||x||^2) \quad \text{as} \space x \to 0 $$ implies $H(0) = \textbf{H}_f(0)$","I am struggling with proof of Morse's lemma in my textbook. The context you need to know: Function , . It has nondegenerate critical point at . Now what happened. In the proof we managed to represent our function the following way: Where is symmetric (can be viewed as a quadratic form at each point ) matrix. Every entry of is a function of . Now they tell "" since the uniqueness of a Taylor expansion and continuity of entries of we can conclude that "" ( denotes hessian matrix of the function ). This ""conclusion"" actually strikes me out. Let's look at the Taylor's expansion of a function near : And since and (critical point) we now can rewrite as: Compare it with earlier result: Does it straightly follows that ? I think ""the uniqueness of Taylor expansion"" is not applicable here since is not a polynomial function of coordinates of . UPDATE As I understood, we need to show that this condition implies","f: \mathbb{R}^m \rightarrow \mathbb{R} f \in C^3 x=0 f(0)=0 f 
f(x) = x^T \cdot H(x) \cdot x
 H(x) x m \times m H(x) C^1 x H(x) H(0)=\textbf{H}_f(0) \textbf{H}_f f f x=0 
f(x)-f(0) = x^T \cdot \nabla f(0) + x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2)
 f(0)=0 \nabla f(0)=0 f(x) 
f(x)=x^T \cdot \textbf{H}_f(0) \cdot x + o(||x||^2)
 
f(x) = x^T \cdot H(x) \cdot x
 H(0)=\textbf{H}_f(0) x^T \cdot H(x) \cdot x x 
x^T \cdot (H(x) - \textbf{H}_f(0)) \cdot x = o(||x||^2) \quad \text{as} \space x \to 0
 H(0) = \textbf{H}_f(0)","['multivariable-calculus', 'continuity', 'taylor-expansion']"
48,Green's Theorem Concepts: Circulation in R2,Green's Theorem Concepts: Circulation in R2,,"I am trying understand how circulation density arises in Green's theorem and I'd like to know if my line of thinking is on the right track. Here it goes :). Idea We know that if we have a vector field $\vec F=P\hat x+Q\hat y\in\mathbb{C}^1$ on an open region $D$ containing the simple region $R$ whose boundary is $\Gamma$ , then we can “cut” $\Gamma$ into $K$ positively-oriented rectangular paths such that $\Gamma=\bigcup_{k=1}^K\Gamma_k$ . Consequently, $$\oint_\Gamma \vec F\cdot d\vec r=\sum_{k=1}^K \oint_{\Gamma_k}\vec F\cdot d\vec r $$ which says the the macroscopic circulation of $\vec F$ along the curve $\Gamma$ is equal to the sum of microscopic circulations over the region $R$ enclosed by $\Gamma$ . Moreover, we know that the circulation along one of these rectangles is $$\oint_{\Gamma_k}\vec F\cdot d\vec r=\bigg(\frac{Q(x+\Delta x,\;y)-Q(x,y)}{\Delta x}-\frac{P(x,\;y+\Delta y)-P(x,y)}{\Delta y}\bigg)\;\Delta x\Delta y$$ (for proof, see Green's Theorem in the Plane: Circulation Density ). Hence, the circulation density, or curl, of $\vec F$ at any point $(x,y)$ in $R$ is given by $$\text{curl}\vec F(x,y)=\lim_{\Delta A\rightarrow 0}\frac{1}{\Delta A}\oint_\Gamma \vec F \cdot d\vec r=Q_x-P_y,$$ which is a scalar function. Now, if we let $\sigma (x,y) = Q_x-P_y$ and integrate over the $R$ , we obtain $$\sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y $$ hence, $$\lim_{n,m\rightarrow\infty}\sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y =\iint_R \sigma(x,y) dxdy=\iint_R (Q_x-P_y )dA$$ whose RHS can be related back to the line integral $\oint_\Gamma\vec F\cdot d\vec r$ giving us what we know as Green's Theorem.","I am trying understand how circulation density arises in Green's theorem and I'd like to know if my line of thinking is on the right track. Here it goes :). Idea We know that if we have a vector field on an open region containing the simple region whose boundary is , then we can “cut” into positively-oriented rectangular paths such that . Consequently, which says the the macroscopic circulation of along the curve is equal to the sum of microscopic circulations over the region enclosed by . Moreover, we know that the circulation along one of these rectangles is (for proof, see Green's Theorem in the Plane: Circulation Density ). Hence, the circulation density, or curl, of at any point in is given by which is a scalar function. Now, if we let and integrate over the , we obtain hence, whose RHS can be related back to the line integral giving us what we know as Green's Theorem.","\vec F=P\hat x+Q\hat y\in\mathbb{C}^1 D R \Gamma \Gamma K \Gamma=\bigcup_{k=1}^K\Gamma_k \oint_\Gamma \vec F\cdot d\vec r=\sum_{k=1}^K \oint_{\Gamma_k}\vec F\cdot d\vec r  \vec F \Gamma R \Gamma \oint_{\Gamma_k}\vec F\cdot d\vec r=\bigg(\frac{Q(x+\Delta x,\;y)-Q(x,y)}{\Delta x}-\frac{P(x,\;y+\Delta y)-P(x,y)}{\Delta y}\bigg)\;\Delta x\Delta y \vec F (x,y) R \text{curl}\vec F(x,y)=\lim_{\Delta A\rightarrow 0}\frac{1}{\Delta A}\oint_\Gamma \vec F \cdot d\vec r=Q_x-P_y, \sigma (x,y) = Q_x-P_y R \sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y  \lim_{n,m\rightarrow\infty}\sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y =\iint_R \sigma(x,y) dxdy=\iint_R (Q_x-P_y )dA \oint_\Gamma\vec F\cdot d\vec r","['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'line-integrals', 'greens-theorem']"
49,Q: Volume involving spherical and polar coordinates,Q: Volume involving spherical and polar coordinates,,I'm trying to find the volume between the surfaces defined by the following equations: $$ x^2 + y^2 + z^2 = b^2 $$ $$ y^2 \tan^2 a = x^2 + z^2 $$ I need to find the volume using two different methods: the first one is by spherical coordinates and the second one by polar coordinates. The problem is I get two different results: Using spherical coordinates I found: $$ x = \rho \sin \phi \cos \theta \\ z = \rho \sin \phi \sin \theta \\ y = \rho \cos \phi $$ $$ V= \int_0^{2\pi}\int_0^{a}\int_0^{b}\rho^2\sin\phi \:d\rho d\phi d\theta = \frac{2\pi}{3}b^3(-\cos(a)+1) $$ and using polar: $$ x = r \cos \theta \\ z = r \sin \theta \\ y = y $$ $$ V = \int_0^{2\pi}\int_0^{b\sin a}\int_0^{\sqrt{b^2-r^2}}r \:dy dr d\theta = -\frac{2\pi}{3}b^3(\cos^3(a)-1) $$ So I know I have made a mistake at some point but I can't find it. If anyone can help I'd appreciate it. Thanks!,I'm trying to find the volume between the surfaces defined by the following equations: I need to find the volume using two different methods: the first one is by spherical coordinates and the second one by polar coordinates. The problem is I get two different results: Using spherical coordinates I found: and using polar: So I know I have made a mistake at some point but I can't find it. If anyone can help I'd appreciate it. Thanks!,"
x^2 + y^2 + z^2 = b^2
 
y^2 \tan^2 a = x^2 + z^2
 
x = \rho \sin \phi \cos \theta \\
z = \rho \sin \phi \sin \theta \\
y = \rho \cos \phi
 
V= \int_0^{2\pi}\int_0^{a}\int_0^{b}\rho^2\sin\phi \:d\rho d\phi d\theta = \frac{2\pi}{3}b^3(-\cos(a)+1)
 
x = r \cos \theta \\
z = r \sin \theta \\
y = y
 
V = \int_0^{2\pi}\int_0^{b\sin a}\int_0^{\sqrt{b^2-r^2}}r \:dy dr d\theta = -\frac{2\pi}{3}b^3(\cos^3(a)-1)
","['calculus', 'multivariable-calculus', 'volume', 'multiple-integral', 'change-of-variable']"
50,Set of possible limits of two-variable functions along curves -- connected?,Set of possible limits of two-variable functions along curves -- connected?,,"I am currently teaching a multivariable calculus class and trying to come up with problems for the final exam, in particular a problem about multivariable limits. The following question popped into my head: Let $f(x,y) = \frac{P(x,y)}{Q(x,y)}$ be a rational function ( $P$ and $Q$ are polynomials) with $P(0,0) = Q(0,0) = 0$ . For different curves $C: [0,1] \to \mathbb{R}^2$ with $C(0) = (0,0)$ , the limits $\lim\limits_{t \to 0} f(C(t))$ may depend on $C$ , or might not even exist. What can we say about the set of (extended) real numbers $L$ which are limits of $f$ along some curve $C$ ? My gut instinct is that this set should be a connected subset of the extended real line, but I haven't been able to prove it. Comments: If it makes the problem easier, feel free to assume that $Q \neq 0$ in a neighborhood of the origin. I don't necessarily care about the most general result, I'm just interested in any kind of result. I don't think it should matter, but let's let our curves be any continuous curves. Would the answer change if we forced our curves to be $C^1$ or $C^\infty$ ? Or algebraic? I only asked that $f$ is a rational function because those are the kinds of functions we tend to give students in multivariable calculus classes -- also, my first attempt at this was to rewrite $\frac{P}{Q} = k$ as $P - kQ = 0$ and try to think about the geometry of that as a polynomial equation in three variables. But I suspect that whatever we could say about rational functions we could also say about functions which are just continuous away from the origin. My two attempts: Rewrite $\frac{P}{Q} = k$ as $P - kQ = 0$ which is now just a polynomial equation. I thought at first that maybe something along the lines of ""for most values of $L$ which are the limit along some curve, there is in fact a curve to $(0,0)$ along which the function is a constant $L$ "" but I'm not sure if that's actually true. Suppose that $A$ and $B$ are curves along which $f$ has limits $a$ and $b$ respectively, and let $c$ be between $a$ and $b$ . Reparametrize them so that $|A(t)| = |B(t)|$ for all $t$ . For each $t$ , look at the circular arc between $A(t)$ and $B(t)$ . I want to use the intermediate value theorem to pick points, for each $t$ , along this arc which are close to $c$ . (The function is continuous and there are points near the origin along $A$ which are close to $a$ and along $B$ which are close to $b$ so there should be points in between them which are close to $c$ .) But it's not clear to me that we can choose such points $C(t)$ for each $t$ in a way that makes the resulting function $t \mapsto C(t)$ continuous.","I am currently teaching a multivariable calculus class and trying to come up with problems for the final exam, in particular a problem about multivariable limits. The following question popped into my head: Let be a rational function ( and are polynomials) with . For different curves with , the limits may depend on , or might not even exist. What can we say about the set of (extended) real numbers which are limits of along some curve ? My gut instinct is that this set should be a connected subset of the extended real line, but I haven't been able to prove it. Comments: If it makes the problem easier, feel free to assume that in a neighborhood of the origin. I don't necessarily care about the most general result, I'm just interested in any kind of result. I don't think it should matter, but let's let our curves be any continuous curves. Would the answer change if we forced our curves to be or ? Or algebraic? I only asked that is a rational function because those are the kinds of functions we tend to give students in multivariable calculus classes -- also, my first attempt at this was to rewrite as and try to think about the geometry of that as a polynomial equation in three variables. But I suspect that whatever we could say about rational functions we could also say about functions which are just continuous away from the origin. My two attempts: Rewrite as which is now just a polynomial equation. I thought at first that maybe something along the lines of ""for most values of which are the limit along some curve, there is in fact a curve to along which the function is a constant "" but I'm not sure if that's actually true. Suppose that and are curves along which has limits and respectively, and let be between and . Reparametrize them so that for all . For each , look at the circular arc between and . I want to use the intermediate value theorem to pick points, for each , along this arc which are close to . (The function is continuous and there are points near the origin along which are close to and along which are close to so there should be points in between them which are close to .) But it's not clear to me that we can choose such points for each in a way that makes the resulting function continuous.","f(x,y) = \frac{P(x,y)}{Q(x,y)} P Q P(0,0) = Q(0,0) = 0 C: [0,1] \to \mathbb{R}^2 C(0) = (0,0) \lim\limits_{t \to 0} f(C(t)) C L f C Q \neq 0 C^1 C^\infty f \frac{P}{Q} = k P - kQ = 0 \frac{P}{Q} = k P - kQ = 0 L (0,0) L A B f a b c a b |A(t)| = |B(t)| t t A(t) B(t) t c A a B b c C(t) t t \mapsto C(t)","['real-analysis', 'limits', 'multivariable-calculus']"
51,convexity proof of a function including ln and sums,convexity proof of a function including ln and sums,,"$$f(x_1,\dots,x_n)=\sum\limits_{i=1}^nx_i\ln x_i-\left(\sum\limits_{i=1}^nx_i\right)\ln\left(\sum\limits_{i=1}^nx_i\right)\rightarrow R_{++}^n$$ How can I prove this is convex on $R_{++}^n$? I tried using the Hessian and couldn't prove it. There is a solution using the gradient and Jensen but very long and complicated.","$$f(x_1,\dots,x_n)=\sum\limits_{i=1}^nx_i\ln x_i-\left(\sum\limits_{i=1}^nx_i\right)\ln\left(\sum\limits_{i=1}^nx_i\right)\rightarrow R_{++}^n$$ How can I prove this is convex on $R_{++}^n$? I tried using the Hessian and couldn't prove it. There is a solution using the gradient and Jensen but very long and complicated.",,['convex-analysis']
52,A linear combination of diffeomorphims still a diffeomorphism?,A linear combination of diffeomorphims still a diffeomorphism?,,"Let $G_1, ..., G_s : \mathbb{R}^s \to \mathbb{R}^s$ be diffeomorphisms on an open unit ball $B \subset \mathbb{R}^s$ , centered at $0$ , onto their images. Let $m_1, .., m_s$ be integers not all $0$ , and let $F = m_1 G_1 + ... + m_s G_s$ . Suppose we know that the determinant of the Jacobian matrix of $F$ at $x$ is non-zero for all $x \in B$ . Does it then follow that $F$ is still a diffeomorphism on $B$ ? If $F$ is not a diffeomorphism on $B$ , is it possible to show that $$ \# F^{-1}(y) < C $$ for all $y$ in $F(B)$ for some positive $C$ ? Any comments are appreciated! Ps edits had been made based on comments","Let be diffeomorphisms on an open unit ball , centered at , onto their images. Let be integers not all , and let . Suppose we know that the determinant of the Jacobian matrix of at is non-zero for all . Does it then follow that is still a diffeomorphism on ? If is not a diffeomorphism on , is it possible to show that for all in for some positive ? Any comments are appreciated! Ps edits had been made based on comments","G_1, ..., G_s : \mathbb{R}^s \to \mathbb{R}^s B \subset \mathbb{R}^s 0 m_1, .., m_s 0 F = m_1 G_1 + ... + m_s G_s F x x \in B F B F B 
\# F^{-1}(y) < C
 y F(B) C","['multivariable-calculus', 'differential-geometry', 'linear-transformations', 'jacobian', 'diffeomorphism']"
53,Divergence theorem given $\overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k}$ and the volume,Divergence theorem given  and the volume,\overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k},"I am trying to calculate $\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS$ using the divergence theorem. It is given that $\overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k}$ and $V$ is a 3-dimensional object having volume $4$ bounded by the surface $S$ . Here are the steps I have taken: Divergence theorem $$\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = \iiint\limits_V \text{div} \overset{\rightharpoonup} F dx\ dy\ dz$$ Find the divergence of $\overset{\rightharpoonup} F$ $$\text{div} \overset{\rightharpoonup} F = \frac{\partial}{\partial x}x + \frac{\partial}{\partial y}y + \frac{\partial}{\partial z}z = 3$$ Substitute into the divergence theorem $$\iiint\limits_V (3)\ dx\ dy\ dz$$ $$3 \cdot \iiint\limits_V\ dx\ dy\ dz$$ Given the volume is 4, $$\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = 3 \cdot 4 = 12$$ Is this correct? My main doubt is whether it is permissible to replace $\iiint\limits_V\ dx\ dy\ dz$ with $4$ in the last step.","I am trying to calculate using the divergence theorem. It is given that and is a 3-dimensional object having volume bounded by the surface . Here are the steps I have taken: Divergence theorem Find the divergence of Substitute into the divergence theorem Given the volume is 4, Is this correct? My main doubt is whether it is permissible to replace with in the last step.",\iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS \overset{\rightharpoonup} F = x \hat{i} + y \hat{j} +z \hat{k} V 4 S \iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = \iiint\limits_V \text{div} \overset{\rightharpoonup} F dx\ dy\ dz \overset{\rightharpoonup} F \text{div} \overset{\rightharpoonup} F = \frac{\partial}{\partial x}x + \frac{\partial}{\partial y}y + \frac{\partial}{\partial z}z = 3 \iiint\limits_V (3)\ dx\ dy\ dz 3 \cdot \iiint\limits_V\ dx\ dy\ dz \iint\limits_S \overset{\rightharpoonup} F \cdot \overset{\rightharpoonup} n\ dS = 3 \cdot 4 = 12 \iiint\limits_V\ dx\ dy\ dz 4,"['multivariable-calculus', 'multiple-integral', 'divergence-theorem']"
54,"If $f_x(x,y)>0$, $f_{xx}(x,y)<0$, $f_y(x,y)>0$, $f_{yy}(x,y)<0$ can $f_{xy} $ change sign?","If , , ,  can  change sign?","f_x(x,y)>0 f_{xx}(x,y)<0 f_y(x,y)>0 f_{yy}(x,y)<0 f_{xy} ","That is, suppose we have a continuous and (at least) twice differentiable function $f(x,y)$ which is increasing but concave in each of its individual arguments $^*$ , Note that these derivatives are all assumed to be non-zero the domain of $f$ is $[0,C]$ with $C>0$ (if changing the domain matters please feel free to let me know. I'm interested) Is it possible for the cross partial derivative to change signs (? (i.e. $\frac{\partial f}{\partial x \partial y} >0$ at some point but $<0$ at another point) If it is possible, what if we look along a line (i.e. fix an $x$ or $y$ value and look along the line from that point) (just answering the bold question is an acceptable answer) If it is possible, can someone provide an example function? If not, I am looking for a proof (or hints for a proof, or some intuition) *By increasing but concave in each argument I mean $\frac{\partial f}{\partial x} >0$ and $\frac{\partial^2 f}{\partial x^2}<0$ , and similar for $y$ Edit: I made question more slightly general. I apologize if someone was already typing an answer. (It is more general because if this edited question is true, then so is the original)","That is, suppose we have a continuous and (at least) twice differentiable function which is increasing but concave in each of its individual arguments , Note that these derivatives are all assumed to be non-zero the domain of is with (if changing the domain matters please feel free to let me know. I'm interested) Is it possible for the cross partial derivative to change signs (? (i.e. at some point but at another point) If it is possible, what if we look along a line (i.e. fix an or value and look along the line from that point) (just answering the bold question is an acceptable answer) If it is possible, can someone provide an example function? If not, I am looking for a proof (or hints for a proof, or some intuition) *By increasing but concave in each argument I mean and , and similar for Edit: I made question more slightly general. I apologize if someone was already typing an answer. (It is more general because if this edited question is true, then so is the original)","f(x,y) ^* f [0,C] C>0 \frac{\partial f}{\partial x \partial y} >0 <0 x y \frac{\partial f}{\partial x} >0 \frac{\partial^2 f}{\partial x^2}<0 y","['multivariable-calculus', 'functions', 'derivatives', 'partial-derivative', 'examples-counterexamples']"
55,"Usefulness of the ""Lagrange function"" $\Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x)$","Usefulness of the ""Lagrange function""","\Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x)","Suppose that our surface $M\subset \Bbb R^n$ (or manifold) is defined to be the level set of a sufficiently nice function $\Phi:\Bbb R^n\to \Bbb R^m$ ( $m<n$ ), i.e. $M = \{\mathbf x\in\Bbb R^n: \Phi(\mathbf x) =\mathbf 0 \}$ . Let $f:\Bbb R^n\to\Bbb R$ be a $C^1$ function, we wish to solve the problem $$\begin{align}  \text{minimize}\quad &f(\mathbf x) \\ \text{subject to}\quad &\mathbf x\in M. \end{align}$$ It is well-known that if $\mathbf x^*\in M$ is a solution to the above problem, then there exists a vector $\mathbf \lambda^*\in\Bbb R^m$ (or a real number if $m=1$ ) such that $$\tag{*}\label{a} \nabla_{\mathbf x} f(\mathbf x^*) = \sum_{i=1}^m \lambda^*_i \nabla_{\mathbf x}\Phi_i(\mathbf x^*). $$ This is called the method of Lagrange multipliers . It is also very popular to rewrite the above using the auxiliary function called the Lagrange function $\Lambda:\Bbb R^{n+m}\to \Bbb R$ defined as $$ \Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x), $$ so that \eqref{a} and the constraint $\mathbf x^*\in M$ can be recasted as $$ \nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0,   $$ or written separately as $$ \partial_{ x_j} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0, \quad   \partial_{\lambda_i} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0 \quad\text{for all $j=1,\dots,n$ and $i=1,\dots,m$}. $$ I have always found the formulation $\nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0$ to be a bit unenlightening. To me it is nothing but a shorthand of writing \eqref{a} and the fact that $\mathbf x^*\in M$ together in a single line, and at times even obscures the important geometric meaning that \eqref{a} conveys (from my experience of being a TA on this subject). Hence my questions today: -Is there anything inherently interesting about the function $\Lambda(\mathbf x,\mathbf \lambda)$ itself (beside the fact that it's an auxiliary function for the Lagrange multiplier method)? -Is there any geometric (or otherwise) intuition that is naturally associated with $\Lambda(\mathbf x,\mathbf \lambda)$ ? -Is there a better answer I can give to my students when they ask ""What does $\Lambda(\mathbf x,\mathbf \lambda)$ represent?"" except ""It's just a shorthand for writing \eqref{a}.""? I'm hopeful that there's probably some good ways of interpreting $\Lambda(\mathbf x,\mathbf \lambda)$ that I'm not aware of yet. Personally, I've been trying to avoid using it altogether if I can because I'm not sure what to make of it. Today might be a good day to fix that.","Suppose that our surface (or manifold) is defined to be the level set of a sufficiently nice function ( ), i.e. . Let be a function, we wish to solve the problem It is well-known that if is a solution to the above problem, then there exists a vector (or a real number if ) such that This is called the method of Lagrange multipliers . It is also very popular to rewrite the above using the auxiliary function called the Lagrange function defined as so that \eqref{a} and the constraint can be recasted as or written separately as I have always found the formulation to be a bit unenlightening. To me it is nothing but a shorthand of writing \eqref{a} and the fact that together in a single line, and at times even obscures the important geometric meaning that \eqref{a} conveys (from my experience of being a TA on this subject). Hence my questions today: -Is there anything inherently interesting about the function itself (beside the fact that it's an auxiliary function for the Lagrange multiplier method)? -Is there any geometric (or otherwise) intuition that is naturally associated with ? -Is there a better answer I can give to my students when they ask ""What does represent?"" except ""It's just a shorthand for writing \eqref{a}.""? I'm hopeful that there's probably some good ways of interpreting that I'm not aware of yet. Personally, I've been trying to avoid using it altogether if I can because I'm not sure what to make of it. Today might be a good day to fix that.","M\subset \Bbb R^n \Phi:\Bbb R^n\to \Bbb R^m m<n M = \{\mathbf x\in\Bbb R^n: \Phi(\mathbf x) =\mathbf 0 \} f:\Bbb R^n\to\Bbb R C^1 \begin{align} 
\text{minimize}\quad &f(\mathbf x) \\
\text{subject to}\quad &\mathbf x\in M.
\end{align} \mathbf x^*\in M \mathbf \lambda^*\in\Bbb R^m m=1 \tag{*}\label{a}
\nabla_{\mathbf x} f(\mathbf x^*) = \sum_{i=1}^m \lambda^*_i \nabla_{\mathbf x}\Phi_i(\mathbf x^*).
 \Lambda:\Bbb R^{n+m}\to \Bbb R 
\Lambda(\mathbf x,\mathbf \lambda) = f(\mathbf x) - \sum_{i=1}^m \mathbf \lambda_i \Phi_i(\mathbf x),
 \mathbf x^*\in M 
\nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0,  
 
\partial_{ x_j} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0, \quad  
\partial_{\lambda_i} \Lambda(\mathbf x^*,\mathbf \lambda^*) = 0 \quad\text{for all j=1,\dots,n and i=1,\dots,m}.
 \nabla_{(\mathbf x,\mathbf \lambda)} \Lambda(\mathbf x^*,\mathbf \lambda^*) = \mathbf 0 \mathbf x^*\in M \Lambda(\mathbf x,\mathbf \lambda) \Lambda(\mathbf x,\mathbf \lambda) \Lambda(\mathbf x,\mathbf \lambda) \Lambda(\mathbf x,\mathbf \lambda)","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'intuition', 'lagrange-multiplier']"
56,"Proof verification: $\iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA$ using Green's theorem.",Proof verification:  using Green's theorem.,"\iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA","In James Stewart's Calculus: Early Transcendentals ( $8$ e), problem $31$ of Section $16.4$ asks us to prove the change of variables formula $$\iint_{R}dx\text{ }dy=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|du\text{ }dv$$ using Green's theorem. Here, $R$ is the image of $S$ under the transformation $T$ defined by the equations $x=g(u,v)$ , $y=h(u,v)$ . Here's my reasoning: Notice that $\iint_{R}dA$ is the area of the region $R$ . It can be shown that Green's theorem implies that this area is given by the line integral $\int_{\partial R}x\text{ }dy$ . Thus, if $\partial R$ can be parameterized by $\textbf{r}(t)=\left<x(t),y(t)\right>$ ( $a\leq t\leq b$ ), we can write \begin{align*} \iint_{R}dA &= \int_{\partial R}x\text{ }dy\\ &= \int_{a}^{b}x(t)y'(t)\text{ }dt \end{align*} It was assumed that the image of $S$ under $T$ is $R$ , so if $T$ preserves orientation, it must be the case that $T$ maps $\partial S$ to $\partial R$ . It follows that \begin{align*} \left<x(t),y(t)\right> &= \textbf{r}(t)\\ &= T\left(\textbf{r}_{S}(t)\right)\\ &= \left<g(\textbf{r}_{S}(t)),h(\textbf{r}_{S}(t))\right>\\ &= \left<g(u(t),v(t)),h(u(t),v(t))\right> \end{align*} for all $t\in[a,b]$ , where $\textbf{r}_{S}(t)=\left<u(t),v(t)\right>$ parameterizes $\partial S$ (the parameter domain doesn't change because $\partial S$ is a closed curve, and thus must return to $\textbf{r}_{S}(a)$ once $t$ reaches $b$ ). We deduce that \begin{align*} x(t) &= g(u(t),v(t))\\ y(t) &= h(u(t),v(t))\\ y'(t) &= h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t) \end{align*} and consequently \begin{align*} \int_{a}^{b}x(t)y'(t)\text{ }dt &= \int_{a}^{b}g(u(t),v(t))\left[h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t)\right]dt\\ &= \int_{a}^{b}[g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\\ & +g(u(t),v(t))h_{v}(u(t),v(t))v'(t)]dt\\ &= \int_{a}^{b}g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\text{ }dt\\ & +\int_{a}^{b}g(u(t),v(t))h_{v}(u(t),v(t))v'(t)\text{ }dt\\ &= \int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv \end{align*} Applying Green's theorem gives \begin{align*} \int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv &= \iint_{S}\left(\frac{\partial}{\partial u}\left(g(u,v)h_{v}(u,v)\right)-\frac{\partial}{\partial v}\left(g(u,v)h_{u}(u,v)\right)\right)dA\\ &= \iint_{S}(g_{u}(u,v)h_{v}(u,v)+g(u,v)h_{vu}(u,v)\\ & -g_{v}(u,v)h_{u}(u,v)-g(u,v)h_{uv}(u,v))\text{ }dA\\ &= \iint_{S}\left(g_{u}(u,v)h_{v}(u,v)-g_{v}(u,v)h_{u}(u,v)\right)dA \end{align*} The last expression is precisely $\left|\frac{\partial (x,y)}{\partial (u,v)}\right|$ (the orientation has not changed), so we may finally write $$\iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA$$ I appreciate any and all feedback. If you identify any mistakes, I kindly ask that you only give me hints so I can correct them myself. Extra : with a few tweaks, this argument can be used to prove the more general result $$\iint_{R}f(x,y)\text{ }dA=\iint_{S}f(g(u,v),h(u,v))\left|\frac{\partial (x,y)}{\partial (u,v)}\right|dA$$","In James Stewart's Calculus: Early Transcendentals ( e), problem of Section asks us to prove the change of variables formula using Green's theorem. Here, is the image of under the transformation defined by the equations , . Here's my reasoning: Notice that is the area of the region . It can be shown that Green's theorem implies that this area is given by the line integral . Thus, if can be parameterized by ( ), we can write It was assumed that the image of under is , so if preserves orientation, it must be the case that maps to . It follows that for all , where parameterizes (the parameter domain doesn't change because is a closed curve, and thus must return to once reaches ). We deduce that and consequently Applying Green's theorem gives The last expression is precisely (the orientation has not changed), so we may finally write I appreciate any and all feedback. If you identify any mistakes, I kindly ask that you only give me hints so I can correct them myself. Extra : with a few tweaks, this argument can be used to prove the more general result","8 31 16.4 \iint_{R}dx\text{ }dy=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|du\text{ }dv R S T x=g(u,v) y=h(u,v) \iint_{R}dA R \int_{\partial R}x\text{ }dy \partial R \textbf{r}(t)=\left<x(t),y(t)\right> a\leq t\leq b \begin{align*}
\iint_{R}dA &= \int_{\partial R}x\text{ }dy\\
&= \int_{a}^{b}x(t)y'(t)\text{ }dt
\end{align*} S T R T T \partial S \partial R \begin{align*}
\left<x(t),y(t)\right> &= \textbf{r}(t)\\
&= T\left(\textbf{r}_{S}(t)\right)\\
&= \left<g(\textbf{r}_{S}(t)),h(\textbf{r}_{S}(t))\right>\\
&= \left<g(u(t),v(t)),h(u(t),v(t))\right>
\end{align*} t\in[a,b] \textbf{r}_{S}(t)=\left<u(t),v(t)\right> \partial S \partial S \textbf{r}_{S}(a) t b \begin{align*}
x(t) &= g(u(t),v(t))\\
y(t) &= h(u(t),v(t))\\
y'(t) &= h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t)
\end{align*} \begin{align*}
\int_{a}^{b}x(t)y'(t)\text{ }dt &= \int_{a}^{b}g(u(t),v(t))\left[h_{u}(u(t),v(t))u'(t)+h_{v}(u(t),v(t))v'(t)\right]dt\\
&= \int_{a}^{b}[g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\\
& +g(u(t),v(t))h_{v}(u(t),v(t))v'(t)]dt\\
&= \int_{a}^{b}g(u(t),v(t))h_{u}(u(t),v(t))u'(t)\text{ }dt\\
& +\int_{a}^{b}g(u(t),v(t))h_{v}(u(t),v(t))v'(t)\text{ }dt\\
&= \int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv
\end{align*} \begin{align*}
\int_{\partial S}g(u,v)h_{u}(u,v)du+g(u,v)h_{v}(u,v)dv &= \iint_{S}\left(\frac{\partial}{\partial u}\left(g(u,v)h_{v}(u,v)\right)-\frac{\partial}{\partial v}\left(g(u,v)h_{u}(u,v)\right)\right)dA\\
&= \iint_{S}(g_{u}(u,v)h_{v}(u,v)+g(u,v)h_{vu}(u,v)\\
& -g_{v}(u,v)h_{u}(u,v)-g(u,v)h_{uv}(u,v))\text{ }dA\\
&= \iint_{S}\left(g_{u}(u,v)h_{v}(u,v)-g_{v}(u,v)h_{u}(u,v)\right)dA
\end{align*} \left|\frac{\partial (x,y)}{\partial (u,v)}\right| \iint_{R}dA=\iint_{S}\left|\frac{\partial(x,y)}{\partial(u,v)}\right|dA \iint_{R}f(x,y)\text{ }dA=\iint_{S}f(g(u,v),h(u,v))\left|\frac{\partial (x,y)}{\partial (u,v)}\right|dA","['multivariable-calculus', 'solution-verification', 'line-integrals', 'greens-theorem']"
57,How define the exterior derivative over a manifold with boundary?,How define the exterior derivative over a manifold with boundary?,,"What shown below is a reference from the text Analysis on Manifolds by James Munkres Definition Let $S$ be a subset of $\Bbb R^k$ ; let $f:S\rightarrow\Bbb R^n$ . We say that $f$ is of class $C^r$ on $S$ if $f$ may be extended to a function $g:U\rightarrow\Bbb R^n$ that is of class $C^r$ on an open set $U$ of $\Bbb R^k$ containing $S$ . Lemma Let $U$ be open in $H^k$ but not in $\Bbb R^k$ ; let $\alpha:U\rightarrow\Bbb R^k$ be of class $C^r$ . Let $\beta:U'\rightarrow\Bbb R^n$ and $\gamma:U''\rightarrow\Bbb R^n$ be two $C^r$ extensions of $\alpha$ defined on an open subsets $U'$ and $U''$ of $\Bbb R^k$ . Then the derivatives of $\beta$ and $\gamma$ are equal in $U$ and in $\overset{\circ} U$ are equal to the derivative of $\alpha$ . Definition Given $x\in\Bbb R^n$ we define a tangent vector to $\Bbb R^n$ at $x$ to be a pair $(x;\vec v)$ where $\vec v\in\Bbb R^n$ . The set of all tangent vectors to $\Bbb R^n$ at $x$ forms a vector space if we define $$ (x;\vec v)+(x;\vec w):=(x;\vec v+\vec w)\\ c(x;\vec v):=(x;c\vec v) $$ It is called the tangent space to $\Bbb R^n$ at $x$ and it is denoted $\mathcal T_x(\Bbb R^n)$ . Definition Let be $A$ open in $\Bbb R^k$ or in $H^k$ ; let $\alpha: A\rightarrow\Bbb R^n$ be of class $C^r$ . Let $x\in A$ , and let $p=\alpha(x)$ . We define a linear transformation $$ \alpha_*\mathcal T_x(\Bbb R^k)\rightarrow\mathcal T_p(\Bbb R^n) $$ by the equation $$ \alpha_*(x;\vec v):=\big(p;D\alpha(x)\cdot\vec v\big) $$ and we call it pushforward of $\alpha$ . Definition Let $M$ be a $k$ -manifold of class $C^r$ in $\Bbb R^n$ . If $p\in M$ chose a coordinate patch $\alpha:U\rightarrow V$ about $p$ , where $U$ is open in $\Bbb R^k$ or $H^k$ . Let $x$ be the point of $U$ such that $\alpha(x)=p$ . The set of all vectors of the form $\alpha_*(x;\vec v)$ where $\vec v$ is a vector in $\Bbb R^k$ is called the tangent space to $M$ at $p$ and it is denoted $\mathcal T_p(M)$ . Said differently $$ \mathcal T_p(M):=\alpha_*\big(\mathcal T_x(\Bbb R^k)\big) $$ Definition Let $A$ be an open set $\Bbb R^n$ . A $k$ -tensor field in $A$ is a function $\omega$ assigning to each $x\in A$ a $k$ -tensor defined on the vector space $\mathcal T_x(\Bbb R^n)$ . That is, $$ \omega(x)\in\mathcal L^k\big(\mathcal T_x(\Bbb R^n)\big) $$ for each $x\in A$ . Thus $\omega(x)$ is a function mapping $k$ -tuples of tangent vectors to $\Bbb R^n$ at $x$ into $\Bbb R$ ; as such, its value on a given $k$ -tuple can be written in the form $$ \big[\omega(x)\big]\big((x;\vec v_1),...,(x;\vec v_k)\big) $$ We require this function to be continuous as a function of $(x,v_1,..,v_k)$ ; if it is of class $C^r$ we say that $\omega$ is a tensor field of class $C^r$ . If it happens that $\omega(x)$ is an alternanting $k$ -tensor for each $x$ , then $\omega$ is called a differential form of order $k$ on $A$ . More generally, if $M$ is an $m$ -manifold in $\Bbb R^n$ then we define a $k$ -tensor field on $M$ to be a function $\omega$ assigning to each $p\in M$ an element of $\mathcal L^k\big(\mathcal T_p(M)\big)$ . If in fact $\omega(p)$ is alternanting for each $p$ then $\omega$ is called a differential form on $M$ . In particular any tensor field on $M$ can be extended to a tensor field defined on an open set of $\Bbb R^n$ containing $M$ but the proof is decidedly non-trivial. Definition Let $A$ be open in $\Bbb R^k$ ; let $\alpha:A\rightarrow\Bbb R^n$ be of class $C^\infty$ ; let $B$ be an open set of $\Bbb R^n$ containing $\alpha[A]$ . We define the pull-back $\alpha^*$ of $\alpha$ to be a linear transformation $$ \alpha^*:\Omega^l(B)\rightarrow\Omega^l(A) $$ such that if $f$ is a zero form on $B$ then $$ [\alpha^*f](x):=f\big(\alpha(x)\big) $$ for each $x\in A$ and if $\omega$ is a $l$ -formon $B$ with $l>0$ then $$ \big[(\alpha^*\omega)(x)\big]\big((x;\vec v_1),...,(x;\vec v_l)\big):=\big[\omega\big(\alpha(x)\big)\big]\big(\alpha_*(x;\vec v_1),...,\alpha_*(x;\vec v_l)\big) $$ for each $x\in A$ and for each $\vec v_1,..,\vec v_k\in\Bbb R^k$ . Now having defined the differential operator $d$ for forms defined over an open set let's go to define it for forms defined over a manifold. So previously we observe that the pushforward of a coordinate patch is a injective function and thus if $\omega$ is a $l$ -form defined over a $k$ -manifold we define a $(l+1)$ -form $d\omega$ by letting that $$ \big[d\omega(y)\big]\big((y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\big):=\Big[d(\alpha^*\omega)\big(\alpha^{-1}(y)\big)\Big]\big((\alpha_*)^{-1}(y;\vec v_1),...,(\alpha_*)^{-1}(y;\vec v_l),(\alpha_*)^{-1}(y;\vec v_{l+1})\big) $$ for any $y\in M$ and for any $\vec v_1,..,\vec v_k,\vec v_{l+1}\in\Bbb R^n$ such that $(y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\in\mathcal T_y(M)$ . So since the domain of a coordinate patch of an interior point is a open subset of $\Bbb R^k$ the above position is consistent with what is above defined; anyway if $y$ is a boundary poin of $M$ then the domain of any coordinate patch about $y$ is not open in $\Bbb R^k$ so that the pull-back of this cordinate system is not defined and moreover if it was defined apparently it would carry a differential forms defined over a manifold to differetial forms defined over an open subset in $H^k$ that is not open in $\Bbb R^k$ so that the form $d(\alpha^*\omega)$ would not be defined. So could someone help me, please?","What shown below is a reference from the text Analysis on Manifolds by James Munkres Definition Let be a subset of ; let . We say that is of class on if may be extended to a function that is of class on an open set of containing . Lemma Let be open in but not in ; let be of class . Let and be two extensions of defined on an open subsets and of . Then the derivatives of and are equal in and in are equal to the derivative of . Definition Given we define a tangent vector to at to be a pair where . The set of all tangent vectors to at forms a vector space if we define It is called the tangent space to at and it is denoted . Definition Let be open in or in ; let be of class . Let , and let . We define a linear transformation by the equation and we call it pushforward of . Definition Let be a -manifold of class in . If chose a coordinate patch about , where is open in or . Let be the point of such that . The set of all vectors of the form where is a vector in is called the tangent space to at and it is denoted . Said differently Definition Let be an open set . A -tensor field in is a function assigning to each a -tensor defined on the vector space . That is, for each . Thus is a function mapping -tuples of tangent vectors to at into ; as such, its value on a given -tuple can be written in the form We require this function to be continuous as a function of ; if it is of class we say that is a tensor field of class . If it happens that is an alternanting -tensor for each , then is called a differential form of order on . More generally, if is an -manifold in then we define a -tensor field on to be a function assigning to each an element of . If in fact is alternanting for each then is called a differential form on . In particular any tensor field on can be extended to a tensor field defined on an open set of containing but the proof is decidedly non-trivial. Definition Let be open in ; let be of class ; let be an open set of containing . We define the pull-back of to be a linear transformation such that if is a zero form on then for each and if is a -formon with then for each and for each . Now having defined the differential operator for forms defined over an open set let's go to define it for forms defined over a manifold. So previously we observe that the pushforward of a coordinate patch is a injective function and thus if is a -form defined over a -manifold we define a -form by letting that for any and for any such that . So since the domain of a coordinate patch of an interior point is a open subset of the above position is consistent with what is above defined; anyway if is a boundary poin of then the domain of any coordinate patch about is not open in so that the pull-back of this cordinate system is not defined and moreover if it was defined apparently it would carry a differential forms defined over a manifold to differetial forms defined over an open subset in that is not open in so that the form would not be defined. So could someone help me, please?","S \Bbb R^k f:S\rightarrow\Bbb R^n f C^r S f g:U\rightarrow\Bbb R^n C^r U \Bbb R^k S U H^k \Bbb R^k \alpha:U\rightarrow\Bbb R^k C^r \beta:U'\rightarrow\Bbb R^n \gamma:U''\rightarrow\Bbb R^n C^r \alpha U' U'' \Bbb R^k \beta \gamma U \overset{\circ} U \alpha x\in\Bbb R^n \Bbb R^n x (x;\vec v) \vec v\in\Bbb R^n \Bbb R^n x 
(x;\vec v)+(x;\vec w):=(x;\vec v+\vec w)\\
c(x;\vec v):=(x;c\vec v)
 \Bbb R^n x \mathcal T_x(\Bbb R^n) A \Bbb R^k H^k \alpha: A\rightarrow\Bbb R^n C^r x\in A p=\alpha(x) 
\alpha_*\mathcal T_x(\Bbb R^k)\rightarrow\mathcal T_p(\Bbb R^n)
 
\alpha_*(x;\vec v):=\big(p;D\alpha(x)\cdot\vec v\big)
 \alpha M k C^r \Bbb R^n p\in M \alpha:U\rightarrow V p U \Bbb R^k H^k x U \alpha(x)=p \alpha_*(x;\vec v) \vec v \Bbb R^k M p \mathcal T_p(M) 
\mathcal T_p(M):=\alpha_*\big(\mathcal T_x(\Bbb R^k)\big)
 A \Bbb R^n k A \omega x\in A k \mathcal T_x(\Bbb R^n) 
\omega(x)\in\mathcal L^k\big(\mathcal T_x(\Bbb R^n)\big)
 x\in A \omega(x) k \Bbb R^n x \Bbb R k 
\big[\omega(x)\big]\big((x;\vec v_1),...,(x;\vec v_k)\big)
 (x,v_1,..,v_k) C^r \omega C^r \omega(x) k x \omega k A M m \Bbb R^n k M \omega p\in M \mathcal L^k\big(\mathcal T_p(M)\big) \omega(p) p \omega M M \Bbb R^n M A \Bbb R^k \alpha:A\rightarrow\Bbb R^n C^\infty B \Bbb R^n \alpha[A] \alpha^* \alpha 
\alpha^*:\Omega^l(B)\rightarrow\Omega^l(A)
 f B 
[\alpha^*f](x):=f\big(\alpha(x)\big)
 x\in A \omega l B l>0 
\big[(\alpha^*\omega)(x)\big]\big((x;\vec v_1),...,(x;\vec v_l)\big):=\big[\omega\big(\alpha(x)\big)\big]\big(\alpha_*(x;\vec v_1),...,\alpha_*(x;\vec v_l)\big)
 x\in A \vec v_1,..,\vec v_k\in\Bbb R^k d \omega l k (l+1) d\omega 
\big[d\omega(y)\big]\big((y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\big):=\Big[d(\alpha^*\omega)\big(\alpha^{-1}(y)\big)\Big]\big((\alpha_*)^{-1}(y;\vec v_1),...,(\alpha_*)^{-1}(y;\vec v_l),(\alpha_*)^{-1}(y;\vec v_{l+1})\big)
 y\in M \vec v_1,..,\vec v_k,\vec v_{l+1}\in\Bbb R^n (y;\vec v_1),...,(y;\vec v_l),(y;\vec v_{l+1})\in\mathcal T_y(M) \Bbb R^k y M y \Bbb R^k H^k \Bbb R^k d(\alpha^*\omega)","['multivariable-calculus', 'differential-geometry', 'differential-forms', 'manifolds-with-boundary', 'exterior-derivative']"
58,Proving Beltrami-Enneper theorem,Proving Beltrami-Enneper theorem,,"Preamble I am trying to prove the Beltrami-Enneper theorem, which states that, given an assymptotic curve $\alpha$ in a surface $S$ that has non-zero curvature everywhere obeys that: $|\tau| = \sqrt{-K}$ Where $\tau$ is the torsion and $K$ is the Gaussian curvature. The conventions in the notation are $N$ is the normal to the surface $T_\alpha, N_\alpha, B_\alpha$ are thr tangent normal and binormal to the curve. $k_1, k_2$ are the principal curvatures of the surface at the evaluated point. $k$ is used to denote curvature. So far I have this: My Work The curve $\alpha$ is an assymptotic curve, which means that at $P \in \alpha$ the normal curvature is 0, so: $\langle N, kN_\alpha \rangle  = 0$ $N$ is a unit length vector orthogonal to $T_\alpha$ and $N_\alpha$ the tangent and normal vectors to the curve, so: $N = T_\alpha \times N_\alpha = B_\alpha \iff N' = \tau N_\alpha$ . So $N'\cdot N' = \tau^2$ . By Euler's theorem: $k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ Cheat This is where I am getting stuck, I took a sneak peak at the solution and it states that: $|N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ Where $\theta$ is the angle between the tangent to the curve and $e_1$ the first principal direction. I don't know how that above relationship was derived, I am looking for the proof that indeed: $|N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ As I failed to derive it on my own. Solution (with doubt) I managed to finish the proof by leveraging something I saw in the proof for Euler's theorem: By Euler's theorem: $k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ . But more importantly: $dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta)$ So $N'\cdot N' = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta)$ By the fact the curve is assymptotic we get $N\cdot N_\alpha = 0 = k_1\cos^2(\theta) + k_2\sin^2(\theta)$ So $\cos^2(\theta) = -k_2\sin^2(\theta) / k_1$ and $\sin^2(\theta) = -k_1\cos^2(\theta) / k_2$ . And thus: $N'\cdot N' = k_1^2\frac{-k_2\sin^2(\theta)}{k_1} + k_2^2\frac{k_1\cos^2(\theta)}{k_2} = k_1k_2\sin^2(\theta) + -k_1k_2\cos^2(\theta) = -k_1k_2$ So $\tau^2 = -k_1k_2$ I am still however not satisfied with my work, because I have no idea why this holds: $$dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta)$$","Preamble I am trying to prove the Beltrami-Enneper theorem, which states that, given an assymptotic curve in a surface that has non-zero curvature everywhere obeys that: Where is the torsion and is the Gaussian curvature. The conventions in the notation are is the normal to the surface are thr tangent normal and binormal to the curve. are the principal curvatures of the surface at the evaluated point. is used to denote curvature. So far I have this: My Work The curve is an assymptotic curve, which means that at the normal curvature is 0, so: is a unit length vector orthogonal to and the tangent and normal vectors to the curve, so: . So . By Euler's theorem: Cheat This is where I am getting stuck, I took a sneak peak at the solution and it states that: Where is the angle between the tangent to the curve and the first principal direction. I don't know how that above relationship was derived, I am looking for the proof that indeed: As I failed to derive it on my own. Solution (with doubt) I managed to finish the proof by leveraging something I saw in the proof for Euler's theorem: By Euler's theorem: . But more importantly: So By the fact the curve is assymptotic we get So and . And thus: So I am still however not satisfied with my work, because I have no idea why this holds:","\alpha S |\tau| = \sqrt{-K} \tau K N T_\alpha, N_\alpha, B_\alpha k_1, k_2 k \alpha P \in \alpha \langle N, kN_\alpha \rangle  = 0 N T_\alpha N_\alpha N = T_\alpha \times N_\alpha = B_\alpha \iff N' = \tau N_\alpha N'\cdot N' = \tau^2 k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta) |N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta) \theta e_1 |N'^2| = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta) k_n = \langle dN_p(v), v \rangle = k_1\cos^2(\theta) + k_2\sin^2(\theta) dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta) N'\cdot N' = k_1^2\cos^2(\theta) + k_2^2\sin^2(\theta) N\cdot N_\alpha = 0 = k_1\cos^2(\theta) + k_2\sin^2(\theta) \cos^2(\theta) = -k_2\sin^2(\theta) / k_1 \sin^2(\theta) = -k_1\cos^2(\theta) / k_2 N'\cdot N' = k_1^2\frac{-k_2\sin^2(\theta)}{k_1} + k_2^2\frac{k_1\cos^2(\theta)}{k_2} = k_1k_2\sin^2(\theta) + -k_1k_2\cos^2(\theta) = -k_1k_2 \tau^2 = -k_1k_2 dN_p(v) = k_1e_1\cos(\theta) + k_2e_2\sin(\theta)","['calculus', 'geometry', 'multivariable-calculus', 'differential-geometry']"
59,How would I find the volume of a torus with an elliptical cross-section using calculus?,How would I find the volume of a torus with an elliptical cross-section using calculus?,,"I know that, by Pappus' centroid theorem, the volume of a torus with an elliptic cross-section of major axis $a$ and minor axis $b$ is $2𝜋^2abc$ , where $c$ is the distance from the center of the torus to the center of the elliptic cross-section. My doubt is how I would prove this using calculus? What I have gotten so far is the parametric equations of the $x$ , $y$ , and $z$ co-ordinates of all the points in the elliptic torus, with the parameters being $u$ and $v$ ( $v$ is the angle between the $a$ point and the $x$ or $y$ axes, and $u$ is the angle at which the cross-section has been rotated). It is: $$ x(u, v) =  (c + a\cos v)\cos u$$ $$ y(u, v) = (c + a\cos v)\sin u$$ $$ z(u, v) = b\sin v$$ This is also what I get from this source . Using these parametric equations how can I derive the formula for the volume of a torus with an elliptic cross-section? I've seen many different things online such as using the Jacobian matrix but I am not too sure how to move on from here. Please help! I'm a high school student so I don't know much.","I know that, by Pappus' centroid theorem, the volume of a torus with an elliptic cross-section of major axis and minor axis is , where is the distance from the center of the torus to the center of the elliptic cross-section. My doubt is how I would prove this using calculus? What I have gotten so far is the parametric equations of the , , and co-ordinates of all the points in the elliptic torus, with the parameters being and ( is the angle between the point and the or axes, and is the angle at which the cross-section has been rotated). It is: This is also what I get from this source . Using these parametric equations how can I derive the formula for the volume of a torus with an elliptic cross-section? I've seen many different things online such as using the Jacobian matrix but I am not too sure how to move on from here. Please help! I'm a high school student so I don't know much.","a b 2𝜋^2abc c x y z u v v a x y u 
x(u, v) =  (c + a\cos v)\cos u 
y(u, v) = (c + a\cos v)\sin u 
z(u, v) = b\sin v","['calculus', 'integration', 'geometry', 'multivariable-calculus', 'differential-geometry']"
60,Divergence and curl of field proportional to itself.,Divergence and curl of field proportional to itself.,,"I have a system of equations that can be put in the form, $\begin{align}\nabla\cdot \mathbf{n}=\mathbf{T}\cdot  \mathbf{n}\end{align}\\\nabla\times \mathbf{n}=\mathbf{T}\times  \mathbf{n},$ with $\mathbf{T}$ a vector such that $\nabla\cdot \mathbf{T}=0,\: \mathbf{T}=(f(y),g(x),0)$ , and $\mathbf{n}$ has to lie on the plane, that is, $\mathbf{n}=(n_x(x,y),n_y(x,y),0)$ . Also some regular data on a closed curve on the plane can be provided. The question is how to find $\mathbf{n}$ ? In terms of $\mathbf{T}$ , course. The system is well posed, so it has solution, but so far that's all I can say. I'd like to have a explicit integral. The most similar problem that I've seen is that of $\nabla\times f=f$ , but here I'm much more constrained. Thanks in advance,","I have a system of equations that can be put in the form, with a vector such that , and has to lie on the plane, that is, . Also some regular data on a closed curve on the plane can be provided. The question is how to find ? In terms of , course. The system is well posed, so it has solution, but so far that's all I can say. I'd like to have a explicit integral. The most similar problem that I've seen is that of , but here I'm much more constrained. Thanks in advance,","\begin{align}\nabla\cdot \mathbf{n}=\mathbf{T}\cdot  \mathbf{n}\end{align}\\\nabla\times \mathbf{n}=\mathbf{T}\times  \mathbf{n}, \mathbf{T} \nabla\cdot \mathbf{T}=0,\: \mathbf{T}=(f(y),g(x),0) \mathbf{n} \mathbf{n}=(n_x(x,y),n_y(x,y),0) \mathbf{n} \mathbf{T} \nabla\times f=f","['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'vector-analysis', 'vector-fields']"
61,Refinement of $\left(\frac{a+1}{a+b} \right)^{\frac25}+\left(\frac{b+1}{b+c} \right)^{\frac25}+\left(\frac{c+1}{c+a} \right)^{\frac25} \geqslant 3$,Refinement of,\left(\frac{a+1}{a+b} \right)^{\frac25}+\left(\frac{b+1}{b+c} \right)^{\frac25}+\left(\frac{c+1}{c+a} \right)^{\frac25} \geqslant 3,"It's a refinement of Prove $\left(\frac{a+1}{a+b} \right)^{\frac25}+\left(\frac{b+1}{b+c} \right)^{\frac25}+\left(\frac{c+1}{c+a} \right)^{\frac25} \geqslant 3$ . It's a attempt to find a simple proof of this fact . Now the refinement : Let $1\leq a\leq 2$ and $x\in[0,3-a]$ then define : $$f(x,a)= \left(\frac{\frac{(1+a)}{(x+a)}+\frac{(x+1)}{(3-x-a+x)}+\frac{(3-x-a+1)}{(3-x+a-a)}}{3}\right)^{\frac{2}{5}}$$ And : $$g(x,a)= \left(\frac{\frac{(x+a)}{(1+a)}+\frac{(3-x-a+x)}{(1+x)}+\frac{(3-x-a+a)}{(3-x+1-a)}}{3}\right)^{-\frac{2}{5}}$$ then we have : $$\left(\frac{(1+a)}{(x+a)} \right)^{\frac25}+\left(\frac{(x+1)}{(3-x-a+x)} \right)^{\frac25}+\left(\frac{(3-x-a+1)}{(3-x+a-a)}\right)^{\frac25} \geqslant\frac{195}{100}f(x,a)+\frac{105}{100}g(x,a) \geqslant 3$$ My refinement is based on two observation : With Jensen's inequality we have $x,y,z>0$ : $$3\left(\frac{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}{3}\right)^{-\frac{2}{5}}\leq x^{\frac{2}{5}}+y^{\frac{2}{5}}+z^{\frac{2}{5}}\leq 3\left(\frac{x+y+z}{3}\right)^{\frac{2}{5}}$$ For the rest I play with the coefficients to give the refinement. For the RHS we put : $$u=\frac{(1+a)}{(x+a)}$$ We make the same things for the others fractions and I think we can prove a more general inequality for $u,v,w$ . For the LHS I have tried to put on the same denominator but currently I don't see any good issue . Update 13/12/2020: We want to show a more general inequality with some constraints on it ,so we have the inequality : $$x^{\frac{2}{5}}+y^{\frac{2}{5}}+z^{\frac{2}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{x+y+z}{3}\right)^{\frac{2}{5}}\quad(1)$$ Due to homogeneity we can put $u=\frac{x}{z}$ and $v=\frac{y}{z}$ and get : $$1+u^{\frac{2}{5}}+v^{\frac{2}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{u}+\frac{1}{v}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+u+v}{3}\right)^{\frac{2}{5}}\quad(I)$$ Now we make the subsitution $u=p^3$ and $v=q^3$ we get : $$1+p^{\frac{6}{5}}+q^{\frac{6}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{p^3}+\frac{1}{q^3}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+p^3+q^3}{3}\right)^{\frac{2}{5}}$$ Now we use Jensen's inequality we have : $$p^{\frac{6}{5}}+q^{\frac{6}{5}}\geq 2\left(\frac{p+q}{2}\right)^{\frac{6}{5}}$$ Remains to show the new bound : $$1+2\left(\frac{p+q}{2}\right)^{\frac{6}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{p^3}+\frac{1}{q^3}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+p^3+q^3}{3}\right)^{\frac{2}{5}}$$ And now I'm stuck ...My last idea was to use the Gauss identity wich states for reals $a,b,c$ : $$a^3+b^3+c^3-3abc=(a+b+c)(a^2+b^2+c^2-ab-bc-ca)$$ Without success ! So I cannot find currently constraints on $x,y,z$ to have inequality $(1)$ For information we have the 2 variables version of the inequality ( $x>0$ ): $$\frac{2}{3}\left(\left(\frac{195}{100}\right) \left(\frac{1+x}{2}\right)^{\frac{2}{5}}+\frac{105}{100} \left(\frac{1+\frac{1}{x}}{2}\right)^{\frac{-2}{5}}\right)\leq 1+x^{\frac{2}{5}}$$ update : 14/12/2020 Starting from $(I)$ we can use Ji Chen's lemma to give some constraints . To works we need to split the coefficient $\frac{195}{100}$ into $1+\frac{95}{100}$ to have three variables . Finally I have tried for the LHS $uvw's$ method to find some others constraints . Question : How to show the refinement ?Is it also true for $2\leq a<3$ ? Thanks !","It's a refinement of Prove $\left(\frac{a+1}{a+b} \right)^{\frac25}+\left(\frac{b+1}{b+c} \right)^{\frac25}+\left(\frac{c+1}{c+a} \right)^{\frac25} \geqslant 3$ . It's a attempt to find a simple proof of this fact . Now the refinement : Let and then define : And : then we have : My refinement is based on two observation : With Jensen's inequality we have : For the rest I play with the coefficients to give the refinement. For the RHS we put : We make the same things for the others fractions and I think we can prove a more general inequality for . For the LHS I have tried to put on the same denominator but currently I don't see any good issue . Update 13/12/2020: We want to show a more general inequality with some constraints on it ,so we have the inequality : Due to homogeneity we can put and and get : Now we make the subsitution and we get : Now we use Jensen's inequality we have : Remains to show the new bound : And now I'm stuck ...My last idea was to use the Gauss identity wich states for reals : Without success ! So I cannot find currently constraints on to have inequality For information we have the 2 variables version of the inequality ( ): update : 14/12/2020 Starting from we can use Ji Chen's lemma to give some constraints . To works we need to split the coefficient into to have three variables . Finally I have tried for the LHS method to find some others constraints . Question : How to show the refinement ?Is it also true for ? Thanks !","1\leq a\leq 2 x\in[0,3-a] f(x,a)= \left(\frac{\frac{(1+a)}{(x+a)}+\frac{(x+1)}{(3-x-a+x)}+\frac{(3-x-a+1)}{(3-x+a-a)}}{3}\right)^{\frac{2}{5}} g(x,a)= \left(\frac{\frac{(x+a)}{(1+a)}+\frac{(3-x-a+x)}{(1+x)}+\frac{(3-x-a+a)}{(3-x+1-a)}}{3}\right)^{-\frac{2}{5}} \left(\frac{(1+a)}{(x+a)} \right)^{\frac25}+\left(\frac{(x+1)}{(3-x-a+x)} \right)^{\frac25}+\left(\frac{(3-x-a+1)}{(3-x+a-a)}\right)^{\frac25} \geqslant\frac{195}{100}f(x,a)+\frac{105}{100}g(x,a) \geqslant 3 x,y,z>0 3\left(\frac{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}{3}\right)^{-\frac{2}{5}}\leq x^{\frac{2}{5}}+y^{\frac{2}{5}}+z^{\frac{2}{5}}\leq 3\left(\frac{x+y+z}{3}\right)^{\frac{2}{5}} u=\frac{(1+a)}{(x+a)} u,v,w x^{\frac{2}{5}}+y^{\frac{2}{5}}+z^{\frac{2}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{x}+\frac{1}{y}+\frac{1}{z}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{x+y+z}{3}\right)^{\frac{2}{5}}\quad(1) u=\frac{x}{z} v=\frac{y}{z} 1+u^{\frac{2}{5}}+v^{\frac{2}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{u}+\frac{1}{v}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+u+v}{3}\right)^{\frac{2}{5}}\quad(I) u=p^3 v=q^3 1+p^{\frac{6}{5}}+q^{\frac{6}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{p^3}+\frac{1}{q^3}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+p^3+q^3}{3}\right)^{\frac{2}{5}} p^{\frac{6}{5}}+q^{\frac{6}{5}}\geq 2\left(\frac{p+q}{2}\right)^{\frac{6}{5}} 1+2\left(\frac{p+q}{2}\right)^{\frac{6}{5}}\geq \frac{105}{100}\left(\frac{\frac{1}{1}+\frac{1}{p^3}+\frac{1}{q^3}}{3}\right)^{-\frac{2}{5}}+\frac{195}{100}\left(\frac{1+p^3+q^3}{3}\right)^{\frac{2}{5}} a,b,c a^3+b^3+c^3-3abc=(a+b+c)(a^2+b^2+c^2-ab-bc-ca) x,y,z (1) x>0 \frac{2}{3}\left(\left(\frac{195}{100}\right) \left(\frac{1+x}{2}\right)^{\frac{2}{5}}+\frac{105}{100} \left(\frac{1+\frac{1}{x}}{2}\right)^{\frac{-2}{5}}\right)\leq 1+x^{\frac{2}{5}} (I) \frac{195}{100} 1+\frac{95}{100} uvw's 2\leq a<3","['multivariable-calculus', 'inequality', 'substitution', 'jensen-inequality']"
62,Differentiable convex function that is not continuously differentiable,Differentiable convex function that is not continuously differentiable,,"Does there exist a convex function that is differentiable over an open convex set but not continuously differentiable there? Any differentiable function defined on an interval is continuously differentiable due to the monotonicity and Darboux property of its derivative. Therefore, the function, if exists, has to reside in a $2$ - or higher-dimensional space. In addition, it needs to be continuously differentiable along any straight line. Any comments or criticism will be appreciated. Thanks. Update : Indeed, there does not exist such a function. See [Corollary 25.5.1, Convex Analysis , Rockafellar 1970].","Does there exist a convex function that is differentiable over an open convex set but not continuously differentiable there? Any differentiable function defined on an interval is continuously differentiable due to the monotonicity and Darboux property of its derivative. Therefore, the function, if exists, has to reside in a - or higher-dimensional space. In addition, it needs to be continuously differentiable along any straight line. Any comments or criticism will be appreciated. Thanks. Update : Indeed, there does not exist such a function. See [Corollary 25.5.1, Convex Analysis , Rockafellar 1970].",2,"['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'convex-analysis']"
63,Chain rule when taking non-dimensionalising an ODE,Chain rule when taking non-dimensionalising an ODE,,"I have a silly question. So let's say we have: $$\frac{d^{2}x}{dt^{2}} = kx$$ Now let's say we pick $X = \frac{x}{x_{c}}$ and $T = \frac{t}{t_{c}}$ . What I don't understand is, if we plug in $Xx_{c}$ for x, and $Tt_{c}$ for t, how come we get: $$\frac{x_{c}d^{2}X}{t_{c}^{2}dT^{2}}$$ How come the $t_{c}$ is squared? Can someone do the math and explain it to me?  Our professo r just said. it was. chain rule but I am not sure how","I have a silly question. So let's say we have: Now let's say we pick and . What I don't understand is, if we plug in for x, and for t, how come we get: How come the is squared? Can someone do the math and explain it to me?  Our professo r just said. it was. chain rule but I am not sure how",\frac{d^{2}x}{dt^{2}} = kx X = \frac{x}{x_{c}} T = \frac{t}{t_{c}} Xx_{c} Tt_{c} \frac{x_{c}d^{2}X}{t_{c}^{2}dT^{2}} t_{c},"['calculus', 'multivariable-calculus']"
64,Changing variable in partial derivative's,Changing variable in partial derivative's,,"I have $u\colon:\mathbb{R}^n\times(0,\infty)\to\mathbb{R}$ smooth, $u=u(x,y)$ . Consider the following changing of variables: $$ z=\biggl(\frac{y}{1-a}\biggr)^{1-a},$$ where $a\in(-1,1)$ , i want to prove that: \begin{equation} y^au_y=u_z. \end{equation} I have found this change of variables in link . I tried to apply the chain rule but without success. Any help would be appreciated.","I have smooth, . Consider the following changing of variables: where , i want to prove that: I have found this change of variables in link . I tried to apply the chain rule but without success. Any help would be appreciated.","u\colon:\mathbb{R}^n\times(0,\infty)\to\mathbb{R} u=u(x,y)  z=\biggl(\frac{y}{1-a}\biggr)^{1-a}, a\in(-1,1) \begin{equation}
y^au_y=u_z.
\end{equation}","['multivariable-calculus', 'partial-derivative', 'change-of-variable']"
65,Surface Integral - Better Parametrization?,Surface Integral - Better Parametrization?,,"So I'm supposed to calculate $$ \iint_S (x^2+y^2)dS $$ Where $S$ Is the part of the plane $z=2x+2y-1$ which is inside the paraboloid $z=x^2 +y^2$ . The way I proceeded was to parametrize $$ S: \begin{cases} x=1+r\cos(\theta),\\  y=1+r\sin(\theta),\\ z=3+2r(\cos(\theta)+\sin(\theta)) \end{cases} $$ where $\theta \in [0,2\pi)$ and, since the circunference projected at the XY plane is centered at (1,1), we have $r \in [\sqrt{2}-1,\sqrt{3+2(\cos(\theta)+\sin(\theta))}]$ Luckly, findind $||\vec{r}_{\theta}\times \vec{r}_r||$ wasn't so hard, and I found it to be $3r$ . The problem is the integral after that becomes painfully complicated (at least for me). I suspect there is a better solution/parametrization. Can somebody help me out? Also, I know there is a way to do it using Stokes theorem, but I'm not suposed to use it on this one.","So I'm supposed to calculate Where Is the part of the plane which is inside the paraboloid . The way I proceeded was to parametrize where and, since the circunference projected at the XY plane is centered at (1,1), we have Luckly, findind wasn't so hard, and I found it to be . The problem is the integral after that becomes painfully complicated (at least for me). I suspect there is a better solution/parametrization. Can somebody help me out? Also, I know there is a way to do it using Stokes theorem, but I'm not suposed to use it on this one.","
\iint_S (x^2+y^2)dS
 S z=2x+2y-1 z=x^2 +y^2 
S:
\begin{cases}
x=1+r\cos(\theta),\\
 y=1+r\sin(\theta),\\
z=3+2r(\cos(\theta)+\sin(\theta))
\end{cases}
 \theta \in [0,2\pi) r \in [\sqrt{2}-1,\sqrt{3+2(\cos(\theta)+\sin(\theta))}] ||\vec{r}_{\theta}\times \vec{r}_r|| 3r","['multivariable-calculus', 'surface-integrals', 'parametrization']"
66,Proving the cross product matrix tranformation identity with an alternative solution,Proving the cross product matrix tranformation identity with an alternative solution,,"I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let $N$ be the normal vector to a surface at a point $P$ , and let $S$ and $T$ be tangent vectors at the point $P$ such that $S \times T = N$ . Given an invertible 3 $\times$ 3 matrix $M$ , show that $(MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T)$ , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix $M$ . The author provided a hint stating we can represent $(MS) \times (MT)$ as $$ (MS) \times (MT) = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} MT $$ We then find a matrix $G$ such that $$ GU = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} M $$ where $$ U = \begin{bmatrix} 0 & -S_{z} & S_{y} \\  S_{z} & 0 & -S_{x} \\  -S_{y} & S_{x} & 0  \end{bmatrix} $$ and show that $G = (\text{det}M)(M^{-1})^{T}$ to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: $$ G = \begin{bmatrix} 0 & -(MS)_{z} & (MS)_{y} \\  (MS)_{z} & 0 & -(MS)_{x} \\  -(MS)_{y} & (MS)_{x} & 0  \end{bmatrix} M U^{-1} $$ At this point, I do not know how to proceed with showing that $G = (\text{det}M)(M^{-1})^{T}$ . How would you proceed? I'd like to ask for hints on solving the problem.","I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let be the normal vector to a surface at a point , and let and be tangent vectors at the point such that . Given an invertible 3 3 matrix , show that , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix . The author provided a hint stating we can represent as We then find a matrix such that where and show that to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: At this point, I do not know how to proceed with showing that . How would you proceed? I'd like to ask for hints on solving the problem.","N P S T P S \times T = N \times M (MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T) M (MS) \times (MT) 
(MS) \times (MT) =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
MT
 G 
GU
=
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
 
U =
\begin{bmatrix}
0 & -S_{z} & S_{y} \\ 
S_{z} & 0 & -S_{x} \\ 
-S_{y} & S_{x} & 0 
\end{bmatrix}
 G = (\text{det}M)(M^{-1})^{T} 
G =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
U^{-1}
 G = (\text{det}M)(M^{-1})^{T}","['matrices', 'transformation']"
67,"Multivariate generalization of 'If $f'(x) > c >0$, then $\lim_{x\to\pm\infty}f(x) = \pm \infty$'?","Multivariate generalization of 'If , then '?",f'(x) > c >0 \lim_{x\to\pm\infty}f(x) = \pm \infty,"In the one dimensional setting we have the following: Assume $$f:\Bbb R\to\Bbb R$$ is differentiable and fulfills $$f'(x) > c >0 ,$$ at least outside of some compact set $K$ . Then we have $$\lim_{x\to\pm\infty}f(x) = \pm \infty,$$ which may be seen for example by mean value theorem. I am wondering how to generalize this to $$ f:\Bbb R^n\to\Bbb R^n.$$ I suspect we have $$\lim_{\|x\|\to \infty}\|f(x)\| = \infty,$$ under some conditions on the Jacobian $J_f$ . By the multivariate mean value theorem, one could get that $$f_i(a) - f_i(b) = \langle J_f(z_i)_i,a-b\rangle ,$$ fore the component functions $f_i$ and some $z_i$ on the line segment between $a$ and $b$ . At this point I am not sure how to put that into a condition on the Jacobian, for example if I use the matrix norm the inequality goes in the 'wrong' direction. Edit : Maybe a good starting point to think about this problem is strong convexity . Assume that $f$ is the gradient of some function $g$ : $$f(x) = \nabla_g(x).$$ $g$ is strongly convex, iff there exists some $m>0$ such that $$\langle \nabla_g(x) - \nabla_g(y),x-y\rangle \geq m \Vert x-y\Vert_2^2 .$$ In particular, $g$ is strongly convex iff the Hessian of $g$ , that is $J_f$ , fullfills $$J_f \geq mI_n,$$ that is the smallest eigenvalue $\lambda_*$ of $J_f$ fulfills $\lambda_*\geq m$ . Then one can conclude by Cauchy-Schwarz: $$ \Vert f(x) - f(y) \Vert \Vert x-y \Vert \geq \langle f(x) -f(y), x-y\rangle^2 \geq m^2 \Vert x-y\Vert^4 .$$ Which shows $\lim_{\|x\|\to \infty}\|f(x)\| = \infty.$ Of course, this line of argument requires that $f$ has a potential. Maybe one can find another condition without this assumption?","In the one dimensional setting we have the following: Assume is differentiable and fulfills at least outside of some compact set . Then we have which may be seen for example by mean value theorem. I am wondering how to generalize this to I suspect we have under some conditions on the Jacobian . By the multivariate mean value theorem, one could get that fore the component functions and some on the line segment between and . At this point I am not sure how to put that into a condition on the Jacobian, for example if I use the matrix norm the inequality goes in the 'wrong' direction. Edit : Maybe a good starting point to think about this problem is strong convexity . Assume that is the gradient of some function : is strongly convex, iff there exists some such that In particular, is strongly convex iff the Hessian of , that is , fullfills that is the smallest eigenvalue of fulfills . Then one can conclude by Cauchy-Schwarz: Which shows Of course, this line of argument requires that has a potential. Maybe one can find another condition without this assumption?","f:\Bbb R\to\Bbb R f'(x) > c >0 , K \lim_{x\to\pm\infty}f(x) = \pm \infty,  f:\Bbb R^n\to\Bbb R^n. \lim_{\|x\|\to \infty}\|f(x)\| = \infty, J_f f_i(a) - f_i(b) = \langle J_f(z_i)_i,a-b\rangle , f_i z_i a b f g f(x) = \nabla_g(x). g m>0 \langle \nabla_g(x) - \nabla_g(y),x-y\rangle \geq m \Vert x-y\Vert_2^2 . g g J_f J_f \geq mI_n, \lambda_* J_f \lambda_*\geq m  \Vert f(x) - f(y) \Vert \Vert x-y \Vert \geq \langle f(x) -f(y), x-y\rangle^2 \geq m^2 \Vert x-y\Vert^4 . \lim_{\|x\|\to \infty}\|f(x)\| = \infty. f","['real-analysis', 'limits', 'multivariable-calculus', 'jacobian']"
68,"Deriving the gradient of $f: \mathbb{R^3} \rightarrow \mathbb{R}$ at any coordinates $(u_1,u_2,u_3)$",Deriving the gradient of  at any coordinates,"f: \mathbb{R^3} \rightarrow \mathbb{R} (u_1,u_2,u_3)","Let $(u_1,u_2,u_3)$ be an coordinates on $\mathbb{R^3}$ s.t there is parametrization  as the following: $$\vec{r}\left(u_{1}, u_{2}, u_{3}\right)=x_{1}\left(u_{1}, u_{2}, u_{3}\right) \hat{i}+x_{2}\left(u_{1}, u_{2}, u_{3}\right) \hat{j}+x_{3}\left(u_{1}, u_{2}, u_{3}\right) \hat{k}, \quad\left(u_{1}, u_{2}, u_{3}\right) \in D$$ assume that $\vec{r} \in C^1$ and let $$\vec{r}_{u_{1}}=\frac{\partial \vec{r}}{\partial u_{1}}, \quad \vec{r}_{u_{2}}=\frac{\partial \vec{r}}{\partial u_{2}}, \quad \vec{r}_{u_{3}}=\frac{\partial \vec{r}}{\partial u_{3}}$$ define the basis $$B=\left\{\vec{r}_{u_{1}}, \vec{r}_{u_{2}}, \vec{r}_{u_{3}}\right\}$$ for $\mathbb{R^3}$ and Let $$\tilde{B}=\left\{\vec{f}_{u_{1}}, \vec{f}_{u_{2}}, \vec{f}_{u_{3}}\right\}$$ be the dual basis s.t $$\left\langle f_{u_{i}}, \vec{r}_{u_{j}}\right\rangle=\delta_{i j}$$ define the matrix $g$ s.t $$g_{i j}=\left\langle\vec{r}_{u_{i}}, \vec{r}_{u_{j}}\right\rangle$$ Let $$f: \mathbb{R^3} \rightarrow \mathbb{R}$$ and $f \in C^1$ assume that $f$ is the function of the coordinates $(u_1,u_2,u_3)$ s.t $$f\left(u_{1}, u_{2}, u_{3}\right)=f\left(\vec{r}\left(u_{1}, u_{2}, u_{3}\right)\right)$$ show that the grad of $f$ in $(u_1,u_2,u_3)$ coordinates is $$\vec{\nabla} f=\sum_{i=1}^{3}(\vec{\nabla} f)_{i} \vec{r}_{u_{i}}=\sum_{i=1}^{3} \sum_{i=1}^{3}\left(g^{-1}\right)_{i j} \frac{\partial f}{\partial u_{j}} \vec{r}_{u_{i}}$$ while $\left(g^{-1}\right)_{i j}$ is the entries of the inverse matrix $g$ . I understood so far that the matrix g is the multiply of $J^TJ$ but I am not sure how it can help me solve this problem",Let be an coordinates on s.t there is parametrization  as the following: assume that and let define the basis for and Let be the dual basis s.t define the matrix s.t Let and assume that is the function of the coordinates s.t show that the grad of in coordinates is while is the entries of the inverse matrix . I understood so far that the matrix g is the multiply of but I am not sure how it can help me solve this problem,"(u_1,u_2,u_3) \mathbb{R^3} \vec{r}\left(u_{1}, u_{2}, u_{3}\right)=x_{1}\left(u_{1}, u_{2}, u_{3}\right) \hat{i}+x_{2}\left(u_{1}, u_{2}, u_{3}\right) \hat{j}+x_{3}\left(u_{1}, u_{2}, u_{3}\right) \hat{k}, \quad\left(u_{1}, u_{2}, u_{3}\right) \in D \vec{r} \in C^1 \vec{r}_{u_{1}}=\frac{\partial \vec{r}}{\partial u_{1}}, \quad \vec{r}_{u_{2}}=\frac{\partial \vec{r}}{\partial u_{2}}, \quad \vec{r}_{u_{3}}=\frac{\partial \vec{r}}{\partial u_{3}} B=\left\{\vec{r}_{u_{1}}, \vec{r}_{u_{2}}, \vec{r}_{u_{3}}\right\} \mathbb{R^3} \tilde{B}=\left\{\vec{f}_{u_{1}}, \vec{f}_{u_{2}}, \vec{f}_{u_{3}}\right\} \left\langle f_{u_{i}}, \vec{r}_{u_{j}}\right\rangle=\delta_{i j} g g_{i j}=\left\langle\vec{r}_{u_{i}}, \vec{r}_{u_{j}}\right\rangle f: \mathbb{R^3} \rightarrow \mathbb{R} f \in C^1 f (u_1,u_2,u_3) f\left(u_{1}, u_{2}, u_{3}\right)=f\left(\vec{r}\left(u_{1}, u_{2}, u_{3}\right)\right) f (u_1,u_2,u_3) \vec{\nabla} f=\sum_{i=1}^{3}(\vec{\nabla} f)_{i} \vec{r}_{u_{i}}=\sum_{i=1}^{3} \sum_{i=1}^{3}\left(g^{-1}\right)_{i j} \frac{\partial f}{\partial u_{j}} \vec{r}_{u_{i}} \left(g^{-1}\right)_{i j} g J^TJ","['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'coordinate-systems']"
69,Need help minimizing the Kullback-Leibler divergence between discrete observations and a complicated distribution.,Need help minimizing the Kullback-Leibler divergence between discrete observations and a complicated distribution.,,"TL;DR: I want to fit the probability distribution $P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz$ to a bunch of discrete observations in order to obtain $N_0$ and $N_1$ which produced them. I fail at solving the following equations and need help at how to approach this: $$\frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right), \hspace{0.5cm}\frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ My motivation comes from this paper: Probabilistic Object Detection: Definition and Evaluation . The authors define a probability distribution over all pixels $(u', v')$ of an image as a product of integrals of two 2D-Gaussian distributions $N_0$ and $N_1$ , which represent bounding box corners. The exact formula is: $$P(u', v') = \int \int_{0, 0}^{u', v'} N_0(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv \int \int_{u', v'}^{H, W} N_1(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv$$ where $H$ and $W$ are width and height of the image. This procedure allows one to get from two bounding-box gaussians to a probability distribution over all pixels - however, I wanted to solve the opposite problem of trying to reconstruct the two gaussians given the pixel-wise probability distribution. To do that, I first want to solve a simpler, 1D version of this problem. Here is an image showing the 1D problem: Visualization of the 1D problem In a 1D setting, the element-wise probability distribution simplifies to this calculation: $$P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz$$ In this example there are 16 discrete probability valuse at each of 16 positions of the 1D array. In my toy example I just made up two gaussians to calculate the resulting $P(x)$ . The ""observations"" are discretizations of that $P$ with some gaussian noise added. My approach to reverse engineer the two gaussians from the observations was to minimize the Kullback-Leibler divergence between observations and the combined integrated distribution. This leads to my problem: After some reformulation of the KL-divergence I'm stuck at actually minimizing it, because the logarithm doesn't act directly on the gaussians but on the integrals. I don't know how to approach the problem from here, or if an analytical solution even exists at all. I'll write down the steps I have done so far. $W$ is the width of the 1D array and $O(x)$ the observation at position x: $$D_{KL} (O||P) = \sum_{x=1}^W O(x) ln \left(  \frac{O(x)}{\int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz} \right)$$ $$= \sum_{x=1}^W O(x) \left[ln (O(x) - ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right) - ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right)\right]$$ $$= \sum_{x=1}^W O(x) ln(O(x) - \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right)- \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right)$$ If I want to minimize this term, that brings me to the following subproblem (each of the optimization variables $\mu_1, \mu_2, \sigma_1, \sigma_2$ is only in one of the terms, so for each only one of the addends is relevant): $$max \hspace{0.1cm} \mu_1,\sigma_1 \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right) \rightarrow$$ $$\frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ $$\frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)$$ This is the point at which I don't really know what to do anymore. If anybody can point me at how to solve this or a completely different approach to get to the two original gaussians without iterative numeric approaches, I'd be really thankful!","TL;DR: I want to fit the probability distribution to a bunch of discrete observations in order to obtain and which produced them. I fail at solving the following equations and need help at how to approach this: My motivation comes from this paper: Probabilistic Object Detection: Definition and Evaluation . The authors define a probability distribution over all pixels of an image as a product of integrals of two 2D-Gaussian distributions and , which represent bounding box corners. The exact formula is: where and are width and height of the image. This procedure allows one to get from two bounding-box gaussians to a probability distribution over all pixels - however, I wanted to solve the opposite problem of trying to reconstruct the two gaussians given the pixel-wise probability distribution. To do that, I first want to solve a simpler, 1D version of this problem. Here is an image showing the 1D problem: Visualization of the 1D problem In a 1D setting, the element-wise probability distribution simplifies to this calculation: In this example there are 16 discrete probability valuse at each of 16 positions of the 1D array. In my toy example I just made up two gaussians to calculate the resulting . The ""observations"" are discretizations of that with some gaussian noise added. My approach to reverse engineer the two gaussians from the observations was to minimize the Kullback-Leibler divergence between observations and the combined integrated distribution. This leads to my problem: After some reformulation of the KL-divergence I'm stuck at actually minimizing it, because the logarithm doesn't act directly on the gaussians but on the integrals. I don't know how to approach the problem from here, or if an analytical solution even exists at all. I'll write down the steps I have done so far. is the width of the 1D array and the observation at position x: If I want to minimize this term, that brings me to the following subproblem (each of the optimization variables is only in one of the terms, so for each only one of the addends is relevant): This is the point at which I don't really know what to do anymore. If anybody can point me at how to solve this or a completely different approach to get to the two original gaussians without iterative numeric approaches, I'd be really thankful!","P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz N_0 N_1 \frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right), \hspace{0.5cm}\frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right) (u', v') N_0 N_1 P(u', v') = \int \int_{0, 0}^{u', v'} N_0(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv \int \int_{u', v'}^{H, W} N_1(u, v) \hspace{0.1cm} du \hspace{0.1cm} dv H W P(x) = \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz P(x) P W O(x) D_{KL} (O||P) = \sum_{x=1}^W O(x) ln \left(  \frac{O(x)}{\int_{0}^{x} N_0(z) \hspace{0.1cm} dz \int_{x}^{W} N_1(z) \hspace{0.1cm} dz} \right) = \sum_{x=1}^W O(x) \left[ln (O(x) - ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right) - ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right)\right] = \sum_{x=1}^W O(x) ln(O(x) - \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{0}^{x} N_0(z) \hspace{0.1cm} dz \right)- \sum_{x=1}^W O(x)\hspace{0.1cm} ln \left( \int_{x}^{W} N_1(z) \hspace{0.1cm} dz \right) \mu_1, \mu_2, \sigma_1, \sigma_2 max \hspace{0.1cm} \mu_1,\sigma_1 \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right) \rightarrow \frac{\partial}{\partial \mu_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right) \frac{\partial}{\partial \sigma_1} \sum_{z=0}^{x} O(x)\hspace{0.1cm} ln \left(\int_{0}^{x} \frac{1}{\sigma_1 \sqrt{2 \pi}} e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} dz \right)","['calculus', 'multivariable-calculus']"
70,"Let C=$\left\{(t\cos(t),t\sin(t),t^2):0\le t\le\frac{\pi}{2}\right\}$. Compute $\int_CF\cdot d\text{x}$",Let C=. Compute,"\left\{(t\cos(t),t\sin(t),t^2):0\le t\le\frac{\pi}{2}\right\} \int_CF\cdot d\text{x}","$\def\hl#1#2{\bbox[#1,1px]{#2}} \def\box#1#2#3#4#5{\color{#2}{\bbox[0px, border: 2px solid #2]{\hl{#3}{\color{white}{\color{#3}{\boxed{\underline{\large\color{#1}{\text{#4}}}\\\color{#1}{#5}\\}}}}}}} \def\verts#1{\left\vert#1\right\vert} \def\Verts#1{\left\Vert#1\right\Vert} \def\pra#1{\left(#1\right)} \def\R{\mathbb{R}} \def\N{\mathbb{N}} \def\Z{\mathbb{Z}}$ $\box{black}{black}{} {Question} {(a)\text{ Let F$\pra{x,y,z}=(y+z,\alpha x+z,x+\beta y)$. For what values of $\alpha$,}\\ \text{$\beta$ is F conservative. For those cases, find $f$ s.t. F$=\nabla f$}\\ (b)\text{ Let C=$\left\{(t\cos(t),t\sin(t),t^2):0\le t\le\frac{\pi}{2}\right\}$. Compute $\int_CF\cdot d\text{x}\hspace{21.3ex}$}}\\ \box{black}{red}{} {Theorem 1.} {\text{Let $U$ be an open subset of $\R^n$ for $n\ge2$, and let $G:U\to\R^n$ be a continuous vector}\\ \text{field. Then the following are equivalent:}\\ \text{1. There exists a function $f:U\to\R$ of class $C^1$ such that $G=\nabla f$.}\\ \text{2. $\int_CG\cdot d\text{x}$ for any closed piecewise smooth oriented curve $C$ in $U$.}\\ \text{3. $\int_{C_1}G\cdot d\text{x}=\int_{C_2}G\cdot d\text{x}$ for any two piecewise smooth oriented curves $C_1,C_2$ in $U$ that}\\ \text{ both start at the same point $P\in U$ and end at the same point $q\in U.$} }\\ \box{black}{green}{} {Def.(Conservative Vector Field)} {\text{A continuous vector field $G:U\to\R^n$ is said to be conservative if any one of these cond-}\\ \text{itions is satisfied.}}\\ \box{black}{red}{} {Theorem 2.} {\text{If G is a conservative vector field of class $C^1$ on an open set $U\subset\R^3$, then curl G$=0.\hspace{4ex}$}}\\ \box{black}{red}{} {Theorem 3.} {\text{If G$:U\to\R^3$ is a $C^1$ vector field, $U$ is a convex, and curl G$=0$, then G is conservative.$\hspace{1.3ex}$}}$ My attempts $(a)$ From Theorem $2.$ and $3.$ we know that If F has convex domain, then curl F $=0$ if and only if F is conservative As F $:U=\R^3\to\R^3$ where $U$ is convex. and \begin{align} \text{curl F}=&\nabla\times F\\ =&(\beta-1,0,\alpha-1) \end{align} i.e. curl F $=0$ if $\beta=1$ and $\alpha=1$ . Therefore F $(x,y,z)=(y+z,x+z,x+y)$ have \begin{align} f(x,y,z)=&\int_a^xF_1(t,b,c)dt+\int_b^yF_2(x,t,c)dt+\int_c^zF_3(x,y,t)dt\\ =&\int_a^xb+c~dt+\int_b^yx+c~dt+\int_c^zx+y~dt\\ =&~x y + x z + y z-a b - a c - b c \end{align} Where $(a,b,c)\in U=\R^3$ , take $(a,b,c)=(0,0,0)$ then $f(x,y,z)=x y + x z + y z$ s.t. F $=\nabla f$ $(b)$ Then by Fundamental Theorem of Line Integrals \begin{align} \int_C \text{F}\cdot d\text{x}=&\int_C \nabla f\cdot d\text{x}\\ =&f\pra{\frac{\pi}{2}\cos\pra{\frac{\pi}{2}},\frac{\pi}{2}\sin\pra{\frac{\pi}{2}},\pra{\frac{\pi}{2}}^2}-f(0,0,0)\\ =&\cos\pra{\frac{\pi}{2}}\sin\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^2+\cos\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^3+\sin\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^3\\ =&\pra{\frac{\pi}{2}}^3 \end{align} Is my solution correct?","My attempts From Theorem and we know that If F has convex domain, then curl F if and only if F is conservative As F where is convex. and i.e. curl F if and . Therefore F have Where , take then s.t. F Then by Fundamental Theorem of Line Integrals Is my solution correct?","\def\hl#1#2{\bbox[#1,1px]{#2}}
\def\box#1#2#3#4#5{\color{#2}{\bbox[0px, border: 2px solid #2]{\hl{#3}{\color{white}{\color{#3}{\boxed{\underline{\large\color{#1}{\text{#4}}}\\\color{#1}{#5}\\}}}}}}}
\def\verts#1{\left\vert#1\right\vert}
\def\Verts#1{\left\Vert#1\right\Vert}
\def\pra#1{\left(#1\right)}
\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}} \box{black}{black}{}
{Question}
{(a)\text{ Let F\pra{x,y,z}=(y+z,\alpha x+z,x+\beta y). For what values of \alpha,}\\
\text{\beta is F conservative. For those cases, find f s.t. F=\nabla f}\\
(b)\text{ Let C=\left\{(t\cos(t),t\sin(t),t^2):0\le t\le\frac{\pi}{2}\right\}. Compute \int_CF\cdot d\text{x}\hspace{21.3ex}}}\\
\box{black}{red}{}
{Theorem 1.}
{\text{Let U be an open subset of \R^n for n\ge2, and let G:U\to\R^n be a continuous vector}\\
\text{field. Then the following are equivalent:}\\
\text{1. There exists a function f:U\to\R of class C^1 such that G=\nabla f.}\\
\text{2. \int_CG\cdot d\text{x} for any closed piecewise smooth oriented curve C in U.}\\
\text{3. \int_{C_1}G\cdot d\text{x}=\int_{C_2}G\cdot d\text{x} for any two piecewise smooth oriented curves C_1,C_2 in U that}\\
\text{ both start at the same point P\in U and end at the same point q\in U.}
}\\
\box{black}{green}{}
{Def.(Conservative Vector Field)}
{\text{A continuous vector field G:U\to\R^n is said to be conservative if any one of these cond-}\\
\text{itions is satisfied.}}\\
\box{black}{red}{}
{Theorem 2.}
{\text{If G is a conservative vector field of class C^1 on an open set U\subset\R^3, then curl G=0.\hspace{4ex}}}\\
\box{black}{red}{}
{Theorem 3.}
{\text{If G:U\to\R^3 is a C^1 vector field, U is a convex, and curl G=0, then G is conservative.\hspace{1.3ex}}} (a) 2. 3. =0 :U=\R^3\to\R^3 U \begin{align}
\text{curl F}=&\nabla\times F\\
=&(\beta-1,0,\alpha-1)
\end{align} =0 \beta=1 \alpha=1 (x,y,z)=(y+z,x+z,x+y) \begin{align}
f(x,y,z)=&\int_a^xF_1(t,b,c)dt+\int_b^yF_2(x,t,c)dt+\int_c^zF_3(x,y,t)dt\\
=&\int_a^xb+c~dt+\int_b^yx+c~dt+\int_c^zx+y~dt\\
=&~x y + x z + y z-a b - a c - b c
\end{align} (a,b,c)\in U=\R^3 (a,b,c)=(0,0,0) f(x,y,z)=x y + x z + y z =\nabla f (b) \begin{align}
\int_C \text{F}\cdot d\text{x}=&\int_C \nabla f\cdot d\text{x}\\
=&f\pra{\frac{\pi}{2}\cos\pra{\frac{\pi}{2}},\frac{\pi}{2}\sin\pra{\frac{\pi}{2}},\pra{\frac{\pi}{2}}^2}-f(0,0,0)\\
=&\cos\pra{\frac{\pi}{2}}\sin\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^2+\cos\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^3+\sin\pra{\frac{\pi}{2}}\pra{\frac{\pi}{2}}^3\\
=&\pra{\frac{\pi}{2}}^3
\end{align}","['calculus', 'multivariable-calculus', 'solution-verification', 'vector-fields', 'line-integrals']"
71,The advantage of Complex Differentiation and Inverse Function Theorem,The advantage of Complex Differentiation and Inverse Function Theorem,,"One interesting phenonmenon in complex analysis is the following, If $f:\mathbb C\to\mathbb C$ is complex differentiable at point $a$ ( $\equiv$ derivative is a spiral similarity), and a local homeomophism with inverse $g$ near $a$ , then $g$ is complex differentiable at point $b=f(a)$ . Its proof is as the page from Ahlfors' Complex Analysis , https://i.sstatic.net/UTdED.png Same argument applies to usual one variable differentiation and probably any normed field, since we are allowed to invert quotient before taking limits on their norm, hence showing an implcit linkage between analysis and algebra. However in multivariable calculus, one cannot invert quotient to prove similar theorem. By taking $f(x+h)=y+k, f(x)=y$ as in usual proof of inverse function theorem, one is required to show that for some real $\lambda>0$ , $|k|\ge \lambda |h|$ , so that one can bound the usual quotient $\dfrac{|h-f'(x)^{-1}k|}{|k|}$ by $\dfrac{|k-f'(x)h|}{|h|}$ up to some multiplicative constant. This is far from inverting quotients. Is there any explicit explanation of this interplay between algebra and analysis? P.S. One interesting corollary found is that if a homeomorphism $f:U\to V$ , where both are subset of $\mathbb R^2$ , and has invertible differential at point $A$ , then its inverse is differentiable at point $f(A)$ . (by normalising function so that it is complex differentiable) P.S.2 Its generalisation (not verified): If $f:\mathbb C^n\to\mathbb C^n$ is local homeomorphism (from $U$ to $V$ ) and differentiable at $a\in U$ with invertible differential, then its local inverse is differentiable at $b$ . By this, if $f:\mathbb R^{2n}\to\mathbb R^{2n}$ satisfies similar condition, its inverse is differentiable at $b$ . P.S.3 Maybe an interesting question is whether one can define some algebraic structure on $\mathbb C^n$ like bicomplex number such that one can invert quotients for proof. (But it just need not be commutative, causing more problem.)","One interesting phenonmenon in complex analysis is the following, If is complex differentiable at point ( derivative is a spiral similarity), and a local homeomophism with inverse near , then is complex differentiable at point . Its proof is as the page from Ahlfors' Complex Analysis , https://i.sstatic.net/UTdED.png Same argument applies to usual one variable differentiation and probably any normed field, since we are allowed to invert quotient before taking limits on their norm, hence showing an implcit linkage between analysis and algebra. However in multivariable calculus, one cannot invert quotient to prove similar theorem. By taking as in usual proof of inverse function theorem, one is required to show that for some real , , so that one can bound the usual quotient by up to some multiplicative constant. This is far from inverting quotients. Is there any explicit explanation of this interplay between algebra and analysis? P.S. One interesting corollary found is that if a homeomorphism , where both are subset of , and has invertible differential at point , then its inverse is differentiable at point . (by normalising function so that it is complex differentiable) P.S.2 Its generalisation (not verified): If is local homeomorphism (from to ) and differentiable at with invertible differential, then its local inverse is differentiable at . By this, if satisfies similar condition, its inverse is differentiable at . P.S.3 Maybe an interesting question is whether one can define some algebraic structure on like bicomplex number such that one can invert quotients for proof. (But it just need not be commutative, causing more problem.)","f:\mathbb C\to\mathbb C a \equiv g a g b=f(a) f(x+h)=y+k, f(x)=y \lambda>0 |k|\ge \lambda |h| \dfrac{|h-f'(x)^{-1}k|}{|k|} \dfrac{|k-f'(x)h|}{|h|} f:U\to V \mathbb R^2 A f(A) f:\mathbb C^n\to\mathbb C^n U V a\in U b f:\mathbb R^{2n}\to\mathbb R^{2n} b \mathbb C^n","['abstract-algebra', 'complex-analysis']"
72,"Find $\iint_Ay\,dA$, where $A$ is defined by $z=x+y^2$,$0\le x\le 1$ and $0\le y\le2$","Find , where  is defined by , and","\iint_Ay\,dA A z=x+y^2 0\le x\le 1 0\le y\le2","Can anyone help me to find the $\iint_Ay\,dA $ where $A$ is defined by $z=x+y^2$ and $0\le x\le1$ and $0\le y\le2$ What I was thinking to do is to put $y=\sqrt{z-x}$ but I am not sure that I am doing right because I cannot find the limits of the integral.",Can anyone help me to find the where is defined by and and What I was thinking to do is to put but I am not sure that I am doing right because I cannot find the limits of the integral.,"\iint_Ay\,dA  A z=x+y^2 0\le x\le1 0\le y\le2 y=\sqrt{z-x}","['integration', 'multivariable-calculus', 'multiple-integral']"
73,Tangent Plane in the graph of $SL(3)$,Tangent Plane in the graph of,SL(3),"Let $SL(3)$ be the matrix $3 \times 3$ with determinant $1$ . $a)$ Show that $SL(3)$ is, locally, graph of a continuous differentiable function. $b)$ Determine the dimension of the tangent plane for each point of this graph. $c)$ Compute the tangent plane of the graph in the point $X_0 = Id$ . The first question I did, using the Implicit Function Theorem, and I found $g: U \subset \mathbb{R}^8 \rightarrow V \subset \mathbb{R}$ . The second, I guess the tangent plane has dimension $8$ . But I do not nkow how compute the tangent plane in $X_0 = Id$ .","Let be the matrix with determinant . Show that is, locally, graph of a continuous differentiable function. Determine the dimension of the tangent plane for each point of this graph. Compute the tangent plane of the graph in the point . The first question I did, using the Implicit Function Theorem, and I found . The second, I guess the tangent plane has dimension . But I do not nkow how compute the tangent plane in .",SL(3) 3 \times 3 1 a) SL(3) b) c) X_0 = Id g: U \subset \mathbb{R}^8 \rightarrow V \subset \mathbb{R} 8 X_0 = Id,"['real-analysis', 'calculus', 'multivariable-calculus']"
74,"verifying the result $\int_{B^4}e^{x^2+y^2-z^2-w^2}\,dx\,dy\,dz\,dw=\pi^2(\sinh(1)+1-\frac{1}{e})$",verifying the result,"\int_{B^4}e^{x^2+y^2-z^2-w^2}\,dx\,dy\,dz\,dw=\pi^2(\sinh(1)+1-\frac{1}{e})","the problem is: $$I=\int_{B^4} e^{x^2+y^2-z^2-w^2} \,dx\,dy\,dz\,dw $$ where $B^4$ is the unit 4 ball, explicitly: $$ B^4=\{(x,y,z,w)\in\Bbb{R}^4 \mid x^2+y^2+z^2+w^2\leq 1\}$$ here is my solution: using fubini's theorem we obtain: $$I=\int_{B^2}e^{-z^2-w^2}\left(\int_{{B^2}_{\sqrt{1-z^2-w^2}}} e^{x^2+y^2} \, dx\, dy\right)\,dz\,dw$$ where $B^2$ is the unit 2 ball and ${B^2}_{\sqrt{1-z^2-w^2}}$ is the 2 ball centred at the origin with radius $\sqrt{1-z^2-w^2}$ . next we calculate $$\int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy$$ using polar cordinates, so: $$\int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy=\int_{0}^{2\pi}\int_{0}^{\sqrt{1-z^2-w^2}}e^{r^2}rdrd\theta=\pi(e^{1-z^2-w^2}-1)$$ plugging this result into the outer integral we get: $$I=\int_{B^2}e^{-z^2-w^2}(\pi(e^{1-z^2-w^2}-1))dzdw=\pi e \int_{B^2}e^{-2(z^2+w^2)}dzdw-\pi \int_{B^2}e^{-z^2-w^2}dzdw$$ again using polar cordinates we receive: $$\int_{B^2}e^{-2(z^2+w^2)}dzdw=\int_{0}^{2\pi}\int_{0}^{1}e^{-2r^2}rdrd\theta=\frac{\pi}{2}(1-e^{-2})$$ and: $$\int_{B^2}e^{-z^2-w^2}dzdw=\int_0^{2\pi}\int_0^1re^{-r^2}drd\theta=\pi(1-\frac{1}{e})$$ plugging this in we get: $$I=\pi^2(\sinh(1)+1-\frac{1}{e})$$ is this solution correct or have I made a mistake along the way?","the problem is: where is the unit 4 ball, explicitly: here is my solution: using fubini's theorem we obtain: where is the unit 2 ball and is the 2 ball centred at the origin with radius . next we calculate using polar cordinates, so: plugging this result into the outer integral we get: again using polar cordinates we receive: and: plugging this in we get: is this solution correct or have I made a mistake along the way?","I=\int_{B^4} e^{x^2+y^2-z^2-w^2} \,dx\,dy\,dz\,dw  B^4  B^4=\{(x,y,z,w)\in\Bbb{R}^4 \mid x^2+y^2+z^2+w^2\leq 1\} I=\int_{B^2}e^{-z^2-w^2}\left(\int_{{B^2}_{\sqrt{1-z^2-w^2}}} e^{x^2+y^2} \, dx\, dy\right)\,dz\,dw B^2 {B^2}_{\sqrt{1-z^2-w^2}} \sqrt{1-z^2-w^2} \int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy \int_{{B^2}_{\sqrt{1-z^2-w^2}}}e^{x^2+y^2}dxdy=\int_{0}^{2\pi}\int_{0}^{\sqrt{1-z^2-w^2}}e^{r^2}rdrd\theta=\pi(e^{1-z^2-w^2}-1) I=\int_{B^2}e^{-z^2-w^2}(\pi(e^{1-z^2-w^2}-1))dzdw=\pi e \int_{B^2}e^{-2(z^2+w^2)}dzdw-\pi \int_{B^2}e^{-z^2-w^2}dzdw \int_{B^2}e^{-2(z^2+w^2)}dzdw=\int_{0}^{2\pi}\int_{0}^{1}e^{-2r^2}rdrd\theta=\frac{\pi}{2}(1-e^{-2}) \int_{B^2}e^{-z^2-w^2}dzdw=\int_0^{2\pi}\int_0^1re^{-r^2}drd\theta=\pi(1-\frac{1}{e}) I=\pi^2(\sinh(1)+1-\frac{1}{e})","['integration', 'multivariable-calculus', 'multiple-integral']"
75,Embedding a manifold with connection in $\mathbb{R}^3$ (also need help with diff. eqs.),Embedding a manifold with connection in  (also need help with diff. eqs.),\mathbb{R}^3,"Say a universe we want to study has two finite spatial dimensions, which we will imagine to describe a square of side length two. For the diff. manifold underlying the Newtonian spacetime with which we choose to study this universe, say we equip the set $M:=\mathbb{R}^{+}_0\times[-1,1]\times[-1,1]$ with $\mathcal{O}_{\text{St.}}|_M$ and $$\mathcal{A}:=\{(M,\text{id}_M)\}$$ Our universe has, at the points corresponding to $(t,0,0)\in M$ for any $t\in\mathbb{R}^{+}_0$ , a point mass of 1 unit mass. This means that on a point mass at coordinates $p=(t,x,y)$ acts a gravitational force of $$F_p:=\frac{G}{x^2+y^2}$$ Newtons. We set $G$ to $1$ . The force vector on the point mass at $p$ may be described with $$\widehat{\text{F}}_p:=(\widehat{\text{F}}^0_p,\widehat{\text{F}}^1_p,\widehat{\text{F}}^2_p)\equiv\left(0,\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2},\frac{\frac{y}{x}}{x^2+y^2}\right)$$ This determines the connection $\nabla$ (I define a connection as a map taking a vector field and a $(p,q)$ -tensor field to another $(p,q)$ -tensor field that satisfies the Leibnitz rule, agrees with partial differentiation for $(0,0)$ -tensor fields, is $C^\infty$ -linear w.r.t. the vector field input and respects addition of the tensor field input) on our manifold, as we set all the connection coefficient functions $\Gamma$ to zero except for $$\Gamma^i_{00}(p):=\widehat{\text{F}}^i_p$$ (A result of the autoparallel equation: Objects in the spacetime $M$ on which no force acts besides gravitation move along straight lines of $M$ as defined by the connection $\nabla$ if and only if we define the $\Gamma$ s this way.) I want to find an injective embedding $\phi:M\rightarrow\mathbb{R}^3$ of this spacetime manifold into three-dimensional Euclidean space such that the pullback connection $\nabla^*$ , coming from the Euclidean connection $\nabla^E$ , is equal to $\nabla$ . The idea behind this is that we would like to see the spacetime $M$ ""unstretched"" - with the curvature encoded into its embedding and resulting ""weird"" shape - in $\mathbb{R}^3$ . What is the best way to calculate what such an embedding of a spacetime would look like? And for which kind of connection carrying manifolds do such $\mathbb{R}^3$ visualizations actually exist? My approach of finding a differential equation for an embedding $\phi$ went as follows (I would be very glad for pointing out blunders in the calculation, as otherwise solving the diff. eqs. underneath is meaningless): Per definition, we have $$\nabla^*_X\ \phi^*(s)=\phi^*\left(\nabla^E_{\phi_*(X)}\ s\right)$$ for any covectors $s\in T^1_0\mathbb{R}^3$ . We will define $\phi^{-1}$ as a map from $\phi(M)$ to $M$ by $\phi^{-1}(p)=q$ iff $\phi(q)=p$ . This leads to: $$\nabla^*_{e_i}\ \phi^*((\phi^{-1})^*(\in^j))=\nabla^*_{e_i}\ \in^j=\sum_q-\Gamma^{*q}_{ij}\cdot\in^q=\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)$$ where $e_i$ and $\in^j$ are the natural basis and induced dual basis vector fields of our chart $(M,\text{id}_M)$ respectively. We want for $\Gamma^{*q}_{ij}=\Gamma^q_{ij}$ to hold and hence get: $$\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)=\left(\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)\right)(e_k)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))$$ We start to run into a notational issue here: $\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)$ is an element of $C^\infty(M)$ , but $\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))$ is an element of $C^\infty(\phi(M))$ . The above equality is therefore, technically, not correct. As the latter term stems from the definition of the covector-field-pullback $\phi^*$ , the problem is that we forgot an input transformation via $(\phi(\cdot))$ at the end of it, i.e., we should write $$\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)(\cdot)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))(\phi(\cdot))$$ We will keep this in mind and drop $(\phi(\cdot))$ again. We can calculate the left-hand side of the last result and use the Leibnitz rule for connections on the right-hand side: $$\sum_q\delta^q_k\cdot(-\Gamma^q_{ij})=\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)(\phi_*(e_k))-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right)$$ This leads to: $$-\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \in^j\bigg((\phi^{-1})^*(\phi_*(e_k))\bigg)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right)$$ $$=\nabla^E_{\phi_*(e_i)}\ \in^j(e_k)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi^m_{*i}e^E_m}\ \phi^n_{*k}e^E_n\right)$$ where we use the Einstein summation convention at the lower index and input of $\nabla^E$ , define $e^E_i$ as the basis vector fields of $\mathbb{R}^3$ and $\phi^a_{*b}$ as the coefficient functions of the linear maps between the tangent spaces $T_pM$ and $T_p\mathbb{R}^3$ provided by $\phi_*(p)$ for each $p\in M$ . Further, we get: $$-\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \delta^j_k-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\nabla^E_{e^E_m}\ \phi^n_{*k}e^E_n\right)$$ $$\stackrel{Leibnitz}{=}0-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\nabla^E_{e^E_m}\ e^E_n+(\nabla^E_{e^E_m}\ \phi^n_{*k})\cdot e^E_n\bigg)\right)$$ $$=-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\Gamma^{E\ a}_{mn}\cdot e^E_a+e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right)$$ where $\Gamma^{E\ q}_{ij}$ are the connection coefficient functions of three-dimensional Euclidean space. Per definition, these functions equal zero everywhere. Hence: $$\Gamma^k_{ij}=((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\right)=\in^j\left((\phi^{-1})_*\bigg(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right)$$ Due to the linearity of $(\phi^{-1})_*$ we have: $$\Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})_*(e^E_n))\right)$$ By $(\phi^{-1})_{*a}^b$ we will denote the coefficient functions of the linear maps from $T_p\mathbb{R}^3$ to $T_pM$ given by $(\phi^{-1})_*$ at every point $p$ of the image $\phi(M)$ . As before, instead of viewing $(\phi^{-1})_{*a}^b$ as a $C^\infty(\phi(M))$ function, as the symbol itself suggests, we will treat it as a $C^\infty(M)$ function via the transformation $(\phi(\cdot))$ . Going on, we get: $$\Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^a_{*n}\cdot e_a\right)=\delta^j_a\cdot\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}$$ Which lets us conclude with: $$\Gamma^k_{ij}=\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}\ \stackrel{Einstein}{:\iff}\ \Gamma^k_{ij}=\sum_m\sum_n\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}$$ The map $\phi$ we search takes values from $\mathbb{R}^3\supset(\mathbb{R}^{+}_0\times[-1,1]\times[-1,1])$ to $\phi(M)\subset\mathbb{R}^3$ , which lets us exercise multivariable calculus on it: $$\phi^a_{*b}(p)=\frac{\partial\phi^a}{\partial e_b}(p)$$ $$(\phi^{-1})^a_{*b}(\phi(p))=\frac{\partial(\phi^{-1})^a(\phi(\cdot))}{\partial e_b}(p)$$ where $\phi^a$ and $(\phi^{-1})^a$ are the $a$ th component functions of $\phi$ and $\phi^{-1}$ respectively. This leads us to the following differential equations for $\phi$ : $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_1}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}$$ $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2}$$ and $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_i}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_k}\right)\cdot \frac{\partial(\phi^{-1})^j}{\partial e_n}\right)(t,x,y)=0$$ for any $(i,j,k)\in\{0,1,2\}^3$ where either $i\neq0$ or $j\neq0$ . Is this set of equations solvable? Can we find the solution? How to approach this? Are there conventions one should respect in their notation? To me it seems that one big problem with this is that we don't know the codomain of our unknown $\phi$ - which is of course what I actually want to know. I am very unexperienced in diff. eqs. and have no approach to this. Is it acceptable that both $\phi$ and its inverse appear in the formula? I would also be interested to know if we can feed such complex systems of diff. equations into computational systems like Wolfram Alpha! Edit: I noticed that, for any invertable differentiable function $f:\mathbb{R}\rightarrow\mathbb{R}$ , $$f^{-1}(f(x))=x\implies\left(\frac{\partial}{\partial x}(f^{-1}\circ f)\right)(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(x))\cdot \frac{\partial f}{\partial x}(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(\cdot))=\frac{1}{\frac{\partial f}{\partial x}}(\cdot)$$ Hence, we can replace the partial deriviatives of $\phi^{-1}$ above with $$\frac{\partial(\phi^{-1})^0}{\partial e_n}(\phi(\cdot))=\frac{1}{\frac{\partial\phi^0}{\partial e_n}}(\cdot)$$ Edit 2: Unfortunately unsuccessful attempt: Assume we are really lucky and for all $(m,n)\in\{0,1,2\}^2\backslash\{(0,0)\}$ at least one of the two functions $\frac{\partial\phi^m}{\partial e_0}$ , $\frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right)$ vanish. Then we have: $$\frac{\frac{y}{x}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)(t,x,y)$$ In more conventional notation: $$\frac{\partial}{\partial t}\left(\frac{\partial\phi^0}{\partial y}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2}$$ $$\implies\frac{\partial\phi^0}{\partial y}(t,x,y)=\int\frac{\frac{y}{x}}{x^2+y^2}\text{d}t=t\cdot\frac{\frac{y}{x}}{x^2+y^2}$$ $$\implies\phi^0(t,x,y)=\int t\cdot\frac{\frac{y}{x}}{x^2+y^2}\text{d}y$$ Freely available math software (like Wolfram Alpha) can solve this integral, and so we get: $$\phi^0(t,x,y)=t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x}$$ If this were true, we would have one coordinate component of our embedding function $\phi$ nailed down. But it fails: If the assumption starting our current approach off holds, we have $$\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)(t,x,y)$$ Which puts the following condition on our calculated component of $\phi$ : $$\frac{\partial}{\partial t}\left(\frac{\partial}{\partial x}\left(t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x}\right)\right)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}$$ But, as simple differentiation shows, this equality is not satisfied.","Say a universe we want to study has two finite spatial dimensions, which we will imagine to describe a square of side length two. For the diff. manifold underlying the Newtonian spacetime with which we choose to study this universe, say we equip the set with and Our universe has, at the points corresponding to for any , a point mass of 1 unit mass. This means that on a point mass at coordinates acts a gravitational force of Newtons. We set to . The force vector on the point mass at may be described with This determines the connection (I define a connection as a map taking a vector field and a -tensor field to another -tensor field that satisfies the Leibnitz rule, agrees with partial differentiation for -tensor fields, is -linear w.r.t. the vector field input and respects addition of the tensor field input) on our manifold, as we set all the connection coefficient functions to zero except for (A result of the autoparallel equation: Objects in the spacetime on which no force acts besides gravitation move along straight lines of as defined by the connection if and only if we define the s this way.) I want to find an injective embedding of this spacetime manifold into three-dimensional Euclidean space such that the pullback connection , coming from the Euclidean connection , is equal to . The idea behind this is that we would like to see the spacetime ""unstretched"" - with the curvature encoded into its embedding and resulting ""weird"" shape - in . What is the best way to calculate what such an embedding of a spacetime would look like? And for which kind of connection carrying manifolds do such visualizations actually exist? My approach of finding a differential equation for an embedding went as follows (I would be very glad for pointing out blunders in the calculation, as otherwise solving the diff. eqs. underneath is meaningless): Per definition, we have for any covectors . We will define as a map from to by iff . This leads to: where and are the natural basis and induced dual basis vector fields of our chart respectively. We want for to hold and hence get: We start to run into a notational issue here: is an element of , but is an element of . The above equality is therefore, technically, not correct. As the latter term stems from the definition of the covector-field-pullback , the problem is that we forgot an input transformation via at the end of it, i.e., we should write We will keep this in mind and drop again. We can calculate the left-hand side of the last result and use the Leibnitz rule for connections on the right-hand side: This leads to: where we use the Einstein summation convention at the lower index and input of , define as the basis vector fields of and as the coefficient functions of the linear maps between the tangent spaces and provided by for each . Further, we get: where are the connection coefficient functions of three-dimensional Euclidean space. Per definition, these functions equal zero everywhere. Hence: Due to the linearity of we have: By we will denote the coefficient functions of the linear maps from to given by at every point of the image . As before, instead of viewing as a function, as the symbol itself suggests, we will treat it as a function via the transformation . Going on, we get: Which lets us conclude with: The map we search takes values from to , which lets us exercise multivariable calculus on it: where and are the th component functions of and respectively. This leads us to the following differential equations for : and for any where either or . Is this set of equations solvable? Can we find the solution? How to approach this? Are there conventions one should respect in their notation? To me it seems that one big problem with this is that we don't know the codomain of our unknown - which is of course what I actually want to know. I am very unexperienced in diff. eqs. and have no approach to this. Is it acceptable that both and its inverse appear in the formula? I would also be interested to know if we can feed such complex systems of diff. equations into computational systems like Wolfram Alpha! Edit: I noticed that, for any invertable differentiable function , Hence, we can replace the partial deriviatives of above with Edit 2: Unfortunately unsuccessful attempt: Assume we are really lucky and for all at least one of the two functions , vanish. Then we have: In more conventional notation: Freely available math software (like Wolfram Alpha) can solve this integral, and so we get: If this were true, we would have one coordinate component of our embedding function nailed down. But it fails: If the assumption starting our current approach off holds, we have Which puts the following condition on our calculated component of : But, as simple differentiation shows, this equality is not satisfied.","M:=\mathbb{R}^{+}_0\times[-1,1]\times[-1,1] \mathcal{O}_{\text{St.}}|_M \mathcal{A}:=\{(M,\text{id}_M)\} (t,0,0)\in M t\in\mathbb{R}^{+}_0 p=(t,x,y) F_p:=\frac{G}{x^2+y^2} G 1 p \widehat{\text{F}}_p:=(\widehat{\text{F}}^0_p,\widehat{\text{F}}^1_p,\widehat{\text{F}}^2_p)\equiv\left(0,\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2},\frac{\frac{y}{x}}{x^2+y^2}\right) \nabla (p,q) (p,q) (0,0) C^\infty \Gamma \Gamma^i_{00}(p):=\widehat{\text{F}}^i_p M M \nabla \Gamma \phi:M\rightarrow\mathbb{R}^3 \nabla^* \nabla^E \nabla M \mathbb{R}^3 \mathbb{R}^3 \phi \nabla^*_X\ \phi^*(s)=\phi^*\left(\nabla^E_{\phi_*(X)}\ s\right) s\in T^1_0\mathbb{R}^3 \phi^{-1} \phi(M) M \phi^{-1}(p)=q \phi(q)=p \nabla^*_{e_i}\ \phi^*((\phi^{-1})^*(\in^j))=\nabla^*_{e_i}\ \in^j=\sum_q-\Gamma^{*q}_{ij}\cdot\in^q=\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right) e_i \in^j (M,\text{id}_M) \Gamma^{*q}_{ij}=\Gamma^q_{ij} \left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)=\left(\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)\right)(e_k)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k)) \left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k) C^\infty(M) \left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k)) C^\infty(\phi(M)) \phi^* (\phi(\cdot)) \left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)(\cdot)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))(\phi(\cdot)) (\phi(\cdot)) \sum_q\delta^q_k\cdot(-\Gamma^q_{ij})=\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)(\phi_*(e_k))-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right) -\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \in^j\bigg((\phi^{-1})^*(\phi_*(e_k))\bigg)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right) =\nabla^E_{\phi_*(e_i)}\ \in^j(e_k)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi^m_{*i}e^E_m}\ \phi^n_{*k}e^E_n\right) \nabla^E e^E_i \mathbb{R}^3 \phi^a_{*b} T_pM T_p\mathbb{R}^3 \phi_*(p) p\in M -\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \delta^j_k-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\nabla^E_{e^E_m}\ \phi^n_{*k}e^E_n\right) \stackrel{Leibnitz}{=}0-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\nabla^E_{e^E_m}\ e^E_n+(\nabla^E_{e^E_m}\ \phi^n_{*k})\cdot e^E_n\bigg)\right) =-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\Gamma^{E\ a}_{mn}\cdot e^E_a+e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right) \Gamma^{E\ q}_{ij} \Gamma^k_{ij}=((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\right)=\in^j\left((\phi^{-1})_*\bigg(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right) (\phi^{-1})_* \Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})_*(e^E_n))\right) (\phi^{-1})_{*a}^b T_p\mathbb{R}^3 T_pM (\phi^{-1})_* p \phi(M) (\phi^{-1})_{*a}^b C^\infty(\phi(M)) C^\infty(M) (\phi(\cdot)) \Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^a_{*n}\cdot e_a\right)=\delta^j_a\cdot\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n} \Gamma^k_{ij}=\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}\ \stackrel{Einstein}{:\iff}\ \Gamma^k_{ij}=\sum_m\sum_n\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n} \phi \mathbb{R}^3\supset(\mathbb{R}^{+}_0\times[-1,1]\times[-1,1]) \phi(M)\subset\mathbb{R}^3 \phi^a_{*b}(p)=\frac{\partial\phi^a}{\partial e_b}(p) (\phi^{-1})^a_{*b}(\phi(p))=\frac{\partial(\phi^{-1})^a(\phi(\cdot))}{\partial e_b}(p) \phi^a (\phi^{-1})^a a \phi \phi^{-1} \phi \left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_1}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2} \left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2} \left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_i}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_k}\right)\cdot \frac{\partial(\phi^{-1})^j}{\partial e_n}\right)(t,x,y)=0 (i,j,k)\in\{0,1,2\}^3 i\neq0 j\neq0 \phi \phi f:\mathbb{R}\rightarrow\mathbb{R} f^{-1}(f(x))=x\implies\left(\frac{\partial}{\partial x}(f^{-1}\circ f)\right)(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(x))\cdot \frac{\partial f}{\partial x}(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(\cdot))=\frac{1}{\frac{\partial f}{\partial x}}(\cdot) \phi^{-1} \frac{\partial(\phi^{-1})^0}{\partial e_n}(\phi(\cdot))=\frac{1}{\frac{\partial\phi^0}{\partial e_n}}(\cdot) (m,n)\in\{0,1,2\}^2\backslash\{(0,0)\} \frac{\partial\phi^m}{\partial e_0} \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right) \frac{\frac{y}{x}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)(t,x,y) \frac{\partial}{\partial t}\left(\frac{\partial\phi^0}{\partial y}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2} \implies\frac{\partial\phi^0}{\partial y}(t,x,y)=\int\frac{\frac{y}{x}}{x^2+y^2}\text{d}t=t\cdot\frac{\frac{y}{x}}{x^2+y^2} \implies\phi^0(t,x,y)=\int t\cdot\frac{\frac{y}{x}}{x^2+y^2}\text{d}y \phi^0(t,x,y)=t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x} \phi \frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)(t,x,y) \phi \frac{\partial}{\partial t}\left(\frac{\partial}{\partial x}\left(t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x}\right)\right)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}","['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'smooth-manifolds', 'connections']"
76,Intuition behind gradient being linear combination of constraint gradients in Lagrange multipliers,Intuition behind gradient being linear combination of constraint gradients in Lagrange multipliers,,"I already understand the intuition behind why the gradient of a function $f$ at its maximum $(x,y)$ subject to some constraint $g$ satisfies: $\nabla f(x,y) = \lambda\nabla g(x,y)$ For some constant $\lambda$ . There are a lot of depictions online of the single constraint case in 2D, where you see that gradients of a function at a point are always perpendicular to the level set of the function at that point. You then conclude that the gradient of $f$ and the gradient of $g$ must be parallel (just a verbal way of expressing the equation above), because $\nabla f$ points in the direction of steepest ascent, and if $f$ is differentiable then it's continuous and the tangent plane is a good local approximation, and if you could move in some direction that increased $f$ but that was also parallel to $\nabla g$ , you would be able to move along the level set of $g$ at $(x,y)$ and increase $f$ a little more without violating the constraint. My problem is that this intuition falls apart with two or more constraints. Somehow this ends up being true for arbitrarily high dimension: $\nabla f(x_1,\ldots,x_D) = \sum_{i=1}^n \lambda_i\nabla g_i(x_1,\ldots,x_D)$ I can see that if we stay in two dimensions and have two constraints, any two non-perpendicular vectors end up spanning the whole space so it must be the case that they can sum to $\nabla f$ . But if the number of dimensions is high, and the number of constraints is smaller than the number of dimensions, it's not obvious to me why $\nabla f$ must be a linear combination of $\nabla g_i$ . What I can accept, is that it must be the case that at the maximum moving in the direction of $\nabla f$ must require moving in a direction that has a non-zero projection onto at least one $\nabla g_i$ . In other words if we consider one pair of $(\nabla f, \nabla g_i)$ , two vectors always lie in some plane, and we can consider $\nabla f$ to be the sum of two vectors: one that is parallel to $\nabla g_i$ and one that is perpendicular to $\nabla g_i$ . Since it must be the case at the maximum that going further in the direction of $\nabla f$ would cause us to violate at least one constraint, there must be at least one $\nabla g_i$ where in a plane that only contains the two of them its part that is parallel to $\nabla f$ is non-zero. But I have no idea how we get from that to a linear combination of all constraints. How do I get an intuition for this? Maybe there is an intuitive visualization for the multiple constraints case? I haven't been able to find one.","I already understand the intuition behind why the gradient of a function at its maximum subject to some constraint satisfies: For some constant . There are a lot of depictions online of the single constraint case in 2D, where you see that gradients of a function at a point are always perpendicular to the level set of the function at that point. You then conclude that the gradient of and the gradient of must be parallel (just a verbal way of expressing the equation above), because points in the direction of steepest ascent, and if is differentiable then it's continuous and the tangent plane is a good local approximation, and if you could move in some direction that increased but that was also parallel to , you would be able to move along the level set of at and increase a little more without violating the constraint. My problem is that this intuition falls apart with two or more constraints. Somehow this ends up being true for arbitrarily high dimension: I can see that if we stay in two dimensions and have two constraints, any two non-perpendicular vectors end up spanning the whole space so it must be the case that they can sum to . But if the number of dimensions is high, and the number of constraints is smaller than the number of dimensions, it's not obvious to me why must be a linear combination of . What I can accept, is that it must be the case that at the maximum moving in the direction of must require moving in a direction that has a non-zero projection onto at least one . In other words if we consider one pair of , two vectors always lie in some plane, and we can consider to be the sum of two vectors: one that is parallel to and one that is perpendicular to . Since it must be the case at the maximum that going further in the direction of would cause us to violate at least one constraint, there must be at least one where in a plane that only contains the two of them its part that is parallel to is non-zero. But I have no idea how we get from that to a linear combination of all constraints. How do I get an intuition for this? Maybe there is an intuitive visualization for the multiple constraints case? I haven't been able to find one.","f (x,y) g \nabla f(x,y) = \lambda\nabla g(x,y) \lambda f g \nabla f f f \nabla g g (x,y) f \nabla f(x_1,\ldots,x_D) = \sum_{i=1}^n \lambda_i\nabla g_i(x_1,\ldots,x_D) \nabla f \nabla f \nabla g_i \nabla f \nabla g_i (\nabla f, \nabla g_i) \nabla f \nabla g_i \nabla g_i \nabla f \nabla g_i \nabla f","['calculus', 'multivariable-calculus', 'vector-spaces', 'lagrange-multiplier']"
77,A singular integral,A singular integral,,"So, I was doing some PDE related computations and I obtained the following integral $$ \iint \frac{(y-y')\,f(x',y')}{(|x-x'|^2+|y-y'|^2)^\frac{3}{2}}\, \left|\frac{x'}{x}\right|^2 \,\mathrm{d} x'\,\mathrm{d} y' $$ where $f$ is a continuous and compactly supported function, odd with respect to its first variable. Is this integral bounded (uniformly with respect to $x$ and $y$ )? I tried to do several changes of variable (for example one can replace $x'$ by $xx'$ to put the singularity in $x$ inside the part written $(..)^\frac{3}{2}$ ). I had a look on litterature on singular integrals but things seems usually more symmetric in these kind of theories. Any ideas, examples, counterexamples or proof of a result are welcome!","So, I was doing some PDE related computations and I obtained the following integral where is a continuous and compactly supported function, odd with respect to its first variable. Is this integral bounded (uniformly with respect to and )? I tried to do several changes of variable (for example one can replace by to put the singularity in inside the part written ). I had a look on litterature on singular integrals but things seems usually more symmetric in these kind of theories. Any ideas, examples, counterexamples or proof of a result are welcome!","
\iint \frac{(y-y')\,f(x',y')}{(|x-x'|^2+|y-y'|^2)^\frac{3}{2}}\, \left|\frac{x'}{x}\right|^2 \,\mathrm{d} x'\,\mathrm{d} y'
 f x y x' xx' x (..)^\frac{3}{2}","['multivariable-calculus', 'convolution', 'integral-inequality', 'functional-inequalities', 'singular-integrals']"
78,Please verify my proof of Gauss's Law,Please verify my proof of Gauss's Law,,"Can someone please verify my proof of Gauss's Law? I directly copied it from my assignment, and it is an extra credit assignment, so I just want someone to tell me if it is completely correct or whether I made an assumption or there is something missing in my proof. We prove Gauss's Law for a single point charge, then use the superposition principle of the electric field to generalize it for multiple charges. We start from Coulomb's Law for the Electric Field: $$\mathbf{E}=\frac{1}{4\pi\varepsilon_0}\frac{Q}{r^2}\hat{\mathbf{r}}$$ where $+Q$ is a point charge. Negative charges change the direction of the entire field, so this proof will work for negative charges as well. Define the origin to be the location of this charge. Mathematically, we can write Coulomb's Law as $$\mathbf{E}=\frac{1}{4\pi\varepsilon_0}\frac{Q}{\lVert{\mathbf{r}\rVert}^3}\mathbf{r}$$ where $\mathbf{r}$ is the outward radial vector relative to the charge. \\ Consider an arbitrary closed surface $S$ that completely surrounds the charge. Consider an arbitrary tiny differential area element vector $d\mathbf{S}$ on $S$ that is normal to $S$ at that point. Then the differential electric flux $d\Phi_E$ is given by $$d\Phi_E=\mathbf{E}\cdot d\mathbf{S}$$ so if we take the closed surface integral around the surface, we get $$\Phi_E=\iint_S\mathbf{E}\cdot d\mathbf{S}$$ or $$\Phi_E=\frac{Q}{4\pi\varepsilon_0}\iint_S\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S}$$ We now show that the integral $$\iint_S\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S}=4\pi$$ We begin with the Divergence Theorem, which states that $$\iiint_V \nabla\cdot\mathbf{E}\;dV=\iint_{\partial V} \mathbf{F}\cdot d\mathbf{S}$$ where $V$ is a simply-connected closed region and $\partial V$ is the boundary surface of $V$ . For simplicity, let $$\mathbf{e}=\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}$$ It can be shown that $\nabla\cdot\mathbf{e}=0$ by writing $$\mathbf{e}=\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}=\frac{x\mathbf{i}+y\mathbf{j}+z\mathbf{k}}{(x^2+y^2+z^2)^{3/2}}$$ and directly using the definition of divergence. We cannot, however, use the divergence theorem directly on the region enclosed by $S$ on $\mathbf{e}$ since $\mathbf{e}$ is not defined at the origin. \\ We now consider a new region $E$ that is such that the outer boundary surface of $E$ is $S$ and the inner boundary of the surface is a sphere centered at the origin with radius $a$ such that the sphere is entirely contained within $S$ . Call the sphere $S_0$ . In this way, the origin is excluded from $E$ which allows us to use the divergence theorem. Now, $$\iiint_E \nabla\cdot\mathbf{e}\: dV = \iint_{\partial E} \mathbf{e}\cdot d\mathbf{S}=\iint_{\partial E} \mathbf{e}\cdot\mathbf{n}\: dS$$ where $\mathbf{n}$ is the outward unit normal vector. Now, $$\iint_{\partial E}\mathbf{e}\cdot\mathbf{n}\: dS=\iint_S \mathbf{e}\cdot\mathbf{n}_1\: dS+\iint_{S_0}\mathbf{e}\cdot(-\mathbf{n}_2)\: dS$$ Where $\mathbf{n}_1$ represents the outward normal of $S$ and $\mathbf{n}_2$ represents the outward normal of $S_0$ . Note that since the outward normal of $\partial E$ is actually the inward normal of $S_0$ , we must add a negative sign to account for this. As a result, we have $$\iiint_E \nabla\cdot\mathbf{e}\: dV=\iint_S \mathbf{e}\cdot d\mathbf{S}-\iint_{S_0}\mathbf{e}\cdot d\mathbf{S}$$ However, we said earlier that $\nabla\cdot\mathbf{e}=0$ , so we get $$\iint_S\mathbf{e}\cdot d\mathbf{S}=\iint_{S_0}\mathbf{e}\cdot d\mathbf{S}$$ This makes it so that the flux of the electric field through any closed surface is equal to the flux of the electric field through a sphere, which easy to find. We have $$\iint_{S_0}\mathbf{e}\cdot d\mathbf{S}=\iint_{S_0}\mathbf{e}\cdot\mathbf{n} \: dS=\iint_{S_0}\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}}\: dS=\iint_{S_0}\frac{1}{a^2} dS $$ where the last part is found by using the unit normal vector of a sphere and the fact that $\mathbf{r}\cdot\mathbf{r}=\lVert{\mathbf{r}\rVert}^2$ . This gets us $$\iint_{S_0}\frac{1}{a^2}dS=\frac{1}{a^2}4\pi a^2=4\pi$$ We just showed that the flux through the sphere is $4\pi$ which means that the flux through the arbitrary surface $S$ is also $4\pi$ . Going back to the initial equation, we get $$\Phi_E=\frac{Q}{4\pi\varepsilon_0}\iint_S \frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S}=\frac{4\pi Q}{4\pi\varepsilon_0}=\frac{Q}{\varepsilon_0}$$ This shows that Gauss's Law holds for one charge. \\ Now, if there are multiple charges $q_1, q_2, \ldots, q_n$ in the surface, then the superposition principle states that the total electric field $\mathbf{E}$ is just the vector sum of the individual electric fields, or $$\mathbf{E}=\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n$$ Thus, $$\Phi_E=\iint_S\mathbf{E}\cdot d\mathbf{S}=\iint_S (\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n)\cdot d\mathbf{S}=\iint_S\mathbf{E}_1\cdot d\mathbf{S}+\iint_S\mathbf{E}_2 \cdot d\mathbf{S}+\ldots+\iint_S\mathbf{E}_n\cdot d\mathbf{S}$$ This equals $$\frac{q_1}{\varepsilon_0}+\frac{q_2}{\varepsilon_0}+\ldots+\frac{q_n}{\varepsilon_0}=\frac{q_1+q_2+\ldots+q_n}{\varepsilon_0}=\frac{q_{enc}}{\varepsilon_0}$$ Using this addition, we can see that Gauss's Law works for continuous charge distributions as well since a continuous charge distribution is simply made up of infinitely many point charges whose electric fields add due to superposition. \\ Finally, say that the charge is outside the closed surface. This means that the closed surface does not enclose the origin. Since $\nabla\cdot\mathbf{E}=0$ , the Divergence Theorem gives $$\iint_S \mathbf{E}\cdot d\mathbf{S}=\iiint_E \nabla\cdot\mathbf{E}\: dV=0$$ where $E$ is the region with $S$ as its boundary. Now because $\mathbf{E}$ is defined everywhere inside the region (as the only place $\mathbf{E}$ isn't defined is the origin, which we have already said is outside the region), we see that $\Phi_E=0$ . This easily extends to multiple charges outside the surface, since the divergence operator has a distributive property if each electric field is defined in the region, which it is. That is, $$\nabla\cdot(\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n)=\nabla\cdot\mathbf{E}_1+\nabla\cdot\mathbf{E}_2+\ldots+\nabla\cdot\mathbf{E}_n$$ which equals zero since each term equals zero. \\ We have thus proved Gauss's Law for the electric field.","Can someone please verify my proof of Gauss's Law? I directly copied it from my assignment, and it is an extra credit assignment, so I just want someone to tell me if it is completely correct or whether I made an assumption or there is something missing in my proof. We prove Gauss's Law for a single point charge, then use the superposition principle of the electric field to generalize it for multiple charges. We start from Coulomb's Law for the Electric Field: where is a point charge. Negative charges change the direction of the entire field, so this proof will work for negative charges as well. Define the origin to be the location of this charge. Mathematically, we can write Coulomb's Law as where is the outward radial vector relative to the charge. \\ Consider an arbitrary closed surface that completely surrounds the charge. Consider an arbitrary tiny differential area element vector on that is normal to at that point. Then the differential electric flux is given by so if we take the closed surface integral around the surface, we get or We now show that the integral We begin with the Divergence Theorem, which states that where is a simply-connected closed region and is the boundary surface of . For simplicity, let It can be shown that by writing and directly using the definition of divergence. We cannot, however, use the divergence theorem directly on the region enclosed by on since is not defined at the origin. \\ We now consider a new region that is such that the outer boundary surface of is and the inner boundary of the surface is a sphere centered at the origin with radius such that the sphere is entirely contained within . Call the sphere . In this way, the origin is excluded from which allows us to use the divergence theorem. Now, where is the outward unit normal vector. Now, Where represents the outward normal of and represents the outward normal of . Note that since the outward normal of is actually the inward normal of , we must add a negative sign to account for this. As a result, we have However, we said earlier that , so we get This makes it so that the flux of the electric field through any closed surface is equal to the flux of the electric field through a sphere, which easy to find. We have where the last part is found by using the unit normal vector of a sphere and the fact that . This gets us We just showed that the flux through the sphere is which means that the flux through the arbitrary surface is also . Going back to the initial equation, we get This shows that Gauss's Law holds for one charge. \\ Now, if there are multiple charges in the surface, then the superposition principle states that the total electric field is just the vector sum of the individual electric fields, or Thus, This equals Using this addition, we can see that Gauss's Law works for continuous charge distributions as well since a continuous charge distribution is simply made up of infinitely many point charges whose electric fields add due to superposition. \\ Finally, say that the charge is outside the closed surface. This means that the closed surface does not enclose the origin. Since , the Divergence Theorem gives where is the region with as its boundary. Now because is defined everywhere inside the region (as the only place isn't defined is the origin, which we have already said is outside the region), we see that . This easily extends to multiple charges outside the surface, since the divergence operator has a distributive property if each electric field is defined in the region, which it is. That is, which equals zero since each term equals zero. \\ We have thus proved Gauss's Law for the electric field.","\mathbf{E}=\frac{1}{4\pi\varepsilon_0}\frac{Q}{r^2}\hat{\mathbf{r}} +Q \mathbf{E}=\frac{1}{4\pi\varepsilon_0}\frac{Q}{\lVert{\mathbf{r}\rVert}^3}\mathbf{r} \mathbf{r} S d\mathbf{S} S S d\Phi_E d\Phi_E=\mathbf{E}\cdot d\mathbf{S} \Phi_E=\iint_S\mathbf{E}\cdot d\mathbf{S} \Phi_E=\frac{Q}{4\pi\varepsilon_0}\iint_S\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S} \iint_S\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S}=4\pi \iiint_V \nabla\cdot\mathbf{E}\;dV=\iint_{\partial V} \mathbf{F}\cdot d\mathbf{S} V \partial V V \mathbf{e}=\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3} \nabla\cdot\mathbf{e}=0 \mathbf{e}=\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}=\frac{x\mathbf{i}+y\mathbf{j}+z\mathbf{k}}{(x^2+y^2+z^2)^{3/2}} S \mathbf{e} \mathbf{e} E E S a S S_0 E \iiint_E \nabla\cdot\mathbf{e}\: dV = \iint_{\partial E} \mathbf{e}\cdot d\mathbf{S}=\iint_{\partial E} \mathbf{e}\cdot\mathbf{n}\: dS \mathbf{n} \iint_{\partial E}\mathbf{e}\cdot\mathbf{n}\: dS=\iint_S \mathbf{e}\cdot\mathbf{n}_1\: dS+\iint_{S_0}\mathbf{e}\cdot(-\mathbf{n}_2)\: dS \mathbf{n}_1 S \mathbf{n}_2 S_0 \partial E S_0 \iiint_E \nabla\cdot\mathbf{e}\: dV=\iint_S \mathbf{e}\cdot d\mathbf{S}-\iint_{S_0}\mathbf{e}\cdot d\mathbf{S} \nabla\cdot\mathbf{e}=0 \iint_S\mathbf{e}\cdot d\mathbf{S}=\iint_{S_0}\mathbf{e}\cdot d\mathbf{S} \iint_{S_0}\mathbf{e}\cdot d\mathbf{S}=\iint_{S_0}\mathbf{e}\cdot\mathbf{n} \: dS=\iint_{S_0}\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot\frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}}\: dS=\iint_{S_0}\frac{1}{a^2} dS  \mathbf{r}\cdot\mathbf{r}=\lVert{\mathbf{r}\rVert}^2 \iint_{S_0}\frac{1}{a^2}dS=\frac{1}{a^2}4\pi a^2=4\pi 4\pi S 4\pi \Phi_E=\frac{Q}{4\pi\varepsilon_0}\iint_S \frac{\mathbf{r}}{\lVert{\mathbf{r}\rVert}^3}\cdot d\mathbf{S}=\frac{4\pi Q}{4\pi\varepsilon_0}=\frac{Q}{\varepsilon_0} q_1, q_2, \ldots, q_n \mathbf{E} \mathbf{E}=\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n \Phi_E=\iint_S\mathbf{E}\cdot d\mathbf{S}=\iint_S (\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n)\cdot d\mathbf{S}=\iint_S\mathbf{E}_1\cdot d\mathbf{S}+\iint_S\mathbf{E}_2
\cdot d\mathbf{S}+\ldots+\iint_S\mathbf{E}_n\cdot d\mathbf{S} \frac{q_1}{\varepsilon_0}+\frac{q_2}{\varepsilon_0}+\ldots+\frac{q_n}{\varepsilon_0}=\frac{q_1+q_2+\ldots+q_n}{\varepsilon_0}=\frac{q_{enc}}{\varepsilon_0} \nabla\cdot\mathbf{E}=0 \iint_S \mathbf{E}\cdot d\mathbf{S}=\iiint_E \nabla\cdot\mathbf{E}\: dV=0 E S \mathbf{E} \mathbf{E} \Phi_E=0 \nabla\cdot(\mathbf{E}_1+\mathbf{E}_2+\ldots+\mathbf{E}_n)=\nabla\cdot\mathbf{E}_1+\nabla\cdot\mathbf{E}_2+\ldots+\nabla\cdot\mathbf{E}_n","['multivariable-calculus', 'solution-verification', 'divergence-theorem']"
79,Orthogonal derivative implies second derivative is null,Orthogonal derivative implies second derivative is null,,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be twice differentiable, such that $f'(x)$ is an orthogonal linear transformation for every $x\in\mathbb{R}^n$ . Prove that $f''(x) = 0$ , for every $x\in\mathbb{R}^n$ . I've been struggling with this one, apparently it doesn't even use the inverse function theorem. Mi idea was to try and use the fact that $f'(x)$ preserves the norm, but that didn't work.","Let be twice differentiable, such that is an orthogonal linear transformation for every . Prove that , for every . I've been struggling with this one, apparently it doesn't even use the inverse function theorem. Mi idea was to try and use the fact that preserves the norm, but that didn't work.",f:\mathbb{R}^n\to \mathbb{R}^n f'(x) x\in\mathbb{R}^n f''(x) = 0 x\in\mathbb{R}^n f'(x),"['real-analysis', 'multivariable-calculus', 'derivatives', 'linear-transformations', 'frechet-derivative']"
80,Wrong counterexamples with 2nd derivative test,Wrong counterexamples with 2nd derivative test,,"Today we learned the 2nd Derivative Test of two-variable functions for testing local minimum/maximum points, and saddle points. We have deduced the law by calculating $$D^2_{\mathbf{v}} f=D_{\mathbf{V}} \left(\nabla f  \cdot \mathbf{v}  \right) = \mathbf{v}^{T} H \mathbf{v},$$ where $H$ is the Hessian matrix $\left[\begin {matrix}f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{matrix}\right].$ However, we discovered that by letting $\mathbf{v}=\left<a,b\right>$ where $||\mathbf{v}||=1$ , $$D_{\mathbf{v}}^2 f= f_{xx}a^2+2f_{xy}ab+f_{yy}b^2,$$ hence if $f_{xx}$ and $f_{yy}$ are both nonzero, then $D_{\mathbf{v}}^2 f$ changes its sign at most two times while $\mathbf{v}$ rotates clockwise on the unit circle.  We thought that this result was absurd, so we tried to construct a counterexample of it. This is what we got: if a function $z=f(x,y)$ is twice differentiable and satisfies $f(t,0)=-At^2, f(0, t)=-Bt^2, f(t, t)=Ct^2, f(t, -t)=Dt^2$ for some $A,B,C,D>0$ and for all sufficiently small $t$ , then $f_{xx}$ and $f_{yy}$ are both nonzero, but the sign of $D_{\mathbf{v}}^2 f$ changes four times as $\mathbf{v}$ rotates once. Comparing with the previous argument, it seems like such function does not exist, but we couldn't prove it. We do not know which is true too. Please help us.","Today we learned the 2nd Derivative Test of two-variable functions for testing local minimum/maximum points, and saddle points. We have deduced the law by calculating where is the Hessian matrix However, we discovered that by letting where , hence if and are both nonzero, then changes its sign at most two times while rotates clockwise on the unit circle.  We thought that this result was absurd, so we tried to construct a counterexample of it. This is what we got: if a function is twice differentiable and satisfies for some and for all sufficiently small , then and are both nonzero, but the sign of changes four times as rotates once. Comparing with the previous argument, it seems like such function does not exist, but we couldn't prove it. We do not know which is true too. Please help us.","D^2_{\mathbf{v}} f=D_{\mathbf{V}} \left(\nabla f  \cdot \mathbf{v}  \right) = \mathbf{v}^{T} H \mathbf{v}, H \left[\begin
{matrix}f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{matrix}\right]. \mathbf{v}=\left<a,b\right> ||\mathbf{v}||=1 D_{\mathbf{v}}^2 f= f_{xx}a^2+2f_{xy}ab+f_{yy}b^2, f_{xx} f_{yy} D_{\mathbf{v}}^2 f \mathbf{v} z=f(x,y) f(t,0)=-At^2, f(0, t)=-Bt^2, f(t, t)=Ct^2, f(t, -t)=Dt^2 A,B,C,D>0 t f_{xx} f_{yy} D_{\mathbf{v}}^2 f \mathbf{v}","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
81,Is it correct to move the $\lim$ operator inside $\| \cdot \|$ in deriving this partial derivative?,Is it correct to move the  operator inside  in deriving this partial derivative?,\lim \| \cdot \|,"Let $X$ be open in $\mathbb R^n$ , $F$ a Banach space, and $m \in \mathbb N^*$ . Suppose $f:X \to F$ such that $\partial_{j_1} \cdots \partial_{j_{m+1}} f$ and $\partial^m f$ exist in a neighborhood of $a$ for all $j_1, \ldots, j_{m+1} \in \{1,\ldots,n\}$ . Assume $h^i = \left  (h_1^i, \ldots, h_n^i\right ) \in \mathbb R^n$ with $1 \le i \le m$ .  We define a map $A$ by $$\begin{array}{l|rcl} A & {(\mathbb R^n)}^m  & \longrightarrow & F \\     & \left  [h^1, \ldots,h^m\right ] & \longmapsto & \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \end{array}$$ I have verified that $A$ is a multilinear map and thus $A \in \mathcal L^m(\mathbb R^n, F)$ . Moreover, it follows from the definition of mixed partial derivative that $\partial^m f(a) \in L^m(\mathbb R^n, F)$ . In the following, I try to prove that $$\partial_j (\partial^m f)(a) = A$$ I'm not sure if, in (5), I correctly move the lim operator inside the norm operator. Could you please verify if this step is correct? Thank you so much! My attempt: First, we have $$\begin{aligned} &\frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \\ ={}& \frac{\partial^m f(a +te_j)[h^1, \ldots,h^m] - \partial^m f(a) [h^1, \ldots,h^m]}{t}  - A [h^1, \ldots,h^m]\\ ={}&  \frac{ \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) \left   (h^1_{j_1} \cdots h^m_{j_m}\right )-  \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right )}{t}  \\ & \quad - \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1} \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \\ ={}&  \sum_{j_1, \ldots, j_m =1}^n \left (h^1_{j_1} \cdots h^m_{j_m} \right) \cdot \left ( \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) -\partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}   -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right ) \end{aligned}$$ It follows that $$\begin{aligned}    & \lim_{t \to 0} \left \| \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} - A\right \| \\ \overset{(1)}{=}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le1,\ldots,\|h^m\|_1 \le 1} \left \|  \sum_{j_1, \ldots, j_m =1}^n  \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \right \|\\ \overset{(2)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \left |h^1_{j_1} \cdots h^m_{j_m} \right | \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\ \overset{(3)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \|h^1\|_1 \cdots \|h^m\|_1  \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\ &  \quad -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\bigg \|\\ \overset{(4)}{\le}{}& \lim_{t \to 0}  \sum_{j_1, \ldots, j_m =1}^n \left \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|\\  ={}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \lim_{t \to 0} \left \|  \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\ \overset{(5)}{=}{}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \left \| \lim_{t \to 0} \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\ ={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \left ( \partial_{j_1}  \cdots \partial_{j_m} f \right ) (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\ ={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\ ={}&  0 \end{aligned}$$ $(1)$ : This follows from the definition of the operator norm of a multilinear map. $(2)$ : This follows from triangle inequality. $(3)$ : It follows from the definition of $\|\cdot\|_1$ that $|h^1_j| \le \|h^1\|_1,\ldots, |h^m_j| \le \|h^m\|_1$ for all $j \in \{1,\ldots,n\}$ . As such, $\left |h^1_{j_1} \cdots h^m_{j_m} \right | =\left |h^1_{j_1}\right | \cdots \left | h^m_{j_m} \right | \le \|h^1\|_1 \cdots \|h^m\|_1$ . $(4)$ : It follows from $\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1$ that $\|h^1\|_1 \cdots \|h^m\|_1 \le 1$ . Hence $$\partial_j (\partial^m f)(a) = A$$","Let be open in , a Banach space, and . Suppose such that and exist in a neighborhood of for all . Assume with .  We define a map by I have verified that is a multilinear map and thus . Moreover, it follows from the definition of mixed partial derivative that . In the following, I try to prove that I'm not sure if, in (5), I correctly move the lim operator inside the norm operator. Could you please verify if this step is correct? Thank you so much! My attempt: First, we have It follows that : This follows from the definition of the operator norm of a multilinear map. : This follows from triangle inequality. : It follows from the definition of that for all . As such, . : It follows from that . Hence","X \mathbb R^n F m \in \mathbb N^* f:X \to F \partial_{j_1} \cdots \partial_{j_{m+1}} f \partial^m f a j_1, \ldots, j_{m+1} \in \{1,\ldots,n\} h^i = \left  (h_1^i, \ldots, h_n^i\right ) \in \mathbb R^n 1 \le i \le m A \begin{array}{l|rcl}
A & {(\mathbb R^n)}^m
 & \longrightarrow & F \\
    & \left  [h^1, \ldots,h^m\right ] & \longmapsto & \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \end{array} A A \in \mathcal L^m(\mathbb R^n, F) \partial^m f(a) \in L^m(\mathbb R^n, F) \partial_j (\partial^m f)(a) = A \begin{aligned}
&\frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \\
={}& \frac{\partial^m f(a +te_j)[h^1, \ldots,h^m] - \partial^m f(a) [h^1, \ldots,h^m]}{t}  - A [h^1, \ldots,h^m]\\
={}&  \frac{ \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) \left   (h^1_{j_1} \cdots h^m_{j_m}\right )-  \sum_{j_1, \ldots, j_m =1}^n \partial_{j_1}  \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right )}{t}  \\
& \quad - \sum_{j_1, \ldots, j_m =1}^n  \partial_j \partial_{j_1} \cdots \partial_{j_m} f (a)  \left  (h^1_{j_1} \cdots h^m_{j_m}\right ) \\
={}&  \sum_{j_1, \ldots, j_m =1}^n \left (h^1_{j_1} \cdots h^m_{j_m} \right) \cdot \left ( \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) -\partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}   -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right )
\end{aligned} \begin{aligned}
   & \lim_{t \to 0} \left \| \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} - A\right \| \\
\overset{(1)}{=}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le1,\ldots,\|h^m\|_1 \le 1} \left \|  \sum_{j_1, \ldots, j_m =1}^n  \frac{\partial^m f(a +te_j) - \partial^m f(a)}{t} \left  [h^1, \ldots,h^m\right ] - A \left  [h^1, \ldots,h^m\right ] \right \|\\
\overset{(2)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \left |h^1_{j_1} \cdots h^m_{j_m} \right | \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\
\overset{(3)}{\le}{}& \lim_{t \to 0} \sup_{\|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1}   \sum_{j_1, \ldots, j_m =1}^n \|h^1\|_1 \cdots \|h^m\|_1  \cdot\bigg \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t}  \\
&  \quad -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\bigg \|\\
\overset{(4)}{\le}{}& \lim_{t \to 0}  \sum_{j_1, \ldots, j_m =1}^n \left \| \frac{ \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|\\
 ={}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \lim_{t \to 0} \left \|  \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\
\overset{(5)}{=}{}&  \color{blue}{\sum_{j_1, \ldots, j_m =1}^n \left \| \lim_{t \to 0} \frac{  \partial_{j_1}  \cdots \partial_{j_m} f (a + te_j) - \partial_{j_1}  \cdots \partial_{j_m} f (a) }{t} -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)\right \|}\\
={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \left ( \partial_{j_1}  \cdots \partial_{j_m} f \right ) (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\
={}&   \sum_{j_1, \ldots, j_m =1}^n \left \|  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a)  -  \partial_j \partial_{j_1}  \cdots \partial_{j_m} f (a) \right \|\\
={}&  0
\end{aligned} (1) (2) (3) \|\cdot\|_1 |h^1_j| \le \|h^1\|_1,\ldots, |h^m_j| \le \|h^m\|_1 j \in \{1,\ldots,n\} \left |h^1_{j_1} \cdots h^m_{j_m} \right | =\left |h^1_{j_1}\right | \cdots \left | h^m_{j_m} \right | \le \|h^1\|_1 \cdots \|h^m\|_1 (4) \|h^1\|_1\le 1,\ldots,\|h^m\|_1 \le 1 \|h^1\|_1 \cdots \|h^m\|_1 \le 1 \partial_j (\partial^m f)(a) = A","['real-analysis', 'multivariable-calculus', 'proof-verification', 'derivatives', 'partial-derivative']"
82,Why doesn't the limit in polar form imply the existence of the limit,Why doesn't the limit in polar form imply the existence of the limit,,"First, I'll give my definition of polar coordinates. Let $f:U\subseteq\mathbb{R}^2\to\mathbb{R}$ . Changing to polar coordinates we get $g:V\subseteq\mathbb{R}^2\to\mathbb{R},g(r,\theta)=f(r\cos\theta,r\sin\theta)$ , where $r>0$ and $\theta\in[0,2\pi)$ . Now my question is, why is this true: $$\lim_{(x,y)\to(0,0)} f(x,y)=L \Rightarrow \lim_{r\to0^+} g(r,\theta)=L $$ . But this isn't: $$\lim_{r\to0^+} g(r,\theta)=L\Rightarrow\lim_{(x,y)\to(0,0)} f(x,y)=L$$ I've seen various counterexamples of this last statement so I know it is false, however I can't really understand why it doesn't work, so any help will be appreciated. Thanks.","First, I'll give my definition of polar coordinates. Let . Changing to polar coordinates we get , where and . Now my question is, why is this true: . But this isn't: I've seen various counterexamples of this last statement so I know it is false, however I can't really understand why it doesn't work, so any help will be appreciated. Thanks.","f:U\subseteq\mathbb{R}^2\to\mathbb{R} g:V\subseteq\mathbb{R}^2\to\mathbb{R},g(r,\theta)=f(r\cos\theta,r\sin\theta) r>0 \theta\in[0,2\pi) \lim_{(x,y)\to(0,0)} f(x,y)=L \Rightarrow \lim_{r\to0^+} g(r,\theta)=L  \lim_{r\to0^+} g(r,\theta)=L\Rightarrow\lim_{(x,y)\to(0,0)} f(x,y)=L","['calculus', 'multivariable-calculus', 'polar-coordinates']"
83,A general formula for the parametrisation of the tangent space (in $\Bbb{R}^n$),A general formula for the parametrisation of the tangent space (in ),\Bbb{R}^n,"I couldn't find a sufficiently general formula for the tangent plane of a parametric surface, so I derived one. The formula I came up with also defines the tangent line of a parametric curve in $n$ -dimensions, the tangent plane to an explicitly defined surface, and the tangent plane to a parametric surface in $n$ -dimensions: Let $\mathbf{f}:\Bbb{R}^m\to\Bbb{R}^n:m<n$ be a parametrisation of an $m$ -dimensional geometric object (possibly but not necessarily a   manifold, algebraic variety, etc.) in Euclidean $n$ -space. The parametrisation of the tangent   space $T_\mathbf{u}\mathbf{f}:\Bbb{R}^m\to\Bbb{R}^n$ at a point $\mathbf{u}\in\Bbb{R}^m$ is given by... $$T_\mathbf{u}\mathbf{f}(\mathbf{x})=\mathbf{J}_\mathbf{f}(\mathbf{u})(\mathbf{x}-\mathbf{u})+\mathbf{f}(\mathbf{u})$$ ...where $\mathbf{J}_\mathbf{f}(\mathbf{u})$ is the Jacobian matrix of $\mathbf{f}$ evaluated at $\mathbf{u}$ . The resulting parametrisation preserves information about local scaling and orientation within the coordinate system. This is particularly noticeable when considering changes to the basis vectors $^*$ of the tangent space between two points $\mathbf{u}$ and $\mathbf{u}'$ along a curve, as shown below (I apologise for the quality, upload limits and all that)... Edit Originally I had written this formula as... $$T_\mathbf{u}f(\mathbf{x})=\sum_i\left(\nabla f_i(\mathbf{u})\cdot(\mathbf{x}-\mathbf{u})+f_i(\mathbf{u})\right)\mathbf{e}^i$$ After Ted Shifrin's comment I realise that this is easier expressed in terms of the Jacobian. Shortly after editing this question, I found this on the Wikipedia article about the Jacobian... $${\displaystyle \mathbf {f} (\mathbf {x} )-\mathbf {f} (\mathbf {p} )=\mathbf {J} _{\mathbf {f} }(\mathbf {p} )(\mathbf {x} -\mathbf {p})+o(\|\mathbf {x} -\mathbf {p} \|)\quad ({\text{as }}\mathbf {x} \to \mathbf {p} )}$$ ...which is very close to my formula. The fact that the Jacobian is the closest linear approximation of vector-valued function explains why $T_\mathbf{u}\mathbf{f}$ is a close linear approximation of $\mathbf{f}$ near $\mathbf{u}$ , but does not explain why this gives the tangent space whenever $m<n$ . Intuitively, this seems like a generalisation of Taylor's theorem to vector-valued functions. Indeed, the equation of the tangent line is given by the Taylor polynomial of degree $1$ ... $$y=f'(a)(x-a)+f(a)$$ I'm not quite sure how to get from here to my formula though, or how to show that $T_\mathbf{u}\mathbf{f}$ is the tangent space of $\mathbf{f}$ for all [differentiable] $f:\Bbb{R}^m\to\Bbb{R}^n$ .","I couldn't find a sufficiently general formula for the tangent plane of a parametric surface, so I derived one. The formula I came up with also defines the tangent line of a parametric curve in -dimensions, the tangent plane to an explicitly defined surface, and the tangent plane to a parametric surface in -dimensions: Let be a parametrisation of an -dimensional geometric object (possibly but not necessarily a   manifold, algebraic variety, etc.) in Euclidean -space. The parametrisation of the tangent   space at a point is given by... ...where is the Jacobian matrix of evaluated at . The resulting parametrisation preserves information about local scaling and orientation within the coordinate system. This is particularly noticeable when considering changes to the basis vectors of the tangent space between two points and along a curve, as shown below (I apologise for the quality, upload limits and all that)... Edit Originally I had written this formula as... After Ted Shifrin's comment I realise that this is easier expressed in terms of the Jacobian. Shortly after editing this question, I found this on the Wikipedia article about the Jacobian... ...which is very close to my formula. The fact that the Jacobian is the closest linear approximation of vector-valued function explains why is a close linear approximation of near , but does not explain why this gives the tangent space whenever . Intuitively, this seems like a generalisation of Taylor's theorem to vector-valued functions. Indeed, the equation of the tangent line is given by the Taylor polynomial of degree ... I'm not quite sure how to get from here to my formula though, or how to show that is the tangent space of for all [differentiable] .",n n \mathbf{f}:\Bbb{R}^m\to\Bbb{R}^n:m<n m n T_\mathbf{u}\mathbf{f}:\Bbb{R}^m\to\Bbb{R}^n \mathbf{u}\in\Bbb{R}^m T_\mathbf{u}\mathbf{f}(\mathbf{x})=\mathbf{J}_\mathbf{f}(\mathbf{u})(\mathbf{x}-\mathbf{u})+\mathbf{f}(\mathbf{u}) \mathbf{J}_\mathbf{f}(\mathbf{u}) \mathbf{f} \mathbf{u} ^* \mathbf{u} \mathbf{u}' T_\mathbf{u}f(\mathbf{x})=\sum_i\left(\nabla f_i(\mathbf{u})\cdot(\mathbf{x}-\mathbf{u})+f_i(\mathbf{u})\right)\mathbf{e}^i {\displaystyle \mathbf {f} (\mathbf {x} )-\mathbf {f} (\mathbf {p} )=\mathbf {J} _{\mathbf {f} }(\mathbf {p} )(\mathbf {x} -\mathbf {p})+o(\|\mathbf {x} -\mathbf {p} \|)\quad ({\text{as }}\mathbf {x} \to \mathbf {p} )} T_\mathbf{u}\mathbf{f} \mathbf{f} \mathbf{u} m<n 1 y=f'(a)(x-a)+f(a) T_\mathbf{u}\mathbf{f} \mathbf{f} f:\Bbb{R}^m\to\Bbb{R}^n,"['geometry', 'multivariable-calculus', 'differential-geometry', 'parametrization', 'tangent-spaces']"
84,How to prove existence of directional derivates and discontinuity on same point?,How to prove existence of directional derivates and discontinuity on same point?,,"\begin{align} f(x,y) = \begin{cases} \frac{x^3y}{x^6 +y^2} & \text{ if } (x,y) ≠ (0,0) \\ \\  0 & \text{ if } (x,y) = (0,0) \end{cases}. \end{align} Prove that all directional derivates exist on $(0,0)$ and prove that $f$ is discontinuous on $(0,0).$ For the first part I used a generic vector $v=(v_1,v_2)$ and the definition I have of a directional derivate: \begin{align} \frac{\partial f}{\partial v} (x_0,y_0)= \lim_{h\to 0}\frac{f(x_0 + hv_1, y_0 + hv_2) - f(x_0, y_0)}{h} \end{align} So I have: \begin{align} \frac{\partial f}{\partial v} (0,0)&= \lim_{h\to 0}\frac{\frac{(hv_1)^3hv_2}{(hv_1)^6 +(hv_2)^2}- 0}{h} \\ \\ & = \lim_{h\to0} \frac{h^3v_1^3v_2}{h^6v_1^6+h^2v_2^2} \\ \\ & = \lim_{h\to0} \frac{hv_1^3v_2}{h^4v_1^6+v_2^2} \\ \\ & = 0 \end{align} Since the limit exist for a generic vector it must be true all directional derivates exist. $(?_1)$ For the second part, since $(0,0)$ is a limit point of the domain, limit and continuity definitions are equivalent. So I looked for paths to get closer to $(0,0)$ that they gave me different results: \begin{align} \lim_{x\to 0}f(x,x³) &= \lim_{x\to 0}\frac{x³x³}{x⁶+x⁶}=\frac{1}{2} \\ \\ \lim_{x\to 0}f(x,0) &= \lim_{x\to 0}\frac{x³(0)}{x⁶+(0)²}=0 \:(?_2) \end{align} Since the results are different it must be that $f$ is discontinuous on $(0,0)$ . So, I want to check if my reasoning was correct, and also clarify the two doubts I marked: $(?_1)$ Is this statement correct? $(?_2)$ I reason that the numerator is exactly 0, while the denominator is something that tends to zero. So, no matter how small the denominator is, the result must be 0. Is this correct? Thanks in advance and my apologies if I formatted something wrong, first question here.","Prove that all directional derivates exist on and prove that is discontinuous on For the first part I used a generic vector and the definition I have of a directional derivate: So I have: Since the limit exist for a generic vector it must be true all directional derivates exist. For the second part, since is a limit point of the domain, limit and continuity definitions are equivalent. So I looked for paths to get closer to that they gave me different results: Since the results are different it must be that is discontinuous on . So, I want to check if my reasoning was correct, and also clarify the two doubts I marked: Is this statement correct? I reason that the numerator is exactly 0, while the denominator is something that tends to zero. So, no matter how small the denominator is, the result must be 0. Is this correct? Thanks in advance and my apologies if I formatted something wrong, first question here.","\begin{align} f(x,y) = \begin{cases} \frac{x^3y}{x^6 +y^2} & \text{ if } (x,y) ≠ (0,0) \\ \\  0 & \text{ if } (x,y) = (0,0) \end{cases}. \end{align} (0,0) f (0,0). v=(v_1,v_2) \begin{align}
\frac{\partial f}{\partial v} (x_0,y_0)= \lim_{h\to 0}\frac{f(x_0 + hv_1, y_0 + hv_2) - f(x_0, y_0)}{h}
\end{align} \begin{align}
\frac{\partial f}{\partial v} (0,0)&= \lim_{h\to 0}\frac{\frac{(hv_1)^3hv_2}{(hv_1)^6 +(hv_2)^2}- 0}{h} \\ \\
& = \lim_{h\to0} \frac{h^3v_1^3v_2}{h^6v_1^6+h^2v_2^2} \\ \\
& = \lim_{h\to0} \frac{hv_1^3v_2}{h^4v_1^6+v_2^2} \\ \\
& = 0
\end{align} (?_1) (0,0) (0,0) \begin{align}
\lim_{x\to 0}f(x,x³) &= \lim_{x\to 0}\frac{x³x³}{x⁶+x⁶}=\frac{1}{2} \\ \\
\lim_{x\to 0}f(x,0) &= \lim_{x\to 0}\frac{x³(0)}{x⁶+(0)²}=0 \:(?_2)
\end{align} f (0,0) (?_1) (?_2)","['calculus', 'limits', 'multivariable-calculus', 'continuity']"
85,Total derivative is continuous. What about partials?,Total derivative is continuous. What about partials?,,"If the total derivative $Df(x)$ of $f:\mathbb{R}^n \to \mathbb{R}^m$ is continuous does it imply that all partial derivatives are continuous? I would say yes of course because one can simply take the composition $(f \circ\pi_i)(x_i)$ , where $\pi_i: \mathbb{R} \to \mathbb{R}^n$ and $\pi_i(x_i)=(a_1, ..., x_i, a_{i+1},..., a_n)$ for a point $a=(a_1, ..., a_n)\in \mathbb{R}^n$ . As both functions $\pi_i$ and $Df(x)$ are continuous their composition is also continuous. But $Df(\pi_i(x_i))$ yields exactly the $i$ -th partial derivative. Is this correct?","If the total derivative of is continuous does it imply that all partial derivatives are continuous? I would say yes of course because one can simply take the composition , where and for a point . As both functions and are continuous their composition is also continuous. But yields exactly the -th partial derivative. Is this correct?","Df(x) f:\mathbb{R}^n \to \mathbb{R}^m (f \circ\pi_i)(x_i) \pi_i: \mathbb{R} \to \mathbb{R}^n \pi_i(x_i)=(a_1, ..., x_i, a_{i+1},..., a_n) a=(a_1, ..., a_n)\in \mathbb{R}^n \pi_i Df(x) Df(\pi_i(x_i)) i","['multivariable-calculus', 'continuity', 'proof-explanation', 'partial-derivative']"
86,Lagrange Multiplier Theorem,Lagrange Multiplier Theorem,,"I'm wondering why the Lagrange multiplier method only works when we assume that $\nabla g$ does not equal to $0$ . I functionally understand why the algebra does not work out if this is that case, but could you provide a conceptual explanation for what this situation represents?","I'm wondering why the Lagrange multiplier method only works when we assume that does not equal to . I functionally understand why the algebra does not work out if this is that case, but could you provide a conceptual explanation for what this situation represents?",\nabla g 0,"['multivariable-calculus', 'lagrange-multiplier']"
87,Iterated integration by parts with multiple variables,Iterated integration by parts with multiple variables,,"Let $\alpha$ be multi-index. Consider the iterated integral $$\int_{a_n}^{b_n} \cdots \int_{a_1}^{b_1} \left(D^{\alpha}f(x)\right)g(x) \, \mathrm{d}x_1 \ldots \mathrm{d}x_n.$$ Does anyone know a slick formula for this, moving all derivatives from $f$ to $g$ ? Of course, this is just iterated integration by parts, but doing this by hand seems like a huge pain. A simple reference would be sufficient, thanks!","Let be multi-index. Consider the iterated integral Does anyone know a slick formula for this, moving all derivatives from to ? Of course, this is just iterated integration by parts, but doing this by hand seems like a huge pain. A simple reference would be sufficient, thanks!","\alpha \int_{a_n}^{b_n} \cdots \int_{a_1}^{b_1} \left(D^{\alpha}f(x)\right)g(x) \, \mathrm{d}x_1 \ldots \mathrm{d}x_n. f g","['integration', 'multivariable-calculus', 'reference-request']"
88,How to differentiate a parametrised curve lying on a surface,How to differentiate a parametrised curve lying on a surface,,"I would like to ask how to differentiate a curve lying on a regular parametrised surface. I came across several questions that involve a curve lying on a surface. Differentiating the parametrised curve was necessary to prove the statements. My teacher gave us the following theorem (or definition?) as a 'clue' but I am struggling to understand why this is true. $\vec r$ ' = $\vec x $$_u$$\cdot$ u'+ $\vec x$$_v$$\cdot$ v' i.e., Let $\vec x$ (u, v) = (f $_1$ (u,v), f $_2$ (u,v), f $_3$ (u,v)) be a regular parametrised surface and $\vec r$ (t) = $\vec x$ (u(t), (v(t)) be a curve lying on the surface Then, $\frac{d\vec r}{ds}$ = ( $\frac{df_1(u, v)}{du}$$\cdot$$\frac{du}{dt}$ + $\frac{df_1(u, v)}{dv}$$\cdot$$\frac{dv}{dt}$ , ..., ...) = $\vec x $$_u$$\cdot$ u'+ $\vec x$$_v$$\cdot$ v' I understand the chain rule, but I don't quite understand why the two derivatives of vector are added together. Is it possible that this is because the following is (might be?) true: $\vec r$ (u, v) = $\alpha$$\vec r$$_u$ + $\beta$$\vec r$$_v$ where they are linearly independent? I am new to this and am still struggling with vector differentiation. I know this question may sound stupid but try not to laugh... Your help will be greatly appreciated. :)","I would like to ask how to differentiate a curve lying on a regular parametrised surface. I came across several questions that involve a curve lying on a surface. Differentiating the parametrised curve was necessary to prove the statements. My teacher gave us the following theorem (or definition?) as a 'clue' but I am struggling to understand why this is true. ' = u'+ v' i.e., Let (u, v) = (f (u,v), f (u,v), f (u,v)) be a regular parametrised surface and (t) = (u(t), (v(t)) be a curve lying on the surface Then, = ( + , ..., ...) = u'+ v' I understand the chain rule, but I don't quite understand why the two derivatives of vector are added together. Is it possible that this is because the following is (might be?) true: (u, v) = + where they are linearly independent? I am new to this and am still struggling with vector differentiation. I know this question may sound stupid but try not to laugh... Your help will be greatly appreciated. :)","\vec r \vec x _u\cdot \vec x_v\cdot \vec x _1 _2 _3 \vec r \vec x \frac{d\vec r}{ds} \frac{df_1(u, v)}{du}\cdot\frac{du}{dt} \frac{df_1(u, v)}{dv}\cdot\frac{dv}{dt} \vec x _u\cdot \vec x_v\cdot \vec r \alpha\vec r_u \beta\vec r_v","['multivariable-calculus', 'differential-geometry']"
89,"Double Integral $\int\limits_0^a\int\limits_0^a\frac{dx\,dy}{(x^2+y^2+a^2)^\frac32}$",Double Integral,"\int\limits_0^a\int\limits_0^a\frac{dx\,dy}{(x^2+y^2+a^2)^\frac32}","How to solve this integral? $$\int_0^a\!\!\!\int_0^a\frac{dx\,dy}{(x^2+y^2+a^2)^\frac32}$$ my attempt $$ \int_0^a\!\!\!\int_0^a\frac{dx \, dy}{(x^2+y^2+a^2)^\frac{3}{2}}= \int_0^a\!\!\!\int_0^a\frac{dx}{(x^2+\rho^2)^\frac{3}{2}}dy\\ \rho^2=y^2+a^2\\ x=\rho\tan\theta\\ dx=\rho\sec^2\theta \, d\theta\\ x^2+\rho^2=\rho^2\sec^2\theta\\ \int_0^a\!\!\!\int_0^{\arctan\frac{a}{\rho}}\frac{\rho\sec\theta}{\rho^3\sec^3\theta}d\theta \, dy= \int_0^a\!\!\!\frac{1}{\rho^2}\!\!\!\int_0^{\arctan\frac{a}{\rho}}\cos\theta \, d\theta \, dy=\\ \int_0^a\frac{1}{\rho^2}\sin\theta\bigg|_0^{\arctan\frac{a}{\rho}} d\theta \, dy= \int_0^a\frac{1}{\rho^2}\frac{x}{\sqrt{x^2+\rho^2}}\bigg|_0^ady=\\ \int_0^a\frac{a}{(y^2+a^2)\sqrt{y^2+2a^2}}dy$$ Update: $$\int_0^a\frac{a}{(y^2+a^2)\sqrt{y^2+2a^2}}dy=\frac{\pi}{6a}$$",How to solve this integral? my attempt Update:,"\int_0^a\!\!\!\int_0^a\frac{dx\,dy}{(x^2+y^2+a^2)^\frac32} 
\int_0^a\!\!\!\int_0^a\frac{dx \, dy}{(x^2+y^2+a^2)^\frac{3}{2}}=
\int_0^a\!\!\!\int_0^a\frac{dx}{(x^2+\rho^2)^\frac{3}{2}}dy\\
\rho^2=y^2+a^2\\
x=\rho\tan\theta\\
dx=\rho\sec^2\theta \, d\theta\\
x^2+\rho^2=\rho^2\sec^2\theta\\
\int_0^a\!\!\!\int_0^{\arctan\frac{a}{\rho}}\frac{\rho\sec\theta}{\rho^3\sec^3\theta}d\theta \, dy=
\int_0^a\!\!\!\frac{1}{\rho^2}\!\!\!\int_0^{\arctan\frac{a}{\rho}}\cos\theta \, d\theta \, dy=\\
\int_0^a\frac{1}{\rho^2}\sin\theta\bigg|_0^{\arctan\frac{a}{\rho}} d\theta \, dy=
\int_0^a\frac{1}{\rho^2}\frac{x}{\sqrt{x^2+\rho^2}}\bigg|_0^ady=\\
\int_0^a\frac{a}{(y^2+a^2)\sqrt{y^2+2a^2}}dy \int_0^a\frac{a}{(y^2+a^2)\sqrt{y^2+2a^2}}dy=\frac{\pi}{6a}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
90,Relation between tensor product and wedge product,Relation between tensor product and wedge product,,"Let $V$ be a vector space, $\mathcal B=\{w_1,\dots,w_k\}$ a orthonormal basis for $V$ , and let $T=w_1^\star\otimes\dots\otimes w_k^\star\in\mathcal L^k(V)$ . I don't understand why is this equivalence true: $$w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\frac 1 {k!}\sum_{\sigma\in S^k}(w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k)$$ Actually, I was sure that the equivalence was $$w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\sum_{\sigma\in S^k}(w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k)$$ and this made sense to me since $w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\mathrm{Det}|w_1,\dots,w_k|=1$ , and $(w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k)=1$ if and only if $\sigma$ is the identity, otherwise is equal to zero (so the sum is equal to $1$ ). Can you tell me where I'm wrong? Thank you in advance","Let be a vector space, a orthonormal basis for , and let . I don't understand why is this equivalence true: Actually, I was sure that the equivalence was and this made sense to me since , and if and only if is the identity, otherwise is equal to zero (so the sum is equal to ). Can you tell me where I'm wrong? Thank you in advance","V \mathcal B=\{w_1,\dots,w_k\} V T=w_1^\star\otimes\dots\otimes w_k^\star\in\mathcal L^k(V) w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\frac 1 {k!}\sum_{\sigma\in S^k}(w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k) w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\sum_{\sigma\in S^k}(w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k) w_1^\star\wedge\dots\wedge w_k^\star(w_1,\dots,w_k)=\mathrm{Det}|w_1,\dots,w_k|=1 (w_1^\star\otimes\dots\otimes w_k^\star)^\sigma(w_1,\dots,w_k)=1 \sigma 1",['multivariable-calculus']
91,Integral with two symmetric matrices,Integral with two symmetric matrices,,"Let ${\bf A} \in M_{n\times n}(\mathbb R)$ be a symmetric positive-definite matrix, ${\bf x}\in \mathbb R^n$ . I know how to obtain (for $p>n/2$ ): $$ \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^p} = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p-\frac{n}{2})}{\Gamma(p)} $$ by moving to a basis that diagonalises matrix ${\bf A}$ . I seek to calculate $$ I = \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^{p}({\bf x}^T {\bf B}{\bf x} + 1)^{q}}$$ where both ${\bf A}$ and ${\bf B}$ are symmetric positive-definite matrices. The same method won't work, as in general there's no way to simultaneously diagonalize ${\bf A}$ and ${\bf B}$ . It can only transform the integral to the form $$ I =\frac{1}{\sqrt{\det{\bf A}}}\int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf x} + 1)^{p}({\bf x}^T {\bf C}{\bf x} + 1)^{q}}$$ where $ {\bf C} = {\bf A}^{-\frac12}{\bf B}{\bf A}^{-\frac12}$ , but I don't know if anything more can be done. EDIT: After searching around a bit more, I've found formula $$ \frac{1}{a^p b^q} = \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}\int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big(a t + b(1-t)\big)^{p+q}}{\rm d}t$$ which allows to write $I$ in the form $$ I = \frac{1}{\sqrt{\det{\bf A}}}\frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} \int_{\mathbb R^n} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big({\bf x}^T \big(t{\bf 1}+(1-t){\bf C}\big){\bf x} + 1\big)^{p+q}} {\rm d}t {\rm d}{\bf x} = \\ = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\sqrt{\det\big(t{\bf 1}+(1-t){\bf C}\big)}} {\rm d}t$$ Therefore if the eigenvalues of ${\bf C}$ are $\lambda_i$ we get $$ I = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 t^{p-1}(1-t)^{q-1}\prod_{i=1}^n\big(t+(1-t)\lambda_i\big)^{-\frac12} {\rm d}t$$ The remaining integral looks like some kind of generalized hypergeometric function, but I don't know which one, and whether it has a simpler representation.","Let be a symmetric positive-definite matrix, . I know how to obtain (for ): by moving to a basis that diagonalises matrix . I seek to calculate where both and are symmetric positive-definite matrices. The same method won't work, as in general there's no way to simultaneously diagonalize and . It can only transform the integral to the form where , but I don't know if anything more can be done. EDIT: After searching around a bit more, I've found formula which allows to write in the form Therefore if the eigenvalues of are we get The remaining integral looks like some kind of generalized hypergeometric function, but I don't know which one, and whether it has a simpler representation.",{\bf A} \in M_{n\times n}(\mathbb R) {\bf x}\in \mathbb R^n p>n/2  \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^p} = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p-\frac{n}{2})}{\Gamma(p)}  {\bf A}  I = \int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf A}{\bf x} + 1)^{p}({\bf x}^T {\bf B}{\bf x} + 1)^{q}} {\bf A} {\bf B} {\bf A} {\bf B}  I =\frac{1}{\sqrt{\det{\bf A}}}\int_{\mathbb R^n} \frac{{\rm d}{\bf x}}{({\bf x}^T {\bf x} + 1)^{p}({\bf x}^T {\bf C}{\bf x} + 1)^{q}}  {\bf C} = {\bf A}^{-\frac12}{\bf B}{\bf A}^{-\frac12}  \frac{1}{a^p b^q} = \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}\int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big(a t + b(1-t)\big)^{p+q}}{\rm d}t I  I = \frac{1}{\sqrt{\det{\bf A}}}\frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} \int_{\mathbb R^n} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\big({\bf x}^T \big(t{\bf 1}+(1-t){\bf C}\big){\bf x} + 1\big)^{p+q}} {\rm d}t {\rm d}{\bf x} = \\ = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 \frac{t^{p-1}(1-t)^{q-1}}{\sqrt{\det\big(t{\bf 1}+(1-t){\bf C}\big)}} {\rm d}t {\bf C} \lambda_i  I = \frac{1}{\sqrt{\det{\bf A}}} \frac{\pi^{n/2}\Gamma(p+q-\frac{n}{2})}{\Gamma(p)\Gamma(q)} \int_0^1 t^{p-1}(1-t)^{q-1}\prod_{i=1}^n\big(t+(1-t)\lambda_i\big)^{-\frac12} {\rm d}t,"['integration', 'matrices', 'multivariable-calculus', 'symmetric-matrices']"
92,"Calculate $\iiint_V\frac{xyz}{x^2+y^2}\,dx\,dy\,dz$",Calculate,"\iiint_V\frac{xyz}{x^2+y^2}\,dx\,dy\,dz","Problem : Calculate $$\iiint_V\frac{xyz}{x^2+y^2}\,dx\,dy\,dz,$$ where $V$ is the domain bounded by the surface $(x^2+y^2+z^2)^2=a^2xy$ and the plane $z=0$ . My solution : We make the following substitution: $$ \begin{cases} x = r\sin\theta\cos\varphi,\\ y = r\sin\theta\sin\varphi,\\ z = r\cos\theta \end{cases} $$ The given surface is symmetrical by $O$ , so we just need to take the integration by the object in the first quadrant. Hence, we have $0\le \theta\le\dfrac\pi2$ and $0\le \varphi\le\dfrac\pi2$ . For $r$ , we have $$r^4 = a^2r^2\sin^2\theta\sin\varphi\cos\varphi \iff r^2 = a^2\sin^2\theta\frac{\sin2\varphi}{2},$$ so $0\le r\le a\sin\theta\cdot\sqrt{\dfrac{\sin2\varphi}{2}}$ . Now we can calculate the integral in spherical coordinate. First, the given function become $$\frac{xyz}{x^2+y^2}=\frac{r^2\sin^2\theta\sin\varphi\cos\varphi\cdot r\cos\theta}{r^2\sin^2\theta}=\frac{r\cos\theta\sin2\varphi}{2},$$ so the integration is equal to $$I=\int^{\pi/2}_0d\varphi\int^{\pi/2}_0d\theta\int_0^{a\sin\theta\cdot\sqrt{\frac{\sin2\varphi}{2}}}\frac{r\cos\theta\sin2\varphi}{2}\cdot r^2\sin\theta\,dr.$$ The calculation: $$ \begin{aligned} I&=2\int^{\pi/2}_0d\varphi\int^{\pi/2}_0d\theta\int_0^{a\sin\theta\cdot\sqrt{\frac{\sin2\varphi}{2}}}\frac{r\cos\theta\sin2\varphi}{2}\cdot r^2\sin\theta\,dr.\\ &=2\int^{\pi/2}_0d\varphi\int^{\pi/2}_0\frac{\sin\theta\cos\theta\sin2\varphi}{2}\cdot\frac{a^4\sin^4\theta\sin^22\varphi}{4}\,d\theta\\ &=2\cdot \frac{a^4}{8}\int^{\pi/2}_0\sin^32\varphi\,d\varphi\int^{\pi/2}_0\sin^5\theta\cos\theta\,d\theta\\ &=\frac{a^4}{4}\cdot\frac 23\cdot\frac 16 = \frac{a^4}{36}. \end{aligned} $$ Question : The correct answer is $\dfrac{a^4}{180}$ . But I can't seem to find any mistake in my argument and calculation. Can someone have a look at this and specify where did I do wrong? Thank you very much.","Problem : Calculate where is the domain bounded by the surface and the plane . My solution : We make the following substitution: The given surface is symmetrical by , so we just need to take the integration by the object in the first quadrant. Hence, we have and . For , we have so . Now we can calculate the integral in spherical coordinate. First, the given function become so the integration is equal to The calculation: Question : The correct answer is . But I can't seem to find any mistake in my argument and calculation. Can someone have a look at this and specify where did I do wrong? Thank you very much.","\iiint_V\frac{xyz}{x^2+y^2}\,dx\,dy\,dz, V (x^2+y^2+z^2)^2=a^2xy z=0 
\begin{cases}
x = r\sin\theta\cos\varphi,\\
y = r\sin\theta\sin\varphi,\\
z = r\cos\theta
\end{cases}
 O 0\le \theta\le\dfrac\pi2 0\le \varphi\le\dfrac\pi2 r r^4 = a^2r^2\sin^2\theta\sin\varphi\cos\varphi \iff r^2 = a^2\sin^2\theta\frac{\sin2\varphi}{2}, 0\le r\le a\sin\theta\cdot\sqrt{\dfrac{\sin2\varphi}{2}} \frac{xyz}{x^2+y^2}=\frac{r^2\sin^2\theta\sin\varphi\cos\varphi\cdot r\cos\theta}{r^2\sin^2\theta}=\frac{r\cos\theta\sin2\varphi}{2}, I=\int^{\pi/2}_0d\varphi\int^{\pi/2}_0d\theta\int_0^{a\sin\theta\cdot\sqrt{\frac{\sin2\varphi}{2}}}\frac{r\cos\theta\sin2\varphi}{2}\cdot r^2\sin\theta\,dr. 
\begin{aligned}
I&=2\int^{\pi/2}_0d\varphi\int^{\pi/2}_0d\theta\int_0^{a\sin\theta\cdot\sqrt{\frac{\sin2\varphi}{2}}}\frac{r\cos\theta\sin2\varphi}{2}\cdot r^2\sin\theta\,dr.\\
&=2\int^{\pi/2}_0d\varphi\int^{\pi/2}_0\frac{\sin\theta\cos\theta\sin2\varphi}{2}\cdot\frac{a^4\sin^4\theta\sin^22\varphi}{4}\,d\theta\\
&=2\cdot \frac{a^4}{8}\int^{\pi/2}_0\sin^32\varphi\,d\varphi\int^{\pi/2}_0\sin^5\theta\cos\theta\,d\theta\\
&=\frac{a^4}{4}\cdot\frac 23\cdot\frac 16 = \frac{a^4}{36}.
\end{aligned}
 \dfrac{a^4}{180}","['integration', 'multivariable-calculus']"
93,How would I find the integration limits for this volume integral?,How would I find the integration limits for this volume integral?,,"If i want to integrate over the the region $R$ such that $R:={(x,y,z):z\geq x^2+y^2,z \geq 2-2x, z\leq 10 -2y}$ , what will the limits be for $x, y$ , more importantly how should I systematically think to find the limits for my integrals? Could someone please help me? Is it correct if my integration limits are as such: $\int^{1}_{0} \int^{1}_{0} \int^{10-2y}_{2-2x} dzdxdy$ ?","If i want to integrate over the the region such that , what will the limits be for , more importantly how should I systematically think to find the limits for my integrals? Could someone please help me? Is it correct if my integration limits are as such: ?","R R:={(x,y,z):z\geq x^2+y^2,z \geq 2-2x, z\leq 10 -2y} x, y \int^{1}_{0} \int^{1}_{0} \int^{10-2y}_{2-2x} dzdxdy",['multivariable-calculus']
94,How does the tensor product relate to the gradient?,How does the tensor product relate to the gradient?,,I only know the definition of the tensor product of two vectors and was reading a text which concerned a function $u:\mathbb{R}^3 \to \mathbb{R}^3$ such that $|u| = 1$ and $u(x) = \frac{x}{|x|}$ . The text claimed that $$\nabla u = \frac{1}{|x|}\left(I-\frac{x}{|x|} \otimes \frac{x}{|x|}\right)$$ and therefore $|\nabla u|^2 = \frac{2}{|x|^2}$ . I have no idea how they actually arrived at the expression for $\nabla u$ and then how did they take the norm of $\nabla u$ ? Because isn't $\nabla u$ a $3 \times 3$ matrix so what matrix norm is being used?,I only know the definition of the tensor product of two vectors and was reading a text which concerned a function such that and . The text claimed that and therefore . I have no idea how they actually arrived at the expression for and then how did they take the norm of ? Because isn't a matrix so what matrix norm is being used?,u:\mathbb{R}^3 \to \mathbb{R}^3 |u| = 1 u(x) = \frac{x}{|x|} \nabla u = \frac{1}{|x|}\left(I-\frac{x}{|x|} \otimes \frac{x}{|x|}\right) |\nabla u|^2 = \frac{2}{|x|^2} \nabla u \nabla u \nabla u 3 \times 3,"['linear-algebra', 'multivariable-calculus', 'partial-differential-equations', 'tensor-products', 'tensors']"
95,"Calculation of a double integral $\int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy$",Calculation of a double integral,"\int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy","I want to calculate the following integral: $$ \int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy. $$ I tried to go first with $y$ which seems the easier of the two, but then the integral with respect to $x$ becomes quite complex. On the other hand, starting with $x$ it is cumbersome and further the integral with respect to $y$ is more complex than the first method... Is there a simple way  to do it??","I want to calculate the following integral: I tried to go first with which seems the easier of the two, but then the integral with respect to becomes quite complex. On the other hand, starting with it is cumbersome and further the integral with respect to is more complex than the first method... Is there a simple way  to do it??","
\int_{-1}^{1}\int_0^2 \sqrt{\left|y-x^2\right|} \,dx \,dy.
 y x x y",['multivariable-calculus']
96,Examine differentiability of $f$,Examine differentiability of,f,"Examine the differentiability of $f$ : $$f(x,y) = \begin{cases}\displaystyle \frac{x^3}{x^2+y^2} &   (x,y) \neq (0,0) \\\\ 0 & (x,y) = (0,0) \end{cases}\:\:. $$ Let's check if $f$ is continuous: let $(x,y) \longrightarrow (0,0)$ . Then $$ \left |\frac{x^3}{x^2+y^2}\right| \le |x|\cdot \left|\frac{x^2}{x^2+y^2}\right | \le |x| \longrightarrow 0, $$ so this is ok. Now let's check the continuity of partial derivatives: \begin{align} \frac{\partial f}{\partial y}(x,y) &= -\frac{2 x^3 y}{\left(x^2+y^2\right)^2}, \\ \frac{\partial f}{\partial x}(x,y) &= \frac{3 x^2}{x^2+y^2}-\frac{2 x^4}{\left(x^2+y^2\right)^2}. \end{align} But I am not sure what should I do now?",Examine the differentiability of : Let's check if is continuous: let . Then so this is ok. Now let's check the continuity of partial derivatives: But I am not sure what should I do now?,"f f(x,y) = \begin{cases}\displaystyle \frac{x^3}{x^2+y^2} &   (x,y) \neq (0,0) \\\\ 0 & (x,y) = (0,0) \end{cases}\:\:.  f (x,y) \longrightarrow (0,0)  \left |\frac{x^3}{x^2+y^2}\right| \le |x|\cdot \left|\frac{x^2}{x^2+y^2}\right | \le |x| \longrightarrow 0,  \begin{align} \frac{\partial f}{\partial y}(x,y) &= -\frac{2 x^3 y}{\left(x^2+y^2\right)^2}, \\
\frac{\partial f}{\partial x}(x,y) &= \frac{3 x^2}{x^2+y^2}-\frac{2 x^4}{\left(x^2+y^2\right)^2}. \end{align}",['multivariable-calculus']
97,Show that $\sin(x+y)$ is differentiable.,Show that  is differentiable.,\sin(x+y),"Show that $f(x,y)=\sin (x+y)$ is differentiable in its domain by the definition i.e. prove $\lim_{(x,y) \rightarrow (x_0, y_0)}\frac{|\sin(x+y)-\sin{(x_0+y_0)}-\cos(x_0+y_0)(x-x_0)-\cos(x_0+y_0)(y-y_0)|}{\|(x,y)-(x_0,y_0)\|} = 0$ I can not find a way to compare $|f(x,y)-z|$ with $\|(x,y)-(x_0,y_0)\|$",Show that is differentiable in its domain by the definition i.e. prove I can not find a way to compare with,"f(x,y)=\sin (x+y) \lim_{(x,y) \rightarrow (x_0, y_0)}\frac{|\sin(x+y)-\sin{(x_0+y_0)}-\cos(x_0+y_0)(x-x_0)-\cos(x_0+y_0)(y-y_0)|}{\|(x,y)-(x_0,y_0)\|} = 0 |f(x,y)-z| \|(x,y)-(x_0,y_0)\|",['multivariable-calculus']
98,"Finding a potential function for $\vec F=\frac{-y}{x^2+y^2}\vec i+\frac{x}{x^2+y^2}\vec j$ on $\Omega=\mathbb{R}^2- \{(x,y)\,|\,y=0,\,x\geq0 \}$",Finding a potential function for  on,"\vec F=\frac{-y}{x^2+y^2}\vec i+\frac{x}{x^2+y^2}\vec j \Omega=\mathbb{R}^2- \{(x,y)\,|\,y=0,\,x\geq0 \}","In one of my Calculus III classes the professor presented the following vector field, defined on the set $S$ of all points $(x,y) \neq(0,0)$ : $$ \bbox[6px,border:1px solid black] { \vec{F}=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j} } $$ Although $\vec{F}$ is not a gradient on $S$ , it is a gradient on the set $\Omega=\mathbb{R}^2-\left\{(x,y)\,|\,y=0, \, x\geq0\right\}$ , i.e, all points in the xy-plane except those on the positive x-axis. In the class, my professor wrote a potential function for $\vec{F}$ on $\Omega$ using the following functions: $$ \bbox[6px,border:1px solid black] { \begin{alignat}{0} \Psi_1(x,y)=-\arctan\left(\frac x y \right), \,y\neq0 &\text{and} &\Psi_2(x,y)=\arctan\left(\frac y x \right), \,x\neq0 \end{alignat} } $$ The first is a potential of $\vec F$ on $\Omega^+=\{(x,y)\in\mathbb{R}^2\,|\,y>0\}$ and on $\Omega^-=\{(x,y)\in\mathbb{R}^2\,|\,y<0\}$ . The last one is a potential of $\vec F$ too, but on $\Omega_+=\{(x,y)\in\mathbb{R}^2\,|\,x>0\}$ and on $\Omega_-=\{(x,y)\in\mathbb{R}^2\,|\,x<0\}$ The potential function wrote by my professor: $$ \bbox[8px,border:1px solid black] { \phi (x,y) = \begin{cases} \Psi_1(x,y) & \text{if $(x,y) \in R_1$} \\[2ex] \Psi_2(x,y)+\frac\pi 2 & \text{if $(x,y) \in R_2$} \\[2ex] \Psi_1(x,y)+2\pi & \text{if $(x,y) \in R_3$} \\ \end{cases} } $$ (Note: I remember that my professor started with the argument that since $\nabla\Psi_1=\vec F = \nabla\Psi_2$ , then $\Psi_1-\Psi_2=k, \,k \in \mathbb{R}$ ) My doubts: I've tried a lot, but still have not figured out how to build $\phi (x,y)$ using $\Psi_1$ and $\Psi_2$ . So, how to find this specific potential function of $\vec F$ ? Why can't I simply write potential functions of $\vec{F}$ in the following ways $$ \phi_A (x,y) = \begin{alignat}{0} \begin{cases} \Psi_1(x,y) & \text{if $(x,y) \in R_1$} \\[2ex] \Psi_2(x,y) & \text{if $(x,y) \in R_2$} \\[2ex] \Psi_1(x,y) & \text{if $(x,y) \in R_3$} \\ \end{cases} &\text{or} &\phi_B(x,y)= \begin{cases} \Psi_1(x,y) & \text{if $y>0$} \\[2ex] 0 & \text{if $y=0$ and $x<0$} \\[2ex] \Psi_1(x,y) & \text{if $y<0$} \\ \end{cases} \end{alignat} $$ since $\nabla\phi_A=\nabla\phi_B=\vec F$ on $\Omega$ ?","In one of my Calculus III classes the professor presented the following vector field, defined on the set of all points : Although is not a gradient on , it is a gradient on the set , i.e, all points in the xy-plane except those on the positive x-axis. In the class, my professor wrote a potential function for on using the following functions: The first is a potential of on and on . The last one is a potential of too, but on and on The potential function wrote by my professor: (Note: I remember that my professor started with the argument that since , then ) My doubts: I've tried a lot, but still have not figured out how to build using and . So, how to find this specific potential function of ? Why can't I simply write potential functions of in the following ways since on ?","S (x,y) \neq(0,0) 
\bbox[6px,border:1px solid black] {
\vec{F}=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}
}
 \vec{F} S \Omega=\mathbb{R}^2-\left\{(x,y)\,|\,y=0, \, x\geq0\right\} \vec{F} \Omega 
\bbox[6px,border:1px solid black] {
\begin{alignat}{0}
\Psi_1(x,y)=-\arctan\left(\frac x y \right), \,y\neq0 &\text{and} &\Psi_2(x,y)=\arctan\left(\frac y x \right), \,x\neq0
\end{alignat}
}
 \vec F \Omega^+=\{(x,y)\in\mathbb{R}^2\,|\,y>0\} \Omega^-=\{(x,y)\in\mathbb{R}^2\,|\,y<0\} \vec F \Omega_+=\{(x,y)\in\mathbb{R}^2\,|\,x>0\} \Omega_-=\{(x,y)\in\mathbb{R}^2\,|\,x<0\} 
\bbox[8px,border:1px solid black] {
\phi (x,y) =
\begin{cases}
\Psi_1(x,y) & \text{if (x,y) \in R_1} \\[2ex]
\Psi_2(x,y)+\frac\pi 2 & \text{if (x,y) \in R_2} \\[2ex]
\Psi_1(x,y)+2\pi & \text{if (x,y) \in R_3} \\
\end{cases}
}
 \nabla\Psi_1=\vec F = \nabla\Psi_2 \Psi_1-\Psi_2=k, \,k \in \mathbb{R} \phi (x,y) \Psi_1 \Psi_2 \vec F \vec{F} 
\phi_A (x,y) =
\begin{alignat}{0}
\begin{cases}
\Psi_1(x,y) & \text{if (x,y) \in R_1} \\[2ex]
\Psi_2(x,y) & \text{if (x,y) \in R_2} \\[2ex]
\Psi_1(x,y) & \text{if (x,y) \in R_3} \\
\end{cases}
&\text{or}
&\phi_B(x,y)=
\begin{cases}
\Psi_1(x,y) & \text{if y>0} \\[2ex]
0 & \text{if y=0 and x<0} \\[2ex]
\Psi_1(x,y) & \text{if y<0} \\
\end{cases}
\end{alignat}
 \nabla\phi_A=\nabla\phi_B=\vec F \Omega","['calculus', 'multivariable-calculus', 'vector-analysis', 'vector-fields', 'differential-operators']"
99,Can we always perturb a map to have distinct singular values?,Can we always perturb a map to have distinct singular values?,,"Let $\mathbb{D}^n$ be the closed $n$ -dimensional unit ball, and let $f:\mathbb{D}^n \to \mathbb{R}^n$ be smooth. Question: Do there there exist $f_n \in C^{\infty}(\mathbb{D}^n, \mathbb{R}^n)$ such that $f_n \to f$ in $W^{1,2}(\mathbb{D}^n, \mathbb{R}^n)$ with the property that for every $p \in \mathbb{D}^n $ , the singular values of $(df_n)_p$ are all distinct? I am fine with the $f_n$ being $C^1$ , if that matters.","Let be the closed -dimensional unit ball, and let be smooth. Question: Do there there exist such that in with the property that for every , the singular values of are all distinct? I am fine with the being , if that matters.","\mathbb{D}^n n f:\mathbb{D}^n \to \mathbb{R}^n f_n \in C^{\infty}(\mathbb{D}^n, \mathbb{R}^n) f_n \to f W^{1,2}(\mathbb{D}^n, \mathbb{R}^n) p \in \mathbb{D}^n  (df_n)_p f_n C^1","['real-analysis', 'multivariable-calculus', 'sobolev-spaces', 'svd', 'perturbation-theory']"
