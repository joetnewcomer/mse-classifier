,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding a closed form for an integral [closed],Finding a closed form for an integral [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to find a closed form for the following integral: $$\int_0^{k\pi}\left(y(x)+y''(x)\right)\sin xdx$$ And I know that $y(k\pi)=a$ and $k$ is a positive integer.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to find a closed form for the following integral: And I know that and is a positive integer.",\int_0^{k\pi}\left(y(x)+y''(x)\right)\sin xdx y(k\pi)=a k,"['real-analysis', 'calculus', 'integration', 'derivatives', 'definite-integrals']"
1,"How can we show $\frac{1-x^n}{1-c^n} + \left(1-\frac{1-x}{1-c}\right)^n \leq 1 $ for all $n \in \mathbb{N}$, $0 \leq c \leq x \leq 1$, $c \neq 1$?","How can we show  for all , , ?",\frac{1-x^n}{1-c^n} + \left(1-\frac{1-x}{1-c}\right)^n \leq 1  n \in \mathbb{N} 0 \leq c \leq x \leq 1 c \neq 1,"How can we show $$\frac{1-x^n}{1-c^n} + \left(1-\frac{1-x}{1-c}\right)^n \leq 1 $$ for all $n \in \mathbb{N}$ , $0 \leq c \leq x   \leq 1, c \neq 1$ ? The context is this probability problem, but of course this problem might be of independent interest to inequality enthusiasts. In my attempt, I have graphed the inequality on Desmos. We might note that the derivative changes sign in the range $c < x < 1$ so it may be unlikely differentiation would be of help.","How can we show for all , ? The context is this probability problem, but of course this problem might be of independent interest to inequality enthusiasts. In my attempt, I have graphed the inequality on Desmos. We might note that the derivative changes sign in the range so it may be unlikely differentiation would be of help.","\frac{1-x^n}{1-c^n} + \left(1-\frac{1-x}{1-c}\right)^n \leq 1  n \in \mathbb{N} 0 \leq c \leq x 
 \leq 1, c \neq 1 c < x < 1","['real-analysis', 'inequality', 'optimization']"
2,Why are zeros of functions so important?,Why are zeros of functions so important?,,"I realise this is a general question. I am self-teaching mathematics and I have observed that the zeros of real and complex functions are of much interest. Question: Why are the zeros of real or complex so important? I was going to say that the zeros completely determine a function because the function can be factorised into factors, with each factor corresponding to one of the zeros. For example, $f(x) = (x-1)(x-3)$ is completely determined by its zeros at $x=1$ and $x=3$ . However this doesn't work for functions like $g(x) = (x-1)(x-3) + 2$ . I appreciate your patience with a question that might be naive.","I realise this is a general question. I am self-teaching mathematics and I have observed that the zeros of real and complex functions are of much interest. Question: Why are the zeros of real or complex so important? I was going to say that the zeros completely determine a function because the function can be factorised into factors, with each factor corresponding to one of the zeros. For example, is completely determined by its zeros at and . However this doesn't work for functions like . I appreciate your patience with a question that might be naive.",f(x) = (x-1)(x-3) x=1 x=3 g(x) = (x-1)(x-3) + 2,"['real-analysis', 'complex-analysis', 'functions']"
3,"Prove $\forall t\in [0,1):\, t\le \frac{1-t^t}{1-t}$",Prove,"\forall t\in [0,1):\, t\le \frac{1-t^t}{1-t}","How do I prove $$\forall t\in [0,1):\,t\le \frac{1-t^t}{1-t}?$$ Do not use derivatives or integrals and assume that irrational exponentiation is defined by limits and define $0^0=1$ . My attempt: Let $t=\frac{1}{a}$ , thus $a\gt 1$ . The case for $t=0$ is trivial. So $$\begin{align}\frac{1}{a}&\le \frac{1-\left(\frac{1}{a}\right)^{\frac{1}{a}}}{1-\frac{1}{a}}\\&=\frac{\left(1-a^{-\frac{1}{a}}\right)a}{a-1}\\&=\frac{a-a^{1-\frac{1}{a}}}{a-1}\\a&\ge \frac{a-1}{a-a^{1-\frac{1}{a}}}\\a^2-a^{2-\frac{1}{a}}-a+1&\ge 0.\end{align}$$ Now $a^{2-\frac{1}{a}}\le a^2$ but I don't know how to use this fact to compare $a^{2-\frac{1}{a}}+a$ to $a^2$ .","How do I prove Do not use derivatives or integrals and assume that irrational exponentiation is defined by limits and define . My attempt: Let , thus . The case for is trivial. So Now but I don't know how to use this fact to compare to .","\forall t\in [0,1):\,t\le \frac{1-t^t}{1-t}? 0^0=1 t=\frac{1}{a} a\gt 1 t=0 \begin{align}\frac{1}{a}&\le \frac{1-\left(\frac{1}{a}\right)^{\frac{1}{a}}}{1-\frac{1}{a}}\\&=\frac{\left(1-a^{-\frac{1}{a}}\right)a}{a-1}\\&=\frac{a-a^{1-\frac{1}{a}}}{a-1}\\a&\ge \frac{a-1}{a-a^{1-\frac{1}{a}}}\\a^2-a^{2-\frac{1}{a}}-a+1&\ge 0.\end{align} a^{2-\frac{1}{a}}\le a^2 a^{2-\frac{1}{a}}+a a^2","['real-analysis', 'inequality']"
4,"Proof that the exponential function is the only solution to dy/dx = y, y(0) = 1 [duplicate]","Proof that the exponential function is the only solution to dy/dx = y, y(0) = 1 [duplicate]",,"This question already has answers here : Are the any non-trivial functions where $f(x)=f'(x)$ not of the form $Ae^x$ (6 answers) Closed 4 years ago . I understand that assuming an analytic solution, we look at the Taylor series and arrive at a unique solution y = exp(x). However how do we know that there are no other non-analytic solutions? (Ideally with as little analysis machinery as possible)","This question already has answers here : Are the any non-trivial functions where $f(x)=f'(x)$ not of the form $Ae^x$ (6 answers) Closed 4 years ago . I understand that assuming an analytic solution, we look at the Taylor series and arrive at a unique solution y = exp(x). However how do we know that there are no other non-analytic solutions? (Ideally with as little analysis machinery as possible)",,"['real-analysis', 'ordinary-differential-equations']"
5,Applications of Manifolds not embedded in Euclidean Space,Applications of Manifolds not embedded in Euclidean Space,,"In the first few pages of his book Introduction to Smooth Manifolds , Lee writes: But   for more sophisticated applications, it is an undue restriction to require   smooth manifolds to be subsets of some ambient Euclidean space. A way to motivate why we develop tools to do calculus in manifolds is through examples in physics. For instance, one might want to do calculus on the function that maps the surface of the earth to its temeprature on the real line, we might want to study electromagnetic properties of a torus, and so forth. In each of these cases, it is easy to study these surfaces as subsets (or submanifolds) of Euclidean space. What are some ""sophisticated applications"" of manifolds outside of math - as Lee writes - that do not allow us to work with manifolds that are embedded in Euclidean space? Edit: I'm obviously looking for examples other than the one Lee gives himself (and perhaps the most popular here): Looking at space-time as a four dimensional manifold, where it doesn't make sense to embed it in an ambient space.","In the first few pages of his book Introduction to Smooth Manifolds , Lee writes: But   for more sophisticated applications, it is an undue restriction to require   smooth manifolds to be subsets of some ambient Euclidean space. A way to motivate why we develop tools to do calculus in manifolds is through examples in physics. For instance, one might want to do calculus on the function that maps the surface of the earth to its temeprature on the real line, we might want to study electromagnetic properties of a torus, and so forth. In each of these cases, it is easy to study these surfaces as subsets (or submanifolds) of Euclidean space. What are some ""sophisticated applications"" of manifolds outside of math - as Lee writes - that do not allow us to work with manifolds that are embedded in Euclidean space? Edit: I'm obviously looking for examples other than the one Lee gives himself (and perhaps the most popular here): Looking at space-time as a four dimensional manifold, where it doesn't make sense to embed it in an ambient space.",,"['real-analysis', 'differential-geometry', 'manifolds', 'physics', 'smooth-manifolds']"
6,An interesting real analysis problem involving integrals,An interesting real analysis problem involving integrals,,"If $f$ is continuos in $[0,1]$ and $\int_0^1 f(x)dx=0$ , then $f(c)=\int_0^c f(x)dx$ for some $c\in (0,1)$ . My attempt: I defined the fuction $g:[0,1]\to \mathbb{R}$ by $g(s)=f(s)-\int_0^{s}f(x)dx$ , for all $s\in [0,1]$ . Notice that $g(0)=f(0)$ and $g(1)=f(1)$ . I considered cases for $f(0)$ and $f(1)$ . For example, if $f(0)f(1)<0$ , then using the Intermediate Value Theorem, we can find $c\in (0,1)$ such that $g(c)=0$ , that is $f(c)=\int_0^c f(x)dx$ . But, I don't know how to proceed in the other case: $f(0)f(1)\geq 0$ .","If is continuos in and , then for some . My attempt: I defined the fuction by , for all . Notice that and . I considered cases for and . For example, if , then using the Intermediate Value Theorem, we can find such that , that is . But, I don't know how to proceed in the other case: .","f [0,1] \int_0^1 f(x)dx=0 f(c)=\int_0^c f(x)dx c\in (0,1) g:[0,1]\to \mathbb{R} g(s)=f(s)-\int_0^{s}f(x)dx s\in [0,1] g(0)=f(0) g(1)=f(1) f(0) f(1) f(0)f(1)<0 c\in (0,1) g(c)=0 f(c)=\int_0^c f(x)dx f(0)f(1)\geq 0","['real-analysis', 'integration']"
7,How to evaluate $\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx$ in an elegant way?,How to evaluate  in an elegant way?,\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx,"How to prove that: $$\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx=\frac74\zeta(3)-\frac34\ln2 \zeta(2)-\frac{\pi}{2}G$$ where $\zeta$ is the Riemann zeta function and $G$ is Catalan constant. I came across this integral while working on evaluating some harmonic series. I am tagging ""harmonic series"" as its pretty related to logarithmic integrals.","How to prove that: where is the Riemann zeta function and is Catalan constant. I came across this integral while working on evaluating some harmonic series. I am tagging ""harmonic series"" as its pretty related to logarithmic integrals.",\int_0^1\frac{\ln x\ln(1+x^2)}{1-x^2}dx=\frac74\zeta(3)-\frac34\ln2 \zeta(2)-\frac{\pi}{2}G \zeta G,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'harmonic-numbers']"
8,Does convergence for Cauchy sequence fail only when the limit is not in the domain?,Does convergence for Cauchy sequence fail only when the limit is not in the domain?,,"I am trying to understand how important is the distinction between Cauchy sequences and convergent sequences in normed vector spaces $E$ . So far I have only come across examples where the Cauchy sequence $\{x_n\}$ where $x_n\in E$ fails to converge only because the limit point is not in $E$ and an extension to $E$ typically by completion fixes the problem. For example: $$x_n\colon[0,1]\to\Bbb R, \quad t\mapsto \sum_{k=0}^n\frac{t^k}{k!},$$ where $E\triangleq \mathcal{P}([0,1])$ is the space of polynomial functions on $[0,1]$ with uniform convergence norm. I want to know if this is the only kind of failure mode for the convergence of a Cauchy sequence.",I am trying to understand how important is the distinction between Cauchy sequences and convergent sequences in normed vector spaces . So far I have only come across examples where the Cauchy sequence where fails to converge only because the limit point is not in and an extension to typically by completion fixes the problem. For example: where is the space of polynomial functions on with uniform convergence norm. I want to know if this is the only kind of failure mode for the convergence of a Cauchy sequence.,"E \{x_n\} x_n\in E E E x_n\colon[0,1]\to\Bbb R, \quad t\mapsto \sum_{k=0}^n\frac{t^k}{k!}, E\triangleq \mathcal{P}([0,1]) [0,1]","['real-analysis', 'convergence-divergence', 'normed-spaces', 'cauchy-sequences']"
9,Show $\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$ Proof verification,Show  Proof verification,\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x),"Show $$\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$$ Proof: By definition: $$f'(x) = \lim_{h \to \ 0} \frac{f(x + h) - f(x)}{h}$$ Using this idea it would imply: $$1)\ \ f''(x) = \lim_{h \to \ 0} \frac{f'(x + h) - f'(x)}{h}$$ As such it is required that I find an expression for $f'(x+h)$ . This is where I'm not sure if the step I took is legitimate. An expression for $f'(x + h)$ is: $$f'(x+h) =  \lim_{h \to \ 0} \frac{f(x + 2h) - f(x + h)}{h}$$ Combining this with the definition of $f'(x)$ and inserting it into 1) you arrive at: $$\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x)$$ As required. Concern: I feel a discomfort with this solution. Even though ""mechanically"" it worked out, if I am taking the limit as $h \rightarrow 0$ that would mean $x + 2h$ and $x + h$ both go to $x$ . But I am attempting to use the idea that $x +2h$ goes to $x + h$ . Perhaps it is a notation idea that I need to communicate better, but I feel it is larger than just that.","Show Proof: By definition: Using this idea it would imply: As such it is required that I find an expression for . This is where I'm not sure if the step I took is legitimate. An expression for is: Combining this with the definition of and inserting it into 1) you arrive at: As required. Concern: I feel a discomfort with this solution. Even though ""mechanically"" it worked out, if I am taking the limit as that would mean and both go to . But I am attempting to use the idea that goes to . Perhaps it is a notation idea that I need to communicate better, but I feel it is larger than just that.",\lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x) f'(x) = \lim_{h \to \ 0} \frac{f(x + h) - f(x)}{h} 1)\ \ f''(x) = \lim_{h \to \ 0} \frac{f'(x + h) - f'(x)}{h} f'(x+h) f'(x + h) f'(x+h) =  \lim_{h \to \ 0} \frac{f(x + 2h) - f(x + h)}{h} f'(x) \lim_{h \to \ 0} \frac{f(x + 2h) - 2f(x+h) + f(x)}{h^{2}} = f''(x) h \rightarrow 0 x + 2h x + h x x +2h x + h,"['real-analysis', 'limits', 'proof-verification']"
10,Several ways to prove that $\sum\limits^\infty_{n=1}\left(1-\frac1{\sqrt{n}}\right)^n$ converges,Several ways to prove that  converges,\sum\limits^\infty_{n=1}\left(1-\frac1{\sqrt{n}}\right)^n,"I believe there are several ways to prove that $\sum\limits^{\infty}_{n=1}\left(1-\frac{1}{\sqrt{n}}\right)^n$ converges. Can you, please, post yours so that we can learn from you? HERE IS ONE Let $n\in\Bbb{N}$ be fixed such that $a_n=\left(1-\frac{1}{\sqrt{n}}\right)^n.$ Then, \begin{align} a_n&=\left(1-\frac{1}{\sqrt{n}}\right)^n \\&=\exp\ln\left(1-\frac{1}{\sqrt{n}}\right)^n\\&=\exp \left[n\ln\left(1-\frac{1}{\sqrt{n}}\right)\right] \\&=\exp\left[ -n\sum^{\infty}_{k=1}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&=\exp\left[ -n\left(\frac{1}{\sqrt{n}}+\frac{1}{2n}+\sum^{\infty}_{k=3}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right)\right]\\&=\exp \left[-\sqrt{n}-\frac{1}{2}-\sum^{\infty}_{k=3}\frac{n}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&\equiv\exp \left(-\sqrt{n}\right)\exp \left(-\frac{1}{2}\right)\end{align} Choose $b_n=\exp \left(-\sqrt{n}\right)$ , so that \begin{align} \dfrac{a_n}{b_n}\to\exp \left(-\frac{1}{2}\right).\end{align} Since $b_n \to 0$ , there exists $N$ such that for all $n\geq N,$ \begin{align} \exp \left(-\sqrt{n}\right)<\dfrac{1}{n^2}.\end{align} Hence, \begin{align}\sum^{\infty}_{n=N}b_n= \sum^{\infty}_{n=N}\exp \left(-\sqrt{n}\right)\leq \sum^{\infty}_{n=N}\dfrac{1}{n^2}<\infty,\end{align} and so, $\sum^{\infty}_{n=1}b_n<\infty\implies \sum^{\infty}_{n=1}a_n<\infty$ by Limit comparison test.","I believe there are several ways to prove that converges. Can you, please, post yours so that we can learn from you? HERE IS ONE Let be fixed such that Then, Choose , so that Since , there exists such that for all Hence, and so, by Limit comparison test.","\sum\limits^{\infty}_{n=1}\left(1-\frac{1}{\sqrt{n}}\right)^n n\in\Bbb{N} a_n=\left(1-\frac{1}{\sqrt{n}}\right)^n. \begin{align} a_n&=\left(1-\frac{1}{\sqrt{n}}\right)^n \\&=\exp\ln\left(1-\frac{1}{\sqrt{n}}\right)^n\\&=\exp \left[n\ln\left(1-\frac{1}{\sqrt{n}}\right)\right] \\&=\exp\left[ -n\sum^{\infty}_{k=1}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&=\exp\left[ -n\left(\frac{1}{\sqrt{n}}+\frac{1}{2n}+\sum^{\infty}_{k=3}\frac{1}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right)\right]\\&=\exp \left[-\sqrt{n}-\frac{1}{2}-\sum^{\infty}_{k=3}\frac{n}{k}\left(\frac{1}{\sqrt{n}}\right)^k\right]\\&\equiv\exp \left(-\sqrt{n}\right)\exp \left(-\frac{1}{2}\right)\end{align} b_n=\exp \left(-\sqrt{n}\right) \begin{align} \dfrac{a_n}{b_n}\to\exp \left(-\frac{1}{2}\right).\end{align} b_n \to 0 N n\geq N, \begin{align} \exp \left(-\sqrt{n}\right)<\dfrac{1}{n^2}.\end{align} \begin{align}\sum^{\infty}_{n=N}b_n= \sum^{\infty}_{n=N}\exp \left(-\sqrt{n}\right)\leq \sum^{\infty}_{n=N}\dfrac{1}{n^2}<\infty,\end{align} \sum^{\infty}_{n=1}b_n<\infty\implies \sum^{\infty}_{n=1}a_n<\infty","['real-analysis', 'sequences-and-series']"
11,Dual norm of $l_1$ of is $l_\infty$,Dual norm of  of is,l_1 l_\infty,"I am trying to show that $l_1$ norm's dual norm is $l_{\infty}$ norm. I have proceeded like the following: $||z||_D = \sup \{z^Tx| ||x||_1\leq 1 \}$ Then: $ z^Tx = \sum_{i=1}^n z_i x_i \leq \sum_{i=1}^n |z_i||x_i| \leq (\max_{i=1}^n |z_i|)\sum_{i=1}^n|x_i|$ Finally since $||x||_1 \leq 1$ , we have $z^Tx \leq \max_{i=1}^n |z_i|$ . With these, I am able to show that $l_{\infty}$ norm of $z$ is an upper bound of $z^Tx$ when $||x||_1\leq 1 $ . But I additionally need to show that it is the least upper bound to satifsy $sup$ , but I am stuck at this point.","I am trying to show that norm's dual norm is norm. I have proceeded like the following: Then: Finally since , we have . With these, I am able to show that norm of is an upper bound of when . But I additionally need to show that it is the least upper bound to satifsy , but I am stuck at this point.",l_1 l_{\infty} ||z||_D = \sup \{z^Tx| ||x||_1\leq 1 \}  z^Tx = \sum_{i=1}^n z_i x_i \leq \sum_{i=1}^n |z_i||x_i| \leq (\max_{i=1}^n |z_i|)\sum_{i=1}^n|x_i| ||x||_1 \leq 1 z^Tx \leq \max_{i=1}^n |z_i| l_{\infty} z z^Tx ||x||_1\leq 1  sup,"['real-analysis', 'linear-algebra', 'functional-analysis', 'normed-spaces']"
12,Summation of this series ${1\over 2}\left[1+{1 \over 3}\left({1 \over 4}\right)^2+ {1 \over 5}\left({1 \over 4}\right)^4+\cdots\right]$,Summation of this series,{1\over 2}\left[1+{1 \over 3}\left({1 \over 4}\right)^2+ {1 \over 5}\left({1 \over 4}\right)^4+\cdots\right],"I have this series, $${1\over 2}\left[1+{1 \over 3}\left({1 \over 4}\right)^2+ {1 \over 5}\left({1 \over 4}\right)^4+\cdots\right]$$ I have shown that this series converges. It looks like some combination of geometric series and harmonic series to me. I am not able to proceed from here. I would love to have some hint. Sorry this time I have nothing to write in ""my efforts"" as I am completely stuck.","I have this series, $${1\over 2}\left[1+{1 \over 3}\left({1 \over 4}\right)^2+ {1 \over 5}\left({1 \over 4}\right)^4+\cdots\right]$$ I have shown that this series converges. It looks like some combination of geometric series and harmonic series to me. I am not able to proceed from here. I would love to have some hint. Sorry this time I have nothing to write in ""my efforts"" as I am completely stuck.",,"['real-analysis', 'sequences-and-series']"
13,"Basis of $C[a,b]$",Basis of,"C[a,b]","The space $C[a,b]$ , space of all real valued continuous functions on $[a,b]$ is an infinite dimensional vector space over the field $\Bbb R$. As every vector space over a field has a basis so definitely $C[a,b]$ has a basis. I want to know a basis of $C[a,b]$. How I can find a basis of it ?","The space $C[a,b]$ , space of all real valued continuous functions on $[a,b]$ is an infinite dimensional vector space over the field $\Bbb R$. As every vector space over a field has a basis so definitely $C[a,b]$ has a basis. I want to know a basis of $C[a,b]$. How I can find a basis of it ?",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'vector-spaces', 'schauder-basis']"
14,Can the closure of a countable set be characterized sequentially?,Can the closure of a countable set be characterized sequentially?,,"Suppose that I have a countable subset $S \subset X$, where $(X, \tau)$ is a topological space that is NOT first countable (so that convergence is characterized by nets and not sequences). I'm interested in the closure $cl(S)$, which is defined as the collection of all limit points of $S$, where limit points of $S$ are defined as all elements $x \in X$ such that for every open set of $X$ containing $x$, there exists some element of $A$ within that open set (but does not equal $x$ itself). My question is then the following: Because the set $S$ is itself countable, can we say that $cl(S)$ is equal to the set of limit points of all sequences of elements in $S$ ? I understand when considering a potential limit point of $A$, there could be an uncountable many open sets that elements in $A$ must fall within, but on the other hand, there are only a countable many elements of $A$ in the first place. On which side of things does this fall? Thanks!","Suppose that I have a countable subset $S \subset X$, where $(X, \tau)$ is a topological space that is NOT first countable (so that convergence is characterized by nets and not sequences). I'm interested in the closure $cl(S)$, which is defined as the collection of all limit points of $S$, where limit points of $S$ are defined as all elements $x \in X$ such that for every open set of $X$ containing $x$, there exists some element of $A$ within that open set (but does not equal $x$ itself). My question is then the following: Because the set $S$ is itself countable, can we say that $cl(S)$ is equal to the set of limit points of all sequences of elements in $S$ ? I understand when considering a potential limit point of $A$, there could be an uncountable many open sets that elements in $A$ must fall within, but on the other hand, there are only a countable many elements of $A$ in the first place. On which side of things does this fall? Thanks!",,"['real-analysis', 'general-topology', 'elementary-set-theory']"
15,All continuous $f$ such that $\sin(f(x)) = \sin(x)$,All continuous  such that,f \sin(f(x)) = \sin(x),"Question Find all continuous $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $\sin(f(x)) = \sin(x)$ $\forall x \in \mathbb{R}$. Here is my thinking for this problem: Since $\sin(k\pi) = 0, \forall k \in \mathbb{Z}$ we require an $f$ which is an integer multiple of $\pi$ at integer multiples of $\pi$ and since $\sin$ is $2\pi$ periodic we require the 'translation' associated with periodicity to be an even multiple of $\pi$. Because of continuity we require a linear solution and due to the fact that $\sin$ is odd, we require the coefficient of the $x$ term to be $1$. Thus the only solutions take the form: $f(x) = x + 2m\pi, m \in \mathbb{Z}$. My question is, have I missed any solutions? How might I know that these are the only ones?","Question Find all continuous $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $\sin(f(x)) = \sin(x)$ $\forall x \in \mathbb{R}$. Here is my thinking for this problem: Since $\sin(k\pi) = 0, \forall k \in \mathbb{Z}$ we require an $f$ which is an integer multiple of $\pi$ at integer multiples of $\pi$ and since $\sin$ is $2\pi$ periodic we require the 'translation' associated with periodicity to be an even multiple of $\pi$. Because of continuity we require a linear solution and due to the fact that $\sin$ is odd, we require the coefficient of the $x$ term to be $1$. Thus the only solutions take the form: $f(x) = x + 2m\pi, m \in \mathbb{Z}$. My question is, have I missed any solutions? How might I know that these are the only ones?",,"['real-analysis', 'continuity', 'functional-equations']"
16,"Working through Baby Rudin, what's the best approach? [duplicate]","Working through Baby Rudin, what's the best approach? [duplicate]",,"This question already has answers here : What are some good math specific study habits? (7 answers) Closed 7 years ago . I'm currently working through Principles of Mathematical Analysis , and I'm thoroughly enjoying it, but I wanted to know from some of you who've read it before, what's the best approach to get the most out of Baby Rudin? I'm hoping to complete most, if not all of the exercises in it, that I'm sure of. No questions there. What I'm not so sure about is note taking, currently I'm taking truly copious amounts of notes, to the point where I've copy-pasted almost everything in the first chapter into my own book. It would be completely illogical to rewrite the whole book out just for the sake of it, and it would take me forever to complete the book that way. I know as I'm typing this, that this is a very subjective matter, and some people prefer to take copious amounts of notes, while others prefer to take none at all, but since this is my first true Definition-Theorem-Proof style book (if you can call it that), I want to find out what's the best way to tackle the book. For those of you who've read Baby Rudin, and more generally for all Definition-Theorem-Proof style books, how do you go about taking down notes? Do you copy-paste most of the stuff (i.e as I did)? Do you only take down notes on proofs and use the book as a reference for definitions and theorems? Do you take down all the definitions and theorems? Do you take down any of the exposition stuff (comments etc.)? Do you take as little notes as possible and only worry about doing the exercises? Do you try and reprove everything and not worry about notes as much? I know that each person studies in their own way, and this may be the softest of soft questions on Math.SE, but I'm highly interested to see how others have gone about working through Baby Rudin.","This question already has answers here : What are some good math specific study habits? (7 answers) Closed 7 years ago . I'm currently working through Principles of Mathematical Analysis , and I'm thoroughly enjoying it, but I wanted to know from some of you who've read it before, what's the best approach to get the most out of Baby Rudin? I'm hoping to complete most, if not all of the exercises in it, that I'm sure of. No questions there. What I'm not so sure about is note taking, currently I'm taking truly copious amounts of notes, to the point where I've copy-pasted almost everything in the first chapter into my own book. It would be completely illogical to rewrite the whole book out just for the sake of it, and it would take me forever to complete the book that way. I know as I'm typing this, that this is a very subjective matter, and some people prefer to take copious amounts of notes, while others prefer to take none at all, but since this is my first true Definition-Theorem-Proof style book (if you can call it that), I want to find out what's the best way to tackle the book. For those of you who've read Baby Rudin, and more generally for all Definition-Theorem-Proof style books, how do you go about taking down notes? Do you copy-paste most of the stuff (i.e as I did)? Do you only take down notes on proofs and use the book as a reference for definitions and theorems? Do you take down all the definitions and theorems? Do you take down any of the exposition stuff (comments etc.)? Do you take as little notes as possible and only worry about doing the exercises? Do you try and reprove everything and not worry about notes as much? I know that each person studies in their own way, and this may be the softest of soft questions on Math.SE, but I'm highly interested to see how others have gone about working through Baby Rudin.",,"['real-analysis', 'analysis', 'reference-request', 'soft-question', 'book-recommendation']"
17,"Evaluating $\int_0^1\int_0^1 e^{\max\{x^2,y^2\}\,}\mathrm dx\,\mathrm dy$",Evaluating,"\int_0^1\int_0^1 e^{\max\{x^2,y^2\}\,}\mathrm dx\,\mathrm dy","The integral again for convenience is $$ I=\int_0^1\int_0^1 e^{\max\{x^2,y^2\}}\,\mathrm dx\,\mathrm dy $$ My thoughts: Ignoring for a moment that the region is a rectangle, I hoped moving to polar coordinates might help. This gives $$ I=\int_0^1\int_0^{2\pi}re^{r^2\max\{\cos^2 t,\sin^2t\}} \, \mathrm dt \, \mathrm dr $$ Then since $|\cos t|\geq |\sin t|$ for $t\in D_1=[-\frac{\pi}{4},\frac{\pi}{4}]\cup [\frac{3\pi}{4},\frac{5\pi}{4}]$ but not for $t\in D_2=[\frac{\pi}{4},\frac{3\pi}{4}]\cup [\frac{5\pi}{4},\frac{7\pi}{4}]$ I think we can break $I$ into  $$ I=\left[\int_0^1\int_{D_1}re^{r^2\cos^2 t}\,\mathrm dt\,\mathrm dr\right] \left[\int_0^1\int_{D_2}re^{r^2\sin^2 t}\,\mathrm dt\,\mathrm dr\right] $$ Aside from the problem of the region not being the same, I am stuck here. Is the work above on the right track? How do I evaluate for $[0,1]\times[0,1]$? Thanks!","The integral again for convenience is $$ I=\int_0^1\int_0^1 e^{\max\{x^2,y^2\}}\,\mathrm dx\,\mathrm dy $$ My thoughts: Ignoring for a moment that the region is a rectangle, I hoped moving to polar coordinates might help. This gives $$ I=\int_0^1\int_0^{2\pi}re^{r^2\max\{\cos^2 t,\sin^2t\}} \, \mathrm dt \, \mathrm dr $$ Then since $|\cos t|\geq |\sin t|$ for $t\in D_1=[-\frac{\pi}{4},\frac{\pi}{4}]\cup [\frac{3\pi}{4},\frac{5\pi}{4}]$ but not for $t\in D_2=[\frac{\pi}{4},\frac{3\pi}{4}]\cup [\frac{5\pi}{4},\frac{7\pi}{4}]$ I think we can break $I$ into  $$ I=\left[\int_0^1\int_{D_1}re^{r^2\cos^2 t}\,\mathrm dt\,\mathrm dr\right] \left[\int_0^1\int_{D_2}re^{r^2\sin^2 t}\,\mathrm dt\,\mathrm dr\right] $$ Aside from the problem of the region not being the same, I am stuck here. Is the work above on the right track? How do I evaluate for $[0,1]\times[0,1]$? Thanks!",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
18,Can the directional derivative fail to be linear?,Can the directional derivative fail to be linear?,,"Is it possible for the directional derivative for a function $f$ in the direction of a vector $v$ , $D_vf(x) = \lim_{h \to 0} \frac{f(x + hv) - f(x)}h$ to exist for every vector $v$ , and yet $v \mapsto D_vf(x)$ fails to be linear? Here we have a function which seems to be displaying that very phenomenon: $$f(x,y) = \frac{32x^3}{x^2 + y^2} - \frac{16x^5}{(x^2 + y^2)^2} - 14x$$ As you can see, if you follow the map at the origin in the direction of any vector, it appears to follow a straight line, i.e. the derivative in that direction is constant in the direction of $v$ . Yet the ""slope"" in each direction is clearly not behaving in a linear manner. Is this really the case? Or is it just a misleading graph? I'm inclined to believe the former, but what I'm looking for is a rigorous argument. Further, under what conditions might a derivative fail to be linear? Or, what are necessary conditions to ensure that a proof that ""the directional derivative is linear"" actually works?","Is it possible for the directional derivative for a function in the direction of a vector , to exist for every vector , and yet fails to be linear? Here we have a function which seems to be displaying that very phenomenon: As you can see, if you follow the map at the origin in the direction of any vector, it appears to follow a straight line, i.e. the derivative in that direction is constant in the direction of . Yet the ""slope"" in each direction is clearly not behaving in a linear manner. Is this really the case? Or is it just a misleading graph? I'm inclined to believe the former, but what I'm looking for is a rigorous argument. Further, under what conditions might a derivative fail to be linear? Or, what are necessary conditions to ensure that a proof that ""the directional derivative is linear"" actually works?","f v D_vf(x) = \lim_{h \to 0} \frac{f(x + hv) - f(x)}h v v \mapsto D_vf(x) f(x,y) = \frac{32x^3}{x^2 + y^2} - \frac{16x^5}{(x^2 + y^2)^2} - 14x v","['real-analysis', 'multivariable-calculus']"
19,"Prove that if $\sum_{n=1}^ \infty na_n$ converges, then $\sum_{n=1}^ \infty a_n$ converges.","Prove that if  converges, then  converges.",\sum_{n=1}^ \infty na_n \sum_{n=1}^ \infty a_n,"Prove that if $\displaystyle \sum_{n=1}^ \infty na_n$ converges, then $\displaystyle\sum_{n=1}^ \infty a_n$ converges. No, $a_n$ are not necessarily positive numbers. I've been trying summation by parts.","Prove that if $\displaystyle \sum_{n=1}^ \infty na_n$ converges, then $\displaystyle\sum_{n=1}^ \infty a_n$ converges. No, $a_n$ are not necessarily positive numbers. I've been trying summation by parts.",,"['real-analysis', 'sequences-and-series', 'analysis']"
20,Is $f(x)=x|x|$ differentiable everywhere?,Is  differentiable everywhere?,f(x)=x|x|,"When $f$ is a function $\mathbf{R}$ to $R$. I know $\lim_{x \to 0+}\frac{f(x)-f(0)}{x-0}= \lim_{x \to 0+}\frac{x^2}{x}=0$ and $\lim_{x \to 0-}\frac{f(x)-f(0)}{x-0}= \lim_{x \to 0-}\frac{-x^2}{x}=0$, but $|x|$ is not differentiable everywhere, so I'm doubting myself.","When $f$ is a function $\mathbf{R}$ to $R$. I know $\lim_{x \to 0+}\frac{f(x)-f(0)}{x-0}= \lim_{x \to 0+}\frac{x^2}{x}=0$ and $\lim_{x \to 0-}\frac{f(x)-f(0)}{x-0}= \lim_{x \to 0-}\frac{-x^2}{x}=0$, but $|x|$ is not differentiable everywhere, so I'm doubting myself.",,"['calculus', 'real-analysis', 'derivatives']"
21,Question about the proof of Rudin's Theorem 2.30,Question about the proof of Rudin's Theorem 2.30,,"The theorem states: Suppose $Y \subset X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some subset $G$ of $X$. I think the proof in the forward direction is relatively clear, however I have some problems relating the backward direction. The proof is relatively quick and goes as (Rudin, pg. 36): If $G$ is open in X and $E = G \cap Y$, every $p \in E$ has a neighborhood $V_p \subset G$ (open ball $B_{r_p}(p) = \{x \in X: d(p, x) < r_p \}$). Then $V_p \cap Y \subset E$, so that $E$ is open relative to Y. In order to prove that $E$ itself is and open set in $Y$, wouldn't we want to prove that for each $p \in E$, there is an open ball contained in $Y$. Thus would it work to remedy the proof by taking a ball for each $p$ with the following radius: $r_p' = \min \{ r_p, \sup_{x \in E} d(p, x) \}$ ? Then we could guarantee that the ball that is guaranteed by the openness of $G$ will let conclude the openness of $E$ relative to $Y$. Thank you very much.","The theorem states: Suppose $Y \subset X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some subset $G$ of $X$. I think the proof in the forward direction is relatively clear, however I have some problems relating the backward direction. The proof is relatively quick and goes as (Rudin, pg. 36): If $G$ is open in X and $E = G \cap Y$, every $p \in E$ has a neighborhood $V_p \subset G$ (open ball $B_{r_p}(p) = \{x \in X: d(p, x) < r_p \}$). Then $V_p \cap Y \subset E$, so that $E$ is open relative to Y. In order to prove that $E$ itself is and open set in $Y$, wouldn't we want to prove that for each $p \in E$, there is an open ball contained in $Y$. Thus would it work to remedy the proof by taking a ball for each $p$ with the following radius: $r_p' = \min \{ r_p, \sup_{x \in E} d(p, x) \}$ ? Then we could guarantee that the ball that is guaranteed by the openness of $G$ will let conclude the openness of $E$ relative to $Y$. Thank you very much.",,['real-analysis']
22,"Let $f : \mathbb{R} → \mathbb{R}$ be continuous, with $f(x)f(f(x)) = 1$ for all $x ∈ \mathbb{R}$. If $f(1000) = 999$, find $f(500)$.","Let  be continuous, with  for all . If , find .",f : \mathbb{R} → \mathbb{R} f(x)f(f(x)) = 1 x ∈ \mathbb{R} f(1000) = 999 f(500),"Let $f : \mathbb{R} → \mathbb{R}$ be continuous, with $f(x)f(f(x)) = 1$ for all $x ∈ \mathbb{R}$. If $f(1000) = 999$, find $f(500)$. I try to solve this problem, but I don't know how to use de continuity on $f$. Is anyone could give me a little hint on the continuity? And please, don't give me the answer to the question.","Let $f : \mathbb{R} → \mathbb{R}$ be continuous, with $f(x)f(f(x)) = 1$ for all $x ∈ \mathbb{R}$. If $f(1000) = 999$, find $f(500)$. I try to solve this problem, but I don't know how to use de continuity on $f$. Is anyone could give me a little hint on the continuity? And please, don't give me the answer to the question.",,[]
23,"What are ""set-theoretic maps""?","What are ""set-theoretic maps""?",,"Can someone explain to me what is the meaning of “set-theoretic maps”? I've encountered this term in real analysis in $n$ variables.  Specifically, I encountered it in the following statement: Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a $C^1$ function such that $Df(x)$, its derivative matrix, is invertible for every point of domain. Let $U$ be an open subset of the domain s.t. $f$ is one-one. Now, define a ""set theoretic function"" $g:f(U)\to\mathbb{R}^n$. Then $g$ is a $C^1$ function.","Can someone explain to me what is the meaning of “set-theoretic maps”? I've encountered this term in real analysis in $n$ variables.  Specifically, I encountered it in the following statement: Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a $C^1$ function such that $Df(x)$, its derivative matrix, is invertible for every point of domain. Let $U$ be an open subset of the domain s.t. $f$ is one-one. Now, define a ""set theoretic function"" $g:f(U)\to\mathbb{R}^n$. Then $g$ is a $C^1$ function.",,"['real-analysis', 'elementary-set-theory', 'terminology']"
24,"If $f$ is in $L^{p}$, prove that $\lim \int_{x}^{x+1} f(t) dt = 0$","If  is in , prove that",f L^{p} \lim \int_{x}^{x+1} f(t) dt = 0,"If $f$ is in $L^p$, prove that $\lim_{x \to \infty} \int_{x}^{x+1} f(t) dt = 0$ It is easy to think that integration must be vanish as $x \to \infty $ but I cannot write them with math. Suppose not. Then there exist $P$ such that if $p\geq P$ then $\int_{p}^{p+1} f(t)dt >0$. So with $p_1=P$ and $p_k=p_{k+1}-1$, $\lim_{n \to \infty}\sum_{k=1}^{n}\int_{p_k}^{p_{k+1}}f(t)dt$ should be $>\infty$(I think) Please help. Thank you!","If $f$ is in $L^p$, prove that $\lim_{x \to \infty} \int_{x}^{x+1} f(t) dt = 0$ It is easy to think that integration must be vanish as $x \to \infty $ but I cannot write them with math. Suppose not. Then there exist $P$ such that if $p\geq P$ then $\int_{p}^{p+1} f(t)dt >0$. So with $p_1=P$ and $p_k=p_{k+1}-1$, $\lim_{n \to \infty}\sum_{k=1}^{n}\int_{p_k}^{p_{k+1}}f(t)dt$ should be $>\infty$(I think) Please help. Thank you!",,"['real-analysis', 'lebesgue-integral', 'lp-spaces']"
25,why use open interval rather than closed interval as open sets for real line topology,why use open interval rather than closed interval as open sets for real line topology,,The set of all closed intervals of the real line also satisfies the open sets axioms. 1.The empty set and X itself are open. 2.Any union of open sets is open. 3.The intersection of any finite number of open sets is open. Why the standard real line topology is defined on open intervals rather than using closed intervals as open sets?,The set of all closed intervals of the real line also satisfies the open sets axioms. 1.The empty set and X itself are open. 2.Any union of open sets is open. 3.The intersection of any finite number of open sets is open. Why the standard real line topology is defined on open intervals rather than using closed intervals as open sets?,,"['real-analysis', 'general-topology']"
26,Positive operator has a positive spectrum?,Positive operator has a positive spectrum?,,"Let $T : \operatorname{dom}(T) \rightarrow H $ be a positive self-adjoint operator, is it then true that $\sigma(T) \subset [0,\infty)$? This is something that sounds natural and I guess that it is true. For bounded operator, this can be easily shown, but the only proof I know cannot be adapted to unbounded operator, so I was wondering if anybody here knows how to show this?","Let $T : \operatorname{dom}(T) \rightarrow H $ be a positive self-adjoint operator, is it then true that $\sigma(T) \subset [0,\infty)$? This is something that sounds natural and I guess that it is true. For bounded operator, this can be easily shown, but the only proof I know cannot be adapted to unbounded operator, so I was wondering if anybody here knows how to show this?",,"['real-analysis', 'analysis']"
27,"If $X=\{x_n\}, Y = \{y_n\}$ be bounded sequences of real numbers. Then, if $x_n \leq y_n~\forall~n$, then $\lim \inf (x_n) \leq \lim \inf(y_n)$ [duplicate]","If  be bounded sequences of real numbers. Then, if , then  [duplicate]","X=\{x_n\}, Y = \{y_n\} x_n \leq y_n~\forall~n \lim \inf (x_n) \leq \lim \inf(y_n)","This question already has answers here : $X_n\leq Y_n$ implies $\liminf X_n \leq \liminf Y_n$ and $\limsup X_n \leq \limsup Y_n$ (4 answers) Closed 9 years ago . If $X=\{x_n\}, Y = \{y_n\}$ be bounded sequences of real numbers. Then, if $x_n \leq y_n~\forall~n$, then show that $\lim \inf (x_n) \leq \lim \inf(y_n)$ and $\lim \sup (x_n) \leq \lim \sup (y_n)$ Attempt: $\lim \inf (x_n)$ represents the least limit point in $X$ and $\lim \inf(y_n)$ represents the least limit point of $Y$. Given that,  $~\forall~m, x_m \leq y_m, $  I am still not able to see any compulsion why the least limit point of $x_n$ should be less than the least limit point of $y_n$. That is, for each index $m, x_m \leq y_m$. But, that doesn't place any restriction on the least limit point of $x_n$ becoming greater than the least limit point of $y_n$? Where could I be going wrong? Thank you for your help.","This question already has answers here : $X_n\leq Y_n$ implies $\liminf X_n \leq \liminf Y_n$ and $\limsup X_n \leq \limsup Y_n$ (4 answers) Closed 9 years ago . If $X=\{x_n\}, Y = \{y_n\}$ be bounded sequences of real numbers. Then, if $x_n \leq y_n~\forall~n$, then show that $\lim \inf (x_n) \leq \lim \inf(y_n)$ and $\lim \sup (x_n) \leq \lim \sup (y_n)$ Attempt: $\lim \inf (x_n)$ represents the least limit point in $X$ and $\lim \inf(y_n)$ represents the least limit point of $Y$. Given that,  $~\forall~m, x_m \leq y_m, $  I am still not able to see any compulsion why the least limit point of $x_n$ should be less than the least limit point of $y_n$. That is, for each index $m, x_m \leq y_m$. But, that doesn't place any restriction on the least limit point of $x_n$ becoming greater than the least limit point of $y_n$? Where could I be going wrong? Thank you for your help.",,"['real-analysis', 'inequality', 'convergence-divergence', 'limsup-and-liminf']"
28,Prove that $\lim_{n \rightarrow \infty} \frac{1}{n}\int^n_0xf(x)dx = 0$,Prove that,\lim_{n \rightarrow \infty} \frac{1}{n}\int^n_0xf(x)dx = 0,"Let $f(x) \geq 0$ be continuous on the interval $[0, \infty)$, and suppose that $\int_0^\infty f(x)dx < \infty$.  Prove that $\displaystyle \lim_{n \rightarrow \infty} \frac{1}{n}\int^n_0xf(x)dx = 0$ I want to use some version of dominated convergence theorem somewhere, and I have that the integral is equal to $\displaystyle \int_0^1nyf(ny)dy$ using change of variables.  Some help would be great.  Thanks.","Let $f(x) \geq 0$ be continuous on the interval $[0, \infty)$, and suppose that $\int_0^\infty f(x)dx < \infty$.  Prove that $\displaystyle \lim_{n \rightarrow \infty} \frac{1}{n}\int^n_0xf(x)dx = 0$ I want to use some version of dominated convergence theorem somewhere, and I have that the integral is equal to $\displaystyle \int_0^1nyf(ny)dy$ using change of variables.  Some help would be great.  Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
29,Multivariable version of the extreme value theorem,Multivariable version of the extreme value theorem,,"The Wikipedia entry on the extreme value theorem says that if $f$ is a real-valued continuous function on a closed and bounded interval $[a,b]$, then $f$ must attain a maximum value, i.e. there exists an $x \in [a,b]$ such that $f(x) \geq f(y)$ for all $y \in [a,b]$. I think that there is a more general version of the extreme value theorem which states a similar result for a closed and bounded subset of $\mathbb{R}^n$. (At least I think I remember hearing about this in a class on metric spaces.) Is there a statement of this more general version of the theorem, hopefully with a reference as well?","The Wikipedia entry on the extreme value theorem says that if $f$ is a real-valued continuous function on a closed and bounded interval $[a,b]$, then $f$ must attain a maximum value, i.e. there exists an $x \in [a,b]$ such that $f(x) \geq f(y)$ for all $y \in [a,b]$. I think that there is a more general version of the extreme value theorem which states a similar result for a closed and bounded subset of $\mathbb{R}^n$. (At least I think I remember hearing about this in a class on metric spaces.) Is there a statement of this more general version of the theorem, hopefully with a reference as well?",,"['real-analysis', 'analysis']"
30,Differentiability of product/composition of function,Differentiability of product/composition of function,,"How will be the product and composition of two functions, where one is differentiable and another is just continuous, behave?I mean to say, if the product or composition is differentiable, then what are the conditions on the functions to hold for their product to be differentiable, if they are? Some trivial checks: Of course, the product/composition is not always differentiable since if we take the differentiable function to be I (or x), then the result is obviously not differentiable. So what I ask for is that when they do; why?","How will be the product and composition of two functions, where one is differentiable and another is just continuous, behave?I mean to say, if the product or composition is differentiable, then what are the conditions on the functions to hold for their product to be differentiable, if they are? Some trivial checks: Of course, the product/composition is not always differentiable since if we take the differentiable function to be I (or x), then the result is obviously not differentiable. So what I ask for is that when they do; why?",,"['real-analysis', 'functional-analysis', 'derivatives']"
31,"Integral $I=\int_0^\infty \frac{e^{\alpha x}-e^{\beta x}}{x(e^{\alpha x}+1)(e^{\beta x}+1)}dx, \ \ \alpha>\beta>0. $",Integral,"I=\int_0^\infty \frac{e^{\alpha x}-e^{\beta x}}{x(e^{\alpha x}+1)(e^{\beta x}+1)}dx, \ \ \alpha>\beta>0. ","$$ I(\alpha,\beta)=\int_0^\infty \frac{e^{\alpha x}-e^{\beta x}}{x(e^{\alpha x}+1)(e^{\beta x}+1)}dx, \ \ \alpha>\beta>0. $$ I am trying to solve this integral.  This is from the old high school days in Bulgaria, although I cannot find the solutions anymore. Thanks","$$ I(\alpha,\beta)=\int_0^\infty \frac{e^{\alpha x}-e^{\beta x}}{x(e^{\alpha x}+1)(e^{\beta x}+1)}dx, \ \ \alpha>\beta>0. $$ I am trying to solve this integral.  This is from the old high school days in Bulgaria, although I cannot find the solutions anymore. Thanks",,"['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
32,Is every Riemann-integrable function lebesgue integrable?,Is every Riemann-integrable function lebesgue integrable?,,"I thought every Riemann integrable function is lebesgue integrable. But here , a user comments that the statement is not correct.  I'm confused.  Can someone clarify?","I thought every Riemann integrable function is lebesgue integrable. But here , a user comments that the statement is not correct.  I'm confused.  Can someone clarify?",,"['real-analysis', 'integration']"
33,$\sum \sqrt{a_n b_n}$ converges when $\sum a_n$ and $\sum b_n$ converge,converges when  and  converge,\sum \sqrt{a_n b_n} \sum a_n \sum b_n,"I am to show that $\sum \sqrt{a_n b_n}$ converges when $\sum a_n$ and $\sum b_n$ converge (here the series are assumed to have non-negative terms). I am unsure how to approach this problem; since I don't know what the series would converge to, I tried using Cauchy's criterion. Hence my goal was to bound $$ \sum_{i=n+1}^{n+k} \sqrt{a_i b_i}$$ for some $n$ large enough and any $k \geq 1.$ I tried to express this in terms of the $a_i$ and $b_i$ separately (to use convergence of the series $\sum a_n$ and $\sum b_n$) by writing the above expression as $$\frac{1}{2} \left( \sum_{i=n+1}^{n+k} (\sqrt{a_i}+\sqrt{b_i})^2 - \sum_{i=n+1}^{n+k} a_i - \sum_{i=n+1}^{n+k} b_i \right),$$ but I'm not sure if this really helps. Thanks for any help.","I am to show that $\sum \sqrt{a_n b_n}$ converges when $\sum a_n$ and $\sum b_n$ converge (here the series are assumed to have non-negative terms). I am unsure how to approach this problem; since I don't know what the series would converge to, I tried using Cauchy's criterion. Hence my goal was to bound $$ \sum_{i=n+1}^{n+k} \sqrt{a_i b_i}$$ for some $n$ large enough and any $k \geq 1.$ I tried to express this in terms of the $a_i$ and $b_i$ separately (to use convergence of the series $\sum a_n$ and $\sum b_n$) by writing the above expression as $$\frac{1}{2} \left( \sum_{i=n+1}^{n+k} (\sqrt{a_i}+\sqrt{b_i})^2 - \sum_{i=n+1}^{n+k} a_i - \sum_{i=n+1}^{n+k} b_i \right),$$ but I'm not sure if this really helps. Thanks for any help.",,"['real-analysis', 'sequences-and-series']"
34,Concerning the sequence $\Big(\dfrac {\tan n}{n}\Big) $,Concerning the sequence,\Big(\dfrac {\tan n}{n}\Big) ,"Is the sequence $\Big(\dfrac {\tan n}{n}\Big) $ convergent ? If not convergent , is it properly divergent i.e. tends to either $+\infty$ or $-\infty$ ? ( Owing to $\tan (n+1)= \dfrac {\tan n + \tan 1}{1- \tan1 \tan n}$ and the non-covergence of $\Big (\tan n \Big)$ it is easy to see that if $\Big(\dfrac {\tan n}{n}\Big) $ is convergent then the limit must be  $0$ , but I can not proceed further . )","Is the sequence $\Big(\dfrac {\tan n}{n}\Big) $ convergent ? If not convergent , is it properly divergent i.e. tends to either $+\infty$ or $-\infty$ ? ( Owing to $\tan (n+1)= \dfrac {\tan n + \tan 1}{1- \tan1 \tan n}$ and the non-covergence of $\Big (\tan n \Big)$ it is easy to see that if $\Big(\dfrac {\tan n}{n}\Big) $ is convergent then the limit must be  $0$ , but I can not proceed further . )",,"['real-analysis', 'sequences-and-series', 'trigonometry']"
35,Polynomials are dense in $L^2$,Polynomials are dense in,L^2,"I know that the function $e^{inx}$ can be uniformly approximated on $[-\pi,\pi]$ by polynomials in $x$. I want to use this to show that polynomials are dense in $L^2([-\pi,\pi])$. Suppose that $f\in L^2([-\pi,\pi])$. I want to show that for any $\epsilon>0$, there is a polynomial $p$ such that $|\int_{-\pi}^\pi (f(x)-p(x))^2dx|<\epsilon$. I was thinking about writing $f$ in terms of its coefficients, i.e. $$f(x)=\sum_{n=-\infty}^\infty \hat{f}(n)e^{inx}$$ But I'm still not sure how this can lead to the polynomial $p$.","I know that the function $e^{inx}$ can be uniformly approximated on $[-\pi,\pi]$ by polynomials in $x$. I want to use this to show that polynomials are dense in $L^2([-\pi,\pi])$. Suppose that $f\in L^2([-\pi,\pi])$. I want to show that for any $\epsilon>0$, there is a polynomial $p$ such that $|\int_{-\pi}^\pi (f(x)-p(x))^2dx|<\epsilon$. I was thinking about writing $f$ in terms of its coefficients, i.e. $$f(x)=\sum_{n=-\infty}^\infty \hat{f}(n)e^{inx}$$ But I'm still not sure how this can lead to the polynomial $p$.",,"['real-analysis', 'measure-theory']"
36,What is the motivation of uniform continuity?,What is the motivation of uniform continuity?,,"At the moment, in my very limited knowledge of mathematics (=my first year at college), I've just seen this concept applied to prove that a function is Riemann integrable. I'd like to know if this concept was motivated for some special reason, or what is the importance in further studies of mathematics.","At the moment, in my very limited knowledge of mathematics (=my first year at college), I've just seen this concept applied to prove that a function is Riemann integrable. I'd like to know if this concept was motivated for some special reason, or what is the importance in further studies of mathematics.",,"['calculus', 'real-analysis']"
37,Looking for a 3D smooth step function,Looking for a 3D smooth step function,,"In 2D, I use $y=e^{\frac{-0.01}{1-|x|^2}}$ to model a step function and smooth the transition. Anyone knows is there any good 3D step function and transiting smoothly. From $x-y$ plane, it should look like (1) A rectangular; (2) An elliptic.","In 2D, I use to model a step function and smooth the transition. Anyone knows is there any good 3D step function and transiting smoothly. From plane, it should look like (1) A rectangular; (2) An elliptic.",y=e^{\frac{-0.01}{1-|x|^2}} x-y,"['real-analysis', 'multivariable-calculus']"
38,Prove that $\sum_{k=1} ^ n k^{-1} = \ln(n) + O(1)$,Prove that,\sum_{k=1} ^ n k^{-1} = \ln(n) + O(1),"I would like to prove that $\sum_{k=1} ^ n k^{-1} = \ln(n) + O(1)$. That is, I would like to show that there is some natural number $N$ large enough so that $n \ge N$ implies: $$|\sum_{k=1}^n k^{-1} - \ln(n) | < M,$$ where $M$ is a positive constant independent of $n$. I think the idea behind the argument must be easy, but I am somehow missing it. I do see that $\sum_{k=1}^n k^{-1}$ can be taken as a sort of approximation, using areas of rectangles, to $\int_1^n \frac{1}{x}dx = \ln(n)$. So maybe the argument uses the integral comparison test? But, anyway, I'm still fumbling with how to proceed. Hints or solutions are greatly appreciated.","I would like to prove that $\sum_{k=1} ^ n k^{-1} = \ln(n) + O(1)$. That is, I would like to show that there is some natural number $N$ large enough so that $n \ge N$ implies: $$|\sum_{k=1}^n k^{-1} - \ln(n) | < M,$$ where $M$ is a positive constant independent of $n$. I think the idea behind the argument must be easy, but I am somehow missing it. I do see that $\sum_{k=1}^n k^{-1}$ can be taken as a sort of approximation, using areas of rectangles, to $\int_1^n \frac{1}{x}dx = \ln(n)$. So maybe the argument uses the integral comparison test? But, anyway, I'm still fumbling with how to proceed. Hints or solutions are greatly appreciated.",,['real-analysis']
39,Proving that a set is complete,Proving that a set is complete,,"I am working on a question and I am looking for some clarification. I can't seem to use what I know to complete the proof. Let $\mathbf{x}_{0} \in \mathbb{R}^n$ and $R>0$. Prove that $U=\left \{ \mathbf{x} \in \mathbb{R}^n : \left \| \mathbf{x} - \mathbf{x_{0}} \right \| \leq R \right \}$ is complete. What I know: $U$ is complete if every Cauchy sequence of points in $U$ converges to a point in $U$. If $\mathbf{x}_k$ is a Cauchy sequence then there is an integer $N$ such that  $$||\mathbf{x}_k - \mathbf{x}_l|| < \epsilon$$ for all $k,l \geq N$ $U$ is the set of points $\mathbf{x} \in \mathbb{R}^n$ such that the distance between $\mathbf{x}$ and $\mathbf{x}_0$ is less than or equal to $R$. A Cauchy sequence in $U$ will be made up of points $\mathbf{x}_{k,1},\mathbf{x}_{k,1},...,\mathbf{x}_{k,n}$ such that $|| \mathbf{x}_{k,1} - \mathbf{x}_{0}||$, $|| \mathbf{x}_{k,2} - \mathbf{x}_{0}||,..., || \mathbf{x}_{k,n} - \mathbf{x}_{0}||$ $\leq R$. It seems that if the sequence is made up of points in $U$ then it must converge to a point in $U$. Any help and clarification would be appreciated.","I am working on a question and I am looking for some clarification. I can't seem to use what I know to complete the proof. Let $\mathbf{x}_{0} \in \mathbb{R}^n$ and $R>0$. Prove that $U=\left \{ \mathbf{x} \in \mathbb{R}^n : \left \| \mathbf{x} - \mathbf{x_{0}} \right \| \leq R \right \}$ is complete. What I know: $U$ is complete if every Cauchy sequence of points in $U$ converges to a point in $U$. If $\mathbf{x}_k$ is a Cauchy sequence then there is an integer $N$ such that  $$||\mathbf{x}_k - \mathbf{x}_l|| < \epsilon$$ for all $k,l \geq N$ $U$ is the set of points $\mathbf{x} \in \mathbb{R}^n$ such that the distance between $\mathbf{x}$ and $\mathbf{x}_0$ is less than or equal to $R$. A Cauchy sequence in $U$ will be made up of points $\mathbf{x}_{k,1},\mathbf{x}_{k,1},...,\mathbf{x}_{k,n}$ such that $|| \mathbf{x}_{k,1} - \mathbf{x}_{0}||$, $|| \mathbf{x}_{k,2} - \mathbf{x}_{0}||,..., || \mathbf{x}_{k,n} - \mathbf{x}_{0}||$ $\leq R$. It seems that if the sequence is made up of points in $U$ then it must converge to a point in $U$. Any help and clarification would be appreciated.",,['real-analysis']
40,Norm of bounded operator on a complex Hilbert space.,Norm of bounded operator on a complex Hilbert space.,,"It is fairly easy to show that for a bounded linear operator $T$ on a Hilbert space $H$ $$\|T\|=\sup_{\|x\|=1,\|y\|=1}|\langle y, Tx \rangle |.$$ If $H$ is a complex Hilbert space, can you show that $$\|T\|=\sup_{\|x\|=1}|\langle x, Tx \rangle |\;?$$","It is fairly easy to show that for a bounded linear operator on a Hilbert space If is a complex Hilbert space, can you show that","T H \|T\|=\sup_{\|x\|=1,\|y\|=1}|\langle y, Tx \rangle |. H \|T\|=\sup_{\|x\|=1}|\langle x, Tx \rangle |\;?","['real-analysis', 'analysis', 'functional-analysis', 'operator-theory']"
41,Closed set in $\mathbb{R}^n$ is closure of some countable subset,Closed set in  is closure of some countable subset,\mathbb{R}^n,Let $A$ be a closed set in $\mathbb{R}^n$ . How can I show that $A$ = closure of $B$ where $B$ is countable ?  Thanks for any help .,Let $A$ be a closed set in $\mathbb{R}^n$ . How can I show that $A$ = closure of $B$ where $B$ is countable ?  Thanks for any help .,,['real-analysis']
42,Which of the following are Dense in $\mathbb{R}^2$?,Which of the following are Dense in ?,\mathbb{R}^2,"Which of the following sets are dense in $\mathbb R^2$ with respect to the usual topology. $\{ (x, y)\in\mathbb R^2 : x\in\mathbb N\}$ $\{ (x, y)\in\mathbb R^2 : x+y\in\mathbb Q\}$ $\{ (x, y)\in\mathbb R^2 : x^2 + y^2 = 5\}$ $\{ (x, y)\in\mathbb R^2 : xy\neq 0\}$. Any hint is welcome.","Which of the following sets are dense in $\mathbb R^2$ with respect to the usual topology. $\{ (x, y)\in\mathbb R^2 : x\in\mathbb N\}$ $\{ (x, y)\in\mathbb R^2 : x+y\in\mathbb Q\}$ $\{ (x, y)\in\mathbb R^2 : x^2 + y^2 = 5\}$ $\{ (x, y)\in\mathbb R^2 : xy\neq 0\}$. Any hint is welcome.",,['real-analysis']
43,"Show that $x \leq f(x) \leq 2x, \forall x\geq0$",Show that,"x \leq f(x) \leq 2x, \forall x\geq0","Prove: $$x \leq f(x) \leq 2x, \forall x\geq0$$ conditions: $f$ is differentiable $f(0) = 0$ $1 \leq f'(x) \le 2, \forall x\ge0$ I've tried to do it by limit defn but couldn't seem to get to the right solution: $$ 1 \le  \lim_{x \to c} \frac{f(x)-f(c)}{x-c} \le 2$$ how do i manipulate them in such a way that I get $$x \leq f(x) \leq 2x, \forall x\geq0 $$ I've also noticed that $f(x)$ is an increasing function as $f'(x) > 0$. Is this information of any use?","Prove: $$x \leq f(x) \leq 2x, \forall x\geq0$$ conditions: $f$ is differentiable $f(0) = 0$ $1 \leq f'(x) \le 2, \forall x\ge0$ I've tried to do it by limit defn but couldn't seem to get to the right solution: $$ 1 \le  \lim_{x \to c} \frac{f(x)-f(c)}{x-c} \le 2$$ how do i manipulate them in such a way that I get $$x \leq f(x) \leq 2x, \forall x\geq0 $$ I've also noticed that $f(x)$ is an increasing function as $f'(x) > 0$. Is this information of any use?",,['real-analysis']
44,Does there exist a countable collection of functions $\mathbb{R}^\mathbb{R}$ satisfying this property?,Does there exist a countable collection of functions  satisfying this property?,\mathbb{R}^\mathbb{R},"Let $\eta>0$. Does there exist a countable collection $\mathcal{F}$ of real-valued functions on $\mathbb{R}$ such that the following properties are equivalent? $x - y > \eta$ for all $f\in\mathcal{F}$, $f(x)>f(y)$ I can make it work for an uncountable collection of functions, but I believe it is false for the countable case. I have tried showing that there does not exist a finite collection with the desired property, but there was a flaw in my argument. I asked a question here to check if a certain statement is true, but it turns out there is a counterexample, so this invalidates my proof for the finite case.","Let $\eta>0$. Does there exist a countable collection $\mathcal{F}$ of real-valued functions on $\mathbb{R}$ such that the following properties are equivalent? $x - y > \eta$ for all $f\in\mathcal{F}$, $f(x)>f(y)$ I can make it work for an uncountable collection of functions, but I believe it is false for the countable case. I have tried showing that there does not exist a finite collection with the desired property, but there was a flaw in my argument. I asked a question here to check if a certain statement is true, but it turns out there is a counterexample, so this invalidates my proof for the finite case.",,"['real-analysis', 'functions']"
45,Show that if $x_n \to x$ then $\sqrt{x_n} \to \sqrt{x}$,Show that if  then,x_n \to x \sqrt{x_n} \to \sqrt{x},"Show that if $x_n \to x$ then $\sqrt{x_n} \to \sqrt{x}$ I have been stuck on this for a while. I tried $$|\sqrt{x_n} - \sqrt{x}| = \frac{|x_n - x|}{|\sqrt{x_n} + \sqrt{x}|},$$ and then I at least can get the top to be as small as I want, so I have $$\frac{\epsilon}{|\sqrt{x+\epsilon} + \sqrt{x}|},$$ but I get stuck here at choosing the N, and I don't know if my first step in breaking down the absolute value is legitimate. Please help.","Show that if $x_n \to x$ then $\sqrt{x_n} \to \sqrt{x}$ I have been stuck on this for a while. I tried $$|\sqrt{x_n} - \sqrt{x}| = \frac{|x_n - x|}{|\sqrt{x_n} + \sqrt{x}|},$$ and then I at least can get the top to be as small as I want, so I have $$\frac{\epsilon}{|\sqrt{x+\epsilon} + \sqrt{x}|},$$ but I get stuck here at choosing the N, and I don't know if my first step in breaking down the absolute value is legitimate. Please help.",,"['real-analysis', 'sequences-and-series', 'limits', 'radicals']"
46,Counterexample to $f_ng_n \not\to fg$ in measure,Counterexample to  in measure,f_ng_n \not\to fg,"I'm looking for a pair of sequences $f_n \to f$ in measure, $g_n \to g$ in measure, where $f_ng_n \not\!\to fg$ in measure. I've tried a number of things with characteristic functions that move around with $n$, and nothing seems to pan out. I've also tried looking at non-Lebesgue measures, like the counting measure. At least, I realize that whatever measure one uses must not be finite, or else $f_ng_n \to fg$ is always true. Any hints?","I'm looking for a pair of sequences $f_n \to f$ in measure, $g_n \to g$ in measure, where $f_ng_n \not\!\to fg$ in measure. I've tried a number of things with characteristic functions that move around with $n$, and nothing seems to pan out. I've also tried looking at non-Lebesgue measures, like the counting measure. At least, I realize that whatever measure one uses must not be finite, or else $f_ng_n \to fg$ is always true. Any hints?",,"['real-analysis', 'measure-theory']"
47,"$F$, Indefinite integral $\implies F$ is Absolutely Continuous",", Indefinite integral  is Absolutely Continuous",F \implies F,"One part of Theorem 14 (Chapter 5) of the book Real Analysis (3rd edition) by Royden, says that: if a function $F$ is an indefinite integral, then it is absolutely continuous. The proof says that this statement is obvious using the following Proposition: Let $f$ be a non-negative function which is integrable over a set $E$. Then given $\varepsilon \gt 0$, there is a $\delta \gt 0$ such that for every set $A\subset E$ with the $m(A)\lt \delta$, we have $$ \int_A f \lt \varepsilon.$$ I am failing to see how to use the above proposition in the proof and thus will need some help. Thanks.","One part of Theorem 14 (Chapter 5) of the book Real Analysis (3rd edition) by Royden, says that: if a function $F$ is an indefinite integral, then it is absolutely continuous. The proof says that this statement is obvious using the following Proposition: Let $f$ be a non-negative function which is integrable over a set $E$. Then given $\varepsilon \gt 0$, there is a $\delta \gt 0$ such that for every set $A\subset E$ with the $m(A)\lt \delta$, we have $$ \int_A f \lt \varepsilon.$$ I am failing to see how to use the above proposition in the proof and thus will need some help. Thanks.",,"['real-analysis', 'measure-theory']"
48,The $L^1$ convergence of $f(x - a_n) \to f(x)$,The  convergence of,L^1 f(x - a_n) \to f(x),"I'm trying to solve something, and I'm stuck. Suppose you have a function $$f \in L^1(\mathbf R) $$ and a sequence  of real numbers that converges to zero: $$ a_n \rightarrow 0 $$ define a sequence of functions by $$ f_n (x)=f(x-a_n)$$ does this sequence converges to $f(x)$ in the norm?","I'm trying to solve something, and I'm stuck. Suppose you have a function $$f \in L^1(\mathbf R) $$ and a sequence  of real numbers that converges to zero: $$ a_n \rightarrow 0 $$ define a sequence of functions by $$ f_n (x)=f(x-a_n)$$ does this sequence converges to $f(x)$ in the norm?",,"['real-analysis', 'functional-analysis']"
49,Does finiteness of $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ imply $\lim\limits_{x\to\infty}f'(x)=0$?,Does finiteness of  and  imply ?,\lim\limits_{x\to\infty}f(x) \lim\limits_{x\to\infty}f'(x) \lim\limits_{x\to\infty}f'(x)=0,"Assume that $f:{\bf R}\to{\bf R}$ is differentiable on ${\bf R}$, and both of $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ are finite. Geometrically, one may have $$\lim_{x\to\infty}f'(x)=0$$  Here is my question : How can one actually prove it? By definition, it suffices to show that $$\lim_{x\to\infty}\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=0$$ i.e. $$\forall \epsilon>0~\exists M>0\quad \textrm{s.t.}\quad x>M\Rightarrow \left|\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\right|<\epsilon$$ For large enough $M$ and small enough $\epsilon$, one has  $$|f(x+h)-f(x)|<\tilde{\epsilon}$$ But I have no idea how to go on.","Assume that $f:{\bf R}\to{\bf R}$ is differentiable on ${\bf R}$, and both of $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ are finite. Geometrically, one may have $$\lim_{x\to\infty}f'(x)=0$$  Here is my question : How can one actually prove it? By definition, it suffices to show that $$\lim_{x\to\infty}\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=0$$ i.e. $$\forall \epsilon>0~\exists M>0\quad \textrm{s.t.}\quad x>M\Rightarrow \left|\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\right|<\epsilon$$ For large enough $M$ and small enough $\epsilon$, one has  $$|f(x+h)-f(x)|<\tilde{\epsilon}$$ But I have no idea how to go on.",,['calculus']
50,Why is this function almost Lipschitz?,Why is this function almost Lipschitz?,,"We are still in the saga of solving the 2002 qualifier. This question 6b has stumped me and I am mostly clueless about it: Say $f:\mathbb{R}\rightarrow \mathbb{R}$ is bounded with a finite constant $B$ such that: $$\frac{|f(x+y)+f(x-y)-2f(x)|}{|y|}\leq B$$ Prove there exists $M(\lVert f \rVert_\infty, B)$ such that for all $x\not=y$ : $$|f(x)-f(y)|\leq M |x-y|\left(1+\ln_+(\frac{1}{|x-y|})\right)$$ Where $\ln_+(x)=\max \{0,\ln(x)\}$ Intuitively this means that away from $y=x$ , $f(x+y)\rightarrow f(x)$ linearly. Close to $x$ , it is still true $f(x+y)\rightarrow f(x)$ but it is slightly perturbed by $\ln_+$ . Here are a couple of facts which have gotten me nowhere: Fact 0. The inequality in $B$ would be an approximation for $f''(x)$ if it were divided by $y^2$ instead of $y$ . This is particularly useless, because we have no regularity associated with $f$ . Even if we did $|f''(x)|\leq \lim M/|y|=\infty$ so this observation cannot be of any help. Fact 1. $\lim_{y\rightarrow 0} |f(x-y)-f(x)|$ exists. One has by inverted triangle inequality that: $$\lim_{y\rightarrow 0}||f(x+y)-f(x)|-|f(x-y)-f(x)||\leq \lim_{y\rightarrow 0 }B|y|=0$$ But taking away the modulus and changing $\lim$ to $\limsup$ : $$\lim_{y\rightarrow 0}\sup_{y\in[-a,a]}(|f(x+y)-f(x)|-|f(x-y)-f(x)|)=0$$ $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x-y)-f(x)|$$ But as we are taking the infimum in a symetric interval, hence we may write: $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)|\leq \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|$$ $$\lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|=\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)|$$ This means that $\lim |f(x+y)-f(x)|$ exists. Fact 2. There is the obvious $y=y-x$ substitution $$\frac{|f(y)+f(2x-y)-2f(x)|}{|y-x|}\leq B$$ $$|f(y)-f(x)|\leq B|y-x|+|f(2x-y)-f(x)|$$","We are still in the saga of solving the 2002 qualifier. This question 6b has stumped me and I am mostly clueless about it: Say is bounded with a finite constant such that: Prove there exists such that for all : Where Intuitively this means that away from , linearly. Close to , it is still true but it is slightly perturbed by . Here are a couple of facts which have gotten me nowhere: Fact 0. The inequality in would be an approximation for if it were divided by instead of . This is particularly useless, because we have no regularity associated with . Even if we did so this observation cannot be of any help. Fact 1. exists. One has by inverted triangle inequality that: But taking away the modulus and changing to : But as we are taking the infimum in a symetric interval, hence we may write: This means that exists. Fact 2. There is the obvious substitution","f:\mathbb{R}\rightarrow \mathbb{R} B \frac{|f(x+y)+f(x-y)-2f(x)|}{|y|}\leq B M(\lVert f \rVert_\infty, B) x\not=y |f(x)-f(y)|\leq M |x-y|\left(1+\ln_+(\frac{1}{|x-y|})\right) \ln_+(x)=\max \{0,\ln(x)\} y=x f(x+y)\rightarrow f(x) x f(x+y)\rightarrow f(x) \ln_+ B f''(x) y^2 y f |f''(x)|\leq \lim M/|y|=\infty \lim_{y\rightarrow 0} |f(x-y)-f(x)| \lim_{y\rightarrow 0}||f(x+y)-f(x)|-|f(x-y)-f(x)||\leq \lim_{y\rightarrow 0 }B|y|=0 \lim \limsup \lim_{y\rightarrow 0}\sup_{y\in[-a,a]}(|f(x+y)-f(x)|-|f(x-y)-f(x)|)=0 \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x-y)-f(x)| \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|\leq\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)|\leq \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)| \lim_{a\rightarrow 0}\sup_{y\in [-a,a]}|f(x+y)-f(x)|=\lim_{a\rightarrow 0}\inf_{y\in [-a,a]}|f(x+y)-f(x)| \lim |f(x+y)-f(x)| y=y-x \frac{|f(y)+f(2x-y)-2f(x)|}{|y-x|}\leq B |f(y)-f(x)|\leq B|y-x|+|f(2x-y)-f(x)|","['real-analysis', 'calculus', 'inequality', 'functional-equations']"
51,"Prove that if two polynomial functions have equal values over a closed interval, they are equal. [duplicate]","Prove that if two polynomial functions have equal values over a closed interval, they are equal. [duplicate]",,"This question already has answers here : Is There A Polynomial That Has Infinitely Many Roots? (12 answers) I don't understand why if two polynomials degrees do not exceed $n$ and they coincide at $n+1$ points then they are equal. [duplicate] (2 answers) Can Two Different Polynomials Agree on an open interval? [duplicate] (2 answers) Closed 5 months ago . I want to prove that two polynomial functions that are equal over a specific interval, $(a, b) \in \mathbb R$ (closed interval with more than one point if that condition is necessary) are equal over $\mathbb R$ . I had an attempt that intuitively makes sense but I want to make the argument more formal. These were my attempts: I have tried expanding the general equation of a polynomial as a product of its roots but got stuck. I then simplified the question. I examined the case of two linear functions equivalent at a point. I figured if it is equivalent at another point (not necessarily over an interval), it would be trivial to prove that the two functions are the same. I moved to extend it to quadratic functions. I figured all we need is three points to pinpoint the expression of the polynomial. Given that the functions are equal over an interval, we have more than enough. I figured that, intuitively, this should continue for any two polynomials of arbitrary degree. Therefore, if two polynomials of $n$ -degree are equal over $n + 1$ points on the plane, then the two polynomials are the same. However, something feels very wishy-washy with this proof. How can I make this proof more formal and are there any alternate proofs?","This question already has answers here : Is There A Polynomial That Has Infinitely Many Roots? (12 answers) I don't understand why if two polynomials degrees do not exceed $n$ and they coincide at $n+1$ points then they are equal. [duplicate] (2 answers) Can Two Different Polynomials Agree on an open interval? [duplicate] (2 answers) Closed 5 months ago . I want to prove that two polynomial functions that are equal over a specific interval, (closed interval with more than one point if that condition is necessary) are equal over . I had an attempt that intuitively makes sense but I want to make the argument more formal. These were my attempts: I have tried expanding the general equation of a polynomial as a product of its roots but got stuck. I then simplified the question. I examined the case of two linear functions equivalent at a point. I figured if it is equivalent at another point (not necessarily over an interval), it would be trivial to prove that the two functions are the same. I moved to extend it to quadratic functions. I figured all we need is three points to pinpoint the expression of the polynomial. Given that the functions are equal over an interval, we have more than enough. I figured that, intuitively, this should continue for any two polynomials of arbitrary degree. Therefore, if two polynomials of -degree are equal over points on the plane, then the two polynomials are the same. However, something feels very wishy-washy with this proof. How can I make this proof more formal and are there any alternate proofs?","(a, b) \in \mathbb R \mathbb R n n + 1","['real-analysis', 'functions', 'polynomials', 'solution-verification', 'proof-writing']"
52,A fascinating sequence of polynomials,A fascinating sequence of polynomials,,"So, I have a sequence of polynomials: $$ p_m(z) = 2\sum_{k=1}^{m-1}(1-\tfrac km)z^k = \frac{2z}{1-z}\Big(1-\frac 1m\sum_{k=0}^{m-1}z^k\Big), $$ where the last expression only holds for $z\neq 1$ . Fascinatingly, when you plot the zeros of $p_m$ , you see that they lie outside of the unit circle line $\mathbb T$ and approach $\mathbb T$ for $m\to\infty$ so that for large $m$ the zeros of $p_m$ almost form the unit circle. However, $z=1$ is never a zero and the gap around $z=1$ is larger than around any other $z\in\mathbb T$ . This gap becomes smaller and smaller with growing $m$ . What I also observed (by plots for many values of $m$ ) is the following: $\operatorname{Re}p_m(e^{it})$ is very close to (but always larger than) $-1$ on an interval $[\delta,2\pi-\delta]$ and approaches $p_m(1) = m-1$ rapidly at the boundary of $[0,2\pi]$ . My question: For given $\delta>0$ , I'd like to find an $m_\delta$ such that for $m\ge m_\delta$ we have $\operatorname{Re}p_m(e^{it})\le 0$ for all $t\in [\delta,2\pi-\delta]$ . I can write down the trigonometric polynomial $\operatorname{Re}p_m(e^{it}) = 2\sum_{k=1}^{m-1}(1-\tfrac km)\cos(kt)$ , but I cannot seem to bound this guy. I'm not even able to show that $\operatorname{Re}p_m(e^{it})\ge -1$ for all $t$ . Can anyone help?","So, I have a sequence of polynomials: where the last expression only holds for . Fascinatingly, when you plot the zeros of , you see that they lie outside of the unit circle line and approach for so that for large the zeros of almost form the unit circle. However, is never a zero and the gap around is larger than around any other . This gap becomes smaller and smaller with growing . What I also observed (by plots for many values of ) is the following: is very close to (but always larger than) on an interval and approaches rapidly at the boundary of . My question: For given , I'd like to find an such that for we have for all . I can write down the trigonometric polynomial , but I cannot seem to bound this guy. I'm not even able to show that for all . Can anyone help?","
p_m(z) = 2\sum_{k=1}^{m-1}(1-\tfrac km)z^k = \frac{2z}{1-z}\Big(1-\frac 1m\sum_{k=0}^{m-1}z^k\Big),
 z\neq 1 p_m \mathbb T \mathbb T m\to\infty m p_m z=1 z=1 z\in\mathbb T m m \operatorname{Re}p_m(e^{it}) -1 [\delta,2\pi-\delta] p_m(1) = m-1 [0,2\pi] \delta>0 m_\delta m\ge m_\delta \operatorname{Re}p_m(e^{it})\le 0 t\in [\delta,2\pi-\delta] \operatorname{Re}p_m(e^{it}) = 2\sum_{k=1}^{m-1}(1-\tfrac km)\cos(kt) \operatorname{Re}p_m(e^{it})\ge -1 t","['real-analysis', 'complex-analysis', 'analysis', 'polynomials']"
53,Calculating $\;\int_{-\infty}^{\infty}\frac{1}{(e^x-1-x)^2+4\pi^2}~dx$,Calculating,\;\int_{-\infty}^{\infty}\frac{1}{(e^x-1-x)^2+4\pi^2}~dx,"I would like to calculate the following integral: $\displaystyle\int_{-\infty}^{+\infty}\frac{1}{\big(e^x-1-x\big)^2+4\pi^2} ~dx$ My attempts: I have tried to use the complex analysis, in particular the Cauchy’s residue theorem which is a powerful tool to evaluate line integrals of analytic functions over closed curves, but the denominator $\big(e^x-1-x\big)^2+4\pi^2=\big(e^x\!-\!1\!-\!x\!+\!2\pi i\big)\big(e^x\!-\!1\!-\!x\!-\!2\pi i\big)$ has infinitely many complex zeros, moreover I cannot get their exact values, but only an approximation of them. I have also tried to put a parameter $b$ in the integral: $I(b)=\displaystyle\int_{-\infty}^{+\infty}\frac{b}{\left(e^x-1-x\right)^2+b^2}~\mathrm dx$ and find a relation between $I(b)$ and its derivative $I’(b)$ , but in this way I get $I’(b)=\dfrac1bI(b)-\displaystyle\int_{-\infty}^{+\infty}\frac{2b^2}{\left[\left(e^x-1-x\right)^2+b^2\right]^2}~\mathrm dx$ and I do not know how I can solve it in order to obtain $I(b)$ . Another attempt I made is to write the integrand function as a series and then integrate it, but I did not even manage to do it. By using numerical methods I got that the result is approximately $\,0.33333\,,$ but I would like to obtain the result exactly which should be $\dfrac13\,.$ Could anyone give me a hint to calculate the integral ?","I would like to calculate the following integral: My attempts: I have tried to use the complex analysis, in particular the Cauchy’s residue theorem which is a powerful tool to evaluate line integrals of analytic functions over closed curves, but the denominator has infinitely many complex zeros, moreover I cannot get their exact values, but only an approximation of them. I have also tried to put a parameter in the integral: and find a relation between and its derivative , but in this way I get and I do not know how I can solve it in order to obtain . Another attempt I made is to write the integrand function as a series and then integrate it, but I did not even manage to do it. By using numerical methods I got that the result is approximately but I would like to obtain the result exactly which should be Could anyone give me a hint to calculate the integral ?","\displaystyle\int_{-\infty}^{+\infty}\frac{1}{\big(e^x-1-x\big)^2+4\pi^2} ~dx \big(e^x-1-x\big)^2+4\pi^2=\big(e^x\!-\!1\!-\!x\!+\!2\pi i\big)\big(e^x\!-\!1\!-\!x\!-\!2\pi i\big) b I(b)=\displaystyle\int_{-\infty}^{+\infty}\frac{b}{\left(e^x-1-x\right)^2+b^2}~\mathrm dx I(b) I’(b) I’(b)=\dfrac1bI(b)-\displaystyle\int_{-\infty}^{+\infty}\frac{2b^2}{\left[\left(e^x-1-x\right)^2+b^2\right]^2}~\mathrm dx I(b) \,0.33333\,, \dfrac13\,.","['real-analysis', 'definite-integrals', 'improper-integrals']"
54,How to deal with odd $m$ in integral $\int_{0}^{\frac{\pi}{4}}(\sin^{6}m x+\cos^{6}m x) \ln (1+\tan x) d x $,How to deal with odd  in integral,m \int_{0}^{\frac{\pi}{4}}(\sin^{6}m x+\cos^{6}m x) \ln (1+\tan x) d x ,"Latest edit Thanks to @Quanto for settling down the question by proving the odd one as: $$I_{2n+1}= \frac{5\pi}{64}\ln2+\frac3{16(2n+1)}\bigg(\frac\pi4-\sum_{j=0}^{2n}\frac{(-1)^j}{2j+1} \bigg)$$ By our results for both odd and even multiples $n$ of $x$ , we can conclude that $$ \lim _{n \rightarrow \infty} \displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(nx)+\cos ^{6}(nx)\right] \ln (1+\tan x) d x =\frac{5 \pi\ln 2}{64} $$ As asked by @Claude Leibovici for the powers other than 6, I had generalised my result to even powers below as an answer: $$ I(m,n):=\int_{0}^{\frac{\pi}{4}}\left[\cos ^{2 m}(2 nx)+\sin ^{2 m}(2 n x)\right] \ln (1+\tan x) d x= \frac{\pi \ln 2}{4} \cdot \frac{(2 m-1) ! !}{(2 m) ! !} $$ In order to evaluate the even case $$\int_{0}^{\frac{\pi}{4}}\left[\sin^{6}(2 n x)+\cos^{6}(2 nx)\right] \ln (1+\tan x) d x $$ we first simplify $\displaystyle \begin{aligned}\sin ^{6}(2 n x)+\cos ^{6}(2 n x) =& {\left[\sin ^{2}(2 n x)+\cos ^{2}(2 n x)\right]\left[\sin ^{4}(2 n x)-\sin ^{2}(2 n x) \cos ^{2}(2 n x)\right) } \\&\left.+\cos ^{4}(2 n x)\right] \\=& 1-3 \sin ^{2}(2 n x) \cos ^{2}(2 n x) \\=& 1-\frac{3}{4} \sin ^{2}(4 n x) \\=& 1-\frac{3}{8}(1-\cos 8 n x) \\=& \frac{1}{8}(5+3 \cos (8nx))\end{aligned} \tag*{} $ To get rid of the natural logarithm, a simple substitution transforms the integral into $\begin{aligned}I &=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln (1+\tan x) d x \\& \stackrel{x\mapsto\frac{\pi}{4}-x}{=} \frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(1+\tan \left(\frac{\pi}{4}-x\right)\right) d x \\&=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(\frac{2}{1+\tan x}\right) d x \\&=\frac{1}{8} \ln 2 \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x) )d x-I \\I &=\frac{\ln 2}{16} \int_{0}^{\frac{\pi}{4}}(5+3 \cos 8  n x) d x\\&=\frac{\ln 2}{16}\left[5 x+\frac{3}{8 n} \sin (8 n x)\right]_0^{\frac{\pi}{4} }\\ &=\frac{5 \pi}{64} \ln 2\end{aligned} \tag*{} $ My Question: How can we deal with the odd one $$\displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(2 n +1)x+\cos ^{6}(2 n +1)x\right] \ln (1+\tan x) d x  ?$$ Can you help?","Latest edit Thanks to @Quanto for settling down the question by proving the odd one as: By our results for both odd and even multiples of , we can conclude that As asked by @Claude Leibovici for the powers other than 6, I had generalised my result to even powers below as an answer: In order to evaluate the even case we first simplify To get rid of the natural logarithm, a simple substitution transforms the integral into My Question: How can we deal with the odd one Can you help?","I_{2n+1}= \frac{5\pi}{64}\ln2+\frac3{16(2n+1)}\bigg(\frac\pi4-\sum_{j=0}^{2n}\frac{(-1)^j}{2j+1} \bigg) n x 
\lim _{n \rightarrow \infty} \displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(nx)+\cos ^{6}(nx)\right] \ln (1+\tan x) d x =\frac{5 \pi\ln 2}{64}
 
I(m,n):=\int_{0}^{\frac{\pi}{4}}\left[\cos ^{2 m}(2 nx)+\sin ^{2 m}(2 n x)\right] \ln (1+\tan x) d x= \frac{\pi \ln 2}{4} \cdot \frac{(2 m-1) ! !}{(2 m) ! !}
 \int_{0}^{\frac{\pi}{4}}\left[\sin^{6}(2 n x)+\cos^{6}(2 nx)\right] \ln (1+\tan x) d x  \displaystyle \begin{aligned}\sin ^{6}(2 n x)+\cos ^{6}(2 n x) =& {\left[\sin ^{2}(2 n x)+\cos ^{2}(2 n x)\right]\left[\sin ^{4}(2 n x)-\sin ^{2}(2 n x) \cos ^{2}(2 n x)\right) } \\&\left.+\cos ^{4}(2 n x)\right] \\=& 1-3 \sin ^{2}(2 n x) \cos ^{2}(2 n x) \\=& 1-\frac{3}{4} \sin ^{2}(4 n x) \\=& 1-\frac{3}{8}(1-\cos 8 n x) \\=& \frac{1}{8}(5+3 \cos (8nx))\end{aligned} \tag*{}  \begin{aligned}I &=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln (1+\tan x) d x \\& \stackrel{x\mapsto\frac{\pi}{4}-x}{=} \frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(1+\tan \left(\frac{\pi}{4}-x\right)\right) d x \\&=\frac{1}{8} \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x)) \ln \left(\frac{2}{1+\tan x}\right) d x \\&=\frac{1}{8} \ln 2 \int_{0}^{\frac{\pi}{4}}(5+3 \cos (8 n x) )d x-I \\I &=\frac{\ln 2}{16} \int_{0}^{\frac{\pi}{4}}(5+3 \cos 8  n x) d x\\&=\frac{\ln 2}{16}\left[5 x+\frac{3}{8 n} \sin (8 n x)\right]_0^{\frac{\pi}{4} }\\ &=\frac{5 \pi}{64} \ln 2\end{aligned} \tag*{}  \displaystyle \int_{0}^{\frac{\pi}{4}}\left[\sin ^{6}(2 n +1)x+\cos ^{6}(2 n +1)x\right] \ln (1+\tan x) d x  ?","['real-analysis', 'calculus', 'integration', 'trigonometry']"
55,Integrating $\int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx$,Integrating,\int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx,"I found the following integral and wanted to know if there is a nice closed form solution in terms of elementary or some special functions (Polylogarithm, Clausen, etc). $$\displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx$$ I know that the integral converges numerically to $\approx 0.403926$ Here is my try using integration by parts: Let $$ du = \frac{\arctan(x)}{x^2} \Longrightarrow u = -\frac{1}{2}\ln(1+x^2) + \ln(x) - \frac{\arctan(x)}{x} $$ $$ v = \arctan(x^2) \Longrightarrow dv = \frac{2x}{x^4+1}$$ Hence $$\displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx \stackrel{IBP}{=} -\frac{\pi^2}{16} - \frac{1}{8}\pi \ln(2)  -2\underbrace{\int_{0}^{1} \frac{x\ln(x)}{x^4+1} dx}_{I_{1}} + 2\underbrace{\int_{0}^{1}\frac{\arctan(x)}{x^4+1}dx}_{I_{2}} + \underbrace{\int_{0}^{1} \frac{x\ln(1+x^2)}{x^4+1}dx}_{I_{3}} $$ Can be proven that $$I_{1} = -\frac{C}{4}$$ where $C$ is the Catalan constant and $$I_{3} = \frac{1}{16} \pi \ln(2) $$ but I'm stuck with $I_{2}$ Another way could be: Define $$\Psi(a) =  \int_{0}^{1} \frac{\arctan(ax)\arctan(x^2)}{x^2} dx$$ Hence $$\Psi'(a) = \int_{0}^{1} \frac{\arctan(x^2)}{x(a^2x^2+1)}dx = \frac{1}{2}\int_{0}^{1} \frac{\arctan(w)}{w(a^2w+1)}dx$$ Using integration by parts, we have: $$du = \frac{\arctan(w)}{1+a^2w} \Longrightarrow u= \frac{1}{1+a^4}\ln\left( \frac{1+a^2w}{\sqrt{1+w^2}} \right) - \frac{a^2-w}{(1+a^4)(1+a^2w)} \arctan(w)  $$ $$ v = \frac{1}{w} \Longrightarrow dv = \ln(w) $$ However, this path seems even more rugged that the other. One last hint could be the following integral: $$\int_{0}^{1} \frac{\arctan(x) \arctan(x^3)}{x} dx = \frac{7}{72}\zeta(3) + \frac{\pi}{3}C - \frac{5\pi}{12}\operatorname{Cl}_{2}\left(\frac{2\pi}{3} \right)$$ where $\operatorname{Cl}_{2}$ is the Clausen function of order 2. However, I do not know the proof of this result either.","I found the following integral and wanted to know if there is a nice closed form solution in terms of elementary or some special functions (Polylogarithm, Clausen, etc). I know that the integral converges numerically to Here is my try using integration by parts: Let Hence Can be proven that where is the Catalan constant and but I'm stuck with Another way could be: Define Hence Using integration by parts, we have: However, this path seems even more rugged that the other. One last hint could be the following integral: where is the Clausen function of order 2. However, I do not know the proof of this result either.",\displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx \approx 0.403926  du = \frac{\arctan(x)}{x^2} \Longrightarrow u = -\frac{1}{2}\ln(1+x^2) + \ln(x) - \frac{\arctan(x)}{x}   v = \arctan(x^2) \Longrightarrow dv = \frac{2x}{x^4+1} \displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx \stackrel{IBP}{=} -\frac{\pi^2}{16} - \frac{1}{8}\pi \ln(2)  -2\underbrace{\int_{0}^{1} \frac{x\ln(x)}{x^4+1} dx}_{I_{1}} + 2\underbrace{\int_{0}^{1}\frac{\arctan(x)}{x^4+1}dx}_{I_{2}} + \underbrace{\int_{0}^{1} \frac{x\ln(1+x^2)}{x^4+1}dx}_{I_{3}}  I_{1} = -\frac{C}{4} C I_{3} = \frac{1}{16} \pi \ln(2)  I_{2} \Psi(a) =  \int_{0}^{1} \frac{\arctan(ax)\arctan(x^2)}{x^2} dx \Psi'(a) = \int_{0}^{1} \frac{\arctan(x^2)}{x(a^2x^2+1)}dx = \frac{1}{2}\int_{0}^{1} \frac{\arctan(w)}{w(a^2w+1)}dx du = \frac{\arctan(w)}{1+a^2w} \Longrightarrow u= \frac{1}{1+a^4}\ln\left( \frac{1+a^2w}{\sqrt{1+w^2}} \right) - \frac{a^2-w}{(1+a^4)(1+a^2w)} \arctan(w)    v = \frac{1}{w} \Longrightarrow dv = \ln(w)  \int_{0}^{1} \frac{\arctan(x) \arctan(x^3)}{x} dx = \frac{7}{72}\zeta(3) + \frac{\pi}{3}C - \frac{5\pi}{12}\operatorname{Cl}_{2}\left(\frac{2\pi}{3} \right) \operatorname{Cl}_{2},"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'closed-form']"
56,Find a value of $\;\lim\limits_{n\rightarrow\infty}n\left ( e- e^{\frac{1}{e}}\uparrow\uparrow n \right )$,Find a value of,\;\lim\limits_{n\rightarrow\infty}n\left ( e- e^{\frac{1}{e}}\uparrow\uparrow n \right ),"Find a value of $$\lim_{n\rightarrow\infty}n\left ( e- e^{\frac{1}{e}}\uparrow\uparrow n \right )$$ For your information $,\quad\uparrow\uparrow$ is a tetration defined as $$a\uparrow\uparrow n:=\underbrace{a^{a^{\cdot^{\cdot^{\cdot^{a}}}}}}_{n}$$ I think we must use the function $\ln x$ here then use the Laurent series of $\ln x.$ But I couldn't find an extension formula of $\ln e^{\frac{1}{e}}\uparrow\uparrow n,$ I need to the help, thanks a real lot !","Find a value of For your information is a tetration defined as I think we must use the function here then use the Laurent series of But I couldn't find an extension formula of I need to the help, thanks a real lot !","\lim_{n\rightarrow\infty}n\left ( e- e^{\frac{1}{e}}\uparrow\uparrow n \right ) ,\quad\uparrow\uparrow a\uparrow\uparrow n:=\underbrace{a^{a^{\cdot^{\cdot^{\cdot^{a}}}}}}_{n} \ln x \ln x. \ln e^{\frac{1}{e}}\uparrow\uparrow n,","['real-analysis', 'limits']"
57,"$(X,d)$ is metric space. $(X,d)$ is compact if and only if any continuous function on $X$ has a maximum.",is metric space.  is compact if and only if any continuous function on  has a maximum.,"(X,d) (X,d) X","$(X,d)$ is metric space. $(X,d)$ is compact if and only if any continuous function on $X$ has a maximum. I dont know whether these functions real valued or not but only real valued functions may make sense, I think. In that case $\Rightarrow$ is easy. About $\Leftarrow $ My 1st attempt: I thought some special functions that I can use e.g. $$\varphi:X\to \mathbb R\\ \varphi(x)=d(x_0,x)$$ for some fix $x_0\in X$ Since this is a continuous function then it attains its maximum on $X$ , that implies $X$ is bounded . With this motivation I can define $$\psi:X\to \mathbb R \\\psi(x)=d(x,X)=\inf\limits_{a\in X}d(x,a)$$ since it attains its maximum so $X=\overline X$ So I have boundness and closedness but these dont really imply compactness in metric spaces. My 2nd attempt was: Considering any sequences in $X$ and using $f(x_{k_n})$ is convergent iff $x_{k_n}$ is convergent. By showing every sequence has convergent subsequence and using continuity of any $f:X\to\mathbb R$ . I am stuck is there hint or answer you can give me?","is metric space. is compact if and only if any continuous function on has a maximum. I dont know whether these functions real valued or not but only real valued functions may make sense, I think. In that case is easy. About My 1st attempt: I thought some special functions that I can use e.g. for some fix Since this is a continuous function then it attains its maximum on , that implies is bounded . With this motivation I can define since it attains its maximum so So I have boundness and closedness but these dont really imply compactness in metric spaces. My 2nd attempt was: Considering any sequences in and using is convergent iff is convergent. By showing every sequence has convergent subsequence and using continuity of any . I am stuck is there hint or answer you can give me?","(X,d) (X,d) X \Rightarrow \Leftarrow  \varphi:X\to \mathbb R\\ \varphi(x)=d(x_0,x) x_0\in X X X \psi:X\to \mathbb R \\\psi(x)=d(x,X)=\inf\limits_{a\in X}d(x,a) X=\overline X X f(x_{k_n}) x_{k_n} f:X\to\mathbb R",['real-analysis']
58,Define $I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx$ for every $n\in\mathbb{N}$. Prove that $\lim_{n\to\infty}nI_n=\frac{1}{\sqrt 2}$.,Define  for every . Prove that .,I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx n\in\mathbb{N} \lim_{n\to\infty}nI_n=\frac{1}{\sqrt 2},"Question: Define $I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx$ for every $n\in\mathbb{N}$ . Prove that $$\lim_{n\to\infty}nI_n=\frac{1}{\sqrt 2}$$ . My approach: Given that $I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx, \forall n\in\mathbb{N}.$ Let us make the substitution $x^n=t$ , then $$nI_n=\int_0^1\frac{dt}{\sqrt{1+t^{-2/n}}}.$$ Now since $0\le t\le 1\implies \frac{1}{t}\ge 1\implies \left(\frac{1}{t}\right)^{2/n}\ge 1 \implies 1+\left(\frac{1}{t}\right)^{2/n}\ge 2\implies \sqrt{1+\left(\frac{1}{t}\right)^{2/n}}\ge \sqrt 2.$ This implies that $$\frac{1}{\sqrt{1+\left(\frac{1}{t}\right)^{2/n}}}\le\frac{1}{\sqrt 2}\\ \implies \int_0^1 \frac{dt}{\sqrt{1+\left(\frac{1}{t}\right)^{2/n}}}\le \int_0^1\frac{dt}{\sqrt 2}=\frac{1}{\sqrt 2}.$$ So, as you can see, I am trying to solve the question using Sandwich theorem. Can someone help me to proceed after this? Also, in $$\lim_{n\to\infty}nI_n=\lim_{n\to\infty}\int_0^1\frac{dt}{\sqrt{1+t^{-2/n}}},$$ the limit and integral interchangeable?","Question: Define for every . Prove that . My approach: Given that Let us make the substitution , then Now since This implies that So, as you can see, I am trying to solve the question using Sandwich theorem. Can someone help me to proceed after this? Also, in the limit and integral interchangeable?","I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx n\in\mathbb{N} \lim_{n\to\infty}nI_n=\frac{1}{\sqrt 2} I_n=\int_0^1\frac{x^n}{\sqrt{x^2+1}}dx, \forall n\in\mathbb{N}. x^n=t nI_n=\int_0^1\frac{dt}{\sqrt{1+t^{-2/n}}}. 0\le t\le 1\implies \frac{1}{t}\ge 1\implies \left(\frac{1}{t}\right)^{2/n}\ge 1 \implies 1+\left(\frac{1}{t}\right)^{2/n}\ge 2\implies \sqrt{1+\left(\frac{1}{t}\right)^{2/n}}\ge \sqrt 2. \frac{1}{\sqrt{1+\left(\frac{1}{t}\right)^{2/n}}}\le\frac{1}{\sqrt 2}\\ \implies \int_0^1 \frac{dt}{\sqrt{1+\left(\frac{1}{t}\right)^{2/n}}}\le \int_0^1\frac{dt}{\sqrt 2}=\frac{1}{\sqrt 2}. \lim_{n\to\infty}nI_n=\lim_{n\to\infty}\int_0^1\frac{dt}{\sqrt{1+t^{-2/n}}},","['real-analysis', 'integration', 'definite-integrals']"
59,Convergence of Approximations of the Identity in $L^p(\mathbb R^d)$,Convergence of Approximations of the Identity in,L^p(\mathbb R^d),"(context: in a comment to an answer of mine mentioned below, a user has asked for the proof of one of the steps) In this answer , one of the steps mentions that If $f\in L^p(\mathbb R^d)$ , $g\in L^1(\mathbb R^d)$ with $\int_{\mathbb R^d} g=1$ and $g_n(x)=n^d\,g(nx)$ , then $$\lim_{n\to\infty}\|f-f*g_n\|_p=0.$$ What would be the proof of this?","(context: in a comment to an answer of mine mentioned below, a user has asked for the proof of one of the steps) In this answer , one of the steps mentions that If , with and , then What would be the proof of this?","f\in L^p(\mathbb R^d) g\in L^1(\mathbb R^d) \int_{\mathbb R^d} g=1 g_n(x)=n^d\,g(nx) \lim_{n\to\infty}\|f-f*g_n\|_p=0.","['real-analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
60,Existence of fixed point for a continuous function on an infinite closed set,Existence of fixed point for a continuous function on an infinite closed set,,"Why is it that a continuous surjection from $X\to X$ has a fixed point when $X=[1,2]\cup[3,\infty)$ and $X=[3,\infty)$ but not when $X=[1,2]\cup[3,7]$ ? When $X=[3,\infty)$ , since the set is closed and connected, therefore the image should also be connected, whence it seems intuitive to expect a fixed point. But, how does $[1,2]\cup[3,\infty)$ have a fixed point but $[1,2]\cup[3,7]$ does not? Any rigorous reasoning? Thanks beforehand.","Why is it that a continuous surjection from has a fixed point when and but not when ? When , since the set is closed and connected, therefore the image should also be connected, whence it seems intuitive to expect a fixed point. But, how does have a fixed point but does not? Any rigorous reasoning? Thanks beforehand.","X\to X X=[1,2]\cup[3,\infty) X=[3,\infty) X=[1,2]\cup[3,7] X=[3,\infty) [1,2]\cup[3,\infty) [1,2]\cup[3,7]","['real-analysis', 'calculus', 'general-topology', 'fixed-point-theorems', 'fixed-points']"
61,"Possible to get a closed form expression, or an upper bound, for $ f(n)=\sum_{m=1}^\infty \bigg(\frac{m+n}{3}\bigg)^{m+n}\bigg(\frac{1}{m}\bigg)^m$?","Possible to get a closed form expression, or an upper bound, for ?", f(n)=\sum_{m=1}^\infty \bigg(\frac{m+n}{3}\bigg)^{m+n}\bigg(\frac{1}{m}\bigg)^m,"Is it possible to get a closed form expression, or an upper bound, for the following function $f$ which is given by an infinite summation: $$ f(n) = \sum_{m=1}^\infty \bigg(\frac{m+n}{3}\bigg)^{m+n} \bigg(\frac{1}{m}\bigg)^m, $$ for $n > 0$ ? Also, $n$ can be assumed to be large if this is any help. Note when $n=0$ , it is simply $$ f(0) = \frac{1}{2}. $$","Is it possible to get a closed form expression, or an upper bound, for the following function which is given by an infinite summation: for ? Also, can be assumed to be large if this is any help. Note when , it is simply","f 
f(n) = \sum_{m=1}^\infty \bigg(\frac{m+n}{3}\bigg)^{m+n} \bigg(\frac{1}{m}\bigg)^m,
 n > 0 n n=0 
f(0) = \frac{1}{2}.
","['real-analysis', 'sequences-and-series', 'summation', 'closed-form']"
62,Prove that $\lim_{n \to \infty} \int_0^1{nx^nf(x)}dx$ is equal to $f(1)$.,Prove that  is equal to .,\lim_{n \to \infty} \int_0^1{nx^nf(x)}dx f(1),"$\mathbf{Question}:$ Let $f$ be a continuous function on $[0,1]$ . Then prove that the limit $\lim_{n \to \infty} \int_0^1{nx^nf(x)}dx$ is equal to $f(1)$ . $\mathbf{Attempt}$ : First, we try to show that the sequence of functions $\{nx^nf(x)\}_{x\in [0,1]}$ is uniformly convergent to $0$ on the restricted domain $[0,1-\epsilon]$ , $0<\epsilon<1$ . Let $\sup_{x\in[0,1]}f(x)= \mathcal{M}$ . Then $|{nx^nf(x)}|\leq n(1-\epsilon)^n|\mathcal{M}|$ for any $x$ and $n(1-\epsilon)^n \to 0$ as $n\to \infty$ . Thereby, $\lim_{n \to \infty}\int_0^{1-\epsilon}nx^nf(x)dx=\int_0^{1-\epsilon}{\lim_{n \to \infty} }nx^nf(x)dx=0$ Now, denote $\sup_{x\in [1-\epsilon,1]} f(x)=M(\epsilon)$ and $\inf_{x\in[1-\epsilon,1]}f(x)=m(\epsilon)$ . $\int_{1-\epsilon}^1nx^n\ m(\epsilon)dx\leq\int_{1-\epsilon}^{1}nx^nf(x)dx\leq \int_{1-\epsilon}^1 nx^n\ M(\epsilon)dx$ . Now, $\lim_{n \to \infty}\int_{1-\epsilon}^1 nx^n\ M(\epsilon)dx =\displaystyle \lim_{n\to\infty}\bigg[ \frac{nx^{n+1}}{n+1}M(\epsilon)\bigg]_{1-\epsilon}^1=M(\epsilon)$ [Similarly the other one is $m(\epsilon)$ ] Clearly, as $\epsilon \to 0 , \ \ M(\epsilon), m(\epsilon) \to f(1)$ , so $\lim_{n \to \infty} \int_0^1{nx^nf(x)}dx=f(1)$ . Is the procedure correct? Kindly verify.","Let be a continuous function on . Then prove that the limit is equal to . : First, we try to show that the sequence of functions is uniformly convergent to on the restricted domain , . Let . Then for any and as . Thereby, Now, denote and . . Now, [Similarly the other one is ] Clearly, as , so . Is the procedure correct? Kindly verify.","\mathbf{Question}: f [0,1] \lim_{n \to \infty} \int_0^1{nx^nf(x)}dx f(1) \mathbf{Attempt} \{nx^nf(x)\}_{x\in [0,1]} 0 [0,1-\epsilon] 0<\epsilon<1 \sup_{x\in[0,1]}f(x)= \mathcal{M} |{nx^nf(x)}|\leq n(1-\epsilon)^n|\mathcal{M}| x n(1-\epsilon)^n \to 0 n\to \infty \lim_{n \to \infty}\int_0^{1-\epsilon}nx^nf(x)dx=\int_0^{1-\epsilon}{\lim_{n \to \infty} }nx^nf(x)dx=0 \sup_{x\in [1-\epsilon,1]} f(x)=M(\epsilon) \inf_{x\in[1-\epsilon,1]}f(x)=m(\epsilon) \int_{1-\epsilon}^1nx^n\ m(\epsilon)dx\leq\int_{1-\epsilon}^{1}nx^nf(x)dx\leq \int_{1-\epsilon}^1 nx^n\ M(\epsilon)dx \lim_{n \to \infty}\int_{1-\epsilon}^1 nx^n\ M(\epsilon)dx =\displaystyle \lim_{n\to\infty}\bigg[ \frac{nx^{n+1}}{n+1}M(\epsilon)\bigg]_{1-\epsilon}^1=M(\epsilon) m(\epsilon) \epsilon \to 0 , \ \ M(\epsilon), m(\epsilon) \to f(1) \lim_{n \to \infty} \int_0^1{nx^nf(x)}dx=f(1)","['real-analysis', 'limits', 'proof-verification', 'uniform-convergence', 'sequence-of-function']"
63,"If $f$ takes every value at most $k$ times, then f is differentiable almost everywhere.","If  takes every value at most  times, then f is differentiable almost everywhere.",f k,"I am stuck at the following problem, I got in an old question paper (real analysis). Let $k>0$ be a natural number and Let $f$ be a continuous function on real line such that $f$ takes any value at most $k$ times. Show that $f$ is differentiable almost everywhere. My hunch is that we can divide the real line (except a few exceptional points) into intervals such that on each of those intervals $f$ is one-to-one and therefore monotonic and hence differentiable almost everywhere.  But, I am not able to make this idea concrete. Any help would be appreciated.","I am stuck at the following problem, I got in an old question paper (real analysis). Let be a natural number and Let be a continuous function on real line such that takes any value at most times. Show that is differentiable almost everywhere. My hunch is that we can divide the real line (except a few exceptional points) into intervals such that on each of those intervals is one-to-one and therefore monotonic and hence differentiable almost everywhere.  But, I am not able to make this idea concrete. Any help would be appreciated.",k>0 f f k f f,"['real-analysis', 'measure-theory', 'derivatives', 'almost-everywhere']"
64,Show that $\frac{f(x)}{x}$ is a decreasing function implies that $f(x)$ is subadditive,Show that  is a decreasing function implies that  is subadditive,\frac{f(x)}{x} f(x),"I am studying Carother's Real Analysis for my qualifying exams. In the book I am to prove that if $f : [0, \infty) \rightarrow [0, \infty)$ is an increasing function, $f(0) = 0$ , and $f(x) > 0$ for all $x > 0$ , then $\frac{f(x)}{x}$ being decreasing for $x > 0$ implies that $f$ is subadditive, or that $f(x + y) \leq f(x) + f(y)$ . So far I have tried: $\frac{f(x+y)}{x+y} \leq \frac{f(y)}{y}$ and $\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x}$ implies that $2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y}$ , so $\frac{f(x+y)}{x+y} \leq 2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y} \leq f(x) + f(y)$ I think that I am close but I can't get rid of the $x + y$ in the denominator. Any help would be appreciated.","I am studying Carother's Real Analysis for my qualifying exams. In the book I am to prove that if is an increasing function, , and for all , then being decreasing for implies that is subadditive, or that . So far I have tried: and implies that , so I think that I am close but I can't get rid of the in the denominator. Any help would be appreciated.","f : [0, \infty) \rightarrow [0, \infty) f(0) = 0 f(x) > 0 x > 0 \frac{f(x)}{x} x > 0 f f(x + y) \leq f(x) + f(y) \frac{f(x+y)}{x+y} \leq \frac{f(y)}{y} \frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} 2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y} \frac{f(x+y)}{x+y} \leq 2\frac{f(x+y)}{x+y} \leq \frac{f(x)}{x} +\frac{f(y)}{y} \leq f(x) + f(y) x + y",['real-analysis']
65,"Is there a continuous function $f$ satisfying $f(x)^2=f\left(x^2\right)$, $f(0)=1$ and $ f(1)=0$?","Is there a continuous function  satisfying ,  and ?",f f(x)^2=f\left(x^2\right) f(0)=1  f(1)=0,"I was wondering if there is a continuous function $f:[0,1]\to\mathbb R$ satisfying $$f(x)^2=f\left(x^2\right)\text,$$ for all $x\in[0,1]$ , $f(0)=1$ and $ f(1)=0$ . Clearly, some easy functions like polynomials are not satisfied. I guess there is a way to construct an example since it only needs a continuous function.","I was wondering if there is a continuous function satisfying for all , and . Clearly, some easy functions like polynomials are not satisfied. I guess there is a way to construct an example since it only needs a continuous function.","f:[0,1]\to\mathbb R f(x)^2=f\left(x^2\right)\text, x\in[0,1] f(0)=1  f(1)=0","['real-analysis', 'functional-equations']"
66,A more rigorous way to show that $x^5 - 3x = 1$ has at least $3$ roots in $\Bbb R$,A more rigorous way to show that  has at least  roots in,x^5 - 3x = 1 3 \Bbb R,"Given an equation: $$ x^5 - 3x = 1 $$ Show that: It has at least $1$ root on $(1, 2)$ ; It has at least $3$ roots on $\Bbb R$ I've started with considering a function $f(x)$ for $x\in [1, 2]$ : $$ f(x) = x^5 - 3x - 1 $$ Then calculating its value on the left and right sides of the closed interval yields: $$ f(1) = -3\\ f(2) = 25 $$ Applying the Intermediate Value Theorem yields that there exists a point for $x_0 \in [1, 2]$ such that $f(x_0) = 0$ . Which means that indeed at least one root exists. However, for the second part of the question if we consider $f(x)$ for $x \in \Bbb R$ , the only way I see is to try and guess the intervals where the function changes its sign and then apply IVT again. Consider for example $f(x)$ for $x \in \{-2, -1, 1, 2\}$ . I see how derivatives could be to the rescue here, the problem is that I'm not allowed to use derivatives . Is there a rigorous way to prove what's stated without guessing and without using derivatives? Thank you!","Given an equation: Show that: It has at least root on ; It has at least roots on I've started with considering a function for : Then calculating its value on the left and right sides of the closed interval yields: Applying the Intermediate Value Theorem yields that there exists a point for such that . Which means that indeed at least one root exists. However, for the second part of the question if we consider for , the only way I see is to try and guess the intervals where the function changes its sign and then apply IVT again. Consider for example for . I see how derivatives could be to the rescue here, the problem is that I'm not allowed to use derivatives . Is there a rigorous way to prove what's stated without guessing and without using derivatives? Thank you!","
x^5 - 3x = 1
 1 (1, 2) 3 \Bbb R f(x) x\in [1, 2] 
f(x) = x^5 - 3x - 1
 
f(1) = -3\\
f(2) = 25
 x_0 \in [1, 2] f(x_0) = 0 f(x) x \in \Bbb R f(x) x \in \{-2, -1, 1, 2\}","['real-analysis', 'calculus', 'polynomials', 'roots']"
67,"Why should the equality of mixed partials be ""intuitively obvious""?","Why should the equality of mixed partials be ""intuitively obvious""?",,"I am reading Ted Shifrin's excellent book Multivariable Mathematics . It claims that the equality of mixed partials is ""an intuitively obvious result, but the proof is quite subtle"". However, I guess I must be thinking in the wrong way, because I do not see the intuition behind this result. This is how I think about it: Let $f:\mathbb{R}^2 \to \mathbb{R}$ . I think of $f_x$ as a ""field of slopes"" in the $x$ -direction. If we analyze the movement in the $y$ direction in this field of slopes, we get $f_{xy}$ . Now $f_y$ is a ""field of slopes"" in the $y$ -direction. If we analyze movement in the $x$ direction here, we get $f_{yx}$ . It's unclear to me why movement in the $x$ -direction in the ""field of $y$ -slopes"" should be the same as movement in the $y$ -direction in the ""field of $x$ -slopes"".","I am reading Ted Shifrin's excellent book Multivariable Mathematics . It claims that the equality of mixed partials is ""an intuitively obvious result, but the proof is quite subtle"". However, I guess I must be thinking in the wrong way, because I do not see the intuition behind this result. This is how I think about it: Let . I think of as a ""field of slopes"" in the -direction. If we analyze the movement in the direction in this field of slopes, we get . Now is a ""field of slopes"" in the -direction. If we analyze movement in the direction here, we get . It's unclear to me why movement in the -direction in the ""field of -slopes"" should be the same as movement in the -direction in the ""field of -slopes"".",f:\mathbb{R}^2 \to \mathbb{R} f_x x y f_{xy} f_y y x f_{yx} x y y x,"['real-analysis', 'analysis', 'multivariable-calculus']"
68,Can the concepts of abstract algebra be visualized as in analysis? [duplicate],Can the concepts of abstract algebra be visualized as in analysis? [duplicate],,This question already has an answer here : Visual approach to abstract algebra (1 answer) Closed last year . I like to visualize everything I study but yet I have found pretty nothing to visualize in abstract algebra.I have studied group theory upto subgroups Cyclic groups and Cosets and Lagrange's theorem.Is there any way of visualizing these things?Please suggest some good reference book/text also which discusses these things and also the motivation/idea behind different theorems and concepts.,This question already has an answer here : Visual approach to abstract algebra (1 answer) Closed last year . I like to visualize everything I study but yet I have found pretty nothing to visualize in abstract algebra.I have studied group theory upto subgroups Cyclic groups and Cosets and Lagrange's theorem.Is there any way of visualizing these things?Please suggest some good reference book/text also which discusses these things and also the motivation/idea behind different theorems and concepts.,,"['real-analysis', 'calculus']"
69,Integral inequality with a strange condition,Integral inequality with a strange condition,,"Let $f$ be a continuously differentiable real valued function on $[0,1]$ . It is given that $\displaystyle \int_{\frac{1}{3}}^{\frac{2}{3}}f(x) dx=0$ Find the minimum value of $\dfrac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_{0}^{1} f(x) dx \right)^2}$ I tried to use Cauchy-Schwarz to show that $$\frac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_0^1 f(x) dx \right)^2} \ge \frac{\left( \int_0^1 \bigl| f(x)f'(x) \bigr| dx \right)^2}{ \left( \int_0^1 f(x) dx \right)^2} \ge \frac{ f(1)^2 - f(0)^2}{2 \left( \int_0^1 f^2 (x) dx \right)^2}$$ But I can't proceed from here. Also, I don't know how to use the condition $\int_{1/3}^{2/3}f(x) dx=0$","Let be a continuously differentiable real valued function on . It is given that Find the minimum value of I tried to use Cauchy-Schwarz to show that But I can't proceed from here. Also, I don't know how to use the condition","f [0,1] \displaystyle \int_{\frac{1}{3}}^{\frac{2}{3}}f(x) dx=0 \dfrac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_{0}^{1} f(x) dx \right)^2} \frac{\int_{0}^{1} (f'(x))^2 dx}{\left( \int_0^1 f(x) dx \right)^2} \ge \frac{\left( \int_0^1 \bigl| f(x)f'(x) \bigr| dx \right)^2}{ \left( \int_0^1 f(x) dx \right)^2} \ge \frac{ f(1)^2 - f(0)^2}{2 \left( \int_0^1 f^2 (x) dx \right)^2} \int_{1/3}^{2/3}f(x) dx=0","['real-analysis', 'integration']"
70,Do we have integral test for double series?,Do we have integral test for double series?,,"To determine convergence of $$\sum_{n=1}^\infty a_n,$$ one can use the integral test if $f(n)=a_n$ satisfies certain properties. Now, if I would like to determine convergence of double series $$\sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij},$$ do we have some 'integral test' for it? I think one can evaluate $$\int\int f(i,j)\,di\,dj.$$","To determine convergence of one can use the integral test if satisfies certain properties. Now, if I would like to determine convergence of double series do we have some 'integral test' for it? I think one can evaluate","\sum_{n=1}^\infty a_n, f(n)=a_n \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij}, \int\int f(i,j)\,di\,dj.","['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
71,Proof verification: $\lim_{n\to\infty}(\sqrt{n^2+1}-n)=0$,Proof verification:,\lim_{n\to\infty}(\sqrt{n^2+1}-n)=0,"I'm having issues forming the discussion part of the proof because I am not sure if I am coming up with the right estimation. Is this an appropriate way of coming up with an estimation? I wrote: We want to show that $\forall \epsilon >0$ , $\exists N>0$ , $N\in \mathbb{N}$ s.t. $n>N \Longrightarrow |(\sqrt{n^2+1}-n)-0|<\epsilon$ . Then we will proceed by simplifying $$ \begin{split} \sqrt{n^2+1}-n  &= \left(\sqrt{n^2+1}-n\right) \times     \frac{\sqrt{n^2+1}+n}{\sqrt{n^2+1}+n}\\  &=\frac{n^2+1-n^2}{\sqrt{n^2+1}+n}\\  &=\frac{1}{\sqrt{n^2+1}+n} \end{split} $$ by using the conjugate.  Now we will proceed by making an estimation, we see that $$ \frac{1}{\sqrt{n^2+1}+n} \leq \frac{1}{n+1}, \quad \text{where } n > 1. $$ So let $\frac{1}{n+1} < \epsilon$ Then by multiplying both sides by $(n+1)$ and dividing both sides by $\epsilon$ we have $\frac{1}{\epsilon}< n+1$ . Now we want to subtract 1 from both sides and we arrive at $\frac{1}{\epsilon}-1 < n$ . We will choose $N=\frac{1}{\epsilon}-1$ for when $n>1$ I'm new to formulating proofs with rigor. Thanks for your help.","I'm having issues forming the discussion part of the proof because I am not sure if I am coming up with the right estimation. Is this an appropriate way of coming up with an estimation? I wrote: We want to show that , , s.t. . Then we will proceed by simplifying by using the conjugate.  Now we will proceed by making an estimation, we see that So let Then by multiplying both sides by and dividing both sides by we have . Now we want to subtract 1 from both sides and we arrive at . We will choose for when I'm new to formulating proofs with rigor. Thanks for your help.","\forall \epsilon >0 \exists N>0 N\in \mathbb{N} n>N \Longrightarrow |(\sqrt{n^2+1}-n)-0|<\epsilon 
\begin{split}
\sqrt{n^2+1}-n
 &= \left(\sqrt{n^2+1}-n\right) \times
    \frac{\sqrt{n^2+1}+n}{\sqrt{n^2+1}+n}\\
 &=\frac{n^2+1-n^2}{\sqrt{n^2+1}+n}\\
 &=\frac{1}{\sqrt{n^2+1}+n}
\end{split}
 
\frac{1}{\sqrt{n^2+1}+n} \leq \frac{1}{n+1},
\quad \text{where } n > 1.
 \frac{1}{n+1} < \epsilon (n+1) \epsilon \frac{1}{\epsilon}< n+1 \frac{1}{\epsilon}-1 < n N=\frac{1}{\epsilon}-1 n>1","['real-analysis', 'proof-verification', 'estimation']"
72,Does $f'(x)$ always remain close to $f(x)/x$ as $x \rightarrow 0$?,Does  always remain close to  as ?,f'(x) f(x)/x x \rightarrow 0,"Consider a function $f: (0,1): \to  (0,1)$ such that $\lim_{x \rightarrow 0+} f(x) = 0$, $\lim_{x \rightarrow 1-} f(x) = 1$; $f$ has a power series expansion around $x=1$ that converges in $(0,1)$: $$ f(x) = 1 - \sum_{k=1}^\infty c_k(1-x)^k. $$ with $c_k \geq 0$ for all $k$. Note that the condition $\lim_{x \rightarrow 0+} f(x) = 0$ implies $\sum_{k=1}^\infty c_k = 1$. A typical example may be $f(x) = x^{2/3}$. Note that the derivative of $f$ may tend to $\infty$ as $x$ approaches $0$ from the right, as in this example. I would like to prove (or find a counter-example) that the derivative $f'(x)$ remains asymptotically close to $f(x)/x$ as $x \rightarrow 0$. By that I mean any of the following: the limit $$ \lim_{x \rightarrow 0} f'(x)x/f(x) $$ exists and is finite and nonzero; or more generally that $f'(x) = \Theta(f(x)/x)$ as $p \rightarrow 0$, where $\Theta$ means ""bounded below and above asymptically"" (see definition here ); or any similar result that asserts that $f'(x)$ does not ""deviate too much"" from $f(x)/x$ as $x \rightarrow 0$. For the example function this is true. In fact the limit exists and equals $2/3$. This seems to be the case (with other values) for all functions I am testing, but how to prove it/disprove it?","Consider a function $f: (0,1): \to  (0,1)$ such that $\lim_{x \rightarrow 0+} f(x) = 0$, $\lim_{x \rightarrow 1-} f(x) = 1$; $f$ has a power series expansion around $x=1$ that converges in $(0,1)$: $$ f(x) = 1 - \sum_{k=1}^\infty c_k(1-x)^k. $$ with $c_k \geq 0$ for all $k$. Note that the condition $\lim_{x \rightarrow 0+} f(x) = 0$ implies $\sum_{k=1}^\infty c_k = 1$. A typical example may be $f(x) = x^{2/3}$. Note that the derivative of $f$ may tend to $\infty$ as $x$ approaches $0$ from the right, as in this example. I would like to prove (or find a counter-example) that the derivative $f'(x)$ remains asymptotically close to $f(x)/x$ as $x \rightarrow 0$. By that I mean any of the following: the limit $$ \lim_{x \rightarrow 0} f'(x)x/f(x) $$ exists and is finite and nonzero; or more generally that $f'(x) = \Theta(f(x)/x)$ as $p \rightarrow 0$, where $\Theta$ means ""bounded below and above asymptically"" (see definition here ); or any similar result that asserts that $f'(x)$ does not ""deviate too much"" from $f(x)/x$ as $x \rightarrow 0$. For the example function this is true. In fact the limit exists and equals $2/3$. This seems to be the case (with other values) for all functions I am testing, but how to prove it/disprove it?",,"['calculus', 'real-analysis', 'functions', 'asymptotics']"
73,Quasi-Cauchy sequences,Quasi-Cauchy sequences,,"Sequence $(x_n)$ is called quasi-Cauchy if $\lim_{n\rightarrow\infty}|x_{n+1}-x_n|=0.$ I need help proving the following theorems: Quasi-Cauchy sequence of real numbers is Cauchy if and only if it has exactly one cluster point. Sequence of real numbers is Cauchy if and only if every subsequence is quasi-Cauchy. I understand the implications to the right (they are trivial), but have trouble proving the opposite way. Any help would be appreciated :)","Sequence $(x_n)$ is called quasi-Cauchy if $\lim_{n\rightarrow\infty}|x_{n+1}-x_n|=0.$ I need help proving the following theorems: Quasi-Cauchy sequence of real numbers is Cauchy if and only if it has exactly one cluster point. Sequence of real numbers is Cauchy if and only if every subsequence is quasi-Cauchy. I understand the implications to the right (they are trivial), but have trouble proving the opposite way. Any help would be appreciated :)",,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
74,Let $x_1=a>0$ and $x_{n+1}=x_n+\frac{1}{x_n} \forall n\in \mathbb N$. Check whether the following sequence converges or diverges.,Let  and . Check whether the following sequence converges or diverges.,x_1=a>0 x_{n+1}=x_n+\frac{1}{x_n} \forall n\in \mathbb N,"Let $x_1=a>0$ and $x_{n+1}=x_n+\frac{1}{x_n} \forall n\in \mathbb N$ .   Check whether the following sequence converges or diverges. When I was in UG my teacher used derivative test for monotonicity. $f(x)=x+\frac{1}{x}, f'(x)=1-\frac{1}{x^2}>0(x>1).$ So, $f(x)$ is increasing. How to prove the sequence is monotonic? Differentiation is coming after the sequences and series. By AM-GM inequality sequence is bounded below. $x_{n+1}=x_n+\frac{1}{x_n}\ge 2\sqrt{x_n.\frac{1}{x_n}}=2  \forall n\in \mathbb N$ . How can I judge whether the sequence bounded above or not? Please help me.","Let and .   Check whether the following sequence converges or diverges. When I was in UG my teacher used derivative test for monotonicity. So, is increasing. How to prove the sequence is monotonic? Differentiation is coming after the sequences and series. By AM-GM inequality sequence is bounded below. . How can I judge whether the sequence bounded above or not? Please help me.","x_1=a>0 x_{n+1}=x_n+\frac{1}{x_n} \forall n\in \mathbb N f(x)=x+\frac{1}{x}, f'(x)=1-\frac{1}{x^2}>0(x>1). f(x) x_{n+1}=x_n+\frac{1}{x_n}\ge 2\sqrt{x_n.\frac{1}{x_n}}=2  \forall n\in \mathbb N",['real-analysis']
75,Prove that the sum of two continuous functions is continuous,Prove that the sum of two continuous functions is continuous,,"$$\lim _{x\rightarrow a} f(x)+g(x) = f(a)+g(a)$$ Let $\epsilon>0$ be given Since $f$ and $g$ are continuous, $|f(x)-f(a)|< \epsilon$ when $0<|x-a|< \delta_f $ and $|g(x)-g(a)|< \epsilon$ $\ $ when $0<|x-a|< \delta_g$ Let $\delta_h$ be defined as $\min(\delta_g ,\delta_f)$ and $h(x)$ be defined as $f(x)+g(x)$ $$\ |f(x)-f(a)|+|g(x)-g(a)|< 2\epsilon\tag{1}$$ $$\ |f(x)-f(a) + g(x)-g(a)|< 2\epsilon$$ $$\ |h(x)-h(a)|< 2\epsilon$$ We can replace $\delta_f$ and $\delta_g$ by $𝛿_h$ to get $|f(x)-f(a)| < \epsilon$ when $0<|x-a|<\delta_h$ and $\ |g(x)-g(a)|< \epsilon$ when $0<|x-a|<\delta_h$ Since (1) is true when $0<|x-a|<\delta_h$ , we can find a $\delta_h$ for every value of $2ε$ for every $2\epsilon>0$ there is a $\delta>0$ such that $|x−a|<\delta\implies ∣h(x)−h(a)∣<2\epsilon$ and for every $2\epsilon$ there exists an $\epsilon$ so for every $\epsilon$ there exists a $\delta$ . Is the proof logically correct and is it worded correctly?","Let be given Since and are continuous, when and when Let be defined as and be defined as We can replace and by to get when and when Since (1) is true when , we can find a for every value of for every there is a such that and for every there exists an so for every there exists a . Is the proof logically correct and is it worded correctly?","\lim _{x\rightarrow a} f(x)+g(x) = f(a)+g(a) \epsilon>0 f g |f(x)-f(a)|< \epsilon 0<|x-a|< \delta_f  |g(x)-g(a)|< \epsilon \  0<|x-a|< \delta_g \delta_h \min(\delta_g ,\delta_f) h(x) f(x)+g(x) \ |f(x)-f(a)|+|g(x)-g(a)|< 2\epsilon\tag{1} \ |f(x)-f(a) + g(x)-g(a)|< 2\epsilon \ |h(x)-h(a)|< 2\epsilon \delta_f \delta_g 𝛿_h |f(x)-f(a)| < \epsilon 0<|x-a|<\delta_h \ |g(x)-g(a)|< \epsilon 0<|x-a|<\delta_h 0<|x-a|<\delta_h \delta_h 2ε 2\epsilon>0 \delta>0 |x−a|<\delta\implies ∣h(x)−h(a)∣<2\epsilon 2\epsilon \epsilon \epsilon \delta","['real-analysis', 'calculus', 'continuity', 'solution-verification', 'epsilon-delta']"
76,Proof that the generalized inverse of an increasing right-continuous function is also right-continuous,Proof that the generalized inverse of an increasing right-continuous function is also right-continuous,,"Let $a:[0,\infty) \to [0,\infty]$ be an increasing right continuous function, and $\tau:[0,\infty) \to [0,\infty]$ be the generalized inverse, i.e. $\tau(s):=\inf\{t\ge 0: a(t)>s\}, \inf \emptyset = \infty$. Prove that $\tau$ is right continuous. Proof. $$\{t:a(t)>s\} = \inf_{\epsilon>0} \{t:a(t)>s+\epsilon\}.$$ Therefore, $\inf\{t\ge 0: a(t)>s\} = \inf_{\epsilon>0} \inf \{t\ge 0: a(t)>s+\epsilon\}$ proving right continuity. I have trouble figuring out why this last equality proves right-continuity. Is there a way to see this immediately? I would greatly appreciate any help.","Let $a:[0,\infty) \to [0,\infty]$ be an increasing right continuous function, and $\tau:[0,\infty) \to [0,\infty]$ be the generalized inverse, i.e. $\tau(s):=\inf\{t\ge 0: a(t)>s\}, \inf \emptyset = \infty$. Prove that $\tau$ is right continuous. Proof. $$\{t:a(t)>s\} = \inf_{\epsilon>0} \{t:a(t)>s+\epsilon\}.$$ Therefore, $\inf\{t\ge 0: a(t)>s\} = \inf_{\epsilon>0} \inf \{t\ge 0: a(t)>s+\epsilon\}$ proving right continuity. I have trouble figuring out why this last equality proves right-continuity. Is there a way to see this immediately? I would greatly appreciate any help.",,"['real-analysis', 'analysis', 'limits', 'continuity']"
77,Convergent sequences and accumulation points [duplicate],Convergent sequences and accumulation points [duplicate],,"This question already has an answer here : If a Cauchy Sequence has an accumulation point, then it converges to said accumulation point (1 answer) Closed 1 year ago . Definitions: Let $a$ be an accumulation point of $A$ . Then $\forall \ \epsilon >0$ , $B_{\epsilon}(a) \setminus \{a\}$ contains an element of $A $ . Question: I have two questions: if $(a_n)_{n\in N}$ is a convergent sequence in $\mathbb{R}$ then, Does the set $\{a_n\}$ have exactly one accumulation point? Or, could it have more than one? If so, does $(a_n)_{n\in N}$ necessarily converge to the said accumulation point? I'm tempted to say no to (1), but I'm afraid that I'm missing something. My counter-example to (1) is $\{a_n\} = \{ 4, 3, 2, 1, 0,0,0,...\}$ (i.e. inserting $0$ s after the 4th element). Then the set has no accumulation point and it converges to 0. Is that correct?","This question already has an answer here : If a Cauchy Sequence has an accumulation point, then it converges to said accumulation point (1 answer) Closed 1 year ago . Definitions: Let be an accumulation point of . Then , contains an element of . Question: I have two questions: if is a convergent sequence in then, Does the set have exactly one accumulation point? Or, could it have more than one? If so, does necessarily converge to the said accumulation point? I'm tempted to say no to (1), but I'm afraid that I'm missing something. My counter-example to (1) is (i.e. inserting s after the 4th element). Then the set has no accumulation point and it converges to 0. Is that correct?","a A \forall \ \epsilon >0 B_{\epsilon}(a) \setminus \{a\} A  (a_n)_{n\in N} \mathbb{R} \{a_n\} (a_n)_{n\in N} \{a_n\} = \{ 4, 3, 2, 1, 0,0,0,...\} 0","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
78,Multivariable uniform convergence and differentiation,Multivariable uniform convergence and differentiation,,"There is a well known theorem on the relationship between uniform convergence of univariate functions and differentiation.  Quoting Theorem 7.17 from Rudin 1976, Principles of Mathematical Analysis : Theorem: Suppose $\{f_n\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0$ on $[a,b]$. If $\{f_n'\}$ converges uniformly on $[a,b]$, then $\{f_n\}$ converges uniformly on $[a,b]$ to a function $f$, and $f'(x) = \lim_{n\rightarrow\infty} f_n'(x)$. My question is, does a generalisation of this theorem exist for sequences of functions $f_n: \mathbb{R}^n \rightarrow \mathbb{R}?$ For example, can $f'_n$ be replaced by $\nabla f_n$ and $[a,b]$ be replaced by a compact set?","There is a well known theorem on the relationship between uniform convergence of univariate functions and differentiation.  Quoting Theorem 7.17 from Rudin 1976, Principles of Mathematical Analysis : Theorem: Suppose $\{f_n\}$ is a sequence of functions, differentiable on $[a, b]$ and such that $\{f_n(x_0)\}$ converges for some point $x_0$ on $[a,b]$. If $\{f_n'\}$ converges uniformly on $[a,b]$, then $\{f_n\}$ converges uniformly on $[a,b]$ to a function $f$, and $f'(x) = \lim_{n\rightarrow\infty} f_n'(x)$. My question is, does a generalisation of this theorem exist for sequences of functions $f_n: \mathbb{R}^n \rightarrow \mathbb{R}?$ For example, can $f'_n$ be replaced by $\nabla f_n$ and $[a,b]$ be replaced by a compact set?",,"['real-analysis', 'multivariable-calculus']"
79,$\lim_{k \to \infty} \frac{x_k}{k^2}$,,\lim_{k \to \infty} \frac{x_k}{k^2},"Let $(x_k)$ be the sequence of real numbers defined as follows: $x_1=1$; $x_{k+1}=x_k+\sqrt {x_k}$ for $k>0$. Find $$\lim_{k \to \infty} \frac{x_k}{k^2}$$ My thought is to find some $y_k$ which is a function of $x_k$ and find the limit of this $y_k$, but I'm not sure if this will work, or what my $y_k$ should be.","Let $(x_k)$ be the sequence of real numbers defined as follows: $x_1=1$; $x_{k+1}=x_k+\sqrt {x_k}$ for $k>0$. Find $$\lim_{k \to \infty} \frac{x_k}{k^2}$$ My thought is to find some $y_k$ which is a function of $x_k$ and find the limit of this $y_k$, but I'm not sure if this will work, or what my $y_k$ should be.",,"['real-analysis', 'sequences-and-series', 'limits']"
80,Polygonally connected open sets,Polygonally connected open sets,,I cannot understand the following theorem: An open set $S$ in $\Re^n$ is connected if and only if it is polygonally connected. I would be thankful if some one could present an intuitive proof of this theorem. Thanks for reading!,I cannot understand the following theorem: An open set $S$ in $\Re^n$ is connected if and only if it is polygonally connected. I would be thankful if some one could present an intuitive proof of this theorem. Thanks for reading!,,['real-analysis']
81,A bounded function in $\Bbb R$ with closed graph is continuous.,A bounded function in  with closed graph is continuous.,\Bbb R,"It is known that if a function $f:\Bbb R\to \Bbb R$ is continuous then its graph is closed. Proof. Let $(x_n)_{n\in\Bbb N}$ be a sequence in $\Bbb R$ so that the sequence $(x_n,f(x_n))_{n\in\Bbb N}$ is convergent in $\Bbb R^2$ at a point $(x,y)\in\Bbb R^2$ . Then as the convergence in $\Bbb R^2$ is point-wise we have $$(x_n,f(x_n)) \xrightarrow{n\to \infty} (x,y) \Longrightarrow x_n\to x \ \  \& \ \  f(x_n)\to y $$ Now, from the continuity of $f$ we have that $$x_n \to x \Longrightarrow f(x_n)\to f(x)$$ and from the uniquence of the limits we assume that $y=f(x)$ . So, $\lim_{n\to\infty}(x_n,f(x_n))=(x,f(x))\in G(f)$ and so $G(f)$ is closed. We know that the converse is not true in general, that is if the graph of a real function $f$ is closed we cannot conclude that $f$ is continuous. One counter-example is the function: $$f: \Bbb R \to \Bbb R, \ \ f(x)=\begin{cases}  \text{$\frac{1}{x} \ \ \ \ $ if } x \neq 0 \\    \text{$0 \ \ \ \ \ $ if } x= 0 \end{cases}$$ $f$ is discontinuous at $x=0$ and $G(f)=\left\{ \left(0,0\right)\right\} \cup\left\{ (x,\frac{1}{x})\big|x\in\mathbb{R}\setminus\left\{ 0\right\} \right\} $ is closed because both of the sets are closed. BUT if we add that $f$ is bounded, then it can be proved that $f$ is continuous. I am having trouble in the proof. Here is my attempt: Attempt of a proof. Let $(x_n)_{n\in\Bbb N}$ be a real sequence that converges to some $x\in\Bbb R$ . We need to prove that $f(x_n)\xrightarrow{n\to \infty}f(x)$ . We have that for all $n\in\Bbb N$ $(x_n,f(x_n))\in G(f)$ and that $((x_n,f(x_n))_{n\in\Bbb N}$ is bounded on $\Bbb R^2$ (since $(x_n)_{n\in\Bbb N}$ is convergent and $f$ is bounded). So from the Bolzano-Weierstrass theorem there exists a subsequence $(x_{k_n})_{n\in\Bbb N}$ of $(x_{n})_{n\in\Bbb N}$ so that $(x_{k_n},f(x_{k_n}))_{n\in\Bbb N}$ converges. Now, because $G(f)$ is a closed set $\exists x'\in\Bbb R :(x_{k_n},f(x_{k_n}))\to (x',f(x')) $ and so $x_{k_n}\to x'$ and $f(x_{k_n})\to f(x')$ . Moreover, $x_n\to x \ \ \& \ \ x_{k_n}\to x' \Longrightarrow x=x'$ and so $f(x_{k_n})\to f(x)$ . I cannot go any further than this.","It is known that if a function is continuous then its graph is closed. Proof. Let be a sequence in so that the sequence is convergent in at a point . Then as the convergence in is point-wise we have Now, from the continuity of we have that and from the uniquence of the limits we assume that . So, and so is closed. We know that the converse is not true in general, that is if the graph of a real function is closed we cannot conclude that is continuous. One counter-example is the function: is discontinuous at and is closed because both of the sets are closed. BUT if we add that is bounded, then it can be proved that is continuous. I am having trouble in the proof. Here is my attempt: Attempt of a proof. Let be a real sequence that converges to some . We need to prove that . We have that for all and that is bounded on (since is convergent and is bounded). So from the Bolzano-Weierstrass theorem there exists a subsequence of so that converges. Now, because is a closed set and so and . Moreover, and so . I cannot go any further than this.","f:\Bbb R\to \Bbb R (x_n)_{n\in\Bbb N} \Bbb R (x_n,f(x_n))_{n\in\Bbb N} \Bbb R^2 (x,y)\in\Bbb R^2 \Bbb R^2 (x_n,f(x_n)) \xrightarrow{n\to \infty} (x,y) \Longrightarrow x_n\to x \ \  \& \ \  f(x_n)\to y  f x_n \to x \Longrightarrow f(x_n)\to f(x) y=f(x) \lim_{n\to\infty}(x_n,f(x_n))=(x,f(x))\in G(f) G(f) f f f: \Bbb R \to \Bbb R, \ \ f(x)=\begin{cases}
 \text{\frac{1}{x} \ \ \ \  if } x \neq 0 \\ 
  \text{0 \ \ \ \ \  if } x= 0
\end{cases} f x=0 G(f)=\left\{ \left(0,0\right)\right\} \cup\left\{ (x,\frac{1}{x})\big|x\in\mathbb{R}\setminus\left\{ 0\right\} \right\}  f f (x_n)_{n\in\Bbb N} x\in\Bbb R f(x_n)\xrightarrow{n\to \infty}f(x) n\in\Bbb N (x_n,f(x_n))\in G(f) ((x_n,f(x_n))_{n\in\Bbb N} \Bbb R^2 (x_n)_{n\in\Bbb N} f (x_{k_n})_{n\in\Bbb N} (x_{n})_{n\in\Bbb N} (x_{k_n},f(x_{k_n}))_{n\in\Bbb N} G(f) \exists x'\in\Bbb R :(x_{k_n},f(x_{k_n}))\to (x',f(x'))  x_{k_n}\to x' f(x_{k_n})\to f(x') x_n\to x \ \ \& \ \ x_{k_n}\to x' \Longrightarrow x=x' f(x_{k_n})\to f(x)",['real-analysis']
82,Is there a mean-value theorem for volume integrals?,Is there a mean-value theorem for volume integrals?,,"I have just been reading about the mean value theorems for integrals, surface integrals and line integrals. I did a Google search for a corresponding theorem of volume integrals, and couldn't find any evidence of one. I would guess that, if $f(x,y,z)$ is a differentiable function defined in the region $\Omega$ in $\mathbb{R}^3$, then: $\iiint_{\Omega}f(x,y,z)dxdydz=f(x_0,y_0,z_0)V(\Omega)$, where $(x_0,y_0,z_0)$ is some point in $\Omega$ and $V(\Omega)$ is the volume of the region $\Omega$. This holds for the trivial example $f(x,y,z)=c$, where $c$ is a constant in the domain of interest. Are there counter-examples, or does such a theorem exist?","I have just been reading about the mean value theorems for integrals, surface integrals and line integrals. I did a Google search for a corresponding theorem of volume integrals, and couldn't find any evidence of one. I would guess that, if $f(x,y,z)$ is a differentiable function defined in the region $\Omega$ in $\mathbb{R}^3$, then: $\iiint_{\Omega}f(x,y,z)dxdydz=f(x_0,y_0,z_0)V(\Omega)$, where $(x_0,y_0,z_0)$ is some point in $\Omega$ and $V(\Omega)$ is the volume of the region $\Omega$. This holds for the trivial example $f(x,y,z)=c$, where $c$ is a constant in the domain of interest. Are there counter-examples, or does such a theorem exist?",,"['real-analysis', 'integration', 'definite-integrals']"
83,Let $f$ be a real uniformly continuous function on the bounded set $E$ in $\mathbb{R^1}$. Prove that $f$ is bounded on $E$,Let  be a real uniformly continuous function on the bounded set  in . Prove that  is bounded on,f E \mathbb{R^1} f E,"Let $f$ be a real uniformly continuous function on the bounded set $E$ in $\mathbb{R^1}$. Prove that $f$ is bounded on $E$ My (Attempted) Proof Since $E$ is bounded, put $\alpha = \sup E$, $\beta = \inf E$. Now since $f$ is uniformly continuous on $E$, we only have to prove convergence of $f$ as $x \in E \to \alpha$ and $x \in E \to \beta$. So let $\{x_n\}$ be a  Cauchy sequence and fix $\epsilon > 0$. Let $\delta$ be such that $d(f(p), f(q)) < \epsilon$. Now take $N$ so large such that $m, n > N \implies d(x_n, x_m) < \delta$. We then have $d(f(x_n), f(x_m)) <  \epsilon$ (By uniform continuity of $f$) Thus $\{f(x_n)\}$ is a Cauchy sequence, and thus converges to some point $L \in \mathbb{R^1}$ $$\begin{aligned}\therefore \text{As} \ \ x_n \to \beta \ , \  \ f(x_n) \to L_1\\ \ \ x_n \to \alpha \ , \  \ f(x_n) \to L_2\\\end{aligned}$$ ( Comment: Since we let $\{x_n\}$ be an arbitrary Cauchy sequence, we can pick two different Cauchy sequences, one that converges to $\beta$, and one that converges to $\alpha$, and thus the above statement holds) Now put $\gamma = \max\{L_1, L_2, f(x)\  | \  x \in E\}$ and $\eta = \min\{L_1, L_2, f(x)\  | \  x \in E\}$. Then we have $\gamma = \sup f[E]$, and $\eta = \inf f[E]$, and thus $f[E]$ is bounded. $\ \square$ First of all if my proof rigorously correct? Secondly, if you have any criticism on my proof-writing skills, please let me know, as I'm always looking to improve. Finally, I proved that $f$ is bounded on $E$ by proving that the sequence $\{f(x_n)\}$ was Cauchy, and would thus converge, but are there other cleaner/more efficient ways of proving that $f$ is bounded on $E$?","Let $f$ be a real uniformly continuous function on the bounded set $E$ in $\mathbb{R^1}$. Prove that $f$ is bounded on $E$ My (Attempted) Proof Since $E$ is bounded, put $\alpha = \sup E$, $\beta = \inf E$. Now since $f$ is uniformly continuous on $E$, we only have to prove convergence of $f$ as $x \in E \to \alpha$ and $x \in E \to \beta$. So let $\{x_n\}$ be a  Cauchy sequence and fix $\epsilon > 0$. Let $\delta$ be such that $d(f(p), f(q)) < \epsilon$. Now take $N$ so large such that $m, n > N \implies d(x_n, x_m) < \delta$. We then have $d(f(x_n), f(x_m)) <  \epsilon$ (By uniform continuity of $f$) Thus $\{f(x_n)\}$ is a Cauchy sequence, and thus converges to some point $L \in \mathbb{R^1}$ $$\begin{aligned}\therefore \text{As} \ \ x_n \to \beta \ , \  \ f(x_n) \to L_1\\ \ \ x_n \to \alpha \ , \  \ f(x_n) \to L_2\\\end{aligned}$$ ( Comment: Since we let $\{x_n\}$ be an arbitrary Cauchy sequence, we can pick two different Cauchy sequences, one that converges to $\beta$, and one that converges to $\alpha$, and thus the above statement holds) Now put $\gamma = \max\{L_1, L_2, f(x)\  | \  x \in E\}$ and $\eta = \min\{L_1, L_2, f(x)\  | \  x \in E\}$. Then we have $\gamma = \sup f[E]$, and $\eta = \inf f[E]$, and thus $f[E]$ is bounded. $\ \square$ First of all if my proof rigorously correct? Secondly, if you have any criticism on my proof-writing skills, please let me know, as I'm always looking to improve. Finally, I proved that $f$ is bounded on $E$ by proving that the sequence $\{f(x_n)\}$ was Cauchy, and would thus converge, but are there other cleaner/more efficient ways of proving that $f$ is bounded on $E$?",,"['real-analysis', 'proof-verification', 'metric-spaces', 'proof-writing', 'alternative-proof']"
84,Proving $\sqrt{6}$ is not part of a field,Proving  is not part of a field,\sqrt{6},"I'm currently in a beginning analysis course, and I am asked to prove that $F =\{a+b\sqrt2 +c\sqrt3 :a,b,c∈Q\}$ is not a field. I know that this violates the first multiplication axiom, that if $x,y \in F$ then $xy \in F$. However, I don't know how to prove that $\sqrt6$ cannot be written in the form $a+b\sqrt2 +c\sqrt3,$ where $a,b,c∈Q$. Is there a way to show this using elementary algebra, and not go into field extensions?","I'm currently in a beginning analysis course, and I am asked to prove that $F =\{a+b\sqrt2 +c\sqrt3 :a,b,c∈Q\}$ is not a field. I know that this violates the first multiplication axiom, that if $x,y \in F$ then $xy \in F$. However, I don't know how to prove that $\sqrt6$ cannot be written in the form $a+b\sqrt2 +c\sqrt3,$ where $a,b,c∈Q$. Is there a way to show this using elementary algebra, and not go into field extensions?",,"['real-analysis', 'field-theory']"
85,Zeros and poles of some meromorphic 1-forms on the riemann sphere,Zeros and poles of some meromorphic 1-forms on the riemann sphere,,"Let $X=\mathbb C_{\infty}$ be the Riemann sphere with the local coordinates $\{z\ ,1/z\}$. I want to show the following two statements: i) There does not exist any non-vanishing holomorphic 1-form on $X$. ii) Where are the poles and zeros of the meromorphic 1-forms $dz$ and $d/z$? Also determine their orders. My attempt: i) Let $w$ be a non-vanishing 1-form on $X$. Then we can write $w=f(z)dz$ in the coordinate $z$ for a holomorphic function $f$. In the other chart we have then $w=f(\frac{1}{z})(-\frac{1}{z^2})d/z$. Now the laurent-series of $f$ around $0$ has only non-negative exponents, hence the above function has a pole in $0$, which is a contradiction to the assumption that $f$ is holomorphic. ii) For $w=1 dz$: $1$ has no zeros or poles in $\mathbb C$. Lets consider $\infty:$ In the other char we have $w=-1/z^2$ which has a pole of order two in zero, hence we have $ord_{\infty}w=-2$ and $ord_p(w)=0$ for $p\in\mathbb C$. For $w=dz/z$: $1/z$ has only a pole (of order $1$) in zero. In the other chart we have $w=-1/z$ which has also just a pole of order 1 in zero. Hence we have $ord_0(w)=-1$, $ord_{\infty}(w)=-1$ and $ord_p(w)=0$ otherwise. Since I am a beginner I would like if someone could check my solutions.  Thanks in advance!:)","Let $X=\mathbb C_{\infty}$ be the Riemann sphere with the local coordinates $\{z\ ,1/z\}$. I want to show the following two statements: i) There does not exist any non-vanishing holomorphic 1-form on $X$. ii) Where are the poles and zeros of the meromorphic 1-forms $dz$ and $d/z$? Also determine their orders. My attempt: i) Let $w$ be a non-vanishing 1-form on $X$. Then we can write $w=f(z)dz$ in the coordinate $z$ for a holomorphic function $f$. In the other chart we have then $w=f(\frac{1}{z})(-\frac{1}{z^2})d/z$. Now the laurent-series of $f$ around $0$ has only non-negative exponents, hence the above function has a pole in $0$, which is a contradiction to the assumption that $f$ is holomorphic. ii) For $w=1 dz$: $1$ has no zeros or poles in $\mathbb C$. Lets consider $\infty:$ In the other char we have $w=-1/z^2$ which has a pole of order two in zero, hence we have $ord_{\infty}w=-2$ and $ord_p(w)=0$ for $p\in\mathbb C$. For $w=dz/z$: $1/z$ has only a pole (of order $1$) in zero. In the other chart we have $w=-1/z$ which has also just a pole of order 1 in zero. Hence we have $ord_0(w)=-1$, $ord_{\infty}(w)=-1$ and $ord_p(w)=0$ otherwise. Since I am a beginner I would like if someone could check my solutions.  Thanks in advance!:)",,"['real-analysis', 'general-topology', 'complex-analysis', 'riemann-surfaces']"
86,Theorem 2.17 from RCA Rudin,Theorem 2.17 from RCA Rudin,,"$\bf 2.17\ $ Theorem $\ $ Suppose $X$ is a locally compact, $\sigma$-compact Hausdorff space. If $\frak M$ and $\mu$ are as described in the statement of Theorem $\it 2.14$, then $\frak M$ and $\mu$ have the following properties: $(a)\ \ $ If $E\in\frak M$ and $\epsilon>0$, there is a closed set $F$ and an open set $V$ such that $F\subset E\subset V$ and $\mu(V-F)<\epsilon$. $(b)\ \ $ $\mu$ is a regular Borel measure on $X$. $(c)\ \ $ If $E\in\frak M$, there are sets $A$ and $B$ such that $A$ is an $F_\sigma$, $B$ is a $G_b$, $A\subset E\subset B$, and $\mu(B-A)=0.$ $\rm P\scriptstyle{\rm ROOF}$ $\quad$ Every closed set $F\subset X$ is a $\sigma$-compact, because $F=\bigcup(F\cap K_n)$. Hence $(a)$ implies that every set $E\in\frak M$ is inner regular. This proves $(b)$. I understood the proof of points $(a)$ and $(c)$. But I can't understand the proof of $(b)$. It's obvious that every closed set is $\sigma$-compact. But how Rudin applies $(a)$ here? We have to show that if $\alpha>0$ then exists compact set $K\subset E$ such that $\mu(K)>\alpha$. Can anyone explain it to me please?","$\bf 2.17\ $ Theorem $\ $ Suppose $X$ is a locally compact, $\sigma$-compact Hausdorff space. If $\frak M$ and $\mu$ are as described in the statement of Theorem $\it 2.14$, then $\frak M$ and $\mu$ have the following properties: $(a)\ \ $ If $E\in\frak M$ and $\epsilon>0$, there is a closed set $F$ and an open set $V$ such that $F\subset E\subset V$ and $\mu(V-F)<\epsilon$. $(b)\ \ $ $\mu$ is a regular Borel measure on $X$. $(c)\ \ $ If $E\in\frak M$, there are sets $A$ and $B$ such that $A$ is an $F_\sigma$, $B$ is a $G_b$, $A\subset E\subset B$, and $\mu(B-A)=0.$ $\rm P\scriptstyle{\rm ROOF}$ $\quad$ Every closed set $F\subset X$ is a $\sigma$-compact, because $F=\bigcup(F\cap K_n)$. Hence $(a)$ implies that every set $E\in\frak M$ is inner regular. This proves $(b)$. I understood the proof of points $(a)$ and $(c)$. But I can't understand the proof of $(b)$. It's obvious that every closed set is $\sigma$-compact. But how Rudin applies $(a)$ here? We have to show that if $\alpha>0$ then exists compact set $K\subset E$ such that $\mu(K)>\alpha$. Can anyone explain it to me please?",,"['real-analysis', 'general-topology', 'measure-theory']"
87,"$f \in C^2(\mathbb R)$ , $(f(x))^2 \le 1$ ; $(f'(x))^2+(f''(x))^2 \le 1 $ ; then is $(f(x))^2+(f'(x))^2 \le 1 $?",",  ;  ; then is ?",f \in C^2(\mathbb R) (f(x))^2 \le 1 (f'(x))^2+(f''(x))^2 \le 1  (f(x))^2+(f'(x))^2 \le 1 ,"Let $f \in C^2(\mathbb R)$ be such that $$(f(x))^2 \le 1 ;  (f'(x))^2+(f''(x))^2 \le 1 , \forall x \in \mathbb R$$  Then is it   true that $(f(x))^2+(f'(x))^2 \le 1 , \forall x \in \mathbb R$ ? I haven't gotten anywhere with this problem . Please help . Thanks in advance","Let $f \in C^2(\mathbb R)$ be such that $$(f(x))^2 \le 1 ;  (f'(x))^2+(f''(x))^2 \le 1 , \forall x \in \mathbb R$$  Then is it   true that $(f(x))^2+(f'(x))^2 \le 1 , \forall x \in \mathbb R$ ? I haven't gotten anywhere with this problem . Please help . Thanks in advance",,"['calculus', 'real-analysis']"
88,$f'(x) = g(f(x)) $ where $g: \mathbb{R} \rightarrow \mathbb{R}$ is smooth. Show $f$ is smooth. [duplicate],where  is smooth. Show  is smooth. [duplicate],f'(x) = g(f(x))  g: \mathbb{R} \rightarrow \mathbb{R} f,"This question already has answers here : The solution of ODE $k'(x) = r(k(x))$ is infinitely differentiable if $r$ is (2 answers) Closed 8 years ago . Suppose $f: \mathbb{R} \rightarrow \mathbb{R} $ is differentiable and $g: \mathbb{R} \rightarrow \mathbb{R} $ is infinitely differentiable, i.e. $ g \in C^{\infty}(\mathbb{R})$, where we know $f'(x) = g(f(x)) $ on $\mathbb{R}$. Show that $ f \in C^{\infty}(\mathbb{R})$. Thus I have to show that $f$ is infinitely differentiable, that is, derivatives of all orders exist. I can assume by induction that all derivatives of order less than, say $n$, exist, and have to show that the $nth$ derivative exists for $f$. I came up with this: $f^{n}(x) = (g \circ  f)^{n-1}(x)$. I somehow have to show that the $(n-1)th$ derivative for this composite function exists. I tried using the chain rule, but it just seems to become more ugly as I continue taking more derivatives. Obviously, I have to use the fact that $g$ is infinitely differentiable as well as the inductive assumption, although I'm not sure how to complete this line of reasoning. Maybe induction isn't even the right way to proceed. Ideas?","This question already has answers here : The solution of ODE $k'(x) = r(k(x))$ is infinitely differentiable if $r$ is (2 answers) Closed 8 years ago . Suppose $f: \mathbb{R} \rightarrow \mathbb{R} $ is differentiable and $g: \mathbb{R} \rightarrow \mathbb{R} $ is infinitely differentiable, i.e. $ g \in C^{\infty}(\mathbb{R})$, where we know $f'(x) = g(f(x)) $ on $\mathbb{R}$. Show that $ f \in C^{\infty}(\mathbb{R})$. Thus I have to show that $f$ is infinitely differentiable, that is, derivatives of all orders exist. I can assume by induction that all derivatives of order less than, say $n$, exist, and have to show that the $nth$ derivative exists for $f$. I came up with this: $f^{n}(x) = (g \circ  f)^{n-1}(x)$. I somehow have to show that the $(n-1)th$ derivative for this composite function exists. I tried using the chain rule, but it just seems to become more ugly as I continue taking more derivatives. Obviously, I have to use the fact that $g$ is infinitely differentiable as well as the inductive assumption, although I'm not sure how to complete this line of reasoning. Maybe induction isn't even the right way to proceed. Ideas?",,"['calculus', 'real-analysis', 'analysis']"
89,On a recursive sequence exercise.,On a recursive sequence exercise.,,"I have the following recursive sequence of which I want to prove the convergence: $$x_{n+1} = \frac{x_n +1}{x_n +2 }$$ and $x_1 = 0$ I have proved that it is bounded above by $1$ and that it is increasing by taking the derivative, but I am told to do it without using derivatives. How could I show that it's bounded and increasing using only elementary methods? In particular I would like to prove that it is bounded by $\frac{\sqrt{5} - 1}{2}$ (this is the limit), but any additional ways to solve it are obviously very appreciated.","I have the following recursive sequence of which I want to prove the convergence: $$x_{n+1} = \frac{x_n +1}{x_n +2 }$$ and $x_1 = 0$ I have proved that it is bounded above by $1$ and that it is increasing by taking the derivative, but I am told to do it without using derivatives. How could I show that it's bounded and increasing using only elementary methods? In particular I would like to prove that it is bounded by $\frac{\sqrt{5} - 1}{2}$ (this is the limit), but any additional ways to solve it are obviously very appreciated.",,"['real-analysis', 'sequences-and-series']"
90,Are strict local minima of a general function always countable?,Are strict local minima of a general function always countable?,,"This came up as a question in my last  real analysis test. Speaking to my teacher after the test, he said he'd forget to mention that the function should be continuous, and in fact there is an easy couterexample for his argument when the function is not continuous. Still after thinking for a long time I was unable to come up with a function with uncoutable strict local minima or with a proof that these don't exist.","This came up as a question in my last  real analysis test. Speaking to my teacher after the test, he said he'd forget to mention that the function should be continuous, and in fact there is an easy couterexample for his argument when the function is not continuous. Still after thinking for a long time I was unable to come up with a function with uncoutable strict local minima or with a proof that these don't exist.",,"['real-analysis', 'analysis']"
91,Functions such that $\sum \frac{1}{x_n}$ diverges $\Longrightarrow \sum \frac{1}{x_nf(x_n)}$ diverge,Functions such that  diverges  diverge,\sum \frac{1}{x_n} \Longrightarrow \sum \frac{1}{x_nf(x_n)},Is there a $f : \mathbb{R}_+ \to \mathbb{R}_+$ such that : $f$ is an increasing bijective map of $\mathbb{R}_+$ into itself. For all $\displaystyle\sum_n \frac{1}{x_n}$ where $(x_n)$ is increasing and potitive : $$\sum \frac{1}{x_n} \; \text{diverge}\; \Longrightarrow \sum \frac{1}{x_nf(x_n)} \; \text{diverge}$$ (From a French oral examination),Is there a $f : \mathbb{R}_+ \to \mathbb{R}_+$ such that : $f$ is an increasing bijective map of $\mathbb{R}_+$ into itself. For all $\displaystyle\sum_n \frac{1}{x_n}$ where $(x_n)$ is increasing and potitive : $$\sum \frac{1}{x_n} \; \text{diverge}\; \Longrightarrow \sum \frac{1}{x_nf(x_n)} \; \text{diverge}$$ (From a French oral examination),,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
92,Theorem 4.22 from baby Rudin,Theorem 4.22 from baby Rudin,,Can anyone explain me what would be if one of them is empty?,Can anyone explain me what would be if one of them is empty?,,['real-analysis']
93,For which conditions on countable sets does continuity implies uniform continuity,For which conditions on countable sets does continuity implies uniform continuity,,"This is an exercise from Thomson and Bruckner's ""Elementary Real Analysis"" (Exercise 5.6.5 page 239): Let $X=\{x_1,x_2,\dots,x_n,\dots\}$. What property must $X$ have so that every function continuous on $X$ is uniformly continuous on $X$? The set $X$ is a subset of $\mathbb {R}$. I worked on the $\varepsilon-\delta$ definitions of limits and uniform continuity, but I coudln't link between them. Could you please help me? Edit : I removed the part My attempt because my result was wrong.","This is an exercise from Thomson and Bruckner's ""Elementary Real Analysis"" (Exercise 5.6.5 page 239): Let $X=\{x_1,x_2,\dots,x_n,\dots\}$. What property must $X$ have so that every function continuous on $X$ is uniformly continuous on $X$? The set $X$ is a subset of $\mathbb {R}$. I worked on the $\varepsilon-\delta$ definitions of limits and uniform continuity, but I coudln't link between them. Could you please help me? Edit : I removed the part My attempt because my result was wrong.",,"['real-analysis', 'continuity', 'uniform-continuity']"
94,"Convergence of $\sum a_{n}$,$\sum a_{n}^{2}$ and $\sum a_{n}^{4}$","Convergence of , and",\sum a_{n} \sum a_{n}^{2} \sum a_{n}^{4},"A Convergent series  of  real  numbers  $\sum a_{n}$  is  given , what  can  be  said  about  the  convergence  of  $\sum a_{n}^{2}$ and  $\sum a_{n}^{4}$. Also , if  only  absolute  convergence  of $\sum a_{n}$ , i.e. convergence  of  $\sum |a_{n}|$  is  given  what  about  convergence  of  $\sum a_{n}^{2}$ and  $\sum a_{n}^{4}$. Please  give  me  some  hints as  to  how  to  proceed. Thanks.","A Convergent series  of  real  numbers  $\sum a_{n}$  is  given , what  can  be  said  about  the  convergence  of  $\sum a_{n}^{2}$ and  $\sum a_{n}^{4}$. Also , if  only  absolute  convergence  of $\sum a_{n}$ , i.e. convergence  of  $\sum |a_{n}|$  is  given  what  about  convergence  of  $\sum a_{n}^{2}$ and  $\sum a_{n}^{4}$. Please  give  me  some  hints as  to  how  to  proceed. Thanks.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
95,"$P$ is a monic polynomial of degree $n$ , then which are correct?","is a monic polynomial of degree  , then which are correct?",P n,"Suppose that $P$ is a monic polynomial of degree $n$ in one variable with real coefficients and $K$ is a real number. Then which of the following statements are necessarily correct ? If $n$ is even and $K>0$ then there exists $x_0\in \mathbb R$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K<0$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For any natural number $n$, and $0<K<1$ then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K \in \mathbb R$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For (3), consider, $P(x)=x^2+5$. Then , there does not exist $x_0\in \mathbb R$ such that $0<e^{-x_0}P(x_0)<1$. So it is FALSE. But what about the others ?","Suppose that $P$ is a monic polynomial of degree $n$ in one variable with real coefficients and $K$ is a real number. Then which of the following statements are necessarily correct ? If $n$ is even and $K>0$ then there exists $x_0\in \mathbb R$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K<0$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For any natural number $n$, and $0<K<1$ then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. If $n$ is odd and $K \in \mathbb R$ , then there exists $x_0 \in \mathbb{R}$ such that $P(x_0)=Ke^{x_0}$. For (3), consider, $P(x)=x^2+5$. Then , there does not exist $x_0\in \mathbb R$ such that $0<e^{-x_0}P(x_0)<1$. So it is FALSE. But what about the others ?",,"['real-analysis', 'analysis']"
96,"If $\sup_n\int_E f_n(x)\ \mathsf dx\leq M\mu(E)$ then the measure of $\{x\in [0,\infty)\mid f(x)>M\}=0$.",If  then the measure of .,"\sup_n\int_E f_n(x)\ \mathsf dx\leq M\mu(E) \{x\in [0,\infty)\mid f(x)>M\}=0","This question came up when I was studying for an analysis qualifying exam: Suppose $f_n\geq 0$ for all $n\geq 1$, $f_n\rightarrow f$ a.e. on $[0,\infty)$ and there exists $M>0$ such that $$\sup_n\int_E f_n(x)\ \mathsf dx\leq M\mu(E)$$ for every measurable set $E\subset [0,\infty)$ with $\mu(E)>0$. Then $\mu\{x\in [0,\infty)\mid f(x)>M\}=0$. ($\mu$ denotes Lebesgue measure on $\mathbb{R}$.) I have been trying to do something with Chebyshev's inequality, but I'm not sure I am on the right track. I would appreciate any pointers. Thanks in advance!","This question came up when I was studying for an analysis qualifying exam: Suppose $f_n\geq 0$ for all $n\geq 1$, $f_n\rightarrow f$ a.e. on $[0,\infty)$ and there exists $M>0$ such that $$\sup_n\int_E f_n(x)\ \mathsf dx\leq M\mu(E)$$ for every measurable set $E\subset [0,\infty)$ with $\mu(E)>0$. Then $\mu\{x\in [0,\infty)\mid f(x)>M\}=0$. ($\mu$ denotes Lebesgue measure on $\mathbb{R}$.) I have been trying to do something with Chebyshev's inequality, but I'm not sure I am on the right track. I would appreciate any pointers. Thanks in advance!",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
97,Counterexamples to Banach Fixed Point (Banach's Contraction) Theorem with relaxed inequalities?,Counterexamples to Banach Fixed Point (Banach's Contraction) Theorem with relaxed inequalities?,,"Banach Fixed Point theorem states: Let $(X,d)$ be a complete metric space. Suppose that $f:X→X$ is a strong contraction, i.e. there exists $q ∈ [0, 1)$ such that $d(f(x),f(y))$ $\le$ $q$ $d(x,y)$, then there is a unique point $x_0∈X$ s.t. $f(x_0)=x_0$ My questions are: 1-  If we allow $q$ to be equal to $1$, does the theorem fail? Could someone provide an example? 2- If we  substitute the strong contraction condition with the following condition: $d(f(x),f(y))$ $<$ $d(x,y)$, does the theorem fail? example?","Banach Fixed Point theorem states: Let $(X,d)$ be a complete metric space. Suppose that $f:X→X$ is a strong contraction, i.e. there exists $q ∈ [0, 1)$ such that $d(f(x),f(y))$ $\le$ $q$ $d(x,y)$, then there is a unique point $x_0∈X$ s.t. $f(x_0)=x_0$ My questions are: 1-  If we allow $q$ to be equal to $1$, does the theorem fail? Could someone provide an example? 2- If we  substitute the strong contraction condition with the following condition: $d(f(x),f(y))$ $<$ $d(x,y)$, does the theorem fail? example?",,"['real-analysis', 'banach-spaces']"
98,"Let $g(x)=f(x)+x$, where $f(x)$ is the Cantor function from $[0,1]$ to $[0,1]$. prove that $B$ is Lebesgue measurable but not Borel measurable.","Let , where  is the Cantor function from  to . prove that  is Lebesgue measurable but not Borel measurable.","g(x)=f(x)+x f(x) [0,1] [0,1] B","Let $g(x)=f(x)+x$, where $f(x)$ is the Cantor function from $[0,1]$ to $[0,1]$. We know for the Cantor set $C$, $g(C)$ contains a nonmeasurable set A. Let $B=g^{-1}(A)$, prove that $B$ is Lebesgue measurable but not Borel measurable. I can show the first one since $B$ is the subset of a null set, which means $B$ is measurable. What about the second one?","Let $g(x)=f(x)+x$, where $f(x)$ is the Cantor function from $[0,1]$ to $[0,1]$. We know for the Cantor set $C$, $g(C)$ contains a nonmeasurable set A. Let $B=g^{-1}(A)$, prove that $B$ is Lebesgue measurable but not Borel measurable. I can show the first one since $B$ is the subset of a null set, which means $B$ is measurable. What about the second one?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
99,Under what condition can converge in $L^1$ imply converge a.e.?,Under what condition can converge in  imply converge a.e.?,L^1,"Let $f_n$ be a sequence of Lebesgue measurable functions on $R^d$. Suppose you have an estimate of the form $\int_{R^d}\left|f_n\right|\le c_n$ where $c_n \downarrow 0$. Can you conclude that $f_n\to 0$ a.e.? If not, what additional conditions on ${c_n}$ would guarantee this? My attempt: I think we cannot conclude that $f_n\to 0$ a.e. For example $A_1=[0,1/2]$, $A_2=[1/2,1]$, $A_3=[0,1/4],\ldots,A_6=[3/4,1]$, $A_7=[0,1/8],\ldots$. If $f_n$ is the indicator function of $A_n$, that is $f_n(x)=1$ if $x\in A_n$ and $f_n(x)=0$ else, then $f_n \to 0$ in all $L^p([0,1])$ because $\|f_n\|_p=\lambda(A_n)^{1/p}\to 0$ but there is no $x\in [0,1]$ with $f_n(x)\to 0$. I have question in what additional conditions on ${c_n}$ would guarantee this? Maybe $c_n$ strictly decreasing? However, I have trouble proving this. Could someone kindly help about this? Thanks!","Let $f_n$ be a sequence of Lebesgue measurable functions on $R^d$. Suppose you have an estimate of the form $\int_{R^d}\left|f_n\right|\le c_n$ where $c_n \downarrow 0$. Can you conclude that $f_n\to 0$ a.e.? If not, what additional conditions on ${c_n}$ would guarantee this? My attempt: I think we cannot conclude that $f_n\to 0$ a.e. For example $A_1=[0,1/2]$, $A_2=[1/2,1]$, $A_3=[0,1/4],\ldots,A_6=[3/4,1]$, $A_7=[0,1/8],\ldots$. If $f_n$ is the indicator function of $A_n$, that is $f_n(x)=1$ if $x\in A_n$ and $f_n(x)=0$ else, then $f_n \to 0$ in all $L^p([0,1])$ because $\|f_n\|_p=\lambda(A_n)^{1/p}\to 0$ but there is no $x\in [0,1]$ with $f_n(x)\to 0$. I have question in what additional conditions on ${c_n}$ would guarantee this? Maybe $c_n$ strictly decreasing? However, I have trouble proving this. Could someone kindly help about this? Thanks!",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral', 'almost-everywhere']"
