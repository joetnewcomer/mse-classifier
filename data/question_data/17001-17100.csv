,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What does this $0$ mean in Wolfram|Alpha's application of the chain rule?,What does this  mean in Wolfram|Alpha's application of the chain rule?,0,"If I enter this command to differentiate $(2+3x)^4$ I get the step-by-step output displayed here . The relevant part of the output is as follows: Possible derivation: $\frac{d}{d x}\left((2+3x)^4\right)$ Using the chain rule, $\frac{d}{d x}\left((3x+2)^4\right) = \frac{d u^4}{d u}0$, where $u=3x+2$ and $\frac{d}{d u}\left(u^4\right)=4u^3$: $4(2+3x)^3\left(\frac{d}{d x}(2+3x)\right)$ I picked here a very simple example that could easily be done by hand. However, Wolfram|Alpha would use the same notation even if I picked a more complex example. I understand in basic terms what needs to be done to reach a solution, but I don't understand what the $0$ is doing in the notation.","If I enter this command to differentiate $(2+3x)^4$ I get the step-by-step output displayed here . The relevant part of the output is as follows: Possible derivation: $\frac{d}{d x}\left((2+3x)^4\right)$ Using the chain rule, $\frac{d}{d x}\left((3x+2)^4\right) = \frac{d u^4}{d u}0$, where $u=3x+2$ and $\frac{d}{d u}\left(u^4\right)=4u^3$: $4(2+3x)^3\left(\frac{d}{d x}(2+3x)\right)$ I picked here a very simple example that could easily be done by hand. However, Wolfram|Alpha would use the same notation even if I picked a more complex example. I understand in basic terms what needs to be done to reach a solution, but I don't understand what the $0$ is doing in the notation.",,"['calculus', 'derivatives', 'wolfram-alpha']"
1,If $abc=1$ so $\sum\limits_{cyc}\sqrt{\frac{a}{b+c}}\geq\frac{9}{\sqrt{a+b+c+15}}$,If  so,abc=1 \sum\limits_{cyc}\sqrt{\frac{a}{b+c}}\geq\frac{9}{\sqrt{a+b+c+15}},"Let $a$, $b$ and $c$ be positive numbers such that $abc=1$. Prove that: $$\sqrt{\frac{a}{b+c}}+\sqrt{\frac{b}{a+c}}+\sqrt{\frac{c}{a+b}}\geq\frac{9}{\sqrt{a+b+c+15}}$$ It seems nice enough. I proved this inequality by Holder, but it quits very ugly. Maybe there is something nice? Thank you!","Let $a$, $b$ and $c$ be positive numbers such that $abc=1$. Prove that: $$\sqrt{\frac{a}{b+c}}+\sqrt{\frac{b}{a+c}}+\sqrt{\frac{c}{a+b}}\geq\frac{9}{\sqrt{a+b+c+15}}$$ It seems nice enough. I proved this inequality by Holder, but it quits very ugly. Maybe there is something nice? Thank you!",,"['calculus', 'inequality', 'contest-math']"
2,help verifying equation $\int_0^ x \frac{1}{1+t^n} dt$,help verifying equation,\int_0^ x \frac{1}{1+t^n} dt,"As a follow up to a previous posting addressing the integral of $1/ (t^n+1)$ for $n\in \Bbb{N}$ I found the following $$\int_0^ x \frac{1}{1+t^n}\, dt=\sum_{i=0}^{\infty}\frac{(i!)(n^i)x^{in+1}} {(x^n+1)^{i+1}\prod_{k=0}^i (kn+1)}$$ My son programmed the equation in CC++, and showed me that it works for $n=1,2$, 3... But as n gets large the computer cannot calculate. Many thanks to the management of this webside. I would like assistance verifying if this equation is correct or wrong. My request is also for others to demonstrate how the equation may be developed. The relevance of the equation is that it generates multiple series/functions. Think of it as a function generator n. E.g. At n= 1 the equation gives the series for $$LN(x+1)$$ At n= 2 the equation gives the series for $$ATAN(x)$$ The equation should be useful to other.","As a follow up to a previous posting addressing the integral of $1/ (t^n+1)$ for $n\in \Bbb{N}$ I found the following $$\int_0^ x \frac{1}{1+t^n}\, dt=\sum_{i=0}^{\infty}\frac{(i!)(n^i)x^{in+1}} {(x^n+1)^{i+1}\prod_{k=0}^i (kn+1)}$$ My son programmed the equation in CC++, and showed me that it works for $n=1,2$, 3... But as n gets large the computer cannot calculate. Many thanks to the management of this webside. I would like assistance verifying if this equation is correct or wrong. My request is also for others to demonstrate how the equation may be developed. The relevance of the equation is that it generates multiple series/functions. Think of it as a function generator n. E.g. At n= 1 the equation gives the series for $$LN(x+1)$$ At n= 2 the equation gives the series for $$ATAN(x)$$ The equation should be useful to other.",,"['calculus', 'integration', 'sequences-and-series', 'power-series', 'indefinite-integrals']"
3,How do I find a closed form of ${\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx$?,How do I find a closed form of ?,{\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx,"How do I evaluate the closed form for $g(n)$? Where n is an integer, $n\ge 1$ $${\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx=g(n)$$ Make  a subsititution $u=\tanh^{-1}{x}\rightarrow dx=sech^2{u}du$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2+4u^2}du=g(n)$$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2[1+\left({2u\over \pi}\right)^2]}du=g(n)$$ $${\pi^{2n-2}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over 1+\left({2u\over \pi}\right)^2}du=g(n)$$ Apply geometric series ${1\over 1+x}=\sum_{k=0}^{\infty}(-1)^kx^k$ $${\pi^{2n-2}\over \zeta(2n)}\cdot{\left({2\over \pi}\right)^{2k}}\sum_{k=0}^{\infty}(-1)^k\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{u^{2k}\tanh^{2n}{u}}du=g(n)$$ I am shrugged here how to evaluate this integral at this point, I would some help please. The first few values for $n=1,2,3,4,...$ are $1,4,22,{428\over 3},...$ From the look at its trend, $g(n)$ only seem to  yield rational values? As for the odd powers, we get zero as a result.[checked through wolfram integrator] $$\int_{-1}^{1}{x^{2n-1}\over \pi^2+(2\tanh^{-1}{x})^2}dx=0$$","How do I evaluate the closed form for $g(n)$? Where n is an integer, $n\ge 1$ $${\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx=g(n)$$ Make  a subsititution $u=\tanh^{-1}{x}\rightarrow dx=sech^2{u}du$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2+4u^2}du=g(n)$$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2[1+\left({2u\over \pi}\right)^2]}du=g(n)$$ $${\pi^{2n-2}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over 1+\left({2u\over \pi}\right)^2}du=g(n)$$ Apply geometric series ${1\over 1+x}=\sum_{k=0}^{\infty}(-1)^kx^k$ $${\pi^{2n-2}\over \zeta(2n)}\cdot{\left({2\over \pi}\right)^{2k}}\sum_{k=0}^{\infty}(-1)^k\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{u^{2k}\tanh^{2n}{u}}du=g(n)$$ I am shrugged here how to evaluate this integral at this point, I would some help please. The first few values for $n=1,2,3,4,...$ are $1,4,22,{428\over 3},...$ From the look at its trend, $g(n)$ only seem to  yield rational values? As for the odd powers, we get zero as a result.[checked through wolfram integrator] $$\int_{-1}^{1}{x^{2n-1}\over \pi^2+(2\tanh^{-1}{x})^2}dx=0$$",,"['calculus', 'integration']"
4,"For large $n$, show that $\int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2} $ nearly equals $\frac{1}{2}$.","For large , show that  nearly equals .",n \int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2}  \frac{1}{2},"For large $n$, show that $$\int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2} $$ nearly equals $\frac{1}{2}$. Integrating by parts we get $$\int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2}=\Bigg(\frac{x^n}{1+x^2}\Bigg)^{1}_{0} - \int\limits_{0}^1 \frac{2x^{n+1}}{(1+x^2)^2}dx$$  The integral $$\int\limits_{0}^1 \frac{2x^{n+1}}{(1+x^2)^2}dx\leq\int\limits_{0}^1 {2x^{n+1}}dx =\frac{2}{n+2}$$ which $\rightarrow 0$ as $n\rightarrow \infty$ Hence the value of the main integral is $1$. Where am I wrong ?","For large $n$, show that $$\int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2} $$ nearly equals $\frac{1}{2}$. Integrating by parts we get $$\int\limits_{0}^{1}\frac{nx^{n-1}dx}{1+x^2}=\Bigg(\frac{x^n}{1+x^2}\Bigg)^{1}_{0} - \int\limits_{0}^1 \frac{2x^{n+1}}{(1+x^2)^2}dx$$  The integral $$\int\limits_{0}^1 \frac{2x^{n+1}}{(1+x^2)^2}dx\leq\int\limits_{0}^1 {2x^{n+1}}dx =\frac{2}{n+2}$$ which $\rightarrow 0$ as $n\rightarrow \infty$ Hence the value of the main integral is $1$. Where am I wrong ?",,"['calculus', 'integration', 'limits', 'definite-integrals', 'integration-by-parts']"
5,Infinitely nested radical expansion of functions,Infinitely nested radical expansion of functions,,"Is where a 'best' way to make a nested radical expansion for an analytic function? This way seems convenient: $$f(x)=a_0+a_1x+a_2x^2+a_3x^3+\dots=\sqrt{a_0^2+2a_0a_1x+(a_1^2+2a_0a_2)x^2+\cdots}=$$ $$=\sqrt{a_0^2+2a_0a_1x+(a_1^2+2a_0a_2)x^2 \sqrt{1+\cdots}}$$ I conjecture that this infinitely nested radical expansion has the same interval of convergence as the original Taylor series. Is this correct? Also, the number of 'roots' we take into account gives us twice the number plus one of the correct terms in the Taylor series. For example: $$e^x=\sqrt{1+2x+2x^2\sqrt{1+\frac{4}{3}x+\frac{10}{9}x^2\sqrt{1+\frac{32}{25}x+\frac{681}{625}x^2\sqrt{1+\dots}}}}=$$ $$=1+x+\frac{x^2}{2}+\frac{x^3}{6}+\frac{x^4}{24}+\frac{x^5}{120}+\frac{x^6}{720}+\dots$$ This expression converges for any $x$. $$\frac{1}{1-x}=\sqrt{1+2x+3x^2\sqrt{1+\frac{8}{3}x+\frac{46}{9}x^2\sqrt{1+\frac{76}{23}x+\frac{4089}{529}x^2\sqrt{1+\dots}}}}=$$ $$=1+x+x^2+x^3+x^4+x^5+x^6+\dots$$ This expression converges for $|x|<1$. This idea may seem pointless, since the coefficients in the radical expansion are very hard to calculate even for functions with 'simple' Taylor series. But is it possible, that some function with 'complicated' Taylor series will have simple nested radical expansion of this kind? For example, consider the easiest nested radical of this kind: $$f(x)=\sqrt{1+x+x^2\sqrt{1+x+x^2\sqrt{1+x+x^2\sqrt{1+\dots}}}}=$$ $$=1+\frac{x}{2}+\frac{3x^2}{8}+\frac{x^3}{16}+\frac{11x^4}{128}-\frac{9x^5}{256}+\frac{27x^6}{1024}+\dots+$$ Though this kind of functions we can always find in closed form, if we assume the infinite nested radical converges. $$f(x)=\sqrt{1+a_1x+a_2x^2\sqrt{1+a_1x+a_2x^2\sqrt{1+a_1x+a_2x^2\sqrt{1+\dots}}}}=$$ $$=\frac{a_2}{2}x^2+\sqrt{1+a_1x+\frac{a_2^2}{4}x^4}$$ I've never seen this topic discussed anywhere, so a reference would be nice. The only thing I've seen is Ramanujan nested radical, and it's usually presented as a funny trick, nothing more.","Is where a 'best' way to make a nested radical expansion for an analytic function? This way seems convenient: $$f(x)=a_0+a_1x+a_2x^2+a_3x^3+\dots=\sqrt{a_0^2+2a_0a_1x+(a_1^2+2a_0a_2)x^2+\cdots}=$$ $$=\sqrt{a_0^2+2a_0a_1x+(a_1^2+2a_0a_2)x^2 \sqrt{1+\cdots}}$$ I conjecture that this infinitely nested radical expansion has the same interval of convergence as the original Taylor series. Is this correct? Also, the number of 'roots' we take into account gives us twice the number plus one of the correct terms in the Taylor series. For example: $$e^x=\sqrt{1+2x+2x^2\sqrt{1+\frac{4}{3}x+\frac{10}{9}x^2\sqrt{1+\frac{32}{25}x+\frac{681}{625}x^2\sqrt{1+\dots}}}}=$$ $$=1+x+\frac{x^2}{2}+\frac{x^3}{6}+\frac{x^4}{24}+\frac{x^5}{120}+\frac{x^6}{720}+\dots$$ This expression converges for any $x$. $$\frac{1}{1-x}=\sqrt{1+2x+3x^2\sqrt{1+\frac{8}{3}x+\frac{46}{9}x^2\sqrt{1+\frac{76}{23}x+\frac{4089}{529}x^2\sqrt{1+\dots}}}}=$$ $$=1+x+x^2+x^3+x^4+x^5+x^6+\dots$$ This expression converges for $|x|<1$. This idea may seem pointless, since the coefficients in the radical expansion are very hard to calculate even for functions with 'simple' Taylor series. But is it possible, that some function with 'complicated' Taylor series will have simple nested radical expansion of this kind? For example, consider the easiest nested radical of this kind: $$f(x)=\sqrt{1+x+x^2\sqrt{1+x+x^2\sqrt{1+x+x^2\sqrt{1+\dots}}}}=$$ $$=1+\frac{x}{2}+\frac{3x^2}{8}+\frac{x^3}{16}+\frac{11x^4}{128}-\frac{9x^5}{256}+\frac{27x^6}{1024}+\dots+$$ Though this kind of functions we can always find in closed form, if we assume the infinite nested radical converges. $$f(x)=\sqrt{1+a_1x+a_2x^2\sqrt{1+a_1x+a_2x^2\sqrt{1+a_1x+a_2x^2\sqrt{1+\dots}}}}=$$ $$=\frac{a_2}{2}x^2+\sqrt{1+a_1x+\frac{a_2^2}{4}x^4}$$ I've never seen this topic discussed anywhere, so a reference would be nice. The only thing I've seen is Ramanujan nested radical, and it's usually presented as a funny trick, nothing more.",,"['calculus', 'functions', 'reference-request', 'nested-radicals']"
6,How to find where a function is increasing at the greatest rate,How to find where a function is increasing at the greatest rate,,"Given the function $f(x) = \frac{1000x^2}{11+x^2}$ on the interval $[0, 3]$, how would I calculate where the function is increasing at the greatest rate? Differentiating the function will give its slope. Since slope is defined as the rate of change, then getting the maxima of the function's derivative will indicate where it is increasing at the greatest rate. The derivative of $f(x)$ is $\frac{22000x}{(11+x^2)^2}$ Applying the first derivative test, the critical number is $\sqrt{\frac{11}{3}}$. The function increases before the critical number and decreases after it, so the critical number is a maximum. $\sqrt{\frac{11}{3}}$ is the answer.","Given the function $f(x) = \frac{1000x^2}{11+x^2}$ on the interval $[0, 3]$, how would I calculate where the function is increasing at the greatest rate? Differentiating the function will give its slope. Since slope is defined as the rate of change, then getting the maxima of the function's derivative will indicate where it is increasing at the greatest rate. The derivative of $f(x)$ is $\frac{22000x}{(11+x^2)^2}$ Applying the first derivative test, the critical number is $\sqrt{\frac{11}{3}}$. The function increases before the critical number and decreases after it, so the critical number is a maximum. $\sqrt{\frac{11}{3}}$ is the answer.",,"['calculus', 'functions', 'proof-verification']"
7,Evaluate the integral $\int \sqrt{(x-a)(b-x)}$,Evaluate the integral,\int \sqrt{(x-a)(b-x)},"I'm trying to figure out how to evaluate the following integral: $$\int \sqrt{(x-a)(b-x)} \, dx $$ I've tried various trig substitutions, but can't seem to get anywhere.  This is an exercise in Apostol's Calculus Volume 1 (Section 6.22, Exercise 46). The solution provided in the text is $$\frac{1}{4} |b-a|(b-a) \arcsin \sqrt{\frac{x-a}{b-a}} + \frac{1}{4} \sqrt{(x-a)(b-x)} (2x-(a+b)) + C.$$","I'm trying to figure out how to evaluate the following integral: $$\int \sqrt{(x-a)(b-x)} \, dx $$ I've tried various trig substitutions, but can't seem to get anywhere.  This is an exercise in Apostol's Calculus Volume 1 (Section 6.22, Exercise 46). The solution provided in the text is $$\frac{1}{4} |b-a|(b-a) \arcsin \sqrt{\frac{x-a}{b-a}} + \frac{1}{4} \sqrt{(x-a)(b-x)} (2x-(a+b)) + C.$$",,"['calculus', 'indefinite-integrals']"
8,Why is it legal to take the antiderivative of both sides of an equation?,Why is it legal to take the antiderivative of both sides of an equation?,,"first, I must apologize for somewhat misleading a title. To save both your and my time, I will go straight to the point. By definition, an indefinite integral, or a primitive, or an antiderivative of a (some condition) function $f(x)$ is any $F(x)$ such that $F'(x)=f(x)$. All well and good. Because any other primitive can be written as $F(x)+C$ for some constant $C$ (and this requires a proof), if we were to denote by $\int f(x)dx$ an antiderivative of $f(x)$, then \begin{equation} \int f(x)dx = F(x)+C. \end{equation} Fine. But here is the part that every textbook seems to have no problem with, but bugs me greatly: Often they say that integrate both sides of the following equation: \begin{equation} f(x)=g(x),\end{equation} to obtain \begin{equation} \int f(x)dx = \int g(x)dx. \end{equation} This looks like an ABSOLUTE nonsense to be for the following reason: IF both sides of the previous equation are TRULLY equal, then surely \begin{equation} \int f(x)dx - \int g(x)dx =0. \end{equation} But \begin{equation} \int f(x)dx - \int g(x)dx =\int (f(x)-g(x))dx = \int 0dx, \end{equation} which then equals $C$, any constant. Surely this is not necessarily 0! So in short, this is my question: IS IT, STRICTLY SPEAKING, LEGAL, TO TAKE THE ANTIDERIVATIVE OF BOTH SIDES OF AN EQUATION?","first, I must apologize for somewhat misleading a title. To save both your and my time, I will go straight to the point. By definition, an indefinite integral, or a primitive, or an antiderivative of a (some condition) function $f(x)$ is any $F(x)$ such that $F'(x)=f(x)$. All well and good. Because any other primitive can be written as $F(x)+C$ for some constant $C$ (and this requires a proof), if we were to denote by $\int f(x)dx$ an antiderivative of $f(x)$, then \begin{equation} \int f(x)dx = F(x)+C. \end{equation} Fine. But here is the part that every textbook seems to have no problem with, but bugs me greatly: Often they say that integrate both sides of the following equation: \begin{equation} f(x)=g(x),\end{equation} to obtain \begin{equation} \int f(x)dx = \int g(x)dx. \end{equation} This looks like an ABSOLUTE nonsense to be for the following reason: IF both sides of the previous equation are TRULLY equal, then surely \begin{equation} \int f(x)dx - \int g(x)dx =0. \end{equation} But \begin{equation} \int f(x)dx - \int g(x)dx =\int (f(x)-g(x))dx = \int 0dx, \end{equation} which then equals $C$, any constant. Surely this is not necessarily 0! So in short, this is my question: IS IT, STRICTLY SPEAKING, LEGAL, TO TAKE THE ANTIDERIVATIVE OF BOTH SIDES OF AN EQUATION?",,"['calculus', 'integration']"
9,Integral of the Laplace-Beltrami Operator multiplied by a function,Integral of the Laplace-Beltrami Operator multiplied by a function,,"I have the following problem: Let $\mathcal{M}$ be a $2D$-manifold in $\mathbb{R}^3$ and let $g$ denote its metric. Furthermore it is known that $\mathcal{M}$ is a closed manifold (i.e. it has no boundary like a sphere, torus, etc.) I believe that above information is sufficient to prove \begin{align} \int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U \rangle_g \, \mathrm{d}\mu, \end{align} for $W \in H^1(\mathcal{M})$ and $U \in C^2(\mathcal{M})$ (I need this for a finite element simulation). Here $\langle , \rangle_g$ denotes the inner product with respect to $g$. According to Wikipedia, above equality holds for all compactly supported functions $W$ and $U$ (source: https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator ). I am not quite sure whether I understand Wiki correctly. Suppose $\mathcal{M}$ is just a closed subset of the $xy$-plane, then the Beltrami operator would simply reduce to the ordinary $2D$-Laplacian and the right hand side would certainly contain an additional integral over $\partial \mathcal{M}$ (if $W$ and $U$ are nonzero there). So to me the compactness of the supports seems insufficient for above equation to hold (could somebody tell me what I am getting wrong here ?). I believe, however, that if $\mathcal{M}$ has no boundary, the boundary integral should vanish and above equation should follow. What I have done: let $S:\Omega \rightarrow \mathcal{M}$ parametrize $\mathcal{M}$, I carried out above integral in $\Omega$ utilizing $S$. Using standard calculus identities, after some steps I arrive at \begin{align} \int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = \int_{\partial \Omega} \left(w \sqrt{|g|} \nabla_\mathcal{M} u \right) \cdot \mathbf{n} \, \mathrm{d}s -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U\rangle_g \, \mathrm{d}\mu \end{align} where $w \equiv W \circ S$ and $\nabla_\mathcal{M} u \equiv \nabla_\mathcal{M} U \circ S$. I believe that the boundary integral on the right hand side should vanish for closed $\mathcal{M}$. The reason is that the local counterparts $w$ and $u$ of $W$ and $U$ satisfy certain continuity constraints across $\partial \Omega$ which translates to $w$ and $\nabla_\mathcal{M} u$ having the same value on two segments of $\partial \Omega$ but with $\mathbf{n}$ pointing in the opposite direction so that the boundary integral vanishes. Of course my explanation lacks mathematical rigor and I was wondering what the best way to prove this is. I believe that there exist proves that avoid integrating over $\Omega$ all together but I don't know where to start looking, also I don't want to read an entire book to understand the concept of wedge-products etc so if the formal proof is complicated could someone refer me to a source that I can cite that proofs exactly above statement ? Thank you in advance.","I have the following problem: Let $\mathcal{M}$ be a $2D$-manifold in $\mathbb{R}^3$ and let $g$ denote its metric. Furthermore it is known that $\mathcal{M}$ is a closed manifold (i.e. it has no boundary like a sphere, torus, etc.) I believe that above information is sufficient to prove \begin{align} \int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U \rangle_g \, \mathrm{d}\mu, \end{align} for $W \in H^1(\mathcal{M})$ and $U \in C^2(\mathcal{M})$ (I need this for a finite element simulation). Here $\langle , \rangle_g$ denotes the inner product with respect to $g$. According to Wikipedia, above equality holds for all compactly supported functions $W$ and $U$ (source: https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator ). I am not quite sure whether I understand Wiki correctly. Suppose $\mathcal{M}$ is just a closed subset of the $xy$-plane, then the Beltrami operator would simply reduce to the ordinary $2D$-Laplacian and the right hand side would certainly contain an additional integral over $\partial \mathcal{M}$ (if $W$ and $U$ are nonzero there). So to me the compactness of the supports seems insufficient for above equation to hold (could somebody tell me what I am getting wrong here ?). I believe, however, that if $\mathcal{M}$ has no boundary, the boundary integral should vanish and above equation should follow. What I have done: let $S:\Omega \rightarrow \mathcal{M}$ parametrize $\mathcal{M}$, I carried out above integral in $\Omega$ utilizing $S$. Using standard calculus identities, after some steps I arrive at \begin{align} \int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = \int_{\partial \Omega} \left(w \sqrt{|g|} \nabla_\mathcal{M} u \right) \cdot \mathbf{n} \, \mathrm{d}s -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U\rangle_g \, \mathrm{d}\mu \end{align} where $w \equiv W \circ S$ and $\nabla_\mathcal{M} u \equiv \nabla_\mathcal{M} U \circ S$. I believe that the boundary integral on the right hand side should vanish for closed $\mathcal{M}$. The reason is that the local counterparts $w$ and $u$ of $W$ and $U$ satisfy certain continuity constraints across $\partial \Omega$ which translates to $w$ and $\nabla_\mathcal{M} u$ having the same value on two segments of $\partial \Omega$ but with $\mathbf{n}$ pointing in the opposite direction so that the boundary integral vanishes. Of course my explanation lacks mathematical rigor and I was wondering what the best way to prove this is. I believe that there exist proves that avoid integrating over $\Omega$ all together but I don't know where to start looking, also I don't want to read an entire book to understand the concept of wedge-products etc so if the formal proof is complicated could someone refer me to a source that I can cite that proofs exactly above statement ? Thank you in advance.",,"['calculus', 'differential-geometry', 'manifolds', 'differential-operators']"
10,Help with verifying integral inequality.,Help with verifying integral inequality.,,"I am looking at problems from a released Fall 14 mock exam. The question in particular is number 2: Let $f$ be a continuous function in $[0,1]$ satisfying the condition: $$ \int_x^1 f(t) dt \geq \frac{1-x^2}{2}$$ for $x \in [0,1]$ Prove that: $$\int_0^1 |f(x)|^2 dx \geq \int_0^1 xf(x)dx$$ This is what I have come up with so far: First off we can ""evaluate"" the integral from $0$ to $1$ : $$\int_0^1 f(t) dt \geq \frac{1-0^2}{2} = \frac{1}{2}$$ Next we know from the Cauchy-Schwartz inequality: $$\left|\int_0^1 f(x) dx\right|^2 \leq \int_0^1 |f(x)|^2 dx$$ So: $$\int_0^1 |f(x)|^2 dx \geq \frac{1}{4}$$ Now for the other equation. I used integration by parts: $$\int_0^1 xf(x)dx = xF(x) - \int_0^1 F(x) dx $$ $$\int_0^1 xf(x) dx \geq 1 \cdot \frac{1}{2} - \int_0^1 F(x) dx $$ But notice that: $$F(x)|_0^1 \geq \frac{1}{2}$$ So: $$\int_0^1 xf(x) dx \geq \frac{1}{2} - \frac{1}{2}$$ $$\int_0^1 xf(x) dx \geq 0$$ Which seems to be right so far (I could of course be wrong). I don't have enough info to close anything out, but it seems to be pointing in the right direction. Any further hints or corrections would be greatly appreciated.","I am looking at problems from a released Fall 14 mock exam. The question in particular is number 2: Let be a continuous function in satisfying the condition: for Prove that: This is what I have come up with so far: First off we can ""evaluate"" the integral from to : Next we know from the Cauchy-Schwartz inequality: So: Now for the other equation. I used integration by parts: But notice that: So: Which seems to be right so far (I could of course be wrong). I don't have enough info to close anything out, but it seems to be pointing in the right direction. Any further hints or corrections would be greatly appreciated.","f [0,1]  \int_x^1 f(t) dt \geq \frac{1-x^2}{2} x \in [0,1] \int_0^1 |f(x)|^2 dx \geq \int_0^1 xf(x)dx 0 1 \int_0^1 f(t) dt \geq \frac{1-0^2}{2} = \frac{1}{2} \left|\int_0^1 f(x) dx\right|^2 \leq \int_0^1 |f(x)|^2 dx \int_0^1 |f(x)|^2 dx \geq \frac{1}{4} \int_0^1 xf(x)dx = xF(x) - \int_0^1 F(x) dx  \int_0^1 xf(x) dx \geq 1 \cdot \frac{1}{2} - \int_0^1 F(x) dx  F(x)|_0^1 \geq \frac{1}{2} \int_0^1 xf(x) dx \geq \frac{1}{2} - \frac{1}{2} \int_0^1 xf(x) dx \geq 0",['calculus']
11,Power series expansion of $e^{-1/x^2}$ at a point different from 0,Power series expansion of  at a point different from 0,e^{-1/x^2},The function $f(x)=e^{-1/x^2}$ ($f(0)=0$) does not have a power series expansion at $z_0=0$. Now my question: Is there a power series for $f$ centered at $z_0\neq0$ with convergence radius greater than $|z_0|$?,The function $f(x)=e^{-1/x^2}$ ($f(0)=0$) does not have a power series expansion at $z_0=0$. Now my question: Is there a power series for $f$ centered at $z_0\neq0$ with convergence radius greater than $|z_0|$?,,['calculus']
12,Why can I multiply both sides by $dx$? [duplicate],Why can I multiply both sides by ? [duplicate],dx,"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 8 years ago . When we start learning about differential equations sometimes we ""multiply"" both sides of the equation by a differential and then integrate. Example: $\frac{dy}{dx}=x$ then $dy=x*dx$ and so on. I have always thought this is a ""shortcut"" since multiplying by a differential doesn't make a lot of sense to me.  But, why does it work? What are we really doing? EDIT: I'll clarify a little more. My specific question is: Why can we treat differentials as real numbers?","This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 8 years ago . When we start learning about differential equations sometimes we ""multiply"" both sides of the equation by a differential and then integrate. Example: $\frac{dy}{dx}=x$ then $dy=x*dx$ and so on. I have always thought this is a ""shortcut"" since multiplying by a differential doesn't make a lot of sense to me.  But, why does it work? What are we really doing? EDIT: I'll clarify a little more. My specific question is: Why can we treat differentials as real numbers?",,['calculus']
13,Convergence and divergence depending on whether $n$ is odd or even,Convergence and divergence depending on whether  is odd or even,n,"It is part of one problem I am working on: I want to prove the following conjecture ($x\ne q\pi$ where $q\in\Bbb Q$) $$\sum_{n=1}^{\infty}\frac{\sin^{2k-1}(nx)}{n^{\alpha}}\quad\text{converges if}\quad0<\alpha\le1,k\in\Bbb N$$   $$\sum_{n=1}^{\infty}\frac{\sin^{2k}(nx)}{n^{\alpha}}\quad\text{diverges if}\quad0<\alpha\le1,k\in\Bbb N$$ So far I have tested cases for $\sin^{1,2,3,4}$ and the results agree with the conjecture. I believe it holds for all natural numbers. I tried using induction but I failed. Could you help me or improve my method? Best regards. My failed induction method, for the $\sin^{2k}$ cases, was as follows: Hypothesis $$\sin^{2k}\theta=\xi_k+T_k(\theta)$$   where $\xi_k\in\Bbb R^+$ (the positivity of $\xi$ is needed in the context of the bigger problem, though it is not necessary in the conjecture), $T_k(\theta)$ is a linear combination of ""first-order"" trignometric terms: $\sin\theta,\sin 2\theta,\sin 3\theta,\cdots$ or $\cos\theta,\cos2\theta,\cos3\theta\cdots$ but not $\sin^2\theta,\sin^3 2\theta,\cos^45\theta$ etc. If this hypothesis holds, then by Abel-Dirichlet criterion it is easy to prove the second part of the conjecture. Proof by induction Initial case $k=1$ is trivial. If for any given $k$ the hypothesis stands, then for $(k+1)$ $$ \begin{align*} \sin^{2k+2}\theta=&\xi_k\sin^2\theta+\sin^2\theta T_k(\theta)\\ =&\xi_k\left(\frac{1-\cos 2\theta}{2}\right)+\left(\frac{1-\cos 2\theta}{2}\right)T_k(\theta) \\ =&\frac12\xi_k-\frac12\xi_k\cos2\theta+\frac12T_k(\theta)-\frac12\cos(2\theta)T_k(\theta) \end{align*} $$   Now the hard part lies in the last term of RHS: it definitely contains terms like $\cos2\theta\sin m\theta$ or $\cos2\theta\cos m\theta$, to reduce them to the ""first order"", it must yield some constant terms, which might threaten the existence of a positive $\xi_{k+1}$. I don't know how to proceed.","It is part of one problem I am working on: I want to prove the following conjecture ($x\ne q\pi$ where $q\in\Bbb Q$) $$\sum_{n=1}^{\infty}\frac{\sin^{2k-1}(nx)}{n^{\alpha}}\quad\text{converges if}\quad0<\alpha\le1,k\in\Bbb N$$   $$\sum_{n=1}^{\infty}\frac{\sin^{2k}(nx)}{n^{\alpha}}\quad\text{diverges if}\quad0<\alpha\le1,k\in\Bbb N$$ So far I have tested cases for $\sin^{1,2,3,4}$ and the results agree with the conjecture. I believe it holds for all natural numbers. I tried using induction but I failed. Could you help me or improve my method? Best regards. My failed induction method, for the $\sin^{2k}$ cases, was as follows: Hypothesis $$\sin^{2k}\theta=\xi_k+T_k(\theta)$$   where $\xi_k\in\Bbb R^+$ (the positivity of $\xi$ is needed in the context of the bigger problem, though it is not necessary in the conjecture), $T_k(\theta)$ is a linear combination of ""first-order"" trignometric terms: $\sin\theta,\sin 2\theta,\sin 3\theta,\cdots$ or $\cos\theta,\cos2\theta,\cos3\theta\cdots$ but not $\sin^2\theta,\sin^3 2\theta,\cos^45\theta$ etc. If this hypothesis holds, then by Abel-Dirichlet criterion it is easy to prove the second part of the conjecture. Proof by induction Initial case $k=1$ is trivial. If for any given $k$ the hypothesis stands, then for $(k+1)$ $$ \begin{align*} \sin^{2k+2}\theta=&\xi_k\sin^2\theta+\sin^2\theta T_k(\theta)\\ =&\xi_k\left(\frac{1-\cos 2\theta}{2}\right)+\left(\frac{1-\cos 2\theta}{2}\right)T_k(\theta) \\ =&\frac12\xi_k-\frac12\xi_k\cos2\theta+\frac12T_k(\theta)-\frac12\cos(2\theta)T_k(\theta) \end{align*} $$   Now the hard part lies in the last term of RHS: it definitely contains terms like $\cos2\theta\sin m\theta$ or $\cos2\theta\cos m\theta$, to reduce them to the ""first order"", it must yield some constant terms, which might threaten the existence of a positive $\xi_{k+1}$. I don't know how to proceed.",,"['calculus', 'sequences-and-series', 'trigonometry', 'induction']"
14,Closed form of an infinite series of integrals $\int_{0}^{\eta} \cos nt \cos t \sqrt{\cos^2 t - \cos^2 \eta}$,Closed form of an infinite series of integrals,\int_{0}^{\eta} \cos nt \cos t \sqrt{\cos^2 t - \cos^2 \eta},"Let  $$ I(n,\eta) = \int_{0}^{\eta} \cos nt \, \cos t \, \sqrt{\cos^2 t - \cos^2 \eta}\; dt  $$ where it is known that $0 < \eta \leq \frac \pi 2$. Is it possible to evaluate $S$, the infinite sum of (even indexed) integrals of the form $I(2k,\eta)$, in closed form? $$ S(\eta) = \sum_{k=0}^{\infty} \; \frac{2k}{(2k)^2-1} \int_{0}^{\eta} \cos 2kt \, \cos t \, \sqrt{\cos^2 t - \cos^2 \eta} \; dt  $$ This integral contains terms similar to that in this question and arises in a similar context.","Let  $$ I(n,\eta) = \int_{0}^{\eta} \cos nt \, \cos t \, \sqrt{\cos^2 t - \cos^2 \eta}\; dt  $$ where it is known that $0 < \eta \leq \frac \pi 2$. Is it possible to evaluate $S$, the infinite sum of (even indexed) integrals of the form $I(2k,\eta)$, in closed form? $$ S(\eta) = \sum_{k=0}^{\infty} \; \frac{2k}{(2k)^2-1} \int_{0}^{\eta} \cos 2kt \, \cos t \, \sqrt{\cos^2 t - \cos^2 \eta} \; dt  $$ This integral contains terms similar to that in this question and arises in a similar context.",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'closed-form']"
15,Intersection of 8 spheres: find the volume,Intersection of 8 spheres: find the volume,,"From a long time ago, I remember a puzzle asking for the common area available to four cows: each cow is attached to a different corner of a square with a rope that has the same length as the sides of the square. One of several 'cow problems' and in this case, it's just the area of intersection of four unit circles, with centers on the four corners of a unit square. Through either geometry or calculus, the area is found to be $1+\pi/3-\sqrt{3}$. The higher-dimensional analogue could be to ask for the volume of intersection of eight unit spheres, with centers on the eight corners of a unit cube. You could describe it as the 'fly zone' of eight flies, attached to... etc. Geometry-wise it's not as simple to sketch/imagine the solid in question and calculus-wise, it's not that simple to set up the right integral. At least not to me, any ideas? Or is this a known problem, if someone has a reference? Addendum : and I guess you could even try generalizing this to the volume of intersection of hyperspheres on the vertices of a hypercube, but I'd already be happy with some input on the 3D-case :-).","From a long time ago, I remember a puzzle asking for the common area available to four cows: each cow is attached to a different corner of a square with a rope that has the same length as the sides of the square. One of several 'cow problems' and in this case, it's just the area of intersection of four unit circles, with centers on the four corners of a unit square. Through either geometry or calculus, the area is found to be $1+\pi/3-\sqrt{3}$. The higher-dimensional analogue could be to ask for the volume of intersection of eight unit spheres, with centers on the eight corners of a unit cube. You could describe it as the 'fly zone' of eight flies, attached to... etc. Geometry-wise it's not as simple to sketch/imagine the solid in question and calculus-wise, it's not that simple to set up the right integral. At least not to me, any ideas? Or is this a known problem, if someone has a reference? Addendum : and I guess you could even try generalizing this to the volume of intersection of hyperspheres on the vertices of a hypercube, but I'd already be happy with some input on the 3D-case :-).",,"['calculus', 'geometry', 'puzzle']"
16,"Find an appropriate trigonometric substitution of the form $x=f(t)$ to simplify the integral $ \int x\sqrt{7x^2+42x+59}\,\mathrm dx $",Find an appropriate trigonometric substitution of the form  to simplify the integral,"x=f(t)  \int x\sqrt{7x^2+42x+59}\,\mathrm dx ","Find an appropriate trigonometric substitution of the form $x=f(t)$ to simplify the integral $$ \int x\sqrt{7x^2+42x+59}\,\mathrm dx .$$ There were never any examples quite like this in class, so I'm clueless as to how to figure out which trig function to use.","Find an appropriate trigonometric substitution of the form $x=f(t)$ to simplify the integral $$ \int x\sqrt{7x^2+42x+59}\,\mathrm dx .$$ There were never any examples quite like this in class, so I'm clueless as to how to figure out which trig function to use.",,"['calculus', 'integration', 'indefinite-integrals']"
17,"Shannon Entropy, prove $H(Wx)=H(x)+\log|\det W|$","Shannon Entropy, prove",H(Wx)=H(x)+\log|\det W|,"I'm doing an essay on ICA (independent component analysis), and I could use some help. In essence, ICA is an algorithm that minimizes the entropy of $n$ $1$-dimensional random variables, but to show this, I need to prove a lemma which I can't seem to prove: if $\displaystyle H(y_1,y_2,\ldots,y_n)=-\int P(y_1,y_2,\ldots,y_n)\log(P(y_1,y_2,\ldots,y_n)) \, dy$ (where $P(y_i)$ is the density function of random variable $y_i$ and $y$ is the vector of these random variables) and if $W$ is some invertible matrix, then $H(Wx)=H(x)+\log|\det W|$. I got this exercise from the youtube lecture on the topic https://www.youtube.com/watch?v=smibJH-0YGc at around 36:50. I would greatly appreciate help proving this lemma. The main obscurity I have is that the limits are unspecified.","I'm doing an essay on ICA (independent component analysis), and I could use some help. In essence, ICA is an algorithm that minimizes the entropy of $n$ $1$-dimensional random variables, but to show this, I need to prove a lemma which I can't seem to prove: if $\displaystyle H(y_1,y_2,\ldots,y_n)=-\int P(y_1,y_2,\ldots,y_n)\log(P(y_1,y_2,\ldots,y_n)) \, dy$ (where $P(y_i)$ is the density function of random variable $y_i$ and $y$ is the vector of these random variables) and if $W$ is some invertible matrix, then $H(Wx)=H(x)+\log|\det W|$. I got this exercise from the youtube lecture on the topic https://www.youtube.com/watch?v=smibJH-0YGc at around 36:50. I would greatly appreciate help proving this lemma. The main obscurity I have is that the limits are unspecified.",,"['calculus', 'probability', 'integration', 'entropy']"
18,Calculating integral of step function,Calculating integral of step function,,"My question is from Apostol's Vol. 1: One-variable calculus with introduction to linear algebra textbook. Page 70. Exercise 10. Given a positive integer $p$ . A step function $s$ is defined on the interval $[0,p]$ as follows: $s(x)=(-1)^nn$ if $x$ lies in the interval $n\le x<n+1$ , where $n=0,1,2,\cdots,p-1;$ $s(p)=0$ . Let $f(p)=\int_0^ps(x)\mathrm dx.$ a) Calculate $f(3),f(4)$ and $f(f(3)).$ b) For what value (or values) of $p$ is $|f(p)|=7$ ? The attempt at a solution. As I understood, $s(x)=(-1)^nn$ can also be expressed as $s(x)=(-1)^{\lfloor x\rfloor}\lfloor x\rfloor$ , and graph of that step function is: Here, $$f(3)=\int_0^3((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1=1,$$ $$f(4)=\int_0^4((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1-3=-2,$$ $$f(f(3))=\int_0^1((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=0,$$ But answers in the book say that $f(4)=-1$ , where am I making mistake or what am I misunderstanding?","My question is from Apostol's Vol. 1: One-variable calculus with introduction to linear algebra textbook. Page 70. Exercise 10. Given a positive integer . A step function is defined on the interval as follows: if lies in the interval , where . Let a) Calculate and b) For what value (or values) of is ? The attempt at a solution. As I understood, can also be expressed as , and graph of that step function is: Here, But answers in the book say that , where am I making mistake or what am I misunderstanding?","p s [0,p] s(x)=(-1)^nn x n\le x<n+1 n=0,1,2,\cdots,p-1; s(p)=0 f(p)=\int_0^ps(x)\mathrm dx. f(3),f(4) f(f(3)). p |f(p)|=7 s(x)=(-1)^nn s(x)=(-1)^{\lfloor x\rfloor}\lfloor x\rfloor f(3)=\int_0^3((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1=1, f(4)=\int_0^4((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=2-1-3=-2, f(f(3))=\int_0^1((-1)^{\lfloor x\rfloor}\lfloor x\rfloor)\mathrm dx=0, f(4)=-1","['calculus', 'ceiling-and-floor-functions']"
19,Use of Poincare Lemma in solving $\nabla \times \textbf{A}(\textbf{r})=\frac{\textbf{r}}{r^3}$ UPDATED,Use of Poincare Lemma in solving  UPDATED,\nabla \times \textbf{A}(\textbf{r})=\frac{\textbf{r}}{r^3},"You are given the following statement of the Poincaré Lemma: If $\Phi_t$ is a one-parameter family of diffeomorphisms on $\mathbb R^n$ (not necessarily a subgroup) and $X_t$ the vector field defined by   $$X_t \circ \Phi_t = \frac{\partial}{\partial t}\,\Phi_t,$$   and if $\beta$ is a closed $k$-form on $\mathbb R^n$ such that   $$\Phi_1^* \beta = \beta, \quad \lim_{\epsilon \to 0} \Phi_\epsilon^* \beta = 0,$$   then $\beta = d\alpha$, where   $$\alpha=\int_0^1 \Phi_t^* i_{X_t}\beta\,dt.$$ Let $U = \mathbb{R}^3 \setminus \{(0,0,z) \}$ (i.e. $\mathbb{R}^3$ with the $z$-axis removed ) and consider $\beta$ on $U$ given by $$\beta = \frac{x \,dy \wedge dz + y \,dz \wedge dx + z \,dx \wedge dy}{(x^2+y^2+z^2)^{3/2}}$$ One can show that $d\beta=0$. Let $\Phi_t(x,y,z)=(x,y,tz)$. One can show that $\Phi_1^*\beta=\beta$, $\lim_{\epsilon \rightarrow 0} \Phi_{\epsilon}^*\beta=0$. So use the Poincaré Lemma to find a vector field $\textbf{A}(\textbf{r})$ on $U$ such that $$\nabla \times \textbf{A}(\textbf{r})=\frac{\textbf{r}}{r^3}$$ I believe that you just need to compute $\displaystyle \alpha=\int_0^1 \Phi_t^* i_{X_t}\beta \,dt$ as $\beta = d\alpha$. $$ \begin{align} \hat{\mathbb{X}}_t &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \hat{\Phi}_t^{-1} \\ &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t\right) \left(x,y,\frac{z}{t}\right) \\ &=\left(0,0,z/t\right) \end{align}$$ Now $\Phi_t \,dx =dx$, $\Phi_t\, dy =dy$ and $\Phi_t\, dz = tdz$. So $$ \begin{align} i_{\hat{X}_t}\beta &= \frac{x}{r^3}i_{\hat{X}_t}(dy \wedge dz)+\frac{y}{r^3}i_{\hat{X}_t}(dz \wedge dx)+\frac{z}{r^3}i_{\hat{X}_t}(dx \wedge dy) \\  &= \frac{x}{r^3}\left(\frac{-z}{t}dy\right)+\frac{y}{r^3}\left(\frac{z}{t}dx\right) \\  &= \frac{-zxt}{r^3}dy+\frac{zty}{r^3}dx \end{align} $$ So $$ \begin{align}\Phi_t^*i_{\hat{X}_t}\beta &= \Phi_t^* \left[\frac{-zxt}{r^3}dy+\frac{zty}{r^3}dx \right] \\  &=  \frac{-(tz)xt}{(x^2+y^2+t^2z^2)^{3/2}}dy+\frac{(tz)ty}{(x^2+y^2+t^2z^2)^{3/2}}dx \\ &= \frac{-xzt^2}{(x^2+y^2+t^2z^2)^{3/2}}dy+\frac{yzt^2}{(x^2+y^2+t^2z^2)^{3/2}}dx \end{align} $$ I am not sure that this is correct as it doesnt look very integrable.","You are given the following statement of the Poincaré Lemma: If $\Phi_t$ is a one-parameter family of diffeomorphisms on $\mathbb R^n$ (not necessarily a subgroup) and $X_t$ the vector field defined by   $$X_t \circ \Phi_t = \frac{\partial}{\partial t}\,\Phi_t,$$   and if $\beta$ is a closed $k$-form on $\mathbb R^n$ such that   $$\Phi_1^* \beta = \beta, \quad \lim_{\epsilon \to 0} \Phi_\epsilon^* \beta = 0,$$   then $\beta = d\alpha$, where   $$\alpha=\int_0^1 \Phi_t^* i_{X_t}\beta\,dt.$$ Let $U = \mathbb{R}^3 \setminus \{(0,0,z) \}$ (i.e. $\mathbb{R}^3$ with the $z$-axis removed ) and consider $\beta$ on $U$ given by $$\beta = \frac{x \,dy \wedge dz + y \,dz \wedge dx + z \,dx \wedge dy}{(x^2+y^2+z^2)^{3/2}}$$ One can show that $d\beta=0$. Let $\Phi_t(x,y,z)=(x,y,tz)$. One can show that $\Phi_1^*\beta=\beta$, $\lim_{\epsilon \rightarrow 0} \Phi_{\epsilon}^*\beta=0$. So use the Poincaré Lemma to find a vector field $\textbf{A}(\textbf{r})$ on $U$ such that $$\nabla \times \textbf{A}(\textbf{r})=\frac{\textbf{r}}{r^3}$$ I believe that you just need to compute $\displaystyle \alpha=\int_0^1 \Phi_t^* i_{X_t}\beta \,dt$ as $\beta = d\alpha$. $$ \begin{align} \hat{\mathbb{X}}_t &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \hat{\Phi}_t^{-1} \\ &= \left(\frac{\partial}{\partial t}\hat{\Phi}_t\right) \left(x,y,\frac{z}{t}\right) \\ &=\left(0,0,z/t\right) \end{align}$$ Now $\Phi_t \,dx =dx$, $\Phi_t\, dy =dy$ and $\Phi_t\, dz = tdz$. So $$ \begin{align} i_{\hat{X}_t}\beta &= \frac{x}{r^3}i_{\hat{X}_t}(dy \wedge dz)+\frac{y}{r^3}i_{\hat{X}_t}(dz \wedge dx)+\frac{z}{r^3}i_{\hat{X}_t}(dx \wedge dy) \\  &= \frac{x}{r^3}\left(\frac{-z}{t}dy\right)+\frac{y}{r^3}\left(\frac{z}{t}dx\right) \\  &= \frac{-zxt}{r^3}dy+\frac{zty}{r^3}dx \end{align} $$ So $$ \begin{align}\Phi_t^*i_{\hat{X}_t}\beta &= \Phi_t^* \left[\frac{-zxt}{r^3}dy+\frac{zty}{r^3}dx \right] \\  &=  \frac{-(tz)xt}{(x^2+y^2+t^2z^2)^{3/2}}dy+\frac{(tz)ty}{(x^2+y^2+t^2z^2)^{3/2}}dx \\ &= \frac{-xzt^2}{(x^2+y^2+t^2z^2)^{3/2}}dy+\frac{yzt^2}{(x^2+y^2+t^2z^2)^{3/2}}dx \end{align} $$ I am not sure that this is correct as it doesnt look very integrable.",,"['calculus', 'differential-geometry', 'derivatives', 'differential-forms', 'exterior-algebra']"
20,Lagrange multipliers: More than one constraint,Lagrange multipliers: More than one constraint,,"I have more or less understood the underlying theory of the Lagrange multiplier method (by using the Implicit Function Theorem). Now, I try to extend this understanding to the general case, where we have more than one constraint. For example we try to maximize/minimize $f(x)$ subject to $g(x)=0$ and $h(x)=0$. As far as I can see, what we should do in this case is simply to build the Lagrange function  $$L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x) $$ and then try to maximize/minimize this function, with respect to constraints $g(x)=0$ and $h(x)=0$. To justify this form of the Lagrange function $L(x,\alpha,\beta)$, I thought the following: Assume that we have a point $x'$ which satisfies $g(x')=0$ and $h(x')=0$. If this point is an extreme point on both constraints $g(x)$ and $h(x)$, then it is $$\nabla f(x') = \lambda_1 \nabla g(x')$$ $$\nabla f(x') = \lambda_2 \nabla h(x')$$. We can unify these in a single equation as: $$\nabla f(x') -\dfrac{\lambda_1}{2}\nabla g(x') - \dfrac{\lambda_2}{2}\nabla h(x') = \nabla f(x') + \alpha \nabla g(x') + \beta \nabla h(x')= 0$$ This partially justifies $L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x)$ for me: Calculate $\nabla_{x}L(x,\alpha,\beta)$, set it equal to zero and solve it; by using $g(x)=0$ and $h(x)=0$ as well. But what disturbs me is that we could not able to find an analytic solution to this most of the time. I have prepared a rough sketch to show it: Here, the constrained extreme points for both $g(x)=0$ and $h(x)=0$ are distinct ($x'$ and $x''$). The only points which satisfy both constraints at the same time are $A$ and $B$. And they are not the extreme points of the both constraint surfaces, it is $\nabla f(A) \neq \alpha \nabla g(A)$ for any $\alpha$ for example. So, my question is, what good is the Lagrangian function $L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x)$ in such a case? It does not provide an analytic solution for such cases, then what is the point of the Lagrange function and the coefficients $\alpha$ and $\beta$ now? Does this form constitute a good structure for numerical optimization algorithms or what? I am confused about that. Thanks in advance.","I have more or less understood the underlying theory of the Lagrange multiplier method (by using the Implicit Function Theorem). Now, I try to extend this understanding to the general case, where we have more than one constraint. For example we try to maximize/minimize $f(x)$ subject to $g(x)=0$ and $h(x)=0$. As far as I can see, what we should do in this case is simply to build the Lagrange function  $$L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x) $$ and then try to maximize/minimize this function, with respect to constraints $g(x)=0$ and $h(x)=0$. To justify this form of the Lagrange function $L(x,\alpha,\beta)$, I thought the following: Assume that we have a point $x'$ which satisfies $g(x')=0$ and $h(x')=0$. If this point is an extreme point on both constraints $g(x)$ and $h(x)$, then it is $$\nabla f(x') = \lambda_1 \nabla g(x')$$ $$\nabla f(x') = \lambda_2 \nabla h(x')$$. We can unify these in a single equation as: $$\nabla f(x') -\dfrac{\lambda_1}{2}\nabla g(x') - \dfrac{\lambda_2}{2}\nabla h(x') = \nabla f(x') + \alpha \nabla g(x') + \beta \nabla h(x')= 0$$ This partially justifies $L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x)$ for me: Calculate $\nabla_{x}L(x,\alpha,\beta)$, set it equal to zero and solve it; by using $g(x)=0$ and $h(x)=0$ as well. But what disturbs me is that we could not able to find an analytic solution to this most of the time. I have prepared a rough sketch to show it: Here, the constrained extreme points for both $g(x)=0$ and $h(x)=0$ are distinct ($x'$ and $x''$). The only points which satisfy both constraints at the same time are $A$ and $B$. And they are not the extreme points of the both constraint surfaces, it is $\nabla f(A) \neq \alpha \nabla g(A)$ for any $\alpha$ for example. So, my question is, what good is the Lagrangian function $L(x,\alpha,\beta) = f(x) + \alpha g(x) + \beta h(x)$ in such a case? It does not provide an analytic solution for such cases, then what is the point of the Lagrange function and the coefficients $\alpha$ and $\beta$ now? Does this form constitute a good structure for numerical optimization algorithms or what? I am confused about that. Thanks in advance.",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
21,Multiple integral differential notation,Multiple integral differential notation,,"When writing a multiple integral, there is sometimes used a shorthand for writing the differential in the integral. This is used particularly in physics. For example in $\mathbb{R}^3$ instead of writing $\mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z$ we sometimes find $\mathrm{d}^3 \bf{x}$ where $\bf{x} \in \mathbb{R}^3$ , or even $\mathrm{d}^3x$ . Where does this come from? I assume this is a completely separate notation from a similar one which might indicate a third order differential, e.g. in the expression $\frac{\mathrm{d}^3y}{\mathrm{d}x^3}$ , as it does not make sense to consider $\mathrm{d}^3 \bf{x}$ as something like ""the $\mathrm{d}$ operator applied three times to x "", even though it is a sort of ""third order"" object used in the multiple integral (as it is $\mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z$ ).","When writing a multiple integral, there is sometimes used a shorthand for writing the differential in the integral. This is used particularly in physics. For example in instead of writing we sometimes find where , or even . Where does this come from? I assume this is a completely separate notation from a similar one which might indicate a third order differential, e.g. in the expression , as it does not make sense to consider as something like ""the operator applied three times to x "", even though it is a sort of ""third order"" object used in the multiple integral (as it is ).",\mathbb{R}^3 \mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z \mathrm{d}^3 \bf{x} \bf{x} \in \mathbb{R}^3 \mathrm{d}^3x \frac{\mathrm{d}^3y}{\mathrm{d}x^3} \mathrm{d}^3 \bf{x} \mathrm{d} \mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z,"['calculus', 'integration', 'multivariable-calculus', 'notation', 'terminology']"
22,Triple integral over an ellipsoid,Triple integral over an ellipsoid,,"Let $E$ be the solid ellipsoid $E = ${$(x,y,z)$ | $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \le 1$} where $a > 0,\: b > 0,\: c > 0$ Evaluate $\int\int \int xyz\: dxdydz$ over: a. the whole ellipsoid b. that part of it in the first quadrant, $x \ge 0,\: y \ge 0,\: z \ge 0$ So for the first part, I made the change of variables $u = \frac{x}{a}$, $v = \frac{y}{b}$, $w = \frac{z}{c}$. The ellipse became a sphere. I then made another change of variables into spherical coordinates. I found that the integral evaluated to $$\frac{-2\pi*a^2b^2c^2}{9}$$ Now I am not sure about part b.) Since I change the ellipsoid into a unit sphere, do I just change the bounds to what they are in the first quadrant (ie, theta ranges from 0 to $\frac{\pi}{2}$ instead of 0 to $2\pi$), then integrate the same way I did in a.)? (Also, if someone could check my answer for a.) that would be great. I have an exam coming and I'm weak in this area. Thanks!)","Let $E$ be the solid ellipsoid $E = ${$(x,y,z)$ | $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \le 1$} where $a > 0,\: b > 0,\: c > 0$ Evaluate $\int\int \int xyz\: dxdydz$ over: a. the whole ellipsoid b. that part of it in the first quadrant, $x \ge 0,\: y \ge 0,\: z \ge 0$ So for the first part, I made the change of variables $u = \frac{x}{a}$, $v = \frac{y}{b}$, $w = \frac{z}{c}$. The ellipse became a sphere. I then made another change of variables into spherical coordinates. I found that the integral evaluated to $$\frac{-2\pi*a^2b^2c^2}{9}$$ Now I am not sure about part b.) Since I change the ellipsoid into a unit sphere, do I just change the bounds to what they are in the first quadrant (ie, theta ranges from 0 to $\frac{\pi}{2}$ instead of 0 to $2\pi$), then integrate the same way I did in a.)? (Also, if someone could check my answer for a.) that would be great. I have an exam coming and I'm weak in this area. Thanks!)",,"['calculus', 'integration', 'geometry', 'multivariable-calculus']"
23,Is there closed form for $\int_0^{\pi/4}\exp(-\sum_{n=1}^{\infty}\frac{\tan^{2n}x}{n+a})\ dx$?,Is there closed form for ?,\int_0^{\pi/4}\exp(-\sum_{n=1}^{\infty}\frac{\tan^{2n}x}{n+a})\ dx,"Is there closed form for  $$I(a)=\int_0^{\pi/4}\exp\left(-\sum_{n=1}^{\infty}\frac{\tan^{2n}x}{n+a}\right)dx $$where is $a\in (-1,3)$ I've tried with $\tan x=u$ and I got the result of sum in term of HurwitzLerchPhi but I failed.","Is there closed form for  $$I(a)=\int_0^{\pi/4}\exp\left(-\sum_{n=1}^{\infty}\frac{\tan^{2n}x}{n+a}\right)dx $$where is $a\in (-1,3)$ I've tried with $\tan x=u$ and I got the result of sum in term of HurwitzLerchPhi but I failed.",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
24,Symbolic manipulation inside integral,Symbolic manipulation inside integral,,"I'm an undergrad who has just completed the standard calculus sequence (1, 2, and multivariable). I've done well in the courses, however, things like the following, which is a derivation of kinetic energy, still confuse me: $$ \mathbf{F} \cdot \mathrm{d}\mathbf{x} = \mathbf{F} \cdot \mathbf{v} \mathrm{d} t = \frac{\mathrm{d}\mathbf{p}}{\mathrm{d}t} \cdot \mathbf{v} \mathrm{d} t = \mathbb{v} \cdot \mathrm{d} \mathbf{p}  = \mathbf{v} \cdot \mathrm{d}(m \mathbf{v}).$$ Taken from here. I want to understand the symbolic manipulation that often occurs when making meaningful integrations. I was taught that the ending 'dx' term simply signifies the variable to be integrated over. However, it is commonly used, for example, as a term to cancel things out. In general, I see a lot of symbolic manipulation with differential elements that I want to understand. Could you recommend something I could read to better understand this stuff? Thank you.","I'm an undergrad who has just completed the standard calculus sequence (1, 2, and multivariable). I've done well in the courses, however, things like the following, which is a derivation of kinetic energy, still confuse me: $$ \mathbf{F} \cdot \mathrm{d}\mathbf{x} = \mathbf{F} \cdot \mathbf{v} \mathrm{d} t = \frac{\mathrm{d}\mathbf{p}}{\mathrm{d}t} \cdot \mathbf{v} \mathrm{d} t = \mathbb{v} \cdot \mathrm{d} \mathbf{p}  = \mathbf{v} \cdot \mathrm{d}(m \mathbf{v}).$$ Taken from here. I want to understand the symbolic manipulation that often occurs when making meaningful integrations. I was taught that the ending 'dx' term simply signifies the variable to be integrated over. However, it is commonly used, for example, as a term to cancel things out. In general, I see a lot of symbolic manipulation with differential elements that I want to understand. Could you recommend something I could read to better understand this stuff? Thank you.",,"['calculus', 'multivariable-calculus', 'physics']"
25,"If $f\left(x-\frac{2}{x}\right) = \sqrt{x-1}$, then what is the value of $f'(1)$","If , then what is the value of",f\left(x-\frac{2}{x}\right) = \sqrt{x-1} f'(1),"Find $f'(1)$ if $$f\left(x-\frac{2}{x}\right) = \sqrt{x-1}$$ My attempt at the question: Let $(x-\dfrac{2}{x})$ be $g(x)$ Then $$f(g(x)) = \sqrt{x-1} $$ Differentiating with respect to x:  $$f'(g(x))\cdot g'(x) = \frac{1}{2\sqrt{x-1}} $$ Therefore $$f'(g(x)) = \frac{1}{2(g'(x))\sqrt{x-1}}  $$ Finding the value of $x$ for which $g(x) = 1$ : $ x=( -1) , x=2$ But as $x\neq (-1)$, as $\sqrt{x-1}$ becomes indeterminant, substitute x = 2. we get: $$f'(1) = \frac13 $$  Which is not the correct answer. The correct answer is supposedly $1$. Need some help as to why my method is wrong.","Find $f'(1)$ if $$f\left(x-\frac{2}{x}\right) = \sqrt{x-1}$$ My attempt at the question: Let $(x-\dfrac{2}{x})$ be $g(x)$ Then $$f(g(x)) = \sqrt{x-1} $$ Differentiating with respect to x:  $$f'(g(x))\cdot g'(x) = \frac{1}{2\sqrt{x-1}} $$ Therefore $$f'(g(x)) = \frac{1}{2(g'(x))\sqrt{x-1}}  $$ Finding the value of $x$ for which $g(x) = 1$ : $ x=( -1) , x=2$ But as $x\neq (-1)$, as $\sqrt{x-1}$ becomes indeterminant, substitute x = 2. we get: $$f'(1) = \frac13 $$  Which is not the correct answer. The correct answer is supposedly $1$. Need some help as to why my method is wrong.",,"['calculus', 'functions']"
26,Motivation for solving integrals of specific forms.,Motivation for solving integrals of specific forms.,,I was reading a book (Problems in One Variable Calculus by IA Maron) and there was a chapter called Integration of a Binomial Differential . I'm quoting a part of text The integral $\int x^m(a+bx^n)^p dx$ can be evaluated in the following ways- Case I. $p$ is an integer . Then if $p>0$ break the binomial up and if $p<0$ then put $x=t^k$ where k is the common denominator of the fractions $m$ and $n$ . Case II. $\frac{m+1}{n}$ is an integer. We put $a+bx^n=t^k$ where k is the denominator of the fraction p. Case III. $\frac{m+1}{n}+p$ is  an integer. We put $a+bx^n=t^kx^n$ where k is the denominator of the fraction p. I am curious about how people came up with these? What were the motivations behind it? What were they thinking when they came up with these substitutions.,I was reading a book (Problems in One Variable Calculus by IA Maron) and there was a chapter called Integration of a Binomial Differential . I'm quoting a part of text The integral can be evaluated in the following ways- Case I. is an integer . Then if break the binomial up and if then put where k is the common denominator of the fractions and . Case II. is an integer. We put where k is the denominator of the fraction p. Case III. is  an integer. We put where k is the denominator of the fraction p. I am curious about how people came up with these? What were the motivations behind it? What were they thinking when they came up with these substitutions.,\int x^m(a+bx^n)^p dx p p>0 p<0 x=t^k m n \frac{m+1}{n} a+bx^n=t^k \frac{m+1}{n}+p a+bx^n=t^kx^n,"['calculus', 'integration', 'indefinite-integrals']"
27,is there any history at all for this notation of partial anti-derivatives?,is there any history at all for this notation of partial anti-derivatives?,,"i have searched but can not find examples of any published book or online articles that use this notation: $$\int f(x,y) \partial x$$ seems it would be useful for example here:  $$\int_I\int_J f(x,y)dxdy = \int_I\color{blue}{\left(\color{black}{\int_J f(x,y)}\partial x\right)}dy$$ is there a history of such notation?  are there problems with such notation? any thoughts/help would be much appreciated thx edited-> some background for the question.. a student writes $$A=xy$$ then writes $$dA=xdy+ydx$$ then the student tries to recover the A by integrating $$\int dA=\int ydx+\int xdy$$ which 'yields' $$A=yx+c(y)+yx+c2(x)=2xy+c(y)+c2(x)$$ which is NOT the correct value of A, the teacher says $\int ydx\ne yx$ the students replies ""sometimes it is"" we routinely compute  $\int ydx=yx$  when doing the inside of a double integral...so in $\int ydx$ sometimes $y$ is held constant and sometimes not.. yet the notation is indistinguishable...","i have searched but can not find examples of any published book or online articles that use this notation: $$\int f(x,y) \partial x$$ seems it would be useful for example here:  $$\int_I\int_J f(x,y)dxdy = \int_I\color{blue}{\left(\color{black}{\int_J f(x,y)}\partial x\right)}dy$$ is there a history of such notation?  are there problems with such notation? any thoughts/help would be much appreciated thx edited-> some background for the question.. a student writes $$A=xy$$ then writes $$dA=xdy+ydx$$ then the student tries to recover the A by integrating $$\int dA=\int ydx+\int xdy$$ which 'yields' $$A=yx+c(y)+yx+c2(x)=2xy+c(y)+c2(x)$$ which is NOT the correct value of A, the teacher says $\int ydx\ne yx$ the students replies ""sometimes it is"" we routinely compute  $\int ydx=yx$  when doing the inside of a double integral...so in $\int ydx$ sometimes $y$ is held constant and sometimes not.. yet the notation is indistinguishable...",,"['calculus', 'integration', 'notation', 'partial-derivative']"
28,"convergence of $\int _1^{\infty} \sin\big(\mathrm{e}^x(x-2)\big)\,dx$",convergence of,"\int _1^{\infty} \sin\big(\mathrm{e}^x(x-2)\big)\,dx","Question: $$\int _1^{\infty} \sin\big(\mathrm{e}^x(x-2)\big)\,dx$$ does this converge? Wolfram|Alpha doesn't have an answer, and I would really know. We tried to use Dirichlet and substituting with $t=e^x$. But couldn't continue","Question: $$\int _1^{\infty} \sin\big(\mathrm{e}^x(x-2)\big)\,dx$$ does this converge? Wolfram|Alpha doesn't have an answer, and I would really know. We tried to use Dirichlet and substituting with $t=e^x$. But couldn't continue",,"['calculus', 'improper-integrals']"
29,"the word ""derivative""","the word ""derivative""",,"When did the word ""derivative"" come into use in calculus, and why? As in Can the word ""derive"" be used to mean ""take the derivative of""? the word ""derivative"" in normal English means ""stemming from"". But $\int f$ ""derives"" from $f$ just as much as does $f'$, and $f'$ ""integrates"" information from $f$ just as much as does $f'$. So who decided that a ratio of fluxions should be called the derivative, and why?","When did the word ""derivative"" come into use in calculus, and why? As in Can the word ""derive"" be used to mean ""take the derivative of""? the word ""derivative"" in normal English means ""stemming from"". But $\int f$ ""derives"" from $f$ just as much as does $f'$, and $f'$ ""integrates"" information from $f$ just as much as does $f'$. So who decided that a ratio of fluxions should be called the derivative, and why?",,"['calculus', 'terminology', 'math-history']"
30,Simplify $\sum_{n=0}^{N}\binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x)$,Simplify,\sum_{n=0}^{N}\binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x),"Simplify the following expression $$S_N = \sum_{n=0}^{N}\binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x), $$ where $a$ is a real number and $f(x)$ is an analytic real function. What is $\lim_n S_n$? Is there a simple intuitive meaning to it? I only found formulas which come close but not quite what I need: Without the $n!$ in the denominator, the expression can be simpified using the Leibniz rule $$ \sum_{n=0}^{N}\binom{N}{n} a^{N-n} \frac{d^n}{dx^n} f(x) =e^{-ax}\frac{d^N}{dx^N}\left[ e^{ax} f(x)\right].$$ Without the binomial coefficient the Taylor series expansion of $f(x)$ around $x$ gives $$\lim_{n\rightarrow\infty}\sum_{n=0}^{N} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x) =a^N f\left(x+a^{-1}\right).$$ EDIT : With an extra set of parentheses $$\left[\sum_{n=0}^N \binom{N}{n}a^{N-n}\right]\left[\sum_{n=0}^{\infty}\frac{1}{n!} \frac{d^n}{dx^n}f(x)  \right] = (1+a)^N f(x+1) $$ On the other hand $$ \lim_{z\rightarrow 0}\frac{1}{N!}\frac{d^N}{dz^N}\sum_n A_n z^n \sum_m B_m z^m =  \lim_{z\rightarrow 0}\frac{1}{N!}\frac{d^N}{dz^N} \sum_{N} \sum_{n} A_{N-n}B_n z^N  = \sum_n A_{N-n}B_n$$ so combining the above two  $$\sum_n \binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x) = \lim_{z\rightarrow 0} \frac{1}{N!}\frac{d^N}{dz^N}\left[ (1+az)^N f(x+z)\right] $$ Is this right? Is there a simpler expression?","Simplify the following expression $$S_N = \sum_{n=0}^{N}\binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x), $$ where $a$ is a real number and $f(x)$ is an analytic real function. What is $\lim_n S_n$? Is there a simple intuitive meaning to it? I only found formulas which come close but not quite what I need: Without the $n!$ in the denominator, the expression can be simpified using the Leibniz rule $$ \sum_{n=0}^{N}\binom{N}{n} a^{N-n} \frac{d^n}{dx^n} f(x) =e^{-ax}\frac{d^N}{dx^N}\left[ e^{ax} f(x)\right].$$ Without the binomial coefficient the Taylor series expansion of $f(x)$ around $x$ gives $$\lim_{n\rightarrow\infty}\sum_{n=0}^{N} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x) =a^N f\left(x+a^{-1}\right).$$ EDIT : With an extra set of parentheses $$\left[\sum_{n=0}^N \binom{N}{n}a^{N-n}\right]\left[\sum_{n=0}^{\infty}\frac{1}{n!} \frac{d^n}{dx^n}f(x)  \right] = (1+a)^N f(x+1) $$ On the other hand $$ \lim_{z\rightarrow 0}\frac{1}{N!}\frac{d^N}{dz^N}\sum_n A_n z^n \sum_m B_m z^m =  \lim_{z\rightarrow 0}\frac{1}{N!}\frac{d^N}{dz^N} \sum_{N} \sum_{n} A_{N-n}B_n z^N  = \sum_n A_{N-n}B_n$$ so combining the above two  $$\sum_n \binom{N}{n} \frac{a^{N-n}}{n!} \frac{d^n}{dx^n} f(x) = \lim_{z\rightarrow 0} \frac{1}{N!}\frac{d^N}{dz^N}\left[ (1+az)^N f(x+z)\right] $$ Is this right? Is there a simpler expression?",,"['calculus', 'limits', 'derivatives', 'summation', 'power-series']"
31,Parity through Series expansion,Parity through Series expansion,,"By Maclaurin series; $\sin{x}=x-\frac{x^3}{3!}+\frac{x^5}{5!}-....$, and we know that period of $\sin{x}$ is $2π$ because $\sin{(x+2π)} = \sin{x}$. But if we consider only the RHS of the above series then how we can tell that this expression (series) is also of period$=2π$?","By Maclaurin series; $\sin{x}=x-\frac{x^3}{3!}+\frac{x^5}{5!}-....$, and we know that period of $\sin{x}$ is $2π$ because $\sin{(x+2π)} = \sin{x}$. But if we consider only the RHS of the above series then how we can tell that this expression (series) is also of period$=2π$?",,['calculus']
32,Volume of ellipsoid bounded by two planes.,Volume of ellipsoid bounded by two planes.,,"I need to find the volume of ellipsoid: $$5x^2 + {y^2\over25} + {3z^2\over4} = 1$$ if the ellipsoid is bounded by $z={-1\over2}$ and $z=1$ planes. I was able to find the total volume of the ellipsoid using the formula $V={4\pi\over3}*abc={40\pi\over3\sqrt15}$. But I don't think i can use this in any way to find the volume of the ellipsoid that is bounded by 2 planes. To find the actual volume, I'm pretty sure I need to solve this: $V=\int_{-1\over2}^1S(x)dx$, where $S(x)$ is the area of the cross section of the ellipsoid, which is an ellipse. Now I think that the right move here would be to get the cross sections that are parallel to the $z$-axis. However my question is, how can I find these areas of the ellipses to plug into the above formula? Any suggestions, would be greatly appreciated!","I need to find the volume of ellipsoid: $$5x^2 + {y^2\over25} + {3z^2\over4} = 1$$ if the ellipsoid is bounded by $z={-1\over2}$ and $z=1$ planes. I was able to find the total volume of the ellipsoid using the formula $V={4\pi\over3}*abc={40\pi\over3\sqrt15}$. But I don't think i can use this in any way to find the volume of the ellipsoid that is bounded by 2 planes. To find the actual volume, I'm pretty sure I need to solve this: $V=\int_{-1\over2}^1S(x)dx$, where $S(x)$ is the area of the cross section of the ellipsoid, which is an ellipse. Now I think that the right move here would be to get the cross sections that are parallel to the $z$-axis. However my question is, how can I find these areas of the ellipses to plug into the above formula? Any suggestions, would be greatly appreciated!",,"['calculus', 'conic-sections', 'volume']"
33,How find this $g(x)=f(x)f(1-x)$ maximum and minimum,How find this  maximum and minimum,g(x)=f(x)f(1-x),"Question: Let  $f: \mathbb{R}\to\mathbb{R}$ is a function such that $$f( \cot x ) = \cos 2x+\sin 2x $$ for all $0 < x < \pi$. Define $$g(x) = f(x) f(1-x) , -1 \leq x \leq 1$$ Find the maximum and minimum values of $g$ on the closed interval $[-1, 1].$ My try: since  $$\sin{2x}=\dfrac{2\tan{x}}{1+\tan^2{x}},\cos{2x}=\dfrac{1-\tan^2{x}}{1+\tan^2{x}}$$so $$\sin{(2x)}+\cos{(2x)}=\dfrac{1-\tan^2{x}+2\tan{x}}{1+\tan^2{x}}$$ so $$f(x)=\dfrac{x^2+2x-1}{x^2+1}$$ then $$g(x)=f(x)f(1-x)=\dfrac{x^2+2x-1}{x^2+1}\cdot\dfrac{(1-x)^2+2(1-x)-1}{(1-x)^2+1},-1\le x\le 1$$ Then I can't.Thank you for your help","Question: Let  $f: \mathbb{R}\to\mathbb{R}$ is a function such that $$f( \cot x ) = \cos 2x+\sin 2x $$ for all $0 < x < \pi$. Define $$g(x) = f(x) f(1-x) , -1 \leq x \leq 1$$ Find the maximum and minimum values of $g$ on the closed interval $[-1, 1].$ My try: since  $$\sin{2x}=\dfrac{2\tan{x}}{1+\tan^2{x}},\cos{2x}=\dfrac{1-\tan^2{x}}{1+\tan^2{x}}$$so $$\sin{(2x)}+\cos{(2x)}=\dfrac{1-\tan^2{x}+2\tan{x}}{1+\tan^2{x}}$$ so $$f(x)=\dfrac{x^2+2x-1}{x^2+1}$$ then $$g(x)=f(x)f(1-x)=\dfrac{x^2+2x-1}{x^2+1}\cdot\dfrac{(1-x)^2+2(1-x)-1}{(1-x)^2+1},-1\le x\le 1$$ Then I can't.Thank you for your help",,['calculus']
34,Mathematics lessons online?,Mathematics lessons online?,,"IN school, I did not really care about my future, until I started working with computers, and getting into programming. I'm doing computer science  at the moment & its currently really easy, but I heard that to do a degree in Computer science in the uni, I need a specific level of mathematics class. Currently my math class is LOW, I can say. I can get a private teacher, but I want to try and learn something myself a little before getting one. What are your suggestions? where can I learn mathematics such as caculus, online? Thank you a lot.","IN school, I did not really care about my future, until I started working with computers, and getting into programming. I'm doing computer science  at the moment & its currently really easy, but I heard that to do a degree in Computer science in the uni, I need a specific level of mathematics class. Currently my math class is LOW, I can say. I can get a private teacher, but I want to try and learn something myself a little before getting one. What are your suggestions? where can I learn mathematics such as caculus, online? Thank you a lot.",,"['calculus', 'self-learning']"
35,Calculus: The tangent line intersects a curve at two points. Find the other point.,Calculus: The tangent line intersects a curve at two points. Find the other point.,,"The line tangent to $y = -x^3 + 2x + 1$ when $x = 1$ intersects the curve in another point.  Find the coordinates of the other point. This was never taught in class, and I have a test on this tomorrow.  This question came off of my test review worksheet, and I don't understand how to solve it.  The answers are on the back, and for this one it says the answer is (-2,5), but I don't understand how to get that. I did the derivative and substituted 1 for x to get the slope of the line: $y = -x^3 + 2x + 1$ $y' = -3x^2 + 2$ $y' = -3 + 2$ $y' = -1$ I don't know where to go from here.","The line tangent to when intersects the curve in another point.  Find the coordinates of the other point. This was never taught in class, and I have a test on this tomorrow.  This question came off of my test review worksheet, and I don't understand how to solve it.  The answers are on the back, and for this one it says the answer is (-2,5), but I don't understand how to get that. I did the derivative and substituted 1 for x to get the slope of the line: I don't know where to go from here.",y = -x^3 + 2x + 1 x = 1 y = -x^3 + 2x + 1 y' = -3x^2 + 2 y' = -3 + 2 y' = -1,['calculus']
36,please check my proof and comment it,please check my proof and comment it,,"Let $\left(\frac1{(2k+1)!}\right)$ be an infinite sequence. I want to show the following limit $\lim \limits_{k \to \infty}{\frac{1}{(2k+1)!}}=0.$ Below is my proof. Please check it, I'm not confident about my ability. For all nonnegative integer of k, we have $0<(2k+1)\le(2k+1)!$ $\frac{1}{2k+1}\ge\frac{1}{(2k+1)!}>0.$ As $k\to\infty, \frac{1}{2k+1}\to0.$ Thus, as $k\to\infty$, $\frac1{(2k+1)!}\to0$","Let $\left(\frac1{(2k+1)!}\right)$ be an infinite sequence. I want to show the following limit $\lim \limits_{k \to \infty}{\frac{1}{(2k+1)!}}=0.$ Below is my proof. Please check it, I'm not confident about my ability. For all nonnegative integer of k, we have $0<(2k+1)\le(2k+1)!$ $\frac{1}{2k+1}\ge\frac{1}{(2k+1)!}>0.$ As $k\to\infty, \frac{1}{2k+1}\to0.$ Thus, as $k\to\infty$, $\frac1{(2k+1)!}\to0$",,['calculus']
37,Approximations of fixed points of tangent.,Approximations of fixed points of tangent.,,"This question comes from an exam, years ago. Show that $f(x)=\tan x-x$, for every positive integer $n$, has exactly one root $x_n$ in the interval $(n\pi,n\pi+\pi/2)$. And show that $$x_n=n\pi+\frac{\pi}{2}-\frac{1}{n\pi}+\text{o}\left(\frac{1}{n}\right).$$ I can prove the claim about the existence of $x_n$, by intermidiate value-theorem, but am stuck by the second point. Since $\tan x$ is not a contraction, we cannot apply the contraction mapping theorem here. Also, using Taylor approximation, or the Lagrange form of the remainder, I arrived at $$\tan x=(x-n\pi)+\frac{f^{(3)}(\theta)}{6}x^3$$ for some $\theta$ between $x$ and $n\pi$.  It seems that, however, I can only obtain information about this $\theta$ along this direction, not about $x_n$. Any hint or help is greatly appreciated. Thanks in advance.","This question comes from an exam, years ago. Show that $f(x)=\tan x-x$, for every positive integer $n$, has exactly one root $x_n$ in the interval $(n\pi,n\pi+\pi/2)$. And show that $$x_n=n\pi+\frac{\pi}{2}-\frac{1}{n\pi}+\text{o}\left(\frac{1}{n}\right).$$ I can prove the claim about the existence of $x_n$, by intermidiate value-theorem, but am stuck by the second point. Since $\tan x$ is not a contraction, we cannot apply the contraction mapping theorem here. Also, using Taylor approximation, or the Lagrange form of the remainder, I arrived at $$\tan x=(x-n\pi)+\frac{f^{(3)}(\theta)}{6}x^3$$ for some $\theta$ between $x$ and $n\pi$.  It seems that, however, I can only obtain information about this $\theta$ along this direction, not about $x_n$. Any hint or help is greatly appreciated. Thanks in advance.",,"['calculus', 'approximation']"
38,A question on differentiation :,A question on differentiation :,,"Let $$f(x)=\sin^{-1}(2x\sqrt{1-x^2})$$ I found out $f'(x)$ in three methods and got three different answers ! 1) Putting $x=\cos\theta$, we get $f(x)=2\cos^{-1}x$, on differentiating this we get $$f'(x)=\frac{-2}{\sqrt{1-x^2}}$$ 2) Putting $x=\sin\theta$, we get $f(x)=2\sin^{-1}x$, on differentiating we get  $$f'(x)=\frac{2}{\sqrt{1-x^2}}$$ This is contradicting with the previous result that we arrived at by substituting $x=\cos\theta$. Can someone help me out with the correct solution for this problem!","Let $$f(x)=\sin^{-1}(2x\sqrt{1-x^2})$$ I found out $f'(x)$ in three methods and got three different answers ! 1) Putting $x=\cos\theta$, we get $f(x)=2\cos^{-1}x$, on differentiating this we get $$f'(x)=\frac{-2}{\sqrt{1-x^2}}$$ 2) Putting $x=\sin\theta$, we get $f(x)=2\sin^{-1}x$, on differentiating we get  $$f'(x)=\frac{2}{\sqrt{1-x^2}}$$ This is contradicting with the previous result that we arrived at by substituting $x=\cos\theta$. Can someone help me out with the correct solution for this problem!",,['calculus']
39,"Unforeseen issue in my MastersThesis: Is there a ""closed form"" solution?","Unforeseen issue in my MastersThesis: Is there a ""closed form"" solution?",,"I'm an electrical engineer and I recently came across an unforeseen issue in my masters thesis because I lack a deeper mathematical education. I want to know for which positive real $x$ the following inequality is an equality: $$ n \log(1 + \tfrac{x}{n}) - \log(1 + x) \leq a\, , \;\;\;\; (\ast) $$ i.e. $$ n \log(1 + \tfrac{x}{n}) - \log(1 + x) - a = 0\, , $$ where $x \in \mathbb{R} \geq 0$, $n\in \mathbb{N}\gg 1$ and $a \in \mathbb{R} > 0$. This is equivalent to $$ (1+\tfrac{x}{n})^n = \mathrm{e}^a\cdot(1 + x) \, $$ which looks basically not so hard. My questions is: Is there a ""closed form"" solution of this equation for the unknown $x$ and I am just too dumb to get it? By closed form solution I mean any solution that I can nicely write like  $$ x \leq \ldots $$ to solve $(\ast)$. If there is no ""closed form"" solution, I would be interested why and how I could have seen this. Unfortunately I'm not really familiar enough with Transcendence Theory or Galois Theory to see this on my own. Thanks! Assuming that $(\ast)$ has no ""closed form"" solution, I also thought about a workaround . Since $\lim\limits_{n \rightarrow \infty} (1 + \tfrac{x}{n})^n = \mathrm{e}^x$, I could write write $$ \mathrm{e}^x - \mathrm{e}^a\cdot(1 + x) = 0 \,  $$ for the case that $n \rightarrow \infty$. I thought about this as an approximation for finite $n$. Unfortunately for finite $n$, $(1 + \tfrac{x}{n})^n < \mathrm{e}^x$ yields and this approximation would clash my basic inequality $(\ast)$. If I could show that $$ \lim\limits_{n \rightarrow \infty} (1 + \tfrac{x}{n})^{n+2} = \mathrm{e}^x $$ and $$ (1 + \tfrac{x}{n})^{n+2} > \mathrm{e}^x $$ for finite $n$, it might be possible to solve  $$ \mathrm{e}^x\cdot(1 + \tfrac{x}{n})^{-2} =  \mathrm{e}^a\cdot(1 + x) $$ for an approximate solution of $(\ast)$. Here, the Lambert W function might be helpful but I didn't succeed on this problem so far as well. Any thoughts on this? Thanks a lot!","I'm an electrical engineer and I recently came across an unforeseen issue in my masters thesis because I lack a deeper mathematical education. I want to know for which positive real $x$ the following inequality is an equality: $$ n \log(1 + \tfrac{x}{n}) - \log(1 + x) \leq a\, , \;\;\;\; (\ast) $$ i.e. $$ n \log(1 + \tfrac{x}{n}) - \log(1 + x) - a = 0\, , $$ where $x \in \mathbb{R} \geq 0$, $n\in \mathbb{N}\gg 1$ and $a \in \mathbb{R} > 0$. This is equivalent to $$ (1+\tfrac{x}{n})^n = \mathrm{e}^a\cdot(1 + x) \, $$ which looks basically not so hard. My questions is: Is there a ""closed form"" solution of this equation for the unknown $x$ and I am just too dumb to get it? By closed form solution I mean any solution that I can nicely write like  $$ x \leq \ldots $$ to solve $(\ast)$. If there is no ""closed form"" solution, I would be interested why and how I could have seen this. Unfortunately I'm not really familiar enough with Transcendence Theory or Galois Theory to see this on my own. Thanks! Assuming that $(\ast)$ has no ""closed form"" solution, I also thought about a workaround . Since $\lim\limits_{n \rightarrow \infty} (1 + \tfrac{x}{n})^n = \mathrm{e}^x$, I could write write $$ \mathrm{e}^x - \mathrm{e}^a\cdot(1 + x) = 0 \,  $$ for the case that $n \rightarrow \infty$. I thought about this as an approximation for finite $n$. Unfortunately for finite $n$, $(1 + \tfrac{x}{n})^n < \mathrm{e}^x$ yields and this approximation would clash my basic inequality $(\ast)$. If I could show that $$ \lim\limits_{n \rightarrow \infty} (1 + \tfrac{x}{n})^{n+2} = \mathrm{e}^x $$ and $$ (1 + \tfrac{x}{n})^{n+2} > \mathrm{e}^x $$ for finite $n$, it might be possible to solve  $$ \mathrm{e}^x\cdot(1 + \tfrac{x}{n})^{-2} =  \mathrm{e}^a\cdot(1 + x) $$ for an approximate solution of $(\ast)$. Here, the Lambert W function might be helpful but I didn't succeed on this problem so far as well. Any thoughts on this? Thanks a lot!",,"['calculus', 'special-functions', 'closed-form']"
40,Find the value of a function with definite integrals,Find the value of a function with definite integrals,,"I am trying to understand a paper of Maynard Smith (1974) , that connects biology with game theory. I don't want to overwhelm you with useless stuff, but I have this definite integrals: $$E(m)=\int_0^m (v-x)p(x) dx - \int_m^\infty mp(x) dx  \tag{1}$$ We want to choose $p(x)$ such that $E(m)$ is the same constant $C$ for all $m$. Now I'll copy exactly what he (Maynard Smith) says: To find $p(x)$ we put $E(m) = E(m+\Delta m)$, so that $$E(m)=\int_0^m (v-x)p(x) dx -\int_m^\infty mp(x) dx=\int_0^{m+\Delta m}(v-x)p(x)dx - \int_{m+\Delta m}^\infty( m+\Delta m) p(x) dx  \tag{2}$$ After a little manipulation, remembering that   $E(m)=\int_0^\infty p(x) dx =1$ this gives $$ vp(m)= 1-\int_0^m p(x) dx  \tag{3}$$ Equation $(3)$ is satisfied by the function    $$ p(x) = (1/v ) e^{-x/v}  \tag{4}$$ which is the equilibrium strategy we are seeking. Since I am not a mathematician (but I have some knowledge of Calculus and Probability) I am having an hard time understanding this. I have no idea how we can go from $(2)$ to $(3)$ and from $(3)$ to $(4)$. Could anyone help?  Thank you.","I am trying to understand a paper of Maynard Smith (1974) , that connects biology with game theory. I don't want to overwhelm you with useless stuff, but I have this definite integrals: $$E(m)=\int_0^m (v-x)p(x) dx - \int_m^\infty mp(x) dx  \tag{1}$$ We want to choose $p(x)$ such that $E(m)$ is the same constant $C$ for all $m$. Now I'll copy exactly what he (Maynard Smith) says: To find $p(x)$ we put $E(m) = E(m+\Delta m)$, so that $$E(m)=\int_0^m (v-x)p(x) dx -\int_m^\infty mp(x) dx=\int_0^{m+\Delta m}(v-x)p(x)dx - \int_{m+\Delta m}^\infty( m+\Delta m) p(x) dx  \tag{2}$$ After a little manipulation, remembering that   $E(m)=\int_0^\infty p(x) dx =1$ this gives $$ vp(m)= 1-\int_0^m p(x) dx  \tag{3}$$ Equation $(3)$ is satisfied by the function    $$ p(x) = (1/v ) e^{-x/v}  \tag{4}$$ which is the equilibrium strategy we are seeking. Since I am not a mathematician (but I have some knowledge of Calculus and Probability) I am having an hard time understanding this. I have no idea how we can go from $(2)$ to $(3)$ and from $(3)$ to $(4)$. Could anyone help?  Thank you.",,"['calculus', 'probability', 'game-theory', 'biology']"
41,Integration question,Integration question,,"I have trouble in integrating the following integral. I would appreciate any help :D $$\int_0^1 \sqrt{-\log x}\, a\, x^{a-1}dx$$ Thanks heaps :D The answer is $\sqrt{\pi}/2(\sqrt{a})$.","I have trouble in integrating the following integral. I would appreciate any help :D $$\int_0^1 \sqrt{-\log x}\, a\, x^{a-1}dx$$ Thanks heaps :D The answer is $\sqrt{\pi}/2(\sqrt{a})$.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
42,showing zero curvature implies a line,showing zero curvature implies a line,,"How can I show that a given (not necessarily unit-speed) parametrization $\gamma(t)$ of a curve in $\mathbb{R}^3$ which exhibits zero curvature is a line ? What I know is that  zero curvature means that $$ \kappa(t) = \frac{\|\langle\dot\gamma(t),\dot\gamma(t)\rangle \ddot\gamma(t) - \langle\dot \gamma(t),\ddot \gamma(t)\rangle\dot \gamma(t)\|}{\|\dot \gamma(t)\|^4} = 0 $$ from which I can deduce that $$ \langle\dot\gamma(t),\dot\gamma(t)\rangle \ddot\gamma(t) = \langle\dot \gamma(t),\ddot \gamma(t)\rangle\dot \gamma(t) $$ that is, $$  \ddot\gamma(t) = \frac{\langle\dot \gamma(t),\ddot \gamma(t)\rangle}{\|\dot\gamma(t)\|^2} \dot \gamma(t) \qquad \text{for all } t\,. $$ Somehow I am blind here - how does this tell me then that $\ddot \gamma(t)$ vanishes identically? For this is what I need to deduce that $\gamma(t)$ is a line .. Many thanks for your hints!","How can I show that a given (not necessarily unit-speed) parametrization $\gamma(t)$ of a curve in $\mathbb{R}^3$ which exhibits zero curvature is a line ? What I know is that  zero curvature means that $$ \kappa(t) = \frac{\|\langle\dot\gamma(t),\dot\gamma(t)\rangle \ddot\gamma(t) - \langle\dot \gamma(t),\ddot \gamma(t)\rangle\dot \gamma(t)\|}{\|\dot \gamma(t)\|^4} = 0 $$ from which I can deduce that $$ \langle\dot\gamma(t),\dot\gamma(t)\rangle \ddot\gamma(t) = \langle\dot \gamma(t),\ddot \gamma(t)\rangle\dot \gamma(t) $$ that is, $$  \ddot\gamma(t) = \frac{\langle\dot \gamma(t),\ddot \gamma(t)\rangle}{\|\dot\gamma(t)\|^2} \dot \gamma(t) \qquad \text{for all } t\,. $$ Somehow I am blind here - how does this tell me then that $\ddot \gamma(t)$ vanishes identically? For this is what I need to deduce that $\gamma(t)$ is a line .. Many thanks for your hints!",,"['calculus', 'differential-geometry', 'riemannian-geometry']"
43,Prove that $\frac{n-1}{n}>\frac{2a_0a_2}{a_1^2}$,Prove that,\frac{n-1}{n}>\frac{2a_0a_2}{a_1^2},Given that the following equation $$p(x)=a_0x^n+a_1x^{n-1}+...+a_{n-1}x+a_n=0$$ has $n$ distinct real roots. Prove that $$\frac{n-1}{n}>\frac{2a_0a_2}{a_1^2}$$,Given that the following equation $$p(x)=a_0x^n+a_1x^{n-1}+...+a_{n-1}x+a_n=0$$ has $n$ distinct real roots. Prove that $$\frac{n-1}{n}>\frac{2a_0a_2}{a_1^2}$$,,"['calculus', 'polynomials']"
44,Arc length formula for the lemniscate,Arc length formula for the lemniscate,,"This question can be homework for elementary calculus. The lemniscate of Bernoulli $C$ is a plane curve defined as follows. Let $a > 0$ be a real number. Let $F_1 = (a, 0)$ and $F_2 = (-a, 0)$ be two points of $\mathbb{R}^2$. Let $C = \{P \in \mathbb{R}^2; PF_1\cdot PF_2 = a^2\}$. Let's get the equation of $C$ in the polar coordinates. Let $P = (r\cos\theta, r\sin\theta)$: $$PF_1^2 = r^2 + a^2 - 2ar\cos\theta, PF_2^2 = r^2 + a^2 + 2ar\cos\theta$$ Hence: $$(r^2 + a^2 - 2ar\cos\theta)(r^2 + a^2 + 2ar\cos\theta) = (r^2 + a^2)^2 - 4a^2r^2\cos^2\theta = a^4$$ $$r^4 + 2r^2a^2 + a^4 - 4a^2r^2\cos^2\theta = a^4$$ $$r^2 = 2a^2(2\cos^2\theta - 1) = 2a^2\cos 2\theta$$ Suppose $P \in C$ is in the first quadrant.  Let $s$ be the arc length between $O = (0, 0)$ and $P$. Then how can we express $s$ by $r$ using an integral? This is a related question .","This question can be homework for elementary calculus. The lemniscate of Bernoulli $C$ is a plane curve defined as follows. Let $a > 0$ be a real number. Let $F_1 = (a, 0)$ and $F_2 = (-a, 0)$ be two points of $\mathbb{R}^2$. Let $C = \{P \in \mathbb{R}^2; PF_1\cdot PF_2 = a^2\}$. Let's get the equation of $C$ in the polar coordinates. Let $P = (r\cos\theta, r\sin\theta)$: $$PF_1^2 = r^2 + a^2 - 2ar\cos\theta, PF_2^2 = r^2 + a^2 + 2ar\cos\theta$$ Hence: $$(r^2 + a^2 - 2ar\cos\theta)(r^2 + a^2 + 2ar\cos\theta) = (r^2 + a^2)^2 - 4a^2r^2\cos^2\theta = a^4$$ $$r^4 + 2r^2a^2 + a^4 - 4a^2r^2\cos^2\theta = a^4$$ $$r^2 = 2a^2(2\cos^2\theta - 1) = 2a^2\cos 2\theta$$ Suppose $P \in C$ is in the first quadrant.  Let $s$ be the arc length between $O = (0, 0)$ and $P$. Then how can we express $s$ by $r$ using an integral? This is a related question .",,"['calculus', 'plane-curves']"
45,limit of derivatives of a function,limit of derivatives of a function,,"I wanna show that for $f:(0,\infty)\rightarrow\mathbb R, x\mapsto\exp(-\frac1{x^2})$ the sum of the derivates of $f$, so $\sum\limits_{n=0}^\infty f^{(n)}(x)$, converges to $0$, so $$\lim\limits_{x\rightarrow0}\sum\limits_{n=0}^\infty f^{(n)}(x)=0$$ It's intuitively clear but I have some issues on writting it down. Can anybody help? Thanks!","I wanna show that for $f:(0,\infty)\rightarrow\mathbb R, x\mapsto\exp(-\frac1{x^2})$ the sum of the derivates of $f$, so $\sum\limits_{n=0}^\infty f^{(n)}(x)$, converges to $0$, so $$\lim\limits_{x\rightarrow0}\sum\limits_{n=0}^\infty f^{(n)}(x)=0$$ It's intuitively clear but I have some issues on writting it down. Can anybody help? Thanks!",,"['calculus', 'analysis']"
46,Question about singularities and path integrals,Question about singularities and path integrals,,"I'm given a vector field that has an obvious singularity at a point $(a,b)$.  In order to learn more about the singularity I place a circle around it with the singularity at it's center.  The line integral for the field across the circle gives me $18\pi \,r$.  What conclusion can I get from the solution to the line integral?","I'm given a vector field that has an obvious singularity at a point $(a,b)$.  In order to learn more about the singularity I place a circle around it with the singularity at it's center.  The line integral for the field across the circle gives me $18\pi \,r$.  What conclusion can I get from the solution to the line integral?",,['calculus']
47,Book advice to brush up on Calculus and PDEs,Book advice to brush up on Calculus and PDEs,,"I just graduated from an engineering school and am about to begin classes as a PhD candidate.  As I review my undergraduate classes and sift through old notes, I find myself wanting for more math knowledge.  While I did well in math classes, I always felt as though I was following steps rather than gaining a depth of understanding and intuition.  To get to the point: Can you recommend any books to brush up on and perhaps gain a more advanced understanding of calculus and PDEs? As an undergraduate, I used Salas Hille and Etgen, which I felt was great at teaching rote steps to solve problems but lacked some depth. Thanks in advance! (Questions regarding a reasonable self-guided book have very frequently been brought up on this forum.  However, I like to think this is unique and apologize if this feels ""repeated."")","I just graduated from an engineering school and am about to begin classes as a PhD candidate.  As I review my undergraduate classes and sift through old notes, I find myself wanting for more math knowledge.  While I did well in math classes, I always felt as though I was following steps rather than gaining a depth of understanding and intuition.  To get to the point: Can you recommend any books to brush up on and perhaps gain a more advanced understanding of calculus and PDEs? As an undergraduate, I used Salas Hille and Etgen, which I felt was great at teaching rote steps to solve problems but lacked some depth. Thanks in advance! (Questions regarding a reasonable self-guided book have very frequently been brought up on this forum.  However, I like to think this is unique and apologize if this feels ""repeated."")",,"['calculus', 'reference-request']"
48,Generalized Coxeter's integral $\displaystyle \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx$,Generalized Coxeter's integral,\displaystyle \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx,"Coxeter's integral is a well-known definite integral given by: $$I = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+2\cos(x)}\right)dx$$ I am well aware of the 11-page excruciatingly long procedure used to evaluate this integral by Paul Nahin. This integral made me wonder whether we can find an expression for a generalized integral (similar to that of Coxeter's) of the form: $$I(a,b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx$$ For simplicity, considering the cases for $a=1$ , we have: $$I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+b\cos(x)}\right) dx$$ I begin simplifying this as Nahin did - using the fact that $$\arccos x = 2 \arccos \left(\sqrt{\frac{1+x}{2}}\right)$$ We can write the integral as $$I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\sqrt{\left(\frac{1+(b+1)\cos(x)}{2 + 2b\cos(x)}\right)}\right) dx$$ $$ = \int_0^{\frac{\pi}{2}} \arctan \sqrt{\left(\frac{1+(b-1)\cos(x)}{1+(b+1)\cos(x)}\right)} dx$$ I have tried two approaches with this integral. The first approach was the same as followed by Nahin for evaluating Coxeter's integral. It turned out to be a dead end and I could proceed no further with that procedure. The second approach was to substitute $t = \tan(\frac{x}{2})$ and simplify. I obtained $$I(b) = 4 \int_0^1 \arctan \sqrt{\left(\frac{b-(b-2)t^2}{(b+2)-bt^2}\right)} \frac{dt}{t^2+1}$$ Then recalling that $$\arctan(b) = b \cdot \int_0^1 \frac{1}{1+b^2u^2} du$$ I rewrote the original integral into a double integral given by: $$I(b) = 4 \int_0^1 \int_0^1 \frac{\sqrt{(b-(b-2)t^2)((b+2)-bt^2)}}{(t^2+1)\cdot \left[(b+2-bt^2) +(b-(b-2)t^2)\cdot u^2 \right]} dt du$$ I am stuck here. I am not sure, but I have a hunch that we will have to substitute something like $$bt^2 = (b+2)sin^2v$$ Followed by another trigonometric substitution $$\tan v = t$$ Can anyone help me simplify this integral further, or suggest a better approach to find $I(b)$ , and if possible - $I(a,b)$ ? Any help is appreciated. Thanks for reading!","Coxeter's integral is a well-known definite integral given by: I am well aware of the 11-page excruciatingly long procedure used to evaluate this integral by Paul Nahin. This integral made me wonder whether we can find an expression for a generalized integral (similar to that of Coxeter's) of the form: For simplicity, considering the cases for , we have: I begin simplifying this as Nahin did - using the fact that We can write the integral as I have tried two approaches with this integral. The first approach was the same as followed by Nahin for evaluating Coxeter's integral. It turned out to be a dead end and I could proceed no further with that procedure. The second approach was to substitute and simplify. I obtained Then recalling that I rewrote the original integral into a double integral given by: I am stuck here. I am not sure, but I have a hunch that we will have to substitute something like Followed by another trigonometric substitution Can anyone help me simplify this integral further, or suggest a better approach to find , and if possible - ? Any help is appreciated. Thanks for reading!","I = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+2\cos(x)}\right)dx I(a,b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{a\cos(x)}{1+b\cos(x)}\right) dx a=1 I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\frac{\cos(x)}{1+b\cos(x)}\right) dx \arccos x = 2 \arccos \left(\sqrt{\frac{1+x}{2}}\right) I(b) = \int_0^{\frac{\pi}{2}} \arccos \left(\sqrt{\left(\frac{1+(b+1)\cos(x)}{2 + 2b\cos(x)}\right)}\right) dx  = \int_0^{\frac{\pi}{2}} \arctan \sqrt{\left(\frac{1+(b-1)\cos(x)}{1+(b+1)\cos(x)}\right)} dx t = \tan(\frac{x}{2}) I(b) = 4 \int_0^1 \arctan \sqrt{\left(\frac{b-(b-2)t^2}{(b+2)-bt^2}\right)} \frac{dt}{t^2+1} \arctan(b) = b \cdot \int_0^1 \frac{1}{1+b^2u^2} du I(b) = 4 \int_0^1 \int_0^1 \frac{\sqrt{(b-(b-2)t^2)((b+2)-bt^2)}}{(t^2+1)\cdot \left[(b+2-bt^2) +(b-(b-2)t^2)\cdot u^2 \right]} dt du bt^2 = (b+2)sin^2v \tan v = t I(b) I(a,b)",['calculus']
49,Calculate $\sum\limits_{n = - \infty }^\infty {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}} $,Calculate,\sum\limits_{n = - \infty }^\infty {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}} ,question: how do we find that: $$ S = \sum\limits_{n =  - \infty }^\infty  {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}} $$ I modified the sum $$\sum\limits_{n =  - \infty }^\infty  {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}}  = \frac{{\log \left( {{{\left( {\frac{1}{3}} \right)}^2}} \right)}}{{\frac{1}{3}}} + \sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}} + \frac{{\log \left( {{{\left( { - n + \frac{1}{3}} \right)}^2}} \right)}}{{ - n + \frac{1}{3}}}} \right)} $$ $${ =  - 3\log \left( 9 \right) + \sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}} - \frac{{\log \left( {{{\left( {n - \frac{1}{3}} \right)}^2}} \right)}}{{n - \frac{1}{3}}}} \right)}  =  - 3\log \left( 9 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {\frac{{3n + 1}}{3}} \right)}}{{3n + 1}} - \frac{{\log \left( {\frac{{3n - 1}}{3}} \right)}}{{3n - 1}}} \right)}  = }$$ $${ =  - 6\log \left( 3 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {3n + 1} \right) - \log 3}}{{3n + 1}} - \frac{{\log \left( {3n - 1} \right) - \log 3}}{{3n - 1}}} \right)}  = }$$ $${ =  - 6\log \left( 3 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log 3}}{{3n - 1}} - \frac{{\log 3}}{{3n + 1}}} \right)}  + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {3n + 1} \right)}}{{3n + 1}} - \frac{{\log \left( {3n - 1} \right)}}{{3n - 1}}} \right)} }$$,question: how do we find that: I modified the sum, S = \sum\limits_{n =  - \infty }^\infty  {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}}  \sum\limits_{n =  - \infty }^\infty  {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}}}  = \frac{{\log \left( {{{\left( {\frac{1}{3}} \right)}^2}} \right)}}{{\frac{1}{3}}} + \sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}} + \frac{{\log \left( {{{\left( { - n + \frac{1}{3}} \right)}^2}} \right)}}{{ - n + \frac{1}{3}}}} \right)}  { =  - 3\log \left( 9 \right) + \sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {{{\left( {n + \frac{1}{3}} \right)}^2}} \right)}}{{n + \frac{1}{3}}} - \frac{{\log \left( {{{\left( {n - \frac{1}{3}} \right)}^2}} \right)}}{{n - \frac{1}{3}}}} \right)}  =  - 3\log \left( 9 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {\frac{{3n + 1}}{3}} \right)}}{{3n + 1}} - \frac{{\log \left( {\frac{{3n - 1}}{3}} \right)}}{{3n - 1}}} \right)}  = } { =  - 6\log \left( 3 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {3n + 1} \right) - \log 3}}{{3n + 1}} - \frac{{\log \left( {3n - 1} \right) - \log 3}}{{3n - 1}}} \right)}  = } { =  - 6\log \left( 3 \right) + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log 3}}{{3n - 1}} - \frac{{\log 3}}{{3n + 1}}} \right)}  + 6\sum\limits_{n = 1}^\infty  {\left( {\frac{{\log \left( {3n + 1} \right)}}{{3n + 1}} - \frac{{\log \left( {3n - 1} \right)}}{{3n - 1}}} \right)} },"['calculus', 'sequences-and-series']"
50,"Integrals of Jacobi $\vartheta$ functions on the interval $[1,+\infty)$",Integrals of Jacobi  functions on the interval,"\vartheta [1,+\infty)","I start from the following obvious observation, which is declared to be( $q=e^{-\pi x}$ ): \begin{aligned} \int_{1}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4 \text{d}x&=\int_{0}^{1}x\vartheta_2(q)^4\vartheta_4(q)^4 \text{d}x\\ &=\frac12\int_{0}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4 \text{d}x=\frac14, \end{aligned} in which the functional equation $\vartheta_2(e^{-\pi/x})=\sqrt{x}\vartheta_4(e^{-\pi x})$ is used. The last equality is derived from $$ \int_{0}^{\infty}x^{s-1}\vartheta_2(q)^4\vartheta_4(q)^4\text{d}x =16\frac{\Gamma(s)}{\pi^s}\lambda(s)\eta(s-3). $$ Then, I am confronted with the new one $$ \int_{1}^{\infty}\theta_2(q)^4\theta_4(q)^4 \text{d}x=\frac{2G}{\pi^2}. $$ These two integrals have cousins widely as well. For example, some experiments suggest \begin{aligned} &\int_{1}^{\infty}(x^2-1)\vartheta_2 (q)^4\vartheta_3(q)^4 =\frac{8G}{\pi^2}-\frac{14}{\pi^3}\zeta(3),\\ &\int_{1}^{\infty}(1+x)^2 \vartheta_2(q)^2\vartheta_3(q)^4\vartheta_4(q)^4\text{d}x =\frac{\Gamma\left ( \frac14 \right)^8}{8\pi^6}, \end{aligned} the integral $$ \int_{1}^{\infty} \left [ (x-1)\vartheta_2(q)^4\vartheta_3(q)^2 +2\sqrt{2}x\vartheta_2(q)^3\vartheta_4(q)^3  \right ]\text{d}x =2-\frac{8G}{\pi^2} $$ included. By substituting $x=K^\prime(k)/K(k)$ , we obtain \begin{aligned} &\int_{0}^{\frac{1}{\sqrt{2} } } K(k)\left ( K(k)+K^\prime(k) \right )^2\text{d}k =\frac{\Gamma\left ( \frac14 \right )^8 }{128\pi^2},\\ &\int_{\frac{1}{\sqrt{2} } }^{1}  \frac{K(k)^2-K^\prime(k)^2}{k} \text{d}k =\pi G-\frac{7}{4}\zeta(3),\\ &\int_{\frac{1}{\sqrt{2} } }^{1}  \left(\frac{K(k)-K^\prime(k)}{k}+\frac{2\sqrt{2}\sqrt{k}K(k)  }{(1-k^2)^{1/4}}  \right)\text{d}k =\frac{\pi^2}{2}-2G. \end{aligned} Question. Is there any ways being able to prove the integrals listed above? The 2nd Question. I wonder if it exists similar ones with the interval $[\sqrt{n},+\infty)$ , where $n\in\mathbb{Q}_{>0}$ .","I start from the following obvious observation, which is declared to be( ): in which the functional equation is used. The last equality is derived from Then, I am confronted with the new one These two integrals have cousins widely as well. For example, some experiments suggest the integral included. By substituting , we obtain Question. Is there any ways being able to prove the integrals listed above? The 2nd Question. I wonder if it exists similar ones with the interval , where .","q=e^{-\pi x} \begin{aligned}
\int_{1}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x&=\int_{0}^{1}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x\\
&=\frac12\int_{0}^{\infty}x\vartheta_2(q)^4\vartheta_4(q)^4
\text{d}x=\frac14,
\end{aligned} \vartheta_2(e^{-\pi/x})=\sqrt{x}\vartheta_4(e^{-\pi x}) 
\int_{0}^{\infty}x^{s-1}\vartheta_2(q)^4\vartheta_4(q)^4\text{d}x
=16\frac{\Gamma(s)}{\pi^s}\lambda(s)\eta(s-3).
 
\int_{1}^{\infty}\theta_2(q)^4\theta_4(q)^4
\text{d}x=\frac{2G}{\pi^2}.
 \begin{aligned}
&\int_{1}^{\infty}(x^2-1)\vartheta_2
(q)^4\vartheta_3(q)^4
=\frac{8G}{\pi^2}-\frac{14}{\pi^3}\zeta(3),\\
&\int_{1}^{\infty}(1+x)^2
\vartheta_2(q)^2\vartheta_3(q)^4\vartheta_4(q)^4\text{d}x
=\frac{\Gamma\left ( \frac14 \right)^8}{8\pi^6},
\end{aligned} 
\int_{1}^{\infty}
\left [ (x-1)\vartheta_2(q)^4\vartheta_3(q)^2
+2\sqrt{2}x\vartheta_2(q)^3\vartheta_4(q)^3  \right ]\text{d}x
=2-\frac{8G}{\pi^2}
 x=K^\prime(k)/K(k) \begin{aligned}
&\int_{0}^{\frac{1}{\sqrt{2} } }
K(k)\left ( K(k)+K^\prime(k) \right )^2\text{d}k
=\frac{\Gamma\left ( \frac14 \right )^8 }{128\pi^2},\\
&\int_{\frac{1}{\sqrt{2} } }^{1} 
\frac{K(k)^2-K^\prime(k)^2}{k} \text{d}k
=\pi G-\frac{7}{4}\zeta(3),\\
&\int_{\frac{1}{\sqrt{2} } }^{1} 
\left(\frac{K(k)-K^\prime(k)}{k}+\frac{2\sqrt{2}\sqrt{k}K(k)  }{(1-k^2)^{1/4}}  \right)\text{d}k
=\frac{\pi^2}{2}-2G.
\end{aligned} [\sqrt{n},+\infty) n\in\mathbb{Q}_{>0}","['calculus', 'integration', 'elliptic-integrals', 'dirichlet-series', 'theta-functions']"
51,show that $(n/a_n)$ takes every positive integer value,show that  takes every positive integer value,(n/a_n),"Suppose $(a_n)$ is a nondecreasing sequence of positive integers with $\lim\limits_{n\to\infty} \dfrac{a_n}n=0$ . Show that $(n/a_n)$ takes every positive integer value. It turns out that the above implies the following: for every positive integer k, there is an integer N so that there are exactly $N$ primes less than $kN.$ Edit: The latter claim follows since $\pi(n)$ is a nondecreasing sequence of integers that is asymptotically logarithmic in $n$ (though this is very nontrivial to prove) $$\lim_n\frac{\pi(n)}{n}=0$$ implies that $n/\pi(n)$ takes any positive integer value. I'm not sure how to prove the second fact. Let $k$ be a positive integer and choose $n$ with $n/a_n = k$ . Suppose for a contradiction that there does not exist an integer $N$ so that there are exactly $N$ primes less than $kN$ . Let $m$ be a positive integer. We want to find n with $n/a_n = m\Leftrightarrow 1/m = a_n/n$ . It might be useful to consider the finite set $\{k : \dfrac{a_{mk}}{mk}\ge 1/m\}$ for each positive integer m. It is finite since $\lim\limits_{n\to\infty} \dfrac{a_n}n = 0$ . $a_n$ is nondecreasing. If $a_n$ is bounded, then since it is nondecreasing, it is eventually constant and equal to say, $d$ . Then for large enough k $kd/a_k = k$ so all integers $\ge k$ can be attained. For integers $<k$ , pick the first $n$ so that $n \ge a_n$ ; there must exist such an n since $\lim\limits_{n\to\infty} a_n/n = 0.$ Then note that $n = a_n$ as if $n > a_n$ , the only way $n-1 \leq a_{n-1} = a_n - s< n-s$ for some $s\ge 0$ is if $s < 0,$ a contradiction. So $1$ is definitely attained. In light of the above, we consider the smallest $k$ so that $\dfrac{a_{mk}}{mk} \ge \dfrac{1}m.$ If $a_{mk} > k,$ then $a_{mk-k} = a_{mk} - s$ for some $s\ge 0$ and by minimality, $k = 1$ or $a_{m(k-1)} \leq k-1$ . The latter implies $k-1 \leq a_{m(k-1)} < k-s$ , which is clearly impossible. Hence the former must occur, so $a_m > 1.$ This doesn't seem to lead to a contradiction though.","Suppose is a nondecreasing sequence of positive integers with . Show that takes every positive integer value. It turns out that the above implies the following: for every positive integer k, there is an integer N so that there are exactly primes less than Edit: The latter claim follows since is a nondecreasing sequence of integers that is asymptotically logarithmic in (though this is very nontrivial to prove) implies that takes any positive integer value. I'm not sure how to prove the second fact. Let be a positive integer and choose with . Suppose for a contradiction that there does not exist an integer so that there are exactly primes less than . Let be a positive integer. We want to find n with . It might be useful to consider the finite set for each positive integer m. It is finite since . is nondecreasing. If is bounded, then since it is nondecreasing, it is eventually constant and equal to say, . Then for large enough k so all integers can be attained. For integers , pick the first so that ; there must exist such an n since Then note that as if , the only way for some is if a contradiction. So is definitely attained. In light of the above, we consider the smallest so that If then for some and by minimality, or . The latter implies , which is clearly impossible. Hence the former must occur, so This doesn't seem to lead to a contradiction though.","(a_n) \lim\limits_{n\to\infty} \dfrac{a_n}n=0 (n/a_n) N kN. \pi(n) n \lim_n\frac{\pi(n)}{n}=0 n/\pi(n) k n n/a_n = k N N kN m n/a_n = m\Leftrightarrow 1/m = a_n/n \{k : \dfrac{a_{mk}}{mk}\ge 1/m\} \lim\limits_{n\to\infty} \dfrac{a_n}n = 0 a_n a_n d kd/a_k = k \ge k <k n n \ge a_n \lim\limits_{n\to\infty} a_n/n = 0. n = a_n n > a_n n-1 \leq a_{n-1} = a_n - s< n-s s\ge 0 s < 0, 1 k \dfrac{a_{mk}}{mk} \ge \dfrac{1}m. a_{mk} > k, a_{mk-k} = a_{mk} - s s\ge 0 k = 1 a_{m(k-1)} \leq k-1 k-1 \leq a_{m(k-1)} < k-s a_m > 1.","['calculus', 'sequences-and-series', 'limits', 'elementary-number-theory', 'contest-math']"
52,Solve the differential equation: $\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}$,Solve the differential equation:,\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}},"Solve the differential equation: $\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}$ My Attempt: $$\frac{2xdx-2ydy}{2x^2\frac{xdy-ydx}{x^2}}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{d(x^2-y^2)}{2x^2d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{\frac{d(x^2-y^2)}{x^2-y^2}}{2\frac{x^2}{x^2-y^2}d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac1{x^2-y^2}\sqrt{\frac{x^2-y^2}{1+x^2-y^2}}d(x^2-y^2)=2\frac{1}{1-(\frac yx)^2}d(\frac yx)$$ Put $x^2-y^2=p, \frac yx=q$ $$\frac1p\frac{\sqrt p}{\sqrt{1+p}}dp=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{p^2+p}}=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{(p+\frac12)^2-\frac14}}=\frac{2dq}{1-q^2}\\ \ln|p+\frac12+\sqrt{p^2+p}|=\ln\frac{1+q}{1-q}+\ln c\\ \implies x^2-y^2+\frac12+\sqrt{x^2-y^2}\sqrt{1+x^2-y^2}=c\frac{x+y}{x-y}$$ The answer given is $\sqrt{x^2-y^2}+\sqrt{1+x^2-y^2}=c\frac{x+y}{\sqrt{x^2-y^2}}$ Can we reach the answer with the approach that I followed? The other approach is mentioned here.",Solve the differential equation: My Attempt: Put The answer given is Can we reach the answer with the approach that I followed? The other approach is mentioned here.,"\frac{xdx-ydy}{xdy-ydx}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}} \frac{2xdx-2ydy}{2x^2\frac{xdy-ydx}{x^2}}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{d(x^2-y^2)}{2x^2d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac{\frac{d(x^2-y^2)}{x^2-y^2}}{2\frac{x^2}{x^2-y^2}d(\frac yx)}=\sqrt{\frac{1+x^2-y^2}{x^2-y^2}}\\ \frac1{x^2-y^2}\sqrt{\frac{x^2-y^2}{1+x^2-y^2}}d(x^2-y^2)=2\frac{1}{1-(\frac yx)^2}d(\frac yx) x^2-y^2=p, \frac yx=q \frac1p\frac{\sqrt p}{\sqrt{1+p}}dp=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{p^2+p}}=\frac{2dq}{1-q^2}\\ \frac{dp}{\sqrt{(p+\frac12)^2-\frac14}}=\frac{2dq}{1-q^2}\\ \ln|p+\frac12+\sqrt{p^2+p}|=\ln\frac{1+q}{1-q}+\ln c\\ \implies x^2-y^2+\frac12+\sqrt{x^2-y^2}\sqrt{1+x^2-y^2}=c\frac{x+y}{x-y} \sqrt{x^2-y^2}+\sqrt{1+x^2-y^2}=c\frac{x+y}{\sqrt{x^2-y^2}}","['calculus', 'integration', 'ordinary-differential-equations', 'contest-math', 'indefinite-integrals']"
53,Strange Result in Contour Integration: Possible Mistake?,Strange Result in Contour Integration: Possible Mistake?,,"(Context) I tried out this proof I made up for fun: Let $a \geq 0.$ Prove that $$\int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \pi\sin\left(\frac{1}{e^{a}}\right).$$ I put this integral and my answer in Desmos with a slider $a$ , and it seems like Desmos is numerically approximating both expressions such that they're both off by a bit, probably around $0.1$ or $0.01$ . (Also, I was treating $\infty$ as a huge number like $3000$ because Desmos sometimes has trouble dealing with it.) (Attempt) Let that integral equal $I$ . Rewriting it gives us $$\int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \Im \int_{-\infty}^{\infty}\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}dz.$$ Let $f(z) = \frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}$ and traverse counterclockwise along this curve $C:= \left[-R,R\right] \cup \Gamma$ , where $C$ is a semicircle on and above the real axis in the complex plane and $R$ is a large radius approaching $\infty$ . The set of singularities is $\left\{i,-i\right\}$ (after setting the denominator equal to $0$ ). Notice that $i$ lives in $C$ , so we can make it our pole. By Cauchy's Residue Theorem, we can rewrite $\oint_C f(z)dz$ as $$2\pi i\text{Res}(f(z), z=i) = \int_{\Gamma}f(z)dz + \int_{-R}^R f(z)dz.$$ Solving the residue, we get $$\text{Res}\left(\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}, z=i\right) = \frac{\exp{(ie^{ia(i)})}}{\frac{d}{dz}\left(z^2+1\right)\Big|_{z=i}} = \frac{\exp\left(ie^{-a}\right)}{2i}.$$ As for the contour integral, it converges to $0$ as $R \to \infty$ . Notice if $z$ is on $\Gamma$ , then $|z| = R$ . Observe from using one of the Triangle Inequalities that $$\left|\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}\right|\le\frac{\left|\exp\left(ie^{iaz}\right)\right|}{\left|\left|z\right|^{2}-1\right|}=\frac{1}{R^{2}-1}.$$ Using the ML-Inequality, observe that $$0 \leq \left|\int_{\Gamma}f(z)dz\right| \leq |f(z)|\left(\frac{2\pi R}{2}\right) \leq \frac{\pi R}{R^2-1}.$$ Taking $R\to\infty$ , we can use the Squeeze Theorem to get $$\lim_{R\to\infty}\left|\int_{\Gamma}f(z)dz\right| = 0,$$ which implies $$\lim_{R\to\infty}\int_{\Gamma}f(z)dz = 0.$$ Going back to $\oint_{C} f(z)dz$ , we take $R\to\infty$ and $\Im$ on both sides to get $$ \eqalign{ \Im\lim_{R\to\infty} 2\pi i\text{Res}(f(z), z=i) &= \Im\lim_{R\to\infty} \int_{\Gamma}f(z)dz + \Im\lim_{R\to\infty} \int_{-R}^R f(z)dz   \cr \Im\lim_{R\to\infty}\int_{-R}^{R}f(z)dz &= I \cr &= \Im\left(\pi \exp{\left(ie^{-a}\right)}\right) \cr &= \pi\sin{\left(\frac{1}{e^a}\right)}. \cr } $$ (Question) Is there something wrong with my proof? I've checked this over and over and I want to say there's nothing wrong and that I didn't miss anything trivial. Still, the approximations Desmos is giving me are sort of bothering me, even though I know Desmos isn't always reliable when evaluating things at $\infty$ . And also, the proof doesn't seem to hold true if $a$ is negative. If my proof is correct, then how come it breaks when $a$ is negative? If someone can shed some light, that would be great. Thank you in advance.","(Context) I tried out this proof I made up for fun: Let Prove that I put this integral and my answer in Desmos with a slider , and it seems like Desmos is numerically approximating both expressions such that they're both off by a bit, probably around or . (Also, I was treating as a huge number like because Desmos sometimes has trouble dealing with it.) (Attempt) Let that integral equal . Rewriting it gives us Let and traverse counterclockwise along this curve , where is a semicircle on and above the real axis in the complex plane and is a large radius approaching . The set of singularities is (after setting the denominator equal to ). Notice that lives in , so we can make it our pole. By Cauchy's Residue Theorem, we can rewrite as Solving the residue, we get As for the contour integral, it converges to as . Notice if is on , then . Observe from using one of the Triangle Inequalities that Using the ML-Inequality, observe that Taking , we can use the Squeeze Theorem to get which implies Going back to , we take and on both sides to get (Question) Is there something wrong with my proof? I've checked this over and over and I want to say there's nothing wrong and that I didn't miss anything trivial. Still, the approximations Desmos is giving me are sort of bothering me, even though I know Desmos isn't always reliable when evaluating things at . And also, the proof doesn't seem to hold true if is negative. If my proof is correct, then how come it breaks when is negative? If someone can shed some light, that would be great. Thank you in advance.","a \geq 0. \int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \pi\sin\left(\frac{1}{e^{a}}\right). a 0.1 0.01 \infty 3000 I \int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \Im \int_{-\infty}^{\infty}\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}dz. f(z) = \frac{\exp\left(ie^{iaz}\right)}{z^{2}+1} C:= \left[-R,R\right] \cup \Gamma C R \infty \left\{i,-i\right\} 0 i C \oint_C f(z)dz 2\pi i\text{Res}(f(z), z=i) = \int_{\Gamma}f(z)dz + \int_{-R}^R f(z)dz. \text{Res}\left(\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}, z=i\right) = \frac{\exp{(ie^{ia(i)})}}{\frac{d}{dz}\left(z^2+1\right)\Big|_{z=i}} = \frac{\exp\left(ie^{-a}\right)}{2i}. 0 R \to \infty z \Gamma |z| = R \left|\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}\right|\le\frac{\left|\exp\left(ie^{iaz}\right)\right|}{\left|\left|z\right|^{2}-1\right|}=\frac{1}{R^{2}-1}. 0 \leq \left|\int_{\Gamma}f(z)dz\right| \leq |f(z)|\left(\frac{2\pi R}{2}\right) \leq \frac{\pi R}{R^2-1}. R\to\infty \lim_{R\to\infty}\left|\int_{\Gamma}f(z)dz\right| = 0, \lim_{R\to\infty}\int_{\Gamma}f(z)dz = 0. \oint_{C} f(z)dz R\to\infty \Im 
\eqalign{
\Im\lim_{R\to\infty} 2\pi i\text{Res}(f(z), z=i) &= \Im\lim_{R\to\infty} \int_{\Gamma}f(z)dz + \Im\lim_{R\to\infty} \int_{-R}^R f(z)dz   \cr
\Im\lim_{R\to\infty}\int_{-R}^{R}f(z)dz &= I \cr
&= \Im\left(\pi \exp{\left(ie^{-a}\right)}\right) \cr
&= \pi\sin{\left(\frac{1}{e^a}\right)}. \cr
}
 \infty a a","['calculus', 'integration', 'complex-analysis', 'improper-integrals', 'contour-integration']"
54,"if $f(x)f(y) = y^h f(x/2) + x^k f(y/2)$ for all positive reals $x,y$, then $f$ must be identically zero","if  for all positive reals , then  must be identically zero","f(x)f(y) = y^h f(x/2) + x^k f(y/2) x,y f","Prove that if $h\neq k\in\mathbb{R}$ and $f$ is a real-valued function so that $f(x)f(y) = y^h f(x/2) + x^k f(y/2)$ for all positive reals $x,y$ , then $f$ must be identically zero. If one equates $f(y)f(x) = f(x)f(y)$ (using the original functional equation), one gets that for $x\neq 1, f(x/2) = A((2x)^h - (2x)^k)$ for some constant $A$ . If one assumes that the coefficients of terms with equal exponents must match, then we get $A=0$ (in more detail, we equate $A^2((2x)^h - (2x)^k)((2y)^h - (2x)^k) = (x^h - x^k) A y^h + x^k A (y^h - y^k).$ But since $h$ and $k$ are real, it's not clear that equating coefficients on both sides is even valid. For polynomials, of course it would be valid since nonconstant polynomials only have finitely many roots. I'm not sure if the set $\{x^i : i \in \mathbb{R}\}$ linearly independent over $\mathbb{R}$ . If the given set is in fact linearly independent, then for any $k\ge 1$ and constants $c_1,\cdots, c_k\in \mathbb{R}$ , if $c_1 x^{i_1} + c_2 x^{i_2}+\cdots + c_k x^{i_k} = 0$ (i.e. it's the zero function) then $c_1=c_2=\cdots = c_k = 0.$","Prove that if and is a real-valued function so that for all positive reals , then must be identically zero. If one equates (using the original functional equation), one gets that for for some constant . If one assumes that the coefficients of terms with equal exponents must match, then we get (in more detail, we equate But since and are real, it's not clear that equating coefficients on both sides is even valid. For polynomials, of course it would be valid since nonconstant polynomials only have finitely many roots. I'm not sure if the set linearly independent over . If the given set is in fact linearly independent, then for any and constants , if (i.e. it's the zero function) then","h\neq k\in\mathbb{R} f f(x)f(y) = y^h f(x/2) + x^k f(y/2) x,y f f(y)f(x) = f(x)f(y) x\neq 1, f(x/2) = A((2x)^h - (2x)^k) A A=0 A^2((2x)^h - (2x)^k)((2y)^h - (2x)^k) = (x^h - x^k) A y^h + x^k A (y^h - y^k). h k \{x^i : i \in \mathbb{R}\} \mathbb{R} k\ge 1 c_1,\cdots, c_k\in \mathbb{R} c_1 x^{i_1} + c_2 x^{i_2}+\cdots + c_k x^{i_k} = 0 c_1=c_2=\cdots = c_k = 0.","['calculus', 'algebra-precalculus', 'contest-math', 'functional-equations']"
55,Finding the volume using Washers,Finding the volume using Washers,,"Problem: Find the volume generated when the region bounded by the given curves and line is revolved about the x-axis. $$ y = 3x - x^2$$ $$ y = 3x $$ Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = 3x$ intersect. \begin{align*} 3x - x^2 &= x \\ -x^2 &= 2x \\ x = 0 &\text{ or } x = 2 \\ V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\ \dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\ \end{align*} Now we have two integrals to evaluate. \begin{align*} \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\ \end{align*} For the second integral we have: \begin{align*} \int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\ \dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\ \dfrac{V}{\pi} &= \dfrac{86 }{15 } \\ V &= \dfrac{86\pi}{15} \end{align*} However, the book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Based upon a comment from John Douma, I realized that I copied the question incorrectly. Here is the revised question with my solution which still has the wrong answer. Problem: Find the volume generated when the region bounded by the given curses and line is revolved about the x-axis. $$ y = 3x - x^2 $$ $$ y = x $$ Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = 3x$ intersect. \begin{align*} 3x - x^2 &= x \\ -x^2 &= -2x \\ x = 0 &\text{ or } x = 2 \\ V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\ \dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\ \end{align*} Now we have two integrals to evaluate. \begin{align*} \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\ \end{align*} For the second integral we have: \begin{align*} \int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\ \dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\ \dfrac{V}{\pi} &= \dfrac{86 }{15 } \\ V &= \dfrac{86\pi}{15} \end{align*} However, the book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Here is an updated answer based upon the comments from DougM. Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = x$ intersect. \begin{align*} 3x - x^2 &= x \\ -x^2 &= -2x \\ x = 0 &\text{ or } x = 2 \\ V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\ \dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\ \end{align*} Now we have two integrals to evaluate. \begin{align*} \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + 9x \Big|_0^2 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 18 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{2}{5} \\ \end{align*} For the second integral we have: \begin{align*} \int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\ \dfrac{V}{\pi} &= \dfrac{2}{5} - \dfrac{8}{3} = \dfrac{ 6 - 24}{15} \\ \dfrac{V}{\pi} &= -\dfrac{18 }{15 } \\ V &= -\dfrac{18\pi}{15} \end{align*} This answer is obviously wrong. The book gets: $ \dfrac{ 56 \pi}{15} $ . Where did I go wrong? Here is an updated answer based upon the comment from N. F. Taussig. I now have a correct solution. Answer: Let $V$ be the volume we are trying to find. The first step is to find the points where $3x - x^2$ and $y = x$ intersect. \begin{align*} 3x - x^2 &= x \\ -x^2 &= 2x \\ x = 0 &\text{ or } x = 2 \\ V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\ \dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\ \end{align*} Now we have two integrals to evaluate. \begin{align*} \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9x^2 \,\, dx \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + \dfrac{9x^3}{3} \Big|_0^2 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + \dfrac{9(8)}{3} \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 24 \\ \int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} \\ \end{align*} For the second integral we have: \begin{align*} \int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\ \dfrac{V}{\pi} &= \dfrac{32}{5} - \dfrac{8}{3} = \dfrac{ 96 - 40}{15} \\ \dfrac{V}{\pi} &= \dfrac{56 }{15 } \\ V &= \dfrac{56\pi}{15} \end{align*} This answer matches that given in the book.","Problem: Find the volume generated when the region bounded by the given curves and line is revolved about the x-axis. Answer: Let be the volume we are trying to find. The first step is to find the points where and intersect. Now we have two integrals to evaluate. For the second integral we have: However, the book gets: . Where did I go wrong? Based upon a comment from John Douma, I realized that I copied the question incorrectly. Here is the revised question with my solution which still has the wrong answer. Problem: Find the volume generated when the region bounded by the given curses and line is revolved about the x-axis. Answer: Let be the volume we are trying to find. The first step is to find the points where and intersect. Now we have two integrals to evaluate. For the second integral we have: However, the book gets: . Where did I go wrong? Here is an updated answer based upon the comments from DougM. Answer: Let be the volume we are trying to find. The first step is to find the points where and intersect. Now we have two integrals to evaluate. For the second integral we have: This answer is obviously wrong. The book gets: . Where did I go wrong? Here is an updated answer based upon the comment from N. F. Taussig. I now have a correct solution. Answer: Let be the volume we are trying to find. The first step is to find the points where and intersect. Now we have two integrals to evaluate. For the second integral we have: This answer matches that given in the book."," y = 3x - x^2  y = 3x  V 3x - x^2 y = 3x \begin{align*}
3x - x^2 &= x \\
-x^2 &= 2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\
\end{align*} \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\
\dfrac{V}{\pi} &= \dfrac{86 }{15 } \\
V &= \dfrac{86\pi}{15}
\end{align*}  \dfrac{ 56 \pi}{15}   y = 3x - x^2   y = x  V 3x - x^2 y = 3x \begin{align*}
3x - x^2 &= x \\
-x^2 &= -2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^2 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^3}{3} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(8)}{3} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 16 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{42}{5} \\
\end{align*} \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{42}{5} - \dfrac{8}{3} = \dfrac{ 3(42) - 5(8)}{15} \\
\dfrac{V}{\pi} &= \dfrac{86 }{15 } \\
V &= \dfrac{86\pi}{15}
\end{align*}  \dfrac{ 56 \pi}{15}  V 3x - x^2 y = x \begin{align*}
3x - x^2 &= x \\
-x^2 &= -2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + 9x \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 18 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{2}{5} \\
\end{align*} \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{2}{5} - \dfrac{8}{3} = \dfrac{ 6 - 24}{15} \\
\dfrac{V}{\pi} &= -\dfrac{18 }{15 } \\
V &= -\dfrac{18\pi}{15}
\end{align*}  \dfrac{ 56 \pi}{15}  V 3x - x^2 y = x \begin{align*}
3x - x^2 &= x \\
-x^2 &= 2x \\
x = 0 &\text{ or } x = 2 \\
V &= \int_0^2 \pi \left( (3x - x^2)^2 - x^2 \right) \,\, dx \\
\dfrac{V}{\pi} &= \int_0^2 (3x - x^2)^2 \,\, dx - \int_0^2 x^2 \,\, dx \\
\end{align*} \begin{align*}
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 (x^2-3x)^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \int_0^2 x^4 - 6x^3 + 9x^2 \,\, dx \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{x^5}{5} - \dfrac{6x^4}{4} + \dfrac{9x^3}{3} \Big|_0^2 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - \dfrac{6(16)}{4} + \dfrac{9(8)}{3} \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} - 24 + 24 \\
\int_0^2 (3x - x^2)^2 \,\, dx &= \dfrac{32}{5} \\
\end{align*} \begin{align*}
\int_0^2 x^2 \,\, dx &= \dfrac{x^3}{3} \Big|_0^2 = \dfrac{8}{3} \\
\dfrac{V}{\pi} &= \dfrac{32}{5} - \dfrac{8}{3} = \dfrac{ 96 - 40}{15} \\
\dfrac{V}{\pi} &= \dfrac{56 }{15 } \\
V &= \dfrac{56\pi}{15}
\end{align*}","['calculus', 'integration', 'solid-of-revolution']"
56,"Difficult Coin Toss Probability Problem (Probability, Calculus)","Difficult Coin Toss Probability Problem (Probability, Calculus)",,"I posted a question earlier, but I couldn't speak English, so the explanation was lacking. sorry. (so I'm asking a question again using a translator.) Anyway, I would like to ask a question about the problem of tossing a coin, which is classified as calculus. . . Problem : If a coin with ""probability of coming up heads"" p is tossed n 1 times, it comes up heads m 1 times. Also, when a coin with ""probability of coming up heads"" q is tossed n 2 times, it comes up heads m 2 times. In this case, find the probability that p < q. ex ) n 1 = 2, m 1 = 1, n 2 = 4, m 2 = 3 : Ans = 5/7 ex2 ) n 1 = 8, m 1 = 4, n 2 = 16, m 2 = 8 : Ans = 1/2 ex3 ) n 1 = 2, m 1 = 0, n 2 = 6, m 2 = 1 : Ans = 8/15 ex4 ) n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1 : Ans = 4/5 . . The equation I established The equation I established was as follows, but when I substituted n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1, I could see that the equation was wrong because 11/70 came out. (Since the answer needs to be 4/5...) (+ Now that I look at it, I wrote the relationship between p and q in reverse.) Please help me with how to set up the equation, or solve this problem. Thank you.","I posted a question earlier, but I couldn't speak English, so the explanation was lacking. sorry. (so I'm asking a question again using a translator.) Anyway, I would like to ask a question about the problem of tossing a coin, which is classified as calculus. . . Problem : If a coin with ""probability of coming up heads"" p is tossed n 1 times, it comes up heads m 1 times. Also, when a coin with ""probability of coming up heads"" q is tossed n 2 times, it comes up heads m 2 times. In this case, find the probability that p < q. ex ) n 1 = 2, m 1 = 1, n 2 = 4, m 2 = 3 : Ans = 5/7 ex2 ) n 1 = 8, m 1 = 4, n 2 = 16, m 2 = 8 : Ans = 1/2 ex3 ) n 1 = 2, m 1 = 0, n 2 = 6, m 2 = 1 : Ans = 8/15 ex4 ) n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1 : Ans = 4/5 . . The equation I established The equation I established was as follows, but when I substituted n 1 = 2, m 1 = 0, n 2 = 2, m 2 = 1, I could see that the equation was wrong because 11/70 came out. (Since the answer needs to be 4/5...) (+ Now that I look at it, I wrote the relationship between p and q in reverse.) Please help me with how to set up the equation, or solve this problem. Thank you.",,"['calculus', 'probability']"
57,Limit of a (rather general) recursive sequence,Limit of a (rather general) recursive sequence,,"So I am struggling with the following problem: Let $F$ be a vector (sub)space of recursive sequences satisfying $x_{i+2} = bx_{i+1} + ax_{i}$ in field $\mathbb{K}$ , with a vector space endomorphism $L: F \to F, (x_{i})_{i} \to (x_{i+1})_i $ . Show that if $a>0$ and $b \neq 0$ , $\lim_{i \to \infty} \frac{x_{i+1}}{x_{i}}$ exists and is equal to one of the eigenvalues of the endomorphism $L$ . I have shown that $F$ is indeed a vector subspace of $\mathbb{K}^\mathbb{N}$ and that the function $L$ is and an endomorphism of this vector subspace. I have proven that the $\dim F = 2$ and that we have a ""natural"" basis of two sequences $f_0 = (1, 0, ...)$ and $f_1 = (0, 1, ...)$ . The endomorphism $L$ can then be described by a matrix: \begin{equation*} A=  \begin{pmatrix} 0 & 1 \\ a & b \end{pmatrix} \end{equation*} in this natural basis. I have then found two eigenvalues, $e_1 = \frac{b + \sqrt{b + 4a}}{2}$ and $e_2 = \frac{b - \sqrt{b + 4a}}{2}$ . The related eigenvectors are then \begin{equation*} v_{1,2} =  \begin{pmatrix} 2 \\ 2e_{1,2}  \end{pmatrix} \end{equation*} We also see that: \begin{equation*} \begin{pmatrix} x_i \\ x_{i+1}  \end{pmatrix} = A^i \begin{pmatrix} x_0 \\ x_1  \end{pmatrix}  \end{equation*} Since $A$ is diagonalizable, we can write: \begin{equation*} \begin{pmatrix} x_i \\ x_{i+1}  \end{pmatrix} = S D^i S^{-1} \begin{pmatrix} x_0 \\ x_1  \end{pmatrix}  \end{equation*} with \begin{equation*} S = \begin{pmatrix} 2 & 2\\ 2e_1 & 2e_2  \end{pmatrix}, \; D =  \begin{pmatrix} e_1 & 0\\ 0 & e_2  \end{pmatrix}, \; S^{-1} = \frac{-1}{4\sqrt{b^2 + 4a}}\begin{pmatrix} 2e_2 & 2e_1\\ 2 & 2  \end{pmatrix} \end{equation*} We can then find the explicit formula for $x_i$ , we get: \begin{equation} x_i = \frac{-1}{4\sqrt{b^2 + 4a}}((4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1) \end{equation} And so \begin{equation*} \lim_{i \to \infty} \frac{x_{i+1}}{x_i} = \lim_{i \to \infty} \frac{(4e_1^{i+1} e_2 + 4e_2^{i+1})x_0 + (4e_1^{i+2} + 4e_2^{i+1})x_1)}{(4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1)} \end{equation*} And that is where I am stuck, if I could prove that this limit exists, I could simply use the fact that \begin{equation*} \theta_{i+1} = \frac{x_{i+1}}{x_{i}} = \frac{bx_i + ax_{i-1}}{x_i} = b + a \frac{x_{i-1}}{x_i} = b + \frac{a}{\theta_{i-1}} \end{equation*} and then $\theta = \lim_{i \to \infty}\frac{x_{i+1}}{x_{i}}$ satisfies \begin{equation*} \theta = b + \frac{a}{\theta} \end{equation*} and so \begin{equation*} \theta = \frac{b \pm \sqrt{b^2 + 4a}}{2} \end{equation*} which is equal to the eigenvalues of $L$ . But how do I prove that this limit exists in the first place? I tried to evaluate the explicit formula that I got, but I didn't find a way to do it. Could you please help?","So I am struggling with the following problem: Let be a vector (sub)space of recursive sequences satisfying in field , with a vector space endomorphism . Show that if and , exists and is equal to one of the eigenvalues of the endomorphism . I have shown that is indeed a vector subspace of and that the function is and an endomorphism of this vector subspace. I have proven that the and that we have a ""natural"" basis of two sequences and . The endomorphism can then be described by a matrix: in this natural basis. I have then found two eigenvalues, and . The related eigenvectors are then We also see that: Since is diagonalizable, we can write: with We can then find the explicit formula for , we get: And so And that is where I am stuck, if I could prove that this limit exists, I could simply use the fact that and then satisfies and so which is equal to the eigenvalues of . But how do I prove that this limit exists in the first place? I tried to evaluate the explicit formula that I got, but I didn't find a way to do it. Could you please help?","F x_{i+2} = bx_{i+1} + ax_{i} \mathbb{K} L: F \to F, (x_{i})_{i} \to (x_{i+1})_i  a>0 b \neq 0 \lim_{i \to \infty} \frac{x_{i+1}}{x_{i}} L F \mathbb{K}^\mathbb{N} L \dim F = 2 f_0 = (1, 0, ...) f_1 = (0, 1, ...) L \begin{equation*} A= 
\begin{pmatrix}
0 & 1 \\
a & b
\end{pmatrix}
\end{equation*} e_1 = \frac{b + \sqrt{b + 4a}}{2} e_2 = \frac{b - \sqrt{b + 4a}}{2} \begin{equation*} v_{1,2} = 
\begin{pmatrix}
2 \\
2e_{1,2} 
\end{pmatrix}
\end{equation*} \begin{equation*}
\begin{pmatrix}
x_i \\
x_{i+1} 
\end{pmatrix} = A^i
\begin{pmatrix}
x_0 \\
x_1 
\end{pmatrix} 
\end{equation*} A \begin{equation*}
\begin{pmatrix}
x_i \\
x_{i+1} 
\end{pmatrix} = S D^i S^{-1}
\begin{pmatrix}
x_0 \\
x_1 
\end{pmatrix} 
\end{equation*} \begin{equation*}
S =
\begin{pmatrix}
2 & 2\\
2e_1 & 2e_2 
\end{pmatrix}, \; D = 
\begin{pmatrix}
e_1 & 0\\
0 & e_2 
\end{pmatrix}, \; S^{-1} = \frac{-1}{4\sqrt{b^2 + 4a}}\begin{pmatrix}
2e_2 & 2e_1\\
2 & 2 
\end{pmatrix}
\end{equation*} x_i \begin{equation}
x_i = \frac{-1}{4\sqrt{b^2 + 4a}}((4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1)
\end{equation} \begin{equation*}
\lim_{i \to \infty} \frac{x_{i+1}}{x_i} = \lim_{i \to \infty} \frac{(4e_1^{i+1} e_2 + 4e_2^{i+1})x_0 + (4e_1^{i+2} + 4e_2^{i+1})x_1)}{(4e_1^i e_2 + 4e_2^i)x_0 + (4e_1^{i+1} + 4e_2^{i})x_1)}
\end{equation*} \begin{equation*}
\theta_{i+1} = \frac{x_{i+1}}{x_{i}} = \frac{bx_i + ax_{i-1}}{x_i} = b + a \frac{x_{i-1}}{x_i} = b + \frac{a}{\theta_{i-1}}
\end{equation*} \theta = \lim_{i \to \infty}\frac{x_{i+1}}{x_{i}} \begin{equation*}
\theta = b + \frac{a}{\theta}
\end{equation*} \begin{equation*}
\theta = \frac{b \pm \sqrt{b^2 + 4a}}{2}
\end{equation*} L","['calculus', 'linear-algebra', 'sequences-and-series', 'recursion']"
58,Is there any other approach available without trig substitution?,Is there any other approach available without trig substitution?,,"Let $f\colon [0, 1] \to \mathbb{R}$ be a continuous function such that for any $x, y \in [0, 1]$ , $xf(y) + yf(x) \le  1$ . Find maximum value of $\int_0^1f(x)\,dx$ . Method given was using $x= \sin\theta$ , we get $$\int_0^1 f(x)\, dx = \int_0^{\pi/2} f(\sin\theta)\cos\theta\, d\theta \tag{I}$$ and similarly by $x= \cos\theta$ , we would get $$\int_0^1 f(x)\,dx = \int_0^{\pi/2} f(\cos\theta)\sin\theta \,d\theta \tag{II},$$ adding $(\text{I})$ and $(\text{II})$ we get max value of the required integral to be $\pi/4$ . Any other approach because the idea is very tricky and one may not realize for this type of substitution.","Let be a continuous function such that for any , . Find maximum value of . Method given was using , we get and similarly by , we would get adding and we get max value of the required integral to be . Any other approach because the idea is very tricky and one may not realize for this type of substitution.","f\colon [0, 1] \to \mathbb{R} x, y \in [0, 1] xf(y) + yf(x) \le  1 \int_0^1f(x)\,dx x= \sin\theta \int_0^1 f(x)\, dx = \int_0^{\pi/2} f(\sin\theta)\cos\theta\, d\theta \tag{I} x= \cos\theta \int_0^1 f(x)\,dx = \int_0^{\pi/2} f(\cos\theta)\sin\theta \,d\theta \tag{II}, (\text{I}) (\text{II}) \pi/4",['calculus']
59,The course of the function $f(x) = \sin^3 (x) + \cos^3 (x)$. [closed],The course of the function . [closed],f(x) = \sin^3 (x) + \cos^3 (x),"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question First, sorry for my english, hope you understand. so I'm examining the course of this function $f(x) = \sin^3 (x) + \cos^3 (x)$ where I need to find everything I can about this function (e.g. definition field, range of values, inflection points, asymptots, global and local max/min, etc.). Now I'm struggling with 3 tasks. Prove there are no limits in $+\infty$ by showing there are different limits of $f(\pi/2+2k\pi)$ and $f(2k\pi)$ where $k$ goes to $+\infty$ . Is this right? Are the results of the limits different? How to get inflection points? I know I get them by solving the 2nd derivative of $f(x)$ which I think I have but I don't know how to get the exact values. graph and inflex points Find intersection of $f(x)$ with axes. I got to the point where I have two equations and I need to prove why is it like that. Thank you in advance edit: Thank you all for helping me with this problem. I've used @Robert Lee solution in the end.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question First, sorry for my english, hope you understand. so I'm examining the course of this function where I need to find everything I can about this function (e.g. definition field, range of values, inflection points, asymptots, global and local max/min, etc.). Now I'm struggling with 3 tasks. Prove there are no limits in by showing there are different limits of and where goes to . Is this right? Are the results of the limits different? How to get inflection points? I know I get them by solving the 2nd derivative of which I think I have but I don't know how to get the exact values. graph and inflex points Find intersection of with axes. I got to the point where I have two equations and I need to prove why is it like that. Thank you in advance edit: Thank you all for helping me with this problem. I've used @Robert Lee solution in the end.",f(x) = \sin^3 (x) + \cos^3 (x) +\infty f(\pi/2+2k\pi) f(2k\pi) k +\infty f(x) f(x),"['calculus', 'limits', 'graphing-functions']"
60,Prove $0 < x < \pi /2 \implies \sin x > x/\sqrt{x^2+1}$ using Mean Value Theorem,Prove  using Mean Value Theorem,0 < x < \pi /2 \implies \sin x > x/\sqrt{x^2+1},"I'm solving the following problem: Show that if $0 < x < \pi /2$ then $\sin x > \dfrac{x}{\sqrt{x^2+1}}$ . One of the hints given is to apply mean value theorem for $\sin (x)$ on the interval $[0,x]$ This is my attempt so far: Let $f(x) = \sin(x)$ Since all trigonometric functions are continuous and $\sin (x)$ is differentiable, mean value theorem can be applied. $$\frac{\sin x - \sin 0}{x - 0} = \cos c$$ $$\frac{\sin x}{x} = \cos c$$ We know that $0 < c < x$ So, $0 < c < x \leq \pi / 2$ So, $0 \leq \cos c < 1$ Also, $\cos c > \cos x$ $$\cos c > \sqrt{1 - \sin^2 x}$$ $$\frac{\sin x}{x} > \sqrt{1 - \sin^2 x}$$ $$\sin x > x\sqrt{1 - \sin^2 x}$$ Now after this I'm stuck. I'm not sure how to bring $\sqrt{x^2 + 1}$ into the proof! I did think over it and was able to find some relations involving it like: $$\sqrt{x^2 + 1} > 1$$ But I think I'm going the wrong path. How should I complete the proof ?","I'm solving the following problem: Show that if then . One of the hints given is to apply mean value theorem for on the interval This is my attempt so far: Let Since all trigonometric functions are continuous and is differentiable, mean value theorem can be applied. We know that So, So, Also, Now after this I'm stuck. I'm not sure how to bring into the proof! I did think over it and was able to find some relations involving it like: But I think I'm going the wrong path. How should I complete the proof ?","0 < x < \pi /2 \sin x > \dfrac{x}{\sqrt{x^2+1}} \sin (x) [0,x] f(x) = \sin(x) \sin (x) \frac{\sin x - \sin 0}{x - 0} = \cos c \frac{\sin x}{x} = \cos c 0 < c < x 0 < c < x \leq \pi / 2 0 \leq \cos c < 1 \cos c > \cos x \cos c > \sqrt{1 - \sin^2 x} \frac{\sin x}{x} > \sqrt{1 - \sin^2 x} \sin x > x\sqrt{1 - \sin^2 x} \sqrt{x^2 + 1} \sqrt{x^2 + 1} > 1","['calculus', 'derivatives', 'trigonometry']"
61,"Prove that the line, containing the segments with lengths the max and min distances from the origin to a circle, contains the center of the circle","Prove that the line, containing the segments with lengths the max and min distances from the origin to a circle, contains the center of the circle",,"( https://www.desmos.com/calculator/nwdvygfw1r for reference) I have a problem trying to prove what my intuition is telling me. I was trying to find the maximum and minimum distances from the origin of the 2D plane to a circumference, and I thought that maybe the distance's extrema formed line segments that when lied up together, were contained in the line formed with the origin and the circumference's center. My strategy then was to find the equation of the line passing through the origin and the circumference center, find the points at which it cut the circumference, and thus calculate the distances from those points to the origin, taking the larger one as the maximum distance and vice versa. I was stumped when I asked myself why exactly my thoughts led me to blindly believe that those two distances should be contained in that line. When I tried to prove it, I thought of brute-forcing it, by calculating the distance formula ( d (O,P), with P being a point on C ), differentiating it, and then setting it equal to zero, but ended up with a hot mess. My attempt for a circumference with the origin inside it proved successful, but I was unable to extend the rationale of the triangle inequality to the case where the origin is external to the circumference, much less when it lies on the circumference. Any help is appreciated! This is my first time posting here and I'm hoping I can get somewhere with this problem. (in Desmos link: how can I prove that if OA and OB are the min and max distances (A,B points on the ciruference), from O to the circumference respectively, then line AB contains C?)","( https://www.desmos.com/calculator/nwdvygfw1r for reference) I have a problem trying to prove what my intuition is telling me. I was trying to find the maximum and minimum distances from the origin of the 2D plane to a circumference, and I thought that maybe the distance's extrema formed line segments that when lied up together, were contained in the line formed with the origin and the circumference's center. My strategy then was to find the equation of the line passing through the origin and the circumference center, find the points at which it cut the circumference, and thus calculate the distances from those points to the origin, taking the larger one as the maximum distance and vice versa. I was stumped when I asked myself why exactly my thoughts led me to blindly believe that those two distances should be contained in that line. When I tried to prove it, I thought of brute-forcing it, by calculating the distance formula ( d (O,P), with P being a point on C ), differentiating it, and then setting it equal to zero, but ended up with a hot mess. My attempt for a circumference with the origin inside it proved successful, but I was unable to extend the rationale of the triangle inequality to the case where the origin is external to the circumference, much less when it lies on the circumference. Any help is appreciated! This is my first time posting here and I'm hoping I can get somewhere with this problem. (in Desmos link: how can I prove that if OA and OB are the min and max distances (A,B points on the ciruference), from O to the circumference respectively, then line AB contains C?)",,"['calculus', 'algebra-precalculus', 'euclidean-geometry', 'analytic-geometry']"
62,Uniqueness of PDE via energy functional,Uniqueness of PDE via energy functional,,"Assume the pde: $$ u_{tt}(t,x) = c^2u_{xx}(t,x) + \sigma u_{txx}(t,x) -\mu u_{t}(t,x), \quad x \in [0,L], t>0 $$ $$ u_x(t,0) = u(t,L) = 0 $$ $$ u(0,x) = \phi(x), u_t(0,x) = \theta(x), x\in[0,L] $$ $$ \phi(L) = \theta(L) = 0, \phi'(0) = \theta '(0) = 0. $$ and the energy functional: $$ V(t) = \int_{0}^L\frac12u_t^2(t,x) + \frac{c^2}{2}u_x^2(t,x)dx $$ To prove uniqueness we'll assume $u_1$ , $u_2$ are both solutions and then define $u$ as $u := u_1 - u_2$ . Then, we observe that $$u(0,x) = u_1(0,x) - u_2(0,x) = \phi(x) - \phi(x) \equiv 0$$ $$u_t(0,x) = u_{1,t}(0,x) - u_{2,t}(0,x) = \theta(x) - \theta(x) \equiv 0$$ So $u_x(0,x) = u_t(0,x) = 0$ . Thus we have $$ V(0) = \int_0^L 0 \, dx = 0 $$ Also, I have already shown that $V(t) \leq V(0)$ so $$ V(t) \leq 0 $$ and since $V(t) \geq 0$ , we have $V(t) \equiv 0$ . Then since the integrand is non-negative: $$ \frac12u_t^2(t,x) + \frac{c^2}{2}u_x^2(t,x) \equiv 0 \quad \quad (1) $$ Question: Does $(1)$ guarantee that $u \equiv 0$ and why?","Assume the pde: and the energy functional: To prove uniqueness we'll assume , are both solutions and then define as . Then, we observe that So . Thus we have Also, I have already shown that so and since , we have . Then since the integrand is non-negative: Question: Does guarantee that and why?","
u_{tt}(t,x) = c^2u_{xx}(t,x) + \sigma u_{txx}(t,x) -\mu u_{t}(t,x), \quad x \in [0,L], t>0
 
u_x(t,0) = u(t,L) = 0
 
u(0,x) = \phi(x), u_t(0,x) = \theta(x), x\in[0,L]
 
\phi(L) = \theta(L) = 0, \phi'(0) = \theta '(0) = 0.
 
V(t) = \int_{0}^L\frac12u_t^2(t,x) + \frac{c^2}{2}u_x^2(t,x)dx
 u_1 u_2 u u := u_1 - u_2 u(0,x) = u_1(0,x) - u_2(0,x) = \phi(x) - \phi(x) \equiv 0 u_t(0,x) = u_{1,t}(0,x) - u_{2,t}(0,x) = \theta(x) - \theta(x) \equiv 0 u_x(0,x) = u_t(0,x) = 0 
V(0) = \int_0^L 0 \, dx = 0
 V(t) \leq V(0) 
V(t) \leq 0
 V(t) \geq 0 V(t) \equiv 0 
\frac12u_t^2(t,x) + \frac{c^2}{2}u_x^2(t,x) \equiv 0 \quad \quad (1)
 (1) u \equiv 0","['calculus', 'partial-differential-equations', 'boundary-value-problem']"
63,"If $\displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3$ $\forall x \ge 1$ then find $f(2)$",If   then find,"\displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3 \forall x \ge 1 f(2)","Let $f$ be a real valued function on $[1,\infty)$ such that $f(1) = 3.$ If $\displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3$ $\forall x \ge 1$ then find $f(2).$ Here is my approach put $x =2$ to get \begin{align*} 2f(2)+ 8 &= 0\\ f(2) &= -4. \end{align*} But if I differentiate both sides I get \begin{align*} 2f(x) &= f(x) + xf'(x) + 3x^2\\ xf'(x) - f(x) &= -3x^2. \end{align*} Solving this gives $f(x) = -3x^2 + cx.$ Using the initial condition I get $$f(x) = -3x^2 + 6x$$ or $$f(2) = 0.$$ By different methods I am getting different values of $f(2).$ Can anyone here please tell me which one is correct and why? Thank you.",Let be a real valued function on such that If then find Here is my approach put to get But if I differentiate both sides I get Solving this gives Using the initial condition I get or By different methods I am getting different values of Can anyone here please tell me which one is correct and why? Thank you.,"f [1,\infty) f(1) = 3. \displaystyle 2 \int_{2}^xf(t)\,dt = xf(x) + x^3 \forall x \ge 1 f(2). x =2 \begin{align*}
2f(2)+ 8 &= 0\\
f(2) &= -4.
\end{align*} \begin{align*}
2f(x) &= f(x) + xf'(x) + 3x^2\\
xf'(x) - f(x) &= -3x^2.
\end{align*} f(x) = -3x^2 + cx. f(x) = -3x^2 + 6x f(2) = 0. f(2).",['calculus']
64,Partial derivative of $(x^3+y^3)^{1/3}$,Partial derivative of,(x^3+y^3)^{1/3},"Let $f(x,y) = (x^3+y^3)^{1/3}$ . Computing the partial derivative with respect to $x$ , we get $$ \frac{\partial f}{\partial x} = \frac{x^2}{(x^3+y^3)^{2/3}}. $$ This can't be evaluated at $(0,0)$ , and the limit doesn't exist at $(0,0)$ either (approaching the origin along the $x$ -axis and the $y$ -axis give different values). However, if we use the definition of the partial derivative at $(0,0)$ we get $$ \frac{\partial f}{\partial x} (0,0) = \lim_{h \to 0} \frac{f(0+h,0) - f(0,0)}{h} = \lim_{h \to 0} \frac{(h^3)^{1/3}}{h} = 1, $$ so the definition seems to tell us that this partial derivative is $1$ . Why does differentiating with respect to $x$ and then taking a limit not give the same answer?","Let . Computing the partial derivative with respect to , we get This can't be evaluated at , and the limit doesn't exist at either (approaching the origin along the -axis and the -axis give different values). However, if we use the definition of the partial derivative at we get so the definition seems to tell us that this partial derivative is . Why does differentiating with respect to and then taking a limit not give the same answer?","f(x,y) = (x^3+y^3)^{1/3} x 
\frac{\partial f}{\partial x} = \frac{x^2}{(x^3+y^3)^{2/3}}.
 (0,0) (0,0) x y (0,0) 
\frac{\partial f}{\partial x} (0,0) = \lim_{h \to 0} \frac{f(0+h,0) - f(0,0)}{h} = \lim_{h \to 0} \frac{(h^3)^{1/3}}{h} = 1,
 1 x","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
65,Prove or disprove : there exist at most two root of $f(x)=f'(x)$.,Prove or disprove : there exist at most two root of .,f(x)=f'(x),"Let $f(x)$ be differentiable over $[0,+\infty)$ , and $f'(x)$ be   increasing and convex over $[0,+\infty).$ If $f(0)=f'(0)=0$ , then   there exist at most two roots of $f(x)=f'(x)$ over $[0,+\infty).$ Apparently, $x=0$ is already a root, hence we only need prove there exists at most one nonzero root else. Maybe we may construct an auxiliary function $F(x):=e^{-x}f(x)$ , then $F'(x)=e^{-x}(f'(x)-f(x))$ . This will help? Besides, notice that we are not told that $f''(x)$ exists.","Let be differentiable over , and be   increasing and convex over If , then   there exist at most two roots of over Apparently, is already a root, hence we only need prove there exists at most one nonzero root else. Maybe we may construct an auxiliary function , then . This will help? Besides, notice that we are not told that exists.","f(x) [0,+\infty) f'(x) [0,+\infty). f(0)=f'(0)=0 f(x)=f'(x) [0,+\infty). x=0 F(x):=e^{-x}f(x) F'(x)=e^{-x}(f'(x)-f(x)) f''(x)",['calculus']
66,partial derivative of multivariable function with respect to another function,partial derivative of multivariable function with respect to another function,,"Say I have a function $f(x,y,z)$ . If I know $t= \sqrt{x+\sqrt{x^2+ y*z}}$ and I know the partials $\large\frac{\partial{f}}{\partial{x}}$ , $\large\frac{\partial{f}}{\partial{y}}$ , $\large\frac{\partial{f}}{\partial{z}}$ , how could I apply the chain rule in order to obtain $\large\frac{\partial{f}}{\partial{t}}$ ? I would have thought: $$\frac{\partial{f}}{\partial{t}} = \frac{\partial{f}}{\partial{x}}\frac{\partial{x}}{\partial{t}} + \frac{\partial{f}}{\partial{y}}\frac{\partial{y}}{\partial{t}} +\frac{\partial{f}}{\partial{z}}\frac{\partial{z}}{\partial{t}}$$ however, I already feel like i'm on the wrong track.  Can anyone give me a start on how to construct $\large\frac{\partial{f}}{\partial{t}}$ in terms of $\large\frac{\partial{f}}{\partial{x}}$ , $\large\frac{\partial{f}}{\partial{y}}$ and $\large\frac{\partial{f}}{\partial{z}}$ ?","Say I have a function . If I know and I know the partials , , , how could I apply the chain rule in order to obtain ? I would have thought: however, I already feel like i'm on the wrong track.  Can anyone give me a start on how to construct in terms of , and ?","f(x,y,z) t= \sqrt{x+\sqrt{x^2+ y*z}} \large\frac{\partial{f}}{\partial{x}} \large\frac{\partial{f}}{\partial{y}} \large\frac{\partial{f}}{\partial{z}} \large\frac{\partial{f}}{\partial{t}} \frac{\partial{f}}{\partial{t}} = \frac{\partial{f}}{\partial{x}}\frac{\partial{x}}{\partial{t}} + \frac{\partial{f}}{\partial{y}}\frac{\partial{y}}{\partial{t}} +\frac{\partial{f}}{\partial{z}}\frac{\partial{z}}{\partial{t}} \large\frac{\partial{f}}{\partial{t}} \large\frac{\partial{f}}{\partial{x}} \large\frac{\partial{f}}{\partial{y}} \large\frac{\partial{f}}{\partial{z}}","['calculus', 'multivariable-calculus', 'partial-derivative']"
67,"Leibniz' Rule: Prove $\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)$",Leibniz' Rule: Prove,"\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)","A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Thm 6.8 Statement of Thm 6.8: Suppose $u$ is harmonic on $\mathbb C$. Then $$v(x,y) := \int_0^y u_x(x,t) dt - \int_0^x u_y(t,0) dt$$  is a harmonic conjugate for $u$. Pf of Thm 6.8: Question: How do we show $$\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)?$$ It's been awhile since I've done PDEs or multivariable calc, but here goes: First approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,t)|_{t=\color{blue}{y}} - u_t(x,t)|_{t=\color{green}{0}} = u_\color{blue}{y}(x,\color{blue}{y}) - u_\color{green}{0?}(x,\color{green}{0})$$ I mainly don't get why we might have the $y$ in the last term $$u_\color{green}{y}(x,\color{green}{0}).$$ Second approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,\color{blue}{y}) - u_t(x,\color{green}{0}) \stackrel{(*)}{=} u_\color{red}{y}(x,\color{blue}{y}) - u_\color{red}{y}(x,\color{green}{0})$$ (*) Here, I guess I get how for the second term we have $u_t(x,\color{green}{0}) = u_\color{red}{y}(x,\color{green}{0}),$ but not how for the first term we have $$u_t(x,\color{blue}{y})=u_\color{red}{y}(x,\color{blue}{y}).$$","A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Thm 6.8 Statement of Thm 6.8: Suppose $u$ is harmonic on $\mathbb C$. Then $$v(x,y) := \int_0^y u_x(x,t) dt - \int_0^x u_y(t,0) dt$$  is a harmonic conjugate for $u$. Pf of Thm 6.8: Question: How do we show $$\int_0^y u_{tt}(x,t) dt = u_y (x,y) - u_y (x,0)?$$ It's been awhile since I've done PDEs or multivariable calc, but here goes: First approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,t)|_{t=\color{blue}{y}} - u_t(x,t)|_{t=\color{green}{0}} = u_\color{blue}{y}(x,\color{blue}{y}) - u_\color{green}{0?}(x,\color{green}{0})$$ I mainly don't get why we might have the $y$ in the last term $$u_\color{green}{y}(x,\color{green}{0}).$$ Second approach: $$\int_0^y u_{tt}(x,t) dt = u_t(x,t)|_{\color{green}{0}}^{\color{blue}{y}} = u_t(x,\color{blue}{y}) - u_t(x,\color{green}{0}) \stackrel{(*)}{=} u_\color{red}{y}(x,\color{blue}{y}) - u_\color{red}{y}(x,\color{green}{0})$$ (*) Here, I guess I get how for the second term we have $u_t(x,\color{green}{0}) = u_\color{red}{y}(x,\color{green}{0}),$ but not how for the first term we have $$u_t(x,\color{blue}{y})=u_\color{red}{y}(x,\color{blue}{y}).$$",,"['calculus', 'integration', 'analysis', 'multivariable-calculus', 'partial-differential-equations']"
68,Generalized Infinite Integration by Parts,Generalized Infinite Integration by Parts,,"During my studies in calc 2, I became fascinated by the integral $\int e^{-x^2}dx$ after hearing from the professor that it has no elementary function as its integral. I came up with an interesting technique to try the integral by using Integration by Parts like so: $$\int e^{-x^2}dx=xe^{-x^2}+2\int x^2e^{-x^2}dx $$ $$\int e^{-x^2}dx=xe^{-x^2}+\frac {2} {3}x^3e^{-x^2}+\int x^4e^{-x^2}dx$$ and continuing this until the pattern became obvious and I came up with the following equation: $$\int e^{-x^2}dx=e^{-x^2}\sum_{i=0}^{\infty}  \frac {2^i} {(2i+1)!!}x^{2i+1}+C$$ where $(2i+1)!!$ denotes the double factorial $(2n+1)(2n-1)(2n-3)...(2 \text{ or } 1)$.  From here, I thought about a generalized case for any infinitely differentiable function $f(x)$ and for an argument raised to any power $n$.  $$\int f(x^n)dx $$ $$u=f(x^n), du=nx^{n-1}f'(x^n), dv=dx, v=x$$ $$\int f(x^n)dx=xf(x^n)-n\int x^nf'(x^n)dx$$ and eventually: $$=\sum_{i=0}^{\infty}\frac {(-n)^{i}x^{ni+1}} {(ni+1)(n(i-1)+1)...(n+1)(1)}f^{(i)}(x^n)+C$$ My question is twofold. First, did I make any glaring mistakes, and second, is this particular formula useful or novel?","During my studies in calc 2, I became fascinated by the integral $\int e^{-x^2}dx$ after hearing from the professor that it has no elementary function as its integral. I came up with an interesting technique to try the integral by using Integration by Parts like so: $$\int e^{-x^2}dx=xe^{-x^2}+2\int x^2e^{-x^2}dx $$ $$\int e^{-x^2}dx=xe^{-x^2}+\frac {2} {3}x^3e^{-x^2}+\int x^4e^{-x^2}dx$$ and continuing this until the pattern became obvious and I came up with the following equation: $$\int e^{-x^2}dx=e^{-x^2}\sum_{i=0}^{\infty}  \frac {2^i} {(2i+1)!!}x^{2i+1}+C$$ where $(2i+1)!!$ denotes the double factorial $(2n+1)(2n-1)(2n-3)...(2 \text{ or } 1)$.  From here, I thought about a generalized case for any infinitely differentiable function $f(x)$ and for an argument raised to any power $n$.  $$\int f(x^n)dx $$ $$u=f(x^n), du=nx^{n-1}f'(x^n), dv=dx, v=x$$ $$\int f(x^n)dx=xf(x^n)-n\int x^nf'(x^n)dx$$ and eventually: $$=\sum_{i=0}^{\infty}\frac {(-n)^{i}x^{ni+1}} {(ni+1)(n(i-1)+1)...(n+1)(1)}f^{(i)}(x^n)+C$$ My question is twofold. First, did I make any glaring mistakes, and second, is this particular formula useful or novel?",,"['calculus', 'integration', 'sequences-and-series']"
69,$\int\dfrac{dx}{x^2-a^2}$,,\int\dfrac{dx}{x^2-a^2},"For evaluating $\int \dfrac{dx}{x^2 - a^2}$, how can we make the substitution $x= a\sec \theta $ because $\sec \theta$ can be 1 and then that would give 1/0 form. So how can we do that and why does it work? Why not use $a\tan \theta$? And: $a^2 \sec^2 \theta$ misses the values less than $a^2$. What do we do about that?","For evaluating $\int \dfrac{dx}{x^2 - a^2}$, how can we make the substitution $x= a\sec \theta $ because $\sec \theta$ can be 1 and then that would give 1/0 form. So how can we do that and why does it work? Why not use $a\tan \theta$? And: $a^2 \sec^2 \theta$ misses the values less than $a^2$. What do we do about that?",,['calculus']
70,"Is there a nice way to evaluate $\iiint_{E}\, \frac{dx\,dy \,dz}{\sqrt{x^2+y^2+(z-b)^2}}$ where $E:x^2+y^2+z^2\leq a^2$",Is there a nice way to evaluate  where,"\iiint_{E}\, \frac{dx\,dy \,dz}{\sqrt{x^2+y^2+(z-b)^2}} E:x^2+y^2+z^2\leq a^2","Is there a nice way to evaluate $$\displaystyle\iiint_{E}\, \dfrac{dx\,dy\,dz}{\sqrt{x^2+y^2+(z-b)^2}}$$ where   $E:x^2+y^2+z^2\leq a^2$ with     $0<a<b$ If I use the standard spherical coordinates (Is there a better transformation?) $$x=p\sin(\phi)\cos(\theta) $$ $$y=p\sin(\phi)\sin(\theta)$$ $$z=p\cos(\phi)$$ $$|J|=p^2\sin(\phi)$$ $$\iiint_{E}\, \frac{dx\,dy \,dz}{\sqrt{x^2+y^2+(z-b)^2}} $$ $$=\int_{\phi=0}^{\pi}\,\int_{\theta=0}^{2\pi}\,\int_{p=0}^{a}\, \frac{p^2\sin(\phi)\,dp\,d\theta \,d\phi}{\sqrt{p^2\sin^2(\phi) + (p\cos(\phi)-b)^2}}$$ $$=\int_{\phi=0}^{\pi}\,\int_{\theta=0}^{2\pi}\,\int_{p=0}^{a}\, \frac{p^2\sin(\phi)\,dp\,d\theta \,d\phi}{\sqrt{(p-b\cos(\phi))^2 + b^2\sin^2(\phi)}}$$ Which I'm finding bit difficult.","Is there a nice way to evaluate $$\displaystyle\iiint_{E}\, \dfrac{dx\,dy\,dz}{\sqrt{x^2+y^2+(z-b)^2}}$$ where   $E:x^2+y^2+z^2\leq a^2$ with     $0<a<b$ If I use the standard spherical coordinates (Is there a better transformation?) $$x=p\sin(\phi)\cos(\theta) $$ $$y=p\sin(\phi)\sin(\theta)$$ $$z=p\cos(\phi)$$ $$|J|=p^2\sin(\phi)$$ $$\iiint_{E}\, \frac{dx\,dy \,dz}{\sqrt{x^2+y^2+(z-b)^2}} $$ $$=\int_{\phi=0}^{\pi}\,\int_{\theta=0}^{2\pi}\,\int_{p=0}^{a}\, \frac{p^2\sin(\phi)\,dp\,d\theta \,d\phi}{\sqrt{p^2\sin^2(\phi) + (p\cos(\phi)-b)^2}}$$ $$=\int_{\phi=0}^{\pi}\,\int_{\theta=0}^{2\pi}\,\int_{p=0}^{a}\, \frac{p^2\sin(\phi)\,dp\,d\theta \,d\phi}{\sqrt{(p-b\cos(\phi))^2 + b^2\sin^2(\phi)}}$$ Which I'm finding bit difficult.",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
71,"How to approximate the sum of a convergent, positive series","How to approximate the sum of a convergent, positive series",,"$$S=\sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}$$ This series converges because the general term goes to $0$ faster than $\dfrac1{n^2}$ I am asked to approximate the series with an error $R<10^{-3}$. How can I do this? I know that $$R=\left\lvert S-S_k\right\rvert=\left\lvert \sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}-\sum_{n=0}^{k}\frac1{e^n(n^2+1)}\right\rvert=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}$$ and that $$\int_{k+1}^\infty \frac1{e^x(x^2+1)}\mathrm dx\le\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\leq\int_{k}^\infty \frac1{e^x(x^2+1)}\mathrm dx$$ My attempt: $$R=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\le\frac1{(k+1)^2+1}\sum_{n=k+1}^{\infty}\frac1{e^n}\underbrace\le_{(1)}\frac1{(k+1)^2+1}\int_k^\infty \frac1{e^x}\mathrm dx= \frac1{(k+1)^2+1}\frac1{e^k}<10^{-3}\quad (*)$$ $(1):$ as the series is decreasing, positive and continuous, I can switch to the integral. $(*)$ is true for $k=4$. Is this a right approach? P.S. The exercise comes from Calculus Problems , 16.18, page 231.","$$S=\sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}$$ This series converges because the general term goes to $0$ faster than $\dfrac1{n^2}$ I am asked to approximate the series with an error $R<10^{-3}$. How can I do this? I know that $$R=\left\lvert S-S_k\right\rvert=\left\lvert \sum_{n=0}^{\infty}\frac1{e^n(n^2+1)}-\sum_{n=0}^{k}\frac1{e^n(n^2+1)}\right\rvert=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}$$ and that $$\int_{k+1}^\infty \frac1{e^x(x^2+1)}\mathrm dx\le\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\leq\int_{k}^\infty \frac1{e^x(x^2+1)}\mathrm dx$$ My attempt: $$R=\sum_{n=k+1}^{\infty}\frac1{e^n(n^2+1)}\le\frac1{(k+1)^2+1}\sum_{n=k+1}^{\infty}\frac1{e^n}\underbrace\le_{(1)}\frac1{(k+1)^2+1}\int_k^\infty \frac1{e^x}\mathrm dx= \frac1{(k+1)^2+1}\frac1{e^k}<10^{-3}\quad (*)$$ $(1):$ as the series is decreasing, positive and continuous, I can switch to the integral. $(*)$ is true for $k=4$. Is this a right approach? P.S. The exercise comes from Calculus Problems , 16.18, page 231.",,"['calculus', 'sequences-and-series', 'approximation']"
72,Evaluating $\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right)$,Evaluating,\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right),"I am trying to evaluate this integral, where $a<1$ $$\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right)$$ It is look obvious to enforce a substitution of $u=cos(x)$ because of it commonly appeared in the integral. $\sin(x)=\sqrt{1-u^2}$ $\mathrm du=-\sin(x)\mathrm dx=-\sqrt{1-u^2}\mathrm dx$ which lead to $$\int_{0}^{1}{\mathrm du\over (u+u^2)\sqrt{1-u^2}}\ln\left({1+au\over 1-au}\right)$$ This part $\ln\left({1+au\over 1-au}\right)$ still not simplify, so we make a substitution of $v=\ln\left({1+au\over 1-au}\right)$ which lead to after a very long process of simplification $$\int_{0}^{k}ve^{v\over 2}\cdot{e^v+1\over e^v-1}\cdot{\mathrm dv\over a(e^v+1)+e^v-1}$$ where $k=\ln\left({1+a\over 1-a}\right)$ This is now involving hyperbolic functions, but I don't how to use it.  Where $\tanh\left(v\over 2\right)={e^v-1\over e^v+1}$ $2e^v\sinh(v)=e^v-1$ $2e^v\cosh(v)=e^v+1$ This is how far I got to, unfortunately I can't continued. Can anyone please point me in the right direction to complete the calculation or help to evalaute the integral. Thank.","I am trying to evaluate this integral, where $a<1$ $$\int_{0}^{\pi\over 2}{\mathrm dx\over \cos(x)+\cos^2(x)}\ln\left({1+a\cos(x)\over 1-a\cos(x)}\right)$$ It is look obvious to enforce a substitution of $u=cos(x)$ because of it commonly appeared in the integral. $\sin(x)=\sqrt{1-u^2}$ $\mathrm du=-\sin(x)\mathrm dx=-\sqrt{1-u^2}\mathrm dx$ which lead to $$\int_{0}^{1}{\mathrm du\over (u+u^2)\sqrt{1-u^2}}\ln\left({1+au\over 1-au}\right)$$ This part $\ln\left({1+au\over 1-au}\right)$ still not simplify, so we make a substitution of $v=\ln\left({1+au\over 1-au}\right)$ which lead to after a very long process of simplification $$\int_{0}^{k}ve^{v\over 2}\cdot{e^v+1\over e^v-1}\cdot{\mathrm dv\over a(e^v+1)+e^v-1}$$ where $k=\ln\left({1+a\over 1-a}\right)$ This is now involving hyperbolic functions, but I don't how to use it.  Where $\tanh\left(v\over 2\right)={e^v-1\over e^v+1}$ $2e^v\sinh(v)=e^v-1$ $2e^v\cosh(v)=e^v+1$ This is how far I got to, unfortunately I can't continued. Can anyone please point me in the right direction to complete the calculation or help to evalaute the integral. Thank.",,[]
73,How can we find the equation of the curve as shown in the figure? [closed],How can we find the equation of the curve as shown in the figure? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Above are the illustrations, below in the red box is the QUESTION '$a$' and '$b$' are the length of the lines as shown in the figure. The two lines are divided into '$n$' equal parts and lines are drawn according to the illustration. The curve formed by the intersection of those lines is the curve with blue stroke. $\theta = \text{Angle between the two lines}$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Above are the illustrations, below in the red box is the QUESTION '$a$' and '$b$' are the length of the lines as shown in the figure. The two lines are divided into '$n$' equal parts and lines are drawn according to the illustration. The curve formed by the intersection of those lines is the curve with blue stroke. $\theta = \text{Angle between the two lines}$",,"['calculus', 'geometry']"
74,Why do we need the axiom of choice? [duplicate],Why do we need the axiom of choice? [duplicate],,"This question already has answers here : Axiom of Choice: What exactly is a choice, and when and why is it needed? (3 answers) Closed 6 years ago . I don't mean why is it important. I mean why can't we just define the ""selector function"" like $S\colon \mathbb{F} \to A, $ such that $S(X) = x \in X$, without an axiom? Why can't we do that but we can, for example, take some set that satisfies a condition from an uncountable family?","This question already has answers here : Axiom of Choice: What exactly is a choice, and when and why is it needed? (3 answers) Closed 6 years ago . I don't mean why is it important. I mean why can't we just define the ""selector function"" like $S\colon \mathbb{F} \to A, $ such that $S(X) = x \in X$, without an axiom? Why can't we do that but we can, for example, take some set that satisfies a condition from an uncountable family?",,"['calculus', 'analysis', 'axioms']"
75,Question on calculation of integral.,Question on calculation of integral.,,"I need to show this proof $$\int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx$$ if and only if $F$ has Lower variance than $G$ for the same mean of $F$ and $G$ and with support $[0,1]$ . Also I know that by the second order stochastic dominance definition $\int_0^x F(T)dT\le \int_0^x G(T)dT \iff \int u(x) dF(x) \ge \int u(x) dF(x)$ The solution is as follows: But I cannot reach from this integral $$\int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx$$ to this integral $U(F)= \int u(x)dF(x) \ge U(G)= \int u(x)dG(x)$ How can I get this last integral from the integral in yellow box? I asked this way.",I need to show this proof if and only if has Lower variance than for the same mean of and and with support . Also I know that by the second order stochastic dominance definition The solution is as follows: But I cannot reach from this integral to this integral How can I get this last integral from the integral in yellow box? I asked this way.,"\int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx F G F G [0,1] \int_0^x F(T)dT\le \int_0^x G(T)dT \iff \int u(x) dF(x) \ge \int u(x) dF(x) \int_0^1\int_0^xF(z)dzdx\le \int_0^1\int_0^xG(z)dzdx U(F)= \int u(x)dF(x) \ge U(G)= \int u(x)dG(x)","['calculus', 'integration']"
76,show this inequality $\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\frac{1}{x_{i}}+n\right)^n$,show this inequality,\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\frac{1}{x_{i}}+n\right)^n,"Let $x_{i}\ge 1$,show that $$\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n\right)^n$$ or  $$\left(\dfrac{\sum_{i=1}^{n}x_{i}+n}{\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n}\right)^n\ge \prod_{i=1}^{n}x_{i}$$ and it seem use AM-GM inequality? $$\sum_{i=1}^{n}x_{i}\ge n\sqrt[n]{x_{1}x_{2}\cdots x_{n}}$$ $$\sum_{i=1}^{n}\dfrac{1}{x_{i}}\ge \dfrac{n}{\sqrt[n]{x_{1}x_{2}\cdots x_{n}}}$$ let $\sqrt[n]{x_{1}x_{2}\cdots x_{n}}=t$,since  $$\Longleftrightarrow \left(\dfrac{t+1}{\frac{1}{t}+1}\right)^n\ge t^n$$But I can't it","Let $x_{i}\ge 1$,show that $$\left(\sum_{i=1}^{n}x_{i}+n\right)^n\ge \left(\prod_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n\right)^n$$ or  $$\left(\dfrac{\sum_{i=1}^{n}x_{i}+n}{\sum_{i=1}^{n}\dfrac{1}{x_{i}}+n}\right)^n\ge \prod_{i=1}^{n}x_{i}$$ and it seem use AM-GM inequality? $$\sum_{i=1}^{n}x_{i}\ge n\sqrt[n]{x_{1}x_{2}\cdots x_{n}}$$ $$\sum_{i=1}^{n}\dfrac{1}{x_{i}}\ge \dfrac{n}{\sqrt[n]{x_{1}x_{2}\cdots x_{n}}}$$ let $\sqrt[n]{x_{1}x_{2}\cdots x_{n}}=t$,since  $$\Longleftrightarrow \left(\dfrac{t+1}{\frac{1}{t}+1}\right)^n\ge t^n$$But I can't it",,"['calculus', 'multivariable-calculus', 'inequality', 'radicals']"
77,A multiplicative Taylor theorem?,A multiplicative Taylor theorem?,,"In a first calculus course it is often that one learns about the Taylor polynomials $$f(x) \approx \sum_{k=0}^{N}\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}$$ Which provide increasingly better approximation to a function which needs to be differentiable at the point as many times as the highest exponent. ($f^{(n)}(x_0)$ must exist). Now to my question. What happens if we replace the sum by a product? Is there some concept we can use to create a multiplicative refinement as contrary to an additive one? $$f(x) \approx \prod_{k=0}^N\mathcal F \{f,k,x_0\}(x)$$ Can we derive what this $\mathcal F$ thing (functional?) could be for this to make sense, and what must we demand of $f$ to hold on or around $x_0$?","In a first calculus course it is often that one learns about the Taylor polynomials $$f(x) \approx \sum_{k=0}^{N}\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}$$ Which provide increasingly better approximation to a function which needs to be differentiable at the point as many times as the highest exponent. ($f^{(n)}(x_0)$ must exist). Now to my question. What happens if we replace the sum by a product? Is there some concept we can use to create a multiplicative refinement as contrary to an additive one? $$f(x) \approx \prod_{k=0}^N\mathcal F \{f,k,x_0\}(x)$$ Can we derive what this $\mathcal F$ thing (functional?) could be for this to make sense, and what must we demand of $f$ to hold on or around $x_0$?",,"['calculus', 'functional-analysis', 'approximation']"
78,"What does it mean for a function to be ""Locally Bijective""?","What does it mean for a function to be ""Locally Bijective""?",,"In my calculus text, it states that a function $f$ can be integrated iff it is Locally Bijective . I'm wondering what this means, since given my (very elementary) understanding of what a bijective function is, I feel like this is somewhat inaccurate. For instance,  $f(x)=-x^2+1$ isn't bijective around x=0, but I can still integrate it in an interval containing $0$. Sorry if this is a really stupid question, just need some help!","In my calculus text, it states that a function $f$ can be integrated iff it is Locally Bijective . I'm wondering what this means, since given my (very elementary) understanding of what a bijective function is, I feel like this is somewhat inaccurate. For instance,  $f(x)=-x^2+1$ isn't bijective around x=0, but I can still integrate it in an interval containing $0$. Sorry if this is a really stupid question, just need some help!",,"['calculus', 'functions']"
79,Check if antiderivative is elementary,Check if antiderivative is elementary,,"I was experimenting with various integration techniques, and I stumbled upon the integral: $$ \int{\frac{\tan{x}}{x} dx} $$ I tried using a number of methods, but I could not solve it. Checking on WolframAlpha, I found that the integral does not have an elementary antiderivative. However, I am not satisfied with simply assuming it's non-elementary if I have difficulty in solving it. My question: Is there a test to determine if an antiderivative is elementary or not?","I was experimenting with various integration techniques, and I stumbled upon the integral: $$ \int{\frac{\tan{x}}{x} dx} $$ I tried using a number of methods, but I could not solve it. Checking on WolframAlpha, I found that the integral does not have an elementary antiderivative. However, I am not satisfied with simply assuming it's non-elementary if I have difficulty in solving it. My question: Is there a test to determine if an antiderivative is elementary or not?",,"['calculus', 'integration']"
80,Derivative of $x|x|$,Derivative of,x|x|,"I am trying to find the derivative of $f(x)=x|x|$ using the defition of derivative. For $x > 0$ I found that $f'(x)=2x$ and for $x<0$ the derivative is $f'(x)=-2x$. Everything is fine up to here. Now I want to check what happens when at $x=0$. By the way, I know that $|x|$ is not differentiable at $x=0$. So I am checking the left & right limits of $f$ when $x$ approaches $0$. $\lim_{x \to 0^-}\cfrac{x|x|}{x} = \lim_{x \to 0^-}\cfrac{x(-x)}{x}=\lim_{x \to 0^-}\cfrac{(-x)}{1} = -0? = 0. $ $\lim_{x \to 0^+}\cfrac{x|x|}{x} = \lim_{x \to 0^4}\cfrac{x(x)}{x}=\lim_{x \to 0^+}\cfrac{(x)}{1} = 0. $ I think that $f$ is not differentiable at $x=0$ since $|x|$ is not differentiable at that point. So , what do I do wrong? Should I write something like $\lim_{x \to 0^-}\cfrac{x|x|}{x} = -0^{-}$ and $\lim_{x \to 0^+}\cfrac{x|x|}{x} =0^{+}$ so that $f'$ does not exist at $x=0$?","I am trying to find the derivative of $f(x)=x|x|$ using the defition of derivative. For $x > 0$ I found that $f'(x)=2x$ and for $x<0$ the derivative is $f'(x)=-2x$. Everything is fine up to here. Now I want to check what happens when at $x=0$. By the way, I know that $|x|$ is not differentiable at $x=0$. So I am checking the left & right limits of $f$ when $x$ approaches $0$. $\lim_{x \to 0^-}\cfrac{x|x|}{x} = \lim_{x \to 0^-}\cfrac{x(-x)}{x}=\lim_{x \to 0^-}\cfrac{(-x)}{1} = -0? = 0. $ $\lim_{x \to 0^+}\cfrac{x|x|}{x} = \lim_{x \to 0^4}\cfrac{x(x)}{x}=\lim_{x \to 0^+}\cfrac{(x)}{1} = 0. $ I think that $f$ is not differentiable at $x=0$ since $|x|$ is not differentiable at that point. So , what do I do wrong? Should I write something like $\lim_{x \to 0^-}\cfrac{x|x|}{x} = -0^{-}$ and $\lim_{x \to 0^+}\cfrac{x|x|}{x} =0^{+}$ so that $f'$ does not exist at $x=0$?",,"['calculus', 'algebra-precalculus', 'derivatives']"
81,How can we know which function is greater than the other without drawing them?,How can we know which function is greater than the other without drawing them?,,"Two functions In other words, what is a a conventional way to know if the function $f(x)$ is greater than $g(x)$, taking into account that sometimes they exchange the highest position after intersecting. Thus a more specific question could be: How to know on each range, which function is greater than the other?","Two functions In other words, what is a a conventional way to know if the function $f(x)$ is greater than $g(x)$, taking into account that sometimes they exchange the highest position after intersecting. Thus a more specific question could be: How to know on each range, which function is greater than the other?",,"['calculus', 'integration', 'functions', 'graphing-functions', 'area']"
82,Why isn't it mathematically rigorous to treat dx's and dy's as variables? [duplicate],Why isn't it mathematically rigorous to treat dx's and dy's as variables? [duplicate],,This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . If I do something like: $$\frac{dy}{dx} = D$$ $$dy = D \times dx$$ People would often say that it is not rigorous to do so. But if we start from the definition of the derivative: $$\lim_{h \to 0}{\frac{f(x + h) - f(x)}{h}} = D$$ And by using the properties of limits we can say: $$\frac{\lim_{h \to 0}{f(x + h) - f(x)}}{\lim_{h \to 0}{h}} = D$$ And then finally: $$\lim_{h \to 0}(f(x + h) - f(x)) = D \times (\lim_{h \to 0} h)$$ Isn't this the same? Or am I missing something?,This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . If I do something like: $$\frac{dy}{dx} = D$$ $$dy = D \times dx$$ People would often say that it is not rigorous to do so. But if we start from the definition of the derivative: $$\lim_{h \to 0}{\frac{f(x + h) - f(x)}{h}} = D$$ And by using the properties of limits we can say: $$\frac{\lim_{h \to 0}{f(x + h) - f(x)}}{\lim_{h \to 0}{h}} = D$$ And then finally: $$\lim_{h \to 0}(f(x + h) - f(x)) = D \times (\lim_{h \to 0} h)$$ Isn't this the same? Or am I missing something?,,"['calculus', 'limits', 'derivatives', 'notation']"
83,Newton conjugate gradient algorithm,Newton conjugate gradient algorithm,,"In this video , the professor describes an algorithm that can be used to find the minimum value of the cost function for linear regression. Here, the cost function is $f$, the gradient is $g_k$ where $k$ is the $kth$ step of the algorithm, $\theta$ is the parameters we want to find to optimize the problem, $d_k$ is the value used to update $\theta$. Here is a screenshot of the slide for reference: Feel free to scroll down near the end for a slide describing what the Newton's algorithm is doing in more detail. My confusion comes from line 6 of the algorithm, the one about the line search. From my understanding of his explanation, the idea is that you increase the value of $\eta_k$ and each time you increase it, you compute your cost function $f$. The moment you get to the minimum, you stop and you use that $\eta_k$. I think this $\eta_k$ is essentially the learning rate you need to immediately go to the minimum. But then if that is the case, why would you need any iterations?  Wouldn't the linear search mean that after one step of the algorithm you're at the minimum? Second Question I have another question that I would love to have answered if possible. So in the previous slide, the professor shows that for the newton's algorithm for linear regression, the $\theta$ after one step is equal to the solution you get from the method of least squares in matrix form. In other words, he says that  you only need one step of the algorithm to get the optimal $\theta$. If this is the case, what is the point of showing us the iterative algorithm in the slide above? Is it because the matrix inverse is computationally expensive? The relevant slide for this question is below: And for those who are interested in what the Newton's algorithm is:","In this video , the professor describes an algorithm that can be used to find the minimum value of the cost function for linear regression. Here, the cost function is $f$, the gradient is $g_k$ where $k$ is the $kth$ step of the algorithm, $\theta$ is the parameters we want to find to optimize the problem, $d_k$ is the value used to update $\theta$. Here is a screenshot of the slide for reference: Feel free to scroll down near the end for a slide describing what the Newton's algorithm is doing in more detail. My confusion comes from line 6 of the algorithm, the one about the line search. From my understanding of his explanation, the idea is that you increase the value of $\eta_k$ and each time you increase it, you compute your cost function $f$. The moment you get to the minimum, you stop and you use that $\eta_k$. I think this $\eta_k$ is essentially the learning rate you need to immediately go to the minimum. But then if that is the case, why would you need any iterations?  Wouldn't the linear search mean that after one step of the algorithm you're at the minimum? Second Question I have another question that I would love to have answered if possible. So in the previous slide, the professor shows that for the newton's algorithm for linear regression, the $\theta$ after one step is equal to the solution you get from the method of least squares in matrix form. In other words, he says that  you only need one step of the algorithm to get the optimal $\theta$. If this is the case, what is the point of showing us the iterative algorithm in the slide above? Is it because the matrix inverse is computationally expensive? The relevant slide for this question is below: And for those who are interested in what the Newton's algorithm is:",,"['calculus', 'linear-algebra', 'optimization', 'numerical-methods', 'conjugate-gradient']"
84,A tricky limit (indeterminate form),A tricky limit (indeterminate form),,"While tutoring I came upon this limit. I know that this limit is obviously 1, but how would I show this formally $$\lim_{\eta\rightarrow\infty}[(2\eta + 5)^x-(2\eta)^x + 1]$$ where $x\in (0,1)$ I've tried logarithms etc.","While tutoring I came upon this limit. I know that this limit is obviously 1, but how would I show this formally $$\lim_{\eta\rightarrow\infty}[(2\eta + 5)^x-(2\eta)^x + 1]$$ where $x\in (0,1)$ I've tried logarithms etc.",,"['calculus', 'algebra-precalculus']"
85,Calculus optimization problem leads to a quartic polynomial - is there a better way?,Calculus optimization problem leads to a quartic polynomial - is there a better way?,,"I am tutoring a student in first-semester Calculus.  He needs to minimize the function $$f(x)=\frac{\sqrt{4+x^2}}{2}+\frac{\sqrt{1+(3-x)^2}}{4}$$ Taking the derivative and setting it equal to zero, we find (after some cleanup) the equation $$\frac{x}{\sqrt{4+x^2}}=\frac{3-x}{2\sqrt{1+(3-x)^2}}$$ At this point, it seems (to me) that the most likely avenue of solution is to square both sides, cross-multiply, and collect like terms.  This leads (after some heavy lifting) to the polynomial equation $$x^4-6x^3+9x^2+8x-12=0$$ Solving quartics is not a lot of fun, but in this case we get lucky:  plugging in $x=1$ we find that it is a solution, because $1-6+9+8-12=0$.  Phew!  And with hindsight, we can see that $\frac{1}{\sqrt{4+1^2}}$ and $\frac{3-1}{2\sqrt{1+(3-1)^2}}$ are both equal to $\frac{2}{\sqrt{5}}$. But it seems unlikely to me that this is the intended method of solution.  First of all, the algebra is considerably thornier than what the student has had to deal with prior to this question.  Second, solving a quartic equation seems way out of bounds for a first-semester Calculus class; it happens that in this case the coefficients sum to $0$, but I don't think a typical student would be expected to notice that. For all of the above reasons, I suspect that there is probably an easier way to solve this problem, but I am at a loss.  Is there some trick that I am missing?","I am tutoring a student in first-semester Calculus.  He needs to minimize the function $$f(x)=\frac{\sqrt{4+x^2}}{2}+\frac{\sqrt{1+(3-x)^2}}{4}$$ Taking the derivative and setting it equal to zero, we find (after some cleanup) the equation $$\frac{x}{\sqrt{4+x^2}}=\frac{3-x}{2\sqrt{1+(3-x)^2}}$$ At this point, it seems (to me) that the most likely avenue of solution is to square both sides, cross-multiply, and collect like terms.  This leads (after some heavy lifting) to the polynomial equation $$x^4-6x^3+9x^2+8x-12=0$$ Solving quartics is not a lot of fun, but in this case we get lucky:  plugging in $x=1$ we find that it is a solution, because $1-6+9+8-12=0$.  Phew!  And with hindsight, we can see that $\frac{1}{\sqrt{4+1^2}}$ and $\frac{3-1}{2\sqrt{1+(3-1)^2}}$ are both equal to $\frac{2}{\sqrt{5}}$. But it seems unlikely to me that this is the intended method of solution.  First of all, the algebra is considerably thornier than what the student has had to deal with prior to this question.  Second, solving a quartic equation seems way out of bounds for a first-semester Calculus class; it happens that in this case the coefficients sum to $0$, but I don't think a typical student would be expected to notice that. For all of the above reasons, I suspect that there is probably an easier way to solve this problem, but I am at a loss.  Is there some trick that I am missing?",,"['calculus', 'optimization']"
86,Verify my proof of $\lim_{x\to \infty} [f(x)+g(x)]= L+M$.,Verify my proof of .,\lim_{x\to \infty} [f(x)+g(x)]= L+M,"I am supposed to prove that $\lim_{x\to \infty} [f(x)+g(x)]= L+M$. Starting to realize I don't really understand the formal definition of a limit, although I do understand the general concept. Anyhow, so far I have: Given $\epsilon>0$, $\exists R_f,R_g$ s.t. $|f(x)-L|<\epsilon/2$ $\forall x>R_f$ and $|g(x)-M|<\epsilon/2$  $\forall x>R_g$ Choose $R>\max\{R_f,R_g\}$. (Do we want max, min, neither?) Suppose $x>R$. Then $x>R_f$ and $x>R_g$ whenever $|f(x)-L|<\epsilon/2$ and $|g(x)-M|<\epsilon/2$. Then $\left\vert\big(f(x)+g(x)\big)-(L+M)\right\vert$ = $\left\vert\big(f(x)-L\big)+\big(g(x)-M\big)\right\vert$ $\le$ $|f(x)-L|+|g(x)-M|$ This implies $|f(x)-L|+|g(x)-M|$ < $\epsilon/2 + \epsilon/2$ < $\epsilon$ QED. Am I headed in the right direction?","I am supposed to prove that $\lim_{x\to \infty} [f(x)+g(x)]= L+M$. Starting to realize I don't really understand the formal definition of a limit, although I do understand the general concept. Anyhow, so far I have: Given $\epsilon>0$, $\exists R_f,R_g$ s.t. $|f(x)-L|<\epsilon/2$ $\forall x>R_f$ and $|g(x)-M|<\epsilon/2$  $\forall x>R_g$ Choose $R>\max\{R_f,R_g\}$. (Do we want max, min, neither?) Suppose $x>R$. Then $x>R_f$ and $x>R_g$ whenever $|f(x)-L|<\epsilon/2$ and $|g(x)-M|<\epsilon/2$. Then $\left\vert\big(f(x)+g(x)\big)-(L+M)\right\vert$ = $\left\vert\big(f(x)-L\big)+\big(g(x)-M\big)\right\vert$ $\le$ $|f(x)-L|+|g(x)-M|$ This implies $|f(x)-L|+|g(x)-M|$ < $\epsilon/2 + \epsilon/2$ < $\epsilon$ QED. Am I headed in the right direction?",,"['calculus', 'limits', 'proof-verification', 'epsilon-delta']"
87,How to evaluate integral $\int_0^{\infty} e^{-x^2} \frac{\sin(a x)}{\sin(b x)} dx$?,How to evaluate integral ?,\int_0^{\infty} e^{-x^2} \frac{\sin(a x)}{\sin(b x)} dx,"I came across the following integral: $$\int_0^{\infty} e^{-x^2} \frac{\sin(a x)}{\sin(b x)} dx$$ while trying to calculate the inverse Laplace transform $$ L_p^{-1} \left[ \frac{\sinh(\alpha\sqrt{p})}{\sinh(\beta\sqrt{p})} \frac{e^{-\gamma\sqrt{p}}}{\sqrt{p}} \right], |\alpha|<\beta, \gamma>0$$ using the Bromwich integral approach. The contour I used is the following: the above mentioned integral arises while doing integration over the segments $L_1^+,L_2^+,\cdots$ and $L_1^-,L_2^-,\cdots$. I have searched for this integral in Prudnikov et. al., Integrals and Series, v.1, but found nothing. I have also tried to evaluate the integral using residue theorem, but could not quite decide which contour to use. Any help is greatly appreciated! P.S. The ILT can be calculated by noticing that $$ 	F[p] = \frac{\sinh (\sqrt{p} \alpha)}{\sinh (\sqrt{p} \beta)}           \frac{e^{-\gamma\sqrt{p}}}{\sqrt{p}}  				 = \sum_{n=0}^{\infty}  				 \left( 					 \frac{e^{-(-\alpha+\beta+\gamma+2n\beta)\sqrt{p}}}{\sqrt{p}} 					 -\frac{e^{-(\alpha+\beta+\gamma+2n\beta)\sqrt{p}}}{\sqrt{p}} 				 \right)$$ using $$L_p^{-1} \left[ \frac{e^{-\alpha\sqrt{p}}}{\sqrt{p}}  \right] = \frac{1}{\sqrt{\pi t}} e^{-\frac{\alpha^2}{4t}}$$ we get $$\begin{align*} 	f(t) 	&= L_p^{-1}[F(p)]   \\ 	&= \sum_{n=0}^{\infty}  	   \left( 			 \frac{ e^{-(-\alpha+\beta+\gamma+2n\beta)^2/4t} }{\sqrt{\pi t}} 			 - \frac{ e^{-(-\alpha+\beta+\gamma+2n\beta)^2/4t} }{\sqrt{\pi t}} 		 \right).  \end{align*}$$ Here I am more interested in calculating the above ILT using the Bromwich integral approach.","I came across the following integral: $$\int_0^{\infty} e^{-x^2} \frac{\sin(a x)}{\sin(b x)} dx$$ while trying to calculate the inverse Laplace transform $$ L_p^{-1} \left[ \frac{\sinh(\alpha\sqrt{p})}{\sinh(\beta\sqrt{p})} \frac{e^{-\gamma\sqrt{p}}}{\sqrt{p}} \right], |\alpha|<\beta, \gamma>0$$ using the Bromwich integral approach. The contour I used is the following: the above mentioned integral arises while doing integration over the segments $L_1^+,L_2^+,\cdots$ and $L_1^-,L_2^-,\cdots$. I have searched for this integral in Prudnikov et. al., Integrals and Series, v.1, but found nothing. I have also tried to evaluate the integral using residue theorem, but could not quite decide which contour to use. Any help is greatly appreciated! P.S. The ILT can be calculated by noticing that $$ 	F[p] = \frac{\sinh (\sqrt{p} \alpha)}{\sinh (\sqrt{p} \beta)}           \frac{e^{-\gamma\sqrt{p}}}{\sqrt{p}}  				 = \sum_{n=0}^{\infty}  				 \left( 					 \frac{e^{-(-\alpha+\beta+\gamma+2n\beta)\sqrt{p}}}{\sqrt{p}} 					 -\frac{e^{-(\alpha+\beta+\gamma+2n\beta)\sqrt{p}}}{\sqrt{p}} 				 \right)$$ using $$L_p^{-1} \left[ \frac{e^{-\alpha\sqrt{p}}}{\sqrt{p}}  \right] = \frac{1}{\sqrt{\pi t}} e^{-\frac{\alpha^2}{4t}}$$ we get $$\begin{align*} 	f(t) 	&= L_p^{-1}[F(p)]   \\ 	&= \sum_{n=0}^{\infty}  	   \left( 			 \frac{ e^{-(-\alpha+\beta+\gamma+2n\beta)^2/4t} }{\sqrt{\pi t}} 			 - \frac{ e^{-(-\alpha+\beta+\gamma+2n\beta)^2/4t} }{\sqrt{\pi t}} 		 \right).  \end{align*}$$ Here I am more interested in calculating the above ILT using the Bromwich integral approach.",,"['calculus', 'complex-analysis', 'definite-integrals', 'laplace-transform', 'contour-integration']"
88,semantics or do I have a gap in my understanding of multivariable limits?,semantics or do I have a gap in my understanding of multivariable limits?,,"Consider the limit $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2}$$ Now, many would argue  that: $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2} = \lim_{(x,y)\to (0,0)} \frac{(  x^2-y^2)( x^2+y^2)}{ x^2-y^2}$$ $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2} = \lim_{(x,y)\to (0,0)} (x^2+y^2) = 0$$ Yet, when I read the (or some, in some book) definition it says if $f(x,y)$ is a real function defined at every point  in  an open disk   containing $(a,b)$ excluding the point $(a,b)$ etc $\epsilon$... etc $\delta$.. If we accept the above definition, or something similar, then the above limit does not exist. However if we define a new function curing the issue, then we have a limit.. such as $$F(x,y)= \begin{cases} \frac{x^4-y^4}{x^2- y^2} &\text{if } x^2\ne y^2 ,\\  x^2+ y^2 &\text{if } x^2=y^2 \end{cases}$$ Then the limit for this function is ok. I just want to make sure I am not missing something. I ask because calculus professors often teach students that the limits must exists and  be equal along ""every"" path to hope for a limit. This breaks down along the path $y=x$.","Consider the limit $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2}$$ Now, many would argue  that: $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2} = \lim_{(x,y)\to (0,0)} \frac{(  x^2-y^2)( x^2+y^2)}{ x^2-y^2}$$ $$\lim_{(x,y)\to (0,0)} \frac{x^4-y^4}{x^2- y^2} = \lim_{(x,y)\to (0,0)} (x^2+y^2) = 0$$ Yet, when I read the (or some, in some book) definition it says if $f(x,y)$ is a real function defined at every point  in  an open disk   containing $(a,b)$ excluding the point $(a,b)$ etc $\epsilon$... etc $\delta$.. If we accept the above definition, or something similar, then the above limit does not exist. However if we define a new function curing the issue, then we have a limit.. such as $$F(x,y)= \begin{cases} \frac{x^4-y^4}{x^2- y^2} &\text{if } x^2\ne y^2 ,\\  x^2+ y^2 &\text{if } x^2=y^2 \end{cases}$$ Then the limit for this function is ok. I just want to make sure I am not missing something. I ask because calculus professors often teach students that the limits must exists and  be equal along ""every"" path to hope for a limit. This breaks down along the path $y=x$.",,"['calculus', 'limits', 'multivariable-calculus']"
89,What is so special about the Schwarz Inequality?,What is so special about the Schwarz Inequality?,,"I am studying Spivak's Calculus and the first two problem sets have  rather lengthy,but very interesting, work-throughs of three proofs for the Schwarz Inequality: $$\sum_{i=1}^{n} x_iy_i   \leq\sqrt{\sum_{i=1}^{n}x_{i}^{2}}\sqrt{\sum_{i=1}^{n}y_{i}^{2}}$$ Spivak calls this inequality the great-grandaddy of all inequalities, but leaves it at that. I, of course, consulted wikipedia Schwarz Inequality which lists very technical explanations, but the page is listed as being incomplete an/or without proper references. Which leads me to ask, on a more basic level, what is so special about the Schwarz Inequality?","I am studying Spivak's Calculus and the first two problem sets have  rather lengthy,but very interesting, work-throughs of three proofs for the Schwarz Inequality: $$\sum_{i=1}^{n} x_iy_i   \leq\sqrt{\sum_{i=1}^{n}x_{i}^{2}}\sqrt{\sum_{i=1}^{n}y_{i}^{2}}$$ Spivak calls this inequality the great-grandaddy of all inequalities, but leaves it at that. I, of course, consulted wikipedia Schwarz Inequality which lists very technical explanations, but the page is listed as being incomplete an/or without proper references. Which leads me to ask, on a more basic level, what is so special about the Schwarz Inequality?",,"['calculus', 'linear-algebra', 'analysis', 'inequality']"
90,It is possible to get a closed-form for $\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}$?,It is possible to get a closed-form for ?,\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2},"I think that will not be useful to compute the Apéry's constant as  $$\zeta(3)=\frac{4}{\pi}\sum_{n=1}^{\infty}\frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx+\frac{1}{3\pi}\left(\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}\right),$$ which is easily deduced from  $$\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx=\frac{1}{12}\left(\frac{3\pi}{n}-\sin(\frac{3\pi}{n})\right).$$ For deduce this last identity I've used an online tool of symbolic calculus. I say that couldn't be useful since neither I don't know how evaluate an alternative of the series of integrals $\sum_{n\geq 1} \frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx$ (if it is feasible, to compute in a distinct form). In any case I would like to know Question. It is know, or it is possible to get a closed-form for $$\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}$$ in terms of known constants? Thanks in advance. When I've do more computations, using some trigonometric tricks I've found also with $\sum_{n=1}^{\infty}\frac{\sin(\frac{\pi}{n})}{n^2}$ and  $\sum_{n=1}^{\infty}\frac{\sin(\frac{2\pi}{n})}{n^2}$. It is easy to check the absolute convergence of such series, thus by comparision test these series are convergents. Using another time Wolfram Alpha (its Online Series Calculator) I don't to get a closed-form (only an approximation is given for such series) in terms of known constant, this is as exact value. Are know these series? When I've used this computation in its (General) Calculator , yes  then I've obtained some closed-form, but the Series Calculator doesn't sure any closed-form. My attemps were using partial summation and Euler-MacLaurin (first approximation) , but I believe that I can not find this real value with these methods.","I think that will not be useful to compute the Apéry's constant as  $$\zeta(3)=\frac{4}{\pi}\sum_{n=1}^{\infty}\frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx+\frac{1}{3\pi}\left(\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}\right),$$ which is easily deduced from  $$\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx=\frac{1}{12}\left(\frac{3\pi}{n}-\sin(\frac{3\pi}{n})\right).$$ For deduce this last identity I've used an online tool of symbolic calculus. I say that couldn't be useful since neither I don't know how evaluate an alternative of the series of integrals $\sum_{n\geq 1} \frac{1}{n^2}\int_0^{\frac{\pi}{2n}}\sin^2(3x)dx$ (if it is feasible, to compute in a distinct form). In any case I would like to know Question. It is know, or it is possible to get a closed-form for $$\sum_{n=1}^{\infty}\frac{\sin(\frac{3\pi}{n})}{n^2}$$ in terms of known constants? Thanks in advance. When I've do more computations, using some trigonometric tricks I've found also with $\sum_{n=1}^{\infty}\frac{\sin(\frac{\pi}{n})}{n^2}$ and  $\sum_{n=1}^{\infty}\frac{\sin(\frac{2\pi}{n})}{n^2}$. It is easy to check the absolute convergence of such series, thus by comparision test these series are convergents. Using another time Wolfram Alpha (its Online Series Calculator) I don't to get a closed-form (only an approximation is given for such series) in terms of known constant, this is as exact value. Are know these series? When I've used this computation in its (General) Calculator , yes  then I've obtained some closed-form, but the Series Calculator doesn't sure any closed-form. My attemps were using partial summation and Euler-MacLaurin (first approximation) , but I believe that I can not find this real value with these methods.",,"['calculus', 'sequences-and-series']"
91,Gradient of vector field notation,Gradient of vector field notation,,"Working in 3D. I know that the gradient is a vector operator defined as $\nabla = [\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}]$. The gradient of a scalar scalar-valued function $f(\vec{x})\in\mathbb{R}$ is $\nabla f(\vec{x}) = [\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}]f(\vec{x}) = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}]$. This makes sense to me. But if we take the gradient of a vector field, say $\vec{f} = [f_1,f_2,f_3]$, I know that this is $\displaystyle \nabla \vec{f} = \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} & \frac{\partial f_1}{\partial z} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} & \frac{\partial f_2}{\partial z} \\ \frac{\partial f_3}{\partial x} & \frac{\partial f_3}{\partial y} & \frac{\partial f_3}{\partial z} \\ \end{bmatrix} $. But how did we get to this? Since both $\nabla$ and $\vec{f}$ are vectors, this seems a bit like an outer product, but writing $\nabla \otimes\vec{f}$ turns out to be the transpose of what I want i.e. $\displaystyle \nabla \otimes\vec{f}=\nabla\vec{f}^T =  \begin{bmatrix} \frac{\partial}{\partial x}\\ \frac{\partial}{\partial y}\\ \frac{\partial}{\partial z} \end{bmatrix} \begin{bmatrix} f_1 & f_2 & f_3 \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_2}{\partial x} & \frac{\partial f_3}{\partial x} \\ \frac{\partial f_1}{\partial y} & \frac{\partial f_2}{\partial y} & \frac{\partial f_3}{\partial y} \\ \frac{\partial f_1}{\partial z} & \frac{\partial f_2}{\partial z} & \frac{\partial f_3}{\partial z} \\ \end{bmatrix} $ Am I not understanding something correctly? What am I doing wrong?","Working in 3D. I know that the gradient is a vector operator defined as $\nabla = [\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}]$. The gradient of a scalar scalar-valued function $f(\vec{x})\in\mathbb{R}$ is $\nabla f(\vec{x}) = [\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}]f(\vec{x}) = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}]$. This makes sense to me. But if we take the gradient of a vector field, say $\vec{f} = [f_1,f_2,f_3]$, I know that this is $\displaystyle \nabla \vec{f} = \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} & \frac{\partial f_1}{\partial z} \\ \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y} & \frac{\partial f_2}{\partial z} \\ \frac{\partial f_3}{\partial x} & \frac{\partial f_3}{\partial y} & \frac{\partial f_3}{\partial z} \\ \end{bmatrix} $. But how did we get to this? Since both $\nabla$ and $\vec{f}$ are vectors, this seems a bit like an outer product, but writing $\nabla \otimes\vec{f}$ turns out to be the transpose of what I want i.e. $\displaystyle \nabla \otimes\vec{f}=\nabla\vec{f}^T =  \begin{bmatrix} \frac{\partial}{\partial x}\\ \frac{\partial}{\partial y}\\ \frac{\partial}{\partial z} \end{bmatrix} \begin{bmatrix} f_1 & f_2 & f_3 \end{bmatrix} = \begin{bmatrix} \frac{\partial f_1}{\partial x} & \frac{\partial f_2}{\partial x} & \frac{\partial f_3}{\partial x} \\ \frac{\partial f_1}{\partial y} & \frac{\partial f_2}{\partial y} & \frac{\partial f_3}{\partial y} \\ \frac{\partial f_1}{\partial z} & \frac{\partial f_2}{\partial z} & \frac{\partial f_3}{\partial z} \\ \end{bmatrix} $ Am I not understanding something correctly? What am I doing wrong?",,"['calculus', 'multivariable-calculus', 'vectors', 'tensor-products', 'tensors']"
92,How to calculate the area of a hyperplane,How to calculate the area of a hyperplane,,"Anyone knows how to calculate the area of a hyperplane defined by $\sum\limits_{i = 1}^n x_i  = a$ with restriction $0 \leqslant x_i \leqslant a$ by integration, or provide a reference? Thank you! PS : I know how to compute the special case when $n=3$ using surface integral. I don't know how to do this in general. In particular I want to solve the following problem. Calculate the area $A$ defined by $\sum\limits_{i = 1}^k x_i\sqrt {np_i q_i} = 0$ with restriction $ - \sqrt {\frac{np_i}{q_i}}  \leqslant {x_i} \leqslant \sqrt {\frac{nq_i}{p_i}} $ where $n\in\Bbb{N^+}$ is a fixed positive integer, and $p_i, q_i\in[0,1]$ are also fixed constants for $i=1,2,\ldots,k$. The answer is $A = \frac{\sqrt {n^{k-1}} }{(k - 1)!} \sqrt {\frac{\sum\limits_{i=1}^k p_i q_i }{\prod\limits_{i = 1}^k p_i q_i }} $. This is from a probability textbook. $n$ is the number of independent trials, $k$ is the number of possible outcome of each trial, with the $i$th outcome having probability $p_i$. $q_i$ is defined as $q_i=1-p_i$. All possible outcomes of the $n$-time independent trials are contained in the hyperplane $A$. The area is then used to prove the ""integral limit theorem"". The textbook states ""it is easy to verify this area by integration"", but I have no idea how to do this. Hope someone can help!","Anyone knows how to calculate the area of a hyperplane defined by $\sum\limits_{i = 1}^n x_i  = a$ with restriction $0 \leqslant x_i \leqslant a$ by integration, or provide a reference? Thank you! PS : I know how to compute the special case when $n=3$ using surface integral. I don't know how to do this in general. In particular I want to solve the following problem. Calculate the area $A$ defined by $\sum\limits_{i = 1}^k x_i\sqrt {np_i q_i} = 0$ with restriction $ - \sqrt {\frac{np_i}{q_i}}  \leqslant {x_i} \leqslant \sqrt {\frac{nq_i}{p_i}} $ where $n\in\Bbb{N^+}$ is a fixed positive integer, and $p_i, q_i\in[0,1]$ are also fixed constants for $i=1,2,\ldots,k$. The answer is $A = \frac{\sqrt {n^{k-1}} }{(k - 1)!} \sqrt {\frac{\sum\limits_{i=1}^k p_i q_i }{\prod\limits_{i = 1}^k p_i q_i }} $. This is from a probability textbook. $n$ is the number of independent trials, $k$ is the number of possible outcome of each trial, with the $i$th outcome having probability $p_i$. $q_i$ is defined as $q_i=1-p_i$. All possible outcomes of the $n$-time independent trials are contained in the hyperplane $A$. The area is then used to prove the ""integral limit theorem"". The textbook states ""it is easy to verify this area by integration"", but I have no idea how to do this. Hope someone can help!",,"['calculus', 'probability', 'multivariable-calculus']"
93,Evaluate the integral: $\int\frac{xe^{2x}}{(1+2x)^2}\ dx$,Evaluate the integral:,\int\frac{xe^{2x}}{(1+2x)^2}\ dx,"$\int\frac{xe^{2x}}{(1+2x)^2}\ dx$ This an integration by parts problem. I am asking for assistance with my method. I was taught to use the LIATE (Logarithmic function, Inverse Trig Function, Algebraic Function, Trigonometric Function, and Exponential function)  mnemonic to decide which function would be ""u"" and ""dv"" so as to be utilized in the following formula: $\int udv= uv-\int v\ du$ u would be what ever first letter of a function which came first in the mnemonic and dv would be the latter. Now this problem is not fitting into my instructions, and I am bit confused as to how to solve it. Please do not leave answers without a clear written explanation in clear English.  You do not have to solve the problem. I just would like to know if my method is correct. No disrespect intended.","$\int\frac{xe^{2x}}{(1+2x)^2}\ dx$ This an integration by parts problem. I am asking for assistance with my method. I was taught to use the LIATE (Logarithmic function, Inverse Trig Function, Algebraic Function, Trigonometric Function, and Exponential function)  mnemonic to decide which function would be ""u"" and ""dv"" so as to be utilized in the following formula: $\int udv= uv-\int v\ du$ u would be what ever first letter of a function which came first in the mnemonic and dv would be the latter. Now this problem is not fitting into my instructions, and I am bit confused as to how to solve it. Please do not leave answers without a clear written explanation in clear English.  You do not have to solve the problem. I just would like to know if my method is correct. No disrespect intended.",,['calculus']
94,Differentiability in R^n,Differentiability in R^n,,"If $f: U \rightarrow \mathbb R^n$ ($U \subset \mathbb R^m$ is an open set) is differentiable and $f(x) \neq 0$ $\forall x \in U$ $\Rightarrow$ $\varphi: U \rightarrow \mathbb R$, $\varphi(x) = \frac {1}{||f(x)||}$ is differentiable. I know how to show that $\varphi$ is differentiable, but I'm having problems to find the differential $\varphi'(x).v$, $\forall v \in \mathbb R^m$.","If $f: U \rightarrow \mathbb R^n$ ($U \subset \mathbb R^m$ is an open set) is differentiable and $f(x) \neq 0$ $\forall x \in U$ $\Rightarrow$ $\varphi: U \rightarrow \mathbb R$, $\varphi(x) = \frac {1}{||f(x)||}$ is differentiable. I know how to show that $\varphi$ is differentiable, but I'm having problems to find the differential $\varphi'(x).v$, $\forall v \in \mathbb R^m$.",,['calculus']
95,Evaluate $\int_{-1}^{1}\int_{x}^{2x-1}dydx$,Evaluate,\int_{-1}^{1}\int_{x}^{2x-1}dydx,$$\int_{-1}^{1}\int_{x}^{2x-1}dydx$$ $$ My attempt: $$I_1:=\int_{-1}^{1}\int_{x}^{2x-1}dydx$$ $$=\int_{-1}^{1}\bigg[\int_{x}^{2x-1}dy\bigg]dx$$ $$=\int_{-1}^{1}\bigg(x-1\bigg)dx=\boxed{\color{red}{-2}}$$ Now I need to evaluate with changing the limis: $$I_2:=\int_{-3}^{-1}\int_{-1}^{\frac{y+1}{2}}dxdy+\int_{-1}^{1}\int_{y}^{\frac{y+1}{2}}dxdy$$ 1) Is it ok that before changing the limits the area is negative? 2) The new limits that I have done are correct?,$$\int_{-1}^{1}\int_{x}^{2x-1}dydx$$ $$ My attempt: $$I_1:=\int_{-1}^{1}\int_{x}^{2x-1}dydx$$ $$=\int_{-1}^{1}\bigg[\int_{x}^{2x-1}dy\bigg]dx$$ $$=\int_{-1}^{1}\bigg(x-1\bigg)dx=\boxed{\color{red}{-2}}$$ Now I need to evaluate with changing the limis: $$I_2:=\int_{-3}^{-1}\int_{-1}^{\frac{y+1}{2}}dxdy+\int_{-1}^{1}\int_{y}^{\frac{y+1}{2}}dxdy$$ 1) Is it ok that before changing the limits the area is negative? 2) The new limits that I have done are correct?,,['calculus']
96,Equality case in elementary form of Holder's Inequality,Equality case in elementary form of Holder's Inequality,,"A well known elementary formulation of Holder's Inequality can be stated as follows: Let $a_{ij}$ for $i = 1, 2, \dots, k; j = 1, 2, \dots, n$ be positive real numbers, and let $p_1, p_2, \dots, p_k$ be positive real numbers such that $\sum_{i=1}^{k} \frac{1}{p_i} = 1$.  Then we have $$\sum_{j = 1}^{n} \prod_{i = 1}^{k} a_{ij} \leq \prod_{i=1}^{k} \left ( \sum_{j = 1}^{n} a_{ij}^{p_i} \right )^{\frac{1}{p_i}}.$$ Does anyone know when equality occurs in this inequality?  Any insights on the equality case and/or a proof would be appreciated. I have seen several proofs of the two sequence case.  Here is one, from Cvetkovski's Inequalities : Perhaps this argument can be generalized to prove the above result.  Thanks in advance!","A well known elementary formulation of Holder's Inequality can be stated as follows: Let $a_{ij}$ for $i = 1, 2, \dots, k; j = 1, 2, \dots, n$ be positive real numbers, and let $p_1, p_2, \dots, p_k$ be positive real numbers such that $\sum_{i=1}^{k} \frac{1}{p_i} = 1$.  Then we have $$\sum_{j = 1}^{n} \prod_{i = 1}^{k} a_{ij} \leq \prod_{i=1}^{k} \left ( \sum_{j = 1}^{n} a_{ij}^{p_i} \right )^{\frac{1}{p_i}}.$$ Does anyone know when equality occurs in this inequality?  Any insights on the equality case and/or a proof would be appreciated. I have seen several proofs of the two sequence case.  Here is one, from Cvetkovski's Inequalities : Perhaps this argument can be generalized to prove the above result.  Thanks in advance!",,"['calculus', 'algebra-precalculus', 'inequality', 'optimization']"
97,Second order differential equation with a variable coefficient. Show |f(x)| is bounded.,Second order differential equation with a variable coefficient. Show |f(x)| is bounded.,,"Was given this question as extra credit on an ODE exam. Didn't have time during the exam to consider it, but I have since then, and I'm stumped. $$ f''(x) + f(x) = -f'(x)x^{2015}$$ $f(x)$ is twice differentiable and continuous everywhere. Show that $|f(x)| < M $ for some real $M$. Hint: compute the derivative of $f'(x)^2 + f(x)^2$. I wrote this from memory, and I hope its correct or I've waiting quite a bit of time trying to figure this out. As per the hint, I compute the derivative and found that $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = 2f''(x)f'(x) + 2f'(x)f(x) = 2f'(x)[f(x) + f''(x)] $$ So, that would be $2f'(x)$ multiplied by the left hand side of the given differential equation. In other words, if we multiply through by $2f'(x)$, we have $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -f'(x)x^{2015}*(2f'(x)) $$  $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -2f'(x)^2x^{2015} $$ I didn't manage to produce anything fruitful from here. I tried to integrate both sides, but could not solve or draw any conclusions from the right hand side. I tried to isolate $x^{2015}$ and then integrate, but I couldn't solve the left hand integral. At this point, I was pretty stumped and tried a bunch of other ideas. I tried to solve the homogeneous system, to no avail. Then there was a fun attempt at creating a system of first order equations that left me with a, predictably, non-constant coefficient matrix. At that point, I decided to step away and ask for some help. Am I headed in the right direction with either of these attempts? If it helps, this is supposed to require only knowledge of calculus, so I'm certain the key lies above, especially considering the hint, but I'm not sure where to proceed. Solutions and hints equally appreciated! EDIT: Forgot to mention, $f(x)$ is twice differentiable and continuous everywhere. Will include at top. Update: Focused on studying for final exams but I will return to this! Still stumped.","Was given this question as extra credit on an ODE exam. Didn't have time during the exam to consider it, but I have since then, and I'm stumped. $$ f''(x) + f(x) = -f'(x)x^{2015}$$ $f(x)$ is twice differentiable and continuous everywhere. Show that $|f(x)| < M $ for some real $M$. Hint: compute the derivative of $f'(x)^2 + f(x)^2$. I wrote this from memory, and I hope its correct or I've waiting quite a bit of time trying to figure this out. As per the hint, I compute the derivative and found that $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = 2f''(x)f'(x) + 2f'(x)f(x) = 2f'(x)[f(x) + f''(x)] $$ So, that would be $2f'(x)$ multiplied by the left hand side of the given differential equation. In other words, if we multiply through by $2f'(x)$, we have $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -f'(x)x^{2015}*(2f'(x)) $$  $$ \frac{d}{dx}[f'(x)^2 + f(x)^2] = -2f'(x)^2x^{2015} $$ I didn't manage to produce anything fruitful from here. I tried to integrate both sides, but could not solve or draw any conclusions from the right hand side. I tried to isolate $x^{2015}$ and then integrate, but I couldn't solve the left hand integral. At this point, I was pretty stumped and tried a bunch of other ideas. I tried to solve the homogeneous system, to no avail. Then there was a fun attempt at creating a system of first order equations that left me with a, predictably, non-constant coefficient matrix. At that point, I decided to step away and ask for some help. Am I headed in the right direction with either of these attempts? If it helps, this is supposed to require only knowledge of calculus, so I'm certain the key lies above, especially considering the hint, but I'm not sure where to proceed. Solutions and hints equally appreciated! EDIT: Forgot to mention, $f(x)$ is twice differentiable and continuous everywhere. Will include at top. Update: Focused on studying for final exams but I will return to this! Still stumped.",,"['calculus', 'ordinary-differential-equations']"
98,Maximum area of a isosceles triangle in a circle with a radius r,Maximum area of a isosceles triangle in a circle with a radius r,,"As said in the title, I'm looking for the maximum area of a isosceles triangle in a circle with a radius $r$. I've split the isosceles triangle in two, and I solve for the area $A=\frac{bh}{2}$*. I have made my base $x$, and solve for the height by using the Pythagorean theorem of the smaller triangle (seen in picture). $h=r+\sqrt{r^2-x^2}$ So my the formula, I think, for both triangles should be $A=x(r+\sqrt{r^2-x^2})$ But after I solved for the derivative, when put ""$= 0$"", and checked on my calculator, I got the maximum to be about $4.3301r$, which differs a lot from my book's answer of $\frac{3\sqrt{3}}{4}r^2$*. Is my formula for the area right? Am I going about this the wrong way, or is it just my derivative that is wrong? Thanks in advance * Edited from original post $A=rx+x\sqrt{r^2-x^2}$ $A'=r+x(\frac{1}{2})(r^2-x^2)^{-\frac{1}{2}}(-2x)+\sqrt{r^2-x^2}$ $r+\sqrt{r^2-x^2}=\frac{x^2}{\sqrt{r^2-x^2}}$ $r\sqrt{r^2-x^2}+(r^2-x^2)=x^2$ $r^2+r\sqrt{r^2-x^2}=2x^2$ $r^2(r^2-x^2)=(2x^2-r^2)^2$ $r^4-r^2x^2=4x^2-4x^2r^2+r^4$ $4x^4=3x^2r^2$ $x=\frac{\sqrt{3}}{2}r$","As said in the title, I'm looking for the maximum area of a isosceles triangle in a circle with a radius $r$. I've split the isosceles triangle in two, and I solve for the area $A=\frac{bh}{2}$*. I have made my base $x$, and solve for the height by using the Pythagorean theorem of the smaller triangle (seen in picture). $h=r+\sqrt{r^2-x^2}$ So my the formula, I think, for both triangles should be $A=x(r+\sqrt{r^2-x^2})$ But after I solved for the derivative, when put ""$= 0$"", and checked on my calculator, I got the maximum to be about $4.3301r$, which differs a lot from my book's answer of $\frac{3\sqrt{3}}{4}r^2$*. Is my formula for the area right? Am I going about this the wrong way, or is it just my derivative that is wrong? Thanks in advance * Edited from original post $A=rx+x\sqrt{r^2-x^2}$ $A'=r+x(\frac{1}{2})(r^2-x^2)^{-\frac{1}{2}}(-2x)+\sqrt{r^2-x^2}$ $r+\sqrt{r^2-x^2}=\frac{x^2}{\sqrt{r^2-x^2}}$ $r\sqrt{r^2-x^2}+(r^2-x^2)=x^2$ $r^2+r\sqrt{r^2-x^2}=2x^2$ $r^2(r^2-x^2)=(2x^2-r^2)^2$ $r^4-r^2x^2=4x^2-4x^2r^2+r^4$ $4x^4=3x^2r^2$ $x=\frac{\sqrt{3}}{2}r$",,"['calculus', 'optimization']"
99,Why are infinite sums so much harder to calculate than the associated infinite integral?,Why are infinite sums so much harder to calculate than the associated infinite integral?,,"It seems that with continuous functions, we have in calculus an apparatus for ""short cutting"" an infinite sum. However, when we move to the discrete case, it seems that we don't have the equivalent theoretical machinery. What is it about sums that makes them so much more intractable than their associated integrals? In other words, why isn't there a fundamental theorem of infinite sums like there is for calculus?","It seems that with continuous functions, we have in calculus an apparatus for ""short cutting"" an infinite sum. However, when we move to the discrete case, it seems that we don't have the equivalent theoretical machinery. What is it about sums that makes them so much more intractable than their associated integrals? In other words, why isn't there a fundamental theorem of infinite sums like there is for calculus?",,"['calculus', 'soft-question']"
