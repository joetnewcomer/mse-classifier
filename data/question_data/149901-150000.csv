,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof of an inequality by induction: $(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + x_1 + x_2 + ... + x_n$,Proof of an inequality by induction:,(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + x_1 + x_2 + ... + x_n,"Let $n \in \mathbb N^+$. Show that if $x_1, x_2, ... , x_n$ are $n$ real numbers such that $-1 \le x_i \le 0$ for each $1 \le i \le n$, then $$(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + x_1 + x_2 + ... + x_n$$ I am using a proof by induction, but I am unable to complete it. I would appreciate any tips on how I could finish the proof. Incomplete Proof Let $P(n)$ be the proposition that $(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + \sum_{i = 1} ^ n x_i$. When $n = 1$, LHS = RHS = $1 + x_i$. Clearly $1 + x_i \ge 1 + x_i$ and so $P(1)$ is true. Assume that $P(k)$ is true, i.e. $(1 + x_1)(1 + x_2)...(1 + x_k) \ge 1 + \sum_{i=1}^k x_i$. Case 1 Suppose $x_{k + 1}  = -1$. Then $1 + x_{k + 1} = 0$, hence LHS = $(1 + x_1)(1 + x_2)...(1 + x_k)(1 + x_{k+1}) = 0$. RHS = $1 + x_1 + x_2 + ... + x_k - 1 = x_1 + x_2 + ... + x_k$ which is always nonpositive since each $x_i$ is nonpositive. Hence RHS $\le 0$ and so LHS $\ge$ RHS. Case 2 Suppose $x_{k+1} = 0$. Then $1 + x_{k+1} = 1$, hence multiplying it to LHS does not change anything. Also, adding $x_{k+1}$ to RHS does not change anything. Therefore, LHS $\ge$ RHS still holds. Case 3 Suppose $-1 < x_{k+1} < 0$. Then $0 < 1 + x_{k+1} < 1$. Adding $x_{k+1}$ to RHS makes the RHS smaller since $x_{k+1}$ is negative. If LHS $= 0$, then multiplying $1 + x_{k+1}$ to LHS does not change anything, and so LHS $\ge$ RHS holds. If LHS > $0$, then multiplying $1 + x_{k+1}$ to LHS makes the LHS smaller since $0 < 1 + x_{k+1} < 1$. In this case LHS $\ge$ RHS does not necessarily hold? ...","Let $n \in \mathbb N^+$. Show that if $x_1, x_2, ... , x_n$ are $n$ real numbers such that $-1 \le x_i \le 0$ for each $1 \le i \le n$, then $$(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + x_1 + x_2 + ... + x_n$$ I am using a proof by induction, but I am unable to complete it. I would appreciate any tips on how I could finish the proof. Incomplete Proof Let $P(n)$ be the proposition that $(1 + x_1)(1 + x_2)...(1 + x_n) \ge 1 + \sum_{i = 1} ^ n x_i$. When $n = 1$, LHS = RHS = $1 + x_i$. Clearly $1 + x_i \ge 1 + x_i$ and so $P(1)$ is true. Assume that $P(k)$ is true, i.e. $(1 + x_1)(1 + x_2)...(1 + x_k) \ge 1 + \sum_{i=1}^k x_i$. Case 1 Suppose $x_{k + 1}  = -1$. Then $1 + x_{k + 1} = 0$, hence LHS = $(1 + x_1)(1 + x_2)...(1 + x_k)(1 + x_{k+1}) = 0$. RHS = $1 + x_1 + x_2 + ... + x_k - 1 = x_1 + x_2 + ... + x_k$ which is always nonpositive since each $x_i$ is nonpositive. Hence RHS $\le 0$ and so LHS $\ge$ RHS. Case 2 Suppose $x_{k+1} = 0$. Then $1 + x_{k+1} = 1$, hence multiplying it to LHS does not change anything. Also, adding $x_{k+1}$ to RHS does not change anything. Therefore, LHS $\ge$ RHS still holds. Case 3 Suppose $-1 < x_{k+1} < 0$. Then $0 < 1 + x_{k+1} < 1$. Adding $x_{k+1}$ to RHS makes the RHS smaller since $x_{k+1}$ is negative. If LHS $= 0$, then multiplying $1 + x_{k+1}$ to LHS does not change anything, and so LHS $\ge$ RHS holds. If LHS > $0$, then multiplying $1 + x_{k+1}$ to LHS makes the LHS smaller since $0 < 1 + x_{k+1} < 1$. In this case LHS $\ge$ RHS does not necessarily hold? ...",,"['analysis', 'inequality', 'induction']"
1,Arzela-Ascoli Anthony Knapp Proof,Arzela-Ascoli Anthony Knapp Proof,,"STATEMENT: (Arzela Ascoli Theorem) If $\left\{f_n\right\}$ is an equicontinuous family of scalar-valued functions defined on a compact Hausdorff space $X$ and if $\left\{f_n\right\}$ has the property that $\left\{f_n(x)\right\}$ is bounded for each $x$, then $\left\{f_n\right\}$ has a uniformly convergent subsequence. Proof: We may assume that there are infinitely many distinct functions $f_n$, since otherwise the assertion is trivial. Let $|f_n(x)|\leq c_x$ for all $n$, and form the product space $C=\prod_{x\in X}\left\{z\in \mathbb{C}\mid |z|\leq c_x\right\} $. The space $C$ is compact by Tychonoff theorem, and we are now assuming that there are infinitely many members of the sequence $\left\{f_n\right\}$ in the space. Let $S$ be the image of the sequences as a subset of $C$. If $S$ were to contain all its limit points, then each $f_n$ would have an open neighborhood in $C$ disjoint from the rest of $S$; these open sets and $S^c$ would form an open cover of $C$ with no finite sub cover, in contradiction to compactness of $C$. Thus $S$ has a limit point $f$ not in $S$. By Lemma 10.47 and the remarks before it, the family $S\cup \left\{f\right\}$ is equicontinuous. Lemma 10.47: Let $\mathcal{F}=\left\{f_\alpha\right\}$ is equicontinuous at $x$ in $X$, then the closure $\mathcal{F}^{cl}$ of $\mathcal{F}$ in the product topology on $\mathbb{C}^X$ is equicontinuous at $x$. QUESTION: How does Knapp conclude that each $f_n$ would have an open neighborhood in $C$ disjoint from $S$.","STATEMENT: (Arzela Ascoli Theorem) If $\left\{f_n\right\}$ is an equicontinuous family of scalar-valued functions defined on a compact Hausdorff space $X$ and if $\left\{f_n\right\}$ has the property that $\left\{f_n(x)\right\}$ is bounded for each $x$, then $\left\{f_n\right\}$ has a uniformly convergent subsequence. Proof: We may assume that there are infinitely many distinct functions $f_n$, since otherwise the assertion is trivial. Let $|f_n(x)|\leq c_x$ for all $n$, and form the product space $C=\prod_{x\in X}\left\{z\in \mathbb{C}\mid |z|\leq c_x\right\} $. The space $C$ is compact by Tychonoff theorem, and we are now assuming that there are infinitely many members of the sequence $\left\{f_n\right\}$ in the space. Let $S$ be the image of the sequences as a subset of $C$. If $S$ were to contain all its limit points, then each $f_n$ would have an open neighborhood in $C$ disjoint from the rest of $S$; these open sets and $S^c$ would form an open cover of $C$ with no finite sub cover, in contradiction to compactness of $C$. Thus $S$ has a limit point $f$ not in $S$. By Lemma 10.47 and the remarks before it, the family $S\cup \left\{f\right\}$ is equicontinuous. Lemma 10.47: Let $\mathcal{F}=\left\{f_\alpha\right\}$ is equicontinuous at $x$ in $X$, then the closure $\mathcal{F}^{cl}$ of $\mathcal{F}$ in the product topology on $\mathbb{C}^X$ is equicontinuous at $x$. QUESTION: How does Knapp conclude that each $f_n$ would have an open neighborhood in $C$ disjoint from $S$.",,"['real-analysis', 'analysis']"
2,"If $f$ and $g$ are continuous, then max(f, g) is continuous and differentiable","If  and  are continuous, then max(f, g) is continuous and differentiable",f g,"If $f$ and $g$ are continuous on $[a, b]$ and differentiable on $(a, b)$, then $\max(f, g)$ is continuous on $[a, b]$ and differentiable on $(a, b)$. I'm asked to either prove or disprove this statement.  So far I'm thinking that it's true because if a function is continuous and differentiable on an interval, it takes on it's max/min over the interval, but I don't know if this is correct, and if it is, I don't know how to prove it.  Any help would be appreciated.","If $f$ and $g$ are continuous on $[a, b]$ and differentiable on $(a, b)$, then $\max(f, g)$ is continuous on $[a, b]$ and differentiable on $(a, b)$. I'm asked to either prove or disprove this statement.  So far I'm thinking that it's true because if a function is continuous and differentiable on an interval, it takes on it's max/min over the interval, but I don't know if this is correct, and if it is, I don't know how to prove it.  Any help would be appreciated.",,"['real-analysis', 'analysis']"
3,An empty subdifferential,An empty subdifferential,,"Can you give me an example of function $f$ defined on an Hilbert space, real valued (extended with $+ \infty$), lower semi continuous, convex and proper for which $\operatorname{dom}(\partial f)= \emptyset$? For $\operatorname{dom}(\partial f)$ I mean the domain of subdifferentiability of $f$.","Can you give me an example of function $f$ defined on an Hilbert space, real valued (extended with $+ \infty$), lower semi continuous, convex and proper for which $\operatorname{dom}(\partial f)= \emptyset$? For $\operatorname{dom}(\partial f)$ I mean the domain of subdifferentiability of $f$.",,"['analysis', 'convex-analysis', 'hilbert-spaces']"
4,Subsequence and Accumulation Point Proof,Subsequence and Accumulation Point Proof,,"Suppose x is an accumulation point of {$a_n : n \in J$}. Show that there is a subsequence of {${a_n : n \in J}$}$_{n=1}^\infty$ that converges to x. I understand the general idea behind the problem that if you take a neighborhood (x - $\epsilon$, x + $\epsilon$) and examine an area of that neighborhood from (x - $\epsilon$, x) with x being the supremum, as well as the accumulation point of the original set, there will be infinitely points in that interval converging to x. I'm just not sure how to define the necessary subsequence. Any help would be greatly appreciated.","Suppose x is an accumulation point of {$a_n : n \in J$}. Show that there is a subsequence of {${a_n : n \in J}$}$_{n=1}^\infty$ that converges to x. I understand the general idea behind the problem that if you take a neighborhood (x - $\epsilon$, x + $\epsilon$) and examine an area of that neighborhood from (x - $\epsilon$, x) with x being the supremum, as well as the accumulation point of the original set, there will be infinitely points in that interval converging to x. I'm just not sure how to define the necessary subsequence. Any help would be greatly appreciated.",,"['real-analysis', 'analysis']"
5,finding sum of a series,finding sum of a series,,I need some hint to find the sum of the series.  $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{n3^n}$$ I calculated using mathematica. it gives the sum as $\log(3/2)$,I need some hint to find the sum of the series.  $$\displaystyle \sum_{n=1}^{\infty}\frac{1}{n3^n}$$ I calculated using mathematica. it gives the sum as $\log(3/2)$,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
6,How do I tell whether axiom of choice is used or not?,How do I tell whether axiom of choice is used or not?,,"I am having a hard time understanding the Axiom of Choice(AC). Say I have an index set $A$ , and a collection of indexed sets {${V_\alpha}$}, where $\alpha$ is a member of $A$. Then, does the difficulty of defining a ""choice function"" come from the fact that (a) elements of each set $V_\alpha$ in the collection might not be ""ordered"" (in some sense) so that whoever is constructing a choice function is not sure which element to choose from each set? OR (b) the collection might be uncountably infinite so that there seems to be no systematic way to go through each set (without missing one) in the collection? Or (c) Is it both? As an illustration of my confusion with AC, please consider the following examples. (1) If I want to prove that the square of a real number is always non-negative, then I would begin my proof by saying that ""Pick any real number $x$."" Here, am I using AC? I am ""choosing"" an element from the set of real numbers $R$, but I am not specifying ""how"" so I feel that I am using AC. On the other hand, however, since I only have one set $R$, it seems intuitively obvious that I can just ""grab"" any element from the set without a problem. (2)  Here is the proof by Rudin of the theorem that monotonic functions have at most countable discontinuities. When Rudin writes that ""with every point x of E, we associate a rational number $r(x)$"", is he using AC here? If I can somehow associate a natural number n(x) with every point x of E, would I still be using AC? I am sorry if my questions are a bit all over the place, but I would appreciate it very much if you could help me understand AC!","I am having a hard time understanding the Axiom of Choice(AC). Say I have an index set $A$ , and a collection of indexed sets {${V_\alpha}$}, where $\alpha$ is a member of $A$. Then, does the difficulty of defining a ""choice function"" come from the fact that (a) elements of each set $V_\alpha$ in the collection might not be ""ordered"" (in some sense) so that whoever is constructing a choice function is not sure which element to choose from each set? OR (b) the collection might be uncountably infinite so that there seems to be no systematic way to go through each set (without missing one) in the collection? Or (c) Is it both? As an illustration of my confusion with AC, please consider the following examples. (1) If I want to prove that the square of a real number is always non-negative, then I would begin my proof by saying that ""Pick any real number $x$."" Here, am I using AC? I am ""choosing"" an element from the set of real numbers $R$, but I am not specifying ""how"" so I feel that I am using AC. On the other hand, however, since I only have one set $R$, it seems intuitively obvious that I can just ""grab"" any element from the set without a problem. (2)  Here is the proof by Rudin of the theorem that monotonic functions have at most countable discontinuities. When Rudin writes that ""with every point x of E, we associate a rational number $r(x)$"", is he using AC here? If I can somehow associate a natural number n(x) with every point x of E, would I still be using AC? I am sorry if my questions are a bit all over the place, but I would appreciate it very much if you could help me understand AC!",,"['analysis', 'set-theory', 'axiom-of-choice']"
7,Linear evolution equation inequality (Evans chapter 7 problem 9),Linear evolution equation inequality (Evans chapter 7 problem 9),,"I'm trying to prove an inequality from Evans' PDE book (Chapter 7 Problem 9). It's inequality (54) in $\S7.1.3$ and (59) in $\S7.2.3$. Problem: Given $u \in H^2(U) \cap H_0^1(U)$ there exists constants $\beta > 0$, $\gamma \ge 0$ such that $$ \beta ||u||_{H^2(U)}^2 \le (Lu, -\Delta  u) + \gamma ||u||_{L^2(U)}^2 $$ Hint : Assume $u$ smooth, $u=0$ on $\partial U$. Transform the term $(Lu, -\Delta u)$ by integrating by parts twice and then estimate the boundary terms. After changing variables locally and using cutoff functions, you may assume the boundary is flat. My Attempt: Integration by parts twice \begin{align*} (Lu, -\Delta u) & = -\int_U Lu\, \Delta u dx \\ & = \int_U D Lu \cdot Du\, dx - \int_{\partial U} Lu \frac{\partial u}{\partial \nu} dx \\ & = \int_U \Delta Lu \, u dx + \int_{\partial U} \left(u \frac{\partial Lu}{\partial \nu} - Lu \frac{\partial u}{\partial \nu}\right) dx \end{align*} If we assume $u=0$ on $\partial U$ then one of the boundary terms will be zero.","I'm trying to prove an inequality from Evans' PDE book (Chapter 7 Problem 9). It's inequality (54) in $\S7.1.3$ and (59) in $\S7.2.3$. Problem: Given $u \in H^2(U) \cap H_0^1(U)$ there exists constants $\beta > 0$, $\gamma \ge 0$ such that $$ \beta ||u||_{H^2(U)}^2 \le (Lu, -\Delta  u) + \gamma ||u||_{L^2(U)}^2 $$ Hint : Assume $u$ smooth, $u=0$ on $\partial U$. Transform the term $(Lu, -\Delta u)$ by integrating by parts twice and then estimate the boundary terms. After changing variables locally and using cutoff functions, you may assume the boundary is flat. My Attempt: Integration by parts twice \begin{align*} (Lu, -\Delta u) & = -\int_U Lu\, \Delta u dx \\ & = \int_U D Lu \cdot Du\, dx - \int_{\partial U} Lu \frac{\partial u}{\partial \nu} dx \\ & = \int_U \Delta Lu \, u dx + \int_{\partial U} \left(u \frac{\partial Lu}{\partial \nu} - Lu \frac{\partial u}{\partial \nu}\right) dx \end{align*} If we assume $u=0$ on $\partial U$ then one of the boundary terms will be zero.",,"['analysis', 'partial-differential-equations']"
8,Hausdorff distance and union of sets,Hausdorff distance and union of sets,,"Let $X$ be a metric space; $A_1$ , $A_2$ , $B_1$ , $B_2$ be non-empty subsets in $X$ . Let $d(\cdot,\cdot)$ be the Hausdorff distance between sets in $X$ . Then $$ d (A_1 \cup A_2 , B_1 \cup B_2) \leq \max \{ d(A_1,B_1), d(A_2,B_2)\}. $$","Let be a metric space; , , , be non-empty subsets in . Let be the Hausdorff distance between sets in . Then","X A_1 A_2 B_1 B_2 X d(\cdot,\cdot) X 
d (A_1 \cup A_2 , B_1 \cup B_2) \leq \max \{ d(A_1,B_1), d(A_2,B_2)\}.
","['analysis', 'metric-spaces', 'geometric-topology']"
9,Analysis on manifolds after course on Lebesgue integration,Analysis on manifolds after course on Lebesgue integration,,"I am an junior currently taking a course on measure theory and Lebesgue integration using Royden's text. Before this, I took a standard intro to analysis course covering the first seven chapters of baby Rudin. My institution doesn't offer a multivariable analysis course. I have room in my schedule next semester for two independent studies on topics of my choice. I want to go to graduate school to pursue a PhD in math and do not want to have any holes in my undergraduate preparation. I'm not yet sure what I direction I want to go in during grad school. Would it be worthwhile for someone in my situation to do an independent study using a book such as Munkres' Analysis on Manifolds or similar such as Spivak's?","I am an junior currently taking a course on measure theory and Lebesgue integration using Royden's text. Before this, I took a standard intro to analysis course covering the first seven chapters of baby Rudin. My institution doesn't offer a multivariable analysis course. I have room in my schedule next semester for two independent studies on topics of my choice. I want to go to graduate school to pursue a PhD in math and do not want to have any holes in my undergraduate preparation. I'm not yet sure what I direction I want to go in during grad school. Would it be worthwhile for someone in my situation to do an independent study using a book such as Munkres' Analysis on Manifolds or similar such as Spivak's?",,"['analysis', 'soft-question', 'advice']"
10,Field isomorphism of $\mathbb{C}$ onto itself,Field isomorphism of  onto itself,\mathbb{C},"I am trying to find a field isomorphism of $\mathbb{C}$ onto itself other than the identity map. The isomorphism preserves the algebraic structures $f(x+y)=f(x)+f(y)$ and $f(xy)=f(x)f(y)$. This means $f(i^2)=f(-1)=f(i)f(i)$. With this in mind, I have tried to come up with a bunch of different bijections $f$ like $f(z)=iz$, which fails that latter algebraic property, since: $$ f(i)=i^2=-1 \Rightarrow f(i)f(i)=1 $$ $$ f(-1)=i $$ Could anyone give me some hints that would lead me to the solution? Because I'm not sure my approach is the righ one.","I am trying to find a field isomorphism of $\mathbb{C}$ onto itself other than the identity map. The isomorphism preserves the algebraic structures $f(x+y)=f(x)+f(y)$ and $f(xy)=f(x)f(y)$. This means $f(i^2)=f(-1)=f(i)f(i)$. With this in mind, I have tried to come up with a bunch of different bijections $f$ like $f(z)=iz$, which fails that latter algebraic property, since: $$ f(i)=i^2=-1 \Rightarrow f(i)f(i)=1 $$ $$ f(-1)=i $$ Could anyone give me some hints that would lead me to the solution? Because I'm not sure my approach is the righ one.",,['analysis']
11,Uniform continuity of continuous functions on compact sets,Uniform continuity of continuous functions on compact sets,,"Assume that $f: \mathbb R \rightarrow \mathbb R$ is continuous function on the compact set $A$ . Does for any $\varepsilon >0$ exist a $\delta >0$ , such that $$ \lvert\, f(x)-f(y)\rvert<\varepsilon \,\,\,\,\,\,\textrm{for every}\,\,\,\, x,y\in A,\,\, \text{with}\,\,\,  \lvert x-y\rvert<\delta? $$","Assume that is continuous function on the compact set . Does for any exist a , such that","f: \mathbb R \rightarrow \mathbb R A \varepsilon >0 \delta >0 
\lvert\, f(x)-f(y)\rvert<\varepsilon \,\,\,\,\,\,\textrm{for every}\,\,\,\, x,y\in A,\,\,
\text{with}\,\,\, 
\lvert x-y\rvert<\delta?
","['calculus', 'real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
12,Find $f$ if $f(f(x))=\sqrt{1-x^2}$,Find  if,f f(f(x))=\sqrt{1-x^2},Find $f$ if $f(f(x))=\sqrt{1-x^2} \land [-1; 1] \subseteq Dom(f)$ $$$$Please give both real and complex functions. Can it be continuous or not (if f is real),Find $f$ if $f(f(x))=\sqrt{1-x^2} \land [-1; 1] \subseteq Dom(f)$ $$$$Please give both real and complex functions. Can it be continuous or not (if f is real),,['analysis']
13,Prove that $P'$ has $n-1$ distinct real roots,Prove that  has  distinct real roots,P' n-1,"Suppose a polynomial $P$ of degree $n$ has $n$ distinct real roots then $P'$ (the derivative of $P$) has $n-1$ distinct real roots. Proof by Induction: Base case: For $n=1$, $P_1 (x)=a_0+a_1x, a_1\neq 0$ has $1$ real (distinct) root, $x=\frac{-a_0}{a_1}$. Then $P_1 ' (x)=a_1$ has $1-1=0 $ real distinct root. Induction step: Assume that the claim holds for some $n\in\mathbb{N}, n>1$. That is, $$P_n (x) = a_0+a_1x+a_2x^2+...+a_nx^n$$ has $n$ distinct real roots implies that $$P_n'(x)=a_1+2a_2x+3a_3x^2+...+na_nx^{n-1}$$ has $n-1$ real distinct roots. Now, suppose that for $n+1$, $$P_{n+1} (x) = a_0+a_1x+a_2x^2+...+a_nx^n+a_{n+1}x^{n+1}$$ has $n+1$ real distinct roots.  Claim is that $P_{n+1}'(x)$ has $n$ real distinct roots. We know: $$P_{n+1}'(x)=a_1+2a_2x+3a_3x^2+...+na_nx^{n-1} +(n+1)a_{n+1} x^n$$. I know by Induction Hypothesis that $$a_1+2a_2x+3a_3x^2+...+na_nx^{n-1}$$ has $n-1$ real distinct roots. But I cannot think how to use this fact to argue the case for $P_{n+1}'(x)$.","Suppose a polynomial $P$ of degree $n$ has $n$ distinct real roots then $P'$ (the derivative of $P$) has $n-1$ distinct real roots. Proof by Induction: Base case: For $n=1$, $P_1 (x)=a_0+a_1x, a_1\neq 0$ has $1$ real (distinct) root, $x=\frac{-a_0}{a_1}$. Then $P_1 ' (x)=a_1$ has $1-1=0 $ real distinct root. Induction step: Assume that the claim holds for some $n\in\mathbb{N}, n>1$. That is, $$P_n (x) = a_0+a_1x+a_2x^2+...+a_nx^n$$ has $n$ distinct real roots implies that $$P_n'(x)=a_1+2a_2x+3a_3x^2+...+na_nx^{n-1}$$ has $n-1$ real distinct roots. Now, suppose that for $n+1$, $$P_{n+1} (x) = a_0+a_1x+a_2x^2+...+a_nx^n+a_{n+1}x^{n+1}$$ has $n+1$ real distinct roots.  Claim is that $P_{n+1}'(x)$ has $n$ real distinct roots. We know: $$P_{n+1}'(x)=a_1+2a_2x+3a_3x^2+...+na_nx^{n-1} +(n+1)a_{n+1} x^n$$. I know by Induction Hypothesis that $$a_1+2a_2x+3a_3x^2+...+na_nx^{n-1}$$ has $n-1$ real distinct roots. But I cannot think how to use this fact to argue the case for $P_{n+1}'(x)$.",,"['analysis', 'polynomials']"
14,Prove the set is not open.,Prove the set is not open.,,"Prove that the set $\mathbb{R}-\{1/n|n \in\mathbb{N}\}$ is not open. OK, so I am having a little trouble. I know that the definition of open set is : iff every point of $A$ is an interior point of $A$. The definition of a closed set: iff its complement, $A^c$ is open. I have made a number line to list the numbers in  the set to find the interior points. I came up with, $(-\infty, 1)\cup(1,1/2)\cup(1/2, 1/3)\cup(1/3,1/4).........$ From my thoughts, the set $A=Int(A)$. Am I on the right track? Further, I am not sure how this set is not open (I guess meaning that it's closed), if all the points of $A$ is in the interior of $A$. Pretty sure I am missing something, so any guidance would be appreciated.","Prove that the set $\mathbb{R}-\{1/n|n \in\mathbb{N}\}$ is not open. OK, so I am having a little trouble. I know that the definition of open set is : iff every point of $A$ is an interior point of $A$. The definition of a closed set: iff its complement, $A^c$ is open. I have made a number line to list the numbers in  the set to find the interior points. I came up with, $(-\infty, 1)\cup(1,1/2)\cup(1/2, 1/3)\cup(1/3,1/4).........$ From my thoughts, the set $A=Int(A)$. Am I on the right track? Further, I am not sure how this set is not open (I guess meaning that it's closed), if all the points of $A$ is in the interior of $A$. Pretty sure I am missing something, so any guidance would be appreciated.",,"['general-topology', 'analysis']"
15,What other definite integrals can be computed in a manner similar to $\int_{-\infty}^\infty e^{-x^2}dx$?,What other definite integrals can be computed in a manner similar to ?,\int_{-\infty}^\infty e^{-x^2}dx,"The technique for computing $\int_{-\infty}^\infty e^{-x^2} dx=\sqrt{\pi}$ by computing the integral squared using polar coordinates is well known. Are there any other integrals that can be computed in a similar way? My question is intentionally vague--any techniques however tangentially related are of interest! (EDIT 1: To be more specific, what I am imagining is whether it is possible that some definite integrals appear algebraically out of a computation of a double or triple integral in some coordinate system. Or, if there are some heuristic arguments that this probably impossible. Or if someone has a better imagination as to a computation being similar to the example I gave is also good.) (EDIT 2: It would also be interesting if someone could example how one might come up with the computation in the original example instead of it being seen as an after the factor sort of thing.)","The technique for computing $\int_{-\infty}^\infty e^{-x^2} dx=\sqrt{\pi}$ by computing the integral squared using polar coordinates is well known. Are there any other integrals that can be computed in a similar way? My question is intentionally vague--any techniques however tangentially related are of interest! (EDIT 1: To be more specific, what I am imagining is whether it is possible that some definite integrals appear algebraically out of a computation of a double or triple integral in some coordinate system. Or, if there are some heuristic arguments that this probably impossible. Or if someone has a better imagination as to a computation being similar to the example I gave is also good.) (EDIT 2: It would also be interesting if someone could example how one might come up with the computation in the original example instead of it being seen as an after the factor sort of thing.)",,"['calculus', 'analysis', 'multivariable-calculus', 'soft-question']"
16,How does one make real functions a differentiable field?,How does one make real functions a differentiable field?,,"If you want to apply the results of differential field theory to actual $\Bbb R\to\Bbb R$ functions, then first of all you have to find operations that make these functions a field. The trouble is that with the standard definition of function multiplication, many functions don't have inverses. You can't really say that the inverse of $x$ is $1/x$, because strictly speaking $x\cdot(1/x)$ is only defined on $\Bbb R - \{0\}$. I imagine the answer is to define multiplication as first multiplying in the traditional sense, and then completing by continuity, but I can't quite work out the details, and either way, I'd like to know what the conventional way of doing it is. Exactly what set of real functions are usually treated as differential fields? The set of differentiable functions defined on all but a set of isolated points of $\Bbb R$? The set of differentiable functions defined on a set dense in $\Bbb R$? What field might we work with if we were trying to prove Liouville's theorem? How is multiplication defined on that (those) field(s)?","If you want to apply the results of differential field theory to actual $\Bbb R\to\Bbb R$ functions, then first of all you have to find operations that make these functions a field. The trouble is that with the standard definition of function multiplication, many functions don't have inverses. You can't really say that the inverse of $x$ is $1/x$, because strictly speaking $x\cdot(1/x)$ is only defined on $\Bbb R - \{0\}$. I imagine the answer is to define multiplication as first multiplying in the traditional sense, and then completing by continuity, but I can't quite work out the details, and either way, I'd like to know what the conventional way of doing it is. Exactly what set of real functions are usually treated as differential fields? The set of differentiable functions defined on all but a set of isolated points of $\Bbb R$? The set of differentiable functions defined on a set dense in $\Bbb R$? What field might we work with if we were trying to prove Liouville's theorem? How is multiplication defined on that (those) field(s)?",,"['analysis', 'differential-field']"
17,sup of A Union B,sup of A Union B,,"Assumgin A,B, two set are upper-bounded. I need to prove that A Union B is also upper bounded and the supremum is max(supA, supB). This question can be explained intuitivly, but how do you prove it in a formal/mathemtical way? Thanks!","Assumgin A,B, two set are upper-bounded. I need to prove that A Union B is also upper bounded and the supremum is max(supA, supB). This question can be explained intuitivly, but how do you prove it in a formal/mathemtical way? Thanks!",,['analysis']
18,"If $a,b \in \Bbb R$, prove that $|ab| \le (a^2+b^2)/2$","If , prove that","a,b \in \Bbb R |ab| \le (a^2+b^2)/2",So far I have the first case when $a=b$: \begin{align*} |ab| &= |b^2|\\ &=|b|^2\\ &=\frac{2|b|^2}2\\ &=\frac{b^2+b^2}2\\ &=\frac{a^2+b^2}2 \end{align*} Case 2: $a>b$ Case 3 $a<b$ I've been stuck on this problem for a few hours now and don't know how to proceed. Am I approaching it wrong? Should I not be thinking about the cases of $a$ in terms of $b$? Thank you for your help.,So far I have the first case when $a=b$: \begin{align*} |ab| &= |b^2|\\ &=|b|^2\\ &=\frac{2|b|^2}2\\ &=\frac{b^2+b^2}2\\ &=\frac{a^2+b^2}2 \end{align*} Case 2: $a>b$ Case 3 $a<b$ I've been stuck on this problem for a few hours now and don't know how to proceed. Am I approaching it wrong? Should I not be thinking about the cases of $a$ in terms of $b$? Thank you for your help.,,['analysis']
19,path connected subspaces of $\mathbb R^2$,path connected subspaces of,\mathbb R^2,"I am trying to prove two statements that visually I think they are ""obvious"", but I am totally lost when it comes to do a formal proof. The statement of the exercise is: Decide whether $\mathbb R^2 \setminus \{(0,0)\}$ and $\mathbb R^2 \setminus \{(x,0)\mid x \in \mathbb R\}$ are path connected. Visually, this is what I see : removing a point from the plane doesn't alterate much. I mean, the only problem I would have is with the points $x$,$y$ that live in a line that also contains the point $(0,0)$. That being the case, I could define a curve joining those two points without passing through $(0,0)$. If I remove a line from the plane, then I am in trouble. This line divides the plane into two regions. If I take a point $x$ that lives in one part and another point $y$ that is contained in the other side, then there is no continuous function that can join me those two points. I have no idea how to prove it without the ""hand-waving"". Also, I have a doubt when it comes to generalizing this idea to $\mathbb R^n$. I suppose that removing a hyperplane is what affects path-connectedness, but I'm not so sure, I am just generalizing from the case of the real line and the plane. Is there any good topology textbook that treats this topic of metric spaces?","I am trying to prove two statements that visually I think they are ""obvious"", but I am totally lost when it comes to do a formal proof. The statement of the exercise is: Decide whether $\mathbb R^2 \setminus \{(0,0)\}$ and $\mathbb R^2 \setminus \{(x,0)\mid x \in \mathbb R\}$ are path connected. Visually, this is what I see : removing a point from the plane doesn't alterate much. I mean, the only problem I would have is with the points $x$,$y$ that live in a line that also contains the point $(0,0)$. That being the case, I could define a curve joining those two points without passing through $(0,0)$. If I remove a line from the plane, then I am in trouble. This line divides the plane into two regions. If I take a point $x$ that lives in one part and another point $y$ that is contained in the other side, then there is no continuous function that can join me those two points. I have no idea how to prove it without the ""hand-waving"". Also, I have a doubt when it comes to generalizing this idea to $\mathbb R^n$. I suppose that removing a hyperplane is what affects path-connectedness, but I'm not so sure, I am just generalizing from the case of the real line and the plane. Is there any good topology textbook that treats this topic of metric spaces?",,"['general-topology', 'analysis', 'connectedness']"
20,Derivative at $0$ of $\int_0^x \sin \frac{1}{t} dt$,Derivative at  of,0 \int_0^x \sin \frac{1}{t} dt,Let $f(x)=\int_0^x \sin \frac{1}{t} dt   \textrm{ for }  x \in \mathbb R$. Is $f$ differentiable at $0$ ?,Let $f(x)=\int_0^x \sin \frac{1}{t} dt   \textrm{ for }  x \in \mathbb R$. Is $f$ differentiable at $0$ ?,,['analysis']
21,Does a neighbourhood need to be a *connected* set?,Does a neighbourhood need to be a *connected* set?,,"I have in my topology/ real analysis textbook the definition of neighbourhood of a point as an open set containing that point. But isn't a neighbourhood necessarily a connected set? Wikipedia also says that ""Intuitively speaking, a neighbourhood of a point is a set containing the point where you can move that point some amount without leaving the set."" My complex analysis textbook also suggests that a neighbourhood of a point is a domain (defined as an open connected set) containing that point. I've been googling, and none of the definitions that I've come across of ""neighbourhood"" have any connectedness condition. What am I missing?","I have in my topology/ real analysis textbook the definition of neighbourhood of a point as an open set containing that point. But isn't a neighbourhood necessarily a connected set? Wikipedia also says that ""Intuitively speaking, a neighbourhood of a point is a set containing the point where you can move that point some amount without leaving the set."" My complex analysis textbook also suggests that a neighbourhood of a point is a domain (defined as an open connected set) containing that point. I've been googling, and none of the definitions that I've come across of ""neighbourhood"" have any connectedness condition. What am I missing?",,"['general-topology', 'analysis', 'terminology']"
22,Uncountably many equivalent Cauchy sequence?,Uncountably many equivalent Cauchy sequence?,,"RTP There exists uncountably many Cauchy sequence of rationals that are equivalent. I am trying to solve the above question, and my understanding is that $\Bbb R$ is a set of equivalent classes of Cauchy sequence of rationals. And two sequences are equivalent if they both have the same limit. So, having prior knowledge (not really knowing the proof) that there are uncountably many real numbers, I want to somehow connect this idea with this problem. Can someone help me out ? I am a beginner at analysis, so it would really help if you could dumb it down.","RTP There exists uncountably many Cauchy sequence of rationals that are equivalent. I am trying to solve the above question, and my understanding is that $\Bbb R$ is a set of equivalent classes of Cauchy sequence of rationals. And two sequences are equivalent if they both have the same limit. So, having prior knowledge (not really knowing the proof) that there are uncountably many real numbers, I want to somehow connect this idea with this problem. Can someone help me out ? I am a beginner at analysis, so it would really help if you could dumb it down.",,"['analysis', 'cauchy-sequences']"
23,Notation Clarification of Koch Curve,Notation Clarification of Koch Curve,,"I am having trouble making sense of the notation used to describe the Koch Curve in the book Getting Aquanted with Fractals . The link will take you to a preview of the book which describes the notation. I am looking at section $1.1.2$ titled The Koch Curve . At each iteration of $f$ applied to $A_{(k)}$, what does $A_{j_1,\ldots.j_k}$ mean? Specifically, what does $A_{j_1, j_2}$ mean in light of $A_{(2)}$? How does this describe the curve? I think a great answer would be to walk through the first few iterations of the Koch Curve explaining what the $A_{j_1, j_2, \ldots, j_k}$ mean. All help is greatly appreciated!","I am having trouble making sense of the notation used to describe the Koch Curve in the book Getting Aquanted with Fractals . The link will take you to a preview of the book which describes the notation. I am looking at section $1.1.2$ titled The Koch Curve . At each iteration of $f$ applied to $A_{(k)}$, what does $A_{j_1,\ldots.j_k}$ mean? Specifically, what does $A_{j_1, j_2}$ mean in light of $A_{(2)}$? How does this describe the curve? I think a great answer would be to walk through the first few iterations of the Koch Curve explaining what the $A_{j_1, j_2, \ldots, j_k}$ mean. All help is greatly appreciated!",,"['analysis', 'notation', 'fractals']"
24,Characteristic functions of intervals,Characteristic functions of intervals,,Could you explain to me what characteristic functions of intervals are? I'm reading something about convergence of sequences of functions and it says there that characteristic functions of intervals are in the set of functions that are the pointwise limits of continuous functions. Do you think you could help me? Thanks.,Could you explain to me what characteristic functions of intervals are? I'm reading something about convergence of sequences of functions and it says there that characteristic functions of intervals are in the set of functions that are the pointwise limits of continuous functions. Do you think you could help me? Thanks.,,"['analysis', 'convergence-divergence']"
25,differentiate f(x) using L'hopital and other problem,differentiate f(x) using L'hopital and other problem,,"Evaluate: $\ \ \ \ \lim_{x\to1}(2-x)^{\tan(\frac{\pi}{2}x)}$ Show that  the  inequality holds: $\ \ \ x^\alpha\leq \alpha x + (1-\alpha)\ \ \ (x\geq0, \,0<\alpha <1)$ Please help me with these.  Either a hint or a full proof will do. Thanks.","Evaluate: $\ \ \ \ \lim_{x\to1}(2-x)^{\tan(\frac{\pi}{2}x)}$ Show that  the  inequality holds: $\ \ \ x^\alpha\leq \alpha x + (1-\alpha)\ \ \ (x\geq0, \,0<\alpha <1)$ Please help me with these.  Either a hint or a full proof will do. Thanks.",,"['calculus', 'analysis']"
26,Existence of a point such that $f(c) = \sqrt{\frac{1}{b-a} \int_{a}^{b}f^{2}(x)dx}$,Existence of a point such that,f(c) = \sqrt{\frac{1}{b-a} \int_{a}^{b}f^{2}(x)dx},"This has been stumping my calculus class: If $f$ is continuous on $[a,b]$ and $f(x)\geq 0$ on $[a,b]$ show that there exists a $c$ in $[a,b]$ such that $f(c) = \sqrt{\frac{1}{b-a} \int_{a}^{b}f^{2}(x)dx}$ Thanks! Attempts: We've tried an argument showing that you can approximate it, and even tried thinking up a function for applying the MVT or Darboux's theorem without much luck (even though we think this is the right idea)","This has been stumping my calculus class: If $f$ is continuous on $[a,b]$ and $f(x)\geq 0$ on $[a,b]$ show that there exists a $c$ in $[a,b]$ such that $f(c) = \sqrt{\frac{1}{b-a} \int_{a}^{b}f^{2}(x)dx}$ Thanks! Attempts: We've tried an argument showing that you can approximate it, and even tried thinking up a function for applying the MVT or Darboux's theorem without much luck (even though we think this is the right idea)",,"['calculus', 'analysis']"
27,Orthogonal Polynomials,Orthogonal Polynomials,,"Let $\{P_n\}_{n=0}^{\infty}$ a family of polynomials in $[a,b]$ where $n$ is the degree of $P_n$. Suppose they are orthogonal with respect to a positive wight function $\rho$. Show that $P_n$ has $n$ zeros in $[a,b]$. From the data we have that for any $n,m$, it is true that: $$\displaystyle \int_{a}^{b} P_n(x)P_m(x)\rho (x) = 0$$ How should I proceed for the desired result?","Let $\{P_n\}_{n=0}^{\infty}$ a family of polynomials in $[a,b]$ where $n$ is the degree of $P_n$. Suppose they are orthogonal with respect to a positive wight function $\rho$. Show that $P_n$ has $n$ zeros in $[a,b]$. From the data we have that for any $n,m$, it is true that: $$\displaystyle \int_{a}^{b} P_n(x)P_m(x)\rho (x) = 0$$ How should I proceed for the desired result?",,"['real-analysis', 'analysis']"
28,"Prove that $f$ is discontinuous at $(0,0)$",Prove that  is discontinuous at,"f (0,0)","Let $f$ be defined by $$ f(x,y) =  \begin{cases} \biggl\lvert \frac{y}{x^2} \biggr\rvert e^{-\bigl\lvert \frac{y}{x^2} \bigr\rvert} , \quad \text{ if $x \neq 0$} \\ 0, \qquad \qquad \quad \text{if $x = 0$.} \end{cases} $$ Prove that $f$ is discontinuous at $(0,0)$. Prove that $f$ is continuous along any line passing through the origin. Hi so I am really stuck here. I am not sure what method to use.","Let $f$ be defined by $$ f(x,y) =  \begin{cases} \biggl\lvert \frac{y}{x^2} \biggr\rvert e^{-\bigl\lvert \frac{y}{x^2} \bigr\rvert} , \quad \text{ if $x \neq 0$} \\ 0, \qquad \qquad \quad \text{if $x = 0$.} \end{cases} $$ Prove that $f$ is discontinuous at $(0,0)$. Prove that $f$ is continuous along any line passing through the origin. Hi so I am really stuck here. I am not sure what method to use.",,"['analysis', 'continuity']"
29,Let $f:\mathbb R \rightarrow \mathbb R$ be continuous with $f(0)=f(1)=0.$,Let  be continuous with,f:\mathbb R \rightarrow \mathbb R f(0)=f(1)=0.,"I was thinking about the following problem: Let $f:\mathbb R \rightarrow \mathbb R$ be continuous with $f(0)=f(1)=0.$ Then which of the following is not possible? (a) $f([0,1])=\{0\},$ (b) $f([0,1])=[0,1),$ (c) $f([0,1])=[0,1],$ (d) $f([0,1])=[-1/2,1/2].$ Can someone point me in the right direction? Thanks in advance for your time.",I was thinking about the following problem: Let be continuous with Then which of the following is not possible? (a) (b) (c) (d) Can someone point me in the right direction? Thanks in advance for your time.,"f:\mathbb R \rightarrow \mathbb R f(0)=f(1)=0. f([0,1])=\{0\}, f([0,1])=[0,1), f([0,1])=[0,1], f([0,1])=[-1/2,1/2].","['real-analysis', 'analysis']"
30,Is there any Elliptic Operator of first order in $U\subset \mathbb R^n$?,Is there any Elliptic Operator of first order in ?,U\subset \mathbb R^n,"Suppose $P=\sum_i a_i \partial/\partial x^i$. It seems for me that there always exists a nonzero $\xi=\{\xi_1, \cdots, \xi_n\}\in \mathbb R^n\setminus 0$ such that the principle symbol $\sigma(P)(x, \xi)=\sum_i a_i\xi_i=0$. Hence by definition it cannot be elliptic. Did I miss anything? So for differential operators acting on functions defined over $U\subset\mathbb R^n$, The only elliptic operator must be of second order? How to define the symbol of $P$ for function $f:\mathbb R^n\to\mathbb R^n$? Can you show me an example of first order ellliptic operator?","Suppose $P=\sum_i a_i \partial/\partial x^i$. It seems for me that there always exists a nonzero $\xi=\{\xi_1, \cdots, \xi_n\}\in \mathbb R^n\setminus 0$ such that the principle symbol $\sigma(P)(x, \xi)=\sum_i a_i\xi_i=0$. Hence by definition it cannot be elliptic. Did I miss anything? So for differential operators acting on functions defined over $U\subset\mathbb R^n$, The only elliptic operator must be of second order? How to define the symbol of $P$ for function $f:\mathbb R^n\to\mathbb R^n$? Can you show me an example of first order ellliptic operator?",,"['analysis', 'partial-differential-equations']"
31,Closed bounded subset of $\mathbb{R}$.,Closed bounded subset of .,\mathbb{R},"Let $X$ be a closed and bounded subset of $\mathbb{R}$. Is it true that $X$ is a finite union of closed intervals in $\mathbb{R}$? (*) I think that if we choose $X$ is a Cantor set, then $X$ doesn't satisfy (*). How can I prove this?","Let $X$ be a closed and bounded subset of $\mathbb{R}$. Is it true that $X$ is a finite union of closed intervals in $\mathbb{R}$? (*) I think that if we choose $X$ is a Cantor set, then $X$ doesn't satisfy (*). How can I prove this?",,"['general-topology', 'analysis']"
32,Uniform continuity of a function transforming Cauchy sequences into Cauchy sequences,Uniform continuity of a function transforming Cauchy sequences into Cauchy sequences,,"Problem: $f:(0,\infty )\rightarrow \mathbb{R}$ defined as: $f(x)=x^{2}$ Can anyone show me how to prove that $f$ transforms Cauchy sequences of elements of $(0,\infty )$ into Cauchy sequences, but $f$ is not uniformly continuous? The purpose of this problem is to show how essential the boundedness of the set on which $f$ is defined is. $f$ is definitely not uniformly continuous because for the two sequences: $\left \{ x_{n} \right \},\left \{ y_{n} \right \}$ defined by: $x_{n}=n+\frac{1}{n}$ and $y_{n}=n$. We have: $\left | x_{n}-y_{n} \right | \to 0$ as $n \to \infty $, but $\left | f(x_{n})-f(y_{n}) \right | \to 2$ as $(n \to \infty )$. Can anyone, please, show me how to prove that $f$ transforms Cauchy sequences of elements of $(0,\infty )$ into Cauchy sequences? Thanks","Problem: $f:(0,\infty )\rightarrow \mathbb{R}$ defined as: $f(x)=x^{2}$ Can anyone show me how to prove that $f$ transforms Cauchy sequences of elements of $(0,\infty )$ into Cauchy sequences, but $f$ is not uniformly continuous? The purpose of this problem is to show how essential the boundedness of the set on which $f$ is defined is. $f$ is definitely not uniformly continuous because for the two sequences: $\left \{ x_{n} \right \},\left \{ y_{n} \right \}$ defined by: $x_{n}=n+\frac{1}{n}$ and $y_{n}=n$. We have: $\left | x_{n}-y_{n} \right | \to 0$ as $n \to \infty $, but $\left | f(x_{n})-f(y_{n}) \right | \to 2$ as $(n \to \infty )$. Can anyone, please, show me how to prove that $f$ transforms Cauchy sequences of elements of $(0,\infty )$ into Cauchy sequences? Thanks",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
33,Parabolic PDE local and global existence,Parabolic PDE local and global existence,,"If you have a local solution to a parabolic PDE (say we know it exists (weakly anyway) from time 0 to T), then if the solution is bounded in an appropriate way (in which norms?) then we can apparently extend the solution globally. Can someone refer me to these results or explain this, please? I also heard that the as the solution $u(t)$ converges a $C^\infty$ function as $t \to T$. Why is that? I thought this had something to do with Sobolev embeddings but I can't see in general how this latter statement can be true (maybe it's just for this case). Thanks.","If you have a local solution to a parabolic PDE (say we know it exists (weakly anyway) from time 0 to T), then if the solution is bounded in an appropriate way (in which norms?) then we can apparently extend the solution globally. Can someone refer me to these results or explain this, please? I also heard that the as the solution $u(t)$ converges a $C^\infty$ function as $t \to T$. Why is that? I thought this had something to do with Sobolev embeddings but I can't see in general how this latter statement can be true (maybe it's just for this case). Thanks.",,"['analysis', 'partial-differential-equations']"
34,Differentiable and $f(0)=0$,Differentiable and,f(0)=0,"I would like to show that if $f\colon\mathbb{R}^n\to \mathbb{R}$ is differentiable and $f(0)=0$ that there exists $g_i\colon\mathbb{R}^n\to\mathbb{R}$ such that $f(x) = \sum_{i=1}^n x^i g_i(x)$. The book (Spivak) has it written like that, but I believe it's more appropriate as $x_i g_i(x)$ (i.e. the $x_i$ are the components and not the powers,... I think?!) I don't see why this should necessarily be true, if we don't assume that $f$ is linear.  Just because $f(0)=0$ and is differentiable doesn't imply linearity. For example if $f(x,y) = x\cdot y$ I don't see how we should split this into a sum of parts $g_1$ and $g_2$.","I would like to show that if $f\colon\mathbb{R}^n\to \mathbb{R}$ is differentiable and $f(0)=0$ that there exists $g_i\colon\mathbb{R}^n\to\mathbb{R}$ such that $f(x) = \sum_{i=1}^n x^i g_i(x)$. The book (Spivak) has it written like that, but I believe it's more appropriate as $x_i g_i(x)$ (i.e. the $x_i$ are the components and not the powers,... I think?!) I don't see why this should necessarily be true, if we don't assume that $f$ is linear.  Just because $f(0)=0$ and is differentiable doesn't imply linearity. For example if $f(x,y) = x\cdot y$ I don't see how we should split this into a sum of parts $g_1$ and $g_2$.",,['analysis']
35,Analogue to Fixed Point Theorem for Compact metric spaces,Analogue to Fixed Point Theorem for Compact metric spaces,,"If $\omega:H \rightarrow H$ (into) is continuous and $H$ is compact, does  $\omega$ fix any of its subsets other than the empty set?","If $\omega:H \rightarrow H$ (into) is continuous and $H$ is compact, does  $\omega$ fix any of its subsets other than the empty set?",,"['general-topology', 'analysis', 'metric-spaces', 'fixed-point-theorems']"
36,"Why is $\int_{n}^{n+1}\int_{n}^x |f'(x)|\,dx \leq \int_{n}^{n+1} |f'(x)|\,dx$ for $x \in [n,n+1]$?",Why is  for ?,"\int_{n}^{n+1}\int_{n}^x |f'(x)|\,dx \leq \int_{n}^{n+1} |f'(x)|\,dx x \in [n,n+1]","Can anyone show me how to prove the following: For $x\in \left [ n,n+1 \right ]$: $$\int_{n}^{n+1}\int_{n}^{x}\left | f'(t) \right |dtdx\leqslant \int_{n}^{n+1}\left | f'(t)\right |dt?$$","Can anyone show me how to prove the following: For $x\in \left [ n,n+1 \right ]$: $$\int_{n}^{n+1}\int_{n}^{x}\left | f'(t) \right |dtdx\leqslant \int_{n}^{n+1}\left | f'(t)\right |dt?$$",,"['calculus', 'real-analysis', 'analysis', 'integration']"
37,Mathematical analysis - text book recommendation sought,Mathematical analysis - text book recommendation sought,,"A rather subjective question, I admit, but I'm looking for a recommendation for a textbook to help me improve my understanding of mathematical analysis. I come from a computing background, with a University level degree. My high-school mathematics was all focused on mechanics/physics rather than pure maths and statistics.  I've followed formal undergraduate courses on signal processing, and informal postgraduate lecture series subsequently - for personal interest, not credit.  I feel confident that I have the skills to digest any well-written undergraduate or masters-level text. I recognise that I am (relatively) weak with respect to analysis when I read the Wikipedia pages for subjects such as: http://en.wikipedia.org/wiki/Non-analytic_smooth_function http://en.wikipedia.org/wiki/Distribution_%28mathematics%29 http://en.wikipedia.org/wiki/Mollifier http://en.wikipedia.org/wiki/Holomorphic_function etc. etc. I am interested to bolster my understanding of the principles of mathematical analysis and to go on from this to improve my understanding of distributions and how they relate to both analytic and non-analytic functions.  While I recognise the value of proofs, and I'm not looking for a reference book from which to crib formulae, my principal interest is in the practical application of theory rather than its elegant abstract justification.  For this reason, I'm drawn more to presentations with intuitive over formal justifications for theorems. Suggestions?","A rather subjective question, I admit, but I'm looking for a recommendation for a textbook to help me improve my understanding of mathematical analysis. I come from a computing background, with a University level degree. My high-school mathematics was all focused on mechanics/physics rather than pure maths and statistics.  I've followed formal undergraduate courses on signal processing, and informal postgraduate lecture series subsequently - for personal interest, not credit.  I feel confident that I have the skills to digest any well-written undergraduate or masters-level text. I recognise that I am (relatively) weak with respect to analysis when I read the Wikipedia pages for subjects such as: http://en.wikipedia.org/wiki/Non-analytic_smooth_function http://en.wikipedia.org/wiki/Distribution_%28mathematics%29 http://en.wikipedia.org/wiki/Mollifier http://en.wikipedia.org/wiki/Holomorphic_function etc. etc. I am interested to bolster my understanding of the principles of mathematical analysis and to go on from this to improve my understanding of distributions and how they relate to both analytic and non-analytic functions.  While I recognise the value of proofs, and I'm not looking for a reference book from which to crib formulae, my principal interest is in the practical application of theory rather than its elegant abstract justification.  For this reason, I'm drawn more to presentations with intuitive over formal justifications for theorems. Suggestions?",,"['analysis', 'reference-request']"
38,Use Lagrange Remainder Theorem to Prove Inequality,Use Lagrange Remainder Theorem to Prove Inequality,,"I'm supposed to use Lagrange Remainder Theorem to prove that $$1 + \frac{x}{2} - \frac{x^2}{8} < \sqrt{1+x} < 1 + \frac{x}{2} \text{ } \text{ if } x>0$$ Obviously, the left and right hand sides look like Taylor series, but how would you do this using Lagrange , it isn't obviously intuitive. Thanks!","I'm supposed to use Lagrange Remainder Theorem to prove that $$1 + \frac{x}{2} - \frac{x^2}{8} < \sqrt{1+x} < 1 + \frac{x}{2} \text{ } \text{ if } x>0$$ Obviously, the left and right hand sides look like Taylor series, but how would you do this using Lagrange , it isn't obviously intuitive. Thanks!",,"['calculus', 'real-analysis', 'analysis']"
39,How to prove $f(n) := \frac{1}{2}(a+(f(n-1))^2)  1-\sqrt{1-a}$,How to prove,f(n) := \frac{1}{2}(a+(f(n-1))^2)  1-\sqrt{1-a},"$a \in [0,1]\hspace{2em}f(1) := 0,\hspace{1em} f(n + 1) := \frac{1}{2}(a+(f(n))^2),\hspace{2em} n \in \mathbb{N}$ Prove: $f(n)  1-\sqrt{1-a}$ I assume that I'll need to convert this recursive function to a non-recursive one in order to prove the inequation. I've already calculated the values for 2 to 4: $\hspace{2em}f(2) = \frac{1}{2}a$ $\hspace{2em}f(3) = \frac{1}{8}a^2 + \frac{1}{2}a$ $\hspace{2em}f(4) = \frac{1}{128}a^4 + \frac{1}{16}a^3 + \frac{1}{8}a^2 + \frac{1}{2}a$ I can see the regularity, but I don't know how to express it with what I learned so far.","$a \in [0,1]\hspace{2em}f(1) := 0,\hspace{1em} f(n + 1) := \frac{1}{2}(a+(f(n))^2),\hspace{2em} n \in \mathbb{N}$ Prove: $f(n)  1-\sqrt{1-a}$ I assume that I'll need to convert this recursive function to a non-recursive one in order to prove the inequation. I've already calculated the values for 2 to 4: $\hspace{2em}f(2) = \frac{1}{2}a$ $\hspace{2em}f(3) = \frac{1}{8}a^2 + \frac{1}{2}a$ $\hspace{2em}f(4) = \frac{1}{128}a^4 + \frac{1}{16}a^3 + \frac{1}{8}a^2 + \frac{1}{2}a$ I can see the regularity, but I don't know how to express it with what I learned so far.",,['analysis']
40,help to prove an inequality,help to prove an inequality,,"Suppose $f\in C^2[a,b]$, $f(a)=f(b)=0$. Then for any $x\in [a, b]$: $$\frac{f(x)}{(x-a)(b-x)}\le \frac{1}{b-a} \int_a^b{|f^{\prime\prime}(t)|dt}.$$ Any help is appreciated.","Suppose $f\in C^2[a,b]$, $f(a)=f(b)=0$. Then for any $x\in [a, b]$: $$\frac{f(x)}{(x-a)(b-x)}\le \frac{1}{b-a} \int_a^b{|f^{\prime\prime}(t)|dt}.$$ Any help is appreciated.",,"['calculus', 'analysis', 'inequality']"
41,Analysis Convergence/Divergence,Analysis Convergence/Divergence,,"Prove there exists a function $f$ such that    $$\int_1^{\infty}f(x)\,dx\text{ converges, but }\int_1^{\infty}|f(x)|\,dx\text{ diverges.}$$ Similarly, prove that there exists a function $g$ such that   $$\int_0^1 g(x)\,dx\text{ converges, but }\int_0^1|g(x)|\,dx\text{ diverges.}$$ All I am able to understand in the first part, is to take an example. I am thinking of something like $(1/2)^n$? I am not sure how to account for the absolute values, and when they say prove, can I just find an example only? I am having trouble of thinking of such a function.","Prove there exists a function $f$ such that    $$\int_1^{\infty}f(x)\,dx\text{ converges, but }\int_1^{\infty}|f(x)|\,dx\text{ diverges.}$$ Similarly, prove that there exists a function $g$ such that   $$\int_0^1 g(x)\,dx\text{ converges, but }\int_0^1|g(x)|\,dx\text{ diverges.}$$ All I am able to understand in the first part, is to take an example. I am thinking of something like $(1/2)^n$? I am not sure how to account for the absolute values, and when they say prove, can I just find an example only? I am having trouble of thinking of such a function.",,['analysis']
42,Question about a puzzle on injecting a subset of $\mathbb{R}$ into $\mathbb{Q}$,Question about a puzzle on injecting a subset of  into,\mathbb{R} \mathbb{Q},"I was just browsing through the Puzzle section on Noam Elkies website. The puzzle can be found here . The solution to the puzzle proves that any well-ordered subset of $\mathbb{R}$ is countable. In the solution, Elkies defines $s(x)$ to be the immediately following element of $x$ for any $x\in S$ for a well ordered subset $S\subset \mathbb{R}$. I want to understand the added note at the bottom. It says to consider any interval $(x,s(x))$. I take it he means to view this interval as an subset of $\mathbb{R}$, for it is empty in $S$? Then I know there always exists a rational between any two reals since $\mathbb{Q}$ is dense in $\mathbb{R}$, so one could map $(x,s(x))$ to some $r(x)\in (x,s(x))$, which would show $S$ is in bijection with a subset of $\mathbb{Q}$, and hence countable. What I don't get is why this mapping doesn't require choice. For any interval $(x,s(x))$, how could we distinquish some $r(x)$ to pick? Are we possibly talking about taking the least rational in $(x,s(x))$ for any $x$? Would that be a way to distinguish which rational to choose?","I was just browsing through the Puzzle section on Noam Elkies website. The puzzle can be found here . The solution to the puzzle proves that any well-ordered subset of $\mathbb{R}$ is countable. In the solution, Elkies defines $s(x)$ to be the immediately following element of $x$ for any $x\in S$ for a well ordered subset $S\subset \mathbb{R}$. I want to understand the added note at the bottom. It says to consider any interval $(x,s(x))$. I take it he means to view this interval as an subset of $\mathbb{R}$, for it is empty in $S$? Then I know there always exists a rational between any two reals since $\mathbb{Q}$ is dense in $\mathbb{R}$, so one could map $(x,s(x))$ to some $r(x)\in (x,s(x))$, which would show $S$ is in bijection with a subset of $\mathbb{Q}$, and hence countable. What I don't get is why this mapping doesn't require choice. For any interval $(x,s(x))$, how could we distinquish some $r(x)$ to pick? Are we possibly talking about taking the least rational in $(x,s(x))$ for any $x$? Would that be a way to distinguish which rational to choose?",,"['analysis', 'elementary-set-theory', 'puzzle', 'axiom-of-choice']"
43,Practice problem from Mean Value Theorem in Real Analysis,Practice problem from Mean Value Theorem in Real Analysis,,"Can someone give an insight on the following problem? I'm not sure how to start the problem. It's a practice problem for ""mean value theorem"" and ""Taylor's Theorem"" so I'm assuming they might be necessary for the proof. Thanks! Let $f: \mathbb{R} \to \mathbb{R}$ be a function. Suppose that $f$ is differentiable, that $f(0)=1$, and that $|f'(x)| \leq 1$ for all $x \in \mathbb{R}$. Prove that $|f(x)| \leq |x|+1$ for all $x \in \mathbb{R}$.","Can someone give an insight on the following problem? I'm not sure how to start the problem. It's a practice problem for ""mean value theorem"" and ""Taylor's Theorem"" so I'm assuming they might be necessary for the proof. Thanks! Let $f: \mathbb{R} \to \mathbb{R}$ be a function. Suppose that $f$ is differentiable, that $f(0)=1$, and that $|f'(x)| \leq 1$ for all $x \in \mathbb{R}$. Prove that $|f(x)| \leq |x|+1$ for all $x \in \mathbb{R}$.",,"['real-analysis', 'analysis']"
44,Boundedness of an integral,Boundedness of an integral,,"Got stuck on this one: Show that there is a constant $C>0$ such that $$\left|\int_0^x \frac{\sin (N+1/2)t}{\sin t/2} dt \right|\le C$$ for all $x\in [-\pi,\pi]$ and integer $N\ge 1$. I thought this should follow from $\int_0^\infty \frac{\sin t}{t}dt<\infty$, but somehow don't get the connection.","Got stuck on this one: Show that there is a constant $C>0$ such that $$\left|\int_0^x \frac{\sin (N+1/2)t}{\sin t/2} dt \right|\le C$$ for all $x\in [-\pi,\pi]$ and integer $N\ge 1$. I thought this should follow from $\int_0^\infty \frac{\sin t}{t}dt<\infty$, but somehow don't get the connection.",,['analysis']
45,Are almost everywhere equal functions asymptotic?,Are almost everywhere equal functions asymptotic?,,"I'm quite new at measure theory and all I know I read by myself, so bear with me if this question is too trivial. Given the definition of asymptotic, can we say that for $x \rightarrow \infty$ two almost everywhere equal functions are asymptotic? I really think that this is not the case, but I haven't been able to find a counterexample. Maybe, if in general this does not hold, would it be right for real functions?","I'm quite new at measure theory and all I know I read by myself, so bear with me if this question is too trivial. Given the definition of asymptotic, can we say that for $x \rightarrow \infty$ two almost everywhere equal functions are asymptotic? I really think that this is not the case, but I haven't been able to find a counterexample. Maybe, if in general this does not hold, would it be right for real functions?",,"['analysis', 'measure-theory']"
46,The limit of Thomae's function at any a such that 0 < a < 1,The limit of Thomae's function at any a such that 0 < a < 1,,"I'm currently reading Spivak's Calculus and I'm having trouble understanding the example with the limit of Thomae's function. The function \begin{align} f(x) = \begin{cases} 0 & \text{$x$ irrational, $0 < x < 1$}\\ \frac{1}{q} & \text{$x = p/q$ in lowest terms, $0 < x < 1$} \end{cases} \end{align} approaches $0$ for any $a \in (0, 1)$ . Looking at this picture , I don't get how that could be true. For instance, if $a = \frac{1}{2}$ , then it seems to me that for $\epsilon = \frac{1}{10}$ there's no $\delta$ such that $0 < \lvert x - a \rvert < \delta \implies \lvert f(x) - 0\rvert < \epsilon$ . What am I missing here?","I'm currently reading Spivak's Calculus and I'm having trouble understanding the example with the limit of Thomae's function. The function approaches for any . Looking at this picture , I don't get how that could be true. For instance, if , then it seems to me that for there's no such that . What am I missing here?","\begin{align}
f(x) = \begin{cases}
0 & \text{x irrational, 0 < x < 1}\\
\frac{1}{q} & \text{x = p/q in lowest terms, 0 < x < 1}
\end{cases}
\end{align} 0 a \in (0, 1) a = \frac{1}{2} \epsilon = \frac{1}{10} \delta 0 < \lvert x - a \rvert < \delta \implies \lvert f(x) - 0\rvert < \epsilon","['real-analysis', 'calculus', 'analysis']"
47,"Existence of $1/(e-1)$ value of a continous and differentiable function $f:[0,1]\to \mathbb R$ given $f(1)=1, f(0)=0$",Existence of  value of a continous and differentiable function  given,"1/(e-1) f:[0,1]\to \mathbb R f(1)=1, f(0)=0","Let $f:[0,1]\to \mathbb R$ be a continous function on $[0,1]$ and differentiable on $(0,1)$ , $f(0)=0, f(1)=1$ . Prove that there exists $c\in (0,1)$ so that $f(c)+\frac{1}{e-1}=f'(c)$ where $e$ is the Euler number. I tried to subs $f(x)=e^{x}g(x)$ so that $f'(x)-f(x)=e^{x}g'(x)$ and define $e^{x}g'(x)=w(x)$ . Then I tried to use the mean value theorem (to ensure the existence of $r \in (0,1)$ s.t. $f'(r)-f(r)=e^{r-1}$ ) however then I have no idea how to continue. My guess is we use intermediate value theorem on $w(x)$ however I don't have any idea on how to do it. Is there any construction to solve this?","Let be a continous function on and differentiable on , . Prove that there exists so that where is the Euler number. I tried to subs so that and define . Then I tried to use the mean value theorem (to ensure the existence of s.t. ) however then I have no idea how to continue. My guess is we use intermediate value theorem on however I don't have any idea on how to do it. Is there any construction to solve this?","f:[0,1]\to \mathbb R [0,1] (0,1) f(0)=0, f(1)=1 c\in (0,1) f(c)+\frac{1}{e-1}=f'(c) e f(x)=e^{x}g(x) f'(x)-f(x)=e^{x}g'(x) e^{x}g'(x)=w(x) r \in (0,1) f'(r)-f(r)=e^{r-1} w(x)","['real-analysis', 'calculus', 'analysis', 'functions', 'mean-value-theorem']"
48,One variable inequality problem $\frac{x\sqrt{|x^2 - 4|}}{x^2 - 4} - 1 > 0$,One variable inequality problem,\frac{x\sqrt{|x^2 - 4|}}{x^2 - 4} - 1 > 0,"Hi I'm having problem with inequality $\frac{x\sqrt{|x^2 - 4|}}{x^2 - 4} - 1 > 0$ I rearranged the equation a bit and I did this $\frac{x\sqrt{|x^2 - 4|}-x^2+4}{x^2 - 4}  > 0$ holds if $(x-2) \cdot(x+2)\cdot (x\sqrt{|x^2 - 4|}-x^2+4)>0$ This part is the most difficult for me $(x\sqrt{|x^2 - 4|}-x^2+4) = 0$ I am unable to find solution of this equation  and I think this is crucial to solving it. I also tried other ways to solve this equation, such as moving 1 to the other side of the equation. And then I multiply both sides by $(x-2)^2 \cdot (x+2)^2$ But I also failed because I probably at some point illegally raised both sides to the second power. I also have answer for that inequality: $x\in(-2,\sqrt{2})\cup(2, \infty)$ I also tried to calculating this equation using programs such as wolfram alpha but only gives me a result and I would like to know how I could solve it. Thank you in advance for your help.","Hi I'm having problem with inequality I rearranged the equation a bit and I did this holds if This part is the most difficult for me I am unable to find solution of this equation  and I think this is crucial to solving it. I also tried other ways to solve this equation, such as moving 1 to the other side of the equation. And then I multiply both sides by But I also failed because I probably at some point illegally raised both sides to the second power. I also have answer for that inequality: I also tried to calculating this equation using programs such as wolfram alpha but only gives me a result and I would like to know how I could solve it. Thank you in advance for your help.","\frac{x\sqrt{|x^2 - 4|}}{x^2 - 4} - 1 > 0 \frac{x\sqrt{|x^2 - 4|}-x^2+4}{x^2 - 4}  > 0 (x-2) \cdot(x+2)\cdot (x\sqrt{|x^2 - 4|}-x^2+4)>0 (x\sqrt{|x^2 - 4|}-x^2+4) = 0 (x-2)^2 \cdot (x+2)^2 x\in(-2,\sqrt{2})\cup(2, \infty)","['linear-algebra', 'analysis', 'inequality']"
49,Question About Collecting Power of $x^n$ in Taylor Expansion of $e^{\sin(x)}$.,Question About Collecting Power of  in Taylor Expansion of .,x^n e^{\sin(x)},"Theorem 1 For real numbers $\left\{a_{j,k}\right\}_{j,k\ge 1},$ if $$\sum_{j=1}^\infty \sum_{k=1}^\infty \left|a_{j,k}\right|<\infty$$ then for any bijection $\sigma : \mathbb{N} \to \mathbb{N}^2$ we have that the sums $$\sum_{j=1}^\infty \sum_{k=1}^\infty a_{j,k} = \sum_{k=1}^\infty \sum_{j=1}^\infty a_{j,k}  = \sum_{n=1}^\infty a_{\sigma(n)}$$ all converge and are equal to one another. Now in some post I saw $$e^y=1+y+\frac12y^2+\frac16y^3+\frac1{24}y^4+\cdots\tag1$$ and that $$\sin(x)=x-\frac16x^3+\cdots\tag2$$ So, in the RHS of $(1)$ replace $y$ with the RHS of $(2)$ . i.e $$e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+$$ Now since series of $\sin(x)$ is absolutely convergence things like $\left(x-\frac16x^3+\cdots\right)^2$ can be computed in any order. so we can collect power of $x$ in $\left(x-\frac16x^3+\cdots\right)^2$ . My question is about why we can do this on $$e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+$$ if  we have $$|1|+\left(|x|+|-\frac16x^3|+\cdots\right)+\frac12 \left(|x|+ |-\frac16x^3|+\cdots\right)^2+\cdots+\tag{3}$$ is finite then we can use above Theorem 1 and collect power of $x^n$ . So my question is sum in $(3)$ is indeed finite . Or there is something different thing that we are using there. Edit: Let $f(x)=\sum a_n x^n$ and $g(x)=\sum b_n x^n$ . assume Radius of convergence of $\sum a_n x^n$ is enough for composition then we have $$f(g(x))=a_0+a_1 \left(\sum b_n x^n \right)^1+a_2 \left(\sum b_n x^n \right)^2 +\cdots$$ Now my question is about rearrange of this series. If it is always true that $$|a_0|+a_1 \left(\sum |b_n x^n| \right)^1+a_2 \left(\sum |b_n x^n |\right)^2 +\cdots$$ Is finite then we can use theorem and it answers my question. If not then why we can rearrange?","Theorem 1 For real numbers if then for any bijection we have that the sums all converge and are equal to one another. Now in some post I saw and that So, in the RHS of replace with the RHS of . i.e Now since series of is absolutely convergence things like can be computed in any order. so we can collect power of in . My question is about why we can do this on if  we have is finite then we can use above Theorem 1 and collect power of . So my question is sum in is indeed finite . Or there is something different thing that we are using there. Edit: Let and . assume Radius of convergence of is enough for composition then we have Now my question is about rearrange of this series. If it is always true that Is finite then we can use theorem and it answers my question. If not then why we can rearrange?","\left\{a_{j,k}\right\}_{j,k\ge 1}, \sum_{j=1}^\infty \sum_{k=1}^\infty \left|a_{j,k}\right|<\infty \sigma : \mathbb{N} \to \mathbb{N}^2 \sum_{j=1}^\infty \sum_{k=1}^\infty a_{j,k}
= \sum_{k=1}^\infty \sum_{j=1}^\infty a_{j,k} 
= \sum_{n=1}^\infty a_{\sigma(n)} e^y=1+y+\frac12y^2+\frac16y^3+\frac1{24}y^4+\cdots\tag1 \sin(x)=x-\frac16x^3+\cdots\tag2 (1) y (2) e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+ \sin(x) \left(x-\frac16x^3+\cdots\right)^2 x \left(x-\frac16x^3+\cdots\right)^2 e^{\sin(x)}=1+\left(x-\frac16x^3+\cdots\right)+\frac12 \left(x-\frac16x^3+\cdots\right)^2+\frac16 \left(x-\frac16x^3+\cdots\right)^3+\cdots+ |1|+\left(|x|+|-\frac16x^3|+\cdots\right)+\frac12 \left(|x|+ |-\frac16x^3|+\cdots\right)^2+\cdots+\tag{3} x^n (3) f(x)=\sum a_n x^n g(x)=\sum b_n x^n \sum a_n x^n f(g(x))=a_0+a_1 \left(\sum b_n x^n \right)^1+a_2 \left(\sum b_n x^n \right)^2 +\cdots |a_0|+a_1 \left(\sum |b_n x^n| \right)^1+a_2 \left(\sum |b_n x^n |\right)^2 +\cdots","['real-analysis', 'sequences-and-series', 'analysis', 'taylor-expansion', 'power-series']"
50,When does the derivative $f'(0)$ of $f(x)= (x^p)^{1/q}$ exist?,When does the derivative  of  exist?,f'(0) f(x)= (x^p)^{1/q},"Let $c=p/q$ where $p,q \in \mathbb Z$ are coprime and $q$ is odd. Define $f:\mathbb R \to \mathbb R$ by $f(x)=x^c=(x^p)^{1/q}$ . Question: When does $f^\prime(0)$ exist? What I've tried (and where I'm stuck): If $c<0$ , then $0^c$ is undefined. So, we must have $c\ge 0$ . If $c=0$ , then $f(x)=1$ for all $x$ and $f^\prime(0)=0$ . (Trivial case.) If $c=1$ , then $f(x)=x$ for all $x$ and $f^\prime(0)=1$ . (Trivial case.) If $0<c<1$ , then it seems that $f^\prime(0)$ is undefined. Is this true and how do I prove this? If $c>1$ , then it seems that $f^\prime(0)=0$ . Is this true and how do I prove this?","Let where are coprime and is odd. Define by . Question: When does exist? What I've tried (and where I'm stuck): If , then is undefined. So, we must have . If , then for all and . (Trivial case.) If , then for all and . (Trivial case.) If , then it seems that is undefined. Is this true and how do I prove this? If , then it seems that . Is this true and how do I prove this?","c=p/q p,q \in \mathbb Z q f:\mathbb R \to \mathbb R f(x)=x^c=(x^p)^{1/q} f^\prime(0) c<0 0^c c\ge 0 c=0 f(x)=1 x f^\prime(0)=0 c=1 f(x)=x x f^\prime(0)=1 0<c<1 f^\prime(0) c>1 f^\prime(0)=0","['calculus', 'analysis']"
51,Convergent Improper Integral,Convergent Improper Integral,,"I am analyzing the following integral $$ \int_0^{+\infty}\frac{\log(x)}{(x-a)(x+b)}dx $$ with $a,b>0$ . Playing a little with the values I concluded that this integral is only convergent when $a=1$ and $b>0$ but I don't know how to prove it. Any ideas? Or maybe my trial-and-error attempts have failed?",I am analyzing the following integral with . Playing a little with the values I concluded that this integral is only convergent when and but I don't know how to prove it. Any ideas? Or maybe my trial-and-error attempts have failed?,"
\int_0^{+\infty}\frac{\log(x)}{(x-a)(x+b)}dx
 a,b>0 a=1 b>0","['calculus', 'analysis', 'improper-integrals']"
52,Proving that $f(x)$ satisfying $2f(x)f(1)\leq f(x)f(1)+1\leq 2f(2x)$ for all $x>0$ is a constant function,Proving that  satisfying  for all  is a constant function,f(x) 2f(x)f(1)\leq f(x)f(1)+1\leq 2f(2x) x>0,"How do I prove the following, where $f:[0,\infty)\to[0,\infty)$ : For $x>0$ if $f(x)$ satisfies $$2f(x)f(1)\leq f(x)f(1)+1\leq 2f(2x)$$ then $f(x)$ is a constant function. I have out found out that when $f(x)=1$ this inequality is satisfied So I tried to prove by contradictory by supposing  where when there exists $a$ that satisfies $f(a)>1$ or when satisfies $0<f(a)<1$ , but I failed.","How do I prove the following, where : For if satisfies then is a constant function. I have out found out that when this inequality is satisfied So I tried to prove by contradictory by supposing  where when there exists that satisfies or when satisfies , but I failed.","f:[0,\infty)\to[0,\infty) x>0 f(x) 2f(x)f(1)\leq f(x)f(1)+1\leq 2f(2x) f(x) f(x)=1 a f(a)>1 0<f(a)<1","['real-analysis', 'analysis', 'functions', 'inequality', 'contest-math']"
53,"Show there exists $c\in(0,1)$ such that $\frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)}$",Show there exists  such that,"c\in(0,1) \frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)}","I got a question like this: Prove there exists $c\in(0,1)$ in which $\displaystyle \frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)}$ , where $f:[0,1]\rightarrow\mathbb{R}$ is a differentiable function on $[0,1]$ such that $f(0)=0$ and $f(x)>0$ for any $x\in(0,1]$ . The first thing that comes to my mind is the Cauchy MVT, I tried with $g(x)=f(x)^2$ and created the term $g'(x)=2f(x)f'(x)$ so I can have the coefficient 2, but I could not proceed. Then I tried to rearrange the terms and notice $f'(1-c)f(c)-f'(c)f(1-c)=f'(c)f(1-c)$ is what we need to prove, the terms in the LHS is actually the derivative of $h(x)=f(x)f(1-x)$ , where $h(0)=h(1)=0$ , but I still failed to proceed. May I look for some advice?","I got a question like this: Prove there exists in which , where is a differentiable function on such that and for any . The first thing that comes to my mind is the Cauchy MVT, I tried with and created the term so I can have the coefficient 2, but I could not proceed. Then I tried to rearrange the terms and notice is what we need to prove, the terms in the LHS is actually the derivative of , where , but I still failed to proceed. May I look for some advice?","c\in(0,1) \displaystyle \frac{f'(1-c)}{f(1-c)}=\frac{2f'(c)}{f(c)} f:[0,1]\rightarrow\mathbb{R} [0,1] f(0)=0 f(x)>0 x\in(0,1] g(x)=f(x)^2 g'(x)=2f(x)f'(x) f'(1-c)f(c)-f'(c)f(1-c)=f'(c)f(1-c) h(x)=f(x)f(1-x) h(0)=h(1)=0","['analysis', 'derivatives', 'mean-value-theorem']"
54,A quite tricky problem about mean value theorem.,A quite tricky problem about mean value theorem.,,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be twice differentiable. Suppose $f'(0)=f'(1)=2$ and $\forall x \in [0,1]$ , $|f''(x)|\leq 4$ . Prove that $|f(1)-f(0)|\leq3$ . I know how to prove that when the bound is 4. It simply needs taylor expansion that $f(1)=f(0)+f'(0)+\frac{f''(c)}{2}$ $f(1)-f(0)\leq2+\frac{f''(c)}{2}$ And the proof can be obtained immediately. But when the bound is 3 it really confuses me. Thank you for any help or advice.","Let be twice differentiable. Suppose and , . Prove that . I know how to prove that when the bound is 4. It simply needs taylor expansion that And the proof can be obtained immediately. But when the bound is 3 it really confuses me. Thank you for any help or advice.","f:\mathbb{R} \rightarrow \mathbb{R} f'(0)=f'(1)=2 \forall x \in [0,1] |f''(x)|\leq 4 |f(1)-f(0)|\leq3 f(1)=f(0)+f'(0)+\frac{f''(c)}{2} f(1)-f(0)\leq2+\frac{f''(c)}{2}","['real-analysis', 'calculus', 'analysis', 'taylor-expansion', 'mean-value-theorem']"
55,"Existence of ""induced measure"" on fibers of a measurable function between measure spaces?","Existence of ""induced measure"" on fibers of a measurable function between measure spaces?",,"Let $f : X\rightarrow Y$ be a measurable function between measure spaces $X,Y$ with measures denoted $\mu,\nu$ respectively. Suppose singleton subsets of $Y$ are measurable; hence fibers of $f$ are measurable subsets in $X$ . My question is, in general does there exist a collection $\{\lambda_y\}_{y\in Y}$ where $\lambda_y$ is a measure on $f^{-1}(y)$ for each $y\in Y$ , such that for every measurable subset $A \subset X$ we have $$ \mu(A) = \int_{y \in Y} \big(\lambda_y(A \cap f^{-1}(y))\big)\,d\nu(y) \;\;? $$ If not in general, then does this hold when, for example, $\mu,\nu$ are $\sigma$ -finite and there is some kind of absolute continuity condition, as in the Radon-Nikodym theorem? My guess would be that this is somehow related to the R-N theorem; however, I'm not sure how to construct any precise relationship between the two situations. Would anyone have any suggestions or hints on how to think about this?","Let be a measurable function between measure spaces with measures denoted respectively. Suppose singleton subsets of are measurable; hence fibers of are measurable subsets in . My question is, in general does there exist a collection where is a measure on for each , such that for every measurable subset we have If not in general, then does this hold when, for example, are -finite and there is some kind of absolute continuity condition, as in the Radon-Nikodym theorem? My guess would be that this is somehow related to the R-N theorem; however, I'm not sure how to construct any precise relationship between the two situations. Would anyone have any suggestions or hints on how to think about this?","f : X\rightarrow Y X,Y \mu,\nu Y f X \{\lambda_y\}_{y\in Y} \lambda_y f^{-1}(y) y\in Y A \subset X 
\mu(A) = \int_{y \in Y} \big(\lambda_y(A \cap f^{-1}(y))\big)\,d\nu(y)
\;\;?
 \mu,\nu \sigma","['analysis', 'measure-theory', 'ergodic-theory', 'measurable-functions']"
56,How do I solve $ \int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}dx$?,How do I solve ?, \int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}dx,"I had to solve this integral: $$ \int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}\mathrm{d}x $$ by the substitution $x=\sqrt{3}t$ the indefinite integral can be written as: $$ \int \frac{x^2}{\sqrt{3+x^2}}\;\mathrm{d}x = \int \frac{3t^2}{\sqrt{1+t^2}}\; \mathrm{d}t $$ at this stage I performed the substitution $\displaystyle t=\frac{e^u-e^{-u}}{2}$ , but the computations are definitely too tough for being a high school exercise. I'd like to know if there's a simpler method, by substitution, that allows to solve it.","I had to solve this integral: by the substitution the indefinite integral can be written as: at this stage I performed the substitution , but the computations are definitely too tough for being a high school exercise. I'd like to know if there's a simpler method, by substitution, that allows to solve it.","
\int_{0}^{1}\frac{x^2}{\sqrt{3+x^2}}\mathrm{d}x
 x=\sqrt{3}t 
\int \frac{x^2}{\sqrt{3+x^2}}\;\mathrm{d}x = \int \frac{3t^2}{\sqrt{1+t^2}}\; \mathrm{d}t
 \displaystyle t=\frac{e^u-e^{-u}}{2}","['integration', 'analysis']"
57,Derivative of Inverse Linear Transformation,Derivative of Inverse Linear Transformation,,"I am going through Sagle and Walde's Introduction to Lie Groups and Lie Algebras and am in the first chapter reviewing (advanced) calculus. I am mostly OK with the advanced calculus stuff, I have been through most of Widder and some of Spivak. There is an exercise in the differentiation section I don't quite get: Show the map $$ f: GL(V) \rightarrow GL(V) : T \rightarrow T^{-1} $$ is differentiable at P and $$ Df(P)T = -P^{-1}TP^{-1}. $$ Thus show $f\in C^{1}(GL(V),End(V))$ I understand the question, however, I don't see how we can end up with a $P^{-1}$ multiplied on both sides. If I say: $$ \begin{align} f(P)&=P^{-1} \\ Pf(P)&=I \\ f(P) + PDf(P) &= 0 \\ PDf(P) &= -f(P) \\ PDf(P) &= -P^{-1} \\ Df(P) &= -P^{-2} \end{align} $$ With this derivative, we would get $$ Df(P)T = -P^{-2}T $$ Additonally, the derivative should be a linear transformation, and so it should have an inverse. In my case, that inverse is $P^{2}$ . In the case of what the book claims, I don't see how there can be a single inverse independent of $T$ . I am guessing I am missing something because the book is Volume 51 in the Academic Press series Pure and Applied Mathematics, so I am sure a lot of good eyes were on it. Any clarification would be appreciated.","I am going through Sagle and Walde's Introduction to Lie Groups and Lie Algebras and am in the first chapter reviewing (advanced) calculus. I am mostly OK with the advanced calculus stuff, I have been through most of Widder and some of Spivak. There is an exercise in the differentiation section I don't quite get: Show the map is differentiable at P and Thus show I understand the question, however, I don't see how we can end up with a multiplied on both sides. If I say: With this derivative, we would get Additonally, the derivative should be a linear transformation, and so it should have an inverse. In my case, that inverse is . In the case of what the book claims, I don't see how there can be a single inverse independent of . I am guessing I am missing something because the book is Volume 51 in the Academic Press series Pure and Applied Mathematics, so I am sure a lot of good eyes were on it. Any clarification would be appreciated.","
f: GL(V) \rightarrow GL(V) : T \rightarrow T^{-1}
 
Df(P)T = -P^{-1}TP^{-1}.
 f\in C^{1}(GL(V),End(V)) P^{-1} 
\begin{align}
f(P)&=P^{-1} \\
Pf(P)&=I \\
f(P) + PDf(P) &= 0 \\
PDf(P) &= -f(P) \\
PDf(P) &= -P^{-1} \\
Df(P) &= -P^{-2}
\end{align}
 
Df(P)T = -P^{-2}T
 P^{2} T","['analysis', 'multivariable-calculus', 'derivatives']"
58,$\underset{n\rightarrow\infty}{\lim} \frac{f_n^{(n)}(\frac{1}{n})}{n!}$,,\underset{n\rightarrow\infty}{\lim} \frac{f_n^{(n)}(\frac{1}{n})}{n!},"I just try to solve a problem, but I'm not sure it's right or not. Here is the problem: Suppose $f_n(x)=x^n\ln{x}$ , $ n \in \mathbb{N}$ . Find $$  \lim_{n\to\infty} \frac{f_n^{(n)}(\frac{1}{n})}{n!} $$ My idea is by Leibniz product rule $$ \left[\left(\frac{1}{n}\right)^n\ln{\frac{1}{n}}\right]^{(n)}=\overset{n}{\underset{k=0}{\sum}}\binom{n}{k}\cdot k!\cdot n^{-2n-k} $$ Since n approaches to infinity, I check the n-th term, $n!\cdot n^{-n}$ . With the denominator, $$ \lim_{n\to \infty}n^{-n}= e^{\lim_{n\to \infty}(-n\ln{n})}=0$$ I appreciate your feedback.","I just try to solve a problem, but I'm not sure it's right or not. Here is the problem: Suppose , . Find My idea is by Leibniz product rule Since n approaches to infinity, I check the n-th term, . With the denominator, I appreciate your feedback.","f_n(x)=x^n\ln{x}  n \in \mathbb{N} 
 \lim_{n\to\infty} \frac{f_n^{(n)}(\frac{1}{n})}{n!}
 
\left[\left(\frac{1}{n}\right)^n\ln{\frac{1}{n}}\right]^{(n)}=\overset{n}{\underset{k=0}{\sum}}\binom{n}{k}\cdot k!\cdot n^{-2n-k}
 n!\cdot n^{-n}  \lim_{n\to \infty}n^{-n}= e^{\lim_{n\to \infty}(-n\ln{n})}=0","['calculus', 'analysis']"
59,$\Bbb Z$ $\times$ $\Bbb N$ Countably Infinite,Countably Infinite,\Bbb Z \times \Bbb N,"I want to do this using an infinite grid as it seems easiest to me but I am open to other suggestions as well to improve my understanding. My attempt: $$\begin{array}{c | c | c | c | c} (0,0) & (1,0) & (-1,0) & (2,0) & (-2,0).....\\ \hline (0,1) & (1,1) & (-1,1) & (2,1) & (-2,1).....\\ \hline (0,2) & (1,2) & (-1,2) & (2,2) & (-2,2)..... \\ \hline (0,3) & (1,3) & (-1,3) & (2,3) & (-2,3)..... \end{array}$$ $$\begin{array}{c | c | c | c | c} 1 & 3 & 6 & 10 & 15.....\\ \hline 2 & 5 & 9 & 14 & 20.....\\ \hline 4 & 8 & 13 & 19 & 26..... \\ \hline 7 & 12 & 18 & 25 & 33..... \end{array}$$ The columns also extend down by an infinite amount. I then tried to create a bijection from $\Bbb N$ $->$ $\Bbb Z$ $\times$ $\Bbb N$ by starting from the top left at $(0,0)$ , and then preceding up each diagonal from bottom left to top right. So $1$ maps to $(0,0)$ , $2$ maps to $(0,1)$ , $3$ maps to $(1,0)$ and so on. This creates a bijection $f$ : $\Bbb N$ $->$ $\Bbb Z$ $\times$ $\Bbb N$ in which each natural number maps to the pair ( $a,b)$ in the grid above. I then explained why this is both injective and surjective to give a bijection.","I want to do this using an infinite grid as it seems easiest to me but I am open to other suggestions as well to improve my understanding. My attempt: The columns also extend down by an infinite amount. I then tried to create a bijection from by starting from the top left at , and then preceding up each diagonal from bottom left to top right. So maps to , maps to , maps to and so on. This creates a bijection : in which each natural number maps to the pair ( in the grid above. I then explained why this is both injective and surjective to give a bijection.","\begin{array}{c | c | c | c | c}
(0,0) & (1,0) & (-1,0) & (2,0) & (-2,0).....\\ \hline (0,1) & (1,1) & (-1,1) & (2,1) & (-2,1).....\\ \hline (0,2) & (1,2) & (-1,2) & (2,2) & (-2,2)..... \\ \hline (0,3) & (1,3) & (-1,3) & (2,3) & (-2,3).....
\end{array} \begin{array}{c | c | c | c | c}
1 & 3 & 6 & 10 & 15.....\\ \hline 2 & 5 & 9 & 14 & 20.....\\ \hline 4 & 8 & 13 & 19 & 26..... \\ \hline 7 & 12 & 18 & 25 & 33.....
\end{array} \Bbb N -> \Bbb Z \times \Bbb N (0,0) 1 (0,0) 2 (0,1) 3 (1,0) f \Bbb N -> \Bbb Z \times \Bbb N a,b)","['combinatorics', 'analysis']"
60,"For $F\in C^1(\mathbb{R}^n)$, if its Jacobian is singular at least one point, could its inverse be Lipschitz?","For , if its Jacobian is singular at least one point, could its inverse be Lipschitz?",F\in C^1(\mathbb{R}^n),"Consider map $F \in C^1(\mathbb{R}^n)$ , if there exists $x_0$ such that Jacobian $JF(x_0)$ is singular, we know that the inverse $F^{-1}$ may exist (e.g. $x\mapsto x^3$ ) and if it exists, it cannot be also in $C^1(\mathbb{R}^n)$ (because it is not differentiable at $x_0$ ). But the question is, in this case, could the inverse be Lipschitz? I can neither prove that the inverse is definitely not Lipschitz nor provide an example. Any hints? Thanks!","Consider map , if there exists such that Jacobian is singular, we know that the inverse may exist (e.g. ) and if it exists, it cannot be also in (because it is not differentiable at ). But the question is, in this case, could the inverse be Lipschitz? I can neither prove that the inverse is definitely not Lipschitz nor provide an example. Any hints? Thanks!",F \in C^1(\mathbb{R}^n) x_0 JF(x_0) F^{-1} x\mapsto x^3 C^1(\mathbb{R}^n) x_0,"['analysis', 'lipschitz-functions']"
61,Proof of the Density Theorem using contradiction?,Proof of the Density Theorem using contradiction?,,"I just started studying Introduction to Real Analysis for the first time. I'm sorry if the following attempt contains elementary mistakes. The Density Theorem states that if $x$ and $y$ are any real numbers with $x < y$ , then there exists a rational number $r$ such that $x < r < y$ . In the book, they use the Archimedean Property to prove the theorem, but I'm thinking of trying to solve it using contradiction as follows: """"Suppose, by contradiction, that $x < y$ but for all rationals $r$ , $r \geq y$ or $r \leq x$ (simply the negation of the wanted conclusion). Case 1: If $r \geq y$ , then $y$ is a lower bound of $\mathbb{Q}$ . Since $\mathbb{Q}$ is unbounded, then this case is impossible. Case 2: If $r \leq x$ , then $x$ is an upper bound of $\mathbb{Q}$ . Similarly, $\mathbb{Q}$ being unbounded implies this is impossible. Case 3: If $ r \geq y$ and $r \leq x$ then, since $x < y$ , $r \geq y > r$ which is impossible (also note that Cases 1 and 2 eliminate Case 3 completely). Since all cases are impossible, the Density Theorem must be true."""" Is this a valid proof? If not, why? Thank you very much.","I just started studying Introduction to Real Analysis for the first time. I'm sorry if the following attempt contains elementary mistakes. The Density Theorem states that if and are any real numbers with , then there exists a rational number such that . In the book, they use the Archimedean Property to prove the theorem, but I'm thinking of trying to solve it using contradiction as follows: """"Suppose, by contradiction, that but for all rationals , or (simply the negation of the wanted conclusion). Case 1: If , then is a lower bound of . Since is unbounded, then this case is impossible. Case 2: If , then is an upper bound of . Similarly, being unbounded implies this is impossible. Case 3: If and then, since , which is impossible (also note that Cases 1 and 2 eliminate Case 3 completely). Since all cases are impossible, the Density Theorem must be true."""" Is this a valid proof? If not, why? Thank you very much.",x y x < y r x < r < y x < y r r \geq y r \leq x r \geq y y \mathbb{Q} \mathbb{Q} r \leq x x \mathbb{Q} \mathbb{Q}  r \geq y r \leq x x < y r \geq y > r,"['real-analysis', 'analysis', 'solution-verification', 'proof-writing', 'alternative-proof']"
62,Show that zeros of a differentiable function is countable when the the intersection of the set of zeros and the set of critical point is empty.,Show that zeros of a differentiable function is countable when the the intersection of the set of zeros and the set of critical point is empty.,,Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function such that $\mathbf{A}=\{x\in\mathbb{R}:f(x)=0\}$ and $\mathbf{B}=\{x\in\mathbb{R}:f'(x)=0\}$ and $\mathbf{A}\cap\mathbf{B}=\phi$ . Then show that $\mathbf{A}$ is countable. What I know is $sin{x}$ is an example of such a function. The cardinality of $\mathbf{A}$ for $sin{x}$ countable. But I don't how to prove this in general for any function satisfying these conditions?,Let be a differentiable function such that and and . Then show that is countable. What I know is is an example of such a function. The cardinality of for countable. But I don't how to prove this in general for any function satisfying these conditions?,f:\mathbb{R}\to\mathbb{R} \mathbf{A}=\{x\in\mathbb{R}:f(x)=0\} \mathbf{B}=\{x\in\mathbb{R}:f'(x)=0\} \mathbf{A}\cap\mathbf{B}=\phi \mathbf{A} sin{x} \mathbf{A} sin{x},"['real-analysis', 'calculus', 'analysis', 'measure-theory']"
63,Does $\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ converges?,Does  converges?,\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n},"I try to study the convergence or divergence of the series $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ . We can see that this series diverges absolutely, because $\displaystyle\sum_{n=2}^\infty\frac{1}{|n+(-1)^n|}$ is a rearrangement of $\displaystyle\sum_{n=2}\frac{1}{n}$ which equals to $+\infty$ . Consequently, we can't say for sure that the rearrangement $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ of $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n}$ converges. That's the part where I am stuck. How exactly can I study the convergence (or not) of the first series? Should I apply Cauchy's criterion and prove that $s_n=\displaystyle\sum_{k=2}^n \frac{(-1)^k}{k+(-1)^k},n\in\mathbb{N} $ is a basic sequence? Thanks.","I try to study the convergence or divergence of the series . We can see that this series diverges absolutely, because is a rearrangement of which equals to . Consequently, we can't say for sure that the rearrangement of converges. That's the part where I am stuck. How exactly can I study the convergence (or not) of the first series? Should I apply Cauchy's criterion and prove that is a basic sequence? Thanks.","\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n} \displaystyle\sum_{n=2}^\infty\frac{1}{|n+(-1)^n|} \displaystyle\sum_{n=2}\frac{1}{n} +\infty \displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n} \displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n} s_n=\displaystyle\sum_{k=2}^n \frac{(-1)^k}{k+(-1)^k},n\in\mathbb{N} ","['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
64,Are rings of power series over a local field complete?,Are rings of power series over a local field complete?,,"Let $K$ be a finite extension of $\mathbb{Q}_p$ , and let $D$ be some disk $D = \{ x\in \overline{K} \mid |x| < c < 1\}$ . Is the set of power series in $K[[T]]$ which converge on $D$ and are bounded on $D$ complete with respect to the supremum norm $\|f\|_\infty=\sup_{x\in \overline{K}, |x|<c} |f(x)|$ ?","Let be a finite extension of , and let be some disk . Is the set of power series in which converge on and are bounded on complete with respect to the supremum norm ?","K \mathbb{Q}_p D D = \{ x\in \overline{K} \mid |x| < c < 1\} K[[T]] D D \|f\|_\infty=\sup_{x\in \overline{K}, |x|<c} |f(x)|","['number-theory', 'analysis', 'p-adic-number-theory', 'local-field']"
65,"Is $f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x} $ a homeomorphism?",Is  a homeomorphism?,"f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x} ","Consider $f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x} $ . I think $f$ is a homeomorphism but I still need to show that the inverse $f^{-1}$ is continuous. For the inverse I calculated if $0\leq\theta<\pi$ that $f^{-1}(e^{i\theta})= \frac{\theta}{\pi} $ and if $\pi<\theta<2\pi$ then $\frac{\theta-2\pi}{\pi}$ . How do I go on for the continuity?",Consider . I think is a homeomorphism but I still need to show that the inverse is continuous. For the inverse I calculated if that and if then . How do I go on for the continuity?,"f:(-1,1)\to \{y\in\mathbb{C}\mid |y| =1\}\setminus\{-1\}:x\mapsto e^{i\pi x}  f f^{-1} 0\leq\theta<\pi f^{-1}(e^{i\theta})= \frac{\theta}{\pi}  \pi<\theta<2\pi \frac{\theta-2\pi}{\pi}",['analysis']
66,"Suppose $f^{\prime\prime}(y) < 0$ at a point $y$, when it is true that $f^{\prime\prime}(z) < 0$ for all $z$ in an interval around $y$?","Suppose  at a point , when it is true that  for all  in an interval around ?",f^{\prime\prime}(y) < 0 y f^{\prime\prime}(z) < 0 z y,"Consider some function $f: \mathbb{R} \to \mathbb{R}$ . Suppose I computed the second derivative of $f$ and then proceeded to evaluate at a point $y$ , in which I found, $$f^{\prime\prime}(y) < 0$$ When it is true that $f^{\prime\prime}(z) < 0$ for all $z$ in an interval around $y$ ? I am thinking of a $\cap$ looking function. If $f^{\prime\prime}$ is continuous and differentiable everywhere, then I can move some epsilon distance away from that point $y$ and the concavity condition still holds, is this right? Are there any other conditions (possibly more relaxed than continuous differentiability of the second derivative) such that this holds.","Consider some function . Suppose I computed the second derivative of and then proceeded to evaluate at a point , in which I found, When it is true that for all in an interval around ? I am thinking of a looking function. If is continuous and differentiable everywhere, then I can move some epsilon distance away from that point and the concavity condition still holds, is this right? Are there any other conditions (possibly more relaxed than continuous differentiability of the second derivative) such that this holds.",f: \mathbb{R} \to \mathbb{R} f y f^{\prime\prime}(y) < 0 f^{\prime\prime}(z) < 0 z y \cap f^{\prime\prime} y,"['calculus', 'analysis', 'derivatives']"
67,Show that $f(x) = e^x \cos(x)$ on $\mathbb{R}$ is a tempered distribution,Show that  on  is a tempered distribution,f(x) = e^x \cos(x) \mathbb{R},"As shown in the title. I know that the anti-derivative of $f(x) = e^x \cos(x)$ is $ \frac{\sin(x)+\cos(x)}{2} e^x$ , whose anti-derivative is $\frac{\sin(x)}{2}e^x$ ...but not sure how to prove it.","As shown in the title. I know that the anti-derivative of is , whose anti-derivative is ...but not sure how to prove it.",f(x) = e^x \cos(x)  \frac{\sin(x)+\cos(x)}{2} e^x \frac{\sin(x)}{2}e^x,"['real-analysis', 'analysis', 'distribution-theory', 'schwartz-space']"
68,"If $\int_0^1 x^n d\mu = 0$ for $n = 0, 1, 2, 3, ...$ then $\mu = 0$",If  for  then,"\int_0^1 x^n d\mu = 0 n = 0, 1, 2, 3, ... \mu = 0","I am trying to solve this problem: Let $\mu$ be a finite, Borel measure on $[0, 1]$ and suppose that $$\int_0^1 x^n d\mu = 0$$ for $n = 0, 1, 2, 3, ...$ . Show that $\mu = 0.$ I assume that the question allows $\mu$ to be signed, otherwise the problem is trivial. One potential strategy is clear: show that $\mu(I) = 0$ for any open interval $I \subseteq [0, 1],$ which implies that the measure of open set is zero. Then by outer regularity, we get that $\mu = 0.$ However, I haven't been able to show that $\mu(I) = 0.$ I tried using the Jordan Decomposition Theorem to write $\mu = \rho_+ - \rho_-$ with $\rho_+ \perp \rho_-$ , but no luck. Thank you very much for any help.","I am trying to solve this problem: Let be a finite, Borel measure on and suppose that for . Show that I assume that the question allows to be signed, otherwise the problem is trivial. One potential strategy is clear: show that for any open interval which implies that the measure of open set is zero. Then by outer regularity, we get that However, I haven't been able to show that I tried using the Jordan Decomposition Theorem to write with , but no luck. Thank you very much for any help.","\mu [0, 1] \int_0^1 x^n d\mu = 0 n = 0, 1, 2, 3, ... \mu = 0. \mu \mu(I) = 0 I \subseteq [0, 1], \mu = 0. \mu(I) = 0. \mu = \rho_+ - \rho_- \rho_+ \perp \rho_-","['integration', 'analysis', 'measure-theory']"
69,Mean Value Theorem related problem,Mean Value Theorem related problem,,"Let $f(x)\leq g(x)$ for all $x \in I$ , where $I$ is an interval $\subseteq$ R. Also, let $f(c) = g(c)$ for some $c \in I$ but not an endpoint. Prove that $f'(c) = g'(c)$ (assume differentiablity) I have tried the Mean Value Theorem, let I = [a,b]. So for $f'(c) = g'(c)$ I will need to prove that $f(a) - f(b) = g(a) - g(b)$ , but I am stuck at this. I have also tried the definition of derivative but I am still unable to go ahead. Please give me some hints on this. Thank you!","Let for all , where is an interval R. Also, let for some but not an endpoint. Prove that (assume differentiablity) I have tried the Mean Value Theorem, let I = [a,b]. So for I will need to prove that , but I am stuck at this. I have also tried the definition of derivative but I am still unable to go ahead. Please give me some hints on this. Thank you!",f(x)\leq g(x) x \in I I \subseteq f(c) = g(c) c \in I f'(c) = g'(c) f'(c) = g'(c) f(a) - f(b) = g(a) - g(b),['analysis']
70,"For all real numbers satisfying $a < b$, there exists an $n \in \mathbb{N}$ such that $a + 1/n < b.$","For all real numbers satisfying , there exists an  such that",a < b n \in \mathbb{N} a + 1/n < b.,"From Stephen Abbott's Understanding Analysis 1.2.11 : For all real numbers satisfying $a < b$ , there exists an $n \in \mathbb{N}$ such that $a + 1/n < b.$ My try: $$\forall a\in \Bbb R, \forall b\in \Bbb R, \exists n \in \Bbb N \space \text{that satisfies}: \space a<b \Rightarrow a+\frac 1n<b$$ $\because a<b $ $\therefore\exists m\in\Bbb R, m>0$ such that $b=a+m$ . So the statement we are proving becomes (substitute $b$ by $a+m$ ): $$\forall a\in \Bbb R, m \in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}: a<a+m \Rightarrow a+\frac 1n < a+m$$ Which is (take $a$ from both sides): $$\forall m\in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}:0<m \Rightarrow \frac 1 n<m$$ We write this as: $$\forall m\in \Bbb R, 0<m, \exists n\in\Bbb N \space \Rightarrow \frac 1 n<m$$ We define set $M = \{m\mid M=\Bbb R \cap m>0\}$ And set $N = \{\frac 1n \mid n \in \Bbb N\}$ $\because\forall q \in \Bbb Z, p\in\Bbb Z, \frac p q\in\Bbb Q$ $\space\space\space n\in\Bbb N\subset\Bbb Z$ $\therefore\frac 1n\in\Bbb Q$ Mention that $m\in\Bbb R$ $\because\Bbb Q\subsetneq\Bbb R$ $\therefore\forall n\in\Bbb N, \exists m \in \Bbb R \Rightarrow \frac 1n \geqslant m$ This statement is equivalent to: $$\forall m\in \Bbb R, 0<m, \nexists n\in\Bbb N \space \Rightarrow \frac 1 n<m$$ Which disproves the statement, which is wrong - What is wrong with my disproof? Also, I am learning how to use mathematical notations properly (because I am only in year 11 but anyway). Please tell me if there is any error in my expressions. Thanks a lot!","From Stephen Abbott's Understanding Analysis 1.2.11 : For all real numbers satisfying , there exists an such that My try: such that . So the statement we are proving becomes (substitute by ): Which is (take from both sides): We write this as: We define set And set Mention that This statement is equivalent to: Which disproves the statement, which is wrong - What is wrong with my disproof? Also, I am learning how to use mathematical notations properly (because I am only in year 11 but anyway). Please tell me if there is any error in my expressions. Thanks a lot!","a < b n \in \mathbb{N} a + 1/n < b. \forall a\in \Bbb R, \forall b\in \Bbb R, \exists n \in \Bbb N \space \text{that satisfies}: \space a<b \Rightarrow a+\frac 1n<b \because a<b  \therefore\exists m\in\Bbb R, m>0 b=a+m b a+m \forall a\in \Bbb R, m \in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}: a<a+m \Rightarrow a+\frac 1n < a+m a \forall m\in \Bbb R, \exists n\in\Bbb N \space \text{that satisfies}:0<m \Rightarrow \frac 1 n<m \forall m\in \Bbb R, 0<m, \exists n\in\Bbb N \space \Rightarrow \frac 1 n<m M = \{m\mid M=\Bbb R \cap m>0\} N = \{\frac 1n \mid n \in \Bbb N\} \because\forall q \in \Bbb Z, p\in\Bbb Z, \frac p q\in\Bbb Q \space\space\space n\in\Bbb N\subset\Bbb Z \therefore\frac 1n\in\Bbb Q m\in\Bbb R \because\Bbb Q\subsetneq\Bbb R \therefore\forall n\in\Bbb N, \exists m \in \Bbb R \Rightarrow \frac 1n \geqslant m \forall m\in \Bbb R, 0<m, \nexists n\in\Bbb N \space \Rightarrow \frac 1 n<m","['real-analysis', 'analysis', 'real-numbers']"
71,Integration with trig substitution,Integration with trig substitution,,"Trying to evaluate this using trig substitution: $$\int \frac {1}{49x^2 + 25}\mathrm{d}x $$ Here's how I'm going about it, using $x = 5/7(\tan\theta)$ $$\int \frac {1}{49\left(x^2 + \frac{25}{49}\right)}\mathrm{d}x $$ $$=\int \frac {1}{49\left(\frac{25}{49}\tan^2\theta + \frac{25}{49}\right)} \mathrm{d}\theta$$ $$=\int \frac {1}{(25\tan^2\theta + 25)} $$ $$=\int \frac {1}{25(\tan^2\theta + 1)}\mathrm{d}\theta $$ $$=\int \frac {1}{25\sec^2\theta}\mathrm{d}\theta $$ $$=\int \frac {\cos^2\theta}{25}\mathrm{d}\theta $$ $$=\frac{1}{50}(\theta + \sin\theta + \cos\theta) $$ To generalize for $x$ , $\theta = \arctan(7x/5)$ $$\frac{1}{50}\left(\arctan\left(\frac{7x}{5}\right) + \sin\left(\arctan\left(\frac{7x}{5}\right)\right) + \cos\left(\arctan\left(\frac{7x}{5}\right)\right)\right) $$ $$\frac{1}{50} \left(\frac{7x}{5\left(\frac{49x^2}{25}+1\right)} + \arctan\left(\frac{7x}{5}\right)\right)$$ But taking the derivative of this gets me: $$ \frac{35}{(49x^2 +25)^2}$$ Where is my mistake?","Trying to evaluate this using trig substitution: Here's how I'm going about it, using To generalize for , But taking the derivative of this gets me: Where is my mistake?",\int \frac {1}{49x^2 + 25}\mathrm{d}x  x = 5/7(\tan\theta) \int \frac {1}{49\left(x^2 + \frac{25}{49}\right)}\mathrm{d}x  =\int \frac {1}{49\left(\frac{25}{49}\tan^2\theta + \frac{25}{49}\right)} \mathrm{d}\theta =\int \frac {1}{(25\tan^2\theta + 25)}  =\int \frac {1}{25(\tan^2\theta + 1)}\mathrm{d}\theta  =\int \frac {1}{25\sec^2\theta}\mathrm{d}\theta  =\int \frac {\cos^2\theta}{25}\mathrm{d}\theta  =\frac{1}{50}(\theta + \sin\theta + \cos\theta)  x \theta = \arctan(7x/5) \frac{1}{50}\left(\arctan\left(\frac{7x}{5}\right) + \sin\left(\arctan\left(\frac{7x}{5}\right)\right) + \cos\left(\arctan\left(\frac{7x}{5}\right)\right)\right)  \frac{1}{50} \left(\frac{7x}{5\left(\frac{49x^2}{25}+1\right)} + \arctan\left(\frac{7x}{5}\right)\right)  \frac{35}{(49x^2 +25)^2},"['real-analysis', 'calculus', 'integration', 'analysis', 'indefinite-integrals']"
72,Prove that a subsequence converges.,Prove that a subsequence converges.,,"Suppose that $b_n^{2} \rightarrow4$ . Prove that $b_n$ has a subsequence that converges to $2$ or $-2$ . I'm not really sure on the approach, I see that simply taking the root yields something similar, but doesn't say much about a subsequence and doesn't really seem like a proof.","Suppose that . Prove that has a subsequence that converges to or . I'm not really sure on the approach, I see that simply taking the root yields something similar, but doesn't say much about a subsequence and doesn't really seem like a proof.",b_n^{2} \rightarrow4 b_n 2 -2,"['sequences-and-series', 'analysis', 'cauchy-sequences']"
73,What is the difference between $\mathbb{R}$ and $\mathbb{R}^1$?,What is the difference between  and ?,\mathbb{R} \mathbb{R}^1,"I am wondering that $\mathbb{R}$ and $\mathbb{R}^1$ are same or not. $\mathbb{R}$ is the real numbers, and $\mathbb{R}^1$ is a set of 1-tuples. I am so stucked on this. Thanks for the support.","I am wondering that and are same or not. is the real numbers, and is a set of 1-tuples. I am so stucked on this. Thanks for the support.",\mathbb{R} \mathbb{R}^1 \mathbb{R} \mathbb{R}^1,"['real-analysis', 'analysis', 'definition', 'real-numbers']"
74,Proving a general reasult in polynomials.,Proving a general reasult in polynomials.,,"If $f(x) = (x-a)(x-b)(x-c)(x-d) - 1$ where $a, b, c, d$ are distinct integers. Prove that $f(x)$ can't be factorized into integer polynomial with $deg 1$ In the above question I proved for three degree polynomial using contradiction. But couldn't use the same for the even degree polynomial.If anyone out there could help me would be of great help.",If where are distinct integers. Prove that can't be factorized into integer polynomial with In the above question I proved for three degree polynomial using contradiction. But couldn't use the same for the even degree polynomial.If anyone out there could help me would be of great help.,"f(x) = (x-a)(x-b)(x-c)(x-d) - 1 a, b, c, d f(x) deg 1","['analysis', 'functions', 'polynomials']"
75,Stokes Theorem application question,Stokes Theorem application question,,"Below is an excerpt from the book ""Partial Differential Equations"" by Evans.  The underlined equation confuses me.  Clearly it is an application of Stokes theorem, and the implication seems to be that if $f$ is any compactly supported smooth function (for simplicity say on all of $\mathbb{R}^n$ ) then $$\int_{\mathbb{R}^n-B_\epsilon(0)} f_{x^i} d{x}=\int_{\partial B_\epsilon(0)} f\cdot \frac{(-x^i)}{\epsilon} d{S}$$ (here i am just replacing $u\phi$ with $f$ and $\nu^i$ with $ \frac{(-x^i)}{\epsilon} $ ).  But I can't get this equation to come out of stokes theorem.  E.g. assume for simplicity $n=2$ (set $(x^1,x^2)=(x,y)$ ), and $\epsilon =1$ . Let $d\theta$ be the 1-form gotten by pulling back (via $\mathbb{R}^2-0\rightarrow S^1, v\mapsto v/|v|$ ) the volume form on $S^1$ .  Then stokes gives $$\int_{S^1}f\cdot(-x) d\theta=\int_{\mathbb{R}^2-B_1(0)}d(f\cdot(-x) d\theta)=\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x}dx\wedge d\theta+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y}dy\wedge d\theta.$$ Now it seems $dx\wedge d\theta = \frac{x}{x^2+y^2}~~ dx\wedge dy$ and $dy\wedge d\theta = \frac{y}{x^2+y^2}~~ dx\wedge dy$ so this gives $$\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x} \cdot \frac{x}{x^2+y^2} ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y} \cdot \frac{y}{x^2+y^2}~~dx dy=$$ $$\int_{\mathbb{R}^2-B_1(0)}- \frac{x}{x^2+y^2}\cdot f~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{x^2}{x^2+y^2}\cdot f_x ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{xy}{x^2+y^2}\cdot f_y~~dx dy.$$ I see no cancellations here or any way to make this look like $$\int_{\mathbb{R}^2-B_1(0)}f_x~~dx dy.$$","Below is an excerpt from the book ""Partial Differential Equations"" by Evans.  The underlined equation confuses me.  Clearly it is an application of Stokes theorem, and the implication seems to be that if is any compactly supported smooth function (for simplicity say on all of ) then (here i am just replacing with and with ).  But I can't get this equation to come out of stokes theorem.  E.g. assume for simplicity (set ), and . Let be the 1-form gotten by pulling back (via ) the volume form on .  Then stokes gives Now it seems and so this gives I see no cancellations here or any way to make this look like","f \mathbb{R}^n \int_{\mathbb{R}^n-B_\epsilon(0)} f_{x^i} d{x}=\int_{\partial B_\epsilon(0)} f\cdot \frac{(-x^i)}{\epsilon} d{S} u\phi f \nu^i  \frac{(-x^i)}{\epsilon}  n=2 (x^1,x^2)=(x,y) \epsilon =1 d\theta \mathbb{R}^2-0\rightarrow S^1, v\mapsto v/|v| S^1 \int_{S^1}f\cdot(-x) d\theta=\int_{\mathbb{R}^2-B_1(0)}d(f\cdot(-x) d\theta)=\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x}dx\wedge d\theta+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y}dy\wedge d\theta. dx\wedge d\theta = \frac{x}{x^2+y^2}~~ dx\wedge dy dy\wedge d\theta = \frac{y}{x^2+y^2}~~ dx\wedge dy \int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial x} \cdot \frac{x}{x^2+y^2} ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}\frac{\partial f\cdot(-x)}{\partial y} \cdot \frac{y}{x^2+y^2}~~dx dy= \int_{\mathbb{R}^2-B_1(0)}- \frac{x}{x^2+y^2}\cdot f~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{x^2}{x^2+y^2}\cdot f_x ~~dx dy+\int_{\mathbb{R}^2-B_1(0)}- \frac{xy}{x^2+y^2}\cdot f_y~~dx dy. \int_{\mathbb{R}^2-B_1(0)}f_x~~dx dy.","['analysis', 'differential-geometry', 'stokes-theorem']"
76,Basic Burgers Equation Energy Estimate,Basic Burgers Equation Energy Estimate,,"This is a super simple estimate and I have been banging my head against the wall that I can't figure it out. I am trying to estimate the $||\cdot||_{H^k}$ norm of $u_t+uu_x=0$ and can't figure out the nonlinear term. I'm reading along with a paper, my goal is $\frac{d}{dt}||\partial_x^ku||_{L^2}^2\lesssim||u_x||_{L^\infty}||\partial_x^k u||_{L^2}^2$ . Is the following valid for estimating the nonlinear term (assuming $u\in C_0^\infty(\textbf{R})$ )? \begin{align*} \left|\int_{\mathbf{R}}\,\partial_x^k{u}\partial_x^k(uu_x)\,dx\right|   &= \left|(-1)^k\int_{\mathbf{R}}\,(uu_x)\partial_x^{2k}(u)\,dx\right| \\ &= \left|\int_{\mathbf{R}}\,(uu_x)\partial_x^{2k}(u)\,dx\right| \\   &\leqslant     ||u_x||_{L^\infty}\left|\int_{\mathbf{R}}\,u\partial_x^{2k}u\,dx     \right| \\ &=     ||u_x||_{L^\infty}\left|(-1)^k\int_{\mathbf{R}}\,(\partial_x^{k}u)^2\,dx     \right| \\   &=||u_x||_{L^\infty}||\partial_x^ku||_{L^2}^2 \end{align*} Seems like cheating and I am wondering if there is an inner product identity I am unaware of which makes this smoother. I am doubting myself because I thought I had it earlier and realized that I had made a pretty bad mistake. Thanks!","This is a super simple estimate and I have been banging my head against the wall that I can't figure it out. I am trying to estimate the norm of and can't figure out the nonlinear term. I'm reading along with a paper, my goal is . Is the following valid for estimating the nonlinear term (assuming )? Seems like cheating and I am wondering if there is an inner product identity I am unaware of which makes this smoother. I am doubting myself because I thought I had it earlier and realized that I had made a pretty bad mistake. Thanks!","||\cdot||_{H^k} u_t+uu_x=0 \frac{d}{dt}||\partial_x^ku||_{L^2}^2\lesssim||u_x||_{L^\infty}||\partial_x^k u||_{L^2}^2 u\in C_0^\infty(\textbf{R}) \begin{align*}
\left|\int_{\mathbf{R}}\,\partial_x^k{u}\partial_x^k(uu_x)\,dx\right|
  &= \left|(-1)^k\int_{\mathbf{R}}\,(uu_x)\partial_x^{2k}(u)\,dx\right| \\
&= \left|\int_{\mathbf{R}}\,(uu_x)\partial_x^{2k}(u)\,dx\right| \\
  &\leqslant
    ||u_x||_{L^\infty}\left|\int_{\mathbf{R}}\,u\partial_x^{2k}u\,dx
    \right| \\
&=
    ||u_x||_{L^\infty}\left|(-1)^k\int_{\mathbf{R}}\,(\partial_x^{k}u)^2\,dx
    \right| \\
  &=||u_x||_{L^\infty}||\partial_x^ku||_{L^2}^2
\end{align*}","['analysis', 'inequality', 'partial-differential-equations']"
77,"Why are orthogonal polynomials ""unique""?","Why are orthogonal polynomials ""unique""?",,"I am reading Numerical Analysis by Walter Gautschi, and the autor says (I quote): [Talks about $\{1, t, t^2, ..., t^n \}$ ] Since a linearly independent set can be orthogonalized by Gram-Schmidt, any measure $d \lambda$ of the type considered generates a unique [emphasis mine] set of (monic) orthogonal polynomials $\pi_j(t) = \pi_j(t; d \lambda)$ , $j=0, 1, 2, ...,$ satisfying $$\deg \pi_j = j, j = 0, 1, 2, ...,$$ $$\int_{\mathbb{R}} \pi_k(t) \pi_{\ell}(t) d \lambda (t) = 0 \text{ if } t \not = \ell.$$ There are called orthogonal polynomials relative to the measure $d \lambda$ . From what I understand, the author is saying that (for example) if we take the space spanned by $1, t, t^2$ and we have a fixed inner product $(u, v) = \int_a^b u(t)v(t)w(t) dt$ , then there exists a unique set of three monic, orthogonal polynomials which span this space. Furthermore, the author seems to say that this follows directly from the Gram-Schmidt process. I can sort of see a reason why these polynomials are unique; having made an orthogonal set $\{p_0, p_1, ..., p_{n-1} \}$ of $n$ polynomials, we want the $p_{n}$ (which has $n+1$ coefficients) to satisfy $$(p_k, p_n) = 0 \text{ for } k = 0, 1, ..., n-1$$ $$\text{leading coefficient = 1}$$ which is a system of $n+1$ equations in $n+1$ variables, so it could just have a unique solution. But I don't see why this system has to be non-singular.","I am reading Numerical Analysis by Walter Gautschi, and the autor says (I quote): [Talks about ] Since a linearly independent set can be orthogonalized by Gram-Schmidt, any measure of the type considered generates a unique [emphasis mine] set of (monic) orthogonal polynomials , satisfying There are called orthogonal polynomials relative to the measure . From what I understand, the author is saying that (for example) if we take the space spanned by and we have a fixed inner product , then there exists a unique set of three monic, orthogonal polynomials which span this space. Furthermore, the author seems to say that this follows directly from the Gram-Schmidt process. I can sort of see a reason why these polynomials are unique; having made an orthogonal set of polynomials, we want the (which has coefficients) to satisfy which is a system of equations in variables, so it could just have a unique solution. But I don't see why this system has to be non-singular.","\{1, t, t^2, ..., t^n \} d \lambda \pi_j(t) = \pi_j(t; d \lambda) j=0, 1, 2, ..., \deg \pi_j = j, j = 0, 1, 2, ..., \int_{\mathbb{R}} \pi_k(t) \pi_{\ell}(t) d \lambda (t) = 0 \text{ if } t \not = \ell. d \lambda 1, t, t^2 (u, v) = \int_a^b u(t)v(t)w(t) dt \{p_0, p_1, ..., p_{n-1} \} n p_{n} n+1 (p_k, p_n) = 0 \text{ for } k = 0, 1, ..., n-1 \text{leading coefficient = 1} n+1 n+1","['linear-algebra', 'analysis', 'numerical-methods']"
78,"In a topological space $X, G$ is open iff $G\cap\overline{A}\subseteq \overline{G\cap A}$",In a topological space  is open iff,"X, G G\cap\overline{A}\subseteq \overline{G\cap A}","In a topological space $X$ , $G$ is open iff $G\cap\overline{A}\subseteq \overline{G\cap A}$ , where $A$ is an arbitrary set in $X$ . To prove this I started from definition of an open set, I could not complete it. I even tried to prove it in metric space using def I could not complete it","In a topological space , is open iff , where is an arbitrary set in . To prove this I started from definition of an open set, I could not complete it. I even tried to prove it in metric space using def I could not complete it",X G G\cap\overline{A}\subseteq \overline{G\cap A} A X,"['general-topology', 'analysis']"
79,Understanding a statement in a paragraph in Royden (4^th edition) on pg.136,Understanding a statement in a paragraph in Royden (4^th edition) on pg.136,,"The paragraph is given below: My question is: Why this statement ""Since complements of sets of measure zero are dense in $\mathbb{R}$ "" is correct? could anyone explain this for me please?","The paragraph is given below: My question is: Why this statement ""Since complements of sets of measure zero are dense in "" is correct? could anyone explain this for me please?",\mathbb{R},"['real-analysis', 'analysis']"
80,Maximization of quotient of quadratic forms,Maximization of quotient of quadratic forms,,"I have a problem with finding: $$\max_{a \in \mathbb{R}^p} \frac{aCa}{aBa},$$ where $C$ and $B$ are $p \times p$ symmetric matrices. After differentiation I get the following result: $$\frac{2aC(aBa) - 2aB(aCa)}{(aBa)(aBa)} =0$$ I cant solve the above equation for $a$ . How to find it?",I have a problem with finding: where and are symmetric matrices. After differentiation I get the following result: I cant solve the above equation for . How to find it?,"\max_{a \in \mathbb{R}^p} \frac{aCa}{aBa}, C B p \times p \frac{2aC(aBa) - 2aB(aCa)}{(aBa)(aBa)} =0 a","['linear-algebra', 'analysis', 'derivatives', 'optimization', 'quadratic-forms']"
81,Liouville Theorem for Harmonic Functions,Liouville Theorem for Harmonic Functions,,"If $u$ is bounded and harmonic in $\mathbb{R}^n$ , then $u$ is constant For any twice differentiable function $u$ defined on an open subset $\Omega$ we have $$u(x)=\int\limits_\Omega G(x,y)\Delta u(y)dy+\int\limits_{\partial\Omega}\frac{\partial G}{\partial n}(x,y)u(y)dS_y$$ where $G$ is the corresponding Green's function. We take the region to be $B_a(0)$ and take $G(x,y)=\Phi(x-y)-\Phi(\frac{|x|}{a}|x^*-y|)$ where $\Phi$ is the fundamental solution to the Laplace equation $\Delta u=0$ and $x^*$ is the inverse of the point $x$ w.r.t. the ball $B_a(0)$ .It turns out then $$u(x)=\int\limits_{\partial\Omega}H(x,y)u(y)dS_y $$ where $$H(x,y)=\frac{a^2-|x|^2}{aw_n}\frac{1}{|x-y|^n}$$ $w_n$ being the surfacae integral of the unit sphere in $\mathbb{R}^n$ . Partially differentiating $H$ we get $$|H_i(0,y)|\leq \frac{n}{w_na^n}$$ which gives $$|\frac{\partial u}{\partial x_i}(0)|\leq \frac{n}{a}\|u\|_\infty$$ Now I want something like $$|\frac{\partial u}{\partial x_i}(x)|\leq \frac{n}{a}\|u\|_\infty \forall x$$ to conclude $u$ is constant. I am not sure how to do so. Any help is appreciated.","If is bounded and harmonic in , then is constant For any twice differentiable function defined on an open subset we have where is the corresponding Green's function. We take the region to be and take where is the fundamental solution to the Laplace equation and is the inverse of the point w.r.t. the ball .It turns out then where being the surfacae integral of the unit sphere in . Partially differentiating we get which gives Now I want something like to conclude is constant. I am not sure how to do so. Any help is appreciated.","u \mathbb{R}^n u u \Omega u(x)=\int\limits_\Omega G(x,y)\Delta u(y)dy+\int\limits_{\partial\Omega}\frac{\partial G}{\partial n}(x,y)u(y)dS_y G B_a(0) G(x,y)=\Phi(x-y)-\Phi(\frac{|x|}{a}|x^*-y|) \Phi \Delta u=0 x^* x B_a(0) u(x)=\int\limits_{\partial\Omega}H(x,y)u(y)dS_y
 H(x,y)=\frac{a^2-|x|^2}{aw_n}\frac{1}{|x-y|^n} w_n \mathbb{R}^n H |H_i(0,y)|\leq \frac{n}{w_na^n} |\frac{\partial u}{\partial x_i}(0)|\leq \frac{n}{a}\|u\|_\infty |\frac{\partial u}{\partial x_i}(x)|\leq \frac{n}{a}\|u\|_\infty \forall x u","['real-analysis', 'analysis', 'partial-differential-equations', 'harmonic-functions']"
82,Are there good lower bounds for the partial sums of the series $\sum 1/\log(n)$?,Are there good lower bounds for the partial sums of the series ?,\sum 1/\log(n),"Consider the partial sums $$S_n = \sum_{k=2}^n\frac{1}{\log(k)}.$$ Are there good lower bounds for $S_n$ as $n\to\infty$ ? I am not necessarily looking for sharp bounds (although they would be nice), just one that is fair. Of course a ""good"" lower bound would be one that go ""very"" fast to infinity.","Consider the partial sums Are there good lower bounds for as ? I am not necessarily looking for sharp bounds (although they would be nice), just one that is fair. Of course a ""good"" lower bound would be one that go ""very"" fast to infinity.",S_n = \sum_{k=2}^n\frac{1}{\log(k)}. S_n n\to\infty,"['real-analysis', 'sequences-and-series', 'analysis', 'logarithms', 'real-numbers']"
83,Let $(G_k)$ a sequence of open and dense subsets. Show that $\bigcap_{k\geqslant 0}G_k$ is also open and dense,Let  a sequence of open and dense subsets. Show that  is also open and dense,(G_k) \bigcap_{k\geqslant 0}G_k,"The exercise says Let $(G_k)$ a sequence of open and dense subsets in a complete metric space. Show that $\bigcap_{k\geqslant 0}G_k$ is also open and dense I have shown that $G:=\bigcap_{k\geqslant 0}G_k$ is dense, however I think that I have a counterexample for $G$ to be also open. If it would be true then it must also be true that $G^\complement =\bigcup_{k\geqslant 0}G_k^\complement $ would be closed and with empty interior. However setting $G_k^\complement :=\{1/k\}$ we have that $G^\complement=\{1/k:k\in \Bbb N_{> 0} \} $ have empty interior in $\Bbb R$ but it is not closed because zero is a limit point of $G^\complement $ . Im wrong or the exercise is wrong?","The exercise says Let a sequence of open and dense subsets in a complete metric space. Show that is also open and dense I have shown that is dense, however I think that I have a counterexample for to be also open. If it would be true then it must also be true that would be closed and with empty interior. However setting we have that have empty interior in but it is not closed because zero is a limit point of . Im wrong or the exercise is wrong?",(G_k) \bigcap_{k\geqslant 0}G_k G:=\bigcap_{k\geqslant 0}G_k G G^\complement =\bigcup_{k\geqslant 0}G_k^\complement  G_k^\complement :=\{1/k\} G^\complement=\{1/k:k\in \Bbb N_{> 0} \}  \Bbb R G^\complement ,"['analysis', 'metric-spaces', 'baire-category']"
84,How to integrate $\int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx$? [duplicate],How to integrate ? [duplicate],\int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx,"This question already has answers here : Integral: $\int_0^1\frac{\mathrm{Li}_2(x^2)}{\sqrt{1-x^2}}dx$ (4 answers) Closed 4 years ago . I'm confronted with a problem: prove that $\displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx=-\frac{\pi}{4}\ln^22$ I've already known $\displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x}{\tan x}dx=\frac{\pi}{2}\ln2\ $ and $\displaystyle \int_{0}^{\frac{\pi}{2}}\ln{\cos(x)}dx=-\frac{\pi}{2}\ln2\ $ (which are actually  equivalent to each other) But they're not sufficient to solve this, I guess.","This question already has answers here : Integral: $\int_0^1\frac{\mathrm{Li}_2(x^2)}{\sqrt{1-x^2}}dx$ (4 answers) Closed 4 years ago . I'm confronted with a problem: prove that I've already known and (which are actually  equivalent to each other) But they're not sufficient to solve this, I guess.",\displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x\ln{\cos(x)}}{\tan x}dx=-\frac{\pi}{4}\ln^22 \displaystyle \int_{0}^{\frac{\pi}{2}}\frac{x}{\tan x}dx=\frac{\pi}{2}\ln2\  \displaystyle \int_{0}^{\frac{\pi}{2}}\ln{\cos(x)}dx=-\frac{\pi}{2}\ln2\ ,"['calculus', 'integration', 'analysis', 'improper-integrals']"
85,Convergence of an infinite series containing the sine function,Convergence of an infinite series containing the sine function,,"Does the following infinite series converge? $\displaystyle \sum _{k=1}^{\infty } \frac{\sin (k)}{\sqrt[3]{k^2+1}}$ In case it converges, does it have a closed-form value or solution? Or just a numerical solution?","Does the following infinite series converge? In case it converges, does it have a closed-form value or solution? Or just a numerical solution?",\displaystyle \sum _{k=1}^{\infty } \frac{\sin (k)}{\sqrt[3]{k^2+1}},"['sequences-and-series', 'analysis', 'convergence-divergence']"
86,About the implication of $\epsilon>0$ being arbitrary.,About the implication of  being arbitrary.,\epsilon>0,"It is indeed a very basic question. I always read proofs stating that since $\epsilon>0$ is arbitrary, then some inequality holds. Also, I've been accepting it without problems. Now, I just figured out that this point is not completely clear to me. For example: as $\epsilon>0$ is arbitrary, we have that $f(x)>z-\epsilon \implies f(x)\geq z$ (I will be more specific here) Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a function, $z\in\mathbb{R}$ a given point and assume $\forall \epsilon>0, [f^{-1}(z-\epsilon,z+\epsilon)]^c=\{x\in \mathbb{R}:\mid f(x)-z\mid \geq \epsilon \}:=A_{\epsilon}$ is countable (on a crazy topology on domain called co-countable). I want to show that as $\epsilon>0$ is arbitrary then actually all $\mathbb{R}$ without the point $x:f(x)=z$ is countable (again, $\mathbb{R}$ together with the crazy co-countable topology). Looking at $\{x\in \mathbb{R}:\mid f(x)-z\mid \geq \epsilon \}$ if $f(x)=z$ were possible, $0\geq \epsilon>0$ which is a contradiction. Both assertions are acceptable to me, but I can't just put (intuitively) $\epsilon=0$ and infer the results. How would you properly argue it?","It is indeed a very basic question. I always read proofs stating that since is arbitrary, then some inequality holds. Also, I've been accepting it without problems. Now, I just figured out that this point is not completely clear to me. For example: as is arbitrary, we have that (I will be more specific here) Let be a function, a given point and assume is countable (on a crazy topology on domain called co-countable). I want to show that as is arbitrary then actually all without the point is countable (again, together with the crazy co-countable topology). Looking at if were possible, which is a contradiction. Both assertions are acceptable to me, but I can't just put (intuitively) and infer the results. How would you properly argue it?","\epsilon>0 \epsilon>0 f(x)>z-\epsilon \implies f(x)\geq z f:\mathbb{R}\rightarrow \mathbb{R} z\in\mathbb{R} \forall \epsilon>0, [f^{-1}(z-\epsilon,z+\epsilon)]^c=\{x\in \mathbb{R}:\mid f(x)-z\mid \geq \epsilon \}:=A_{\epsilon} \epsilon>0 \mathbb{R} x:f(x)=z \mathbb{R} \{x\in \mathbb{R}:\mid f(x)-z\mid \geq \epsilon \} f(x)=z 0\geq \epsilon>0 \epsilon=0","['analysis', 'self-learning']"
87,Convolution proof (symmetric),Convolution proof (symmetric),,"I want to show, that for some functions $ f,g \in L^1(\mathbb{R^n})$ hold: $$f*g=g*f$$ My proof so far: $ f*g= \int_{\mathbb{R^n}} f(x-y)f(y) dy$ Substitution: $\phi(y) =x-y$ This leads to: $$\int_{\mathbb{R^n}} f(\phi(y)) g(y) | Det(D \phi(y)| = \int_{\mathbb{R^n}} f(\phi(y)) g(y) (-1)^n dy$$ How can I go on from there?","I want to show, that for some functions hold: My proof so far: Substitution: This leads to: How can I go on from there?"," f,g \in L^1(\mathbb{R^n}) f*g=g*f  f*g= \int_{\mathbb{R^n}} f(x-y)f(y) dy \phi(y) =x-y \int_{\mathbb{R^n}} f(\phi(y)) g(y) | Det(D \phi(y)| = \int_{\mathbb{R^n}} f(\phi(y)) g(y) (-1)^n dy","['real-analysis', 'integration', 'analysis', 'convolution']"
88,A question about applying Birkhoff's ergodic Theorem.,A question about applying Birkhoff's ergodic Theorem.,,"Suppose that I have the transformation $$T(x,y)=(x+a,y+b), a,b \in \mathbb{R}$$ from the 2-dimensional torus $\mathbb{R}^2/\mathbb{Z}^2$ to itself. We know that this transformation in ergodic with respect to the Lebesgue 2-d measure on the torus if and only if $a,b $ are rationally independent which means $na+mb \in \mathbb{Z}$ if and only if $n=m=0$ . Now according to Birkhoff's ergodic theorem for any function $f\in C(\mathbb{R}^2/\mathbb{Z}^2,\mathbb{R})$ we have that $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f(T^j(x,y))=\int f dm$$ for almost all $(x,y) \in \mathbb{R}^2/\mathbb{Z}^2$ , where $T^j$ are the iterates of $T$ and $m$ is the 2-d Lebesgue measure and the integral is over $[0,1]\times[0,1]$ . Let's say that we choose the function $f(x,y)=\cos (2\pi y)$ which is continuous in $[0,1]\times[0,1]$ and we choose $a\not \in \mathbb{Q}$ and $b=0$ . Then $T$ is ergodic and $T^j(x,y)=(x+ja,y)$ , so we have that $$\lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\cos(2\pi y)=\int_{0}^{1}\int_{0}^{1} \cos (2\pi y) dydx=0$$ But this cannot be true almost everywhere since the sum on the left is equal with $\cos (2\pi y)$ . Now my Question is what am I doing wrong?","Suppose that I have the transformation from the 2-dimensional torus to itself. We know that this transformation in ergodic with respect to the Lebesgue 2-d measure on the torus if and only if are rationally independent which means if and only if . Now according to Birkhoff's ergodic theorem for any function we have that for almost all , where are the iterates of and is the 2-d Lebesgue measure and the integral is over . Let's say that we choose the function which is continuous in and we choose and . Then is ergodic and , so we have that But this cannot be true almost everywhere since the sum on the left is equal with . Now my Question is what am I doing wrong?","T(x,y)=(x+a,y+b), a,b \in \mathbb{R} \mathbb{R}^2/\mathbb{Z}^2 a,b  na+mb \in \mathbb{Z} n=m=0 f\in C(\mathbb{R}^2/\mathbb{Z}^2,\mathbb{R}) \lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}f(T^j(x,y))=\int f dm (x,y) \in \mathbb{R}^2/\mathbb{Z}^2 T^j T m [0,1]\times[0,1] f(x,y)=\cos (2\pi y) [0,1]\times[0,1] a\not \in \mathbb{Q} b=0 T T^j(x,y)=(x+ja,y) \lim_{n\to\infty}\frac{1}{n}\sum_{j=0}^{n-1}\cos(2\pi y)=\int_{0}^{1}\int_{0}^{1} \cos (2\pi y) dydx=0 \cos (2\pi y)","['real-analysis', 'analysis', 'ergodic-theory']"
89,"Does $\partial B(x_0, r) \subseteq \{x \in X : d(x_0,x) = r \}$ hold in an arbitrary metric space?",Does  hold in an arbitrary metric space?,"\partial B(x_0, r) \subseteq \{x \in X : d(x_0,x) = r \}","Let $X$ be a metric space, and let $B(x_0, r)$ denote the open ball of radius $r$ centred at $x_0 \in X$ . Does the statement $\partial B(x_0, r) \subseteq \{x \in X : d(x_0,x) = r \}$ hold true always? A similar question posted in Math StackExchange As answered in the link above, we know that the boundary is not the set $\{x \in X : d(x_0,x) = r \}$ . however, it seems that the statement $\partial B \subseteq \{x \in X : d(x_0,x) = r \}$ is true. Considering the example in the link above, the boundary of any set in the discrete metric is the empty set which is a subset of $\{x \in X : d(x_0,x) = r \}$ . But I am not too certain. Anyone mind showing me some counterexample? Thank you.","Let be a metric space, and let denote the open ball of radius centred at . Does the statement hold true always? A similar question posted in Math StackExchange As answered in the link above, we know that the boundary is not the set . however, it seems that the statement is true. Considering the example in the link above, the boundary of any set in the discrete metric is the empty set which is a subset of . But I am not too certain. Anyone mind showing me some counterexample? Thank you.","X B(x_0, r) r x_0 \in X \partial B(x_0, r) \subseteq \{x \in X : d(x_0,x) = r \} \{x \in X : d(x_0,x) = r \} \partial B \subseteq \{x \in X : d(x_0,x) = r \} \{x \in X : d(x_0,x) = r \}","['analysis', 'metric-spaces']"
90,"Find work of the vector field $K(x,y,z)=(-y,x+z,-y+y^3+z)$ along the curve which is the intersection of the sphere of radius 1 and the plane $x=z$",Find work of the vector field  along the curve which is the intersection of the sphere of radius 1 and the plane,"K(x,y,z)=(-y,x+z,-y+y^3+z) x=z","I'm asked to calculate the work of the vector field $K(x,y,z)=(-y,x+z,-y+y^3+z)$ along the curve which is the intersection of the sphere of radius 1 and the plane $x=z$ : a) directly b) using Stoke's theorem My problem is that I obtain different results in both cases. For a) (correction : see edit 1 below), I use as parametrization $(\cos(\theta),\sin(\theta),\cos(\theta))$ . The gradient is $(-\sin(\theta),\cos(\theta),-\sin(\theta))$ . The dot product of that with our vector field in polar coordinates $(-\sin(\theta),2\cos(\theta),-\sin(\theta)+\sin^3(\theta)+\cos(\theta))$ is simply $2-\sin^4(\theta)-\sin(\theta)\cos(\theta)$ . If we integrate that from $0$ to $2\pi$ , we get $\frac{13\pi}{4}$ noting that $\sin(\theta)\cos(\theta)$ vanishes. For b), the curl of our vector field is $(3y^2-2,0,2)$ . Our parametrization is $(r\cos(\theta),r\sin(\theta),r\cos(\theta))$ . The cross product of the partial derivatives is simply $(-r,0,r)$ . The dot product of that with our curl in polar coordinates is $4r-3r^3\sin^2(\theta)$ . So now, we have $$\int_{0}^{2\pi}\int_{0}^{1}(4r-3r^3\sin^2(\theta))rdrd\theta=\pi\int_{0}^{1}(8r^2-3r^4)rdr=\frac{31\pi}{15}$$ So I don't see where I did something wrong, or where I forgot something. Thanks for your help ! Edit 1: Okay, as was pointed out in the comments and in one answer, I used a wrong parametrization in a). I should use $\frac{\cos(\theta)}{\sqrt{2}}$ instead of $\cos(\theta)$ for the parametrization of $x$ and $z$ . In this case, we get the dot product of $(-\sin(\theta), \sqrt{2}\cos(\theta), -\sin(\theta)+\sin^3(\theta)+\frac{\cos(\theta)}{\sqrt{2}})$ and $(\frac{-\sin(\theta)}{\sqrt{2}}, \cos(\theta),\frac{-\sin(\theta)}{\sqrt{2}}) $ which yields $$\frac{\sin^2(\theta)}{\sqrt{2}}+\sqrt{2}\cos^2(\theta)+\frac{\sin^2(\theta)}{\sqrt{2}}-\frac{\sin^4(\theta)}{\sqrt{2}}-\frac{\sin(\theta)\cos(\theta)}{2}$$ . If we integrate that from $0$ to $2\pi$ , the last term vanishes and using the common trig identity, we simply integrate $\sqrt{2}-\frac{\sin^4(\theta)}{\sqrt{2}}$ . So we get $\frac{13\pi}{4\sqrt{2}}$ . But it's still different fom b) though. Edit 2 : As was pointed out in the comments, I also need to use the new parametrization for part b). But even so, I get $\frac{31\pi}{15\sqrt{2}}$ The cross product of the derivatives is $(\frac{-r}{\sqrt{2}},0,\frac{r}{\sqrt{2}})$ . We need to integrate from $0$ to $2\pi$ $d\theta$ and from $0$ to $1$ $dr$ the following (the curl remains the same): $(\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})r=\frac{4r^2}{\sqrt{2}}-\frac{3r^4sin^2(\theta)}{\sqrt{2}}$ which yields $\frac{8\pi r^3}{3\sqrt{2}}-\frac{3\pi r^5}{5\sqrt{2}}$ evaluated from $0$ to $1$ and so we get $\frac{31\pi}{15\sqrt{2}}$ Edit 3 : problem solved. I multiplied by an extra $r$ in b). I should not have $(\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})r$ but simply $(\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})$ See my answer below.","I'm asked to calculate the work of the vector field along the curve which is the intersection of the sphere of radius 1 and the plane : a) directly b) using Stoke's theorem My problem is that I obtain different results in both cases. For a) (correction : see edit 1 below), I use as parametrization . The gradient is . The dot product of that with our vector field in polar coordinates is simply . If we integrate that from to , we get noting that vanishes. For b), the curl of our vector field is . Our parametrization is . The cross product of the partial derivatives is simply . The dot product of that with our curl in polar coordinates is . So now, we have So I don't see where I did something wrong, or where I forgot something. Thanks for your help ! Edit 1: Okay, as was pointed out in the comments and in one answer, I used a wrong parametrization in a). I should use instead of for the parametrization of and . In this case, we get the dot product of and which yields . If we integrate that from to , the last term vanishes and using the common trig identity, we simply integrate . So we get . But it's still different fom b) though. Edit 2 : As was pointed out in the comments, I also need to use the new parametrization for part b). But even so, I get The cross product of the derivatives is . We need to integrate from to and from to the following (the curl remains the same): which yields evaluated from to and so we get Edit 3 : problem solved. I multiplied by an extra in b). I should not have but simply See my answer below.","K(x,y,z)=(-y,x+z,-y+y^3+z) x=z (\cos(\theta),\sin(\theta),\cos(\theta)) (-\sin(\theta),\cos(\theta),-\sin(\theta)) (-\sin(\theta),2\cos(\theta),-\sin(\theta)+\sin^3(\theta)+\cos(\theta)) 2-\sin^4(\theta)-\sin(\theta)\cos(\theta) 0 2\pi \frac{13\pi}{4} \sin(\theta)\cos(\theta) (3y^2-2,0,2) (r\cos(\theta),r\sin(\theta),r\cos(\theta)) (-r,0,r) 4r-3r^3\sin^2(\theta) \int_{0}^{2\pi}\int_{0}^{1}(4r-3r^3\sin^2(\theta))rdrd\theta=\pi\int_{0}^{1}(8r^2-3r^4)rdr=\frac{31\pi}{15} \frac{\cos(\theta)}{\sqrt{2}} \cos(\theta) x z (-\sin(\theta), \sqrt{2}\cos(\theta), -\sin(\theta)+\sin^3(\theta)+\frac{\cos(\theta)}{\sqrt{2}}) (\frac{-\sin(\theta)}{\sqrt{2}}, \cos(\theta),\frac{-\sin(\theta)}{\sqrt{2}})  \frac{\sin^2(\theta)}{\sqrt{2}}+\sqrt{2}\cos^2(\theta)+\frac{\sin^2(\theta)}{\sqrt{2}}-\frac{\sin^4(\theta)}{\sqrt{2}}-\frac{\sin(\theta)\cos(\theta)}{2} 0 2\pi \sqrt{2}-\frac{\sin^4(\theta)}{\sqrt{2}} \frac{13\pi}{4\sqrt{2}} \frac{31\pi}{15\sqrt{2}} (\frac{-r}{\sqrt{2}},0,\frac{r}{\sqrt{2}}) 0 2\pi d\theta 0 1 dr (\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})r=\frac{4r^2}{\sqrt{2}}-\frac{3r^4sin^2(\theta)}{\sqrt{2}} \frac{8\pi r^3}{3\sqrt{2}}-\frac{3\pi r^5}{5\sqrt{2}} 0 1 \frac{31\pi}{15\sqrt{2}} r (\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})r (\frac{4r}{\sqrt{2}}-\frac{3r^3sin^2(\theta)}{\sqrt{2}})","['real-analysis', 'calculus']"
91,"Explanation for that for a particle, $v^2 = (dl / dt)^2 = (dl)^2 / (dt)^2,$ $l$ is the arch that the particle traces out","Explanation for that for a particle,   is the arch that the particle traces out","v^2 = (dl / dt)^2 = (dl)^2 / (dt)^2, l","In the book of Mechanics by Landau, at page 8, it is claimed that $$v^2 = (dl / dt)^2 = (dl)^2 / (dt)^2,$$ where $v$ is a the velocity of the particle in the cartesian   coordinates, $l$ is the arch that the particle traces out. I know that this is completely a abus d notation ; however, is there any intuitive way of seeing this ?","In the book of Mechanics by Landau, at page 8, it is claimed that where is a the velocity of the particle in the cartesian   coordinates, is the arch that the particle traces out. I know that this is completely a abus d notation ; however, is there any intuitive way of seeing this ?","v^2 = (dl / dt)^2 = (dl)^2 / (dt)^2, v l","['analysis', 'physics', 'mathematical-physics', 'classical-mechanics', 'euler-lagrange-equation']"
92,Cauchy Hadamard formula and starting index of power series,Cauchy Hadamard formula and starting index of power series,,The radius of convergence $r$ can be calculated for every power series $\sum_{k=0}^\infty a_k z^k$ with $a_k\in \mathbb C$ and $z\in \mathbb C$ by using the Cauchy Hadamard formula: $r = \limsup_{k\to\infty} |\frac{1}{a_k}|^{1/k}$ . In every textbook I find the power series starting from $k=0$ . Can the Cauchy Hadamard formula also be used for a power series $\sum_{k=1}^\infty a_k z^k$ directly (ie. without changing the index of the power series before calculating $r$ )?,The radius of convergence can be calculated for every power series with and by using the Cauchy Hadamard formula: . In every textbook I find the power series starting from . Can the Cauchy Hadamard formula also be used for a power series directly (ie. without changing the index of the power series before calculating )?,r \sum_{k=0}^\infty a_k z^k a_k\in \mathbb C z\in \mathbb C r = \limsup_{k\to\infty} |\frac{1}{a_k}|^{1/k} k=0 \sum_{k=1}^\infty a_k z^k r,['analysis']
93,"Let $f:[a,b]\to\Bbb{R}$ be continuous. Does $\max\{|f(x)|:a\leq x\leq b\}$ exist?",Let  be continuous. Does  exist?,"f:[a,b]\to\Bbb{R} \max\{|f(x)|:a\leq x\leq b\}","Let $f:[a,b]\to\Bbb{R}$ be continuous. Does \begin{align}\max\{|f(x)|:a\leq x\leq b\} \end{align} exist? MY WORK I believe it does and I want to prove it. Since $f:[a,b]\to\Bbb{R}$ is continuous, then $f$ is uniformly continuous. Let $\epsilon> 0$ be given, then $\exists\, \delta>$ such that $\forall x,y\in [a,b]$ with $|x-y|<\delta,$ it implies $|f(x)-f(y)|<\epsilon.$ Then, for $a\leq x\leq b,$ \begin{align} f(x)=f(b)+[f(x)-f(b)]\end{align} \begin{align} |f(x)|\leq |f(b)|+|f(x)-f(b)|\end{align} \begin{align} \max\limits_{a\leq x\leq b}|f(x)|\leq |f(b)|+\max\limits_{a\leq x\leq b}|f(x)-f(b)|\end{align} I am stuck at this point. Please, can anyone show me how to continue from here?","Let $f:[a,b]\to\Bbb{R}$ be continuous. Does \begin{align}\max\{|f(x)|:a\leq x\leq b\} \end{align} exist? MY WORK I believe it does and I want to prove it. Since $f:[a,b]\to\Bbb{R}$ is continuous, then $f$ is uniformly continuous. Let $\epsilon> 0$ be given, then $\exists\, \delta>$ such that $\forall x,y\in [a,b]$ with $|x-y|<\delta,$ it implies $|f(x)-f(y)|<\epsilon.$ Then, for $a\leq x\leq b,$ \begin{align} f(x)=f(b)+[f(x)-f(b)]\end{align} \begin{align} |f(x)|\leq |f(b)|+|f(x)-f(b)|\end{align} \begin{align} \max\limits_{a\leq x\leq b}|f(x)|\leq |f(b)|+\max\limits_{a\leq x\leq b}|f(x)-f(b)|\end{align} I am stuck at this point. Please, can anyone show me how to continue from here?",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
94,Is any open subset of $R^n$ a union of countable hypercubes?,Is any open subset of  a union of countable hypercubes?,R^n,"I'm trying to extend the proof seen here that any open subset of $\mathbb{R}$ can be written as a countable union of disjoint open intervals. Apparently it seems like an analogous proof for $\mathbb{R}^n$. However I can't think of any equivalence relation that would allow me to proceed with my sketch. The sketch is like: Define an equivalence relation $\sim$ on $\mathbb{R}^n$ Show that the equivalence class $\sim(x)$ is non-empty and open Prove that $\sim(x)\bigcap \sim(y) \neq \emptyset \Rightarrow \sim(x)=\sim(y)$ Demonstrate that $\sim(x)$ is an hypercube $\prod_{i=1}^N(a_i,b_i)$ Show that the set of equivalence classes D is countable Deduce that $\bigcup D$ is equal to our open subset","I'm trying to extend the proof seen here that any open subset of $\mathbb{R}$ can be written as a countable union of disjoint open intervals. Apparently it seems like an analogous proof for $\mathbb{R}^n$. However I can't think of any equivalence relation that would allow me to proceed with my sketch. The sketch is like: Define an equivalence relation $\sim$ on $\mathbb{R}^n$ Show that the equivalence class $\sim(x)$ is non-empty and open Prove that $\sim(x)\bigcap \sim(y) \neq \emptyset \Rightarrow \sim(x)=\sim(y)$ Demonstrate that $\sim(x)$ is an hypercube $\prod_{i=1}^N(a_i,b_i)$ Show that the set of equivalence classes D is countable Deduce that $\bigcup D$ is equal to our open subset",,"['real-analysis', 'general-topology', 'analysis']"
95,Counter example of Lusin's theorem,Counter example of Lusin's theorem,,"The characteristic function of rationals in [0, 1] satisfies the hypothesis of Lusin's theorem. But it is no-where continuous on [0, 1]. But Lusin's theorem implies that it should be continuous on a positive measure subset of [0, 1]. What am I missing here?","The characteristic function of rationals in [0, 1] satisfies the hypothesis of Lusin's theorem. But it is no-where continuous on [0, 1]. But Lusin's theorem implies that it should be continuous on a positive measure subset of [0, 1]. What am I missing here?",,['analysis']
96,"$A \subseteq \mathbb{R}^{2}$ is not homeomorphic to an open subset of $\mathbb{R^{n}}$, $n\geq 3$","is not homeomorphic to an open subset of ,",A \subseteq \mathbb{R}^{2} \mathbb{R^{n}} n\geq 3,"An open subset $A \subseteq \mathbb{R}^{2}$ is not homeomorphic to an open subset of $\mathbb{R^{n}}$ for $n\geq 3$. This question is from my General Topology class. We have seen up to part of algebraic topology. My attempt: Suppose a homeomorphism $f \colon X \to A$, where $X\subseteq \mathbb{R}^{n}$ is an open set. Then we can restrict this homeomorphism to an open ball contained in $X$ and, using some more homeomorphisms, suppose $X = \mathbb{R}^{n}$. Therefore we can also suppose $A \subseteq \mathbb{R}^{2}$ is an open simply connected set. Now, because of $\mathbb{R}^{n}-\{0\}$ is simply connected whenever $n\geq 3$, $f(\mathbb{R}^{n}-\{0\}) = A - \{f(0)\}$ is simply connected. We want to deny that $A - \{f(0)\}$ is simply connected. In order to do that pick a $\delta > 0$ such that $B_{f(0)}[\delta] \subset A$. Then define the following function: $$ \alpha \colon \mathbb{S}^{1} \to A - \{f(0)\}; ~ \alpha(z) = z\delta + f(0)$$ A set  $X \subseteq \mathbb{R}^{n}$ is simply connected when every continuous function $p \colon \mathbb{S}^{1}\to X$ has a continuous extension $\overline{p}\colon \mathbb{B}^{2} \to X$. I was not able to show that this function has not continuous extension to $\mathbb{B}^{2}$. Is it true? Any help would be appreciated but I would prefer help in my attempt. Thank you.","An open subset $A \subseteq \mathbb{R}^{2}$ is not homeomorphic to an open subset of $\mathbb{R^{n}}$ for $n\geq 3$. This question is from my General Topology class. We have seen up to part of algebraic topology. My attempt: Suppose a homeomorphism $f \colon X \to A$, where $X\subseteq \mathbb{R}^{n}$ is an open set. Then we can restrict this homeomorphism to an open ball contained in $X$ and, using some more homeomorphisms, suppose $X = \mathbb{R}^{n}$. Therefore we can also suppose $A \subseteq \mathbb{R}^{2}$ is an open simply connected set. Now, because of $\mathbb{R}^{n}-\{0\}$ is simply connected whenever $n\geq 3$, $f(\mathbb{R}^{n}-\{0\}) = A - \{f(0)\}$ is simply connected. We want to deny that $A - \{f(0)\}$ is simply connected. In order to do that pick a $\delta > 0$ such that $B_{f(0)}[\delta] \subset A$. Then define the following function: $$ \alpha \colon \mathbb{S}^{1} \to A - \{f(0)\}; ~ \alpha(z) = z\delta + f(0)$$ A set  $X \subseteq \mathbb{R}^{n}$ is simply connected when every continuous function $p \colon \mathbb{S}^{1}\to X$ has a continuous extension $\overline{p}\colon \mathbb{B}^{2} \to X$. I was not able to show that this function has not continuous extension to $\mathbb{B}^{2}$. Is it true? Any help would be appreciated but I would prefer help in my attempt. Thank you.",,"['general-topology', 'analysis', 'algebraic-topology']"
97,Non-wandering set (definitions),Non-wandering set (definitions),,"For a map $f\colon X\to X$, a point $p\in X$ is called non-wandering , provided for every neighborhood $U$ of $p$ there exists an integer $n>0$ such that $f^n(U)\cap U\neq\emptyset$. Sometimes, I also read the following definition: For either maps or flows a point $p\in X$ is said to be a nonwandering point, if given any neighborhood $U$ of $p$ there exists a sequence of times $t_n\to\infty$ such that $f^{t_n}(p)\in U$ for all $n$. Hence, non-wandering points are points whose neighborhoods the map visits infinitely often. Are these definitions equivalent? Does the first definition also imply that each neighborhood of $p$ is visited infinitely often?","For a map $f\colon X\to X$, a point $p\in X$ is called non-wandering , provided for every neighborhood $U$ of $p$ there exists an integer $n>0$ such that $f^n(U)\cap U\neq\emptyset$. Sometimes, I also read the following definition: For either maps or flows a point $p\in X$ is said to be a nonwandering point, if given any neighborhood $U$ of $p$ there exists a sequence of times $t_n\to\infty$ such that $f^{t_n}(p)\in U$ for all $n$. Hence, non-wandering points are points whose neighborhoods the map visits infinitely often. Are these definitions equivalent? Does the first definition also imply that each neighborhood of $p$ is visited infinitely often?",,"['analysis', 'dynamical-systems', 'topological-dynamics']"
98,Monotone approximating sequence for measurable function,Monotone approximating sequence for measurable function,,"Let $f\in L^2(K)$ (with values in $\mathbb{R})$, where $K\subseteq\mathbb{R}^n$ is compact. Then one knows that there exists a sequence $f_n$ of continuous functions on $K$ that converges pointwise almost everwhere to $f$. For example one can get this by judiciously taking a subsequence of a sequence of continuous functions converging to $f$ in $L^2(K)$. I'm wondering, is it possible to get a sequence of continuous functions $f'_n$ that converges to $f$ pointwise almost everywhere, with the property that $f'_n(x)\geq f'_m(x)$ whenver $n\geq m$? Thanks!","Let $f\in L^2(K)$ (with values in $\mathbb{R})$, where $K\subseteq\mathbb{R}^n$ is compact. Then one knows that there exists a sequence $f_n$ of continuous functions on $K$ that converges pointwise almost everwhere to $f$. For example one can get this by judiciously taking a subsequence of a sequence of continuous functions converging to $f$ in $L^2(K)$. I'm wondering, is it possible to get a sequence of continuous functions $f'_n$ that converges to $f$ pointwise almost everywhere, with the property that $f'_n(x)\geq f'_m(x)$ whenver $n\geq m$? Thanks!",,"['real-analysis', 'analysis', 'measure-theory']"
99,How to prove that $\int_{0}^{\infty}\frac{t}{x^2+t^2}\cos(ax)dx=\frac{\pi}{2}e^{-at}$.,How to prove that .,\int_{0}^{\infty}\frac{t}{x^2+t^2}\cos(ax)dx=\frac{\pi}{2}e^{-at},"I want to prove that for $t,a>0$: $$f_a(t)=\int_{0}^{\infty}\frac{t}{x^2+t^2}\cos(ax)dx=\frac{\pi}{2}e^{-at}$$ It is easy to prove that  $$f_a''(t)=a^2f_a(t),$$ then $$f_a(t)=c_1e^{at}+c_2e^{-at}$$ Cleary $\lim\limits_{t\to 0}f_a(t)=0$ and therefore $c_1=0$. But I could not justify that $c_2=\frac{\pi}{2}$. Any idea? Thank you very much.","I want to prove that for $t,a>0$: $$f_a(t)=\int_{0}^{\infty}\frac{t}{x^2+t^2}\cos(ax)dx=\frac{\pi}{2}e^{-at}$$ It is easy to prove that  $$f_a''(t)=a^2f_a(t),$$ then $$f_a(t)=c_1e^{at}+c_2e^{-at}$$ Cleary $\lim\limits_{t\to 0}f_a(t)=0$ and therefore $c_1=0$. But I could not justify that $c_2=\frac{\pi}{2}$. Any idea? Thank you very much.",,"['integration', 'analysis', 'improper-integrals']"
