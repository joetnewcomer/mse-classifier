,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lost on rational and Jordan forms,Lost on rational and Jordan forms,,"I'm having a lot of trouble trying to understand rational canonical form, primary rational canonical form, and Jordan form. I've looked at the posts about this, but I haven't been able to understand those concepts. I've been given the following matrix which is associated to an endomorphism $\phi$ of $V=\mathbb{R}^{4}$ $$M=\begin{bmatrix}  1 & 0 & 0 & -1\\  -1 & 0 & -1 & 2\\  2 & 2 & 2 & -2\\  0 & 0 & 0 &  1\\ \end{bmatrix}$$ I have to compute the following: Invariant factors of $\phi$ and a base of $V$ for which the matrix associated to $\phi$ is the rational canonical form of $M$. Determine a basis of $V$ for which the matrix associated to $\phi$ is the primary rational canonical form of $M$. The real Jordan canonical form associated to $M$. Now it comes my try and my doubts: The characteristic polynomial is $(x-1)^2(x^2-2x+2)$ which has just one eigenvalue $(1)$ with algebraic multiplicity $2$. From here, it is easy to see that the minimal polynomial is $(x-1)^2(x^2-2x+2)$ (by multiplying matrixes). However, I've been told that I can also know the minimal polynomial by computing  $dim(Nuc(I-M))$ i.e. $(4-rank(I-M))$ and realising it is $1$ i.e. $(4-3)$, but I do not understand that. Therefore, the only invariant factor is the minimal polynomial, and then the rational canonical form is: $$C=\begin{bmatrix}  0 & 0 & 0 & -2\\  1 & 0 & 0 & 6\\  0 & 1 & 0 & -7\\  0 & 0 & 1 &  4\\ \end{bmatrix}$$ since $(x-1)^2(x^2-2x+2)= (x^4-4x^3+7x^2-6x+2)$. Now I'm afraid I don't know how to compute the basis I've been asked for. Also, in case that there were two invariant factors, for example, our minimal polynomial and $(x-3)$, I'd know how to write the rational canonical form, but I'm not sure whether the order of the companion matrixes matters. My trouble is quite the same here. I know that the primary rational canonical form is : $$R=\begin{bmatrix}  0 & -1 & 0 & 0\\  1 & 2 & 0 & 0\\  0 & 0 & 0 & -2\\  0 & 0 & 1 &  2\\ \end{bmatrix}$$ since the elementary divisors are $(x-1)^2$ and $(x^2-2x+2)$ but I'm not sure of the order. And once again, I don't know how to get the basis. Finally, $dim(Nuc(I-M))= 1$ as I just said, so there is 1 jordan block of size at least 1, and therefore, $dim(Nuc(I-M)^2)$ must be $2$ and we have one jordan block for the eigenvalue $1$ and it has size 2. The problem comes with the block(s) associated to $(x^2-2x+2)$. The jordan form should be    $$J=\begin{bmatrix}  1 & 0 & 0 & 0\\  1 & 1 & 0 & 0\\  0 & 0 & 1 & -1\\  0 & 0 & 1 &  1\\ \end{bmatrix}$$ but I don't know why. Sorry for the long post and thanks in advance.","I'm having a lot of trouble trying to understand rational canonical form, primary rational canonical form, and Jordan form. I've looked at the posts about this, but I haven't been able to understand those concepts. I've been given the following matrix which is associated to an endomorphism $\phi$ of $V=\mathbb{R}^{4}$ $$M=\begin{bmatrix}  1 & 0 & 0 & -1\\  -1 & 0 & -1 & 2\\  2 & 2 & 2 & -2\\  0 & 0 & 0 &  1\\ \end{bmatrix}$$ I have to compute the following: Invariant factors of $\phi$ and a base of $V$ for which the matrix associated to $\phi$ is the rational canonical form of $M$. Determine a basis of $V$ for which the matrix associated to $\phi$ is the primary rational canonical form of $M$. The real Jordan canonical form associated to $M$. Now it comes my try and my doubts: The characteristic polynomial is $(x-1)^2(x^2-2x+2)$ which has just one eigenvalue $(1)$ with algebraic multiplicity $2$. From here, it is easy to see that the minimal polynomial is $(x-1)^2(x^2-2x+2)$ (by multiplying matrixes). However, I've been told that I can also know the minimal polynomial by computing  $dim(Nuc(I-M))$ i.e. $(4-rank(I-M))$ and realising it is $1$ i.e. $(4-3)$, but I do not understand that. Therefore, the only invariant factor is the minimal polynomial, and then the rational canonical form is: $$C=\begin{bmatrix}  0 & 0 & 0 & -2\\  1 & 0 & 0 & 6\\  0 & 1 & 0 & -7\\  0 & 0 & 1 &  4\\ \end{bmatrix}$$ since $(x-1)^2(x^2-2x+2)= (x^4-4x^3+7x^2-6x+2)$. Now I'm afraid I don't know how to compute the basis I've been asked for. Also, in case that there were two invariant factors, for example, our minimal polynomial and $(x-3)$, I'd know how to write the rational canonical form, but I'm not sure whether the order of the companion matrixes matters. My trouble is quite the same here. I know that the primary rational canonical form is : $$R=\begin{bmatrix}  0 & -1 & 0 & 0\\  1 & 2 & 0 & 0\\  0 & 0 & 0 & -2\\  0 & 0 & 1 &  2\\ \end{bmatrix}$$ since the elementary divisors are $(x-1)^2$ and $(x^2-2x+2)$ but I'm not sure of the order. And once again, I don't know how to get the basis. Finally, $dim(Nuc(I-M))= 1$ as I just said, so there is 1 jordan block of size at least 1, and therefore, $dim(Nuc(I-M)^2)$ must be $2$ and we have one jordan block for the eigenvalue $1$ and it has size 2. The problem comes with the block(s) associated to $(x^2-2x+2)$. The jordan form should be    $$J=\begin{bmatrix}  1 & 0 & 0 & 0\\  1 & 1 & 0 & 0\\  0 & 0 & 1 & -1\\  0 & 0 & 1 &  1\\ \end{bmatrix}$$ but I don't know why. Sorry for the long post and thanks in advance.",,"['linear-algebra', 'matrices', 'polynomials', 'jordan-normal-form']"
1,A determinantal equality,A determinantal equality,,"Mark Kac wrote a paper about asymptotics of determinants whose main diagonal is taken from a function $f$, with $-1$ on the super and sub-diagonals.  Specifically, $$ D_n = \begin{vmatrix}  f(1/n) & -1 & 0 & \cdots & &0 \\ -1 & f(2/n) & -1 & \cdots \\ 0& -1 & f(3/n) \\ &&& \ddots \\ &&&& f((n-1)/n) & -1 \\ &&&& -1 & f(1) \end{vmatrix} $$ He then writes, ""We begin with the elementary formula $$\frac{1}{\sqrt{D_n}} = \frac{1}{(\sqrt{\pi})^n} \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \exp\left[-\sum_{k=1}^n -\frac{1}{2}f\left(\frac{k}{n}\right) x_k^2 + 2\sum_{k=1}^{n-1} x_kx_{k+1}\right]dx_1dx_2\cdots dx_n $$ My question is, Where does this elementary formula come from?? The paper is:  Asymptotic behavior of a class of determinants, L'Enseignement Mathematique, pp 177-183, 15(1969)","Mark Kac wrote a paper about asymptotics of determinants whose main diagonal is taken from a function $f$, with $-1$ on the super and sub-diagonals.  Specifically, $$ D_n = \begin{vmatrix}  f(1/n) & -1 & 0 & \cdots & &0 \\ -1 & f(2/n) & -1 & \cdots \\ 0& -1 & f(3/n) \\ &&& \ddots \\ &&&& f((n-1)/n) & -1 \\ &&&& -1 & f(1) \end{vmatrix} $$ He then writes, ""We begin with the elementary formula $$\frac{1}{\sqrt{D_n}} = \frac{1}{(\sqrt{\pi})^n} \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \exp\left[-\sum_{k=1}^n -\frac{1}{2}f\left(\frac{k}{n}\right) x_k^2 + 2\sum_{k=1}^{n-1} x_kx_{k+1}\right]dx_1dx_2\cdots dx_n $$ My question is, Where does this elementary formula come from?? The paper is:  Asymptotic behavior of a class of determinants, L'Enseignement Mathematique, pp 177-183, 15(1969)",,"['linear-algebra', 'determinant']"
2,Equidistant sequence in a normed space,Equidistant sequence in a normed space,,"Let $(X, \|\cdot\|)$ be a normed linear space of dimension $n < \infty$. Is it always true that we can find a sequence of $m = n + 1$ points $x_i$ such that $\|x_i - x_j\| = c > 0$ for all $i\neq j$? Can we choose $m$ to be bigger than $n+1$ without violating the first property? In case $X$ is not finite-dimensional, does there always exist a countable sequence of $x_i$ with the desired property? Inspired by this question .","Let $(X, \|\cdot\|)$ be a normed linear space of dimension $n < \infty$. Is it always true that we can find a sequence of $m = n + 1$ points $x_i$ such that $\|x_i - x_j\| = c > 0$ for all $i\neq j$? Can we choose $m$ to be bigger than $n+1$ without violating the first property? In case $X$ is not finite-dimensional, does there always exist a countable sequence of $x_i$ with the desired property? Inspired by this question .",,"['linear-algebra', 'banach-spaces']"
3,Forecast equivalence between two steady-state Kalman filters,Forecast equivalence between two steady-state Kalman filters,,"I have two related steady-state Kalman filter problems that I want to prove satisfy a condition associated with their respective Kalman gains. I am not really looking for a complete proof since this is likely require quite a bit of work. However, any ideas about where to start or useful results that I could use would be much appreciated. Problem 1: $${\bf P} ={\bf F}({\bf I}_{n}-{\bf K} {\bf H}){\bf P}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf K}\equiv {\bf P} {\bf H^\top}\left({\bf H}{\bf P} {\bf H}^\top+ \frac{1}{\gamma}{\bf R} \right)^{-1}$$ where ${\bf P}$, ${\bf F}$, and ${\bf Q}$ are $n\times n$, ${\bf H}$ is $r\times n$, ${\bf K}$ is $n\times r$, ${\bf R}$ is  $r\times r$ and symmetric, ${\bf Q}$ is diagonal. Problem 2: $${\bf W} ={\bf A}({\bf I}_{2n}-{\bf L} {\bf B}){\bf W}{\bf A}^\top+{\bf C}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf L}\equiv {\bf W} {\bf B}^\top\left({\bf B}{\bf W} {\bf B}^\top+ {\bf R} \right)^{-1}$$ where ${\bf W}$, ${\bf A}$ and ${\bf C}$ are $2n\times 2n$, and ${\bf B}$ is $r\times 2n$, and ${\bf L}$ is $2n\times r$. Relationship between the two problems: $${\bf A} \equiv \left[ \begin{matrix} {\bf F} & {\bf0} \\ {\bf K} {\bf H} {\bf F} & ({\bf I}_{n}-{\bf K} {\bf H}){\bf F} \end{matrix} \right], \;\;\;\; {\bf B}^\top \equiv \left[ \begin{matrix} {\bf H}^\top & {\bf 0} \end{matrix} \right], \;\;\;\; {\bf C} \equiv \left[ \begin{matrix} {\bf Q} & {\bf Q}{\bf H}^\top{\bf K}^\top \\ {\bf K}{\bf H}{\bf Q} & {\bf K}{\bf H} {\bf Q}{\bf H}^\top{\bf K}^\top \end{matrix} \right]$$ also let $${\bf L} \equiv \left[ \begin{matrix}  {\bf L}_{1} \\ {\bf L}_{2} \end{matrix} \right], \;\;\;\; {\bf W} \equiv \left[ \begin{matrix} {\bf W}_{11} & {\bf W}_{21}' \\ {\bf W}_{21} & {\bf W}_{22} \end{matrix} \right]$$ Want to prove: $${\bf K} = \gamma {\bf L}_{1} + (1-\gamma) {\bf L}_{2} $$ All matrices are real. Doing a bit of algebra, it is easy to show that \begin{align} {\bf L}_{1} &={\bf W}_{11}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1} \\[1.5ex] {\bf L}_{2} &={\bf W}_{21}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1} \end{align} and that \begin{align} {\bf W}_{11} &= {\bf F}({\bf I}_n-{\bf L}_{1}{\bf H}){\bf W}_{11}{\bf F}^\top+{\bf Q} \\ {\bf W}_{21} &={\bf K}{\bf H}{\bf W}_{11}+({\bf I}_n-{\bf K}{\bf H}){\bf F}{\bf W}_{21}({\bf I}_n-{\bf H}^\top {\bf L}_{1}^\top){\bf F}^\top \end{align}","I have two related steady-state Kalman filter problems that I want to prove satisfy a condition associated with their respective Kalman gains. I am not really looking for a complete proof since this is likely require quite a bit of work. However, any ideas about where to start or useful results that I could use would be much appreciated. Problem 1: $${\bf P} ={\bf F}({\bf I}_{n}-{\bf K} {\bf H}){\bf P}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf K}\equiv {\bf P} {\bf H^\top}\left({\bf H}{\bf P} {\bf H}^\top+ \frac{1}{\gamma}{\bf R} \right)^{-1}$$ where ${\bf P}$, ${\bf F}$, and ${\bf Q}$ are $n\times n$, ${\bf H}$ is $r\times n$, ${\bf K}$ is $n\times r$, ${\bf R}$ is  $r\times r$ and symmetric, ${\bf Q}$ is diagonal. Problem 2: $${\bf W} ={\bf A}({\bf I}_{2n}-{\bf L} {\bf B}){\bf W}{\bf A}^\top+{\bf C}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf L}\equiv {\bf W} {\bf B}^\top\left({\bf B}{\bf W} {\bf B}^\top+ {\bf R} \right)^{-1}$$ where ${\bf W}$, ${\bf A}$ and ${\bf C}$ are $2n\times 2n$, and ${\bf B}$ is $r\times 2n$, and ${\bf L}$ is $2n\times r$. Relationship between the two problems: $${\bf A} \equiv \left[ \begin{matrix} {\bf F} & {\bf0} \\ {\bf K} {\bf H} {\bf F} & ({\bf I}_{n}-{\bf K} {\bf H}){\bf F} \end{matrix} \right], \;\;\;\; {\bf B}^\top \equiv \left[ \begin{matrix} {\bf H}^\top & {\bf 0} \end{matrix} \right], \;\;\;\; {\bf C} \equiv \left[ \begin{matrix} {\bf Q} & {\bf Q}{\bf H}^\top{\bf K}^\top \\ {\bf K}{\bf H}{\bf Q} & {\bf K}{\bf H} {\bf Q}{\bf H}^\top{\bf K}^\top \end{matrix} \right]$$ also let $${\bf L} \equiv \left[ \begin{matrix}  {\bf L}_{1} \\ {\bf L}_{2} \end{matrix} \right], \;\;\;\; {\bf W} \equiv \left[ \begin{matrix} {\bf W}_{11} & {\bf W}_{21}' \\ {\bf W}_{21} & {\bf W}_{22} \end{matrix} \right]$$ Want to prove: $${\bf K} = \gamma {\bf L}_{1} + (1-\gamma) {\bf L}_{2} $$ All matrices are real. Doing a bit of algebra, it is easy to show that \begin{align} {\bf L}_{1} &={\bf W}_{11}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1} \\[1.5ex] {\bf L}_{2} &={\bf W}_{21}{\bf H}^\top({\bf H}{\bf W}_{11}{\bf H}^\top+{\bf R})^{-1} \end{align} and that \begin{align} {\bf W}_{11} &= {\bf F}({\bf I}_n-{\bf L}_{1}{\bf H}){\bf W}_{11}{\bf F}^\top+{\bf Q} \\ {\bf W}_{21} &={\bf K}{\bf H}{\bf W}_{11}+({\bf I}_n-{\bf K}{\bf H}){\bf F}{\bf W}_{21}({\bf I}_n-{\bf H}^\top {\bf L}_{1}^\top){\bf F}^\top \end{align}",,"['linear-algebra', 'bayesian-network', 'kalman-filter']"
4,Definition of the rank of infinite matrix,Definition of the rank of infinite matrix,,"How is the rank of an infinite matrix defined?  Is it the same as in the finite case, i.e. the number of elements in a basis for some matrix?  How are the dimensionalities of the column and null spaces defined for infinite matrices?  I have searched quite a bit online and have found no discussion of these issues.","How is the rank of an infinite matrix defined?  Is it the same as in the finite case, i.e. the number of elements in a basis for some matrix?  How are the dimensionalities of the column and null spaces defined for infinite matrices?  I have searched quite a bit online and have found no discussion of these issues.",,"['linear-algebra', 'matrices', 'infinity']"
5,General formula for the exponential of a block matrix,General formula for the exponential of a block matrix,,"Let $$ M = \begin{pmatrix}A& B\\C& D\end{pmatrix}. $$ Is there a general formula for $e^M$ in terms of the sub-matrices $A$, $B$, $C$ and $D$? If not, can anything useful be said about the properties of $e^M$, given properties of its component blocks? I've tried a few different approaches but haven't got anywhere, mostly because the formula $e^{A + B}=e^Ae^B$ only works if $A$ and $B$ commute. If it helps, I'm most interested in the case where $M$ is a symmetryic real matrix, and an 'infinitesimal stochastic' matrix in particular. (An infinitesimal stochastic matrix is a symmetric real matrix whose column sums are all zero, and which has no negative off-diagonal elements.) If there are other special sub-classes of matrices for which something useful can be said here, I would be interested to know that.","Let $$ M = \begin{pmatrix}A& B\\C& D\end{pmatrix}. $$ Is there a general formula for $e^M$ in terms of the sub-matrices $A$, $B$, $C$ and $D$? If not, can anything useful be said about the properties of $e^M$, given properties of its component blocks? I've tried a few different approaches but haven't got anywhere, mostly because the formula $e^{A + B}=e^Ae^B$ only works if $A$ and $B$ commute. If it helps, I'm most interested in the case where $M$ is a symmetryic real matrix, and an 'infinitesimal stochastic' matrix in particular. (An infinitesimal stochastic matrix is a symmetric real matrix whose column sums are all zero, and which has no negative off-diagonal elements.) If there are other special sub-classes of matrices for which something useful can be said here, I would be interested to know that.",,"['linear-algebra', 'matrices', 'block-matrices', 'matrix-exponential']"
6,Is there a decomposition $U U^T$?,Is there a decomposition ?,U U^T,We know that there exist the Choleski decomposition $ M = L L^T $ where $M$ is a positive definite matrix and $L$ a lower triangular one. Does it exist a similar decomposition in $ M = U U^T$ with $U$ upper triangular?,We know that there exist the Choleski decomposition $ M = L L^T $ where $M$ is a positive definite matrix and $L$ a lower triangular one. Does it exist a similar decomposition in $ M = U U^T$ with $U$ upper triangular?,,"['linear-algebra', 'matrices', 'matrix-decomposition']"
7,Square root of differentiation,Square root of differentiation,,"Let $T=d/dx$ be the  differentiation operator on vector  space $V=C^{\infty}(\mathbb{R})$, the  space of real (complex) valued smooth  maps on real line. To what extent, all subvector  space  $W\subseteq V$ with the following properties are classified? W  is  $T$-invariant and the restriction of $T$ to  $W$ has an square  root: That is, there  is  an operator $S$  on $W$ with $S^{2}=T$ In particular, does $V$satisfy this property? If  the answer is no, what is  an example of  a non zero $W$ which is  maximal with the above property?","Let $T=d/dx$ be the  differentiation operator on vector  space $V=C^{\infty}(\mathbb{R})$, the  space of real (complex) valued smooth  maps on real line. To what extent, all subvector  space  $W\subseteq V$ with the following properties are classified? W  is  $T$-invariant and the restriction of $T$ to  $W$ has an square  root: That is, there  is  an operator $S$  on $W$ with $S^{2}=T$ In particular, does $V$satisfy this property? If  the answer is no, what is  an example of  a non zero $W$ which is  maximal with the above property?",,"['linear-algebra', 'vector-spaces', 'operator-theory']"
8,Measure change/similarity between two affine transformations,Measure change/similarity between two affine transformations,,"I have two affine transformations $A_1$ and $A_2$ consisting only of a rotation matrix $R_i$ and a translation vector $\overrightarrow{t_i}$ (all in 3D space): $$A_i = \left[ \begin{array}{ccc|c} \, & R_i & & \vec{t_i} \ \\ 0 & \ldots & 0 & 1 \end{array} \right]$$ Now I want to measure how much ""change"" occurs between $A_1$ and $A_2$, or put differently how similar $A_1$ and $A_2$ are. I thought of calculating the relative transformation $A'$ between $A_1$ and $A_2$ and apply the euclidean norm. The norm's difference from $1$ could be a measure of ""similarity"" $s$ : $$A' = {A_1}^{-1} \cdot A_2$$ $$s = ||A'||_2-1$$ Is this a valid way? Is there a better (=numerically less sensitive and computationally less expensive) way which exploits the fact that $A_i$ only consists of rotation and translation?","I have two affine transformations $A_1$ and $A_2$ consisting only of a rotation matrix $R_i$ and a translation vector $\overrightarrow{t_i}$ (all in 3D space): $$A_i = \left[ \begin{array}{ccc|c} \, & R_i & & \vec{t_i} \ \\ 0 & \ldots & 0 & 1 \end{array} \right]$$ Now I want to measure how much ""change"" occurs between $A_1$ and $A_2$, or put differently how similar $A_1$ and $A_2$ are. I thought of calculating the relative transformation $A'$ between $A_1$ and $A_2$ and apply the euclidean norm. The norm's difference from $1$ could be a measure of ""similarity"" $s$ : $$A' = {A_1}^{-1} \cdot A_2$$ $$s = ||A'||_2-1$$ Is this a valid way? Is there a better (=numerically less sensitive and computationally less expensive) way which exploits the fact that $A_i$ only consists of rotation and translation?",,['linear-algebra']
9,"Notation/terminology for ""independent"" subspaces/subalgebras","Notation/terminology for ""independent"" subspaces/subalgebras",,"Let $V$ denote a vector space (or any other kind of algebraic structure). Question. Letting $I$ denote a fixed set and $X$ denote an $I$-indexed family of subspaces (subalgebras) of $V$, is there better   notation than $\bigoplus_{i:I} X_i = \sum_{i:I}X_i$ and/or   $\bigsqcup_{i:I} X_i = \bigvee_{i:I}X_i$ to mean that the projection   $\bigsqcup_{i:I} X_i \twoheadrightarrow \bigvee_{i:I}X_i$ is injective? Also: is there accepted terminology for this condition? For example, if $A$ denotes an $n \times n$ real matrix and $X : \mathrm{Eigenvalue}(A) \rightarrow \mathrm{Subspace}(\mathbb{R}^n)$ is the corresponding eigenspace function, then I'd like to be able to say: ""$X$ satisfies [whatever].""","Let $V$ denote a vector space (or any other kind of algebraic structure). Question. Letting $I$ denote a fixed set and $X$ denote an $I$-indexed family of subspaces (subalgebras) of $V$, is there better   notation than $\bigoplus_{i:I} X_i = \sum_{i:I}X_i$ and/or   $\bigsqcup_{i:I} X_i = \bigvee_{i:I}X_i$ to mean that the projection   $\bigsqcup_{i:I} X_i \twoheadrightarrow \bigvee_{i:I}X_i$ is injective? Also: is there accepted terminology for this condition? For example, if $A$ denotes an $n \times n$ real matrix and $X : \mathrm{Eigenvalue}(A) \rightarrow \mathrm{Subspace}(\mathbb{R}^n)$ is the corresponding eigenspace function, then I'd like to be able to say: ""$X$ satisfies [whatever].""",,"['linear-algebra', 'abstract-algebra', 'notation', 'terminology']"
10,Over what rings is the Hefferonian determinant unique?,Over what rings is the Hefferonian determinant unique?,,"Fix an $n\in\mathbb{N}$ and a field $\mathbb{K}$. A lot of texts in linear algebra like to define the determinant function on $\operatorname{M}_n\left(\mathbb{K}\right)$ as the unique function $\operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ which is alternating and multilinear on the rows of the matrix and sends the identity matrix $I_n$ to $1$. This definition generalizes verbatim to the case when $\mathbb{K}$ is a commutative ring. Jim Hefferon's Linear Algebra (version 22 Dec 2014) (Definition 2.1 in Chapter Four) uses a slightly modified version of this definition. In my notations, a Hefferonian determinant function means a function $f : \operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ having the following four properties: If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by adding a multiple of a row of $A$ to another row of $A$, then $f\left(B\right) = f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by switching two rows, then $f\left(B\right) = - f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by multiplying a row by a scalar $\lambda$, then $f\left(B\right) = \lambda f\left(A\right)$. We have $f\left(I_n\right) = 1$. Hefferon then shows that such a function $f$ is unique when $\mathbb{K}$ is a field. It is easy to show that, more generally, there is a unique Hefferonian determinant function when $\mathbb{K}$ is an integral domain (namely, the usual determinant $\det$). Out of curiosity, I am wondering how far this uniqueness statement can be generalized. It doesn't feel right to expect it to hold over an arbitrary commutative ring $\mathbb{K}$, as things like Gaussian elimination just will not work and there will not be any quotient field to salvage them. But I am not sure how to find a counterexample. Ideas? Notice that Hefferon is not the only author who defines a determinant in such a strange way. The definition of a determinant in Theorem 1.50 of Peter J. Olver's and Chehrzad Shakiban's Applied Linear Algebra (2006) is similar to Hefferon's. Instead of property 4, it requires $f$ to send any upper-triangular matrix to the product of its diagonal entries. This is stronger than Hefferon's property 4, and I am wondering if it is actually stronger or just equivalent? Apparently the popularity of these unnatural definitions is due to the opinion that genuine multilinearity is too difficult for students to grasp; I am not sure if this justifies them, but I believe that the question it posts is interesting!","Fix an $n\in\mathbb{N}$ and a field $\mathbb{K}$. A lot of texts in linear algebra like to define the determinant function on $\operatorname{M}_n\left(\mathbb{K}\right)$ as the unique function $\operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ which is alternating and multilinear on the rows of the matrix and sends the identity matrix $I_n$ to $1$. This definition generalizes verbatim to the case when $\mathbb{K}$ is a commutative ring. Jim Hefferon's Linear Algebra (version 22 Dec 2014) (Definition 2.1 in Chapter Four) uses a slightly modified version of this definition. In my notations, a Hefferonian determinant function means a function $f : \operatorname{M}_n\left(\mathbb{K}\right) \to \mathbb{K}$ having the following four properties: If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by adding a multiple of a row of $A$ to another row of $A$, then $f\left(B\right) = f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by switching two rows, then $f\left(B\right) = - f\left(A\right)$. If $A \in \operatorname{M}_n\left(\mathbb{K}\right)$ and $B \in \operatorname{M}_n\left(\mathbb{K}\right)$ are such that $B$ is obtained from $A$ by multiplying a row by a scalar $\lambda$, then $f\left(B\right) = \lambda f\left(A\right)$. We have $f\left(I_n\right) = 1$. Hefferon then shows that such a function $f$ is unique when $\mathbb{K}$ is a field. It is easy to show that, more generally, there is a unique Hefferonian determinant function when $\mathbb{K}$ is an integral domain (namely, the usual determinant $\det$). Out of curiosity, I am wondering how far this uniqueness statement can be generalized. It doesn't feel right to expect it to hold over an arbitrary commutative ring $\mathbb{K}$, as things like Gaussian elimination just will not work and there will not be any quotient field to salvage them. But I am not sure how to find a counterexample. Ideas? Notice that Hefferon is not the only author who defines a determinant in such a strange way. The definition of a determinant in Theorem 1.50 of Peter J. Olver's and Chehrzad Shakiban's Applied Linear Algebra (2006) is similar to Hefferon's. Instead of property 4, it requires $f$ to send any upper-triangular matrix to the product of its diagonal entries. This is stronger than Hefferon's property 4, and I am wondering if it is actually stronger or just equivalent? Apparently the popularity of these unnatural definitions is due to the opinion that genuine multilinearity is too difficult for students to grasp; I am not sure if this justifies them, but I believe that the question it posts is interesting!",,"['linear-algebra', 'commutative-algebra', 'determinant', 'algebraic-k-theory']"
11,Understanding Householder Transformations,Understanding Householder Transformations,,"I've been thinking about Householder transformation for the past few days and one point appears to be escaping my insight. I hope that the following description helps someone correct my understanding and point me in the right direction. Based on a book I've been reading, in contrast to the popular convention, I'll assume that all vectors in the following discussion are row vectors . Let $\textbf{e}_0$ denote the leading row basis vector in $n$-dimensional Euclidean space. The goal is to transform a row vector, $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$ using an orthogonal, involutary matrix, $\Theta$. The vectors, $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are as shown in Figure 1 below. Suppose now that the difference $\textbf{z}-||\textbf{z}||\textbf{e}_0$ is aligned in the direction of some row vector, $\textbf{g}$. However, we do not know ""how much"" of $g$ is represented by $\textbf{z}-||\textbf{z}||e_0$. Let's call it $2a\textbf{g}$. This vector is shown in yellow in Figure 2 below. Since $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are equal in length, they form the sides of an isosceles triangle. A perpendicular from origin to $2a\textbf{g}$ bisects it. Therefore, the vector $(\textbf{z}-a\textbf{g})$ is orthogonal to $\textbf{g}$. That is, $\begin{eqnarray} &\phantom{\Rightarrow}&(\textbf{z}-a\textbf{g})\textbf{g}^T=0 \\ &\Rightarrow& \textbf{z}\textbf{g}^T-a\textbf{g}\textbf{g}^T=0 \\ &\Rightarrow& a =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T. \end{eqnarray}$ On the other hand, based on Figure 2, we see that performing the vector addition $(\textbf{z}-2a\textbf{g})$ will take us to $||\textbf{z}||\textbf{e}_0$ which is the reflection of $\textbf{z}$ w.r.t the perpendicular bisector from origin to $2a\textbf{g}$. In other words, $\begin{eqnarray} &\phantom{\Rightarrow}& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2a\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T\textbf{g} \\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}\;(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g}). \end{eqnarray}$ From the last equation, we can read off $\Theta=(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$ as the required orthogonal, involutary matrix that transforms $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$. Now, an example. Suppose that $\textbf{z} = [1\;\;0.75\;\;0.25]$. Here, $||\textbf{z}||=1.2748$. Hence, the transformation we are looking for is $\textbf{z}_t=||\textbf{z}||\textbf{e}_0 = [1.2748\;\;0\;\;0]$. Let us now write, $\textbf{g}=\textbf{z}-||\textbf{z}||\textbf{e}_0=[-0.2748\;\;0.75\;\;0.25]$. Notice that according to Figure 2, $\textbf{z}-||\textbf{z}||\textbf{e}_0$ should in fact be $2a\textbf{g}$. But, if we do the computation, we'll see that $a=0.5$ regardless of what $\textbf{z}$ is. That is, $\textbf{g}=2a\textbf{g}$. In fact, if we substitute $\textbf{g}=\textbf{z}-||\textbf{z}||\textbf{e}_0$ in $a =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T$ above, we can show with a little bit of algebra that $a=0.5$. After finding $\Theta$ by evaluating $(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$, we get some numbers and we will see that $\textbf{z}\,\Theta=[1.2748\;\;0\;\;0]$. The point that I'm missing is as follows: if we had called the difference, $\textbf{z}-||\textbf{z}||e_0$ in the third paragraph above as simply $\textbf{g}$, we will not be able to find $a$ and hence $\Theta$. But, I know from Figure 2 that I might as well call $\textbf{z}-||\textbf{z}||e_0$ as $\textbf{g}$ and observe that $a$ needs to be $0.5$ based on the properties of an isosceles triangle here. I'm missing some intuition here. Any comments? Thanks. Update : Can I instead make the following argument to explain how the Householder transformation matrix, $\Theta$ is derived by replacing a portion of the above discussion as follows: Suppose now that the difference $\textbf{z}-||\textbf{z}||\textbf{e}_0$ is $\textbf{g}$, i.e. , $\textbf{g}:=\textbf{z}-||\textbf{z}||\textbf{e}_0$. This vector is shown in yellow in Figure 3 below. Since $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are equal in length, they form the sides of an isosceles triangle. A perpendicular from origin to $\textbf{g}$ bisects it. Therefore, the vector $(\textbf{z}-0.5\textbf{g})$ is orthogonal to $\textbf{g}$. That is, $\begin{eqnarray} &\phantom{\Rightarrow}&(\textbf{z}-0.5\textbf{g})\textbf{g}^T=0 \\ &\Rightarrow& \textbf{z}\textbf{g}^T-0.5\textbf{g}\textbf{g}^T=0 \\ &\Rightarrow& 0.5 =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T. \end{eqnarray}$ On the other hand, based on Figure 3, we see that performing the vector addition $(\textbf{z}-\textbf{g})$ will take us to $||\textbf{z}||\textbf{e}_0$ which is the reflection of $\textbf{z}$ w.r.t the perpendicular bisector from origin to $\textbf{g}$. In other words, $\begin{eqnarray} &\phantom{\Rightarrow}& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2\times 0.5\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T\textbf{g} \;\;\;\;\;\textrm{    (where, we replaced 0.5 with } ||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T \textrm{    from before)}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}\;(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g}). \end{eqnarray}$ From the last equation, we can read off $\Theta=(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$ as the required orthogonal, involutary matrix that transforms $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$. As you can see, this explanation appears very roundabout. Is there an alternative way? Thanks for your comments.","I've been thinking about Householder transformation for the past few days and one point appears to be escaping my insight. I hope that the following description helps someone correct my understanding and point me in the right direction. Based on a book I've been reading, in contrast to the popular convention, I'll assume that all vectors in the following discussion are row vectors . Let $\textbf{e}_0$ denote the leading row basis vector in $n$-dimensional Euclidean space. The goal is to transform a row vector, $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$ using an orthogonal, involutary matrix, $\Theta$. The vectors, $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are as shown in Figure 1 below. Suppose now that the difference $\textbf{z}-||\textbf{z}||\textbf{e}_0$ is aligned in the direction of some row vector, $\textbf{g}$. However, we do not know ""how much"" of $g$ is represented by $\textbf{z}-||\textbf{z}||e_0$. Let's call it $2a\textbf{g}$. This vector is shown in yellow in Figure 2 below. Since $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are equal in length, they form the sides of an isosceles triangle. A perpendicular from origin to $2a\textbf{g}$ bisects it. Therefore, the vector $(\textbf{z}-a\textbf{g})$ is orthogonal to $\textbf{g}$. That is, $\begin{eqnarray} &\phantom{\Rightarrow}&(\textbf{z}-a\textbf{g})\textbf{g}^T=0 \\ &\Rightarrow& \textbf{z}\textbf{g}^T-a\textbf{g}\textbf{g}^T=0 \\ &\Rightarrow& a =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T. \end{eqnarray}$ On the other hand, based on Figure 2, we see that performing the vector addition $(\textbf{z}-2a\textbf{g})$ will take us to $||\textbf{z}||\textbf{e}_0$ which is the reflection of $\textbf{z}$ w.r.t the perpendicular bisector from origin to $2a\textbf{g}$. In other words, $\begin{eqnarray} &\phantom{\Rightarrow}& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2a\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T\textbf{g} \\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}\;(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g}). \end{eqnarray}$ From the last equation, we can read off $\Theta=(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$ as the required orthogonal, involutary matrix that transforms $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$. Now, an example. Suppose that $\textbf{z} = [1\;\;0.75\;\;0.25]$. Here, $||\textbf{z}||=1.2748$. Hence, the transformation we are looking for is $\textbf{z}_t=||\textbf{z}||\textbf{e}_0 = [1.2748\;\;0\;\;0]$. Let us now write, $\textbf{g}=\textbf{z}-||\textbf{z}||\textbf{e}_0=[-0.2748\;\;0.75\;\;0.25]$. Notice that according to Figure 2, $\textbf{z}-||\textbf{z}||\textbf{e}_0$ should in fact be $2a\textbf{g}$. But, if we do the computation, we'll see that $a=0.5$ regardless of what $\textbf{z}$ is. That is, $\textbf{g}=2a\textbf{g}$. In fact, if we substitute $\textbf{g}=\textbf{z}-||\textbf{z}||\textbf{e}_0$ in $a =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T$ above, we can show with a little bit of algebra that $a=0.5$. After finding $\Theta$ by evaluating $(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$, we get some numbers and we will see that $\textbf{z}\,\Theta=[1.2748\;\;0\;\;0]$. The point that I'm missing is as follows: if we had called the difference, $\textbf{z}-||\textbf{z}||e_0$ in the third paragraph above as simply $\textbf{g}$, we will not be able to find $a$ and hence $\Theta$. But, I know from Figure 2 that I might as well call $\textbf{z}-||\textbf{z}||e_0$ as $\textbf{g}$ and observe that $a$ needs to be $0.5$ based on the properties of an isosceles triangle here. I'm missing some intuition here. Any comments? Thanks. Update : Can I instead make the following argument to explain how the Householder transformation matrix, $\Theta$ is derived by replacing a portion of the above discussion as follows: Suppose now that the difference $\textbf{z}-||\textbf{z}||\textbf{e}_0$ is $\textbf{g}$, i.e. , $\textbf{g}:=\textbf{z}-||\textbf{z}||\textbf{e}_0$. This vector is shown in yellow in Figure 3 below. Since $\textbf{z}$ and $||\textbf{z}||\textbf{e}_0$ are equal in length, they form the sides of an isosceles triangle. A perpendicular from origin to $\textbf{g}$ bisects it. Therefore, the vector $(\textbf{z}-0.5\textbf{g})$ is orthogonal to $\textbf{g}$. That is, $\begin{eqnarray} &\phantom{\Rightarrow}&(\textbf{z}-0.5\textbf{g})\textbf{g}^T=0 \\ &\Rightarrow& \textbf{z}\textbf{g}^T-0.5\textbf{g}\textbf{g}^T=0 \\ &\Rightarrow& 0.5 =||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T. \end{eqnarray}$ On the other hand, based on Figure 3, we see that performing the vector addition $(\textbf{z}-\textbf{g})$ will take us to $||\textbf{z}||\textbf{e}_0$ which is the reflection of $\textbf{z}$ w.r.t the perpendicular bisector from origin to $\textbf{g}$. In other words, $\begin{eqnarray} &\phantom{\Rightarrow}& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2\times 0.5\textbf{g}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}-2||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T\textbf{g} \;\;\;\;\;\textrm{    (where, we replaced 0.5 with } ||\textbf{g}||^{-2}\textbf{z}\textbf{g}^T \textrm{    from before)}\\ &\Rightarrow& ||\textbf{z}||\textbf{e}_0 = \textbf{z}\;(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g}). \end{eqnarray}$ From the last equation, we can read off $\Theta=(\,\textbf{I}-2||\textbf{g}||^{-2}\textbf{g}^T\textbf{g})$ as the required orthogonal, involutary matrix that transforms $\textbf{z}$ to the form $||\textbf{z}||\textbf{e}_0$. As you can see, this explanation appears very roundabout. Is there an alternative way? Thanks for your comments.",,['linear-algebra']
12,Equivalence of system of nonlinear equations,Equivalence of system of nonlinear equations,,"Let $A\in\mathbb{R}^{n\times n}$ be a positive semidefinite matrix, $b\in\mathbb{R}^n$, $k>0$, and $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be a positive function. Consider the system of nonlinear equations $$ (1) \quad Ax=-k\frac{x}{g(x)}-b. $$ Let $A^+$ be the Moore–Penrose pseudoinverse of $A$. For every $q\in\mathbb{R}^n$, consider the system of nonlinear equations $$ (2)\quad  x=-kA^+\frac{x}{g(x)}-A^+b+(I-A^+A)q. $$ Find $q\in\mathbb{R}^n$ such that (1) is equivalent to (2). My attempt 1) Observe that if $A$ is non-singular then (1) is equivalent to (2) for all $q\in\mathbb{R}^n$. Indeed, since $A$ is non-singular, $A^+=A^{-1}$ and so (2) is rewritten as $$ x=-kA^{-1}\frac{x}{g(x)}-A^{-1}b+(I-A^{-1}A)q $$ which is equivalent to (1). 2) For each soluion $x$ of (1) there exists $q\in\mathbb{R}^n$ such that $x$ is the solution set of (2). Indeed, suppose that $x$ is a solution of (1). Then $$ Ax=-k\frac{x}{g(x)}-b. $$ Multiplying both sides of the above equation with $AA^+$ we obtain $$ AA^+Ax=-AA^+k\frac{x}{g(x)}-AA^+b. $$ Since $AA^+A=A$, the above equation is rewritten as $$ A\left(x+kA^+\frac{x}{g(x)}+A^+b\right)=0. $$ Therefore, there exits $q\in\mathbb{R}^n$ such that  $$ x+kA^+\frac{x}{g(x)}+A^+b=(I-A^+A)q $$ or $$ x=-kA^+\frac{x}{g(x)}-A^+b+(I-A^+A)q. $$ Hence, $x$ is the solution of (2). Thank you for all kind help and comments.","Let $A\in\mathbb{R}^{n\times n}$ be a positive semidefinite matrix, $b\in\mathbb{R}^n$, $k>0$, and $g:\mathbb{R}^n\rightarrow\mathbb{R}$ be a positive function. Consider the system of nonlinear equations $$ (1) \quad Ax=-k\frac{x}{g(x)}-b. $$ Let $A^+$ be the Moore–Penrose pseudoinverse of $A$. For every $q\in\mathbb{R}^n$, consider the system of nonlinear equations $$ (2)\quad  x=-kA^+\frac{x}{g(x)}-A^+b+(I-A^+A)q. $$ Find $q\in\mathbb{R}^n$ such that (1) is equivalent to (2). My attempt 1) Observe that if $A$ is non-singular then (1) is equivalent to (2) for all $q\in\mathbb{R}^n$. Indeed, since $A$ is non-singular, $A^+=A^{-1}$ and so (2) is rewritten as $$ x=-kA^{-1}\frac{x}{g(x)}-A^{-1}b+(I-A^{-1}A)q $$ which is equivalent to (1). 2) For each soluion $x$ of (1) there exists $q\in\mathbb{R}^n$ such that $x$ is the solution set of (2). Indeed, suppose that $x$ is a solution of (1). Then $$ Ax=-k\frac{x}{g(x)}-b. $$ Multiplying both sides of the above equation with $AA^+$ we obtain $$ AA^+Ax=-AA^+k\frac{x}{g(x)}-AA^+b. $$ Since $AA^+A=A$, the above equation is rewritten as $$ A\left(x+kA^+\frac{x}{g(x)}+A^+b\right)=0. $$ Therefore, there exits $q\in\mathbb{R}^n$ such that  $$ x+kA^+\frac{x}{g(x)}+A^+b=(I-A^+A)q $$ or $$ x=-kA^+\frac{x}{g(x)}-A^+b+(I-A^+A)q. $$ Hence, $x$ is the solution of (2). Thank you for all kind help and comments.",,"['linear-algebra', 'systems-of-equations']"
13,Norm of the inverse of a tridiagonal,Norm of the inverse of a tridiagonal,,"Let's take a tridiagonal matrix (in general not Toeplitz, nor symmetric) $$L=\begin{pmatrix}a_1 & -b_1 & & & \\ -c_1 & a_2 & -b_2 \\ & -c_2 & \ddots & \ddots\\ & & \ddots\end{pmatrix}$$ and suppose that it's an M-matrix, diagonally dominant, with $a_i,b_i,c_i>0$, $a_i\ge c_{i-1}+ b_i$, ($c_0=b_n=0$) and such that there exists at least an index $i$ with $a_i> c_{i-1}+ b_i$. I'm interested in an upper bound for the infinity norm of the inverse. I already know that if $0<\gamma\le a_i-c_{i-1}- b_i$ for all indexes, then $$\|L^{-1}\|_{\infty}\le \gamma^{-1}$$ and, more in general, if $w\ge 0$ and $Lw\ge \gamma e$, then $$\|L^{-1}\|_{\infty}\le \gamma^{-1}\|w\|_{\infty}$$ I also know that for the particular matrix with $a_i=2, b_i=c_i=1$, we have $$\|L^{-1}\|_{\infty}\le \frac{(n+1)^2}{8}$$ Is there some general result of this kind? And if the matrix is Toeplitz, there's a rule for the dependence of such a bound to $n^k$ for some $k$? Note: This Link may be useful.","Let's take a tridiagonal matrix (in general not Toeplitz, nor symmetric) $$L=\begin{pmatrix}a_1 & -b_1 & & & \\ -c_1 & a_2 & -b_2 \\ & -c_2 & \ddots & \ddots\\ & & \ddots\end{pmatrix}$$ and suppose that it's an M-matrix, diagonally dominant, with $a_i,b_i,c_i>0$, $a_i\ge c_{i-1}+ b_i$, ($c_0=b_n=0$) and such that there exists at least an index $i$ with $a_i> c_{i-1}+ b_i$. I'm interested in an upper bound for the infinity norm of the inverse. I already know that if $0<\gamma\le a_i-c_{i-1}- b_i$ for all indexes, then $$\|L^{-1}\|_{\infty}\le \gamma^{-1}$$ and, more in general, if $w\ge 0$ and $Lw\ge \gamma e$, then $$\|L^{-1}\|_{\infty}\le \gamma^{-1}\|w\|_{\infty}$$ I also know that for the particular matrix with $a_i=2, b_i=c_i=1$, we have $$\|L^{-1}\|_{\infty}\le \frac{(n+1)^2}{8}$$ Is there some general result of this kind? And if the matrix is Toeplitz, there's a rule for the dependence of such a bound to $n^k$ for some $k$? Note: This Link may be useful.",,"['linear-algebra', 'matrices', 'normed-spaces', 'inverse']"
14,How to calculate this special determinant,How to calculate this special determinant,,$$\left| {\begin{array}{*{20}{c}} 1&{{a_1} + a_1^{ - 1}}& \cdots &{a_1^{n - 1} + a_1^{n - 3} + a_1^{n - 5} +  \cdots  + a_1^{1 - n}}\\ 1&{{a_2} + a_2^{ - 1}}& \cdots &{a_2^{n - 1} + a_2^{n - 3} + a_2^{n - 5} +  \cdots  + a_2^{1 - n}}\\  \vdots & \vdots & \vdots & \vdots \\ 1&{{a_n} + a_n^{ - 1}}& \cdots &{a_n^{n - 1} + a_n^{n - 3} + a_n^{n - 5} +  \cdots  + a_n^{1 - n}} \end{array}} \right|$$ How to calculate it? Is it possible to transform it into Vandermonde? I want to know whether it can never be zero if $a_i\ne a_j$. Any suggestion is appreciated!,$$\left| {\begin{array}{*{20}{c}} 1&{{a_1} + a_1^{ - 1}}& \cdots &{a_1^{n - 1} + a_1^{n - 3} + a_1^{n - 5} +  \cdots  + a_1^{1 - n}}\\ 1&{{a_2} + a_2^{ - 1}}& \cdots &{a_2^{n - 1} + a_2^{n - 3} + a_2^{n - 5} +  \cdots  + a_2^{1 - n}}\\  \vdots & \vdots & \vdots & \vdots \\ 1&{{a_n} + a_n^{ - 1}}& \cdots &{a_n^{n - 1} + a_n^{n - 3} + a_n^{n - 5} +  \cdots  + a_n^{1 - n}} \end{array}} \right|$$ How to calculate it? Is it possible to transform it into Vandermonde? I want to know whether it can never be zero if $a_i\ne a_j$. Any suggestion is appreciated!,,"['linear-algebra', 'matrix-rank']"
15,Rotate an area around a diagonal line.,Rotate an area around a diagonal line.,,"I know how to find the volume of the figure formed when you rotate a $2$-dimensional area around a horizontal or vertical line, but what if it were a diagonal line instead? For example: Rotate the area between the curve $y=x^2$ and $y=x$ around the line $y=x$. That should create a diagonal ""football"" shape, starting at the origin. Anyways, the way I thought to go about the problem was to rotate the equation $45°$ to the right, and then solve the problem as if it were rotated around the x-axis. So I decided to convert $y=x^2$ to a polar equation, add $\frac{pi}{4}$ to $\theta$, and then turn it back into a rectangular equation. That worked, and it gave me this equation, which gives a diagonal parabola opening toward the first quadrant: $\frac{\sqrt{2}}{2}(y+x) = \frac{1}{2}xy(x^2+y^2)$ Only problem is that I need to isolate $y$, such that I can solve this problem like an ordinary rotation problem, and that seems impossible. What should I do to solve the problem? I don't know much linear algebra, but could that be used to solve this instead?","I know how to find the volume of the figure formed when you rotate a $2$-dimensional area around a horizontal or vertical line, but what if it were a diagonal line instead? For example: Rotate the area between the curve $y=x^2$ and $y=x$ around the line $y=x$. That should create a diagonal ""football"" shape, starting at the origin. Anyways, the way I thought to go about the problem was to rotate the equation $45°$ to the right, and then solve the problem as if it were rotated around the x-axis. So I decided to convert $y=x^2$ to a polar equation, add $\frac{pi}{4}$ to $\theta$, and then turn it back into a rectangular equation. That worked, and it gave me this equation, which gives a diagonal parabola opening toward the first quadrant: $\frac{\sqrt{2}}{2}(y+x) = \frac{1}{2}xy(x^2+y^2)$ Only problem is that I need to isolate $y$, such that I can solve this problem like an ordinary rotation problem, and that seems impossible. What should I do to solve the problem? I don't know much linear algebra, but could that be used to solve this instead?",,"['calculus', 'linear-algebra', 'rotations']"
16,Did I correctly derive the scheme for this PDE using the Crank Nicolson Method?,Did I correctly derive the scheme for this PDE using the Crank Nicolson Method?,,"I'm taking an Applied Numerical Methods course this semester, and I was given the following homework problem: Basically, before I begin writing any sort of code, I would like to ensure that I have set everything up correctly. I took the starting PDE and used the Crank-Nicolson method to bring it down to the following: $$ \frac{u_m^{n+1}-u_m^n}{k} = 0.4\frac{u_{m+1}^{n+1}-2u_{m}^{n+1}+u_{m-1}^{n+1} + u_{m+1}^{n}-2u_{m}^{n}+u_{m-1}^{n}}{2h^2} - 0.2\frac{u_{m+1}^{n+1}-u_{m-1}^{n+1}-u_{m+1}^{n}+u_{m-1}^{n}}{4h} + \frac{u_m^{n+1}+u_m^{n}}{2} $$ To get this, I took the starting equation (first one on the list in the problem) and shifted focus up by 1/2k (or n + 1/2).  Then, from this point, for the u_t side I took the centered difference and for the other side of the equation I took the averages [(u_m^n+1 + u_m_n) /2].  Then, each of the components on the right hand side had two numerator components.  I took a centered difference of the appropriate derivative of each of these components, recombined the fractions, and obtained my result above.  This is basically me describing the Crank Nicolson method.  I very well may have done something wrong here, so if something seems off please let me know! Finally, I plugged in h=k=0.001 and separated the n's from the n+1's, and I got $$ -200.05u_{m-1}^{n+1}+400.9995u_{m}^{n+1}-199.95u_{m+1}^{n+1} = 199.95u_{m-1}^{n}-398.9995u_{m}^{n}+200.05u_{m+1}^{n} $$ Then, if I'm not mistaken, this formula gives me information on the following points Then from here, I just use MATLAB to create a system of 1000 equations with 1000 variables. The scheme doesn't look too pretty, but at least according to my calculations it is the correct formula for the problem.  I may have missed the mark a bit (or a lot) here, so I'd really appreciate if someone could tell me what they think of my work (both my results and my method) thus far so I may continue with the problem.","I'm taking an Applied Numerical Methods course this semester, and I was given the following homework problem: Basically, before I begin writing any sort of code, I would like to ensure that I have set everything up correctly. I took the starting PDE and used the Crank-Nicolson method to bring it down to the following: $$ \frac{u_m^{n+1}-u_m^n}{k} = 0.4\frac{u_{m+1}^{n+1}-2u_{m}^{n+1}+u_{m-1}^{n+1} + u_{m+1}^{n}-2u_{m}^{n}+u_{m-1}^{n}}{2h^2} - 0.2\frac{u_{m+1}^{n+1}-u_{m-1}^{n+1}-u_{m+1}^{n}+u_{m-1}^{n}}{4h} + \frac{u_m^{n+1}+u_m^{n}}{2} $$ To get this, I took the starting equation (first one on the list in the problem) and shifted focus up by 1/2k (or n + 1/2).  Then, from this point, for the u_t side I took the centered difference and for the other side of the equation I took the averages [(u_m^n+1 + u_m_n) /2].  Then, each of the components on the right hand side had two numerator components.  I took a centered difference of the appropriate derivative of each of these components, recombined the fractions, and obtained my result above.  This is basically me describing the Crank Nicolson method.  I very well may have done something wrong here, so if something seems off please let me know! Finally, I plugged in h=k=0.001 and separated the n's from the n+1's, and I got $$ -200.05u_{m-1}^{n+1}+400.9995u_{m}^{n+1}-199.95u_{m+1}^{n+1} = 199.95u_{m-1}^{n}-398.9995u_{m}^{n}+200.05u_{m+1}^{n} $$ Then, if I'm not mistaken, this formula gives me information on the following points Then from here, I just use MATLAB to create a system of 1000 equations with 1000 variables. The scheme doesn't look too pretty, but at least according to my calculations it is the correct formula for the problem.  I may have missed the mark a bit (or a lot) here, so I'd really appreciate if someone could tell me what they think of my work (both my results and my method) thus far so I may continue with the problem.",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'proof-verification', 'numerical-methods']"
17,Lie derivative and simultaneous diagonalizability,Lie derivative and simultaneous diagonalizability,,"I just arrived at this theorem: Let $M$ be an $n$-manifold and let  $\{X_j\}_{j\le k}$ be a collecion of $k$ vector fields    and $p \in V \subset M$ satisfying: 1) $\{X_j(p)\}_{j\le k}$ is linearly independent 2) $[X_j,X_i]_a = 0$ for all $a$ in $V$. $\implies$ There exists a chart $(x,U)$ with $U \subset V$ that satisfies   $X_i=\frac{\partial}{\partial x^i}$ for all $i \le k$ The analogy with simultaneously diagonalizable operators (iff commuting) seems very strong. Is there a deep algebraic result behind this?","I just arrived at this theorem: Let $M$ be an $n$-manifold and let  $\{X_j\}_{j\le k}$ be a collecion of $k$ vector fields    and $p \in V \subset M$ satisfying: 1) $\{X_j(p)\}_{j\le k}$ is linearly independent 2) $[X_j,X_i]_a = 0$ for all $a$ in $V$. $\implies$ There exists a chart $(x,U)$ with $U \subset V$ that satisfies   $X_i=\frac{\partial}{\partial x^i}$ for all $i \le k$ The analogy with simultaneously diagonalizable operators (iff commuting) seems very strong. Is there a deep algebraic result behind this?",,"['linear-algebra', 'abstract-algebra', 'differential-geometry']"
18,"Relationship between eigenvalues of two related, Euclidean distance matrices","Relationship between eigenvalues of two related, Euclidean distance matrices",,"If $X=\{x_1,\ldots,x_N\}$ is a set of points in $\mathbb{R}^n$ then one can generate a Euclidean distance matrix $D = [d_{ij}]$ where $d_{ij}=\Vert x_i-x_j\Vert_2^2$ is the square of the Euclidean distance from point $i$ to point $j$.  Note that $X$ is real and symmetric (all real eigenvalues). Suppose that I remove an element from $X$ to create $X^{\prime}$.  The corresponding matrix $D^{\prime}$ is related to $D$.  It is formed from $D$ by deleting the row and column corresponding to the element removed from $X$. My question is: is there any relationship between the (non-zero) eigenvalues of $D$ and the non-zero eigenvalues of $D^{\prime}$?  Certainly, counting up multiplicities, $D^{\prime}$ has one fewer eigenvalue.  It is also known that any such Euclidean Distance Matrix has (at most) $n+2$ non-zero eigenvalues; it has exactly $n+2$ non-zero eigenvalues whenever $N>n$ and all the points in $X$ do not lie in some $N-1$ dimensional subspace. EDIT: If it matters, in the case that I'm looking at, $n=3$ and $N$ is much bigger (on the order of 100s).","If $X=\{x_1,\ldots,x_N\}$ is a set of points in $\mathbb{R}^n$ then one can generate a Euclidean distance matrix $D = [d_{ij}]$ where $d_{ij}=\Vert x_i-x_j\Vert_2^2$ is the square of the Euclidean distance from point $i$ to point $j$.  Note that $X$ is real and symmetric (all real eigenvalues). Suppose that I remove an element from $X$ to create $X^{\prime}$.  The corresponding matrix $D^{\prime}$ is related to $D$.  It is formed from $D$ by deleting the row and column corresponding to the element removed from $X$. My question is: is there any relationship between the (non-zero) eigenvalues of $D$ and the non-zero eigenvalues of $D^{\prime}$?  Certainly, counting up multiplicities, $D^{\prime}$ has one fewer eigenvalue.  It is also known that any such Euclidean Distance Matrix has (at most) $n+2$ non-zero eigenvalues; it has exactly $n+2$ non-zero eigenvalues whenever $N>n$ and all the points in $X$ do not lie in some $N-1$ dimensional subspace. EDIT: If it matters, in the case that I'm looking at, $n=3$ and $N$ is much bigger (on the order of 100s).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
19,Orthonormal basis of polynomials,Orthonormal basis of polynomials,,"I am trying to find an orthonormal basis of the vector space $P^{3}(t)$ with an inner product defined by $$\langle f, g \rangle = \int_0^1f(t)g(t)dt$$ by applying the gram schmit alogorotin to ${(1,1+t,t+t^2,t^3)}$=$(v_1,v_2,v_3,v_4)$ Here is what I have done so far; Suppose first we just look for an orthogonal basis, say $(w_1,w_2,w_3,w_4)$ Set $ w_1=v_1, ie  , w_1=1$ Then $w_2= v_2 - \lt \frac{v_2, w_1}{w_1,w_1} \gt w_1$ $$w_2= (1+t)- \frac{\int_0^1 1+t  dt}{\int_0^1 1 dt}(1)$$ $w_2=t-\frac{1}{2}$ $$w_3= (t+t^2)-\frac{\int_0^1 t+t^2 dt}{\int_0^1 1 dt}(1)-\frac{\int_0^1 t^3+t^2/2-t/2 dt}{\int_0^1(t-1/2)^2dt}(t-\frac{1}{2})$$ $w_3=(t+t^2)-\frac{5}{6}-2t+1=t^2-t+\frac{1}{6}$ $$w_4= t^3-\frac{1}{4}-\frac{\int_0^1 t^4-\frac{t^3}{2} dt}{\int_0^1 (t-\frac{1}{2})^2dt}(t-\frac{1}{2})-\frac{\int_0^1 t^5-t^4+t^3/6 dt}{\int_0^1(t^2-t+1/6)^2dt}(t^2-t+1/6)$$ Which I seem to evaluate to be $t^3-3t^2/2-12t/5+39/20$ I keep seeming to get issues with finding $w_4$ though.I don't know if it just arithmetic errors or what. I get something new or huge fractions and I don't know it that would make sense. I set it up the same way as I would any of the others, but I use $v_4.$ Anyways, if someone could show me that and the normalization that'd be great. ( I know to normalize we divide just by the length of the vector). Plus, is what I have so far valid? Anything I'm missing or any tips/tricks?","I am trying to find an orthonormal basis of the vector space $P^{3}(t)$ with an inner product defined by $$\langle f, g \rangle = \int_0^1f(t)g(t)dt$$ by applying the gram schmit alogorotin to ${(1,1+t,t+t^2,t^3)}$=$(v_1,v_2,v_3,v_4)$ Here is what I have done so far; Suppose first we just look for an orthogonal basis, say $(w_1,w_2,w_3,w_4)$ Set $ w_1=v_1, ie  , w_1=1$ Then $w_2= v_2 - \lt \frac{v_2, w_1}{w_1,w_1} \gt w_1$ $$w_2= (1+t)- \frac{\int_0^1 1+t  dt}{\int_0^1 1 dt}(1)$$ $w_2=t-\frac{1}{2}$ $$w_3= (t+t^2)-\frac{\int_0^1 t+t^2 dt}{\int_0^1 1 dt}(1)-\frac{\int_0^1 t^3+t^2/2-t/2 dt}{\int_0^1(t-1/2)^2dt}(t-\frac{1}{2})$$ $w_3=(t+t^2)-\frac{5}{6}-2t+1=t^2-t+\frac{1}{6}$ $$w_4= t^3-\frac{1}{4}-\frac{\int_0^1 t^4-\frac{t^3}{2} dt}{\int_0^1 (t-\frac{1}{2})^2dt}(t-\frac{1}{2})-\frac{\int_0^1 t^5-t^4+t^3/6 dt}{\int_0^1(t^2-t+1/6)^2dt}(t^2-t+1/6)$$ Which I seem to evaluate to be $t^3-3t^2/2-12t/5+39/20$ I keep seeming to get issues with finding $w_4$ though.I don't know if it just arithmetic errors or what. I get something new or huge fractions and I don't know it that would make sense. I set it up the same way as I would any of the others, but I use $v_4.$ Anyways, if someone could show me that and the normalization that'd be great. ( I know to normalize we divide just by the length of the vector). Plus, is what I have so far valid? Anything I'm missing or any tips/tricks?",,"['linear-algebra', 'inner-products']"
20,Hermitian Matrices with At Most Pair-wise Eigenvalue Degeneracy,Hermitian Matrices with At Most Pair-wise Eigenvalue Degeneracy,,"Let $n\in2\mathbb{N}$ be given. Let $H\in Mat_{n\times n}(\mathbb{C})$ be a Hermitian traceless matrix such that its eigenvalues have at most pairwise degeneracy. (That is, if the eigenvalues are $\{\lambda_i\}_{i=1}^n$ then we allow that $\lambda_i = \lambda_{i+1} \forall i\in\{1,3,\dots,n-1\}$, but no other degeneracies other than that.) Conjecture : The set of all such matrices $H$ is homotopy-type-equivalent to $S^m$ for some $m\in\mathbb{N}$. Question: Is this conjecture true? If yes, how to prove it, if no, is there anything else this is homotopy-type-equivalent to?","Let $n\in2\mathbb{N}$ be given. Let $H\in Mat_{n\times n}(\mathbb{C})$ be a Hermitian traceless matrix such that its eigenvalues have at most pairwise degeneracy. (That is, if the eigenvalues are $\{\lambda_i\}_{i=1}^n$ then we allow that $\lambda_i = \lambda_{i+1} \forall i\in\{1,3,\dots,n-1\}$, but no other degeneracies other than that.) Conjecture : The set of all such matrices $H$ is homotopy-type-equivalent to $S^m$ for some $m\in\mathbb{N}$. Question: Is this conjecture true? If yes, how to prove it, if no, is there anything else this is homotopy-type-equivalent to?",,"['linear-algebra', 'general-topology', 'algebraic-topology']"
21,Rank of a matrix whose all entries have the form $m^k$,Rank of a matrix whose all entries have the form,m^k,"The original problem is: Compute the determinant $$\begin{vmatrix}  1^k & 2^k & 3^k  & \cdots  & n^k  \\   2^k& 3^k  & 4^k  &\cdots  & (n+1)^k \\   3^k& 4^k & 5^k  &\cdots  & (n+2)^k \\   \vdots&\vdots  &\vdots  &\ddots   & \vdots\\   n^k& (n+1)^k  & (n+2)^k  &\cdots  &(2n-1)^k  \end{vmatrix}$$ where $n\ge 2$ and $0\le k\le n-2.$ The answer is $0$, but I want to find the rank of the above matrix. While solving the original one, I found a beautiful identity. That is, for all $x$: $$\displaystyle \sum _{l=0}^{k+1} {{k+1}\choose l}\cdot (-1)^l\cdot (x+l)^k=0$$ I have two questions: Base on the above identity, is the rank of the matrix equal to $k+1$? Is there a name for the above identity?","The original problem is: Compute the determinant $$\begin{vmatrix}  1^k & 2^k & 3^k  & \cdots  & n^k  \\   2^k& 3^k  & 4^k  &\cdots  & (n+1)^k \\   3^k& 4^k & 5^k  &\cdots  & (n+2)^k \\   \vdots&\vdots  &\vdots  &\ddots   & \vdots\\   n^k& (n+1)^k  & (n+2)^k  &\cdots  &(2n-1)^k  \end{vmatrix}$$ where $n\ge 2$ and $0\le k\le n-2.$ The answer is $0$, but I want to find the rank of the above matrix. While solving the original one, I found a beautiful identity. That is, for all $x$: $$\displaystyle \sum _{l=0}^{k+1} {{k+1}\choose l}\cdot (-1)^l\cdot (x+l)^k=0$$ I have two questions: Base on the above identity, is the rank of the matrix equal to $k+1$? Is there a name for the above identity?",,"['linear-algebra', 'determinant', 'matrix-rank']"
22,Maximum element of Perron vector,Maximum element of Perron vector,,"Suppose $A$ is an entrywise nonnegative matrix that is symmetric, irreducible and has a zero diagonal. By Perron-Frobenius theorem, the spectral radius $\rho(A)$ is an eigenvalue and $A$ has, up to scaling, a unique entrywise positive eigenvector $v$ (i.e. the Perron vector ) for this eigenvalue. Let $r_i$ denotes the $i$-th row sum of $A$. In general, the maximum row sum of $A$ and the maximum entry of $v$ occur in different positions. For example, when $$ A=\pmatrix{0&0&0&2\\ 0&0&3&2\\ 0&3&0&3\\ 2&2&3&0}, $$ the Perron vector is $(0.2067,\ 0.5236,\ 0.5908,\ 0.5780)^\top$. Therefore the maximum entry of the Perron vector is the third one, but the maximum row sum of $A$ occurs in the fourth row. However, in the computer experiments that I have carried out, this kind of exceptional cases are relatively infrequent. So, it seems that by imposing some mild conditions, one may force $\arg\max_i r_i=\arg\max_i v_i$. Now my question is: Under what conditions can we conclude that $\arg\max_i r_i=\arg\max_i v_i$? (Let's ignore the pathological cases where $\arg\max_i r_i$ or $\arg\max_i v_i$ are not unique.) This question arose when I was pondering what could happen if we apply Google style ranking to a weighted undirected graph. If $\arg\max_i r_i=\arg\max_i v_i$, there is no need to compute the Perron vector if I only want to find the highest-ranked member.","Suppose $A$ is an entrywise nonnegative matrix that is symmetric, irreducible and has a zero diagonal. By Perron-Frobenius theorem, the spectral radius $\rho(A)$ is an eigenvalue and $A$ has, up to scaling, a unique entrywise positive eigenvector $v$ (i.e. the Perron vector ) for this eigenvalue. Let $r_i$ denotes the $i$-th row sum of $A$. In general, the maximum row sum of $A$ and the maximum entry of $v$ occur in different positions. For example, when $$ A=\pmatrix{0&0&0&2\\ 0&0&3&2\\ 0&3&0&3\\ 2&2&3&0}, $$ the Perron vector is $(0.2067,\ 0.5236,\ 0.5908,\ 0.5780)^\top$. Therefore the maximum entry of the Perron vector is the third one, but the maximum row sum of $A$ occurs in the fourth row. However, in the computer experiments that I have carried out, this kind of exceptional cases are relatively infrequent. So, it seems that by imposing some mild conditions, one may force $\arg\max_i r_i=\arg\max_i v_i$. Now my question is: Under what conditions can we conclude that $\arg\max_i r_i=\arg\max_i v_i$? (Let's ignore the pathological cases where $\arg\max_i r_i$ or $\arg\max_i v_i$ are not unique.) This question arose when I was pondering what could happen if we apply Google style ranking to a weighted undirected graph. If $\arg\max_i r_i=\arg\max_i v_i$, there is no need to compute the Perron vector if I only want to find the highest-ranked member.",,"['linear-algebra', 'matrices', 'nonnegative-matrices']"
23,Eigenvalues of Overlapping block diagonal matrices,Eigenvalues of Overlapping block diagonal matrices,,I look for eigenvalues of general overlapping block diagonal matrices. e.g. $$\left[ \begin{matrix} 1 & 4 & 0 & 0 & 0 & 0\\ 4 & 2 & 3 & 2 & 0 & 0\\ 0 & 3 & 3 & 8 & 0 & 0\\ 0 & 2 & 8 & 3 & 1 & 2\\ 0 & 0 & 0 & 1 & 4 & 5\\ 0 & 0 & 0 & 2 & 5 & 1 \end{matrix} \right] $$ Where the blocks are overlapping. $$ \mathbf{A} = \left[ \begin{matrix} \mathbf{A}_1 & \mathbf{0} \\ \mathbf{0} & \mathbf{A}_2 \\ \end{matrix} \right] $$ In which the eigenvalues of $\mathbf{A}_1$ and $\mathbf{A}_2$ is already given.  Is there a way that one can calculate the eigenvalues of $\mathbf{A}$ given those conditions? We know $\mathbf{A}_1$ and $\mathbf{A}_2$ are Hermitian. I thought of perturbation theory to model it as a block diagonal plus a perturbation matrix added as error... thanks,I look for eigenvalues of general overlapping block diagonal matrices. e.g. $$\left[ \begin{matrix} 1 & 4 & 0 & 0 & 0 & 0\\ 4 & 2 & 3 & 2 & 0 & 0\\ 0 & 3 & 3 & 8 & 0 & 0\\ 0 & 2 & 8 & 3 & 1 & 2\\ 0 & 0 & 0 & 1 & 4 & 5\\ 0 & 0 & 0 & 2 & 5 & 1 \end{matrix} \right] $$ Where the blocks are overlapping. $$ \mathbf{A} = \left[ \begin{matrix} \mathbf{A}_1 & \mathbf{0} \\ \mathbf{0} & \mathbf{A}_2 \\ \end{matrix} \right] $$ In which the eigenvalues of $\mathbf{A}_1$ and $\mathbf{A}_2$ is already given.  Is there a way that one can calculate the eigenvalues of $\mathbf{A}$ given those conditions? We know $\mathbf{A}_1$ and $\mathbf{A}_2$ are Hermitian. I thought of perturbation theory to model it as a block diagonal plus a perturbation matrix added as error... thanks,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
24,Computing N cofactors,Computing N cofactors,,"I have an $N\times N$ matrix of small size (say $N=20$). Is there a way to evaluate the $N$ cofactors of the elements of the first row, faster than in the obvious way as $N$ independent determinant computations, involving $O(N^4)$ operations ? For instance, can we exploit Gaussian elimination with partial pivoting or Gram-Schmidt orthogonalization to get them in a single go ?","I have an $N\times N$ matrix of small size (say $N=20$). Is there a way to evaluate the $N$ cofactors of the elements of the first row, faster than in the obvious way as $N$ independent determinant computations, involving $O(N^4)$ operations ? For instance, can we exploit Gaussian elimination with partial pivoting or Gram-Schmidt orthogonalization to get them in a single go ?",,['linear-algebra']
25,Geometric interpretation of $x_1^2y_1^2+x_2^2y_2^2+x_3^2y_3^2+\dots$,Geometric interpretation of,x_1^2y_1^2+x_2^2y_2^2+x_3^2y_3^2+\dots,"Say $x$ and $y$ are two $L_2$ unit vectors of size $n$. In that case the inner product: $$x_1y_1+x_2y_2+x_3y_3+\dots+x_ny_n$$ Is the cosine of the angle between them. For an application I was originally interested in the angle like this, but I have only been able to achieve the squared inner product: $$x_1^2y_1^2+x_2^2y_2^2+x_3^2y_3^2+\dots+x_n^2y_n^2$$ And now I wonder if this has any interesting geometrical interpretations? I suppose it can't be too closely related to the angle between the vectors, given the value can only be in the interval $[0,1]$. I suppose this is also similar to what we have in the Cauchy Schartz inequality, after some rewriting, but I'm not sure what the geometric intepretation is of this. Any ideas?","Say $x$ and $y$ are two $L_2$ unit vectors of size $n$. In that case the inner product: $$x_1y_1+x_2y_2+x_3y_3+\dots+x_ny_n$$ Is the cosine of the angle between them. For an application I was originally interested in the angle like this, but I have only been able to achieve the squared inner product: $$x_1^2y_1^2+x_2^2y_2^2+x_3^2y_3^2+\dots+x_n^2y_n^2$$ And now I wonder if this has any interesting geometrical interpretations? I suppose it can't be too closely related to the angle between the vectors, given the value can only be in the interval $[0,1]$. I suppose this is also similar to what we have in the Cauchy Schartz inequality, after some rewriting, but I'm not sure what the geometric intepretation is of this. Any ideas?",,"['linear-algebra', 'geometry', 'algebraic-geometry', 'inner-products']"
26,Show that $(1+a_1x+\ldots+a_rx^r)^k=1+x+x^{r+1}q(x)$,Show that,(1+a_1x+\ldots+a_rx^r)^k=1+x+x^{r+1}q(x),"Fixed $k\ge 1$. Show that for each $r$, you can find $a_1,\cdot\cdot\cdot,a_r\in \mathbb{F}$ such that :$$(1+a_1x+\cdot\cdot\cdot+a_rx^r)^k=1+x+x^{r+1}q(x)$$   where $q(x)$ is a polynomial. Any ideas? I tried using induction as follows: for the base case, $r=1:$ We wish that $$(1+a_1x)^k = 1+x+x^2q(x)$$ Setting $a_1 = 1$ we get: $$1 + x + \text{other terms} = 1 + x + x^2q(x)$$ $$\text{other terms} = x^2q(x)$$","Fixed $k\ge 1$. Show that for each $r$, you can find $a_1,\cdot\cdot\cdot,a_r\in \mathbb{F}$ such that :$$(1+a_1x+\cdot\cdot\cdot+a_rx^r)^k=1+x+x^{r+1}q(x)$$   where $q(x)$ is a polynomial. Any ideas? I tried using induction as follows: for the base case, $r=1:$ We wish that $$(1+a_1x)^k = 1+x+x^2q(x)$$ Setting $a_1 = 1$ we get: $$1 + x + \text{other terms} = 1 + x + x^2q(x)$$ $$\text{other terms} = x^2q(x)$$",,"['calculus', 'linear-algebra', 'polynomials', 'induction']"
27,Decomposition of order-$n$ tensors,Decomposition of order- tensors,n,"If $V$ is a finite-dimensional vector space, then $V\otimes V\cong\mathbf{S}^2(V)\oplus\bigwedge^2(V)$. The first summand on the right is the symmetric part of $V\otimes V$ and the second summand is the anti-symmetric part.  This identification of vector spaces is just the decomposition of order-$2$ tensors into their symmetric and anti-symmetric parts. (When $V$ is $1$-dimensional, we simply have $V\otimes V\cong\mathbf{S}^2(V)$.) Without imposing assumptions on the dimension of $V$, what is the corresponding decomposition for $V\otimes V\otimes V$?  There should be an $\mathbf{S}^3(V)$ and a $\bigwedge^3(V)$, but what are the ""mixed"" terms? Basically, I'd like a decomposition for $\bigotimes^nV$. I'm not so interested in the (anti-)symmetrization formulas, i.e. I'm not worried about the specific isomorphism.","If $V$ is a finite-dimensional vector space, then $V\otimes V\cong\mathbf{S}^2(V)\oplus\bigwedge^2(V)$. The first summand on the right is the symmetric part of $V\otimes V$ and the second summand is the anti-symmetric part.  This identification of vector spaces is just the decomposition of order-$2$ tensors into their symmetric and anti-symmetric parts. (When $V$ is $1$-dimensional, we simply have $V\otimes V\cong\mathbf{S}^2(V)$.) Without imposing assumptions on the dimension of $V$, what is the corresponding decomposition for $V\otimes V\otimes V$?  There should be an $\mathbf{S}^3(V)$ and a $\bigwedge^3(V)$, but what are the ""mixed"" terms? Basically, I'd like a decomposition for $\bigotimes^nV$. I'm not so interested in the (anti-)symmetrization formulas, i.e. I'm not worried about the specific isomorphism.",,"['linear-algebra', 'vector-spaces', 'tensor-products', 'tensors']"
28,"If we have SVD of a submatrix, is that useful for SVD of full matrix?","If we have SVD of a submatrix, is that useful for SVD of full matrix?",,"Say we have a matrix that has a singular value decomposition $M_1=U_1\Sigma_1V_1$, and we have another matrix $M_2$, which has the same number of rows.  Can we say anything about the SVD of their concatenation, $M=[M_1 M_2]$ that reuses the SVD of $M_1$?  In particular, if we know the decomposition of $M_1$ can we calculate the SVD of $M$ more quickly? Purpose: I want to be able to speed up calculation of SVD if we've already computed SVD for a submatrix.","Say we have a matrix that has a singular value decomposition $M_1=U_1\Sigma_1V_1$, and we have another matrix $M_2$, which has the same number of rows.  Can we say anything about the SVD of their concatenation, $M=[M_1 M_2]$ that reuses the SVD of $M_1$?  In particular, if we know the decomposition of $M_1$ can we calculate the SVD of $M$ more quickly? Purpose: I want to be able to speed up calculation of SVD if we've already computed SVD for a submatrix.",,"['linear-algebra', 'svd']"
29,Sum of square of absolute values of roots of a polynomial,Sum of square of absolute values of roots of a polynomial,,"If $\alpha_1,\dots,\alpha_n$ are roots of a polynomial $$P(z)=z^n+a_1z^{n-1}+\dots+a_{n-1}z+1,$$then how can one express the sum $$|\alpha_1|^2+\dots+|\alpha_n|^2$$in terms of $a_i$'s? Thanks.","If $\alpha_1,\dots,\alpha_n$ are roots of a polynomial $$P(z)=z^n+a_1z^{n-1}+\dots+a_{n-1}z+1,$$then how can one express the sum $$|\alpha_1|^2+\dots+|\alpha_n|^2$$in terms of $a_i$'s? Thanks.",,"['linear-algebra', 'polynomials', 'roots']"
30,"Invertibility of block matrices, with the property of being symmetric, positive definite, and of full rank:","Invertibility of block matrices, with the property of being symmetric, positive definite, and of full rank:",,"If A and B are real matrices, with A being symmetric, B having at least as many columns as rows, and the matrix C defined as: $$         \begin{bmatrix}         A & B^T \\         B & 0 \\               \end{bmatrix} $$ how can I prove that: 1) C is invertible, if A is positive definite and B of full rank and, 2) Is C always invertible, if A is invertible and B of full rank? My attempt so far was to sketch the block matrices. For part 1) I let A be of size $nxn$.  Since A is positive definite, it is invertible, and thus has full rank. I let B be of size $(k-(n+1)+1)x(p)$ = $(k-n)x(p)$, so that $B^T$ is of size $(p)x(k-n)$.  Then C is of size $kxk$. Then I try to argue that, this $kxk$ square matrix has full rank, with rank = k, which implies that C is invertible.  B has full row rank, and $B^T$ has full column rank. Am I sort of close to the answer?  I'm basically trying to avoid the usage of determinants of block matrices, as I'm not all that comfortable with that method - but perhaps it's necessary for this question. For part 2)  My work for part 1), if it's correct, would imply that, yes, C is always invertible if A is invertible and B of full rank.  I get the feeling, though, that there is a counterexample. Thanks in advance for your help,","If A and B are real matrices, with A being symmetric, B having at least as many columns as rows, and the matrix C defined as: $$         \begin{bmatrix}         A & B^T \\         B & 0 \\               \end{bmatrix} $$ how can I prove that: 1) C is invertible, if A is positive definite and B of full rank and, 2) Is C always invertible, if A is invertible and B of full rank? My attempt so far was to sketch the block matrices. For part 1) I let A be of size $nxn$.  Since A is positive definite, it is invertible, and thus has full rank. I let B be of size $(k-(n+1)+1)x(p)$ = $(k-n)x(p)$, so that $B^T$ is of size $(p)x(k-n)$.  Then C is of size $kxk$. Then I try to argue that, this $kxk$ square matrix has full rank, with rank = k, which implies that C is invertible.  B has full row rank, and $B^T$ has full column rank. Am I sort of close to the answer?  I'm basically trying to avoid the usage of determinants of block matrices, as I'm not all that comfortable with that method - but perhaps it's necessary for this question. For part 2)  My work for part 1), if it's correct, would imply that, yes, C is always invertible if A is invertible and B of full rank.  I get the feeling, though, that there is a counterexample. Thanks in advance for your help,",,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank']"
31,Eigenvalues of a general block Hermitian matrix,Eigenvalues of a general block Hermitian matrix,,"I have a question regarding the eigenvalues of a block Hermitian matrix as a function of the eigenvalues of the diagonal block matrices and the off-diagonal matrices. In particular, I am interested in the $2 \times 2$ block case. I have checked some previous posts [1]: Eigenvalues of certain block Hermitian matrix and in Wikipedia, and it is clear to me that the solution for the case $$M_1 = \begin{pmatrix} A & B\\ B &A \end{pmatrix}$$ where $M_1$ , $A$ and $B$ are Hermitian, can be derived. Nevertheless, I would like to know if it is possible, in the following case $$M_2 = \begin{pmatrix} A & B\\ B^{H} &C \end{pmatrix}$$ where $M_2$ , $A$ and $C$ are Hermitian and $B$ corresponds to the off-diagonal block, to say something about the eigenvalues of $M_2$ as a function of the eigenvalues of $A$ and $C$ and the matrix $B$ .","I have a question regarding the eigenvalues of a block Hermitian matrix as a function of the eigenvalues of the diagonal block matrices and the off-diagonal matrices. In particular, I am interested in the block case. I have checked some previous posts [1]: Eigenvalues of certain block Hermitian matrix and in Wikipedia, and it is clear to me that the solution for the case where , and are Hermitian, can be derived. Nevertheless, I would like to know if it is possible, in the following case where , and are Hermitian and corresponds to the off-diagonal block, to say something about the eigenvalues of as a function of the eigenvalues of and and the matrix .",2 \times 2 M_1 = \begin{pmatrix} A & B\\ B &A \end{pmatrix} M_1 A B M_2 = \begin{pmatrix} A & B\\ B^{H} &C \end{pmatrix} M_2 A C B M_2 A C B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'hermitian-matrices']"
32,The intuition of the rank theorem,The intuition of the rank theorem,,"In Rudin's Principle of Math Analysis, I know the proof of the rank theorem. However, I fail to understand its content. 9.32 Theorem: Suppose $m,n,$ are nonnegative integers, $m\geq r,n\geq r$, $F$ is a $C^1$ mapping of an open set $E\subset R^n$ into $R^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A=F'(a)$, Let $Y_1$ be the range of A, and let $P$ be the projection in $R^m$ whose range if $Y_1$. Let $Y_2$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^n$, with $a\in U,U\subset E$ and there is a 1-1 mapping $H$ of $V$ onto $U$(whose inverse is also of class $C^1$) such that (66) $F(H(x))=Ax+\phi(Ax)$ $(x\in V)$ where $\phi$ is a $C^1$ mapping of open set $A(V)\subset Y_1$ into $Y_2$. It is clear that F is a manifold of dimension r. However, what is the purpose of H? I do not understand the content of H. Or should I treat H as a chart of the manifold described by F?","In Rudin's Principle of Math Analysis, I know the proof of the rank theorem. However, I fail to understand its content. 9.32 Theorem: Suppose $m,n,$ are nonnegative integers, $m\geq r,n\geq r$, $F$ is a $C^1$ mapping of an open set $E\subset R^n$ into $R^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A=F'(a)$, Let $Y_1$ be the range of A, and let $P$ be the projection in $R^m$ whose range if $Y_1$. Let $Y_2$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^n$, with $a\in U,U\subset E$ and there is a 1-1 mapping $H$ of $V$ onto $U$(whose inverse is also of class $C^1$) such that (66) $F(H(x))=Ax+\phi(Ax)$ $(x\in V)$ where $\phi$ is a $C^1$ mapping of open set $A(V)\subset Y_1$ into $Y_2$. It is clear that F is a manifold of dimension r. However, what is the purpose of H? I do not understand the content of H. Or should I treat H as a chart of the manifold described by F?",,"['real-analysis', 'linear-algebra', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
33,List the primes for which the following system of linear equations DOES NOT have a solution in $\mathbb{Z}_p$,List the primes for which the following system of linear equations DOES NOT have a solution in,\mathbb{Z}_p,Let $p$ be a prime and consider the field $\Bbb Z_p$. List the primes for which the following system of linear equations does not have a solution in $\Bbb Z_p$: $$ \begin{align} 5x + 3y &= 4 \tag{1}\\ 3x + 6y & = 1\tag{2} \end{align}$$ My try is as follows: Determinant of the coefficient matrix is $21$. $21$ will be 0 if $p=3$ or $7$. So the answer will be $3$ and $7$. is it correct?,Let $p$ be a prime and consider the field $\Bbb Z_p$. List the primes for which the following system of linear equations does not have a solution in $\Bbb Z_p$: $$ \begin{align} 5x + 3y &= 4 \tag{1}\\ 3x + 6y & = 1\tag{2} \end{align}$$ My try is as follows: Determinant of the coefficient matrix is $21$. $21$ will be 0 if $p=3$ or $7$. So the answer will be $3$ and $7$. is it correct?,,['linear-algebra']
34,"Prove that $\sin(x+a), \sin(x+b),\sin(x+c), \hspace{5pt} a,b,c \in \mathbb{R}$ are linearly dependent",Prove that  are linearly dependent,"\sin(x+a), \sin(x+b),\sin(x+c), \hspace{5pt} a,b,c \in \mathbb{R}","I just want an answer verification (or not). We have that: $\begin{array}[t]{l} \sin(x+a)=\cos a\cdot \sin x + \sin a \cdot \cos x\\ \sin(x+b)=\cos b\cdot \sin x + \sin b \cdot \cos x\\ \sin(x+c)=\cos c\cdot \sin x + \sin c \cdot \cos x\\ \end{array}$ Since $\sin(x+a), \sin (x+b), \sin(x+c) \in  \hspace{5pt} V=\langle\cos x , \sin x\rangle$ and $\dim V=2\implies$ given functions cannot be a basis of $V$. Consequently, they are linearly dependent.","I just want an answer verification (or not). We have that: $\begin{array}[t]{l} \sin(x+a)=\cos a\cdot \sin x + \sin a \cdot \cos x\\ \sin(x+b)=\cos b\cdot \sin x + \sin b \cdot \cos x\\ \sin(x+c)=\cos c\cdot \sin x + \sin c \cdot \cos x\\ \end{array}$ Since $\sin(x+a), \sin (x+b), \sin(x+c) \in  \hspace{5pt} V=\langle\cos x , \sin x\rangle$ and $\dim V=2\implies$ given functions cannot be a basis of $V$. Consequently, they are linearly dependent.",,['linear-algebra']
35,finding the algebraic dimension of $\ell^p$ spaces,finding the algebraic dimension of  spaces,\ell^p,"I want to know ""how we can find the algebraic dimension(the cardinal number of the Hamel basis) for $\ell^p$ spaces."" What can we say about $\ell^p(I)$, where $I$ is an infinite set?\ Moreover, for any given cardinal number $\alpha \neq \aleph_0$, is there a Banach space $X$ that its algebraic dimension equal $\alpha$?","I want to know ""how we can find the algebraic dimension(the cardinal number of the Hamel basis) for $\ell^p$ spaces."" What can we say about $\ell^p(I)$, where $I$ is an infinite set?\ Moreover, for any given cardinal number $\alpha \neq \aleph_0$, is there a Banach space $X$ that its algebraic dimension equal $\alpha$?",,"['linear-algebra', 'banach-spaces']"
36,solving equation also involving unknown matrix in trace,solving equation also involving unknown matrix in trace,,"Given two real $m$ x $k$ matrices $A_1$ and $B_1$ and two $k$ x $k$ real matrices $A_2$ and $B_2$ I want to solve the following equation for $Q$. $Q$ is an orthogonal matrix, i.e. $Q^TQ=I$. $$\frac{tr(Q^TA_1^TB_1)}{tr(A_1^TA_1)} Q^TA_1^TB_1 + Q^TA_2^TB_2 = symmetric$$ The solution to a similar problem (without the trace expression) has e.g. been described by Schönemann (1966, p. 2) and goes like this. $$  Q^TA_1^TB_1 + Q^TA_2^TB_2 = symmetric \\ Q^T(\underbrace{A_1^TB_1 + A_2^TB_2}_{C}) = symmetric \\ Q^TC = C^TQ \\ C = QC^TQ \\ CC^T = QC^TQQ^TCQ^T = QC^TCQ^T $$ With $CC^T$ and $C^TC$ being diagonizable and having the same latent roots, let $$ CC^T = WDW^T \text{and} C^TC = VDV^T \\ \text{with} \\ I = W^TW = WW^T = V^TV = VV^T$$ We get $$ WDW^T = QVDV^TQ^T$$ and thus $$W=QV \\ \text{and} \\  Q=WV^T$$ I tried work out an argument along the same lines but do not know how to do that with the trace expression which also contains $Q$. Any ideas? PS. I am a psychologist, no mathematician, so please bear with me ;) Schönemann, P. H. (1966). A generalized solution of the orthogonal procrustes problem. Psychometrika , 31 (1), 1–10. doi:10.1007/BF02289451","Given two real $m$ x $k$ matrices $A_1$ and $B_1$ and two $k$ x $k$ real matrices $A_2$ and $B_2$ I want to solve the following equation for $Q$. $Q$ is an orthogonal matrix, i.e. $Q^TQ=I$. $$\frac{tr(Q^TA_1^TB_1)}{tr(A_1^TA_1)} Q^TA_1^TB_1 + Q^TA_2^TB_2 = symmetric$$ The solution to a similar problem (without the trace expression) has e.g. been described by Schönemann (1966, p. 2) and goes like this. $$  Q^TA_1^TB_1 + Q^TA_2^TB_2 = symmetric \\ Q^T(\underbrace{A_1^TB_1 + A_2^TB_2}_{C}) = symmetric \\ Q^TC = C^TQ \\ C = QC^TQ \\ CC^T = QC^TQQ^TCQ^T = QC^TCQ^T $$ With $CC^T$ and $C^TC$ being diagonizable and having the same latent roots, let $$ CC^T = WDW^T \text{and} C^TC = VDV^T \\ \text{with} \\ I = W^TW = WW^T = V^TV = VV^T$$ We get $$ WDW^T = QVDV^TQ^T$$ and thus $$W=QV \\ \text{and} \\  Q=WV^T$$ I tried work out an argument along the same lines but do not know how to do that with the trace expression which also contains $Q$. Any ideas? PS. I am a psychologist, no mathematician, so please bear with me ;) Schönemann, P. H. (1966). A generalized solution of the orthogonal procrustes problem. Psychometrika , 31 (1), 1–10. doi:10.1007/BF02289451",,"['linear-algebra', 'matrices', 'trace']"
37,"Find minimal set of cosets $C$, so that each $2$ vectors in $A_n$ are in one coset in $C$","Find minimal set of cosets , so that each  vectors in  are in one coset in",C 2 A_n C,"Let $F_2^n$ be the set of all vectors of length $n$ with values of $0$ or $1$ and $A_n$ = $F_2^n \setminus(11\ldots1)$. Set $A_n$ contains all vectors except one with all $1$s. We can consider cosets of codimention $1$ that lies in $A_n$. For example solutions of equation $x_1+x_2=1$ is such a coset. I am interested in this problem: find minimal set of cosets $C$, so that each $2$ vectors in $A_n$ are in one coset in $C$. I have found this solution: $x_1=0, \ldots,x_n=0,\ x_i+x_j=1$ for all $1 \le i<j\le n$. It contains $n+ {n\choose 2}=\frac{n(n+1)}{2}$ cosets. I need help finding better solution or proving that minimal set of cosets has size $\theta(n^2)$ so no linear solution is possible.","Let $F_2^n$ be the set of all vectors of length $n$ with values of $0$ or $1$ and $A_n$ = $F_2^n \setminus(11\ldots1)$. Set $A_n$ contains all vectors except one with all $1$s. We can consider cosets of codimention $1$ that lies in $A_n$. For example solutions of equation $x_1+x_2=1$ is such a coset. I am interested in this problem: find minimal set of cosets $C$, so that each $2$ vectors in $A_n$ are in one coset in $C$. I have found this solution: $x_1=0, \ldots,x_n=0,\ x_i+x_j=1$ for all $1 \le i<j\le n$. It contains $n+ {n\choose 2}=\frac{n(n+1)}{2}$ cosets. I need help finding better solution or proving that minimal set of cosets has size $\theta(n^2)$ so no linear solution is possible.",,"['linear-algebra', 'finite-fields']"
38,"Reference Request: Prereqs for Lecture Notes on ""Abstract Linear Algebra""","Reference Request: Prereqs for Lecture Notes on ""Abstract Linear Algebra""",,"I just found this set of lecture notes on linear algebra which seems to go over several things I've been wondering about as I study linear algebra.  Unfortunately there are very few exercises in the text and I'm not even positive I have the skill to prove the first few lemmas myself.  I have read the first couple of sections so far, and I think I understand the material as well I as can while not having any exercises to practice on -- which is not well enough .  So my question is: Is there a book/ reference which goes over this same material, but can be more easily self-studied?  Or else can anyone recommend a text which will at least prepare me enough to be able to figure these lecture notes out on my own?","I just found this set of lecture notes on linear algebra which seems to go over several things I've been wondering about as I study linear algebra.  Unfortunately there are very few exercises in the text and I'm not even positive I have the skill to prove the first few lemmas myself.  I have read the first couple of sections so far, and I think I understand the material as well I as can while not having any exercises to practice on -- which is not well enough .  So my question is: Is there a book/ reference which goes over this same material, but can be more easily self-studied?  Or else can anyone recommend a text which will at least prepare me enough to be able to figure these lecture notes out on my own?",,['linear-algebra']
39,The square root of positive definite matrix,The square root of positive definite matrix,,"Let $M$ be the manifold of real positive definite $n \times n$ matrices, define a mapping $i:A \to \sqrt A$ (where $A\in M$ and $\sqrt A$ means the unique positive definite square root of $A$). Please show that $i$ is smooth.","Let $M$ be the manifold of real positive definite $n \times n$ matrices, define a mapping $i:A \to \sqrt A$ (where $A\in M$ and $\sqrt A$ means the unique positive definite square root of $A$). Please show that $i$ is smooth.",,"['matrices', 'differential-geometry']"
40,"Show that $M = \left(\begin{smallmatrix} A & 0 \\ 0 & A^{-1} \\ \end{smallmatrix}\right)$ is a product of elementary matrices, where $A \in GL_n(R)$.","Show that  is a product of elementary matrices, where .",M = \left(\begin{smallmatrix} A & 0 \\ 0 & A^{-1} \\ \end{smallmatrix}\right) A \in GL_n(R),"If $A\in GL_n(R)$ , where $R$ is a commutative ring with identity, I would like to prove $$          M=\begin{pmatrix}           A & 0 \\           0 & A^{-1} \\          \end{pmatrix}\in E_{2n}(R), $$ i.e., $M$ is a product of elementary matrices, where by elementary matrices I mean the matrices obtained by performing elementary row and column operations on the identity matrix $I$ which are defined by $L_j \mapsto \lambda L_i+L_j$ for $L_j$ a column or a row. I'm trying to solve this question by brute force, making these operations on this matrix and showing this matrix is equivalent to the identity one. I would like to know if there is a simpler and more elegant method to solve this question. Thanks in advance.","If , where is a commutative ring with identity, I would like to prove i.e., is a product of elementary matrices, where by elementary matrices I mean the matrices obtained by performing elementary row and column operations on the identity matrix which are defined by for a column or a row. I'm trying to solve this question by brute force, making these operations on this matrix and showing this matrix is equivalent to the identity one. I would like to know if there is a simpler and more elegant method to solve this question. Thanks in advance.","A\in GL_n(R) R 
         M=\begin{pmatrix}
          A & 0 \\
          0 & A^{-1} \\
         \end{pmatrix}\in E_{2n}(R),
 M I L_j \mapsto \lambda L_i+L_j L_j","['linear-algebra', 'abstract-algebra', 'matrices']"
41,The relationship between CPTP maps and quadratic forms,The relationship between CPTP maps and quadratic forms,,"Let $H$ be a finite-dimensional Hilbert space (so there is a canonical isomorphism $H\cong H^*$). For a Hilbert space $H$ define $B(H)$ to be the space of linear operators on $H$; we have $B(H)\cong H\otimes H^*$. Suppose $\Lambda:B(H)\to B(H)$ is a CPTP (completely positive trace preserving) map. In particular it is linear, so we can view it as an element of $B(H)\otimes B(H)^*\cong (H\otimes H^*)\otimes (H\otimes H^*)^*$. There is an isomorphism $F$ from this space to $[(H\otimes H^*)\otimes (H\otimes H^*)]^*$, the space of bilinear (or quadratic) forms on $H\otimes H^*$. The map is defined by $F(|E_{kl}\rangle\langle E_{ij}|)=\langle E_{ki}|\otimes \langle E_{lj}|$. (I think--if another map makes more sense then replace this.) Note here we are viewing matrices as vectors in the space $B(H)$. Question : What is the condition on $F(\Lambda)$ that is equivalent to $\Lambda$ being CPTP (and why might it intuitively hold), and is there a way to prove this directly? Is this a useful perspective? By the Kraus representation, the CPTP maps $\Lambda$ are exactly those in the form $\Lambda(\rho)=\sum_i A_i\rho A_i^{\dagger}$. The above then maps $\Lambda$ to the quadratic form $\sum_i\langle A_i|\otimes \langle A_i|$, which is positive semidefinite quadratic form. In particular, for example, this tells us that we need at most $n^2$ matrices $A_i$. So $F(\Lambda)$ being positive semidefinite is a necessary condition for $\Lambda$ to be CPTP; what is a necessary and sufficient condition? Furthermore, this was proved using Kraus representation, but could it be proved directly (and hence the Krause representation would be a corollary)?","Let $H$ be a finite-dimensional Hilbert space (so there is a canonical isomorphism $H\cong H^*$). For a Hilbert space $H$ define $B(H)$ to be the space of linear operators on $H$; we have $B(H)\cong H\otimes H^*$. Suppose $\Lambda:B(H)\to B(H)$ is a CPTP (completely positive trace preserving) map. In particular it is linear, so we can view it as an element of $B(H)\otimes B(H)^*\cong (H\otimes H^*)\otimes (H\otimes H^*)^*$. There is an isomorphism $F$ from this space to $[(H\otimes H^*)\otimes (H\otimes H^*)]^*$, the space of bilinear (or quadratic) forms on $H\otimes H^*$. The map is defined by $F(|E_{kl}\rangle\langle E_{ij}|)=\langle E_{ki}|\otimes \langle E_{lj}|$. (I think--if another map makes more sense then replace this.) Note here we are viewing matrices as vectors in the space $B(H)$. Question : What is the condition on $F(\Lambda)$ that is equivalent to $\Lambda$ being CPTP (and why might it intuitively hold), and is there a way to prove this directly? Is this a useful perspective? By the Kraus representation, the CPTP maps $\Lambda$ are exactly those in the form $\Lambda(\rho)=\sum_i A_i\rho A_i^{\dagger}$. The above then maps $\Lambda$ to the quadratic form $\sum_i\langle A_i|\otimes \langle A_i|$, which is positive semidefinite quadratic form. In particular, for example, this tells us that we need at most $n^2$ matrices $A_i$. So $F(\Lambda)$ being positive semidefinite is a necessary condition for $\Lambda$ to be CPTP; what is a necessary and sufficient condition? Furthermore, this was proved using Kraus representation, but could it be proved directly (and hence the Krause representation would be a corollary)?",,"['linear-algebra', 'hilbert-spaces']"
42,Prove: matrix A is diagonalizable iff exp(A) is diagonalizble,Prove: matrix A is diagonalizable iff exp(A) is diagonalizble,,"I need to prove: matrix A is diagonalizable iff $\exp(A)$ is diagonalizble. exp means exponent function. I know to prove that if $A$ is diagonalizable so $\exp(A)$ is diagonalizable, but have a problem with the other side. can I write $P^{-1}.\exp(A).P=D$ (since $\exp(A)$ is diagonalizable  ) and operate log on both sides of the equation? if yes I'm done, or any other hints?","I need to prove: matrix A is diagonalizable iff $\exp(A)$ is diagonalizble. exp means exponent function. I know to prove that if $A$ is diagonalizable so $\exp(A)$ is diagonalizable, but have a problem with the other side. can I write $P^{-1}.\exp(A).P=D$ (since $\exp(A)$ is diagonalizable  ) and operate log on both sides of the equation? if yes I'm done, or any other hints?",,"['linear-algebra', 'matrices', 'exponentiation']"
43,A power of the characteristic polynomial,A power of the characteristic polynomial,,"Let $A$ be a square matrix with real or complex coefficients of size $n$. Define its characteristic polynomial by $\chi_A(X) = \det(A-XI_n)$ (or $\det(XI_n-A)$ if you prefer). The question is  : Prove that there exists a positive integer $q$ such that $\chi_A^q(A)=0$ where $\chi_A^q(X)= \chi_A(X)\times\cdots\times \chi_A(X)$ without using the Cayley-Hamilton theorem or the definition of an ideal of a ring. My problem with this problem is to find a ""good"" proof that doesn't mimic a proof of Cayley-Hamilton theorem. I know that it is easy to see (with a dimensional argument) that there exists a annihilating polynomial. But here I can't think of anything. An attempt of solution in the real or complex case : We know that there exists an annihilating polynomial (since $(I_n,A,A^2,\cdots,A^{n^2})$ can't be linearly independent, where $I_n$ is the identity matrix), we denote by $P$ such a polynomial. Then $P=\prod_{i=1}^p \left( X-\lambda_i\right)^{\alpha_i}$ ($\lambda_i\in \mathbb{C}$). We can assume that the $\lambda_i$ are roots of the characteristic polynomial. If not, then by definition of the characteristic polynomial $A-\lambda_iI_n$ is invertible. We can multiply $P$ by $(A-\lambda_iI_n)^{-\alpha_i}$, we still get an annihilating polynomial, and since we can permute all the terms, the term $\left( X-\lambda_i\right)^{\alpha_i}$ disappears. So if we take a sufficiently large power of the characteristic polynomial of $A$, we recover all the factors of $P$ times an another polynomial. This gives the desired result. What do you think of my attempt? Any ideas for the case where the coefficients of $A$ belong to an arbitrary abelian ring?","Let $A$ be a square matrix with real or complex coefficients of size $n$. Define its characteristic polynomial by $\chi_A(X) = \det(A-XI_n)$ (or $\det(XI_n-A)$ if you prefer). The question is  : Prove that there exists a positive integer $q$ such that $\chi_A^q(A)=0$ where $\chi_A^q(X)= \chi_A(X)\times\cdots\times \chi_A(X)$ without using the Cayley-Hamilton theorem or the definition of an ideal of a ring. My problem with this problem is to find a ""good"" proof that doesn't mimic a proof of Cayley-Hamilton theorem. I know that it is easy to see (with a dimensional argument) that there exists a annihilating polynomial. But here I can't think of anything. An attempt of solution in the real or complex case : We know that there exists an annihilating polynomial (since $(I_n,A,A^2,\cdots,A^{n^2})$ can't be linearly independent, where $I_n$ is the identity matrix), we denote by $P$ such a polynomial. Then $P=\prod_{i=1}^p \left( X-\lambda_i\right)^{\alpha_i}$ ($\lambda_i\in \mathbb{C}$). We can assume that the $\lambda_i$ are roots of the characteristic polynomial. If not, then by definition of the characteristic polynomial $A-\lambda_iI_n$ is invertible. We can multiply $P$ by $(A-\lambda_iI_n)^{-\alpha_i}$, we still get an annihilating polynomial, and since we can permute all the terms, the term $\left( X-\lambda_i\right)^{\alpha_i}$ disappears. So if we take a sufficiently large power of the characteristic polynomial of $A$, we recover all the factors of $P$ times an another polynomial. This gives the desired result. What do you think of my attempt? Any ideas for the case where the coefficients of $A$ belong to an arbitrary abelian ring?",,"['linear-algebra', 'proof-verification', 'alternative-proof']"
44,proving a theorem of alternative,proving a theorem of alternative,,"I've read the following exercise in my book: Let $A\in\mathbb R^{m\times n},b\in\mathbb R^m,c\in\mathbb R^n$. Then exactly one holds: $Ax=0,c^t\cdot x=1$ with $x\geq0$ has a solution $A^ty\geq c$ has a solution I've tried to prove it but having some troubles with the solution. My attempt: Suppose 1. and 2. are right. Then  $$y^t\cdot x\leq(A^ty)^t\cdot x=y^t\cdot Ax=0$$This is a contradiction. So both aren't solvable together. So suppose 1. hasn't got any solution. How can you show 2. has a solution? Thanks for helping! Edit: I want to use Farkas' Lemma .","I've read the following exercise in my book: Let $A\in\mathbb R^{m\times n},b\in\mathbb R^m,c\in\mathbb R^n$. Then exactly one holds: $Ax=0,c^t\cdot x=1$ with $x\geq0$ has a solution $A^ty\geq c$ has a solution I've tried to prove it but having some troubles with the solution. My attempt: Suppose 1. and 2. are right. Then  $$y^t\cdot x\leq(A^ty)^t\cdot x=y^t\cdot Ax=0$$This is a contradiction. So both aren't solvable together. So suppose 1. hasn't got any solution. How can you show 2. has a solution? Thanks for helping! Edit: I want to use Farkas' Lemma .",,['linear-algebra']
45,A challenge question in determinant of real matrices!,A challenge question in determinant of real matrices!,,"Suppose that $n\in \mathbb N -\{1\}$ and $a_{11},a_{12},\ldots,a_{nn}$ are $n^2$ distinct real numbers, prove that there is some enumeration of $a_{ij}$'s like $b_{ij}\ (i,j=1,2,\ldots,n)$ such that,the determinant of the matrix $B=[b_{ij}]$ isn't zero!","Suppose that $n\in \mathbb N -\{1\}$ and $a_{11},a_{12},\ldots,a_{nn}$ are $n^2$ distinct real numbers, prove that there is some enumeration of $a_{ij}$'s like $b_{ij}\ (i,j=1,2,\ldots,n)$ such that,the determinant of the matrix $B=[b_{ij}]$ isn't zero!",,"['linear-algebra', 'matrices', 'determinant']"
46,Set geometry and inclusion,Set geometry and inclusion,,"I would like to prove that the set of the symmetric positive semi-definite matrices which is defined as $$\Delta_2= \{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad \|S-\big(Y^TY\big)^{1/2}\|_F\leq\epsilon c^*\}$$ is the biggest set by construction (according to $\epsilon$) included in $$\Delta_1=\{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad S=\Big((Y-AX)^T(Y-AX)\Big)^{1/2}, \|A\|_1\leq \epsilon\}$$ where $(\cdot)^{1/2}$ is the principal square root operator, $\mathbb{S}_{m,m}$ is the set of the symmetric positive semi-definite matrices, $X\in\cal M_{n,m}$ and $Y\in\cal M_{n,m}$ are given matrices, $A\in\cal M_{n,n}$ is a variable matrix and $\|\cdot\|_1$ is the summation of the absolute value of the entries of a matrix. The small term $\epsilon$ is given and the other constant small term $c^*$ is known too where $c^*\leq \frac{1}{m\|V\|\|X\|}$ and $V=Y\big(Y^TY\big)^{-1/2}$ to ensure that $\exists~A$ such that $\|A\|_1\leq\epsilon$ when $S\in\Delta_2$ and hence $S\in\Delta_1$. Thus, by this construction it's clear here that $\Delta_2\subseteq\Delta_1$. In other words, I would like to prove that $\exists ~\delta$ where $\epsilon_0=\epsilon+\delta$ such that we can find $$S_2\in\Delta_2'= \{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad \|S-\big(Y^TY\big)^{1/2}\|_F\leq\epsilon_0 c^*\}$$ and $S_2\notin \Delta_1$. My second question is how to compute the distance in $\|\cdot\|_p$ from the center of the circular set $\Delta_2$ which is $\big(Y^TY\big)^{1/2}$ to the furthest edge of $\Delta_1$.","I would like to prove that the set of the symmetric positive semi-definite matrices which is defined as $$\Delta_2= \{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad \|S-\big(Y^TY\big)^{1/2}\|_F\leq\epsilon c^*\}$$ is the biggest set by construction (according to $\epsilon$) included in $$\Delta_1=\{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad S=\Big((Y-AX)^T(Y-AX)\Big)^{1/2}, \|A\|_1\leq \epsilon\}$$ where $(\cdot)^{1/2}$ is the principal square root operator, $\mathbb{S}_{m,m}$ is the set of the symmetric positive semi-definite matrices, $X\in\cal M_{n,m}$ and $Y\in\cal M_{n,m}$ are given matrices, $A\in\cal M_{n,n}$ is a variable matrix and $\|\cdot\|_1$ is the summation of the absolute value of the entries of a matrix. The small term $\epsilon$ is given and the other constant small term $c^*$ is known too where $c^*\leq \frac{1}{m\|V\|\|X\|}$ and $V=Y\big(Y^TY\big)^{-1/2}$ to ensure that $\exists~A$ such that $\|A\|_1\leq\epsilon$ when $S\in\Delta_2$ and hence $S\in\Delta_1$. Thus, by this construction it's clear here that $\Delta_2\subseteq\Delta_1$. In other words, I would like to prove that $\exists ~\delta$ where $\epsilon_0=\epsilon+\delta$ such that we can find $$S_2\in\Delta_2'= \{S\in\mathbb{S}_{m,m} \quad \text{s.t.}\quad \|S-\big(Y^TY\big)^{1/2}\|_F\leq\epsilon_0 c^*\}$$ and $S_2\notin \Delta_1$. My second question is how to compute the distance in $\|\cdot\|_p$ from the center of the circular set $\Delta_2$ which is $\big(Y^TY\big)^{1/2}$ to the furthest edge of $\Delta_1$.",,"['linear-algebra', 'geometry']"
47,Particular determinant made of powers of algebraic numbers is nonzero?,Particular determinant made of powers of algebraic numbers is nonzero?,,"Let $P$ be a degree-two polynomial, with roots $\alpha,\beta$. Is there a simple condition on $P$ (or on $\alpha,\beta$), equivalent to the following : $$ (*)\alpha^i\beta^j-\alpha^j\beta^i+\beta^i-\beta^j+\alpha^j-\alpha^i= \left|\begin{array}{cc} \alpha^i-1 &\beta^i-1 \\ \alpha^j-1 & \beta^j-1  \end{array}\right| \neq 0, \ \text{for any } i,j\in{\mathbb Z}\setminus \lbrace 0\rbrace,i \neq j. $$ If the coefficients of $P$ are integers, is (*) a decidable question ? Motivation : (*) is equivalent to the fact that any linear recurrence sequence of the form $c_1\alpha^i+c_2\beta^i$ does not attain any value more than twice, unless it is zero.","Let $P$ be a degree-two polynomial, with roots $\alpha,\beta$. Is there a simple condition on $P$ (or on $\alpha,\beta$), equivalent to the following : $$ (*)\alpha^i\beta^j-\alpha^j\beta^i+\beta^i-\beta^j+\alpha^j-\alpha^i= \left|\begin{array}{cc} \alpha^i-1 &\beta^i-1 \\ \alpha^j-1 & \beta^j-1  \end{array}\right| \neq 0, \ \text{for any } i,j\in{\mathbb Z}\setminus \lbrace 0\rbrace,i \neq j. $$ If the coefficients of $P$ are integers, is (*) a decidable question ? Motivation : (*) is equivalent to the fact that any linear recurrence sequence of the form $c_1\alpha^i+c_2\beta^i$ does not attain any value more than twice, unless it is zero.",,"['linear-algebra', 'algebraic-number-theory']"
48,Find the projection of $b$ onto the column space of $A$,Find the projection of  onto the column space of,b A,"$A=   \left[ {\begin{array}{ccccc}    1 & 1 \\    1 & -1 \\    -2 & 4   \end{array} } \right] $ and $b =    \left[ {\begin{array}{cccc}    1  \\    2 \\    7   \end{array} } \right] $ I use the formula $p = A(A^{T}A)^{-1}A^{T}b$ $A^{T}A =    \left[ {\begin{array}{ccccc}    6 & -8 \\    -8 & 18 \\   \end{array} } \right]$ $(A^{T}A)^{-1} = \frac{1}{44}    \left[ {\begin{array}{ccccc}    18 & 8 \\    8 & 6 \\   \end{array} } \right]$ $A^{T}b =   \left[ {\begin{array}{ccccc}    -11 \\    27 \\   \end{array} } \right]$ Putting everything together, $p =  \frac{1}{44}\left[ {\begin{array}{ccccc}    1 & 1 \\    1 & -1 \\    -2 & 4   \end{array} } \right] \left[ {\begin{array}{ccccc}    18 & 8 \\    8 & 6 \\   \end{array} } \right] \left[ {\begin{array}{ccccc}    -11 \\    27 \\   \end{array} } \right]= \frac{1}{44}\left[ {\begin{array}{ccccc}    92 \\    -56 \\ 260  \end{array} } \right]$ Which is completely different from the book's answer $p=(3,0,6)$. Where did I go wrong? Also, if the question asked for a projection of $b$ onto the row space of $A$, would I take the transpose of $A$ first, say let $B = A^{T}$ then use the formula $p = B(B^{T}B)^{-1}B^{T}b$?","$A=   \left[ {\begin{array}{ccccc}    1 & 1 \\    1 & -1 \\    -2 & 4   \end{array} } \right] $ and $b =    \left[ {\begin{array}{cccc}    1  \\    2 \\    7   \end{array} } \right] $ I use the formula $p = A(A^{T}A)^{-1}A^{T}b$ $A^{T}A =    \left[ {\begin{array}{ccccc}    6 & -8 \\    -8 & 18 \\   \end{array} } \right]$ $(A^{T}A)^{-1} = \frac{1}{44}    \left[ {\begin{array}{ccccc}    18 & 8 \\    8 & 6 \\   \end{array} } \right]$ $A^{T}b =   \left[ {\begin{array}{ccccc}    -11 \\    27 \\   \end{array} } \right]$ Putting everything together, $p =  \frac{1}{44}\left[ {\begin{array}{ccccc}    1 & 1 \\    1 & -1 \\    -2 & 4   \end{array} } \right] \left[ {\begin{array}{ccccc}    18 & 8 \\    8 & 6 \\   \end{array} } \right] \left[ {\begin{array}{ccccc}    -11 \\    27 \\   \end{array} } \right]= \frac{1}{44}\left[ {\begin{array}{ccccc}    92 \\    -56 \\ 260  \end{array} } \right]$ Which is completely different from the book's answer $p=(3,0,6)$. Where did I go wrong? Also, if the question asked for a projection of $b$ onto the row space of $A$, would I take the transpose of $A$ first, say let $B = A^{T}$ then use the formula $p = B(B^{T}B)^{-1}B^{T}b$?",,['linear-algebra']
49,A question about inner product and Gram-Schmidt process,A question about inner product and Gram-Schmidt process,,"Let there be the following bilinear form: $\int_0^1f(x)g(x)x\,dx$, which acts on the polynomials with degree $\leq2$.  I needed to prove it's an inner product and then find an orthonormal basis. I needed to use Gram-Schmidt proccess. So, when I make the vectors I find to be of length one, what's the inner product I use? Lets say some vector for basis if $h$, then the normal of the vectors is $\sqrt{\int_0^1h(x)h(x)x\,dx}$, or is it the 'standard' inner product $\sqrt{\int_0^1h(x)h(x)\,dx}$? In other words, when a basis is orthnormal, it is orthogonal and of length one with accordance to some specific inner product and not necessarily others? Thanks in advance!","Let there be the following bilinear form: $\int_0^1f(x)g(x)x\,dx$, which acts on the polynomials with degree $\leq2$.  I needed to prove it's an inner product and then find an orthonormal basis. I needed to use Gram-Schmidt proccess. So, when I make the vectors I find to be of length one, what's the inner product I use? Lets say some vector for basis if $h$, then the normal of the vectors is $\sqrt{\int_0^1h(x)h(x)x\,dx}$, or is it the 'standard' inner product $\sqrt{\int_0^1h(x)h(x)\,dx}$? In other words, when a basis is orthnormal, it is orthogonal and of length one with accordance to some specific inner product and not necessarily others? Thanks in advance!",,['linear-algebra']
50,"Prove that $n$ is even and $|A| \in \{-1,1 \}$",Prove that  is even and,"n |A| \in \{-1,1 \}","Let $A \in M_{n} (\mathbb R)$, such that $A^2=-I_{n}$. Prove that $n$ is even and $|A| \in \{-1,1 \}$. I started by compute the determinant of both sides: $A^2=-I_{n}\Leftrightarrow$ $|A^2|=|-I_{n}|\Leftrightarrow$ $|A|\cdot |A|=(-1)^n|I_{n}|\Leftrightarrow$ $|A|\cdot |A|=(-1)^n$ It's known that $|A|$ is a real number. The product of two equal real numbers is always positive. So $n$ must be even. I can write: $|A|\cdot |A|=1$ To get the previous result there are two possibilities, $|A|=1$ or $|A|=-1$. My doubt is if there is more posibilities. Thanks.","Let $A \in M_{n} (\mathbb R)$, such that $A^2=-I_{n}$. Prove that $n$ is even and $|A| \in \{-1,1 \}$. I started by compute the determinant of both sides: $A^2=-I_{n}\Leftrightarrow$ $|A^2|=|-I_{n}|\Leftrightarrow$ $|A|\cdot |A|=(-1)^n|I_{n}|\Leftrightarrow$ $|A|\cdot |A|=(-1)^n$ It's known that $|A|$ is a real number. The product of two equal real numbers is always positive. So $n$ must be even. I can write: $|A|\cdot |A|=1$ To get the previous result there are two possibilities, $|A|=1$ or $|A|=-1$. My doubt is if there is more posibilities. Thanks.",,['linear-algebra']
51,Minimum L1 norm may not obtain the sparsest solution?,Minimum L1 norm may not obtain the sparsest solution?,,"Here is a paper called For Most Large Underdetermined Systems of Equations, the Minimal L1-norm Near-Solution Approximates the Sparsest Near-Solution . However, I did not quite get its definition of sparse, and under what conditions, the minimal L1 norm solution is not the sparsest one. Actually I construct a matrix (in Matlab): A = [1     0     1     1     1     0     0     0     0     0; ...      0     1     1     1     0     1     0     0     0     0;...      0    1/4    0    7/16   0     0     0     0     0     0;...     1/3    0    7/9    0     2     0     0     0     0     0;...     1/9   1/8 119/216  0     0     4     0     0     0     0]; and, b = [9 8 2 3 4]'; According to the paper, the L1 norm is to $min||x||_1$ subject to $||b - Ax||_2 \leq \epsilon$. Suppose its solution is $\hat x_{\epsilon}$. It also mentioned whenever there exists a sparse solution $x_0$, $Ax_0 = b$, and there are at most $(1+M^{-1})/4$ nonzero elements ($M$ is the maximal coherence between any two columns of $A$), then it satisfies, $||\hat x_{\epsilon} - x_0||_2\leq 3\epsilon$ When I tried to solved such L1 norm linear equation ( L1 magic package ), I got the solution [1 0 3.4286 4.5714 0 0 0 0 0 0]'; Yes this is sparser than L2 norm solution: [2 1 3 4 0 0 0 0 0 0]'; But in fact this is also a solution of the original matrix equation: [9 8 0 0 0 0 0 0 0 0]'; and it is sparser. The reason why L1 norm minimization does not pick this vector as the solution is because its L1 norm is larger ($17$ compared with $9$). Did I miss something? I did not find the definition of sparse in the paper, is it possible that my linear equation construction doesn't meet some of the conditions mentioned in the paper? I look forward to hear a reasonable analysis on what's going on with my simulation. Thanks.","Here is a paper called For Most Large Underdetermined Systems of Equations, the Minimal L1-norm Near-Solution Approximates the Sparsest Near-Solution . However, I did not quite get its definition of sparse, and under what conditions, the minimal L1 norm solution is not the sparsest one. Actually I construct a matrix (in Matlab): A = [1     0     1     1     1     0     0     0     0     0; ...      0     1     1     1     0     1     0     0     0     0;...      0    1/4    0    7/16   0     0     0     0     0     0;...     1/3    0    7/9    0     2     0     0     0     0     0;...     1/9   1/8 119/216  0     0     4     0     0     0     0]; and, b = [9 8 2 3 4]'; According to the paper, the L1 norm is to $min||x||_1$ subject to $||b - Ax||_2 \leq \epsilon$. Suppose its solution is $\hat x_{\epsilon}$. It also mentioned whenever there exists a sparse solution $x_0$, $Ax_0 = b$, and there are at most $(1+M^{-1})/4$ nonzero elements ($M$ is the maximal coherence between any two columns of $A$), then it satisfies, $||\hat x_{\epsilon} - x_0||_2\leq 3\epsilon$ When I tried to solved such L1 norm linear equation ( L1 magic package ), I got the solution [1 0 3.4286 4.5714 0 0 0 0 0 0]'; Yes this is sparser than L2 norm solution: [2 1 3 4 0 0 0 0 0 0]'; But in fact this is also a solution of the original matrix equation: [9 8 0 0 0 0 0 0 0 0]'; and it is sparser. The reason why L1 norm minimization does not pick this vector as the solution is because its L1 norm is larger ($17$ compared with $9$). Did I miss something? I did not find the definition of sparse in the paper, is it possible that my linear equation construction doesn't meet some of the conditions mentioned in the paper? I look forward to hear a reasonable analysis on what's going on with my simulation. Thanks.",,"['linear-algebra', 'matrices', 'numerical-methods', 'normed-spaces', 'convex-optimization']"
52,Find the rank of the matrix,Find the rank of the matrix,,"Let $X,Y\in\mathbb R^n$ be two non zero (column) vectors. Let $Y^T$ denote the transpose of Y. Let A = $X Y^T$. What is the rank of $A$ and what is the necessary and sufficient condition for the matrix $B=I-2XX^T$ to be orthogonal","Let $X,Y\in\mathbb R^n$ be two non zero (column) vectors. Let $Y^T$ denote the transpose of Y. Let A = $X Y^T$. What is the rank of $A$ and what is the necessary and sufficient condition for the matrix $B=I-2XX^T$ to be orthogonal",,"['linear-algebra', 'matrices']"
53,Singular value decomposition of an arbitrary anti-symmetric ($A=-A^{T}$) complex matrix,Singular value decomposition of an arbitrary anti-symmetric () complex matrix,A=-A^{T},"I am a physicist and very much used to the fact that any self-adjoint matrix ($H^{\dagger} =H$) in a finite-dimensional complex linear space can be uniquely specified by (a) the set of its (real) eigenvalues, and (b) the unitary matrix built from its (orthonormal) eigenvectors: $$H = U^{\dagger} \cdot \rm{diag}\{ h_1, h_2, \ldots h_n \} \cdot U$$ where $(\cdot)^{\dagger} \equiv (\cdot)^{*T}$ denotes conjugate transpose. I need a generalization of this for the classes of symmetric ($S^{T} =S$) and anti-symmetic  ($A=-A^{T}$) complex matrices. The symmetric case seems easy: $$S = U^{T}\cdot \rm{diag}\{ s_1, s_2, \ldots s_n \} \cdot U$$ where the singular values $s_1, s_2 \dots$ are non-negative reals and $U$  again is a generic the unitary matrix. (Here is a more precise statement , accounting for the extra choice of signs). But I have a difficulty identifying the general singular value decomposition structure for an arbitrary anti-symmetirc matrix . I've observed numerically that the rank of $A$ is at most $n-1$ EDIT: when $n$ is odd ($n$ is the dimensionality of the linear space), so generalization needs to be 'clever'. Can you help?","I am a physicist and very much used to the fact that any self-adjoint matrix ($H^{\dagger} =H$) in a finite-dimensional complex linear space can be uniquely specified by (a) the set of its (real) eigenvalues, and (b) the unitary matrix built from its (orthonormal) eigenvectors: $$H = U^{\dagger} \cdot \rm{diag}\{ h_1, h_2, \ldots h_n \} \cdot U$$ where $(\cdot)^{\dagger} \equiv (\cdot)^{*T}$ denotes conjugate transpose. I need a generalization of this for the classes of symmetric ($S^{T} =S$) and anti-symmetic  ($A=-A^{T}$) complex matrices. The symmetric case seems easy: $$S = U^{T}\cdot \rm{diag}\{ s_1, s_2, \ldots s_n \} \cdot U$$ where the singular values $s_1, s_2 \dots$ are non-negative reals and $U$  again is a generic the unitary matrix. (Here is a more precise statement , accounting for the extra choice of signs). But I have a difficulty identifying the general singular value decomposition structure for an arbitrary anti-symmetirc matrix . I've observed numerically that the rank of $A$ is at most $n-1$ EDIT: when $n$ is odd ($n$ is the dimensionality of the linear space), so generalization needs to be 'clever'. Can you help?",,"['linear-algebra', 'matrices', 'spectral-theory']"
54,Product of positive matrices,Product of positive matrices,,"I have two positive-definite matrices $A$ and $B$ (not necessarily symmetric), and we have $AB=BA$, is there any theorem that ensures that the product of $A$ and $B$, $AB$ is positive definite? Or semi-positive definite?","I have two positive-definite matrices $A$ and $B$ (not necessarily symmetric), and we have $AB=BA$, is there any theorem that ensures that the product of $A$ and $B$, $AB$ is positive definite? Or semi-positive definite?",,"['linear-algebra', 'matrices']"
55,Orthornomal matrices [duplicate],Orthornomal matrices [duplicate],,"This question already has answers here : Orthonormal Matrices-Intuition (5 answers) Closed 10 years ago . Is there a more direct reason for the following: If the columns of $n\times n$ square matrix are orthonormal, then its rows are also orthonormal. The standard proof involves showing that left inverse of a matrix is same as the right inverse and thereby concluding that if $Q^TQ = I$, then $QQ^T = I$. This seems to be more of an algebraic manipulation. Can someone offer me a geometric insight? Thanks","This question already has answers here : Orthonormal Matrices-Intuition (5 answers) Closed 10 years ago . Is there a more direct reason for the following: If the columns of $n\times n$ square matrix are orthonormal, then its rows are also orthonormal. The standard proof involves showing that left inverse of a matrix is same as the right inverse and thereby concluding that if $Q^TQ = I$, then $QQ^T = I$. This seems to be more of an algebraic manipulation. Can someone offer me a geometric insight? Thanks",,"['linear-algebra', 'abstract-algebra', 'euclidean-geometry', 'intuition', 'orthonormal']"
56,Linear dimension of Banach spaces,Linear dimension of Banach spaces,,"Let $X$ be some vector space (over $\mathbb{C}$ ). Note that if $X$ is of finite dimension we can identify $X$ with $\mathbb{C}^n$ for some natural $n$ and endow it with a norm $\|x\|=|x_1|+\ldots+|x_n|$ . Thus $X$ becomes a banach space. However, if $X$ has a countable dimension, we cannot endow it with a banach norm! Really, suppose $\{x_1,x_2,\ldots\}$ is a countable Hamel base in $X$ . It is easy to see that $X_n=\mathrm{Span}(x_1,\ldots, x_n)$ is closed and does not contain any ball. But $X$ is a union of $X_n$ hence $X$ cannot be a full metric space under Baire theorem! So, we see that the linear dimension of banach space cannot be countable. I'd like to ask are there any other cardinal numbers $\mathfrak{K}$ such that no banach space has the linear dimension $\mathfrak{K}$ .","Let be some vector space (over ). Note that if is of finite dimension we can identify with for some natural and endow it with a norm . Thus becomes a banach space. However, if has a countable dimension, we cannot endow it with a banach norm! Really, suppose is a countable Hamel base in . It is easy to see that is closed and does not contain any ball. But is a union of hence cannot be a full metric space under Baire theorem! So, we see that the linear dimension of banach space cannot be countable. I'd like to ask are there any other cardinal numbers such that no banach space has the linear dimension .","X \mathbb{C} X X \mathbb{C}^n n \|x\|=|x_1|+\ldots+|x_n| X X \{x_1,x_2,\ldots\} X X_n=\mathrm{Span}(x_1,\ldots, x_n) X X_n X \mathfrak{K} \mathfrak{K}",['linear-algebra']
57,Orthogonal Procrustes Variant,Orthogonal Procrustes Variant,,"(author note: this question was also asked on mathoverflow ). The orthogonal Procrustes problem seeks a matrix $M$ that minimizes $||AM-B||_F$ subject to $M^TM=I$, where $M$ is $d\times d$ and both $A$ and $B$ are $n\times d$. Geometrically, $M$ rotates a set of $n$ vectors (the rows of $A$) so that they best align with another set of $n$ vectors (the rows of $B$) in a least-squares sense. It has been shown that there is an elegant closed-form solution to this problem, namely $M=UV^T$ where $A^TB=U\Sigma V^T$. I am interested in solving a similar problem and wondering if there is an equally elegant closed-form solution. Given vector sets $A$ and $B$ as before, I would like to find $M$ that makes the vectors in $A$ as ""orthogonal"" as possible to corresponding vectors in $B$. That is, I want $a_i^TMb_i$ to be as close to zero as possible for $i\in[1,n]$, where $a_i$ and $b_i$ represent particular rows of $A$ and $B$: $M^*=\underset{M}{\operatorname{argmin}}\sum_i (a_i^TMb_i)^2$ subject to $M^TM=I$. Is there a name for this formulation? Anyone know of a solution that can be easily expressed (and computed) as a function of $A$ and $B$, or a reason that it cannot?","(author note: this question was also asked on mathoverflow ). The orthogonal Procrustes problem seeks a matrix $M$ that minimizes $||AM-B||_F$ subject to $M^TM=I$, where $M$ is $d\times d$ and both $A$ and $B$ are $n\times d$. Geometrically, $M$ rotates a set of $n$ vectors (the rows of $A$) so that they best align with another set of $n$ vectors (the rows of $B$) in a least-squares sense. It has been shown that there is an elegant closed-form solution to this problem, namely $M=UV^T$ where $A^TB=U\Sigma V^T$. I am interested in solving a similar problem and wondering if there is an equally elegant closed-form solution. Given vector sets $A$ and $B$ as before, I would like to find $M$ that makes the vectors in $A$ as ""orthogonal"" as possible to corresponding vectors in $B$. That is, I want $a_i^TMb_i$ to be as close to zero as possible for $i\in[1,n]$, where $a_i$ and $b_i$ represent particular rows of $A$ and $B$: $M^*=\underset{M}{\operatorname{argmin}}\sum_i (a_i^TMb_i)^2$ subject to $M^TM=I$. Is there a name for this formulation? Anyone know of a solution that can be easily expressed (and computed) as a function of $A$ and $B$, or a reason that it cannot?",,"['linear-algebra', 'optimization', 'procrustes-problem']"
58,Inverse of Sum of Matrix Inverses,Inverse of Sum of Matrix Inverses,,"Given $N$ positive-definite matrices $\Lambda_i$, I need to efficiently compute $\Gamma_N$, where $$ \Gamma_n = \left(\sum_{i=1}^n \Lambda_i^{-1}\right)^{-1}. $$ Applying the Woodbury matrix identity gives a recurrence relation $$ \Gamma_n = \Gamma_{n-1} - \Gamma_{n-1} \left(\sum_{i=1}^n \Lambda_i\right)^{-1} \Gamma_{n-1},  $$ but directly implementing this method would require $N-1$ matrix inversions (or  Cholesky decompositions) to compute $\Gamma_N$. Is there a cleverer way to compute $\Gamma_N$ that is both computationally efficient and numerically stable?","Given $N$ positive-definite matrices $\Lambda_i$, I need to efficiently compute $\Gamma_N$, where $$ \Gamma_n = \left(\sum_{i=1}^n \Lambda_i^{-1}\right)^{-1}. $$ Applying the Woodbury matrix identity gives a recurrence relation $$ \Gamma_n = \Gamma_{n-1} - \Gamma_{n-1} \left(\sum_{i=1}^n \Lambda_i\right)^{-1} \Gamma_{n-1},  $$ but directly implementing this method would require $N-1$ matrix inversions (or  Cholesky decompositions) to compute $\Gamma_N$. Is there a cleverer way to compute $\Gamma_N$ that is both computationally efficient and numerically stable?",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
59,"Perturbation in characteristic p, or Why, really, does Lie's theorem fail?","Perturbation in characteristic p, or Why, really, does Lie's theorem fail?",,"While recalling some basics of Lie theory, I found a funny proof of the main lemma in Lie's theorem on triangularity of representations of solvable Lie algebras. It turns out that this proof has a very clear and intuitive idea (I wonder why they don't explain it to students), which I present below in a nonrigorous way. My question is: how, exactly, can you see from the idea that its formalization requires characteristic 0 (or not less than the dimension, or whatever)? What goes wrong with the suggested geometric picture in characteristic $p$? Or what should be added to the informal language I use to make this explicit? Of course I also have a formal version of it, involving characteristic polynomials, and it does spit out some constants that have to be nonzero in the end, but I'm asking for a geometric explanation. So here it goes. Let $\mathfrak{h} \subset \mathfrak{g}$ be an ideal in a Lie algebra, let $V$ be a finite-dimensional representation of $\mathfrak{g}$, and let $\lambda$ be a weight of its restriction to $\mathfrak{h}$. Then the corresponding weight space $V_\lambda$ is $\mathfrak{g}$-invariant. ""Proof"". Clearly everything boils down to showing that $\langle \lambda, [g,h] \rangle = 0$ for all $g \in \mathfrak{g}, h \in \mathfrak{h}$. Now let $\langle \lambda, h\rangle$ be an eigenvalue of $h$ of multiplicity at least $m$, for all $h \in \mathfrak{h}$. Let's extend our ring of scalars by a nilpotent $\epsilon^2 = 0$ to make an infinitesimal perturbation. On the one hand, $h + \epsilon [g,h]$ is still in $\mathfrak{h}$ (or rather in its extension), since it is an ideal. So it has an eigenvalue $\langle \lambda, h \rangle + \epsilon \langle \lambda, [g,h] \rangle$ of multiplicity $\ge m$. On the other hand, $h + \epsilon [g, h]$ is conjugate to $h$ (i.e. $h + \epsilon [g, h] = (1 + \epsilon g) h (1 + \epsilon g)^{-1}$), thus it must have the same spectrum as $h$ does. Therefore, in addition to having an eigenvalue $\langle \lambda, h \rangle + \epsilon \langle \lambda, [g,h] \rangle$ it also has an eigenvalue $\langle \lambda, h \rangle$, both of multiplicity $\ge m$. Therefore, either $\langle \lambda, [g, h] \rangle = 0$ for all $h$ (in which case there is nothing left to prove), or $h$ must have had the eigenvalue $\lambda$ with multiplicity $\ge m+1$. Moreover, this actually happens for all $h$. Now run the same argument for $m+1$ instead of $m$, and at some point you will exceed the dimension.","While recalling some basics of Lie theory, I found a funny proof of the main lemma in Lie's theorem on triangularity of representations of solvable Lie algebras. It turns out that this proof has a very clear and intuitive idea (I wonder why they don't explain it to students), which I present below in a nonrigorous way. My question is: how, exactly, can you see from the idea that its formalization requires characteristic 0 (or not less than the dimension, or whatever)? What goes wrong with the suggested geometric picture in characteristic $p$? Or what should be added to the informal language I use to make this explicit? Of course I also have a formal version of it, involving characteristic polynomials, and it does spit out some constants that have to be nonzero in the end, but I'm asking for a geometric explanation. So here it goes. Let $\mathfrak{h} \subset \mathfrak{g}$ be an ideal in a Lie algebra, let $V$ be a finite-dimensional representation of $\mathfrak{g}$, and let $\lambda$ be a weight of its restriction to $\mathfrak{h}$. Then the corresponding weight space $V_\lambda$ is $\mathfrak{g}$-invariant. ""Proof"". Clearly everything boils down to showing that $\langle \lambda, [g,h] \rangle = 0$ for all $g \in \mathfrak{g}, h \in \mathfrak{h}$. Now let $\langle \lambda, h\rangle$ be an eigenvalue of $h$ of multiplicity at least $m$, for all $h \in \mathfrak{h}$. Let's extend our ring of scalars by a nilpotent $\epsilon^2 = 0$ to make an infinitesimal perturbation. On the one hand, $h + \epsilon [g,h]$ is still in $\mathfrak{h}$ (or rather in its extension), since it is an ideal. So it has an eigenvalue $\langle \lambda, h \rangle + \epsilon \langle \lambda, [g,h] \rangle$ of multiplicity $\ge m$. On the other hand, $h + \epsilon [g, h]$ is conjugate to $h$ (i.e. $h + \epsilon [g, h] = (1 + \epsilon g) h (1 + \epsilon g)^{-1}$), thus it must have the same spectrum as $h$ does. Therefore, in addition to having an eigenvalue $\langle \lambda, h \rangle + \epsilon \langle \lambda, [g,h] \rangle$ it also has an eigenvalue $\langle \lambda, h \rangle$, both of multiplicity $\ge m$. Therefore, either $\langle \lambda, [g, h] \rangle = 0$ for all $h$ (in which case there is nothing left to prove), or $h$ must have had the eigenvalue $\lambda$ with multiplicity $\ge m+1$. Moreover, this actually happens for all $h$. Now run the same argument for $m+1$ instead of $m$, and at some point you will exceed the dimension.",,"['linear-algebra', 'lie-algebras', 'infinitesimals', 'positive-characteristic']"
60,Towards a Quantum Peter Weyl Theorem,Towards a Quantum Peter Weyl Theorem,,"This is taken from Timmermann's Invitation to Quantum Groups and Duality . Let $(A,\Delta)$ be a *-Hopf algebra and let $\chi:V\rightarrow V\otimes A$ be a corepresentation of $(A,\Delta)$ on a vector space $V$. Call a vector subspace $W\subset V$ invariant with respect to $\chi$ if $\chi(W)\subset W\otimes A$. Question If $\chi$ is unitary and $W\subset V$ is an invariant subspace, then the orthogonal complement $W^\perp$ is invariant also. Timmermann proceeds as follows. Define a map from the dual of $A$, $\pi:A'\rightarrow \hom(V)$ by $$\pi(f)v:=(I\otimes f)\chi(v).$$ Timmerman proves that $W\subset V$ is invariant with respect to $\chi$ if and only if $\pi(A')W\subset W$. Now he simply states ""Now the claim follows from a standard argument."" The problem is that I don't see what this standard argument is. Initially I said suppose that $\tilde{w}\in W^\perp$ so that $\langle\tilde{w},w\rangle=0$. Then I calculated $$\langle \pi(f)\tilde{w},w\rangle=\sum\langle {\tilde{w}}_{(0)}f(\tilde{w}_{(1)}),w\rangle$$ where the Sweedler notation is used. This didn't really go anywhere. The next idea was to say that $\pi$ was non-degenerate and use Timmermann's result that $\pi$ is an algebra homomorphism of $A'$ with the multiplication/convolution $(f,g)\mapsto (f\otimes g)\Delta$. The next part was what do inverses of $f$ look like here? $$(f\otimes f^{-1})\Delta(a)=1?$$ If I could do something like this I could certainly carry out a standard argument (familiar to me from the classical Peter Weyl Theorem). Let $\tilde{w}\in W^\perp$ and $w\in W$: $$\langle \pi(f)\tilde{w},w\rangle=\langle \pi(f^{-1})\pi(f)\tilde{w},\pi(f^{-1})w\rangle$$ but I can't quite justify those calculations. Can anybody out there help?","This is taken from Timmermann's Invitation to Quantum Groups and Duality . Let $(A,\Delta)$ be a *-Hopf algebra and let $\chi:V\rightarrow V\otimes A$ be a corepresentation of $(A,\Delta)$ on a vector space $V$. Call a vector subspace $W\subset V$ invariant with respect to $\chi$ if $\chi(W)\subset W\otimes A$. Question If $\chi$ is unitary and $W\subset V$ is an invariant subspace, then the orthogonal complement $W^\perp$ is invariant also. Timmermann proceeds as follows. Define a map from the dual of $A$, $\pi:A'\rightarrow \hom(V)$ by $$\pi(f)v:=(I\otimes f)\chi(v).$$ Timmerman proves that $W\subset V$ is invariant with respect to $\chi$ if and only if $\pi(A')W\subset W$. Now he simply states ""Now the claim follows from a standard argument."" The problem is that I don't see what this standard argument is. Initially I said suppose that $\tilde{w}\in W^\perp$ so that $\langle\tilde{w},w\rangle=0$. Then I calculated $$\langle \pi(f)\tilde{w},w\rangle=\sum\langle {\tilde{w}}_{(0)}f(\tilde{w}_{(1)}),w\rangle$$ where the Sweedler notation is used. This didn't really go anywhere. The next idea was to say that $\pi$ was non-degenerate and use Timmermann's result that $\pi$ is an algebra homomorphism of $A'$ with the multiplication/convolution $(f,g)\mapsto (f\otimes g)\Delta$. The next part was what do inverses of $f$ look like here? $$(f\otimes f^{-1})\Delta(a)=1?$$ If I could do something like this I could certainly carry out a standard argument (familiar to me from the classical Peter Weyl Theorem). Let $\tilde{w}\in W^\perp$ and $w\in W$: $$\langle \pi(f)\tilde{w},w\rangle=\langle \pi(f^{-1})\pi(f)\tilde{w},\pi(f^{-1})w\rangle$$ but I can't quite justify those calculations. Can anybody out there help?",,"['linear-algebra', 'representation-theory', 'quantum-groups']"
61,Maximizing the smallest eigenvalue of a linear combination of matrices,Maximizing the smallest eigenvalue of a linear combination of matrices,,"I have an engineering background. At work, I came across the following problem \begin{align} &\max_{\lambda,y_i\in \mathbb{R}}~\lambda \\\  s.t.~&\left(\mathbf{A}_0+\sum_{i=1}^{K}y_i\mathbf{A}_i\right)-\lambda\mathbf{I} \succeq 0 \end{align} where $\mathbf{A}_i$ are all Hermitian matrices. I know that this is called a Linear Matrix Inequality (LMI) problem and can be solved by a general convex package (e.g., CVX). To me, it seems like we are looking for a matrix formed from the linear combination of given Hermitian matrices whose smallest eigenvalue is as maximum as possible among all such combinations. I was wondering if they are iterative algorithms to solve this problem which are simple to implement. Please point me to relevant references.","I have an engineering background. At work, I came across the following problem \begin{align} &\max_{\lambda,y_i\in \mathbb{R}}~\lambda \\\  s.t.~&\left(\mathbf{A}_0+\sum_{i=1}^{K}y_i\mathbf{A}_i\right)-\lambda\mathbf{I} \succeq 0 \end{align} where $\mathbf{A}_i$ are all Hermitian matrices. I know that this is called a Linear Matrix Inequality (LMI) problem and can be solved by a general convex package (e.g., CVX). To me, it seems like we are looking for a matrix formed from the linear combination of given Hermitian matrices whose smallest eigenvalue is as maximum as possible among all such combinations. I was wondering if they are iterative algorithms to solve this problem which are simple to implement. Please point me to relevant references.",,"['linear-algebra', 'matrices', 'convex-analysis', 'convex-optimization', 'semidefinite-programming']"
62,How do I prove that in every commuting family there is a common eigenvector?,How do I prove that in every commuting family there is a common eigenvector?,,"The proof given by my textbook is highly non-satisfying. The author adopted some magic-like ""reductio ad absurdum"" and the proof (although is correct) didn't reveal the nature of this problem. I made my own effort into it and tried a different approach. Yet I can't finish it. Let $\mathscr{F}$ be a commuting family in $M_n(\mathbb{C}^n)$, and $A\in\mathscr{F}$, then $A$ has $n$ eigenvalues. We pick one, say $\lambda$. Let $x$ be one of its eigenvector. We can easily prove that, if $A$ has no other eigenvector with eigenvalue $\lambda$ that linearly independent with $x$, which means that $\{cx|c\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$, then $x$ is a common eigenvector. Because $\forall B\in\mathscr{F}$ and $\forall y\in \{cx|c\in\mathbb{C}\}$, $$A(Bx)=ABx=BAx=B(Ax)=B(\lambda x)=\lambda (Bx)$$, so that $Bx$ has to be in $\{cx|c\in\mathbb{C}\}$, that is, $Bx=c_0x$ for some $c_0\in\mathbb{C}$, which means $x$ is a eigenvector of $B$ too. But what if there are vectors satisfying $Ax=\lambda x$ that's not in  $\{cx|c\in\mathbb{C}\}$? Well, then we should have a set of linearly independent eigenvectors $\{x_1,x_2,...,x_k\}$, that $\{c_1x_1+c_2x_2+...+c_kx_k|c_i\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$. Now, I have a reasonable hypothesis that there exists some $x=c_1x_1+c_2x_2+...+c_kx_k$, that can be proven to be a common eigenvector of $\mathscr{F}$. I've tried some approaches to prove it but all failed. Do you guys believe it's true? And if it is true then how do I prove it?","The proof given by my textbook is highly non-satisfying. The author adopted some magic-like ""reductio ad absurdum"" and the proof (although is correct) didn't reveal the nature of this problem. I made my own effort into it and tried a different approach. Yet I can't finish it. Let $\mathscr{F}$ be a commuting family in $M_n(\mathbb{C}^n)$, and $A\in\mathscr{F}$, then $A$ has $n$ eigenvalues. We pick one, say $\lambda$. Let $x$ be one of its eigenvector. We can easily prove that, if $A$ has no other eigenvector with eigenvalue $\lambda$ that linearly independent with $x$, which means that $\{cx|c\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$, then $x$ is a common eigenvector. Because $\forall B\in\mathscr{F}$ and $\forall y\in \{cx|c\in\mathbb{C}\}$, $$A(Bx)=ABx=BAx=B(Ax)=B(\lambda x)=\lambda (Bx)$$, so that $Bx$ has to be in $\{cx|c\in\mathbb{C}\}$, that is, $Bx=c_0x$ for some $c_0\in\mathbb{C}$, which means $x$ is a eigenvector of $B$ too. But what if there are vectors satisfying $Ax=\lambda x$ that's not in  $\{cx|c\in\mathbb{C}\}$? Well, then we should have a set of linearly independent eigenvectors $\{x_1,x_2,...,x_k\}$, that $\{c_1x_1+c_2x_2+...+c_kx_k|c_i\in\mathbb{C}\}$ are the only vectors satisfying $Ax=\lambda x$. Now, I have a reasonable hypothesis that there exists some $x=c_1x_1+c_2x_2+...+c_kx_k$, that can be proven to be a common eigenvector of $\mathscr{F}$. I've tried some approaches to prove it but all failed. Do you guys believe it's true? And if it is true then how do I prove it?",,['matrices']
63,"What does ""generic"" mean in this context, and is it related to generic points in algebraic geometry?","What does ""generic"" mean in this context, and is it related to generic points in algebraic geometry?",,"In ""The characteristic polynomial and determinant are not ad hoc constructions"" by Skip Garibaldi, available at http://arxiv.org/abs/math/0203276 the characteristic polynomial is defined as the minimal polynomial of a ""generic element.""  Specifically, this is the setup: Let $F$ be a field and $A$ a finite-dimensional $F$-algebra. Let $\{a_1, \ldots, a_m\}$ be an $F$-basis for $A$. Let $R = F[t_1, \ldots, t_m]$ and $K = \operatorname{Frac} R = F(t_1, > \ldots, t_m)$. Consider $\gamma = \sum_i a_i \otimes t_i \in A \otimes_F K$, which we   call a generic element . Since $A \otimes_F K$ is a finite-dimensional $K$-vector space,   $\operatorname{span} \{1, \gamma, \gamma^2, \ldots \}$ is   finite-dimensional, so $\gamma$ has a minimal polynomial $m \in K[x]$,   and it's easy to see that in fact $m \in R[x]$.  We define the   characteristic polynomial of $a = \sum_i c_i a_i \in A$ to be the   image of $m$ under the map $R[x] \to F[x]$ given by $t_i \mapsto c_i$. This is a nice argument, but it still feels a bit ad-hoc to me: we have to explicitly calculate that doesn't rely on the choice of $F$-basis, for instance, and why is $\gamma$ specifically a ""generic"" element of $A \otimes_F K$? I mean, I understand the idea of taking a matrix filled with indeterminates and computing its minimal polynomial, but it doesn't seem to be a natural thing to do from a purely algebraic perspective. Is there some way to rephrase this argument so that we can view $A$ as having a scheme structure and to regard $m$ as the ""minimal polynomial of the generic point"" or something of the sort?  I've tried a couple things but just get to a point where it seems like we're commingling elements of a -- is it called an ""algebra scheme?"" -- together with functions on it, and I get confused by the whole thing.  But that could just be my inexperience.","In ""The characteristic polynomial and determinant are not ad hoc constructions"" by Skip Garibaldi, available at http://arxiv.org/abs/math/0203276 the characteristic polynomial is defined as the minimal polynomial of a ""generic element.""  Specifically, this is the setup: Let $F$ be a field and $A$ a finite-dimensional $F$-algebra. Let $\{a_1, \ldots, a_m\}$ be an $F$-basis for $A$. Let $R = F[t_1, \ldots, t_m]$ and $K = \operatorname{Frac} R = F(t_1, > \ldots, t_m)$. Consider $\gamma = \sum_i a_i \otimes t_i \in A \otimes_F K$, which we   call a generic element . Since $A \otimes_F K$ is a finite-dimensional $K$-vector space,   $\operatorname{span} \{1, \gamma, \gamma^2, \ldots \}$ is   finite-dimensional, so $\gamma$ has a minimal polynomial $m \in K[x]$,   and it's easy to see that in fact $m \in R[x]$.  We define the   characteristic polynomial of $a = \sum_i c_i a_i \in A$ to be the   image of $m$ under the map $R[x] \to F[x]$ given by $t_i \mapsto c_i$. This is a nice argument, but it still feels a bit ad-hoc to me: we have to explicitly calculate that doesn't rely on the choice of $F$-basis, for instance, and why is $\gamma$ specifically a ""generic"" element of $A \otimes_F K$? I mean, I understand the idea of taking a matrix filled with indeterminates and computing its minimal polynomial, but it doesn't seem to be a natural thing to do from a purely algebraic perspective. Is there some way to rephrase this argument so that we can view $A$ as having a scheme structure and to regard $m$ as the ""minimal polynomial of the generic point"" or something of the sort?  I've tried a couple things but just get to a point where it seems like we're commingling elements of a -- is it called an ""algebra scheme?"" -- together with functions on it, and I get confused by the whole thing.  But that could just be my inexperience.",,"['linear-algebra', 'algebraic-geometry']"
64,Matrices Intuition,Matrices Intuition,,"I am currently studying matrix algebra. The axioms and theorems of this form of algebra are a bit different from the high school algebra I did. However one knows that one is dealing with real numbers in that form of algebra and one can always associate a physical object with a number to see if a particular theorems makes sense. For example, x-2=0 , find x. Now one can associate 2 and x with apples and see the operation '-' as eating apples and then having 2 apples and eaten x apples you get 0 apples, hence x must be 2 (apples) in this case. Similarly is there anything physical that I can associate with a matrix so that I can call upon my intuition whenever confronted with a new theorem/result instead of blindly following algebra rules and axioms associated with a matrix. Any help in this regard will be appreciated. In short: Please associate a common physical object that obeys all the rules of algebra of a matrix.","I am currently studying matrix algebra. The axioms and theorems of this form of algebra are a bit different from the high school algebra I did. However one knows that one is dealing with real numbers in that form of algebra and one can always associate a physical object with a number to see if a particular theorems makes sense. For example, x-2=0 , find x. Now one can associate 2 and x with apples and see the operation '-' as eating apples and then having 2 apples and eaten x apples you get 0 apples, hence x must be 2 (apples) in this case. Similarly is there anything physical that I can associate with a matrix so that I can call upon my intuition whenever confronted with a new theorem/result instead of blindly following algebra rules and axioms associated with a matrix. Any help in this regard will be appreciated. In short: Please associate a common physical object that obeys all the rules of algebra of a matrix.",,"['linear-algebra', 'algebra-precalculus', 'matrices', 'intuition']"
65,Confirmation of correctness in proof regarding norm preserving operator,Confirmation of correctness in proof regarding norm preserving operator,,"I just want to know if my solutions are correct for the following problem (euclidean norm assumed): A linear transformation $T:\Bbb R^n \to \Bbb R^n$ is norm preserving if $|T(x)| = |x|$ for all $x \in \Bbb R^n$, and inner product preserving if $\left\langle T(x),T(y)\right\rangle = \left\langle x, y\right\rangle$. $(a)$ Prove that $T$ is norm preserving if and only if $T$ is inner product preserving. $(b)$ Prove that such a linear transformation is $1$-$1$ and $T^{-1}$ is of the same sort. My solution for $(a)$ was: suppose $T$ is inner product preserving, then since $|x|=\sqrt{\left\langle x,x \right\rangle}$ we have that $|T(x)| = \sqrt{\left\langle T(x),T(x)\right\rangle}=\sqrt{\left\langle x, x\right\rangle} = |x|$ so that $T$ is norm preserving. Suppose now that $T$ is norm preserving, so we have the polarization identity: $$\left\langle T(x), T(y)\right\rangle=\frac{|T(x)+T(y)|^2 - |T(x)-T(y)|^2}{4}$$ But by virtue of linearity $T(x)+T(y) = T(x+y)$ and $T(x)-T(y)=T(x-y)$ and so since $T$ preserves norms we have $\left\langle T(x),T(y)\right\rangle = \left\langle x,y\right\rangle$ so that $T$ is inner product preserving also. My solution for $(b)$ was: Suppose that $T(x) = 0$, then we have $|T(x)| = 0$, but since $T$ is norm preserving $|x| = 0$ and this implies $x = 0$ so that $T$ is injective. Also by the rank-nullity theorem $T$ is surjective because $\dim \ker T = 0$ and so $\dim \operatorname{Im} T = \dim \Bbb R^n$. Now, because of that, given $y \in \Bbb R^n$ there's some $x \in \Bbb R^n$ such that $ y = T(x)$. In that case, consider the norm of $T^{-1}(y)$, because of the analysis that there's some $x$ such that $y= T(x)$ we will have: $$T^{-1}(y) = x$$ Now recalling that $T$ preserves norms, we have that $|x| = |T(x)|$, but $T(x) = y$ so that $|x| = |y|$, so taking the norm on the above equation we finally have: $$\left|T^{-1}(y)\right| = |x| = |T(x)| = |y|$$ Are my solutions correct? Thanks very much in advance!","I just want to know if my solutions are correct for the following problem (euclidean norm assumed): A linear transformation $T:\Bbb R^n \to \Bbb R^n$ is norm preserving if $|T(x)| = |x|$ for all $x \in \Bbb R^n$, and inner product preserving if $\left\langle T(x),T(y)\right\rangle = \left\langle x, y\right\rangle$. $(a)$ Prove that $T$ is norm preserving if and only if $T$ is inner product preserving. $(b)$ Prove that such a linear transformation is $1$-$1$ and $T^{-1}$ is of the same sort. My solution for $(a)$ was: suppose $T$ is inner product preserving, then since $|x|=\sqrt{\left\langle x,x \right\rangle}$ we have that $|T(x)| = \sqrt{\left\langle T(x),T(x)\right\rangle}=\sqrt{\left\langle x, x\right\rangle} = |x|$ so that $T$ is norm preserving. Suppose now that $T$ is norm preserving, so we have the polarization identity: $$\left\langle T(x), T(y)\right\rangle=\frac{|T(x)+T(y)|^2 - |T(x)-T(y)|^2}{4}$$ But by virtue of linearity $T(x)+T(y) = T(x+y)$ and $T(x)-T(y)=T(x-y)$ and so since $T$ preserves norms we have $\left\langle T(x),T(y)\right\rangle = \left\langle x,y\right\rangle$ so that $T$ is inner product preserving also. My solution for $(b)$ was: Suppose that $T(x) = 0$, then we have $|T(x)| = 0$, but since $T$ is norm preserving $|x| = 0$ and this implies $x = 0$ so that $T$ is injective. Also by the rank-nullity theorem $T$ is surjective because $\dim \ker T = 0$ and so $\dim \operatorname{Im} T = \dim \Bbb R^n$. Now, because of that, given $y \in \Bbb R^n$ there's some $x \in \Bbb R^n$ such that $ y = T(x)$. In that case, consider the norm of $T^{-1}(y)$, because of the analysis that there's some $x$ such that $y= T(x)$ we will have: $$T^{-1}(y) = x$$ Now recalling that $T$ preserves norms, we have that $|x| = |T(x)|$, but $T(x) = y$ so that $|x| = |y|$, so taking the norm on the above equation we finally have: $$\left|T^{-1}(y)\right| = |x| = |T(x)| = |y|$$ Are my solutions correct? Thanks very much in advance!",,['linear-algebra']
66,Positive and unitary operator over a finite dimensional vector space,Positive and unitary operator over a finite dimensional vector space,,"I have got the below question as part of an homework assignment: Let $T$ be a linear operator on the finite-dimensional inner product space $V$, and suppose $T$ is both positive and unitary. Prove $T = I$. I tried to prove it the following way: $T$ is normal $\Rightarrow V$ has an orthonormal basis consisting of characteristic vectors of $T$ $T$ is unitary $\Rightarrow$ every characteristic value $|\lambda| = 1$. To get a contradiction, assume that there exists some characteristic value $\lambda = -1$. $\Rightarrow$  there is a vector $v \in V $ that $T(v) = -v$ and  $v$ is not zero looking on the inner product: $\langle T(v),v\rangle \  =\  \langle -v,v\rangle \  =\ -\langle v,v\rangle $ and $ \ \langle v,v\rangle \  = \ ||v||^2 > 0$  so $\ \langle T(v), v\rangle \  < \ 0$ and this contradicts the fact that $T$ is positive. $\Rightarrow$ all the characteristic values $\lambda_i = 1$. Can anyone suggest another way of proving it? I'd love to get some feedback too.","I have got the below question as part of an homework assignment: Let $T$ be a linear operator on the finite-dimensional inner product space $V$, and suppose $T$ is both positive and unitary. Prove $T = I$. I tried to prove it the following way: $T$ is normal $\Rightarrow V$ has an orthonormal basis consisting of characteristic vectors of $T$ $T$ is unitary $\Rightarrow$ every characteristic value $|\lambda| = 1$. To get a contradiction, assume that there exists some characteristic value $\lambda = -1$. $\Rightarrow$  there is a vector $v \in V $ that $T(v) = -v$ and  $v$ is not zero looking on the inner product: $\langle T(v),v\rangle \  =\  \langle -v,v\rangle \  =\ -\langle v,v\rangle $ and $ \ \langle v,v\rangle \  = \ ||v||^2 > 0$  so $\ \langle T(v), v\rangle \  < \ 0$ and this contradicts the fact that $T$ is positive. $\Rightarrow$ all the characteristic values $\lambda_i = 1$. Can anyone suggest another way of proving it? I'd love to get some feedback too.",,['linear-algebra']
67,When can two linear operators on a finite-dimensional space be simultaneously Jordanized?,When can two linear operators on a finite-dimensional space be simultaneously Jordanized?,,"IN a comment to Qiaochu's answer here it is mentioned that two commuting matrices can be simultaneously Jordanized (sorry that this sounds less appealing then ""diagonalized"" :P ), i.e. can be brought to a Jordan normal form by the same similarity transformation. I was wondering about the converse - when can two linear operators acting on a finite-dimensional vector space (over an algebraically closed field) be simultaneously Jordanized? Unlike the case of simultaneous diagonalization, I don't think commutativity is forced on the transformations in this case, and I'm interested in other natural conditions which guarantee that this is possible. EDIT: as Georges pointed out, the statements that two commuting matrices are simultaneously Jordanizable is in fact wrong. Nevertheless, I am still interested in interesting conditions on a pair of operators which ensures a simultaneous Jordanization (of course, there are some obvious sufficient conditions, i.e. that the two matrices are actually diagonalizable and commute, but this is not very appealing...)","IN a comment to Qiaochu's answer here it is mentioned that two commuting matrices can be simultaneously Jordanized (sorry that this sounds less appealing then ""diagonalized"" :P ), i.e. can be brought to a Jordan normal form by the same similarity transformation. I was wondering about the converse - when can two linear operators acting on a finite-dimensional vector space (over an algebraically closed field) be simultaneously Jordanized? Unlike the case of simultaneous diagonalization, I don't think commutativity is forced on the transformations in this case, and I'm interested in other natural conditions which guarantee that this is possible. EDIT: as Georges pointed out, the statements that two commuting matrices are simultaneously Jordanizable is in fact wrong. Nevertheless, I am still interested in interesting conditions on a pair of operators which ensures a simultaneous Jordanization (of course, there are some obvious sufficient conditions, i.e. that the two matrices are actually diagonalizable and commute, but this is not very appealing...)",,"['linear-algebra', 'linear-transformations', 'jordan-normal-form']"
68,Linear least squares: overdetermined system necessary? and finding solutions?,Linear least squares: overdetermined system necessary? and finding solutions?,,"The wikipedia article on linear least squares only considers overdetermined systems (rows $\geq$ columns). I'm confused if this assumption is really necessary or not. Given any matrix $A, \|Ax - b\|^2$ is convex and differentiable so the minimum is given by solving the normal equations $A^T Ax = A^T b$. If $A$ has full rank then $A^TA$ is invertible and a unique $x$ exists. If $A$ does not have full rank, then I still know that there is a unique $z \in \operatorname{range}(A)$ that minimizes the least squares problem by the projection theorem. So I have some $x_0$ such that $z = Ax_0$. Question 1: In the case that $A$ does not have full rank, the projection theorem gives the existence of a minimizer $x_0$. How can I prove the existence of $x_0$ directly from the normal equations $A^TAx = A^Tb$? aPriori, it doesn't appear that this system needs to be consistent. Now that I have $x_0$ as a solution, I want to find all other solutions. From my differential equations class, I would guess that all other minimizers are given by $x_0+\ker(A)$. To prove this (I think) I suppose that $y$ is any other minimizer. Then by the uniqueness of $z$, I have $Ay = z$. And so $Ay - Ax_0 = 0$ and $y-x_0 \in \ker(A)$, which says $y\in x_0 + \ker(A)$. However, these two observations look contradictory: Suppose A has full rank and a non trivial kernel (overdetermined and full rank). The second observation says that $y\in x_0 + \ker(A)$ is a minimizer, indeed, $\|Ay - b\| = \|A(x_0+x) -b\| = \|Ax_0 -b\| =$ minimum. However, $A^TA$ is invertible, so the normal equations say that $x_0$ is unique and given by $x_0 = (A^TA)^{-1}A^Tb.$ Question 2: When $A$ has full rank and a nontrivial kernel, the normal equations says the minimizer $x_0 := (A^TA)^{-1}A^Tb$ is unique. Direct verification shows this is not the case, as any $x\in x_0 + \ker(A)$ is a minimizer. Where is the error in my reasoning that led me to this paradox?","The wikipedia article on linear least squares only considers overdetermined systems (rows $\geq$ columns). I'm confused if this assumption is really necessary or not. Given any matrix $A, \|Ax - b\|^2$ is convex and differentiable so the minimum is given by solving the normal equations $A^T Ax = A^T b$. If $A$ has full rank then $A^TA$ is invertible and a unique $x$ exists. If $A$ does not have full rank, then I still know that there is a unique $z \in \operatorname{range}(A)$ that minimizes the least squares problem by the projection theorem. So I have some $x_0$ such that $z = Ax_0$. Question 1: In the case that $A$ does not have full rank, the projection theorem gives the existence of a minimizer $x_0$. How can I prove the existence of $x_0$ directly from the normal equations $A^TAx = A^Tb$? aPriori, it doesn't appear that this system needs to be consistent. Now that I have $x_0$ as a solution, I want to find all other solutions. From my differential equations class, I would guess that all other minimizers are given by $x_0+\ker(A)$. To prove this (I think) I suppose that $y$ is any other minimizer. Then by the uniqueness of $z$, I have $Ay = z$. And so $Ay - Ax_0 = 0$ and $y-x_0 \in \ker(A)$, which says $y\in x_0 + \ker(A)$. However, these two observations look contradictory: Suppose A has full rank and a non trivial kernel (overdetermined and full rank). The second observation says that $y\in x_0 + \ker(A)$ is a minimizer, indeed, $\|Ay - b\| = \|A(x_0+x) -b\| = \|Ax_0 -b\| =$ minimum. However, $A^TA$ is invertible, so the normal equations say that $x_0$ is unique and given by $x_0 = (A^TA)^{-1}A^Tb.$ Question 2: When $A$ has full rank and a nontrivial kernel, the normal equations says the minimizer $x_0 := (A^TA)^{-1}A^Tb$ is unique. Direct verification shows this is not the case, as any $x\in x_0 + \ker(A)$ is a minimizer. Where is the error in my reasoning that led me to this paradox?",,['linear-algebra']
69,Practical question about a fixpoint,Practical question about a fixpoint,,"Let $c\in \Bbb R^n$ be some fixed vector and $A\in \Bbb R^{n\times n}$ be a fixed matrix with non-negative elements. Consider the map $f:\Bbb R^n\to\Bbb R^n$ given by $$   f^i(x^1,\dots,x^n) =    \begin{cases}     1,&x^i\geq c^i \\     0, &x^i<c^i.   \end{cases} $$ By abusing notation, why may think that $f$ is a vector-valued indicator function $f(x) = 1_{\{x\geq c\}}$. Consider further the following dynamics: $$   x_{k+1} = Af(x_{k}). \tag{1} $$ Clearly, not matter where we start, it takes only finitely many iterations (at most $n$) to converge to a fixpoint of $Af(\cdot)$. I wonder, however, whether there is a handy formula for such a fixpoint given the initial value $x_0$. Please, feel free to retag. Updated: let me elaborate on what I know. Denote $F(x) = Af(x)$. Note that since $A$ has non-negative elements only, it holds that $x_{k+1}\leq x_{k}$ for $k\geq 1$ where the inequality shall be understood as an element-wise one. Since $f$ is a monotone function, $f(x_{k+1})\leq f(x_{k})$ as well, thus there are two cases. If $f(x_{k+1}) = f(x_k)$ then  $$    F(x_{k+1}) = Af(x_{k+1}) = Af(x_k) = x_{k+1}  $$ and thus $x_{k+1}\in \mathsf{Fix}(F)$. It $f(x_{k+1})\neq f(x_k)$, then for some $1\leq i\leq n$ it holds that $f^i(x_k) =1$ but $f^i(x_{k+1}) = 0$. Since $f$ most $n$ components, the case 2. can happen at most $n$ times. In fact, I know that  $$   \mathsf{Fix}(F|x_0) = F^m(x_0) $$ where $m$ is a number of non-zero components of $f(x_0)$. I wonder, though if there is a better formula.","Let $c\in \Bbb R^n$ be some fixed vector and $A\in \Bbb R^{n\times n}$ be a fixed matrix with non-negative elements. Consider the map $f:\Bbb R^n\to\Bbb R^n$ given by $$   f^i(x^1,\dots,x^n) =    \begin{cases}     1,&x^i\geq c^i \\     0, &x^i<c^i.   \end{cases} $$ By abusing notation, why may think that $f$ is a vector-valued indicator function $f(x) = 1_{\{x\geq c\}}$. Consider further the following dynamics: $$   x_{k+1} = Af(x_{k}). \tag{1} $$ Clearly, not matter where we start, it takes only finitely many iterations (at most $n$) to converge to a fixpoint of $Af(\cdot)$. I wonder, however, whether there is a handy formula for such a fixpoint given the initial value $x_0$. Please, feel free to retag. Updated: let me elaborate on what I know. Denote $F(x) = Af(x)$. Note that since $A$ has non-negative elements only, it holds that $x_{k+1}\leq x_{k}$ for $k\geq 1$ where the inequality shall be understood as an element-wise one. Since $f$ is a monotone function, $f(x_{k+1})\leq f(x_{k})$ as well, thus there are two cases. If $f(x_{k+1}) = f(x_k)$ then  $$    F(x_{k+1}) = Af(x_{k+1}) = Af(x_k) = x_{k+1}  $$ and thus $x_{k+1}\in \mathsf{Fix}(F)$. It $f(x_{k+1})\neq f(x_k)$, then for some $1\leq i\leq n$ it holds that $f^i(x_k) =1$ but $f^i(x_{k+1}) = 0$. Since $f$ most $n$ components, the case 2. can happen at most $n$ times. In fact, I know that  $$   \mathsf{Fix}(F|x_0) = F^m(x_0) $$ where $m$ is a number of non-zero components of $f(x_0)$. I wonder, though if there is a better formula.",,"['real-analysis', 'linear-algebra']"
70,numerical linear algebra tricks for repeated sums and inversions with symmetric positive-definite matrices,numerical linear algebra tricks for repeated sums and inversions with symmetric positive-definite matrices,,"I'm doing the following procedure to get the max-likelihood estimate of a matrix-variate normal distribution from $r$ samples of matrices in $\mathbb{R}^{n \times p}$ (algorithm from Dutilleul (1999) ): $$ \begin{align} V &= \frac{1}{r n} \sum_{i=1}^r X_i^T U^{-1} X_i \\ U &= \frac{1}{r p} \sum_{i=1}^r X_i V^{-1} X_i^T \end{align} $$ and repeating until convergence. ($U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$). I need to do this for moderately large $n$ and $p$ (on the order of 1,000 each), with somewhat smaller $r$ (order of 200), and I need to do it over and over and over again, so any algorithmic speed increases would be very handy. That paper didn't really talk about the practical implementation of the algorithm, though. Since $U$ and $V$ are symmetric positive-definite, it seems that there should be some trick smarter than just doing the sums as written (using a solve call rather than an inverse and multiply for the right half, of course). Currently, I'm not exploiting the SPD-ness at all. I can Cholesky-factorize $U$ and $V$ before doing each sum and then using a solver that exploits that, which will presumably save some work in the sum over $i$. Is it possible to just directly get a Cholesky-style factor of $U$ and $V$ in the sum, rather than the full matrix? The outer-product-with-an-spd-matrix-in-the-middle form is super tempting for that, but of course the sum means you can't just add together the Cholesky factors of each summand. Can I exploit some structure here, more than just doing $X_i$ times the result of a LU solver for $U^{-1} X_i$? Would it maybe be better to do $L_U^{-1} X_i$ and outer product that with itself, for example, or something smarter? As it turns out, I don't actually need $U$ and $V$, just $\det(V \otimes U) = \det(U)^n \det(V)^p$. Is there some shortcut I can take that'll get the determinants out without having to do as much work? Seems unlikely, because of the iterative nature, but people out there are smarter than me. :) (Also posted with some additional programming-related components on StackOverflow .)","I'm doing the following procedure to get the max-likelihood estimate of a matrix-variate normal distribution from $r$ samples of matrices in $\mathbb{R}^{n \times p}$ (algorithm from Dutilleul (1999) ): $$ \begin{align} V &= \frac{1}{r n} \sum_{i=1}^r X_i^T U^{-1} X_i \\ U &= \frac{1}{r p} \sum_{i=1}^r X_i V^{-1} X_i^T \end{align} $$ and repeating until convergence. ($U \in \mathbb{R}^{p \times p}$, $V \in \mathbb{R}^{n \times n}$). I need to do this for moderately large $n$ and $p$ (on the order of 1,000 each), with somewhat smaller $r$ (order of 200), and I need to do it over and over and over again, so any algorithmic speed increases would be very handy. That paper didn't really talk about the practical implementation of the algorithm, though. Since $U$ and $V$ are symmetric positive-definite, it seems that there should be some trick smarter than just doing the sums as written (using a solve call rather than an inverse and multiply for the right half, of course). Currently, I'm not exploiting the SPD-ness at all. I can Cholesky-factorize $U$ and $V$ before doing each sum and then using a solver that exploits that, which will presumably save some work in the sum over $i$. Is it possible to just directly get a Cholesky-style factor of $U$ and $V$ in the sum, rather than the full matrix? The outer-product-with-an-spd-matrix-in-the-middle form is super tempting for that, but of course the sum means you can't just add together the Cholesky factors of each summand. Can I exploit some structure here, more than just doing $X_i$ times the result of a LU solver for $U^{-1} X_i$? Would it maybe be better to do $L_U^{-1} X_i$ and outer product that with itself, for example, or something smarter? As it turns out, I don't actually need $U$ and $V$, just $\det(V \otimes U) = \det(U)^n \det(V)^p$. Is there some shortcut I can take that'll get the determinants out without having to do as much work? Seems unlikely, because of the iterative nature, but people out there are smarter than me. :) (Also posted with some additional programming-related components on StackOverflow .)",,"['linear-algebra', 'numerical-linear-algebra']"
71,Square root of a squared block matrix,Square root of a squared block matrix,,"I’m trying to compute the square root of the following squared block matrix: \begin{equation} M=\begin{bmatrix} A &B\\ C &D\\ \end{bmatrix} \end{equation} (that is $M^{1/2}$) as function of $A$, $B$, $C$, $D$ wich are all square matrices. Can you help me? I sincerely thank you! :) All the best GoodSpirit","I’m trying to compute the square root of the following squared block matrix: \begin{equation} M=\begin{bmatrix} A &B\\ C &D\\ \end{bmatrix} \end{equation} (that is $M^{1/2}$) as function of $A$, $B$, $C$, $D$ wich are all square matrices. Can you help me? I sincerely thank you! :) All the best GoodSpirit",,"['linear-algebra', 'matrices', 'block-matrices']"
72,Whats wrong with my matrix calculations?,Whats wrong with my matrix calculations?,,"I am trying to solve this matrix for $(x_1, x_2, x_3)$ $$\pmatrix{     -1 & -2& -3 \\      3 & 2 & 1 \\ 1& 1 & 1 \\     2 & 4& 1      }*  \pmatrix{    x_1\\ x_2 \\ x_3 \\     }= \pmatrix{    -6\\ 6 \\ 3 \\ 7 \\   }$$ Here are my calculations: $$\left( \begin{array}{ccc|c}     -1 &  -2& -3 & -6 \\     3 &  2& 1 & 6 \\     1 &  1& 1 & 3 \\     2 &  4& 1 & 7 \\         \end{array} \right)$$ row4 = row1*2 + row4 && row2 = row3*3 -row2 $$\left( \begin{array}{ccc|c}     -1 &  -2& -3 & -6 \\     0 &  -1& 2 & -3 \\     1 &  1& 1 & 3 \\     0 &  0& -5 & -5 \\         \end{array} \right)$$ row4 = row4/-5 && row1 = row3 + row1 $$\left( \begin{array}{ccc|c}     0 &  -1& -2 & -3 \\     0 &  -1& 2 & -3 \\     1 &  1& 1 & 3 \\     0 &  0& 1 & 1 \\         \end{array} \right)$$ row1 = row4*4 + row1 $$\left( \begin{array}{ccc|c}     1 &  1& -1 & 3 \\     0 &  -1& 2 & -3 \\     0 &  0& 1 & 1 \\     0 &  0& 0 & 1 \\         \end{array} \right)$$ I get as a solution an unsolvable matrix; however, the matrix should be solvable. What was wrong with my calulations? I cannot see the problem. Thx in advance!!!","I am trying to solve this matrix for $(x_1, x_2, x_3)$ $$\pmatrix{     -1 & -2& -3 \\      3 & 2 & 1 \\ 1& 1 & 1 \\     2 & 4& 1      }*  \pmatrix{    x_1\\ x_2 \\ x_3 \\     }= \pmatrix{    -6\\ 6 \\ 3 \\ 7 \\   }$$ Here are my calculations: $$\left( \begin{array}{ccc|c}     -1 &  -2& -3 & -6 \\     3 &  2& 1 & 6 \\     1 &  1& 1 & 3 \\     2 &  4& 1 & 7 \\         \end{array} \right)$$ row4 = row1*2 + row4 && row2 = row3*3 -row2 $$\left( \begin{array}{ccc|c}     -1 &  -2& -3 & -6 \\     0 &  -1& 2 & -3 \\     1 &  1& 1 & 3 \\     0 &  0& -5 & -5 \\         \end{array} \right)$$ row4 = row4/-5 && row1 = row3 + row1 $$\left( \begin{array}{ccc|c}     0 &  -1& -2 & -3 \\     0 &  -1& 2 & -3 \\     1 &  1& 1 & 3 \\     0 &  0& 1 & 1 \\         \end{array} \right)$$ row1 = row4*4 + row1 $$\left( \begin{array}{ccc|c}     1 &  1& -1 & 3 \\     0 &  -1& 2 & -3 \\     0 &  0& 1 & 1 \\     0 &  0& 0 & 1 \\         \end{array} \right)$$ I get as a solution an unsolvable matrix; however, the matrix should be solvable. What was wrong with my calulations? I cannot see the problem. Thx in advance!!!",,"['linear-algebra', 'matrices']"
73,Linear algebra estimates,Linear algebra estimates,,"Here is something that has been troubleing me lately. I don't know if it is true of not. I suspect it is. Suppose that $A,B$ are two $n \times n$ matrices with complex entries. $A^t = A$, $\bar B^t = B$, and B is positive i.e. $\bar \xi ^t B \xi > 0$ for any $\xi \in \Bbb C^n$. If $$|\xi ^t A \xi| \leq |\xi|^2 + \bar \xi^t B \xi \ for \ any \ \xi \in \Bbb C^n.$$ Is it true that there exists a symmetric matrix $C$ with complex entries such that  $$|\xi ^t C \xi| \leq |\xi|^2$$ $$|\xi ^t (A-C) \xi| \leq \bar \xi^t B \xi \ for \ any \ \xi \in \Bbb C^n.$$ In the case $n=1$, this is obviously true, but in higher dimensions this does not seem obvious.","Here is something that has been troubleing me lately. I don't know if it is true of not. I suspect it is. Suppose that $A,B$ are two $n \times n$ matrices with complex entries. $A^t = A$, $\bar B^t = B$, and B is positive i.e. $\bar \xi ^t B \xi > 0$ for any $\xi \in \Bbb C^n$. If $$|\xi ^t A \xi| \leq |\xi|^2 + \bar \xi^t B \xi \ for \ any \ \xi \in \Bbb C^n.$$ Is it true that there exists a symmetric matrix $C$ with complex entries such that  $$|\xi ^t C \xi| \leq |\xi|^2$$ $$|\xi ^t (A-C) \xi| \leq \bar \xi^t B \xi \ for \ any \ \xi \in \Bbb C^n.$$ In the case $n=1$, this is obviously true, but in higher dimensions this does not seem obvious.",,"['linear-algebra', 'complex-analysis', 'analysis']"
74,What is a good metric to compare matrices?,What is a good metric to compare matrices?,,"I have a matrix that I obtained from theoretical computation and I have another matrix which I obtained by actual data collection. How do I compare the two matrices? How do I state that one matrix is significantly different from the other? I have looked into: Frobenius Norm but I don't know what should be my threshold for comparison. Looking at max and min deviation of one matrix from the other. But again, what's a good threshold? I thought that to say one matrix varies significantly from the other, I would need to quantify the variance of those terms and compare how the deviation compares to the variance (or SD) but sadly, I don't have data for that and neither can I theoretically estimate any variances. I am kind of lost in what would be a good metric.","I have a matrix that I obtained from theoretical computation and I have another matrix which I obtained by actual data collection. How do I compare the two matrices? How do I state that one matrix is significantly different from the other? I have looked into: Frobenius Norm but I don't know what should be my threshold for comparison. Looking at max and min deviation of one matrix from the other. But again, what's a good threshold? I thought that to say one matrix varies significantly from the other, I would need to quantify the variance of those terms and compare how the deviation compares to the variance (or SD) but sadly, I don't have data for that and neither can I theoretically estimate any variances. I am kind of lost in what would be a good metric.",,"['linear-algebra', 'matrices']"
75,Condition number of $2 \times 2$ block matrix in terms of the singular values of the off-diagonal blocks,Condition number of  block matrix in terms of the singular values of the off-diagonal blocks,2 \times 2,If $A$ is $m \times n$ matrix such that $ m \geq n $ and $B$ is the block matrix $$ B = \begin{bmatrix}I & A \\ A^T & 0 \end{bmatrix} $$ then what is the condition number of $B$ in terms of singular values of $A$ ?,If is matrix such that and is the block matrix then what is the condition number of in terms of singular values of ?,A m \times n  m \geq n  B  B = \begin{bmatrix}I & A \\ A^T & 0 \end{bmatrix}  B A,"['linear-algebra', 'matrices', 'block-matrices', 'singular-values', 'condition-number']"
76,Minimizing maximum absolute column sum norm of the residual between a matrix and its $k$-rank approximation,Minimizing maximum absolute column sum norm of the residual between a matrix and its -rank approximation,k,Let $X \in \mathbb{R}^{m\times n}$ be a matrix with rank $r$. How can we find the optimal $\tilde{X} \in \mathbb{R}^{m\times n}$ whose rank is $k$ where $k\leq r$ and the reconstruction error in terms of maximum absolute column sum norm is minimized? That is $$e=\|X-\tilde{X}\|_1 = \max_j \sum_i|X_{ij}-\tilde{X}_{ij}|$$ is minimized. I know that the optimal solution is the $k$-reduced SVD when Frobenius-norm or spectral norm is minimized. But I'm not familiar with optimization related to $l_1$-norm and I want to use it to find sparse bases to use in an audio processing project. Is there an analytic closed form solution for this problem? Or can I find the $k$-rank approximation in an iterative numerical method?,Let $X \in \mathbb{R}^{m\times n}$ be a matrix with rank $r$. How can we find the optimal $\tilde{X} \in \mathbb{R}^{m\times n}$ whose rank is $k$ where $k\leq r$ and the reconstruction error in terms of maximum absolute column sum norm is minimized? That is $$e=\|X-\tilde{X}\|_1 = \max_j \sum_i|X_{ij}-\tilde{X}_{ij}|$$ is minimized. I know that the optimal solution is the $k$-reduced SVD when Frobenius-norm or spectral norm is minimized. But I'm not familiar with optimization related to $l_1$-norm and I want to use it to find sparse bases to use in an audio processing project. Is there an analytic closed form solution for this problem? Or can I find the $k$-rank approximation in an iterative numerical method?,,"['linear-algebra', 'optimization', 'approximation', 'normed-spaces']"
77,Extension of Cheeger's inequality with distinguished vertices,Extension of Cheeger's inequality with distinguished vertices,,"The standard Cheeger's inequality for graph $G$ states that $\frac{1}{2}$ $\lambda$ < $\phi(G)$ < $\sqrt{2\lambda}$ where $\lambda$ is the second smallest eigenvalue of the normalized Laplacian of G, and $\phi(G)$ is the conductance of G. Now suppose we are also given two special vertices $s$ and $t$, and would like to consider only cuts that separate $s$ and $t$. Can we have a similar result relating the conductance in this special case, and some Rayleigh coefficient of the normalized Laplacian of the graph?","The standard Cheeger's inequality for graph $G$ states that $\frac{1}{2}$ $\lambda$ < $\phi(G)$ < $\sqrt{2\lambda}$ where $\lambda$ is the second smallest eigenvalue of the normalized Laplacian of G, and $\phi(G)$ is the conductance of G. Now suppose we are also given two special vertices $s$ and $t$, and would like to consider only cuts that separate $s$ and $t$. Can we have a similar result relating the conductance in this special case, and some Rayleigh coefficient of the normalized Laplacian of the graph?",,"['linear-algebra', 'graph-theory', 'spectral-graph-theory']"
78,A solution to a system of homogeneous polynomials in three variables,A solution to a system of homogeneous polynomials in three variables,,"If one has (say) five degree $3$ homogeneous polynomials $f_1,f_2,f_3,f_4,f_5$ in three variables $x,y,z$, and $f_j(x_0,y_0,z_0)=0$ for all $j$ for some fixed $(x_0,y_0,z_0)$, can we conclude that there must be some relationship between some of these polynomials? In short, is there something linear algebraic that one can exploit to show that this can't happen given certain conditions on these $f_j$? Obviously given any specific set of these guys we can use ""back substitution"" in order to determine whether they can have simultaneous zeros, but I would love to know if algebraic geometry and friends can offer a more general approach.","If one has (say) five degree $3$ homogeneous polynomials $f_1,f_2,f_3,f_4,f_5$ in three variables $x,y,z$, and $f_j(x_0,y_0,z_0)=0$ for all $j$ for some fixed $(x_0,y_0,z_0)$, can we conclude that there must be some relationship between some of these polynomials? In short, is there something linear algebraic that one can exploit to show that this can't happen given certain conditions on these $f_j$? Obviously given any specific set of these guys we can use ""back substitution"" in order to determine whether they can have simultaneous zeros, but I would love to know if algebraic geometry and friends can offer a more general approach.",,['linear-algebra']
79,Points and lines covering them,Points and lines covering them,,"Let $n$ be a positive integer. A subset $S$ of points in plane satisfies the following conditions: a) We can't find $n$ lines in plane, such that every element of $S$ belongs to at least one of these lines. b) For every $X\in S$, we can find $n$ lines in plane, such that every element of $S-\{X\}$ belongs to at least one of these lines. Find the maximum number of elements of $S$. As you can see here , The problem can be solved using linear algebra. But is there a pure combinatorics way to prove it?","Let $n$ be a positive integer. A subset $S$ of points in plane satisfies the following conditions: a) We can't find $n$ lines in plane, such that every element of $S$ belongs to at least one of these lines. b) For every $X\in S$, we can find $n$ lines in plane, such that every element of $S-\{X\}$ belongs to at least one of these lines. Find the maximum number of elements of $S$. As you can see here , The problem can be solved using linear algebra. But is there a pure combinatorics way to prove it?",,"['linear-algebra', 'combinatorics', 'combinatorial-geometry']"
80,Fourier matrix - multiplicity of eigenvalues?,Fourier matrix - multiplicity of eigenvalues?,,"This question is Miscellaneous Exercise M.10 in Chapter 8 ( Bilinear Forms ) of Artin's Algebra . (The sentences in italics are due to me.) The row and column indices in the $n \times n$ Fourier matrix $A$ run from $0$ to $n-1$, and the $i, j$ entry is $n^{-1/2}\;\zeta^{ij}$, with $\zeta = e^{2 \pi i / n} \;$. ( The author defined the matrix as $[\zeta^{ij}]$; I added the $n^{-1/2}$ factor for normalisation purposes. ) This matrix solves the following interpolation problem: Given complex numbers $b_0, \ldots, b_{n-1}$, find a complex polynomial $f(t) = c_0 + c_1 t + \cdots + c_{n-1} t^{n-1}$ such that $f(\zeta^{\nu}) = b_{\nu}$. (a) Explain how the matrix solves the problem. (b) Prove that $A$ is symmetric and normal, and compute $A^2$. (c) ( This part is marked with a star. ) Determine the eigenvalues of $A$. ( I presume that we are required to determine the multiplicity of eigenvalues as well. ) My work: (a) The interpolation problem boils down to solving a linear system $n^{1/2}Ac = b$ with $c = [c_\nu]$ and $b = [b_{\nu}]$. Since $A$ is unitary, the solution is given by $c = n^{-1/2} A^{\ast} b$. (b) Symmetry is obvious. A routine calculation shows that $A$ is unitary (hence normal). Also, $$ A^2_{ij} = \frac{1}{n} \sum_{k} \zeta^{k(i+j)} = \begin{cases} 1, &\text{if } i+j=0 \pmod n, \\ 0, &\text{otherwise.} \end{cases} $$ Thus $A^2$ is a permutation matrix, the permutation corresponding to which sends any index $i$ to the unique index $j$ such that $i + j \equiv 0 \pmod n$. This is a product of disjoint transpositions. (c) From the above description, it is easy to determine the spectrum of $A^2$: its eigenvalues are $\pm 1$, and the multiplicity of $-1$ is  $$ \begin{cases} \frac{n-1}{2}, &\text{odd } n, \\ \frac{n}{2} - 1, &\text{even } n. \end{cases} $$ (The corresponding eigenvectors are of the form $e_i + e_j$ and $e_i - e_j$, where $i + j = 0 \pmod n$.) Now, the eigenvalues of $A$ are in the set $\{ \pm 1, \pm i \}$, but I cannot determine the individual multiplicities. I appreciate any hints.","This question is Miscellaneous Exercise M.10 in Chapter 8 ( Bilinear Forms ) of Artin's Algebra . (The sentences in italics are due to me.) The row and column indices in the $n \times n$ Fourier matrix $A$ run from $0$ to $n-1$, and the $i, j$ entry is $n^{-1/2}\;\zeta^{ij}$, with $\zeta = e^{2 \pi i / n} \;$. ( The author defined the matrix as $[\zeta^{ij}]$; I added the $n^{-1/2}$ factor for normalisation purposes. ) This matrix solves the following interpolation problem: Given complex numbers $b_0, \ldots, b_{n-1}$, find a complex polynomial $f(t) = c_0 + c_1 t + \cdots + c_{n-1} t^{n-1}$ such that $f(\zeta^{\nu}) = b_{\nu}$. (a) Explain how the matrix solves the problem. (b) Prove that $A$ is symmetric and normal, and compute $A^2$. (c) ( This part is marked with a star. ) Determine the eigenvalues of $A$. ( I presume that we are required to determine the multiplicity of eigenvalues as well. ) My work: (a) The interpolation problem boils down to solving a linear system $n^{1/2}Ac = b$ with $c = [c_\nu]$ and $b = [b_{\nu}]$. Since $A$ is unitary, the solution is given by $c = n^{-1/2} A^{\ast} b$. (b) Symmetry is obvious. A routine calculation shows that $A$ is unitary (hence normal). Also, $$ A^2_{ij} = \frac{1}{n} \sum_{k} \zeta^{k(i+j)} = \begin{cases} 1, &\text{if } i+j=0 \pmod n, \\ 0, &\text{otherwise.} \end{cases} $$ Thus $A^2$ is a permutation matrix, the permutation corresponding to which sends any index $i$ to the unique index $j$ such that $i + j \equiv 0 \pmod n$. This is a product of disjoint transpositions. (c) From the above description, it is easy to determine the spectrum of $A^2$: its eigenvalues are $\pm 1$, and the multiplicity of $-1$ is  $$ \begin{cases} \frac{n-1}{2}, &\text{odd } n, \\ \frac{n}{2} - 1, &\text{even } n. \end{cases} $$ (The corresponding eigenvectors are of the form $e_i + e_j$ and $e_i - e_j$, where $i + j = 0 \pmod n$.) Now, the eigenvalues of $A$ are in the set $\{ \pm 1, \pm i \}$, but I cannot determine the individual multiplicities. I appreciate any hints.",,"['linear-algebra', 'matrices', 'fourier-analysis', 'eigenvalues-eigenvectors']"
81,Finding conjugacy classes of $PGL_{2}(\mathbb{F}_{q})$,Finding conjugacy classes of,PGL_{2}(\mathbb{F}_{q}),"Assume $q$ is odd. How does one go about finding the conjugacy classes of $PGL_{2}(\mathbb{F}_{q})$? I know that for $GL_{2}(\mathbb{F}_{q})$, one can consider the possible Jordan Normal Forms of the matrices and with some luck, choose representatives whose conjugacy class is large enough such that when I sum all the conjugacy class sizes I get the whole group.","Assume $q$ is odd. How does one go about finding the conjugacy classes of $PGL_{2}(\mathbb{F}_{q})$? I know that for $GL_{2}(\mathbb{F}_{q})$, one can consider the possible Jordan Normal Forms of the matrices and with some luck, choose representatives whose conjugacy class is large enough such that when I sum all the conjugacy class sizes I get the whole group.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices']"
82,How to generate an $n \times n$ rotation matrix?,How to generate an  rotation matrix?,n \times n,"It is well known that the $2 \times 2$ rotation matrix is given by, $$\left[ \begin{array}{cc} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\ \end{array} \right]$$ and that there are  $3 \times 3$ rotation matrices to describe rotation in 3-dimensions, $$R_{x}(\theta)=\left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & \cos(\theta) & -\sin(\theta) \\ 0 & \sin(\theta) & \cos(\theta) \\ \end{array} \right]$$ $$R_{y}(\theta)=\left[ \begin{array}{ccc} \cos(\theta) & 0 & \sin(\theta) \\ 0 & 1 & 0 \\ -\sin(\theta) & 0 & \cos(\theta) \\ \end{array} \right]$$ $$R_{z}(\theta)=\left[ \begin{array}{ccc} \cos(\theta) & -\sin(\theta) & 0 \\ \sin(\theta) & \cos(\theta) & 0 \\ 0 & 0 & 1 \\ \end{array} \right]$$ That is, there are three elements of the rotation group $SO(3)$, and there is one element of $SO(2)$. In general, I found that for $\phi$ elements of $SO(d)$, where $d$ is the dimension that $$\phi = \frac{d(d-1)}{2}$$ Yet, how is it that the explicit representation of say $R_{i}(\theta)$ is derived, where $i$ is some arbitrary element of $SO(d)$. Are these things all manually computed or is there a general formula/method for determining what they are? The reason I am asking is that I have a functional in configuration space $T$ that depends on a parameter $R_{j}^{i}$ that is summing over dimensions $i,j$. $R_{j}^{i} \in SO(d)$ is representing an $i \times j$ rotation matrix (note $d=i=j$) and I am having trouble explicitly constructing an $n$-dimensional example since I don't know how to represent an $n \times n$ rotation matrix. Any references to papers, or original responses are welcome.","It is well known that the $2 \times 2$ rotation matrix is given by, $$\left[ \begin{array}{cc} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \\ \end{array} \right]$$ and that there are  $3 \times 3$ rotation matrices to describe rotation in 3-dimensions, $$R_{x}(\theta)=\left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & \cos(\theta) & -\sin(\theta) \\ 0 & \sin(\theta) & \cos(\theta) \\ \end{array} \right]$$ $$R_{y}(\theta)=\left[ \begin{array}{ccc} \cos(\theta) & 0 & \sin(\theta) \\ 0 & 1 & 0 \\ -\sin(\theta) & 0 & \cos(\theta) \\ \end{array} \right]$$ $$R_{z}(\theta)=\left[ \begin{array}{ccc} \cos(\theta) & -\sin(\theta) & 0 \\ \sin(\theta) & \cos(\theta) & 0 \\ 0 & 0 & 1 \\ \end{array} \right]$$ That is, there are three elements of the rotation group $SO(3)$, and there is one element of $SO(2)$. In general, I found that for $\phi$ elements of $SO(d)$, where $d$ is the dimension that $$\phi = \frac{d(d-1)}{2}$$ Yet, how is it that the explicit representation of say $R_{i}(\theta)$ is derived, where $i$ is some arbitrary element of $SO(d)$. Are these things all manually computed or is there a general formula/method for determining what they are? The reason I am asking is that I have a functional in configuration space $T$ that depends on a parameter $R_{j}^{i}$ that is summing over dimensions $i,j$. $R_{j}^{i} \in SO(d)$ is representing an $i \times j$ rotation matrix (note $d=i=j$) and I am having trouble explicitly constructing an $n$-dimensional example since I don't know how to represent an $n \times n$ rotation matrix. Any references to papers, or original responses are welcome.",,"['linear-algebra', 'group-theory', 'matrices']"
83,basis free volume form for a symplectic vector space,basis free volume form for a symplectic vector space,,"It's easy to show, using a symplectic basis, that if $\omega$ is a symplectic form on a $2n$-dimensional vector space $V$, then $\omega^n \neq 0$ (see for example Jason DeVito's excellent answer to this question ). I'd like to be able to prove it without choosing a basis (the symplectic analogue to a question I asked recently about inner products ), and I think it should be possible, but so far I've been unable. Let $\omega\in \Lambda^2 V^*$, where $V$ is an inner product space of dim $2n$. I want to show that $\omega^n=0 \Leftrightarrow\omega$ is degenerate. $\Leftarrow:$ $\omega$ is degenerate, so there is some element $u\in V$ with $\omega(u,v)=0, \forall v\in V.$ Then let $\{e^i\}$ be a basis containing $u$. Therefore $\omega^n(e_1\wedge\dotsb\wedge e_{2n})=0,$ and any other $2n$-vector is collinear with $e_1\wedge\dotsb\wedge e_{2n},$ so $\omega^n=0.$ $\Rightarrow:$ Given the canonical isomorphism $V\cong V^*$, we have an associated element $\omega^\sharp=\sum_i q^i\wedge p^i.$ Let $W$ be the span of the $q^i,p^i.$ If there are fewer than $2n$ distinct elements in $\{q^i,p^i\}_i$, then $\dim W<2n$ so there is a vector $u\in W^\perp$ which is in the kernel of $\omega.$ If there are $2n$ distinct $q^i,p^i$, hence exactly $n$ terms in the sum then $\omega^n=q^1\wedge p^1\wedge\dotsb \wedge p^n\wedge q^n$, but since $\omega^n=0$, there is some linear dependence among the $q^i,p^i$, so the dimension of their span is again less than $2n$, so we have an element in the kernel. If there are more than $n$ terms, then $\omega^n$ will be a sum of volume elements, which we stipulate vanishes. Can I find an element of the kernel here? (One way maybe is to say $\omega^n=\operatorname{Pf}(\omega)\text{vol}$ which vanishes iff $\det\omega$ does, but this seems like cheating) (Previous apparently nonsensical argument below:because $\omega$ is nondegenerate, we have an isomorphism $\tilde{\omega}\colon V\to V^*$ given by $v \mapsto (u\mapsto \omega(v,u))$. This induces an isomorphism $\det(\tilde{\omega})\colon \Lambda^{2n} V\to \Lambda^{2n} V^*$. Then let $u = u_1\wedge \dotsb \wedge u_{2n}$ and $v = v_1\wedge \dotsb \wedge v_{2n}$ be rank $2n$ multivectors in $\Lambda^{2n} V$ ... next step?? This doesn't seem to work right.)","It's easy to show, using a symplectic basis, that if $\omega$ is a symplectic form on a $2n$-dimensional vector space $V$, then $\omega^n \neq 0$ (see for example Jason DeVito's excellent answer to this question ). I'd like to be able to prove it without choosing a basis (the symplectic analogue to a question I asked recently about inner products ), and I think it should be possible, but so far I've been unable. Let $\omega\in \Lambda^2 V^*$, where $V$ is an inner product space of dim $2n$. I want to show that $\omega^n=0 \Leftrightarrow\omega$ is degenerate. $\Leftarrow:$ $\omega$ is degenerate, so there is some element $u\in V$ with $\omega(u,v)=0, \forall v\in V.$ Then let $\{e^i\}$ be a basis containing $u$. Therefore $\omega^n(e_1\wedge\dotsb\wedge e_{2n})=0,$ and any other $2n$-vector is collinear with $e_1\wedge\dotsb\wedge e_{2n},$ so $\omega^n=0.$ $\Rightarrow:$ Given the canonical isomorphism $V\cong V^*$, we have an associated element $\omega^\sharp=\sum_i q^i\wedge p^i.$ Let $W$ be the span of the $q^i,p^i.$ If there are fewer than $2n$ distinct elements in $\{q^i,p^i\}_i$, then $\dim W<2n$ so there is a vector $u\in W^\perp$ which is in the kernel of $\omega.$ If there are $2n$ distinct $q^i,p^i$, hence exactly $n$ terms in the sum then $\omega^n=q^1\wedge p^1\wedge\dotsb \wedge p^n\wedge q^n$, but since $\omega^n=0$, there is some linear dependence among the $q^i,p^i$, so the dimension of their span is again less than $2n$, so we have an element in the kernel. If there are more than $n$ terms, then $\omega^n$ will be a sum of volume elements, which we stipulate vanishes. Can I find an element of the kernel here? (One way maybe is to say $\omega^n=\operatorname{Pf}(\omega)\text{vol}$ which vanishes iff $\det\omega$ does, but this seems like cheating) (Previous apparently nonsensical argument below:because $\omega$ is nondegenerate, we have an isomorphism $\tilde{\omega}\colon V\to V^*$ given by $v \mapsto (u\mapsto \omega(v,u))$. This induces an isomorphism $\det(\tilde{\omega})\colon \Lambda^{2n} V\to \Lambda^{2n} V^*$. Then let $u = u_1\wedge \dotsb \wedge u_{2n}$ and $v = v_1\wedge \dotsb \wedge v_{2n}$ be rank $2n$ multivectors in $\Lambda^{2n} V$ ... next step?? This doesn't seem to work right.)",,"['linear-algebra', 'symplectic-geometry']"
84,Orthogonality of the decomposition of a vector space over one of its endomorphisms,Orthogonality of the decomposition of a vector space over one of its endomorphisms,,"Let $V$ be a finite-dimensional real inner product space and let $\tau$ be an endomorphism of $V$. Let $V=V_1 \bigoplus \cdots \bigoplus V_r$ be the decomposition of $V$ into $\tau$-invariant and $\tau$-cyclic subspaces, corresponding to the elementary divisor decomposition of $\tau$.  I know that if $\tau$ is normal with respect to the inner product, then $V_i \perp V_j$ $\forall i \neq j$. Do there exist any more general conditions on $\tau$, i.e. less strong than normality, such that the subspaces $V_i$ are orthogonal? Thank you.","Let $V$ be a finite-dimensional real inner product space and let $\tau$ be an endomorphism of $V$. Let $V=V_1 \bigoplus \cdots \bigoplus V_r$ be the decomposition of $V$ into $\tau$-invariant and $\tau$-cyclic subspaces, corresponding to the elementary divisor decomposition of $\tau$.  I know that if $\tau$ is normal with respect to the inner product, then $V_i \perp V_j$ $\forall i \neq j$. Do there exist any more general conditions on $\tau$, i.e. less strong than normality, such that the subspaces $V_i$ are orthogonal? Thank you.",,['linear-algebra']
85,Finding vectors in not in union of subspaces [duplicate],Finding vectors in not in union of subspaces [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: A vector space over $R$ is not a countable union of proper subspaces This is a single step in a larger homework problem that I'm having difficulty with. Consider a finite set of vectors $A$ in $\mathbb{R}^n$ of length $k$ $A$ has $m =\binom{k}{n-1}$ subsets of size $n-1$, designated by $A_1,A_2,...,A_m$ Let $S_j$ be the subspace spanned by $A_j$ (so $dim(S_j) \le n-1)$ Show that there must be an element in $\mathbb{R}^n$ that is not in any $S_j$ for all $j\in\{1...n\}$ This makes intuitive sense to me for $\mathbb{R}^2$ if thought of as the cartesian plane. Given any finite set of lines in $\mathbb{R}^2$,there must be a point in $\mathbb{R}^2$ that is not on any of those lines. I can't think of a way to demonstrate this, even in $\mathbb{R}^2$.","This question already has answers here : Closed 12 years ago . Possible Duplicate: A vector space over $R$ is not a countable union of proper subspaces This is a single step in a larger homework problem that I'm having difficulty with. Consider a finite set of vectors $A$ in $\mathbb{R}^n$ of length $k$ $A$ has $m =\binom{k}{n-1}$ subsets of size $n-1$, designated by $A_1,A_2,...,A_m$ Let $S_j$ be the subspace spanned by $A_j$ (so $dim(S_j) \le n-1)$ Show that there must be an element in $\mathbb{R}^n$ that is not in any $S_j$ for all $j\in\{1...n\}$ This makes intuitive sense to me for $\mathbb{R}^2$ if thought of as the cartesian plane. Given any finite set of lines in $\mathbb{R}^2$,there must be a point in $\mathbb{R}^2$ that is not on any of those lines. I can't think of a way to demonstrate this, even in $\mathbb{R}^2$.",,['linear-algebra']
86,How do you find the base of a module given a set of generators?,How do you find the base of a module given a set of generators?,,"Doing a little self-study, and I'm given a submodule of $\mathbb{Z}^3$ generated by $f_1=(1,0,-1)$, $f_2=(2,-3,1)$, $f_3=(0,3,1)$ and $f_4=(3,1,5)$. How do I find a base of the submodule? I put them in a matrix and reduced $$ \begin{pmatrix} 1 & 2 & 0 & 3 \\ 0 & -3 & 3 & 1 \\ -1 & 1 & 1 & 5\end{pmatrix} \sim \begin{pmatrix} 1 & 2 & 0 & 3 \\ 0 & -3 & 3 & 1 \\ 0 & 0 & 4 & 9\end{pmatrix}. $$ Does this mean I just take $\{f_1,f_2,f_3\}$ as a base for the submodule? What's the general way to do these computations?","Doing a little self-study, and I'm given a submodule of $\mathbb{Z}^3$ generated by $f_1=(1,0,-1)$, $f_2=(2,-3,1)$, $f_3=(0,3,1)$ and $f_4=(3,1,5)$. How do I find a base of the submodule? I put them in a matrix and reduced $$ \begin{pmatrix} 1 & 2 & 0 & 3 \\ 0 & -3 & 3 & 1 \\ -1 & 1 & 1 & 5\end{pmatrix} \sim \begin{pmatrix} 1 & 2 & 0 & 3 \\ 0 & -3 & 3 & 1 \\ 0 & 0 & 4 & 9\end{pmatrix}. $$ Does this mean I just take $\{f_1,f_2,f_3\}$ as a base for the submodule? What's the general way to do these computations?",,"['linear-algebra', 'abstract-algebra']"
87,Solutions to equation in matrix form,Solutions to equation in matrix form,,"Suppose $x=[x_1, x_2, \cdots, x_n]^t$, $b=[b_1,b_2,\cdots,b_n]^t$ with $b_i\in K$ and $A\in M_n(K)$, where $K$ is a field. There are well known criteria for the system of equations $Ax=b$, by considering rank of $A$ and $[A|b]$. If we consider the equation $x^tAx=\lambda$, for $\lambda \in K$, what are the criteria for the existance of solution and simple methods to solve it?","Suppose $x=[x_1, x_2, \cdots, x_n]^t$, $b=[b_1,b_2,\cdots,b_n]^t$ with $b_i\in K$ and $A\in M_n(K)$, where $K$ is a field. There are well known criteria for the system of equations $Ax=b$, by considering rank of $A$ and $[A|b]$. If we consider the equation $x^tAx=\lambda$, for $\lambda \in K$, what are the criteria for the existance of solution and simple methods to solve it?",,"['linear-algebra', 'number-theory']"
88,Generalization of Plücker embedding of the Grassmannian manifold,Generalization of Plücker embedding of the Grassmannian manifold,,"Let $V$ be any vector space and $1 \leq k \leq n$ . By $\operatorname{Grass}_n(V)$ I mean the Grassmannian of $n$ -codimensional subspaces of $V$ , that is, $n$ -dimensional quotients of $V$ . Then we can define a natural map $$\textstyle \omega_{n,k} : \operatorname{Grass}_n(V) \to \operatorname{Grass}_{\Large\binom{n}{k}}\left(\bigwedge^k V\right), \quad (q : V \twoheadrightarrow W) \mapsto (\bigwedge^k q : \bigwedge^k V \twoheadrightarrow \bigwedge^k W).$$ Remark that for $k=n$ we get the usual Plücker embedding $\mathrm{Grass}_n(V) \hookrightarrow \mathbb{P}(\Lambda^n V)$ . Questions. How are the maps $\omega_{n,k}$ called? Are they also embeddings? Can you give me some reference where they are studied? More generally, for a quasi-coherent module $\mathcal{E}$ on a scheme $S$ we get a morphism of $S$ -schemes $$\textstyle \omega_{n,k} : \operatorname{\textbf{Grass}}_n(\mathcal{E}) \to \operatorname{\textbf{Grass}}_{\Large \binom{n}{k}}\left(\bigwedge^k \mathcal{E}\right).$$ The notation is taken from EGA I, §9.7. Is this is a closed immersion? If not, is it the case under some additional assumption? What else can be said about this morphism?","Let be any vector space and . By I mean the Grassmannian of -codimensional subspaces of , that is, -dimensional quotients of . Then we can define a natural map Remark that for we get the usual Plücker embedding . Questions. How are the maps called? Are they also embeddings? Can you give me some reference where they are studied? More generally, for a quasi-coherent module on a scheme we get a morphism of -schemes The notation is taken from EGA I, §9.7. Is this is a closed immersion? If not, is it the case under some additional assumption? What else can be said about this morphism?","V 1 \leq k \leq n \operatorname{Grass}_n(V) n V n V \textstyle \omega_{n,k} : \operatorname{Grass}_n(V) \to \operatorname{Grass}_{\Large\binom{n}{k}}\left(\bigwedge^k V\right), \quad (q : V \twoheadrightarrow W) \mapsto (\bigwedge^k q : \bigwedge^k V \twoheadrightarrow \bigwedge^k W). k=n \mathrm{Grass}_n(V) \hookrightarrow \mathbb{P}(\Lambda^n V) \omega_{n,k} \mathcal{E} S S \textstyle \omega_{n,k} : \operatorname{\textbf{Grass}}_n(\mathcal{E}) \to \operatorname{\textbf{Grass}}_{\Large \binom{n}{k}}\left(\bigwedge^k \mathcal{E}\right).","['linear-algebra', 'algebraic-geometry', 'reference-request', 'grassmannian']"
89,Restriction of trivariate polynomial to $1$ variable,Restriction of trivariate polynomial to  variable,1,"Let $p(x,y,z): \mathbb{F}^3 \to \mathbb{F}$ be a trivariate polynomial of degree $d \ll |\mathbb{F}|$. We choose uniformly at random an affine $1$-dimentional space $\ell = \{(a_1,a_2,a_3)t + (b_1,b_2,b_3) : t \in \mathbb{F}\}$. What is a good bound on the probability that the restriction of $p$ to $\ell$ (as a univariate polynomial) is of degree strictly less than $d$? Thanks","Let $p(x,y,z): \mathbb{F}^3 \to \mathbb{F}$ be a trivariate polynomial of degree $d \ll |\mathbb{F}|$. We choose uniformly at random an affine $1$-dimentional space $\ell = \{(a_1,a_2,a_3)t + (b_1,b_2,b_3) : t \in \mathbb{F}\}$. What is a good bound on the probability that the restriction of $p$ to $\ell$ (as a univariate polynomial) is of degree strictly less than $d$? Thanks",,"['linear-algebra', 'abstract-algebra', 'polynomials']"
90,How many ways to choose $l$ vectors in $n$-dimensional space such that every $k$-subset is independent,How many ways to choose  vectors in -dimensional space such that every -subset is independent,l n k,"Working in $F_q^n$. How many different ways do we have to choose $l$ vectors such that every subset of size $k$ of them is linearly independent. (Assume n is large) My Progress : For the first k vectors, just keep out of the subspace spanned so far, so for $1 \leq i \leq k$ we have $q^n-q^{i-1}$ ways to choose the $i^{th}$ vector. But the next ones are harder. Edit : Although the assignment asks me to find the exact number, a good lower bound can be helpful.","Working in $F_q^n$. How many different ways do we have to choose $l$ vectors such that every subset of size $k$ of them is linearly independent. (Assume n is large) My Progress : For the first k vectors, just keep out of the subspace spanned so far, so for $1 \leq i \leq k$ we have $q^n-q^{i-1}$ ways to choose the $i^{th}$ vector. But the next ones are harder. Edit : Although the assignment asks me to find the exact number, a good lower bound can be helpful.",,[]
91,"linear system solution, iterative vs direct","linear system solution, iterative vs direct",,"Dear all, I have systems like $(A - \lambda B) X = F$ where lambda is being updated inside a loop. I also have a limited number of eigenvectors of the matrix pair (A, B), say 40 eigenpair from a previous analysis step. I could get the results with direct solution however due to repeated factorization of the operator matrix, I run into the O(n^3) wall. I was wondering if that could be possible to solve this system by some iterative methods. I, myself, tried some iterative methods, cg, minres, and others firstly in MATLAB, however cg like methods do not even converge due to the nature of the operator matrix since that can become indefinite in the loop, minres seems to converge hovewer that also does not converge in reasonable iteration counts. Also in these tries I had to use a PD preconditioner and do a factorization, which also induces other costs. Basically are there some kind of a mathematical tricks for this kind of problem, where some limited number of eigenpairs are available from a previous analysis step? As a poor engineer I tried all I know, so any further ideas? TIA, Umut","Dear all, I have systems like $(A - \lambda B) X = F$ where lambda is being updated inside a loop. I also have a limited number of eigenvectors of the matrix pair (A, B), say 40 eigenpair from a previous analysis step. I could get the results with direct solution however due to repeated factorization of the operator matrix, I run into the O(n^3) wall. I was wondering if that could be possible to solve this system by some iterative methods. I, myself, tried some iterative methods, cg, minres, and others firstly in MATLAB, however cg like methods do not even converge due to the nature of the operator matrix since that can become indefinite in the loop, minres seems to converge hovewer that also does not converge in reasonable iteration counts. Also in these tries I had to use a PD preconditioner and do a factorization, which also induces other costs. Basically are there some kind of a mathematical tricks for this kind of problem, where some limited number of eigenpairs are available from a previous analysis step? As a poor engineer I tried all I know, so any further ideas? TIA, Umut",,['linear-algebra']
92,Determinant of $A^T A$ where $A$ is a block lower triangular matrix,Determinant of  where  is a block lower triangular matrix,A^T A A,"Is there a trick or simple way to compute $$\text{det}(A^T A)$$ where $A \in  \mathbb{R}^{m \times n}$ , $m \neq n$ , is a block lower triangular matrix? An example of such a matrix would be $$ \begin{pmatrix} \mathbf{A} & \mathbf{0} & \mathbf{0} \\ \mathbf{B} & \mathbf{C} & \mathbf{0} \\ \mathbf{D} & \mathbf{E} & \mathbf{F} \\ \end{pmatrix}, $$ where all bold face sub-matrices are, for example, $3 \times 2$ . I know that if $A$ is a square matrix with square blocks, it is just the product of the determinants of the blocks on the diagonal. However, I'm interested in the case when the blocks are non-square. Note: If it helps, we may assume that the matrix $A$ has full column rank. In addition to the determinant, is there a simple way to find the inverse, i.e., $(A^T A)^{-1}$ ?","Is there a trick or simple way to compute where , , is a block lower triangular matrix? An example of such a matrix would be where all bold face sub-matrices are, for example, . I know that if is a square matrix with square blocks, it is just the product of the determinants of the blocks on the diagonal. However, I'm interested in the case when the blocks are non-square. Note: If it helps, we may assume that the matrix has full column rank. In addition to the determinant, is there a simple way to find the inverse, i.e., ?","\text{det}(A^T A) A \in  \mathbb{R}^{m \times n} m \neq n 
\begin{pmatrix}
\mathbf{A} & \mathbf{0} & \mathbf{0} \\
\mathbf{B} & \mathbf{C} & \mathbf{0} \\
\mathbf{D} & \mathbf{E} & \mathbf{F} \\
\end{pmatrix},
 3 \times 2 A A (A^T A)^{-1}","['linear-algebra', 'matrices']"
93,Analytical expression for the determinant of block tridiagonal matrix,Analytical expression for the determinant of block tridiagonal matrix,,"I have a $3n\times3n$ matrix $M$ that is in the following block tridiagonal form: $$M=\begin{pmatrix}  A & B^T & 0\\ B & A & UBU \\ 0 & UB^T U & A\\ \end{pmatrix}$$ where $A,B,U$ are $n\times n$ real matrices. I also know that $A$ and $U$ are symmetric and $U$ is orthogonal (it is the exchange matrix ), and that $\det{B} = 0$ . I want to know if a closed-form analytical expression for the determinant of $M$ exists. What I have tried so far: From the paper mentioned in this post , I found that one can write $$\det{M} = \det{\Lambda_1}\det{\Lambda_2}\det{\Lambda_3},$$ with $\Lambda_1 = A$ , $\Lambda_2 = A - B \Lambda_1^{-1} B^T$ , and $\Lambda_3 = A - (UB^TU)\Lambda_2^{-1}(UBU)$ . I have not been able to progress beyond this point as I do not know of some clever way to do the inversions . Thanks!","I have a matrix that is in the following block tridiagonal form: where are real matrices. I also know that and are symmetric and is orthogonal (it is the exchange matrix ), and that . I want to know if a closed-form analytical expression for the determinant of exists. What I have tried so far: From the paper mentioned in this post , I found that one can write with , , and . I have not been able to progress beyond this point as I do not know of some clever way to do the inversions . Thanks!","3n\times3n M M=\begin{pmatrix} 
A & B^T & 0\\
B & A & UBU \\
0 & UB^T U & A\\
\end{pmatrix} A,B,U n\times n A U U \det{B} = 0 M \det{M} = \det{\Lambda_1}\det{\Lambda_2}\det{\Lambda_3}, \Lambda_1 = A \Lambda_2 = A - B \Lambda_1^{-1} B^T \Lambda_3 = A - (UB^TU)\Lambda_2^{-1}(UBU)","['linear-algebra', 'matrices', 'determinant', 'block-matrices', 'tridiagonal-matrices']"
94,Number of vectors in a set & span of a set,Number of vectors in a set & span of a set,,"I needed clarification on a linear algebra question that I had: Given the matrices $v_1 = \begin{bmatrix}     1 \\     1 \\       1 \\  \end{bmatrix}, $ $v_2 = \begin{bmatrix}     1 \\     -1 \\       1 \\  \end{bmatrix}$ and $v_3 = \begin{bmatrix}     1 \\     1 \\       -1 \\  \end{bmatrix}$, 1) How many vectors does the set {${v_1, v_2, v_3}$} have? 2) How many vectors are in Span{$v_1, v_2, v_3$}? I think the answer to #1 is 3 , simply because there are three matrices, and the answer to #2 is infinite , since there are an infinite number of linear combinations that can be made using these vectors. I am uncertain of these answers, though.","I needed clarification on a linear algebra question that I had: Given the matrices $v_1 = \begin{bmatrix}     1 \\     1 \\       1 \\  \end{bmatrix}, $ $v_2 = \begin{bmatrix}     1 \\     -1 \\       1 \\  \end{bmatrix}$ and $v_3 = \begin{bmatrix}     1 \\     1 \\       -1 \\  \end{bmatrix}$, 1) How many vectors does the set {${v_1, v_2, v_3}$} have? 2) How many vectors are in Span{$v_1, v_2, v_3$}? I think the answer to #1 is 3 , simply because there are three matrices, and the answer to #2 is infinite , since there are an infinite number of linear combinations that can be made using these vectors. I am uncertain of these answers, though.",,[]
95,Linear algebra problem from dummite & foote,Linear algebra problem from dummite & foote,,"Let $V$ be a finite dimensional vector space over $\mathbb{Q}$ and suppose $T$ is a nonsingular linear transformation of $V$ such that $T^{-1} = T^2 + T$. Prove that the dimension of $V$ is divisible by $3$. If the dimension of $V$ is precisely $3$, prove that all such transformations $T$ are similar. So applying $T$ to both sides of the given equation gives us $T^3 + T^2 - I = 0$, hence the minimal polynomial $m(x)$ of $T$ divides $x^3 + x^2 + 1$. But this polynomial is irreducible over $\mathbb{Q}$ by the rational root test, hence $m(x) = x^3 + x^2 + 1$. The structure theorem for finitely-generated modules over PIDs tells that $$(V,T) \cong \bigoplus_{i=1}^{t}\frac{\mathbb{Q}[x]}{(a_i(x))},$$ where $t \geq 1$ and $a_i(x) \mid a_{i+1}(x)$ and $a_t(x) = m(x)$. But since $m(x)$ is irreducible, we must have each $a_i(x) = m(x)$, therefore $$(V,T) \cong \left(\frac{\mathbb{Q}[x]}{(f(x))}\right)^t.$$ Now the dimension of $V$ equals $3t$ and is thus divisible by $3$. But doesn't it then also follow that given a dimension $3t$, any two such transformations $T$ make $V$ isomorphic as a $\mathbb{Q}[x]$-module to the above, hence are similar? It seems true in general, not just for $\dim(V) = 3$. Thanks!","Let $V$ be a finite dimensional vector space over $\mathbb{Q}$ and suppose $T$ is a nonsingular linear transformation of $V$ such that $T^{-1} = T^2 + T$. Prove that the dimension of $V$ is divisible by $3$. If the dimension of $V$ is precisely $3$, prove that all such transformations $T$ are similar. So applying $T$ to both sides of the given equation gives us $T^3 + T^2 - I = 0$, hence the minimal polynomial $m(x)$ of $T$ divides $x^3 + x^2 + 1$. But this polynomial is irreducible over $\mathbb{Q}$ by the rational root test, hence $m(x) = x^3 + x^2 + 1$. The structure theorem for finitely-generated modules over PIDs tells that $$(V,T) \cong \bigoplus_{i=1}^{t}\frac{\mathbb{Q}[x]}{(a_i(x))},$$ where $t \geq 1$ and $a_i(x) \mid a_{i+1}(x)$ and $a_t(x) = m(x)$. But since $m(x)$ is irreducible, we must have each $a_i(x) = m(x)$, therefore $$(V,T) \cong \left(\frac{\mathbb{Q}[x]}{(f(x))}\right)^t.$$ Now the dimension of $V$ equals $3t$ and is thus divisible by $3$. But doesn't it then also follow that given a dimension $3t$, any two such transformations $T$ make $V$ isomorphic as a $\mathbb{Q}[x]$-module to the above, hence are similar? It seems true in general, not just for $\dim(V) = 3$. Thanks!",,['linear-algebra']
96,Standard Basis of $SU(2)$--where does the 1/2 come from?,Standard Basis of --where does the 1/2 come from?,SU(2),"The most common matrix representation of $SU(2)$ is given by  $$ \begin{pmatrix} a & b\\ b^* & -a^*\\ \end{pmatrix} $$ where $a,b\in\mathbb{C}$. If we denote real components by the subscript $r$, imaginary by $i$, one can clearly write this as $$ \begin{pmatrix} a & b\\ -b^* & a^*\\ \end{pmatrix}  = a_r\begin{pmatrix} 1 & 0\\ 0 & 1\\ \end{pmatrix} +b_r\begin{pmatrix} 0 & 1 \\ -1 & 0 \\ \end{pmatrix} +b_i\begin{pmatrix} 0 & i \\ i & 0 \\ \end{pmatrix} +a_i\begin{pmatrix} i & 0 \\ 0 & -i \\ \end{pmatrix}, $$ so the Lie algebra should be obtained by discarding the coefficient of the identity matrix and identifying the remaining traceless matrices as the generators of $\mathfrak{su}(2)$. We quickly realize these are $i\sigma_a$, where $a = 1,2,3$ denote the Pauli spin matrices. Many books however define the standard basis of $\mathfrak{su}(2)$ as $(i/2)\sigma_a$. While this would normally be simply a trivial convention, R. Gilmore, on page 107 of ""Lie Groups, Physics, and Geometry"", seems to use this to show the $SU(2)\rightarrow SO(3)$ cover is 2:1 by splitting $\mathfrak{su}(2)$ into the form $\mathfrak{su}(2)\ni X = S_1 + (ia_i/2)\sigma_3$ where $S_1 =  (ib_i/2)\sigma_1 + (ib_r/2)\sigma_2$ and exponentiating $S_1$ and $(ia_i/2)\sigma_3$ individually to give $$SU(2) \ni U = \exp(S_1)\times\begin{pmatrix} e^{ia_i/2} & 0\\ 0 & e^{-ia_i/2}\\ \end{pmatrix}.$$ Using a similar splitting of $\mathfrak{so}(3)$ into two components (an $SO(3)/SO(2)$ quotient and $SO(2)$) which are individually exponentiated, he concludes the quotients $SO(3)/SO(2)$ and $SU(2)/U(1)$ are both topologically $S^2$, thus bijective, and the thus the cover must be 2:1 because putting $\theta \mapsto \theta + 2\pi$ is a complete rotation in $SO(2)$, while $a_i \mapsto a_i + 4\pi$ is a complete rotation in the given representation of $U(1)$. I've been wanting to emulate this argument but can't do so without the factor of $1/2$. My questions are thus as follows: (1) where does the $1/2$ come from? Is it just convention or something else? (2) Is there any easy way to argue the cover is $2:1$ without using the $1/2$? (3) I thought that generally the whole Lie algebra had to be exponentiated to map back onto the Lie group. How does separating the Lie algebra and exponentiating the terms individually make sense in the context of the BCH formula? Does this have to do with irreducible representations?","The most common matrix representation of $SU(2)$ is given by  $$ \begin{pmatrix} a & b\\ b^* & -a^*\\ \end{pmatrix} $$ where $a,b\in\mathbb{C}$. If we denote real components by the subscript $r$, imaginary by $i$, one can clearly write this as $$ \begin{pmatrix} a & b\\ -b^* & a^*\\ \end{pmatrix}  = a_r\begin{pmatrix} 1 & 0\\ 0 & 1\\ \end{pmatrix} +b_r\begin{pmatrix} 0 & 1 \\ -1 & 0 \\ \end{pmatrix} +b_i\begin{pmatrix} 0 & i \\ i & 0 \\ \end{pmatrix} +a_i\begin{pmatrix} i & 0 \\ 0 & -i \\ \end{pmatrix}, $$ so the Lie algebra should be obtained by discarding the coefficient of the identity matrix and identifying the remaining traceless matrices as the generators of $\mathfrak{su}(2)$. We quickly realize these are $i\sigma_a$, where $a = 1,2,3$ denote the Pauli spin matrices. Many books however define the standard basis of $\mathfrak{su}(2)$ as $(i/2)\sigma_a$. While this would normally be simply a trivial convention, R. Gilmore, on page 107 of ""Lie Groups, Physics, and Geometry"", seems to use this to show the $SU(2)\rightarrow SO(3)$ cover is 2:1 by splitting $\mathfrak{su}(2)$ into the form $\mathfrak{su}(2)\ni X = S_1 + (ia_i/2)\sigma_3$ where $S_1 =  (ib_i/2)\sigma_1 + (ib_r/2)\sigma_2$ and exponentiating $S_1$ and $(ia_i/2)\sigma_3$ individually to give $$SU(2) \ni U = \exp(S_1)\times\begin{pmatrix} e^{ia_i/2} & 0\\ 0 & e^{-ia_i/2}\\ \end{pmatrix}.$$ Using a similar splitting of $\mathfrak{so}(3)$ into two components (an $SO(3)/SO(2)$ quotient and $SO(2)$) which are individually exponentiated, he concludes the quotients $SO(3)/SO(2)$ and $SU(2)/U(1)$ are both topologically $S^2$, thus bijective, and the thus the cover must be 2:1 because putting $\theta \mapsto \theta + 2\pi$ is a complete rotation in $SO(2)$, while $a_i \mapsto a_i + 4\pi$ is a complete rotation in the given representation of $U(1)$. I've been wanting to emulate this argument but can't do so without the factor of $1/2$. My questions are thus as follows: (1) where does the $1/2$ come from? Is it just convention or something else? (2) Is there any easy way to argue the cover is $2:1$ without using the $1/2$? (3) I thought that generally the whole Lie algebra had to be exponentiated to map back onto the Lie group. How does separating the Lie algebra and exponentiating the terms individually make sense in the context of the BCH formula? Does this have to do with irreducible representations?",,"['linear-algebra', 'lie-groups', 'lie-algebras', 'rotations']"
97,Simultaneously diagonalizable without distinct eigenvalues,Simultaneously diagonalizable without distinct eigenvalues,,"It is a well known result that if $u$ and $v$ are two diagonalizable endomorphisms of a $\mathbb{C}$ finite-dimensional linear space $E$, if $u$ (or $v$) has distinct eigenvalues and if $u$ and $v$ commute, then $u$ and $v$ are simultaneously diagonalizable. I am wondering what happens when we drop the hypothesis that $u$ or $v$ has distinct eigenvalues ? The usual proof consists in saying that if $(\lambda_{1},\ldots,\lambda_{n})$ are the distinct eigenvalues of $u$ and $E_{i} = \mathrm{ker}(u- \lambda_{i} \mathrm{Id})$ then $g_{\vert E_{i}}$ (the endomorphism induced by $g$ on $E_{i}$) is diagonalizable. Then if $B_{i}$ is a basis of $E_{i}$ composed of eigenvectors for $g$, $(B_{1},\ldots,B_{n})$ is a basis of $E$ which diagonalize both $u$ and $v$. What changes if the engenvalues are not distinct ?","It is a well known result that if $u$ and $v$ are two diagonalizable endomorphisms of a $\mathbb{C}$ finite-dimensional linear space $E$, if $u$ (or $v$) has distinct eigenvalues and if $u$ and $v$ commute, then $u$ and $v$ are simultaneously diagonalizable. I am wondering what happens when we drop the hypothesis that $u$ or $v$ has distinct eigenvalues ? The usual proof consists in saying that if $(\lambda_{1},\ldots,\lambda_{n})$ are the distinct eigenvalues of $u$ and $E_{i} = \mathrm{ker}(u- \lambda_{i} \mathrm{Id})$ then $g_{\vert E_{i}}$ (the endomorphism induced by $g$ on $E_{i}$) is diagonalizable. Then if $B_{i}$ is a basis of $E_{i}$ composed of eigenvectors for $g$, $(B_{1},\ldots,B_{n})$ is a basis of $E$ which diagonalize both $u$ and $v$. What changes if the engenvalues are not distinct ?",,"['linear-algebra', 'diagonalization']"
98,Combination of linear functions that give the derivative operator,Combination of linear functions that give the derivative operator,,"Let $D$ be the derivative operator and $C^\infty$ the set of functions   derivable infinitely many times. Here $f^n=f\circ f\circ\cdots\circ f\text{, }n\text{ times}$ It can be easily shown that there exists no linear function $f\in\mathcal{L}(C^\infty)$  such that $f^2=D$. If I am not wrong, this can be easily extended to the non existence of a function $f\in\mathcal{L}(C^\infty)$ such that $\prod f=D$ as a finite product. What can be said for an infinite product? How can one (dis)approve of the existence of a linear function $f\in\mathcal{L}(C^\infty)$ such that $f^n=D$ ? It seems unlikely to me, but I have found no way to prove it.","Let $D$ be the derivative operator and $C^\infty$ the set of functions   derivable infinitely many times. Here $f^n=f\circ f\circ\cdots\circ f\text{, }n\text{ times}$ It can be easily shown that there exists no linear function $f\in\mathcal{L}(C^\infty)$  such that $f^2=D$. If I am not wrong, this can be easily extended to the non existence of a function $f\in\mathcal{L}(C^\infty)$ such that $\prod f=D$ as a finite product. What can be said for an infinite product? How can one (dis)approve of the existence of a linear function $f\in\mathcal{L}(C^\infty)$ such that $f^n=D$ ? It seems unlikely to me, but I have found no way to prove it.",,"['linear-algebra', 'derivatives']"
99,Question about complete orthonormal basis,Question about complete orthonormal basis,,"Let $V$ be an inner product space. Let $W$ be the Hilbert space obtained as the completion of $V$. Is there a complete orthonormal basis of $V$ which is still complete in $W$? This is true if we assume that $V$ is separable (Schumidt's method), but I don't know if this is true or not in general.","Let $V$ be an inner product space. Let $W$ be the Hilbert space obtained as the completion of $V$. Is there a complete orthonormal basis of $V$ which is still complete in $W$? This is true if we assume that $V$ is separable (Schumidt's method), but I don't know if this is true or not in general.",,['linear-algebra']
