,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Distinction between ""measure differential equations"" and ""differential equations in distributions""?","Distinction between ""measure differential equations"" and ""differential equations in distributions""?",,"Is there a universally recognized term for ODEs considered in the sense of distributions used to describe impulsive/discontinuous processes? I noticed that some authors call such ODEs ""measure differential equations"" while others use the term ""differential equations in distributions"". But I don't see a major difference between them. Can anyone please make the point clear? Thank you.","Is there a universally recognized term for ODEs considered in the sense of distributions used to describe impulsive/discontinuous processes? I noticed that some authors call such ODEs ""measure differential equations"" while others use the term ""differential equations in distributions"". But I don't see a major difference between them. Can anyone please make the point clear? Thank you.",,"['ordinary-differential-equations', 'distribution-theory']"
1,Homogeneous Differential Equations Inspiration,Homogeneous Differential Equations Inspiration,,"Homogeneous first order differential equations can be solved by substituting $y/x = v$. I was wondering what is the inspiration for this. I am trying to understand the thinking behind this substitution. It is a known property of homogeneous polynomials that they can be expressed as functions of $y/x$, but how could you be sure that once you do that, the variables are separable? Again, the homogeneous polynomials $M$ and $N$ have to be of the same degree, otherwise, they are not separable. What's the thinking behind that? HFODEs are one of simplest type, so I was wondering if there's an intuitive way one could think about this (or is there any other analytic algebraic property of homogeneous polynomials that I am missing?)","Homogeneous first order differential equations can be solved by substituting $y/x = v$. I was wondering what is the inspiration for this. I am trying to understand the thinking behind this substitution. It is a known property of homogeneous polynomials that they can be expressed as functions of $y/x$, but how could you be sure that once you do that, the variables are separable? Again, the homogeneous polynomials $M$ and $N$ have to be of the same degree, otherwise, they are not separable. What's the thinking behind that? HFODEs are one of simplest type, so I was wondering if there's an intuitive way one could think about this (or is there any other analytic algebraic property of homogeneous polynomials that I am missing?)",,"['ordinary-differential-equations', 'intuition', 'math-history']"
2,"Laplace transform of $y''' - 3y'' + 3y' - y = (t^2)e^t$ where $y(0)=1$, $y'(0)=0$, $y'' = -2$","Laplace transform of  where , ,",y''' - 3y'' + 3y' - y = (t^2)e^t y(0)=1 y'(0)=0 y'' = -2,"Any ideas? I got:- $$s^3 - 2s^2 + 3s - 4/(s(s^2 + 3) + 1))$$ but I got it wrong, obviously, because it does not simplify into any inverse laplaces.","Any ideas? I got:- $$s^3 - 2s^2 + 3s - 4/(s(s^2 + 3) + 1))$$ but I got it wrong, obviously, because it does not simplify into any inverse laplaces.",,"['ordinary-differential-equations', 'laplace-transform']"
3,Second-Order Differential Equation--Frobenius Method,Second-Order Differential Equation--Frobenius Method,,"I'm studying for a qualifying examination and am stuck on the following question.  Could anyone give me some help? $$y''+\frac{\sin x}{x}y'+\frac{2\cos(x+x^2)-\frac{2}{(x-1)^2}+4x}{x^2}y=0$$ Find all singular points of the equation and classify them as regular/irregular.  Then find the first term in a series in powers of $x-1$ for each of two linearly independent solutions as $x\rightarrow1$. I think the singular points are 0 and 1, and they are both regular.  But I am having some trouble with the series solution.","I'm studying for a qualifying examination and am stuck on the following question.  Could anyone give me some help? $$y''+\frac{\sin x}{x}y'+\frac{2\cos(x+x^2)-\frac{2}{(x-1)^2}+4x}{x^2}y=0$$ Find all singular points of the equation and classify them as regular/irregular.  Then find the first term in a series in powers of $x-1$ for each of two linearly independent solutions as $x\rightarrow1$. I think the singular points are 0 and 1, and they are both regular.  But I am having some trouble with the series solution.",,['ordinary-differential-equations']
4,Directional field of a nonlinear differential equation,Directional field of a nonlinear differential equation,,"We are given the differential equation $\large\frac{\textrm dy}{\textrm dx}(x)=\frac{1}{3y(x)+x^2}+\frac{2}{4+x^2}$ and its directional field (see image below). If this equation is solvable in closed form, I suspect it is not an easy solution and Wolfram Alpha confirms this idea by not giving such a solution. We are asked: $\lim\limits_{x\to \infty}y(x)$ for the solution with $y(0)=-1$ and the solution with $y(0)=1$. For both of these I simply look at the directional field and follow the lines. So for $y(0)=-1$, I guess $\lim\limits_{x\to \infty}y(x)=-\infty$ and for $y(0)=1$, I guess $\lim\limits_{x\to \infty}y(x)=3$. for what initial conditions the solution $y(x)$ is bounded for all $x\in\mathbb{R}$. Once again I look at the directional field and guess that all solutions with initial conditions under or on the parabola that we see aren't bounded. So I say that all solutions with $y(0)>0$ are bounded. The way I have ""solved"" these problems does not feel very nice and I am not sure it is right. I would like to hear if there are ways I could find the things I am asked using the differential equation, without relying as much on the directional field. I would also appreciate suggestions on how to find the solution which corresponds to the parabola (I'm not sure of course it's a parabola, but I think so) that we see.","We are given the differential equation $\large\frac{\textrm dy}{\textrm dx}(x)=\frac{1}{3y(x)+x^2}+\frac{2}{4+x^2}$ and its directional field (see image below). If this equation is solvable in closed form, I suspect it is not an easy solution and Wolfram Alpha confirms this idea by not giving such a solution. We are asked: $\lim\limits_{x\to \infty}y(x)$ for the solution with $y(0)=-1$ and the solution with $y(0)=1$. For both of these I simply look at the directional field and follow the lines. So for $y(0)=-1$, I guess $\lim\limits_{x\to \infty}y(x)=-\infty$ and for $y(0)=1$, I guess $\lim\limits_{x\to \infty}y(x)=3$. for what initial conditions the solution $y(x)$ is bounded for all $x\in\mathbb{R}$. Once again I look at the directional field and guess that all solutions with initial conditions under or on the parabola that we see aren't bounded. So I say that all solutions with $y(0)>0$ are bounded. The way I have ""solved"" these problems does not feel very nice and I am not sure it is right. I would like to hear if there are ways I could find the things I am asked using the differential equation, without relying as much on the directional field. I would also appreciate suggestions on how to find the solution which corresponds to the parabola (I'm not sure of course it's a parabola, but I think so) that we see.",,['calculus']
5,First Order Homogeneous Differential Equation (with a Linear shift),First Order Homogeneous Differential Equation (with a Linear shift),,Is there a simpler method to this solve this equation $$(x-2y+1)\text dx+(4x-3y-6)\text dy=0$$ $$\frac{\text dy}{\text dx}=\frac{2y-x-1}{4x-3y-6}$$ $$\frac{\text d Y}{ \text d X}=\frac{2(Y+k)-(X+h)-1}{4(X+h)-3(Y+k)-6}$$ $$\frac{\text d Y}{ \text d X}=\frac{2Y-X+(2k-h-1)}{4X-3Y+(4h-3k-6)}$$ $$2k-h-1=0\qquad4h-3k-6=0$$ $$h=2k-1\qquad 4(2k-1)-3k-6=0$$ $$5k-10=0\qquad k=2\qquad h=2(2)-1\qquad h=3$$ $$y=Y+2\qquad\qquad x=X+3$$ $$Y=y-2\qquad\qquad X=x-3$$ $$\frac{\text d Y}{\text d X}=\frac{2Y-X}{4X-3Y}=\frac{2\left(\frac{Y}{X}\right)-1}{4-3\left(\frac{Y}{X}\right)}$$ $$\left(\frac{Y}{X}\right)=V\qquad Y=VX$$ $$\frac{\text d Y}{\text d X}=X\frac{\text d V}{\text d X}+V$$ $$X\frac{\text d V}{\text d X}+V=\frac{2V-1}{4-3V}$$ $$X\frac{\text d V}{\text d X}=\frac{2V-1}{4-3V}-V$$ $$X\frac{\text d V}{\text d X}=\frac{2V-1}{4-3V}-V\left(\frac{4-3V}{4-3V}\right)$$ $$X\frac{\text d V}{\text d X}=\frac{3V^2-2V-1}{4-3V}$$ $$\frac{\text dX}{X}=\frac{4-3V}{3V^2-2V-1}\text dV$$ $$\int\frac{\text dX}{X}=\int\frac{4-3V}{3V^2-2V-1}\text dV$$ $$\ln X=\int\frac{4-3V}{3V^2-2V-1}\text dV$$ $$3V^2-2V-1=(V-1) \overline {\bigg)3V^2-2V-1}$$ $$\qquad3V+1$$$$=(V-1) \overline {\bigg)3V^2-2V-1}$$$$\quad\qquad3V^2-3V$$$$\qquad\qquad\qquad\qquad V-1$$$$\qquad\qquad\qquad\qquad \underline{V-1}$$$$\qquad\qquad\qquad\qquad 0$$ $$3V^2-2V-1=(V-1)(3V+1)$$ $$\frac{4-3V}{3V^2-2V-1}=\frac{4-3V}{(V-1)(3V+1)}=\frac{\alpha}{(V-1)}+\frac{\beta }{(3V+1)}=\frac{(3V+1)\alpha+(V-1)\beta}{(V-1)(3V+1)}$$ $$4-3V=(3V+1)\alpha+(V-1)\beta$$  $$4-3V=3V\alpha+\alpha +V\beta-\beta$$ $$4-3V=\alpha-\beta+(3\alpha +\beta)V$$ $$4=\alpha-\beta$$ $$-3=3\alpha+\beta$$ $$4+\beta=\alpha$$ $$-3=3(4+\beta)+\beta$$ $$-15=4\beta$$ $$\beta=\frac{-15}{4}$$ $$4=\alpha-\left(\frac{-15}{4}\right)$$ $$\alpha=4-\frac{15}{4}=\frac{16}{4}-\frac{15}{4}=\frac14$$ $$\ln X=\int\frac{1/4}{V-1}-\frac{15/4}{3V+1}\text d V$$ $$\ln X=\frac{1}{4}\int\frac{1}{V-1}-\frac{15}{3V+1}\text d V$$ $$4\ln X=\ln(V-1)-5\ln(3V+1)+c$$ $$\ln \left(X^4\right)=\ln\left({\frac{(V-1)}{(3V+1)^5}}\right)+c$$ $$X^4=\frac{V-1}{(3V+1)^5}\times e^c$$ $$(3V+1)^5X^4=e^c(V-1)$$ $$(3V+1)^5X^5=AX(V-1)$$ $$(3XV+X)^5=A(XV-X)$$ $$(3X\tfrac{Y}{X}+X)^5=A(X\tfrac{Y}{X}-X)$$ $$(3Y+X)^5=A(Y-X)$$ $$(3(y-2)+(x-3))^5=A((y-2)-(x-3))$$ $$(3y+x-9)^5=A(y-x+1)$$,Is there a simpler method to this solve this equation $$(x-2y+1)\text dx+(4x-3y-6)\text dy=0$$ $$\frac{\text dy}{\text dx}=\frac{2y-x-1}{4x-3y-6}$$ $$\frac{\text d Y}{ \text d X}=\frac{2(Y+k)-(X+h)-1}{4(X+h)-3(Y+k)-6}$$ $$\frac{\text d Y}{ \text d X}=\frac{2Y-X+(2k-h-1)}{4X-3Y+(4h-3k-6)}$$ $$2k-h-1=0\qquad4h-3k-6=0$$ $$h=2k-1\qquad 4(2k-1)-3k-6=0$$ $$5k-10=0\qquad k=2\qquad h=2(2)-1\qquad h=3$$ $$y=Y+2\qquad\qquad x=X+3$$ $$Y=y-2\qquad\qquad X=x-3$$ $$\frac{\text d Y}{\text d X}=\frac{2Y-X}{4X-3Y}=\frac{2\left(\frac{Y}{X}\right)-1}{4-3\left(\frac{Y}{X}\right)}$$ $$\left(\frac{Y}{X}\right)=V\qquad Y=VX$$ $$\frac{\text d Y}{\text d X}=X\frac{\text d V}{\text d X}+V$$ $$X\frac{\text d V}{\text d X}+V=\frac{2V-1}{4-3V}$$ $$X\frac{\text d V}{\text d X}=\frac{2V-1}{4-3V}-V$$ $$X\frac{\text d V}{\text d X}=\frac{2V-1}{4-3V}-V\left(\frac{4-3V}{4-3V}\right)$$ $$X\frac{\text d V}{\text d X}=\frac{3V^2-2V-1}{4-3V}$$ $$\frac{\text dX}{X}=\frac{4-3V}{3V^2-2V-1}\text dV$$ $$\int\frac{\text dX}{X}=\int\frac{4-3V}{3V^2-2V-1}\text dV$$ $$\ln X=\int\frac{4-3V}{3V^2-2V-1}\text dV$$ $$3V^2-2V-1=(V-1) \overline {\bigg)3V^2-2V-1}$$ $$\qquad3V+1$$$$=(V-1) \overline {\bigg)3V^2-2V-1}$$$$\quad\qquad3V^2-3V$$$$\qquad\qquad\qquad\qquad V-1$$$$\qquad\qquad\qquad\qquad \underline{V-1}$$$$\qquad\qquad\qquad\qquad 0$$ $$3V^2-2V-1=(V-1)(3V+1)$$ $$\frac{4-3V}{3V^2-2V-1}=\frac{4-3V}{(V-1)(3V+1)}=\frac{\alpha}{(V-1)}+\frac{\beta }{(3V+1)}=\frac{(3V+1)\alpha+(V-1)\beta}{(V-1)(3V+1)}$$ $$4-3V=(3V+1)\alpha+(V-1)\beta$$  $$4-3V=3V\alpha+\alpha +V\beta-\beta$$ $$4-3V=\alpha-\beta+(3\alpha +\beta)V$$ $$4=\alpha-\beta$$ $$-3=3\alpha+\beta$$ $$4+\beta=\alpha$$ $$-3=3(4+\beta)+\beta$$ $$-15=4\beta$$ $$\beta=\frac{-15}{4}$$ $$4=\alpha-\left(\frac{-15}{4}\right)$$ $$\alpha=4-\frac{15}{4}=\frac{16}{4}-\frac{15}{4}=\frac14$$ $$\ln X=\int\frac{1/4}{V-1}-\frac{15/4}{3V+1}\text d V$$ $$\ln X=\frac{1}{4}\int\frac{1}{V-1}-\frac{15}{3V+1}\text d V$$ $$4\ln X=\ln(V-1)-5\ln(3V+1)+c$$ $$\ln \left(X^4\right)=\ln\left({\frac{(V-1)}{(3V+1)^5}}\right)+c$$ $$X^4=\frac{V-1}{(3V+1)^5}\times e^c$$ $$(3V+1)^5X^4=e^c(V-1)$$ $$(3V+1)^5X^5=AX(V-1)$$ $$(3XV+X)^5=A(XV-X)$$ $$(3X\tfrac{Y}{X}+X)^5=A(X\tfrac{Y}{X}-X)$$ $$(3Y+X)^5=A(Y-X)$$ $$(3(y-2)+(x-3))^5=A((y-2)-(x-3))$$ $$(3y+x-9)^5=A(y-x+1)$$,,['ordinary-differential-equations']
6,Solving a particular differential equation,Solving a particular differential equation,,"I have a differential equation: $$y''-\frac{3}{2(1+x)(2-x)}y'+\frac{3}{4(1+x)(2-x)}y-\frac{Kf(x)(1+x)^2y}{2x(1+x)(2-x)}=0$$ Here $K>0$ is a fixed constant and $f(x)$ is some (as yet) unknown function of $x$, which is in our hands to chose. What I want is that I should decide $f(x)$ suitably to find a solution $y(x)$ of the above with the condition $y(0)=1$ where $y(x)$ is a rational function of $x$, i.e. a quotient of two polynomials. The solution should not be free of $K$. I tried setting $f(x)$ so that the coefficient of $K$ becomes $1$ but the differential equation turned out to be so complicated that I could not solve it. Can anyone offer any help or suggestions please?","I have a differential equation: $$y''-\frac{3}{2(1+x)(2-x)}y'+\frac{3}{4(1+x)(2-x)}y-\frac{Kf(x)(1+x)^2y}{2x(1+x)(2-x)}=0$$ Here $K>0$ is a fixed constant and $f(x)$ is some (as yet) unknown function of $x$, which is in our hands to chose. What I want is that I should decide $f(x)$ suitably to find a solution $y(x)$ of the above with the condition $y(0)=1$ where $y(x)$ is a rational function of $x$, i.e. a quotient of two polynomials. The solution should not be free of $K$. I tried setting $f(x)$ so that the coefficient of $K$ becomes $1$ but the differential equation turned out to be so complicated that I could not solve it. Can anyone offer any help or suggestions please?",,[]
7,How to find eigenvector of one second order differential equation for Hermit?,How to find eigenvector of one second order differential equation for Hermit?,,"After read http://math.tut.fi/~piche/pde/pde.pdf , do not know how to calculate eigenvector How to find eigenvector of one second order differential equation? why some people use sin as eigenvector? is it only sin can be eigenvector? The problem is for eigenfunction expansion,  first step is finding eigenvalue and eigenvector,  but do not know how to calculate eigenvector for differential equation for example Maple code x*diff(f(x), x$2) + 2*x*diff(f(x),x) + f(x) = 0     x*diff(f(x), x$2) + 2*x*diff(f(x),x) + x = 0 Updated sol := dsolve(t*diff(phi(x),x$2)-x*diff(phi(x),x)+n*phi(x),phi(x)); phi := unapply(rhs(sol), x); BC := [phi(0)=0,phi(1)=0]; with(linalg): Ccoef := genmatrix(BC, [_C1,_C2]); CharEqn := det(Ccoef) = 0;  restart; sol := dsolve(t*diff(phi(x,t,n),x$2)-x*diff(phi(x,t,n),x)+n*phi(x,t,n),phi(x,t,n)); phi := unapply(rhs(sol), x); BC := [phi(0,0,0)=0,phi(1,1,1)=0]; with(linalg): Ccoef := genmatrix(BC, [_C1,_C2]); CharEqn := det(Ccoef) = 0; **sorry only Sunday have time to seriously read this file, i find the sin function coming from the step of calculating characteristic equation use pdf file's method to calculate above differential equation for eignvector, this equation is Hermit after tried, characteristic equation is zero, it imply no eigenvector i guess this calculation maple code has something wrong how to calculate this?** Updated 2 Originally i expect to find Hermit H(x) and then use sum(H*z^m/m!, m=0..infinity) to find a A*exp(B) where B is in term of z and t and it is just a simple formula now following the steps, i guess the H is the solution of green function about the expansion it become more compicated for H(x), and i find there is a D[2] but do not know where it come from. then do not know which step is H(x), i just guess vterm or vv sol := dsolve(t*diff(phi(x),x$2)-x*diff(phi(x),x)+n*phi(x),phi(x)); phi := unapply(rhs(sol),x); odetest(sol,ode); eq1:=limit(rhs(sol),x=0,right)=0; eq2:=eval(rhs(sol),x=1)=0; Ccoef := LinearAlgebra:-GenerateMatrix([eq1,eq2],[_C1,_C2]); CharEqn:=LinearAlgebra:-Determinant(%[1])=0; solve(CharEqn,t); step1 := map(xi->simplify(subs(t=RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))),xi)),Ccoef); with(linalg): NN := nullspace(step1); subs(_C1=NN[1][1],_C2=NN[1][2],t=RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))),phi(x));  phi := (n,t,x) -> KummerM(1/2-(1/2)*n, 3/2, (1/2)*x^2/RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))))*x;  assume(j,posint): interface(showassumed=0): Gterm := unapply(-phi(n,t,x)*phi(n,t,x)*exp(-lambda(j)*t)/int(phi(n,t,x)^2,x=0..1),(j,n,x,y,t)): G:=Sum(Gterm(j,n,x,y,t),j=1..infinity); vterm := int(D[2](Gterm)(n,1,x,t-tau),tau=0..t); vv := sum(Sum(op(n,vterm),j=1..infinity),n=1..2);","After read http://math.tut.fi/~piche/pde/pde.pdf , do not know how to calculate eigenvector How to find eigenvector of one second order differential equation? why some people use sin as eigenvector? is it only sin can be eigenvector? The problem is for eigenfunction expansion,  first step is finding eigenvalue and eigenvector,  but do not know how to calculate eigenvector for differential equation for example Maple code x*diff(f(x), x$2) + 2*x*diff(f(x),x) + f(x) = 0     x*diff(f(x), x$2) + 2*x*diff(f(x),x) + x = 0 Updated sol := dsolve(t*diff(phi(x),x$2)-x*diff(phi(x),x)+n*phi(x),phi(x)); phi := unapply(rhs(sol), x); BC := [phi(0)=0,phi(1)=0]; with(linalg): Ccoef := genmatrix(BC, [_C1,_C2]); CharEqn := det(Ccoef) = 0;  restart; sol := dsolve(t*diff(phi(x,t,n),x$2)-x*diff(phi(x,t,n),x)+n*phi(x,t,n),phi(x,t,n)); phi := unapply(rhs(sol), x); BC := [phi(0,0,0)=0,phi(1,1,1)=0]; with(linalg): Ccoef := genmatrix(BC, [_C1,_C2]); CharEqn := det(Ccoef) = 0; **sorry only Sunday have time to seriously read this file, i find the sin function coming from the step of calculating characteristic equation use pdf file's method to calculate above differential equation for eignvector, this equation is Hermit after tried, characteristic equation is zero, it imply no eigenvector i guess this calculation maple code has something wrong how to calculate this?** Updated 2 Originally i expect to find Hermit H(x) and then use sum(H*z^m/m!, m=0..infinity) to find a A*exp(B) where B is in term of z and t and it is just a simple formula now following the steps, i guess the H is the solution of green function about the expansion it become more compicated for H(x), and i find there is a D[2] but do not know where it come from. then do not know which step is H(x), i just guess vterm or vv sol := dsolve(t*diff(phi(x),x$2)-x*diff(phi(x),x)+n*phi(x),phi(x)); phi := unapply(rhs(sol),x); odetest(sol,ode); eq1:=limit(rhs(sol),x=0,right)=0; eq2:=eval(rhs(sol),x=1)=0; Ccoef := LinearAlgebra:-GenerateMatrix([eq1,eq2],[_C1,_C2]); CharEqn:=LinearAlgebra:-Determinant(%[1])=0; solve(CharEqn,t); step1 := map(xi->simplify(subs(t=RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))),xi)),Ccoef); with(linalg): NN := nullspace(step1); subs(_C1=NN[1][1],_C2=NN[1][2],t=RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))),phi(x));  phi := (n,t,x) -> KummerM(1/2-(1/2)*n, 3/2, (1/2)*x^2/RootOf(KummerM(1/2-(1/2)*n, 3/2, 1/(2*_Z))))*x;  assume(j,posint): interface(showassumed=0): Gterm := unapply(-phi(n,t,x)*phi(n,t,x)*exp(-lambda(j)*t)/int(phi(n,t,x)^2,x=0..1),(j,n,x,y,t)): G:=Sum(Gterm(j,n,x,y,t),j=1..infinity); vterm := int(D[2](Gterm)(n,1,x,t-tau),tau=0..t); vv := sum(Sum(op(n,vterm),j=1..infinity),n=1..2);",,"['ordinary-differential-equations', 'partial-differential-equations']"
8,"To express $f(x,z)=\sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{n!}$ as known functions",To express  as known functions,"f(x,z)=\sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{n!}","$\alpha,\beta >0$ $$f(x,z)=\sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{n!}$$ $$\frac{\partial{f(x,z)}}{\partial z}=\beta \sum \limits_{n=1}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta \sum \limits_{n=1}^\infty \frac{e^{-\alpha n^2 x+\beta n (2 \frac{ \alpha}{\beta} x+  z_1)}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} \sum \limits_{n=1}^\infty \frac{e^{-\alpha (n-1)^2 x+\beta (n-1) z_1}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} \sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z_1}}{n!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} f(x,z_1)$$ I do not know how to solve this kind  differential equations. Do you know how to solve that? Can we express the function as known functions such as Jacobi Theta Functions etc? Also  could you please share your knowledge about the function if you know it. Thanks a lot for answers EDIT: Another property is: $$-\alpha\frac{\partial^2{f(x,z)}}{\partial z^2}=\beta^2 \frac{\partial{f(x,z)}}{\partial x} $$","$\alpha,\beta >0$ $$f(x,z)=\sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{n!}$$ $$\frac{\partial{f(x,z)}}{\partial z}=\beta \sum \limits_{n=1}^\infty \frac{e^{-\alpha n^2 x+\beta n z}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta \sum \limits_{n=1}^\infty \frac{e^{-\alpha n^2 x+\beta n (2 \frac{ \alpha}{\beta} x+  z_1)}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} \sum \limits_{n=1}^\infty \frac{e^{-\alpha (n-1)^2 x+\beta (n-1) z_1}}{(n-1)!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} \sum \limits_{n=0}^\infty \frac{e^{-\alpha n^2 x+\beta n z_1}}{n!}$$ $$\frac{\partial{f(x,z)}}{\partial z}|_{z=2 \frac{ \alpha}{\beta} x+ z_1}=\beta e^{\alpha x+ \beta z_1} f(x,z_1)$$ I do not know how to solve this kind  differential equations. Do you know how to solve that? Can we express the function as known functions such as Jacobi Theta Functions etc? Also  could you please share your knowledge about the function if you know it. Thanks a lot for answers EDIT: Another property is: $$-\alpha\frac{\partial^2{f(x,z)}}{\partial z^2}=\beta^2 \frac{\partial{f(x,z)}}{\partial x} $$",,"['sequences-and-series', 'ordinary-differential-equations', 'partial-differential-equations', 'special-functions']"
9,Solve $\ddot\theta +k\sin(2\theta)=0$ given initial value and constraints,Solve  given initial value and constraints,\ddot\theta +k\sin(2\theta)=0,"How is it possible to deduce from the equation $$\ddot\theta +k\sin(2\theta)=0$$ where $\theta=\theta(t)$ and $\tan(\theta)={b(t)\over a(t)}$, $k$ is constant, and $a(0)=a_0$, $a(t)^2+ b(t)^2=a_0^2$.  that $a(t)=a_0\operatorname{sech}(c t)$ where $c$ is a constant? Thanks","How is it possible to deduce from the equation $$\ddot\theta +k\sin(2\theta)=0$$ where $\theta=\theta(t)$ and $\tan(\theta)={b(t)\over a(t)}$, $k$ is constant, and $a(0)=a_0$, $a(t)^2+ b(t)^2=a_0^2$.  that $a(t)=a_0\operatorname{sech}(c t)$ where $c$ is a constant? Thanks",,"['ordinary-differential-equations', 'trigonometry']"
10,Bessel differential equation with random parameter,Bessel differential equation with random parameter,,"I know that the following differential equation: $$x^2\frac{d^2y(x)}{dx^2}+x\frac{dy(x)}{dx}+(x^2-\alpha^2)y(x)$$ has the solution: $$y(x)=C_1\cdot J_\alpha(x)+C_2\cdot Y_\alpha(x)$$ In my case, the differential equation is of the same form, but the parameter $\alpha$ is a random variable having a Gaussian distribution with zero mean and variance $\sigma$. I have problems to find the distribution of the solution $y(x)$. Can someone give me a hint? Thanks.","I know that the following differential equation: $$x^2\frac{d^2y(x)}{dx^2}+x\frac{dy(x)}{dx}+(x^2-\alpha^2)y(x)$$ has the solution: $$y(x)=C_1\cdot J_\alpha(x)+C_2\cdot Y_\alpha(x)$$ In my case, the differential equation is of the same form, but the parameter $\alpha$ is a random variable having a Gaussian distribution with zero mean and variance $\sigma$. I have problems to find the distribution of the solution $y(x)$. Can someone give me a hint? Thanks.",,"['ordinary-differential-equations', 'stochastic-calculus']"
11,An ODE problem(Riccati Equation),An ODE problem(Riccati Equation),,For the sulotion of the OED: $\frac{dy}{dx}=1+x+y^2+xy$. This is a Riccati Equation! Anyone can give me hint?,For the sulotion of the OED: $\frac{dy}{dx}=1+x+y^2+xy$. This is a Riccati Equation! Anyone can give me hint?,,['ordinary-differential-equations']
12,Distinction between error estimator and error indicator,Distinction between error estimator and error indicator,,"When solving differential equations numerically one can incur discretization error and one can construct a posteriori error estimates to approximate the true error. There is a distinction often made between ""error estimators"" and ""error indicators"". What is the difference between the two?","When solving differential equations numerically one can incur discretization error and one can construct a posteriori error estimates to approximate the true error. There is a distinction often made between ""error estimators"" and ""error indicators"". What is the difference between the two?",,"['ordinary-differential-equations', 'terminology', 'computational-mathematics']"
13,Is there a strategy for solving a non-autonomous differential equation?,Is there a strategy for solving a non-autonomous differential equation?,,"I'm curious about techniques for solving a nonautonomous* system in the case of a non-linear differential equation.  There's a simple example in my textbook (Hirsch, Smale, Devaney) where we obtain the following nonautonomous equation (after linearizing about the origin) $$ x'(t) = x + y_0^2 e^{-2 t}$$ In this case, we simply guess a particular solution (which is obvious from the given equation) and everything follows through in a straightforward manner.  In a paper I'm working through I have an equation that looks something like $$X'(t) = \frac{t}{6} - \frac{(a + X)^2}{t^2},$$ and I'm not sure what strategies I should have at my disposal here.  The literature on non-autonomous non-linear systems seems to be rather scarce from the bit of googling I've done.  Any insight would be much appreciated. *By non-autonomous I simply mean there is a $t$ hanging around on the right-side of the equation.","I'm curious about techniques for solving a nonautonomous* system in the case of a non-linear differential equation.  There's a simple example in my textbook (Hirsch, Smale, Devaney) where we obtain the following nonautonomous equation (after linearizing about the origin) $$ x'(t) = x + y_0^2 e^{-2 t}$$ In this case, we simply guess a particular solution (which is obvious from the given equation) and everything follows through in a straightforward manner.  In a paper I'm working through I have an equation that looks something like $$X'(t) = \frac{t}{6} - \frac{(a + X)^2}{t^2},$$ and I'm not sure what strategies I should have at my disposal here.  The literature on non-autonomous non-linear systems seems to be rather scarce from the bit of googling I've done.  Any insight would be much appreciated. *By non-autonomous I simply mean there is a $t$ hanging around on the right-side of the equation.",,['ordinary-differential-equations']
14,Which branch of mathematics is this and what are the introductory references?,Which branch of mathematics is this and what are the introductory references?,,"I am self-studying a physics textbook on waves. While discussing solutions to linear homogeneous ODEs, the author talked about the exponential as ""irreducible"" solutions  and on a footnote, said that they were, formally, ""irreducible representations of the translation group"". I am understanding the material that is presented in the book, but I think I would like to explore the relevant area of mathematics alongside. Which branch of mathematics is this and what are the introductory references? Another persistent theme in the book is solving differential equations (oscillating systems, but a general approach would also be good for me) using arguments such as linearity and symmetry. What are good books on differential equations that approach the subject in a similar fashion and are accessible. I have studied calculus and ODEs before, but it was very ""application oriented"" and most of the material was presented as ""methods"" (i.e you have a linear 1st order ODE, you use integrating factor, etc). I know with a lot of contemplation I can connect some dots myself, but it will be helpful to have some guiding material.","I am self-studying a physics textbook on waves. While discussing solutions to linear homogeneous ODEs, the author talked about the exponential as ""irreducible"" solutions  and on a footnote, said that they were, formally, ""irreducible representations of the translation group"". I am understanding the material that is presented in the book, but I think I would like to explore the relevant area of mathematics alongside. Which branch of mathematics is this and what are the introductory references? Another persistent theme in the book is solving differential equations (oscillating systems, but a general approach would also be good for me) using arguments such as linearity and symmetry. What are good books on differential equations that approach the subject in a similar fashion and are accessible. I have studied calculus and ODEs before, but it was very ""application oriented"" and most of the material was presented as ""methods"" (i.e you have a linear 1st order ODE, you use integrating factor, etc). I know with a lot of contemplation I can connect some dots myself, but it will be helpful to have some guiding material.",,"['reference-request', 'ordinary-differential-equations', 'self-learning']"
15,Solving 2nd order ODE with non-constant coefficient of form 1/x,Solving 2nd order ODE with non-constant coefficient of form 1/x,,"I have a differential equation of the form $$a y'' + b y/x = E y$$ (The origin is a 1D Schrödinger equation for a potential of the form $-1/x$). I am only interested in the ground state energy, i.e. the lowest order solution. Is there a good, systematic way to tackle this? I used a lot of hand waving: I said that for $x \rightarrow \infty$, the potential term is negligible and the equation is a simple homogeneous 2nd order ODE with constant coefficients, which has solution $e^{-kx}$ for some $k$. So as an overall ansatz I choose  $$f(x)e^{-kx}$$, which yields  $$a (f'' - 2k f' + k^2 f) + b f/x = E f$$. I then argue -- that is where the hand-waving occurs -- that the ground state would have a polynomial of the lowest possible order for $f$. A constant (order $0$) is not possible, since then nothing cancels the $1/x$ in the equation, so I try the ansatz $f(x) = x$. With that, I can indeed solve the equation and obtain conditions for $k$ and $E$: $$-2ka + b = 0$$ $$ak^2 = E$$ This allows me to solve for $k$ and $E$. But is there a better, more rigorous way?","I have a differential equation of the form $$a y'' + b y/x = E y$$ (The origin is a 1D Schrödinger equation for a potential of the form $-1/x$). I am only interested in the ground state energy, i.e. the lowest order solution. Is there a good, systematic way to tackle this? I used a lot of hand waving: I said that for $x \rightarrow \infty$, the potential term is negligible and the equation is a simple homogeneous 2nd order ODE with constant coefficients, which has solution $e^{-kx}$ for some $k$. So as an overall ansatz I choose  $$f(x)e^{-kx}$$, which yields  $$a (f'' - 2k f' + k^2 f) + b f/x = E f$$. I then argue -- that is where the hand-waving occurs -- that the ground state would have a polynomial of the lowest possible order for $f$. A constant (order $0$) is not possible, since then nothing cancels the $1/x$ in the equation, so I try the ansatz $f(x) = x$. With that, I can indeed solve the equation and obtain conditions for $k$ and $E$: $$-2ka + b = 0$$ $$ak^2 = E$$ This allows me to solve for $k$ and $E$. But is there a better, more rigorous way?",,['ordinary-differential-equations']
16,problem: Linear equation question,problem: Linear equation question,,"the question is: $$(y^2+xy^3)\mathrm dx + (5y^2-xy+y^3\sin(y))\mathrm dy = 0$$ can any body tell me how to solve this linear equation?? when I tried to solve this the expression of integrating factor becomes too much difficult, may be i calculated it wrong...  Any help will be appreciated.  Thanks!","the question is: $$(y^2+xy^3)\mathrm dx + (5y^2-xy+y^3\sin(y))\mathrm dy = 0$$ can any body tell me how to solve this linear equation?? when I tried to solve this the expression of integrating factor becomes too much difficult, may be i calculated it wrong...  Any help will be appreciated.  Thanks!",,['ordinary-differential-equations']
17,Uniqueness mild solution of $\dot{x} = A x$,Uniqueness mild solution of,\dot{x} = A x,"Let $A$ be the infinitesimal generator of a $C_0$-semigroup $(S(t))_{t \geq 0}$. Now, for every $x_0 \in X$ the map $t \mapsto S(t) x_0$ is a mild solution of $$ \dot{x} = Ax, \quad x(0) = x_0.\tag{*} $$ Now, a continuous function $x: [0, \infty) \to X$ is called a mild solution of $\text{(*)}$ if $\int_0^t x(s) \, ds \in D(A)$ where $D(A)$ is the domain of $A$, $x(0) = x_0$ and $$x(t) - x(0) = A \int_0^t x(\tau) \, d\tau \text{ for all $t \geq 0$}.$$ Now, I have a proof of this but it uses Hille's theorem, but it is quite involved (needs a few tricks) and Hille's theorem is not elementary, does someone know an elementary proof of the uniqueness?","Let $A$ be the infinitesimal generator of a $C_0$-semigroup $(S(t))_{t \geq 0}$. Now, for every $x_0 \in X$ the map $t \mapsto S(t) x_0$ is a mild solution of $$ \dot{x} = Ax, \quad x(0) = x_0.\tag{*} $$ Now, a continuous function $x: [0, \infty) \to X$ is called a mild solution of $\text{(*)}$ if $\int_0^t x(s) \, ds \in D(A)$ where $D(A)$ is the domain of $A$, $x(0) = x_0$ and $$x(t) - x(0) = A \int_0^t x(\tau) \, d\tau \text{ for all $t \geq 0$}.$$ Now, I have a proof of this but it uses Hille's theorem, but it is quite involved (needs a few tricks) and Hille's theorem is not elementary, does someone know an elementary proof of the uniqueness?",,"['ordinary-differential-equations', 'semigroup-of-operators']"
18,solving a univariate equation with a sum of exponentials,solving a univariate equation with a sum of exponentials,,"I am interested in a method to find the roots of the following equation: \begin{equation} f(t) = \sum_{i=1}^n \alpha_i e^{\beta_i t} + \gamma t + \delta = 0. \end{equation} For my application, coefficients $\alpha_i$, $\beta_i$, $\gamma$, and $\delta$ are real.  $n$ is typically a small integer, say 10.  In particular I am interested in the smallest positive real root of $f$. For those interested, this equation arises when attempting to compute the point of intersection between the solution to the linear ODE \begin{align} \dot x(t) &= Ax(t) + b & (A = A^T) \\  x(0) &= x_0 \end{align} and the boundary of a set of linear constraints \begin{equation} Cx(t) \ge d. \end{equation} The initial point is always feasible $(Cx_0\ge d)$.  For my purposes, all matrices and vectors are real.","I am interested in a method to find the roots of the following equation: \begin{equation} f(t) = \sum_{i=1}^n \alpha_i e^{\beta_i t} + \gamma t + \delta = 0. \end{equation} For my application, coefficients $\alpha_i$, $\beta_i$, $\gamma$, and $\delta$ are real.  $n$ is typically a small integer, say 10.  In particular I am interested in the smallest positive real root of $f$. For those interested, this equation arises when attempting to compute the point of intersection between the solution to the linear ODE \begin{align} \dot x(t) &= Ax(t) + b & (A = A^T) \\  x(0) &= x_0 \end{align} and the boundary of a set of linear constraints \begin{equation} Cx(t) \ge d. \end{equation} The initial point is always feasible $(Cx_0\ge d)$.  For my purposes, all matrices and vectors are real.",,"['linear-algebra', 'ordinary-differential-equations', 'roots']"
19,Does $x'' + x + \arctan x = \lambda \sin t$ have a $2\pi$ periodic solution when $\lambda <2$？,Does  have a  periodic solution when ？,x'' + x + \arctan x = \lambda \sin t 2\pi \lambda <2,"It's easy to show that the equation $x'' + x + \arctan x = \lambda \sin t$ doesn't have a $2\pi$ periodic solution when $\lambda \geq 2$ : Suppose $\lambda \geq 2$ and $\varphi(t)$ is a $2\pi$ periodic solution then $$ \begin{align} \lambda \pi &= \lambda \int _{0}^{2\pi} \sin ^{2}t \, dt\\ &= \int _{0}^{2\pi} \varphi'' (t) \sin t + \varphi (t) \sin t + \arctan \varphi(t) \sin t \, dt\\ & = \int _{0}^{2\pi} \arctan \varphi(t) \sin t \, dt\\ & < \frac{\pi}{2} \int _{0}^{2\pi} |\sin t | \, dt\\ & = 2\pi \end{align} $$ And this is a contradiction since $\lambda \geq 2$ . What can we say about the $2\pi$ periodic solution when $\lambda <2$ ? I tried to solve this equation using mathematica and when $\lambda \geq 2$ there is no numerical solution while when $\lambda <2$ there is one, so I think that there should be one $2\pi$ periodic solution when $\lambda <2$ ，but I don't know how to prove it.","It's easy to show that the equation doesn't have a periodic solution when : Suppose and is a periodic solution then And this is a contradiction since . What can we say about the periodic solution when ? I tried to solve this equation using mathematica and when there is no numerical solution while when there is one, so I think that there should be one periodic solution when ，but I don't know how to prove it.","x'' + x + \arctan x = \lambda \sin t 2\pi \lambda \geq 2 \lambda \geq 2 \varphi(t) 2\pi 
\begin{align}
\lambda \pi &= \lambda \int _{0}^{2\pi} \sin ^{2}t \, dt\\
&= \int _{0}^{2\pi} \varphi'' (t) \sin t + \varphi (t) \sin t + \arctan \varphi(t) \sin t \, dt\\
& = \int _{0}^{2\pi} \arctan \varphi(t) \sin t \, dt\\
& < \frac{\pi}{2} \int _{0}^{2\pi} |\sin t | \, dt\\
& = 2\pi
\end{align}
 \lambda \geq 2 2\pi \lambda <2 \lambda \geq 2 \lambda <2 2\pi \lambda <2","['calculus', 'ordinary-differential-equations', 'analysis', 'periodic-functions']"
20,Dynamics of a sliding cube on the $XY$ and $YZ$ planes,Dynamics of a sliding cube on the  and  planes,XY YZ,"A cube with side length $a$ , is initially placed with one vertex at the origin, and its faces parallel to the coordinate planes ( $XY, XZ, YZ$ ) and totally lying in the first octant. Then its rotated and shifted such that its base which was initially coincident with the $XY$ plane, now makes an angle of $45^\circ$ with the $XY$ plane and the $YZ$ plane. The cube is then given an infinitesimal push in order to cause to start sliding without friction on both of these planes. Question :  Find the location of the center of the cube and the angle its base makes with the $XY$ plane as a function of time, from $t = 0$ till the final time the base is coincident with the $XY$ plane again. My Attempt: Assuming the cube has a uniform density, and mass $m$ , and that the gravitational acceleration is $g$ pointing the negative $Z$ direction. Also, let $(x,y,z)$ be the location of the center of mass of the cube. And let $\theta$ be the angle the base of the cube makes with the $XY$ plane. There will a reaction $F_z$ from the $XY$ plane, therefore, $  m \ddot{z} = F_z - m g $ and also, there will be a reaction $F_x$ from the $YZ$ wall pointing the positive $X$ direction, therefore, $ m \ddot{x} = F_x $ Now, taking the moments about the lower edge (the edge touching the $XY$ plane) , we get $ - m g (a/2) \cos \theta + F_x (a/2) \sin \theta = \tau_Y = I_Y \ddot{\theta} $ We need to relate $(x,y,z)$ with $\theta$ , and this given by $ (x,y,z) = ( (a/2) ( \cos \theta + \sin \theta ) , a/2 , (a/2) (\cos \theta + \sin \theta ) ) $ So that $ (\dot{x}, \dot{y}, \dot{z} ) = (a/2) \dot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta ) $ And $ (\ddot{x}, \ddot{y}, \ddot{z} ) = (a/2) \ddot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta ) + (a/2) ( \dot{\theta} )^2 ( - \cos \theta - \sin \theta, 0, - \cos \theta - \sin \theta ) $ The moment of inertia of a cube about an axis through its center is $\dfrac{m a^2}{6}$ .  Therefore, the moment of inertia about its edge is (by the parallel axis theorem) equal to $\dfrac{2 m a^2}{3} $ Plugging all this into moment equation, we get $- 6 g \cos \theta +  3 a  \bigg( \ddot{\theta}( \cos \theta - \sin \theta ) - (\dot{\theta})^2 (\cos \theta + \sin \theta) \bigg) = 4 a \ddot{\theta} $ And this is a second-order non-linear ordinary differential equation in $\theta(t)$ with initial conditions $\theta(0) = \dfrac{\pi}{4} $ and $ \dot{\theta}(0) = 0 $ .  It can be solved numerically with the Runge-Kutta 4th-Order method, with very precise results. That I all I could come up with.  Your comments on the correctness of this analysis, as well as alternative solutions are much appreciated.","A cube with side length , is initially placed with one vertex at the origin, and its faces parallel to the coordinate planes ( ) and totally lying in the first octant. Then its rotated and shifted such that its base which was initially coincident with the plane, now makes an angle of with the plane and the plane. The cube is then given an infinitesimal push in order to cause to start sliding without friction on both of these planes. Question :  Find the location of the center of the cube and the angle its base makes with the plane as a function of time, from till the final time the base is coincident with the plane again. My Attempt: Assuming the cube has a uniform density, and mass , and that the gravitational acceleration is pointing the negative direction. Also, let be the location of the center of mass of the cube. And let be the angle the base of the cube makes with the plane. There will a reaction from the plane, therefore, and also, there will be a reaction from the wall pointing the positive direction, therefore, Now, taking the moments about the lower edge (the edge touching the plane) , we get We need to relate with , and this given by So that And The moment of inertia of a cube about an axis through its center is .  Therefore, the moment of inertia about its edge is (by the parallel axis theorem) equal to Plugging all this into moment equation, we get And this is a second-order non-linear ordinary differential equation in with initial conditions and .  It can be solved numerically with the Runge-Kutta 4th-Order method, with very precise results. That I all I could come up with.  Your comments on the correctness of this analysis, as well as alternative solutions are much appreciated.","a XY, XZ, YZ XY 45^\circ XY YZ XY t = 0 XY m g Z (x,y,z) \theta XY F_z XY   m \ddot{z} = F_z - m g  F_x YZ X  m \ddot{x} = F_x  XY  - m g (a/2) \cos \theta + F_x (a/2) \sin \theta = \tau_Y = I_Y \ddot{\theta}  (x,y,z) \theta  (x,y,z) = ( (a/2) ( \cos \theta + \sin \theta ) , a/2 , (a/2) (\cos \theta + \sin \theta ) )   (\dot{x}, \dot{y}, \dot{z} ) = (a/2) \dot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta )   (\ddot{x}, \ddot{y}, \ddot{z} ) = (a/2) \ddot{\theta} ( \cos \theta - \sin \theta , 0, \cos \theta - \sin \theta ) + (a/2) ( \dot{\theta} )^2 ( - \cos \theta - \sin \theta, 0, - \cos \theta - \sin \theta )  \dfrac{m a^2}{6} \dfrac{2 m a^2}{3}  - 6 g \cos \theta +  3 a  \bigg( \ddot{\theta}( \cos \theta - \sin \theta ) - (\dot{\theta})^2 (\cos \theta + \sin \theta) \bigg) = 4 a \ddot{\theta}  \theta(t) \theta(0) = \dfrac{\pi}{4}   \dot{\theta}(0) = 0 ","['ordinary-differential-equations', 'physics', 'nonlinear-dynamics']"
21,Direction for solving 3rd order non-linear ode $f(f''' - f'') + f'^2 = 0$,Direction for solving 3rd order non-linear ode,f(f''' - f'') + f'^2 = 0,"What the title says - I was playing around with an equation and I ended up with the following $$ f(f''' - f'') + f'^2 = 0$$ It's nice because all the coefficients are $1$ I think, but that might not be exactly true either because of how it's multiplied. I've played around with it a bit but it's weird enough that I can't really make much progress on this. I did find that a solution for this is $f = c$ where $c$ is a constant. Then I get $c(0) + 0 = 0$ which is true but also doesn't feel like it's a complete solution. Is there a chance I could get some advice on how to proceed with this?","What the title says - I was playing around with an equation and I ended up with the following It's nice because all the coefficients are I think, but that might not be exactly true either because of how it's multiplied. I've played around with it a bit but it's weird enough that I can't really make much progress on this. I did find that a solution for this is where is a constant. Then I get which is true but also doesn't feel like it's a complete solution. Is there a chance I could get some advice on how to proceed with this?", f(f''' - f'') + f'^2 = 0 1 f = c c c(0) + 0 = 0,"['calculus', 'ordinary-differential-equations']"
22,multiple general solutions for this differential equation,multiple general solutions for this differential equation,,"In the differential equation below, one way to solve it is to replace y with cos(u) or sin(u). But, when I tried both cos(u) and sin(u), general solutions for y seem different. Is this expected or did I do something wrong? $ y{}' = \sqrt{1-y^{^{2}}}$ By the way, I got the following answers. When I replace y with sin(u), y=sin(x+c) where c is constant. When I replace y with cos(u), y=cos(-x+c) where c is constant.","In the differential equation below, one way to solve it is to replace y with cos(u) or sin(u). But, when I tried both cos(u) and sin(u), general solutions for y seem different. Is this expected or did I do something wrong? By the way, I got the following answers. When I replace y with sin(u), y=sin(x+c) where c is constant. When I replace y with cos(u), y=cos(-x+c) where c is constant.", y{}' = \sqrt{1-y^{^{2}}},['ordinary-differential-equations']
23,Harmonic oscillator differential equation question,Harmonic oscillator differential equation question,,"Consider a harmonic oscillator subject to a frictional force proportional to velocity: $$\ddot{x}+2\gamma\dot{x}+\omega^2x=0.$$ Here $\dot{x}$ and $\ddot{x}$ are $\frac{dx}{dt}$ and $\frac{d^2x}{dt^2}.$ Assume $\omega>\gamma.$ Interpreting this equation as a system of equations for two functions $x\left ( t\right ) $ and $\nu\left ( t\right ) = \dot{x} \left ( t\right ) $ , solve this system of equations and show that the solutions are: $$\begin{gathered} x\left(t\right)=\mathrm{e}^{-\gamma t}\left[x_0\cos\left(\Omega t\right)+x_0\frac\gamma\Omega\sin\left(\Omega t\right)+\nu_0\frac1\Omega\sin\left(\Omega t\right)\right]; \\ \nu\left(t\right)=\mathrm{e}^{-\gamma t}\left[-x_{0}\frac{\omega^{2}}\Omega\sin\left(\Omega t\right)+\nu_{0}\cos\left(\Omega t\right)-\nu_{0}\frac\gamma\Omega\sin\left(\Omega t\right)\right].  \end{gathered}$$ Here, $x_0= x\left ( 0\right ) $ and $\nu_0= \nu\left ( 0\right ) $ are initial conditions. What is the frequency $\Omega$ of these oscillations? My attempt: Let's introduce a new variable $v(t) = \dot{x}(t)$ so that we have $$\begin{cases}\dot{x}&=v\\\dot{v}&=-2\gamma v-\omega^2x&\end{cases}$$ Now, we have a system of first-order ordinary differential equations. We can rewrite this in matrix form as: $$\frac d{dt}\begin{bmatrix}x\\v\end{bmatrix}=\begin{bmatrix}0\times x+1\times v\\-\omega^2\times x+(-2\gamma)\times v\end{bmatrix}=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix}\begin{bmatrix}x\\v\end{bmatrix}$$ The solution to this system can be found by diagonalizing the coefficient matrix. Let's denote the coefficient matrix by $A$ : $$A=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix}$$ The eigenvalues $\lambda$ and eigenvectors $\mathbf{v}$ of $A$ satisfy the equation: $\det(A-\lambda I)=0$ , so we have $$\det\begin{pmatrix}-\lambda&1\\-\omega^2&-2\gamma-\lambda\end{pmatrix}=0 \implies\lambda=-\gamma\pm\sqrt{\gamma^2-\omega^2}$$ Since $\omega > \gamma$ , the term under the square root is negative, resulting in complex conjugate eigenvalues. Let's denote $\Omega = \sqrt{\omega^2 - \gamma^2}$ , so the eigenvalues become: $$\lambda=-\gamma\pm i\Omega.$$ Now, we can write down the general solution using these eigenvalues. The general solution has the form: $$\begin{bmatrix}x(t)\\v(t)\end{bmatrix}=c_1\begin{bmatrix}1\\-\gamma+i\Omega\end{bmatrix}e^{(-\gamma+i\Omega)t}+c_2\begin{bmatrix}1\\-\gamma-i\Omega\end{bmatrix}e^{(-\gamma-i\Omega)t}$$ $$\begin{aligned} &x\left(t\right) =c_1e^{(-\gamma+i\Omega)t}+c_2e^{(-\gamma-i\Omega)t}  \\ &v(t) =(-\gamma+i\Omega)c_1e^{(-\gamma+i\Omega)t}+(-\gamma-i\Omega)c_2e^{(-\gamma-i\Omega)t}  \end{aligned}$$ Then, finding the constants $c_1$ and $c_2$ in terms of $x_0$ and $v_0$ I get $$\frac{x_0 i \Omega+v_0+\gamma x_0}{2i \Omega}=c_1$$ and $$-\frac{v_0+\gamma x_0-i \Omega x_0}{2i \Omega}=c_2,$$ if I haven't made any errors. Thus plugging in and applying Euler's formula, $$\begin{gathered} x(t)=\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t)) \\ +\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t))  \end{gathered}$$ and $$\begin{gathered} \begin{aligned}v(t)=(-\gamma+i\Omega)\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t))\end{aligned} \\ +(-\gamma-i\Omega)\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t))  \end{gathered}$$ But I am having trouble simplifying it to get to the solutions as written. Btw, am I correct in assuming $\Omega$ as frequency in this physical context? Furthermore, how do we calculate the frequency $Ω$ of these oscillations?","Consider a harmonic oscillator subject to a frictional force proportional to velocity: Here and are and Assume Interpreting this equation as a system of equations for two functions and , solve this system of equations and show that the solutions are: Here, and are initial conditions. What is the frequency of these oscillations? My attempt: Let's introduce a new variable so that we have Now, we have a system of first-order ordinary differential equations. We can rewrite this in matrix form as: The solution to this system can be found by diagonalizing the coefficient matrix. Let's denote the coefficient matrix by : The eigenvalues and eigenvectors of satisfy the equation: , so we have Since , the term under the square root is negative, resulting in complex conjugate eigenvalues. Let's denote , so the eigenvalues become: Now, we can write down the general solution using these eigenvalues. The general solution has the form: Then, finding the constants and in terms of and I get and if I haven't made any errors. Thus plugging in and applying Euler's formula, and But I am having trouble simplifying it to get to the solutions as written. Btw, am I correct in assuming as frequency in this physical context? Furthermore, how do we calculate the frequency of these oscillations?","\ddot{x}+2\gamma\dot{x}+\omega^2x=0. \dot{x} \ddot{x} \frac{dx}{dt} \frac{d^2x}{dt^2}. \omega>\gamma. x\left ( t\right )  \nu\left ( t\right ) = \dot{x} \left ( t\right )  \begin{gathered}
x\left(t\right)=\mathrm{e}^{-\gamma t}\left[x_0\cos\left(\Omega t\right)+x_0\frac\gamma\Omega\sin\left(\Omega t\right)+\nu_0\frac1\Omega\sin\left(\Omega t\right)\right]; \\
\nu\left(t\right)=\mathrm{e}^{-\gamma t}\left[-x_{0}\frac{\omega^{2}}\Omega\sin\left(\Omega t\right)+\nu_{0}\cos\left(\Omega t\right)-\nu_{0}\frac\gamma\Omega\sin\left(\Omega t\right)\right]. 
\end{gathered} x_0= x\left ( 0\right )  \nu_0= \nu\left ( 0\right )  \Omega v(t) = \dot{x}(t) \begin{cases}\dot{x}&=v\\\dot{v}&=-2\gamma v-\omega^2x&\end{cases} \frac d{dt}\begin{bmatrix}x\\v\end{bmatrix}=\begin{bmatrix}0\times x+1\times v\\-\omega^2\times x+(-2\gamma)\times v\end{bmatrix}=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix}\begin{bmatrix}x\\v\end{bmatrix} A A=\begin{bmatrix}0&1\\-\omega^2&-2\gamma\end{bmatrix} \lambda \mathbf{v} A \det(A-\lambda I)=0 \det\begin{pmatrix}-\lambda&1\\-\omega^2&-2\gamma-\lambda\end{pmatrix}=0 \implies\lambda=-\gamma\pm\sqrt{\gamma^2-\omega^2} \omega > \gamma \Omega = \sqrt{\omega^2 - \gamma^2} \lambda=-\gamma\pm i\Omega. \begin{bmatrix}x(t)\\v(t)\end{bmatrix}=c_1\begin{bmatrix}1\\-\gamma+i\Omega\end{bmatrix}e^{(-\gamma+i\Omega)t}+c_2\begin{bmatrix}1\\-\gamma-i\Omega\end{bmatrix}e^{(-\gamma-i\Omega)t} \begin{aligned}
&x\left(t\right) =c_1e^{(-\gamma+i\Omega)t}+c_2e^{(-\gamma-i\Omega)t}  \\
&v(t) =(-\gamma+i\Omega)c_1e^{(-\gamma+i\Omega)t}+(-\gamma-i\Omega)c_2e^{(-\gamma-i\Omega)t} 
\end{aligned} c_1 c_2 x_0 v_0 \frac{x_0 i \Omega+v_0+\gamma x_0}{2i \Omega}=c_1 -\frac{v_0+\gamma x_0-i \Omega x_0}{2i \Omega}=c_2, \begin{gathered}
x(t)=\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t)) \\
+\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t)) 
\end{gathered} \begin{gathered}
\begin{aligned}v(t)=(-\gamma+i\Omega)\left(\frac{x_0i\Omega+v_0+\gamma x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)+i\sin(\Omega t))\end{aligned} \\
+(-\gamma-i\Omega)\left(-\frac{v_0+\gamma x_0-i\Omega x_0}{2i\Omega}\right)e^{-\gamma t}(\cos(\Omega t)-i\sin(\Omega t)) 
\end{gathered} \Omega Ω","['ordinary-differential-equations', 'solution-verification', 'eigenvalues-eigenvectors', 'physics', 'classical-mechanics']"
24,What is a numerical method to solve IVP at irregular singular point?,What is a numerical method to solve IVP at irregular singular point?,,"I have tried to search for numerical method to solve IVP with irregular singular point but didn't find any. What is the proper method I can use to solve such equations? $$y''+\frac{y'}{x^2}+y=x^3+6x+3$$ With initial conditions $$y(0)=0, y'(0)=0$$ Obviously, collocation method (which I know) won't work. I'm not interested in this particular problem, it's only an example.","I have tried to search for numerical method to solve IVP with irregular singular point but didn't find any. What is the proper method I can use to solve such equations? With initial conditions Obviously, collocation method (which I know) won't work. I'm not interested in this particular problem, it's only an example.","y''+\frac{y'}{x^2}+y=x^3+6x+3 y(0)=0, y'(0)=0","['ordinary-differential-equations', 'numerical-methods', 'singularity']"
25,Behaviour of the solution of a second order ODE,Behaviour of the solution of a second order ODE,,"I am currently studying the following second order ODE \begin{cases} \ddot y(x)\left(\ln(x) - 2\ln(y(x))\right) - 2\frac{(\dot y(x))^2}{y(x)} = 0 &\text{in }[0,T]\\ y(0) = 0\\ \dot y(T) = c \end{cases} for $c$ a positive constant and $T > 0$ and arbitrarily small constant. Clearly, solving this ODE explicitly is not doable. But actually for my purpose I don't need a whole solution but rather to understand the behaviour of $\dot y(x)$ near $0$ . I would like to know if $\lim_{x \to 0} \dot y(x) = 0 $ or not. Being not really familiar with the tools used in ODE theory, I was wondering if one of you guide me on this topic. I solved the ODE numerically using Maple and the software gave me the following plots where I fixed $T = 1/2$ and the initial conditions set at $c = 1, 1/2$ and $1/3$ corresponding to $1, 2$ and $3$ respectively. It seems that the slope at the origin is always strictly positive, whatever initial condition I set (if I decrease $c$ or $T$ , the behaviour remains the same). However, I have absolutely no idea how to do that. Any help?","I am currently studying the following second order ODE for a positive constant and and arbitrarily small constant. Clearly, solving this ODE explicitly is not doable. But actually for my purpose I don't need a whole solution but rather to understand the behaviour of near . I would like to know if or not. Being not really familiar with the tools used in ODE theory, I was wondering if one of you guide me on this topic. I solved the ODE numerically using Maple and the software gave me the following plots where I fixed and the initial conditions set at and corresponding to and respectively. It seems that the slope at the origin is always strictly positive, whatever initial condition I set (if I decrease or , the behaviour remains the same). However, I have absolutely no idea how to do that. Any help?","\begin{cases}
\ddot y(x)\left(\ln(x) - 2\ln(y(x))\right) - 2\frac{(\dot y(x))^2}{y(x)} = 0 &\text{in }[0,T]\\
y(0) = 0\\
\dot y(T) = c
\end{cases} c T > 0 \dot y(x) 0 \lim_{x \to 0} \dot y(x) = 0  T = 1/2 c = 1, 1/2 1/3 1, 2 3 c T","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives', 'graphing-functions']"
26,How to solve the differential equation given below,How to solve the differential equation given below,,"How to solve the following differential equation: $$ y' = \frac{\sqrt[3]{x-2y}}{\sin(x)} $$ We can rewrite it as $$ y= \frac{1}{2} (x- y'^3 \sin^3(x))$$ I tried to substitute $y'=p$ and then proceed, but I did not conclude anything. $\textbf{Edit:}$ I tried to rewrite it as: $$ dy - \frac{\sqrt[3]{x-2y}}{\sin x} dx=0 $$ And then find the integrating factor: $$ \mu = \mu (x) = e^{- \int \frac{\sqrt[3]{x-2y}}{\sin x}dx} $$ However there is no elementary way to solve the integral on the exponent. $\textbf{Edit 2}$ I saw this Calculate Laurent series for $1/ \sin(z)$ So one way is perhaps to decompose the function: $$ f(x) = \frac{1}{\sin(x)} $$ However I'm not familiar with solutions of DE with series and I don't know under what conditions we can do that.","How to solve the following differential equation: We can rewrite it as I tried to substitute and then proceed, but I did not conclude anything. I tried to rewrite it as: And then find the integrating factor: However there is no elementary way to solve the integral on the exponent. I saw this Calculate Laurent series for $1/ \sin(z)$ So one way is perhaps to decompose the function: However I'm not familiar with solutions of DE with series and I don't know under what conditions we can do that.", y' = \frac{\sqrt[3]{x-2y}}{\sin(x)}   y= \frac{1}{2} (x- y'^3 \sin^3(x)) y'=p \textbf{Edit:}  dy - \frac{\sqrt[3]{x-2y}}{\sin x} dx=0   \mu = \mu (x) = e^{- \int \frac{\sqrt[3]{x-2y}}{\sin x}dx}  \textbf{Edit 2}  f(x) = \frac{1}{\sin(x)} ,['ordinary-differential-equations']
27,Apply Power Series Solution Method on Solving ODE with non-integer x order term $ \frac{d^2y}{dx^2}+x^{1/2}y=0 $,Apply Power Series Solution Method on Solving ODE with non-integer x order term, \frac{d^2y}{dx^2}+x^{1/2}y=0 ,"Problem: Use series solution expansion to solve ODE in region x>0 $$ \frac{d^2y}{dx^2}+x^{1/2}y=0 $$ Attempts: Assume $y=\sum^\infty_{n=0} a_nx^n$ , then $\frac{d^2y}{dx^2}=\sum^\infty_{n=2} n(n-1)a_nx^{n-2}$ . The ODE can then be expressed in series expansion, followed by: $\sum^{\infty}_{n=2}n(n-1)a_nx^{n-2}+\sum^{\infty}_{n=2}a_nx^{n+1/2} =0$ by switching indic, we get $\sum^{\infty}_{n=0}(n+2)(n+1)a_{n+2}x^{n}+a_nx^{n+1/2}=0$ Here is where I get confused: for the equation to hold, every n substitution in the expression will result in 0 and the recurrence relationship can be obtained. But here, say if I let $n=0$ , the recurrence relationship will become involved with x, as shown, $2\cdot 1\cdot a_2=a_n\cdot x^{1/2}$ But I doubt it is correct because then the constants $a_n$ will become x-dependent. I also attempted to solve it on Mathematica, it seemed like somehow the solution can be found to be Bessel functions, $y(x)=C_1\sqrt{x}\;\Gamma (\frac{3}{5}) J_{-\frac{2}{5}}(\frac{4x^{5/4}}{5})+C_2\sqrt{x}\;\Gamma (\frac{7}{5}) J_{\frac{2}{5}}(\frac{4x^{5/4}}{5})$ but I really am not sure whether that is correct or how to get there","Problem: Use series solution expansion to solve ODE in region x>0 Attempts: Assume , then . The ODE can then be expressed in series expansion, followed by: by switching indic, we get Here is where I get confused: for the equation to hold, every n substitution in the expression will result in 0 and the recurrence relationship can be obtained. But here, say if I let , the recurrence relationship will become involved with x, as shown, But I doubt it is correct because then the constants will become x-dependent. I also attempted to solve it on Mathematica, it seemed like somehow the solution can be found to be Bessel functions, but I really am not sure whether that is correct or how to get there", \frac{d^2y}{dx^2}+x^{1/2}y=0  y=\sum^\infty_{n=0} a_nx^n \frac{d^2y}{dx^2}=\sum^\infty_{n=2} n(n-1)a_nx^{n-2} \sum^{\infty}_{n=2}n(n-1)a_nx^{n-2}+\sum^{\infty}_{n=2}a_nx^{n+1/2} =0 \sum^{\infty}_{n=0}(n+2)(n+1)a_{n+2}x^{n}+a_nx^{n+1/2}=0 n=0 2\cdot 1\cdot a_2=a_n\cdot x^{1/2} a_n y(x)=C_1\sqrt{x}\;\Gamma (\frac{3}{5}) J_{-\frac{2}{5}}(\frac{4x^{5/4}}{5})+C_2\sqrt{x}\;\Gamma (\frac{7}{5}) J_{\frac{2}{5}}(\frac{4x^{5/4}}{5}),"['calculus', 'ordinary-differential-equations', 'power-series']"
28,"Why is this the ""most general"" first order ODE that can be solved?","Why is this the ""most general"" first order ODE that can be solved?",,"Braun's book (p. 58) says that the ODE $$\frac{d}{dt}[\Phi(t,y)]=0\tag{1}$$ is clearly the most general first-order differential equation that we can solve . Of course, all equations having that form can be  (implicitly) solved: $$\Phi(t,y)=c.$$ However, the converse (stated by Braun) is not ""clear"" for me. So, how could we explain it? For example, the most general first-order ODE is $$F(t,y,y')=0.\tag{2}$$ Suppose that this ODE have a solution $$G(t,y)=0.\tag{3}$$ Doesn't Braun's statement mean that (2) and (3) imply that $(2)$ can be written in the form (1)? If so, how to prove it? If not, what does Braun mean?","Braun's book (p. 58) says that the ODE is clearly the most general first-order differential equation that we can solve . Of course, all equations having that form can be  (implicitly) solved: However, the converse (stated by Braun) is not ""clear"" for me. So, how could we explain it? For example, the most general first-order ODE is Suppose that this ODE have a solution Doesn't Braun's statement mean that (2) and (3) imply that can be written in the form (1)? If so, how to prove it? If not, what does Braun mean?","\frac{d}{dt}[\Phi(t,y)]=0\tag{1} \Phi(t,y)=c. F(t,y,y')=0.\tag{2} G(t,y)=0.\tag{3} (2)",['ordinary-differential-equations']
29,The functional differential equation $f'(x) = f(f(x))$ [duplicate],The functional differential equation  [duplicate],f'(x) = f(f(x)),"This question already has answers here : Does a non-trivial solution exist for $f'(x)=f(f(x))$? (6 answers) Closed 8 months ago . In Maths 505 I found an interesting functional differential equation (FDE) problem ( https://youtu.be/C6fZVwqhbnE?si=Trxk-KYNmQUeEPCv ) which asks for the solution of the equation $$f'(x) = f(f(x))\tag{1}$$ No further conditions were given. I had never before studied this type of equations. Here is what I did so far. The first attempt was suggested by the author of the problem: make the power ansatz with two parameters $$f(x) = a x^{b }\tag{2}$$ Then we have $f'(x) = a b x^{b-1}$ and $f(f(x)) = a( a x^{b })^{b} = a^{1+b} x^{b^2}$ from which we deduce $b^2=b-1$ and $b=a^{b}$ . These equations have the solutions $$b_{\pm} = \frac{1\pm i \sqrt{3}}{2}=e^{\pm \frac{i \pi}{3}}\tag{3a}$$ and $$a=b^{1/b}\tag{3b}$$ So that we have found two solutions to $(1)$ $$f_\pm(x) = b_\pm^{1/b_\pm} x^{b_\pm}\tag{3c}$$ Expanding this using Euler's formula we can calculate the real and imaginary parts of $f(x)$ . This shows that we have obtained two complex functions of the argument $x$ which we assumed he to to be real. We can study the properties of the solution, plot graphs etc. but one problem appears - and remains - urgently: a dfferential question of first order must have one free parameter to adapt the solution to an initial condition. Here I am stuck since I can't accomodate an arbitrary constant in the solution found by a power ansatz. Second approach: Let us assume the initial condition $$f(1)=1\tag{4}$$ This simplifies the arithmetic considerably. Indeed, we find $f'(1) = f(f(1)) = f(1) = 1$ , and next $f''(x) = \frac{d}{dx} f(f(x)) = f'(f(x))\cdot  f'(x) = f(f(f(x))) \cdot f(f(x))$ giving $f''(1) = 1$ , $f'''(1) = 2$ etc. Continuing this procedure (conveniently done in Mathematica) we find for $f(1)$ and the first nine derivatives of $f$ at $x=1$ the strongly increasing series $$f^{(k)}(1)|_{k=0}^{k=9} = \{1,1,2,7,37,269,2535,29738,421790,7076459\}\tag{5}$$ This series is contained in OEIS as https://oeis.org/A001028 and reads A001028     E.g.f. satisfies A'(x) = 1 + A(A(x)), A(0)=0. This remark confirms our suspection: ""The e.g.f. is diverging"". Hence we have not obtained a valid series expansion around $x=1$ . Here I'm stuck again. Hopefully someone can solve the problem with, a proper initial condition, say f(1) = 1/2$. EDIT I am grateful to a user (who deleted the contribution shortly after publication) who notified me that the problem was studied earlier by Alex Jones in: https://www.quora.com/How-can-I-solve-f-x-f-f-x where my first attempt was described. The general problem was not attacked there.","This question already has answers here : Does a non-trivial solution exist for $f'(x)=f(f(x))$? (6 answers) Closed 8 months ago . In Maths 505 I found an interesting functional differential equation (FDE) problem ( https://youtu.be/C6fZVwqhbnE?si=Trxk-KYNmQUeEPCv ) which asks for the solution of the equation No further conditions were given. I had never before studied this type of equations. Here is what I did so far. The first attempt was suggested by the author of the problem: make the power ansatz with two parameters Then we have and from which we deduce and . These equations have the solutions and So that we have found two solutions to Expanding this using Euler's formula we can calculate the real and imaginary parts of . This shows that we have obtained two complex functions of the argument which we assumed he to to be real. We can study the properties of the solution, plot graphs etc. but one problem appears - and remains - urgently: a dfferential question of first order must have one free parameter to adapt the solution to an initial condition. Here I am stuck since I can't accomodate an arbitrary constant in the solution found by a power ansatz. Second approach: Let us assume the initial condition This simplifies the arithmetic considerably. Indeed, we find , and next giving , etc. Continuing this procedure (conveniently done in Mathematica) we find for and the first nine derivatives of at the strongly increasing series This series is contained in OEIS as https://oeis.org/A001028 and reads A001028     E.g.f. satisfies A'(x) = 1 + A(A(x)), A(0)=0. This remark confirms our suspection: ""The e.g.f. is diverging"". Hence we have not obtained a valid series expansion around . Here I'm stuck again. Hopefully someone can solve the problem with, a proper initial condition, say f(1) = 1/2$. EDIT I am grateful to a user (who deleted the contribution shortly after publication) who notified me that the problem was studied earlier by Alex Jones in: https://www.quora.com/How-can-I-solve-f-x-f-f-x where my first attempt was described. The general problem was not attacked there.","f'(x) = f(f(x))\tag{1} f(x) = a x^{b }\tag{2} f'(x) = a b x^{b-1} f(f(x)) = a( a x^{b })^{b} = a^{1+b} x^{b^2} b^2=b-1 b=a^{b} b_{\pm} = \frac{1\pm i \sqrt{3}}{2}=e^{\pm \frac{i \pi}{3}}\tag{3a} a=b^{1/b}\tag{3b} (1) f_\pm(x) = b_\pm^{1/b_\pm} x^{b_\pm}\tag{3c} f(x) x f(1)=1\tag{4} f'(1) = f(f(1)) = f(1) = 1 f''(x) = \frac{d}{dx} f(f(x)) = f'(f(x))\cdot  f'(x) = f(f(f(x))) \cdot f(f(x)) f''(1) = 1 f'''(1) = 2 f(1) f x=1 f^{(k)}(1)|_{k=0}^{k=9} = \{1,1,2,7,37,269,2535,29738,421790,7076459\}\tag{5} x=1","['ordinary-differential-equations', 'functional-equations']"
30,how to visualise DAE as ODE on manifold,how to visualise DAE as ODE on manifold,,"In many papers and textbooks, there is always a brief comment that differential-algebraic equations (DAE) can be considered as a system of ordinary differential equation (ODE) on manifold. But then nowhere is it explained what it actually means. I desperately want to see what does it mean and visualize that manifold. What are the necessary steps to do? Is the algebraic part defining some sort of manifold and if so, how? Some (possibly schematic only) concrete example is much appreciated. Edit. To keep things simple, lets focus only on index-1 DAEs as suggested in the comment. Edit2 Is it like the follwoing? For every (consistent) initial values, I have to find all solutions and the result is the manifold? Is the resulting quite different from what I would get when DAE is further reduced do ODE?","In many papers and textbooks, there is always a brief comment that differential-algebraic equations (DAE) can be considered as a system of ordinary differential equation (ODE) on manifold. But then nowhere is it explained what it actually means. I desperately want to see what does it mean and visualize that manifold. What are the necessary steps to do? Is the algebraic part defining some sort of manifold and if so, how? Some (possibly schematic only) concrete example is much appreciated. Edit. To keep things simple, lets focus only on index-1 DAEs as suggested in the comment. Edit2 Is it like the follwoing? For every (consistent) initial values, I have to find all solutions and the result is the manifold? Is the resulting quite different from what I would get when DAE is further reduced do ODE?",,"['ordinary-differential-equations', 'differential-algebraic-equations']"
31,Monodromy Properties of Heun Functions in the Singularity Parameter,Monodromy Properties of Heun Functions in the Singularity Parameter,,"Hello I'm studying a physical problem involving Heun equation and the local Heun functions. Heun equation is the Fuchsian differential equation \begin{equation} \frac {d^2w}{dz^2} +  \left[\frac{\gamma}{z}+ \frac{\delta}{z-1} + \frac{\epsilon}{z-a} \right]  \frac {dw}{dz}  + \frac {\alpha \beta z -q} {z(z-1)(z-a)} w = 0 \end{equation} with the constraint \begin{equation} \epsilon=\alpha+\beta-\gamma-\delta+1 \end{equation} The solution which is holomorphic and equals $1$ at the singular point $z = 0$ is called the local Heun function denoted as $H\ell(a, q, \alpha, \beta, \gamma, \delta, z)$ . I am interested in the properties of this function as a function of the ""singularity parameter"" $a$ , in particular branch points and monodromies. Is there any paper or reference I should read?","Hello I'm studying a physical problem involving Heun equation and the local Heun functions. Heun equation is the Fuchsian differential equation with the constraint The solution which is holomorphic and equals at the singular point is called the local Heun function denoted as . I am interested in the properties of this function as a function of the ""singularity parameter"" , in particular branch points and monodromies. Is there any paper or reference I should read?","\begin{equation}
\frac {d^2w}{dz^2} + 
\left[\frac{\gamma}{z}+ \frac{\delta}{z-1} + \frac{\epsilon}{z-a} \right] 
\frac {dw}{dz} 
+ \frac {\alpha \beta z -q} {z(z-1)(z-a)} w = 0
\end{equation} \begin{equation}
\epsilon=\alpha+\beta-\gamma-\delta+1
\end{equation} 1 z = 0 H\ell(a, q, \alpha, \beta, \gamma, \delta, z) a",['ordinary-differential-equations']
32,Connection between Lame equation with Weierstrass and elliptic sine,Connection between Lame equation with Weierstrass and elliptic sine,,"Lame function is the solution of the following equation, $$\frac{d^2w}{dz^2}+\left(A+B\wp(z)\right)w=0,$$ where $A$ and $B$ are constants and $\wp(z)$ is the Weierstrass elliptic function. Wiki says that the most important case corresponds to $B\wp(z)=-\kappa^2\mathrm{sn}^2(\kappa,z)$ , where $\mathrm{sn}(\kappa,z)$ is the elliptic sine function. Moreover, the Lame equation with elliptic sine function can be quite simply transformed into the Mathieu equation by setting $\kappa\rightarrow 0^{+}$ , which gives $$\kappa^2\mathrm{sn}^2(\kappa,z)=\kappa^2\sin z+\mathcal{O}(\kappa^3).$$ I would like to understand the following: Is it possible to obtain the elliptic sine function from the Weierstrass elliptic function? Is it possible to obtain the sine/cosine function from the Weierstrass elliptic function? I have a feeling that it is tightly related to the Inozemtsev limit, which corresponds to setting one of the periods in Weierstrass function to infinity with some additional tricks.","Lame function is the solution of the following equation, where and are constants and is the Weierstrass elliptic function. Wiki says that the most important case corresponds to , where is the elliptic sine function. Moreover, the Lame equation with elliptic sine function can be quite simply transformed into the Mathieu equation by setting , which gives I would like to understand the following: Is it possible to obtain the elliptic sine function from the Weierstrass elliptic function? Is it possible to obtain the sine/cosine function from the Weierstrass elliptic function? I have a feeling that it is tightly related to the Inozemtsev limit, which corresponds to setting one of the periods in Weierstrass function to infinity with some additional tricks.","\frac{d^2w}{dz^2}+\left(A+B\wp(z)\right)w=0, A B \wp(z) B\wp(z)=-\kappa^2\mathrm{sn}^2(\kappa,z) \mathrm{sn}(\kappa,z) \kappa\rightarrow 0^{+} \kappa^2\mathrm{sn}^2(\kappa,z)=\kappa^2\sin z+\mathcal{O}(\kappa^3).","['ordinary-differential-equations', 'elliptic-functions']"
33,Convergence of Euler scheme for ODEs,Convergence of Euler scheme for ODEs,,"Consider the initial value problem for the ODE \begin{align} \frac{dy}{dt}&=f(y), \\ y(0)&=y_0, \end{align} where $f$ is a Lipschitz continuous function on $\mathbb{R}.$ Since $f$ is globally Lipschitz, the IVP admits a unique solution globally on $\mathbb R.$ Now, consider the Euler approximation given by \begin{align} y^{n+1}=y^ n+ \Delta t f(y^n). \end{align} For any $T>0$ and a $\Delta t > 0,$ with $N_0 \Delta t=T,$ how to prove the following estimate: \begin{align} \left|y(T)-y^{N_0}\right| \leq C \Delta t, \end{align} where $C>0$ and is independent of $\Delta t.$ P.S.: I am looking for a very general proof, i.e, for any time $T>0$ and with minimal restrictions on $f.$ I would greatly appreciate a clean proof or a precise reference that contains the proof.","Consider the initial value problem for the ODE where is a Lipschitz continuous function on Since is globally Lipschitz, the IVP admits a unique solution globally on Now, consider the Euler approximation given by For any and a with how to prove the following estimate: where and is independent of P.S.: I am looking for a very general proof, i.e, for any time and with minimal restrictions on I would greatly appreciate a clean proof or a precise reference that contains the proof.","\begin{align}
\frac{dy}{dt}&=f(y), \\
y(0)&=y_0,
\end{align} f \mathbb{R}. f \mathbb R. \begin{align}
y^{n+1}=y^
n+ \Delta t f(y^n).
\end{align} T>0 \Delta t > 0, N_0 \Delta t=T, \begin{align}
\left|y(T)-y^{N_0}\right| \leq C \Delta t,
\end{align} C>0 \Delta t. T>0 f.","['ordinary-differential-equations', 'analysis', 'numerical-methods', 'eulers-method']"
34,How to solve the integral in solving this differential equation? [closed],How to solve the integral in solving this differential equation? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $$ y' = -x^2y + \tanh(x+3) $$ $$ a(x) = -x^2 $$ $$ A(x) = -x^3/3 $$ $$ y = e^{-x^3/3} \int{e^{x^3/3} \tanh(x+3)\mathrm{d}x} $$ I can't solve this integral. I tried to write $\tanh(x+3)$ as exponential but couldn't solve it anyway.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I can't solve this integral. I tried to write as exponential but couldn't solve it anyway.", y' = -x^2y + \tanh(x+3)   a(x) = -x^2   A(x) = -x^3/3   y = e^{-x^3/3} \int{e^{x^3/3} \tanh(x+3)\mathrm{d}x}  \tanh(x+3),"['integration', 'ordinary-differential-equations']"
35,Solving the laplacian over a square,Solving the laplacian over a square,,"I am preparing for an upcoming exam, and I was solving the following boundary value problem $$\Delta u(x,y)=0,\ (x,y)\in [0,\pi]^2$$ $$u_y(x,0)=0,\ u_y(x,\pi)=0$$ $$u(0,y)=0$$ $$u(\pi,y)=\cos^2y$$ I tried separation of variables, asusming $u(x,y)=X(x)Y(y)$ and solved it, putting in the boundary conditions and got (assuming the solutions are not trivially zero) $$X(x)=A(e^{nx}-e^{-nx})$$ $$Y(y)=D\cos(ny)$$ for some constants $A,D$ . I then put them together and wrote $$u(x,y)=\sum_{n=1}^\infty A_n(e^{nx}-e^{-nx})\cos(ny)$$ where the $A_n$ are constants to be determined. Am I correct in writing this? Assuming it is correct, I put in the last boundary condition and using the orthogonality of the fourier basis, I get $$A_n=\frac{2/\pi}{e^{n\pi}-e^{-n\pi}}\int_0^\pi \cos^2y\cos(ny)\ \mathrm dy$$ which only survives when $n=2$ (for the rest, it is $0$ ). For $n=2$ , it gives $$A_2=\frac{1}{2(e^{2\pi}-e^{-2\pi})}$$ Then plugging in this in the solution, I have $$u(x,y)=\frac1{2(e^{2\pi}-e^{-2\pi})}(e^{2x}-e^{-2x})\cos(2y)$$ But here is the problem. The final solution doesn't seem to be satisfying the last boundary condition. It gives $$u(\pi,y)=\frac12\cos(2y)=\cos^2y-\frac12$$ where there is an extra factor of $1/2$ . I have been looking at it for hours but I can't find where the error creeps in. Any help is appreciated!","I am preparing for an upcoming exam, and I was solving the following boundary value problem I tried separation of variables, asusming and solved it, putting in the boundary conditions and got (assuming the solutions are not trivially zero) for some constants . I then put them together and wrote where the are constants to be determined. Am I correct in writing this? Assuming it is correct, I put in the last boundary condition and using the orthogonality of the fourier basis, I get which only survives when (for the rest, it is ). For , it gives Then plugging in this in the solution, I have But here is the problem. The final solution doesn't seem to be satisfying the last boundary condition. It gives where there is an extra factor of . I have been looking at it for hours but I can't find where the error creeps in. Any help is appreciated!","\Delta u(x,y)=0,\ (x,y)\in [0,\pi]^2 u_y(x,0)=0,\ u_y(x,\pi)=0 u(0,y)=0 u(\pi,y)=\cos^2y u(x,y)=X(x)Y(y) X(x)=A(e^{nx}-e^{-nx}) Y(y)=D\cos(ny) A,D u(x,y)=\sum_{n=1}^\infty A_n(e^{nx}-e^{-nx})\cos(ny) A_n A_n=\frac{2/\pi}{e^{n\pi}-e^{-n\pi}}\int_0^\pi \cos^2y\cos(ny)\ \mathrm dy n=2 0 n=2 A_2=\frac{1}{2(e^{2\pi}-e^{-2\pi})} u(x,y)=\frac1{2(e^{2\pi}-e^{-2\pi})}(e^{2x}-e^{-2x})\cos(2y) u(\pi,y)=\frac12\cos(2y)=\cos^2y-\frac12 1/2","['ordinary-differential-equations', 'partial-differential-equations', 'solution-verification', 'harmonic-functions']"
36,First order differential equations - issue,First order differential equations - issue,,"I understand the existence and uniqueness theorem as follows: Let $\dot{x}=F(t,x)$ and $F'_x(t,x)$ be continuous on a rectangle $\Gamma$ , and let $(t_0,x_0)$ be points (of our initial value problem) inside $\Gamma$ . Then, there exists exactly one local solution passing through $(t_0,x_0)$ . The rectangle $\Gamma$ is defined as: $\Gamma=\{(t,x):|t-t_0| \le a, |x-x_0| \le b\}$ , where $a,b>0$ are constants. If these conditions are met, there exists a unique solution for our initial value problem, that might only exist for an interval smaller than $(t_0-a,t_0+a)$ . Indeed, defining $M= max_{(t,x) \in \Gamma} |F(t,x)|$ , and $r=min \{ a,b/M \}$ , our initial value problem has a unique solution on $(t_0 - r, t_0 +r)$ Now, by considering the following initial value problem: $\dot{x}=x^2$ , and $x(0)=1$ , there exists a unique solution because $x^2$ is continuous everywhere (existence), and $2x$ is still continuous everywhere(uniqueness). Thus, these two functions are trivially continuous around $(0,1)$ . The particular solution yields: $x(t)= \frac{1}{1-t}$ , which exists for all $t \in(- \infty,1)$ . However, if I look at the theorem, I always expect a solution symmetric around $t_0$ , i.e. $(t_0 - r, t_0 +r)$ . Why is this not the case? Update: Source: Knut Sydsaeter, Peter Hammond,Atle Seierstad, - Further Mathematics for Economic Analysis (2009)","I understand the existence and uniqueness theorem as follows: Let and be continuous on a rectangle , and let be points (of our initial value problem) inside . Then, there exists exactly one local solution passing through . The rectangle is defined as: , where are constants. If these conditions are met, there exists a unique solution for our initial value problem, that might only exist for an interval smaller than . Indeed, defining , and , our initial value problem has a unique solution on Now, by considering the following initial value problem: , and , there exists a unique solution because is continuous everywhere (existence), and is still continuous everywhere(uniqueness). Thus, these two functions are trivially continuous around . The particular solution yields: , which exists for all . However, if I look at the theorem, I always expect a solution symmetric around , i.e. . Why is this not the case? Update: Source: Knut Sydsaeter, Peter Hammond,Atle Seierstad, - Further Mathematics for Economic Analysis (2009)","\dot{x}=F(t,x) F'_x(t,x) \Gamma (t_0,x_0) \Gamma (t_0,x_0) \Gamma \Gamma=\{(t,x):|t-t_0| \le a, |x-x_0| \le b\} a,b>0 (t_0-a,t_0+a) M= max_{(t,x) \in \Gamma} |F(t,x)| r=min \{ a,b/M \} (t_0 - r, t_0 +r) \dot{x}=x^2 x(0)=1 x^2 2x (0,1) x(t)= \frac{1}{1-t} t \in(- \infty,1) t_0 (t_0 - r, t_0 +r)","['ordinary-differential-equations', 'derivatives']"
37,Phase spaces for second order differential equations - Arnold's ODE,Phase spaces for second order differential equations - Arnold's ODE,,"this is my first post here, so please excuse me if I do anything wrong I'm a high school student, and my question is about phase planes, which I got interested in after learning about differential equations. In Arnold's ODE book, he introduces the notion of a phase plane here: This phase plane seems to be different from other vector fields I've seen, which has normally axes $x$ against $t$ , or $y$ against $x$ . It seems to be different because it is '2 dimensional' and that this case arises for second order differential equations. Gilbert Strang's course on YouTube also introduces this, but doesn't explicitly explain why the axes of the phase plane are $y'$ and $y$ . A comment in the video said that it is because 'any second order ODE can be transformed into a system of 2 first order ODEs'. My question is that I do not understand the process of defining the coordinates $x_1$ and $x_2$ of the phase space to be the position and velocity, instead of maybe the velocity and time, or using a three dimensional vector field, with position velocity and time. I know that time is specific to this case, which is time-dependent, but is time somehow built into the solution to the differential equation? I am not sure where time fits into this and how we can interpret the phase space without time. It would be helpful enough to point me in any further reading I would have to do to understand this, instead of a full answer if possible. I have tried to look for an explanation which relates creating phase plots to transforming second order equations to a system of first order ones, but to no avail. Thanks very much to anyone who can help! References: What is ""Phase Space"" in differential equations and classical mechanics? What is the defininition of phase Space in simple terms? Arnold's ODE computation of phase velocity","this is my first post here, so please excuse me if I do anything wrong I'm a high school student, and my question is about phase planes, which I got interested in after learning about differential equations. In Arnold's ODE book, he introduces the notion of a phase plane here: This phase plane seems to be different from other vector fields I've seen, which has normally axes against , or against . It seems to be different because it is '2 dimensional' and that this case arises for second order differential equations. Gilbert Strang's course on YouTube also introduces this, but doesn't explicitly explain why the axes of the phase plane are and . A comment in the video said that it is because 'any second order ODE can be transformed into a system of 2 first order ODEs'. My question is that I do not understand the process of defining the coordinates and of the phase space to be the position and velocity, instead of maybe the velocity and time, or using a three dimensional vector field, with position velocity and time. I know that time is specific to this case, which is time-dependent, but is time somehow built into the solution to the differential equation? I am not sure where time fits into this and how we can interpret the phase space without time. It would be helpful enough to point me in any further reading I would have to do to understand this, instead of a full answer if possible. I have tried to look for an explanation which relates creating phase plots to transforming second order equations to a system of first order ones, but to no avail. Thanks very much to anyone who can help! References: What is ""Phase Space"" in differential equations and classical mechanics? What is the defininition of phase Space in simple terms? Arnold's ODE computation of phase velocity",x t y x y' y x_1 x_2,"['ordinary-differential-equations', 'vector-fields', 'mathematical-modeling']"
38,"Find the Green’s function $G(x, t)$ using a Laplace transform in $t$",Find the Green’s function  using a Laplace transform in,"G(x, t) t","Given the wave equation: $G_{tt} - G_{xx} = \delta(x-x_{0})\delta(t-t_{0})$ , with conditions: $-\infty < x < \infty,t>0$ , $G(x,0) = 0, G_{t}(x,0) = 0$ . Use Laplace transform to find the Green's function for this equation. -- After applying the Laplace transform to the equation in $t$ , I get: $$s^{2}\bar{G} - sG(x,0) - G_{t}(x,0) - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0})$$ By the initial condition: $$s^{2}\bar{G} - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0})$$ But I am struggling here, after solving the homogenous equation $s^{2}\bar{G} - \bar{G''} = 0$ , I got $\bar{G} = A(s)e^{xs}+B(s)e^{-xs}$ . And $\bar{G}$ makes the left hand side of the above equation zero. Are there better ways to deal with the left hand side?","Given the wave equation: , with conditions: , . Use Laplace transform to find the Green's function for this equation. -- After applying the Laplace transform to the equation in , I get: By the initial condition: But I am struggling here, after solving the homogenous equation , I got . And makes the left hand side of the above equation zero. Are there better ways to deal with the left hand side?","G_{tt} - G_{xx} = \delta(x-x_{0})\delta(t-t_{0}) -\infty < x < \infty,t>0 G(x,0) = 0, G_{t}(x,0) = 0 t s^{2}\bar{G} - sG(x,0) - G_{t}(x,0) - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0}) s^{2}\bar{G} - \bar{G_{xx}} = e^{-st_{0}}\delta(x-x_{0}) s^{2}\bar{G} - \bar{G''} = 0 \bar{G} = A(s)e^{xs}+B(s)e^{-xs} \bar{G}","['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform', 'greens-function']"
39,Kernel of Spherically Symmetric Diffusion Equation,Kernel of Spherically Symmetric Diffusion Equation,,"I'm trying to find the Kernel of the 3D Diffusion Equation but my lecture notes are unclear on the procedure. $$\frac{\partial u}{\partial t} = D \nabla ^2u$$ $$u(\vec{x}, 0) = R(|\vec{x}|)$$ I need to show that the solutions might be written as: $$u(|\vec{x}|, t) = \int ^{+\infty}_0 dr r^2 R(r)K(r, |\vec{x}|; t)$$ where $K$ is the kernel which I also need to find explicitly. THIS IS WHAT I HAVE TRIED: I started by transforming the equation: $\frac{\partial \tilde{u}}{\partial t} = -Dk^2\tilde{u} \Leftrightarrow \tilde{u}(k,t) = \tilde{u}(k,0)\exp(-Dk^2t)$ Then using Radial / Spherical Polar Fourier Transform: $$u(x,t) = \sqrt{\frac{2}{\pi}}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\tilde{u}(k,t)dk = \sqrt{\frac{2}{\pi}}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\tilde{u}(k,0)\exp{(-Dk^2t)}dk$$ $$u(x,t) = \frac{2}{\pi}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\exp{(-Dk^2t)}dk\int^{+\infty}_0r^2\frac{\sin(kr)}{kr}R(r)dr$$ $$u(x,t) = \int^{+\infty}_0r^2R(r)K(r, |\vec{x}|;t)dr$$ where here the kernel is: $\frac{2}{\pi}\int^{+\infty}_{0}k^2\frac{\sin(kr)}{k^2r^2}\exp{(-Dk^2t)}dk$ I don't know how to handle the integral and I don't even know if what I am doing is good. Any ideas? Many thanks!",I'm trying to find the Kernel of the 3D Diffusion Equation but my lecture notes are unclear on the procedure. I need to show that the solutions might be written as: where is the kernel which I also need to find explicitly. THIS IS WHAT I HAVE TRIED: I started by transforming the equation: Then using Radial / Spherical Polar Fourier Transform: where here the kernel is: I don't know how to handle the integral and I don't even know if what I am doing is good. Any ideas? Many thanks!,"\frac{\partial u}{\partial t} = D \nabla ^2u u(\vec{x}, 0) = R(|\vec{x}|) u(|\vec{x}|, t) = \int ^{+\infty}_0 dr r^2 R(r)K(r, |\vec{x}|; t) K \frac{\partial \tilde{u}}{\partial t} = -Dk^2\tilde{u} \Leftrightarrow \tilde{u}(k,t) = \tilde{u}(k,0)\exp(-Dk^2t) u(x,t) = \sqrt{\frac{2}{\pi}}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\tilde{u}(k,t)dk = \sqrt{\frac{2}{\pi}}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\tilde{u}(k,0)\exp{(-Dk^2t)}dk u(x,t) = \frac{2}{\pi}\int^{+\infty}_0k^2\frac{\sin(kr)}{kr}\exp{(-Dk^2t)}dk\int^{+\infty}_0r^2\frac{\sin(kr)}{kr}R(r)dr u(x,t) = \int^{+\infty}_0r^2R(r)K(r, |\vec{x}|;t)dr \frac{2}{\pi}\int^{+\infty}_{0}k^2\frac{\sin(kr)}{k^2r^2}\exp{(-Dk^2t)}dk","['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-transform']"
40,Integral transform for finding what polynomial solves a differential equation.,Integral transform for finding what polynomial solves a differential equation.,,"If the following kind of differential equations: $$ \frac{d^2f}{d^2t} + \frac{df}{dt} + f = 0 \\ $$ Have a solution made up by a linear combination of exponentials; which can be easily find by using the laplace transform: $$ \mathcal L\{ f(t) \} = \int_{0}^{\infty} { f(t) e^{-st} dt} \\ $$ I was wondering if, assuming the solution of the following differential equation: $$ t^2\frac{d^2f}{d^2t} + t\frac{df}{dt} + f = 0 $$ Is a linear combinations of terms like: $$ f(t) = A t^r $$ Could be silly to define a transform like this: $$ J\{ f(t) \} \equiv \int_{0}^{\infty} { f(t) \; t^{s} dt} \\ $$ My problem is: $$ J\{ tf'(t) \} = \int_{0}^{\infty} { f'(t) \; t^{s + 1} dt} = t^{s + 1} f(t)\Bigg |_{t = 0}^{\infty} - (s + 1) \int_{0}^{\infty} f(t) \; t^{s} dt $$ This integral do not converge, so I cannot find the solution of the equation. Is is possible to assume that: $$ \lim_{t \to \infty} t^{s + 1} f(t) = 0 $$ It is possible to apply this transform, or it is a non-sense? I was expecting something like (take this as an example): $$ \hat{f}(s) = \frac{s^2 f''(0) + s f'(0)}{s^2 - s + 1}  $$ And last but not least, how would I find the inverse transform?","If the following kind of differential equations: Have a solution made up by a linear combination of exponentials; which can be easily find by using the laplace transform: I was wondering if, assuming the solution of the following differential equation: Is a linear combinations of terms like: Could be silly to define a transform like this: My problem is: This integral do not converge, so I cannot find the solution of the equation. Is is possible to assume that: It is possible to apply this transform, or it is a non-sense? I was expecting something like (take this as an example): And last but not least, how would I find the inverse transform?","
\frac{d^2f}{d^2t} + \frac{df}{dt} + f = 0 \\
 
\mathcal L\{ f(t) \} = \int_{0}^{\infty} { f(t) e^{-st} dt} \\
 
t^2\frac{d^2f}{d^2t} + t\frac{df}{dt} + f = 0
 
f(t) = A t^r
 
J\{ f(t) \} \equiv \int_{0}^{\infty} { f(t) \; t^{s} dt} \\
 
J\{ tf'(t) \} = \int_{0}^{\infty} { f'(t) \; t^{s + 1} dt} = t^{s + 1} f(t)\Bigg |_{t = 0}^{\infty} - (s + 1) \int_{0}^{\infty} f(t) \; t^{s} dt
 
\lim_{t \to \infty} t^{s + 1} f(t) = 0
 
\hat{f}(s) = \frac{s^2 f''(0) + s f'(0)}{s^2 - s + 1} 
",['ordinary-differential-equations']
41,"Solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g(t)=1-e^{-at}$.",Solution  of the ODE  is a concave function of .,"x:\mathbb R_+\to [0,1] \dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at}) g(t)=1-e^{-at}","Is any solution $x_t:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g_t=1-e^{-at}$ for any $a>0$ ? Note: A solution has to converge to the critical point $x_* = \sqrt{3}-1$ , otherwise it would explode (and leave the interval $[0,1]$ , so it won't be a solution). There is in fact unique such solution. This is a simplification of the question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. , which is a simplification of my original problem: System of quadratic autonomous ODEs - convexity of the solution curve A slightly more general question would be: Is any solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - b(1-x) (1-e^{-at})$ a concave function of $g_t=1-e^{-at}$ for any $a,b>0$ ? I will appreciate a solution or a reference to any similar problems.","Is any solution of the ODE is a concave function of for any ? Note: A solution has to converge to the critical point , otherwise it would explode (and leave the interval , so it won't be a solution). There is in fact unique such solution. This is a simplification of the question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. , which is a simplification of my original problem: System of quadratic autonomous ODEs - convexity of the solution curve A slightly more general question would be: Is any solution of the ODE a concave function of for any ? I will appreciate a solution or a reference to any similar problems.","x_t:\mathbb R_+\to [0,1] \dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at}) g_t=1-e^{-at} a>0 x_* = \sqrt{3}-1 [0,1] x:\mathbb R_+\to [0,1] \dot x = \tfrac 1 2 x^2 - b(1-x) (1-e^{-at}) g_t=1-e^{-at} a,b>0","['calculus', 'ordinary-differential-equations', 'convex-analysis', 'examples-counterexamples', 'nonlinear-analysis']"
42,System of quadratic autonomous ODEs - convexity of the solution curve,System of quadratic autonomous ODEs - convexity of the solution curve,,"Crossposted on MathOverflow Problem: For a given parameter $a>0$ , consider the following autonomous system of ODEs for $(x,y,z): \mathbb R_+\to [0,1)^3$ : \begin{align*} \dot{x}_t &= (1-x_t) (z_t-x_ty_t) &=:F^x(x_t,y_t,z_t) \\ \dot{y}_t &= \tfrac 1 2 y_t^2 - (a+x_ty_t)(1-y_t) &=:F^y(x_t,y_t,z_t) \\ \dot{z}_t &= \tfrac 1 2 z_t^2 - \tfrac 1 2 y_t^2 + (a+x_ty_t)z_t &=:F^z(x_t,y_t,z_t) \end{align*} together with the initial condition $x_0=0$ . By solution I refer to the profile $(x_t,y_t,z_t)\in [0,1)^3$ that is defined for all $t\geq 0$ (there is unique point $(0,y_0,z_0)\in [0,1)^3$ such that the orbit passing through it does not explode). Prove (analytically) that a) the curve consisting of points $(x_t,y_t)_{t\geq 0}$ is a convex; b) the curve consisting of points $(x_t,z_t)_{t\geq 0}$ is decreasing. Question: Reference to any related paper / book. Figure 1: Solution as a function of $t$ (for $a=0.1$ ). $t$ "" /> Figure 2: Solution represented by functions $Y,Z$ such that $(x_t,y_t,z_t)= (x_t,Y(x_t),Z(x_t))$ for all $t\geq 0$ (the red point represents the critical point of the system): $Y$ and $Z$ ."" /> Note: I called the system quadratic in the title because substituting $u_t = x_t y_t$ converts the original (cubic) system into a quadratic one. Context: I obtained this system of ODEs when studying patent race with private information : Two firms compete making a patent, they exert a costly effort that translates into the hazard rate of making a discovery. To patent a firm has to make two consecutive discoveries, the first one that does so wins (reward 1) whilst the other loses (reward 0). Having made the first discovery is firms private information, the rival only infers a posterior belief about it based on the fact that nobody patented yet. In the presented equations this belief is represented by variable $x_t$ ; $y_t$ is the effort of a firm that has made the first discovery already; $z_t$ is the effort of a firm that has made no discovery yet, and the parameter $a>0$ is the rate at which future payoffs are being discounted. Basic Observations I have analytical proofs of the following properties: The system has a unique critical point $(x_*,y_*,z_*)\in [0,1)^3$ . For every solution, $t\mapsto x_t$ is increasing and the trajectory $(x_t,y_t,z_t)$ converges to the critical point $(x_*,y_*,z_*)$ . The Jacobian $J$ of the system has one negative eigenvalue $\lambda_1$ and the other eigenvalues have strictly positive real parts. Thus, by Hartman-Grobman Theorem the eigenvector $v=(v^x,v^y,v^z)$ associated with $\lambda_1$ determines the direction in which a trajectory can converge to the critical point, and there is unique local solution near the critical point with $x_t$ increasing. The local solution can be extended till $x=0$ is reaches, so there exist unique functions $Y(x),Z(x):[0,x_*)\to [0,1)$ such that $y_t = Y(x_t)$ and $z_t = Z(x_t)$ for any solution of the initial problem with the initial condition $x_0=\hat x$ . Current state of research: I found a way to prove some basic properties like that $Y(x)$ is increasing and $Z(x)$ is decreasing and I'm working on proving that $Y(x)$ is convex. However, I prove everything using methods that I develop using just elementary mathematical analysis. I belive there should be some standard methods that I could apply instead of developing my own. More details on the methods that I use: The method that I've been using utilises the fact that at a given $\alpha>0$ and $x\in [0,x^*]$ $$y\mapsto G(y;x,\alpha)=F^y(x,y,Z(x)) - \alpha F^x(x,y,Z(x))$$ is a quadratic function and it can be shown that $Y'(x) \geq \alpha$ whenever $G(y;x,\alpha)>0$ . Then I consider the isocline $\hat Y(x;\alpha)$ given implicitly by $G(\hat Y(x;\alpha);x,\alpha)=0$ , i.e. the value that $y$ would need to have at $x$ in order for the slope $Y'(x;y)$ of the orbit passing by $(x,y,Z(x))$ to be equal to $\alpha$ . In the next step I show that $\hat Y'(x;\alpha) < \alpha$ and so the curves $Y(x)$ and $\hat Y(x;\alpha)$ only cross at the critical point $x_*$ . This way I can show some desirable properties of $Y(x)$ , using some assumptions on $Z(x)$ . Then I use similar method to prove properties of $Z(x)$ using assumptions on $Y(x)$ . Finally, I show that both properties of $Y(x)$ and $Z(x)$ hold (without the argument being cyclical). I tried to formulate an abstraction / simplification of this question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. Solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g(t)=1-e^{-at}$. Objective: I'd be grateful for any reference to a related studies.","Crossposted on MathOverflow Problem: For a given parameter , consider the following autonomous system of ODEs for : together with the initial condition . By solution I refer to the profile that is defined for all (there is unique point such that the orbit passing through it does not explode). Prove (analytically) that a) the curve consisting of points is a convex; b) the curve consisting of points is decreasing. Question: Reference to any related paper / book. Figure 1: Solution as a function of (for ). $t$ "" /> Figure 2: Solution represented by functions such that for all (the red point represents the critical point of the system): $Y$ and ."" /> Note: I called the system quadratic in the title because substituting converts the original (cubic) system into a quadratic one. Context: I obtained this system of ODEs when studying patent race with private information : Two firms compete making a patent, they exert a costly effort that translates into the hazard rate of making a discovery. To patent a firm has to make two consecutive discoveries, the first one that does so wins (reward 1) whilst the other loses (reward 0). Having made the first discovery is firms private information, the rival only infers a posterior belief about it based on the fact that nobody patented yet. In the presented equations this belief is represented by variable ; is the effort of a firm that has made the first discovery already; is the effort of a firm that has made no discovery yet, and the parameter is the rate at which future payoffs are being discounted. Basic Observations I have analytical proofs of the following properties: The system has a unique critical point . For every solution, is increasing and the trajectory converges to the critical point . The Jacobian of the system has one negative eigenvalue and the other eigenvalues have strictly positive real parts. Thus, by Hartman-Grobman Theorem the eigenvector associated with determines the direction in which a trajectory can converge to the critical point, and there is unique local solution near the critical point with increasing. The local solution can be extended till is reaches, so there exist unique functions such that and for any solution of the initial problem with the initial condition . Current state of research: I found a way to prove some basic properties like that is increasing and is decreasing and I'm working on proving that is convex. However, I prove everything using methods that I develop using just elementary mathematical analysis. I belive there should be some standard methods that I could apply instead of developing my own. More details on the methods that I use: The method that I've been using utilises the fact that at a given and is a quadratic function and it can be shown that whenever . Then I consider the isocline given implicitly by , i.e. the value that would need to have at in order for the slope of the orbit passing by to be equal to . In the next step I show that and so the curves and only cross at the critical point . This way I can show some desirable properties of , using some assumptions on . Then I use similar method to prove properties of using assumptions on . Finally, I show that both properties of and hold (without the argument being cyclical). I tried to formulate an abstraction / simplification of this question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. Solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g(t)=1-e^{-at}$. Objective: I'd be grateful for any reference to a related studies.","a>0 (x,y,z): \mathbb R_+\to [0,1)^3 \begin{align*}
\dot{x}_t &= (1-x_t) (z_t-x_ty_t)
&=:F^x(x_t,y_t,z_t) \\
\dot{y}_t &= \tfrac 1 2 y_t^2 - (a+x_ty_t)(1-y_t)
&=:F^y(x_t,y_t,z_t) \\
\dot{z}_t &= \tfrac 1 2 z_t^2 - \tfrac 1 2 y_t^2 + (a+x_ty_t)z_t
&=:F^z(x_t,y_t,z_t)
\end{align*} x_0=0 (x_t,y_t,z_t)\in [0,1)^3 t\geq 0 (0,y_0,z_0)\in [0,1)^3 (x_t,y_t)_{t\geq 0} (x_t,z_t)_{t\geq 0} t a=0.1 Y,Z (x_t,y_t,z_t)= (x_t,Y(x_t),Z(x_t)) t\geq 0 Z u_t = x_t y_t x_t y_t z_t a>0 (x_*,y_*,z_*)\in [0,1)^3 t\mapsto x_t (x_t,y_t,z_t) (x_*,y_*,z_*) J \lambda_1 v=(v^x,v^y,v^z) \lambda_1 x_t x=0 Y(x),Z(x):[0,x_*)\to [0,1) y_t = Y(x_t) z_t = Z(x_t) x_0=\hat x Y(x) Z(x) Y(x) \alpha>0 x\in [0,x^*] y\mapsto G(y;x,\alpha)=F^y(x,y,Z(x)) - \alpha F^x(x,y,Z(x)) Y'(x) \geq \alpha G(y;x,\alpha)>0 \hat Y(x;\alpha) G(\hat Y(x;\alpha);x,\alpha)=0 y x Y'(x;y) (x,y,Z(x)) \alpha \hat Y'(x;\alpha) < \alpha Y(x) \hat Y(x;\alpha) x_* Y(x) Z(x) Z(x) Y(x) Y(x) Z(x)","['real-analysis', 'ordinary-differential-equations', 'convex-analysis', 'dynamical-systems', 'differential-games']"
43,"If an initial value problem has a solution on $[0,a)$ for all $a>0$, will it have a solution on the whole $[0,\infty)$?","If an initial value problem has a solution on  for all , will it have a solution on the whole ?","[0,a) a>0 [0,\infty)","Consider the initial value problem on $[0,\infty)$ : $$x'(t)=f(t, x(t)) \qquad x(0)=0,\label{1}\tag{$*$}$$ where $f:(0,\infty)\times\mathbb R\to\mathbb R$ is a continuous function. Assume that for every $a>0$ , there exists a $C^1$ solution $x(t)$ to the initial value problem \eqref{1} on $[0,a)$ with $x\in C([0,a))\cap C^1((0,a))$ (the solution is not necessarily unique). Can we conclude that there is a solution to the initial value problem \eqref{1} on the whole $[0,\infty)$ ? This problem comes from one of my friends. Attempt. I think the answer is “no” . To find a counter-example, I tried to construct an $f$ such that the initial value problem $(*)$ has a solution $x(t)$ on $[0,T)$ with the property $$x(t)\sim \frac1{T-t}, \qquad t\sim T-$$ for all $T>0$ ; and such that $(*)$ doesn't have global solutions on the whole $[0,\infty)$ . However, the function $\frac1{T-t}$ doesn't take the value $0$ at $t=0$ , so I considered instead $$x(t)=\frac{\eta(t)}{T-t}, \qquad t\in[0,T)\tag{1}$$ for some good function $\eta(t)$ with $\eta(0)=0$ . Now the function $(1)$ satisfies $$x'(t)=\frac{\eta(t)}{(T-t)^2}+\frac{\eta'(t)}{T-t},\qquad t\in[0,T),$$ which impiles that $x$ is a solution to the following initial value problem on $[0, T)$ : $$x'(t)=\frac{1}{\eta(t)}(x(t))^2+\frac{\eta'(t)}{\eta(t)}x(t),\qquad x(0)=0. \tag{2}$$ However, the initial value problem $(2)$ has a trivial global solution $x\equiv0$ and many non-trivial global solutions $x(t)=\frac{\eta(t)}{C-t}$ on $t\in[0,\infty)$ for $C<0$ . Therefore, this method doesn't work . Any help would be appreciated! Note . This problem has been cross-posted in MO and has been answered by Saúl RM . Nevertheless, more examples or comments are welcome!","Consider the initial value problem on : where is a continuous function. Assume that for every , there exists a solution to the initial value problem \eqref{1} on with (the solution is not necessarily unique). Can we conclude that there is a solution to the initial value problem \eqref{1} on the whole ? This problem comes from one of my friends. Attempt. I think the answer is “no” . To find a counter-example, I tried to construct an such that the initial value problem has a solution on with the property for all ; and such that doesn't have global solutions on the whole . However, the function doesn't take the value at , so I considered instead for some good function with . Now the function satisfies which impiles that is a solution to the following initial value problem on : However, the initial value problem has a trivial global solution and many non-trivial global solutions on for . Therefore, this method doesn't work . Any help would be appreciated! Note . This problem has been cross-posted in MO and has been answered by Saúl RM . Nevertheless, more examples or comments are welcome!","[0,\infty) x'(t)=f(t, x(t)) \qquad x(0)=0,\label{1}\tag{*} f:(0,\infty)\times\mathbb R\to\mathbb R a>0 C^1 x(t) [0,a) x\in C([0,a))\cap C^1((0,a)) [0,\infty) f (*) x(t) [0,T) x(t)\sim \frac1{T-t}, \qquad t\sim T- T>0 (*) [0,\infty) \frac1{T-t} 0 t=0 x(t)=\frac{\eta(t)}{T-t}, \qquad t\in[0,T)\tag{1} \eta(t) \eta(0)=0 (1) x'(t)=\frac{\eta(t)}{(T-t)^2}+\frac{\eta'(t)}{T-t},\qquad t\in[0,T), x [0, T) x'(t)=\frac{1}{\eta(t)}(x(t))^2+\frac{\eta'(t)}{\eta(t)}x(t),\qquad x(0)=0. \tag{2} (2) x\equiv0 x(t)=\frac{\eta(t)}{C-t} t\in[0,\infty) C<0","['real-analysis', 'ordinary-differential-equations', 'initial-value-problems']"
44,Well-posedness for ODEs with discontinuous right hand side,Well-posedness for ODEs with discontinuous right hand side,,"Consider the IVP for ODE with discontinuous right hand side, \begin{eqnarray} \dot{y}&=&f(y,y) \quad \quad \text{for }t>0,\\ y(0)&=&y_0, \end{eqnarray} where, $f\in L^{\infty}(\mathbb{R} \times\mathbb{R})$ and $\sup\limits_{x\in \mathbb{R}} {|f(x,\cdot)|_{Lip}< \infty}.$ A natural candidate for the weak solutions is \begin{eqnarray} y(t)=y_0+\int\limits_0^t f(y(s),y(s))ds. \end{eqnarray} Clearly $y\in C([0,\infty))$ and satisfies the ODE in the sense of distribution. Is this weak formulation for IVP well-posed? Do we need additional regularity assumptions like $F(\cdot,c)$ piecewise constant to prove the uniqueness? P.S. : A typical example of  of ODE with discontinuous right hand side can be $f(y,y)=\lfloor y \rfloor y$ where $\lfloor \cdot \rfloor$ denotes the greatest integer function. I understand that the problem in general may admit multiple weak solutions. For example the initial value problem $\dot{y}=\frac{1}{2y}$ and $y(0)=0$ admits multiple soltions; namely $y=0$ and $y(t)=\sqrt{t}$ .","Consider the IVP for ODE with discontinuous right hand side, where, and A natural candidate for the weak solutions is Clearly and satisfies the ODE in the sense of distribution. Is this weak formulation for IVP well-posed? Do we need additional regularity assumptions like piecewise constant to prove the uniqueness? P.S. : A typical example of  of ODE with discontinuous right hand side can be where denotes the greatest integer function. I understand that the problem in general may admit multiple weak solutions. For example the initial value problem and admits multiple soltions; namely and .","\begin{eqnarray}
\dot{y}&=&f(y,y) \quad \quad \text{for }t>0,\\
y(0)&=&y_0,
\end{eqnarray} f\in L^{\infty}(\mathbb{R} \times\mathbb{R}) \sup\limits_{x\in \mathbb{R}} {|f(x,\cdot)|_{Lip}< \infty}. \begin{eqnarray}
y(t)=y_0+\int\limits_0^t f(y(s),y(s))ds.
\end{eqnarray} y\in C([0,\infty)) F(\cdot,c) f(y,y)=\lfloor y \rfloor y \lfloor \cdot \rfloor \dot{y}=\frac{1}{2y} y(0)=0 y=0 y(t)=\sqrt{t}","['ordinary-differential-equations', 'analysis', 'measure-theory']"
45,Converting SDE to ODE via Dynkin's formula,Converting SDE to ODE via Dynkin's formula,,"I'm reading on Dynkin's formula. Given a stationary diffusion process $X(t)$ in the form $$dX(t) = \mu(X(t))dt +\sigma(X(t))dW$$ , as a generalization of the 2nd fundamental theorem of Calculus $F(b)-F(a)=\int_a^b f(x)dx$ , we have Dynkin's formula $$\mathbb E^{X(t)=x}f(X(T))=f(x) + \mathbb E^{X(t)=x} \int_t^T \mathscr Lf(X(s)) ds$$ , where $\mathscr L$ is the infinitesimal generator of $X(t)$ , ie $\mathscr L = \mu \frac{\partial}{\partial x} + \frac12 \sigma^2 \frac{\partial^2}{\partial x^2}$ . So far so good. But then I came into this A celebration of Dynkin’s formula slides of Prof. Tommi Sottinen . It gives an application of Dynkin's formula, which got me totally lost -- Consider the 2nd order ordinary differential equations (ODE) on $u(x)$ : $$\mu(x)u'(x)+\frac12 \sigma^2(x) u''(x) = 0, \qquad x\in (a,b)\subset \mathbb R$$ with boundary conditions $u(a)$ and $u(b)$ . Let $X$ be the Ito diffusion $$dX=\mu(X) dt+\sigma(X) dW$$ It follows from the Dynkin's formula that $$u(x)=\mathbb E^x u(X(\tau))$$ , where $\tau$ is the first exit time of $X$ from the interval $(a,b)$ . I really couldn't understand this SDE->ODE convertion. Why does it work? So far my trying to understand it is -- The ODE $\mu(x)u'(x)+\frac12 \sigma^2(x) u''(x) = 0$ is just $$\mathscr Lu(x)=0$$ Substitue with $u(x)=\mathbb E^x u(X(\tau))$ we get $$\mathscr L \mathbb E^x u(X(t)) = 0$$ , which is equivalent to $$\mathbb E^x \mathscr L u(X(t)) = 0$$ , which means $\mathscr Lu(X(t))$ is a martingale. But how can this be derived from Dynkin's formula?","I'm reading on Dynkin's formula. Given a stationary diffusion process in the form , as a generalization of the 2nd fundamental theorem of Calculus , we have Dynkin's formula , where is the infinitesimal generator of , ie . So far so good. But then I came into this A celebration of Dynkin’s formula slides of Prof. Tommi Sottinen . It gives an application of Dynkin's formula, which got me totally lost -- Consider the 2nd order ordinary differential equations (ODE) on : with boundary conditions and . Let be the Ito diffusion It follows from the Dynkin's formula that , where is the first exit time of from the interval . I really couldn't understand this SDE->ODE convertion. Why does it work? So far my trying to understand it is -- The ODE is just Substitue with we get , which is equivalent to , which means is a martingale. But how can this be derived from Dynkin's formula?","X(t) dX(t) = \mu(X(t))dt +\sigma(X(t))dW F(b)-F(a)=\int_a^b f(x)dx \mathbb E^{X(t)=x}f(X(T))=f(x) + \mathbb E^{X(t)=x} \int_t^T \mathscr Lf(X(s)) ds \mathscr L X(t) \mathscr L = \mu \frac{\partial}{\partial x} + \frac12 \sigma^2 \frac{\partial^2}{\partial x^2} u(x) \mu(x)u'(x)+\frac12 \sigma^2(x) u''(x) = 0, \qquad x\in
(a,b)\subset \mathbb R u(a) u(b) X dX=\mu(X) dt+\sigma(X) dW u(x)=\mathbb E^x u(X(\tau)) \tau X (a,b) \mu(x)u'(x)+\frac12 \sigma^2(x) u''(x) = 0 \mathscr Lu(x)=0 u(x)=\mathbb E^x u(X(\tau)) \mathscr L \mathbb E^x u(X(t)) = 0 \mathbb E^x \mathscr L u(X(t)) = 0 \mathscr Lu(X(t))","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations']"
46,"Is there a systematic way of knowing what happens to the set of solutions of a differential equation if you ""differentiate"" the differential equation?","Is there a systematic way of knowing what happens to the set of solutions of a differential equation if you ""differentiate"" the differential equation?",,"Suppose we know all functions $\phi$ that are solutions of the differential equation $$G(x)=F_1(x,y(x),y'(x),\dots,y^{(n)}(x))=0$$ for some arbitrary function $F_1$ . Clearly those same $\phi$ also satisfy the differential equation $$G'(x)=F_2(x,y(x),y'(x),\dots,y^{(n+1)}(x))=0$$ obtained by ""differentiating the differential equation"". Intuitively, I assume new solutions are generated, but I'm not certain if that is always the case and if there is a systematic way to relate those new solutions to the ones we already have for the original differential equation and to the functional form of $F_1$ and $F_2$ . For example, the solutions to the differential equation $$y'=0$$ are of the form $y(x)=C$ for some constant $C$ . ""Differentiating"" the differential equation, we get $$y''=0$$ which has solutions of the form $y(x)=C_1x+C_2$ for some constants $C_1$ and $C_2$ . The set of solutions of the second differential equation contains the solutions of the first one, as expected, but also contains new solutions, which in this case are antiderivatives of the original solutions. It doesn't seem that things work out always as they do for this simple case, though. Is there a systematic way of knowing what will happen to the solution set of a differential equation if you ""differentiate"" it?","Suppose we know all functions that are solutions of the differential equation for some arbitrary function . Clearly those same also satisfy the differential equation obtained by ""differentiating the differential equation"". Intuitively, I assume new solutions are generated, but I'm not certain if that is always the case and if there is a systematic way to relate those new solutions to the ones we already have for the original differential equation and to the functional form of and . For example, the solutions to the differential equation are of the form for some constant . ""Differentiating"" the differential equation, we get which has solutions of the form for some constants and . The set of solutions of the second differential equation contains the solutions of the first one, as expected, but also contains new solutions, which in this case are antiderivatives of the original solutions. It doesn't seem that things work out always as they do for this simple case, though. Is there a systematic way of knowing what will happen to the solution set of a differential equation if you ""differentiate"" it?","\phi G(x)=F_1(x,y(x),y'(x),\dots,y^{(n)}(x))=0 F_1 \phi G'(x)=F_2(x,y(x),y'(x),\dots,y^{(n+1)}(x))=0 F_1 F_2 y'=0 y(x)=C C y''=0 y(x)=C_1x+C_2 C_1 C_2","['calculus', 'ordinary-differential-equations']"
47,Exercises from section 14.3 of Katok's Introduction to the Modern Theory of Dynamical Systems,Exercises from section 14.3 of Katok's Introduction to the Modern Theory of Dynamical Systems,,"I am reading the book "" Introduction to the Modern Theory of Dynamical Systems "" by Anatole Katok and Boris Hasselblatt. I find myself in section 14.3 which deals with minimal sets (specifically from Schwartz's theorem). I am trying to do the following exercises: Show that any $C^1$ flow on the orientable surface of genus $g$ has no more than $g$ different minimal sets that are not fixed points or periodic orbits. Given an orientable surface of genus $g$ and $1\leq k\leq g$ show that there exists a $C^1$ flow with exactly $k$ nowhere-dense minimal sets that are not fixed point or circles. I don't know how to attack the problems, for example if $M$ were an orientable manifold of genus $3$ then it can be thought of as a $3-$ torus then using Schwartz's theorem comes to mind: Let $M$ be a two-dimensional differentiable manifold of class $C^2$ compact and connected. Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . A minimal set $\mu\subset M$ can be either: a singular point, or a periodic orbit, or an entire manifold $M$ which is homeomorphic to the torus $\mathbb T^2$ . but I can't really see how this can help me since the flow is of class $C^1$ . Any ideas please? I found the following hint given by the author, for the first question: Use the fact that any $g+1$ disjoint closed curves divide the surface, and the Poincaré-Bendixson theorem.","I am reading the book "" Introduction to the Modern Theory of Dynamical Systems "" by Anatole Katok and Boris Hasselblatt. I find myself in section 14.3 which deals with minimal sets (specifically from Schwartz's theorem). I am trying to do the following exercises: Show that any flow on the orientable surface of genus has no more than different minimal sets that are not fixed points or periodic orbits. Given an orientable surface of genus and show that there exists a flow with exactly nowhere-dense minimal sets that are not fixed point or circles. I don't know how to attack the problems, for example if were an orientable manifold of genus then it can be thought of as a torus then using Schwartz's theorem comes to mind: Let be a two-dimensional differentiable manifold of class compact and connected. Let be a flow of class in . A minimal set can be either: a singular point, or a periodic orbit, or an entire manifold which is homeomorphic to the torus . but I can't really see how this can help me since the flow is of class . Any ideas please? I found the following hint given by the author, for the first question: Use the fact that any disjoint closed curves divide the surface, and the Poincaré-Bendixson theorem.",C^1 g g g 1\leq k\leq g C^1 k M 3 3- M C^2 \varphi: \mathbb R \times M \to M C^2 M \mu\subset M M \mathbb T^2 C^1 g+1,"['ordinary-differential-equations', 'dynamical-systems', 'smooth-manifolds', 'surfaces']"
48,Questions about Poincaré–Bendixson theorem,Questions about Poincaré–Bendixson theorem,,"I just finished reading Schwartz's article on the generalization of the Poincaré-Bendixson theorem to compact two-dimensional manifolds. Which says: Let $M$ be a two-dimensional differentiable manifold of class $C^2$ compact and connected. Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . A minimal set $\mu\subset M$ can be either: a singular point, or a periodic orbit, or an entire manifold $M$ which is homeomorphic to the torus $T^2$ . But a few questions popped into my head that I couldn't answer: Do you know any references where I can find the proof of the Poincaré-Bendixson theorem in the projective plane $\mathbb P^2$ ? What happens to $\mathbb P^3$ ? Does anyone know any example of dimension 3 that is under the conditions of Schwartz's theorem whose omega limit is different from a singular point, a closed orbit and the torus? Do you know any differentiable manifold of dimension greater than two where the flow is interesting to study? That is to say, some variety in which it is possible to attempt some sort of smooth generalization of the Poincaré-Bendixson theorem? I was quite impressed with the $C^2$ class flows in the torus. Does anyone know what happens to $C^2$ flows in the $2-$ torus? or in the $n-$ torus? I know there are a lot of questions and I'm sure they'll get my attention because of that, but all the questions are closely related to the same topic and I didn't want to create too many separate questions because there might be some future readers wondering the same thing. I hope you can help me clear these doubts and continue learning.","I just finished reading Schwartz's article on the generalization of the Poincaré-Bendixson theorem to compact two-dimensional manifolds. Which says: Let be a two-dimensional differentiable manifold of class compact and connected. Let be a flow of class in . A minimal set can be either: a singular point, or a periodic orbit, or an entire manifold which is homeomorphic to the torus . But a few questions popped into my head that I couldn't answer: Do you know any references where I can find the proof of the Poincaré-Bendixson theorem in the projective plane ? What happens to ? Does anyone know any example of dimension 3 that is under the conditions of Schwartz's theorem whose omega limit is different from a singular point, a closed orbit and the torus? Do you know any differentiable manifold of dimension greater than two where the flow is interesting to study? That is to say, some variety in which it is possible to attempt some sort of smooth generalization of the Poincaré-Bendixson theorem? I was quite impressed with the class flows in the torus. Does anyone know what happens to flows in the torus? or in the torus? I know there are a lot of questions and I'm sure they'll get my attention because of that, but all the questions are closely related to the same topic and I didn't want to create too many separate questions because there might be some future readers wondering the same thing. I hope you can help me clear these doubts and continue learning.",M C^2 \varphi: \mathbb R \times M \to M C^2 M \mu\subset M M T^2 \mathbb P^2 \mathbb P^3 C^2 C^2 2- n-,"['ordinary-differential-equations', 'dynamical-systems', 'smooth-manifolds']"
49,Method of energy functions to show boundedness of 2nd Order ODE,Method of energy functions to show boundedness of 2nd Order ODE,,"I made the following system from the typical $x_1=x, x_2=x'$ transformation: $x_1'=x_2\\x_2'=\sin^2(x_1)-x_2\cos^2(x_1)$ I then used the general energy function $E(t,x_1,x_2)=\frac{1}{2}(x_2)^2+\int_{0}^{x_1}g(s)ds$ when $x''+f(x)x'+g(x)=0$ . Here, $E(t,x_1,x_2)=\frac{1}{2}(x_2)^2-\int_{0}^{x_1}\sin^2(s)ds$ And $\frac{dE}{dt}=-(x_2)^2\cos^2(x_1)≤0$ after applying the FTC, substituting, and cancelling terms. So the system is always dissipating energy, that is, for any time t, $E(t)≤E(0)$ . I want to show the boundedness of $x_2$ , that is, $|x_2(t)|≤K,$ for some constant $K$ and $t≥0$ . I get to the inequality: $\frac{1}{2}(x_2(t))^2-\int_{0}^{x_1(t)}\sin^2(s)ds≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0))$ Or if you prefer: $\frac{1}{2}(x_2(t))^2-\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0))$ . I cannot figure out how to remove the terms with $x_1(t)$ to isolate $|x_2(t)|$ while also keeping the ≤ inequality... Note: $-\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))<0$ , for all $x_1(t)$ Any help? Update: I've since shifted my efforts to first showing $x_1$ is bounded and then moving terms around in the inequality to show $x_2$ is bounded. I think this might be the only way to do it, but I'm not confident my method was correct...","I made the following system from the typical transformation: I then used the general energy function when . Here, And after applying the FTC, substituting, and cancelling terms. So the system is always dissipating energy, that is, for any time t, . I want to show the boundedness of , that is, for some constant and . I get to the inequality: Or if you prefer: . I cannot figure out how to remove the terms with to isolate while also keeping the ≤ inequality... Note: , for all Any help? Update: I've since shifted my efforts to first showing is bounded and then moving terms around in the inequality to show is bounded. I think this might be the only way to do it, but I'm not confident my method was correct...","x_1=x, x_2=x' x_1'=x_2\\x_2'=\sin^2(x_1)-x_2\cos^2(x_1) E(t,x_1,x_2)=\frac{1}{2}(x_2)^2+\int_{0}^{x_1}g(s)ds x''+f(x)x'+g(x)=0 E(t,x_1,x_2)=\frac{1}{2}(x_2)^2-\int_{0}^{x_1}\sin^2(s)ds \frac{dE}{dt}=-(x_2)^2\cos^2(x_1)≤0 E(t)≤E(0) x_2 |x_2(t)|≤K, K t≥0 \frac{1}{2}(x_2(t))^2-\int_{0}^{x_1(t)}\sin^2(s)ds≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0)) \frac{1}{2}(x_2(t))^2-\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))≤\frac{1}{2}(x_2(0))^2-\frac{x_1(0)}{2}+\frac{1}{4}\sin(2x_1(0)) x_1(t) |x_2(t)| -\frac{x_1(t)}{2}+\frac{1}{4}\sin(2x_1(t))<0 x_1(t) x_1 x_2","['ordinary-differential-equations', 'upper-lower-bounds', 'bounded-variation']"
50,Lemma 6 in the proof of Schwartz's theorem,Lemma 6 in the proof of Schwartz's theorem,,"I'm reading Chapter 10 of Sotomayor's book on Schwartz's theorem on flows on a compact two-dimensional surface, which follows the original proof . My first question is here: The following statement is found in either of the two links above. Letting $W$ be open in $(-1,1)$ and such that $G\subset W\subset \overline{W}\subset V$ , we summarize te properties of $f$ : $G=(-1,1)-\displaystyle\bigcup_{i=1}^\infty (a_i,b_i)\subset W$ $f(G)=G$ $f^k(v)=v$ and $v\in G$ implies $k=0$ $G$ is minimal for $f$ , in the following sense that $G$ does not contain any closed proper subset $K$ such that $f(K)=K.$ I manage to understand the test that is done in Sotomayor's book until before the red box, I just don't understand that very well. I know that there must exist a neighborhood $\tilde{V}$ of $\varphi(t_0,p)$ such that $\tilde{V}\cap\tilde{K_0}=\emptyset$ . But I don't know how to get to that part of red. Just that part of red is what leads to a contradiction. On the other hand, I also have problems understanding the proof of Lemma 6: There exists a interval $(a,b)$ such that $a<b;\;a,b\in G$ and $f^k((a, b))\subset W$ for all $k\geq 0$ . Assuming that $f$ is monotone. At one point in the demonstration they mention the following, $$ s_0\in G-K_1\Leftrightarrow\text{ for all }\epsilon>0\text{ we have }\;G\cap (s_0-\epsilon,s_0)\neq\emptyset\wedge G\cap (s_0,s_0+\epsilon)\neq\emptyset $$ I understand its use but I don't know how to get to test that if and only if mentioned above. Thanks to anyone who can help me understand this demo. Please be clear and if possible detailed to understand the essence of this demonstration.","I'm reading Chapter 10 of Sotomayor's book on Schwartz's theorem on flows on a compact two-dimensional surface, which follows the original proof . My first question is here: The following statement is found in either of the two links above. Letting be open in and such that , we summarize te properties of : and implies is minimal for , in the following sense that does not contain any closed proper subset such that I manage to understand the test that is done in Sotomayor's book until before the red box, I just don't understand that very well. I know that there must exist a neighborhood of such that . But I don't know how to get to that part of red. Just that part of red is what leads to a contradiction. On the other hand, I also have problems understanding the proof of Lemma 6: There exists a interval such that and for all . Assuming that is monotone. At one point in the demonstration they mention the following, I understand its use but I don't know how to get to test that if and only if mentioned above. Thanks to anyone who can help me understand this demo. Please be clear and if possible detailed to understand the essence of this demonstration.","W (-1,1) G\subset W\subset \overline{W}\subset V f G=(-1,1)-\displaystyle\bigcup_{i=1}^\infty (a_i,b_i)\subset W f(G)=G f^k(v)=v v\in G k=0 G f G K f(K)=K. \tilde{V} \varphi(t_0,p) \tilde{V}\cap\tilde{K_0}=\emptyset (a,b) a<b;\;a,b\in G f^k((a, b))\subset W k\geq 0 f 
s_0\in G-K_1\Leftrightarrow\text{ for all }\epsilon>0\text{ we have }\;G\cap (s_0-\epsilon,s_0)\neq\emptyset\wedge G\cap (s_0,s_0+\epsilon)\neq\emptyset
","['ordinary-differential-equations', 'dynamical-systems']"
51,Ansatz for defocusing NLS equation,Ansatz for defocusing NLS equation,,"I am interested in finding a solution of the NLS equation (defocusing): $$i \partial_t u = -\frac 12 u_{xx} + |u|^2 u$$ I hope to find a solution that is somewhat similar to that of the NLS equation (focusing): $$i \partial_t u = -\frac 12 u_{xx} - |u|^2 u$$ I'll outline the procedure for this focusing case here-- We may consider a solution of the form $u = v(x) e^{it}$ where $v(x) \in \mathbb R$ . Plugging this in, we arrive at the ODE for $v$ : $$-v = -\frac 12 v'' - v^3$$ Which we can rewrite as: $$\frac 12 \left((v')^2\right)' = v'\partial_v (v^2 - \frac 12 v^4)$$ Integrating both sides (and using B.C. vanishing at $\pm \infty$ ), we arrive at: $$\frac{dv}{dx} = \pm\sqrt{2 v^2 - v^4} $$ Rewriting, we have: $$\int \frac{dv}{\sqrt{2v^2 - v^4}}= -\frac{\tanh ^{-1}\left(\sqrt{1-2 v^{2}}\right)}{\sqrt{2}} = \pm x + c_0$$ For a positive soliton centered at the origin, we then have: $$v(x) = \sqrt{2}\operatorname{sech}(\sqrt{2}\cdot x)$$ Now, going back to the defocusing case, if we likewise propose an ansatz of the form $u = v(x) e^{it}$ where $v(x)$ vanishes at $\pm \infty$ , we have $v(x)$ solves the ODE: $$v + v^3= \frac 12 v''$$ But multiplying both sides by $v$ and integrating on $\mathbb R$ , we have: $$\int_\mathbb{R} v^2 + v^4 + \frac 12 v'^2 dx = 0 \implies v(x) = 0$$ Hence, for this case, there is no such non-trivial solution of this ODE if $v$ is real. Now, my question is this: is there an alternative ansatz I can use here for the defocusing case that would yield an analogous result to the focusing case? Indeed, if we try: $v(x) = i \sqrt{2} \operatorname{sech}(\sqrt{2} \cdot x)$ this solves the ODE, but it fails for the ansatz (it is not real, so the $ | \cdot |^2$ term causes issues).","I am interested in finding a solution of the NLS equation (defocusing): I hope to find a solution that is somewhat similar to that of the NLS equation (focusing): I'll outline the procedure for this focusing case here-- We may consider a solution of the form where . Plugging this in, we arrive at the ODE for : Which we can rewrite as: Integrating both sides (and using B.C. vanishing at ), we arrive at: Rewriting, we have: For a positive soliton centered at the origin, we then have: Now, going back to the defocusing case, if we likewise propose an ansatz of the form where vanishes at , we have solves the ODE: But multiplying both sides by and integrating on , we have: Hence, for this case, there is no such non-trivial solution of this ODE if is real. Now, my question is this: is there an alternative ansatz I can use here for the defocusing case that would yield an analogous result to the focusing case? Indeed, if we try: this solves the ODE, but it fails for the ansatz (it is not real, so the term causes issues).",i \partial_t u = -\frac 12 u_{xx} + |u|^2 u i \partial_t u = -\frac 12 u_{xx} - |u|^2 u u = v(x) e^{it} v(x) \in \mathbb R v -v = -\frac 12 v'' - v^3 \frac 12 \left((v')^2\right)' = v'\partial_v (v^2 - \frac 12 v^4) \pm \infty \frac{dv}{dx} = \pm\sqrt{2 v^2 - v^4}  \int \frac{dv}{\sqrt{2v^2 - v^4}}= -\frac{\tanh ^{-1}\left(\sqrt{1-2 v^{2}}\right)}{\sqrt{2}} = \pm x + c_0 v(x) = \sqrt{2}\operatorname{sech}(\sqrt{2}\cdot x) u = v(x) e^{it} v(x) \pm \infty v(x) v + v^3= \frac 12 v'' v \mathbb R \int_\mathbb{R} v^2 + v^4 + \frac 12 v'^2 dx = 0 \implies v(x) = 0 v v(x) = i \sqrt{2} \operatorname{sech}(\sqrt{2} \cdot x)  | \cdot |^2,"['ordinary-differential-equations', 'partial-differential-equations', 'physics']"
52,Is there an easier way to derive this equation for this mechanics problem?,Is there an easier way to derive this equation for this mechanics problem?,,"We're given a semicircle whose total mass $M$ is evenly distributed along its contour and a little particle of mass $m$ is dropped from its top left corner as in the image below all over an horizontal plane. There is no friction between the semicircle and the particle nor between the semicircle and the ground (so no rolling without slipping!). $P_0$ represents the particle at the start of the movement and $P$ represents it on a random position after the release of it. The center of the semicircle is $O$ . The angles that I used to describe this crazy movement were $\theta = \angle P_0OA$ and $\alpha = AOP$ and, as I'm not that great with rigid body dynamics, I actually solved this problem using Lagrange's equations and I couldn't help but to notice that one of the equations that describe this movement is fairly simple: $$2\frac gR \cos (\alpha) = (\ddot \theta + \ddot \alpha) 2\cos (\theta) - \pi \ddot \theta \sin(\theta+\alpha)$$ it is pretty remarkable that this equation does not involve the masses $m$ nor $M$ and I've been wondering if there is an easy way to get to this equation by analysing the momentum of the semicircle. But I struggled a bit with the acceleration of the system in a non inertial frame and I wonder if someone could hint on how to tackle it without Lagrange's equations. This is the Lagrangian I got and I'm 100% sure that it is correct: $$ \mathcal L (\theta, \alpha)= -\frac{R^2}{2(M+m)}(\frac 2{\pi}M \dot \theta \cos(\theta) + m \sin(\theta+\alpha)(\dot \theta + \dot \alpha))^2 + \frac{MR^2 \dot \theta ^2}2 + \frac{mR^2(\dot \theta + \dot \alpha)^2}2 + mgR\sin(\theta +\alpha) + \frac 2{\pi} MR\cos(\theta)g$$ EDIT: @eyeballfrog managed to turn that equation in a pendulum like equation: $$\frac gR \sin\theta +(\pi/2)\ddot{\theta} - \cos\theta \ddot{x} = 0$$ where $x$ is the size of $OO_0$ (with signal), looks like a pendulum in a non inertial frame","We're given a semicircle whose total mass is evenly distributed along its contour and a little particle of mass is dropped from its top left corner as in the image below all over an horizontal plane. There is no friction between the semicircle and the particle nor between the semicircle and the ground (so no rolling without slipping!). represents the particle at the start of the movement and represents it on a random position after the release of it. The center of the semicircle is . The angles that I used to describe this crazy movement were and and, as I'm not that great with rigid body dynamics, I actually solved this problem using Lagrange's equations and I couldn't help but to notice that one of the equations that describe this movement is fairly simple: it is pretty remarkable that this equation does not involve the masses nor and I've been wondering if there is an easy way to get to this equation by analysing the momentum of the semicircle. But I struggled a bit with the acceleration of the system in a non inertial frame and I wonder if someone could hint on how to tackle it without Lagrange's equations. This is the Lagrangian I got and I'm 100% sure that it is correct: EDIT: @eyeballfrog managed to turn that equation in a pendulum like equation: where is the size of (with signal), looks like a pendulum in a non inertial frame","M m P_0 P O \theta = \angle P_0OA \alpha = AOP 2\frac gR \cos (\alpha) = (\ddot \theta + \ddot \alpha) 2\cos (\theta) - \pi \ddot \theta \sin(\theta+\alpha) m M  \mathcal L (\theta, \alpha)= -\frac{R^2}{2(M+m)}(\frac 2{\pi}M \dot \theta \cos(\theta) + m \sin(\theta+\alpha)(\dot \theta + \dot \alpha))^2 + \frac{MR^2 \dot \theta ^2}2 + \frac{mR^2(\dot \theta + \dot \alpha)^2}2 + mgR\sin(\theta +\alpha) + \frac 2{\pi} MR\cos(\theta)g \frac gR \sin\theta +(\pi/2)\ddot{\theta} - \cos\theta \ddot{x} = 0 x OO_0","['ordinary-differential-equations', 'classical-mechanics', 'kinematics', 'rigid-transformation']"
53,How we can solve this differential equation $y'=x^2 e^{x^3}-\ln(y^2)$,How we can solve this differential equation,y'=x^2 e^{x^3}-\ln(y^2),"What is the way to solve this differential equation : $$y'=x^2 e^{x^3}-\ln(y^2)$$ I thought at start to make it exact but I got no luck there for the factor , Does it need series or Laplace? May anyone please tell me how to solve it ?","What is the way to solve this differential equation : I thought at start to make it exact but I got no luck there for the factor , Does it need series or Laplace? May anyone please tell me how to solve it ?",y'=x^2 e^{x^3}-\ln(y^2),['ordinary-differential-equations']
54,"Draw phase diagram of $x''+\sin x=0$ around $(0,0)$ manually",Draw phase diagram of  around  manually,"x''+\sin x=0 (0,0)","I am going to draw the phase digram of $x''(t)+\sin(x(t))=0$ around point $o=(0,0)$ manually . My ideas so far: Let $y=\frac{dx}{dt}$ . Then : $$\begin{align} \frac{dx}{dt}=y \qquad \frac{dy}{dt}=-\sin x \end{align}$$ If I use first order approximation, then I have: \begin{equation} \frac{dx}{dt}=y \qquad \frac{dy}{dt}=-x \end{equation} This coefficient matrix  of this equation set is: \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix} and it's eigenvalue is $i,-i$ . So, the null solution of the approximation equation set is a center point. But the null solution of the orgin equation set can be center point or focus point. How can I distinguish them? P.S., by appling Lyapunov function $V(x,y)=\frac{1}{2}y^2+1-\cos x$ to the orign equation set, I know $(0,0)$ is stable. Also, by coding from wolframalpha, I know $(0,0)$ is a center point, but I would like to know how to prove it.","I am going to draw the phase digram of around point manually . My ideas so far: Let . Then : If I use first order approximation, then I have: This coefficient matrix  of this equation set is: and it's eigenvalue is . So, the null solution of the approximation equation set is a center point. But the null solution of the orgin equation set can be center point or focus point. How can I distinguish them? P.S., by appling Lyapunov function to the orign equation set, I know is stable. Also, by coding from wolframalpha, I know is a center point, but I would like to know how to prove it.","x''(t)+\sin(x(t))=0 o=(0,0) y=\frac{dx}{dt} \begin{align}
\frac{dx}{dt}=y \qquad \frac{dy}{dt}=-\sin x
\end{align} \begin{equation}
\frac{dx}{dt}=y \qquad \frac{dy}{dt}=-x
\end{equation} \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix} i,-i V(x,y)=\frac{1}{2}y^2+1-\cos x (0,0) (0,0)",['ordinary-differential-equations']
55,Does Weierstrass approximation theorem have any application to differential equations?,Does Weierstrass approximation theorem have any application to differential equations?,,"Weierstrass approximation theorem states that continuous periodic functions can be uniformly approximated by trigonometric polynomials. I wonder if it has any applications in differential equations, where you can approximate a solution by approximating a function, and then prove rigorously that it does converge to a real solution. I would be very grateful if you could give me some references.","Weierstrass approximation theorem states that continuous periodic functions can be uniformly approximated by trigonometric polynomials. I wonder if it has any applications in differential equations, where you can approximate a solution by approximating a function, and then prove rigorously that it does converge to a real solution. I would be very grateful if you could give me some references.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'reference-request']"
56,"It is possible to find a solution to $y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0,$ $\,y'(0)=0,\,y(0)= 1/4$?",It is possible to find a solution to  ?,"y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0, \,y'(0)=0,\,y(0)= 1/4","It is possible to find an exact solution (hopefully in ""close form"") to $$y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0, \,y'(0)=0,\,y(0)= 1/4$$ ? How?... There exist a value of $t^*=\,?$ from which $y(t)=0,\,\forall t\geq t^*$ ? PS: If I multiply the equation by $i$ the terms looks alike Hilbert Transforms... I wasn´t able to do something with this, but maybe it helps someone else. **** 3rd Added later**** The main things I would like to know (in case that the solution is to hard to find): i) How it behaves for $t<0$ ? ii) Does it have a ending time $t_f$ from where $y(t)=0,\, t>t_f$ ? iii) Does the solution $y(t)$ been a compact-supported function? iv) Does the solution $y(t)$ been a bump function $\in C_c^{\infty}$ ?, v) Does the solution $y(t)$ starts with a discontinuity? vi) Does the solution $y(t)$ lives always in the reals $y(t) \in \mathbb{R}\,\forall t$ ? vi) Is the solution unique? This how the solution looks on Wolfram-Alpha . **** 2nd Added later**** Following a comment and since I am interested in a solution with $y\geq 0\Rightarrow |y|\equiv y$ , following the graph of Wolfram-Alpha it also happen than $y'\leq 0\Rightarrow |y'|\equiv -y'$ , and also using that $\text{sgn}(y)\sqrt{|y|} \equiv \frac{y}{|y|}\sqrt{|y|}\equiv \frac{y}{\sqrt{|y|}}$ , I could change the equation into another equation that I believe are not ""formally"" equivalent, but will give the same solution to the initial value problem: $$y'' +\sqrt{y}+i \sqrt{y'}=0, \,y'(0)=0,\,y(0)= 1/4$$ and in Wolfram-Alpha the plot looks similar, but I also get stack this time, maybe some knows how to solve this other differential equation. Plese note this analysis is only valid for this case $y(0)=1/4$ , since for higher initial values it can be seen that the solution do oscillations around $y=0$ so the assumption $y>0$ is not always hold. ***** 1st Added later ***** Since someone close the question because of lack of background, I will explain why I think is an interesting question. Recently I have learned here on MSE that no non-constant real-valued and continuous ANALYTIC function can be of finite-duration (since its compact-supported in the time variable), and also that no finite-duration function could be a solution to a linear ordinary differential equation (Linear ODE) - because of uniqueness-related-issues of solutions at the beginning and at the end which becomes zero outside them, so, since everything I have seen in engineering, which is been modeled or through Linear ODE or through solutions than can be described as a Power Series (analytical), are only approximations, since NO FINITE-DURATION solution to a system could be described by them. I have been looking for a Theory that described continuous-time finite-duration systems and its solutions (which now I know must be non-linear), but I can´t find yet any related theory.... All I have found yet on the web are abstract things I don´t understand (modern differential topology/geometry), but all they starts with assumptions of, or, with Linear Operators or Analytical Manifolds (which I already know can´t model a finite-duration solution), or with Smooth Manifolds or Smooth diffeomorphism, which only can be of finite-duration if at their support in the time-axis the function is zero, which is too restrictive (because, if it is of finite-duration, is of compact support in the time variable, and the only way a smooth function could be of compact support is through been a bump-function in the time axis, requiring that their values and also all their derivatives are zero at the beginning and at the end, to keep smoothness). As example, the simple model of a particle that experience a elastic collision with a wall, if the position vs time function is continuous (so no ""teleporting"" is allowed), the derivative/speed will show a bounded ""jump discontinuity"", which will become a singularity in the acceleration profile, so smoothness is no a desirable requirement (is too restrictive). Since finite-duration non-smooth phenomena is the most abundant kind of system in daily life, I was expecting to find a lot of math related on internet, but disappointingly, I have only found a few (four) papers working with continuous-time finite-duration differential equations (I take the question from one of them): 1 , 2 , 3 and 4 . From my side, I believe I have found a way to find solutions to segments of functions that are already solutions of initial value problems Linear ODE, this by taking their finite-duration Fourier Transform, without needing to take the convolution with the rectangular function that made the ""cut"" of the full-time-solution, and avoiding the problem of the discontinuities at the edges of the compact-supported domain, keeping true the Parseval's relation - and this scheme is also useful for finding the finite-duration Fourier transform of non-linear functions if I know beforehand the finite-duration Fourier transform of its first or its second derivative, jointly with the border conditions - I explained it here .... but unfortunately, It doesn´t work with finite-duration systems which are necessarily non-linear (but is an easy alternative to being taking convolutions). I have already know that if the solution start and end at zero, if smooth, it is a bump function (for which I am not really interested, since it implies a self-emerging system), but they also are quite complicated, and the only differential equation I found from them is in here , which is non-linear and also time-delayed, but is defined from all $t \in \mathbb{R}$ even where its solution lives only in a compact-support, so it fulfill what I have learned so far. From the mentioned papers, I understand now that there exists continuous non-linear systems with finite-duration solutions, which solutions are not unique, so I am trying to understand If every continuous non-linear system could support finite-duration solutions or not, and if not, How you can identify systems with finite-duration solutions?, and also, If there are restrictions imposed to the solutions because of been of finite duration, like example, if they can have unbounded derivative or not because of it (my intuition says they will have more restricted conditions because of causality issues since an infinite speed violate every possible constant-causality-speed model, but is just speculation).... And many other questions rise, since non-linear systems don´t preserve the superposition principle, How is possible that Maxwell's wave equations solutions to be ""right"" if finite-duration phenomena are non-linear? Which spurious effects could be introducing modeling them with infinite-duration signals?, speaking in ""physicists terms"", I quite shocked with the ""so little amount"" of papers about finite-duration physics that could be found published on Google (maybe I am using wrong words, so any help with finding these theory will be of great help), and maybe huge restrictions could rise: as example, if a continuous function is of finite-duration, just because of these two restrictions, the function is also bounded, and it Fourier Transform is Analytic, which are a huge restrictions just because of been of finite-duration... maybe other issues will rise because of it finite extension. But before, I am trying to understand these papers, which are a bit advanced for me (but not unattainable as the differential topology topics), and for this, I am trying to find the solution of the presented equation, which is supposed to be a continuous-time finite-duration differential equation, and when plotted, it looks like the half of a bumped-function similar to a Gaussian kernel, but I don´t know if its really becoming zero for every time that is bigger than an specific $t^*$ , or if it behaves as vanishing at infinity as the Gaussian function does, so, not been a truly finite-duration solution. An also, I don´t know what is happening at the beginning: it is just starting at zero previous the beginning (implying a jump discontinuity at the start)? or have non-zero values outside its finite-duration (maybe, becoming a complex function)?, What is happening with the derivative at the discontinuity at the beginning?... all these question requiring to know the specific solution. Hope its clear now, and hope you get interested as I am into these continuous-time finite-duration systems.","It is possible to find an exact solution (hopefully in ""close form"") to ? How?... There exist a value of from which ? PS: If I multiply the equation by the terms looks alike Hilbert Transforms... I wasn´t able to do something with this, but maybe it helps someone else. **** 3rd Added later**** The main things I would like to know (in case that the solution is to hard to find): i) How it behaves for ? ii) Does it have a ending time from where ? iii) Does the solution been a compact-supported function? iv) Does the solution been a bump function ?, v) Does the solution starts with a discontinuity? vi) Does the solution lives always in the reals ? vi) Is the solution unique? This how the solution looks on Wolfram-Alpha . **** 2nd Added later**** Following a comment and since I am interested in a solution with , following the graph of Wolfram-Alpha it also happen than , and also using that , I could change the equation into another equation that I believe are not ""formally"" equivalent, but will give the same solution to the initial value problem: and in Wolfram-Alpha the plot looks similar, but I also get stack this time, maybe some knows how to solve this other differential equation. Plese note this analysis is only valid for this case , since for higher initial values it can be seen that the solution do oscillations around so the assumption is not always hold. ***** 1st Added later ***** Since someone close the question because of lack of background, I will explain why I think is an interesting question. Recently I have learned here on MSE that no non-constant real-valued and continuous ANALYTIC function can be of finite-duration (since its compact-supported in the time variable), and also that no finite-duration function could be a solution to a linear ordinary differential equation (Linear ODE) - because of uniqueness-related-issues of solutions at the beginning and at the end which becomes zero outside them, so, since everything I have seen in engineering, which is been modeled or through Linear ODE or through solutions than can be described as a Power Series (analytical), are only approximations, since NO FINITE-DURATION solution to a system could be described by them. I have been looking for a Theory that described continuous-time finite-duration systems and its solutions (which now I know must be non-linear), but I can´t find yet any related theory.... All I have found yet on the web are abstract things I don´t understand (modern differential topology/geometry), but all they starts with assumptions of, or, with Linear Operators or Analytical Manifolds (which I already know can´t model a finite-duration solution), or with Smooth Manifolds or Smooth diffeomorphism, which only can be of finite-duration if at their support in the time-axis the function is zero, which is too restrictive (because, if it is of finite-duration, is of compact support in the time variable, and the only way a smooth function could be of compact support is through been a bump-function in the time axis, requiring that their values and also all their derivatives are zero at the beginning and at the end, to keep smoothness). As example, the simple model of a particle that experience a elastic collision with a wall, if the position vs time function is continuous (so no ""teleporting"" is allowed), the derivative/speed will show a bounded ""jump discontinuity"", which will become a singularity in the acceleration profile, so smoothness is no a desirable requirement (is too restrictive). Since finite-duration non-smooth phenomena is the most abundant kind of system in daily life, I was expecting to find a lot of math related on internet, but disappointingly, I have only found a few (four) papers working with continuous-time finite-duration differential equations (I take the question from one of them): 1 , 2 , 3 and 4 . From my side, I believe I have found a way to find solutions to segments of functions that are already solutions of initial value problems Linear ODE, this by taking their finite-duration Fourier Transform, without needing to take the convolution with the rectangular function that made the ""cut"" of the full-time-solution, and avoiding the problem of the discontinuities at the edges of the compact-supported domain, keeping true the Parseval's relation - and this scheme is also useful for finding the finite-duration Fourier transform of non-linear functions if I know beforehand the finite-duration Fourier transform of its first or its second derivative, jointly with the border conditions - I explained it here .... but unfortunately, It doesn´t work with finite-duration systems which are necessarily non-linear (but is an easy alternative to being taking convolutions). I have already know that if the solution start and end at zero, if smooth, it is a bump function (for which I am not really interested, since it implies a self-emerging system), but they also are quite complicated, and the only differential equation I found from them is in here , which is non-linear and also time-delayed, but is defined from all even where its solution lives only in a compact-support, so it fulfill what I have learned so far. From the mentioned papers, I understand now that there exists continuous non-linear systems with finite-duration solutions, which solutions are not unique, so I am trying to understand If every continuous non-linear system could support finite-duration solutions or not, and if not, How you can identify systems with finite-duration solutions?, and also, If there are restrictions imposed to the solutions because of been of finite duration, like example, if they can have unbounded derivative or not because of it (my intuition says they will have more restricted conditions because of causality issues since an infinite speed violate every possible constant-causality-speed model, but is just speculation).... And many other questions rise, since non-linear systems don´t preserve the superposition principle, How is possible that Maxwell's wave equations solutions to be ""right"" if finite-duration phenomena are non-linear? Which spurious effects could be introducing modeling them with infinite-duration signals?, speaking in ""physicists terms"", I quite shocked with the ""so little amount"" of papers about finite-duration physics that could be found published on Google (maybe I am using wrong words, so any help with finding these theory will be of great help), and maybe huge restrictions could rise: as example, if a continuous function is of finite-duration, just because of these two restrictions, the function is also bounded, and it Fourier Transform is Analytic, which are a huge restrictions just because of been of finite-duration... maybe other issues will rise because of it finite extension. But before, I am trying to understand these papers, which are a bit advanced for me (but not unattainable as the differential topology topics), and for this, I am trying to find the solution of the presented equation, which is supposed to be a continuous-time finite-duration differential equation, and when plotted, it looks like the half of a bumped-function similar to a Gaussian kernel, but I don´t know if its really becoming zero for every time that is bigger than an specific , or if it behaves as vanishing at infinity as the Gaussian function does, so, not been a truly finite-duration solution. An also, I don´t know what is happening at the beginning: it is just starting at zero previous the beginning (implying a jump discontinuity at the start)? or have non-zero values outside its finite-duration (maybe, becoming a complex function)?, What is happening with the derivative at the discontinuity at the beginning?... all these question requiring to know the specific solution. Hope its clear now, and hope you get interested as I am into these continuous-time finite-duration systems.","y''+\sqrt{|y|}\operatorname{sgn}(y)+\sqrt{|y'|}\operatorname{sgn}(y')=0, \,y'(0)=0,\,y(0)= 1/4 t^*=\,? y(t)=0,\,\forall t\geq t^* i t<0 t_f y(t)=0,\, t>t_f y(t) y(t) \in C_c^{\infty} y(t) y(t) y(t) \in \mathbb{R}\,\forall t y\geq 0\Rightarrow |y|\equiv y y'\leq 0\Rightarrow |y'|\equiv -y' \text{sgn}(y)\sqrt{|y|} \equiv \frac{y}{|y|}\sqrt{|y|}\equiv \frac{y}{\sqrt{|y|}} y'' +\sqrt{y}+i \sqrt{y'}=0, \,y'(0)=0,\,y(0)= 1/4 y(0)=1/4 y=0 y>0 t \in \mathbb{R} t^*","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'singular-solution', 'finite-duration']"
57,Decay of linear system with damping,Decay of linear system with damping,,"Let us consider the following linear system with damping: $$ \begin{cases} u_t - u_x = -\frac{1}{2} a(x) (u+v)\\ v_t + v_x = -\frac{1}{2} a(x) (u+v) \end{cases} $$ where $a(x) = \mathbf{1}_{(-\infty,-1)\cup(1,\infty)}(x)$ . Let's write the solution as $w=(u,v)$ corresponding to initial data $w_0 := w(0,\cdot) \in L^1(\mathbb R) \cap L^2(\mathbb R)$ . If $a \equiv 1$ , we can prove that $$w=w_1+w_2$$ where $$\|w_1\|_{L^2} \lesssim e^{-\alpha t}\|w_0\|_{L^2}, \qquad \|w_2\|_{L^\infty} \lesssim t^{-1/2}\|w_0\|_{L^1}$$ using Fourier transform. Can we prove something similar also for the choice of $as$ as $a(x) = \mathbf{1}_{(-\infty,-1)\cup(1,\infty)}(x)$ ?","Let us consider the following linear system with damping: where . Let's write the solution as corresponding to initial data . If , we can prove that where using Fourier transform. Can we prove something similar also for the choice of as ?","
\begin{cases}
u_t - u_x = -\frac{1}{2} a(x) (u+v)\\
v_t + v_x = -\frac{1}{2} a(x) (u+v)
\end{cases}
 a(x) = \mathbf{1}_{(-\infty,-1)\cup(1,\infty)}(x) w=(u,v) w_0 := w(0,\cdot) \in L^1(\mathbb R) \cap L^2(\mathbb R) a \equiv 1 w=w_1+w_2 \|w_1\|_{L^2} \lesssim e^{-\alpha t}\|w_0\|_{L^2}, \qquad \|w_2\|_{L^\infty} \lesssim t^{-1/2}\|w_0\|_{L^1} as a(x) = \mathbf{1}_{(-\infty,-1)\cup(1,\infty)}(x)","['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations']"
58,Hopf bifurcation computation fail,Hopf bifurcation computation fail,,"For few days I am working on Hopf bifurcation of a system like this: $$ \frac{dx}{dt}=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\ \frac{dy}{dt}=\frac{1-y-cx^2 y }{1+x^2} $$ above $\alpha$ is the bifurcation parameter and $a,\;b,\;c$ are real parameters.  I want to determine the parameter value that the Hopf bifurcation would occur. I have 2 sources (a thesis and an article focusing on another set of equations) for finding the parameters, I tried to recreate their results but I failed miserably.  Found steady state values as $x=x(\alpha),\;y=y(x)$ $$ 0=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\ 0=\frac{1-y-cx^2 y }{1+x^2}\\ $$ then for combining two equations from y $$ y=\frac{1}{\alpha}\frac{x^2+bx+1}{x^2  + a x }\\ y=\frac{1}{1+cx^2} $$ and got an equation for $x$ and $\alpha$ only $$ {\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x } $$ now stated bifurcation parameter in terms of the stated state. Then for $$ F(x,y)=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\ G(x,y)=\frac{1-y-cx^2 y }{1+x^2} $$ wrote the characteristic equation and solve for lambda to find eigenvalues of the system. Meanwhile replaced $$ y\to\frac{1}{1+cx^2}\\ {\alpha}\to\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x } $$ at the characteristic equation solution. For finding Hopf bifurcation I look for the the values of $x$ making the eigenvalue of the Jacobian $0$ , such that by replacing those $x$ values can find the exact Hopf b. value from $$ {\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x } $$ however it does give wrong answers. Can you trace something wrong here? (Let's say everything looks good can you please suggest a document where each step in the computation is clear so that I can go through and realize where i do wrong) Thank you for your time.","For few days I am working on Hopf bifurcation of a system like this: above is the bifurcation parameter and are real parameters.  I want to determine the parameter value that the Hopf bifurcation would occur. I have 2 sources (a thesis and an article focusing on another set of equations) for finding the parameters, I tried to recreate their results but I failed miserably.  Found steady state values as then for combining two equations from y and got an equation for and only now stated bifurcation parameter in terms of the stated state. Then for wrote the characteristic equation and solve for lambda to find eigenvalues of the system. Meanwhile replaced at the characteristic equation solution. For finding Hopf bifurcation I look for the the values of making the eigenvalue of the Jacobian , such that by replacing those values can find the exact Hopf b. value from however it does give wrong answers. Can you trace something wrong here? (Let's say everything looks good can you please suggest a document where each step in the computation is clear so that I can go through and realize where i do wrong) Thank you for your time.","
\frac{dx}{dt}=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
\frac{dy}{dt}=\frac{1-y-cx^2 y }{1+x^2}
 \alpha a,\;b,\;c x=x(\alpha),\;y=y(x) 
0=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
0=\frac{1-y-cx^2 y }{1+x^2}\\
 
y=\frac{1}{\alpha}\frac{x^2+bx+1}{x^2  + a x }\\
y=\frac{1}{1+cx^2}
 x \alpha 
{\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
 
F(x,y)=\alpha\frac{x^2 y + a x y}{x^2+bx+1}-1\\
G(x,y)=\frac{1-y-cx^2 y }{1+x^2}
 
y\to\frac{1}{1+cx^2}\\
{\alpha}\to\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
 x 0 x 
{\alpha}=\frac{(1+cx^2)(x^2+bx+1)}{x^2  + a x }
","['ordinary-differential-equations', 'nonlinear-system', 'nonlinear-analysis']"
59,Determining characteristics of the differential equation based on the behavior of its solutions,Determining characteristics of the differential equation based on the behavior of its solutions,,"I am trying to solve this problem, and I have some ideas of how to do it, but I can't quite pin it down. All the solutions to the equation: $$x''+p(t)x'+q(t)x=0$$ tend to $0$ as $t$ tends to $0$ . What can be said about $p(t)$ ? I have tried drawing all the different ways I can think of that a function can tend to zero (decreasing oscillations, or something like $e^{-x}$ or $-e^{-x}$ .) From there, I can find some relationships between $x$ , $x'$ , and $x''$ . For example, if the function is positive, it must at some point start to decrease, but that decrease must eventually slow down, or else the function becomes negative. And if it's negative, it must at some point start to increase and then the increase slows down. From that sort of analysis, I think I've concluded at least that $p(t)$ is bounded from below, but I'm not sure if that's sufficient, and I haven't proved it rigorously at all, I've just drawn some pictures. Does anyone have an idea of a theorem that might be applicable here, or of a more rigorous way of approaching this? I have a sense that Gronwall's Inequality might be applicable, but I don't know exactly how.","I am trying to solve this problem, and I have some ideas of how to do it, but I can't quite pin it down. All the solutions to the equation: tend to as tends to . What can be said about ? I have tried drawing all the different ways I can think of that a function can tend to zero (decreasing oscillations, or something like or .) From there, I can find some relationships between , , and . For example, if the function is positive, it must at some point start to decrease, but that decrease must eventually slow down, or else the function becomes negative. And if it's negative, it must at some point start to increase and then the increase slows down. From that sort of analysis, I think I've concluded at least that is bounded from below, but I'm not sure if that's sufficient, and I haven't proved it rigorously at all, I've just drawn some pictures. Does anyone have an idea of a theorem that might be applicable here, or of a more rigorous way of approaching this? I have a sense that Gronwall's Inequality might be applicable, but I don't know exactly how.",x''+p(t)x'+q(t)x=0 0 t 0 p(t) e^{-x} -e^{-x} x x' x'' p(t),['ordinary-differential-equations']
60,Solve BVP $y''-y=t^2$ by green's function,Solve BVP  by green's function,y''-y=t^2,"Construct the green's function for the following boundary value problem and use it to find the solution of $$y''-y=t^2,\quad y(0)=0,y(1)=0$$ I got the complementary solution as: $$y_c=c_1e^t+c_2e^{-t}$$ Following this answer , let $y_1=e^t$ and $y_2=e^{-t}$ . Then $$\textrm{Wronskian}=W(y_1,y_2)=-2$$ $$G(t,u)=\frac{y_1(u)y_2(t)-y_1(t)y_2(u)}{W(y_1,y_2)}=\frac{e^ue^{-t}-e^te^{-u}}{-2}$$ Thus, $$ \begin{align} y_p&=\int_0^tG(t,u)f(u)\:du\\ &=\int _0^t\:\left(\frac{e^ue^{-t}-e^te^{-u}}{-2}\cdot \:u^2\right)\:du\\ &=e^{-t}\left(-2e^t-t^2e^t+e^{2t}+1\right) \end{align} $$ But the solution mentioned in the book was, $$-\frac{\sinh (1-t)}{\sinh 1}\left[t^2\cosh t-2t\sinh t+2\cosh t-2\right]-\frac{\sinh t}{\sinh 1}\left[t^2\cosh(1-t)+2t\sinh (1-t)+2\cosh(1-t)-3\right]$$ Did I do anything wrong when following that answer? Any help will be appreciated.","Construct the green's function for the following boundary value problem and use it to find the solution of I got the complementary solution as: Following this answer , let and . Then Thus, But the solution mentioned in the book was, Did I do anything wrong when following that answer? Any help will be appreciated.","y''-y=t^2,\quad y(0)=0,y(1)=0 y_c=c_1e^t+c_2e^{-t} y_1=e^t y_2=e^{-t} \textrm{Wronskian}=W(y_1,y_2)=-2 G(t,u)=\frac{y_1(u)y_2(t)-y_1(t)y_2(u)}{W(y_1,y_2)}=\frac{e^ue^{-t}-e^te^{-u}}{-2} 
\begin{align}
y_p&=\int_0^tG(t,u)f(u)\:du\\
&=\int _0^t\:\left(\frac{e^ue^{-t}-e^te^{-u}}{-2}\cdot \:u^2\right)\:du\\
&=e^{-t}\left(-2e^t-t^2e^t+e^{2t}+1\right)
\end{align}
 -\frac{\sinh (1-t)}{\sinh 1}\left[t^2\cosh t-2t\sinh t+2\cosh t-2\right]-\frac{\sinh t}{\sinh 1}\left[t^2\cosh(1-t)+2t\sinh (1-t)+2\cosh(1-t)-3\right]","['ordinary-differential-equations', 'boundary-value-problem', 'greens-function']"
61,L-stable discretization method with optimal stability region,L-stable discretization method with optimal stability region,,"I am looking for discretization methods of continuous linear time invariant systems with the properties: It is L-stable (meaning it is A-stable and its stability function approaches zero in the limit) It maps the open left half of the complex s-plane to the interior of the unit circle of the z-plane. Some background: Consider the system $$ \begin{align} \dot{x}(t) &= f(t,x(t))  \\  x(0) &= x_0 \end{align}\tag{1} $$ where $f:R^m\to R^m$ is smooth and $x\in R^m$ . A numerical solution to the initial value problem $(1)$ can be computed by the general (potentially implicit) Runge-Kutta method of order $s$ and step-witdth $h$ : $$ \begin{align} x_{n+1} &= x_n + h \sum_{i = 1}^s b_i k_{ni} \\ k_{ni} &= f\Big(t_n + c_i h, x_n + h \sum_{j = 1}^s a_{ij}k_{nj}\Big) \end{align} \tag{2} $$ The coefficients $a_{ij},b_i,c_i$ in $(2)$ are defined by the Butcher tableau: $$ \begin{array} {c|c} c & A  \\ \hline & b^T \end{array}= \begin{array} {c|cccc} c_1 & a_{11} & a_{12} & \dots & a_{1s} \\ c_2 & a_{21} & a_{22} & \dots & a_{2s} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ c_s & a_{s1} & a_{s2} & \dots & a_{ss} \\ \hline & b_1 & b_2 & \dots & b_s \end{array} $$ Different methods have different Butcher tableaus. For example: $$ \begin{array}{|c|c|c|} \hline \text{Method} & \text{Butcher tableau} \\ \hline \text{Explicit Euler} & A = 0, b = 1, c = 0 \\ \hline \text{Implicit Euler} & A = 1, b = 1, c = 1 \\ \hline \text{Heun} & A = \begin{pmatrix}0&0\\1&0\end{pmatrix}, b = \begin{pmatrix}\frac{1}{2}\\ \frac{1}{2}\end{pmatrix}, c = \begin{pmatrix}0\\ 1\end{pmatrix}  \\ \hline \text{Runge Kutta 4} & A = \begin{pmatrix}0&0&0&0\\\frac{1}{2}&0&0&0\\ 0&\frac{1}{2}&0&0\\0&0&1&0 \end{pmatrix}, b = \begin{pmatrix}\frac{1}{6}\\\frac{1}{3}\\ \frac{1}{3}\\ \frac{1}{6}\\ \end{pmatrix}, c = \begin{pmatrix}0\\ \frac{1}{2}\\ \frac{1}{2} \\ 1\end{pmatrix} \\ \hline \text{Trapezoidal} & A = \begin{pmatrix}0&0\\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}, b = \begin{pmatrix}\frac{1}{2}\\ \frac{1}{2}\end{pmatrix}, c = \begin{pmatrix}0\\ 1\end{pmatrix} \\ \hline \end{array} $$ Each method has a stability region $S$ which is given by $$ S = \{ z \in \mathbb{C} : |\phi(z)| \leq 1 \} \tag{3} $$ In Eq. $(3)$ the function $\phi$ is the stability function of the considered method which is constructed from its Butcher tableau: $$ \phi(z) = \frac{\det(I - z A + z \mathbf{1} b^T)}{\det(I - z A)} \tag{4} $$ Here $I$ is $s \times s$ identity matrix and $\mathbf{1}$ a $s \times 1$ vector of ones. The stability regions (in yellow) for the five methods look like this: Now define A-stability : A method is A-stable if its stability region contains the left half-plane of $\mathbb{C}$ , meaning that $\{ z \in \mathbb{C} : Re(z) \leq 0 \} \subset S$ . Here $Re(z)$ is the real part of $z$ . As can be seen in the plots above, out of the the five example methods, only the implicit Euler and the trapezoidal method are A-stable. Now define L-stability : A method is L-stable if it is A-stable and if $$ \lim_{Re(z) \rightarrow -\infty} |\phi(z)| = 0 $$ Note : A-stability and L-stability are different concepts than asymptotic stability and Lyapunov stability (the naming is a bit unfortunate in my opinion). To come back to my question, I am looking for a method that is L-stable (1. condition) and whose stability region is exactly the left half of the complex plane (2. condition). In other words, I am looking for a L-stable method with stability region $S = \{ z \in \mathbb{C} : Re(z) \leq 0 \}$ . For example the billinear transform (trapezoidal method) satisfies the second condition. However its stability function $\phi$ satisfies $$ \lim_{Re(z) \rightarrow -\infty} |\phi(z)| = \lim_{Re(z) \rightarrow -\infty} \Big|\frac{1 + 0.5 z}{1 - 0.5z}\Big| = 1 \neq 0 $$ So it is not L-stable. On the other hand, the implicit Euler method is L-stable because $$ \lim_{Re(z) \rightarrow -\infty} |\phi(z)| = \lim_{Re(z) \rightarrow -\infty} \Big|\frac{1}{1 - z}\Big| = 0 $$ However it doesn't satisfy the second condition because its stability region is ""larger"" than the left half of the complex plane. Question : Is there a method that has both properties?","I am looking for discretization methods of continuous linear time invariant systems with the properties: It is L-stable (meaning it is A-stable and its stability function approaches zero in the limit) It maps the open left half of the complex s-plane to the interior of the unit circle of the z-plane. Some background: Consider the system where is smooth and . A numerical solution to the initial value problem can be computed by the general (potentially implicit) Runge-Kutta method of order and step-witdth : The coefficients in are defined by the Butcher tableau: Different methods have different Butcher tableaus. For example: Each method has a stability region which is given by In Eq. the function is the stability function of the considered method which is constructed from its Butcher tableau: Here is identity matrix and a vector of ones. The stability regions (in yellow) for the five methods look like this: Now define A-stability : A method is A-stable if its stability region contains the left half-plane of , meaning that . Here is the real part of . As can be seen in the plots above, out of the the five example methods, only the implicit Euler and the trapezoidal method are A-stable. Now define L-stability : A method is L-stable if it is A-stable and if Note : A-stability and L-stability are different concepts than asymptotic stability and Lyapunov stability (the naming is a bit unfortunate in my opinion). To come back to my question, I am looking for a method that is L-stable (1. condition) and whose stability region is exactly the left half of the complex plane (2. condition). In other words, I am looking for a L-stable method with stability region . For example the billinear transform (trapezoidal method) satisfies the second condition. However its stability function satisfies So it is not L-stable. On the other hand, the implicit Euler method is L-stable because However it doesn't satisfy the second condition because its stability region is ""larger"" than the left half of the complex plane. Question : Is there a method that has both properties?","
\begin{align}
\dot{x}(t) &= f(t,x(t))  \\
 x(0) &= x_0
\end{align}\tag{1}
 f:R^m\to R^m x\in R^m (1) s h 
\begin{align}
x_{n+1} &= x_n + h \sum_{i = 1}^s b_i k_{ni} \\
k_{ni} &= f\Big(t_n + c_i h, x_n + h \sum_{j = 1}^s a_{ij}k_{nj}\Big)
\end{align} \tag{2}
 a_{ij},b_i,c_i (2) 
\begin{array}
{c|c}
c & A  \\
\hline
& b^T
\end{array}=
\begin{array}
{c|cccc}
c_1 & a_{11} & a_{12} & \dots & a_{1s} \\
c_2 & a_{21} & a_{22} & \dots & a_{2s} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_s & a_{s1} & a_{s2} & \dots & a_{ss} \\
\hline
& b_1 & b_2 & \dots & b_s
\end{array}
 
\begin{array}{|c|c|c|}
\hline
\text{Method} & \text{Butcher tableau} \\ \hline
\text{Explicit Euler} & A = 0, b = 1, c = 0 \\ \hline
\text{Implicit Euler} & A = 1, b = 1, c = 1 \\ \hline
\text{Heun} & A = \begin{pmatrix}0&0\\1&0\end{pmatrix}, b = \begin{pmatrix}\frac{1}{2}\\ \frac{1}{2}\end{pmatrix}, c = \begin{pmatrix}0\\ 1\end{pmatrix}  \\ \hline
\text{Runge Kutta 4} & A = \begin{pmatrix}0&0&0&0\\\frac{1}{2}&0&0&0\\ 0&\frac{1}{2}&0&0\\0&0&1&0 \end{pmatrix}, b = \begin{pmatrix}\frac{1}{6}\\\frac{1}{3}\\ \frac{1}{3}\\ \frac{1}{6}\\ \end{pmatrix}, c = \begin{pmatrix}0\\ \frac{1}{2}\\ \frac{1}{2} \\ 1\end{pmatrix} \\ \hline
\text{Trapezoidal} & A = \begin{pmatrix}0&0\\ \frac{1}{2} & \frac{1}{2}\end{pmatrix}, b = \begin{pmatrix}\frac{1}{2}\\ \frac{1}{2}\end{pmatrix}, c = \begin{pmatrix}0\\ 1\end{pmatrix} \\ \hline
\end{array}
 S 
S = \{ z \in \mathbb{C} : |\phi(z)| \leq 1 \} \tag{3}
 (3) \phi 
\phi(z) = \frac{\det(I - z A + z \mathbf{1} b^T)}{\det(I - z A)} \tag{4}
 I s \times s \mathbf{1} s \times 1 \mathbb{C} \{ z \in \mathbb{C} : Re(z) \leq 0 \} \subset S Re(z) z 
\lim_{Re(z) \rightarrow -\infty} |\phi(z)| = 0
 S = \{ z \in \mathbb{C} : Re(z) \leq 0 \} \phi 
\lim_{Re(z) \rightarrow -\infty} |\phi(z)| = \lim_{Re(z) \rightarrow -\infty} \Big|\frac{1 + 0.5 z}{1 - 0.5z}\Big| = 1 \neq 0
 
\lim_{Re(z) \rightarrow -\infty} |\phi(z)| = \lim_{Re(z) \rightarrow -\infty} \Big|\frac{1}{1 - z}\Big| = 0
","['integration', 'ordinary-differential-equations', 'numerical-methods', 'dynamical-systems', 'control-theory']"
62,Regular singular points on curves,Regular singular points on curves,,"The reference for this post is $11^{th}$ section of the article, N. Katz, Nilpotent connections and the monodromy theorem . Suppose $k$ is a field of characteristic $0$ and $K/k$ is the function field of a smooth, projective, irreducible curve over $k$ , $W$ is a finite-dimensional $K$ -vector space. A connection $$\nabla : W \to \Omega^1_{K/k} \otimes W$$ is simply an additive map satisfying the Leibniz rule just almost as same as the cases for connection of vector bundle over smooth manifolds, read $$\nabla(fw) = df \otimes w + f\nabla (w) \ \forall f \in K, w \in W.$$ Equivalently, we can define a connection to be a map $\nabla: \mathrm{Der}_k(K,K) \to \mathrm{End}_k(W)$ . Now suppose $\mathfrak{p}$ is a closed point, $\mathcal{O}_{\mathfrak{p}}$ its local ring, $\mathfrak{m}_{\mathfrak{p}}$ its maxiaml ideal, $\mathrm{ord}_{\mathfrak{p}}: K \to \mathbb{Z} \cup \left \{\infty \right\}$ the discrete valuation at $\mathfrak{p}$ , so in particular $$\mathcal{O} = \left \{f \in K \mid \mathrm{ord}(f) \geq 0 \right \}$$ $$\mathfrak{m} = \left \{f \in K \mid \mathrm{ord}(f) \geq 1 \right \}.$$ We now have an equality $$\left \{D \in \mathrm{Der}_k(K,K) \mid D(\mathfrak{m}) \subset \mathfrak{m} \right \} = (h\frac{d}{dh})\mathcal{O}.$$ If there is a base $\mathbf{e}$ of $W$ (over K) such that $$\nabla\left(h\frac{d}{dh}\right)\mathbf{e} = B\mathbf{e} \ \text{for some} \ B \in M_{\mathrm{dim}(W)}(\mathcal{O})$$ then $\mathfrak{p}$ is called a regular singular point . A theorem of Turrittin asserts that there is a closed relation between regular singular points and cyclic vectors, i.e. vector $w \in W$ such that there exists a non-zero derivation such that $w,(\nabla(D))(w),...,(\nabla(D))^k(w),...$ span $W$ over $K$ . In that case, we say $(W,\nabla)$ is cyclic and $w$ a cyclic vector Theorem . [Turrittin] Assume that $(W,\nabla)$ is cyclic with a cyclic vector $w$ , $\mathfrak{p}$ a closed point and $h$ a uniformizer at $\mathfrak{p}$ . Then the following statements are equivalent $(W,\nabla)$ does not have a regular singular point at $\mathfrak{p}$ . In terms of the basis $$\mathbf{e} = \left(w, \nabla\left(h\frac{d}{dh}\right)(w),...\nabla\left(h\frac{d}{dh}\right)^{n-1}(w) \right)^t,$$ the connection is expressed as $$\nabla\left(h\frac{d}{dh}\right)\mathbf{e} = \begin{pmatrix} 0 & 1 & ... & 0 &0 \\ . &  &...  &  & ...\\ 0 & 0 &  & 0 & 1\\ -f_0 & -f_1 & . &  .& -f_{n-1} \end{pmatrix}\mathbf{e}$$ and for some $i$ , $\mathrm{ord}(f_i) < 0$ . From the theorem, we deduce that if $A$ is a $K$ -algebra, then any $K$ -linear map $W \to A$ compatible with the connection is one-one corresponding to an element $f \in A$ satisfying the differential equation $$\frac{d^n f}{dh^n} + f_{n-1}\frac{d^{(n-1)}f}{dh^{(n-1)}}+...+f_0=0.$$ Here we're back to diffential equation ""in the"" analytic world, such a equation of the form above is said to have a regular singular point at $a$ if $f_{n-i}$ has a pole of order at most $i$ at $a$ . If so, the equation can be solved by the so-called Frobenius method . The matrix appears in the theorem is nothing but making a change of variables, i.e. transforming a $n$ -order equation into a system of $n$ 1-order equations and a cyclic basis in this case is just $(f,f',f^{""},...,f^{(n-1)})$ . If we view the resulting system of equations as a connection on the trivial bundle then the connection has form $$\nabla_{\frac{d}{dz}} = \frac{d}{dz} +  \begin{pmatrix} 0 & 1 & ... & 0 &0 \\ . &  &...  &  & ...\\ 0 & 0 &  & 0 & 1\\ -f_0 & -f_1 & . &  .& -f_{n-1} \end{pmatrix}dz$$ My question. ""What"" plays the role of $\nabla\left(h\frac{d}{dh}\right)$ (algebraic manner) in the analytical world? Maybe I'm confused now but let's pick up a concrete example, say $f^{""}+\frac{p}{z}f'+\frac{q}{z^2}f=0$ ( $p,q$ holomorphic), it obviously has a regular singular point at $z=0$ but its matrix is $\begin{pmatrix} 0 & 1\\   \frac{q}{z^2}& \frac{p}{z}  \end{pmatrix}$ so by Turrrittin's theorem, it does not have a regular singular at $z=0$ ? What is the reason for this contradiction? Perhaps I missed some point, any explanation is appreciate. Is there any motivation for considering the condition $D(\mathfrak{m})\subset \mathfrak{m}$ ? Update . The second question is solved.","The reference for this post is section of the article, N. Katz, Nilpotent connections and the monodromy theorem . Suppose is a field of characteristic and is the function field of a smooth, projective, irreducible curve over , is a finite-dimensional -vector space. A connection is simply an additive map satisfying the Leibniz rule just almost as same as the cases for connection of vector bundle over smooth manifolds, read Equivalently, we can define a connection to be a map . Now suppose is a closed point, its local ring, its maxiaml ideal, the discrete valuation at , so in particular We now have an equality If there is a base of (over K) such that then is called a regular singular point . A theorem of Turrittin asserts that there is a closed relation between regular singular points and cyclic vectors, i.e. vector such that there exists a non-zero derivation such that span over . In that case, we say is cyclic and a cyclic vector Theorem . [Turrittin] Assume that is cyclic with a cyclic vector , a closed point and a uniformizer at . Then the following statements are equivalent does not have a regular singular point at . In terms of the basis the connection is expressed as and for some , . From the theorem, we deduce that if is a -algebra, then any -linear map compatible with the connection is one-one corresponding to an element satisfying the differential equation Here we're back to diffential equation ""in the"" analytic world, such a equation of the form above is said to have a regular singular point at if has a pole of order at most at . If so, the equation can be solved by the so-called Frobenius method . The matrix appears in the theorem is nothing but making a change of variables, i.e. transforming a -order equation into a system of 1-order equations and a cyclic basis in this case is just . If we view the resulting system of equations as a connection on the trivial bundle then the connection has form My question. ""What"" plays the role of (algebraic manner) in the analytical world? Maybe I'm confused now but let's pick up a concrete example, say ( holomorphic), it obviously has a regular singular point at but its matrix is so by Turrrittin's theorem, it does not have a regular singular at ? What is the reason for this contradiction? Perhaps I missed some point, any explanation is appreciate. Is there any motivation for considering the condition ? Update . The second question is solved.","11^{th} k 0 K/k k W K \nabla : W \to \Omega^1_{K/k} \otimes W \nabla(fw) = df \otimes w + f\nabla (w) \ \forall f \in K, w \in W. \nabla: \mathrm{Der}_k(K,K) \to \mathrm{End}_k(W) \mathfrak{p} \mathcal{O}_{\mathfrak{p}} \mathfrak{m}_{\mathfrak{p}} \mathrm{ord}_{\mathfrak{p}}: K \to \mathbb{Z} \cup \left \{\infty \right\} \mathfrak{p} \mathcal{O} = \left \{f \in K \mid \mathrm{ord}(f) \geq 0 \right \} \mathfrak{m} = \left \{f \in K \mid \mathrm{ord}(f) \geq 1 \right \}. \left \{D \in \mathrm{Der}_k(K,K) \mid D(\mathfrak{m}) \subset \mathfrak{m} \right \} = (h\frac{d}{dh})\mathcal{O}. \mathbf{e} W \nabla\left(h\frac{d}{dh}\right)\mathbf{e} = B\mathbf{e} \ \text{for some} \ B \in M_{\mathrm{dim}(W)}(\mathcal{O}) \mathfrak{p} w \in W w,(\nabla(D))(w),...,(\nabla(D))^k(w),... W K (W,\nabla) w (W,\nabla) w \mathfrak{p} h \mathfrak{p} (W,\nabla) \mathfrak{p} \mathbf{e} = \left(w, \nabla\left(h\frac{d}{dh}\right)(w),...\nabla\left(h\frac{d}{dh}\right)^{n-1}(w) \right)^t, \nabla\left(h\frac{d}{dh}\right)\mathbf{e} = \begin{pmatrix}
0 & 1 & ... & 0 &0 \\
. &  &...  &  & ...\\
0 & 0 &  & 0 & 1\\
-f_0 & -f_1 & . &  .& -f_{n-1}
\end{pmatrix}\mathbf{e} i \mathrm{ord}(f_i) < 0 A K K W \to A f \in A \frac{d^n f}{dh^n} + f_{n-1}\frac{d^{(n-1)}f}{dh^{(n-1)}}+...+f_0=0. a f_{n-i} i a n n (f,f',f^{""},...,f^{(n-1)}) \nabla_{\frac{d}{dz}} = \frac{d}{dz} +  \begin{pmatrix}
0 & 1 & ... & 0 &0 \\
. &  &...  &  & ...\\
0 & 0 &  & 0 & 1\\
-f_0 & -f_1 & . &  .& -f_{n-1}
\end{pmatrix}dz \nabla\left(h\frac{d}{dh}\right) f^{""}+\frac{p}{z}f'+\frac{q}{z^2}f=0 p,q z=0 \begin{pmatrix}
0 & 1\\ 
 \frac{q}{z^2}& \frac{p}{z} 
\end{pmatrix} z=0 D(\mathfrak{m})\subset \mathfrak{m}","['ordinary-differential-equations', 'algebraic-geometry', 'algebraic-curves', 'connections']"
63,Intuition behind the method of characteristics,Intuition behind the method of characteristics,,"I have learnt the method of characteristics in a past PDE course, but it has always been taught as a sort of sequence of steps which lead to a solution. This has always bothered me and I would like to understand the geometric picture behind the method and some of the consequences that follow (i.e. how and what it means by a solution propagating along characteristics). I have tried going through a few resources including books by Strauss and Evans, lecture notes such as this one by Stanford , and other posts on this website ( Explaining the method of characteristics ). Despite all this something is still not clicking intuitively. Here is what I have understood so far, let's take the following PDE as an example: $$a(x,y) u_x + b(x,y)u_y = c(x,y)$$ We first observe that the PDE can be written as $$\bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle \cdot \bigl \langle u_x, u_y, -1 \bigr\rangle = 0$$ which tells us that the vector $V = \bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle$ must lie in the tangent plane to the graph/surface $S = \{(x,y,u(x,y))\}$ . Hence we would like to determine $S$ such that for each point $(x,y,z) \in S$ , its tangent plane contains the vector $V$ . This amounts to to finding a curve $\mathcal{C}$ which lies in $S$ . We parametrize $\mathcal{C}$ by a variable $s$ , so that the tangent vector $V$ is now $$V_s = \bigl \langle a\bigl(x(s),y(s)\bigr), b\bigl(x(s),y(s)\bigr), c\bigl(x(s),y(s)\bigr)\bigr\rangle$$ Thus the curve $\mathcal{C} = \{(x(s), y(s), z(s))\}$ results in the following system of ODEs: $$\frac{dx}{ds} = a\bigl(x(s), y(s)\bigr)\\ \frac{dy}{ds} = b\bigl(x(s), y(s)\bigr)\\ \frac{dz}{ds} = c\bigl(x(s), y(s)\bigr)$$ We finally obtain our characteristic curves by solving the above system of equations. Taking the union of all the resulting characteristic curves results in a solution surface for our original PDE. Somehow, after solving a first-order PDE using this method, for example taking the transport equation, it becomes apparent that $z(x,t)$ is constant along the lines $x-at = x_0$ but how? Furthermore, how does this imply that $u(x,t) = z(x,t) = f(x-at)$ ? My questions: Why do we parametrize the curve $\mathcal{C}$ to begin with? What does it mean that characteristic curves propagate discontinuities or solutions? How does one infer this from this picture? I am still unclear what these curves are ultimately telling us. Does it tell us the ""path"" that some Cauchy data travels in the $xyz$ plane? Are there any mistakes in my understanding? The definition of $S$ seems to come out of nowhere, what is the motivation behind using such a surface?","I have learnt the method of characteristics in a past PDE course, but it has always been taught as a sort of sequence of steps which lead to a solution. This has always bothered me and I would like to understand the geometric picture behind the method and some of the consequences that follow (i.e. how and what it means by a solution propagating along characteristics). I have tried going through a few resources including books by Strauss and Evans, lecture notes such as this one by Stanford , and other posts on this website ( Explaining the method of characteristics ). Despite all this something is still not clicking intuitively. Here is what I have understood so far, let's take the following PDE as an example: We first observe that the PDE can be written as which tells us that the vector must lie in the tangent plane to the graph/surface . Hence we would like to determine such that for each point , its tangent plane contains the vector . This amounts to to finding a curve which lies in . We parametrize by a variable , so that the tangent vector is now Thus the curve results in the following system of ODEs: We finally obtain our characteristic curves by solving the above system of equations. Taking the union of all the resulting characteristic curves results in a solution surface for our original PDE. Somehow, after solving a first-order PDE using this method, for example taking the transport equation, it becomes apparent that is constant along the lines but how? Furthermore, how does this imply that ? My questions: Why do we parametrize the curve to begin with? What does it mean that characteristic curves propagate discontinuities or solutions? How does one infer this from this picture? I am still unclear what these curves are ultimately telling us. Does it tell us the ""path"" that some Cauchy data travels in the plane? Are there any mistakes in my understanding? The definition of seems to come out of nowhere, what is the motivation behind using such a surface?","a(x,y) u_x + b(x,y)u_y = c(x,y) \bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle \cdot \bigl \langle u_x, u_y, -1 \bigr\rangle = 0 V = \bigl \langle a(x,y), b(x,y), c(x,y)\bigr\rangle S = \{(x,y,u(x,y))\} S (x,y,z) \in S V \mathcal{C} S \mathcal{C} s V V_s = \bigl \langle a\bigl(x(s),y(s)\bigr), b\bigl(x(s),y(s)\bigr), c\bigl(x(s),y(s)\bigr)\bigr\rangle \mathcal{C} = \{(x(s), y(s), z(s))\} \frac{dx}{ds} = a\bigl(x(s), y(s)\bigr)\\
\frac{dy}{ds} = b\bigl(x(s), y(s)\bigr)\\
\frac{dz}{ds} = c\bigl(x(s), y(s)\bigr) z(x,t) x-at = x_0 u(x,t) = z(x,t) = f(x-at) \mathcal{C} xyz S","['real-analysis', 'calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
64,Mathematically: are magnetic field lines (almost) closed loops?,Mathematically: are magnetic field lines (almost) closed loops?,,"Question: Let $X:[0, 1] \rightarrow \mathbb{R}^3$ be a smooth curve, and $\mathbf{B}$ be the vector field in $\mathbb{R}^3$ defined as $$ \mathbf{B}(y) = \int_{\mathbb{R}^3} \frac{y-X(t)}{\|y-X(t)\|^3} \times X'(t) \ dt.$$ Considering the flow of $\mathbf{B}$ , here are some possibilities for its orbits: singleton (where $\mathbf{B} = 0$ ) closed loop (periodic orbit) Not closed, but limits (including point at infinity) as $t \rightarrow \infty$ and $t \rightarrow -\infty$ both exist. Can there exist other possibilites? (e.g. orbit which asymptotically approaches to a closed loop) Details: By Biot-Savart law, $\mathbf{B}$ is the magnetic field generated by current through the curve $X$ (I must have missed some constants), and the flow of $\mathbf{B}$ is called 'magnetic field lines.' I could found affirmative answers like 'Yes, because there is no magnetic monopole', but I desire to find mathematical proof/counterexample.","Question: Let be a smooth curve, and be the vector field in defined as Considering the flow of , here are some possibilities for its orbits: singleton (where ) closed loop (periodic orbit) Not closed, but limits (including point at infinity) as and both exist. Can there exist other possibilites? (e.g. orbit which asymptotically approaches to a closed loop) Details: By Biot-Savart law, is the magnetic field generated by current through the curve (I must have missed some constants), and the flow of is called 'magnetic field lines.' I could found affirmative answers like 'Yes, because there is no magnetic monopole', but I desire to find mathematical proof/counterexample.","X:[0, 1] \rightarrow \mathbb{R}^3 \mathbf{B} \mathbb{R}^3  \mathbf{B}(y) = \int_{\mathbb{R}^3} \frac{y-X(t)}{\|y-X(t)\|^3} \times X'(t) \ dt. \mathbf{B} \mathbf{B} = 0 t \rightarrow \infty t \rightarrow -\infty \mathbf{B} X \mathbf{B}","['ordinary-differential-equations', 'differential-geometry', 'mathematical-physics']"
65,Asking for an example of stiff ODE,Asking for an example of stiff ODE,,"I am currently studying on the numerical solutions of ODEs and I'm trying to calculate the difference of the exact solution to numerically evaluated solution and then just plot the absolute difference. I'm doing this so I can eventually see how good these numerical methods can actually calculate the error. For example, here is an example from numerical recipes: $$u' = 998u + 1998v$$ $$v' = -999u - 1999v$$ with boundary conditions $$u(0) = 1 $$ $$v(0) = 0$$ With the exact (analytical) solution of $$u = 2e^{-x} − e^{-1000x} $$ $$v = -e^{-x} + e^{-1000x} $$ Now when I use the BDF method with rtol=1e-12,atol=1e-30 and plot the differences, I get: Now the thing is, I think these equations aren't just stiff enough to really challenge the solver, and I'm looking for a multi-scale ODE with difference of at least $10^{10}$ which has an exact solution. Can anyone suggest such an ODE?","I am currently studying on the numerical solutions of ODEs and I'm trying to calculate the difference of the exact solution to numerically evaluated solution and then just plot the absolute difference. I'm doing this so I can eventually see how good these numerical methods can actually calculate the error. For example, here is an example from numerical recipes: with boundary conditions With the exact (analytical) solution of Now when I use the BDF method with rtol=1e-12,atol=1e-30 and plot the differences, I get: Now the thing is, I think these equations aren't just stiff enough to really challenge the solver, and I'm looking for a multi-scale ODE with difference of at least which has an exact solution. Can anyone suggest such an ODE?",u' = 998u + 1998v v' = -999u - 1999v u(0) = 1  v(0) = 0 u = 2e^{-x} − e^{-1000x}  v = -e^{-x} + e^{-1000x}  10^{10},"['ordinary-differential-equations', 'numerical-methods']"
66,The flux of a the negative gradient flow of a Morse-Bott function on a compact manifold converges to a critical value?,The flux of a the negative gradient flow of a Morse-Bott function on a compact manifold converges to a critical value?,,"Let $(M, g)$ be a compact Riemannian manifold and $f: M \rightarrow \mathbb{R}$ be a Morse-Bott function, i.e. the set a critical points of $f$ , $Crit(f)$ , has connected components which are smooth manifolds and which have as tangent spaces $T_x Crit(f) = \ker \nabla^2_x f$ , (where $\nabla^2_x f: T_x M \rightarrow T_xM$ is the linear operator obtained via $g$ from the hessian $f_{**,x} : T_x M \times T_x M \rightarrow \mathbb{R}$ defined as $f_{**,x}(v, w) = v(W(f))$ for $W \in \Gamma(TM)$ any extension of $w$ (this is well defined and symmetric at critical points) ) Let $\nabla f \in \Gamma(TM)$ be defined by $g(\nabla f, w) = w(f)$ and consider the flow of $-\nabla f$ denoted $\phi_t(y)$ . I am trying to see why for any $y \in M$ it happens that $\lim\limits_{t \rightarrow \infty} \phi_t(y) \in Crit(f)$ . My attempt: Since $M$ is compact, $\phi_t(y)$ is defined for all $t \in \mathbb{R}$ . If the set $A_y:= \{ \phi_t(y) : t \in \mathbb{R} \}$ were closed, then, since $M$ is compact, this set would also be compact, and by Weierstrass $f$ would have to attain its minimum on it. Since moving along the flowlines of the negative gradient can only decrease $f$ , this means that the minimum is attained at $x:= \lim\limits_{t \rightarrow \infty} \phi_t(y)$ , so then $x$ would be a critical value for $f|_{A_y}$ . But even ignoring the fact that I don't know why $A_y$ is necessarily closed, I don't see why if $x$ is a critical value for $f|_{A_y}$ , then it is a critical value for $f$ as well. I am thinking that this attempt not enough, as it doesn't use at all the fact that $f$ is a Morse-Bott function. But I don't see how to use this fact. I also know that $\nabla_x^2 f (v) = \nabla_V \nabla f$ for $x \in Crit(f)$ and $v \in T_xM$ and $V$ a vector field extending $v$ , where $\nabla_V (\cdot)$ in the RHS is the Levi-Civita connexion of $g$ , but I can't see how to use this either.","Let be a compact Riemannian manifold and be a Morse-Bott function, i.e. the set a critical points of , , has connected components which are smooth manifolds and which have as tangent spaces , (where is the linear operator obtained via from the hessian defined as for any extension of (this is well defined and symmetric at critical points) ) Let be defined by and consider the flow of denoted . I am trying to see why for any it happens that . My attempt: Since is compact, is defined for all . If the set were closed, then, since is compact, this set would also be compact, and by Weierstrass would have to attain its minimum on it. Since moving along the flowlines of the negative gradient can only decrease , this means that the minimum is attained at , so then would be a critical value for . But even ignoring the fact that I don't know why is necessarily closed, I don't see why if is a critical value for , then it is a critical value for as well. I am thinking that this attempt not enough, as it doesn't use at all the fact that is a Morse-Bott function. But I don't see how to use this fact. I also know that for and and a vector field extending , where in the RHS is the Levi-Civita connexion of , but I can't see how to use this either.","(M, g) f: M \rightarrow \mathbb{R} f Crit(f) T_x Crit(f) = \ker \nabla^2_x f \nabla^2_x f: T_x M \rightarrow T_xM g f_{**,x} : T_x M \times T_x M \rightarrow \mathbb{R} f_{**,x}(v, w) = v(W(f)) W \in \Gamma(TM) w \nabla f \in \Gamma(TM) g(\nabla f, w) = w(f) -\nabla f \phi_t(y) y \in M \lim\limits_{t \rightarrow \infty} \phi_t(y) \in Crit(f) M \phi_t(y) t \in \mathbb{R} A_y:= \{ \phi_t(y) : t \in \mathbb{R} \} M f f x:= \lim\limits_{t \rightarrow \infty} \phi_t(y) x f|_{A_y} A_y x f|_{A_y} f f \nabla_x^2 f (v) = \nabla_V \nabla f x \in Crit(f) v \in T_xM V v \nabla_V (\cdot) g","['ordinary-differential-equations', 'differential-geometry', 'morse-theory', 'gradient-flows']"
67,Why is a Laplace transform with unfixed contour a good Ansatz to solve Bessel's differential equation? In which situations is this a good Ansatz?,Why is a Laplace transform with unfixed contour a good Ansatz to solve Bessel's differential equation? In which situations is this a good Ansatz?,,"I've seen a remarkable Ansatz in solving an ODE with non constant coefficients and I am wondering: Why does this Ansatz work so well? Here is the description of the problem, the Ansatz and why I think it is remarkable: Consider the homogeneous linear ODE $$xy''+y'+xy = 0.$$ This equation has a solution space spanned by Bessel functions $Y_0$ (diverging for $x\rightarrow 0$ ) and $J_0$ (finite for the same limit). One can obtain an integral representation of $J_0$ by using the following Ansatz: $$y(x) = \int_C e^{xz}P(z) \mathrm{d}z,$$ where $C$ is an appropriately chosen contour in the complex plane. Of course, with such an Ansatz one wants to reduce the ODE to an algebraic equation, but normally, one achieves this via a Fourier or a Laplace transform. However, in both those approaches the contour is by definition fixed and I am wondering, where the insight comes from to set hopes in a Fourier style Ansatz with an unfixed contour. Are there other ODEs solveable by cooking up a solution using this Ansatz? What are the  typical features of such ODEs? Let me show how the Ansatz is employed to find $J_0$ , because this could help in answering my question. It is employed in two steps: Firstly, using the ODE, one obtains an equation that provides a simple ODE, secondly, this solution's boundary condition fixes the contour. One starts by plugging in the Ansatz to obtain $$\int_C (xz^2 + z + x)e^{xz}P(z) \mathrm{d}z = 0$$ Then one assumes that there is an $S(z)$ such that the integrand equals the total derivative of $e^{xz}S(z)$ . This requirement then yields the condition that $e^{xw}S(w)$ vanishes for $w$ in the boundary of the contour and relates $S$ and $P$ as follows: $$(xz^2 + z + x)P = x S + S'$$ This eqn holds for all x and thus we can compare the coefficients in front of $x^{0}$ and $x^{1}$ From those two conditions, we can divide out $P$ (assume $P$ to be non vanishing on the contour) to obtain a good ODE for $S$ reading $$ \frac{S'}{S} = \frac{z}{z^2 + 1},$$ whose solution provides us with $S = \sqrt{z^2 + 1}$ from which we can infer a contour running from $-i$ to $i$ , e.g. a straight line (due to the boundary condition on $e^{xw}S(w)$ ). Massaging $y$ a bit further, one ends up with the desired $J_0$ .","I've seen a remarkable Ansatz in solving an ODE with non constant coefficients and I am wondering: Why does this Ansatz work so well? Here is the description of the problem, the Ansatz and why I think it is remarkable: Consider the homogeneous linear ODE This equation has a solution space spanned by Bessel functions (diverging for ) and (finite for the same limit). One can obtain an integral representation of by using the following Ansatz: where is an appropriately chosen contour in the complex plane. Of course, with such an Ansatz one wants to reduce the ODE to an algebraic equation, but normally, one achieves this via a Fourier or a Laplace transform. However, in both those approaches the contour is by definition fixed and I am wondering, where the insight comes from to set hopes in a Fourier style Ansatz with an unfixed contour. Are there other ODEs solveable by cooking up a solution using this Ansatz? What are the  typical features of such ODEs? Let me show how the Ansatz is employed to find , because this could help in answering my question. It is employed in two steps: Firstly, using the ODE, one obtains an equation that provides a simple ODE, secondly, this solution's boundary condition fixes the contour. One starts by plugging in the Ansatz to obtain Then one assumes that there is an such that the integrand equals the total derivative of . This requirement then yields the condition that vanishes for in the boundary of the contour and relates and as follows: This eqn holds for all x and thus we can compare the coefficients in front of and From those two conditions, we can divide out (assume to be non vanishing on the contour) to obtain a good ODE for reading whose solution provides us with from which we can infer a contour running from to , e.g. a straight line (due to the boundary condition on ). Massaging a bit further, one ends up with the desired .","xy''+y'+xy = 0. Y_0 x\rightarrow 0 J_0 J_0 y(x) = \int_C e^{xz}P(z) \mathrm{d}z, C J_0 \int_C (xz^2 + z + x)e^{xz}P(z) \mathrm{d}z = 0 S(z) e^{xz}S(z) e^{xw}S(w) w S P (xz^2 + z + x)P = x S + S' x^{0} x^{1} P P S  \frac{S'}{S} = \frac{z}{z^2 + 1}, S = \sqrt{z^2 + 1} -i i e^{xw}S(w) y J_0","['ordinary-differential-equations', 'laplace-transform', 'fourier-transform', 'bessel-functions']"
68,"Differential equation $y' = (2y^2 + x)/(3y^2 + 5)$ (Apostol, section 8.28, ex. 30)","Differential equation  (Apostol, section 8.28, ex. 30)",y' = (2y^2 + x)/(3y^2 + 5),"Problem This is from Apostol's Calculus book, section 8.28, exercise 30. Let $y = f(x)$ be that solution of the differential equation $$y' = \dfrac{2y^2 + x}{3y^2 + 5}$$ which satisfies the initial condition $f(0) = 0$ . (a) The differential equation shows that $f'(0) = 0$ . Discuss whether $f$ has a relative maximum or minimum or neither at 0. (b) Notice that $f'(x) \geq 0$ for each $x \geq 0$ and that $f'(x) \geq \frac{2}{3}$ for each $x \geq \frac{10}{3}$ . Exhibit two positive numbers $a$ and $b$ such that $f(x) > ax - b$ for each $x \geq \frac{10}{3}$ . (c) Show that $x/y^2 \to 0$ as $x \to +\infty$ . (d) Show that $y/x$ tends to a finite limit as $x \to +\infty$ and determine this limit. Solution attempt Please, I would like to ask for verification of my attempt below. (a) The second derivative is given by: $$y'' = \dfrac{(3y^2 + 5)(4yy' + 1) - 6yy'(2y^2 + x)}{(3y^2 + 5)^2}$$ At zero, the value is: $$f''(0) = \dfrac{5}{(5)^2} = \dfrac{1}{5}$$ Since this value is positive, we can conclude that the concavity of $f$ at 0 is up, so $f$ has a relative minimum at 0. (b) Since $f'(x) \geq \frac{2}{3}$ for each $x \geq \frac{10}{3}$ , in order to have the line $ax - b$ below $f(x)$ , we can choose $a = \frac{2}{3}$ . Also, we can choose $b$ such that $ax - b = 0$ at $x = \frac{10}{3}$ (since $f(x) \geq 0$ there). This gives: $$\frac{2}{3} \cdot \frac{10}{3} - b = 0 \implies b = \frac{20}{9}$$ So, the numbers are $a = \frac{2}{3}$ and $b = \frac{20}{9}$ . (c) In item (b), we found that: $$y > \frac{2}{3}x - \frac{20}{9}$$ for $x \geq \frac{10}{3}$ . This implies: $$\frac{3}{2} y + \frac{10}{3} > x$$ for $x \geq \frac{10}{3}$ . Dividing both sides by $y^2$ , this becomes: $$\dfrac{3}{2y} + \dfrac{10}{3y^2} > \dfrac{x}{y^2}$$ Also, we know that $x/y^2 \geq 0$ , so we have: $$\dfrac{3}{2y} + \dfrac{10}{3y^2} > x/y^2 \geq 0$$ Since we know that $y > \frac{2}{3}x - \frac{20}{9}$ , we have that $y \to +\infty$ as $x \to +\infty$ , so the left-hand side of the left-hand inequality above tends to zero as $x \to \infty$ : $$\dfrac{3}{2y} + \dfrac{10}{3y^2} \to 0\text{ as }x \to \infty$$ Therefore, by the squeeze theorem, it follows that $x/y^2 \to 0$ as $x \to +\infty$ . (d) From item (c), we know that $x/y^2 \to 0$ as $x \to +\infty$ . So: $$\begin{aligned}     \lim_{x \to +\infty} y' &= \lim_{x \to +\infty} \dfrac{2y^2 + x}{3y^2 + 5} \\     &= \lim_{x \to +\infty} \dfrac{2 + x/y^2}{3 + 5/y^2} \\     &= \dfrac{2 + 0}{3 + 0} \\     &= \dfrac{2}{3} \end{aligned}$$ So, as $x \to +\infty$ , the function $y$ approaches a line of slope $\frac{2}{3}$ . That is, $y$ approaches a line of the form $y=\frac{2}{3}x + C$ , so $y/x \to \frac{2}{3}$ as $x \to +\infty$ . The argument in part (d) seems a bit informal; I'm not sure how to make it more rigorous.","Problem This is from Apostol's Calculus book, section 8.28, exercise 30. Let be that solution of the differential equation which satisfies the initial condition . (a) The differential equation shows that . Discuss whether has a relative maximum or minimum or neither at 0. (b) Notice that for each and that for each . Exhibit two positive numbers and such that for each . (c) Show that as . (d) Show that tends to a finite limit as and determine this limit. Solution attempt Please, I would like to ask for verification of my attempt below. (a) The second derivative is given by: At zero, the value is: Since this value is positive, we can conclude that the concavity of at 0 is up, so has a relative minimum at 0. (b) Since for each , in order to have the line below , we can choose . Also, we can choose such that at (since there). This gives: So, the numbers are and . (c) In item (b), we found that: for . This implies: for . Dividing both sides by , this becomes: Also, we know that , so we have: Since we know that , we have that as , so the left-hand side of the left-hand inequality above tends to zero as : Therefore, by the squeeze theorem, it follows that as . (d) From item (c), we know that as . So: So, as , the function approaches a line of slope . That is, approaches a line of the form , so as . The argument in part (d) seems a bit informal; I'm not sure how to make it more rigorous.","y = f(x) y' = \dfrac{2y^2 + x}{3y^2 + 5} f(0) = 0 f'(0) = 0 f f'(x) \geq 0 x \geq 0 f'(x) \geq \frac{2}{3} x \geq \frac{10}{3} a b f(x) > ax - b x \geq \frac{10}{3} x/y^2 \to 0 x \to +\infty y/x x \to +\infty y'' = \dfrac{(3y^2 + 5)(4yy' + 1) - 6yy'(2y^2 + x)}{(3y^2 + 5)^2} f''(0) = \dfrac{5}{(5)^2} = \dfrac{1}{5} f f f'(x) \geq \frac{2}{3} x \geq \frac{10}{3} ax - b f(x) a = \frac{2}{3} b ax - b = 0 x = \frac{10}{3} f(x) \geq 0 \frac{2}{3} \cdot \frac{10}{3} - b = 0 \implies b = \frac{20}{9} a = \frac{2}{3} b = \frac{20}{9} y > \frac{2}{3}x - \frac{20}{9} x \geq \frac{10}{3} \frac{3}{2} y + \frac{10}{3} > x x \geq \frac{10}{3} y^2 \dfrac{3}{2y} + \dfrac{10}{3y^2} > \dfrac{x}{y^2} x/y^2 \geq 0 \dfrac{3}{2y} + \dfrac{10}{3y^2} > x/y^2 \geq 0 y > \frac{2}{3}x - \frac{20}{9} y \to +\infty x \to +\infty x \to \infty \dfrac{3}{2y} + \dfrac{10}{3y^2} \to 0\text{ as }x \to \infty x/y^2 \to 0 x \to +\infty x/y^2 \to 0 x \to +\infty \begin{aligned}
    \lim_{x \to +\infty} y' &= \lim_{x \to +\infty} \dfrac{2y^2 + x}{3y^2 + 5} \\
    &= \lim_{x \to +\infty} \dfrac{2 + x/y^2}{3 + 5/y^2} \\
    &= \dfrac{2 + 0}{3 + 0} \\
    &= \dfrac{2}{3}
\end{aligned} x \to +\infty y \frac{2}{3} y y=\frac{2}{3}x + C y/x \to \frac{2}{3} x \to +\infty","['calculus', 'ordinary-differential-equations', 'solution-verification']"
69,Multivariable ODE with linear and quadratic terms,Multivariable ODE with linear and quadratic terms,,"Suppose $z(t):\mathbb R_+\to\mathbb R^n$ satisfies the ODE $$\frac{dz}{dt}=Az+\sum_k (C_kz)\odot(C_kz)$$ for matrices $A,C_1,\ldots,C_m\in\mathbb R^{n\times n}$ . Here $\odot$ denotes the Hadamard product. Is there any hope of writing a closed form solution for $z(t)$ ? In one dimension it seems the standard integrating factors trick works, but I’m not sure how to generalize.","Suppose satisfies the ODE for matrices . Here denotes the Hadamard product. Is there any hope of writing a closed form solution for ? In one dimension it seems the standard integrating factors trick works, but I’m not sure how to generalize.","z(t):\mathbb R_+\to\mathbb R^n \frac{dz}{dt}=Az+\sum_k (C_kz)\odot(C_kz) A,C_1,\ldots,C_m\in\mathbb R^{n\times n} \odot z(t)","['linear-algebra', 'ordinary-differential-equations']"
70,Tools or methods for solving a bizarre integral equation,Tools or methods for solving a bizarre integral equation,,"I am interested in solving (or determining that no solution exists) the following equation $$ f(x) + \int_{x^{-1}\tau}^1 f(xy)f(y)dy = G(x) $$ for a function $f:[\tau,1]\rightarrow \mathbb{R}$ given a target function $G:[\tau,1]\rightarrow \mathbb{R}$ , where $\tau\in(0,1)$ . For certain functions $G$ , a viable approach to solving this is the fixed point iteration $$ f_{n+1}(x) = G(x) - \int_{x^{-1}\tau}^1 f_n(xy)f_n(y)dy. $$ In particular, this works if the right hand side $G$ is sufficiently small. However, for larger functions $G$ this naive fixed point iteration fails to converge (I have observed this numerically). I also believe that it is possible that for certain functions $G$ the equation cannot be solved, although I have not been able to prove this. Are there any techniques for approaching an non-linear integral equation of this form, either numerically or theoretically? Unfortunately, no amount of differentiating can remove the integral and give an ODE, since the integrand depends on $x$ in a non-trivial manner. Has anyone seen something similar before?","I am interested in solving (or determining that no solution exists) the following equation for a function given a target function , where . For certain functions , a viable approach to solving this is the fixed point iteration In particular, this works if the right hand side is sufficiently small. However, for larger functions this naive fixed point iteration fails to converge (I have observed this numerically). I also believe that it is possible that for certain functions the equation cannot be solved, although I have not been able to prove this. Are there any techniques for approaching an non-linear integral equation of this form, either numerically or theoretically? Unfortunately, no amount of differentiating can remove the integral and give an ODE, since the integrand depends on in a non-trivial manner. Has anyone seen something similar before?","
f(x) + \int_{x^{-1}\tau}^1 f(xy)f(y)dy = G(x)
 f:[\tau,1]\rightarrow \mathbb{R} G:[\tau,1]\rightarrow \mathbb{R} \tau\in(0,1) G 
f_{n+1}(x) = G(x) - \int_{x^{-1}\tau}^1 f_n(xy)f_n(y)dy.
 G G G x","['real-analysis', 'integration', 'ordinary-differential-equations']"
71,How to solve the following Laplace equation,How to solve the following Laplace equation,,"Im struggling to find a solution for the following problem $$ \begin{cases} \varDelta u\left(x,y\right)=0 & \forall\left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\ u\left(x,0\right)=\sin\left(\pi x\right) & \forall x\in[0,2]\\ u\left(x,2\right)=\sin\left(3\pi x\right) & \forall x\in[0,2]\\ u\left(0,y\right)=0 & \forall y\in[0,2]\\ u\left(2,y\right)=\min\left\{ y,\left(2-y\right)\right\}  & \forall y\in[0,2] \end{cases} $$ I'll explain what I have tried. I dont know to solve this equation as is. What I do know is to solve Laplace equation's with homogenous boudary conditions. So I tried to find a function $ w(x,y) $ such that either $$ \begin{cases} u\left(x,0\right)-w\left(x,0\right)=0, & \forall x\in[0,2]\\ u\left(x,2\right)-w\left(x,2\right)=0 & \forall x\in[0,2] \end{cases} $$ Or $$ \begin{cases} u\left(0,y\right)-w\left(0,y\right)=0, & \forall y\in[0,2]\\ u\left(2,y\right)-w\left(2,y\right)=0 & \forall y\in[0,2] \end{cases} $$ So I defined $ w\left(x,y\right)=\sin\left(\pi x+\pi xy\right) $ , and then I defined $ v\left(x,y\right)=u\left(x,y\right)-w\left(x,y\right) $ So that $$  \begin{cases} \varDelta v\left(x,y\right)=\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right) & \left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\ v\left(x,0\right)=0 & \forall x\in[0,2]\\ v\left(x,2\right)=0 & \forall x\in[0,2]\\ v\left(0,y\right)=0 & \forall y\in[0,2]\\ v\left(2,y\right)=\min\left(y,\left(2-y\right)\right)-\sin\left(\pi x+\pi xy\right) & \forall y\in[0,2] \end{cases}$$ I do know how to solve this equation, but it requires too much work and Im sure there is a simpler solution. I'll show my way to solve this equatoin: I'll use the Fourier series by the orthogonal system $ \left\{ \sin\left(\frac{\pi n}{2}y\right)\right\} _{n=1}^{\infty} $ (which consists of eigenfunctions of the Laplace eigenvalue equation in the $ y $ direction in the homogenous case) And I'll write each function as its generlized Fourier series: $ \begin{cases} v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}X_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}Z_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\ \min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right) \end{cases} $ One can check that the following holds: $ \begin{cases} X_{n}=\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\\ Z_{n}=-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\\ \\ \end{cases} $ So we get $$ \begin{cases} v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=-\sum_{n=1}^{\infty}\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\ \left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\ \min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right) \end{cases} $$ And we now how to calculate $ A_n $ and $ C_n $ because its gonna be the inner product in $L^2 $ between $\sin\left(\frac{\pi n}{2}y\right) $ and the compatible function. So if we'll insert each series into Laplace equation, and use the uniquness of the coefficients, we'll get an ordinary differential equation $ \frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)=A_{n} $ And we have intial condition because $$ 0=v\left(0,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(0\right)\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(0\right)=0 $$ And $ v\left(2,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(2\right)\sin\left(\frac{\pi n}{2}y\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(2\right)=A_{n} $ So if I'll work hard enough and calculate all those integrals, I will get a solution for $Y_n $ and therefore also for $ v(x,y) $ and then for $ u(x,y) $ . But Im looking for much simple way. Thanks in advance. UPDATE: I tried to define $ w\left(x,y\right)=\frac{\left(2-y\right)}{2}\sin\left(\pi x\right)+\frac{y}{2}\sin\left(3\pi x\right) $ And solve as I explained, but it also lead me to an endless algebra calculations.","Im struggling to find a solution for the following problem I'll explain what I have tried. I dont know to solve this equation as is. What I do know is to solve Laplace equation's with homogenous boudary conditions. So I tried to find a function such that either Or So I defined , and then I defined So that I do know how to solve this equation, but it requires too much work and Im sure there is a simpler solution. I'll show my way to solve this equatoin: I'll use the Fourier series by the orthogonal system (which consists of eigenfunctions of the Laplace eigenvalue equation in the direction in the homogenous case) And I'll write each function as its generlized Fourier series: One can check that the following holds: So we get And we now how to calculate and because its gonna be the inner product in between and the compatible function. So if we'll insert each series into Laplace equation, and use the uniquness of the coefficients, we'll get an ordinary differential equation And we have intial condition because And So if I'll work hard enough and calculate all those integrals, I will get a solution for and therefore also for and then for . But Im looking for much simple way. Thanks in advance. UPDATE: I tried to define And solve as I explained, but it also lead me to an endless algebra calculations."," \begin{cases}
\varDelta u\left(x,y\right)=0 & \forall\left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\
u\left(x,0\right)=\sin\left(\pi x\right) & \forall x\in[0,2]\\
u\left(x,2\right)=\sin\left(3\pi x\right) & \forall x\in[0,2]\\
u\left(0,y\right)=0 & \forall y\in[0,2]\\
u\left(2,y\right)=\min\left\{ y,\left(2-y\right)\right\}  & \forall y\in[0,2]
\end{cases}   w(x,y)   \begin{cases}
u\left(x,0\right)-w\left(x,0\right)=0, & \forall x\in[0,2]\\
u\left(x,2\right)-w\left(x,2\right)=0 & \forall x\in[0,2]
\end{cases}   \begin{cases}
u\left(0,y\right)-w\left(0,y\right)=0, & \forall y\in[0,2]\\
u\left(2,y\right)-w\left(2,y\right)=0 & \forall y\in[0,2]
\end{cases}   w\left(x,y\right)=\sin\left(\pi x+\pi xy\right)   v\left(x,y\right)=u\left(x,y\right)-w\left(x,y\right)    \begin{cases}
\varDelta v\left(x,y\right)=\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right) & \left(x,y\right)\in\left(0,2\right)\times\left(0,2\right)\\
v\left(x,0\right)=0 & \forall x\in[0,2]\\
v\left(x,2\right)=0 & \forall x\in[0,2]\\
v\left(0,y\right)=0 & \forall y\in[0,2]\\
v\left(2,y\right)=\min\left(y,\left(2-y\right)\right)-\sin\left(\pi x+\pi xy\right) & \forall y\in[0,2]
\end{cases}  \left\{ \sin\left(\frac{\pi n}{2}y\right)\right\} _{n=1}^{\infty}   y   \begin{cases}
v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}X_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}Z_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\
\min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right)
\end{cases}   \begin{cases}
X_{n}=\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\\
Z_{n}=-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\\
\\
\end{cases}   \begin{cases}
v\left(x,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial x^{2}}\left(x,y\right)=\sum_{n=1}^{\infty}\frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\frac{\partial^{2}v}{\partial y^{2}}\left(x,y\right)=-\sum_{n=1}^{\infty}\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)\sin\left(\frac{\pi n}{2}y\right)\\
\left(\left(\pi+\pi y\right)^{2}+\left(\pi x\right)^{2}\right)\sin\left(\pi x+\pi xy\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\\
\min\left(y,\left(2-y\right)\right)-\sin\left(\pi+\pi xy\right)=\sum_{n=1}^{\infty}C_{n}\sin\left(\frac{\pi n}{2}y\right)
\end{cases}   A_n   C_n  L^2  \sin\left(\frac{\pi n}{2}y\right)   \frac{d^{2}}{dx^{2}}Y_{n}\left(x\right)-\frac{\pi^{2}n^{2}}{4}Y_{n}\left(x\right)=A_{n}   0=v\left(0,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(0\right)\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(0\right)=0   v\left(2,y\right)=\sum_{n=1}^{\infty}Y_{n}\left(2\right)\sin\left(\frac{\pi n}{2}y\right)=\sum_{n=1}^{\infty}A_{n}\sin\left(\frac{\pi n}{2}y\right)\to Y_{n}\left(2\right)=A_{n}  Y_n   v(x,y)   u(x,y)   w\left(x,y\right)=\frac{\left(2-y\right)}{2}\sin\left(\pi x\right)+\frac{y}{2}\sin\left(3\pi x\right) ","['ordinary-differential-equations', 'partial-differential-equations']"
72,"How to prove that if $g*g=0$, then $g=0$?","How to prove that if , then ?",g*g=0 g=0,"Given a continuous function $g$ , if the convolution $g*g(t)$ (defined as: $\int^t_0(g(r)g(t−r))dr$ equals $0$ , $\forall t\geq 0$ , then $g=0$ . My attempt was to use Laplace transformation, but such transformation for the function could be non-existent. I'm also aware of the Titchmarsh theorem, but all the proofs I've found involve material that I hadn't studied yet. The proof for the claim above should neither be very lengthy, nor include material that is studied in Harmonic Analysis. How can one prove this claim using relatively simple means?","Given a continuous function , if the convolution (defined as: equals , , then . My attempt was to use Laplace transformation, but such transformation for the function could be non-existent. I'm also aware of the Titchmarsh theorem, but all the proofs I've found involve material that I hadn't studied yet. The proof for the claim above should neither be very lengthy, nor include material that is studied in Harmonic Analysis. How can one prove this claim using relatively simple means?",g g*g(t) \int^t_0(g(r)g(t−r))dr 0 \forall t\geq 0 g=0,"['calculus', 'ordinary-differential-equations', 'laplace-transform', 'convolution']"
73,Well-posedness of non-local equations. Deterministic. Singular. Reference request.,Well-posedness of non-local equations. Deterministic. Singular. Reference request.,,"Any references would be much appreciated. I'm looking for some well-posedness results for flow problems of the following type (note my issue is that I'm considering a singular non-local term). Let $\mathcal{P}(\mathbb{R}^d)$ be the space of probability measures on $\mathbb{R}^d$ . Let $\mu_0$ be some absolutely continuous (w.r.t Lebesgue) that is in $\mathcal{P}(\mathbb{R}^d)$ , lets $\textit{also}$ denote its density by $\mu_0$ . Consider the following flow $X:\mathbb{R}^+\times \mathbb{R}^d\to\mathbb{R}^d$ of the vector field $b$ \begin{equation} \begin{cases} &\partial_t X = b[\mu_0]\circ X, \\ & X(0,\cdot)=\text{id}. \end{cases} \end{equation} where $b:\mathcal{P}(\mathbb{R}^d)\times \mathbb{R}^d \to \mathbb{R}^d$ is a non-local  vector field $b[\mu](x)=\int B(x-y)\mu(dy) $ , in the form of a convolution. At least formally the push-forward measure $\mu(t,\cdot)=\big(X(t,\cdot)\big)_{\#}\mu_0$ solves the continuity equation \begin{equation}     \begin{cases}     \partial_t \mu(t,\cdot) + \text{div} \big( \mu(t,\cdot) b[\mu_0]\big)=0     \\     \mu(t,\cdot)|_{t=0}=\mu_0.     \end{cases} \end{equation} I'm interested well-posedness of the above two equations and their relation via the push-forward. DiPerna-Lions theory (""Ordinary differential equations, transport theory and Sobolev spaces"") has some stuff but im interested in the case of singular kernals $B$ , for example \begin{equation*} B(r):=     C \begin{cases}    {\|r\|^{-(d-2)}} & \text{for}~d>2 \\  \log\|r\|  &\text{for}~d=2 \end{cases}, \end{equation*} for some $C\in \mathbb{R}$ .","Any references would be much appreciated. I'm looking for some well-posedness results for flow problems of the following type (note my issue is that I'm considering a singular non-local term). Let be the space of probability measures on . Let be some absolutely continuous (w.r.t Lebesgue) that is in , lets denote its density by . Consider the following flow of the vector field where is a non-local  vector field , in the form of a convolution. At least formally the push-forward measure solves the continuity equation I'm interested well-posedness of the above two equations and their relation via the push-forward. DiPerna-Lions theory (""Ordinary differential equations, transport theory and Sobolev spaces"") has some stuff but im interested in the case of singular kernals , for example for some .","\mathcal{P}(\mathbb{R}^d) \mathbb{R}^d \mu_0 \mathcal{P}(\mathbb{R}^d) \textit{also} \mu_0 X:\mathbb{R}^+\times \mathbb{R}^d\to\mathbb{R}^d b \begin{equation}
\begin{cases}
&\partial_t X = b[\mu_0]\circ X,
\\
& X(0,\cdot)=\text{id}.
\end{cases}
\end{equation} b:\mathcal{P}(\mathbb{R}^d)\times \mathbb{R}^d \to \mathbb{R}^d b[\mu](x)=\int B(x-y)\mu(dy)  \mu(t,\cdot)=\big(X(t,\cdot)\big)_{\#}\mu_0 \begin{equation}
    \begin{cases}
    \partial_t \mu(t,\cdot) + \text{div} \big( \mu(t,\cdot) b[\mu_0]\big)=0
    \\
    \mu(t,\cdot)|_{t=0}=\mu_0.
    \end{cases}
\end{equation} B \begin{equation*}
B(r):= 
   C \begin{cases}
   {\|r\|^{-(d-2)}} & \text{for}~d>2
\\
 \log\|r\|  &\text{for}~d=2
\end{cases},
\end{equation*} C\in \mathbb{R}","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'fluid-dynamics', 'singularity']"
74,Find the control for this system of ODE minimizing the energy.,Find the control for this system of ODE minimizing the energy.,,"Suppose I have the system of ODE $$x'(t)=Ax(t)+Bu(t)$$ $$x(0)=x_0$$ $t$ is defined on an interval $I$ of $\mathbb{R}$ containing $0$ , for $x(t)=(x_1(t),...x_n(t))^T$ , $x_i : I \to \mathbb{R} $ differentible on $I$ , $i=1,2,...,n$ , $A$ is a real valued $n \times n$ matrix, $B \in \mathbb{R}^n$ and $u :I \to \mathbb{R}$ is the control. Find the control $u$ such that $$x(T)=x_T$$ for $T \in I$ and $$\int_0^T u^2(t)dt$$ is minimal. I am asked to solve this by following the steps below, Step 1) find the solution of $$x'(t)=Ax(t)+Bu(t)$$ Step 2) define a linear operator $L$ such that $x(T)=x_T$ can be written as $$Lu=x_T$$ Step 3) find minimal quadratic solution for the above equation. I am asked to first solve $x'=Ax+Bu$ , so by using variation of parameters and the exponential matrix, I get the fundamental solution $$x(t)=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds$$ Then I must define a linear operator, the operator $$Lu=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds$$ is not linear but the particular solution $$Lu=\int_0^t e^{A(t-s)}Bu(s)ds$$ is, so I am guessing that I only need the particular solution (is this right? and if yes why?) So I get $$x(T)=Lu=\int_0^T e^{A(T-s)}Bu(s)ds=x_T$$ So now I need to find a minimal quadratic solution to this integral equation, I suppose that means I should find a $u$ such that $$\int_0^T u^2(s)ds$$ is minimal, I am not sure on how to approach this, one idea I had is to use Lagrange multipliers, so I define $$h=\int_0^T u^2(s)ds-\int_0^T \lambda (s) e^{A(T-s)} B u(s)ds$$ where $\lambda (s)$ are the Lagrange multipliers, but I am not sure whether this is right. I am aware of other approaches to such problems like for example, by defining the Hamiltonian $H=u^2+\lambda (Ax+Bu)$ of the problem and solving the equations $$\dot{\lambda }=-H_x$$ $$\dot{x}=H_{\lambda }$$ Such that $$H_u=0$$ but I don't know if this is related to the approach I am asked to take. Can you help?","Suppose I have the system of ODE is defined on an interval of containing , for , differentible on , , is a real valued matrix, and is the control. Find the control such that for and is minimal. I am asked to solve this by following the steps below, Step 1) find the solution of Step 2) define a linear operator such that can be written as Step 3) find minimal quadratic solution for the above equation. I am asked to first solve , so by using variation of parameters and the exponential matrix, I get the fundamental solution Then I must define a linear operator, the operator is not linear but the particular solution is, so I am guessing that I only need the particular solution (is this right? and if yes why?) So I get So now I need to find a minimal quadratic solution to this integral equation, I suppose that means I should find a such that is minimal, I am not sure on how to approach this, one idea I had is to use Lagrange multipliers, so I define where are the Lagrange multipliers, but I am not sure whether this is right. I am aware of other approaches to such problems like for example, by defining the Hamiltonian of the problem and solving the equations Such that but I don't know if this is related to the approach I am asked to take. Can you help?","x'(t)=Ax(t)+Bu(t) x(0)=x_0 t I \mathbb{R} 0 x(t)=(x_1(t),...x_n(t))^T x_i : I \to \mathbb{R}  I i=1,2,...,n A n \times n B \in \mathbb{R}^n u :I \to \mathbb{R} u x(T)=x_T T \in I \int_0^T u^2(t)dt x'(t)=Ax(t)+Bu(t) L x(T)=x_T Lu=x_T x'=Ax+Bu x(t)=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds Lu=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds Lu=\int_0^t e^{A(t-s)}Bu(s)ds x(T)=Lu=\int_0^T e^{A(T-s)}Bu(s)ds=x_T u \int_0^T u^2(s)ds h=\int_0^T u^2(s)ds-\int_0^T \lambda (s) e^{A(T-s)} B u(s)ds \lambda (s) H=u^2+\lambda (Ax+Bu) \dot{\lambda }=-H_x \dot{x}=H_{\lambda } H_u=0","['ordinary-differential-equations', 'operator-theory', 'control-theory', 'integral-equations', 'optimal-control']"
75,Null Space of a Differential Equation,Null Space of a Differential Equation,,"I am studying the differential equations section of Calc 2 — Homogeneous Linear 2nd Order O.D.Es with constant coefficients (what a mouthful). The example given is $y''+2y'-8y=0$ , with general solution $y(x)=Ae^{2x}+Be^{-4x}$ . I was introduced to the idea that differentiation is a linear transformation in my linear algebra course. If we take the differential equation, $y''+2y'-8y$ to be our linear transformation, does that mean that the general solution, $y(x)$ , is our nullspace? Furthermore, could we say that a basis for our nullspace is the vectors $e^{2x}$ and $e^{-4x}$ . My course mentions nothing about this, but the ideas feel similar. Could anyone provide some insight into if these seemingly separate topics of study are connected? Any further reading on this link would be great.","I am studying the differential equations section of Calc 2 — Homogeneous Linear 2nd Order O.D.Es with constant coefficients (what a mouthful). The example given is , with general solution . I was introduced to the idea that differentiation is a linear transformation in my linear algebra course. If we take the differential equation, to be our linear transformation, does that mean that the general solution, , is our nullspace? Furthermore, could we say that a basis for our nullspace is the vectors and . My course mentions nothing about this, but the ideas feel similar. Could anyone provide some insight into if these seemingly separate topics of study are connected? Any further reading on this link would be great.",y''+2y'-8y=0 y(x)=Ae^{2x}+Be^{-4x} y''+2y'-8y y(x) e^{2x} e^{-4x},"['linear-algebra', 'ordinary-differential-equations', 'soft-question', 'linear-transformations']"
76,Do repeated roots (and Real Jordan form) for ODE's come up in real world applications of ODE's,Do repeated roots (and Real Jordan form) for ODE's come up in real world applications of ODE's,,"An equation like $y^{\prime \prime} + 2 y^{\prime} + y = 0$ has repeated roots: The characteristic polynomial is $r^2 + 2r + 1$ which has repeated roots $(-1,-1)$ . Two basic solutions of the ODE are then $e^{-t}$ and $t e^{-t}$ . More general examples exist with systems of ODE's and involve real Jordan form (aka real cannonical form) as explained in the textbook by Hirsch and Smale. My question is whether this has real applications. In order to get repeated roots, the ""true"" ODE would have to be exactly a certain way and any slight deviation would destroy the repeated roots. I suppose people call non-repeated roots ""generic"". So this would seem very rare. Does this come up real applications? I could maybe see it in physics or somethings where there are well-defined laws governing a system but hard to see it in a setting where the ODE is really just a model of what is in reality a more complex system.","An equation like has repeated roots: The characteristic polynomial is which has repeated roots . Two basic solutions of the ODE are then and . More general examples exist with systems of ODE's and involve real Jordan form (aka real cannonical form) as explained in the textbook by Hirsch and Smale. My question is whether this has real applications. In order to get repeated roots, the ""true"" ODE would have to be exactly a certain way and any slight deviation would destroy the repeated roots. I suppose people call non-repeated roots ""generic"". So this would seem very rare. Does this come up real applications? I could maybe see it in physics or somethings where there are well-defined laws governing a system but hard to see it in a setting where the ODE is really just a model of what is in reality a more complex system.","y^{\prime \prime} + 2 y^{\prime} + y = 0 r^2 + 2r + 1 (-1,-1) e^{-t} t e^{-t}","['ordinary-differential-equations', 'mathematical-modeling', 'applications']"
77,Normal forms of differential equations and vector fields,Normal forms of differential equations and vector fields,,"INTRODUCTION Consider an equation $$ \dot{x} = Ax + \varphi(x), $$ where $x \in \mathbb{R}^n$ , $A \neq 0$ is constant matrix, $\varphi(x)$ is a germ of smooth vector field in $0$ s.t. $$ \left\{ \begin{align*} &\varphi(0) = 0\\ &\frac{d\varphi}{dx}(0) = 0 \end{align*}\right. . $$ There are plenty of methods to prove that this equation is smoothly equivalent to its linearisation $\dot{y} = Ay$ in some neighbourhood of zero. For example, there is method of homological equation described in Arnold's book. Or we can just substitute some unknown function  and derive an ODE for this function. The first question is, what other methods are known and which of them can be generalised to the case of parametric families of vector fields? MY APPROACH I've also conducted an experiment, and came up with the following method. We can introduce a parameter $\varepsilon$ : $$ \dot{x}(t,\varepsilon) = Ax(t,\varepsilon) + \varepsilon\varphi(x(t,\varepsilon)). $$ When $\varepsilon = 0$ we get the linearised system, and when $\varepsilon = 1$ we have the original system. Now denote $$ y = \frac{\partial x(t,\varepsilon)}{\partial \varepsilon}. $$ Then we can differentiate the whole equation w.r.t. $\varepsilon$ : $$ \frac{\partial}{\partial \varepsilon}\dot{x}(t,\varepsilon) = \frac{\partial}{\partial \varepsilon}Ax(t,\varepsilon) + \frac{\partial}{\partial \varepsilon}\varepsilon\varphi(x(t,\varepsilon)), $$ $$ \dot{y}(t,\varepsilon) = A y(t,\varepsilon) + \varphi(x(t,\varepsilon)) + \varepsilon\varphi'(x(t,\varepsilon))y(t,\varepsilon). $$ We know that the solution $x(t,\varepsilon)$ exists, is unique and smooth in some neighbourhood of zero $U$ for all $\varepsilon \in [0,1]$ , so everything is alright and we can define smooth functions $F(t,\varepsilon) = \varphi(x(t,\varepsilon))$ , $f(t,\varepsilon) = \varepsilon\varphi'(x(t,\varepsilon)) + A$ in $U \times [0,1]$ . Then our equation $$ \dot{y}(t,\varepsilon) = f(t,\varepsilon)y(t,\varepsilon) + F(t,\varepsilon) $$ has a unique smooth solution which satisfies a condition $y(0,\varepsilon) = 0$ maybe in some smaller neighbourhood of zero $W$ for all $\varepsilon \in [0,1]$ . Finally, if we have two systems $$ \dot{x_0}(t) = A x_0(t) $$ and $$ \dot{x_1}(t) = A x_1(t) + \varphi(x_1(t)), $$ then the substitution $$ x_1(t) = x_0(t) + \int_{0}^{1} y(t, \xi) \, d\xi $$ turns the second system into the first one in some neighbourhood of zero $W$ . Is my approach correct? If not, how can it be fixed? Can we improve it? Can we extend it to the case of parametric families of vector fields?","INTRODUCTION Consider an equation where , is constant matrix, is a germ of smooth vector field in s.t. There are plenty of methods to prove that this equation is smoothly equivalent to its linearisation in some neighbourhood of zero. For example, there is method of homological equation described in Arnold's book. Or we can just substitute some unknown function  and derive an ODE for this function. The first question is, what other methods are known and which of them can be generalised to the case of parametric families of vector fields? MY APPROACH I've also conducted an experiment, and came up with the following method. We can introduce a parameter : When we get the linearised system, and when we have the original system. Now denote Then we can differentiate the whole equation w.r.t. : We know that the solution exists, is unique and smooth in some neighbourhood of zero for all , so everything is alright and we can define smooth functions , in . Then our equation has a unique smooth solution which satisfies a condition maybe in some smaller neighbourhood of zero for all . Finally, if we have two systems and then the substitution turns the second system into the first one in some neighbourhood of zero . Is my approach correct? If not, how can it be fixed? Can we improve it? Can we extend it to the case of parametric families of vector fields?","
\dot{x} = Ax + \varphi(x),
 x \in \mathbb{R}^n A \neq 0 \varphi(x) 0 
\left\{
\begin{align*}
&\varphi(0) = 0\\
&\frac{d\varphi}{dx}(0) = 0
\end{align*}\right. .
 \dot{y} = Ay \varepsilon 
\dot{x}(t,\varepsilon) = Ax(t,\varepsilon) + \varepsilon\varphi(x(t,\varepsilon)).
 \varepsilon = 0 \varepsilon = 1 
y = \frac{\partial x(t,\varepsilon)}{\partial \varepsilon}.
 \varepsilon 
\frac{\partial}{\partial \varepsilon}\dot{x}(t,\varepsilon) = \frac{\partial}{\partial \varepsilon}Ax(t,\varepsilon) + \frac{\partial}{\partial \varepsilon}\varepsilon\varphi(x(t,\varepsilon)),
 
\dot{y}(t,\varepsilon) = A y(t,\varepsilon) + \varphi(x(t,\varepsilon)) + \varepsilon\varphi'(x(t,\varepsilon))y(t,\varepsilon).
 x(t,\varepsilon) U \varepsilon \in [0,1] F(t,\varepsilon) = \varphi(x(t,\varepsilon)) f(t,\varepsilon) = \varepsilon\varphi'(x(t,\varepsilon)) + A U \times [0,1] 
\dot{y}(t,\varepsilon) = f(t,\varepsilon)y(t,\varepsilon) + F(t,\varepsilon)
 y(0,\varepsilon) = 0 W \varepsilon \in [0,1] 
\dot{x_0}(t) = A x_0(t)
 
\dot{x_1}(t) = A x_1(t) + \varphi(x_1(t)),
 
x_1(t) = x_0(t) + \int_{0}^{1} y(t, \xi) \, d\xi
 W","['ordinary-differential-equations', 'dynamical-systems', 'vector-fields']"
78,"What is the maximal interval for the IVP $x'(t) = x(t)(1-x(t))$, with $ x(0) = x_{0}$?","What is the maximal interval for the IVP , with ?",x'(t) = x(t)(1-x(t))  x(0) = x_{0},"I am new to ODEs and I'm stuck on this problem. I will write down my attempt.  The IVP in question is: \begin{cases} x'(t) = x(t)(1-x(t)), \\ x(0) = x_{0}. \end{cases} I applied separation of variables. I defined the function $f: X \mapsto \mathbb{R}$ , $X := X_{1} \times X_{2},$ \begin{align} f(t,x) := g(t)\cdot h(x) = x(t)(1-x(t)) \end{align} such that $g: X_{1} \mapsto \mathbb{R}$ and $h: X_{2}\mapsto \mathbb{R}$ , with $g(t) := 1$ and $h(x) := x(1-x), X_{1} := \mathbb{R}$ and $X_{2} := \mathbb{R}$ .  Then I defined the solution $x : I \mapsto \mathbb{R}$ , defined on an interval $I \subset \mathbb{R}$ such that $0 = t_{0} \in I$ too. Firstly, I checked when $h(x) = 0$ : \begin{align} h(x_{0}) = 0 \implies x_{0} = 0 \text{ or } x_{0} = 1. \end{align} Hence, \begin{align} \text{for } x_{0} = 0 \implies x(t) = 0; \\ \text{for } x_{0} = 1 \implies x(t) = 1. \end{align} Afterwards, I solved the equation given that $h(\xi) \neq 0$ ( $\implies x_{0} \neq 0$ and $x_{0} \neq 1$ ), $$ \int_{x_{0}}^{x}\frac{1}{h(\xi)} d\xi = \int_{t_{0}}^{t} g(\eta) d\eta, $$ i.e. \begin{align*} \int_{x_{0}}^{x}\frac{1}{\xi(1-\xi)} d\xi = \int_{0}^{t}  d\eta &\implies (\ln\lvert\xi\rvert - \ln\lvert 1-\xi\rvert)\big\rvert_{x_{0}}^{x} = t \\ &\implies \ln\Big\lvert\frac{\xi}{1-\xi}\Big\rvert\Bigg\rvert_{x_{0}}^{x} = t \\ &\implies \ln\Big\lvert\frac{x}{1-x}\Big\rvert - \ln\Big\lvert\frac{x_{0}}{1-x_{0}}\Big\rvert = t \\ &\implies \ln\Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = t  \\ &\implies \Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = e^{t}. \end{align*} I distinguished two cases: When $h(x_{0}) > 0 \implies h(x) > 0$ , so \begin{align*} &\frac{x(1-x_{0})}{x_{0}(1-x)} = e^{t}  \implies &x(t) = \frac{e^{t}x_{0}}{1-x_{0}(1-e^{t})}. \end{align*} Here is where I'm having issues with finding the maximal interval. Since, $h(x_{0}) > 0$ then \begin{align*} x_{0}(1-x_{0})>0 \implies \end{align*} \begin{cases} x_{0}>0 \\ 1-x_{0}>0 \end{cases} $$ \textbf{or} $$ \begin{cases} x_{0}<0 \\ 1-x_{0}<0 \end{cases} which results in $$ 0 < x_{0} < 1 .$$ Additionally, $$x_{0} \neq \frac{1}{1-e^{t}}.$$ However, in this case, $x(t)$ exists for $t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\}$ , since $$ 1-x_{0}(1-e^{t}) \neq 0 \implies t \neq \ln\Big(1- \frac{1}{x_{0}}\Big) $$ which is a problem because the bounds on $x_{0}$ make it so that $\ln(1- \frac{1}{x_{0}})$ can't exist. So, I'm wondering what I did wrong and it's confusing me a lot. For $h(x_{0})<0$ I got the same $x(t)$ but the bounds on $x_{0}$ are $$ x_{0} < 0 \cup x_{0} > 1 $$ which in this case works with $t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\}$ . What confuses me even more, is that by differentiating $x(t)$ you get the ODE in the IVP, so it's presumably right (?). I'm not sure if I get the wrong interval because I used the wrong approach, or I messed up the signs, or something else I'm not noticing. Any help/suggestions would be appreciated.","I am new to ODEs and I'm stuck on this problem. I will write down my attempt.  The IVP in question is: I applied separation of variables. I defined the function , such that and , with and and .  Then I defined the solution , defined on an interval such that too. Firstly, I checked when : Hence, Afterwards, I solved the equation given that ( and ), i.e. I distinguished two cases: When , so Here is where I'm having issues with finding the maximal interval. Since, then which results in Additionally, However, in this case, exists for , since which is a problem because the bounds on make it so that can't exist. So, I'm wondering what I did wrong and it's confusing me a lot. For I got the same but the bounds on are which in this case works with . What confuses me even more, is that by differentiating you get the ODE in the IVP, so it's presumably right (?). I'm not sure if I get the wrong interval because I used the wrong approach, or I messed up the signs, or something else I'm not noticing. Any help/suggestions would be appreciated.","\begin{cases}
x'(t) = x(t)(1-x(t)), \\
x(0) = x_{0}.
\end{cases} f: X \mapsto \mathbb{R} X := X_{1} \times X_{2}, \begin{align}
f(t,x) := g(t)\cdot h(x) = x(t)(1-x(t))
\end{align} g: X_{1} \mapsto \mathbb{R} h: X_{2}\mapsto \mathbb{R} g(t) := 1 h(x) := x(1-x), X_{1} := \mathbb{R} X_{2} := \mathbb{R} x : I \mapsto \mathbb{R} I \subset \mathbb{R} 0 = t_{0} \in I h(x) = 0 \begin{align}
h(x_{0}) = 0 \implies x_{0} = 0 \text{ or } x_{0} = 1.
\end{align} \begin{align}
\text{for } x_{0} = 0 \implies x(t) = 0; \\
\text{for } x_{0} = 1 \implies x(t) = 1.
\end{align} h(\xi) \neq 0 \implies x_{0} \neq 0 x_{0} \neq 1 
\int_{x_{0}}^{x}\frac{1}{h(\xi)} d\xi = \int_{t_{0}}^{t} g(\eta) d\eta,
 \begin{align*}
\int_{x_{0}}^{x}\frac{1}{\xi(1-\xi)} d\xi = \int_{0}^{t}  d\eta
&\implies (\ln\lvert\xi\rvert - \ln\lvert 1-\xi\rvert)\big\rvert_{x_{0}}^{x} = t \\
&\implies \ln\Big\lvert\frac{\xi}{1-\xi}\Big\rvert\Bigg\rvert_{x_{0}}^{x} = t \\
&\implies \ln\Big\lvert\frac{x}{1-x}\Big\rvert - \ln\Big\lvert\frac{x_{0}}{1-x_{0}}\Big\rvert = t \\
&\implies \ln\Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = t  \\ &\implies \Big\lvert\frac{x(1-x_{0})}{x_{0}(1-x)}\Big\rvert = e^{t}.
\end{align*} h(x_{0}) > 0 \implies h(x) > 0 \begin{align*}
&\frac{x(1-x_{0})}{x_{0}(1-x)} = e^{t} 
\implies &x(t) = \frac{e^{t}x_{0}}{1-x_{0}(1-e^{t})}.
\end{align*} h(x_{0}) > 0 \begin{align*}
x_{0}(1-x_{0})>0 \implies
\end{align*} \begin{cases}
x_{0}>0 \\
1-x_{0}>0
\end{cases}  \textbf{or}  \begin{cases}
x_{0}<0 \\
1-x_{0}<0
\end{cases}  0 < x_{0} < 1 . x_{0} \neq \frac{1}{1-e^{t}}. x(t) t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\}  1-x_{0}(1-e^{t}) \neq 0 \implies t \neq \ln\Big(1- \frac{1}{x_{0}}\Big)  x_{0} \ln(1- \frac{1}{x_{0}}) h(x_{0})<0 x(t) x_{0}  x_{0} < 0 \cup x_{0} > 1  t \in \mathbb{R} \setminus \{\ln(1- \frac{1}{x_{0}})\} x(t)","['calculus', 'integration']"
79,Why is $x(t)$ is $(k+1)$ continuously differentiable?,Why is  is  continuously differentiable?,x(t) (k+1),"Hello in my ODE lecture I have the following statement but I don't see why exactly it's true: Let the following first-order differential equation : $$\dot x = f(t,x(t)) \tag{1}$$ Defined by the function $f(t,x)\colon \mathbb R \times \mathbb R ^n \rightarrow \mathbb R^n  $ , s.t $f(t,x)$ is $k$ continuously differentiable for all the variables $(t,x_1,...,x_n)$ If we suppose that $x = x(t) \colon I \rightarrow \mathbb R^n  $ is a continuously differentiable solution of $(1)$ . Hence $x(t)$ is $(k+1)$ continuously differentiable. If anyone can explain why this statement is necessarily true it would be a lot appreciated. Thanks in advance","Hello in my ODE lecture I have the following statement but I don't see why exactly it's true: Let the following first-order differential equation : Defined by the function , s.t is continuously differentiable for all the variables If we suppose that is a continuously differentiable solution of . Hence is continuously differentiable. If anyone can explain why this statement is necessarily true it would be a lot appreciated. Thanks in advance","\dot x = f(t,x(t)) \tag{1} f(t,x)\colon \mathbb R \times \mathbb R ^n \rightarrow \mathbb R^n   f(t,x) k (t,x_1,...,x_n) x = x(t) \colon I \rightarrow \mathbb R^n   (1) x(t) (k+1)",['ordinary-differential-equations']
80,Solve $\frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}.$,Solve,\frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}.,"Solve $\frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}.$ My attempt $\frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}.$ $\implies \left(x^2+y^2\right)^{\frac{5}{2}}dy-y^2dx =0$ Given equation is not homogeneous, so, we can't solve using homogeneous method. Given equation is not total derivative form. So, that method won't be applicable. Comparing with $Mdx+Ndy=0$ I checked whether the equation is exact or not, I found $\frac{\partial M}{\partial y} \neq \frac{\partial M}{\partial y}. $ Can you please help me? Thank you.","Solve My attempt Given equation is not homogeneous, so, we can't solve using homogeneous method. Given equation is not total derivative form. So, that method won't be applicable. Comparing with I checked whether the equation is exact or not, I found Can you please help me? Thank you.",\frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}. \frac{dy}{dx}=\frac{y^2}{\left(x^2+y^2\right)^{\frac{5}{2}}}. \implies \left(x^2+y^2\right)^{\frac{5}{2}}dy-y^2dx =0 Mdx+Ndy=0 \frac{\partial M}{\partial y} \neq \frac{\partial M}{\partial y}. ,['ordinary-differential-equations']
81,$f(0)=0$ and $f'(x)=f(x)^2$ [duplicate],and  [duplicate],f(0)=0 f'(x)=f(x)^2,"This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Closed 3 years ago . At first sight, this exercise seems to be already seen many times in this website but I could not find a Analysis-1 level proof of the following : Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $\forall x \in \mathbb{R}$ , we have $f'(x)=[f(x)]^2$ . Show that $f(x)=0, \forall x \in \mathbb{R}$ . My attempt is : Since $f'(x) \geq 0$ $\forall x$ , we have that $\forall x \in \mathbb R^+ f(x) \ge 0$ . By contradiction, suppose that $\exists a > 0$ such that $f(a)>0$ . Since $f'=f^2\geq0$ , we get that $f(x)>0$ for all $x \in (a, +\infty)$ . Let $g: (a,+\infty) \to \mathbb{R}$ be given by $g(x)=\frac{1}{f(x)}$ . As $g'(x)=-1$ we have $g(x)=-x+c$ , $c \in \mathbb{R}$ . So, $f(x)=\frac1{-x+c}, \forall x \in (a,+\infty)$ . From $f(a)>0$ we get that $c>a$ . But then $f$ isn't continuous at $x=c$ , a contradiction. I think I showed that the function is constantly equal to $0$ on $(0,+\infty)$ , but how do I show it on $(-\infty,0)$ ? Should I just do the same proof with the g function on $(-\infty,a)$ with $a < 0$ ?","This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Closed 3 years ago . At first sight, this exercise seems to be already seen many times in this website but I could not find a Analysis-1 level proof of the following : Let be a differentiable function such that and , we have . Show that . My attempt is : Since , we have that . By contradiction, suppose that such that . Since , we get that for all . Let be given by . As we have , . So, . From we get that . But then isn't continuous at , a contradiction. I think I showed that the function is constantly equal to on , but how do I show it on ? Should I just do the same proof with the g function on with ?","f: \mathbb{R} \to \mathbb{R} f(0)=0 \forall x \in \mathbb{R} f'(x)=[f(x)]^2 f(x)=0, \forall x \in \mathbb{R} f'(x) \geq 0 \forall x \forall x \in \mathbb R^+ f(x) \ge 0 \exists a > 0 f(a)>0 f'=f^2\geq0 f(x)>0 x \in (a, +\infty) g: (a,+\infty) \to \mathbb{R} g(x)=\frac{1}{f(x)} g'(x)=-1 g(x)=-x+c c \in \mathbb{R} f(x)=\frac1{-x+c}, \forall x \in (a,+\infty) f(a)>0 c>a f x=c 0 (0,+\infty) (-\infty,0) (-\infty,a) a < 0","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'continuity']"
82,"References required for ""minimal switches to control a switched linear system""","References required for ""minimal switches to control a switched linear system""",,"A switched linear system of ODEs is defined as a system of ODEs of the form $\dot{x}(t) = A_i(t)x(t)$ , with $A_i$ for $i=1,2,...,N$ being a fixed number of matrices. More precisely, given a $\sigma : [0,t] \to \{1,2,...,N\}$ and an initial point $\mathbf{x} \in \Bbb R^n$ ,we could talk about (assuming existence and uniqueness of solution) the solution to $\dot{x}(t) = A_{\sigma(t)}x(t)$ with $x(0) = \mathbf x$ , and look at its trajectory. In the research of switched linear systems, often problems of controllability(See here ) are considered. Other topics of research include minimizing , among all $\sigma$ suitable (e.g. steering to a certain point/region), a quadratic functional of $\sigma$ . However, I am not aware of any research looking at the minimum number of switches to reach a point/region. More precisely, consider a fixed $\mathbf x \in \mathbb R^n$ , a destination region $S \subset \mathbb R^n$ with $\mathbf x \notin S$ , a fixed $T>0$ (think of $T$ as being very large) and let $$\Sigma = \{\sigma : [0,T] \to \{1,2,...,N\}, \text{ The solution  of $\dot{x}(t) = A_{\sigma(t)}x(t)$ reaches $S$ before time $T$}\}$$ Assume that $\Sigma$ is non-empty. Now, for each $\sigma \in \Sigma$ , the number of times it switches is its number of discontinuities, so define $D(\sigma) = n$ if $\sigma$ has $n$ discontinuities, and $D(\sigma) = \infty$ if $\sigma$ has infinitely many discontinuities. Is there literature available on studying $\min_{\sigma \in \Sigma}D(\sigma)$ ? In words : Assuming there is a control steering $\mathbf x$ to $S$ , there is a control with at most $\_\_\_$ switches also steering $\mathbf x$ to $S$ . I would like to know if there are papers/textbooks which discuss this exact question, in the context of linear switched systems. I am aware of Pontryagin's maximum principle, but do not know how it would provide structural results for this question, so if anybody could suggest an approach via PMP I would be happy as well.","A switched linear system of ODEs is defined as a system of ODEs of the form , with for being a fixed number of matrices. More precisely, given a and an initial point ,we could talk about (assuming existence and uniqueness of solution) the solution to with , and look at its trajectory. In the research of switched linear systems, often problems of controllability(See here ) are considered. Other topics of research include minimizing , among all suitable (e.g. steering to a certain point/region), a quadratic functional of . However, I am not aware of any research looking at the minimum number of switches to reach a point/region. More precisely, consider a fixed , a destination region with , a fixed (think of as being very large) and let Assume that is non-empty. Now, for each , the number of times it switches is its number of discontinuities, so define if has discontinuities, and if has infinitely many discontinuities. Is there literature available on studying ? In words : Assuming there is a control steering to , there is a control with at most switches also steering to . I would like to know if there are papers/textbooks which discuss this exact question, in the context of linear switched systems. I am aware of Pontryagin's maximum principle, but do not know how it would provide structural results for this question, so if anybody could suggest an approach via PMP I would be happy as well.","\dot{x}(t) = A_i(t)x(t) A_i i=1,2,...,N \sigma : [0,t] \to \{1,2,...,N\} \mathbf{x} \in \Bbb R^n \dot{x}(t) = A_{\sigma(t)}x(t) x(0) = \mathbf x \sigma \sigma \mathbf x \in \mathbb R^n S \subset \mathbb R^n \mathbf x \notin S T>0 T \Sigma = \{\sigma : [0,T] \to \{1,2,...,N\}, \text{ The solution  of \dot{x}(t) = A_{\sigma(t)}x(t) reaches S before time T}\} \Sigma \sigma \in \Sigma D(\sigma) = n \sigma n D(\sigma) = \infty \sigma \min_{\sigma \in \Sigma}D(\sigma) \mathbf x S \_\_\_ \mathbf x S","['ordinary-differential-equations', 'optimization', 'optimal-control']"
83,A second order ODE involving Jacobi elliptic functions,A second order ODE involving Jacobi elliptic functions,,"I have recently got stuck on this ODE in my studies of rogue waves on elliptic backgrounds: $$ y'' + q(x) y' + w(x) y = 0 $$ where $$ q(x) = m \frac{sn(x|m)\,cn(x|m)}{dn(x|m)}, \\ w(x) = \left(\lambda^2 + dn(x|m)^2 - i m \lambda \frac{sn(x|m)\,cn(x|m)}{dn(x|m)} \right) $$ The parameter $\lambda \in \mathbb{C}$ and $m \in [0,1]$ is the elliptic parameter of the Jacobi elliptic functions $\text{dn}(.|.), \, \text{cn}(.|.)$ and $\text{sn}(.|.)$ . I am only interested in $x \in \mathbb{R}$ . Note that (according to Mathematica), the period of $q(x)$ is $4K(m)$ where $K$ is the complete elliptic integral of the first kind. The real and imaginary parts of $w(x)$ have the same period as $q(x)$ . The initial value problem: $$ y(0) = 2i\sin(\alpha),\\ y'(0) = 2(\lambda i \sin(\alpha) + i \cos(\beta)) $$ where $\alpha, \beta \in \mathbb{C}$ are some parameters that depend on $\lambda$ in some complicated way. Are there any specific tricks to help me find an analytical solution for this ODE? Does anyone have any pointers? I have checked a few handbooks on Jacobi elliptic functions but haven't had much luck, I have no formal training in elliptic functions and not too much in ODEs. I have solved it numerically but I'm wondering if it's possible to solve it analytically for completeness. EDIT: I have taken a look at this paper but it's a little too complicated for me. However, based on the transformation they present in equation 15: $$ y(x) = \exp\left(-\frac{1}{2}\int q(x)  dx\right) $$ we can reduce the first equation to: $$ u''(x) - r(x) u(x) = 0 $$ where $$ r(x) = \frac{1}{2} q'(x) + \frac{1}{4} q^2(x) - w(x) $$ According to Mathematica: $$ r(x) = \frac{3 (m-1)+\text{dn}(x|m) \left(-\text{dn}(x|m) \left(3 \text{dn}(x|m)^2+4 \lambda ^2+m-2\right)+4 i m \text{cn}(x|m) \text{sn}(x|m)\right)}{4 \text{dn}(x|m)^2} $$ I have expanded this in a Laurent series as suggested in the comments, but I am not sure how to properly manipulate the product of the series of $r(x) u(x)$ on the RHS of $u'' = r(x) u$ .","I have recently got stuck on this ODE in my studies of rogue waves on elliptic backgrounds: where The parameter and is the elliptic parameter of the Jacobi elliptic functions and . I am only interested in . Note that (according to Mathematica), the period of is where is the complete elliptic integral of the first kind. The real and imaginary parts of have the same period as . The initial value problem: where are some parameters that depend on in some complicated way. Are there any specific tricks to help me find an analytical solution for this ODE? Does anyone have any pointers? I have checked a few handbooks on Jacobi elliptic functions but haven't had much luck, I have no formal training in elliptic functions and not too much in ODEs. I have solved it numerically but I'm wondering if it's possible to solve it analytically for completeness. EDIT: I have taken a look at this paper but it's a little too complicated for me. However, based on the transformation they present in equation 15: we can reduce the first equation to: where According to Mathematica: I have expanded this in a Laurent series as suggested in the comments, but I am not sure how to properly manipulate the product of the series of on the RHS of .","
y'' + q(x) y' + w(x) y = 0
 
q(x) = m \frac{sn(x|m)\,cn(x|m)}{dn(x|m)}, \\
w(x) = \left(\lambda^2 + dn(x|m)^2 - i m \lambda \frac{sn(x|m)\,cn(x|m)}{dn(x|m)} \right)
 \lambda \in \mathbb{C} m \in [0,1] \text{dn}(.|.), \, \text{cn}(.|.) \text{sn}(.|.) x \in \mathbb{R} q(x) 4K(m) K w(x) q(x) 
y(0) = 2i\sin(\alpha),\\
y'(0) = 2(\lambda i \sin(\alpha) + i \cos(\beta))
 \alpha, \beta \in \mathbb{C} \lambda 
y(x) = \exp\left(-\frac{1}{2}\int q(x)  dx\right)
 
u''(x) - r(x) u(x) = 0
 
r(x) = \frac{1}{2} q'(x) + \frac{1}{4} q^2(x) - w(x)
 
r(x) = \frac{3 (m-1)+\text{dn}(x|m) \left(-\text{dn}(x|m) \left(3 \text{dn}(x|m)^2+4 \lambda ^2+m-2\right)+4 i m \text{cn}(x|m) \text{sn}(x|m)\right)}{4 \text{dn}(x|m)^2}
 r(x) u(x) u'' = r(x) u","['ordinary-differential-equations', 'elliptic-functions']"
84,Solve equation including $f(x+c) + f(x)$ for $f(x)$.,Solve equation including  for .,f(x+c) + f(x) f(x),"How to solve the following equation for $F(x)$ ? $$ (1-\alpha)\beta \cdot F(x+c) - \alpha \cdot F(x) = \alpha + (1-\alpha)\beta - {\alpha \cdot \gamma \over x + \phi}$$ where $\alpha, \beta, \gamma, \phi$ are all constant and $\in (0,1)$ ; $F(x)$ needs to be a $cdf$ . Or more general, is there a formal way to simplify and solve an equation for $f(x)$ of the following form: $$ f(x+c) + f(x) = g(x)$$ where $g(x)$ is known? Thank you in advance!","How to solve the following equation for ? where are all constant and ; needs to be a . Or more general, is there a formal way to simplify and solve an equation for of the following form: where is known? Thank you in advance!","F(x)  (1-\alpha)\beta \cdot F(x+c) - \alpha \cdot F(x) = \alpha + (1-\alpha)\beta - {\alpha \cdot \gamma \over x + \phi} \alpha, \beta, \gamma, \phi \in (0,1) F(x) cdf f(x)  f(x+c) + f(x) = g(x) g(x)","['ordinary-differential-equations', 'functional-equations', 'delay-differential-equations']"
85,Prove the limit of an IVP is zero.,Prove the limit of an IVP is zero.,,"Let $U\in\mathbb{R}^d$ be open and let $F\in C^1(U, \mathbb{R}^d)$ . Fix $p\in U$ . Consider the IVP $$ \dot{x}=F(x),\ x(0)=p $$ Let $(T_-, T_+)$ be the maximal interval be the maximal interval of existence. Assume $$ \lim_{t\uparrow T_+}x(t)=\zeta\in U $$ Prove that $F(\zeta)=0$ . I just learned the extensibility of solution and have difficulty in working out the answer. However, by the extensibility theory, it seems $T_+$ has to be $+\infty$ . Do I mistaken something? Please give a concrete explaination. Any hint or answer of the original question could be very helpful!","Let be open and let . Fix . Consider the IVP Let be the maximal interval be the maximal interval of existence. Assume Prove that . I just learned the extensibility of solution and have difficulty in working out the answer. However, by the extensibility theory, it seems has to be . Do I mistaken something? Please give a concrete explaination. Any hint or answer of the original question could be very helpful!","U\in\mathbb{R}^d F\in C^1(U, \mathbb{R}^d) p\in U 
\dot{x}=F(x),\ x(0)=p
 (T_-, T_+) 
\lim_{t\uparrow T_+}x(t)=\zeta\in U
 F(\zeta)=0 T_+ +\infty","['ordinary-differential-equations', 'initial-value-problems']"
86,"""There are only 5 non-linear differential equations with fixed singularities.""","""There are only 5 non-linear differential equations with fixed singularities.""",,"In the essay the pernicious influence of mathematics on philosophy Gian Carlo Rota makes an offhand remark: it is a fact that there are only five non-linear differential equations with fixed singularities What does this remark mean? In particular what does it mean for a differential equation to have ""fixed singularities"" and what theorem states that there only 5 such differential equations? (Is the context of this statement ordinary or partial differential equations?)","In the essay the pernicious influence of mathematics on philosophy Gian Carlo Rota makes an offhand remark: it is a fact that there are only five non-linear differential equations with fixed singularities What does this remark mean? In particular what does it mean for a differential equation to have ""fixed singularities"" and what theorem states that there only 5 such differential equations? (Is the context of this statement ordinary or partial differential equations?)",,"['ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'soft-question']"
87,Derive an expression for Airy Bi function?,Derive an expression for Airy Bi function?,,"I am looking to find an explicit integral formula for $\operatorname{Bi}(x)$ via solving Airy's equation: $${\mathrm d^2y\over\mathrm dx^2}-xy=0\tag1$$ Currently I am able to solve for $\operatorname{Ai}(x)$ using the means of Fourier transform: Let $F(\omega)$ denote the Fourier transform of $y(x)$ . If we take Fourier transform on both side of (1), then $$ -\omega^2F(\omega)-iF'(\omega)=0\tag2 $$ Solving (2) yields $$ F(\omega)=F(0)e^{i\omega^3/3}\tag3 $$ Take inverse Fourier transform on (3) gives $$ y={F(0)\over2\pi}\int_{-\infty}^\infty e^{i(\omega x+\omega^3/3)}\mathrm d\omega $$ Due to the fact that $\int_{-\infty}^\infty f(x)\mathrm dx=\int_0^\infty[f(x)+f(-x)]\mathrm dx$ , we are able to get rid of the complex exponential: $$ y={F(0)\over\pi}\int_0^\infty\cos\left(\omega x+{\omega^3\over3}\right)\mathrm d\omega\tag4 $$ and (4) is identical to $y=F(0)\operatorname{Ai}(x)$ . Fourier transform is only valid for square-integrable funcntion, so the particular solution $\operatorname{Ai}(x)$ is only the square-integrable branch of the more general solution: $$ y=C_1\operatorname{Ai}(x)+C_2\operatorname{Bi}(x) $$ However, I would like to derive an explicit integral formula for $\operatorname{Bi}(x)$ , so I plug $y=f(x)\operatorname{Ai}(x)$ back into (1) and obtain $$ f''(x)\operatorname{Ai}(x)+2f'(x)\operatorname{Ai}'(x)+f(x)\operatorname{Ai}''(x)-xf(x)\operatorname{Ai}(x)=0 \\ f(x)[\operatorname{Ai}''(x)-x\operatorname{Ai}(x)]+f''(x)\operatorname{Ai}(x)+f'(x)\operatorname{Ai}'(x)=0 $$ $$ f''(x)\operatorname{Ai}(x)+2f'(x)\operatorname{Ai}'(x)=0\tag5 $$ Via some basic algebraic operation, I arrived at $$ {f''(x)\over f'(x)}=-2{\operatorname{Ai}'(x)\over\operatorname{Ai}(x)}\tag6 $$ Integrate and exponentiate on both side of (6) gives $$ f'(x)={C\over\operatorname{Ai}^2(x)} $$ and eventually I got stuck on $$ f(x)=C\int{\mathrm dx\over\operatorname{Ai}^2(x)} $$ Although this integral became a simple business if we made use of the Wronskian of $\operatorname{Ai}(x)$ and $\operatorname{Bi}(x)$ , but I wonder if it is possible for me to obtain an explicit integral formula $\operatorname{Bi}(x)$ via this integral. Alternatively, could somebody provide some other ways for me to find this integral formula","I am looking to find an explicit integral formula for via solving Airy's equation: Currently I am able to solve for using the means of Fourier transform: Let denote the Fourier transform of . If we take Fourier transform on both side of (1), then Solving (2) yields Take inverse Fourier transform on (3) gives Due to the fact that , we are able to get rid of the complex exponential: and (4) is identical to . Fourier transform is only valid for square-integrable funcntion, so the particular solution is only the square-integrable branch of the more general solution: However, I would like to derive an explicit integral formula for , so I plug back into (1) and obtain Via some basic algebraic operation, I arrived at Integrate and exponentiate on both side of (6) gives and eventually I got stuck on Although this integral became a simple business if we made use of the Wronskian of and , but I wonder if it is possible for me to obtain an explicit integral formula via this integral. Alternatively, could somebody provide some other ways for me to find this integral formula","\operatorname{Bi}(x) {\mathrm d^2y\over\mathrm dx^2}-xy=0\tag1 \operatorname{Ai}(x) F(\omega) y(x) 
-\omega^2F(\omega)-iF'(\omega)=0\tag2
 
F(\omega)=F(0)e^{i\omega^3/3}\tag3
 
y={F(0)\over2\pi}\int_{-\infty}^\infty e^{i(\omega x+\omega^3/3)}\mathrm d\omega
 \int_{-\infty}^\infty f(x)\mathrm dx=\int_0^\infty[f(x)+f(-x)]\mathrm dx 
y={F(0)\over\pi}\int_0^\infty\cos\left(\omega x+{\omega^3\over3}\right)\mathrm d\omega\tag4
 y=F(0)\operatorname{Ai}(x) \operatorname{Ai}(x) 
y=C_1\operatorname{Ai}(x)+C_2\operatorname{Bi}(x)
 \operatorname{Bi}(x) y=f(x)\operatorname{Ai}(x) 
f''(x)\operatorname{Ai}(x)+2f'(x)\operatorname{Ai}'(x)+f(x)\operatorname{Ai}''(x)-xf(x)\operatorname{Ai}(x)=0 \\
f(x)[\operatorname{Ai}''(x)-x\operatorname{Ai}(x)]+f''(x)\operatorname{Ai}(x)+f'(x)\operatorname{Ai}'(x)=0
 
f''(x)\operatorname{Ai}(x)+2f'(x)\operatorname{Ai}'(x)=0\tag5
 
{f''(x)\over f'(x)}=-2{\operatorname{Ai}'(x)\over\operatorname{Ai}(x)}\tag6
 
f'(x)={C\over\operatorname{Ai}^2(x)}
 
f(x)=C\int{\mathrm dx\over\operatorname{Ai}^2(x)}
 \operatorname{Ai}(x) \operatorname{Bi}(x) \operatorname{Bi}(x)","['real-analysis', 'ordinary-differential-equations', 'special-functions', 'airy-functions']"
88,"Show that $\frac{dy}{dx} = 5y +28 \cos(y), y(0) = 54$ has a unique solution on $\mathbb{R}$",Show that  has a unique solution on,"\frac{dy}{dx} = 5y +28 \cos(y), y(0) = 54 \mathbb{R}","Show that $\frac{dy}{dx} = 5y +28 \cos(y), y(0) = 54$ has a unique solution on $\mathbb{R}$ . This is a spin off of one of the problems in Berkeley Problems in Mathematics. My solution (attempt) is quite alot shorter than the one presented by the authours ( they show that a unique solution exists on some neighbourhood of $(0,54)$ using a local version of Picard's theorem and then use IFT to find an explicit solution on this neighbourhood and prove that this solution is valid on $\mathbb{R}$ ) so I wanted to check that I hadn't missed something. Here is my solution: Let $f(x,y)= 5y +28\cos(y)$ . Fix $h >0$ . By basic properties of continuous functions $f$ is continuous on $[-h,h] \times \mathbb{R}$ and moreover Lipschitz in $y$ on this strip. This follows from, $|f_y (x,y)|=|5-28\sin(y)| \leq 5+28|\sin(y)| \leq 5+28 = 33$ and the MVT. Picard's theorem applies and we see that the IVP has a unique solution on $[-h,h]$ . But $h$ was arbitrary so the IVP has a solution on all of $\mathbb{R}$ . $\blacksquare$ Is this correct? In general I am bit unsure about how to prove the uniqueness/existence of global solutions... analytic continuation or global Picard?! Note the version of Picard's theorem I am using is The IVP $y'(x) = f(x,y), y(a)=b$ , has a unique solution on $\mathbb{R}$ provided, $\forall h:$ $f$ is continuous on $[a-h, a+h] \times \mathbb{R}$ $f$ is Lipschitz in y on $[a-h, a+h] \times \mathbb{R}$ .","Show that has a unique solution on . This is a spin off of one of the problems in Berkeley Problems in Mathematics. My solution (attempt) is quite alot shorter than the one presented by the authours ( they show that a unique solution exists on some neighbourhood of using a local version of Picard's theorem and then use IFT to find an explicit solution on this neighbourhood and prove that this solution is valid on ) so I wanted to check that I hadn't missed something. Here is my solution: Let . Fix . By basic properties of continuous functions is continuous on and moreover Lipschitz in on this strip. This follows from, and the MVT. Picard's theorem applies and we see that the IVP has a unique solution on . But was arbitrary so the IVP has a solution on all of . Is this correct? In general I am bit unsure about how to prove the uniqueness/existence of global solutions... analytic continuation or global Picard?! Note the version of Picard's theorem I am using is The IVP , has a unique solution on provided, is continuous on is Lipschitz in y on .","\frac{dy}{dx} = 5y +28 \cos(y), y(0) = 54 \mathbb{R} (0,54) \mathbb{R} f(x,y)= 5y +28\cos(y) h >0 f [-h,h] \times \mathbb{R} y |f_y (x,y)|=|5-28\sin(y)| \leq 5+28|\sin(y)| \leq 5+28 = 33 [-h,h] h \mathbb{R} \blacksquare y'(x) = f(x,y), y(a)=b \mathbb{R} \forall h: f [a-h, a+h] \times \mathbb{R} f [a-h, a+h] \times \mathbb{R}","['real-analysis', 'calculus']"
89,"Prove that if A has imaginary eigenvalues and the flows $e^{tA}$ and $e^{tB}$ are topologically conjugate, then B has the same imaginary eigenvalues.","Prove that if A has imaginary eigenvalues and the flows  and  are topologically conjugate, then B has the same imaginary eigenvalues.",e^{tA} e^{tB},"Show that if A has eigenvalues of the form $\pm i \beta$ , with $\beta \neq 0$ and the flows $e^{tA}$ and $e^{tB}$ are topologically conjugate, then $\pm i \beta$ also are eigenvalues of $B$ . Definitions: $(i)$ Let $(f^t)_t$ and $(g^t)_t$ be flows of the following autonomous differential equations $x' = F(x)$ , $F: \mathcal{U} \longrightarrow \mathbb{R}^d$ and $y' = G(y), G: \mathcal{V} \longrightarrow \mathbb{R}^d$ , with $F$ and $G$ continuous funtions defined over open sets. The flows $(f^t)_t$ and $(g^t)_t$ are said to be topologically conjugate if there exists a homeomorphism $h: \mathcal{U} \longrightarrow \mathcal{V}$ such that $f^t(x)$ is defined if and only if $g^t(h(x))$ is defined. In such case, $h(f^t(x)) = g^t(h(x))$ . $(ii)$ Two flows $(f^t)_t$ and $(g^t)_t$ in $\mathbb{R}^d$ are said to be linearly conjugate if there exists a linear isomorphism $h: \mathbb{R}^d \longrightarrow \mathbb{R}^d$ such that $h \circ f^t = g^t \circ h,$ for every $t \in \mathbb{R}$ . Attempt: My initial idea was to exhibit a linear isomorphism that conjugates $e^{tA}$ and $e^{tB}$ , because we know that such flows are linearly conjugate if and only if $A$ and $B$ are similar, therefore have the same eigenvalues. I also tried to prove that such flows are $C^1$ -conjugate, but did not succeed. Any help would be appreciated!","Show that if A has eigenvalues of the form , with and the flows and are topologically conjugate, then also are eigenvalues of . Definitions: Let and be flows of the following autonomous differential equations , and , with and continuous funtions defined over open sets. The flows and are said to be topologically conjugate if there exists a homeomorphism such that is defined if and only if is defined. In such case, . Two flows and in are said to be linearly conjugate if there exists a linear isomorphism such that for every . Attempt: My initial idea was to exhibit a linear isomorphism that conjugates and , because we know that such flows are linearly conjugate if and only if and are similar, therefore have the same eigenvalues. I also tried to prove that such flows are -conjugate, but did not succeed. Any help would be appreciated!","\pm i \beta \beta \neq 0 e^{tA} e^{tB} \pm i \beta B (i) (f^t)_t (g^t)_t x' = F(x) F: \mathcal{U} \longrightarrow \mathbb{R}^d y' = G(y), G: \mathcal{V} \longrightarrow \mathbb{R}^d F G (f^t)_t (g^t)_t h: \mathcal{U} \longrightarrow \mathcal{V} f^t(x) g^t(h(x)) h(f^t(x)) = g^t(h(x)) (ii) (f^t)_t (g^t)_t \mathbb{R}^d h: \mathbb{R}^d \longrightarrow \mathbb{R}^d h \circ f^t = g^t \circ h, t \in \mathbb{R} e^{tA} e^{tB} A B C^1","['ordinary-differential-equations', 'dynamical-systems']"
90,"Find the dimension of $V = \{f \in C^k [0, 1] : a_n f^{(n)}(t)+\cdots+a_0 f (t) = 0 \textrm{ for all } t \in [0, 1]\}$.",Find the dimension of .,"V = \{f \in C^k [0, 1] : a_n f^{(n)}(t)+\cdots+a_0 f (t) = 0 \textrm{ for all } t \in [0, 1]\}","Using the additive properties of differentiation I proved that $V$ is a subspace. The fact that $\overline{O} \in V$ is also trivial. However, I am stuck as to how to use the property of the continuity and differentiability of $f \in V$ to find out the dimension of $V$ . Any help would be appreciated. EDIT: The $a_i$ from $i = 0$ to $i = n $ are fixed and are $\in \Bbb{R}$ .","Using the additive properties of differentiation I proved that is a subspace. The fact that is also trivial. However, I am stuck as to how to use the property of the continuity and differentiability of to find out the dimension of . Any help would be appreciated. EDIT: The from to are fixed and are .",V \overline{O} \in V f \in V V a_i i = 0 i = n  \in \Bbb{R},"['real-analysis', 'linear-algebra', 'ordinary-differential-equations', 'vector-spaces']"
91,Example of a flow that does not preserve volume measure (autonomous ode),Example of a flow that does not preserve volume measure (autonomous ode),,"Consider the autonomous differential equation in $\mathcal{U} = \mathbb{R} \times (0, +\infty)$ given by $$x' = \dfrac{x^2}{1+x^2y^2}, y' = 0.$$ Justify that the respective flow is complete (i.é, defined for all $t \in \mathbb{R}$ ). Show that given $T>0$ , there exist limited open sets $X \subset \mathcal{U}$ such that $X_T = \{f^t(x,y); (x,y) \in X \hspace{0.1cm} \text{and} \hspace{0.1cm} t \in \left[0,T\right]\}$ has infinite volume measure. Attempt: Consider the initial condition of the equation as $\gamma (0) = (x_0,y_0)$ . Since $y' = 0$ , I know that $y = y_0$ , with $y_0 > 0$ . I was able to show that the flow of this autonomous equation is $$f^t(x_0,y_0) = \left( \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) \overline{+} \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}, y_0 \right),$$ which is defined for every $t \in \mathbb{R}$ . Hence, the respective flow is complete. Given $T >0$ , my initial idea was to consider $X = (a_1, a_2) \times (0,y_0) \subset \mathcal{U}$ , with $$a_1 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) - \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}$$ $$ \hspace{0.1cm} \text{and} \hspace{0.1cm}$$ $$ a_2 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) +\left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}, $$ which is an open and limited set. But I'm not sure if $X_T$ is a set with infinite volume measure. Can anyone help me conclude or even help me construct a different limited open set? Any help would be appreciated!","Consider the autonomous differential equation in given by Justify that the respective flow is complete (i.é, defined for all ). Show that given , there exist limited open sets such that has infinite volume measure. Attempt: Consider the initial condition of the equation as . Since , I know that , with . I was able to show that the flow of this autonomous equation is which is defined for every . Hence, the respective flow is complete. Given , my initial idea was to consider , with which is an open and limited set. But I'm not sure if is a set with infinite volume measure. Can anyone help me conclude or even help me construct a different limited open set? Any help would be appreciated!","\mathcal{U} = \mathbb{R} \times (0, +\infty) x' = \dfrac{x^2}{1+x^2y^2}, y' = 0. t \in \mathbb{R} T>0 X \subset \mathcal{U} X_T = \{f^t(x,y); (x,y) \in X \hspace{0.1cm} \text{and} \hspace{0.1cm} t \in \left[0,T\right]\} \gamma (0) = (x_0,y_0) y' = 0 y = y_0 y_0 > 0 f^t(x_0,y_0) = \left( \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) \overline{+} \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}, y_0 \right), t \in \mathbb{R} T >0 X = (a_1, a_2) \times (0,y_0) \subset \mathcal{U} a_1 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) - \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}  \hspace{0.1cm} \text{and} \hspace{0.1cm}  a_2 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) +\left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0},  X_T","['ordinary-differential-equations', 'measure-theory', 'dynamical-systems']"
92,An implementation of a general solution of Abel's equations,An implementation of a general solution of Abel's equations,,"Introduction An Abel equation of the second kind in its canonical form is writen as $$y(x)y'(x)-y(x)=f(x)\quad  (6)$$ for arbitrary $f(x)$ . This equation has a general solution derived by 1 2 (Check Wikipedia article for Abel equation of the first kind , which in some cases reduces to the above form) What I am trying to do is to solve the equation $$y(x)y'(x)-y(x)=Ax^2  $$ over the reals with $A$ constant. This equation does not appear in the tabulated solutions of Polyanin, Manzhirov , so I tried to implement the method from the paper 1 . However, I noticed that the solution does not agree with the numerics, so I tried to run my code on an equation that we know the solution to, which is: $$y(x)y'(x)-y(x)=x  $$ The result is summarised below using the language of Mathematica. As we can see, the analytical plot does not belong to the solution family seen in the second picture. Goal I am trying to write a code that would apply the method described in 1 to solve the general Abel equation. The solution can be applied in any programming language. Method Code (*Solving for y*) ζ[x_] = Log[Abs[x + 2 λ]] F[x_] = ζ[x] G[ζ_] =  1/16 ((ζ Sin[ζ] + Cos[ζ]) CosIntegral[ζ] +   Cos[ζ]^2)*(4 ζ CosIntegral[ζ] +    Cos[ζ] )/(ζ CosIntegral[ζ])^3 Exp[-ζ] -  2 F[ζ]  a = -4; b[ζ_] = 3 + 4 (G[ζ] + F[ζ]) Exp[-ζ] c[ζ_] = - 4 (G[ζ] + 2 F[ζ]) Exp[-ζ]  p[ζ_] = -a^2/3 + b[ζ] q[ζ_] = 2 (a/3)^3 - a b[ζ]/3 + c[ζ] NN = Solve[Z^3 + p Z + q == 0, Z] /. {p -> p[ζ],   q -> q[ζ]} /. ζ -> ζ[x]; y = Table[ Table[1/2 (x + 2 λ) ((Z /. NN[[ii]]) + 1/3), {ii, 1,   3}], {λ, 1, 2}];  (*Benchmarking*) DSolve[Y'[X] Y[X] - Y[X] == X, Y[X], X]  p1 = Plot[y, {x, 0, 10}] p2 = Table[ ContourPlot[ 1/10 ((5 + Sqrt[5]) Log[       1 + Sqrt[5] - (2 Y)/X] - (-5 + Sqrt[5]) Log[-1 + Sqrt[5] + (        2 Y)/X]) == II - Log[X], {X, 0, 10}, {Y, -10, 15}], {II, 0,  3, 0.2}]; Show[p2] Plots The plots look not alike References Dimitrios E. Panayotounakos, ""Exact analytic solutions of unsolvable classes of first and second"" order nonlinear ODEs (Part I: Abel’s equations) Panayotounakos, Dimitrios E.; Zarmpoutis, Theodoros I, ""Construction of Exact Parametric or Closed Form Solutions of Some Unsolvable Classes of Nonlinear ODEs (Abel's Nonlinear ODEs of the First Kind and Relative Degenerate Equations)"".","Introduction An Abel equation of the second kind in its canonical form is writen as for arbitrary . This equation has a general solution derived by 1 2 (Check Wikipedia article for Abel equation of the first kind , which in some cases reduces to the above form) What I am trying to do is to solve the equation over the reals with constant. This equation does not appear in the tabulated solutions of Polyanin, Manzhirov , so I tried to implement the method from the paper 1 . However, I noticed that the solution does not agree with the numerics, so I tried to run my code on an equation that we know the solution to, which is: The result is summarised below using the language of Mathematica. As we can see, the analytical plot does not belong to the solution family seen in the second picture. Goal I am trying to write a code that would apply the method described in 1 to solve the general Abel equation. The solution can be applied in any programming language. Method Code (*Solving for y*) ζ[x_] = Log[Abs[x + 2 λ]] F[x_] = ζ[x] G[ζ_] =  1/16 ((ζ Sin[ζ] + Cos[ζ]) CosIntegral[ζ] +   Cos[ζ]^2)*(4 ζ CosIntegral[ζ] +    Cos[ζ] )/(ζ CosIntegral[ζ])^3 Exp[-ζ] -  2 F[ζ]  a = -4; b[ζ_] = 3 + 4 (G[ζ] + F[ζ]) Exp[-ζ] c[ζ_] = - 4 (G[ζ] + 2 F[ζ]) Exp[-ζ]  p[ζ_] = -a^2/3 + b[ζ] q[ζ_] = 2 (a/3)^3 - a b[ζ]/3 + c[ζ] NN = Solve[Z^3 + p Z + q == 0, Z] /. {p -> p[ζ],   q -> q[ζ]} /. ζ -> ζ[x]; y = Table[ Table[1/2 (x + 2 λ) ((Z /. NN[[ii]]) + 1/3), {ii, 1,   3}], {λ, 1, 2}];  (*Benchmarking*) DSolve[Y'[X] Y[X] - Y[X] == X, Y[X], X]  p1 = Plot[y, {x, 0, 10}] p2 = Table[ ContourPlot[ 1/10 ((5 + Sqrt[5]) Log[       1 + Sqrt[5] - (2 Y)/X] - (-5 + Sqrt[5]) Log[-1 + Sqrt[5] + (        2 Y)/X]) == II - Log[X], {X, 0, 10}, {Y, -10, 15}], {II, 0,  3, 0.2}]; Show[p2] Plots The plots look not alike References Dimitrios E. Panayotounakos, ""Exact analytic solutions of unsolvable classes of first and second"" order nonlinear ODEs (Part I: Abel’s equations) Panayotounakos, Dimitrios E.; Zarmpoutis, Theodoros I, ""Construction of Exact Parametric or Closed Form Solutions of Some Unsolvable Classes of Nonlinear ODEs (Abel's Nonlinear ODEs of the First Kind and Relative Degenerate Equations)"".",y(x)y'(x)-y(x)=f(x)\quad  (6) f(x) y(x)y'(x)-y(x)=Ax^2   A y(x)y'(x)-y(x)=x  ,"['ordinary-differential-equations', 'nonlinear-analysis']"
93,How to solve Cauchy-Euler ODE $x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x$?,How to solve Cauchy-Euler ODE ?,x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x,"The equation is $x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x$ . Solving for the homogeneous solution is simple: the characteristic equation results in roots 1 and 2 so we have $y_c = Ax + Bx^2$ , which works. Since $y_p = u(x) y_1 + v(x) y_2$ by variation of parameters, the Wronskian is $x^2$ , and $u(x) = - \int \frac{x^2 (x \sin x)}{x^2}$ and $v(x) = \int \frac{x (x \sin x)}{x^2}$ we should get $y_p = -x \int x \sin x + x^2 \int \sin x$ . However, this solution doesn't work. Plugging the equation into Wolfram, the particular integral should be $y_p = -x \int \frac{\sin x}{x} + x^2 \int \frac{\cos x}{x} - x\sin x$ . I've tried to reverse engineer what the values of $u(x)$ and $v(x)$ but they don't make sense. What is missing here?","The equation is . Solving for the homogeneous solution is simple: the characteristic equation results in roots 1 and 2 so we have , which works. Since by variation of parameters, the Wronskian is , and and we should get . However, this solution doesn't work. Plugging the equation into Wolfram, the particular integral should be . I've tried to reverse engineer what the values of and but they don't make sense. What is missing here?",x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x y_c = Ax + Bx^2 y_p = u(x) y_1 + v(x) y_2 x^2 u(x) = - \int \frac{x^2 (x \sin x)}{x^2} v(x) = \int \frac{x (x \sin x)}{x^2} y_p = -x \int x \sin x + x^2 \int \sin x y_p = -x \int \frac{\sin x}{x} + x^2 \int \frac{\cos x}{x} - x\sin x u(x) v(x),['ordinary-differential-equations']
94,How can I apply 4th order Runge-Kutta to a Laplacian equation in spherical coordinates?,How can I apply 4th order Runge-Kutta to a Laplacian equation in spherical coordinates?,,"Can someone please help me solve the Poisson-Boltzmann equation in spherical coordinates over the domain $r\in(r_0, \infty)$ with Runge-Kutta: $$ \frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2\frac{\partial\psi}{\partial r}\right) =a \sinh \left(\frac{\psi}{\psi_0}\right) $$ here $r$ is the radial coordinate, $a$ and $\psi_0$ are constants. The boundary conditions are $\psi=\psi_0$ and $\psi'=b$ at $r=r_0$ . I made an attempt to put the equation into a form suitable for Runge-Kutta integration with a $u$ -substitution $u/r=\psi$ , yielding $$ \frac{\partial^2u}{\partial r^2}=a r\sinh\left(\frac{u}{r\psi_0}\right) $$ And, following from this the Runge-Kutta method can be applied $$ \frac{\partial}{\partial r}\begin{bmatrix}  u  \\ \partial u/\partial r \\ \end{bmatrix}=f\left(u, \frac{\partial u}{\partial r}, r\right)=\begin{bmatrix}  \partial u/\partial r \\ ar\sinh\left(\frac{u}{r\psi_0}\right) \end{bmatrix} $$ This seems to work, but my question is: Is there a more direct way of applying Runge-Kutta without such a substitution?","Can someone please help me solve the Poisson-Boltzmann equation in spherical coordinates over the domain with Runge-Kutta: here is the radial coordinate, and are constants. The boundary conditions are and at . I made an attempt to put the equation into a form suitable for Runge-Kutta integration with a -substitution , yielding And, following from this the Runge-Kutta method can be applied This seems to work, but my question is: Is there a more direct way of applying Runge-Kutta without such a substitution?","r\in(r_0, \infty) 
\frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2\frac{\partial\psi}{\partial r}\right) =a \sinh \left(\frac{\psi}{\psi_0}\right)
 r a \psi_0 \psi=\psi_0 \psi'=b r=r_0 u u/r=\psi 
\frac{\partial^2u}{\partial r^2}=a r\sinh\left(\frac{u}{r\psi_0}\right)
 
\frac{\partial}{\partial r}\begin{bmatrix} 
u  \\
\partial u/\partial r \\
\end{bmatrix}=f\left(u, \frac{\partial u}{\partial r}, r\right)=\begin{bmatrix} 
\partial u/\partial r \\
ar\sinh\left(\frac{u}{r\psi_0}\right)
\end{bmatrix}
","['ordinary-differential-equations', 'numerical-methods', 'spherical-coordinates', 'poissons-equation', 'runge-kutta-methods']"
95,Characteristics for second order PDE in higher than 2 dimensions,Characteristics for second order PDE in higher than 2 dimensions,,"I'm trying to understand characteristics of second order hyperbolic equations. When the number of dimensions is 2, say $$ au_{xx} + 2b u_{xy} + cu_{yy} = 0, \tag{1} $$ we have a change of variables $(x,y) \mapsto (\xi, \eta)$ that satisfies very particular conditions. In particular we reduce (1) to $$ w_{xy} = 0 \tag{2} $$ which can be solved. In doing so we learn a wealth of information about the characteristics of this equation, and the solution's behavior along these characteristics. My question is: is there a generalization of this technique to higher dimensions? For higher dimensions there are more than one mixed partial term, which is where I'm getting stuck. The motivation for this is understanding wave propagation in 3 spatial (and 1 time) dimensions. Edit: For an equation of the form $g^{ij}(x)\partial_{ij}u = 0$ , with $g_{ij}$ a Lorentzian metric, I think this is a bit easier (theoretically, at least). Since it's a Lorentzian metric, we can pick ""null coordinates"" $\xi$ so that, say, in $n$ -dimensions, $\xi^1$ and $\xi^2$ are null coordinates. Then along the null hypersurfaces $\{\xi^1 = a\}$ and $\{\xi^2 = b\}$ the solution's ""outgoing"" derivative (i.e. the outgoing derivative to $\{\xi^1 = a\}$ is $\partial_{\xi^1} u$ ) must satisfy a first order PDE. In a manner similar to the transport equation, I think this gives ""propagation equations"" for the outgoing derivative along the null (characteristic) hypersurfaces. See Rendall's paper (1990) on the characteristic initial value problem.","I'm trying to understand characteristics of second order hyperbolic equations. When the number of dimensions is 2, say we have a change of variables that satisfies very particular conditions. In particular we reduce (1) to which can be solved. In doing so we learn a wealth of information about the characteristics of this equation, and the solution's behavior along these characteristics. My question is: is there a generalization of this technique to higher dimensions? For higher dimensions there are more than one mixed partial term, which is where I'm getting stuck. The motivation for this is understanding wave propagation in 3 spatial (and 1 time) dimensions. Edit: For an equation of the form , with a Lorentzian metric, I think this is a bit easier (theoretically, at least). Since it's a Lorentzian metric, we can pick ""null coordinates"" so that, say, in -dimensions, and are null coordinates. Then along the null hypersurfaces and the solution's ""outgoing"" derivative (i.e. the outgoing derivative to is ) must satisfy a first order PDE. In a manner similar to the transport equation, I think this gives ""propagation equations"" for the outgoing derivative along the null (characteristic) hypersurfaces. See Rendall's paper (1990) on the characteristic initial value problem.","
au_{xx} + 2b u_{xy} + cu_{yy} = 0, \tag{1}
 (x,y) \mapsto (\xi, \eta) 
w_{xy} = 0 \tag{2}
 g^{ij}(x)\partial_{ij}u = 0 g_{ij} \xi n \xi^1 \xi^2 \{\xi^1 = a\} \{\xi^2 = b\} \{\xi^1 = a\} \partial_{\xi^1} u","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'wave-equation', 'hyperbolic-equations']"
96,Qualitative behavior of a seemingly simple ODE,Qualitative behavior of a seemingly simple ODE,,"In case you're curious about context---there isn't one.  I am thinking about this out-of-the-blue because it is mysterious and interesting. Let $ a, p, q, y_0 $ be positive constants.  Consider the ordinary differential equation in $ y = y(x) $ , $$ y' = y^p - a x^q, y(0) = y_0$$ Let us only consider the behavior of the solution for $ x \geq 0 $ and $ y(x) \geq 0 $ .  Depending on the constants, the solution will either go to infinity, or go to zero (we consider the solution undefined after $ y(x) < 0 $ ).  This is obvious on inspection, but in case it's not obvious to you, I recommend you plot some numerical solutions for $ p = 2, q = 3, a = 1 $ . $ p = 2, q = 3, a = 1 $ ""> I am investigating the conditions under which this solution escapes to infinity.  I am not aware of any closed-form solutions for this ODE.  I'd like to understand the boundary between solutions that are escaping to infinity and those that are going to zero (it's not hard to show that there's no other possibility). I don't know of many tools here.  From my undergrad ODEs, I am vaguely familiar with a Lyapunov method.  Viz., we might try to define a function $ F = F(x,y) $ satisfying some inequality like $$ (*) \frac{d}{dx} \left[ F(x, y(x)) \right] \geq 0 $$ Such functions, if chosen well, may allow us to put certain upper envelopes on solutions that go to zero, and put lower envelopes on solutions that go to infinity.  In the following discussion, always assume $ x \geq 0, y \geq 0 $ . For instance, if we try $ F(x,y) = y^{r_1}(y^{r_2} + b x^{r_3}) $ for some appropriate (positive) constants $ b, r_1, r_2, r_3 $ , we have $ F = 0 $ only if $ y = 0 $ .  If we can additionally show that $ (*) $ holds, then we produce a lower-envelope for solutions that escape to infinity.  There is some hope that this might work.  Indeed, we have $$ \frac{d}{dx}\left[ F(x, y(x)) \right] = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} y' = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} (y^p - a x^q) $$ It seems like some wizardry with the arithmetic-geometric inequality and good choice of constants can allow one to make this non-negative.  Can anyone help?","In case you're curious about context---there isn't one.  I am thinking about this out-of-the-blue because it is mysterious and interesting. Let be positive constants.  Consider the ordinary differential equation in , Let us only consider the behavior of the solution for and .  Depending on the constants, the solution will either go to infinity, or go to zero (we consider the solution undefined after ).  This is obvious on inspection, but in case it's not obvious to you, I recommend you plot some numerical solutions for . $ p = 2, q = 3, a = 1 $ ""> I am investigating the conditions under which this solution escapes to infinity.  I am not aware of any closed-form solutions for this ODE.  I'd like to understand the boundary between solutions that are escaping to infinity and those that are going to zero (it's not hard to show that there's no other possibility). I don't know of many tools here.  From my undergrad ODEs, I am vaguely familiar with a Lyapunov method.  Viz., we might try to define a function satisfying some inequality like Such functions, if chosen well, may allow us to put certain upper envelopes on solutions that go to zero, and put lower envelopes on solutions that go to infinity.  In the following discussion, always assume . For instance, if we try for some appropriate (positive) constants , we have only if .  If we can additionally show that holds, then we produce a lower-envelope for solutions that escape to infinity.  There is some hope that this might work.  Indeed, we have It seems like some wizardry with the arithmetic-geometric inequality and good choice of constants can allow one to make this non-negative.  Can anyone help?"," a, p, q, y_0   y = y(x)   y' = y^p - a x^q, y(0) = y_0  x \geq 0   y(x) \geq 0   y(x) < 0   p = 2, q = 3, a = 1   F = F(x,y)  
(*) \frac{d}{dx} \left[ F(x, y(x)) \right] \geq 0
  x \geq 0, y \geq 0   F(x,y) = y^{r_1}(y^{r_2} + b x^{r_3})   b, r_1, r_2, r_3   F = 0   y = 0   (*)  
\frac{d}{dx}\left[ F(x, y(x)) \right] = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} y' = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} (y^p - a x^q)
","['ordinary-differential-equations', 'inequality', 'lyapunov-functions']"
97,Convert second-order ODE to first order system,Convert second-order ODE to first order system,,I want to convert the equation $y'' + y' + \sin y = 0$ into a system of first order ODEs. I said: $$u = y\enspace v = y'$$ $$u' = y'\enspace v' = y'' = -y' - \sin y$$ Hence we have the system: $$u' = v$$ $$v' = -v -\sin u$$ Is this correct? Or should $u = \sin y$ at the beginning?,I want to convert the equation into a system of first order ODEs. I said: Hence we have the system: Is this correct? Or should at the beginning?,y'' + y' + \sin y = 0 u = y\enspace v = y' u' = y'\enspace v' = y'' = -y' - \sin y u' = v v' = -v -\sin u u = \sin y,"['ordinary-differential-equations', 'mathematical-modeling']"
98,Constant vector field on the torus $\mathbb{T}^{2n}$ is symplectic,Constant vector field on the torus  is symplectic,\mathbb{T}^{2n},"Let $\mathbb{T}^{2n}=\mathbb{R}^{2n}/\mathbb{Z^{2n}}$ be the $2n$ -torus, which we equip with the unique symplectic form $\omega$ that pulls back to the standard symplectic form on $\mathbb{R}^{2n}$ under the natural projection $\pi:\mathbb{R}^{2n}\to\mathbb{R}^{2n}/\mathbb{Z^{2n}}$ . We identify the tangent space $T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n}$ for all $x\in\mathbb{T}^{2n}$ . Fix some $v\in\mathbb{R}^{2n}$ and define the vector field $X\in\mathcal{X}(\mathbb{T}^{2n})$ by $X(x)=v$ . Then this is supposed to be an example of a vector field which is symplectic but not Hamiltonian for $v\neq 0$ . I know how to show that it is not Hamiltonian. To show that it is symplectic, we have to show that $d\iota_X\omega=d(\omega(X,\cdot))=0$ . By Cartan's magic formula and the closedness of $\omega$ , this is equivalent to showing that $$ \mathcal{L}_X\omega=\frac{d}{dt}\bigg|_{t=0}((\phi_X^t)^* \omega)=0 $$ Thus, we have to compute the flow $\phi_X^t$ . Note that $\frac{d}{dt}\phi_X^t(y)=X_{\phi_X^t(y)}=v$ for all $y$ . Thus, do we have that $\phi_X^t(y)=y+vt$ , where now we view $v\in\mathbb{T}^{2n}$ ? And do we have that $\mathcal{L}_X\omega=0$ ?","Let be the -torus, which we equip with the unique symplectic form that pulls back to the standard symplectic form on under the natural projection . We identify the tangent space for all . Fix some and define the vector field by . Then this is supposed to be an example of a vector field which is symplectic but not Hamiltonian for . I know how to show that it is not Hamiltonian. To show that it is symplectic, we have to show that . By Cartan's magic formula and the closedness of , this is equivalent to showing that Thus, we have to compute the flow . Note that for all . Thus, do we have that , where now we view ? And do we have that ?","\mathbb{T}^{2n}=\mathbb{R}^{2n}/\mathbb{Z^{2n}} 2n \omega \mathbb{R}^{2n} \pi:\mathbb{R}^{2n}\to\mathbb{R}^{2n}/\mathbb{Z^{2n}} T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n} x\in\mathbb{T}^{2n} v\in\mathbb{R}^{2n} X\in\mathcal{X}(\mathbb{T}^{2n}) X(x)=v v\neq 0 d\iota_X\omega=d(\omega(X,\cdot))=0 \omega 
\mathcal{L}_X\omega=\frac{d}{dt}\bigg|_{t=0}((\phi_X^t)^* \omega)=0
 \phi_X^t \frac{d}{dt}\phi_X^t(y)=X_{\phi_X^t(y)}=v y \phi_X^t(y)=y+vt v\in\mathbb{T}^{2n} \mathcal{L}_X\omega=0","['ordinary-differential-equations', 'differential-geometry']"
99,Integral Equation to Differential Equation,Integral Equation to Differential Equation,,"I am trying to return the following equation back to a differential equation form but I am rather stuck, and I am not sure how it can be done. I have read posts on here but to no avail. I want to turn $$ p(t) = \frac{\int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau}{ 1 + \int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau }, \quad \text{where} \quad \rho(t) = \int_t^\infty (\lambda(t') - \mu )dt'. $$ So far I've done the following steps: $$ \frac{p}{1 - p} = \int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau $$ $$ \frac{p}{1-p} = q(t) = - \int_{\infty}^{t}\mu e^{\rho(\tau)} d\tau $$ $$ \frac{dq}{dt} = - \mu e^{\rho(t)} $$ $$ \frac{dp}{dt} = -(1-p)^2\mu e^{\rho(t)} $$ However I am almost certain that the solution should have no exponetial term on the RHS and should be a quadratic with $\lambda(t) \text{ and } \mu$ as coefficients. I would appreciate any and all help, thanks!","I am trying to return the following equation back to a differential equation form but I am rather stuck, and I am not sure how it can be done. I have read posts on here but to no avail. I want to turn So far I've done the following steps: However I am almost certain that the solution should have no exponetial term on the RHS and should be a quadratic with as coefficients. I would appreciate any and all help, thanks!","
p(t) = \frac{\int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau}{ 1 + \int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau }, \quad \text{where} \quad \rho(t) = \int_t^\infty (\lambda(t') - \mu )dt'.
 
\frac{p}{1 - p} = \int_{t}^{\infty}\mu e^{\rho(\tau)} d\tau
 
\frac{p}{1-p} = q(t) = - \int_{\infty}^{t}\mu e^{\rho(\tau)} d\tau
 
\frac{dq}{dt} = - \mu e^{\rho(t)}
 
\frac{dp}{dt} = -(1-p)^2\mu e^{\rho(t)}
 \lambda(t) \text{ and } \mu","['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
