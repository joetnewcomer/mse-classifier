,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Convergence of Fourier sine and cosine series,Convergence of Fourier sine and cosine series,,"Discuss whether or not it is possible to have a Fourier series   $$a_0+\sum_{k=1}^\infty[a_k\cos(kx)+b_k\sin(kx)]$$ converge for all    $x$ without either  $$a_0+\sum_{k=1}^\infty a_k\cos(kx) \text{ or }  \sum_{k=1}^\infty b_k\sin(kx)$$ converging. This is a problem in Bressoud's analysis book and my solution is as follows: ""No, because if we let $f(x)=a_0+\sum_{k=1}^\infty[a_k\cos(kx)+b_k\sin(kx)]$, then the two other series are obtained by taking $\frac{f(x)\pm f(-x)}{2}$ and since $f(x)$ is convergent for all $x$ the sine and cosine series should also be convergent."" Here is the hint from the back of the book: If the Fourier series converges at $x=0$, then $\sum_{k=1}^\infty a_k$   converges, and therefore the partial sums of $\sum_{k=1}^\infty a_k$   are bounded. Although I think my solution is correct (please correct me if I'm wrong) I still would like to see other solutions and in particular understand the author's hint since I can't see how the boundedness of partial sums can help. Thanks!","Discuss whether or not it is possible to have a Fourier series   $$a_0+\sum_{k=1}^\infty[a_k\cos(kx)+b_k\sin(kx)]$$ converge for all    $x$ without either  $$a_0+\sum_{k=1}^\infty a_k\cos(kx) \text{ or }  \sum_{k=1}^\infty b_k\sin(kx)$$ converging. This is a problem in Bressoud's analysis book and my solution is as follows: ""No, because if we let $f(x)=a_0+\sum_{k=1}^\infty[a_k\cos(kx)+b_k\sin(kx)]$, then the two other series are obtained by taking $\frac{f(x)\pm f(-x)}{2}$ and since $f(x)$ is convergent for all $x$ the sine and cosine series should also be convergent."" Here is the hint from the back of the book: If the Fourier series converges at $x=0$, then $\sum_{k=1}^\infty a_k$   converges, and therefore the partial sums of $\sum_{k=1}^\infty a_k$   are bounded. Although I think my solution is correct (please correct me if I'm wrong) I still would like to see other solutions and in particular understand the author's hint since I can't see how the boundedness of partial sums can help. Thanks!",,"['real-analysis', 'fourier-series']"
1,"Show that the limit $\lim_{(x,y)\to(0,0)}\frac{y+\sin3x-3x}{y+x^5}$ exist/does not exist.",Show that the limit  exist/does not exist.,"\lim_{(x,y)\to(0,0)}\frac{y+\sin3x-3x}{y+x^5}","How do I show that this limit exist/does not exist? My assumption is that it does not; however I don't see how I can apply the two-path test to verify this. $$\lim_{(x,y)\to(0,0)}\dfrac{y+\sin3x-3x}{y+x^5} = L,$$ $$\lim_{y\to0}\dfrac{y+\sin(0)-(0)}{y+(0)^5} = 1,$$ But I don't see any second path that simplifies this well; $$\lim_{x\to0}\dfrac{(0)+\sin x-x}{(0)+x^5} = \:?$$",How do I show that this limit exist/does not exist? My assumption is that it does not; however I don't see how I can apply the two-path test to verify this. But I don't see any second path that simplifies this well;,"\lim_{(x,y)\to(0,0)}\dfrac{y+\sin3x-3x}{y+x^5} = L, \lim_{y\to0}\dfrac{y+\sin(0)-(0)}{y+(0)^5} = 1, \lim_{x\to0}\dfrac{(0)+\sin x-x}{(0)+x^5} = \:?","['calculus', 'real-analysis', 'limits']"
2,Exercise 27 from chapter 4 (“Hilbert Spaces: An Introduction”) of Stein & Shakarchi's “Real Analysis” 2,Exercise 27 from chapter 4 (“Hilbert Spaces: An Introduction”) of Stein & Shakarchi's “Real Analysis” 2,,"I'm having trouble with the following problem: Prove that the operator $$Tf(x) = \frac{1}{\pi} \int_0^\infty \frac{f(y)}{x+y} dy$$ is bounded on $L^2(0,\infty)$ with norm $||T|| \leq 1$. I have no idea where to start for this problem.  I guess I can write $Tf(x) = \frac{1}{\pi} \int_0^\infty K(x,y)f(y) dy$  with $K(x,y) = \frac{1}{x+y}$ but I don't see how I can proceed from here. Can anyone help me out? Thanks!","I'm having trouble with the following problem: Prove that the operator $$Tf(x) = \frac{1}{\pi} \int_0^\infty \frac{f(y)}{x+y} dy$$ is bounded on $L^2(0,\infty)$ with norm $||T|| \leq 1$. I have no idea where to start for this problem.  I guess I can write $Tf(x) = \frac{1}{\pi} \int_0^\infty K(x,y)f(y) dy$  with $K(x,y) = \frac{1}{x+y}$ but I don't see how I can proceed from here. Can anyone help me out? Thanks!",,"['real-analysis', 'functional-analysis']"
3,Show that all complex Radon measures on a locally compact and $\sigma$-compact Hausdorff space is a Banach space,Show that all complex Radon measures on a locally compact and -compact Hausdorff space is a Banach space,\sigma,"Let $X$ be a locally compact Hausdorff space which is also $\sigma$-compact, and let $M(X)$ be the vector space of all complex Radon measures with the total variation norm $\|\mu\|:=|\mu|(X)$. Show that $M(X)$ is a Banach space. I know that we can invoke the Riesz representation theorem to conclude that $M(X)\equiv C_0(X\to\mathbf{C})^*$, since the dual is always a Banach space, then the claim follows. I want to prove it without using the Riesz representation theorem. Let $(\mu_n)$ be a Cauchy sequence in $M(X)$, we observe that  $$|\mu_m(E)-\mu_n(E)|\leq |\mu_m-\mu_n|(E)\leq \|\mu_m-\mu_n\|$$ for any measurable set $E$, thus $(\mu_n(E))$ is a Cauchy sequence, we can then define $\mu(E):=\lim_{n\to\infty}\mu_n(E)$. But I get stuck here. I don't know how to show the following two statments $\mu(E)$ is a complex Radon measure. $(\mu_n)$ converges to $\mu$ in $M(X)$. For the first statement, let $E_1,E_2,\dots$ be disjoint measurable sets of $X$, we have to show $$\mu(\bigcup_{m=1}^\infty E_m)=\sum_{m=1}^\infty \mu(E_m).$$ By the definition of $\mu$, this is equivalent to $$\lim_{n\to\infty}\sum_{m=1}^\infty\mu_n(E_m)=\sum_{m=1}^\infty\lim_{n\to\infty}\mu_n(E_m).$$","Let $X$ be a locally compact Hausdorff space which is also $\sigma$-compact, and let $M(X)$ be the vector space of all complex Radon measures with the total variation norm $\|\mu\|:=|\mu|(X)$. Show that $M(X)$ is a Banach space. I know that we can invoke the Riesz representation theorem to conclude that $M(X)\equiv C_0(X\to\mathbf{C})^*$, since the dual is always a Banach space, then the claim follows. I want to prove it without using the Riesz representation theorem. Let $(\mu_n)$ be a Cauchy sequence in $M(X)$, we observe that  $$|\mu_m(E)-\mu_n(E)|\leq |\mu_m-\mu_n|(E)\leq \|\mu_m-\mu_n\|$$ for any measurable set $E$, thus $(\mu_n(E))$ is a Cauchy sequence, we can then define $\mu(E):=\lim_{n\to\infty}\mu_n(E)$. But I get stuck here. I don't know how to show the following two statments $\mu(E)$ is a complex Radon measure. $(\mu_n)$ converges to $\mu$ in $M(X)$. For the first statement, let $E_1,E_2,\dots$ be disjoint measurable sets of $X$, we have to show $$\mu(\bigcup_{m=1}^\infty E_m)=\sum_{m=1}^\infty \mu(E_m).$$ By the definition of $\mu$, this is equivalent to $$\lim_{n\to\infty}\sum_{m=1}^\infty\mu_n(E_m)=\sum_{m=1}^\infty\lim_{n\to\infty}\mu_n(E_m).$$",,['real-analysis']
4,"If $\frac{x^2+ax+3}{x^2+x+a}$ takes all real values, prove $4a^3+39<0$","If  takes all real values, prove",\frac{x^2+ax+3}{x^2+x+a} 4a^3+39<0,"If $\frac{x^2+ax+3}{x^2+x+a}$ takes all real values for possible real values of $x$, then prove that $4a^3+39<0$. Here is how I approached it. Let $$\frac{x^2+ax+3}{x^2+x+a}=y$$ Then, $$(y-1)x^2+(y-a)x+(ay-3)=0$$ We want all those $y$, for which there is a real $x$, that is, we want $y$ such that this quadratic has real roots. So, the discriminant $\Delta \geq 0$. $$(y-a)^2-4(y-1)(ay-3) \geq 0$$ On simplifying, we obtain $$(1-4a)y^2+(2a+12)y+(a^2-12) \geq 0$$ We want to find those $a$ for which this is true for all $y$. So, the discriminant $\Delta \leq 0$ (so that the parabola never crosses the $x$ axis.) and $(1-4a)>0$ (so that it faces upwards and is always above the $x$ axis.) This gives  $$(2a+12)^2-4(1-4a)(a^2-12) \leq 0$$ $$(a+6)^2-(1-4a)(a^2-12) \leq 0$$ which simplifies to  $$a^3-9a+12 \leq 0$$ which is not what I set out to achieve. Where did I go wrong? And is there any other method to do this? (Perhaps Calculus based?)","If $\frac{x^2+ax+3}{x^2+x+a}$ takes all real values for possible real values of $x$, then prove that $4a^3+39<0$. Here is how I approached it. Let $$\frac{x^2+ax+3}{x^2+x+a}=y$$ Then, $$(y-1)x^2+(y-a)x+(ay-3)=0$$ We want all those $y$, for which there is a real $x$, that is, we want $y$ such that this quadratic has real roots. So, the discriminant $\Delta \geq 0$. $$(y-a)^2-4(y-1)(ay-3) \geq 0$$ On simplifying, we obtain $$(1-4a)y^2+(2a+12)y+(a^2-12) \geq 0$$ We want to find those $a$ for which this is true for all $y$. So, the discriminant $\Delta \leq 0$ (so that the parabola never crosses the $x$ axis.) and $(1-4a)>0$ (so that it faces upwards and is always above the $x$ axis.) This gives  $$(2a+12)^2-4(1-4a)(a^2-12) \leq 0$$ $$(a+6)^2-(1-4a)(a^2-12) \leq 0$$ which simplifies to  $$a^3-9a+12 \leq 0$$ which is not what I set out to achieve. Where did I go wrong? And is there any other method to do this? (Perhaps Calculus based?)",,"['real-analysis', 'functions', 'proof-verification', 'quadratics']"
5,Mean value theorem for twice differentiable function [duplicate],Mean value theorem for twice differentiable function [duplicate],,"This question already has answers here : How does one prove the Taylor's Theorem by the Cauchy's Mean Value Theorem? (3 answers) Closed 8 years ago . Let $f:(0,\infty)\to \Bbb R$ be a twice differentiable function. In this answer, it is asserted that the MVT lets one write $$f(x+h)=f(x)+f'(x)h+\frac12 f''(\xi)h^2$$ for some $\xi\in (x,x+h)$. It is not clear to me why this should be the case. Using the MVT, one can write $f''(\xi)h=f'(x+h)-f'(x)$ for some $\xi\in (x,x+h)$. Using that, the claim rephrases to $$f(x+h)=f(x)+\frac{1}{2}f'(x)h+\frac{1}{2}f'(x+h)h$$ and I don't see why that should hold. I'm sure I'm being stupid, therefore I much welcome clarification.","This question already has answers here : How does one prove the Taylor's Theorem by the Cauchy's Mean Value Theorem? (3 answers) Closed 8 years ago . Let $f:(0,\infty)\to \Bbb R$ be a twice differentiable function. In this answer, it is asserted that the MVT lets one write $$f(x+h)=f(x)+f'(x)h+\frac12 f''(\xi)h^2$$ for some $\xi\in (x,x+h)$. It is not clear to me why this should be the case. Using the MVT, one can write $f''(\xi)h=f'(x+h)-f'(x)$ for some $\xi\in (x,x+h)$. Using that, the claim rephrases to $$f(x+h)=f(x)+\frac{1}{2}f'(x)h+\frac{1}{2}f'(x+h)h$$ and I don't see why that should hold. I'm sure I'm being stupid, therefore I much welcome clarification.",,"['calculus', 'real-analysis']"
6,Cardinality of $\sigma$-algebra generated by an infinite family of sets,Cardinality of -algebra generated by an infinite family of sets,\sigma,"Let $\mathcal{F}$ be an infinite family of subset of $X$ of cardinality $\kappa$ (thus $\kappa$ is an infinite cardinal). From the recursive description of generated $\sigma$-algebra, I know that the $\sigma$-algebra $\langle \mathcal{F}\rangle$ which is generated by $\mathcal{F}$ has cardinality at most $\kappa^{\aleph_0}$. On the other hand, since $\langle \mathcal{F}\rangle $ contains $\mathcal{F}$,  $\langle \mathcal{F}\rangle$ has cardinality at least $\kappa$. Thus we have $\kappa\leq |\langle \mathcal{F}\rangle |\leq \kappa^{\aleph_0}$. Is is true that $|\langle \mathcal{F}\rangle |=\kappa^{\aleph_0}$? I'm not very familiar with cardinal arithmetic, thanks for any help.","Let $\mathcal{F}$ be an infinite family of subset of $X$ of cardinality $\kappa$ (thus $\kappa$ is an infinite cardinal). From the recursive description of generated $\sigma$-algebra, I know that the $\sigma$-algebra $\langle \mathcal{F}\rangle$ which is generated by $\mathcal{F}$ has cardinality at most $\kappa^{\aleph_0}$. On the other hand, since $\langle \mathcal{F}\rangle $ contains $\mathcal{F}$,  $\langle \mathcal{F}\rangle$ has cardinality at least $\kappa$. Thus we have $\kappa\leq |\langle \mathcal{F}\rangle |\leq \kappa^{\aleph_0}$. Is is true that $|\langle \mathcal{F}\rangle |=\kappa^{\aleph_0}$? I'm not very familiar with cardinal arithmetic, thanks for any help.",,"['real-analysis', 'set-theory']"
7,On the relation of Completeness Axiom of real numbers and Well Ordering Axiom,On the relation of Completeness Axiom of real numbers and Well Ordering Axiom,,In my abstract algebra book one of the first facts stated is the Well Ordering Principle: (*) Every non-empty set of positive integers has a smallest member. In real analysis on the other hand one of the first things introduced are the real numbers and their Completeness Axiom: Every nonempty set of real numbers having an upper bound must have a least upper bound. Which is equivalent to: (**) Every nonempty set of real numbers having a lower bound must have a biggest lower bound (infimum). It has never been mentioned in any book I've read and I don't know if they have anything to do with each other but (*) and (**) seem to me to be such that (**) implies (*). Is the Well Ordering Principle a consequence of the Completeness of   the real numbers? Or do they have nothing to do with each other? How   should I think of them in terms of how they relate to each other? Is it okay to see one as a consequence of the other?,In my abstract algebra book one of the first facts stated is the Well Ordering Principle: (*) Every non-empty set of positive integers has a smallest member. In real analysis on the other hand one of the first things introduced are the real numbers and their Completeness Axiom: Every nonempty set of real numbers having an upper bound must have a least upper bound. Which is equivalent to: (**) Every nonempty set of real numbers having a lower bound must have a biggest lower bound (infimum). It has never been mentioned in any book I've read and I don't know if they have anything to do with each other but (*) and (**) seem to me to be such that (**) implies (*). Is the Well Ordering Principle a consequence of the Completeness of   the real numbers? Or do they have nothing to do with each other? How   should I think of them in terms of how they relate to each other? Is it okay to see one as a consequence of the other?,,"['real-analysis', 'order-theory', 'real-numbers', 'axioms']"
8,Show $x^n+ax+b=0$ has most two solutions,Show  has most two solutions,x^n+ax+b=0,"For any real numbers $a$ and $b$ and even natural number $n$, Show $x^n+ax+b=0$ has most two solutions for all $x$ in $\mathbb{R}$. let $f(x)=x^n+ax+b$. Set $f(x)$ and $a$ be $0$, then we have $x^n+b=0$. So $x=\pm b^{1/n}$. $f$ has two solutions. To yeld a contradiction. Now assume that $f$ has more than two solutions. As $f$ is a polynomial on $\mathbb{R}$, it is differentiable on $\mathbb{R}$; thus $f'(x)=nx^{n-1}+a$. Set $f'(x)=0$, then $0=nx^{n-1}+a\Rightarrow x=\left(\frac{-a}{n}\right)^{1/(n-1)}$. At $x=\left(\frac{-a}{n}\right)^{1/(n-1)}$, $f(\left(\frac{-a}{n}\right)^{1/(n-1)})$ is the only extreme value of $f$ which contradicts with $f$ has more than two solutions. Can someone check this solution? I am not sure right or not. If not right, please give me a hint or suggestion. Thanks","For any real numbers $a$ and $b$ and even natural number $n$, Show $x^n+ax+b=0$ has most two solutions for all $x$ in $\mathbb{R}$. let $f(x)=x^n+ax+b$. Set $f(x)$ and $a$ be $0$, then we have $x^n+b=0$. So $x=\pm b^{1/n}$. $f$ has two solutions. To yeld a contradiction. Now assume that $f$ has more than two solutions. As $f$ is a polynomial on $\mathbb{R}$, it is differentiable on $\mathbb{R}$; thus $f'(x)=nx^{n-1}+a$. Set $f'(x)=0$, then $0=nx^{n-1}+a\Rightarrow x=\left(\frac{-a}{n}\right)^{1/(n-1)}$. At $x=\left(\frac{-a}{n}\right)^{1/(n-1)}$, $f(\left(\frac{-a}{n}\right)^{1/(n-1)})$ is the only extreme value of $f$ which contradicts with $f$ has more than two solutions. Can someone check this solution? I am not sure right or not. If not right, please give me a hint or suggestion. Thanks",,"['real-analysis', 'proof-verification']"
9,"If $\int_A f\,dm = 0$ for all $A$ having some fixed measure $C$, then $f = 0$ almost everywhere","If  for all  having some fixed measure , then  almost everywhere","\int_A f\,dm = 0 A C f = 0","Let $ f \in L^1[0,1]$. Assume that there is a constant C, with $0 < C < 1$, such that for every measurable set $A \subset [0,1] $ with $m(A)=C$, we have  $ \int_{A} f dm = 0 $.  Prove that     $f = 0$ almost everywhere. I tried to do my contradiction but I could not get my head around it. Any hints or ideas are appreciated.","Let $ f \in L^1[0,1]$. Assume that there is a constant C, with $0 < C < 1$, such that for every measurable set $A \subset [0,1] $ with $m(A)=C$, we have  $ \int_{A} f dm = 0 $.  Prove that     $f = 0$ almost everywhere. I tried to do my contradiction but I could not get my head around it. Any hints or ideas are appreciated.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'almost-everywhere']"
10,A discontinuous almost everywhere map does not admit an invariant measure,A discontinuous almost everywhere map does not admit an invariant measure,,"Let's consider a map $T: X \rightarrow X$ so that it's discontinuous almost everywhere (in particular, let $X = \mathbb{R}$, and $T = 1_{\mathbb{Q}}$ -- Dirichlet function). Is it true that $T$ does not admit an invariant measure (e.g. $\lambda(X) = \lambda(T^{-1}(X))$)? The original problem is to find a map from $3$-dimensional ball to itself with the given property. Bogolybov-Krylov theorem states that for every compact space $X$ and for every continuous $T$ such measure does exist. So, it's reasonable to look for a very ""bad"" maps, such as considered before.","Let's consider a map $T: X \rightarrow X$ so that it's discontinuous almost everywhere (in particular, let $X = \mathbb{R}$, and $T = 1_{\mathbb{Q}}$ -- Dirichlet function). Is it true that $T$ does not admit an invariant measure (e.g. $\lambda(X) = \lambda(T^{-1}(X))$)? The original problem is to find a map from $3$-dimensional ball to itself with the given property. Bogolybov-Krylov theorem states that for every compact space $X$ and for every continuous $T$ such measure does exist. So, it's reasonable to look for a very ""bad"" maps, such as considered before.",,"['real-analysis', 'measure-theory', 'ergodic-theory']"
11,"Is the set $\big\{(x,y) : x^2 + y^2 \leq 1\big\}$ compact in $\left(\mathbb{R}^2,\sigma\right)$?",Is the set  compact in ?,"\big\{(x,y) : x^2 + y^2 \leq 1\big\} \left(\mathbb{R}^2,\sigma\right)","Define $\sigma$ by $$\sigma \big( \left( x_1,y_1\right) ,\left( x_2,y_2\right) \big) =   \begin{cases}        \hfill \big|{y_1 - y_2}\big|  \hfill & \text{ if $x_1 = x_2$} \\       \hfill \big|{x_1 - x_2}\big| + \big|y_1\big|+\big|y_2\big| \hfill & \text{ if $x_1 \neq x_2$} \\   \end{cases} $$ I've already proved that $\sigma$ is a metric.  I've played around trying to sketch $B_\sigma \big(\left(0,0\right),1\big)$ and have come to the conclusion that it is a square rotated 45 degrees and not containing the edges.  It's vertices are at $(0,1), (0,-1), (-1,0), (1,0)$ and it is fully contained within the unit circle. e.g. $\big<\big>$ if the ""lines"" were dotted. Unfortunately, this has yet to produce an epiphany.","Define $\sigma$ by $$\sigma \big( \left( x_1,y_1\right) ,\left( x_2,y_2\right) \big) =   \begin{cases}        \hfill \big|{y_1 - y_2}\big|  \hfill & \text{ if $x_1 = x_2$} \\       \hfill \big|{x_1 - x_2}\big| + \big|y_1\big|+\big|y_2\big| \hfill & \text{ if $x_1 \neq x_2$} \\   \end{cases} $$ I've already proved that $\sigma$ is a metric.  I've played around trying to sketch $B_\sigma \big(\left(0,0\right),1\big)$ and have come to the conclusion that it is a square rotated 45 degrees and not containing the edges.  It's vertices are at $(0,1), (0,-1), (-1,0), (1,0)$ and it is fully contained within the unit circle. e.g. $\big<\big>$ if the ""lines"" were dotted. Unfortunately, this has yet to produce an epiphany.",,"['real-analysis', 'general-topology']"
12,"For which $\mathcal{F} \subset C[0,1]$ does there exist a sequence converging pointwise to the supremum?",For which  does there exist a sequence converging pointwise to the supremum?,"\mathcal{F} \subset C[0,1]","This question is following this one . Take a subset $\mathcal{F} \subset C[0,1]$ and consider  $$g(x)= \sup{ \{f(x)\mid f\in \mathcal{F}} \}.$$ Added : $g$ may take infinite values. What property(ies) should have $\mathcal{F}$ in order that it exists a sequence $(f_n)$ of elements of $\mathcal{F}$ that converges pointwise to $g$? I know that the question is open. Answers can be in the form of examples. Please don't shoot to quick to close it because it is an open question!","This question is following this one . Take a subset $\mathcal{F} \subset C[0,1]$ and consider  $$g(x)= \sup{ \{f(x)\mid f\in \mathcal{F}} \}.$$ Added : $g$ may take infinite values. What property(ies) should have $\mathcal{F}$ in order that it exists a sequence $(f_n)$ of elements of $\mathcal{F}$ that converges pointwise to $g$? I know that the question is open. Answers can be in the form of examples. Please don't shoot to quick to close it because it is an open question!",,"['real-analysis', 'general-topology', 'convergence-divergence']"
13,"$C([a,b])$ is separable",is separable,"C([a,b])","We will show that $\mathbb{Q}[x]$ is dense in $C([a,b]).$ This follows from the Weierstrass Approximation Theorem, since given any $f \in C([a,b])$, there exists a sequence of polynomials $$\{p_n\}_{n \in \mathbb{N}} \xrightarrow{u} f.$$ Now, label the real coefficients of $p_n$ as $\{a_0,a_1, \ldots a_n\}.$ Take sequences of rationals $\{q_{i_k} \}_{k \in \mathbb{N}} \to a_i$ with $q_{i_k} \leq a_i$. Let $r_{n,k} \in \mathbb{Q}[x]$ be the polynomial with these coefficents. Clearly then $$\{r_{n,k}\}_{k \in \mathbb{N}} \xrightarrow{u} p_n$$ for each $p_n$ since given $\epsilon >0$ and $K \in \mathbb{N}$ large enough, $$||r_{n,K} - p_n||_{C([a,b])} \leq \epsilon \sum_{i=1}^{n}|x_0^i|$$ for some $x_0 \in [a,b]$ since $r_{n,K} - p_n$ is continuous on the compact set $[a,b]$ at attains a maximum at, say, $x_0$. This $x_0$ works for all $k \geq K$ since the rationals converge to $a_i$ from the left.   This then implies that $$\lim_{n \to \infty} \lim_{ k \to \infty} p_{n,k}  = f,$$ i.e.  $$\{r_{n,k}\}_{n \in \mathbb{N}, \, k \in \mathbb{N}} \xrightarrow{u} f$$ implying $C([a,b])$ is separable. Now my question concerns the bit about the $x_0$. Is it fair to say that this $x_0$ is the same for all polynomials if the rationals converge on the left or right (the point being from one side only so that the signs never change)? Is this even necessary? Any other details I need add?","We will show that $\mathbb{Q}[x]$ is dense in $C([a,b]).$ This follows from the Weierstrass Approximation Theorem, since given any $f \in C([a,b])$, there exists a sequence of polynomials $$\{p_n\}_{n \in \mathbb{N}} \xrightarrow{u} f.$$ Now, label the real coefficients of $p_n$ as $\{a_0,a_1, \ldots a_n\}.$ Take sequences of rationals $\{q_{i_k} \}_{k \in \mathbb{N}} \to a_i$ with $q_{i_k} \leq a_i$. Let $r_{n,k} \in \mathbb{Q}[x]$ be the polynomial with these coefficents. Clearly then $$\{r_{n,k}\}_{k \in \mathbb{N}} \xrightarrow{u} p_n$$ for each $p_n$ since given $\epsilon >0$ and $K \in \mathbb{N}$ large enough, $$||r_{n,K} - p_n||_{C([a,b])} \leq \epsilon \sum_{i=1}^{n}|x_0^i|$$ for some $x_0 \in [a,b]$ since $r_{n,K} - p_n$ is continuous on the compact set $[a,b]$ at attains a maximum at, say, $x_0$. This $x_0$ works for all $k \geq K$ since the rationals converge to $a_i$ from the left.   This then implies that $$\lim_{n \to \infty} \lim_{ k \to \infty} p_{n,k}  = f,$$ i.e.  $$\{r_{n,k}\}_{n \in \mathbb{N}, \, k \in \mathbb{N}} \xrightarrow{u} f$$ implying $C([a,b])$ is separable. Now my question concerns the bit about the $x_0$. Is it fair to say that this $x_0$ is the same for all polynomials if the rationals converge on the left or right (the point being from one side only so that the signs never change)? Is this even necessary? Any other details I need add?",,"['real-analysis', 'analysis', 'functional-analysis', 'proof-verification']"
14,caratheodory measurability condition,caratheodory measurability condition,,"(1)  If $E$ is a subset of $\mathbb R$ with finite outer measure, i.e. $m^{*}(E) <\infty$; and   (2) $E$ is not Lebesgue measurable, i.e. there exists $F$ such that  $m^{*} (F) < m^{*}(EF) + m^{*}(FE^{c}).$ [Claim] There exists an open set $O\supset E$ with finite outer measure, such  that   $$m^{*}(O E^{c})  > m^{*} (O)  - m^{*} (E).$$ My  questions are the following. (1) Is the above claim correct? I've seen this in Royden's book, but have a bit concern. Note that a set $E$ is Leb measurable if  it satisfies caratheodory condition: $m^{*} (F) = m^{*}(EF) + m^{*}(FE^{c})$ for  all subset $F$. If the claim is yes, it seems that measurability condition can  be reduced from all subset $F$ to Borel set $F$. (2) If the claim is correct, a proof is needed.","(1)  If $E$ is a subset of $\mathbb R$ with finite outer measure, i.e. $m^{*}(E) <\infty$; and   (2) $E$ is not Lebesgue measurable, i.e. there exists $F$ such that  $m^{*} (F) < m^{*}(EF) + m^{*}(FE^{c}).$ [Claim] There exists an open set $O\supset E$ with finite outer measure, such  that   $$m^{*}(O E^{c})  > m^{*} (O)  - m^{*} (E).$$ My  questions are the following. (1) Is the above claim correct? I've seen this in Royden's book, but have a bit concern. Note that a set $E$ is Leb measurable if  it satisfies caratheodory condition: $m^{*} (F) = m^{*}(EF) + m^{*}(FE^{c})$ for  all subset $F$. If the claim is yes, it seems that measurability condition can  be reduced from all subset $F$ to Borel set $F$. (2) If the claim is correct, a proof is needed.",,['real-analysis']
15,Strange definition of real differentiable function,Strange definition of real differentiable function,,"I am currently refreshing my complex analysis knowledge (I had a course many years ago, but I've forgotten almost all of it). The German textbook I'm using, which is Funktionentheorie by Fischer & Lieb, states the following definition of real differentiability A function $g:U\to\Bbb R$ is said to be real differentiable in the point $z_0=x_0+iy_0$ of the domain $U\subseteq\Bbb C$ if there exist real-valued functions $\Delta_1$ and $\Delta_2$ on $U$ which are continuous at $z_0$ such that for all $z=x+iy\in U$ we have   $$ g(z) = g(z_0) + \Delta_1(z)(x-x_0) + \Delta_2(z)(y-y_0) \qquad\qquad (1) $$ I found myself wondering how this is related to the usual definition of a partially differentiable function. If some function $g:U\to\Bbb R$ satisfies this condition, and we define $\Delta_1^{y_0}(x)=\Delta_1(x,y_0)$ and $\Delta_2^{x_0} = \Delta_2(x_0,-)$, then  $$ g(x,y_0) = g(x_0,y_0) + \Delta_1^{y_0}(x)(x-x_0) \quad\text{and}\quad g(x_0,y) = g(x_0,y_0) + \Delta_2^{x_0}(y)(y-y_0) $$ so $\Delta_1^{y_0}(x_0) = \frac{\partial g}{\partial x}(z_0) = \lim_{x\to x_0}\frac{g(x,y_0)-g(x_0,y_0)}{x-x_0}$ and $\Delta_2^{x_0}(z_0) = \frac{\partial g}{\partial y}(z_0) = \lim_{y\to y_0}\frac{g(x_0,y)-g(x_0,y_0)}{y-y_0}$. So the partial derivatives exist. Moreover, $g$ is continuous at $z_0$. Now assume for the other direction that $g$ is continuous at $z_0$ and that $\Delta_1^{y_0}$ and $\Delta_2^{x_0}$ exist as above (so they are continuous at $x_0$ and $y_0$, respectively). I figured out that if we set $$ \Delta_1(x,y) = \frac12\left(\Delta_1^{y_0}(x) + \frac{g(x,y)-g(x_0,y)}{x-x_0}\right) \\ \Delta_2(x,y) = \frac12\left(\Delta_2^{x_0}(y) + \frac{g(x,y)-g(x,y_0)}{y-y_0}\right) $$ then this satisfies (1). Note that we let the right-hand summand in the first line assume the value $\Delta_1^{y_0}(x_0)$ at $(x_0,y_0)$, and an arbitrary value at any other $(x_0,y)$, and similarly for the summand in then second line. Now my problem: I have trouble showing that the right-hand summand in the first line is continuous at $(x_0,y_0)$. Any ideas? Or maybe definition above is not equivalent to the existence of the partial derivatives? Is it even equivalent to anything else? Some words about the context: In the textbook, three pages later, it is shown that a function $f=g+ih:U\to\Bbb C$ is complex differentiable in $z_0$ if and only if $\frac{\partial f}{\partial \overline z}(z_0)=0$ and the functions $g$ and $h$ are real differentiable in the sense of (1).","I am currently refreshing my complex analysis knowledge (I had a course many years ago, but I've forgotten almost all of it). The German textbook I'm using, which is Funktionentheorie by Fischer & Lieb, states the following definition of real differentiability A function $g:U\to\Bbb R$ is said to be real differentiable in the point $z_0=x_0+iy_0$ of the domain $U\subseteq\Bbb C$ if there exist real-valued functions $\Delta_1$ and $\Delta_2$ on $U$ which are continuous at $z_0$ such that for all $z=x+iy\in U$ we have   $$ g(z) = g(z_0) + \Delta_1(z)(x-x_0) + \Delta_2(z)(y-y_0) \qquad\qquad (1) $$ I found myself wondering how this is related to the usual definition of a partially differentiable function. If some function $g:U\to\Bbb R$ satisfies this condition, and we define $\Delta_1^{y_0}(x)=\Delta_1(x,y_0)$ and $\Delta_2^{x_0} = \Delta_2(x_0,-)$, then  $$ g(x,y_0) = g(x_0,y_0) + \Delta_1^{y_0}(x)(x-x_0) \quad\text{and}\quad g(x_0,y) = g(x_0,y_0) + \Delta_2^{x_0}(y)(y-y_0) $$ so $\Delta_1^{y_0}(x_0) = \frac{\partial g}{\partial x}(z_0) = \lim_{x\to x_0}\frac{g(x,y_0)-g(x_0,y_0)}{x-x_0}$ and $\Delta_2^{x_0}(z_0) = \frac{\partial g}{\partial y}(z_0) = \lim_{y\to y_0}\frac{g(x_0,y)-g(x_0,y_0)}{y-y_0}$. So the partial derivatives exist. Moreover, $g$ is continuous at $z_0$. Now assume for the other direction that $g$ is continuous at $z_0$ and that $\Delta_1^{y_0}$ and $\Delta_2^{x_0}$ exist as above (so they are continuous at $x_0$ and $y_0$, respectively). I figured out that if we set $$ \Delta_1(x,y) = \frac12\left(\Delta_1^{y_0}(x) + \frac{g(x,y)-g(x_0,y)}{x-x_0}\right) \\ \Delta_2(x,y) = \frac12\left(\Delta_2^{x_0}(y) + \frac{g(x,y)-g(x,y_0)}{y-y_0}\right) $$ then this satisfies (1). Note that we let the right-hand summand in the first line assume the value $\Delta_1^{y_0}(x_0)$ at $(x_0,y_0)$, and an arbitrary value at any other $(x_0,y)$, and similarly for the summand in then second line. Now my problem: I have trouble showing that the right-hand summand in the first line is continuous at $(x_0,y_0)$. Any ideas? Or maybe definition above is not equivalent to the existence of the partial derivatives? Is it even equivalent to anything else? Some words about the context: In the textbook, three pages later, it is shown that a function $f=g+ih:U\to\Bbb C$ is complex differentiable in $z_0$ if and only if $\frac{\partial f}{\partial \overline z}(z_0)=0$ and the functions $g$ and $h$ are real differentiable in the sense of (1).",,"['real-analysis', 'complex-analysis', 'continuity', 'definition']"
16,Confused by Monotone class theorem for functions,Confused by Monotone class theorem for functions,,"Monotone Class Theorem has two types. One is Monotone class theorem for sets and the other for functions. I have no doubt for sets. Here is a reference of definition of Monotone Class Theorem for functions from Probability: Theory and Examples, Durrett Rick, 4.1-edition . Theorem 6.1.3. Monotone class theorem . Let $\mathcal {A}$ be a $\pi$ -system that contains $\Omega$ and let $\mathcal {H}$ be a collection of real-valued functions that satisfies: (i) If $A ∈ \mathcal {A}$ , then $1_A ∈ \mathcal {H}$ . (ii) If $f, g ∈ \mathcal {H}$ , then $f + g$ , and $cf ∈ \mathcal {H}$ for any real number $c$ . (iii) If $f_n ∈ \mathcal {H}$ are nonnegative and increase to a bounded function $f$ , then $f ∈ \mathcal {H}$ . Then $\mathcal {H}$ contains all bounded functions measurable with respect to $\sigma (\mathcal {A})$ . Proof. The assumption $\Omega \in \mathcal{A}$ , (ii), and (iii) imply that $\mathcal {G} = \{A : 1_A ∈ \mathcal {H}\}$ is a $\lambda$ -system so by (i) and the $\pi − \lambda$ theorem, Theorem 2.1.2, $\mathcal {G} ⊃ \sigma(A)$ . (ii) implies $\mathcal{H}$ contains all simple functions, and (iii) implies that $\mathcal{H}$ contains all bounded measurable functions. I can't understand one sentence from the proof, ""(ii) implies $\mathcal{H}$ contains all simple functions"". Why? P.S. I think ""measurable with respect to $\sigma(\mathcal{A})$ "" means $\sigma(\mathcal {A})$ -measurable. I know $\mathcal{F}$ -measurable mapping that is suppose $(\Omega, \mathcal{F})$ and $(E, \mathcal{E})$ are two measurable spaces, $f$ is a mapping from $\Omega \to E$ . $f$ is a $\mathcal{F}$ -measurable mapping if $f^{-1}(A) \in \mathcal{F}$ for $\forall A \in \mathcal{E}$ . So $\sigma(\mathcal {A})$ -measurable probably means those $f: (\Omega, \sigma (\mathcal{A})) \to ([0, +\infty), \mathcal{B}{[0, +\infty)})$ where $\mathcal{B}{[0, +\infty)}$ denotes Borel $\sigma$ -algebra on $[0, +\infty)$ . Update: The author didn't make the sentence clear. So I try to make it a little bit more complete. Let $\forall f$ is $\sigma(\mathcal{A})$ -measurable. I suppose $g_n =\sum_{k=0}^{n2^n -1} \frac{k}{2^n} I_{\{\frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n}\}} + n I_{f^+ \ge n}$ where $f^+ = f ∨ 0$ . Then $g_n \in \mathcal{H}$ coz $f$ is $\sigma(\mathcal{A})$ -measurable that is $\{x: \frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n} \} \in \sigma(\mathcal{A})$ and $\{x: f^+ \ge n\} \in \sigma(\mathcal{A})$ . Then $g_n$ increasingly converges to $f^+$ and so does $f^-$ . $f = f^+ - f^- \in \mathcal{H}$ . Please note that I can only get simple functions defined on $\{x: \frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n} \}$ and $\{x: f^+ \ge n\}$ are in $\mathcal{H}$ . I can't prove $\mathcal{H}$ will contains all simple functions. Am I right?","Monotone Class Theorem has two types. One is Monotone class theorem for sets and the other for functions. I have no doubt for sets. Here is a reference of definition of Monotone Class Theorem for functions from Probability: Theory and Examples, Durrett Rick, 4.1-edition . Theorem 6.1.3. Monotone class theorem . Let be a -system that contains and let be a collection of real-valued functions that satisfies: (i) If , then . (ii) If , then , and for any real number . (iii) If are nonnegative and increase to a bounded function , then . Then contains all bounded functions measurable with respect to . Proof. The assumption , (ii), and (iii) imply that is a -system so by (i) and the theorem, Theorem 2.1.2, . (ii) implies contains all simple functions, and (iii) implies that contains all bounded measurable functions. I can't understand one sentence from the proof, ""(ii) implies contains all simple functions"". Why? P.S. I think ""measurable with respect to "" means -measurable. I know -measurable mapping that is suppose and are two measurable spaces, is a mapping from . is a -measurable mapping if for . So -measurable probably means those where denotes Borel -algebra on . Update: The author didn't make the sentence clear. So I try to make it a little bit more complete. Let is -measurable. I suppose where . Then coz is -measurable that is and . Then increasingly converges to and so does . . Please note that I can only get simple functions defined on and are in . I can't prove will contains all simple functions. Am I right?","\mathcal {A} \pi \Omega \mathcal {H} A ∈ \mathcal {A} 1_A ∈ \mathcal {H} f, g ∈ \mathcal {H} f + g cf ∈ \mathcal {H} c f_n ∈ \mathcal {H} f f ∈ \mathcal {H} \mathcal {H} \sigma (\mathcal {A}) \Omega \in \mathcal{A} \mathcal {G} = \{A : 1_A ∈ \mathcal {H}\} \lambda \pi − \lambda \mathcal {G} ⊃ \sigma(A) \mathcal{H} \mathcal{H} \mathcal{H} \sigma(\mathcal{A}) \sigma(\mathcal {A}) \mathcal{F} (\Omega, \mathcal{F}) (E, \mathcal{E}) f \Omega \to E f \mathcal{F} f^{-1}(A) \in \mathcal{F} \forall A \in \mathcal{E} \sigma(\mathcal {A}) f: (\Omega, \sigma (\mathcal{A})) \to ([0, +\infty), \mathcal{B}{[0, +\infty)}) \mathcal{B}{[0, +\infty)} \sigma [0, +\infty) \forall f \sigma(\mathcal{A}) g_n =\sum_{k=0}^{n2^n -1} \frac{k}{2^n} I_{\{\frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n}\}} + n I_{f^+ \ge n} f^+ = f ∨ 0 g_n \in \mathcal{H} f \sigma(\mathcal{A}) \{x: \frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n} \} \in \sigma(\mathcal{A}) \{x: f^+ \ge n\} \in \sigma(\mathcal{A}) g_n f^+ f^- f = f^+ - f^- \in \mathcal{H} \{x: \frac{k}{2^n} \le f^+ ＜ \frac{k+1}{2^n} \} \{x: f^+ \ge n\} \mathcal{H} \mathcal{H}","['real-analysis', 'measure-theory', 'probability']"
17,Proving that and how $ \frac{1}{n}\sum\limits_{p\le n}\lfloor n/p \rfloor - \sum\limits_{p\le n} 1/p $ approaches $0$,Proving that and how  approaches, \frac{1}{n}\sum\limits_{p\le n}\lfloor n/p \rfloor - \sum\limits_{p\le n} 1/p  0,"Let $p$ denote a generic prime number. By Mertens' second theorem, the sequence $$\sum\limits_{\ p \le n} \frac1p - \log\log n$$ converges to the Meissel-Mertens constant $M\approx 0.2614972$. Now let $\omega(n)$ be the number of distinct prime factors of $n$. Analogously, we have $$\lim_{n\to\infty} \frac{1}{n}\sum\limits_{k\le n} \omega(k) - \log\log n = M,$$ hence combining this, the first result and $$\sum\limits_{k\le n} \omega(k)=\sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor, $$ we find $$\bbox[5px,border:2px solid #B2B550]{\lim_{n\to\infty} \frac{1}{n} \sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor - \sum_{p\le n} \frac{1}{p}=0. }\tag{$\star$}$$ How could we prove $(\star)$ directly? Is it known if the sequence is negative for all $n>2$, or at least if it changes sign finitely often?","Let $p$ denote a generic prime number. By Mertens' second theorem, the sequence $$\sum\limits_{\ p \le n} \frac1p - \log\log n$$ converges to the Meissel-Mertens constant $M\approx 0.2614972$. Now let $\omega(n)$ be the number of distinct prime factors of $n$. Analogously, we have $$\lim_{n\to\infty} \frac{1}{n}\sum\limits_{k\le n} \omega(k) - \log\log n = M,$$ hence combining this, the first result and $$\sum\limits_{k\le n} \omega(k)=\sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor, $$ we find $$\bbox[5px,border:2px solid #B2B550]{\lim_{n\to\infty} \frac{1}{n} \sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor - \sum_{p\le n} \frac{1}{p}=0. }\tag{$\star$}$$ How could we prove $(\star)$ directly? Is it known if the sequence is negative for all $n>2$, or at least if it changes sign finitely often?",,"['real-analysis', 'sequences-and-series', 'number-theory', 'prime-numbers', 'alternative-proof']"
18,"For which $1\le p\le\infty$ does $u$ belong to $W^{1,p}(\Omega)$?",For which  does  belong to ?,"1\le p\le\infty u W^{1,p}(\Omega)",Hi could anyone help with a solution for problem 7 Evans PDE chapter 5? I think it is basically about checking which $p$ allows $$\int_{\Omega} |u|^p dx+\int_{\Omega}|Du|^p dx<\infty$$ ? But I tend to generate messy output as I always do... Please help...,Hi could anyone help with a solution for problem 7 Evans PDE chapter 5? I think it is basically about checking which $p$ allows $$\int_{\Omega} |u|^p dx+\int_{\Omega}|Du|^p dx<\infty$$ ? But I tend to generate messy output as I always do... Please help...,,"['real-analysis', 'partial-differential-equations']"
19,A unusual inequality about function $\ln$,A unusual inequality about function,\ln,"These day,I met a unusual inequality when I solve a difficult problem, and proving the inequality means I have done the work! Could you show me how to prove it or deny it? By the way, I believe that it's true! Prove that, for all $t > 0$ , \begin{align*} &4\ln t\ln (t + 2) - \ln t\ln (t + 1) - 3\ln t\ln (t + 3)\\  + &4\ln (t + 1)\ln (t + 3) - 3\ln (t + 1)\ln (t + 2) - \ln (t + 2)\ln \left( {t + 3} \right)>0. \end{align*} Let $$f\left( t \right) = 4\ln t\ln \left( {t + 2} \right) - \ln t\ln \left( {t + 1} \right) - 3\ln t\ln \left( {t + 3} \right) + 4\ln \left( {t + 1} \right)\ln \left( {t + 3} \right) - 3\ln \left( {t + 1} \right)\ln \left( {t + 2} \right) - \ln \left( {t + 2} \right)\ln \left( {t + 3} \right),$$ We have $$f'\left( t \right) = \frac{{2\left[ {{t^2}\ln t - 3{{\left( {t + 1} \right)}^2}\ln \left( {t + 1} \right) + 3{{\left( {t + 2} \right)}^2}\ln \left( {t + 2} \right) - {{\left( {t + 3} \right)}^2}\ln \left( {t + 3} \right)} \right]}}{{t\left( {t + 1} \right)\left( {t + 2} \right)\left( {t + 3} \right)}}.$$ Let $$g\left( t \right) = {t^2}\ln t - 3{\left( {t + 1} \right)^2}\ln \left( {t + 1} \right) + 3{\left( {t + 2} \right)^2}\ln \left( {t + 2} \right) - {\left( {t + 3} \right)^2}\ln \left( {t + 3} \right),$$ we got $$g'\left( t \right) = 2\left[ {t\ln t - 3\left( {t + 1} \right)\ln \left( {t + 1} \right) + 3\left( {t + 2} \right)\ln \left( {t + 2} \right) - \left( {t + 3} \right)\ln \left( {t + 3} \right)} \right].$$ And let $$h\left( x \right) = t\ln t - 3\left( {t + 1} \right)\ln \left( {t + 1} \right) + 3\left( {t + 2} \right)\ln \left( {t + 2} \right) - \left( {t + 3} \right)\ln \left( {t + 3} \right),$$ we have \begin{align*} h'\left( x \right) &= \ln t - 3\ln \left( {t + 1} \right) + 3\ln \left( {t + 2} \right) - \ln \left( {t + 3} \right)\\ &= \ln \frac{{t{{\left( {t + 2} \right)}^3}}}{{{{\left( {t + 1} \right)}^3}\left( {t + 3} \right)}} = \ln \left[ {1 - \frac{{2t + 3}}{{{{\left( {t + 1} \right)}^3}\left( {t + 3} \right)}}} \right] < 0. \end{align*} However, it seems that there are no use!","These day,I met a unusual inequality when I solve a difficult problem, and proving the inequality means I have done the work! Could you show me how to prove it or deny it? By the way, I believe that it's true! Prove that, for all , Let We have Let we got And let we have However, it seems that there are no use!","t > 0 \begin{align*}
&4\ln t\ln (t + 2) - \ln t\ln (t + 1) - 3\ln t\ln (t + 3)\\
 + &4\ln (t + 1)\ln (t + 3) - 3\ln (t + 1)\ln (t + 2) - \ln (t + 2)\ln \left( {t + 3} \right)>0.
\end{align*} f\left( t \right) = 4\ln t\ln \left( {t + 2} \right) - \ln t\ln \left( {t + 1} \right) - 3\ln t\ln \left( {t + 3} \right) + 4\ln \left( {t + 1} \right)\ln \left( {t + 3} \right) - 3\ln \left( {t + 1} \right)\ln \left( {t + 2} \right) - \ln \left( {t + 2} \right)\ln \left( {t + 3} \right), f'\left( t \right) = \frac{{2\left[ {{t^2}\ln t - 3{{\left( {t + 1} \right)}^2}\ln \left( {t + 1} \right) + 3{{\left( {t + 2} \right)}^2}\ln \left( {t + 2} \right) - {{\left( {t + 3} \right)}^2}\ln \left( {t + 3} \right)} \right]}}{{t\left( {t + 1} \right)\left( {t + 2} \right)\left( {t + 3} \right)}}. g\left( t \right) = {t^2}\ln t - 3{\left( {t + 1} \right)^2}\ln \left( {t + 1} \right) + 3{\left( {t + 2} \right)^2}\ln \left( {t + 2} \right) - {\left( {t + 3} \right)^2}\ln \left( {t + 3} \right), g'\left( t \right) = 2\left[ {t\ln t - 3\left( {t + 1} \right)\ln \left( {t + 1} \right) + 3\left( {t + 2} \right)\ln \left( {t + 2} \right) - \left( {t + 3} \right)\ln \left( {t + 3} \right)} \right]. h\left( x \right) = t\ln t - 3\left( {t + 1} \right)\ln \left( {t + 1} \right) + 3\left( {t + 2} \right)\ln \left( {t + 2} \right) - \left( {t + 3} \right)\ln \left( {t + 3} \right), \begin{align*}
h'\left( x \right) &= \ln t - 3\ln \left( {t + 1} \right) + 3\ln \left( {t + 2} \right) - \ln \left( {t + 3} \right)\\
&= \ln \frac{{t{{\left( {t + 2} \right)}^3}}}{{{{\left( {t + 1} \right)}^3}\left( {t + 3} \right)}} = \ln \left[ {1 - \frac{{2t + 3}}{{{{\left( {t + 1} \right)}^3}\left( {t + 3} \right)}}} \right] < 0.
\end{align*}","['real-analysis', 'calculus', 'analysis', 'derivatives', 'inequality']"
20,"Show $\forall\varepsilon>0\,\lim_{n\to\infty}\frac{\#\{\text{positive divisors of n}\}}{n^\varepsilon}=0$",Show,"\forall\varepsilon>0\,\lim_{n\to\infty}\frac{\#\{\text{positive divisors of n}\}}{n^\varepsilon}=0","Show that $\forall\varepsilon>0,$ $$\lim_{n\to\infty}\frac{\#\{\text{positive divisors of  n}\}}{n^\varepsilon}=0$$ I'm trying to solve this problem for a long time, but I'm really stuck I have totally no idea where to start. I tried replacing $\varepsilon$ by $\frac{1}{k}$ where $k$ is a natural number, and show the statement is true for all $k$ by induction, but I haven't succeeded and it doesn't seem promising. If you give me any advice or comment, I would greatly appreciate. Thank you.","Show that $\forall\varepsilon>0,$ $$\lim_{n\to\infty}\frac{\#\{\text{positive divisors of  n}\}}{n^\varepsilon}=0$$ I'm trying to solve this problem for a long time, but I'm really stuck I have totally no idea where to start. I tried replacing $\varepsilon$ by $\frac{1}{k}$ where $k$ is a natural number, and show the statement is true for all $k$ by induction, but I haven't succeeded and it doesn't seem promising. If you give me any advice or comment, I would greatly appreciate. Thank you.",,"['real-analysis', 'analysis', 'elementary-number-theory', 'divisor-counting-function']"
21,which functions can be obtained as a composition of a continuous function with itself? [duplicate],which functions can be obtained as a composition of a continuous function with itself? [duplicate],,"This question already has answers here : On the functional square root of $x^2+1$ (9 answers) Closed 9 years ago . let $f(x)=x^2$, then $f(f(x))=x^4$, so $x^4$ is a continuous function from $\Bbb R$ to $\Bbb R$ which can be obtained as $f\circ f$ for a continuous $f\colon \Bbb R\to \Bbb R$. general example: for $f(x)=|x|^{\sqrt{d}}$, $d>0$, we have $(f\circ f)(x)=|x|^d$. I wonder if it is possible to find for example continuous functions $f\colon \Bbb R\to \Bbb R$ such that $f(f(x))=x^2+1$ $f(f(x))=x^3$ $f(f(x))=\sin x$ $f(f(x))=\arcsin x$ $f(f(x))=\ln x$ $f(f(x))=e^x$ Is it known? I don't know which theory can be applied to such problem, I formulated these questions while walking with my dog :-) Are there any general results or counterexamples?","This question already has answers here : On the functional square root of $x^2+1$ (9 answers) Closed 9 years ago . let $f(x)=x^2$, then $f(f(x))=x^4$, so $x^4$ is a continuous function from $\Bbb R$ to $\Bbb R$ which can be obtained as $f\circ f$ for a continuous $f\colon \Bbb R\to \Bbb R$. general example: for $f(x)=|x|^{\sqrt{d}}$, $d>0$, we have $(f\circ f)(x)=|x|^d$. I wonder if it is possible to find for example continuous functions $f\colon \Bbb R\to \Bbb R$ such that $f(f(x))=x^2+1$ $f(f(x))=x^3$ $f(f(x))=\sin x$ $f(f(x))=\arcsin x$ $f(f(x))=\ln x$ $f(f(x))=e^x$ Is it known? I don't know which theory can be applied to such problem, I formulated these questions while walking with my dog :-) Are there any general results or counterexamples?",,"['real-analysis', 'functions', 'function-and-relation-composition']"
22,A limit with the harmonic series,A limit with the harmonic series,,How can we prove the following (similar) limits? $$\sum_{k=1}^n \frac{1}{k} (\ln 2 - \frac{1}{n+2} - \frac{1}{n+3} - \cdots -\frac{1}{2n + 2}) \to 0. $$ $$\sum_{k=1}^n \frac{1}{k} (\ln 3 - \frac{1}{n+2} - \frac{1}{n+3} - \cdots -\frac{1}{3n + 3}) \to 0. $$,How can we prove the following (similar) limits? $$\sum_{k=1}^n \frac{1}{k} (\ln 2 - \frac{1}{n+2} - \frac{1}{n+3} - \cdots -\frac{1}{2n + 2}) \to 0. $$ $$\sum_{k=1}^n \frac{1}{k} (\ln 3 - \frac{1}{n+2} - \frac{1}{n+3} - \cdots -\frac{1}{3n + 3}) \to 0. $$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'harmonic-numbers']"
23,Proving function is $C^k$,Proving function is,C^k,"This question is from an exercise in Way of Analysis (section 10.2.4 problem 20). If $f: \mathbb{R} \rightarrow \mathbb{R}$ is $C^k$ and $f$ is even, then show $F: \mathbb{R}^n \rightarrow \mathbb{R}$ given by $F(x) = f(|x|)$ is also $C^k$. $C^k$ just means the function has $k$ continuous derivatives, and $|x|$ is referring to the regular Euclidean norm. I am not sure how to use the fact $f$ is even in the proof and having trouble for a general $k$. Any hints, even on a particular case like $k=2$, are welcome. My workings: As a comment suggests $f(x) = F(x)$ for $n = 1$. For $n = 2$, $F(x) = f(\sqrt {x_1^2 + x_2^2}).$ I'm stuck on where to proceed from here. Is there some form of induction here that I am not seeing?","This question is from an exercise in Way of Analysis (section 10.2.4 problem 20). If $f: \mathbb{R} \rightarrow \mathbb{R}$ is $C^k$ and $f$ is even, then show $F: \mathbb{R}^n \rightarrow \mathbb{R}$ given by $F(x) = f(|x|)$ is also $C^k$. $C^k$ just means the function has $k$ continuous derivatives, and $|x|$ is referring to the regular Euclidean norm. I am not sure how to use the fact $f$ is even in the proof and having trouble for a general $k$. Any hints, even on a particular case like $k=2$, are welcome. My workings: As a comment suggests $f(x) = F(x)$ for $n = 1$. For $n = 2$, $F(x) = f(\sqrt {x_1^2 + x_2^2}).$ I'm stuck on where to proceed from here. Is there some form of induction here that I am not seeing?",,['real-analysis']
24,"Showing that $P_r(x)=\frac{1-r^2}{1-2r\cos x+r^2}\rightarrow 0$ uniformly on $[-\pi,-\delta]\cup[\delta,\pi]$ as $r\uparrow 1$",Showing that  uniformly on  as,"P_r(x)=\frac{1-r^2}{1-2r\cos x+r^2}\rightarrow 0 [-\pi,-\delta]\cup[\delta,\pi] r\uparrow 1","Let $0<r<1$ and consider the series $$s = \sum_{n=-\infty}^\infty r^{|n|}e^{inx}.$$ I have shown that the series converges uniformely to $$P_r(x)=\frac{1-r^2}{1-2r\cos x+r^2}$$ on all of $\mathbb{R}$. Now I am asked to... Show that for every $0<\delta<\pi$, $P_r(x)\rightarrow 0$ uniformely on the intervals $[-\pi,-\delta,]\cup[\delta,\pi]$ as $r\uparrow 1$. Now, first off I am not entirely sure what this means: In order to talk about uniform convergence, do we not need a sequence of functions? Should I construct such a sequence to work with, e.g. $\{P_{1-\frac{1}{n}}(x)\}_n$? If so, does the problem now become: Given $\epsilon>0$, find $N\in\mathbb{N}$ such that if $n\ge N$ then $|P_{1-\frac{1}{n}}(x)-0|<\epsilon$ on all of $[-\pi,-\delta]\cup[\delta,\pi]$?","Let $0<r<1$ and consider the series $$s = \sum_{n=-\infty}^\infty r^{|n|}e^{inx}.$$ I have shown that the series converges uniformely to $$P_r(x)=\frac{1-r^2}{1-2r\cos x+r^2}$$ on all of $\mathbb{R}$. Now I am asked to... Show that for every $0<\delta<\pi$, $P_r(x)\rightarrow 0$ uniformely on the intervals $[-\pi,-\delta,]\cup[\delta,\pi]$ as $r\uparrow 1$. Now, first off I am not entirely sure what this means: In order to talk about uniform convergence, do we not need a sequence of functions? Should I construct such a sequence to work with, e.g. $\{P_{1-\frac{1}{n}}(x)\}_n$? If so, does the problem now become: Given $\epsilon>0$, find $N\in\mathbb{N}$ such that if $n\ge N$ then $|P_{1-\frac{1}{n}}(x)-0|<\epsilon$ on all of $[-\pi,-\delta]\cup[\delta,\pi]$?",,"['real-analysis', 'fourier-series', 'uniform-convergence', 'harmonic-functions']"
25,Prove that $\int_0^1 \frac{dx}{f^2(x)+1} \le \frac{ \pi}{4}$,Prove that,\int_0^1 \frac{dx}{f^2(x)+1} \le \frac{ \pi}{4},"Let $f:[0,1] \to \mathbb{R}$ be a differentiable function, for which $f'(x) \ge 1 , \forall x\in [0,1]$, and $f(1)=1$. Prove that: $$\int_0^1 \frac{dx}{f^2(x)+1} \le \frac{ \pi}{4}$$ From the hyphotesis, we deduce that $f(0) \le 0$. This doesn't help very much, because if we write $$\int_0^1 \frac{dx}{f^2(x)+1} \le \int_0^1 \frac{f'(x)}{f^2(x)+1} dx =\arctan(f(x))|_0^1=\frac{ \pi }{4}-\arctan(f(0)) \ge \frac{ \pi }{4}$$ Another attempt is: We notice that $ \frac{ \pi }{4} = \int_0^1 \frac{dx}{x^2+1}$, so we only need to prove that $\int_0^1 \frac{dx}{f^2(x)+1} \le \int_0^1 \frac{dx}{x^2+1}$. This can be written as: $$ 0 \le \int_0^1 \frac{(f(x)-x)(f(x)+x)}{(x^2+1)(f^2(x)+1)} dx $$ whichi, I think, isn't very easy to prove. Also, I tried to use that the function $x \to f(x)-x$ is increasing (in fact, I used it when I proved that $f(0) \le 0$), but nothing.","Let $f:[0,1] \to \mathbb{R}$ be a differentiable function, for which $f'(x) \ge 1 , \forall x\in [0,1]$, and $f(1)=1$. Prove that: $$\int_0^1 \frac{dx}{f^2(x)+1} \le \frac{ \pi}{4}$$ From the hyphotesis, we deduce that $f(0) \le 0$. This doesn't help very much, because if we write $$\int_0^1 \frac{dx}{f^2(x)+1} \le \int_0^1 \frac{f'(x)}{f^2(x)+1} dx =\arctan(f(x))|_0^1=\frac{ \pi }{4}-\arctan(f(0)) \ge \frac{ \pi }{4}$$ Another attempt is: We notice that $ \frac{ \pi }{4} = \int_0^1 \frac{dx}{x^2+1}$, so we only need to prove that $\int_0^1 \frac{dx}{f^2(x)+1} \le \int_0^1 \frac{dx}{x^2+1}$. This can be written as: $$ 0 \le \int_0^1 \frac{(f(x)-x)(f(x)+x)}{(x^2+1)(f^2(x)+1)} dx $$ whichi, I think, isn't very easy to prove. Also, I tried to use that the function $x \to f(x)-x$ is increasing (in fact, I used it when I proved that $f(0) \le 0$), but nothing.",,['real-analysis']
26,Showing the set with a $\sup$ has a convergent sequence,Showing the set with a  has a convergent sequence,\sup,"Let $A$ be a set in $\mathbb{R}$, non-empty and bounded above. Prove that $s = \sup A$ if and only if $s$ is an upper bound of $A$ and there exists a sequence $(s_n)$ in $A$ which converges to $s$. I am a little uncertain, given the fact there is no statement suggesting sequences in $A$ are monotone in anyway, so how can we say they are bounded? Next, I realised it says ""there exists a sequence"". So this would mean we need only show that there is at least one sequence in the set $A$ that converges? I started as: $\Longrightarrow$ Assume $ s = \sup A$ Then let $\epsilon> 0 $ be arbitrary, meaning $|s - \epsilon| < s$ is no longer an upper-bound for the set $A$ since, there exists some $s_N \in A$ such that $|s - \epsilon| < s_N \leq s$ Now here is where I run into trouble , since I can't make the statement ""For some $n \geq N, s_n \geq s_N$ such that $s - \epsilon< s_N < s_n \leq s < s+\epsilon$"" I can't make a statement about convergence, since there is no condition that we have an increasing sequence in this set. But since the question says ""there exists a sequence which converges"", am I allowed to simply say ""take some increasing sequence in $A$ that...""? Or am I missing some sort of idea here, and my method/approach is entirely wrong?","Let $A$ be a set in $\mathbb{R}$, non-empty and bounded above. Prove that $s = \sup A$ if and only if $s$ is an upper bound of $A$ and there exists a sequence $(s_n)$ in $A$ which converges to $s$. I am a little uncertain, given the fact there is no statement suggesting sequences in $A$ are monotone in anyway, so how can we say they are bounded? Next, I realised it says ""there exists a sequence"". So this would mean we need only show that there is at least one sequence in the set $A$ that converges? I started as: $\Longrightarrow$ Assume $ s = \sup A$ Then let $\epsilon> 0 $ be arbitrary, meaning $|s - \epsilon| < s$ is no longer an upper-bound for the set $A$ since, there exists some $s_N \in A$ such that $|s - \epsilon| < s_N \leq s$ Now here is where I run into trouble , since I can't make the statement ""For some $n \geq N, s_n \geq s_N$ such that $s - \epsilon< s_N < s_n \leq s < s+\epsilon$"" I can't make a statement about convergence, since there is no condition that we have an increasing sequence in this set. But since the question says ""there exists a sequence which converges"", am I allowed to simply say ""take some increasing sequence in $A$ that...""? Or am I missing some sort of idea here, and my method/approach is entirely wrong?",,"['real-analysis', 'proof-verification']"
27,Extension to the Cantor function,Extension to the Cantor function,,"Prove that $g:C\rightarrow I=[0,1]$ uniquely extends to a continuous monotone increasing map $g:I\rightarrow I$, where $C=\{0,1\}^{\mathbb{N}}$ (;$\{0,1\}$ is equipped with the discrete topology) and $g(\{a_n\})=\sum_{i=1}^{\infty} \frac{a_i}{2^i}$. I guess that the problem wants me to construct the Cantor function, but I don't have any clue to extend $C$ to $I$. Since usually we don't write like $C\subset I$ , it doesn't seem to make sense for me, even though $C$ is homeomorphic to the Cantor set, a subset of $I$. Any help will be appreciated.","Prove that $g:C\rightarrow I=[0,1]$ uniquely extends to a continuous monotone increasing map $g:I\rightarrow I$, where $C=\{0,1\}^{\mathbb{N}}$ (;$\{0,1\}$ is equipped with the discrete topology) and $g(\{a_n\})=\sum_{i=1}^{\infty} \frac{a_i}{2^i}$. I guess that the problem wants me to construct the Cantor function, but I don't have any clue to extend $C$ to $I$. Since usually we don't write like $C\subset I$ , it doesn't seem to make sense for me, even though $C$ is homeomorphic to the Cantor set, a subset of $I$. Any help will be appreciated.",,"['real-analysis', 'general-topology', 'cantor-set']"
28,Convergence of $\sum\limits_{k=2}^\infty \frac{|\sin kx|}{\log k}$,Convergence of,\sum\limits_{k=2}^\infty \frac{|\sin kx|}{\log k},"For what values of $x \in \mathbb{R}$ do the series  $$ \sum\limits_{k=2}^\infty \frac{|\sin kx|}{\log k} $$ converge (and how do you prove the rest diverge)? The series converge trivially at $x = n\pi$ where $n \in \mathbb{Z}$, but I'm not sure about any others.","For what values of $x \in \mathbb{R}$ do the series  $$ \sum\limits_{k=2}^\infty \frac{|\sin kx|}{\log k} $$ converge (and how do you prove the rest diverge)? The series converge trivially at $x = n\pi$ where $n \in \mathbb{Z}$, but I'm not sure about any others.",,"['calculus', 'real-analysis', 'sequences-and-series']"
29,$\lim_{x\to 0}\frac{\sin(x)-x+\frac{x^3}{3!}-\frac{x^5}{5!}}{m x^n}=\frac{8}{7!}$,,\lim_{x\to 0}\frac{\sin(x)-x+\frac{x^3}{3!}-\frac{x^5}{5!}}{m x^n}=\frac{8}{7!},If    $$\lim_{x\to 0}\dfrac{\sin(x)-x+\dfrac{x^3}{3!}-\dfrac{x^5}{5!}}{m x^n}=\dfrac{8}{7!}$$ then find $m+n$: My attempts: note that  $$\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + o(x^7)$$ at  $0$ This makes our limit equal to : $$\lim\limits_{x \to 0} \dfrac{- \dfrac{x^7}{7!} + o(x^7)}{mx^n}$$ Taking $n=7$ then: $$\lim\limits_{x \to 0} \dfrac{- \dfrac{1}{7!}+o(1)}{m}$$ We can take  $m=\dfrac{-1}{8}$. then $m+n=\dfrac{-1}{8}+7$ Am i right Is there any other way,If    $$\lim_{x\to 0}\dfrac{\sin(x)-x+\dfrac{x^3}{3!}-\dfrac{x^5}{5!}}{m x^n}=\dfrac{8}{7!}$$ then find $m+n$: My attempts: note that  $$\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + o(x^7)$$ at  $0$ This makes our limit equal to : $$\lim\limits_{x \to 0} \dfrac{- \dfrac{x^7}{7!} + o(x^7)}{mx^n}$$ Taking $n=7$ then: $$\lim\limits_{x \to 0} \dfrac{- \dfrac{1}{7!}+o(1)}{m}$$ We can take  $m=\dfrac{-1}{8}$. then $m+n=\dfrac{-1}{8}+7$ Am i right Is there any other way,,"['calculus', 'real-analysis', 'limits', 'contest-math']"
30,Suppose given that $ \lim_{x \to 0}f(x)=1 \lim_{x \to 0}g(x)=\infty \lim_{x \to 0}g(x)(f(x)-1)=c $,Suppose given that, \lim_{x \to 0}f(x)=1 \lim_{x \to 0}g(x)=\infty \lim_{x \to 0}g(x)(f(x)-1)=c ,Suppose given that $$ \lim_{x \to 0}f(x)=1 \\ \lim_{x \to 0}g(x)=\infty \\ \lim_{x \to 0}g(x)(f(x)-1)=c  $$ Then find the value of $ \lim_{x \to 0} f(x)^{g(x)} $ So this is what I've done. Please verify if its correct approach @Macavity $$ f(x)^{g(x)} =  e^{\log{f(x)^{g(x)}}} = e^{{g(x)\log f(x)}}  $$  $(1)$ Now from Taylor series expansion of $\log(1+x)$ Substitute $x$ by $f(x)-1 $ in the Taylor series $$\log f(x)=  (f(x)-1) - \frac12(f(x)-1)^2 \cdots$$    $(2)$ From $(1)$ and $(2)$ By taking only the first term of the $\log f(x) $ expansion $$ \lim_{x \to 0} f(x)^{g(x)}= e^c  $$ Is this approach correct?,Suppose given that $$ \lim_{x \to 0}f(x)=1 \\ \lim_{x \to 0}g(x)=\infty \\ \lim_{x \to 0}g(x)(f(x)-1)=c  $$ Then find the value of $ \lim_{x \to 0} f(x)^{g(x)} $ So this is what I've done. Please verify if its correct approach @Macavity $$ f(x)^{g(x)} =  e^{\log{f(x)^{g(x)}}} = e^{{g(x)\log f(x)}}  $$  $(1)$ Now from Taylor series expansion of $\log(1+x)$ Substitute $x$ by $f(x)-1 $ in the Taylor series $$\log f(x)=  (f(x)-1) - \frac12(f(x)-1)^2 \cdots$$    $(2)$ From $(1)$ and $(2)$ By taking only the first term of the $\log f(x) $ expansion $$ \lim_{x \to 0} f(x)^{g(x)}= e^c  $$ Is this approach correct?,,"['calculus', 'real-analysis', 'limits', 'limits-without-lhopital']"
31,Find a countable set with the given property.,Find a countable set with the given property.,,"Given any extended-valued $f$ on $(-\infty,+\infty)$, prove that there exists a countable set $D$ with the following property. For each $t\in\mathbb R$, there exist $t_n\in D$, $t_n\to t$ such that $f(t)=\lim_{n\to\infty}{f(t_n)}$. The assertion remains true if ""$t_n\to t$"" is replaced by ""$t_n\downarrow t$"" or ""$t_n\uparrow t$"". This is an exercise in Kai Lai Chung's A course in Probability Theory , and a hint is given:""Consider the graph $(t,f(t))$ and introduce a metric."" I think it is similar to the fact that $\mathbb R^n$ is separable, but what metric should we define on the graph?","Given any extended-valued $f$ on $(-\infty,+\infty)$, prove that there exists a countable set $D$ with the following property. For each $t\in\mathbb R$, there exist $t_n\in D$, $t_n\to t$ such that $f(t)=\lim_{n\to\infty}{f(t_n)}$. The assertion remains true if ""$t_n\to t$"" is replaced by ""$t_n\downarrow t$"" or ""$t_n\uparrow t$"". This is an exercise in Kai Lai Chung's A course in Probability Theory , and a hint is given:""Consider the graph $(t,f(t))$ and introduce a metric."" I think it is similar to the fact that $\mathbb R^n$ is separable, but what metric should we define on the graph?",,"['real-analysis', 'stochastic-processes', 'metric-spaces']"
32,a countable dense subset of Lipschitz functions,a countable dense subset of Lipschitz functions,,"Suppose $(X,d)$ is a metric space and let $\mathcal{L}$ be the space of bounded Lipschitz functions on $X$. Let $D$ be a countable dense subset of $X$ and consider the set of functions $$h_{q_1,q_2,k,y}(x)=\min\{(q_1+q_2d(x,y)),k\}, \ q_1, q_2, k \in \mathbb{Q}, \ q_2,k\in(0,1), \ y \in D$$ and let $\mathcal{D}$ the set generated by these functions by taking $\inf$ over finitely many of them. Let $\mathcal{C}=\{\lambda f\ | \ \lambda\in\mathbb{Q}, f\in \mathcal{D}\}$. Is it true that $\mathcal{C}$ is dense (with the uniform metric) in $\mathcal{L}$? If so, how can I prove it? Hints or references are also greatly appreciated. UPDATE copper.hat pointed out this is false when $X=\mathbb{R}$. Is it true if $(X,d)$ is compact? (Reference: page 107 of this book: http://www.springer.com/birkhauser/mathematics/book/978-3-7643-8721-1 )","Suppose $(X,d)$ is a metric space and let $\mathcal{L}$ be the space of bounded Lipschitz functions on $X$. Let $D$ be a countable dense subset of $X$ and consider the set of functions $$h_{q_1,q_2,k,y}(x)=\min\{(q_1+q_2d(x,y)),k\}, \ q_1, q_2, k \in \mathbb{Q}, \ q_2,k\in(0,1), \ y \in D$$ and let $\mathcal{D}$ the set generated by these functions by taking $\inf$ over finitely many of them. Let $\mathcal{C}=\{\lambda f\ | \ \lambda\in\mathbb{Q}, f\in \mathcal{D}\}$. Is it true that $\mathcal{C}$ is dense (with the uniform metric) in $\mathcal{L}$? If so, how can I prove it? Hints or references are also greatly appreciated. UPDATE copper.hat pointed out this is false when $X=\mathbb{R}$. Is it true if $(X,d)$ is compact? (Reference: page 107 of this book: http://www.springer.com/birkhauser/mathematics/book/978-3-7643-8721-1 )",,"['real-analysis', 'analysis', 'metric-spaces', 'approximation-theory']"
33,Monotone Class Theorem for Functions,Monotone Class Theorem for Functions,,"Suppose $\mathcal F$ is a collection of real-valued functions on $X$ such that the constant functions are in $\mathcal F$ and $f + g$, $fg$, and $cf$ are in $\mathcal F$ whenever $f, g \in \mathcal F$ and $c \in \mathbb R$. Suppose $f \in \mathcal F$ whenever $f_n \to f$ and each $f_n \in \mathcal F$. Define the function $$\textbf1_A(x) = \begin{cases} 1, & x \in A;\\ 0, & x \notin A. \end{cases}$$ Prove that $\mathcal A = \{A \subseteq X : \textbf 1_A \in \mathcal F\}$ is a $\sigma$-algebra. I have the following result at my disposal: Monotone Class Theorem : Suppose $\mathcal A_0$ is an algebra, $\mathcal A$ is the smallest $\sigma$-algebra containing $\mathcal A_0$, and $\mathcal M$ is the smallest monotone class containing $\mathcal A_0$. Then $\mathcal M = \mathcal A$. Notes : here are some observations I've made: $\emptyset \in \mathcal A$ since $\textbf 1_\emptyset = 0$ is constant and $\mathcal F$ contains all constant functions, similarly $X \in \mathcal A$. If $A \in \mathcal A$, then $\textbf 1_A \in \mathcal F$; notice that $1 - \textbf 1_A = \textbf 1_{A^c} \in \mathcal F$ since finite sums are of elements in $\mathcal F$ are in $\mathcal F$, hence $A^c \in \mathcal A$. Let $A_1, A_2 \in \mathcal A$, then $\textbf 1_{A_1}, \textbf 1_{A_2} \in \mathcal F$. Observe that $\textbf 1_{A_1 \cap A_2} = \textbf 1_{A_1} \cdot \textbf 1_{A_2} \in \mathcal F$. By mathematical induction if $A_1, \ldots, A_n \in \mathcal A$, then $\bigcap_{i = 1}^n A_i \in \mathcal A$. Since we have finite intersections and complements, we get finite unions: $\bigcup_{i = 1}^n A_i \in \mathcal A$. So $\mathcal A$ is an algebra. At this point we can choose to make $\mathcal A_0 := \mathcal A$ our algebra. The problem with showing the result directly comes down to showing if $A_i \in \mathcal A$, then $\bigcup_{i = 1}^\infty A_i \in \mathcal A$. I'm guessing I want to show that $\mathcal M := \mathcal A_0$ is a monotone class. Once we've done this, since $\mathcal A_0 = \mathcal M$, $\mathcal M$ is obviously the smallest monotone class containing $\mathcal A_0$. By the monotone class theorem $\mathcal M = \sigma(\mathcal A_0)$ is a $\sigma$-algebra, correct? ( Question 1 ) So let's try to show that $\mathcal M$ is a monotone class. Let $A_k \in \mathcal M$ with $A_k \nearrow A$. In particular, we have that $A_1 \subseteq A_2 \subseteq \cdots$ and $A := \bigcup_{k = 1}^\infty A_k$. Observe that (by hypothesis), $$\lim_{n \to \infty} \textbf 1_{A_n} = \textbf 1_{\bigcup_{k = 1}^\infty A_k}$$ and hence $\bigcup_{k = 1}^\infty A_k \in \mathcal A$. Is this step valid? It seems obvious, but I'm not sure if I've made an assumption here that I shouldn't have ( Question 2 ) Let $A_k \in \mathcal M$ with $A_k \searrow A$. In particular, we have that $A_1 \supseteq A_2 \supseteq \cdots$ and $A := \bigcap_{k = 1}^\infty A_k$. Observe that (by hypothesis) $$\lim_{n \to \infty} \textbf 1_{A_n} = \textbf 1_{\bigcap_{k = 1}^\infty A_k}$$ and hence $\bigcap_{k = 1}^\infty A_k \in \mathcal A$. If the reasoning in the above two steps is valid about the limit being correct, then can I skip having to use the monotone class theorem and just observe the following? ( Question 3 ) Let $A_k \in \mathcal A$, then $\textbf 1_{A_k} \in \mathcal F$. Define $B_n = \bigcup_{k = 1}^n A_k \in \mathcal A$ since we've already established that it is an algebra and algebras have finite unions. Define $f_n = \textbf 1_{B_n}$ and notice that $\lim_{n \to \infty} f_n = \lim_{n \to \infty} \textbf 1_{B_n} = \textbf 1_{\cup_{k = 1}^\infty A_k} \in \mathcal F$ by hypothesis. Hence $\bigcup_{k = 1}^\infty A_k \in \mathcal A$. Conclude by definition that $\mathcal A$ is a $\sigma$-algebra. If my reasoning fails somewhere, I would appreciate any hints/full proofs that establish the result using the monotone class theorem, if just because I feel I lack understanding of how to use it and I think this is a good example where it's usable.","Suppose $\mathcal F$ is a collection of real-valued functions on $X$ such that the constant functions are in $\mathcal F$ and $f + g$, $fg$, and $cf$ are in $\mathcal F$ whenever $f, g \in \mathcal F$ and $c \in \mathbb R$. Suppose $f \in \mathcal F$ whenever $f_n \to f$ and each $f_n \in \mathcal F$. Define the function $$\textbf1_A(x) = \begin{cases} 1, & x \in A;\\ 0, & x \notin A. \end{cases}$$ Prove that $\mathcal A = \{A \subseteq X : \textbf 1_A \in \mathcal F\}$ is a $\sigma$-algebra. I have the following result at my disposal: Monotone Class Theorem : Suppose $\mathcal A_0$ is an algebra, $\mathcal A$ is the smallest $\sigma$-algebra containing $\mathcal A_0$, and $\mathcal M$ is the smallest monotone class containing $\mathcal A_0$. Then $\mathcal M = \mathcal A$. Notes : here are some observations I've made: $\emptyset \in \mathcal A$ since $\textbf 1_\emptyset = 0$ is constant and $\mathcal F$ contains all constant functions, similarly $X \in \mathcal A$. If $A \in \mathcal A$, then $\textbf 1_A \in \mathcal F$; notice that $1 - \textbf 1_A = \textbf 1_{A^c} \in \mathcal F$ since finite sums are of elements in $\mathcal F$ are in $\mathcal F$, hence $A^c \in \mathcal A$. Let $A_1, A_2 \in \mathcal A$, then $\textbf 1_{A_1}, \textbf 1_{A_2} \in \mathcal F$. Observe that $\textbf 1_{A_1 \cap A_2} = \textbf 1_{A_1} \cdot \textbf 1_{A_2} \in \mathcal F$. By mathematical induction if $A_1, \ldots, A_n \in \mathcal A$, then $\bigcap_{i = 1}^n A_i \in \mathcal A$. Since we have finite intersections and complements, we get finite unions: $\bigcup_{i = 1}^n A_i \in \mathcal A$. So $\mathcal A$ is an algebra. At this point we can choose to make $\mathcal A_0 := \mathcal A$ our algebra. The problem with showing the result directly comes down to showing if $A_i \in \mathcal A$, then $\bigcup_{i = 1}^\infty A_i \in \mathcal A$. I'm guessing I want to show that $\mathcal M := \mathcal A_0$ is a monotone class. Once we've done this, since $\mathcal A_0 = \mathcal M$, $\mathcal M$ is obviously the smallest monotone class containing $\mathcal A_0$. By the monotone class theorem $\mathcal M = \sigma(\mathcal A_0)$ is a $\sigma$-algebra, correct? ( Question 1 ) So let's try to show that $\mathcal M$ is a monotone class. Let $A_k \in \mathcal M$ with $A_k \nearrow A$. In particular, we have that $A_1 \subseteq A_2 \subseteq \cdots$ and $A := \bigcup_{k = 1}^\infty A_k$. Observe that (by hypothesis), $$\lim_{n \to \infty} \textbf 1_{A_n} = \textbf 1_{\bigcup_{k = 1}^\infty A_k}$$ and hence $\bigcup_{k = 1}^\infty A_k \in \mathcal A$. Is this step valid? It seems obvious, but I'm not sure if I've made an assumption here that I shouldn't have ( Question 2 ) Let $A_k \in \mathcal M$ with $A_k \searrow A$. In particular, we have that $A_1 \supseteq A_2 \supseteq \cdots$ and $A := \bigcap_{k = 1}^\infty A_k$. Observe that (by hypothesis) $$\lim_{n \to \infty} \textbf 1_{A_n} = \textbf 1_{\bigcap_{k = 1}^\infty A_k}$$ and hence $\bigcap_{k = 1}^\infty A_k \in \mathcal A$. If the reasoning in the above two steps is valid about the limit being correct, then can I skip having to use the monotone class theorem and just observe the following? ( Question 3 ) Let $A_k \in \mathcal A$, then $\textbf 1_{A_k} \in \mathcal F$. Define $B_n = \bigcup_{k = 1}^n A_k \in \mathcal A$ since we've already established that it is an algebra and algebras have finite unions. Define $f_n = \textbf 1_{B_n}$ and notice that $\lim_{n \to \infty} f_n = \lim_{n \to \infty} \textbf 1_{B_n} = \textbf 1_{\cup_{k = 1}^\infty A_k} \in \mathcal F$ by hypothesis. Hence $\bigcup_{k = 1}^\infty A_k \in \mathcal A$. Conclude by definition that $\mathcal A$ is a $\sigma$-algebra. If my reasoning fails somewhere, I would appreciate any hints/full proofs that establish the result using the monotone class theorem, if just because I feel I lack understanding of how to use it and I think this is a good example where it's usable.",,"['real-analysis', 'measure-theory', 'monotone-class-theorem']"
34,Riemann Integrability in $\Bbb R^2$,Riemann Integrability in,\Bbb R^2,"Define the General Subdivision $S$ of a rectangle $R$ in $\Bbb R^2$ as a collection $E_1,...,E_k$ of Jordan regions such that none of them has interior points in common, and: $$R \subset \bigcup_{i=1}^k E_i$$ The norm of $S$ is defined as: $$d(S)=\max \left[\operatorname{diam}(E_i:1<i<k)\right]$$ Where $diam$ denotes denotes the diameter of each rectangle. Also, if $f$ is a continuous in $R$ and for each $i$, $(x_i, y_i)$ is in $E_i$ then the sum $$S(f,S,(x_i,y_i))= \sum_{i=i}^k f(x_i,y_i)  \operatorname{vol}(E_i) $$ is named the Riemann sum of $f$ in S . I need to show that for every $\epsilon >0$ there is a $\delta >0$ such that if $d(S)<\delta$ then: $$\left| \iint_R f \ dA -S(f,S,(x_i,y_i)) \right| < \epsilon $$ The definition of integral I've been studying is given by the superior and inferior sums: $$\iint_R f \ dA= \sup \{L(f,P) \mid \text {P is a partition of } \mathbb{R}\}= \inf \{ U(f,P) \mid \text {P is a partition of } \mathbb{R} \}$$ I tried using the Cauchy Criterion but i couldn't prove what I need. Any help will be great, thanks in advance.","Define the General Subdivision $S$ of a rectangle $R$ in $\Bbb R^2$ as a collection $E_1,...,E_k$ of Jordan regions such that none of them has interior points in common, and: $$R \subset \bigcup_{i=1}^k E_i$$ The norm of $S$ is defined as: $$d(S)=\max \left[\operatorname{diam}(E_i:1<i<k)\right]$$ Where $diam$ denotes denotes the diameter of each rectangle. Also, if $f$ is a continuous in $R$ and for each $i$, $(x_i, y_i)$ is in $E_i$ then the sum $$S(f,S,(x_i,y_i))= \sum_{i=i}^k f(x_i,y_i)  \operatorname{vol}(E_i) $$ is named the Riemann sum of $f$ in S . I need to show that for every $\epsilon >0$ there is a $\delta >0$ such that if $d(S)<\delta$ then: $$\left| \iint_R f \ dA -S(f,S,(x_i,y_i)) \right| < \epsilon $$ The definition of integral I've been studying is given by the superior and inferior sums: $$\iint_R f \ dA= \sup \{L(f,P) \mid \text {P is a partition of } \mathbb{R}\}= \inf \{ U(f,P) \mid \text {P is a partition of } \mathbb{R} \}$$ I tried using the Cauchy Criterion but i couldn't prove what I need. Any help will be great, thanks in advance.",,"['real-analysis', 'integration', 'multivariable-calculus', 'riemann-sum']"
35,Question regarding Radon-Nikodym derivative...,Question regarding Radon-Nikodym derivative...,,"The problems are as follows: (1) Let $X=[0,1]$ with Lebesuge measure and consider probability measures $\nu,\mu$ given by densities $f,g$ as follows:   $$\nu(E)=\int_{E} f\;dm\;\;and\;\;\mu(E)=\int_{E}g\;dm$$   $\forall E \subset [0,1]$ s.t $E$ measurable. Suppose $f(x),g(x)>0$ $\forall x\in [0,1]$. Is $\nu$ absolutely continuos w.rt to $\mu$? If it is, determine the Radon-Nikodym derivative $\frac{d\nu}{d\mu}$. Is $\mu$ absolutely continuos w.r.t to $\nu$? (2) For a point $x$, define the Dirac measure $\delta_x$ to be   $$\delta_x=\chi_{A}$$   where $\chi_{A}$ is the indicator function over set A. For a fixed set $B$, define the Lebesgue measure restricted to $B$ by $m_B(A)=m(A \cap B)$. Let $\mu=\delta_1 + m_{[2,4]}$ and $\nu=\delta_0+m_{(1,2)}$. Show that $\nu$ is singular to $\mu$. So, here's what I've done: 1) Since $f(x),g(x)>0$ $\forall x \in [0,1]$, it follows that   $$\nu(E),\mu(E)>0 \iff m(E)>0$$   and   $$\nu(E),\mu(E)=0 \iff m(E)=0$$   $$\implies \nu(E) << \mu(E)\;\;\;,\;\;\; \mu(E) << \nu(E)$$ We wish to find $\frac{d\nu}{d\mu}$. Having $f=\frac{d\nu}{dm}$ and $g=\frac{d\mu}{dm}$, and then dividing $f$ by $g$, we obtain   $$\frac{\frac{d\nu}{dm}}{\frac{d\mu}{dm}}=\frac{f}{g}$$   $$\implies \frac{d\nu}{d\mu}=\frac{f(x)}{g(x)}$$   $\square$ (2) Have   $$\mu=\delta_1 + m_{[2,4]}\;\;,\;\;\nu=\delta_0+m_{(1,2)}$$   $$\implies \mu(A)=\delta_1(A)+m_{[2,4]}(A)=\delta_1(A) + m(A\cap [2,4])$$   $$\nu(A)=\delta_0(A)+m_{(1,2)}(A)=\delta_0(A)+m(A\cap(1,2))$$   Notice that at each delta, we're dealing with a single point. In particular, the singletons $\{1\}=[1,1]$ and $\{0\}=[0,0]$. Thus,   $$\mu(A)=\delta_1(A) + m(A\cap [2,4])=\mu(A\cap([1,1]\cup [2,4]))$$   $$\nu(A)=\delta_0(A)+m(A\cap(1,2))=\nu(A\cap([0,0]\cup (1,2)))$$   Since $(A\cap([1,1]\cup [2,4])\cap(A\cap([0,0]\cup (1,2)))=\emptyset$, it follows that $\mu$ and $\nu$ are singular to one another. $\square$ Did I pull some nonsense? Radon and I have not been getting along so well, so I've had trouble properly wrapping my head around these findings. If I've gone down the wrong path, how I would I go about solving these problems?","The problems are as follows: (1) Let $X=[0,1]$ with Lebesuge measure and consider probability measures $\nu,\mu$ given by densities $f,g$ as follows:   $$\nu(E)=\int_{E} f\;dm\;\;and\;\;\mu(E)=\int_{E}g\;dm$$   $\forall E \subset [0,1]$ s.t $E$ measurable. Suppose $f(x),g(x)>0$ $\forall x\in [0,1]$. Is $\nu$ absolutely continuos w.rt to $\mu$? If it is, determine the Radon-Nikodym derivative $\frac{d\nu}{d\mu}$. Is $\mu$ absolutely continuos w.r.t to $\nu$? (2) For a point $x$, define the Dirac measure $\delta_x$ to be   $$\delta_x=\chi_{A}$$   where $\chi_{A}$ is the indicator function over set A. For a fixed set $B$, define the Lebesgue measure restricted to $B$ by $m_B(A)=m(A \cap B)$. Let $\mu=\delta_1 + m_{[2,4]}$ and $\nu=\delta_0+m_{(1,2)}$. Show that $\nu$ is singular to $\mu$. So, here's what I've done: 1) Since $f(x),g(x)>0$ $\forall x \in [0,1]$, it follows that   $$\nu(E),\mu(E)>0 \iff m(E)>0$$   and   $$\nu(E),\mu(E)=0 \iff m(E)=0$$   $$\implies \nu(E) << \mu(E)\;\;\;,\;\;\; \mu(E) << \nu(E)$$ We wish to find $\frac{d\nu}{d\mu}$. Having $f=\frac{d\nu}{dm}$ and $g=\frac{d\mu}{dm}$, and then dividing $f$ by $g$, we obtain   $$\frac{\frac{d\nu}{dm}}{\frac{d\mu}{dm}}=\frac{f}{g}$$   $$\implies \frac{d\nu}{d\mu}=\frac{f(x)}{g(x)}$$   $\square$ (2) Have   $$\mu=\delta_1 + m_{[2,4]}\;\;,\;\;\nu=\delta_0+m_{(1,2)}$$   $$\implies \mu(A)=\delta_1(A)+m_{[2,4]}(A)=\delta_1(A) + m(A\cap [2,4])$$   $$\nu(A)=\delta_0(A)+m_{(1,2)}(A)=\delta_0(A)+m(A\cap(1,2))$$   Notice that at each delta, we're dealing with a single point. In particular, the singletons $\{1\}=[1,1]$ and $\{0\}=[0,0]$. Thus,   $$\mu(A)=\delta_1(A) + m(A\cap [2,4])=\mu(A\cap([1,1]\cup [2,4]))$$   $$\nu(A)=\delta_0(A)+m(A\cap(1,2))=\nu(A\cap([0,0]\cup (1,2)))$$   Since $(A\cap([1,1]\cup [2,4])\cap(A\cap([0,0]\cup (1,2)))=\emptyset$, it follows that $\mu$ and $\nu$ are singular to one another. $\square$ Did I pull some nonsense? Radon and I have not been getting along so well, so I've had trouble properly wrapping my head around these findings. If I've gone down the wrong path, how I would I go about solving these problems?",,"['real-analysis', 'measure-theory', 'proof-verification', 'lebesgue-measure']"
36,"Convergence of monotone $f_n:[0, \infty) \rightarrow [0,1]$ to continuous, monotone $g$ is uniform","Convergence of monotone  to continuous, monotone  is uniform","f_n:[0, \infty) \rightarrow [0,1] g","Let $g: [0, \infty) \rightarrow [0,1]$ be a continuous, monotone increasing function where $g(0)=0$ and $g(x)\rightarrow 1$ as $x \rightarrow \infty$. Also let $f_n:[0, \infty) \rightarrow [0,1]$ be a monotone increasing function for each $n \geq 1$ (not necessarily continuous). I want to prove that if $f_n \rightarrow g$ pointwise, then $f_n \rightarrow g$ uniformly on $[0, \infty)$. I thought separating into two cases might be useful. Since $g: [0, \infty) \rightarrow [0,1]$, for any given $\epsilon >0$, there exists $M$ such that $\vert g(x) - 1 \vert < \epsilon$ for all $x > M$. Then $\sup \vert f_n (x) - g(x) \vert = \vert f_n(M) -g(M) \vert  \rightarrow 0$ as $n \rightarrow \infty$. Does this prove the uniform convergence on $[M, \infty)$? For the convergence on $[0,M]$, I cannot apply Dini's Theorem since $f_n$ are not assumed to be continuous. But $g$ is uniform continuous on $[0,M]$. Maybe this helps but I cannot see it.","Let $g: [0, \infty) \rightarrow [0,1]$ be a continuous, monotone increasing function where $g(0)=0$ and $g(x)\rightarrow 1$ as $x \rightarrow \infty$. Also let $f_n:[0, \infty) \rightarrow [0,1]$ be a monotone increasing function for each $n \geq 1$ (not necessarily continuous). I want to prove that if $f_n \rightarrow g$ pointwise, then $f_n \rightarrow g$ uniformly on $[0, \infty)$. I thought separating into two cases might be useful. Since $g: [0, \infty) \rightarrow [0,1]$, for any given $\epsilon >0$, there exists $M$ such that $\vert g(x) - 1 \vert < \epsilon$ for all $x > M$. Then $\sup \vert f_n (x) - g(x) \vert = \vert f_n(M) -g(M) \vert  \rightarrow 0$ as $n \rightarrow \infty$. Does this prove the uniform convergence on $[M, \infty)$? For the convergence on $[0,M]$, I cannot apply Dini's Theorem since $f_n$ are not assumed to be continuous. But $g$ is uniform continuous on $[0,M]$. Maybe this helps but I cannot see it.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
37,"orthonormal sequence in $L^2[0,1]$ - how to prove these following equivalent terms?",orthonormal sequence in  - how to prove these following equivalent terms?,"L^2[0,1]","I've been asked this following very interesting question and would like some help figuring out why it is true :) Let $u_n$ be an orthonormal sequence in $L^2[0,1]$ Prove that the following are equivalent: $u_n$ is complete (orthonormal basis) for every $x\in [0,1]$ : $x=\sum_{n=1}^{\infty} |\int_{0}^{x} u_n(t)dt|^2 $ $0.5=\sum_{n=1}^{\infty} \int_{0}^{1} |\int_{0}^{x} u_n(t)dt|^2 dx $ My thoughts so far: a. if $u_n$ is complete, I could use Parseval's identity with functions $f=1_{[0,x]}$ and get condition 2, and integrate it to get condition 3 ( p.s: why could I change the order of the inifite sum with that $\int_{0}^{1}$? ) b. The other way aronud is harder. I figured out that using the definition of orthonormal basis would be a little difficult, So probably I'm supposed to show the Parsevals' identity, or something like that. no luck there. I would like some guide if you could. Thanks! Edit: I tried this question for a couple more hours and couldn't solve. A more direct hint would be appreciated.","I've been asked this following very interesting question and would like some help figuring out why it is true :) Let $u_n$ be an orthonormal sequence in $L^2[0,1]$ Prove that the following are equivalent: $u_n$ is complete (orthonormal basis) for every $x\in [0,1]$ : $x=\sum_{n=1}^{\infty} |\int_{0}^{x} u_n(t)dt|^2 $ $0.5=\sum_{n=1}^{\infty} \int_{0}^{1} |\int_{0}^{x} u_n(t)dt|^2 dx $ My thoughts so far: a. if $u_n$ is complete, I could use Parseval's identity with functions $f=1_{[0,x]}$ and get condition 2, and integrate it to get condition 3 ( p.s: why could I change the order of the inifite sum with that $\int_{0}^{1}$? ) b. The other way aronud is harder. I figured out that using the definition of orthonormal basis would be a little difficult, So probably I'm supposed to show the Parsevals' identity, or something like that. no luck there. I would like some guide if you could. Thanks! Edit: I tried this question for a couple more hours and couldn't solve. A more direct hint would be appreciated.",,"['real-analysis', 'functional-analysis']"
38,Using the Inverse Function Theorem prove that $(\sin^{-1}x)'$ = $\frac{1}{\sqrt{1-x^2}}$.,Using the Inverse Function Theorem prove that  = .,(\sin^{-1}x)' \frac{1}{\sqrt{1-x^2}},"Using the Inverse Function Theorem prove that $(\sin^{-1}x)'$ = $\frac{1}{\sqrt{1-x^2}}$. Proof: Let $f(x) = \sin x$, for $x$ in $(-1,1)$.  Then let $x_{0}$ be in (-1,1). Then $f'(x_{0})$ = $\cos(x_{0})\neq 0$, where $f'(x_{0})$ $> 0$. So $f$ is increasing thus one to one. Then using the Inverse Function Theorem,  $(f^{-1})'(y_{0})$ = $\frac{1}{\cos(\sin^{-1}y_{0})}$ = $sec(x_{0})$ = $\frac{1}{\sqrt{1-y_{0}^2}}$ . Is this a a correct way to prove it? Please any suggestion/feedback would be appreciated. And can someone please verify the intervals are correct since I am working with inverses. Thank you very much.","Using the Inverse Function Theorem prove that $(\sin^{-1}x)'$ = $\frac{1}{\sqrt{1-x^2}}$. Proof: Let $f(x) = \sin x$, for $x$ in $(-1,1)$.  Then let $x_{0}$ be in (-1,1). Then $f'(x_{0})$ = $\cos(x_{0})\neq 0$, where $f'(x_{0})$ $> 0$. So $f$ is increasing thus one to one. Then using the Inverse Function Theorem,  $(f^{-1})'(y_{0})$ = $\frac{1}{\cos(\sin^{-1}y_{0})}$ = $sec(x_{0})$ = $\frac{1}{\sqrt{1-y_{0}^2}}$ . Is this a a correct way to prove it? Please any suggestion/feedback would be appreciated. And can someone please verify the intervals are correct since I am working with inverses. Thank you very much.",,"['real-analysis', 'derivatives', 'continuity', 'inverse']"
39,An argument from a blog article of Terence Tao,An argument from a blog article of Terence Tao,,"Let $A_1, A_2, A_3, \ldots , A_m$ be positive semi-definite Hermitian matrices and then consider the polynomial $p(z,z_1,z_2,\ldots,z_m) = \det(z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)$ Now Tao argues that if $z,z_1,z_2,\ldots,z_m$ have a positive imaginary part then the ""skew-adjoint part"" (what is this?) of $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m $ is strictly positive definite and hence the quadratic form $\operatorname{Im} [ \langle (z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)v, v \rangle   ]$ is non-degenerate and hence it follows that $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m$ is non-singular. Can someone kindly help understand what happened here?","Let $A_1, A_2, A_3, \ldots , A_m$ be positive semi-definite Hermitian matrices and then consider the polynomial $p(z,z_1,z_2,\ldots,z_m) = \det(z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)$ Now Tao argues that if $z,z_1,z_2,\ldots,z_m$ have a positive imaginary part then the ""skew-adjoint part"" (what is this?) of $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m $ is strictly positive definite and hence the quadratic form $\operatorname{Im} [ \langle (z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)v, v \rangle   ]$ is non-degenerate and hence it follows that $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m$ is non-singular. Can someone kindly help understand what happened here?",,"['real-analysis', 'linear-algebra', 'complex-analysis', 'functional-analysis', 'quadratic-forms']"
40,Is this partial converse of the excision property for outer measure true?,Is this partial converse of the excision property for outer measure true?,,"From here on out let us assume $A \subsetneq B$ and that $B$ is measurable. The excision property states that if $A$ is measurable and of finite measure, then $$m^*(B-A)=m^*(B)-m^*(A)\,\,\,\,(*)$$ I am interested in the converse (which I approach by contraposition). Question: If $A$ is not measurable, does $(*)$ fail for all measurable $B$ or just for some measurable $B$? The excision property relies on the fact that $m^*(B)=m^*(B\cap A) + m^*(B-A)$ for any set $B$ when $A$ is measurable. I know it must fail for some $B$ or else $A$ would be measurable, but I would like to conclude that $(*)$ fails for all $B$ when $A$ is a nonmeasurable subset of $B$. What have I tried? Well I don't have any concrete examples of nonmeasurable sets in the sense that I don't know the outer measure of any nonmeasurable set. All I know is that they exists and how to make them, but since I rely on the axiom of choice to make such a set, I can't seem to do anything with it.","From here on out let us assume $A \subsetneq B$ and that $B$ is measurable. The excision property states that if $A$ is measurable and of finite measure, then $$m^*(B-A)=m^*(B)-m^*(A)\,\,\,\,(*)$$ I am interested in the converse (which I approach by contraposition). Question: If $A$ is not measurable, does $(*)$ fail for all measurable $B$ or just for some measurable $B$? The excision property relies on the fact that $m^*(B)=m^*(B\cap A) + m^*(B-A)$ for any set $B$ when $A$ is measurable. I know it must fail for some $B$ or else $A$ would be measurable, but I would like to conclude that $(*)$ fails for all $B$ when $A$ is a nonmeasurable subset of $B$. What have I tried? Well I don't have any concrete examples of nonmeasurable sets in the sense that I don't know the outer measure of any nonmeasurable set. All I know is that they exists and how to make them, but since I rely on the axiom of choice to make such a set, I can't seem to do anything with it.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
41,"Prove that f is uniformly continuous on [0, ∞].","Prove that f is uniformly continuous on [0, ∞].",,"Exercise: Suppose that f:[0, ∞] → R is continuous, and that there is an real value L, such that f(x) → L as x → ∞. Prove that f is uniformly continuous on [0, ∞]. Attempt: I will try to use the definition and theorem. Definition:  a function f: E → R is uniformly continuous iff for every ε > 0, there is a δ > 0 such that |x-a| < δ and x,a are elements in E implies |f(x) - f(a)| < ε. Theorem: Suppose a < b and that f: (a,b) → R. Then f is uniformly continuous on (a,b) iff f can be continuously extended to [a,b]; that is iff there is a continuous function g: [a,b] → R such that f(x) = g(x), and x is in (a,b). Then, suppose that f:[0, ∞] → R, and that there is an real value L, such that f(x) → L as x → ∞.  Then let ε > 0. Then since [0,∞), we can try to make it of the form [0,N]. So it can look as the theorem. Suppose x >= N so that |f(x) - L | < ε. Then there is δ > 0 such that |x-a| < δ, and x,a are in [0,N], so that |f(x) - f(a)| < ε. Can someone please help me? I don't know how to continue. Thank you in advance.","Exercise: Suppose that f:[0, ∞] → R is continuous, and that there is an real value L, such that f(x) → L as x → ∞. Prove that f is uniformly continuous on [0, ∞]. Attempt: I will try to use the definition and theorem. Definition:  a function f: E → R is uniformly continuous iff for every ε > 0, there is a δ > 0 such that |x-a| < δ and x,a are elements in E implies |f(x) - f(a)| < ε. Theorem: Suppose a < b and that f: (a,b) → R. Then f is uniformly continuous on (a,b) iff f can be continuously extended to [a,b]; that is iff there is a continuous function g: [a,b] → R such that f(x) = g(x), and x is in (a,b). Then, suppose that f:[0, ∞] → R, and that there is an real value L, such that f(x) → L as x → ∞.  Then let ε > 0. Then since [0,∞), we can try to make it of the form [0,N]. So it can look as the theorem. Suppose x >= N so that |f(x) - L | < ε. Then there is δ > 0 such that |x-a| < δ, and x,a are in [0,N], so that |f(x) - f(a)| < ε. Can someone please help me? I don't know how to continue. Thank you in advance.",,"['real-analysis', 'limits', 'uniform-continuity']"
42,Deriving the analytical properties of the logarithm from an algebraic definition.,Deriving the analytical properties of the logarithm from an algebraic definition.,,"Definition: The base $a$ logarithm ($a\in]0,1[\cup]1,+\infty[$) is the continuous function defined by: $\log_a(xy)=\log_a(x)+\log_a(y)~~\forall x,y>0$ and $\log_a(a)=1$ If I used this definition of the logarithm how can deduce the derivative of $\log_a(x)$ and $\lim\limits_{x\to +\infty} \log_a(x)$? Is the continuity necessary in the definition or should I replace it with another property (weaker or stronger) like uniform continuity, differentiability, Holder-continuity, etc.","Definition: The base $a$ logarithm ($a\in]0,1[\cup]1,+\infty[$) is the continuous function defined by: $\log_a(xy)=\log_a(x)+\log_a(y)~~\forall x,y>0$ and $\log_a(a)=1$ If I used this definition of the logarithm how can deduce the derivative of $\log_a(x)$ and $\lim\limits_{x\to +\infty} \log_a(x)$? Is the continuity necessary in the definition or should I replace it with another property (weaker or stronger) like uniform continuity, differentiability, Holder-continuity, etc.",,"['real-analysis', 'logarithms', 'definition']"
43,Is there any method to get a finite sum for $1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots$?,Is there any method to get a finite sum for ?,1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots,"As we can see on Wikipedia , there are some algebraic methods that give us finite sums for the Grandi's series $$1-1+1-1+1-1+1-1+\cdots$$ Let $S$ be the sum of the Grandi's series. Then $S=(1-1)+(1-1)+\cdots=0+0+\cdots=0$ $S=1+(-1+1)+(-1+1)+\cdots=1+0+0+\cdots=1$ $1-S=S$ so that $S=1/2$ The algebraic manipulations above are not allowed because the Grandi's series doesn't converges. Is there something similar related to the harmonic series? In other words, is there any ""incorrect algebraic way"" to get a finite sum for $$1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots \;?$$ Thanks.","As we can see on Wikipedia , there are some algebraic methods that give us finite sums for the Grandi's series $$1-1+1-1+1-1+1-1+\cdots$$ Let $S$ be the sum of the Grandi's series. Then $S=(1-1)+(1-1)+\cdots=0+0+\cdots=0$ $S=1+(-1+1)+(-1+1)+\cdots=1+0+0+\cdots=1$ $1-S=S$ so that $S=1/2$ The algebraic manipulations above are not allowed because the Grandi's series doesn't converges. Is there something similar related to the harmonic series? In other words, is there any ""incorrect algebraic way"" to get a finite sum for $$1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots \;?$$ Thanks.",,"['calculus', 'real-analysis', 'sequences-and-series', 'divergent-series']"
44,Is Banach space a correct context to study sequences and series?,Is Banach space a correct context to study sequences and series?,,"Recently, I have reviewed elementary analysis and I realized that every theorem in the text(Rudin-PMA) about series can be generalized to Banach space. Here is an example. Below is the theorem stated in the text: Let $\sum a_n,  \sum b_n$ be convergent series in $\mathbb{C}$ . If $\sum a_n$ is absolutely convergent, then $\sum \sum_{k=0}^n a_k b_{n-k} = \sum a_n \sum b_n$ . I realized that this theorem can be generalized in the context of Banach space. That is, Let $(V,\| \cdot \|)$ be a Banach space over $\mathbb{K}$ Let $\sum v_n$ be a convergent series in $V$ Let $\sum c_n$ be a convergent series in $\mathbb{K}$ . If one of those series is absolutely convergent, then $\sum \sum_{k=0}^n c_k v_{n-k} = \sum c_n \sum v_n$ . Just like the example, i found that ratio test, comparison test, Drichlet test etc. can be generalized into Banach space. Is Banach space the right generalization to study series or is there another well-known context to study series?","Recently, I have reviewed elementary analysis and I realized that every theorem in the text(Rudin-PMA) about series can be generalized to Banach space. Here is an example. Below is the theorem stated in the text: Let be convergent series in . If is absolutely convergent, then . I realized that this theorem can be generalized in the context of Banach space. That is, Let be a Banach space over Let be a convergent series in Let be a convergent series in . If one of those series is absolutely convergent, then . Just like the example, i found that ratio test, comparison test, Drichlet test etc. can be generalized into Banach space. Is Banach space the right generalization to study series or is there another well-known context to study series?","\sum a_n,  \sum b_n \mathbb{C} \sum a_n \sum \sum_{k=0}^n a_k b_{n-k} = \sum a_n \sum b_n (V,\| \cdot \|) \mathbb{K} \sum v_n V \sum c_n \mathbb{K} \sum \sum_{k=0}^n c_k v_{n-k} = \sum c_n \sum v_n","['real-analysis', 'sequences-and-series', 'banach-spaces']"
45,How to show $ \left(\frac{1-x}{2}\right)^p+\left(\frac{1+x}{2}\right)^p \leq \frac{1+x^p}{2}$ [duplicate],How to show  [duplicate], \left(\frac{1-x}{2}\right)^p+\left(\frac{1+x}{2}\right)^p \leq \frac{1+x^p}{2},"This question already has an answer here : Showing $\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p$ (1 answer) Closed 9 years ago . When $p\geq 2$ and $0\leq x\leq1$, how does one show the inequalities $$ \left(\frac{1-x}{2}\right)^p+\left(\frac{1+x}{2}\right)^p \leq \frac{1+x^p}{2}$$ and $$ 2(1+x^p)\leq (1+x)^p + (1-x)^p \ ?$$ The first one looks like a correction of the parallelogram law for powers greater than $p=2$, I've tried using that and rearranging but it hasn't worked. I've also tried differentiating but then I don't know how to handle the new expressions. This is from a qualification exam on analysis. Basically the problem becomes trivial if one shows the inequalities. Thank you!","This question already has an answer here : Showing $\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p$ (1 answer) Closed 9 years ago . When $p\geq 2$ and $0\leq x\leq1$, how does one show the inequalities $$ \left(\frac{1-x}{2}\right)^p+\left(\frac{1+x}{2}\right)^p \leq \frac{1+x^p}{2}$$ and $$ 2(1+x^p)\leq (1+x)^p + (1-x)^p \ ?$$ The first one looks like a correction of the parallelogram law for powers greater than $p=2$, I've tried using that and rearranging but it hasn't worked. I've also tried differentiating but then I don't know how to handle the new expressions. This is from a qualification exam on analysis. Basically the problem becomes trivial if one shows the inequalities. Thank you!",,"['real-analysis', 'inequality']"
46,Exchange the order of the two limits,Exchange the order of the two limits,,"Suppose both limits exist, when is it true that $$\lim_{x\to a}\lim_{y\to b} f(x,y) = \lim_{y\to b}\lim_{x\to a} f(x,y) ?$$ and further when is it true that these two limits are equal to $\lim\limits_{(x,y)\to(a,b)} f(x,y)$, assuming the latter exists? Since we are not dealing with sequences of functions, most convergence theorems don't work here.","Suppose both limits exist, when is it true that $$\lim_{x\to a}\lim_{y\to b} f(x,y) = \lim_{y\to b}\lim_{x\to a} f(x,y) ?$$ and further when is it true that these two limits are equal to $\lim\limits_{(x,y)\to(a,b)} f(x,y)$, assuming the latter exists? Since we are not dealing with sequences of functions, most convergence theorems don't work here.",,"['real-analysis', 'limits', 'multivariable-calculus']"
47,Calculating limit in parts. Why possible?,Calculating limit in parts. Why possible?,,"Let $f$, continuous function, differentiable at $x=1$ and $f(1)>0$. Consider the following equation: $$\lim \limits_{x\to 1} \left(\frac{\color{Blue}{f(x)-f(1)}(x-1)}{\color{Blue}{(x-1)}f(1)}\right)^\frac{1}{\log x} = \lim \limits_{x\to 1} \left( \frac{\color {Blue}{f'(1)}(x-1)}{f(1)} \right)^\frac{1}{\log x}$$ My question is: Why can you evaluate the blue expression on the LHS as $f'(1)$ before taking the limit? Moreover, the whole expression is powered by $\frac{1}{\log x}$ It seems to me like claiming $$\lim\limits_{n\to\infty}(1+\frac{1}{n})^n = \lim\limits_{n\to\infty}(1+0)^n = 1$$ So what's making the first valid whereas the second is nonsense.","Let $f$, continuous function, differentiable at $x=1$ and $f(1)>0$. Consider the following equation: $$\lim \limits_{x\to 1} \left(\frac{\color{Blue}{f(x)-f(1)}(x-1)}{\color{Blue}{(x-1)}f(1)}\right)^\frac{1}{\log x} = \lim \limits_{x\to 1} \left( \frac{\color {Blue}{f'(1)}(x-1)}{f(1)} \right)^\frac{1}{\log x}$$ My question is: Why can you evaluate the blue expression on the LHS as $f'(1)$ before taking the limit? Moreover, the whole expression is powered by $\frac{1}{\log x}$ It seems to me like claiming $$\lim\limits_{n\to\infty}(1+\frac{1}{n})^n = \lim\limits_{n\to\infty}(1+0)^n = 1$$ So what's making the first valid whereas the second is nonsense.",,"['calculus', 'real-analysis', 'limits', 'exponentiation']"
48,Prove that $x\sqrt{1-x^2} \leq \sin x \leq x$,Prove that,x\sqrt{1-x^2} \leq \sin x \leq x,"Use the mean value theorem to prove that if $0 \leq x \leq 1$, then $$x\sqrt{1-x^2} \leq \sin x \leq x$$ The theorem guarantees the existence of a point, but not an inequality, so I don't know how to begin.","Use the mean value theorem to prove that if $0 \leq x \leq 1$, then $$x\sqrt{1-x^2} \leq \sin x \leq x$$ The theorem guarantees the existence of a point, but not an inequality, so I don't know how to begin.",,"['real-analysis', 'trigonometry', 'inequality']"
49,Rotation number of inverse maps on the circle.,Rotation number of inverse maps on the circle.,,"I'm still a bit lost in my studies of rotation numbers. Any help is much appreciated! Let's say we have a homeomorphism $F: \mathbb{R} \rightarrow \mathbb{R}$ which is a lift of a homeomorphism $f:S^1 \rightarrow S^1$ of the circle. The homeo $f$ is assumed to be orientation preserving, i.e. $F(x+1) = F(x) +1$ for all $x \in \mathbb{R}$. The rotation number  $$ \rho(F,x) = \lim_{n \rightarrow \infty} \frac{F^n(x) - x}{n} $$ exists for every $x \in \mathbb{R}$ and is constant, i.e. $\rho(F,x) = \rho(F,y)$ for all $x,y \in \mathbb{R}$. Let $F^{-1}$ be the inverse of $F$. I know that $\rho(F^{-1},x)$ also exists for every $x \in \mathbb{R}$. What I want to show now is that $$ \rho(F,x) + \rho(F^{-1},x) = 0 \quad\text{for all } x \in \mathbb{R}. $$ Somehow I am still stuck. What I managed so far was to calculate $$ \frac{F^n(x) - x}{n} = -\frac{x - F^n(x)}{n} = -\frac{F^{-n} \circ F^n(x) - F^n(x)}{n}.  $$ This almost looks like a solution to me since if I can show that if the right hand side $\frac{F^{-n} \circ F^n(x) - F^n(x)}{n}$ converges to $\rho(F^{-1},x)$ for $n \rightarrow \infty$, I am done. I know already that this term is convergent because the left hand side is convergent. I also now that for every fixed $k \in \mathbb{N}$ the term $$  \frac{F^{-n} \circ F^k(x) - F^k(x)}{n} \quad\text{converges to } \rho(F^{-1}) \text{ for }  n \rightarrow \infty. $$ Meh, I'm lost. Sorry if this is a stupid question.","I'm still a bit lost in my studies of rotation numbers. Any help is much appreciated! Let's say we have a homeomorphism $F: \mathbb{R} \rightarrow \mathbb{R}$ which is a lift of a homeomorphism $f:S^1 \rightarrow S^1$ of the circle. The homeo $f$ is assumed to be orientation preserving, i.e. $F(x+1) = F(x) +1$ for all $x \in \mathbb{R}$. The rotation number  $$ \rho(F,x) = \lim_{n \rightarrow \infty} \frac{F^n(x) - x}{n} $$ exists for every $x \in \mathbb{R}$ and is constant, i.e. $\rho(F,x) = \rho(F,y)$ for all $x,y \in \mathbb{R}$. Let $F^{-1}$ be the inverse of $F$. I know that $\rho(F^{-1},x)$ also exists for every $x \in \mathbb{R}$. What I want to show now is that $$ \rho(F,x) + \rho(F^{-1},x) = 0 \quad\text{for all } x \in \mathbb{R}. $$ Somehow I am still stuck. What I managed so far was to calculate $$ \frac{F^n(x) - x}{n} = -\frac{x - F^n(x)}{n} = -\frac{F^{-n} \circ F^n(x) - F^n(x)}{n}.  $$ This almost looks like a solution to me since if I can show that if the right hand side $\frac{F^{-n} \circ F^n(x) - F^n(x)}{n}$ converges to $\rho(F^{-1},x)$ for $n \rightarrow \infty$, I am done. I know already that this term is convergent because the left hand side is convergent. I also now that for every fixed $k \in \mathbb{N}$ the term $$  \frac{F^{-n} \circ F^k(x) - F^k(x)}{n} \quad\text{converges to } \rho(F^{-1}) \text{ for }  n \rightarrow \infty. $$ Meh, I'm lost. Sorry if this is a stupid question.",,"['real-analysis', 'algebraic-topology', 'dynamical-systems', 'circles', 'rotations']"
50,"simple way to show $|| \partial_x \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy||_{\infty} = O(||f||_{\infty})$ in $\mathbb{R}^3$",simple way to show  in,"|| \partial_x \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy||_{\infty} = O(||f||_{\infty}) \mathbb{R}^3","We are set in $\mathbb{R}^3$. Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a $C^1_0$ function, i.e. continuously differentiable with compact support. Let $\epsilon > 0$ be small. I need to show that the derivative  $$ || \partial_x [\int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3}  f(y)dy] ||_{\infty}$$  of this integral function exists and can be bounded from above by a constant times $||f||_{\infty}$ for $\epsilon \rightarrow 0$. We can take any $\infty$ norm we want for the derivative, we're differentiating a function $\mathbb{R}^3 \rightarrow \mathbb{R}^3$ so the derivative is a matrix, but we can take for example supremum over its entries, so  $$ \max_{i \in \{1,2,3\}} | \partial_{x_i} [\int_{B(x,\epsilon)} \frac{x_1 - y_1}{|x-y|^3}  f(y)dy] |$$ for example (or something else if it's easier, but I guess it shouldn't differ much). I want to show that this quantity can be bounded from above by $C ||f||_{\infty}$ with $\epsilon \rightarrow 0$. I can't just push the derivative in and say that $\partial_x \frac{x-y}{|x-y|^3} \approx |x-y|^{-3}$ because that won't let me conclude - $|x-y|^{-3}$ is not integrable around $0$ in $\mathbb{R}^3$. I was thinking about pushing the derivative inside and computing explicitly using spherical coordiantes but there are two issues: first it seems really cumbersome and I'd rather avoid it and use simpler arguments if possible and second - I'd still have to argue why I can push the derivative inside. Any suggestions are welcome. edit: when I think about it I imagine an actual difference quotient when I consider two sphere centered at $x$ and $x +h$ and I guess that I could take care of the overlapping part by 'pairing up' the points with factors $f(y)$ and $f(y+h)$, taking supremum of $\nabla f$ and integrating - that would go to $0$ with $\epsilon \rightarrow 0$ and I expect that the parts that I wouldn't get to pair up with anything will have measure suffiently small (so that the integrals are comparable with $|h|$) so that I can use the  $||f||_{\infty}$ bound there but I'm having a hard time writing it down Edit : my more precise although still incomplete try. I show how to bound the derivative assuming it exists: ok, after all it seems to be not that difficult, which gets me a bit worried so it would be great if someone could point out a mistake I might've made. I guess the only tricky part is showing that the function is indeed differentiable - if we knew that it would suffice to show that  $$ \frac{1}{|h|} | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy - \int_{B(x + h,\epsilon)} \frac{x+h-y}{|x+h-y|^3} f(y) dy| \leq C \epsilon$$ for some constant $C$. a change of variables $y = y-h$ in the second integral gives $$ \frac{1}{|h|} | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy - \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y+h) dy|$$ i.e. $$  | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} \frac{f(y) - f(y+h)}{|h|} dy | \leq \int_{B(x,\epsilon)} \frac{1}{|x-y|^2} ||\nabla f||_{\infty} dy \leq C ||\nabla f||_{\infty} \epsilon$$  because $\int_{B(x,\epsilon)} \frac{1}{|x-y|^2} = C \epsilon$, hence the whole thing goes to $0$ with $\epsilon$. Nevertheless we still need to know that it is differentiable however, the paper I'm reading clearly states that we bound it by a constant times $||f||_{\infty}$ which confuses me since if I knew it was differentiable I could say it tends to $0$ with $\epsilon$ and if I don't know whether it's differentiable then I can't really talk about bounding the derivative edit2 : ok, I see where I went wrong - I got the wrong area of integration after my change of variables. I still believe though that if I knew the derivative exists I could replace the second (wrong) integral  $$ \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y+h) dy| $$ with $$ \int_{B(x,\epsilon - 2|h|)} \frac{x-y}{|x-y|^3} f(y+h) dy|$$ plus some remainder which would then lead to a bound involving $||f||_{\infty}$ but we need differentiability to do that anyway.","We are set in $\mathbb{R}^3$. Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a $C^1_0$ function, i.e. continuously differentiable with compact support. Let $\epsilon > 0$ be small. I need to show that the derivative  $$ || \partial_x [\int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3}  f(y)dy] ||_{\infty}$$  of this integral function exists and can be bounded from above by a constant times $||f||_{\infty}$ for $\epsilon \rightarrow 0$. We can take any $\infty$ norm we want for the derivative, we're differentiating a function $\mathbb{R}^3 \rightarrow \mathbb{R}^3$ so the derivative is a matrix, but we can take for example supremum over its entries, so  $$ \max_{i \in \{1,2,3\}} | \partial_{x_i} [\int_{B(x,\epsilon)} \frac{x_1 - y_1}{|x-y|^3}  f(y)dy] |$$ for example (or something else if it's easier, but I guess it shouldn't differ much). I want to show that this quantity can be bounded from above by $C ||f||_{\infty}$ with $\epsilon \rightarrow 0$. I can't just push the derivative in and say that $\partial_x \frac{x-y}{|x-y|^3} \approx |x-y|^{-3}$ because that won't let me conclude - $|x-y|^{-3}$ is not integrable around $0$ in $\mathbb{R}^3$. I was thinking about pushing the derivative inside and computing explicitly using spherical coordiantes but there are two issues: first it seems really cumbersome and I'd rather avoid it and use simpler arguments if possible and second - I'd still have to argue why I can push the derivative inside. Any suggestions are welcome. edit: when I think about it I imagine an actual difference quotient when I consider two sphere centered at $x$ and $x +h$ and I guess that I could take care of the overlapping part by 'pairing up' the points with factors $f(y)$ and $f(y+h)$, taking supremum of $\nabla f$ and integrating - that would go to $0$ with $\epsilon \rightarrow 0$ and I expect that the parts that I wouldn't get to pair up with anything will have measure suffiently small (so that the integrals are comparable with $|h|$) so that I can use the  $||f||_{\infty}$ bound there but I'm having a hard time writing it down Edit : my more precise although still incomplete try. I show how to bound the derivative assuming it exists: ok, after all it seems to be not that difficult, which gets me a bit worried so it would be great if someone could point out a mistake I might've made. I guess the only tricky part is showing that the function is indeed differentiable - if we knew that it would suffice to show that  $$ \frac{1}{|h|} | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy - \int_{B(x + h,\epsilon)} \frac{x+h-y}{|x+h-y|^3} f(y) dy| \leq C \epsilon$$ for some constant $C$. a change of variables $y = y-h$ in the second integral gives $$ \frac{1}{|h|} | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y) dy - \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y+h) dy|$$ i.e. $$  | \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} \frac{f(y) - f(y+h)}{|h|} dy | \leq \int_{B(x,\epsilon)} \frac{1}{|x-y|^2} ||\nabla f||_{\infty} dy \leq C ||\nabla f||_{\infty} \epsilon$$  because $\int_{B(x,\epsilon)} \frac{1}{|x-y|^2} = C \epsilon$, hence the whole thing goes to $0$ with $\epsilon$. Nevertheless we still need to know that it is differentiable however, the paper I'm reading clearly states that we bound it by a constant times $||f||_{\infty}$ which confuses me since if I knew it was differentiable I could say it tends to $0$ with $\epsilon$ and if I don't know whether it's differentiable then I can't really talk about bounding the derivative edit2 : ok, I see where I went wrong - I got the wrong area of integration after my change of variables. I still believe though that if I knew the derivative exists I could replace the second (wrong) integral  $$ \int_{B(x,\epsilon)} \frac{x-y}{|x-y|^3} f(y+h) dy| $$ with $$ \int_{B(x,\epsilon - 2|h|)} \frac{x-y}{|x-y|^3} f(y+h) dy|$$ plus some remainder which would then lead to a bound involving $||f||_{\infty}$ but we need differentiability to do that anyway.",,"['real-analysis', 'integration', 'analysis', 'derivatives']"
51,Is this space a Hilbert Space?,Is this space a Hilbert Space?,,"I have a space of continuously differentiable functions on [a, b] with the dot product defined in this way: $ x \cdot y = \int_a^b \! [x(t)y(t) + x'(t)y'(t)] \, \mathrm{d}t. $ Is this space a Hilbert Space? I think that completness of the space should be checked, but i don't know how to do it. Comparing with the space of continuous functions on [a, b] (not mandatory differentiable) which has the dot product $ x \cdot y = \int_a^b \! x(t)y(t) \, \mathrm{d}t $  i see that my space and dot product (with derivatives) exclude some standart functional sequences that help to prove that the space of continuous functions is incomplete. I mean that, for example, this functional sequence $ f_n(t) =  \begin{cases} -1, & \text{if }t\text{ in [-1, -1/n]} \\ nt, & \text{if }t\text{ in [-1/n, 1/n]} \\ 1, & \text{if }t\text{ in [1/n, 1]} \end{cases} $ shows that the space of continuous functions is incomplete, but it is not appliable to my problem, because it is not continuously differentiable.","I have a space of continuously differentiable functions on [a, b] with the dot product defined in this way: $ x \cdot y = \int_a^b \! [x(t)y(t) + x'(t)y'(t)] \, \mathrm{d}t. $ Is this space a Hilbert Space? I think that completness of the space should be checked, but i don't know how to do it. Comparing with the space of continuous functions on [a, b] (not mandatory differentiable) which has the dot product $ x \cdot y = \int_a^b \! x(t)y(t) \, \mathrm{d}t $  i see that my space and dot product (with derivatives) exclude some standart functional sequences that help to prove that the space of continuous functions is incomplete. I mean that, for example, this functional sequence $ f_n(t) =  \begin{cases} -1, & \text{if }t\text{ in [-1, -1/n]} \\ nt, & \text{if }t\text{ in [-1/n, 1/n]} \\ 1, & \text{if }t\text{ in [1/n, 1]} \end{cases} $ shows that the space of continuous functions is incomplete, but it is not appliable to my problem, because it is not continuously differentiable.",,"['real-analysis', 'analysis', 'functional-analysis', 'hilbert-spaces', 'cauchy-sequences']"
52,uniformly convergence on compact metric space,uniformly convergence on compact metric space,,"Let $K$ be a compact metric space. Let $\{f_n\}_{n=1}^\infty$ be a sequence of continuous functions on $K$ such that $f_n$ converges to a function $f$ pointwise on $K$. on Walt. Rudin's book Principles of mathematical analysis, 7.13, if we assume (1). $f$ is continuous; (2). $f_n(x)\geq f_{n+1}(x)$ for all $x\in K$ and all $n$; then it is proved that $f_n$ converges to $f$ uniformly on $K$. Is there counterexample satisfying (1) but not (2)? And satisfying (2) but not (1)?","Let $K$ be a compact metric space. Let $\{f_n\}_{n=1}^\infty$ be a sequence of continuous functions on $K$ such that $f_n$ converges to a function $f$ pointwise on $K$. on Walt. Rudin's book Principles of mathematical analysis, 7.13, if we assume (1). $f$ is continuous; (2). $f_n(x)\geq f_{n+1}(x)$ for all $x\in K$ and all $n$; then it is proved that $f_n$ converges to $f$ uniformly on $K$. Is there counterexample satisfying (1) but not (2)? And satisfying (2) but not (1)?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'functions']"
53,Convergence equivalence,Convergence equivalence,,"If $G_k(\mathbb{R}^m)=\{ W: W$ is subspace of $\mathbb{R}^m, \dim W=k \}$ and consider in $G_k(\mathbb{R}^m)$ one topology $\tau$ where $U\in \tau$ is open iff  the set $\widehat{U}=\lbrace v: v\in W\backslash \lbrace 0\rbrace, \mbox{for some}  \ W\in U \rbrace$ is open in $\mathbb{R}^m$ I'm testing the following: If for every $k\in \mathbb{N}$ there is a basis $\lbrace u^k_1,\ldots,u^k_n\rbrace$ of $S_k$ such that $\Lambda=\lbrace \displaystyle{\lim_{ k \rightarrow +\infty}}u^k_1,\ldots,\displaystyle{\lim_{ k \rightarrow +\infty}}u^k_n\rbrace$ is a basis of $S$ then $\lbrace S_k\rbrace \subset G_n(\mathbb{R}^m)$ converges to $S\in G_n(\mathbb{R}^m)$ . My progress: Let $U$ open of $S$ in $G_n(\mathbb{R}^m)$ . Given that $\widehat{U}$ is open in $\mathbb{R}^m$ such that $\Lambda \subset \widehat{U}$ , there $k_0 \in \mathbb{N}$ such that $u^k_j \in \widehat{U}$ for all $k\geq k_0$ and all $1\leq j\leq n$ . There $W_j\in U$ such that $u^k_j\in W_j$ . But as showing $W_j=S_k$ for all $1\leq j\leq n$ ? Note: The other direction of the statement is also true but I only need the above. This is an example of open","If is subspace of and consider in one topology where is open iff  the set is open in I'm testing the following: If for every there is a basis of such that is a basis of then converges to . My progress: Let open of in . Given that is open in such that , there such that for all and all . There such that . But as showing for all ? Note: The other direction of the statement is also true but I only need the above. This is an example of open","G_k(\mathbb{R}^m)=\{ W: W \mathbb{R}^m, \dim W=k \} G_k(\mathbb{R}^m) \tau U\in \tau \widehat{U}=\lbrace v: v\in W\backslash \lbrace 0\rbrace, \mbox{for some}  \ W\in U \rbrace \mathbb{R}^m k\in \mathbb{N} \lbrace u^k_1,\ldots,u^k_n\rbrace S_k \Lambda=\lbrace \displaystyle{\lim_{ k \rightarrow +\infty}}u^k_1,\ldots,\displaystyle{\lim_{ k \rightarrow +\infty}}u^k_n\rbrace S \lbrace S_k\rbrace \subset G_n(\mathbb{R}^m) S\in G_n(\mathbb{R}^m) U S G_n(\mathbb{R}^m) \widehat{U} \mathbb{R}^m \Lambda \subset \widehat{U} k_0 \in \mathbb{N} u^k_j \in \widehat{U} k\geq k_0 1\leq j\leq n W_j\in U u^k_j\in W_j W_j=S_k 1\leq j\leq n","['real-analysis', 'general-topology', 'grassmannian']"
54,Stokes theorem and Sobolev spaces.,Stokes theorem and Sobolev spaces.,,"I am interested under which regularity condition is Stokes' theorem is still valid. For concreteness I am interested in the following problem Let's consider a domain $\Omega$ in $\mathbb{R}^{3}$  given in cylindrical coordinates by $0\le z\le 1 $,$0\le \theta\le2\pi$,$0\le \rho\le b$. Now let $f\in H^{2}(\Omega)$. If we apply Stokes' theorem for the domain $\Omega_{\epsilon}$ in $\mathbb{R}^{3}$  given in cylindrical coordinates by $0\le z\le 1 $,$0\le \theta\le2\pi$,$\epsilon\le \rho\le b$ we have: \begin{equation} \int_{\Omega}\nabla\cdot \nabla fd\Omega=\int_{\partial\Omega_{\epsilon}}\frac{\partial f}{\partial x^{i}} n^{i} dS \end{equation} which is well-defined because the trace theorem warranty that $\frac{\partial f}{\partial x^{i}}\in H^{1/2}$. Now in the case when $\epsilon\rightarrow0$ the boundary integral is: \begin{equation} \lim_{\epsilon\rightarrow 0}\int_{\partial\Omega_{\epsilon}}\frac{\partial f}{\partial x^{i}} n^{i} dS=\int\frac{\partial f}{\partial x^{i}}b d\theta dz-\lim_{\epsilon\rightarrow 0}\int\frac{\partial f}{\partial x^{i}}\epsilon d\theta dz \end{equation} If $f$ is smooth then the limit vanishes. However in the low differentiability case I have the following questions: In the limit $\frac{\partial f}{\partial x^{i}}$ has to be restricted to a codimension 2 domain. What can I say about the regularity of the trace in the codimension 2? In case $f$ has even lower regularity such that the trace in the codimension 2 case is in a negative Sobolev space,should the limit be consider to be a functional? What's is the lower differentiability required for Stokes' theorem to hold?","I am interested under which regularity condition is Stokes' theorem is still valid. For concreteness I am interested in the following problem Let's consider a domain $\Omega$ in $\mathbb{R}^{3}$  given in cylindrical coordinates by $0\le z\le 1 $,$0\le \theta\le2\pi$,$0\le \rho\le b$. Now let $f\in H^{2}(\Omega)$. If we apply Stokes' theorem for the domain $\Omega_{\epsilon}$ in $\mathbb{R}^{3}$  given in cylindrical coordinates by $0\le z\le 1 $,$0\le \theta\le2\pi$,$\epsilon\le \rho\le b$ we have: \begin{equation} \int_{\Omega}\nabla\cdot \nabla fd\Omega=\int_{\partial\Omega_{\epsilon}}\frac{\partial f}{\partial x^{i}} n^{i} dS \end{equation} which is well-defined because the trace theorem warranty that $\frac{\partial f}{\partial x^{i}}\in H^{1/2}$. Now in the case when $\epsilon\rightarrow0$ the boundary integral is: \begin{equation} \lim_{\epsilon\rightarrow 0}\int_{\partial\Omega_{\epsilon}}\frac{\partial f}{\partial x^{i}} n^{i} dS=\int\frac{\partial f}{\partial x^{i}}b d\theta dz-\lim_{\epsilon\rightarrow 0}\int\frac{\partial f}{\partial x^{i}}\epsilon d\theta dz \end{equation} If $f$ is smooth then the limit vanishes. However in the low differentiability case I have the following questions: In the limit $\frac{\partial f}{\partial x^{i}}$ has to be restricted to a codimension 2 domain. What can I say about the regularity of the trace in the codimension 2? In case $f$ has even lower regularity such that the trace in the codimension 2 case is in a negative Sobolev space,should the limit be consider to be a functional? What's is the lower differentiability required for Stokes' theorem to hold?",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
55,"How to show $\{a^n \bmod \alpha\}_{n \in \mathbb{N}}$ is dense in $[0,\alpha]$ if $a > 1$ is trancendental over ${\mathbb Q}[\alpha]$",How to show  is dense in  if  is trancendental over,"\{a^n \bmod \alpha\}_{n \in \mathbb{N}} [0,\alpha] a > 1 {\mathbb Q}[\alpha]","How to show $\{a^n \bmod \alpha\}_{n \in \mathbb{N}}$ is dense in $[0,\alpha]$ if $a > 1$ is trancendental over ${\mathbb Q}[\alpha]$? If $a$ is transcendental over ${\mathbb Q}[\alpha]$ then the integer multiples $\{na \bmod \alpha\}_{n \in \mathbb{N}}$ are dense in $[0,\alpha]$. But what about the positive integer powers of $a$ when $a > 1$? It seems like $\{a^n \bmod \alpha\}_{n \in \mathbb{N}}$ should be dense if $a > 1$ is transcendental over ${\mathbb Q}[\alpha]$. But how to prove it?","How to show $\{a^n \bmod \alpha\}_{n \in \mathbb{N}}$ is dense in $[0,\alpha]$ if $a > 1$ is trancendental over ${\mathbb Q}[\alpha]$? If $a$ is transcendental over ${\mathbb Q}[\alpha]$ then the integer multiples $\{na \bmod \alpha\}_{n \in \mathbb{N}}$ are dense in $[0,\alpha]$. But what about the positive integer powers of $a$ when $a > 1$? It seems like $\{a^n \bmod \alpha\}_{n \in \mathbb{N}}$ should be dense if $a > 1$ is transcendental over ${\mathbb Q}[\alpha]$. But how to prove it?",,"['real-analysis', 'transcendental-numbers']"
56,Prove functions defined by sup and inf are continuous [duplicate],Prove functions defined by sup and inf are continuous [duplicate],,"This question already has answers here : Supremum of a Continuous Function is Continuous (4 answers) Closed 5 years ago . Suppose $f$ is continuous on $[a,b]$. Show that the functions defined by $m(x)=\inf\{f(y):y\in[a,x]\}$ and $M(x)=\sup\{f(y):y\in[a,x]\}$ are well defined and are also continuous on $[a,b]$ I have already managed to prove that they are well defined, since $f$ is continuous on $[a,b]$ so it is bounded. Therefore for every $x\in[a,b]$, the set $\{f(y):y\in[a,x]\}$ has a supremum and an infimum. To prove that they are continuous on $[a,b]$, I've taken an arbitrary sequence $\{x_n\}$ contained in $[a,b]$ converging to some number $c\in[a,b]$ and am attempting to show that $m(x_n)\rightarrow m(c)$ and $M(x_n)\rightarrow M(c)$, but I'm not quite sure how. Any help would be appreciated, thanks!","This question already has answers here : Supremum of a Continuous Function is Continuous (4 answers) Closed 5 years ago . Suppose $f$ is continuous on $[a,b]$. Show that the functions defined by $m(x)=\inf\{f(y):y\in[a,x]\}$ and $M(x)=\sup\{f(y):y\in[a,x]\}$ are well defined and are also continuous on $[a,b]$ I have already managed to prove that they are well defined, since $f$ is continuous on $[a,b]$ so it is bounded. Therefore for every $x\in[a,b]$, the set $\{f(y):y\in[a,x]\}$ has a supremum and an infimum. To prove that they are continuous on $[a,b]$, I've taken an arbitrary sequence $\{x_n\}$ contained in $[a,b]$ converging to some number $c\in[a,b]$ and am attempting to show that $m(x_n)\rightarrow m(c)$ and $M(x_n)\rightarrow M(c)$, but I'm not quite sure how. Any help would be appreciated, thanks!",,['real-analysis']
57,"real analysis, chebyshev's inequality","real analysis, chebyshev's inequality",,"Suppose $f$ is a non negative integrable function on a measure space $(X,M,μ)$. Prove that: $$\lim_{t \rightarrow \infty} t\cdot \mu(\{x:f(x)\geq t\} )=0.$$ Can you help me please?","Suppose $f$ is a non negative integrable function on a measure space $(X,M,μ)$. Prove that: $$\lim_{t \rightarrow \infty} t\cdot \mu(\{x:f(x)\geq t\} )=0.$$ Can you help me please?",,"['real-analysis', 'integration', 'measure-theory']"
58,"De-mystifying tricks in proof – If $\{x_n\}$ converges, then Cesaro Mean converges.","De-mystifying tricks in proof – If  converges, then Cesaro Mean converges.",\{x_n\},"Stephen Abbott . Understanding Analysis (2016 2 edn) . p. 55. Not a duplicate . Exercise 2.3.11 (Cesaro Means). (a) Show if $\{x_n\}$ is a convergent sequence, then the sequences given by the averages $\{\dfrac{x_1 + x_2 + ... + x_n}{n}\}$ converges to the same limit. (Not a duplicate ) I rewrote and colored the official solution. Let $\epsilon>0$ be arbitrary. Then we need to find an $N \in \mathbb{N} \qquad  \ni n \geq N \implies\ |\frac{x_1 + x_2 + ... + x_n}{n} - L|< \epsilon \tag{1}$ . Question posits $(x_{n}) \to L$ . So $\exists \; M \in \mathbb{N} \ni n \ge M \implies |x_{n}-L|< M \quad (2)$ . $\text{Also } \exists \; C \ni n \ge C \implies |x_{n}-L|< \epsilon/2. \quad \tag{3}$ My question 1. Where does (3) issue from? How to presage $\epsilon/2$ ?  Normally you start with $\epsilon$ . Doesn't the same argument prove all the terms can be bounded? Why simply 'the early terms in the averages can be bounded' ? Please expatiate on ""Because the original sequence is convergent, we suspect that we can bound ... we will be breaking the limit in two at the end"" ? I still don't understand how ""we suspect"" these bounds? Now for all $n \ge C$ , we can write How to presage rewriting $\color{red}{L = nL/n}?$ How to presage spltting the sum between $x_{C - 1}$ and $x_C$ ? Now apply the Triangle Inequality to each of the $n$ $(x_i - L)$ terms. $\begin{align}         \le & \frac{1}{n}( & \color{green}{\left| x_{1}-L\right| +\ldots +\left| x_{c-1}-L\right|}  & + \color{brown}{\left| x_{C}-L\right| +\ldots +\left| x_{n}-L\right|} & )\\     & & \color{green}{\text{Each of these $(C - 1)$ terms < M by (2)} } & \color{brown}{ \text{ Each of these $(n - C)$ terms < $\epsilon/2$ by (3)}} & \\ \le & \frac{1}{n}( & \color{green}{(C - 1)M} & + \color{brown}{\frac{e}{2}(n - C)} & ). \tag{4}\\ \end{align}$ Why is the last inequality (4) above $\le$ ? Why not $<$ like (2) and (3)? Because C and M are fixed constants at this point, we may choose $N_2$ so that $\color{green}{(C - 1)M}\frac{1}{n}  < e/2 \tag{5}$ for all $n \ge N_2$ . Finally, let $N$ [in $(1)$ ] $= \max\{C, N_2\}$ be the desired $N$ . Why are we allowed to choose $N_2$ so that $\color{green}{(C - 1)M}\frac{1}{n} < e/2$ ? $C \in \mathbb{N} \quad \therefore n - C < n \iff \color{magenta}{\frac{n - C}{n}} < 1 \tag{6}$ $\begin{align} \text{Equation (4) is} &  & \color{green}{(C - 1)M}\frac{1}{n}  & + \frac{e}{2}\color{magenta}{\frac{n - C}{n}}. \\ \text{Then by (5) and (6),} & & < e/2 & + e/2\color{magenta}{(1)}/  QED. \\  \end{align}$","Stephen Abbott . Understanding Analysis (2016 2 edn) . p. 55. Not a duplicate . Exercise 2.3.11 (Cesaro Means). (a) Show if is a convergent sequence, then the sequences given by the averages converges to the same limit. (Not a duplicate ) I rewrote and colored the official solution. Let be arbitrary. Then we need to find an . Question posits . So . My question 1. Where does (3) issue from? How to presage ?  Normally you start with . Doesn't the same argument prove all the terms can be bounded? Why simply 'the early terms in the averages can be bounded' ? Please expatiate on ""Because the original sequence is convergent, we suspect that we can bound ... we will be breaking the limit in two at the end"" ? I still don't understand how ""we suspect"" these bounds? Now for all , we can write How to presage rewriting How to presage spltting the sum between and ? Now apply the Triangle Inequality to each of the terms. Why is the last inequality (4) above ? Why not like (2) and (3)? Because C and M are fixed constants at this point, we may choose so that for all . Finally, let [in ] be the desired . Why are we allowed to choose so that ?","\{x_n\} \{\dfrac{x_1 + x_2 + ... + x_n}{n}\} \epsilon>0 N \in \mathbb{N} \qquad  \ni n \geq N \implies\ |\frac{x_1 + x_2 + ... + x_n}{n} - L|< \epsilon \tag{1} (x_{n}) \to L \exists \; M \in \mathbb{N} \ni n \ge M \implies |x_{n}-L|< M \quad (2) \text{Also } \exists \; C \ni n \ge C \implies |x_{n}-L|< \epsilon/2. \quad \tag{3} \epsilon/2 \epsilon n \ge C \color{red}{L = nL/n}? x_{C - 1} x_C n (x_i - L) \begin{align}
        \le & \frac{1}{n}( & \color{green}{\left| x_{1}-L\right| +\ldots +\left| x_{c-1}-L\right|}  & + \color{brown}{\left| x_{C}-L\right| +\ldots +\left| x_{n}-L\right|} & )\\
    & & \color{green}{\text{Each of these (C - 1) terms < M by (2)} } & \color{brown}{ \text{ Each of these (n - C) terms < \epsilon/2 by (3)}} & \\
\le & \frac{1}{n}( & \color{green}{(C - 1)M} & + \color{brown}{\frac{e}{2}(n - C)} & ). \tag{4}\\
\end{align} \le < N_2 \color{green}{(C - 1)M}\frac{1}{n}  < e/2 \tag{5} n \ge N_2 N (1) = \max\{C, N_2\} N N_2 \color{green}{(C - 1)M}\frac{1}{n} < e/2 C \in \mathbb{N} \quad \therefore n - C < n \iff \color{magenta}{\frac{n - C}{n}} < 1 \tag{6} \begin{align}
\text{Equation (4) is} &  & \color{green}{(C - 1)M}\frac{1}{n}  & + \frac{e}{2}\color{magenta}{\frac{n - C}{n}}. \\
\text{Then by (5) and (6),} & & < e/2 & + e/2\color{magenta}{(1)}/  QED. \\ 
\end{align}",['real-analysis']
59,"For a system of PDEs, how many equations are needed generally for the system to have unique solution?","For a system of PDEs, how many equations are needed generally for the system to have unique solution?",,"For an algebraic system of equations or a system of ordinary differential equations the following rule holds:(right?) the total number of unknown variables must be equal to the number of equations (and also the same number of boundary conditions are needed, but that's not my question) Is it generally correct for a system of PDEs too? I asked this specific question on physics.SE, where one of the users presented the following example in comments saing that it is not the case for a system of PDEs : $$\partial_x f=0 \,\,\,\,\,\partial_y f=0  \,\,\,\,\text{(two equations)}$$ $$\to f(x,y)=0 \,\,\,\text{(unique solution)}$$ (I've seen this question (without answer), which asks about number of needed boundary conditions; my question is about the number of independent equations needed)","For an algebraic system of equations or a system of ordinary differential equations the following rule holds:(right?) the total number of unknown variables must be equal to the number of equations (and also the same number of boundary conditions are needed, but that's not my question) Is it generally correct for a system of PDEs too? I asked this specific question on physics.SE, where one of the users presented the following example in comments saing that it is not the case for a system of PDEs : $$\partial_x f=0 \,\,\,\,\,\partial_y f=0  \,\,\,\,\text{(two equations)}$$ $$\to f(x,y)=0 \,\,\,\text{(unique solution)}$$ (I've seen this question (without answer), which asks about number of needed boundary conditions; my question is about the number of independent equations needed)",,"['real-analysis', 'partial-differential-equations', 'systems-of-equations']"
60,Existence of a continuous function.,Existence of a continuous function.,,"Question is to check Which of the following statements are true? There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : 2x^2+3y^2=1\}\rightarrow \mathbb{R}$ which is one-one. There exists a continuous function $f: (-1,1)\rightarrow (-1,1]$ which is one one and onto. There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : y^2=4x\}\rightarrow \mathbb{R}$ which is one-one. There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\}\rightarrow \mathbb{R}$ which is Onto. There exists a continuous function $f : S^1=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\} \to \mathbb{R}$ which is one-one. What i have done so far is : Solution for $1$ : we see that $\{(x,y)\in \mathbb{R}^2 : 2x^2+3y^2=1\}$ is connected and compact... Suppose we have continuous function $f :S \rightarrow \mathbb{R}$ which is one one then we would see that $S$ is homeomorphic to $f(S)$* Now, $S$ is connected so is $f(S)$ and $S$ is compact so is $f(S)$.Thus $f(S)=[a,b]$ for some $a,b\in \mathbb{R}$ suppose I remove a point $c\in(a,b)$ then $f(S)$ would have two connected components where as the corresponding subset of $S$ obtained by removing $f^{-1}(c)$ is connected which is absurd. Thus there exist no continuous one one map $f :S \rightarrow \mathbb{R}$ Solution for $2$ : Suppose we have continuous bijection $f: (-1,1)\rightarrow (-1,1]$ then we have $t\in(-1,1)$ such that $f(t)=1$. Now, any continuous injection has to be such that $a<b$ implies $f(a)\leq f(b)$ or $f(a)\geq f(b)$ for all $a,b$ (I do not how does one call this property as) Assuming $f$ is increasing and  $f(t)=1$ then $f(m)\geq 1$ for all $m\in (t,1)$ which is a contradiction to $f$ being injective... Assuming $f$ is decreasing and  $f(t)=1$ then $f(m)\geq 1$ for all $m\in (-1,t)$ which is a contradiction to $f$ being injective... Thus there is no continuous bijection from $(-1,1)$ to $(-1,1]$ Solution for $3$ : we see that $\{(x,y)\in \mathbb{R}^2 : y^2=4x\}$ is connected (Not Compact. sorry for my laziness and thanks to gaoxinge :D )... Suppose we have continuous function $f :S \rightarrow \mathbb{R}$ which is one one then we would see that $S$ is homeomorphic to $f(S)$* Now, $S$ is connected so is $f(S)$ and $S$ is compact so is $f(S)$.Thus $f(S)=[a,b]$ for some $a,b\in \mathbb{R}$ I though of applying same idea as i have done to check First question but then i am not getting any negative result. Moreover I guess that such a map exists and i can actually find a homeomorphism to $\mathbb{R}$. I am unable to write explicitly but What i would do is i would bend the parabola so that it would coincide with real line (I am not able to express it precisely please try to understand something from this). But how would i write ""bending"" as a function and how would i show that this is a homeomorphism. Please help in that case and more over is my intuition correct? Solution for $4$ : Continuous image of compact space is compact but $\mathbb{R}=f(S)=f(\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\})$ is not compact though $S=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\}$ is compact. Solution for $5$ : for the same reason that i have treid to explain in First question there is no continuous function $f : S^1=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\} \to \mathbb{R}$ which is one-one. Please see if this justification is sufficient and if it is please let me know if there are any other ways of seeing these things. Thank you. P.S: I have used one result which i feel is worth sharing : continuous bijection from a compact space to Hausdorff space is homeomorphism","Question is to check Which of the following statements are true? There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : 2x^2+3y^2=1\}\rightarrow \mathbb{R}$ which is one-one. There exists a continuous function $f: (-1,1)\rightarrow (-1,1]$ which is one one and onto. There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : y^2=4x\}\rightarrow \mathbb{R}$ which is one-one. There exists a continuous function  $f: \{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\}\rightarrow \mathbb{R}$ which is Onto. There exists a continuous function $f : S^1=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\} \to \mathbb{R}$ which is one-one. What i have done so far is : Solution for $1$ : we see that $\{(x,y)\in \mathbb{R}^2 : 2x^2+3y^2=1\}$ is connected and compact... Suppose we have continuous function $f :S \rightarrow \mathbb{R}$ which is one one then we would see that $S$ is homeomorphic to $f(S)$* Now, $S$ is connected so is $f(S)$ and $S$ is compact so is $f(S)$.Thus $f(S)=[a,b]$ for some $a,b\in \mathbb{R}$ suppose I remove a point $c\in(a,b)$ then $f(S)$ would have two connected components where as the corresponding subset of $S$ obtained by removing $f^{-1}(c)$ is connected which is absurd. Thus there exist no continuous one one map $f :S \rightarrow \mathbb{R}$ Solution for $2$ : Suppose we have continuous bijection $f: (-1,1)\rightarrow (-1,1]$ then we have $t\in(-1,1)$ such that $f(t)=1$. Now, any continuous injection has to be such that $a<b$ implies $f(a)\leq f(b)$ or $f(a)\geq f(b)$ for all $a,b$ (I do not how does one call this property as) Assuming $f$ is increasing and  $f(t)=1$ then $f(m)\geq 1$ for all $m\in (t,1)$ which is a contradiction to $f$ being injective... Assuming $f$ is decreasing and  $f(t)=1$ then $f(m)\geq 1$ for all $m\in (-1,t)$ which is a contradiction to $f$ being injective... Thus there is no continuous bijection from $(-1,1)$ to $(-1,1]$ Solution for $3$ : we see that $\{(x,y)\in \mathbb{R}^2 : y^2=4x\}$ is connected (Not Compact. sorry for my laziness and thanks to gaoxinge :D )... Suppose we have continuous function $f :S \rightarrow \mathbb{R}$ which is one one then we would see that $S$ is homeomorphic to $f(S)$* Now, $S$ is connected so is $f(S)$ and $S$ is compact so is $f(S)$.Thus $f(S)=[a,b]$ for some $a,b\in \mathbb{R}$ I though of applying same idea as i have done to check First question but then i am not getting any negative result. Moreover I guess that such a map exists and i can actually find a homeomorphism to $\mathbb{R}$. I am unable to write explicitly but What i would do is i would bend the parabola so that it would coincide with real line (I am not able to express it precisely please try to understand something from this). But how would i write ""bending"" as a function and how would i show that this is a homeomorphism. Please help in that case and more over is my intuition correct? Solution for $4$ : Continuous image of compact space is compact but $\mathbb{R}=f(S)=f(\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\})$ is not compact though $S=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\}$ is compact. Solution for $5$ : for the same reason that i have treid to explain in First question there is no continuous function $f : S^1=\{(x,y)\in \mathbb{R}^2 : x^2+y^2=1\} \to \mathbb{R}$ which is one-one. Please see if this justification is sufficient and if it is please let me know if there are any other ways of seeing these things. Thank you. P.S: I have used one result which i feel is worth sharing : continuous bijection from a compact space to Hausdorff space is homeomorphism",,['real-analysis']
61,Inverse rule to the L'Hôpital's rule,Inverse rule to the L'Hôpital's rule,,"If in L'Hôpital's rule we have that: $f,g :(a,b)\to \mathbb{R}$ , there exist $f'(x)$ , $g'(x)$ , and $g'(x)\ne0$ , $$\lim_{x\to a^+} \frac{f(x)}{g(x)} = L,$$ and also $\lim_{x\to a^+} f(x)=\lim_{x\to a^+} g(x) = 0$ , must it be also $$\lim_{x\to a^+}\frac{f'(x)}{g'(x)} = L$$ or not? I think it is wrong in some cases, but I can't find an example.","If in L'Hôpital's rule we have that: , there exist , , and , and also , must it be also or not? I think it is wrong in some cases, but I can't find an example.","f,g :(a,b)\to \mathbb{R} f'(x) g'(x) g'(x)\ne0 \lim_{x\to a^+} \frac{f(x)}{g(x)} = L, \lim_{x\to a^+} f(x)=\lim_{x\to a^+} g(x) = 0 \lim_{x\to a^+}\frac{f'(x)}{g'(x)} = L",['real-analysis']
62,Uniqueness is a consequence of a non-vanishing autonomous flux,Uniqueness is a consequence of a non-vanishing autonomous flux,,"Let $f: \mathbb R\to\mathbb R$ continuous and $f(x)\ne 0$, or all $x\in\mathbb R$. Let also $\varphi,\psi : \mathbb R\to\mathbb R$, continuously differentiable functions satisfying $$ \varphi'(t)=f\big(\varphi(t)\big),\quad \psi'(t)=f\big(\psi(t)\big), $$ for all $t\in\mathbb R$. Show that there exists a $\tau\in\mathbb R$, such that $$ \psi(t)=\varphi(t-\tau),\quad \text{for all $t\in\mathbb R$}. $$ Note. Here uniqueness is the question, but in order to even consider proving it, one has to first make sure that the ranges of the two solutions intersect!","Let $f: \mathbb R\to\mathbb R$ continuous and $f(x)\ne 0$, or all $x\in\mathbb R$. Let also $\varphi,\psi : \mathbb R\to\mathbb R$, continuously differentiable functions satisfying $$ \varphi'(t)=f\big(\varphi(t)\big),\quad \psi'(t)=f\big(\psi(t)\big), $$ for all $t\in\mathbb R$. Show that there exists a $\tau\in\mathbb R$, such that $$ \psi(t)=\varphi(t-\tau),\quad \text{for all $t\in\mathbb R$}. $$ Note. Here uniqueness is the question, but in order to even consider proving it, one has to first make sure that the ranges of the two solutions intersect!",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
63,Newton-Raphson for reciprocal square root,Newton-Raphson for reciprocal square root,,"I have a question about using Newton-Raphson to refine a guess of the reciprocal square root function. The reciprocal square root of $a$ is the number $x$ which satisfies the following equation: $$x^{-2} = a$$ So we are looking for the root of the following equation: $$x^{-2} - a = 0$$ Applying the Newton-Raphson method then leads to the following: $$x_{n+1} = x_n - {f(x_n) \over f'(x_n)} = x_n - {x_n^{-2} - a \over -2x_n^{-3}} = x_n(1.5 - 0.5ax_n^2)$$ Before looking up the above standard solution, I tried to come up with my own equation: $$x^2 = {1 \over a}$$ In this case, we are looking for the root of a different equation: $$x^2 - {1 \over a} = 0$$ And the Newton-Raphson method gives us: $$x_{n+1} = x_n - {f(x_n) \over f'(x_n)} = x_n - {x_n^2 - {1 \over a} \over 2x_n} = 0.5(x_n + {1 \over ax_n})$$ Is there anything wrong with this alternative approach, and why would I choose one over the other?","I have a question about using Newton-Raphson to refine a guess of the reciprocal square root function. The reciprocal square root of $a$ is the number $x$ which satisfies the following equation: $$x^{-2} = a$$ So we are looking for the root of the following equation: $$x^{-2} - a = 0$$ Applying the Newton-Raphson method then leads to the following: $$x_{n+1} = x_n - {f(x_n) \over f'(x_n)} = x_n - {x_n^{-2} - a \over -2x_n^{-3}} = x_n(1.5 - 0.5ax_n^2)$$ Before looking up the above standard solution, I tried to come up with my own equation: $$x^2 = {1 \over a}$$ In this case, we are looking for the root of a different equation: $$x^2 - {1 \over a} = 0$$ And the Newton-Raphson method gives us: $$x_{n+1} = x_n - {f(x_n) \over f'(x_n)} = x_n - {x_n^2 - {1 \over a} \over 2x_n} = 0.5(x_n + {1 \over ax_n})$$ Is there anything wrong with this alternative approach, and why would I choose one over the other?",,"['real-analysis', 'convergence-divergence', 'numerical-methods', 'approximation']"
64,proving that $S_n$ is Cauchy.,proving that  is Cauchy.,S_n,"$$S_n = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + ... + \frac{(-1)^{n+1}}{2n-1} $$ Show that $(S_n)$ is a Cauchy sequence and hence that it converges to limit $L$. Show that $\frac{2}{3} < L < \frac{13}{15}$ To show that $(S_n)$ is Cauchy sequence, I tried $|S_n - S_m| < \epsilon$ Let $n = m+k$ $|S_n - S_m|$ $ = |\frac{(-1)^{m+2}}{2(m+1)-1}+ ... + \frac{(-1)^{m+k+1}}{2(m+k)-1}|$ Now I don't know where to go from here. Thanks.","$$S_n = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + ... + \frac{(-1)^{n+1}}{2n-1} $$ Show that $(S_n)$ is a Cauchy sequence and hence that it converges to limit $L$. Show that $\frac{2}{3} < L < \frac{13}{15}$ To show that $(S_n)$ is Cauchy sequence, I tried $|S_n - S_m| < \epsilon$ Let $n = m+k$ $|S_n - S_m|$ $ = |\frac{(-1)^{m+2}}{2(m+1)-1}+ ... + \frac{(-1)^{m+k+1}}{2(m+k)-1}|$ Now I don't know where to go from here. Thanks.",,"['real-analysis', 'sequences-and-series', 'analysis', 'cauchy-sequences']"
65,Iterative method for matrix differential equation,Iterative method for matrix differential equation,,"Let $A$ and $X(t)$ be $n\times n$ matrices. I want to solve the matrix differential equation $$\dfrac{dX}{dt}(t)=AX(t)$$ with $X(0)=I$ (the $n\times n$ identity matrix) using the Picard iterative process . So I want $X_0(t)=I$ for all $t\in\mathbb{R}$, and $$X_k(t)=I+\int_0^tAX_{k-1}(s)ds$$ So I compute $X_1(t)=I+\int_0^tAds=I+tA$. Iterating, I have $X_2(t)=I+\int_0^tA(I+sA)ds=I+At+\dfrac{A^2t^2}{2}$. Iterating once more, I have $X_3(t)=I+\int_0^tA(I+As+\dfrac{A^2s^2}{2})ds=I+At+\dfrac{A^2t^2}{2}+\dfrac{A^3t^3}{6}$ Then the limit will eventually be $X_\infty(t)=I+\sum_{k=1}^\infty\dfrac{(At)^k}{k!}$. And it seems to be a solution of the original equation. How do I know that the iterative method is guaranteed to work in a case like this though? Is there some general theorem to verify that?","Let $A$ and $X(t)$ be $n\times n$ matrices. I want to solve the matrix differential equation $$\dfrac{dX}{dt}(t)=AX(t)$$ with $X(0)=I$ (the $n\times n$ identity matrix) using the Picard iterative process . So I want $X_0(t)=I$ for all $t\in\mathbb{R}$, and $$X_k(t)=I+\int_0^tAX_{k-1}(s)ds$$ So I compute $X_1(t)=I+\int_0^tAds=I+tA$. Iterating, I have $X_2(t)=I+\int_0^tA(I+sA)ds=I+At+\dfrac{A^2t^2}{2}$. Iterating once more, I have $X_3(t)=I+\int_0^tA(I+As+\dfrac{A^2s^2}{2})ds=I+At+\dfrac{A^2t^2}{2}+\dfrac{A^3t^3}{6}$ Then the limit will eventually be $X_\infty(t)=I+\sum_{k=1}^\infty\dfrac{(At)^k}{k!}$. And it seems to be a solution of the original equation. How do I know that the iterative method is guaranteed to work in a case like this though? Is there some general theorem to verify that?",,"['real-analysis', 'matrices', 'ordinary-differential-equations']"
66,Proving a sequence is Cauchy given some qualities about the sequence,Proving a sequence is Cauchy given some qualities about the sequence,,"I've got a sequence $x_n$ such that I've proved $b\leq x_n \leq c$, and $|x_{n+1}-x_{n}|\leq \frac{4}{9}|x_n-x_{n-1}|$ However I'm not very familiar with Cauchy sequences, so I don't know how to exactly prove it's Cauchy. I've gotten so far: Let $m,n>0$. Then $m=n+a$. So \begin{align*} |x_m-x_n| &= |x_{n+a}-x_n|\\ &= |(x_{n+a}-x_{n+a-1})+(x_{n+a-1}-x_{n+a-2})+\ ...\ +(x_{n+1}-x_n)|\\ &\leq a|x_n-x_{n-1}| \end{align*} I know I'm close, but I'm not sure where to go from here. EDIT: \begin{align*} |x_m-x_n| &= |x_{n+a}-x_n|\\ &= |(x_{n+a}-x_{n+a-1})+(x_{n+a-1}-x_{n+a-2})+\ ...\ +(x_{n+1}-x_n)|\\ &\leq \Sigma_{i=1}^{i=a}(\frac{4}{9})^i * |x_{n}-x_{n-1}|\\ &\leq (c-b)\frac{4}{9}\cdot \frac{1-(4/9)^a}{5/9}\\ &\leq \frac{4}{5}(c-b)(1-(\frac{4}{9})^a) \end{align*} But now I'm not sure how to say it's Cauchy. EDIT 2: $$\frac{4}{5}(c-b)(1-(\frac{4}{9})^a) \leq \frac{4}{5}(c-b)$$ So let $n_0 = \epsilon * \frac{5}{4(c-b)}$. Then for any $\epsilon > 0$,   $|x_m-x_n|<\epsilon$ whenever $m,n > \frac{5}{2}\epsilon$.","I've got a sequence $x_n$ such that I've proved $b\leq x_n \leq c$, and $|x_{n+1}-x_{n}|\leq \frac{4}{9}|x_n-x_{n-1}|$ However I'm not very familiar with Cauchy sequences, so I don't know how to exactly prove it's Cauchy. I've gotten so far: Let $m,n>0$. Then $m=n+a$. So \begin{align*} |x_m-x_n| &= |x_{n+a}-x_n|\\ &= |(x_{n+a}-x_{n+a-1})+(x_{n+a-1}-x_{n+a-2})+\ ...\ +(x_{n+1}-x_n)|\\ &\leq a|x_n-x_{n-1}| \end{align*} I know I'm close, but I'm not sure where to go from here. EDIT: \begin{align*} |x_m-x_n| &= |x_{n+a}-x_n|\\ &= |(x_{n+a}-x_{n+a-1})+(x_{n+a-1}-x_{n+a-2})+\ ...\ +(x_{n+1}-x_n)|\\ &\leq \Sigma_{i=1}^{i=a}(\frac{4}{9})^i * |x_{n}-x_{n-1}|\\ &\leq (c-b)\frac{4}{9}\cdot \frac{1-(4/9)^a}{5/9}\\ &\leq \frac{4}{5}(c-b)(1-(\frac{4}{9})^a) \end{align*} But now I'm not sure how to say it's Cauchy. EDIT 2: $$\frac{4}{5}(c-b)(1-(\frac{4}{9})^a) \leq \frac{4}{5}(c-b)$$ So let $n_0 = \epsilon * \frac{5}{4(c-b)}$. Then for any $\epsilon > 0$,   $|x_m-x_n|<\epsilon$ whenever $m,n > \frac{5}{2}\epsilon$.",,"['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
67,"If $X$ is a compact set, when does $f'(0)$ exist?","If  is a compact set, when does  exist?",X f'(0),"Let $X$ be a compact set in $\mathbb R$ and for $t\geq 0$ define $f(t)$ as $$f(t) = m(\{x\in \mathbb R| \exists y \in X : |x-y| \leq t\})$$ Under what conditions for $X$ does $f'(0)$ exist? ($m$ is the Lebesgue measure) If $X$ is finite, then $f'(0)$ exist. Also, if $X$ contains an interval, f'(0) does not exist. I also showed that if $X=\{\frac{1}{2^n}, n\in \mathbb N\} \cup \{0\}$, then $f'(0)$ does not exist. If I am correct, this seems very strange to me, because $X$ has measure $0$ and I would think that for the derivative at zero to exist, the nominator $f(t)-f(0) = f(t)-m(X)$ has to go to $0$ faster than the denominator $t$ and the fastest way for this to happen would be if $X$ had measure $0$, so that $f$ doesn't ""measure"" a lot of points. So, now I am at a loss and can't find what other properties $X$ must have. Thanks for any help and I hope I am clear enough in my explanation! UPDATE : With the help of the comments from Niels Diepeveen, I managed to do some work on this, but I am still stuck on a point. My work: Let $\cal{C}$ be the collection of the connected components of $X$. The claim is that $f'(0)$ exists if and only if $\cal{C}$ is finite. $(\Leftarrow$) If $\cal{C}$ is finite, then $\cal{C}$ $= \{X_1, X_2,..., X_k\}$. Each $X_i$ is closed and they are pairwise disjoint, therefore there is a $t_0$ such that $$\forall x \text{ with }  d(x,X_i) \leq t_0 \Rightarrow d(x, X_j) > t_o, \forall j \neq i$$ Additionaly each $X_i$ is measurable as a closed and bounded set. Hence, for $t \leq t_o$, $f(t) = \sum_{i=1}^{k} m(X_i) + 2kt$ which means that $f'(0) = 2k$. $(\Rightarrow)$ Suppose that $f'(0)$ exists but $\cal{C}$ is infinite. Let $M$ be given. Then there is a $k \in \mathbb{N}$ such that $2k>M$. Then, as before, we find a $t_0$ for $k$ of the sets in $\cal{C}$, let them be $\{X_1,...X_k\}$ such that $$\forall x \text{ with }  d(x,X_i) \leq t_0 \Rightarrow d(x, X_j) > t_o, \forall j \neq i, i,j \in \{0,...,k\}$$ Now the problem is, that if I take $t \leq t_o$ I would like to have $$\frac{f(t)-f(0)}{t-0} \geq \frac{f(t_0) - m(X)}{t_0} \geq \frac{m(X) + 2kt_0 -m(X)}{t_0} = 2k > M$$ and then the problem would be solved. But I don't see how I can prove that first inequality (the others are easy). It actually means that the function $\frac{f(t)}{t}$ is decrasing, which seems logical to me, but I can't prove it.","Let $X$ be a compact set in $\mathbb R$ and for $t\geq 0$ define $f(t)$ as $$f(t) = m(\{x\in \mathbb R| \exists y \in X : |x-y| \leq t\})$$ Under what conditions for $X$ does $f'(0)$ exist? ($m$ is the Lebesgue measure) If $X$ is finite, then $f'(0)$ exist. Also, if $X$ contains an interval, f'(0) does not exist. I also showed that if $X=\{\frac{1}{2^n}, n\in \mathbb N\} \cup \{0\}$, then $f'(0)$ does not exist. If I am correct, this seems very strange to me, because $X$ has measure $0$ and I would think that for the derivative at zero to exist, the nominator $f(t)-f(0) = f(t)-m(X)$ has to go to $0$ faster than the denominator $t$ and the fastest way for this to happen would be if $X$ had measure $0$, so that $f$ doesn't ""measure"" a lot of points. So, now I am at a loss and can't find what other properties $X$ must have. Thanks for any help and I hope I am clear enough in my explanation! UPDATE : With the help of the comments from Niels Diepeveen, I managed to do some work on this, but I am still stuck on a point. My work: Let $\cal{C}$ be the collection of the connected components of $X$. The claim is that $f'(0)$ exists if and only if $\cal{C}$ is finite. $(\Leftarrow$) If $\cal{C}$ is finite, then $\cal{C}$ $= \{X_1, X_2,..., X_k\}$. Each $X_i$ is closed and they are pairwise disjoint, therefore there is a $t_0$ such that $$\forall x \text{ with }  d(x,X_i) \leq t_0 \Rightarrow d(x, X_j) > t_o, \forall j \neq i$$ Additionaly each $X_i$ is measurable as a closed and bounded set. Hence, for $t \leq t_o$, $f(t) = \sum_{i=1}^{k} m(X_i) + 2kt$ which means that $f'(0) = 2k$. $(\Rightarrow)$ Suppose that $f'(0)$ exists but $\cal{C}$ is infinite. Let $M$ be given. Then there is a $k \in \mathbb{N}$ such that $2k>M$. Then, as before, we find a $t_0$ for $k$ of the sets in $\cal{C}$, let them be $\{X_1,...X_k\}$ such that $$\forall x \text{ with }  d(x,X_i) \leq t_0 \Rightarrow d(x, X_j) > t_o, \forall j \neq i, i,j \in \{0,...,k\}$$ Now the problem is, that if I take $t \leq t_o$ I would like to have $$\frac{f(t)-f(0)}{t-0} \geq \frac{f(t_0) - m(X)}{t_0} \geq \frac{m(X) + 2kt_0 -m(X)}{t_0} = 2k > M$$ and then the problem would be solved. But I don't see how I can prove that first inequality (the others are easy). It actually means that the function $\frac{f(t)}{t}$ is decrasing, which seems logical to me, but I can't prove it.",,"['real-analysis', 'measure-theory']"
68,Sum-Product Generating Functions,Sum-Product Generating Functions,,"Let $A_n$ be a family of sequences $\{a_i\}_{i=1}^n$ of length $n$. I'll refer to sequence elements of $A_n$ as $a$.  Then define $$G(z):=\sum_{a\in A_n}\prod_{i=1}^n(z+a_i).$$ Here's one possible concrete example. Let $A_n=S_n$, the permutation group on $n$ elements. This is a rather easy example because the product term is the same for all elements of $S_n$ and we get the nice expression $G(z)=n!z^{[n]}$ where $z^{[n]}:=(z+1)\cdots (z+n)$. I'm interested in what information can be extracted from $G(z)$ about $A_n$. In particular, is there a common name or reference for the family of such generating functions of this sum-product form ? For example: $$\lim_{r\rightarrow\infty}\frac{G(r)}{r^n}=|A_n|.$$ In other words we can extract the size of $A_n$.","Let $A_n$ be a family of sequences $\{a_i\}_{i=1}^n$ of length $n$. I'll refer to sequence elements of $A_n$ as $a$.  Then define $$G(z):=\sum_{a\in A_n}\prod_{i=1}^n(z+a_i).$$ Here's one possible concrete example. Let $A_n=S_n$, the permutation group on $n$ elements. This is a rather easy example because the product term is the same for all elements of $S_n$ and we get the nice expression $G(z)=n!z^{[n]}$ where $z^{[n]}:=(z+1)\cdots (z+n)$. I'm interested in what information can be extracted from $G(z)$ about $A_n$. In particular, is there a common name or reference for the family of such generating functions of this sum-product form ? For example: $$\lim_{r\rightarrow\infty}\frac{G(r)}{r^n}=|A_n|.$$ In other words we can extract the size of $A_n$.",,"['real-analysis', 'combinatorics', 'generating-functions']"
69,$\mathbb{R}^{n}$ analysis book that uses Lebesgue integrals,analysis book that uses Lebesgue integrals,\mathbb{R}^{n},"A professor told me it would be unwise to study an $\mathbb{R}^{n}$ analysis book that uses Riemann integral. However, I do not know any book that doesn't. Is there something that covers material similar to Spivak's Calculus on Manifolds, but using Lebesgue integrals instead? I don't think these topics are generally covered in a measure theory book.","A professor told me it would be unwise to study an $\mathbb{R}^{n}$ analysis book that uses Riemann integral. However, I do not know any book that doesn't. Is there something that covers material similar to Spivak's Calculus on Manifolds, but using Lebesgue integrals instead? I don't think these topics are generally covered in a measure theory book.",,"['real-analysis', 'reference-request']"
70,Total Variation and indefinite integrals,Total Variation and indefinite integrals,,"Suppose $f$ is Lebesgue integrable on $[a,b]$ and $F(x) = \int^x_a f(t) dt$, $x \in [a,b]$. Show that $F$ has bounded variation, and the total variation $T^b_a(F)$ satisfies $$ T^b_a(F) = \int^b_a |f(t)|dt. $$ Now, here is what I have so far. Show $F$ is of BV is simple: let $a=x_0 < x_1 < \dots < x_k=b$ be any partition/subdivision of $[a,b]$. Then  $$ \sum^k_{i=1} |F(x_i)-F(x_{i-1})| = \sum^k_{i=1} \left|\int^{x_i}_{x_{i-1}} f(t) dt\right| \leq \sum^k_{i=1} \int^{x_i}_{x_{i-1}} |f(t)| dt = \int^b_a |f(t)| dt. $$ Thus $T^b_a(F) \leq \int^b_a|f(t)| dt < \infty$. Therefore $F$ is of bounded variation. On the other hand, I also know that $F$ is an indefinite integral iff $F$ is absolutely continuous. This would have also implied $F$ is BV since AC $\Rightarrow$ BV. I also know that if $F$ is BV, then $F'$ exists a.e.(almost everywhere). I also have an old homework question I did which showed $F'=f$ a.e. with the assumptions from the start. Thus all I need is that $\int|F'| \leq T^b_a(F)$. I think this works: for every $x \in [a,b]$, we know that $F(x) = P^x_a(F) - N^x_a(F) + F(a)$; P and N are the positive and negative variations respectively. Then $\dfrac{d}{dx} [F(x)] = \dfrac{d}{dx}[P^x_a(F)] - \dfrac{d}{dx}[N^x_a(F)]$ a.e. Thus $|F'| \leq \dfrac{d}{dx}[P^x_a(F)] + \dfrac{d}{dx}[N^x_a(F)] = \dfrac{d}{dx}[T^x_a(F)]$ a.e. Finally  $$ \int^b_a |F'| \leq \int^b_a \dfrac{d}{dx} T^x_a(F) \leq T^b_a(F). $$ Assuming I can show anything I have stated and didn't show proof of, this looks like it works to me. Any comments would be greatly appreciated.","Suppose $f$ is Lebesgue integrable on $[a,b]$ and $F(x) = \int^x_a f(t) dt$, $x \in [a,b]$. Show that $F$ has bounded variation, and the total variation $T^b_a(F)$ satisfies $$ T^b_a(F) = \int^b_a |f(t)|dt. $$ Now, here is what I have so far. Show $F$ is of BV is simple: let $a=x_0 < x_1 < \dots < x_k=b$ be any partition/subdivision of $[a,b]$. Then  $$ \sum^k_{i=1} |F(x_i)-F(x_{i-1})| = \sum^k_{i=1} \left|\int^{x_i}_{x_{i-1}} f(t) dt\right| \leq \sum^k_{i=1} \int^{x_i}_{x_{i-1}} |f(t)| dt = \int^b_a |f(t)| dt. $$ Thus $T^b_a(F) \leq \int^b_a|f(t)| dt < \infty$. Therefore $F$ is of bounded variation. On the other hand, I also know that $F$ is an indefinite integral iff $F$ is absolutely continuous. This would have also implied $F$ is BV since AC $\Rightarrow$ BV. I also know that if $F$ is BV, then $F'$ exists a.e.(almost everywhere). I also have an old homework question I did which showed $F'=f$ a.e. with the assumptions from the start. Thus all I need is that $\int|F'| \leq T^b_a(F)$. I think this works: for every $x \in [a,b]$, we know that $F(x) = P^x_a(F) - N^x_a(F) + F(a)$; P and N are the positive and negative variations respectively. Then $\dfrac{d}{dx} [F(x)] = \dfrac{d}{dx}[P^x_a(F)] - \dfrac{d}{dx}[N^x_a(F)]$ a.e. Thus $|F'| \leq \dfrac{d}{dx}[P^x_a(F)] + \dfrac{d}{dx}[N^x_a(F)] = \dfrac{d}{dx}[T^x_a(F)]$ a.e. Finally  $$ \int^b_a |F'| \leq \int^b_a \dfrac{d}{dx} T^x_a(F) \leq T^b_a(F). $$ Assuming I can show anything I have stated and didn't show proof of, this looks like it works to me. Any comments would be greatly appreciated.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'bounded-variation']"
71,"Denseness of the set $\{f: \int_0^1 x^\alpha f''(x) dx = \int_0^1 x^\beta f''(x) dx = 0 \}$ in $C[0,1]$",Denseness of the set  in,"\{f: \int_0^1 x^\alpha f''(x) dx = \int_0^1 x^\beta f''(x) dx = 0 \} C[0,1]","Let $\alpha, \beta \in (-1,1) \setminus \{ 0 \}$. Is it true that the set $$ \left\{f \in C^2[0,1]: \int_0^1 x^\alpha f''(x) dx = \int_0^1 x^\beta f''(x) dx = 0 \right\} $$ is dense in $C[0,1]$? I think it is not, but have no idea how to prove it. In case $\alpha, \beta \geq 1$ it is quite easy since we can use integration by parts twice and rewrite the condition without derivatives, but I do not know what to do if $\alpha, \beta < 1$.","Let $\alpha, \beta \in (-1,1) \setminus \{ 0 \}$. Is it true that the set $$ \left\{f \in C^2[0,1]: \int_0^1 x^\alpha f''(x) dx = \int_0^1 x^\beta f''(x) dx = 0 \right\} $$ is dense in $C[0,1]$? I think it is not, but have no idea how to prove it. In case $\alpha, \beta \geq 1$ it is quite easy since we can use integration by parts twice and rewrite the condition without derivatives, but I do not know what to do if $\alpha, \beta < 1$.",,"['real-analysis', 'functional-analysis']"
72,Naively estimating the factorial,Naively estimating the factorial,,"A naive way to estimate the factorial is $n! \geq (a+1) (a+2) \dots n \geq a^{n-a}$ for any $a$. For example, it gives $n! \geq (n/2)^{n/2}$ and slightly better $n! \geq (n/3)^{2n/3}$. I am interested in how strong this naive estimation it can be. For which $a$ we get the best estimate? If my calculations are correct, this happens when $a = \frac{n}{W(e n)}$ where $W$ is Lambert W. Can the expression $\left(\frac{n}{W(e n)}\right)^{n-\frac{n}{W(e n)}}$ be simplified, or sensibly estimated by elementary functions?","A naive way to estimate the factorial is $n! \geq (a+1) (a+2) \dots n \geq a^{n-a}$ for any $a$. For example, it gives $n! \geq (n/2)^{n/2}$ and slightly better $n! \geq (n/3)^{2n/3}$. I am interested in how strong this naive estimation it can be. For which $a$ we get the best estimate? If my calculations are correct, this happens when $a = \frac{n}{W(e n)}$ where $W$ is Lambert W. Can the expression $\left(\frac{n}{W(e n)}\right)^{n-\frac{n}{W(e n)}}$ be simplified, or sensibly estimated by elementary functions?",,"['real-analysis', 'estimation', 'lambert-w']"
73,Show sequence ${a_n} = \sqrt[n]{{{3^n} + {5^n}}}$ is monotone decreasing,Show sequence  is monotone decreasing,{a_n} = \sqrt[n]{{{3^n} + {5^n}}},"(a)  Show that sequence ${a_n} = \sqrt[n]{{{3^n} + {5^n}}}$ is monotone decreasing Proof Let ${a_n} = \sqrt[n]{{{3^n} + {5^n}}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^n} + 1} \right]^{\frac{1}{n}}}$  then ${a_k} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}}$ ${a_{k + 1}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ We Know   ${\left( {\frac{3}{5}} \right)^k} \geqslant {\left( {\frac{3}{5}} \right)^{k + 1}}$ ${\left( {\frac{3}{5}} \right)^k} + 1 \geqslant {\left( {\frac{3}{5}} \right)^{k + 1}} + 1$ ${\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{{k + 1}}}} \geqslant {\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ ${\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}} \geqslant {\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{{k + 1}}}} \geqslant $  ${\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ $5{\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}} \geqslant 5{\left [{{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ ${a_k} \geqslant {a_{k + 1}}$ So ${a_n} = \sqrt[n]{{{3^n} + {5^n}}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^n} + 1} \right]^{\frac{1}{n}}}$ is monotone decreasing . I not sure it True or False ,I hope someone help me thank.","(a)  Show that sequence ${a_n} = \sqrt[n]{{{3^n} + {5^n}}}$ is monotone decreasing Proof Let ${a_n} = \sqrt[n]{{{3^n} + {5^n}}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^n} + 1} \right]^{\frac{1}{n}}}$  then ${a_k} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}}$ ${a_{k + 1}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ We Know   ${\left( {\frac{3}{5}} \right)^k} \geqslant {\left( {\frac{3}{5}} \right)^{k + 1}}$ ${\left( {\frac{3}{5}} \right)^k} + 1 \geqslant {\left( {\frac{3}{5}} \right)^{k + 1}} + 1$ ${\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{{k + 1}}}} \geqslant {\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ ${\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}} \geqslant {\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{{k + 1}}}} \geqslant $  ${\left[ {{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ $5{\left[ {{{\left( {\frac{3}{5}} \right)}^k} + 1} \right]^{\frac{1}{k}}} \geqslant 5{\left [{{{\left( {\frac{3}{5}} \right)}^{k + 1}} + 1} \right]^{\frac{1}{{k + 1}}}}$ ${a_k} \geqslant {a_{k + 1}}$ So ${a_n} = \sqrt[n]{{{3^n} + {5^n}}} = 5{\left[ {{{\left( {\frac{3}{5}} \right)}^n} + 1} \right]^{\frac{1}{n}}}$ is monotone decreasing . I not sure it True or False ,I hope someone help me thank.",,['real-analysis']
74,Continuity of max (moving the domain and the function),Continuity of max (moving the domain and the function),,"Let $f:\mathbb{R}\times[0,1]\to\mathbb{R}$ continuous and $c:\mathbb{R}\to[0,1]$ continuous. Consider $$F:\mathbb{R}\to\mathbb{R},\ \ F(x)=\max_{t\in[0,c(x)]}f(x,t)$$ Is $F$ continuous? I believe it is true, but I've difficulties to prove it. I managed to prove that fixing the parameter in one of the two places then the obtained function is continuous, i.e. $$x\mapsto\max_{t\in[0,c(x_0)]}f(x,t) \qquad\text{and}\qquad x\mapsto\max_{t\in[0,c(x)]}f(x_0,t) \qquad\text{are continuous.}$$ But now it seems not trivial to conclude by the triangle inequality...","Let $f:\mathbb{R}\times[0,1]\to\mathbb{R}$ continuous and $c:\mathbb{R}\to[0,1]$ continuous. Consider $$F:\mathbb{R}\to\mathbb{R},\ \ F(x)=\max_{t\in[0,c(x)]}f(x,t)$$ Is $F$ continuous? I believe it is true, but I've difficulties to prove it. I managed to prove that fixing the parameter in one of the two places then the obtained function is continuous, i.e. $$x\mapsto\max_{t\in[0,c(x_0)]}f(x,t) \qquad\text{and}\qquad x\mapsto\max_{t\in[0,c(x)]}f(x_0,t) \qquad\text{are continuous.}$$ But now it seems not trivial to conclude by the triangle inequality...",,"['real-analysis', 'general-topology']"
75,"Uniqueness of Ordinary Differential Equations in $D^{'}$, the space of Schwartz distribuitions","Uniqueness of Ordinary Differential Equations in , the space of Schwartz distribuitions",D^{'},"Let $m \in \mathbb{N}$. For $k=1,...,m$ let $a_k : \mathbb{R} \rightarrow \mathbb{C}$ be a $C^{\infty}$ function. And suppose that: $a_m(x) \neq 0  \; \forall x \in [x_0, \infty[$ And let P be the differential operator: $P(\frac{d}{dx})=\displaystyle{\sum_{k=0}^m} a_k (x) \frac{d^k}{dx^k}$ Given $f \in  C(\mathbb{R},\mathbb{C})$ how do I show that the solution to the following differential equation in $D^{'}([x_0,\infty])$, the space of Schwartz distribuitions is unique? $PU(x)=f(x)$, $\; \; \;$if $x > x_0$, $\; \; \;$ $U^{(k)}(x_0)=0, \; \; k=0,...,m-1$ I've tried proving that the only solution to $PU(x)=0$, $\; \; \;$if $x > x_0$, $\; \; \;$ $U^{(k)}(x_0)=0, \; \; k=0,...,m-1$ Is the null distribuition but I get stuck. Any hint would be appreciated.","Let $m \in \mathbb{N}$. For $k=1,...,m$ let $a_k : \mathbb{R} \rightarrow \mathbb{C}$ be a $C^{\infty}$ function. And suppose that: $a_m(x) \neq 0  \; \forall x \in [x_0, \infty[$ And let P be the differential operator: $P(\frac{d}{dx})=\displaystyle{\sum_{k=0}^m} a_k (x) \frac{d^k}{dx^k}$ Given $f \in  C(\mathbb{R},\mathbb{C})$ how do I show that the solution to the following differential equation in $D^{'}([x_0,\infty])$, the space of Schwartz distribuitions is unique? $PU(x)=f(x)$, $\; \; \;$if $x > x_0$, $\; \; \;$ $U^{(k)}(x_0)=0, \; \; k=0,...,m-1$ I've tried proving that the only solution to $PU(x)=0$, $\; \; \;$if $x > x_0$, $\; \; \;$ $U^{(k)}(x_0)=0, \; \; k=0,...,m-1$ Is the null distribuition but I get stuck. Any hint would be appreciated.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'distribution-theory']"
76,"Bilinear forms on C[0,1]","Bilinear forms on C[0,1]",,"Let $C[0,1]$ be the vector space of real-valued continuous functions on $[0,1]$. Then $$B(f,g) = \int_0^1{f(x)g(x)\, dx}$$ is a bilinear form on $C[0,1]$. More generally, if $k:[0,1]^2\rightarrow \mathbb{R}$ is continuous, then $$B_k(f,g) = \int_{[0,1]^2}{f(x)g(y)k(x,y)\,dx\, dy}$$ is a bilinear form. In Keith Conrad's notes on bilinear forms, he asks if there is a $k$ corresponding to the first example. My intuition first suggested the characteristic function on the diagonal, but that is not continuous. Surely it must be something like this, but I can't think of what it should be. There is another question where he asks us to find conditions on $k$ under which $B_k$ will be a symmetric bilinear form. I sketched a proof that its both necessary and sufficient for $k(x,y) = k(y,x)$, but my proof is not pretty. Is there a natural proof?","Let $C[0,1]$ be the vector space of real-valued continuous functions on $[0,1]$. Then $$B(f,g) = \int_0^1{f(x)g(x)\, dx}$$ is a bilinear form on $C[0,1]$. More generally, if $k:[0,1]^2\rightarrow \mathbb{R}$ is continuous, then $$B_k(f,g) = \int_{[0,1]^2}{f(x)g(y)k(x,y)\,dx\, dy}$$ is a bilinear form. In Keith Conrad's notes on bilinear forms, he asks if there is a $k$ corresponding to the first example. My intuition first suggested the characteristic function on the diagonal, but that is not continuous. Surely it must be something like this, but I can't think of what it should be. There is another question where he asks us to find conditions on $k$ under which $B_k$ will be a symmetric bilinear form. I sketched a proof that its both necessary and sufficient for $k(x,y) = k(y,x)$, but my proof is not pretty. Is there a natural proof?",,"['real-analysis', 'multilinear-algebra', 'bilinear-form']"
77,"""Nearly"" Harmonic Series","""Nearly"" Harmonic Series",,"It's well known that $$ \sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon}} < \infty, \ \forall \varepsilon >0. $$ What happens if we replace $\varepsilon$ with $\varepsilon_n \downarrow 0$? WolframAlpha says  $$ \sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon_n}} $$ converges for $$ \varepsilon_n = \frac{1}{\sqrt{\log(n+1)}} $$ and diverges for $$ \varepsilon_n = \frac{1}{\log(n+1)}. $$ So the question is: can we find a ""borderline"" decrease rate such that the series converge for $\varepsilon_n$ approaching zero slower than it and diverges if $\varepsilon_n$ decreases faster?","It's well known that $$ \sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon}} < \infty, \ \forall \varepsilon >0. $$ What happens if we replace $\varepsilon$ with $\varepsilon_n \downarrow 0$? WolframAlpha says  $$ \sum_{n=1}^{\infty} \frac{1}{n^{1+\varepsilon_n}} $$ converges for $$ \varepsilon_n = \frac{1}{\sqrt{\log(n+1)}} $$ and diverges for $$ \varepsilon_n = \frac{1}{\log(n+1)}. $$ So the question is: can we find a ""borderline"" decrease rate such that the series converge for $\varepsilon_n$ approaching zero slower than it and diverges if $\varepsilon_n$ decreases faster?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
78,Confusion over calculus notation (differentials/derivatives),Confusion over calculus notation (differentials/derivatives),,"I have read from multiple sources that dy/dx is not to be interpreted as a ratio as the idea of 'dy' and 'dx' themselves will lead to logical difficulties. However, I have seen in many areas (e.g. thermodynamics) where notations involving dx etc. are used constantly. For example, in http://en.wikipedia.org/wiki/Total_differential , it seems to tread df/dt as a ratio by multiplying dt on both sides to cancel out the denominator. So in what cases are we allowed to manipulate the notation as if they are quotients?","I have read from multiple sources that dy/dx is not to be interpreted as a ratio as the idea of 'dy' and 'dx' themselves will lead to logical difficulties. However, I have seen in many areas (e.g. thermodynamics) where notations involving dx etc. are used constantly. For example, in http://en.wikipedia.org/wiki/Total_differential , it seems to tread df/dt as a ratio by multiplying dt on both sides to cancel out the denominator. So in what cases are we allowed to manipulate the notation as if they are quotients?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'notation']"
79,Difference of powers inequality,Difference of powers inequality,,"Is there any inequality involving difference of powers?  I want to estimate say $x^p - y^p$, or $|x^p -y^p|$ hopefully in terms of $|x-y|^p$ Edit : $0 < p <\infty$, and I'm sure that there will be cases depending on $p$","Is there any inequality involving difference of powers?  I want to estimate say $x^p - y^p$, or $|x^p -y^p|$ hopefully in terms of $|x-y|^p$ Edit : $0 < p <\infty$, and I'm sure that there will be cases depending on $p$",,"['real-analysis', 'inequality']"
80,"Show that $\lim((\int_{a}^{b}f^{n})^\frac{1}{n})=\sup\{f(x):x\in[a,b]\}$ [duplicate]",Show that  [duplicate],"\lim((\int_{a}^{b}f^{n})^\frac{1}{n})=\sup\{f(x):x\in[a,b]\}","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 11 years ago . Another exercise (this one is 7.2.18) from ""Introduction to Real Analysis"" by Bartle and Sherbert that I'm struggling with: Let $f$ be continuous on $[a,b]$, let $f(x)>=0$ for $x\in[a,b]$, and let $M_{n}:=(\int_{a}^{b}f^{n})^\frac{1}{n}$. Show that $\lim(M_{n})=\sup\{f(x):x\in[a,b]\}$. I think that as $f$ is continuous, thus bounded, on given interval, one should start with representing $M_{n}$ as $(M^{n}\int_{a}^{b}(\frac{f}{M})^{n})^\frac{1}{n}$, where $M$ is essential supremum of $f$ on given interval, and then proceed from there; however, I'm not sure how to formalize further steps... Note that this particular exercise is following exercises on MVT for integrals. So another approach I tried was to follow from the fact that there exists $c_{1},\ldots,c_{n}\in[a,b]$ such that $M_{n}=(f(c_{1})\int_{a}^{b}f^{n-1})^\frac{1}{n}=\ldots=(f(c_{1})\ldots f(c_{n})(b-a))^\frac{1}{n}$, but to no avail.","This question already has answers here : If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$? (2 answers) Closed 11 years ago . Another exercise (this one is 7.2.18) from ""Introduction to Real Analysis"" by Bartle and Sherbert that I'm struggling with: Let $f$ be continuous on $[a,b]$, let $f(x)>=0$ for $x\in[a,b]$, and let $M_{n}:=(\int_{a}^{b}f^{n})^\frac{1}{n}$. Show that $\lim(M_{n})=\sup\{f(x):x\in[a,b]\}$. I think that as $f$ is continuous, thus bounded, on given interval, one should start with representing $M_{n}$ as $(M^{n}\int_{a}^{b}(\frac{f}{M})^{n})^\frac{1}{n}$, where $M$ is essential supremum of $f$ on given interval, and then proceed from there; however, I'm not sure how to formalize further steps... Note that this particular exercise is following exercises on MVT for integrals. So another approach I tried was to follow from the fact that there exists $c_{1},\ldots,c_{n}\in[a,b]$ such that $M_{n}=(f(c_{1})\int_{a}^{b}f^{n-1})^\frac{1}{n}=\ldots=(f(c_{1})\ldots f(c_{n})(b-a))^\frac{1}{n}$, but to no avail.",,['real-analysis']
81,Prove the limit problems,Prove the limit problems,,"I got two problems asking for the proof of the limit: Prove the following limit: $$\sup_{x\ge 0}\ x e^{x^2}\int_x^\infty e^{-t^2} \, dt={1\over 2}.$$ and, Prove the following limit: $$\sup_{x\gt 0}\ x\int_0^\infty {e^{-px}\over {p+1}} \, dp=1.$$ I may feel that these two problems are of the same kind. World anyone please help me with one of them and I may figure out the other one? Many thanks!","I got two problems asking for the proof of the limit: Prove the following limit: $$\sup_{x\ge 0}\ x e^{x^2}\int_x^\infty e^{-t^2} \, dt={1\over 2}.$$ and, Prove the following limit: $$\sup_{x\gt 0}\ x\int_0^\infty {e^{-px}\over {p+1}} \, dp=1.$$ I may feel that these two problems are of the same kind. World anyone please help me with one of them and I may figure out the other one? Many thanks!",,['real-analysis']
82,Strictly monotone functions such that $x= f(\frac{x^2}{f(x)})$ [duplicate],Strictly monotone functions such that  [duplicate],x= f(\frac{x^2}{f(x)}),"This question already has answers here : Find all strictly monotone $f:(0,+\infty) \to (0, +\infty)$ such that $f(\frac{x^2}{f(x)})=x.$ (2 answers) Closed 3 years ago . What are the strictly monotone functions $f\colon (0,\infty)\to (0,\infty)$ which satisfy $x= f(\tfrac{x^2}{f(x)})$ for $x>0$. I cannot find any other than $f(x)=x$.","This question already has answers here : Find all strictly monotone $f:(0,+\infty) \to (0, +\infty)$ such that $f(\frac{x^2}{f(x)})=x.$ (2 answers) Closed 3 years ago . What are the strictly monotone functions $f\colon (0,\infty)\to (0,\infty)$ which satisfy $x= f(\tfrac{x^2}{f(x)})$ for $x>0$. I cannot find any other than $f(x)=x$.",,"['real-analysis', 'functional-equations']"
83,Is the following function continuous at $x = 0$?,Is the following function continuous at ?,x = 0,"Define $f: \mathbb{R} \to \mathbb{R}$ by $$ f(x) = \cases{ x - 1 \ \ \text{ if } x \in \mathbb{Q} \\1 - x \ \ \text{ if } x \not\in \mathbb{Q}. }$$ I'm trying to prove whether or not $f$ is continuous at $x = 0$. Here's my strategy so far. Proof Sketch Construct a sequence of irrationals $\{x_n\}$ such that $\{x_n\} \to 0$ (which can be done as a result of the density of $\mathbb{R} \setminus \mathbb{Q}$ in $\mathbb{R}$). Now examine $\lim_{n \to \infty} f(x_n) = 1 - 0 = 1$. However $f(0) = 0 - 1 = -1$, since $0$ is a rational number. Thus, we have found a sequence converging to $0$ such that $\lim_{n \to \infty} f(x_n) \neq f(0)$, and, as a result, $f$ is discontinuous at $x = 0.$ Update: Proving $f$ continuous at $x = 1$ We wish to show that for every $\epsilon > 0$ there exists $\delta > 0$ such that $$|x - 1| < \delta \implies |f(x) - f(1)| < \epsilon.$$ Since $x = 1$ is rational, $f(1) = x - 1 = 1 - 1= 0$. So, equivalently, we need $$|x - 1| < \delta \implies |f(x)| < \epsilon.$$ We now take two cases. Assume first that $x$ is rational, so $f(x) = x - 1$. Now set $\delta = \epsilon.$ Clearly, $$|x - 1| < \delta = \epsilon \implies |f(x)| = |x - 1| < \epsilon.$$ The next case is similar. Assume $x$ is irrational, so $f(x) = 1 - x.$ Again, set $\delta = \epsilon.$ We have $$|x - 1| < \delta  = \epsilon \implies |x - 1| = |1 - x| < \epsilon.$$","Define $f: \mathbb{R} \to \mathbb{R}$ by $$ f(x) = \cases{ x - 1 \ \ \text{ if } x \in \mathbb{Q} \\1 - x \ \ \text{ if } x \not\in \mathbb{Q}. }$$ I'm trying to prove whether or not $f$ is continuous at $x = 0$. Here's my strategy so far. Proof Sketch Construct a sequence of irrationals $\{x_n\}$ such that $\{x_n\} \to 0$ (which can be done as a result of the density of $\mathbb{R} \setminus \mathbb{Q}$ in $\mathbb{R}$). Now examine $\lim_{n \to \infty} f(x_n) = 1 - 0 = 1$. However $f(0) = 0 - 1 = -1$, since $0$ is a rational number. Thus, we have found a sequence converging to $0$ such that $\lim_{n \to \infty} f(x_n) \neq f(0)$, and, as a result, $f$ is discontinuous at $x = 0.$ Update: Proving $f$ continuous at $x = 1$ We wish to show that for every $\epsilon > 0$ there exists $\delta > 0$ such that $$|x - 1| < \delta \implies |f(x) - f(1)| < \epsilon.$$ Since $x = 1$ is rational, $f(1) = x - 1 = 1 - 1= 0$. So, equivalently, we need $$|x - 1| < \delta \implies |f(x)| < \epsilon.$$ We now take two cases. Assume first that $x$ is rational, so $f(x) = x - 1$. Now set $\delta = \epsilon.$ Clearly, $$|x - 1| < \delta = \epsilon \implies |f(x)| = |x - 1| < \epsilon.$$ The next case is similar. Assume $x$ is irrational, so $f(x) = 1 - x.$ Again, set $\delta = \epsilon.$ We have $$|x - 1| < \delta  = \epsilon \implies |x - 1| = |1 - x| < \epsilon.$$",,"['real-analysis', 'analysis', 'continuity']"
84,Smoothness in $\mathbb{R}^n$,Smoothness in,\mathbb{R}^n,"Embarrasingly simple question, but I got the feeling that I cannot see the forrest for the trees right now: If I have a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and want to show that it is in $C^\infty(\mathbb{R}^2)$, is it enough to show that $\forall (x, y) \in \mathbb{R}^2$ the functions restricted to one of the parameters, $f_x: \mathbb{R} \rightarrow \mathbb{R}, t \mapsto f(x, t)$ and $f_y: \mathbb{R} \rightarrow \mathbb{R}, t \mapsto f(t, y)$ are in $C^\infty(\mathbb{R})$? I have a gut feeling that this could be problematic, but I can't seem to construct a counterexample. Any help would be much appreciated.","Embarrasingly simple question, but I got the feeling that I cannot see the forrest for the trees right now: If I have a function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ and want to show that it is in $C^\infty(\mathbb{R}^2)$, is it enough to show that $\forall (x, y) \in \mathbb{R}^2$ the functions restricted to one of the parameters, $f_x: \mathbb{R} \rightarrow \mathbb{R}, t \mapsto f(x, t)$ and $f_y: \mathbb{R} \rightarrow \mathbb{R}, t \mapsto f(t, y)$ are in $C^\infty(\mathbb{R})$? I have a gut feeling that this could be problematic, but I can't seem to construct a counterexample. Any help would be much appreciated.",,"['real-analysis', 'multivariable-calculus']"
85,Helly's selection theorem (For sequence of monotonic functions),Helly's selection theorem (For sequence of monotonic functions),,"Let $\{f_n\}$ be a sequence of monotonically increasing functions on $\mathbb{R}$. Let $\{f_n\}$ be uniformly bounded on $\mathbb{R}$. Then, there exists a subsequence $\{f_{n_k}\}$ pointwise convergent to some $f$. Now, assume $f$ is continuous on $\mathbb{R}$. Here, I want to prove that $f_{n_k}\rightarrow f$ uniformly on $\mathbb{R}$. How do i prove this ? I have proven that "" $\forall \epsilon>0,\exists K\in\mathbb{N}$ such that $k≧K \Rightarrow \forall x\in\mathbb{R}, |f(x)-f_{n_k}(x)||<\epsilon \bigvee f_{n_k}(x) < \inf f + \epsilon \bigvee \sup f - \epsilon < f_{n_k}(x)$ "". The argument is in the link below. I don't understand why above statement implies ""$f_{n_k}\rightarrow f$ uniformly on $\mathbb{R}$"". Please explain me how.. Reference ; http://www.math.umn.edu/~jodeit/course/SP6S06.pdf Thank you in advance!","Let $\{f_n\}$ be a sequence of monotonically increasing functions on $\mathbb{R}$. Let $\{f_n\}$ be uniformly bounded on $\mathbb{R}$. Then, there exists a subsequence $\{f_{n_k}\}$ pointwise convergent to some $f$. Now, assume $f$ is continuous on $\mathbb{R}$. Here, I want to prove that $f_{n_k}\rightarrow f$ uniformly on $\mathbb{R}$. How do i prove this ? I have proven that "" $\forall \epsilon>0,\exists K\in\mathbb{N}$ such that $k≧K \Rightarrow \forall x\in\mathbb{R}, |f(x)-f_{n_k}(x)||<\epsilon \bigvee f_{n_k}(x) < \inf f + \epsilon \bigvee \sup f - \epsilon < f_{n_k}(x)$ "". The argument is in the link below. I don't understand why above statement implies ""$f_{n_k}\rightarrow f$ uniformly on $\mathbb{R}$"". Please explain me how.. Reference ; http://www.math.umn.edu/~jodeit/course/SP6S06.pdf Thank you in advance!",,['real-analysis']
86,Every open set in $\mathbb{R}$ is the disjoint union of open intervals,Every open set in  is the disjoint union of open intervals,\mathbb{R},"I know this is a standard question and that I can easily find solutions on this site or elsewhere. However, I came up with a proposed proof and would like someone to review it for me. If this is known, my apologies. Let $A_{\alpha}$ be a family of open intervals. Given $\alpha, \alpha'$ we say $A_{\alpha} \sim A_{\alpha'}$ if there exist $\alpha_{1}, ..., \alpha_{n}$ such that $A_{\alpha} \cap A_{\alpha_{1}} \neq \emptyset, ..., A_{\alpha_{n}} \cap A_{\alpha'} \neq \emptyset$. We see that $\sim$ is an equivalence relation. Consider $A$ to be an equivalence class and $F$ to be the union of all elements of $A$. Considering $a = \inf F$, $b = \sup F$ (where $a, b$ take values in the extended reals), we claim $F = (a, b)$. Let $a < x < b$. It suffices to see $x \in F$. This is clear since there exists $\alpha, \alpha'$ with $A_{\alpha}, A_{\alpha'}$ in $A$ such that $A_{\alpha}$ contains points smaller than $x$ (since $x$ is not the infimum) and $A_{\alpha'}$ contains points greater than $x$. Taking $\alpha_{1}, ..., \alpha_{n}$ as in the definition we see that for some $i$, $A_{\alpha_{i}}$ contains $x$. If it is not true that $A_{\alpha} \sim A_{\alpha'}$ then $A_{\alpha} \cap A_{\alpha'} = \emptyset$. Thus each $F$ is disjoint, and the union of $A_{\alpha}$ is the union of the $F$. Therefore any open subset of $\mathbb{R}$ is the union of disjoint open intervals","I know this is a standard question and that I can easily find solutions on this site or elsewhere. However, I came up with a proposed proof and would like someone to review it for me. If this is known, my apologies. Let $A_{\alpha}$ be a family of open intervals. Given $\alpha, \alpha'$ we say $A_{\alpha} \sim A_{\alpha'}$ if there exist $\alpha_{1}, ..., \alpha_{n}$ such that $A_{\alpha} \cap A_{\alpha_{1}} \neq \emptyset, ..., A_{\alpha_{n}} \cap A_{\alpha'} \neq \emptyset$. We see that $\sim$ is an equivalence relation. Consider $A$ to be an equivalence class and $F$ to be the union of all elements of $A$. Considering $a = \inf F$, $b = \sup F$ (where $a, b$ take values in the extended reals), we claim $F = (a, b)$. Let $a < x < b$. It suffices to see $x \in F$. This is clear since there exists $\alpha, \alpha'$ with $A_{\alpha}, A_{\alpha'}$ in $A$ such that $A_{\alpha}$ contains points smaller than $x$ (since $x$ is not the infimum) and $A_{\alpha'}$ contains points greater than $x$. Taking $\alpha_{1}, ..., \alpha_{n}$ as in the definition we see that for some $i$, $A_{\alpha_{i}}$ contains $x$. If it is not true that $A_{\alpha} \sim A_{\alpha'}$ then $A_{\alpha} \cap A_{\alpha'} = \emptyset$. Thus each $F$ is disjoint, and the union of $A_{\alpha}$ is the union of the $F$. Therefore any open subset of $\mathbb{R}$ is the union of disjoint open intervals",,"['real-analysis', 'general-topology']"
87,"Limit of $a_n$ is $0$ iff Limit of $a_n \sin(n t)$ is $0$ for all $t\in[0,1]$",Limit of  is  iff Limit of  is  for all,"a_n 0 a_n \sin(n t) 0 t\in[0,1]","I'd like to prove\begin{align} \lim_{n\rightarrow\infty}a_n=0 \iff \lim_{n\rightarrow\infty}a_n\sin(n t)=0 \quad \forall t\in[0,1]\end{align} Since $\sin$ is bounded one of the implications is trivial. For the other one letting $t=\frac{\pi}{4 l}, l\in\mathbb{N}$ implies $a_{n_k}\rightarrow 0$ for all subsequences satisfying $(n_k)_{k\in\mathbb{N}}\subset\mathbb{N}\setminus 4 l \mathbb{N}$. Does this lead anywhere?","I'd like to prove\begin{align} \lim_{n\rightarrow\infty}a_n=0 \iff \lim_{n\rightarrow\infty}a_n\sin(n t)=0 \quad \forall t\in[0,1]\end{align} Since $\sin$ is bounded one of the implications is trivial. For the other one letting $t=\frac{\pi}{4 l}, l\in\mathbb{N}$ implies $a_{n_k}\rightarrow 0$ for all subsequences satisfying $(n_k)_{k\in\mathbb{N}}\subset\mathbb{N}\setminus 4 l \mathbb{N}$. Does this lead anywhere?",,"['real-analysis', 'sequences-and-series']"
88,Entropy of a Linear Toral Automorphism,Entropy of a Linear Toral Automorphism,,"I'm trying to calculate the entropy of the Linear Toral Automorphism induced by $$f(x,y,z)=(x,y+x,y+z)$$ This is an exercise in the Katok book. This map has all eigenvalues ​​equal to 1. But I do not want to use that $~~ h_{top}(f)= log (max|\lambda_i|)$. would like to use Katok's suggestion that says that the cardinality separate sets grow quadratically with $ n $  where $ n $ is the size of the orbit. But I can not see it clearly.","I'm trying to calculate the entropy of the Linear Toral Automorphism induced by $$f(x,y,z)=(x,y+x,y+z)$$ This is an exercise in the Katok book. This map has all eigenvalues ​​equal to 1. But I do not want to use that $~~ h_{top}(f)= log (max|\lambda_i|)$. would like to use Katok's suggestion that says that the cardinality separate sets grow quadratically with $ n $  where $ n $ is the size of the orbit. But I can not see it clearly.",,"['real-analysis', 'general-topology']"
89,does there exist a discrete set whose image is dense,does there exist a discrete set whose image is dense,,"I want to know whether my proof is correct or not : Does there exist a descrete set whose image is dense in $S^1$ under the map $e^{2\pi ix}$ from $\mathbb{R}\rightarrow S^1$? my attempt is : We know that there is a 1-1 correspondence between $S^1$ and $\displaystyle\frac{\mathbb R}{\mathbb Z}$ that is $x$ corresponds to $e^{2\pi i x}$ We know that the numbers of the form $m+n\sqrt2$ are dense in $\mathbb R$ (I don't quite know how to prove this!). So these numbers are dense even in the interval $[0,1]$ This shows that the equivalence classes of $n\sqrt2$ are dense in $\displaystyle\frac{\mathbb R}{\mathbb Z}$ Therefore the numbers of the form $e^{2\pi ix}$ where $x=n\sqrt2$ are dense in $S^1$ Now $A\subset R := A=\{\sqrt{2}n: n\in\mathbb Z\}$  is a discrete subset of $\mathbb R$ Therefore, if $f:\mathbb R\rightarrow S^1:= x\mapsto e^{2\pi ix}$, then $f(A)$ is dense in $S^1$","I want to know whether my proof is correct or not : Does there exist a descrete set whose image is dense in $S^1$ under the map $e^{2\pi ix}$ from $\mathbb{R}\rightarrow S^1$? my attempt is : We know that there is a 1-1 correspondence between $S^1$ and $\displaystyle\frac{\mathbb R}{\mathbb Z}$ that is $x$ corresponds to $e^{2\pi i x}$ We know that the numbers of the form $m+n\sqrt2$ are dense in $\mathbb R$ (I don't quite know how to prove this!). So these numbers are dense even in the interval $[0,1]$ This shows that the equivalence classes of $n\sqrt2$ are dense in $\displaystyle\frac{\mathbb R}{\mathbb Z}$ Therefore the numbers of the form $e^{2\pi ix}$ where $x=n\sqrt2$ are dense in $S^1$ Now $A\subset R := A=\{\sqrt{2}n: n\in\mathbb Z\}$  is a discrete subset of $\mathbb R$ Therefore, if $f:\mathbb R\rightarrow S^1:= x\mapsto e^{2\pi ix}$, then $f(A)$ is dense in $S^1$",,['real-analysis']
90,"Is the $ L^{p}$$[0,1]$ norm continuous in p?",Is the  norm continuous in p?," L^{p} [0,1]","I ran into the following problem when I was doing my homework, and I have no thoughts on where I should start with: (1) If $f\in L^{2}$, show that $\displaystyle \lim_{p \rightarrow 1^{+}}\int_{[0,1]}|f|^{p}=\int_{[0,1]}|f|$ (2) If $0<p$, show that $\displaystyle \lim_{q\rightarrow p^{-}}||f||_{q}=||f||_{p}$ My first thought was Generalized LDCT, but it didn't seem to work. I also made some other attempts but none of them were successful... Can anybody give me some hints on how I should look at this question? Also, I know if $p\rightarrow\infty$ then $||f||_{p}\rightarrow||f||_{\infty}$ on $[0,1]$, but does similar continuity in p holds for other $L^{p}[0,1]$ norms in general? Thank you! Edit: Sorry if I did not make it clear enough in the question. All $L^{p}$ refers to $L^p[0,1]$. The first question is found here (thanks to t.b.), but the second question remains, mainly because $f$ is not guaranteed to be in any $L^{p}$.","I ran into the following problem when I was doing my homework, and I have no thoughts on where I should start with: (1) If $f\in L^{2}$, show that $\displaystyle \lim_{p \rightarrow 1^{+}}\int_{[0,1]}|f|^{p}=\int_{[0,1]}|f|$ (2) If $0<p$, show that $\displaystyle \lim_{q\rightarrow p^{-}}||f||_{q}=||f||_{p}$ My first thought was Generalized LDCT, but it didn't seem to work. I also made some other attempts but none of them were successful... Can anybody give me some hints on how I should look at this question? Also, I know if $p\rightarrow\infty$ then $||f||_{p}\rightarrow||f||_{\infty}$ on $[0,1]$, but does similar continuity in p holds for other $L^{p}[0,1]$ norms in general? Thank you! Edit: Sorry if I did not make it clear enough in the question. All $L^{p}$ refers to $L^p[0,1]$. The first question is found here (thanks to t.b.), but the second question remains, mainly because $f$ is not guaranteed to be in any $L^{p}$.",,"['real-analysis', 'measure-theory', 'limits']"
91,different phrasings of L'Hôpital's rule,different phrasings of L'Hôpital's rule,,"We've got three different phrasings of L'Hôpital's rule and got a little bit confused about the subtle differences between them. A Let $-\infty \leq a < b \leq +\infty$ and let $f,g:]a,b[ \to \mathbb{R}$ be two functions, differentiable on $]a,b[$, and let $g'(x) \not= 0 \forall x \in ]a,b[$. If further more on of the following cases applies Case 1. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = 0$. Case 2. $\lim\limits_{x \to a^+} g(x) = \pm \infty$. and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists, then we have $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. B Let $-\infty \leq a < b \leq +\infty$ and let $f,g:]a,b[ \to \mathbb{R}$ be two functions, differentiable on $]a,b[$, and let $g'(x) \not= 0 \forall x \in [a,b]$. If further more on of the following cases applies Case 1. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = 0$. Case 2. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = \pm \infty$. and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists, then we have $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. C Let $a,b \in \mathbb{R}$ with $a < b$ and let $f,g:[a,b] \to \mathbb{R}$ be two functions, differentiable on $[a,b]$, and let $g'(x) \not= 0 \forall x \in [a,b]$. If further more $f(a) = g(a) = 0$ and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists,then we have $g(x) \not= 0 \forall x \in ]a,b]$ and $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. A and B seem to be nearly the same A is lacking the condition for $\lim f(x)$ in case 2 - isn't this necessary? B requires $g'(x) \not= 0$ on the closed set $[a,b]$ - why is this? $g$ itself isn't even defined on $[a,b]$ and actually we're not working on the extended reals so the following leads to trouble $g'(x) \not= 0 \forall x \in [-\infty,+\infty]$. C is somehow different Since $a$ isn't allowed to be $-\infty$ and $b$ is not allowed to be $+\infty$, C is not including the same functions as A and B. It requires differentiability and $g'(x) \not= 0$ on the closed set $[a,b]$ in contrast to A and B. It lacks the second case. So C only helps us, if we can't calculate the limit of $\frac{f(x)}{g(x)}$ since $f(x)=0$ and $g(x)=0$. We additionally get information about $g(x)$ on the half-open interval $]a,b]$. Maybe someone can help explaining the subtle differences and how the actually come into account. Further more I noticed, that the rule is considering $\lim\limits_{x \to a+}$ - when applying this rule we often just write $\lim\limits_{x \to a}$. This is just being not completely strict?","We've got three different phrasings of L'Hôpital's rule and got a little bit confused about the subtle differences between them. A Let $-\infty \leq a < b \leq +\infty$ and let $f,g:]a,b[ \to \mathbb{R}$ be two functions, differentiable on $]a,b[$, and let $g'(x) \not= 0 \forall x \in ]a,b[$. If further more on of the following cases applies Case 1. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = 0$. Case 2. $\lim\limits_{x \to a^+} g(x) = \pm \infty$. and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists, then we have $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. B Let $-\infty \leq a < b \leq +\infty$ and let $f,g:]a,b[ \to \mathbb{R}$ be two functions, differentiable on $]a,b[$, and let $g'(x) \not= 0 \forall x \in [a,b]$. If further more on of the following cases applies Case 1. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = 0$. Case 2. $\lim\limits_{x \to a^+} f(x) = \lim\limits_{x \to a^+} g(x) = \pm \infty$. and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists, then we have $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. C Let $a,b \in \mathbb{R}$ with $a < b$ and let $f,g:[a,b] \to \mathbb{R}$ be two functions, differentiable on $[a,b]$, and let $g'(x) \not= 0 \forall x \in [a,b]$. If further more $f(a) = g(a) = 0$ and $L := \lim\limits_{x \to a^+} \frac{f'(x)}{g'(x)}$ exists,then we have $g(x) \not= 0 \forall x \in ]a,b]$ and $\lim\limits_{x \to a^+} \frac{f(x)}{g(x)} = L$. A and B seem to be nearly the same A is lacking the condition for $\lim f(x)$ in case 2 - isn't this necessary? B requires $g'(x) \not= 0$ on the closed set $[a,b]$ - why is this? $g$ itself isn't even defined on $[a,b]$ and actually we're not working on the extended reals so the following leads to trouble $g'(x) \not= 0 \forall x \in [-\infty,+\infty]$. C is somehow different Since $a$ isn't allowed to be $-\infty$ and $b$ is not allowed to be $+\infty$, C is not including the same functions as A and B. It requires differentiability and $g'(x) \not= 0$ on the closed set $[a,b]$ in contrast to A and B. It lacks the second case. So C only helps us, if we can't calculate the limit of $\frac{f(x)}{g(x)}$ since $f(x)=0$ and $g(x)=0$. We additionally get information about $g(x)$ on the half-open interval $]a,b]$. Maybe someone can help explaining the subtle differences and how the actually come into account. Further more I noticed, that the rule is considering $\lim\limits_{x \to a+}$ - when applying this rule we often just write $\lim\limits_{x \to a}$. This is just being not completely strict?",,"['real-analysis', 'limits', 'convergence-divergence', 'derivatives']"
92,Is this set corresponding to a bounded linear operator necessarily open?,Is this set corresponding to a bounded linear operator necessarily open?,,"Let $\Lambda : X \to X$ be a bounded linear operator on a Banach space $X$. My question is whether the set $$ \{\lambda \in \mathbb C: \lambda I - \Lambda \quad\text{is surjective} \} $$ is necessarily open. The above set is similar to the resolvent set of $\Lambda$, which is defined to be the set of all $\lambda \in \mathbb C$ such that $\lambda I - \Lambda$ is invertible; I know that the resolvent set is always open. However, what about the set above? For reference, it was a problem on a past qualifying exam (see problem 6 ) to prove that the set is in fact open. I'm not sure if they meant to indicate the resolvent set, or if the problem is correct as stated.","Let $\Lambda : X \to X$ be a bounded linear operator on a Banach space $X$. My question is whether the set $$ \{\lambda \in \mathbb C: \lambda I - \Lambda \quad\text{is surjective} \} $$ is necessarily open. The above set is similar to the resolvent set of $\Lambda$, which is defined to be the set of all $\lambda \in \mathbb C$ such that $\lambda I - \Lambda$ is invertible; I know that the resolvent set is always open. However, what about the set above? For reference, it was a problem on a past qualifying exam (see problem 6 ) to prove that the set is in fact open. I'm not sure if they meant to indicate the resolvent set, or if the problem is correct as stated.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
93,integral of Laplacian of a positive function,integral of Laplacian of a positive function,,"I've encountered the following, rather elementary, problem: $K$ is a compact subset of some 2-dimensional oriented manifold with smooth boundary, $f$ is a positive smooth function on $K$ that vanishes on the border $\partial K$. I want to show that the integral of the Laplacian of $f$ is non-positive, where Laplacian is given by $\Delta f = (f_{xx} + f_{yy}) dx \wedge dy$ in local coordinates. It seems that one can use Stokes to compute: $$ \int_K \Delta f = \int_{\partial K} \nabla f \cdot \mathbf{n} \ dl \leq 0 $$ where $\mathbf{n}$ is the normal vector and the inequality follows from the fact that since ""$f$ decreases in the direction of the boundary"", on the boundary we have $\nabla f \cdot \mathbf{n} \leq 0$. I was wondering if there is some more elegant reason for the inequality in question. Also: is the given argument correct (my analysis is a bit rusty ;) ) ?","I've encountered the following, rather elementary, problem: $K$ is a compact subset of some 2-dimensional oriented manifold with smooth boundary, $f$ is a positive smooth function on $K$ that vanishes on the border $\partial K$. I want to show that the integral of the Laplacian of $f$ is non-positive, where Laplacian is given by $\Delta f = (f_{xx} + f_{yy}) dx \wedge dy$ in local coordinates. It seems that one can use Stokes to compute: $$ \int_K \Delta f = \int_{\partial K} \nabla f \cdot \mathbf{n} \ dl \leq 0 $$ where $\mathbf{n}$ is the normal vector and the inequality follows from the fact that since ""$f$ decreases in the direction of the boundary"", on the boundary we have $\nabla f \cdot \mathbf{n} \leq 0$. I was wondering if there is some more elegant reason for the inequality in question. Also: is the given argument correct (my analysis is a bit rusty ;) ) ?",,"['real-analysis', 'analysis', 'differential-geometry', 'integration']"
94,Relationship between Hölder continuity and differentiability of Brownian motion,Relationship between Hölder continuity and differentiability of Brownian motion,,"I came across the following exercise: Let $(B_t)_{t\geq 0}$ be a Brownian motion. Show that, almost surely, there is no interval $(r,s)$ on which $t\to B_t$ is Hölder continuous of exponent $\alpha$ for any $\alpha>\frac{1}{2}$. Explain the relation of this result to the differentiability properties of $B$. I'm happy about the first part, but am wondering how this relates to the differentiability of $B$. This property alone isn't strong enough to ensure that the paths are nowhere differentiable (which they are). Does it perhaps imply that $B$ is almost surely not differentiable on any open interval? So it would be informative to answer the question: If $f:\mathbb{R}\to\mathbb{R}$ is not Lipschitz on any open interval, then does every open interval contain a point at which $f$ is non-differentiable? Thank you.","I came across the following exercise: Let $(B_t)_{t\geq 0}$ be a Brownian motion. Show that, almost surely, there is no interval $(r,s)$ on which $t\to B_t$ is Hölder continuous of exponent $\alpha$ for any $\alpha>\frac{1}{2}$. Explain the relation of this result to the differentiability properties of $B$. I'm happy about the first part, but am wondering how this relates to the differentiability of $B$. This property alone isn't strong enough to ensure that the paths are nowhere differentiable (which they are). Does it perhaps imply that $B$ is almost surely not differentiable on any open interval? So it would be informative to answer the question: If $f:\mathbb{R}\to\mathbb{R}$ is not Lipschitz on any open interval, then does every open interval contain a point at which $f$ is non-differentiable? Thank you.",,"['real-analysis', 'stochastic-processes']"
95,Cantor diagonalization method for subsequences,Cantor diagonalization method for subsequences,,"I'm confused about constructing a family of subsequence using a diagonalization procedure. Often I see the following argument during a proof: ""using a diagonalization procedure we can assume....."" What exactly is meant by this? I just know Cantor's diagonalization method for proving some cardinality stuff. Unfortunately I don't see how this apply in this situation. I'm aware of asking a very general thing, but hopefully it's clear what I try to ask. Just to be sure that there's no misunderstanding, here is an example: Suppose $X$ is a separable space and $(\phi_k)_{k\in \mathbb{N}}$ is a bounded subset of the dual of $X$ (topological dual). Then there is a subsequence $\Omega \subset \mathbb{N}$ and an element $\phi \in X^* $ such that $ \phi_k $ converges weak$^*$ to $\phi$ for this subsequence. Now in the proof, we argue: let $(x_n)$ be dense in $X$. Then looking at $ \phi_k(x_1) $. This is bounded sequence in $ \mathbb{R} $ therefore it exists a subset $\Omega_1 \subset \mathbb{N} $ such that $\phi_k(x_1)$ converges for $ k\in \Omega_1$. Repeating this for ever $n \in \mathbb{N}$ we get an decreasing family of subsets (subsequence) of $\mathbb{N}$, $$\mathbb{N}\supset \Omega_1 \supset \dots \supset \Omega_l \dots \supset \Omega$$ Why can I do this? Should I choose a subsequence of $\Omega_l$ using Bolzano-Weierstrass to get $\Omega_{l+1}$? Why do I know there's a $\Omega$ ? Why do I know that $\lim_k\phi_k(x_n)$ converges for ever $n\in \mathbb{N}$ using $\Omega$? Are there any assumption to apply this diagonalization argument? How does the diagonalization argument works in this example? I'm very thankful for your help. If you need further explanations on what I exactly want to ask, let me know.","I'm confused about constructing a family of subsequence using a diagonalization procedure. Often I see the following argument during a proof: ""using a diagonalization procedure we can assume....."" What exactly is meant by this? I just know Cantor's diagonalization method for proving some cardinality stuff. Unfortunately I don't see how this apply in this situation. I'm aware of asking a very general thing, but hopefully it's clear what I try to ask. Just to be sure that there's no misunderstanding, here is an example: Suppose $X$ is a separable space and $(\phi_k)_{k\in \mathbb{N}}$ is a bounded subset of the dual of $X$ (topological dual). Then there is a subsequence $\Omega \subset \mathbb{N}$ and an element $\phi \in X^* $ such that $ \phi_k $ converges weak$^*$ to $\phi$ for this subsequence. Now in the proof, we argue: let $(x_n)$ be dense in $X$. Then looking at $ \phi_k(x_1) $. This is bounded sequence in $ \mathbb{R} $ therefore it exists a subset $\Omega_1 \subset \mathbb{N} $ such that $\phi_k(x_1)$ converges for $ k\in \Omega_1$. Repeating this for ever $n \in \mathbb{N}$ we get an decreasing family of subsets (subsequence) of $\mathbb{N}$, $$\mathbb{N}\supset \Omega_1 \supset \dots \supset \Omega_l \dots \supset \Omega$$ Why can I do this? Should I choose a subsequence of $\Omega_l$ using Bolzano-Weierstrass to get $\Omega_{l+1}$? Why do I know there's a $\Omega$ ? Why do I know that $\lim_k\phi_k(x_n)$ converges for ever $n\in \mathbb{N}$ using $\Omega$? Are there any assumption to apply this diagonalization argument? How does the diagonalization argument works in this example? I'm very thankful for your help. If you need further explanations on what I exactly want to ask, let me know.",,['real-analysis']
96,Slope definition of $e$,Slope definition of,e,"A common definition of $e$ is given as $$e = \lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{n}$$ which can be proven to be equivalent to $$e=\lim_{h\rightarrow 0}\ \left(1+h\right)^{\frac{1}{h}}$$ The most practical use of $e$ in elementary calculus is however given as $$1=\lim_{h\rightarrow 0}\frac{e^h - 1}{h}$$ which is used as a statement the slope of $e^x$ at $x=0$ is $1$ allowing one to prove that $\frac{d}{dx}e^x=e^x$. It appears trivial to prove that the two limits given above are equivalent, but I cannot seem to make any progress without making some illegal limit operations. I suspect the problem is deeper than it appears (I suspect the trouble is that although we have defined $e$, we have not actually said anything about what $e^x$ is). How does one rigorously proceed from the given definition of $e$ to the slope limit?","A common definition of $e$ is given as $$e = \lim_{n\rightarrow\infty}\left(1+\frac{1}{n}\right)^{n}$$ which can be proven to be equivalent to $$e=\lim_{h\rightarrow 0}\ \left(1+h\right)^{\frac{1}{h}}$$ The most practical use of $e$ in elementary calculus is however given as $$1=\lim_{h\rightarrow 0}\frac{e^h - 1}{h}$$ which is used as a statement the slope of $e^x$ at $x=0$ is $1$ allowing one to prove that $\frac{d}{dx}e^x=e^x$. It appears trivial to prove that the two limits given above are equivalent, but I cannot seem to make any progress without making some illegal limit operations. I suspect the problem is deeper than it appears (I suspect the trouble is that although we have defined $e$, we have not actually said anything about what $e^x$ is). How does one rigorously proceed from the given definition of $e$ to the slope limit?",,"['calculus', 'real-analysis', 'limits', 'exponentiation']"
97,Extension of real analytic map on the unit circle,Extension of real analytic map on the unit circle,,"Given a real-analytic map $f : \mathbb{S}^1 \rightarrow \mathbb{S}^1$, where $$\mathbb{S}^1 = \{z \in \mathbb{C} : |z| = 1\},$$ does it admit a complex-analytic extension $\tilde{f} : U \rightarrow V$, where $U$ and $V$ are open subsets of $\mathbb{C}$ containing $\mathbb{S}^1$? If so, how can you prove it? I'd appreciate a proof as elementary as possible (but complete). Thanks.","Given a real-analytic map $f : \mathbb{S}^1 \rightarrow \mathbb{S}^1$, where $$\mathbb{S}^1 = \{z \in \mathbb{C} : |z| = 1\},$$ does it admit a complex-analytic extension $\tilde{f} : U \rightarrow V$, where $U$ and $V$ are open subsets of $\mathbb{C}$ containing $\mathbb{S}^1$? If so, how can you prove it? I'd appreciate a proof as elementary as possible (but complete). Thanks.",,"['real-analysis', 'complex-analysis', 'circles']"
98,Limits of Monotone Functions,Limits of Monotone Functions,,"I've been studying about limits of functions using Introduction to Analysis by Gaughan.  A few days ago I asked this question Limits of Functions about the limits of functions. The motivation was curiosity about whether the idea of proving that a function has a limit at a given point could be generalized to proving that a given function had limits at all points in it's domain.  After reading the next section of the book, Limits of Monotone Functions , I have another question along the same lines. In that section the author gives the following theorem and support lemma: Lemma: Let $f : [\alpha, \beta] \to \mathbb R$ be increasing. Let $U(x) = \inf\{f(y) : x < y\}$ and $L(x) = \sup\{f(y) : y  < x\}$ for $x \in (\alpha, \beta)$ . Then $f$ has a limit at $x_0\in (\alpha,\beta)$ iff $U(x_0) = L(x_0)$ , and in this case $\lim f(x) = f(x_0) = U(x_0) = L(x_0)$ . Theorem: Let $f : [\alpha, \beta]\to \mathbb R$ be monotone. Then $$D = \{x : x \in (\alpha, \beta)\text{ and }f\text{ does not have a limit at }x\}$$ is countable. If $f$ has a limit at $x_0 \in (\alpha,\beta)$ then $\lim f(x) = f(x_0)$ . The author goes on to say the idea is that a monotone function will have a limit everywhere except possibly at a countable set of points.  My question is: if you restrict the domain of a function, as he did in the text, can this be applied to functions that aren't strictly monotone?  As an example, $x^2$ is monotone on $[0, \infty)$ . Based on my understanding, you could use this to show that a limit exists for all points in say $[1,3]$ .  While not exactly what I was looking for in my first post, this would go a long way towards that end.","I've been studying about limits of functions using Introduction to Analysis by Gaughan.  A few days ago I asked this question Limits of Functions about the limits of functions. The motivation was curiosity about whether the idea of proving that a function has a limit at a given point could be generalized to proving that a given function had limits at all points in it's domain.  After reading the next section of the book, Limits of Monotone Functions , I have another question along the same lines. In that section the author gives the following theorem and support lemma: Lemma: Let be increasing. Let and for . Then has a limit at iff , and in this case . Theorem: Let be monotone. Then is countable. If has a limit at then . The author goes on to say the idea is that a monotone function will have a limit everywhere except possibly at a countable set of points.  My question is: if you restrict the domain of a function, as he did in the text, can this be applied to functions that aren't strictly monotone?  As an example, is monotone on . Based on my understanding, you could use this to show that a limit exists for all points in say .  While not exactly what I was looking for in my first post, this would go a long way towards that end.","f : [\alpha, \beta] \to \mathbb R U(x) = \inf\{f(y) : x < y\} L(x) = \sup\{f(y) : y  < x\} x \in (\alpha, \beta) f x_0\in (\alpha,\beta) U(x_0) = L(x_0) \lim f(x) = f(x_0) = U(x_0) = L(x_0) f : [\alpha, \beta]\to \mathbb R D = \{x : x \in (\alpha, \beta)\text{ and }f\text{ does not have a limit at }x\} f x_0 \in (\alpha,\beta) \lim f(x) = f(x_0) x^2 [0, \infty) [1,3]",['real-analysis']
99,"Completing the differential equation from exercise 10.23 in Tom Apostol's ""Mathematical Analysis""","Completing the differential equation from exercise 10.23 in Tom Apostol's ""Mathematical Analysis""",,"I found this answer , outlining the exercise, to be interesting.  However, I have trouble solving the differential equation. The question starts by attempting to solve the following integral without complex analysis: $$\int_0^\infty\frac{\cos\;x}{1+x^2}\mathrm{d}x$$ ...So we let $$ F(y) = \int\limits_{0}^{\infty} \frac{\sin xy}{x(1+x^2)} \ dx \ \ \text{for} \quad\quad y > 0$$ Next, the portion I'm referring to (from which this question proceeds) starts with $$\displaystyle F''(y) - F(y) + \pi/2 = 0$$ I find part of a solution to the differential equation to be $$F(y)=\pi/2+e^y c_1 + e^{-y}c_2$$ I'm having trouble finding the constants.  Could someone please explain this step in great detail, as I'm somewhat of a novice.","I found this answer , outlining the exercise, to be interesting.  However, I have trouble solving the differential equation. The question starts by attempting to solve the following integral without complex analysis: $$\int_0^\infty\frac{\cos\;x}{1+x^2}\mathrm{d}x$$ ...So we let $$ F(y) = \int\limits_{0}^{\infty} \frac{\sin xy}{x(1+x^2)} \ dx \ \ \text{for} \quad\quad y > 0$$ Next, the portion I'm referring to (from which this question proceeds) starts with $$\displaystyle F''(y) - F(y) + \pi/2 = 0$$ I find part of a solution to the differential equation to be $$F(y)=\pi/2+e^y c_1 + e^{-y}c_2$$ I'm having trouble finding the constants.  Could someone please explain this step in great detail, as I'm somewhat of a novice.",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
