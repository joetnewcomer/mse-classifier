,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Showing $\int_{0}^{2\pi}\cos(x)\cos(2x)\cos(3x)\,dx = \frac\pi2$",Showing,"\int_{0}^{2\pi}\cos(x)\cos(2x)\cos(3x)\,dx = \frac\pi2","An integral from MIT Integration Bee: Show that $$I = \int_{0}^{2\pi}\cos(x)\cos(2x) \cos(3x)\,dx = \frac\pi2$$ This integral appeared in the 2019 paper. Below is my own solution: $$\begin{align} I &= \int_{0}^{2\pi}\cos(x)\cos(2x)(\cos x\cos2x-\sin x\sin2x)\,dx \\[6pt] &= \int_{0}^{2\pi}\cos^2(x)\cos^2(2x) \,dx -\int_{0}^{2\pi}\cos(x)\cos(2x)\sin(x)\sin(2x)\, dx \end{align}$$ Replacing $\cos^2(x)= \frac{1+\cos(2x)}{2}$ for the first integral and $\sin(x)\cos(x)= \sin(2x)/2 $ for the second, we get $$\begin{align} &\int_{0}^{2\pi}\frac{1+\cos(2x)}{2}\cos^2(2x) dx-\frac{1}{2}\int_{0}^{2\pi}\cos(2x)\sin^2(2x) dx \\[6pt] =\; &\frac{1}{2}\left(\int_{0}^{2\pi}\cos^2(2x)dx \, + \int_{0}^{2\pi}\cos(2x)\cos(4x)dx  \right) \\[6pt] =\; &\frac{1}{2} \left( \pi + 0\right) \qquad \text{$\because$ the orthogonality of $\cos(mx)$} \\[6pt] =\; &\frac\pi2 \end{align}$$ This solution is rather awkward, and I'm sure there's a better and faster approach to this integral. Could anyone provide a more elegant solution(or a sketch of it)? Thanks.","An integral from MIT Integration Bee: Show that This integral appeared in the 2019 paper. Below is my own solution: Replacing for the first integral and for the second, we get This solution is rather awkward, and I'm sure there's a better and faster approach to this integral. Could anyone provide a more elegant solution(or a sketch of it)? Thanks.","I = \int_{0}^{2\pi}\cos(x)\cos(2x)
\cos(3x)\,dx = \frac\pi2 \begin{align}
I &= \int_{0}^{2\pi}\cos(x)\cos(2x)(\cos x\cos2x-\sin x\sin2x)\,dx \\[6pt]
&= \int_{0}^{2\pi}\cos^2(x)\cos^2(2x) \,dx -\int_{0}^{2\pi}\cos(x)\cos(2x)\sin(x)\sin(2x)\, dx
\end{align} \cos^2(x)= \frac{1+\cos(2x)}{2} \sin(x)\cos(x)= \sin(2x)/2  \begin{align}
&\int_{0}^{2\pi}\frac{1+\cos(2x)}{2}\cos^2(2x) dx-\frac{1}{2}\int_{0}^{2\pi}\cos(2x)\sin^2(2x) dx \\[6pt]
=\; &\frac{1}{2}\left(\int_{0}^{2\pi}\cos^2(2x)dx \, + \int_{0}^{2\pi}\cos(2x)\cos(4x)dx  \right) \\[6pt]
=\; &\frac{1}{2} \left( \pi + 0\right) \qquad \text{\because the orthogonality of \cos(mx)} \\[6pt]
=\; &\frac\pi2
\end{align}","['calculus', 'integration', 'definite-integrals']"
1,"How to evaluate $\int_{0}^{+\infty}\exp(-ax^2-\frac b{x^2})\,dx$ for $a,b>0$",How to evaluate  for,"\int_{0}^{+\infty}\exp(-ax^2-\frac b{x^2})\,dx a,b>0","How can I evaluate $$I=\int_{0}^{+\infty}\!e^{-ax^2-\frac b{x^2}}\,dx$$ for $a,b>0$? My methods: Let $a,b > 0$ and let $$I(b)=\int_{0}^{+\infty}e^{-ax^2-\frac b{x^2}}\,dx.$$ Then $$I'(b)=\int_{0}^{\infty}-\frac{1}{x^2}e^{-ax^2-\frac b{x^2}}\,dx.$$ What the other methods that can I use to evaluate it? Thank you.","How can I evaluate $$I=\int_{0}^{+\infty}\!e^{-ax^2-\frac b{x^2}}\,dx$$ for $a,b>0$? My methods: Let $a,b > 0$ and let $$I(b)=\int_{0}^{+\infty}e^{-ax^2-\frac b{x^2}}\,dx.$$ Then $$I'(b)=\int_{0}^{\infty}-\frac{1}{x^2}e^{-ax^2-\frac b{x^2}}\,dx.$$ What the other methods that can I use to evaluate it? Thank you.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
2,what is sine of a real number,what is sine of a real number,,"I never understand what the trigonometric function sine is.. We had a table that has values of sine for different angles, we by hearted it and applied to some problems and there ends the matter. Till then, sine function is related to triangles, angles. Then comes the graph. We have been told that the figure below is the graph of the function sine. This function takes angles and gives numbers between $-1$ and $1$ and we have been told that it is a continuous function as it is clear from the graph. Then comes taylor expansion of sine and we have $$\sin (x)=x-\frac{x^3}{3!}+\cdots$$ I know that for any infinitely differentiable function, we have taylor expansion. But how do we define differentiability of the function sine? We define differentiability of a function from real numbers to real numbers.. But sine is a function that takes angles and gives real numbers.. Then how do we define differentiability of such a function? are we saying the real number $1$ is the degree 1? I am confused.. Help me.. Above content is a copy paste of a mail i received from my friend, a 1 st year undergraduate. I could answer some things vaguely i am not happy with my own answers. So, I am posting it here.. Help us (me and my friend) to understand sine function in a better way.","I never understand what the trigonometric function sine is.. We had a table that has values of sine for different angles, we by hearted it and applied to some problems and there ends the matter. Till then, sine function is related to triangles, angles. Then comes the graph. We have been told that the figure below is the graph of the function sine. This function takes angles and gives numbers between $-1$ and $1$ and we have been told that it is a continuous function as it is clear from the graph. Then comes taylor expansion of sine and we have $$\sin (x)=x-\frac{x^3}{3!}+\cdots$$ I know that for any infinitely differentiable function, we have taylor expansion. But how do we define differentiability of the function sine? We define differentiability of a function from real numbers to real numbers.. But sine is a function that takes angles and gives real numbers.. Then how do we define differentiability of such a function? are we saying the real number $1$ is the degree 1? I am confused.. Help me.. Above content is a copy paste of a mail i received from my friend, a 1 st year undergraduate. I could answer some things vaguely i am not happy with my own answers. So, I am posting it here.. Help us (me and my friend) to understand sine function in a better way.",,"['calculus', 'trigonometry']"
3,Evaluate the integral $\int_0^{\infty} \left(\frac{\log x \arctan x}{x}\right)^2 \ dx$,Evaluate the integral,\int_0^{\infty} \left(\frac{\log x \arctan x}{x}\right)^2 \ dx,"Some rumours point out that the integral you see might be evaluated in a  straightforward way. But rumours are sometimes just rumours. Could you confirm/refute it? $$ \int_0^{\infty}\left[\frac{\log\left(x\right)\arctan\left(x\right)}{x}\right]^{2} \,{\rm d}x $$ EDIT W|A tells the integral evaluates $0$ but this is not true. Then how do I exactly compute it?","Some rumours point out that the integral you see might be evaluated in a  straightforward way. But rumours are sometimes just rumours. Could you confirm/refute it? $$ \int_0^{\infty}\left[\frac{\log\left(x\right)\arctan\left(x\right)}{x}\right]^{2} \,{\rm d}x $$ EDIT W|A tells the integral evaluates $0$ but this is not true. Then how do I exactly compute it?",,"['calculus', 'integration']"
4,Is every injective function invertible?,Is every injective function invertible?,,"Is every injective function invertible? How could I prove such thing? (Or is it just a necessary but not sufficient condition?) If $f:A\rightarrow B$ is injective then $f(a) = f(b) \Rightarrow a = b$ for all $a,b\in A$. If $f(x)=y$ is invertible, there is some function $g(y)=x$. I don't see how to put these things together, so help would be appreciated.","Is every injective function invertible? How could I prove such thing? (Or is it just a necessary but not sufficient condition?) If $f:A\rightarrow B$ is injective then $f(a) = f(b) \Rightarrow a = b$ for all $a,b\in A$. If $f(x)=y$ is invertible, there is some function $g(y)=x$. I don't see how to put these things together, so help would be appreciated.",,"['calculus', 'functions', 'elementary-set-theory']"
5,"Evaluating $\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}}\, \mathrm{d}x$",Evaluating,"\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}}\, \mathrm{d}x","I have to evaluate: $$\int_{0}^{\pi/2}\frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}}\, \mathrm{d}x. $$ I can't get the right answer! So please help me out!","I have to evaluate: $$\int_{0}^{\pi/2}\frac{\sqrt{\sin x}}{\sqrt{\sin x}+\sqrt{\cos x}}\, \mathrm{d}x. $$ I can't get the right answer! So please help me out!",,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
6,"Evaluate the integral $\int^{\frac{\pi}{2}}_0 \frac{\sin^3x}{\sin^3x+\cos^3x}\,\mathrm dx$. [duplicate]",Evaluate the integral . [duplicate],"\int^{\frac{\pi}{2}}_0 \frac{\sin^3x}{\sin^3x+\cos^3x}\,\mathrm dx","This question already has answers here : How to compute $\int_0^{\pi/2}\frac{\sin^3 t}{\sin^3 t+\cos^3 t}dt$? (4 answers) Closed 10 years ago . Evaluate the integral $$\int^{\frac{\pi}{2}}_0 \frac{\sin^3x}{\sin^3x+\cos^3x}\, \mathrm dx.$$ How can i evaluate this one? Didn't find any clever substitute and integration by parts doesn't lead anywhere (I think). Any guidelines please?","This question already has answers here : How to compute $\int_0^{\pi/2}\frac{\sin^3 t}{\sin^3 t+\cos^3 t}dt$? (4 answers) Closed 10 years ago . Evaluate the integral $$\int^{\frac{\pi}{2}}_0 \frac{\sin^3x}{\sin^3x+\cos^3x}\, \mathrm dx.$$ How can i evaluate this one? Didn't find any clever substitute and integration by parts doesn't lead anywhere (I think). Any guidelines please?",,"['calculus', 'integration', 'definite-integrals']"
7,An interesting definite integral $\int_0^1(1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1})^2 (1+4x+7x^2+\cdot\cdot\cdot+(3n-2)x^{n-1})~dx=n^3$,An interesting definite integral,\int_0^1(1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1})^2 (1+4x+7x^2+\cdot\cdot\cdot+(3n-2)x^{n-1})~dx=n^3,"How to prove $~~ \forall n\in\mathbb{N}^+$ , \begin{align}I_n=\int_0^1(1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1})^2 (1+4x+7x^2+\cdot\cdot\cdot+(3n-2)x^{n-1})~dx=n^3.\end{align} My Try: Define $\displaystyle S(n)=\sum_{k=0}^{n-1}x^k=1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1}=\frac{x^n-1}{x-1}$ . Then, \begin{align}\frac{d}{dx}S(n)=S'(n)=1+2x+3x^2+\cdot\cdot\cdot(n-1)x^{n-2}=\sum_{k=0}^{n-1}kx^{k-1}.\end{align} Therefore, \begin{align} I_n&=\int_0^1 S^2(n)\left(3S'(n+1)-2S(n)\right)~dx\\ &=3\int_0^1 S^2(n)S'(n+1)~dx-2\int_0^1 S^3(n)~dx\\ &=3\int_0^1 S^2(n)(S'(n)+nx^{n-1})~dx-2\int_0^1 S^3(n)~dx\\ &=3\int_0^1 S^2(n)~d(S(n))+3\int_0^1 S^2(n)(nx^{n-1})~dx-2\int_0^1 S^3(n)~dx\\ &=n^3-1+\int_0^1 S^2(n)(3nx^{n-1}-2S(n))~dx\\ &=n^3-1+\int_0^1 \left(\frac{x^n-1}{x-1}\right)^2\left(3nx^{n-1}-2\cdot\frac{x^n-1}{x-1}\right)~dx \end{align} So the question becomes: Prove \begin{align}I'=\int_0^1 \left(\frac{x^n-1}{x-1}\right)^2\left(3nx^{n-1}-2\cdot\frac{x^n-1}{x-1}\right)~dx=1.\end{align} \begin{align}I'&=\int_0^1 \frac{3nx^{n-1}(x^n-1)^2}{(x-1)^2}-\frac{2(x^n-1)^3}{(x-1)^3}~dx\\ &=\int_0^1 \frac{(x-1)^2\left(\frac d {dx} (x^n-1)^3\right)-2(x^n-1)^3(x-1)}{(x-1)^4}~dx\\ &=\int_0^1 \frac d {dx} \left(\frac{(x^n-1)^3}{(x-1)^2}\right)~dx\\ &=\lim_{x \to 1} \frac{(x^n-1)^3}{(x-1)^2}-\frac{(0^n-1)^3}{(0-1)^2}\\ \end{align} $$\therefore I'=1.$$ \begin{align}\therefore I_n=n^3.\end{align} There MUST be other BETTER ways evaluating $I_n$ . Could anyone give me some better solutions? Thanks.","How to prove , My Try: Define . Then, Therefore, So the question becomes: Prove There MUST be other BETTER ways evaluating . Could anyone give me some better solutions? Thanks.","~~ \forall n\in\mathbb{N}^+ \begin{align}I_n=\int_0^1(1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1})^2 (1+4x+7x^2+\cdot\cdot\cdot+(3n-2)x^{n-1})~dx=n^3.\end{align} \displaystyle S(n)=\sum_{k=0}^{n-1}x^k=1+x+x^2+x^3+\cdot\cdot\cdot+x^{n-1}=\frac{x^n-1}{x-1} \begin{align}\frac{d}{dx}S(n)=S'(n)=1+2x+3x^2+\cdot\cdot\cdot(n-1)x^{n-2}=\sum_{k=0}^{n-1}kx^{k-1}.\end{align} \begin{align}
I_n&=\int_0^1 S^2(n)\left(3S'(n+1)-2S(n)\right)~dx\\
&=3\int_0^1 S^2(n)S'(n+1)~dx-2\int_0^1 S^3(n)~dx\\
&=3\int_0^1 S^2(n)(S'(n)+nx^{n-1})~dx-2\int_0^1 S^3(n)~dx\\
&=3\int_0^1 S^2(n)~d(S(n))+3\int_0^1 S^2(n)(nx^{n-1})~dx-2\int_0^1 S^3(n)~dx\\ &=n^3-1+\int_0^1 S^2(n)(3nx^{n-1}-2S(n))~dx\\
&=n^3-1+\int_0^1 \left(\frac{x^n-1}{x-1}\right)^2\left(3nx^{n-1}-2\cdot\frac{x^n-1}{x-1}\right)~dx
\end{align} \begin{align}I'=\int_0^1 \left(\frac{x^n-1}{x-1}\right)^2\left(3nx^{n-1}-2\cdot\frac{x^n-1}{x-1}\right)~dx=1.\end{align} \begin{align}I'&=\int_0^1 \frac{3nx^{n-1}(x^n-1)^2}{(x-1)^2}-\frac{2(x^n-1)^3}{(x-1)^3}~dx\\
&=\int_0^1 \frac{(x-1)^2\left(\frac d {dx} (x^n-1)^3\right)-2(x^n-1)^3(x-1)}{(x-1)^4}~dx\\
&=\int_0^1 \frac d {dx} \left(\frac{(x^n-1)^3}{(x-1)^2}\right)~dx\\
&=\lim_{x \to 1} \frac{(x^n-1)^3}{(x-1)^2}-\frac{(0^n-1)^3}{(0-1)^2}\\
\end{align} \therefore I'=1. \begin{align}\therefore I_n=n^3.\end{align} I_n","['calculus', 'integration', 'definite-integrals']"
8,Is there a(n elementary) function whose derivative we cannot integrate?,Is there a(n elementary) function whose derivative we cannot integrate?,,"Say, for example, I take a reasonably-complicated function $f(x)=\tanh[\ln(x^x)]$, and differentiate it to get $$f'(x)=\frac{4x^{2x} [1+\ln(x)]}{(x^{2x}+1)^2}.$$ Now, to integrate this, I imagine, would be very difficult and time-consuming. My question is: does there exist a function $f$ whose derivative $f'$ we can't integrate (without differentiating $f$ in the first place), using substitution, parts and/or partial fractions (or other integration methods)? My motivation for this is that it's very easy to differentiate even a ridiculously-complicated function (using the chain and/or product rule), but I've often wondered whether we could get back to the original function by integrating this derivative. If I'm not articulating myself clearly enough, please ask me to explain further. Thanks! Edit I know, from the Fundamental Theorem of Calculus, that such a function can be integrated, but, other than knowing the fact that this is the derivative of a suitable function, could it be impossible to reverse-engineer the problem, to get $f'$ back to $f$ (using only methods of integration)?","Say, for example, I take a reasonably-complicated function $f(x)=\tanh[\ln(x^x)]$, and differentiate it to get $$f'(x)=\frac{4x^{2x} [1+\ln(x)]}{(x^{2x}+1)^2}.$$ Now, to integrate this, I imagine, would be very difficult and time-consuming. My question is: does there exist a function $f$ whose derivative $f'$ we can't integrate (without differentiating $f$ in the first place), using substitution, parts and/or partial fractions (or other integration methods)? My motivation for this is that it's very easy to differentiate even a ridiculously-complicated function (using the chain and/or product rule), but I've often wondered whether we could get back to the original function by integrating this derivative. If I'm not articulating myself clearly enough, please ask me to explain further. Thanks! Edit I know, from the Fundamental Theorem of Calculus, that such a function can be integrated, but, other than knowing the fact that this is the derivative of a suitable function, could it be impossible to reverse-engineer the problem, to get $f'$ back to $f$ (using only methods of integration)?",,"['calculus', 'soft-question']"
9,"Intuition for a physical real line vs. a physical ""hyperreal line""","Intuition for a physical real line vs. a physical ""hyperreal line""",,"As a mathematical structure, I have no problem with the hyperreals. But I came across the following from Keisler's book ""Elementary Calculus: An Infinitesimal Approach"". ""We have no way of knowing what a line in physical space is really   like. It might be like the hyperreal line, the real line, or neither.   However, in applications of the calculus, it is helpful to imagine a   line in physical space as a hyperreal line."" We get real number answers to physical problems in distance, from integers to transcendental numbers. However, we never get a nonzero number $w$ s.t. $w \lt 1/k, \forall k \in \mathbb N$ coming from a physical calculation, theoretical or applied. Isn't this what makes physical distances real and not hyperreal? Is it really possible that our space was never (locally) Euclidean all this time? Where are these infinitesmals and why are they hiding? Secondly, we say the real line, by construction from completing the rationals, has ""no holes"". Yet, the reals are a proper subfield of the hyperreals. Where do these infinitesmals ""fit"" on the real line to make a hyperreal line when there is no room for them to fit? In other words, if we begin by assuming a physical line segment is a hyperreal line segment and then (mathematically) remove all the infinitesmals, we get a hyperreal line segment with ""holes"" in the form of missing hyperreal points, but this just gives a real line segment, which has no holes. There seems to be problems in assuming physical lines can be hyperreal lines.","As a mathematical structure, I have no problem with the hyperreals. But I came across the following from Keisler's book ""Elementary Calculus: An Infinitesimal Approach"". ""We have no way of knowing what a line in physical space is really   like. It might be like the hyperreal line, the real line, or neither.   However, in applications of the calculus, it is helpful to imagine a   line in physical space as a hyperreal line."" We get real number answers to physical problems in distance, from integers to transcendental numbers. However, we never get a nonzero number $w$ s.t. $w \lt 1/k, \forall k \in \mathbb N$ coming from a physical calculation, theoretical or applied. Isn't this what makes physical distances real and not hyperreal? Is it really possible that our space was never (locally) Euclidean all this time? Where are these infinitesmals and why are they hiding? Secondly, we say the real line, by construction from completing the rationals, has ""no holes"". Yet, the reals are a proper subfield of the hyperreals. Where do these infinitesmals ""fit"" on the real line to make a hyperreal line when there is no room for them to fit? In other words, if we begin by assuming a physical line segment is a hyperreal line segment and then (mathematically) remove all the infinitesmals, we get a hyperreal line segment with ""holes"" in the form of missing hyperreal points, but this just gives a real line segment, which has no holes. There seems to be problems in assuming physical lines can be hyperreal lines.",,"['calculus', 'soft-question', 'nonstandard-analysis', 'infinitesimals']"
10,Why use the derivative and not the symmetric derivative?,Why use the derivative and not the symmetric derivative?,,"The symmetric derivative is always equal to the regular derivative when it exists, and still isn't defined for jump discontinuities. From what I can tell the only differences are that a symmetric derivative will give the 'expected slope' for removable discontinuities, and the average slope at cusps. These seem like extremely reasonable quantities to work with (especially the former), so I'm wondering why the 'typical' derivative isn't taken to be this one. What advantage is there to taking $\lim\limits_{h\to0}\frac{f(x+h)-f(x)} h$ as the main quantity of interest instead? Why would we want to use the one that's defined less often?","The symmetric derivative is always equal to the regular derivative when it exists, and still isn't defined for jump discontinuities. From what I can tell the only differences are that a symmetric derivative will give the 'expected slope' for removable discontinuities, and the average slope at cusps. These seem like extremely reasonable quantities to work with (especially the former), so I'm wondering why the 'typical' derivative isn't taken to be this one. What advantage is there to taking $\lim\limits_{h\to0}\frac{f(x+h)-f(x)} h$ as the main quantity of interest instead? Why would we want to use the one that's defined less often?",,"['calculus', 'functions', 'derivatives']"
11,Limit of $\frac{x^{x^x}}{x}$ as $x\to 0^+$,Limit of  as,\frac{x^{x^x}}{x} x\to 0^+,"I've encountered the following problem: Evaluate $$\lim_{x\to 0^+}\cfrac{x^{x^x}}{x}.$$ This is readily a ""$\frac{0}{0}$"" form, so I used L'Hopital's rule, but it got seriously messy, and fast. Can anyone recommend an alternative approach?","I've encountered the following problem: Evaluate $$\lim_{x\to 0^+}\cfrac{x^{x^x}}{x}.$$ This is readily a ""$\frac{0}{0}$"" form, so I used L'Hopital's rule, but it got seriously messy, and fast. Can anyone recommend an alternative approach?",,"['calculus', 'limits']"
12,Finding $\int_0^{\frac{\pi}{2}}\arctan\left(\sin x\right)dx$,Finding,\int_0^{\frac{\pi}{2}}\arctan\left(\sin x\right)dx,"$$\int_0^{\frac{\pi}{2}}\arctan\left(\sin x\right)dx$$ I try to solve it, but failed. Who can help me to find it? I encountered this integral when trying to solve $\displaystyle{\int_0^\pi\frac{x\cos(x)}{1+\sin^2(x)}\,dx}$.","$$\int_0^{\frac{\pi}{2}}\arctan\left(\sin x\right)dx$$ I try to solve it, but failed. Who can help me to find it? I encountered this integral when trying to solve $\displaystyle{\int_0^\pi\frac{x\cos(x)}{1+\sin^2(x)}\,dx}$.",,"['calculus', 'integration', 'definite-integrals']"
13,"If integral is zero and function is continuous and non negative, then what about the function? [duplicate]","If integral is zero and function is continuous and non negative, then what about the function? [duplicate]",,"This question already has answers here : Prove the integral of $f$ is positive if $f ≥ 0$, $f$ continuous at $x_0$ and $f(x_0)>0$ (2 answers) Closed 10 years ago . If $f$ is continuous on $[a,b]$, $f(x)≥0$ on $[a,b]$ and $$\int_{a}^{b} f(x) =0$$ then prove that $f(x)=0$ for all $x \in [a,b]$. I tried with Riemann's definite integral definition but couldn't proceed","This question already has answers here : Prove the integral of $f$ is positive if $f ≥ 0$, $f$ continuous at $x_0$ and $f(x_0)>0$ (2 answers) Closed 10 years ago . If $f$ is continuous on $[a,b]$, $f(x)≥0$ on $[a,b]$ and $$\int_{a}^{b} f(x) =0$$ then prove that $f(x)=0$ for all $x \in [a,b]$. I tried with Riemann's definite integral definition but couldn't proceed",,"['calculus', 'integration']"
14,Integrate : $\int \frac{x^2}{(x\cos x -\sin x)(x\sin x +\cos x)}dx$,Integrate :,\int \frac{x^2}{(x\cos x -\sin x)(x\sin x +\cos x)}dx,$$\int \frac{x^2}{(x\cos x -\sin x)(x\sin x +\cos x)}\ dx$$ My approach : Dividing the denominator by $\cos^2x$ we get $\dfrac{x^2\sec^2x }{(x -\tan x)(x\tan x +1)}$ then $$\int \frac{x^2\sec^2x}{x^2\tan x -x\tan^2x+x-\tan x}\ dx=\int \frac{x^2(1+\tan^2x)}{x^2\tan x -x\tan^2x+x-\tan x}dx$$ But I am not getting any relation between numerator and denominator so that I will take any substitution and solve further please suggest whether it is correct and how to proceed in this. Thanks.,$$\int \frac{x^2}{(x\cos x -\sin x)(x\sin x +\cos x)}\ dx$$ My approach : Dividing the denominator by $\cos^2x$ we get $\dfrac{x^2\sec^2x }{(x -\tan x)(x\tan x +1)}$ then $$\int \frac{x^2\sec^2x}{x^2\tan x -x\tan^2x+x-\tan x}\ dx=\int \frac{x^2(1+\tan^2x)}{x^2\tan x -x\tan^2x+x-\tan x}dx$$ But I am not getting any relation between numerator and denominator so that I will take any substitution and solve further please suggest whether it is correct and how to proceed in this. Thanks.,,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
15,"Closed form solution to $\int_0^1\arctan^2(x)\,\sqrt{x}\,dx$",Closed form solution to,"\int_0^1\arctan^2(x)\,\sqrt{x}\,dx","I need to compute this integral: $$\int_0^1\arctan^2(x)\,\sqrt{x}\,dx$$ I tried integration by parts, and also introducing a parameter $\arctan(a\,x)$ and differentiation wrt it, but these approaches did not lead to anything useful. Please help.","I need to compute this integral: $$\int_0^1\arctan^2(x)\,\sqrt{x}\,dx$$ I tried integration by parts, and also introducing a parameter $\arctan(a\,x)$ and differentiation wrt it, but these approaches did not lead to anything useful. Please help.",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'trigonometry']"
16,Calculate $\sum\limits_{k=0}^{\infty}\frac{1}{{2k \choose k}}$,Calculate,\sum\limits_{k=0}^{\infty}\frac{1}{{2k \choose k}},Calculate $$\sum \limits_{k=0}^{\infty}\frac{1}{{2k \choose k}}$$ I use software to complete the  series is  $\frac{2}{27} \left(18+\sqrt{3} \pi \right)$ I have no idea about it.  :|,Calculate $$\sum \limits_{k=0}^{\infty}\frac{1}{{2k \choose k}}$$ I use software to complete the  series is  $\frac{2}{27} \left(18+\sqrt{3} \pi \right)$ I have no idea about it.  :|,,"['calculus', 'sequences-and-series', 'number-theory', 'analytic-number-theory']"
17,What is the difference between continuous derivative and derivative?,What is the difference between continuous derivative and derivative?,,"What is the difference between continuous derivative and derivative? According to my teacher's solution to the assignment, it seems there exits a difference between continuous derivative and derivative. However, aunt Google does not tell me what I want. Edit: Here is a example. $$f(x) = \begin{cases} k & \text{if }x=0 \\ \frac{1-\cos(2x)}{x} & \text{otherwise} \end{cases}$$ Is $f$ continuous but not having continuous derivative at $0$ ? Thanks:)","What is the difference between continuous derivative and derivative? According to my teacher's solution to the assignment, it seems there exits a difference between continuous derivative and derivative. However, aunt Google does not tell me what I want. Edit: Here is a example. Is continuous but not having continuous derivative at ? Thanks:)","f(x)
= \begin{cases}
k & \text{if }x=0 \\
\frac{1-\cos(2x)}{x} & \text{otherwise}
\end{cases} f 0","['calculus', 'derivatives']"
18,Evaluating $\sum_{n \geq 1}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right)$,Evaluating,\sum_{n \geq 1}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right),"Is there a direct way to evaluate the following series? $$ \sum_{n=1}^{\infty}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right)=\frac12\ln^2 2. \tag1 $$ I've tried telescoping sums unsuccessfully. The convergence is clear. Given the simplicity of the result, I'm inclined to think it might exist an elegant way to get $(1)$.","Is there a direct way to evaluate the following series? $$ \sum_{n=1}^{\infty}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right)=\frac12\ln^2 2. \tag1 $$ I've tried telescoping sums unsuccessfully. The convergence is clear. Given the simplicity of the result, I'm inclined to think it might exist an elegant way to get $(1)$.",,"['calculus', 'integration', 'sequences-and-series', 'logarithms', 'closed-form']"
19,"$xf(y)+yf(x)\leq 1$ for all $x,y\in[0,1]$ implies $\int_0^1 f(x) \,dx\leq\frac{\pi}{4}$",for all  implies,"xf(y)+yf(x)\leq 1 x,y\in[0,1] \int_0^1 f(x) \,dx\leq\frac{\pi}{4}","I want to show that if $f\colon [0,1]\to\mathbb{R}$ is continuous and  $xf(y)+yf(x)\leq 1$ for all $x,y\in[0,1]$ then we have the following inequality: $$\int_0^1 f(x) \, dx\leq\frac{\pi}{4}.$$ The $\pi$ on the right hand side suggests we have to do something with a geometric function. Letting $f(x) = \frac{1}{1+x^2}$ we have equality but this function does not satisfy $xf(y)+yf(x)\leq 1$.","I want to show that if $f\colon [0,1]\to\mathbb{R}$ is continuous and  $xf(y)+yf(x)\leq 1$ for all $x,y\in[0,1]$ then we have the following inequality: $$\int_0^1 f(x) \, dx\leq\frac{\pi}{4}.$$ The $\pi$ on the right hand side suggests we have to do something with a geometric function. Letting $f(x) = \frac{1}{1+x^2}$ we have equality but this function does not satisfy $xf(y)+yf(x)\leq 1$.",,"['calculus', 'contest-math', 'problem-solving']"
20,Integral $\int_{0}^1\frac{\ln\frac{3+x}{3-x}}{\sqrt{x(1-x)}}dx$,Integral,\int_{0}^1\frac{\ln\frac{3+x}{3-x}}{\sqrt{x(1-x)}}dx,"I have a problem with the following integral: $$ \int_{0}^{1}\ln\left(\,3 + x \over 3 - x\,\right)\, {{\rm d}x \over \,\sqrt{\,x\left(\,1 - x\,\right)\,}\,} $$ The first idea was to use the integration by parts because $$ \int{{\rm d}x \over \,\sqrt{x\left(\,1 - x\,\right)\,}\,} =\arcsin\left(\,2x - 1\,\right) + C $$ but what would be the next step is not clear. Another idea would be expand $\ln\left(\,\cdot\right)$ into Taylor series but it seems to be even worse option. So, what are the other options?","I have a problem with the following integral: $$ \int_{0}^{1}\ln\left(\,3 + x \over 3 - x\,\right)\, {{\rm d}x \over \,\sqrt{\,x\left(\,1 - x\,\right)\,}\,} $$ The first idea was to use the integration by parts because $$ \int{{\rm d}x \over \,\sqrt{x\left(\,1 - x\,\right)\,}\,} =\arcsin\left(\,2x - 1\,\right) + C $$ but what would be the next step is not clear. Another idea would be expand $\ln\left(\,\cdot\right)$ into Taylor series but it seems to be even worse option. So, what are the other options?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
21,Ordinary generating function for $\binom{3n}{n}$,Ordinary generating function for,\binom{3n}{n},"The ordinary generating function for the central binomial coefficients, that is, $$\displaystyle \sum_{n=0}^{\infty} \binom{2n}{n} x^{n} = \frac{1}{\sqrt{1-4x}} \, , \quad |x| < \frac{1}{4},$$ can be derived by using the duplication formula for the gamma function and the generalized binomial theorem. But what about the ordinary generating function for $ \displaystyle  \binom{3n}{n}$ ? According to Wolfram Alpha, $$ \sum_{n=0}^{\infty} \binom{3n}{n} x^{n} = \frac{2\cos \left(\frac{1}{3} \arcsin \left(\frac{3 \sqrt{3x}}{2} \right)\right)}{\sqrt{4-27x}} \, , \quad |x| < \frac{4}{27}. $$ Any suggestions on how to prove this? EDIT : Approaching this problem using the fact that $$ \text{Res} \Big[ \frac{(1+z)^{3n}}{z^{n+1}},0 \Big] = \binom{3n}{n},$$ I get $$ \sum_{n=0}^{\infty} \binom{3n}{n} x^{n} = -\frac{1}{2 \pi i x} \int_{C} \frac{dz}{z^{3}+3z^{2}+3z - \frac{z}{x}+1},$$ where $C$ is a circle centered at $z=0$ such that every point on the circle satisfies $ \displaystyle\Big|\frac{x(1+z)^{3}}{z} \Big| < 1$ . Evaluating that contour integral would appear to be quite difficult.","The ordinary generating function for the central binomial coefficients, that is, can be derived by using the duplication formula for the gamma function and the generalized binomial theorem. But what about the ordinary generating function for ? According to Wolfram Alpha, Any suggestions on how to prove this? EDIT : Approaching this problem using the fact that I get where is a circle centered at such that every point on the circle satisfies . Evaluating that contour integral would appear to be quite difficult.","\displaystyle \sum_{n=0}^{\infty} \binom{2n}{n} x^{n} = \frac{1}{\sqrt{1-4x}} \, , \quad |x| < \frac{1}{4},  \displaystyle  \binom{3n}{n}  \sum_{n=0}^{\infty} \binom{3n}{n} x^{n} = \frac{2\cos \left(\frac{1}{3} \arcsin \left(\frac{3 \sqrt{3x}}{2} \right)\right)}{\sqrt{4-27x}} \, , \quad |x| < \frac{4}{27}.   \text{Res} \Big[ \frac{(1+z)^{3n}}{z^{n+1}},0 \Big] = \binom{3n}{n},  \sum_{n=0}^{\infty} \binom{3n}{n} x^{n} = -\frac{1}{2 \pi i x} \int_{C} \frac{dz}{z^{3}+3z^{2}+3z - \frac{z}{x}+1}, C z=0  \displaystyle\Big|\frac{x(1+z)^{3}}{z} \Big| < 1","['calculus', 'sequences-and-series', 'binomial-coefficients', 'generating-functions']"
22,Settle a classroom argument - do there exist any functions that satisfy this property involving Taylor polynomials?,Settle a classroom argument - do there exist any functions that satisfy this property involving Taylor polynomials?,,"I'm going to apologize in advance; I might at some points say Taylor series instead of Maclaurin series. OK, so backstory: My calculus class recently went over Taylor series and Taylor polynomials. It seemed basic enough. Using the ratio test we were able to prove the radius of convergence of these series as well. For example, we derived that: $$ e^x = \sum_{n=0}^\infty\dfrac{x^n}{n!} $$ using the ratio test we can find that the series converges $\forall x$ However, today we had a substitute that talked about Taylor's theorem and Taylor's formula defined as the sum of an $n$th order Taylor polynomial plus the remainder. $$ f(x) = P_n(x) + R(x) $$ $$ R(x) = \dfrac{f^{n+1}(c)(x-a)^{n+1}}{(n+1)!} $$ The substitute teacher then told us that in order to prove that the Taylor polynomial converges to the original function, you must show that  $$ \lim_{n\rightarrow\infty}R(x)=0 $$ Well, after this statement the flood gates opened with a few students asking why you can't just use the ratio test to show the Taylor series converges $\forall x$ like we did for $e^x$. The substitute said that the ratio test only proved convergence, while this proved it converged to the actual function. The students then said that if we already proved that the Taylor series is the function at an infinite amount of points, if the series converges, doesn't that mean that it converges to the function? We had already done an example previously in class where: $$ f(x)=\begin{cases} 0,&\text{ if }x=0;\\ e^{-\frac{1}{x^2}},&\text{ if }x\neq 0. \end{cases} $$ This function's Taylor polynomial converges to 0 at every point. However, it doesn't converge to the function at every point. My classmates said this was a cop-out and ""didn't count"" because it was a piecewise function. So is there an example of a function whose Taylor polynomial converges on some interval, but does not converge to the function entirely on that interval? Also a proof would be cool if you could explain why the students or the teacher were wrong.","I'm going to apologize in advance; I might at some points say Taylor series instead of Maclaurin series. OK, so backstory: My calculus class recently went over Taylor series and Taylor polynomials. It seemed basic enough. Using the ratio test we were able to prove the radius of convergence of these series as well. For example, we derived that: $$ e^x = \sum_{n=0}^\infty\dfrac{x^n}{n!} $$ using the ratio test we can find that the series converges $\forall x$ However, today we had a substitute that talked about Taylor's theorem and Taylor's formula defined as the sum of an $n$th order Taylor polynomial plus the remainder. $$ f(x) = P_n(x) + R(x) $$ $$ R(x) = \dfrac{f^{n+1}(c)(x-a)^{n+1}}{(n+1)!} $$ The substitute teacher then told us that in order to prove that the Taylor polynomial converges to the original function, you must show that  $$ \lim_{n\rightarrow\infty}R(x)=0 $$ Well, after this statement the flood gates opened with a few students asking why you can't just use the ratio test to show the Taylor series converges $\forall x$ like we did for $e^x$. The substitute said that the ratio test only proved convergence, while this proved it converged to the actual function. The students then said that if we already proved that the Taylor series is the function at an infinite amount of points, if the series converges, doesn't that mean that it converges to the function? We had already done an example previously in class where: $$ f(x)=\begin{cases} 0,&\text{ if }x=0;\\ e^{-\frac{1}{x^2}},&\text{ if }x\neq 0. \end{cases} $$ This function's Taylor polynomial converges to 0 at every point. However, it doesn't converge to the function at every point. My classmates said this was a cop-out and ""didn't count"" because it was a piecewise function. So is there an example of a function whose Taylor polynomial converges on some interval, but does not converge to the function entirely on that interval? Also a proof would be cool if you could explain why the students or the teacher were wrong.",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'taylor-expansion', 'uniform-convergence']"
23,The shortest distance between any two distinct points is the line segment joining them.How can I see why this is true?,The shortest distance between any two distinct points is the line segment joining them.How can I see why this is true?,,"On a euclidean plane, the shortest distance between any two distinct points is the line segment joining them. How can I see why this is true?","On a euclidean plane, the shortest distance between any two distinct points is the line segment joining them. How can I see why this is true?",,"['calculus', 'general-topology', 'geometry', 'euclidean-geometry', 'analytic-geometry']"
24,Two individuals are walking around a cylindrical tower. What is the probability that they can see each other?,Two individuals are walking around a cylindrical tower. What is the probability that they can see each other?,,"It'd be of the greatest interest to have not only a rigorous solution, but also an intuitive insight onto this simple yet very difficult problem: Let there exist some tower which has the shape of a cylinder and whose   radius is A. Further, let this tower be surrounded by a walking lane   whose width is B. Now, there are two individuals who are on the walk;   what is the probability that they're able to see each other?","It'd be of the greatest interest to have not only a rigorous solution, but also an intuitive insight onto this simple yet very difficult problem: Let there exist some tower which has the shape of a cylinder and whose   radius is A. Further, let this tower be surrounded by a walking lane   whose width is B. Now, there are two individuals who are on the walk;   what is the probability that they're able to see each other?",,"['calculus', 'probability', 'plane-geometry', 'solid-geometry', 'geometric-probability']"
25,Evaluating $\int_0^1\frac{x^{2/3}(1-x)^{-1/3}}{1-x+x^2}dx$,Evaluating,\int_0^1\frac{x^{2/3}(1-x)^{-1/3}}{1-x+x^2}dx,"How can we prove $$\int_0^1\frac{x^{2/3}(1-x)^{-1/3}}{1-x+x^2}\mathrm{d} x=\frac{2\pi}{3\sqrt 3}?$$ Thought 1 It cannot be solved by using contour integration directly. If we replace $-1/3$ with $-2/3$ or $1/3$ or something else, we can use contour integration directly to solve it. Thought 2 I have tried substitution $x=t^3$ and $x=1-t$ . None of them worked. But I noticed that the form of $1-x+x^2$ does not change while applying $x=1-t$ . Thought 3 Recall the integral representation of $_2F_1$ function, I was able to convert it into a formula with $_2F_1\left(2/3,1;4/3; e^{\pi i/3}\right)$ involved. But I think it will only make the integral more ""complex"". Moreover, I prefer a elementary approach. (But I also appreciate hypergeometric approach)","How can we prove Thought 1 It cannot be solved by using contour integration directly. If we replace with or or something else, we can use contour integration directly to solve it. Thought 2 I have tried substitution and . None of them worked. But I noticed that the form of does not change while applying . Thought 3 Recall the integral representation of function, I was able to convert it into a formula with involved. But I think it will only make the integral more ""complex"". Moreover, I prefer a elementary approach. (But I also appreciate hypergeometric approach)","\int_0^1\frac{x^{2/3}(1-x)^{-1/3}}{1-x+x^2}\mathrm{d} x=\frac{2\pi}{3\sqrt 3}? -1/3 -2/3 1/3 x=t^3 x=1-t 1-x+x^2 x=1-t _2F_1 _2F_1\left(2/3,1;4/3; e^{\pi i/3}\right)","['calculus', 'integration', 'definite-integrals', 'hypergeometric-function']"
26,How to improve accuracy when solving calculus questions,How to improve accuracy when solving calculus questions,,"I find calculus to be a really interesting topic to study, and from what I've experienced it simply boils down to applying algebra to more complicated concepts. I understand calculus and can easily formulate proofs for myself as refreshers for things I don't quite remember. However, when it comes to actually solving calculus problems, I really struggle in terms of accuracy. No matter what problem I approach, I always end up making stupid mistakes or miscalculations. For example, today I was doing a practice problem that involved applying integrals to a distance/velocity problem to find the total distance a particle traveled, given the s(t) function that represents position versus time. It took me three lengthy attempts to solve the problem before I got the correct answer, and EACH attempt paradoxically yielded three different answers (the last being the correct). So the one solution I read in another post on Stack Exchange -- to take things slowly -- does not help, because when I solve calculus problems like a snail, I (mostly) do things correctly, but at the cost of time. This means that on timed exams, I may get more than half the questions correct, but I won't have enough time to finish the rest. Others suggest practicing over and over to hone my skills so that I don't trip up and make these mistakes...but that doesn't help either. In fact, I've been practicing what I learned in my AP Calculus AB course for about a year now, and yet I still continue to frequently make miscalculations. Again, what frustrates me is that I fully comprehend introductory calculus topics; it's not the application of calculus concepts or the use of formulas that gives me trouble, but rather it's maintaining accuracy while working quickly and efficiently. Does anyone have suggestions on how I can alleviate my problem? I'm about to take a 2nd semester Calculus course in college when the Fall starts and I'm afraid that my grade will suffer if I continue to make these careless mistakes.","I find calculus to be a really interesting topic to study, and from what I've experienced it simply boils down to applying algebra to more complicated concepts. I understand calculus and can easily formulate proofs for myself as refreshers for things I don't quite remember. However, when it comes to actually solving calculus problems, I really struggle in terms of accuracy. No matter what problem I approach, I always end up making stupid mistakes or miscalculations. For example, today I was doing a practice problem that involved applying integrals to a distance/velocity problem to find the total distance a particle traveled, given the s(t) function that represents position versus time. It took me three lengthy attempts to solve the problem before I got the correct answer, and EACH attempt paradoxically yielded three different answers (the last being the correct). So the one solution I read in another post on Stack Exchange -- to take things slowly -- does not help, because when I solve calculus problems like a snail, I (mostly) do things correctly, but at the cost of time. This means that on timed exams, I may get more than half the questions correct, but I won't have enough time to finish the rest. Others suggest practicing over and over to hone my skills so that I don't trip up and make these mistakes...but that doesn't help either. In fact, I've been practicing what I learned in my AP Calculus AB course for about a year now, and yet I still continue to frequently make miscalculations. Again, what frustrates me is that I fully comprehend introductory calculus topics; it's not the application of calculus concepts or the use of formulas that gives me trouble, but rather it's maintaining accuracy while working quickly and efficiently. Does anyone have suggestions on how I can alleviate my problem? I'm about to take a 2nd semester Calculus course in college when the Fall starts and I'm afraid that my grade will suffer if I continue to make these careless mistakes.",,"['calculus', 'soft-question']"
27,Average distance from center of circle,Average distance from center of circle,,"Using calculus, we can show that the average distance of a point in a circle to the center is $2R/3$ , where $R$ is the radius. However, I have a separate way of approaching this question through intuition that gives me a different answer, and I'd like to know why my intuition fails. For each $\theta\in [0,2\pi)$ , we can consider the line segment of that angle from the center of the circle to the boundary. On this line segment, the average distance from the center should be $R/2$ . Then the average distance from the center over all points in the circle should just be $R/2$ as well, since we can cover the circle with these line segments. Why does this intuitive approach give the wrong answer? My best guess is that these line segments all share the origin, so this method counts the origin's distance from itself multiple times, thereby throwing off the average by decreasing it, which agrees with the fact that we know the actual answer is greater. However, couldn't I just look at the average distance from the center for the open line segments that exclude the center? The average distance for these open line segments should still be $R/2$ , and then I could apply the same argument for covering the circle with the open line segments. This time, I'd be missing the center, but missing a single point shouldn't throw off the answer. Why does this argument not work?","Using calculus, we can show that the average distance of a point in a circle to the center is , where is the radius. However, I have a separate way of approaching this question through intuition that gives me a different answer, and I'd like to know why my intuition fails. For each , we can consider the line segment of that angle from the center of the circle to the boundary. On this line segment, the average distance from the center should be . Then the average distance from the center over all points in the circle should just be as well, since we can cover the circle with these line segments. Why does this intuitive approach give the wrong answer? My best guess is that these line segments all share the origin, so this method counts the origin's distance from itself multiple times, thereby throwing off the average by decreasing it, which agrees with the fact that we know the actual answer is greater. However, couldn't I just look at the average distance from the center for the open line segments that exclude the center? The average distance for these open line segments should still be , and then I could apply the same argument for covering the circle with the open line segments. This time, I'd be missing the center, but missing a single point shouldn't throw off the answer. Why does this argument not work?","2R/3 R \theta\in [0,2\pi) R/2 R/2 R/2","['calculus', 'probability', 'geometry', 'intuition']"
28,"What is ""advanced calculus""?","What is ""advanced calculus""?",,"I've seen ""advanced calculus"" as a prerequisite to some courses. What are people referring to when they say that? Is it a particular set of topics? What are some key examples of books that cover these topics? What are the prerequisites to advanced calculus? Is it normally an undergrad or grad-level subject? P.S. There is a related question here but I wanted to ask a more focused one.","I've seen ""advanced calculus"" as a prerequisite to some courses. What are people referring to when they say that? Is it a particular set of topics? What are some key examples of books that cover these topics? What are the prerequisites to advanced calculus? Is it normally an undergrad or grad-level subject? P.S. There is a related question here but I wanted to ask a more focused one.",,"['calculus', 'soft-question']"
29,Proving that $\int_0^\infty\frac{J_{2a}(2x)~J_{2b}(2x)}{x^{2n+1}}~dx~=~\frac12\cdot\frac{(a+b-n-1)!~(2n)!}{(n+a+b)!~(n+a-b)!~(n-a+b)!}$,Proving that,\int_0^\infty\frac{J_{2a}(2x)~J_{2b}(2x)}{x^{2n+1}}~dx~=~\frac12\cdot\frac{(a+b-n-1)!~(2n)!}{(n+a+b)!~(n+a-b)!~(n-a+b)!},"How could we prove that $$\int_0^\infty\frac{J_{2a}(2x)~J_{2b}(2x)}{x^{2n+1}}~dx~=~\frac12\cdot\frac{(a+b-n-1)!~(2n)!}{(n+a+b)!~(n+a-b)!~(n-a+b)!}$$ for $a+b>n>-\dfrac12$ ? Inspired by this question , I sought to find $($a justification for$)$ the closed form expressions of the following two integrals: $~\displaystyle\int_0^\infty\frac{J_A(x)}{x^N}~dx~$ and $~\displaystyle\int_0^\infty\frac{J_A(x)~J_B(x)}{x^N}~dx.~$ For the former, we have $~\displaystyle\int_0^\infty\frac{J_{2k+1}(2x)}{x^{2n}}~dx~=~\frac12\cdot\frac{(k-n)!}{(k+n)!}~,~$ for $k>n>\dfrac14~,~$ which I was ultimately able to “justify” $($sort of$)$ in a highly unorthodox manner, using a certain trigonometric integral expression for the Bessel function , and then carelessly $($and shamelessly$)$ exchanging the order of integration. Unfortunately, even such underhanded tricks have failed me when attempting to approach the latter. Can anybody here help me ? Thank you !","How could we prove that $$\int_0^\infty\frac{J_{2a}(2x)~J_{2b}(2x)}{x^{2n+1}}~dx~=~\frac12\cdot\frac{(a+b-n-1)!~(2n)!}{(n+a+b)!~(n+a-b)!~(n-a+b)!}$$ for $a+b>n>-\dfrac12$ ? Inspired by this question , I sought to find $($a justification for$)$ the closed form expressions of the following two integrals: $~\displaystyle\int_0^\infty\frac{J_A(x)}{x^N}~dx~$ and $~\displaystyle\int_0^\infty\frac{J_A(x)~J_B(x)}{x^N}~dx.~$ For the former, we have $~\displaystyle\int_0^\infty\frac{J_{2k+1}(2x)}{x^{2n}}~dx~=~\frac12\cdot\frac{(k-n)!}{(k+n)!}~,~$ for $k>n>\dfrac14~,~$ which I was ultimately able to “justify” $($sort of$)$ in a highly unorthodox manner, using a certain trigonometric integral expression for the Bessel function , and then carelessly $($and shamelessly$)$ exchanging the order of integration. Unfortunately, even such underhanded tricks have failed me when attempting to approach the latter. Can anybody here help me ? Thank you !",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'bessel-functions']"
30,When to differentiate under the integral sign?,When to differentiate under the integral sign?,,"I'm finishing up a semester of multivariable calculus and will be taking a course on analysis this Spring. In any of the calculus courses I've taken, we never covered anything beyond the standard techniques of integration ( $u$ -sub, parts, etc.) One of the techniques I saw used recently which I had not heard of was differentiation under the integral sign , which makes use of the fact that: $$\frac{d}{dx} \int_a^bf(x,t)dt = \int_a^b \frac{\partial}{\partial x}f(x,t)dt $$ in solving integrals. My question is, is there ever an indication that this should be used? Is there any explainable intuition or rule of thumb for the use of differentiation under the integral sign?","I'm finishing up a semester of multivariable calculus and will be taking a course on analysis this Spring. In any of the calculus courses I've taken, we never covered anything beyond the standard techniques of integration ( -sub, parts, etc.) One of the techniques I saw used recently which I had not heard of was differentiation under the integral sign , which makes use of the fact that: in solving integrals. My question is, is there ever an indication that this should be used? Is there any explainable intuition or rule of thumb for the use of differentiation under the integral sign?","u \frac{d}{dx} \int_a^bf(x,t)dt = \int_a^b \frac{\partial}{\partial x}f(x,t)dt ","['calculus', 'integration', 'multivariable-calculus', 'derivatives', 'soft-question']"
31,$\lim_{x\to0^{+}} x \ln x$ without l'Hopital's rule,without l'Hopital's rule,\lim_{x\to0^{+}} x \ln x,"I have a midterm coming up and on the past exams the hard question(s) usually involve some form of $\lim_{x\to0^{+}} x \ln x$. However, we're not allowed to use l'Hopital's rule, on this year's exam anyways. So how can I evaluate said limit without l'Hopital's rule? I got somewhere with another approach, don't know if it's useful: $\lim_{x\to0^{+}} x \ln x = \lim_{x\to0^{+}} x^2 \ln (x^2) = L$ $= (\lim_{x\to0^{+}} 2x)(\lim_{x\to0^{+}} x \ln x)$ $= 0 * L$ Then I just need to prove that L is finite/exists (which means it must be 0)","I have a midterm coming up and on the past exams the hard question(s) usually involve some form of $\lim_{x\to0^{+}} x \ln x$. However, we're not allowed to use l'Hopital's rule, on this year's exam anyways. So how can I evaluate said limit without l'Hopital's rule? I got somewhere with another approach, don't know if it's useful: $\lim_{x\to0^{+}} x \ln x = \lim_{x\to0^{+}} x^2 \ln (x^2) = L$ $= (\lim_{x\to0^{+}} 2x)(\lim_{x\to0^{+}} x \ln x)$ $= 0 * L$ Then I just need to prove that L is finite/exists (which means it must be 0)",,"['calculus', 'limits']"
32,Inverse of a bijection f is equal to its derivative,Inverse of a bijection f is equal to its derivative,,Does there exist a differentiable bijection $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f'(x) = f^{-1}(x)$ ?,Does there exist a differentiable bijection $f: \mathbb{R} \rightarrow \mathbb{R}$ such that $f'(x) = f^{-1}(x)$ ?,,"['calculus', 'derivatives']"
33,Where do the factorials come from in the Taylor series?,Where do the factorials come from in the Taylor series?,,"Unfortunately, I don't have much detail to give here. But is the general idea to cancel out the constant obtained from taking the derivative. For instance, say my function was $f(x)=f_0+f_1x+f_2x^2+\dotsb$ Then $f'(x)=f_1+2f_2x+\dotsb$ . And if the expansion is centered around $x=0$ , then \begin{align}f'(0)&=0 \\ f''(0)&=2f_2\\ f'''(0)&=3\cdot 2f_3.\\ \end{align} Therefore \begin{align} f_0&=f(0) \\ f_1&=\frac{f'(0)}{1} \\ f_2&=\frac{f''(0)}{2} \end{align} And so forth. Is that where the factorial comes from? It is quite clear for a polynomial, but what about a trig function such as $\sin(x)$ other than using Taylor's formula?","Unfortunately, I don't have much detail to give here. But is the general idea to cancel out the constant obtained from taking the derivative. For instance, say my function was Then . And if the expansion is centered around , then Therefore And so forth. Is that where the factorial comes from? It is quite clear for a polynomial, but what about a trig function such as other than using Taylor's formula?","f(x)=f_0+f_1x+f_2x^2+\dotsb f'(x)=f_1+2f_2x+\dotsb x=0 \begin{align}f'(0)&=0 \\
f''(0)&=2f_2\\
f'''(0)&=3\cdot 2f_3.\\
\end{align} \begin{align}
f_0&=f(0) \\
f_1&=\frac{f'(0)}{1} \\
f_2&=\frac{f''(0)}{2}
\end{align} \sin(x)","['calculus', 'taylor-expansion']"
34,"How come such different methods result in the same number, $e$?","How come such different methods result in the same number, ?",e,"I guess the proof of the identity $$ \sum_{n = 0}^{\infty} \frac{1}{n!} \equiv \lim_{x \to \infty} \left(1 + \frac{1}{x}\right)^x $$ explains the connection between such different calculations. How is it done?","I guess the proof of the identity $$ \sum_{n = 0}^{\infty} \frac{1}{n!} \equiv \lim_{x \to \infty} \left(1 + \frac{1}{x}\right)^x $$ explains the connection between such different calculations. How is it done?",,"['calculus', 'sequences-and-series', 'limits', 'irrational-numbers']"
35,"Prove $\gamma_1\left(\frac34\right)-\gamma_1\left(\frac14\right)=\pi\,\left(\gamma+4\ln2+3\ln\pi-4\ln\Gamma\left(\frac14\right)\right)$",Prove,"\gamma_1\left(\frac34\right)-\gamma_1\left(\frac14\right)=\pi\,\left(\gamma+4\ln2+3\ln\pi-4\ln\Gamma\left(\frac14\right)\right)","Please help me to prove this identity: $$\gamma_1\left(\frac{3}{4}\right)-\gamma_1\left(\frac{1}{4}\right)=\pi\,\left(\gamma+4\ln2+3\ln\pi-4\ln\Gamma\left(\frac{1}{4}\right)\right),$$ where $\gamma_n(a)$ is a generalized Stieltjes constant and $\gamma$ is the Euler-Mascheroni constant .","Please help me to prove this identity: $$\gamma_1\left(\frac{3}{4}\right)-\gamma_1\left(\frac{1}{4}\right)=\pi\,\left(\gamma+4\ln2+3\ln\pi-4\ln\Gamma\left(\frac{1}{4}\right)\right),$$ where $\gamma_n(a)$ is a generalized Stieltjes constant and $\gamma$ is the Euler-Mascheroni constant .",,"['calculus', 'special-functions']"
36,What is wrong with this funny proof that 2 = 4 using infinite exponentiation?,What is wrong with this funny proof that 2 = 4 using infinite exponentiation?,,"$\newcommand\iddots{\mathinner{   \kern1mu\raise1pt{.}   \kern2mu\raise4pt{.}   \kern2mu\raise7pt{\Rule{0pt}{7pt}{0pt}.}   \kern1mu }}$ Out of boredom, I decided to recall the following equation: $$x^{x^{{x}^{\iddots}}} = 2.$$ Which, I simply rewrote like this: $x^2 = 2$ , and therefore $x = \sqrt{2}$ . Then I took a look at the more general form: $$x^{x^{{x}^{\iddots}}} = p.$$ I then concluded that $x = (p)^{\frac{1}{p}}$ , and this was the solution set for all $p$ . Then I thought for a while and determined that there are multiple values where this function equals the square root of $2$ , so it's not injective. However, this led to an awkward statement based on the fact that $\sqrt{2} = (4)^{\frac{1}{4}}$ . We deduce the following: The equation $x^{x^{{x}^{\iddots}}} = 2$ and the equation $x^{x^{{x}^{\iddots}}} = 4$ have the same solution, namely $x = \sqrt{2}$ . So we conclude that $2 = 4$ . I showed this to friend of mine, and he conjectured that there are no valid solutions for all $p > e$ . To prove this, I tried writing the following: I tried differentiating $f(x) = (x)^{(1/x)}$ and setting it equal to $0$ to find the maximum. I determined that there was a maximum at the point where $x = e$ . Does this mean that the maximum possible solution $x$ for the infinite power is $e^{(1/e)}$ therefore showing that the maximum value for $p$ is indeed $p = e?$","Out of boredom, I decided to recall the following equation: Which, I simply rewrote like this: , and therefore . Then I took a look at the more general form: I then concluded that , and this was the solution set for all . Then I thought for a while and determined that there are multiple values where this function equals the square root of , so it's not injective. However, this led to an awkward statement based on the fact that . We deduce the following: The equation and the equation have the same solution, namely . So we conclude that . I showed this to friend of mine, and he conjectured that there are no valid solutions for all . To prove this, I tried writing the following: I tried differentiating and setting it equal to to find the maximum. I determined that there was a maximum at the point where . Does this mean that the maximum possible solution for the infinite power is therefore showing that the maximum value for is indeed","\newcommand\iddots{\mathinner{
  \kern1mu\raise1pt{.}
  \kern2mu\raise4pt{.}
  \kern2mu\raise7pt{\Rule{0pt}{7pt}{0pt}.}
  \kern1mu
}} x^{x^{{x}^{\iddots}}} = 2. x^2 = 2 x = \sqrt{2} x^{x^{{x}^{\iddots}}} = p. x = (p)^{\frac{1}{p}} p 2 \sqrt{2} = (4)^{\frac{1}{4}} x^{x^{{x}^{\iddots}}} = 2 x^{x^{{x}^{\iddots}}} = 4 x = \sqrt{2} 2 = 4 p > e f(x) = (x)^{(1/x)} 0 x = e x e^{(1/e)} p p = e?","['calculus', 'algebra-precalculus', 'exponentiation', 'fake-proofs', 'power-towers']"
37,"If a function is Riemann integrable, then it is Lebesgue integrable and 2 integrals are the same?","If a function is Riemann integrable, then it is Lebesgue integrable and 2 integrals are the same?",,"Is is true that if a function is Riemann integrable, then it is Lebesgue integrable with the same value? If it's true, how to prove it? If it's false, what is a counterexample?","Is is true that if a function is Riemann integrable, then it is Lebesgue integrable with the same value? If it's true, how to prove it? If it's false, what is a counterexample?",,"['calculus', 'integration', 'functions', 'lebesgue-integral', 'riemann-sum']"
38,"If two integrals are equal, then the functions are the same","If two integrals are equal, then the functions are the same",,"Assume that we have two continuous functions $f(x)$ and $g(x)$. If the integrals $\int_{x_1}^{x_2}f dx$ and $\int_{x_1}^{x_2}g dx$ are the same for any choice of $x_1$ and $x_2$, then the functions are also the same? My attempt: I fix $x_1$ and let $x_2$ vary. Then I take the derivative of both integrals. It sounds right. I am missing something? Any better solution?","Assume that we have two continuous functions $f(x)$ and $g(x)$. If the integrals $\int_{x_1}^{x_2}f dx$ and $\int_{x_1}^{x_2}g dx$ are the same for any choice of $x_1$ and $x_2$, then the functions are also the same? My attempt: I fix $x_1$ and let $x_2$ vary. Then I take the derivative of both integrals. It sounds right. I am missing something? Any better solution?",,"['calculus', 'integration', 'definite-integrals']"
39,How to calculate the limit that seems very complex..,How to calculate the limit that seems very complex..,,"Someone gives me a limit about trigonometric function and combinatorial numbers. $I=\displaystyle \lim_{n\to\infty}\left(\frac{\sin\frac{1}{n^2}+\binom{n}{1}\sin\frac{2}{n^2}+\binom{n}{2}\sin\frac{3}{n^2}\cdots\binom{n}{n}\sin\frac{n+1}{n^2}}{\cos\frac{1}{n^2}+\binom{n}{1}\cos\frac{2}{n^2}+\binom{n}{2}\cos\frac{3}{n^2}\cdots\binom{n}{n}\cos\frac{n+1}{n^2}}+1\right)^n$ when $n$ is big enough, $\displaystyle 0\leqslant\frac{n+1}{n^2}\leqslant \frac\pi 2$ I tried $\displaystyle \frac{x}{1+x}\leqslant\sin x\leqslant x$ Use $\sin x\leqslant x$, I got $I\leqslant e$, But use another inequality I got nothing. I don't know the answer is $e$ or not. Who can help me. Thanks.","Someone gives me a limit about trigonometric function and combinatorial numbers. $I=\displaystyle \lim_{n\to\infty}\left(\frac{\sin\frac{1}{n^2}+\binom{n}{1}\sin\frac{2}{n^2}+\binom{n}{2}\sin\frac{3}{n^2}\cdots\binom{n}{n}\sin\frac{n+1}{n^2}}{\cos\frac{1}{n^2}+\binom{n}{1}\cos\frac{2}{n^2}+\binom{n}{2}\cos\frac{3}{n^2}\cdots\binom{n}{n}\cos\frac{n+1}{n^2}}+1\right)^n$ when $n$ is big enough, $\displaystyle 0\leqslant\frac{n+1}{n^2}\leqslant \frac\pi 2$ I tried $\displaystyle \frac{x}{1+x}\leqslant\sin x\leqslant x$ Use $\sin x\leqslant x$, I got $I\leqslant e$, But use another inequality I got nothing. I don't know the answer is $e$ or not. Who can help me. Thanks.",,"['calculus', 'limits', 'binomial-coefficients']"
40,Integrating $x^2e^{-x}$ using Feynman's trick? [duplicate],Integrating  using Feynman's trick? [duplicate],x^2e^{-x},"This question already has an answer here : Differentiation under integral sign (Gamma function) (1 answer) Closed 9 years ago . In the second episode of season $8$ of ""The Big Bang Theory,"" which aired yesterday night, it is stated that one can integrate $x^2e^{-x}$ by using Feynman's trick of differentiating under the integral. Is this actually true, and if so, how to do it? And is it ""better,"" in any sense, than the usual way of doing it by integration by parts?","This question already has an answer here : Differentiation under integral sign (Gamma function) (1 answer) Closed 9 years ago . In the second episode of season $8$ of ""The Big Bang Theory,"" which aired yesterday night, it is stated that one can integrate $x^2e^{-x}$ by using Feynman's trick of differentiating under the integral. Is this actually true, and if so, how to do it? And is it ""better,"" in any sense, than the usual way of doing it by integration by parts?",,['calculus']
41,Prove $\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx$,Prove,\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx,"I am trying to prove  $$\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx$$ This problem is a classic, but I seem to be missing one step or the understanding of two steps which I will outline below. $$I_n := \int\cos^n x \ dx = \int\cos^{n-1} x \cos x \ dx \tag{1}$$ First question : why rewrite the original instead of immediately integrating by parts of $\int \cos^n x \ dx$? Integrate by parts with $$u = \cos^{n-1} x, dv = \cos x \ dx \implies du = (n-1)\cos^{n-2} x \cdot -\sin x, v = \sin x$$ which leads to $$I_n = \sin x \ \cos^{n-1} x +\int\sin^2 x (n-1) \ \cos^{n-2} x \ dx \tag{2}$$ Since $(n-1)$ is a constant, we can throw it out front of the integral: $$I_n = \sin x \ \cos^{n-1} x +(n-1)\int\sin^2 x \ \cos^{n-2} x \ dx\tag{3}$$ I can transform the integral a bit because $\sin^2 x + cos^2 x = 1 \implies \sin^2 x = 1-\cos^2 x$ $$I_n = \sin x \ \cos^{n-1} x + (n-1)\int(1-\cos^2 x) \ \cos^{n-2} x \ dx \tag{4}$$ According to Wikipedia as noted here , this simplifies to: $$I_n = \sin x \ \cos^{n-1} x + (n-1) \int \cos^{n-2} x \ dx - (n-1)\int(\cos^n x) \ dx \tag{5}$$ Question 2 : How did they simplify the integral of $\int(1-\cos^2 x) \ dx$ to $\int(\cos^n x) \ dx$? Assuming knowledge of equation 5, I see how to rewrite it as $$I_n = \sin x \ \cos^{n-1} x + (n-1) I_{n-2} x  - (n-1) I_{n} \tag{6}$$ and solve for $I_n$. I had tried exploiting the fact that  $$\cos^2 x = \frac{1}{2} \cos(2x) + \frac{1}{2} $$ and trying to deal with $\int 1 \ dx - \int \frac{1}{2} \cos (2x) + \frac{1}{2} \ dx$ which left me with $\frac{x}{2} - \frac{1}{4} \sin(2x)$ after integrating those pieces. Putting it all together I have: $$I_n = \sin x \ \cos^{n-1} x + (n-1) I_{n-2} x \left(-(n-1) (\frac{x}{2} - \frac{1}{4} \sin 2x) \right) \tag{7}$$ but I'm unsure how to write the last few terms as an expression of $I_{something}$ to get it to match the usual reduction formula of $$\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx$$","I am trying to prove  $$\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx$$ This problem is a classic, but I seem to be missing one step or the understanding of two steps which I will outline below. $$I_n := \int\cos^n x \ dx = \int\cos^{n-1} x \cos x \ dx \tag{1}$$ First question : why rewrite the original instead of immediately integrating by parts of $\int \cos^n x \ dx$? Integrate by parts with $$u = \cos^{n-1} x, dv = \cos x \ dx \implies du = (n-1)\cos^{n-2} x \cdot -\sin x, v = \sin x$$ which leads to $$I_n = \sin x \ \cos^{n-1} x +\int\sin^2 x (n-1) \ \cos^{n-2} x \ dx \tag{2}$$ Since $(n-1)$ is a constant, we can throw it out front of the integral: $$I_n = \sin x \ \cos^{n-1} x +(n-1)\int\sin^2 x \ \cos^{n-2} x \ dx\tag{3}$$ I can transform the integral a bit because $\sin^2 x + cos^2 x = 1 \implies \sin^2 x = 1-\cos^2 x$ $$I_n = \sin x \ \cos^{n-1} x + (n-1)\int(1-\cos^2 x) \ \cos^{n-2} x \ dx \tag{4}$$ According to Wikipedia as noted here , this simplifies to: $$I_n = \sin x \ \cos^{n-1} x + (n-1) \int \cos^{n-2} x \ dx - (n-1)\int(\cos^n x) \ dx \tag{5}$$ Question 2 : How did they simplify the integral of $\int(1-\cos^2 x) \ dx$ to $\int(\cos^n x) \ dx$? Assuming knowledge of equation 5, I see how to rewrite it as $$I_n = \sin x \ \cos^{n-1} x + (n-1) I_{n-2} x  - (n-1) I_{n} \tag{6}$$ and solve for $I_n$. I had tried exploiting the fact that  $$\cos^2 x = \frac{1}{2} \cos(2x) + \frac{1}{2} $$ and trying to deal with $\int 1 \ dx - \int \frac{1}{2} \cos (2x) + \frac{1}{2} \ dx$ which left me with $\frac{x}{2} - \frac{1}{4} \sin(2x)$ after integrating those pieces. Putting it all together I have: $$I_n = \sin x \ \cos^{n-1} x + (n-1) I_{n-2} x \left(-(n-1) (\frac{x}{2} - \frac{1}{4} \sin 2x) \right) \tag{7}$$ but I'm unsure how to write the last few terms as an expression of $I_{something}$ to get it to match the usual reduction formula of $$\int\cos^n x \ dx = \frac{1}n \cos^{n-1}x \sin x + \frac{n-1}{n}\int\cos^{n-2} x \ dx$$",,"['calculus', 'integration', 'indefinite-integrals', 'reduction-formula']"
42,Common student mistakes/misconceptions in a first year calculus course,Common student mistakes/misconceptions in a first year calculus course,,"What are the common mistakes and misconceptions students make in a first year calculus course? More importantly: What can I do to prevent/rectify them? Context: Soon I will be doing some calculus lecturing. As this is the first time I've been entrusted with this responsibility, I've been thinking a lot about what I can do beyond regurgitating the material. I've had some experience doing tutorials (I imagine this would be equivalent to what a T.A. does in the U.S.) but lecturing is different as I will be introducing the material as opposed to reinforcing it. Obviously becoming a good (or even average) lecturer takes time and experience, and can not be obtained via a single answer to any question I could possibly ask here. Instead, I chose to ask the questions above. I asked the first question because I don't think I can accurately answer it myself until I've taught the course at least once - I'd rather be able to address these issues the first time around. The second question is more general. There are many well-known mistakes students make when first learning mathematics, but they are well-known because they occur frequently and continue to do so over time. The fact that these mistakes/misconceptions continue to occur means that these particular issues haven't been resolved. The topics that will be covered in the course are: Differential Equations (separable, linear second order constant coefficients) Applications of Calculus (volume of revolution) Limits (not including $\epsilon - \delta$ definition) Continuity Taylor Series I know that this post may be too general/not suitable for this site. If this is the case, I apologise.","What are the common mistakes and misconceptions students make in a first year calculus course? More importantly: What can I do to prevent/rectify them? Context: Soon I will be doing some calculus lecturing. As this is the first time I've been entrusted with this responsibility, I've been thinking a lot about what I can do beyond regurgitating the material. I've had some experience doing tutorials (I imagine this would be equivalent to what a T.A. does in the U.S.) but lecturing is different as I will be introducing the material as opposed to reinforcing it. Obviously becoming a good (or even average) lecturer takes time and experience, and can not be obtained via a single answer to any question I could possibly ask here. Instead, I chose to ask the questions above. I asked the first question because I don't think I can accurately answer it myself until I've taught the course at least once - I'd rather be able to address these issues the first time around. The second question is more general. There are many well-known mistakes students make when first learning mathematics, but they are well-known because they occur frequently and continue to do so over time. The fact that these mistakes/misconceptions continue to occur means that these particular issues haven't been resolved. The topics that will be covered in the course are: Differential Equations (separable, linear second order constant coefficients) Applications of Calculus (volume of revolution) Limits (not including $\epsilon - \delta$ definition) Continuity Taylor Series I know that this post may be too general/not suitable for this site. If this is the case, I apologise.",,"['calculus', 'education']"
43,Help with Seemingly Hopeless Double Integral,Help with Seemingly Hopeless Double Integral,,"I hate to be that guy to just post an integration problem and ask how to solve it so I'll give a little relevant info Okay, so I'm working on a physics project and my professor proposed that the following double integral could potentially solve a problem that I've used an alternative method to solve: $$I=\int_0^\pi\int_0^\rho\frac{t^2\sin\phi\left(t\cos\phi-d\right)}{\left[t^2\sin^2\phi+\left(t\cos\phi-d\right)^2\right]^{3/2}}\;dt d\phi$$ $\rho$ is an arbitrary, strictly positive real constant $d$ is a real constant that satisfies $d>\rho$ This integral's value could provide immense insight into fields of uniform, solid spherical objects, so it's actually pretty important for my work. After some quick attempts to simplify, I decided to try some integral calculators with set values. Needless to say, the result after the first integral seemed so hopeless that I couldn't imagine simplifying and integrating again--not to mention then generalising constant inputs to their original variable form. However, there is a strong likelihood that $I$ simplifies to one of the following two solutions: $$\text{1.This solution comes from inverse square laws}$$ $$I=\frac{1}{d^2}$$ $$\text{2. This solution comes from a separate computation that I did (integrals below)}$$ $$I=\left(1-\frac{\rho^2}{5d^2}\right)\left[\frac{3}{2\rho^2}+\frac{3(\rho^2-d^2)}{4d\rho^3}\ln\left(\frac{d+\rho}{d-\rho}\right)\right]$$ Although it looks like these are vastly different answers, given $\rho=1$ and $d=10$ , you get the following outputs from $(1)$ and $(2)$ : $$1.\; I=0.01$$ $$2.\; I\approx 0.01000046$$ Here's the ratio of solution (2) over (1) for $\rho\in(0,1),\;d\in(0,50)$ I tried to tackle this problem differently than my professor, and set up the following integrals to solve the problem that lead to solution $(2)$ : $$\frac{9}{4\rho^6}\left[\;\int\limits_{d-\rho}^{d+\rho}x\left[x-\frac{x^2+d^2-\rho^2}{2d}\right]\left[\frac{(x+d)^2-\rho^2}{4d\cdot x}\right]\;dx\right]\cdot\left[\;\int\limits_{d-\rho}^{d+\rho}\frac{\rho^2-(x-d)^2}{2d\cdot x}\;dx\right]$$ Where you come in If the double integral is correctly composed (which my professor felt confident with), I need someone skilled in integration to solve said double integral . I've given two possible solutions and it's probable that the answer will be one of those. If it's solution $(1)$ , I know that mine will have an error and you will essentially have proved the inverse square law for gravitational and electric fields. If it's solution $(2)$ , then this will be far more exciting to me but less likely. If it's neither, then there are several possible implications BOUNTY I'm willing to award the following bounties for solving the double integral at the beginning. Since certain solutions have stronger implications (as explained above), I'm rewarding the following bounties: +200 rep if you verify solution $(1)$ +500 rep if you verify solution $(2)$ +75 rep for any other solutions (note they'll have to be verified by a second user first) QUESTIONS If you have any additional questions feel free to ask, and thanks for reading all this!","I hate to be that guy to just post an integration problem and ask how to solve it so I'll give a little relevant info Okay, so I'm working on a physics project and my professor proposed that the following double integral could potentially solve a problem that I've used an alternative method to solve: is an arbitrary, strictly positive real constant is a real constant that satisfies This integral's value could provide immense insight into fields of uniform, solid spherical objects, so it's actually pretty important for my work. After some quick attempts to simplify, I decided to try some integral calculators with set values. Needless to say, the result after the first integral seemed so hopeless that I couldn't imagine simplifying and integrating again--not to mention then generalising constant inputs to their original variable form. However, there is a strong likelihood that simplifies to one of the following two solutions: Although it looks like these are vastly different answers, given and , you get the following outputs from and : Here's the ratio of solution (2) over (1) for I tried to tackle this problem differently than my professor, and set up the following integrals to solve the problem that lead to solution : Where you come in If the double integral is correctly composed (which my professor felt confident with), I need someone skilled in integration to solve said double integral . I've given two possible solutions and it's probable that the answer will be one of those. If it's solution , I know that mine will have an error and you will essentially have proved the inverse square law for gravitational and electric fields. If it's solution , then this will be far more exciting to me but less likely. If it's neither, then there are several possible implications BOUNTY I'm willing to award the following bounties for solving the double integral at the beginning. Since certain solutions have stronger implications (as explained above), I'm rewarding the following bounties: +200 rep if you verify solution +500 rep if you verify solution +75 rep for any other solutions (note they'll have to be verified by a second user first) QUESTIONS If you have any additional questions feel free to ask, and thanks for reading all this!","I=\int_0^\pi\int_0^\rho\frac{t^2\sin\phi\left(t\cos\phi-d\right)}{\left[t^2\sin^2\phi+\left(t\cos\phi-d\right)^2\right]^{3/2}}\;dt d\phi \rho d d>\rho I \text{1.This solution comes from inverse square laws} I=\frac{1}{d^2} \text{2. This solution comes from a separate computation that I did (integrals below)} I=\left(1-\frac{\rho^2}{5d^2}\right)\left[\frac{3}{2\rho^2}+\frac{3(\rho^2-d^2)}{4d\rho^3}\ln\left(\frac{d+\rho}{d-\rho}\right)\right] \rho=1 d=10 (1) (2) 1.\; I=0.01 2.\; I\approx 0.01000046 \rho\in(0,1),\;d\in(0,50) (2) \frac{9}{4\rho^6}\left[\;\int\limits_{d-\rho}^{d+\rho}x\left[x-\frac{x^2+d^2-\rho^2}{2d}\right]\left[\frac{(x+d)^2-\rho^2}{4d\cdot x}\right]\;dx\right]\cdot\left[\;\int\limits_{d-\rho}^{d+\rho}\frac{\rho^2-(x-d)^2}{2d\cdot x}\;dx\right] (1) (2) (1) (2)","['calculus', 'integration', 'multivariable-calculus', 'physics']"
44,Slick proofs that if $\sum\limits_{k=1}^\infty \frac{a_k}{k}$ converges then $\lim\limits_{n\to\infty} \frac{1}{n}\sum\limits_{k=1}^n a_k=0$,Slick proofs that if  converges then,\sum\limits_{k=1}^\infty \frac{a_k}{k} \lim\limits_{n\to\infty} \frac{1}{n}\sum\limits_{k=1}^n a_k=0,"I'm looking for slick proofs that if $a_n$ is a sequence of complex numbers such that $\sum\limits_{k=1}^\infty \frac{a_k}{k}$ converges then $\lim\limits_{n\to\infty} \frac{1}{n}\sum\limits_{k=1}^n a_k=0$. My not so slick proof: Let $A_x=\sum\limits_{k=1}^{x} a_k$ and apply an Abel sum with the function $f(x)=\frac{1}{x}$. We get $$\sum\limits_{k=1}^n \frac{a_k}{k}-\int\limits_1^n\frac{A_x}{x^2}dx=\frac{A_n}{n}.$$ Of course we can split the integral into chunks to get $$\sum\limits_{k=1}^n \frac{a_k}{k}-\sum\limits_{i=1}^n \left( \frac{1}{k+1}-\frac{1}{k} \right) A_k=\frac{A_n}{n}.$$ If we expand $A_k$ on the left side, it telescopes and yields $$\sum\limits_{k=1}^n \frac{a_k}{k} + \sum\limits_{k=1}^n \left( 1-\frac{1}{k+1}\right) a_k=\frac{A_n}{n}\iff \frac{(n-1)A_n}{n}=\sum\limits_{k=1}^n\frac{a_k}{k(k+1)}.$$ The norm of the right side  of the equation is clearly $\mathcal O(\log(n))$ (using the triangle inequality and the fact that $\frac{a_k}{k}$ is bounded). The desired result follows after dividing both sides by $n-1$. I would also appreciate some proof verification. Initially, I thought that the problem was going to be really easy, but it took me a bit of effort. Am I missing something major? The original problem is for real sequences, but I don't think this helps. Obviously if the sequence converged absolutely then it would also be absolutely trivial.","I'm looking for slick proofs that if $a_n$ is a sequence of complex numbers such that $\sum\limits_{k=1}^\infty \frac{a_k}{k}$ converges then $\lim\limits_{n\to\infty} \frac{1}{n}\sum\limits_{k=1}^n a_k=0$. My not so slick proof: Let $A_x=\sum\limits_{k=1}^{x} a_k$ and apply an Abel sum with the function $f(x)=\frac{1}{x}$. We get $$\sum\limits_{k=1}^n \frac{a_k}{k}-\int\limits_1^n\frac{A_x}{x^2}dx=\frac{A_n}{n}.$$ Of course we can split the integral into chunks to get $$\sum\limits_{k=1}^n \frac{a_k}{k}-\sum\limits_{i=1}^n \left( \frac{1}{k+1}-\frac{1}{k} \right) A_k=\frac{A_n}{n}.$$ If we expand $A_k$ on the left side, it telescopes and yields $$\sum\limits_{k=1}^n \frac{a_k}{k} + \sum\limits_{k=1}^n \left( 1-\frac{1}{k+1}\right) a_k=\frac{A_n}{n}\iff \frac{(n-1)A_n}{n}=\sum\limits_{k=1}^n\frac{a_k}{k(k+1)}.$$ The norm of the right side  of the equation is clearly $\mathcal O(\log(n))$ (using the triangle inequality and the fact that $\frac{a_k}{k}$ is bounded). The desired result follows after dividing both sides by $n-1$. I would also appreciate some proof verification. Initially, I thought that the problem was going to be really easy, but it took me a bit of effort. Am I missing something major? The original problem is for real sequences, but I don't think this helps. Obviously if the sequence converged absolutely then it would also be absolutely trivial.",,"['calculus', 'sequences-and-series', 'inequality', 'convergence-divergence']"
45,Trigonometic Substitution VS Hyperbolic substitution,Trigonometic Substitution VS Hyperbolic substitution,,"The following tables were taken from University of Pennsylvania's page about Calculus : Trigonometric Substitution Hyperbolic Substitution As you can see, the forms $1+x^2$ and $x^2-1$ are repeated in the tables. How does one know when a trigonometric substitution is more suitable to a problem than a hyperbolic substitution and vice versa?","The following tables were taken from University of Pennsylvania's page about Calculus : Trigonometric Substitution Hyperbolic Substitution As you can see, the forms $1+x^2$ and $x^2-1$ are repeated in the tables. How does one know when a trigonometric substitution is more suitable to a problem than a hyperbolic substitution and vice versa?",,['calculus']
46,"Evaluating $\int_{0}^{\pi/3}\ln^2 \left ( \sin x \right )\,dx$",Evaluating,"\int_{0}^{\pi/3}\ln^2 \left ( \sin x \right )\,dx","Good evening! I want to compute the integral $\displaystyle \int_{0}^{\pi/3}\ln^2 \left ( \sin x \right )\,dx$.  However I find it extremely difficult. What I've tried is rewritting it as: $\begin{aligned} \int_{0}^{\pi/3}\ln^2\left ( \sin x \right )\,dx &=\int_{0}^{\pi/3}\left [  \ln \left ( \sin x \right )  \right ]^2\,dx \\   &= \int_{0}^{\pi/3}\left [ \ln \left ( \frac{e^{-ix}-e^{ix}}{2i} \right ) \right ]^2\,dx\\   &= \int_{0}^{\pi/3}\left [ \ln \left ( e^{-ix}-e^{ix} \right )-\ln 2i \right ]^2\, dx\\   &= \int_{0}^{\pi/3}\left ( \ln^2\left ( e^{-ix}-e^{ix} \right )-2\ln 2i \ln \left ( e^{-ix}-e^{ix} \right ) +\ln^2 2i\right )\,dx\\   &= \int_{0}^{\pi/3} \ln^2 \left ( e^{-ix}-e^{ix} \right )\,dx-2\int_{0}^{\pi/3}\ln 2i \ln \left ( e^{-ix}-e^{ix} \right )\,dx +\int_{0}^{\pi/3}\ln^2 2i \,dx \\  \end{aligned}$ I wrote the first integral as: $\begin{aligned} \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix}-e^{ix} \right )\,dx &= \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix}\left ( 1-e^{-2ix} \right ) \right )\,dx\\   &= \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix} \right )\,dx+\int_{0}^{\pi/3}\ln^2 \left ( 1-e^{-2ix} \right )\,dx\\   \end{aligned}$ Now I used MacLaurin's Expasion of $\ln(1-x)$ for the second integral, so that I can express it as complex series, which is the following: $$\ln^2\left ( 1-e^{-2ix} \right )= \sum_{m=1}^{\infty}\sum_{k=1}^{\infty}\frac{e^{-2ikx}}{k}\frac{e^{-2imx}}{m} \implies \int_{0}^{\pi/3}\ln^2\left ( 1-e^{-2ix} \right )\,dx=\int_{0}^{\pi/3}\left ( \sum_{m=1}^{\infty}\sum_{k=1}^{\infty}\frac{e^{-2ikx}}{k}\frac{e^{-2imx}}{m} \right )\,dx$$ I'm pretty confident that I can alter summation and integral. I don't know if this can help. And this is where I stopped. I can't play around with the middle integral, because of that $\ln(2i)$ term which bothers me, otherwise I would apply the same technic with the MacLaurin expansion. Something also tells me that the last integral should be discarded. Because it's a complex one, but I have not dwelved in it further so I'm not quite sure if there are no cancellations with the other integrals I have, because they all contain complex parts. Any help would be appreciated. P.S: This is not homework.","Good evening! I want to compute the integral $\displaystyle \int_{0}^{\pi/3}\ln^2 \left ( \sin x \right )\,dx$.  However I find it extremely difficult. What I've tried is rewritting it as: $\begin{aligned} \int_{0}^{\pi/3}\ln^2\left ( \sin x \right )\,dx &=\int_{0}^{\pi/3}\left [  \ln \left ( \sin x \right )  \right ]^2\,dx \\   &= \int_{0}^{\pi/3}\left [ \ln \left ( \frac{e^{-ix}-e^{ix}}{2i} \right ) \right ]^2\,dx\\   &= \int_{0}^{\pi/3}\left [ \ln \left ( e^{-ix}-e^{ix} \right )-\ln 2i \right ]^2\, dx\\   &= \int_{0}^{\pi/3}\left ( \ln^2\left ( e^{-ix}-e^{ix} \right )-2\ln 2i \ln \left ( e^{-ix}-e^{ix} \right ) +\ln^2 2i\right )\,dx\\   &= \int_{0}^{\pi/3} \ln^2 \left ( e^{-ix}-e^{ix} \right )\,dx-2\int_{0}^{\pi/3}\ln 2i \ln \left ( e^{-ix}-e^{ix} \right )\,dx +\int_{0}^{\pi/3}\ln^2 2i \,dx \\  \end{aligned}$ I wrote the first integral as: $\begin{aligned} \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix}-e^{ix} \right )\,dx &= \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix}\left ( 1-e^{-2ix} \right ) \right )\,dx\\   &= \int_{0}^{\pi/3}\ln^2 \left ( e^{-ix} \right )\,dx+\int_{0}^{\pi/3}\ln^2 \left ( 1-e^{-2ix} \right )\,dx\\   \end{aligned}$ Now I used MacLaurin's Expasion of $\ln(1-x)$ for the second integral, so that I can express it as complex series, which is the following: $$\ln^2\left ( 1-e^{-2ix} \right )= \sum_{m=1}^{\infty}\sum_{k=1}^{\infty}\frac{e^{-2ikx}}{k}\frac{e^{-2imx}}{m} \implies \int_{0}^{\pi/3}\ln^2\left ( 1-e^{-2ix} \right )\,dx=\int_{0}^{\pi/3}\left ( \sum_{m=1}^{\infty}\sum_{k=1}^{\infty}\frac{e^{-2ikx}}{k}\frac{e^{-2imx}}{m} \right )\,dx$$ I'm pretty confident that I can alter summation and integral. I don't know if this can help. And this is where I stopped. I can't play around with the middle integral, because of that $\ln(2i)$ term which bothers me, otherwise I would apply the same technic with the MacLaurin expansion. Something also tells me that the last integral should be discarded. Because it's a complex one, but I have not dwelved in it further so I'm not quite sure if there are no cancellations with the other integrals I have, because they all contain complex parts. Any help would be appreciated. P.S: This is not homework.",,"['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'improper-integrals']"
47,If $f(f(x)) = x$ and $f(0) = 1$ then what is the value of $\int_0^1 (x - f(x)) ^{2n} dx$,If  and  then what is the value of,f(f(x)) = x f(0) = 1 \int_0^1 (x - f(x)) ^{2n} dx,"I've a question which is mentioned below. If $f(f(x)) = x$ where $x \in [0, 1]$ and $f(0) = 1$ , then find the value of $\displaystyle\int_0^1 (x - f(x))^{2n} dx$ where $n\in \mathbb{N}.$ I tried it and I think I've solved it. I wish if someone could check my work or provide any alternative or easy way to do the same. Here's my work. Given that $f(f(x)) = x$ so $f(x) = f^{-1}(x)$ $\forall\ x\in [0, 1] $ assuming inverse exists . And since $f(0)  = 1$ , we have $f^{-1}(0) = 1$ . Now, from the definition of inverse function, graph of $f(x)$ and $f^{-1}(x)$ are mirror images of each other about the line $y = x$ . Since $f(0) = 1, f^{-1}(1)  = 0$ . This means that $f(x)$ and $f^{-1}(x)$ are coincident lines with equation $x + y = 1$ or $f(x) = 1 -x$ . Graph of the same is attached below. Now Since, $f(x) = 1-x$ , our next task is to find $\displaystyle \int_0^1 (x - f(x))^{2n} dx$ i.e. $\displaystyle\int_0^1 (2x - 1)^{2n} dx$ which is not a hard nut to crack. We have, $$\int_0^1 (2x - 1)^{2n} dx = \frac{(2x-1)^{2n+1}}{2(2n+1)}\bigg|^1_0 = \frac{1}{2(2n+1)} + \frac{1}{2(2n+1)} = \boxed{\frac{1}{2n+1}}.$$ I'm unsure if assuming inverse of the function exists is right. Can anyone please check my work and let me know if I can improve it?","I've a question which is mentioned below. If where and , then find the value of where I tried it and I think I've solved it. I wish if someone could check my work or provide any alternative or easy way to do the same. Here's my work. Given that so assuming inverse exists . And since , we have . Now, from the definition of inverse function, graph of and are mirror images of each other about the line . Since . This means that and are coincident lines with equation or . Graph of the same is attached below. Now Since, , our next task is to find i.e. which is not a hard nut to crack. We have, I'm unsure if assuming inverse of the function exists is right. Can anyone please check my work and let me know if I can improve it?","f(f(x)) = x x \in [0, 1] f(0) = 1 \displaystyle\int_0^1 (x - f(x))^{2n} dx n\in \mathbb{N}. f(f(x)) = x f(x) = f^{-1}(x) \forall\ x\in [0, 1]  f(0)  = 1 f^{-1}(0) = 1 f(x) f^{-1}(x) y = x f(0) = 1, f^{-1}(1)  = 0 f(x) f^{-1}(x) x + y = 1 f(x) = 1 -x f(x) = 1-x \displaystyle \int_0^1 (x - f(x))^{2n} dx \displaystyle\int_0^1 (2x - 1)^{2n} dx \int_0^1 (2x - 1)^{2n} dx = \frac{(2x-1)^{2n+1}}{2(2n+1)}\bigg|^1_0 = \frac{1}{2(2n+1)} + \frac{1}{2(2n+1)} = \boxed{\frac{1}{2n+1}}.","['calculus', 'integration', 'functions', 'definite-integrals']"
48,Finding the convergence interval of $\sum_{n=0}^\infty\frac{n!x^n}{n^n}$.,Finding the convergence interval of .,\sum_{n=0}^\infty\frac{n!x^n}{n^n},"I want to find the convergence interval of the infinite series $\sum\limits_{n=0}^\infty \dfrac{n!x^n}{n^n}$ . I will use the ratio test: if I call $u_n = \dfrac{n!x^n}{n^n}$ , the ratio test says that, if the following is true for some values of $x$ , the series will be convergent for these values of $x$ : $$\lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|<1$$ So, I will first calculate the value of $\left|\dfrac{u_{n+1}}{u_n}\right|$ : $$\left|\dfrac{\dfrac{(n+1)!x^{n+1}}{(n+1)^{n+1}}}{\dfrac{n!x^n}{n^n}}\right|=\dfrac{(n+1)!|x|^{n+1}}{(n+1)^{n+1}}\times\dfrac{n^n}{n!|x|^n}=\frac{(n+1)n^n|x|}{(n+1)^{n+1}}=|x|\left(\frac{n}{n+1}\right)^n$$ So, $\lim\limits_{n\to+\infty}\left|\dfrac{u_{n+1}}{u_n}\right|$ becomes: $$\lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|=\lim_{n\to+\infty}|x|\left(\frac{n}{n+1}\right)^n=|x|\lim_{n\to+\infty}\left(\frac{n}{n+1}\right)^n$$ Now I must evaluate the value of $\lim\limits_{n\to+\infty}\left(\dfrac{n}{n+1}\right)^n$ . For this, let $y = \left(\dfrac{n}{n+1}\right)^n$ ; so, instead of calculating $\lim\limits_{n\to+\infty}y$ , I will first calculate $\lim\limits_{n\to+\infty}\ln y$ : $$\lim_{n\to+\infty}\ln y=\lim_{n\to+\infty}\ln \left(\dfrac{n}{n+1}\right)^n=\lim_{n\to+\infty}n\ln\left(\frac{n}{n+1}\right) =\lim_{n\to+\infty}\frac{\ln\left(\frac{n}{n+1}\right)}{\frac{1}{n}}$$ Applying L'Hôpital's rule: $$\lim_{n\to+\infty}\frac{\ln\left(\frac{n}{n+1}\right)}{\frac{1}{n}} =\lim_{n\to+\infty}\frac{\frac{1}{n(n+1)}}{-\frac{1}{n^2}} =\lim_{n\to+\infty}\left(-\frac{n}{n+1}\right)=-1$$ Now, since we know that $\lim\limits_{n\to+\infty}\ln y = -1$ , we have that: $$\lim_{n\to+\infty}y=\lim_{n\to+\infty}e^{\ln y} = e^{-1} = \frac{1}{e}$$ . Substituting this back into the expression $\lim\limits_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right| = |x|\lim\limits_{n\to+\infty}\left(\frac{n}{n+1}\right)^n$ , we have that the limit of $\left|\dfrac{u_{n+1}}{u_n}\right|$ as $n\to+\infty$ is: $$\lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|=\frac{|x|}{e}$$ Therefore, the series will certainly be convergent for the values of $x$ for which $\dfrac{|x|}{e}<1$ , that is, $|x|<e$ . So, I know that the series is convergent for $-e < x < e$ , but I have to test whether the series is convergent at $x = e$ or $x = -e$ . That is, I have to test whether $\sum\limits_{n=0}^{\infty} \dfrac{n!e^n}{n^n}$ and $\sum\limits_{n=0}^{\infty} \dfrac{(-1)^nn!e^n}{n^n}$ are convergent. Since these limits don't approach zero, I know they are both divergent, but I'm not sure how to find the limits, because of the factorial function. Also, I can't use integral test here, because of the factorial. Probably I should use comparison test, but I haven't found any divergent series to which to compare it. Any hints? Thank you in advance. Edit: Using the suggestion by Ragib Zaman in the answer below, since the Taylor polynomial $P_n(x)$ of $e^x$ at $a=0$ is $$e^x = 1 + x + \dfrac{x^2}{2!} + \cdots + \dfrac{x^n}{n!}+\cdots,$$ if we substitute $n$ for $x$ we see that $e^n>\dfrac{n^n}{n!}$ ; therefore, $\dfrac{n!e^n}{n^n} > 1$ , and, thus, we show that $\sum\limits_{n=0}^{\infty} \dfrac{n!e^n}{n^n}$ is divergent, because its term doesn't approach zero. $\sum\limits_{n=0}^{\infty} \dfrac{(-1)^nn!e^n}{n^n}$ is also divergent, because, although the absolute value of the ratio between two successive terms, $|e|\left(\frac{n}{n+1}\right)^n$ , approaches 1 as $n\to\infty$ , it approaches 1 from values bigger than 1; therefore, the absolute value of a term is always greater than the absolute value of the previous term.","I want to find the convergence interval of the infinite series . I will use the ratio test: if I call , the ratio test says that, if the following is true for some values of , the series will be convergent for these values of : So, I will first calculate the value of : So, becomes: Now I must evaluate the value of . For this, let ; so, instead of calculating , I will first calculate : Applying L'Hôpital's rule: Now, since we know that , we have that: . Substituting this back into the expression , we have that the limit of as is: Therefore, the series will certainly be convergent for the values of for which , that is, . So, I know that the series is convergent for , but I have to test whether the series is convergent at or . That is, I have to test whether and are convergent. Since these limits don't approach zero, I know they are both divergent, but I'm not sure how to find the limits, because of the factorial function. Also, I can't use integral test here, because of the factorial. Probably I should use comparison test, but I haven't found any divergent series to which to compare it. Any hints? Thank you in advance. Edit: Using the suggestion by Ragib Zaman in the answer below, since the Taylor polynomial of at is if we substitute for we see that ; therefore, , and, thus, we show that is divergent, because its term doesn't approach zero. is also divergent, because, although the absolute value of the ratio between two successive terms, , approaches 1 as , it approaches 1 from values bigger than 1; therefore, the absolute value of a term is always greater than the absolute value of the previous term.","\sum\limits_{n=0}^\infty \dfrac{n!x^n}{n^n} u_n = \dfrac{n!x^n}{n^n} x x \lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|<1 \left|\dfrac{u_{n+1}}{u_n}\right| \left|\dfrac{\dfrac{(n+1)!x^{n+1}}{(n+1)^{n+1}}}{\dfrac{n!x^n}{n^n}}\right|=\dfrac{(n+1)!|x|^{n+1}}{(n+1)^{n+1}}\times\dfrac{n^n}{n!|x|^n}=\frac{(n+1)n^n|x|}{(n+1)^{n+1}}=|x|\left(\frac{n}{n+1}\right)^n \lim\limits_{n\to+\infty}\left|\dfrac{u_{n+1}}{u_n}\right| \lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|=\lim_{n\to+\infty}|x|\left(\frac{n}{n+1}\right)^n=|x|\lim_{n\to+\infty}\left(\frac{n}{n+1}\right)^n \lim\limits_{n\to+\infty}\left(\dfrac{n}{n+1}\right)^n y = \left(\dfrac{n}{n+1}\right)^n \lim\limits_{n\to+\infty}y \lim\limits_{n\to+\infty}\ln y \lim_{n\to+\infty}\ln y=\lim_{n\to+\infty}\ln \left(\dfrac{n}{n+1}\right)^n=\lim_{n\to+\infty}n\ln\left(\frac{n}{n+1}\right)
=\lim_{n\to+\infty}\frac{\ln\left(\frac{n}{n+1}\right)}{\frac{1}{n}} \lim_{n\to+\infty}\frac{\ln\left(\frac{n}{n+1}\right)}{\frac{1}{n}}
=\lim_{n\to+\infty}\frac{\frac{1}{n(n+1)}}{-\frac{1}{n^2}}
=\lim_{n\to+\infty}\left(-\frac{n}{n+1}\right)=-1 \lim\limits_{n\to+\infty}\ln y = -1 \lim_{n\to+\infty}y=\lim_{n\to+\infty}e^{\ln y} = e^{-1} = \frac{1}{e} \lim\limits_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right| = |x|\lim\limits_{n\to+\infty}\left(\frac{n}{n+1}\right)^n \left|\dfrac{u_{n+1}}{u_n}\right| n\to+\infty \lim_{n\to+\infty}\left|\frac{u_{n+1}}{u_n}\right|=\frac{|x|}{e} x \dfrac{|x|}{e}<1 |x|<e -e < x < e x = e x = -e \sum\limits_{n=0}^{\infty} \dfrac{n!e^n}{n^n} \sum\limits_{n=0}^{\infty} \dfrac{(-1)^nn!e^n}{n^n} P_n(x) e^x a=0 e^x = 1 + x + \dfrac{x^2}{2!} + \cdots + \dfrac{x^n}{n!}+\cdots, n x e^n>\dfrac{n^n}{n!} \dfrac{n!e^n}{n^n} > 1 \sum\limits_{n=0}^{\infty} \dfrac{n!e^n}{n^n} \sum\limits_{n=0}^{\infty} \dfrac{(-1)^nn!e^n}{n^n} |e|\left(\frac{n}{n+1}\right)^n n\to\infty","['calculus', 'sequences-and-series', 'power-series']"
49,Is it possible to integrate $\int \frac{1} {{\sin x+\sec^2x}}{d}x$,Is it possible to integrate,\int \frac{1} {{\sin x+\sec^2x}}{d}x,"I am a newbie in learning topic of integration.  My friend asked me to find indefinite integral shown below $$I=\int \frac{1} {{\sin x+\sec^2x}} \, \mathrm{d}x \tag 1$$ What I tried until now is the substitution $t=\sin x$ and $\frac{\textrm{d}t}{\textrm{d}x}=\cos x$ . Now, converting equation $(1)$ in terms of $t$ to get $$I=\int \frac{(1-t^2)^{1/2}} {{1+t(1-t^2)}} \, \mathrm{d}t$$ But, as you can see, it became more complicated than the original equation $(1)$ . So, can anybody help me to integrate this integral?","I am a newbie in learning topic of integration.  My friend asked me to find indefinite integral shown below What I tried until now is the substitution and . Now, converting equation in terms of to get But, as you can see, it became more complicated than the original equation . So, can anybody help me to integrate this integral?","I=\int \frac{1} {{\sin x+\sec^2x}} \, \mathrm{d}x \tag 1 t=\sin x \frac{\textrm{d}t}{\textrm{d}x}=\cos x (1) t I=\int \frac{(1-t^2)^{1/2}} {{1+t(1-t^2)}} \, \mathrm{d}t (1)","['calculus', 'integration', 'indefinite-integrals', 'trigonometric-integrals']"
50,Closed form for ${\large\int}_0^1\frac{\ln^3x}{\sqrt{x^2-x+1}}dx$,Closed form for,{\large\int}_0^1\frac{\ln^3x}{\sqrt{x^2-x+1}}dx,"This is a follow-up to my earlier question Closed form for ${\large\int}_0^1\frac{\ln^2x}{\sqrt{1-x+x^2}}dx$ . Is there a closed form for this integral? $$I=\int_0^1\frac{\ln^3x}{\sqrt{x^2-x+1}}dx\tag1$$ Mathematica and Maple cannot evaluate it directly. A numeric approximation for it is $$I\approx-6.1665252325192513801994672415450909679747097867356795481...\tag2$$ (click here to see more digits). As I mentioned in the earlier question, Mathematica is able to find a closed form for a parameterized integral in terms of the Appell hypergeometric function : $$I(a)=\int_0^1\frac{x^a}{\sqrt{x^2-x+1}}dx\\=\frac1{a+1}F_1\left(a+1;\frac{1}{2},\frac{1}{2};a+2;(-1)^{\small1/3},-(-1)^{\small2/3}\right),\tag3$$ but taking a derivative from this looks a hard problem.","This is a follow-up to my earlier question Closed form for ${\large\int}_0^1\frac{\ln^2x}{\sqrt{1-x+x^2}}dx$ . Is there a closed form for this integral? $$I=\int_0^1\frac{\ln^3x}{\sqrt{x^2-x+1}}dx\tag1$$ Mathematica and Maple cannot evaluate it directly. A numeric approximation for it is $$I\approx-6.1665252325192513801994672415450909679747097867356795481...\tag2$$ (click here to see more digits). As I mentioned in the earlier question, Mathematica is able to find a closed form for a parameterized integral in terms of the Appell hypergeometric function : $$I(a)=\int_0^1\frac{x^a}{\sqrt{x^2-x+1}}dx\\=\frac1{a+1}F_1\left(a+1;\frac{1}{2},\frac{1}{2};a+2;(-1)^{\small1/3},-(-1)^{\small2/3}\right),\tag3$$ but taking a derivative from this looks a hard problem.",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'polylogarithm']"
51,Solution of a meme integral: $\int \frac{x \sin(x)}{1+\cos(x)^2}\mathrm{d}x$,Solution of a meme integral:,\int \frac{x \sin(x)}{1+\cos(x)^2}\mathrm{d}x,"Context A few days ago I saw a meme published on a mathematics page in which they joked about the fact that $$\int\frac{x\sin(x)}{1+\cos(x)^2}\mathrm{d}x$$ was very long (and they put a screen shot of the result obtained using Wolfram) (I avoid posting the whole screen because it is actually very very long, just open this link .) My work However, out of curiosity I tried to do it and got the following result: $${\color{blue}{\int_{0}^{x}\frac{t\sin(t)}{1+\cos(t)^2}\mathrm{d}t=-x\operatorname{tan^{-1}}(\cos(x))-2\sum_{n=1}^{\infty}(-1)^n\frac{(\sqrt{2}-1)^{2n-1}}{(2n-1)^2}\sin((2n-1)x)}}$$ Which is ridiculously shorter than the whole result Wolfram gives haha Question I would like to express the last series in terms of known functions, but I can't (I'm pretty sure it's the imaginary part of the dilogarithm, but I can't bring it back), could anyone help me? I'll leave a link for Desmos if anyone wants to work on it.","Context A few days ago I saw a meme published on a mathematics page in which they joked about the fact that was very long (and they put a screen shot of the result obtained using Wolfram) (I avoid posting the whole screen because it is actually very very long, just open this link .) My work However, out of curiosity I tried to do it and got the following result: Which is ridiculously shorter than the whole result Wolfram gives haha Question I would like to express the last series in terms of known functions, but I can't (I'm pretty sure it's the imaginary part of the dilogarithm, but I can't bring it back), could anyone help me? I'll leave a link for Desmos if anyone wants to work on it.",\int\frac{x\sin(x)}{1+\cos(x)^2}\mathrm{d}x {\color{blue}{\int_{0}^{x}\frac{t\sin(t)}{1+\cos(t)^2}\mathrm{d}t=-x\operatorname{tan^{-1}}(\cos(x))-2\sum_{n=1}^{\infty}(-1)^n\frac{(\sqrt{2}-1)^{2n-1}}{(2n-1)^2}\sin((2n-1)x)}},"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'polylogarithm']"
52,Show that $\sum^{6}_{i=1} a_{i}=\frac{15}{2}$ and $ \sum^{6}_{i=1} a^{2}_{i}=\frac{45}{4} \implies \prod_{i=1}^{6} a_{i} \leq \frac{5}{2}$,Show that  and,\sum^{6}_{i=1} a_{i}=\frac{15}{2}  \sum^{6}_{i=1} a^{2}_{i}=\frac{45}{4} \implies \prod_{i=1}^{6} a_{i} \leq \frac{5}{2},"Let $a_{i}$, $1 \leq i \leq 6,$ be real numbers such that $\displaystyle\hspace{1.2 in}\sum^{6}_{i=1} a_{i}=\frac{15}{2}\;\;$ and $\;\;\displaystyle\sum^{6}_{i=1} a^{2}_{i}=\frac{45}{4}$. Prove that $\hspace{.15 in}\displaystyle\prod_{i=1}^{6} a_{i} \leq \frac{5}{2} $. I was thinking if I consider the first summation and extended it, it going be pretty long which $a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+a_{6}= \frac{15}{2}$ and the second one like $a^{2}_{1}+a^{2}_{2}+a^{2}_{3}+a^{2}_{4}+a^{2}_{5}+a^{2}_{6}= \frac{45}{4}$. But I do not think this is the shortest of doing that, I am wondering if someone would be able to give me a hint so I can think better than this. Thank you","Let $a_{i}$, $1 \leq i \leq 6,$ be real numbers such that $\displaystyle\hspace{1.2 in}\sum^{6}_{i=1} a_{i}=\frac{15}{2}\;\;$ and $\;\;\displaystyle\sum^{6}_{i=1} a^{2}_{i}=\frac{45}{4}$. Prove that $\hspace{.15 in}\displaystyle\prod_{i=1}^{6} a_{i} \leq \frac{5}{2} $. I was thinking if I consider the first summation and extended it, it going be pretty long which $a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+a_{6}= \frac{15}{2}$ and the second one like $a^{2}_{1}+a^{2}_{2}+a^{2}_{3}+a^{2}_{4}+a^{2}_{5}+a^{2}_{6}= \frac{45}{4}$. But I do not think this is the shortest of doing that, I am wondering if someone would be able to give me a hint so I can think better than this. Thank you",,"['calculus', 'inequality', 'summation']"
53,The differential equation $y' = \frac{\ln(x^2+y^2)}{x^2 + y^2}$,The differential equation,y' = \frac{\ln(x^2+y^2)}{x^2 + y^2},"In my university, the integral calculus teacher gave me this differential equation to solve. $$ y' = \frac{\ln(x^2+y^2)}{x^2 + y^2} $$ I don't have any clue of what form the solution of this differential equation has.","In my university, the integral calculus teacher gave me this differential equation to solve. I don't have any clue of what form the solution of this differential equation has.", y' = \frac{\ln(x^2+y^2)}{x^2 + y^2} ,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'logarithms']"
54,Looking for examples of Discrete / Continuous complementary approaches,Looking for examples of Discrete / Continuous complementary approaches,,"Among many fascinating sides of mathematics, there is one that I praise, especially for didactic purposes : the parallels that can be drawn between some ""Continuous"" and  ""Discrete"" concepts. I am looking for examples bringing a help to a global understanding... Disclaimer : Being driven, as said above, mainly by didactic purposes, I am not in need for  full rigor here although I do not deny at all the interest of having a rigorous approach in other contexts where it can be essential to show in particular in which sense the continuous ""object"" is the limit of its discrete counterparts. I should appreciate if some colleagues can give examples of their own, in the style ""my favorite one is..."", or references to works about this theme. Let me provide, on my side, five examples : 1st example: How to obtain the equations of certain epicycloids, here a nephroid : Consider a $N$ -sided regular polygon $A_1,A_2,\cdots A_N$ with any integer $N$ large enough, say around $50$ . Let us connect every point $A_k$ to point $A_{3k}$ by a line segment (we assume a cyclic numbering). As can be seen on Fig. 1, a certain envelope curve is ""suggested"". Question : which (smooth) curve is behind this construction ? Answer : Let us consider two consecutive line segments like those represented on Fig. 1 with a larger width : the evolution speed of $A_{3k} \to A_{3k'}$ where $k'=k+1$ is three times the evolution speed of $A_{k} \to A_{k'}$ , the pivoting of the line segment takes place at the point (of the line segment) which is 3 times closer to $A_k$ than to $A_{3k}$ (the weights' ratio 3:1 comes from the size ratio of ''homothetic'' triangles $P_kA_kA_k'$ and $P_kA_{3k}A_{3k'}$ .) Said in an algebraic way : $$P_k=\tfrac{3}{4}e^{ika}+\tfrac{1}{4}e^{3ika}$$ ( $A_k$ is identified with $e^{ika}$ with $a:=\tfrac{2 \pi}{N}$ ). Replacing now discrete values $ka$ by a continuous parameter $t$ , we get $$z=\tfrac{3}{4}e^{it}+\tfrac{1}{4}e^{3it}$$ i.e., a parametric representation of the nephroid, or the equivalent real equations : $$\begin{cases}x=\tfrac{3}{4}\cos(t)+\tfrac{1}{4}\cos(3t)\\ y=\tfrac{3}{4}\sin(t)+\tfrac{1}{4}\sin(3t)\end{cases}$$ Fig. 1 : The nephroid as an envelope. It can be viewed as the trajectory of a point of a small circle with radius $\dfrac14$ rolling inside a circle with radius $1$ . Remark: if, instead of connecting $A_k$ to $A_{3k}$ , we had connected it to $A_{2k}$ , we would have obtained a cardioid, with $A_{4k}$ an astroid, etc. 2nd example: Coupling ''second derivative $ \ \leftrightarrow  \ \min \ $ kernel'' : All functions considered here are at least $C^2$ , but function $K$ . Let $f:[0,1] \rightarrow \mathbb{R}$ and $K:[0,1]\times[0,1]\rightarrow \mathbb{R}$ (a so-called ""kernel"") defined by $K(x,y):=\min(x,y)$ . Let us associate $f$ with function $\varphi(f)=g$ defined by $$\tag{1}g(y)=\int_{t=0}^{t=1} K(t,y)f(t)dt=\int_{t=0}^{t=1} \min(t,y)f(t)dt$$ We can get rid of "" $\min$ "" function by decomposing the integral into : $$\tag{2}g(y)=\int_{t=0}^{t=y} t f(t)dt+\int_{t=y}^{t=1} y f(t)dt$$ $$\tag{3}g(y)=\int_{t=0}^{t=y} t f(t)dt - y F(y)$$ where we have set $$\tag{4}F(y):=\int_{t=1}^{t=y}f(t)dt \ \ \ \ \ \ \ \ \text{Remark:} \ \ \ F'(y)=f(y)$$ Let us differentiate (4) twice : $$\tag{5}g'(y)=y f(y) - 1 F(y) - y f(y) = -F(y)$$ $$\tag{6}g''(y)= -f(y)  \ \ \Longleftrightarrow \ \ f(y)=-g''(y)$$ Said otherwise, the inverse of transform $f \rightarrow \varphi(f)=g$ is: $$\tag{7}\varphi^{-1} = \text{opposite of the second derivative.}$$ This connexion with the second derivative is rather unexpected... Had we taken a discrete approach, what would have been found ? The discrete equivalents of $\varphi$ and $\varphi^{-1}$ are matrices : $$\bf{M}=\begin{pmatrix}1&1&1&\cdots&\cdots&1\\1&2&2&\cdot&\cdots&2\\1&2&3&\cdots&\cdots&3\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\1&2&3&\cdots&\cdots&n \end{pmatrix} \ \ \textbf{and}$$ $$\bf{D}=\begin{pmatrix}2&-1&&&&\\-1&2&-1&&&\\&-1&2&-1&&\\&&\ddots&\ddots&\ddots&\\&&&-1&2&-1\\&&&&-1&1 \end{pmatrix}$$ that verify matrix identity: $\bf{M}^{-1}=\bf{D}$ in analogy with (7). Indeed, Nothing to say about the connection of matrix $\bf{M}$ with coefficients $\bf{M}_{i,j}=min(i,j)$ with operator $K$ . tridiagonal matrix $\bf{D}$ is well known (in particular by all people doing discretization) to be ""the"" discrete analog of the second derivative due to the classical approximation: $$f''(x)\approx\dfrac{1}{2h^2}(f(x-h)-2f(x)+f(x+h))$$ that can easily be obtained using Taylor expansions. The exceptional value $1$ at the bottom right of $\bf{D}$ is explained by discrete boundary conditions. Remark: this correspondence between ""min"" operator and second derivative is not mine ; I known for a long time but  I am unable to trace back where I saw it at first (hopefully in a signal processing book). If somebody has a reference ? Connected : the eigenvalues of $D$ are remarkable ( http://www.math.nthu.edu.tw/~amen/2005/040903-7.pdf ) In the same vein : computation of adjoint operators . 3rd example : the Itô integral. One could think that the Lebesgue integral (1902) is the ultimate theory of integration, correcting the imperfections of the theory elaborated by Riemann some 50 years before. This is not the case. In particular, Itô has defined (1942) a new kind of integral which is now essential in probability and finance. Its principle, roughly said, is that infinitesimal ""deterministic"" increments ""dt"" are replaced by random increments of brownian motion type ""dW"" as formalized by Einstein (1905), then by Wiener (1923). Let us give an image of it. Let us first recall definitions of brownian motion $W(t)$ or $W_t$ , ( $W$ for Wiener), an informal one, and a formal one: Informal : A ""particle"" starting at $x=0$ at time $t$ , jumps ""at the next instant"" $t+dt$ , to a nearby position; either on the left or on the right, the amplitude and sign of the jump being governed by a normal distribution $N(x,\sigma^2)$ with an infinitesimal fixed standard deviation $\sigma.$ $\text{Formal}: \ \ W_t:=G_0 t+\sqrt{2}\sum_{n=1}^{\infty}G_n\dfrac{\sin(\pi n t)}{\pi n}$ , with $G_n$ iid $N(0,1)$ random variables. (Other definitions exist. This one, under the form of a ""random Fourier series"" is handy for many computations). Let us now consider one of the fundamental formulas of Itô's integral, for a continuously differentiable function $f$ : $$\tag{8}\begin{equation} \displaystyle\int_0^t f(W(s))dW(s) = \displaystyle\int_0^{W(t)} f(\lambda)d \lambda - \frac{1}{2}\displaystyle\int_0^t f'(W(s))ds. \end{equation}$$ Remark: The integral sign on the LHS of (8) defines Itô's integral, whereas the integrals on the RHS have to be understood in the sense of Riemann/Lebesgue. The presence of the second term on the RHS is rather puzzling, isnt'it ? Question: how can be understood/justified this second integral ? Szabados has proposed (1990) (see ( https://mathoverflow.net/questions/16163/discrete-version-of-itos-lemma )) a discrete analog of formula (8). Here is how it runs: Theorem: Let $f:\mathbb{Z} \longrightarrow \mathbb{R}$ . let us define : $$ \tag{9}\begin{equation} F(k)=\left\{ \begin{matrix} \dfrac{1}{2}f(0)+\displaystyle\sum_{j=1}^{k-1} f(j)+\dfrac{1}{2}f(k) & if & k \geq 1 & \ \ (a)\\ 0  & if & k = 0 & \ \ (b)\\ -\dfrac{1}{2}f(k)-\displaystyle\sum_{j=k+1}^{-1} f(j)-\dfrac{1}{2}f(0)  & if & k \leq -1 & \ \ (c) \end{matrix} \right. \end{equation} $$ Remarks: We will work only on (a) and its particular case (b). (a) is nothing else than the ""trapezoid formula"" explaining in particular factors $\dfrac{1}{2}$ in front of $f(0)$ et $f(k)$ . Let us now define a family of Random Variables $X_k$ , $k=1, 2, \cdots $ , iid on $\{-1,1\}$ with $P(X_k=-1)=P(X_k=1)=\frac{1}{2}$ , and let $$ \begin{equation} S_n= \displaystyle\sum_{k=1}^n X_k.  \end{equation} $$ Then $$ \tag{10}\begin{equation} \forall n, \ \ \displaystyle\sum_{i=0}^{n}f(S_i)X_{i+1} = F(S_{n+1})-\dfrac{1}{2}\displaystyle\sum_{i=0}^{n}\dfrac{f(S_{i+1})-f(S_{i})}{X_{i+1}} \end{equation} $$ Remark : Please note analogies : between $\frac{f(S_{i+1})-f(S_{i})}{X_{i+1}}$ and $f'(S_i)$ . between $F(k)$ and $\displaystyle\int_{\lambda=0}^{\lambda=k}f(\lambda)d\lambda$ . For example, a) If $f$ is identity function ( $\forall k \  f(k)=k$ ), definition (9)(a) gives : $$ \begin{equation} F(k)=\frac{1}{2}(k-1)k+\frac{1}{2}k=\dfrac{1}{2}k^2. \tag{11} \end{equation} $$ which doesn't come as a surprise : the 'discrete antiderivative' of $k$ is $\frac{1}{2}k^2$ ... (the formula in (11) remains in fact the same for $k<0$ ). b) If $f$ is the ""squaring function"" ( $\forall k, \ f(k)=k^2$ ), (9)(a) becomes : $$ \begin{equation} \text{If} \  k>0, \ \ \  F(k)=\frac{1}{6}(k-1)k(2k-1)+\frac{1}{2}k^2=\dfrac{1}{3}k^3+\dfrac{1}{6}k. \tag{12} \end{equation} $$ This time, a new term $\dfrac{1}{6}k$ has entered into the play. Proof of the Theorem: The definition allows to write : \begin{equation}F(S_{i+1})-F(S_i)=f(S_i)X_{i+1}+\frac{1}{2}\dfrac{f(S_{i+1})-f(S_i)}{X_{i+1}} \tag{13} \end{equation} In fact, proving (11) can be split into two cases: either $X_{i+1}=1$ , or $X_{i+1}=-1$ . Let us consider the first case (the second case is similar): the RHS of (13) becomes $f(S_i)+\frac{1}{2}(f(S_{i+1})-f(S_i))=\frac{1}{2}(f(S_{i+1})+f(S_i))$ which is the area variation in the trapezoid formula ; Summing all equations in (10) gives the desired identity. An example of application : Let $f(t)=t$ ; we get $$\displaystyle\sum_{i=0}^{n}S_iX_{i+1} = F(S_{n+1})-\frac{n}{2}=\dfrac{1}{2}S_{n+1}^2-\frac{n}{2}.$$ which appears as the discrete equivalent of the celebrated formula: $$ \begin{equation} \displaystyle\int_0^t W(s)dW(s) = \frac{1}{2}W(t)^2-\frac{1}{2}t. \end{equation} $$ One can establish the autocorrelation of $W_t$ process is $$cov(W_s,W_t)=E(W_sW_t)-E(W_s)E(W_t)=\min(s,t),$$ (see ( Autocorrelation of a Wiener Process proof )) providing an unexpected connection with the second example... Last remark : Another kind of integral based on a discrete definition : the gauge integral ( https://math.vanderbilt.edu/schectex/ccc/gauge/ ). 4th example (Darboux sums) : Here is a discrete formula : $$\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$$ (see a proof in Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ ) Has this formula a continuous ""counterpart"" ? Taking the logarithm on both sides, and dividing by $n$ , we get : $$\tfrac1n \sum_{k=1}^n \ln \sin \tfrac{k \pi}{n}=\tfrac{\ln(n)}{n}-\ln(2)\tfrac{n-1}{n}$$ Letting now $n \to \infty$ , we obtain the rather classical integral : $$\int_0^1 \ln(\sin(\pi x))dx=-\ln(2)$$ 5th example : bivariate cdfs (cumulative probability density functions). Let $(X,Y)$ a pair of Random Variables with pdf $f_{X,Y}$ and cdf : $$F_{X,Y}(x,y):=P(X \leq x \ \& \ Y \leq y).$$ Take a look at this formula : $$P(x_1<X \leq x_2, \ \ y_1<Y \leq y_2)=F_{XY}(x_2,y_2)-F_{XY}(x_1,y_2)-F_{XY}(x_2,y_1)+F_{XY}(x_1,y_1)\tag{14}$$ ( https://www.probabilitycourse.com/chapter5/5_2_2_joint_cdf.php ) It is the discrete equivalent of the continuous definition of $F_{XY}$ as the mixed second order partial derivative of $F_{X,Y}$ , under the assumption that $F$ is a $C^2$ function : $$f_{XY}(x,y)=\dfrac{\partial^2 F_{X,Y}}{\partial x \partial y}(x,y).\tag{15}$$ Do you see why ? Hint : make $x_2 \to x_1$ and make $y_2 \to y_1$ and assimilate the LHS of (14) with $f(x_1,y_1)dxdy$ . Final remarks : A remarkable text about this analogy in Physics : https://www.lptmc.jussieu.fr/user/lesne/MSCS-Lesne.pdf In linear algebra, continuous analogs of some fundamental factorizations ( https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2014.0585 ). A similar question on MathOverflow mentionning in particular the following well written book ""Computing the continuous discretely"" by  by Beck and Robins. There are many other tracks, e.g., connections with graphs ""Discrete and continuous : two sides of the same."" by L. Lovàsz or this one ( http://jimhuang.org/CDNDSP.pdf ), discrete vs. continuous versions of the logistic equation ( https://math.stackexchange.com/q/3328867 ), etc. In the epidemiology domain: ""Discrete versus continuous-time models of malaria infections"" lecture notes by Lucy Crooks, ETH Zürich . Another example in probability: the connection between a discrete and a continuous distribution, i.e., the Poisson( $\lambda$ ) distribution and the $\Gamma(n)$ distribution which is well treated in [this answer] ( https://math.stackexchange.com/q/2228023 ).","Among many fascinating sides of mathematics, there is one that I praise, especially for didactic purposes : the parallels that can be drawn between some ""Continuous"" and  ""Discrete"" concepts. I am looking for examples bringing a help to a global understanding... Disclaimer : Being driven, as said above, mainly by didactic purposes, I am not in need for  full rigor here although I do not deny at all the interest of having a rigorous approach in other contexts where it can be essential to show in particular in which sense the continuous ""object"" is the limit of its discrete counterparts. I should appreciate if some colleagues can give examples of their own, in the style ""my favorite one is..."", or references to works about this theme. Let me provide, on my side, five examples : 1st example: How to obtain the equations of certain epicycloids, here a nephroid : Consider a -sided regular polygon with any integer large enough, say around . Let us connect every point to point by a line segment (we assume a cyclic numbering). As can be seen on Fig. 1, a certain envelope curve is ""suggested"". Question : which (smooth) curve is behind this construction ? Answer : Let us consider two consecutive line segments like those represented on Fig. 1 with a larger width : the evolution speed of where is three times the evolution speed of , the pivoting of the line segment takes place at the point (of the line segment) which is 3 times closer to than to (the weights' ratio 3:1 comes from the size ratio of ''homothetic'' triangles and .) Said in an algebraic way : ( is identified with with ). Replacing now discrete values by a continuous parameter , we get i.e., a parametric representation of the nephroid, or the equivalent real equations : Fig. 1 : The nephroid as an envelope. It can be viewed as the trajectory of a point of a small circle with radius rolling inside a circle with radius . Remark: if, instead of connecting to , we had connected it to , we would have obtained a cardioid, with an astroid, etc. 2nd example: Coupling ''second derivative kernel'' : All functions considered here are at least , but function . Let and (a so-called ""kernel"") defined by . Let us associate with function defined by We can get rid of "" "" function by decomposing the integral into : where we have set Let us differentiate (4) twice : Said otherwise, the inverse of transform is: This connexion with the second derivative is rather unexpected... Had we taken a discrete approach, what would have been found ? The discrete equivalents of and are matrices : that verify matrix identity: in analogy with (7). Indeed, Nothing to say about the connection of matrix with coefficients with operator . tridiagonal matrix is well known (in particular by all people doing discretization) to be ""the"" discrete analog of the second derivative due to the classical approximation: that can easily be obtained using Taylor expansions. The exceptional value at the bottom right of is explained by discrete boundary conditions. Remark: this correspondence between ""min"" operator and second derivative is not mine ; I known for a long time but  I am unable to trace back where I saw it at first (hopefully in a signal processing book). If somebody has a reference ? Connected : the eigenvalues of are remarkable ( http://www.math.nthu.edu.tw/~amen/2005/040903-7.pdf ) In the same vein : computation of adjoint operators . 3rd example : the Itô integral. One could think that the Lebesgue integral (1902) is the ultimate theory of integration, correcting the imperfections of the theory elaborated by Riemann some 50 years before. This is not the case. In particular, Itô has defined (1942) a new kind of integral which is now essential in probability and finance. Its principle, roughly said, is that infinitesimal ""deterministic"" increments ""dt"" are replaced by random increments of brownian motion type ""dW"" as formalized by Einstein (1905), then by Wiener (1923). Let us give an image of it. Let us first recall definitions of brownian motion or , ( for Wiener), an informal one, and a formal one: Informal : A ""particle"" starting at at time , jumps ""at the next instant"" , to a nearby position; either on the left or on the right, the amplitude and sign of the jump being governed by a normal distribution with an infinitesimal fixed standard deviation , with iid random variables. (Other definitions exist. This one, under the form of a ""random Fourier series"" is handy for many computations). Let us now consider one of the fundamental formulas of Itô's integral, for a continuously differentiable function : Remark: The integral sign on the LHS of (8) defines Itô's integral, whereas the integrals on the RHS have to be understood in the sense of Riemann/Lebesgue. The presence of the second term on the RHS is rather puzzling, isnt'it ? Question: how can be understood/justified this second integral ? Szabados has proposed (1990) (see ( https://mathoverflow.net/questions/16163/discrete-version-of-itos-lemma )) a discrete analog of formula (8). Here is how it runs: Theorem: Let . let us define : Remarks: We will work only on (a) and its particular case (b). (a) is nothing else than the ""trapezoid formula"" explaining in particular factors in front of et . Let us now define a family of Random Variables , , iid on with , and let Then Remark : Please note analogies : between and . between and . For example, a) If is identity function ( ), definition (9)(a) gives : which doesn't come as a surprise : the 'discrete antiderivative' of is ... (the formula in (11) remains in fact the same for ). b) If is the ""squaring function"" ( ), (9)(a) becomes : This time, a new term has entered into the play. Proof of the Theorem: The definition allows to write : In fact, proving (11) can be split into two cases: either , or . Let us consider the first case (the second case is similar): the RHS of (13) becomes which is the area variation in the trapezoid formula ; Summing all equations in (10) gives the desired identity. An example of application : Let ; we get which appears as the discrete equivalent of the celebrated formula: One can establish the autocorrelation of process is (see ( Autocorrelation of a Wiener Process proof )) providing an unexpected connection with the second example... Last remark : Another kind of integral based on a discrete definition : the gauge integral ( https://math.vanderbilt.edu/schectex/ccc/gauge/ ). 4th example (Darboux sums) : Here is a discrete formula : (see a proof in Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ ) Has this formula a continuous ""counterpart"" ? Taking the logarithm on both sides, and dividing by , we get : Letting now , we obtain the rather classical integral : 5th example : bivariate cdfs (cumulative probability density functions). Let a pair of Random Variables with pdf and cdf : Take a look at this formula : ( https://www.probabilitycourse.com/chapter5/5_2_2_joint_cdf.php ) It is the discrete equivalent of the continuous definition of as the mixed second order partial derivative of , under the assumption that is a function : Do you see why ? Hint : make and make and assimilate the LHS of (14) with . Final remarks : A remarkable text about this analogy in Physics : https://www.lptmc.jussieu.fr/user/lesne/MSCS-Lesne.pdf In linear algebra, continuous analogs of some fundamental factorizations ( https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2014.0585 ). A similar question on MathOverflow mentionning in particular the following well written book ""Computing the continuous discretely"" by  by Beck and Robins. There are many other tracks, e.g., connections with graphs ""Discrete and continuous : two sides of the same."" by L. Lovàsz or this one ( http://jimhuang.org/CDNDSP.pdf ), discrete vs. continuous versions of the logistic equation ( https://math.stackexchange.com/q/3328867 ), etc. In the epidemiology domain: ""Discrete versus continuous-time models of malaria infections"" lecture notes by Lucy Crooks, ETH Zürich . Another example in probability: the connection between a discrete and a continuous distribution, i.e., the Poisson( ) distribution and the distribution which is well treated in [this answer] ( https://math.stackexchange.com/q/2228023 ).","N A_1,A_2,\cdots A_N N 50 A_k A_{3k} A_{3k} \to A_{3k'} k'=k+1 A_{k} \to A_{k'} A_k A_{3k} P_kA_kA_k' P_kA_{3k}A_{3k'} P_k=\tfrac{3}{4}e^{ika}+\tfrac{1}{4}e^{3ika} A_k e^{ika} a:=\tfrac{2 \pi}{N} ka t z=\tfrac{3}{4}e^{it}+\tfrac{1}{4}e^{3it} \begin{cases}x=\tfrac{3}{4}\cos(t)+\tfrac{1}{4}\cos(3t)\\
y=\tfrac{3}{4}\sin(t)+\tfrac{1}{4}\sin(3t)\end{cases} \dfrac14 1 A_k A_{3k} A_{2k} A_{4k}  \ \leftrightarrow  \ \min \  C^2 K f:[0,1] \rightarrow \mathbb{R} K:[0,1]\times[0,1]\rightarrow \mathbb{R} K(x,y):=\min(x,y) f \varphi(f)=g \tag{1}g(y)=\int_{t=0}^{t=1} K(t,y)f(t)dt=\int_{t=0}^{t=1} \min(t,y)f(t)dt \min \tag{2}g(y)=\int_{t=0}^{t=y} t f(t)dt+\int_{t=y}^{t=1} y f(t)dt \tag{3}g(y)=\int_{t=0}^{t=y} t f(t)dt - y F(y) \tag{4}F(y):=\int_{t=1}^{t=y}f(t)dt \ \ \ \ \ \ \ \ \text{Remark:} \ \ \ F'(y)=f(y) \tag{5}g'(y)=y f(y) - 1 F(y) - y f(y) = -F(y) \tag{6}g''(y)= -f(y)  \ \ \Longleftrightarrow \ \ f(y)=-g''(y) f \rightarrow \varphi(f)=g \tag{7}\varphi^{-1} = \text{opposite of the second derivative.} \varphi \varphi^{-1} \bf{M}=\begin{pmatrix}1&1&1&\cdots&\cdots&1\\1&2&2&\cdot&\cdots&2\\1&2&3&\cdots&\cdots&3\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\1&2&3&\cdots&\cdots&n
\end{pmatrix} \ \ \textbf{and} \bf{D}=\begin{pmatrix}2&-1&&&&\\-1&2&-1&&&\\&-1&2&-1&&\\&&\ddots&\ddots&\ddots&\\&&&-1&2&-1\\&&&&-1&1
\end{pmatrix} \bf{M}^{-1}=\bf{D} \bf{M} \bf{M}_{i,j}=min(i,j) K \bf{D} f''(x)\approx\dfrac{1}{2h^2}(f(x-h)-2f(x)+f(x+h)) 1 \bf{D} D W(t) W_t W x=0 t t+dt N(x,\sigma^2) \sigma. \text{Formal}: \ \ W_t:=G_0 t+\sqrt{2}\sum_{n=1}^{\infty}G_n\dfrac{\sin(\pi n t)}{\pi n} G_n N(0,1) f \tag{8}\begin{equation}
\displaystyle\int_0^t f(W(s))dW(s) = \displaystyle\int_0^{W(t)} f(\lambda)d \lambda - \frac{1}{2}\displaystyle\int_0^t f'(W(s))ds.
\end{equation} f:\mathbb{Z} \longrightarrow \mathbb{R} 
\tag{9}\begin{equation}
F(k)=\left\{
\begin{matrix}
\dfrac{1}{2}f(0)+\displaystyle\sum_{j=1}^{k-1} f(j)+\dfrac{1}{2}f(k) & if & k \geq 1 & \ \ (a)\\
0  & if & k = 0 & \ \ (b)\\
-\dfrac{1}{2}f(k)-\displaystyle\sum_{j=k+1}^{-1} f(j)-\dfrac{1}{2}f(0)  & if & k \leq -1 & \ \ (c)
\end{matrix}
\right.
\end{equation}
 \dfrac{1}{2} f(0) f(k) X_k k=1, 2, \cdots  \{-1,1\} P(X_k=-1)=P(X_k=1)=\frac{1}{2} 
\begin{equation}
S_n= \displaystyle\sum_{k=1}^n X_k. 
\end{equation}
 
\tag{10}\begin{equation}
\forall n, \ \ \displaystyle\sum_{i=0}^{n}f(S_i)X_{i+1} = F(S_{n+1})-\dfrac{1}{2}\displaystyle\sum_{i=0}^{n}\dfrac{f(S_{i+1})-f(S_{i})}{X_{i+1}}
\end{equation}
 \frac{f(S_{i+1})-f(S_{i})}{X_{i+1}} f'(S_i) F(k) \displaystyle\int_{\lambda=0}^{\lambda=k}f(\lambda)d\lambda f \forall k \  f(k)=k 
\begin{equation}
F(k)=\frac{1}{2}(k-1)k+\frac{1}{2}k=\dfrac{1}{2}k^2.
\tag{11}
\end{equation}
 k \frac{1}{2}k^2 k<0 f \forall k, \ f(k)=k^2 
\begin{equation}
\text{If} \  k>0, \ \ \  F(k)=\frac{1}{6}(k-1)k(2k-1)+\frac{1}{2}k^2=\dfrac{1}{3}k^3+\dfrac{1}{6}k.
\tag{12}
\end{equation}
 \dfrac{1}{6}k \begin{equation}F(S_{i+1})-F(S_i)=f(S_i)X_{i+1}+\frac{1}{2}\dfrac{f(S_{i+1})-f(S_i)}{X_{i+1}}
\tag{13}
\end{equation} X_{i+1}=1 X_{i+1}=-1 f(S_i)+\frac{1}{2}(f(S_{i+1})-f(S_i))=\frac{1}{2}(f(S_{i+1})+f(S_i)) f(t)=t \displaystyle\sum_{i=0}^{n}S_iX_{i+1} = F(S_{n+1})-\frac{n}{2}=\dfrac{1}{2}S_{n+1}^2-\frac{n}{2}. 
\begin{equation}
\displaystyle\int_0^t W(s)dW(s) = \frac{1}{2}W(t)^2-\frac{1}{2}t.
\end{equation}
 W_t cov(W_s,W_t)=E(W_sW_t)-E(W_s)E(W_t)=\min(s,t), \prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}} n \tfrac1n \sum_{k=1}^n \ln \sin \tfrac{k \pi}{n}=\tfrac{\ln(n)}{n}-\ln(2)\tfrac{n-1}{n} n \to \infty \int_0^1 \ln(\sin(\pi x))dx=-\ln(2) (X,Y) f_{X,Y} F_{X,Y}(x,y):=P(X \leq x \ \& \ Y \leq y). P(x_1<X \leq x_2, \ \ y_1<Y \leq y_2)=F_{XY}(x_2,y_2)-F_{XY}(x_1,y_2)-F_{XY}(x_2,y_1)+F_{XY}(x_1,y_1)\tag{14} F_{XY} F_{X,Y} F C^2 f_{XY}(x,y)=\dfrac{\partial^2 F_{X,Y}}{\partial x \partial y}(x,y).\tag{15} x_2 \to x_1 y_2 \to y_1 f(x_1,y_1)dxdy \lambda \Gamma(n)","['calculus', 'linear-algebra', 'probability', 'big-list']"
55,Was Euler right?,Was Euler right?,,"We have a differential equation $$   y + y' = f(x) $$ and assume $f$ is infinitely differentiable. And we want to find particular solution. Then,I set $$   y_p = f(x)-f'(x)+f''(x)...., $$ i.e., $y_p=\sum_{i=0}^{\infty} (-1)^{n} f^{(n)}(x)$. It is easy to observe this formal sum is apparently a particular solution and it really works for all polynomial. For example, if $f(x) = x^3$ then $y_p = x^3 - 3x^2 + 6x - 6$ and it is true. But, when $f(x)=\sin x$ we have $$   y_p=(1-1+1-1....) \sin x - (1-1+1-1...) \cos x $$ since $(1-1+1-1..)=\sum_{i=0}^{\infty}(-1)^n$ is divergent we have no result. Up to here, there is nothing interesting. Euler thought that $\sum_{i=0}^{\infty}(-1)^n = 1/2$ since its result change $(1,0,1,0...)$ for finite sum (It is thought to be famous mistake of Euler. Of course, nobody blames him. In his time, convergency was not defined exactly). But, if take this sum as $1/2$ as Euler said $$   y_p = (\sin x - \cos x)/2 $$ and surprisingly, it is really the particular solution of $y+y'=\sin(x)$. Why does it work? Should we take this sum as $1/2$?","We have a differential equation $$   y + y' = f(x) $$ and assume $f$ is infinitely differentiable. And we want to find particular solution. Then,I set $$   y_p = f(x)-f'(x)+f''(x)...., $$ i.e., $y_p=\sum_{i=0}^{\infty} (-1)^{n} f^{(n)}(x)$. It is easy to observe this formal sum is apparently a particular solution and it really works for all polynomial. For example, if $f(x) = x^3$ then $y_p = x^3 - 3x^2 + 6x - 6$ and it is true. But, when $f(x)=\sin x$ we have $$   y_p=(1-1+1-1....) \sin x - (1-1+1-1...) \cos x $$ since $(1-1+1-1..)=\sum_{i=0}^{\infty}(-1)^n$ is divergent we have no result. Up to here, there is nothing interesting. Euler thought that $\sum_{i=0}^{\infty}(-1)^n = 1/2$ since its result change $(1,0,1,0...)$ for finite sum (It is thought to be famous mistake of Euler. Of course, nobody blames him. In his time, convergency was not defined exactly). But, if take this sum as $1/2$ as Euler said $$   y_p = (\sin x - \cos x)/2 $$ and surprisingly, it is really the particular solution of $y+y'=\sin(x)$. Why does it work? Should we take this sum as $1/2$?",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
56,Differential forms without derivatives?,Differential forms without derivatives?,,"Recently, I've drawn increasingly attracted to using differential forms for routine calculus computations - for instance, I've come to like the equation $$dy=2x\,dx$$ which clearly states that the rate of change of $y$ is $2x$ times the rate of change of $x$ far better than the equation $$\frac{dy}{dx}=2x$$ which states that ""the derivative of $y$ with respect to $x$ "" (whatever that means) is $2x$ . I like the former equation due to the way that the differential form notation easily generalizes to many dimensions (since rates of change can be written as sums of several other rates, if desired), to implicit equations (since no variable is prioritized), and to problems involving multiple variables with relations between them (since differential forms allow for substitution just the same as any other object we do algebra with). While it's easy enough to explain the notation of differential forms intuitively - and indeed, that is why I like them - it seems very difficult to pin down what they are formally for someone who is unfamiliar with calculus and linear algebra, since the usual definition of ""A differential 1-form on a smooth manifold is an a section of the cotangent space of the manifold"" rests upon a solid foundation in the calculus of functions $\mathbb R^n\rightarrow\mathbb R^n$ - so is pretty useless for explanation! In particular, suppose I wanted to set up an intuitive framework where we imagine $y=x^2$ as defining a parabola that we may freely move upon. It's not so hard to see that if $x$ is negative, then increasing $x$ would decrease $y$ and if $x$ is positive, increasing $x$ increases $y$ - and that as we get further from $x=0$ , changing $x$ by a little by changes $y$ by an ever growing proportion - and that we could also, equally well, imagine changing $y$ and see what happens to $x$ . Basically, we have some set of states that we could be in, and we recognize that, if we had some ""velocity"" and were changing the state in some smooth manner, the quantities $y$ and $x$ would also be changing at some rates - and the way that these changes responded to a change in state are somehow encapsulated in the symbols $dy$ and $dx$ - and finally, that these rates turn out to be related if $y$ always equals $x^2$ . This seems all well and good until you want to define $dx$ and $dy$ and to try to prove $dy=2x\,dx$ . This formal side works out fine if you take the usual quotient $\lim_{h\rightarrow 0}\frac{(y+h)^2 - y^2}{h}$ and while this might be fine later to argue about a relation, it doesn't get us any closer to understanding what $dx$ and $dy$ are - and we'd be breaking an inherent symmetry of the differential forms by declaring that our theory only will work if $y$ is a function of $x$ . We could think about parameterized curves on $y=x^2$ and define instantaneous velocity, and say that $dy$ and $dx$ are (linear) rules for assigning rates of change to these velocities, but now our differential forms look very abstract - and we still had to define calculus to define velocity. Maybe we could more explicitly try to think of differential forms as ""local approximations of a function up to a linear term"", but this seems rather abstract. The most promising idea I can think of would be to search of a way to satisfying describe a tangent space as some sort of space of allowed ""velocities"" at a point on a curve/surface and then to define differential forms on that space - but I don't have a good sense of how to do this. Is there a good way to explain differential 1-forms as the fundamental objects of calculus, without relying upon pre-existing calculus knowledge?","Recently, I've drawn increasingly attracted to using differential forms for routine calculus computations - for instance, I've come to like the equation which clearly states that the rate of change of is times the rate of change of far better than the equation which states that ""the derivative of with respect to "" (whatever that means) is . I like the former equation due to the way that the differential form notation easily generalizes to many dimensions (since rates of change can be written as sums of several other rates, if desired), to implicit equations (since no variable is prioritized), and to problems involving multiple variables with relations between them (since differential forms allow for substitution just the same as any other object we do algebra with). While it's easy enough to explain the notation of differential forms intuitively - and indeed, that is why I like them - it seems very difficult to pin down what they are formally for someone who is unfamiliar with calculus and linear algebra, since the usual definition of ""A differential 1-form on a smooth manifold is an a section of the cotangent space of the manifold"" rests upon a solid foundation in the calculus of functions - so is pretty useless for explanation! In particular, suppose I wanted to set up an intuitive framework where we imagine as defining a parabola that we may freely move upon. It's not so hard to see that if is negative, then increasing would decrease and if is positive, increasing increases - and that as we get further from , changing by a little by changes by an ever growing proportion - and that we could also, equally well, imagine changing and see what happens to . Basically, we have some set of states that we could be in, and we recognize that, if we had some ""velocity"" and were changing the state in some smooth manner, the quantities and would also be changing at some rates - and the way that these changes responded to a change in state are somehow encapsulated in the symbols and - and finally, that these rates turn out to be related if always equals . This seems all well and good until you want to define and and to try to prove . This formal side works out fine if you take the usual quotient and while this might be fine later to argue about a relation, it doesn't get us any closer to understanding what and are - and we'd be breaking an inherent symmetry of the differential forms by declaring that our theory only will work if is a function of . We could think about parameterized curves on and define instantaneous velocity, and say that and are (linear) rules for assigning rates of change to these velocities, but now our differential forms look very abstract - and we still had to define calculus to define velocity. Maybe we could more explicitly try to think of differential forms as ""local approximations of a function up to a linear term"", but this seems rather abstract. The most promising idea I can think of would be to search of a way to satisfying describe a tangent space as some sort of space of allowed ""velocities"" at a point on a curve/surface and then to define differential forms on that space - but I don't have a good sense of how to do this. Is there a good way to explain differential 1-forms as the fundamental objects of calculus, without relying upon pre-existing calculus knowledge?","dy=2x\,dx y 2x x \frac{dy}{dx}=2x y x 2x \mathbb R^n\rightarrow\mathbb R^n y=x^2 x x y x x y x=0 x y y x y x dy dx y x^2 dx dy dy=2x\,dx \lim_{h\rightarrow 0}\frac{(y+h)^2 - y^2}{h} dx dy y x y=x^2 dy dx","['calculus', 'differential-geometry', 'differential-forms']"
57,Is there an integral for $\frac{1}{\zeta(3)} $?,Is there an integral for ?,\frac{1}{\zeta(3)} ,"There are many integral representations for $\zeta(3)$ Some lesser known are for instance : $$\int_0^1\frac{x(1-x)}{\sin\pi x}\text{d}x= 7\frac{\zeta(3)}{\pi^3} $$ $$\int_0^1 \frac{\operatorname{li}(x)^3 \space (x-1)}{x^3} \text{d}x = \frac{\zeta(3)}{4} $$ $$\int_0^\pi x(\pi - x) \csc(x) \space \text{d}x = 7 \space  \zeta(3) $$ $$ \int_0^{\infty} \frac{\tanh^2(x)}{x^2} \text{d}x = \frac{ 14 \space \zeta(3)}{\pi^2} $$ $$\int_0^{\frac{\pi}{2}} x \log\tan x \;\text{d}x=\frac{7}{8}\zeta(3)$$ $\zeta(2) $ also has many integral representations as does $ \frac{1}{\zeta(2)} $ , although this is probably because $\frac{1}{\pi}$ and $\frac{1}{\pi^2} $ have many. Well I suspect that because I know no simple integral expression for $\frac{1}{\zeta(3)} $ . My question is: is there some interesting integral $^*$ whose result is simply $\frac{1}{\zeta(3)}$ ? Note $^*$ Interesting integral means that things like $$\int\limits_0^{+\infty} e^{- \zeta(3) \space x}\ \text{d}x = \frac{1}{\zeta(3)} $$ are not a good answer to my question.","There are many integral representations for Some lesser known are for instance : also has many integral representations as does , although this is probably because and have many. Well I suspect that because I know no simple integral expression for . My question is: is there some interesting integral whose result is simply ? Note Interesting integral means that things like are not a good answer to my question.",\zeta(3) \int_0^1\frac{x(1-x)}{\sin\pi x}\text{d}x= 7\frac{\zeta(3)}{\pi^3}  \int_0^1 \frac{\operatorname{li}(x)^3 \space (x-1)}{x^3} \text{d}x = \frac{\zeta(3)}{4}  \int_0^\pi x(\pi - x) \csc(x) \space \text{d}x = 7 \space  \zeta(3)   \int_0^{\infty} \frac{\tanh^2(x)}{x^2} \text{d}x = \frac{ 14 \space \zeta(3)}{\pi^2}  \int_0^{\frac{\pi}{2}} x \log\tan x \;\text{d}x=\frac{7}{8}\zeta(3) \zeta(2)   \frac{1}{\zeta(2)}  \frac{1}{\pi} \frac{1}{\pi^2}  \frac{1}{\zeta(3)}  ^* \frac{1}{\zeta(3)} ^* \int\limits_0^{+\infty} e^{- \zeta(3) \space x}\ \text{d}x = \frac{1}{\zeta(3)} ,"['calculus', 'integration', 'riemann-zeta', 'big-list']"
58,Escaping from a circle of fat lions.,Escaping from a circle of fat lions.,,"You are surrounded, by X fat lions equally spaced around a circle of radius 200 meters in an open field.  While making your escape plan you note several things: they are slow, they can only travel at one tenth of your speed, they are stupid,  they can only move directly at your current position, and they can’t cooperate with one another. If any lion gets within 1 meter of you, you will be eaten. What is the maximum value of X for which you have a strategy to escape from them?","You are surrounded, by X fat lions equally spaced around a circle of radius 200 meters in an open field.  While making your escape plan you note several things: they are slow, they can only travel at one tenth of your speed, they are stupid,  they can only move directly at your current position, and they can’t cooperate with one another. If any lion gets within 1 meter of you, you will be eaten. What is the maximum value of X for which you have a strategy to escape from them?",,"['calculus', 'recreational-mathematics', 'puzzle', 'differential-games']"
59,Prove a $\pi$ inequality: $\left(1+\frac1\pi\right)^{\pi+1}<\pi$,Prove a  inequality:,\pi \left(1+\frac1\pi\right)^{\pi+1}<\pi,"Prove $$\left(1+\frac{1}{\pi}\right)^{\pi+1}<\pi$$    without using calculator I have tried to show that the derivative of $f(x)=x-\left(1+\frac{1}{x}\right)^{x+1}$ is greater than zero , at $x=\pi$ but, it is too hard for me.","Prove $$\left(1+\frac{1}{\pi}\right)^{\pi+1}<\pi$$    without using calculator I have tried to show that the derivative of $f(x)=x-\left(1+\frac{1}{x}\right)^{x+1}$ is greater than zero , at $x=\pi$ but, it is too hard for me.",,"['calculus', 'inequality']"
60,Asymptotic decay rate of an infinite product of sinc functions,Asymptotic decay rate of an infinite product of sinc functions,,"( related to my previous question ) Consider the function $$f(x)=\prod_{n=0}^\infty\operatorname{sinc}\left(\frac{\pi \, x}{2^n}\right),\quad\color{gray}{x\ge0},\tag1$$ where $\operatorname{sinc}(z)$ denotes the sinc function . The function $f(x)$ has zeros at positive integers, and oscillates with a quickly decaying amplitude. Its signs on the intervals between consecutive zeros follow the same pattern as the Thue–Morse sequence . Can we find a real-valued function $g(x)$ (elementary, if possible), such that it is analytic, positive and monotone decreasing (and having monotone derivatives of any order, if possible) and satisfies $$\lim_{t \to \infty} \frac 1 {t-t_0} \int_{t_0}^t \frac{|f(x)|}{g(x)} \, dx=1\tag2$$ for large enough $t_0$? Or, at least, $$\color{gray}{\exists A > 0, \, \exists B > A, \, \forall t > t_0,} \, A < \frac 1 {t-t_0} \int_{t_0}^t \frac{|f(x)|}{g(x)} \, dx < B\tag3$$ for large enough $t_0$? The ""for large enough"" provision means that $g(x)$ is permitted to be not monotone, or have zeros or discontinuities, or be not defined at all for small $t,$ and we only care about its ""eventual"" behavior. Empirically, it looks like $g(x)$ should decay faster than any negative power of $x$, but slower than exponentially. I'm thinking something close to $\exp(-\log^2x)$, but perhaps not exactly that. Update: Maybe this expansion can be useful: $$\prod _{n=0}^m \operatorname{sinc}\left(\frac{\pi\,x}{2^n}\right) = \frac{2^{\binom m2}}{(\pi\,x)^{m+1}} \sum_{n=0}^{2^m-1} t_n\,\sin\left(\!\frac {\pi\,m}2+\frac{2n+1}{2^m}\,\pi\,x\!\right),\tag4$$ where $t_0=1,\,t_n=(-1)^n\,t_{\lfloor n/2\rfloor}$ (the signed Thue–Morse sequence).","( related to my previous question ) Consider the function $$f(x)=\prod_{n=0}^\infty\operatorname{sinc}\left(\frac{\pi \, x}{2^n}\right),\quad\color{gray}{x\ge0},\tag1$$ where $\operatorname{sinc}(z)$ denotes the sinc function . The function $f(x)$ has zeros at positive integers, and oscillates with a quickly decaying amplitude. Its signs on the intervals between consecutive zeros follow the same pattern as the Thue–Morse sequence . Can we find a real-valued function $g(x)$ (elementary, if possible), such that it is analytic, positive and monotone decreasing (and having monotone derivatives of any order, if possible) and satisfies $$\lim_{t \to \infty} \frac 1 {t-t_0} \int_{t_0}^t \frac{|f(x)|}{g(x)} \, dx=1\tag2$$ for large enough $t_0$? Or, at least, $$\color{gray}{\exists A > 0, \, \exists B > A, \, \forall t > t_0,} \, A < \frac 1 {t-t_0} \int_{t_0}^t \frac{|f(x)|}{g(x)} \, dx < B\tag3$$ for large enough $t_0$? The ""for large enough"" provision means that $g(x)$ is permitted to be not monotone, or have zeros or discontinuities, or be not defined at all for small $t,$ and we only care about its ""eventual"" behavior. Empirically, it looks like $g(x)$ should decay faster than any negative power of $x$, but slower than exponentially. I'm thinking something close to $\exp(-\log^2x)$, but perhaps not exactly that. Update: Maybe this expansion can be useful: $$\prod _{n=0}^m \operatorname{sinc}\left(\frac{\pi\,x}{2^n}\right) = \frac{2^{\binom m2}}{(\pi\,x)^{m+1}} \sum_{n=0}^{2^m-1} t_n\,\sin\left(\!\frac {\pi\,m}2+\frac{2n+1}{2^m}\,\pi\,x\!\right),\tag4$$ where $t_0=1,\,t_n=(-1)^n\,t_{\lfloor n/2\rfloor}$ (the signed Thue–Morse sequence).",,"['calculus', 'sequences-and-series', 'limits', 'asymptotics', 'infinite-product']"
61,Is the Maclaurin series expansion of $\sin x$ related to the inclusion-exclusion principle?,Is the Maclaurin series expansion of  related to the inclusion-exclusion principle?,\sin x,"When I see the alternating signs in the infinite series expansion of $\sin x$, I'm reminded of the inclusion-exclusion principle. Could there be any way to visualize it in such a way? Also, is there an elementary reason why the Taylor series approximates a function? I've read the wikipedia entry but didn't understand it so it's greatly appreciated if someone can point me in the right direction.","When I see the alternating signs in the infinite series expansion of $\sin x$, I'm reminded of the inclusion-exclusion principle. Could there be any way to visualize it in such a way? Also, is there an elementary reason why the Taylor series approximates a function? I've read the wikipedia entry but didn't understand it so it's greatly appreciated if someone can point me in the right direction.",,"['calculus', 'combinatorics', 'trigonometry', 'taylor-expansion', 'inclusion-exclusion']"
62,A cotangent series related to the zeta function,A cotangent series related to the zeta function,,"$$\sin x =  x\prod_{n=1}^\infty \left[1-\frac{x^2}{n^2\pi^2}\right]$$ If you apply $\log$ to both sides and derivate: $$\cot x = \frac{1}{x} - \sum_{n=1}^\infty \left[\frac{2x}{n^2\pi^2} \frac{1}{1-\frac{x^2}{n^2\pi^2}}\right]$$ I have to make the expansion: $$\frac{1}{1-\frac{x^2}{n^2\pi^2}} = 1 + {\left(\frac{x^2}{n^2\pi^2}\right)} + \left(\frac{x^2}{n^2\pi^2}\right)^2 + \left(\frac{x^2}{n^2\pi^2}\right)^3 + \cdots \tag{1}$$ But for this, I'm gonna expand the infinite sum: $$\cot x = \frac{1}{x} - \left(\frac{2x}{1^2\pi^2} \frac{1}{1-\frac{x^2}{1^2\pi^2}} + \frac{2x}{2^2\pi^2} \frac{1}{1-\frac{x^2}{2^2\pi^2}} + \frac{2x}{3^2\pi^2} \frac{1}{1-\frac{x^2}{3^2\pi^2}}+\cdots\right)$$ So I can understand the boundaries of this expansion (cause the series $\frac{1}{1-x}=1 + x + x^2 + x^3 + \cdots$ as long as $|x|<1$. So, am I right in saying that I can do the expansion $(1)$ as long as $|x|<\pi$? Because for $x=\pi$ we have: $$\frac{1}{1-\frac{\pi^2}{1^2\pi^2}} = \frac{1}{1-1}$$ And if $|x|<\pi$ the other terms like: $$\frac{1}{1-\frac{\pi^2}{2^2\pi^2}}$$ Can be expanded by $(1)$ too. So: $$\cot x = \frac{1}{x} - \sum_{n=1}^\infty \left[\frac{2x}{n^2\pi^2} \left(1 + {\left(\frac{x^2}{n^2\pi^2}\right)} + \left(\frac{x^2}{n^2\pi^2}\right)^2 + \left(\frac{x^2}{n^2\pi^2}\right)^3 + \cdots \right)\right]\tag{$|x|<\pi$}$$ $$x \cot x = 1 - 2\sum_{n=1}^\infty \left[\frac{x^2}{n^2\pi^2} + \frac{x^4}{n^4\pi^4} + \frac{x^6}{n^6\pi^6} + \frac{x^8}{n^8\pi^8} + \cdots\right]$$ And finally the famous:  $$x \cot x = 1 - 2\sum_{n=1}^{\infty} \left[\zeta(2n)\frac{x^{2n}}{\pi^{2n}}\right]\tag{$|x|<\pi$}$$ Am I right with the boundaries for $x$?","$$\sin x =  x\prod_{n=1}^\infty \left[1-\frac{x^2}{n^2\pi^2}\right]$$ If you apply $\log$ to both sides and derivate: $$\cot x = \frac{1}{x} - \sum_{n=1}^\infty \left[\frac{2x}{n^2\pi^2} \frac{1}{1-\frac{x^2}{n^2\pi^2}}\right]$$ I have to make the expansion: $$\frac{1}{1-\frac{x^2}{n^2\pi^2}} = 1 + {\left(\frac{x^2}{n^2\pi^2}\right)} + \left(\frac{x^2}{n^2\pi^2}\right)^2 + \left(\frac{x^2}{n^2\pi^2}\right)^3 + \cdots \tag{1}$$ But for this, I'm gonna expand the infinite sum: $$\cot x = \frac{1}{x} - \left(\frac{2x}{1^2\pi^2} \frac{1}{1-\frac{x^2}{1^2\pi^2}} + \frac{2x}{2^2\pi^2} \frac{1}{1-\frac{x^2}{2^2\pi^2}} + \frac{2x}{3^2\pi^2} \frac{1}{1-\frac{x^2}{3^2\pi^2}}+\cdots\right)$$ So I can understand the boundaries of this expansion (cause the series $\frac{1}{1-x}=1 + x + x^2 + x^3 + \cdots$ as long as $|x|<1$. So, am I right in saying that I can do the expansion $(1)$ as long as $|x|<\pi$? Because for $x=\pi$ we have: $$\frac{1}{1-\frac{\pi^2}{1^2\pi^2}} = \frac{1}{1-1}$$ And if $|x|<\pi$ the other terms like: $$\frac{1}{1-\frac{\pi^2}{2^2\pi^2}}$$ Can be expanded by $(1)$ too. So: $$\cot x = \frac{1}{x} - \sum_{n=1}^\infty \left[\frac{2x}{n^2\pi^2} \left(1 + {\left(\frac{x^2}{n^2\pi^2}\right)} + \left(\frac{x^2}{n^2\pi^2}\right)^2 + \left(\frac{x^2}{n^2\pi^2}\right)^3 + \cdots \right)\right]\tag{$|x|<\pi$}$$ $$x \cot x = 1 - 2\sum_{n=1}^\infty \left[\frac{x^2}{n^2\pi^2} + \frac{x^4}{n^4\pi^4} + \frac{x^6}{n^6\pi^6} + \frac{x^8}{n^8\pi^8} + \cdots\right]$$ And finally the famous:  $$x \cot x = 1 - 2\sum_{n=1}^{\infty} \left[\zeta(2n)\frac{x^{2n}}{\pi^{2n}}\right]\tag{$|x|<\pi$}$$ Am I right with the boundaries for $x$?",,"['calculus', 'sequences-and-series', 'riemann-zeta']"
63,How to solve this series with a log on the denominator?,How to solve this series with a log on the denominator?,,"So I stumbled upon a series I can't quite solve: $$S=\sum_{n=2}^\infty\frac{(-1)^n}{n\ln(n)}\approx0.526418$$ Notice that if we generalize this: $$S(x)=\sum_{n=2}^\infty\frac{(-1)^n}{n^x\ln(n)}$$ and differentiate with respect to $x$ , we get $$S'(x)=\sum_{n=2}^\infty\frac{(-1)^{n+1}}{n^x}=\eta(x)-1$$ where $\eta$ is the Dirichlet eta function.  It then seems natural to try and integrate backwards: $$S(x)=S(x_0)+x_0-x+\int_{x_0}^x\eta(x)\ dx$$ But I'm not sure how to compute this into some sort of closed form. Also accepting any proofs that this series doesn't have a closed form. By closed form, I mean something involving well-known constants such as $\pi,e,\gamma$ , the Gamma function, the Reimann zeta function, and solutions to algebraic differential equations with algebraic initial conditions.","So I stumbled upon a series I can't quite solve: Notice that if we generalize this: and differentiate with respect to , we get where is the Dirichlet eta function.  It then seems natural to try and integrate backwards: But I'm not sure how to compute this into some sort of closed form. Also accepting any proofs that this series doesn't have a closed form. By closed form, I mean something involving well-known constants such as , the Gamma function, the Reimann zeta function, and solutions to algebraic differential equations with algebraic initial conditions.","S=\sum_{n=2}^\infty\frac{(-1)^n}{n\ln(n)}\approx0.526418 S(x)=\sum_{n=2}^\infty\frac{(-1)^n}{n^x\ln(n)} x S'(x)=\sum_{n=2}^\infty\frac{(-1)^{n+1}}{n^x}=\eta(x)-1 \eta S(x)=S(x_0)+x_0-x+\int_{x_0}^x\eta(x)\ dx \pi,e,\gamma","['calculus', 'sequences-and-series', 'definite-integrals']"
64,"Evaluating $\int_0^\pi \frac{x}{(\sin x)^{\sin (\cos x)}}\,\mathrm{d}x$ [closed]",Evaluating  [closed],"\int_0^\pi \frac{x}{(\sin x)^{\sin (\cos x)}}\,\mathrm{d}x","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Evaluate   $$\int_0^\pi \frac{x}{(\sin x)^{\sin (\cos x)}}\,\mathrm{d}x.$$ I tried using by parts and complex numbers along with series expansion but I was unable to find the answer. Please Help!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Evaluate   $$\int_0^\pi \frac{x}{(\sin x)^{\sin (\cos x)}}\,\mathrm{d}x.$$ I tried using by parts and complex numbers along with series expansion but I was unable to find the answer. Please Help!",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'improper-integrals']"
65,Are there any limit questions which are easier to solve using methods other than l'Hopital's Rule?,Are there any limit questions which are easier to solve using methods other than l'Hopital's Rule?,,"Are there any limit questions which are easier to solve using methods other than l'Hopital's Rule? It seems like for every limit that results in an indeterminate form, you may as well use lHopital's Rule rather than any of the methods that are taught prior to the rule, such as factoring, rationalizing, trig limits, etc. EDIT: These are great answers, thanks!","Are there any limit questions which are easier to solve using methods other than l'Hopital's Rule? It seems like for every limit that results in an indeterminate form, you may as well use lHopital's Rule rather than any of the methods that are taught prior to the rule, such as factoring, rationalizing, trig limits, etc. EDIT: These are great answers, thanks!",,"['calculus', 'limits', 'limits-without-lhopital']"
66,"How to show this formula to get a square root of a number in ""just few seconds"" is true?","How to show this formula to get a square root of a number in ""just few seconds"" is true?",,"I don't remember in which topic I found it but I know it was there. And I still have not find a proof of this nice approximation. Let $x$ be a non perfect square number. If $y$ is the closer perfect square to $x$ such that $y < x$ then $$\sqrt{x}\approx \sqrt{y}+\frac{x-y}{2\cdot \sqrt{y}}$$ And it gives at the maximum two correct decimals after the decimal point. My first reaction to try to find from where this formula goes, was to expand it, I found $$2\sqrt{xy}\approx y +\sqrt{2y}\cdot(x-y)$$ and I tried to find a remarkable identity but I failed and I'm still stuck there. Also I don't know how should I prove the maximum of two correct digits after the comma. If we look for example to the sqare root time of $1000$ , $961$ is the closer perfect square which verifies the condition. Then we have $\sqrt{1000} \approx 31 + \frac{39}{2*31} = 31.62903...$ and with a calculator we have $\sqrt{1000} = 31.6227766017...$ which is quite good. Any hints would be appreciate, thank you in advance.","I don't remember in which topic I found it but I know it was there. And I still have not find a proof of this nice approximation. Let be a non perfect square number. If is the closer perfect square to such that then And it gives at the maximum two correct decimals after the decimal point. My first reaction to try to find from where this formula goes, was to expand it, I found and I tried to find a remarkable identity but I failed and I'm still stuck there. Also I don't know how should I prove the maximum of two correct digits after the comma. If we look for example to the sqare root time of , is the closer perfect square which verifies the condition. Then we have and with a calculator we have which is quite good. Any hints would be appreciate, thank you in advance.",x y x y < x \sqrt{x}\approx \sqrt{y}+\frac{x-y}{2\cdot \sqrt{y}} 2\sqrt{xy}\approx y +\sqrt{2y}\cdot(x-y) 1000 961 \sqrt{1000} \approx 31 + \frac{39}{2*31} = 31.62903... \sqrt{1000} = 31.6227766017...,"['calculus', 'approximation', 'radicals']"
67,What is the most efficient method to evaluate this indefinite integral?,What is the most efficient method to evaluate this indefinite integral?,,"$$\int x^5 e^x\,\mathrm{d}x$$ Is there another, more efficient way to solve this integral that is not integration by parts?","$$\int x^5 e^x\,\mathrm{d}x$$ Is there another, more efficient way to solve this integral that is not integration by parts?",,"['calculus', 'integration', 'indefinite-integrals']"
68,Is the arc length always irrational between two rational points?,Is the arc length always irrational between two rational points?,,"Recently I was wondering: Why does pi have an irrational value as it is simply the ratio of diameter to circumference of a circle? As the value of diameter is rational then the irrationality must come from the circumference. Then I used calculus to calculate the arc length of various functions with curved graphs (between two rational points) and found the arc length two be irrational again. Do all curved paths have irrational lengths? My logic is that while calculating the arc length (calculus) we assume that the arc is composed of infinitely small line segments and we are never close the real value and unlike the area under a curve, there do not exist an upper and lower limit which converges to the same value. If yes, are these the reasons irrational values exist in the first place?","Recently I was wondering: Why does pi have an irrational value as it is simply the ratio of diameter to circumference of a circle? As the value of diameter is rational then the irrationality must come from the circumference. Then I used calculus to calculate the arc length of various functions with curved graphs (between two rational points) and found the arc length two be irrational again. Do all curved paths have irrational lengths? My logic is that while calculating the arc length (calculus) we assume that the arc is composed of infinitely small line segments and we are never close the real value and unlike the area under a curve, there do not exist an upper and lower limit which converges to the same value. If yes, are these the reasons irrational values exist in the first place?",,"['calculus', 'irrational-numbers', 'pi', 'arc-length']"
69,Approximate $\sqrt{e}$ by hand,Approximate  by hand,\sqrt{e},I have seen this question many times as an example of provoking creativity. I wonder how many ways there are to approximate $\sqrt{e}$ by hand as accurately as possible. The obvious way I can think of is to use Taylor expansion. Thanks,I have seen this question many times as an example of provoking creativity. I wonder how many ways there are to approximate $\sqrt{e}$ by hand as accurately as possible. The obvious way I can think of is to use Taylor expansion. Thanks,,"['calculus', 'sequences-and-series', 'algebra-precalculus', 'exponential-function', 'recreational-mathematics']"
70,Evaluate $\frac{0!}{4!}+\frac{1!}{5!}+\frac{2!}{6!}+\frac{3!}{7!}+\frac{4!}{8!}+\cdots$,Evaluate,\frac{0!}{4!}+\frac{1!}{5!}+\frac{2!}{6!}+\frac{3!}{7!}+\frac{4!}{8!}+\cdots,$$\frac{0!}{4!}+\frac{1!}{5!}+\frac{2!}{6!}+\frac{3!}{7!}+\frac{4!}{8!}+\frac{5!}{9!}+\frac{6!}{10!}+\cdots$$ This goes up to infinity. Trying finite cases may help. My Attempt :It seems that it is going to be $\frac{1}{18}$. My calculations show that its going near $\frac{1}{18}$.,$$\frac{0!}{4!}+\frac{1!}{5!}+\frac{2!}{6!}+\frac{3!}{7!}+\frac{4!}{8!}+\frac{5!}{9!}+\frac{6!}{10!}+\cdots$$ This goes up to infinity. Trying finite cases may help. My Attempt :It seems that it is going to be $\frac{1}{18}$. My calculations show that its going near $\frac{1}{18}$.,,"['calculus', 'sequences-and-series', 'factorial']"
71,Please show $\int_0^\infty x^{2n} e^{-x^2}\mathrm dx=\frac{(2n)!}{2^{2n}n!}\frac{\sqrt{\pi}}{2}$  without gamma function?,Please show   without gamma function?,\int_0^\infty x^{2n} e^{-x^2}\mathrm dx=\frac{(2n)!}{2^{2n}n!}\frac{\sqrt{\pi}}{2},Prove: $$\int_0^\infty x^{2n} e^{-x^2}\mathrm dx=\frac{(2n)!}{2^{2n}n!}\frac{\sqrt{\pi}}{2}$$ Thanks!,Prove: $$\int_0^\infty x^{2n} e^{-x^2}\mathrm dx=\frac{(2n)!}{2^{2n}n!}\frac{\sqrt{\pi}}{2}$$ Thanks!,,"['calculus', 'analysis', 'integration']"
72,How to prove that perpendicular from right angled vertex to the hypotenuse is at most half the length of hypotenuse of a right triangle?,How to prove that perpendicular from right angled vertex to the hypotenuse is at most half the length of hypotenuse of a right triangle?,,"In a right-angled triangle $\Delta ABC$ , prove that the perpendicular $BD$ , drawn from the right-angled vertex $B$ to the hypotenuse $AC$ , is at most half the hypotenuse $AC$ . My approach: Assume that $AB=x$ , $BC=y$ , $AC=k$ where $k$ is some arbitrary constant I used Pythagoras theorem in $\Delta ABC$ $$y^2=k^2-x^2,\  y=\sqrt{k^2-x^2}$$ I used formula of area of right triangle ABC by two methods & equate them $$\frac12(BD)\cdot(AC)=\frac12x\cdot y\implies BD=\frac{xy}{k}$$ $$BD=\frac{x\sqrt{k^2-x^2}}{k}$$ I differentiated $BD$ with respect to $x$ $$\frac{d}{dx}BD=\frac{k^2-2x^2}{\sqrt{k^2-x^2}}$$ putting $d(BD)/dx=0$ , I got $x=k/\sqrt2$ & $y=k/\sqrt2$ The maximum length of altitude BD will be $$\frac{xy}{k}=\frac{(k/\sqrt2)\cdot(k/\sqrt2)}{k}=\frac k2$$ Above value proves that maximum value of $BD$ is half the hypotenuse AC. It is fine but I don't want to use this lengthy proof by calculus . My question: Is there any simple or easy proof by using trigonometry, geometry, or other way?","In a right-angled triangle , prove that the perpendicular , drawn from the right-angled vertex to the hypotenuse , is at most half the hypotenuse . My approach: Assume that , , where is some arbitrary constant I used Pythagoras theorem in I used formula of area of right triangle ABC by two methods & equate them I differentiated with respect to putting , I got & The maximum length of altitude BD will be Above value proves that maximum value of is half the hypotenuse AC. It is fine but I don't want to use this lengthy proof by calculus . My question: Is there any simple or easy proof by using trigonometry, geometry, or other way?","\Delta ABC BD B AC AC AB=x BC=y AC=k k \Delta ABC y^2=k^2-x^2,\  y=\sqrt{k^2-x^2} \frac12(BD)\cdot(AC)=\frac12x\cdot y\implies BD=\frac{xy}{k} BD=\frac{x\sqrt{k^2-x^2}}{k} BD x \frac{d}{dx}BD=\frac{k^2-2x^2}{\sqrt{k^2-x^2}} d(BD)/dx=0 x=k/\sqrt2 y=k/\sqrt2 \frac{xy}{k}=\frac{(k/\sqrt2)\cdot(k/\sqrt2)}{k}=\frac k2 BD","['calculus', 'geometry']"
73,"If $f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12}$, then $\int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx =\,$?","If , then ?","f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12} \int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx =\,","This is a question from a practice workbook for a college entrance exam. Let $$f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12}.$$ Find $$\int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx.$$ While I know that computing $f(f(x))$ is an option, it is very time consuming and wouldn't be practical considering the time limit of the exam. I believe there must be a more elegant solution. Looking at the limits, I tried to find useful things about $f(\frac{1}{7}+\frac{6}{7}-x)$ The relation I obtained was that $f(x) + f(1-x) = 12/12 = 1$ . I don't know how to use this for the direct integral of $f(f(x)).$","This is a question from a practice workbook for a college entrance exam. Let Find While I know that computing is an option, it is very time consuming and wouldn't be practical considering the time limit of the exam. I believe there must be a more elegant solution. Looking at the limits, I tried to find useful things about The relation I obtained was that . I don't know how to use this for the direct integral of","f(x) = \frac{x^3}{3} -\frac{x^2}{2} + x + \frac{1}{12}. \int_{\frac{1}{7}}^{\frac{6}{7}}f(f(x))\,dx. f(f(x)) f(\frac{1}{7}+\frac{6}{7}-x) f(x) + f(1-x) = 12/12 = 1 f(f(x)).","['calculus', 'integration', 'functions', 'definite-integrals']"
74,How do I evaluate this integral $\int_0^\pi{\frac{{{x^2}}}{{\sqrt 5-2\cos x}}}\operatorname d\!x$?,How do I evaluate this integral ?,\int_0^\pi{\frac{{{x^2}}}{{\sqrt 5-2\cos x}}}\operatorname d\!x,"Show that $$\int\limits_0^\pi{\frac{{{x^2}}}{{\sqrt 5-2\cos x}}}\operatorname d\!x =\frac{{{\pi^3}}}{{15}}+2\pi \ln^2 \left({\frac{{1+\sqrt 5 }}{2}}\right).$$ I don't have any idea how to start, but maybe I could use the Polylogarithm .","Show that $$\int\limits_0^\pi{\frac{{{x^2}}}{{\sqrt 5-2\cos x}}}\operatorname d\!x =\frac{{{\pi^3}}}{{15}}+2\pi \ln^2 \left({\frac{{1+\sqrt 5 }}{2}}\right).$$ I don't have any idea how to start, but maybe I could use the Polylogarithm .",,"['calculus', 'integration', 'definite-integrals', 'contour-integration']"
75,"Integral $\int_0^1\frac{\log(x)\log(1+x)}{\sqrt{1-x}}\,dx$",Integral,"\int_0^1\frac{\log(x)\log(1+x)}{\sqrt{1-x}}\,dx","I'm trying to evaluate this definite integral: $$\int_0^1\frac{\log(x) \log(1+x)}{\sqrt{1-x}} dx$$ It's clear that the result can be expressed in terms of derivatives of a hypergeometric function with respect to its parameters. I obtained the following form: $$4 \left(1 - \log 2\right){_2F_1}^{(0,1,0,0)}\left(1, 0; \tfrac{3}{2}; -1\right) - 2 {_2F_1}^{(1,1,0,0)}\left(1, 0; \tfrac{3}{2}; -1\right) - 2{_2F_1}^{(0,1,1,0)}\left(1, 0; \tfrac{3}{2}; -1\right)$$ Is it possible to expand these derivatives to some explicit form and further simplify this result? Or maybe you could suggest a different way to evaluate this integral that gives a simpler result without going through hypergeometric functions?","I'm trying to evaluate this definite integral: $$\int_0^1\frac{\log(x) \log(1+x)}{\sqrt{1-x}} dx$$ It's clear that the result can be expressed in terms of derivatives of a hypergeometric function with respect to its parameters. I obtained the following form: $$4 \left(1 - \log 2\right){_2F_1}^{(0,1,0,0)}\left(1, 0; \tfrac{3}{2}; -1\right) - 2 {_2F_1}^{(1,1,0,0)}\left(1, 0; \tfrac{3}{2}; -1\right) - 2{_2F_1}^{(0,1,1,0)}\left(1, 0; \tfrac{3}{2}; -1\right)$$ Is it possible to expand these derivatives to some explicit form and further simplify this result? Or maybe you could suggest a different way to evaluate this integral that gives a simpler result without going through hypergeometric functions?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'hypergeometric-function']"
76,How to prove $\sin(1/x)$ is not uniformly continuous,How to prove  is not uniformly continuous,\sin(1/x),"How do I go about proving $f(x)=\sin(1/x)$ is not uniformly continuous? (Or: different question, but same intention* how do I prove that $x\sin(x)$ is not uniformly continuous) *I'm trying to grasp how one would prove $f$ is not uniformly continuous for functions other than the simple $x^n$. I have seen one technique being to set an $\epsilon$ and set $x, y$ in the form of $\delta$ (e.g. $\delta/2$, etc.) then subsequently proving that $f(x)-f(y)\ge\epsilon$","How do I go about proving $f(x)=\sin(1/x)$ is not uniformly continuous? (Or: different question, but same intention* how do I prove that $x\sin(x)$ is not uniformly continuous) *I'm trying to grasp how one would prove $f$ is not uniformly continuous for functions other than the simple $x^n$. I have seen one technique being to set an $\epsilon$ and set $x, y$ in the form of $\delta$ (e.g. $\delta/2$, etc.) then subsequently proving that $f(x)-f(y)\ge\epsilon$",,['calculus']
77,"A definite integral with trigonometric functions: $\int_{0}^{\pi/2} x^{2} \sqrt{\tan x} \sin(2x) \, \mathrm{d}x$",A definite integral with trigonometric functions:,"\int_{0}^{\pi/2} x^{2} \sqrt{\tan x} \sin(2x) \, \mathrm{d}x","How could we get a closed form for the following integral ?: \begin{equation} \int_{0}^{\pi/2}x^{2}\,\sqrt{\tan\left(x\right)}\, \sin\left(2x\right)\,\mathrm{d}x \label{1}\tag{1} \end{equation} While the antiderivative of $\sqrt{\tan\left(x\right)}\sin\left(2x\right)$ can be expressed in terms of elementary functions according to Wolfram Alpha , the antiderivative of $x^{2}\sqrt{\tan\left(x\right)}\sin\left(2x\right)$ seeminly cannot be expressed in closed form. In order to evaluate (\ref{1}), would introducing a parameter and differentating under the integral sign be helpful ?.","How could we get a closed form for the following integral ?: While the antiderivative of can be expressed in terms of elementary functions according to Wolfram Alpha , the antiderivative of seeminly cannot be expressed in closed form. In order to evaluate (\ref{1}), would introducing a parameter and differentating under the integral sign be helpful ?.","\begin{equation}
\int_{0}^{\pi/2}x^{2}\,\sqrt{\tan\left(x\right)}\,
\sin\left(2x\right)\,\mathrm{d}x \label{1}\tag{1}
\end{equation} \sqrt{\tan\left(x\right)}\sin\left(2x\right) x^{2}\sqrt{\tan\left(x\right)}\sin\left(2x\right)","['calculus', 'integration', 'trigonometry']"
78,Functions that are their Own nth Derivatives for Real $n$,Functions that are their Own nth Derivatives for Real,n,"Consider (non-trivial) functions that are their own nth derivatives.  For instance $\frac{\mathrm{d}}{\mathrm{d}x} e^x = e^x$ $\frac{\mathrm{d}^2}{\mathrm{d}x^2} e^{-x} = e^{-x}$ $\frac{\mathrm{d}^3}{\mathrm{d}x^3} e^{\frac{-x}{2}}\sin(\frac{\sqrt{3}x}{2}) = e^{\frac{-x}{2}}\sin(\frac{\sqrt{3}x}{2})$ $\frac{\mathrm{d}^4}{\mathrm{d}x^4} \sin x = \sin x$ $\cdots$ Let $f_n(x)$ be the function that is it's own nth derivative.  I believe (but I'm not sure) for nonnegative integer $n$, this function can be written as the following infinite polynomial: $f_n(x) = 1 + \cos(\frac{2\pi}{n})x + \cos(\frac{4\pi}{n})\frac{x^2}{2!} + \cos(\frac{6\pi}{n})\frac{x^3}{3!} + \cdots + \cos(\frac{2t\pi}{n})\frac{x^t}{t!} + \cdots$ Is there some sense in which this function can be extended to real n using fractional derivatives?  Would it then be possible to graph $z(n, x) = f_n(x)$, and would this function be smooth and continuous on both $n$ and $x$ axes?  Or would it have many discontinuities?","Consider (non-trivial) functions that are their own nth derivatives.  For instance $\frac{\mathrm{d}}{\mathrm{d}x} e^x = e^x$ $\frac{\mathrm{d}^2}{\mathrm{d}x^2} e^{-x} = e^{-x}$ $\frac{\mathrm{d}^3}{\mathrm{d}x^3} e^{\frac{-x}{2}}\sin(\frac{\sqrt{3}x}{2}) = e^{\frac{-x}{2}}\sin(\frac{\sqrt{3}x}{2})$ $\frac{\mathrm{d}^4}{\mathrm{d}x^4} \sin x = \sin x$ $\cdots$ Let $f_n(x)$ be the function that is it's own nth derivative.  I believe (but I'm not sure) for nonnegative integer $n$, this function can be written as the following infinite polynomial: $f_n(x) = 1 + \cos(\frac{2\pi}{n})x + \cos(\frac{4\pi}{n})\frac{x^2}{2!} + \cos(\frac{6\pi}{n})\frac{x^3}{3!} + \cdots + \cos(\frac{2t\pi}{n})\frac{x^t}{t!} + \cdots$ Is there some sense in which this function can be extended to real n using fractional derivatives?  Would it then be possible to graph $z(n, x) = f_n(x)$, and would this function be smooth and continuous on both $n$ and $x$ axes?  Or would it have many discontinuities?",,"['calculus', 'ordinary-differential-equations', 'fractional-calculus', 'mittag-leffler-function']"
79,Misunderstandings of the fundamental theorem of calculus?,Misunderstandings of the fundamental theorem of calculus?,,"One part of the fundamental theorem of calculus is that $$F(x)=\int_a^x f(t)\;dt\tag1$$ However, $$\int_a^b f(t)\;dt=F(b)-F(a)\tag2$$ So my first question is why doesn’t equation 1 take the form of $\int_a^x f(t)\;dt=F(x)-F(a)$? Where did $F(a)$ disappear to? Also, whenever you see an integral in the form of $F(x)=\int_x^a f(t)\;dt$, why is it that you must always change it to $F(x)=-\int_a^x f(t)\;dt$? That is, why is it necessary for $x$ to be the upper limit and not the lower? I know that it’s written in the fundamental theorem of calculus as the upper limit, but why?","One part of the fundamental theorem of calculus is that $$F(x)=\int_a^x f(t)\;dt\tag1$$ However, $$\int_a^b f(t)\;dt=F(b)-F(a)\tag2$$ So my first question is why doesn’t equation 1 take the form of $\int_a^x f(t)\;dt=F(x)-F(a)$? Where did $F(a)$ disappear to? Also, whenever you see an integral in the form of $F(x)=\int_x^a f(t)\;dt$, why is it that you must always change it to $F(x)=-\int_a^x f(t)\;dt$? That is, why is it necessary for $x$ to be the upper limit and not the lower? I know that it’s written in the fundamental theorem of calculus as the upper limit, but why?",,"['calculus', 'integration', 'definite-integrals']"
80,What is the time derivative of speed?,What is the time derivative of speed?,,"For a particle in projectile motion with a constant upward acceleration of $-9.8\,\mathrm{m/s},$ the time derivative of its speed looks like this: The time derivative of velocity is acceleration, but what is the time derivative of speed , specifically $\sqrt{(x'(t))^2+(y'(t))^2}$ ?","For a particle in projectile motion with a constant upward acceleration of the time derivative of its speed looks like this: The time derivative of velocity is acceleration, but what is the time derivative of speed , specifically ?","-9.8\,\mathrm{m/s}, \sqrt{(x'(t))^2+(y'(t))^2}","['calculus', 'terminology']"
81,There is no smallest infinity in calculus?,There is no smallest infinity in calculus?,,"Somewhat of a basic question, but I tried mixing set theory and calculus and the result is a giant mess. From set theory (assume ZFC) we know there is a smallest infinite cardinal, $\aleph_0$ , and that infinite numbers are well ordered, $\aleph_1 > \aleph_0$ etc Now if we move to the world of calculus, even there, there is a difference between one infinity and the other. $\lim_{x \to \infty} x = \infty$ , and $\lim_{x \to \infty} e^x = \infty$ , but they are not the same, you could say that the $e^x$ one is bigger, because $\lim_{x\to \infty} \frac{e^x}{x} = \infty$ as can be shown easily with L'Hôpital's rule. This leads me to believe that unlike in set theory, in calculus there is no smallest infinity, since if $\lim_{x \to \infty} f(x)= \infty$ , then $\lim_{x \to \infty} \log (f(x) = \infty$ but a smaller $\infty$ . So which version is ""correct""? Is there really a smallest infinity like in set theory? or we can keep getting smaller and smaller to no end like in calculus? Or both are correct in different context? I'm a bit confused. Which also leads to another question, when we say in calculus that some limit tends to $\infty$ , which $\infty$ are we talking about? $\aleph_2$ ? $\aleph_0$ ?","Somewhat of a basic question, but I tried mixing set theory and calculus and the result is a giant mess. From set theory (assume ZFC) we know there is a smallest infinite cardinal, , and that infinite numbers are well ordered, etc Now if we move to the world of calculus, even there, there is a difference between one infinity and the other. , and , but they are not the same, you could say that the one is bigger, because as can be shown easily with L'Hôpital's rule. This leads me to believe that unlike in set theory, in calculus there is no smallest infinity, since if , then but a smaller . So which version is ""correct""? Is there really a smallest infinity like in set theory? or we can keep getting smaller and smaller to no end like in calculus? Or both are correct in different context? I'm a bit confused. Which also leads to another question, when we say in calculus that some limit tends to , which are we talking about? ? ?",\aleph_0 \aleph_1 > \aleph_0 \lim_{x \to \infty} x = \infty \lim_{x \to \infty} e^x = \infty e^x \lim_{x\to \infty} \frac{e^x}{x} = \infty \lim_{x \to \infty} f(x)= \infty \lim_{x \to \infty} \log (f(x) = \infty \infty \infty \infty \aleph_2 \aleph_0,"['calculus', 'limits', 'elementary-set-theory', 'cardinals', 'infinity']"
82,"Calculating $\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx$",Calculating,"\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx","Do you see any fast way of calculating this one? $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx$$ Numerically, it's about $$\approx 111.024457130115028409990464833072173251135063166330638343951498907293$$ or in a predicted closed form $$\frac{4 }{3}\pi ^3+32 \pi  \log (2).$$ Ideas, suggestions, opinions are welcome, and the solutions are optionals . Supplementary question for the integrals lovers: calculate in closed form $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^3 \, dx.$$ As a note, it would be remarkable to be able to find a solution for the generalization below $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^n \, dx.$$","Do you see any fast way of calculating this one? $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^2 \, dx$$ Numerically, it's about $$\approx 111.024457130115028409990464833072173251135063166330638343951498907293$$ or in a predicted closed form $$\frac{4 }{3}\pi ^3+32 \pi  \log (2).$$ Ideas, suggestions, opinions are welcome, and the solutions are optionals . Supplementary question for the integrals lovers: calculate in closed form $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^3 \, dx.$$ As a note, it would be remarkable to be able to find a solution for the generalization below $$\int_0^{\infty } \left(\text{Li}_2\left(-\frac{1}{x^2}\right)\right)^n \, dx.$$",,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'polylogarithm']"
83,"Proof of strictly increasing nature of $y(x)=x^{x^{x^{\ldots}}}$ on $[1,e^{\frac{1}{e}})$?",Proof of strictly increasing nature of  on ?,"y(x)=x^{x^{x^{\ldots}}} [1,e^{\frac{1}{e}})","The title is fairly self explanatory: I have been trying to rigorously prove that $y(x)=x^{x^{x^{\ldots}}}$ is a strictly increasing function over the interval  $[1,e^{\frac{1}{e}})$ for a while now, primarily by exploring various manipulations using logarithms and polylogarithms but have gotten nowhere. Although it is simple enough to show that $y(\sqrt{2})>y(1)$ and if $y'(x)>0$ for some $x \in [1,e^{\frac{1}{e}})$ then $y'(x)>0$ for all $x \in [1,e^{\frac{1}{e}})$ (since either $y$ must be strictly increasing or strictly decreasing), I am not satisfied by the rigor of this argument, although perhaps this is me being too finicky. This lack of progress has led me to explore the possibility that it is only strictly non-decreasing but this loosening of constraints has not helped at all. When it comes to proving that it is a function I've been at a loss as to where I might even begin. Any and all insights are welcome.","The title is fairly self explanatory: I have been trying to rigorously prove that $y(x)=x^{x^{x^{\ldots}}}$ is a strictly increasing function over the interval  $[1,e^{\frac{1}{e}})$ for a while now, primarily by exploring various manipulations using logarithms and polylogarithms but have gotten nowhere. Although it is simple enough to show that $y(\sqrt{2})>y(1)$ and if $y'(x)>0$ for some $x \in [1,e^{\frac{1}{e}})$ then $y'(x)>0$ for all $x \in [1,e^{\frac{1}{e}})$ (since either $y$ must be strictly increasing or strictly decreasing), I am not satisfied by the rigor of this argument, although perhaps this is me being too finicky. This lack of progress has led me to explore the possibility that it is only strictly non-decreasing but this loosening of constraints has not helped at all. When it comes to proving that it is a function I've been at a loss as to where I might even begin. Any and all insights are welcome.",,"['calculus', 'derivatives', 'tetration', 'lambert-w', 'hyperoperation']"
84,How can I sum the infinite series  $\frac{1}{5} - \frac{1\cdot4}{5\cdot10} + \frac{1\cdot4\cdot7}{5\cdot10\cdot15} - \cdots\qquad$,How can I sum the infinite series,\frac{1}{5} - \frac{1\cdot4}{5\cdot10} + \frac{1\cdot4\cdot7}{5\cdot10\cdot15} - \cdots\qquad,"How can I find the sum of the infinite series  $$\frac{1}{5} - \frac{1\cdot4}{5\cdot10} + \frac{1\cdot4\cdot7}{5\cdot10\cdot15} - \cdots\qquad ?$$ My attempt at a solution - I saw that I could rewrite it as  $$\frac{1}{5}\left(1 - \frac{4}{10} \left( 1 - \frac{7}{15} \left(\cdots \left(1 - \frac{3k - 2}{5k}\left( \cdots \right)\right)\right.\right.\right.$$  and that $\frac{3k - 2}{5k} \to \frac{3}{5}$ as $k$ grows larger. Using this I thought it might converge to $\frac{1}{8}$, but I was wrong, the initial terms deviate significantly from $\frac{3}{5}$. According to Wolfram Alpha it converges to $1-\frac{\sqrt[3]{5}}{2}$. How can I get that ?","How can I find the sum of the infinite series  $$\frac{1}{5} - \frac{1\cdot4}{5\cdot10} + \frac{1\cdot4\cdot7}{5\cdot10\cdot15} - \cdots\qquad ?$$ My attempt at a solution - I saw that I could rewrite it as  $$\frac{1}{5}\left(1 - \frac{4}{10} \left( 1 - \frac{7}{15} \left(\cdots \left(1 - \frac{3k - 2}{5k}\left( \cdots \right)\right)\right.\right.\right.$$  and that $\frac{3k - 2}{5k} \to \frac{3}{5}$ as $k$ grows larger. Using this I thought it might converge to $\frac{1}{8}$, but I was wrong, the initial terms deviate significantly from $\frac{3}{5}$. According to Wolfram Alpha it converges to $1-\frac{\sqrt[3]{5}}{2}$. How can I get that ?",,"['calculus', 'sequences-and-series', 'analysis']"
85,What is the difference between a scalar and a vector field?,What is the difference between a scalar and a vector field?,,"Could someone please indicate precisely the difference between a scalar and a vector field? I find no matter how many times I try to understand, but I always am confused in the end. So what exactly makes them different?","Could someone please indicate precisely the difference between a scalar and a vector field? I find no matter how many times I try to understand, but I always am confused in the end. So what exactly makes them different?",,"['calculus', 'multivariable-calculus']"
86,"Intuitively, why should the coefficient of the derivative of $x^n$ be $n$?","Intuitively, why should the coefficient of the derivative of  be ?",x^n n,"I am able to differentiate $x^n$ with respect to $x$ from first principles using the definition of differentiation. Also it seems natural that the gradient of a finite polynomial will be one order lower. However the fact that the coefficient of the derivative of $x^5$ should be $5$, for example, seems less obvious. Is there a way of showing this result so that it is intuitive?","I am able to differentiate $x^n$ with respect to $x$ from first principles using the definition of differentiation. Also it seems natural that the gradient of a finite polynomial will be one order lower. However the fact that the coefficient of the derivative of $x^5$ should be $5$, for example, seems less obvious. Is there a way of showing this result so that it is intuitive?",,"['calculus', 'derivatives', 'polynomials', 'soft-question']"
87,Is $0$ an Infinitesimal?,Is  an Infinitesimal?,0,"For the definition of Infinitesimal, wikipedia says In common speech, an infinitesimal object is an object which is   smaller than any feasible measurement, but not zero in size; or, so   small that it cannot be distinguished from zero by any available   means. MathWorld says An infinitesimal is some quantity that is explicitly nonzero and yet   smaller in absolute value than any real quantity. BUT I met some definition of Infinitesimal in textbooks says If $\lim_{{{x}\to{x}_{{0}}}} f{{\left({x}\right)}}={0}$, then we call   $f(x)$ is an infinitesimal when ${x}\to{x}_{{0}}$. I found the textbooks's definition conflict with the above two definitions.. Obviously, $f(x)=0$ satisfy the textbooks's definition ,then can we call 0 an Infinitesimal ?","For the definition of Infinitesimal, wikipedia says In common speech, an infinitesimal object is an object which is   smaller than any feasible measurement, but not zero in size; or, so   small that it cannot be distinguished from zero by any available   means. MathWorld says An infinitesimal is some quantity that is explicitly nonzero and yet   smaller in absolute value than any real quantity. BUT I met some definition of Infinitesimal in textbooks says If $\lim_{{{x}\to{x}_{{0}}}} f{{\left({x}\right)}}={0}$, then we call   $f(x)$ is an infinitesimal when ${x}\to{x}_{{0}}$. I found the textbooks's definition conflict with the above two definitions.. Obviously, $f(x)=0$ satisfy the textbooks's definition ,then can we call 0 an Infinitesimal ?",,"['calculus', 'limits', 'infinitesimals']"
88,What is the difference between differentiability of a function and continuity of its derivative?,What is the difference between differentiability of a function and continuity of its derivative?,,"I am sort of confused regarding differentiable functions, continuous derivatives, and continuous functions. And I just want to make sure I'm thinking about this correctly. (1) If you have a function that's continuous everywhere, then this doesn't necessarily mean its derivative exists everywhere, correct? e.g., $$f(x) = |x|$$ has an undefined derivative at $x=0$ (2) So this above function, even though its continuous, does not have a continuous derivative? (3) Now say you have a derivative that's continuous everywhere, then this doesn't necessarily mean the underlying function is continuous everywhere, correct? For example, consider $$ f(x) = \begin{cases} 1 - x \ \  \ \ \ x<0 \\ 2 - x \ \ \ \ \ x \geq 0 \end{cases} $$ So its derivative is -1 everywhere, hence continuous, but the function itself is not continuous? So what does a function with a continuous derivative say about the underlying function?","I am sort of confused regarding differentiable functions, continuous derivatives, and continuous functions. And I just want to make sure I'm thinking about this correctly. (1) If you have a function that's continuous everywhere, then this doesn't necessarily mean its derivative exists everywhere, correct? e.g., has an undefined derivative at (2) So this above function, even though its continuous, does not have a continuous derivative? (3) Now say you have a derivative that's continuous everywhere, then this doesn't necessarily mean the underlying function is continuous everywhere, correct? For example, consider So its derivative is -1 everywhere, hence continuous, but the function itself is not continuous? So what does a function with a continuous derivative say about the underlying function?","f(x) = |x| x=0 
f(x) = \begin{cases}
1 - x \ \  \ \ \ x<0 \\
2 - x \ \ \ \ \ x \geq 0
\end{cases}
","['calculus', 'derivatives']"
89,The roots of Hermite polynomials are all real?,The roots of Hermite polynomials are all real?,,The Hermite polynomials are defined as $$H_n(x)=(-1)^n e^{x^2}\dfrac{d^n}{dx^n}e^{-x^2}.$$ How does one prove that all the roots of the Hermite polynomial are real?,The Hermite polynomials are defined as $$H_n(x)=(-1)^n e^{x^2}\dfrac{d^n}{dx^n}e^{-x^2}.$$ How does one prove that all the roots of the Hermite polynomial are real?,,"['calculus', 'special-functions', 'orthogonal-polynomials']"
90,How can I find this limit involving thrice-iterated logarithm?,How can I find this limit involving thrice-iterated logarithm?,,$$\lim_{x \to 0}\dfrac{\ln \ln \ln  \left[x+(1+x)^{(1+x)^{1/x}/x}\right]+x\left[1-\dfrac{1}{e^{e+1}}\right]}{x^2}$$ How can I find the limit of this question? Any hint. Thank you so much.,$$\lim_{x \to 0}\dfrac{\ln \ln \ln  \left[x+(1+x)^{(1+x)^{1/x}/x}\right]+x\left[1-\dfrac{1}{e^{e+1}}\right]}{x^2}$$ How can I find the limit of this question? Any hint. Thank you so much.,,"['calculus', 'limits', 'logarithms']"
91,Why do Lagrange Multipliers work?,Why do Lagrange Multipliers work?,,I know that the Lagrange multiplier method helps us evaluate critical points of $f$ on the closed boundary of the restriction. In other words we solve:$$\nabla f=\lambda \nabla g$$ But why does this method actually work? Can someone please give me an explanation. Thank you!,I know that the Lagrange multiplier method helps us evaluate critical points of $f$ on the closed boundary of the restriction. In other words we solve:$$\nabla f=\lambda \nabla g$$ But why does this method actually work? Can someone please give me an explanation. Thank you!,,"['calculus', 'optimization']"
92,"Integral $\int_0^1 \log \Gamma(x)\cos (2\pi n x)\, dx=\frac{1}{4n}$",Integral,"\int_0^1 \log \Gamma(x)\cos (2\pi n x)\, dx=\frac{1}{4n}","$$ I:=\int_0^1 \log \Gamma(x)\cos (2\pi n x)\, dx=\frac{1}{4n}. $$ Thank you. The Gamma function is given by $\Gamma(n)=(n-1)!$ and its integral representation is $$ \Gamma(x)=\int_0^\infty t^{x-1} e^{-t}\, dt. $$ If we write the gamma function as an integral we end up with a more complicated double integral.  And I am not too equipped with tools for dealing with gamma functions inside integrals. We can possibly try $$ \Re\bigg[\int_0^1 \log \Gamma(x)e^{2\pi i n x}\, dx\bigg]=\frac{1}{4n}. $$ but I still do not where to go from here.  Thanks.","$$ I:=\int_0^1 \log \Gamma(x)\cos (2\pi n x)\, dx=\frac{1}{4n}. $$ Thank you. The Gamma function is given by $\Gamma(n)=(n-1)!$ and its integral representation is $$ \Gamma(x)=\int_0^\infty t^{x-1} e^{-t}\, dt. $$ If we write the gamma function as an integral we end up with a more complicated double integral.  And I am not too equipped with tools for dealing with gamma functions inside integrals. We can possibly try $$ \Re\bigg[\int_0^1 \log \Gamma(x)e^{2\pi i n x}\, dx\bigg]=\frac{1}{4n}. $$ but I still do not where to go from here.  Thanks.",,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'gamma-function']"
93,Show $ \int_0^\infty\left(1-x\sin\frac 1 x\right)dx = \frac\pi 4 $,Show, \int_0^\infty\left(1-x\sin\frac 1 x\right)dx = \frac\pi 4 ,How to show that $$ \int_0^\infty\left(1-x\sin\frac{1}{x}\right)dx=\frac{\pi}{4} $$ ?,How to show that $$ \int_0^\infty\left(1-x\sin\frac{1}{x}\right)dx=\frac{\pi}{4} $$ ?,,"['calculus', 'integration', 'improper-integrals']"
94,What is divergence in image processing?,What is divergence in image processing?,,"What is the difference between gradient and divergence? I understood that gradient points in the direction of steepest ascent and divergence measures source strength. I couldn't relate this to the concept of divergence in image processing. What is divergence in image processing, and how is it related to the gradient? I have also asked a related question: https://dsp.stackexchange.com/questions/14606/anisotropic-diffusion . I couldn't understand the mathematics in it, but I roughly understood the theory. I need to understand the mathematics for the implementation. I am trying to understand the equations in the above link: can you explain what can be concluded from these equations? How do they differ from each other?","What is the difference between gradient and divergence? I understood that gradient points in the direction of steepest ascent and divergence measures source strength. I couldn't relate this to the concept of divergence in image processing. What is divergence in image processing, and how is it related to the gradient? I have also asked a related question: https://dsp.stackexchange.com/questions/14606/anisotropic-diffusion . I couldn't understand the mathematics in it, but I roughly understood the theory. I need to understand the mathematics for the implementation. I am trying to understand the equations in the above link: can you explain what can be concluded from these equations? How do they differ from each other?",,"['calculus', 'machine-learning', 'image-processing']"
95,Axiom of choice and calculus,Axiom of choice and calculus,,"I thought many results in calculus need axiom choice. For example, I thought one needs AC to prove that a bounded sequence in the real line has a convergent subsequence. Recently I was taught that one only needs mathematical induction to prove it. So here are my questions. Can most results in calculus be proved without AC? If yes, what are the exceptions, to name a few? Obviously a theorem which uses Zorn's lemma most likely does need AC. So please exclude obvious ones. Edit By ""without AC"", I mean without any form of AC, i.e. countable or not, dependent or not. In other words, within ZF. Edit One of the motivations of my question is as follows. People often unconciously use AC to prove theorems. And it often turns out that their uses of AC are unnecessary. For example, an infinite subset of a compact metric space has a limit point. In his book ""Principles of mathematical analysis"", Rudin proves this by choosing a suitable neighborhood of every point of the space. He uses AC here, though he doesn't say so. However, since a compact metric space is separable, you can avoid AC to prove this theorem. Edit I'll make the above statement clearer. You can even avoid countable AC to prove the above theorem. In other words, you can prove it within ZF. Edit I'll make my questions clearer and more specific. By calculus, I mean classical analysis in Euclidean spaces. Take, for example, Rudin's ""Principles of mathematical analysis"". Can all the results in this book be proved within ZF? If not, what are the exceptions?","I thought many results in calculus need axiom choice. For example, I thought one needs AC to prove that a bounded sequence in the real line has a convergent subsequence. Recently I was taught that one only needs mathematical induction to prove it. So here are my questions. Can most results in calculus be proved without AC? If yes, what are the exceptions, to name a few? Obviously a theorem which uses Zorn's lemma most likely does need AC. So please exclude obvious ones. Edit By ""without AC"", I mean without any form of AC, i.e. countable or not, dependent or not. In other words, within ZF. Edit One of the motivations of my question is as follows. People often unconciously use AC to prove theorems. And it often turns out that their uses of AC are unnecessary. For example, an infinite subset of a compact metric space has a limit point. In his book ""Principles of mathematical analysis"", Rudin proves this by choosing a suitable neighborhood of every point of the space. He uses AC here, though he doesn't say so. However, since a compact metric space is separable, you can avoid AC to prove this theorem. Edit I'll make the above statement clearer. You can even avoid countable AC to prove the above theorem. In other words, you can prove it within ZF. Edit I'll make my questions clearer and more specific. By calculus, I mean classical analysis in Euclidean spaces. Take, for example, Rudin's ""Principles of mathematical analysis"". Can all the results in this book be proved within ZF? If not, what are the exceptions?",,"['calculus', 'set-theory', 'axiom-of-choice']"
96,integrate square of $\arctan x$.  Tricky,integrate square of .  Tricky,\arctan x,"$$\int \left(\frac{\tan^{-1}x}{x-\tan^{-1}x}\right)^{2}dx$$ I ran across an integral I am having a time solving. The solution merely works out to $\displaystyle\frac{1+x\tan^{-1}x}{\tan^{-1}x-x}$, but for the life of me I can not find a suitable method to tackle it. Does anyone have any hints on a good strategy, substitution, parts, etc?. Thanks much.","$$\int \left(\frac{\tan^{-1}x}{x-\tan^{-1}x}\right)^{2}dx$$ I ran across an integral I am having a time solving. The solution merely works out to $\displaystyle\frac{1+x\tan^{-1}x}{\tan^{-1}x-x}$, but for the life of me I can not find a suitable method to tackle it. Does anyone have any hints on a good strategy, substitution, parts, etc?. Thanks much.",,"['calculus', 'trigonometry', 'integration']"
97,Closed form of $I=\int_{0}^{\pi/2} \tan^{-1} \bigg( \frac{\cos(x)}{\sin(x) - 1 - \sqrt{2}} \bigg) \tan(x)\;dx$,Closed form of,I=\int_{0}^{\pi/2} \tan^{-1} \bigg( \frac{\cos(x)}{\sin(x) - 1 - \sqrt{2}} \bigg) \tan(x)\;dx,"Does the integral below have a closed-form: $$I=\int_{0}^{\pi/2} \tan^{-1} \bigg( \frac{\cos(x)}{\sin(x) - 1 - \sqrt{2}} \bigg) \tan(x)\;dx,$$ where $\tan^{-1} (\cdot)$ is inverse tangent function. Neither Wolfram|Alpha nor Maple 13 can return possible closed-form of the integral. I cannot also find a similar integral in G&R 8th edition. Its numeric integral computed by Maple 13 is $$I=-0.60581547102487915432009247784178206365553774419860 ...$$ This integral came up in discussion during symbolic computation seminar. Our professor asked us to help him to find the closed form of several integrals. This one comes from the study in topic special function: Inverse Tangent Integral. As I said in my comment and I'll add some details, we have tried many substitutions, standard techniques such as: integration by parts, differentiation under integral sign, etc. We also tried method of countour integration, but none of them gave promising result so far. We have been evaluating this integral since 2 weeks ago but no success. So, I thought it's about time to ask you for help. Can you help me out to find its closed-form, please? Can someone help to prove the closed-form given by users Cleo and Anastasiya Romanova? Thanks.","Does the integral below have a closed-form: $$I=\int_{0}^{\pi/2} \tan^{-1} \bigg( \frac{\cos(x)}{\sin(x) - 1 - \sqrt{2}} \bigg) \tan(x)\;dx,$$ where $\tan^{-1} (\cdot)$ is inverse tangent function. Neither Wolfram|Alpha nor Maple 13 can return possible closed-form of the integral. I cannot also find a similar integral in G&R 8th edition. Its numeric integral computed by Maple 13 is $$I=-0.60581547102487915432009247784178206365553774419860 ...$$ This integral came up in discussion during symbolic computation seminar. Our professor asked us to help him to find the closed form of several integrals. This one comes from the study in topic special function: Inverse Tangent Integral. As I said in my comment and I'll add some details, we have tried many substitutions, standard techniques such as: integration by parts, differentiation under integral sign, etc. We also tried method of countour integration, but none of them gave promising result so far. We have been evaluating this integral since 2 weeks ago but no success. So, I thought it's about time to ask you for help. Can you help me out to find its closed-form, please? Can someone help to prove the closed-form given by users Cleo and Anastasiya Romanova? Thanks.",,"['calculus', 'integration', 'definite-integrals', 'contour-integration', 'closed-form']"
98,"Evaluate definite integral $\int_{-1}^1 \exp(1/(x^2-1)) \, dx$",Evaluate definite integral,"\int_{-1}^1 \exp(1/(x^2-1)) \, dx","How to evaluate the following definite integral: $$\int_{-1}^1 \exp\left(\frac1{x^2-1}\right) \, dx$$ It seems that indefinite integral also cannot be expressed in standard functions. I would like any solution in popular elementary or non-elementary functions.","How to evaluate the following definite integral: $$\int_{-1}^1 \exp\left(\frac1{x^2-1}\right) \, dx$$ It seems that indefinite integral also cannot be expressed in standard functions. I would like any solution in popular elementary or non-elementary functions.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
99,Compute $\sum_{n=1}^\infty\frac{H_n^2H_n^{(2)}}{n^3}$,Compute,\sum_{n=1}^\infty\frac{H_n^2H_n^{(2)}}{n^3},"How to prove $$\sum_{n=1}^\infty\frac{H_n^2H_n^{(2)}}{n^3}=\frac{19}{2}\zeta(3)\zeta(4)-2\zeta(2)\zeta(5)-7\zeta(7)\ ?$$ where $H_n^{(p)}=1+\frac1{2^p}+\cdots+\frac1{n^p}$ is the $n$ th generalized harmonic number of order $p$ . This series is very advanced and can be found evaluated in the book (Almost) Impossible Integrals, Sums and Series page 300 using only series manipulations, but luckily I was able to evaluate it using only integration, some harmonic identities and results of easy Euler sums. Can we prove the equality above in different methods besides series manipulation and the idea of my solution below? All approaches are highly appreciated. Solution is posted in the answer section. Thanks","How to prove where is the th generalized harmonic number of order . This series is very advanced and can be found evaluated in the book (Almost) Impossible Integrals, Sums and Series page 300 using only series manipulations, but luckily I was able to evaluate it using only integration, some harmonic identities and results of easy Euler sums. Can we prove the equality above in different methods besides series manipulation and the idea of my solution below? All approaches are highly appreciated. Solution is posted in the answer section. Thanks",\sum_{n=1}^\infty\frac{H_n^2H_n^{(2)}}{n^3}=\frac{19}{2}\zeta(3)\zeta(4)-2\zeta(2)\zeta(5)-7\zeta(7)\ ? H_n^{(p)}=1+\frac1{2^p}+\cdots+\frac1{n^p} n p,"['calculus', 'integration', 'sequences-and-series', 'riemann-zeta', 'harmonic-numbers']"
